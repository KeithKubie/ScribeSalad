WEBVTT
Kind: captions
Language: en

00:00:01.870 --> 00:00:04.160
MALE SPEAKER: I'll read
a quick bio about Daniel.

00:00:04.160 --> 00:00:07.410
He's the author of The New
York Times bestseller "Demon,"

00:00:07.410 --> 00:00:09.920
as well as "Freedom," "Kill
Decision," and his newest

00:00:09.920 --> 00:00:11.640
book, "Influx."

00:00:11.640 --> 00:00:14.645
Former systems analyst to
Fortune 1000 companies,

00:00:14.645 --> 00:00:16.020
Daniel has designed
and developed

00:00:16.020 --> 00:00:17.860
software for the
defense, finance,

00:00:17.860 --> 00:00:19.220
and entertainment industries.

00:00:19.220 --> 00:00:23.510
He is a past speaker at
TEDGlobal, MIT Media Lab, NASA

00:00:23.510 --> 00:00:27.640
Ames, The Long Now
Foundation, Microsoft, Amazon,

00:00:27.640 --> 00:00:30.720
and he was at South By
Southwest earlier this week.

00:00:30.720 --> 00:00:33.939
He lives in L.A.
Welcome, Daniel.

00:00:33.939 --> 00:00:34.980
DANIEL SUAREZ: Thank you.

00:00:38.080 --> 00:00:39.480
It's always good
to visit Google.

00:00:39.480 --> 00:00:41.030
Always love this.

00:00:41.030 --> 00:00:42.260
Yeah, so the new book.

00:00:42.260 --> 00:00:45.149
What I'll do before we start
is just briefly describe

00:00:45.149 --> 00:00:45.690
the new book.

00:00:45.690 --> 00:00:48.320
I don't know how many of you
have had a chance to read it.

00:00:48.320 --> 00:00:52.670
This book, "Influx,"
is a bit more sci-fi

00:00:52.670 --> 00:00:54.230
than my previous books.

00:00:54.230 --> 00:00:56.290
For those familiar
with my work, I

00:00:56.290 --> 00:00:58.870
tend to do tech
thrillers that are just

00:00:58.870 --> 00:01:01.510
over the horizon,
very near-term.

00:01:01.510 --> 00:01:02.970
And in this
particular book I was

00:01:02.970 --> 00:01:06.770
trying to capture something
that was, to me, a larger trend,

00:01:06.770 --> 00:01:09.470
and I wanted to write it
on a really epic stage.

00:01:09.470 --> 00:01:12.070
So I was using more
sci-fi elements--

00:01:12.070 --> 00:01:14.750
anti-gravity fusion,
artificial intelligence,

00:01:14.750 --> 00:01:15.990
immortality-- my god.

00:01:15.990 --> 00:01:18.810
Yeah, a whole bunch of things.

00:01:18.810 --> 00:01:26.160
So "Influx" is the story of a
very talented, young physicist

00:01:26.160 --> 00:01:28.480
who conceives of
a gravity mirror.

00:01:28.480 --> 00:01:31.610
A technology that can reflect
gravity in various directions.

00:01:31.610 --> 00:01:33.380
And rather than
get a Nobel Prize,

00:01:33.380 --> 00:01:38.230
he's grabbed by a
secretive group that

00:01:38.230 --> 00:01:43.030
is trying to prevent disruptive
technologies from rapidly

00:01:43.030 --> 00:01:44.000
altering society.

00:01:44.000 --> 00:01:46.797
They're fearful of the
social ramifications of it.

00:01:46.797 --> 00:01:49.130
That's really what I was
trying to get out in the story,

00:01:49.130 --> 00:01:55.290
was this impulse to preserve
the status quo that humans have.

00:01:55.290 --> 00:01:58.220
And we've seen shades of it.

00:01:58.220 --> 00:02:02.030
If we go back through
history, we see in the 1600s

00:02:02.030 --> 00:02:03.970
the efforts by the
Church at the time

00:02:03.970 --> 00:02:06.600
to stop the spread of
the printing press.

00:02:06.600 --> 00:02:09.770
Or the telescope, for instance,
would be a good example.

00:02:09.770 --> 00:02:11.570
But we can fast
forward to today.

00:02:11.570 --> 00:02:14.690
We were just talking
about Tesla and efforts

00:02:14.690 --> 00:02:19.080
to stop that car company from
doing business in various areas

00:02:19.080 --> 00:02:20.860
without franchise operations.

00:02:20.860 --> 00:02:26.160
Also let's see, the Time--
the Comcast Time Warner deal.

00:02:26.160 --> 00:02:28.810
That's another example
of preserving status quo.

00:02:28.810 --> 00:02:31.010
So a little less violent
than the third Inquisition,

00:02:31.010 --> 00:02:32.600
but similar impulse.

00:02:32.600 --> 00:02:35.260
So that's really why
I wrote this book,

00:02:35.260 --> 00:02:39.670
was to study that particular
impulse of human nature,

00:02:39.670 --> 00:02:41.570
and to do it in a
very exciting story.

00:02:41.570 --> 00:02:42.970
That was the goal.

00:02:42.970 --> 00:02:44.959
So here I am.

00:02:44.959 --> 00:02:45.750
MALE SPEAKER: Nice.

00:02:45.750 --> 00:02:47.950
Thanks a lot for coming.

00:02:47.950 --> 00:02:50.640
So can you talk a little
bit about the difference

00:02:50.640 --> 00:02:54.190
about writing a book that's sort
of about near-term technology

00:02:54.190 --> 00:02:56.292
versus something more sci-fi?

00:02:56.292 --> 00:02:58.750
Some of the differences from
your experience of writing it?

00:02:58.750 --> 00:02:59.730
DANIEL SUAREZ: Sure.

00:02:59.730 --> 00:03:02.686
What I would do is typically
when I'm writing a book,

00:03:02.686 --> 00:03:04.310
I will do an immense
amount of research

00:03:04.310 --> 00:03:05.660
on the topic I'm interested in.

00:03:05.660 --> 00:03:08.490
And for those who remember
"Kill Decision," my last book

00:03:08.490 --> 00:03:11.320
on drones, for instance,
autonomous drones.

00:03:11.320 --> 00:03:14.070
I read lots of books on
swarming intelligence.

00:03:14.070 --> 00:03:16.610
I examined lots of
technologies for how

00:03:16.610 --> 00:03:19.650
to get drones to work
together, all of that stuff.

00:03:19.650 --> 00:03:21.270
And that's very realistic stuff.

00:03:21.270 --> 00:03:25.230
For this book, the
elephant in the room for me

00:03:25.230 --> 00:03:26.570
was anti-gravity.

00:03:26.570 --> 00:03:29.280
In other words, I foolishly
created this brand

00:03:29.280 --> 00:03:31.500
where I have very, very
authentic technologies

00:03:31.500 --> 00:03:34.520
in my book, and then I set about
writing an anti-gravity story.

00:03:34.520 --> 00:03:36.100
That makes it tough.

00:03:36.100 --> 00:03:41.100
And so I did a lot of research
on theoretical physics.

00:03:41.100 --> 00:03:43.390
And then I finally sat
down with some physicists

00:03:43.390 --> 00:03:48.190
and tried to pester them
into finding some gray area

00:03:48.190 --> 00:03:51.140
in knowledge of
gravitation, where

00:03:51.140 --> 00:03:54.420
I could put this
technology for my story.

00:03:54.420 --> 00:03:57.650
Basically, I sat
down with two people,

00:03:57.650 --> 00:04:00.370
one in particular--
what I wanted to do

00:04:00.370 --> 00:04:03.330
was to make it so that
a physicist, or somebody

00:04:03.330 --> 00:04:06.350
who is at least
knowledgeable about science,

00:04:06.350 --> 00:04:07.960
could read this
book and enjoy it.

00:04:07.960 --> 00:04:11.840
That they could see that it
is often a theoretical area

00:04:11.840 --> 00:04:13.660
we're not sure of just yet.

00:04:13.660 --> 00:04:15.610
And generally the
way I did this was,

00:04:15.610 --> 00:04:17.390
I would keep on
making suggestions

00:04:17.390 --> 00:04:20.050
by talking with this
physicist friend at JPL,

00:04:20.050 --> 00:04:23.480
and whenever he'd say,
that's highly improbable,

00:04:23.480 --> 00:04:25.670
I would say, ah, but
there's a chance.

00:04:25.670 --> 00:04:28.230
And then we would go from there.

00:04:28.230 --> 00:04:30.430
So I absolved him
of all my crimes

00:04:30.430 --> 00:04:32.110
against physics in
the acknowledgements,

00:04:32.110 --> 00:04:34.110
just so that he's not
embarrassed to go to work.

00:04:34.110 --> 00:04:36.760
It's like, I had
nothing to do with that.

00:04:36.760 --> 00:04:39.370
But that's, for instance,
how I would try to do that.

00:04:39.370 --> 00:04:43.120
I really try to go that
extra mile, so that

00:04:43.120 --> 00:04:46.660
even if you have a
pretty good grasp of how

00:04:46.660 --> 00:04:48.330
the world, the
universe, functions,

00:04:48.330 --> 00:04:50.771
how gravitation functions,
that you can go with it.

00:04:50.771 --> 00:04:52.270
Now some folks have
brought up to me

00:04:52.270 --> 00:04:53.895
that the beginning
of the book, there's

00:04:53.895 --> 00:04:55.922
parts that are a little
dense on the science.

00:04:55.922 --> 00:04:57.630
It's like a little
fence around the story

00:04:57.630 --> 00:05:00.450
that once you step into it,
it's much more accessible,

00:05:00.450 --> 00:05:02.970
but I wanted to get that
out of the way early on.

00:05:02.970 --> 00:05:04.680
I wanted to establish
to my bonafides

00:05:04.680 --> 00:05:08.170
that I had done this research,
that the story is grounded

00:05:08.170 --> 00:05:11.510
to the degree that
it can be in reality.

00:05:11.510 --> 00:05:14.800
So that when we then go off into
the more fantastical elements,

00:05:14.800 --> 00:05:16.390
you feel that you're
in capable hands.

00:05:16.390 --> 00:05:18.290
So, that would be one example.

00:05:18.290 --> 00:05:22.230
And artificial intelligence,
same process essentially.

00:05:22.230 --> 00:05:25.620
I bother experts
until I finally find

00:05:25.620 --> 00:05:29.919
an area where I can hide where
I'm going to begin my story.

00:05:29.919 --> 00:05:30.710
MALE SPEAKER: Sure.

00:05:30.710 --> 00:05:32.090
Now some of the
technologies that

00:05:32.090 --> 00:05:37.200
are in "Influx" like quantum
computing, and nanotechnology.

00:05:37.200 --> 00:05:40.160
Those are a little
bit closer to-- like,

00:05:40.160 --> 00:05:42.470
there's no gravity
mirrors out there

00:05:42.470 --> 00:05:44.310
being created,
probably, right now.

00:05:44.310 --> 00:05:46.980
But there are people working
on quantum computing,

00:05:46.980 --> 00:05:50.420
and nanotech, and,
to some extent,

00:05:50.420 --> 00:05:54.370
different types of biologies.

00:05:54.370 --> 00:05:58.440
So I guess for any of those,
how did you think about those,

00:05:58.440 --> 00:06:01.090
and how did you research those,
which were sort of closer

00:06:01.090 --> 00:06:02.310
to where we are now?

00:06:02.310 --> 00:06:04.470
DANIEL SUAREZ: Well
those to me, again,

00:06:04.470 --> 00:06:07.590
the antagonists of
the story, the BTC,

00:06:07.590 --> 00:06:09.860
the Bureau of
Technology Control,

00:06:09.860 --> 00:06:12.100
has supposedly been
around since the '60s.

00:06:12.100 --> 00:06:15.030
So, whereas we'd
say, well, we're

00:06:15.030 --> 00:06:17.400
working on quantum
computers now, they'd say,

00:06:17.400 --> 00:06:19.770
well, we are doing
that in the '70s.

00:06:19.770 --> 00:06:23.470
It's kind of the ongoing joke
that cancer had been cured back

00:06:23.470 --> 00:06:26.160
in '85, so these guys
are sort of looking back

00:06:26.160 --> 00:06:27.260
at a past we didn't live.

00:06:27.260 --> 00:06:31.330
And that's the dramatic, or
the sci-fi, conceit of it.

00:06:31.330 --> 00:06:33.770
So researching that, a
lot of what I was doing

00:06:33.770 --> 00:06:36.120
was using things that
we're experiencing now

00:06:36.120 --> 00:06:38.700
as if they're in the ancient,
well, the distant, past

00:06:38.700 --> 00:06:40.360
for these folks.

00:06:40.360 --> 00:06:44.650
For instance, the fusion
parts, the tokamak designs,

00:06:44.650 --> 00:06:48.390
the magnetic containment
vessels, all of that stuff.

00:06:48.390 --> 00:06:52.420
We're still working on that
with ITER, very similar design,

00:06:52.420 --> 00:06:55.030
we just simply make it so that
they have solved those problems

00:06:55.030 --> 00:06:58.270
and then shrunk them down by
several orders of magnitude.

00:06:58.270 --> 00:07:00.570
But also part of that
research involved

00:07:00.570 --> 00:07:02.430
going up to the National
Ignition Facility

00:07:02.430 --> 00:07:03.231
and touring that.

00:07:03.231 --> 00:07:05.230
And I don't know if
anybody's ever gone to that?

00:07:05.230 --> 00:07:07.140
Man, amazing.

00:07:07.140 --> 00:07:10.010
First of all, it looks
like Doctor No's layer.

00:07:10.010 --> 00:07:11.150
It just does.

00:07:11.150 --> 00:07:13.670
You go around-- complete with
the guys with the overalls,

00:07:13.670 --> 00:07:15.430
and the hard hats,
and all they're

00:07:15.430 --> 00:07:18.054
missing are little machine guns,
and they look like the drones.

00:07:18.054 --> 00:07:23.010
But really amazing facility, and
I'll describe it very briefly.

00:07:23.010 --> 00:07:25.870
National Ignition Facility
is a very different fusion

00:07:25.870 --> 00:07:29.290
experiment that is being done at
Lawrence Livermore Laboratory.

00:07:29.290 --> 00:07:34.190
It's 192 of the most
powerful lasers ever created,

00:07:34.190 --> 00:07:37.450
all focused onto a
little pea-sized target.

00:07:37.450 --> 00:07:39.240
Now bear in mind, each
one of these lasers

00:07:39.240 --> 00:07:40.360
is the biggest ever built.

00:07:40.360 --> 00:07:43.670
So they built one, and then
they said, let's build 191 more.

00:07:43.670 --> 00:07:45.500
And they focused them
all on this spot.

00:07:45.500 --> 00:07:47.730
And for that, about a
billionth of a second,

00:07:47.730 --> 00:07:50.860
they would use the equivalent
of all of the electricity being

00:07:50.860 --> 00:07:53.900
used, at that moment,
in the United States,

00:07:53.900 --> 00:07:55.470
focus it right on this target.

00:07:55.470 --> 00:07:59.840
To achieve, the goal is, fusion
for a billionth of a second.

00:07:59.840 --> 00:08:02.590
And they want to do that ten
times a second, like an engine.

00:08:02.590 --> 00:08:05.100
To create, not a sustainable
fusion reaction, but one

00:08:05.100 --> 00:08:09.160
that burst produces more energy
briefly than is put into it,

00:08:09.160 --> 00:08:11.510
and that energy is fed
back into the lasers,

00:08:11.510 --> 00:08:14.440
and then it just keeps
on running a turbine.

00:08:14.440 --> 00:08:18.810
They just, about a month ago,
got more energy out of it

00:08:18.810 --> 00:08:19.940
than they put into it.

00:08:19.940 --> 00:08:22.460
So that was a real
watershed moment.

00:08:22.460 --> 00:08:23.680
It made the news briefly.

00:08:23.680 --> 00:08:24.750
I think it might have
been a little too

00:08:24.750 --> 00:08:27.500
complex for a lot of people to
notice the importance of it,

00:08:27.500 --> 00:08:29.147
but I was like, yeah, all right.

00:08:29.147 --> 00:08:30.730
And people were
looking at me strange.

00:08:30.730 --> 00:08:32.150
It's like, that is
really big to be

00:08:32.150 --> 00:08:33.691
able to run the city
of San Francisco

00:08:33.691 --> 00:08:36.309
on two gallons of
sea water for a year.

00:08:36.309 --> 00:08:37.970
That would be a big deal.

00:08:37.970 --> 00:08:39.789
Now, pointedly for
this story, what

00:08:39.789 --> 00:08:44.240
I found so interesting
in that is the goal--

00:08:44.240 --> 00:08:46.652
they have patents
on all of this--

00:08:46.652 --> 00:08:49.110
but the goal is that they want
to make these patents freely

00:08:49.110 --> 00:08:52.600
available to the world, to
mankind, to solve our energy

00:08:52.600 --> 00:08:54.030
problem once and for all.

00:08:54.030 --> 00:08:55.800
And this is really
the exact opposite

00:08:55.800 --> 00:08:57.110
of what the BTC would be doing.

00:08:57.110 --> 00:08:59.850
So I was very,
very, well, proud,

00:08:59.850 --> 00:09:02.960
as a taxpayer to say that was
the goal of that facility.

00:09:02.960 --> 00:09:06.990
To say, if we solve the energy
problem once and for all,

00:09:06.990 --> 00:09:10.134
for all of history, we're going
to give it to mankind for free.

00:09:10.134 --> 00:09:11.800
So it's an amazing
facility, if you ever

00:09:11.800 --> 00:09:13.850
get a chance to go
see it, go see it.

00:09:13.850 --> 00:09:14.700
It's really cool.

00:09:14.700 --> 00:09:16.680
MALE SPEAKER:
Sounds really cool.

00:09:16.680 --> 00:09:19.610
So if there is no
real BTC out there

00:09:19.610 --> 00:09:23.230
that's trying to block
good technologies, what

00:09:23.230 --> 00:09:25.840
do you think could be slowing
down the pace of innovation,

00:09:25.840 --> 00:09:31.230
or not, I guess, making
really hard problems

00:09:31.230 --> 00:09:33.047
take so long to solve?

00:09:33.047 --> 00:09:33.880
DANIEL SUAREZ: Yeah.

00:09:33.880 --> 00:09:37.850
Well just to be clear, I'm not
one of the tin foil hat people.

00:09:37.850 --> 00:09:40.900
I don't believe there
is a BTC out there

00:09:40.900 --> 00:09:44.070
trying to stop technology,
and hiding immortality

00:09:44.070 --> 00:09:45.470
and all of these other things.

00:09:45.470 --> 00:09:48.270
That was really a
metaphor for that impulse.

00:09:48.270 --> 00:09:52.030
If, for instance, I
described it in this story

00:09:52.030 --> 00:09:54.510
as I think it's
happening, which is

00:09:54.510 --> 00:09:56.730
a lot of boring meetings
in boardrooms where people

00:09:56.730 --> 00:09:59.490
are trying to protect
vested interests,

00:09:59.490 --> 00:10:00.540
economic investments.

00:10:00.540 --> 00:10:02.030
That's not a very exciting book.

00:10:02.030 --> 00:10:04.790
So I wanted to make
it a bit more sci-fi.

00:10:04.790 --> 00:10:06.476
But, I think a lot
of the time it is.

00:10:06.476 --> 00:10:08.100
It's preservation
and this is, I think,

00:10:08.100 --> 00:10:09.470
a completely natural impulse.

00:10:09.470 --> 00:10:13.830
When there is a substantial
investment in an existing

00:10:13.830 --> 00:10:16.950
market, I think there's a lot
of reluctance to disrupt it.

00:10:16.950 --> 00:10:19.740
And I think this has been going
on throughout history, not just

00:10:19.740 --> 00:10:21.850
economic, but also
political power,

00:10:21.850 --> 00:10:27.582
preserving the status quo
is a very strong impulse.

00:10:27.582 --> 00:10:28.290
But I don't know.

00:10:28.290 --> 00:10:31.800
I would say that change,
as disruptive as it

00:10:31.800 --> 00:10:35.640
is, the ability to ride
change, and to adapt to it,

00:10:35.640 --> 00:10:38.530
is the only thing that's
going to sustain civilization.

00:10:38.530 --> 00:10:42.930
We've seen this-- the efforts to
preserve our fossil-based fuel

00:10:42.930 --> 00:10:45.479
systems are, in many
ways, killing us.

00:10:45.479 --> 00:10:47.020
Now we have to make
that change, it's

00:10:47.020 --> 00:10:48.780
going to be a very
disruptive change.

00:10:48.780 --> 00:10:52.190
How many industries are going
to disappear and change?

00:10:52.190 --> 00:10:54.670
How many careers are
going to go away?

00:10:54.670 --> 00:10:56.500
How many various
classes of society

00:10:56.500 --> 00:10:58.860
are going to see
that disruption?

00:10:58.860 --> 00:11:01.130
But that-- I think
embracing that is really

00:11:01.130 --> 00:11:03.170
the only course
for the human race.

00:11:03.170 --> 00:11:07.580
And it seems obvious, but
then when you go do it,

00:11:07.580 --> 00:11:08.920
that's when the problem happens.

00:11:08.920 --> 00:11:12.430
And that's the reason why I
think we have this inertia,

00:11:12.430 --> 00:11:14.830
all of this invested
energy and science

00:11:14.830 --> 00:11:17.150
into what we were doing.

00:11:17.150 --> 00:11:19.450
And sometimes it's very
difficult to unplug that.

00:11:19.450 --> 00:11:21.130
I'll give you a great example.

00:11:21.130 --> 00:11:24.670
Again, up at the National
Ignition Facility,

00:11:24.670 --> 00:11:28.130
after you see this terrific
tour and you feel very confident

00:11:28.130 --> 00:11:30.190
about it, they sit you
down and they say, now,

00:11:30.190 --> 00:11:32.340
let's say it works
perfectly right now.

00:11:32.340 --> 00:11:35.280
Let's say we can have
fusion right this second.

00:11:35.280 --> 00:11:38.590
They proceed to show you how
it will be immensely difficult

00:11:38.590 --> 00:11:41.450
in the next 25 to 30 years to
roll that out around the world,

00:11:41.450 --> 00:11:44.020
to change the existing system,
even if it were available

00:11:44.020 --> 00:11:44.889
right now.

00:11:44.889 --> 00:11:46.930
And what that means to
all the particulate matter

00:11:46.930 --> 00:11:47.950
in the atmosphere now.

00:11:47.950 --> 00:11:50.382
So pretty much it
is already going

00:11:50.382 --> 00:11:51.590
to be a tremendous challenge.

00:11:51.590 --> 00:11:54.180
So, again, embracing change
at a very basic level.

00:11:54.180 --> 00:11:57.500
How we get energy is number
one, and we're having difficulty

00:11:57.500 --> 00:11:59.670
doing that even in the
face of tremendous evidence

00:11:59.670 --> 00:12:00.760
we should be.

00:12:00.760 --> 00:12:03.870
So that's how I think
it manifests itself, is

00:12:03.870 --> 00:12:08.960
a bit of fear, and then also an
immense practicality that has

00:12:08.960 --> 00:12:13.021
been probably evolved into us
over many millions of years.

00:12:13.021 --> 00:12:14.520
That once you have
a good thing, try

00:12:14.520 --> 00:12:16.590
to keep it going as
long as possible.

00:12:16.590 --> 00:12:19.790
And in a world where technology
can rapidly change the world,

00:12:19.790 --> 00:12:22.630
we almost have to rewire
ourselves to embrace change

00:12:22.630 --> 00:12:24.024
a bit more.

00:12:24.024 --> 00:12:24.960
MALE SPEAKER: Sure.

00:12:24.960 --> 00:12:26.810
Cool.

00:12:26.810 --> 00:12:29.760
Now, you began writing "Influx"
before all the Snowden stuff

00:12:29.760 --> 00:12:31.470
came out.

00:12:31.470 --> 00:12:35.410
And obviously the topic of
surveillance and privacy

00:12:35.410 --> 00:12:39.730
has gotten pretty big over
the last couple of years.

00:12:39.730 --> 00:12:43.350
I'm curious to hear your
thoughts on that and how

00:12:43.350 --> 00:12:44.850
it might apply to
Google as well.

00:12:44.850 --> 00:12:46.430
DANIEL SUAREZ: Sure.

00:12:46.430 --> 00:12:49.790
I go to DEF CON and
Black Hat a lot,

00:12:49.790 --> 00:12:53.100
and I think for a lot
of IT security people,

00:12:53.100 --> 00:12:56.800
the revelations about the
NSA were not such a surprise.

00:12:56.800 --> 00:13:00.520
I think that a lot of data
exists out there-- well,

00:13:00.520 --> 00:13:02.910
let's face it, a lot of
the products being used,

00:13:02.910 --> 00:13:06.740
a lot of the way
consumer information

00:13:06.740 --> 00:13:11.270
moves through society, that's
really the, not the profit

00:13:11.270 --> 00:13:14.530
incentive, but that's the
economic model, increasingly.

00:13:14.530 --> 00:13:19.130
And I think that to the
extent that we're all

00:13:19.130 --> 00:13:20.904
carrying these devices
we love, we sort of

00:13:20.904 --> 00:13:22.570
have a love-hate
relationship with them.

00:13:22.570 --> 00:13:23.840
Man, I love my smartphone.

00:13:23.840 --> 00:13:27.800
At the same time I'm conflicted
about the fact how many times

00:13:27.800 --> 00:13:30.340
I'll be hovering over
this EULA agreement going,

00:13:30.340 --> 00:13:33.020
ugh, they need that, really?

00:13:33.020 --> 00:13:34.000
Ugh, really?

00:13:34.000 --> 00:13:38.060
And I really sit down
and think, how far am I

00:13:38.060 --> 00:13:39.830
willing to agree to this?

00:13:39.830 --> 00:13:43.420
And I think we've reached the
point that now as a society,

00:13:43.420 --> 00:13:46.332
we sort of have to establish
what that baseline is.

00:13:46.332 --> 00:13:48.040
Again, these technologies
have rolled out

00:13:48.040 --> 00:13:52.400
so quickly that society didn't
really get a chance to decide.

00:13:52.400 --> 00:13:54.660
Instead these applications
and the networks

00:13:54.660 --> 00:13:58.040
arose, people find
tremendous utility with them,

00:13:58.040 --> 00:13:59.850
and now we as a
society have to come

00:13:59.850 --> 00:14:02.330
to an agreement about
what that baseline is.

00:14:02.330 --> 00:14:04.757
I think that there's no
doubt that we like them.

00:14:04.757 --> 00:14:07.090
I think everybody wants to
have a more informed decision

00:14:07.090 --> 00:14:10.220
about how they could use
them, and I will fess up

00:14:10.220 --> 00:14:12.810
that I read very few
EULAs all the way through.

00:14:12.810 --> 00:14:15.330
Wasn't there an end-user
license agreement--

00:14:15.330 --> 00:14:18.500
I can't remember the company,
they had an award of, I think,

00:14:18.500 --> 00:14:19.710
$5,000?

00:14:19.710 --> 00:14:21.816
Like, if you get all the
way down to paragraph 95

00:14:21.816 --> 00:14:23.690
and you read this, it's
like, just email here

00:14:23.690 --> 00:14:25.731
and we'll give you $5,000,
the first person who--

00:14:25.731 --> 00:14:27.800
and one person claimed it.

00:14:27.800 --> 00:14:30.130
Nobody reads EULAs,
but they're essentially

00:14:30.130 --> 00:14:31.420
these social contracts.

00:14:31.420 --> 00:14:35.220
And that's why I'm saying
if we read one, basically,

00:14:35.220 --> 00:14:39.460
as something that we establish
as a civilization what we're

00:14:39.460 --> 00:14:42.490
willing to put up with, and
then I think a lot of it

00:14:42.490 --> 00:14:43.710
can follow that.

00:14:43.710 --> 00:14:45.790
But I don't know.

00:14:45.790 --> 00:14:51.680
Privacy-- when you design a
network that is inherently

00:14:51.680 --> 00:14:54.390
sharing information, all
that rich information,

00:14:54.390 --> 00:14:57.530
it doesn't surprise
me when people use it.

00:14:57.530 --> 00:15:01.950
Has anyone here seen the MIT's
Reality Mining application?

00:15:01.950 --> 00:15:02.450
Nobody?

00:15:02.450 --> 00:15:03.330
Anybody?

00:15:03.330 --> 00:15:05.270
This is a research
project that was

00:15:05.270 --> 00:15:07.820
done for about
1,000 cell phones.

00:15:07.820 --> 00:15:10.080
And what they would do is
they would get respondents

00:15:10.080 --> 00:15:12.660
to agree to have their
geolocation tracked

00:15:12.660 --> 00:15:14.834
over the course of about,
I think, two months.

00:15:14.834 --> 00:15:16.750
And from that, they were
able to quickly build

00:15:16.750 --> 00:15:19.550
an algorithm that could predict
what every single person was

00:15:19.550 --> 00:15:24.260
going to do next, like 93.4%
percent chance, analyzing just

00:15:24.260 --> 00:15:27.260
a month of geolocation data--
where people were going to go,

00:15:27.260 --> 00:15:30.096
what they were going to do next,
with 93.4% percent accuracy.

00:15:30.096 --> 00:15:31.720
And that kind of
shows the power of it.

00:15:31.720 --> 00:15:34.380
So there's a tremendous
incentive to use it.

00:15:34.380 --> 00:15:37.680
Drag it all into the light
and have everybody discuss it,

00:15:37.680 --> 00:15:41.500
and as a society decide where
we're comfortable putting it.

00:15:41.500 --> 00:15:45.470
In the past, technologies
like the camera

00:15:45.470 --> 00:15:48.700
have freaked people out, and
it takes decades for society

00:15:48.700 --> 00:15:49.680
to catch up with it.

00:15:49.680 --> 00:15:51.780
And this amazed me.

00:15:51.780 --> 00:15:54.960
I think the book was called-- I
can't remember the name of it.

00:15:54.960 --> 00:15:56.880
Tim Wu, I think his
name-- the book was?

00:15:56.880 --> 00:15:59.860
But it was talking about how
when the portable camera first

00:15:59.860 --> 00:16:02.740
came out, people were outraged.

00:16:02.740 --> 00:16:05.300
The idea that you could get
your photo taken in the street.

00:16:05.300 --> 00:16:08.160
It's a tremendous
privacy violation

00:16:08.160 --> 00:16:10.190
and there were angry
editorials, and it

00:16:10.190 --> 00:16:12.330
took 20 or 30 years for
people start to realize,

00:16:12.330 --> 00:16:14.320
oh, well, OK, we
can manage this.

00:16:14.320 --> 00:16:16.350
So I think it'll follow
very much like that.

00:16:16.350 --> 00:16:18.560
There will be litigation,
there will be debate,

00:16:18.560 --> 00:16:20.750
there will be op-eds,
and legislation,

00:16:20.750 --> 00:16:22.340
and eventually
we'll settle down,

00:16:22.340 --> 00:16:27.160
and we will have these
20, 30, 50 years from now.

00:16:27.160 --> 00:16:28.690
MALE SPEAKER: Makes sense.

00:16:28.690 --> 00:16:31.474
Now Google is working on
a lot of new technologies,

00:16:31.474 --> 00:16:33.390
a lot of things that
are, sort of a little far

00:16:33.390 --> 00:16:35.480
out, and some near
more near-term,

00:16:35.480 --> 00:16:39.040
some that have come out already,
that are probably pushing,

00:16:39.040 --> 00:16:41.660
a little bit, what
society is OK with,

00:16:41.660 --> 00:16:44.076
and definitely
changing behavior.

00:16:44.076 --> 00:16:45.950
There's a number of them
we could talk about.

00:16:45.950 --> 00:16:47.870
I'm curious which
of those-- which

00:16:47.870 --> 00:16:49.670
of the projects that
we're working on here

00:16:49.670 --> 00:16:53.130
are you excited about,
are you cautious about?

00:16:53.130 --> 00:16:54.840
DANIEL SUAREZ: Sure.

00:16:54.840 --> 00:16:55.390
Wow.

00:16:55.390 --> 00:16:57.490
I'll try to be very
politic as I say this

00:16:57.490 --> 00:17:01.670
but-- automated cars
I think are cool.

00:17:01.670 --> 00:17:03.280
Those were in "Demon."

00:17:03.280 --> 00:17:05.240
Not the same way,
they don't kill people

00:17:05.240 --> 00:17:07.560
when you guys make them.

00:17:07.560 --> 00:17:10.510
But there was a really
magical moment when

00:17:10.510 --> 00:17:14.770
I was up in Mountain View and
they put me in the automated

00:17:14.770 --> 00:17:16.982
car, I was driving
down the highway.

00:17:16.982 --> 00:17:19.440
That was pretty freaky because
I'd received a lot of emails

00:17:19.440 --> 00:17:21.129
from people saying,
well, that's really

00:17:21.129 --> 00:17:22.670
cool in your book
and all, but that's

00:17:22.670 --> 00:17:24.579
30 or 40 or 50 years from now.

00:17:24.579 --> 00:17:28.510
And I was like, yeah, selfie.

00:17:28.510 --> 00:17:30.630
But that's very cool.

00:17:30.630 --> 00:17:33.000
I think-- also, I
spent many years

00:17:33.000 --> 00:17:34.640
working in logistics software.

00:17:34.640 --> 00:17:37.170
I could immediately see
the utility of that.

00:17:37.170 --> 00:17:41.190
I think automated vehicles,
in terms of logistics,

00:17:41.190 --> 00:17:43.140
it's just a no brainer.

00:17:43.140 --> 00:17:44.890
It's going to be
tremendously useful.

00:17:44.890 --> 00:17:47.745
And then the idea of having cars
that are part time autonomous

00:17:47.745 --> 00:17:48.850
and part times not.

00:17:48.850 --> 00:17:51.730
When you get down into
a city core, having cars

00:17:51.730 --> 00:17:52.900
communicate with each other.

00:17:52.900 --> 00:17:55.025
It's like, you want to get
here, we'll work it out.

00:17:55.025 --> 00:17:58.180
We'll do the every other
merge, you guys screw it up.

00:17:58.180 --> 00:18:01.800
And that will make things
more-- it'll probably

00:18:01.800 --> 00:18:06.944
be cars that are algorithmically
written up to be jerks, right?

00:18:06.944 --> 00:18:08.760
They'll cut in.

00:18:08.760 --> 00:18:10.850
That's right, it'll be a
particular brand of car.

00:18:10.850 --> 00:18:12.560
But, no.

00:18:12.560 --> 00:18:15.530
Basically, that is
interesting to me.

00:18:15.530 --> 00:18:17.910
We see things
like-- Google Glass,

00:18:17.910 --> 00:18:21.290
I think is a perfect example of
it making people uncomfortable.

00:18:21.290 --> 00:18:23.730
And again, I think
that bright line

00:18:23.730 --> 00:18:26.837
will be written by how
people react to it.

00:18:26.837 --> 00:18:29.170
And we see that already, you
occasionally see news items

00:18:29.170 --> 00:18:31.210
where people get
very angry about it.

00:18:31.210 --> 00:18:33.030
That is exactly
that type of debate.

00:18:33.030 --> 00:18:35.430
What will happen is it'll
move back and forth, maybe

00:18:35.430 --> 00:18:37.400
the camera goes off,
maybe it stays on,

00:18:37.400 --> 00:18:42.160
maybe it looks a different data
as opposed to just visual data,

00:18:42.160 --> 00:18:45.220
but that technology
interests me tremendously.

00:18:45.220 --> 00:18:48.700
Because of course I had the
heads-up glasses in "Demon."

00:18:48.700 --> 00:18:50.060
It was for a different purpose.

00:18:50.060 --> 00:18:52.480
It was to disrupt
all of civilization.

00:18:52.480 --> 00:18:56.410
I don't think that's the purpose
of what you guys do, clearly,

00:18:56.410 --> 00:18:59.002
but I think Google
Glass interests me.

00:18:59.002 --> 00:18:59.710
[AUDIENCE LAUGHS]

00:18:59.710 --> 00:19:01.543
DANIEL SUAREZ: Yeah, I
know, I really wanted

00:19:01.543 --> 00:19:04.300
to hear a, well, hell no, man.

00:19:04.300 --> 00:19:06.720
But that interests
me, again, I could

00:19:06.720 --> 00:19:10.760
see it being tremendously useful
because it frees the hands.

00:19:10.760 --> 00:19:15.580
Just that there's that component
that people will weigh in on I

00:19:15.580 --> 00:19:17.919
think, over the next few years.

00:19:17.919 --> 00:19:18.835
MALE SPEAKER: Exactly.

00:19:18.835 --> 00:19:21.150
DANIEL SUAREZ: Oh,
also, one more thing.

00:19:21.150 --> 00:19:24.690
Google Earth Street View, just
hands down the best writer

00:19:24.690 --> 00:19:26.610
research tool ever.

00:19:26.610 --> 00:19:29.200
I mean it just is.

00:19:29.200 --> 00:19:31.724
I spend so much
time-- it's like if I

00:19:31.724 --> 00:19:33.390
want to travel to
someplace, even if I'm

00:19:33.390 --> 00:19:36.550
going to go to a foreign
location to do research,

00:19:36.550 --> 00:19:40.480
I can find out precisely
where I want to go using it.

00:19:40.480 --> 00:19:44.830
I discovered a certain location
at the end of this book

00:19:44.830 --> 00:19:48.040
using Google Earth, which was
a very improbable location,

00:19:48.040 --> 00:19:50.390
and yet I can get all
of this-- so yeah.

00:19:50.390 --> 00:19:53.449
So whoever here who
works on that, thank you.

00:19:53.449 --> 00:19:54.240
MALE SPEAKER: Nice.

00:19:54.240 --> 00:19:56.980
Thank you, no, the Google
Earth and Maps are awesome.

00:19:56.980 --> 00:20:00.520
In terms of "Influx," do you
have any lessons or themes

00:20:00.520 --> 00:20:02.817
that you hope readers
will take out of it?

00:20:02.817 --> 00:20:03.900
DANIEL SUAREZ: Well, yeah.

00:20:03.900 --> 00:20:08.210
Again, this is probably not the
audience I need to say it to.

00:20:08.210 --> 00:20:11.870
You guys would tell me this,
and that is embracing change.

00:20:11.870 --> 00:20:14.490
Constant incremental
change to me--

00:20:14.490 --> 00:20:17.860
the idea of creating a society
that naturally embraces

00:20:17.860 --> 00:20:20.510
that and doesn't
constantly tried to set up

00:20:20.510 --> 00:20:23.010
the perfect order,
because there's

00:20:23.010 --> 00:20:24.970
no such thing as
the perfect system.

00:20:24.970 --> 00:20:28.310
And in the same way that
nature doesn't just say, well,

00:20:28.310 --> 00:20:33.670
that deer is perfect, we're done
with deer now, deer are solved.

00:20:33.670 --> 00:20:36.700
And I think that having
an inherently open society

00:20:36.700 --> 00:20:38.490
naturally leads to this.

00:20:38.490 --> 00:20:41.577
It's, again, when I look at
societies like North Korea,

00:20:41.577 --> 00:20:43.660
for instance, I think even
North Korean government

00:20:43.660 --> 00:20:45.780
said that they have
perfected society,

00:20:45.780 --> 00:20:47.676
so they were like,
done with that.

00:20:47.676 --> 00:20:49.050
And that's a very
worrisome thing

00:20:49.050 --> 00:20:53.550
because again, everybody
debating, arguing,

00:20:53.550 --> 00:20:56.380
freely exchanging ideas, even
if they don't agree, especially

00:20:56.380 --> 00:20:59.670
if they don't agree, that to
me is a vibrant civilization.

00:20:59.670 --> 00:21:01.260
One where people
say what they think,

00:21:01.260 --> 00:21:04.730
try things, try new
things and make mistakes.

00:21:04.730 --> 00:21:08.010
There was a sociological study--
there was a sociologist who

00:21:08.010 --> 00:21:10.950
got a bunch of people
together to solve a problem,

00:21:10.950 --> 00:21:13.902
and what he was looking
at was group dynamics.

00:21:13.902 --> 00:21:15.360
And the problem
with this, they had

00:21:15.360 --> 00:21:18.290
a pile of straws
and a couple clips,

00:21:18.290 --> 00:21:20.240
and the goal was
to try to see how

00:21:20.240 --> 00:21:23.460
quickly a small group of people
could stack these the highest,

00:21:23.460 --> 00:21:25.270
make the tallest sculpture.

00:21:25.270 --> 00:21:27.670
And they had MBAs, and
they had military people,

00:21:27.670 --> 00:21:30.350
and they had physicians, and
all these people try to do it.

00:21:30.350 --> 00:21:33.620
And the winner, hands
down, was five-year-olds,

00:21:33.620 --> 00:21:35.369
because they weren't
afraid to fail.

00:21:35.369 --> 00:21:37.660
They would just immediately
try a whole bunch of stuff,

00:21:37.660 --> 00:21:39.618
rather than trying to
reason it out like, well,

00:21:39.618 --> 00:21:41.450
if we do-- they
would you just do it.

00:21:41.450 --> 00:21:43.533
And try it and fail, and
fail, and every time they

00:21:43.533 --> 00:21:46.370
failed they would learn a bit,
and they time and time again

00:21:46.370 --> 00:21:47.260
would win.

00:21:47.260 --> 00:21:49.220
So that's the sort
of civilization

00:21:49.220 --> 00:21:51.450
I'd love to be part of,
sort of a bunch of people

00:21:51.450 --> 00:21:56.370
trying and failing--
five-year-old, basically.

00:21:56.370 --> 00:21:58.820
That came out poorly
but, you know.

00:21:58.820 --> 00:22:02.810
MALE SPEAKER: Well we like to
be like five-year-olds here too.

00:22:02.810 --> 00:22:05.240
I guess to wrap up my
questions before we open up

00:22:05.240 --> 00:22:09.030
to the audience, can you
give us a bit of background,

00:22:09.030 --> 00:22:11.760
in terms of how you got
into science fiction writing

00:22:11.760 --> 00:22:14.500
from your tech
consulting background?

00:22:14.500 --> 00:22:17.017
And then also, where you
see yourself going next?

00:22:17.017 --> 00:22:17.850
DANIEL SUAREZ: Sure.

00:22:17.850 --> 00:22:20.720
OK.

00:22:20.720 --> 00:22:23.260
I was a systems
consultant for many years.

00:22:23.260 --> 00:22:25.690
I worked with big
companies to solve

00:22:25.690 --> 00:22:29.630
complex logistical problems,
particularly the last seven

00:22:29.630 --> 00:22:31.680
years I was working
in that career.

00:22:31.680 --> 00:22:34.430
And that involved really
trying to wring out

00:22:34.430 --> 00:22:37.010
a lot of efficiency and
distribution networks.

00:22:37.010 --> 00:22:38.880
And, I don't know,
there's something--

00:22:38.880 --> 00:22:40.630
I had been reading a
lot of tech thrillers

00:22:40.630 --> 00:22:42.590
that kind of upset me
because the technology

00:22:42.590 --> 00:22:43.760
sucked in them, you know?

00:22:43.760 --> 00:22:45.760
I'd read it and I was
like, that's not possible.

00:22:45.760 --> 00:22:48.480
And then I was also
trying to figure out a way

00:22:48.480 --> 00:22:51.914
to get across to other
people that I knew,

00:22:51.914 --> 00:22:54.330
some of the concerns I had
about the way things were being

00:22:54.330 --> 00:22:56.310
designed, sort of how
we were getting rid

00:22:56.310 --> 00:22:59.930
of resiliency, squeezing
every ounce of fat

00:22:59.930 --> 00:23:04.910
out of our systems, whether
they be transportation, energy,

00:23:04.910 --> 00:23:07.860
whatever it is, power.

00:23:07.860 --> 00:23:09.674
And then it just
hit me that I'd like

00:23:09.674 --> 00:23:11.840
to write a thriller about
it, because a white paper,

00:23:11.840 --> 00:23:13.215
nobody's going to read that.

00:23:13.215 --> 00:23:15.470
I actually was thinking
of writing an article

00:23:15.470 --> 00:23:18.130
and sending it in to
some software magazine.

00:23:18.130 --> 00:23:22.160
But I wound up writing a
thriller, which was "Demon,"

00:23:22.160 --> 00:23:24.430
and I couldn't get a
publisher, an agent, for it,

00:23:24.430 --> 00:23:26.100
so then I self published it.

00:23:26.100 --> 00:23:28.080
And then it just
really succeeded,

00:23:28.080 --> 00:23:31.140
beyond my wildest imagination.

00:23:31.140 --> 00:23:33.732
And from that point on, it
really made sense for me

00:23:33.732 --> 00:23:35.440
to continue writing
about-- first of all,

00:23:35.440 --> 00:23:38.940
I found it to be a lot
of fun, very rewarding.

00:23:38.940 --> 00:23:42.300
Also, even though I wrote the
first book under a pen name,

00:23:42.300 --> 00:23:44.520
Leinad Zeraus, which
is my name backwards.

00:23:44.520 --> 00:23:48.020
I did that-- and it was
very weak encryption.

00:23:48.020 --> 00:23:51.354
But the other thing is
that gamers-- this I though

00:23:51.354 --> 00:23:53.520
was fascinating-- gamers
would look at that and say,

00:23:53.520 --> 00:23:54.776
Daniel Suarez.

00:23:54.776 --> 00:23:56.150
And it occurred
to me, the reason

00:23:56.150 --> 00:23:59.600
is, I don't know how many
people here have joined a system

00:23:59.600 --> 00:24:03.320
and seen your name is taken, so
they just reverse the letters.

00:24:03.320 --> 00:24:05.440
And so a lot of gamers
would look at my name

00:24:05.440 --> 00:24:07.850
and immediately recognize it.

00:24:07.850 --> 00:24:10.620
But when I finally
sort of came out,

00:24:10.620 --> 00:24:12.700
and it turns out that
I was this guy who

00:24:12.700 --> 00:24:14.970
was working for
these big companies,

00:24:14.970 --> 00:24:18.250
and I wrote this hacking
book about tearing apart

00:24:18.250 --> 00:24:21.080
big companies, I was thinking,
oh, this is going to be ugly.

00:24:21.080 --> 00:24:23.300
But it turns out a
lot of those people

00:24:23.300 --> 00:24:26.400
said, hey, I'm
writing a novel too.

00:24:26.400 --> 00:24:28.680
So I'm still friends
with all of them

00:24:28.680 --> 00:24:30.380
and it worked out very well.

00:24:30.380 --> 00:24:35.040
So I got to keep a lot of the
people that I knew as friends

00:24:35.040 --> 00:24:37.820
and started writing more and
more books, a little further

00:24:37.820 --> 00:24:39.580
afield from software.

00:24:39.580 --> 00:24:42.720
Of course, "Kill Decision" was
all about robotics and drones.

00:24:42.720 --> 00:24:45.430
And I've always loved
technology, always

00:24:45.430 --> 00:24:46.530
loved computers.

00:24:46.530 --> 00:24:48.720
But I don't have a
computer science degree,

00:24:48.720 --> 00:24:50.700
I have an English
literature degree.

00:24:50.700 --> 00:24:52.992
So I think I always
took a very, I

00:24:52.992 --> 00:24:55.729
don't want to say holistic
approach, to technology,

00:24:55.729 --> 00:24:57.770
but a lot of what I would
do, particularly when I

00:24:57.770 --> 00:24:59.872
was doing code reviews
for big companies,

00:24:59.872 --> 00:25:02.330
is I would spend a great deal
of time talking to the people

00:25:02.330 --> 00:25:04.780
to understand how the
software got written.

00:25:04.780 --> 00:25:08.650
Not just what was in every
module, what the source

00:25:08.650 --> 00:25:11.160
code was, but under
what conditions,

00:25:11.160 --> 00:25:14.480
what psychological environment--
or social environment's

00:25:14.480 --> 00:25:16.130
probably better.

00:25:16.130 --> 00:25:18.580
And I had a great deal of
success doing things like that,

00:25:18.580 --> 00:25:21.770
finding problems, as a result
of talking to the people and how

00:25:21.770 --> 00:25:24.270
they related to each other,
not necessarily code.

00:25:24.270 --> 00:25:28.870
So I sort of bring an English
major sensibility to tech,

00:25:28.870 --> 00:25:31.930
and so I think that I'm
continuing that with my books.

00:25:31.930 --> 00:25:34.800
I love talking to people,
researchers, about

00:25:34.800 --> 00:25:38.860
new technologies they're
working on and then connecting

00:25:38.860 --> 00:25:39.950
these various components.

00:25:39.950 --> 00:25:43.750
A lot of my books, for instance,
I combine existing technologies

00:25:43.750 --> 00:25:46.940
in new ways, in ways people
might not have thought of.

00:25:46.940 --> 00:25:50.140
And that's, I think,
why I love it so much.

00:25:50.140 --> 00:25:51.750
I will continue doing that.

00:25:51.750 --> 00:25:53.110
I love writing books.

00:25:53.110 --> 00:25:54.730
And I'm no doubt
going to continue

00:25:54.730 --> 00:25:56.300
writing tech thrillers.

00:25:56.300 --> 00:25:59.085
Sci-fi thriller, like I
said, I put these generally

00:25:59.085 --> 00:25:59.960
in the same category.

00:25:59.960 --> 00:26:02.750
I'm just using a
slightly different tool

00:26:02.750 --> 00:26:05.830
from the shelf to achieve
a lot of the same ends.

00:26:05.830 --> 00:26:08.120
It's been said that
sci-fi in general

00:26:08.120 --> 00:26:10.874
is never about the future,
it's always about the present.

00:26:10.874 --> 00:26:12.290
And so in that
sense I don't think

00:26:12.290 --> 00:26:14.375
it's a big stretch
to do a bit of sci-fi

00:26:14.375 --> 00:26:16.905
and then some tech thrillers.

00:26:16.905 --> 00:26:18.030
MALE SPEAKER: Sounds great.

00:26:18.030 --> 00:26:20.030
We're excited to see the
next stuff you work on.

00:26:20.030 --> 00:26:22.002
DANIEL SUAREZ: So am I.

00:26:22.002 --> 00:26:23.960
AUDIENCE: I have a question
about Malaysia Air.

00:26:23.960 --> 00:26:27.380
And I think one of the things
that people find so frustrating

00:26:27.380 --> 00:26:31.290
about the story is that
because we're so now used

00:26:31.290 --> 00:26:34.370
to this technology being able
to solve anything instantly,

00:26:34.370 --> 00:26:37.500
and you can see everything all
the time and know everything,

00:26:37.500 --> 00:26:40.910
and everybody on that plane has
a smartphone in their pocket.

00:26:40.910 --> 00:26:41.760
And yet, it's gone.

00:26:41.760 --> 00:26:43.990
And it's a week now and
it's still a mystery.

00:26:43.990 --> 00:26:46.490
I was wondering if you might
talk about that, sort of in why

00:26:46.490 --> 00:26:48.240
I think it's befuddling
people so much

00:26:48.240 --> 00:26:50.560
and bringing up a lot
of conspiracy theories.

00:26:50.560 --> 00:26:52.900
And I think a big part
of that is related to,

00:26:52.900 --> 00:26:55.150
we can't believe in this
age that this can happen.

00:26:55.150 --> 00:26:57.191
DANIEL SUAREZ: This is a
great question actually.

00:26:57.191 --> 00:26:59.030
It really is because
I obviously have

00:26:59.030 --> 00:27:03.830
been-- for future audiences who
look at this, around the time

00:27:03.830 --> 00:27:06.630
that this is taped, a Malaysian
airliner has disappeared.

00:27:06.630 --> 00:27:10.060
And with all hands, it doesn't
look like it's crashed,

00:27:10.060 --> 00:27:11.480
because of the
Rolls Royce engines

00:27:11.480 --> 00:27:14.240
had some sensors in them, that
every 30 minutes would report

00:27:14.240 --> 00:27:16.960
via satellite that
they were up to stuff,

00:27:16.960 --> 00:27:19.340
aside from being on the
bottom of the ocean.

00:27:19.340 --> 00:27:23.510
So I think what this does
is brings up a great point.

00:27:23.510 --> 00:27:27.900
People think of technology
as this monolithic solution

00:27:27.900 --> 00:27:30.500
that just deals with
things, when really they're

00:27:30.500 --> 00:27:33.740
these tiny little data
points, or connections, how

00:27:33.740 --> 00:27:35.500
it connects to the
rest of society.

00:27:35.500 --> 00:27:37.375
I think a lot of us
might have been surprised

00:27:37.375 --> 00:27:41.290
at how could an entire
airliner not be on radar screen

00:27:41.290 --> 00:27:42.110
at all time.

00:27:42.110 --> 00:27:45.100
But they aren't, apparently,
aside from military radars,

00:27:45.100 --> 00:27:48.690
which are very closely held
in secret, both their sweep

00:27:48.690 --> 00:27:50.890
and the extent of the
stations they have.

00:27:50.890 --> 00:27:53.110
Civilian radar, a lot
of it is augmented

00:27:53.110 --> 00:27:55.320
by transponders on machines.

00:27:55.320 --> 00:27:59.460
So here, if it has in fact
been stolen and spirited away

00:27:59.460 --> 00:28:03.085
to some other location,
as opposed to crashing,

00:28:03.085 --> 00:28:04.460
there was a
knowledgeable person.

00:28:04.460 --> 00:28:07.070
A person knowledgeable
about these systems and just

00:28:07.070 --> 00:28:09.330
the few connection
points that connected it

00:28:09.330 --> 00:28:12.270
to the rest of society,
which is the transponders

00:28:12.270 --> 00:28:13.840
get turned off.

00:28:13.840 --> 00:28:16.490
There was, I think,
also a mapping module

00:28:16.490 --> 00:28:19.300
that gets turned
off for navigation.

00:28:19.300 --> 00:28:21.790
They, of course, if
they have satellites

00:28:21.790 --> 00:28:24.300
in a cell phone on board they
can turn off that transceiver

00:28:24.300 --> 00:28:26.710
so they're not making
satellite connections.

00:28:26.710 --> 00:28:29.740
And then from there, they can
change-- now my understanding

00:28:29.740 --> 00:28:33.270
was that the belief is that
they went down another air

00:28:33.270 --> 00:28:35.980
corridor that's pretty heavily
traveled that would go either

00:28:35.980 --> 00:28:37.430
to the Middle East or Europe.

00:28:37.430 --> 00:28:39.384
And so they blended
in with traffic.

00:28:39.384 --> 00:28:40.800
Anybody looking
at a plane is just

00:28:40.800 --> 00:28:41.990
going to think it's
a plane and assume

00:28:41.990 --> 00:28:44.080
that all of their
transponders would be on.

00:28:44.080 --> 00:28:47.760
It is amazing that in
2014 you could basically

00:28:47.760 --> 00:28:51.420
disappear with an entire
plane filled with people.

00:28:51.420 --> 00:28:53.091
But I'll bet that
as a result of what

00:28:53.091 --> 00:28:55.090
happens-- because we
haven't seen the other shoe

00:28:55.090 --> 00:28:57.200
drop on this.

00:28:57.200 --> 00:29:00.450
Because was it stolen?

00:29:00.450 --> 00:29:02.140
What happened to
the people on it?

00:29:02.140 --> 00:29:04.185
And if so, what is going
to be done with it?

00:29:04.185 --> 00:29:06.290
Is it going to be
rebranded, repainted,

00:29:06.290 --> 00:29:07.860
and used for some
terrible purpose?

00:29:07.860 --> 00:29:11.740
Or is this just a
mistake, did it crash?

00:29:11.740 --> 00:29:13.990
The process by which we
track planes, I think,

00:29:13.990 --> 00:29:15.770
is going to change as a result.

00:29:15.770 --> 00:29:18.800
It definitely is
because the fact

00:29:18.800 --> 00:29:21.600
that everyone's looking for
it and it just disappeared.

00:29:21.600 --> 00:29:22.100
You know?

00:29:22.100 --> 00:29:23.940
That's shocking.

00:29:23.940 --> 00:29:24.700
And it shocks me.

00:29:24.700 --> 00:29:27.460
And again, every time these
types of things happen,

00:29:27.460 --> 00:29:30.062
I bookmark a lot of stuff
and I research stuff.

00:29:30.062 --> 00:29:31.770
And whenever one of
these things happen--

00:29:31.770 --> 00:29:33.603
and not to make a light
of it-- but whenever

00:29:33.603 --> 00:29:36.750
there is some calamity,
my inbox fills up.

00:29:36.750 --> 00:29:39.690
Somebody revives a
60,000 year-old virus,

00:29:39.690 --> 00:29:42.000
and man, you have to have
this as your next book.

00:29:42.000 --> 00:29:43.280
You've got to do it, here.

00:29:43.280 --> 00:29:45.430
It's like sometimes it's
too low hanging fruit.

00:29:45.430 --> 00:29:46.630
It's a little obvious.

00:29:46.630 --> 00:29:50.440
But in the case of
technology gone awry,

00:29:50.440 --> 00:29:52.330
I do take a great
interest in it.

00:29:52.330 --> 00:29:54.705
That's why I was very excited
when you asked the question

00:29:54.705 --> 00:29:58.980
because how tenuous the
thread is, what connects us

00:29:58.980 --> 00:30:00.180
to the rest of society.

00:30:00.180 --> 00:30:01.830
Particularly when
you're out-- that's

00:30:01.830 --> 00:30:03.850
why I like the movie
"Cast Away," for instance.

00:30:03.850 --> 00:30:07.100
You look at it, it's like, he
should be-- oh, that's right,

00:30:07.100 --> 00:30:10.850
you can't-- because we really
have lanes across these vast

00:30:10.850 --> 00:30:11.540
spaces.

00:30:11.540 --> 00:30:14.740
And civilization really
kind of frays at the edge.

00:30:14.740 --> 00:30:19.260
And I guess we'll find
out what happens to it.

00:30:19.260 --> 00:30:20.910
Gee, dare I make a prediction?

00:30:20.910 --> 00:30:23.250
I do think they will
find that plane.

00:30:23.250 --> 00:30:26.920
I hope that the people
involved are still alive,

00:30:26.920 --> 00:30:30.020
being kept somewhere, I hope.

00:30:30.020 --> 00:30:33.350
But I think it's-- the best case
scenario is somebody was just

00:30:33.350 --> 00:30:36.022
stealing a plane, that
seems very unlikely.

00:30:36.022 --> 00:30:36.980
It seems very unlikely.

00:30:36.980 --> 00:30:39.900
But it will probably find
it abandoned somewhere,

00:30:39.900 --> 00:30:41.960
when they realize
that-- they figured out

00:30:41.960 --> 00:30:43.880
that it didn't crash.

00:30:43.880 --> 00:30:45.696
And probably months
from now, they'll

00:30:45.696 --> 00:30:47.070
find it in some
hangar somewhere,

00:30:47.070 --> 00:30:50.050
on the Andaman Islands,
in the Indian Ocean,

00:30:50.050 --> 00:30:53.200
and hopefully the people are OK.

00:30:53.200 --> 00:30:55.150
So I hope that
answered your question.

00:31:00.518 --> 00:31:02.960
AUDIENCE: It's a good sign the
very first page a your book

00:31:02.960 --> 00:31:04.050
is intriguing to me.

00:31:04.050 --> 00:31:06.406
That says, "The future
is already here,

00:31:06.406 --> 00:31:08.030
it's just not very
evenly distributed."

00:31:08.030 --> 00:31:09.430
DANIEL SUAREZ: Yeah,
that's William Gibson.

00:31:09.430 --> 00:31:10.204
Yeah.

00:31:10.204 --> 00:31:14.460
AUDIENCE: And I think when
you talk about people fearing

00:31:14.460 --> 00:31:18.200
technology, there is the
sort of status quo fear,

00:31:18.200 --> 00:31:20.700
but there's also
this fundamental fear

00:31:20.700 --> 00:31:25.090
that technology will
drive us further apart

00:31:25.090 --> 00:31:27.100
and be more polarizing.

00:31:27.100 --> 00:31:28.570
And I wonder if
you could comment

00:31:28.570 --> 00:31:33.130
on the future of
technology as a way

00:31:33.130 --> 00:31:35.600
to bring people closer together.

00:31:35.600 --> 00:31:36.600
DANIEL SUAREZ: Yeah, OK.

00:31:36.600 --> 00:31:42.460
So the question being
that, is technology--

00:31:42.460 --> 00:31:47.020
is that fear valid in any way,
that increasing technology will

00:31:47.020 --> 00:31:49.850
bring us further apart as
opposed to connecting us?

00:31:49.850 --> 00:31:53.020
I find the Amish to be very
interesting in this case,

00:31:53.020 --> 00:31:56.630
because I never thought that
the Amish could use cell phones,

00:31:56.630 --> 00:31:57.810
but apparently they can.

00:31:57.810 --> 00:32:00.607
And talking to Kevin
Kelly-- Kevin Kelly's

00:32:00.607 --> 00:32:02.190
this great expert
on the Amish, that's

00:32:02.190 --> 00:32:06.135
why he's got the beard I guess,
but he-- when he was writing

00:32:06.135 --> 00:32:08.760
about this, what amazed me-- and
"What Technology Wants," which

00:32:08.760 --> 00:32:10.593
is a book he wrote,
it's a fascinating book,

00:32:10.593 --> 00:32:12.890
highly recommend it.

00:32:12.890 --> 00:32:15.720
He talks about how they
will ingest technology

00:32:15.720 --> 00:32:19.610
into Amish society, and their
sole, very purposeful direction

00:32:19.610 --> 00:32:21.870
there is to make sure
that it doesn't break up

00:32:21.870 --> 00:32:23.256
their community.

00:32:23.256 --> 00:32:23.930
That's it.

00:32:23.930 --> 00:32:27.100
That's the sole overriding--
so they, for instance, they

00:32:27.100 --> 00:32:29.880
will have, and this
part amazed me,

00:32:29.880 --> 00:32:32.200
Amish maids running
CNC milling equipment,

00:32:32.200 --> 00:32:35.586
I mean programming these--
because for work purposes,

00:32:35.586 --> 00:32:37.710
they're allowed to use
computers, and robotic arms,

00:32:37.710 --> 00:32:39.560
and all of this other
stuff-- Amish people,

00:32:39.560 --> 00:32:41.520
with the cap and everything.

00:32:41.520 --> 00:32:44.270
Then when they go home though,
they don't use it there.

00:32:44.270 --> 00:32:46.020
So the idea is when
you're in a workplace,

00:32:46.020 --> 00:32:48.240
you can use very
different technology sets

00:32:48.240 --> 00:32:49.550
than when you're at home.

00:32:49.550 --> 00:32:52.417
And so, for using a cell
phone they would have,

00:32:52.417 --> 00:32:54.500
some distance away, on the
edge of their property,

00:32:54.500 --> 00:32:56.291
a box holding their
cell phones because you

00:32:56.291 --> 00:32:58.110
don't want that phone--
you want to make it

00:32:58.110 --> 00:33:00.970
so that you absolutely
need to use it.

00:33:00.970 --> 00:33:04.200
And so, otherwise, if
you're not needful of having

00:33:04.200 --> 00:33:06.940
to have a long
distance conversation,

00:33:06.940 --> 00:33:08.790
you'll talk to the
people around you.

00:33:08.790 --> 00:33:10.020
So that was their focus.

00:33:10.020 --> 00:33:13.100
And a lot of that
makes sense, really.

00:33:13.100 --> 00:33:15.640
I think when we sit around
in places where everybody's

00:33:15.640 --> 00:33:18.240
on their cell phone, I
think we'll get through

00:33:18.240 --> 00:33:19.820
that type of-- I
don't want to say

00:33:19.820 --> 00:33:22.920
dysfunction-- but
alienation, in a few years.

00:33:22.920 --> 00:33:26.910
I think, again, we'll come
to some social contract,

00:33:26.910 --> 00:33:29.190
some agreement-- and we
see this now already,

00:33:29.190 --> 00:33:30.740
where it's OK to
use a cell phone,

00:33:30.740 --> 00:33:32.150
where people frown on it.

00:33:32.150 --> 00:33:34.483
I don't know how many of you
have gone to a dinner where

00:33:34.483 --> 00:33:37.480
everybody piles the phone up,
and the first person to use it

00:33:37.480 --> 00:33:39.240
has to buy a round of drinks.

00:33:39.240 --> 00:33:43.200
That's a perfect example,
it's like penalty system

00:33:43.200 --> 00:33:47.360
for misusing technology
in the social context.

00:33:47.360 --> 00:33:48.460
But we'll still have them.

00:33:48.460 --> 00:33:50.360
And I do think they
bring us closer together.

00:33:50.360 --> 00:33:52.960
There's people that
I would not have

00:33:52.960 --> 00:33:55.580
talked to, people I
went to high school,

00:33:55.580 --> 00:33:58.880
that I'm now still very close
with because of technology.

00:33:58.880 --> 00:34:02.610
The depth of that relationship,
and how deep we get,

00:34:02.610 --> 00:34:07.480
sure, you could make an argument
that having 20,000 social media

00:34:07.480 --> 00:34:11.004
friends is not friends, so
let's choose a different word.

00:34:11.004 --> 00:34:13.170
But I still think that we're
more connected and more

00:34:13.170 --> 00:34:17.050
social than if we didn't
have these technologies.

00:34:17.050 --> 00:34:21.449
I will say though, that
there is an aspect of it

00:34:21.449 --> 00:34:23.800
where-- how to put this?

00:34:23.800 --> 00:34:26.872
We have to be mindful of what
the purpose is of the system.

00:34:26.872 --> 00:34:29.330
In other words, if the system
is to drive advertising or do

00:34:29.330 --> 00:34:32.389
something else, everybody just
needs to be aware of that.

00:34:32.389 --> 00:34:34.020
In other words, what
you actually see,

00:34:34.020 --> 00:34:37.610
what is put before
you as important,

00:34:37.610 --> 00:34:39.750
I think there needs to
be some clarity there.

00:34:39.750 --> 00:34:42.250
Because, if in any way
there are algorithms

00:34:42.250 --> 00:34:45.659
weighing in that somehow
modify your social circle,

00:34:45.659 --> 00:34:48.130
that is keenly
interesting to me.

00:34:48.130 --> 00:34:51.400
Because to the extent that
we control our social circle,

00:34:51.400 --> 00:34:54.960
and see the things that
we-- we direct what we see.

00:34:54.960 --> 00:34:57.700
I think it's very
interesting, very positive.

00:34:57.700 --> 00:35:01.540
It's where you have the power
of software enabling things

00:35:01.540 --> 00:35:03.099
to happen invisibly.

00:35:03.099 --> 00:35:05.390
Again, I'm one of those people
who always, always wants

00:35:05.390 --> 00:35:09.370
to basically tear the black
box open and take a look at it.

00:35:09.370 --> 00:35:12.790
Now that's not always possible
in modern society, obviously,

00:35:12.790 --> 00:35:14.110
but that's my impulse.

00:35:14.110 --> 00:35:21.330
I tend to think of the
social-- how would I put it?

00:35:21.330 --> 00:35:24.060
Well, these networks
are becoming society,

00:35:24.060 --> 00:35:27.440
and to that degree it's
sort of civic concern.

00:35:27.440 --> 00:35:31.170
So I like to understand
the inner workings of it.

00:35:31.170 --> 00:35:34.490
Again, for the same reason,
I don't want my phone

00:35:34.490 --> 00:35:37.090
doing a bunch of things
I didn't ask it to do

00:35:37.090 --> 00:35:39.400
or didn't agree to.

00:35:39.400 --> 00:35:43.080
If I have any concerns,
that's where they lie.

00:35:43.080 --> 00:35:47.080
But I do think technology,
by and large, makes society

00:35:47.080 --> 00:35:49.089
function much more smoothly.

00:35:49.089 --> 00:35:50.880
And can put people in
touch with each other

00:35:50.880 --> 00:35:52.090
who never would meet.

00:35:52.090 --> 00:35:55.930
And there are literally hundreds
of examples of that for myself.

00:35:55.930 --> 00:35:59.280
I have been connected with
so many readers by email,

00:35:59.280 --> 00:36:01.970
and then later social media,
and then, just right now,

00:36:01.970 --> 00:36:04.970
in South By Southwest, I
met a great many readers,

00:36:04.970 --> 00:36:06.670
who turned out to
be very cool people,

00:36:06.670 --> 00:36:09.060
that I don't think I
would've as easily met

00:36:09.060 --> 00:36:10.620
it wasn't for this technology.

00:36:10.620 --> 00:36:13.384
So it's hard to say that
that's not a social boon.

00:36:17.650 --> 00:36:19.864
MALE SPEAKER: I think
you're [INAUDIBLE] over.

00:36:19.864 --> 00:36:20.670
AUDIENCE: Hi.

00:36:20.670 --> 00:36:21.860
DANIEL SUAREZ: Hello.

00:36:21.860 --> 00:36:25.590
AUDIENCE: So would you
agree that technology begets

00:36:25.590 --> 00:36:29.160
technology, so computers
enable better computers,

00:36:29.160 --> 00:36:32.310
and the internet helps us
build a better internet?

00:36:32.310 --> 00:36:34.880
Do you think we're accelerating
and then do you ever think

00:36:34.880 --> 00:36:37.620
about an end-game, where
suddenly like-- [POP] we just--

00:36:37.620 --> 00:36:40.980
DANIEL SUAREZ:
Singularity city, right.

00:36:40.980 --> 00:36:44.450
I think about that, more
or less all the time,

00:36:44.450 --> 00:36:46.930
because that's
kind of my job now.

00:36:46.930 --> 00:36:48.105
It can be unhealthy.

00:36:48.105 --> 00:36:49.350
It's like, what's your job?

00:36:49.350 --> 00:36:51.725
Well, I think about how the
world's going to end, mostly.

00:36:54.090 --> 00:36:55.260
No.

00:36:55.260 --> 00:36:57.540
And by the way, I'm not a
doom and gloom guy at all,

00:36:57.540 --> 00:36:59.070
just to preface this.

00:36:59.070 --> 00:37:02.070
I am very optimistic about
where technology will bring us.

00:37:02.070 --> 00:37:05.740
I like to think what I do is
sort of like spotting icebergs.

00:37:05.740 --> 00:37:07.800
I might suggest some
course corrections,

00:37:07.800 --> 00:37:10.350
but I'm still happy
I'm on the boat.

00:37:10.350 --> 00:37:13.720
And I'll give you a
good example of-- yes,

00:37:13.720 --> 00:37:19.420
I do think technologies,
they sort of catapult us

00:37:19.420 --> 00:37:21.730
to different, higher
orbits in a way.

00:37:21.730 --> 00:37:24.872
And to me an apt
example would be,

00:37:24.872 --> 00:37:26.580
the cell phone was
around for many years,

00:37:26.580 --> 00:37:28.710
and the computer was
around for many years,

00:37:28.710 --> 00:37:31.370
and then somebody shrunk it
down into this handheld thing

00:37:31.370 --> 00:37:37.130
called a smartphone and boom-- a
whole different thing happened.

00:37:37.130 --> 00:37:39.141
Both of those
technologies existed,

00:37:39.141 --> 00:37:41.140
but combining them together
in a mobile platform

00:37:41.140 --> 00:37:42.074
changed everything.

00:37:42.074 --> 00:37:44.740
I think we're going to see a lot
of that in coming years, which,

00:37:44.740 --> 00:37:47.550
what's going to make--
robotics is a good example.

00:37:47.550 --> 00:37:51.260
We will wrestle, as a society,
with automation and robotics.

00:37:51.260 --> 00:37:54.050
I think a great many
good things will happen.

00:37:54.050 --> 00:38:01.530
And I think the parable here is
that idea of the last company

00:38:01.530 --> 00:38:03.640
to make buggy whips.

00:38:03.640 --> 00:38:06.670
They made the best
damn buggy whips ever,

00:38:06.670 --> 00:38:08.880
but eventually we didn't
need them anymore.

00:38:08.880 --> 00:38:12.117
And yet, lot of
horses lost their jobs

00:38:12.117 --> 00:38:13.700
and there was a big
shift, but I think

00:38:13.700 --> 00:38:16.140
we will endeavor
to do other things.

00:38:16.140 --> 00:38:18.190
So the hope is, that
is what happens,

00:38:18.190 --> 00:38:21.210
that new industries are created.

00:38:21.210 --> 00:38:23.387
Again, software jobs,
and they didn't exist.

00:38:23.387 --> 00:38:25.720
So we're going to have upheaval
in the meantime, though.

00:38:25.720 --> 00:38:29.710
I think some serious upheaval in
terms of job loss, dislocation.

00:38:29.710 --> 00:38:31.620
And it's getting
through those that I

00:38:31.620 --> 00:38:33.120
think are going to
be our challenge.

00:38:33.120 --> 00:38:34.710
Not where we're going.

00:38:34.710 --> 00:38:37.450
I think as a species,
as a society,

00:38:37.450 --> 00:38:40.400
as a civilization, the more
technology we have, well,

00:38:40.400 --> 00:38:41.560
we're going to need it.

00:38:41.560 --> 00:38:42.935
We're going to
need it because we

00:38:42.935 --> 00:38:46.120
have some very serious problems
in terms of climate change,

00:38:46.120 --> 00:38:50.160
and that's just for
starters, but also

00:38:50.160 --> 00:38:52.300
dealing with large
numbers of people

00:38:52.300 --> 00:38:54.670
at a time of climate change.

00:38:54.670 --> 00:38:56.470
That's going to be
a whole other thing,

00:38:56.470 --> 00:38:58.370
rapidly shifting populations.

00:38:58.370 --> 00:38:59.760
And then there's space.

00:38:59.760 --> 00:39:02.140
That, to me, is the
logical thing to do next.

00:39:02.140 --> 00:39:05.019
I think it's about time
that we got busy on that.

00:39:05.019 --> 00:39:06.560
And there's plenty
of room out there,

00:39:06.560 --> 00:39:09.850
so we're going to need every bit
of our technological know-how

00:39:09.850 --> 00:39:11.570
to keep us alive out there.

00:39:11.570 --> 00:39:14.070
Not only that, but
to build sustainable,

00:39:14.070 --> 00:39:16.890
self-enclosed biospheres.

00:39:16.890 --> 00:39:18.450
There are so many
challenges for us,

00:39:18.450 --> 00:39:20.240
and technology will
make it all possible.

00:39:20.240 --> 00:39:22.310
I'm very excited about it.

00:39:22.310 --> 00:39:24.510
I think we will go in leaps.

00:39:24.510 --> 00:39:27.510
We'll have these
pivotal inventions

00:39:27.510 --> 00:39:29.440
that take us to the next level.

00:39:29.440 --> 00:39:32.230
There will be disruption
centered around those leaps,

00:39:32.230 --> 00:39:34.290
but getting to the
next level to me

00:39:34.290 --> 00:39:37.150
is-- I guess it's like a game.

00:39:37.150 --> 00:39:40.945
I'm a big game fan and
getting to the next level

00:39:40.945 --> 00:39:44.110
is literally what we're
trying to do, time and time

00:39:44.110 --> 00:39:44.960
and time again.

00:39:44.960 --> 00:39:46.360
And as far as game over?

00:39:46.360 --> 00:39:48.530
Sure, I guess the game
will be over eventually

00:39:48.530 --> 00:39:50.760
but, I don't know,
not for a long time.

00:39:50.760 --> 00:39:52.343
Hopefully a long,
long, long time.

00:39:55.654 --> 00:39:57.509
MALE SPEAKER: One of you guys.

00:39:57.509 --> 00:40:00.050
AUDIENCE: You said you were an
optimist and, you know I think

00:40:00.050 --> 00:40:02.890
I'm optimistic by nature, but
I'm not sure it's-- sometimes

00:40:02.890 --> 00:40:04.640
with technology, it
doesn't seem rational.

00:40:04.640 --> 00:40:06.014
I mean, if you
think about maybe,

00:40:06.014 --> 00:40:09.800
do you address the Bill
Joy, slash Kaczynski, slash

00:40:09.800 --> 00:40:11.650
Kurzweil discussion
argument a little?

00:40:11.650 --> 00:40:14.060
DANIEL SUAREZ: Yeah.

00:40:14.060 --> 00:40:14.670
OK.

00:40:14.670 --> 00:40:20.180
Is optimism rational when
it comes to technology?

00:40:20.180 --> 00:40:22.120
Actually, it's a fair
point, it really is.

00:40:22.120 --> 00:40:25.830
But I would say this, I look at
technologies like the blinding

00:40:25.830 --> 00:40:26.820
laser.

00:40:26.820 --> 00:40:29.320
This is an immensely
effective weapon.

00:40:29.320 --> 00:40:32.520
It is insidious, it's cheap,
you can blind everybody

00:40:32.520 --> 00:40:35.610
on a battlefield if you have
the proper swivelling head,

00:40:35.610 --> 00:40:38.550
anybody who's looking
is blinded, for good.

00:40:38.550 --> 00:40:40.390
And yet, they're not used.

00:40:40.390 --> 00:40:42.360
There's a prohibition
against them.

00:40:42.360 --> 00:40:45.650
Pretty much every country
says, no, let's not use these.

00:40:45.650 --> 00:40:48.460
That to me right there
is a slice of optimism,

00:40:48.460 --> 00:40:50.580
that there's
limit-- again, we've

00:40:50.580 --> 00:40:53.030
had nuclear weapons
for half a century now,

00:40:53.030 --> 00:40:56.780
we're still here, that's another
tremendous, tremendous argument

00:40:56.780 --> 00:40:58.110
in our favor.

00:40:58.110 --> 00:41:00.790
I think we're capable of
pulling back from the brinks.

00:41:00.790 --> 00:41:04.420
There's things we'll do,
even in large numbers,

00:41:04.420 --> 00:41:06.050
and things we won't do.

00:41:06.050 --> 00:41:10.476
So it's that sociability
of mammals, I think.

00:41:10.476 --> 00:41:11.850
This was one of
the reasons why I

00:41:11.850 --> 00:41:14.580
was so concerned about
automation of warfare

00:41:14.580 --> 00:41:17.080
with robotics, is
because if we take

00:41:17.080 --> 00:41:21.390
these evolved prohibitions,
or limits, to violence

00:41:21.390 --> 00:41:24.080
out of the system, and
instead we imbued robots

00:41:24.080 --> 00:41:27.810
with a sort of an insectoid,
or a colony brain--

00:41:27.810 --> 00:41:30.840
and again, you give it the
same brain as a species that's

00:41:30.840 --> 00:41:32.450
not concerned about
self preservation

00:41:32.450 --> 00:41:34.260
because it has to
queen that does that.

00:41:34.260 --> 00:41:37.850
Again, the fact that I gave that
TED Talk and talked about it,

00:41:37.850 --> 00:41:40.540
and a lot of people afterwards
were talking to me about it,

00:41:40.540 --> 00:41:42.800
people in very
interesting positions.

00:41:42.800 --> 00:41:44.240
That gives me optimism.

00:41:44.240 --> 00:41:46.790
The fact that people can
hear these reasoned arguments

00:41:46.790 --> 00:41:49.600
and say, well, that might
be good in the short-term,

00:41:49.600 --> 00:41:50.570
but let's not do it.

00:41:50.570 --> 00:41:53.460
I mean, we're still
here, our lifespans

00:41:53.460 --> 00:41:57.380
are pretty much double from
what they were a century ago.

00:41:57.380 --> 00:41:59.840
A lot of that's
technology and reason.

00:41:59.840 --> 00:42:02.610
Yes, there's horrible
warfare, but on the whole,

00:42:02.610 --> 00:42:05.880
warfare is going down.

00:42:05.880 --> 00:42:09.130
It's hard not to be a bit
optimistic about that.

00:42:09.130 --> 00:42:12.880
Now, again, that's
not-- what do they say,

00:42:12.880 --> 00:42:17.300
past results don't
guarantee future results--

00:42:17.300 --> 00:42:20.987
but I think that technology
is pretty much the best

00:42:20.987 --> 00:42:21.820
thing we have going.

00:42:21.820 --> 00:42:24.790
And really, what it is, is
it's a physical manifestation

00:42:24.790 --> 00:42:26.980
of human will, it's reason.

00:42:26.980 --> 00:42:28.990
It's reason personified.

00:42:28.990 --> 00:42:30.690
That's what I like
about technology.

00:42:30.690 --> 00:42:32.660
Because you can
see how it's used,

00:42:32.660 --> 00:42:34.410
you can talk about how
it was constructed,

00:42:34.410 --> 00:42:36.190
and then modify
it, and then we can

00:42:36.190 --> 00:42:38.990
have that constant
iterative conversation.

00:42:38.990 --> 00:42:40.820
That's to me with
technology represents.

00:42:40.820 --> 00:42:44.139
Which is why I'm
optimistic about it.

00:42:44.139 --> 00:42:46.180
But then, you're right,
there are occasional ones

00:42:46.180 --> 00:42:48.060
that make you wonder
like, well, how

00:42:48.060 --> 00:42:50.200
long are we going to be here?

00:42:50.200 --> 00:42:52.390
But thankfully that's
not all the time.

00:42:52.390 --> 00:42:55.330
Most of the time when I see
a new technology I think,

00:42:55.330 --> 00:42:57.980
oh cool, they're doing that now.

00:42:57.980 --> 00:43:00.380
I think medieval
farmers probably

00:43:00.380 --> 00:43:02.921
didn't have as much
entertainment as far as

00:43:02.921 --> 00:43:03.420
that goes.

00:43:03.420 --> 00:43:05.795
It's like, wow, we're still
doing crops just like we did,

00:43:05.795 --> 00:43:06.810
and my father.

00:43:06.810 --> 00:43:09.685
We get a much more interesting
life cycle for technology.

00:43:14.780 --> 00:43:17.330
AUDIENCE: So this is a little
bit of a two-part question.

00:43:17.330 --> 00:43:19.070
Everybody else has
asked you a bunch

00:43:19.070 --> 00:43:20.860
of really cool questions
about technology.

00:43:20.860 --> 00:43:23.068
My question is more about
the writing side of things.

00:43:23.068 --> 00:43:24.146
DANIEL SUAREZ: OK.

00:43:24.146 --> 00:43:25.770
AUDIENCE: So I really
enjoy your books.

00:43:25.770 --> 00:43:26.050
DANIEL SUAREZ: Thank you.

00:43:26.050 --> 00:43:28.560
AUDIENCE: A couple of my friends
just pestered me over and over,

00:43:28.560 --> 00:43:30.160
you have to read "Demon,"
you have to read "Demon."

00:43:30.160 --> 00:43:31.440
DANIEL SUAREZ: Thank
them for that, please.

00:43:31.440 --> 00:43:33.356
AUDIENCE: And I picked
it up and it was great.

00:43:33.356 --> 00:43:37.270
And I think your books-- they're
really good at sucking you in.

00:43:37.270 --> 00:43:39.760
I've been a voracious
reader my entire life,

00:43:39.760 --> 00:43:42.270
and the best books are
the ones where I sit down,

00:43:42.270 --> 00:43:46.260
and I'm reading the book, and
the entire world is just not

00:43:46.260 --> 00:43:47.540
interesting to me.

00:43:47.540 --> 00:43:49.480
And when I'm not
reading the book,

00:43:49.480 --> 00:43:51.246
I'm plotting how to
get back to reading

00:43:51.246 --> 00:43:53.440
the book until I'm finished.

00:43:53.440 --> 00:43:55.980
And so I think you
do a really good job

00:43:55.980 --> 00:43:57.870
of structuring the
books and making them--

00:43:57.870 --> 00:43:58.200
DANIEL SUAREZ: Thank you.

00:43:58.200 --> 00:43:59.658
AUDIENCE: --readable
in that sense,

00:43:59.658 --> 00:44:02.990
and I was wondering what your
process is for doing that.

00:44:02.990 --> 00:44:06.350
Are you somebody who has
a really strong outline

00:44:06.350 --> 00:44:07.210
before you start?

00:44:07.210 --> 00:44:09.242
Or do you-- like, OK,
I've got an endpoint,

00:44:09.242 --> 00:44:10.700
and I'm going to
sort of make it up

00:44:10.700 --> 00:44:12.560
as I go along and
just naturally,

00:44:12.560 --> 00:44:15.350
where the characters take
me and connect from here?

00:44:15.350 --> 00:44:17.270
And the second
piece is, I've had

00:44:17.270 --> 00:44:19.370
a lot of good luck over
the years discovering

00:44:19.370 --> 00:44:22.750
new people through
existing authors.

00:44:22.750 --> 00:44:24.542
So I now have-- you
know, Daniel Suarez

00:44:24.542 --> 00:44:26.750
comes out with a new book,
I'm going to pick that up.

00:44:26.750 --> 00:44:27.595
DANIEL SUAREZ: Cool.

00:44:27.595 --> 00:44:28.220
AUDIENCE: Good.

00:44:28.220 --> 00:44:30.210
But are there people
like that for you?

00:44:30.210 --> 00:44:32.080
Who are you reading
who you think

00:44:32.080 --> 00:44:34.109
is doing really interesting
things right now?

00:44:34.109 --> 00:44:36.150
Because, hopefully, there's
somebody on that list

00:44:36.150 --> 00:44:37.490
that I don't have on mine.

00:44:37.490 --> 00:44:38.890
DANIEL SUAREZ: OK.

00:44:38.890 --> 00:44:40.760
The first question
is a great question.

00:44:40.760 --> 00:44:44.220
I like talking about
the craft of writing.

00:44:44.220 --> 00:44:49.380
That effect that you talk about,
where your sensory perception

00:44:49.380 --> 00:44:53.420
is overridden by fiction, I
call that casting the spell,

00:44:53.420 --> 00:44:56.004
essentially, because I enjoy
it when it happens to me,

00:44:56.004 --> 00:44:57.420
when I'm reading
a book and then I

00:44:57.420 --> 00:45:00.360
forget that I'm a person
sitting in a chair.

00:45:00.360 --> 00:45:03.010
I'm instead in some
medieval place or whatever.

00:45:03.010 --> 00:45:05.872
That is really-- talk about
an amazing technology.

00:45:05.872 --> 00:45:07.330
That somebody could
squiggle things

00:45:07.330 --> 00:45:10.245
and that we could make that work
at all is just mind blowing.

00:45:10.245 --> 00:45:11.640
It really is.

00:45:11.640 --> 00:45:16.780
So I have great awe for that
rather old, amazing technology.

00:45:16.780 --> 00:45:21.760
As far as how I go
about crafting a book,

00:45:21.760 --> 00:45:23.550
I very definitely
use an outline.

00:45:23.550 --> 00:45:25.850
And maybe that's the
software guy in me,

00:45:25.850 --> 00:45:30.130
you're designing databases,
but thrillers in particular

00:45:30.130 --> 00:45:33.600
are all about pacing.

00:45:33.600 --> 00:45:36.280
I suppose I acquired
the skill set

00:45:36.280 --> 00:45:40.820
to do this by reading a
bunch of thrillers and books.

00:45:40.820 --> 00:45:44.420
But also, actually, I
think a big piece of it

00:45:44.420 --> 00:45:45.420
was role playing games.

00:45:45.420 --> 00:45:49.280
I used to be a DM,
second edition D&amp;D.

00:45:49.280 --> 00:45:52.310
And if you want honest
feedback, I must tell you,

00:45:52.310 --> 00:45:55.796
having a group of teenage guys,
or college-level guys around,

00:45:55.796 --> 00:45:57.670
you will get immediate,
very honest, feedback

00:45:57.670 --> 00:46:00.200
about how things are going.

00:46:00.200 --> 00:46:03.090
So that gives you a sense
of pacing, story pacing,

00:46:03.090 --> 00:46:05.330
because it is very
much a knife's edge

00:46:05.330 --> 00:46:09.340
when you're talking about a
suspense or thriller book.

00:46:09.340 --> 00:46:11.350
If it's going too
easy, it gets boring.

00:46:11.350 --> 00:46:12.930
If it's too hard,
it's frustrating.

00:46:12.930 --> 00:46:16.370
And so what I simply
do is write a lot.

00:46:16.370 --> 00:46:18.997
I'll typically write twice
as much as I need to,

00:46:18.997 --> 00:46:20.830
and then I will let it
cool off for a while.

00:46:20.830 --> 00:46:23.110
Now, again, I will do
most of the writing--

00:46:23.110 --> 00:46:25.990
I'd say most of the writing
process is in the planning,

00:46:25.990 --> 00:46:27.640
is in structuring.

00:46:27.640 --> 00:46:30.690
I will typically start
with a three-act structure

00:46:30.690 --> 00:46:32.510
with certain key
points, turning points--

00:46:32.510 --> 00:46:34.620
first act, turning
point, second act,

00:46:34.620 --> 00:46:37.690
turning point-- as a framework.

00:46:37.690 --> 00:46:40.630
Now that still leaves a lot of
room for individual creativity,

00:46:40.630 --> 00:46:43.390
but this tripartite
structure has

00:46:43.390 --> 00:46:47.980
been very powerful--
archetypes, long,

00:46:47.980 --> 00:46:50.360
long stories from many years
ago follow this structure,

00:46:50.360 --> 00:46:53.625
and it seems to map pretty
well with the human brain.

00:46:53.625 --> 00:46:55.500
Let's face it, just a
beginning, middle, end,

00:46:55.500 --> 00:46:57.440
and right there that
just makes sense.

00:46:57.440 --> 00:47:02.070
And to be cognizant of that
structure as you build it,

00:47:02.070 --> 00:47:03.930
it's very key with me.

00:47:03.930 --> 00:47:08.540
And then, basically,
I will write a lot

00:47:08.540 --> 00:47:12.390
and then take a merciless,
just a machete, to it later.

00:47:12.390 --> 00:47:14.980
That to me is the key thing,
being able to be honest with

00:47:14.980 --> 00:47:17.360
yourself about what
works and what doesn't.

00:47:17.360 --> 00:47:21.490
Having an internal
editor is so important,

00:47:21.490 --> 00:47:24.860
because I find when I take
about half of my book away,

00:47:24.860 --> 00:47:28.310
it's a lot better,
a lot tighter.

00:47:28.310 --> 00:47:31.297
I don't know what process
others, other writers, use

00:47:31.297 --> 00:47:32.630
but that is very definitely one.

00:47:32.630 --> 00:47:36.120
So yeah, it involves
writing about 300,000 words

00:47:36.120 --> 00:47:38.330
for 150,000 word book.

00:47:38.330 --> 00:47:39.780
But the other thing
I would say is

00:47:39.780 --> 00:47:42.280
when you're starting
out a book-- I'll

00:47:42.280 --> 00:47:44.880
run into people who have a
great deal of difficulty getting

00:47:44.880 --> 00:47:46.755
started on the book,
because they're focused,

00:47:46.755 --> 00:47:48.150
like, how do I begin this?

00:47:48.150 --> 00:47:50.610
And I just say just write it,
because that first chapter,

00:47:50.610 --> 00:47:51.190
it's doomed.

00:47:51.190 --> 00:47:53.354
Don't even think it's
going to be in the book.

00:47:53.354 --> 00:47:54.770
Why have any stress
over something

00:47:54.770 --> 00:47:56.400
that you're absolutely
going to delete later,

00:47:56.400 --> 00:47:57.280
because you're going to hate it.

00:47:57.280 --> 00:47:58.750
Because what's going to
happen is, you're going to go,

00:47:58.750 --> 00:48:00.944
even if you have a
very careful structure,

00:48:00.944 --> 00:48:02.610
you're going to go
write that whole book

00:48:02.610 --> 00:48:05.592
and really, for me, that
first chapter so important.

00:48:05.592 --> 00:48:07.300
Typically, it's best
informed by the rest

00:48:07.300 --> 00:48:08.621
of the book that follows it.

00:48:08.621 --> 00:48:10.120
So that when I'm
done with the book,

00:48:10.120 --> 00:48:11.950
I'll go back to
the first chapter,

00:48:11.950 --> 00:48:14.540
and find out all these seeds
and threads that I need to plant

00:48:14.540 --> 00:48:15.530
and tie back in.

00:48:15.530 --> 00:48:19.650
I'll typically go back and
weave threads, various subplots

00:48:19.650 --> 00:48:21.460
and stories, throughout
the whole book,

00:48:21.460 --> 00:48:23.330
and have them surface
at important points

00:48:23.330 --> 00:48:26.500
to reinforce things,
themes, characters.

00:48:26.500 --> 00:48:27.000
Yeah.

00:48:27.000 --> 00:48:28.416
So you can see you
can wind me up.

00:48:28.416 --> 00:48:31.020
I love talking the craft of
writing because, in many ways,

00:48:31.020 --> 00:48:32.290
it is programming.

00:48:32.290 --> 00:48:38.200
It's-- things have to
compile in the human brain.

00:48:38.200 --> 00:48:40.550
It's like, ah,
that's not compiling.

00:48:40.550 --> 00:48:42.300
And I guess character
names are variables,

00:48:42.300 --> 00:48:44.950
but I'll finish it there.

00:48:44.950 --> 00:48:47.220
Yeah, so that's
generally how I do it.

00:48:47.220 --> 00:48:50.110
And the other one was, who
am I reading right now?

00:48:50.110 --> 00:48:52.640
Or, what authors
do I really like?

00:48:52.640 --> 00:48:54.200
I read a lot of
nonfiction, so I'm

00:48:54.200 --> 00:48:56.850
going to try to focus
on some fiction writers.

00:48:56.850 --> 00:49:01.140
I just read-- oh, god, "Name of
the Wind," "Name of the Wind,"

00:49:01.140 --> 00:49:02.020
what is it?

00:49:02.020 --> 00:49:03.224
Yes.

00:49:03.224 --> 00:49:05.390
He has a cool name, I
remembered-- what is it again?

00:49:05.390 --> 00:49:05.690
AUDIENCE: Patrick Rothfuss.

00:49:05.690 --> 00:49:07.023
DANIEL SUAREZ: Patrick Rothfuss.

00:49:07.023 --> 00:49:08.610
I enjoy that very much.

00:49:08.610 --> 00:49:10.870
I think also I tend,
lately, to be reading

00:49:10.870 --> 00:49:13.220
so many medieval fantasy
books because I've

00:49:13.220 --> 00:49:14.740
read so many
thrillers that I keep

00:49:14.740 --> 00:49:16.002
looking for the scaffolding.

00:49:16.002 --> 00:49:17.960
All of the things that
I just described to you,

00:49:17.960 --> 00:49:21.000
I'd be like, well, where
is the second act turning?

00:49:21.000 --> 00:49:23.510
It's sort of like, knowing
that as a craft makes it tough

00:49:23.510 --> 00:49:24.710
sometimes to read them.

00:49:24.710 --> 00:49:28.760
Although I just Andy
Weir's book, "Martian."

00:49:28.760 --> 00:49:29.389
Loved it.

00:49:29.389 --> 00:49:30.430
Just absolutely loved it.

00:49:30.430 --> 00:49:32.500
And what I loved in
particular about his book is,

00:49:32.500 --> 00:49:34.777
he does not shrink
away from science.

00:49:34.777 --> 00:49:36.610
For those who aren't
familiar with his book,

00:49:36.610 --> 00:49:39.430
he basically has a guy find
himself stranded on Mars

00:49:39.430 --> 00:49:42.510
as part of a team, and
using all of the science

00:49:42.510 --> 00:49:45.280
and scientific knowledge
at his disposal to survive.

00:49:45.280 --> 00:49:47.100
And I found it to
be very interesting.

00:49:47.100 --> 00:49:50.260
And it's not like-- I don't
know how many people remember

00:49:50.260 --> 00:49:51.740
"Robinson Crusoe"-- on Mars.

00:49:51.740 --> 00:49:53.400
No, it's not that.

00:49:53.400 --> 00:49:56.140
He does not run into a Friday
who's dropped-- no, it's

00:49:56.140 --> 00:49:58.750
a much more realistic book
and I enjoyed it very much.

00:49:58.750 --> 00:50:04.240
But again, I enjoyed it because
he did the heavy lifting.

00:50:04.240 --> 00:50:07.400
He did a lot of research to
really back it up with science,

00:50:07.400 --> 00:50:09.110
and then he wove a
story into it that

00:50:09.110 --> 00:50:11.590
had a really compelling pace.

00:50:11.590 --> 00:50:13.509
And it was constantly
raising the stakes.

00:50:13.509 --> 00:50:15.050
That's another thing
with a thriller,

00:50:15.050 --> 00:50:17.282
is you want to be
raising the stakes.

00:50:17.282 --> 00:50:19.490
And it sounds kind of cheesy
when I say it like that,

00:50:19.490 --> 00:50:23.750
but in effect, that's building
that action, involving us more,

00:50:23.750 --> 00:50:25.245
having characters we care about.

00:50:25.245 --> 00:50:26.120
That's another thing.

00:50:26.120 --> 00:50:27.860
That's something
that, for instance, I

00:50:27.860 --> 00:50:30.910
don't get a lot when I see some
movies, a lot of big action

00:50:30.910 --> 00:50:32.680
movies.

00:50:32.680 --> 00:50:34.650
Very often I'll be
finding myself saying,

00:50:34.650 --> 00:50:36.770
why do I care about this person?

00:50:36.770 --> 00:50:37.680
Because I'm told to.

00:50:37.680 --> 00:50:38.812
That's not enough.

00:50:38.812 --> 00:50:41.270
And when it's done right, and
you get your buttons pressed,

00:50:41.270 --> 00:50:43.300
you don't mind getting
your buttons pressed.

00:50:43.300 --> 00:50:46.300
But if you get sore about
it, it's not being done well.

00:50:46.300 --> 00:50:49.000
So hopefully I answered
your two questions.

00:50:49.000 --> 00:50:49.680
OK.

00:50:49.680 --> 00:50:51.680
MALE SPEAKER: I think we
have time for one more.

00:50:56.850 --> 00:50:58.340
AUDIENCE: So I
guess my question is

00:50:58.340 --> 00:50:59.920
about the people's
social aspect.

00:50:59.920 --> 00:51:00.410
DANIEL SUAREZ: Yes.

00:51:00.410 --> 00:51:02.285
AUDIENCE: You gave the
example of the camera,

00:51:02.285 --> 00:51:05.110
and over the course of
time, through back and forth

00:51:05.110 --> 00:51:08.010
discussion and op-eds
and everything else,

00:51:08.010 --> 00:51:10.110
people slowly
began to accept it.

00:51:10.110 --> 00:51:13.150
But in this time
our social discourse

00:51:13.150 --> 00:51:14.600
isn't nearly as polite.

00:51:14.600 --> 00:51:17.920
And it's a lot more, I'm right,
you're wrong, and that's final,

00:51:17.920 --> 00:51:20.360
and not this back
and forth discussion.

00:51:20.360 --> 00:51:23.460
And I wonder what you think
that's going to play into,

00:51:23.460 --> 00:51:24.970
this forward progress
of technology,

00:51:24.970 --> 00:51:27.289
when people are saying,
I'm right, you're wrong--

00:51:27.289 --> 00:51:28.080
DANIEL SUAREZ: Yes.

00:51:28.080 --> 00:51:29.240
AUDIENCE: --that's it.

00:51:29.240 --> 00:51:32.449
DANIEL SUAREZ: I feel better
about that for this reason.

00:51:32.449 --> 00:51:33.990
I'm trying to remember
the election--

00:51:33.990 --> 00:51:36.990
I think it was the election
of the 1880's and whoever

00:51:36.990 --> 00:51:38.810
was running for
office then, it's

00:51:38.810 --> 00:51:41.470
like one of these Taft
presidents, guys you barely

00:51:41.470 --> 00:51:44.340
remember-- but his opponent at
the time, in the newspapers,

00:51:44.340 --> 00:51:47.730
accused him of being a cannibal.

00:51:47.730 --> 00:51:51.430
And I thought, wow, man,
talk about-- so apparently,

00:51:51.430 --> 00:51:54.230
going back to the time of
Benjamin Franklin, Jefferson,

00:51:54.230 --> 00:51:56.550
all these pamphlets
would just be out,

00:51:56.550 --> 00:51:58.380
political in other
words, just the most

00:51:58.380 --> 00:51:59.920
vile lies about people.

00:51:59.920 --> 00:52:02.160
And when you take
a look at that past

00:52:02.160 --> 00:52:05.262
and think about how we do
things now, it's almost genteel.

00:52:05.262 --> 00:52:06.970
It's like, I really
don't agree with you.

00:52:06.970 --> 00:52:08.580
I think you're a
terrible person,

00:52:08.580 --> 00:52:11.140
not that you're a cannibal.

00:52:11.140 --> 00:52:15.670
So I think that history
shows us that we really

00:52:15.670 --> 00:52:16.920
haven't changed all that much.

00:52:16.920 --> 00:52:20.690
And if we have, we
are mindful of how

00:52:20.690 --> 00:52:22.859
we'll look when somebody
does a Google search on us.

00:52:22.859 --> 00:52:24.275
You'll sound like
a raving lunatic

00:52:24.275 --> 00:52:26.440
if you said somebody
was a cannibal.

00:52:26.440 --> 00:52:30.870
But I don't think that
that, that, I don't know,

00:52:30.870 --> 00:52:33.990
the cordiality of
it, is as necessary.

00:52:33.990 --> 00:52:37.010
And maybe it's because I grew
up going through the New Jersey

00:52:37.010 --> 00:52:40.100
public school
system, but I think

00:52:40.100 --> 00:52:42.370
a vigorous debate,
let's call it,

00:52:42.370 --> 00:52:45.730
is a huge corrective for us.

00:52:45.730 --> 00:52:47.600
I think being
polite to each other

00:52:47.600 --> 00:52:49.270
isn't as-- now, you
were also talking

00:52:49.270 --> 00:52:52.460
about deliberate mistruths,
I imagine, right?

00:52:52.460 --> 00:52:55.400
People digging in their
heels, and that old idea

00:52:55.400 --> 00:52:58.240
that you're entitled to your
opinion but not your facts.

00:52:58.240 --> 00:52:59.890
That, to me, is the
bigger challenge,

00:52:59.890 --> 00:53:03.810
is muddying the waters
with fake facts.

00:53:03.810 --> 00:53:09.140
And, boy, this is where I'll go
on a soapbox about education.

00:53:09.140 --> 00:53:11.850
Public school education
in New Jersey, 1970s,

00:53:11.850 --> 00:53:13.960
I got a tremendously
good education.

00:53:13.960 --> 00:53:17.300
In particular, we have
formal logic, right?

00:53:17.300 --> 00:53:20.100
To learn how to think
critically, this to me

00:53:20.100 --> 00:53:22.680
is a big thing.

00:53:22.680 --> 00:53:26.040
Again, I don't have kids, so,
sorry for those-- but as far

00:53:26.040 --> 00:53:29.430
as education goes, I think that
should be one of the overriding

00:53:29.430 --> 00:53:31.180
purposes, is to
learn how to think,

00:53:31.180 --> 00:53:33.620
not necessarily rote memory
of all these thoughts.

00:53:33.620 --> 00:53:36.600
And be able to separate the
wheat from the chaff using

00:53:36.600 --> 00:53:38.310
your own brain to
look at the evidence.

00:53:38.310 --> 00:53:40.684
That's the thing that I think
is missing in society right

00:53:40.684 --> 00:53:43.020
now, generally.

00:53:43.020 --> 00:53:45.970
And I think it makes people
susceptible to some highly

00:53:45.970 --> 00:53:48.540
questionable opinions
about things,

00:53:48.540 --> 00:53:50.250
very uninformed
opinions about things.

00:53:50.250 --> 00:53:51.857
Because it shouldn't matter.

00:53:51.857 --> 00:53:52.690
It shouldn't matter.

00:53:52.690 --> 00:53:55.760
If you have the ability to
separate fact from fiction,

00:53:55.760 --> 00:53:57.260
or at least give
it a good shot, you

00:53:57.260 --> 00:54:00.810
should be able to chain
together causation and come

00:54:00.810 --> 00:54:03.970
to a reasonable conclusion as
a member of a civil society.

00:54:03.970 --> 00:54:07.880
And boy, that it sounds
very optimistic, I know.

00:54:07.880 --> 00:54:09.910
But it's happened before.

00:54:09.910 --> 00:54:12.210
And again, I think about
my education, again,

00:54:12.210 --> 00:54:14.930
a public school education.

00:54:14.930 --> 00:54:17.000
I was able to work
alongside people from all

00:54:17.000 --> 00:54:19.780
around the world, my
clients, and my education

00:54:19.780 --> 00:54:22.030
held up to anywhere.

00:54:22.030 --> 00:54:27.560
And that always reassures me,
that it's absolutely possible,

00:54:27.560 --> 00:54:29.700
we just try to instill that
in the next generation.

00:54:29.700 --> 00:54:31.500
It's a big challenge,
I know, particularly

00:54:31.500 --> 00:54:33.630
in a very politicized
environment.

00:54:33.630 --> 00:54:35.140
But that would be it.

00:54:35.140 --> 00:54:38.310
Teaching people how to think
critically and analyze evidence

00:54:38.310 --> 00:54:40.590
will help us get
rid of-- at least

00:54:40.590 --> 00:54:45.415
moderate the influence
of, basically, bullshit.

00:54:45.415 --> 00:54:46.790
MALE SPEAKER:
Daniel told me he's

00:54:46.790 --> 00:54:49.280
happy to stay for a little
bit, sign books for you guys,

00:54:49.280 --> 00:54:52.270
but in the meantime, thank
you so much for coming--

00:54:52.270 --> 00:54:52.380
DANIEL SUAREZ: Oh,
it was my pleasure.

00:54:52.380 --> 00:54:54.463
MALE SPEAKER: --for the
thought-provoking lecture.

00:54:54.463 --> 00:54:55.782
DANIEL SUAREZ: Thank you.

