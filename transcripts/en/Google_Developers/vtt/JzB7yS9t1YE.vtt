WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.892
[MUSIC PLAYING]

00:00:05.892 --> 00:00:08.390
LILY PENG: Hi everybody.

00:00:08.390 --> 00:00:10.120
My name is Lily Peng.

00:00:10.120 --> 00:00:16.250
I'm a physician by training and
I work on the Google medical--

00:00:16.250 --> 00:00:19.810
well, Google AI
health-care team.

00:00:19.810 --> 00:00:21.370
I am a product manager.

00:00:21.370 --> 00:00:25.330
And today we're going to talk to
you about a couple of projects

00:00:25.330 --> 00:00:30.520
that we have been
working on in our group.

00:00:30.520 --> 00:00:33.330
So first off, I think
you'll get a lot of this,

00:00:33.330 --> 00:00:36.590
so I'm not going to
go over this too much.

00:00:36.590 --> 00:00:39.360
But because we
apply deep learning

00:00:39.360 --> 00:00:41.250
to medical information,
I kind of wanted

00:00:41.250 --> 00:00:46.650
to just define a few terms
that get used quite a bit

00:00:46.650 --> 00:00:48.690
but are somewhat poorly defined.

00:00:48.690 --> 00:00:51.570
So first off, artificial
intelligence-- this

00:00:51.570 --> 00:00:55.230
is a pretty broad term and it
encompasses that grand project

00:00:55.230 --> 00:00:58.150
to build a nonhuman
intelligence.

00:00:58.150 --> 00:01:00.180
Machine learning is
a particular type

00:01:00.180 --> 00:01:05.160
of artificial
intelligence, I suppose,

00:01:05.160 --> 00:01:08.950
that teaches machines
to be smarter.

00:01:08.950 --> 00:01:11.080
And deep learning
is a particular type

00:01:11.080 --> 00:01:13.080
of machine learning which
you guys have probably

00:01:13.080 --> 00:01:17.890
heard about quite a bit and will
hear about quite a bit more.

00:01:17.890 --> 00:01:20.830
So first of all, what
is deep learning?

00:01:20.830 --> 00:01:25.360
So it's a modern reincarnation
of artificial neural networks,

00:01:25.360 --> 00:01:28.960
which actually was
invented in the 1960s.

00:01:28.960 --> 00:01:33.840
It's a collection of simple
trainable units, organized

00:01:33.840 --> 00:01:34.830
in layers.

00:01:34.830 --> 00:01:40.020
And they work together to solve
or model complicated tasks.

00:01:40.020 --> 00:01:45.170
So in general, with smaller
data sets and limited compute,

00:01:45.170 --> 00:01:48.780
which is what we had
in the 1980s and '90s,

00:01:48.780 --> 00:01:51.000
other approaches
generally work better.

00:01:51.000 --> 00:01:55.350
But with larger data sets
and larger model sizes

00:01:55.350 --> 00:01:58.770
and more compute power, we
find that neural networks

00:01:58.770 --> 00:02:00.690
work much better.

00:02:00.690 --> 00:02:02.970
So there's actually
just two takeaways

00:02:02.970 --> 00:02:05.820
that I want you guys
to get from this slide.

00:02:05.820 --> 00:02:08.160
One is that deep learning
trains algorithms

00:02:08.160 --> 00:02:11.350
that are very accurate
when given enough data.

00:02:11.350 --> 00:02:14.490
And two, that deep
learning can do this

00:02:14.490 --> 00:02:17.580
without feature engineering.

00:02:17.580 --> 00:02:20.890
And that means without
explicitly writing the rules.

00:02:20.890 --> 00:02:23.410
So what do I mean by that?

00:02:23.410 --> 00:02:25.650
Well in traditional
computer vision,

00:02:25.650 --> 00:02:28.050
we spend a lot of
time writing the rules

00:02:28.050 --> 00:02:32.760
that a machine should follow to
make a certain prediction task.

00:02:32.760 --> 00:02:34.980
In convolutional
neural networks,

00:02:34.980 --> 00:02:37.170
we actually spend very
little time in feature

00:02:37.170 --> 00:02:38.880
engineering and
writing these rules.

00:02:38.880 --> 00:02:42.050
Most of the time we
spend in data preparation

00:02:42.050 --> 00:02:46.800
and numerical optimization
and model architecture.

00:02:46.800 --> 00:02:50.040
So I get this
question quite a bit.

00:02:50.040 --> 00:02:52.980
And the question is, how
much data is enough data

00:02:52.980 --> 00:02:54.750
for a deep neural network?

00:02:54.750 --> 00:02:57.330
Well in general, more is better.

00:02:57.330 --> 00:03:01.350
But there are diminishing
returns beyond a certain point.

00:03:01.350 --> 00:03:03.630
And a general rule
of thumb is that we

00:03:03.630 --> 00:03:08.370
like to have about 5,000
positives per class.

00:03:08.370 --> 00:03:11.730
But the key thing is
good and relevant data--

00:03:11.730 --> 00:03:12.960
so garbage in, garbage out.

00:03:15.480 --> 00:03:18.810
The model will predict very
well what you ask it to predict.

00:03:25.510 --> 00:03:27.710
So when you think about
where machine learning,

00:03:27.710 --> 00:03:31.472
and especially deep learning,
can make the biggest impact,

00:03:31.472 --> 00:03:32.930
it's really in
places where there's

00:03:32.930 --> 00:03:35.630
lots of data to look through.

00:03:35.630 --> 00:03:38.930
One of our directors, Greg
Corrado, puts it best.

00:03:38.930 --> 00:03:43.580
Deep learning is really good for
tasks that you've done 10,000

00:03:43.580 --> 00:03:47.300
times, and on the 10,001st time,
you're just sick of it and you

00:03:47.300 --> 00:03:48.750
don't want to do it anymore.

00:03:48.750 --> 00:03:51.830
So this is really great for
health care in screening

00:03:51.830 --> 00:03:54.560
applications where you
see a lot of patients

00:03:54.560 --> 00:03:57.000
that are potentially normal.

00:03:57.000 --> 00:04:00.410
It's also great where
expertise is limited.

00:04:00.410 --> 00:04:04.880
So here on the right
you see a graph

00:04:04.880 --> 00:04:09.360
of the shortage of
radiologists kind of worldwide.

00:04:09.360 --> 00:04:12.320
And this is also true for
other medical specialties,

00:04:12.320 --> 00:04:14.480
but radiologists
are sort of here.

00:04:14.480 --> 00:04:21.579
And we basically see a worldwide
shortage of medical expertise.

00:04:21.579 --> 00:04:24.610
So one of the
screening applications

00:04:24.610 --> 00:04:30.150
that our group has worked on
is with diabetic retinopathy.

00:04:30.150 --> 00:04:32.230
We call it DR
because it's easier

00:04:32.230 --> 00:04:34.480
to say than diabetic
retinopathy.

00:04:34.480 --> 00:04:37.890
And it's the fastest growing
cause of preventable blindness.

00:04:37.890 --> 00:04:42.010
All 450 million people with
diabetes are at risk and need

00:04:42.010 --> 00:04:43.150
to be screened once a year.

00:04:45.860 --> 00:04:48.440
This is done by taking
a picture of the back

00:04:48.440 --> 00:04:51.170
of the eye with a special
camera, as you see here.

00:04:51.170 --> 00:04:54.420
And the picture looks
a little bit like that.

00:04:54.420 --> 00:04:58.970
And so what a doctor does when
they get an image like this

00:04:58.970 --> 00:05:04.340
is they grade it on a scale of
one to five from no disease,

00:05:04.340 --> 00:05:06.770
so healthy, to
proliferate disease,

00:05:06.770 --> 00:05:09.050
which is the end stage.

00:05:09.050 --> 00:05:12.440
And when they do grading, they
look for sometimes very subtle

00:05:12.440 --> 00:05:15.530
findings, little things
called micro aneurysms

00:05:15.530 --> 00:05:19.010
that are outpouchings in the
blood vessels of the eye.

00:05:19.010 --> 00:05:23.750
And that indicates
how bad your diabetes

00:05:23.750 --> 00:05:27.200
is affecting your vision.

00:05:27.200 --> 00:05:29.850
So unfortunately in
many parts of the world,

00:05:29.850 --> 00:05:33.120
there are just not enough
eye doctors to do this task.

00:05:33.120 --> 00:05:36.635
So with one of our
partners in India,

00:05:36.635 --> 00:05:39.630
or actually a couple of
our partners in India,

00:05:39.630 --> 00:05:44.120
there is a shortage of 127,000
eye doctors in the nation.

00:05:44.120 --> 00:05:47.660
And as a result,
about 45% of patients

00:05:47.660 --> 00:05:52.110
suffer some sort of vision loss
before the disease is detected.

00:05:52.110 --> 00:05:55.460
Now as you recall, I
said that this disease

00:05:55.460 --> 00:05:57.330
was completely preventable.

00:05:57.330 --> 00:06:01.860
So again, this is something
that should not be happening.

00:06:01.860 --> 00:06:04.070
So what we decided to
do was we partnered

00:06:04.070 --> 00:06:06.570
with a couple of
hospitals in India,

00:06:06.570 --> 00:06:09.560
as well as a screening
provider in the US.

00:06:09.560 --> 00:06:15.110
And we got about 130,000 images
for this first go around.

00:06:15.110 --> 00:06:18.590
We hired 54 ophthalmologists
and built a labeling tool.

00:06:18.590 --> 00:06:21.230
And then the 54
ophthalmologists actually

00:06:21.230 --> 00:06:23.270
graded these images
on this scale,

00:06:23.270 --> 00:06:25.550
from no DR to proliferative.

00:06:25.550 --> 00:06:27.650
The interesting thing was
that there was actually

00:06:27.650 --> 00:06:30.770
a little bit of variability in
how doctors call the images.

00:06:30.770 --> 00:06:36.160
And so we actually got about
880,000 diagnoses in all.

00:06:36.160 --> 00:06:40.380
And with this labelled data set,
we put it through a fairly well

00:06:40.380 --> 00:06:42.550
known convolutional neural net.

00:06:42.550 --> 00:06:43.840
This is called Inception.

00:06:43.840 --> 00:06:47.540
I think lot of you guys
may be familiar with it.

00:06:47.540 --> 00:06:52.870
It's generally used to classify
cats and dogs for our photo app

00:06:52.870 --> 00:06:54.910
or for some other search apps.

00:06:54.910 --> 00:06:57.940
And we just repurposed
it to do fundus images.

00:06:57.940 --> 00:07:00.280
So the other thing
that we learned

00:07:00.280 --> 00:07:02.890
while we were
doing this work was

00:07:02.890 --> 00:07:05.020
that while it was
really useful to have

00:07:05.020 --> 00:07:07.930
this five-point
diagnosis, it was also

00:07:07.930 --> 00:07:10.150
incredibly useful
to give doctors

00:07:10.150 --> 00:07:14.170
feedback on housekeeping
predictions like image quality,

00:07:14.170 --> 00:07:16.050
whether this is a
left or right eye,

00:07:16.050 --> 00:07:18.880
or which part of
the retina this is.

00:07:18.880 --> 00:07:21.520
So we added that to
the network as well.

00:07:24.120 --> 00:07:25.945
So how well does it do?

00:07:25.945 --> 00:07:28.820
So this is the first
version of our model

00:07:28.820 --> 00:07:34.730
that we published in a medical
journal in 2016 I believe.

00:07:34.730 --> 00:07:38.930
And right here on
the left is a chart

00:07:38.930 --> 00:07:41.810
of the performance of
the model in aggregate

00:07:41.810 --> 00:07:43.970
over about 10,000 images.

00:07:43.970 --> 00:07:49.310
Sensitivity is on the y-axis,
and then 1 minus specificity

00:07:49.310 --> 00:07:51.050
is on the x-axis.

00:07:51.050 --> 00:07:53.960
So sensitivity is a
percentage of the time when

00:07:53.960 --> 00:07:55.880
a patient has a
disease and you've

00:07:55.880 --> 00:07:59.300
got that right, when the
model was calling the disease.

00:07:59.300 --> 00:08:01.880
And then specificity
is the proportion

00:08:01.880 --> 00:08:04.940
of patients that don't have
the disease that the model

00:08:04.940 --> 00:08:06.800
or the doctor got right.

00:08:06.800 --> 00:08:09.050
And you can see
you want something

00:08:09.050 --> 00:08:11.630
with high sensitivity
and high specificity.

00:08:11.630 --> 00:08:13.400
And so up and to the right--

00:08:13.400 --> 00:08:16.040
or up and to the left is good.

00:08:16.040 --> 00:08:17.870
And you can see
here on the chart

00:08:17.870 --> 00:08:20.060
that the little dots
are the doctors that

00:08:20.060 --> 00:08:22.700
were grading the same set.

00:08:22.700 --> 00:08:25.850
So we get pretty
close to the doctor.

00:08:25.850 --> 00:08:28.760
And these are board-certified
US physicians.

00:08:28.760 --> 00:08:32.299
And these are ophthalmologists,
general ophthalmologists

00:08:32.299 --> 00:08:34.289
by training.

00:08:34.289 --> 00:08:36.230
In fact if you look
at the F score, which

00:08:36.230 --> 00:08:40.669
is a combined measure of both
sensitivity and specificity,

00:08:40.669 --> 00:08:43.340
we're just a little better
than the median ophthalmologist

00:08:43.340 --> 00:08:44.450
in this particular study.

00:08:47.590 --> 00:08:49.830
So since then we've
improved the model.

00:08:49.830 --> 00:08:53.940
So last year about December
2016 we were sort of on par

00:08:53.940 --> 00:08:55.260
with generalists.

00:08:55.260 --> 00:08:57.040
And then this year--

00:08:57.040 --> 00:08:59.160
this is a new paper
that we published--

00:08:59.160 --> 00:09:01.440
we actually used
retinal specialists

00:09:01.440 --> 00:09:03.060
to grade the images.

00:09:03.060 --> 00:09:04.080
So they're specialists.

00:09:04.080 --> 00:09:06.810
We also had them argue
when they disagreed

00:09:06.810 --> 00:09:08.880
about what the diagnosis was.

00:09:08.880 --> 00:09:10.830
And you can see when we
train the model using

00:09:10.830 --> 00:09:14.190
that as the ground truth, the
model predicted that quite well

00:09:14.190 --> 00:09:14.740
as well.

00:09:14.740 --> 00:09:16.740
So this year we're
sort of on par

00:09:16.740 --> 00:09:18.570
with the retina specialists.

00:09:18.570 --> 00:09:20.820
And this weighted
kappa thing is just

00:09:20.820 --> 00:09:22.780
agreement on the
five-class level.

00:09:22.780 --> 00:09:25.080
And you can see that,
essentially, we're

00:09:25.080 --> 00:09:28.200
sort of in between the
ophthalmologists and the retina

00:09:28.200 --> 00:09:29.970
specialists, in fact
kind of in between

00:09:29.970 --> 00:09:31.267
the retinal specialists.

00:09:34.010 --> 00:09:37.040
Another thing that
we've been working on

00:09:37.040 --> 00:09:39.530
beyond improving the
models is actually

00:09:39.530 --> 00:09:42.800
trying to have the
networks explain

00:09:42.800 --> 00:09:44.780
how it's making a prediction.

00:09:44.780 --> 00:09:47.120
So again, taking a
playbook or a play

00:09:47.120 --> 00:09:49.820
out of the playbook
from the consumer world,

00:09:49.820 --> 00:09:53.990
we started using this
technique called show me where.

00:09:53.990 --> 00:09:57.290
And this is where
using an image,

00:09:57.290 --> 00:09:59.990
we actually generate
a heat map of where

00:09:59.990 --> 00:10:03.450
the relevant pixels are for
this particular prediction.

00:10:03.450 --> 00:10:06.560
So here you can see a
picture of a Pomeranian.

00:10:06.560 --> 00:10:08.360
And the heat map
shows you that there

00:10:08.360 --> 00:10:10.400
is something in the
face of the Pomeranian

00:10:10.400 --> 00:10:13.460
that makes it look Pomeranian-y.

00:10:13.460 --> 00:10:19.880
And on the right here, you
kind of have an Afghan hound,

00:10:19.880 --> 00:10:23.570
and the network's
highlighting the Afghan hound.

00:10:23.570 --> 00:10:27.230
So using this very
similar technique,

00:10:27.230 --> 00:10:28.820
we applied it to
the fundus images

00:10:28.820 --> 00:10:31.690
and we said, show me where.

00:10:31.690 --> 00:10:34.960
So this is a case
of mild disease.

00:10:34.960 --> 00:10:37.450
And I can tell it's
mild disease because--

00:10:37.450 --> 00:10:39.776
well, it looks
completely normal to me.

00:10:39.776 --> 00:10:41.650
I can't tell that there
is any disease there.

00:10:41.650 --> 00:10:43.360
But a highly
trained doctor would

00:10:43.360 --> 00:10:46.807
be able to pick out little
thing called microaneurysms

00:10:46.807 --> 00:10:47.890
where the green spots are.

00:10:50.770 --> 00:10:53.660
Here's a picture of
moderate disease.

00:10:53.660 --> 00:10:56.320
And this is a little
worse because you can see

00:10:56.320 --> 00:10:59.860
some bleeding at the ends here.

00:10:59.860 --> 00:11:03.100
And actually I don't
know if I can signal,

00:11:03.100 --> 00:11:04.630
but there's a bleeding there.

00:11:04.630 --> 00:11:06.860
And the heat map--

00:11:06.860 --> 00:11:08.550
so here's a heat map.

00:11:08.550 --> 00:11:10.990
You can see that it
picks up the bleeding.

00:11:10.990 --> 00:11:14.570
But there's two
artifacts in this image.

00:11:14.570 --> 00:11:20.980
So there is a dust spot,
just like a little dark spot.

00:11:20.980 --> 00:11:23.440
And then there is
this little reflection

00:11:23.440 --> 00:11:24.640
in the middle of the image.

00:11:24.640 --> 00:11:26.350
And you could tell
that the model just

00:11:26.350 --> 00:11:30.220
ignores it, essentially.

00:11:30.220 --> 00:11:31.540
So what's next?

00:11:31.540 --> 00:11:32.380
We trained a model.

00:11:32.380 --> 00:11:35.470
We showed that it's
somewhat explainable.

00:11:35.470 --> 00:11:37.330
We think it's doing
the right thing.

00:11:37.330 --> 00:11:38.290
What's next?

00:11:38.290 --> 00:11:42.122
Well, we actually have to deploy
this into health-care systems.

00:11:42.122 --> 00:11:44.080
And we're partnering with
health-care providers

00:11:44.080 --> 00:11:46.150
and companies to bring
this to patients.

00:11:46.150 --> 00:11:50.427
And actually Dr. Jess Mega,
who is going to speak after me,

00:11:50.427 --> 00:11:52.760
is going to have a little
more details about this effort

00:11:52.760 --> 00:11:53.260
there.

00:11:56.270 --> 00:11:58.670
So I've given the
screening application.

00:11:58.670 --> 00:12:01.070
And here's an
application in diagnosis

00:12:01.070 --> 00:12:02.540
that we're working on.

00:12:02.540 --> 00:12:07.850
So in this particular example,
we're talking about a disease--

00:12:07.850 --> 00:12:09.650
well, we're talking
about breast cancer,

00:12:09.650 --> 00:12:11.990
but we're talking about
metastases of breast cancer

00:12:11.990 --> 00:12:14.330
into nearby lymph nodes.

00:12:14.330 --> 00:12:18.170
So when a patient is
diagnosed with breast cancer

00:12:18.170 --> 00:12:22.490
and the primary breast
cancer is removed,

00:12:22.490 --> 00:12:24.260
the surgeon spends
some time taking out

00:12:24.260 --> 00:12:27.770
what we call lymph nodes
so that we can examine

00:12:27.770 --> 00:12:31.670
to see whether or not the
breast cancer has metastasized

00:12:31.670 --> 00:12:32.650
to those nodes.

00:12:32.650 --> 00:12:35.960
And that has an impact on
how you treat the patient.

00:12:35.960 --> 00:12:40.560
So reading these lymph nodes
is actually not an easy task.

00:12:40.560 --> 00:12:45.350
And in fact about in 24% of
biopsies when they went back

00:12:45.350 --> 00:12:49.560
to look at them, the 24% had
a change in nodal status.

00:12:49.560 --> 00:12:52.190
Which means that if it was
positive, it was read negative,

00:12:52.190 --> 00:12:54.674
and it was negative,
read positive.

00:12:54.674 --> 00:12:55.840
So that's a really big deal.

00:12:55.840 --> 00:12:57.932
It's one in four.

00:12:57.932 --> 00:12:59.390
The interesting
thing is that there

00:12:59.390 --> 00:13:01.100
was another study
published that showed

00:13:01.100 --> 00:13:03.630
that a pathologist
with unlimited time,

00:13:03.630 --> 00:13:05.780
not overwhelmed
with data, actually

00:13:05.780 --> 00:13:10.790
is quite sensitive, so
94% sensitivity in finding

00:13:10.790 --> 00:13:12.590
the tumors.

00:13:12.590 --> 00:13:15.310
When you put time
constraint on the patient,

00:13:15.310 --> 00:13:18.020
their sensitivity-- or
sorry, on the provider,

00:13:18.020 --> 00:13:21.260
on the pathologist,
the sensitivity drops.

00:13:21.260 --> 00:13:24.110
And people will
start overlooking

00:13:24.110 --> 00:13:27.590
where little metastases may be.

00:13:27.590 --> 00:13:32.090
So in this picture there's a
tiny metastasis right there.

00:13:32.090 --> 00:13:35.600
And that's usually small things
like this that are missed.

00:13:35.600 --> 00:13:39.050
And this is not surprising
given that so much information

00:13:39.050 --> 00:13:40.560
is in each slide.

00:13:40.560 --> 00:13:42.935
So one of these
slides, if digitized,

00:13:42.935 --> 00:13:44.900
is about 10 gigapixels.

00:13:44.900 --> 00:13:48.470
And that's literally a
needle in a haystack.

00:13:48.470 --> 00:13:51.410
The interesting thing is that
pathologists can actually

00:13:51.410 --> 00:13:55.400
find 73% of the cancers if they
spend all their time looking

00:13:55.400 --> 00:13:58.910
for it with zero false
positives per slide.

00:13:58.910 --> 00:14:02.360
So we trained a model that
can help with this task.

00:14:02.360 --> 00:14:05.390
It actually finds about
95% of the cancer lesions

00:14:05.390 --> 00:14:07.670
and it has eight false
positives per slide.

00:14:07.670 --> 00:14:09.860
So clearly an
ideal system is one

00:14:09.860 --> 00:14:13.940
that is very sensitive using the
model, but also quite specific,

00:14:13.940 --> 00:14:16.310
that relies on the pathologist
to actually look over

00:14:16.310 --> 00:14:21.560
the false positives and
calling them false positives.

00:14:21.560 --> 00:14:23.230
So this is very
promising and we're

00:14:23.230 --> 00:14:26.050
working on validation
in the clinic right now.

00:14:26.050 --> 00:14:29.110
In terms of reader
studies, how this actually

00:14:29.110 --> 00:14:33.100
interacts with the doctor
is really quite important.

00:14:33.100 --> 00:14:35.620
And clearly there are
applications to other tissues.

00:14:35.620 --> 00:14:39.310
I talked about lymph nodes,
but we have some early studies

00:14:39.310 --> 00:14:42.062
that actually show that this
works for prostate cancer,

00:14:42.062 --> 00:14:43.270
as well, for Gleason grading.

00:14:46.940 --> 00:14:49.370
So in the previous
examples we talked

00:14:49.370 --> 00:14:52.610
about how deep learning can
produce the algorithms that

00:14:52.610 --> 00:14:54.500
are very accurate.

00:14:54.500 --> 00:14:58.010
And they tend to make calls that
a doctor might already make.

00:14:58.010 --> 00:15:01.280
But what about predicting things
that doctors don't currently

00:15:01.280 --> 00:15:02.750
do from imaging?

00:15:02.750 --> 00:15:05.039
So as you recall from the
beginning of the talk,

00:15:05.039 --> 00:15:06.830
one of the great things
about deep learning

00:15:06.830 --> 00:15:09.800
is that you can train
very accurate algorithms

00:15:09.800 --> 00:15:13.080
without explicitly
writing rules.

00:15:13.080 --> 00:15:16.590
So this allows us to make
completely new discoveries.

00:15:16.590 --> 00:15:18.992
So the picture on the
left is from a paper

00:15:18.992 --> 00:15:20.450
that we published
recently where we

00:15:20.450 --> 00:15:23.150
trained deep-learning
models to predict a variety

00:15:23.150 --> 00:15:25.150
of cardiovascular risk factors.

00:15:25.150 --> 00:15:27.870
And that includes age,
self-reported sex,

00:15:27.870 --> 00:15:31.430
smoking status, blood pressure,
things that doctors generally

00:15:31.430 --> 00:15:35.270
consider right now to assess the
patient's cardiovascular risk

00:15:35.270 --> 00:15:38.150
and make proper treatment
recommendations.

00:15:38.150 --> 00:15:41.300
So it turns out
that we can not only

00:15:41.300 --> 00:15:44.910
predict many of these
factors, and quite accurately,

00:15:44.910 --> 00:15:47.720
but we can actually directly
predict a five-year risk

00:15:47.720 --> 00:15:49.940
of a cardiac event.

00:15:49.940 --> 00:15:54.200
So this work is quite
early, really pulmonary,

00:15:54.200 --> 00:15:58.460
and the AUC for this
prediction is 0.7.

00:15:58.460 --> 00:16:02.240
What that number is means is
that if given two pictures, one

00:16:02.240 --> 00:16:06.080
picture of a patient that did
not have a cardiovascular event

00:16:06.080 --> 00:16:10.820
and one picture of a patient
who did, it is right about 70%

00:16:10.820 --> 00:16:12.020
of the time.

00:16:12.020 --> 00:16:14.872
Most doctors is
around 50% of time,

00:16:14.872 --> 00:16:16.580
because it's kind of
a random-- like it's

00:16:16.580 --> 00:16:20.270
hard to do based on a
retinal image alone.

00:16:20.270 --> 00:16:22.640
So why is this exciting?

00:16:22.640 --> 00:16:24.500
Well normally when
a doctor tries

00:16:24.500 --> 00:16:29.400
to assess your risk for
cardiovascular disease,

00:16:29.400 --> 00:16:30.770
there are needles involved.

00:16:30.770 --> 00:16:34.130
So I don't know if anyone
has gotten blood cholesterol

00:16:34.130 --> 00:16:34.970
screening.

00:16:34.970 --> 00:16:38.540
You fast the night before and
then we take some blood samples

00:16:38.540 --> 00:16:40.710
and then we assess your risk.

00:16:40.710 --> 00:16:44.690
So again, I want to emphasize
that this is really early on.

00:16:44.690 --> 00:16:47.690
But these results
support the idea

00:16:47.690 --> 00:16:49.220
that we may be able
to use something

00:16:49.220 --> 00:16:51.830
like an image to make new
predictions that we couldn't

00:16:51.830 --> 00:16:53.300
make before.

00:16:53.300 --> 00:16:55.400
And this might be able
to be done in sort

00:16:55.400 --> 00:16:56.430
of a noninvasive manner.

00:16:59.500 --> 00:17:03.970
So I've given a few
examples, three examples

00:17:03.970 --> 00:17:07.690
of how deep learning can really
increase both availability

00:17:07.690 --> 00:17:10.329
and accuracy in health care.

00:17:10.329 --> 00:17:13.329
And one of the things that
I want to kind of also

00:17:13.329 --> 00:17:18.579
acknowledge here is the
reason why this has become

00:17:18.579 --> 00:17:22.150
more and more exciting is,
I think, because TensorFlow

00:17:22.150 --> 00:17:23.109
is open source.

00:17:23.109 --> 00:17:26.470
So this kind of open standard
from general machine learning

00:17:26.470 --> 00:17:27.730
is being applied everywhere.

00:17:27.730 --> 00:17:30.310
So I've given examples of work
that we've done at Google,

00:17:30.310 --> 00:17:33.580
but there's a lot of work that's
being done across the community

00:17:33.580 --> 00:17:37.430
at other medical centers
that are very similar.

00:17:37.430 --> 00:17:40.810
And so we're really
excited about what

00:17:40.810 --> 00:17:45.340
this technology can bring
to the field of health care.

00:17:45.340 --> 00:17:48.250
And with that, I'd like
to introduce Jess Mega.

00:17:48.250 --> 00:17:51.144
Unlike me, she is a real doctor.

00:17:51.144 --> 00:17:53.060
And she's the chief
medical officer at Verily.

00:17:59.612 --> 00:18:02.340
JESSICA MEGA: Well thank
you all for being here.

00:18:02.340 --> 00:18:04.740
And thank you Lily
for kicking us off.

00:18:04.740 --> 00:18:08.970
I think the excitement
around AI and health care

00:18:08.970 --> 00:18:11.640
could not be greater.

00:18:11.640 --> 00:18:13.590
As you heard, my
name is Jess Mega.

00:18:13.590 --> 00:18:16.560
I'm a cardiologist and
am so excited to be

00:18:16.560 --> 00:18:19.030
part of the Alphabet family.

00:18:19.030 --> 00:18:22.500
Verily grew out of
Google and Google X.

00:18:22.500 --> 00:18:27.000
And we are focused solely on
health care and life sciences.

00:18:27.000 --> 00:18:30.330
And our mission is to take
the world's health information

00:18:30.330 --> 00:18:35.250
and make it useful so that
patients live healthier lives.

00:18:35.250 --> 00:18:39.720
And the example that I'll talk
about today focuses on diabetes

00:18:39.720 --> 00:18:44.490
and really lends itself to the
conversation that Lily started.

00:18:44.490 --> 00:18:46.680
But I think it's very
important to pause

00:18:46.680 --> 00:18:49.020
and think about
health data broadly.

00:18:49.020 --> 00:18:52.650
Right now, any individual
who's in the audience today

00:18:52.650 --> 00:18:56.360
has about several
gigabytes of health data.

00:18:56.360 --> 00:18:58.110
But if you think about
health in the years

00:18:58.110 --> 00:19:00.510
to come and think
about genomics,

00:19:00.510 --> 00:19:04.530
molecular technologies,
imaging, sensor data,

00:19:04.530 --> 00:19:07.020
patient-reported data,
electronic health records

00:19:07.020 --> 00:19:09.900
and claims, we're
talking about huge sums

00:19:09.900 --> 00:19:11.670
of data, gigabytes of data.

00:19:11.670 --> 00:19:13.230
And at Verily and
at Alphabet, we're

00:19:13.230 --> 00:19:15.790
committed to stay ahead of this
so that we can help patients.

00:19:18.570 --> 00:19:21.950
The reason we're focusing
initially some of our efforts

00:19:21.950 --> 00:19:25.760
on diabetes is this is
an urgent health issue.

00:19:25.760 --> 00:19:29.150
About 1 in 10
people has diabetes.

00:19:29.150 --> 00:19:31.100
And when you have
diabetes, it affects

00:19:31.100 --> 00:19:34.952
how you handle sugar
glucose in the body.

00:19:34.952 --> 00:19:36.410
And if you think
about prediabetes,

00:19:36.410 --> 00:19:38.840
the condition before
someone has diabetes,

00:19:38.840 --> 00:19:40.610
that's one in three people.

00:19:40.610 --> 00:19:45.510
That would be the entire center
section of the audience today.

00:19:45.510 --> 00:19:47.670
Now what happens when
your body handles

00:19:47.670 --> 00:19:51.150
glucose in a different way, you
can have downstream effects.

00:19:51.150 --> 00:19:54.090
You heard Lilly talk about
diabetic retinopathy.

00:19:54.090 --> 00:19:57.180
People can have problems
with their heart, kidneys,

00:19:57.180 --> 00:19:59.100
and peripheral neuropathy.

00:19:59.100 --> 00:20:02.640
So this is the type of disease
that we need to get ahead of.

00:20:02.640 --> 00:20:06.780
But we have two main issues
that we're trying to address.

00:20:06.780 --> 00:20:09.660
The first one is
an information gap.

00:20:09.660 --> 00:20:13.290
So even the most adherent
patients with diabetes--

00:20:13.290 --> 00:20:15.360
and my grandfather
was one of these--

00:20:15.360 --> 00:20:18.770
would check his blood
sugar four times a day.

00:20:18.770 --> 00:20:20.929
And I don't know if
anyone today has been

00:20:20.929 --> 00:20:22.220
able to have any of the snacks.

00:20:22.220 --> 00:20:24.320
I actually had some of
the caramel popcorn.

00:20:24.320 --> 00:20:25.890
Did anyone have any of that?

00:20:25.890 --> 00:20:27.740
Yeah, that was great,
right, except probably

00:20:27.740 --> 00:20:31.002
our biology and our glucose
is going up and down.

00:20:31.002 --> 00:20:32.960
So if I didn't check my
glucose in that moment,

00:20:32.960 --> 00:20:35.510
we wouldn't have
captured that data.

00:20:35.510 --> 00:20:38.540
So we know biology is
happening all of the time.

00:20:38.540 --> 00:20:41.480
When I see patients in the
hospital as a cardiologist,

00:20:41.480 --> 00:20:43.970
I can see someone's heart
rate, their blood pressure, all

00:20:43.970 --> 00:20:46.530
of these vital
signs in real time.

00:20:46.530 --> 00:20:50.070
And then people go home, but
biology is still happening.

00:20:50.070 --> 00:20:54.200
So there's an information
gap, especially with diabetes.

00:20:54.200 --> 00:20:56.310
The second issue
is a decision gap.

00:20:56.310 --> 00:21:00.140
You may see a care provider
once a year, twice a year,

00:21:00.140 --> 00:21:03.390
but health decisions are
happening every single day.

00:21:03.390 --> 00:21:06.050
They're happening
weekly, daily, hourly.

00:21:06.050 --> 00:21:10.010
And how do we decide
to close this gap?

00:21:10.010 --> 00:21:11.990
At Verily we're focusing
on three key missions.

00:21:11.990 --> 00:21:15.986
And this can be true for almost
every project we take on.

00:21:15.986 --> 00:21:17.360
We're thinking
about how to shift

00:21:17.360 --> 00:21:21.380
from episodic and reactive care
to much more proactive care.

00:21:21.380 --> 00:21:23.570
And in order to do that
and to get to the point

00:21:23.570 --> 00:21:25.820
where we can really use
the power of that AI,

00:21:25.820 --> 00:21:27.850
we have to do three things.

00:21:27.850 --> 00:21:30.550
We have to think about
collecting the right data.

00:21:30.550 --> 00:21:34.180
And today I'll be talking about
continuous glucose monitoring.

00:21:34.180 --> 00:21:37.420
How do you then organize this
data so that it's in a format

00:21:37.420 --> 00:21:41.470
that we can unlock and activate
and truly help patients?

00:21:41.470 --> 00:21:43.720
So whether we do this
in the field of diabetes

00:21:43.720 --> 00:21:46.330
that you'll hear about today
or with our surgical robots,

00:21:46.330 --> 00:21:47.515
this is the general premise.

00:21:50.040 --> 00:21:53.190
The first thing to think about
is the collection of data.

00:21:53.190 --> 00:21:55.590
And you heard Lily say
garbage in, garbage out.

00:21:55.590 --> 00:21:58.590
We can't look for insights
unless we understand

00:21:58.590 --> 00:22:00.000
what we're looking at.

00:22:00.000 --> 00:22:02.820
And one thing that has been
absolutely revolutionary

00:22:02.820 --> 00:22:06.240
is thinking about extremely
small biocompatible

00:22:06.240 --> 00:22:07.620
electronics.

00:22:07.620 --> 00:22:09.930
So we are working on
next-generation sensing.

00:22:09.930 --> 00:22:12.680
And you can see a
demonstration here.

00:22:12.680 --> 00:22:14.360
What this will lead
to, for example,

00:22:14.360 --> 00:22:17.270
with extremely small continuous
glucose monitors where we're

00:22:17.270 --> 00:22:19.820
partnering to create
some of these tools,

00:22:19.820 --> 00:22:21.950
this will lead to
more-seamless integration.

00:22:21.950 --> 00:22:25.160
So again, you don't just
have a few glucose values,

00:22:25.160 --> 00:22:27.230
but we understand how
your body is handling

00:22:27.230 --> 00:22:28.970
sugar, or someone
with type 2 diabetes,

00:22:28.970 --> 00:22:31.400
in a more continuous fashion.

00:22:31.400 --> 00:22:34.250
It also helps us
understand not only

00:22:34.250 --> 00:22:35.930
what happens at a
population level

00:22:35.930 --> 00:22:38.870
but what might happen
on an individual level

00:22:38.870 --> 00:22:41.090
when you are ingesting
certain foods.

00:22:41.090 --> 00:22:44.270
And the final thing is to really
try to reduce costs of devices

00:22:44.270 --> 00:22:48.550
so that we can really
democratize health.

00:22:48.550 --> 00:22:51.370
The next aim is, how do we
organize all of this data?

00:22:51.370 --> 00:22:54.660
And I can speak both as a
patient and as a physician.

00:22:54.660 --> 00:22:57.480
The thing that people will
say is, data's amazing,

00:22:57.480 --> 00:23:01.170
but please don't overwhelm
us with a tsunami of data.

00:23:01.170 --> 00:23:03.270
You need to organize it.

00:23:03.270 --> 00:23:05.250
And so we've
partnered with Sanofi

00:23:05.250 --> 00:23:07.680
on a company called Onduo.

00:23:07.680 --> 00:23:09.360
And the idea is
to put the patient

00:23:09.360 --> 00:23:11.490
in the center of
their care and help

00:23:11.490 --> 00:23:13.410
simplify diabetes management.

00:23:13.410 --> 00:23:15.780
This really gets to
the heart of someone

00:23:15.780 --> 00:23:18.670
who is going to be
happier and healthier.

00:23:18.670 --> 00:23:20.710
So what does it actually mean?

00:23:20.710 --> 00:23:22.270
What we try to do
is empower people

00:23:22.270 --> 00:23:23.600
with their glucose control.

00:23:23.600 --> 00:23:26.260
So we turned to the American
Diabetes Association

00:23:26.260 --> 00:23:29.560
and look at the glucose
ranges that are recommended.

00:23:29.560 --> 00:23:31.390
People then get a
graph that shows you

00:23:31.390 --> 00:23:33.760
what your day looks like
and the percentage of time

00:23:33.760 --> 00:23:35.050
that you are in range--

00:23:35.050 --> 00:23:37.360
again, giving a
patient or a user

00:23:37.360 --> 00:23:40.280
that data so they can be the
center of their decisions--

00:23:40.280 --> 00:23:44.540
and then finally tracking
steps through Google Fit.

00:23:44.540 --> 00:23:46.820
The next goal then is
to try to understand

00:23:46.820 --> 00:23:50.810
how glucose is pairing with
your activity and your diet.

00:23:50.810 --> 00:23:52.430
So here there's an
app that prompts

00:23:52.430 --> 00:23:54.230
for the photo of the food.

00:23:54.230 --> 00:23:57.770
And then using image recognition
and using Google's TensorFlow,

00:23:57.770 --> 00:24:00.110
we can identify the food.

00:24:00.110 --> 00:24:02.210
And this is where the
true personal insights

00:24:02.210 --> 00:24:04.040
start to become real.

00:24:04.040 --> 00:24:05.960
Because if you eat
a certain meal,

00:24:05.960 --> 00:24:08.510
it's helpful to understand
how your body ends up

00:24:08.510 --> 00:24:09.920
relating to it.

00:24:09.920 --> 00:24:12.140
And there's some really
interesting preliminary data

00:24:12.140 --> 00:24:14.660
suggesting that the
microbiome may change

00:24:14.660 --> 00:24:17.330
the way I responded to
a banana, for example,

00:24:17.330 --> 00:24:18.960
or you might respond.

00:24:18.960 --> 00:24:20.720
And that's important
to know because all

00:24:20.720 --> 00:24:23.840
of a sudden those general
recommendations that we make

00:24:23.840 --> 00:24:26.570
as a doc-- so if someone
comes to see me in clinic

00:24:26.570 --> 00:24:29.580
and they have type 2
diabetes I might say, OK,

00:24:29.580 --> 00:24:31.200
here are the things
you need to do.

00:24:31.200 --> 00:24:33.200
You need to watch
your diet, exercise,

00:24:33.200 --> 00:24:34.490
take your oral medications.

00:24:34.490 --> 00:24:36.287
I need you to also
take insulin, exercise.

00:24:36.287 --> 00:24:38.120
You've got to see your
foot doctor, your eye

00:24:38.120 --> 00:24:39.620
doctor, your
primary-care doctor,

00:24:39.620 --> 00:24:40.640
and the endocrinologist.

00:24:40.640 --> 00:24:43.190
And that's a lot to integrate.

00:24:43.190 --> 00:24:45.530
And so what we try
to do is also pair

00:24:45.530 --> 00:24:49.220
all of this information in a
simple way with a care lead.

00:24:49.220 --> 00:24:52.280
This is a person that helps
someone on their journey

00:24:52.280 --> 00:24:55.470
as this information is surfaced.

00:24:55.470 --> 00:24:58.050
And if you look in the middle
of what I'm showing you here

00:24:58.050 --> 00:25:00.300
on what the care lead and
what the person is seeing,

00:25:00.300 --> 00:25:02.610
you'll see a number
of different lines.

00:25:02.610 --> 00:25:06.200
And I want us to drill
down and look into that.

00:25:06.200 --> 00:25:08.660
This is showing you the
difference between the data

00:25:08.660 --> 00:25:13.190
you might see in an
episodic glucose example

00:25:13.190 --> 00:25:16.100
or what you're seeing with
the continuous glucose monitor

00:25:16.100 --> 00:25:19.220
enabled by this new sensing.

00:25:19.220 --> 00:25:21.770
And so let's say we drill down
into this continuous glucose

00:25:21.770 --> 00:25:25.220
monitor and we look
at a cluster of days.

00:25:25.220 --> 00:25:27.140
This is an example.

00:25:27.140 --> 00:25:29.570
We might start to see patterns.

00:25:29.570 --> 00:25:31.820
And as Lily mentioned, this
is not the type of thing

00:25:31.820 --> 00:25:35.540
that an individual patient, care
lead, or physician would end up

00:25:35.540 --> 00:25:36.980
digging through,
but this is where

00:25:36.980 --> 00:25:41.080
you start to unlock the
power of learning models.

00:25:41.080 --> 00:25:45.740
Because what we can
start to see is a cluster

00:25:45.740 --> 00:25:48.650
of different mornings.

00:25:48.650 --> 00:25:50.750
We'll make a
positive association

00:25:50.750 --> 00:25:53.450
that everyone's eating
incredibly healthy here

00:25:53.450 --> 00:25:57.410
at Google I/O, so maybe that's
a cluster of the red mornings.

00:25:57.410 --> 00:26:00.290
But we go back into our regular
lives and we get stressed

00:26:00.290 --> 00:26:03.200
and we're eating a
different cluster of foods.

00:26:03.200 --> 00:26:05.840
But instead of, again,
giving general advice,

00:26:05.840 --> 00:26:08.570
we can use different
models to point out,

00:26:08.570 --> 00:26:11.150
it seems like
something is going on.

00:26:11.150 --> 00:26:13.700
With one patient,
for example, we

00:26:13.700 --> 00:26:16.240
were seeing a cluster
around Wednesdays.

00:26:16.240 --> 00:26:18.860
So what's going
on on Wednesdays?

00:26:18.860 --> 00:26:21.170
Is it that the person
is going and stopping

00:26:21.170 --> 00:26:23.630
by a particular location,
or maybe there's

00:26:23.630 --> 00:26:25.220
a lot of stress that day.

00:26:25.220 --> 00:26:27.500
But again, instead of
giving general care,

00:26:27.500 --> 00:26:31.040
we can start to target care
in the most comprehensive

00:26:31.040 --> 00:26:33.840
and actionable example.

00:26:33.840 --> 00:26:35.990
So again, thinking about
what we're talking about,

00:26:35.990 --> 00:26:40.070
collecting data, organizing
it, and then activating it

00:26:40.070 --> 00:26:43.210
and making it
extremely relevant.

00:26:43.210 --> 00:26:46.190
So that is the way we're
thinking about diabetes care,

00:26:46.190 --> 00:26:48.880
and that is the way
AI is going to work.

00:26:48.880 --> 00:26:51.239
We heard this morning
in another discussion,

00:26:51.239 --> 00:26:52.780
we've got to think
about the problems

00:26:52.780 --> 00:26:56.140
that we're going to solve and
use these tools to really make

00:26:56.140 --> 00:26:58.480
a difference.

00:26:58.480 --> 00:27:00.960
So what are some other
ways that we can think

00:27:00.960 --> 00:27:03.930
about activating information?

00:27:03.930 --> 00:27:07.410
And we heard from Lily
that diabetic retinopathy

00:27:07.410 --> 00:27:11.140
is one of the leading
causes of blindness.

00:27:11.140 --> 00:27:14.740
So even if we have
excellent glucose care,

00:27:14.740 --> 00:27:18.060
there may be times where you
start to have end organ damage.

00:27:18.060 --> 00:27:20.070
And I had mentioned
that elevated glucose

00:27:20.070 --> 00:27:25.240
levels can end up affecting
the fundus and the retina.

00:27:25.240 --> 00:27:28.590
Now we know that
people with diabetes

00:27:28.590 --> 00:27:31.290
should undergo screening.

00:27:31.290 --> 00:27:33.120
But earlier in the
talk I gave you

00:27:33.120 --> 00:27:35.640
the laundry list of what
we're asking patients

00:27:35.640 --> 00:27:38.070
to do who have diabetes.

00:27:38.070 --> 00:27:40.770
And so what we're trying to
do with this collaboration

00:27:40.770 --> 00:27:43.230
with Google is figure
out, how do we actually

00:27:43.230 --> 00:27:45.000
get ahead of the
product and think

00:27:45.000 --> 00:27:48.360
about an end-to-end
solution so that we realize

00:27:48.360 --> 00:27:51.870
and bring down the
challenges that exist today.

00:27:51.870 --> 00:27:55.680
Because the issue, in terms
of getting screened, one of it

00:27:55.680 --> 00:27:57.630
is accessibility,
and the other one

00:27:57.630 --> 00:28:01.200
is having access to optometrists
and ophthalmologists.

00:28:01.200 --> 00:28:03.510
And this is a problem
in the United States

00:28:03.510 --> 00:28:05.490
as well as in developing worlds.

00:28:05.490 --> 00:28:08.080
So this is a problem,
not something just local.

00:28:08.080 --> 00:28:11.040
This is something that we
think very globally about when

00:28:11.040 --> 00:28:14.310
we think about the solution.

00:28:14.310 --> 00:28:17.580
We looked at this data
earlier and this idea

00:28:17.580 --> 00:28:22.170
that we can take algorithms and
increase both the sensitivity

00:28:22.170 --> 00:28:25.860
and specificity of diagnosing
diabetic retinopathy

00:28:25.860 --> 00:28:27.640
and macular edema.

00:28:27.640 --> 00:28:29.610
And this is data that
was published in "JAMA"

00:28:29.610 --> 00:28:31.650
as Lily nicely outlined.

00:28:31.650 --> 00:28:33.540
The question then
is, how do we think

00:28:33.540 --> 00:28:35.040
about creating this product?

00:28:35.040 --> 00:28:37.920
Because the beauty of working
at places like Alphabet

00:28:37.920 --> 00:28:40.560
and working with partners
like you all here today is we

00:28:40.560 --> 00:28:42.630
can think about, what
problem are we solving,

00:28:42.630 --> 00:28:44.165
create the algorithms.

00:28:44.165 --> 00:28:46.290
But we then need to step
back and say, what does it

00:28:46.290 --> 00:28:48.630
mean to operate in the
space of health care

00:28:48.630 --> 00:28:50.640
and in the space
of life science?

00:28:50.640 --> 00:28:53.700
We need to think about the image
acquisition, the algorithm,

00:28:53.700 --> 00:28:55.440
and then delivering
that information

00:28:55.440 --> 00:28:57.780
both to physicians
as well as patients.

00:28:57.780 --> 00:29:01.110
So what we're doing is
taking this information

00:29:01.110 --> 00:29:03.900
and now working with
some of our partners.

00:29:03.900 --> 00:29:07.860
There's a promising pilot that's
currently ongoing both here as

00:29:07.860 --> 00:29:10.710
well as in India, and
we're so encouraged to hear

00:29:10.710 --> 00:29:12.115
the early feedback.

00:29:12.115 --> 00:29:13.740
And there are two
pieces of information

00:29:13.740 --> 00:29:15.480
I wanted to share with you.

00:29:15.480 --> 00:29:18.690
One is that looking at
this early observations,

00:29:18.690 --> 00:29:22.230
we're seeing higher
accuracy with AI

00:29:22.230 --> 00:29:24.360
than with a manual greater.

00:29:24.360 --> 00:29:27.097
And the thing that's
important as a physician--

00:29:27.097 --> 00:29:29.430
I don't know if there are any
other doctors in the room,

00:29:29.430 --> 00:29:31.305
but the piece I always
tell people is there's

00:29:31.305 --> 00:29:34.020
going to be room for
health-care providers.

00:29:34.020 --> 00:29:37.870
What these tools are doing is
merely helping us do our job.

00:29:37.870 --> 00:29:41.190
So sometimes people ask
me, is technology and AI

00:29:41.190 --> 00:29:44.910
going to replace physicians or
replace the health-care system?

00:29:44.910 --> 00:29:47.550
And the way I think about it
is, it just augments the work

00:29:47.550 --> 00:29:48.520
we do.

00:29:48.520 --> 00:29:50.020
If you think about
the stethoscope--

00:29:50.020 --> 00:29:52.560
so I'm a cardiologist,
and the stethoscope

00:29:52.560 --> 00:29:55.140
was invented about
200 years ago.

00:29:55.140 --> 00:29:56.790
It doesn't replace
the work we do.

00:29:56.790 --> 00:29:58.710
It merely augments
the work we do.

00:29:58.710 --> 00:30:01.830
And I think you're going to see
a similar theme as we continue

00:30:01.830 --> 00:30:04.320
to think about ways of
bringing care in a more

00:30:04.320 --> 00:30:06.280
effective way to patients.

00:30:06.280 --> 00:30:09.060
So the first thing here is that
the AI was performing better

00:30:09.060 --> 00:30:10.999
than the manual grader.

00:30:10.999 --> 00:30:12.540
And then the second
thing is to think

00:30:12.540 --> 00:30:14.290
about that base of patients.

00:30:14.290 --> 00:30:16.920
How do we truly
democratize care?

00:30:16.920 --> 00:30:19.800
And so the other encouraging
piece from the pilot

00:30:19.800 --> 00:30:21.810
was this idea that
we could start

00:30:21.810 --> 00:30:26.400
to increase the base of patients
treated with the algorithm.

00:30:26.400 --> 00:30:28.590
Now as it turns
out, I would love

00:30:28.590 --> 00:30:30.920
to say that it's really easy
to do everything in health

00:30:30.920 --> 00:30:31.836
care and life science.

00:30:31.836 --> 00:30:34.950
But as it turns out,
it takes a huge village

00:30:34.950 --> 00:30:37.270
to do this kind of work.

00:30:37.270 --> 00:30:38.310
So what's next?

00:30:38.310 --> 00:30:41.220
What is on the path
to clinical adoption?

00:30:41.220 --> 00:30:43.260
And this is what makes
it incredibly exciting

00:30:43.260 --> 00:30:46.830
to be a doctor working with
so many talented technologists

00:30:46.830 --> 00:30:48.250
and engineers.

00:30:48.250 --> 00:30:51.540
We need to now partner with
different clinical sites

00:30:51.540 --> 00:30:53.160
that I noted here.

00:30:53.160 --> 00:30:56.020
We also partner
deeply with the FDA,

00:30:56.020 --> 00:30:59.770
as well as regulatory
agencies in Europe and beyond.

00:30:59.770 --> 00:31:02.160
And one thing at Verily
that we've decided to do

00:31:02.160 --> 00:31:05.220
is to be part of what's called
the FDA precertification

00:31:05.220 --> 00:31:06.330
program.

00:31:06.330 --> 00:31:09.780
We know that bringing new
technologies and new algorithms

00:31:09.780 --> 00:31:11.579
into health care is
critical, but we now

00:31:11.579 --> 00:31:13.620
need to figure out how to
do that in a way that's

00:31:13.620 --> 00:31:14.940
both safe and effective.

00:31:14.940 --> 00:31:17.010
And I'm proud of us
at Alphabet for really

00:31:17.010 --> 00:31:18.900
staying ahead of
that and partnering

00:31:18.900 --> 00:31:21.212
with groups like the FDA.

00:31:21.212 --> 00:31:22.920
The second thing that's
important to note

00:31:22.920 --> 00:31:25.230
is that we partner
deeply at Verily

00:31:25.230 --> 00:31:29.400
with Google as well as other
partners like Nikon and Optus.

00:31:29.400 --> 00:31:31.230
All of these pieces
come together

00:31:31.230 --> 00:31:33.810
to try to transform care.

00:31:33.810 --> 00:31:35.730
But I know that if
we do this correctly,

00:31:35.730 --> 00:31:39.720
there's a huge opportunity not
only in diabetes but really

00:31:39.720 --> 00:31:42.654
in this entire world
of health information.

00:31:42.654 --> 00:31:44.070
It's interesting
to think about it

00:31:44.070 --> 00:31:46.740
as a physician who spends
most of my time taking care

00:31:46.740 --> 00:31:48.810
of patients in the
hospital, how can we

00:31:48.810 --> 00:31:51.150
start to push more
of the access to care

00:31:51.150 --> 00:31:53.100
outside of the hospital?

00:31:53.100 --> 00:31:55.800
But I know that
if we do this well

00:31:55.800 --> 00:31:58.890
and if we stay ahead of
it, we can close this gap.

00:31:58.890 --> 00:32:02.460
We can figure out ways to
become more preventative.

00:32:02.460 --> 00:32:04.750
We can collect the
right information.

00:32:04.750 --> 00:32:06.790
We can create the
infrastructure to organize it.

00:32:06.790 --> 00:32:10.325
And most importantly, we will
figure out how to activate it.

00:32:10.325 --> 00:32:11.700
But I want everyone
to know here,

00:32:11.700 --> 00:32:14.280
this is not the type of
work that we can do alone.

00:32:14.280 --> 00:32:16.650
It really takes
all of us together.

00:32:16.650 --> 00:32:18.889
And we at Verily, we at
Google, and we at Alphabet

00:32:18.889 --> 00:32:20.680
look forward to partnering
with all of you.

00:32:20.680 --> 00:32:23.280
So please help us
on this journey.

00:32:23.280 --> 00:32:25.350
Lily and I will be
here after these talks.

00:32:25.350 --> 00:32:26.850
We're happy to chat
with all of you.

00:32:26.850 --> 00:32:28.646
And thank you for
spending time at I/O.

00:32:28.646 --> 00:32:31.788
[MUSIC PLAYING]

