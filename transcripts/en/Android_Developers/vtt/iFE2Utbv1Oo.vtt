WEBVTT
Kind: captions
Language: en

00:00:05.974 --> 00:00:07.140
DAVID SEHR: Hello, everyone.

00:00:07.140 --> 00:00:08.020
I'm David Sehr.

00:00:08.020 --> 00:00:10.810
I'm the lead of the Mountain
View Android Runtime Team.

00:00:10.810 --> 00:00:13.670
We're the folks that build the
code that runs on your phone

00:00:13.670 --> 00:00:16.640
and loads your applications,
runs them, and manages

00:00:16.640 --> 00:00:17.560
their memory.

00:00:23.610 --> 00:00:26.810
So last year our team
presented the evolution of ART,

00:00:26.810 --> 00:00:28.490
and we described some
of the techniques

00:00:28.490 --> 00:00:30.656
we were using in the runtime,
and how you can really

00:00:30.656 --> 00:00:33.380
see the goodness that we bring.

00:00:33.380 --> 00:00:37.029
So last year we talked about
Profile Guided Compilation,

00:00:37.029 --> 00:00:38.820
how you can make your
app experience better

00:00:38.820 --> 00:00:41.390
by understanding how
programs run on your phone.

00:00:41.390 --> 00:00:45.620
We talked about how important
it is to keep memory usage down

00:00:45.620 --> 00:00:47.700
and to make memory
allocation blindingly fast.

00:00:50.910 --> 00:00:53.550
And we talked about
just-in-time compilation,

00:00:53.550 --> 00:00:55.050
and how we can get
great performance

00:00:55.050 --> 00:00:59.390
from your applications without
the annoying optimizing dialog.

00:00:59.390 --> 00:01:01.940
So you heard last year
about how great things are,

00:01:01.940 --> 00:01:05.000
but it was just the beginning.

00:01:05.000 --> 00:01:06.500
Now that your phone
knows more about

00:01:06.500 --> 00:01:08.600
how your applications
execute, we

00:01:08.600 --> 00:01:11.745
can focus on loading only the
parts that are important to you

00:01:11.745 --> 00:01:16.760
so apps load faster
and use less memory.

00:01:16.760 --> 00:01:18.540
And speaking of
less memory, we made

00:01:18.540 --> 00:01:21.110
your applications spend
less time reclaiming,

00:01:21.110 --> 00:01:24.670
and even less time
allocating memory.

00:01:24.670 --> 00:01:26.750
And of course with the
just-in-time compiler,

00:01:26.750 --> 00:01:29.810
we can focus on making your
applications run even faster.

00:01:29.810 --> 00:01:31.480
Nicolas and Aart will
be talking to you

00:01:31.480 --> 00:01:32.730
in just a moment about that.

00:01:36.810 --> 00:01:40.210
You'll notice I mentioned
using less memory twice.

00:01:40.210 --> 00:01:42.064
So let's start by
talking about that.

00:01:42.064 --> 00:01:44.230
I'm going to begin by talking
a little bit about how

00:01:44.230 --> 00:01:46.830
we rearrange your application
to use less memory.

00:01:46.830 --> 00:01:48.560
And then Mathieu is
going to tell you

00:01:48.560 --> 00:01:53.680
about how we improved our
heap sizes, reduced the jank,

00:01:53.680 --> 00:01:57.320
reduced the pause times, and
have even better allocation

00:01:57.320 --> 00:01:59.970
times.

00:01:59.970 --> 00:02:02.410
First, a little review of
how Android applications

00:02:02.410 --> 00:02:04.930
work on your Android phone.

00:02:04.930 --> 00:02:07.280
An application comes
in an APK file, which

00:02:07.280 --> 00:02:09.430
contains one or more dex files.

00:02:09.430 --> 00:02:13.900
These contain the instructions
for executing your application.

00:02:13.900 --> 00:02:16.930
ART uses the dex file to get
information about instructions

00:02:16.930 --> 00:02:20.680
to run, strings, class
relationships, and lots

00:02:20.680 --> 00:02:21.825
of other things.

00:02:21.825 --> 00:02:25.340
And the dex files are
loaded into memory by ART.

00:02:25.340 --> 00:02:28.750
Loading these files has both
a RAM and a startup time cost.

00:02:31.400 --> 00:02:33.920
So when you run,
for instance, Maps,

00:02:33.920 --> 00:02:35.680
one of our favorite
applications,

00:02:35.680 --> 00:02:37.880
your phone reads the
dex files into memory,

00:02:37.880 --> 00:02:40.736
and you can see amounts
of thousands of pages.

00:02:40.736 --> 00:02:42.360
Multiple megabytes
worth of information

00:02:42.360 --> 00:02:44.472
are loaded into memory.

00:02:44.472 --> 00:02:46.430
Now the dex files were
produced by a developer,

00:02:46.430 --> 00:02:48.910
and the typical developer
tools don't really

00:02:48.910 --> 00:02:51.430
know about the use cases
that are going to be

00:02:51.430 --> 00:02:53.540
important to you on your phone.

00:02:53.540 --> 00:02:55.840
So the dex files may
have unimportant things

00:02:55.840 --> 00:03:03.065
on the same pages as important
things like, for example,

00:03:03.065 --> 00:03:03.940
the way you use Maps.

00:03:03.940 --> 00:03:06.370
You may only use some of the
methods that were in the dex

00:03:06.370 --> 00:03:12.610
files, or some of the strings
that are there are not

00:03:12.610 --> 00:03:15.241
seen in your use case.

00:03:15.241 --> 00:03:19.480
As I told you
before, in Android N,

00:03:19.480 --> 00:03:22.100
ART introduced profile
based compilation.

00:03:22.100 --> 00:03:24.420
In Android O, we
have clever new uses

00:03:24.420 --> 00:03:27.240
for this, several clever
new uses, in fact.

00:03:27.240 --> 00:03:28.990
But one of the ones
I'm here to talk about

00:03:28.990 --> 00:03:31.600
is improving dex file locality.

00:03:31.600 --> 00:03:33.540
The idea is simple.

00:03:33.540 --> 00:03:36.090
Use the JIT profile information
to move the important things,

00:03:36.090 --> 00:03:37.631
the things that have
been used by you

00:03:37.631 --> 00:03:40.770
and your usage of the
applications, closer together.

00:03:40.770 --> 00:03:45.680
And this is all done seamlessly
and transparently to the user

00:03:45.680 --> 00:03:47.420
as we compile applications
on the device.

00:03:50.241 --> 00:03:52.240
So after we run your
application once, at least,

00:03:52.240 --> 00:03:53.630
to collect the
profile information,

00:03:53.630 --> 00:03:55.879
we use information about
what parts of the application

00:03:55.879 --> 00:03:57.290
were important to you.

00:03:57.290 --> 00:03:59.330
We gather the important
methods together,

00:03:59.330 --> 00:04:01.540
and leave the unimportant
methods together.

00:04:01.540 --> 00:04:04.200
As you can see on
the right there,

00:04:04.200 --> 00:04:09.160
a lot fewer pages are accessed,
especially the methods,

00:04:09.160 --> 00:04:12.760
when we use profile information.

00:04:12.760 --> 00:04:15.260
Now let's take a look at
what type of RAM improvements

00:04:15.260 --> 00:04:17.680
we see after launching a few
apps, a few of our favorites

00:04:17.680 --> 00:04:18.519
again.

00:04:18.519 --> 00:04:20.518
As you can see, there's
a significant reduction,

00:04:20.518 --> 00:04:24.480
typically around a third, in
these applications, less memory

00:04:24.480 --> 00:04:29.347
used by dex after layout.

00:04:29.347 --> 00:04:31.180
This RAM reduction also
improves launch time

00:04:31.180 --> 00:04:33.440
on devices that have slower
flash, because you're

00:04:33.440 --> 00:04:36.040
pulling in fewer pages and
hence spending less time paging

00:04:36.040 --> 00:04:39.329
applications off of the flash.

00:04:39.329 --> 00:04:40.370
So that's it for my part.

00:04:40.370 --> 00:04:42.828
Now over to Mathieu, who is
going to tell you about our New

00:04:42.828 --> 00:04:44.500
Garbage Collector.

00:04:44.500 --> 00:04:46.490
MATHIEU CHARTIER:
Thank you, David.

00:04:46.490 --> 00:04:47.900
Another thing that saves RAM--

00:04:47.900 --> 00:04:48.400
[APPLAUSE]

00:04:48.400 --> 00:04:54.330
--is the new concurrent and
copying Garbage Collector.

00:04:54.330 --> 00:04:57.630
Now Android has had a compacting
garbage collector that

00:04:57.630 --> 00:05:00.740
runs for background
apps since Android L.

00:05:00.740 --> 00:05:03.360
But unfortunately, since this
collector is non-concurrent,

00:05:03.360 --> 00:05:05.940
it meant that there was a GC
pause for the entire duration.

00:05:09.670 --> 00:05:12.380
So basically the problem
of having a GC pause

00:05:12.380 --> 00:05:14.490
for the entire
duration of the GC

00:05:14.490 --> 00:05:16.920
is that it can last
hundreds of milliseconds,

00:05:16.920 --> 00:05:18.920
and this would very
likely cause jank

00:05:18.920 --> 00:05:22.410
for foreground applications.

00:05:22.410 --> 00:05:25.200
In Android O, ART now
concurrently compacts

00:05:25.200 --> 00:05:29.150
the heaps of background and
foreground applications.

00:05:29.150 --> 00:05:32.890
This enables compaction of
many long-lived applications,

00:05:32.890 --> 00:05:35.360
such as the Android system
process and Google Play

00:05:35.360 --> 00:05:37.350
services.

00:05:37.350 --> 00:05:39.080
Since these processes
were long-lived,

00:05:39.080 --> 00:05:42.010
they tended to have a high
fragmentation over the time

00:05:42.010 --> 00:05:44.810
that they were executing.

00:05:44.810 --> 00:05:48.350
Let's take a look
at the GC process.

00:05:48.350 --> 00:05:52.440
Unlike its predecessor,
the new GC is region based.

00:05:52.440 --> 00:05:55.000
Starting out, the GC
does a brief pause

00:05:55.000 --> 00:05:58.450
to identify which regions
we are going to evacuate.

00:05:58.450 --> 00:06:02.690
These regions are also
known as the source regions.

00:06:02.690 --> 00:06:05.200
Threads then resume
from the pause

00:06:05.200 --> 00:06:08.730
after having walked
their stacks.

00:06:08.730 --> 00:06:13.390
Next up is the largest phase
of the GC, the copying phase.

00:06:13.390 --> 00:06:15.500
In the copying phase,
reachable objects

00:06:15.500 --> 00:06:19.360
are copied from the source
regions to destination regions.

00:06:19.360 --> 00:06:21.660
Finally, in the
reclaim phase, the GC

00:06:21.660 --> 00:06:23.860
frees the RAM for
the source regions.

00:06:26.910 --> 00:06:31.860
Starting with the pause phase,
the pause is pretty small.

00:06:31.860 --> 00:06:33.730
During the pause,
one of the key steps

00:06:33.730 --> 00:06:37.370
is identifying which
regions to evacuate.

00:06:37.370 --> 00:06:39.570
The goal of
evacuation is to copy

00:06:39.570 --> 00:06:41.770
all of the reachable
objects out of regions

00:06:41.770 --> 00:06:44.500
with high fragmentation.

00:06:44.500 --> 00:06:46.290
After this is
accomplished, the GC

00:06:46.290 --> 00:06:50.040
can release the RAM
for these regions.

00:06:50.040 --> 00:06:52.720
In this example, the GC
picks the middle two regions

00:06:52.720 --> 00:06:55.720
as the source regions, because
these both have more than 20%

00:06:55.720 --> 00:06:56.888
fragmentation.

00:06:59.640 --> 00:07:03.070
Next up is the copying phase.

00:07:03.070 --> 00:07:04.830
In the copying
phase, the GC copies

00:07:04.830 --> 00:07:06.890
all reachable objects
from the source regions

00:07:06.890 --> 00:07:08.980
to the destination
regions with the goal

00:07:08.980 --> 00:07:12.050
that no objects will
reference the source regions

00:07:12.050 --> 00:07:15.340
after collection has completed.

00:07:15.340 --> 00:07:17.830
The GC also updates the
references to these regions

00:07:17.830 --> 00:07:19.705
to point to the new
addresses of the objects.

00:07:23.030 --> 00:07:25.550
Now since application threads
are running concurrently

00:07:25.550 --> 00:07:27.357
during the Garbage
Collector, the GC

00:07:27.357 --> 00:07:29.690
needs a way to make sure that
these threads don't end up

00:07:29.690 --> 00:07:33.310
reading a field that
points to a source region.

00:07:33.310 --> 00:07:35.674
To accomplish this,
the GC uses a technique

00:07:35.674 --> 00:07:36.590
called a read barrier.

00:07:39.820 --> 00:07:41.740
A read barrier is a
small amount of work

00:07:41.740 --> 00:07:43.940
done for every field read.

00:07:43.940 --> 00:07:45.630
The read barrier
prevents threads

00:07:45.630 --> 00:07:47.750
from ever seeing references
to the source regions

00:07:47.750 --> 00:07:50.020
by intercepting the reads
and then copying the objects

00:07:50.020 --> 00:07:53.046
to the destination regions.

00:07:53.046 --> 00:07:54.420
In this example,
there's a thread

00:07:54.420 --> 00:07:56.670
attempting to read foo.x.

00:07:56.670 --> 00:08:00.540
This is a reference to a bar
object in the source region.

00:08:00.540 --> 00:08:03.090
The read barrier intercepts
this read, copies the object

00:08:03.090 --> 00:08:06.190
to the destination region, and
also returns the new address.

00:08:10.088 --> 00:08:12.510
The copying process
continues copying,

00:08:12.510 --> 00:08:15.250
moving objects as well as doing
read barriers, if necessary,

00:08:15.250 --> 00:08:18.150
until there are no longer
any references to the source

00:08:18.150 --> 00:08:20.620
region.

00:08:20.620 --> 00:08:23.045
At this point, the GC
begins the reclaim phase.

00:08:26.430 --> 00:08:28.550
As you can see here,
there are no longer

00:08:28.550 --> 00:08:30.401
any references to
the source regions,

00:08:30.401 --> 00:08:32.400
so the Garbage Collector
can free all of the RAM

00:08:32.400 --> 00:08:33.514
for these regions.

00:08:33.514 --> 00:08:36.630
And what we are
left with is a heap

00:08:36.630 --> 00:08:38.572
that has much less
wasted RAM compared

00:08:38.572 --> 00:08:39.780
to when the collection began.

00:08:43.204 --> 00:08:45.120
Now you might be wondering
how much RAM can we

00:08:45.120 --> 00:08:47.880
save by compacting foreground
applications as well

00:08:47.880 --> 00:08:49.860
as background applications.

00:08:49.860 --> 00:08:53.010
Well, the average heap
size is 32% smaller,

00:08:53.010 --> 00:08:55.570
compared to the Android N
Concurrent Mark Sweep Garbage

00:08:55.570 --> 00:08:57.420
Collector.

00:08:57.420 --> 00:09:00.300
And RAM lost to GC overhead
is also a little bit smaller.

00:09:03.740 --> 00:09:06.420
One important factor about
concurrent garbage collectors

00:09:06.420 --> 00:09:09.940
is how long application
threads are suspended.

00:09:09.940 --> 00:09:12.850
Every millisecond that the
threads are paused or suspended

00:09:12.850 --> 00:09:14.730
is one less
millisecond to prepare

00:09:14.730 --> 00:09:18.040
the next frame for the UI.

00:09:18.040 --> 00:09:19.800
With the new GC,
the average pause

00:09:19.800 --> 00:09:23.830
time is 0.4 milliseconds,
compared to 2.5 milliseconds

00:09:23.830 --> 00:09:28.400
in Android N. And the 99%
worst case for large heaps

00:09:28.400 --> 00:09:36.160
is 2.6 milliseconds, instead of
11 milliseconds in Android N.

00:09:36.160 --> 00:09:38.250
Finally, always
compacting the heap

00:09:38.250 --> 00:09:41.160
has enabled ART to switch to a
new thread local bump pointer

00:09:41.160 --> 00:09:43.260
allocator.

00:09:43.260 --> 00:09:46.830
This allocator is simpler
and faster than the free list

00:09:46.830 --> 00:09:49.580
allocator it replaces.

00:09:49.580 --> 00:09:52.210
Overall, this means the
allocation is around 70%

00:09:52.210 --> 00:09:55.100
faster compared to Android N.
And if you go further back,

00:09:55.100 --> 00:09:58.020
allocations are 18
times faster than KitKat

00:09:58.020 --> 00:10:00.540
on a device adjusted basis.

00:10:00.540 --> 00:10:04.803
And now off to Nicolas
for other optimizations.

00:10:04.803 --> 00:10:06.767
[APPLAUSE]

00:10:10.700 --> 00:10:12.700
NICOLAS GEOFFRAY:
Thank you, Mathieu.

00:10:12.700 --> 00:10:15.150
So besides managing
application memory,

00:10:15.150 --> 00:10:20.250
ART is also responsible for
executing application code.

00:10:20.250 --> 00:10:22.070
And for every Android
release, our team

00:10:22.070 --> 00:10:26.810
spends a significant time
optimizing wherever we can.

00:10:26.810 --> 00:10:28.610
So in this talk,
we're going to share

00:10:28.610 --> 00:10:30.500
with you two major
accomplishments

00:10:30.500 --> 00:10:32.250
we've made for this release.

00:10:32.250 --> 00:10:34.410
The first is a
performance case study.

00:10:34.410 --> 00:10:37.170
And the second is a new
optimization formula for loops.

00:10:40.546 --> 00:10:42.420
Let me start with the
performance case study.

00:10:42.420 --> 00:10:46.760
And then Aart will later talk
about loop optimizations.

00:10:46.760 --> 00:10:49.220
So to validate all the
optimizations we do,

00:10:49.220 --> 00:10:52.880
we have our own benchmarks
where we look for regressions

00:10:52.880 --> 00:10:53.880
and improvements.

00:10:53.880 --> 00:10:56.670
But we also want to make
sure that optimizations do

00:10:56.670 --> 00:11:00.140
matter for real Android apps.

00:11:00.140 --> 00:11:03.320
So for the O release, we have
invested our optimizations

00:11:03.320 --> 00:11:05.640
on one of our Android
applications, Sheets.

00:11:08.480 --> 00:11:10.840
Over the years, the ART
team and the Sheets team

00:11:10.840 --> 00:11:14.280
have worked on benchmarking
the core computational logic

00:11:14.280 --> 00:11:16.950
in the Sheets app.

00:11:16.950 --> 00:11:18.910
Our Sheets benchmark
suite is composed

00:11:18.910 --> 00:11:23.080
of three types of benchmarks,
benchmarking low-level runtime

00:11:23.080 --> 00:11:25.390
capabilities, core
Sheets formula

00:11:25.390 --> 00:11:29.545
evaluation, and benchmarks
that shuffle around themselves.

00:11:32.520 --> 00:11:35.510
And we're very excited to share
with you that in Android O,

00:11:35.510 --> 00:11:38.115
we've made significant
improvements to ART's runtime

00:11:38.115 --> 00:11:43.900
and compiler, up to the point
that we're now running eight

00:11:43.900 --> 00:11:46.070
out of the nine
benchmarks in Sheets

00:11:46.070 --> 00:11:48.740
from two times to
three times faster,

00:11:48.740 --> 00:11:50.700
compared to our
previous released Ouga.

00:11:56.420 --> 00:11:59.160
You may remember a
similar graph to this one

00:11:59.160 --> 00:12:02.300
that we presented at the
Keynote two days ago.

00:12:02.300 --> 00:12:05.530
This is actually a snapshot
of our benchmark Money Train

00:12:05.530 --> 00:12:09.210
tool, where we track
over time the impact

00:12:09.210 --> 00:12:14.390
of each individual change we
do in ART on our benchmarks.

00:12:14.390 --> 00:12:17.800
For this graph, we see that
the Sheets aggregate score

00:12:17.800 --> 00:12:21.530
keeps on improving month after
month, between the Android N

00:12:21.530 --> 00:12:24.237
and Android O release.

00:12:24.237 --> 00:12:26.570
So let me, people, explain
to you the major improvements

00:12:26.570 --> 00:12:32.450
we've made for O. So one
major change is of course

00:12:32.450 --> 00:12:36.280
the New Garbage Collector
that Mathieu just introduced.

00:12:36.280 --> 00:12:40.066
Overall, it improved the
performance of Sheets by 40%,

00:12:40.066 --> 00:12:41.440
thanks to thread
local allocation

00:12:41.440 --> 00:12:43.773
buffer and faster collections.

00:12:47.560 --> 00:12:49.370
The next major
improvement we made

00:12:49.370 --> 00:12:54.870
is our Inliner, which
gave us another 20% boost.

00:12:54.870 --> 00:12:58.400
And this optimization was one of
the easiest optimizations we've

00:12:58.400 --> 00:13:02.014
made, because you already
had an Inliner in N.

00:13:02.014 --> 00:13:03.680
But what happened in
N is that now we're

00:13:03.680 --> 00:13:06.010
doing all the compilation
in the background,

00:13:06.010 --> 00:13:08.090
and we only compile
important things.

00:13:08.090 --> 00:13:13.180
So we can avoid the code
bloating when we inline.

00:13:13.180 --> 00:13:15.640
With those two improvements
implemented in N,

00:13:15.640 --> 00:13:20.470
it was a lot easier for us to
do a more aggressive inlining

00:13:20.470 --> 00:13:25.160
for O. So for example, we
now inline for dex files.

00:13:25.160 --> 00:13:27.410
We inline methods which
could end up throwing.

00:13:27.410 --> 00:13:30.240
And we give a much larger
inlining budget so more

00:13:30.240 --> 00:13:31.170
methods get inlined.

00:13:34.584 --> 00:13:36.250
Well, let's move to
an optimization that

00:13:36.250 --> 00:13:39.690
did require some work, and it
helped us increase the Sheets

00:13:39.690 --> 00:13:42.386
code by 15%.

00:13:42.386 --> 00:13:43.760
This is called--
the optimization

00:13:43.760 --> 00:13:45.061
is called Code sinking.

00:13:48.500 --> 00:13:52.150
It's an optimization that
essentially moves instructions

00:13:52.150 --> 00:13:56.070
next to instructions
that actually use them.

00:13:56.070 --> 00:13:58.690
So typically what you want
to do is, instructions that

00:13:58.690 --> 00:14:02.920
are rarely being used,
you want to move them

00:14:02.920 --> 00:14:04.860
closer to where they are used.

00:14:04.860 --> 00:14:07.290
So the instructions that
are not in the regular flow

00:14:07.290 --> 00:14:09.460
of your application,
you want to move them

00:14:09.460 --> 00:14:13.050
to the exceptional cases.

00:14:13.050 --> 00:14:14.169
That's fairly abstract.

00:14:14.169 --> 00:14:16.460
So let me give you an example
to make it more concrete.

00:14:20.110 --> 00:14:23.620
Here's an excerpt
of Sheets code.

00:14:23.620 --> 00:14:25.940
There's a method
called createRange.

00:14:25.940 --> 00:14:28.690
It takes two integers
and returns a new range.

00:14:31.410 --> 00:14:35.640
The createRange method
calls a helper method

00:14:35.640 --> 00:14:38.920
called checkCondition that will
check the required conditions

00:14:38.920 --> 00:14:41.610
for creating a range.

00:14:41.610 --> 00:14:46.460
If those conditions are not
met, it will throw an error.

00:14:46.460 --> 00:14:52.650
Notice how the checkCondition
is a var arg test method.

00:14:52.650 --> 00:14:55.430
And this is important
to notice, because it

00:14:55.430 --> 00:15:00.420
will affect the code that is
being sent to our runtime.

00:15:00.420 --> 00:15:02.550
Here's how.

00:15:02.550 --> 00:15:06.230
So you start with
this simple method.

00:15:06.230 --> 00:15:10.650
The Java compiler actually
desugars the var args code

00:15:10.650 --> 00:15:14.630
to allocating a new object--

00:15:14.630 --> 00:15:19.470
allocating a new array, putting
in an array boxed versions

00:15:19.470 --> 00:15:22.780
of start and end, and
then passing this array

00:15:22.780 --> 00:15:24.940
to checkCondition.

00:15:24.940 --> 00:15:26.590
And this isn't Android specific.

00:15:26.590 --> 00:15:28.140
This is what the
Java compiler does

00:15:28.140 --> 00:15:29.431
when it compiles the byte code.

00:15:32.030 --> 00:15:34.810
And these instructions
are not completely free.

00:15:34.810 --> 00:15:36.740
Yes, we've made
allocations extremely fast

00:15:36.740 --> 00:15:38.390
with the New Garbage Collector.

00:15:38.390 --> 00:15:41.060
But having to create this array
and the boxed versions of start

00:15:41.060 --> 00:15:43.420
and end, all the
time the method is

00:15:43.420 --> 00:15:46.900
executed is not really ideal.

00:15:46.900 --> 00:15:51.940
So here's how we avoid
executing it completely.

00:15:51.940 --> 00:15:55.510
First, we can easily inline
to call the check condition.

00:15:55.510 --> 00:15:58.720
So internally this is how the
code is transformed in ART.

00:16:02.800 --> 00:16:07.420
Notice how args now is
only used in the If branch.

00:16:10.790 --> 00:16:13.060
So what the compiler
can do is move

00:16:13.060 --> 00:16:16.400
the allocation and
the boxing closer

00:16:16.400 --> 00:16:21.120
to where args is being used
in the exceptional flow.

00:16:21.120 --> 00:16:23.260
So at the end of this
optimization, what

00:16:23.260 --> 00:16:25.840
the compiler has made
sure of is that only

00:16:25.840 --> 00:16:28.690
the required instructions will
be executed in the normal flow.

00:16:33.930 --> 00:16:35.950
The last optimization
I want to mention here

00:16:35.950 --> 00:16:38.308
is Class Hierarchy Analysis.

00:16:41.380 --> 00:16:43.800
Class Hierarchy Analysis is
a pretty common technique

00:16:43.800 --> 00:16:46.280
in object oriented
language, when

00:16:46.280 --> 00:16:50.520
a runtime will try to infer
classes and methods that

00:16:50.520 --> 00:16:56.570
can be made final even though
they aren't marked as final.

00:16:56.570 --> 00:16:59.700
Having this information
internally gives

00:16:59.700 --> 00:17:04.440
a lot of room for the compiler
for more optimizations.

00:17:04.440 --> 00:17:06.230
For example, you can
do more inlining.

00:17:09.730 --> 00:17:12.040
And because Java has
done any class loading,

00:17:12.040 --> 00:17:15.569
it will bail out of
these optimizations

00:17:15.569 --> 00:17:19.349
if suddenly a new
class gets loaded

00:17:19.349 --> 00:17:22.550
and all the optimizations we
did by inferring final classes

00:17:22.550 --> 00:17:24.300
and methods become invalid.

00:17:29.560 --> 00:17:30.935
So this sums up
the optimizations

00:17:30.935 --> 00:17:33.380
I want to talk to you about.

00:17:33.380 --> 00:17:35.000
For the record, we
also did a bunch

00:17:35.000 --> 00:17:38.510
of other improvements
for this release,

00:17:38.510 --> 00:17:42.040
such as faster type checks,
faster accessing of compiler

00:17:42.040 --> 00:17:47.110
code for classes of strings,
faster JNI transitions, more

00:17:47.110 --> 00:17:51.470
compiler intrinsics, and
a lot more that we do not

00:17:51.470 --> 00:17:54.181
have the space to list here.

00:17:54.181 --> 00:17:55.680
So for the interest
of time, I'm not

00:17:55.680 --> 00:17:58.776
going to go into details
on these optimizations.

00:17:58.776 --> 00:18:00.150
Instead I'm going
to hand it over

00:18:00.150 --> 00:18:02.460
to Aart, who is
going to tell you

00:18:02.460 --> 00:18:08.235
about a whole new set
of loop optimizations.

00:18:08.235 --> 00:18:10.223
[APPLAUSE]

00:18:13.710 --> 00:18:15.650
AART BIK: Thank you, Nicolas.

00:18:15.650 --> 00:18:19.260
So if you don't look
at virtual method

00:18:19.260 --> 00:18:21.790
overhead or general
runtime overhead,

00:18:21.790 --> 00:18:24.980
programs tend to spend
most of the time in loops.

00:18:24.980 --> 00:18:27.880
So besides all the good
stuff that Nicolas already

00:18:27.880 --> 00:18:29.880
talked about, it
can really pay off

00:18:29.880 --> 00:18:32.440
by optimizing
loops specifically.

00:18:32.440 --> 00:18:36.900
And I will discuss some
of these in this part.

00:18:36.900 --> 00:18:39.900
Optimizations really always
consist of an analysis part

00:18:39.900 --> 00:18:42.640
where you look at the program
and an optimization part

00:18:42.640 --> 00:18:45.110
where you actually perform
the transformations.

00:18:45.110 --> 00:18:48.690
And you can see here a list
of all the work we did for O.

00:18:48.690 --> 00:18:54.020
But I'll only touch on a few of
those in the interest of time.

00:18:54.020 --> 00:18:56.470
But before I do that, let's
first look at what loop

00:18:56.470 --> 00:18:58.670
optimizations can do for you.

00:18:58.670 --> 00:19:02.700
So here you see a graph that
puts Android O versus Android N

00:19:02.700 --> 00:19:04.870
performance, so
higher is better.

00:19:04.870 --> 00:19:07.470
And the color encoding
shows you specifically

00:19:07.470 --> 00:19:11.330
where loop optimizations
in general, the blue stuff,

00:19:11.330 --> 00:19:14.800
has helped, and the red stuff
where vectorization, which I'll

00:19:14.800 --> 00:19:17.100
talk about briefly, has helped.

00:19:17.100 --> 00:19:20.750
You see that benchmarks on
the left, like Loop and Sum,

00:19:20.750 --> 00:19:23.830
really get unrealistic
high speed ups.

00:19:23.830 --> 00:19:27.100
And it is really as a result
of that we broke the benchmark.

00:19:27.100 --> 00:19:28.870
So we were able
to transform loops

00:19:28.870 --> 00:19:32.490
into closed form expressions
that execute really fast.

00:19:32.490 --> 00:19:34.510
And although that's
always nice to have

00:19:34.510 --> 00:19:37.920
stability in your compiler,
it's not very realistic.

00:19:37.920 --> 00:19:40.040
But as you go to the
right of the graph,

00:19:40.040 --> 00:19:42.850
you see more
realistic speed ups.

00:19:42.850 --> 00:19:47.790
LU and Linpack obtained 10%
improvement by vectorization.

00:19:51.540 --> 00:19:54.650
So central to all
optimizations is always

00:19:54.650 --> 00:19:56.760
Induction Variable Recognition.

00:19:56.760 --> 00:19:59.330
And that consists of
finding expressions

00:19:59.330 --> 00:20:03.600
that progress in a regular and
predictable way in your loop.

00:20:03.600 --> 00:20:07.020
And the most common example
is a linear induction.

00:20:07.020 --> 00:20:09.330
So here in this
example, the basic loop

00:20:09.330 --> 00:20:11.490
counter i is a linear induction.

00:20:11.490 --> 00:20:15.350
But also the expression j, shown
in blue, is a linear induction.

00:20:15.350 --> 00:20:18.480
Every time around
iteration, it increments

00:20:18.480 --> 00:20:22.120
by loop invariant constants.

00:20:22.120 --> 00:20:24.380
There's many more
induction variables.

00:20:24.380 --> 00:20:27.540
And you see some of them
depicted here in the graph.

00:20:27.540 --> 00:20:29.970
And detecting as many
of them as possible

00:20:29.970 --> 00:20:32.060
is always good for
the next phase,

00:20:32.060 --> 00:20:33.490
the actual optimizations.

00:20:36.010 --> 00:20:39.360
So let's look at what loop
optimizations can benefit

00:20:39.360 --> 00:20:41.830
from induction variables.

00:20:41.830 --> 00:20:45.300
So here you see a somewhat
synthetic example,

00:20:45.300 --> 00:20:48.870
where the compiler can easily
see that in the innermost loop,

00:20:48.870 --> 00:20:52.220
the increment to sum
actually is very predictable.

00:20:52.220 --> 00:20:55.000
Since the loop only
iterates 100 times,

00:20:55.000 --> 00:20:58.780
it can actually replace
the sum, hoist it out

00:20:58.780 --> 00:21:03.600
with a closed form expression
that just adds 100 at a time.

00:21:03.600 --> 00:21:05.350
After you've done
that, you can actually

00:21:05.350 --> 00:21:08.590
hoist it again out of the next
loop if you take a little bit

00:21:08.590 --> 00:21:10.450
care of the fact
that the loop may not

00:21:10.450 --> 00:21:12.790
be taken when N is negative.

00:21:12.790 --> 00:21:14.630
So after that, the
whole computation

00:21:14.630 --> 00:21:16.430
has been hoisted
out of the loop,

00:21:16.430 --> 00:21:19.000
and the whole double
loop can be eliminated.

00:21:19.000 --> 00:21:22.999
So in this case, the whole loop
is replaced by closed form.

00:21:22.999 --> 00:21:25.540
And that's one of the examples
that I showed at the beginning

00:21:25.540 --> 00:21:27.480
where we broke the benchmark.

00:21:27.480 --> 00:21:30.860
And obviously your
code will probably not

00:21:30.860 --> 00:21:33.500
benefit this greatly
from optimizations,

00:21:33.500 --> 00:21:36.250
but having this
ability can really

00:21:36.250 --> 00:21:40.090
kick in, like after you
inline library code.

00:21:40.090 --> 00:21:42.070
Not to the same
degree, but it's still

00:21:42.070 --> 00:21:45.490
nice to be able to optimize
your induction variables.

00:21:49.310 --> 00:21:52.000
Another example where induction
variables can help you

00:21:52.000 --> 00:21:54.410
is the Bounds Check Elimination.

00:21:54.410 --> 00:21:56.600
So when you access
an array, you always

00:21:56.600 --> 00:21:58.970
need to test to see whether
the subscripts can go out

00:21:58.970 --> 00:21:59.530
of bounds.

00:21:59.530 --> 00:22:01.650
And if they do, you
throw an exception.

00:22:01.650 --> 00:22:04.450
But if you know the except range
that induction variables will

00:22:04.450 --> 00:22:08.620
take, you can often eliminate
those tests completely.

00:22:08.620 --> 00:22:12.780
So here you see an example
that both the axes to a and b

00:22:12.780 --> 00:22:14.550
will never go out of bounds.

00:22:14.550 --> 00:22:17.390
So the compiler can
statically remove those tests,

00:22:17.390 --> 00:22:20.130
and the program, as a result,
will execute a lot faster.

00:22:24.720 --> 00:22:28.790
Induction variables also
tell how often loops iterate.

00:22:28.790 --> 00:22:31.380
So if you know that it
is only a few times,

00:22:31.380 --> 00:22:34.400
you can actually
completely unroll the loop.

00:22:34.400 --> 00:22:37.600
And the advantage of unrolling
is that you completely remove

00:22:37.600 --> 00:22:39.320
the loop control overhead.

00:22:39.320 --> 00:22:41.290
It reduces the code
size, because you don't

00:22:41.290 --> 00:22:44.000
have the control to iterate.

00:22:44.000 --> 00:22:47.110
And it often enables
constant folding.

00:22:47.110 --> 00:22:49.450
So you see an example
here where the blue part

00:22:49.450 --> 00:22:51.930
shows that the loop only
iterates from 10 to 10,

00:22:51.930 --> 00:22:53.980
so it actually
only iterates once.

00:22:53.980 --> 00:22:56.820
So you can replace the whole
loop with a single statement.

00:22:56.820 --> 00:23:00.290
And as a result, you can also
constant fold to multiplication

00:23:00.290 --> 00:23:02.410
and already do that
at compile time.

00:23:02.410 --> 00:23:05.080
So the program will
run a lot faster.

00:23:05.080 --> 00:23:08.450
Again, typical code won't
optimize right away.

00:23:08.450 --> 00:23:11.870
But after inlining or other
forms of specializations,

00:23:11.870 --> 00:23:13.310
these iterations occur.

00:23:13.310 --> 00:23:16.780
And they can help
improve your performance.

00:23:21.037 --> 00:23:23.120
So the last loop optimization
I want to talk about

00:23:23.120 --> 00:23:27.300
is the ability of
Android O to take

00:23:27.300 --> 00:23:29.690
advantage of SIMD instructions.

00:23:29.690 --> 00:23:31.750
So SIMD instructions
are instructions

00:23:31.750 --> 00:23:34.710
that perform a single
operation simultaneously

00:23:34.710 --> 00:23:36.890
to multiple data operands.

00:23:36.890 --> 00:23:40.320
So you see an example here
where one instruction actually

00:23:40.320 --> 00:23:43.190
does four additions
at the same time.

00:23:43.190 --> 00:23:44.870
And all our target platforms--

00:23:44.870 --> 00:23:50.090
ARM, Intel, MIPS-- they
have such instructions.

00:23:50.090 --> 00:23:51.590
And if you take
advantage of them,

00:23:51.590 --> 00:23:53.630
it can greatly improve
the performance

00:23:53.630 --> 00:23:55.260
of certain classes of programs.

00:23:59.540 --> 00:24:02.610
So converting sequential
code-- and we typically focus

00:24:02.610 --> 00:24:05.080
on loops, but it doesn't have
to be restricted to that--

00:24:05.080 --> 00:24:07.500
into code that exploits
the SIMD instructions,

00:24:07.500 --> 00:24:10.270
that process is
called vectorization.

00:24:10.270 --> 00:24:13.560
And it's illustrated
here, where on the left

00:24:13.560 --> 00:24:15.762
you see a loop that
iterates sequentially.

00:24:15.762 --> 00:24:17.470
And on the right, you
see the vector loop

00:24:17.470 --> 00:24:20.620
that actually goes by four.

00:24:20.620 --> 00:24:22.290
In order to do this
transformation,

00:24:22.290 --> 00:24:26.310
this vectorization, you need
some very specific analysis.

00:24:26.310 --> 00:24:28.710
You have to detect and
resolve memory conflicts

00:24:28.710 --> 00:24:31.570
between the loop iterations
to see if you can actually

00:24:31.570 --> 00:24:33.180
execute them [INAUDIBLE].

00:24:33.180 --> 00:24:35.890
You may want to be a little bit
more strict about alignments,

00:24:35.890 --> 00:24:41.170
because SIMD instructions often
require stricter alignments

00:24:41.170 --> 00:24:42.280
on the memory.

00:24:42.280 --> 00:24:45.360
And you want to detect
idiomatic constructs--

00:24:45.360 --> 00:24:47.270
you'll see an example
of that shortly--

00:24:47.270 --> 00:24:49.960
stuff that you cannot really
express with single operators

00:24:49.960 --> 00:24:51.770
in your sequential code.

00:24:51.770 --> 00:24:53.460
But if you detect
them, you can map them

00:24:53.460 --> 00:24:56.096
onto very efficient
SIMD instructions.

00:24:59.830 --> 00:25:01.790
So let's just explore
this whole vectorization

00:25:01.790 --> 00:25:03.510
with a case study.

00:25:03.510 --> 00:25:06.980
Suppose that you want to
display a stream of pictures,

00:25:06.980 --> 00:25:10.140
like these paintings here on
the graph, on your display.

00:25:10.140 --> 00:25:12.340
And you want to
transition smoothly

00:25:12.340 --> 00:25:14.110
from one picture to the other.

00:25:14.110 --> 00:25:16.329
You don't want to just show
one and then the other.

00:25:16.329 --> 00:25:18.370
You want to sort of have
a transition in between.

00:25:18.370 --> 00:25:20.320
And you want to
do that real time.

00:25:20.320 --> 00:25:22.150
You don't know the
stream in advance.

00:25:22.150 --> 00:25:24.320
And of course you want to
do it in an efficient way.

00:25:27.580 --> 00:25:29.400
So one way to get
a smooth transition

00:25:29.400 --> 00:25:31.910
is actually performing
a cross-fade.

00:25:31.910 --> 00:25:34.810
So what you do is called
rounding halving add.

00:25:34.810 --> 00:25:37.800
So basically you take the
effects of two pictures,

00:25:37.800 --> 00:25:40.300
and you show that in
between the two pictures.

00:25:40.300 --> 00:25:44.460
So in the example here, you
see the shipwreck on the top

00:25:44.460 --> 00:25:47.550
and the quiet fire
on the bottom.

00:25:47.550 --> 00:25:49.650
And the picture that
you show in between

00:25:49.650 --> 00:25:52.290
will sort of be an average
of the two pictures,

00:25:52.290 --> 00:25:53.760
showing them at the same time.

00:25:53.760 --> 00:25:56.790
It forms a nice,
smooth transition.

00:25:56.790 --> 00:25:58.570
So how do you code
it, and you don't want

00:25:58.570 --> 00:25:59.960
to go to a native solution.

00:25:59.960 --> 00:26:01.790
You don't want to use the NDK.

00:26:01.790 --> 00:26:03.500
You don't want to use GPU code.

00:26:03.500 --> 00:26:05.610
You just want to
stay at source level.

00:26:09.160 --> 00:26:12.830
So here you see a method that
could do such a cross-fade.

00:26:12.830 --> 00:26:15.150
So you have two
incoming byte arrays

00:26:15.150 --> 00:26:18.560
with 3% of pictures x1 and x2.

00:26:18.560 --> 00:26:23.490
You go for a zero extension
and an averaging operation,

00:26:23.490 --> 00:26:26.170
and then you compute the x out.

00:26:26.170 --> 00:26:29.420
So keeping this at source
level is, of course,

00:26:29.420 --> 00:26:32.840
a much easier way of
expressing such an algorithm.

00:26:32.840 --> 00:26:35.420
It's easy to write, test,
debug, maintain, et cetera.

00:26:35.420 --> 00:26:37.690
You don't have to use the NDK.

00:26:37.690 --> 00:26:39.270
You don't have to use GPU code.

00:26:43.260 --> 00:26:47.470
So Android N will actually
translate the loop

00:26:47.470 --> 00:26:50.230
that I just showed to the
sequential code on the left.

00:26:50.230 --> 00:26:54.020
And Android O will now
generate the SIMD code

00:26:54.020 --> 00:26:55.490
shown on the right.

00:26:55.490 --> 00:26:57.770
And I don't expect you to
address this whole assembly

00:26:57.770 --> 00:26:58.340
right away.

00:26:58.340 --> 00:27:01.750
But just focus on the parts
that have been highlighted.

00:27:01.750 --> 00:27:05.800
So first of all, in orange, you
see that the sequential loop

00:27:05.800 --> 00:27:06.990
goes by one.

00:27:06.990 --> 00:27:09.650
It does one byte
at a time before it

00:27:09.650 --> 00:27:10.910
goes to the other one.

00:27:10.910 --> 00:27:14.980
In contrast, the
SIMD code goes by 16.

00:27:14.980 --> 00:27:18.384
It does 16 bytes
at the same time.

00:27:18.384 --> 00:27:19.800
The other thing
that you notice is

00:27:19.800 --> 00:27:24.000
that the loop body is a lot
shorter for the SIMD code.

00:27:24.000 --> 00:27:26.930
And that's a result of
the yellow highlighted

00:27:26.930 --> 00:27:28.010
instructions.

00:27:28.010 --> 00:27:30.170
On the left, there's
a lot of instructions

00:27:30.170 --> 00:27:32.160
required to do
the zero extension

00:27:32.160 --> 00:27:34.820
and the rounding halving add.

00:27:34.820 --> 00:27:37.730
On the right, there's a
single idiomatic instruction

00:27:37.730 --> 00:27:40.010
that can take care of that.

00:27:40.010 --> 00:27:44.490
So all this combined
makes sure that the loop

00:27:44.490 --> 00:27:48.850
runs about 10 times faster
if you just look at the loop.

00:27:51.440 --> 00:27:54.665
However, if you look at
the real application,

00:27:54.665 --> 00:27:58.620
where this loop is actually part
of a larger thing where there's

00:27:58.620 --> 00:28:01.350
a handler requesting
new pictures, et cetera,

00:28:01.350 --> 00:28:03.420
you can see that
vectorization made

00:28:03.420 --> 00:28:06.070
a difference between
rendering at the slow 20

00:28:06.070 --> 00:28:10.840
frames per second, and the
other vectorization we rendered

00:28:10.840 --> 00:28:12.550
at 60 frames per second.

00:28:12.550 --> 00:28:15.100
And that's actually
as fast as you can go.

00:28:15.100 --> 00:28:17.250
You cannot render much faster.

00:28:17.250 --> 00:28:19.510
So that really shows
that vectorization

00:28:19.510 --> 00:28:21.060
has given you the
power to render

00:28:21.060 --> 00:28:22.720
at a very acceptable speed.

00:28:22.720 --> 00:28:25.470
And you actually have
CPU cycles to spare,

00:28:25.470 --> 00:28:27.360
because it doesn't need
the full power to go

00:28:27.360 --> 00:28:30.160
to the 60 frames per second.

00:28:30.160 --> 00:28:33.200
And all this was made
possible by the vectorization.

00:28:33.200 --> 00:28:38.400
You don't need to write a
tedious NDK code anymore.

00:28:38.400 --> 00:28:42.450
You can just express the
loop at source level.

00:28:42.450 --> 00:28:46.100
[MUSIC PLAYING]

