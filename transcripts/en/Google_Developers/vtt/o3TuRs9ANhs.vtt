WEBVTT
Kind: captions
Language: en

00:00:03.000 --> 00:00:05.033
Slatkin: Just--Hey, okay.

00:00:05.033 --> 00:00:11.534
Hi. First session today,
so let's get into it.

00:00:11.534 --> 00:00:15.133
This is "Offline Processing
on App Engine: A Look Ahead."

00:00:15.133 --> 00:00:16.567
I'm Brett Slatkin.

00:00:16.567 --> 00:00:21.067
I'm a software engineer
on the Google App Engine team.

00:00:21.067 --> 00:00:22.801
And I'm really excited
to be able to talk to you

00:00:22.801 --> 00:00:24.501
about some
really cool features

00:00:24.501 --> 00:00:25.701
that we've been working on
for a while

00:00:25.701 --> 00:00:30.834
that we're going to tell you
all about today.

00:00:30.834 --> 00:00:33.467
Oh, yeah, and really quick,
some people thought

00:00:33.467 --> 00:00:35.968
offline processing
was talking about mobile phones

00:00:35.968 --> 00:00:39.234
and offline--
different kind of offline.

00:00:39.234 --> 00:00:42.901
This is offline processing--
doing work offline,

00:00:42.901 --> 00:00:45.067
as opposed to using
your mobile phone

00:00:45.067 --> 00:00:46.367
when you don't have
connectivity.

00:00:46.367 --> 00:00:47.734
So if you're in
the wrong session,

00:00:47.734 --> 00:00:51.567
you won't hurt my feelings
if you leave.

00:00:51.567 --> 00:00:52.634
All right,
so here's our agenda.

00:00:52.634 --> 00:00:54.767
We're gonna talk about
a new API:

00:00:54.767 --> 00:00:56.467
the Task Queue API.

00:00:56.467 --> 00:00:58.300
We're gonna talk about
what tasks are.

00:00:58.300 --> 00:01:00.267
We're gonna talk about
web hooks.

00:01:00.267 --> 00:01:02.868
We're gonna talk about
push versus pull,

00:01:02.868 --> 00:01:05.734
and the performance
of those models.

00:01:05.734 --> 00:01:08.300
We're gonna talk about
idempotence, queues,

00:01:08.300 --> 00:01:09.367
and throttling.

00:01:09.367 --> 00:01:12.033
Task names and ETAs.

00:01:12.033 --> 00:01:14.734
And then we're gonna have some
example applications

00:01:14.734 --> 00:01:16.267
using the Task Queue
interspersed

00:01:16.267 --> 00:01:17.801
through the presentation,

00:01:17.801 --> 00:01:20.267
including sending email,
Schema migration,

00:01:20.267 --> 00:01:22.567
and write-behind cache.

00:01:22.567 --> 00:01:24.133
And then finally we're gonna
talk about the future

00:01:24.133 --> 00:01:25.767
of offline processing
on App Engine,

00:01:25.767 --> 00:01:29.467
and what we have planned--
what's coming.

00:01:29.467 --> 00:01:33.067
So I have a Moderator page
set up.

00:01:33.067 --> 00:01:36.133
It's tinyurl.com/offlinetalk.

00:01:36.133 --> 00:01:39.167
And you can give immediate
feedback about this presentation

00:01:39.167 --> 00:01:42.267
on haveasec.com/io.

00:01:42.267 --> 00:01:47.534
So please start putting
your questions in there.

00:01:47.534 --> 00:01:49.501
All right, so let's just start
from the beginning.

00:01:49.501 --> 00:01:51.734
Let's talk about
the motivation

00:01:51.734 --> 00:01:55.300
of what we're trying
to accomplish here.

00:01:55.300 --> 00:01:57.400
So hopefully, as you know,
if you've tried it out,

00:01:57.400 --> 00:01:59.534
Google App Engine
is great for web apps.

00:01:59.534 --> 00:02:01.767
We're very good for
request-based,

00:02:01.767 --> 00:02:05.534
database-backed applications.

00:02:05.534 --> 00:02:08.434
But that's a lot
of online processing.

00:02:08.434 --> 00:02:10.734
You have to do things
within the boundaries

00:02:10.734 --> 00:02:12.267
of a single request.

00:02:12.267 --> 00:02:14.901
There's a limit to what 
you can do during that time.

00:02:14.901 --> 00:02:17.334
There are a lot of applications
that fit that model,

00:02:17.334 --> 00:02:18.767
but there are a lot
that don't.

00:02:18.767 --> 00:02:21.667
And that's why background
and batch processing are

00:02:21.667 --> 00:02:25.100
two highly requested features
that people have been asking for

00:02:25.100 --> 00:02:26.601
over and over again.

00:02:26.601 --> 00:02:29.300
We've delivered that in part
with cron.

00:02:29.300 --> 00:02:31.400
And cron's very useful
for doing work periodically

00:02:31.400 --> 00:02:32.834
every so often.

00:02:32.834 --> 00:02:35.667
But it's really not good enough
if you want to do a lot of work

00:02:35.667 --> 00:02:38.400
or if you want to do work at a
very high frequency, high rate.

00:02:38.400 --> 00:02:42.634
And if we actually delivered
background and batch processing,

00:02:42.634 --> 00:02:45.434
it would enable a whole range
of new applications

00:02:45.434 --> 00:02:46.501
on App Engine.

00:02:46.501 --> 00:02:48.434
So, you know,
some people think that

00:02:48.434 --> 00:02:50.467
not having offline processing
is a deal breaker,

00:02:50.467 --> 00:02:55.234
and we're hoping
to remedy that.

00:02:55.234 --> 00:02:57.501
So again, for background--

00:02:57.501 --> 00:02:59.467
Why do you want to do
background processing?

00:02:59.467 --> 00:03:01.534
Well, I want to do work
continuously

00:03:01.534 --> 00:03:02.868
without user requests.

00:03:02.868 --> 00:03:05.334
I don't want to have to be bound
to a single user request

00:03:05.334 --> 00:03:08.267
in order
to get something done.

00:03:08.267 --> 00:03:10.467
I want to incrementally
process data.

00:03:10.467 --> 00:03:12.567
I want to compute some results,
aggregations,

00:03:12.567 --> 00:03:14.901
whatever, what have you.

00:03:14.901 --> 00:03:16.267
I also want to
smooth out load patterns

00:03:16.267 --> 00:03:18.901
and reduce user latency.

00:03:18.901 --> 00:03:20.934
So these are a few of them,
and we'll get into

00:03:20.934 --> 00:03:23.601
what that actually means.

00:03:23.601 --> 00:03:24.901
But what's really cool
about this is that

00:03:24.901 --> 00:03:27.734
this is a new style of
computation on App Engine.

00:03:27.734 --> 00:03:29.667
We're doing something
fundamentally different

00:03:29.667 --> 00:03:31.567
than everything
we've done before.

00:03:31.567 --> 00:03:33.300
Up until now, it's really about,
you know,

00:03:33.300 --> 00:03:36.000
what can I do in a single
request from my user?

00:03:36.000 --> 00:03:38.200
My talk yesterday,
and a lot of articles

00:03:38.200 --> 00:03:40.234
you've seen
on our web site talk about

00:03:40.234 --> 00:03:42.968
ways to optimize
for single requests,

00:03:42.968 --> 00:03:45.267
ways to get a lot of work done
in a single request.

00:03:45.267 --> 00:03:47.501
And we can finally
move past that

00:03:47.501 --> 00:03:49.300
with some of the systems
we're building here.

00:03:49.300 --> 00:03:52.267
So we're getting into
new territory,

00:03:52.267 --> 00:03:56.534
and it's really exciting.

00:03:56.534 --> 00:03:58.734
All right, so let's have
an introduction.

00:03:58.734 --> 00:04:03.200
So we're going to talk about
our new API for App Engine.

00:04:03.200 --> 00:04:05.501
We're calling it
the Task Queue API.

00:04:05.501 --> 00:04:08.400
Now, the Task Queue API
is part of a new thing

00:04:08.400 --> 00:04:10.634
we're calling App Engine Labs.

00:04:10.634 --> 00:04:14.567
Labs is similar
to other Google Labs.

00:04:14.567 --> 00:04:17.701
The word "labs."

00:04:17.701 --> 00:04:19.601
So this means that
the API may change

00:04:19.601 --> 00:04:21.667
until it's graduated
from Labs.

00:04:21.667 --> 00:04:23.234
What does that mean?

00:04:23.234 --> 00:04:25.100
So that means that
we're gonna try our best

00:04:25.100 --> 00:04:28.033
to not make any backwards,
incompatible changes

00:04:28.033 --> 00:04:29.300
to the API right now.

00:04:29.300 --> 00:04:30.868
We're gonna try
to keep it stable,

00:04:30.868 --> 00:04:32.567
but we're going to make
some small changes.

00:04:32.567 --> 00:04:34.968
We're gonna maybe
move some things around,

00:04:34.968 --> 00:04:36.400
make the API nicer in some ways.

00:04:36.400 --> 00:04:37.734
We're gonna listen
to your feedback,

00:04:37.734 --> 00:04:39.767
extend it,
take things away,

00:04:39.767 --> 00:04:42.400
and make it
a really good API.

00:04:42.400 --> 00:04:43.767
Once it's graduated
from Labs,

00:04:43.767 --> 00:04:47.400
then we'll be dedicated
to having

00:04:47.400 --> 00:04:49.467
a backwards-compatible API
for you to use

00:04:49.467 --> 00:04:50.968
like the rest of our APIs,

00:04:50.968 --> 00:04:52.868
where we don't break
your application

00:04:52.868 --> 00:04:55.901
with subsequent releases
of our system

00:04:55.901 --> 00:04:58.834
unless we specifically
send you an email telling you,

00:04:58.834 --> 00:05:03.167
and have a full version change,
which we have not yet done.

00:05:03.167 --> 00:05:04.567
So there'll be
a lot of warning

00:05:04.567 --> 00:05:07.334
if we end up having
to change it.

00:05:07.334 --> 00:05:10.567
Another part of App Engine Labs
is that we haven't specified

00:05:10.567 --> 00:05:11.934
how we're gonna do billing yet.

00:05:11.934 --> 00:05:13.767
We're still figuring out
the details of billing,

00:05:13.767 --> 00:05:16.434
how this will be billed.

00:05:16.434 --> 00:05:19.667
So we'll, you know,
look into that later

00:05:19.667 --> 00:05:21.067
if you're wondering
about pricing,

00:05:21.067 --> 00:05:25.734
and how that fits into
the rest of the billing system.

00:05:25.734 --> 00:05:29.234
And hopefully this will be
the first of many APIs

00:05:29.234 --> 00:05:31.434
that we're putting in Labs.

00:05:31.434 --> 00:05:35.133
Now, Task Queue API
is not released today,

00:05:35.133 --> 00:05:36.968
unfortunately--
I'm sorry to say that.

00:05:36.968 --> 00:05:38.601
But it should launch
in a couple of weeks.

00:05:38.601 --> 00:05:40.901
And I do--
it's not vaporware.

00:05:40.901 --> 00:05:42.167
I have live--

00:05:42.167 --> 00:05:44.501
I have the live version
of the Task Queue running

00:05:44.501 --> 00:05:47.367
in our actual
production clusters.

00:05:47.367 --> 00:05:50.133
And it's on App Spot,
so you'll be able to

00:05:50.133 --> 00:05:52.601
actually see it in action and
take it for a spin for yourself.

00:05:52.601 --> 00:05:55.300
And it has working code,

00:05:55.300 --> 00:05:59.968
and so I'll be
sharing that with you.

00:05:59.968 --> 00:06:02.701
Okay, so let's just do
some overview, some background,

00:06:02.701 --> 00:06:05.400
since this is kind of
a new thing for a lot of people.

00:06:05.400 --> 00:06:06.567
So what is a task queue?

00:06:06.567 --> 00:06:09.033
Well, it's a simple idea,
in general.

00:06:09.033 --> 00:06:12.634
You describe the work
that you want to do right now.

00:06:12.634 --> 00:06:14.634
So, you know,
you have some idea--

00:06:14.634 --> 00:06:16.567
I need to do something.

00:06:16.567 --> 00:06:18.467
You describe it, and you save
that description

00:06:18.467 --> 00:06:20.467
of the work somewhere,
very quickly.

00:06:20.467 --> 00:06:24.300
And then later on, you come back
and you execute that work.

00:06:24.300 --> 00:06:26.734
Three steps.

00:06:26.734 --> 00:06:30.067
The work that you have saved--
the descriptions you've saved

00:06:30.067 --> 00:06:32.300
are executed in the order
they were received.

00:06:32.300 --> 00:06:35.767
So it's a best effort,
first in, first out queue.

00:06:35.767 --> 00:06:38.400
So things that were added first
get done first.

00:06:38.400 --> 00:06:40.467
Things added later
will get done later.

00:06:40.467 --> 00:06:42.734
Okay, another important thing
is that

00:06:42.734 --> 00:06:44.267
if execution fails
at this work,

00:06:44.267 --> 00:06:46.334
it'll be retried
until it's successful.

00:06:46.334 --> 00:06:49.868
So you have a very high level
of confidence

00:06:49.868 --> 00:06:53.367
that something you put
in a queue will be completed.

00:06:53.367 --> 00:06:56.400
Now, the smallest example
of a task queue API

00:06:56.400 --> 00:06:57.701
would be something like this:

00:06:57.701 --> 00:07:00.200
taskqueue.add,
some description of the work.

00:07:00.200 --> 00:07:01.567
Whatever that means.

00:07:01.567 --> 00:07:07.100
Do this later.

00:07:07.100 --> 00:07:08.701
What is a task queue?

00:07:08.701 --> 00:07:10.734
Why is it beneficial?
Why is it useful?

00:07:10.734 --> 00:07:12.067
Well, first of all,
it's asynchronous.

00:07:12.067 --> 00:07:14.634
Why do anything right now
if I could do it later?

00:07:14.634 --> 00:07:17.000
I'm very lazy.
I don't want to have to wait.

00:07:17.000 --> 00:07:20.434
So instead of, you know,
doing some very expensive,

00:07:20.434 --> 00:07:21.734
very high-latency operation,

00:07:21.734 --> 00:07:23.801
I'll just record the fact
that I have to do it

00:07:23.801 --> 00:07:24.968
at some point in the future

00:07:24.968 --> 00:07:27.033
and then just
deal with it later.

00:07:27.033 --> 00:07:30.467
Because of that--
So that's one part.

00:07:30.467 --> 00:07:33.534
The other part is that
it lets me do many operations

00:07:33.534 --> 00:07:35.100
at the same time
asynchronously,

00:07:35.100 --> 00:07:36.567
so that I don't have
dependencies

00:07:36.567 --> 00:07:41.567
between my code of waiting
and stuff like that.

00:07:41.567 --> 00:07:44.267
It's fundamentally
more event-driven.

00:07:44.267 --> 00:07:47.234
This also results
in lower latency for our users,

00:07:47.234 --> 00:07:48.601
so tasks are lightweight.

00:07:48.601 --> 00:07:51.033
They're about three times faster
than our Datastore.

00:07:51.033 --> 00:07:52.434
So if you want to save a task

00:07:52.434 --> 00:07:54.200
to describe to do some work
in the future,

00:07:54.200 --> 00:07:56.100
it's really fast.

00:07:56.100 --> 00:07:57.868
Tasks are also reliable.

00:07:57.868 --> 00:07:59.267
So what I was saying before--

00:07:59.267 --> 00:08:01.934
once you write a task,
the guarantee is that

00:08:01.934 --> 00:08:03.501
it will eventually complete,

00:08:03.501 --> 00:08:05.968
depending on
what your code does.

00:08:05.968 --> 00:08:09.367
But if you have working code,
it will eventually complete.

00:08:09.367 --> 00:08:12.167
And tasks are scalable.

00:08:12.167 --> 00:08:14.501
The storage of new tasks
has no contention.

00:08:14.501 --> 00:08:17.267
You're just appending to the end
of a very large list.

00:08:17.267 --> 00:08:20.601
So you can keep it pending
all the way to the end.

00:08:20.601 --> 00:08:23.434
We can store those tasks
across a lot of machines.

00:08:23.434 --> 00:08:26.067
So we can scale that out
very well.

00:08:26.067 --> 00:08:27.868
And the best part is that
you can parallelize work

00:08:27.868 --> 00:08:29.167
with multiple workers.

00:08:29.167 --> 00:08:30.534
You can have a lot of workers
pulling off

00:08:30.534 --> 00:08:33.501
the front of a queue
and completing work,

00:08:33.501 --> 00:08:34.868
getting it done in parallel,

00:08:34.868 --> 00:08:38.234
so you can do a lot
of computation simultaneously.

00:08:38.234 --> 00:08:39.534
And there are a lot of features
you can add

00:08:39.534 --> 00:08:44.200
to this basic concept.

00:08:44.200 --> 00:08:46.167
So historically,
what is a task queue?

00:08:46.167 --> 00:08:49.200
Well, UNIX had "at"
and "batch" commands.

00:08:49.200 --> 00:08:53.400
Do this work, run this shell
command at this time.

00:08:53.400 --> 00:08:55.934
And they had ways
of evaluating a queue,

00:08:55.934 --> 00:08:57.767
adding things to a queue.

00:08:57.767 --> 00:09:00.334
It's kind of an old style
of doing it.

00:09:00.334 --> 00:09:01.767
What you hear about--
a lot of people

00:09:01.767 --> 00:09:03.200
in the Django
and Rails community do

00:09:03.200 --> 00:09:06.000
is they use a kind of
poor man's queue.

00:09:06.000 --> 00:09:08.067
Some people have done this
on App Engine already.

00:09:08.067 --> 00:09:12.167
You use cron jobs and flat files
to build a queue.

00:09:12.167 --> 00:09:15.267
You append an entity
or a line to a queue

00:09:15.267 --> 00:09:17.334
for every time you need
to do some more work.

00:09:17.334 --> 00:09:19.033
And then every so often,
you wake up

00:09:19.033 --> 00:09:22.801
and just consume the whole file
and the whole queue

00:09:22.801 --> 00:09:26.033
with a cron job.

00:09:26.033 --> 00:09:28.767
For a lot of people,
that's what a task queue is.

00:09:28.767 --> 00:09:30.534
But there are a lot of
reliability

00:09:30.534 --> 00:09:32.067
and scalability issues here.

00:09:32.067 --> 00:09:33.367
You're locked into
a single machine

00:09:33.367 --> 00:09:34.968
that contains a queue.

00:09:34.968 --> 00:09:37.501
It's very hard
to parallelize work.

00:09:37.501 --> 00:09:40.534
And there are
some failure scenarios

00:09:40.534 --> 00:09:42.033
that you have to deal with.

00:09:42.033 --> 00:09:46.634
It works, but it's really
not ideal.

00:09:46.634 --> 00:09:49.067
But there are other
task queue systems out there

00:09:49.067 --> 00:09:50.501
that are good,

00:09:50.501 --> 00:09:51.667
and let's talk about
some of them.

00:09:51.667 --> 00:09:54.000
So there's a long list.

00:09:54.000 --> 00:09:58.734
It's a very, very long
laundry list.

00:09:58.734 --> 00:10:00.033
Here I have *MQ.

00:10:00.033 --> 00:10:02.033
*MQ is all the MQ systems
out there.

00:10:02.033 --> 00:10:04.868
There's ActiveMQ from Apache.

00:10:04.868 --> 00:10:07.234
There's RabbitMQ,
which is Erlang.

00:10:07.234 --> 00:10:11.200
There's Microsoft MQ,
WebSphere MQ.

00:10:11.200 --> 00:10:13.868
Basically any vendor MQ.

00:10:13.868 --> 00:10:15.901
There's Amazon Simple Queue
Service,

00:10:15.901 --> 00:10:17.634
Azure queues, from Microsoft.

00:10:17.634 --> 00:10:20.834
There's TheSchwartz
open-source queuing service.

00:10:20.834 --> 00:10:23.400
Twisted has some queue-like
asynchronous stuff in it.

00:10:23.400 --> 00:10:27.200
Starling is an in-memory queue
that's getting kind of popular.

00:10:27.200 --> 00:10:29.033
Beanstalkd is kind of
the same idea.

00:10:29.033 --> 00:10:31.501
And there's a lot
of others.

00:10:31.501 --> 00:10:33.200
But one thing I'd like
to point out here is that

00:10:33.200 --> 00:10:36.300
task queues are often conflated
with pub-sub messaging.

00:10:36.300 --> 00:10:39.300
People talk to us and they say,
"We want task queues."

00:10:39.300 --> 00:10:41.300
Or no, "We want pub-sub,"
or "We want JMS."

00:10:41.300 --> 00:10:43.334
Or, "I want a messaging system."

00:10:43.334 --> 00:10:44.367
It's like, well,
what do you want?

00:10:44.367 --> 00:10:45.901
What do you
actually want to do?

00:10:45.901 --> 00:10:48.567
So let's define what we're
actually talking about here

00:10:48.567 --> 00:10:50.534
so you can understand
the motivation of our API

00:10:50.534 --> 00:10:52.934
and why it is the way it is.

00:10:52.934 --> 00:10:57.200
So queuing systems
maximize data throughput.

00:10:57.200 --> 00:10:58.634
That's what they're for.

00:10:58.634 --> 00:11:01.734
Routers, data pipelines--
these are queuing systems.

00:11:01.734 --> 00:11:05.300
They fully saturate a network,
CPU, disk.

00:11:05.300 --> 00:11:08.400
They're meant for throughput.
That's the goal.

00:11:08.400 --> 00:11:11.467
Pub-sub systems are about
transactions and decoupling.

00:11:11.467 --> 00:11:13.167
Now, what do I mean
by "decoupling"?

00:11:13.167 --> 00:11:16.567
I mean you can have
consumers and producers

00:11:16.567 --> 00:11:17.901
on the same network,

00:11:17.901 --> 00:11:21.300
maybe multiple consumers
for a single producer,

00:11:21.300 --> 00:11:23.100
and the consumers and producers
don't need to know

00:11:23.100 --> 00:11:24.267
anything about each other.

00:11:24.267 --> 00:11:26.234
You can change the software
on either side.

00:11:26.234 --> 00:11:28.167
They have some contract
between them

00:11:28.167 --> 00:11:29.667
that lets them
be decoupled.

00:11:29.667 --> 00:11:33.367
Businesses really like this
because it lets them be agile

00:11:33.367 --> 00:11:35.367
and change vendors
and, you know,

00:11:35.367 --> 00:11:37.234
get around lock
and all this other stuff.

00:11:37.234 --> 00:11:39.200
Pub-sub is very useful
for that.

00:11:39.200 --> 00:11:42.968
It's good at large numbers of
small transactions per second.

00:11:42.968 --> 00:11:44.567
It's good for one-to-many
fan-out.

00:11:44.567 --> 00:11:46.167
When you're changing
the receivers, you're adding

00:11:46.167 --> 00:11:48.334
new business logic,
new business rules.

00:11:48.334 --> 00:11:52.267
And a lot of the time,
pub-sub systems, MQ systems

00:11:52.267 --> 00:11:54.601
have a lot of extra things
added to them.

00:11:54.601 --> 00:11:57.400
They're got filtering.
They've got guaranteed ordering.

00:11:57.400 --> 00:11:59.501
You can have two-phase commit
in there.

00:11:59.501 --> 00:12:02.167
There's a whole slew of other
rules and transformations

00:12:02.167 --> 00:12:05.133
that a lot
of these systems do.

00:12:05.133 --> 00:12:07.534
And that's all very
interesting stuff.

00:12:07.534 --> 00:12:09.701
There are some really good
use cases for it.

00:12:09.701 --> 00:12:12.934
But our goal here is queuing,
not pub-sub.

00:12:12.934 --> 00:12:14.901
We're looking
for high throughput,

00:12:14.901 --> 00:12:16.434
maximizing data throughput.

00:12:16.434 --> 00:12:23.367
So that's an important
distinction to keep in mind.

00:12:23.367 --> 00:12:26.367
So here's how
a normal task-queuing system,

00:12:26.367 --> 00:12:28.567
or MQ system, works.

00:12:28.567 --> 00:12:31.834
You have a queue,
newer tasks going to the right.

00:12:31.834 --> 00:12:34.901
You have some queue mediator
or a set of queue mediators.

00:12:34.901 --> 00:12:37.100
They look
at the head of the queue.

00:12:37.100 --> 00:12:39.133
They pop off that head.

00:12:39.133 --> 00:12:41.434
And then you have
a series of workers

00:12:41.434 --> 00:12:46.234
that are polling the queue
mediator for new tasks.

00:12:46.234 --> 00:12:49.400
So worker A, B, and C constantly
are contacting the mediator

00:12:49.400 --> 00:12:51.033
saying, "Hey, do you have
any work yet?"

00:12:51.033 --> 00:12:52.801
And the queue mediator says no,

00:12:52.801 --> 00:12:54.334
and then it goes
back and forth.

00:12:54.334 --> 00:12:56.000
And the workers
are always running.

00:12:56.000 --> 00:12:58.000
They're always polling.

00:12:58.000 --> 00:13:00.467
They're sitting there
doing nothing most of the time,

00:13:00.467 --> 00:13:03.434
depending on what
your queue is doing.

00:13:03.434 --> 00:13:07.133
Now, polling has problems.
A lot of problems.

00:13:07.133 --> 00:13:09.234
I--Yeah, I don't like polling
at all.

00:13:09.234 --> 00:13:12.267
The worker sits in a loop
polling the front of the queue.

00:13:12.267 --> 00:13:13.434
That means
it's not event-driven.

00:13:13.434 --> 00:13:14.601
That's wasted work.

00:13:14.601 --> 00:13:16.434
That's just a lot of,
hey, you know--

00:13:16.434 --> 00:13:18.634
like a little child tugging
at somebody's shirt

00:13:18.634 --> 00:13:19.901
or something like that.

00:13:19.901 --> 00:13:23.300
It's just a waste of time
for the worker.

00:13:23.300 --> 00:13:25.734
The workers stay resident
when there's no work to do,

00:13:25.734 --> 00:13:27.467
and that just wastes
machine resources.

00:13:27.467 --> 00:13:30.234
You have all these workers for
peak load that you don't need

00:13:30.234 --> 00:13:32.667
just sitting there
doing nothing.

00:13:32.667 --> 00:13:35.567
Another problem is that
it's a fixed number of workers.

00:13:35.567 --> 00:13:37.300
You have this IT burden.

00:13:37.300 --> 00:13:39.400
Admins have to manually add
more workers

00:13:39.400 --> 00:13:41.033
to keep up with the queue

00:13:41.033 --> 00:13:43.067
or the queue
grows without bounds.

00:13:43.067 --> 00:13:44.734
So you need to sit there
and say, "Oh, man,

00:13:44.734 --> 00:13:47.067
"at noon my queue gets
to 1 million items long.

00:13:47.067 --> 00:13:48.601
"I really need to spool up
a few more workers

00:13:48.601 --> 00:13:49.868
during that time."

00:13:49.868 --> 00:13:52.634
Or you just leave
those workers on all the time.

00:13:52.634 --> 00:13:54.667
So you've got this
management bird in your head

00:13:54.667 --> 00:13:56.067
you have to think about
with polling

00:13:56.067 --> 00:13:58.300
to make sure that you kind of
keep things under control.

00:13:58.300 --> 00:14:01.501
And you have to yourself,
as an administrator,

00:14:01.501 --> 00:14:04.601
be constantly readjusting
your worker pool

00:14:04.601 --> 00:14:06.667
to properly handle
your task queue

00:14:06.667 --> 00:14:10.267
to make sure it's
low latency.

00:14:10.267 --> 00:14:12.033
And the other problem
with polling is that

00:14:12.033 --> 00:14:14.167
there's a limited amount
of optimization possible.

00:14:14.167 --> 00:14:16.801
So many systems
do fake a polling interface

00:14:16.801 --> 00:14:19.567
with something that's
event-driven under the hood.

00:14:19.567 --> 00:14:21.334
Essentially they use
long-live connections that say,

00:14:21.334 --> 00:14:22.868
"Hey, give me something
whenever you're ready

00:14:22.868 --> 00:14:24.200
and I'll just hang
on this connection."

00:14:24.200 --> 00:14:26.334
And then the queue mediator
eventually gets a task,

00:14:26.334 --> 00:14:28.634
and then sends it
down the wire.

00:14:28.634 --> 00:14:31.300
So you can reduce
some of the latency

00:14:31.300 --> 00:14:34.033
from when a task is enqueued
to when it's executed.

00:14:34.033 --> 00:14:36.467
But you still have this
fundamental problem of workers.

00:14:36.467 --> 00:14:41.701
Number of workers,
queue size, et cetera.

00:14:41.701 --> 00:14:45.601
So how is our task queue
different?

00:14:45.601 --> 00:14:48.033
We don't poll, we push.

00:14:48.033 --> 00:14:50.100
We push tasks to your app.

00:14:50.100 --> 00:14:53.634
There's no polling
necessary.

00:14:53.634 --> 00:14:55.567
That's--That's big.

00:14:55.567 --> 00:14:58.801
The second big thing is that
we are doing tasks

00:14:58.801 --> 00:15:00.300
as web hooks.

00:15:00.300 --> 00:15:01.868
So when you want
to execute a task,

00:15:01.868 --> 00:15:03.667
we're using web hooks
as the interface.

00:15:03.667 --> 00:15:04.801
Now, what are web hooks?

00:15:04.801 --> 00:15:07.767
It's a RESTful,
push-based interface

00:15:07.767 --> 00:15:09.267
for doing work.

00:15:09.267 --> 00:15:10.868
It's a concept that's used
outside of Google,

00:15:10.868 --> 00:15:12.300
outside of App Engine.

00:15:12.300 --> 00:15:14.667
But we really like the style,
and a lot of our coming APIs

00:15:14.667 --> 00:15:15.901
are gonna use this style.

00:15:15.901 --> 00:15:17.300
You should go to this
Wikipedia page

00:15:17.300 --> 00:15:18.801
if you want to know more.

00:15:18.801 --> 00:15:22.434
Essentially, what you're doing
is describing an interface

00:15:22.434 --> 00:15:23.968
through simple HTTP.

00:15:23.968 --> 00:15:28.234
A set of URLs, post body, says
here's the input interface

00:15:28.234 --> 00:15:30.634
and here's the output interface
that I expect.

00:15:30.634 --> 00:15:35.133
So your standard communication
interface becomes HTTP.

00:15:35.133 --> 00:15:37.267
That's all web hooks are.

00:15:37.267 --> 00:15:39.300
A great example
of a web hook system is

00:15:39.300 --> 00:15:42.267
Google Code has a post-commit
web hook,

00:15:42.267 --> 00:15:44.868
so that anytime someone commits
to the SVN repository,

00:15:44.868 --> 00:15:47.767
you can get an HTTP post
to your web server

00:15:47.767 --> 00:15:50.234
so you can do all kinds
of clean up or send emails

00:15:50.234 --> 00:15:51.868
or do announcement,
et cetera.

00:15:51.868 --> 00:15:54.133
So it's a really easy way
to interface

00:15:54.133 --> 00:15:58.267
server-to-server communication
to integrate systems together.

00:15:58.267 --> 00:16:01.367
So web hooks are really great.

00:16:01.367 --> 00:16:05.033
Now, in App Engine,
we're doing tasks as web hooks.

00:16:05.033 --> 00:16:07.100
So a task--
When you define a task,

00:16:07.100 --> 00:16:08.701
you're just defining
an HTTP request

00:16:08.701 --> 00:16:10.534
you want to have executed.

00:16:10.534 --> 00:16:14.200
You give us the URL,
the body of the HTTP request,

00:16:14.200 --> 00:16:17.934
the method, headers,
query string parameters,

00:16:17.934 --> 00:16:19.901
content type, et cetera.

00:16:19.901 --> 00:16:22.100
You just define, hey,
I want this request later on

00:16:22.100 --> 00:16:24.567
on this handler.

00:16:24.567 --> 00:16:25.968
You enqueue that task,

00:16:25.968 --> 00:16:28.567
and then we send your app
the request later.

00:16:28.567 --> 00:16:31.968
And then the interface is,
if your web hook--

00:16:31.968 --> 00:16:35.133
So we make that request with all
the parameters you wanted.

00:16:35.133 --> 00:16:38.901
And then if your web hook
gives us back an HTTP 200 okay,

00:16:38.901 --> 00:16:40.934
then it's done.

00:16:40.934 --> 00:16:43.801
Anything else, we will say
that the task failed,

00:16:43.801 --> 00:16:46.133
and we'll back off
and retry later.

00:16:46.133 --> 00:16:48.000
So it's really simple.

00:16:48.000 --> 00:16:49.901
The interface is whatever
you want it to be.

00:16:49.901 --> 00:16:54.467
It's just HTTP.

00:16:54.467 --> 00:16:56.567
So here's a really
simple example of this.

00:16:56.567 --> 00:16:59.367
I've defined--
This is Python.

00:16:59.367 --> 00:17:02.467
I've defined
a web request handler here.

00:17:02.467 --> 00:17:06.100
It takes posts,
and it's using our mail API

00:17:06.100 --> 00:17:07.567
to send an email.

00:17:07.567 --> 00:17:10.901
So it's sending an email
from me@example.com.

00:17:10.901 --> 00:17:13.501
It's pulling some
query string posts

00:17:13.501 --> 00:17:18.000
for some form-encoded
parameters.

00:17:18.000 --> 00:17:20.734
So I get the "to,"
a "subject," and the "body."

00:17:20.734 --> 00:17:22.267
And I just send the mail.
That's it.

00:17:22.267 --> 00:17:23.567
It's just a really simple--
You could have a form

00:17:23.567 --> 00:17:25.834
that drives this,
very simply.

00:17:25.834 --> 00:17:28.267
And then to enqueue a task,
I just say, oh, yeah,

00:17:28.267 --> 00:17:30.734
here's where my handle is--
handler is.

00:17:30.734 --> 00:17:33.400
Here are the parameters
that I want to encode.

00:17:33.400 --> 00:17:35.501
And that's it. Add it.

00:17:35.501 --> 00:17:40.200
So this calls this,
just sometime in the future.

00:17:40.200 --> 00:17:42.200
Now, why would you want
to do it in this case?

00:17:42.200 --> 00:17:44.667
Well, our mail API
is synchronous.

00:17:44.667 --> 00:17:47.501
So when you send an email,
you're looking at, you know,

00:17:47.501 --> 00:17:52.167
500 milliseconds, one second,
two seconds to send that email.

00:17:52.167 --> 00:17:54.000
If a user's clicking
a button,

00:17:54.000 --> 00:17:55.834
that could be
a bad experience for them.

00:17:55.834 --> 00:17:58.167
So by doing it this way,
you're looking at, you know,

00:17:58.167 --> 00:18:02.868
10, 20, 30 milliseconds
for getting the task enqueued,

00:18:02.868 --> 00:18:04.834
and you're on your way.

00:18:04.834 --> 00:18:10.133
It's a lot faster.

00:18:10.133 --> 00:18:12.701
So, demo.

00:18:12.701 --> 00:18:16.200
This is a funny demo because
I don't have Gmail loaded.

00:18:16.200 --> 00:18:17.667
So I'll just show you
how fast it is.

00:18:17.667 --> 00:18:20.167
I don't know if you've ever
used mail sending,

00:18:20.167 --> 00:18:22.834
but I'm just gonna send
an email to myself.

00:18:22.834 --> 00:18:26.501
So by the time that reloads,
I've sent an email to myself.

00:18:26.501 --> 00:18:27.834
It's really fast.

00:18:27.834 --> 00:18:29.701
That's me enqueuing a task
over and over again.

00:18:29.701 --> 00:18:30.934
I'm not gonna bring up my email.

00:18:30.934 --> 00:18:32.901
I have all kinds of
personal conversations

00:18:32.901 --> 00:18:34.400
with my grandma.

00:18:34.400 --> 00:18:36.801
So I'm sorry I didn't
clear those out before today.

00:18:36.801 --> 00:18:39.901
But you get the idea,
it's quicker.

00:18:39.901 --> 00:18:41.734
Um--
Oh, wrong slide.

00:18:41.734 --> 00:18:44.601
All right...

00:18:44.601 --> 00:18:48.701
So let's talk about
how our task queue works.

00:18:48.701 --> 00:18:50.834
So it looks exactly the same,
except we've inverted

00:18:50.834 --> 00:18:52.367
a lot of the relationships here.

00:18:52.367 --> 00:18:54.734
We still have a task queue.

00:18:54.734 --> 00:18:56.434
We still have
a queue mediator,

00:18:56.434 --> 00:18:59.801
but the arrows go
in the other direction.

00:18:59.801 --> 00:19:02.434
The queue mediator's
pulling off of head.

00:19:02.434 --> 00:19:04.868
And then--
Whenever there's work.

00:19:04.868 --> 00:19:07.901
And then it's sending it
to your request handler.

00:19:07.901 --> 00:19:09.901
If there's no work to do--

00:19:09.901 --> 00:19:11.601
you don't have any
request handlers active,

00:19:11.601 --> 00:19:13.167
there are no active threads--

00:19:13.167 --> 00:19:15.734
you're not wasting
any resources on workers

00:19:15.734 --> 00:19:18.534
that aren't doing anything.

00:19:18.534 --> 00:19:20.801
Conversely, if you have
more work to do,

00:19:20.801 --> 00:19:23.033
we just spool up
more threads.

00:19:23.033 --> 00:19:24.567
It's just the way
that App Engine works

00:19:24.567 --> 00:19:25.701
handling HTTP requests.

00:19:25.701 --> 00:19:27.667
We do the exact same thing
for tasks.

00:19:27.667 --> 00:19:29.434
So if you have more work to do
in your queues,

00:19:29.434 --> 00:19:33.801
we will spool up a lot
of threads to consume them.

00:19:33.801 --> 00:19:35.300
And as soon as
that work is done,

00:19:35.300 --> 00:19:37.634
we'll pull those threads
back in again.

00:19:37.634 --> 00:19:40.400
And they don't cost anything
to have--

00:19:40.400 --> 00:19:43.100
It doesn't cost anything
to have those threads resident.

00:19:43.100 --> 00:19:46.033
So we're automatically adjusting
the worker threads

00:19:46.033 --> 00:19:47.267
based on the load

00:19:47.267 --> 00:19:48.501
to the needs
of your application

00:19:48.501 --> 00:19:51.601
and the needs of your queue.

00:19:51.601 --> 00:19:54.434
So you get to save
a lot of waste--

00:19:54.434 --> 00:19:56.567
Just from that inversion
of the model--

00:19:56.567 --> 00:20:02.734
from pull and pull to push.

00:20:02.734 --> 00:20:06.367
So yeah, we add worker threads,
depending on the workload.

00:20:06.367 --> 00:20:08.868
The maximum number of threads
depends on your throughput.

00:20:08.868 --> 00:20:11.567
So if your tasks take,
you know, 30 seconds each,

00:20:11.567 --> 00:20:13.734
you're gonna have a lower
overall throughput

00:20:13.734 --> 00:20:15.467
of tasks per second.

00:20:15.467 --> 00:20:20.501
But, yeah--but we have
a high maximum rate limit

00:20:20.501 --> 00:20:22.601
for tasks per second processed.

00:20:22.601 --> 00:20:25.868
So you can get
a lot of work done.

00:20:25.868 --> 00:20:27.734
And another thing that's great
is that it's just integrated

00:20:27.734 --> 00:20:29.534
into the admin console
as normal requests.

00:20:29.534 --> 00:20:31.734
So all the things that
you already use with App Engine

00:20:31.734 --> 00:20:34.467
to debug your requests
and look at your logs

00:20:34.467 --> 00:20:35.667
and figure out
how things work,

00:20:35.667 --> 00:20:37.767
they continue to work right now.

00:20:37.767 --> 00:20:39.934
The application and request logs
are still searchable.

00:20:39.934 --> 00:20:42.167
You can get all of your logs
from your request handlers.

00:20:42.167 --> 00:20:45.033
You can see the web hook
response codes:

00:20:45.033 --> 00:20:47.367
200 OK or 500s.

00:20:47.367 --> 00:20:48.434
You can scan through
your logs,

00:20:48.434 --> 00:20:50.634
do regex searches,
all that stuff.

00:20:50.634 --> 00:20:53.667
The dashboard statistics
and error-rate monitoring

00:20:53.667 --> 00:20:54.868
are still there.

00:20:54.868 --> 00:20:56.200
So one of the things
that's really great

00:20:56.200 --> 00:20:59.534
about our admin console is
we show per URL CPU usage

00:20:59.534 --> 00:21:00.868
on average.

00:21:00.868 --> 00:21:04.534
So using that exact same idea,
you can have a URL

00:21:04.534 --> 00:21:05.801
that's your mail worker,

00:21:05.801 --> 00:21:08.167
or a URL that's
your compute worker,

00:21:08.167 --> 00:21:10.133
and you can see, oh,
each one of my tasks

00:21:10.133 --> 00:21:13.167
is taking 200 milliseconds
of CPU time on average.

00:21:13.167 --> 00:21:17.634
You can use that same scheme
of URLs and URL monitoring

00:21:17.634 --> 00:21:21.634
to optimize your application.

00:21:21.634 --> 00:21:24.834
CPU...sorry, what?

00:21:24.834 --> 00:21:26.734
CPU cycle?

00:21:26.734 --> 00:21:29.601
man: [indistinct]

00:21:29.601 --> 00:21:31.400
Slatkin: Oh, 200 milliseconds
of CPU time.

00:21:31.400 --> 00:21:34.400
So...well,
ask me that question later.

00:21:34.400 --> 00:21:37.834
On App Engine, we define
resources in terms of CPU time.

00:21:37.834 --> 00:21:41.300
So it's a standard CPU.
Yeah.

00:21:41.300 --> 00:21:44.067
So...oh, yeah, the same thing
with error rates.

00:21:44.067 --> 00:21:45.434
We monitor error rates.

00:21:45.434 --> 00:21:47.033
So if you want to see
how many tasks in your queue

00:21:47.033 --> 00:21:49.534
have an error,
you get to see that too.

00:21:49.534 --> 00:21:51.834
So all the nice things
in the admin console,

00:21:51.834 --> 00:21:56.167
including graphs,
they're all there.

00:21:56.167 --> 00:21:59.100
So let's get into some details
and some more examples.

00:21:59.100 --> 00:22:01.267
So first of all
I want to warn you

00:22:01.267 --> 00:22:03.634
about something called
idempotence.

00:22:03.634 --> 00:22:05.901
Idempotence is a mouthful
of a word.

00:22:05.901 --> 00:22:08.000
If you're a programmer,
you've probably heard it before,

00:22:08.000 --> 00:22:10.734
but it took me a while to
remember exactly what it meant.

00:22:10.734 --> 00:22:13.868
Effectively, what idempotence is
is that, you know,

00:22:13.868 --> 00:22:16.801
we need to be able to run
the same tasks repeatedly

00:22:16.801 --> 00:22:19.033
without harmful effects.

00:22:19.033 --> 00:22:21.033
Or at least effects
that are acceptable,

00:22:21.033 --> 00:22:24.000
like sending a duplicate email.

00:22:24.000 --> 00:22:25.400
Now, why is this
necessary?

00:22:25.400 --> 00:22:27.801
It's necessary because
failure can happen at any time.

00:22:27.801 --> 00:22:30.868
With our Task Queue API,
we're giving you a guarantee

00:22:30.868 --> 00:22:33.801
that the task will be retried
until it's successful.

00:22:33.801 --> 00:22:39.167
But it's on you to actually
implement the task handler.

00:22:39.167 --> 00:22:42.033
So you need to keep in mind
that your tasks may run twice

00:22:42.033 --> 00:22:43.701
on accident--

00:22:43.701 --> 00:22:46.067
either because of your code,
or because of a failure

00:22:46.067 --> 00:22:47.767
of one of our systems,

00:22:47.767 --> 00:22:50.100
or even in good cases
without server failure,

00:22:50.100 --> 00:22:51.868
it's still theoretically
possible

00:22:51.868 --> 00:22:54.067
for a task to run twice.

00:22:54.067 --> 00:22:57.033
So you need to be idempotent.

00:22:57.033 --> 00:22:59.667
It's your responsibility
as the application developer

00:22:59.667 --> 00:23:01.834
to ensure that your tasks
are idempotent

00:23:01.834 --> 00:23:03.634
so that if you run
the same task twice,

00:23:03.634 --> 00:23:07.167
you have a way to figure out
either I did this before,

00:23:07.167 --> 00:23:10.434
or I can do this again,
and it's not harmful.

00:23:10.434 --> 00:23:12.667
[inaudible question]

00:23:12.667 --> 00:23:14.901
And they won't run in parallel.
That's a good question.

00:23:14.901 --> 00:23:18.434
One task is only running
on one thread at one time,

00:23:18.434 --> 00:23:21.267
but there are still
tiny little edge cases

00:23:21.267 --> 00:23:22.667
where it could be two.

00:23:22.667 --> 00:23:25.467
And we're getting into
some cookbook kind of stuff

00:23:25.467 --> 00:23:26.868
to make sure that
that doesn't happen too.

00:23:26.868 --> 00:23:29.067
But yeah,
you should be guaranteed

00:23:29.067 --> 00:23:31.601
that you have tasks running--

00:23:31.601 --> 00:23:34.100
Only one task should be running
in one place at a time.

00:23:34.100 --> 00:23:35.667
But yeah,
it's the same concept.

00:23:35.667 --> 00:23:39.467
Idempotent operations
require you to--

00:23:39.467 --> 00:23:44.000
Require your application
to allow for repeats,

00:23:44.000 --> 00:23:46.534
sometimes simultaneously.

00:23:46.534 --> 00:23:49.267
[inaudible question]

00:23:49.267 --> 00:23:51.400
Right, so the question is
could you use memcache

00:23:51.400 --> 00:23:52.834
to say I already did this?

00:23:52.834 --> 00:23:54.767
Or the Datastore--yeah, you
could do something like that.

00:23:54.767 --> 00:23:56.367
There are some latency issues
with doing that,

00:23:56.367 --> 00:23:57.534
but yeah, there's some--

00:23:57.534 --> 00:23:58.734
You could use memcache locks.

00:23:58.734 --> 00:24:00.167
You could use a few
other techniques

00:24:00.167 --> 00:24:04.634
to make sure that you don't
do things multiple times.

00:24:04.634 --> 00:24:07.334
But again, the burden is on you
as a developer.

00:24:07.334 --> 00:24:12.434
We don't do this for you,
but we give you tools to do it.

00:24:12.434 --> 00:24:14.467
Now let's talk about queues.

00:24:14.467 --> 00:24:15.634
We talked about tasks.

00:24:15.634 --> 00:24:18.734
We also have queues--
This is Task Queue API.

00:24:18.734 --> 00:24:22.300
So each task is added
to a single queue for execution.

00:24:22.300 --> 00:24:25.868
And we allow for multiple queues
in an application.

00:24:25.868 --> 00:24:27.133
So what is a queue?

00:24:27.133 --> 00:24:30.901
Well, a queue provides isolation
and separation of tasks.

00:24:30.901 --> 00:24:34.834
So that means that, you know,
it's a FIFO queue.

00:24:34.834 --> 00:24:37.901
Tasks enter at the back
and go to the front.

00:24:37.901 --> 00:24:40.767
Maybe one queue is slower
than some other queues.

00:24:40.767 --> 00:24:42.100
You want to have isolation
between them.

00:24:42.100 --> 00:24:44.667
Maybe you want one queue
to run faster than another.

00:24:44.667 --> 00:24:47.767
You can do all that
by configuring your queues.

00:24:47.767 --> 00:24:50.033
A queue in our system
is really just a line.

00:24:50.033 --> 00:24:53.234
It's a queue--a list of items
that have to progress

00:24:53.234 --> 00:24:54.434
in that order.

00:24:54.434 --> 00:24:57.033
And so if you care about
the ordering

00:24:57.033 --> 00:24:59.634
across multiple tasks
with different priorities,

00:24:59.634 --> 00:25:00.767
or something like that,

00:25:00.767 --> 00:25:02.968
you can use different queues
to control that.

00:25:02.968 --> 00:25:05.434
And you do it
with a queue.yaml file.

00:25:05.434 --> 00:25:07.033
So here's an example.
I have two queues--

00:25:07.033 --> 00:25:12.601
a mail_queue, which I only want
to have work 2,000 times a day,

00:25:12.601 --> 00:25:14.400
and I have a speedy_queue,
which I want running

00:25:14.400 --> 00:25:18.434
at a maximum consume rate
of five per second.

00:25:18.434 --> 00:25:20.701
So this is fundamentally
different than

00:25:20.701 --> 00:25:22.267
your normal task-queuing system.

00:25:22.267 --> 00:25:23.601
With a lot of task-queuing
systems,

00:25:23.601 --> 00:25:25.601
you split up new workers
when you think you need

00:25:25.601 --> 00:25:26.901
to do more work.

00:25:26.901 --> 00:25:28.167
On our side we're, like, hey,

00:25:28.167 --> 00:25:30.701
we're gonna run a lot
of tasks per second.

00:25:30.701 --> 00:25:33.901
You need to tell us
a reasonable maximum

00:25:33.901 --> 00:25:37.200
so that we don't
overload things.

00:25:37.200 --> 00:25:38.834
And in the future
we've thought about

00:25:38.834 --> 00:25:42.334
also putting a maximum number
of tasks in flight

00:25:42.334 --> 00:25:43.868
simultaneously per queue

00:25:43.868 --> 00:25:47.167
or a maximum burn rate
of CPU per queue--

00:25:47.167 --> 00:25:49.133
something you could specify
in this file.

00:25:49.133 --> 00:25:50.567
There's a lot of things
we could add here.

00:25:50.567 --> 00:25:54.067
We're still figuring out
what you guys want.

00:25:54.067 --> 00:25:55.767
But why would you actually
want to throttle?

00:25:55.767 --> 00:25:59.667
Well, you may want
to combine work periodically,

00:25:59.667 --> 00:26:02.634
execute the work in batches.

00:26:02.634 --> 00:26:04.734
You may want to ensure
the stability of your workload.

00:26:04.734 --> 00:26:06.434
Make sure that your CPU
and bandwidth

00:26:06.434 --> 00:26:08.934
and dollar spend
is consistent.

00:26:08.934 --> 00:26:11.400
You can use queue throttling
to make sure that

00:26:11.400 --> 00:26:12.968
you don't exceed the maximum
writes per second

00:26:12.968 --> 00:26:16.267
of a single entity group
in the Datastore.

00:26:16.267 --> 00:26:17.667
Make sure that you don't
overload a partner site

00:26:17.667 --> 00:26:20.100
with web service calls,
send two emails at a time,

00:26:20.100 --> 00:26:22.667
or hit your email quota.

00:26:22.667 --> 00:26:25.133
And it also just enables
the prioritization of work.

00:26:25.133 --> 00:26:27.767
So remember that tasks
are defined as a web hook.

00:26:27.767 --> 00:26:29.767
Tasks go to any URL.

00:26:29.767 --> 00:26:34.467
So you have as many receivers
as you want as you have URLS.

00:26:34.467 --> 00:26:36.968
So the routing of the tasks
is totally independent

00:26:36.968 --> 00:26:38.300
of the queues.

00:26:38.300 --> 00:26:41.000
The queues are just a way
of throttling your tasks.

00:26:41.000 --> 00:26:45.300
So think of queues as a way
to do flow control of your tasks

00:26:45.300 --> 00:26:47.934
that's independent of what
the tasks actually do.

00:26:47.934 --> 00:26:49.267
And to illustrate that

00:26:49.267 --> 00:26:52.734
I have this many-to-many
queue throttling diagram.

00:26:52.734 --> 00:26:55.601
So on the left side I have
the mail tasks--

00:26:55.601 --> 00:26:57.934
mail tasks, fetch tasks,
and compute tasks.

00:26:57.934 --> 00:27:00.234
They're totally different tasks.
They go to different handlers.

00:27:00.234 --> 00:27:02.567
Completely separate
from everything else.

00:27:02.567 --> 00:27:04.000
On the right side I have
a bunch of workers

00:27:04.000 --> 00:27:06.767
that handle each of those
corresponding tasks.

00:27:06.767 --> 00:27:09.167
Now, I have three queues
at a different rate.

00:27:09.167 --> 00:27:11.000
Each providing, say,
a different level of service,

00:27:11.000 --> 00:27:12.334
a different CPU burn

00:27:12.334 --> 00:27:14.968
depending on how important
I think something is.

00:27:14.968 --> 00:27:17.133
If I have a customer that
I think is really important,

00:27:17.133 --> 00:27:18.868
I want their email
to go out first.

00:27:18.868 --> 00:27:20.801
Other free customers,
I put them in the queue

00:27:20.801 --> 00:27:21.934
with the rest of the people
who have

00:27:21.934 --> 00:27:24.801
low-priority emails to send.

00:27:24.801 --> 00:27:28.033
So the queue, again,
is not bound to the task.

00:27:28.033 --> 00:27:29.133
They're independent.

00:27:29.133 --> 00:27:31.167
The queue is a way
of keeping track

00:27:31.167 --> 00:27:34.000
and controlling
your task's progress.

00:27:34.000 --> 00:27:38.033
So keep that in mind.

00:27:38.033 --> 00:27:40.300
All right, so let's do
some more concrete examples.

00:27:40.300 --> 00:27:42.767
The first one is Schema--

00:27:42.767 --> 00:27:45.534
So we did the first one,
which was sending mail.

00:27:45.534 --> 00:27:47.234
Another one is Schema migration.

00:27:47.234 --> 00:27:50.200
People have said, "How do I do a
Schema migration on App Engine?"

00:27:50.200 --> 00:27:51.767
We finally have
a decent answer for you.

00:27:51.767 --> 00:27:54.834
So before, you could use
a cron job

00:27:54.834 --> 00:27:56.734
to slowly iterate
through your entities,

00:27:56.734 --> 00:27:58.868
migrate them, and then store
your entity location

00:27:58.868 --> 00:28:00.601
in memcache.

00:28:00.601 --> 00:28:02.267
The problem with this
is that it was slow.

00:28:02.267 --> 00:28:04.767
You'd have to periodically wake
up, do a little bit of work,

00:28:04.767 --> 00:28:06.901
and then go back to sleep,
over and over.

00:28:06.901 --> 00:28:08.901
You couldn't parallelize
any of the work.

00:28:08.901 --> 00:28:11.767
Just generally it was slow.
It wasn't event driven.

00:28:11.767 --> 00:28:13.968
You had to do a lot
of waiting.

00:28:13.968 --> 00:28:17.534
Another option was to use
a remote_api or the bulkloader

00:28:17.534 --> 00:28:19.767
to dump the whole dataset
and re-upload it.

00:28:19.767 --> 00:28:22.868
That's slow--it uses a lot
of bandwidth, a lot of CPU.

00:28:22.868 --> 00:28:24.934
A lot of the time your
transformations are very tiny.

00:28:24.934 --> 00:28:26.767
Sometimes you can edit
your entities in place.

00:28:26.767 --> 00:28:28.267
You don't even have
to rewrite them.

00:28:28.267 --> 00:28:30.234
So there's just a lot of cost
associated

00:28:30.234 --> 00:28:34.067
with dumping
your entire Datastore.

00:28:34.067 --> 00:28:38.334
Now, with the task API,
it gets a lot easier.

00:28:38.334 --> 00:28:40.868
You define a handler to query
for the next "N" entities.

00:28:40.868 --> 00:28:43.133
You modify them,
update them in batch,

00:28:43.133 --> 00:28:44.801
and then enqueue a task
to resume

00:28:44.801 --> 00:28:46.567
after the current position.

00:28:46.567 --> 00:28:49.033
And a failure at any point
will cause a task

00:28:49.033 --> 00:28:50.267
to be retried later,

00:28:50.267 --> 00:28:52.767
picking up exactly
where you left off.

00:28:52.767 --> 00:28:54.534
So let's look at some code
to do this.

00:28:54.534 --> 00:28:57.033
So first of all I have--
Here's my really simple Schema.

00:28:57.033 --> 00:28:59.534
I have two model classes:

00:28:59.534 --> 00:29:02.200
a FirstUserKind,
a SecondUserKind.

00:29:02.200 --> 00:29:04.133
Initially I just had a name.

00:29:04.133 --> 00:29:06.634
Later I realized I want to
be able to sort my last names,

00:29:06.634 --> 00:29:09.100
so I have the SecondUserKind,

00:29:09.100 --> 00:29:11.300
with the first
and last name separated.

00:29:11.300 --> 00:29:12.934
Then I have two migration
functions.

00:29:12.934 --> 00:29:15.300
I have--give me
a SecondUserKind from the first,

00:29:15.300 --> 00:29:16.701
and vice versa.

00:29:16.701 --> 00:29:18.701
So you'll see that to get
a SecondUserKind,

00:29:18.701 --> 00:29:20.701
I just split on the space,

00:29:20.701 --> 00:29:22.667
and that gets me
the first and last name.

00:29:22.667 --> 00:29:24.501
And for the other way around,

00:29:24.501 --> 00:29:26.934
I just combined them together.

00:29:31.834 --> 00:29:33.367
So here's a migration function.

00:29:33.367 --> 00:29:35.701
This is inside
a web request handler

00:29:35.701 --> 00:29:38.467
just like any other
web request handler

00:29:38.467 --> 00:29:43.000
that would be receiving
the web hook call for the task.

00:29:43.000 --> 00:29:45.200
So I query--

00:29:45.200 --> 00:29:47.133
So here there's a few
hidden variables.

00:29:47.133 --> 00:29:48.601
I'll explain what they mean.

00:29:48.601 --> 00:29:51.801
So the from_kind is
the model class

00:29:51.801 --> 00:29:54.367
that I'm migrating from.

00:29:54.367 --> 00:29:57.601
So that'll be either
first or second kind.

00:29:57.601 --> 00:30:01.000
And then--so I will start
a query on those.

00:30:01.000 --> 00:30:02.801
If I have a position--

00:30:02.801 --> 00:30:04.734
Like, if I'm iterating
through the list,

00:30:04.734 --> 00:30:06.534
I have a start position
I'm keeping track of,

00:30:06.534 --> 00:30:09.801
so I'm doing a key query
to page through all my data

00:30:09.801 --> 00:30:11.234
as I do my migration.

00:30:11.234 --> 00:30:13.834
So that's what this "if" is.

00:30:13.834 --> 00:30:16.567
So then I grab ten
of the old entities.

00:30:16.567 --> 00:30:18.334
If there are
no old entities done--

00:30:18.334 --> 00:30:21.300
If there are no
old entities left, I'm done.

00:30:21.300 --> 00:30:23.734
Otherwise, I figure out,
you know,

00:30:23.734 --> 00:30:25.901
here are my old entities--
I get the next one.

00:30:25.901 --> 00:30:28.968
So this is where I'll start
for the next batch.

00:30:28.968 --> 00:30:30.901
And then I migrate
all my entities

00:30:30.901 --> 00:30:32.067
to create the new list.

00:30:32.067 --> 00:30:35.167
So migrate "X" for "X" in old.

00:30:35.167 --> 00:30:38.234
Then I put the new entities,
and I delete the old entities.

00:30:38.234 --> 00:30:40.033
And then I enqueue
another task

00:30:40.033 --> 00:30:42.801
to do the exact same
web request handler again,

00:30:42.801 --> 00:30:46.434
with the start
equal to my next position

00:30:46.434 --> 00:30:48.801
and then the kind being
the same migration path.

00:30:48.801 --> 00:30:53.267
So the same migration kind--
the "from" kind.

00:30:53.267 --> 00:30:55.667
Now, what's really interesting
about this example

00:30:55.667 --> 00:30:57.868
is it illustrates the important
order of operations

00:30:57.868 --> 00:30:59.434
for idempotence.

00:30:59.434 --> 00:31:03.234
So for instance, you'll notice
that I first migra--

00:31:03.234 --> 00:31:05.000
So first I query
for all the old ones,

00:31:05.000 --> 00:31:07.167
and then I put the new ones
in the Datastore.

00:31:07.167 --> 00:31:10.601
So if the query failed, I'll
start in exactly the same place.

00:31:10.601 --> 00:31:12.701
If putting the new ones
in the Datastore--

00:31:12.701 --> 00:31:14.801
Sorry, if the order
were reversed,

00:31:14.801 --> 00:31:17.834
and I deleted the old ones
before putting the new ones,

00:31:17.834 --> 00:31:19.300
I could lose data.

00:31:19.300 --> 00:31:22.200
So I want to migrate--
write the new ones,

00:31:22.200 --> 00:31:24.367
have a duplicate data
before I delete the old ones...

00:31:24.367 --> 00:31:26.968
so that we can have
failures there.

00:31:26.968 --> 00:31:28.434
Another thing that's
not clear from this

00:31:28.434 --> 00:31:30.133
is that in the migration
function,

00:31:30.133 --> 00:31:33.167
I need to make sure that
I can put the same entity twice.

00:31:33.167 --> 00:31:36.267
So if I migrate
from Schema One to Schema B--

00:31:36.267 --> 00:31:37.968
Schema A to Schema B--

00:31:37.968 --> 00:31:42.234
I can overwrite Schema B
if the task reruns

00:31:42.234 --> 00:31:43.734
over and over again.

00:31:43.734 --> 00:31:45.267
And that's idempotence--
to make sure that

00:31:45.267 --> 00:31:47.400
if there's a failure
during that put,

00:31:47.400 --> 00:31:50.501
that I don't have
duplicate data.

00:31:50.501 --> 00:31:53.300
And then finally,

00:31:53.300 --> 00:31:55.467
if the delete fails
halfway through, that's fine.

00:31:55.467 --> 00:31:56.868
I've already done a put,

00:31:56.868 --> 00:31:58.400
and my query won't
give me anything,

00:31:58.400 --> 00:32:02.868
so I'll just move on.

00:32:02.868 --> 00:32:07.000
So I can give you
a demo of this.

00:32:07.000 --> 00:32:09.300
So again, this is running
on App Spot.

00:32:09.300 --> 00:32:11.467
So here's Schema migrations.

00:32:11.467 --> 00:32:15.834
So I have a series of names:
Hungry Time, Fluffy Cat,

00:32:15.834 --> 00:32:18.334
Upside Down, Thunder Sandwich--
a bunch of weird ones.

00:32:18.334 --> 00:32:20.067
These are some names
of people I know.

00:32:20.067 --> 00:32:23.133
And I have a little script
to add names,

00:32:23.133 --> 00:32:25.400
and I can migrate
from the first entity kind

00:32:25.400 --> 00:32:26.934
to the second entity kind.

00:32:26.934 --> 00:32:30.400
So I'll just click the button,
it'll start migrating,

00:32:30.400 --> 00:32:32.400
and it's done.

00:32:32.400 --> 00:32:35.834
Now, you'll notice that
it returned a partial migration

00:32:35.834 --> 00:32:38.033
before the request was
even done.

00:32:38.033 --> 00:32:40.000
So let me go back
the other way

00:32:40.000 --> 00:32:41.033
to show you what
I'm talking about--

00:32:41.033 --> 00:32:42.634
how fast it is.

00:32:42.634 --> 00:32:43.934
Now let's go second to first.

00:32:43.934 --> 00:32:46.734
I click the button,
and it's already--

00:32:46.734 --> 00:32:48.834
three of them are
already done.

00:32:48.834 --> 00:32:51.634
And by the time I reload again,
now they're all done.

00:32:51.634 --> 00:32:54.901
So if you actually
go into--

00:32:54.901 --> 00:32:58.467
Here I have my request logs
to show you this in action.

00:32:58.467 --> 00:33:01.934
You'll see--

00:33:01.934 --> 00:33:05.634
You see, here's
my start migration request.

00:33:05.634 --> 00:33:09.000
And it's at 56 seconds.

00:33:09.000 --> 00:33:10.801
This is hard to read, but I'll
just explain it to you.

00:33:10.801 --> 00:33:17.334
So this served a 302 result
at 56.83 seconds.

00:33:17.334 --> 00:33:21.367
By 56.87 seconds--
40 milliseconds later--

00:33:21.367 --> 00:33:24.534
my migration task was
already in flight.

00:33:24.534 --> 00:33:30.534
And then after that,
at 56.95, right here,

00:33:30.534 --> 00:33:33.133
I've finally refreshed the page.

00:33:33.133 --> 00:33:34.934
That means that
we're dispatching the tasks

00:33:34.934 --> 00:33:36.234
to your workers

00:33:36.234 --> 00:33:38.400
before the browser completes
a round-trip.

00:33:38.400 --> 00:33:40.501
That's how fast
we're doing this.

00:33:40.501 --> 00:33:43.634
When you add a task,
it can actually be reentrant,

00:33:43.634 --> 00:33:48.133
so that the task is executing
before the API call comes back.

00:33:48.133 --> 00:33:51.167
So it's very fast.

00:33:56.501 --> 00:33:59.834
All right, so we have
two more little things to cover,

00:33:59.834 --> 00:34:01.000
and then we'll have
one more example,

00:34:01.000 --> 00:34:02.868
and then we'll get
to some questions.

00:34:02.868 --> 00:34:04.234
Let's keep going here.

00:34:04.234 --> 00:34:05.667
Another nice thing we have,

00:34:05.667 --> 00:34:08.334
which is an advanced feature
of Tasks, is we have an ETA.

00:34:08.334 --> 00:34:12.133
This is more of like the "at"
and "batch" UNIX commands.

00:34:12.133 --> 00:34:14.033
It's an estimated time
of arrival.

00:34:14.033 --> 00:34:16.667
It says how long until
the task should be executed.

00:34:16.667 --> 00:34:18.868
So I can add a task now and say,
hey, in five minutes

00:34:18.868 --> 00:34:20.934
I want to do this,
in one hour I want to do this,

00:34:20.934 --> 00:34:22.367
in a day I want to do this.

00:34:22.367 --> 00:34:25.000
This is very different
than a visibility time-out.

00:34:25.000 --> 00:34:26.501
Visibility time-outs
are something you hear about

00:34:26.501 --> 00:34:30.567
in Azure Queues
and Simple Queue Service

00:34:30.567 --> 00:34:31.868
and a few others.

00:34:31.868 --> 00:34:34.200
Visibility time-outs
are more about eliminating

00:34:34.200 --> 00:34:37.334
possibly overlapping workers.

00:34:37.334 --> 00:34:40.067
This is unrelated to those.

00:34:40.067 --> 00:34:42.534
So this is just useful
for defining work to do

00:34:42.534 --> 00:34:44.534
in the relatively near future.

00:34:44.534 --> 00:34:46.133
It's more fine-grained

00:34:46.133 --> 00:34:49.934
and gives you more
programmatic control than cron.

00:34:49.934 --> 00:34:52.334
And it has a bunch
of useful things you can do.

00:34:52.334 --> 00:34:56.267
You can periodically clear
a cache, flush a buffer,

00:34:56.267 --> 00:34:59.000
report some incremental results,
prioritize some tasks

00:34:59.000 --> 00:35:01.934
based on a schedule
with a short time window

00:35:01.934 --> 00:35:05.300
or based on a schedule of some
algorithm you've developed.

00:35:05.300 --> 00:35:08.200
I can think of a bunch of 'em,
but you know,

00:35:08.200 --> 00:35:10.234
if you know that, oh,
five minutes from now,

00:35:10.234 --> 00:35:11.734
I need to wake up
and do something,

00:35:11.734 --> 00:35:13.167
that's what an ETA is good for,

00:35:13.167 --> 00:35:15.467
without having to have
another cron job sitting there

00:35:15.467 --> 00:35:16.968
polling for new work to do.

00:35:16.968 --> 00:35:22.534
So again, ETAs are supposed
to eliminate polling.

00:35:22.534 --> 00:35:24.501
Another thing that we provide
are names.

00:35:24.501 --> 00:35:28.334
So each task has a unique name
given by the app.

00:35:28.334 --> 00:35:29.801
If you don't give us
an app--

00:35:29.801 --> 00:35:32.133
If the app doesn't
give us a name,

00:35:32.133 --> 00:35:35.434
then we will auto-generate
an ID for you.

00:35:35.434 --> 00:35:38.467
But what's interesting is that
if you give us this name,

00:35:38.467 --> 00:35:40.200
we'll do something cool
with it.

00:35:40.200 --> 00:35:41.968
So when the task is done,

00:35:41.968 --> 00:35:44.334
we'll actually tombstone
that task for a few days.

00:35:44.334 --> 00:35:47.567
We'd like to make that
configurable, eventually.

00:35:47.567 --> 00:35:48.734
But what this does is,

00:35:48.734 --> 00:35:51.000
if you add another task
with the same name,

00:35:51.000 --> 00:35:52.334
you'll get an error.

00:35:52.334 --> 00:35:56.133
So this lets you enforce
"only-once" semantics.

00:35:56.133 --> 00:35:57.367
So why is that useful?

00:35:57.367 --> 00:35:59.367
Well, I showed you
my Schema migrate button.

00:35:59.367 --> 00:36:02.133
My dataset's kind of trivial.
It doesn't matter.

00:36:02.133 --> 00:36:04.300
What if I click that button
twice in a row really fast?

00:36:04.300 --> 00:36:05.667
What would happen?

00:36:05.667 --> 00:36:07.601
I'd have a bunch of workers
working in parallel

00:36:07.601 --> 00:36:11.100
trying to hurt each other,
potentially breaking my system.

00:36:11.100 --> 00:36:15.534
So with only-once semantics,

00:36:15.534 --> 00:36:18.033
I can make it so that
the first task I insert

00:36:18.033 --> 00:36:21.167
to kick off the process
will only ever be inserted once.

00:36:21.167 --> 00:36:23.067
And if it's inserted again,
you'll get an error.

00:36:23.067 --> 00:36:27.200
So I can be guaranteed that
the beginning of this flow

00:36:27.200 --> 00:36:30.901
or some part of task flow
will only happen once.

00:36:30.901 --> 00:36:34.667
So, you know, migrate the Schema
for these entities once,

00:36:34.667 --> 00:36:38.701
and only once.

00:36:38.701 --> 00:36:41.367
Okay, so now let's have
another concrete example

00:36:41.367 --> 00:36:42.701
of some of the things
you can do.

00:36:42.701 --> 00:36:44.334
I'm trying to just give you
a taste for these--

00:36:44.334 --> 00:36:48.067
these various options.

00:36:48.067 --> 00:36:52.167
So this one's...
it's a little more complicated.

00:36:52.167 --> 00:36:56.701
So the other ones were
kind of about offline work,

00:36:56.701 --> 00:37:01.767
talking about Schema migration
or latency elimination.

00:37:01.767 --> 00:37:04.467
This one's load elimination.

00:37:04.467 --> 00:37:06.100
This is a way of making your app
more efficient,

00:37:06.100 --> 00:37:10.367
more cost-effective
using the Task Queue.

00:37:10.367 --> 00:37:12.701
So what is a write-behind cache?

00:37:12.701 --> 00:37:15.667
So a write-behind cache
minimizes writes

00:37:15.667 --> 00:37:18.000
by repeatedly flushing
a cache.

00:37:18.000 --> 00:37:22.934
So essentially, what you do is
you write new data--

00:37:22.934 --> 00:37:25.767
modified data--to the cache,
and only to the cache.

00:37:25.767 --> 00:37:28.767
Like an in-memory cache.

00:37:28.767 --> 00:37:30.367
And then you periodically
read that cache

00:37:30.367 --> 00:37:32.667
and persist it to disk.

00:37:32.667 --> 00:37:34.133
Now, why would you want
to do this?

00:37:34.133 --> 00:37:37.334
Well, the benefit is that
database writes are no longer--

00:37:37.334 --> 00:37:41.067
they no longer increase
as a function of user traffic.

00:37:41.067 --> 00:37:43.634
So you can actually put your
database writes on a schedule.

00:37:43.634 --> 00:37:45.767
Say, hey, I don't want to do
a database write

00:37:45.767 --> 00:37:47.734
more than once per second.

00:37:47.734 --> 00:37:50.033
So then you can turn
100 cache writes per second

00:37:50.033 --> 00:37:52.000
into one database write
per second.

00:37:52.000 --> 00:37:55.567
You've somehow suddenly
decoupled your traffic

00:37:55.567 --> 00:37:57.868
from your database
load patterns.

00:37:57.868 --> 00:37:59.567
So that gives you a huge amount
of scalability

00:37:59.567 --> 00:38:02.934
that you couldn't do before.

00:38:02.934 --> 00:38:04.834
Now, there's a problem.

00:38:04.834 --> 00:38:07.934
There's a small time window
where you can lose the data

00:38:07.934 --> 00:38:09.834
that was in the cache.

00:38:09.834 --> 00:38:10.934
But if you think about it,

00:38:10.934 --> 00:38:12.100
there's always
a small time window

00:38:12.100 --> 00:38:13.434
where you can lose this data.

00:38:13.434 --> 00:38:14.901
The data can be lost
over the wire

00:38:14.901 --> 00:38:16.033
at any point in time.

00:38:16.033 --> 00:38:18.334
The response can
not get back to the client.

00:38:18.334 --> 00:38:21.868
Even a Datastore transaction
can fail halfway through.

00:38:21.868 --> 00:38:23.734
It would roll back
in that case.

00:38:23.734 --> 00:38:25.868
But, you know,
you are always dealing

00:38:25.868 --> 00:38:27.334
with small failure windows.

00:38:27.334 --> 00:38:33.400
So I think it plays well
into those same failure windows.

00:38:33.400 --> 00:38:36.667
Now, I have a diagram
to show you how this works.

00:38:36.667 --> 00:38:40.033
This is a sequence diagram--
a UML-style sequence diagram.

00:38:40.033 --> 00:38:41.434
If you're not familiar
with this,

00:38:41.434 --> 00:38:44.567
time goes both down and right,
kind of.

00:38:44.567 --> 00:38:47.434
But yeah, time is going towards
the bottom of the screen.

00:38:47.434 --> 00:38:50.534
So the user is using
a web application.

00:38:50.534 --> 00:38:53.934
And they do some
data modification.

00:38:53.934 --> 00:38:56.767
That goes to a request handler,
which writes into the cache,

00:38:56.767 --> 00:38:58.934
which is a very fast
operation.

00:38:58.934 --> 00:39:02.767
Then they add a task, which is
also a very fast operation.

00:39:02.767 --> 00:39:05.667
And remember again,
the cache is super-fast

00:39:05.667 --> 00:39:06.968
for writes and reads.

00:39:06.968 --> 00:39:09.067
And adding tasks
has no contention--

00:39:09.067 --> 00:39:10.067
also very fast.

00:39:10.067 --> 00:39:12.067
So it scales very well.

00:39:12.067 --> 00:39:14.167
Then we return a response
to the user

00:39:14.167 --> 00:39:16.534
either using the cache
as the Datastore--

00:39:16.534 --> 00:39:20.067
the cache
as the source of record--

00:39:20.067 --> 00:39:22.567
or just telling the user
to wait a little while.

00:39:22.567 --> 00:39:24.234
Now, sometime in the future,

00:39:24.234 --> 00:39:27.434
the Task Queue
will dispatch that task.

00:39:27.434 --> 00:39:29.934
The tasks will go
to a request handler.

00:39:29.934 --> 00:39:31.000
I'm kind of not
showing that here

00:39:31.000 --> 00:39:33.000
because it's kind of confusing.

00:39:33.000 --> 00:39:36.968
But the task, as it executes,
will do a periodic read

00:39:36.968 --> 00:39:39.501
from the cache,

00:39:39.501 --> 00:39:43.133
and then it will go and write
in batch to the Datastore

00:39:43.133 --> 00:39:45.901
the data from the cache.

00:39:45.901 --> 00:39:50.901
So you see that we have--
we write to cache at the top.

00:39:50.901 --> 00:39:53.667
That's step one.
That's all the stuff up here.

00:39:53.667 --> 00:39:55.667
And then down here is
where we actually

00:39:55.667 --> 00:40:00.400
purge the cache,
or flush the cache to disk.

00:40:00.400 --> 00:40:01.934
Now, the problem that
can happen here

00:40:01.934 --> 00:40:05.534
is the cache can fail
in this window.

00:40:05.534 --> 00:40:08.701
If you saw what
I showed you before,

00:40:08.701 --> 00:40:11.000
tasks are dispatched
in a reentrant manner.

00:40:11.000 --> 00:40:12.901
They happen really fast.

00:40:12.901 --> 00:40:14.767
So that window
can be very small.

00:40:14.767 --> 00:40:17.100
And the window is also
configurable by your code.

00:40:17.100 --> 00:40:21.868
You can set how often
you want to purge the cache--

00:40:21.868 --> 00:40:23.767
or write the cache out.

00:40:23.767 --> 00:40:25.267
So it's up to you
to figure out

00:40:25.267 --> 00:40:26.501
what your tolerance
for error is,

00:40:26.501 --> 00:40:27.834
and what you're willing
to accept

00:40:27.834 --> 00:40:31.033
in terms of failures.

00:40:31.033 --> 00:40:34.467
But our cache is very stable.
Memcache is very reliable.

00:40:34.467 --> 00:40:36.667
And most of the time--
the vast majority of the time--

00:40:36.667 --> 00:40:39.067
this is a huge win,
and this cache failure

00:40:39.067 --> 00:40:42.701
is not a big problem.

00:40:42.701 --> 00:40:44.567
Oh, and also mention that
there are a lot of things

00:40:44.567 --> 00:40:45.767
you can do
on the client side

00:40:45.767 --> 00:40:47.634
to deal with cache failures
like this.

00:40:47.634 --> 00:40:49.701
So the client can do
an operation,

00:40:49.701 --> 00:40:51.834
and then verify
that it actually worked,

00:40:51.834 --> 00:40:56.767
you know,
10, 20 seconds later.

00:40:56.767 --> 00:40:59.400
So let's go to the really
concrete example

00:40:59.400 --> 00:41:01.100
of how to use
write-behind cache.

00:41:01.100 --> 00:41:03.834
So many people have asked for,
a bunch of times,

00:41:03.834 --> 00:41:06.634
our page-hit counters,
or just counters in general.

00:41:06.634 --> 00:41:08.968
So without the Task Queue API,

00:41:08.968 --> 00:41:10.734
you had to use something
like sharded counters,

00:41:10.734 --> 00:41:12.901
which are relatively expensive,
problematic,

00:41:12.901 --> 00:41:16.100
use a lot of storage space,
still have time-outs.

00:41:16.100 --> 00:41:19.801
They were a good Band-Aid,
but they really don't give you

00:41:19.801 --> 00:41:21.467
any control over
write throughput.

00:41:21.467 --> 00:41:23.267
You're still doing
a Datastore write

00:41:23.267 --> 00:41:26.801
on every single increment.

00:41:26.801 --> 00:41:30.033
With Task Queue,
we use a write-behind cache.

00:41:30.033 --> 00:41:32.067
Every time a hit comes in,

00:41:32.067 --> 00:41:34.801
you just increment a counter
in memcache,

00:41:34.801 --> 00:41:36.834
and then enqueue a task.

00:41:36.834 --> 00:41:40.267
Then, with your task, you just
copy the value from memcache

00:41:40.267 --> 00:41:42.434
and then write it
to Datastore.

00:41:42.434 --> 00:41:43.834
And you use the queue
throttle rates

00:41:43.834 --> 00:41:45.234
that we talked about before

00:41:45.234 --> 00:41:46.667
to limit the max writes
per second

00:41:46.667 --> 00:41:48.300
you want in your Datastore.

00:41:48.300 --> 00:41:51.033
So some of you, if you've
put our Datastore to the test,

00:41:51.033 --> 00:41:54.400
you know that if you exceed some
amount of writes per second

00:41:54.400 --> 00:41:56.400
for a single entity group
or a single entity,

00:41:56.400 --> 00:41:58.934
you'll start to get
some time-outs or contention.

00:41:58.934 --> 00:42:00.701
This is a way
of eliminating that.

00:42:00.701 --> 00:42:02.834
You just say, hey, I know that,
experimentally,

00:42:02.834 --> 00:42:04.167
I've verified
that this is the fastest

00:42:04.167 --> 00:42:05.501
I can write this entity.

00:42:05.501 --> 00:42:08.100
I will just throttle my queue
to that many per second,

00:42:08.100 --> 00:42:10.267
and that's that.

00:42:10.267 --> 00:42:12.834
And then on the client's side,
from the request handler's side,

00:42:12.834 --> 00:42:15.033
you can read from memcache
or the Datastore.

00:42:15.033 --> 00:42:16.767
Both are sufficient sources
of record

00:42:16.767 --> 00:42:20.234
because the window between
when the cache is updated

00:42:20.234 --> 00:42:22.133
and the Datastore is updated
is very small.

00:42:22.133 --> 00:42:27.167
Or it's as small
as you want it to be.

00:42:27.167 --> 00:42:29.868
So let's look at some code.
I have a counter.

00:42:29.868 --> 00:42:32.000
Here it's an IntegerProperty.

00:42:32.000 --> 00:42:33.167
I have "indexed" set to false,

00:42:33.167 --> 00:42:34.601
just because I want it
to be fast.

00:42:34.601 --> 00:42:36.133
That's a new feature we have.

00:42:36.133 --> 00:42:40.434
It lets you have
an unindexed property.

00:42:40.434 --> 00:42:42.434
My CounterHandler,
this is where, actually--

00:42:42.434 --> 00:42:44.467
This is the user code
that increments the counter.

00:42:44.467 --> 00:42:47.934
So I pull the key
out of the requests.

00:42:47.934 --> 00:42:52.400
I go to memcache and I try
to increment the key.

00:42:52.400 --> 00:42:54.234
If the key was not already
in memcache,

00:42:54.234 --> 00:42:55.734
I'll get back "none."

00:42:55.734 --> 00:42:59.467
So then I'll try to add the key
to memcache with the number "1."

00:42:59.467 --> 00:43:04.434
If my add was not successful,
that will also fail,

00:43:04.434 --> 00:43:06.734
which means that somebody else
added that number

00:43:06.734 --> 00:43:08.467
at the same time I did,

00:43:08.467 --> 00:43:10.200
which means that I can
actually increment again.

00:43:10.200 --> 00:43:11.801
This is kind of a funny
kind of, like,

00:43:11.801 --> 00:43:13.701
two-step lock scheme
with memcache

00:43:13.701 --> 00:43:15.467
to make sure that
you never drop a count.

00:43:15.467 --> 00:43:19.234
But effectively,
these three lines of code here,

00:43:19.234 --> 00:43:23.934
they just ensure that
I'm incrementing by one.

00:43:23.934 --> 00:43:28.167
And then once I've done that,
I add a dirty flag,

00:43:28.167 --> 00:43:31.367
which basically just says
in memcache

00:43:31.367 --> 00:43:37.934
that this counter needs
to be written to Datastore.

00:43:37.934 --> 00:43:40.634
And what's key here is
I'm using memcache.add.

00:43:40.634 --> 00:43:43.834
memcache.add only adds
if it hasn't been added before.

00:43:43.834 --> 00:43:46.167
And if it's already
been added,

00:43:46.167 --> 00:43:47.701
then I get back "false."

00:43:47.701 --> 00:43:51.767
And so what that means is that
I only add a task to the queue

00:43:51.767 --> 00:43:55.367
when I know there's not already
a task in the queue.

00:43:55.367 --> 00:43:57.000
Okay?

00:43:57.000 --> 00:44:00.734
So if there's a task in flight
or a task that's gonna run,

00:44:00.734 --> 00:44:02.300
I don't do anything.

00:44:02.300 --> 00:44:04.467
I just sit there
because I know, eventually,

00:44:04.467 --> 00:44:06.767
the flush is gonna happen.

00:44:06.767 --> 00:44:09.400
But if the task
isn't going to run,

00:44:09.400 --> 00:44:14.501
then I enqueue the task.

00:44:14.501 --> 00:44:17.767
So then on the worker side,
I just have another--

00:44:17.767 --> 00:44:19.300
It's another web request
handler.

00:44:19.300 --> 00:44:21.000
You'll see I have
my task defined.

00:44:21.000 --> 00:44:25.367
I have a worker URL and the key
of the thing to flush.

00:44:25.367 --> 00:44:26.868
And I go in and I say--

00:44:26.868 --> 00:44:29.234
First I delete
the memcache key.

00:44:29.234 --> 00:44:31.901
And that immediately will
get rid of that block

00:44:31.901 --> 00:44:34.267
so that new tasks
can be added to the queue.

00:44:34.267 --> 00:44:36.400
And that's to get rid
of the window

00:44:36.400 --> 00:44:37.467
of any race conditions.

00:44:37.467 --> 00:44:38.567
So that's kind of
a funny lock

00:44:38.567 --> 00:44:41.133
around the task
that's been enqueued.

00:44:41.133 --> 00:44:44.267
Then I go and I get the value
out of memcache--

00:44:44.267 --> 00:44:46.601
the current counter value.

00:44:46.601 --> 00:44:49.501
If it's none, then I know
that I have a cache failure.

00:44:49.501 --> 00:44:51.734
And that's the error condition:
I failed.

00:44:51.734 --> 00:44:53.234
And that should hopefully be
a small window,

00:44:53.234 --> 00:44:54.734
but it's possible.

00:44:54.734 --> 00:44:57.701
So I'd have, maybe, some better
error-handling code in there.

00:44:57.701 --> 00:45:00.767
And then I just create
a counter with a named key,

00:45:00.767 --> 00:45:02.434
I set the value,
and I put it in the Datastore.

00:45:02.434 --> 00:45:04.634
That's it.

00:45:04.634 --> 00:45:08.033
Now, if this task fails
at any moment,

00:45:08.033 --> 00:45:09.434
it should be able
to pick up again.

00:45:09.434 --> 00:45:10.968
The memcache delete
will just do nothing

00:45:10.968 --> 00:45:12.667
because I'm running
the task again.

00:45:12.667 --> 00:45:15.033
If there were another task
that are executed again

00:45:15.033 --> 00:45:16.834
on top of this one,
they basically just

00:45:16.834 --> 00:45:18.234
do the same thing--
that's fine.

00:45:18.234 --> 00:45:19.734
The queue will
throttle them properly,

00:45:19.734 --> 00:45:22.634
so I don't have to worry about
exceeding my number

00:45:22.634 --> 00:45:25.267
of entity group writes
per second.

00:45:25.267 --> 00:45:29.067
And then the counter-put
will put over the top

00:45:29.067 --> 00:45:30.734
of whatever value's there.

00:45:30.734 --> 00:45:33.334
So I have a very safe way
of updating a counter

00:45:33.334 --> 00:45:37.033
and controlling the rate
of writes.

00:45:37.033 --> 00:45:41.901
So demo...

00:45:41.901 --> 00:45:44.100
So I already have
a few here.

00:45:44.100 --> 00:45:46.133
People have been messing around.
That's cool.

00:45:46.133 --> 00:45:49.901
Add "foo" so I can
increment the count.

00:45:49.901 --> 00:45:54.834
Let's see if that's working.
I might have broken my code.

00:45:54.834 --> 00:45:56.934
Yeah, it looks like
I broke my code.

00:45:56.934 --> 00:45:59.968
Well, it works.
Now, you're saying--

00:45:59.968 --> 00:46:01.634
Well, you're probably saying
to yourself,

00:46:01.634 --> 00:46:03.000
"I could write that.

00:46:03.000 --> 00:46:04.367
"I have something that looks
just like that.

00:46:04.367 --> 00:46:06.267
"You add a count,
and it shows up in a list.

00:46:06.267 --> 00:46:07.801
"Why are you using task queues
to do that?

00:46:07.801 --> 00:46:09.400
That's so silly."

00:46:09.400 --> 00:46:14.234
But earlier when I was doing
this demo...

00:46:14.234 --> 00:46:18.701
Yeah, is it working now?

00:46:18.701 --> 00:46:22.334
Okay, there we go.
Yeah, so it is increasing.

00:46:22.334 --> 00:46:25.467
I guess it just doesn't like
"foo."

00:46:25.467 --> 00:46:27.567
That's cool. Cool.
All right, it's working.

00:46:27.567 --> 00:46:30.267
Yeah, so I say "meep,"
I click "increment."

00:46:30.267 --> 00:46:33.467
By the time it's reloaded,
meep's at seven...right?

00:46:33.467 --> 00:46:34.767
I probably got some errors
in there.

00:46:34.767 --> 00:46:36.400
So what happens is,
if a task fails,

00:46:36.400 --> 00:46:38.767
we back off for a little while
and then try again.

00:46:38.767 --> 00:46:42.467
The code's a little buggy, so
that's probably what's going on.

00:46:42.467 --> 00:46:45.133
But let's look at my log.

00:46:45.133 --> 00:46:49.367
I have a counter here,
and I've done--

00:46:49.367 --> 00:46:50.868
So here's a counter
with a 302.

00:46:50.868 --> 00:46:53.868
This is my post.
That's when I said "increment."

00:46:53.868 --> 00:46:57.701
Then you see--so you see
there's a post right here.

00:46:57.701 --> 00:47:00.801
Then the write-behind
comes through.

00:47:00.801 --> 00:47:04.934
That's the task running.
So the first one comes at--

00:47:04.934 --> 00:47:08.634
Okay, so I say increment
this counter at 18.1 seconds...

00:47:08.634 --> 00:47:10.601
18.10.

00:47:10.601 --> 00:47:13.767
The task is already running
by 18.19.

00:47:13.767 --> 00:47:15.367
And then my browser does
a round-trip

00:47:15.367 --> 00:47:19.100
and finally re-polls the data
at 18.42.

00:47:19.100 --> 00:47:21.801
So from the user's perspective,

00:47:21.801 --> 00:47:24.634
it's if the page did
the operation itself.

00:47:24.634 --> 00:47:26.567
But it didn't work that way.

00:47:26.567 --> 00:47:27.634
It's all done
in the background,

00:47:27.634 --> 00:47:33.801
but it just happens
really quickly.

00:47:33.801 --> 00:47:36.801
Okay, so those are
the practical things you can do.

00:47:36.801 --> 00:47:38.467
I'm just gonna talk
about the future real quick,

00:47:38.467 --> 00:47:40.300
and then we'll get
into some questions.

00:47:40.300 --> 00:47:43.067
So Task Queue is coming soon.

00:47:43.067 --> 00:47:44.834
We're gonna release
the Task Queue API

00:47:44.834 --> 00:47:45.868
in App Engine Labs.

00:47:45.868 --> 00:47:46.901
It's Python-only at first,

00:47:46.901 --> 00:47:48.834
but Java is coming soon after.

00:47:48.834 --> 00:47:50.934
The Java support in the works
will support

00:47:50.934 --> 00:47:54.767
both the web hooks interface
and, we plan, JMS integration.

00:47:54.767 --> 00:47:57.667
Now, like I said before,
this is not pub-sub,

00:47:57.667 --> 00:47:59.133
but there are
some very good mappings

00:47:59.133 --> 00:48:02.167
between queuing systems
and pub-subs.

00:48:02.167 --> 00:48:05.701
So we're looking to have
some level of JMS compatibility

00:48:05.701 --> 00:48:09.834
so that you can use JMS systems
with our Task Queue API.

00:48:09.834 --> 00:48:13.701
We're also working on a lot more
Task Queue API features

00:48:13.701 --> 00:48:15.901
in the coming months.

00:48:15.901 --> 00:48:18.267
A lot of management functions,
like flushing a queue.

00:48:18.267 --> 00:48:20.701
We want to have the ability
to view the contents of queues

00:48:20.701 --> 00:48:22.033
in the admin console.

00:48:22.033 --> 00:48:23.200
And we want you
to be able to get

00:48:23.200 --> 00:48:24.834
other web hook notification
events

00:48:24.834 --> 00:48:28.300
when a queue has finished
all of its processing,

00:48:28.300 --> 00:48:33.701
or any kind of meta events
with the queue.

00:48:33.701 --> 00:48:35.634
But let's talk more
about the future

00:48:35.634 --> 00:48:38.033
of just offline processing
and App Engine in general.

00:48:38.033 --> 00:48:43.434
So with batch processing,
the Task Queue API is good

00:48:43.434 --> 00:48:47.100
for small datasets--you know,
100,000 rows or less.

00:48:47.100 --> 00:48:49.400
But we really need some
more tools for parallelization

00:48:49.400 --> 00:48:52.534
and high throughput processing
of Datastore entities.

00:48:52.534 --> 00:48:55.033
We need a really efficient way
of splitting all of your data

00:48:55.033 --> 00:48:59.601
into pieces so that you can
chunk through it in parallel.

00:48:59.601 --> 00:49:01.834
Right now we don't have
enough tools for that.

00:49:01.834 --> 00:49:03.601
So we're working on those.

00:49:03.601 --> 00:49:05.067
We also need some
better features

00:49:05.067 --> 00:49:08.734
for doing aggregations and
statistics kind of calculations

00:49:08.734 --> 00:49:11.300
with your entities.

00:49:11.300 --> 00:49:13.534
So we'll hopefully
get to those soon.

00:49:13.534 --> 00:49:15.834
We also want to support
MapReduce.

00:49:15.834 --> 00:49:18.667
Full-on MapReduce--
a MapReduce abstraction.

00:49:18.667 --> 00:49:21.133
I don't know where this fits
on our timeline.

00:49:21.133 --> 00:49:23.033
We're gonna talk more about it
as time goes on.

00:49:23.033 --> 00:49:25.501
But before we get there,
we need some more tools.

00:49:25.501 --> 00:49:28.767
We need more efficient
intermediary storage.

00:49:28.767 --> 00:49:31.367
The Datastore is quick,
but it's not fast enough.

00:49:31.367 --> 00:49:34.267
We need something very fast
if we want to use a lot of data.

00:49:34.267 --> 00:49:36.801
We need something that's better
for shorting and shuffling.

00:49:36.801 --> 00:49:39.267
We've got a lot of work
to do.

00:49:39.267 --> 00:49:42.133
And especially because we want
our MapReduce abstraction

00:49:42.133 --> 00:49:45.200
to work with both small datasets
and very large datasets--

00:49:45.200 --> 00:49:51.167
over a terabyte or bigger.

00:49:51.167 --> 00:49:53.434
So to wrap up...

00:49:53.434 --> 00:49:56.467
use the Task Queue API,
once it's launched.

00:49:56.467 --> 00:49:58.801
Make your existing app faster,
lower latency.

00:49:58.801 --> 00:50:00.367
Scale your app further
with lower costs.

00:50:00.367 --> 00:50:03.100
Add new functionality
you couldn't implement before.

00:50:03.100 --> 00:50:04.334
The sky's the limit.

00:50:04.334 --> 00:50:06.834
You know, you can implement
an event web site

00:50:06.834 --> 00:50:08.100
for you and your friends.

00:50:08.100 --> 00:50:09.367
You could make
a feed aggregator.

00:50:09.367 --> 00:50:10.534
There's all kinds of stuff
you can do now

00:50:10.534 --> 00:50:12.400
that you could never do before.

00:50:12.400 --> 00:50:14.934
And take advantage of web hooks
for easy debugging.

00:50:14.934 --> 00:50:17.300
Web hooks let you use a forum
to test your workers.

00:50:17.300 --> 00:50:20.667
It's ridiculously easy.

00:50:20.667 --> 00:50:22.834
So let's get to questions.

00:50:22.834 --> 00:50:24.033
I think we have
ten minutes left.

00:50:24.033 --> 00:50:25.767
There are mics here and here.

00:50:25.767 --> 00:50:27.634
We also have them
on Moderator.

00:50:27.634 --> 00:50:31.934
And if you have more feedback,
you can go to haveasec.com/io.

00:50:31.934 --> 00:50:35.434
[applause]

00:50:41.934 --> 00:50:43.367
Okay, so I'm just gonna
go through

00:50:43.367 --> 00:50:44.601
the first three Moderator,

00:50:44.601 --> 00:50:47.133
and then I'll do
the other questions.

00:50:47.133 --> 00:50:49.534
So if you guys want
to talk to each other,

00:50:49.534 --> 00:50:51.634
please go outside the room
so we can all talk together here

00:50:51.634 --> 00:50:54.067
about this talk still.

00:50:54.067 --> 00:50:56.133
We'll also be at office hours
for anyone who has questions.

00:50:56.133 --> 00:50:59.167
So come on by.
I will be there all day.

00:50:59.167 --> 00:51:02.234
Okay, so are tasks limited
by the 30-second time limit?

00:51:02.234 --> 00:51:05.501
Yes, they are.
They are.

00:51:05.501 --> 00:51:08.234
So we let you segment tasks
into multiple tasks.

00:51:08.234 --> 00:51:10.834
We let tasks add new tasks
when they're done.

00:51:10.834 --> 00:51:13.701
It's pretty easy
to segment your data.

00:51:13.701 --> 00:51:15.968
We'd like them to be able
to run longer.

00:51:15.968 --> 00:51:17.334
We're looking into it.

00:51:17.334 --> 00:51:22.267
But yes, for now they are
also limited to 30 seconds.

00:51:22.267 --> 00:51:24.868
Can we limit the number
of retries

00:51:24.868 --> 00:51:26.567
and have a method
to process failed tasks

00:51:26.567 --> 00:51:29.501
with, say, a failback
or resolution handler?

00:51:29.501 --> 00:51:32.067
I think you can do a lot of this
in user space right now

00:51:32.067 --> 00:51:35.601
by having some auxiliary data.

00:51:35.601 --> 00:51:37.767
But yeah, this is one
of those features

00:51:37.767 --> 00:51:41.367
that we could totally add on top
of the existing abstraction.

00:51:41.367 --> 00:51:43.801
So it's something
we'll definitely consider

00:51:43.801 --> 00:51:47.400
with all the other features
we'd like to add.

00:51:47.400 --> 00:51:50.234
Will the APIs provide
the ability

00:51:50.234 --> 00:51:52.601
to query the queue,
see how deep,

00:51:52.601 --> 00:51:54.100
the processing rates,
intercept tasks

00:51:54.100 --> 00:51:55.934
that have retried
a bunch of times?

00:51:55.934 --> 00:51:58.834
So yeah, we'd like to provide
APIs for all this stuff.

00:51:58.834 --> 00:52:00.701
We're starting with
the admin console

00:52:00.701 --> 00:52:02.868
to give you a lot
of good diagnostics

00:52:02.868 --> 00:52:05.734
into how your queues are going,
how old the tasks are,

00:52:05.734 --> 00:52:08.767
flow rates, error rates,
et cetera.

00:52:08.767 --> 00:52:10.801
A lot of this centers around
what the admin console needs

00:52:10.801 --> 00:52:13.467
to give you a great picture
of what your queues are doing.

00:52:13.467 --> 00:52:18.000
And we'd also like to provide
those APIs to our users.

00:52:18.000 --> 00:52:20.534
Okay, I'll get to this one
in a minute.

00:52:20.534 --> 00:52:22.934
So let's start with just
questions in person. Yeah?

00:52:22.934 --> 00:52:25.200
man: Yeah, going back to your
migration example,

00:52:25.200 --> 00:52:26.834
you mentioned
how important it is

00:52:26.834 --> 00:52:30.868
to put the migrated entity
before deleting the old one.

00:52:30.868 --> 00:52:33.267
I wonder whether you use
key names to identify--

00:52:33.267 --> 00:52:34.567
Slatkin: That's exactly
what you use.

00:52:34.567 --> 00:52:37.534
Yeah, you use key names
or the ID to identify the data.

00:52:37.534 --> 00:52:38.968
So yeah.

00:52:38.968 --> 00:52:41.367
man: Can you put the new entity
with the same key name

00:52:41.367 --> 00:52:42.701
as the old one
before deleting it?

00:52:42.701 --> 00:52:43.968
Slatkin: Yes, if the kinds are
different, you can.

00:52:43.968 --> 00:52:47.300
So IDs and key names are
specific to an entity kind,

00:52:47.300 --> 00:52:48.734
like a model class.

00:52:48.734 --> 00:52:50.834
So you can use the same ID
for both,

00:52:50.834 --> 00:52:52.267
and they won't affect
each other

00:52:52.267 --> 00:52:55.567
because the primary key
is actually used as the kind.

00:52:55.567 --> 00:52:56.968
So yeah,
that's how you do it.

00:52:56.968 --> 00:53:00.000
man: So the migrated entity
will have to be a new kind?

00:53:00.000 --> 00:53:02.200
Slatkin: It would be--exactly.
It would be a new kind.

00:53:02.200 --> 00:53:03.701
If you didn't want to do
a new kind,

00:53:03.701 --> 00:53:05.901
you'd need to do some other kind
of transformation on the key.

00:53:05.901 --> 00:53:07.968
Yeah. Over here.

00:53:07.968 --> 00:53:10.334
man: If my app is not
in App Engine,

00:53:10.334 --> 00:53:13.167
but I want to utilize
your framework for queuing,

00:53:13.167 --> 00:53:15.234
is the API gonna be available
publicly

00:53:15.234 --> 00:53:16.901
so that I could hit it
with requests,

00:53:16.901 --> 00:53:18.934
and queue and pull items
from it?

00:53:18.934 --> 00:53:20.067
Slatkin: Right,
that's a good question.

00:53:20.067 --> 00:53:21.501
So, you know, can I--

00:53:21.501 --> 00:53:25.234
So the first answer is
we have no plans,

00:53:25.234 --> 00:53:28.634
or nothing to announce
to that right now.

00:53:28.634 --> 00:53:30.968
There's nothing stopping you
from implementing this

00:53:30.968 --> 00:53:32.734
as an App Engine app itself.

00:53:32.734 --> 00:53:34.934
You could--

00:53:34.934 --> 00:53:37.000
Especially if you had
some queue operations

00:53:37.000 --> 00:53:38.734
to actually, like...

00:53:38.734 --> 00:53:40.534
have some introspection
on a queue,

00:53:40.534 --> 00:53:42.067
you could do things like that.

00:53:42.067 --> 00:53:43.934
You could also use App Engine
to run a queue

00:53:43.934 --> 00:53:45.634
and actually send
web hook requests

00:53:45.634 --> 00:53:48.734
back to your own
data center

00:53:48.734 --> 00:53:52.200
or to some other
virtualized hosting server

00:53:52.200 --> 00:53:54.334
or any other provider.

00:53:54.334 --> 00:53:56.033
So, you know, you can build
an App Engine app

00:53:56.033 --> 00:53:57.701
to basically do whatever you
want with your queue.

00:53:57.701 --> 00:53:59.100
It's up to you to kind of
figure out

00:53:59.100 --> 00:54:00.634
how you want to do it.

00:54:00.634 --> 00:54:04.234
But the fundamental difference
is that it's push, not pull.

00:54:04.234 --> 00:54:07.200
So our processing model's very
different from a standard queue,

00:54:07.200 --> 00:54:09.133
where you're constantly pulling
off the list.

00:54:09.133 --> 00:54:11.968
man: So I would write an app
that I could hit from outside

00:54:11.968 --> 00:54:13.667
and give my tasks to,

00:54:13.667 --> 00:54:15.767
and then you guys
push it back to me when--

00:54:15.767 --> 00:54:17.567
Slatkin: That's totally an
option if you want to do that.

00:54:17.567 --> 00:54:20.367
Yeah, you use a URL fetch API
to do that. Yep.

00:54:20.367 --> 00:54:21.701
Over here.

00:54:21.701 --> 00:54:23.801
man: Yeah, I think you kind of
announced something

00:54:23.801 --> 00:54:25.200
called App Engine Labs.

00:54:25.200 --> 00:54:27.234
Can you talk about
what that is

00:54:27.234 --> 00:54:30.434
and how an
App Engine Lab app works?

00:54:30.434 --> 00:54:32.601
Slatkin: Sure.
So App Engine Labs is gonna be

00:54:32.601 --> 00:54:35.300
a name space within
our API packages

00:54:35.300 --> 00:54:37.767
that lets you import
new modules.

00:54:37.767 --> 00:54:41.767
And packages in that name space
will be--

00:54:41.767 --> 00:54:44.434
Like I said, they might have
some incompatible changes

00:54:44.434 --> 00:54:46.033
that we're gonna make over time.

00:54:46.033 --> 00:54:49.067
We're gonna warn you anytime we
do those incompatible changes.

00:54:49.067 --> 00:54:50.968
And then eventually,
when it graduates from Labs,

00:54:50.968 --> 00:54:52.834
we're gonna move it into
our normal name space,

00:54:52.834 --> 00:54:54.534
at which point it will be
backward compatible.

00:54:54.534 --> 00:54:56.100
It will not break your code
unless we do

00:54:56.100 --> 00:55:00.133
a full version change
where in your app.yaml

00:55:00.133 --> 00:55:02.367
you actually have to specify
a new major version.

00:55:02.367 --> 00:55:05.901
So...and yeah, we're hoping
to release more APIs

00:55:05.901 --> 00:55:07.734
under the Labs moniker.

00:55:07.734 --> 00:55:09.601
The idea is that
we don't really know

00:55:09.601 --> 00:55:12.434
exactly what we want yet,
and we won't have user feedback.

00:55:12.434 --> 00:55:14.534
So this is a way of us
giving you a preview

00:55:14.534 --> 00:55:18.501
without locking ourselves
into a problem.

00:55:18.501 --> 00:55:20.901
And the other part is what
I said about billing.

00:55:20.901 --> 00:55:22.934
We're still trying to figure out
how to bill for this properly.

00:55:22.934 --> 00:55:26.300
So that's part of it.

00:55:26.300 --> 00:55:28.968
Okay, let me just do another
off of Moderator.

00:55:28.968 --> 00:55:30.367
How do you prevent
a malicious user

00:55:30.367 --> 00:55:33.667
from using tasks
to create tasks...forkbomb?

00:55:33.667 --> 00:55:36.367
How do you avoid punishing
legitimately busy sites?

00:55:36.367 --> 00:55:39.534
Yeah, this is
a very good question.

00:55:39.534 --> 00:55:42.100
So quotas are one way.

00:55:42.100 --> 00:55:46.334
By setting a reasonable,
small, free limit--

00:55:46.334 --> 00:55:48.067
You know,
a small or free limit,

00:55:48.067 --> 00:55:51.267
we can prevent sites
from doing runaway queuing

00:55:51.267 --> 00:55:53.868
and kind of hosing themselves.

00:55:53.868 --> 00:55:57.067
We would hope that sites
that are paying customers

00:55:57.067 --> 00:55:58.968
who want to do
some useful work

00:55:58.968 --> 00:56:01.000
wouldn't try to hose themselves.

00:56:01.000 --> 00:56:03.200
But I think this comes into
kind of queue management

00:56:03.200 --> 00:56:06.100
and queue introspection tools.

00:56:06.100 --> 00:56:08.000
You know, you want to know
if this has happened

00:56:08.000 --> 00:56:09.167
if you haven't--

00:56:09.167 --> 00:56:11.067
Because you can forkbomb
yourself on accident.

00:56:11.067 --> 00:56:13.801
So you want to make sure that
you can diagnose this problem.

00:56:13.801 --> 00:56:15.200
Maybe get alerts
when it happens.

00:56:15.200 --> 00:56:17.000
Stop a queue, flush a queue.

00:56:17.000 --> 00:56:18.534
There's a lot of tooling
we can do around this

00:56:18.534 --> 00:56:22.734
to avoid forkbombs
from hurting you.

00:56:22.734 --> 00:56:25.701
And how do we avoid punishing
legitimately busy sites?

00:56:25.701 --> 00:56:28.968
I think that just fits into
our whole scheme of quotas.

00:56:28.968 --> 00:56:30.767
You know, we have
reasonable quotas set

00:56:30.767 --> 00:56:32.501
for both free and paying apps.

00:56:32.501 --> 00:56:34.234
And as long as you're
within those limits,

00:56:34.234 --> 00:56:36.968
it shouldn't make a difference.

00:56:36.968 --> 00:56:38.367
Over here.

00:56:38.367 --> 00:56:40.834
man: You've spoken before about
warm versus cold apps,

00:56:40.834 --> 00:56:43.667
and how the first request is
more expensive and slower

00:56:43.667 --> 00:56:47.634
than subsequent requests
in a limited period of time.

00:56:47.634 --> 00:56:49.601
How does queuing work
with that?

00:56:49.601 --> 00:56:51.701
Slatkin: Yeah, so it's just like
the rest of our infrastructure.

00:56:51.701 --> 00:56:53.501
We try to maximize
our cache-hit rate

00:56:53.501 --> 00:56:55.267
to your runtimes.

00:56:55.267 --> 00:56:59.267
So you know, we'll try to keep
using a hot runtime if we can.

00:56:59.267 --> 00:57:00.834
It uses the same
infrastructure serving

00:57:00.834 --> 00:57:02.400
to do that.

00:57:02.400 --> 00:57:04.367
Let me do a couple more
off the Moderator real quick.

00:57:04.367 --> 00:57:06.834
Is there a way to define
the retry policy?

00:57:06.834 --> 00:57:09.267
So this is probably, like,
how much to back off,

00:57:09.267 --> 00:57:10.601
how quickly to back off.

00:57:10.601 --> 00:57:12.033
Again, this is one
of those features

00:57:12.033 --> 00:57:14.300
that we want to look into more.

00:57:14.300 --> 00:57:16.367
We know that this is a feature
people would like to have.

00:57:16.367 --> 00:57:18.033
For now we're gonna have
just a simple

00:57:18.033 --> 00:57:19.868
exponential back-off scheme.

00:57:19.868 --> 00:57:21.901
I think it starts
at 100 milliseconds.

00:57:21.901 --> 00:57:22.934
Something like that.

00:57:22.934 --> 00:57:25.267
And we'll go from there.

00:57:25.267 --> 00:57:26.601
Will there be a free quota?

00:57:26.601 --> 00:57:28.200
Yes, there will be
a free quota.

00:57:28.200 --> 00:57:30.801
We'd still like to stick with
the "it's free to get started"

00:57:30.801 --> 00:57:32.734
with App Engine.

00:57:32.734 --> 00:57:36.734
And we'd like you to start
using this Task Queue API

00:57:36.734 --> 00:57:38.501
as soon as we release it.

00:57:38.501 --> 00:57:41.000
Will the post request be
limited to one meg?

00:57:41.000 --> 00:57:43.400
We're still figuring out
what the maximum size

00:57:43.400 --> 00:57:45.601
of tasks should be.

00:57:45.601 --> 00:57:48.334
So you know,
it could be anywhere

00:57:48.334 --> 00:57:50.567
from 100K to one meg,
10K.

00:57:50.567 --> 00:57:51.634
We're not really sure.

00:57:51.634 --> 00:57:53.033
We're figuring out
that the best--

00:57:53.033 --> 00:57:55.934
We don't want people necessarily
using the Task Queue

00:57:55.934 --> 00:57:57.200
for storage.

00:57:57.200 --> 00:57:59.534
It should be storing
a description of work to do.

00:57:59.534 --> 00:58:01.100
So we are finding
the right balance

00:58:01.100 --> 00:58:03.133
in terms of scalability.

00:58:03.133 --> 00:58:05.767
And so we'll announce that
when we announce

00:58:05.767 --> 00:58:09.300
the rest of the documentation.

00:58:09.300 --> 00:58:12.267
And the last one
I'll take here is

00:58:12.267 --> 00:58:15.300
is it possible to manually
delete those tombstones?

00:58:15.300 --> 00:58:16.834
Right now it's not.

00:58:16.834 --> 00:58:20.534
But we want to let you specify
the tombstoning policy

00:58:20.534 --> 00:58:22.968
for name tasks
so that you can say, hey,

00:58:22.968 --> 00:58:24.601
I want tasks to be
immediately garbage collected,

00:58:24.601 --> 00:58:25.868
or I don't.

00:58:25.868 --> 00:58:27.901
Or I want them to be
garbage collected in 20 minutes.

00:58:27.901 --> 00:58:29.801
We're still figuring out
some more features there.

00:58:29.801 --> 00:58:32.868
But we're aware of that
as something that people want.

00:58:32.868 --> 00:58:34.567
Over here.

00:58:34.567 --> 00:58:36.501
man: Could you give us answer

00:58:36.501 --> 00:58:38.968
when the Java support
would come?

00:58:38.968 --> 00:58:41.200
Is it, like, weeks,
months, or--

00:58:41.200 --> 00:58:43.067
And in the meantime,
what's the best way

00:58:43.067 --> 00:58:48.334
to circumvent that using, like,
mix of Python, Java, or...

00:58:48.334 --> 00:58:50.634
Slatkin: Yeah,
so I can't give you

00:58:50.634 --> 00:58:52.734
a good timeline
on the Java support.

00:58:52.734 --> 00:58:54.734
I mean, we'd like it to be
as soon as possible.

00:58:54.734 --> 00:58:57.100
So, I mean, within weeks
of our Python launch.

00:58:57.100 --> 00:58:59.367
But I'm not really sure.

00:58:59.367 --> 00:59:01.334
Yeah, you could use
a hybrid model

00:59:01.334 --> 00:59:03.534
of a Python app
with a task queue,

00:59:03.534 --> 00:59:05.234
and doing URL fetches
between them.

00:59:05.234 --> 00:59:08.534
I wouldn't encourage you
to do that yet.

00:59:08.534 --> 00:59:10.033
So hopefully we'll have
code written

00:59:10.033 --> 00:59:11.801
so you can actually
take advantage of this in Java

00:59:11.801 --> 00:59:13.367
as soon as possible.

00:59:13.367 --> 00:59:16.100
That's the best answer
I can give for now.

00:59:16.100 --> 00:59:17.567
Over here.

00:59:17.567 --> 00:59:19.534
man: How would you go about
counting the number of entities

00:59:19.534 --> 00:59:21.300
of a particular kind
in the Datastore?

00:59:21.300 --> 00:59:24.667
Is that worth using a queue
to do?

00:59:24.667 --> 00:59:27.434
Slatkin: Yeah, so you can do
some aggregations like that.

00:59:27.434 --> 00:59:30.267
Essentially you just do
the Schema migration,

00:59:30.267 --> 00:59:32.634
except you store the counter
in memcache.

00:59:32.634 --> 00:59:34.567
And you just migrate through
all of the entities

00:59:34.567 --> 00:59:37.868
doing nothing,
and store the count.

00:59:37.868 --> 00:59:39.100
And by the time
you get to the end,

00:59:39.100 --> 00:59:42.234
then the memcache contains
the total.

00:59:42.234 --> 00:59:44.300
You can do all kinds
of aggregations that way--

00:59:44.300 --> 00:59:47.234
sums, averages,
et cetera, et cetera.

00:59:47.234 --> 00:59:50.534
So, you know, an aggregation
is just a Schema migration

00:59:50.534 --> 00:59:52.667
that does nothing,
I guess, so...yep.

00:59:52.667 --> 00:59:54.767
Over here.

00:59:54.767 --> 00:59:57.467
man: Yeah, since tasks
are just URL requests,

00:59:57.467 --> 01:00:00.834
is there some way that
when the request is run

01:00:00.834 --> 01:00:02.801
it can know if it's being run
as a task?

01:00:02.801 --> 01:00:04.167
I was thinking about
the mail example.

01:00:04.167 --> 01:00:05.567
Slatkin: Yeah, we're gonna have
parts of the queue

01:00:05.567 --> 01:00:07.167
that let you have
some introspection

01:00:07.167 --> 01:00:08.501
into the request itself.

01:00:08.501 --> 01:00:10.067
So you'd say,
"Am I a task?"

01:00:10.067 --> 01:00:13.033
And we'll be able to make sure
that Task Queue handlers

01:00:13.033 --> 01:00:16.767
aren't run by
non-administrators, et cetera.

01:00:16.767 --> 01:00:19.300
So yeah, we're gonna have
all that stuff.

01:00:19.300 --> 01:00:21.200
All right,
that's all the time we've got.

01:00:21.200 --> 01:00:22.801
Please visit us
in office hours.

01:00:22.801 --> 01:00:24.534
We'll answer more
of your questions.

01:00:24.534 --> 01:00:26.467
And thanks a lot for coming.
[applause]

