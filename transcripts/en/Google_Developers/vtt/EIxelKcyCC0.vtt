WEBVTT
Kind: captions
Language: en

00:00:03.500 --> 00:00:03.650
MIKE AIZATSKYI: Good

00:00:03.650 --> 00:00:05.910
afternoon, ladies and gentlemen.

00:00:05.910 --> 00:00:07.720
I'm really excited to
see you here today.

00:00:07.720 --> 00:00:11.470
I'm Mike Aizatskyi from App
Engine Engineering Team.

00:00:11.470 --> 00:00:15.240
And today we would like to talk
to you about App Engine

00:00:15.240 --> 00:00:16.790
and MapReduce.

00:00:16.790 --> 00:00:20.600
So the agenda of today's talk
is to go briefly through

00:00:20.600 --> 00:00:24.150
MapReduce Computational Model,
because I assume some of you

00:00:24.150 --> 00:00:26.720
would like another reminder
of what it is.

00:00:26.720 --> 00:00:29.820
Then we'll remember the Mapper
library, which we've now

00:00:29.820 --> 00:00:32.049
released like a year ago with
previous Google I/O.

00:00:32.049 --> 00:00:34.910
Then I'll make a couple
announcements.

00:00:34.910 --> 00:00:37.360
I'm going to talk about some
technical stuff to get you

00:00:37.360 --> 00:00:41.400
interested in how we did some
things in App Engine Team.

00:00:41.400 --> 00:00:43.860
And throughout the whole
presentation, I'm going to

00:00:43.860 --> 00:00:46.650
show you several examples and
demos just to keep you

00:00:46.650 --> 00:00:48.170
entertained.

00:00:48.170 --> 00:00:50.620
So let's start with MapReduce
Computational Model.

00:00:50.620 --> 00:00:55.240
You might know that MapReduce
was created at Google many

00:00:55.240 --> 00:00:58.070
years ago to solve
problems with

00:00:58.070 --> 00:01:00.240
efficient distributed computing.

00:01:00.240 --> 00:01:04.420
And really, absolutely
every team uses

00:01:04.420 --> 00:01:07.020
MapReduce for something.

00:01:07.020 --> 00:01:09.590
I'm absolutely sure that even
Android team uses MapReduce

00:01:09.590 --> 00:01:12.860
for doing some stuff, for
doing some population

00:01:12.860 --> 00:01:14.480
statistics, whatever.

00:01:14.480 --> 00:01:19.140
And it's been a real success at
Google, and we use it for

00:01:19.140 --> 00:01:19.740
everything.

00:01:19.740 --> 00:01:22.500
And this is a diagram of
how it looks like.

00:01:22.500 --> 00:01:25.050
It might be a little bit
confusing, so we'll go through

00:01:25.050 --> 00:01:26.310
it step by step.

00:01:26.310 --> 00:01:29.580
But it basically has three major
steps, and those are

00:01:29.580 --> 00:01:32.130
called map, shuffle,
and reduce.

00:01:32.130 --> 00:01:35.740
So the map shuffle is some kind
of user code which runs

00:01:35.740 --> 00:01:36.990
over input data.

00:01:36.990 --> 00:01:38.660
That input data might be

00:01:38.660 --> 00:01:42.010
database, some files, whatever.

00:01:42.010 --> 00:01:45.300
And the goal of the code is to
output key and value pairs.

00:01:45.300 --> 00:01:47.630
And the MapReduce framework
should take those key value

00:01:47.630 --> 00:01:50.100
pairs and should store
them somewhere for

00:01:50.100 --> 00:01:51.790
other steps to consume.

00:01:51.790 --> 00:01:55.080
And as you see, the input is
usually sharded, like split in

00:01:55.080 --> 00:01:57.700
parts, so that all of them can
be processed and paralleled

00:01:57.700 --> 00:02:01.610
really, really fast. And as
I said, this is user code.

00:02:01.610 --> 00:02:05.000
The second step of MapReduce
is a shuffle step.

00:02:05.000 --> 00:02:09.229
And the shuffle phase has to
collate values with the same

00:02:09.229 --> 00:02:09.930
key together.

00:02:09.930 --> 00:02:11.790
It has to go through all
the values which are

00:02:11.790 --> 00:02:13.680
generated by map phase.

00:02:13.680 --> 00:02:15.800
It should find similar,
same keys.

00:02:15.800 --> 00:02:20.070
It should bundle those values
together, and it should output

00:02:20.070 --> 00:02:22.360
key and list of value pairs.

00:02:22.360 --> 00:02:24.810
And this step has no user
code whatsoever.

00:02:24.810 --> 00:02:27.240
This is a generic step of
web-produced framework,

00:02:27.240 --> 00:02:30.840
provided usually as some kind
of service, as some kind of

00:02:30.840 --> 00:02:33.690
magic black box which
does shuffle.

00:02:33.690 --> 00:02:35.800
And the third part is reduce.

00:02:35.800 --> 00:02:38.130
And the reduce function will
actually take the shuffle

00:02:38.130 --> 00:02:41.720
output, which is the key, and
list of value pairs, and it

00:02:41.720 --> 00:02:43.020
does something with that.

00:02:43.020 --> 00:02:45.120
It might compute something more

00:02:45.120 --> 00:02:46.100
interesting from that data.

00:02:46.100 --> 00:02:48.340
It might save it to database.

00:02:48.340 --> 00:02:49.870
It might do something.

00:02:49.870 --> 00:02:52.290
And this is again
the user code.

00:02:52.290 --> 00:02:56.190
And again, it's goal is to
process the shuffle output as

00:02:56.190 --> 00:03:00.700
fast as possible, and it's
good if it's parallel.

00:03:00.700 --> 00:03:05.320
So once again, this is the
diagram of MapReduce

00:03:05.320 --> 00:03:07.420
Computational Model, not
the implementation

00:03:07.420 --> 00:03:09.300
but just the model.

00:03:09.300 --> 00:03:13.930
So we have map step, key values,
shuffle bundles same

00:03:13.930 --> 00:03:16.930
keys together, and reduce,
process, and shuffle output.

00:03:20.210 --> 00:03:23.640
So the way we work at App
Engine, we usually take

00:03:23.640 --> 00:03:27.480
something which we already have
at Google and which works

00:03:27.480 --> 00:03:30.250
for Google, which works for
us, and we give it to you.

00:03:30.250 --> 00:03:31.480
We give it to our developers.

00:03:31.480 --> 00:03:35.060
We bundle it somehow, and we
give you access to that same

00:03:35.060 --> 00:03:36.540
set of technologies.

00:03:36.540 --> 00:03:40.050
And this is exactly what we
wanted to do with MapReduce.

00:03:40.050 --> 00:03:43.340
But when we started working in
that, we realized that there

00:03:43.340 --> 00:03:46.480
are lots of problems
with this approach,

00:03:46.480 --> 00:03:48.120
unique to the App Engine.

00:03:48.120 --> 00:03:51.140
First of all, App Engine has
scaling dimensions which we

00:03:51.140 --> 00:03:52.760
usually don't have at Google.

00:03:52.760 --> 00:03:56.410
App Engine has lots and lots
of applications, much more

00:03:56.410 --> 00:03:58.526
than Google applications.

00:03:58.526 --> 00:04:01.910
And we expect many of them that
will run MapReduce at the

00:04:01.910 --> 00:04:05.760
same time, and that might cause
additional scalability

00:04:05.760 --> 00:04:06.230
problems.

00:04:06.230 --> 00:04:08.230
The other problem
is isolation.

00:04:08.230 --> 00:04:14.430
We do not want anyone running
something to kill a royal

00:04:14.430 --> 00:04:15.650
wedding website.

00:04:15.650 --> 00:04:17.970
If you run MapReduce, you want
royal wedding to be up.

00:04:17.970 --> 00:04:20.610
So we want to completely isolate
all applications from

00:04:20.610 --> 00:04:26.340
each other, but still we want
them to have good performance.

00:04:26.340 --> 00:04:30.880
It might be surprising, but we
also want to rate limit.

00:04:30.880 --> 00:04:33.510
Originally, MapReduce at Google
was designed to go as

00:04:33.510 --> 00:04:34.890
fast as possible.

00:04:34.890 --> 00:04:39.810
It wants to process all the
data in shortest amount of

00:04:39.810 --> 00:04:41.050
time possible.

00:04:41.050 --> 00:04:43.870
And that means that we can
consume thousands of dollars

00:04:43.870 --> 00:04:45.450
resources per minute.

00:04:45.450 --> 00:04:48.730
And not everyone is happy about
that, because you might

00:04:48.730 --> 00:04:51.130
run MapReduce and it will
consume all your daily

00:04:51.130 --> 00:04:53.560
resources in 15 minutes.

00:04:53.560 --> 00:04:56.350
It will kill your online
traffic, so you might want to

00:04:56.350 --> 00:04:59.330
rate limit it and say, OK, don't
run in five minutes.

00:04:59.330 --> 00:05:02.690
Please run over two hours,
because I have short-term

00:05:02.690 --> 00:05:04.670
quotas which I don't
want to consume.

00:05:04.670 --> 00:05:07.370
And taken to the extreme, there
are even three apps

00:05:07.370 --> 00:05:10.540
which have a very, very limited
amount of quotas, and

00:05:10.540 --> 00:05:12.310
they still want to
run MapReduces.

00:05:12.310 --> 00:05:15.430
And they might say, OK, run this
for the whole week, but

00:05:15.430 --> 00:05:17.550
just get it done.

00:05:17.550 --> 00:05:21.000
But please, please, please do
not kill my application, which

00:05:21.000 --> 00:05:23.880
is not a goal for MapReduce,
Google's MapReduce to run as

00:05:23.880 --> 00:05:27.095
slow as possible.

00:05:27.095 --> 00:05:30.300
We also have a protection issue,
too, because there a

00:05:30.300 --> 00:05:32.250
lots of users out there.

00:05:32.250 --> 00:05:33.440
Some of them are malicious.

00:05:33.440 --> 00:05:36.690
Some of them are just not
doing the right thing.

00:05:36.690 --> 00:05:39.310
And we want everyone to protect
from each other.

00:05:39.310 --> 00:05:41.210
We want to protect
Google from you.

00:05:41.210 --> 00:05:44.140
We want to protect you from
each other, so that nobody

00:05:44.140 --> 00:05:46.190
steals over there.

00:05:46.190 --> 00:05:51.480
So when we analyzed all this,
we realized that it's not a

00:05:51.480 --> 00:05:54.260
really good idea to take the
Google's MapReduce and give it

00:05:54.260 --> 00:05:55.240
to you as is.

00:05:55.240 --> 00:05:57.980
We realized that we need
to do lots and lots

00:05:57.980 --> 00:05:59.320
of additional work.

00:05:59.320 --> 00:06:02.590
And that's why, a year ago, we
released the Mapper Library,

00:06:02.590 --> 00:06:05.020
which implemented the first
step of MapReduce,

00:06:05.020 --> 00:06:06.600
which is map phase.

00:06:06.600 --> 00:06:10.895
We released it at Google I/O
2010, in this building.

00:06:10.895 --> 00:06:14.160
Since then, it's been heavily
used by developers everywhere,

00:06:14.160 --> 00:06:15.795
outside, inside Google.

00:06:15.795 --> 00:06:17.550
We even use it in
App Engine Team.

00:06:17.550 --> 00:06:19.180
We use it in admin console.

00:06:19.180 --> 00:06:21.190
We generate reports with that.

00:06:21.190 --> 00:06:24.880
We have new index pipeline,
which is in the works, which

00:06:24.880 --> 00:06:25.840
uses Mapper Library.

00:06:25.840 --> 00:06:28.790
And lots of people-- like, there
was recently a blog post

00:06:28.790 --> 00:06:31.420
about guys using
Mapper Library,

00:06:31.420 --> 00:06:32.830
which is already useful.

00:06:32.830 --> 00:06:35.040
And during the whole year,
we've been working on

00:06:35.040 --> 00:06:36.330
improving that.

00:06:36.330 --> 00:06:39.340
And they had quite
a lot of set of

00:06:39.340 --> 00:06:41.300
improvements to the library.

00:06:41.300 --> 00:06:43.290
Let me go through
some of them.

00:06:43.290 --> 00:06:47.850
We have Control API, so that
you can programatically

00:06:47.850 --> 00:06:49.300
control your jobs.

00:06:49.300 --> 00:06:51.660
So that you can, for example,
create a cron job, which will

00:06:51.660 --> 00:06:53.970
run everyday and start Mapper.

00:06:53.970 --> 00:06:56.960
We have custom mutation pools,
which is the way to batch some

00:06:56.960 --> 00:07:00.940
work within map function calls
and process it more

00:07:00.940 --> 00:07:01.650
efficiently.

00:07:01.650 --> 00:07:05.760
We have namespaces support, so
that normally you can run your

00:07:05.760 --> 00:07:08.350
Mapper over some fixed
namespace, if you use

00:07:08.350 --> 00:07:10.790
namespace, or some several
namespaces.

00:07:10.790 --> 00:07:15.370
You can even run Mappers over
namespaces, not the data

00:07:15.370 --> 00:07:17.960
inside namespaces but just over
namespaces, so that you

00:07:17.960 --> 00:07:20.100
can analyze your namespaces.

00:07:20.100 --> 00:07:21.030
You can do some kind of

00:07:21.030 --> 00:07:22.940
migration, statistics, whatever.

00:07:22.940 --> 00:07:26.292
We have implemented better
sharding with scatter indices.

00:07:26.292 --> 00:07:29.215
So they think sharding
is now pretty good.

00:07:29.215 --> 00:07:34.000
And there are many more small
improvements to the library.

00:07:34.000 --> 00:07:38.640
So if we take a look at this,
and we ask a question, what

00:07:38.640 --> 00:07:41.740
should we do to take
Mapper Library to

00:07:41.740 --> 00:07:43.750
the MapReduce Library?

00:07:43.750 --> 00:07:46.390
The first thing is we need
some kind of storage for

00:07:46.390 --> 00:07:49.630
intermediate data, because map
jobs, they tend to output lots

00:07:49.630 --> 00:07:53.330
and lots of data
in MapReducers.

00:07:53.330 --> 00:07:56.020
And for that one, we designed
the Files API, which we'll go

00:07:56.020 --> 00:07:58.560
through in more detail later.

00:07:58.560 --> 00:08:02.830
And we released it in March
2011, just two months ago.

00:08:02.830 --> 00:08:05.860
We also have to implement
the shuffler service.

00:08:05.860 --> 00:08:09.470
And reducer is not a problem,
because reducer is actually

00:08:09.470 --> 00:08:10.510
the mapper.

00:08:10.510 --> 00:08:12.710
It just goes over
shuffle output.

00:08:12.710 --> 00:08:14.660
And of course, we have
to write lots and

00:08:14.660 --> 00:08:16.690
lots of glue code.

00:08:16.690 --> 00:08:20.070
So today, I'm happy to say
that we're launching the

00:08:20.070 --> 00:08:21.230
shuffler functionality.

00:08:21.230 --> 00:08:25.450
And this actually has two
different functionalities.

00:08:25.450 --> 00:08:28.800
First of all, we have a full
in-memory user space,

00:08:28.800 --> 00:08:32.419
task-driven open source shuffler
for kind of small

00:08:32.419 --> 00:08:33.500
data stats.

00:08:33.500 --> 00:08:35.730
And small, I mean like, hundreds
of megabytes, maybe

00:08:35.730 --> 00:08:37.730
gigabytes, maybe even more.

00:08:37.730 --> 00:08:39.370
Who knows?

00:08:39.370 --> 00:08:41.510
This is completely open
source for Python.

00:08:41.510 --> 00:08:42.799
Java comes soon.

00:08:42.799 --> 00:08:45.300
It will be open source, too.

00:08:45.300 --> 00:08:48.530
We also announced that we want
to start trusted testers

00:08:48.530 --> 00:08:50.250
access to big shuffler.

00:08:50.250 --> 00:08:53.260
So if you have some kind of data
set, which you need to

00:08:53.260 --> 00:08:57.500
run MapReducer on, and you have
hundreds of gigabytes of

00:08:57.500 --> 00:09:00.670
data and you want to run
MapReducer, just get in touch

00:09:00.670 --> 00:09:04.010
with us and we will see how
we can work for you.

00:09:04.010 --> 00:09:06.820
And we also, of course, written
all the integration

00:09:06.820 --> 00:09:09.885
pieces which are needed to
run your MapReduce jobs.

00:09:09.885 --> 00:09:13.380
And they are part of Mapper
Library, which we are now

00:09:13.380 --> 00:09:15.510
deciding to call MapReduce
Library.

00:09:15.510 --> 00:09:17.550
It's not a map library
anymore.

00:09:17.550 --> 00:09:22.470
So without going--

00:09:22.470 --> 00:09:24.280
we'll go to the technical
details later.

00:09:24.280 --> 00:09:27.600
But first, let's go through some
simple examples of how

00:09:27.600 --> 00:09:29.300
MapReducers look like.

00:09:29.300 --> 00:09:31.640
And the first really simple
example, which is the, hello

00:09:31.640 --> 00:09:36.120
world, of MapReduce, is a
word count MapReduce.

00:09:36.120 --> 00:09:40.050
And the goal of MapReduce is to
take some set of text, some

00:09:40.050 --> 00:09:44.690
text, and calculate how many
times a single word appears in

00:09:44.690 --> 00:09:47.490
the text, like basically
statistics of distributions.

00:09:47.490 --> 00:09:49.590
And map function is
really simple.

00:09:49.590 --> 00:09:53.720
It's almost like, two lines,
even if you throw in some

00:09:53.720 --> 00:09:54.880
additional declarations.

00:09:54.880 --> 00:09:57.720
Basically you split the text in
words, and your yield word

00:09:57.720 --> 00:10:00.060
is the key and empty
screen is values.

00:10:00.060 --> 00:10:02.290
We're not using values
here, per se.

00:10:02.290 --> 00:10:04.790
We're just going to need
them to count those.

00:10:04.790 --> 00:10:08.470
So if you take the famous quote
from the Pulp Fiction,

00:10:08.470 --> 00:10:12.370
then for five words, we can
output five key value pairs.

00:10:12.370 --> 00:10:16.140
Then the shuffler will take this
data, will process it,

00:10:16.140 --> 00:10:18.610
and then will bundle
all values for

00:10:18.610 --> 00:10:19.800
the same keys together.

00:10:19.800 --> 00:10:23.410
So we'll have three
pairs that maybe--

00:10:23.410 --> 00:10:26.110
that are going to have all two
values from the input, which

00:10:26.110 --> 00:10:28.430
are empty screens
that also two.

00:10:28.430 --> 00:10:30.530
And baby's going to
have only one.

00:10:30.530 --> 00:10:32.690
And reduce phase just
ignores values.

00:10:32.690 --> 00:10:35.890
It just counts how many
of those there are.

00:10:35.890 --> 00:10:39.280
So it's going to output
Zed's two times.

00:10:39.280 --> 00:10:40.250
Dead is two times.

00:10:40.250 --> 00:10:43.180
And baby is one time.

00:10:43.180 --> 00:10:47.855
So let's see how this works.

00:10:47.855 --> 00:10:50.260
Is it large enough?

00:10:50.260 --> 00:10:53.920
So unfortunately, this stuff
runs for several minutes in

00:10:53.920 --> 00:10:55.650
the data set that they
chose, so I decided

00:10:55.650 --> 00:10:58.390
not to run it directly.

00:10:58.390 --> 00:11:01.700
And I took 90 books,
90-something books from

00:11:01.700 --> 00:11:05.600
Gutenberg Project, which
amounts to tens of

00:11:05.600 --> 00:11:06.880
megabytes of text.

00:11:06.880 --> 00:11:08.530
And I run this MapReduce.

00:11:08.530 --> 00:11:10.260
It took six minutes.

00:11:10.260 --> 00:11:13.740
And you can see that
this is our new UI

00:11:13.740 --> 00:11:15.450
for multi-stage processes.

00:11:15.450 --> 00:11:17.400
We're going to talk abut about
this with it later.

00:11:17.400 --> 00:11:20.640
But you can see that the whole
MapReduce count took, like,

00:11:20.640 --> 00:11:22.570
six minutes, 40 seconds.

00:11:22.570 --> 00:11:25.030
Map phase took one minute,
40 seconds.

00:11:25.030 --> 00:11:26.940
Shuffle took three minutes.

00:11:26.940 --> 00:11:30.230
And reduce took one minute,
50 seconds.

00:11:30.230 --> 00:11:34.190
And there were some additional
steps, like cleanup and some

00:11:34.190 --> 00:11:35.470
statistics steps.

00:11:35.470 --> 00:11:37.470
And if you take a reduce
job, which has

00:11:37.470 --> 00:11:39.540
finished in a minute--

00:11:39.540 --> 00:11:42.320
here it is one minute, 50
seconds-- we can see that it

00:11:42.320 --> 00:11:48.010
read 78 gigabytes of
intermediate data, which is

00:11:48.010 --> 00:11:48.250
quite good.

00:11:48.250 --> 00:11:50.360
I'm told that shuffler is
supposed to process hundreds

00:11:50.360 --> 00:11:53.190
of megabytes, but we're actually
being run to being

00:11:53.190 --> 00:11:56.120
able to run it like over 78
gigabytes without big

00:11:56.120 --> 00:12:00.820
problems. The output--

00:12:00.820 --> 00:12:02.020
how much?

00:12:02.020 --> 00:12:04.410
One gigabyte of data.

00:12:04.410 --> 00:12:10.910
And the total input
size was just--

00:12:10.910 --> 00:12:15.830
the input size, we have, like,
50 megabytes of text, books.

00:12:15.830 --> 00:12:18.500
This is how its output
looks like.

00:12:18.500 --> 00:12:22.050
So we have all the words,
which are in some order.

00:12:22.050 --> 00:12:25.820
They're kind of sorted, but
we'll talk about this later.

00:12:25.820 --> 00:12:29.840
But we can see that captivity,
or like, I don't know what it

00:12:29.840 --> 00:12:33.280
is, but it's actually
present only once.

00:12:33.280 --> 00:12:43.860
So if we use some kind of graph,
with this word, we can

00:12:43.860 --> 00:12:46.790
see that it actually happens
only once, in this only book.

00:12:46.790 --> 00:12:49.360
And yeah, it works.

00:12:49.360 --> 00:12:53.135
So this is the word
count MapReduce.

00:12:53.135 --> 00:12:57.580
And now let's improve our word
count MapReduce to build the

00:12:57.580 --> 00:13:01.380
simplest possible, really useful
MapReduce, which is a

00:13:01.380 --> 00:13:04.350
building block for, even
for google.com.

00:13:04.350 --> 00:13:07.950
And this MapReduce is called
building inverse index, which

00:13:07.950 --> 00:13:11.800
means that we want to build a
huge map which allows you to

00:13:11.800 --> 00:13:14.650
look up which documents
have a given word.

00:13:14.650 --> 00:13:17.270
And you can actually say
that this is how

00:13:17.270 --> 00:13:18.560
Google Count works.

00:13:18.560 --> 00:13:24.083
So if you have internet on your
CD-ROM and you upload it

00:13:24.083 --> 00:13:26.250
to App Engine, then you
can build the Google

00:13:26.250 --> 00:13:28.170
Count search index.

00:13:28.170 --> 00:13:32.000
So we use the same stuff, but
instead of ignoring file name,

00:13:32.000 --> 00:13:34.660
instead of outputting empty
screens, we'll output the file

00:13:34.660 --> 00:13:36.860
name, or document name, or
something which identifies

00:13:36.860 --> 00:13:39.410
where this word comes from.

00:13:39.410 --> 00:13:42.900
And the only thing we do in
reduce, we just remove

00:13:42.900 --> 00:13:44.050
duplicate values.

00:13:44.050 --> 00:13:48.170
We just built the list that this
word is going to occurs

00:13:48.170 --> 00:13:50.770
in this document, this document,
and this document.

00:13:50.770 --> 00:13:55.600
Of course, instead of just
duplicating documents, you can

00:13:55.600 --> 00:13:58.010
calculate how many times
it occurs in documents.

00:13:58.010 --> 00:14:01.810
So later you can compute some
kind of ranking, whatever.

00:14:01.810 --> 00:14:05.160
But this is the simplest one.

00:14:05.160 --> 00:14:08.390
Again, I'm going to show you
that it's actually ran a

00:14:08.390 --> 00:14:10.510
little bit slower
for some reason.

00:14:10.510 --> 00:14:12.250
It's undetermined
minutes of time.

00:14:12.250 --> 00:14:17.680
But yeah, it took six
minutes, 50 seconds.

00:14:17.680 --> 00:14:22.700
If you take a look at the size
of intermediate data--

00:14:22.700 --> 00:14:24.960
here it is.

00:14:24.960 --> 00:14:29.080
We can see that we actually
processed 170 gigabytes of

00:14:29.080 --> 00:14:31.190
intermediate data.

00:14:31.190 --> 00:14:34.200
And the whole MapReduce took six
minutes, which we think is

00:14:34.200 --> 00:14:36.140
actually quite good.

00:14:36.140 --> 00:14:42.380
So here is how the output looks
like for the whole 60

00:14:42.380 --> 00:14:46.370
megabytes of books from
Gutenberg Project.

00:14:46.370 --> 00:14:47.540
So you can pick--

00:14:47.540 --> 00:14:50.520
now you can get this document.

00:14:50.520 --> 00:14:53.740
And you can actually build a
web storage, which has some

00:14:53.740 --> 00:14:55.260
kind of search here.

00:14:55.260 --> 00:14:59.120
So let's verify that it works.

00:14:59.120 --> 00:15:02.540
Let's take a look at aviators.

00:15:02.540 --> 00:15:04.946
Here it is--

00:15:04.946 --> 00:15:10.150
want to see aviators,
everything.

00:15:10.150 --> 00:15:12.520
You can actually see that there
are only two documents

00:15:12.520 --> 00:15:19.260
which have this word, 1059
and 3061, which is true.

00:15:19.260 --> 00:15:24.110
So I'm quite confident
that it works.

00:15:24.110 --> 00:15:28.915
So these were two most common
and the simplest MapReducers

00:15:28.915 --> 00:15:32.200
you could write.

00:15:32.200 --> 00:15:34.690
Later in this talk, I'll show
you a little bit more

00:15:34.690 --> 00:15:36.830
complicated and interesting
MapReduce.

00:15:36.830 --> 00:15:39.020
But now I would like
to dive into

00:15:39.020 --> 00:15:40.700
technical bits of our solutions.

00:15:40.700 --> 00:15:43.560
I'd like to talk about how
we did what we did.

00:15:43.560 --> 00:15:46.570
And actually I would like to
show you that you could build

00:15:46.570 --> 00:15:50.190
something, or you can build
something, similar or modified

00:15:50.190 --> 00:15:53.510
to your needs, now that
you know how it works.

00:15:53.510 --> 00:15:56.280
And I'm going to talk
about two things.

00:15:56.280 --> 00:16:00.400
I'm going to talk about Files
API, which was our solution to

00:16:00.400 --> 00:16:01.340
MapReduce storage problem.

00:16:01.340 --> 00:16:03.650
And I'm going to talk about
User-Space Shuffler, about how

00:16:03.650 --> 00:16:06.100
we built it, which algorithm
did we use,

00:16:06.100 --> 00:16:07.320
and so on, so forth.

00:16:07.320 --> 00:16:08.270
So Files API.

00:16:08.270 --> 00:16:11.970
As I told you, and as you have
seen, MapReduce jobs generate

00:16:11.970 --> 00:16:13.710
lots of intermediate data.

00:16:13.710 --> 00:16:17.690
This was a tiny, simple
MapReduce, which is just an

00:16:17.690 --> 00:16:21.320
example, and processes just 60
megabytes of input data.

00:16:21.320 --> 00:16:25.280
And it generated 170 gigabytes
of intermediate data.

00:16:25.280 --> 00:16:27.830
And of course, you have
to store it somewhere.

00:16:27.830 --> 00:16:31.900
So before Files API, we had
three storage systems. We had

00:16:31.900 --> 00:16:34.520
Data Store, which is kind of
expensive because it's

00:16:34.520 --> 00:16:38.320
replicated over the multiple
data centers, lots of copies.

00:16:38.320 --> 00:16:42.510
And it also has one megabyte
entity limit, which makes

00:16:42.510 --> 00:16:45.580
storing 170 gigabytes of
data kind of hard.

00:16:45.580 --> 00:16:46.570
You have to split it.

00:16:46.570 --> 00:16:48.350
You have to remember where
you put its chunk.

00:16:48.350 --> 00:16:50.000
You have to read it in order.

00:16:50.000 --> 00:16:51.470
It's not a pleasant task.

00:16:51.470 --> 00:16:54.330
We also had a blobstore,
which is really

00:16:54.330 --> 00:16:56.400
good, but it's redundant.

00:16:56.400 --> 00:16:58.610
The only way you could get
data in, you could upload

00:16:58.610 --> 00:17:00.615
files through a CPU
form, which does

00:17:00.615 --> 00:17:01.870
not work for MapReduce.

00:17:01.870 --> 00:17:03.290
And you had Memcache.

00:17:03.290 --> 00:17:04.940
Memcache is kind of good.

00:17:04.940 --> 00:17:07.450
It's really fast, but it's small
and it's not reliable.

00:17:07.450 --> 00:17:10.290
You can never be sure that the
data you put in is still

00:17:10.290 --> 00:17:12.660
there, because there might
be memory pressure

00:17:12.660 --> 00:17:15.050
or something else.

00:17:15.050 --> 00:17:18.800
So that's why we designed
the Files API.

00:17:18.800 --> 00:17:22.640
We wanted to give you familiar
files-like interface to some

00:17:22.640 --> 00:17:26.010
kind of virtual file systems.
These are not local files.

00:17:26.010 --> 00:17:27.839
These are virtual file
systems. They

00:17:27.839 --> 00:17:30.290
look like local files.

00:17:30.290 --> 00:17:34.320
If you have ever accessed local
files, you will find it

00:17:34.320 --> 00:17:36.330
similar to you.

00:17:36.330 --> 00:17:37.760
We released it in 1.4.3.

00:17:37.760 --> 00:17:39.800
We integrate it with Mapper
Library the way I'm

00:17:39.800 --> 00:17:40.900
going to show you.

00:17:40.900 --> 00:17:44.800
And we kind of consider it
to be a low-level API.

00:17:44.800 --> 00:17:47.660
It's not something that
every user, we

00:17:47.660 --> 00:17:48.370
think, is going to need.

00:17:48.370 --> 00:17:51.320
Most users will probably just
use MapReduce Library.

00:17:51.320 --> 00:17:53.390
But you might find some
interesting needs

00:17:53.390 --> 00:17:54.370
for this one, too.

00:17:54.370 --> 00:17:57.440
And it's also still an
experimental library,

00:17:57.440 --> 00:17:58.140
experimental API.

00:17:58.140 --> 00:18:00.530
We're still looking
at how it works.

00:18:00.530 --> 00:18:02.140
We're still trying to
understand what you

00:18:02.140 --> 00:18:04.370
need, what we need.

00:18:04.370 --> 00:18:06.160
And these files--

00:18:06.160 --> 00:18:09.460
I said that the API is really
familiar to local file

00:18:09.460 --> 00:18:14.980
systems. But it has some
huge differences.

00:18:14.980 --> 00:18:18.240
Basically, the biggest
difference here is that files

00:18:18.240 --> 00:18:21.020
have to stay writable
and readable.

00:18:21.020 --> 00:18:23.050
In writable state, you can
write the file but

00:18:23.050 --> 00:18:23.660
you cannot read it.

00:18:23.660 --> 00:18:25.540
In readable state, you
can read but you

00:18:25.540 --> 00:18:26.990
cannot write to it.

00:18:26.990 --> 00:18:30.730
And all files start
in writable.

00:18:30.730 --> 00:18:33.700
Later, when you're done writing,
you can finalize the

00:18:33.700 --> 00:18:38.150
file, and you can transfer
it to readable state.

00:18:38.150 --> 00:18:39.870
There is no way out
of readable state.

00:18:39.870 --> 00:18:42.500
Once it's finalized,
it's forever.

00:18:42.500 --> 00:18:46.820
All writes are append-only, so
there is no random access.

00:18:46.820 --> 00:18:51.540
All writes are atomic, meaning
that if you output 100 bytes,

00:18:51.540 --> 00:18:54.265
then nothing is going to get
in between of those.

00:18:54.265 --> 00:18:56.750
And they are fully serializable
between

00:18:56.750 --> 00:19:00.130
concurrent clients, meaning that
you might have lots of

00:19:00.130 --> 00:19:03.380
big round tasks, lots of tasking
tasks tasked right

00:19:03.380 --> 00:19:06.440
into the same file at the same
time and we're going to make

00:19:06.440 --> 00:19:11.520
sure that all the data does not
interweave between them.

00:19:11.520 --> 00:19:14.510
And they also say that some
concrete file systems, they

00:19:14.510 --> 00:19:17.915
might have some additional APIs
for dealing with their

00:19:17.915 --> 00:19:18.960
own properties.

00:19:18.960 --> 00:19:22.840
And every file system is going
to have their own reliability,

00:19:22.840 --> 00:19:27.290
or like volubility constraints
or guarantees.

00:19:27.290 --> 00:19:29.840
Files API doesn't
know about that.

00:19:29.840 --> 00:19:32.360
In March, we'll release
the first file system.

00:19:32.360 --> 00:19:34.600
This is a blobstore
filesystem.

00:19:34.600 --> 00:19:37.980
So we actually took the
read-only blobstore facility

00:19:37.980 --> 00:19:39.270
and we made it writable.

00:19:39.270 --> 00:19:41.740
So you can now write directly
to blobstore.

00:19:41.740 --> 00:19:43.460
You can create really
huge files.

00:19:43.460 --> 00:19:48.420
There is no limitation aside
64-bit plans, which I think is

00:19:48.420 --> 00:19:51.350
going to be enough for the
next thousand years.

00:19:51.350 --> 00:19:54.820
Once you finalize the file,
those files are fully durable.

00:19:54.820 --> 00:19:58.100
They're going to be replicated
across multiple data centers.

00:19:58.100 --> 00:20:01.890
And they're going to have the
same reliability guarantees as

00:20:01.890 --> 00:20:03.830
high-replication data store.

00:20:03.830 --> 00:20:07.620
But the important point here is
that writable files are not

00:20:07.620 --> 00:20:11.090
durable, meaning that if
something bad happens, for

00:20:11.090 --> 00:20:14.660
example, we have data store
failure, or we might have a

00:20:14.660 --> 00:20:16.270
maintenance period.

00:20:16.270 --> 00:20:18.640
So you might lose
your files while

00:20:18.640 --> 00:20:20.250
they're in writable state.

00:20:20.250 --> 00:20:23.050
It is especially so that while
you write, we don't replicate

00:20:23.050 --> 00:20:24.340
it, so that [UNINTELLIGIBLE]

00:20:24.340 --> 00:20:25.770
is good.

00:20:25.770 --> 00:20:28.140
Because they think that
MapReduce is going to be the

00:20:28.140 --> 00:20:30.150
primary consumer of this API.

00:20:30.150 --> 00:20:31.050
OK.

00:20:31.050 --> 00:20:32.990
If something bad has happened,
you're going to restart

00:20:32.990 --> 00:20:35.965
MapReduce jobs.

00:20:35.965 --> 00:20:39.110
And since this is blobstore, you
can fetch a blob key for

00:20:39.110 --> 00:20:42.200
finalized file and use the
familiar blobstore API.

00:20:42.200 --> 00:20:44.680
So if you want to download that
file or serve it to the

00:20:44.680 --> 00:20:47.900
user, you could get the blob key
and serve it directly to

00:20:47.900 --> 00:20:50.340
the user without writing
too many code.

00:20:50.340 --> 00:20:53.230
This way you can create a, I
don't know, you can write a

00:20:53.230 --> 00:20:55.690
code which generates
some kind of movie.

00:20:55.690 --> 00:20:58.060
You just write the blobstore
frame by frame, then you

00:20:58.060 --> 00:21:01.150
finalize it, and you
can serve it.

00:21:01.150 --> 00:21:03.890
This is a simple Python example
of how you could

00:21:03.890 --> 00:21:06.890
access it if Java
has its own API.

00:21:06.890 --> 00:21:09.520
And yeah, we don't have much
time to go through all of

00:21:09.520 --> 00:21:14.430
those, but basically it looks
familiar to the language.

00:21:14.430 --> 00:21:17.290
It's mostly like the files
you have in Python.

00:21:17.290 --> 00:21:19.515
So first of all, you create
the files in blobstore.

00:21:19.515 --> 00:21:22.820
And you specifically say, OK, I
want a file from blobstore.

00:21:22.820 --> 00:21:24.060
Then you open it to write.

00:21:24.060 --> 00:21:26.230
You write data.

00:21:26.230 --> 00:21:31.240
And once you're done, you say,
OK, finalize the file.

00:21:31.240 --> 00:21:32.910
To read it, it's
the same stuff.

00:21:32.910 --> 00:21:33.910
You can open file.

00:21:33.910 --> 00:21:34.700
You can read from it.

00:21:34.700 --> 00:21:36.720
You can seek in the file.

00:21:36.720 --> 00:21:39.960
Write is append-only, and read
can have random access.

00:21:39.960 --> 00:21:42.815
So you can do lots of crazy,
interesting things.

00:21:42.815 --> 00:21:46.520
And if you want to get blob key,
you just call, get blob

00:21:46.520 --> 00:21:51.240
key function, and you're going
to get a blob key for filing.

00:21:51.240 --> 00:21:53.750
As I said, we integrate
with Mapper Library.

00:21:53.750 --> 00:21:57.230
We edit some kind of output
writers support.

00:21:57.230 --> 00:21:58.840
So there's one line
in MapReduce.

00:21:58.840 --> 00:22:00.200
It's [UNINTELLIGIBLE].

00:22:00.200 --> 00:22:03.040
It would just specify, OK, I
want the output of this Mapper

00:22:03.040 --> 00:22:05.640
function to go directly
to blobstore.

00:22:05.640 --> 00:22:08.600
This way, by having single-line
map function, we

00:22:08.600 --> 00:22:13.530
just confirm send it to the csv
line, or xml, whatever you

00:22:13.530 --> 00:22:16.390
can-- you can easily write a
mapper which will export your

00:22:16.390 --> 00:22:17.485
data into blobstore.

00:22:17.485 --> 00:22:20.110
And since you have control API,
you can actually create a

00:22:20.110 --> 00:22:24.700
cron job, and do a copy of your
data every day, or every

00:22:24.700 --> 00:22:27.330
week, or run it whenever
you need.

00:22:27.330 --> 00:22:31.350
We also plan to migrate, both
download it and upload it this

00:22:31.350 --> 00:22:32.630
functionality.

00:22:32.630 --> 00:22:34.730
We're not there yet, but it
will eventually come.

00:22:34.730 --> 00:22:39.730
But meanwhile, it's now easy to
get the data in some sort

00:22:39.730 --> 00:22:43.120
of downloadable format.

00:22:43.120 --> 00:22:45.190
And we also have some low-level
features, which

00:22:45.190 --> 00:22:47.820
we're not going to spend lots
of time, but basically have

00:22:47.820 --> 00:22:49.320
exclusive locks.

00:22:49.320 --> 00:22:50.530
You can lock files.

00:22:50.530 --> 00:22:54.130
You can make sure that no
one else accesses it.

00:22:54.130 --> 00:22:59.150
And we have sequence keys, so
that if you need some kind of

00:22:59.150 --> 00:23:01.460
guarantee that you have written
the data you're

00:23:01.460 --> 00:23:03.950
already writing, because request
might be retried,

00:23:03.950 --> 00:23:07.160
possibly might be rerun, or
request might come the second

00:23:07.160 --> 00:23:08.460
time from the user.

00:23:08.460 --> 00:23:11.505
That there is some notion of
sequence key and the API will

00:23:11.505 --> 00:23:13.260
take care of you.

00:23:13.260 --> 00:23:17.860
If you need this, read the API
documentation and source code.

00:23:17.860 --> 00:23:21.660
And in the future, we're going
to introduce more file

00:23:21.660 --> 00:23:25.010
systems. The first one which we
really, really want to get

00:23:25.010 --> 00:23:27.440
out, we call it Tempfile
system.

00:23:27.440 --> 00:23:31.180
This is going to be much faster,
much cheaper, because

00:23:31.180 --> 00:23:33.340
it's not going to replicated
anywhere.

00:23:33.340 --> 00:23:35.280
But it's not going to
be durable at all,

00:23:35.280 --> 00:23:36.940
even finalized files.

00:23:36.940 --> 00:23:40.180
It might lose those if something
bad happens.

00:23:40.180 --> 00:23:43.420
And you're going to have each
file be stored only for

00:23:43.420 --> 00:23:47.770
several days, because we want
this to be a MapReduce banking

00:23:47.770 --> 00:23:48.960
file system.

00:23:48.960 --> 00:23:51.410
We want it to be really fast. We
want it to be able to store

00:23:51.410 --> 00:23:56.420
terabytes there without
selling your house.

00:23:59.150 --> 00:24:03.180
Another thing is that we want
to integrate with other

00:24:03.180 --> 00:24:06.670
storage technologies, which
we got at Google.

00:24:06.670 --> 00:24:11.070
Yeah, just keep looking at
we're doing and we have--

00:24:11.070 --> 00:24:15.140
we're going to have lots of
exciting file systems coming.

00:24:15.140 --> 00:24:16.760
So this was Files API.

00:24:16.760 --> 00:24:19.870
And now let's talk about the
bread and butter of our

00:24:19.870 --> 00:24:23.270
MapReduce implementation, which
is User-Space Shuffler.

00:24:23.270 --> 00:24:27.110
So as I told you, we're going
to have two shufflers: the

00:24:27.110 --> 00:24:30.530
User-Space, which is kind of
simple, open source, and it

00:24:30.530 --> 00:24:34.050
has its limits on the data it
can process; and we're going

00:24:34.050 --> 00:24:37.380
to have a huge shuffler service,
which is hidden,

00:24:37.380 --> 00:24:39.970
which is just an API code which
says, OK, shuffle this

00:24:39.970 --> 00:24:42.840
for me, and is going to process
lots and lots of data

00:24:42.840 --> 00:24:44.160
really, really quickly.

00:24:44.160 --> 00:24:46.690
But we wanted to have User-Space
Shuffler for small

00:24:46.690 --> 00:24:47.975
and medium jobs.

00:24:47.975 --> 00:24:50.760
And as you remember, the job
of the shuffler is to

00:24:50.760 --> 00:24:53.310
consolidate values for the
same key together.

00:24:53.310 --> 00:24:56.040
So look around the data,
find those same

00:24:56.040 --> 00:24:58.200
keys, values together.

00:24:58.200 --> 00:25:01.580
And even that we want to have
this User-Space Shuffler,

00:25:01.580 --> 00:25:03.160
meaning it is in full
source code.

00:25:03.160 --> 00:25:05.320
It has no new App Engine
components.

00:25:05.320 --> 00:25:07.610
Anyone could have built
something like that

00:25:07.610 --> 00:25:10.000
We still want it to be
reasonably fast, reasonably

00:25:10.000 --> 00:25:12.160
scalable, and reasonably
efficient.

00:25:12.160 --> 00:25:15.690
Because we just saw that,
yeah, 170 gigabyte

00:25:15.690 --> 00:25:16.470
is not a big deal.

00:25:16.470 --> 00:25:19.130
It's just a couple minutes,
which is good.

00:25:19.130 --> 00:25:23.030
So the way we're going
to arrive at shuffler

00:25:23.030 --> 00:25:25.950
implementation is we're going
to start with really, really

00:25:25.950 --> 00:25:26.810
stupid shuffler.

00:25:26.810 --> 00:25:29.750
We're going to improve it step
by step until we get to the

00:25:29.750 --> 00:25:31.020
file algorithm.

00:25:31.020 --> 00:25:33.610
And the first inaugural
shuffler is just load

00:25:33.610 --> 00:25:36.050
everything to memory, sort it.

00:25:36.050 --> 00:25:38.430
And read the sorted array.

00:25:38.430 --> 00:25:39.890
This is going to
look like this.

00:25:39.890 --> 00:25:43.120
Once you have your values sorted
by key, you can just

00:25:43.120 --> 00:25:45.970
read and see, OK, this is the
same key, grab this value.

00:25:45.970 --> 00:25:47.090
Same key, grab this value.

00:25:47.090 --> 00:25:48.630
OK, this is another key.

00:25:48.630 --> 00:25:51.030
Here are your values.

00:25:51.030 --> 00:25:54.290
It's really just a couple
lines of code, but

00:25:54.290 --> 00:25:56.030
unfortunately, it has
lots of problems.

00:25:56.030 --> 00:26:00.390
First of all, it's memory-bound
algorithm, by how

00:26:00.390 --> 00:26:02.980
much data you can
fit in memory.

00:26:02.980 --> 00:26:06.360
Like, if you're having a
separate workstation, like

00:26:06.360 --> 00:26:11.630
server, computer, you might be
lucky to have 64 gigabytes.

00:26:11.630 --> 00:26:16.140
So it's actually memory-bound,
and there is no way to

00:26:16.140 --> 00:26:19.120
increase performance by throwing
more resources into

00:26:19.120 --> 00:26:20.920
it, because it's
single-thread.

00:26:20.920 --> 00:26:24.630
It's just sorted, no
way out of it.

00:26:24.630 --> 00:26:30.480
So we're going to improve this
one by chunking our input data

00:26:30.480 --> 00:26:35.310
into chunks, and store the
data into Files API.

00:26:35.310 --> 00:26:39.570
And then each chunk will be
sorted by its own. so those

00:26:39.570 --> 00:26:42.490
chunks should be of some size,
because when you're writing in

00:26:42.490 --> 00:26:45.830
App Engine, in Python, it's not
really memory-efficient,

00:26:45.830 --> 00:26:49.010
our chunk size is
like five megs.

00:26:49.010 --> 00:26:51.890
So we're going to sort each one
of those, store it, and

00:26:51.890 --> 00:26:54.090
then we're going to merge stored
all chunks together

00:26:54.090 --> 00:26:55.180
using external merge.

00:26:55.180 --> 00:26:58.270
Or even as MapReduce Library
does, we're just going to

00:26:58.270 --> 00:27:00.580
merge-read without
sorting, just

00:27:00.580 --> 00:27:02.940
merge-read from all of those.

00:27:02.940 --> 00:27:04.660
This is really simple.

00:27:04.660 --> 00:27:09.150
Your upper two blobs go to the
first chunks, lower three to

00:27:09.150 --> 00:27:10.310
the second chunk.

00:27:10.310 --> 00:27:12.280
And then you're going to
merge-read by maintaining

00:27:12.280 --> 00:27:14.070
pointers into each
one of those.

00:27:14.070 --> 00:27:15.330
You read from all of them.

00:27:15.330 --> 00:27:17.800
You see, OK, blue,
blue, blue, blue.

00:27:17.800 --> 00:27:19.610
Then you move each
one of them.

00:27:19.610 --> 00:27:22.960
And yeah, it's going to work.

00:27:22.960 --> 00:27:25.960
And the properties of this
approach is that first of all,

00:27:25.960 --> 00:27:28.030
this is no longer
memory-bound.

00:27:28.030 --> 00:27:31.150
You can sort as many chunks as
you want, as soon as each one

00:27:31.150 --> 00:27:32.770
of those fit in memory.

00:27:32.770 --> 00:27:36.000
Another good thing that this
sorting is parallel.

00:27:36.000 --> 00:27:40.410
You can throw in thousand
computers, give each one a

00:27:40.410 --> 00:27:43.060
chunk, and they're going
to finish in parallel.

00:27:43.060 --> 00:27:45.330
But unfortunately, the
merge phase, merge

00:27:45.330 --> 00:27:47.000
phase is not parallel.

00:27:47.000 --> 00:27:49.850
It's just one phase in a single
computer, which has to

00:27:49.850 --> 00:27:52.980
merge-read all of them.

00:27:52.980 --> 00:27:56.610
It is kind of difficult and slow
to merge-read from too

00:27:56.610 --> 00:27:59.800
many files, like you could
have 10,000s of files, it

00:27:59.800 --> 00:28:02.615
should have some kind of heap
structure to maintain pointers

00:28:02.615 --> 00:28:06.040
into 10,000s of files.

00:28:06.040 --> 00:28:10.220
Yeah, it's getting really,
really slow once you crank up

00:28:10.220 --> 00:28:12.650
number of files.

00:28:12.650 --> 00:28:15.870
So we're going to improve
this, too.

00:28:15.870 --> 00:28:18.130
And the way we're going to
improve it is we're going to

00:28:18.130 --> 00:28:20.710
use the hash code of the key.

00:28:20.710 --> 00:28:24.380
So it's really like, if hash
codes of the key is divisible

00:28:24.380 --> 00:28:28.485
by two and not divisible by two,
then we can process all

00:28:28.485 --> 00:28:30.640
those keys, two keys,
separately.

00:28:30.640 --> 00:28:32.760
Like, all even keys
go into one chunk.

00:28:32.760 --> 00:28:35.575
All odd keys go to
another chunk.

00:28:35.575 --> 00:28:38.500
And we can run the same
algorithm from the previous

00:28:38.500 --> 00:28:40.160
step on all those
chunks together.

00:28:43.110 --> 00:28:45.410
We don't have to worry about
merge-reading all of them,

00:28:45.410 --> 00:28:48.840
because each one of them has
different hash keys.

00:28:48.840 --> 00:28:50.800
This is how it's
going to look.

00:28:50.800 --> 00:28:53.250
It looks quite nice, you know.

00:28:53.250 --> 00:28:55.180
And then I needed
a fourth color.

00:28:55.180 --> 00:28:58.590
But basically once again, we
take a look at each block

00:28:58.590 --> 00:29:00.250
output from Mapper.

00:29:00.250 --> 00:29:03.640
We separate it into chunks
by hash code.

00:29:03.640 --> 00:29:06.860
Like, blue and red goes to the
top, green and yellow goes to

00:29:06.860 --> 00:29:08.420
the bottom.

00:29:08.420 --> 00:29:13.870
Then, we sort data from
each hash chunk.

00:29:13.870 --> 00:29:16.130
So we sort some greens
and yellows.

00:29:16.130 --> 00:29:17.860
We sort some blues and greens.

00:29:17.860 --> 00:29:19.450
And then we're going
to do a merge-read

00:29:19.450 --> 00:29:22.520
from only those chunks.

00:29:22.520 --> 00:29:25.380
And this approach is actually
quite good.

00:29:25.380 --> 00:29:28.750
Because by having hash code, you
can split your output data

00:29:28.750 --> 00:29:31.800
into as many hash code-based
chunks as you want.

00:29:31.800 --> 00:29:32.910
You want thousands?

00:29:32.910 --> 00:29:33.940
You got it.

00:29:33.940 --> 00:29:36.310
You want 10,000?

00:29:36.310 --> 00:29:40.230
So you can have, merge, as
parallel as possible.

00:29:40.230 --> 00:29:43.360
You can run thousands of
merge-reads in parallel.

00:29:43.360 --> 00:29:44.730
All sorting is in parallel.

00:29:44.730 --> 00:29:47.550
There is no memory-bound
thing here.

00:29:47.550 --> 00:29:52.180
If you have to read too many
data, just increase number of

00:29:52.180 --> 00:29:54.616
hash chunks and process
them separately.

00:29:54.616 --> 00:29:57.220
And this is actually the
shuffler, which we released

00:29:57.220 --> 00:29:58.325
today for Python.

00:29:58.325 --> 00:30:02.410
And the Java version is going
to come really, really soon.

00:30:02.410 --> 00:30:05.820
As far as I know, even someone
is working on that.

00:30:09.260 --> 00:30:11.130
Yeah, this was the shuffler.

00:30:11.130 --> 00:30:12.860
This is the way it works.

00:30:12.860 --> 00:30:16.420
Of course, the goal is more
complicated than that, because

00:30:16.420 --> 00:30:18.370
we have to deal with lots
of tiny details.

00:30:18.370 --> 00:30:20.426
But this is the idea.

00:30:20.426 --> 00:30:27.940
And we soon realized that our
MapReducer consisted of

00:30:27.940 --> 00:30:30.410
several offline processes
like map.

00:30:30.410 --> 00:30:33.180
You have to finish map before
you start shuffle.

00:30:33.180 --> 00:30:35.090
Shuffle, you have to
finish shuffle

00:30:35.090 --> 00:30:36.630
before you start reduce.

00:30:36.630 --> 00:30:40.800
So we designed a new API to
chain complex works together,

00:30:40.800 --> 00:30:43.220
and we called it the
Pipeline API.

00:30:43.220 --> 00:30:45.975
And this is actually the glue
which holds the Mapper,

00:30:45.975 --> 00:30:50.160
Shuffler, and Reducer, and
which makes it a single

00:30:50.160 --> 00:30:51.520
MapReduce job.

00:30:51.520 --> 00:30:53.720
And our MapReduce Library
is now fully

00:30:53.720 --> 00:30:54.980
integrated with Pipeline.

00:30:54.980 --> 00:30:58.220
And that UI which you saw was
multiple stages that actually

00:30:58.220 --> 00:30:59.680
come from Pipeline API.

00:30:59.680 --> 00:31:03.020
It's an API which takes care of
all that detail, that, OK,

00:31:03.020 --> 00:31:06.420
after you're done with user, you
have to run cleanup jobs

00:31:06.420 --> 00:31:08.220
to clean up all those
intermediate files.

00:31:08.220 --> 00:31:11.220
This is the API which
enables to do that.

00:31:11.220 --> 00:31:14.500
And if you're interested in this
API, I want to emphasize

00:31:14.500 --> 00:31:17.520
the talk of my colleague,
Brett Slatkin.

00:31:17.520 --> 00:31:20.040
It's called "Large Scale Data
Analysis Using the App Engine

00:31:20.040 --> 00:31:25.060
Pipeline API." I think
it's 4:00 something.

00:31:25.060 --> 00:31:28.710
So yeah, please come in if
you're interested in running

00:31:28.710 --> 00:31:30.180
your own complex processes.

00:31:32.770 --> 00:31:36.970
And now, I promised you a more
complicated, more involving

00:31:36.970 --> 00:31:38.790
example of MapReducer.

00:31:38.790 --> 00:31:43.590
And the examples, we're going
to write is which I call

00:31:43.590 --> 00:31:45.080
distinguishing phrases.

00:31:45.080 --> 00:31:48.970
So the idea is to take a look
at multiple books and figure

00:31:48.970 --> 00:31:53.300
out if some phrase has been used
in one particular book a

00:31:53.300 --> 00:31:55.680
lot, but not used in
the other books.

00:31:55.680 --> 00:31:59.300
Like, if some authors have their
own signature phrases,

00:31:59.300 --> 00:32:03.060
or some books have signature
phrases.

00:32:03.060 --> 00:32:05.790
The way we're going to do it
is we're going to split the

00:32:05.790 --> 00:32:09.610
text into ngrams. ngrams
is a sequence of n

00:32:09.610 --> 00:32:11.620
consequent words together.

00:32:11.620 --> 00:32:15.480
And you can pick the n which
you like, so I ran the

00:32:15.480 --> 00:32:17.480
examples for n equals 4.

00:32:17.480 --> 00:32:20.170
The problem here, if you
increase n a lot, like if you

00:32:20.170 --> 00:32:24.290
use 10 grams, then 10 grams
are probably quite unique

00:32:24.290 --> 00:32:26.500
because these are complete
sentences already.

00:32:26.500 --> 00:32:28.080
And they're quite unique.

00:32:28.080 --> 00:32:31.010
And if you use n equals 2, this
is not even a phrase.

00:32:31.010 --> 00:32:32.640
This is just two words.

00:32:32.640 --> 00:32:35.200
So I picked 4.

00:32:35.200 --> 00:32:37.790
And for its use, the first thing
we're going to do, we're

00:32:37.790 --> 00:32:40.675
going to check if this phrase
is actually frequent.

00:32:40.675 --> 00:32:42.430
We're not interested
in something the

00:32:42.430 --> 00:32:44.780
author used only once.

00:32:44.780 --> 00:32:47.880
So we check if it occurred
10 times.

00:32:47.880 --> 00:32:51.750
Then we're going to check if a
single author or a single book

00:32:51.750 --> 00:32:55.870
has this phrase more than every
other book combined,

00:32:55.870 --> 00:32:58.460
which means that this is really
unique for this book

00:32:58.460 --> 00:33:00.420
because it's more than
everyone else.

00:33:00.420 --> 00:33:04.130
And we're going to run
this on the same data

00:33:04.130 --> 00:33:07.330
set, which is 90 books.

00:33:07.330 --> 00:33:09.020
Let me show you how it went.

00:33:09.020 --> 00:33:12.210
So it took like, seven
minutes or something.

00:33:12.210 --> 00:33:15.520
This is, as I told,
the UI which comes

00:33:15.520 --> 00:33:18.390
from Pipeline library.

00:33:18.390 --> 00:33:20.490
So we have map, line,
shuffle, reduce.

00:33:20.490 --> 00:33:22.810
If you actually open the
shuffle, you can see that

00:33:22.810 --> 00:33:24.700
there are lots of
sort pipelines.

00:33:24.700 --> 00:33:29.470
And each sort pipeline, is
certain one hash based bucket

00:33:29.470 --> 00:33:34.530
the way I explained to you.

00:33:34.530 --> 00:33:37.780
And yeah, so it finished
in like,

00:33:37.780 --> 00:33:40.040
seven minutes, 25 seconds.

00:33:40.040 --> 00:33:43.970
Just to have an idea of if this
seven minutes is good or

00:33:43.970 --> 00:33:47.460
not, I wrote a simple Python
program, which uses the same

00:33:47.460 --> 00:33:50.750
MapReduce Computational Model,
but completely in-memory in a

00:33:50.750 --> 00:33:56.850
single computer, and in one
thread, because it's Python.

00:33:56.850 --> 00:33:59.330
And on my computer, it
took like, almost

00:33:59.330 --> 00:34:01.050
20 minutes to complete.

00:34:01.050 --> 00:34:03.760
So this already faster than
writing in computer.

00:34:03.760 --> 00:34:07.790
And this actually scales, so if
you increase the data set,

00:34:07.790 --> 00:34:10.020
you could just increase number
of shards, and you're not

00:34:10.020 --> 00:34:14.210
supposed to see a huge
time increase.

00:34:14.210 --> 00:34:19.510
And when I was creating this
demo, I wasn't even sure that

00:34:19.510 --> 00:34:22.510
this was going to work, but it
turns out there are lots and

00:34:22.510 --> 00:34:24.809
lots of unique phrases
in books.

00:34:24.809 --> 00:34:26.570
And this is the output.

00:34:26.570 --> 00:34:28.674
There are lots and lots
of phrases here

00:34:28.674 --> 00:34:31.510
which you could use.

00:34:31.510 --> 00:34:32.090
Here it is.

00:34:32.090 --> 00:34:34.370
Money price of corn.

00:34:34.370 --> 00:34:36.699
Let's just check.

00:34:45.400 --> 00:34:46.419
Oh, this is not fair.

00:34:46.419 --> 00:34:48.900
This is just one book which
always talks about

00:34:48.900 --> 00:34:50.090
money price of corn.

00:34:50.090 --> 00:34:53.500
Let's pick something else.

00:34:53.500 --> 00:34:54.469
This is Spanish.

00:34:54.469 --> 00:34:56.070
Yeah, these are the same.

00:34:56.070 --> 00:34:58.090
It's surprising that this
is unique, right?

00:35:03.430 --> 00:35:06.260
Yeah, so you can see that there
were three books which

00:35:06.260 --> 00:35:08.500
use the phrase only once.

00:35:08.500 --> 00:35:10.100
"These are the same"
something.

00:35:10.100 --> 00:35:14.860
And there is one book which
used it seven times.

00:35:14.860 --> 00:35:18.810
And I'm just curious,
this is the--

00:35:18.810 --> 00:35:19.750
what is this?

00:35:19.750 --> 00:35:22.250
This is an old book of
Leonardo da Vinci.

00:35:22.250 --> 00:35:27.240
So you can say that in this book
set, the use of the same

00:35:27.240 --> 00:35:31.130
phrase, kind of uniquely
identifies Leonardo da Vinci.

00:35:31.130 --> 00:35:36.030
If you have a huge corpus of
text, I guess it might become

00:35:36.030 --> 00:35:37.005
really interesting.

00:35:37.005 --> 00:35:39.250
And this was all like--

00:35:39.250 --> 00:35:41.880
if you throw in all the details
which I omitted here,

00:35:41.880 --> 00:35:45.890
like count occurrences in line
of functions, it's like 50

00:35:45.890 --> 00:35:46.920
lines of code.

00:35:46.920 --> 00:35:52.030
It's not a big deal, and it's
really something interesting.

00:35:52.030 --> 00:35:56.260
So this was all I wanted to talk
to you about today, and

00:35:56.260 --> 00:36:02.070
let me summarize what
I was talking about.

00:36:02.070 --> 00:36:05.530
So starting today, I did a push
couple hours ago into

00:36:05.530 --> 00:36:06.690
[UNINTELLIGIBLE].

00:36:06.690 --> 00:36:09.060
You can grab the source
code, and you

00:36:09.060 --> 00:36:10.270
can take a look examples.

00:36:10.270 --> 00:36:11.710
The communication
is coming soon.

00:36:11.710 --> 00:36:15.910
And you can run small, medium,
and who knows how big they can

00:36:15.910 --> 00:36:17.170
be, these User-Space
Shufflers.

00:36:17.170 --> 00:36:19.460
We haven't actually pushed
the limit yet.

00:36:19.460 --> 00:36:22.190
You can run those MapReducers
today in Python.

00:36:22.190 --> 00:36:23.820
Java's going to come
really soon.

00:36:23.820 --> 00:36:26.300
If you're interested in
helping us to get Java

00:36:26.300 --> 00:36:33.740
implementation, just write us
an email and we'll tell you

00:36:33.740 --> 00:36:34.740
where we need help.

00:36:34.740 --> 00:36:37.360
And I know there are already
people helping us.

00:36:37.360 --> 00:36:38.120
Right?

00:36:38.120 --> 00:36:38.906
Yeah.

00:36:38.906 --> 00:36:42.530
And if you're interested in
large MapReducers, like maybe

00:36:42.530 --> 00:36:44.860
we should call them huge
MapReducers, get

00:36:44.860 --> 00:36:45.700
in touch with us.

00:36:45.700 --> 00:36:50.790
We'll give you the API to access
the full-blown shuffler

00:36:50.790 --> 00:36:53.070
for certain terabytes of data.

00:36:53.070 --> 00:36:54.480
I don't know.

00:36:54.480 --> 00:36:57.350
So now I'm ready to answer
any questions

00:36:57.350 --> 00:37:06.230
you might have. Thanks.

00:37:06.230 --> 00:37:08.800
It looks like magic, but you
know, I didn't quite

00:37:08.800 --> 00:37:13.080
understand the shuffle part
and the merge portion.

00:37:13.080 --> 00:37:15.440
You know, as a framework,
it looks very complete.

00:37:15.440 --> 00:37:19.890
But when you run it on Google
App Engine, you have the task

00:37:19.890 --> 00:37:21.570
use which allows
you to do map.

00:37:21.570 --> 00:37:26.520
But as far as the merge and the
shuffling is concerned, if

00:37:26.520 --> 00:37:31.060
you run it parallel-ly as
different execution threads,

00:37:31.060 --> 00:37:35.090
you need to have a common place
where you can do merge.

00:37:35.090 --> 00:37:38.590
It can even be a data store,
or it can be memory.

00:37:38.590 --> 00:37:43.220
If it's data store, the current
BigTable doesn't allow

00:37:43.220 --> 00:37:46.170
you to do a Select
For Update thing.

00:37:46.170 --> 00:37:50.690
Which means if you try to
implement the same inventory

00:37:50.690 --> 00:37:52.462
from two different threads--

00:37:52.462 --> 00:37:52.840
right.

00:37:52.840 --> 00:37:55.730
So when you do an update, and
when you read something else,

00:37:55.730 --> 00:37:57.570
it gives you dirty
data, right?

00:37:57.570 --> 00:37:59.530
MIKE AIZATSKYI: So
basically, yes.

00:37:59.530 --> 00:38:02.160
The Pipeline API gives
you some kind of

00:38:02.160 --> 00:38:05.000
synchronization point.

00:38:05.000 --> 00:38:07.810
It takes care of all
of that by itself.

00:38:07.810 --> 00:38:11.190
It knows all the limitations
of App Engine platforms. It

00:38:11.190 --> 00:38:13.730
knows that it stores data
in data stores.

00:38:13.730 --> 00:38:16.870
It knows that it's bad to access
the same entity from

00:38:16.870 --> 00:38:18.910
multiple requests at
the same time.

00:38:18.910 --> 00:38:22.340
And Pipeline API hides all
the complexity for you.

00:38:22.340 --> 00:38:25.610
It just defines a job as a
simple Python function.

00:38:25.610 --> 00:38:29.300
And you say, OK, run it after
this one completes, and pass

00:38:29.300 --> 00:38:30.390
all the output here.

00:38:30.390 --> 00:38:34.100
And it's going to take
care of that for you.

00:38:34.100 --> 00:38:36.190
OK.

00:38:36.190 --> 00:38:36.520
Right.

00:38:36.520 --> 00:38:37.740
So where do we get--

00:38:37.740 --> 00:38:41.250
because we are trying to do this
for aggregate queries,

00:38:41.250 --> 00:38:43.820
and you're not able to figure
out a way within the--

00:38:43.820 --> 00:38:45.680
MIKE AIZATSKYI: It doesn't use
any kind of aggregate queries.

00:38:45.680 --> 00:38:48.460
It uses key names in a
really interesting

00:38:48.460 --> 00:38:50.938
fashion to achieve that.

00:38:50.938 --> 00:38:51.370
Right.

00:38:51.370 --> 00:38:54.210
But key name, you know,
aggregating information is the

00:38:54.210 --> 00:38:56.650
objective of merging to
a key name, right?

00:38:56.650 --> 00:38:57.200
MIKE AIZATSKYI: Yeah.

00:38:57.200 --> 00:38:59.300
For a use case like
aggregate queries.

00:38:59.300 --> 00:39:02.300
MIKE AIZATSKYI: Yeah, but just
take a look at source codes.

00:39:02.300 --> 00:39:03.550
It's all open source.

00:39:08.780 --> 00:39:10.690
Anyone else want
to be on tape?

00:39:14.780 --> 00:39:17.270
So then we're done.

00:39:17.270 --> 00:39:18.560
Thank you a lot for coming.

00:39:18.560 --> 00:39:22.890
If you have any questions which
you want to ask me off

00:39:22.890 --> 00:39:25.630
the record, feel free
to come to me.

00:39:25.630 --> 00:39:27.170
Thanks a lot.

