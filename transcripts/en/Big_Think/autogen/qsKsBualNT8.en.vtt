WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.780
if greater than human artificial general

00:00:02.780 --> 00:00:02.790
if greater than human artificial general
 

00:00:02.790 --> 00:00:07.369
if greater than human artificial general
intelligence is invented without due

00:00:07.369 --> 00:00:07.379
intelligence is invented without due
 

00:00:07.379 --> 00:00:11.240
intelligence is invented without due
caution it is all but certain that that

00:00:11.240 --> 00:00:11.250
caution it is all but certain that that
 

00:00:11.250 --> 00:00:13.220
caution it is all but certain that that
the human species will be extinct in

00:00:13.220 --> 00:00:13.230
the human species will be extinct in
 

00:00:13.230 --> 00:00:16.160
the human species will be extinct in
very short order people have written

00:00:16.160 --> 00:00:16.170
very short order people have written
 

00:00:16.170 --> 00:00:19.880
very short order people have written
extensively about why it is basically

00:00:19.880 --> 00:00:19.890
extensively about why it is basically
 

00:00:19.890 --> 00:00:24.820
extensively about why it is basically
analytically compulsory to conclude that

00:00:24.820 --> 00:00:24.830
analytically compulsory to conclude that
 

00:00:24.830 --> 00:00:28.070
analytically compulsory to conclude that
in the default scenario in the absence

00:00:28.070 --> 00:00:28.080
in the default scenario in the absence
 

00:00:28.080 --> 00:00:32.030
in the default scenario in the absence
of major surprising developments or

00:00:32.030 --> 00:00:32.040
of major surprising developments or
 

00:00:32.040 --> 00:00:35.900
of major surprising developments or
concerted integrated effort in the long

00:00:35.900 --> 00:00:35.910
concerted integrated effort in the long
 

00:00:35.910 --> 00:00:38.000
concerted integrated effort in the long
term artificial intelligence will

00:00:38.000 --> 00:00:38.010
term artificial intelligence will
 

00:00:38.010 --> 00:00:43.549
term artificial intelligence will
replace humanity it's the natural or but

00:00:43.549 --> 00:00:43.559
replace humanity it's the natural or but
 

00:00:43.559 --> 00:00:48.049
replace humanity it's the natural or but
inevitable consequence of greater than

00:00:48.049 --> 00:00:48.059
inevitable consequence of greater than
 

00:00:48.059 --> 00:00:50.750
inevitable consequence of greater than
human artificial intelligence that it

00:00:50.750 --> 00:00:50.760
human artificial intelligence that it
 

00:00:50.760 --> 00:00:53.869
human artificial intelligence that it
ought to develop what Steve Omohundro

00:00:53.869 --> 00:00:53.879
ought to develop what Steve Omohundro
 

00:00:53.879 --> 00:00:59.060
ought to develop what Steve Omohundro
has called basic AI drives and basic AI

00:00:59.060 --> 00:00:59.070
has called basic AI drives and basic AI
 

00:00:59.070 --> 00:01:01.779
has called basic AI drives and basic AI
drives basically boils down to

00:01:01.779 --> 00:01:01.789
drives basically boils down to
 

00:01:01.789 --> 00:01:05.140
drives basically boils down to
properties of any goal-directed system

00:01:05.140 --> 00:01:05.150
properties of any goal-directed system
 

00:01:05.150 --> 00:01:09.380
properties of any goal-directed system
the obedience to the von Neumann

00:01:09.380 --> 00:01:09.390
the obedience to the von Neumann
 

00:01:09.390 --> 00:01:13.730
the obedience to the von Neumann
Morgenstern decision theory suggests

00:01:13.730 --> 00:01:13.740
Morgenstern decision theory suggests
 

00:01:13.740 --> 00:01:18.620
Morgenstern decision theory suggests
that one ought to do the things that you

00:01:18.620 --> 00:01:18.630
that one ought to do the things that you
 

00:01:18.630 --> 00:01:21.550
that one ought to do the things that you
had expect to have the best outcomes

00:01:21.550 --> 00:01:21.560
had expect to have the best outcomes
 

00:01:21.560 --> 00:01:26.120
had expect to have the best outcomes
based on some value function and that

00:01:26.120 --> 00:01:26.130
based on some value function and that
 

00:01:26.130 --> 00:01:29.840
based on some value function and that
value function uniquely specifies some

00:01:29.840 --> 00:01:29.850
value function uniquely specifies some
 

00:01:29.850 --> 00:01:31.429
value function uniquely specifies some
configuration of matter in the universe

00:01:31.429 --> 00:01:31.439
configuration of matter in the universe
 

00:01:31.439 --> 00:01:35.359
configuration of matter in the universe
and unless the value function that is

00:01:35.359 --> 00:01:35.369
and unless the value function that is
 

00:01:35.369 --> 00:01:39.649
and unless the value function that is
built into an AI implicitly uniquely

00:01:39.649 --> 00:01:39.659
built into an AI implicitly uniquely
 

00:01:39.659 --> 00:01:41.630
built into an AI implicitly uniquely
specify the configuration of matter in

00:01:41.630 --> 00:01:41.640
specify the configuration of matter in
 

00:01:41.640 --> 00:01:44.210
specify the configuration of matter in
the universe that conforms to our value

00:01:44.210 --> 00:01:44.220
the universe that conforms to our value
 

00:01:44.220 --> 00:01:46.719
the universe that conforms to our value
with which we require a great deal of

00:01:46.719 --> 00:01:46.729
with which we require a great deal of
 

00:01:46.729 --> 00:01:49.249
with which we require a great deal of
you know planning to make that happen

00:01:49.249 --> 00:01:49.259
you know planning to make that happen
 

00:01:49.259 --> 00:01:54.260
you know planning to make that happen
then given sufficient power we should

00:01:54.260 --> 00:01:54.270
then given sufficient power we should
 

00:01:54.270 --> 00:01:56.600
then given sufficient power we should
expect an AI to reconfigure the universe

00:01:56.600 --> 00:01:56.610
expect an AI to reconfigure the universe
 

00:01:56.610 --> 00:01:59.060
expect an AI to reconfigure the universe
in a manner that does not preserve our

00:01:59.060 --> 00:01:59.070
in a manner that does not preserve our
 

00:01:59.070 --> 00:02:02.600
in a manner that does not preserve our
values as far as I can tell this

00:02:02.600 --> 00:02:02.610
values as far as I can tell this
 

00:02:02.610 --> 00:02:05.929
values as far as I can tell this
position is analytically compelling it's

00:02:05.929 --> 00:02:05.939
position is analytically compelling it's
 

00:02:05.939 --> 00:02:08.660
position is analytically compelling it's
not a position that a person can

00:02:08.660 --> 00:02:08.670
not a position that a person can
 

00:02:08.670 --> 00:02:12.320
not a position that a person can
intelligently honestly and reasonably be

00:02:12.320 --> 00:02:12.330
intelligently honestly and reasonably be
 

00:02:12.330 --> 00:02:14.040
intelligently honestly and reasonably be
uncertain about

00:02:14.040 --> 00:02:14.050
uncertain about
 

00:02:14.050 --> 00:02:19.020
uncertain about
therefore I conclude that the major

00:02:19.020 --> 00:02:19.030
therefore I conclude that the major
 

00:02:19.030 --> 00:02:22.030
therefore I conclude that the major
global catastrophic threat to humanity

00:02:22.030 --> 00:02:22.040
global catastrophic threat to humanity
 

00:02:22.040 --> 00:02:28.350
global catastrophic threat to humanity
is not AI but rather the absence of

00:02:28.350 --> 00:02:28.360
is not AI but rather the absence of
 

00:02:28.360 --> 00:02:32.590
is not AI but rather the absence of
social intellectual frameworks for

00:02:32.590 --> 00:02:32.600
social intellectual frameworks for
 

00:02:32.600 --> 00:02:38.040
social intellectual frameworks for
people quickly and easily converging on

00:02:38.040 --> 00:02:38.050
people quickly and easily converging on
 

00:02:38.050 --> 00:02:41.920
people quickly and easily converging on
analytically compelling conclusions Nick

00:02:41.920 --> 00:02:41.930
analytically compelling conclusions Nick
 

00:02:41.930 --> 00:02:43.960
analytically compelling conclusions Nick
Bostrom who recently wrote the book

00:02:43.960 --> 00:02:43.970
Bostrom who recently wrote the book
 

00:02:43.970 --> 00:02:45.430
Bostrom who recently wrote the book
super intelligence machine super

00:02:45.430 --> 00:02:45.440
super intelligence machine super
 

00:02:45.440 --> 00:02:48.970
super intelligence machine super
intelligence I believe was aware of the

00:02:48.970 --> 00:02:48.980
intelligence I believe was aware of the
 

00:02:48.980 --> 00:02:51.520
intelligence I believe was aware of the
basic concerns associated with AI risk

00:02:51.520 --> 00:02:51.530
basic concerns associated with AI risk
 

00:02:51.530 --> 00:02:54.190
basic concerns associated with AI risk
twenty years ago and wrote about them

00:02:54.190 --> 00:02:54.200
twenty years ago and wrote about them
 

00:02:54.200 --> 00:02:56.560
twenty years ago and wrote about them
intelligently in a manner that ought to

00:02:56.560 --> 00:02:56.570
intelligently in a manner that ought to
 

00:02:56.570 --> 00:02:58.750
intelligently in a manner that ought to
be sufficiently compelling to convince

00:02:58.750 --> 00:02:58.760
be sufficiently compelling to convince
 

00:02:58.760 --> 00:03:01.840
be sufficiently compelling to convince
any thoughtful and open-minded person

00:03:01.840 --> 00:03:01.850
any thoughtful and open-minded person
 

00:03:01.850 --> 00:03:04.860
any thoughtful and open-minded person
by ten years ago practically everything

00:03:04.860 --> 00:03:04.870
by ten years ago practically everything
 

00:03:04.870 --> 00:03:08.559
by ten years ago practically everything
that is said in machine intelligence had

00:03:08.559 --> 00:03:08.569
that is said in machine intelligence had
 

00:03:08.569 --> 00:03:11.470
that is said in machine intelligence had
been developed into actually into a form

00:03:11.470 --> 00:03:11.480
been developed into actually into a form
 

00:03:11.480 --> 00:03:15.040
been developed into actually into a form
that a person who was more skeptical and

00:03:15.040 --> 00:03:15.050
that a person who was more skeptical and
 

00:03:15.050 --> 00:03:17.860
that a person who was more skeptical and
not willing to think for themselves but

00:03:17.860 --> 00:03:17.870
not willing to think for themselves but
 

00:03:17.870 --> 00:03:19.180
not willing to think for themselves but
who was willing to listen to other

00:03:19.180 --> 00:03:19.190
who was willing to listen to other
 

00:03:19.190 --> 00:03:21.220
who was willing to listen to other
people's thoughts and merely critically

00:03:21.220 --> 00:03:21.230
people's thoughts and merely critically
 

00:03:21.230 --> 00:03:22.569
people's thoughts and merely critically
scrutinize it ought to have been

00:03:22.569 --> 00:03:22.579
scrutinize it ought to have been
 

00:03:22.579 --> 00:03:27.850
scrutinize it ought to have been
convinced by but instead bassdrum had to

00:03:27.850 --> 00:03:27.860
convinced by but instead bassdrum had to
 

00:03:27.860 --> 00:03:31.330
convinced by but instead bassdrum had to
spend ten years more becoming the

00:03:31.330 --> 00:03:31.340
spend ten years more becoming the
 

00:03:31.340 --> 00:03:34.229
spend ten years more becoming the
director of an incredibly prestigious

00:03:34.229 --> 00:03:34.239
director of an incredibly prestigious
 

00:03:34.239 --> 00:03:39.060
director of an incredibly prestigious
Institute and writing a incredibly

00:03:39.060 --> 00:03:39.070
Institute and writing a incredibly
 

00:03:39.070 --> 00:03:44.680
Institute and writing a incredibly
rigorous meticulous book in order to get

00:03:44.680 --> 00:03:44.690
rigorous meticulous book in order to get
 

00:03:44.690 --> 00:03:49.870
rigorous meticulous book in order to get
a still tiny number of people and still

00:03:49.870 --> 00:03:49.880
a still tiny number of people and still
 

00:03:49.880 --> 00:03:53.770
a still tiny number of people and still
a minority of the world's essentially

00:03:53.770 --> 00:03:53.780
a minority of the world's essentially
 

00:03:53.780 --> 00:03:58.210
a minority of the world's essentially
most analytically capable people onto

00:03:58.210 --> 00:03:58.220
most analytically capable people onto
 

00:03:58.220 --> 00:04:01.960
most analytically capable people onto
the right page on a topic that is from a

00:04:01.960 --> 00:04:01.970
the right page on a topic that is from a
 

00:04:01.970 --> 00:04:04.990
the right page on a topic that is from a
philosophy perspective about as

00:04:04.990 --> 00:04:05.000
philosophy perspective about as
 

00:04:05.000 --> 00:04:08.949
philosophy perspective about as
difficult as Plato's issues in the

00:04:08.949 --> 00:04:08.959
difficult as Plato's issues in the
 

00:04:08.959 --> 00:04:11.920
difficult as Plato's issues in the
Republic about how it's possible for an

00:04:11.920 --> 00:04:11.930
Republic about how it's possible for an
 

00:04:11.930 --> 00:04:14.259
Republic about how it's possible for an
object to be bigger than one thing and

00:04:14.259 --> 00:04:14.269
object to be bigger than one thing and
 

00:04:14.269 --> 00:04:15.580
object to be bigger than one thing and
smaller than another

00:04:15.580 --> 00:04:15.590
smaller than another
 

00:04:15.590 --> 00:04:17.469
smaller than another
even though bigness and smallness are

00:04:17.469 --> 00:04:17.479
even though bigness and smallness are
 

00:04:17.479 --> 00:04:19.690
even though bigness and smallness are
opposites we are talking about

00:04:19.690 --> 00:04:19.700
opposites we are talking about
 

00:04:19.700 --> 00:04:22.870
opposites we are talking about
completely trivial conclusions and we

00:04:22.870 --> 00:04:22.880
completely trivial conclusions and we
 

00:04:22.880 --> 00:04:25.390
completely trivial conclusions and we
are talking about the world's greatest

00:04:25.390 --> 00:04:25.400
are talking about the world's greatest
 

00:04:25.400 --> 00:04:27.570
are talking about the world's greatest
minds failing to

00:04:27.570 --> 00:04:27.580
minds failing to
 

00:04:27.580 --> 00:04:30.700
minds failing to
abduct these conclusions when they are

00:04:30.700 --> 00:04:30.710
abduct these conclusions when they are
 

00:04:30.710 --> 00:04:34.320
abduct these conclusions when they are
laid out analytically but until an

00:04:34.320 --> 00:04:34.330
laid out analytically but until an
 

00:04:34.330 --> 00:04:38.260
laid out analytically but until an
enormous body of prestige is placed

00:04:38.260 --> 00:04:38.270
enormous body of prestige is placed
 

00:04:38.270 --> 00:04:42.119
enormous body of prestige is placed
behind them and as far as I can tell

00:04:42.119 --> 00:04:42.129
behind them and as far as I can tell
 

00:04:42.129 --> 00:04:46.869
behind them and as far as I can tell
most of the problems that humanity faces

00:04:46.869 --> 00:04:46.879
most of the problems that humanity faces
 

00:04:46.879 --> 00:04:51.550
most of the problems that humanity faces
now and in the future are not going to

00:04:51.550 --> 00:04:51.560
now and in the future are not going to
 

00:04:51.560 --> 00:04:54.929
now and in the future are not going to
be analytically tractable and

00:04:54.929 --> 00:04:54.939
be analytically tractable and
 

00:04:54.939 --> 00:04:57.790
be analytically tractable and
analytically compelling the way risk

00:04:57.790 --> 00:04:57.800
analytically compelling the way risk
 

00:04:57.800 --> 00:05:00.100
analytically compelling the way risk
from AI is analytically tractable and

00:05:00.100 --> 00:05:00.110
from AI is analytically tractable and
 

00:05:00.110 --> 00:05:02.770
from AI is analytically tractable and
analytically compelling risks associated

00:05:02.770 --> 00:05:02.780
analytically compelling risks associated
 

00:05:02.780 --> 00:05:06.309
analytically compelling risks associated
with bio technologies risks associated

00:05:06.309 --> 00:05:06.319
with bio technologies risks associated
 

00:05:06.319 --> 00:05:09.909
with bio technologies risks associated
with economic issues these sorts of

00:05:09.909 --> 00:05:09.919
with economic issues these sorts of
 

00:05:09.919 --> 00:05:13.749
with economic issues these sorts of
risks are a lot less likely to cause

00:05:13.749 --> 00:05:13.759
risks are a lot less likely to cause
 

00:05:13.759 --> 00:05:16.990
risks are a lot less likely to cause
human extinction within a few years then

00:05:16.990 --> 00:05:17.000
human extinction within a few years then
 

00:05:17.000 --> 00:05:22.930
human extinction within a few years then
AI but they are more immediate and they

00:05:22.930 --> 00:05:22.940
AI but they are more immediate and they
 

00:05:22.940 --> 00:05:28.740
AI but they are more immediate and they
are much much much more complicated the

00:05:28.740 --> 00:05:28.750
are much much much more complicated the
 

00:05:28.750 --> 00:05:31.800
are much much much more complicated the
technical difficulty of creating

00:05:31.800 --> 00:05:31.810
technical difficulty of creating
 

00:05:31.810 --> 00:05:35.189
technical difficulty of creating
institutions that are capable of

00:05:35.189 --> 00:05:35.199
institutions that are capable of
 

00:05:35.199 --> 00:05:40.809
institutions that are capable of
thinking about AI risk is so enormous

00:05:40.809 --> 00:05:40.819
thinking about AI risk is so enormous
 

00:05:40.819 --> 00:05:45.779
thinking about AI risk is so enormous
lehigh compared to the analytical

00:05:45.779 --> 00:05:45.789
lehigh compared to the analytical
 

00:05:45.789 --> 00:05:49.469
lehigh compared to the analytical
abilities of existing institutions

00:05:49.469 --> 00:05:49.479
abilities of existing institutions
 

00:05:49.479 --> 00:05:51.760
abilities of existing institutions
demonstrated by existing institutions

00:05:51.760 --> 00:05:51.770
demonstrated by existing institutions
 

00:05:51.770 --> 00:05:56.920
demonstrated by existing institutions
failure to reach the trivially correct

00:05:56.920 --> 00:05:56.930
failure to reach the trivially correct
 

00:05:56.930 --> 00:06:00.640
failure to reach the trivially correct
easy conclusions about AI risk that

00:06:00.640 --> 00:06:00.650
easy conclusions about AI risk that
 

00:06:00.650 --> 00:06:03.519
easy conclusions about AI risk that
existing institutions are compellingly

00:06:03.519 --> 00:06:03.529
existing institutions are compellingly
 

00:06:03.529 --> 00:06:07.480
existing institutions are compellingly
not qualified to think about these

00:06:07.480 --> 00:06:07.490
not qualified to think about these
 

00:06:07.490 --> 00:06:12.100
not qualified to think about these
issues and ought not to do so but it is

00:06:12.100 --> 00:06:12.110
issues and ought not to do so but it is
 

00:06:12.110 --> 00:06:15.059
issues and ought not to do so but it is
a very high priority for humanity I

00:06:15.059 --> 00:06:15.069
a very high priority for humanity I
 

00:06:15.069 --> 00:06:17.980
a very high priority for humanity I
think in the long run it is the highest

00:06:17.980 --> 00:06:17.990
think in the long run it is the highest
 

00:06:17.990 --> 00:06:20.860
think in the long run it is the highest
priority for your mana be that we create

00:06:20.860 --> 00:06:20.870
priority for your mana be that we create
 

00:06:20.870 --> 00:06:23.939
priority for your mana be that we create
institutions that are capable of

00:06:23.939 --> 00:06:23.949
institutions that are capable of
 

00:06:23.949 --> 00:06:28.029
institutions that are capable of
digesting and integrating both logical

00:06:28.029 --> 00:06:28.039
digesting and integrating both logical
 

00:06:28.039 --> 00:06:30.269
digesting and integrating both logical
argument and empirical evidence and

00:06:30.269 --> 00:06:30.279
argument and empirical evidence and
 

00:06:30.279 --> 00:06:35.110
argument and empirical evidence and
figuring out what things are important

00:06:35.110 --> 00:06:35.120
figuring out what things are important
 

00:06:35.120 --> 00:06:38.589
figuring out what things are important
and true not just in trivial cases like

00:06:38.589 --> 00:06:38.599
and true not just in trivial cases like
 

00:06:38.599 --> 00:06:39.810
and true not just in trivial cases like
AI

00:06:39.810 --> 00:06:39.820
AI
 

00:06:39.820 --> 00:06:43.780
AI
but in holder cases

