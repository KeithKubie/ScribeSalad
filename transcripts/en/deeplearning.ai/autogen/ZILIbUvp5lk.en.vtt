WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:01.969
 
very very deep neural networks are

00:00:01.969 --> 00:00:01.979
very very deep neural networks are
 

00:00:01.979 --> 00:00:04.430
very very deep neural networks are
difficult to train because of vanishing

00:00:04.430 --> 00:00:04.440
difficult to train because of vanishing
 

00:00:04.440 --> 00:00:06.320
difficult to train because of vanishing
and exploding gradients types of

00:00:06.320 --> 00:00:06.330
and exploding gradients types of
 

00:00:06.330 --> 00:00:08.690
and exploding gradients types of
problems in this video you learn about

00:00:08.690 --> 00:00:08.700
problems in this video you learn about
 

00:00:08.700 --> 00:00:10.430
problems in this video you learn about
skip connections which allows you to

00:00:10.430 --> 00:00:10.440
skip connections which allows you to
 

00:00:10.440 --> 00:00:12.860
skip connections which allows you to
take the activation from one layer and

00:00:12.860 --> 00:00:12.870
take the activation from one layer and
 

00:00:12.870 --> 00:00:15.560
take the activation from one layer and
suddenly feed it to another layer even

00:00:15.560 --> 00:00:15.570
suddenly feed it to another layer even
 

00:00:15.570 --> 00:00:17.269
suddenly feed it to another layer even
much deeper in the neural network and

00:00:17.269 --> 00:00:17.279
much deeper in the neural network and
 

00:00:17.279 --> 00:00:19.460
much deeper in the neural network and
using that you really build business

00:00:19.460 --> 00:00:19.470
using that you really build business
 

00:00:19.470 --> 00:00:21.710
using that you really build business
which enables you to Train very very

00:00:21.710 --> 00:00:21.720
which enables you to Train very very
 

00:00:21.720 --> 00:00:23.810
which enables you to Train very very
deep in that service sometimes even net

00:00:23.810 --> 00:00:23.820
deep in that service sometimes even net
 

00:00:23.820 --> 00:00:25.880
deep in that service sometimes even net
worth of over a hundred layers let's

00:00:25.880 --> 00:00:25.890
worth of over a hundred layers let's
 

00:00:25.890 --> 00:00:26.570
worth of over a hundred layers let's
take a look

00:00:26.570 --> 00:00:26.580
take a look
 

00:00:26.580 --> 00:00:28.730
take a look
rest nets are built out of something

00:00:28.730 --> 00:00:28.740
rest nets are built out of something
 

00:00:28.740 --> 00:00:31.009
rest nets are built out of something
called a residual block let's first

00:00:31.009 --> 00:00:31.019
called a residual block let's first
 

00:00:31.019 --> 00:00:33.680
called a residual block let's first
describe what that is here are two

00:00:33.680 --> 00:00:33.690
describe what that is here are two
 

00:00:33.690 --> 00:00:35.360
describe what that is here are two
layers of a neural network where you

00:00:35.360 --> 00:00:35.370
layers of a neural network where you
 

00:00:35.370 --> 00:00:37.430
layers of a neural network where you
start off with some activation and there

00:00:37.430 --> 00:00:37.440
start off with some activation and there
 

00:00:37.440 --> 00:00:40.160
start off with some activation and there
L then you go to a L plus 1 and then the

00:00:40.160 --> 00:00:40.170
L then you go to a L plus 1 and then the
 

00:00:40.170 --> 00:00:43.280
L then you go to a L plus 1 and then the
activation two layers later is a L plus

00:00:43.280 --> 00:00:43.290
activation two layers later is a L plus
 

00:00:43.290 --> 00:00:45.860
activation two layers later is a L plus
two so to go through the steps in this

00:00:45.860 --> 00:00:45.870
two so to go through the steps in this
 

00:00:45.870 --> 00:00:49.760
two so to go through the steps in this
computation you have a L and then the

00:00:49.760 --> 00:00:49.770
computation you have a L and then the
 

00:00:49.770 --> 00:00:51.939
computation you have a L and then the
first thing you do is you apply this

00:00:51.939 --> 00:00:51.949
first thing you do is you apply this
 

00:00:51.949 --> 00:00:55.819
first thing you do is you apply this
linear operator to it which is governed

00:00:55.819 --> 00:00:55.829
linear operator to it which is governed
 

00:00:55.829 --> 00:00:57.260
linear operator to it which is governed
by this equation

00:00:57.260 --> 00:00:57.270
by this equation
 

00:00:57.270 --> 00:01:02.260
by this equation
to go from a L to compute Z L plus one

00:01:02.260 --> 00:01:02.270
to go from a L to compute Z L plus one
 

00:01:02.270 --> 00:01:04.880
to go from a L to compute Z L plus one
by multiplying by the weight matrix and

00:01:04.880 --> 00:01:04.890
by multiplying by the weight matrix and
 

00:01:04.890 --> 00:01:09.380
by multiplying by the weight matrix and
adding that bias vector after that you

00:01:09.380 --> 00:01:09.390
adding that bias vector after that you
 

00:01:09.390 --> 00:01:16.359
adding that bias vector after that you
apply the relu the linearity to get a o

00:01:16.359 --> 00:01:16.369
apply the relu the linearity to get a o
 

00:01:16.369 --> 00:01:19.789
apply the relu the linearity to get a o
plus one and that's governed by this

00:01:19.789 --> 00:01:19.799
plus one and that's governed by this
 

00:01:19.799 --> 00:01:23.390
plus one and that's governed by this
equation where a L plus one is G of ZL

00:01:23.390 --> 00:01:23.400
equation where a L plus one is G of ZL
 

00:01:23.400 --> 00:01:26.929
equation where a L plus one is G of ZL
plus 1 then in the next layer you apply

00:01:26.929 --> 00:01:26.939
plus 1 then in the next layer you apply
 

00:01:26.939 --> 00:01:31.910
plus 1 then in the next layer you apply
this linear step again so it's governed

00:01:31.910 --> 00:01:31.920
this linear step again so it's governed
 

00:01:31.920 --> 00:01:34.190
this linear step again so it's governed
by that equation right so this is quite

00:01:34.190 --> 00:01:34.200
by that equation right so this is quite
 

00:01:34.200 --> 00:01:35.960
by that equation right so this is quite
similar to this equation we saw on the

00:01:35.960 --> 00:01:35.970
similar to this equation we saw on the
 

00:01:35.970 --> 00:01:40.840
similar to this equation we saw on the
left and then finally you apply another

00:01:40.840 --> 00:01:40.850
left and then finally you apply another
 

00:01:40.850 --> 00:01:45.190
left and then finally you apply another
relu operation which is now governed by

00:01:45.190 --> 00:01:45.200
relu operation which is now governed by
 

00:01:45.200 --> 00:01:48.649
relu operation which is now governed by
that equation where g here would be the

00:01:48.649 --> 00:01:48.659
that equation where g here would be the
 

00:01:48.659 --> 00:01:53.300
that equation where g here would be the
relu non-linearity and this gives you

00:01:53.300 --> 00:01:53.310
relu non-linearity and this gives you
 

00:01:53.310 --> 00:01:57.770
relu non-linearity and this gives you
you know a L plus 2 so in other words

00:01:57.770 --> 00:01:57.780
you know a L plus 2 so in other words
 

00:01:57.780 --> 00:02:03.080
you know a L plus 2 so in other words
for information from al to flow to al

00:02:03.080 --> 00:02:03.090
for information from al to flow to al
 

00:02:03.090 --> 00:02:05.569
for information from al to flow to al
plus 2 it needs to go through all of

00:02:05.569 --> 00:02:05.579
plus 2 it needs to go through all of
 

00:02:05.579 --> 00:02:08.350
plus 2 it needs to go through all of
these steps which I'm going to call the

00:02:08.350 --> 00:02:08.360
these steps which I'm going to call the
 

00:02:08.360 --> 00:02:09.680
these steps which I'm going to call the
main

00:02:09.680 --> 00:02:09.690
main
 

00:02:09.690 --> 00:02:14.270
main
of this set of layers in a position that

00:02:14.270 --> 00:02:14.280
of this set of layers in a position that
 

00:02:14.280 --> 00:02:16.700
of this set of layers in a position that
we're going to make a change to this

00:02:16.700 --> 00:02:16.710
we're going to make a change to this
 

00:02:16.710 --> 00:02:20.330
we're going to make a change to this
we're gonna take Al and just fast

00:02:20.330 --> 00:02:20.340
we're gonna take Al and just fast
 

00:02:20.340 --> 00:02:24.560
we're gonna take Al and just fast
forward it copy it much further into the

00:02:24.560 --> 00:02:24.570
forward it copy it much further into the
 

00:02:24.570 --> 00:02:29.950
forward it copy it much further into the
neural network to here and just add al

00:02:29.950 --> 00:02:29.960
neural network to here and just add al
 

00:02:29.960 --> 00:02:32.660
neural network to here and just add al
before applying the non-linearity they

00:02:32.660 --> 00:02:32.670
before applying the non-linearity they
 

00:02:32.670 --> 00:02:35.270
before applying the non-linearity they
really non-linearity and I'm gonna call

00:02:35.270 --> 00:02:35.280
really non-linearity and I'm gonna call
 

00:02:35.280 --> 00:02:38.750
really non-linearity and I'm gonna call
this the shortcut so rather than needing

00:02:38.750 --> 00:02:38.760
this the shortcut so rather than needing
 

00:02:38.760 --> 00:02:41.810
this the shortcut so rather than needing
to follow the main path the information

00:02:41.810 --> 00:02:41.820
to follow the main path the information
 

00:02:41.820 --> 00:02:44.780
to follow the main path the information
from Al can now follow a shortcut to go

00:02:44.780 --> 00:02:44.790
from Al can now follow a shortcut to go
 

00:02:44.790 --> 00:02:46.670
from Al can now follow a shortcut to go
much deeper into the neural network and

00:02:46.670 --> 00:02:46.680
much deeper into the neural network and
 

00:02:46.680 --> 00:02:48.830
much deeper into the neural network and
what that means is that this last

00:02:48.830 --> 00:02:48.840
what that means is that this last
 

00:02:48.840 --> 00:02:51.590
what that means is that this last
equation goes away and we instead have

00:02:51.590 --> 00:02:51.600
equation goes away and we instead have
 

00:02:51.600 --> 00:02:55.510
equation goes away and we instead have
that the output al plus 2 is the rather

00:02:55.510 --> 00:02:55.520
that the output al plus 2 is the rather
 

00:02:55.520 --> 00:03:00.110
that the output al plus 2 is the rather
non-linearity G applied to ZL plus 2 as

00:03:00.110 --> 00:03:00.120
non-linearity G applied to ZL plus 2 as
 

00:03:00.120 --> 00:03:03.590
non-linearity G applied to ZL plus 2 as
before but now plus al so the addition

00:03:03.590 --> 00:03:03.600
before but now plus al so the addition
 

00:03:03.600 --> 00:03:07.310
before but now plus al so the addition
of this al here it makes this a residual

00:03:07.310 --> 00:03:07.320
of this al here it makes this a residual
 

00:03:07.320 --> 00:03:09.710
of this al here it makes this a residual
block and in pictures you can also

00:03:09.710 --> 00:03:09.720
block and in pictures you can also
 

00:03:09.720 --> 00:03:12.350
block and in pictures you can also
modify this picture on top by drawing

00:03:12.350 --> 00:03:12.360
modify this picture on top by drawing
 

00:03:12.360 --> 00:03:16.010
modify this picture on top by drawing
this if your shortcut to go here I'm

00:03:16.010 --> 00:03:16.020
this if your shortcut to go here I'm
 

00:03:16.020 --> 00:03:19.100
this if your shortcut to go here I'm
going to draw it as going into this

00:03:19.100 --> 00:03:19.110
going to draw it as going into this
 

00:03:19.110 --> 00:03:22.520
going to draw it as going into this
second layer here because the shortcut

00:03:22.520 --> 00:03:22.530
second layer here because the shortcut
 

00:03:22.530 --> 00:03:25.070
second layer here because the shortcut
is actually added before the relative

00:03:25.070 --> 00:03:25.080
is actually added before the relative
 

00:03:25.080 --> 00:03:26.960
is actually added before the relative
non-linearity so each of these nodes

00:03:26.960 --> 00:03:26.970
non-linearity so each of these nodes
 

00:03:26.970 --> 00:03:28.699
non-linearity so each of these nodes
here where that applies a linear

00:03:28.699 --> 00:03:28.709
here where that applies a linear
 

00:03:28.709 --> 00:03:31.580
here where that applies a linear
function and a value so al was being

00:03:31.580 --> 00:03:31.590
function and a value so al was being
 

00:03:31.590 --> 00:03:32.330
function and a value so al was being
injected

00:03:32.330 --> 00:03:32.340
injected
 

00:03:32.340 --> 00:03:34.280
injected
after the linear part but before the

00:03:34.280 --> 00:03:34.290
after the linear part but before the
 

00:03:34.290 --> 00:03:36.560
after the linear part but before the
rather part and sometimes instead of the

00:03:36.560 --> 00:03:36.570
rather part and sometimes instead of the
 

00:03:36.570 --> 00:03:38.810
rather part and sometimes instead of the
terms shortcut you also hear the term

00:03:38.810 --> 00:03:38.820
terms shortcut you also hear the term
 

00:03:38.820 --> 00:03:41.690
terms shortcut you also hear the term
skip connection and that refers to Al

00:03:41.690 --> 00:03:41.700
skip connection and that refers to Al
 

00:03:41.700 --> 00:03:43.970
skip connection and that refers to Al
just skipping over a layer or kind of

00:03:43.970 --> 00:03:43.980
just skipping over a layer or kind of
 

00:03:43.980 --> 00:03:46.520
just skipping over a layer or kind of
skipping over almost two layers in order

00:03:46.520 --> 00:03:46.530
skipping over almost two layers in order
 

00:03:46.530 --> 00:03:49.670
skipping over almost two layers in order
to pass this information deeper into the

00:03:49.670 --> 00:03:49.680
to pass this information deeper into the
 

00:03:49.680 --> 00:03:53.030
to pass this information deeper into the
neural network so what the inventors of

00:03:53.030 --> 00:03:53.040
neural network so what the inventors of
 

00:03:53.040 --> 00:03:55.400
neural network so what the inventors of
ResNet so that would be climbing her

00:03:55.400 --> 00:03:55.410
ResNet so that would be climbing her
 

00:03:55.410 --> 00:03:59.180
ResNet so that would be climbing her
Sally Jiang shouting Ren and Jenson what

00:03:59.180 --> 00:03:59.190
Sally Jiang shouting Ren and Jenson what
 

00:03:59.190 --> 00:04:01.520
Sally Jiang shouting Ren and Jenson what
they found was that using residual

00:04:01.520 --> 00:04:01.530
they found was that using residual
 

00:04:01.530 --> 00:04:01.970
they found was that using residual
blocks

00:04:01.970 --> 00:04:01.980
blocks
 

00:04:01.980 --> 00:04:04.760
blocks
allows you to train much deeper neural

00:04:04.760 --> 00:04:04.770
allows you to train much deeper neural
 

00:04:04.770 --> 00:04:07.220
allows you to train much deeper neural
networks and the way you build a

00:04:07.220 --> 00:04:07.230
networks and the way you build a
 

00:04:07.230 --> 00:04:09.260
networks and the way you build a
resident is by taking many of these

00:04:09.260 --> 00:04:09.270
resident is by taking many of these
 

00:04:09.270 --> 00:04:11.930
resident is by taking many of these
residual blocks blocks like these and

00:04:11.930 --> 00:04:11.940
residual blocks blocks like these and
 

00:04:11.940 --> 00:04:14.479
residual blocks blocks like these and
stacking them together to form a deep

00:04:14.479 --> 00:04:14.489
stacking them together to form a deep
 

00:04:14.489 --> 00:04:17.870
stacking them together to form a deep
network so let's look at this network

00:04:17.870 --> 00:04:17.880
network so let's look at this network
 

00:04:17.880 --> 00:04:20.659
network so let's look at this network
this is not a residual network um this

00:04:20.659 --> 00:04:20.669
this is not a residual network um this
 

00:04:20.669 --> 00:04:21.770
this is not a residual network um this
is called as a

00:04:21.770 --> 00:04:21.780
is called as a
 

00:04:21.780 --> 00:04:23.450
is called as a
playing that's work this is a

00:04:23.450 --> 00:04:23.460
playing that's work this is a
 

00:04:23.460 --> 00:04:27.260
playing that's work this is a
terminology of the resonant paper to

00:04:27.260 --> 00:04:27.270
terminology of the resonant paper to
 

00:04:27.270 --> 00:04:29.510
terminology of the resonant paper to
turn this into residence what you do is

00:04:29.510 --> 00:04:29.520
turn this into residence what you do is
 

00:04:29.520 --> 00:04:32.300
turn this into residence what you do is
you add all those skip connections

00:04:32.300 --> 00:04:32.310
you add all those skip connections
 

00:04:32.310 --> 00:04:34.390
you add all those skip connections
although those short circuit connections

00:04:34.390 --> 00:04:34.400
although those short circuit connections
 

00:04:34.400 --> 00:04:39.610
although those short circuit connections
like so so every two layers ends up with

00:04:39.610 --> 00:04:39.620
like so so every two layers ends up with
 

00:04:39.620 --> 00:04:44.210
like so so every two layers ends up with
that additional change that we saw on

00:04:44.210 --> 00:04:44.220
that additional change that we saw on
 

00:04:44.220 --> 00:04:46.670
that additional change that we saw on
their previous slide to turn each of

00:04:46.670 --> 00:04:46.680
their previous slide to turn each of
 

00:04:46.680 --> 00:04:50.060
their previous slide to turn each of
these into a residual block so this

00:04:50.060 --> 00:04:50.070
these into a residual block so this
 

00:04:50.070 --> 00:04:52.879
these into a residual block so this
picture shows five residual blocks stack

00:04:52.879 --> 00:04:52.889
picture shows five residual blocks stack
 

00:04:52.889 --> 00:04:56.150
picture shows five residual blocks stack
together and this is a residual Network

00:04:56.150 --> 00:04:56.160
together and this is a residual Network
 

00:04:56.160 --> 00:04:59.840
together and this is a residual Network
and it turns out that if you use you

00:04:59.840 --> 00:04:59.850
and it turns out that if you use you
 

00:04:59.850 --> 00:05:01.760
and it turns out that if you use you
know a standard optimization algorithm

00:05:01.760 --> 00:05:01.770
know a standard optimization algorithm
 

00:05:01.770 --> 00:05:04.250
know a standard optimization algorithm
such as gradient descents or one of the

00:05:04.250 --> 00:05:04.260
such as gradient descents or one of the
 

00:05:04.260 --> 00:05:06.350
such as gradient descents or one of the
fancier optimization algorithms to train

00:05:06.350 --> 00:05:06.360
fancier optimization algorithms to train
 

00:05:06.360 --> 00:05:08.950
fancier optimization algorithms to train
a plane network so without all the extra

00:05:08.950 --> 00:05:08.960
a plane network so without all the extra
 

00:05:08.960 --> 00:05:12.440
a plane network so without all the extra
residual without all the extra shortcuts

00:05:12.440 --> 00:05:12.450
residual without all the extra shortcuts
 

00:05:12.450 --> 00:05:13.940
residual without all the extra shortcuts
or skipped connections I just drew in

00:05:13.940 --> 00:05:13.950
or skipped connections I just drew in
 

00:05:13.950 --> 00:05:17.330
or skipped connections I just drew in
and perkily you find that as you

00:05:17.330 --> 00:05:17.340
and perkily you find that as you
 

00:05:17.340 --> 00:05:19.100
and perkily you find that as you
increase the number of layers the

00:05:19.100 --> 00:05:19.110
increase the number of layers the
 

00:05:19.110 --> 00:05:20.600
increase the number of layers the
training error will tend to decrease

00:05:20.600 --> 00:05:20.610
training error will tend to decrease
 

00:05:20.610 --> 00:05:22.670
training error will tend to decrease
after a while but then they'll tend to

00:05:22.670 --> 00:05:22.680
after a while but then they'll tend to
 

00:05:22.680 --> 00:05:27.140
after a while but then they'll tend to
go back up and in theory as you make a

00:05:27.140 --> 00:05:27.150
go back up and in theory as you make a
 

00:05:27.150 --> 00:05:29.870
go back up and in theory as you make a
new network deeper you know it should

00:05:29.870 --> 00:05:29.880
new network deeper you know it should
 

00:05:29.880 --> 00:05:32.000
new network deeper you know it should
only do better and better on the

00:05:32.000 --> 00:05:32.010
only do better and better on the
 

00:05:32.010 --> 00:05:34.430
only do better and better on the
training set right so the theory in

00:05:34.430 --> 00:05:34.440
training set right so the theory in
 

00:05:34.440 --> 00:05:36.710
training set right so the theory in
theory having a deeper network should

00:05:36.710 --> 00:05:36.720
theory having a deeper network should
 

00:05:36.720 --> 00:05:37.640
theory having a deeper network should
only hope

00:05:37.640 --> 00:05:37.650
only hope
 

00:05:37.650 --> 00:05:41.240
only hope
but in practice on reality having a play

00:05:41.240 --> 00:05:41.250
but in practice on reality having a play
 

00:05:41.250 --> 00:05:43.250
but in practice on reality having a play
network so now the ResNet but having

00:05:43.250 --> 00:05:43.260
network so now the ResNet but having
 

00:05:43.260 --> 00:05:45.820
network so now the ResNet but having
play network this very deep means that

00:05:45.820 --> 00:05:45.830
play network this very deep means that
 

00:05:45.830 --> 00:05:48.560
play network this very deep means that
your optimization algorithm just as much

00:05:48.560 --> 00:05:48.570
your optimization algorithm just as much
 

00:05:48.570 --> 00:05:50.719
your optimization algorithm just as much
harder time in training and so in

00:05:50.719 --> 00:05:50.729
harder time in training and so in
 

00:05:50.729 --> 00:05:53.270
harder time in training and so in
reality your training error gets worse

00:05:53.270 --> 00:05:53.280
reality your training error gets worse
 

00:05:53.280 --> 00:05:55.460
reality your training error gets worse
if you pick a network that's too deep

00:05:55.460 --> 00:05:55.470
if you pick a network that's too deep
 

00:05:55.470 --> 00:05:58.490
if you pick a network that's too deep
but what happens with res Nets is that

00:05:58.490 --> 00:05:58.500
but what happens with res Nets is that
 

00:05:58.500 --> 00:06:00.860
but what happens with res Nets is that
even as a number of layers gets deeper

00:06:00.860 --> 00:06:00.870
even as a number of layers gets deeper
 

00:06:00.870 --> 00:06:03.200
even as a number of layers gets deeper
you can have the performance of the

00:06:03.200 --> 00:06:03.210
you can have the performance of the
 

00:06:03.210 --> 00:06:06.469
you can have the performance of the
trading ever to keep on going down you

00:06:06.469 --> 00:06:06.479
trading ever to keep on going down you
 

00:06:06.479 --> 00:06:08.450
trading ever to keep on going down you
know even retrain a network with over a

00:06:08.450 --> 00:06:08.460
know even retrain a network with over a
 

00:06:08.460 --> 00:06:10.990
know even retrain a network with over a
hundred layers and then now some people

00:06:10.990 --> 00:06:11.000
hundred layers and then now some people
 

00:06:11.000 --> 00:06:12.830
hundred layers and then now some people
experimenting with networks there are

00:06:12.830 --> 00:06:12.840
experimenting with networks there are
 

00:06:12.840 --> 00:06:14.390
experimenting with networks there are
over a thousand layers

00:06:14.390 --> 00:06:14.400
over a thousand layers
 

00:06:14.400 --> 00:06:16.550
over a thousand layers
although I don't see that use much in

00:06:16.550 --> 00:06:16.560
although I don't see that use much in
 

00:06:16.560 --> 00:06:19.010
although I don't see that use much in
practice yet but by taking these

00:06:19.010 --> 00:06:19.020
practice yet but by taking these
 

00:06:19.020 --> 00:06:21.290
practice yet but by taking these
activations being XOR these intermediate

00:06:21.290 --> 00:06:21.300
activations being XOR these intermediate
 

00:06:21.300 --> 00:06:24.290
activations being XOR these intermediate
activations and allowing it to go much

00:06:24.290 --> 00:06:24.300
activations and allowing it to go much
 

00:06:24.300 --> 00:06:26.480
activations and allowing it to go much
deeper than your network this really

00:06:26.480 --> 00:06:26.490
deeper than your network this really
 

00:06:26.490 --> 00:06:29.150
deeper than your network this really
hosts with the vanishing and exploding

00:06:29.150 --> 00:06:29.160
hosts with the vanishing and exploding
 

00:06:29.160 --> 00:06:31.430
hosts with the vanishing and exploding
gradient problems and allows you to

00:06:31.430 --> 00:06:31.440
gradient problems and allows you to
 

00:06:31.440 --> 00:06:33.290
gradient problems and allows you to
Train much deeper neural networks

00:06:33.290 --> 00:06:33.300
Train much deeper neural networks
 

00:06:33.300 --> 00:06:35.600
Train much deeper neural networks
without really appreciable loss in

00:06:35.600 --> 00:06:35.610
without really appreciable loss in
 

00:06:35.610 --> 00:06:37.700
without really appreciable loss in
performance and maybe at some point this

00:06:37.700 --> 00:06:37.710
performance and maybe at some point this
 

00:06:37.710 --> 00:06:39.590
performance and maybe at some point this
will Plateau this will flatten out and

00:06:39.590 --> 00:06:39.600
will Plateau this will flatten out and
 

00:06:39.600 --> 00:06:41.840
will Plateau this will flatten out and
doesn't help that much the deeper and

00:06:41.840 --> 00:06:41.850
doesn't help that much the deeper and
 

00:06:41.850 --> 00:06:44.749
doesn't help that much the deeper and
deeper networks but residents are so

00:06:44.749 --> 00:06:44.759
deeper networks but residents are so
 

00:06:44.759 --> 00:06:47.779
deeper networks but residents are so
even effective that helping train very

00:06:47.779 --> 00:06:47.789
even effective that helping train very
 

00:06:47.789 --> 00:06:50.570
even effective that helping train very
deep networks so you've now gotten an

00:06:50.570 --> 00:06:50.580
deep networks so you've now gotten an
 

00:06:50.580 --> 00:06:52.969
deep networks so you've now gotten an
overview of how resonance work and in

00:06:52.969 --> 00:06:52.979
overview of how resonance work and in
 

00:06:52.979 --> 00:06:55.730
overview of how resonance work and in
fact in this week's program exercise you

00:06:55.730 --> 00:06:55.740
fact in this week's program exercise you
 

00:06:55.740 --> 00:06:57.920
fact in this week's program exercise you
get to implement these ideas and see it

00:06:57.920 --> 00:06:57.930
get to implement these ideas and see it
 

00:06:57.930 --> 00:06:58.879
get to implement these ideas and see it
work for yourself

00:06:58.879 --> 00:06:58.889
work for yourself
 

00:06:58.889 --> 00:07:01.310
work for yourself
but next I want to share of you better

00:07:01.310 --> 00:07:01.320
but next I want to share of you better
 

00:07:01.320 --> 00:07:03.680
but next I want to share of you better
intuition or even more intuition about

00:07:03.680 --> 00:07:03.690
intuition or even more intuition about
 

00:07:03.690 --> 00:07:06.770
intuition or even more intuition about
why resinous work so well let's go onto

00:07:06.770 --> 00:07:06.780
why resinous work so well let's go onto
 

00:07:06.780 --> 00:07:09.530
why resinous work so well let's go onto
the next video

