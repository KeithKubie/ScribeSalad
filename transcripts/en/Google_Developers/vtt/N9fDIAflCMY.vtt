WEBVTT
Kind: captions
Language: en

00:00:06.765 --> 00:00:08.140
JOSH GORDON:
Classifiers are only

00:00:08.140 --> 00:00:10.270
as good as the
features you provide.

00:00:10.270 --> 00:00:12.060
That means coming up
with good features

00:00:12.060 --> 00:00:14.740
is one of your most important
jobs in machine learning.

00:00:14.740 --> 00:00:17.060
But what makes a good
feature, and how can you tell?

00:00:17.060 --> 00:00:19.400
If you're doing
binary classification,

00:00:19.400 --> 00:00:21.670
then a good feature
makes it easy to decide

00:00:21.670 --> 00:00:23.270
between two different things.

00:00:23.270 --> 00:00:26.100
For example, imagine we
wanted to write a classifier

00:00:26.100 --> 00:00:29.090
to tell the difference
between two types of dogs--

00:00:29.090 --> 00:00:30.890
greyhounds and Labradors.

00:00:30.890 --> 00:00:34.090
Here we'll use two features--
the dog's height in inches

00:00:34.090 --> 00:00:35.490
and their eye color.

00:00:35.490 --> 00:00:38.490
Just for this toy example,
let's make a couple assumptions

00:00:38.490 --> 00:00:40.930
about dogs to keep
things simple.

00:00:40.930 --> 00:00:43.050
First, we'll say that
greyhounds are usually

00:00:43.050 --> 00:00:44.180
taller than Labradors.

00:00:44.180 --> 00:00:47.020
Next, we'll pretend that
dogs have only two eye

00:00:47.020 --> 00:00:48.750
colors-- blue and brown.

00:00:48.750 --> 00:00:50.760
And we'll say the
color of their eyes

00:00:50.760 --> 00:00:53.160
doesn't depend on
the breed of dog.

00:00:53.160 --> 00:00:55.520
This means that one of
these features is useful

00:00:55.520 --> 00:00:57.480
and the other tells us nothing.

00:00:57.480 --> 00:01:01.260
To understand why, we'll
visualize them using a toy

00:01:01.260 --> 00:01:02.970
dataset I'll create.

00:01:02.970 --> 00:01:04.300
Let's begin with height.

00:01:04.300 --> 00:01:06.650
How useful do you
think this feature is?

00:01:06.650 --> 00:01:08.070
Well, on average,
greyhounds tend

00:01:08.070 --> 00:01:11.310
to be a couple inches taller
than Labradors, but not always.

00:01:11.310 --> 00:01:13.736
There's a lot of
variation in the world.

00:01:13.736 --> 00:01:15.110
So when we think
of a feature, we

00:01:15.110 --> 00:01:17.620
have to consider how it
looks for different values

00:01:17.620 --> 00:01:19.630
in a population.

00:01:19.630 --> 00:01:22.360
Let's head into Python for
a programmatic example.

00:01:22.360 --> 00:01:24.440
I'm creating a
population of 1,000

00:01:24.440 --> 00:01:27.737
dogs-- 50-50 greyhound Labrador.

00:01:27.737 --> 00:01:29.070
I'll give each of them a height.

00:01:29.070 --> 00:01:31.500
For this example, we'll
say that greyhounds

00:01:31.500 --> 00:01:35.510
are on average 28 inches
tall and Labradors are 24.

00:01:35.510 --> 00:01:37.564
Now, all dogs are
a bit different.

00:01:37.564 --> 00:01:39.480
Let's say that height
is normally distributed,

00:01:39.480 --> 00:01:42.790
so we'll make both of these
plus or minus 4 inches.

00:01:42.790 --> 00:01:44.660
This will give us two
arrays of numbers,

00:01:44.660 --> 00:01:47.200
and we can visualize
them in a histogram.

00:01:47.200 --> 00:01:49.520
I'll add a parameter so
greyhounds are in red

00:01:49.520 --> 00:01:51.320
and Labradors are in blue.

00:01:51.320 --> 00:01:53.320
Now we can run our script.

00:01:53.320 --> 00:01:57.460
This shows how many dogs in our
population have a given height.

00:01:57.460 --> 00:01:58.960
There's a lot of
data on the screen,

00:01:58.960 --> 00:02:03.202
so let's simplify it and
look at it piece by piece.

00:02:03.202 --> 00:02:05.230
We'll start with
dogs on the far left

00:02:05.230 --> 00:02:08.600
of the distribution-- say,
who are about 20 inches tall.

00:02:08.600 --> 00:02:11.380
Imagine I asked you to predict
whether a dog with his height

00:02:11.380 --> 00:02:13.300
was a lab or a greyhound.

00:02:13.300 --> 00:02:14.180
What would you do?

00:02:14.180 --> 00:02:16.710
Well, you could figure out
the probability of each type

00:02:16.710 --> 00:02:18.670
of dog given their height.

00:02:18.670 --> 00:02:20.940
Here, it's more likely
the dog is a lab.

00:02:20.940 --> 00:02:22.967
On the other hand,
if we go all the way

00:02:22.967 --> 00:02:24.550
to the right of the
histogram and look

00:02:24.550 --> 00:02:26.950
at a dog who is
35 inches tall, we

00:02:26.950 --> 00:02:29.450
can be pretty confident
they're a greyhound.

00:02:29.450 --> 00:02:31.300
Now, what about a
dog in the middle?

00:02:31.300 --> 00:02:33.520
You can see the graph
gives us less information

00:02:33.520 --> 00:02:36.750
here, because the probability
of each type of dog is close.

00:02:36.750 --> 00:02:40.220
So height is a useful
feature, but it's not perfect.

00:02:40.220 --> 00:02:42.280
That's why in machine
learning, you almost always

00:02:42.280 --> 00:02:43.482
need multiple features.

00:02:43.482 --> 00:02:45.440
Otherwise, you could just
write an if statement

00:02:45.440 --> 00:02:47.160
instead of bothering
with the classifier.

00:02:47.160 --> 00:02:50.590
To figure out what types
of features you should use,

00:02:50.590 --> 00:02:52.390
do a thought experiment.

00:02:52.390 --> 00:02:53.820
Pretend you're the classifier.

00:02:53.820 --> 00:02:55.870
If you were trying to
figure out if this dog is

00:02:55.870 --> 00:03:00.167
a lab or a greyhound, what other
things would you want to know?

00:03:00.167 --> 00:03:01.750
You might ask about
their hair length,

00:03:01.750 --> 00:03:04.680
or how fast they can run,
or how much they weigh.

00:03:04.680 --> 00:03:06.980
Exactly how many
features you should use

00:03:06.980 --> 00:03:08.550
is more of an art
than a science,

00:03:08.550 --> 00:03:10.720
but as a rule of thumb,
think about how many you'd

00:03:10.720 --> 00:03:12.620
need to solve the problem.

00:03:12.620 --> 00:03:15.590
Now let's look at another
feature like eye color.

00:03:15.590 --> 00:03:17.470
Just for this toy
example, let's imagine

00:03:17.470 --> 00:03:20.500
dogs have only two eye
colors, blue and brown.

00:03:20.500 --> 00:03:22.100
And let's say the
color of their eyes

00:03:22.100 --> 00:03:24.500
doesn't depend on
the breed of dog.

00:03:24.500 --> 00:03:28.590
Here's what a histogram might
look like for this example.

00:03:28.590 --> 00:03:32.170
For most values, the
distribution is about 50/50.

00:03:32.170 --> 00:03:33.850
So this feature
tells us nothing,

00:03:33.850 --> 00:03:36.110
because it doesn't correlate
with the type of dog.

00:03:36.110 --> 00:03:39.200
Including a useless feature
like this in your training

00:03:39.200 --> 00:03:41.940
data can hurt your
classifier's accuracy.

00:03:41.940 --> 00:03:45.210
That's because there's a chance
they might appear useful purely

00:03:45.210 --> 00:03:48.430
by accident, especially if
you have only a small amount

00:03:48.430 --> 00:03:50.040
of training data.

00:03:50.040 --> 00:03:52.320
You also want your
features to be independent.

00:03:52.320 --> 00:03:54.600
And independent
features give you

00:03:54.600 --> 00:03:56.870
different types of information.

00:03:56.870 --> 00:03:59.360
Imagine we already have a
feature-- height and inches--

00:03:59.360 --> 00:04:00.800
in our dataset.

00:04:00.800 --> 00:04:02.250
Ask yourself,
would it be helpful

00:04:02.250 --> 00:04:05.800
if we added another feature,
like height in centimeters?

00:04:05.800 --> 00:04:08.230
No, because it's perfectly
correlated with one

00:04:08.230 --> 00:04:09.410
we already have.

00:04:09.410 --> 00:04:12.650
It's good practice to remove
highly correlated features

00:04:12.650 --> 00:04:14.032
from your training data.

00:04:14.032 --> 00:04:15.490
That's because a
lot of classifiers

00:04:15.490 --> 00:04:18.190
aren't smart enough to
realize that height in inches

00:04:18.190 --> 00:04:20.200
in centimeters are
the same thing,

00:04:20.200 --> 00:04:23.340
so they might double count
how important this feature is.

00:04:23.340 --> 00:04:26.600
Last, you want your features
to be easy to understand.

00:04:26.600 --> 00:04:28.730
For a new example,
imagine you want

00:04:28.730 --> 00:04:30.330
to predict how many
days it will take

00:04:30.330 --> 00:04:33.580
to mail a letter between
two different cities.

00:04:33.580 --> 00:04:37.130
The farther apart the cities
are, the longer it will take.

00:04:37.130 --> 00:04:39.650
A great feature to use
would be the distance

00:04:39.650 --> 00:04:42.200
between the cities in miles.

00:04:42.200 --> 00:04:44.220
A much worse pair
of features to use

00:04:44.220 --> 00:04:47.160
would be the city's locations
given by their latitude

00:04:47.160 --> 00:04:48.260
and longitude.

00:04:48.260 --> 00:04:48.970
And here's why.

00:04:48.970 --> 00:04:51.120
I can look at the
distance and make

00:04:51.120 --> 00:04:54.100
a good guess of how long it
will take the letter to arrive.

00:04:54.100 --> 00:04:56.880
But learning the relationship
between latitude, longitude,

00:04:56.880 --> 00:05:00.020
and time is much harder
and would require many more

00:05:00.020 --> 00:05:01.986
examples in your training data.

00:05:01.986 --> 00:05:03.360
Now, there are
techniques you can

00:05:03.360 --> 00:05:05.970
use to figure out exactly
how useful your features are,

00:05:05.970 --> 00:05:08.920
and even what combinations
of them are best,

00:05:08.920 --> 00:05:11.390
so you never have to
leave it to chance.

00:05:11.390 --> 00:05:13.770
We'll get to those
in a future episode.

00:05:13.770 --> 00:05:16.230
Coming up next time, we'll
continue building our intuition

00:05:16.230 --> 00:05:17.750
for supervised learning.

00:05:17.750 --> 00:05:19.680
We'll show how different
types of classifiers

00:05:19.680 --> 00:05:22.290
can be used to solve the same
problem and dive a little bit

00:05:22.290 --> 00:05:24.240
deeper into how they work.

00:05:24.240 --> 00:05:27.220
Thanks very much for watching,
and I'll see you then.

