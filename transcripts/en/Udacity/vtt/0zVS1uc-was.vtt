WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.430
The next background technology that I'm telling

00:00:02.430 --> 00:00:06.680
about is software RAID. I mentioned that hardware

00:00:06.680 --> 00:00:09.790
RAID has two problems. The first problem being

00:00:09.790 --> 00:00:12.100
small writes and we said we can get

00:00:12.100 --> 00:00:14.420
rid of the small write problem by using

00:00:14.420 --> 00:00:17.080
log structure file systems. But the hardware RAID

00:00:17.080 --> 00:00:19.410
also has another problem and that is it

00:00:19.410 --> 00:00:25.100
is employing multiple hardware drives. And hardware RAID,

00:00:25.100 --> 00:00:29.390
generally speaking, is very expensive proposition. On the

00:00:29.390 --> 00:00:31.760
other hand, in a local area network, we

00:00:31.760 --> 00:00:34.090
have lots of compute power distributed over the

00:00:34.090 --> 00:00:37.910
LAN and every node on the LAN has

00:00:37.910 --> 00:00:41.240
associated with it are disks. So could we

00:00:41.240 --> 00:00:44.130
not use the disks that are available on

00:00:44.130 --> 00:00:47.670
local data network for doing exactly the same

00:00:47.670 --> 00:00:50.240
thing that we did with hardware RAID? And that

00:00:50.240 --> 00:00:56.690
is, stripe a file across the disks of all the nodes that are in the

00:00:56.690 --> 00:01:02.850
local area network. So that's the idea behind the Zebra file system that was

00:01:02.850 --> 00:01:07.840
built at UC Berkeley, which was the first one to experiment with this software

00:01:07.840 --> 00:01:11.640
RAID technology. It combines both lock structure

00:01:11.640 --> 00:01:15.280
file system and the RAID technology, lock-structured file

00:01:15.280 --> 00:01:20.130
system in order to get rid of the small write problem, and the RAID technology

00:01:20.130 --> 00:01:27.085
to get the parallelism you want in a file server to be able to read from the

00:01:27.085 --> 00:01:30.090
disks in parallel, so that you can reduce

00:01:30.090 --> 00:01:33.780
the latency for serving client requests. The idea

00:01:33.780 --> 00:01:35.940
here is that you're going to use commodity

00:01:35.940 --> 00:01:40.480
hardware available as nodes connected to disks on

00:01:40.480 --> 00:01:43.958
a local area network. And since LFS, lock

00:01:43.958 --> 00:01:47.400
structured file system, is good for getting rid of

00:01:47.400 --> 00:01:49.880
the small write problem, we are going to employ

00:01:49.880 --> 00:01:54.150
LFS as the technology for the file server. So

00:01:54.150 --> 00:01:57.070
in this case, what we have are not data

00:01:57.070 --> 00:02:00.820
files but we have log segments that have to

00:02:00.820 --> 00:02:05.500
be written out to the disk in an LFS. And what we're going to do is we're going

00:02:05.500 --> 00:02:08.750
to stripe the log segment on multiple nodes

00:02:08.750 --> 00:02:11.220
of the disks in software and that is the

00:02:11.220 --> 00:02:13.820
RAID technology. So if this is a log segment

00:02:13.820 --> 00:02:18.250
that represents the changes made to several different files

00:02:18.250 --> 00:02:21.720
on a particular client node, then the software

00:02:21.720 --> 00:02:24.640
RAID, what it will do, is then take this

00:02:24.640 --> 00:02:27.740
log segment and stripe it. Part one of the

00:02:27.740 --> 00:02:30.650
log segment on this node, part two on this,

00:02:30.650 --> 00:02:35.730
part three on this, part four on this and the ECC that corresponds to these four

00:02:35.730 --> 00:02:41.040
parts of the log segments into a fifth drive. That's the idea

00:02:41.040 --> 00:02:43.380
in software RAID. Exactly similar to the

00:02:43.380 --> 00:02:46.560
hardware RAID except software is doing this

00:02:46.560 --> 00:02:52.570
striping of the log segment on multiple nodes available in a local area network.

