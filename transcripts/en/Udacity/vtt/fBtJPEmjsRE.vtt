WEBVTT
Kind: captions
Language: en

00:00:00.025 --> 00:00:02.980
So, or hm, that's a joke.

00:00:02.980 --> 00:00:03.969
Did you get that joke, Megan?

00:00:03.969 --> 00:00:06.310
You got the joke.

00:00:06.310 --> 00:00:09.300
She's quiet today,
which is an unusual thing for Meg.

00:00:09.300 --> 00:00:13.185
Here's how it works,
if we have the HMM, so

00:00:13.185 --> 00:00:16.135
if we know the HMM,
we know a and b and pi.

00:00:17.435 --> 00:00:20.455
We can use the forward algorithm and
the backward algorithm, and

00:00:20.455 --> 00:00:22.745
this is, part I'm not going to show you.

00:00:22.745 --> 00:00:25.315
To estimate, that's what's cap E.

00:00:25.315 --> 00:00:29.966
The distribution of what state I'm
likely to be in a time t, okay?

00:00:29.966 --> 00:00:31.686
because one has to do with
the sequence is forwards,

00:00:31.686 --> 00:00:33.295
one has to do with
the sequence is backward.

00:00:33.295 --> 00:00:35.950
I can evaluate the probability of all,
of the whole sequence.

00:00:35.950 --> 00:00:39.968
That allows me to estimate,
what's typically called gamma i of t,

00:00:39.968 --> 00:00:44.406
the probability that I'm in state one
at time t, state two, state three,

00:00:44.406 --> 00:00:45.126
all right?

00:00:45.126 --> 00:00:46.940
I estimate that distribution.

00:00:46.940 --> 00:00:52.360
Well, if you give me that distribution,
and I've actually seen

00:00:52.360 --> 00:00:57.640
at every time t what was observed, then
given the likelihood that I'm in each of

00:00:57.640 --> 00:01:00.950
the states, probability that I'm in each
of the states, and the observed data,

00:01:00.950 --> 00:01:05.410
I can figure out what
the most likely b sub j of k.

00:01:05.410 --> 00:01:08.000
That is, what do I think
the emission probabilities are?

00:01:08.000 --> 00:01:10.629
And I picked the ones that
would maximize the total

00:01:10.629 --> 00:01:12.480
observation probability.

00:01:12.480 --> 00:01:16.560
It's no different than saying maximum
likelihood, you know, if, if I, if I'm

00:01:16.560 --> 00:01:20.360
drawing something, and I pull, you know,
seven, I pull ten things out, and seven

00:01:20.360 --> 00:01:23.810
of them are black, and three are red, I
say there's a 70% chance of being black.

00:01:23.810 --> 00:01:27.270
That value is the one that
would've maximized the likelihood

00:01:27.270 --> 00:01:28.730
of pulling out seven of them.

00:01:28.730 --> 00:01:30.430
It's a maximization procedure.

00:01:30.430 --> 00:01:35.280
So I can choose the b sub k's that
maximize the total observation,

00:01:35.280 --> 00:01:39.940
given those probabil,
the idea which state I'm in, okay?

00:01:39.940 --> 00:01:44.820
Also, given that distribution,
I can also say well, if I think there's

00:01:44.820 --> 00:01:49.580
a 75% chance that I'm in
state one at this time, and

00:01:49.580 --> 00:01:54.650
a 90% chance that I'm in state two at
the next time, that tells me that well,

00:01:54.650 --> 00:01:59.280
you know, the likelihood of going from
one to two must be kind of high, right?

00:01:59.280 --> 00:02:02.693
And basically by looking over all of
time and looking at my guess about

00:02:02.693 --> 00:02:07.070
the states, I can come up with the best
possible transition probabilities.

00:02:07.070 --> 00:02:11.640
And when I say best, is when, again,
the ones that maximize the probability.

00:02:11.640 --> 00:02:15.910
That gives me my new a i j's,
and I've got new b k's.

00:02:15.910 --> 00:02:18.970
I can also guess new pi's,
but those never matter.

00:02:18.970 --> 00:02:21.190
That's my new machine, right?

00:02:21.190 --> 00:02:24.630
So after I do the end step, called the
maximization step, of getting the new

00:02:24.630 --> 00:02:29.180
A's and B's, I now have a new machine,
I can go back and do the E step again.

00:02:29.180 --> 00:02:32.810
I can estimate the probabilities of
being at a particular state of time,

00:02:32.810 --> 00:02:34.830
T, and I redo that process.

00:02:34.830 --> 00:02:38.083
And that iterative process,
it's called expectation maximization.

00:02:38.083 --> 00:02:42.580
The particular algorithm for HMMs
was called the Baum-Welch algorithm.

00:02:42.580 --> 00:02:46.830
Like I said, there's plenty of tutorials
out there for you to look at it.

00:02:46.830 --> 00:02:50.430
The fundamental idea is that by using
those recursive algorithms, and

00:02:50.430 --> 00:02:53.550
then estimating the probability of being
at a particular state at a particular

00:02:53.550 --> 00:02:55.990
time, I can re-estimate the machine.

