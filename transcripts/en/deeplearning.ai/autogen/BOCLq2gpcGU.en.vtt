WEBVTT
Kind: captions
Language: en

00:00:00.179 --> 00:00:02.480
 
in addition to l2 regularization and

00:00:02.480 --> 00:00:02.490
in addition to l2 regularization and
 

00:00:02.490 --> 00:00:04.550
in addition to l2 regularization and
drop our regularization their few other

00:00:04.550 --> 00:00:04.560
drop our regularization their few other
 

00:00:04.560 --> 00:00:06.740
drop our regularization their few other
techniques for reducing over sitting in

00:00:06.740 --> 00:00:06.750
techniques for reducing over sitting in
 

00:00:06.750 --> 00:00:08.509
techniques for reducing over sitting in
your neural network let's take a look

00:00:08.509 --> 00:00:08.519
your neural network let's take a look
 

00:00:08.519 --> 00:00:10.160
your neural network let's take a look
let's say you're fitting a CAD

00:00:10.160 --> 00:00:10.170
let's say you're fitting a CAD
 

00:00:10.170 --> 00:00:12.440
let's say you're fitting a CAD
classifier if you are overfitting

00:00:12.440 --> 00:00:12.450
classifier if you are overfitting
 

00:00:12.450 --> 00:00:14.870
classifier if you are overfitting
getting more training data can help but

00:00:14.870 --> 00:00:14.880
getting more training data can help but
 

00:00:14.880 --> 00:00:16.730
getting more training data can help but
getting more training data can be

00:00:16.730 --> 00:00:16.740
getting more training data can be
 

00:00:16.740 --> 00:00:19.279
getting more training data can be
expensive and sometimes just can't get

00:00:19.279 --> 00:00:19.289
expensive and sometimes just can't get
 

00:00:19.289 --> 00:00:22.250
expensive and sometimes just can't get
more data but what you can do is augment

00:00:22.250 --> 00:00:22.260
more data but what you can do is augment
 

00:00:22.260 --> 00:00:23.779
more data but what you can do is augment
your training set by taking an image

00:00:23.779 --> 00:00:23.789
your training set by taking an image
 

00:00:23.789 --> 00:00:26.359
your training set by taking an image
like this and for example flipping

00:00:26.359 --> 00:00:26.369
like this and for example flipping
 

00:00:26.369 --> 00:00:28.550
like this and for example flipping
horizontally and adding that also to

00:00:28.550 --> 00:00:28.560
horizontally and adding that also to
 

00:00:28.560 --> 00:00:31.099
horizontally and adding that also to
training set so now instead of just this

00:00:31.099 --> 00:00:31.109
training set so now instead of just this
 

00:00:31.109 --> 00:00:32.840
training set so now instead of just this
one example when your training set you

00:00:32.840 --> 00:00:32.850
one example when your training set you
 

00:00:32.850 --> 00:00:35.360
one example when your training set you
can add this to your training example so

00:00:35.360 --> 00:00:35.370
can add this to your training example so
 

00:00:35.370 --> 00:00:38.090
can add this to your training example so
by flipping your images horizontally you

00:00:38.090 --> 00:00:38.100
by flipping your images horizontally you
 

00:00:38.100 --> 00:00:39.530
by flipping your images horizontally you
could you know really double the size

00:00:39.530 --> 00:00:39.540
could you know really double the size
 

00:00:39.540 --> 00:00:41.330
could you know really double the size
your training set because your training

00:00:41.330 --> 00:00:41.340
your training set because your training
 

00:00:41.340 --> 00:00:43.549
your training set because your training
set is now a bit redundant this isn't as

00:00:43.549 --> 00:00:43.559
set is now a bit redundant this isn't as
 

00:00:43.559 --> 00:00:45.170
set is now a bit redundant this isn't as
good as if you had collected an

00:00:45.170 --> 00:00:45.180
good as if you had collected an
 

00:00:45.180 --> 00:00:48.290
good as if you had collected an
additional M set of brand new

00:00:48.290 --> 00:00:48.300
additional M set of brand new
 

00:00:48.300 --> 00:00:50.810
additional M set of brand new
independent examples but you could do

00:00:50.810 --> 00:00:50.820
independent examples but you could do
 

00:00:50.820 --> 00:00:53.630
independent examples but you could do
this without needing to pay the expense

00:00:53.630 --> 00:00:53.640
this without needing to pay the expense
 

00:00:53.640 --> 00:00:55.310
this without needing to pay the expense
of going out to take me click more

00:00:55.310 --> 00:00:55.320
of going out to take me click more
 

00:00:55.320 --> 00:00:57.830
of going out to take me click more
pictures of cats and then other than

00:00:57.830 --> 00:00:57.840
pictures of cats and then other than
 

00:00:57.840 --> 00:01:00.650
pictures of cats and then other than
surfing horizontally you can also take

00:01:00.650 --> 00:01:00.660
surfing horizontally you can also take
 

00:01:00.660 --> 00:01:02.869
surfing horizontally you can also take
random props of the image so here we've

00:01:02.869 --> 00:01:02.879
random props of the image so here we've
 

00:01:02.879 --> 00:01:05.630
random props of the image so here we've
rotated and so a randomly zoom into the

00:01:05.630 --> 00:01:05.640
rotated and so a randomly zoom into the
 

00:01:05.640 --> 00:01:07.609
rotated and so a randomly zoom into the
invention this pool looks like a tag but

00:01:07.609 --> 00:01:07.619
invention this pool looks like a tag but
 

00:01:07.619 --> 00:01:09.859
invention this pool looks like a tag but
so by taking random distortions and

00:01:09.859 --> 00:01:09.869
so by taking random distortions and
 

00:01:09.869 --> 00:01:11.330
so by taking random distortions and
transformations in the image you can

00:01:11.330 --> 00:01:11.340
transformations in the image you can
 

00:01:11.340 --> 00:01:13.310
transformations in the image you can
augment your data set and make

00:01:13.310 --> 00:01:13.320
augment your data set and make
 

00:01:13.320 --> 00:01:16.609
augment your data set and make
additional fake training examples again

00:01:16.609 --> 00:01:16.619
additional fake training examples again
 

00:01:16.619 --> 00:01:18.770
additional fake training examples again
these extra fake training examples they

00:01:18.770 --> 00:01:18.780
these extra fake training examples they
 

00:01:18.780 --> 00:01:20.660
these extra fake training examples they
don't add as much information as you

00:01:20.660 --> 00:01:20.670
don't add as much information as you
 

00:01:20.670 --> 00:01:23.020
don't add as much information as you
were to go on and get a brand new

00:01:23.020 --> 00:01:23.030
were to go on and get a brand new
 

00:01:23.030 --> 00:01:26.390
were to go on and get a brand new
independent example of a cat but because

00:01:26.390 --> 00:01:26.400
independent example of a cat but because
 

00:01:26.400 --> 00:01:28.070
independent example of a cat but because
you could do this you know almost a free

00:01:28.070 --> 00:01:28.080
you could do this you know almost a free
 

00:01:28.080 --> 00:01:29.690
you could do this you know almost a free
other than for some computational cost

00:01:29.690 --> 00:01:29.700
other than for some computational cost
 

00:01:29.700 --> 00:01:33.310
other than for some computational cost
or this can be an inexpensive way to

00:01:33.310 --> 00:01:33.320
or this can be an inexpensive way to
 

00:01:33.320 --> 00:01:36.830
or this can be an inexpensive way to
give your data this can be an

00:01:36.830 --> 00:01:36.840
give your data this can be an
 

00:01:36.840 --> 00:01:38.390
give your data this can be an
inexpensive way to give your algorithm

00:01:38.390 --> 00:01:38.400
inexpensive way to give your algorithm
 

00:01:38.400 --> 00:01:40.069
inexpensive way to give your algorithm
more data and therefore no sort of

00:01:40.069 --> 00:01:40.079
more data and therefore no sort of
 

00:01:40.079 --> 00:01:42.050
more data and therefore no sort of
regular eyes it and reduce all the 15

00:01:42.050 --> 00:01:42.060
regular eyes it and reduce all the 15
 

00:01:42.060 --> 00:01:44.810
regular eyes it and reduce all the 15
and by synthesizing examples like this

00:01:44.810 --> 00:01:44.820
and by synthesizing examples like this
 

00:01:44.820 --> 00:01:45.830
and by synthesizing examples like this
what you're really telling your

00:01:45.830 --> 00:01:45.840
what you're really telling your
 

00:01:45.840 --> 00:01:47.569
what you're really telling your
algorithm is that is something that's a

00:01:47.569 --> 00:01:47.579
algorithm is that is something that's a
 

00:01:47.579 --> 00:01:50.359
algorithm is that is something that's a
cat then slipping on horizontally is

00:01:50.359 --> 00:01:50.369
cat then slipping on horizontally is
 

00:01:50.369 --> 00:01:52.429
cat then slipping on horizontally is
still account notice eyes inserted

00:01:52.429 --> 00:01:52.439
still account notice eyes inserted
 

00:01:52.439 --> 00:01:54.109
still account notice eyes inserted
vertically because maybe we don't want

00:01:54.109 --> 00:01:54.119
vertically because maybe we don't want
 

00:01:54.119 --> 00:01:56.209
vertically because maybe we don't want
upside-down cats right and then also

00:01:56.209 --> 00:01:56.219
upside-down cats right and then also
 

00:01:56.219 --> 00:01:58.190
upside-down cats right and then also
maybe randomly zooming and are the

00:01:58.190 --> 00:01:58.200
maybe randomly zooming and are the
 

00:01:58.200 --> 00:02:00.319
maybe randomly zooming and are the
inventions pretty slow account for

00:02:00.319 --> 00:02:00.329
inventions pretty slow account for
 

00:02:00.329 --> 00:02:02.090
inventions pretty slow account for
optical character recognition you can

00:02:02.090 --> 00:02:02.100
optical character recognition you can
 

00:02:02.100 --> 00:02:04.130
optical character recognition you can
also open your data set by take a digit

00:02:04.130 --> 00:02:04.140
also open your data set by take a digit
 

00:02:04.140 --> 00:02:07.310
also open your data set by take a digit
and imposing random rotations and

00:02:07.310 --> 00:02:07.320
and imposing random rotations and
 

00:02:07.320 --> 00:02:08.320
and imposing random rotations and
distortions to

00:02:08.320 --> 00:02:08.330
distortions to
 

00:02:08.330 --> 00:02:10.240
distortions to
so if you add these things to your

00:02:10.240 --> 00:02:10.250
so if you add these things to your
 

00:02:10.250 --> 00:02:12.040
so if you add these things to your
training set you know these are also

00:02:12.040 --> 00:02:12.050
training set you know these are also
 

00:02:12.050 --> 00:02:16.180
training set you know these are also
still digit fours for illustration I

00:02:16.180 --> 00:02:16.190
still digit fours for illustration I
 

00:02:16.190 --> 00:02:18.610
still digit fours for illustration I
applied a very strong distortion so this

00:02:18.610 --> 00:02:18.620
applied a very strong distortion so this
 

00:02:18.620 --> 00:02:21.190
applied a very strong distortion so this
looks very way before in practice you

00:02:21.190 --> 00:02:21.200
looks very way before in practice you
 

00:02:21.200 --> 00:02:23.200
looks very way before in practice you
don't need to distort the for quite as

00:02:23.200 --> 00:02:23.210
don't need to distort the for quite as
 

00:02:23.210 --> 00:02:24.640
don't need to distort the for quite as
aggressively but just a more subtle

00:02:24.640 --> 00:02:24.650
aggressively but just a more subtle
 

00:02:24.650 --> 00:02:27.190
aggressively but just a more subtle
distortion than what I'm showing here to

00:02:27.190 --> 00:02:27.200
distortion than what I'm showing here to
 

00:02:27.200 --> 00:02:29.110
distortion than what I'm showing here to
make this example clearer for you right

00:02:29.110 --> 00:02:29.120
make this example clearer for you right
 

00:02:29.120 --> 00:02:30.670
make this example clearer for you right
but the most subtle distortion is

00:02:30.670 --> 00:02:30.680
but the most subtle distortion is
 

00:02:30.680 --> 00:02:33.220
but the most subtle distortion is
usually used in practice because this

00:02:33.220 --> 00:02:33.230
usually used in practice because this
 

00:02:33.230 --> 00:02:35.830
usually used in practice because this
looks like really warp divorce so data

00:02:35.830 --> 00:02:35.840
looks like really warp divorce so data
 

00:02:35.840 --> 00:02:38.830
looks like really warp divorce so data
augmentation can be used as

00:02:38.830 --> 00:02:38.840
augmentation can be used as
 

00:02:38.840 --> 00:02:40.870
augmentation can be used as
regularization techniques and effects

00:02:40.870 --> 00:02:40.880
regularization techniques and effects
 

00:02:40.880 --> 00:02:43.300
regularization techniques and effects
similar to regularization there's one

00:02:43.300 --> 00:02:43.310
similar to regularization there's one
 

00:02:43.310 --> 00:02:44.650
similar to regularization there's one
other technique that is often used

00:02:44.650 --> 00:02:44.660
other technique that is often used
 

00:02:44.660 --> 00:02:47.380
other technique that is often used
called early stopping so what you're

00:02:47.380 --> 00:02:47.390
called early stopping so what you're
 

00:02:47.390 --> 00:02:48.880
called early stopping so what you're
going to do is as you run gradient

00:02:48.880 --> 00:02:48.890
going to do is as you run gradient
 

00:02:48.890 --> 00:02:52.930
going to do is as you run gradient
descent you're going to plot your either

00:02:52.930 --> 00:02:52.940
descent you're going to plot your either
 

00:02:52.940 --> 00:02:55.260
descent you're going to plot your either
training error or your zero one

00:02:55.260 --> 00:02:55.270
training error or your zero one
 

00:02:55.270 --> 00:02:57.130
training error or your zero one
classification error on the training set

00:02:57.130 --> 00:02:57.140
classification error on the training set
 

00:02:57.140 --> 00:02:59.530
classification error on the training set
or just plot the cost function J

00:02:59.530 --> 00:02:59.540
or just plot the cost function J
 

00:02:59.540 --> 00:03:01.510
or just plot the cost function J
optimizing and that should decrease

00:03:01.510 --> 00:03:01.520
optimizing and that should decrease
 

00:03:01.520 --> 00:03:04.210
optimizing and that should decrease
monotonically like so all right because

00:03:04.210 --> 00:03:04.220
monotonically like so all right because
 

00:03:04.220 --> 00:03:05.860
monotonically like so all right because
as you train hopefully you're trading

00:03:05.860 --> 00:03:05.870
as you train hopefully you're trading
 

00:03:05.870 --> 00:03:07.990
as you train hopefully you're trading
around your cost function J chikki

00:03:07.990 --> 00:03:08.000
around your cost function J chikki
 

00:03:08.000 --> 00:03:10.420
around your cost function J chikki
please so what's early stopping what you

00:03:10.420 --> 00:03:10.430
please so what's early stopping what you
 

00:03:10.430 --> 00:03:12.640
please so what's early stopping what you
do is you plot this and you'll also plot

00:03:12.640 --> 00:03:12.650
do is you plot this and you'll also plot
 

00:03:12.650 --> 00:03:17.650
do is you plot this and you'll also plot
your def set error and again this could

00:03:17.650 --> 00:03:17.660
your def set error and again this could
 

00:03:17.660 --> 00:03:19.180
your def set error and again this could
be a classification error and

00:03:19.180 --> 00:03:19.190
be a classification error and
 

00:03:19.190 --> 00:03:21.040
be a classification error and
development variable something like the

00:03:21.040 --> 00:03:21.050
development variable something like the
 

00:03:21.050 --> 00:03:23.440
development variable something like the
cost function like the logistic loss of

00:03:23.440 --> 00:03:23.450
cost function like the logistic loss of
 

00:03:23.450 --> 00:03:25.449
cost function like the logistic loss of
the log loss of evaluation or

00:03:25.449 --> 00:03:25.459
the log loss of evaluation or
 

00:03:25.459 --> 00:03:27.220
the log loss of evaluation or
death's-head now once you find is that

00:03:27.220 --> 00:03:27.230
death's-head now once you find is that
 

00:03:27.230 --> 00:03:29.199
death's-head now once you find is that
your death set error will usually go

00:03:29.199 --> 00:03:29.209
your death set error will usually go
 

00:03:29.209 --> 00:03:31.120
your death set error will usually go
down for a while and then it will

00:03:31.120 --> 00:03:31.130
down for a while and then it will
 

00:03:31.130 --> 00:03:32.440
down for a while and then it will
increase from there

00:03:32.440 --> 00:03:32.450
increase from there
 

00:03:32.450 --> 00:03:35.650
increase from there
so what early stopping does is you say

00:03:35.650 --> 00:03:35.660
so what early stopping does is you say
 

00:03:35.660 --> 00:03:38.170
so what early stopping does is you say
well it looks like your new network is

00:03:38.170 --> 00:03:38.180
well it looks like your new network is
 

00:03:38.180 --> 00:03:40.120
well it looks like your new network is
doing best around that elevation so

00:03:40.120 --> 00:03:40.130
doing best around that elevation so
 

00:03:40.130 --> 00:03:41.620
doing best around that elevation so
we're just going to stop training on

00:03:41.620 --> 00:03:41.630
we're just going to stop training on
 

00:03:41.630 --> 00:03:43.750
we're just going to stop training on
your network halfway and you'll take one

00:03:43.750 --> 00:03:43.760
your network halfway and you'll take one
 

00:03:43.760 --> 00:03:46.150
your network halfway and you'll take one
of the value achieved this dead set

00:03:46.150 --> 00:03:46.160
of the value achieved this dead set
 

00:03:46.160 --> 00:03:49.270
of the value achieved this dead set
error so why does this work well when

00:03:49.270 --> 00:03:49.280
error so why does this work well when
 

00:03:49.280 --> 00:03:51.550
error so why does this work well when
you haven't run many iterations for your

00:03:51.550 --> 00:03:51.560
you haven't run many iterations for your
 

00:03:51.560 --> 00:03:53.770
you haven't run many iterations for your
neural network yet your parameters W

00:03:53.770 --> 00:03:53.780
neural network yet your parameters W
 

00:03:53.780 --> 00:03:55.780
neural network yet your parameters W
will be close to zero because you know

00:03:55.780 --> 00:03:55.790
will be close to zero because you know
 

00:03:55.790 --> 00:03:57.910
will be close to zero because you know
with random initialization you probably

00:03:57.910 --> 00:03:57.920
with random initialization you probably
 

00:03:57.920 --> 00:04:00.250
with random initialization you probably
initialize W to small random values so

00:04:00.250 --> 00:04:00.260
initialize W to small random values so
 

00:04:00.260 --> 00:04:02.710
initialize W to small random values so
before you train for a long time W is

00:04:02.710 --> 00:04:02.720
before you train for a long time W is
 

00:04:02.720 --> 00:04:04.449
before you train for a long time W is
still quite small and that's the

00:04:04.449 --> 00:04:04.459
still quite small and that's the
 

00:04:04.459 --> 00:04:06.880
still quite small and that's the
integrate as you train W get bigger and

00:04:06.880 --> 00:04:06.890
integrate as you train W get bigger and
 

00:04:06.890 --> 00:04:09.130
integrate as you train W get bigger and
bigger and bigger and so here maybe you

00:04:09.130 --> 00:04:09.140
bigger and bigger and so here maybe you
 

00:04:09.140 --> 00:04:11.290
bigger and bigger and so here maybe you
have a much larger value of the

00:04:11.290 --> 00:04:11.300
have a much larger value of the
 

00:04:11.300 --> 00:04:14.199
have a much larger value of the
parameters W for your neural network so

00:04:14.199 --> 00:04:14.209
parameters W for your neural network so
 

00:04:14.209 --> 00:04:16.450
parameters W for your neural network so
what early stopping does is by stopping

00:04:16.450 --> 00:04:16.460
what early stopping does is by stopping
 

00:04:16.460 --> 00:04:19.479
what early stopping does is by stopping
halfway you have only a you know mid

00:04:19.479 --> 00:04:19.489
halfway you have only a you know mid
 

00:04:19.489 --> 00:04:21.830
halfway you have only a you know mid
size right

00:04:21.830 --> 00:04:21.840
size right
 

00:04:21.840 --> 00:04:26.780
size right
w I'm so similar to l2 regularization by

00:04:26.780 --> 00:04:26.790
w I'm so similar to l2 regularization by
 

00:04:26.790 --> 00:04:29.000
w I'm so similar to l2 regularization by
picking a new network was smaller norm

00:04:29.000 --> 00:04:29.010
picking a new network was smaller norm
 

00:04:29.010 --> 00:04:31.850
picking a new network was smaller norm
for your parameters W hopefully your new

00:04:31.850 --> 00:04:31.860
for your parameters W hopefully your new
 

00:04:31.860 --> 00:04:34.909
for your parameters W hopefully your new
network is overfitting less and the term

00:04:34.909 --> 00:04:34.919
network is overfitting less and the term
 

00:04:34.919 --> 00:04:36.740
network is overfitting less and the term
early stopping refers to the fact that

00:04:36.740 --> 00:04:36.750
early stopping refers to the fact that
 

00:04:36.750 --> 00:04:38.330
early stopping refers to the fact that
you're just stopping the training of

00:04:38.330 --> 00:04:38.340
you're just stopping the training of
 

00:04:38.340 --> 00:04:41.360
you're just stopping the training of
your new network early I sometimes use

00:04:41.360 --> 00:04:41.370
your new network early I sometimes use
 

00:04:41.370 --> 00:04:43.010
your new network early I sometimes use
early stopping when training on your

00:04:43.010 --> 00:04:43.020
early stopping when training on your
 

00:04:43.020 --> 00:04:44.990
early stopping when training on your
network but it does have one downside

00:04:44.990 --> 00:04:45.000
network but it does have one downside
 

00:04:45.000 --> 00:04:46.370
network but it does have one downside
let me explain

00:04:46.370 --> 00:04:46.380
let me explain
 

00:04:46.380 --> 00:04:48.290
let me explain
I think the machine learning process as

00:04:48.290 --> 00:04:48.300
I think the machine learning process as
 

00:04:48.300 --> 00:04:50.870
I think the machine learning process as
comprising several different steps one

00:04:50.870 --> 00:04:50.880
comprising several different steps one
 

00:04:50.880 --> 00:04:53.210
comprising several different steps one
is that you want an algorithm so

00:04:53.210 --> 00:04:53.220
is that you want an algorithm so
 

00:04:53.220 --> 00:04:56.030
is that you want an algorithm so
optimize the cost function J and we have

00:04:56.030 --> 00:04:56.040
optimize the cost function J and we have
 

00:04:56.040 --> 00:04:57.860
optimize the cost function J and we have
various tools to do that you know such

00:04:57.860 --> 00:04:57.870
various tools to do that you know such
 

00:04:57.870 --> 00:05:00.800
various tools to do that you know such
as gradient descent and then we'll talk

00:05:00.800 --> 00:05:00.810
as gradient descent and then we'll talk
 

00:05:00.810 --> 00:05:02.690
as gradient descent and then we'll talk
later about other algorithms like

00:05:02.690 --> 00:05:02.700
later about other algorithms like
 

00:05:02.700 --> 00:05:06.320
later about other algorithms like
momentum and algorithm and rmsprop and

00:05:06.320 --> 00:05:06.330
momentum and algorithm and rmsprop and
 

00:05:06.330 --> 00:05:08.840
momentum and algorithm and rmsprop and
atom and so on but then after optimizing

00:05:08.840 --> 00:05:08.850
atom and so on but then after optimizing
 

00:05:08.850 --> 00:05:12.140
atom and so on but then after optimizing
the cost function J as you also wanted

00:05:12.140 --> 00:05:12.150
the cost function J as you also wanted
 

00:05:12.150 --> 00:05:15.770
the cost function J as you also wanted
to not over fit and we have some tools

00:05:15.770 --> 00:05:15.780
to not over fit and we have some tools
 

00:05:15.780 --> 00:05:18.100
to not over fit and we have some tools
to do that such as your regularization

00:05:18.100 --> 00:05:18.110
to do that such as your regularization
 

00:05:18.110 --> 00:05:22.370
to do that such as your regularization
getting more data and so on now in

00:05:22.370 --> 00:05:22.380
getting more data and so on now in
 

00:05:22.380 --> 00:05:24.140
getting more data and so on now in
machine learning we already have so many

00:05:24.140 --> 00:05:24.150
machine learning we already have so many
 

00:05:24.150 --> 00:05:26.090
machine learning we already have so many
hyper parameters to search over is

00:05:26.090 --> 00:05:26.100
hyper parameters to search over is
 

00:05:26.100 --> 00:05:28.580
hyper parameters to search over is
already very complicated to choose among

00:05:28.580 --> 00:05:28.590
already very complicated to choose among
 

00:05:28.590 --> 00:05:31.370
already very complicated to choose among
the space of possible algorithms and so

00:05:31.370 --> 00:05:31.380
the space of possible algorithms and so
 

00:05:31.380 --> 00:05:33.469
the space of possible algorithms and so
I find machine learning easier to think

00:05:33.469 --> 00:05:33.479
I find machine learning easier to think
 

00:05:33.479 --> 00:05:36.379
I find machine learning easier to think
about when you have one set of tools for

00:05:36.379 --> 00:05:36.389
about when you have one set of tools for
 

00:05:36.389 --> 00:05:38.659
about when you have one set of tools for
optimizing the cost function J and when

00:05:38.659 --> 00:05:38.669
optimizing the cost function J and when
 

00:05:38.669 --> 00:05:40.339
optimizing the cost function J and when
you're focusing on authorizing the cost

00:05:40.339 --> 00:05:40.349
you're focusing on authorizing the cost
 

00:05:40.349 --> 00:05:42.560
you're focusing on authorizing the cost
function J all you care about is finding

00:05:42.560 --> 00:05:42.570
function J all you care about is finding
 

00:05:42.570 --> 00:05:46.219
function J all you care about is finding
W and B so that J of W B is as small as

00:05:46.219 --> 00:05:46.229
W and B so that J of W B is as small as
 

00:05:46.229 --> 00:05:47.360
W and B so that J of W B is as small as
possible you just don't think about

00:05:47.360 --> 00:05:47.370
possible you just don't think about
 

00:05:47.370 --> 00:05:49.040
possible you just don't think about
anything else other than producing this

00:05:49.040 --> 00:05:49.050
anything else other than producing this
 

00:05:49.050 --> 00:05:53.089
anything else other than producing this
and then it's completely separate tasks

00:05:53.089 --> 00:05:53.099
and then it's completely separate tasks
 

00:05:53.099 --> 00:05:56.390
and then it's completely separate tasks
to not overstate in other words to

00:05:56.390 --> 00:05:56.400
to not overstate in other words to
 

00:05:56.400 --> 00:05:57.379
to not overstate in other words to
reduce the Arians

00:05:57.379 --> 00:05:57.389
reduce the Arians
 

00:05:57.389 --> 00:05:58.670
reduce the Arians
and when you're doing that you have a

00:05:58.670 --> 00:05:58.680
and when you're doing that you have a
 

00:05:58.680 --> 00:06:01.879
and when you're doing that you have a
second set of tools of doing it and this

00:06:01.879 --> 00:06:01.889
second set of tools of doing it and this
 

00:06:01.889 --> 00:06:04.360
second set of tools of doing it and this
principle is sometimes called

00:06:04.360 --> 00:06:04.370
principle is sometimes called
 

00:06:04.370 --> 00:06:07.610
principle is sometimes called
orthogonalization and this is idea that

00:06:07.610 --> 00:06:07.620
orthogonalization and this is idea that
 

00:06:07.620 --> 00:06:09.800
orthogonalization and this is idea that
you want to think about one task at a

00:06:09.800 --> 00:06:09.810
you want to think about one task at a
 

00:06:09.810 --> 00:06:10.460
you want to think about one task at a
time

00:06:10.460 --> 00:06:10.470
time
 

00:06:10.470 --> 00:06:12.140
time
I'll see you more about also

00:06:12.140 --> 00:06:12.150
I'll see you more about also
 

00:06:12.150 --> 00:06:14.690
I'll see you more about also
organization in a later video so if you

00:06:14.690 --> 00:06:14.700
organization in a later video so if you
 

00:06:14.700 --> 00:06:16.070
organization in a later video so if you
don't fully get the concept yet don't

00:06:16.070 --> 00:06:16.080
don't fully get the concept yet don't
 

00:06:16.080 --> 00:06:18.379
don't fully get the concept yet don't
worry about it but to me the main

00:06:18.379 --> 00:06:18.389
worry about it but to me the main
 

00:06:18.389 --> 00:06:21.080
worry about it but to me the main
downside is early swapping is that this

00:06:21.080 --> 00:06:21.090
downside is early swapping is that this
 

00:06:21.090 --> 00:06:24.440
downside is early swapping is that this
couples these two toss so you no longer

00:06:24.440 --> 00:06:24.450
couples these two toss so you no longer
 

00:06:24.450 --> 00:06:26.420
couples these two toss so you no longer
can work on these two problems

00:06:26.420 --> 00:06:26.430
can work on these two problems
 

00:06:26.430 --> 00:06:28.909
can work on these two problems
independently because by stopping

00:06:28.909 --> 00:06:28.919
independently because by stopping
 

00:06:28.919 --> 00:06:31.130
independently because by stopping
gradient descent early you're sort of

00:06:31.130 --> 00:06:31.140
gradient descent early you're sort of
 

00:06:31.140 --> 00:06:32.899
gradient descent early you're sort of
breaking whatever you're doing to

00:06:32.899 --> 00:06:32.909
breaking whatever you're doing to
 

00:06:32.909 --> 00:06:34.550
breaking whatever you're doing to
optimize the cost function J because now

00:06:34.550 --> 00:06:34.560
optimize the cost function J because now
 

00:06:34.560 --> 00:06:35.510
optimize the cost function J because now
you're not doing

00:06:35.510 --> 00:06:35.520
you're not doing
 

00:06:35.520 --> 00:06:37.490
you're not doing
a jar reducing the cost function genius

00:06:37.490 --> 00:06:37.500
a jar reducing the cost function genius
 

00:06:37.500 --> 00:06:39.469
a jar reducing the cost function genius
or not done that that well and then

00:06:39.469 --> 00:06:39.479
or not done that that well and then
 

00:06:39.479 --> 00:06:42.740
or not done that that well and then
you're also simultaneously trying to not

00:06:42.740 --> 00:06:42.750
you're also simultaneously trying to not
 

00:06:42.750 --> 00:06:44.629
you're also simultaneously trying to not
overstate so instead of using different

00:06:44.629 --> 00:06:44.639
overstate so instead of using different
 

00:06:44.639 --> 00:06:46.430
overstate so instead of using different
tools to solve the two problems you're

00:06:46.430 --> 00:06:46.440
tools to solve the two problems you're
 

00:06:46.440 --> 00:06:48.290
tools to solve the two problems you're
using one two they kind of mix us the

00:06:48.290 --> 00:06:48.300
using one two they kind of mix us the
 

00:06:48.300 --> 00:06:51.490
using one two they kind of mix us the
two and this just makes the set of

00:06:51.490 --> 00:06:51.500
two and this just makes the set of
 

00:06:51.500 --> 00:06:54.409
two and this just makes the set of
things you could try a more complicated

00:06:54.409 --> 00:06:54.419
things you could try a more complicated
 

00:06:54.419 --> 00:06:57.320
things you could try a more complicated
to think about rather than you think

00:06:57.320 --> 00:06:57.330
to think about rather than you think
 

00:06:57.330 --> 00:06:59.330
to think about rather than you think
early stopping one alternative is just

00:06:59.330 --> 00:06:59.340
early stopping one alternative is just
 

00:06:59.340 --> 00:07:02.450
early stopping one alternative is just
use l2 regularization then you can just

00:07:02.450 --> 00:07:02.460
use l2 regularization then you can just
 

00:07:02.460 --> 00:07:03.920
use l2 regularization then you can just
train the neural network as long as

00:07:03.920 --> 00:07:03.930
train the neural network as long as
 

00:07:03.930 --> 00:07:04.460
train the neural network as long as
possible

00:07:04.460 --> 00:07:04.470
possible
 

00:07:04.470 --> 00:07:06.619
possible
I find it this makes the search space of

00:07:06.619 --> 00:07:06.629
I find it this makes the search space of
 

00:07:06.629 --> 00:07:08.659
I find it this makes the search space of
type of parameters easier to decompose

00:07:08.659 --> 00:07:08.669
type of parameters easier to decompose
 

00:07:08.669 --> 00:07:10.879
type of parameters easier to decompose
and each of the search over but the

00:07:10.879 --> 00:07:10.889
and each of the search over but the
 

00:07:10.889 --> 00:07:12.050
and each of the search over but the
downside of this though is that you

00:07:12.050 --> 00:07:12.060
downside of this though is that you
 

00:07:12.060 --> 00:07:14.180
downside of this though is that you
might have to try a lot of values of the

00:07:14.180 --> 00:07:14.190
might have to try a lot of values of the
 

00:07:14.190 --> 00:07:16.159
might have to try a lot of values of the
regularization parameter lambda and so

00:07:16.159 --> 00:07:16.169
regularization parameter lambda and so
 

00:07:16.169 --> 00:07:19.189
regularization parameter lambda and so
this makes searching over many values of

00:07:19.189 --> 00:07:19.199
this makes searching over many values of
 

00:07:19.199 --> 00:07:20.960
this makes searching over many values of
lambda more computationally expensive

00:07:20.960 --> 00:07:20.970
lambda more computationally expensive
 

00:07:20.970 --> 00:07:23.689
lambda more computationally expensive
and the real advantage of early stopping

00:07:23.689 --> 00:07:23.699
and the real advantage of early stopping
 

00:07:23.699 --> 00:07:25.879
and the real advantage of early stopping
is that running the gradient descent

00:07:25.879 --> 00:07:25.889
is that running the gradient descent
 

00:07:25.889 --> 00:07:28.159
is that running the gradient descent
process just once you get to try out

00:07:28.159 --> 00:07:28.169
process just once you get to try out
 

00:07:28.169 --> 00:07:31.100
process just once you get to try out
values of small W midsize W at large W

00:07:31.100 --> 00:07:31.110
values of small W midsize W at large W
 

00:07:31.110 --> 00:07:33.589
values of small W midsize W at large W
without needing to try a lot of values

00:07:33.589 --> 00:07:33.599
without needing to try a lot of values
 

00:07:33.599 --> 00:07:36.469
without needing to try a lot of values
of the regularization LT regularization

00:07:36.469 --> 00:07:36.479
of the regularization LT regularization
 

00:07:36.479 --> 00:07:40.399
of the regularization LT regularization
hybrid parameter lambda um if this

00:07:40.399 --> 00:07:40.409
hybrid parameter lambda um if this
 

00:07:40.409 --> 00:07:42.290
hybrid parameter lambda um if this
concept doesn't completely make sense

00:07:42.290 --> 00:07:42.300
concept doesn't completely make sense
 

00:07:42.300 --> 00:07:44.120
concept doesn't completely make sense
yet don't worry about it we'll talk

00:07:44.120 --> 00:07:44.130
yet don't worry about it we'll talk
 

00:07:44.130 --> 00:07:45.830
yet don't worry about it we'll talk
about orthogonalization in greater

00:07:45.830 --> 00:07:45.840
about orthogonalization in greater
 

00:07:45.840 --> 00:07:47.870
about orthogonalization in greater
detail in the later video I think this

00:07:47.870 --> 00:07:47.880
detail in the later video I think this
 

00:07:47.880 --> 00:07:49.850
detail in the later video I think this
would make a bit more sense this

00:07:49.850 --> 00:07:49.860
would make a bit more sense this
 

00:07:49.860 --> 00:07:51.860
would make a bit more sense this
presence disadvantages many people do

00:07:51.860 --> 00:07:51.870
presence disadvantages many people do
 

00:07:51.870 --> 00:07:53.930
presence disadvantages many people do
use it I personally prefer to just use

00:07:53.930 --> 00:07:53.940
use it I personally prefer to just use
 

00:07:53.940 --> 00:07:55.969
use it I personally prefer to just use
l2 regularization and try different

00:07:55.969 --> 00:07:55.979
l2 regularization and try different
 

00:07:55.979 --> 00:07:58.339
l2 regularization and try different
values of lambda that's assuming you can

00:07:58.339 --> 00:07:58.349
values of lambda that's assuming you can
 

00:07:58.349 --> 00:08:00.920
values of lambda that's assuming you can
afford a computation to do so but early

00:08:00.920 --> 00:08:00.930
afford a computation to do so but early
 

00:08:00.930 --> 00:08:02.480
afford a computation to do so but early
stopping does let you get a similar

00:08:02.480 --> 00:08:02.490
stopping does let you get a similar
 

00:08:02.490 --> 00:08:04.580
stopping does let you get a similar
effect without needing to explicitly try

00:08:04.580 --> 00:08:04.590
effect without needing to explicitly try
 

00:08:04.590 --> 00:08:07.010
effect without needing to explicitly try
lots of different values of lambda so

00:08:07.010 --> 00:08:07.020
lots of different values of lambda so
 

00:08:07.020 --> 00:08:09.499
lots of different values of lambda so
you've now seen how to use data

00:08:09.499 --> 00:08:09.509
you've now seen how to use data
 

00:08:09.509 --> 00:08:11.719
you've now seen how to use data
augmentation as well as if you wish

00:08:11.719 --> 00:08:11.729
augmentation as well as if you wish
 

00:08:11.729 --> 00:08:13.999
augmentation as well as if you wish
early stopping in order to reduce

00:08:13.999 --> 00:08:14.009
early stopping in order to reduce
 

00:08:14.009 --> 00:08:16.279
early stopping in order to reduce
variance and prevent overfitting is in

00:08:16.279 --> 00:08:16.289
variance and prevent overfitting is in
 

00:08:16.289 --> 00:08:18.800
variance and prevent overfitting is in
your network next let's talk about some

00:08:18.800 --> 00:08:18.810
your network next let's talk about some
 

00:08:18.810 --> 00:08:20.930
your network next let's talk about some
techniques for such an optimization

00:08:20.930 --> 00:08:20.940
techniques for such an optimization
 

00:08:20.940 --> 00:08:25.099
techniques for such an optimization
problem to make your training go quickly

