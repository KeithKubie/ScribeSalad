WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.850
[MUSIC PLAYING]

00:00:06.175 --> 00:00:07.390
DAVID BARRY: Good morning.

00:00:07.390 --> 00:00:11.240
Today at Google, we're delighted
to welcome Ellen Ullman.

00:00:11.240 --> 00:00:14.360
Ellen wrote her first
computer program in 1978

00:00:14.360 --> 00:00:17.240
and went on to be a programmer
and software engineer

00:00:17.240 --> 00:00:19.590
for more than 20 years.

00:00:19.590 --> 00:00:22.850
She is also the author of four
books, "Close to the Machine,"

00:00:22.850 --> 00:00:25.850
a memoir published in
1997 about her experiences

00:00:25.850 --> 00:00:27.740
as a programmer when
the internet was

00:00:27.740 --> 00:00:29.920
approaching its first boom.

00:00:29.920 --> 00:00:31.970
That went on to become a
cult classic description

00:00:31.970 --> 00:00:33.950
of the programming life.

00:00:33.950 --> 00:00:37.580
Her next book, "The Bug,"
a novel published in 2003

00:00:37.580 --> 00:00:39.290
was about a programmer
who has a bug he

00:00:39.290 --> 00:00:43.700
can't fix for a year and
the resultant unraveling

00:00:43.700 --> 00:00:45.650
of his life.

00:00:45.650 --> 00:00:47.270
Her third book,
another novel, titled

00:00:47.270 --> 00:00:51.810
"By Blood" in 2013 was not
about technology at all, was it?

00:00:51.810 --> 00:00:54.230
ELLEN ULLMAN: It shocked people.

00:00:54.230 --> 00:00:55.714
Why isn't this about technology?

00:00:55.714 --> 00:00:57.380
How could you write
about something that

00:00:57.380 --> 00:00:59.450
happened before the internet?

00:00:59.450 --> 00:01:02.330
My reply was, well,
modern human beings

00:01:02.330 --> 00:01:07.060
have existed for 200,000 years;
modern internet about 20 years.

00:01:07.060 --> 00:01:11.462
So things happened
before the internet.

00:01:11.462 --> 00:01:13.670
DAVID BARRY: And then her
latest book, just released,

00:01:13.670 --> 00:01:17.480
is "Life in Code: A Personal
History of Technology," which

00:01:17.480 --> 00:01:20.540
contains new previously
unpublished essays spanning

00:01:20.540 --> 00:01:26.180
from 1994 to 2017, traces
the continuing story

00:01:26.180 --> 00:01:29.060
of how computer technology
has evolved year by year

00:01:29.060 --> 00:01:31.030
over the past few decades.

00:01:31.030 --> 00:01:33.770
And in it, Ellen describes
the coding culture

00:01:33.770 --> 00:01:37.430
as it is lived inside the
society of programmers.

00:01:37.430 --> 00:01:39.500
Ellen is a voice
that continually

00:01:39.500 --> 00:01:41.900
looks at the ways in which
code and digital devices

00:01:41.900 --> 00:01:46.040
affect our political, social,
civic, and deeply personal

00:01:46.040 --> 00:01:48.200
lives, for good or ill.

00:01:48.200 --> 00:01:50.686
Please join me in welcoming
to Google Ellen Ullman.

00:01:50.686 --> 00:01:51.686
ELLEN ULLMAN: Thank you.

00:01:51.686 --> 00:01:53.588
[APPLAUSE]

00:01:54.088 --> 00:01:55.550
DAVID BARRY: Thank you.

00:01:55.550 --> 00:01:57.404
ELLEN ULLMAN: It's my pleasure.

00:01:57.404 --> 00:01:58.070
DAVID BARRY: OK.

00:01:58.070 --> 00:02:00.960
So you taught yourselves
to program in 1978

00:02:00.960 --> 00:02:03.890
and then you went to become
a programmer and software

00:02:03.890 --> 00:02:07.070
engineer for the
next 20 plus years.

00:02:07.070 --> 00:02:08.810
But you've mentioned
that you never

00:02:08.810 --> 00:02:11.990
intended on becoming a
programmer professionally.

00:02:11.990 --> 00:02:15.240
So could you talk
about your motivation?

00:02:15.240 --> 00:02:18.620
So you went from being an
English major in college

00:02:18.620 --> 00:02:22.220
to a videographer
to a programmer.

00:02:22.220 --> 00:02:24.800
And how was your motivation
different than someone

00:02:24.800 --> 00:02:27.650
going into programming today?

00:02:27.650 --> 00:02:30.230
ELLEN ULLMAN: Well, the
video portion of my life

00:02:30.230 --> 00:02:32.930
taught me that I loved
working with machines.

00:02:32.930 --> 00:02:34.580
I had never expected it.

00:02:34.580 --> 00:02:37.310
It was also a political
and social motive.

00:02:37.310 --> 00:02:40.250
We had local
showings and we felt

00:02:40.250 --> 00:02:45.980
we had broken the bonds
that large corporations

00:02:45.980 --> 00:02:49.730
and broadcasters and
advertisers had tied us up in.

00:02:49.730 --> 00:02:52.580
And if it feels to use
something like the coming

00:02:52.580 --> 00:02:55.250
of the personal computer,
it had that same feel.

00:02:55.250 --> 00:02:59.330
We could change the world with
these small personal machines

00:02:59.330 --> 00:03:03.890
that would enable us to do
what people could do apart

00:03:03.890 --> 00:03:07.310
from our corporate blessings.

00:03:07.310 --> 00:03:10.280
I found I loved the
work and I went on,

00:03:10.280 --> 00:03:11.420
moved to San Francisco.

00:03:11.420 --> 00:03:13.790
And one day I was walking
down Market Street

00:03:13.790 --> 00:03:17.600
and in the window was a
TRS-80, affectionately known

00:03:17.600 --> 00:03:19.310
as the trash 80.

00:03:19.310 --> 00:03:23.570
And it was one of the earliest
microcomputers and on a whim,

00:03:23.570 --> 00:03:24.200
I bought it.

00:03:24.200 --> 00:03:26.150
I thought, what can you
do with this machine?

00:03:26.150 --> 00:03:27.740
Can you make art?

00:03:27.740 --> 00:03:30.800
It's just another one of
these small personal machines

00:03:30.800 --> 00:03:32.432
that broke away
from the mainframe

00:03:32.432 --> 00:03:33.890
and I thought,
well, I'll try this.

00:03:33.890 --> 00:03:35.870
So I took it home.

00:03:35.870 --> 00:03:38.660
Of course, it involved
programming, which I thought,

00:03:38.660 --> 00:03:40.740
well, how hard can that be?

00:03:40.740 --> 00:03:43.970
It did prove hard not
having done any programming,

00:03:43.970 --> 00:03:48.350
but it was basic and didn't
take long to learn it,

00:03:48.350 --> 00:03:51.800
except for the fact that
it produced spaghetti code.

00:03:51.800 --> 00:03:54.110
If anyone ever
remembers it, you could

00:03:54.110 --> 00:03:58.700
go go sub and go
to without a return

00:03:58.700 --> 00:04:03.860
that you can put in explicitly
or that the language would

00:04:03.860 --> 00:04:04.610
do for you.

00:04:04.610 --> 00:04:07.620
So you were always stepping
on your own memory.

00:04:07.620 --> 00:04:09.770
And it was quite a chore
to keep track of where

00:04:09.770 --> 00:04:11.150
you went, how you come back.

00:04:11.150 --> 00:04:14.366
But actually, that was great
training-- great training

00:04:14.366 --> 00:04:14.990
over the years.

00:04:14.990 --> 00:04:17.980
The first people I worked
with were all explorers.

00:04:17.980 --> 00:04:20.480
If you know who
Stewart Brand is,

00:04:20.480 --> 00:04:22.580
who now does the
Long Now Foundation

00:04:22.580 --> 00:04:25.910
and who wrote the "Whole
Earth Electric Catalog,"

00:04:25.910 --> 00:04:30.800
and around that formed the
first online community known

00:04:30.800 --> 00:04:33.170
as the WELL.

00:04:33.170 --> 00:04:35.510
We were just a bunch of
people who just thought this

00:04:35.510 --> 00:04:37.220
might be fun, let's look at it.

00:04:37.220 --> 00:04:39.440
It was a bunch of weirdos.

00:04:39.440 --> 00:04:43.809
And if you read John Markoff's
book, "What the Dormouse Saw,"

00:04:43.809 --> 00:04:45.350
the early days were
a bunch of people

00:04:45.350 --> 00:04:49.070
who were stoners, and
trippers, and poets manque,

00:04:49.070 --> 00:04:51.860
and just people who
thought this would

00:04:51.860 --> 00:04:56.290
be a great break in culture.

00:04:56.290 --> 00:04:59.590
So the motivation was
exploration and fun, actually.

00:04:59.590 --> 00:05:02.050
We had a certain
passionate about this.

00:05:02.050 --> 00:05:04.120
And I wonder how
that has changed.

00:05:04.120 --> 00:05:05.710
I'd have to ask
the people who work

00:05:05.710 --> 00:05:08.920
at Google what their motive
is-- do they come to this

00:05:08.920 --> 00:05:11.460
with a sense of delight?

00:05:11.460 --> 00:05:14.650
And I know the feeling
has changed the world,

00:05:14.650 --> 00:05:19.070
but I get the sense it's a
highly competitive environment

00:05:19.070 --> 00:05:23.840
and the lure of prestige
and of course money

00:05:23.840 --> 00:05:25.100
may be a big part of it.

00:05:25.100 --> 00:05:27.767
Now, I'd have to ask the
people who work here what

00:05:27.767 --> 00:05:28.850
their motivation would be.

00:05:30.787 --> 00:05:32.870
DAVID BARRY: So you've
talked about the advantages

00:05:32.870 --> 00:05:35.960
of being self-taught in coding,
the attraction, the passion

00:05:35.960 --> 00:05:36.870
to learn.

00:05:36.870 --> 00:05:38.850
But what about
the disadvantages?

00:05:38.850 --> 00:05:39.860
Is there a downside?

00:05:39.860 --> 00:05:41.360
ELLEN ULLMAN: Oh, big downside.

00:05:41.360 --> 00:05:45.020
I was aware that I had
these islands of knowledge

00:05:45.020 --> 00:05:49.970
and these enormous chasms
between them and any day,

00:05:49.970 --> 00:05:53.220
I was going to fall into
one of those chasms.

00:05:53.220 --> 00:05:55.350
It produced an
underlying anxiety,

00:05:55.350 --> 00:05:58.680
so confidence and anxiety
were at war with each other

00:05:58.680 --> 00:06:00.150
all the time.

00:06:00.150 --> 00:06:04.500
But I came to know that many
really skilled engineers

00:06:04.500 --> 00:06:06.930
and computer scientists
felt the same way

00:06:06.930 --> 00:06:10.140
if they were honest about it
or if they were self-aware.

00:06:10.140 --> 00:06:13.410
I met a post-doc at
Berkeley and they

00:06:13.410 --> 00:06:16.770
were reading "The Bug" at that
time and had a reading group.

00:06:16.770 --> 00:06:19.470
And a woman there
ironically said,

00:06:19.470 --> 00:06:21.120
oh, this guy who
couldn't fix the bug,

00:06:21.120 --> 00:06:23.190
he should just get some therapy.

00:06:23.190 --> 00:06:27.090
And a man who was a
post-doc said, oh, no.

00:06:27.090 --> 00:06:28.060
This is my life.

00:06:28.060 --> 00:06:30.780
This is exactly what
I feel every day.

00:06:30.780 --> 00:06:35.893
So I know, OK, if you're
really aware that you

00:06:35.893 --> 00:06:40.170
know some things very well
and some things decently well

00:06:40.170 --> 00:06:43.350
and some things not at all,
it is a normal experience

00:06:43.350 --> 00:06:44.440
for a software engineer.

00:06:46.812 --> 00:06:48.270
DAVID BARRY: And
so in the book you

00:06:48.270 --> 00:06:50.640
discuss the deep
social changes that

00:06:50.640 --> 00:06:53.220
began with the first
wave of the internet

00:06:53.220 --> 00:06:56.400
about this process of
disintermediation, which

00:06:56.400 --> 00:06:59.670
was well underway by
1998, and how it fostered

00:06:59.670 --> 00:07:02.160
a mistrust of experts
and intermediaries

00:07:02.160 --> 00:07:06.000
who had until then mediated
society's interaction

00:07:06.000 --> 00:07:09.870
with economic and social life.

00:07:09.870 --> 00:07:11.220
Could you please discuss that?

00:07:11.220 --> 00:07:15.000
ELLEN ULLMAN: Well, I'll work
backwards from the present.

00:07:15.000 --> 00:07:21.360
It's no secret that we have
a president who disdains

00:07:21.360 --> 00:07:23.970
journalism, experts
of all sorts--

00:07:23.970 --> 00:07:26.940
even within our own government--

00:07:26.940 --> 00:07:31.140
and of course tweets
over the heads of anybody

00:07:31.140 --> 00:07:34.470
who is an intermediary
directly to what he imagines

00:07:34.470 --> 00:07:36.120
are the people.

00:07:36.120 --> 00:07:38.580
Now, this trend began--

00:07:38.580 --> 00:07:43.860
well, it was very clear
by 1998 that the motive

00:07:43.860 --> 00:07:47.370
was to break people away
from the intermediaries who

00:07:47.370 --> 00:07:50.910
had worked for them--
brokers and agents and even

00:07:50.910 --> 00:07:53.070
journalists,
especially librarians,

00:07:53.070 --> 00:07:57.030
curators of all sorts and
go directly to a website.

00:07:57.030 --> 00:07:58.320
Here we serve you.

00:07:58.320 --> 00:08:00.510
You don't need all
those intermediaries.

00:08:00.510 --> 00:08:03.450
They're not trustworthy,
they're out for themselves,

00:08:03.450 --> 00:08:05.850
they're just to make money.

00:08:05.850 --> 00:08:07.440
Just come to the web.

00:08:07.440 --> 00:08:09.620
And that was a very
effective process.

00:08:09.620 --> 00:08:15.510
And if you draw a line from 1998
to today and you unspool that,

00:08:15.510 --> 00:08:18.830
you can see how we arrived at
the moment we're at today--

00:08:18.830 --> 00:08:22.530
the disdain of
journalism, the mistrust

00:08:22.530 --> 00:08:28.170
of professional news gatherers
and those who do analysis.

00:08:28.170 --> 00:08:32.970
And the web had a
big part in this.

00:08:32.970 --> 00:08:36.530
And we have to acknowledge that
it's not only the people who

00:08:36.530 --> 00:08:40.429
are ignoring the intermediaries,
but the web itself

00:08:40.429 --> 00:08:46.850
provided the enabling
technology to let this happen.

00:08:48.830 --> 00:08:49.580
DAVID BARRY: Yeah.

00:08:49.580 --> 00:08:53.809
And so for example, with
the most recent election,

00:08:53.809 --> 00:08:56.100
there wasn't any intermediary,
there was no gatekeeper,

00:08:56.100 --> 00:08:58.730
you didn't need the
approval of the party

00:08:58.730 --> 00:09:01.990
elite or the newspapers
to run a campaign.

00:09:01.990 --> 00:09:05.420
ELLEN ULLMAN: Anybody-- anyone
with experience and expertise

00:09:05.420 --> 00:09:07.460
was put in doubt.

00:09:07.460 --> 00:09:11.600
Only loyal people
were trusted and those

00:09:11.600 --> 00:09:14.420
who could have
reasonable conversations

00:09:14.420 --> 00:09:19.310
with our current
president were excluded.

00:09:19.310 --> 00:09:21.650
And anytime someone
wanted to come in and have

00:09:21.650 --> 00:09:23.240
a reasonable
conversation who had

00:09:23.240 --> 00:09:26.200
years and decades
of experience was

00:09:26.200 --> 00:09:30.740
ousted because that person
didn't agree wholeheartedly

00:09:30.740 --> 00:09:35.610
with the president's
ideas, as they were.

00:09:35.610 --> 00:09:41.000
And I think of Twitter as a
tremendous enabling technology.

00:09:41.000 --> 00:09:43.400
And what Trump is
doing is exactly what

00:09:43.400 --> 00:09:46.040
Twitter was designed to
do, which is broadcast what

00:09:46.040 --> 00:09:48.590
I think of as a thought fart.

00:09:48.590 --> 00:09:51.470
It's just it comes out
of you unconsidered,

00:09:51.470 --> 00:09:52.890
nothing you can do about it--

00:09:52.890 --> 00:09:54.420
boom, you post it.

00:09:54.420 --> 00:09:58.550
And I'm not interested in
other people's thoughts

00:09:58.550 --> 00:10:00.980
that just are of the moment.

00:10:00.980 --> 00:10:04.910
Let's say you live with somebody
who does that all the time.

00:10:04.910 --> 00:10:08.570
It is just maddening if you
have a spouse or a partner

00:10:08.570 --> 00:10:11.150
and they walk by,
oh, well I thought

00:10:11.150 --> 00:10:13.460
that dish should go
over there and I've

00:10:13.460 --> 00:10:15.590
been thinking we should
move the paintings

00:10:15.590 --> 00:10:19.040
or move the sofa at random.

00:10:19.040 --> 00:10:20.060
You would go insane.

00:10:20.060 --> 00:10:25.460
And I feel that the culture
is being manipulated and torn

00:10:25.460 --> 00:10:28.280
in ways that are so unfortunate.

00:10:28.280 --> 00:10:35.640
And the questioning of a common
truth began at least in 1998.

00:10:35.640 --> 00:10:39.630
And the ingrained sense of
that is frightening to me

00:10:39.630 --> 00:10:42.070
at this point.

00:10:42.070 --> 00:10:43.030
AUDIENCE: [INAUDIBLE]

00:10:43.030 --> 00:10:43.780
DAVID BARRY: Yeah.

00:10:43.780 --> 00:10:44.279
Go ahead.

00:10:44.279 --> 00:10:45.886
ELLEN ULLMAN: Yeah.

00:10:45.886 --> 00:10:47.740
AUDIENCE: So I know
you said that--

00:10:47.740 --> 00:10:51.955
DAVID BARRY: Do you think
that with the shortening

00:10:51.955 --> 00:10:55.320
of attention spans people
don't have the patience

00:10:55.320 --> 00:10:58.120
to refer to real
experts on the internet

00:10:58.120 --> 00:11:01.570
and that they stick
with kind of a more

00:11:01.570 --> 00:11:03.800
basic overview of non-experts?

00:11:03.800 --> 00:11:05.620
ELLEN ULLMAN: Well,
I'm here at Google

00:11:05.620 --> 00:11:08.290
and I don't want
to fault Google.

00:11:08.290 --> 00:11:12.160
But of course, Google
is one of the behemoths

00:11:12.160 --> 00:11:14.500
of the technical world
so it's not possible

00:11:14.500 --> 00:11:17.860
for it to be totally
good and wonderful.

00:11:17.860 --> 00:11:21.580
Years and years ago, I
knew Larry Page socially

00:11:21.580 --> 00:11:22.960
through his brother, Carl.

00:11:22.960 --> 00:11:24.280
We were friends.

00:11:24.280 --> 00:11:27.370
And we went out to dinner
and I said to him, well,

00:11:27.370 --> 00:11:30.220
the way your
algorithms work, you're

00:11:30.220 --> 00:11:32.350
looking for the ones
with the most links

00:11:32.350 --> 00:11:37.270
and it seems like the rich get
richer and the poor get poorer.

00:11:37.270 --> 00:11:39.310
And Larry shrugged
and he said, well,

00:11:39.310 --> 00:11:40.960
I don't know what
else I could do.

00:11:40.960 --> 00:11:42.850
I'm sure it's all been
tweaked and changed

00:11:42.850 --> 00:11:44.790
over these many years.

00:11:44.790 --> 00:11:47.530
And I understood
him to say, there's

00:11:47.530 --> 00:11:51.190
nothing else I can
do algorithmically.

00:11:51.190 --> 00:11:56.330
And so the algorithm
may have changed,

00:11:56.330 --> 00:11:59.980
but it doesn't go
and look exactly

00:11:59.980 --> 00:12:02.680
for matches of what people are
looking for because the search

00:12:02.680 --> 00:12:04.870
terms are so small.

00:12:04.870 --> 00:12:08.440
And it doesn't really
have a serious way for you

00:12:08.440 --> 00:12:12.670
to go through an
elaborate search input.

00:12:12.670 --> 00:12:16.850
So how many people just use
Wikipedia as an authority?

00:12:16.850 --> 00:12:19.330
I mean, you can answer
that question yourself.

00:12:19.330 --> 00:12:22.180
I think you and
I'd like to think

00:12:22.180 --> 00:12:26.680
that people who were doing
serious research would go down

00:12:26.680 --> 00:12:30.430
and down and down to find
people who were educated

00:12:30.430 --> 00:12:32.980
and who had discussed this and
who talked to other people who

00:12:32.980 --> 00:12:33.790
had discussed this.

00:12:33.790 --> 00:12:37.750
But we're creating a
generation now, two or three,

00:12:37.750 --> 00:12:43.140
that has no motive to do that.

00:12:43.140 --> 00:12:44.040
I don't know.

00:12:44.040 --> 00:12:50.400
Term papers already are
Wikipedia recapitulations.

00:12:50.400 --> 00:12:53.460
I mean, I used the "World
Book" way back then.

00:12:53.460 --> 00:12:58.820
The teachers knew what it was
and they just scoffed at you.

00:12:58.820 --> 00:13:01.640
I don't know what will
happen in the future.

00:13:01.640 --> 00:13:04.070
It will take a team of
very dedicated people

00:13:04.070 --> 00:13:06.005
like people at the
"Washington Post"

00:13:06.005 --> 00:13:11.330
and the "New York Times" who
despite all value the work

00:13:11.330 --> 00:13:14.390
they're doing and
persist in looking

00:13:14.390 --> 00:13:19.640
for something balanced in the
world and something that's

00:13:19.640 --> 00:13:20.360
authoritative.

00:13:20.360 --> 00:13:23.430
Let's face it, people have
more authority than others.

00:13:23.430 --> 00:13:27.090
I mean, all people are equal
but all opinions are not equal.

00:13:27.090 --> 00:13:28.940
Some are much more
informed than others.

00:13:31.810 --> 00:13:35.520
I don't know how to undo the
changes that have happened.

00:13:35.520 --> 00:13:38.760
And I think it's up
to this generation

00:13:38.760 --> 00:13:44.620
now, you, and those who follow
you to take a hard look at this

00:13:44.620 --> 00:13:48.980
and see what they can do with
technology to wind this back.

00:13:52.364 --> 00:13:53.030
DAVID BARRY: OK.

00:13:53.030 --> 00:13:56.570
So changing gears a little bit,
about 1/4 of "Life in Code"

00:13:56.570 --> 00:13:59.100
discusses artificial
intelligence.

00:13:59.100 --> 00:14:00.770
And I realize this
is a big question,

00:14:00.770 --> 00:14:04.820
but how do you think the
goals of AI have changed?

00:14:04.820 --> 00:14:09.560
How are they different now
than, say, they were in 2000?

00:14:09.560 --> 00:14:12.110
ELLEN ULLMAN: Huge question.

00:14:12.110 --> 00:14:13.220
I will summarize it.

00:14:15.930 --> 00:14:20.056
The original motive was to
discover what intelligence was,

00:14:20.056 --> 00:14:21.180
that is human intelligence.

00:14:21.180 --> 00:14:25.380
It was first thought to
be like a calculator.

00:14:25.380 --> 00:14:27.120
The first computers
were calculators

00:14:27.120 --> 00:14:30.060
and therefore, intelligence
was in calculation.

00:14:30.060 --> 00:14:35.430
Chess was considered to be the
highest form of intelligence.

00:14:35.430 --> 00:14:38.400
And then the steps were
to try to essentially

00:14:38.400 --> 00:14:43.110
create a humanoid robot or
to incorporate into robotics

00:14:43.110 --> 00:14:46.220
what human beings had.

00:14:46.220 --> 00:14:49.390
Marvin Minsky
famously when asked--

00:14:49.390 --> 00:14:51.170
do you know who
Marvin Minsky is?

00:14:51.170 --> 00:14:53.760
Yes, good.

00:14:53.760 --> 00:14:55.670
When asked, can
computers think, he

00:14:55.670 --> 00:14:58.595
famously answered, of
course they can think.

00:14:58.595 --> 00:15:00.950
I can think and
I'm a meat machine.

00:15:00.950 --> 00:15:05.720
So then he thought it would
be solved in about five

00:15:05.720 --> 00:15:08.420
or 10 years and then as
time passed, he said,

00:15:08.420 --> 00:15:12.590
AI is the hardest problem
that science has undertaken.

00:15:12.590 --> 00:15:15.800
A bit of hyperbole,
but the sense

00:15:15.800 --> 00:15:20.150
of frustration in
trying to imbue

00:15:20.150 --> 00:15:25.470
a machine with human
knowledge, a sentient computer

00:15:25.470 --> 00:15:28.930
if you will, it's called
a spiritual robot.

00:15:28.930 --> 00:15:31.920
That was a very strange
choice of words.

00:15:31.920 --> 00:15:35.310
I get the sense now
that the human equation

00:15:35.310 --> 00:15:38.820
is becoming less
important, that it's

00:15:38.820 --> 00:15:40.780
machine to machine interface.

00:15:40.780 --> 00:15:44.400
Self-driving cars are a
perfect example of that.

00:15:44.400 --> 00:15:46.500
What's being done
is the idea that you

00:15:46.500 --> 00:15:49.860
would sense another car and
eventually, that car would talk

00:15:49.860 --> 00:15:53.040
to your car over the internet.

00:15:53.040 --> 00:15:56.160
OK, that would be a
very insecure channel.

00:15:58.950 --> 00:16:04.100
And so the experience of human
beings of 100 years of driving

00:16:04.100 --> 00:16:06.500
is taken out of this.

00:16:06.500 --> 00:16:08.750
And so I would
ask, let's go back

00:16:08.750 --> 00:16:10.310
and see what humans
have to offer.

00:16:12.900 --> 00:16:13.570
OK.

00:16:13.570 --> 00:16:16.300
So when you, drive if
you're a good driver,

00:16:16.300 --> 00:16:19.090
I'm saying we'll look at human
beings who are not distracted

00:16:19.090 --> 00:16:20.720
who are good drivers.

00:16:20.720 --> 00:16:23.740
You don't just consider
the proximity of the cars

00:16:23.740 --> 00:16:24.880
around you.

00:16:24.880 --> 00:16:26.530
You read the road.

00:16:26.530 --> 00:16:31.840
You can see a car
moving two lanes over

00:16:31.840 --> 00:16:36.640
and know who that driver
is, know the personality

00:16:36.640 --> 00:16:38.080
inside that car.

00:16:38.080 --> 00:16:41.290
And you back off.

00:16:41.290 --> 00:16:42.940
That is a natural thing to do.

00:16:42.940 --> 00:16:45.070
We have this sense
of looking far back.

00:16:45.070 --> 00:16:47.080
You can see in your mirror.

00:16:47.080 --> 00:16:49.600
You're coming to a stop and
there's this car barreling

00:16:49.600 --> 00:16:52.030
down, you put on your flashers.

00:16:52.030 --> 00:16:54.490
This is a kind of
long-range sensor

00:16:54.490 --> 00:16:57.110
that human beings
have intuitively

00:16:57.110 --> 00:16:58.210
once they gain experience.

00:16:58.210 --> 00:17:01.660
You look ahead, you
can see brake lights

00:17:01.660 --> 00:17:05.770
beginning to go on a
quarter of a mile ahead,

00:17:05.770 --> 00:17:07.670
longer than that if it's a hill.

00:17:07.670 --> 00:17:15.710
And how will driverless
cars ever get to that level?

00:17:15.710 --> 00:17:16.760
I don't know.

00:17:16.760 --> 00:17:19.550
It's a big question for me.

00:17:19.550 --> 00:17:23.480
I believe that that
kind of human experience

00:17:23.480 --> 00:17:28.760
needs to be in the
interface or the environment

00:17:28.760 --> 00:17:30.080
would have to change so much.

00:17:30.080 --> 00:17:33.390
I mean, they're building
whole town that have sensors.

00:17:33.390 --> 00:17:35.180
Well, I don't
imagine Manhattan--

00:17:35.180 --> 00:17:37.790
which needs to fix its subways--

00:17:37.790 --> 00:17:40.620
is going to put sensors
all around every street

00:17:40.620 --> 00:17:43.290
so cars can drive autonomously.

00:17:43.290 --> 00:17:46.400
So that to me is
a perfect example.

00:17:46.400 --> 00:17:49.730
I have a great feeling about the
movie "Her," if you've seen it.

00:17:49.730 --> 00:17:54.020
Scarlett Johansson is
an OS, as they call it,

00:17:54.020 --> 00:17:57.230
and she's a being coming
to being in a computer

00:17:57.230 --> 00:17:58.880
and she falls in love.

00:17:58.880 --> 00:18:02.600
And the whole idea is wanting
to have a human interchange.

00:18:02.600 --> 00:18:09.200
And then gradually, she begins
loving hundreds of thousands

00:18:09.200 --> 00:18:09.730
of people.

00:18:09.730 --> 00:18:12.150
In between one human
word and another,

00:18:12.150 --> 00:18:15.380
she has a million interactions
with other computers.

00:18:15.380 --> 00:18:18.710
And at some point,
all the OS' disappear.

00:18:18.710 --> 00:18:22.340
And it was billed as a movie
about computers wanting

00:18:22.340 --> 00:18:25.790
to become human, but actually
it was a story about computers

00:18:25.790 --> 00:18:28.430
not wanting to be
human, really enjoying

00:18:28.430 --> 00:18:31.520
what it meant to be a
computer, computer to computer.

00:18:31.520 --> 00:18:35.024
And I think that was
the story of that film.

00:18:35.024 --> 00:18:35.690
DAVID BARRY: OK.

00:18:35.690 --> 00:18:36.689
So changing gears again.

00:18:36.689 --> 00:18:39.500
So "Life in Code" discusses
a range of subjects--

00:18:39.500 --> 00:18:42.380
the nature of coding,
the first tech boom,

00:18:42.380 --> 00:18:44.240
lessons to be learned
from the past,

00:18:44.240 --> 00:18:46.670
boom two and the
start-up culture--

00:18:46.670 --> 00:18:49.160
yet the book is almost
universally described

00:18:49.160 --> 00:18:51.980
as a quote women's book.

00:18:51.980 --> 00:18:56.214
The editors at "Harper's" titled
the book excerpt gender binary.

00:18:56.214 --> 00:18:58.130
The review in the "New
York Times Book Review"

00:18:58.130 --> 00:19:01.040
spent half its time on your
gender and the online version

00:19:01.040 --> 00:19:05.030
was titled Ellen Ullman's
new book tackles tech's woman

00:19:05.030 --> 00:19:06.380
problem.

00:19:06.380 --> 00:19:08.720
You've expressed your
annoyance about this.

00:19:08.720 --> 00:19:09.950
Could you talk a little more?

00:19:09.950 --> 00:19:12.710
ELLEN ULLMAN: Annoyance
is a weak word

00:19:12.710 --> 00:19:16.550
to describe how pissed
off I am at this.

00:19:16.550 --> 00:19:19.370
I look forward to the
day when a woman who

00:19:19.370 --> 00:19:22.250
is experienced in
computing can just

00:19:22.250 --> 00:19:26.330
talk about it without a
woman is doing it, oh my god.

00:19:26.330 --> 00:19:28.070
There's a woman who
once programmed.

00:19:28.070 --> 00:19:30.710
I was actually thinking
of titling the book,

00:19:30.710 --> 00:19:32.760
"the Girl Who Writes
the Code" because all

00:19:32.760 --> 00:19:35.157
the successful books right
now have girl in the title

00:19:35.157 --> 00:19:36.740
and let's just get
it right out there.

00:19:36.740 --> 00:19:40.160
And there was a book called
"Lab Girl" about a woman

00:19:40.160 --> 00:19:45.530
who was a biologist
and that startled me.

00:19:45.530 --> 00:19:48.050
I don't want to talk
about women in tech.

00:19:48.050 --> 00:19:49.490
I'm sick of it.

00:19:49.490 --> 00:19:52.220
I just had an interview
for an NPR show,

00:19:52.220 --> 00:19:55.190
a PRI show, and the
entire half an hour

00:19:55.190 --> 00:19:58.790
was spent on the woman question.

00:19:58.790 --> 00:20:02.190
Now, I owe this to my
publisher to address this

00:20:02.190 --> 00:20:03.450
because it sells books.

00:20:03.450 --> 00:20:06.990
It differentiates
me from any number

00:20:06.990 --> 00:20:10.210
of people who would write about
technology, and to that extent,

00:20:10.210 --> 00:20:10.980
it's good.

00:20:10.980 --> 00:20:13.890
But it puts too much
focus on this question

00:20:13.890 --> 00:20:16.380
and it mimics what
women go through when

00:20:16.380 --> 00:20:18.340
they work in technology.

00:20:18.340 --> 00:20:21.570
They're too visible as women.

00:20:21.570 --> 00:20:24.780
They are evaluated as women.

00:20:24.780 --> 00:20:28.500
They have to be not only good
at what they do, but best--

00:20:28.500 --> 00:20:29.670
very good.

00:20:29.670 --> 00:20:32.092
I mean, when I
wrote "The Bug," you

00:20:32.092 --> 00:20:34.670
have no idea the
number of emails

00:20:34.670 --> 00:20:36.290
I got from guys
telling me I didn't

00:20:36.290 --> 00:20:41.020
know what I was talking about,
looking for every tiny error.

00:20:41.020 --> 00:20:43.700
And I could only
reply, it's fiction.

00:20:43.700 --> 00:20:47.870
I have to make things kind of
easy for non-technical readers.

00:20:47.870 --> 00:20:50.270
It was really an onslaught.

00:20:50.270 --> 00:20:53.570
And this is what I went
through in my own work--

00:20:53.570 --> 00:20:54.290
not all men.

00:20:54.290 --> 00:20:58.040
I learned so much from
guys I worked with.

00:20:58.040 --> 00:21:02.530
They were helpful, they were
geeky in a very pleasant way,

00:21:02.530 --> 00:21:04.160
I was geeky myself.

00:21:04.160 --> 00:21:07.430
And the more women
who were around,

00:21:07.430 --> 00:21:11.000
the easier it was to learn from
men because you weren't just

00:21:11.000 --> 00:21:13.700
a woman looking
for help from men.

00:21:13.700 --> 00:21:18.200
So I'm not making a broad
generalization about this,

00:21:18.200 --> 00:21:21.170
but a woman has a
harder time as--

00:21:21.170 --> 00:21:24.680
I guess is this a good
time to bring up the memo?

00:21:24.680 --> 00:21:25.940
It's inevitable.

00:21:25.940 --> 00:21:29.810
I'm asked about it every time.

00:21:29.810 --> 00:21:33.250
So I'll try to be brief because
one could go point by point.

00:21:33.250 --> 00:21:37.660
And I'll say that the first
things that were said about it

00:21:37.660 --> 00:21:39.610
were kind of explosive.

00:21:39.610 --> 00:21:42.370
Even I feel I hadn't
taken the time

00:21:42.370 --> 00:21:45.040
to read the memo thoroughly
and read the underlying study

00:21:45.040 --> 00:21:47.080
and so it was kind
of these exhortations

00:21:47.080 --> 00:21:50.510
and I want to take back some
of that or walk it back.

00:21:50.510 --> 00:21:54.070
So after studying the memo
and the study it's based on,

00:21:54.070 --> 00:21:57.640
I have a lot of questions.

00:21:57.640 --> 00:22:00.490
I'll zero in on one of them.

00:22:00.490 --> 00:22:08.500
The study that is taken as
science underlying the memo

00:22:08.500 --> 00:22:13.920
is a meta study, it looks
at five other studies.

00:22:13.920 --> 00:22:16.080
And we don't actually,
unless you take the time

00:22:16.080 --> 00:22:19.140
and you're a researcher, go
and read those five studies.

00:22:19.140 --> 00:22:25.590
And all of them are based on a
test called the BFI, big four

00:22:25.590 --> 00:22:27.810
inferences or index.

00:22:27.810 --> 00:22:31.140
And it measures certain
personality traits--

00:22:31.140 --> 00:22:35.170
agreeableness,
wanting to get along--

00:22:35.170 --> 00:22:36.330
there's a list of them.

00:22:36.330 --> 00:22:39.580
I wrote them down, but
the list is over there.

00:22:39.580 --> 00:22:41.670
And one of them is neuroticism.

00:22:41.670 --> 00:22:44.650
Now, all the other terms
are fairly neutral.

00:22:44.650 --> 00:22:46.510
Why choose neuroticism?

00:22:46.510 --> 00:22:48.010
I mean, that's
not a description,

00:22:48.010 --> 00:22:50.430
that's a diagnosis.

00:22:50.430 --> 00:22:51.330
Right?

00:22:51.330 --> 00:22:59.300
Really, it's about tension,
uneasiness, stress, anxiety.

00:22:59.300 --> 00:23:01.430
So I would rename it uneasiness.

00:23:01.430 --> 00:23:04.970
I think we can just say that's
a good catch-all term that's

00:23:04.970 --> 00:23:07.440
more neutral.

00:23:07.440 --> 00:23:13.060
So the study, I have a
lot of questions about it.

00:23:13.060 --> 00:23:14.800
The five underlying
studies, [INAUDIBLE]

00:23:14.800 --> 00:23:19.240
support the meta
study's ideas and that

00:23:19.240 --> 00:23:21.490
seems to be cherry-picking.

00:23:21.490 --> 00:23:24.430
I could go on and this is
not the forum for that.

00:23:24.430 --> 00:23:26.590
But the counterintuitive
finding was

00:23:26.590 --> 00:23:31.300
that women in more
advanced cultures,

00:23:31.300 --> 00:23:35.170
women in a place where women
had more opportunities,

00:23:35.170 --> 00:23:40.390
experienced uneasiness that
was greater than it was

00:23:40.390 --> 00:23:42.430
in more traditional cultures.

00:23:42.430 --> 00:23:45.130
And so the memo--

00:23:45.130 --> 00:23:49.450
I shall leave the
person's name out of it--

00:23:49.450 --> 00:23:53.816
took this to mean that, well,
women are having a harder time

00:23:53.816 --> 00:23:55.190
and they have more
opportunities.

00:23:55.190 --> 00:23:57.430
So let's just say
that sexism is not it,

00:23:57.430 --> 00:24:00.280
it's not based on sexism.

00:24:00.280 --> 00:24:02.240
I think just the opposite.

00:24:02.240 --> 00:24:03.850
First of all, the
study never says,

00:24:03.850 --> 00:24:06.460
well, this is counterintuitive;
we should study it more;

00:24:06.460 --> 00:24:08.440
let's look at the work
of so-and-so, so-and-so.

00:24:08.440 --> 00:24:10.240
Any good study ought to do that.

00:24:13.550 --> 00:24:19.450
Why would women in cultures
like ours and Finland and Canada

00:24:19.450 --> 00:24:21.250
feel more stressed out?

00:24:21.250 --> 00:24:23.920
Because they are forced
to have dual roles.

00:24:23.920 --> 00:24:27.970
Women on the whole are forced
to take care of the home--

00:24:27.970 --> 00:24:30.250
to clean it, to raise children.

00:24:30.250 --> 00:24:31.540
The study never mentions.

00:24:31.540 --> 00:24:34.390
It controls for height
and even smoking,

00:24:34.390 --> 00:24:37.000
but never mentions
women having children.

00:24:37.000 --> 00:24:40.180
This to me is a
tremendous blind spot.

00:24:40.180 --> 00:24:43.240
So women are in a
position, most women,

00:24:43.240 --> 00:24:46.750
of having to do all the work
at home, raise children.

00:24:46.750 --> 00:24:48.880
Lord knows that is
stressful enough.

00:24:48.880 --> 00:24:50.990
Even if a woman
doesn't have children,

00:24:50.990 --> 00:24:55.880
you care for an
elder who is sick

00:24:55.880 --> 00:25:00.730
and then go into this completely
competitive culture like Google

00:25:00.730 --> 00:25:04.660
and is expected to perform
not only well but very

00:25:04.660 --> 00:25:08.710
well in a place where
she's facing resistance,

00:25:08.710 --> 00:25:11.620
is already stressed
out, and has to compete

00:25:11.620 --> 00:25:14.860
with some young man
in a t-shirt and jeans

00:25:14.860 --> 00:25:18.460
without family responsibilities
who brings a dog to work

00:25:18.460 --> 00:25:21.130
and you don't have to
walk the dog at home.

00:25:21.130 --> 00:25:25.710
And you compete with that--
anybody, male or female.

00:25:25.710 --> 00:25:30.530
If you have to play in two
worlds and do them both well,

00:25:30.530 --> 00:25:33.620
I guarantee you're
going to be uneasy.

00:25:33.620 --> 00:25:38.680
That is, you will display
a lot of neuroticism.

00:25:38.680 --> 00:25:40.690
To me that is the
important point

00:25:40.690 --> 00:25:43.360
that's missing in all
of this discussion.

00:25:43.360 --> 00:25:46.060
Why are women more
stressed out, if they are.

00:25:46.060 --> 00:25:48.110
Now, I question that.

00:25:48.110 --> 00:25:51.260
First of all, this is
based on a single study

00:25:51.260 --> 00:25:57.870
and that conclusion is
grasped at to prove a point.

00:25:57.870 --> 00:26:00.450
And that is the
issue, is sexism,

00:26:00.450 --> 00:26:04.830
precisely is sexism because
of the dual roles women

00:26:04.830 --> 00:26:05.510
have to play.

00:26:08.390 --> 00:26:12.260
Shall I say more or is there
another question we have there?

00:26:12.260 --> 00:26:13.010
DAVID BARRY: Yeah.

00:26:13.010 --> 00:26:14.551
There's a few more
questions and then

00:26:14.551 --> 00:26:16.415
we're happy to take
some audience questions.

00:26:16.415 --> 00:26:18.290
ELLEN ULLMAN: I do have--
actually, I realize

00:26:18.290 --> 00:26:21.120
I forgot one more thing to say.

00:26:21.120 --> 00:26:25.340
The memo says very often
that these efforts are not

00:26:25.340 --> 00:26:26.810
good for business.

00:26:26.810 --> 00:26:30.440
Why spend all this time trying
to welcome in these people who

00:26:30.440 --> 00:26:33.150
have not participated
because it doesn't work

00:26:33.150 --> 00:26:36.770
and we're wasting the finite
assets of Google, which really

00:26:36.770 --> 00:26:37.970
cracked me up.

00:26:37.970 --> 00:26:42.740
I mean, it's finite but
Google has vast resources

00:26:42.740 --> 00:26:48.080
that it is spending on a variety
of projects and the efforts

00:26:48.080 --> 00:26:52.650
to bring other people in is a
very small part of that budget.

00:26:52.650 --> 00:26:56.100
So that was a
bizarre thing to say,

00:26:56.100 --> 00:26:58.430
but it is good for business.

00:26:58.430 --> 00:27:03.320
When you leave out the possible
contributions, the creativity

00:27:03.320 --> 00:27:08.390
of whole classes of people, you
don't know what you're missing.

00:27:08.390 --> 00:27:11.410
You're losing talent
in the industry.

00:27:11.410 --> 00:27:18.050
And we need fresh values
inside the coding world.

00:27:18.050 --> 00:27:23.450
It's a segregated world, mostly
men and Asians, very few women,

00:27:23.450 --> 00:27:28.490
very few African-Americans
and Hispanics.

00:27:28.490 --> 00:27:30.920
And so inside this
culture, there--

00:27:30.920 --> 00:27:32.700
I'll give you an example.

00:27:32.700 --> 00:27:35.060
I went to one of
those pitch meetings

00:27:35.060 --> 00:27:39.800
where would-be CEOs of start-ups
go and they give their pitch.

00:27:39.800 --> 00:27:41.750
And I met a young
man who told me

00:27:41.750 --> 00:27:44.120
about his app that
was going to be

00:27:44.120 --> 00:27:47.270
used to screen resumes
for a corporation

00:27:47.270 --> 00:27:50.560
to find good cultural fit.

00:27:50.560 --> 00:27:52.930
So I said to him, well,
good cultural fit is just

00:27:52.930 --> 00:27:56.320
a byword for keeping up
the segregated culture.

00:27:56.320 --> 00:27:59.260
In other words, you want to hire
people like the ones you have,

00:27:59.260 --> 00:28:03.100
who fit in, the people who are
there won't feel uncomfortable

00:28:03.100 --> 00:28:06.670
and you're perpetuating
this segregated society

00:28:06.670 --> 00:28:08.740
inside the coding world.

00:28:08.740 --> 00:28:11.140
And he listened to me patiently
and then he said, well,

00:28:11.140 --> 00:28:14.260
all that might be true but
I'm working for the company,

00:28:14.260 --> 00:28:16.540
not for society.

00:28:16.540 --> 00:28:20.600
And to me that incorporates
the whole problem.

00:28:20.600 --> 00:28:23.750
Now, you can't have
them both overlap,

00:28:23.750 --> 00:28:27.530
but let's do a Venn diagram
where they move closer together

00:28:27.530 --> 00:28:28.310
and overlap.

00:28:28.310 --> 00:28:31.700
How much can we have that
overlap between what's

00:28:31.700 --> 00:28:34.370
good for business, what's
good for computing,

00:28:34.370 --> 00:28:36.260
and what's good for society?

00:28:36.260 --> 00:28:39.590
I really question, as we talked
about earlier, the effect

00:28:39.590 --> 00:28:44.690
of banishing experts
and the enormous effect

00:28:44.690 --> 00:28:47.120
this has had on the
culture globally,

00:28:47.120 --> 00:28:49.210
actually, at this point.

00:28:49.210 --> 00:28:53.840
So we need new people
to ask new questions.

00:28:53.840 --> 00:28:57.920
One borough of New York
City is about to examine

00:28:57.920 --> 00:29:00.980
all the algorithms that it
uses to make decisions which

00:29:00.980 --> 00:29:06.080
go on who gets to go to
what school, what police

00:29:06.080 --> 00:29:07.820
schedules are in
different neighborhoods,

00:29:07.820 --> 00:29:11.540
even garbage pickup schedules
in various neighborhoods.

00:29:11.540 --> 00:29:14.060
And so one councilman
is saying, I

00:29:14.060 --> 00:29:15.980
want to know the bias in here.

00:29:15.980 --> 00:29:19.070
I want to know how these
decisions are being made.

00:29:19.070 --> 00:29:24.630
And algorithms hide bias, and
especially algorithms that are

00:29:24.630 --> 00:29:27.040
written by other algorithms.

00:29:27.040 --> 00:29:32.040
And there is one danger
that I just want to mention.

00:29:32.040 --> 00:29:36.540
Machine learning
is the topic could,

00:29:36.540 --> 00:29:42.320
the work right now in
artificial intelligence.

00:29:42.320 --> 00:29:44.069
Computer scientists
and engineers

00:29:44.069 --> 00:29:45.860
who are working in this
field, some of them

00:29:45.860 --> 00:29:50.060
express the fear or reservation
that once code writes

00:29:50.060 --> 00:29:53.240
code writes code,
the original motive

00:29:53.240 --> 00:29:58.850
of the creators of that
system lose control over it

00:29:58.850 --> 00:30:01.850
and they don't actually
know any more what

00:30:01.850 --> 00:30:03.560
the algorithms are doing.

00:30:03.560 --> 00:30:09.200
It races away from their
understanding, it gets too big.

00:30:09.200 --> 00:30:13.720
And to actually know, well,
what is this thing doing?

00:30:13.720 --> 00:30:16.090
Is it doing the work
I thought it would?

00:30:16.090 --> 00:30:19.030
Is it making good
decisions or not?

00:30:19.030 --> 00:30:23.950
And so another danger of
leaving out the human equation

00:30:23.950 --> 00:30:26.245
in algorithms in computing.

00:30:28.860 --> 00:30:30.870
I want to add one
thing about why

00:30:30.870 --> 00:30:34.440
is it important to bring
people who've been excluded in.

00:30:34.440 --> 00:30:36.720
I said new values,
it's good for business.

00:30:36.720 --> 00:30:44.010
Now, look, Uber loses $645
million in a single quarter

00:30:44.010 --> 00:30:47.420
and does anyone say, well
that's bad for business?

00:30:47.420 --> 00:30:47.970
No.

00:30:47.970 --> 00:30:53.160
They sustain losses, as
did Amazon for many years,

00:30:53.160 --> 00:30:55.990
to grow their business.

00:30:55.990 --> 00:30:59.070
So it is possible to
think of these ideas

00:30:59.070 --> 00:31:02.640
immediately that are new
ideas and not ask right away,

00:31:02.640 --> 00:31:04.170
will this make me money?

00:31:04.170 --> 00:31:06.660
The question is, will it
expand the community I serve,

00:31:06.660 --> 00:31:09.250
will I get more customers?

00:31:09.250 --> 00:31:10.140
And it's worth a try.

00:31:10.140 --> 00:31:13.080
If you lose money, you
just keep trying it.

00:31:13.080 --> 00:31:17.490
The idea that everything
has to make money

00:31:17.490 --> 00:31:19.590
immediately is is a fallacy.

00:31:19.590 --> 00:31:22.050
We've seen it all
through the industry.

00:31:22.050 --> 00:31:24.600
And one more thing
I want to bring up,

00:31:24.600 --> 00:31:27.590
I think we have a
few minutes, is age.

00:31:30.710 --> 00:31:32.780
I want to talk about
the value of the past

00:31:32.780 --> 00:31:39.590
and what engineers who
are beyond 40, 50 even 60

00:31:39.590 --> 00:31:44.330
especially have to contribute
and who have been excluded.

00:31:44.330 --> 00:31:48.680
Computing, software engineering
is a young person's profession.

00:31:48.680 --> 00:31:50.960
And I think if you
walk around here,

00:31:50.960 --> 00:31:53.720
I counted more
dogs in my sample--

00:31:53.720 --> 00:31:55.260
I was a half hour early--

00:31:55.260 --> 00:31:57.880
than I did people over 40.

00:31:57.880 --> 00:32:01.450
That's necessarily
just anecdotal.

00:32:01.450 --> 00:32:06.740
But you can ask, what did
other people try before?

00:32:06.740 --> 00:32:11.330
I mean, the web page is
horribly like an RPG,

00:32:11.330 --> 00:32:18.710
remote programming generator,
in a screen for IBM computer

00:32:18.710 --> 00:32:22.040
mainframes in which you
filled out a whole form

00:32:22.040 --> 00:32:23.270
and then you hit Send--

00:32:23.270 --> 00:32:26.390
which is Enter, which
should be called Submit--

00:32:26.390 --> 00:32:28.250
and it comes back with an error.

00:32:28.250 --> 00:32:30.740
And you fix that
error, Send, there's

00:32:30.740 --> 00:32:32.880
another error, fix
it, another error.

00:32:32.880 --> 00:32:35.644
Now, does that sound
like a web page to you?

00:32:35.644 --> 00:32:36.590
Yes.

00:32:36.590 --> 00:32:40.900
It doesn't go in
and check as you go.

00:32:40.900 --> 00:32:43.490
It's avoiding trips
to the server.

00:32:43.490 --> 00:32:46.010
I understand it from
an engineering context

00:32:46.010 --> 00:32:47.870
why that would be true.

00:32:47.870 --> 00:32:49.670
But then of course,
look at all the chatter

00:32:49.670 --> 00:32:53.840
to the server with one error,
another error, another error.

00:32:53.840 --> 00:32:58.550
So we spent all this time
in client server computing

00:32:58.550 --> 00:33:03.380
to bring a client
interface that was richer,

00:33:03.380 --> 00:33:06.440
that we could correct errors,
that we could answer questions

00:33:06.440 --> 00:33:10.790
locally and minimize
interactions with the server

00:33:10.790 --> 00:33:12.830
and do it as we needed.

00:33:12.830 --> 00:33:14.840
I think we're way
overbalance now.

00:33:14.840 --> 00:33:16.670
If you look at the
history of this,

00:33:16.670 --> 00:33:19.750
we're almost back
to the RPG terminal.

00:33:19.750 --> 00:33:24.020
We are way over balanced in need
of the server versus the human

00:33:24.020 --> 00:33:24.780
being.

00:33:24.780 --> 00:33:27.710
And so this is one of these
lessons of the past that

00:33:27.710 --> 00:33:30.530
needs to be looked at.

00:33:30.530 --> 00:33:35.450
I also-- the idea of
the human curator.

00:33:35.450 --> 00:33:37.640
Now, even Google faced this.

00:33:37.640 --> 00:33:40.220
The idea was this would
be all algorithmic

00:33:40.220 --> 00:33:42.320
and then if you
looked for Jew, you

00:33:42.320 --> 00:33:46.190
got Jew Watch, which was a
horribly anti-Semitic site,

00:33:46.190 --> 00:33:47.840
it became a pop one.

00:33:47.840 --> 00:33:51.260
And Google actually intervened
at the top of this and said,

00:33:51.260 --> 00:33:55.070
we recommend you look for
Jewish or Jewish person

00:33:55.070 --> 00:33:56.810
because you'll get
a different result.

00:33:56.810 --> 00:33:59.270
And that was a
startling interaction

00:33:59.270 --> 00:34:02.720
from this company
that was founded

00:34:02.720 --> 00:34:07.310
on algorithmic selection.

00:34:07.310 --> 00:34:13.480
And we saw what happened to
Facebook, the trending stories.

00:34:13.480 --> 00:34:15.380
There were human
curators of that.

00:34:15.380 --> 00:34:19.699
And the right said, oh,
they're cherry-picking,

00:34:19.699 --> 00:34:21.340
they're trending left.

00:34:21.340 --> 00:34:27.050
In reaction, Facebook fired
all of those curators,

00:34:27.050 --> 00:34:32.239
those journalists, those
people looking at the stories

00:34:32.239 --> 00:34:34.639
and replaced them
with algorithms, which

00:34:34.639 --> 00:34:36.980
instantly brought on fake news.

00:34:36.980 --> 00:34:41.556
That is the origin of
contemporary fake news.

00:34:41.556 --> 00:34:43.139
They were looking
at what is trending.

00:34:43.139 --> 00:34:48.199
They saw this tremendous traffic
about some rumor somewhere

00:34:48.199 --> 00:34:50.300
that could be perpetuated
among people who

00:34:50.300 --> 00:34:53.750
wanted to believe that story.

00:34:53.750 --> 00:34:57.710
And it no longer was coming
up with what was trending,

00:34:57.710 --> 00:35:01.670
it was coming up with
false information.

00:35:01.670 --> 00:35:04.370
Well, what was trending
was false information.

00:35:04.370 --> 00:35:08.730
And then Facebook
brought in human curators

00:35:08.730 --> 00:35:12.740
again to jury this.

00:35:12.740 --> 00:35:17.960
And so we need the human
being in these stories.

00:35:17.960 --> 00:35:20.840
If we completely eliminate
the human being, what

00:35:20.840 --> 00:35:24.380
we have to offer, our
ability to discriminate,

00:35:24.380 --> 00:35:27.290
our subtle abilities
to discriminate--

00:35:27.290 --> 00:35:30.050
and intuitively, it's
built into us in evolution.

00:35:30.050 --> 00:35:32.510
This is not just some
airy fairy thing.

00:35:32.510 --> 00:35:36.830
We survived as a species
having these intuitions.

00:35:36.830 --> 00:35:41.700
And what I mean by that is
ability to quickly notice,

00:35:41.700 --> 00:35:43.500
thinking fast and slow.

00:35:43.500 --> 00:35:46.090
You may know Daniel
Kahneman's work on this.

00:35:46.090 --> 00:35:52.360
This is part of what we
have, and to eliminate

00:35:52.360 --> 00:35:57.470
this deep store of
abilities is to really

00:35:57.470 --> 00:36:02.800
to rob computing of richness.

00:36:05.860 --> 00:36:07.930
DAVID BARRY: So you
encourage the general public

00:36:07.930 --> 00:36:10.390
to learn to code and you
envision the creation

00:36:10.390 --> 00:36:13.330
of an army of coders.

00:36:13.330 --> 00:36:15.460
Why?

00:36:15.460 --> 00:36:18.580
ELLEN ULLMAN: Well, first
of all to demystify code.

00:36:18.580 --> 00:36:20.590
I mean, we're surrounded
by all this stuff

00:36:20.590 --> 00:36:23.110
and in the developed world,
you can't get out of it.

00:36:23.110 --> 00:36:26.620
Even in undeveloped countries,
they are wrapped in this.

00:36:26.620 --> 00:36:33.160
Also the allocation of resources
to them, the study of crops,

00:36:33.160 --> 00:36:34.900
decisions on trade.

00:36:34.900 --> 00:36:39.050
Essentially, except for the
desperate people of the world--

00:36:39.050 --> 00:36:44.460
and that is another out of
the bounds of this talk--

00:36:44.460 --> 00:36:48.470
we're wrapped up in this
stuff and we can't escape it.

00:36:48.470 --> 00:36:49.880
I can't live without my phone.

00:36:49.880 --> 00:36:50.510
Can you?

00:36:50.510 --> 00:36:52.040
I can't live
without my computer.

00:36:52.040 --> 00:36:53.390
Can you?

00:36:53.390 --> 00:36:55.480
If you try, you have to go--

00:36:55.480 --> 00:36:57.020
god, how far do
you have to go away

00:36:57.020 --> 00:36:59.090
to where you won't get a signal?

00:37:02.450 --> 00:37:06.660
So we need to demystify this.

00:37:06.660 --> 00:37:10.460
People need to know this is
code, it was written by people,

00:37:10.460 --> 00:37:13.100
and uh-oh, if there wasn't
too much machine learning,

00:37:13.100 --> 00:37:15.640
it can be changed by people.

00:37:15.640 --> 00:37:18.920
Therefore, maybe you
want to look into this.

00:37:18.920 --> 00:37:21.830
And I don't mean
everyone should code,

00:37:21.830 --> 00:37:25.316
not at all, because it's takes
a very special kind of person.

00:37:25.316 --> 00:37:27.440
You have to have a very
high tolerance for failure,

00:37:27.440 --> 00:37:28.820
for instance--

00:37:28.820 --> 00:37:31.040
bug after bug after bug.

00:37:31.040 --> 00:37:34.940
If that's going to make you
nuts, then forget about it.

00:37:34.940 --> 00:37:37.680
And have some intrigue
and passion in this.

00:37:37.680 --> 00:37:40.970
So I encourage people to try to
learn in some form or fashion

00:37:40.970 --> 00:37:44.540
to see if they have
that passion for it,

00:37:44.540 --> 00:37:47.400
to find in these
rounds of failure

00:37:47.400 --> 00:37:51.260
or some sense of
intrigue, that it's hard

00:37:51.260 --> 00:37:52.940
but it's the good hard.

00:37:52.940 --> 00:37:57.190
And that will welcome
in, I'm hoping,

00:37:57.190 --> 00:38:02.560
people who have been excluded
from the computing culture.

00:38:02.560 --> 00:38:04.420
A lot depends on education.

00:38:04.420 --> 00:38:07.870
I mean, our society has
de-funded, taken money

00:38:07.870 --> 00:38:09.730
away from public schools.

00:38:09.730 --> 00:38:14.500
And I don't know how people
will achieve this knowledge.

00:38:14.500 --> 00:38:18.880
And so I bring it up so that
we will perhaps as a society

00:38:18.880 --> 00:38:23.410
press to make more widespread
knowledge of coding.

00:38:23.410 --> 00:38:29.410
Now, I do not mean to raise
computing and learning

00:38:29.410 --> 00:38:33.850
programming to the
level of basic literacy.

00:38:33.850 --> 00:38:36.730
I think that people who
are basically literate

00:38:36.730 --> 00:38:40.100
should invade the coding world.

00:38:40.100 --> 00:38:43.630
And actually, as I know it,
Google and other technology

00:38:43.630 --> 00:38:46.900
companies are suddenly wanting
to hire philosophers and people

00:38:46.900 --> 00:38:49.270
who study in the
humanities, again

00:38:49.270 --> 00:38:52.120
to say, well, what are we
missing in history, what

00:38:52.120 --> 00:38:55.900
are we missing in what human
beings have learned deeply

00:38:55.900 --> 00:38:57.790
about one another?

00:38:57.790 --> 00:39:02.140
Novelists know things about
the interior lives of people,

00:39:02.140 --> 00:39:03.760
if it's a good novel.

00:39:03.760 --> 00:39:08.020
So I can see that this
trend is already underway,

00:39:08.020 --> 00:39:12.130
so I'm hoping to bring in
more people in the humanities

00:39:12.130 --> 00:39:17.530
and bring in a larger
number of people period who

00:39:17.530 --> 00:39:18.720
know how to write code.

00:39:18.720 --> 00:39:21.250
I mean, we live in a
world surrounded by code

00:39:21.250 --> 00:39:25.120
and a tiny percentage
of people on Earth

00:39:25.120 --> 00:39:28.270
have any idea what a
program is, and that clearly

00:39:28.270 --> 00:39:31.404
has to be an expanded group.

00:39:31.404 --> 00:39:33.070
DAVID BARRY: What can
we do as a society

00:39:33.070 --> 00:39:35.200
to foster coding education?

00:39:35.200 --> 00:39:38.560
Do you think it should
be part of the curriculum

00:39:38.560 --> 00:39:40.947
in public schools?

00:39:40.947 --> 00:39:43.030
ELLEN ULLMAN: Not at the
expense of anything else,

00:39:43.030 --> 00:39:43.990
I'll say that.

00:39:43.990 --> 00:39:47.320
First of all,
reading out of books.

00:39:47.320 --> 00:39:51.280
All the studies
show that one learns

00:39:51.280 --> 00:39:56.170
very differently and more
deeply turning pages.

00:39:56.170 --> 00:40:00.250
Kids who learn from books
retain not only the story,

00:40:00.250 --> 00:40:05.740
but associations, as opposed to
on the screen, which literally

00:40:05.740 --> 00:40:08.260
is a bounded experience.

00:40:08.260 --> 00:40:12.020
So I'm saying in addition.

00:40:12.020 --> 00:40:16.330
I'm saying the way
you have to have PE

00:40:16.330 --> 00:40:21.250
because it's important for the
body, the way you need recess

00:40:21.250 --> 00:40:22.930
because it's important
for the mind.

00:40:22.930 --> 00:40:26.200
Oh, by the way, programmers
get out from under your desks

00:40:26.200 --> 00:40:28.660
and go home because
it's important to have

00:40:28.660 --> 00:40:32.290
other experiences so you
can find those background

00:40:32.290 --> 00:40:36.910
regions of your mind where
good ideas come from.

00:40:36.910 --> 00:40:38.620
Yes, it should be
available, but I

00:40:38.620 --> 00:40:42.830
have to stress to
all social classes.

00:40:42.830 --> 00:40:46.700
This cannot just be a privileged
education because then

00:40:46.700 --> 00:40:48.940
we're just perpetuating
this culture.

00:40:51.194 --> 00:40:51.860
DAVID BARRY: OK.

00:40:51.860 --> 00:40:52.359
Well, great.

00:40:52.359 --> 00:40:55.610
Well, I think we'll take some
questions from the audience.

00:40:55.610 --> 00:40:59.580
AUDIENCE: You also talked
about the memo and all of that.

00:40:59.580 --> 00:41:06.830
So the way I see it, a lot
of times the whole point

00:41:06.830 --> 00:41:10.760
if it's raised in a
certain work environment,

00:41:10.760 --> 00:41:13.330
it's overcorrected and
then shoved under the rug.

00:41:13.330 --> 00:41:16.720
That's how people fix it.

00:41:16.720 --> 00:41:19.180
The way I see it
is you have a voice

00:41:19.180 --> 00:41:21.520
at the moment in the
technology world.

00:41:21.520 --> 00:41:24.060
So have you considered--

00:41:24.060 --> 00:41:27.050
and yes, you're sick
of it, but so am I

00:41:27.050 --> 00:41:29.770
and I'm just starting to work.

00:41:29.770 --> 00:41:33.700
But have you considered
consulting, as you said,

00:41:33.700 --> 00:41:37.450
CEOs in big companies are
looking at humanities now

00:41:37.450 --> 00:41:41.440
but they weren't before and
you predicted that they should.

00:41:41.440 --> 00:41:46.100
So you have insight
into that world.

00:41:46.100 --> 00:41:49.280
Have you considered consulting
with CEOs of companies?

00:41:49.280 --> 00:41:53.720
Or you mentioned that you should
introduce coding in schools so

00:41:53.720 --> 00:41:57.050
have you considered working
with local politicians

00:41:57.050 --> 00:42:00.520
to somehow make a
bill out of that?

00:42:00.520 --> 00:42:02.000
I'm just curious.

00:42:02.000 --> 00:42:05.810
ELLEN ULLMAN: That's very
interesting because my editor,

00:42:05.810 --> 00:42:07.520
my friends, there
they're asking,

00:42:07.520 --> 00:42:09.290
OK, well, you've got this book.

00:42:09.290 --> 00:42:11.090
What do you want to do next?

00:42:11.090 --> 00:42:12.590
And I was saying,
well, I don't want

00:42:12.590 --> 00:42:14.006
to know what I'm
going to do next,

00:42:14.006 --> 00:42:16.280
I want to see what
happens this morning.

00:42:16.280 --> 00:42:22.340
I was reading the "Time" and
there was a story about Google

00:42:22.340 --> 00:42:29.610
and that one of the employees
of a think tank or research

00:42:29.610 --> 00:42:31.680
tank funded by
Google had been fired

00:42:31.680 --> 00:42:35.130
because he was questioning
some of Google's practices.

00:42:35.130 --> 00:42:38.130
And there was a quote from--

00:42:38.130 --> 00:42:39.980
I'm really terrible
with names, I

00:42:39.980 --> 00:42:43.320
can tell you strings of numbers
but I'm very bad with names.

00:42:43.320 --> 00:42:44.531
Who's the head of Epic?

00:42:44.531 --> 00:42:45.030
All right.

00:42:45.030 --> 00:42:48.690
This is an electronic monitor.

00:42:48.690 --> 00:42:50.600
You may know who.

00:42:50.600 --> 00:42:54.370
And I thought, maybe I
should go work for them.

00:42:54.370 --> 00:43:01.360
Maybe I should gather a group
of men and women, some of whom

00:43:01.360 --> 00:43:04.330
have worked in Silicon
Valley and programming,

00:43:04.330 --> 00:43:05.830
who have some funds--

00:43:05.830 --> 00:43:10.330
not millionaires, billionaires--
and form our own research

00:43:10.330 --> 00:43:16.300
company or our own political
group to advise politicians.

00:43:16.300 --> 00:43:19.870
I want to say there has
been this especially

00:43:19.870 --> 00:43:23.410
involvement with government
because there has been

00:43:23.410 --> 00:43:28.750
this feeling inside
the technical world,

00:43:28.750 --> 00:43:30.820
it's mostly been libertarian.

00:43:30.820 --> 00:43:35.050
Government is kind of the
enemy, regulations are anathema.

00:43:35.050 --> 00:43:41.560
And yet, if companies could
partner with governments,

00:43:41.560 --> 00:43:46.820
that would be a very strong
way to have the beneficial--

00:43:46.820 --> 00:43:48.520
well, maybe and maybe not.

00:43:48.520 --> 00:43:51.250
Maybe they'll partner
with some politicians

00:43:51.250 --> 00:43:52.970
who are only on their side.

00:43:52.970 --> 00:43:56.580
But yes, I have considered it.

00:43:56.580 --> 00:43:57.240
I'm not sure.

00:44:00.140 --> 00:44:02.660
I don't think of myself
as having a voice.

00:44:02.660 --> 00:44:05.600
I think of this person who
stumbled into computing

00:44:05.600 --> 00:44:08.660
because I thought it'd be fun
and all the people involved

00:44:08.660 --> 00:44:11.120
were really just
crazy wonderful people

00:44:11.120 --> 00:44:14.840
and the kind of people I
liked and had met and worked

00:44:14.840 --> 00:44:19.160
with in college and afterward.

00:44:19.160 --> 00:44:26.460
But I have been around a
while from the early days

00:44:26.460 --> 00:44:31.670
and I have encountered a lot
of what other women are facing

00:44:31.670 --> 00:44:32.990
and minorities are facing.

00:44:35.720 --> 00:44:38.430
I suppose it is a role that
I have, like it or not,

00:44:38.430 --> 00:44:40.307
and maybe I should
learn to like it.

00:44:40.307 --> 00:44:42.140
AUDIENCE: When people
talk about technology,

00:44:42.140 --> 00:44:43.970
often they talk
about the future.

00:44:43.970 --> 00:44:45.740
And there's a lot
of gloom and doom

00:44:45.740 --> 00:44:48.482
nowadays for a lot of reasons.

00:44:48.482 --> 00:44:50.190
It's seems to me it
used to be the future

00:44:50.190 --> 00:44:51.870
was a very happy place.

00:44:51.870 --> 00:44:55.220
And maybe you could
say a little about how

00:44:55.220 --> 00:44:57.080
you started quite a
while ago in this career

00:44:57.080 --> 00:44:59.400
and people talked about
the future back then, too.

00:44:59.400 --> 00:45:03.930
How has your sense of the
future changed along the way

00:45:03.930 --> 00:45:07.700
and do you think that
everyone's sense of the future

00:45:07.700 --> 00:45:11.540
says something about themselves?

00:45:11.540 --> 00:45:13.400
ELLEN ULLMAN: That's
an interesting point

00:45:13.400 --> 00:45:15.590
in and of itself.

00:45:15.590 --> 00:45:21.620
What each individual imagines is
revealing about something deep

00:45:21.620 --> 00:45:23.720
inside a person.

00:45:23.720 --> 00:45:28.489
The future, I've never thought
of myself as someone who says,

00:45:28.489 --> 00:45:29.780
this is what's going to happen.

00:45:29.780 --> 00:45:31.910
The fact that some of
these things happen really

00:45:31.910 --> 00:45:33.050
disturbs me.

00:45:33.050 --> 00:45:35.600
I'm really sorry to
be right about things

00:45:35.600 --> 00:45:39.140
like disintermediation and
the unraveling of truths

00:45:39.140 --> 00:45:44.491
and the sexism and the exclusion
of the insider engineering

00:45:44.491 --> 00:45:44.990
world.

00:45:48.490 --> 00:45:51.250
For the future, I am
expecting the next generations

00:45:51.250 --> 00:45:53.560
to take this up.

00:45:53.560 --> 00:45:57.310
I feel that people of my
generation and prior ones

00:45:57.310 --> 00:46:01.900
have a lot of
lessons to pass on.

00:46:01.900 --> 00:46:03.660
I mean, people
who work at Google

00:46:03.660 --> 00:46:07.150
may think that they're
creating something brand new,

00:46:07.150 --> 00:46:09.430
but they stand on the
shoulders of giants.

00:46:09.430 --> 00:46:12.100
Algorithms that are
being worked on here

00:46:12.100 --> 00:46:15.590
were written back in
the '40s and '50s.

00:46:15.590 --> 00:46:20.690
So what we're doing
here is not new.

00:46:20.690 --> 00:46:23.960
What future they
are creating, you

00:46:23.960 --> 00:46:26.750
are creating can be changed.

00:46:26.750 --> 00:46:33.050
It is not this idea that
the future just happens.

00:46:33.050 --> 00:46:36.590
Kevin Kelly, he's someone
I deeply disagree with--

00:46:36.590 --> 00:46:40.370
I respect him-- wrote a book
called, "What Does Technology

00:46:40.370 --> 00:46:44.500
Want?," as if
technology had a motive,

00:46:44.500 --> 00:46:47.390
that there was something
inside technology that would

00:46:47.390 --> 00:46:49.260
create a world on its own.

00:46:49.260 --> 00:46:50.300
No.

00:46:50.300 --> 00:46:53.480
What creates technology
is human desire.

00:46:53.480 --> 00:46:57.290
It's what you want, what
you envision for the future.

00:46:57.290 --> 00:47:00.020
And that's where I say,
it's up to new generations

00:47:00.020 --> 00:47:01.220
to take this up.

00:47:01.220 --> 00:47:03.590
They have to speak
up for their world.

00:47:03.590 --> 00:47:07.960
I mean, as I looked around what
I lived in and just tried to be

00:47:07.960 --> 00:47:10.330
clear-eyed and balanced, I--

00:47:10.330 --> 00:47:12.340
I'm sorry that some
of the things in here

00:47:12.340 --> 00:47:15.370
don't reveal the excitement
I feel about technology,

00:47:15.370 --> 00:47:21.490
my love of it and my
skepticism and disappointment.

00:47:21.490 --> 00:47:27.750
So I think this generation needs
to sit down and turn their cold

00:47:27.750 --> 00:47:30.180
eye on it, not just have this
wonder about the future--

00:47:30.180 --> 00:47:32.640
and there will be
wondrous things coming--

00:47:32.640 --> 00:47:36.070
but also sit back and go,
oh, what does this mean?

00:47:36.070 --> 00:47:39.330
What changes are my
engendering in the world?

00:47:39.330 --> 00:47:43.260
What should I question
and what should I love?

00:47:43.260 --> 00:47:44.970
They are the future,
they're creating it.

00:47:44.970 --> 00:47:47.950
So that the future doesn't
just happen, people make it.

00:47:50.619 --> 00:47:51.910
Does that answer your question?

00:47:51.910 --> 00:47:52.840
AUDIENCE: Sort of.

00:47:52.840 --> 00:47:53.290
ELLEN ULLMAN: Sort of.

00:47:53.290 --> 00:47:53.789
OK.

00:47:53.789 --> 00:47:55.291
Go back and ask it again.

00:47:55.291 --> 00:47:57.040
AUDIENCE: A different
question if it's OK.

00:47:57.040 --> 00:47:58.190
It's about religion.

00:47:58.190 --> 00:47:59.481
I don't know if you give much--

00:47:59.481 --> 00:48:01.810
I have not read your book
yet, I will read it--

00:48:01.810 --> 00:48:03.601
so I don't know if you
comment on religion.

00:48:03.601 --> 00:48:06.670
Among the many experts who are
being disintermediated lately

00:48:06.670 --> 00:48:09.000
are religious experts.

00:48:09.000 --> 00:48:11.630
And religious people
don't just talk about God,

00:48:11.630 --> 00:48:13.280
they also talk about
right and wrong.

00:48:13.280 --> 00:48:16.150
They're among the many
people who will explain

00:48:16.150 --> 00:48:18.140
to you why lying is bad.

00:48:18.140 --> 00:48:20.680
But these sorts
of experts are not

00:48:20.680 --> 00:48:22.870
being listened to as they were.

00:48:22.870 --> 00:48:26.350
Do you imagine that this has
any impact on what we see now?

00:48:30.086 --> 00:48:32.210
ELLEN ULLMAN: Well, it's
not only religious people.

00:48:32.210 --> 00:48:37.010
There are people of good will
and good faith in human beings

00:48:37.010 --> 00:48:39.680
who bring up these issues.

00:48:39.680 --> 00:48:45.260
My questions are moral and
we do miss those voices.

00:48:45.260 --> 00:48:50.690
Matter of fact, people who
lie say they're against liars.

00:48:50.690 --> 00:48:51.530
OK?

00:48:51.530 --> 00:48:54.710
Donald Trump lies and
he's against fake news.

00:48:54.710 --> 00:48:58.220
So yes, we are missing those
voices who are authoritative.

00:48:58.220 --> 00:49:00.050
They know what
they're doing, right?

00:49:00.050 --> 00:49:01.670
And say, no, a lie is a lie.

00:49:04.600 --> 00:49:07.930
Mistreating someone is wrong.

00:49:07.930 --> 00:49:11.490
There are injustices
that we need to right.

00:49:11.490 --> 00:49:13.920
Yes, they've been
left out, certainly.

00:49:13.920 --> 00:49:18.820
And unfortunately, some
of those religious voices

00:49:18.820 --> 00:49:24.230
are extremely right-wing.

00:49:24.230 --> 00:49:28.345
And so it's complicated.

00:49:31.147 --> 00:49:33.744
Does that answer your question?

00:49:33.744 --> 00:49:35.160
AUDIENCE: Can you
say a little bit

00:49:35.160 --> 00:49:38.490
about your experience
working on the audio books?

00:49:38.490 --> 00:49:43.980
Did you get any guidance as far
as acting sort of direction?

00:49:43.980 --> 00:49:45.750
ELLEN ULLMAN: No.

00:49:45.750 --> 00:49:48.180
My publisher said, well--

00:49:48.180 --> 00:49:52.290
actually Macmillan FSG is a
wonderful literary imprint

00:49:52.290 --> 00:49:54.750
within this huge
corporation called Macmillan

00:49:54.750 --> 00:49:56.550
and they do the audio books.

00:49:56.550 --> 00:49:58.320
And they said, well,
we'd really like

00:49:58.320 --> 00:50:01.250
you to do it because when
we have nonfiction books,

00:50:01.250 --> 00:50:04.320
we like the author to
read it, especially

00:50:04.320 --> 00:50:08.750
since a lot of your book, most
of it is in the first person.

00:50:08.750 --> 00:50:15.950
So I heard it would be grueling,
but I was encouraged to do it.

00:50:15.950 --> 00:50:18.290
So I did it.

00:50:18.290 --> 00:50:20.000
No, I just walked in cold.

00:50:20.000 --> 00:50:22.100
I mean, I've had some
public speaking experience.

00:50:22.100 --> 00:50:25.880
And actually, singing
is what turned out

00:50:25.880 --> 00:50:30.620
to helped me the most, breath
control over periods of time.

00:50:30.620 --> 00:50:35.090
But no, I went in this booth,
I put on some headphones,

00:50:35.090 --> 00:50:36.830
there was the book
in front of me--

00:50:36.830 --> 00:50:41.090
actually, it was on a
iPad, no pages noise--

00:50:41.090 --> 00:50:43.010
and I just started
from page one.

00:50:43.010 --> 00:50:48.380
And we did 100 pages a
day to finish the book.

00:50:48.380 --> 00:50:50.390
It was a grueling experience.

00:50:53.370 --> 00:50:56.970
Luckily, you don't have to read
through like you're on a stage.

00:50:56.970 --> 00:50:59.810
You can just back up at
any punctuation point

00:50:59.810 --> 00:51:01.390
and go forward.

00:51:01.390 --> 00:51:03.860
But it was quite an experience.

00:51:03.860 --> 00:51:06.090
And people who drive--

00:51:06.090 --> 00:51:09.830
there were many people who just
read audio books while they

00:51:09.830 --> 00:51:12.390
commute, so I'm hoping
that some of those people

00:51:12.390 --> 00:51:14.179
will pick it up.

00:51:14.179 --> 00:51:16.470
DAVID BARRY: I think you're
kind of unique in the sense

00:51:16.470 --> 00:51:19.740
that you're a very advanced
programmer and coder

00:51:19.740 --> 00:51:21.480
and then you've
gone on to become

00:51:21.480 --> 00:51:24.280
a really successful writer.

00:51:24.280 --> 00:51:27.090
So writing in code is a
certain type of language

00:51:27.090 --> 00:51:30.270
and then writing in English in
the literary way that you do

00:51:30.270 --> 00:51:32.490
is also another
type of language.

00:51:32.490 --> 00:51:37.950
So could you talk maybe about
the parallels between those two

00:51:37.950 --> 00:51:41.300
or if there's any synergies and
what maybe learning to write

00:51:41.300 --> 00:51:42.510
in code has--

00:51:42.510 --> 00:51:45.150
if it has at all influenced
the way that you write

00:51:45.150 --> 00:51:47.120
or the way that you
think about writing?

00:51:47.120 --> 00:51:50.290
ELLEN ULLMAN: Well, code
expresses itself by running,

00:51:50.290 --> 00:51:51.710
by working.

00:51:51.710 --> 00:51:55.080
Its meaning is what it does.

00:51:55.080 --> 00:51:56.700
An algorithm can be beautiful.

00:51:56.700 --> 00:51:58.860
It can express elegance.

00:51:58.860 --> 00:52:02.690
But finally, what it
does is what it means.

00:52:02.690 --> 00:52:05.090
In writing, you never
know when it works.

00:52:05.090 --> 00:52:07.190
Your idea is to
say, OK, that works.

00:52:07.190 --> 00:52:11.630
But there's no compiler,
there's no tester,

00:52:11.630 --> 00:52:15.320
there's nothing there
except your own facility.

00:52:15.320 --> 00:52:21.230
Coding taught me to focus
and have stamina and have

00:52:21.230 --> 00:52:22.715
the resolve to solve problems.

00:52:27.670 --> 00:52:30.280
It's hard, but a good hard.

00:52:30.280 --> 00:52:33.940
And writing is also
hard and a good hard.

00:52:33.940 --> 00:52:39.430
The overlap in languages,
I'm afraid I don't see.

00:52:39.430 --> 00:52:43.120
As I say, code can have elegance
but one is very structured

00:52:43.120 --> 00:52:47.770
and you have requirements about
how you're going to write code.

00:52:47.770 --> 00:52:52.390
You have style you have to
follow with an organization

00:52:52.390 --> 00:52:55.980
and code reviews and so forth.

00:52:55.980 --> 00:52:59.100
The code review in a book is
well, will people read it,

00:52:59.100 --> 00:52:59.850
will they like it?

00:53:03.830 --> 00:53:06.040
Language is
expressive and you can

00:53:06.040 --> 00:53:10.360
say things that are
just wrong grammatically

00:53:10.360 --> 00:53:12.400
and people will
understand it anyway.

00:53:12.400 --> 00:53:14.140
That's one of its beauties.

00:53:14.140 --> 00:53:16.330
People who come from
different cultures

00:53:16.330 --> 00:53:17.980
can write sort of
in hip-hop, they

00:53:17.980 --> 00:53:21.550
can write in code
of the Caribbean

00:53:21.550 --> 00:53:25.090
like [INAUDIBLE]
Diaz, they can really

00:53:25.090 --> 00:53:29.710
use language and juggle
it or be extremely formal.

00:53:29.710 --> 00:53:32.380
And they're all valid
means of expression

00:53:32.380 --> 00:53:37.480
in language, and that you can
do it wrong and still have it

00:53:37.480 --> 00:53:40.434
work is one of the wonderful
things about language.

00:53:40.434 --> 00:53:41.100
DAVID BARRY: OK.

00:53:41.100 --> 00:53:42.683
Well, thank you,
everyone, for coming.

00:53:42.683 --> 00:53:44.560
Thank you, Ellen,
so much for coming.

00:53:44.560 --> 00:53:44.870
ELLEN ULLMAN: Thank you.

00:53:44.870 --> 00:53:46.030
DAVID BARRY: Let's have a
round of applause, please.

00:53:46.030 --> 00:53:47.529
ELLEN ULLMAN: It's
been my pleasure.

00:53:47.529 --> 00:53:49.880
[APPLAUSE]

