WEBVTT
Kind: captions
Language: en

00:00:01.020 --> 00:00:02.300
- [Narrator] This program is presented

00:00:02.300 --> 00:00:04.760
by University of California Television.

00:00:04.760 --> 00:00:05.950
Like what you learn?

00:00:05.950 --> 00:00:09.190
visit our website or follow
us on Facebook and Twitter

00:00:09.190 --> 00:00:12.070
to keep up with the latest UCTV programs.

00:00:12.070 --> 00:00:14.109
Also make sure to check out and subscribe

00:00:14.109 --> 00:00:16.035
to our YouTube original channel,

00:00:16.035 --> 00:00:19.710
UCTV Prime, available only on YouTube.

00:00:25.700 --> 00:00:28.200
(light music)

00:00:57.480 --> 00:01:02.480
- Good afternoon everyone,
my name is Mary Comerio.

00:01:02.480 --> 00:01:05.251
I am Professor in the Graduate
School in architecture

00:01:05.251 --> 00:01:08.910
and the chair of the Hitchcock
Professorship Committee.

00:01:08.910 --> 00:01:11.600
We are pleased, along
with the Graduate Council,

00:01:11.600 --> 00:01:14.156
to present Jeff Hawkins,
this year's speaker

00:01:14.156 --> 00:01:17.932
in the Charles M. and Martha
Hitchcock lecture series.

00:01:17.932 --> 00:01:19.891
As a condition of this bequest,

00:01:19.891 --> 00:01:22.724
we are obligated and happy to tell you

00:01:22.724 --> 00:01:26.120
how the endowment came to UC Berkeley.

00:01:26.120 --> 00:01:28.820
It's a story that
exemplifies the many ways

00:01:28.820 --> 00:01:31.270
this campus is linked to the history

00:01:31.270 --> 00:01:33.910
of California and the Bay Area.

00:01:33.910 --> 00:01:37.450
Dr Charles Hitchcock, a
physician in for the Army,

00:01:37.450 --> 00:01:40.162
came to San Francisco during the Gold Rush

00:01:40.162 --> 00:01:43.500
where he opened a
thriving private practice.

00:01:43.500 --> 00:01:47.500
In 1885, Charles
established a professorship

00:01:47.500 --> 00:01:49.640
here at Berkeley, an expression

00:01:49.640 --> 00:01:52.740
of his long held interest in education.

00:01:52.740 --> 00:01:55.710
His daughter Lillie Hitchcock Coit,

00:01:55.710 --> 00:01:57.580
still treasured in San Francisco

00:01:57.580 --> 00:02:01.570
for her colorful personality
as well as her generosity,

00:02:01.570 --> 00:02:04.580
greatly expanded her
father's original gift

00:02:04.580 --> 00:02:07.540
to establish the
professorship at UC Berkeley,

00:02:07.540 --> 00:02:09.960
making it possible for us to present

00:02:09.960 --> 00:02:13.780
a series of lectures, this
very long and wonderful series

00:02:13.780 --> 00:02:15.727
which we have had for many years.

00:02:15.727 --> 00:02:17.398
The Hitchcock Fund has become

00:02:17.398 --> 00:02:19.580
one of the most cherished endowments

00:02:19.580 --> 00:02:21.529
of the University of California,

00:02:21.529 --> 00:02:24.450
recognizing the highest distinction

00:02:24.450 --> 00:02:26.930
of scholarly thought and achievement.

00:02:26.930 --> 00:02:29.040
Thank You Lillie and Charles.

00:02:29.040 --> 00:02:32.127
And now a few words about Jeff Hawkins.

00:02:32.127 --> 00:02:35.050
Jeff Hawkins has a multi faceted career

00:02:35.050 --> 00:02:37.130
as an inventor, engineer,

00:02:37.130 --> 00:02:40.817
neuroscientist, author, entrepreneur.

00:02:40.817 --> 00:02:43.446
In his book on intelligence,

00:02:43.446 --> 00:02:48.446
he describes his life as
animated by two passions,

00:02:48.560 --> 00:02:51.550
mobile computing and neuroscience.

00:02:51.550 --> 00:02:54.600
As the founder of Palm and Handspring,

00:02:54.600 --> 00:02:57.570
Hawkins was at the forefront
of mobile computing

00:02:57.570 --> 00:03:00.349
and developed landmark
products like the PalmPilot

00:03:00.349 --> 00:03:02.930
and Treo smartphone.

00:03:02.930 --> 00:03:05.450
His lifelong interest in neuroscience

00:03:05.450 --> 00:03:08.460
led him to UC Berkeley
as a graduate student

00:03:08.460 --> 00:03:10.270
in integrative biology,

00:03:10.270 --> 00:03:13.117
and to found the Redwood
Neuroscience Institute,

00:03:13.117 --> 00:03:14.840
aimed at understanding

00:03:14.840 --> 00:03:17.699
how the neocortex processes information.

00:03:17.699 --> 00:03:22.460
In 2005 Hawkins gifted the
Redwood Neuroscience Institute

00:03:22.460 --> 00:03:25.014
to UC Berkeley, where it now exists

00:03:25.014 --> 00:03:29.180
as the Redwood Center for
Theoretical Neuroscience.

00:03:29.180 --> 00:03:31.150
His latest company, Numenta,

00:03:31.150 --> 00:03:33.539
brings his two passions together.

00:03:33.539 --> 00:03:37.156
At Numenta, Hawkins is developing
new computer technologies

00:03:37.156 --> 00:03:40.770
modeled on the workings of the neocortex.

00:03:40.770 --> 00:03:43.471
This approach, hierarchical
temporal memory,

00:03:43.471 --> 00:03:46.130
allows machines to extract patterns

00:03:46.130 --> 00:03:47.630
from complex data streams

00:03:47.630 --> 00:03:50.460
and predict what is likely to occur.

00:03:50.460 --> 00:03:53.990
The company's latest product is Grok,

00:03:53.990 --> 00:03:57.110
a cloud based engine
that makes predictions

00:03:57.110 --> 00:03:58.610
from streaming data.

00:03:58.610 --> 00:04:01.540
Hawkins hopes that Numenta
will play a catalytic role

00:04:01.540 --> 00:04:04.500
in the emerging fields
of machine intelligence.

00:04:06.250 --> 00:04:09.350
Hawkins received a BS in
electrical engineering

00:04:09.350 --> 00:04:12.840
from Cornell University in 1979.

00:04:12.840 --> 00:04:15.360
He came to UC Berkeley
as a graduate student

00:04:15.360 --> 00:04:18.750
in integrative biology in 1986.

00:04:18.750 --> 00:04:22.476
In 1988 he returned to
the computing community,

00:04:22.476 --> 00:04:25.035
ultimately to found Palm computing,

00:04:25.035 --> 00:04:27.943
and then continued deep
engagement with neuroscience

00:04:27.943 --> 00:04:31.150
through the Redwood
Neuroscience Institute.

00:04:31.150 --> 00:04:32.980
The Institute was aimed at developing

00:04:32.980 --> 00:04:36.685
a theoretical framework for
the thalamocortical system

00:04:36.685 --> 00:04:40.240
and fostered a vibrant
intellectual atmosphere

00:04:40.240 --> 00:04:43.410
among researchers, postdocs, and students.

00:04:43.410 --> 00:04:45.490
Hawkins' account of the neocortex

00:04:45.490 --> 00:04:48.550
on intelligence was published in 2005,

00:04:48.550 --> 00:04:50.220
to critical acclaim.

00:04:50.220 --> 00:04:52.760
Hawkins was elected to
the National Academy

00:04:52.760 --> 00:04:55.700
of Engineering in 2003.

00:04:55.700 --> 00:04:58.662
Please join me in a warm
welcome for Jeff Hawkins.

00:04:58.662 --> 00:05:01.745
(audience applauding)

00:05:05.720 --> 00:05:06.570
- Thank you Mary.

00:05:07.680 --> 00:05:09.360
Oh, let's trade places here.

00:05:09.360 --> 00:05:11.200
So hopefully this microphone's okay?

00:05:11.200 --> 00:05:12.890
Not too loud?

00:05:12.890 --> 00:05:13.723
Great.

00:05:13.723 --> 00:05:15.660
Thank you Mary for that
lovely introduction.

00:05:16.670 --> 00:05:18.900
And it's a real honor to be here.

00:05:18.900 --> 00:05:21.140
This is a very
distinguished lecture series

00:05:21.140 --> 00:05:25.040
and it goes back for many
years and it's a treasure.

00:05:25.040 --> 00:05:25.873
It's a pleasure.

00:05:25.873 --> 00:05:28.120
I've been looking forward to
it all year to being here.

00:05:28.120 --> 00:05:29.913
So I'm gonna to give two talks,

00:05:29.913 --> 00:05:32.630
today and tomorrow,
all about intelligence.

00:05:32.630 --> 00:05:34.739
Today is intelligence and the brain and

00:05:34.739 --> 00:05:37.110
tomorrow intelligence and machines.

00:05:38.190 --> 00:05:40.600
Mary told a little bit
of my story already,

00:05:40.600 --> 00:05:41.730
which I was going to introduce here

00:05:41.730 --> 00:05:43.990
about how I how I came
to be on this stage,

00:05:45.150 --> 00:05:47.350
and so I won't go through that all again.

00:05:47.350 --> 00:05:49.170
But I'll tell you a little bit

00:05:49.170 --> 00:05:50.730
more flavor about the time I was here

00:05:50.730 --> 00:05:52.020
as a graduate student at Berkeley.

00:05:52.020 --> 00:05:56.080
This was, it goes back to actually 1979,

00:05:56.080 --> 00:05:57.830
when I just came out of an undergraduate,

00:05:57.830 --> 00:05:59.820
I got my undergraduate degree,

00:05:59.820 --> 00:06:02.420
and I had read the September of '79 issue

00:06:02.420 --> 00:06:05.030
of Scientific American, which
was a single topic issue

00:06:05.030 --> 00:06:07.350
on the brain and there were stories by

00:06:07.350 --> 00:06:09.060
very many famous neuroscientists,

00:06:09.060 --> 00:06:12.340
Eric Kandel, Hubel and Wiesel et cetera,

00:06:12.340 --> 00:06:16.820
and the very last article was
by Francis Crick of DNA fame.

00:06:16.820 --> 00:06:18.190
And Francis wrote, he said,

00:06:18.190 --> 00:06:20.040
this is all well and good,
we have all this data

00:06:20.040 --> 00:06:22.510
about the brain, we know all
this about the neuroscience

00:06:22.510 --> 00:06:24.830
and the neurons and diseases
of the brain of and so on.

00:06:24.830 --> 00:06:26.390
He says, but don't be mistaken,

00:06:26.390 --> 00:06:28.970
we have no idea what the
hell's going on up there.

00:06:28.970 --> 00:06:30.980
And he says, what we're lacking is

00:06:30.980 --> 00:06:33.720
a theoretical framework
and this excited me.

00:06:33.720 --> 00:06:35.360
It was like oh my gosh
we have all this data

00:06:35.360 --> 00:06:37.080
about the brain but we really don't

00:06:37.080 --> 00:06:38.640
understand what's going on, how it worked.

00:06:38.640 --> 00:06:40.220
You need a theoretical foundation

00:06:40.220 --> 00:06:42.300
for it and I said this is something we

00:06:42.300 --> 00:06:43.810
should be able to do in my lifetime, it's

00:06:43.810 --> 00:06:45.940
something I want to work on and I

00:06:45.940 --> 00:06:48.370
basically dedicated my career to that at

00:06:48.370 --> 00:06:49.980
that point in time.

00:06:49.980 --> 00:06:54.200
And my first attempt was at
MIT where I tried to become a

00:06:54.200 --> 00:06:55.741
graduate student there in the AI lab and

00:06:55.741 --> 00:06:58.450
they asked me, they said, well they said,

00:06:58.450 --> 00:06:59.283
"Why are you here?" and I said,

00:06:59.283 --> 00:07:00.440
"Well I want to build
intelligent machines,"

00:07:00.440 --> 00:07:02.540
and they said, "That's
great we do too," and then I

00:07:02.540 --> 00:07:04.200
said, "But I want to study brains first to

00:07:04.200 --> 00:07:06.350
"see how they work," and they
said, "Oh, that's stupid.

00:07:06.350 --> 00:07:07.510
"Why would you do that?

00:07:07.510 --> 00:07:09.711
"A brain is just a messy computer
and why would you study a

00:07:09.711 --> 00:07:12.310
"messy computer when you
could study good ones?"

00:07:12.310 --> 00:07:13.860
So I didn't get into MIT and

00:07:13.860 --> 00:07:15.780
that's why I snuck into Berkeley because

00:07:15.780 --> 00:07:17.120
Berkeley I said I'll just go in as a

00:07:17.120 --> 00:07:18.650
biology person, they won't know the

00:07:18.650 --> 00:07:21.990
difference and so I
got in, it was actually

00:07:21.990 --> 00:07:23.870
the biophysics program
but didn't really matter.

00:07:23.870 --> 00:07:26.670
There was no theoretical
neuroscience back then.

00:07:26.670 --> 00:07:28.020
And at that time

00:07:28.020 --> 00:07:30.700
my first six months here, I took classes

00:07:30.700 --> 00:07:32.350
I took Jeffrey Weiner's anatomy class

00:07:32.350 --> 00:07:33.890
which was great, I read Kandel's book

00:07:33.890 --> 00:07:36.906
twice and I wrote a long thesis

00:07:36.906 --> 00:07:39.500
proposal about what I wanted to do as a

00:07:39.500 --> 00:07:41.910
graduate student and
Professor Frank Wargulan

00:07:41.910 --> 00:07:43.310
was very kind and head of the chairman,

00:07:43.310 --> 00:07:46.100
he was the chairman in graduate
neurobiology at the time.

00:07:46.100 --> 00:07:47.110
He read it he had some other

00:07:47.110 --> 00:07:48.590
faculty read it and he said this is really

00:07:48.590 --> 00:07:50.230
good, in fact it's a proposal of what I'm

00:07:50.230 --> 00:07:52.540
gonna tell you about in my talk.

00:07:52.540 --> 00:07:55.020
And he says, "Unfortunately
you can't do this here."

00:07:55.020 --> 00:07:55.853
I go, "What'd he mean?"

00:07:55.853 --> 00:07:57.710
I just got all this effort
to come here as a student.

00:07:57.710 --> 00:07:59.540
He said, "Well no one's doing this

00:07:59.540 --> 00:08:01.893
"No faculty is working on models neocortex

00:08:01.893 --> 00:08:03.710
"from a theoretical point of view.

00:08:03.710 --> 00:08:06.270
"In fact, I don't know anyone
in the world doing this."

00:08:06.270 --> 00:08:08.260
This is in 1986 and he said

00:08:08.260 --> 00:08:09.810
as a graduate student you got to work

00:08:09.810 --> 00:08:11.500
for somebody in somebody's lab and

00:08:11.500 --> 00:08:13.961
there's nobody doing what you want to do.

00:08:13.961 --> 00:08:15.670
I was really kind of bummed out and

00:08:15.670 --> 00:08:17.660
so what I ended up doing after six

00:08:17.660 --> 00:08:19.410
months of being a regular student, I took

00:08:19.410 --> 00:08:22.084
the next year and for a course of a year

00:08:22.084 --> 00:08:23.870
I would come to Berkeley, I kept my

00:08:23.870 --> 00:08:26.170
student status, I'd
come to Berkeley once a

00:08:26.170 --> 00:08:28.102
week because I wanted to read papers and

00:08:28.102 --> 00:08:30.350
you couldn't, there was no internet back

00:08:30.350 --> 00:08:31.830
then and to read papers you had to go

00:08:31.830 --> 00:08:33.120
to university library.

00:08:33.120 --> 00:08:34.810
So every week I would come
to Berkeley with a list of

00:08:34.810 --> 00:08:36.170
papers I want to read, I had to visit

00:08:36.170 --> 00:08:37.990
multiple libraries on the campus.

00:08:37.990 --> 00:08:39.447
I'd go look at these papers,
I would photocopy the

00:08:39.447 --> 00:08:41.830
ones that were interesting to me, I'd come

00:08:41.830 --> 00:08:43.400
back home, read them over the course of

00:08:43.400 --> 00:08:45.430
the next week, come back the next week

00:08:45.430 --> 00:08:46.900
with a set of of other papers I wanted to

00:08:46.900 --> 00:08:48.650
do and I did this for about a year.

00:08:48.650 --> 00:08:51.660
And it was a very great knowledge

00:08:51.660 --> 00:08:53.220
gaining exercise about the history of

00:08:53.220 --> 00:08:55.270
neuroscience, I could
study whatever I wanted.

00:08:55.270 --> 00:08:56.610
But eventually I had to make a

00:08:56.610 --> 00:08:58.590
living and I couldn't do that forever so

00:08:58.590 --> 00:09:00.330
that's why I went back to work which I

00:09:00.330 --> 00:09:01.830
thought was going to be for four years

00:09:01.830 --> 00:09:04.220
but it turned out to
be for about 16 years.

00:09:04.220 --> 00:09:06.530
Because I had very fortunate success in

00:09:06.530 --> 00:09:08.320
my business life and finally I was able

00:09:08.320 --> 00:09:10.450
to extricate myself and I want tell you

00:09:10.450 --> 00:09:11.283
one more thing.

00:09:11.283 --> 00:09:12.116
When I started the

00:09:12.116 --> 00:09:12.949
Redwood Neuroscience Institute, this is a

00:09:12.949 --> 00:09:13.790
crazy idea.

00:09:13.790 --> 00:09:15.690
How do you start a new science institute?

00:09:15.690 --> 00:09:16.523
Who's going to come up,

00:09:16.523 --> 00:09:17.356
who's going to show up,

00:09:17.356 --> 00:09:19.470
who's gonna be the first one here?

00:09:19.470 --> 00:09:21.890
I said, it was not even my idea,
some neuroscientist friends

00:09:21.890 --> 00:09:23.140
of mine said why don't
you start this institute?

00:09:23.140 --> 00:09:23.973
That's what's required.

00:09:23.973 --> 00:09:24.806
I said I can't do that.

00:09:24.806 --> 00:09:27.380
I said well you if you help me I'll do it.

00:09:27.380 --> 00:09:28.730
And one of the people

00:09:28.730 --> 00:09:30.210
helped me was Christof Koch, another one

00:09:30.210 --> 00:09:31.330
was Bob Knight at Berkeley.

00:09:31.330 --> 00:09:32.590
I talked to Bob who was head of

00:09:32.590 --> 00:09:33.880
the Helen Wills Neuroscience Institute

00:09:33.880 --> 00:09:34.713
at the time.

00:09:34.713 --> 00:09:35.710
I said Bob would you help me do this?

00:09:35.710 --> 00:09:38.220
He goes sure, we need a
theoretical component here.

00:09:38.220 --> 00:09:40.120
And so from the very

00:09:40.120 --> 00:09:41.610
beginning there was a close association

00:09:41.610 --> 00:09:43.310
between the Redwood Neuroscience Institute

00:09:43.310 --> 00:09:44.260
and Berkeley.

00:09:44.260 --> 00:09:45.430
That's a little bit ironic because

00:09:45.430 --> 00:09:46.827
the Redwood Neuroscience
Institute was located right

00:09:46.827 --> 00:09:49.350
next to Stanford and they didn't want to

00:09:49.350 --> 00:09:50.760
really do much with us at all.

00:09:50.760 --> 00:09:52.750
But Berkeley was very open so we came

00:09:52.750 --> 00:09:54.410
up and we did some classes and we

00:09:54.410 --> 00:09:55.520
exchanged students and so on.

00:09:55.520 --> 00:09:57.040
So from the very beginning it was a good

00:09:57.040 --> 00:09:58.880
collaboration and when it came time

00:09:58.880 --> 00:10:00.420
for me to start my own lab and my own

00:10:00.420 --> 00:10:02.060
business that Numenta, we had to decide

00:10:02.060 --> 00:10:03.343
what to do with the
Neuroscience Institute,

00:10:03.343 --> 00:10:05.450
Moving it to Berkeley made a lot of sense.

00:10:05.450 --> 00:10:06.992
And Bruno Olshausen who's here

00:10:06.992 --> 00:10:10.250
was the head guy who was
helping me out at RNI

00:10:10.250 --> 00:10:12.510
and he's now a faculty here at Berkeley.

00:10:12.510 --> 00:10:15.450
So the long story, here
I am 30 years into it,

00:10:15.450 --> 00:10:17.370
I've been working on this for a long time.

00:10:17.370 --> 00:10:20.430
So we're gonna just jump into it.

00:10:20.430 --> 00:10:22.880
This is Francis Crick's words.

00:10:22.880 --> 00:10:24.000
This is what exactly he wrote.

00:10:24.000 --> 00:10:25.770
What is conspicuously lacking is a broad

00:10:25.770 --> 00:10:28.210
framework of ideas in which
to interpret these results.

00:10:28.210 --> 00:10:30.560
The results he was talking
about were the empirical results

00:10:30.560 --> 00:10:31.590
of neuroscience.

00:10:31.590 --> 00:10:34.260
He said look, lots of data, no theory.

00:10:34.260 --> 00:10:35.093
And this is what I've

00:10:35.093 --> 00:10:36.670
been working on and I want to tell you

00:10:36.670 --> 00:10:40.000
about the progress we've
made on this so far.

00:10:40.000 --> 00:10:42.430
I set out two questions I wanted to solve.

00:10:42.430 --> 00:10:43.860
One was what are the operating

00:10:43.860 --> 00:10:45.370
principles of the neocortex?

00:10:45.370 --> 00:10:47.730
What are the theory behind it?

00:10:47.730 --> 00:10:49.020
It's not like an equation
or something like

00:10:49.020 --> 00:10:50.650
that, but there's a set of principles.

00:10:50.650 --> 00:10:51.816
I also realized that once we had those

00:10:51.816 --> 00:10:54.570
principles, we would be able to build

00:10:54.570 --> 00:10:56.680
systems that work on those principles.

00:10:56.680 --> 00:10:58.775
And we'd be able to build
machines that exhibit

00:10:58.775 --> 00:11:01.432
the same principles of the
neocortex and so those two

00:11:01.432 --> 00:11:04.540
came hand in hand and they
complemented each other.

00:11:05.580 --> 00:11:07.230
The way I go about this is the following.

00:11:07.230 --> 00:11:08.190
You start with the brain

00:11:08.190 --> 00:11:10.190
and we start with anatomy and physiology.

00:11:10.190 --> 00:11:12.170
These are constraints on the problem.

00:11:12.170 --> 00:11:13.260
We know a tremendous amount of

00:11:13.260 --> 00:11:15.410
how the brain organized
and wired and so on.

00:11:15.410 --> 00:11:17.020
These are not things you can ignore.

00:11:17.020 --> 00:11:18.290
If we want to understand the principles

00:11:18.290 --> 00:11:19.585
of intelligence you need to have to be

00:11:19.585 --> 00:11:23.090
copacetic with those
those brain principles.

00:11:23.090 --> 00:11:24.920
So this I took this seriously,

00:11:24.920 --> 00:11:27.330
very seriously, getting totally embedded

00:11:27.330 --> 00:11:29.340
with the anatomy and physiology and

00:11:29.340 --> 00:11:31.300
histology of the brain.

00:11:31.300 --> 00:11:32.133
Then once you

00:11:32.133 --> 00:11:33.690
understand the principles and you can

00:11:33.690 --> 00:11:35.810
elaborate them, you can model it and you

00:11:35.810 --> 00:11:37.820
can first do that in software which is

00:11:37.820 --> 00:11:39.170
what we're doing today
and I'm going to talk

00:11:39.170 --> 00:11:40.840
more about that tomorrow, and then

00:11:40.840 --> 00:11:42.390
ultimately you can do it in hardware and

00:11:42.390 --> 00:11:44.190
we have conversations with people trying

00:11:44.190 --> 00:11:45.860
to build this stuff today.

00:11:45.860 --> 00:11:47.810
So this is the basic premise.

00:11:47.810 --> 00:11:49.820
My today's talk is going
to start with the anatomy

00:11:49.820 --> 00:11:52.492
and physiology and talk about
the theoretical principles.

00:11:52.492 --> 00:11:53.630
Tomorrow I'm going to

00:11:53.630 --> 00:11:54.750
start with the theoretical principles

00:11:54.750 --> 00:11:56.065
and talk about how we build this stuff

00:11:56.065 --> 00:11:57.210
and where is it going.

00:11:59.660 --> 00:12:00.880
So let's start with the brain again.

00:12:00.880 --> 00:12:03.950
That's a little bit more visible here.

00:12:03.950 --> 00:12:07.400
We're gonna start a little bit
of a history about the brain.

00:12:07.400 --> 00:12:09.000
You know if you go back in the beginning

00:12:09.000 --> 00:12:10.080
when people really trying to figure out

00:12:10.080 --> 00:12:11.990
how the organs of the body worked, people

00:12:11.990 --> 00:12:13.460
take tissue and they look at it under a

00:12:13.460 --> 00:12:14.876
microscope and everywhere you looked if

00:12:14.876 --> 00:12:17.500
you looked at you know a liver or kidney

00:12:17.500 --> 00:12:19.490
or muscle you would see cells.

00:12:19.490 --> 00:12:21.670
And there's like looking
at peas in a ball.

00:12:21.670 --> 00:12:22.890
They're all they are lined up under a

00:12:22.890 --> 00:12:23.970
microscope, but when you looked at the

00:12:23.970 --> 00:12:25.680
brain it didn't look like that.

00:12:25.680 --> 00:12:28.180
It looked like a bowl of
micro spaghetti and it

00:12:28.180 --> 00:12:30.500
was not clear in the late 1800s that

00:12:30.500 --> 00:12:32.660
actually the brain was
made of separate cells.

00:12:32.660 --> 00:12:33.920
This was a question.

00:12:33.920 --> 00:12:35.800
And many people thought
it wasn't, they thought

00:12:35.800 --> 00:12:37.420
it was some sort of magic tubular

00:12:37.420 --> 00:12:39.540
mass of jelly or something like that.

00:12:39.540 --> 00:12:42.230
And and it was one of the
most famous neuroscientists

00:12:42.230 --> 00:12:45.340
of all time, Santiago Ramon Y Cajal

00:12:45.340 --> 00:12:46.620
and every neuroscientist knows this guy,

00:12:46.620 --> 00:12:49.199
he's like the king of neuroscientists.

00:12:49.199 --> 00:12:51.254
He was a Spanish guy,
he basically mapped out

00:12:51.254 --> 00:12:54.840
the entire nervous
system for many species.

00:12:54.840 --> 00:12:57.840
Cajal came along with
a new technique that he

00:12:57.840 --> 00:13:00.190
didn't discover but he was exploiting

00:13:00.190 --> 00:13:02.270
for staining cells where you could stain

00:13:02.270 --> 00:13:04.950
completely a cell and
see what it looked like.

00:13:04.950 --> 00:13:07.070
And when he did that, he started

00:13:07.070 --> 00:13:08.440
making these wonderful pictures.

00:13:08.440 --> 00:13:10.450
Here's one of Cajal's
pictures on the right here

00:13:10.450 --> 00:13:12.543
of a classic neuron in the neocortex.

00:13:12.543 --> 00:13:14.719
And what he showed was that actually the

00:13:14.719 --> 00:13:16.870
brain is made of individual cells.

00:13:16.870 --> 00:13:18.430
Even though they branch all over the place

00:13:18.430 --> 00:13:20.370
and they look like they're
connected, they don't.

00:13:20.370 --> 00:13:21.560
The cytoplasm doesn't go

00:13:21.560 --> 00:13:23.250
between and it's just like tissue like

00:13:23.250 --> 00:13:24.700
everywhere else in the brain.

00:13:24.700 --> 00:13:27.600
Now this has become known
as the neuron doctrine,

00:13:27.600 --> 00:13:29.254
it's the founding principle of which all

00:13:29.254 --> 00:13:31.760
neuroscience is based, is essentially

00:13:31.760 --> 00:13:33.531
that the brain is made
of a bunch of cells.

00:13:33.531 --> 00:13:35.350
A corollary to that which is not

00:13:35.350 --> 00:13:36.740
often stated but we should just get it

00:13:36.740 --> 00:13:38.400
out of the way is the brain is not made

00:13:38.400 --> 00:13:40.070
of anything else, there's no magic pixie

00:13:40.070 --> 00:13:42.880
dust in there, it's just a bunch of cells.

00:13:42.880 --> 00:13:44.330
And if we want to understand

00:13:44.330 --> 00:13:45.560
intelligence, we need to understand how

00:13:45.560 --> 00:13:48.060
the cells interact and what they do.

00:13:48.060 --> 00:13:49.520
There's no room for a mind that's

00:13:49.520 --> 00:13:50.400
separate from the brain.

00:13:50.400 --> 00:13:51.233
There's no room

00:13:51.233 --> 00:13:53.450
for a mind that can do extraterrestrial

00:13:53.450 --> 00:13:55.240
communications and things like that.

00:13:55.240 --> 00:13:57.430
So until we have proof otherwise, until

00:13:57.430 --> 00:13:59.480
we've eliminated every other possibility,

00:13:59.480 --> 00:14:01.390
we have to think of
everything the mind does,

00:14:01.390 --> 00:14:05.140
everything we think of as
humanity is a bunch of cells

00:14:05.140 --> 00:14:07.070
doing this in our head.

00:14:07.070 --> 00:14:07.903
Okay.

00:14:07.903 --> 00:14:09.032
Now already you can see on this picture

00:14:09.032 --> 00:14:11.030
that these cells are very unusual.

00:14:11.030 --> 00:14:13.520
They have a cell body
which is fairly small

00:14:13.520 --> 00:14:14.850
in that picture and they have these

00:14:14.850 --> 00:14:16.565
branching arborizations called dendrites

00:14:16.565 --> 00:14:18.590
and the connections to the cell where we

00:14:18.590 --> 00:14:20.430
seize information, it's all arrayed on

00:14:20.430 --> 00:14:22.060
those dendrites, they're not even on the

00:14:22.060 --> 00:14:24.030
cell body, at least not
the excitatory ones.

00:14:24.030 --> 00:14:25.390
A typical neuron has several

00:14:25.390 --> 00:14:27.260
thousand of these connections arranged

00:14:27.260 --> 00:14:28.920
along the dendrite, this is a very

00:14:28.920 --> 00:14:32.410
complex system, and lots of inputs.

00:14:33.875 --> 00:14:35.880
There's another part of the

00:14:35.880 --> 00:14:38.050
neuron which I'm going to add to the

00:14:38.050 --> 00:14:40.150
picture right now which is the axon.

00:14:40.150 --> 00:14:42.220
This is the output of
the neuron and it's a

00:14:42.220 --> 00:14:43.960
single fiber where the spikes from

00:14:43.960 --> 00:14:45.690
a neuron go out to other cells and you

00:14:45.690 --> 00:14:46.924
can see here the first thing is that

00:14:46.924 --> 00:14:50.680
the axon spreads locally, it makes

00:14:50.680 --> 00:14:53.620
connections to lots of other cells nearby.

00:14:53.620 --> 00:14:55.380
Thousands of them typically.

00:14:55.380 --> 00:14:56.880
And those connections
are very interesting.

00:14:56.880 --> 00:14:58.800
They don't connect back
to the original cell.

00:14:58.800 --> 00:15:00.480
They don't connect many to any

00:15:00.480 --> 00:15:02.530
particular cell, particular other cell

00:15:02.530 --> 00:15:03.959
may get one or two of these guys and

00:15:03.959 --> 00:15:05.960
most cells don't get a connection at all

00:15:05.960 --> 00:15:07.850
so it's very densely packed in there.

00:15:07.850 --> 00:15:11.460
But this we make a lot
of connections locally.

00:15:11.460 --> 00:15:13.030
Then the axon typically as shown

00:15:13.030 --> 00:15:15.550
in this picture continues on and goes

00:15:15.550 --> 00:15:17.410
someplace else and makes more connections.

00:15:17.410 --> 00:15:18.243
So it makes a lot of

00:15:18.243 --> 00:15:19.440
connections locally and then it goes

00:15:19.440 --> 00:15:21.170
elsewhere, it makes other connections.

00:15:21.170 --> 00:15:24.060
And this is the the basic idea of what

00:15:24.060 --> 00:15:26.400
neurons look like in the brain.

00:15:26.400 --> 00:15:28.940
We have somewhere around
30 billion to a 100 billion

00:15:28.940 --> 00:15:30.710
of these cells in your neocortex.

00:15:31.843 --> 00:15:34.320
And we wanna understand
how they work together.

00:15:35.890 --> 00:15:37.630
If you take a, we actually

00:15:37.630 --> 00:15:39.144
today have a fairly good idea about how

00:15:39.144 --> 00:15:41.290
these cells work, meaning what their

00:15:41.290 --> 00:15:43.170
properties are, how their ion channels

00:15:43.170 --> 00:15:45.369
work, how they grow and how they die and

00:15:45.369 --> 00:15:47.675
all kinds of wonderful stuff like that.

00:15:47.675 --> 00:15:49.670
So that's fairly well understood.

00:15:49.670 --> 00:15:51.390
We have good models of these cells.

00:15:51.390 --> 00:15:55.176
However, in the real
brain we find the cells on

00:15:55.176 --> 00:15:56.820
their own don't do much.

00:15:56.820 --> 00:15:58.380
You can eliminate any
cell and it doesn't really

00:15:58.380 --> 00:16:01.126
matter, it's the collection
of cells that matter.

00:16:01.126 --> 00:16:03.020
It's ensembles of cells that matter.

00:16:03.020 --> 00:16:04.620
Cells working together.

00:16:04.620 --> 00:16:05.690
Here's another picture from

00:16:05.690 --> 00:16:08.590
Cajal, and back this is over a 100

00:16:08.590 --> 00:16:09.760
years ago he made these pictures but

00:16:09.760 --> 00:16:10.732
they're still quite good.

00:16:10.732 --> 00:16:11.935
In this picture

00:16:11.935 --> 00:16:13.830
you see a quite a few cells in it.

00:16:13.830 --> 00:16:15.520
This is still a very small percent of

00:16:15.520 --> 00:16:16.830
the cells that you might find in a

00:16:16.830 --> 00:16:17.680
region like this.

00:16:17.680 --> 00:16:20.648
This is a two millimeter high view of

00:16:20.648 --> 00:16:23.620
the cells in a slice of the neocortex.

00:16:23.620 --> 00:16:24.960
And already you see some things

00:16:24.960 --> 00:16:26.263
popping up here, when we see these cells

00:16:26.263 --> 00:16:27.529
in groups we see that they're

00:16:27.529 --> 00:16:29.660
sort of grouped in layers.

00:16:29.660 --> 00:16:32.220
Cajal labeled these with
numbers on the side there.

00:16:32.220 --> 00:16:33.053
You can see those layers are

00:16:33.053 --> 00:16:34.400
different by the type of cell types,

00:16:34.400 --> 00:16:36.360
by the dendritic arborizations where

00:16:36.360 --> 00:16:37.502
they project and so on.

00:16:37.502 --> 00:16:39.803
And so there's this layer
effect and it's also

00:16:39.803 --> 00:16:41.740
a vertical sort of calmer effect you

00:16:41.740 --> 00:16:44.000
can see in this picture as well.

00:16:44.000 --> 00:16:47.120
So our goal is to try
to figure out how those

00:16:47.120 --> 00:16:49.190
cells work and make you smart.

00:16:49.190 --> 00:16:50.970
You know, I didn't say this already

00:16:50.970 --> 00:16:52.290
but I'll say it right
now, the brain is a very

00:16:52.290 --> 00:16:53.720
complex organ, there's many different

00:16:53.720 --> 00:16:54.700
components to it.

00:16:54.700 --> 00:16:56.650
My interest is the neocortex.

00:16:56.650 --> 00:16:57.841
The neocortex is about 60% of

00:16:57.841 --> 00:16:59.391
the volume of your brain.

00:16:59.391 --> 00:17:02.890
It's the big wrinkly
thing on top, you know it

00:17:02.890 --> 00:17:04.305
sort of surrounds the rest of brain,

00:17:04.305 --> 00:17:05.710
it's like a sheath.

00:17:05.710 --> 00:17:07.610
It's a thin sheet of cells.

00:17:07.610 --> 00:17:09.570
It's like two millimeters thick.

00:17:09.570 --> 00:17:11.160
And it's the locus of what you might

00:17:11.160 --> 00:17:12.980
call all high-level intelligence.

00:17:12.980 --> 00:17:14.912
All high-level vision is in the neocortex,

00:17:14.912 --> 00:17:17.950
language whether it's spoken
language, written language.

00:17:18.940 --> 00:17:22.760
The mathematics and physics and so on.

00:17:22.760 --> 00:17:25.476
My neocortex is generating
my speech right now

00:17:25.476 --> 00:17:28.510
and your neocortex is understanding it.

00:17:28.510 --> 00:17:29.500
The other parts of the brain are

00:17:29.500 --> 00:17:31.397
important and they
interact with the neocortex

00:17:31.397 --> 00:17:32.790
in various ways.

00:17:32.790 --> 00:17:34.340
My goal is really just to understand how

00:17:34.340 --> 00:17:35.830
the neocortex itself works and

00:17:35.830 --> 00:17:38.430
we're going to focus our
comments on that today.

00:17:39.360 --> 00:17:40.720
Okay, so what we want to do is

00:17:40.720 --> 00:17:43.229
figure out, you know, why do the cells

00:17:43.229 --> 00:17:44.893
have all these synapses on them?

00:17:44.893 --> 00:17:46.521
Which is kind of a mystery and I'm going

00:17:46.521 --> 00:17:48.550
to pose an explanation today, and

00:17:48.550 --> 00:17:50.110
how do they work in groups like this?

00:17:50.110 --> 00:17:53.270
What is the information
processing going on in here?

00:17:53.270 --> 00:17:54.740
All right so let's go to that.

00:17:54.740 --> 00:17:56.610
Here's the agenda for my talk.

00:17:56.610 --> 00:17:57.746
I'm going to talk about the neocortex as a

00:17:57.746 --> 00:17:59.330
predictive modeling system.

00:17:59.330 --> 00:18:01.603
I'm then going to talk
about three attributes

00:18:01.603 --> 00:18:04.011
of the neocortex that in my work and my

00:18:04.011 --> 00:18:06.660
company we three years ago we had

00:18:06.660 --> 00:18:07.580
a real sort of breakthrough in

00:18:07.580 --> 00:18:09.370
understanding these attributes and so

00:18:09.370 --> 00:18:10.520
I'm gonna focus on them today.

00:18:10.520 --> 00:18:12.320
One is the way the brain
represents information

00:18:12.320 --> 00:18:15.070
which is sparse distributed
representations.

00:18:15.070 --> 00:18:16.250
The second thing is how

00:18:16.250 --> 00:18:18.370
it learns sequences of those patterns.

00:18:18.370 --> 00:18:19.790
I'm going to argue that sequence memory

00:18:19.790 --> 00:18:22.300
is the primary memory in your neocortex.

00:18:22.300 --> 00:18:23.430
And then finally I'm gonna talk about

00:18:23.430 --> 00:18:25.091
how the online learning or how we learn

00:18:25.091 --> 00:18:27.420
continuously, how the
how the memory formed

00:18:27.420 --> 00:18:29.630
in online learning.

00:18:29.630 --> 00:18:31.750
All right so let's just
jump right into it.

00:18:31.750 --> 00:18:33.520
The neocortex is a predictive modeling

00:18:33.520 --> 00:18:35.670
system, so here you see
a picture of a neocortex.

00:18:35.670 --> 00:18:38.047
Maybe you can't, imagine that
little line drawing there,

00:18:38.047 --> 00:18:40.274
and what it does is it receives

00:18:40.274 --> 00:18:41.871
input from your senses.

00:18:41.871 --> 00:18:44.680
Now we think of the three
primary senses as vision,

00:18:44.680 --> 00:18:45.513
hearing, and touch.

00:18:45.513 --> 00:18:47.220
There's many more, in fact, vision itself

00:18:47.220 --> 00:18:48.480
is more than one sense.

00:18:48.480 --> 00:18:50.150
We think of these as three primary senses.

00:18:50.150 --> 00:18:51.140
The first thing you have to know

00:18:51.140 --> 00:18:52.960
is that they're not singular senses.

00:18:52.960 --> 00:18:54.866
The vision, the retina, is really

00:18:54.866 --> 00:18:56.860
like a million senses.

00:18:56.860 --> 00:18:59.280
If there's a million fibers
on your optic nerve going

00:18:59.280 --> 00:19:01.580
from the retina to the neocortex.

00:19:01.580 --> 00:19:03.190
It's not like a picture, it's actually a

00:19:03.190 --> 00:19:04.813
million separate little sensors that are

00:19:04.813 --> 00:19:06.430
arranged topologically.

00:19:06.430 --> 00:19:08.605
The same thing with your
body or somatic senses.

00:19:08.605 --> 00:19:10.380
There's an array of
sensors on your body that's

00:19:10.380 --> 00:19:11.990
got another million fibers coming at

00:19:11.990 --> 00:19:13.571
your brain and your cochlear printer

00:19:13.571 --> 00:19:16.683
produces about 30,000, again, individual

00:19:16.683 --> 00:19:19.310
nerve fibers coming into the brain.

00:19:19.310 --> 00:19:21.718
So you have this massive multi million

00:19:21.718 --> 00:19:24.390
bundle of nerves activations coming to

00:19:24.390 --> 00:19:25.820
your brain and they're changing very

00:19:25.820 --> 00:19:28.250
rapidly, that's what we
mean by high velocity.

00:19:28.250 --> 00:19:29.150
They're changing the order of

00:19:29.150 --> 00:19:30.720
milliseconds, tens of milliseconds and

00:19:30.720 --> 00:19:31.830
hundreds of milliseconds.

00:19:31.830 --> 00:19:33.986
It's like a fire hose of
millions of fibers flipping

00:19:33.986 --> 00:19:36.460
on and off constantly and this is what

00:19:36.460 --> 00:19:38.440
the brain works with, this is it.

00:19:38.440 --> 00:19:39.911
Once those patterns enter the brain,

00:19:39.911 --> 00:19:42.106
they're no longer light, sound, and touch.

00:19:42.106 --> 00:19:44.140
It doesn't exist inside the brain.

00:19:44.140 --> 00:19:46.920
It's just patterns of
activity, and the brain

00:19:46.920 --> 00:19:49.904
has to build a model of the
world from those patterns.

00:19:49.904 --> 00:19:51.210
You start this at birth,

00:19:51.210 --> 00:19:53.520
when you're born you have the structure of

00:19:53.520 --> 00:19:55.780
the neocortex but it
doesn't know anything.

00:19:55.780 --> 00:19:56.900
It doesn't know about Berkeley

00:19:56.900 --> 00:19:58.570
or universities or chandeliers or

00:19:58.570 --> 00:20:00.690
lecterns or computers or water cups or

00:20:00.690 --> 00:20:03.150
toothbrushes or cars.

00:20:03.150 --> 00:20:03.983
Anything.

00:20:03.983 --> 00:20:05.580
It has to learn everything
in the world, it has to

00:20:05.580 --> 00:20:07.295
learn what your environment's like and it

00:20:07.295 --> 00:20:08.930
has to build this model of the world.

00:20:08.930 --> 00:20:11.040
The amount of things
that learns is amazing,

00:20:11.040 --> 00:20:12.930
you're not even aware of
all the things you know.

00:20:12.930 --> 00:20:15.510
And it has to build this
from the stream of data.

00:20:16.350 --> 00:20:17.500
One thing we can say

00:20:17.500 --> 00:20:19.080
once it's built this model of the world

00:20:19.080 --> 00:20:21.245
and new patterns come in, it recognizes

00:20:21.245 --> 00:20:23.110
those patterns and it does a few things.

00:20:23.110 --> 00:20:25.010
One does it makes predictions and I

00:20:25.010 --> 00:20:26.197
wrote extensively about this in my book

00:20:26.197 --> 00:20:28.450
on intelligence that the brain is

00:20:28.450 --> 00:20:29.560
constantly making predictions.

00:20:29.560 --> 00:20:30.393
You're not

00:20:30.393 --> 00:20:31.226
aware of most of them.

00:20:31.226 --> 00:20:32.840
You're constantly predicting
what you're going to see,

00:20:32.840 --> 00:20:34.639
what you're going to feel,
what you're going to hear.

00:20:34.639 --> 00:20:36.160
If I do a simple gesture just

00:20:36.160 --> 00:20:38.220
like putting my hand on this lectern and

00:20:38.220 --> 00:20:40.730
I've never done this in
this exact spot before,

00:20:40.730 --> 00:20:43.450
as my hand comes down my
brain has expectations.

00:20:43.450 --> 00:20:44.790
If my hand went too far and

00:20:44.790 --> 00:20:46.330
passed an inch through this lecture and

00:20:46.330 --> 00:20:47.870
it would say whoa, something's wrong.

00:20:47.870 --> 00:20:49.980
If it felt like water or cold or metal or

00:20:49.980 --> 00:20:52.332
jello, it would all say something's wrong.

00:20:52.332 --> 00:20:54.450
And you're even thinking about this but

00:20:54.450 --> 00:20:56.160
your brain is constantly
making expectations.

00:20:56.160 --> 00:20:57.980
You're predicting what I'm going to say.

00:20:57.980 --> 00:21:00.040
You may not be able to
predict the exact words.

00:21:00.040 --> 00:21:01.541
Sometimes you can, like what
is the word at the end of

00:21:01.541 --> 00:21:04.050
this sentence.

00:21:04.050 --> 00:21:05.320
But other times it's just

00:21:05.320 --> 00:21:06.550
you're predicting actions but if

00:21:06.550 --> 00:21:08.450
something is wrong you know it.

00:21:08.450 --> 00:21:10.280
And so part of prediction
is also detecting

00:21:10.280 --> 00:21:11.160
anomalies in the world.

00:21:11.160 --> 00:21:13.720
So we know that the brain is
detecting anomalies as well.

00:21:13.720 --> 00:21:15.140
And finally, everywhere you look in the

00:21:15.140 --> 00:21:17.033
neocortex it's generating actions.

00:21:17.033 --> 00:21:18.780
So again as I said earlier my neocortex is

00:21:18.780 --> 00:21:21.740
speaking right now and it's controlling

00:21:21.740 --> 00:21:22.720
a high-level action so

00:21:22.720 --> 00:21:23.900
other parts of the brain are also doing

00:21:23.900 --> 00:21:25.610
behavior but the neocortex is doing

00:21:25.610 --> 00:21:27.110
all sorts of high-level behavior.

00:21:27.110 --> 00:21:29.180
Okay, so this is what we want to do.

00:21:29.180 --> 00:21:31.410
We have this high velocity
data stream building the model.

00:21:31.410 --> 00:21:32.243
Now if I were to tell you the

00:21:32.243 --> 00:21:33.925
three top attributes, my top three

00:21:33.925 --> 00:21:36.260
attributes explaining the principles how

00:21:36.260 --> 00:21:38.080
the neocortex works, these are them.

00:21:38.080 --> 00:21:39.510
Starting with number one.

00:21:39.510 --> 00:21:41.240
It's a hierarchy.

00:21:41.240 --> 00:21:43.710
Although it looks like a sheet of cells,

00:21:43.710 --> 00:21:45.410
it's connected in a way that is literally

00:21:45.410 --> 00:21:47.474
like a hierarchy of regions.

00:21:47.474 --> 00:21:50.420
And this has been well
known for a long period of

00:21:50.420 --> 00:21:51.253
time here.

00:21:51.253 --> 00:21:53.320
Information flows into regions

00:21:53.320 --> 00:21:54.630
at the bottom of the hierarchy.

00:21:54.630 --> 00:21:56.470
This is a caricature drawing,
it's not a picture of

00:21:56.470 --> 00:21:57.777
a real hierarchy in a brain.

00:21:57.777 --> 00:22:00.860
It goes into the bottom of
this hierarchy and it flows

00:22:00.860 --> 00:22:02.590
up and it also flows back down and it

00:22:02.590 --> 00:22:03.720
converges, there's a little opening

00:22:03.720 --> 00:22:05.190
and it converges and it comes down.

00:22:05.190 --> 00:22:08.439
We also know that these
regions in the hierarchy are

00:22:08.439 --> 00:22:10.830
very very similar.

00:22:10.830 --> 00:22:13.300
They look the same and it is believed

00:22:13.300 --> 00:22:14.790
and we act as if they are the same.

00:22:14.790 --> 00:22:16.350
They're actually doing basically the

00:22:16.350 --> 00:22:17.716
same thing, there's variations on a theme

00:22:17.716 --> 00:22:19.950
here but the regions at different points

00:22:19.950 --> 00:22:21.140
in the hierarchy ones are connecting the

00:22:21.140 --> 00:22:22.670
vision and touch and hearing are actually

00:22:22.670 --> 00:22:24.030
all doing the same thing, they're all

00:22:24.030 --> 00:22:26.060
doing the same sort of
information processing.

00:22:26.060 --> 00:22:28.090
There's tremendous
evidence to support this.

00:22:28.090 --> 00:22:28.923
This was first proposed

00:22:28.923 --> 00:22:31.220
in 1979 by Vernon Mountcastle.

00:22:31.220 --> 00:22:33.450
So we have a hierarchy of
cortical regions that are

00:22:33.450 --> 00:22:34.470
all doing the same thing.

00:22:34.470 --> 00:22:36.580
And if they're all doing
the same thing and the whole

00:22:36.580 --> 00:22:38.360
thing does prediction model building,

00:22:38.360 --> 00:22:40.220
prediction anomaly detection and motor

00:22:40.220 --> 00:22:41.920
behavior then every region is doing

00:22:41.920 --> 00:22:42.753
those things.

00:22:42.753 --> 00:22:45.510
It's building a model of its
world, it's making predictions,

00:22:45.510 --> 00:22:47.750
detecting anomalies,
and generating behavior.

00:22:47.750 --> 00:22:49.730
So that makes your job
a little bit easier.

00:22:50.700 --> 00:22:51.671
Here's a picture of what a

00:22:51.671 --> 00:22:53.640
real hierarchy looks like in a brain.

00:22:53.640 --> 00:22:54.473
This is a famous,

00:22:54.473 --> 00:22:55.470
all the neuroscientists in the room will

00:22:55.470 --> 00:22:57.070
know this picture immediately, this is

00:22:57.070 --> 00:22:58.950
from Fellerman and Vanessen and it shows

00:22:58.950 --> 00:23:00.410
in the macaque monkey some of the

00:23:00.410 --> 00:23:02.120
regions in the visual cortex.

00:23:02.120 --> 00:23:03.450
We don't need to study this in detail,

00:23:03.450 --> 00:23:04.870
there's little colored boxes or different

00:23:04.870 --> 00:23:06.550
regions in the neocortex and you can see

00:23:06.550 --> 00:23:07.880
there's quite a few layers of them, quite

00:23:07.880 --> 00:23:09.080
a few of these guys and they're all

00:23:09.080 --> 00:23:10.620
hooked together, different species of

00:23:10.620 --> 00:23:12.160
different hierarchies, we don't really

00:23:12.160 --> 00:23:13.400
know what the humans look like but it's

00:23:13.400 --> 00:23:15.200
probably something similar to this.

00:23:15.200 --> 00:23:16.460
We're not going to go
into more detail here

00:23:16.460 --> 00:23:19.370
but this hierarchy is a fairly
complex thing and it's real.

00:23:19.370 --> 00:23:20.203
Okay.

00:23:20.203 --> 00:23:22.380
The second principle here that I argue

00:23:22.380 --> 00:23:24.170
is that the primary memory function

00:23:24.170 --> 00:23:25.820
which is going on and the cortex is a

00:23:25.820 --> 00:23:29.380
memory system, is a sequence memory.

00:23:29.380 --> 00:23:30.420
Now I want you think
about this, may not be

00:23:30.420 --> 00:23:32.814
obvious to you, but when you are trying

00:23:32.814 --> 00:23:34.470
to understand the world.

00:23:34.470 --> 00:23:35.303
For example,

00:23:35.303 --> 00:23:36.600
you're trying to understand my speech.

00:23:36.600 --> 00:23:37.810
The patterns are coming in time.

00:23:37.810 --> 00:23:39.570
It's like you're listening to a melody.

00:23:39.570 --> 00:23:41.570
The order which the
patterns occur, rapidly

00:23:41.570 --> 00:23:42.982
changing order, the rapid change you see

00:23:42.982 --> 00:23:44.740
patterns is important.

00:23:44.740 --> 00:23:46.150
It's a sequence of patterns.

00:23:46.150 --> 00:23:47.260
The same is true when I touch

00:23:47.260 --> 00:23:49.080
things and the same is true of my eyes,

00:23:49.080 --> 00:23:50.290
I'm constantly moving my eyes in the

00:23:50.290 --> 00:23:53.037
world and there's a pattern,
a sequence coming in.

00:23:53.037 --> 00:23:54.310
For you to understand

00:23:54.310 --> 00:23:55.890
or infer on the world you have to have a

00:23:55.890 --> 00:23:57.034
memory of sequences.

00:23:57.034 --> 00:23:57.980
Similarly, if I'm going

00:23:57.980 --> 00:23:59.230
to make predictions I have to have a

00:23:59.230 --> 00:24:00.850
memory of what follows next.

00:24:00.850 --> 00:24:02.480
You know, what follows
this, what is typically

00:24:02.480 --> 00:24:04.562
after this, and that's a sequence memory.

00:24:04.562 --> 00:24:06.650
If I'm going to generate motor behavior,

00:24:06.650 --> 00:24:08.004
think about what happens for me

00:24:08.004 --> 00:24:10.410
to create this speech right now.

00:24:10.410 --> 00:24:13.802
I am creating a very
fast temporal pattern on

00:24:13.802 --> 00:24:17.020
dozens of muscles in my voice box that

00:24:17.020 --> 00:24:18.890
are going in this complex pattern.

00:24:18.890 --> 00:24:21.110
I am not making this up,
I've said these words

00:24:21.110 --> 00:24:23.210
before, I've said
sentences like this before.

00:24:23.210 --> 00:24:24.520
I'm doing little variations on a theme

00:24:24.520 --> 00:24:26.720
here and there, but basically I'm playing

00:24:26.720 --> 00:24:29.600
back sequence memory of very complex

00:24:29.600 --> 00:24:31.930
sequences and we do this
at all our behavior.

00:24:31.930 --> 00:24:33.350
Sequence memory is key to

00:24:33.350 --> 00:24:34.972
understanding inference and prediction

00:24:34.972 --> 00:24:36.410
and I believe this is the key to

00:24:36.410 --> 00:24:38.630
understanding how the hierarchy works.

00:24:38.630 --> 00:24:40.390
In fact, the idea basically
is in the hierarchy

00:24:40.390 --> 00:24:41.910
is you learn sequences of sequences of

00:24:41.910 --> 00:24:45.330
sequences, you go up the
hierarchy, you build stability.

00:24:45.330 --> 00:24:46.163
Okay.

00:24:46.163 --> 00:24:47.635
The third component here is

00:24:47.635 --> 00:24:49.880
the way the brain represents information.

00:24:49.880 --> 00:24:51.409
Sparse distributed representations.

00:24:51.409 --> 00:24:53.547
It's been known for a long
time that when you look in the

00:24:53.547 --> 00:24:56.720
brain you find that at any point in time

00:24:56.720 --> 00:24:58.251
most of the cells are relatively quiet

00:24:58.251 --> 00:24:59.729
and a few of the cells are very

00:24:59.729 --> 00:25:01.336
relatively active.

00:25:01.336 --> 00:25:03.760
It's sparse, that's
what we mean by sparse.

00:25:03.760 --> 00:25:06.390
It's very few things are very active,

00:25:06.390 --> 00:25:08.140
most things are quiet, and
there's a lot of inhibition

00:25:08.140 --> 00:25:09.576
that's making this happen.

00:25:09.576 --> 00:25:12.070
It's been you know Bruno Olshausen

00:25:12.070 --> 00:25:13.470
I mentioned earlier he'd say he wrote

00:25:13.470 --> 00:25:14.730
one of the seminal papers on sparse

00:25:14.730 --> 00:25:16.580
coding in the visual system, looking at

00:25:16.580 --> 00:25:18.480
it from an efficiency point of view.

00:25:18.480 --> 00:25:20.587
But only recently that I fully understand

00:25:20.587 --> 00:25:22.920
some of the attributes of sparse
distributed representation

00:25:22.920 --> 00:25:24.110
and we're going to talk about it.

00:25:24.110 --> 00:25:25.960
So everywhere you look in the brain,

00:25:25.960 --> 00:25:27.534
even coming out of the out of the senses

00:25:27.534 --> 00:25:29.115
you see sparse activity

00:25:29.115 --> 00:25:31.540
between regions, within regions,

00:25:31.540 --> 00:25:33.130
everywhere you go it's sparse and this

00:25:33.130 --> 00:25:34.310
is not an accident.

00:25:34.310 --> 00:25:35.950
There are some properties
of sparse representations

00:25:35.950 --> 00:25:37.280
which are essential for building

00:25:37.280 --> 00:25:39.960
intelligence and I'm
going to talk about them.

00:25:39.960 --> 00:25:43.650
Okay so I'm going now go in detail

00:25:43.650 --> 00:25:44.900
on two of these items.

00:25:44.900 --> 00:25:46.350
I'm not going to talk
further about hierarchy.

00:25:46.350 --> 00:25:48.100
I'm going to talk about sparse distributed

00:25:48.100 --> 00:25:49.280
representations and I'm talking about

00:25:49.280 --> 00:25:51.220
sequence memory, and we're gonna talk

00:25:51.220 --> 00:25:53.184
about how groups of cells can learn

00:25:53.184 --> 00:25:55.667
sequences of sparse
distributed representations.

00:25:55.667 --> 00:25:56.860
So now I'm going to

00:25:56.860 --> 00:25:58.020
switch into a little bit more like a

00:25:58.020 --> 00:25:59.940
computer scientist mode of thinking here

00:25:59.940 --> 00:26:02.034
when we talk about sparse
distributed representations

00:26:02.034 --> 00:26:03.660
and I'm going to first tell

00:26:03.660 --> 00:26:05.449
you about how representations are used

00:26:05.449 --> 00:26:07.110
formed in a computer and then contrast

00:26:07.110 --> 00:26:10.240
that to sparse
representations in the brain.

00:26:10.240 --> 00:26:11.480
So in a computer we typically use

00:26:11.480 --> 00:26:13.257
something called dense representations.

00:26:13.257 --> 00:26:15.327
You might think of this, we have a few

00:26:15.327 --> 00:26:17.070
number of bits maybe eight like an a

00:26:17.070 --> 00:26:20.330
byte or 128 bits or 64
bits, something like that.

00:26:20.330 --> 00:26:21.830
We use all the combinations of

00:26:21.830 --> 00:26:23.770
ones and zeros, so if I have eight bits I

00:26:23.770 --> 00:26:26.180
can use all from 0000000

00:26:26.180 --> 00:26:28.040
into 11111111

00:26:28.040 --> 00:26:30.480
and everything in between, it's all valid.

00:26:30.480 --> 00:26:32.270
The individual bits don't mean

00:26:32.270 --> 00:26:35.253
anything, here's an example ASCII code.

00:26:35.253 --> 00:26:37.280
ASCII code is an 8-bit code for

00:26:37.280 --> 00:26:39.190
letters and so the letter M is

00:26:39.190 --> 00:26:41.420
represented by 01101101.

00:26:42.490 --> 00:26:44.101
However, if I say to you,

00:26:44.101 --> 00:26:46.680
what do those bits mean in an ASCII code?

00:26:46.680 --> 00:26:49.480
What does the third bit in that
that encode mean something?

00:26:49.480 --> 00:26:50.690
It doesn't mean anything.

00:26:50.690 --> 00:26:52.370
It says I have to look at the whole thing.

00:26:52.370 --> 00:26:53.340
There's nothing about that

00:26:53.340 --> 00:26:55.640
representation which tells me what it is.

00:26:55.640 --> 00:26:58.260
There's nothing about
those bits that say M.

00:26:58.260 --> 00:27:00.080
It's just an arbitrary assignment.

00:27:00.080 --> 00:27:01.830
The bits themselves have to be looked at

00:27:01.830 --> 00:27:03.019
as a group and some programmer decided

00:27:03.019 --> 00:27:04.850
one time that that should be an M and

00:27:04.850 --> 00:27:07.340
we're gonna and the computer
it knows nothing about that.

00:27:08.310 --> 00:27:10.060
Now these representations are assigned.

00:27:10.060 --> 00:27:11.850
They're not learned, they're just made up

00:27:11.850 --> 00:27:13.362
in some sense, and that's a great way of

00:27:13.362 --> 00:27:14.628
working on computers.

00:27:14.628 --> 00:27:16.420
In the brain we don't have that.

00:27:16.420 --> 00:27:17.880
We have sparse distributed
representations.

00:27:17.880 --> 00:27:19.460
Let me just tell you what they are.

00:27:19.460 --> 00:27:21.330
First of all you have thousands of bits.

00:27:21.330 --> 00:27:23.490
Now when I say bits you
can think of cells, okay?

00:27:23.490 --> 00:27:25.550
So when I'm talking about
bits it's the same as cells.

00:27:25.550 --> 00:27:26.383
When I say we there's

00:27:26.383 --> 00:27:28.760
a few active bits, I mean
there's a few active cells.

00:27:28.760 --> 00:27:30.640
So I might have thousands of cells

00:27:30.640 --> 00:27:31.890
or thousands of bits.

00:27:31.890 --> 00:27:33.000
You need to have a large number.

00:27:33.000 --> 00:27:36.699
We typically work with 2,000
bits in my work at Numenta.

00:27:36.699 --> 00:27:39.830
And there are mostly zeros in a few ones.

00:27:39.830 --> 00:27:43.230
We will typically use 2% activation

00:27:43.230 --> 00:27:44.820
so we'll have 40 one bits

00:27:44.820 --> 00:27:46.733
and 1960 zero bits.

00:27:46.733 --> 00:27:48.899
And so this is a sparse representation

00:27:48.899 --> 00:27:51.640
I show you example there
just showing a bunch of zeros

00:27:51.640 --> 00:27:53.160
going off to the distance here.

00:27:53.160 --> 00:27:55.480
Now the difference here
in the brain is that

00:27:55.480 --> 00:27:57.580
each bit has some sort
of semantic meaning,

00:27:57.580 --> 00:27:59.600
each bit has some meaning on its own.

00:27:59.600 --> 00:28:01.290
You just, it's not like this is

00:28:01.290 --> 00:28:03.330
an arbitrary bit and or an arbitrary cell,

00:28:03.330 --> 00:28:04.240
one moment it means this and

00:28:04.240 --> 00:28:05.870
something a moment means something else.

00:28:05.870 --> 00:28:07.250
They have some sort of semantic meaning.

00:28:07.250 --> 00:28:08.790
We may not be able to figure that out

00:28:08.790 --> 00:28:09.820
very easily but there's some sort of

00:28:09.820 --> 00:28:12.070
semantic meaning and what we do is we

00:28:12.070 --> 00:28:15.260
pick the top bits, the top semantic

00:28:15.260 --> 00:28:16.920
meanings for a representation.

00:28:16.920 --> 00:28:19.580
And this'll become clear but
let me give you an example.

00:28:19.580 --> 00:28:21.501
This is a made-up example for

00:28:21.501 --> 00:28:24.430
for analogy, it's not
the way we would do it

00:28:24.430 --> 00:28:26.000
and it's not the way
it's done in the brain.

00:28:26.000 --> 00:28:26.833
But let's say I wanted to

00:28:26.833 --> 00:28:28.310
represent a letter like the letter M in

00:28:28.310 --> 00:28:30.150
a sparse distributed representation.

00:28:30.150 --> 00:28:32.370
I would come up with 2,000 attributes.

00:28:32.370 --> 00:28:34.850
I might have attributes
for this is a vowel or

00:28:34.850 --> 00:28:35.730
this is a consonant.

00:28:35.730 --> 00:28:37.690
I might have attributes for
where in the alphabet this

00:28:37.690 --> 00:28:38.523
letter is.

00:28:38.523 --> 00:28:40.123
I might have attributes
for what it sounds like.

00:28:40.123 --> 00:28:41.383
Is it O sound or E sound,

00:28:41.383 --> 00:28:43.320
is it a hard sound or a soft sound

00:28:43.320 --> 00:28:44.600
is it a fricative sound.

00:28:44.600 --> 00:28:46.280
I might have attributes how it's drawn,

00:28:46.280 --> 00:28:48.150
is it drawn with descenders or ascenders,

00:28:48.150 --> 00:28:50.880
is it a closed shape or
an open shape and so on.

00:28:50.880 --> 00:28:52.650
And then when I wanted
to represent a letter

00:28:52.650 --> 00:28:54.220
I would pick the 40 attributes that

00:28:54.220 --> 00:28:56.170
best represent that letter and therefore

00:28:56.170 --> 00:28:57.900
the meaning of the letter, the meaning of

00:28:57.900 --> 00:28:59.244
the thing is actually incorporated in

00:28:59.244 --> 00:29:01.140
the encoding itself.

00:29:01.140 --> 00:29:03.890
And that's a really great thing to start.

00:29:03.890 --> 00:29:05.500
Now we're gonna have to
learn these meanings,

00:29:05.500 --> 00:29:07.120
the brain doesn't know these bit meanings,

00:29:07.120 --> 00:29:08.310
doesn't know what the cells represent

00:29:08.310 --> 00:29:10.290
when you're born, it has to learn them.

00:29:10.290 --> 00:29:13.330
But the basic idea is as I said.

00:29:13.330 --> 00:29:15.320
Now there's some great
properties that come

00:29:15.320 --> 00:29:17.300
with sparse distributed
representations and

00:29:17.300 --> 00:29:18.490
I'm gonna go through those, this is

00:29:18.490 --> 00:29:19.700
pretty key to understanding everything

00:29:19.700 --> 00:29:21.030
else that's going on.

00:29:21.030 --> 00:29:22.750
Let's start with the simplest one.

00:29:22.750 --> 00:29:24.303
Let's say if I wanted to
take two sparse distributed

00:29:24.303 --> 00:29:26.650
representations and
see if they're similar.

00:29:26.650 --> 00:29:28.100
I could just basically line them

00:29:28.100 --> 00:29:30.160
up and say if they have similar bits one

00:29:30.160 --> 00:29:31.860
bits in the same locations they have

00:29:31.860 --> 00:29:33.630
semantic similarity.

00:29:33.630 --> 00:29:34.940
If they don't, they're semantically

00:29:34.940 --> 00:29:36.780
different and I can see well if I have

00:29:36.780 --> 00:29:38.308
41 bits I can be anywhere between no

00:29:38.308 --> 00:29:40.580
overlap and 40 bits overlap.

00:29:40.580 --> 00:29:42.630
But I can easily compare
them and wherever I have

00:29:42.630 --> 00:29:44.830
a one bit in common is
semantically similar.

00:29:45.890 --> 00:29:46.910
The second thing is that what if

00:29:46.910 --> 00:29:48.510
I wanted to store one of these patterns

00:29:48.510 --> 00:29:49.540
and recognize it later?

00:29:49.540 --> 00:29:51.330
We're gonna certainly gonna
want to do that in the brain.

00:29:51.330 --> 00:29:52.660
We want say oh I've seen this before.

00:29:52.660 --> 00:29:53.650
What is it?

00:29:53.650 --> 00:29:54.520
How would I go about that?

00:29:54.520 --> 00:29:55.750
In a computer we might save all

00:29:55.750 --> 00:29:58.170
2,000 bits, but here we're
not going to do that.

00:29:58.170 --> 00:29:59.660
We're gonna say look let's just save the

00:29:59.660 --> 00:30:02.161
locations of the 40 bits that are one and

00:30:02.161 --> 00:30:04.530
we would say in computer language we say

00:30:04.530 --> 00:30:06.750
we store the indices of the ones.

00:30:06.750 --> 00:30:09.410
So I have a list of 40 indices saying okay

00:30:09.410 --> 00:30:11.080
what locations are the one bits and now if

00:30:11.080 --> 00:30:12.717
a new pattern comes at
the end I'll say hey

00:30:12.717 --> 00:30:14.650
do I have ones in those same locations?

00:30:14.650 --> 00:30:16.380
If I find those ones in the same location

00:30:16.380 --> 00:30:17.910
I know I have the whole
bit, the whole pattern.

00:30:17.910 --> 00:30:19.520
I don't have to look at all the bits.

00:30:19.520 --> 00:30:21.020
I only have to look at the ones

00:30:21.020 --> 00:30:23.300
that were one before
and know where they are.

00:30:23.300 --> 00:30:24.680
That's gonna work very well.

00:30:24.680 --> 00:30:26.490
But what if I came along to you and I

00:30:26.490 --> 00:30:28.040
said okay, we have a problem.

00:30:28.040 --> 00:30:31.270
We can't store the
location of all 40 bits.

00:30:31.270 --> 00:30:32.108
All 41 bits.

00:30:32.108 --> 00:30:35.040
We can only store 10 randomly.

00:30:35.040 --> 00:30:37.370
You can pick 10 randomly but that's it.

00:30:37.370 --> 00:30:39.770
And so we now have 10 indices.

00:30:39.770 --> 00:30:41.460
We call this subsampling.

00:30:41.460 --> 00:30:43.180
And you say well will that work?

00:30:43.180 --> 00:30:46.280
If I if I see those 10,

00:30:46.280 --> 00:30:47.650
I have those 10 indices, a new pattern

00:30:47.650 --> 00:30:48.960
comes in, I say, is this the one I saw

00:30:48.960 --> 00:30:50.670
before, is this the one I saved?

00:30:50.670 --> 00:30:52.260
I'll go down and say
look, the 10 ones in the

00:30:52.260 --> 00:30:54.440
right locations, I'd say that's good.

00:30:54.440 --> 00:30:55.490
And you might say well that's not good,

00:30:55.490 --> 00:30:56.323
I could be wrong.

00:30:56.323 --> 00:30:59.890
Maybe I got 10 of them right
but the other 30 are wrong.

00:30:59.890 --> 00:31:00.800
Well that could happen.

00:31:00.800 --> 00:31:02.950
It's very very unlikely to happen,

00:31:02.950 --> 00:31:05.050
but even if it did
happen what does it mean?

00:31:05.050 --> 00:31:06.470
It means I have now made a

00:31:06.470 --> 00:31:08.230
mistake but the mistake is for something

00:31:08.230 --> 00:31:11.640
semantically similar to the
thing I stored originally.

00:31:11.640 --> 00:31:14.030
Even though I didn't get
save all the bits correctly

00:31:14.030 --> 00:31:16.080
and maybe a few of them
are wrong, basically

00:31:16.080 --> 00:31:17.637
I have a very semantic similarity to the

00:31:17.637 --> 00:31:20.470
thing I saw before and that's good enough.

00:31:20.470 --> 00:31:21.500
And we're gonna take advantage of

00:31:21.500 --> 00:31:22.600
this because we're gonna be, these are

00:31:22.600 --> 00:31:23.740
actually going to be connections in the

00:31:23.740 --> 00:31:24.790
brain, we don't have to connect

00:31:24.790 --> 00:31:25.870
to everything, you only
have to connect to a

00:31:25.870 --> 00:31:27.690
small number to be pretty certain we got

00:31:27.690 --> 00:31:28.920
the right pattern.

00:31:28.920 --> 00:31:31.605
Now the following property
here is one of union.

00:31:31.605 --> 00:31:35.170
Imagine I took 10 sparse
distributed representation,

00:31:35.170 --> 00:31:37.068
10 2,000 bit patterns and I ordered them

00:31:37.068 --> 00:31:39.290
together so wherever
there's a bit I've added it

00:31:39.290 --> 00:31:41.033
up with a bit so I have 10 of these guys,

00:31:41.033 --> 00:31:43.905
each has 40 bits on or 2% activity,

00:31:43.905 --> 00:31:46.290
and I create a new one which is 2,000 bits

00:31:46.290 --> 00:31:50.420
which has about about 20% of the bits on.

00:31:50.420 --> 00:31:51.390
I just order them together.

00:31:51.390 --> 00:31:52.590
This is a one-way operation.

00:31:52.590 --> 00:31:53.423
I can't undo it.

00:31:53.423 --> 00:31:54.540
I can't say oh given this

00:31:54.540 --> 00:31:56.750
union tell me what the
individual bits were.

00:31:56.750 --> 00:31:58.290
Individual pattern, I can't do that.

00:31:58.290 --> 00:32:00.270
But I can do something very interesting.

00:32:00.270 --> 00:32:01.380
Almost as good.

00:32:01.380 --> 00:32:02.590
I can say here's a new

00:32:02.590 --> 00:32:04.170
sparseness distributed representation.

00:32:04.170 --> 00:32:06.313
Is it one of the original 10?

00:32:06.313 --> 00:32:08.880
So given the union and a
new one, is it one of the

00:32:08.880 --> 00:32:12.050
original 10, and the
answer is I can do that.

00:32:12.050 --> 00:32:13.660
I can do it very reliably.

00:32:13.660 --> 00:32:15.850
All I have to say are
the ones in the new bit

00:32:15.850 --> 00:32:17.439
in the ones in the union

00:32:17.439 --> 00:32:20.410
and if they are, then that's good.

00:32:20.410 --> 00:32:22.310
You might say well that could be an error.

00:32:22.310 --> 00:32:23.740
If you're following this
you might say look I

00:32:23.740 --> 00:32:25.700
could get maybe matching up one bit from

00:32:25.700 --> 00:32:27.060
one of those 10 and another bit from

00:32:27.060 --> 00:32:28.420
another of those 10 and so on.

00:32:28.420 --> 00:32:30.280
This is astronomically unlikely to happen.

00:32:30.280 --> 00:32:31.780
A little bit of math shows you that.

00:32:31.780 --> 00:32:33.260
And so this is a good thing.

00:32:33.260 --> 00:32:34.950
Why do we want to care about this?

00:32:34.950 --> 00:32:37.210
I said earlier the
brain makes predictions.

00:32:37.210 --> 00:32:38.043
What the brain does is

00:32:38.043 --> 00:32:39.510
actually it's predicting lots of things

00:32:39.510 --> 00:32:40.560
all the time.

00:32:40.560 --> 00:32:41.600
It's not predicting one thing,

00:32:41.600 --> 00:32:43.400
it's predicting many things and

00:32:43.400 --> 00:32:44.660
the way you make some
predictions, we're gonna see

00:32:44.660 --> 00:32:47.363
is it makes a sort of
a union of predictions

00:32:47.363 --> 00:32:49.920
and we want to know if
something unexpected happens

00:32:49.920 --> 00:32:51.880
we have to say hey what
actually happened was one

00:32:51.880 --> 00:32:53.230
of the predictions or was it not one of

00:32:53.230 --> 00:32:54.230
the predictions?

00:32:54.230 --> 00:32:55.100
So I can't always tell

00:32:55.100 --> 00:32:56.740
you what my predictions are but I can

00:32:56.740 --> 00:32:57.645
tell you if this thing that happened

00:32:57.645 --> 00:32:58.700
wasn't correct.

00:32:59.880 --> 00:33:01.270
Alright so those are our properties.

00:33:01.270 --> 00:33:02.103
If you're going to forget

00:33:02.103 --> 00:33:03.740
everything else I talk about today and

00:33:03.740 --> 00:33:05.118
tomorrow and you want to remember

00:33:05.118 --> 00:33:08.120
one thing from my talk,
we can remember this.

00:33:08.120 --> 00:33:10.340
And I'll state this right very
clear, I'm certain of this.

00:33:10.340 --> 00:33:13.555
That all intelligent
machines, biological or

00:33:13.555 --> 00:33:16.980
otherwise, are going to be
based on sparse representations.

00:33:16.980 --> 00:33:20.040
These properties are critical
and they solve some very

00:33:20.040 --> 00:33:21.205
fundamental problems that have been

00:33:21.205 --> 00:33:23.900
bothering people for a long time.

00:33:23.900 --> 00:33:26.260
So sparse distributed
representations is the language of

00:33:26.260 --> 00:33:28.570
the brain and that's
what we want to do that.

00:33:28.570 --> 00:33:29.640
Okay so now we're gonna do

00:33:29.640 --> 00:33:33.300
the next step here, we're gonna
talk about sequence memory.

00:33:33.300 --> 00:33:35.696
I already argued why it's important, but

00:33:35.696 --> 00:33:38.190
I'm gonna tell you how we learn

00:33:38.190 --> 00:33:40.890
sequences of these sparse
distributed representations.

00:33:41.780 --> 00:33:43.630
So let's just jump right into it here.

00:33:43.630 --> 00:33:45.500
We're gonna do a little
bit more neuroscience.

00:33:45.500 --> 00:33:46.790
Here's a picture of our brain again,

00:33:46.790 --> 00:33:47.740
a very light picture of our brain.

00:33:47.740 --> 00:33:48.810
We're going to zoom in our little

00:33:48.810 --> 00:33:50.280
section in the neocortex,
hopefully you can

00:33:50.280 --> 00:33:51.280
see that better.

00:33:51.280 --> 00:33:53.880
There's a picture of a little
slice of the neocortex.

00:33:53.880 --> 00:33:55.670
There you can see these layers going

00:33:55.670 --> 00:33:57.800
left to right, those are layers of cells,

00:33:57.800 --> 00:33:59.570
and you can also if you're close to this

00:33:59.570 --> 00:34:00.690
you can see there's sort of a vertical

00:34:00.690 --> 00:34:02.680
orientation of columns in there as well.

00:34:02.680 --> 00:34:03.880
We're going to zoom in one of those

00:34:03.880 --> 00:34:05.250
layers and look at a little caricature

00:34:05.250 --> 00:34:06.562
drawing that I've made here.

00:34:06.562 --> 00:34:09.510
Those little circles
represent cells in the brain.

00:34:09.510 --> 00:34:12.700
I'm just making so for
illustrative purposes.

00:34:12.700 --> 00:34:15.490
And when you look at a section
of a layer of cells in your

00:34:15.490 --> 00:34:17.870
cortex you see two principles.

00:34:17.870 --> 00:34:20.630
One principle is that, represented by the

00:34:20.630 --> 00:34:23.190
green arrow, is that there's
a columnar structure.

00:34:23.190 --> 00:34:24.570
The cells that are vertically

00:34:24.570 --> 00:34:27.330
aligned in a very skinny column tend to

00:34:27.330 --> 00:34:28.810
have the same feed-forward response

00:34:28.810 --> 00:34:31.390
properties, they tend
to behave the same way.

00:34:31.390 --> 00:34:33.310
Very few connections actually vertically,

00:34:33.310 --> 00:34:35.640
but there's a there's a vertical thing.

00:34:35.640 --> 00:34:37.060
So if I give an input to the brain,

00:34:37.060 --> 00:34:38.360
those cells will all have a similar

00:34:38.360 --> 00:34:39.936
feed-forward response property.

00:34:39.936 --> 00:34:43.300
The orange thing, the
orange arrow basically

00:34:43.300 --> 00:34:45.130
represents where most of
the connections occur.

00:34:45.130 --> 00:34:46.772
95% of the connections are just

00:34:46.772 --> 00:34:48.520
cells and columns nearby.

00:34:49.520 --> 00:34:51.910
So we have a very vertical orientation

00:34:51.910 --> 00:34:53.240
in terms of performance, all these cells

00:34:53.240 --> 00:34:54.740
in the column are doing
the same thing roughly,

00:34:54.740 --> 00:34:57.138
and then we have most of the
connections are horizontal.

00:34:57.138 --> 00:35:00.050
If I now zoom in a one of those cells,

00:35:00.920 --> 00:35:02.250
this is that little picture of the neuron

00:35:02.250 --> 00:35:04.740
I showed you earlier so we
don't to go through that again,

00:35:04.740 --> 00:35:06.020
but now I'm going to zoom in on one of

00:35:06.020 --> 00:35:09.850
the dendrites and this is a picture of a

00:35:09.850 --> 00:35:11.890
real dendrite on a neuron, and if you can

00:35:11.890 --> 00:35:13.080
I'll try to walk you through it.

00:35:13.080 --> 00:35:14.370
It's a little section.

00:35:14.370 --> 00:35:17.310
It's only probably in that
picture maybe 40 microns wide or

00:35:17.310 --> 00:35:18.972
something like that, and you can see

00:35:18.972 --> 00:35:20.950
there's a branch going on in there and

00:35:20.950 --> 00:35:22.640
you can see the actual synapses, the

00:35:22.640 --> 00:35:24.370
actual spines where the synapses connect,

00:35:24.370 --> 00:35:27.040
those are the little presented
appendages going up and down,

00:35:27.040 --> 00:35:29.740
they're really packed in
about one micron apart.

00:35:29.740 --> 00:35:32.210
Now these these spines are all over,

00:35:32.210 --> 00:35:34.420
these synapses are all
over the dendritic tree.

00:35:34.420 --> 00:35:35.380
We've learned something

00:35:35.380 --> 00:35:38.240
in recent years which is very important.

00:35:38.240 --> 00:35:41.250
That when these these dendritic

00:35:41.250 --> 00:35:43.850
regions, you can take a
little section of a dendrite.

00:35:43.850 --> 00:35:45.380
It acts in a very unusual way.

00:35:46.910 --> 00:35:47.743
When you have a bunch of

00:35:47.743 --> 00:35:49.550
synapses active at the same, if I have

00:35:49.550 --> 00:35:51.311
one synapse active, it has almost no

00:35:51.311 --> 00:35:52.983
effect on the cell body.

00:35:52.983 --> 00:35:56.570
But if I have 10 or 20 of them
active at the same time in

00:35:56.570 --> 00:35:59.300
the same location, it
has a very large effect

00:35:59.300 --> 00:36:00.320
on the cell body.

00:36:00.320 --> 00:36:02.980
It's as if they're like
a coincidence detector.

00:36:02.980 --> 00:36:04.140
It's like I've had all bunch of patterns

00:36:04.140 --> 00:36:05.510
coming in at once.

00:36:05.510 --> 00:36:08.120
It says oh that's a whole
group at once, that's good.

00:36:08.120 --> 00:36:09.160
If I only have some of them

00:36:09.160 --> 00:36:11.190
or a few of them, doesn't do that.

00:36:11.190 --> 00:36:13.800
And so this is something
that's a very, very

00:36:13.800 --> 00:36:16.025
important property and we're
going to need to exploit that

00:36:16.025 --> 00:36:17.750
in our in our model here.

00:36:19.140 --> 00:36:20.890
Just to give a little bit
more flavor for that since

00:36:20.890 --> 00:36:22.670
this is the neuroscience talk so there's

00:36:22.670 --> 00:36:24.500
a little bit depth in
it and I don't hope I

00:36:24.500 --> 00:36:25.430
don't lose too many of you here.

00:36:25.430 --> 00:36:27.860
If I look at that neuron
carefully we can actually

00:36:27.860 --> 00:36:30.620
make a distinguish between
two types of dendrites.

00:36:30.620 --> 00:36:31.710
There's the ones that are very

00:36:31.710 --> 00:36:33.510
close to the cell body, these are called

00:36:33.510 --> 00:36:35.500
proximal dendrites, and if I have an

00:36:35.500 --> 00:36:37.450
input there it has a linear effect on

00:36:37.450 --> 00:36:38.835
the cell, meaning it's like if I have one

00:36:38.835 --> 00:36:41.050
spike coming in and that the cell gets

00:36:41.050 --> 00:36:42.490
depolarized a little
bit, if I have two it's

00:36:42.490 --> 00:36:44.410
twice as much, and three
and following and so on.

00:36:44.410 --> 00:36:45.990
So it's a linear summation and this

00:36:45.990 --> 00:36:47.000
is where we find most of the

00:36:47.000 --> 00:36:49.518
feed-forward connections
to the cell occurring.

00:36:49.518 --> 00:36:52.132
Then there are the distal dendrites.

00:36:52.132 --> 00:36:53.390
These are the ones that are

00:36:53.390 --> 00:36:54.933
further away from the cell body and they

00:36:54.933 --> 00:36:57.390
work like I just said a little moment ago.

00:36:57.390 --> 00:36:58.940
There are dozens of these regions,

00:36:58.940 --> 00:37:00.700
they're nonlinear
summation, they act sort of

00:37:00.700 --> 00:37:02.530
like coincidence detectors and this

00:37:02.530 --> 00:37:03.880
is where we primarily find

00:37:03.880 --> 00:37:06.390
connections to other cells nearby.

00:37:06.390 --> 00:37:08.950
And these are these are
important properties for us.

00:37:08.950 --> 00:37:11.929
We are going to model
this with a model neuron.

00:37:11.929 --> 00:37:14.070
And this is a fairly
sophisticated model neuron,

00:37:14.070 --> 00:37:16.090
this is a picture of the
neurons we use at Numenta,

00:37:16.090 --> 00:37:18.060
and it basically models exactly

00:37:18.060 --> 00:37:19.720
what I just said there on the left.

00:37:19.720 --> 00:37:21.430
You have a cell body which is that little

00:37:21.430 --> 00:37:23.460
square, we have the green dots

00:37:23.460 --> 00:37:25.980
representing the proximal synapses and

00:37:25.980 --> 00:37:27.480
then the blue guys arranged in those

00:37:27.480 --> 00:37:29.460
little dendritic segments are representing

00:37:29.460 --> 00:37:32.480
the distal synapses and we align them as

00:37:32.480 --> 00:37:34.050
a series of dendritic segments,

00:37:34.050 --> 00:37:35.711
not in the branch like
we see in the brain but

00:37:35.711 --> 00:37:37.450
in an array like that.

00:37:37.450 --> 00:37:39.220
And has the same properties.

00:37:39.220 --> 00:37:40.440
So we're going to build now,

00:37:40.440 --> 00:37:41.570
we're going to build networks with these

00:37:41.570 --> 00:37:43.000
guys, and we're going to learn how to

00:37:43.000 --> 00:37:45.080
form sparse distributed
representations and

00:37:45.080 --> 00:37:46.420
learn sequences of them.

00:37:46.420 --> 00:37:48.180
So let's come back to this picture.

00:37:48.180 --> 00:37:50.290
We are now going to take
that model neuron I just told

00:37:50.290 --> 00:37:51.760
you about and we're going to arrange

00:37:51.760 --> 00:37:53.820
them in a layer of cells and in this

00:37:53.820 --> 00:37:55.680
picture here in the center bottom, we're

00:37:55.680 --> 00:37:57.106
showing a lot of little rectangles, a lot

00:37:57.106 --> 00:37:58.523
of little cubes, each one of those is one

00:37:58.523 --> 00:38:00.553
of our model neurons, if you will, and

00:38:00.553 --> 00:38:02.839
I've shown them in an array like we'd find

00:38:02.839 --> 00:38:05.700
in the brain and in columns that are

00:38:05.700 --> 00:38:07.950
only four cells high in
this particular case.

00:38:07.950 --> 00:38:09.670
And the colors of those cells are

00:38:09.670 --> 00:38:11.150
their activation states which I'll talk

00:38:11.150 --> 00:38:12.101
about in a moment.

00:38:12.101 --> 00:38:15.447
Okay so we want to understand
is how does this structure,

00:38:15.447 --> 00:38:19.383
using those neurons, form
sparse representations

00:38:19.383 --> 00:38:22.100
and how does it learn
sequences and how does it make

00:38:22.100 --> 00:38:23.520
predictions and how does it detect

00:38:23.520 --> 00:38:24.850
anomalies and so on.

00:38:24.850 --> 00:38:26.940
Which is I think is what
basically is going on

00:38:26.940 --> 00:38:28.124
in neural structure.

00:38:28.124 --> 00:38:32.560
Okay, so let's now zoom
in on a layer of cells.

00:38:32.560 --> 00:38:33.990
You can think of this like the layer of

00:38:33.990 --> 00:38:35.700
bits in our sparse
distributed representation

00:38:35.700 --> 00:38:36.770
but this is a two-dimensional layer of

00:38:36.770 --> 00:38:38.610
cells here, each one of those cells as I'm

00:38:38.610 --> 00:38:40.110
showing down here, and we have some

00:38:40.110 --> 00:38:41.970
feed-forward input coming from a sensor

00:38:41.970 --> 00:38:43.620
from another region in the brain going

00:38:43.620 --> 00:38:44.990
into this area.

00:38:44.990 --> 00:38:46.800
And that feed-forward
input is going on the

00:38:46.800 --> 00:38:48.860
proximal dendrites of proximal synapses

00:38:48.860 --> 00:38:50.260
if you will and what we're showing in

00:38:50.260 --> 00:38:51.880
the colorization here,
it's just one of our

00:38:51.880 --> 00:38:53.498
simulations, the colorization here is

00:38:53.498 --> 00:38:56.240
representing the level of input activity

00:38:56.240 --> 00:38:57.073
that it's getting.

00:38:57.073 --> 00:38:58.360
It's not the firing rate
of the cell, it's just a

00:38:58.360 --> 00:39:00.940
depolarization of the cell,
how much input it's getting.

00:39:00.940 --> 00:39:01.950
Some are getting more

00:39:01.950 --> 00:39:03.000
and some are getting less.

00:39:03.000 --> 00:39:05.762
They're sampling from
the input space and the

00:39:05.762 --> 00:39:08.450
ones that get the most fire first and

00:39:08.450 --> 00:39:09.830
they inhibit the other guys.

00:39:09.830 --> 00:39:11.570
And so this little
yellow thing is basically

00:39:11.570 --> 00:39:13.710
inhibitory field and basically the guys

00:39:13.710 --> 00:39:16.050
who get the most activity are the ones who

00:39:16.050 --> 00:39:17.430
inhibit everybody else.

00:39:17.430 --> 00:39:19.640
And what we have now is we
have a sparse distributed

00:39:19.640 --> 00:39:22.160
representation of our input space.

00:39:22.160 --> 00:39:23.750
I won't go through how that is learned and

00:39:23.750 --> 00:39:25.030
how it will exactly work because I want to

00:39:25.030 --> 00:39:26.650
get to the sequence memory, but this is

00:39:26.650 --> 00:39:28.560
how we ended up with this pattern.

00:39:28.560 --> 00:39:30.390
So here we have our array of cells, this

00:39:30.390 --> 00:39:31.750
is just like our zeros and ones.

00:39:31.750 --> 00:39:33.900
The white cells are zeros
and the red cells are one,

00:39:33.900 --> 00:39:35.000
those are the active ones.

00:39:35.000 --> 00:39:36.740
I'm not showing you
2,000 columns here, only

00:39:36.740 --> 00:39:38.090
about a quarter of that just to make

00:39:38.090 --> 00:39:39.600
it look visible but you can see I have a

00:39:39.600 --> 00:39:40.850
sparse activation here.

00:39:42.270 --> 00:39:45.600
Now over time the patterns
on the input change.

00:39:45.600 --> 00:39:49.680
This is time one and
okay so here's time two.

00:39:49.680 --> 00:39:52.160
So immagine now as I'm speaking, as you're

00:39:52.160 --> 00:39:54.280
doing things, these patterns are changing

00:39:54.280 --> 00:39:55.630
constantly in these cells.

00:39:55.630 --> 00:39:57.460
There are some cells becoming
active and a moment later

00:39:57.460 --> 00:39:59.110
other cells are becoming active as the

00:39:59.110 --> 00:40:00.530
patterns in the world change.

00:40:00.530 --> 00:40:03.230
And this is a sequence and we
want to learn the sequence,

00:40:03.230 --> 00:40:04.214
we want to learn like well at

00:40:04.214 --> 00:40:06.671
this point how do I pick
what's going to happen next?

00:40:06.671 --> 00:40:09.500
And we do that on a cell by cell basis

00:40:09.500 --> 00:40:10.580
and here's how.

00:40:10.580 --> 00:40:14.750
When a cell becomes active
it says who is active

00:40:14.750 --> 00:40:17.120
recently nearby because if they're

00:40:17.120 --> 00:40:18.880
active recently nearby and I see them

00:40:18.880 --> 00:40:20.938
again, I'm gonna predict my own activity.

00:40:20.938 --> 00:40:22.820
So a cell will come along and say look I

00:40:22.820 --> 00:40:24.313
just became active, let me look around

00:40:24.313 --> 00:40:26.340
and sample from the people who are

00:40:26.340 --> 00:40:28.983
nearby who was active just a moment ago.

00:40:28.983 --> 00:40:31.350
And we're going to form
connections to that and

00:40:31.350 --> 00:40:33.150
we're going to do that on our
distal dendrites down here.

00:40:33.150 --> 00:40:34.910
So this is like a coincidence detector.

00:40:34.910 --> 00:40:37.330
This is like our index of 10 patterns.

00:40:37.330 --> 00:40:39.190
We are going to basically
say I'm going to try to

00:40:39.190 --> 00:40:42.740
recognize the previous
state by forming connections

00:40:42.740 --> 00:40:45.400
to it and if I see that, I will become in

00:40:45.400 --> 00:40:47.690
a predictive state, I'll
predict my own activity.

00:40:47.690 --> 00:40:49.740
So if I see this, I'll
predict my own activity.

00:40:49.740 --> 00:40:51.404
Every cell is doing this all the time

00:40:51.404 --> 00:40:53.891
and so when a new pattern
comes in, we have all these

00:40:53.891 --> 00:40:56.980
yellow states, these cells
in a predictive state,

00:40:56.980 --> 00:40:57.870
they're all saying hey I might

00:40:57.870 --> 00:41:00.280
become active next, I
might become active next.

00:41:00.280 --> 00:41:01.380
Now in this case there's more

00:41:01.380 --> 00:41:02.980
yellow cells of predictive states than

00:41:02.980 --> 00:41:04.790
there are red ones and that's

00:41:04.790 --> 00:41:06.220
because imagine I had some patterns,

00:41:06.220 --> 00:41:08.730
I had A followed by B or I had A followed

00:41:08.730 --> 00:41:11.180
by C or I had A followed by D.

00:41:11.180 --> 00:41:14.230
And if I show you A it's
going to predict B, C, and D.

00:41:14.230 --> 00:41:15.160
It's a union of those things.

00:41:15.160 --> 00:41:16.730
So that's what you're
basically seeing happening

00:41:16.730 --> 00:41:20.130
going on here and this is the basics

00:41:20.130 --> 00:41:21.990
of a transition memory or beginning of

00:41:21.990 --> 00:41:23.730
a sequence memory because I'm learning

00:41:23.730 --> 00:41:25.920
how one state goes to another state.

00:41:26.770 --> 00:41:30.890
As I've shown it here,
it has a major flaw.

00:41:30.890 --> 00:41:32.580
It's what we call a first order memory,

00:41:32.580 --> 00:41:34.930
meaning it only can go
back one step in time

00:41:36.310 --> 00:41:39.980
and that means I can't use
information that occurred

00:41:39.980 --> 00:41:42.230
a long time ago, all I can
use is what happened earlier.

00:41:42.230 --> 00:41:45.680
Let's say my patterns were X, A, B.

00:41:45.680 --> 00:41:49.150
Or Y, A, C.

00:41:49.150 --> 00:41:50.850
Or Z, A, D.

00:41:50.850 --> 00:41:52.480
If I could go back two steps and say oh

00:41:52.480 --> 00:41:55.410
it's X followed by A,
then I can predict it's B.

00:41:55.410 --> 00:41:58.560
If it was Y followed by
A, I could predict it's C.

00:41:58.560 --> 00:42:00.320
And that would be a second order memory.

00:42:00.320 --> 00:42:03.140
We want a system that can go
arbitrarily long sequences,

00:42:03.140 --> 00:42:04.962
very long melodies if you will.

00:42:04.962 --> 00:42:07.470
And this system isn't going to do that so

00:42:07.470 --> 00:42:08.470
the way we're going to solve that

00:42:08.470 --> 00:42:10.960
problem is we're going to
add cells in the column.

00:42:10.960 --> 00:42:12.110
Essentially we're going to use the

00:42:12.110 --> 00:42:13.710
columns and multiple cells

00:42:13.710 --> 00:42:15.610
to solve this problem and

00:42:15.610 --> 00:42:17.310
let me show you how it's done.

00:42:17.310 --> 00:42:18.490
Variable order sequence memory.

00:42:18.490 --> 00:42:21.110
Let's start with our zeros and ones,

00:42:21.110 --> 00:42:23.870
our representation of a sparse
distributed representation.

00:42:23.870 --> 00:42:25.170
And I'm now going to assign

00:42:25.170 --> 00:42:26.960
instead of each of these bits being a one

00:42:26.960 --> 00:42:28.710
I'm going to make it a column of cells.

00:42:28.710 --> 00:42:31.245
So I now have 10 cells above it so each

00:42:31.245 --> 00:42:34.200
column is now one of my ones or zeros

00:42:34.200 --> 00:42:35.420
and I'm going to pick one,

00:42:35.420 --> 00:42:37.640
those are supposed to be little
circles above the ones and

00:42:37.640 --> 00:42:39.460
zeros, I don't know if it
looks like that to you.

00:42:39.460 --> 00:42:41.870
But anyway I have 10 cells
and I'm just gonna pick

00:42:41.870 --> 00:42:43.190
for every one bit,

00:42:43.190 --> 00:42:45.305
every column that's active
I'm gonna pick one cell

00:42:45.305 --> 00:42:47.250
and say that's the one that's on.

00:42:47.250 --> 00:42:48.083
And I'm gonna pick that for the

00:42:48.083 --> 00:42:50.000
another one bit and another one bit and

00:42:50.000 --> 00:42:52.140
I made an arbitrary assignment like this.

00:42:52.140 --> 00:42:53.570
I can now do the same thing again in a

00:42:53.570 --> 00:42:55.764
different representation, here
is the same representation,

00:42:55.764 --> 00:42:58.270
the same ones and zeros, but I'm going to

00:42:58.270 --> 00:43:00.570
pick a difference set
of cells in that column.

00:43:01.990 --> 00:43:02.823
Now think about this.

00:43:02.823 --> 00:43:05.080
I have now the same input
but I'm representing in

00:43:05.080 --> 00:43:06.490
two different ways.

00:43:06.490 --> 00:43:08.430
At the same time I can say
they're the same because

00:43:08.430 --> 00:43:09.750
they're the same columns but

00:43:09.750 --> 00:43:11.220
at the other hand I could

00:43:11.220 --> 00:43:12.878
say they're different
because I use different cells

00:43:12.878 --> 00:43:14.778
And even though some of the
cells might be the same,

00:43:14.778 --> 00:43:18.100
nine out of 10 will be different
and that's good enough.

00:43:18.100 --> 00:43:19.020
Think of it another way.

00:43:19.020 --> 00:43:22.640
If I have 40 active columns
of 40 bits that are one

00:43:22.640 --> 00:43:24.310
and I've 10 cells per column, there are

00:43:24.310 --> 00:43:27.280
10 to the 40th ways to
represent the same input.

00:43:27.280 --> 00:43:28.480
I have 10 to the 40th ways of

00:43:28.480 --> 00:43:30.660
representing an A or
10 to the 40th ways of

00:43:30.660 --> 00:43:32.980
repeating a pattern or note in a melody

00:43:32.980 --> 00:43:34.330
if you will.

00:43:34.330 --> 00:43:35.910
And I'll say a sentence here,

00:43:35.910 --> 00:43:38.760
I'm gonna use the sound
to in multiple ways.

00:43:38.760 --> 00:43:41.434
I'm gonna say there are
too many tutus to count.

00:43:41.434 --> 00:43:42.450
Right?

00:43:42.450 --> 00:43:44.400
Well I use the same sound coming into

00:43:44.400 --> 00:43:46.900
your head over and over
again, the to sound,

00:43:46.900 --> 00:43:48.282
but in context your brain has to

00:43:48.282 --> 00:43:50.013
represent them differently because it

00:43:50.013 --> 00:43:53.020
hears them as different, it
perceives them as different.

00:43:53.020 --> 00:43:54.240
And there has to be a way of doing that

00:43:54.240 --> 00:43:56.210
and I propose the columnar structure

00:43:56.210 --> 00:43:58.044
like this is a perfect way of doing this.

00:43:58.044 --> 00:44:01.830
And it gives us a huge capacity.

00:44:01.830 --> 00:44:03.210
So now we can go back and we do,

00:44:03.210 --> 00:44:04.200
I'm not gonna walk you through all the

00:44:04.200 --> 00:44:06.220
details of this, but if I do the same

00:44:06.220 --> 00:44:08.930
thing in a column and array of layer

00:44:08.930 --> 00:44:10.930
cells and a cell now makes connections to

00:44:10.930 --> 00:44:13.140
other cells in this larger assemblage

00:44:13.140 --> 00:44:17.930
here we basically build a high

00:44:17.930 --> 00:44:19.740
capacity sequence memory.

00:44:19.740 --> 00:44:21.741
Every cell is gonna be
predicting its next state

00:44:21.741 --> 00:44:23.490
and when they do that we can follow

00:44:23.490 --> 00:44:25.920
these and make predictions
and go through time.

00:44:25.920 --> 00:44:27.640
I'm going to give you a
little bit more flavor on it,

00:44:27.640 --> 00:44:29.810
a little bit deeper so
you can, if it gets hard

00:44:29.810 --> 00:44:32.730
you can just I'll come back
to you in a second here.

00:44:32.730 --> 00:44:33.760
Something easier.

00:44:33.760 --> 00:44:36.021
Part of this theory is that when a

00:44:36.021 --> 00:44:39.180
column is predicted or not predicted it

00:44:39.180 --> 00:44:40.310
behaves differently.

00:44:40.310 --> 00:44:43.150
So if it wasn't predicted
and a new input comes along

00:44:43.150 --> 00:44:44.420
and that column is selected,

00:44:44.420 --> 00:44:46.040
as far as all the cells
it's like I don't know

00:44:46.040 --> 00:44:47.100
which cell is supposed to be active right

00:44:47.100 --> 00:44:48.840
now, it's like I'm gonna burst 'em all,

00:44:48.840 --> 00:44:50.100
they're all become active.

00:44:50.100 --> 00:44:52.560
And this is a way like
you can sort of say hoo,

00:44:52.560 --> 00:44:55.240
unexpected thing happened or it allows

00:44:55.240 --> 00:44:56.500
you to pick up a melody in the middle of

00:44:56.500 --> 00:44:57.730
the melody, something like that.

00:44:57.730 --> 00:45:00.140
If if you had good
predictions and you knew

00:45:00.140 --> 00:45:01.450
exactly what you're tracking and you knew

00:45:01.450 --> 00:45:02.440
exactly where you're going and

00:45:02.440 --> 00:45:03.440
everything was working just like you

00:45:03.440 --> 00:45:04.790
expected, then you'd only have

00:45:04.790 --> 00:45:06.980
activations of one cell per column.

00:45:06.980 --> 00:45:08.330
We see this kind of thing when unexpected

00:45:08.330 --> 00:45:09.600
things happen in the
brain you see a lot more

00:45:09.600 --> 00:45:11.460
activity going on.

00:45:12.400 --> 00:45:14.460
So this picture here I'm

00:45:14.460 --> 00:45:15.740
going to show you, there's three columns

00:45:15.740 --> 00:45:17.160
I'm gonna highlight here.

00:45:17.160 --> 00:45:17.993
Two of them,

00:45:17.993 --> 00:45:20.180
the two on the left have one cell

00:45:20.180 --> 00:45:21.620
predicted that's the yellow state and

00:45:21.620 --> 00:45:23.160
the one on the right which is the heart

00:45:23.160 --> 00:45:25.160
of the C has no cells predicted and if

00:45:25.160 --> 00:45:26.333
all those columns become active in the

00:45:26.333 --> 00:45:28.610
next state then you'll see the one on

00:45:28.610 --> 00:45:30.090
the right, all the cells in the

00:45:30.090 --> 00:45:31.490
column become active
and the one on the left

00:45:31.490 --> 00:45:32.510
only one cell become active.

00:45:32.510 --> 00:45:34.736
So we can detect anomalies on a sort of

00:45:34.736 --> 00:45:36.770
column by column basis, this sort of

00:45:36.770 --> 00:45:38.690
attribute by attribute basis.

00:45:38.690 --> 00:45:39.523
Okay.

00:45:39.523 --> 00:45:41.760
So you put all this
together in ways that you

00:45:41.760 --> 00:45:43.080
can read about if you want and you end

00:45:43.080 --> 00:45:45.940
up with a very powerful sequence memory.

00:45:45.940 --> 00:45:47.660
It's a variable order sequence memory,

00:45:47.660 --> 00:45:49.140
it can learn very complex

00:45:49.140 --> 00:45:50.670
long temporal patterns,

00:45:50.670 --> 00:45:53.230
it makes multiple
simultaneous predictions,

00:45:53.230 --> 00:45:56.230
it can detect when what
actually happens is correct or

00:45:56.230 --> 00:45:59.420
not correct and it can
tell you why and why not.

00:45:59.420 --> 00:46:00.940
It is a high-capacity memory.

00:46:00.940 --> 00:46:02.590
I'm not going to prove that to you here or

00:46:02.590 --> 00:46:04.780
demonstrate it today but trust me that

00:46:04.780 --> 00:46:06.710
you can, a simple memory of just 2,000

00:46:06.710 --> 00:46:09.110
columns can learn millions of transitions.

00:46:09.110 --> 00:46:11.580
It is a very high capacity system.

00:46:11.580 --> 00:46:14.380
And it's very kind of
difficult to max it out.

00:46:14.380 --> 00:46:15.556
It's distributed.

00:46:15.556 --> 00:46:17.624
Think about this, there's
not a single point of

00:46:17.624 --> 00:46:19.600
failure in the system.

00:46:19.600 --> 00:46:21.950
I can eliminate columns,
I can eliminate cells,

00:46:21.950 --> 00:46:23.060
I can eliminate dendrites,

00:46:23.060 --> 00:46:25.086
I can eliminate multiple
ones of those things

00:46:25.086 --> 00:46:27.970
and it keeps on working.

00:46:27.970 --> 00:46:29.140
It grades gracefully

00:46:29.140 --> 00:46:30.670
all the way down because of that all the

00:46:30.670 --> 00:46:32.149
properties involved here.

00:46:32.149 --> 00:46:35.240
And finally it does
semantic generalization.

00:46:35.240 --> 00:46:36.240
What do I mean by that?

00:46:36.240 --> 00:46:37.180
Let's say I have a series of

00:46:37.180 --> 00:46:38.013
patterns coming in.

00:46:38.013 --> 00:46:39.890
And remember these bits
of semantic meaning,

00:46:39.890 --> 00:46:41.920
they're learning but they
have semantic meaning.

00:46:41.920 --> 00:46:43.004
And I learned some sequence of these

00:46:43.004 --> 00:46:44.830
patterns and now I give you a new

00:46:44.830 --> 00:46:47.935
pattern and it's the same sequence but

00:46:47.935 --> 00:46:49.830
actually it's different elements, they're

00:46:49.830 --> 00:46:50.663
slightly different.

00:46:50.663 --> 00:46:52.830
Different numbers of some
of them some of the bits are

00:46:52.830 --> 00:46:54.420
different, some of the
columns are different.

00:46:54.420 --> 00:46:55.389
It'll be able to say you know what,

00:46:55.389 --> 00:46:56.887
these are different patterns but I

00:46:56.887 --> 00:46:59.012
recognize the sequence, I recognize there

00:46:59.012 --> 00:47:01.570
are different spatial patterns coming in,

00:47:01.570 --> 00:47:02.670
different notes if you will, but I

00:47:02.670 --> 00:47:04.437
recognizes it's the same basic melody

00:47:04.437 --> 00:47:06.584
and I will make predictions based on

00:47:06.584 --> 00:47:09.810
previous knowledge on a
different environment.

00:47:09.810 --> 00:47:11.165
And so it generalizes in a

00:47:11.165 --> 00:47:13.990
in a semantic way which is
a very desirable attribute.

00:47:15.340 --> 00:47:16.300
Okay.

00:47:16.300 --> 00:47:18.720
Now I'm going to the third
attribute I was going

00:47:18.720 --> 00:47:20.360
to talk about as part of this theory is

00:47:20.360 --> 00:47:22.110
the requirements for online learning.

00:47:22.110 --> 00:47:23.890
You know, think about what
does online learning mean?

00:47:23.890 --> 00:47:25.410
It means that when data is

00:47:25.410 --> 00:47:27.800
coming into the brain,
you don't get the story.

00:47:27.800 --> 00:47:28.633
You know when we think about

00:47:28.633 --> 00:47:30.230
computers we bring data into a database

00:47:30.230 --> 00:47:32.020
and we look at it, but here the brain

00:47:32.020 --> 00:47:33.530
doesn't do that, the brain it comes in

00:47:33.530 --> 00:47:35.420
and has to learn right away, it has no

00:47:35.420 --> 00:47:37.170
chance to store the sensory data coming

00:47:37.170 --> 00:47:38.040
from your senses.

00:47:38.040 --> 00:47:40.570
It's like I need to infer,
I need to make predictions,

00:47:40.570 --> 00:47:42.852
and I need to learn all in one fell swoop.

00:47:42.852 --> 00:47:45.250
So online learning is the concept

00:47:45.250 --> 00:47:46.830
of that you're continuously learning.

00:47:46.830 --> 00:47:48.510
That's the machine learning term for that.

00:47:48.510 --> 00:47:50.530
So it's pretty simple actually.

00:47:50.530 --> 00:47:52.100
Essentially you have to train on

00:47:52.100 --> 00:47:54.260
all the time, all new
inputs have to be trained.

00:47:54.260 --> 00:47:55.550
It might be noise, it might be

00:47:55.550 --> 00:47:57.400
some thing you're never
going to see again,

00:47:57.400 --> 00:47:58.750
but you still have to train

00:47:58.750 --> 00:47:59.810
because it might be something you're

00:47:59.810 --> 00:48:02.200
going to see again if it's a novel thing.

00:48:02.200 --> 00:48:03.160
Essentially if a pattern

00:48:03.160 --> 00:48:04.630
does not repeat, then we're going to

00:48:04.630 --> 00:48:06.930
forget it and if a pattern does repeat

00:48:06.930 --> 00:48:08.943
then we want to strengthen
it and remember.

00:48:08.943 --> 00:48:11.660
We model this in a way

00:48:11.660 --> 00:48:14.650
similar to our interpretation of what's

00:48:14.650 --> 00:48:15.483
going on in the biology.

00:48:15.483 --> 00:48:17.730
Let me just walk you through,
here's our model again of a

00:48:17.730 --> 00:48:19.510
layer of cells, there's our model of our

00:48:19.510 --> 00:48:21.610
neuron, and then over on the right is our

00:48:21.610 --> 00:48:24.160
picture of the dendritic
and the spines on it.

00:48:24.160 --> 00:48:25.104
The one thing we've learned over the

00:48:25.104 --> 00:48:26.505
years, it used to be believed that

00:48:26.505 --> 00:48:28.420
learning only occurred in the

00:48:28.420 --> 00:48:30.300
strengthening and weakening of synapses.

00:48:30.300 --> 00:48:33.180
In fact many people
still they say as rote.

00:48:33.180 --> 00:48:34.013
But we've learned that

00:48:34.013 --> 00:48:36.140
synapses can form very rapidly.

00:48:36.140 --> 00:48:38.020
That I can have an axon
and a dendrite that are

00:48:38.020 --> 00:48:40.640
near each other and there's no synapse,

00:48:40.640 --> 00:48:42.980
but if they both fire at the same time,

00:48:42.980 --> 00:48:45.370
that a new synapse can grow very rapidly

00:48:45.370 --> 00:48:47.080
in a matter of minutes or a minute or so.

00:48:47.080 --> 00:48:48.560
I mean you can watch movies of this

00:48:48.560 --> 00:48:50.620
stuff on YouTube, it's pretty amazing.

00:48:50.620 --> 00:48:54.030
So these these synapses
can grow and ungrow.

00:48:54.030 --> 00:48:55.800
And so instead of just increasing the

00:48:55.800 --> 00:48:57.080
weight or decreasing the weight, we can

00:48:57.080 --> 00:48:58.448
grow new synapses which is a much more

00:48:58.448 --> 00:49:00.610
powerful concept and so we've adapted

00:49:00.610 --> 00:49:02.200
that in our models.

00:49:02.200 --> 00:49:03.670
Here's how we do it.

00:49:03.670 --> 00:49:06.770
We consider there's two
things about a synapse.

00:49:06.770 --> 00:49:08.810
A connection between two neuro cells.

00:49:08.810 --> 00:49:10.430
There is the growth of it and then

00:49:10.430 --> 00:49:11.880
whether it's connected.

00:49:11.880 --> 00:49:14.190
Whether it's connected, the
wait if you will, we make

00:49:14.190 --> 00:49:15.280
it binary.

00:49:15.280 --> 00:49:16.210
I'm not saying that it's binary

00:49:16.210 --> 00:49:17.370
in the brain but it's good enough for

00:49:17.370 --> 00:49:18.760
what we need to do so it's either

00:49:18.760 --> 00:49:20.440
a connector that's not connected but

00:49:20.440 --> 00:49:22.150
however it can grow on a scale,

00:49:22.150 --> 00:49:24.730
meaning I can be from no
growth to a little bit of

00:49:24.730 --> 00:49:25.950
growth and at some point once it gets

00:49:25.950 --> 00:49:27.150
above a certain threshold you say it's

00:49:27.150 --> 00:49:28.661
connected and if I continue increasing

00:49:28.661 --> 00:49:31.160
this what we call the permanence, it just

00:49:31.160 --> 00:49:32.480
gets stronger and stronger, it's harder

00:49:32.480 --> 00:49:34.050
and harder for it to forget.

00:49:34.050 --> 00:49:35.960
So by training over and over again,

00:49:35.960 --> 00:49:37.880
it's not like making the synapse stronger

00:49:37.880 --> 00:49:38.900
as much as it's making it

00:49:38.900 --> 00:49:40.740
harder to forget this thing.

00:49:40.740 --> 00:49:42.440
And there's a lot of, we didn't make this

00:49:42.440 --> 00:49:43.680
up, there's a lot of evidence for this,

00:49:43.680 --> 00:49:44.840
I read a paper once where someone

00:49:44.840 --> 00:49:47.350
suggested this idea and we've adopted it.

00:49:47.350 --> 00:49:49.800
Okay so that's basically the system

00:49:49.800 --> 00:49:50.830
we've come up with.

00:49:50.830 --> 00:49:54.320
When we simulate this stuff in our work,

00:49:54.320 --> 00:49:57.330
we typically as I said we
had like 2,000 columns,

00:49:57.330 --> 00:49:58.163
we typically use about

00:49:58.163 --> 00:50:00.750
30 cells per column, each cell has about

00:50:00.750 --> 00:50:03.470
128 dendritic segments,
each dendritic segment

00:50:03.470 --> 00:50:05.280
can have up to 40 synapses on 'em,

00:50:05.280 --> 00:50:06.570
these are numbers that are right in

00:50:06.570 --> 00:50:09.850
the range of realism on real neurons.

00:50:09.850 --> 00:50:11.350
And what you end up with then is

00:50:11.350 --> 00:50:13.100
you've got 2,000 columns

00:50:13.100 --> 00:50:15.210
in one of our simulations, 60,000 neurons,

00:50:15.210 --> 00:50:17.220
5,000 synapses per neuron, and about

00:50:17.220 --> 00:50:19.180
300 million synapses total.

00:50:19.180 --> 00:50:20.800
This would be equivalent
to a very small section of

00:50:20.800 --> 00:50:22.910
the neocortex but it turns out is

00:50:22.910 --> 00:50:25.108
a very robust and powerful tool for

00:50:25.108 --> 00:50:28.460
discovering patterns in
complex data streams.

00:50:29.510 --> 00:50:31.262
And we build we build hundreds

00:50:31.262 --> 00:50:33.110
of these models every day in my

00:50:33.110 --> 00:50:35.230
office because we're in a production

00:50:35.230 --> 00:50:36.620
system so they really work.

00:50:38.220 --> 00:50:40.384
Okay so now I'm going to
basically sort of go back

00:50:40.384 --> 00:50:41.770
to where we started from.

00:50:41.770 --> 00:50:44.250
Say well we're trying to
find a broad framework of

00:50:44.250 --> 00:50:45.660
ideas in which to interpret the

00:50:45.660 --> 00:50:48.480
neuroscience empirical data.

00:50:48.480 --> 00:50:51.448
And the question is how
good are we doing on this?

00:50:51.448 --> 00:50:53.750
And you know we have a long way to go

00:50:53.750 --> 00:50:55.470
but I'm also pretty happy with how far

00:50:55.470 --> 00:50:56.320
we've gone so far.

00:50:56.320 --> 00:50:59.120
So here's some of the
things that I didn't know

00:50:59.120 --> 00:51:00.370
30 years ago.

00:51:00.370 --> 00:51:02.960
Maybe someone else knew
them but I didn't know them.

00:51:02.960 --> 00:51:04.690
We now know that the neocortex build a

00:51:04.690 --> 00:51:06.150
predictive model of the world, it's a

00:51:06.150 --> 00:51:07.747
memory system, it's not a computer,

00:51:07.747 --> 00:51:09.920
and that system must be trained.

00:51:12.790 --> 00:51:14.920
In my book I called this
the memory prediction

00:51:14.920 --> 00:51:16.410
framework so if those who follow the

00:51:16.410 --> 00:51:18.820
work, our work, I use that
term to describe this.

00:51:18.820 --> 00:51:19.780
It's basically saying hey we

00:51:19.780 --> 00:51:22.390
have a memory system and
it makes predictions.

00:51:22.390 --> 00:51:23.420
The second thing is we know

00:51:23.420 --> 00:51:24.783
that there's a hierarchy of regions

00:51:24.783 --> 00:51:27.431
and that in those hierarchy regions

00:51:27.431 --> 00:51:31.028
I speculate that a sequence memory and

00:51:31.028 --> 00:51:32.560
with that sequence memory we can do

00:51:32.560 --> 00:51:33.910
inference or pattern recognition,

00:51:33.910 --> 00:51:36.350
prediction, anomaly detection,
and motor generation.

00:51:36.350 --> 00:51:37.183
There's a lot of things we

00:51:37.183 --> 00:51:38.780
don't know about this yet but I'm

00:51:38.780 --> 00:51:40.240
arguing that's the basic principle of

00:51:40.240 --> 00:51:42.106
what's going on in each layer in each

00:51:42.106 --> 00:51:43.680
region of the hierarchy.

00:51:45.625 --> 00:51:48.720
We call that hierarchical
temporal memory because

00:51:48.720 --> 00:51:49.938
it's basically describing the hierarchy

00:51:49.938 --> 00:51:53.090
of temporal memory regions.

00:51:53.090 --> 00:51:55.330
And then finally this is
the work I described today

00:51:55.330 --> 00:51:57.730
which is really recent,
happened the last three years,

00:51:57.730 --> 00:52:00.055
where I'm arguing that each layer of cells

00:52:00.055 --> 00:52:02.110
in a region is a sequence memory.

00:52:02.110 --> 00:52:03.890
So we've got maybe five layers of cells in

00:52:03.890 --> 00:52:05.640
typical region of cortex, each one is

00:52:05.640 --> 00:52:06.710
learning some type of sequence for

00:52:06.710 --> 00:52:08.380
various reasons, feed-forward, feedback,

00:52:08.380 --> 00:52:09.880
motor control, and so on.

00:52:09.880 --> 00:52:12.010
It's based on sparse
distributed representations,

00:52:12.010 --> 00:52:13.863
we now have a much better idea of what's

00:52:13.863 --> 00:52:17.000
sparse distributed
representations are about.

00:52:17.000 --> 00:52:18.693
It explains why there's columns

00:52:18.693 --> 00:52:21.500
and why, how those columns are

00:52:21.500 --> 00:52:22.650
representing the feed flow data.

00:52:22.650 --> 00:52:24.914
There's a lot of physical,
biophysical, excuse me,

00:52:24.914 --> 00:52:29.440
physiological data
which matches with this.

00:52:29.440 --> 00:52:32.040
It explains how cells in
the column can represent the

00:52:32.040 --> 00:52:34.450
feed-forward data in different contexts.

00:52:34.450 --> 00:52:36.330
Because I want for the first time that

00:52:36.330 --> 00:52:38.185
I'm aware of we have a model of why

00:52:38.185 --> 00:52:41.280
cells have so many, have nonlinear

00:52:41.280 --> 00:52:42.437
dendrites and how there's so many

00:52:42.437 --> 00:52:44.820
synapses on them, and why they're

00:52:44.820 --> 00:52:46.270
distributed the way they are and why

00:52:46.270 --> 00:52:48.250
they grow the way they are.

00:52:48.250 --> 00:52:51.428
And whether you, I'm pretty
confident these ideas

00:52:51.428 --> 00:52:54.723
are basically correct but
if nothing else they work

00:52:54.723 --> 00:52:56.280
and they're really cool and they do a

00:52:56.280 --> 00:52:57.570
lot of things we've been trying to do

00:52:57.570 --> 00:52:59.710
and they solve a lot of constraints.

00:52:59.710 --> 00:53:01.160
We call that the cortical learning

00:53:01.160 --> 00:53:02.830
algorithm because it's basically saying

00:53:02.830 --> 00:53:04.043
hey we think this is the basic learning

00:53:04.043 --> 00:53:06.128
mechanism that's going on between groups

00:53:06.128 --> 00:53:08.400
of cells and distributed memory

00:53:08.400 --> 00:53:10.660
formations and we're pretty excited

00:53:10.660 --> 00:53:12.170
about that.

00:53:12.170 --> 00:53:14.780
Okay, we have a long way to go but this

00:53:14.780 --> 00:53:17.500
actually, my mind it may seem like not

00:53:17.500 --> 00:53:19.460
much for 30 years, but this is actually

00:53:19.460 --> 00:53:21.140
pretty good, I'm really happy with this,

00:53:21.140 --> 00:53:22.943
and I'm gonna tell you about tomorrow

00:53:22.943 --> 00:53:25.950
how we actually build this stuff and how

00:53:25.950 --> 00:53:28.440
we're applying it to problems and some

00:53:28.440 --> 00:53:29.730
of the, you know, how do we turn

00:53:29.730 --> 00:53:31.320
this into, like, really intelligent

00:53:31.320 --> 00:53:33.250
machines but there's a long way to go

00:53:33.250 --> 00:53:35.680
there but this is enough to get started.

00:53:35.680 --> 00:53:37.630
And that's what we're trying to do.

00:53:37.630 --> 00:53:39.820
I want to end here with
just a couple pictures.

00:53:39.820 --> 00:53:40.950
This is a picture of the

00:53:40.950 --> 00:53:42.320
Redwood Neuroscience Institute on the

00:53:42.320 --> 00:53:43.930
last day before it became the Redwood

00:53:43.930 --> 00:53:45.530
Center for Theoretical Neuroscience.

00:53:45.530 --> 00:53:47.300
There's Bruno and myself and there's

00:53:47.300 --> 00:53:48.646
Tony Bell and a few other people and

00:53:48.646 --> 00:53:51.120
there's the people at Numenta now.

00:53:51.120 --> 00:53:52.940
Obviously all this work is team effort,

00:53:52.940 --> 00:53:55.450
nothing is done individually and these

00:53:55.450 --> 00:53:57.540
are the people I've worked
with over the years.

00:53:57.540 --> 00:53:58.991
So that's it thank you very much.

00:53:58.991 --> 00:54:01.408
(applauding)

00:54:06.970 --> 00:54:07.803
- Question.

00:54:07.803 --> 00:54:09.120
Where do the patterns get their

00:54:09.120 --> 00:54:11.196
representational capacity?

00:54:11.196 --> 00:54:12.710
That is, I like

00:54:12.710 --> 00:54:15.791
the example of the different
senses of the word to,

00:54:15.791 --> 00:54:18.271
the infinitive, the numeral, and

00:54:18.271 --> 00:54:20.060
of course there must be something

00:54:20.060 --> 00:54:21.453
corresponding in the brain to that and

00:54:21.453 --> 00:54:24.003
what the picture will show us there a set

00:54:24.003 --> 00:54:26.620
of patterns of neurons.

00:54:26.620 --> 00:54:29.230
Now what fact about the patterns makes

00:54:29.230 --> 00:54:33.520
them represent different
meanings of the word to?

00:54:33.520 --> 00:54:35.300
- All right so the question,

00:54:35.300 --> 00:54:36.810
I'm gonna repeat that and because everyone

00:54:36.810 --> 00:54:38.620
didn't hear it or 'cause I get to

00:54:38.620 --> 00:54:40.090
interpret it the way I want so I get to

00:54:40.090 --> 00:54:41.831
answer the question that I want to answer.

00:54:41.831 --> 00:54:43.840
(laughing)

00:54:43.840 --> 00:54:47.680
Which is how do we learn, how
do those representations of

00:54:47.680 --> 00:54:48.682
those bits, how do they come about

00:54:48.682 --> 00:54:50.890
where do those different
representations come from?

00:54:50.890 --> 00:54:53.006
The semantic meaning of those bits.

00:54:53.006 --> 00:54:55.180
What makes them represent 'em?

00:54:55.180 --> 00:54:59.380
Well first we have to realize
starting right at the senses

00:54:59.380 --> 00:55:01.034
they have they have semantic meaning.

00:55:01.034 --> 00:55:02.570
Right coming off the

00:55:02.570 --> 00:55:04.320
retina or coming off the cochlea they have

00:55:04.320 --> 00:55:05.190
semantic meaning.

00:55:05.190 --> 00:55:07.320
It may not be the kind of
meaning you think of semantic

00:55:07.320 --> 00:55:09.430
meaning, but I can say oh this ganglion

00:55:09.430 --> 00:55:12.530
cell in the cochlea represents a range

00:55:12.530 --> 00:55:14.560
of frequencies in the sound pattern and

00:55:14.560 --> 00:55:16.310
this one coming off the retina means I

00:55:16.310 --> 00:55:19.460
have an edge of, I have a certain

00:55:19.460 --> 00:55:21.240
sort of on-off pattern in this

00:55:21.240 --> 00:55:23.630
particular point of my visual space.

00:55:23.630 --> 00:55:25.280
These are sort of small semantic meanings

00:55:25.280 --> 00:55:27.730
but they're there from the
beginning and what happens in the

00:55:27.730 --> 00:55:30.130
hierarchy, and we understand this mostly

00:55:30.130 --> 00:55:32.340
but not a 100%, what
happens in the hierarchy

00:55:32.340 --> 00:55:34.905
as you as you take a whole bunch of

00:55:34.905 --> 00:55:37.466
converging sparse representations and

00:55:37.466 --> 00:55:41.220
you run it through that first activity

00:55:41.220 --> 00:55:42.870
I told you there where you project to the

00:55:42.870 --> 00:55:44.981
proximal synapses, what it does is it

00:55:44.981 --> 00:55:47.320
forms new representations that are

00:55:47.320 --> 00:55:48.970
combinations with the old ones, and it

00:55:48.970 --> 00:55:51.370
forms the representations
that are most common.

00:55:51.370 --> 00:55:52.790
So when we did this in the visual system,

00:55:52.790 --> 00:55:54.504
I didn't show this, we would end up,

00:55:54.504 --> 00:55:56.270
I would take, like, bit

00:55:56.270 --> 00:55:58.270
patterns from a retina and we'd end up

00:55:58.270 --> 00:56:00.062
forming the kind of patterns you see in

00:56:00.062 --> 00:56:02.133
V1 which are lying orientation segments

00:56:02.133 --> 00:56:03.790
or different types of patterns you'd see

00:56:03.790 --> 00:56:05.084
in the world and we tested this in

00:56:05.084 --> 00:56:06.666
various ways and so now you have a

00:56:06.666 --> 00:56:09.000
little bit more sophisticated

00:56:09.000 --> 00:56:11.660
a representation of a bit.

00:56:11.660 --> 00:56:12.918
Then we learn sequences of those.

00:56:12.918 --> 00:56:15.040
So what we now when we learn sequences

00:56:15.040 --> 00:56:16.352
of those representations, now we have

00:56:16.352 --> 00:56:18.860
cells that are essentially saying

00:56:18.860 --> 00:56:23.510
okay, this is this spatial feature in

00:56:23.510 --> 00:56:25.910
a sequence and it's a lot of like the

00:56:25.910 --> 00:56:27.810
sort of complex cells in V1 where you

00:56:27.810 --> 00:56:29.820
say okay it's anything in this movement

00:56:29.820 --> 00:56:30.860
in this direction.

00:56:30.860 --> 00:56:33.090
And you do this over and over
again in the hierarchy and as

00:56:33.090 --> 00:56:34.580
you go up the hierarchy you find more and

00:56:34.580 --> 00:56:37.190
more complex patterns.

00:56:37.190 --> 00:56:39.130
A lot of people who have been
working on this the idea for

00:56:39.130 --> 00:56:41.300
many years in vision but I think what

00:56:41.300 --> 00:56:44.210
they've really missed in a strong way is

00:56:44.210 --> 00:56:46.185
how to use time in sequence memory in

00:56:46.185 --> 00:56:47.930
addition to this combining a bit so you

00:56:47.930 --> 00:56:49.650
have these, you've this forming new

00:56:49.650 --> 00:56:51.270
spatial patterns that are the common

00:56:51.270 --> 00:56:53.330
patterns from sequences of those and new

00:56:53.330 --> 00:56:55.080
ones of those and honestly I can't say I

00:56:55.080 --> 00:56:57.390
understand it all but in the end

00:56:57.390 --> 00:56:59.380
that's how it has to come out of that.

00:56:59.380 --> 00:57:00.660
And we've been able to exhibit up to a

00:57:00.660 --> 00:57:02.940
certain level so perhaps not the fully

00:57:05.110 --> 00:57:06.480
answer you'd like but it's the best I

00:57:06.480 --> 00:57:07.313
can give today.

00:57:08.380 --> 00:57:09.810
Yes this gentleman.

00:57:09.810 --> 00:57:11.380
- [Man] Have you gained any insight

00:57:11.380 --> 00:57:13.350
into the purpose of sleep and

00:57:13.350 --> 00:57:16.140
for instance why prolonged absence of

00:57:16.140 --> 00:57:18.900
sleep distorts the mechanism of the

00:57:18.900 --> 00:57:21.404
brain and sensory processing?

00:57:21.404 --> 00:57:24.420
- You know, sleep is
another one of those areas

00:57:24.420 --> 00:57:26.550
which has huge amount of literature on.

00:57:26.550 --> 00:57:29.080
It is known very clearly that sleep is

00:57:29.080 --> 00:57:31.580
necessary to live.

00:57:31.580 --> 00:57:33.370
Sleep deprived people eventually die.

00:57:33.370 --> 00:57:36.400
It's also known to consolidate memories.

00:57:36.400 --> 00:57:38.850
There's a whole interplay
that I didn't talk about at

00:57:38.850 --> 00:57:41.780
all here between the
hippocampus and the cortex.

00:57:41.780 --> 00:57:43.020
The hippocampus is the location

00:57:43.020 --> 00:57:44.924
of episodic memories, things you learn

00:57:44.924 --> 00:57:48.250
very quickly and if you lose your

00:57:48.250 --> 00:57:49.600
hippocampus you won't form any new

00:57:49.600 --> 00:57:50.975
memories of the certain types and

00:57:50.975 --> 00:57:53.930
it's believed that sleep has a way of

00:57:53.930 --> 00:57:55.340
taking those some of the things that are

00:57:55.340 --> 00:57:57.900
stored up in the hippocampus

00:57:57.900 --> 00:58:00.321
and retraining them into the cortex.

00:58:00.321 --> 00:58:04.210
We don't model that, we don't try to.

00:58:04.210 --> 00:58:08.430
Until I find a need for it,
I'm not going to do that.

00:58:08.430 --> 00:58:10.150
Now we're not trying to emulate humans.

00:58:10.150 --> 00:58:12.000
We actually in this system
I showed you here we have no

00:58:12.000 --> 00:58:13.970
concept of episodic memory.

00:58:13.970 --> 00:58:15.890
We think we can add
that if we wanted to but

00:58:15.890 --> 00:58:17.960
there's no need to at the moment and

00:58:17.960 --> 00:58:20.150
we're able to learn these things without

00:58:20.150 --> 00:58:22.200
remembering exact details of stuff,

00:58:22.200 --> 00:58:24.250
so the short answer is
no we're not modeling,

00:58:24.250 --> 00:58:28.100
we have some ideas how we
would, and we didn't gain any

00:58:28.100 --> 00:58:29.900
insights why humans need to do that.

00:58:32.210 --> 00:58:33.480
Alright thank you again, I'm gonna be

00:58:33.480 --> 00:58:34.480
around here for a few minutes for those

00:58:34.480 --> 00:58:35.456
who want to talk to me.

00:58:35.456 --> 00:58:37.136
(applauding)

00:58:37.136 --> 00:58:39.719
(light music)

