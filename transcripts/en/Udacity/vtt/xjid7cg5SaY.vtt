WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:02.370
Let's now look at
the experimental results.

00:00:02.370 --> 00:00:05.000
We will start with
the best case numbers.

00:00:05.000 --> 00:00:09.800
To gather the best case numbers,
they used a synthetic load in which

00:00:09.800 --> 00:00:14.980
they varied the number of requests that
are issued against the web server,

00:00:14.980 --> 00:00:18.100
and every single one of the requests
is for the exact same file.

00:00:18.100 --> 00:00:19.300
Like for instance,

00:00:19.300 --> 00:00:24.140
every single one of the requests
is trying to get index.html.

00:00:24.140 --> 00:00:27.510
This is the best case
because really in reality

00:00:27.510 --> 00:00:31.290
clients will likely be asking for
different files, and

00:00:31.290 --> 00:00:35.230
in this pathological best case it's
likely basically the file will be in

00:00:35.230 --> 00:00:40.745
cash so every one of these requests
will be serviced as fast as possible.

00:00:40.745 --> 00:00:44.590
There definitely won't be any need for
any kind of disk IO.

00:00:44.590 --> 00:00:47.740
So for the best case experiments,
they measure bandwidth and

00:00:47.740 --> 00:00:52.800
they do that, they vary the file
size of zero to 200 kilobytes and

00:00:52.800 --> 00:00:57.310
they measure bandwidth as the n, the
number of requests, times the file size

00:00:57.310 --> 00:01:02.650
over the time that it takes to process
the n number of requests for this file.

00:01:03.670 --> 00:01:05.310
By varying the file size,

00:01:05.310 --> 00:01:10.040
they varied the work that both the web
server performs on each request but

00:01:10.040 --> 00:01:13.610
also the amount of bytes that
are generated on a request.

00:01:13.610 --> 00:01:17.620
You sort of assume that as we increase
the file size that the bandwidth

00:01:17.620 --> 00:01:19.060
will start increasing.

00:01:20.190 --> 00:01:22.210
So let's look at the results now.

00:01:22.210 --> 00:01:27.350
The results show the curves for every
one of the cases that they compare.

00:01:27.350 --> 00:01:31.980
The flash results are the green bar,
SPED is the single process

00:01:31.980 --> 00:01:36.930
event driven model, MT,
multi-threaded, MP, multi-process,

00:01:36.930 --> 00:01:41.580
Apache, this bottom curve, corresponds
to the Apache implementation And Zeus,

00:01:41.580 --> 00:01:43.950
that corresponds to the darker blue.

00:01:43.950 --> 00:01:48.380
This is the SPED module that
has two instances of SPED so

00:01:48.380 --> 00:01:51.350
the dual process event driven model.

00:01:51.350 --> 00:01:53.300
We can make the following observations.

00:01:53.300 --> 00:01:58.420
First, for all of the curves,
initially when the file size is small,

00:01:58.420 --> 00:02:03.390
bandwidth is slow, and as the file size
increases, the bandwidth increases.

00:02:03.390 --> 00:02:06.370
We see that all of the implementations
have very similar results.

00:02:07.660 --> 00:02:08.660
SPED is really the best.

00:02:08.660 --> 00:02:12.680
That's the single process event driven,
and that's expected because it doesn't

00:02:12.680 --> 00:02:16.960
have any threads or processes among
which it needs to context switch.

00:02:16.960 --> 00:02:21.940
Flash is similar but it performs that
extra check for the memory presence.

00:02:21.940 --> 00:02:26.140
In this case,
because this is the single file tree.

00:02:26.140 --> 00:02:30.780
So every single one of the requests is
for the single file, there's no need for

00:02:30.780 --> 00:02:31.990
blocking I/O.

00:02:31.990 --> 00:02:35.460
So none of the helper processes
will be invoked, but nonetheless,

00:02:35.460 --> 00:02:36.540
this check is performed.

00:02:36.540 --> 00:02:40.130
So that's why we see a little
bit lower performance for flash.

00:02:40.130 --> 00:02:41.330
Zeus has an anomaly.

00:02:41.330 --> 00:02:43.510
Its performance drops here a little bit,
and

00:02:43.510 --> 00:02:47.710
that has to do with some misalignment
for some of the DMA operations.

00:02:47.710 --> 00:02:52.795
So not all of the optimizations are
bug-proof in the Zeus implementation.

00:02:52.795 --> 00:02:57.720
For the multi-thread and
multi-process models, the performance

00:02:57.720 --> 00:03:02.040
is slower because of the context
switching and extra synchronization.

00:03:02.040 --> 00:03:04.140
And the performance of
Apache is the worst,

00:03:04.140 --> 00:03:08.790
because it doesn't have any
optimizations that the others implement.

00:03:08.790 --> 00:03:12.660
Now, since real clients don't
behave like the synthetic workload,

00:03:12.660 --> 00:03:16.300
we need to look at what happens
with some of the realistic traces,

00:03:16.300 --> 00:03:18.370
the Owlnet and the CS trace.

00:03:18.370 --> 00:03:20.880
Let's take a look at
the Owlnet trace first.

00:03:20.880 --> 00:03:23.270
First we see that for the Owlnet trace,

00:03:23.270 --> 00:03:27.730
the performance is very similar to
the best case with SPED and Flash

00:03:27.730 --> 00:03:33.130
being the best and then Multi-thread and
Multi-process and Apache dropping down.

00:03:33.130 --> 00:03:36.270
Note that we're not including
the Zeus performance.

00:03:36.270 --> 00:03:40.060
The reason for this trend is because
the Owlnet trace is the small trace,

00:03:40.060 --> 00:03:44.460
so most of it will fit in the cache and
we'll have a similar behavior like what

00:03:44.460 --> 00:03:49.200
we had in the best case, where all the
requests are serviced from the cache.

00:03:49.200 --> 00:03:51.940
Sometimes, however,
blocking I/O is required.

00:03:51.940 --> 00:03:54.340
It mostly fits in the cache.

00:03:54.340 --> 00:03:57.260
Given this,
given the blocking I/O possibility,

00:03:57.260 --> 00:03:59.580
SPED will occasionally block.

00:03:59.580 --> 00:04:03.450
Where as in Flash their helper processes
will help resolve the problem.

00:04:03.450 --> 00:04:06.930
And that's why we see here that the
performance of Flash is slightly higher

00:04:06.930 --> 00:04:09.320
than the performance of the SPED.

00:04:09.320 --> 00:04:12.860
Now if we take a look at what's
happening with the CS trace, this,

00:04:12.860 --> 00:04:15.070
remember, is a larger trace.

00:04:15.070 --> 00:04:16.839
So it will mostly require I/O.

00:04:16.839 --> 00:04:21.170
It's not going to fed in the cache,
in memory in the system.

00:04:21.170 --> 00:04:24.570
Since the system does not support
asynchronous I/O operations,

00:04:24.570 --> 00:04:27.770
the performance of SPED
will drop significantly.

00:04:27.770 --> 00:04:29.630
So relative to where it was,

00:04:29.630 --> 00:04:33.590
close to Flash, now it's significantly
below Flash and, in fact,

00:04:33.590 --> 00:04:37.254
it's below the multi-process and
the multi-threaded implementations.

00:04:37.254 --> 00:04:40.073
Considering the multi-thread and
the multi-process,

00:04:40.073 --> 00:04:43.581
we see that the multi-threaded is
better than the multi-process, and

00:04:43.581 --> 00:04:47.089
the main reason for that is that
the multi-threaded implementation has

00:04:47.089 --> 00:04:48.980
a smaller memory footprint.

00:04:48.980 --> 00:04:53.600
The smaller memory footprint means that
there will be more memory available to

00:04:53.600 --> 00:04:57.570
cache files,
in turn that will lead to less I/O, so

00:04:57.570 --> 00:04:59.740
this is a better implementation.

00:04:59.740 --> 00:05:02.490
In addition, the synchronization and
coordination and

00:05:02.490 --> 00:05:06.900
contact switching between threads in a
multi-thread implementation is cheaper,

00:05:06.900 --> 00:05:12.420
it happens faster than long processes
in a multi-process implementation.

00:05:12.420 --> 00:05:15.320
In all cases, Flash performs best.

00:05:15.320 --> 00:05:19.237
Again, it has the smaller memory
footprint compared to multi-threaded and

00:05:19.237 --> 00:05:22.920
the multi-process, and that results
in more memory available for

00:05:22.920 --> 00:05:25.720
caching files or caching headers.

00:05:25.720 --> 00:05:29.140
As a result of that,
fewer requests will lead to a blocking

00:05:29.140 --> 00:05:32.180
I Operation which further
speeds things up.

00:05:32.180 --> 00:05:36.180
And finally, given that everything
happens in the same address space,

00:05:36.180 --> 00:05:37.320
there isn't a need for

00:05:37.320 --> 00:05:42.260
explicit synchronization like with the
multi-threaded or multi-process model.

00:05:42.260 --> 00:05:45.860
And this is what makes Flash
perform best, in this case.

00:05:45.860 --> 00:05:51.370
In both of those cases, Apache performed
worse, so let's try to understand

00:05:51.370 --> 00:05:55.660
if there's really an impact of
the optimizations performed in Flash.

00:05:55.660 --> 00:05:58.890
And here the results represent
the different optimizations.

00:05:58.890 --> 00:06:01.790
The performance that's scattered with

00:06:01.790 --> 00:06:05.320
Flash without any optimizations
that's the bottom line.

00:06:05.320 --> 00:06:10.270
Then Flash with the path only
optimizations, so the path only, that's

00:06:10.270 --> 00:06:15.700
the directory lookup caching, so that's
like the computation caching part.

00:06:15.700 --> 00:06:18.520
Then the red line here,
the path and maps, so

00:06:18.520 --> 00:06:23.330
this includes caching of the directory
lookup plus caching of the file.

00:06:23.330 --> 00:06:26.800
And then the final bar, so
the final line, the black line,

00:06:26.800 --> 00:06:28.420
that includes all of the optimization.

00:06:28.420 --> 00:06:30.330
So this is the directory lookup,

00:06:30.330 --> 00:06:34.890
the file caching as well as
the header computations of the file.

00:06:34.890 --> 00:06:37.920
And we see that as we add
some of the optimizations,

00:06:37.920 --> 00:06:43.340
this impacts the connection rates of
the performance that can be achieved

00:06:43.340 --> 00:06:46.530
by the web server
significantly improves.

00:06:46.530 --> 00:06:49.580
We're able to sustain
a higher connection rate

00:06:49.580 --> 00:06:52.190
as we add these optimizations.

00:06:52.190 --> 00:06:53.400
This tells us two things.

00:06:53.400 --> 00:06:57.420
First, that these optimizations
are indeed very important.

00:06:57.420 --> 00:07:00.840
And second, they tell us that the
performance of Apache would have been

00:07:00.840 --> 00:07:04.390
also impacted,
if it had integrated some of

00:07:04.390 --> 00:07:06.550
these same optimizations as
the other implementations.

