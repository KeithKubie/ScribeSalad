WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.988
[MUSIC PLAYING]

00:00:07.017 --> 00:00:07.850
DIANE GREENE: Hello.

00:00:10.715 --> 00:00:12.200
FEI-FEI LI: Hi.

00:00:12.200 --> 00:00:14.470
DIANE GREENE: Who's
interested in AI?

00:00:14.470 --> 00:00:15.940
[CHEERING]

00:00:17.410 --> 00:00:19.050
Me too.

00:00:19.050 --> 00:00:20.260
Me three.

00:00:20.260 --> 00:00:20.780
OK.

00:00:20.780 --> 00:00:24.030
So I'm the moderator today.

00:00:24.030 --> 00:00:27.650
I'm Diane Greene, and
I'm running Google Cloud

00:00:27.650 --> 00:00:30.110
and on the Alphabet board.

00:00:30.110 --> 00:00:34.400
And I'm going to
briefly introduce

00:00:34.400 --> 00:00:38.990
our really amazing
guests we have here.

00:00:38.990 --> 00:00:41.150
I also live on the
Stanford campus,

00:00:41.150 --> 00:00:43.350
so I've known one of our
guests for a long time,

00:00:43.350 --> 00:00:45.740
because she's a neighbor.

00:00:45.740 --> 00:00:47.480
So let me just introduce them.

00:00:47.480 --> 00:00:52.940
First is Dr. Fei-Fei Li, and
she is the Chief Scientist

00:00:52.940 --> 00:00:54.740
for Google Cloud.

00:00:54.740 --> 00:01:01.310
She also runs AI Lab at Stanford
University, the Vision Lab,

00:01:01.310 --> 00:01:05.239
and then she also
founded SAILORS,

00:01:05.239 --> 00:01:07.760
which is now
AI4ALL, which you'll

00:01:07.760 --> 00:01:10.040
hear about a little bit later.

00:01:10.040 --> 00:01:13.650
And is there anything you
want to add to that, Fei-Fei?

00:01:13.650 --> 00:01:14.900
FEI-FEI LI: I'm your neighbor.

00:01:14.900 --> 00:01:16.325
[LAUGHTER]

00:01:18.230 --> 00:01:19.610
That's the best.

00:01:19.610 --> 00:01:26.520
DIANE GREENE: And so now
we have Greg Corrado.

00:01:26.520 --> 00:01:32.250
And actually there's
one amazing coincidence.

00:01:32.250 --> 00:01:37.010
Both Fei-Fei and Greg were
undergraduate physics majors

00:01:37.010 --> 00:01:42.530
at Princeton together
at the same time.

00:01:42.530 --> 00:01:44.900
And didn't really know
each other that well

00:01:44.900 --> 00:01:46.527
in the 18-person class.

00:01:46.527 --> 00:01:48.110
FEI-FEI LI: We were
studying too hard.

00:01:48.110 --> 00:01:51.200
GREG CORRADO: No, it was
kind of surprising to go

00:01:51.200 --> 00:01:56.150
to undergrad together, neither
of us in computer science,

00:01:56.150 --> 00:01:59.710
and then rejoin later only
once we were here at Google.

00:01:59.710 --> 00:02:03.740
DIANE GREENE: All paths lead
to AI and neural networks

00:02:03.740 --> 00:02:05.340
and so forth.

00:02:05.340 --> 00:02:08.389
But anyhow, so Greg is
the Principal Scientist

00:02:08.389 --> 00:02:09.830
in the Google Brain Group.

00:02:09.830 --> 00:02:11.870
He co-founded it.

00:02:11.870 --> 00:02:14.390
And more recently,
he's been doing

00:02:14.390 --> 00:02:18.530
a lot of amazing work in
health with neural networks

00:02:18.530 --> 00:02:19.580
and machine learning.

00:02:19.580 --> 00:02:24.590
He has a PhD in
neuroscience from Stanford.

00:02:24.590 --> 00:02:28.590
And so he came into AI in
a very interesting way.

00:02:28.590 --> 00:02:32.060
And maybe he'll talk about the
similarities between the brain

00:02:32.060 --> 00:02:33.500
and what's going on in AI.

00:02:33.500 --> 00:02:35.760
Would you like to
add anything else?

00:02:35.760 --> 00:02:37.010
GREG CORRADO: No, sounds good.

00:02:37.010 --> 00:02:38.580
DIANE GREENE: OK.

00:02:38.580 --> 00:02:41.690
So I thought since both of them
have been involved in the AI

00:02:41.690 --> 00:02:47.750
field for a while
and it's recently

00:02:47.750 --> 00:02:49.460
become a really
big deal, but it'd

00:02:49.460 --> 00:02:53.990
be nice to get a little
perspective on the history,

00:02:53.990 --> 00:02:59.040
yours in Vision and yours
in neuroscience, about AI

00:02:59.040 --> 00:03:04.760
and how it was so natural for
it to evolve to where it is now

00:03:04.760 --> 00:03:06.242
and what you're doing.

00:03:06.242 --> 00:03:07.200
And start with Fei-Fei.

00:03:07.200 --> 00:03:08.810
FEI-FEI LI: I guess I'll start.

00:03:08.810 --> 00:03:12.860
So first of all, AI is
a very nascent field

00:03:12.860 --> 00:03:17.190
in the history of science
of human civilization.

00:03:17.190 --> 00:03:20.680
This is a field of
only 60 years of age.

00:03:20.680 --> 00:03:25.120
And it started with a very, very
simple but fundamental quest--

00:03:25.120 --> 00:03:27.380
is can machines think?

00:03:27.380 --> 00:03:30.500
And we all know thinkers
and thought leaders

00:03:30.500 --> 00:03:34.370
like Alan Turing challenged
humanity with that question.

00:03:34.370 --> 00:03:36.260
Can machines think?

00:03:36.260 --> 00:03:41.850
So about 60 years ago, a group
of very pioneering scientists,

00:03:41.850 --> 00:03:45.800
computer scientists like
Marvin Minsky, John McCarthy,

00:03:45.800 --> 00:03:47.600
started really this field.

00:03:47.600 --> 00:03:51.560
In fact, John McCarthy, who
founded Stanford's AI lab,

00:03:51.560 --> 00:03:55.200
coined the very word
artificial intelligence.

00:03:55.200 --> 00:03:58.370
So where do we begin to
build machines that think?

00:03:58.370 --> 00:04:03.440
Humanity is best at
looking inward in ourselves

00:04:03.440 --> 00:04:08.460
and try to draw inspiration
from who we are.

00:04:08.460 --> 00:04:11.000
So we started thinking
about building machines that

00:04:11.000 --> 00:04:13.320
resemble human thinking.

00:04:13.320 --> 00:04:15.830
And when you think about
human intelligence,

00:04:15.830 --> 00:04:18.980
you start thinking about
different aspects like ability

00:04:18.980 --> 00:04:22.820
to reason and ability
to see and ability

00:04:22.820 --> 00:04:27.350
to hear, to speak, to move
around, make decisions,

00:04:27.350 --> 00:04:28.710
manipulate.

00:04:28.710 --> 00:04:35.830
So AI started from that very
core, foundational dream

00:04:35.830 --> 00:04:39.380
60 years ago, started
to proliferate

00:04:39.380 --> 00:04:44.970
as a field of multiple subfield,
which includes robotics,

00:04:44.970 --> 00:04:47.660
computer vision, natural
language processing,

00:04:47.660 --> 00:04:49.550
speech recognition.

00:04:49.550 --> 00:04:52.040
And then a very
important development

00:04:52.040 --> 00:04:55.140
happened around
the '80s and '90s,

00:04:55.140 --> 00:04:58.820
which is a sister field called
machine learning started

00:04:58.820 --> 00:04:59.780
to blossom.

00:04:59.780 --> 00:05:03.800
And that's a field combining
statistical learnings,

00:05:03.800 --> 00:05:06.830
statistics, with
computer science.

00:05:06.830 --> 00:05:11.870
And combining the quest of
machine intelligence, which

00:05:11.870 --> 00:05:18.200
is what AI was born out of,
with the tools and capabilities

00:05:18.200 --> 00:05:19.540
of machine learning.

00:05:19.540 --> 00:05:22.850
AI as a field went
through an extremely

00:05:22.850 --> 00:05:27.090
fruitful, productive,
blossoming period of time.

00:05:27.090 --> 00:05:33.020
And fast-forward to the
second decade of 21st century.

00:05:33.020 --> 00:05:36.950
The latest machine learning
booming that we are observing

00:05:36.950 --> 00:05:39.110
is called deep
learning, which has

00:05:39.110 --> 00:05:43.430
a deep root in neuroscience,
which I'll let you talk about.

00:05:43.430 --> 00:05:46.250
And so combining
deep learning as

00:05:46.250 --> 00:05:50.240
a powerful statistical
machine learning tool

00:05:50.240 --> 00:05:54.290
with the quest of making
machines more intelligent.

00:05:54.290 --> 00:05:59.090
Whether it's to see or is
it to hear or to speak,

00:05:59.090 --> 00:06:01.310
we're seeing this blossom.

00:06:01.310 --> 00:06:05.480
And last I just want to
say, three critical factors

00:06:05.480 --> 00:06:11.310
converged around
the last decade,

00:06:11.310 --> 00:06:17.960
which is the 2000s and the
beginning of 2010s, which are

00:06:17.960 --> 00:06:20.730
the three computing factors.

00:06:20.730 --> 00:06:23.270
One is the advance
of hardware that

00:06:23.270 --> 00:06:27.630
enabled more powerful
and capable computing.

00:06:27.630 --> 00:06:30.690
Second is the
emergence of big data,

00:06:30.690 --> 00:06:35.940
powerful data that can drive
the statistical learning

00:06:35.940 --> 00:06:37.140
algorithms.

00:06:37.140 --> 00:06:41.520
And I was lucky to be involved
myself in some of the effort.

00:06:41.520 --> 00:06:45.060
And then the third one is the
advances of machine learning

00:06:45.060 --> 00:06:47.020
and deep learning algorithms.

00:06:47.020 --> 00:06:53.130
So this convergence
of three major factors

00:06:53.130 --> 00:06:57.240
brought us the AI boom
that we're seeing today.

00:06:57.240 --> 00:07:02.070
And Google has been
investing in all three areas,

00:07:02.070 --> 00:07:04.350
honestly, earlier
than the curve.

00:07:04.350 --> 00:07:10.020
Most of the effort started
even in early 2000s.

00:07:10.020 --> 00:07:14.490
And as a company, we're
doing a lot of AI work

00:07:14.490 --> 00:07:18.120
from research to products.

00:07:18.120 --> 00:07:22.200
GREG CORRADO: And it's been
really interesting to watch

00:07:22.200 --> 00:07:26.850
the divergence in exploration
in various academic fields

00:07:26.850 --> 00:07:31.260
and then the re-convergence as
we see ideas that are aligned.

00:07:31.260 --> 00:07:34.590
So it wasn't, as Fei-Fei
says, it wasn't so long

00:07:34.590 --> 00:07:39.960
ago that fields like cognitive
science, neuroscience,

00:07:39.960 --> 00:07:42.120
artificial intelligence,
even things

00:07:42.120 --> 00:07:45.090
that we don't talk about
much more like cybernetics,

00:07:45.090 --> 00:07:48.420
were really all aligned
in a single discipline.

00:07:48.420 --> 00:07:50.700
And then they've moved
apart from each other

00:07:50.700 --> 00:07:53.220
and explored these
ideas independently

00:07:53.220 --> 00:07:55.560
for a couple of decades.

00:07:55.560 --> 00:08:00.180
And then with the renaissance
in artificial neural networks

00:08:00.180 --> 00:08:02.550
and deep learning,
we're starting

00:08:02.550 --> 00:08:05.470
to see some re-convergence.

00:08:05.470 --> 00:08:08.730
So some of these ideas
that were popular

00:08:08.730 --> 00:08:11.820
only in a small community
for a couple of decades

00:08:11.820 --> 00:08:14.430
are now coming back
into the mainstream

00:08:14.430 --> 00:08:18.300
of what artificial intelligence
is, what statistical pattern

00:08:18.300 --> 00:08:21.990
recognition is, and it's
really been delightful to see.

00:08:21.990 --> 00:08:24.370
But it's not just one idea.

00:08:24.370 --> 00:08:26.310
It's actually multiple
ideas that you

00:08:26.310 --> 00:08:30.270
see that were maintained
for a long time in fields

00:08:30.270 --> 00:08:34.600
like cognitive science that
are coming back into the fold.

00:08:34.600 --> 00:08:37.289
So another example
beyond deep learning

00:08:37.289 --> 00:08:39.280
is actually
reinforcement learning.

00:08:39.280 --> 00:08:42.990
So for the longest time, if
you looked at a university

00:08:42.990 --> 00:08:45.330
catalog of courses
and you were looking

00:08:45.330 --> 00:08:48.750
for any mention of reinforcement
learning whatsoever,

00:08:48.750 --> 00:08:50.970
you were going to find
it in a psychology

00:08:50.970 --> 00:08:53.820
department or a cognitive
science department.

00:08:53.820 --> 00:08:55.740
But today, as we
all know, we look

00:08:55.740 --> 00:08:59.370
at reinforcement learning
as a new opportunity,

00:08:59.370 --> 00:09:01.290
as something that
we actually look

00:09:01.290 --> 00:09:04.380
at for the future of AI that
might be something that's

00:09:04.380 --> 00:09:06.750
important to get
machines to really learn

00:09:06.750 --> 00:09:09.500
in completely
dynamic environments,

00:09:09.500 --> 00:09:13.710
in environments where they
have to explore entirely

00:09:13.710 --> 00:09:14.770
new stimuli.

00:09:14.770 --> 00:09:19.230
So I've been really excited to
see how this convergence has

00:09:19.230 --> 00:09:23.160
happened back in the
direction from those ideas

00:09:23.160 --> 00:09:25.650
into mainstream
computer science.

00:09:25.650 --> 00:09:28.470
And I think that there's
some hope for exchange

00:09:28.470 --> 00:09:30.120
back in the other direction.

00:09:30.120 --> 00:09:33.510
So neuroscientists and
cognitive scientists

00:09:33.510 --> 00:09:36.150
today are starting
to ask whether we

00:09:36.150 --> 00:09:39.240
can take the kind of
computer vision models

00:09:39.240 --> 00:09:45.840
that Fei-Fei helped pioneer and
use those as hypotheses for how

00:09:45.840 --> 00:09:48.690
it is that neural systems
actually compute, how

00:09:48.690 --> 00:09:52.250
our own biological brains see.

00:09:52.250 --> 00:09:54.420
And I think that
that's really exciting

00:09:54.420 --> 00:09:57.810
to see this kind of
exchange between disciplines

00:09:57.810 --> 00:10:01.612
that have been separated
for a little while.

00:10:01.612 --> 00:10:04.070
DIANE GREENE: You know, one
little piece of history I think

00:10:04.070 --> 00:10:07.010
that's also interesting
is what you did, Fei-Fei,

00:10:07.010 --> 00:10:14.090
with ImageNet, which is a nice
way of explaining building

00:10:14.090 --> 00:10:17.480
these neural networks where
you labeled all these images

00:10:17.480 --> 00:10:21.590
and then people could
refine their algorithms by--

00:10:21.590 --> 00:10:23.840
go ahead and explain
that just real quickly.

00:10:23.840 --> 00:10:24.830
FEI-FEI LI: OK, sure.

00:10:24.830 --> 00:10:30.320
So about 10 years ago, the whole
community of computer vision,

00:10:30.320 --> 00:10:35.330
which is a subfield of AI, was
working on a holy grail problem

00:10:35.330 --> 00:10:38.570
of object recognition,
which is you open your eyes,

00:10:38.570 --> 00:10:41.390
you can see the
world full of objects

00:10:41.390 --> 00:10:44.810
like flowers, chairs, people.

00:10:44.810 --> 00:10:48.410
And that's a building block
of visual intelligence

00:10:48.410 --> 00:10:50.870
and intelligence in general.

00:10:50.870 --> 00:10:54.020
And to crack that problem,
we were building, as a field,

00:10:54.020 --> 00:10:56.090
different machine
learning models.

00:10:56.090 --> 00:11:00.140
We're making small progress, but
we're hitting a lot of walls.

00:11:00.140 --> 00:11:04.460
And when my student and I
started working on this problem

00:11:04.460 --> 00:11:06.860
and started thinking
deeply about what

00:11:06.860 --> 00:11:10.580
is missing in the way we're
approaching this problem,

00:11:10.580 --> 00:11:13.310
we recognize this
important interplay

00:11:13.310 --> 00:11:17.690
between data as statistical
machine learning models.

00:11:17.690 --> 00:11:21.560
They really reinforce each other
in very deep mathematical ways

00:11:21.560 --> 00:11:25.100
that we're not going to
talk about the details here.

00:11:25.100 --> 00:11:29.450
That realization was also
inspired by human vision.

00:11:29.450 --> 00:11:31.610
If you look at how
children learn,

00:11:31.610 --> 00:11:34.670
it's a lot of learning
through big data

00:11:34.670 --> 00:11:37.130
experiences and exploration.

00:11:37.130 --> 00:11:40.850
So combining that, we
decided to put together

00:11:40.850 --> 00:11:46.010
a pretty epic
effort of we wanted

00:11:46.010 --> 00:11:49.280
to label all the images we
can get on the internet.

00:11:49.280 --> 00:11:52.040
And of course, we
Google Searched a lot

00:11:52.040 --> 00:11:54.740
and we downloaded
billions of images

00:11:54.740 --> 00:11:58.040
and used crowdsourcing
technology

00:11:58.040 --> 00:12:01.790
to label all the
images, organize them

00:12:01.790 --> 00:12:07.780
into a data set of 50
million images, organized

00:12:07.780 --> 00:12:14.840
in 22,000 categories of
objects, and put that together,

00:12:14.840 --> 00:12:16.995
and that's the ImageNet project.

00:12:16.995 --> 00:12:20.240
And we democratized it
to the research world

00:12:20.240 --> 00:12:22.940
and released the open source.

00:12:22.940 --> 00:12:28.370
And then starting
in 2010, we held

00:12:28.370 --> 00:12:32.600
an international challenge
for the whole AI community

00:12:32.600 --> 00:12:34.800
called ImageNet Challenge.

00:12:34.800 --> 00:12:37.890
And one of the
teams from Toronto,

00:12:37.890 --> 00:12:42.920
which is now at Google,
won the ImageNet Challenge

00:12:42.920 --> 00:12:46.400
with the deep learning
convolutional neural network

00:12:46.400 --> 00:12:47.240
model.

00:12:47.240 --> 00:12:49.370
And that was year 2012.

00:12:49.370 --> 00:12:53.770
And a lot of people think
the combination of ImageNet

00:12:53.770 --> 00:12:56.420
and the deep learning
model in 2012

00:12:56.420 --> 00:12:59.300
was the onset of what Greg--

00:12:59.300 --> 00:13:01.870
DIANE GREENE: A way to
compare how they were doing.

00:13:01.870 --> 00:13:03.650
And it was really good.

00:13:03.650 --> 00:13:05.410
So yeah.

00:13:05.410 --> 00:13:10.700
And so Greg, you've been doing a
lot of brain-inspired research,

00:13:10.700 --> 00:13:12.260
very interesting research.

00:13:12.260 --> 00:13:15.680
And I know you've been doing a
lot of very impactful research

00:13:15.680 --> 00:13:16.880
in the health area.

00:13:16.880 --> 00:13:19.119
Could you tell us a
little bit about that?

00:13:19.119 --> 00:13:19.910
GREG CORRADO: Sure.

00:13:19.910 --> 00:13:24.980
So I mean, I think the
ImageNet example actually

00:13:24.980 --> 00:13:28.310
sort of sets a
playbook for how we

00:13:28.310 --> 00:13:31.190
can try to approach a problem.

00:13:31.190 --> 00:13:33.950
The kind of machine
learning and AI

00:13:33.950 --> 00:13:36.680
that is most practical
and most useful today

00:13:36.680 --> 00:13:39.680
is ones where machines
learn through imitation.

00:13:39.680 --> 00:13:43.520
It's an imitation game
where if you have examples

00:13:43.520 --> 00:13:46.220
of a task being
performed correctly,

00:13:46.220 --> 00:13:48.270
the machine can learn
to imitate this.

00:13:48.270 --> 00:13:50.460
And this is called
supervised learning.

00:13:50.460 --> 00:13:53.480
And so what happened in
the image recognition

00:13:53.480 --> 00:13:59.240
case is that by Fei-Fei building
an object recognition data set,

00:13:59.240 --> 00:14:01.040
we could all focus
on that problem

00:14:01.040 --> 00:14:04.130
in a really concrete,
tractable way

00:14:04.130 --> 00:14:06.440
in order to compare
different methods.

00:14:06.440 --> 00:14:09.860
And it turned out that
methods like deep learning

00:14:09.860 --> 00:14:11.900
and artificial
neural networks were

00:14:11.900 --> 00:14:15.050
able to do something really
interesting in that space

00:14:15.050 --> 00:14:20.210
that previous machine learning
and artificial intelligence

00:14:20.210 --> 00:14:24.290
methods had not, which was that
they were able to go directly

00:14:24.290 --> 00:14:29.300
from the data to the predictions
and break the problem up

00:14:29.300 --> 00:14:33.110
into many smaller steps
without having being

00:14:33.110 --> 00:14:35.824
told exactly how to do that.

00:14:35.824 --> 00:14:38.240
So that's what we were doing
before is that we were trying

00:14:38.240 --> 00:14:42.260
to engineer features or cues,
things that we could see

00:14:42.260 --> 00:14:44.960
in the stimuli that
then we would do

00:14:44.960 --> 00:14:47.300
a little bit of statistical
learning on to figure out

00:14:47.300 --> 00:14:49.400
how to combine these signals.

00:14:49.400 --> 00:14:52.130
But with artificial neural
networks and deep learning,

00:14:52.130 --> 00:14:55.490
we're actually learning to
do those things all together.

00:14:55.490 --> 00:14:57.980
And this applies not
only to computer vision,

00:14:57.980 --> 00:15:00.350
but it applies to most
things that you could

00:15:00.350 --> 00:15:02.300
imagine a machine imitating.

00:15:02.300 --> 00:15:04.280
And so the kinds of
things that we've

00:15:04.280 --> 00:15:10.090
done like with Google Smart
Reply and now Smart Compose,

00:15:10.090 --> 00:15:12.200
we're taking that same approach.

00:15:12.200 --> 00:15:15.770
That if you have a lot of
text data, which it turns out

00:15:15.770 --> 00:15:18.230
the internet is full of,
what you can actually do

00:15:18.230 --> 00:15:21.620
is you can look at
the sequence of words

00:15:21.620 --> 00:15:28.190
so far in a conversation
or in an email exchange

00:15:28.190 --> 00:15:32.300
and try to guess
what comes next.

00:15:32.300 --> 00:15:34.610
DIANE GREENE: I'm going to
interrupt here a little bit

00:15:34.610 --> 00:15:37.020
and get a little more
provocative here.

00:15:37.020 --> 00:15:38.020
GREG CORRADO: All right.

00:15:38.020 --> 00:15:43.190
DIANE GREENE: So you're talking
about neural-inspired machine

00:15:43.190 --> 00:15:45.180
learning and so forth.

00:15:45.180 --> 00:15:50.120
And so this artificial
intelligence

00:15:50.120 --> 00:15:55.010
is kind of bringing into
question what are we humans?

00:15:55.010 --> 00:15:56.990
And then there's
this thing out there

00:15:56.990 --> 00:16:02.290
called AGI, Artificial
General Intelligence.

00:16:02.290 --> 00:16:03.710
What do you think's
going on here?

00:16:03.710 --> 00:16:05.475
Are we getting to AGI?

00:16:05.475 --> 00:16:08.340
GREG CORRADO: I
really don't think so.

00:16:08.340 --> 00:16:12.230
So there's a variety of
opinions in the community.

00:16:12.230 --> 00:16:15.110
But my feeling is
that, OK, we've finally

00:16:15.110 --> 00:16:17.240
gotten artificial
neural networks

00:16:17.240 --> 00:16:20.060
to be able to recognize
photos of cats.

00:16:20.060 --> 00:16:22.800
That's really great.

00:16:22.800 --> 00:16:25.460
We also now can--

00:16:25.460 --> 00:16:29.390
DIANE GREENE: Fei-Fei, was that
AGI when we recognized a cat?

00:16:29.390 --> 00:16:30.320
FEI-FEI LI: No.

00:16:30.320 --> 00:16:33.130
That's not enough to define AGI.

00:16:33.130 --> 00:16:35.630
GREG CORRADO: So the kind of
thing that's working well right

00:16:35.630 --> 00:16:39.140
now is this sort of
pattern recognition,

00:16:39.140 --> 00:16:42.200
this immediate response
where we're able to recognize

00:16:42.200 --> 00:16:44.030
something kind of reflexively.

00:16:44.030 --> 00:16:46.190
And we now have, I
believe, machines

00:16:46.190 --> 00:16:49.492
can do pattern recognition
every bit as well as humans can.

00:16:49.492 --> 00:16:51.200
And that's why they
can recognize objects

00:16:51.200 --> 00:16:53.408
in photos, that's why they
can do speech recognition,

00:16:53.408 --> 00:16:56.210
and that's why they can
win at a game like Go.

00:16:56.210 --> 00:17:00.410
But that is only one small
sliver, a tiny sliver,

00:17:00.410 --> 00:17:03.770
of what goes into something
like intelligence.

00:17:03.770 --> 00:17:07.010
Notions of memory and
planning and strategy

00:17:07.010 --> 00:17:09.980
and contingencies, even
emotional intelligence, these

00:17:09.980 --> 00:17:14.880
are things that we haven't
even scratched the surface.

00:17:14.880 --> 00:17:17.720
And so to me, I feel
like it's really a leap

00:17:17.720 --> 00:17:21.500
too far to imagine that
having finally cracked pattern

00:17:21.500 --> 00:17:24.650
recognition, after
some decades of trying,

00:17:24.650 --> 00:17:27.839
that we are therefore on
the verge of cracking all

00:17:27.839 --> 00:17:30.350
of these other problems that
go into what constitutes

00:17:30.350 --> 00:17:31.622
general intelligence.

00:17:31.622 --> 00:17:33.080
DIANE GREENE:
Although we have gone

00:17:33.080 --> 00:17:36.690
way faster than either of
you ever expected us to go,

00:17:36.690 --> 00:17:38.090
I believe.

00:17:38.090 --> 00:17:39.800
FEI-FEI LI: Yes and no.

00:17:39.800 --> 00:17:45.730
Humanity has a tendency
to overestimate

00:17:45.730 --> 00:17:48.140
short-term progress
and underestimate

00:17:48.140 --> 00:17:49.270
long-term progress.

00:17:49.270 --> 00:17:53.240
So eventually, we will be
achieving things that we cannot

00:17:53.240 --> 00:17:53.760
dream of.

00:17:53.760 --> 00:17:57.930
But Diane and Greg, I want
to just give a simple example

00:17:57.930 --> 00:18:00.680
to define AGI.

00:18:00.680 --> 00:18:05.960
So the definition of AGI, again,
is an introspective definition

00:18:05.960 --> 00:18:08.840
of what humans and human
intelligence can do.

00:18:08.840 --> 00:18:13.460
I have a two-year-old daughter
who doesn't like napping.

00:18:13.460 --> 00:18:17.780
And I thought I'm
smart enough to scheme

00:18:17.780 --> 00:18:21.290
to put her in a very complicated
sleeping bag that doesn't

00:18:21.290 --> 00:18:24.740
get herself out of the crib.

00:18:24.740 --> 00:18:27.680
And just a couple
of months ago, I

00:18:27.680 --> 00:18:31.230
was on the monitor watching
this kid, two-year-old,

00:18:31.230 --> 00:18:35.060
where for the first
time I was training her

00:18:35.060 --> 00:18:37.160
for napping by herself.

00:18:37.160 --> 00:18:38.660
She was very angry.

00:18:38.660 --> 00:18:42.950
So she looked around, figured
out a weak spot on the crib

00:18:42.950 --> 00:18:45.190
where she might be
able to climb out,

00:18:45.190 --> 00:18:48.680
figured out how to unzip
her complicated sleeping

00:18:48.680 --> 00:18:54.290
bag that I thought I schemed
to try to prevent that,

00:18:54.290 --> 00:18:56.390
and figured out a
way to climb out

00:18:56.390 --> 00:18:58.640
of a crib that's
way taller than who

00:18:58.640 --> 00:19:01.750
she is and managed
to escape safely

00:19:01.750 --> 00:19:05.420
and without breaking her legs.

00:19:05.420 --> 00:19:08.890
DIANE GREENE: OK, how about
AGI equivalent to my cat

00:19:08.890 --> 00:19:11.450
or equivalent to a mouse?

00:19:11.450 --> 00:19:14.133
FEI-FEI LI: If you're
shifting the definition, sure.

00:19:14.133 --> 00:19:15.770
DIANE GREENE: I see, OK.

00:19:15.770 --> 00:19:17.450
FEI-FEI LI: But
even cat, I think

00:19:17.450 --> 00:19:20.850
there are things that a
cat is capable of doing.

00:19:20.850 --> 00:19:22.700
GREG CORRADO: So I
do think that if you

00:19:22.700 --> 00:19:27.080
look at an organism like a
cat from a behavioral level,

00:19:27.080 --> 00:19:29.720
like how cats behave
and how they respond

00:19:29.720 --> 00:19:32.420
to their environments, I
think that you could imagine

00:19:32.420 --> 00:19:35.660
a world where you have
something like a toy that

00:19:35.660 --> 00:19:39.200
is for entertainment
purposes that approximates

00:19:39.200 --> 00:19:42.800
a cat in a bunch of ways in
that the sorts of behaviors

00:19:42.800 --> 00:19:45.620
that the human observe, you're
like, oh, it walks around.

00:19:45.620 --> 00:19:47.000
It doesn't bump into things.

00:19:47.000 --> 00:19:48.680
It meows at me every
once in a while.

00:19:48.680 --> 00:19:51.150
I do believe that we can
build a system like that.

00:19:51.150 --> 00:19:54.620
But what you can't do is
you can't take that robot

00:19:54.620 --> 00:19:59.990
and then dump it in the
forest and have it figure out

00:19:59.990 --> 00:20:04.600
what it needs to do in order to
survive and make things work.

00:20:04.600 --> 00:20:05.910
FEI-FEI LI: But it's a goal.

00:20:05.910 --> 00:20:07.524
It's a healthy goal.

00:20:07.524 --> 00:20:08.940
DIANE GREENE: It's
a healthy goal.

00:20:08.940 --> 00:20:13.410
And along the way, at
least we all three agree

00:20:13.410 --> 00:20:18.150
that AI's capacity to help
us solve all our big problems

00:20:18.150 --> 00:20:21.260
is going to outweigh
any kind of negative,

00:20:21.260 --> 00:20:24.660
and we're pretty excited
about that, I guess.

00:20:24.660 --> 00:20:28.620
In Cloud, you're kind of doing
some cool things with AutoML

00:20:28.620 --> 00:20:30.290
and so forth.

00:20:30.290 --> 00:20:32.790
FEI-FEI LI: Yeah,
so we talk a lot,

00:20:32.790 --> 00:20:37.980
Diane, about the belief of
building benevolent technology

00:20:37.980 --> 00:20:39.570
for human use.

00:20:39.570 --> 00:20:42.760
Our technology
reflect our values.

00:20:42.760 --> 00:20:46.590
So I personally, and I know
Greg's whole team is working

00:20:46.590 --> 00:20:52.950
on bringing AI to people and to
the fields that really need it

00:20:52.950 --> 00:20:57.180
to make a positive difference.

00:20:57.180 --> 00:21:02.610
So at Cloud, we're very lucky
to be working with customers

00:21:02.610 --> 00:21:07.200
and partners from all kinds
of vertical industries,

00:21:07.200 --> 00:21:09.900
from health care
where we collaborate,

00:21:09.900 --> 00:21:12.480
to agriculture,
to sustainability,

00:21:12.480 --> 00:21:18.300
to entertainment, to retail,
to commerce, to finance, where

00:21:18.300 --> 00:21:21.960
our customers bring some of the
toughest problem and their pain

00:21:21.960 --> 00:21:24.630
points, and we can work
with them hand-in-hand

00:21:24.630 --> 00:21:26.130
to solve some of that.

00:21:26.130 --> 00:21:30.510
So for example, recently
we rolled out AutoML.

00:21:30.510 --> 00:21:34.290
And that is the
recognition of the pain

00:21:34.290 --> 00:21:35.880
of entering machine learning.

00:21:35.880 --> 00:21:38.580
It's still a highly
technical field.

00:21:38.580 --> 00:21:40.800
The bar is still high.

00:21:40.800 --> 00:21:44.700
Not enough people
are trained experts

00:21:44.700 --> 00:21:46.650
in the world of
machine learning.

00:21:46.650 --> 00:21:51.690
But yet our industry
already has so much need

00:21:51.690 --> 00:21:57.390
to tag pictures, understand
imageries, just as an example,

00:21:57.390 --> 00:21:58.030
in vision.

00:21:58.030 --> 00:22:02.980
So how do we answer
that call of need?

00:22:02.980 --> 00:22:07.110
So we've worked hard and
thought about this suite

00:22:07.110 --> 00:22:12.840
of product called AutoML
where the customer--

00:22:12.840 --> 00:22:16.710
we lower the entry
barrier by relieving them

00:22:16.710 --> 00:22:21.450
from coding machine learning
custom models themselves.

00:22:21.450 --> 00:22:24.150
All they have to
do is to give us

00:22:24.150 --> 00:22:28.930
the kind of-- provide the kind
of data and concept they need.

00:22:28.930 --> 00:22:34.940
Here's an example of a
ramen company in Tokyo

00:22:34.940 --> 00:22:39.540
that has many shops
of ramens and they

00:22:39.540 --> 00:22:42.600
want to build an app
that recognize the ramens

00:22:42.600 --> 00:22:45.720
from different ramen stores.

00:22:45.720 --> 00:22:48.000
And they give us the
pictures of ramens

00:22:48.000 --> 00:22:50.730
and the concepts of their store.

00:22:50.730 --> 00:22:53.330
One store, two store, three.

00:22:53.330 --> 00:22:56.280
And what we do is
to use a technique,

00:22:56.280 --> 00:22:59.130
a machine learning technique
that Google and many others

00:22:59.130 --> 00:23:02.690
have developed called
learning to learn, and then

00:23:02.690 --> 00:23:09.150
build a customized model for the
customer that recognize ramens

00:23:09.150 --> 00:23:10.830
for their different stores.

00:23:10.830 --> 00:23:13.290
And then the customer
can take that model

00:23:13.290 --> 00:23:14.880
to do what they want.

00:23:14.880 --> 00:23:17.400
DIANE GREENE: I can
write a little C++,

00:23:17.400 --> 00:23:18.690
maybe some JavaScript.

00:23:18.690 --> 00:23:20.260
Could I do AutoML?

00:23:20.260 --> 00:23:21.490
FEI-FEI LI: Absolutely.

00:23:21.490 --> 00:23:22.470
Absolutely.

00:23:22.470 --> 00:23:27.450
We're working with teams that
they don't have not even C++

00:23:27.450 --> 00:23:28.290
experience.

00:23:28.290 --> 00:23:32.700
And we have a drag
and drop interface,

00:23:32.700 --> 00:23:36.270
and you can use AutoML that way.

00:23:36.270 --> 00:23:38.820
GREG CORRADO: Because I really
believe that there are so

00:23:38.820 --> 00:23:43.530
many problems that can be solved
using this technique that it's

00:23:43.530 --> 00:23:46.770
critical that we share as
much as possible about how

00:23:46.770 --> 00:23:47.520
these things work.

00:23:47.520 --> 00:23:49.860
I don't believe that
these technologies should

00:23:49.860 --> 00:23:52.500
live in walled
gardens, but instead we

00:23:52.500 --> 00:23:54.090
should develop tools
that can be used

00:23:54.090 --> 00:23:55.480
by everyone in the community.

00:23:55.480 --> 00:23:58.600
And that's part of why we have
a very aggressive open source

00:23:58.600 --> 00:24:04.440
stance to our software
packages, particularly in AI.

00:24:04.440 --> 00:24:06.210
And that includes
things like TensorFlow

00:24:06.210 --> 00:24:07.950
that are available
completely freely,

00:24:07.950 --> 00:24:09.990
and it includes the
kinds of services

00:24:09.990 --> 00:24:13.290
that are available on Cloud
to do the kind of compute,

00:24:13.290 --> 00:24:16.950
storage, and model tuning and
serving that you need to use

00:24:16.950 --> 00:24:18.420
these things in practice.

00:24:18.420 --> 00:24:21.510
And I think it's amazing
that the same tools

00:24:21.510 --> 00:24:24.430
that my applied
machine learning team

00:24:24.430 --> 00:24:27.900
uses to tackle problems
that we're interested

00:24:27.900 --> 00:24:31.410
in, those same tools are
accessible to all of you

00:24:31.410 --> 00:24:35.250
as well to try to solve the
same problems in the same way.

00:24:35.250 --> 00:24:42.060
And I've been really excited
with how great the uptake is

00:24:42.060 --> 00:24:45.120
and how we're seeing
expanding to other languages.

00:24:45.120 --> 00:24:46.560
Mentioning JavaScript.

00:24:46.560 --> 00:24:52.330
Quick plug for tensorflow.js
is actually really awesome.

00:24:52.330 --> 00:24:54.780
DIANE GREENE: Oh, and you
should probably run it on a TPU.

00:24:54.780 --> 00:24:55.905
GREG CORRADO: Yes, exactly.

00:24:58.002 --> 00:25:01.210
DIANE GREENE: It does
give a nice boost.

00:25:01.210 --> 00:25:06.910
So you're building, I mean,
with machine learning,

00:25:06.910 --> 00:25:09.280
we're bringing it to
market in so many ways,

00:25:09.280 --> 00:25:13.330
because we have the tools
to build your own models,

00:25:13.330 --> 00:25:14.380
the TensorFlow.

00:25:14.380 --> 00:25:20.210
We have the AutoML that
brings it to any programmer.

00:25:20.210 --> 00:25:23.780
And then what's going
on with all the APIs,

00:25:23.780 --> 00:25:27.230
and how is that going to
affect every industry,

00:25:27.230 --> 00:25:29.000
and what do you
see going on there?

00:25:29.000 --> 00:25:33.470
FEI-FEI LI: So Cloud
already has a suite

00:25:33.470 --> 00:25:37.280
of APIs for a lot of
our industry partners

00:25:37.280 --> 00:25:42.740
and customers, from Translate
to Speech to Vision.

00:25:42.740 --> 00:25:44.880
DIANE GREENE: Which are
based on models we build.

00:25:44.880 --> 00:25:45.986
FEI-FEI LI: Yes.

00:25:45.986 --> 00:25:50.120
For example, Box
is a major partner

00:25:50.120 --> 00:25:55.790
with Google Cloud where they
recognize a tremendous need

00:25:55.790 --> 00:26:01.910
for organizing customers'
imagery data to help customers.

00:26:01.910 --> 00:26:07.190
So they actually use Google's
Vision API to do that.

00:26:07.190 --> 00:26:12.140
And that's a model easily
delivered to our customers

00:26:12.140 --> 00:26:13.612
through our service.

00:26:13.612 --> 00:26:15.320
DIANE GREENE: Yeah,
it's pretty exciting.

00:26:15.320 --> 00:26:19.160
I mean, Greg, how do you
think that's going to play out

00:26:19.160 --> 00:26:20.420
in the health industry?

00:26:20.420 --> 00:26:22.130
I know you've been
thinking about that.

00:26:22.130 --> 00:26:24.410
GREG CORRADO: So health
care is one of the problems

00:26:24.410 --> 00:26:26.420
that a bunch of people
are working on at Google,

00:26:26.420 --> 00:26:29.600
and a lot of people are working
on outside as well, because I

00:26:29.600 --> 00:26:33.050
think there's a huge opportunity
to use these technologies

00:26:33.050 --> 00:26:36.350
to expand the availability and
the accuracy of health care.

00:26:36.350 --> 00:26:41.840
And part of that is because
doctors today are basically

00:26:41.840 --> 00:26:45.290
trying to weather an
information hurricane in order

00:26:45.290 --> 00:26:46.970
to provide care.

00:26:46.970 --> 00:26:50.570
And so I think
there are thousands

00:26:50.570 --> 00:26:55.490
of individual opportunities to
make doctors' work more fluid,

00:26:55.490 --> 00:26:58.520
to build tools to solve
problems that they want solved,

00:26:58.520 --> 00:27:01.280
and to do things
that help patients

00:27:01.280 --> 00:27:02.570
and improve patient care.

00:27:02.570 --> 00:27:05.540
DIANE GREENE: I mean, I
think you were telling me

00:27:05.540 --> 00:27:09.620
that so many doctors are
so unhappy because they

00:27:09.620 --> 00:27:12.980
have so much drudgery to do.

00:27:12.980 --> 00:27:14.420
Is this a big breakthrough?

00:27:14.420 --> 00:27:15.711
GREG CORRADO: Yeah, absolutely.

00:27:15.711 --> 00:27:18.980
I mean, I believe that
there's been a great--

00:27:18.980 --> 00:27:23.390
when you go to a doctor, you're
looking for medical attention.

00:27:23.390 --> 00:27:25.790
And right now a huge
amount of their attention

00:27:25.790 --> 00:27:28.670
is not actually focused on
the practice of medicine,

00:27:28.670 --> 00:27:31.140
but is focused on a
whole bunch of other work

00:27:31.140 --> 00:27:34.430
that they have to do
that doesn't require

00:27:34.430 --> 00:27:38.210
the kind of insights
and care and connection

00:27:38.210 --> 00:27:40.190
the real practice
of medicine does.

00:27:40.190 --> 00:27:43.580
And so I believe that
machine learning and AI

00:27:43.580 --> 00:27:46.340
is going to come
in for health care

00:27:46.340 --> 00:27:50.060
through assistive technologies
that help the doctors do

00:27:50.060 --> 00:27:52.040
what they want to do better.

00:27:52.040 --> 00:27:55.490
DIANE GREENE: By understanding
what they do in a system.

00:27:55.490 --> 00:27:57.130
No substitute for the humans.

00:27:57.130 --> 00:27:58.295
GREG CORRADO: No.

00:27:58.295 --> 00:27:59.420
FEI-FEI LI: No substitutes.

00:27:59.420 --> 00:28:01.300
DIANE GREENE: Speaking
of human, Fei-Fei,

00:28:01.300 --> 00:28:06.470
do you want to talk a
little bit about why

00:28:06.470 --> 00:28:10.612
you think this humanistic
AI approach is so critical?

00:28:10.612 --> 00:28:11.320
FEI-FEI LI: Yeah.

00:28:11.320 --> 00:28:11.930
Thank you.

00:28:11.930 --> 00:28:17.180
So if we look at the history
of AI, we've entered phase two.

00:28:17.180 --> 00:28:22.750
The first 60 years is AI as more
or less a niche technical field

00:28:22.750 --> 00:28:26.270
where we're still laying
down scientific foundations.

00:28:26.270 --> 00:28:28.610
But starting this
point on, AI is

00:28:28.610 --> 00:28:32.940
one of the biggest drivers
of societal changes to come.

00:28:32.940 --> 00:28:37.670
So how do we think about
AI in the next phase?

00:28:37.670 --> 00:28:40.340
What is the frame of mind
that should be driving

00:28:40.340 --> 00:28:42.800
us has been on top of my mind.

00:28:42.800 --> 00:28:47.090
And I think deeply about the
need for human-centered AI,

00:28:47.090 --> 00:28:52.910
which in my opinion, includes
three elements to complete

00:28:52.910 --> 00:28:56.030
the human-center AI thinking.

00:28:56.030 --> 00:29:00.160
The first element is really
advancing AI to the next stage.

00:29:00.160 --> 00:29:03.440
And here we bring our
collective background

00:29:03.440 --> 00:29:06.170
from neuroscience,
cognitive science.

00:29:06.170 --> 00:29:11.250
Whether we're getting to
AGI tomorrow or in 50 years,

00:29:11.250 --> 00:29:17.820
there is a need for AI to be
a lot more flexible, nuanced,

00:29:17.820 --> 00:29:21.680
learn faster, and
more unsupervised,

00:29:21.680 --> 00:29:26.390
semi-supervised
[INAUDIBLE] learning ways

00:29:26.390 --> 00:29:28.370
to be able to
understand emotion,

00:29:28.370 --> 00:29:30.840
to be able to
communicate with humans.

00:29:30.840 --> 00:29:32.960
So that is the more
human-centered way

00:29:32.960 --> 00:29:36.260
of advancing AI science.

00:29:36.260 --> 00:29:39.650
The second part is the
human-center AI technology

00:29:39.650 --> 00:29:43.910
and application is that I love
what you're saying that there's

00:29:43.910 --> 00:29:45.990
no substitute for humans.

00:29:45.990 --> 00:29:48.740
This technology,
like all technology,

00:29:48.740 --> 00:29:53.810
is to enhance humans, to augment
humans, not to replace humans.

00:29:53.810 --> 00:29:55.970
We'll replace certain tasks.

00:29:55.970 --> 00:30:00.830
We'll replace humans out of
danger or tasks that we cannot

00:30:00.830 --> 00:30:01.880
perform.

00:30:01.880 --> 00:30:06.440
But the bottom line is we can
use AI to help our doctors,

00:30:06.440 --> 00:30:09.830
to help our disaster
relief workers,

00:30:09.830 --> 00:30:12.650
to help decision makers.

00:30:12.650 --> 00:30:16.310
So there is a lot of
technology in robotics,

00:30:16.310 --> 00:30:20.140
in design, in natural
language processing that

00:30:20.140 --> 00:30:22.690
is centered around
human-centered AI

00:30:22.690 --> 00:30:24.760
technology and application.

00:30:24.760 --> 00:30:27.340
The third element
of human-centered AI

00:30:27.340 --> 00:30:30.640
is really to combine
the thinking of AI

00:30:30.640 --> 00:30:34.690
as a technology as well
as the societal impact.

00:30:34.690 --> 00:30:39.500
We are so nascent in seeing
the impact of this technology.

00:30:39.500 --> 00:30:42.340
But already, like
Diane said, that we

00:30:42.340 --> 00:30:45.310
are seeing the impact
in different ways, ways

00:30:45.310 --> 00:30:47.350
that we might not even predict.

00:30:47.350 --> 00:30:49.210
So I think it's
really important.

00:30:49.210 --> 00:30:51.700
And it's a responsibility
of everyone

00:30:51.700 --> 00:30:54.520
from academia to
industry to government

00:30:54.520 --> 00:30:57.830
to bring social
scientists, philosophers,

00:30:57.830 --> 00:31:02.170
law scholars, policy
makers, ethicists,

00:31:02.170 --> 00:31:08.930
and historians at the table and
to study more deeply about AI's

00:31:08.930 --> 00:31:11.280
social and humanistic impact.

00:31:11.280 --> 00:31:16.510
And that is the three
elements of human-centered AI.

00:31:16.510 --> 00:31:18.620
DIANE GREENE: That's
pretty wonderful.

00:31:18.620 --> 00:31:23.080
And I think we at Google here,
Alphabet, are working as hard

00:31:23.080 --> 00:31:27.100
as we can to do humanistic AI.

00:31:27.100 --> 00:31:31.930
You mentioned what we need
to be careful about out there

00:31:31.930 --> 00:31:33.580
with AI and regulatory.

00:31:33.580 --> 00:31:36.640
What are some of
the barriers to--

00:31:36.640 --> 00:31:38.920
I think every
company in the world

00:31:38.920 --> 00:31:41.930
has a use for AI
in many, many ways.

00:31:41.930 --> 00:31:44.770
I mean, it's just exploding
in all the verticals.

00:31:44.770 --> 00:31:47.650
But there are some
impediments to adoption.

00:31:47.650 --> 00:31:51.310
For example, in the
financial industry

00:31:51.310 --> 00:31:54.940
they need to have something
called explainable AI.

00:31:54.940 --> 00:31:57.790
And could you just talk about
some of the different barriers

00:31:57.790 --> 00:32:01.420
you see to being able
to take advantage of AI?

00:32:01.420 --> 00:32:03.614
FEI-FEI LI: We should
start with health care.

00:32:03.614 --> 00:32:05.635
GREG CORRADO: Yeah,
so I think that there

00:32:05.635 --> 00:32:07.760
are a bunch of really
important things to consider.

00:32:07.760 --> 00:32:10.000
So one of the things
is, of course, we

00:32:10.000 --> 00:32:14.650
want to have machine
learning systems that

00:32:14.650 --> 00:32:18.820
are designed to fit the
needs of the folks that are

00:32:18.820 --> 00:32:20.230
using them and applying them.

00:32:20.230 --> 00:32:24.280
And that can often include
not just giving me the answer,

00:32:24.280 --> 00:32:28.270
but telling me something
about how that was derived.

00:32:28.270 --> 00:32:29.910
So some kind of explainability.

00:32:29.910 --> 00:32:32.560
So in the health care
space, for example,

00:32:32.560 --> 00:32:35.410
we've been working on a bunch
of things in medical imaging.

00:32:35.410 --> 00:32:38.590
And it's not acceptable to
just tell the doctor that,

00:32:38.590 --> 00:32:41.020
oh, something looks
fishy in this x-ray

00:32:41.020 --> 00:32:43.570
or this pathology slide
or this retinal scan.

00:32:43.570 --> 00:32:46.910
You have to tell them, well,
what do you think is wrong?

00:32:46.910 --> 00:32:48.700
But more importantly,
you actually

00:32:48.700 --> 00:32:51.010
have to show them
where in the image

00:32:51.010 --> 00:32:53.020
you think the evidence
for that conclusion

00:32:53.020 --> 00:32:55.270
lies so that they
can then look at it

00:32:55.270 --> 00:32:58.710
and decide whether they
concur or they disagree

00:32:58.710 --> 00:33:01.030
or, oh, well, there was
a speck of dust there

00:33:01.030 --> 00:33:03.740
and that's what the
machine is picking up on.

00:33:03.740 --> 00:33:07.290
And the good news is that these
things actually are possible.

00:33:07.290 --> 00:33:12.100
And I think there's kind of
been this unfortunate mythology

00:33:12.100 --> 00:33:16.180
that AI and deep learning in
particular is a black box.

00:33:16.180 --> 00:33:18.850
And it really isn't.

00:33:18.850 --> 00:33:21.910
We didn't study how it worked,
because for a long time

00:33:21.910 --> 00:33:23.369
it really didn't work that well.

00:33:23.369 --> 00:33:24.910
But now that it's
working well, there

00:33:24.910 --> 00:33:26.284
are a lot of tools
and techniques

00:33:26.284 --> 00:33:29.380
that go into examining
how these systems work.

00:33:29.380 --> 00:33:32.530
And I think explainability
is a big part of it

00:33:32.530 --> 00:33:35.980
in terms of making these
things available for a bunch

00:33:35.980 --> 00:33:37.060
of applications.

00:33:37.060 --> 00:33:39.850
FEI-FEI LI: So in addition
to the explainability,

00:33:39.850 --> 00:33:42.250
I would add bias.

00:33:42.250 --> 00:33:46.120
I think bias is an issue
we need to address in AI.

00:33:46.120 --> 00:33:51.220
And I see bias, from where I
sit, two major kind of bias

00:33:51.220 --> 00:33:52.490
we need to address.

00:33:52.490 --> 00:33:55.750
One is the pipeline
of AI development,

00:33:55.750 --> 00:33:58.330
starting from the
bias of the data

00:33:58.330 --> 00:34:00.500
to the outcome of the bias.

00:34:00.500 --> 00:34:05.140
And we have heard a lot
about if the machine learning

00:34:05.140 --> 00:34:12.760
algorithm is fed with data that
does not represent the problem

00:34:12.760 --> 00:34:17.480
domain in a fair way,
we will introduce bias.

00:34:17.480 --> 00:34:22.510
Whether it's missing a
group of people's data

00:34:22.510 --> 00:34:28.540
or biasing it to a
skewed distribution,

00:34:28.540 --> 00:34:32.310
those are things that would
have deep consequences,

00:34:32.310 --> 00:34:35.110
whether you're in the health
care domain or finance

00:34:35.110 --> 00:34:37.159
or legal decision making.

00:34:37.159 --> 00:34:43.389
So I think that is a huge
issue very nicely that Google

00:34:43.389 --> 00:34:44.889
is already addressing that.

00:34:44.889 --> 00:34:48.299
We have a whole team at
Google working on bias.

00:34:48.299 --> 00:34:49.090
DIANE GREENE: Yeah.

00:34:49.090 --> 00:34:50.370
That's true.

00:34:50.370 --> 00:34:53.139
FEI-FEI LI: And another
bias I think is important

00:34:53.139 --> 00:34:55.530
is the people who
are developing AIs.

00:34:55.530 --> 00:35:00.610
The human bias and the lack of
diversity is also another bias.

00:35:00.610 --> 00:35:02.060
DIANE GREENE: It's so important.

00:35:02.060 --> 00:35:05.710
And that kind of brings
me to maybe some of our--

00:35:05.710 --> 00:35:07.430
we're getting close to the end.

00:35:07.430 --> 00:35:13.240
But where is AI going?

00:35:13.240 --> 00:35:15.640
I mean, how prevalent
is it going to be?

00:35:15.640 --> 00:35:18.640
I mean, we look at our
universities and these machine

00:35:18.640 --> 00:35:22.770
learning classes have
800 people, 900 people.

00:35:22.770 --> 00:35:24.130
There is such a demand.

00:35:24.130 --> 00:35:27.560
Every computer science
graduate wants to know it.

00:35:27.560 --> 00:35:28.420
Where is it going?

00:35:28.420 --> 00:35:32.410
I mean, will every high
school graduating senior

00:35:32.410 --> 00:35:37.510
be able to customize AI
to their own purposes?

00:35:37.510 --> 00:35:43.000
And what does it look like
five, 10 years from now?

00:35:45.580 --> 00:35:48.620
FEI-FEI LI: So from a
technology point of view,

00:35:48.620 --> 00:35:52.980
I think that because of
the tremendous investment

00:35:52.980 --> 00:35:55.230
in resource, both in
the private sector

00:35:55.230 --> 00:35:58.980
as well in the
public sector now,

00:35:58.980 --> 00:36:03.420
many countries are waking
up to investing AI,

00:36:03.420 --> 00:36:08.760
we're going to see a
huge continue development

00:36:08.760 --> 00:36:10.020
of AI technology.

00:36:10.020 --> 00:36:13.250
I'm mostly excited
either at Cloud

00:36:13.250 --> 00:36:15.810
or seeing what
Greg's team is doing,

00:36:15.810 --> 00:36:19.590
AI being delivered to the
industries that really

00:36:19.590 --> 00:36:25.230
matter to people's lives and the
work quality and productivity.

00:36:25.230 --> 00:36:27.150
But Diane, I think
you're also asking

00:36:27.150 --> 00:36:33.501
is how are we educating
more people in AI?

00:36:33.501 --> 00:36:35.250
DIANE GREENE: Both
making it easier to use

00:36:35.250 --> 00:36:39.990
and educating them and
what's it going to look like?

00:36:39.990 --> 00:36:42.630
What do you predict?

00:36:42.630 --> 00:36:44.500
FEI-FEI LI: That's a
really tough question,

00:36:44.500 --> 00:36:47.340
because at the core of
today's AI is still calculus.

00:36:47.340 --> 00:36:48.920
And that's not going to change.

00:36:48.920 --> 00:36:54.870
GREG CORRADO: So I think that
from the kind of tech industry

00:36:54.870 --> 00:36:58.530
perspective or from the computer
science education perspective,

00:36:58.530 --> 00:37:01.770
I think that we're going
to see AI and ML become

00:37:01.770 --> 00:37:05.190
as essential as networking is.

00:37:05.190 --> 00:37:06.932
No one really thinks
about, oh, well,

00:37:06.932 --> 00:37:08.640
I'm going to write
some software and it's

00:37:08.640 --> 00:37:10.980
going to be standalone on
a box and it's not going

00:37:10.980 --> 00:37:12.199
to have a TCPI connection.

00:37:12.199 --> 00:37:13.740
We all know that
you're going to have

00:37:13.740 --> 00:37:16.110
a TCPI connection at the
end of the day somewhere.

00:37:16.110 --> 00:37:19.740
And everyone understands the
basics of the networking stack.

00:37:19.740 --> 00:37:23.740
And that's not just at
the level of engineers.

00:37:23.740 --> 00:37:27.900
That's the level of
designers, of executives,

00:37:27.900 --> 00:37:31.489
of product developers
and leaders.

00:37:31.489 --> 00:37:33.030
And the same thing,
I think, is going

00:37:33.030 --> 00:37:35.160
to happen with machine
learning and AI, which

00:37:35.160 --> 00:37:38.190
is that designers are going to
start to understand, how can I

00:37:38.190 --> 00:37:42.690
make a completely revolutionary
kind of product that folds

00:37:42.690 --> 00:37:46.660
in machine learning the same
way that we fold in networking

00:37:46.660 --> 00:37:49.830
and internet technologies into
almost everything we build?

00:37:49.830 --> 00:37:52.320
So I think we're going
to see tremendous uptake

00:37:52.320 --> 00:37:54.880
and it becoming kind of
a pervasive background

00:37:54.880 --> 00:37:56.640
part of the technologies.

00:37:56.640 --> 00:37:59.310
But I think in that
process the ways

00:37:59.310 --> 00:38:01.720
that we use AI are
going to evolve.

00:38:01.720 --> 00:38:03.240
So I think right
now we're seeing

00:38:03.240 --> 00:38:06.360
a lot of things where
AI and machine learning

00:38:06.360 --> 00:38:11.370
add some spice, some extra, a
little coolness on a feature.

00:38:11.370 --> 00:38:13.680
And I think that what
you're going to see over

00:38:13.680 --> 00:38:15.510
the next decade is
you're going to see more

00:38:15.510 --> 00:38:19.050
of a core integration into
what it means for the product

00:38:19.050 --> 00:38:20.152
to actually work.

00:38:20.152 --> 00:38:22.110
And I think that one of
the great opportunities

00:38:22.110 --> 00:38:25.260
there is actually going
to be the development

00:38:25.260 --> 00:38:28.860
of artificial
emotional intelligence

00:38:28.860 --> 00:38:32.820
that allows products to actually
have much more natural and much

00:38:32.820 --> 00:38:34.860
more fluid human interaction.

00:38:34.860 --> 00:38:37.950
We're beginning to see that in
the Assistant now with speech

00:38:37.950 --> 00:38:40.710
recognition, speech
synthesis, understanding

00:38:40.710 --> 00:38:42.390
dialogues and exchanges.

00:38:42.390 --> 00:38:45.720
But I think that this
is still in its infancy.

00:38:45.720 --> 00:38:48.360
We're going to get to a
point where the products

00:38:48.360 --> 00:38:51.180
that we build, they interact
with humans in the way

00:38:51.180 --> 00:38:55.590
that the humans find most
useful just out of the box.

00:38:55.590 --> 00:38:58.260
FEI-FEI LI: And I spend a lot
of time with high schoolers,

00:38:58.260 --> 00:39:01.200
because I really
believe in the future.

00:39:01.200 --> 00:39:03.920
We always talk about
AI changing the world.

00:39:03.920 --> 00:39:08.340
And I always say the question
is, who is changing AI?

00:39:08.340 --> 00:39:12.960
And to me, bringing more
human mission thinking

00:39:12.960 --> 00:39:16.170
into technology development
and thought leadership

00:39:16.170 --> 00:39:17.670
is really important.

00:39:17.670 --> 00:39:19.890
Not only important
for the future

00:39:19.890 --> 00:39:24.030
of our technology and the value
we instill in our technology,

00:39:24.030 --> 00:39:27.660
but also in bringing the
diverse group of students

00:39:27.660 --> 00:39:32.130
and future leaders into
the development of AI.

00:39:32.130 --> 00:39:37.770
So at [? Server ?] at Google,
we all work a lot on this issue.

00:39:37.770 --> 00:39:39.930
And personally,
I'm very involved

00:39:39.930 --> 00:39:43.200
with AI4ALL, which
is a nonprofit that

00:39:43.200 --> 00:39:47.010
educates high schoolers
around the country

00:39:47.010 --> 00:39:49.770
from diverse backgrounds,
whether they're

00:39:49.770 --> 00:39:53.670
girls or students of
underrepresented minority

00:39:53.670 --> 00:39:54.480
groups.

00:39:54.480 --> 00:39:59.340
And we bring them
onto university campus

00:39:59.340 --> 00:40:05.304
and work with them on AI
thinking and AI studies.

00:40:05.304 --> 00:40:06.720
DIANE GREENE: And
at Google, we're

00:40:06.720 --> 00:40:11.610
just completely committed
to bringing all our best

00:40:11.610 --> 00:40:15.235
technologies to
everybody in the world.

00:40:15.235 --> 00:40:16.860
And we're doing that
through the cloud,

00:40:16.860 --> 00:40:18.450
and we're bringing
these tools, we're

00:40:18.450 --> 00:40:21.570
bringing these APIs
and the training

00:40:21.570 --> 00:40:24.390
and the partnering
and the processors.

00:40:24.390 --> 00:40:26.940
And we're pretty excited
to see what all you

00:40:26.940 --> 00:40:28.980
guys are going to do with it.

00:40:28.980 --> 00:40:30.075
Thank you very much.

00:40:30.075 --> 00:40:31.408
GREG CORRADO: Thanks, everybody.

00:40:31.408 --> 00:40:34.570
[MUSIC PLAYING]

