WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:02.050
FRANK CHEN: So hi everyone.

00:00:02.050 --> 00:00:02.820
I'm Frank.

00:00:02.820 --> 00:00:05.790
And I work on the Google Brain
team working on TensorFlow.

00:00:05.790 --> 00:00:07.710
And today for the first
part of this talk,

00:00:07.710 --> 00:00:10.740
I'm going to talk to you about
accelerating machine learning

00:00:10.740 --> 00:00:13.410
with Google Cloud TPUs.

00:00:13.410 --> 00:00:16.200
So the motivation question
here is, why is Google

00:00:16.200 --> 00:00:17.310
building accelerators?

00:00:19.830 --> 00:00:21.650
I'm always hesitant
to predict this,

00:00:21.650 --> 00:00:24.720
but if you look at the
data, this has been--

00:00:24.720 --> 00:00:26.520
the end of Moore's
law has been going on

00:00:26.520 --> 00:00:29.730
for the past 10 or 15 years,
where we don't really see

00:00:29.730 --> 00:00:33.810
the 52% year-on-year growth
in single-threaded performance

00:00:33.810 --> 00:00:38.820
that we saw from the late
1980s through the early 2000s

00:00:38.820 --> 00:00:41.130
anymore, where now
single-threaded performance

00:00:41.130 --> 00:00:44.880
for CPUs is really growing at
a rate of about maybe 3% or 5%

00:00:44.880 --> 00:00:45.580
per year.

00:00:45.580 --> 00:00:47.205
So what this means
is that I can't just

00:00:47.205 --> 00:00:49.230
wait 18 months for my
machine learning models

00:00:49.230 --> 00:00:50.280
to train twice as fast.

00:00:50.280 --> 00:00:52.140
This doesn't work anymore.

00:00:52.140 --> 00:00:55.110
At the same time,
organizations are

00:00:55.110 --> 00:00:58.900
dealing with more
data than ever before.

00:00:58.900 --> 00:01:01.740
You have people uploading
hundreds and hundreds

00:01:01.740 --> 00:01:04.260
of hours of video every
minute to YouTube.

00:01:04.260 --> 00:01:06.240
People are leaving
product reviews on Amazon.

00:01:06.240 --> 00:01:09.990
People are using chat
systems, such as WhatsApp.

00:01:09.990 --> 00:01:12.672
People are talking about
personal assistance

00:01:12.672 --> 00:01:13.630
and so on and so forth.

00:01:13.630 --> 00:01:15.421
So more data is generated
than ever before.

00:01:15.421 --> 00:01:21.480
And organizations
are just not really

00:01:21.480 --> 00:01:24.840
equipped to make sense of
them to use them properly.

00:01:24.840 --> 00:01:28.360
And the third thread is
that at the same time,

00:01:28.360 --> 00:01:30.690
we have this sort of
exponential increase

00:01:30.690 --> 00:01:34.740
in the amount of compute needed
by these machine learning

00:01:34.740 --> 00:01:36.010
models.

00:01:36.010 --> 00:01:39.480
This is a very interesting
blog post by OpenAI.

00:01:39.480 --> 00:01:42.900
In late 2012,
where we just had--

00:01:42.900 --> 00:01:45.990
where deep learning was
first becoming useful.

00:01:45.990 --> 00:01:47.550
We have like
AlexNet, and we have

00:01:47.550 --> 00:01:50.100
Dropout, which used a fair
amount of computing power,

00:01:50.100 --> 00:01:53.610
but not that much compared
to in late 2017 where

00:01:53.610 --> 00:01:57.780
DeepMind published the
AlphaGo Zero and AlphaGo.

00:01:57.780 --> 00:02:02.410
In the Alpha Zero paper, we
see in about six, seven years,

00:02:02.410 --> 00:02:07.650
we see the compute demand
increase by 300,000 times.

00:02:07.650 --> 00:02:12.660
So this puts a huge
strain on companies'

00:02:12.660 --> 00:02:14.160
compute infrastructure.

00:02:14.160 --> 00:02:15.300
So what does this all mean?

00:02:15.300 --> 00:02:18.060
The end of Moore's law plus this
sort of exponential increase

00:02:18.060 --> 00:02:21.900
in computer requirements means
that we need a new approach

00:02:21.900 --> 00:02:23.610
for doing machine learning.

00:02:23.610 --> 00:02:25.460
At the same time, of
course, everyone still

00:02:25.460 --> 00:02:27.360
wants to do compute,
do machine learning,

00:02:27.360 --> 00:02:29.640
training faster and cheaper.

00:02:29.640 --> 00:02:32.037
So that's why Google is
building specialized hardware.

00:02:32.037 --> 00:02:33.870
Now, the second question
you might be asking

00:02:33.870 --> 00:02:36.400
is, what sort of accelerators
is Google building?

00:02:36.400 --> 00:02:38.160
So from the title
of my talk, you

00:02:38.160 --> 00:02:41.280
know that Google is building
a type of accelerator

00:02:41.280 --> 00:02:43.800
that we call Tensor Processing
Units, which are really

00:02:43.800 --> 00:02:47.340
specialized ASICs designed
for machine learning.

00:02:47.340 --> 00:02:50.610
This is the first
generation of our TPUs

00:02:50.610 --> 00:02:52.850
we introduced back
in 2015 at Google

00:02:52.850 --> 00:02:56.190
I/O. The second
generation of TPUs

00:02:56.190 --> 00:02:59.250
now called Cloud TPU
version 2 that we introduced

00:02:59.250 --> 00:03:00.960
at Google I/O last year.

00:03:00.960 --> 00:03:03.390
And then these Cloud
TPU version 2's

00:03:03.390 --> 00:03:09.810
can be combined into pods
called Cloud TPU v2 Pods.

00:03:09.810 --> 00:03:11.590
And of course, at
Google I/O this year,

00:03:11.590 --> 00:03:16.134
we introduced the third
generation of cloud TPUs.

00:03:16.134 --> 00:03:16.800
From air cooled.

00:03:16.800 --> 00:03:18.240
Now it's liquid cooled.

00:03:18.240 --> 00:03:21.210
And of course, you can
link a bunch of them

00:03:21.210 --> 00:03:24.432
up into a pod
configuration as well.

00:03:24.432 --> 00:03:26.640
So what are the differences
between these generations

00:03:26.640 --> 00:03:28.020
of TPUs?

00:03:28.020 --> 00:03:32.010
So the first version
of TPUs, it was really

00:03:32.010 --> 00:03:33.210
designed for inference only.

00:03:33.210 --> 00:03:36.090
So it did about 92
teraops of innate.

00:03:36.090 --> 00:03:39.720
The second generation of
TPUs does both training

00:03:39.720 --> 00:03:41.070
and inference.

00:03:41.070 --> 00:03:42.780
It operates on
floating point numbers.

00:03:42.780 --> 00:03:45.240
It does about 180 teraflops.

00:03:45.240 --> 00:03:48.000
And it has about 64 gigs of HBM.

00:03:48.000 --> 00:03:50.160
And the third
generation to TPUs,

00:03:50.160 --> 00:03:51.700
it's a big leap in performance.

00:03:51.700 --> 00:03:54.310
So now we are doing
420 teraflops.

00:03:54.310 --> 00:03:55.810
And we doubled the
amount of memory.

00:03:55.810 --> 00:03:58.290
So now it's 128 gigs of HBM.

00:03:58.290 --> 00:04:01.140
And again, it does
training and inference.

00:04:01.140 --> 00:04:03.500
And of course, we see
the same sort of progress

00:04:03.500 --> 00:04:05.640
with Cloud TPU Pods as well.

00:04:05.640 --> 00:04:10.590
Our 2017 pods did
about 11.5 petaflops.

00:04:10.590 --> 00:04:14.935
That is 11,500
teraflops of compute

00:04:14.935 --> 00:04:17.430
with 4 terabytes of HBM.

00:04:17.430 --> 00:04:21.089
And our new generation of
pods does over 100 petaflops

00:04:21.089 --> 00:04:23.042
with 32 terabytes of HBM.

00:04:23.042 --> 00:04:24.750
And of course, the
new generation of pods

00:04:24.750 --> 00:04:26.340
is also liquid cooled.

00:04:26.340 --> 00:04:28.420
We have a new chip architecture.

00:04:28.420 --> 00:04:30.750
So that's all well
and good, but really,

00:04:30.750 --> 00:04:34.020
what we are looking for here
is not just peak performance,

00:04:34.020 --> 00:04:35.880
but cost effective performance.

00:04:35.880 --> 00:04:40.200
So take this very commonly
used image recognition model,

00:04:40.200 --> 00:04:41.340
called ResNet 50.

00:04:41.340 --> 00:04:44.670
If you train it on, again,
a very common dataset

00:04:44.670 --> 00:04:47.640
called ImageNet, we
achieve about 4,100 images

00:04:47.640 --> 00:04:49.240
per second on real data.

00:04:49.240 --> 00:04:55.170
We also achieve that while
getting state of the art

00:04:55.170 --> 00:04:56.260
final accuracy numbers.

00:04:56.260 --> 00:04:59.620
So in this case, it's
93% top 5 accuracy

00:04:59.620 --> 00:05:01.330
on the ImageNet dataset.

00:05:01.330 --> 00:05:04.930
And we can train
this ResNet model

00:05:04.930 --> 00:05:07.360
in about 7 hours and 47 minutes.

00:05:07.360 --> 00:05:10.112
And this is actually
a huge improvement.

00:05:10.112 --> 00:05:12.070
If you look at the original
paper by Kaiming He

00:05:12.070 --> 00:05:14.403
and others where they introduce
the ResNet architecture,

00:05:14.403 --> 00:05:16.870
they took weeks and weeks to
train one of these models.

00:05:16.870 --> 00:05:19.330
And now with one
TPU, we can train it

00:05:19.330 --> 00:05:21.590
in 7 hours and 47 minutes.

00:05:21.590 --> 00:05:24.970
And of course, these things
are available on Google Cloud.

00:05:24.970 --> 00:05:27.460
So the current training,
so it takes about--

00:05:27.460 --> 00:05:32.890
if you pay for the resource
on demand, it's about $36.

00:05:32.890 --> 00:05:35.800
And if you pay for it
using Google Cloud's

00:05:35.800 --> 00:05:37.870
preemptible instances,
it is about $11.

00:05:37.870 --> 00:05:40.192
So it's getting
pretty cheap to train.

00:05:40.192 --> 00:05:42.650
And of course, we want to do
the cost effective performance

00:05:42.650 --> 00:05:43.494
at scale.

00:05:43.494 --> 00:05:45.410
So if you're trying the
same model, ResNet 50,

00:05:45.410 --> 00:05:47.960
on a Cloud TPU
version 2 Pod, you

00:05:47.960 --> 00:05:50.930
are getting something like
219,000 images per second

00:05:50.930 --> 00:05:51.950
of training performance.

00:05:51.950 --> 00:05:54.500
You get the same finer accuracy.

00:05:54.500 --> 00:05:56.630
And training time goes
from about eight hours

00:05:56.630 --> 00:05:57.930
to about eight minutes.

00:05:57.930 --> 00:06:00.000
So again, that's a
huge improvement.

00:06:00.000 --> 00:06:03.320
And this gets us into
the region of we can just

00:06:03.320 --> 00:06:07.700
iterate on-- you can
just go train a model,

00:06:07.700 --> 00:06:09.830
go get a cup of
coffee, come back,

00:06:09.830 --> 00:06:11.580
and then you can
see the results.

00:06:11.580 --> 00:06:14.960
So it gets into almost
interactive levels

00:06:14.960 --> 00:06:17.870
of machine learning, of being
able to do machine learning

00:06:17.870 --> 00:06:19.260
research and development.

00:06:19.260 --> 00:06:21.370
So that's great.

00:06:21.370 --> 00:06:25.310
Then the next question will be,
how do these accelerators work?

00:06:25.310 --> 00:06:29.630
So today we are going to zoom
in on the second generation

00:06:29.630 --> 00:06:30.530
of Cloud TPUs.

00:06:30.530 --> 00:06:32.460
So again, this is
what it looks like.

00:06:32.460 --> 00:06:37.362
This is one entire Cloud
TPU board that you see here.

00:06:37.362 --> 00:06:39.070
And the first thing
that you want to know

00:06:39.070 --> 00:06:42.120
is that Cloud TPUs are really
network-attached devices.

00:06:42.120 --> 00:06:45.020
So if I want to use a Cloud TPU
on Google Cloud, what happens

00:06:45.020 --> 00:06:45.890
is that I create it.

00:06:45.890 --> 00:06:48.200
I go to the Google
Cloud Console,

00:06:48.200 --> 00:06:49.810
and I create a Cloud TPU.

00:06:49.810 --> 00:06:53.000
And then I create a
Google Compute Engine VM.

00:06:53.000 --> 00:06:55.500
And then under VM, I just
have to install TensorFlow.

00:06:55.500 --> 00:06:58.004
So literally, I have to
do PIP install TensorFlow.

00:06:58.004 --> 00:06:59.420
And then I can
start writing code.

00:06:59.420 --> 00:07:01.730
I don't have drivers to install.

00:07:01.730 --> 00:07:03.290
You can use a
clean Ubuntu image.

00:07:03.290 --> 00:07:06.750
You can use the machine
learning images that we provide.

00:07:06.750 --> 00:07:10.790
So it's really very simple
to get started with.

00:07:10.790 --> 00:07:14.290
So each TPU is connected
to a host server

00:07:14.290 --> 00:07:16.420
with 32 lanes of PCI Express.

00:07:16.420 --> 00:07:18.740
So each TPU-- so the
thing here to note

00:07:18.740 --> 00:07:21.670
is that the TPU itself
is like an accelerator.

00:07:21.670 --> 00:07:25.210
So you can think
of it like GPUs.

00:07:25.210 --> 00:07:25.990
So it doesn't run.

00:07:25.990 --> 00:07:28.060
You can't run Linux
on it by itself.

00:07:28.060 --> 00:07:29.560
So it's connected
to the host server

00:07:29.560 --> 00:07:32.620
by 32 lanes of PCI Express
to make sure that we

00:07:32.620 --> 00:07:34.120
can transfer training data in.

00:07:34.120 --> 00:07:37.550
We can get our results
back out quickly.

00:07:37.550 --> 00:07:39.550
And of course, you can
see on this board clearly

00:07:39.550 --> 00:07:41.980
there are four fairly
large heat sinks.

00:07:41.980 --> 00:07:46.890
Underneath each heat
sink is a Cloud TPU chip.

00:07:46.890 --> 00:07:48.720
So zooming in on
the chip, so here's

00:07:48.720 --> 00:07:53.490
a very simplified diagram
of the chip layout.

00:07:53.490 --> 00:07:56.050
So as you can see, each
chip has two cores.

00:07:56.050 --> 00:07:58.910
It's connected to 16
gigabytes of HBM each.

00:07:58.910 --> 00:08:00.570
And there are very
fast interconnects

00:08:00.570 --> 00:08:03.060
that connect these chips
to other chips on the board

00:08:03.060 --> 00:08:06.450
and across the entire pod.

00:08:06.450 --> 00:08:11.070
So each chip does about-- each
core does about 22.5 teraflops.

00:08:11.070 --> 00:08:14.970
And each core consists of a
scalar unit, a vector unit,

00:08:14.970 --> 00:08:15.750
and a matrix unit.

00:08:15.750 --> 00:08:22.480
And we are operating mostly on
full 32's with one exception.

00:08:22.480 --> 00:08:24.090
So zooming on a
matrix unit, this

00:08:24.090 --> 00:08:27.060
is where all the dense matrix
math and dense convolution

00:08:27.060 --> 00:08:27.810
happens.

00:08:27.810 --> 00:08:30.690
So the matrix unit is
implemented as a 128

00:08:30.690 --> 00:08:34.620
by 128 systolic array that
does bfloat16 multiplies

00:08:34.620 --> 00:08:36.123
and float32 accumulates.

00:08:36.123 --> 00:08:38.789
So there are two terms here that
you might not be familiar with,

00:08:38.789 --> 00:08:40.799
bloat16 and systolic arrays.

00:08:40.799 --> 00:08:44.100
So I'm going to go through
each of these in turn.

00:08:44.100 --> 00:08:46.702
So here's a brief guide
to floating point formats.

00:08:46.702 --> 00:08:49.160
So if you are doing machine
learning training and inference

00:08:49.160 --> 00:08:51.380
today, you're
probably using fp32,

00:08:51.380 --> 00:08:53.900
or what's called
single-precision IEEE

00:08:53.900 --> 00:08:55.260
floating point format.

00:08:55.260 --> 00:08:57.350
So in this case, you
have one signed bit,

00:08:57.350 --> 00:09:02.000
eight exponent bits, and
about 23 significant bits.

00:09:02.000 --> 00:09:05.660
And that allows you to represent
a range of numbers from 10

00:09:05.660 --> 00:09:08.960
to the negative 38 to
about 10 to the 38.

00:09:08.960 --> 00:09:12.560
So it's a fairly wide range of
numbers that you can represent.

00:09:12.560 --> 00:09:15.320
So in recent years,
people have been

00:09:15.320 --> 00:09:18.190
trying to train neural
networks on fp16,

00:09:18.190 --> 00:09:21.620
or what's half-precision
IEEE floating point format.

00:09:21.620 --> 00:09:26.630
And people at TensorFlow
and across the industry

00:09:26.630 --> 00:09:29.420
have been trying to make this
work well and seamlessly,

00:09:29.420 --> 00:09:31.370
but the truth of
the matter is you

00:09:31.370 --> 00:09:34.430
have to make some
modifications to many models

00:09:34.430 --> 00:09:37.850
for it to train properly
if you're only using fp16,

00:09:37.850 --> 00:09:40.770
mainly because of issues
like managing gradient,

00:09:40.770 --> 00:09:43.770
or you have to do log
scaling, all sorts of things.

00:09:43.770 --> 00:09:45.710
And the reason is
because the range

00:09:45.710 --> 00:09:49.010
of representable
numbers for fp16

00:09:49.010 --> 00:09:51.240
is much narrower than for fp32.

00:09:51.240 --> 00:09:55.310
So the range here is just
from about 6 to the 6 times

00:09:55.310 --> 00:09:57.530
10 to the negative
8 to about 65,000.

00:09:57.530 --> 00:10:00.480
So that's a much narrower
range of numbers.

00:10:00.480 --> 00:10:03.640
So what did the folks
at Google Brain do?

00:10:03.640 --> 00:10:05.690
So what Google Brain
did is that we came up

00:10:05.690 --> 00:10:08.240
with a floating point
format called bfloat16.

00:10:08.240 --> 00:10:11.720
So what bfloat16 is,
it is like float32,

00:10:11.720 --> 00:10:15.950
except we drop the last
16 bits of mantissa.

00:10:15.950 --> 00:10:19.580
So this results in the same
bit, the same exponent bits,

00:10:19.580 --> 00:10:24.281
but only 7 bits of mantissa
instead of 23 bits.

00:10:24.281 --> 00:10:26.780
In this way we can represent
the same range of numbers, just

00:10:26.780 --> 00:10:28.400
at a much lower position.

00:10:28.400 --> 00:10:31.560
And it turns out that you don't
need all that much precision

00:10:31.560 --> 00:10:33.560
for neural network training,
but you do actually

00:10:33.560 --> 00:10:36.710
need all the range.

00:10:36.710 --> 00:10:39.650
And then the second
term is systolic arrays.

00:10:39.650 --> 00:10:41.210
So rather than
trying to describe

00:10:41.210 --> 00:10:43.340
what a systolic array
is, I will just show you

00:10:43.340 --> 00:10:45.110
a little animation I made up.

00:10:45.110 --> 00:10:46.700
So in this case,
we are computing y

00:10:46.700 --> 00:10:51.080
equals a very simple matrix
times vector computation.

00:10:51.080 --> 00:10:53.180
So you're computing
y equals w times

00:10:53.180 --> 00:10:57.050
x, where w is a
3-by-3 matrix and x

00:10:57.050 --> 00:10:58.430
is a three-element vector.

00:10:58.430 --> 00:11:01.040
And we are computing x
with a batch size of three.

00:11:01.040 --> 00:11:03.020
So we have already
loaded all the weights

00:11:03.020 --> 00:11:04.460
into the matrix unit.

00:11:04.460 --> 00:11:06.050
And if we start the
first clock cycle,

00:11:06.050 --> 00:11:09.680
you'll see that the first
element of the first vector

00:11:09.680 --> 00:11:11.480
is loaded into the matrix unit.

00:11:11.480 --> 00:11:15.770
And then we multiply
the position 1,

00:11:15.770 --> 00:11:21.350
1 of w with the first
element of the first vector.

00:11:21.350 --> 00:11:23.137
In the second clock
cycle, what happens

00:11:23.137 --> 00:11:24.470
is that more weights are loaded.

00:11:24.470 --> 00:11:28.550
So we are doing more
multiplications.

00:11:28.550 --> 00:11:30.890
At the same time, we
are pushing the results

00:11:30.890 --> 00:11:34.130
from the previous round of
multiplication [INAUDIBLE]..

00:11:34.130 --> 00:11:38.660
So that in the case
of the yellow box

00:11:38.660 --> 00:11:40.910
right there, we are not just
doing the multiplication.

00:11:40.910 --> 00:11:43.400
We are also summing the result
of the multiplication that

00:11:43.400 --> 00:11:46.220
happens within the box with
the result from the box

00:11:46.220 --> 00:11:47.420
to the left of it.

00:11:47.420 --> 00:11:49.184
And then this continues.

00:11:49.184 --> 00:11:51.350
As you can see, you are
utilizing a lot more compute

00:11:51.350 --> 00:11:57.150
now until you get
the outputs out.

00:11:57.150 --> 00:12:00.930
So what this effectively is,
is a 2D field of compute.

00:12:00.930 --> 00:12:03.510
So it allows us to put
a lot of compute units

00:12:03.510 --> 00:12:05.940
within a very small
amount of chip area.

00:12:05.940 --> 00:12:11.070
So if we optimize on the cost
of the chip, because the bigger

00:12:11.070 --> 00:12:12.200
the chip, the bigger--

00:12:12.200 --> 00:12:13.740
the higher the cost.

00:12:13.740 --> 00:12:15.800
And with a chip architecture
that's also built

00:12:15.800 --> 00:12:18.210
for pipelining-- that
is we can fill the--

00:12:18.210 --> 00:12:22.410
so in this previous example, we
only had a batch size of three.

00:12:22.410 --> 00:12:24.117
But if you have
bigger batch sizes,

00:12:24.117 --> 00:12:25.950
if your chip architecture
is built for this,

00:12:25.950 --> 00:12:28.560
you can just always
fill the matrix units.

00:12:28.560 --> 00:12:31.800
And this means that we get very
high throughput for our matrix

00:12:31.800 --> 00:12:35.310
multiplications, which is
really at the heart of a lot

00:12:35.310 --> 00:12:38.090
of these deep learning models.

00:12:38.090 --> 00:12:39.610
So OK, cool.

00:12:39.610 --> 00:12:43.540
How do I use these accelerators?

00:12:43.540 --> 00:12:47.430
So our recommendation is that
you start with our Cloud TPU

00:12:47.430 --> 00:12:48.310
Reference Models.

00:12:48.310 --> 00:12:50.320
These are high performance,
open source models.

00:12:50.320 --> 00:12:53.160
They are licensed under, I
think, the Apache license.

00:12:53.160 --> 00:12:56.120
They implement very common
and also cutting-edge

00:12:56.120 --> 00:12:57.150
model architectures.

00:12:57.150 --> 00:12:59.721
Internally, we test them for
performance and accuracy.

00:12:59.721 --> 00:13:02.220
And you can use these and get
up and running really quickly.

00:13:02.220 --> 00:13:03.709
And you can modify
them as needed.

00:13:03.709 --> 00:13:06.000
So you can train and run, of
course, on assembled data,

00:13:06.000 --> 00:13:08.437
on your own data, and
so on and so forth.

00:13:08.437 --> 00:13:10.020
And we have a lot
of reference models.

00:13:10.020 --> 00:13:15.030
So I gave you examples of ResNet
50 and other image recognition

00:13:15.030 --> 00:13:17.400
networks, but you
can also do things

00:13:17.400 --> 00:13:20.220
like machine translation,
language modeling, speech

00:13:20.220 --> 00:13:22.230
recognition, image generation.

00:13:22.230 --> 00:13:25.620
We have all these models just
as sample models for cloud

00:13:25.620 --> 00:13:29.540
TPUs if you want to
get started with them.

00:13:29.540 --> 00:13:30.190
Great.

00:13:30.190 --> 00:13:30.940
So remember these?

00:13:30.940 --> 00:13:33.680
Remember those pods?

00:13:33.680 --> 00:13:35.390
It turns out for a
lot of our models,

00:13:35.390 --> 00:13:37.680
we have not only optimized
them for single TPUs,

00:13:37.680 --> 00:13:39.890
we've also optimized
for TPU pods.

00:13:39.890 --> 00:13:43.700
For instance, take
the ResNet 50 example

00:13:43.700 --> 00:13:46.340
that I quoted performance
figures for earlier.

00:13:46.340 --> 00:13:49.070
In this case, you've got
training on a single Cloud TPU.

00:13:49.070 --> 00:13:51.080
This is really
literally all you do.

00:13:51.080 --> 00:13:52.950
You download the--
you start a TPU.

00:13:52.950 --> 00:13:54.200
You download TensorFlow.

00:13:54.200 --> 00:13:56.340
You clone the Git repository.

00:13:56.340 --> 00:13:58.820
And then you just basically
call Python, and just

00:13:58.820 --> 00:14:00.260
say, point it to the TPU.

00:14:00.260 --> 00:14:01.820
Point it where our data is.

00:14:01.820 --> 00:14:03.216
Tell me what the batch size is.

00:14:03.216 --> 00:14:05.090
Tell me how many steps
you want to train for.

00:14:05.090 --> 00:14:06.080
And then bam.

00:14:06.080 --> 00:14:07.332
Off you go.

00:14:07.332 --> 00:14:09.290
It turns out that training
on the Cloud TPU Pod

00:14:09.290 --> 00:14:10.530
is not that different.

00:14:10.530 --> 00:14:13.194
Instead of starting a Cloud
TPU, you start a Cloud TPU pod.

00:14:13.194 --> 00:14:15.110
And really, the only
things you have to modify

00:14:15.110 --> 00:14:17.690
is the name of the
TPU, the training batch

00:14:17.690 --> 00:14:20.540
size, and the number
of training steps.

00:14:20.540 --> 00:14:23.360
So the reference model-- so in
this case, the reference model

00:14:23.360 --> 00:14:27.320
for ResNet 50 uses like
fairly recent techniques,

00:14:27.320 --> 00:14:29.630
such as the LARS optimizer
and label smoothing

00:14:29.630 --> 00:14:32.000
to achieve the
target accuracy so

00:14:32.000 --> 00:14:34.310
that you don't have to
re-implement all these changes.

00:14:34.310 --> 00:14:35.643
We have already done it for you.

00:14:35.643 --> 00:14:38.240
So a lot of our reference
models scale up from one

00:14:38.240 --> 00:14:40.930
TPU all the way to a pod.

00:14:40.930 --> 00:14:44.590
So of course, you aren't
limited to reference models.

00:14:44.590 --> 00:14:47.170
So when you build your
own models, of course,

00:14:47.170 --> 00:14:50.240
you build them with TensorFlow.

00:14:50.240 --> 00:14:51.990
And when you build
models with TensorFlow,

00:14:51.990 --> 00:14:54.810
there are really two things
that you have to think about.

00:14:54.810 --> 00:14:58.712
There is the thing that most
people focus their energy on,

00:14:58.712 --> 00:15:00.670
which is the network
architecture itself, which

00:15:00.670 --> 00:15:03.070
is running on the accelerator.

00:15:03.070 --> 00:15:06.396
But a lot of what people
neglect is the input pipeline.

00:15:06.396 --> 00:15:08.020
So basically, moving
our training data,

00:15:08.020 --> 00:15:11.230
reading them, decompressing
them, parsing them,

00:15:11.230 --> 00:15:13.720
performing data augmentation,
and patching them,

00:15:13.720 --> 00:15:15.580
and then sending it
into the accelerators.

00:15:15.580 --> 00:15:18.110
A lot of people don't think
about this as a problem,

00:15:18.110 --> 00:15:21.790
but really, for these sort of
high performance accelerators,

00:15:21.790 --> 00:15:28.570
this sort of limits performance,
because if your training

00:15:28.570 --> 00:15:32.210
pipelines slow, then accelerator
is just idle half the time.

00:15:32.210 --> 00:15:34.400
So phase one, build
an input pipeline.

00:15:34.400 --> 00:15:38.760
So this is a very simple
input pipeline for ResNet 50.

00:15:38.760 --> 00:15:40.010
So you have an input function.

00:15:40.010 --> 00:15:41.094
You list a bunch of files.

00:15:41.094 --> 00:15:41.801
You shuffle them.

00:15:41.801 --> 00:15:42.540
You repeat them.

00:15:42.540 --> 00:15:43.770
And then you send it out.

00:15:43.770 --> 00:15:45.200
So this is great.

00:15:45.200 --> 00:15:47.456
Guess what the
performance of this.

00:15:47.456 --> 00:15:49.972
This is 150 images per second.

00:15:49.972 --> 00:15:51.680
So even if you run
this on the Cloud TPU,

00:15:51.680 --> 00:15:54.130
you're getting 150 images per
second for training, which

00:15:54.130 --> 00:15:58.130
is not great, because Cloud TPUs
can do 4,000 images per second.

00:15:58.130 --> 00:15:59.020
So what you do?

00:15:59.020 --> 00:16:00.010
You have a bottleneck.

00:16:00.010 --> 00:16:01.427
So how do you
improve performance?

00:16:01.427 --> 00:16:02.426
You find the bottleneck.

00:16:02.426 --> 00:16:03.610
You optimize the bottleneck.

00:16:03.610 --> 00:16:05.380
And of course, you
repeat until you

00:16:05.380 --> 00:16:07.360
get the desired performance.

00:16:07.360 --> 00:16:10.360
And Cloud TPUs actually provide
a fairly comprehensive set

00:16:10.360 --> 00:16:12.790
of profiling tools.

00:16:12.790 --> 00:16:14.554
So in this case,
you can see what's--

00:16:14.554 --> 00:16:15.970
in this case, this
is TensorBoard.

00:16:15.970 --> 00:16:20.680
So you can bring up a profile
of what's happening on your TPU,

00:16:20.680 --> 00:16:22.724
on the host, and
so on and so forth.

00:16:22.724 --> 00:16:24.890
And then you can see that,
oh, there are large gaps.

00:16:24.890 --> 00:16:27.880
So this means that the CPU
is idle waiting for data.

00:16:27.880 --> 00:16:29.260
And this is not great.

00:16:29.260 --> 00:16:32.500
So a simplified
like representation

00:16:32.500 --> 00:16:34.360
of what's happening on
TensorBoard right now

00:16:34.360 --> 00:16:35.660
is something like this.

00:16:35.660 --> 00:16:37.420
So in this case,
we have an extract.

00:16:37.420 --> 00:16:38.837
We have a transformer
with a load.

00:16:38.837 --> 00:16:40.878
And then we have the
training on the accelerator.

00:16:40.878 --> 00:16:42.572
And they are all
happening sequentially.

00:16:42.572 --> 00:16:43.780
And this is not great, right?

00:16:43.780 --> 00:16:46.480
Because what is
really happening here

00:16:46.480 --> 00:16:48.250
is that you're
leaving the CPU idle.

00:16:48.250 --> 00:16:50.950
And you're leaving
the accelerator idle.

00:16:50.950 --> 00:16:54.160
And these two things are
the biggest cost factors

00:16:54.160 --> 00:16:56.030
in your training pipeline.

00:16:56.030 --> 00:16:58.740
So what you really want to do
is to do something like this.

00:16:58.740 --> 00:17:01.310
You're overlapping
every single step.

00:17:01.310 --> 00:17:06.190
And you are utilizing all of the
expensive bits in your computer

00:17:06.190 --> 00:17:08.119
to the fullest extent.

00:17:08.119 --> 00:17:09.940
So the accelerator
is 100% utilized.

00:17:09.940 --> 00:17:13.000
The CPU is only idle slightly.

00:17:13.000 --> 00:17:14.760
And the disk is idle,
but that's fine.

00:17:17.290 --> 00:17:19.849
And to do pipelining
is really easy.

00:17:19.849 --> 00:17:22.329
So you just have to
really modify one thing.

00:17:22.329 --> 00:17:25.030
So you see the
second to last line.

00:17:25.030 --> 00:17:27.709
Instead of doing-- just
do dataset.prefetch.

00:17:27.709 --> 00:17:29.500
And this just ensures
that everything above

00:17:29.500 --> 00:17:31.870
is pipeline with
accelerator training.

00:17:31.870 --> 00:17:34.360
And of course, you also
want to do parallel reads,

00:17:34.360 --> 00:17:37.090
because reading from
many files is faster than

00:17:37.090 --> 00:17:39.340
reading from one.

00:17:39.340 --> 00:17:40.920
And there are many
other techniques

00:17:40.920 --> 00:17:43.560
that I won't go into today,
because I don't have time.

00:17:43.560 --> 00:17:46.860
So you can use sloppy
interleave, fused dataset

00:17:46.860 --> 00:17:47.820
operators.

00:17:47.820 --> 00:17:50.455
We have a good performance
guide on the TensorFlow website

00:17:50.455 --> 00:17:52.830
that tells you how you can
optimize your input pipelines.

00:17:52.830 --> 00:17:54.990
I encourage you to take a look.

00:17:54.990 --> 00:17:57.960
But this is sort of a partially
optimized input pipeline.

00:17:57.960 --> 00:18:00.450
It's slightly longer
than our simple one,

00:18:00.450 --> 00:18:03.485
but this does over
2,000 images per second.

00:18:03.485 --> 00:18:05.610
And if you want the fully
optimized image pipeline,

00:18:05.610 --> 00:18:10.740
please take a look at
our TPU sample code.

00:18:10.740 --> 00:18:11.240
OK.

00:18:11.240 --> 00:18:11.740
Cool.

00:18:11.740 --> 00:18:14.562
Now comes the fun part,
building your model.

00:18:14.562 --> 00:18:16.270
So the first way you
can build your model

00:18:16.270 --> 00:18:17.650
is actually with Keras.

00:18:17.650 --> 00:18:20.350
So we have experimental
Keras integration available

00:18:20.350 --> 00:18:24.190
starting with TensorFlow
1.11, which will be coming out

00:18:24.190 --> 00:18:26.210
in about two to three weeks.

00:18:26.210 --> 00:18:26.980
So you can build--

00:18:26.980 --> 00:18:30.970
so you can write your models
in Keras as per normal.

00:18:30.970 --> 00:18:33.070
And the only real thing
that you have to modify

00:18:33.070 --> 00:18:35.890
is basically create what's
called a cluster resolver,

00:18:35.890 --> 00:18:38.050
give it a name, create
a distribution strategy,

00:18:38.050 --> 00:18:41.210
and call the
keras_to_tpu_model function.

00:18:41.210 --> 00:18:42.790
And this will
transform your model

00:18:42.790 --> 00:18:45.015
to something that's
compatible for the TPU.

00:18:45.015 --> 00:18:46.390
And then after
that, you can just

00:18:46.390 --> 00:18:49.710
do the simple sort of
model.compile, model.fit,

00:18:49.710 --> 00:18:54.090
and all the Keras goodness
that you know and love.

00:18:54.090 --> 00:18:57.470
And in TensorFlow 1.12, which
is the release after this,

00:18:57.470 --> 00:18:58.980
we are going to
make it even easier.

00:18:58.980 --> 00:19:01.920
So you don't even have to call
keras_to_tpu_model anymore.

00:19:01.920 --> 00:19:04.400
You can just call
model.compile directly.

00:19:04.400 --> 00:19:06.890
And then this will work.

00:19:06.890 --> 00:19:07.640
Great.

00:19:07.640 --> 00:19:08.806
You don't want to use Keras.

00:19:08.806 --> 00:19:10.670
You want to use
something lower level.

00:19:10.670 --> 00:19:12.290
So we also have a
solution for that.

00:19:12.290 --> 00:19:14.480
You can use something called
TensorFlow Distribution

00:19:14.480 --> 00:19:14.984
Strategy.

00:19:14.984 --> 00:19:17.150
I think there was a talk
about Distribution Strategy

00:19:17.150 --> 00:19:17.759
yesterday.

00:19:17.759 --> 00:19:20.300
So if you missed that, I think
the video will be online soon.

00:19:20.300 --> 00:19:22.760
So you should take
a look at that.

00:19:22.760 --> 00:19:26.310
So in this case, this
is using the estimator

00:19:26.310 --> 00:19:27.680
of Distribution Strategy.

00:19:27.680 --> 00:19:29.200
So you can write
your model function

00:19:29.200 --> 00:19:30.340
like you see on the left.

00:19:30.340 --> 00:19:31.714
You can write your
input function

00:19:31.714 --> 00:19:33.045
like you see on the top right.

00:19:33.045 --> 00:19:35.170
And again, the only thing
you really have to modify

00:19:35.170 --> 00:19:36.490
is a couple lines.

00:19:36.490 --> 00:19:40.094
Again, create a cluster resolve,
or create a TPU strategy.

00:19:40.094 --> 00:19:42.760
And then you can just pass it in
through the estimator function,

00:19:42.760 --> 00:19:43.510
train.distribute.

00:19:43.510 --> 00:19:46.180
So this will let
it work on TPUs.

00:19:46.180 --> 00:19:48.840
So that's all great.

00:19:48.840 --> 00:19:51.630
And so are people
using these TPUs?

00:19:51.630 --> 00:19:53.340
People are, in fact.

00:19:53.340 --> 00:19:57.000
So here's a case study of an
architecture search project

00:19:57.000 --> 00:20:01.230
that's done by a group
from Stanford and MIT.

00:20:01.230 --> 00:20:03.870
So they did parallel runs
using hundreds and hundreds

00:20:03.870 --> 00:20:06.540
of cloud TPUs from the
TensorFlow Research Cloud

00:20:06.540 --> 00:20:10.260
Program, which is where we
are providing 1,000 free TPUs

00:20:10.260 --> 00:20:11.410
to academic researchers.

00:20:11.410 --> 00:20:12.960
So if you're
academic researchers,

00:20:12.960 --> 00:20:15.430
I encourage you to
look into this program.

00:20:15.430 --> 00:20:20.790
So each blue dot in this image
is a run on a TPU training

00:20:20.790 --> 00:20:23.970
an ImageNet scale,
a convolution RNN.

00:20:23.970 --> 00:20:26.100
So each run used to
take hours and hours

00:20:26.100 --> 00:20:29.160
to train on other
hardware, but on TPUs,

00:20:29.160 --> 00:20:32.100
because they have
access to so many TPUs,

00:20:32.100 --> 00:20:35.250
they can do hundreds and
hundreds of these runs.

00:20:35.250 --> 00:20:36.720
So what they were
trying to do was

00:20:36.720 --> 00:20:39.930
that they were trying to search
for a model that was a better

00:20:39.930 --> 00:20:41.970
fit for the data
that you record, say,

00:20:41.970 --> 00:20:44.100
if you put electrodes
in my brain

00:20:44.100 --> 00:20:46.710
and look at what my
visual cortex is trying

00:20:46.710 --> 00:20:48.430
to do when I look at things.

00:20:48.430 --> 00:20:49.980
So they are trying
to find analogs,

00:20:49.980 --> 00:20:51.480
trying to find a
neural network that

00:20:51.480 --> 00:20:57.370
was a closer analog to
the primate visual cortex.

00:20:57.370 --> 00:21:00.879
So it turns out that-- so
here's a diagram of the space

00:21:00.879 --> 00:21:01.920
that they were searching.

00:21:01.920 --> 00:21:03.690
And it turns out that
across a population

00:21:03.690 --> 00:21:05.580
of many different
models, they found

00:21:05.580 --> 00:21:07.740
that the red connections
were sort of selected

00:21:07.740 --> 00:21:10.710
for the search
versus the others.

00:21:10.710 --> 00:21:13.050
And what happens is
that they went back

00:21:13.050 --> 00:21:16.380
and compared the models
to some of the signals

00:21:16.380 --> 00:21:20.370
that were recording, that the
biologists were recording,

00:21:20.370 --> 00:21:22.530
and they found that the
convolution RNNs were

00:21:22.530 --> 00:21:24.450
a much better fit
for neural signals,

00:21:24.450 --> 00:21:27.495
for instance, in v4,
in IT, than in other

00:21:27.495 --> 00:21:30.900
[INAUDIBLE],, like convolution,
or feet forward models

00:21:30.900 --> 00:21:32.580
that you see in the
literature today.

00:21:32.580 --> 00:21:35.580
So this is a really new
and exciting direction

00:21:35.580 --> 00:21:38.400
that a research group was able
to do from scratch with access

00:21:38.400 --> 00:21:39.540
to lots of compute.

00:21:39.540 --> 00:21:42.720
So you can not just
train models on TPUs,

00:21:42.720 --> 00:21:46.960
you can search for them
basically automatically, too.

00:21:46.960 --> 00:21:49.790
And so, finally, of
course, Cloud TPUs today,

00:21:49.790 --> 00:21:51.776
Cloud TPU version 2
today is available,

00:21:51.776 --> 00:21:53.400
is generally available
on Google Cloud.

00:21:53.400 --> 00:21:55.020
If you want to learn
more about them,

00:21:55.020 --> 00:22:00.670
go to cloud.google.com/tpu
to get started.

00:22:00.670 --> 00:22:01.170
All right.

00:22:01.170 --> 00:22:03.660
So now Alex will present
some new functionality

00:22:03.660 --> 00:22:06.180
that lets you write the
accelerator code more easily.

00:22:06.180 --> 00:22:07.730
Alex.

