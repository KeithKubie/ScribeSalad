WEBVTT
Kind: captions
Language: en

00:00:01.434 --> 00:00:02.600
LAURENCE MORONEY: All right.

00:00:02.600 --> 00:00:03.930
Shall we get started?

00:00:03.930 --> 00:00:07.594
So thanks, everybody, for
coming to this session.

00:00:07.594 --> 00:00:09.260
I'm going to be talking
about TensorFlow

00:00:09.260 --> 00:00:12.530
and particularly TensorFlow from
a programmer's perspective--

00:00:12.530 --> 00:00:14.390
so machine learning
for programmers.

00:00:14.390 --> 00:00:17.060
I'd like to show some code
samples of using TensorFlow

00:00:17.060 --> 00:00:20.510
in some simple scenarios as
well as one slightly more

00:00:20.510 --> 00:00:21.530
advanced scenario.

00:00:21.530 --> 00:00:23.446
But before I do that, I
always like to just do

00:00:23.446 --> 00:00:24.860
a little bit of a level set.

00:00:24.860 --> 00:00:27.439
And if you were at the
previous session, sorry,

00:00:27.439 --> 00:00:29.480
some of the content's
going to be similar to what

00:00:29.480 --> 00:00:30.920
you've seen already.

00:00:30.920 --> 00:00:32.870
But when I like
to think about AI,

00:00:32.870 --> 00:00:35.840
and when I come to conferences
like this one about AI,

00:00:35.840 --> 00:00:37.730
or if I read the news
about AI, there's

00:00:37.730 --> 00:00:42.170
always stories about what it
can do or what it might do,

00:00:42.170 --> 00:00:45.210
but there's not a whole lot
about what it actually is.

00:00:45.210 --> 00:00:47.360
So part of my mandate and
part of what I actually

00:00:47.360 --> 00:00:51.560
like to educate people
around is from a programmer's

00:00:51.560 --> 00:00:54.710
perspective, what AI actually
is, what it is for you,

00:00:54.710 --> 00:00:57.750
what you can begin to
learn how to program,

00:00:57.750 --> 00:01:00.920
and then how you can apply it
to your business scenarios.

00:01:00.920 --> 00:01:04.650
But we're also at the
cusp of this revolution

00:01:04.650 --> 00:01:06.960
in this technology,
and lots of people

00:01:06.960 --> 00:01:09.750
are calling it like the
fourth Industrial Revolution.

00:01:09.750 --> 00:01:11.250
And for me, I can
only describe it

00:01:11.250 --> 00:01:15.860
as like it's the third big
shift in my own personal career.

00:01:15.860 --> 00:01:18.870
And so for me, the first one
came in the early to mid-90s

00:01:18.870 --> 00:01:20.700
when the web came about.

00:01:20.700 --> 00:01:22.620
And if you remember
when the web came about,

00:01:22.620 --> 00:01:24.480
we were all desktop programmers.

00:01:24.480 --> 00:01:26.100
I personally-- my
first job was I

00:01:26.100 --> 00:01:28.320
was a Visual Basic
programmer programming

00:01:28.320 --> 00:01:29.550
Windows applications.

00:01:29.550 --> 00:01:31.725
Anybody ever do that?

00:01:31.725 --> 00:01:33.810
It was fun, wasn't it?

00:01:33.810 --> 00:01:36.600
And so then the web came around,
and what happened with the web

00:01:36.600 --> 00:01:39.570
then is it changed the
audience of your application

00:01:39.570 --> 00:01:42.270
from one person at a time
to many people at a time.

00:01:42.270 --> 00:01:43.839
You had to start
thinking differently

00:01:43.839 --> 00:01:45.630
about how you built
your applications to be

00:01:45.630 --> 00:01:47.730
able to scale it to
lots of people using it.

00:01:47.730 --> 00:01:49.259
And also, the runtime changed.

00:01:49.259 --> 00:01:51.300
Instead of you being able
to write something that

00:01:51.300 --> 00:01:53.280
had complete control
over the machine,

00:01:53.280 --> 00:01:56.040
you would write something to
this sort of virtual machine

00:01:56.040 --> 00:01:57.157
that the browser gave you.

00:01:57.157 --> 00:01:59.490
And then maybe that browser
would have plugins like Java

00:01:59.490 --> 00:02:00.480
and stuff like
that you could use

00:02:00.480 --> 00:02:01.860
to make it more intelligent.

00:02:01.860 --> 00:02:03.750
But as a result, what
ended up happening

00:02:03.750 --> 00:02:08.340
was this paradigm shift gave
birth to whole new industries.

00:02:08.340 --> 00:02:10.650
And I work for a small
company called Google.

00:02:10.650 --> 00:02:12.430
Anybody heard of them?

00:02:12.430 --> 00:02:15.360
And so things like
Google weren't possible.

00:02:15.360 --> 00:02:17.950
Anybody remember gophers?

00:02:17.950 --> 00:02:20.360
Yeah, so that's really
old school, right?

00:02:20.360 --> 00:02:22.700
What gophers where were almost
the opposite of a search

00:02:22.700 --> 00:02:23.200
engine.

00:02:23.200 --> 00:02:24.290
A search engine, like--

00:02:24.290 --> 00:02:25.940
you type something into
it, and it has already

00:02:25.940 --> 00:02:27.590
found the results, and
it gives them to you.

00:02:27.590 --> 00:02:29.180
A gopher was this
little application

00:02:29.180 --> 00:02:31.130
that you would send out
into the nascent internet,

00:02:31.130 --> 00:02:33.546
and it would crawl everywhere,
a little bit like a spider,

00:02:33.546 --> 00:02:35.580
and then come back
with results for you.

00:02:35.580 --> 00:02:37.520
So for me, whoever
had the great idea

00:02:37.520 --> 00:02:40.217
to say, let's flip
the axes on that

00:02:40.217 --> 00:02:42.050
and come up with this
new business paradigm,

00:02:42.050 --> 00:02:44.400
ended up building the
first search engines.

00:02:44.400 --> 00:02:47.060
And as a result, companies like
Google and Yahoo were born.

00:02:47.060 --> 00:02:48.800
Ditto with things
like Facebook--

00:02:48.800 --> 00:02:50.883
that wouldn't have been
possible with the browser.

00:02:50.883 --> 00:02:52.220
Can you imagine trying to--

00:02:52.220 --> 00:02:54.740
pre-internet, where there
was no standard protocol

00:02:54.740 --> 00:02:57.352
for communication, and you'd
write desktop applications--

00:02:57.352 --> 00:02:59.060
can imagine being able
to build something

00:02:59.060 --> 00:03:00.268
like a Facebook or a Twitter?

00:03:00.268 --> 00:03:01.500
It just wasn't possible.

00:03:01.500 --> 00:03:02.990
So that became--
to me, the web was

00:03:02.990 --> 00:03:06.480
this first great tectonic shift
in my own personal career.

00:03:06.480 --> 00:03:09.590
The second one then came with
the advent of the smartphone.

00:03:09.590 --> 00:03:11.840
So now users had
this device that they

00:03:11.840 --> 00:03:14.180
can put in their
pocket that's got

00:03:14.180 --> 00:03:15.440
lots of computational power.

00:03:15.440 --> 00:03:16.290
It's got memory.

00:03:16.290 --> 00:03:17.810
It's got storage,
and it's loaded

00:03:17.810 --> 00:03:21.300
with sensors like cameras,
and GPS, et cetera.

00:03:21.300 --> 00:03:23.240
Now think about the
types of applications

00:03:23.240 --> 00:03:24.620
that you could build with that.

00:03:24.620 --> 00:03:28.220
Now it's a case of companies
like Uber became possible.

00:03:28.220 --> 00:03:30.440
Now, I personally
believe, by the way

00:03:30.440 --> 00:03:32.570
that all of the
applications are built

00:03:32.570 --> 00:03:35.540
by introverts because you'll
see all of these great things

00:03:35.540 --> 00:03:38.377
that you can do nowadays is
because they serve introverts.

00:03:38.377 --> 00:03:40.460
I'm highly introverted,
and one thing I hate to do

00:03:40.460 --> 00:03:42.419
is stand on a street
corner and hail a taxi.

00:03:42.419 --> 00:03:44.960
So when Uber came along, it was
like a dream come true for me

00:03:44.960 --> 00:03:46.751
that I could just do
something on my phone,

00:03:46.751 --> 00:03:47.900
and a car would show up.

00:03:47.900 --> 00:03:48.830
And now it's shopping.

00:03:48.830 --> 00:03:50.288
It's the same kind
of thing, right?

00:03:50.288 --> 00:03:53.090
I personally really
dislike going to a store

00:03:53.090 --> 00:03:54.965
and having somebody
say, can I help you?

00:03:54.965 --> 00:03:56.090
Can I do something for you?

00:03:56.090 --> 00:03:57.404
Can I help you find something?

00:03:57.404 --> 00:03:58.070
I'm introverted.

00:03:58.070 --> 00:04:00.486
I want to go find it myself,
put my eyes down, and take it

00:04:00.486 --> 00:04:02.380
to the cash register,
and pay for it.

00:04:02.380 --> 00:04:05.480
And now online shopping,
it's done the same thing.

00:04:05.480 --> 00:04:07.770
So I don't know why I went
down that rabbit hole,

00:04:07.770 --> 00:04:11.960
but it's just one that I find
that the second tectonic shift

00:04:11.960 --> 00:04:14.270
has been the advent of
the mobile application

00:04:14.270 --> 00:04:16.579
so that these new businesses,
these new companies

00:04:16.579 --> 00:04:17.840
became possible.

00:04:17.840 --> 00:04:19.339
So the third one
that I'm seeing now

00:04:19.339 --> 00:04:21.810
is the AI and the machine
learning revolution.

00:04:21.810 --> 00:04:23.700
Now, there's so much
hype around this,

00:04:23.700 --> 00:04:26.902
so I like to draw a
diagram of the hype cycle.

00:04:26.902 --> 00:04:28.610
And so if you think
about the hype cycle,

00:04:28.610 --> 00:04:30.770
every hype cycle starts
off with some kind

00:04:30.770 --> 00:04:32.930
of technological trigger.

00:04:32.930 --> 00:04:34.820
Now, with AI and
machine learning,

00:04:34.820 --> 00:04:36.500
that technological
trigger really

00:04:36.500 --> 00:04:37.970
happened a long time ago.

00:04:37.970 --> 00:04:39.470
Machine learning
has been something,

00:04:39.470 --> 00:04:42.300
and AI has been something
that's been in universities.

00:04:42.300 --> 00:04:44.660
It's been in industry
for quite some time--

00:04:44.660 --> 00:04:45.890
decades.

00:04:45.890 --> 00:04:49.520
So it's only relatively
recently that the intersection

00:04:49.520 --> 00:04:52.610
of compute power and data
has made it possible so

00:04:52.610 --> 00:04:56.270
that now everybody can jump
on board-- not just university

00:04:56.270 --> 00:04:57.072
researchers.

00:04:57.072 --> 00:04:59.030
And with the power of
things such as TensorFlow

00:04:59.030 --> 00:05:01.340
that I'm going to show
later, anybody with a laptop

00:05:01.340 --> 00:05:03.146
can start building
neural networks

00:05:03.146 --> 00:05:04.520
where in the past
neural networks

00:05:04.520 --> 00:05:07.100
were reserved for the
very best of universities.

00:05:07.100 --> 00:05:09.800
So that technological
trigger that's rebooted,

00:05:09.800 --> 00:05:11.840
in many ways, the
AI infrastructure,

00:05:11.840 --> 00:05:13.839
has only happened in
the last few years.

00:05:13.839 --> 00:05:15.380
And with any hype
cycle, what happens

00:05:15.380 --> 00:05:18.650
is you end up with this peak
of increased expectations

00:05:18.650 --> 00:05:20.990
where everybody is thinking
AI's going to be the be-all

00:05:20.990 --> 00:05:22.615
and end-all, and will
change the world,

00:05:22.615 --> 00:05:24.830
and will change
everything as we know it,

00:05:24.830 --> 00:05:27.260
before it falls into the
trough of disillusionment.

00:05:27.260 --> 00:05:29.570
And then at some point,
we get enlightenment,

00:05:29.570 --> 00:05:31.560
and then we head up
into the productivity.

00:05:31.560 --> 00:05:33.101
So when you think
about the web, when

00:05:33.101 --> 00:05:35.522
you think about mobile
phones and those revolutions

00:05:35.522 --> 00:05:37.730
that I spoke about, they
all went through this cycle,

00:05:37.730 --> 00:05:39.620
and AI went through this cycle.

00:05:39.620 --> 00:05:42.837
Now, you can ask 100 people
where we are on this life cycle

00:05:42.837 --> 00:05:45.170
right now, and you'd probably
get 100 different answers.

00:05:45.170 --> 00:05:46.310
But I'm going to
give my answer that I

00:05:46.310 --> 00:05:47.857
think we're right about here.

00:05:47.857 --> 00:05:49.690
And when we start looking
at the news cycle,

00:05:49.690 --> 00:05:51.500
it kind of shows that.

00:05:51.500 --> 00:05:52.650
We start looking at news.

00:05:52.650 --> 00:05:54.899
We start looking at
glossy marketing videos.

00:05:54.899 --> 00:05:55.940
AI's is going to do this.

00:05:55.940 --> 00:05:57.350
AI's was going to do that.

00:05:57.350 --> 00:05:59.930
At the end of the day, AI
isn't really doing any of that.

00:05:59.930 --> 00:06:02.480
It's smart people
building neural networks

00:06:02.480 --> 00:06:04.992
with a new form of, a new
metaphor for programming

00:06:04.992 --> 00:06:06.950
have been the ones who've
been able to do them,

00:06:06.950 --> 00:06:09.330
have been able to build
out these new scenarios.

00:06:09.330 --> 00:06:11.060
So we're still
heading up that curve

00:06:11.060 --> 00:06:12.655
of increased expectations.

00:06:12.655 --> 00:06:14.030
And at some point,
we're probably

00:06:14.030 --> 00:06:16.070
going to end up in the
trough of disillusionment

00:06:16.070 --> 00:06:20.180
before things will get real and
you'll be able to really build

00:06:20.180 --> 00:06:23.971
whatever the Uber or the
Google of the AI generation's

00:06:23.971 --> 00:06:24.470
going to be.

00:06:24.470 --> 00:06:26.344
It may be somebody in
this room will do that.

00:06:26.344 --> 00:06:28.370
I don't know.

00:06:28.370 --> 00:06:31.460
So at Google, we have
this graph that we

00:06:31.460 --> 00:06:34.460
draw that we train
our internal engineers

00:06:34.460 --> 00:06:37.450
and our internal folks around AI
and around the hype around AI.

00:06:37.450 --> 00:06:39.960
And we like to layer
it in these three ways.

00:06:39.960 --> 00:06:42.050
First of all, AI,
from a high level,

00:06:42.050 --> 00:06:43.730
is the ability to
program a computer

00:06:43.730 --> 00:06:45.710
to act like an
intelligent human.

00:06:45.710 --> 00:06:47.442
And how do you do that?

00:06:47.442 --> 00:06:49.400
There might be some
traditional coding in that,

00:06:49.400 --> 00:06:51.316
but there may also be
something called machine

00:06:51.316 --> 00:06:52.410
learning in that.

00:06:52.410 --> 00:06:55.070
And what machine learning is
all about is instead of writing

00:06:55.070 --> 00:06:58.380
code where it's all about how
the human solves a problem,

00:06:58.380 --> 00:07:00.890
how they think about a
problem, and expressing that

00:07:00.890 --> 00:07:04.580
in a language like
Java, C#, or C++,

00:07:04.580 --> 00:07:07.040
it's a case of you train
a computer by getting it

00:07:07.040 --> 00:07:10.190
to recognize patterns and then
open up whole new scenarios

00:07:10.190 --> 00:07:10.747
in that way.

00:07:10.747 --> 00:07:12.830
I'm going to talk about
that in a little bit more.

00:07:12.830 --> 00:07:15.049
And then another
part of that is deep

00:07:15.049 --> 00:07:16.840
learning with the idea
behind deep learning

00:07:16.840 --> 00:07:20.280
is now machines being able
to take over some of the role

00:07:20.280 --> 00:07:24.390
that humans are taking in
the machine learning phase.

00:07:24.390 --> 00:07:26.830
And where machine
learning is all about--

00:07:26.830 --> 00:07:29.730
I'm going to, for
example, show a slide next

00:07:29.730 --> 00:07:31.470
about activity detection.

00:07:31.470 --> 00:07:33.780
But in the case of
activity detection,

00:07:33.780 --> 00:07:36.960
instead of me explicitly
programming a computer

00:07:36.960 --> 00:07:39.780
to detect activities,
I will train a computer

00:07:39.780 --> 00:07:42.099
based on people doing
those activities.

00:07:42.099 --> 00:07:43.140
So let me think about it.

00:07:43.140 --> 00:07:44.410
Let me describe it this way.

00:07:44.410 --> 00:07:47.030
First of all, how many people
in this room are coders?

00:07:47.030 --> 00:07:48.086
Have written code?

00:07:48.086 --> 00:07:48.960
Oh, wow, most of you.

00:07:48.960 --> 00:07:50.010
OK, cool.

00:07:50.010 --> 00:07:52.117
What languages, out of interest?

00:07:52.117 --> 00:07:52.950
Just shout them out.

00:07:52.950 --> 00:07:54.060
[INTERPOSING VOICES]

00:07:54.060 --> 00:07:54.600
LAURENCE MORONEY: C#.

00:07:54.600 --> 00:07:55.099
Thank you.

00:07:55.099 --> 00:07:56.650
[INTERPOSING VOICES]

00:07:56.650 --> 00:07:58.011
LAURENCE MORONEY: Python.

00:07:58.011 --> 00:07:58.510
OK.

00:07:58.510 --> 00:08:00.260
I've written about a
bunch of books on C#.

00:08:00.260 --> 00:08:01.412
I still love it.

00:08:01.412 --> 00:08:03.620
I don't get to use it anymore,
but it's nice to hear.

00:08:03.620 --> 00:08:04.630
So I heard C#.

00:08:04.630 --> 00:08:07.080
I heard Python.

00:08:07.080 --> 00:08:07.890
C++?

00:08:07.890 --> 00:08:08.886
OK, cool.

00:08:08.886 --> 00:08:11.010
Now, what do all of these
languages have in common?

00:08:11.010 --> 00:08:11.510
Ruby?

00:08:11.510 --> 00:08:12.030
Nice.

00:08:12.030 --> 00:08:15.018
What do all of these
languages have in common?

00:08:15.018 --> 00:08:17.160
That you, as a developer,
have to figure out

00:08:17.160 --> 00:08:20.329
how to express a problem
in that language, right?

00:08:20.329 --> 00:08:22.870
So if you think about if you're
building a problem, if you're

00:08:22.870 --> 00:08:25.790
building an application
for activity detection,

00:08:25.790 --> 00:08:28.710
and say you want to detect an
activity of somebody walking--

00:08:28.710 --> 00:08:30.510
like I'm wearing a
smartwatch right now.

00:08:30.510 --> 00:08:33.390
I love it because since I
started wearing smartwatches,

00:08:33.390 --> 00:08:35.789
I became much more
conscious of my own fitness.

00:08:35.789 --> 00:08:39.830
And I think about how this
smartwatch monitors my activity

00:08:39.830 --> 00:08:41.204
that when I start
running, I want

00:08:41.204 --> 00:08:43.320
it to know that I'm running,
so it logs that I'm running.

00:08:43.320 --> 00:08:45.000
When I start walking,
I want it to do

00:08:45.000 --> 00:08:46.530
the same thing,
and count calories,

00:08:46.530 --> 00:08:47.830
and all that kind of stuff.

00:08:47.830 --> 00:08:50.413
But if you think about it from
a coding perspective, how would

00:08:50.413 --> 00:08:53.437
you build a smartwatch like
this one if you're a coder?

00:08:53.437 --> 00:08:55.770
Now, you might, for example,
be able to detect the speed

00:08:55.770 --> 00:08:57.660
that the person's moving
at, and you'd write

00:08:57.660 --> 00:08:59.540
a little bit of code like this.

00:08:59.540 --> 00:09:02.370
If speed is less than 4,
then the person's walking.

00:09:02.370 --> 00:09:04.650
That's kind of naive because
if you're walking uphill,

00:09:04.650 --> 00:09:05.560
you're probably going slower.

00:09:05.560 --> 00:09:07.560
If you're walking downhill,
you're going faster.

00:09:07.560 --> 00:09:09.184
But I'll just keep
it simple like that.

00:09:09.184 --> 00:09:12.360
So in code, you have
a problem, and you

00:09:12.360 --> 00:09:14.016
have to express the
problem in a way

00:09:14.016 --> 00:09:16.140
that the computer understands
that you can compile,

00:09:16.140 --> 00:09:18.520
and then you build an
application out of it.

00:09:18.520 --> 00:09:20.460
So now I say, OK,
what if I'm running?

00:09:20.460 --> 00:09:23.850
If I'm running, well, I can
probably go by the speed again.

00:09:23.850 --> 00:09:26.340
And I say, hey, if my speed
is less than a certain amount,

00:09:26.340 --> 00:09:27.480
I'm walking.

00:09:27.480 --> 00:09:28.470
Otherwise, I'm running.

00:09:28.470 --> 00:09:29.100
I go, OK.

00:09:29.100 --> 00:09:30.851
Now I've built an
activity detector,

00:09:30.851 --> 00:09:32.850
and it detects if I'm
walking or if I'm running.

00:09:32.850 --> 00:09:33.630
Pretty cool.

00:09:33.630 --> 00:09:35.440
Pretty easy to do with code.

00:09:35.440 --> 00:09:35.970
So I go, OK.

00:09:35.970 --> 00:09:38.100
Now, my next scenario is biking.

00:09:38.100 --> 00:09:38.880
And I go, OK.

00:09:38.880 --> 00:09:41.310
If I'm going based on the
speed, the data of my speed,

00:09:41.310 --> 00:09:42.630
I can do a similar thing.

00:09:42.630 --> 00:09:45.660
If I say if my speed is less
than this much, I'm walking.

00:09:45.660 --> 00:09:48.780
Otherwise, I'm running,
or otherwise I'm biking.

00:09:48.780 --> 00:09:49.530
So great.

00:09:49.530 --> 00:09:51.660
I've now written an
activity detector--

00:09:51.660 --> 00:09:55.230
a very naive activity detector--
just by looking at the speed

00:09:55.230 --> 00:09:56.850
that the person's moving at.

00:09:56.850 --> 00:09:59.280
But now my boss
loves to play golf,

00:09:59.280 --> 00:10:00.720
and he's like, this is great.

00:10:00.720 --> 00:10:03.720
I want you to detect golf, and
tell me when I'm playing golf,

00:10:03.720 --> 00:10:05.970
and calculate what I'm
doing when I'm playing golf.

00:10:05.970 --> 00:10:07.870
How do I do that?

00:10:07.870 --> 00:10:09.510
I'm in what I call,
as a programmer,

00:10:09.510 --> 00:10:12.464
the oh crap phase because now
I realize that all of this code

00:10:12.464 --> 00:10:14.880
that I've written, and all
this code that I'm maintaining,

00:10:14.880 --> 00:10:16.588
I now have to throw
away because it can't

00:10:16.588 --> 00:10:18.090
be used in something like this.

00:10:18.090 --> 00:10:21.780
This scenario just doesn't
become possible with the code

00:10:21.780 --> 00:10:23.010
that I've written.

00:10:23.010 --> 00:10:25.710
So when I think about going
back to the revolutions

00:10:25.710 --> 00:10:26.700
that I spoke about--

00:10:26.700 --> 00:10:28.410
for example,
something like an Uber

00:10:28.410 --> 00:10:30.635
wouldn't have been possible
before the mobile phone.

00:10:30.635 --> 00:10:32.760
Something like a Google
wouldn't have been possible

00:10:32.760 --> 00:10:33.840
before the web.

00:10:33.840 --> 00:10:35.430
And something like
my golf detector,

00:10:35.430 --> 00:10:36.888
it wouldn't be
possible or would be

00:10:36.888 --> 00:10:40.150
extremely difficult
without machine learning.

00:10:40.150 --> 00:10:42.540
So what is machine learning?

00:10:42.540 --> 00:10:44.610
So traditional
programming I like

00:10:44.610 --> 00:10:46.560
to summarize in a
diagram like this one.

00:10:46.560 --> 00:10:50.910
And traditional programming is a
case of you express rules using

00:10:50.910 --> 00:10:53.840
a programming language like
Ruby, or C#, or whatever.

00:10:53.840 --> 00:10:55.992
And you have data
that you feed into it,

00:10:55.992 --> 00:10:57.450
and you compile
that into something

00:10:57.450 --> 00:10:58.840
that gives you answers.

00:10:58.840 --> 00:11:00.330
So keeping the
very simple example

00:11:00.330 --> 00:11:02.121
that I have of an
activity detector, that's

00:11:02.121 --> 00:11:04.055
giving me the answer
of you're playing golf.

00:11:04.055 --> 00:11:04.680
You're running.

00:11:04.680 --> 00:11:06.570
You're walking-- all
those kind of things.

00:11:06.570 --> 00:11:09.587
The machine learning revolution
just flips the axes on this.

00:11:09.587 --> 00:11:11.670
So the idea behind the
machine learning revolution

00:11:11.670 --> 00:11:15.570
is now I feed in answers, I feed
in data, and I get out rules.

00:11:15.570 --> 00:11:18.600
So instead of me needing
to have the intelligence

00:11:18.600 --> 00:11:21.450
to define the rules
for something,

00:11:21.450 --> 00:11:24.300
this revolution is
saying that, OK, I'm

00:11:24.300 --> 00:11:26.670
going to tell a computer
that I'm doing this,

00:11:26.670 --> 00:11:28.500
I'm doing that, I'm
doing the other,

00:11:28.500 --> 00:11:30.210
and it's going to
figure out the rules.

00:11:30.210 --> 00:11:31.626
It's going to match
those patterns

00:11:31.626 --> 00:11:33.340
and figure out the rules for me.

00:11:33.340 --> 00:11:37.040
So now something like my
activity detector for golf,

00:11:37.040 --> 00:11:39.820
and walking, and
running changes.

00:11:39.820 --> 00:11:44.380
So now instead of me writing the
code for that, I would say, OK.

00:11:44.380 --> 00:11:46.460
I'm going to get lots
of people to walk.

00:11:46.460 --> 00:11:48.000
I'm going to get lots of
people to wear whatever

00:11:48.000 --> 00:11:50.333
sensor it is-- like maybe
it's a watch or a smartphone--

00:11:50.333 --> 00:11:51.007
in their pocket.

00:11:51.007 --> 00:11:52.590
And I'm going to
gather all that data,

00:11:52.590 --> 00:11:53.965
and I'm going to
tell a computer,

00:11:53.965 --> 00:11:55.460
this is what walking looks like.

00:11:55.460 --> 00:11:56.730
I'm going to do the
same for running.

00:11:56.730 --> 00:11:58.230
I'm going to do the
same for biking.

00:11:58.230 --> 00:12:00.150
And I may as well do
the same for golfing.

00:12:00.150 --> 00:12:02.250
So now my scenario
becomes expandable,

00:12:02.250 --> 00:12:04.830
and I can start detecting things
that I previously would not

00:12:04.830 --> 00:12:06.430
have been able to detect.

00:12:06.430 --> 00:12:09.450
So I've opened up new scenarios
that I previously would not be

00:12:09.450 --> 00:12:13.181
able to program by using
if-then rules or using whatever

00:12:13.181 --> 00:12:13.680
language--

00:12:13.680 --> 00:12:14.640
C.

00:12:14.640 --> 00:12:17.142
Anybody remember
the language Prolog?

00:12:17.142 --> 00:12:17.850
Anybody use that?

00:12:17.850 --> 00:12:18.756
Yeah.

00:12:18.756 --> 00:12:20.130
Even Prolog couldn't
handle that,

00:12:20.130 --> 00:12:22.129
even though they said
Prolog was an AI language.

00:12:22.129 --> 00:12:26.220
So the idea behind this
is it kind of emulates

00:12:26.220 --> 00:12:28.440
how the human mind works.

00:12:28.440 --> 00:12:30.690
So instead of me
telling the computer

00:12:30.690 --> 00:12:33.930
by having the intelligence to
know what golf looks like, I

00:12:33.930 --> 00:12:36.390
train the computer by
taking data about what

00:12:36.390 --> 00:12:39.480
golf looks like, and the
computer recognizes that data,

00:12:39.480 --> 00:12:40.439
matches that data.

00:12:40.439 --> 00:12:42.230
So in the future, when
I give it more data,

00:12:42.230 --> 00:12:44.040
it will say, that kind
of looks like golf,

00:12:44.040 --> 00:12:45.840
so I'm going to
say this is golf.

00:12:45.840 --> 00:12:48.280
So we talk about learning.

00:12:48.280 --> 00:12:49.660
We talk about the human brain.

00:12:49.660 --> 00:12:51.630
So I always like to
think like, well,

00:12:51.630 --> 00:12:53.730
think about how you
learn something--

00:12:53.730 --> 00:12:55.906
like maybe this game.

00:12:55.906 --> 00:12:57.030
Anybody remember this game?

00:12:57.030 --> 00:12:59.360
Everybody knows how to
play this game, right?

00:12:59.360 --> 00:13:00.965
It seems, by the
way, this game has

00:13:00.965 --> 00:13:02.340
different names
in every country,

00:13:02.340 --> 00:13:03.990
and it's always
hard to remember.

00:13:03.990 --> 00:13:05.895
I grew up calling it
knots and crosses.

00:13:05.895 --> 00:13:07.390
Ed's nodding.

00:13:07.390 --> 00:13:09.165
Most people grew
up maybe calling

00:13:09.165 --> 00:13:11.040
in this country tic-tac-toe.

00:13:11.040 --> 00:13:13.416
I gave a talk similar to this
in Japan earlier this year,

00:13:13.416 --> 00:13:15.248
and they had this really
strange name that I

00:13:15.248 --> 00:13:16.580
couldn't remember for it.

00:13:16.580 --> 00:13:18.930
But this is a very
simple game, right?

00:13:18.930 --> 00:13:21.310
Now, if I were to ask you
to play that game right now,

00:13:21.310 --> 00:13:25.240
and it's your move,
where would you go?

00:13:25.240 --> 00:13:28.030
How many people would
go in the center?

00:13:28.030 --> 00:13:30.170
How many people would
not go in the center?

00:13:30.170 --> 00:13:32.620
We need to talk.

00:13:32.620 --> 00:13:35.140
So you've probably learned
this as a young child--

00:13:35.140 --> 00:13:37.420
and maybe you teach
this to children.

00:13:37.420 --> 00:13:39.130
But the strategy of
winning this game.

00:13:39.130 --> 00:13:41.597
If it's your turn, you
will never win this game,

00:13:41.597 --> 00:13:43.180
unless you're playing
against somebody

00:13:43.180 --> 00:13:44.763
who doesn't know how
to play the game,

00:13:44.763 --> 00:13:46.750
by not going in
the center first.

00:13:46.750 --> 00:13:48.690
Now, remember how
you learned that.

00:13:48.690 --> 00:13:49.270
OK?

00:13:49.270 --> 00:13:51.100
If you have a really
tough teacher like me,

00:13:51.100 --> 00:13:52.750
I would teach my
kids by beating them

00:13:52.750 --> 00:13:56.237
every time at the game
and that kind of stuff.

00:13:56.237 --> 00:13:58.570
So if they would start in the
corner, I would beat them.

00:13:58.570 --> 00:13:59.800
And they would start
somewhere else,

00:13:59.800 --> 00:14:01.299
and I would beat
them-- at the game.

00:14:01.299 --> 00:14:02.740
And keep doing
this kind of thing

00:14:02.740 --> 00:14:04.552
until they eventually
figured out

00:14:04.552 --> 00:14:06.010
that they have to
go in the center,

00:14:06.010 --> 00:14:07.780
or they're going to lose.

00:14:07.780 --> 00:14:11.920
So that was a case of this is
how the human brain learns.

00:14:11.920 --> 00:14:15.920
So how do we teach a
computer the same way?

00:14:15.920 --> 00:14:18.310
Now think about, for
example, if your kids goes,

00:14:18.310 --> 00:14:21.070
and they've never seen
this board before.

00:14:21.070 --> 00:14:23.789
So in this society, we read
left to right, top to bottom.

00:14:23.789 --> 00:14:25.330
So the first thing
they'd probably do

00:14:25.330 --> 00:14:27.001
is go in the top
left-hand corner.

00:14:27.001 --> 00:14:29.000
And then you'd go in the
center, and then they'd

00:14:29.000 --> 00:14:29.680
go somewhere else.

00:14:29.680 --> 00:14:31.180
And you go somewhere else, and
then they go somewhere else,

00:14:31.180 --> 00:14:33.160
and you get three in a
row, and you beat them.

00:14:33.160 --> 00:14:35.770
They now have what, in
machine language parlance,

00:14:35.770 --> 00:14:37.510
is a labeled example.

00:14:37.510 --> 00:14:38.650
They see the board.

00:14:38.650 --> 00:14:40.470
They remember what
they did on the board,

00:14:40.470 --> 00:14:42.821
and that's been
labeled as they lost.

00:14:42.821 --> 00:14:44.320
Then they might
play again, and they

00:14:44.320 --> 00:14:46.546
have another labeled
example of they lost.

00:14:46.546 --> 00:14:48.670
And they'll keep doing that
until they have labeled

00:14:48.670 --> 00:14:50.920
examples of tying and
then maybe, eventually,

00:14:50.920 --> 00:14:52.780
labeled examples of winning.

00:14:52.780 --> 00:14:54.970
So knowing how to
learn is a step

00:14:54.970 --> 00:14:56.970
towards this kind
of intelligence.

00:14:56.970 --> 00:14:58.970
And this is when we talk
about machine learning,

00:14:58.970 --> 00:15:00.700
we get our data,
and we label them.

00:15:00.700 --> 00:15:02.380
It's exactly the
same as teaching

00:15:02.380 --> 00:15:06.010
a child how to play tic-tac-toe
or knots and crosses.

00:15:06.010 --> 00:15:08.840
So let's take a look at--

00:15:08.840 --> 00:15:11.080
so if I go back to this
diagram for a moment

00:15:11.080 --> 00:15:13.840
before I look at some
code, now the idea

00:15:13.840 --> 00:15:16.450
is thinking in terms
of tic-tac-toe,

00:15:16.450 --> 00:15:19.420
you have the answers of
experience of playing the game.

00:15:19.420 --> 00:15:21.100
You have the labels for that--

00:15:21.100 --> 00:15:22.810
that you won, you
lost, whatever.

00:15:22.810 --> 00:15:25.810
And out of that, as a human,
you'd begin to infer the rules.

00:15:25.810 --> 00:15:29.380
Did anybody ever teach
you you must go first?

00:15:29.380 --> 00:15:30.850
Must go in the center first.

00:15:30.850 --> 00:15:33.370
If you don't go in the center
first, you go in a corner.

00:15:33.370 --> 00:15:35.380
If you don't go in a
corner, you block somebody

00:15:35.380 --> 00:15:37.416
with two in a row?

00:15:37.416 --> 00:15:39.040
You don't learn by
those if-then rules.

00:15:39.040 --> 00:15:41.050
I know I didn't, and most
people I speak to didn't.

00:15:41.050 --> 00:15:43.008
So as a result, they
ended up playing the game,

00:15:43.008 --> 00:15:44.981
and they infer the
rules for themselves,

00:15:44.981 --> 00:15:47.230
and it's exactly the same
thing with machine learning.

00:15:47.230 --> 00:15:48.310
So you build something.

00:15:48.310 --> 00:15:50.260
The computer learns
how to infer the rules

00:15:50.260 --> 00:15:51.437
with a neural network.

00:15:51.437 --> 00:15:53.020
And then at runtime,
you give it data,

00:15:53.020 --> 00:15:54.770
and it will give you
back classifications,

00:15:54.770 --> 00:15:58.510
or predictions, or give you
back intelligent answers based

00:15:58.510 --> 00:16:01.000
on the data that you've given it

00:16:01.000 --> 00:16:02.299
So let's look at some code.

00:16:02.299 --> 00:16:04.090
So this is what we call
the training phase.

00:16:04.090 --> 00:16:06.010
This is what we call
the inference phase.

00:16:06.010 --> 00:16:07.810
But enough theory.

00:16:07.810 --> 00:16:11.440
So I like to explain a
lot of this in coding.

00:16:11.440 --> 00:16:13.570
So a very simple
Hello, World scenario,

00:16:13.570 --> 00:16:17.770
as all programmers have, is I'm
going to get use some numbers.

00:16:17.770 --> 00:16:19.780
And I'm going to give
you some numbers,

00:16:19.780 --> 00:16:22.654
and there's a relationship
between these numbers.

00:16:22.654 --> 00:16:25.070
And let's see who can figure
out what the relationship is.

00:16:25.070 --> 00:16:25.870
Are you ready?

00:16:25.870 --> 00:16:26.370
OK.

00:16:26.370 --> 00:16:28.060
Here's the numbers .

00:16:28.060 --> 00:16:31.330
So where x is minus
1, y is minus 3.

00:16:31.330 --> 00:16:34.912
Where x is 0, y is minus
1, et cetera, et cetera.

00:16:34.912 --> 00:16:37.120
Can you see the relationship
between the x and the y?

00:16:42.180 --> 00:16:47.070
So if y equals something,
what would y equal?

00:16:47.070 --> 00:16:48.367
AUDIENCE: 2x minus 1.

00:16:48.367 --> 00:16:49.575
LAURENCE MORONEY: 2x minus 1.

00:16:49.575 --> 00:16:50.520
Excellent.

00:16:50.520 --> 00:16:53.130
So the relationship here
is y equals 2x minus 1.

00:16:53.130 --> 00:16:54.785
How do you know that?

00:16:54.785 --> 00:16:55.660
How did you get that?

00:16:55.660 --> 00:16:57.060
AUDIENCE: [INAUDIBLE]

00:16:57.060 --> 00:16:58.310
LAURENCE MORONEY: What's that?

00:16:58.310 --> 00:16:59.260
AUDIENCE: [INAUDIBLE]

00:16:59.260 --> 00:17:00.080
LAURENCE MORONEY: I
can't hear you, sorry.

00:17:00.080 --> 00:17:00.966
AUDIENCE: It's
called a linear fit.

00:17:00.966 --> 00:17:02.341
LAURENCE MORONEY:
Oh, linear fit.

00:17:02.341 --> 00:17:03.290
OK, thanks.

00:17:03.290 --> 00:17:03.790
Yeah.

00:17:03.790 --> 00:17:07.710
So you've probably done some
basic geometry in school,

00:17:07.710 --> 00:17:10.050
and you think about usually
there's a relationship.

00:17:10.050 --> 00:17:12.400
Y equals mx plus c,
something along those lines.

00:17:12.400 --> 00:17:14.160
So you start plugging
in m and the c in.

00:17:14.160 --> 00:17:15.618
And then your mind
ultimately finds

00:17:15.618 --> 00:17:16.890
something that works, right?

00:17:16.890 --> 00:17:17.550
So you go, OK.

00:17:17.550 --> 00:17:19.800
Well, if y is minus
3, maybe that's

00:17:19.800 --> 00:17:22.170
a couple of x's, which
will give me minus 2.

00:17:22.170 --> 00:17:24.390
And I'll subtract 1 from
that, give me minus 3.

00:17:24.390 --> 00:17:25.890
And then I'll try
that with 0 and 1.

00:17:25.890 --> 00:17:26.589
Yep, that works.

00:17:26.589 --> 00:17:27.880
Now I'll try that with 1 and 1.

00:17:27.880 --> 00:17:28.572
That works.

00:17:28.572 --> 00:17:30.780
So what happened is there
were a couple of parameters

00:17:30.780 --> 00:17:32.780
around the y that
you started guessing

00:17:32.780 --> 00:17:35.280
what those parameters were and
started trying to fit them in

00:17:35.280 --> 00:17:36.780
to get that relationship.

00:17:36.780 --> 00:17:39.270
That's exactly what a
neural network does,

00:17:39.270 --> 00:17:42.450
and that's exactly the process
of training a neural network.

00:17:42.450 --> 00:17:44.580
When you train a neural
network to try and pick

00:17:44.580 --> 00:17:47.280
a relationship between numbers
like this, all it's doing

00:17:47.280 --> 00:17:52.350
is guessing those random
parameters, calculating--

00:17:52.350 --> 00:17:53.882
look through each
of the parameters,

00:17:53.882 --> 00:17:56.340
calculate which ones it got
right, which ones it got wrong,

00:17:56.340 --> 00:17:58.710
calculate how far it
got them wrong by,

00:17:58.710 --> 00:18:02.010
and then try and come up with
new values that would be closer

00:18:02.010 --> 00:18:03.295
to getting more of them right.

00:18:03.295 --> 00:18:04.920
And that's the process
called training.

00:18:04.920 --> 00:18:06.586
So whenever you see
training and talking

00:18:06.586 --> 00:18:09.450
about needing lots of cycles for
training, needing lots of GPU

00:18:09.450 --> 00:18:11.970
time for training, all the
computer is doing is trying,

00:18:11.970 --> 00:18:14.880
failing, trying, failing,
trying, failing, but each time

00:18:14.880 --> 00:18:16.810
getting a little
closer to the answer.

00:18:16.810 --> 00:18:18.870
So let's look at
the code for that.

00:18:18.870 --> 00:18:22.362
So using TensorFlow
and using Keras--

00:18:22.362 --> 00:18:23.820
I don't have the
code on my laptop,

00:18:23.820 --> 00:18:24.960
so I've got to look
back at the screen.

00:18:24.960 --> 00:18:25.830
Sorry.

00:18:25.830 --> 00:18:27.944
So using TensorFlow
and Keras, here's

00:18:27.944 --> 00:18:29.610
how I'm going to
define a neural network

00:18:29.610 --> 00:18:32.050
to do that linear fitting
in just a few lines.

00:18:32.050 --> 00:18:32.910
So the first thing
I'm going to do

00:18:32.910 --> 00:18:34.650
is I'm going to create
my neural network.

00:18:34.650 --> 00:18:36.790
This is the simplest
possible neural network.

00:18:36.790 --> 00:18:38.687
It's got one layer
with one neuron in it.

00:18:38.687 --> 00:18:40.020
And this is the code to do that.

00:18:40.020 --> 00:18:42.290
So where you see
keras.layers.Dense(units=1,

00:18:42.290 --> 00:18:46.740
input shape=1), that's all
that I'm doing is I'm saying

00:18:46.740 --> 00:18:47.910
I've got a single neuron.

00:18:47.910 --> 00:18:49.950
I'm going to pass a
single number into that,

00:18:49.950 --> 00:18:51.720
and you're going to try and
figure out what the number I

00:18:51.720 --> 00:18:53.220
want to come out of that is.

00:18:53.220 --> 00:18:55.360
So very, very simple.

00:18:55.360 --> 00:18:57.630
So then my next line
of code is remember

00:18:57.630 --> 00:19:00.030
I said all a neural
network is going to do

00:19:00.030 --> 00:19:01.830
is try and guess
the parameters that

00:19:01.830 --> 00:19:03.630
will make all the numbers fit?

00:19:03.630 --> 00:19:06.840
So it will come up with
a couple of rough guesses

00:19:06.840 --> 00:19:07.920
for these parameters.

00:19:07.920 --> 00:19:09.780
And then it has
these two functions.

00:19:09.780 --> 00:19:12.600
One's called a loss function,
and one's called an optimizer.

00:19:12.600 --> 00:19:13.800
And all they're doing is--

00:19:13.800 --> 00:19:16.170
if you remember that set
of six numbers I gave you,

00:19:16.170 --> 00:19:17.110
it's saying, OK.

00:19:17.110 --> 00:19:20.250
Well if y equals something
times x plus something,

00:19:20.250 --> 00:19:22.270
I'm going to guess
those two somethings.

00:19:22.270 --> 00:19:25.640
I'm going to measure how
many of my y's I got right.

00:19:25.640 --> 00:19:28.530
I'm going to measure how far
I'm wrong in all of the ones

00:19:28.530 --> 00:19:30.210
that I got wrong,
and then I'm going

00:19:30.210 --> 00:19:33.130
to try and guess new values
for those somethings.

00:19:33.130 --> 00:19:34.800
So the loss function
is the part where

00:19:34.800 --> 00:19:36.690
it's measuring how
far it got wrong,

00:19:36.690 --> 00:19:38.950
and the optimizer is saying, OK.

00:19:38.950 --> 00:19:40.840
Here's what I got the last time.

00:19:40.840 --> 00:19:43.380
I'm going to try to guess
these new parameters,

00:19:43.380 --> 00:19:46.410
and I'll keep going until
I get y equals 2x minus 1

00:19:46.410 --> 00:19:47.785
or something along those lines.

00:19:47.785 --> 00:19:48.660
So that's all you do.

00:19:48.660 --> 00:19:49.826
You just compile your model.

00:19:49.826 --> 00:19:51.300
You specify the loss function.

00:19:51.300 --> 00:19:52.920
You specify the optimizer.

00:19:52.920 --> 00:19:55.020
These are both really
heavy mathy things.

00:19:55.020 --> 00:19:56.730
One of the nice
things about Keras,

00:19:56.730 --> 00:19:57.690
one of the nice things
about TensorFlow,

00:19:57.690 --> 00:19:59.070
is they're all done for you.

00:19:59.070 --> 00:20:01.010
You're just going to
specify them in code.

00:20:01.010 --> 00:20:02.426
And so I'm going
to say, I'm going

00:20:02.426 --> 00:20:04.560
to try the mean squared
error as my loss function.

00:20:04.560 --> 00:20:06.101
And I'm going to
try something called

00:20:06.101 --> 00:20:08.010
SGD, which is
stochastic gradient

00:20:08.010 --> 00:20:09.960
descent as my optimizer.

00:20:09.960 --> 00:20:11.790
And every time it
loops around, it's

00:20:11.790 --> 00:20:14.610
going to just guess new
parameters based on those.

00:20:14.610 --> 00:20:15.300
OK.

00:20:15.300 --> 00:20:16.883
So then the next
thing I'm going to do

00:20:16.883 --> 00:20:19.460
is I'm going to feed my
values into my neural network.

00:20:19.460 --> 00:20:21.540
So I'm going to
say, my x is going

00:20:21.540 --> 00:20:24.690
to be this array-- minus
1, 0, 1, et cetera.

00:20:24.690 --> 00:20:26.560
My y is that going
to be this array.

00:20:26.560 --> 00:20:28.320
So here I'm creating the data.

00:20:28.320 --> 00:20:30.721
And so I just get
them, and I load them

00:20:30.721 --> 00:20:31.720
into a couple of arrays.

00:20:31.720 --> 00:20:33.870
This is Python code, by the way.

00:20:33.870 --> 00:20:37.080
And now all that I'm going to
ask my neural network to do

00:20:37.080 --> 00:20:39.340
is to try and come
up with an answer.

00:20:39.340 --> 00:20:42.220
And I do that with
the fit method.

00:20:42.220 --> 00:20:45.780
So here I just say, hey, try
and fit my x's to my y's.

00:20:45.780 --> 00:20:50.320
And this epochs=500 means you're
going to just try 500 times.

00:20:50.320 --> 00:20:52.070
So it's going to loop
500 times like that.

00:20:52.070 --> 00:20:54.445
Remember I was saying it's
going to get those parameters.

00:20:54.445 --> 00:20:55.620
It's going to get it wrong.

00:20:55.620 --> 00:20:56.510
It's going to optimize.

00:20:56.510 --> 00:20:57.300
It's going to guess again.

00:20:57.300 --> 00:20:58.020
It's going to get it wrong.

00:20:58.020 --> 00:20:59.100
It's going to optimize.

00:20:59.100 --> 00:21:02.220
So in this case in my code, I'm
just saying do that 500 times.

00:21:02.220 --> 00:21:03.990
And at the end of
those 500 times,

00:21:03.990 --> 00:21:05.490
it's going to come
up with a model

00:21:05.490 --> 00:21:07.006
that if I gave it a y-- sorry.

00:21:07.006 --> 00:21:08.880
If I give it an x, it's
going to give me what

00:21:08.880 --> 00:21:10.810
it thinks the y is for that x.

00:21:10.810 --> 00:21:11.310
OK.

00:21:11.310 --> 00:21:13.650
And you do that using
model.predict().

00:21:13.650 --> 00:21:16.422
So if I pass it model.predict()
for the value 10,

00:21:16.422 --> 00:21:17.880
what do you think
it would give me?

00:21:21.000 --> 00:21:23.640
If you remember the numbers
from earlier, y is 2x minus 1.

00:21:23.640 --> 00:21:25.710
What do you think it would give?

00:21:25.710 --> 00:21:27.450
19, right?

00:21:27.450 --> 00:21:31.110
It doesn't because it will
give me something really close

00:21:31.110 --> 00:21:31.740
to 19.

00:21:31.740 --> 00:21:34.090
It gives me about
18.97, and I'm going

00:21:34.090 --> 00:21:36.240
to try to run the code
in a moment to show.

00:21:36.240 --> 00:21:38.824
But why do you think
it would do that?

00:21:38.824 --> 00:21:40.170
AUDIENCE: [INAUDIBLE]

00:21:40.170 --> 00:21:40.590
LAURENCE MORONEY: What's that?

00:21:40.590 --> 00:21:41.760
AUDIENCE: It's predicting.

00:21:41.760 --> 00:21:43.176
LAURENCE MORONEY:
It's predicting.

00:21:43.176 --> 00:21:46.524
And it's just been trained
on a very few pieces of data.

00:21:46.524 --> 00:21:48.690
With those six pieces of
data, it looks like a line,

00:21:48.690 --> 00:21:50.680
and it looks like a
linear relationship,

00:21:50.680 --> 00:21:52.360
but it might not be.

00:21:52.360 --> 00:21:55.770
There's room for error there
that with the fact that I'm

00:21:55.770 --> 00:21:57.662
training on very,
very little data,

00:21:57.662 --> 00:22:00.120
this could be a small part of
a line that, for all we know,

00:22:00.120 --> 00:22:03.180
goes like this instead of
being linear once you move out

00:22:03.180 --> 00:22:04.160
of those points.

00:22:04.160 --> 00:22:05.701
And as a result,
those kind of things

00:22:05.701 --> 00:22:08.250
get factored into the model
as the model's training on it.

00:22:08.250 --> 00:22:10.170
So you'll see it's going
to get a very close answer,

00:22:10.170 --> 00:22:11.940
but it's not going to
be an exact answer.

00:22:11.940 --> 00:22:14.730
Let me see if I can
get the code running.

00:22:14.730 --> 00:22:16.480
It's a little complex
with this laptop.

00:22:20.539 --> 00:22:22.580
When I'm presenting, it's
hard to move stuff over

00:22:22.580 --> 00:22:23.300
to that screen.

00:22:23.300 --> 00:22:24.140
Just one second.

00:22:32.060 --> 00:22:35.610
This requires some mouse-fu.

00:22:35.610 --> 00:22:36.110
All right.

00:22:36.110 --> 00:22:39.700
So I have that code.

00:22:39.700 --> 00:22:40.660
Let's see.

00:22:40.660 --> 00:22:41.190
Yeah.

00:22:41.190 --> 00:22:43.620
So you can see that code I
have now running up there.

00:22:43.620 --> 00:22:48.930
And if you look right at the
bottom of the screen over here,

00:22:48.930 --> 00:22:51.360
we can see here's where it has
actually done the training.

00:22:51.360 --> 00:22:53.830
It's done 500 epochs
worth of training.

00:22:53.830 --> 00:22:55.650
And then when I called
the model.predict(),

00:22:55.650 --> 00:23:00.326
it gave me this answer,
which is 18.976414.

00:23:00.326 --> 00:23:01.950
And so that was one
that I ran earlier.

00:23:01.950 --> 00:23:04.158
I'm just going to try and
run it again now, if I can.

00:23:06.544 --> 00:23:07.710
But it's really hard to see.

00:23:11.558 --> 00:23:15.080
So I'll click that Run arrow.

00:23:15.080 --> 00:23:17.260
So this IDE is
PyCharm, by the way.

00:23:17.260 --> 00:23:19.305
So you see that it ran
very quickly because it's

00:23:19.305 --> 00:23:20.706
a very simple neural network.

00:23:20.706 --> 00:23:22.330
And as a result, I
was able to train it

00:23:22.330 --> 00:23:24.890
through 500 epochs in whatever
that is-- half a second.

00:23:24.890 --> 00:23:26.140
What did it give me this time?

00:23:26.140 --> 00:23:28.000
Was it 18.9747?

00:23:28.000 --> 00:23:29.880
Is that what I see?

00:23:29.880 --> 00:23:33.090
So again, very simple neural
network, very simple code,

00:23:33.090 --> 00:23:36.400
but this just shows some of
the basics for how it works.

00:23:36.400 --> 00:23:39.940
So next, I want to just get to
a slightly more advanced example

00:23:39.940 --> 00:23:41.040
once I get my slides back.

00:23:45.690 --> 00:23:47.560
Whoops.

00:23:47.560 --> 00:23:49.130
OK.

00:23:49.130 --> 00:23:50.390
So that was very simple.

00:23:50.390 --> 00:23:51.500
That was Hello, World.

00:23:51.500 --> 00:23:54.470
We all remember our first Hello,
World program which we wrote.

00:23:54.470 --> 00:23:56.840
If you wrote it in Java,
it was like 10 lines.

00:23:56.840 --> 00:23:59.150
If you wrote it in
C#, it was five lines.

00:23:59.150 --> 00:24:01.240
If you wrote it in
Python, it was one line.

00:24:01.240 --> 00:24:03.530
If you wrote it in C++,
it was like 300 lines.

00:24:03.530 --> 00:24:06.244
[LAUGHTER]

00:24:06.244 --> 00:24:07.160
Do you remember that--

00:24:07.160 --> 00:24:09.460
I remember Petzold's book
on programming Windows.

00:24:09.460 --> 00:24:10.710
Anybody ever read that?

00:24:10.710 --> 00:24:13.712
The whole first chapter was
how to do Hello, World in MFC,

00:24:13.712 --> 00:24:15.400
and it was like 15 pages long.

00:24:15.400 --> 00:24:16.822
I thought it was great.

00:24:16.822 --> 00:24:18.280
But that was a
pretty easy example.

00:24:18.280 --> 00:24:20.320
That, to me, is the Hello,
World of machine learning-- just

00:24:20.320 --> 00:24:21.653
doing that basic linear fitting.

00:24:21.653 --> 00:24:24.740
But let's think about
something more complicated.

00:24:24.740 --> 00:24:26.560
So here are some
items of clothing.

00:24:26.560 --> 00:24:28.660
Now, as a human, you are looking
at these items of clothing,

00:24:28.660 --> 00:24:30.160
and you've instantly
classified them.

00:24:30.160 --> 00:24:31.660
And you instantly
recognize them, or at least

00:24:31.660 --> 00:24:32.786
hopefully most of them.

00:24:32.786 --> 00:24:34.660
But think about the
difficulty for a computer

00:24:34.660 --> 00:24:35.470
to classify them.

00:24:35.470 --> 00:24:38.800
For example, there are
two shoes on this slide.

00:24:38.800 --> 00:24:40.780
One is the high heel
shoe in the upper right,

00:24:40.780 --> 00:24:43.720
and one is the sneaker
in the second row.

00:24:43.720 --> 00:24:46.660
But they look really
different to each other--

00:24:46.660 --> 00:24:48.970
other than the fact
that they're both red,

00:24:48.970 --> 00:24:51.880
and you think they
vaguely fit a foot.

00:24:51.880 --> 00:24:54.700
The high heel, obviously your
foot has to change to fit it.

00:24:54.700 --> 00:24:57.179
And the sneaker,
the foot is flat.

00:24:57.179 --> 00:24:59.470
But as a human brain, we
automatically recognize these,

00:24:59.470 --> 00:25:01.030
and we see these as shoes.

00:25:01.030 --> 00:25:03.790
Or if we look at the two shirts
in the image, one of them

00:25:03.790 --> 00:25:06.400
doesn't have arms
because we automatically

00:25:06.400 --> 00:25:08.920
see it as being folded--
the one with the tie.

00:25:08.920 --> 00:25:10.832
And then the green one
in the lower left--

00:25:10.832 --> 00:25:12.790
we already know it's a
shirt-- it's a t-shirt--

00:25:12.790 --> 00:25:14.680
because we recognize it as such.

00:25:14.680 --> 00:25:16.720
But think about how would
you program a computer

00:25:16.720 --> 00:25:19.156
to recognize these things,
given the differences?

00:25:19.156 --> 00:25:20.780
It's really hard to
tell the difference

00:25:20.780 --> 00:25:24.560
between a high heeled shoe
and a sneaker, for example.

00:25:24.560 --> 00:25:26.710
So the idea behind this
is there's actually a data

00:25:26.710 --> 00:25:28.430
set called Fashion MNIST.

00:25:28.430 --> 00:25:31.780
And what it does is it gets
70,000 items of clothing,

00:25:31.780 --> 00:25:34.420
and it's labeled those
70,000 items of clothing

00:25:34.420 --> 00:25:37.750
in 10 different classes from
shirts, to shoes, to handbags,

00:25:37.750 --> 00:25:38.990
and all that kind of thing.

00:25:38.990 --> 00:25:40.990
And it's built into Keras.

00:25:40.990 --> 00:25:43.810
So one of the really neat things
that came out of the research

00:25:43.810 --> 00:25:46.930
behind this, by the way, is
that the images are only 28

00:25:46.930 --> 00:25:48.710
by 28 pixels.

00:25:48.710 --> 00:25:51.160
So if you think about, it's
faster to train a computer

00:25:51.160 --> 00:25:52.720
if you're using less data.

00:25:52.720 --> 00:25:54.910
You saw how quickly I trained
with my linear example

00:25:54.910 --> 00:25:55.695
earlier on.

00:25:55.695 --> 00:25:57.070
But if I were to
try and train it

00:25:57.070 --> 00:25:59.320
with high definition
images of handbags

00:25:59.320 --> 00:26:01.540
and that kind of stuff,
it would still work,

00:26:01.540 --> 00:26:02.747
but it would just be slower.

00:26:02.747 --> 00:26:05.080
And a lot of the research
that's gone into this dataset,

00:26:05.080 --> 00:26:07.142
they've actually been
able to train and show

00:26:07.142 --> 00:26:09.100
how to train a neural
network that all you need

00:26:09.100 --> 00:26:11.560
is a 28 by 28
pixel image for you

00:26:11.560 --> 00:26:13.090
to be able to tell
the difference

00:26:13.090 --> 00:26:14.680
between different
items of clothing.

00:26:14.680 --> 00:26:17.055
As you are doing probably
right now, you can take a look,

00:26:17.055 --> 00:26:18.429
and you see which
ones are pants,

00:26:18.429 --> 00:26:20.020
which ones are
shoes, which ones are

00:26:20.020 --> 00:26:21.800
handbags, that kind of thing.

00:26:21.800 --> 00:26:23.530
So this allows us to
build a model that's

00:26:23.530 --> 00:26:25.480
very, very quick to train.

00:26:25.480 --> 00:26:28.030
And if I take a look,
here's an example

00:26:28.030 --> 00:26:30.940
of one item of clothing
in 28 by 28 pixels.

00:26:30.940 --> 00:26:33.310
And you automatically
recognize that, right?

00:26:33.310 --> 00:26:36.500
It's a boot, or a shoe, or
something along those lines.

00:26:36.500 --> 00:26:38.830
And so this is the kind
of resolution of data--

00:26:38.830 --> 00:26:41.870
all you need to be able to
build an accurate classifier.

00:26:41.870 --> 00:26:43.880
So let's look at
the code for that.

00:26:43.880 --> 00:26:46.570
So if you remember earlier on,
the code that I was building

00:26:46.570 --> 00:26:48.910
was I created the
neural network.

00:26:48.910 --> 00:26:50.920
I compiled a neural
network by specifying

00:26:50.920 --> 00:26:54.550
the loss function and the
optimizer, and then I fit it.

00:26:54.550 --> 00:26:56.564
So in this case, a
little bit more complex.

00:26:56.564 --> 00:26:59.230
Your code's going to look like--
you're going to use TensorFlow.

00:26:59.230 --> 00:27:01.900
From TensorFlow, you're going
to import the Keras namespace

00:27:01.900 --> 00:27:05.290
because the Keras namespace
really nicely gives you access

00:27:05.290 --> 00:27:07.250
to that Fashion MNIST dataset.

00:27:07.250 --> 00:27:09.250
So think about all the
code that you'd typically

00:27:09.250 --> 00:27:11.950
have to write to download
those 70,000 images,

00:27:11.950 --> 00:27:14.290
download their
labels, correspond

00:27:14.290 --> 00:27:16.349
a label with an image,
load all of that in--

00:27:16.349 --> 00:27:17.140
that kind of stuff.

00:27:17.140 --> 00:27:19.772
All that coding is
saved and just put

00:27:19.772 --> 00:27:20.980
into these two lines of code.

00:27:20.980 --> 00:27:23.620
And that's one of the neat
things also about Python that I

00:27:23.620 --> 00:27:26.260
find that makes Python great for
machine learning because that

00:27:26.260 --> 00:27:29.680
second line of code there
where it's like train_images,

00:27:29.680 --> 00:27:33.430
train_labels, test_images,
test_labels equals

00:27:33.430 --> 00:27:36.130
fashion_mnist.load_data(),
what that's actually doing is

00:27:36.130 --> 00:27:38.320
it's loading data from the
dataset which is stored

00:27:38.320 --> 00:27:39.220
in the cloud.

00:27:39.220 --> 00:27:43.600
It's sorting that 70,000
items of data into four sets.

00:27:43.600 --> 00:27:46.720
Those four sets are then split
into two sets, one for training

00:27:46.720 --> 00:27:47.759
and one for testing.

00:27:47.759 --> 00:27:50.050
And that data is going to
contain-- the one on the left

00:27:50.050 --> 00:27:54.190
there, the training
images, is 60,000 images

00:27:54.190 --> 00:27:56.020
and 60,000 labels.

00:27:56.020 --> 00:27:58.989
And then the other side is
10,000 images and 10,000 labels

00:27:58.989 --> 00:28:00.530
that you're going
to use for testing.

00:28:00.530 --> 00:28:02.830
Now, anybody guess why would
you separate them like this?

00:28:02.830 --> 00:28:03.940
Why would you have
a different set

00:28:03.940 --> 00:28:05.815
for testing than you
would have for training?

00:28:05.815 --> 00:28:07.290
AUDIENCE: [INAUDIBLE]

00:28:07.290 --> 00:28:09.440
LAURENCE MORONEY: The
clue's in the name.

00:28:09.440 --> 00:28:11.120
So how do you know
your neural network

00:28:11.120 --> 00:28:12.953
is going to work unless
you've got something

00:28:12.953 --> 00:28:14.050
to test it against?

00:28:14.050 --> 00:28:16.070
Earlier, we could test
with our linear thing

00:28:16.070 --> 00:28:19.580
by feeding 10 in because I
know I'm expecting 2x minus 1

00:28:19.580 --> 00:28:20.504
to give me 19.

00:28:20.504 --> 00:28:21.920
But now it's a
case of, well, it'd

00:28:21.920 --> 00:28:24.652
be great for me to be able
to test it against something

00:28:24.652 --> 00:28:26.610
that's known, against
something that's labeled,

00:28:26.610 --> 00:28:28.940
so I can measure the
accuracy as I go forward.

00:28:28.940 --> 00:28:30.387
So that's all I
got to do in code.

00:28:30.387 --> 00:28:31.970
So now if I come
back here, let's look

00:28:31.970 --> 00:28:34.178
at how we actually define
the neural net-- oh, sorry.

00:28:34.178 --> 00:28:36.110
Before I do that, so
the training images

00:28:36.110 --> 00:28:38.360
are things like the boot
that I showed you earlier on.

00:28:38.360 --> 00:28:40.134
It's 28 by 28 pixels.

00:28:40.134 --> 00:28:41.550
The labels are
actually just going

00:28:41.550 --> 00:28:43.910
to be numbers rather than
like a word like shoe.

00:28:43.910 --> 00:28:45.201
Why do you think that would be?

00:28:47.796 --> 00:28:49.420
So that you can define
your own labels,

00:28:49.420 --> 00:28:51.490
and you're not
limited to English.

00:28:51.490 --> 00:28:55.090
So for example, 09 in English
could be an ankle boots.

00:28:55.090 --> 00:28:57.110
The second one is in Chinese.

00:28:57.110 --> 00:28:58.690
The third one is in Japanese.

00:28:58.690 --> 00:29:01.900
And the fourth language,
can anybody guess?

00:29:01.900 --> 00:29:03.810
Brog ruitin?

00:29:03.810 --> 00:29:05.380
That's actually Irish Gaelic.

00:29:05.380 --> 00:29:06.130
Sorry, I'm biased.

00:29:06.130 --> 00:29:08.110
I have to put some in.

00:29:08.110 --> 00:29:10.510
So now, for example, I
could build a classifier

00:29:10.510 --> 00:29:12.520
not just to give me
items of clothing

00:29:12.520 --> 00:29:14.300
but to do it in
different languages.

00:29:14.300 --> 00:29:16.540
So that's just what my labels
are going to look like.

00:29:16.540 --> 00:29:18.040
So now let's take
a look at the code

00:29:18.040 --> 00:29:20.150
for defining my neural network.

00:29:20.150 --> 00:29:22.750
So here is-- if you remember
the first line of code

00:29:22.750 --> 00:29:25.300
where I defined the single
layer with the single neuron

00:29:25.300 --> 00:29:28.130
for the classification, this is
what it's going to look like.

00:29:28.130 --> 00:29:31.760
And this is all it takes to
build this clothing classifier.

00:29:31.760 --> 00:29:33.460
So you see there are
three layers here.

00:29:33.460 --> 00:29:36.200
The first layer, where it says
keras.layers.Flatten(input

00:29:36.200 --> 00:29:41.570
shape=(28, 28)), all that is
is I'm defining a layer to take

00:29:41.570 --> 00:29:44.270
in 28 squared values.

00:29:44.270 --> 00:29:47.670
Remember, the image is a
square of 28 by 28 pixels,

00:29:47.670 --> 00:29:49.810
but you don't feed a neural
network with a square.

00:29:49.810 --> 00:29:52.550
You feed it with a
flat layer of values.

00:29:52.550 --> 00:29:55.434
In this case, the values
are between 0 and 256.

00:29:55.434 --> 00:29:57.100
So I'm just flattening
that out, and I'm

00:29:57.100 --> 00:29:58.510
saying, that's my first layer.

00:29:58.510 --> 00:30:00.970
You're going to take in
whatever 28 squared is.

00:30:00.970 --> 00:30:03.990
My second layer now
is just 128 neurons,

00:30:03.990 --> 00:30:05.740
and there's an activation
function on them

00:30:05.740 --> 00:30:07.400
which I'll explain in a moment.

00:30:07.400 --> 00:30:09.540
And then my third layer
is going to be 10 neurons.

00:30:09.540 --> 00:30:11.290
Why do you think there
are 10 in that one?

00:30:11.290 --> 00:30:12.580
Can anybody guess?

00:30:12.580 --> 00:30:16.630
So 28 squared for the
inputs, 10 for the output.

00:30:16.630 --> 00:30:18.810
Anybody remember where the
number 10 was mentioned?

00:30:18.810 --> 00:30:19.750
AUDIENCE: [INAUDIBLE]

00:30:19.750 --> 00:30:20.620
LAURENCE MORONEY:
Yeah, number of labels.

00:30:20.620 --> 00:30:22.610
There were 10 different classes.

00:30:22.610 --> 00:30:24.245
So what happens when
a neural network,

00:30:24.245 --> 00:30:26.870
when you train it like this one,
it's not just going to pop out

00:30:26.870 --> 00:30:28.470
and give you an answer and
say, this is number 03,

00:30:28.470 --> 00:30:29.770
or this is number 04.

00:30:29.770 --> 00:30:31.420
Typically what will
happen is that you

00:30:31.420 --> 00:30:34.570
want to have 10 outputs for
your 10 different labels,

00:30:34.570 --> 00:30:37.210
and each output is going to
give you a probability that it

00:30:37.210 --> 00:30:38.209
is that label.

00:30:38.209 --> 00:30:40.750
So for example, the boot that
I showed earlier on was labeled

00:30:40.750 --> 00:30:44.252
09, so neuron 0 is going to
give me a very low number.

00:30:44.252 --> 00:30:46.210
Neuron 1 is going to give
me a very low number.

00:30:46.210 --> 00:30:48.280
Neuron 2 is going to give
me a very low number.

00:30:48.280 --> 00:30:50.570
Neuron 9 is going to give
me a very high number.

00:30:50.570 --> 00:30:52.444
And then by looking at
the outputs across all

00:30:52.444 --> 00:30:54.190
of these neurons,
I can now determine

00:30:54.190 --> 00:30:57.460
which one the neural network
thinks it's classified for.

00:30:57.460 --> 00:30:59.814
Remember, we're training
this with a bunch of data.

00:30:59.814 --> 00:31:02.230
So I'm giving it a whole bunch
of data to say this is what

00:31:02.230 --> 00:31:03.431
a number 09 looks like.

00:31:03.431 --> 00:31:04.930
This is what a
number 04 looks like.

00:31:04.930 --> 00:31:06.550
This is what a
number 03 looks like.

00:31:06.550 --> 00:31:08.170
By saying, OK, this
is what they are.

00:31:08.170 --> 00:31:10.030
I encode the data
in the same way.

00:31:10.030 --> 00:31:12.380
And as a result, we'll
get our output like this.

00:31:12.380 --> 00:31:15.280
Now, every neuron has what's
called an activation function.

00:31:15.280 --> 00:31:18.140
And the idea behind that-- it's
a very mathy kind of thing.

00:31:18.140 --> 00:31:22.330
But in programmer's terms, the
tf.nn.relu that you see there--

00:31:22.330 --> 00:31:24.850
if you think about
this in terms of code,

00:31:24.850 --> 00:31:28.210
if I say if x is greater
than zero, return x.

00:31:28.210 --> 00:31:30.070
Else, return zero.

00:31:30.070 --> 00:31:30.730
OK?

00:31:30.730 --> 00:31:33.225
Very simple function, and
that's what the relu is.

00:31:33.225 --> 00:31:36.826
And all that's going to do is
as the code is being filtered in

00:31:36.826 --> 00:31:38.200
and then down into
those neurons,

00:31:38.200 --> 00:31:40.534
all of the stuff that's
negative just gets filtered out.

00:31:40.534 --> 00:31:42.200
So as a result, it
makes it much quicker

00:31:42.200 --> 00:31:44.320
for you to train your
neural network by getting

00:31:44.320 --> 00:31:45.910
rid of things that are
negative, they're by rid

00:31:45.910 --> 00:31:47.050
of things you don't need.

00:31:47.050 --> 00:31:49.600
So every time when you specify
a layer in a neural network,

00:31:49.600 --> 00:31:51.720
there's usually an activation
function like that.

00:31:51.720 --> 00:31:53.290
Relu is one of the
most common ones

00:31:53.290 --> 00:31:55.330
that you'll see, particularly
for classification

00:31:55.330 --> 00:31:56.450
things like this.

00:31:56.450 --> 00:31:58.540
But again, relu is
a very mathy thing.

00:31:58.540 --> 00:32:00.566
A lot of times, you go
to the documentation,

00:32:00.566 --> 00:32:01.690
you'll wonder what relu is.

00:32:01.690 --> 00:32:02.450
You'll go look it up.

00:32:02.450 --> 00:32:04.116
You'll see a page
full of Greek letters.

00:32:04.116 --> 00:32:05.500
I don't understand that stuff.

00:32:05.500 --> 00:32:07.750
So for me, something
like relu is as simple

00:32:07.750 --> 00:32:10.315
as if x is greater
than zero, return x.

00:32:10.315 --> 00:32:11.980
Else, return zero.

00:32:11.980 --> 00:32:12.497
All right.

00:32:12.497 --> 00:32:14.080
So now I've defined
my neural network.

00:32:14.080 --> 00:32:15.550
And the next thing
I'm going to do,

00:32:15.550 --> 00:32:17.590
you'll see the
same code as we saw

00:32:17.590 --> 00:32:19.180
earlier on where
what I'm going to do

00:32:19.180 --> 00:32:20.722
is compile my neural network.

00:32:20.722 --> 00:32:22.180
And in compiling
my neural network,

00:32:22.180 --> 00:32:23.830
I've got to specify
the loss, and I've

00:32:23.830 --> 00:32:25.285
got to specify the optimizer.

00:32:25.285 --> 00:32:27.160
Now, there's a whole
bunch of different types

00:32:27.160 --> 00:32:28.000
of loss functions.

00:32:28.000 --> 00:32:29.666
There's a whole bunch
of different types

00:32:29.666 --> 00:32:30.670
of optimizer functions.

00:32:30.670 --> 00:32:34.660
When you read academic research
papers around AI, a lot of them

00:32:34.660 --> 00:32:37.910
specialize on these to say,
for this type of problem,

00:32:37.910 --> 00:32:40.750
you should use a loss function
of sparse categorical cross

00:32:40.750 --> 00:32:42.480
entropy because x.

00:32:42.480 --> 00:32:44.050
For this type of
problem, you should

00:32:44.050 --> 00:32:47.170
use an optimizer, which is
an Adam-based optimizer,

00:32:47.170 --> 00:32:48.370
because x.

00:32:48.370 --> 00:32:50.320
A lot of this as a
programmer, you just

00:32:50.320 --> 00:32:52.360
have to learn through
trial and error.

00:32:52.360 --> 00:32:55.930
I could specify the same loss
function and the same optimizer

00:32:55.930 --> 00:32:58.560
that I use for my linear
and then try and train

00:32:58.560 --> 00:33:00.310
my neural network, see
how accurate it is,

00:33:00.310 --> 00:33:01.157
how quick it is.

00:33:01.157 --> 00:33:03.490
And then I could try these
ones, see how accurate it is,

00:33:03.490 --> 00:33:04.370
see how quick it is.

00:33:04.370 --> 00:33:06.310
There's a lot of trial
and error in that way.

00:33:06.310 --> 00:33:08.350
And understanding which
ones to use right now

00:33:08.350 --> 00:33:09.970
is an inexact science.

00:33:09.970 --> 00:33:13.390
It's a lot like, for example,
as a traditional coder, which

00:33:13.390 --> 00:33:15.970
is better-- using a
for loop or a do loop?

00:33:15.970 --> 00:33:18.550
Which is better-- using
a while or using a when?

00:33:18.550 --> 00:33:19.714
Those type of things.

00:33:19.714 --> 00:33:21.130
And as a result,
you see as you're

00:33:21.130 --> 00:33:22.450
building your neural
networks, there's

00:33:22.450 --> 00:33:24.310
a lot of trial and error
that you'll do here.

00:33:24.310 --> 00:33:26.020
But reading academic
papers can certainly

00:33:26.020 --> 00:33:28.130
help if you can understand them.

00:33:28.130 --> 00:33:32.104
So in this case now, like
for the Fashion MNIST,

00:33:32.104 --> 00:33:33.520
after a bit of
trial and error, we

00:33:33.520 --> 00:33:35.634
ended up selecting
for the tutorial

00:33:35.634 --> 00:33:37.300
to use these two
functions. / But as you

00:33:37.300 --> 00:33:38.883
read through the
documentation, you'll

00:33:38.883 --> 00:33:40.700
see all the functions
that are available.

00:33:40.700 --> 00:33:43.420
So in this case, I'm training
it with an AdamOptimizer.

00:33:43.420 --> 00:33:45.130
And remember, the
process of training,

00:33:45.130 --> 00:33:48.000
every iteration it will
make a guess that says, OK.

00:33:48.000 --> 00:33:50.107
This piece of data,
I think it's a shoe.

00:33:50.107 --> 00:33:50.940
OK, it's not a shoe.

00:33:50.940 --> 00:33:51.510
It's a dress.

00:33:51.510 --> 00:33:53.250
Why did I get it wrong?

00:33:53.250 --> 00:33:56.040
I'll use my loss function to
calculate where I got it wrong,

00:33:56.040 --> 00:33:58.470
and then I'll use my
optimizer to change

00:33:58.470 --> 00:34:00.660
my weights on the next
loop to try and see

00:34:00.660 --> 00:34:01.677
if I can get it better.

00:34:01.677 --> 00:34:03.510
This is what the neural
network is thinking.

00:34:03.510 --> 00:34:05.551
This how it works as you're
actually training it.

00:34:05.551 --> 00:34:07.020
So in this case,
the AdamOptimizer

00:34:07.020 --> 00:34:08.811
is what it's using to
do that optimization.

00:34:08.811 --> 00:34:14.590
The categorical cross entropy
is what it's using for the loss.

00:34:14.590 --> 00:34:17.610
So now if I train it, it's the
same thing that we saw earlier

00:34:17.610 --> 00:34:19.080
on-- model.fit().

00:34:19.080 --> 00:34:21.750
So all I'm going to say
is, hey, model.fit().

00:34:21.750 --> 00:34:25.020
I'm going to train it with
the input images and the input

00:34:25.020 --> 00:34:27.090
labels, and in this
case, I'm going

00:34:27.090 --> 00:34:28.660
to train it for five epochs.

00:34:28.660 --> 00:34:29.159
OK.

00:34:29.159 --> 00:34:31.714
So that epochs number,
it's up to you to tweak it.

00:34:31.714 --> 00:34:33.630
What you'll do as you're
training your network

00:34:33.630 --> 00:34:35.088
and as you're
testing your network,

00:34:35.088 --> 00:34:36.630
you'll see how accurate it is.

00:34:36.630 --> 00:34:39.570
Sometimes you can get the
process called converging,

00:34:39.570 --> 00:34:41.250
means as it gets more
and more accurate,

00:34:41.250 --> 00:34:42.570
sometimes you'll
find convergence

00:34:42.570 --> 00:34:43.440
in only a few epochs.

00:34:43.440 --> 00:34:45.179
Sometimes, you'll need
hundreds of epochs.

00:34:45.179 --> 00:34:46.560
Of course, the bigger
and more complex

00:34:46.560 --> 00:34:48.545
the dataset, and the more
labels that you have,

00:34:48.545 --> 00:34:50.670
the longer it takes to
actually train and converge.

00:34:50.670 --> 00:34:52.980
But the Fashion MNIST
dataset, actually

00:34:52.980 --> 00:34:55.949
using the neural network that I
defined in the previous slide,

00:34:55.949 --> 00:34:57.990
five epochs is actually
pretty accurate.

00:34:57.990 --> 00:35:01.060
It gets there pretty
quickly with just five.

00:35:01.060 --> 00:35:01.660
OK.

00:35:01.660 --> 00:35:06.170
And now if I then just want to
test it and the model itself--

00:35:06.170 --> 00:35:09.460
again, the important object
here is the model object.

00:35:09.460 --> 00:35:12.480
So if I call model.evaluate(),
and I pass it the test images

00:35:12.480 --> 00:35:15.960
and the text labels, it will
then iterate through the 10,000

00:35:15.960 --> 00:35:17.640
test images and test labels.

00:35:17.640 --> 00:35:18.420
It will calculate.

00:35:18.420 --> 00:35:21.024
It will say, I think
it's going to be this.

00:35:21.024 --> 00:35:22.440
It will compare
it with the label.

00:35:22.440 --> 00:35:24.030
If it gets it right,
it improves its score.

00:35:24.030 --> 00:35:25.620
If it gets it wrong,
it decreases its score,

00:35:25.620 --> 00:35:27.090
and it gives you
that score back.

00:35:27.090 --> 00:35:28.950
So the idea here
is-- remember earlier

00:35:28.950 --> 00:35:32.220
when we separated the data into
60,000 for training and 10,000

00:35:32.220 --> 00:35:33.000
for test?

00:35:33.000 --> 00:35:35.899
Instead of you manually writing
all that code to do all that,

00:35:35.899 --> 00:35:38.190
you can just call the evaluate()
function on the model,

00:35:38.190 --> 00:35:40.773
pass it the test stuff, and it
will give you back the results.

00:35:40.773 --> 00:35:44.030
It will do all that looping
and checking for you.

00:35:44.030 --> 00:35:44.750
All right.

00:35:47.970 --> 00:35:51.550
And then, of course, if I
want to predict an image,

00:35:51.550 --> 00:35:54.930
if I have my own images, and
I've formatted them into 28

00:35:54.930 --> 00:35:57.450
by 28 grayscale, and
I put them into a set,

00:35:57.450 --> 00:35:59.670
now I can just say
model.predict() my images,

00:35:59.670 --> 00:36:01.680
and it will give me back
a set of predictions.

00:36:01.680 --> 00:36:03.730
Now, what do those
predictions look like?

00:36:03.730 --> 00:36:07.320
So for every image, because the
output of the neural network

00:36:07.320 --> 00:36:08.070
was--

00:36:08.070 --> 00:36:10.440
there were 10 layers,
so every image

00:36:10.440 --> 00:36:13.230
is going to give you
back a set of 10 numbers.

00:36:13.230 --> 00:36:15.660
And those 10 numbers, as
I mentioned earlier on,

00:36:15.660 --> 00:36:18.450
nine of them should be very
close to 0, and one of them

00:36:18.450 --> 00:36:19.894
should be very close to 1.

00:36:19.894 --> 00:36:21.810
And then using the one
that's very close to 1,

00:36:21.810 --> 00:36:23.580
you could determine
your prediction

00:36:23.580 --> 00:36:25.920
to be whatever that
item of clothing is.

00:36:25.920 --> 00:36:29.580
So if I demo this
and show it in code--

00:36:29.580 --> 00:36:30.330
let's see.

00:36:30.330 --> 00:36:32.570
Go back here.

00:36:32.570 --> 00:36:35.180
It's really hard to
see it, so forgive me.

00:36:38.940 --> 00:36:40.812
Whoops.

00:36:40.812 --> 00:36:42.020
I'm going to select Fashion--

00:36:42.020 --> 00:36:43.850
oh.

00:36:43.850 --> 00:36:45.396
I really need a mouse.

00:36:45.396 --> 00:36:47.990
I'm going to select Fashion.

00:36:47.990 --> 00:36:48.490
OK.

00:36:48.490 --> 00:36:49.880
And can you see
the fashion code,

00:36:49.880 --> 00:36:51.687
or is it's still
showing the linear code?

00:36:51.687 --> 00:36:52.910
AUDIENCE: [INAUDIBLE]

00:36:52.910 --> 00:36:54.090
LAURENCE MORONEY: Is
that fashion right there?

00:36:54.090 --> 00:36:54.965
AUDIENCE: [INAUDIBLE]

00:36:54.965 --> 00:36:56.410
LAURENCE MORONEY: All right.

00:36:56.410 --> 00:36:57.170
OK.

00:36:57.170 --> 00:36:59.730
Did I just close it?

00:36:59.730 --> 00:37:00.230
I'm sorry.

00:37:00.230 --> 00:37:02.040
It's really hard to see.

00:37:02.040 --> 00:37:04.790
So let me go back.

00:37:04.790 --> 00:37:05.690
Is that one fashion?

00:37:05.690 --> 00:37:07.090
AUDIENCE: [INAUDIBLE]

00:37:07.090 --> 00:37:08.510
LAURENCE MORONEY: Up one?

00:37:08.510 --> 00:37:09.120
All right.

00:37:09.120 --> 00:37:09.480
That one?

00:37:09.480 --> 00:37:10.380
AUDIENCE: [INAUDIBLE]

00:37:10.380 --> 00:37:11.254
LAURENCE MORONEY: OK.

00:37:11.254 --> 00:37:14.280
So here's the code that I was
showing on the earlier slide.

00:37:14.280 --> 00:37:16.950
So this is exactly the same
code that was on my slides.

00:37:16.950 --> 00:37:18.070
I'm just going to go down.

00:37:18.070 --> 00:37:20.194
There's one thing I've done
here that I didn't show

00:37:20.194 --> 00:37:23.040
on the slides, and that
was the images themselves

00:37:23.040 --> 00:37:27.234
were grayscales, so every
pixel was between 0 and 256.

00:37:27.234 --> 00:37:28.650
For training my
neural network, it

00:37:28.650 --> 00:37:30.780
was just easier for me
to normalize that data.

00:37:30.780 --> 00:37:34.080
So instead of it
being from 0 to 255,

00:37:34.080 --> 00:37:37.582
it's a value from 0 to 1, which
is relative to that value.

00:37:37.582 --> 00:37:39.540
And that's what those
two lines of codes there.

00:37:39.540 --> 00:37:41.040
And that's one of the things
that makes Python really

00:37:41.040 --> 00:37:42.290
useful for this kind of thing.

00:37:42.290 --> 00:37:45.140
Because I can just say
that train_images set

00:37:45.140 --> 00:37:48.240
is a set of 60,000
28 by 28 images,

00:37:48.240 --> 00:37:50.100
and I can just say
divide that by 255,

00:37:50.100 --> 00:37:51.600
and that normalized that for me.

00:37:51.600 --> 00:37:53.474
So that's one of the
things that makes Python

00:37:53.474 --> 00:37:54.940
really handy in data science.

00:37:54.940 --> 00:37:57.040
So we can see it's
just the same code.

00:37:57.040 --> 00:38:00.534
So I'm going to do a bit of
live audience participation.

00:38:00.534 --> 00:38:02.200
Hopefully, I can get
it to work with us.

00:38:02.200 --> 00:38:05.650
So remember I said there
are 10,000 testing images?

00:38:05.650 --> 00:38:06.150
OK.

00:38:06.150 --> 00:38:13.102
So somebody give me a
number between 0 and 99,999.

00:38:13.102 --> 00:38:14.500
AUDIENCE: [INAUDIBLE]

00:38:14.500 --> 00:38:16.136
LAURENCE MORONEY: Don't be shy.

00:38:16.136 --> 00:38:17.010
AUDIENCE: [INAUDIBLE]

00:38:17.010 --> 00:38:18.520
LAURENCE MORONEY: What's that?

00:38:18.520 --> 00:38:19.560
Just 27?

00:38:19.560 --> 00:38:20.620
OK.

00:38:20.620 --> 00:38:25.510
So hopefully I can see
it so I can get it.

00:38:25.510 --> 00:38:28.060
That's not 27, is it?

00:38:28.060 --> 00:38:28.580
OK.

00:38:28.580 --> 00:38:30.671
27.

00:38:30.671 --> 00:38:32.780
And here-- 27.

00:38:32.780 --> 00:38:35.440
I tested it earlier
what value 4560.

00:38:35.440 --> 00:38:37.310
So what's going
to happen here is

00:38:37.310 --> 00:38:40.010
that I'm going to train the
neural network to identify

00:38:40.010 --> 00:38:41.270
those pieces of clothing.

00:38:41.270 --> 00:38:44.400
And so-- or to be able to
identify pieces of clothing.

00:38:44.400 --> 00:38:47.330
I have no idea what piece
of clothing number 27

00:38:47.330 --> 00:38:48.499
is in the test set.

00:38:48.499 --> 00:38:50.790
But what it's going to do
once it's done is by the end,

00:38:50.790 --> 00:38:53.800
you'll see it says print
the test labels for 27.

00:38:53.800 --> 00:38:55.750
So whatever item
of clothing 27 is,

00:38:55.750 --> 00:38:57.560
there's is a pre-assigned
label for that.

00:38:57.560 --> 00:38:58.520
It will print that out.

00:38:58.520 --> 00:38:59.978
And then the next
thing it'll do is

00:38:59.978 --> 00:39:02.347
it will print out what the
predicted label will be.

00:39:02.347 --> 00:39:04.680
And hopefully, the two of
them are going to be the same.

00:39:04.680 --> 00:39:07.280
There's about a 90% chance, if
I remember right from this one,

00:39:07.280 --> 00:39:08.300
that they will.

00:39:08.300 --> 00:39:10.760
So if I run it, it's going
to take a little longer

00:39:10.760 --> 00:39:12.266
than the previous one.

00:39:12.266 --> 00:39:14.390
So now we can see it starting
to train the network.

00:39:14.390 --> 00:39:15.546
AUDIENCE: [INAUDIBLE]

00:39:15.546 --> 00:39:17.670
LAURENCE MORONEY: And
because I'm doing in PyCharm,

00:39:17.670 --> 00:39:19.266
I can see in my debug window.

00:39:19.266 --> 00:39:20.390
So you can see the epochs--

00:39:20.390 --> 00:39:22.820
epoch 2, epoch 3, epoch 4.

00:39:22.820 --> 00:39:26.490
This accuracy number here is how
accurate it is against testing.

00:39:26.490 --> 00:39:29.295
So it's about 89% correct.

00:39:29.295 --> 00:39:31.670
And then you see it's actually
printed two numbers below,

00:39:31.670 --> 00:39:33.020
and they're both 0.

00:39:33.020 --> 00:39:35.060
So that means for item
of clothing number 27,

00:39:35.060 --> 00:39:36.500
that class was 0.

00:39:36.500 --> 00:39:38.960
And then the predicted for
that class was actually also 0,

00:39:38.960 --> 00:39:39.770
so it got it right.

00:39:39.770 --> 00:39:40.430
Yay.

00:39:40.430 --> 00:39:43.312
Anybody want to try one
more just to prove that?

00:39:43.312 --> 00:39:44.145
[INTERPOSING VOICES]

00:39:44.145 --> 00:39:45.110
LAURENCE MORONEY:
Let's see if we can--

00:39:45.110 --> 00:39:45.590
what's that?

00:39:45.590 --> 00:39:46.131
AUDIENCE: 42.

00:39:46.131 --> 00:39:48.300
LAURENCE MORONEY: 42, I love it.

00:39:48.300 --> 00:39:52.510
That's the ultimate answer,
but what is the question?

00:39:52.510 --> 00:39:53.010
OK.

00:39:53.010 --> 00:39:53.510
42.

00:39:56.110 --> 00:40:00.980
And I'm guessing 42 is probably
also item 0, but let's see.

00:40:00.980 --> 00:40:04.190
Hopefully, I haven't broken
any of the bracketing.

00:40:04.190 --> 00:40:05.734
Let me run it again.

00:40:05.734 --> 00:40:07.400
So because it's running
all of the code,

00:40:07.400 --> 00:40:09.350
it's just going to
train the network again.

00:40:09.350 --> 00:40:09.850
OK.

00:40:09.850 --> 00:40:12.590
There's epoch 2, epoch 3.

00:40:12.590 --> 00:40:13.160
Hello.

00:40:13.160 --> 00:40:15.172
There we go.

00:40:15.172 --> 00:40:16.880
So let's remember
earlier I said I'm just

00:40:16.880 --> 00:40:18.050
training it for five epochs.

00:40:18.050 --> 00:40:19.740
It just makes it a
little bit quicker.

00:40:19.740 --> 00:40:22.330
And I'm also seeing-- if
you look at the convergence,

00:40:22.330 --> 00:40:24.640
on epoch 1 it was 82% accurate.

00:40:24.640 --> 00:40:26.980
Oh, we got it wrong for 42.

00:40:26.980 --> 00:40:29.330
It predicted it would be a
6, but it's actually a 3.

00:40:29.330 --> 00:40:32.800
But the first epoch you
see, this accuracy figure--

00:40:32.800 --> 00:40:33.980
82.45%.

00:40:33.980 --> 00:40:36.710
That means it calculated
it was 82% accurate.

00:40:36.710 --> 00:40:39.549
The second epoch, 86%
accurate; the third, 87%;

00:40:39.549 --> 00:40:40.840
all the way down to the fifth--

00:40:40.840 --> 00:40:41.750
89%.

00:40:41.750 --> 00:40:43.760
I could probably train
it for 500 epochs,

00:40:43.760 --> 00:40:45.080
but we don't have the time.

00:40:45.080 --> 00:40:48.110
But then it might be more
likely to get number 42 correct.

00:40:48.110 --> 00:40:50.300
And thanks, Mr. Douglas
Adams, that you've actually

00:40:50.300 --> 00:40:53.160
given me one that doesn't work,
so I can go back and test it.

00:40:53.160 --> 00:40:54.200
OK.

00:40:54.200 --> 00:40:56.570
So that's Fashion
MNIST and how it works.

00:40:56.570 --> 00:41:00.130
And so hopefully, this was
a good introduction to you

00:41:00.130 --> 00:41:03.590
for really the concept from
a programmer's perspective

00:41:03.590 --> 00:41:05.510
of what machine
learning is all about.

00:41:05.510 --> 00:41:06.980
And I always like
to say at talks

00:41:06.980 --> 00:41:09.627
that if you only take one
slide away from this talk,

00:41:09.627 --> 00:41:11.210
if you've never done
machine learning,

00:41:11.210 --> 00:41:13.418
or you want to get into
programming machine learning,

00:41:13.418 --> 00:41:14.390
take this one here.

00:41:14.390 --> 00:41:16.640
Because this is really what
the core of the revolution

00:41:16.640 --> 00:41:18.431
is all about, and
hopefully the code that I

00:41:18.431 --> 00:41:20.372
showed you demonstrates that--

00:41:20.372 --> 00:41:21.830
that machine learning
is really all

00:41:21.830 --> 00:41:23.871
about taking answers and
data and feeding them in

00:41:23.871 --> 00:41:25.130
to get rules out.

00:41:25.130 --> 00:41:29.540
I didn't write a single line
of code there today that says,

00:41:29.540 --> 00:41:33.770
this is a t-shirt, or this is
a jacket, or this is a handbag.

00:41:33.770 --> 00:41:34.820
This has sleeves.

00:41:34.820 --> 00:41:36.620
If has sleeves, then is t-shirt.

00:41:36.620 --> 00:41:38.550
If has heels, then is shoe.

00:41:38.550 --> 00:41:40.550
I didn't have to write
any of that kind of code.

00:41:40.550 --> 00:41:42.140
I just trained
something on the data

00:41:42.140 --> 00:41:44.570
using the below thing, the
below part of the diagram--

00:41:44.570 --> 00:41:46.490
feeding in answers,
feeding in data,

00:41:46.490 --> 00:41:49.230
building a model that will
then infer the rules about it.

00:41:49.230 --> 00:41:51.020
So with that, I just want
to say thank you very much,

00:41:51.020 --> 00:41:53.020
and I hope you enjoy the
rest of the conference.

00:41:53.020 --> 00:41:54.100
[APPLAUSE]

