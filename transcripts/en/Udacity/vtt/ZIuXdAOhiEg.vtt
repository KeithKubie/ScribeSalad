WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:02.900
Hi everybody, welcome to our second mini course in

00:00:02.900 --> 00:00:06.770
machine learning. This sections going to be on unsupervised learning.

00:00:06.770 --> 00:00:09.890
&gt;&gt; That sounds fun. Hi Michael.

00:00:09.890 --> 00:00:11.650
&gt;&gt; Oh, hey Charles, I didn't even know you were here.

00:00:11.650 --> 00:00:13.270
&gt;&gt; That's because I'm not really there.

00:00:13.270 --> 00:00:15.970
&gt;&gt; Oh. [LAUGH]

00:00:15.970 --> 00:00:17.620
I'm glad to hear your voice regardless.

00:00:17.620 --> 00:00:19.500
&gt;&gt; Good, I'm always happy to hear your voice

00:00:19.500 --> 00:00:23.010
Michael, and I'm looking forward to hearing about un-dash-supervised learning.

00:00:23.010 --> 00:00:25.190
&gt;&gt; Well, unsupervised learning's going to be a

00:00:25.190 --> 00:00:30.640
series of lectures that we do. But today's lecture Is on PZTTMNIIAOOI.

00:00:30.640 --> 00:00:31.595
&gt;&gt; Ok...

00:00:31.595 --> 00:00:37.170
&gt;&gt; Which is randomized optimization. I took

00:00:37.170 --> 00:00:39.103
the letters of "optimization" and I randomized them.

00:00:39.103 --> 00:00:41.980
&gt;&gt; [LAUGH]

00:00:41.980 --> 00:00:43.760
You're such a nerd, I love it! Okay.

00:00:43.760 --> 00:00:46.910
&gt;&gt; Alright so so the plan is to talk about optimization, and

00:00:46.910 --> 00:00:50.310
in particular to focus on some algorithms that use randomization to be

00:00:50.310 --> 00:00:53.730
more effective. But let's let's talk a little bit about optimization

00:00:53.730 --> 00:00:55.950
for a moment, because that's something that has come up, but we

00:00:55.950 --> 00:00:58.890
haven't really spent any time on it. So, what I'm thinking about,

00:00:58.890 --> 00:01:01.470
when I, when I talk about optimization, I imagine that there's some

00:01:01.470 --> 00:01:04.560
input space X, which is, you know kind of like in the

00:01:04.560 --> 00:01:09.050
machine learning setting and we've also, we're also given access to an

00:01:09.050 --> 00:01:13.170
objective function or a fitness function, F, that maps any of the

00:01:13.170 --> 00:01:16.450
inputs in the the input space to a real number, a score.

00:01:16.450 --> 00:01:18.500
Sometimes called fitness, sometimes called the objective,

00:01:18.500 --> 00:01:21.950
sometimes called score. It could be any number

00:01:21.950 --> 00:01:25.820
of things. But in the setting that we're talking about right now, the goal is to

00:01:25.820 --> 00:01:31.420
find some value, x*, I didn't mean to put space, such that the fitness value for

00:01:31.420 --> 00:01:38.050
that x* is equal to or maybe as close as possible to the maximum possible value.

00:01:38.050 --> 00:01:42.080
&gt;&gt; So that's like [INAUDIBLE] like we were doing with [INAUDIBLE].

00:01:42.080 --> 00:01:45.190
Yeah, exactly, right. So of all the possible x's to choose, would we like

00:01:45.190 --> 00:01:48.010
to choose the one that, that causes the function value to be the highest.

00:01:48.010 --> 00:01:48.760
&gt;&gt; Okay.

00:01:49.940 --> 00:01:51.980
&gt;&gt; Yeah, I, I wrote it this way, though, because I thought

00:01:51.980 --> 00:01:56.470
it would probably be helpful if it's like, yeah, because we'd be

00:01:56.470 --> 00:02:01.290
pretty happy with the with an x* that isn't necessarily the best,

00:02:01.290 --> 00:02:04.360
but it's like near arc max, right? Something that's close to the best.

00:02:04.360 --> 00:02:05.340
&gt;&gt; Okay.

00:02:05.340 --> 00:02:07.080
&gt;&gt; So,

00:02:07.080 --> 00:02:08.650
so this is a really important sub-problem. It

00:02:08.650 --> 00:02:10.038
comes up very often, I was wondering if

00:02:10.038 --> 00:02:13.100
you could think of examples where that might

00:02:13.100 --> 00:02:15.278
be a good thing to do. Like in life.

00:02:15.278 --> 00:02:17.180
&gt;&gt; Well, Like in life.

00:02:17.180 --> 00:02:19.130
&gt;&gt; Computation, which is life.

00:02:19.130 --> 00:02:23.050
&gt;&gt; Computation is life. And life is computation. Well, believe it

00:02:23.050 --> 00:02:24.730
or not, I was actually just talking to someone the other

00:02:24.730 --> 00:02:28.800
day who's a chemical engineer and works at a chemical plant.

00:02:28.800 --> 00:02:32.590
And he says there's all these parameters they have to tune when

00:02:32.590 --> 00:02:34.840
they mix their little magical chemicals. And if they do

00:02:34.840 --> 00:02:37.660
it just right, they end up with something that, you

00:02:37.660 --> 00:02:40.080
know, is exactly the right temperature, comes out just right.

00:02:40.080 --> 00:02:42.610
If they're wrong at all, if some of their temperature is

00:02:42.610 --> 00:02:45.750
off a little bit or anything is wrong, then it

00:02:45.750 --> 00:02:47.510
ends up costing a lot of money and not coming

00:02:47.510 --> 00:02:49.310
out to be as good as they want it to

00:02:49.310 --> 00:02:54.880
be. So, you know, factories and chemicals and optimization and parameters.

00:02:54.880 --> 00:02:56.950
&gt;&gt; Factory,

00:02:56.950 --> 00:02:59.850
chemical. I'm not sure that's a general catergory of

00:02:59.850 --> 00:03:02.670
problem but I guess, I guess, maybe the one way

00:03:02.670 --> 00:03:05.640
to think about it is We'll call it process

00:03:07.510 --> 00:03:10.500
control. So if you've got a process that you're trying

00:03:10.500 --> 00:03:12.840
to put together. And you have some way of

00:03:12.840 --> 00:03:16.220
measuring how, how well it's going, like yield or cost,

00:03:16.220 --> 00:03:18.810
or something like that. Then you could imagine optimizing

00:03:18.810 --> 00:03:22.050
the, the process, itself, to try to improve your score.

00:03:22.050 --> 00:03:24.060
&gt;&gt; Okay. Yeah I think that's what it is.

00:03:25.410 --> 00:03:27.840
&gt;&gt; You know route finding is kind of like this as well. Right.

00:03:27.840 --> 00:03:30.650
So just. You know, find me the best way to get to Georgia.

00:03:30.650 --> 00:03:32.940
&gt;&gt; What about root finding?

00:03:32.940 --> 00:03:34.870
&gt;&gt; Oooh, I see what you did there.

00:03:34.870 --> 00:03:35.150
&gt;&gt; Mm mm.

00:03:35.150 --> 00:03:37.330
&gt;&gt; So you could think of root finding as being a

00:03:37.330 --> 00:03:40.065
kind of optimization as well. If you've got a function and

00:03:40.065 --> 00:03:45.240
you're trying figure out where it crosses the the origin. You

00:03:45.240 --> 00:03:47.460
might add you might set that up as a optimization problem.

00:03:47.460 --> 00:03:50.310
You might say well of all the different positions I could

00:03:50.310 --> 00:03:54.450
be in, I want to minimize the distance between the axis

00:03:54.450 --> 00:03:57.830
and the value at the point where I chose it. Find

00:03:57.830 --> 00:04:00.520
me the best one. Which, you know, is going to be right there.

00:04:00.520 --> 00:04:05.240
&gt;&gt; Actually, you know, when you put it like that. How about neural networks?

00:04:05.240 --> 00:04:09.040
&gt;&gt; Nice. So, that's, yeah let's get it back to the

00:04:09.040 --> 00:04:11.210
learning settings. So, what is, what do you mean by neural networks?

00:04:11.210 --> 00:04:12.180
&gt;&gt; Well, I mean,

00:04:12.180 --> 00:04:15.800
everything we've been talking about so far. X, you know is some kind of stand

00:04:15.800 --> 00:04:19.149
in for parameters of something. A process or

00:04:19.149 --> 00:04:22.330
I don't know whatever. So if the weights

00:04:22.330 --> 00:04:27.760
are just parameters of a neural network, then we want to find the x that

00:04:27.760 --> 00:04:31.350
I guess minimizes our error over some training

00:04:31.350 --> 00:04:33.490
set, or, or upcoming test sets or something.

00:04:33.490 --> 00:04:35.470
&gt;&gt; Yeah, so minimizing error is a kind

00:04:35.470 --> 00:04:37.470
of optimization. It's not a max in this case,

00:04:37.470 --> 00:04:41.170
but if we I guess, negate it you want to maximize

00:04:41.170 --> 00:04:45.680
the negative error, that it's maximized when the error is zero.

00:04:45.680 --> 00:04:45.980
&gt;&gt; Right.

00:04:45.980 --> 00:04:50.810
&gt;&gt; Cool, is any other learning related topics that

00:04:50.810 --> 00:04:52.770
you can think of that, that have optimization in them?

00:04:52.770 --> 00:04:54.410
&gt;&gt; Well I would guess anything with

00:04:54.410 --> 00:04:57.060
parameters, where some parameters are better than others

00:04:57.060 --> 00:04:58.230
and you have some way of measuring

00:04:58.230 --> 00:05:00.410
how good they are, is an optimization problem.

00:05:01.490 --> 00:05:02.690
&gt;&gt; So, decision

00:05:02.690 --> 00:05:03.220
trees?

00:05:03.220 --> 00:05:04.420
&gt;&gt; Sure.

00:05:04.420 --> 00:05:07.300
&gt;&gt; So, so what's the parameters there?

00:05:07.300 --> 00:05:11.170
&gt;&gt; There the order of the nodes, the actually

00:05:11.170 --> 00:05:14.130
nodes, like what's the first node? Yeah, the whole structure

00:05:14.130 --> 00:05:16.250
of the tree. So it's not a continuous value like

00:05:16.250 --> 00:05:18.200
a weight, but it is a structure where we could

00:05:18.200 --> 00:05:20.130
try to optimize over the structure. There's nothing in

00:05:20.130 --> 00:05:21.660
the way that I set this up here that makes

00:05:21.660 --> 00:05:25.150
it so that X has to be continuous or anything

00:05:25.150 --> 00:05:28.480
like that. We just need a way of mapping inputs

00:05:28.480 --> 00:05:29.300
to scores.

00:05:29.300 --> 00:05:33.730
&gt;&gt; Okay. So all of machine learning. Everything we

00:05:33.730 --> 00:05:35.970
did in the first third of the class is optimization.

00:05:35.970 --> 00:05:38.460
&gt;&gt; There is some optimization in each of the

00:05:38.460 --> 00:05:41.650
pieces of it. Yeah, exactly. because often what we say,

00:05:41.650 --> 00:05:45.300
given this training set, find me a classifier that does

00:05:45.300 --> 00:05:47.620
wel on the training set and that's an optimization problem.

00:05:47.620 --> 00:05:47.970
&gt;&gt; Hmm.

