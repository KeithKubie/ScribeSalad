WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.381
[MUSIC PLAYING]

00:00:10.143 --> 00:00:12.660
PETER BARRON: You were
just saying Peter also used

00:00:12.660 --> 00:00:14.530
to find time to
write about things

00:00:14.530 --> 00:00:17.654
like the crunchiness
of crisps with the FT--

00:00:17.654 --> 00:00:19.570
PETER BAZALGETTE: I got
my eye on this yogurt.

00:00:19.570 --> 00:00:21.600
PETER BARRON: Yes,
exactly, so be careful.

00:00:21.600 --> 00:00:23.200
But he certainly packs it in.

00:00:23.200 --> 00:00:25.340
And it's great to
see this new book.

00:00:25.340 --> 00:00:27.850
And I think the
number of people here

00:00:27.850 --> 00:00:31.480
can attest that, in your
introduction to the book,

00:00:31.480 --> 00:00:35.500
you say the empathy instinct
is an idea whose time has come.

00:00:35.500 --> 00:00:37.680
And boy has it come.

00:00:37.680 --> 00:00:42.130
And this was written before, I
think, Trump became president.

00:00:42.130 --> 00:00:45.890
But it does seem to be
the spirit of the age.

00:00:45.890 --> 00:00:48.140
So tell us how the book came
about in the first place.

00:00:48.140 --> 00:00:50.460
How did you get into
the idea of empathy?

00:00:50.460 --> 00:00:54.040
PETER BAZALGETTE:
So in a sentence,

00:00:54.040 --> 00:00:57.040
empathy is usually
defined by people

00:00:57.040 --> 00:01:00.010
as seeing something from
somebody else's point of view,

00:01:00.010 --> 00:01:03.190
being able to put yourself
in somebody else's shoes.

00:01:03.190 --> 00:01:06.920
And I was really
interested for two reasons.

00:01:06.920 --> 00:01:09.820
First is, as you kindly
said in the introduction,

00:01:09.820 --> 00:01:12.400
I've spent the last
four years fighting

00:01:12.400 --> 00:01:15.880
the corner of public
funding of arts and culture.

00:01:15.880 --> 00:01:19.437
And I got really
interested in whether we

00:01:19.437 --> 00:01:21.520
made a good enough case
for public funding of arts

00:01:21.520 --> 00:01:21.970
and culture.

00:01:21.970 --> 00:01:24.053
And that said, well, what
are the benefits of arts

00:01:24.053 --> 00:01:24.610
and culture?

00:01:24.610 --> 00:01:29.590
And along with education, social
benefits, economic benefits,

00:01:29.590 --> 00:01:32.590
there are what we call
intrinsic benefits, things

00:01:32.590 --> 00:01:37.480
like identity and entertainment
and those sort of qualities.

00:01:37.480 --> 00:01:40.660
And one of the phrases you came
up with is empathetic citizens.

00:01:40.660 --> 00:01:43.000
Because if you think about
arts and popular culture--

00:01:43.000 --> 00:01:44.440
think about movies, books.

00:01:44.440 --> 00:01:47.176
And I go into some detail
about this in the book.

00:01:47.176 --> 00:01:50.870
They are actually about
telling human stories that

00:01:50.870 --> 00:01:53.220
enable you to put yourself
in somebody else's shoes,

00:01:53.220 --> 00:01:55.330
see things from other
people's points of view.

00:01:55.330 --> 00:02:00.120
So that was point of origin
number one, really, sort

00:02:00.120 --> 00:02:02.850
of trying to define what the
benefits of arts and culture

00:02:02.850 --> 00:02:04.010
are.

00:02:04.010 --> 00:02:05.040
And I mean that broadly.

00:02:05.040 --> 00:02:06.915
I'm not talking just
theater and visual arts.

00:02:06.915 --> 00:02:11.950
I'm talking about broadcasting,
TV, film, in radio, and so on.

00:02:11.950 --> 00:02:16.180
The second thing was I was
asked by the government

00:02:16.180 --> 00:02:20.734
to chair something called a
Holocaust Memorial Foundation.

00:02:20.734 --> 00:02:22.400
I don't know if you
know, we do actually

00:02:22.400 --> 00:02:25.350
have a Holocaust
Memorial in this country.

00:02:25.350 --> 00:02:25.970
You knew that?

00:02:25.970 --> 00:02:27.920
AUDIENCE: Yeah,
it's in [INAUDIBLE].

00:02:27.920 --> 00:02:29.112
PETER BAZALGETTE: Ah, no.

00:02:29.112 --> 00:02:30.320
AUDIENCE: There is one there.

00:02:30.320 --> 00:02:32.153
PETER BAZALGETTE: You're
very knowledgeable.

00:02:32.153 --> 00:02:34.590
There's something called the
Holocaust Center in Newark,

00:02:34.590 --> 00:02:37.090
in Nottinghamshire, which is
actually set up by something

00:02:37.090 --> 00:02:38.170
called the Smith family.

00:02:38.170 --> 00:02:42.510
But no, we have a
small rock in Hyde Park

00:02:42.510 --> 00:02:45.240
that not even most Jewish
people know is there.

00:02:45.240 --> 00:02:47.220
And the prime minister,
David Cameron,

00:02:47.220 --> 00:02:50.432
met a whole lot of
survivors of the camps

00:02:50.432 --> 00:02:52.140
who were in their late
80s, early 90s who

00:02:52.140 --> 00:02:57.090
go around the schools and
talk about the Holocaust.

00:02:57.090 --> 00:03:00.360
And he realized they won't
be with us in 10 years time.

00:03:00.360 --> 00:03:02.550
And he asked the
question, how are we going

00:03:02.550 --> 00:03:04.410
to commemorate the Holocaust?

00:03:04.410 --> 00:03:07.230
So I got to be
asked to chair this.

00:03:07.230 --> 00:03:08.769
And we've got a
design competition

00:03:08.769 --> 00:03:10.560
and we're building a
new Holocaust Memorial

00:03:10.560 --> 00:03:14.020
in Victoria Tower Gardens,
so on and so forth.

00:03:14.020 --> 00:03:16.080
But I got to meet
the survivors myself.

00:03:16.080 --> 00:03:17.970
And you get to
think about what is

00:03:17.970 --> 00:03:22.510
a society like that appears to
be apparently without empathy.

00:03:22.510 --> 00:03:24.600
Now there's some
nuances to that.

00:03:24.600 --> 00:03:26.730
But I'll just say
apparently without empathy.

00:03:26.730 --> 00:03:29.970
And I got really
interested talking to them,

00:03:29.970 --> 00:03:36.200
because in the first
chapter of the book,

00:03:36.200 --> 00:03:40.370
I write about the
Holocaust in Germany,

00:03:40.370 --> 00:03:43.922
the genocide in Armenia,
and the genocide in Rwanda.

00:03:43.922 --> 00:03:45.630
And they have some
common themes to them.

00:03:45.630 --> 00:03:49.730
But one of them is that society
seems to switch off its empathy

00:03:49.730 --> 00:03:52.460
and become aggressive to people
who are different from you.

00:03:52.460 --> 00:03:54.410
So those are the two origins.

00:03:54.410 --> 00:03:56.118
PETER BARRON: And
you're got a quote here

00:03:56.118 --> 00:03:59.750
from Obama on the back
of the book, which

00:03:59.750 --> 00:04:02.630
is, "I think we're going
to have to talk more

00:04:02.630 --> 00:04:04.579
about the empathy
deficit, the ability

00:04:04.579 --> 00:04:06.370
to put ourselves in
somebody else's shoes."

00:04:06.370 --> 00:04:09.080
So you're suggesting that--

00:04:09.080 --> 00:04:11.420
Obama's suggesting too--
but there's a suggestion

00:04:11.420 --> 00:04:16.190
that we're in a similar
era now as the era that

00:04:16.190 --> 00:04:17.140
caused the Holocaust.

00:04:17.140 --> 00:04:18.889
And a lot of people
are making comparisons

00:04:18.889 --> 00:04:20.450
between the '30s and now.

00:04:20.450 --> 00:04:21.980
Do you see a parallel?

00:04:21.980 --> 00:04:25.520
PETER BAZALGETTE: I think that's
putting it a bit strongly.

00:04:25.520 --> 00:04:29.190
We do have really healthy
democratic institutions.

00:04:29.190 --> 00:04:31.940
But one of the things we've seen
in the last 12 to 18 months--

00:04:31.940 --> 00:04:34.790
and I'd really be interested
to hear what your view is,

00:04:34.790 --> 00:04:39.170
whatever your point of view,
is we've seen the language

00:04:39.170 --> 00:04:40.290
of politics become--

00:04:40.290 --> 00:04:42.380
well, you've been producing
political programs

00:04:42.380 --> 00:04:44.772
on telly yourself, Peter,
for quite a few years

00:04:44.772 --> 00:04:45.980
before you worked for Google.

00:04:45.980 --> 00:04:47.244
So you'll have a view too.

00:04:47.244 --> 00:04:48.660
But the language
of politics seems

00:04:48.660 --> 00:04:51.260
to have become
coarser and angrier.

00:04:51.260 --> 00:04:53.870
Personally I think it's to
do with post-credit crunch

00:04:53.870 --> 00:04:57.470
and to do with the fact
that people feel poorer

00:04:57.470 --> 00:04:58.910
and feel neglected.

00:04:58.910 --> 00:05:00.530
Certainly we had
this experience,

00:05:00.530 --> 00:05:04.580
did we not last year, whether
you voted to remain in the EU

00:05:04.580 --> 00:05:06.500
or whether you voted
to leave the EU--

00:05:06.500 --> 00:05:08.275
and I'd have a
pretty strong guess

00:05:08.275 --> 00:05:10.150
of what the percentage
in this room would be,

00:05:10.150 --> 00:05:11.750
but let's not go there.

00:05:11.750 --> 00:05:14.240
We did feel a rather
divided country.

00:05:14.240 --> 00:05:16.790
But no, I don't think
we're approaching that.

00:05:16.790 --> 00:05:22.620
But I think we have to watch it.

00:05:22.620 --> 00:05:25.860
And another thing I'm
really interested in

00:05:25.860 --> 00:05:28.950
is what people in this room
think about the digital era.

00:05:28.950 --> 00:05:32.430
I wrote a chapter in the book
called "The Digital Dystopia."

00:05:32.430 --> 00:05:36.090
It's a given what the benefits
of the digital era are.

00:05:36.090 --> 00:05:39.440
And you in this
room embody them.

00:05:39.440 --> 00:05:42.890
But I think you're also aware
of what the downsides are,

00:05:42.890 --> 00:05:47.270
of how you've had to deal
recently with Holocaust denial

00:05:47.270 --> 00:05:49.710
and how that is
searched for, and what

00:05:49.710 --> 00:05:51.620
your responsibilities
as an organization

00:05:51.620 --> 00:05:53.274
are in that context
by the way, which

00:05:53.274 --> 00:05:54.440
has been really interesting.

00:05:54.440 --> 00:05:56.930
We were talking about
it just beforehand.

00:05:56.930 --> 00:06:02.930
And so I am quite
concerned at the way

00:06:02.930 --> 00:06:08.300
in which textual communication
is nonempathetic.

00:06:08.300 --> 00:06:13.950
We spend a lot of our time,
when we talk to other people,

00:06:13.950 --> 00:06:16.440
you have about eight
or 10 different regions

00:06:16.440 --> 00:06:20.890
of your brain that help you
be empathetic to other people.

00:06:20.890 --> 00:06:24.036
So we know this
because the MRI scanner

00:06:24.036 --> 00:06:26.160
has been the big breakthrough
in the last 25 years.

00:06:26.160 --> 00:06:29.520
The MRI scanner is allowing
us to map the human brain.

00:06:29.520 --> 00:06:35.730
And it's not just for broken
legs from skiing holidays.

00:06:35.730 --> 00:06:38.552
And we now know what the
different parts of the brain

00:06:38.552 --> 00:06:39.760
are that do different things.

00:06:39.760 --> 00:06:41.920
And so when you're having a
conversation with somebody,

00:06:41.920 --> 00:06:42.920
you can read their face.

00:06:42.920 --> 00:06:45.210
You can see what the
results of your words are.

00:06:45.210 --> 00:06:48.480
You can see if
they're happy or sad.

00:06:48.480 --> 00:06:51.210
That is an empathetic
communication,

00:06:51.210 --> 00:06:54.210
unless of course you
happen to be psychopathic,

00:06:54.210 --> 00:06:56.424
which none of you look
particularly psychopathic,

00:06:56.424 --> 00:06:57.840
or you happen to
be, for instance,

00:06:57.840 --> 00:06:59.270
on the autistic spectrum.

00:06:59.270 --> 00:07:00.690
And there, in those
circumstances,

00:07:00.690 --> 00:07:02.880
some of your empathy
circuit won't work

00:07:02.880 --> 00:07:05.010
and you'll find it more
difficult to connect

00:07:05.010 --> 00:07:06.290
with some of those things.

00:07:06.290 --> 00:07:12.300
But textual communication
online is profoundly unemphatic.

00:07:12.300 --> 00:07:15.360
And you don't see the
result of your words.

00:07:15.360 --> 00:07:18.180
And cyberbullying and revenge
porn and all the other things

00:07:18.180 --> 00:07:23.340
we know about, plus the fact
I'm rather intrigued at the way

00:07:23.340 --> 00:07:25.980
that people who have what we
as a consensus would say has

00:07:25.980 --> 00:07:28.530
a warped view-- let's
take Holocaust denial

00:07:28.530 --> 00:07:29.816
as an example--

00:07:29.816 --> 00:07:31.440
are now able to
connect with each other

00:07:31.440 --> 00:07:33.840
in a way they never could
beforehand and reinforce

00:07:33.840 --> 00:07:35.790
each other's prejudices.

00:07:35.790 --> 00:07:37.710
And we get to a
position where we

00:07:37.710 --> 00:07:43.010
tend to only talk to other
people who agree with us.

00:07:43.010 --> 00:07:46.947
And part of empathy, putting
yourself in other people's

00:07:46.947 --> 00:07:48.780
shoes, seeing other
people's point of views,

00:07:48.780 --> 00:07:51.300
listening to different
points of view.

00:07:51.300 --> 00:07:53.910
PETER BARRON: So can
we define empathy?

00:07:53.910 --> 00:07:56.559
I mean, we have a lot
of engineers here,

00:07:56.559 --> 00:07:57.600
a lot of scientists here.

00:07:57.600 --> 00:07:59.830
So you say you can
measure empathy.

00:07:59.830 --> 00:08:01.980
So first of all, can you
define what empathy is?

00:08:01.980 --> 00:08:04.240
And how do you measure empathy?

00:08:04.240 --> 00:08:08.540
PETER BAZALGETTE: So
you define empathy

00:08:08.540 --> 00:08:11.960
as the ability to see
something from somebody else's

00:08:11.960 --> 00:08:15.380
point of view, to put yourself
in somebody else's shoes.

00:08:15.380 --> 00:08:19.040
There are two essential
elements to empathy.

00:08:19.040 --> 00:08:23.680
One is cognitive empathy and
the other is emotional empathy.

00:08:23.680 --> 00:08:27.130
Cognitive empathy
is the thing that

00:08:27.130 --> 00:08:30.694
allows you to understand what
makes somebody else tick.

00:08:30.694 --> 00:08:32.860
It doesn't necessarily make
you sympathetic to them.

00:08:32.860 --> 00:08:34.299
It makes you understand.

00:08:34.299 --> 00:08:36.730
I always like to give
the example of Iago

00:08:36.730 --> 00:08:41.390
in "Othello" who understands
Othello perfectly, can play him

00:08:41.390 --> 00:08:44.840
like a fiddle, but has
no sympathy for him.

00:08:44.840 --> 00:08:47.260
In fact he uses
it sociopathically

00:08:47.260 --> 00:08:49.250
and you might say
psychopathically.

00:08:49.250 --> 00:08:50.830
So cognitive
empathy is that bit.

00:08:50.830 --> 00:08:53.900
The emotional empathy
is your ability

00:08:53.900 --> 00:08:57.200
to essentially experience the
emotions the other person is

00:08:57.200 --> 00:08:58.150
having.

00:08:58.150 --> 00:09:00.020
And that is a
position of sympathy.

00:09:00.020 --> 00:09:03.860
So those are those two
elements of empathy

00:09:03.860 --> 00:09:05.750
when you define it further.

00:09:05.750 --> 00:09:07.830
There's another
point about empathy,

00:09:07.830 --> 00:09:10.400
which is why I said there
was a nuance to my saying,

00:09:10.400 --> 00:09:13.300
you know, it is the answer
to things like racism.

00:09:13.300 --> 00:09:15.890
There are primatologists
who believe

00:09:15.890 --> 00:09:18.650
that empathy is both
the answer to racism

00:09:18.650 --> 00:09:21.080
but the cause of racism.

00:09:21.080 --> 00:09:23.510
So this goes as follows.

00:09:23.510 --> 00:09:25.880
There are really
good primatologists.

00:09:25.880 --> 00:09:28.985
And the best one is a
guy called Franz de Waal

00:09:28.985 --> 00:09:30.860
who wrote a book called
"The Age of Empathy."

00:09:30.860 --> 00:09:33.680
And he's worked with
primates all his life.

00:09:33.680 --> 00:09:35.960
And he's discovered that
primates have about 80%

00:09:35.960 --> 00:09:38.640
of human empathy equipment.

00:09:38.640 --> 00:09:41.060
And what he says is that
empathy is something

00:09:41.060 --> 00:09:44.630
that can make you loyal to
your family, your group,

00:09:44.630 --> 00:09:48.215
your own tribe, team
of football supporters,

00:09:48.215 --> 00:09:50.900
group of football supporters,
and make you hostile to people

00:09:50.900 --> 00:09:52.070
outside your group.

00:09:52.070 --> 00:09:56.290
And that's quite a sort
of primeval influence

00:09:56.290 --> 00:09:59.030
in the way we've grown up.

00:09:59.030 --> 00:10:07.780
So he believes that
empathy is both

00:10:07.780 --> 00:10:12.040
possibly the root of
racism, or one of them,

00:10:12.040 --> 00:10:13.390
but also that it's the answer.

00:10:13.390 --> 00:10:16.750
And that's why I love art's
and culture's role in this,

00:10:16.750 --> 00:10:18.250
because that sort
of empathy that

00:10:18.250 --> 00:10:20.440
enables you to see the
point of view of somebody

00:10:20.440 --> 00:10:22.787
outside your group.

00:10:22.787 --> 00:10:24.370
I always like-- can
I give an example?

00:10:24.370 --> 00:10:26.587
PETER BARRON: Yes, please do.

00:10:26.587 --> 00:10:28.420
PETER BAZALGETTE: I
like to give the example

00:10:28.420 --> 00:10:30.100
of a play called "Red Velvet."

00:10:30.100 --> 00:10:32.350
Did anybody ever see the
play "Red Velvet," which

00:10:32.350 --> 00:10:35.210
was on at the Tricycle Theatre
in North London about three

00:10:35.210 --> 00:10:37.390
or four years ago,
won a lot of awards?

00:10:37.390 --> 00:10:39.010
Adrian Lester stared in it--

00:10:39.010 --> 00:10:41.000
amazing story.

00:10:41.000 --> 00:10:42.520
If I can have two
minutes on this?

00:10:42.520 --> 00:10:44.154
PETER BARRON: Sure, sure.

00:10:44.154 --> 00:10:46.570
PETER BAZALGETTE: There was a
black actor, American actor,

00:10:46.570 --> 00:10:51.320
called Ira Aldridge who
came to the UK in the 1820s,

00:10:51.320 --> 00:10:53.380
unbelievably.

00:10:53.380 --> 00:10:56.072
And he was a very good actor.

00:10:56.072 --> 00:10:58.030
And he was a jobbing
actor going around playing

00:10:58.030 --> 00:10:59.950
in different places in different
capitals around Europe.

00:10:59.950 --> 00:11:01.450
And he happened to
be in London when

00:11:01.450 --> 00:11:04.330
Edmund Kean, who was the
big Shakespearean actor

00:11:04.330 --> 00:11:07.870
of the time, was playing
Othello at Covent Garden.

00:11:07.870 --> 00:11:09.850
And Edmund Kean was ill.

00:11:09.850 --> 00:11:20.340
And so they put Ira Aldridge in
to play Othello in about 1825,

00:11:20.340 --> 00:11:21.850
something like that.

00:11:21.850 --> 00:11:24.580
There were massive
complaints from the cast

00:11:24.580 --> 00:11:27.730
and from the public
at a black man being

00:11:27.730 --> 00:11:31.300
put in the part of a black man.

00:11:31.300 --> 00:11:35.740
And the play ends with him
putting on white makeup,

00:11:35.740 --> 00:11:39.400
because the only parts he can
get are playing a white man.

00:11:39.400 --> 00:11:44.240
And it's an incredibly
powerful play.

00:11:44.240 --> 00:11:50.570
And if you are a white person in
a predominately white society,

00:11:50.570 --> 00:11:52.520
it just begins to
help you put yourself

00:11:52.520 --> 00:11:54.735
in the shoes of how
other people might feel,

00:11:54.735 --> 00:11:56.980
who have a different skin color.

00:11:56.980 --> 00:11:58.870
And so that's what I
mean about the power.

00:11:58.870 --> 00:12:03.700
Now you could all give me
examples of books you've read,

00:12:03.700 --> 00:12:06.375
or films you've seen that
have a similar effect.

00:12:06.375 --> 00:12:08.500
Sometimes you wouldn't even
analyze it's happening.

00:12:08.500 --> 00:12:11.320
But it's the extraordinary
power of arts and culture

00:12:11.320 --> 00:12:13.040
to help you--

00:12:13.040 --> 00:12:15.740
it's pro-empathetic,
and it's the opposite

00:12:15.740 --> 00:12:17.870
of that thing about empathy
being the root of it.

00:12:17.870 --> 00:12:18.620
PETER BARRON: Yes.

00:12:18.620 --> 00:12:20.536
Let's talk a little bit
more about technology,

00:12:20.536 --> 00:12:25.010
because you mentioned that it
can be an unempathetic place.

00:12:25.010 --> 00:12:28.270
Emails can look very stark and
we don't pick up the signals

00:12:28.270 --> 00:12:28.770
and so on.

00:12:28.770 --> 00:12:30.186
But you actually
talk a little bit

00:12:30.186 --> 00:12:33.710
in the book about the
effect that some aspects

00:12:33.710 --> 00:12:36.950
of the internet and technology
have on people's brains.

00:12:36.950 --> 00:12:40.790
You talked about kids in South
Korea who are gaming too much--

00:12:40.790 --> 00:12:42.050
PETER BAZALGETTE: Oh, they're
talking about digital dementia.

00:12:42.050 --> 00:12:43.280
PETER BARRON: Digital dementia--

00:12:43.280 --> 00:12:44.400
PETER BAZALGETTE: I'm not
sure whether [INAUDIBLE].

00:12:44.400 --> 00:12:46.730
PETER BARRON: --or the
effect that pornography

00:12:46.730 --> 00:12:48.460
has on empathy, for example.

00:12:48.460 --> 00:12:49.945
PETER BAZALGETTE:
Well the striatum

00:12:49.945 --> 00:12:52.070
is the part of the brain
which is pleasure seeking.

00:12:52.070 --> 00:12:53.510
I'm sure some of you know that.

00:12:53.510 --> 00:12:56.030
And people who have a
prolonged pornography habit

00:12:56.030 --> 00:12:58.190
have a very shrunken
striatum because it

00:12:58.190 --> 00:13:00.500
takes more and more
extreme pornography

00:13:00.500 --> 00:13:03.680
to please that part of your
brain which gives you pleasure.

00:13:06.290 --> 00:13:13.060
Yes, so what are we,
15 years, should we

00:13:13.060 --> 00:13:15.049
say, into a digital millennium?

00:13:15.049 --> 00:13:16.090
PETER BARRON: Suppose so.

00:13:16.090 --> 00:13:18.310
PETER BAZALGETTE: And
you guys are absolutely

00:13:18.310 --> 00:13:20.420
at the forefront of it.

00:13:20.420 --> 00:13:22.400
And on balance it is
a beneficial thing.

00:13:22.400 --> 00:13:24.620
So I would never say
it's not beneficial.

00:13:24.620 --> 00:13:25.995
But nothing is
purely beneficial.

00:13:25.995 --> 00:13:28.120
PETER BARRON: You make that
very clear in the book.

00:13:28.120 --> 00:13:30.170
PETER BAZALGETTE: Nothing's
purely beneficial.

00:13:30.170 --> 00:13:35.450
And I don't think we've
begun to understand

00:13:35.450 --> 00:13:37.880
what this always
on world of ours

00:13:37.880 --> 00:13:40.550
is doing to our human relations.

00:13:40.550 --> 00:13:42.688
There's some wonderful
research in America.

00:13:45.620 --> 00:13:48.550
The power of empathy is
face to face communication.

00:13:48.550 --> 00:13:51.320
And within a family,
the relationship

00:13:51.320 --> 00:13:53.600
between parents and
children is all about face

00:13:53.600 --> 00:13:54.662
to face communication.

00:13:54.662 --> 00:13:55.370
We all know that.

00:13:55.370 --> 00:13:56.940
You have families.

00:13:56.940 --> 00:13:58.400
You are either in
families, you've

00:13:58.400 --> 00:14:00.080
got children, or whatever.

00:14:00.080 --> 00:14:03.020
And you also know that in a
more independent world where

00:14:03.020 --> 00:14:05.480
children have their own
devices to entertain them,

00:14:05.480 --> 00:14:08.063
they can make their own eating
decisions because of snack food

00:14:08.063 --> 00:14:10.709
and so on, that actually
the time-- if you make time,

00:14:10.709 --> 00:14:13.250
the time you get together as a
family is around a meal table,

00:14:13.250 --> 00:14:14.840
if you still do that.

00:14:14.840 --> 00:14:18.560
And it's so interesting
that people like Steve Jobs

00:14:18.560 --> 00:14:22.040
made a point of doing
that and banning iPads

00:14:22.040 --> 00:14:24.560
from his own children
for hours of the day

00:14:24.560 --> 00:14:27.200
so he could get his
family together.

00:14:27.200 --> 00:14:29.722
And there's research in
America now that says--

00:14:29.722 --> 00:14:31.180
they've looked at
families together

00:14:31.180 --> 00:14:32.884
in McDonald's and other places.

00:14:32.884 --> 00:14:34.550
And the parents are
with their children,

00:14:34.550 --> 00:14:36.650
but the parents are on
their mobile phones.

00:14:36.650 --> 00:14:39.030
And they're not connecting
with their children.

00:14:39.030 --> 00:14:40.675
And this is not me wailing.

00:14:40.675 --> 00:14:42.050
And you know,
somebody accused me

00:14:42.050 --> 00:14:44.030
in a review of the book
of being Mary Whitehouse.

00:14:44.030 --> 00:14:46.070
And some of you are too young
to know who Mary Whitehouse was.

00:14:46.070 --> 00:14:47.195
But some of you may not be.

00:14:47.195 --> 00:14:49.070
But she was a sort of
campaigner against--

00:14:49.070 --> 00:14:51.290
she was an extremely
methodistical campaign

00:14:51.290 --> 00:14:54.110
against pornography
in the 1980s.

00:14:54.110 --> 00:14:56.690
I'm not a Mary Whitehouse.

00:14:56.690 --> 00:14:59.390
I generally think,
though, that we have not

00:14:59.390 --> 00:15:01.640
begun to analyze what
the digital world is

00:15:01.640 --> 00:15:02.930
doing to our human relations.

00:15:02.930 --> 00:15:04.410
And we need to be
very aware of it.

00:15:04.410 --> 00:15:07.230
And that was one of my
motives in writing the book.

00:15:07.230 --> 00:15:10.420
So if we have an
empathy deficit,

00:15:10.420 --> 00:15:14.930
what are your prescriptions
for doing something about it?

00:15:14.930 --> 00:15:20.810
So I've divided the book
into different chapters

00:15:20.810 --> 00:15:22.070
about different subjects.

00:15:22.070 --> 00:15:24.200
One is indeed about early years.

00:15:24.200 --> 00:15:28.035
And I call it the nurture
and nature of empathy.

00:15:28.035 --> 00:15:30.160
The first 2 and 1/2 years
when the brain is forming

00:15:30.160 --> 00:15:32.510
are the critical years
in children's lives.

00:15:32.510 --> 00:15:36.320
You need to look at the
Romanian orphans locked away

00:15:36.320 --> 00:15:37.970
without any human contact.

00:15:37.970 --> 00:15:41.210
They didn't have
any ability even

00:15:41.210 --> 00:15:43.664
to communicate let
alone anything else.

00:15:43.664 --> 00:15:45.080
And that's a
critical [INAUDIBLE].

00:15:45.080 --> 00:15:48.170
So I think that, for
instance, baby manuals

00:15:48.170 --> 00:15:50.030
don't give it enough--

00:15:50.030 --> 00:15:52.250
I mean, it is
mentioned these days.

00:15:52.250 --> 00:15:55.020
I went back to the baby
manuals on the shelf

00:15:55.020 --> 00:15:57.770
when my kids were young in
the late '80s, early '90s,

00:15:57.770 --> 00:16:01.280
and I could find enema but
I couldn't find empathy

00:16:01.280 --> 00:16:02.704
in the index.

00:16:02.704 --> 00:16:03.870
So I think we've come along.

00:16:03.870 --> 00:16:07.950
But I don't think it's
properly thought about

00:16:07.950 --> 00:16:11.270
in how we connect
with small children

00:16:11.270 --> 00:16:13.620
and how they grow up in
that critical period.

00:16:13.620 --> 00:16:16.820
Then I've-- and we've covered it
already, the digital dystopia.

00:16:16.820 --> 00:16:19.280
We need to think really
hard about how we maintain

00:16:19.280 --> 00:16:21.400
human relations in
the digital era,

00:16:21.400 --> 00:16:26.760
because parts of the digital
era militate against it.

00:16:26.760 --> 00:16:31.920
But I've also written
about empathy and health.

00:16:31.920 --> 00:16:37.650
So the evidence is that people
in the health care system

00:16:37.650 --> 00:16:41.666
and in social care are
better, feel better,

00:16:41.666 --> 00:16:43.540
and get better more
quickly if they feel they

00:16:43.540 --> 00:16:44.831
are being treated with empathy.

00:16:47.170 --> 00:16:51.460
But in a health system
which has finite resources

00:16:51.460 --> 00:16:54.110
and infinite demand--

00:16:54.110 --> 00:16:56.900
and you've seen all the
statistics in the last few days

00:16:56.900 --> 00:17:00.349
about how many more people
they're trying to treat in A&amp;E,

00:17:00.349 --> 00:17:02.390
and you know about the
Mid Staff scandal of four,

00:17:02.390 --> 00:17:04.099
five years ago.

00:17:04.099 --> 00:17:07.400
So we need to think really
hard about entraining doctors,

00:17:07.400 --> 00:17:08.940
nurses--

00:17:08.940 --> 00:17:10.470
and care workers
in particular who

00:17:10.470 --> 00:17:12.178
are particularly badly
paid and therefore

00:17:12.178 --> 00:17:13.599
not very well trained--

00:17:13.599 --> 00:17:16.829
is how we prepare them to
look after people properly.

00:17:16.829 --> 00:17:18.710
Now this has a lot
of nuances to it.

00:17:18.710 --> 00:17:21.500
If a doctor is so empathetic--
talking of cognitive

00:17:21.500 --> 00:17:22.849
and emotional empathy--

00:17:22.849 --> 00:17:26.420
if a doctor is so
empathetic he or she just

00:17:26.420 --> 00:17:27.920
bursts into tears
every time they're

00:17:27.920 --> 00:17:30.930
faced with a person who's
got cancer, what use is that?

00:17:30.930 --> 00:17:32.180
They're useless.

00:17:32.180 --> 00:17:34.655
So this is a very nuanced
and careful thing.

00:17:34.655 --> 00:17:36.530
Anyway, I won't go on
about the health thing,

00:17:36.530 --> 00:17:38.365
but we could discuss it further.

00:17:38.365 --> 00:17:39.740
Then there's
another chapter I've

00:17:39.740 --> 00:17:42.050
written about the
criminal justice system.

00:17:42.050 --> 00:17:48.050
So we have a retributive system
based on Victorian ideals

00:17:48.050 --> 00:17:50.570
where we think people
deserve to be punished.

00:17:50.570 --> 00:17:52.460
And our recidivism
rate is hovering

00:17:52.460 --> 00:17:55.820
around 50% that is reoffending.

00:17:55.820 --> 00:17:57.900
In Norway it's 20%.

00:17:57.900 --> 00:18:01.100
And we under-resource
our prisons.

00:18:01.100 --> 00:18:03.800
And we don't connect with
people on a one to one basis

00:18:03.800 --> 00:18:08.130
to help them get
better, to rehabilitate.

00:18:08.130 --> 00:18:10.270
Many of them are
damaged individuals.

00:18:10.270 --> 00:18:12.300
One in four people in
prisons were in care.

00:18:12.300 --> 00:18:14.567
And that goes back to
what I was saying earlier.

00:18:14.567 --> 00:18:16.400
Imagine what it's like
for children in care.

00:18:16.400 --> 00:18:18.020
There's lots of
documentation about this,

00:18:18.020 --> 00:18:20.353
by the way, about children
who didn't get the one to one

00:18:20.353 --> 00:18:22.250
attention that we like
to give our children

00:18:22.250 --> 00:18:23.990
or that we enjoyed
from our parents.

00:18:23.990 --> 00:18:27.260
By the way, Simon Baron-Cohen,
the professor of psychology

00:18:27.260 --> 00:18:29.739
at Cambridge who's done
brilliant work on empathy

00:18:29.739 --> 00:18:31.280
and the different
parts of the brain,

00:18:31.280 --> 00:18:34.670
because he's our leading expert
on autism and the autistic

00:18:34.670 --> 00:18:36.250
spectrum and Asperger's.

00:18:36.250 --> 00:18:38.960
And he calls it the
internal pot of gold

00:18:38.960 --> 00:18:40.700
that your parents give you.

00:18:40.700 --> 00:18:44.300
And I tell the story in the
book about James Fallon,

00:18:44.300 --> 00:18:47.900
who is a neuroscientist
in America, who was doing

00:18:47.900 --> 00:18:51.480
some research on psychopaths.

00:18:51.480 --> 00:18:55.987
And as a control, he did brain
scans of some psychopaths

00:18:55.987 --> 00:18:57.570
and brain scans of
his family members,

00:18:57.570 --> 00:18:59.653
because he just wanted a
control to compare there.

00:18:59.653 --> 00:19:02.655
And he found that his brain
was the most psychopathic,

00:19:02.655 --> 00:19:04.494
showed the most
psychopathic trends.

00:19:04.494 --> 00:19:05.910
And he then worked
out that he had

00:19:05.910 --> 00:19:09.750
several traits of psychopathy.

00:19:09.750 --> 00:19:12.690
And then he said, "Well why am
I not a fully blown psychopath?"

00:19:12.690 --> 00:19:16.380
Because he had his mother's
influence when he was young.

00:19:16.380 --> 00:19:17.910
And that's what
Simon Baron-Cohen

00:19:17.910 --> 00:19:19.480
calls the internal pot of gold.

00:19:19.480 --> 00:19:21.240
So anyway, there's the
thing about health,

00:19:21.240 --> 00:19:23.340
there's the thing
about criminal justice.

00:19:23.340 --> 00:19:25.500
And then there's
the pro-empathetic.

00:19:25.500 --> 00:19:30.630
If you continue to promote
arts and popular culture--

00:19:30.630 --> 00:19:34.207
and that includes public
service broadcasting, BBC--

00:19:34.207 --> 00:19:36.040
PETER BARRON: Talk more
in-depth about that.

00:19:36.040 --> 00:19:38.250
So we're talking about
our internal pot of gold.

00:19:38.250 --> 00:19:42.000
And you're really saying
that the arts fill up

00:19:42.000 --> 00:19:46.090
your internal pot of gold
and keep it healthy, yes?

00:19:46.090 --> 00:19:47.530
So how does that work?

00:19:47.530 --> 00:19:50.680
PETER BAZALGETTE: Well, I
mean, does anybody here--

00:19:50.680 --> 00:19:52.720
I mean, you may find radio
rather old fashioned.

00:19:52.720 --> 00:19:55.404
But does anybody here listen
to "The Archers" on Radio 4?

00:19:55.404 --> 00:19:56.570
PETER BARRON: Yeah, come on.

00:19:56.570 --> 00:19:58.570
PETER BAZALGETTE:
Yeah, it's one bloke--

00:19:58.570 --> 00:19:59.950
one bloke.

00:19:59.950 --> 00:20:01.750
Surely some of you do.

00:20:01.750 --> 00:20:04.570
Even if it's a podcast
you must listen to it.

00:20:04.570 --> 00:20:06.130
Anyway, the point
is "The Archers--"

00:20:06.130 --> 00:20:07.780
I'm just going to
take one example.

00:20:07.780 --> 00:20:10.130
"The Archers" is a publicly
funded radio soap opera

00:20:10.130 --> 00:20:12.862
that you're a huge fan of sir.

00:20:12.862 --> 00:20:14.320
You will be able
to tell us, it had

00:20:14.320 --> 00:20:17.260
a major storyline
last year, listened to

00:20:17.260 --> 00:20:21.620
by millions of people, about
violence within a marriage,

00:20:21.620 --> 00:20:24.180
with a very well worked
out storyline going on

00:20:24.180 --> 00:20:26.010
over a long period of time.

00:20:26.010 --> 00:20:28.500
And that made many
people understand

00:20:28.500 --> 00:20:33.210
what that phenomenon is, how to
report it, how to deal with it,

00:20:33.210 --> 00:20:36.940
and what it's like for
people involved in it.

00:20:36.940 --> 00:20:39.750
But I could tell you about
"Eastenders" and "Coronation

00:20:39.750 --> 00:20:40.850
Street."

00:20:40.850 --> 00:20:42.600
PETER BARRON: Well
actually "Big Brother,"

00:20:42.600 --> 00:20:43.470
and you mentioned
this in the book,

00:20:43.470 --> 00:20:46.080
because Peter was that the
man behind "Big Brother" who

00:20:46.080 --> 00:20:47.610
brought it to Britain.

00:20:47.610 --> 00:20:49.050
PETER BAZALGETTE: So most
people blame me for that.

00:20:49.050 --> 00:20:49.780
PETER BARRON: Over many years.

00:20:49.780 --> 00:20:51.920
And it's certainly not
been uncontroversial.

00:20:51.920 --> 00:20:53.689
And "The Daily Mail"
and others have--

00:20:53.689 --> 00:20:55.480
PETER BAZALGETTE: We
love "The Daily Mail."

00:20:55.480 --> 00:20:57.666
PETER BARRON: --attacked
you in strong language.

00:20:57.666 --> 00:20:59.040
But actually you
make an argument

00:20:59.040 --> 00:21:01.530
that, particularly
in the early days,

00:21:01.530 --> 00:21:05.190
that "Big Brother" actually
had an empathetic benefit

00:21:05.190 --> 00:21:05.790
to the nation.

00:21:05.790 --> 00:21:09.550
PETER BAZALGETTE: Well, look,
I don't want to overstate this.

00:21:09.550 --> 00:21:11.370
You'll get me into trouble.

00:21:11.370 --> 00:21:14.550
It's an entertainment
show made for money

00:21:14.550 --> 00:21:17.700
with commercials around
it, invented by a Dutchman

00:21:17.700 --> 00:21:19.620
who became very wealthy
on the back of it.

00:21:19.620 --> 00:21:21.300
And so let me not
sort of make out

00:21:21.300 --> 00:21:25.980
that it was some sort
of religious crusade.

00:21:25.980 --> 00:21:27.640
It was not.

00:21:27.640 --> 00:21:34.350
It had an interesting side
effect, let me put it that way.

00:21:34.350 --> 00:21:35.730
You put 12 people in the house--

00:21:35.730 --> 00:21:38.090
I mean, it's part of
mainstream television now.

00:21:38.090 --> 00:21:39.965
There are lots of
different programs doing it

00:21:39.965 --> 00:21:42.380
in different ways.

00:21:42.380 --> 00:21:46.300
In the UK, it turned out
the people who voted--

00:21:46.300 --> 00:21:48.120
and this is a
popularity contest--

00:21:48.120 --> 00:21:52.400
people who voted were looking
for authentic decent people

00:21:52.400 --> 00:21:53.920
that they approved
of, who they felt

00:21:53.920 --> 00:21:55.730
behaved in a decent manner.

00:21:55.730 --> 00:21:57.356
We have some sort of--

00:21:57.356 --> 00:21:58.730
In America, it's
quite different.

00:21:58.730 --> 00:22:00.600
The winners of "Big
Brother" in America

00:22:00.600 --> 00:22:03.950
are always the people who
played the competition hardest

00:22:03.950 --> 00:22:05.240
and cheated and backstabbed.

00:22:05.240 --> 00:22:06.500
They would get voted first.

00:22:06.500 --> 00:22:08.540
So just shows you how
different countries

00:22:08.540 --> 00:22:10.480
have different cultures--

00:22:10.480 --> 00:22:11.480
interesting.

00:22:11.480 --> 00:22:16.130
But in the UK we had
a winner, a gay man

00:22:16.130 --> 00:22:19.430
in an early series, which I
think was quite significant.

00:22:19.430 --> 00:22:23.270
And a gay woman in the first
series was the runner up.

00:22:23.270 --> 00:22:28.640
We had a winner who had Tourette
syndrome, who was apparently

00:22:28.640 --> 00:22:30.560
foul mouthed.

00:22:30.560 --> 00:22:33.680
It turned out to be somebody
who was a completely

00:22:33.680 --> 00:22:38.240
charming bloke, but had this
compulsion, this mental tick

00:22:38.240 --> 00:22:39.690
as it were.

00:22:39.690 --> 00:22:42.530
And the public discovered
behind that foul mouth

00:22:42.530 --> 00:22:45.059
was a very, very nice--
and we had a transsexual.

00:22:45.059 --> 00:22:46.850
And these were all
people who were somewhat

00:22:46.850 --> 00:22:49.430
stereotyped and demonized by
the tabloid newspapers when

00:22:49.430 --> 00:22:51.567
they first appeared.

00:22:51.567 --> 00:22:53.150
And all I would say
about that-- and I

00:22:53.150 --> 00:22:55.358
don't want to make, as I
say, too many claims for it.

00:22:55.358 --> 00:22:56.600
It's just an aside--

00:22:56.600 --> 00:22:58.970
is that it did
show that when you

00:22:58.970 --> 00:23:04.520
tell a human story in the
arena of popular culture,

00:23:04.520 --> 00:23:07.382
you quite often get a
perspective about somebody

00:23:07.382 --> 00:23:08.840
you previously
would have dismissed

00:23:08.840 --> 00:23:10.149
in a stereotypical way.

00:23:10.149 --> 00:23:11.690
PETER BARRON: Right,
good, thank you.

00:23:11.690 --> 00:23:14.190
Let's go to the audience
for some questions.

00:23:14.190 --> 00:23:15.884
SPEAKER: I've got
a microphone here

00:23:15.884 --> 00:23:17.050
if anyone would like to ask.

00:23:20.690 --> 00:23:22.370
PETER BARRON:
Gosh, No questions.

00:23:22.370 --> 00:23:23.560
Oh, we've got one over here.

00:23:23.560 --> 00:23:25.110
Excellent.

00:23:25.110 --> 00:23:27.270
PETER BAZALGETTE: Mic's coming.

00:23:27.270 --> 00:23:28.530
Hello.

00:23:28.530 --> 00:23:30.510
AUDIENCE: So I am
particularly concerned

00:23:30.510 --> 00:23:35.400
about this issue, this
topic, and out of it

00:23:35.400 --> 00:23:36.695
the switch, the empathy switch.

00:23:36.695 --> 00:23:39.070
PETER BAZALGETTE: You mean
switching it on it on and off?

00:23:39.070 --> 00:23:39.190
AUDIENCE: Yes.

00:23:39.190 --> 00:23:40.380
PETER BAZALGETTE:
Yeah, of course.

00:23:40.380 --> 00:23:41.213
AUDIENCE: Of course.

00:23:41.213 --> 00:23:44.220
And I'm thinking, and I've
been thinking about this

00:23:44.220 --> 00:23:47.130
and how individuals
are actually choosing

00:23:47.130 --> 00:23:49.110
to switch their
empathy off as it

00:23:49.110 --> 00:23:51.210
suits them on various examples.

00:23:51.210 --> 00:23:56.180
And it happens for most of
us on a daily basis, I think.

00:23:56.180 --> 00:23:58.560
If and when we go
to the supermarket,

00:23:58.560 --> 00:24:01.047
it's as simple as that.

00:24:01.047 --> 00:24:03.130
So I'd like to address the
questions of the public

00:24:03.130 --> 00:24:05.760
if you don't mind.

00:24:05.760 --> 00:24:09.540
How many of you think
they love animals?

00:24:09.540 --> 00:24:12.650
Can you raise your hand?

00:24:12.650 --> 00:24:13.750
OK.

00:24:13.750 --> 00:24:14.380
How many--

00:24:14.380 --> 00:24:17.730
PETER BAZALGETTE: This is only
for human beings by the way.

00:24:17.730 --> 00:24:21.190
AUDIENCE: And how many
of you are meat-eaters?

00:24:21.190 --> 00:24:23.330
PETER BAZALGETTE:
Oh that's a toughie.

00:24:23.330 --> 00:24:24.460
Go on.

00:24:24.460 --> 00:24:25.360
Keep going.

00:24:25.360 --> 00:24:27.210
AUDIENCE: I don't want
to hijack this topic.

00:24:27.210 --> 00:24:27.720
PETER BAZALGETTE: No, it's fine.

00:24:27.720 --> 00:24:28.210
It's fine.

00:24:28.210 --> 00:24:28.280
It's fine.

00:24:28.280 --> 00:24:28.350
It's fine.

00:24:28.350 --> 00:24:30.600
AUDIENCE: But I'm thinking
that it's a paradox,

00:24:30.600 --> 00:24:33.510
don't you think, to say and
think that you love animals,

00:24:33.510 --> 00:24:37.110
but at the same time
to eat their bodies

00:24:37.110 --> 00:24:39.925
and accept that they
are being killed in--

00:24:39.925 --> 00:24:41.550
PETER BAZALGETTE:
Are you a vegetarian?

00:24:41.550 --> 00:24:42.487
AUDIENCE: I am vegan.

00:24:42.487 --> 00:24:43.680
PETER BAZALGETTE:
You're a vegan.

00:24:43.680 --> 00:24:44.130
AUDIENCE: Yes.

00:24:44.130 --> 00:24:45.546
PETER BAZALGETTE:
Good, excellent.

00:24:45.546 --> 00:24:48.545
AUDIENCE: Well I am supposed
to stand to my beliefs.

00:24:48.545 --> 00:24:51.010
PETER BAZALGETTE: No, no,
I'm respecting your beliefs

00:24:51.010 --> 00:24:52.290
and I'm very pleased
you've made them.

00:24:52.290 --> 00:24:53.960
I don't want to
interrupt you by the way.

00:24:53.960 --> 00:24:54.918
AUDIENCE: Right, right.

00:24:54.918 --> 00:24:56.850
So I consider this
as being an exercise,

00:24:56.850 --> 00:25:01.620
a daily exercise in everyone
of switching their empathy off.

00:25:01.620 --> 00:25:05.300
Because if we would turn
our empathy back on,

00:25:05.300 --> 00:25:08.820
then we wouldn't choose that
chicken or that piece of lamb

00:25:08.820 --> 00:25:11.130
or beef in the supermarket.

00:25:11.130 --> 00:25:12.907
And that's a conscious choice.

00:25:12.907 --> 00:25:14.990
And I think that most of
us do not think about it.

00:25:14.990 --> 00:25:17.910
Or even worse, we choose
not to think about it.

00:25:17.910 --> 00:25:21.810
And going forward, having
this exercise, daily exercise

00:25:21.810 --> 00:25:25.410
of switching our empathy off,
doesn't that make us more--

00:25:25.410 --> 00:25:26.790
and I'm just giving one example.

00:25:26.790 --> 00:25:27.990
There are others.

00:25:27.990 --> 00:25:33.210
Doesn't that make us more prone
to listening to what society

00:25:33.210 --> 00:25:37.260
dictates to us and being
part to other trends

00:25:37.260 --> 00:25:39.926
where the empathy is
switched on and off,

00:25:39.926 --> 00:25:42.300
and especially off, because
we already have the exercise.

00:25:42.300 --> 00:25:44.310
We're already capable.

00:25:44.310 --> 00:25:46.980
And this is kind of
what I think makes

00:25:46.980 --> 00:25:49.500
some people more prone to it.

00:25:49.500 --> 00:25:52.620
PETER BAZALGETTE: Good
that's a very, really good--

00:25:52.620 --> 00:25:54.800
no, it's a really interesting
and good challenge.

00:25:54.800 --> 00:25:55.770
AUDIENCE: What do you think?

00:25:55.770 --> 00:25:56.940
PETER BAZALGETTE: Well let
me put an alternative point

00:25:56.940 --> 00:25:57.523
of view to it.

00:25:57.523 --> 00:25:58.590
AUDIENCE: Yes please.

00:25:58.590 --> 00:26:01.048
PETER BAZALGETTE: And thank
you very much for the question.

00:26:03.170 --> 00:26:05.480
Our ability to switch
our empathy off

00:26:05.480 --> 00:26:08.510
is what keeps us sane.

00:26:08.510 --> 00:26:10.740
There's something called
emotional burnout.

00:26:10.740 --> 00:26:13.240
If your empathy is
always switched on,

00:26:13.240 --> 00:26:17.280
you watch the television news,
see what's happening in Syria,

00:26:17.280 --> 00:26:19.680
you'd go mad with grief.

00:26:19.680 --> 00:26:22.036
And you'd cease to
operate as a human being.

00:26:22.036 --> 00:26:23.160
Yes, no, tell me I'm wrong.

00:26:23.160 --> 00:26:24.360
Go ahead.

00:26:24.360 --> 00:26:26.832
No, have the mic back.

00:26:26.832 --> 00:26:29.520
AUDIENCE: But that's
the emotional empathy

00:26:29.520 --> 00:26:31.260
that we all have to
switch off, isn't it?

00:26:31.260 --> 00:26:33.020
It's not the cognitive one.

00:26:33.020 --> 00:26:36.002
PETER BAZALGETTE: Well, it's
an interplay of the two.

00:26:36.002 --> 00:26:37.210
It's an interplay of the two.

00:26:41.800 --> 00:26:43.810
Both operate in different ways.

00:26:43.810 --> 00:26:46.450
And doctors have to have a
very subtle understanding

00:26:46.450 --> 00:26:48.850
of how you connect those
two and how you use them.

00:26:48.850 --> 00:26:54.520
But all I'm saying is generally,
that's how we operate.

00:26:54.520 --> 00:26:57.340
And it's very interesting
in news, which you've worked

00:26:57.340 --> 00:26:59.680
in Peter, how it works that--

00:27:02.720 --> 00:27:04.330
when David Cameron
was prime minister

00:27:04.330 --> 00:27:06.220
and the Syrian
refugees were flooding

00:27:06.220 --> 00:27:10.070
across the
Mediterranean, he'd seen

00:27:10.070 --> 00:27:11.720
the research that
said people were

00:27:11.720 --> 00:27:13.204
very worried about immigration.

00:27:13.204 --> 00:27:15.620
Which by the way they were
when the Kindertransport Jewish

00:27:15.620 --> 00:27:17.610
children were trying--

00:27:17.610 --> 00:27:20.294
were trying to get the Jewish
children in the late 1930s.

00:27:20.294 --> 00:27:21.710
Similarly there
were people saying

00:27:21.710 --> 00:27:22.910
we can't take all these people.

00:27:22.910 --> 00:27:23.690
We don't want these people.

00:27:23.690 --> 00:27:24.273
Who are there?

00:27:24.273 --> 00:27:27.310
They have nothing to do with us.

00:27:27.310 --> 00:27:29.860
We find it very difficult to
empathize with a population

00:27:29.860 --> 00:27:32.500
but very easy to empathise
with an individual story.

00:27:32.500 --> 00:27:35.080
Alan Kurdi, which I
write about in the book,

00:27:35.080 --> 00:27:38.230
was the little boy, tragically
3 and 1/2 in trainers face

00:27:38.230 --> 00:27:39.976
down drowned on the beach.

00:27:39.976 --> 00:27:41.600
And Cameron then
agreed-- it turned out

00:27:41.600 --> 00:27:43.720
to be 350 we heard
the other day.

00:27:43.720 --> 00:27:46.300
But he agreed to
take, I think, 12,000.

00:27:46.300 --> 00:27:49.090
And public sentiment changed.

00:27:49.090 --> 00:27:52.120
There was empathy switched off
and then switched on again.

00:27:52.120 --> 00:27:54.460
It's actually completely
natural for you

00:27:54.460 --> 00:27:56.790
to switch your
empathy on and off.

00:27:56.790 --> 00:28:00.780
It's a completely
natural mechanism.

00:28:00.780 --> 00:28:04.190
If you walked down
the street and stopped

00:28:04.190 --> 00:28:10.910
at every person who may need
help, you'd never get to work.

00:28:10.910 --> 00:28:13.570
And if you didn't get to work,
you wouldn't-- because you

00:28:13.570 --> 00:28:14.240
wouldn't be--

00:28:14.240 --> 00:28:16.740
so that's a slightly-- it may
sound slightly absurd example,

00:28:16.740 --> 00:28:20.160
but we constantly
switch it on and off.

00:28:20.160 --> 00:28:23.790
My argument is, we need to be--
and you're about to tell me

00:28:23.790 --> 00:28:25.690
I'm wrong by the
way, but anyway.

00:28:25.690 --> 00:28:30.120
But my argument is that we need
to be much more aware of it

00:28:30.120 --> 00:28:35.610
and enhance it for
better public discourse,

00:28:35.610 --> 00:28:38.240
for better public services
and all the rest of it,

00:28:38.240 --> 00:28:40.246
but to appreciate the
subtleties and the nuance.

00:28:40.246 --> 00:28:41.370
Now have one more go at me.

00:28:41.370 --> 00:28:41.550
Go on.

00:28:41.550 --> 00:28:43.380
AUDIENCE: But then
where's the moral?

00:28:43.380 --> 00:28:44.370
Where's the ethics?

00:28:44.370 --> 00:28:46.260
I mean if it's OK
to switch empathy

00:28:46.260 --> 00:28:49.370
on and off between good and evil
and you're always oscillating--

00:28:49.370 --> 00:28:51.745
PETER BAZALGETTE: No, no, no,
but empathy is an instinct.

00:28:51.745 --> 00:28:52.740
It's not a moral code.

00:28:52.740 --> 00:28:54.690
What I said earlier
was that empathy

00:28:54.690 --> 00:29:00.000
operates with moral
code, and with a sense

00:29:00.000 --> 00:29:01.530
of reciprocity and fairness.

00:29:01.530 --> 00:29:03.300
And all those things
have to interrelate.

00:29:03.300 --> 00:29:04.990
It's not the same
thing as a moral code.

00:29:04.990 --> 00:29:05.160
AUDIENCE: It's not.

00:29:05.160 --> 00:29:06.090
PETER BAZALGETTE: That
is a separate thing--

00:29:06.090 --> 00:29:06.450
AUDIENCE: It's not.

00:29:06.450 --> 00:29:06.930
PETER BAZALGETTE:
--separate construct.

00:29:06.930 --> 00:29:07.790
AUDIENCE: But it impacts it--

00:29:07.790 --> 00:29:07.970
PETER BAZALGETTE:
And by the way,

00:29:07.970 --> 00:29:10.620
you have a different moral
code to me, because yes, I

00:29:10.620 --> 00:29:12.720
love dogs, but yes, I eat meat.

00:29:12.720 --> 00:29:17.445
And in your eyes, that
is, as you've made clear,

00:29:17.445 --> 00:29:19.170
a contradictory position.

00:29:19.170 --> 00:29:21.600
But it's one I can live with.

00:29:21.600 --> 00:29:24.030
PETER BARRON: I think we
should give others a chance.

00:29:24.030 --> 00:29:24.640
Let's move on.

00:29:24.640 --> 00:29:26.681
PETER BAZALGETTE: But
they're fantastic questions

00:29:26.681 --> 00:29:29.070
and really good challenges,
really interesting.

00:29:29.070 --> 00:29:30.050
PETER BARRON: Next
question over here.

00:29:30.050 --> 00:29:31.924
AUDIENCE: I might actually
hop in and then go

00:29:31.924 --> 00:29:34.020
there, very prosaic.

00:29:34.020 --> 00:29:36.180
A lot of my friends think
very similarly to me.

00:29:36.180 --> 00:29:38.652
So we get together and
we say the same things

00:29:38.652 --> 00:29:40.110
and we agree with
each other and we

00:29:40.110 --> 00:29:42.630
think it's fabulous,
which means we're not

00:29:42.630 --> 00:29:45.210
exposed to these conflicting
views and opinions.

00:29:45.210 --> 00:29:46.507
How do I go about doing that?

00:29:46.507 --> 00:29:47.340
What's the best way?

00:29:51.851 --> 00:29:53.850
PETER BAZALGETTE: That's
a really good question.

00:29:53.850 --> 00:29:57.740
I think the answer's
in this room, isn't it?

00:29:57.740 --> 00:30:01.640
I mean, the very thing that
I said in the digital era

00:30:01.640 --> 00:30:04.130
where we're tending to connect
with people we agree with,

00:30:04.130 --> 00:30:07.722
we have the ability to connect
with people we disagree with.

00:30:07.722 --> 00:30:09.180
And that, of course,
is the essence

00:30:09.180 --> 00:30:10.470
of democratic discourse.

00:30:13.680 --> 00:30:15.630
Are we a more polarized
society now than we

00:30:15.630 --> 00:30:17.810
were five or 10 years ago?

00:30:17.810 --> 00:30:20.090
PETER BARRON: Well I
think the Brexit thing

00:30:20.090 --> 00:30:21.650
and what's happened
in the States

00:30:21.650 --> 00:30:23.542
suggests we are more polarized.

00:30:23.542 --> 00:30:25.250
PETER BAZALGETTE: But
you know something,

00:30:25.250 --> 00:30:26.850
maybe we were
polarized all along

00:30:26.850 --> 00:30:28.730
but we never just
put the question.

00:30:28.730 --> 00:30:33.830
So we went through a
polarizing experience.

00:30:33.830 --> 00:30:35.680
Maybe people felt
that way already.

00:30:35.680 --> 00:30:41.000
So you could say that the number
of places that voted to remain

00:30:41.000 --> 00:30:43.150
remind me of the
English Civil War.

00:30:43.150 --> 00:30:45.540
In the English Civil
War in the 17th century,

00:30:45.540 --> 00:30:47.740
various cities were for
the king and various cities

00:30:47.740 --> 00:30:48.980
were for Cromwell.

00:30:48.980 --> 00:30:51.680
And if you look at the
cities that were for remain,

00:30:51.680 --> 00:30:57.950
London, Brighton, Bristol,
Oxford, Cambridge, Manchester,

00:30:57.950 --> 00:31:01.490
Newcastle just,
these are all places

00:31:01.490 --> 00:31:08.060
with sunrise industries, strong
higher education, growth, jobs,

00:31:08.060 --> 00:31:14.550
strong economies basically,
and modern economies.

00:31:14.550 --> 00:31:17.210
And if you look at the places
that voted massively to leave,

00:31:17.210 --> 00:31:20.630
they're depressed.

00:31:20.630 --> 00:31:22.729
When you live in
the London bubble,

00:31:22.729 --> 00:31:24.770
we haven't thought about
that very much, have we?

00:31:24.770 --> 00:31:26.310
I hadn't thought
about it enough.

00:31:26.310 --> 00:31:26.540
PETER BARRON: Yeah.

00:31:26.540 --> 00:31:28.081
I mean it definitely
works both ways.

00:31:28.081 --> 00:31:29.210
We talk about empathy.

00:31:29.210 --> 00:31:32.850
Of course the Syrian
kids on the beach,

00:31:32.850 --> 00:31:35.540
that provoked a huge amount
of empathy among people.

00:31:35.540 --> 00:31:39.140
But it's harder, perhaps,
to demonstrate empathy

00:31:39.140 --> 00:31:41.540
for the people
living in Doncaster

00:31:41.540 --> 00:31:43.490
who feel that they
are being overwhelmed

00:31:43.490 --> 00:31:48.810
by refugees or immigrants coming
into their towns and so on.

00:31:48.810 --> 00:31:50.780
So I think it's
kind of important,

00:31:50.780 --> 00:31:53.960
isn't it, to be empathetic
in all directions.

00:31:53.960 --> 00:31:55.430
And I think going
back to news, I

00:31:55.430 --> 00:31:58.730
think the point about a
good way of doing journalism

00:31:58.730 --> 00:32:00.890
is to take news from
all different sides

00:32:00.890 --> 00:32:04.210
and expose those rather than
drive one view all the time.

00:32:04.210 --> 00:32:06.370
PETER BAZALGETTE:
And politicians

00:32:06.370 --> 00:32:11.582
have to listen in the
end to be successful.

00:32:11.582 --> 00:32:14.040
Question whether the current
president of the United States

00:32:14.040 --> 00:32:15.390
is a listener.

00:32:15.390 --> 00:32:17.130
I think probably not.

00:32:17.130 --> 00:32:19.411
PETER BARRON: Next question.

00:32:19.411 --> 00:32:20.410
PETER BAZALGETTE: Hello.

00:32:20.410 --> 00:32:22.190
AUDIENCE: Hey.

00:32:22.190 --> 00:32:25.220
How do you think we
should apply empathy,

00:32:25.220 --> 00:32:29.760
both cognitive and
sympathetic, to people

00:32:29.760 --> 00:32:35.550
who are very far away from
our own political opinions

00:32:35.550 --> 00:32:37.740
and standpoints?

00:32:37.740 --> 00:32:41.430
I feel that it's very
easy, especially when

00:32:41.430 --> 00:32:44.910
you take someone who's kind
of on the polar opposite

00:32:44.910 --> 00:32:47.460
of yourself, it's
very easy to think,

00:32:47.460 --> 00:32:52.200
oh, they're being deluded
or they're just pure evil

00:32:52.200 --> 00:32:53.280
or something like that.

00:32:53.280 --> 00:32:59.860
But how should we actually
approach these people?

00:32:59.860 --> 00:33:05.080
Should we see their opinions as
being divorced from themselves

00:33:05.080 --> 00:33:07.495
as their fundamental nature?

00:33:10.000 --> 00:33:12.730
Because at the same time you
could also use this argument

00:33:12.730 --> 00:33:18.350
to kind of take people
who most of us would think

00:33:18.350 --> 00:33:21.770
are quite evil,
bad people and just

00:33:21.770 --> 00:33:25.182
say, oh, that's simply a
political opinion they had.

00:33:25.182 --> 00:33:26.390
So how does that [INAUDIBLE]?

00:33:26.390 --> 00:33:29.730
PETER BAZALGETTE: That's
a really good question.

00:33:29.730 --> 00:33:32.550
I was talking earlier
about the interplay

00:33:32.550 --> 00:33:35.430
of empathy and ethics.

00:33:35.430 --> 00:33:38.330
So there are things
that unite us,

00:33:38.330 --> 00:33:43.370
even people we have very strong
political disagreements about--

00:33:43.370 --> 00:33:49.010
the rule of law, property,
the ethics of free speech,

00:33:49.010 --> 00:33:50.360
a democratic society.

00:33:50.360 --> 00:33:54.090
There are various
things we subscribe to.

00:33:54.090 --> 00:33:57.330
And so there is a
distinction between people

00:33:57.330 --> 00:34:01.350
who come within our generally
agreed ethics and people

00:34:01.350 --> 00:34:02.910
who are way beyond it.

00:34:02.910 --> 00:34:05.730
There is a dilemma nowadays--

00:34:05.730 --> 00:34:09.239
and I talk about it in the
chapter on criminal justice.

00:34:09.239 --> 00:34:15.540
There's a dilemma today because
the old statement of, say,

00:34:15.540 --> 00:34:19.500
the police or religious
leaders, such and such

00:34:19.500 --> 00:34:24.030
was pure evil is
not good enough.

00:34:24.030 --> 00:34:26.620
Personally, you know, I'm
not actually religious

00:34:26.620 --> 00:34:28.800
and I don't believe in
something called the devil.

00:34:28.800 --> 00:34:31.560
But I think the devil was a
way of articulating people

00:34:31.560 --> 00:34:34.830
who lacked empathy,
who were psychopathic,

00:34:34.830 --> 00:34:38.340
who were not able to have
any emotional empathy

00:34:38.340 --> 00:34:39.659
for other people.

00:34:39.659 --> 00:34:43.110
And the question is,
well, if they have a brain

00:34:43.110 --> 00:34:46.989
malfunction, how can you, in
any way, hold them responsible?

00:34:46.989 --> 00:34:51.920
And the answer is, one, you need
to protect society from them.

00:34:51.920 --> 00:34:53.699
So you need to
lock people up who

00:34:53.699 --> 00:34:56.340
are a danger to the
rest of society.

00:34:56.340 --> 00:34:59.340
But two, for society's
own good, you

00:34:59.340 --> 00:35:02.340
need to have a
principle of being

00:35:02.340 --> 00:35:04.440
responsible for your
actions, even if you are

00:35:04.440 --> 00:35:07.770
suffering from a malfunction.

00:35:07.770 --> 00:35:14.060
So all of those things
say that if people

00:35:14.060 --> 00:35:19.830
are within what
is legal, what is

00:35:19.830 --> 00:35:23.070
agreed as being the ethics
of a democratic society

00:35:23.070 --> 00:35:25.934
that we subscribe to, then
they may disagree with us,

00:35:25.934 --> 00:35:28.350
but we should listen to them
and they're a legitimate part

00:35:28.350 --> 00:35:30.120
of the democratic discourse.

00:35:30.120 --> 00:35:34.080
If they're beyond that, if
they are murdering people

00:35:34.080 --> 00:35:40.710
or if they're involved in,
say, racial hatred, race crime,

00:35:40.710 --> 00:35:41.790
that's something else.

00:35:41.790 --> 00:35:43.060
And that's a distinction.

00:35:43.060 --> 00:35:43.800
PETER BARRON: Yes,
but there are people

00:35:43.800 --> 00:35:45.480
who would argue that
you should even be

00:35:45.480 --> 00:35:46.740
empathetic towards terrorists.

00:35:46.740 --> 00:35:48.690
I mean, there's a
fascinating interview

00:35:48.690 --> 00:35:51.651
with Ian Paisley Jr. on the
radio the other day talking

00:35:51.651 --> 00:35:52.650
about Martin McGuinness.

00:35:52.650 --> 00:35:56.550
And it was a masterclass
in empathy toward someone

00:35:56.550 --> 00:36:00.585
who used to be the commander
in chief of the IRA.

00:36:00.585 --> 00:36:02.460
And people like Jonathan
Powell, for example,

00:36:02.460 --> 00:36:04.460
say in the end you've got
to talk to all terrorists.

00:36:04.460 --> 00:36:05.060
PETER BAZALGETTE:
Quoted in the book.

00:36:05.060 --> 00:36:06.768
PETER BARRON: You've
got to talk to ISIS.

00:36:06.768 --> 00:36:12.150
So could you be empathetic
towards at least the causes

00:36:12.150 --> 00:36:14.910
of ISIS if not the
individuals themselves?

00:36:14.910 --> 00:36:16.680
PETER BAZALGETTE:
Well, in the end,

00:36:16.680 --> 00:36:19.860
you settle disputes
of that sort,

00:36:19.860 --> 00:36:22.416
Northern Ireland, by talking.

00:36:22.416 --> 00:36:24.540
Personally I believe peace
came to Northern Ireland

00:36:24.540 --> 00:36:29.630
because the young hotheads
driven by testosterone

00:36:29.630 --> 00:36:32.310
had children and then indeed
grandchildren in the case

00:36:32.310 --> 00:36:35.430
of some of them, Gerry Adams.

00:36:35.430 --> 00:36:38.230
And I think they wanted a
future for their children

00:36:38.230 --> 00:36:39.230
and their grandchildren.

00:36:39.230 --> 00:36:41.430
And I think they changed
their point of view.

00:36:41.430 --> 00:36:45.250
And I think their
testosterone levels went down.

00:36:45.250 --> 00:36:50.100
But yes, in the end you have to
talk to terrorists for peace.

00:36:50.100 --> 00:36:52.500
But that doesn't mean,
does it, that you

00:36:52.500 --> 00:36:56.150
endorse their terrorism,
which is a different thing.

00:36:56.150 --> 00:36:58.280
But I do quote Jonathan
Powell's quote there.

00:36:58.280 --> 00:37:02.660
In fact it was Hugh
Gaitskell who said that--

00:37:02.660 --> 00:37:05.180
he was referring
to the independence

00:37:05.180 --> 00:37:09.800
leaders in British colonies
in Africa in the 1950s.

00:37:09.800 --> 00:37:11.540
He said it always
ends up with drinks

00:37:11.540 --> 00:37:14.060
in the Dorchester, which is
a similar way of Jonathan

00:37:14.060 --> 00:37:15.469
Powell's statement.

00:37:15.469 --> 00:37:17.510
I don't think I perfectly
answered your question,

00:37:17.510 --> 00:37:20.464
but I've tried to address it.

00:37:20.464 --> 00:37:21.630
PETER BARRON: Next question.

00:37:29.890 --> 00:37:37.830
AUDIENCE: So it seems to me that
we tend to shut down empathy

00:37:37.830 --> 00:37:39.840
in response to fear.

00:37:39.840 --> 00:37:42.510
To what extent does the
evidence bear this out

00:37:42.510 --> 00:37:44.370
as being generally true?

00:37:44.370 --> 00:37:49.050
And if it does, what can
we do about it to stop--

00:37:49.050 --> 00:37:52.770
you know, it seems to be very
easy to stir up fear and get

00:37:52.770 --> 00:37:55.270
people to react,
to sort of ignore

00:37:55.270 --> 00:37:58.860
their sort of empathy instincts.

00:37:58.860 --> 00:38:01.670
PETER BAZALGETTE: Yeah, just
unpack fear or examples,

00:38:01.670 --> 00:38:03.570
what you're talking about.

00:38:03.570 --> 00:38:06.540
AUDIENCE: For example, sort
of tabloids stirring up

00:38:06.540 --> 00:38:08.730
panic over immigration,
for example,

00:38:08.730 --> 00:38:17.730
or Nigel Farage talking about
health tourists coming here

00:38:17.730 --> 00:38:21.810
to get HIV treatment,
that sort of thing.

00:38:21.810 --> 00:38:26.430
It seems often some
politicians will deliberately

00:38:26.430 --> 00:38:27.960
play on our fears
in order to get

00:38:27.960 --> 00:38:29.620
us to shut down our empathy.

00:38:29.620 --> 00:38:30.760
How do we combat that?

00:38:30.760 --> 00:38:33.840
How do we get people to be
empathic in the face of fear?

00:38:38.610 --> 00:38:40.689
PETER BAZALGETTE: It's
such a good question.

00:38:40.689 --> 00:38:43.230
Do you personally-- sorry, just
could you check the mic back?

00:38:43.230 --> 00:38:44.700
Sorry.

00:38:44.700 --> 00:38:47.910
Do you personally feel--
because the examples you give

00:38:47.910 --> 00:38:50.010
are to do with media
representation or media

00:38:50.010 --> 00:38:52.230
quoting people in
a particular way.

00:38:52.230 --> 00:38:54.150
Do you feel that
our public discourse

00:38:54.150 --> 00:38:57.030
has got more extreme
in the last five years?

00:38:57.030 --> 00:38:57.960
Is that your sense?

00:38:57.960 --> 00:38:58.695
How do you feel?

00:38:58.695 --> 00:39:00.676
AUDIENCE: I feel that
it has, but I'm not sure

00:39:00.676 --> 00:39:02.550
whether it's really true
or whether it's just

00:39:02.550 --> 00:39:06.310
my perception, that I'm noticing
sort of frequency bias type

00:39:06.310 --> 00:39:08.310
thing, that I'm noticing
more examples of things

00:39:08.310 --> 00:39:10.164
that sort of trigger me.

00:39:10.164 --> 00:39:11.580
PETER BAZALGETTE:
So here, there's

00:39:11.580 --> 00:39:13.320
no perfect answer
to your question.

00:39:13.320 --> 00:39:15.970
But here's a couple of points.

00:39:15.970 --> 00:39:18.480
One is, I said that you
have to link empathy

00:39:18.480 --> 00:39:21.660
not only to ethics but a sense
of reciprocity and fairness.

00:39:21.660 --> 00:39:24.740
If you said that the British
health service, enormously

00:39:24.740 --> 00:39:27.180
under pressure as it is,
was opened to all comers

00:39:27.180 --> 00:39:28.680
and anybody could
come here and have

00:39:28.680 --> 00:39:32.119
a child in a maternity
hospital or get any treatment

00:39:32.119 --> 00:39:33.660
they want from
anywhere in the world,

00:39:33.660 --> 00:39:35.739
that would be patently
absurd because we simply

00:39:35.739 --> 00:39:36.780
don't have the resources.

00:39:36.780 --> 00:39:38.550
And you might say,
actually, we are

00:39:38.550 --> 00:39:42.314
going to prioritize the
people in this country who

00:39:42.314 --> 00:39:44.230
pay the taxes that fund
it in the first place.

00:39:44.230 --> 00:39:47.220
And that's fairness.

00:39:47.220 --> 00:39:48.780
And so that's part
of the answer.

00:39:51.340 --> 00:39:53.409
And I can't tell you
where the line's drawn

00:39:53.409 --> 00:39:54.700
or what the perfect balance is.

00:39:54.700 --> 00:39:57.820
I just acknowledge that those
are different factors in it.

00:39:57.820 --> 00:40:02.640
But in terms of extremism
or extreme representation,

00:40:02.640 --> 00:40:06.320
and indeed
misrepresentation, now we're

00:40:06.320 --> 00:40:08.150
coming to the
dreaded phrase fake

00:40:08.150 --> 00:40:10.639
news, which is, I
think, a phrase that you

00:40:10.639 --> 00:40:12.680
think about in this building
very often actually.

00:40:17.540 --> 00:40:24.200
I personally think that we need
trusted and reliable sources

00:40:24.200 --> 00:40:26.930
of news and information to
let the democratic society

00:40:26.930 --> 00:40:28.097
function.

00:40:28.097 --> 00:40:30.680
I would observe-- it may sound
like I'm going off of one here,

00:40:30.680 --> 00:40:32.200
but bear with me.

00:40:32.200 --> 00:40:36.940
I would observe that most of
those sources of impartial news

00:40:36.940 --> 00:40:39.010
or news that tries
to be impartial

00:40:39.010 --> 00:40:41.560
would have to be subsidized
because they're not profitable.

00:40:41.560 --> 00:40:43.600
By which I mean the BBC
has the license fee.

00:40:43.600 --> 00:40:44.770
"The Guardian" loses money.

00:40:44.770 --> 00:40:46.090
"The Times" loses money.

00:40:49.150 --> 00:40:52.330
"Huffington Post" has
been bought by somebody

00:40:52.330 --> 00:40:56.194
and is maintained, but
I don't think on its own

00:40:56.194 --> 00:40:56.860
it's profitable.

00:40:56.860 --> 00:41:01.000
It's just part of a
bigger online presence.

00:41:01.000 --> 00:41:03.896
Sky News loses money.

00:41:03.896 --> 00:41:06.520
PETER BARRON: But it is the case
that stronger flavors are more

00:41:06.520 --> 00:41:07.420
commercially viable.

00:41:07.420 --> 00:41:11.080
So you see Fox News
and so on where--

00:41:11.080 --> 00:41:14.430
PETER BAZALGETTE: And Fox News
with its latest news on Sweden

00:41:14.430 --> 00:41:17.570
that the president of the United
States was able to rely on.

00:41:17.570 --> 00:41:20.590
So what am I saying?

00:41:20.590 --> 00:41:23.889
This is not just a sort of
motherhood and apple pie

00:41:23.889 --> 00:41:25.930
answer, because I tell
you what I'm going to say.

00:41:25.930 --> 00:41:29.020
I'm going to say that those
sources of news and information

00:41:29.020 --> 00:41:31.875
are more important
today, where we

00:41:31.875 --> 00:41:33.500
are today in the
digital era, than they

00:41:33.500 --> 00:41:36.590
were 20 or 30 years ago when
they pretty well the only news

00:41:36.590 --> 00:41:38.200
outlets.

00:41:38.200 --> 00:41:40.120
And I think so far
they've been very

00:41:40.120 --> 00:41:44.170
poor at promoting themselves
in the digital universe.

00:41:44.170 --> 00:41:48.490
And you may say that search
has a part to play in this too.

00:41:48.490 --> 00:41:51.650
And that is a massive
challenge to your organization.

00:41:51.650 --> 00:41:58.280
How do you have a
legitimate source of search

00:41:58.280 --> 00:41:59.634
that has veracity?

00:41:59.634 --> 00:42:02.050
PETER BARRON: But we would say
that the diversity of views

00:42:02.050 --> 00:42:04.990
that are now available compared
with 20 or 30 years ago

00:42:04.990 --> 00:42:08.260
when you might have had one or
two or three sources of news

00:42:08.260 --> 00:42:09.520
are infinitely wider.

00:42:09.520 --> 00:42:11.600
But of course the danger
of the filter bubble

00:42:11.600 --> 00:42:12.850
is real in some circumstances.

00:42:12.850 --> 00:42:13.240
PETER BAZALGETTE: Exactly.

00:42:13.240 --> 00:42:14.490
So we've got some challenges.

00:42:14.490 --> 00:42:17.350
So those would be the two
points I'd make anyway.

00:42:17.350 --> 00:42:20.720
One, don't forget that there
has to be a sense of fairness.

00:42:20.720 --> 00:42:23.140
And two, we need to
think very carefully

00:42:23.140 --> 00:42:24.860
about how we do report.

00:42:24.860 --> 00:42:29.860
And nobody is saying
you can't report

00:42:29.860 --> 00:42:33.560
a speech of Nigel Farage's that
some people may find extreme.

00:42:33.560 --> 00:42:36.760
We're simply saying, can you
go to a pretty prominent source

00:42:36.760 --> 00:42:39.970
that analyzes the claims and
says, are they true or not,

00:42:39.970 --> 00:42:40.930
without fear or favor.

00:42:40.930 --> 00:42:42.415
That's terribly important.

00:42:42.415 --> 00:42:44.630
SPEAKER: I think we have
time for two more questions.

00:42:44.630 --> 00:42:45.800
There's one back here.

00:42:49.840 --> 00:42:52.840
AUDIENCE: What about
the feeling of guilt

00:42:52.840 --> 00:42:56.740
if we have the empathy
switch turned off,

00:42:56.740 --> 00:42:59.980
but we would kind
of like to be on,

00:42:59.980 --> 00:43:01.712
how to deal with that feeling?

00:43:01.712 --> 00:43:03.670
It's something that I
personally struggle with.

00:43:03.670 --> 00:43:06.460
I'm sure other people as well.

00:43:06.460 --> 00:43:09.130
So what's your opinion on that?

00:43:12.010 --> 00:43:13.760
PETER BAZALGETTE: Can
you, not necessarily

00:43:13.760 --> 00:43:15.176
about your own
experience, can you

00:43:15.176 --> 00:43:16.970
give an example,
just one example?

00:43:16.970 --> 00:43:17.996
Make one up if you like.

00:43:17.996 --> 00:43:19.370
You don't have to
talk about what

00:43:19.370 --> 00:43:21.036
happens to you in
your own personal life

00:43:21.036 --> 00:43:23.900
necessarily, but give an
example of a situation

00:43:23.900 --> 00:43:29.260
where you're conscious of
you're not being very empathetic

00:43:29.260 --> 00:43:30.740
and you're a bit
guilty about it.

00:43:30.740 --> 00:43:35.260
AUDIENCE: Yeah, an example,
a friend ends up at hospital,

00:43:35.260 --> 00:43:40.370
and you would really want
to visit him or do something

00:43:40.370 --> 00:43:45.891
for him or her, but
you just don't feel it

00:43:45.891 --> 00:43:47.140
and you would like to feel it.

00:43:55.170 --> 00:43:57.300
PETER BAZALGETTE: You
could spend all your time

00:43:57.300 --> 00:44:00.196
visiting people in hospital.

00:44:00.196 --> 00:44:01.570
And the point I
made earlier, you

00:44:01.570 --> 00:44:03.100
wouldn't be earning a living.

00:44:03.100 --> 00:44:03.910
You'd be starving.

00:44:03.910 --> 00:44:06.040
Your family would be starving.

00:44:06.040 --> 00:44:08.110
You'd be exhausted.

00:44:08.110 --> 00:44:10.900
And you'd end up not being
very sympathetic to the people

00:44:10.900 --> 00:44:12.670
you're confronted with.

00:44:12.670 --> 00:44:14.200
So where is the balance?

00:44:14.200 --> 00:44:16.860
You need a balance, don't you?

00:44:16.860 --> 00:44:21.190
But in terms of
friends in hospital,

00:44:21.190 --> 00:44:26.830
I'd suggest to you if
they're a really good friend,

00:44:26.830 --> 00:44:28.990
you would visit then.

00:44:28.990 --> 00:44:30.950
You would make time.

00:44:30.950 --> 00:44:32.830
Do you know something,
the very fact

00:44:32.830 --> 00:44:37.314
that you pose the question to
yourself hugely encourages me.

00:44:37.314 --> 00:44:39.230
And I think the question
you've asked yourself

00:44:39.230 --> 00:44:41.506
is the answer to your question.

00:44:41.506 --> 00:44:42.880
PETER BARRON: Do
the right thing.

00:44:42.880 --> 00:44:43.838
PETER BAZALGETTE: Yeah.

00:44:43.838 --> 00:44:45.740
But the fact you've
asked the question--

00:44:45.740 --> 00:44:46.120
PETER BARRON: It
means you know--

00:44:46.120 --> 00:44:47.550
PETER BAZALGETTE: --is
fantastically encouraging--

00:44:47.550 --> 00:44:47.950
PETER BARRON: --you
know [INAUDIBLE].

00:44:47.950 --> 00:44:50.366
PETER BAZALGETTE: --because
you're conscious of the issue.

00:44:50.366 --> 00:44:51.270
That's fantastic.

00:45:02.854 --> 00:45:05.020
AUDIENCE: So you talk about
kind of the power of one

00:45:05.020 --> 00:45:09.710
to one, empathy, and how arts
are really useful for this.

00:45:09.710 --> 00:45:12.880
And we've had a lot of changes
with sort of the advent of VR

00:45:12.880 --> 00:45:15.340
and the ability to place
yourself with someone

00:45:15.340 --> 00:45:17.436
without having to
physically be with them,

00:45:17.436 --> 00:45:19.310
and sort of the scale
that that can generate.

00:45:19.310 --> 00:45:21.643
So what's your view on some
of these sort of VR stories,

00:45:21.643 --> 00:45:24.490
like the UN Syrian story
and the power that that

00:45:24.490 --> 00:45:26.440
has to sort of spread empathy?

00:45:26.440 --> 00:45:28.570
PETER BAZALGETTE: Yes,
I've written, briefly,

00:45:28.570 --> 00:45:30.880
about virtual reality
in the book actually,

00:45:30.880 --> 00:45:36.200
because it is quite interesting.

00:45:36.200 --> 00:45:40.600
If you take the terrible
conditions in that refugee

00:45:40.600 --> 00:45:45.160
camp in Calais, the two
dimensional film of it

00:45:45.160 --> 00:45:48.640
repeated again and
again on the news

00:45:48.640 --> 00:45:50.640
pretty well gets you to
switch your empathy off,

00:45:50.640 --> 00:45:52.690
as we were discussing earlier.

00:45:52.690 --> 00:45:56.080
But if you can put a headset on
and you can explore it and take

00:45:56.080 --> 00:45:59.650
decisions to turn
left or right and see

00:45:59.650 --> 00:46:03.170
what you see and
interact with it,

00:46:03.170 --> 00:46:06.490
which is perhaps the crucial
word, it is quite powerful.

00:46:06.490 --> 00:46:10.930
So there are people
doing work with the UN

00:46:10.930 --> 00:46:14.140
at the moment who are
very interested in how

00:46:14.140 --> 00:46:18.490
VR may be a very empathetic
medium, and a more empathetic

00:46:18.490 --> 00:46:20.082
medium than we've met before.

00:46:20.082 --> 00:46:21.790
And there's even a
marketing organization

00:46:21.790 --> 00:46:25.660
in New York that is
attempting to use that quality

00:46:25.660 --> 00:46:31.450
to get us to connect more with
brand and commercial messages.

00:46:31.450 --> 00:46:33.730
So it's a really interesting
question you asked.

00:46:33.730 --> 00:46:34.960
And people are looking at it.

00:46:34.960 --> 00:46:38.457
It's potentially very powerful.

00:46:38.457 --> 00:46:40.415
PETER BARRON: Great,
thank you very much indeed

00:46:40.415 --> 00:46:42.164
Peter, wonderful stuff,
fascinating stuff.

00:46:42.164 --> 00:46:44.997
[APPLAUSE]

00:46:44.997 --> 00:46:47.330
PETER BAZALGETTE: Can I just
thank everybody for coming?

00:46:47.330 --> 00:46:49.913
I mean, it's lovely to see some
many people and lovely to have

00:46:49.913 --> 00:46:52.370
such fantastic questions and
so engaging with the subject.

00:46:52.370 --> 00:46:53.360
So thank you very much indeed.

00:46:53.360 --> 00:46:54.530
I really appreciate it.

00:46:54.530 --> 00:46:54.850
PETER BARRON: Thank you.

00:46:54.850 --> 00:46:56.990
And I think you're able to
hang on for a little bit

00:46:56.990 --> 00:46:57.480
to sign some books.

00:46:57.480 --> 00:46:58.190
PETER BAZALGETTE: Yeah, sure.

00:46:58.190 --> 00:46:59.030
PETER BARRON: So if
you've got a book,

00:46:59.030 --> 00:47:00.238
please form an orderly queue.

00:47:00.238 --> 00:47:02.230
Thank you very much indeed.

