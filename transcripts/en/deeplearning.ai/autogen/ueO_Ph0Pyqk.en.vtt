WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.000
 
in the last video you learn about the

00:00:02.000 --> 00:00:02.010
in the last video you learn about the
 

00:00:02.010 --> 00:00:03.530
in the last video you learn about the
softmax there in the softmax activation

00:00:03.530 --> 00:00:03.540
softmax there in the softmax activation
 

00:00:03.540 --> 00:00:06.110
softmax there in the softmax activation
function in this video you deepen your

00:00:06.110 --> 00:00:06.120
function in this video you deepen your
 

00:00:06.120 --> 00:00:08.240
function in this video you deepen your
understanding of softmax classification

00:00:08.240 --> 00:00:08.250
understanding of softmax classification
 

00:00:08.250 --> 00:00:10.580
understanding of softmax classification
and also learn how to train a model that

00:00:10.580 --> 00:00:10.590
and also learn how to train a model that
 

00:00:10.590 --> 00:00:12.490
and also learn how to train a model that
uses a soft mask layer

00:00:12.490 --> 00:00:12.500
uses a soft mask layer
 

00:00:12.500 --> 00:00:15.350
uses a soft mask layer
recall our earlier example where the

00:00:15.350 --> 00:00:15.360
recall our earlier example where the
 

00:00:15.360 --> 00:00:18.620
recall our earlier example where the
open layer computes 0 as follows so

00:00:18.620 --> 00:00:18.630
open layer computes 0 as follows so
 

00:00:18.630 --> 00:00:20.960
open layer computes 0 as follows so
there are four classes sequels for then

00:00:20.960 --> 00:00:20.970
there are four classes sequels for then
 

00:00:20.970 --> 00:00:22.820
there are four classes sequels for then
zeros can be 4 by 1 dimensional vector

00:00:22.820 --> 00:00:22.830
zeros can be 4 by 1 dimensional vector
 

00:00:22.830 --> 00:00:25.640
zeros can be 4 by 1 dimensional vector
and we said we compute T which is this

00:00:25.640 --> 00:00:25.650
and we said we compute T which is this
 

00:00:25.650 --> 00:00:28.070
and we said we compute T which is this
temporary variable that performs element

00:00:28.070 --> 00:00:28.080
temporary variable that performs element
 

00:00:28.080 --> 00:00:31.160
temporary variable that performs element
wise exponentiation and then finally if

00:00:31.160 --> 00:00:31.170
wise exponentiation and then finally if
 

00:00:31.170 --> 00:00:33.410
wise exponentiation and then finally if
the activation function for your output

00:00:33.410 --> 00:00:33.420
the activation function for your output
 

00:00:33.420 --> 00:00:36.500
the activation function for your output
layer G of L is the softmax activation

00:00:36.500 --> 00:00:36.510
layer G of L is the softmax activation
 

00:00:36.510 --> 00:00:39.560
layer G of L is the softmax activation
function then the output will be this

00:00:39.560 --> 00:00:39.570
function then the output will be this
 

00:00:39.570 --> 00:00:42.020
function then the output will be this
basically taking the temporary variable

00:00:42.020 --> 00:00:42.030
basically taking the temporary variable
 

00:00:42.030 --> 00:00:45.889
basically taking the temporary variable
T and normalizing it to sum to 1 so this

00:00:45.889 --> 00:00:45.899
T and normalizing it to sum to 1 so this
 

00:00:45.899 --> 00:00:49.970
T and normalizing it to sum to 1 so this
then becomes a of L so you notice that

00:00:49.970 --> 00:00:49.980
then becomes a of L so you notice that
 

00:00:49.980 --> 00:00:52.220
then becomes a of L so you notice that
in the Z vector the biggest element was

00:00:52.220 --> 00:00:52.230
in the Z vector the biggest element was
 

00:00:52.230 --> 00:00:55.279
in the Z vector the biggest element was
5 and the biggest probability ends up

00:00:55.279 --> 00:00:55.289
5 and the biggest probability ends up
 

00:00:55.289 --> 00:00:57.860
5 and the biggest probability ends up
being dispersed probability the name

00:00:57.860 --> 00:00:57.870
being dispersed probability the name
 

00:00:57.870 --> 00:01:00.830
being dispersed probability the name
soft mass comes from contrasting it to

00:01:00.830 --> 00:01:00.840
soft mass comes from contrasting it to
 

00:01:00.840 --> 00:01:04.039
soft mass comes from contrasting it to
what's called a hard max which would

00:01:04.039 --> 00:01:04.049
what's called a hard max which would
 

00:01:04.049 --> 00:01:06.679
what's called a hard max which would
have taken the vector Z and mapped it to

00:01:06.679 --> 00:01:06.689
have taken the vector Z and mapped it to
 

00:01:06.689 --> 00:01:10.280
have taken the vector Z and mapped it to
this vector so hard max function will

00:01:10.280 --> 00:01:10.290
this vector so hard max function will
 

00:01:10.290 --> 00:01:12.230
this vector so hard max function will
look at the elements of Z and just put a

00:01:12.230 --> 00:01:12.240
look at the elements of Z and just put a
 

00:01:12.240 --> 00:01:14.480
look at the elements of Z and just put a
1 in the position of the biggest

00:01:14.480 --> 00:01:14.490
1 in the position of the biggest
 

00:01:14.490 --> 00:01:17.330
1 in the position of the biggest
elements of Z and then zeros everywhere

00:01:17.330 --> 00:01:17.340
elements of Z and then zeros everywhere
 

00:01:17.340 --> 00:01:20.600
elements of Z and then zeros everywhere
else and so there is a very hard max

00:01:20.600 --> 00:01:20.610
else and so there is a very hard max
 

00:01:20.610 --> 00:01:23.030
else and so there is a very hard max
where the bigger element gets a output

00:01:23.030 --> 00:01:23.040
where the bigger element gets a output
 

00:01:23.040 --> 00:01:24.740
where the bigger element gets a output
of 1 and everything else gives an output

00:01:24.740 --> 00:01:24.750
of 1 and everything else gives an output
 

00:01:24.750 --> 00:01:27.830
of 1 and everything else gives an output
of 0 whereas in contrast the soft max is

00:01:27.830 --> 00:01:27.840
of 0 whereas in contrast the soft max is
 

00:01:27.840 --> 00:01:31.880
of 0 whereas in contrast the soft max is
a more gentle mapping from Z to these

00:01:31.880 --> 00:01:31.890
a more gentle mapping from Z to these
 

00:01:31.890 --> 00:01:34.490
a more gentle mapping from Z to these
probabilities so I'm not sure if this is

00:01:34.490 --> 00:01:34.500
probabilities so I'm not sure if this is
 

00:01:34.500 --> 00:01:36.560
probabilities so I'm not sure if this is
a great name but that Lisa does the

00:01:36.560 --> 00:01:36.570
a great name but that Lisa does the
 

00:01:36.570 --> 00:01:38.780
a great name but that Lisa does the
intuition behind why we call it a soft

00:01:38.780 --> 00:01:38.790
intuition behind why we call it a soft
 

00:01:38.790 --> 00:01:41.960
intuition behind why we call it a soft
max always in contrast to the hard

00:01:41.960 --> 00:01:41.970
max always in contrast to the hard
 

00:01:41.970 --> 00:01:44.630
max always in contrast to the hard
max and one thing I did really show but

00:01:44.630 --> 00:01:44.640
max and one thing I did really show but
 

00:01:44.640 --> 00:01:47.749
max and one thing I did really show but
as alluded to is that softmax regression

00:01:47.749 --> 00:01:47.759
as alluded to is that softmax regression
 

00:01:47.759 --> 00:01:49.810
as alluded to is that softmax regression
or the softmax activation function

00:01:49.810 --> 00:01:49.820
or the softmax activation function
 

00:01:49.820 --> 00:01:52.160
or the softmax activation function
generalizes the logistics activation

00:01:52.160 --> 00:01:52.170
generalizes the logistics activation
 

00:01:52.170 --> 00:01:54.560
generalizes the logistics activation
function to see constants rather than

00:01:54.560 --> 00:01:54.570
function to see constants rather than
 

00:01:54.570 --> 00:01:57.170
function to see constants rather than
just two courses and it turns out that

00:01:57.170 --> 00:01:57.180
just two courses and it turns out that
 

00:01:57.180 --> 00:02:02.569
just two courses and it turns out that
if C is equal to 2 then soft max with C

00:02:02.569 --> 00:02:02.579
if C is equal to 2 then soft max with C
 

00:02:02.579 --> 00:02:05.899
if C is equal to 2 then soft max with C
equals to 2 essentially reduces to

00:02:05.899 --> 00:02:05.909
equals to 2 essentially reduces to
 

00:02:05.909 --> 00:02:08.570
equals to 2 essentially reduces to
logistic regression and I'm not

00:02:08.570 --> 00:02:08.580
logistic regression and I'm not
 

00:02:08.580 --> 00:02:11.210
logistic regression and I'm not
prove this in this video but the rough

00:02:11.210 --> 00:02:11.220
prove this in this video but the rough
 

00:02:11.220 --> 00:02:13.280
prove this in this video but the rough
outline for the prove is that is C

00:02:13.280 --> 00:02:13.290
outline for the prove is that is C
 

00:02:13.290 --> 00:02:16.420
outline for the prove is that is C
equals to two and if you apply softmax

00:02:16.420 --> 00:02:16.430
equals to two and if you apply softmax
 

00:02:16.430 --> 00:02:21.230
equals to two and if you apply softmax
then the output layer al well I'll put

00:02:21.230 --> 00:02:21.240
then the output layer al well I'll put
 

00:02:21.240 --> 00:02:23.630
then the output layer al well I'll put
two numbers the C equals two so maybe it

00:02:23.630 --> 00:02:23.640
two numbers the C equals two so maybe it
 

00:02:23.640 --> 00:02:29.450
two numbers the C equals two so maybe it
outputs 0.842 and 0.158 right these two

00:02:29.450 --> 00:02:29.460
outputs 0.842 and 0.158 right these two
 

00:02:29.460 --> 00:02:31.100
outputs 0.842 and 0.158 right these two
numbers always happens on to one and

00:02:31.100 --> 00:02:31.110
numbers always happens on to one and
 

00:02:31.110 --> 00:02:32.900
numbers always happens on to one and
because these two numbers always at the

00:02:32.900 --> 00:02:32.910
because these two numbers always at the
 

00:02:32.910 --> 00:02:34.400
because these two numbers always at the
center one they're actually redundant

00:02:34.400 --> 00:02:34.410
center one they're actually redundant
 

00:02:34.410 --> 00:02:35.750
center one they're actually redundant
and maybe don't need to bother to

00:02:35.750 --> 00:02:35.760
and maybe don't need to bother to
 

00:02:35.760 --> 00:02:37.130
and maybe don't need to bother to
compute two of them maybe we just need

00:02:37.130 --> 00:02:37.140
compute two of them maybe we just need
 

00:02:37.140 --> 00:02:39.740
compute two of them maybe we just need
to compute one of them and it turns out

00:02:39.740 --> 00:02:39.750
to compute one of them and it turns out
 

00:02:39.750 --> 00:02:41.360
to compute one of them and it turns out
that the way you end up computing that

00:02:41.360 --> 00:02:41.370
that the way you end up computing that
 

00:02:41.370 --> 00:02:45.620
that the way you end up computing that
number reduces to the way that logistic

00:02:45.620 --> 00:02:45.630
number reduces to the way that logistic
 

00:02:45.630 --> 00:02:48.140
number reduces to the way that logistic
regression is computing this single

00:02:48.140 --> 00:02:48.150
regression is computing this single
 

00:02:48.150 --> 00:02:50.060
regression is computing this single
whole point so that wasn't much of a

00:02:50.060 --> 00:02:50.070
whole point so that wasn't much of a
 

00:02:50.070 --> 00:02:52.970
whole point so that wasn't much of a
proof but the takeaway from this is that

00:02:52.970 --> 00:02:52.980
proof but the takeaway from this is that
 

00:02:52.980 --> 00:02:55.610
proof but the takeaway from this is that
top X regression is a generalization of

00:02:55.610 --> 00:02:55.620
top X regression is a generalization of
 

00:02:55.620 --> 00:02:57.560
top X regression is a generalization of
logistic regression to more than two

00:02:57.560 --> 00:02:57.570
logistic regression to more than two
 

00:02:57.570 --> 00:02:59.840
logistic regression to more than two
classes now let's look at how you would

00:02:59.840 --> 00:02:59.850
classes now let's look at how you would
 

00:02:59.850 --> 00:03:02.390
classes now let's look at how you would
actually train a neural network with a

00:03:02.390 --> 00:03:02.400
actually train a neural network with a
 

00:03:02.400 --> 00:03:04.340
actually train a neural network with a
soft max output layer so in particular

00:03:04.340 --> 00:03:04.350
soft max output layer so in particular
 

00:03:04.350 --> 00:03:06.860
soft max output layer so in particular
let's define the loss function you use

00:03:06.860 --> 00:03:06.870
let's define the loss function you use
 

00:03:06.870 --> 00:03:08.720
let's define the loss function you use
to Train in your network let's take an

00:03:08.720 --> 00:03:08.730
to Train in your network let's take an
 

00:03:08.730 --> 00:03:10.340
to Train in your network let's take an
example let's say you have an example in

00:03:10.340 --> 00:03:10.350
example let's say you have an example in
 

00:03:10.350 --> 00:03:13.000
example let's say you have an example in
your training sets where the output is

00:03:13.000 --> 00:03:13.010
your training sets where the output is
 

00:03:13.010 --> 00:03:15.949
your training sets where the output is
where the target output the grouchy

00:03:15.949 --> 00:03:15.959
where the target output the grouchy
 

00:03:15.959 --> 00:03:18.530
where the target output the grouchy
label is zero one zero zero so the

00:03:18.530 --> 00:03:18.540
label is zero one zero zero so the
 

00:03:18.540 --> 00:03:20.930
label is zero one zero zero so the
example from the previous video this

00:03:20.930 --> 00:03:20.940
example from the previous video this
 

00:03:20.940 --> 00:03:22.759
example from the previous video this
means that this is an image of a cat

00:03:22.759 --> 00:03:22.769
means that this is an image of a cat
 

00:03:22.769 --> 00:03:26.030
means that this is an image of a cat
because it falls into plus one and now

00:03:26.030 --> 00:03:26.040
because it falls into plus one and now
 

00:03:26.040 --> 00:03:27.680
because it falls into plus one and now
let's say that your new network is

00:03:27.680 --> 00:03:27.690
let's say that your new network is
 

00:03:27.690 --> 00:03:32.270
let's say that your new network is
currently outputting y hat equals so Y

00:03:32.270 --> 00:03:32.280
currently outputting y hat equals so Y
 

00:03:32.280 --> 00:03:33.320
currently outputting y hat equals so Y
hat will be a vector of probabilities

00:03:33.320 --> 00:03:33.330
hat will be a vector of probabilities
 

00:03:33.330 --> 00:03:37.190
hat will be a vector of probabilities
goes sum to one or point one zero point

00:03:37.190 --> 00:03:37.200
goes sum to one or point one zero point
 

00:03:37.200 --> 00:03:39.470
goes sum to one or point one zero point
four so you can check that sum to 1 and

00:03:39.470 --> 00:03:39.480
four so you can check that sum to 1 and
 

00:03:39.480 --> 00:03:43.610
four so you can check that sum to 1 and
this is going to be K L so the new net

00:03:43.610 --> 00:03:43.620
this is going to be K L so the new net
 

00:03:43.620 --> 00:03:45.500
this is going to be K L so the new net
was not doing very well in this example

00:03:45.500 --> 00:03:45.510
was not doing very well in this example
 

00:03:45.510 --> 00:03:47.390
was not doing very well in this example
because there's actually cat to miss and

00:03:47.390 --> 00:03:47.400
because there's actually cat to miss and
 

00:03:47.400 --> 00:03:49.699
because there's actually cat to miss and
only a 20% chance that this is a cat so

00:03:49.699 --> 00:03:49.709
only a 20% chance that this is a cat so
 

00:03:49.709 --> 00:03:52.310
only a 20% chance that this is a cat so
didn't do very well in this example so

00:03:52.310 --> 00:03:52.320
didn't do very well in this example so
 

00:03:52.320 --> 00:03:54.770
didn't do very well in this example so
what's the last function you want to use

00:03:54.770 --> 00:03:54.780
what's the last function you want to use
 

00:03:54.780 --> 00:03:57.350
what's the last function you want to use
to train this new network in softmax

00:03:57.350 --> 00:03:57.360
to train this new network in softmax
 

00:03:57.360 --> 00:03:59.120
to train this new network in softmax
qualification the last we typically use

00:03:59.120 --> 00:03:59.130
qualification the last we typically use
 

00:03:59.130 --> 00:04:03.110
qualification the last we typically use
is an extra sum of J equals 1 to 4 and

00:04:03.110 --> 00:04:03.120
is an extra sum of J equals 1 to 4 and
 

00:04:03.120 --> 00:04:06.680
is an extra sum of J equals 1 to 4 and
it's really sum from 1 for C in the

00:04:06.680 --> 00:04:06.690
it's really sum from 1 for C in the
 

00:04:06.690 --> 00:04:08.720
it's really sum from 1 for C in the
general case and I just use for here of

00:04:08.720 --> 00:04:08.730
general case and I just use for here of
 

00:04:08.730 --> 00:04:15.020
general case and I just use for here of
Y log or YJ log Y hat ok so let's look

00:04:15.020 --> 00:04:15.030
Y log or YJ log Y hat ok so let's look
 

00:04:15.030 --> 00:04:18.349
Y log or YJ log Y hat ok so let's look
at our single example above to better

00:04:18.349 --> 00:04:18.359
at our single example above to better
 

00:04:18.359 --> 00:04:20.020
at our single example above to better
understand what happens

00:04:20.020 --> 00:04:20.030
understand what happens
 

00:04:20.030 --> 00:04:24.070
understand what happens
notice that in this example y 1 equals y

00:04:24.070 --> 00:04:24.080
notice that in this example y 1 equals y
 

00:04:24.080 --> 00:04:28.290
notice that in this example y 1 equals y
3 equals y 4 equals 0 because those are

00:04:28.290 --> 00:04:28.300
3 equals y 4 equals 0 because those are
 

00:04:28.300 --> 00:04:33.280
3 equals y 4 equals 0 because those are
zeros and only y 2 is equal to 1 so if

00:04:33.280 --> 00:04:33.290
zeros and only y 2 is equal to 1 so if
 

00:04:33.290 --> 00:04:35.680
zeros and only y 2 is equal to 1 so if
you look at this summation all the terms

00:04:35.680 --> 00:04:35.690
you look at this summation all the terms
 

00:04:35.690 --> 00:04:39.850
you look at this summation all the terms
with 0 values of YJ u equal to 0 and the

00:04:39.850 --> 00:04:39.860
with 0 values of YJ u equal to 0 and the
 

00:04:39.860 --> 00:04:42.970
with 0 values of YJ u equal to 0 and the
only term you're left with is negative Y

00:04:42.970 --> 00:04:42.980
only term you're left with is negative Y
 

00:04:42.980 --> 00:04:47.890
only term you're left with is negative Y
2 log Y hat 2 because when you sum over

00:04:47.890 --> 00:04:47.900
2 log Y hat 2 because when you sum over
 

00:04:47.900 --> 00:04:50.230
2 log Y hat 2 because when you sum over
the indices of J all the terms will end

00:04:50.230 --> 00:04:50.240
the indices of J all the terms will end
 

00:04:50.240 --> 00:04:52.510
the indices of J all the terms will end
up 0 except when J is equal to 2 and

00:04:52.510 --> 00:04:52.520
up 0 except when J is equal to 2 and
 

00:04:52.520 --> 00:04:54.940
up 0 except when J is equal to 2 and
because y 2 is equal to 1 this is just

00:04:54.940 --> 00:04:54.950
because y 2 is equal to 1 this is just
 

00:04:54.950 --> 00:04:59.170
because y 2 is equal to 1 this is just
negative log Y hat - so what this means

00:04:59.170 --> 00:04:59.180
negative log Y hat - so what this means
 

00:04:59.180 --> 00:05:01.659
negative log Y hat - so what this means
is that if your learning algorithm is

00:05:01.659 --> 00:05:01.669
is that if your learning algorithm is
 

00:05:01.669 --> 00:05:04.750
is that if your learning algorithm is
trying to make this small because use

00:05:04.750 --> 00:05:04.760
trying to make this small because use
 

00:05:04.760 --> 00:05:06.250
trying to make this small because use
gradient descent to you know try to

00:05:06.250 --> 00:05:06.260
gradient descent to you know try to
 

00:05:06.260 --> 00:05:07.960
gradient descent to you know try to
reduce the loss on your training set

00:05:07.960 --> 00:05:07.970
reduce the loss on your training set
 

00:05:07.970 --> 00:05:10.810
reduce the loss on your training set
then the only way to make this small is

00:05:10.810 --> 00:05:10.820
then the only way to make this small is
 

00:05:10.820 --> 00:05:13.000
then the only way to make this small is
to make this small and the only way to

00:05:13.000 --> 00:05:13.010
to make this small and the only way to
 

00:05:13.010 --> 00:05:16.750
to make this small and the only way to
do that is to make Y hat to as big as

00:05:16.750 --> 00:05:16.760
do that is to make Y hat to as big as
 

00:05:16.760 --> 00:05:19.930
do that is to make Y hat to as big as
possible and these are probabilities so

00:05:19.930 --> 00:05:19.940
possible and these are probabilities so
 

00:05:19.940 --> 00:05:22.120
possible and these are probabilities so
it can never be bigger than 1 but this

00:05:22.120 --> 00:05:22.130
it can never be bigger than 1 but this
 

00:05:22.130 --> 00:05:25.300
it can never be bigger than 1 but this
kind of makes sense because if X for

00:05:25.300 --> 00:05:25.310
kind of makes sense because if X for
 

00:05:25.310 --> 00:05:27.430
kind of makes sense because if X for
this example is a picture of a cat then

00:05:27.430 --> 00:05:27.440
this example is a picture of a cat then
 

00:05:27.440 --> 00:05:29.500
this example is a picture of a cat then
you want to that output probability to

00:05:29.500 --> 00:05:29.510
you want to that output probability to
 

00:05:29.510 --> 00:05:30.940
you want to that output probability to
be as big as possible

00:05:30.940 --> 00:05:30.950
be as big as possible
 

00:05:30.950 --> 00:05:32.980
be as big as possible
so more generally what does loss

00:05:32.980 --> 00:05:32.990
so more generally what does loss
 

00:05:32.990 --> 00:05:35.020
so more generally what does loss
function does is it looks at whatever is

00:05:35.020 --> 00:05:35.030
function does is it looks at whatever is
 

00:05:35.030 --> 00:05:36.820
function does is it looks at whatever is
the ground truth class in your training

00:05:36.820 --> 00:05:36.830
the ground truth class in your training
 

00:05:36.830 --> 00:05:38.710
the ground truth class in your training
set and it tries to meet the

00:05:38.710 --> 00:05:38.720
set and it tries to meet the
 

00:05:38.720 --> 00:05:41.050
set and it tries to meet the
corresponding probability of that Clause

00:05:41.050 --> 00:05:41.060
corresponding probability of that Clause
 

00:05:41.060 --> 00:05:43.000
corresponding probability of that Clause
as high as possible if you're familiar

00:05:43.000 --> 00:05:43.010
as high as possible if you're familiar
 

00:05:43.010 --> 00:05:44.980
as high as possible if you're familiar
with maximum likelihood estimation

00:05:44.980 --> 00:05:44.990
with maximum likelihood estimation
 

00:05:44.990 --> 00:05:47.290
with maximum likelihood estimation
statistics this turns out to be a form

00:05:47.290 --> 00:05:47.300
statistics this turns out to be a form
 

00:05:47.300 --> 00:05:49.420
statistics this turns out to be a form
with maximum likely estimation but if

00:05:49.420 --> 00:05:49.430
with maximum likely estimation but if
 

00:05:49.430 --> 00:05:50.469
with maximum likely estimation but if
you don't know what that means don't

00:05:50.469 --> 00:05:50.479
you don't know what that means don't
 

00:05:50.479 --> 00:05:52.690
you don't know what that means don't
worry about it the intuition we just

00:05:52.690 --> 00:05:52.700
worry about it the intuition we just
 

00:05:52.700 --> 00:05:55.360
worry about it the intuition we just
talked about will suffice now this is

00:05:55.360 --> 00:05:55.370
talked about will suffice now this is
 

00:05:55.370 --> 00:05:56.890
talked about will suffice now this is
the loss on a single training example

00:05:56.890 --> 00:05:56.900
the loss on a single training example
 

00:05:56.900 --> 00:06:00.610
the loss on a single training example
how about the cost J on the entire

00:06:00.610 --> 00:06:00.620
how about the cost J on the entire
 

00:06:00.620 --> 00:06:03.130
how about the cost J on the entire
training set so the cost of the setting

00:06:03.130 --> 00:06:03.140
training set so the cost of the setting
 

00:06:03.140 --> 00:06:06.100
training set so the cost of the setting
the parameters you know and so on of all

00:06:06.100 --> 00:06:06.110
the parameters you know and so on of all
 

00:06:06.110 --> 00:06:08.800
the parameters you know and so on of all
the ways and biases you define that as

00:06:08.800 --> 00:06:08.810
the ways and biases you define that as
 

00:06:08.810 --> 00:06:10.990
the ways and biases you define that as
pretty much what you guess some will be

00:06:10.990 --> 00:06:11.000
pretty much what you guess some will be
 

00:06:11.000 --> 00:06:13.870
pretty much what you guess some will be
on Thai training set of the loss your

00:06:13.870 --> 00:06:13.880
on Thai training set of the loss your
 

00:06:13.880 --> 00:06:16.390
on Thai training set of the loss your
learning algorithms predictions summed

00:06:16.390 --> 00:06:16.400
learning algorithms predictions summed
 

00:06:16.400 --> 00:06:19.240
learning algorithms predictions summed
over your training examples and so what

00:06:19.240 --> 00:06:19.250
over your training examples and so what
 

00:06:19.250 --> 00:06:21.520
over your training examples and so what
you do is use gradient descent in order

00:06:21.520 --> 00:06:21.530
you do is use gradient descent in order
 

00:06:21.530 --> 00:06:24.490
you do is use gradient descent in order
to try to minimize this cost finally one

00:06:24.490 --> 00:06:24.500
to try to minimize this cost finally one
 

00:06:24.500 --> 00:06:26.680
to try to minimize this cost finally one
more implementational detail notice is

00:06:26.680 --> 00:06:26.690
more implementational detail notice is
 

00:06:26.690 --> 00:06:28.930
more implementational detail notice is
that would because sequel is equal to 4

00:06:28.930 --> 00:06:28.940
that would because sequel is equal to 4
 

00:06:28.940 --> 00:06:32.620
that would because sequel is equal to 4
y is a 4 by 1 vector and Y hat is also a

00:06:32.620 --> 00:06:32.630
y is a 4 by 1 vector and Y hat is also a
 

00:06:32.630 --> 00:06:34.080
y is a 4 by 1 vector and Y hat is also a
4 by 1

00:06:34.080 --> 00:06:34.090
4 by 1
 

00:06:34.090 --> 00:06:35.700
4 by 1
so if you are using a vectorized

00:06:35.700 --> 00:06:35.710
so if you are using a vectorized
 

00:06:35.710 --> 00:06:38.070
so if you are using a vectorized
implementation the matrix capital y is

00:06:38.070 --> 00:06:38.080
implementation the matrix capital y is
 

00:06:38.080 --> 00:06:44.010
implementation the matrix capital y is
going to be y 1 y 2 to y em stacked

00:06:44.010 --> 00:06:44.020
going to be y 1 y 2 to y em stacked
 

00:06:44.020 --> 00:06:46.980
going to be y 1 y 2 to y em stacked
horizontally and so for example if this

00:06:46.980 --> 00:06:46.990
horizontally and so for example if this
 

00:06:46.990 --> 00:06:48.780
horizontally and so for example if this
example up here is your first training

00:06:48.780 --> 00:06:48.790
example up here is your first training
 

00:06:48.790 --> 00:06:50.970
example up here is your first training
example then the first column of this

00:06:50.970 --> 00:06:50.980
example then the first column of this
 

00:06:50.980 --> 00:06:54.720
example then the first column of this
matrix Y will be 0 1 0 0 and then your

00:06:54.720 --> 00:06:54.730
matrix Y will be 0 1 0 0 and then your
 

00:06:54.730 --> 00:06:57.210
matrix Y will be 0 1 0 0 and then your
second example in the second example is

00:06:57.210 --> 00:06:57.220
second example in the second example is
 

00:06:57.220 --> 00:07:00.120
second example in the second example is
a dog who the third example is a none of

00:07:00.120 --> 00:07:00.130
a dog who the third example is a none of
 

00:07:00.130 --> 00:07:03.540
a dog who the third example is a none of
the above and so on and then this matrix

00:07:03.540 --> 00:07:03.550
the above and so on and then this matrix
 

00:07:03.550 --> 00:07:06.690
the above and so on and then this matrix
capital y will end up being a 4 by M

00:07:06.690 --> 00:07:06.700
capital y will end up being a 4 by M
 

00:07:06.700 --> 00:07:09.660
capital y will end up being a 4 by M
dimensional matrix and similarly Y hat

00:07:09.660 --> 00:07:09.670
dimensional matrix and similarly Y hat
 

00:07:09.670 --> 00:07:13.310
dimensional matrix and similarly Y hat
will be y hat one stack up horizontally

00:07:13.310 --> 00:07:13.320
will be y hat one stack up horizontally
 

00:07:13.320 --> 00:07:16.860
will be y hat one stack up horizontally
going through Y hat M so this is

00:07:16.860 --> 00:07:16.870
going through Y hat M so this is
 

00:07:16.870 --> 00:07:20.220
going through Y hat M so this is
actually why I had one or the output on

00:07:20.220 --> 00:07:20.230
actually why I had one or the output on
 

00:07:20.230 --> 00:07:23.160
actually why I had one or the output on
the first training example then so I had

00:07:23.160 --> 00:07:23.170
the first training example then so I had
 

00:07:23.170 --> 00:07:28.500
the first training example then so I had
to read this open 3 0.2 0.1 0.4 and so

00:07:28.500 --> 00:07:28.510
to read this open 3 0.2 0.1 0.4 and so
 

00:07:28.510 --> 00:07:30.630
to read this open 3 0.2 0.1 0.4 and so
on and why hide yourself will also be

00:07:30.630 --> 00:07:30.640
on and why hide yourself will also be
 

00:07:30.640 --> 00:07:33.660
on and why hide yourself will also be
for my M dimensional matrix finally

00:07:33.660 --> 00:07:33.670
for my M dimensional matrix finally
 

00:07:33.670 --> 00:07:35.550
for my M dimensional matrix finally
let's take a look at how you implement

00:07:35.550 --> 00:07:35.560
let's take a look at how you implement
 

00:07:35.560 --> 00:07:37.350
let's take a look at how you implement
gradient descent when you have a soft

00:07:37.350 --> 00:07:37.360
gradient descent when you have a soft
 

00:07:37.360 --> 00:07:40.350
gradient descent when you have a soft
max output layer so this output layer

00:07:40.350 --> 00:07:40.360
max output layer so this output layer
 

00:07:40.360 --> 00:07:44.400
max output layer so this output layer
will compute Z L which is C by 1 run our

00:07:44.400 --> 00:07:44.410
will compute Z L which is C by 1 run our
 

00:07:44.410 --> 00:07:46.950
will compute Z L which is C by 1 run our
example 4 by 1 and then you apply the

00:07:46.950 --> 00:07:46.960
example 4 by 1 and then you apply the
 

00:07:46.960 --> 00:07:51.240
example 4 by 1 and then you apply the
softmax activation function to get a L

00:07:51.240 --> 00:07:51.250
softmax activation function to get a L
 

00:07:51.250 --> 00:07:55.530
softmax activation function to get a L
or Y hat and then that in turn allows

00:07:55.530 --> 00:07:55.540
or Y hat and then that in turn allows
 

00:07:55.540 --> 00:07:59.430
or Y hat and then that in turn allows
you to compute the loss so we've talked

00:07:59.430 --> 00:07:59.440
you to compute the loss so we've talked
 

00:07:59.440 --> 00:08:01.860
you to compute the loss so we've talked
about how to implement the forward

00:08:01.860 --> 00:08:01.870
about how to implement the forward
 

00:08:01.870 --> 00:08:04.320
about how to implement the forward
propagation step of the neural network

00:08:04.320 --> 00:08:04.330
propagation step of the neural network
 

00:08:04.330 --> 00:08:06.150
propagation step of the neural network
to get these outputs and to compute that

00:08:06.150 --> 00:08:06.160
to get these outputs and to compute that
 

00:08:06.160 --> 00:08:08.730
to get these outputs and to compute that
loss how about the back propagation step

00:08:08.730 --> 00:08:08.740
loss how about the back propagation step
 

00:08:08.740 --> 00:08:11.130
loss how about the back propagation step
or gradient descent turns out that the

00:08:11.130 --> 00:08:11.140
or gradient descent turns out that the
 

00:08:11.140 --> 00:08:13.050
or gradient descent turns out that the
key step of the queue equation you need

00:08:13.050 --> 00:08:13.060
key step of the queue equation you need
 

00:08:13.060 --> 00:08:14.790
key step of the queue equation you need
to initialize back prop is this

00:08:14.790 --> 00:08:14.800
to initialize back prop is this
 

00:08:14.800 --> 00:08:17.130
to initialize back prop is this
expression that the derivative with

00:08:17.130 --> 00:08:17.140
expression that the derivative with
 

00:08:17.140 --> 00:08:19.350
expression that the derivative with
respect to Z at the last layer this

00:08:19.350 --> 00:08:19.360
respect to Z at the last layer this
 

00:08:19.360 --> 00:08:22.260
respect to Z at the last layer this
turns out you can compute this Y hat the

00:08:22.260 --> 00:08:22.270
turns out you can compute this Y hat the
 

00:08:22.270 --> 00:08:26.250
turns out you can compute this Y hat the
4 by 1 vector minus y the 4.1 Vestas you

00:08:26.250 --> 00:08:26.260
4 by 1 vector minus y the 4.1 Vestas you
 

00:08:26.260 --> 00:08:28.530
4 by 1 vector minus y the 4.1 Vestas you
notice that all of these are going to be

00:08:28.530 --> 00:08:28.540
notice that all of these are going to be
 

00:08:28.540 --> 00:08:30.210
notice that all of these are going to be
four by one vectors when you have four

00:08:30.210 --> 00:08:30.220
four by one vectors when you have four
 

00:08:30.220 --> 00:08:32.610
four by one vectors when you have four
classes and si by one in a more general

00:08:32.610 --> 00:08:32.620
classes and si by one in a more general
 

00:08:32.620 --> 00:08:35.760
classes and si by one in a more general
case and so this screen by our usual

00:08:35.760 --> 00:08:35.770
case and so this screen by our usual
 

00:08:35.770 --> 00:08:37.740
case and so this screen by our usual
definition of what is DZ this is the

00:08:37.740 --> 00:08:37.750
definition of what is DZ this is the
 

00:08:37.750 --> 00:08:39.150
definition of what is DZ this is the
partial derivative of the cost function

00:08:39.150 --> 00:08:39.160
partial derivative of the cost function
 

00:08:39.160 --> 00:08:43.620
partial derivative of the cost function
with respect to ZL if you're an expert

00:08:43.620 --> 00:08:43.630
with respect to ZL if you're an expert
 

00:08:43.630 --> 00:08:46.910
with respect to ZL if you're an expert
in calculus you can derive this yourself

00:08:46.910 --> 00:08:46.920
in calculus you can derive this yourself
 

00:08:46.920 --> 00:08:49.130
in calculus you can derive this yourself
or if you explain calculus you can try

00:08:49.130 --> 00:08:49.140
or if you explain calculus you can try
 

00:08:49.140 --> 00:08:50.870
or if you explain calculus you can try
to divide this yourself that using this

00:08:50.870 --> 00:08:50.880
to divide this yourself that using this
 

00:08:50.880 --> 00:08:53.090
to divide this yourself that using this
formula will also just work fine if you

00:08:53.090 --> 00:08:53.100
formula will also just work fine if you
 

00:08:53.100 --> 00:08:54.139
formula will also just work fine if you
ever need to enter in this from scratch

00:08:54.139 --> 00:08:54.149
ever need to enter in this from scratch
 

00:08:54.149 --> 00:08:58.069
ever need to enter in this from scratch
both this you can then compute D ZL and

00:08:58.069 --> 00:08:58.079
both this you can then compute D ZL and
 

00:08:58.079 --> 00:08:59.630
both this you can then compute D ZL and
then sort of start off the background

00:08:59.630 --> 00:08:59.640
then sort of start off the background
 

00:08:59.640 --> 00:09:02.090
then sort of start off the background
process to compute all the derivatives

00:09:02.090 --> 00:09:02.100
process to compute all the derivatives
 

00:09:02.100 --> 00:09:04.400
process to compute all the derivatives
you need throughout your neural network

00:09:04.400 --> 00:09:04.410
you need throughout your neural network
 

00:09:04.410 --> 00:09:07.040
you need throughout your neural network
but it turns out that in this week's

00:09:07.040 --> 00:09:07.050
but it turns out that in this week's
 

00:09:07.050 --> 00:09:09.290
but it turns out that in this week's
programming exercise we'll start to use

00:09:09.290 --> 00:09:09.300
programming exercise we'll start to use
 

00:09:09.300 --> 00:09:10.730
programming exercise we'll start to use
one of the deep learning programming

00:09:10.730 --> 00:09:10.740
one of the deep learning programming
 

00:09:10.740 --> 00:09:12.380
one of the deep learning programming
frameworks and for those foreign

00:09:12.380 --> 00:09:12.390
frameworks and for those foreign
 

00:09:12.390 --> 00:09:14.600
frameworks and for those foreign
frameworks usually it turns out you just

00:09:14.600 --> 00:09:14.610
frameworks usually it turns out you just
 

00:09:14.610 --> 00:09:16.579
frameworks usually it turns out you just
need to focus on getting the for profit

00:09:16.579 --> 00:09:16.589
need to focus on getting the for profit
 

00:09:16.589 --> 00:09:19.069
need to focus on getting the for profit
right and so long as you specify the

00:09:19.069 --> 00:09:19.079
right and so long as you specify the
 

00:09:19.079 --> 00:09:21.470
right and so long as you specify the
program where the four top parts the

00:09:21.470 --> 00:09:21.480
program where the four top parts the
 

00:09:21.480 --> 00:09:24.019
program where the four top parts the
pruning framework will figure out how to

00:09:24.019 --> 00:09:24.029
pruning framework will figure out how to
 

00:09:24.029 --> 00:09:25.910
pruning framework will figure out how to
do back prop or how to do the backward

00:09:25.910 --> 00:09:25.920
do back prop or how to do the backward
 

00:09:25.920 --> 00:09:30.019
do back prop or how to do the backward
pass for you so this expression is worth

00:09:30.019 --> 00:09:30.029
pass for you so this expression is worth
 

00:09:30.029 --> 00:09:31.460
pass for you so this expression is worth
keep in mind for if you ever need to

00:09:31.460 --> 00:09:31.470
keep in mind for if you ever need to
 

00:09:31.470 --> 00:09:32.300
keep in mind for if you ever need to
implement

00:09:32.300 --> 00:09:32.310
implement
 

00:09:32.310 --> 00:09:34.190
implement
softmax regression or soft by

00:09:34.190 --> 00:09:34.200
softmax regression or soft by
 

00:09:34.200 --> 00:09:36.019
softmax regression or soft by
classification from scratch although you

00:09:36.019 --> 00:09:36.029
classification from scratch although you
 

00:09:36.029 --> 00:09:37.790
classification from scratch although you
won't actually need this industries from

00:09:37.790 --> 00:09:37.800
won't actually need this industries from
 

00:09:37.800 --> 00:09:40.280
won't actually need this industries from
exercise because the programming

00:09:40.280 --> 00:09:40.290
exercise because the programming
 

00:09:40.290 --> 00:09:42.170
exercise because the programming
framework you use will take care of this

00:09:42.170 --> 00:09:42.180
framework you use will take care of this
 

00:09:42.180 --> 00:09:44.930
framework you use will take care of this
derivative computation for you so that's

00:09:44.930 --> 00:09:44.940
derivative computation for you so that's
 

00:09:44.940 --> 00:09:47.630
derivative computation for you so that's
it for soft max classification with it

00:09:47.630 --> 00:09:47.640
it for soft max classification with it
 

00:09:47.640 --> 00:09:49.370
it for soft max classification with it
you can now implement learning

00:09:49.370 --> 00:09:49.380
you can now implement learning
 

00:09:49.380 --> 00:09:51.290
you can now implement learning
algorithms to catalyze the inference

00:09:51.290 --> 00:09:51.300
algorithms to catalyze the inference
 

00:09:51.300 --> 00:09:54.019
algorithms to catalyze the inference
into not just one of two classes but one

00:09:54.019 --> 00:09:54.029
into not just one of two classes but one
 

00:09:54.029 --> 00:09:57.680
into not just one of two classes but one
of the different classes next I want to

00:09:57.680 --> 00:09:57.690
of the different classes next I want to
 

00:09:57.690 --> 00:09:59.600
of the different classes next I want to
show you some of the deep learning

00:09:59.600 --> 00:09:59.610
show you some of the deep learning
 

00:09:59.610 --> 00:10:01.610
show you some of the deep learning
programming frameworks which can meet

00:10:01.610 --> 00:10:01.620
programming frameworks which can meet
 

00:10:01.620 --> 00:10:03.410
programming frameworks which can meet
you much more efficient in terms of

00:10:03.410 --> 00:10:03.420
you much more efficient in terms of
 

00:10:03.420 --> 00:10:05.090
you much more efficient in terms of
implementing deep learning algorithms

00:10:05.090 --> 00:10:05.100
implementing deep learning algorithms
 

00:10:05.100 --> 00:10:07.160
implementing deep learning algorithms
let's go onto the next video to discuss

00:10:07.160 --> 00:10:07.170
let's go onto the next video to discuss
 

00:10:07.170 --> 00:10:09.350
let's go onto the next video to discuss
that

