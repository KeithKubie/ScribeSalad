1
00:00:00,960 --> 00:00:03,420
The Joe Rogan experience.
Yeah.

2
00:00:03,530 --> 00:00:08,460
The unintended consequences are something
that I've been taken very seriously

3
00:00:08,461 --> 00:00:12,060
lately when I'm paying
attention to technology as a,

4
00:00:12,420 --> 00:00:16,790
as it's used in social media.
Um, particularly the one,

5
00:00:16,800 --> 00:00:20,670
one of the things is disturbed me quite
a bit over the last few weeks is that

6
00:00:20,671 --> 00:00:24,000
there's a model that they use in,
not intentionally,

7
00:00:24,720 --> 00:00:28,370
but there's a model that they use
to get people upset about things,

8
00:00:28,710 --> 00:00:31,430
show you things in your feed that,
uh,

9
00:00:31,530 --> 00:00:36,030
you argue against because that makes
you click on them more and you engage in

10
00:00:36,031 --> 00:00:36,810
them more.

11
00:00:36,810 --> 00:00:41,220
And because of the fact that we have
this advertiser based model where people

12
00:00:41,221 --> 00:00:45,060
are trying to get clicks because they
want to get ads on their page and the more

13
00:00:45,210 --> 00:00:47,430
clicks they get,
the more money they get from those ads.

14
00:00:47,610 --> 00:00:50,280
And so they want to
incentivize people to go there.

15
00:00:50,281 --> 00:00:51,990
And the best way through the algorithm,

16
00:00:52,150 --> 00:00:55,860
what they've found without doing it
on purpose is to get people upset and

17
00:00:55,861 --> 00:00:59,490
they're pushing people into these little
information ghettos that are really

18
00:00:59,491 --> 00:01:00,480
dangerous.
Right.

19
00:01:00,510 --> 00:01:05,040
And we've gotten to this point where this
is just an accepted part of our lives

20
00:01:05,250 --> 00:01:08,140
that you go to check your Google
feed or your Facebook feed and all,

21
00:01:08,141 --> 00:01:11,900
but what the fuck are they doing? Is this
real? They're going to pass this. Yeah.

22
00:01:12,000 --> 00:01:12,810
Goddammit.

23
00:01:12,810 --> 00:01:17,130
And then you get mad and then you engage
with people online and then it results

24
00:01:17,131 --> 00:01:20,850
in more revenue.
But getting them to stop that.

25
00:01:21,030 --> 00:01:22,770
If you had to go to Facebook and say,
hey,

26
00:01:22,890 --> 00:01:26,310
hey mark Zuckerberg and know you have
fucking $100 billion or whatever you got,

27
00:01:26,490 --> 00:01:28,340
but you can't make any
more money this way. Right.

28
00:01:28,380 --> 00:01:30,300
Because what you're doing
is fucking up society. Yeah.

29
00:01:30,320 --> 00:01:32,130
Cause you're encouraging dissent.

30
00:01:32,340 --> 00:01:37,020
You're encouraging people to be upset and
arguments and you're doing it at great

31
00:01:37,021 --> 00:01:41,120
financial reward, but great
societal costs. Yeah. So stop. Yeah.

32
00:01:41,430 --> 00:01:43,580
And he's not going to do it.
Right. Well, he may not do it.

33
00:01:43,581 --> 00:01:46,170
That comes back to the point about
regulation did the question is how big is

34
00:01:46,171 --> 00:01:49,380
your stick? One of the guys who
was the founder of faces yes.

35
00:01:49,381 --> 00:01:52,080
Is now coming out and saying, hey,
Facebook needs to be broken up.

36
00:01:52,410 --> 00:01:55,500
And then he was one of the
original founders and he's like,

37
00:01:55,501 --> 00:01:58,830
it has gotten so far out of hand,
it's so far away from where it is.

38
00:01:58,831 --> 00:02:02,850
It's literally affecting global
politics. Yeah, well it is.

39
00:02:02,851 --> 00:02:06,690
And so one option is to break it up
that it seems to have worked pretty well

40
00:02:06,691 --> 00:02:10,350
with at t and T. Um, another
option is to regulate it,

41
00:02:10,530 --> 00:02:14,520
which I is in my mind would be a
better approach. And that is to say,

42
00:02:14,880 --> 00:02:17,760
here's what's okay and
here's what's not okay.

43
00:02:17,761 --> 00:02:19,350
And this stuff is really intricate.

44
00:02:19,380 --> 00:02:22,980
You have to really get down
beneath these algorithms,

45
00:02:22,981 --> 00:02:25,770
which are unbelievably complex.
But you're exactly right. I mean,

46
00:02:25,771 --> 00:02:29,370
what we're seeing now is we
are being pushed into it.

47
00:02:29,430 --> 00:02:30,870
I said information ghettos,

48
00:02:30,871 --> 00:02:33,870
but it's like information
barricades and so pushed into camps.

49
00:02:34,380 --> 00:02:38,520
Which camp are you on? It's so
dangerous because the old, I mean this,

50
00:02:38,521 --> 00:02:42,930
this country is based on not everybody
agreeing that having a process where

51
00:02:42,931 --> 00:02:44,610
people come and they work it
out and they say, you know,

52
00:02:44,611 --> 00:02:47,040
I'm not perfectly happy with this outcome,

53
00:02:47,370 --> 00:02:51,050
but here's a compromise and if
we can't compromise and our,

54
00:02:51,060 --> 00:02:53,850
our civic culture is going to break
down and there's so much, I mean,

55
00:02:53,851 --> 00:02:56,460
people don't see these pillars
that are holding up our society.

56
00:02:56,461 --> 00:03:00,490
I'd lived in Cambodia for two years and
if you don't have civic pillars under

57
00:03:00,491 --> 00:03:04,990
your society, society's look very, very
different. Everyone's life experiences,

58
00:03:04,991 --> 00:03:07,060
right? We kind of take for
granted. You can go out the door,

59
00:03:07,061 --> 00:03:10,600
walk to Starbucks and not get shot or a,

60
00:03:10,601 --> 00:03:13,780
you can have your house. Something
happened, your house gets Rod,

61
00:03:13,781 --> 00:03:18,040
you call the police and the police aren't
the ones who've robbed your house or

62
00:03:18,041 --> 00:03:20,590
they're not in, I mean there's
all these kinds of crazy things.

63
00:03:20,860 --> 00:03:25,120
If we break down the foundations
that underpin our lives,

64
00:03:25,121 --> 00:03:29,290
that's really dangerous. What I was kinda
getting at was that through what this,

65
00:03:29,680 --> 00:03:33,860
this process of this algorithm,
how this algorithm selects things that,

66
00:03:33,861 --> 00:03:36,640
that chose you in your feet and how
people are getting upset by this and how

67
00:03:36,641 --> 00:03:40,840
this is generating massive amounts of
revenue once it's already happened.

68
00:03:40,841 --> 00:03:42,160
It's very difficult to stop.

69
00:03:42,161 --> 00:03:46,810
And my concern would be that this would
be a similar thing when it comes to

70
00:03:46,811 --> 00:03:50,920
genetic engineering or we're saying we
need to be able to put regulations on

71
00:03:50,921 --> 00:03:55,060
this. We needed to be able to establish
that. But once it gets out of the bag,

72
00:03:55,090 --> 00:03:56,980
once it gets rolling,
and I have,

73
00:03:57,370 --> 00:04:00,460
you remember when Mark Zuckerberg sat
in front of all those politicians,

74
00:04:00,461 --> 00:04:03,760
they had no fucking idea what they
were talking about. You make money.

75
00:04:03,800 --> 00:04:06,250
It's such piss,
poor prepares and it just,

76
00:04:06,251 --> 00:04:08,920
it shows you like these are the
people that are looking out for us.

77
00:04:08,921 --> 00:04:13,450
Good fucking luck. These are Luddites.
They're dumb asses. They're fools, right?

78
00:04:13,480 --> 00:04:17,230
And they're, they don't know anything
about, some are better and worse,

79
00:04:17,231 --> 00:04:22,231
but yet almost everyone was underwhelming
and under impressive in that hearing,

80
00:04:22,601 --> 00:04:23,231
in that hearing.

81
00:04:23,231 --> 00:04:26,050
Yet the fact that they're dealing with
one of the most important moments of our

82
00:04:26,051 --> 00:04:26,590
time,

83
00:04:26,590 --> 00:04:31,420
but they didn't bring on some sort of
a like legitimate technology expert.

84
00:04:31,421 --> 00:04:34,340
It could explain the pitfalls.
Yes. And do so in a way that the,

85
00:04:34,360 --> 00:04:35,380
the rest of the world's could know.

86
00:04:36,100 --> 00:04:39,140
So they're not going to protect us
from genetic engineering either. Right.

87
00:04:39,160 --> 00:04:42,730
Because they are generalists in terms
of their education for the most part and

88
00:04:42,731 --> 00:04:44,850
they're not, they're not
concerned. This is not,

89
00:04:45,040 --> 00:04:47,050
they're concerned with raising
money for their campaign.

90
00:04:47,050 --> 00:04:50,110
The concern with getting reelected.
That's their concerned with, yeah.

91
00:04:50,111 --> 00:04:55,111
I totally agree with you that if we wait
to focus on this issue until it becomes

92
00:04:56,200 --> 00:04:56,920
a crisis,

93
00:04:56,920 --> 00:04:59,950
it's going to be too late because all
the big decisions will have been made.

94
00:05:00,250 --> 00:05:03,450
The reason why I wrote this
book, the reason why, you know,

95
00:05:03,550 --> 00:05:08,550
I'm on my almost week three of this book
tour doing events like this every day

96
00:05:09,131 --> 00:05:13,660
is what I am saying. Every form that I
can is this is really important to you.

97
00:05:13,661 --> 00:05:17,670
Kind of. We were watching the news
yesterday. They had this royal baby in,

98
00:05:17,710 --> 00:05:20,910
in the UK. Like, I don't give a shit.
It doesn't affect my life and anybody.

99
00:05:21,040 --> 00:05:26,040
But what does it play now is the future
of our entire species and our democracy

100
00:05:26,291 --> 00:05:26,831
and our lives.

101
00:05:26,831 --> 00:05:31,831
And we have to be focusing on those things
because we have a moment now where we

102
00:05:32,501 --> 00:05:37,501
can get to a certain extent influence
how these revolutions play out.

103
00:05:37,930 --> 00:05:39,100
And if we just wait around,

104
00:05:39,101 --> 00:05:42,550
if we're distracted and we're focusing
on all this stuff that's sucking up our

105
00:05:42,551 --> 00:05:46,660
attention, and whether it's Trump or
Brexit or Muller and all these things,

106
00:05:46,661 --> 00:05:49,960
I mean, we're spent, how much of our
time are we spending focused on is fine,

107
00:05:50,020 --> 00:05:51,250
let's pay a little bit of attention.

108
00:05:51,251 --> 00:05:54,480
But there's really big stuff
50 years from now, 100 years,

109
00:05:54,490 --> 00:05:56,110
and no one's going to look back and say,
oh,

110
00:05:56,111 --> 00:06:00,170
that was the age of Trump or that they're
going to say that was the age when

111
00:06:00,171 --> 00:06:02,750
after almost 4 billion years of evolution,

112
00:06:02,751 --> 00:06:04,940
humans took control of their
own evolutionary process.

113
00:06:04,941 --> 00:06:07,700
And it's huge and it's
going to change all of life.

114
00:06:08,180 --> 00:06:12,140
And what I'm trying to do is to say
everybody has to have a seat at the table.

115
00:06:12,141 --> 00:06:16,400
Whether you're a conservative Christian,
whether you're a,

116
00:06:16,401 --> 00:06:18,650
a biohacking transhumanist,

117
00:06:18,830 --> 00:06:21,800
everybody needs to be at the table.

118
00:06:21,800 --> 00:06:24,320
Because we talking about is
the future of our species.

119
00:06:24,590 --> 00:06:26,240
We're talking about the
future of our species,

120
00:06:26,241 --> 00:06:30,680
but are we even capable of understanding
the consequences of these actions to

121
00:06:30,800 --> 00:06:32,820
the stuff that we're
discussing like right now,

122
00:06:32,821 --> 00:06:37,130
or I'm not like I'm talking about
it, right? If someone said, hey,

123
00:06:37,131 --> 00:06:41,150
you've got to go speak in front of people
about the consequences of an in a very

124
00:06:41,151 --> 00:06:44,900
clear one hour presentation,
I'd be like, no, I'm not. Well,

125
00:06:44,901 --> 00:06:48,270
I'm what I'm talking about. One, we can go
together. So you're good. Thank you. Um,

126
00:06:48,370 --> 00:06:51,830
but to the reason why I've written this
book hacking Darwin is I want it say if

127
00:06:51,831 --> 00:06:56,240
you could read just one book and it's
written just for everybody in a very clear

128
00:06:56,241 --> 00:06:58,430
way with a lot of jokes
that I think are funny.

129
00:06:58,431 --> 00:07:01,370
My mother laughed at them as well,
that you get it.

130
00:07:01,730 --> 00:07:06,050
And then once you know just the
basics as a human being, anybody,

131
00:07:06,350 --> 00:07:09,350
it has an equal right to be part of this,

132
00:07:09,380 --> 00:07:13,970
of this conversation as the top scientist
or the leaders of any of any countries,

133
00:07:13,980 --> 00:07:15,030
but would agree with you there.
Yeah,

134
00:07:15,050 --> 00:07:17,840
but I don't think that other people
are going to see it that way.

135
00:07:18,200 --> 00:07:20,420
I think the people that are in
control, they're not going to say, hey,

136
00:07:20,421 --> 00:07:23,810
we need to find, we need to be fair with
everyone, all the citizens of the world.

137
00:07:24,020 --> 00:07:26,680
How do you feel we should proceed?
No, but that's why we have to,

138
00:07:26,690 --> 00:07:29,130
that's why we need this
bottom up ground swell.

139
00:07:29,190 --> 00:07:32,210
But we can't have a bottom
up ground swell if people,

140
00:07:32,330 --> 00:07:36,170
if just general people aren't even aware
of what the issues are. And that's,

141
00:07:36,200 --> 00:07:40,250
that's the challenge. And that's why
forums like yours are just so important.

142
00:07:40,250 --> 00:07:43,070
I mean, you have all of these
people and then you know,

143
00:07:43,071 --> 00:07:46,250
maybe everyone doesn't listen to this
podcast. Say, all right, I get it.

144
00:07:46,251 --> 00:07:47,750
I can go give that hour long speech.

145
00:07:48,110 --> 00:07:52,310
But you can read a couple books and then
you can give an an hour speech because

146
00:07:52,340 --> 00:07:55,670
the issues like, yes, there
are scientific issues,

147
00:07:55,700 --> 00:07:57,620
but this isn't a
conversation about science.

148
00:07:57,621 --> 00:07:59,900
This is about values and
ethics in our future.

149
00:07:59,901 --> 00:08:02,180
And it has to be a conversation
for everybody. Yeah.

150
00:08:02,181 --> 00:08:04,640
It's not just a scientific conversation.

151
00:08:04,700 --> 00:08:08,680
It's a conversation about the future of
the species. The species will be awesome.

152
00:08:08,770 --> 00:08:13,340
Yeah. And that's something we're wholly
unqualified. No, it's you. But here's,

153
00:08:13,550 --> 00:08:15,800
here's a little vote for optimism.
Okay.

154
00:08:15,950 --> 00:08:18,890
We have never been this
literate as a species for,

155
00:08:18,950 --> 00:08:20,570
we've never been this educated.

156
00:08:20,720 --> 00:08:23,600
I don't think it was if we ever been
this nice either. Well, I hope so.

157
00:08:23,600 --> 00:08:25,580
I really did want do,
if you look at it,

158
00:08:25,581 --> 00:08:28,570
all the wars and all the murder that
used to happen, it's actually a,

159
00:08:28,571 --> 00:08:30,140
this is the best time ever to be alive.

160
00:08:30,260 --> 00:08:33,640
Still Sucks for people that are
in bad situation. But it's, yes,

161
00:08:34,010 --> 00:08:37,490
on average it's better. And we've
never been this connected. So we have,

162
00:08:37,580 --> 00:08:39,050
so I am the book,

163
00:08:39,051 --> 00:08:42,440
I call for a species wide dialogue on
the future of human genetic engineering.

164
00:08:42,441 --> 00:08:45,080
You think go, well, that's
nuts. 7 billion people on Earth.

165
00:08:45,081 --> 00:08:46,970
How are they gonna how
are they going to do that?

166
00:08:47,360 --> 00:08:52,340
But we have the opportunity and we
have to try. Because you don't want,

167
00:08:52,341 --> 00:08:54,610
look with the beginning of the
genetically modified crops here,

168
00:08:54,620 --> 00:08:56,820
the scientists were
actually really responsible,

169
00:08:57,120 --> 00:09:00,810
but the regular people weren't consulted
and they felt that these guys just did

170
00:09:00,811 --> 00:09:04,950
it to me. So if you have all the marches
with genetically modified organisms,

171
00:09:05,250 --> 00:09:08,960
you that we are entering the
era of genetically modified
humans and that's going

172
00:09:08,961 --> 00:09:10,680
to scare the shit out of people.

173
00:09:10,920 --> 00:09:13,800
And so we need to start preparing and
we need to make people feel that they're

174
00:09:13,801 --> 00:09:18,240
respected and included, and our government
leaders aren't going to do it for us.

175
00:09:18,270 --> 00:09:21,540
So we have to find ways
of engaging ourselves.

176
00:09:21,541 --> 00:09:25,770
And that's why with me with the book, I
set up a website where people can, um,

177
00:09:25,800 --> 00:09:28,950
can share their views,
debate with other people.

178
00:09:28,951 --> 00:09:31,980
I really want everybody to
be part of this conversation.

