WEBVTT
Kind: captions
Language: en

00:00:00.089 --> 00:00:02.480
 
welcome back everybody or for those of

00:00:02.480 --> 00:00:02.490
welcome back everybody or for those of
 

00:00:02.490 --> 00:00:03.830
welcome back everybody or for those of
you who are just able to join us this

00:00:03.830 --> 00:00:03.840
you who are just able to join us this
 

00:00:03.840 --> 00:00:07.309
you who are just able to join us this
evening welcome to the opening day of

00:00:07.309 --> 00:00:07.319
evening welcome to the opening day of
 

00:00:07.319 --> 00:00:10.250
evening welcome to the opening day of
the inaugural CMU K&amp;L gates conference

00:00:10.250 --> 00:00:10.260
the inaugural CMU K&amp;L gates conference
 

00:00:10.260 --> 00:00:14.299
the inaugural CMU K&amp;L gates conference
on ethics and AI I'm David Danks one of

00:00:14.299 --> 00:00:14.309
on ethics and AI I'm David Danks one of
 

00:00:14.309 --> 00:00:16.039
on ethics and AI I'm David Danks one of
the co-organizers along with the Lenoir

00:00:16.039 --> 00:00:16.049
the co-organizers along with the Lenoir
 

00:00:16.049 --> 00:00:18.920
the co-organizers along with the Lenoir
Bosch of the conference today and

00:00:18.920 --> 00:00:18.930
Bosch of the conference today and
 

00:00:18.930 --> 00:00:19.760
Bosch of the conference today and
tomorrow

00:00:19.760 --> 00:00:19.770
tomorrow
 

00:00:19.770 --> 00:00:21.920
tomorrow
that is really bringing together a group

00:00:21.920 --> 00:00:21.930
that is really bringing together a group
 

00:00:21.930 --> 00:00:23.390
that is really bringing together a group
of interdisciplinary and

00:00:23.390 --> 00:00:23.400
of interdisciplinary and
 

00:00:23.400 --> 00:00:26.089
of interdisciplinary and
multidisciplinary thought leaders to

00:00:26.089 --> 00:00:26.099
multidisciplinary thought leaders to
 

00:00:26.099 --> 00:00:29.210
multidisciplinary thought leaders to
help advance the conversation around the

00:00:29.210 --> 00:00:29.220
help advance the conversation around the
 

00:00:29.220 --> 00:00:30.790
help advance the conversation around the
areas of ethics and AI the ways in which

00:00:30.790 --> 00:00:30.800
areas of ethics and AI the ways in which
 

00:00:30.800 --> 00:00:32.930
areas of ethics and AI the ways in which
technologies especially computational

00:00:32.930 --> 00:00:32.940
technologies especially computational
 

00:00:32.940 --> 00:00:35.990
technologies especially computational
technologies are able to help us support

00:00:35.990 --> 00:00:36.000
technologies are able to help us support
 

00:00:36.000 --> 00:00:38.869
technologies are able to help us support
and advance human values and one a

00:00:38.869 --> 00:00:38.879
and advance human values and one a
 

00:00:38.879 --> 00:00:39.889
and advance human values and one a
really important part of this

00:00:39.889 --> 00:00:39.899
really important part of this
 

00:00:39.899 --> 00:00:42.500
really important part of this
conversation is is the distinguished

00:00:42.500 --> 00:00:42.510
conversation is is the distinguished
 

00:00:42.510 --> 00:00:44.119
conversation is is the distinguished
keynote lecture that we have this

00:00:44.119 --> 00:00:44.129
keynote lecture that we have this
 

00:00:44.129 --> 00:00:46.580
keynote lecture that we have this
evening which is really I think going to

00:00:46.580 --> 00:00:46.590
evening which is really I think going to
 

00:00:46.590 --> 00:00:48.170
evening which is really I think going to
be a highlight of the conference and

00:00:48.170 --> 00:00:48.180
be a highlight of the conference and
 

00:00:48.180 --> 00:00:51.380
be a highlight of the conference and
introduce the speaker it is my honor to

00:00:51.380 --> 00:00:51.390
introduce the speaker it is my honor to
 

00:00:51.390 --> 00:00:55.430
introduce the speaker it is my honor to
bring to the podium mr. Mike cassis who

00:00:55.430 --> 00:00:55.440
bring to the podium mr. Mike cassis who
 

00:00:55.440 --> 00:00:56.840
bring to the podium mr. Mike cassis who
is the chairman of the management

00:00:56.840 --> 00:00:56.850
is the chairman of the management
 

00:00:56.850 --> 00:00:59.479
is the chairman of the management
committee of the law firm K&amp;L Gates as

00:00:59.479 --> 00:00:59.489
committee of the law firm K&amp;L Gates as
 

00:00:59.489 --> 00:01:03.020
committee of the law firm K&amp;L Gates as
well as a world leader in investment

00:01:03.020 --> 00:01:03.030
well as a world leader in investment
 

00:01:03.030 --> 00:01:04.690
well as a world leader in investment
management and regulatory compliance

00:01:04.690 --> 00:01:04.700
management and regulatory compliance
 

00:01:04.700 --> 00:01:06.640
management and regulatory compliance
Mike

00:01:06.640 --> 00:01:06.650
Mike
 

00:01:06.650 --> 00:01:10.830
Mike
[Applause]

00:01:10.830 --> 00:01:10.840
[Applause]
 

00:01:10.840 --> 00:01:13.810
[Applause]
thank you very much and it's a pleasure

00:01:13.810 --> 00:01:13.820
thank you very much and it's a pleasure
 

00:01:13.820 --> 00:01:15.880
thank you very much and it's a pleasure
being here and I have to tell you it is

00:01:15.880 --> 00:01:15.890
being here and I have to tell you it is
 

00:01:15.890 --> 00:01:20.190
being here and I have to tell you it is
an honor to introduce dr. Eric Horvitz

00:01:20.190 --> 00:01:20.200
an honor to introduce dr. Eric Horvitz
 

00:01:20.200 --> 00:01:22.930
an honor to introduce dr. Eric Horvitz
when you think of the purpose of this

00:01:22.930 --> 00:01:22.940
when you think of the purpose of this
 

00:01:22.940 --> 00:01:25.810
when you think of the purpose of this
inaugural lecture to talk about current

00:01:25.810 --> 00:01:25.820
inaugural lecture to talk about current
 

00:01:25.820 --> 00:01:28.000
inaugural lecture to talk about current
trends future trends challenges and

00:01:28.000 --> 00:01:28.010
trends future trends challenges and
 

00:01:28.010 --> 00:01:31.420
trends future trends challenges and
opportunities in ethics and artificial

00:01:31.420 --> 00:01:31.430
opportunities in ethics and artificial
 

00:01:31.430 --> 00:01:33.550
opportunities in ethics and artificial
intelligence you couldn't think of a

00:01:33.550 --> 00:01:33.560
intelligence you couldn't think of a
 

00:01:33.560 --> 00:01:37.359
intelligence you couldn't think of a
better person to do it than Eric eric

00:01:37.359 --> 00:01:37.369
better person to do it than Eric eric
 

00:01:37.369 --> 00:01:40.270
better person to do it than Eric eric
has been active in addressing many of

00:01:40.270 --> 00:01:40.280
has been active in addressing many of
 

00:01:40.280 --> 00:01:43.270
has been active in addressing many of
these issues for over 30 years he's

00:01:43.270 --> 00:01:43.280
these issues for over 30 years he's
 

00:01:43.280 --> 00:01:46.359
these issues for over 30 years he's
currently the director of Microsoft

00:01:46.359 --> 00:01:46.369
currently the director of Microsoft
 

00:01:46.369 --> 00:01:51.580
currently the director of Microsoft
research labs and and at Microsoft as a

00:01:51.580 --> 00:01:51.590
research labs and and at Microsoft as a
 

00:01:51.590 --> 00:01:54.550
research labs and and at Microsoft as a
technical fellow and in Redmond

00:01:54.550 --> 00:01:54.560
technical fellow and in Redmond
 

00:01:54.560 --> 00:01:57.940
technical fellow and in Redmond
Washington as I mentioned his whole

00:01:57.940 --> 00:01:57.950
Washington as I mentioned his whole
 

00:01:57.950 --> 00:02:01.300
Washington as I mentioned his whole
career has focused on that he has been

00:02:01.300 --> 00:02:01.310
career has focused on that he has been
 

00:02:01.310 --> 00:02:04.749
career has focused on that he has been
has this unique skill that in this area

00:02:04.749 --> 00:02:04.759
has this unique skill that in this area
 

00:02:04.759 --> 00:02:09.210
has this unique skill that in this area
where can be so technical and very

00:02:09.210 --> 00:02:09.220
where can be so technical and very
 

00:02:09.220 --> 00:02:12.760
where can be so technical and very
difficult in order to take all the

00:02:12.760 --> 00:02:12.770
difficult in order to take all the
 

00:02:12.770 --> 00:02:14.890
difficult in order to take all the
various issues that are being faced in

00:02:14.890 --> 00:02:14.900
various issues that are being faced in
 

00:02:14.900 --> 00:02:19.600
various issues that are being faced in
order to describe it in well I would say

00:02:19.600 --> 00:02:19.610
order to describe it in well I would say
 

00:02:19.610 --> 00:02:22.630
order to describe it in well I would say
in layman's terms he has his excellent

00:02:22.630 --> 00:02:22.640
in layman's terms he has his excellent
 

00:02:22.640 --> 00:02:26.020
in layman's terms he has his excellent
skill to take these concepts and let the

00:02:26.020 --> 00:02:26.030
skill to take these concepts and let the
 

00:02:26.030 --> 00:02:28.390
skill to take these concepts and let the
average person understand the

00:02:28.390 --> 00:02:28.400
average person understand the
 

00:02:28.400 --> 00:02:30.100
average person understand the
opportunities the challenges the

00:02:30.100 --> 00:02:30.110
opportunities the challenges the
 

00:02:30.110 --> 00:02:33.039
opportunities the challenges the
benefits he has worked on safety issues

00:02:33.039 --> 00:02:33.049
benefits he has worked on safety issues
 

00:02:33.049 --> 00:02:35.949
benefits he has worked on safety issues
in the Space Shuttle but also at the

00:02:35.949 --> 00:02:35.959
in the Space Shuttle but also at the
 

00:02:35.959 --> 00:02:38.949
in the Space Shuttle but also at the
same time he has done fantastic work and

00:02:38.949 --> 00:02:38.959
same time he has done fantastic work and
 

00:02:38.959 --> 00:02:41.770
same time he has done fantastic work and
AE dealing with email systems and

00:02:41.770 --> 00:02:41.780
AE dealing with email systems and
 

00:02:41.780 --> 00:02:44.170
AE dealing with email systems and
scheduling systems so every day how

00:02:44.170 --> 00:02:44.180
scheduling systems so every day how
 

00:02:44.180 --> 00:02:46.240
scheduling systems so every day how
every day lives have been changed by

00:02:46.240 --> 00:02:46.250
every day lives have been changed by
 

00:02:46.250 --> 00:02:49.360
every day lives have been changed by
some of the work he has done you know

00:02:49.360 --> 00:02:49.370
some of the work he has done you know
 

00:02:49.370 --> 00:02:51.910
some of the work he has done you know
Eric he's really a double doctor not

00:02:51.910 --> 00:02:51.920
Eric he's really a double doctor not
 

00:02:51.920 --> 00:02:55.000
Eric he's really a double doctor not
only does he have PhD he also has an MD

00:02:55.000 --> 00:02:55.010
only does he have PhD he also has an MD
 

00:02:55.010 --> 00:02:57.190
only does he have PhD he also has an MD
so a lot of his work has been done

00:02:57.190 --> 00:02:57.200
so a lot of his work has been done
 

00:02:57.200 --> 00:03:00.759
so a lot of his work has been done
focusing in the medical field one of his

00:03:00.759 --> 00:03:00.769
focusing in the medical field one of his
 

00:03:00.769 --> 00:03:02.949
focusing in the medical field one of his
challenges has been and worried always

00:03:02.949 --> 00:03:02.959
challenges has been and worried always
 

00:03:02.959 --> 00:03:05.590
challenges has been and worried always
has been is as doctors in under

00:03:05.590 --> 00:03:05.600
has been is as doctors in under
 

00:03:05.600 --> 00:03:07.660
has been is as doctors in under
stressful conditions and emergency rooms

00:03:07.660 --> 00:03:07.670
stressful conditions and emergency rooms
 

00:03:07.670 --> 00:03:10.270
stressful conditions and emergency rooms
and other situations get a lot of data

00:03:10.270 --> 00:03:10.280
and other situations get a lot of data
 

00:03:10.280 --> 00:03:12.940
and other situations get a lot of data
how do they analyze that and help the

00:03:12.940 --> 00:03:12.950
how do they analyze that and help the
 

00:03:12.950 --> 00:03:15.009
how do they analyze that and help the
patient understand that that some of the

00:03:15.009 --> 00:03:15.019
patient understand that that some of the
 

00:03:15.019 --> 00:03:18.130
patient understand that that some of the
efforts that Eric has focused on his he

00:03:18.130 --> 00:03:18.140
efforts that Eric has focused on his he
 

00:03:18.140 --> 00:03:21.370
efforts that Eric has focused on his he
did his dissertation 25 years ago

00:03:21.370 --> 00:03:21.380
did his dissertation 25 years ago
 

00:03:21.380 --> 00:03:22.309
did his dissertation 25 years ago
talking

00:03:22.309 --> 00:03:22.319
talking
 

00:03:22.319 --> 00:03:26.059
talking
about these issues and his first

00:03:26.059 --> 00:03:26.069
about these issues and his first
 

00:03:26.069 --> 00:03:28.910
about these issues and his first
publication was on Pathfinder which was

00:03:28.910 --> 00:03:28.920
publication was on Pathfinder which was
 

00:03:28.920 --> 00:03:32.780
publication was on Pathfinder which was
a tool to help internships in order to

00:03:32.780 --> 00:03:32.790
a tool to help internships in order to
 

00:03:32.790 --> 00:03:35.300
a tool to help internships in order to
in the diagnostic area in order to

00:03:35.300 --> 00:03:35.310
in the diagnostic area in order to
 

00:03:35.310 --> 00:03:38.000
in the diagnostic area in order to
simplify information they receive to

00:03:38.000 --> 00:03:38.010
simplify information they receive to
 

00:03:38.010 --> 00:03:41.629
simplify information they receive to
help them make better decisions he has

00:03:41.629 --> 00:03:41.639
help them make better decisions he has
 

00:03:41.639 --> 00:03:43.369
help them make better decisions he has
been an elite recently he's been a

00:03:43.369 --> 00:03:43.379
been an elite recently he's been a
 

00:03:43.379 --> 00:03:47.119
been an elite recently he's been a
leader to examine the many difficult

00:03:47.119 --> 00:03:47.129
leader to examine the many difficult
 

00:03:47.129 --> 00:03:49.000
leader to examine the many difficult
ethical social and psychological

00:03:49.000 --> 00:03:49.010
ethical social and psychological
 

00:03:49.010 --> 00:03:53.740
ethical social and psychological
questions raised by advancement in AI in

00:03:53.740 --> 00:03:53.750
questions raised by advancement in AI in
 

00:03:53.750 --> 00:03:56.659
questions raised by advancement in AI in
2009 while he was apprentice the

00:03:56.659 --> 00:03:56.669
2009 while he was apprentice the
 

00:03:56.669 --> 00:03:58.190
2009 while he was apprentice the
president of the Association of

00:03:58.190 --> 00:03:58.200
president of the Association of
 

00:03:58.200 --> 00:04:00.140
president of the Association of
advancement artificial intelligence

00:04:00.140 --> 00:04:00.150
advancement artificial intelligence
 

00:04:00.150 --> 00:04:04.789
advancement artificial intelligence
commonly referred to as triple AI he

00:04:04.789 --> 00:04:04.799
commonly referred to as triple AI he
 

00:04:04.799 --> 00:04:06.589
commonly referred to as triple AI he
also chaired the groundbreaking a

00:04:06.589 --> 00:04:06.599
also chaired the groundbreaking a
 

00:04:06.599 --> 00:04:09.349
also chaired the groundbreaking a
similar AI study which brought together

00:04:09.349 --> 00:04:09.359
similar AI study which brought together
 

00:04:09.359 --> 00:04:11.449
similar AI study which brought together
blue-ribbon group many of those people

00:04:11.449 --> 00:04:11.459
blue-ribbon group many of those people
 

00:04:11.459 --> 00:04:14.360
blue-ribbon group many of those people
are in this room to identify examine the

00:04:14.360 --> 00:04:14.370
are in this room to identify examine the
 

00:04:14.370 --> 00:04:16.879
are in this room to identify examine the
potential human social impacts of AI

00:04:16.879 --> 00:04:16.889
potential human social impacts of AI
 

00:04:16.889 --> 00:04:19.759
potential human social impacts of AI
research and development the study of

00:04:19.759 --> 00:04:19.769
research and development the study of
 

00:04:19.769 --> 00:04:22.339
research and development the study of
self accept has helped set the agenda

00:04:22.339 --> 00:04:22.349
self accept has helped set the agenda
 

00:04:22.349 --> 00:04:25.790
self accept has helped set the agenda
for many the topics at this session here

00:04:25.790 --> 00:04:25.800
for many the topics at this session here
 

00:04:25.800 --> 00:04:29.659
for many the topics at this session here
at this past day and and tomorrow he

00:04:29.659 --> 00:04:29.669
at this past day and and tomorrow he
 

00:04:29.669 --> 00:04:33.560
at this past day and and tomorrow he
profound at the 100 year study on AI to

00:04:33.560 --> 00:04:33.570
profound at the 100 year study on AI to
 

00:04:33.570 --> 00:04:35.779
profound at the 100 year study on AI to
anticipate the diverse effects of

00:04:35.779 --> 00:04:35.789
anticipate the diverse effects of
 

00:04:35.789 --> 00:04:38.029
anticipate the diverse effects of
artificial intelligence on every aspect

00:04:38.029 --> 00:04:38.039
artificial intelligence on every aspect
 

00:04:38.039 --> 00:04:41.510
artificial intelligence on every aspect
of our lives in the coming century that

00:04:41.510 --> 00:04:41.520
of our lives in the coming century that
 

00:04:41.520 --> 00:04:44.839
of our lives in the coming century that
is writing eric is not a doomsayer he's

00:04:44.839 --> 00:04:44.849
is writing eric is not a doomsayer he's
 

00:04:44.849 --> 00:04:46.909
is writing eric is not a doomsayer he's
not a futurist he's a very practical

00:04:46.909 --> 00:04:46.919
not a futurist he's a very practical
 

00:04:46.919 --> 00:04:49.850
not a futurist he's a very practical
person taking all the different aspects

00:04:49.850 --> 00:04:49.860
person taking all the different aspects
 

00:04:49.860 --> 00:04:54.080
person taking all the different aspects
of this burgeoning area in that and

00:04:54.080 --> 00:04:54.090
of this burgeoning area in that and
 

00:04:54.090 --> 00:04:57.290
of this burgeoning area in that and
using it to reshape our lives and also

00:04:57.290 --> 00:04:57.300
using it to reshape our lives and also
 

00:04:57.300 --> 00:04:59.990
using it to reshape our lives and also
use the challenges and opportunities to

00:04:59.990 --> 00:05:00.000
use the challenges and opportunities to
 

00:05:00.000 --> 00:05:04.189
use the challenges and opportunities to
help society in a AI area he is Eric has

00:05:04.189 --> 00:05:04.199
help society in a AI area he is Eric has
 

00:05:04.199 --> 00:05:06.709
help society in a AI area he is Eric has
received numerous rewards he's a fellow

00:05:06.709 --> 00:05:06.719
received numerous rewards he's a fellow
 

00:05:06.719 --> 00:05:08.689
received numerous rewards he's a fellow
the National Academy of Engineering the

00:05:08.689 --> 00:05:08.699
the National Academy of Engineering the
 

00:05:08.699 --> 00:05:11.209
the National Academy of Engineering the
American Academy of Arts and Sciences

00:05:11.209 --> 00:05:11.219
American Academy of Arts and Sciences
 

00:05:11.219 --> 00:05:12.820
American Academy of Arts and Sciences
the American College of medical

00:05:12.820 --> 00:05:12.830
the American College of medical
 

00:05:12.830 --> 00:05:16.670
the American College of medical
informatics the triple AI as I mentioned

00:05:16.670 --> 00:05:16.680
informatics the triple AI as I mentioned
 

00:05:16.680 --> 00:05:18.589
informatics the triple AI as I mentioned
and the Association for Computing

00:05:18.589 --> 00:05:18.599
and the Association for Computing
 

00:05:18.599 --> 00:05:21.379
and the Association for Computing
Machinery or common referred to as ACM

00:05:21.379 --> 00:05:21.389
Machinery or common referred to as ACM
 

00:05:21.389 --> 00:05:24.200
Machinery or common referred to as ACM
he has buys government agencies

00:05:24.200 --> 00:05:24.210
he has buys government agencies
 

00:05:24.210 --> 00:05:27.110
he has buys government agencies
foundations major research institutions

00:05:27.110 --> 00:05:27.120
foundations major research institutions
 

00:05:27.120 --> 00:05:29.270
foundations major research institutions
and is testified in front of the US

00:05:29.270 --> 00:05:29.280
and is testified in front of the US
 

00:05:29.280 --> 00:05:33.830
and is testified in front of the US
Congress in 2015 he received both the

00:05:33.830 --> 00:05:33.840
Congress in 2015 he received both the
 

00:05:33.840 --> 00:05:35.670
Congress in 2015 he received both the
Triple A I died

00:05:35.670 --> 00:05:35.680
Triple A I died
 

00:05:35.680 --> 00:05:37.950
Triple A I died
prize for outstanding artificial

00:05:37.950 --> 00:05:37.960
prize for outstanding artificial
 

00:05:37.960 --> 00:05:39.779
prize for outstanding artificial
intelligence research and real-world

00:05:39.779 --> 00:05:39.789
intelligence research and real-world
 

00:05:39.789 --> 00:05:43.890
intelligence research and real-world
domains and also the AIC M triple a I

00:05:43.890 --> 00:05:43.900
domains and also the AIC M triple a I
 

00:05:43.900 --> 00:05:49.529
domains and also the AIC M triple a I
new award named for CMU zone Allen Neuer

00:05:49.529 --> 00:05:49.539
new award named for CMU zone Allen Neuer
 

00:05:49.539 --> 00:05:51.540
new award named for CMU zone Allen Neuer
for its contribution to artificial

00:05:51.540 --> 00:05:51.550
for its contribution to artificial
 

00:05:51.550 --> 00:05:54.140
for its contribution to artificial
intelligence and human-computer

00:05:54.140 --> 00:05:54.150
intelligence and human-computer
 

00:05:54.150 --> 00:05:57.749
intelligence and human-computer
interaction integration which integrated

00:05:57.749 --> 00:05:57.759
interaction integration which integrated
 

00:05:57.759 --> 00:06:01.170
interaction integration which integrated
both computing and decision sciences he

00:06:01.170 --> 00:06:01.180
both computing and decision sciences he
 

00:06:01.180 --> 00:06:02.850
both computing and decision sciences he
has developed some of the core methods

00:06:02.850 --> 00:06:02.860
has developed some of the core methods
 

00:06:02.860 --> 00:06:04.710
has developed some of the core methods
and systems advanced understanding of

00:06:04.710 --> 00:06:04.720
and systems advanced understanding of
 

00:06:04.720 --> 00:06:07.170
and systems advanced understanding of
machine intelligence and works to

00:06:07.170 --> 00:06:07.180
machine intelligence and works to
 

00:06:07.180 --> 00:06:10.320
machine intelligence and works to
harmonize these advances to improve our

00:06:10.320 --> 00:06:10.330
harmonize these advances to improve our
 

00:06:10.330 --> 00:06:13.469
harmonize these advances to improve our
everyday lives as technology is

00:06:13.469 --> 00:06:13.479
everyday lives as technology is
 

00:06:13.479 --> 00:06:15.570
everyday lives as technology is
spreading in all aspects of our lives

00:06:15.570 --> 00:06:15.580
spreading in all aspects of our lives
 

00:06:15.580 --> 00:06:19.230
spreading in all aspects of our lives
communities and societies Eric has

00:06:19.230 --> 00:06:19.240
communities and societies Eric has
 

00:06:19.240 --> 00:06:22.589
communities and societies Eric has
always been a leader in this area again

00:06:22.589 --> 00:06:22.599
always been a leader in this area again
 

00:06:22.599 --> 00:06:25.560
always been a leader in this area again
he's very practical he balances the

00:06:25.560 --> 00:06:25.570
he's very practical he balances the
 

00:06:25.570 --> 00:06:29.460
he's very practical he balances the
pluses and the minuses and he turns them

00:06:29.460 --> 00:06:29.470
pluses and the minuses and he turns them
 

00:06:29.470 --> 00:06:32.279
pluses and the minuses and he turns them
into areas that will improve up day to

00:06:32.279 --> 00:06:32.289
into areas that will improve up day to
 

00:06:32.289 --> 00:06:36.270
into areas that will improve up day to
day life I'm honored to have him we are

00:06:36.270 --> 00:06:36.280
day life I'm honored to have him we are
 

00:06:36.280 --> 00:06:38.180
day life I'm honored to have him we are
all honored to have him deliver this

00:06:38.180 --> 00:06:38.190
all honored to have him deliver this
 

00:06:38.190 --> 00:06:41.310
all honored to have him deliver this
neural lecture at the K announced

00:06:41.310 --> 00:06:41.320
neural lecture at the K announced
 

00:06:41.320 --> 00:06:43.740
neural lecture at the K announced
distinguished lecture series Eric

00:06:43.740 --> 00:06:43.750
distinguished lecture series Eric
 

00:06:43.750 --> 00:06:50.460
distinguished lecture series Eric
welcome thank you very much

00:06:50.460 --> 00:06:50.470
 
 

00:06:50.470 --> 00:06:52.930
 
well if I ever need a campaign director

00:06:52.930 --> 00:06:52.940
well if I ever need a campaign director
 

00:06:52.940 --> 00:06:54.780
well if I ever need a campaign director
political office I've know where to look

00:06:54.780 --> 00:06:54.790
political office I've know where to look
 

00:06:54.790 --> 00:06:57.850
political office I've know where to look
so I thought I'd go back a bit to why

00:06:57.850 --> 00:06:57.860
so I thought I'd go back a bit to why
 

00:06:57.860 --> 00:06:59.980
so I thought I'd go back a bit to why
I'm so excited about AI in open world it

00:06:59.980 --> 00:06:59.990
I'm so excited about AI in open world it
 

00:06:59.990 --> 00:07:01.360
I'm so excited about AI in open world it
all started with my reading as an

00:07:01.360 --> 00:07:01.370
all started with my reading as an
 

00:07:01.370 --> 00:07:04.110
all started with my reading as an
undergraduate of herb Symons writings

00:07:04.110 --> 00:07:04.120
undergraduate of herb Symons writings
 

00:07:04.120 --> 00:07:07.020
undergraduate of herb Symons writings
about how people in AI artifacts are

00:07:07.020 --> 00:07:07.030
about how people in AI artifacts are
 

00:07:07.030 --> 00:07:09.640
about how people in AI artifacts are
relatively simple systems emerged

00:07:09.640 --> 00:07:09.650
relatively simple systems emerged
 

00:07:09.650 --> 00:07:12.340
relatively simple systems emerged
immersed in large complex environments

00:07:12.340 --> 00:07:12.350
immersed in large complex environments
 

00:07:12.350 --> 00:07:14.910
immersed in large complex environments
and therefore we must contend with

00:07:14.910 --> 00:07:14.920
and therefore we must contend with
 

00:07:14.920 --> 00:07:16.960
and therefore we must contend with
inescapable uncertainties in

00:07:16.960 --> 00:07:16.970
inescapable uncertainties in
 

00:07:16.970 --> 00:07:19.060
inescapable uncertainties in
incompleteness and this raised lots of

00:07:19.060 --> 00:07:19.070
incompleteness and this raised lots of
 

00:07:19.070 --> 00:07:20.140
incompleteness and this raised lots of
questions for me that I took into my

00:07:20.140 --> 00:07:20.150
questions for me that I took into my
 

00:07:20.150 --> 00:07:22.600
questions for me that I took into my
graduate work about what were technical

00:07:22.600 --> 00:07:22.610
graduate work about what were technical
 

00:07:22.610 --> 00:07:25.330
graduate work about what were technical
paths forward in this world going from a

00:07:25.330 --> 00:07:25.340
paths forward in this world going from a
 

00:07:25.340 --> 00:07:27.310
paths forward in this world going from a
lab environment into the open world what

00:07:27.310 --> 00:07:27.320
lab environment into the open world what
 

00:07:27.320 --> 00:07:28.780
lab environment into the open world what
would that take in the future in

00:07:28.780 --> 00:07:28.790
would that take in the future in
 

00:07:28.790 --> 00:07:31.030
would that take in the future in
building medical systems that work just

00:07:31.030 --> 00:07:31.040
building medical systems that work just
 

00:07:31.040 --> 00:07:33.700
building medical systems that work just
fine in the lab what happened in the

00:07:33.700 --> 00:07:33.710
fine in the lab what happened in the
 

00:07:33.710 --> 00:07:36.450
fine in the lab what happened in the
hospital and in the doctor's office and

00:07:36.450 --> 00:07:36.460
hospital and in the doctor's office and
 

00:07:36.460 --> 00:07:38.860
hospital and in the doctor's office and
in sorry to field technologies and

00:07:38.860 --> 00:07:38.870
in sorry to field technologies and
 

00:07:38.870 --> 00:07:40.540
in sorry to field technologies and
watching them work with people and

00:07:40.540 --> 00:07:40.550
watching them work with people and
 

00:07:40.550 --> 00:07:43.380
watching them work with people and
people work with them this other

00:07:43.380 --> 00:07:43.390
people work with them this other
 

00:07:43.390 --> 00:07:46.090
people work with them this other
interesting challenge or set of

00:07:46.090 --> 00:07:46.100
interesting challenge or set of
 

00:07:46.100 --> 00:07:47.680
interesting challenge or set of
challenge problems came to the fore on

00:07:47.680 --> 00:07:47.690
challenge problems came to the fore on
 

00:07:47.690 --> 00:07:49.930
challenge problems came to the fore on
societal influences and practices and

00:07:49.930 --> 00:07:49.940
societal influences and practices and
 

00:07:49.940 --> 00:07:54.940
societal influences and practices and
policies the open world AI problem that

00:07:54.940 --> 00:07:54.950
policies the open world AI problem that
 

00:07:54.950 --> 00:07:56.230
policies the open world AI problem that
was a much broader one that I had been

00:07:56.230 --> 00:07:56.240
was a much broader one that I had been
 

00:07:56.240 --> 00:08:00.070
was a much broader one that I had been
used to now when I took on the trip the

00:08:00.070 --> 00:08:00.080
used to now when I took on the trip the
 

00:08:00.080 --> 00:08:02.140
used to now when I took on the trip the
presidency of the triple AI the the

00:08:02.140 --> 00:08:02.150
presidency of the triple AI the the
 

00:08:02.150 --> 00:08:04.210
presidency of the triple AI the the
larger society of AI scientists in the

00:08:04.210 --> 00:08:04.220
larger society of AI scientists in the
 

00:08:04.220 --> 00:08:04.690
larger society of AI scientists in the
world

00:08:04.690 --> 00:08:04.700
world
 

00:08:04.700 --> 00:08:08.110
world
I made the theme of my presidency AI in

00:08:08.110 --> 00:08:08.120
I made the theme of my presidency AI in
 

00:08:08.120 --> 00:08:10.690
I made the theme of my presidency AI in
the open world and as part of that

00:08:10.690 --> 00:08:10.700
the open world and as part of that
 

00:08:10.700 --> 00:08:12.310
the open world and as part of that
although most of my talk and I was a

00:08:12.310 --> 00:08:12.320
although most of my talk and I was a
 

00:08:12.320 --> 00:08:14.170
although most of my talk and I was a
slide from that lecture that I gave the

00:08:14.170 --> 00:08:14.180
slide from that lecture that I gave the
 

00:08:14.180 --> 00:08:15.730
slide from that lecture that I gave the
presidential lecture that presidents

00:08:15.730 --> 00:08:15.740
presidential lecture that presidents
 

00:08:15.740 --> 00:08:19.060
presidential lecture that presidents
gave at Triple A I focused mostly on

00:08:19.060 --> 00:08:19.070
gave at Triple A I focused mostly on
 

00:08:19.070 --> 00:08:20.800
gave at Triple A I focused mostly on
technical issues that's really exciting

00:08:20.800 --> 00:08:20.810
technical issues that's really exciting
 

00:08:20.810 --> 00:08:23.290
technical issues that's really exciting
questions you have to ask about what's

00:08:23.290 --> 00:08:23.300
questions you have to ask about what's
 

00:08:23.300 --> 00:08:24.760
questions you have to ask about what's
the system helping you get systems

00:08:24.760 --> 00:08:24.770
the system helping you get systems
 

00:08:24.770 --> 00:08:26.290
the system helping you get systems
self-knowledge that it doesn't know

00:08:26.290 --> 00:08:26.300
self-knowledge that it doesn't know
 

00:08:26.300 --> 00:08:26.950
self-knowledge that it doesn't know
everything

00:08:26.950 --> 00:08:26.960
everything
 

00:08:26.960 --> 00:08:28.930
everything
that it has blind spots when it works in

00:08:28.930 --> 00:08:28.940
that it has blind spots when it works in
 

00:08:28.940 --> 00:08:30.820
that it has blind spots when it works in
the world and learns to become better

00:08:30.820 --> 00:08:30.830
the world and learns to become better
 

00:08:30.830 --> 00:08:32.890
the world and learns to become better
for example or knows what to ask people

00:08:32.890 --> 00:08:32.900
for example or knows what to ask people
 

00:08:32.900 --> 00:08:36.969
for example or knows what to ask people
for help I also asked and made a

00:08:36.969 --> 00:08:36.979
for help I also asked and made a
 

00:08:36.979 --> 00:08:38.290
for help I also asked and made a
commitment that I would bring together

00:08:38.290 --> 00:08:38.300
commitment that I would bring together
 

00:08:38.300 --> 00:08:41.230
commitment that I would bring together
some great computer scientists probably

00:08:41.230 --> 00:08:41.240
some great computer scientists probably
 

00:08:41.240 --> 00:08:43.030
some great computer scientists probably
the first time we assembled AI research

00:08:43.030 --> 00:08:43.040
the first time we assembled AI research
 

00:08:43.040 --> 00:08:44.650
the first time we assembled AI research
scientists to ask questions about

00:08:44.650 --> 00:08:44.660
scientists to ask questions about
 

00:08:44.660 --> 00:08:46.930
scientists to ask questions about
long-term AI features at the

00:08:46.930 --> 00:08:46.940
long-term AI features at the
 

00:08:46.940 --> 00:08:49.050
long-term AI features at the
intersection of AI people and society

00:08:49.050 --> 00:08:49.060
intersection of AI people and society
 

00:08:49.060 --> 00:08:52.030
intersection of AI people and society
now the front page of the New York Times

00:08:52.030 --> 00:08:52.040
now the front page of the New York Times
 

00:08:52.040 --> 00:08:53.620
now the front page of the New York Times
reported this about the meeting that we

00:08:53.620 --> 00:08:53.630
reported this about the meeting that we
 

00:08:53.630 --> 00:08:55.900
reported this about the meeting that we
had after several months of breakout

00:08:55.900 --> 00:08:55.910
had after several months of breakout
 

00:08:55.910 --> 00:08:58.810
had after several months of breakout
groups and so on but while these topics

00:08:58.810 --> 00:08:58.820
groups and so on but while these topics
 

00:08:58.820 --> 00:09:01.060
groups and so on but while these topics
were touched upon the future of machines

00:09:01.060 --> 00:09:01.070
were touched upon the future of machines
 

00:09:01.070 --> 00:09:02.830
were touched upon the future of machines
that might become smarter than us and

00:09:02.830 --> 00:09:02.840
that might become smarter than us and
 

00:09:02.840 --> 00:09:05.650
that might become smarter than us and
and and and not let us intervene we

00:09:05.650 --> 00:09:05.660
and and and not let us intervene we
 

00:09:05.660 --> 00:09:07.960
and and and not let us intervene we
mostly focused on short-term disruptions

00:09:07.960 --> 00:09:07.970
mostly focused on short-term disruptions
 

00:09:07.970 --> 00:09:11.970
mostly focused on short-term disruptions
and possibilities ethics challenges and

00:09:11.970 --> 00:09:11.980
and possibilities ethics challenges and
 

00:09:11.980 --> 00:09:16.690
and possibilities ethics challenges and
legal challenges and questions and it

00:09:16.690 --> 00:09:16.700
legal challenges and questions and it
 

00:09:16.700 --> 00:09:18.910
legal challenges and questions and it
was a very rich set of findings

00:09:18.910 --> 00:09:18.920
was a very rich set of findings
 

00:09:18.920 --> 00:09:23.100
was a very rich set of findings
now that was February of 2009 and in

00:09:23.100 --> 00:09:23.110
now that was February of 2009 and in
 

00:09:23.110 --> 00:09:26.560
now that was February of 2009 and in
July of 2009 at Microsoft Research geoff

00:09:26.560 --> 00:09:26.570
July of 2009 at Microsoft Research geoff
 

00:09:26.570 --> 00:09:28.329
July of 2009 at Microsoft Research geoff
hinton and two graduate students

00:09:28.329 --> 00:09:28.339
hinton and two graduate students
 

00:09:28.339 --> 00:09:30.940
hinton and two graduate students
visiting us started looking at neural

00:09:30.940 --> 00:09:30.950
visiting us started looking at neural
 

00:09:30.950 --> 00:09:33.490
visiting us started looking at neural
net systems of the form that Geoff had

00:09:33.490 --> 00:09:33.500
net systems of the form that Geoff had
 

00:09:33.500 --> 00:09:35.440
net systems of the form that Geoff had
been looking at all along but had come

00:09:35.440 --> 00:09:35.450
been looking at all along but had come
 

00:09:35.450 --> 00:09:37.930
been looking at all along but had come
into disfavor given that they didn't

00:09:37.930 --> 00:09:37.940
into disfavor given that they didn't
 

00:09:37.940 --> 00:09:39.610
into disfavor given that they didn't
work as well as some of the traditional

00:09:39.610 --> 00:09:39.620
work as well as some of the traditional
 

00:09:39.620 --> 00:09:41.680
work as well as some of the traditional
machine learning methods but we

00:09:41.680 --> 00:09:41.690
machine learning methods but we
 

00:09:41.690 --> 00:09:43.450
machine learning methods but we
discovered that summer that these

00:09:43.450 --> 00:09:43.460
discovered that summer that these
 

00:09:43.460 --> 00:09:46.300
discovered that summer that these
methods were simply famished for data

00:09:46.300 --> 00:09:46.310
methods were simply famished for data
 

00:09:46.310 --> 00:09:49.300
methods were simply famished for data
all those years and when we fed them

00:09:49.300 --> 00:09:49.310
all those years and when we fed them
 

00:09:49.310 --> 00:09:51.880
all those years and when we fed them
data all of a sudden we had speech

00:09:51.880 --> 00:09:51.890
data all of a sudden we had speech
 

00:09:51.890 --> 00:09:54.329
data all of a sudden we had speech
recognition that actually worked I

00:09:54.329 --> 00:09:54.339
recognition that actually worked I
 

00:09:54.339 --> 00:09:56.290
recognition that actually worked I
shouldn't say that that way cuz Roger

00:09:56.290 --> 00:09:56.300
shouldn't say that that way cuz Roger
 

00:09:56.300 --> 00:09:58.740
shouldn't say that that way cuz Roger
Eddy's in the audience but worked well

00:09:58.740 --> 00:09:58.750
Eddy's in the audience but worked well
 

00:09:58.750 --> 00:10:03.579
Eddy's in the audience but worked well
to the level of human Trent professional

00:10:03.579 --> 00:10:03.589
to the level of human Trent professional
 

00:10:03.589 --> 00:10:06.730
to the level of human Trent professional
transcriptionist today same methods

00:10:06.730 --> 00:10:06.740
transcriptionist today same methods
 

00:10:06.740 --> 00:10:10.210
transcriptionist today same methods
applied to the image object recognition

00:10:10.210 --> 00:10:10.220
applied to the image object recognition
 

00:10:10.220 --> 00:10:13.840
applied to the image object recognition
and image classification gave us object

00:10:13.840 --> 00:10:13.850
and image classification gave us object
 

00:10:13.850 --> 00:10:16.630
and image classification gave us object
recognition recently at for the data set

00:10:16.630 --> 00:10:16.640
recognition recently at for the data set
 

00:10:16.640 --> 00:10:19.510
recognition recently at for the data set
that's being looked at human level as

00:10:19.510 --> 00:10:19.520
that's being looked at human level as
 

00:10:19.520 --> 00:10:20.890
that's being looked at human level as
well as reading challenges and other

00:10:20.890 --> 00:10:20.900
well as reading challenges and other
 

00:10:20.900 --> 00:10:22.870
well as reading challenges and other
challenges using the same basic set of

00:10:22.870 --> 00:10:22.880
challenges using the same basic set of
 

00:10:22.880 --> 00:10:26.020
challenges using the same basic set of
methods that are really being fed with

00:10:26.020 --> 00:10:26.030
methods that are really being fed with
 

00:10:26.030 --> 00:10:29.260
methods that are really being fed with
incredible quantities of data and beyond

00:10:29.260 --> 00:10:29.270
incredible quantities of data and beyond
 

00:10:29.270 --> 00:10:31.090
incredible quantities of data and beyond
simple wedges we're beginning to

00:10:31.090 --> 00:10:31.100
simple wedges we're beginning to
 

00:10:31.100 --> 00:10:33.970
simple wedges we're beginning to
assemble pipelines that involve several

00:10:33.970 --> 00:10:33.980
assemble pipelines that involve several
 

00:10:33.980 --> 00:10:36.700
assemble pipelines that involve several
different kinds of methods natural

00:10:36.700 --> 00:10:36.710
different kinds of methods natural
 

00:10:36.710 --> 00:10:38.740
different kinds of methods natural
language analyses combined with vision

00:10:38.740 --> 00:10:38.750
language analyses combined with vision
 

00:10:38.750 --> 00:10:42.010
language analyses combined with vision
to give us automated captioning systems

00:10:42.010 --> 00:10:42.020
to give us automated captioning systems
 

00:10:42.020 --> 00:10:45.160
to give us automated captioning systems
like you see here and the R&amp;D continues

00:10:45.160 --> 00:10:45.170
like you see here and the R&amp;D continues
 

00:10:45.170 --> 00:10:47.380
like you see here and the R&amp;D continues
now with recognizing behaviors from

00:10:47.380 --> 00:10:47.390
now with recognizing behaviors from
 

00:10:47.390 --> 00:10:52.480
now with recognizing behaviors from
video for example now these interesting

00:10:52.480 --> 00:10:52.490
video for example now these interesting
 

00:10:52.490 --> 00:10:55.420
video for example now these interesting
new developments have been pressed very

00:10:55.420 --> 00:10:55.430
new developments have been pressed very
 

00:10:55.430 --> 00:10:57.040
new developments have been pressed very
quickly into service by the big

00:10:57.040 --> 00:10:57.050
quickly into service by the big
 

00:10:57.050 --> 00:10:59.530
quickly into service by the big
companies and by in academia and by

00:10:59.530 --> 00:10:59.540
companies and by in academia and by
 

00:10:59.540 --> 00:11:01.449
companies and by in academia and by
people that the maker community for

00:11:01.449 --> 00:11:01.459
people that the maker community for
 

00:11:01.459 --> 00:11:03.130
people that the maker community for
example described translator now

00:11:03.130 --> 00:11:03.140
example described translator now
 

00:11:03.140 --> 00:11:05.170
example described translator now
translates between languages speech to

00:11:05.170 --> 00:11:05.180
translates between languages speech to
 

00:11:05.180 --> 00:11:07.570
translates between languages speech to
speech in a methodology that just a few

00:11:07.570 --> 00:11:07.580
speech in a methodology that just a few
 

00:11:07.580 --> 00:11:09.730
speech in a methodology that just a few
years ago had us at the edge of our

00:11:09.730 --> 00:11:09.740
years ago had us at the edge of our
 

00:11:09.740 --> 00:11:12.070
years ago had us at the edge of our
seats with prototypes that barely worked

00:11:12.070 --> 00:11:12.080
seats with prototypes that barely worked
 

00:11:12.080 --> 00:11:15.220
seats with prototypes that barely worked
and there are widely available tools now

00:11:15.220 --> 00:11:15.230
and there are widely available tools now
 

00:11:15.230 --> 00:11:16.130
and there are widely available tools now
the

00:11:16.130 --> 00:11:16.140
the
 

00:11:16.140 --> 00:11:18.770
the
to be used and programmed with by almost

00:11:18.770 --> 00:11:18.780
to be used and programmed with by almost
 

00:11:18.780 --> 00:11:21.320
to be used and programmed with by almost
anybody that give us the power in

00:11:21.320 --> 00:11:21.330
anybody that give us the power in
 

00:11:21.330 --> 00:11:26.030
anybody that give us the power in
software to see faces recognize gender

00:11:26.030 --> 00:11:26.040
software to see faces recognize gender
 

00:11:26.040 --> 00:11:32.150
software to see faces recognize gender
and emotion and so on now that said the

00:11:32.150 --> 00:11:32.160
and emotion and so on now that said the
 

00:11:32.160 --> 00:11:34.310
and emotion and so on now that said the
reason I'm so excited about this field

00:11:34.310 --> 00:11:34.320
reason I'm so excited about this field
 

00:11:34.320 --> 00:11:38.000
reason I'm so excited about this field
these days even beyond some of the the

00:11:38.000 --> 00:11:38.010
these days even beyond some of the the
 

00:11:38.010 --> 00:11:41.840
these days even beyond some of the the
foundations of intelligence and the

00:11:41.840 --> 00:11:41.850
foundations of intelligence and the
 

00:11:41.850 --> 00:11:44.660
foundations of intelligence and the
questions there which I should say to be

00:11:44.660 --> 00:11:44.670
questions there which I should say to be
 

00:11:44.670 --> 00:11:47.240
questions there which I should say to be
honest most of which remain unanswered

00:11:47.240 --> 00:11:47.250
honest most of which remain unanswered
 

00:11:47.250 --> 00:11:49.880
honest most of which remain unanswered
which makes it very exciting the

00:11:49.880 --> 00:11:49.890
which makes it very exciting the
 

00:11:49.890 --> 00:11:51.320
which makes it very exciting the
opportunities to have big influence in

00:11:51.320 --> 00:11:51.330
opportunities to have big influence in
 

00:11:51.330 --> 00:11:53.990
opportunities to have big influence in
the world are huge so this shows you

00:11:53.990 --> 00:11:54.000
the world are huge so this shows you
 

00:11:54.000 --> 00:11:55.940
the world are huge so this shows you
this readmissions predictor we built

00:11:55.940 --> 00:11:55.950
this readmissions predictor we built
 

00:11:55.950 --> 00:11:57.500
this readmissions predictor we built
several years ago and shipped around the

00:11:57.500 --> 00:11:57.510
several years ago and shipped around the
 

00:11:57.510 --> 00:12:00.980
several years ago and shipped around the
world for Microsoft Research at the time

00:12:00.980 --> 00:12:00.990
world for Microsoft Research at the time
 

00:12:00.990 --> 00:12:03.680
world for Microsoft Research at the time
we knew that 20% of Medicare reimbursed

00:12:03.680 --> 00:12:03.690
we knew that 20% of Medicare reimbursed
 

00:12:03.690 --> 00:12:05.900
we knew that 20% of Medicare reimbursed
patients were bouncing back to hospitals

00:12:05.900 --> 00:12:05.910
patients were bouncing back to hospitals
 

00:12:05.910 --> 00:12:09.260
patients were bouncing back to hospitals
within a month and the estimates for the

00:12:09.260 --> 00:12:09.270
within a month and the estimates for the
 

00:12:09.270 --> 00:12:11.450
within a month and the estimates for the
costs of preventable readmissions were

00:12:11.450 --> 00:12:11.460
costs of preventable readmissions were
 

00:12:11.460 --> 00:12:14.210
costs of preventable readmissions were
20 billion dollars in the US alone why

00:12:14.210 --> 00:12:14.220
20 billion dollars in the US alone why
 

00:12:14.220 --> 00:12:15.590
20 billion dollars in the US alone why
wouldn't we want to build a system that

00:12:15.590 --> 00:12:15.600
wouldn't we want to build a system that
 

00:12:15.600 --> 00:12:18.170
wouldn't we want to build a system that
could learn to predict which patients at

00:12:18.170 --> 00:12:18.180
could learn to predict which patients at
 

00:12:18.180 --> 00:12:19.640
could learn to predict which patients at
discharge time are going to bounce back

00:12:19.640 --> 00:12:19.650
discharge time are going to bounce back
 

00:12:19.650 --> 00:12:22.970
discharge time are going to bounce back
to give him special care monitoring and

00:12:22.970 --> 00:12:22.980
to give him special care monitoring and
 

00:12:22.980 --> 00:12:25.750
to give him special care monitoring and
packages that would keep them healthier

00:12:25.750 --> 00:12:25.760
packages that would keep them healthier
 

00:12:25.760 --> 00:12:29.020
packages that would keep them healthier
this brings up this issue about ethics I

00:12:29.020 --> 00:12:29.030
this brings up this issue about ethics I
 

00:12:29.030 --> 00:12:33.380
this brings up this issue about ethics I
believe we have a ethical imperative to

00:12:33.380 --> 00:12:33.390
believe we have a ethical imperative to
 

00:12:33.390 --> 00:12:37.430
believe we have a ethical imperative to
apply AI to the best that we can to save

00:12:37.430 --> 00:12:37.440
apply AI to the best that we can to save
 

00:12:37.440 --> 00:12:39.410
apply AI to the best that we can to save
lives and to raise the quality of life

00:12:39.410 --> 00:12:39.420
lives and to raise the quality of life
 

00:12:39.420 --> 00:12:42.830
lives and to raise the quality of life
where we can several years ago just a

00:12:42.830 --> 00:12:42.840
where we can several years ago just a
 

00:12:42.840 --> 00:12:44.030
where we can several years ago just a
few years ago the British Medical

00:12:44.030 --> 00:12:44.040
few years ago the British Medical
 

00:12:44.040 --> 00:12:47.270
few years ago the British Medical
Journal estimated and this confirmed

00:12:47.270 --> 00:12:47.280
Journal estimated and this confirmed
 

00:12:47.280 --> 00:12:49.970
Journal estimated and this confirmed
other studies that the third most common

00:12:49.970 --> 00:12:49.980
other studies that the third most common
 

00:12:49.980 --> 00:12:53.680
other studies that the third most common
cause of death in the United States is

00:12:53.680 --> 00:12:53.690
cause of death in the United States is
 

00:12:53.690 --> 00:12:57.800
cause of death in the United States is
preventable error in hospitals right

00:12:57.800 --> 00:12:57.810
preventable error in hospitals right
 

00:12:57.810 --> 00:13:01.640
preventable error in hospitals right
after heart disease in cancer it's like

00:13:01.640 --> 00:13:01.650
after heart disease in cancer it's like
 

00:13:01.650 --> 00:13:05.300
after heart disease in cancer it's like
a city the size of Oakland quietly

00:13:05.300 --> 00:13:05.310
a city the size of Oakland quietly
 

00:13:05.310 --> 00:13:08.270
a city the size of Oakland quietly
disappearing every year or Miami without

00:13:08.270 --> 00:13:08.280
disappearing every year or Miami without
 

00:13:08.280 --> 00:13:11.690
disappearing every year or Miami without
any front page news about it powerful

00:13:11.690 --> 00:13:11.700
any front page news about it powerful
 

00:13:11.700 --> 00:13:14.050
any front page news about it powerful
she asked the question could a I systems

00:13:14.050 --> 00:13:14.060
she asked the question could a I systems
 

00:13:14.060 --> 00:13:16.430
she asked the question could a I systems
augment or extend human intelligence to

00:13:16.430 --> 00:13:16.440
augment or extend human intelligence to
 

00:13:16.440 --> 00:13:19.040
augment or extend human intelligence to
be like the the safety net under the

00:13:19.040 --> 00:13:19.050
be like the the safety net under the
 

00:13:19.050 --> 00:13:21.470
be like the the safety net under the
bridge workers catching errors when they

00:13:21.470 --> 00:13:21.480
bridge workers catching errors when they
 

00:13:21.480 --> 00:13:23.930
bridge workers catching errors when they
could happen now here's an example of

00:13:23.930 --> 00:13:23.940
could happen now here's an example of
 

00:13:23.940 --> 00:13:25.010
could happen now here's an example of
work like that just give you a sense

00:13:25.010 --> 00:13:25.020
work like that just give you a sense
 

00:13:25.020 --> 00:13:26.390
work like that just give you a sense
before we head into the rough edges of

00:13:26.390 --> 00:13:26.400
before we head into the rough edges of
 

00:13:26.400 --> 00:13:27.950
before we head into the rough edges of
AI thought we should start by talking

00:13:27.950 --> 00:13:27.960
AI thought we should start by talking
 

00:13:27.960 --> 00:13:29.059
AI thought we should start by talking
about the upside

00:13:29.059 --> 00:13:29.069
about the upside
 

00:13:29.069 --> 00:13:31.859
about the upside
this is a model that we built working

00:13:31.859 --> 00:13:31.869
this is a model that we built working
 

00:13:31.869 --> 00:13:33.329
this is a model that we built working
with this is emotion by arty and mark

00:13:33.329 --> 00:13:33.339
with this is emotion by arty and mark
 

00:13:33.339 --> 00:13:36.389
with this is emotion by arty and mark
Braverman two postdocs at Microsoft and

00:13:36.389 --> 00:13:36.399
Braverman two postdocs at Microsoft and
 

00:13:36.399 --> 00:13:38.460
Braverman two postdocs at Microsoft and
Jason Gatewood an emergency room

00:13:38.460 --> 00:13:38.470
Jason Gatewood an emergency room
 

00:13:38.470 --> 00:13:41.100
Jason Gatewood an emergency room
physician at a large urban hospital

00:13:41.100 --> 00:13:41.110
physician at a large urban hospital
 

00:13:41.110 --> 00:13:43.889
physician at a large urban hospital
using 20 years of data and here's the

00:13:43.889 --> 00:13:43.899
using 20 years of data and here's the
 

00:13:43.899 --> 00:13:47.369
using 20 years of data and here's the
nice nature of the training set for an

00:13:47.369 --> 00:13:47.379
nice nature of the training set for an
 

00:13:47.379 --> 00:13:50.069
nice nature of the training set for an
emergency room department Andy look at

00:13:50.069 --> 00:13:50.079
emergency room department Andy look at
 

00:13:50.079 --> 00:13:52.470
emergency room department Andy look at
all patients that were discharged and

00:13:52.470 --> 00:13:52.480
all patients that were discharged and
 

00:13:52.480 --> 00:13:54.809
all patients that were discharged and
came back within 48 hours to the

00:13:54.809 --> 00:13:54.819
came back within 48 hours to the
 

00:13:54.819 --> 00:13:57.900
came back within 48 hours to the
hospital within two days were admitted

00:13:57.900 --> 00:13:57.910
hospital within two days were admitted
 

00:13:57.910 --> 00:13:58.799
hospital within two days were admitted
to the hospital

00:13:58.799 --> 00:13:58.809
to the hospital
 

00:13:58.809 --> 00:14:00.780
to the hospital
with a significant problem as an

00:14:00.780 --> 00:14:00.790
with a significant problem as an
 

00:14:00.790 --> 00:14:04.049
with a significant problem as an
inpatient and the primary diagnosis was

00:14:04.049 --> 00:14:04.059
inpatient and the primary diagnosis was
 

00:14:04.059 --> 00:14:05.249
inpatient and the primary diagnosis was
nowhere on the chart

00:14:05.249 --> 00:14:05.259
nowhere on the chart
 

00:14:05.259 --> 00:14:06.689
nowhere on the chart
when they were discharged from the

00:14:06.689 --> 00:14:06.699
when they were discharged from the
 

00:14:06.699 --> 00:14:10.169
when they were discharged from the
emergency room I would call that an

00:14:10.169 --> 00:14:10.179
emergency room I would call that an
 

00:14:10.179 --> 00:14:13.319
emergency room I would call that an
expert human surprise something that

00:14:13.319 --> 00:14:13.329
expert human surprise something that
 

00:14:13.329 --> 00:14:14.850
expert human surprise something that
support would surprise most you know

00:14:14.850 --> 00:14:14.860
support would surprise most you know
 

00:14:14.860 --> 00:14:16.710
support would surprise most you know
yesterday this was something was hiding

00:14:16.710 --> 00:14:16.720
yesterday this was something was hiding
 

00:14:16.720 --> 00:14:18.480
yesterday this was something was hiding
in the cognitive shadows for example of

00:14:18.480 --> 00:14:18.490
in the cognitive shadows for example of
 

00:14:18.490 --> 00:14:21.449
in the cognitive shadows for example of
the expert so when this model runs it's

00:14:21.449 --> 00:14:21.459
the expert so when this model runs it's
 

00:14:21.459 --> 00:14:23.059
the expert so when this model runs it's
not replicating what the human knows

00:14:23.059 --> 00:14:23.069
not replicating what the human knows
 

00:14:23.069 --> 00:14:25.169
not replicating what the human knows
it's thinking about what the human

00:14:25.169 --> 00:14:25.179
it's thinking about what the human
 

00:14:25.179 --> 00:14:26.970
it's thinking about what the human
doesn't know what's at the frontier of

00:14:26.970 --> 00:14:26.980
doesn't know what's at the frontier of
 

00:14:26.980 --> 00:14:30.150
doesn't know what's at the frontier of
its experts knowledge and when it says

00:14:30.150 --> 00:14:30.160
its experts knowledge and when it says
 

00:14:30.160 --> 00:14:32.970
its experts knowledge and when it says
hey before you let this patient go this

00:14:32.970 --> 00:14:32.980
hey before you let this patient go this
 

00:14:32.980 --> 00:14:34.949
hey before you let this patient go this
might surprise you do you want to see

00:14:34.949 --> 00:14:34.959
might surprise you do you want to see
 

00:14:34.959 --> 00:14:36.749
might surprise you do you want to see
what I'm thinking a physician might

00:14:36.749 --> 00:14:36.759
what I'm thinking a physician might
 

00:14:36.759 --> 00:14:39.809
what I'm thinking a physician might
listen carefully and this work has led

00:14:39.809 --> 00:14:39.819
listen carefully and this work has led
 

00:14:39.819 --> 00:14:41.939
listen carefully and this work has led
to richer work right now this is where

00:14:41.939 --> 00:14:41.949
to richer work right now this is where
 

00:14:41.949 --> 00:14:44.280
to richer work right now this is where
the student daily on a problem called

00:14:44.280 --> 00:14:44.290
the student daily on a problem called
 

00:14:44.290 --> 00:14:46.350
the student daily on a problem called
failure to rescue in hospitals that's a

00:14:46.350 --> 00:14:46.360
failure to rescue in hospitals that's a
 

00:14:46.360 --> 00:14:49.980
failure to rescue in hospitals that's a
really scary phrase which means death

00:14:49.980 --> 00:14:49.990
really scary phrase which means death
 

00:14:49.990 --> 00:14:51.989
really scary phrase which means death
happened in the hospital after a

00:14:51.989 --> 00:14:51.999
happened in the hospital after a
 

00:14:51.999 --> 00:14:53.460
happened in the hospital after a
treatable complication

00:14:53.460 --> 00:14:53.470
treatable complication
 

00:14:53.470 --> 00:14:55.230
treatable complication
set in which then led to a cascade

00:14:55.230 --> 00:14:55.240
set in which then led to a cascade
 

00:14:55.240 --> 00:14:58.530
set in which then led to a cascade
typically an organ system cascade on to

00:14:58.530 --> 00:14:58.540
typically an organ system cascade on to
 

00:14:58.540 --> 00:15:01.980
typically an organ system cascade on to
death now these two examples are

00:15:01.980 --> 00:15:01.990
death now these two examples are
 

00:15:01.990 --> 00:15:04.110
death now these two examples are
examples we're talking about AI designs

00:15:04.110 --> 00:15:04.120
examples we're talking about AI designs
 

00:15:04.120 --> 00:15:06.749
examples we're talking about AI designs
for complementarity where we reason

00:15:06.749 --> 00:15:06.759
for complementarity where we reason
 

00:15:06.759 --> 00:15:08.759
for complementarity where we reason
explicitly about the blind spots and

00:15:08.759 --> 00:15:08.769
explicitly about the blind spots and
 

00:15:08.769 --> 00:15:11.069
explicitly about the blind spots and
biases and gaps that people have in

00:15:11.069 --> 00:15:11.079
biases and gaps that people have in
 

00:15:11.079 --> 00:15:13.829
biases and gaps that people have in
their cognition and think about

00:15:13.829 --> 00:15:13.839
their cognition and think about
 

00:15:13.839 --> 00:15:16.039
their cognition and think about
explicitly how does machine intellect

00:15:16.039 --> 00:15:16.049
explicitly how does machine intellect
 

00:15:16.049 --> 00:15:18.869
explicitly how does machine intellect
work with people work with cognitive

00:15:18.869 --> 00:15:18.879
work with people work with cognitive
 

00:15:18.879 --> 00:15:21.210
work with people work with cognitive
psychology machine learning and planning

00:15:21.210 --> 00:15:21.220
psychology machine learning and planning
 

00:15:21.220 --> 00:15:23.939
psychology machine learning and planning
to extend our abilities designing for

00:15:23.939 --> 00:15:23.949
to extend our abilities designing for
 

00:15:23.949 --> 00:15:28.199
to extend our abilities designing for
complementarity many AI advances will be

00:15:28.199 --> 00:15:28.209
complementarity many AI advances will be
 

00:15:28.209 --> 00:15:30.509
complementarity many AI advances will be
applied in this way and make the world

00:15:30.509 --> 00:15:30.519
applied in this way and make the world
 

00:15:30.519 --> 00:15:34.199
applied in this way and make the world
better for it now that said just in like

00:15:34.199 --> 00:15:34.209
better for it now that said just in like
 

00:15:34.209 --> 00:15:35.999
better for it now that said just in like
if you think about just in three weeks

00:15:35.999 --> 00:15:36.009
if you think about just in three weeks
 

00:15:36.009 --> 00:15:38.759
if you think about just in three weeks
or so or four weeks these are actual

00:15:38.759 --> 00:15:38.769
or so or four weeks these are actual
 

00:15:38.769 --> 00:15:41.400
or so or four weeks these are actual
headlines

00:15:41.400 --> 00:15:41.410
 
 

00:15:41.410 --> 00:15:45.540
 
talking about the rough edges that we're

00:15:45.540 --> 00:15:45.550
talking about the rough edges that we're
 

00:15:45.550 --> 00:15:50.160
talking about the rough edges that we're
facing and critical problems with

00:15:50.160 --> 00:15:50.170
facing and critical problems with
 

00:15:50.170 --> 00:15:52.320
facing and critical problems with
advances in this field so here we have

00:15:52.320 --> 00:15:52.330
advances in this field so here we have
 

00:15:52.330 --> 00:15:54.480
advances in this field so here we have
on one hand their incredible promise

00:15:54.480 --> 00:15:54.490
on one hand their incredible promise
 

00:15:54.490 --> 00:15:57.270
on one hand their incredible promise
which I which is which is energizing and

00:15:57.270 --> 00:15:57.280
which I which is which is energizing and
 

00:15:57.280 --> 00:15:59.580
which I which is which is energizing and
exciting but we have to get this other

00:15:59.580 --> 00:15:59.590
exciting but we have to get this other
 

00:15:59.590 --> 00:16:02.280
exciting but we have to get this other
side right and do our best with it it's

00:16:02.280 --> 00:16:02.290
side right and do our best with it it's
 

00:16:02.290 --> 00:16:04.620
side right and do our best with it it's
critical so the way I like to abstract

00:16:04.620 --> 00:16:04.630
critical so the way I like to abstract
 

00:16:04.630 --> 00:16:06.750
critical so the way I like to abstract
some of these problems in my mind are in

00:16:06.750 --> 00:16:06.760
some of these problems in my mind are in
 

00:16:06.760 --> 00:16:10.080
some of these problems in my mind are in
terms of capabilities can we reason

00:16:10.080 --> 00:16:10.090
terms of capabilities can we reason
 

00:16:10.090 --> 00:16:13.290
terms of capabilities can we reason
about and work with the blind spots and

00:16:13.290 --> 00:16:13.300
about and work with the blind spots and
 

00:16:13.300 --> 00:16:15.000
about and work with the blind spots and
biases in our algorithms

00:16:15.000 --> 00:16:15.010
biases in our algorithms
 

00:16:15.010 --> 00:16:17.760
biases in our algorithms
they'll probably always be there how can

00:16:17.760 --> 00:16:17.770
they'll probably always be there how can
 

00:16:17.770 --> 00:16:19.890
they'll probably always be there how can
we work with them and minimize their

00:16:19.890 --> 00:16:19.900
we work with them and minimize their
 

00:16:19.900 --> 00:16:25.830
we work with them and minimize their
impact and their existence values what

00:16:25.830 --> 00:16:25.840
impact and their existence values what
 

00:16:25.840 --> 00:16:30.270
impact and their existence values what
are the utilities the values represented

00:16:30.270 --> 00:16:30.280
are the utilities the values represented
 

00:16:30.280 --> 00:16:31.470
are the utilities the values represented
in reasoning systems when they do

00:16:31.470 --> 00:16:31.480
in reasoning systems when they do
 

00:16:31.480 --> 00:16:34.710
in reasoning systems when they do
cost-benefit analyses and with the word

00:16:34.710 --> 00:16:34.720
cost-benefit analyses and with the word
 

00:16:34.720 --> 00:16:37.710
cost-benefit analyses and with the word
agency in the decision analysis field

00:16:37.710 --> 00:16:37.720
agency in the decision analysis field
 

00:16:37.720 --> 00:16:40.710
agency in the decision analysis field
we often say for a decision who's the

00:16:40.710 --> 00:16:40.720
we often say for a decision who's the
 

00:16:40.720 --> 00:16:42.750
we often say for a decision who's the
principal agent of that decision which

00:16:42.750 --> 00:16:42.760
principal agent of that decision which
 

00:16:42.760 --> 00:16:48.270
principal agent of that decision which
means who's whose resources and life is

00:16:48.270 --> 00:16:48.280
means who's whose resources and life is
 

00:16:48.280 --> 00:16:48.870
means who's whose resources and life is
at stake

00:16:48.870 --> 00:16:48.880
at stake
 

00:16:48.880 --> 00:16:52.560
at stake
who is making the decision a doctor

00:16:52.560 --> 00:16:52.570
who is making the decision a doctor
 

00:16:52.570 --> 00:16:54.600
who is making the decision a doctor
might be helping a patient but the

00:16:54.600 --> 00:16:54.610
might be helping a patient but the
 

00:16:54.610 --> 00:16:58.980
might be helping a patient but the
patient is the principle agent and then

00:16:58.980 --> 00:16:58.990
patient is the principle agent and then
 

00:16:58.990 --> 00:17:01.620
patient is the principle agent and then
misuse this is a very very rich and

00:17:01.620 --> 00:17:01.630
misuse this is a very very rich and
 

00:17:01.630 --> 00:17:03.750
misuse this is a very very rich and
intriguing area and this is we're gonna

00:17:03.750 --> 00:17:03.760
intriguing area and this is we're gonna
 

00:17:03.760 --> 00:17:05.760
intriguing area and this is we're gonna
require attention human rights

00:17:05.760 --> 00:17:05.770
require attention human rights
 

00:17:05.770 --> 00:17:07.860
require attention human rights
possibility by elections legal ethical

00:17:07.860 --> 00:17:07.870
possibility by elections legal ethical
 

00:17:07.870 --> 00:17:10.080
possibility by elections legal ethical
and privacy challenges physical and

00:17:10.080 --> 00:17:10.090
and privacy challenges physical and
 

00:17:10.090 --> 00:17:13.430
and privacy challenges physical and
emotional harms exclusion from

00:17:13.430 --> 00:17:13.440
emotional harms exclusion from
 

00:17:13.440 --> 00:17:17.280
emotional harms exclusion from
consequential services manipulation of

00:17:17.280 --> 00:17:17.290
consequential services manipulation of
 

00:17:17.290 --> 00:17:20.880
consequential services manipulation of
attention through capture engagement and

00:17:20.880 --> 00:17:20.890
attention through capture engagement and
 

00:17:20.890 --> 00:17:23.760
attention through capture engagement and
persuasion one thing we don't think

00:17:23.760 --> 00:17:23.770
persuasion one thing we don't think
 

00:17:23.770 --> 00:17:25.110
persuasion one thing we don't think
about a lot which actually I was happy

00:17:25.110 --> 00:17:25.120
about a lot which actually I was happy
 

00:17:25.120 --> 00:17:26.640
about a lot which actually I was happy
to hear came up this morning in the

00:17:26.640 --> 00:17:26.650
to hear came up this morning in the
 

00:17:26.650 --> 00:17:31.130
to hear came up this morning in the
first session is that and I asked myself

00:17:31.130 --> 00:17:31.140
first session is that and I asked myself
 

00:17:31.140 --> 00:17:34.350
first session is that and I asked myself
why early on in my career didn't I think

00:17:34.350 --> 00:17:34.360
why early on in my career didn't I think
 

00:17:34.360 --> 00:17:38.180
why early on in my career didn't I think
about the the rich deep societal

00:17:38.180 --> 00:17:38.190
about the the rich deep societal
 

00:17:38.190 --> 00:17:41.040
about the the rich deep societal
challenges and rough edges and it was

00:17:41.040 --> 00:17:41.050
challenges and rough edges and it was
 

00:17:41.050 --> 00:17:44.390
challenges and rough edges and it was
because we were building one-off systems

00:17:44.390 --> 00:17:44.400
because we were building one-off systems
 

00:17:44.400 --> 00:17:47.870
because we were building one-off systems
maybe we could sell a few at a startup

00:17:47.870 --> 00:17:47.880
maybe we could sell a few at a startup
 

00:17:47.880 --> 00:17:50.490
maybe we could sell a few at a startup
we could talk to the people who were

00:17:50.490 --> 00:17:50.500
we could talk to the people who were
 

00:17:50.500 --> 00:17:52.380
we could talk to the people who were
crazy enough to buy these crazy systems

00:17:52.380 --> 00:17:52.390
crazy enough to buy these crazy systems
 

00:17:52.390 --> 00:17:54.450
crazy enough to buy these crazy systems
and tell them how to use it and fill

00:17:54.450 --> 00:17:54.460
and tell them how to use it and fill
 

00:17:54.460 --> 00:17:55.260
and tell them how to use it and fill
them in on the down

00:17:55.260 --> 00:17:55.270
them in on the down
 

00:17:55.270 --> 00:17:58.680
them in on the down
and upsides but now we mean this is

00:17:58.680 --> 00:17:58.690
and upsides but now we mean this is
 

00:17:58.690 --> 00:18:01.200
and upsides but now we mean this is
often unstated we're in a world because

00:18:01.200 --> 00:18:01.210
often unstated we're in a world because
 

00:18:01.210 --> 00:18:02.700
often unstated we're in a world because
of how much the world change because of

00:18:02.700 --> 00:18:02.710
of how much the world change because of
 

00:18:02.710 --> 00:18:05.120
of how much the world change because of
the web and the connectivity it brought

00:18:05.120 --> 00:18:05.130
the web and the connectivity it brought
 

00:18:05.130 --> 00:18:11.190
the web and the connectivity it brought
of ease of replicating reusing anything

00:18:11.190 --> 00:18:11.200
of ease of replicating reusing anything
 

00:18:11.200 --> 00:18:14.070
of ease of replicating reusing anything
that we build at internet scale which

00:18:14.070 --> 00:18:14.080
that we build at internet scale which
 

00:18:14.080 --> 00:18:17.310
that we build at internet scale which
means that nuances in a single solution

00:18:17.310 --> 00:18:17.320
means that nuances in a single solution
 

00:18:17.320 --> 00:18:19.669
means that nuances in a single solution
will call a single solution Facebook

00:18:19.669 --> 00:18:19.679
will call a single solution Facebook
 

00:18:19.679 --> 00:18:23.270
will call a single solution Facebook
it's a single app have wide and deep

00:18:23.270 --> 00:18:23.280
it's a single app have wide and deep
 

00:18:23.280 --> 00:18:25.530
it's a single app have wide and deep
societal influences so in the back of my

00:18:25.530 --> 00:18:25.540
societal influences so in the back of my
 

00:18:25.540 --> 00:18:27.330
societal influences so in the back of my
mind I'm always thinking this when I

00:18:27.330 --> 00:18:27.340
mind I'm always thinking this when I
 

00:18:27.340 --> 00:18:29.490
mind I'm always thinking this when I
think about AI right it's like in the

00:18:29.490 --> 00:18:29.500
think about AI right it's like in the
 

00:18:29.500 --> 00:18:32.220
think about AI right it's like in the
back of my mind it means like we build

00:18:32.220 --> 00:18:32.230
back of my mind it means like we build
 

00:18:32.230 --> 00:18:36.080
back of my mind it means like we build
something and it could have big effects

00:18:36.080 --> 00:18:36.090
something and it could have big effects
 

00:18:36.090 --> 00:18:40.530
something and it could have big effects
worldwide that said let me just walk

00:18:40.530 --> 00:18:40.540
worldwide that said let me just walk
 

00:18:40.540 --> 00:18:42.360
worldwide that said let me just walk
through some of these issues and how I

00:18:42.360 --> 00:18:42.370
through some of these issues and how I
 

00:18:42.370 --> 00:18:44.490
through some of these issues and how I
think about them and maybe we can Mabel

00:18:44.490 --> 00:18:44.500
think about them and maybe we can Mabel
 

00:18:44.500 --> 00:18:46.130
think about them and maybe we can Mabel
get to do some Q&amp;A on this as well on

00:18:46.130 --> 00:18:46.140
get to do some Q&amp;A on this as well on
 

00:18:46.140 --> 00:18:51.330
get to do some Q&amp;A on this as well on
capability is entrust loud C was said to

00:18:51.330 --> 00:18:51.340
capability is entrust loud C was said to
 

00:18:51.340 --> 00:18:53.430
capability is entrust loud C was said to
have said the translation knowing that

00:18:53.430 --> 00:18:53.440
have said the translation knowing that
 

00:18:53.440 --> 00:18:56.460
have said the translation knowing that
you do not know is the best and the

00:18:56.460 --> 00:18:56.470
you do not know is the best and the
 

00:18:56.470 --> 00:18:59.250
you do not know is the best and the
translation goes not knowing that you do

00:18:59.250 --> 00:18:59.260
translation goes not knowing that you do
 

00:18:59.260 --> 00:19:03.180
translation goes not knowing that you do
not know is an illness can we give our

00:19:03.180 --> 00:19:03.190
not know is an illness can we give our
 

00:19:03.190 --> 00:19:05.990
not know is an illness can we give our
systems the ability to know and not know

00:19:05.990 --> 00:19:06.000
systems the ability to know and not know
 

00:19:06.000 --> 00:19:10.290
systems the ability to know and not know
so at the risk of sounding like Donald

00:19:10.290 --> 00:19:10.300
so at the risk of sounding like Donald
 

00:19:10.300 --> 00:19:16.080
so at the risk of sounding like Donald
Rumsfeld in there there are now systems

00:19:16.080 --> 00:19:16.090
Rumsfeld in there there are now systems
 

00:19:16.090 --> 00:19:18.330
Rumsfeld in there there are now systems
that are making life-and-death decisions

00:19:18.330 --> 00:19:18.340
that are making life-and-death decisions
 

00:19:18.340 --> 00:19:22.340
that are making life-and-death decisions
and you're seeing recent deaths about

00:19:22.340 --> 00:19:22.350
and you're seeing recent deaths about
 

00:19:22.350 --> 00:19:27.299
and you're seeing recent deaths about
perform about the physical world based

00:19:27.299 --> 00:19:27.309
perform about the physical world based
 

00:19:27.309 --> 00:19:29.640
perform about the physical world based
on perceptual streams so this is the

00:19:29.640 --> 00:19:29.650
on perceptual streams so this is the
 

00:19:29.650 --> 00:19:32.790
on perceptual streams so this is the
view I have in my Tesla I couldn't

00:19:32.790 --> 00:19:32.800
view I have in my Tesla I couldn't
 

00:19:32.800 --> 00:19:33.750
view I have in my Tesla I couldn't
resist buying one of these when that

00:19:33.750 --> 00:19:33.760
resist buying one of these when that
 

00:19:33.760 --> 00:19:35.100
resist buying one of these when that
came out given the fact that I was gonna

00:19:35.100 --> 00:19:35.110
came out given the fact that I was gonna
 

00:19:35.110 --> 00:19:37.380
came out given the fact that I was gonna
do self do it self-driving someday in

00:19:37.380 --> 00:19:37.390
do self do it self-driving someday in
 

00:19:37.390 --> 00:19:39.060
do self do it self-driving someday in
quotes from the salesperson and indeed

00:19:39.060 --> 00:19:39.070
quotes from the salesperson and indeed
 

00:19:39.070 --> 00:19:41.390
quotes from the salesperson and indeed
software came down and I started doing

00:19:41.390 --> 00:19:41.400
software came down and I started doing
 

00:19:41.400 --> 00:19:43.140
software came down and I started doing
automated driving with the autopilot

00:19:43.140 --> 00:19:43.150
automated driving with the autopilot
 

00:19:43.150 --> 00:19:47.190
automated driving with the autopilot
feature so when I'm driving along and

00:19:47.190 --> 00:19:47.200
feature so when I'm driving along and
 

00:19:47.200 --> 00:19:50.310
feature so when I'm driving along and
you might hear in an autopilot you see a

00:19:50.310 --> 00:19:50.320
you might hear in an autopilot you see a
 

00:19:50.320 --> 00:19:52.500
you might hear in an autopilot you see a
little view of the car and what the car

00:19:52.500 --> 00:19:52.510
little view of the car and what the car
 

00:19:52.510 --> 00:19:55.560
little view of the car and what the car
seeing the senses are seeing and at some

00:19:55.560 --> 00:19:55.570
seeing the senses are seeing and at some
 

00:19:55.570 --> 00:19:56.610
seeing the senses are seeing and at some
point in time the car might lose

00:19:56.610 --> 00:19:56.620
point in time the car might lose
 

00:19:56.620 --> 00:19:58.230
point in time the car might lose
confidence and what you see it you hear

00:19:58.230 --> 00:19:58.240
confidence and what you see it you hear
 

00:19:58.240 --> 00:20:00.900
confidence and what you see it you hear
an alarm will sound and you get a

00:20:00.900 --> 00:20:00.910
an alarm will sound and you get a
 

00:20:00.910 --> 00:20:02.730
an alarm will sound and you get a
message like this auto steer is no

00:20:02.730 --> 00:20:02.740
message like this auto steer is no
 

00:20:02.740 --> 00:20:04.260
message like this auto steer is no
longer steering model as confidently

00:20:04.260 --> 00:20:04.270
longer steering model as confidently
 

00:20:04.270 --> 00:20:06.570
longer steering model as confidently
take over and the car throws back

00:20:06.570 --> 00:20:06.580
take over and the car throws back
 

00:20:06.580 --> 00:20:08.250
take over and the car throws back
control to you no matter what your

00:20:08.250 --> 00:20:08.260
control to you no matter what your
 

00:20:08.260 --> 00:20:10.980
control to you no matter what your
you grab the wheel you know and this is

00:20:10.980 --> 00:20:10.990
you grab the wheel you know and this is
 

00:20:10.990 --> 00:20:14.280
you grab the wheel you know and this is
called unknown unknown because we we we

00:20:14.280 --> 00:20:14.290
called unknown unknown because we we we
 

00:20:14.290 --> 00:20:17.280
called unknown unknown because we we we
trust that the car has a well calibrated

00:20:17.280 --> 00:20:17.290
trust that the car has a well calibrated
 

00:20:17.290 --> 00:20:19.620
trust that the car has a well calibrated
model of confidence and when it doesn't

00:20:19.620 --> 00:20:19.630
model of confidence and when it doesn't
 

00:20:19.630 --> 00:20:23.190
model of confidence and when it doesn't
know something now think about the the

00:20:23.190 --> 00:20:23.200
know something now think about the the
 

00:20:23.200 --> 00:20:25.590
know something now think about the the
challenge of designing a car to be

00:20:25.590 --> 00:20:25.600
challenge of designing a car to be
 

00:20:25.600 --> 00:20:28.560
challenge of designing a car to be
self-driving do the people up the Tesla

00:20:28.560 --> 00:20:28.570
self-driving do the people up the Tesla
 

00:20:28.570 --> 00:20:32.310
self-driving do the people up the Tesla
factory or Ford or at Toyota really have

00:20:32.310 --> 00:20:32.320
factory or Ford or at Toyota really have
 

00:20:32.320 --> 00:20:33.600
factory or Ford or at Toyota really have
the ability to go through all

00:20:33.600 --> 00:20:33.610
the ability to go through all
 

00:20:33.610 --> 00:20:35.700
the ability to go through all
possibilities this one got me one night

00:20:35.700 --> 00:20:35.710
possibilities this one got me one night
 

00:20:35.710 --> 00:20:38.820
possibilities this one got me one night
late night zooming along on the highway

00:20:38.820 --> 00:20:38.830
late night zooming along on the highway
 

00:20:38.830 --> 00:20:41.040
late night zooming along on the highway
I saw cones in the distance while an

00:20:41.040 --> 00:20:41.050
I saw cones in the distance while an
 

00:20:41.050 --> 00:20:43.950
I saw cones in the distance while an
auto pilot and I wondered does the Tesla

00:20:43.950 --> 00:20:43.960
auto pilot and I wondered does the Tesla
 

00:20:43.960 --> 00:20:45.690
auto pilot and I wondered does the Tesla
know about cones that you know they'd

00:20:45.690 --> 00:20:45.700
know about cones that you know they'd
 

00:20:45.700 --> 00:20:48.530
know about cones that you know they'd
sweep across the road and close a lane I

00:20:48.530 --> 00:20:48.540
sweep across the road and close a lane I
 

00:20:48.540 --> 00:20:52.560
sweep across the road and close a lane I
waited and waited and then I said hands

00:20:52.560 --> 00:20:52.570
waited and waited and then I said hands
 

00:20:52.570 --> 00:20:54.420
waited and waited and then I said hands
on the wheel it's not gonna know it's

00:20:54.420 --> 00:20:54.430
on the wheel it's not gonna know it's
 

00:20:54.430 --> 00:20:57.390
on the wheel it's not gonna know it's
gonna hit him head-on but I had no

00:20:57.390 --> 00:20:57.400
gonna hit him head-on but I had no
 

00:20:57.400 --> 00:21:00.140
gonna hit him head-on but I had no
knowledge about what the car didn't know

00:21:00.140 --> 00:21:00.150
knowledge about what the car didn't know
 

00:21:00.150 --> 00:21:02.730
knowledge about what the car didn't know
this is another example people know

00:21:02.730 --> 00:21:02.740
this is another example people know
 

00:21:02.740 --> 00:21:05.340
this is another example people know
Genet wing and xt Wang they're of two

00:21:05.340 --> 00:21:05.350
Genet wing and xt Wang they're of two
 

00:21:05.350 --> 00:21:08.010
Genet wing and xt Wang they're of two
lungs from CMU they were driving with me

00:21:08.010 --> 00:21:08.020
lungs from CMU they were driving with me
 

00:21:08.020 --> 00:21:10.350
lungs from CMU they were driving with me
to an exec retreat when we in autopilot

00:21:10.350 --> 00:21:10.360
to an exec retreat when we in autopilot
 

00:21:10.360 --> 00:21:12.510
to an exec retreat when we in autopilot
when we encountered this scene and it

00:21:12.510 --> 00:21:12.520
when we encountered this scene and it
 

00:21:12.520 --> 00:21:14.690
when we encountered this scene and it
wasn't clear to me that the Tesla

00:21:14.690 --> 00:21:14.700
wasn't clear to me that the Tesla
 

00:21:14.700 --> 00:21:17.640
wasn't clear to me that the Tesla
designer has had this in mind we think

00:21:17.640 --> 00:21:17.650
designer has had this in mind we think
 

00:21:17.650 --> 00:21:19.890
designer has had this in mind we think
the system had a lot of pilot so this is

00:21:19.890 --> 00:21:19.900
the system had a lot of pilot so this is
 

00:21:19.900 --> 00:21:21.990
the system had a lot of pilot so this is
the challenge of unknown unknowns and

00:21:21.990 --> 00:21:22.000
the challenge of unknown unknowns and
 

00:21:22.000 --> 00:21:23.880
the challenge of unknown unknowns and
it's a real challenge we call these

00:21:23.880 --> 00:21:23.890
it's a real challenge we call these
 

00:21:23.890 --> 00:21:26.970
it's a real challenge we call these
blind spots now in the technical work we

00:21:26.970 --> 00:21:26.980
blind spots now in the technical work we
 

00:21:26.980 --> 00:21:28.860
blind spots now in the technical work we
can actually even use deep neural nets

00:21:28.860 --> 00:21:28.870
can actually even use deep neural nets
 

00:21:28.870 --> 00:21:31.770
can actually even use deep neural nets
to learn to focus them on the problem of

00:21:31.770 --> 00:21:31.780
to learn to focus them on the problem of
 

00:21:31.780 --> 00:21:34.590
to learn to focus them on the problem of
learning to predict confidence in how a

00:21:34.590 --> 00:21:34.600
learning to predict confidence in how a
 

00:21:34.600 --> 00:21:38.280
learning to predict confidence in how a
base level system will work and then

00:21:38.280 --> 00:21:38.290
base level system will work and then
 

00:21:38.290 --> 00:21:40.860
base level system will work and then
there's new work on my team and other

00:21:40.860 --> 00:21:40.870
there's new work on my team and other
 

00:21:40.870 --> 00:21:43.680
there's new work on my team and other
teams coming about which are ways to

00:21:43.680 --> 00:21:43.690
teams coming about which are ways to
 

00:21:43.690 --> 00:21:46.380
teams coming about which are ways to
triage human engineering attention to

00:21:46.380 --> 00:21:46.390
triage human engineering attention to
 

00:21:46.390 --> 00:21:49.770
triage human engineering attention to
find blind spots to identify unknown

00:21:49.770 --> 00:21:49.780
find blind spots to identify unknown
 

00:21:49.780 --> 00:21:52.170
find blind spots to identify unknown
unknowns in the open world this is early

00:21:52.170 --> 00:21:52.180
unknowns in the open world this is early
 

00:21:52.180 --> 00:21:54.960
unknowns in the open world this is early
research but it's critical and even in

00:21:54.960 --> 00:21:54.970
research but it's critical and even in
 

00:21:54.970 --> 00:21:56.850
research but it's critical and even in
reinforcement learning and in this paper

00:21:56.850 --> 00:21:56.860
reinforcement learning and in this paper
 

00:21:56.860 --> 00:21:59.220
reinforcement learning and in this paper
here how do you discover blind spots in

00:21:59.220 --> 00:21:59.230
here how do you discover blind spots in
 

00:21:59.230 --> 00:22:02.130
here how do you discover blind spots in
a system that learned its whole life how

00:22:02.130 --> 00:22:02.140
a system that learned its whole life how
 

00:22:02.140 --> 00:22:04.110
a system that learned its whole life how
to live and do its thing in a simulator

00:22:04.110 --> 00:22:04.120
to live and do its thing in a simulator
 

00:22:04.120 --> 00:22:06.750
to live and do its thing in a simulator
now put in the real world where it

00:22:06.750 --> 00:22:06.760
now put in the real world where it
 

00:22:06.760 --> 00:22:08.250
now put in the real world where it
should it be scared about where should

00:22:08.250 --> 00:22:08.260
should it be scared about where should
 

00:22:08.260 --> 00:22:12.950
should it be scared about where should
it be a little bit nervous or anxious

00:22:12.950 --> 00:22:12.960
it be a little bit nervous or anxious
 

00:22:12.960 --> 00:22:15.090
it be a little bit nervous or anxious
but beyond the technology and I think

00:22:15.090 --> 00:22:15.100
but beyond the technology and I think
 

00:22:15.100 --> 00:22:17.010
but beyond the technology and I think
that it's very good research and I'm

00:22:17.010 --> 00:22:17.020
that it's very good research and I'm
 

00:22:17.020 --> 00:22:18.420
that it's very good research and I'm
sure like I see great research coming

00:22:18.420 --> 00:22:18.430
sure like I see great research coming
 

00:22:18.430 --> 00:22:19.710
sure like I see great research coming
out of the CMU departments which have

00:22:19.710 --> 00:22:19.720
out of the CMU departments which have
 

00:22:19.720 --> 00:22:20.890
out of the CMU departments which have
been leading

00:22:20.890 --> 00:22:20.900
been leading
 

00:22:20.900 --> 00:22:22.930
been leading
I can I can now say what the number one

00:22:22.930 --> 00:22:22.940
I can I can now say what the number one
 

00:22:22.940 --> 00:22:25.840
I can I can now say what the number one
department in the country per US News &amp;

00:22:25.840 --> 00:22:25.850
department in the country per US News &amp;
 

00:22:25.850 --> 00:22:26.410
department in the country per US News &amp;
World Report

00:22:26.410 --> 00:22:26.420
World Report
 

00:22:26.420 --> 00:22:30.640
World Report
round of applause I would actually say

00:22:30.640 --> 00:22:30.650
round of applause I would actually say
 

00:22:30.650 --> 00:22:34.240
round of applause I would actually say
look US News World News apart I'd say

00:22:34.240 --> 00:22:34.250
look US News World News apart I'd say
 

00:22:34.250 --> 00:22:36.660
look US News World News apart I'd say
well deserved

00:22:36.660 --> 00:22:36.670
well deserved
 

00:22:36.670 --> 00:22:39.820
well deserved
but beyond technology there's the idea

00:22:39.820 --> 00:22:39.830
but beyond technology there's the idea
 

00:22:39.830 --> 00:22:43.210
but beyond technology there's the idea
of practices and designs so why not what

00:22:43.210 --> 00:22:43.220
of practices and designs so why not what
 

00:22:43.220 --> 00:22:46.360
of practices and designs so why not what
is the policy at which we we defer the

00:22:46.360 --> 00:22:46.370
is the policy at which we we defer the
 

00:22:46.370 --> 00:22:48.610
is the policy at which we we defer the
fielding of technology we say it's not

00:22:48.610 --> 00:22:48.620
fielding of technology we say it's not
 

00:22:48.620 --> 00:22:51.220
fielding of technology we say it's not
ready maybe for trials but it's not

00:22:51.220 --> 00:22:51.230
ready maybe for trials but it's not
 

00:22:51.230 --> 00:22:53.860
ready maybe for trials but it's not
ready to be used by by the popular

00:22:53.860 --> 00:22:53.870
ready to be used by by the popular
 

00:22:53.870 --> 00:22:56.830
ready to be used by by the popular
populations or do we need to have phased

00:22:56.830 --> 00:22:56.840
populations or do we need to have phased
 

00:22:56.840 --> 00:22:59.680
populations or do we need to have phased
trial like the FDA clinical trial phase

00:22:59.680 --> 00:22:59.690
trial like the FDA clinical trial phase
 

00:22:59.690 --> 00:23:01.750
trial like the FDA clinical trial phase
one phase two and post marketing

00:23:01.750 --> 00:23:01.760
one phase two and post marketing
 

00:23:01.760 --> 00:23:04.000
one phase two and post marketing
surveillance required study and

00:23:04.000 --> 00:23:04.010
surveillance required study and
 

00:23:04.010 --> 00:23:06.250
surveillance required study and
standards of reporting during these

00:23:06.250 --> 00:23:06.260
standards of reporting during these
 

00:23:06.260 --> 00:23:09.220
standards of reporting during these
times so we know what's going on but say

00:23:09.220 --> 00:23:09.230
times so we know what's going on but say
 

00:23:09.230 --> 00:23:11.260
times so we know what's going on but say
we want to feel technology how do we

00:23:11.260 --> 00:23:11.270
we want to feel technology how do we
 

00:23:11.270 --> 00:23:14.140
we want to feel technology how do we
disclose the nature of the behavior the

00:23:14.140 --> 00:23:14.150
disclose the nature of the behavior the
 

00:23:14.150 --> 00:23:16.840
disclose the nature of the behavior the
risks and the failure modes and there's

00:23:16.840 --> 00:23:16.850
risks and the failure modes and there's
 

00:23:16.850 --> 00:23:19.930
risks and the failure modes and there's
a whole field outside of AI and in many

00:23:19.930 --> 00:23:19.940
a whole field outside of AI and in many
 

00:23:19.940 --> 00:23:22.510
a whole field outside of AI and in many
cases outside of computer science called

00:23:22.510 --> 00:23:22.520
cases outside of computer science called
 

00:23:22.520 --> 00:23:26.400
cases outside of computer science called
failsafe design like air brakes on

00:23:26.400 --> 00:23:26.410
failsafe design like air brakes on
 

00:23:26.410 --> 00:23:28.780
failsafe design like air brakes on
trains invented in Pittsburgh actually

00:23:28.780 --> 00:23:28.790
trains invented in Pittsburgh actually
 

00:23:28.790 --> 00:23:30.490
trains invented in Pittsburgh actually
at a used worldwide one of the first

00:23:30.490 --> 00:23:30.500
at a used worldwide one of the first
 

00:23:30.500 --> 00:23:32.650
at a used worldwide one of the first
examples of failsafe designs where if

00:23:32.650 --> 00:23:32.660
examples of failsafe designs where if
 

00:23:32.660 --> 00:23:36.670
examples of failsafe designs where if
anything goes wrong the system stops we

00:23:36.670 --> 00:23:36.680
anything goes wrong the system stops we
 

00:23:36.680 --> 00:23:39.130
anything goes wrong the system stops we
can fold designs but with blind spots

00:23:39.130 --> 00:23:39.140
can fold designs but with blind spots
 

00:23:39.140 --> 00:23:40.600
can fold designs but with blind spots
that we can't get rid of potentially

00:23:40.600 --> 00:23:40.610
that we can't get rid of potentially
 

00:23:40.610 --> 00:23:44.190
that we can't get rid of potentially
with new kinds of engineering practices

00:23:44.190 --> 00:23:44.200
with new kinds of engineering practices
 

00:23:44.200 --> 00:23:46.780
with new kinds of engineering practices
this area of bias and fairness is

00:23:46.780 --> 00:23:46.790
this area of bias and fairness is
 

00:23:46.790 --> 00:23:47.770
this area of bias and fairness is
getting quite a bit of attention and

00:23:47.770 --> 00:23:47.780
getting quite a bit of attention and
 

00:23:47.780 --> 00:23:49.870
getting quite a bit of attention and
it's important one the article by by

00:23:49.870 --> 00:23:49.880
it's important one the article by by
 

00:23:49.880 --> 00:23:52.240
it's important one the article by by
Julia Angwin and and co-authors focused

00:23:52.240 --> 00:23:52.250
Julia Angwin and and co-authors focused
 

00:23:52.250 --> 00:23:55.690
Julia Angwin and and co-authors focused
on this idea that or the reality that

00:23:55.690 --> 00:23:55.700
on this idea that or the reality that
 

00:23:55.700 --> 00:23:57.670
on this idea that or the reality that
there was a system called compass being

00:23:57.670 --> 00:23:57.680
there was a system called compass being
 

00:23:57.680 --> 00:24:00.790
there was a system called compass being
used throughout the country to help

00:24:00.790 --> 00:24:00.800
used throughout the country to help
 

00:24:00.800 --> 00:24:05.070
used throughout the country to help
judges make decisions on bail and on

00:24:05.070 --> 00:24:05.080
judges make decisions on bail and on
 

00:24:05.080 --> 00:24:09.480
judges make decisions on bail and on
holding a person charged with a crime

00:24:09.480 --> 00:24:09.490
holding a person charged with a crime
 

00:24:09.490 --> 00:24:11.500
holding a person charged with a crime
versus letting them go before their

00:24:11.500 --> 00:24:11.510
versus letting them go before their
 

00:24:11.510 --> 00:24:13.960
versus letting them go before their
court date thinking about whether they

00:24:13.960 --> 00:24:13.970
court date thinking about whether they
 

00:24:13.970 --> 00:24:16.080
court date thinking about whether they
commit a violent crime in the mean time

00:24:16.080 --> 00:24:16.090
commit a violent crime in the mean time
 

00:24:16.090 --> 00:24:20.260
commit a violent crime in the mean time
and the article about the challenges

00:24:20.260 --> 00:24:20.270
and the article about the challenges
 

00:24:20.270 --> 00:24:21.340
and the article about the challenges
that were discovered some of the

00:24:21.340 --> 00:24:21.350
that were discovered some of the
 

00:24:21.350 --> 00:24:22.210
that were discovered some of the
challenges and they're still being

00:24:22.210 --> 00:24:22.220
challenges and they're still being
 

00:24:22.220 --> 00:24:23.710
challenges and they're still being
investigated because it's not all

00:24:23.710 --> 00:24:23.720
investigated because it's not all
 

00:24:23.720 --> 00:24:27.130
investigated because it's not all
straightforward that here's two cases

00:24:27.130 --> 00:24:27.140
straightforward that here's two cases
 

00:24:27.140 --> 00:24:29.110
straightforward that here's two cases
that you know sort of were analyzed by

00:24:29.110 --> 00:24:29.120
that you know sort of were analyzed by
 

00:24:29.120 --> 00:24:31.480
that you know sort of were analyzed by
the data and some of the risk reports

00:24:31.480 --> 00:24:31.490
the data and some of the risk reports
 

00:24:31.490 --> 00:24:34.510
the data and some of the risk reports
that that came up now it's

00:24:34.510 --> 00:24:34.520
that that came up now it's
 

00:24:34.520 --> 00:24:37.330
that that came up now it's
just the the high-profile cases that

00:24:37.330 --> 00:24:37.340
just the the high-profile cases that
 

00:24:37.340 --> 00:24:39.850
just the the high-profile cases that
have these problems its prolific so at

00:24:39.850 --> 00:24:39.860
have these problems its prolific so at
 

00:24:39.860 --> 00:24:43.510
have these problems its prolific so at
Microsoft and a Howard a roboticist

00:24:43.510 --> 00:24:43.520
Microsoft and a Howard a roboticist
 

00:24:43.520 --> 00:24:45.850
Microsoft and a Howard a roboticist
visited with me for a year year and a

00:24:45.850 --> 00:24:45.860
visited with me for a year year and a
 

00:24:45.860 --> 00:24:47.110
visited with me for a year year and a
half ago and we looked at some of

00:24:47.110 --> 00:24:47.120
half ago and we looked at some of
 

00:24:47.120 --> 00:24:48.970
half ago and we looked at some of
Microsoft's cognitive services and this

00:24:48.970 --> 00:24:48.980
Microsoft's cognitive services and this
 

00:24:48.980 --> 00:24:50.650
Microsoft's cognitive services and this
is a cognitive service as we call them

00:24:50.650 --> 00:24:50.660
is a cognitive service as we call them
 

00:24:50.660 --> 00:24:52.570
is a cognitive service as we call them
these are services available on the

00:24:52.570 --> 00:24:52.580
these are services available on the
 

00:24:52.580 --> 00:24:54.160
these are services available on the
Azure web available for use in

00:24:54.160 --> 00:24:54.170
Azure web available for use in
 

00:24:54.170 --> 00:24:56.740
Azure web available for use in
programming for a motion detection and

00:24:56.740 --> 00:24:56.750
programming for a motion detection and
 

00:24:56.750 --> 00:24:59.350
programming for a motion detection and
we discovered that the emotion pacifier

00:24:59.350 --> 00:24:59.360
we discovered that the emotion pacifier
 

00:24:59.360 --> 00:25:01.930
we discovered that the emotion pacifier
it's not as charged as race but the

00:25:01.930 --> 00:25:01.940
it's not as charged as race but the
 

00:25:01.940 --> 00:25:03.760
it's not as charged as race but the
emotion classifier had certain failure

00:25:03.760 --> 00:25:03.770
emotion classifier had certain failure
 

00:25:03.770 --> 00:25:06.460
emotion classifier had certain failure
modes for children and we went and

00:25:06.460 --> 00:25:06.470
modes for children and we went and
 

00:25:06.470 --> 00:25:08.590
modes for children and we went and
looked and studied the data set we had

00:25:08.590 --> 00:25:08.600
looked and studied the data set we had
 

00:25:08.600 --> 00:25:10.570
looked and studied the data set we had
to find it and who did the data set and

00:25:10.570 --> 00:25:10.580
to find it and who did the data set and
 

00:25:10.580 --> 00:25:12.580
to find it and who did the data set and
oh yes it didn't have a lot of children

00:25:12.580 --> 00:25:12.590
oh yes it didn't have a lot of children
 

00:25:12.590 --> 00:25:15.460
oh yes it didn't have a lot of children
and it it was and therefore it was it

00:25:15.460 --> 00:25:15.470
and it it was and therefore it was it
 

00:25:15.470 --> 00:25:17.080
and it it was and therefore it was it
was biased and that is the performance

00:25:17.080 --> 00:25:17.090
was biased and that is the performance
 

00:25:17.090 --> 00:25:20.320
was biased and that is the performance
didn't work as well in some areas and

00:25:20.320 --> 00:25:20.330
didn't work as well in some areas and
 

00:25:20.330 --> 00:25:22.540
didn't work as well in some areas and
this is the two who other studies like

00:25:22.540 --> 00:25:22.550
this is the two who other studies like
 

00:25:22.550 --> 00:25:24.640
this is the two who other studies like
this blitz a large-scale efforts inside

00:25:24.640 --> 00:25:24.650
this blitz a large-scale efforts inside
 

00:25:24.650 --> 00:25:25.930
this blitz a large-scale efforts inside
Microsoft and more in that in a few

00:25:25.930 --> 00:25:25.940
Microsoft and more in that in a few
 

00:25:25.940 --> 00:25:29.530
Microsoft and more in that in a few
minutes to come up with ways to detect

00:25:29.530 --> 00:25:29.540
minutes to come up with ways to detect
 

00:25:29.540 --> 00:25:34.020
minutes to come up with ways to detect
and address biases and our services

00:25:34.020 --> 00:25:34.030
 
 

00:25:34.030 --> 00:25:39.790
 
values and agency let me just show you a

00:25:39.790 --> 00:25:39.800
values and agency let me just show you a
 

00:25:39.800 --> 00:25:42.580
values and agency let me just show you a
curve here this curve up on though on

00:25:42.580 --> 00:25:42.590
curve here this curve up on though on
 

00:25:42.590 --> 00:25:45.280
curve here this curve up on though on
the screen here isn't the typical kind

00:25:45.280 --> 00:25:45.290
the screen here isn't the typical kind
 

00:25:45.290 --> 00:25:47.350
the screen here isn't the typical kind
of performance curve that computer

00:25:47.350 --> 00:25:47.360
of performance curve that computer
 

00:25:47.360 --> 00:25:51.010
of performance curve that computer
scientists look at when they build a

00:25:51.010 --> 00:25:51.020
scientists look at when they build a
 

00:25:51.020 --> 00:25:53.230
scientists look at when they build a
pattern recognizer or machine learn

00:25:53.230 --> 00:25:53.240
pattern recognizer or machine learn
 

00:25:53.240 --> 00:25:55.450
pattern recognizer or machine learn
classifier from data and they typically

00:25:55.450 --> 00:25:55.460
classifier from data and they typically
 

00:25:55.460 --> 00:25:56.740
classifier from data and they typically
hold out a little bit of data usually

00:25:56.740 --> 00:25:56.750
hold out a little bit of data usually
 

00:25:56.750 --> 00:25:58.180
hold out a little bit of data usually
the most recent data they let's say in a

00:25:58.180 --> 00:25:58.190
the most recent data they let's say in a
 

00:25:58.190 --> 00:25:59.680
the most recent data they let's say in a
medical case because the most recent

00:25:59.680 --> 00:25:59.690
medical case because the most recent
 

00:25:59.690 --> 00:26:03.160
medical case because the most recent
data it looks like the data most looks

00:26:03.160 --> 00:26:03.170
data it looks like the data most looks
 

00:26:03.170 --> 00:26:05.890
data it looks like the data most looks
like the data you'll see in the world so

00:26:05.890 --> 00:26:05.900
like the data you'll see in the world so
 

00:26:05.900 --> 00:26:07.120
like the data you'll see in the world so
they hold that out from the training and

00:26:07.120 --> 00:26:07.130
they hold that out from the training and
 

00:26:07.130 --> 00:26:08.260
they hold that out from the training and
they say we're going to test our system

00:26:08.260 --> 00:26:08.270
they say we're going to test our system
 

00:26:08.270 --> 00:26:10.420
they say we're going to test our system
with this holdout set that the the

00:26:10.420 --> 00:26:10.430
with this holdout set that the the
 

00:26:10.430 --> 00:26:12.010
with this holdout set that the the
learning algorithm has not seen before

00:26:12.010 --> 00:26:12.020
learning algorithm has not seen before
 

00:26:12.020 --> 00:26:13.720
learning algorithm has not seen before
and then you generate one of these

00:26:13.720 --> 00:26:13.730
and then you generate one of these
 

00:26:13.730 --> 00:26:15.670
and then you generate one of these
curves and what you see on there is a

00:26:15.670 --> 00:26:15.680
curves and what you see on there is a
 

00:26:15.680 --> 00:26:17.740
curves and what you see on there is a
true positive and a false positive rate

00:26:17.740 --> 00:26:17.750
true positive and a false positive rate
 

00:26:17.750 --> 00:26:20.230
true positive and a false positive rate
so it tells us for example and this is

00:26:20.230 --> 00:26:20.240
so it tells us for example and this is
 

00:26:20.240 --> 00:26:23.770
so it tells us for example and this is
actually the the one of the receiver

00:26:23.770 --> 00:26:23.780
actually the the one of the receiver
 

00:26:23.780 --> 00:26:25.750
actually the the one of the receiver
operator characteristic curves as

00:26:25.750 --> 00:26:25.760
operator characteristic curves as
 

00:26:25.760 --> 00:26:27.610
operator characteristic curves as
they're called one of the curves from

00:26:27.610 --> 00:26:27.620
they're called one of the curves from
 

00:26:27.620 --> 00:26:29.560
they're called one of the curves from
the readmissions prediction work we done

00:26:29.560 --> 00:26:29.570
the readmissions prediction work we done
 

00:26:29.570 --> 00:26:30.910
the readmissions prediction work we done
and one of the early curves in our

00:26:30.910 --> 00:26:30.920
and one of the early curves in our
 

00:26:30.920 --> 00:26:33.130
and one of the early curves in our
research from about 15 years of medical

00:26:33.130 --> 00:26:33.140
research from about 15 years of medical
 

00:26:33.140 --> 00:26:35.020
research from about 15 years of medical
data of readmissions to a hospital

00:26:35.020 --> 00:26:35.030
data of readmissions to a hospital
 

00:26:35.030 --> 00:26:37.390
data of readmissions to a hospital
within 30 days so this the idea was we

00:26:37.390 --> 00:26:37.400
within 30 days so this the idea was we
 

00:26:37.400 --> 00:26:38.710
within 30 days so this the idea was we
want to predict when patients will

00:26:38.710 --> 00:26:38.720
want to predict when patients will
 

00:26:38.720 --> 00:26:40.840
want to predict when patients will
return to the hospital within a month

00:26:40.840 --> 00:26:40.850
return to the hospital within a month
 

00:26:40.850 --> 00:26:43.090
return to the hospital within a month
it's actually tracked by Medicare to

00:26:43.090 --> 00:26:43.100
it's actually tracked by Medicare to
 

00:26:43.100 --> 00:26:46.220
it's actually tracked by Medicare to
penalize able

00:26:46.220 --> 00:26:46.230
 
 

00:26:46.230 --> 00:26:49.590
 
quality measure and it says here that if

00:26:49.590 --> 00:26:49.600
quality measure and it says here that if
 

00:26:49.600 --> 00:26:53.039
quality measure and it says here that if
you operated this threshold here we can

00:26:53.039 --> 00:26:53.049
you operated this threshold here we can
 

00:26:53.049 --> 00:26:56.220
you operated this threshold here we can
tell that the classify will tell you you

00:26:56.220 --> 00:26:56.230
tell that the classify will tell you you
 

00:26:56.230 --> 00:26:59.879
tell that the classify will tell you you
know if it says patients going to come

00:26:59.879 --> 00:26:59.889
know if it says patients going to come
 

00:26:59.889 --> 00:27:01.889
know if it says patients going to come
back three quarters of the time it will

00:27:01.889 --> 00:27:01.899
back three quarters of the time it will
 

00:27:01.899 --> 00:27:04.110
back three quarters of the time it will
be correct but the false positive rate

00:27:04.110 --> 00:27:04.120
be correct but the false positive rate
 

00:27:04.120 --> 00:27:07.619
be correct but the false positive rate
will be twenty five percent they won't

00:27:07.619 --> 00:27:07.629
will be twenty five percent they won't
 

00:27:07.629 --> 00:27:10.049
will be twenty five percent they won't
come back and you can imagine moving

00:27:10.049 --> 00:27:10.059
come back and you can imagine moving
 

00:27:10.059 --> 00:27:12.029
come back and you can imagine moving
this threshold around and having what

00:27:12.029 --> 00:27:12.039
this threshold around and having what
 

00:27:12.039 --> 00:27:13.350
this threshold around and having what
are called different operating

00:27:13.350 --> 00:27:13.360
are called different operating
 

00:27:13.360 --> 00:27:15.149
are called different operating
characteristics or operating ranges for

00:27:15.149 --> 00:27:15.159
characteristics or operating ranges for
 

00:27:15.159 --> 00:27:18.029
characteristics or operating ranges for
this classifier well it turns out if you

00:27:18.029 --> 00:27:18.039
this classifier well it turns out if you
 

00:27:18.039 --> 00:27:19.470
this classifier well it turns out if you
think about what does it mean for a true

00:27:19.470 --> 00:27:19.480
think about what does it mean for a true
 

00:27:19.480 --> 00:27:22.320
think about what does it mean for a true
positive and a false positive it means

00:27:22.320 --> 00:27:22.330
positive and a false positive it means
 

00:27:22.330 --> 00:27:23.820
positive and a false positive it means
something very different for your spam

00:27:23.820 --> 00:27:23.830
something very different for your spam
 

00:27:23.830 --> 00:27:28.440
something very different for your spam
filter right then for a patient and for

00:27:28.440 --> 00:27:28.450
filter right then for a patient and for
 

00:27:28.450 --> 00:27:31.230
filter right then for a patient and for
a cardiac patient for example this gets

00:27:31.230 --> 00:27:31.240
a cardiac patient for example this gets
 

00:27:31.240 --> 00:27:33.090
a cardiac patient for example this gets
at the idea of costs and benefits and

00:27:33.090 --> 00:27:33.100
at the idea of costs and benefits and
 

00:27:33.100 --> 00:27:34.950
at the idea of costs and benefits and
depending on where I picked that

00:27:34.950 --> 00:27:34.960
depending on where I picked that
 

00:27:34.960 --> 00:27:38.220
depending on where I picked that
threshold really affects the performance

00:27:38.220 --> 00:27:38.230
threshold really affects the performance
 

00:27:38.230 --> 00:27:40.049
threshold really affects the performance
of the system in a way that affects the

00:27:40.049 --> 00:27:40.059
of the system in a way that affects the
 

00:27:40.059 --> 00:27:42.480
of the system in a way that affects the
end users costs and benefits that they

00:27:42.480 --> 00:27:42.490
end users costs and benefits that they
 

00:27:42.490 --> 00:27:45.779
end users costs and benefits that they
receive and this includes significant

00:27:45.779 --> 00:27:45.789
receive and this includes significant
 

00:27:45.789 --> 00:27:50.490
receive and this includes significant
health implications now looking at this

00:27:50.490 --> 00:27:50.500
health implications now looking at this
 

00:27:50.500 --> 00:27:56.820
health implications now looking at this
more broadly the golden go back here

00:27:56.820 --> 00:27:56.830
more broadly the golden go back here
 

00:27:56.830 --> 00:27:59.039
more broadly the golden go back here
most machine learning research takes

00:27:59.039 --> 00:27:59.049
most machine learning research takes
 

00:27:59.049 --> 00:28:01.169
most machine learning research takes
data sets and gives you probability

00:28:01.169 --> 00:28:01.179
data sets and gives you probability
 

00:28:01.179 --> 00:28:03.090
data sets and gives you probability
distributions about what you care about

00:28:03.090 --> 00:28:03.100
distributions about what you care about
 

00:28:03.100 --> 00:28:04.259
distributions about what you care about
like a prediction

00:28:04.259 --> 00:28:04.269
like a prediction
 

00:28:04.269 --> 00:28:06.320
like a prediction
what disease does the patient have

00:28:06.320 --> 00:28:06.330
what disease does the patient have
 

00:28:06.330 --> 00:28:09.029
what disease does the patient have
what's the wind flow going to be over a

00:28:09.029 --> 00:28:09.039
what's the wind flow going to be over a
 

00:28:09.039 --> 00:28:11.850
what's the wind flow going to be over a
over a city United States from wind

00:28:11.850 --> 00:28:11.860
over a city United States from wind
 

00:28:11.860 --> 00:28:15.869
over a city United States from wind
sensors what a motion if somebody have

00:28:15.869 --> 00:28:15.879
sensors what a motion if somebody have
 

00:28:15.879 --> 00:28:18.269
sensors what a motion if somebody have
on their face right now and we need to

00:28:18.269 --> 00:28:18.279
on their face right now and we need to
 

00:28:18.279 --> 00:28:20.460
on their face right now and we need to
do more of the full decision analysis to

00:28:20.460 --> 00:28:20.470
do more of the full decision analysis to
 

00:28:20.470 --> 00:28:22.799
do more of the full decision analysis to
cause decision-making based on the

00:28:22.799 --> 00:28:22.809
cause decision-making based on the
 

00:28:22.809 --> 00:28:24.149
cause decision-making based on the
predictive model is where the rubber

00:28:24.149 --> 00:28:24.159
predictive model is where the rubber
 

00:28:24.159 --> 00:28:26.909
predictive model is where the rubber
meets the road and so I call this

00:28:26.909 --> 00:28:26.919
meets the road and so I call this
 

00:28:26.919 --> 00:28:29.549
meets the road and so I call this
they're kind of like the golden data to

00:28:29.549 --> 00:28:29.559
they're kind of like the golden data to
 

00:28:29.559 --> 00:28:31.320
they're kind of like the golden data to
predictions the decisions pipeline and

00:28:31.320 --> 00:28:31.330
predictions the decisions pipeline and
 

00:28:31.330 --> 00:28:34.110
predictions the decisions pipeline and
this is where we really get into value

00:28:34.110 --> 00:28:34.120
this is where we really get into value
 

00:28:34.120 --> 00:28:36.240
this is where we really get into value
and it's not just automation it's Vic

00:28:36.240 --> 00:28:36.250
and it's not just automation it's Vic
 

00:28:36.250 --> 00:28:37.740
and it's not just automation it's Vic
for recommendation engines as well to

00:28:37.740 --> 00:28:37.750
for recommendation engines as well to
 

00:28:37.750 --> 00:28:39.269
for recommendation engines as well to
off to a young doctor for example

00:28:39.269 --> 00:28:39.279
off to a young doctor for example
 

00:28:39.279 --> 00:28:40.499
off to a young doctor for example
looking at the output of one of these

00:28:40.499 --> 00:28:40.509
looking at the output of one of these
 

00:28:40.509 --> 00:28:44.340
looking at the output of one of these
systems now think about the decision

00:28:44.340 --> 00:28:44.350
systems now think about the decision
 

00:28:44.350 --> 00:28:47.249
systems now think about the decision
model the decision model takes in a set

00:28:47.249 --> 00:28:47.259
model the decision model takes in a set
 

00:28:47.259 --> 00:28:49.440
model the decision model takes in a set
of probabilities and based on a

00:28:49.440 --> 00:28:49.450
of probabilities and based on a
 

00:28:49.450 --> 00:28:51.629
of probabilities and based on a
cost-benefit consideration outputs the

00:28:51.629 --> 00:28:51.639
cost-benefit consideration outputs the
 

00:28:51.639 --> 00:28:53.879
cost-benefit consideration outputs the
ideal action to take and that depends on

00:28:53.879 --> 00:28:53.889
ideal action to take and that depends on
 

00:28:53.889 --> 00:28:56.970
ideal action to take and that depends on
the details in a decision model which

00:28:56.970 --> 00:28:56.980
the details in a decision model which
 

00:28:56.980 --> 00:28:57.950
the details in a decision model which
captured

00:28:57.950 --> 00:28:57.960
captured
 

00:28:57.960 --> 00:29:01.070
captured
the value or the utility structure in

00:29:01.070 --> 00:29:01.080
the value or the utility structure in
 

00:29:01.080 --> 00:29:02.960
the value or the utility structure in
the person being affected that's the

00:29:02.960 --> 00:29:02.970
the person being affected that's the
 

00:29:02.970 --> 00:29:04.700
the person being affected that's the
intention the principal agent of

00:29:04.700 --> 00:29:04.710
intention the principal agent of
 

00:29:04.710 --> 00:29:07.400
intention the principal agent of
decision now we discovered when we

00:29:07.400 --> 00:29:07.410
decision now we discovered when we
 

00:29:07.410 --> 00:29:10.520
decision now we discovered when we
shipped our readmission system that a

00:29:10.520 --> 00:29:10.530
shipped our readmission system that a
 

00:29:10.530 --> 00:29:12.830
shipped our readmission system that a
large hospital system in the Midwest of

00:29:12.830 --> 00:29:12.840
large hospital system in the Midwest of
 

00:29:12.840 --> 00:29:15.140
large hospital system in the Midwest of
the United States was playing with

00:29:15.140 --> 00:29:15.150
the United States was playing with
 

00:29:15.150 --> 00:29:19.130
the United States was playing with
thresholds and they said when the

00:29:19.130 --> 00:29:19.140
thresholds and they said when the
 

00:29:19.140 --> 00:29:20.930
thresholds and they said when the
Microsoft score it's actually a

00:29:20.930 --> 00:29:20.940
Microsoft score it's actually a
 

00:29:20.940 --> 00:29:22.340
Microsoft score it's actually a
probability but they called it a

00:29:22.340 --> 00:29:22.350
probability but they called it a
 

00:29:22.350 --> 00:29:25.580
probability but they called it a
Microsoft score is greater than 25 they

00:29:25.580 --> 00:29:25.590
Microsoft score is greater than 25 they
 

00:29:25.590 --> 00:29:27.980
Microsoft score is greater than 25 they
will initiate a special package for a

00:29:27.980 --> 00:29:27.990
will initiate a special package for a
 

00:29:27.990 --> 00:29:29.690
will initiate a special package for a
patient which costs quite a bit of money

00:29:29.690 --> 00:29:29.700
patient which costs quite a bit of money
 

00:29:29.700 --> 00:29:31.730
patient which costs quite a bit of money
but they'll initiate it they'll take

00:29:31.730 --> 00:29:31.740
but they'll initiate it they'll take
 

00:29:31.740 --> 00:29:33.860
but they'll initiate it they'll take
that charge and it turns out by doing

00:29:33.860 --> 00:29:33.870
that charge and it turns out by doing
 

00:29:33.870 --> 00:29:36.530
that charge and it turns out by doing
that they saved 1.4 million dollars per

00:29:36.530 --> 00:29:36.540
that they saved 1.4 million dollars per
 

00:29:36.540 --> 00:29:40.070
that they saved 1.4 million dollars per
year and they're in the hospital we sat

00:29:40.070 --> 00:29:40.080
year and they're in the hospital we sat
 

00:29:40.080 --> 00:29:43.010
year and they're in the hospital we sat
down and did a real a rich decision

00:29:43.010 --> 00:29:43.020
down and did a real a rich decision
 

00:29:43.020 --> 00:29:44.600
down and did a real a rich decision
analysis with many costs and benefits

00:29:44.600 --> 00:29:44.610
analysis with many costs and benefits
 

00:29:44.610 --> 00:29:48.560
analysis with many costs and benefits
and and and and look this up and so on

00:29:48.560 --> 00:29:48.570
and and and and look this up and so on
 

00:29:48.570 --> 00:29:52.340
and and and and look this up and so on
and showed that the actual details were

00:29:52.340 --> 00:29:52.350
and showed that the actual details were
 

00:29:52.350 --> 00:29:54.740
and showed that the actual details were
quite nuanced and it really depended on

00:29:54.740 --> 00:29:54.750
quite nuanced and it really depended on
 

00:29:54.750 --> 00:29:57.910
quite nuanced and it really depended on
careful modeling of the utility function

00:29:57.910 --> 00:29:57.920
careful modeling of the utility function
 

00:29:57.920 --> 00:30:01.160
careful modeling of the utility function
so you can't just ship a classifier and

00:30:01.160 --> 00:30:01.170
so you can't just ship a classifier and
 

00:30:01.170 --> 00:30:04.250
so you can't just ship a classifier and
have it be used in the world and expect

00:30:04.250 --> 00:30:04.260
have it be used in the world and expect
 

00:30:04.260 --> 00:30:06.320
have it be used in the world and expect
it to do the right thing for everybody

00:30:06.320 --> 00:30:06.330
it to do the right thing for everybody
 

00:30:06.330 --> 00:30:08.630
it to do the right thing for everybody
because how health you know different

00:30:08.630 --> 00:30:08.640
because how health you know different
 

00:30:08.640 --> 00:30:10.400
because how health you know different
attributes of what you care about in a

00:30:10.400 --> 00:30:10.410
attributes of what you care about in a
 

00:30:10.410 --> 00:30:12.020
attributes of what you care about in a
health outcome go into the utility

00:30:12.020 --> 00:30:12.030
health outcome go into the utility
 

00:30:12.030 --> 00:30:18.170
health outcome go into the utility
structure here so you have to ask in any

00:30:18.170 --> 00:30:18.180
structure here so you have to ask in any
 

00:30:18.180 --> 00:30:22.550
structure here so you have to ask in any
a I system whose utility function what

00:30:22.550 --> 00:30:22.560
a I system whose utility function what
 

00:30:22.560 --> 00:30:25.280
a I system whose utility function what
cost benefit analysis who assessed it

00:30:25.280 --> 00:30:25.290
cost benefit analysis who assessed it
 

00:30:25.290 --> 00:30:27.950
cost benefit analysis who assessed it
who refined it who inspected it could we

00:30:27.950 --> 00:30:27.960
who refined it who inspected it could we
 

00:30:27.960 --> 00:30:32.450
who refined it who inspected it could we
reveal it to disclose it to now on

00:30:32.450 --> 00:30:32.460
reveal it to disclose it to now on
 

00:30:32.460 --> 00:30:37.340
reveal it to disclose it to now on
misuses it's as we all know it's quite

00:30:37.340 --> 00:30:37.350
misuses it's as we all know it's quite
 

00:30:37.350 --> 00:30:39.670
misuses it's as we all know it's quite
possible to use these technologies to

00:30:39.670 --> 00:30:39.680
possible to use these technologies to
 

00:30:39.680 --> 00:30:44.620
possible to use these technologies to
empower and amplify the the tyranny of

00:30:44.620 --> 00:30:44.630
empower and amplify the the tyranny of
 

00:30:44.630 --> 00:30:48.920
empower and amplify the the tyranny of
leaders who don't have the best don't

00:30:48.920 --> 00:30:48.930
leaders who don't have the best don't
 

00:30:48.930 --> 00:30:50.920
leaders who don't have the best don't
have the best in mind for their populace

00:30:50.920 --> 00:30:50.930
have the best in mind for their populace
 

00:30:50.930 --> 00:30:53.030
have the best in mind for their populace
freedom of association freedom of

00:30:53.030 --> 00:30:53.040
freedom of association freedom of
 

00:30:53.040 --> 00:30:55.550
freedom of association freedom of
expression new kinds of surveillance in

00:30:55.550 --> 00:30:55.560
expression new kinds of surveillance in
 

00:30:55.560 --> 00:30:59.060
expression new kinds of surveillance in
policing but also a misuse commitment

00:30:59.060 --> 00:30:59.070
policing but also a misuse commitment
 

00:30:59.070 --> 00:31:01.610
policing but also a misuse commitment
can be included to include the risk of

00:31:01.610 --> 00:31:01.620
can be included to include the risk of
 

00:31:01.620 --> 00:31:03.920
can be included to include the risk of
deaths that they're serious injury in a

00:31:03.920 --> 00:31:03.930
deaths that they're serious injury in a
 

00:31:03.930 --> 00:31:05.420
deaths that they're serious injury in a
system that's us failing too much to be

00:31:05.420 --> 00:31:05.430
system that's us failing too much to be
 

00:31:05.430 --> 00:31:07.700
system that's us failing too much to be
shipped or a system that's unfair and

00:31:07.700 --> 00:31:07.710
shipped or a system that's unfair and
 

00:31:07.710 --> 00:31:10.000
shipped or a system that's unfair and
it's denying consequential services and

00:31:10.000 --> 00:31:10.010
it's denying consequential services and
 

00:31:10.010 --> 00:31:13.040
it's denying consequential services and
resources to people or a system that's

00:31:13.040 --> 00:31:13.050
resources to people or a system that's
 

00:31:13.050 --> 00:31:16.030
resources to people or a system that's
manipulating the attention beliefs

00:31:16.030 --> 00:31:16.040
manipulating the attention beliefs
 

00:31:16.040 --> 00:31:21.070
manipulating the attention beliefs
voting patterns and cognition of people

00:31:21.070 --> 00:31:21.080
voting patterns and cognition of people
 

00:31:21.080 --> 00:31:24.590
voting patterns and cognition of people
let me talk about that last notion right

00:31:24.590 --> 00:31:24.600
let me talk about that last notion right
 

00:31:24.600 --> 00:31:25.940
let me talk about that last notion right
now because it's been on my mind quite a

00:31:25.940 --> 00:31:25.950
now because it's been on my mind quite a
 

00:31:25.950 --> 00:31:28.250
now because it's been on my mind quite a
bit lately and I'm so much in a few

00:31:28.250 --> 00:31:28.260
bit lately and I'm so much in a few
 

00:31:28.260 --> 00:31:29.240
bit lately and I'm so much in a few
minutes we have a working group of

00:31:29.240 --> 00:31:29.250
minutes we have a working group of
 

00:31:29.250 --> 00:31:30.740
minutes we have a working group of
Microsoft right now looking at this very

00:31:30.740 --> 00:31:30.750
Microsoft right now looking at this very
 

00:31:30.750 --> 00:31:36.530
Microsoft right now looking at this very
carefully the idea of using the of the

00:31:36.530 --> 00:31:36.540
carefully the idea of using the of the
 

00:31:36.540 --> 00:31:38.690
carefully the idea of using the of the
best minds coming out of our grad school

00:31:38.690 --> 00:31:38.700
best minds coming out of our grad school
 

00:31:38.700 --> 00:31:41.150
best minds coming out of our grad school
departments in machine learning going

00:31:41.150 --> 00:31:41.160
departments in machine learning going
 

00:31:41.160 --> 00:31:43.400
departments in machine learning going
off to work at companies that are

00:31:43.400 --> 00:31:43.410
off to work at companies that are
 

00:31:43.410 --> 00:31:46.040
off to work at companies that are
harnessing their talents to optimize

00:31:46.040 --> 00:31:46.050
harnessing their talents to optimize
 

00:31:46.050 --> 00:31:50.150
harnessing their talents to optimize
what your attention is focused on how

00:31:50.150 --> 00:31:50.160
what your attention is focused on how
 

00:31:50.160 --> 00:31:51.440
what your attention is focused on how
much time are you spending on an

00:31:51.440 --> 00:31:51.450
much time are you spending on an
 

00:31:51.450 --> 00:31:54.170
much time are you spending on an
application so if you look at this pie

00:31:54.170 --> 00:31:54.180
application so if you look at this pie
 

00:31:54.180 --> 00:31:55.850
application so if you look at this pie
chart here all the things you attend to

00:31:55.850 --> 00:31:55.860
chart here all the things you attend to
 

00:31:55.860 --> 00:31:59.170
chart here all the things you attend to
during the day we know now that

00:31:59.170 --> 00:31:59.180
during the day we know now that
 

00:31:59.180 --> 00:32:01.820
during the day we know now that
large-scale data analysis and planning

00:32:01.820 --> 00:32:01.830
large-scale data analysis and planning
 

00:32:01.830 --> 00:32:06.790
large-scale data analysis and planning
and personalized policies are being used

00:32:06.790 --> 00:32:06.800
and personalized policies are being used
 

00:32:06.800 --> 00:32:10.910
and personalized policies are being used
to make to maximize your attention and

00:32:10.910 --> 00:32:10.920
to make to maximize your attention and
 

00:32:10.920 --> 00:32:14.990
to make to maximize your attention and
time on applications and this is in

00:32:14.990 --> 00:32:15.000
time on applications and this is in
 

00:32:15.000 --> 00:32:16.940
time on applications and this is in
accordance with optimizing the dollars

00:32:16.940 --> 00:32:16.950
accordance with optimizing the dollars
 

00:32:16.950 --> 00:32:19.100
accordance with optimizing the dollars
back to the people writing those

00:32:19.100 --> 00:32:19.110
back to the people writing those
 

00:32:19.110 --> 00:32:21.560
back to the people writing those
algorithms now back to her appointment's

00:32:21.560 --> 00:32:21.570
algorithms now back to her appointment's
 

00:32:21.570 --> 00:32:23.660
algorithms now back to her appointment's
again herb Simon talked about a poverty

00:32:23.660 --> 00:32:23.670
again herb Simon talked about a poverty
 

00:32:23.670 --> 00:32:26.030
again herb Simon talked about a poverty
of human attention coming in a world

00:32:26.030 --> 00:32:26.040
of human attention coming in a world
 

00:32:26.040 --> 00:32:29.060
of human attention coming in a world
full of information and distraction it's

00:32:29.060 --> 00:32:29.070
full of information and distraction it's
 

00:32:29.070 --> 00:32:30.740
full of information and distraction it's
an important concept to think about what

00:32:30.740 --> 00:32:30.750
an important concept to think about what
 

00:32:30.750 --> 00:32:32.570
an important concept to think about what
this is doing to our daily lives

00:32:32.570 --> 00:32:32.580
this is doing to our daily lives
 

00:32:32.580 --> 00:32:36.590
this is doing to our daily lives
I like Tori to use the less polite

00:32:36.590 --> 00:32:36.600
I like Tori to use the less polite
 

00:32:36.600 --> 00:32:39.110
I like Tori to use the less polite
phrase that these are adversarial

00:32:39.110 --> 00:32:39.120
phrase that these are adversarial
 

00:32:39.120 --> 00:32:42.830
phrase that these are adversarial
attacks on human attention for someone

00:32:42.830 --> 00:32:42.840
attacks on human attention for someone
 

00:32:42.840 --> 00:32:44.990
attacks on human attention for someone
else has gained whether it be

00:32:44.990 --> 00:32:45.000
else has gained whether it be
 

00:32:45.000 --> 00:32:49.810
else has gained whether it be
shareholders or owners of a company so

00:32:49.810 --> 00:32:49.820
shareholders or owners of a company so
 

00:32:49.820 --> 00:32:52.370
shareholders or owners of a company so
thinking forward on where we're going in

00:32:52.370 --> 00:32:52.380
thinking forward on where we're going in
 

00:32:52.380 --> 00:32:53.810
thinking forward on where we're going in
the world with this and we're what our

00:32:53.810 --> 00:32:53.820
the world with this and we're what our
 

00:32:53.820 --> 00:32:55.340
the world with this and we're what our
kids will be immersed in and our

00:32:55.340 --> 00:32:55.350
kids will be immersed in and our
 

00:32:55.350 --> 00:32:58.480
kids will be immersed in and our
grandkids we have to think deeply about

00:32:58.480 --> 00:32:58.490
grandkids we have to think deeply about
 

00:32:58.490 --> 00:33:02.200
grandkids we have to think deeply about
preferences on time well spent

00:33:02.200 --> 00:33:02.210
preferences on time well spent
 

00:33:02.210 --> 00:33:05.270
preferences on time well spent
long-term preferences on what we want to

00:33:05.270 --> 00:33:05.280
long-term preferences on what we want to
 

00:33:05.280 --> 00:33:07.490
long-term preferences on what we want to
be doing with our time and thinking

00:33:07.490 --> 00:33:07.500
be doing with our time and thinking
 

00:33:07.500 --> 00:33:11.060
be doing with our time and thinking
deeply about policies disclosure

00:33:11.060 --> 00:33:11.070
deeply about policies disclosure
 

00:33:11.070 --> 00:33:12.830
deeply about policies disclosure
shouldn't I be told that there's an

00:33:12.830 --> 00:33:12.840
shouldn't I be told that there's an
 

00:33:12.840 --> 00:33:16.520
shouldn't I be told that there's an
algorithm focused on my mind that

00:33:16.520 --> 00:33:16.530
algorithm focused on my mind that
 

00:33:16.530 --> 00:33:19.220
algorithm focused on my mind that
maximizes the time of engagement I will

00:33:19.220 --> 00:33:19.230
maximizes the time of engagement I will
 

00:33:19.230 --> 00:33:21.680
maximizes the time of engagement I will
spend in this application controls in

00:33:21.680 --> 00:33:21.690
spend in this application controls in
 

00:33:21.690 --> 00:33:24.080
spend in this application controls in
education

00:33:24.080 --> 00:33:24.090
education
 

00:33:24.090 --> 00:33:27.270
education
perhaps as interesting and intriguing

00:33:27.270 --> 00:33:27.280
perhaps as interesting and intriguing
 

00:33:27.280 --> 00:33:30.539
perhaps as interesting and intriguing
and concerning is AI manipulation you

00:33:30.539 --> 00:33:30.549
and concerning is AI manipulation you
 

00:33:30.549 --> 00:33:34.110
and concerning is AI manipulation you
know we have people even here at CMU

00:33:34.110 --> 00:33:34.120
know we have people even here at CMU
 

00:33:34.120 --> 00:33:36.240
know we have people even here at CMU
study marketing it's a great concept and

00:33:36.240 --> 00:33:36.250
study marketing it's a great concept and
 

00:33:36.250 --> 00:33:38.340
study marketing it's a great concept and
topic and people have long studied

00:33:38.340 --> 00:33:38.350
topic and people have long studied
 

00:33:38.350 --> 00:33:40.950
topic and people have long studied
propaganda they're experts at that the

00:33:40.950 --> 00:33:40.960
propaganda they're experts at that the
 

00:33:40.960 --> 00:33:42.180
propaganda they're experts at that the
different words and there are different

00:33:42.180 --> 00:33:42.190
different words and there are different
 

00:33:42.190 --> 00:33:46.620
different words and there are different
intentions but the idea of turning away

00:33:46.620 --> 00:33:46.630
intentions but the idea of turning away
 

00:33:46.630 --> 00:33:48.860
intentions but the idea of turning away
from back-of-the-envelope theories and

00:33:48.860 --> 00:33:48.870
from back-of-the-envelope theories and
 

00:33:48.870 --> 00:33:52.799
from back-of-the-envelope theories and
sketches to large-scale data centric

00:33:52.799 --> 00:33:52.809
sketches to large-scale data centric
 

00:33:52.809 --> 00:33:56.159
sketches to large-scale data centric
manipulation is concerning so work by

00:33:56.159 --> 00:33:56.169
manipulation is concerning so work by
 

00:33:56.169 --> 00:33:57.539
manipulation is concerning so work by
Seymour and Tully a couple years ago

00:33:57.539 --> 00:33:57.549
Seymour and Tully a couple years ago
 

00:33:57.549 --> 00:33:59.100
Seymour and Tully a couple years ago
that I was talking about a few years ago

00:33:59.100 --> 00:33:59.110
that I was talking about a few years ago
 

00:33:59.110 --> 00:34:01.860
that I was talking about a few years ago
when this first came up it took a large

00:34:01.860 --> 00:34:01.870
when this first came up it took a large
 

00:34:01.870 --> 00:34:04.620
when this first came up it took a large
number of Twitter feeds including geotag

00:34:04.620 --> 00:34:04.630
number of Twitter feeds including geotag
 

00:34:04.630 --> 00:34:06.450
number of Twitter feeds including geotag
Twitter's Twitter feeds where is

00:34:06.450 --> 00:34:06.460
Twitter's Twitter feeds where is
 

00:34:06.460 --> 00:34:08.760
Twitter's Twitter feeds where is
somebody right now what are they doing

00:34:08.760 --> 00:34:08.770
somebody right now what are they doing
 

00:34:08.770 --> 00:34:11.159
somebody right now what are they doing
and so on

00:34:11.159 --> 00:34:11.169
and so on
 

00:34:11.169 --> 00:34:12.599
and so on
what are they tweeted about in the past

00:34:12.599 --> 00:34:12.609
what are they tweeted about in the past
 

00:34:12.609 --> 00:34:16.260
what are they tweeted about in the past
and showed how you can build very

00:34:16.260 --> 00:34:16.270
and showed how you can build very
 

00:34:16.270 --> 00:34:20.490
and showed how you can build very
personalized tweets in this case tweets

00:34:20.490 --> 00:34:20.500
personalized tweets in this case tweets
 

00:34:20.500 --> 00:34:22.950
personalized tweets in this case tweets
being written by a neural network that

00:34:22.950 --> 00:34:22.960
being written by a neural network that
 

00:34:22.960 --> 00:34:26.280
being written by a neural network that
would maximally have a maximal

00:34:26.280 --> 00:34:26.290
would maximally have a maximal
 

00:34:26.290 --> 00:34:28.859
would maximally have a maximal
probability of a user clicking on a link

00:34:28.859 --> 00:34:28.869
probability of a user clicking on a link
 

00:34:28.869 --> 00:34:33.419
probability of a user clicking on a link
and even created toolkits you can now

00:34:33.419 --> 00:34:33.429
and even created toolkits you can now
 

00:34:33.429 --> 00:34:36.119
and even created toolkits you can now
download and use to build your own Robo

00:34:36.119 --> 00:34:36.129
download and use to build your own Robo
 

00:34:36.129 --> 00:34:40.820
download and use to build your own Robo
tweeter we've heard a lot about

00:34:40.820 --> 00:34:40.830
tweeter we've heard a lot about
 

00:34:40.830 --> 00:34:42.869
tweeter we've heard a lot about
Cambridge analytical recently and

00:34:42.869 --> 00:34:42.879
Cambridge analytical recently and
 

00:34:42.879 --> 00:34:46.589
Cambridge analytical recently and
Facebook data well a few years ago when

00:34:46.589 --> 00:34:46.599
Facebook data well a few years ago when
 

00:34:46.599 --> 00:34:48.690
Facebook data well a few years ago when
I made this slide before we had such

00:34:48.690 --> 00:34:48.700
I made this slide before we had such
 

00:34:48.700 --> 00:34:51.000
I made this slide before we had such
noteworthy news I was talking about the

00:34:51.000 --> 00:34:51.010
noteworthy news I was talking about the
 

00:34:51.010 --> 00:34:52.859
noteworthy news I was talking about the
concerns in less academic paper by

00:34:52.859 --> 00:34:52.869
concerns in less academic paper by
 

00:34:52.869 --> 00:34:55.530
concerns in less academic paper by
Kaczynski at L that showed from just

00:34:55.530 --> 00:34:55.540
Kaczynski at L that showed from just
 

00:34:55.540 --> 00:34:57.329
Kaczynski at L that showed from just
Facebook data from a little bit of

00:34:57.329 --> 00:34:57.339
Facebook data from a little bit of
 

00:34:57.339 --> 00:34:59.880
Facebook data from a little bit of
training data I'm not sure what little

00:34:59.880 --> 00:34:59.890
training data I'm not sure what little
 

00:34:59.890 --> 00:35:03.420
training data I'm not sure what little
bit was by the way but but here the

00:35:03.420 --> 00:35:03.430
bit was by the way but but here the
 

00:35:03.430 --> 00:35:05.730
bit was by the way but but here the
inferences that are being made from just

00:35:05.730 --> 00:35:05.740
inferences that are being made from just
 

00:35:05.740 --> 00:35:09.050
inferences that are being made from just
a facebook scan of your Facebook of feed

00:35:09.050 --> 00:35:09.060
a facebook scan of your Facebook of feed
 

00:35:09.060 --> 00:35:11.370
a facebook scan of your Facebook of feed
are you single on orally or in

00:35:11.370 --> 00:35:11.380
are you single on orally or in
 

00:35:11.380 --> 00:35:14.790
are you single on orally or in
relationship are your parents together a

00:35:14.790 --> 00:35:14.800
relationship are your parents together a
 

00:35:14.800 --> 00:35:19.740
relationship are your parents together a
twenty one Christianity versus Islam gay

00:35:19.740 --> 00:35:19.750
twenty one Christianity versus Islam gay
 

00:35:19.750 --> 00:35:23.430
twenty one Christianity versus Islam gay
lesbian or straight uses drugs and

00:35:23.430 --> 00:35:23.440
lesbian or straight uses drugs and
 

00:35:23.440 --> 00:35:27.480
lesbian or straight uses drugs and
alcohol satisfied with life that's

00:35:27.480 --> 00:35:27.490
alcohol satisfied with life that's
 

00:35:27.490 --> 00:35:29.030
alcohol satisfied with life that's
interesting

00:35:29.030 --> 00:35:29.040
interesting
 

00:35:29.040 --> 00:35:35.160
interesting
emotional stability age gender and so on

00:35:35.160 --> 00:35:35.170
emotional stability age gender and so on
 

00:35:35.170 --> 00:35:36.870
emotional stability age gender and so on
so there's clearly going to be need for

00:35:36.870 --> 00:35:36.880
so there's clearly going to be need for
 

00:35:36.880 --> 00:35:39.300
so there's clearly going to be need for
some reflection and guidance in all

00:35:39.300 --> 00:35:39.310
some reflection and guidance in all
 

00:35:39.310 --> 00:35:40.860
some reflection and guidance in all
those areas that I mentioned today the

00:35:40.860 --> 00:35:40.870
those areas that I mentioned today the
 

00:35:40.870 --> 00:35:43.710
those areas that I mentioned today the
capabilities and had a handle blind

00:35:43.710 --> 00:35:43.720
capabilities and had a handle blind
 

00:35:43.720 --> 00:35:47.580
capabilities and had a handle blind
spots and biases the values and the idea

00:35:47.580 --> 00:35:47.590
spots and biases the values and the idea
 

00:35:47.590 --> 00:35:50.940
spots and biases the values and the idea
of principle agency and the this whole

00:35:50.940 --> 00:35:50.950
of principle agency and the this whole
 

00:35:50.950 --> 00:35:54.660
of principle agency and the this whole
area of the the potential harms that

00:35:54.660 --> 00:35:54.670
area of the the potential harms that
 

00:35:54.670 --> 00:35:58.200
area of the the potential harms that
might come with use of these systems to

00:35:58.200 --> 00:35:58.210
might come with use of these systems to
 

00:35:58.210 --> 00:36:02.640
might come with use of these systems to
human rights and related areas so I see

00:36:02.640 --> 00:36:02.650
human rights and related areas so I see
 

00:36:02.650 --> 00:36:05.340
human rights and related areas so I see
that there's a continuing a need for I

00:36:05.340 --> 00:36:05.350
that there's a continuing a need for I
 

00:36:05.350 --> 00:36:08.340
that there's a continuing a need for I
would say continuing processes like the

00:36:08.340 --> 00:36:08.350
would say continuing processes like the
 

00:36:08.350 --> 00:36:10.230
would say continuing processes like the
endowment that came to CMU for this

00:36:10.230 --> 00:36:10.240
endowment that came to CMU for this
 

00:36:10.240 --> 00:36:14.340
endowment that came to CMU for this
program for study and productivity we

00:36:14.340 --> 00:36:14.350
program for study and productivity we
 

00:36:14.350 --> 00:36:15.870
program for study and productivity we
need to fund these kinds of programs and

00:36:15.870 --> 00:36:15.880
need to fund these kinds of programs and
 

00:36:15.880 --> 00:36:19.530
need to fund these kinds of programs and
centers there are roles for industry for

00:36:19.530 --> 00:36:19.540
centers there are roles for industry for
 

00:36:19.540 --> 00:36:22.170
centers there are roles for industry for
the academy for civil society and

00:36:22.170 --> 00:36:22.180
the academy for civil society and
 

00:36:22.180 --> 00:36:26.610
the academy for civil society and
government five years ago or so my wife

00:36:26.610 --> 00:36:26.620
government five years ago or so my wife
 

00:36:26.620 --> 00:36:30.690
government five years ago or so my wife
and I came up with a crazy idea and this

00:36:30.690 --> 00:36:30.700
and I came up with a crazy idea and this
 

00:36:30.700 --> 00:36:33.810
and I came up with a crazy idea and this
was at the kind of the five-year point

00:36:33.810 --> 00:36:33.820
was at the kind of the five-year point
 

00:36:33.820 --> 00:36:36.000
was at the kind of the five-year point
after the Asilomar study the triple AI

00:36:36.000 --> 00:36:36.010
after the Asilomar study the triple AI
 

00:36:36.010 --> 00:36:38.550
after the Asilomar study the triple AI
study and we looked at the report we

00:36:38.550 --> 00:36:38.560
study and we looked at the report we
 

00:36:38.560 --> 00:36:40.080
study and we looked at the report we
said so many interesting questions came

00:36:40.080 --> 00:36:40.090
said so many interesting questions came
 

00:36:40.090 --> 00:36:43.140
said so many interesting questions came
up and so many were so prescient in

00:36:43.140 --> 00:36:43.150
up and so many were so prescient in
 

00:36:43.150 --> 00:36:45.720
up and so many were so prescient in
terms of what was being said about what

00:36:45.720 --> 00:36:45.730
terms of what was being said about what
 

00:36:45.730 --> 00:36:47.670
terms of what was being said about what
we needed to care about two or three

00:36:47.670 --> 00:36:47.680
we needed to care about two or three
 

00:36:47.680 --> 00:36:50.370
we needed to care about two or three
things were like just dead on maybe we

00:36:50.370 --> 00:36:50.380
things were like just dead on maybe we
 

00:36:50.380 --> 00:36:51.390
things were like just dead on maybe we
were lucky but they were very

00:36:51.390 --> 00:36:51.400
were lucky but they were very
 

00:36:51.400 --> 00:36:52.830
were lucky but they were very
interesting and if we had followed some

00:36:52.830 --> 00:36:52.840
interesting and if we had followed some
 

00:36:52.840 --> 00:36:53.820
interesting and if we had followed some
of the guidance we would have been

00:36:53.820 --> 00:36:53.830
of the guidance we would have been
 

00:36:53.830 --> 00:36:55.980
of the guidance we would have been
further ahead we thought let's do

00:36:55.980 --> 00:36:55.990
further ahead we thought let's do
 

00:36:55.990 --> 00:36:59.160
further ahead we thought let's do
another study plus five years but it

00:36:59.160 --> 00:36:59.170
another study plus five years but it
 

00:36:59.170 --> 00:37:01.080
another study plus five years but it
really was clear to me this would have

00:37:01.080 --> 00:37:01.090
really was clear to me this would have
 

00:37:01.090 --> 00:37:03.660
really was clear to me this would have
to happen plus five years several times

00:37:03.660 --> 00:37:03.670
to happen plus five years several times
 

00:37:03.670 --> 00:37:06.120
to happen plus five years several times
and so we thought why don't we endow

00:37:06.120 --> 00:37:06.130
and so we thought why don't we endow
 

00:37:06.130 --> 00:37:08.070
and so we thought why don't we endow
Stanford with with it with the study and

00:37:08.070 --> 00:37:08.080
Stanford with with it with the study and
 

00:37:08.080 --> 00:37:09.360
Stanford with with it with the study and
John Hennessy at the time thought this

00:37:09.360 --> 00:37:09.370
John Hennessy at the time thought this
 

00:37:09.370 --> 00:37:11.130
John Hennessy at the time thought this
was a great idea even though the

00:37:11.130 --> 00:37:11.140
was a great idea even though the
 

00:37:11.140 --> 00:37:12.120
was a great idea even though the
Development Office what we were crazy

00:37:12.120 --> 00:37:12.130
Development Office what we were crazy
 

00:37:12.130 --> 00:37:15.990
Development Office what we were crazy
but every five years forever have a

00:37:15.990 --> 00:37:16.000
but every five years forever have a
 

00:37:16.000 --> 00:37:18.090
but every five years forever have a
standing committee that that works with

00:37:18.090 --> 00:37:18.100
standing committee that that works with
 

00:37:18.100 --> 00:37:19.830
standing committee that that works with
a with a study committee that does a

00:37:19.830 --> 00:37:19.840
a with a study committee that does a
 

00:37:19.840 --> 00:37:21.960
a with a study committee that does a
study that does that same that proactive

00:37:21.960 --> 00:37:21.970
study that does that same that proactive
 

00:37:21.970 --> 00:37:25.500
study that does that same that proactive
advice for governments academia the

00:37:25.500 --> 00:37:25.510
advice for governments academia the
 

00:37:25.510 --> 00:37:27.300
advice for governments academia the
public at large civil society on

00:37:27.300 --> 00:37:27.310
public at large civil society on
 

00:37:27.310 --> 00:37:29.370
public at large civil society on
problems coming to the fore and the

00:37:29.370 --> 00:37:29.380
problems coming to the fore and the
 

00:37:29.380 --> 00:37:32.130
problems coming to the fore and the
first report was published about a year

00:37:32.130 --> 00:37:32.140
first report was published about a year
 

00:37:32.140 --> 00:37:36.550
first report was published about a year
and a half ago

00:37:36.550 --> 00:37:36.560
 
 

00:37:36.560 --> 00:37:39.010
 
what about corporate accountability that

00:37:39.010 --> 00:37:39.020
what about corporate accountability that
 

00:37:39.020 --> 00:37:43.500
what about corporate accountability that
was kind of philanthropic and academic

00:37:43.500 --> 00:37:43.510
 
 

00:37:43.510 --> 00:37:45.820
 
I've been doing a lot of reading lately

00:37:45.820 --> 00:37:45.830
I've been doing a lot of reading lately
 

00:37:45.830 --> 00:37:48.490
I've been doing a lot of reading lately
of Peter Drucker's work who knows who

00:37:48.490 --> 00:37:48.500
of Peter Drucker's work who knows who
 

00:37:48.500 --> 00:37:49.840
of Peter Drucker's work who knows who
Peter Drucker was in this room

00:37:49.840 --> 00:37:49.850
Peter Drucker was in this room
 

00:37:49.850 --> 00:37:52.600
Peter Drucker was in this room
all these Heinz people are here and

00:37:52.600 --> 00:37:52.610
all these Heinz people are here and
 

00:37:52.610 --> 00:37:55.960
all these Heinz people are here and
others so Peter Drucker I think this

00:37:55.960 --> 00:37:55.970
others so Peter Drucker I think this
 

00:37:55.970 --> 00:37:58.810
others so Peter Drucker I think this
defines some some of the 20th century

00:37:58.810 --> 00:37:58.820
defines some some of the 20th century
 

00:37:58.820 --> 00:38:01.480
defines some some of the 20th century
deep thinking about business practices I

00:38:01.480 --> 00:38:01.490
deep thinking about business practices I
 

00:38:01.490 --> 00:38:03.640
deep thinking about business practices I
think his whole he came up with the

00:38:03.640 --> 00:38:03.650
think his whole he came up with the
 

00:38:03.650 --> 00:38:05.790
think his whole he came up with the
whole idea of an MBA being even a field

00:38:05.790 --> 00:38:05.800
whole idea of an MBA being even a field
 

00:38:05.800 --> 00:38:08.620
whole idea of an MBA being even a field
he said that profit is not the primary

00:38:08.620 --> 00:38:08.630
he said that profit is not the primary
 

00:38:08.630 --> 00:38:11.200
he said that profit is not the primary
goal only in a central condition for

00:38:11.200 --> 00:38:11.210
goal only in a central condition for
 

00:38:11.210 --> 00:38:13.330
goal only in a central condition for
sustainability and that a company's

00:38:13.330 --> 00:38:13.340
sustainability and that a company's
 

00:38:13.340 --> 00:38:16.060
sustainability and that a company's
primary primary primary responsibility

00:38:16.060 --> 00:38:16.070
primary primary primary responsibility
 

00:38:16.070 --> 00:38:18.330
primary primary primary responsibility
is to service customers here's a quote

00:38:18.330 --> 00:38:18.340
is to service customers here's a quote
 

00:38:18.340 --> 00:38:20.920
is to service customers here's a quote
leaders in every single institution and

00:38:20.920 --> 00:38:20.930
leaders in every single institution and
 

00:38:20.930 --> 00:38:24.060
leaders in every single institution and
in every sector not just business are

00:38:24.060 --> 00:38:24.070
in every sector not just business are
 

00:38:24.070 --> 00:38:27.220
in every sector not just business are
responsible also for the community as a

00:38:27.220 --> 00:38:27.230
responsible also for the community as a
 

00:38:27.230 --> 00:38:31.180
responsible also for the community as a
whole we're not in it just to maximize

00:38:31.180 --> 00:38:31.190
whole we're not in it just to maximize
 

00:38:31.190 --> 00:38:37.780
whole we're not in it just to maximize
shareholder value that said I've been

00:38:37.780 --> 00:38:37.790
shareholder value that said I've been
 

00:38:37.790 --> 00:38:38.920
shareholder value that said I've been
impressed that I want to be to an

00:38:38.920 --> 00:38:38.930
impressed that I want to be to an
 

00:38:38.930 --> 00:38:41.110
impressed that I want to be to an
advertisement for Microsoft here but

00:38:41.110 --> 00:38:41.120
advertisement for Microsoft here but
 

00:38:41.120 --> 00:38:42.570
advertisement for Microsoft here but
I've been impressed with the the

00:38:42.570 --> 00:38:42.580
I've been impressed with the the
 

00:38:42.580 --> 00:38:45.970
I've been impressed with the the
responsiveness of the company to

00:38:45.970 --> 00:38:45.980
responsiveness of the company to
 

00:38:45.980 --> 00:38:49.870
responsiveness of the company to
concerns about the long term futures and

00:38:49.870 --> 00:38:49.880
concerns about the long term futures and
 

00:38:49.880 --> 00:38:52.840
concerns about the long term futures and
even short term needs and and rough

00:38:52.840 --> 00:38:52.850
even short term needs and and rough
 

00:38:52.850 --> 00:38:56.290
even short term needs and and rough
edges with with AI systems and we set up

00:38:56.290 --> 00:38:56.300
edges with with AI systems and we set up
 

00:38:56.300 --> 00:38:59.080
edges with with AI systems and we set up
about two years ago the ether committee

00:38:59.080 --> 00:38:59.090
about two years ago the ether committee
 

00:38:59.090 --> 00:39:01.540
about two years ago the ether committee
it stands loosely for AI and ethics in

00:39:01.540 --> 00:39:01.550
it stands loosely for AI and ethics in
 

00:39:01.550 --> 00:39:04.540
it stands loosely for AI and ethics in
engineering and research there are seven

00:39:04.540 --> 00:39:04.550
engineering and research there are seven
 

00:39:04.550 --> 00:39:08.950
engineering and research there are seven
working groups it's been an intensive

00:39:08.950 --> 00:39:08.960
working groups it's been an intensive
 

00:39:08.960 --> 00:39:14.860
working groups it's been an intensive
effort touching on every issue that I

00:39:14.860 --> 00:39:14.870
effort touching on every issue that I
 

00:39:14.870 --> 00:39:18.190
effort touching on every issue that I
mentioned today and I should say that

00:39:18.190 --> 00:39:18.200
mentioned today and I should say that
 

00:39:18.200 --> 00:39:21.070
mentioned today and I should say that
I'm happy to say that this committee has

00:39:21.070 --> 00:39:21.080
I'm happy to say that this committee has
 

00:39:21.080 --> 00:39:24.850
I'm happy to say that this committee has
teeth so when it came to so some of them

00:39:24.850 --> 00:39:24.860
teeth so when it came to so some of them
 

00:39:24.860 --> 00:39:26.710
teeth so when it came to so some of them
so mentioned humanitarian issues and

00:39:26.710 --> 00:39:26.720
so mentioned humanitarian issues and
 

00:39:26.720 --> 00:39:30.390
so mentioned humanitarian issues and
some questions have come up about can't

00:39:30.390 --> 00:39:30.400
some questions have come up about can't
 

00:39:30.400 --> 00:39:32.880
some questions have come up about can't
what could a company do when it comes to

00:39:32.880 --> 00:39:32.890
what could a company do when it comes to
 

00:39:32.890 --> 00:39:37.090
what could a company do when it comes to
technologies being used that put it at

00:39:37.090 --> 00:39:37.100
technologies being used that put it at
 

00:39:37.100 --> 00:39:40.510
technologies being used that put it at
risk for human rights commitments and I

00:39:40.510 --> 00:39:40.520
risk for human rights commitments and I
 

00:39:40.520 --> 00:39:44.260
risk for human rights commitments and I
should say that the the ether committee

00:39:44.260 --> 00:39:44.270
should say that the the ether committee
 

00:39:44.270 --> 00:39:46.450
should say that the the ether committee
has has made recommendations which have

00:39:46.450 --> 00:39:46.460
has has made recommendations which have
 

00:39:46.460 --> 00:39:48.640
has has made recommendations which have
gone all the way up to the senior

00:39:48.640 --> 00:39:48.650
gone all the way up to the senior
 

00:39:48.650 --> 00:39:50.360
gone all the way up to the senior
leadership and

00:39:50.360 --> 00:39:50.370
leadership and
 

00:39:50.370 --> 00:39:54.060
leadership and
significant sales have been cut off and

00:39:54.060 --> 00:39:54.070
significant sales have been cut off and
 

00:39:54.070 --> 00:39:59.160
significant sales have been cut off and
in other sales very explicit limitations

00:39:59.160 --> 00:39:59.170
in other sales very explicit limitations
 

00:39:59.170 --> 00:40:00.840
in other sales very explicit limitations
were written down in a terms of usage

00:40:00.840 --> 00:40:00.850
were written down in a terms of usage
 

00:40:00.850 --> 00:40:05.400
were written down in a terms of usage
including may not use data-driven

00:40:05.400 --> 00:40:05.410
including may not use data-driven
 

00:40:05.410 --> 00:40:08.220
including may not use data-driven
pattern recognition for use in face

00:40:08.220 --> 00:40:08.230
pattern recognition for use in face
 

00:40:08.230 --> 00:40:13.010
pattern recognition for use in face
recognition or predictions of this type

00:40:13.010 --> 00:40:13.020
recognition or predictions of this type
 

00:40:13.020 --> 00:40:16.340
recognition or predictions of this type
so if committee has working groups and

00:40:16.340 --> 00:40:16.350
so if committee has working groups and
 

00:40:16.350 --> 00:40:19.770
so if committee has working groups and
we refer to the guiding principles on

00:40:19.770 --> 00:40:19.780
we refer to the guiding principles on
 

00:40:19.780 --> 00:40:22.320
we refer to the guiding principles on
business and human rights and Microsoft

00:40:22.320 --> 00:40:22.330
business and human rights and Microsoft
 

00:40:22.330 --> 00:40:24.600
business and human rights and Microsoft
has its own global global human rights

00:40:24.600 --> 00:40:24.610
has its own global global human rights
 

00:40:24.610 --> 00:40:26.460
has its own global global human rights
statement we take very seriously and

00:40:26.460 --> 00:40:26.470
statement we take very seriously and
 

00:40:26.470 --> 00:40:28.890
statement we take very seriously and
that we see going right up to the top of

00:40:28.890 --> 00:40:28.900
that we see going right up to the top of
 

00:40:28.900 --> 00:40:31.760
that we see going right up to the top of
the company if we say we see a challenge

00:40:31.760 --> 00:40:31.770
the company if we say we see a challenge
 

00:40:31.770 --> 00:40:34.620
the company if we say we see a challenge
we point at this document and we and we

00:40:34.620 --> 00:40:34.630
we point at this document and we and we
 

00:40:34.630 --> 00:40:38.120
we point at this document and we and we
say this is a risk to our commitment

00:40:38.120 --> 00:40:38.130
say this is a risk to our commitment
 

00:40:38.130 --> 00:40:43.730
say this is a risk to our commitment
selling this technology to this customer

00:40:43.730 --> 00:40:43.740
 
 

00:40:43.740 --> 00:40:45.720
 
moving to the community at larger

00:40:45.720 --> 00:40:45.730
moving to the community at larger
 

00:40:45.730 --> 00:40:46.740
moving to the community at larger
community I want to talk a little about

00:40:46.740 --> 00:40:46.750
community I want to talk a little about
 

00:40:46.750 --> 00:40:49.320
community I want to talk a little about
the the before I finish up here the the

00:40:49.320 --> 00:40:49.330
the the before I finish up here the the
 

00:40:49.330 --> 00:40:53.000
the the before I finish up here the the
partnership on AI which was a consortium

00:40:53.000 --> 00:40:53.010
partnership on AI which was a consortium
 

00:40:53.010 --> 00:40:55.920
partnership on AI which was a consortium
set up about started about three years

00:40:55.920 --> 00:40:55.930
set up about started about three years
 

00:40:55.930 --> 00:40:59.660
set up about started about three years
ago it started by with leadership by the

00:40:59.660 --> 00:40:59.670
ago it started by with leadership by the
 

00:40:59.670 --> 00:41:02.040
ago it started by with leadership by the
research leads at the research

00:41:02.040 --> 00:41:02.050
research leads at the research
 

00:41:02.050 --> 00:41:06.230
research leads at the research
laboratories at amazon google deepmind

00:41:06.230 --> 00:41:06.240
laboratories at amazon google deepmind
 

00:41:06.240 --> 00:41:08.850
laboratories at amazon google deepmind
Microsoft Facebook and IBM we all know

00:41:08.850 --> 00:41:08.860
Microsoft Facebook and IBM we all know
 

00:41:08.860 --> 00:41:12.090
Microsoft Facebook and IBM we all know
each other as colleagues and when it

00:41:12.090 --> 00:41:12.100
each other as colleagues and when it
 

00:41:12.100 --> 00:41:13.680
each other as colleagues and when it
came to discussions at conferences we

00:41:13.680 --> 00:41:13.690
came to discussions at conferences we
 

00:41:13.690 --> 00:41:15.570
came to discussions at conferences we
said I wonder if we can drag our

00:41:15.570 --> 00:41:15.580
said I wonder if we can drag our
 

00:41:15.580 --> 00:41:17.910
said I wonder if we can drag our
companies whether they're kicking us

00:41:17.910 --> 00:41:17.920
companies whether they're kicking us
 

00:41:17.920 --> 00:41:19.920
companies whether they're kicking us
kicking and screaming or they're happy

00:41:19.920 --> 00:41:19.930
kicking and screaming or they're happy
 

00:41:19.930 --> 00:41:23.490
kicking and screaming or they're happy
with all this into a consortium to a

00:41:23.490 --> 00:41:23.500
with all this into a consortium to a
 

00:41:23.500 --> 00:41:27.120
with all this into a consortium to a
nonprofit focused on best practices

00:41:27.120 --> 00:41:27.130
nonprofit focused on best practices
 

00:41:27.130 --> 00:41:29.730
nonprofit focused on best practices
across a set of interesting questions

00:41:29.730 --> 00:41:29.740
across a set of interesting questions
 

00:41:29.740 --> 00:41:32.100
across a set of interesting questions
and challenge areas in at the

00:41:32.100 --> 00:41:32.110
and challenge areas in at the
 

00:41:32.110 --> 00:41:33.780
and challenge areas in at the
intersection of AI people on society

00:41:33.780 --> 00:41:33.790
intersection of AI people on society
 

00:41:33.790 --> 00:41:35.870
intersection of AI people on society
including safety critical systems

00:41:35.870 --> 00:41:35.880
including safety critical systems
 

00:41:35.880 --> 00:41:39.080
including safety critical systems
fairness accountable and transparent

00:41:39.080 --> 00:41:39.090
fairness accountable and transparent
 

00:41:39.090 --> 00:41:44.460
fairness accountable and transparent
algorithms human AI collaboration labor

00:41:44.460 --> 00:41:44.470
algorithms human AI collaboration labor
 

00:41:44.470 --> 00:41:48.240
algorithms human AI collaboration labor
AI labor in the economy deeper social

00:41:48.240 --> 00:41:48.250
AI labor in the economy deeper social
 

00:41:48.250 --> 00:41:49.950
AI labor in the economy deeper social
and societal influences and we're

00:41:49.950 --> 00:41:49.960
and societal influences and we're
 

00:41:49.960 --> 00:41:51.060
and societal influences and we're
talking about some of the issues I

00:41:51.060 --> 00:41:51.070
talking about some of the issues I
 

00:41:51.070 --> 00:41:52.110
talking about some of the issues I
talked about with persuasion and

00:41:52.110 --> 00:41:52.120
talked about with persuasion and
 

00:41:52.120 --> 00:41:56.040
talked about with persuasion and
manipulation and so on and it's a

00:41:56.040 --> 00:41:56.050
manipulation and so on and it's a
 

00:41:56.050 --> 00:41:58.460
manipulation and so on and it's a
balance it led by a balanced board

00:41:58.460 --> 00:41:58.470
balance it led by a balanced board
 

00:41:58.470 --> 00:42:01.400
balance it led by a balanced board
including folks representing the ACLU

00:42:01.400 --> 00:42:01.410
including folks representing the ACLU
 

00:42:01.410 --> 00:42:03.180
including folks representing the ACLU
nonprofit AI research

00:42:03.180 --> 00:42:03.190
nonprofit AI research
 

00:42:03.190 --> 00:42:06.300
nonprofit AI research
foundations economists privacy experts

00:42:06.300 --> 00:42:06.310
foundations economists privacy experts
 

00:42:06.310 --> 00:42:10.680
foundations economists privacy experts
nai scientists recently we brought on

00:42:10.680 --> 00:42:10.690
nai scientists recently we brought on
 

00:42:10.690 --> 00:42:13.410
nai scientists recently we brought on
Tara Lyons to be the executive director

00:42:13.410 --> 00:42:13.420
Tara Lyons to be the executive director
 

00:42:13.420 --> 00:42:15.680
Tara Lyons to be the executive director
and she's doing a great job getting this

00:42:15.680 --> 00:42:15.690
and she's doing a great job getting this
 

00:42:15.690 --> 00:42:20.099
and she's doing a great job getting this
operation up into the air so moving

00:42:20.099 --> 00:42:20.109
operation up into the air so moving
 

00:42:20.109 --> 00:42:25.160
operation up into the air so moving
ahead I see the direction we need to

00:42:25.160 --> 00:42:25.170
ahead I see the direction we need to
 

00:42:25.170 --> 00:42:28.710
ahead I see the direction we need to
pursue includes ongoing research on the

00:42:28.710 --> 00:42:28.720
pursue includes ongoing research on the
 

00:42:28.720 --> 00:42:30.210
pursue includes ongoing research on the
principles the foundations they're so

00:42:30.210 --> 00:42:30.220
principles the foundations they're so
 

00:42:30.220 --> 00:42:32.430
principles the foundations they're so
exciting and we need to better

00:42:32.430 --> 00:42:32.440
exciting and we need to better
 

00:42:32.440 --> 00:42:35.460
exciting and we need to better
understand human intelligence and its

00:42:35.460 --> 00:42:35.470
understand human intelligence and its
 

00:42:35.470 --> 00:42:39.059
understand human intelligence and its
computational variants we need to work

00:42:39.059 --> 00:42:39.069
computational variants we need to work
 

00:42:39.069 --> 00:42:41.640
computational variants we need to work
to apply anything that we know and have

00:42:41.640 --> 00:42:41.650
to apply anything that we know and have
 

00:42:41.650 --> 00:42:45.960
to apply anything that we know and have
learned to key societal challenge

00:42:45.960 --> 00:42:45.970
learned to key societal challenge
 

00:42:45.970 --> 00:42:49.020
learned to key societal challenge
problems we need to continue to identify

00:42:49.020 --> 00:42:49.030
problems we need to continue to identify
 

00:42:49.030 --> 00:42:54.120
problems we need to continue to identify
and address costs and influences of the

00:42:54.120 --> 00:42:54.130
and address costs and influences of the
 

00:42:54.130 --> 00:42:55.800
and address costs and influences of the
technologies and ethical and human

00:42:55.800 --> 00:42:55.810
technologies and ethical and human
 

00:42:55.810 --> 00:42:58.680
technologies and ethical and human
rights challenges and I say that we need

00:42:58.680 --> 00:42:58.690
rights challenges and I say that we need
 

00:42:58.690 --> 00:43:00.450
rights challenges and I say that we need
to collaborate widely on the

00:43:00.450 --> 00:43:00.460
to collaborate widely on the
 

00:43:00.460 --> 00:43:02.609
to collaborate widely on the
technologies and their influence on

00:43:02.609 --> 00:43:02.619
technologies and their influence on
 

00:43:02.619 --> 00:43:05.910
technologies and their influence on
society in a multi-party stakeholder

00:43:05.910 --> 00:43:05.920
society in a multi-party stakeholder
 

00:43:05.920 --> 00:43:08.910
society in a multi-party stakeholder
approach including economists public

00:43:08.910 --> 00:43:08.920
approach including economists public
 

00:43:08.920 --> 00:43:11.730
approach including economists public
policy people psychologists and so on

00:43:11.730 --> 00:43:11.740
policy people psychologists and so on
 

00:43:11.740 --> 00:43:13.710
policy people psychologists and so on
and let me just end back to her

00:43:13.710 --> 00:43:13.720
and let me just end back to her
 

00:43:13.720 --> 00:43:16.920
and let me just end back to her
assignment again and I want to basically

00:43:16.920 --> 00:43:16.930
assignment again and I want to basically
 

00:43:16.930 --> 00:43:21.030
assignment again and I want to basically
set the bar lower for us I think I think

00:43:21.030 --> 00:43:21.040
set the bar lower for us I think I think
 

00:43:21.040 --> 00:43:22.800
set the bar lower for us I think I think
what we're facing some very tough

00:43:22.800 --> 00:43:22.810
what we're facing some very tough
 

00:43:22.810 --> 00:43:25.319
what we're facing some very tough
problems and I want to just talk about

00:43:25.319 --> 00:43:25.329
problems and I want to just talk about
 

00:43:25.329 --> 00:43:28.440
problems and I want to just talk about
satisficing one of herbes words and he

00:43:28.440 --> 00:43:28.450
satisficing one of herbes words and he
 

00:43:28.450 --> 00:43:31.760
satisficing one of herbes words and he
said that professional responsibility is

00:43:31.760 --> 00:43:31.770
said that professional responsibility is
 

00:43:31.770 --> 00:43:34.490
said that professional responsibility is
not to discover the laws of the universe

00:43:34.490 --> 00:43:34.500
not to discover the laws of the universe
 

00:43:34.500 --> 00:43:39.599
not to discover the laws of the universe
but to act responsibly in the world by

00:43:39.599 --> 00:43:39.609
but to act responsibly in the world by
 

00:43:39.609 --> 00:43:42.300
but to act responsibly in the world by
transforming existing situations into

00:43:42.300 --> 00:43:42.310
transforming existing situations into
 

00:43:42.310 --> 00:43:46.020
transforming existing situations into
more preferred ones I'll stop there

00:43:46.020 --> 00:43:46.030
more preferred ones I'll stop there
 

00:43:46.030 --> 00:43:47.100
more preferred ones I'll stop there
Thanks

00:43:47.100 --> 00:43:47.110
Thanks
 

00:43:47.110 --> 00:43:56.600
Thanks
[Applause]

00:43:56.600 --> 00:43:56.610
 
 

00:43:56.610 --> 00:43:58.830
 
thank you so much it's a really

00:43:58.830 --> 00:43:58.840
thank you so much it's a really
 

00:43:58.840 --> 00:44:00.900
thank you so much it's a really
wonderful talk and I think fit all of

00:44:00.900 --> 00:44:00.910
wonderful talk and I think fit all of
 

00:44:00.910 --> 00:44:03.150
wonderful talk and I think fit all of
the themes I think every single one of

00:44:03.150 --> 00:44:03.160
the themes I think every single one of
 

00:44:03.160 --> 00:44:05.340
the themes I think every single one of
the session themes over the course of it

00:44:05.340 --> 00:44:05.350
the session themes over the course of it
 

00:44:05.350 --> 00:44:08.820
the session themes over the course of it
I tried I wanted to start by asking

00:44:08.820 --> 00:44:08.830
I tried I wanted to start by asking
 

00:44:08.830 --> 00:44:12.210
I tried I wanted to start by asking
about actually a theme that has been a

00:44:12.210 --> 00:44:12.220
about actually a theme that has been a
 

00:44:12.220 --> 00:44:14.490
about actually a theme that has been a
part of your research since the

00:44:14.490 --> 00:44:14.500
part of your research since the
 

00:44:14.500 --> 00:44:17.580
part of your research since the
beginning which is this idea that we

00:44:17.580 --> 00:44:17.590
beginning which is this idea that we
 

00:44:17.590 --> 00:44:23.160
beginning which is this idea that we
need to fit nicely with features human

00:44:23.160 --> 00:44:23.170
need to fit nicely with features human
 

00:44:23.170 --> 00:44:26.310
need to fit nicely with features human
cognitive biases and so in particular

00:44:26.310 --> 00:44:26.320
cognitive biases and so in particular
 

00:44:26.320 --> 00:44:28.830
cognitive biases and so in particular
there are now many calls for AI to be

00:44:28.830 --> 00:44:28.840
there are now many calls for AI to be
 

00:44:28.840 --> 00:44:31.680
there are now many calls for AI to be
more transparent or explainable could

00:44:31.680 --> 00:44:31.690
more transparent or explainable could
 

00:44:31.690 --> 00:44:32.940
more transparent or explainable could
you say a bit about what you think is

00:44:32.940 --> 00:44:32.950
you say a bit about what you think is
 

00:44:32.950 --> 00:44:36.450
you say a bit about what you think is
needed for AI beyond just accuracy or

00:44:36.450 --> 00:44:36.460
needed for AI beyond just accuracy or
 

00:44:36.460 --> 00:44:40.440
needed for AI beyond just accuracy or
simple performance measures like that so

00:44:40.440 --> 00:44:40.450
simple performance measures like that so
 

00:44:40.450 --> 00:44:42.860
simple performance measures like that so
there are many applications of AI

00:44:42.860 --> 00:44:42.870
there are many applications of AI
 

00:44:42.870 --> 00:44:45.270
there are many applications of AI
including many applications that you

00:44:45.270 --> 00:44:45.280
including many applications that you
 

00:44:45.280 --> 00:44:47.900
including many applications that you
might say are running under the hood so

00:44:47.900 --> 00:44:47.910
might say are running under the hood so
 

00:44:47.910 --> 00:44:50.490
might say are running under the hood so
Windows 10 for example has machine

00:44:50.490 --> 00:44:50.500
Windows 10 for example has machine
 

00:44:50.500 --> 00:44:52.260
Windows 10 for example has machine
learning running all the time in a

00:44:52.260 --> 00:44:52.270
learning running all the time in a
 

00:44:52.270 --> 00:44:54.750
learning running all the time in a
private way under the hood guessing what

00:44:54.750 --> 00:44:54.760
private way under the hood guessing what
 

00:44:54.760 --> 00:44:57.600
private way under the hood guessing what
you'll do next and and then prefetching

00:44:57.600 --> 00:44:57.610
you'll do next and and then prefetching
 

00:44:57.610 --> 00:44:59.280
you'll do next and and then prefetching
and pre-loading so everything's faster

00:44:59.280 --> 00:44:59.290
and pre-loading so everything's faster
 

00:44:59.290 --> 00:45:01.830
and pre-loading so everything's faster
magically if it's done well and I think

00:45:01.830 --> 00:45:01.840
magically if it's done well and I think
 

00:45:01.840 --> 00:45:03.690
magically if it's done well and I think
it is it's been running for for a number

00:45:03.690 --> 00:45:03.700
it is it's been running for for a number
 

00:45:03.700 --> 00:45:05.070
it is it's been running for for a number
of years and different versions since

00:45:05.070 --> 00:45:05.080
of years and different versions since
 

00:45:05.080 --> 00:45:08.790
of years and different versions since
Windows 7 we don't ask for transparency

00:45:08.790 --> 00:45:08.800
Windows 7 we don't ask for transparency
 

00:45:08.800 --> 00:45:09.720
Windows 7 we don't ask for transparency
on that unless you have a special

00:45:09.720 --> 00:45:09.730
on that unless you have a special
 

00:45:09.730 --> 00:45:11.130
on that unless you have a special
request as to what's going on exactly

00:45:11.130 --> 00:45:11.140
request as to what's going on exactly
 

00:45:11.140 --> 00:45:14.460
request as to what's going on exactly
want it to be very efficient in human

00:45:14.460 --> 00:45:14.470
want it to be very efficient in human
 

00:45:14.470 --> 00:45:15.930
want it to be very efficient in human
facing technologies like dialogue

00:45:15.930 --> 00:45:15.940
facing technologies like dialogue
 

00:45:15.940 --> 00:45:18.000
facing technologies like dialogue
systems we want to have we might want to

00:45:18.000 --> 00:45:18.010
systems we want to have we might want to
 

00:45:18.010 --> 00:45:19.860
systems we want to have we might want to
have other features of you know human

00:45:19.860 --> 00:45:19.870
have other features of you know human
 

00:45:19.870 --> 00:45:22.190
have other features of you know human
more human-like comfortable with humans

00:45:22.190 --> 00:45:22.200
more human-like comfortable with humans
 

00:45:22.200 --> 00:45:25.290
more human-like comfortable with humans
new music don't are not weirded out by

00:45:25.290 --> 00:45:25.300
new music don't are not weirded out by
 

00:45:25.300 --> 00:45:28.740
new music don't are not weirded out by
the technology assurance about of

00:45:28.740 --> 00:45:28.750
the technology assurance about of
 

00:45:28.750 --> 00:45:31.710
the technology assurance about of
privacy you know when it comes to

00:45:31.710 --> 00:45:31.720
privacy you know when it comes to
 

00:45:31.720 --> 00:45:33.390
privacy you know when it comes to
transparency it's interesting question I

00:45:33.390 --> 00:45:33.400
transparency it's interesting question I
 

00:45:33.400 --> 00:45:36.090
transparency it's interesting question I
think that it's gonna vary depending on

00:45:36.090 --> 00:45:36.100
think that it's gonna vary depending on
 

00:45:36.100 --> 00:45:42.210
think that it's gonna vary depending on
usage I see some day us wanting to make

00:45:42.210 --> 00:45:42.220
usage I see some day us wanting to make
 

00:45:42.220 --> 00:45:44.820
usage I see some day us wanting to make
sure that some independent party that we

00:45:44.820 --> 00:45:44.830
sure that some independent party that we
 

00:45:44.830 --> 00:45:48.150
sure that some independent party that we
trust as a proxy has certified for

00:45:48.150 --> 00:45:48.160
trust as a proxy has certified for
 

00:45:48.160 --> 00:45:50.750
trust as a proxy has certified for
example the data sets that were used the

00:45:50.750 --> 00:45:50.760
example the data sets that were used the
 

00:45:50.760 --> 00:45:54.420
example the data sets that were used the
the processes in place that that systems

00:45:54.420 --> 00:45:54.430
the processes in place that that systems
 

00:45:54.430 --> 00:45:56.610
the processes in place that that systems
are fair and unbiased person protected

00:45:56.610 --> 00:45:56.620
are fair and unbiased person protected
 

00:45:56.620 --> 00:45:59.010
are fair and unbiased person protected
variables for example but the best ovens

00:45:59.010 --> 00:45:59.020
variables for example but the best ovens
 

00:45:59.020 --> 00:46:02.070
variables for example but the best ovens
were used on underwear as laboratory or

00:46:02.070 --> 00:46:02.080
were used on underwear as laboratory or
 

00:46:02.080 --> 00:46:03.960
were used on underwear as laboratory or
or or or at FDA

00:46:03.960 --> 00:46:03.970
or or or at FDA
 

00:46:03.970 --> 00:46:05.880
or or or at FDA
somebody's looking at this

00:46:05.880 --> 00:46:05.890
somebody's looking at this
 

00:46:05.890 --> 00:46:08.880
somebody's looking at this
as best practice and relieves me a bit

00:46:08.880 --> 00:46:08.890
as best practice and relieves me a bit
 

00:46:08.890 --> 00:46:10.800
as best practice and relieves me a bit
of the need to go inspecting and why

00:46:10.800 --> 00:46:10.810
of the need to go inspecting and why
 

00:46:10.810 --> 00:46:13.280
of the need to go inspecting and why
don't you know all about what's going on

00:46:13.280 --> 00:46:13.290
don't you know all about what's going on
 

00:46:13.290 --> 00:46:15.570
don't you know all about what's going on
in the case I think we need to sort of

00:46:15.570 --> 00:46:15.580
in the case I think we need to sort of
 

00:46:15.580 --> 00:46:18.150
in the case I think we need to sort of
think through a system working with an

00:46:18.150 --> 00:46:18.160
think through a system working with an
 

00:46:18.160 --> 00:46:21.210
think through a system working with an
expert how to best explain inferences

00:46:21.210 --> 00:46:21.220
expert how to best explain inferences
 

00:46:21.220 --> 00:46:23.370
expert how to best explain inferences
how to do sensitivity analysis do

00:46:23.370 --> 00:46:23.380
how to do sensitivity analysis do
 

00:46:23.380 --> 00:46:24.690
how to do sensitivity analysis do
something called counterfactuals

00:46:24.690 --> 00:46:24.700
something called counterfactuals
 

00:46:24.700 --> 00:46:26.340
something called counterfactuals
what would have happened if this patient

00:46:26.340 --> 00:46:26.350
what would have happened if this patient
 

00:46:26.350 --> 00:46:28.020
what would have happened if this patient
didn't have that black mole but still

00:46:28.020 --> 00:46:28.030
didn't have that black mole but still
 

00:46:28.030 --> 00:46:33.420
didn't have that black mole but still
had this other dermatological do

00:46:33.420 --> 00:46:33.430
had this other dermatological do
 

00:46:33.430 --> 00:46:35.310
had this other dermatological do
visualizations we found in that work

00:46:35.310 --> 00:46:35.320
visualizations we found in that work
 

00:46:35.320 --> 00:46:38.220
visualizations we found in that work
that I showed you that showing this this

00:46:38.220 --> 00:46:38.230
that I showed you that showing this this
 

00:46:38.230 --> 00:46:40.860
that I showed you that showing this this
2d visualization of how utility how cost

00:46:40.860 --> 00:46:40.870
2d visualization of how utility how cost
 

00:46:40.870 --> 00:46:43.590
2d visualization of how utility how cost
and benefit interacted under uncertainty

00:46:43.590 --> 00:46:43.600
and benefit interacted under uncertainty
 

00:46:43.600 --> 00:46:46.620
and benefit interacted under uncertainty
for a population was very useful in in

00:46:46.620 --> 00:46:46.630
for a population was very useful in in
 

00:46:46.630 --> 00:46:52.490
for a population was very useful in in
understanding policy at a hospital I

00:46:52.490 --> 00:46:52.500
understanding policy at a hospital I
 

00:46:52.500 --> 00:46:57.150
understanding policy at a hospital I
think we can't look at AI as one thing a

00:46:57.150 --> 00:46:57.160
think we can't look at AI as one thing a
 

00:46:57.160 --> 00:46:59.160
think we can't look at AI as one thing a
high-level comment here it's an

00:46:59.160 --> 00:46:59.170
high-level comment here it's an
 

00:46:59.170 --> 00:47:03.360
high-level comment here it's an
interesting phrase I think artificial is

00:47:03.360 --> 00:47:03.370
interesting phrase I think artificial is
 

00:47:03.370 --> 00:47:04.980
interesting phrase I think artificial is
actually an unfortunate I wish it was

00:47:04.980 --> 00:47:04.990
actually an unfortunate I wish it was
 

00:47:04.990 --> 00:47:06.780
actually an unfortunate I wish it was
called computational intelligence it's

00:47:06.780 --> 00:47:06.790
called computational intelligence it's
 

00:47:06.790 --> 00:47:08.700
called computational intelligence it's
artificial scares people I think that

00:47:08.700 --> 00:47:08.710
artificial scares people I think that
 

00:47:08.710 --> 00:47:12.270
artificial scares people I think that
the word but it's a it's it refers to a

00:47:12.270 --> 00:47:12.280
the word but it's a it's it refers to a
 

00:47:12.280 --> 00:47:15.510
the word but it's a it's it refers to a
large set of related disciplines or

00:47:15.510 --> 00:47:15.520
large set of related disciplines or
 

00:47:15.520 --> 00:47:18.660
large set of related disciplines or
radon understanding the kinds of things

00:47:18.660 --> 00:47:18.670
radon understanding the kinds of things
 

00:47:18.670 --> 00:47:21.060
radon understanding the kinds of things
that people can do with ease from a

00:47:21.060 --> 00:47:21.070
that people can do with ease from a
 

00:47:21.070 --> 00:47:23.460
that people can do with ease from a
computational lens and there's many

00:47:23.460 --> 00:47:23.470
computational lens and there's many
 

00:47:23.470 --> 00:47:25.620
computational lens and there's many
things it's a it's a it's a wipe it's a

00:47:25.620 --> 00:47:25.630
things it's a it's a it's a wipe it's a
 

00:47:25.630 --> 00:47:27.930
things it's a it's a it's a wipe it's a
it's an empty canvas with lots of pink

00:47:27.930 --> 00:47:27.940
it's an empty canvas with lots of pink
 

00:47:27.940 --> 00:47:29.400
it's an empty canvas with lots of pink
and looking to many things with it

00:47:29.400 --> 00:47:29.410
and looking to many things with it
 

00:47:29.410 --> 00:47:31.410
and looking to many things with it
and each of those points in that space

00:47:31.410 --> 00:47:31.420
and each of those points in that space
 

00:47:31.420 --> 00:47:32.820
and each of those points in that space
will have different requirements in

00:47:32.820 --> 00:47:32.830
will have different requirements in
 

00:47:32.830 --> 00:47:35.310
will have different requirements in
terms of how best to to adapt to human

00:47:35.310 --> 00:47:35.320
terms of how best to to adapt to human
 

00:47:35.320 --> 00:47:42.420
terms of how best to to adapt to human
beings how best to to extend or

00:47:42.420 --> 00:47:42.430
beings how best to to extend or
 

00:47:42.430 --> 00:47:44.640
beings how best to to extend or
constrain the system in a variety in

00:47:44.640 --> 00:47:44.650
constrain the system in a variety in
 

00:47:44.650 --> 00:47:46.890
constrain the system in a variety in
ways that make it more usable acceptable

00:47:46.890 --> 00:47:46.900
ways that make it more usable acceptable
 

00:47:46.900 --> 00:47:49.830
ways that make it more usable acceptable
for human human interaction well this

00:47:49.830 --> 00:47:49.840
for human human interaction well this
 

00:47:49.840 --> 00:47:52.500
for human human interaction well this
idea that that the AI we have to think

00:47:52.500 --> 00:47:52.510
idea that that the AI we have to think
 

00:47:52.510 --> 00:47:54.810
idea that that the AI we have to think
not just about the AI but what it's for

00:47:54.810 --> 00:47:54.820
not just about the AI but what it's for
 

00:47:54.820 --> 00:47:57.120
not just about the AI but what it's for
so it's AI for a purpose for a goal

00:47:57.120 --> 00:47:57.130
so it's AI for a purpose for a goal
 

00:47:57.130 --> 00:47:59.400
so it's AI for a purpose for a goal
whether that's you know defined in a

00:47:59.400 --> 00:47:59.410
whether that's you know defined in a
 

00:47:59.410 --> 00:48:02.100
whether that's you know defined in a
loss function or some other way I guess

00:48:02.100 --> 00:48:02.110
loss function or some other way I guess
 

00:48:02.110 --> 00:48:03.480
loss function or some other way I guess
that raises a question because of course

00:48:03.480 --> 00:48:03.490
that raises a question because of course
 

00:48:03.490 --> 00:48:05.520
that raises a question because of course
diverse people can have Traverse goals

00:48:05.520 --> 00:48:05.530
diverse people can have Traverse goals
 

00:48:05.530 --> 00:48:07.110
diverse people can have Traverse goals
that might even come into conflict with

00:48:07.110 --> 00:48:07.120
that might even come into conflict with
 

00:48:07.120 --> 00:48:09.510
that might even come into conflict with
one another so is that something that

00:48:09.510 --> 00:48:09.520
one another so is that something that
 

00:48:09.520 --> 00:48:11.580
one another so is that something that
you think falls to the developer does

00:48:11.580 --> 00:48:11.590
you think falls to the developer does
 

00:48:11.590 --> 00:48:13.770
you think falls to the developer does
that fall to the user to figure out what

00:48:13.770 --> 00:48:13.780
that fall to the user to figure out what
 

00:48:13.780 --> 00:48:16.260
that fall to the user to figure out what
the goals are should we allow the AI to

00:48:16.260 --> 00:48:16.270
the goals are should we allow the AI to
 

00:48:16.270 --> 00:48:17.170
the goals are should we allow the AI to
try and learn

00:48:17.170 --> 00:48:17.180
try and learn
 

00:48:17.180 --> 00:48:19.559
try and learn
goals are or is that a fraud project

00:48:19.559 --> 00:48:19.569
goals are or is that a fraud project
 

00:48:19.569 --> 00:48:21.760
goals are or is that a fraud project
well let's just talk about what's

00:48:21.760 --> 00:48:21.770
well let's just talk about what's
 

00:48:21.770 --> 00:48:23.849
well let's just talk about what's
shipping today we're talking about

00:48:23.849 --> 00:48:23.859
shipping today we're talking about
 

00:48:23.859 --> 00:48:27.460
shipping today we're talking about
relatively fixed technologies that

00:48:27.460 --> 00:48:27.470
relatively fixed technologies that
 

00:48:27.470 --> 00:48:30.280
relatively fixed technologies that
recognize patterns and do very

00:48:30.280 --> 00:48:30.290
recognize patterns and do very
 

00:48:30.290 --> 00:48:33.190
recognize patterns and do very
relatively simple things so when we

00:48:33.190 --> 00:48:33.200
relatively simple things so when we
 

00:48:33.200 --> 00:48:34.599
relatively simple things so when we
build those systems we want to annotate

00:48:34.599 --> 00:48:34.609
build those systems we want to annotate
 

00:48:34.609 --> 00:48:37.000
build those systems we want to annotate
them very carefully including how we

00:48:37.000 --> 00:48:37.010
them very carefully including how we
 

00:48:37.010 --> 00:48:38.859
them very carefully including how we
test performance with which is like this

00:48:38.859 --> 00:48:38.869
test performance with which is like this
 

00:48:38.869 --> 00:48:40.839
test performance with which is like this
system was built from this kind of data

00:48:40.839 --> 00:48:40.849
system was built from this kind of data
 

00:48:40.849 --> 00:48:44.589
system was built from this kind of data
its intended purpose was to detect age

00:48:44.589 --> 00:48:44.599
its intended purpose was to detect age
 

00:48:44.599 --> 00:48:48.250
its intended purpose was to detect age
or gender or the emotional status of

00:48:48.250 --> 00:48:48.260
or gender or the emotional status of
 

00:48:48.260 --> 00:48:53.530
or gender or the emotional status of
someone's facial features this system

00:48:53.530 --> 00:48:53.540
someone's facial features this system
 

00:48:53.540 --> 00:48:55.690
someone's facial features this system
performs this well how do we know that

00:48:55.690 --> 00:48:55.700
performs this well how do we know that
 

00:48:55.700 --> 00:48:57.970
performs this well how do we know that
here is the representative test that we

00:48:57.970 --> 00:48:57.980
here is the representative test that we
 

00:48:57.980 --> 00:49:01.349
here is the representative test that we
used to even understand this performance

00:49:01.349 --> 00:49:01.359
used to even understand this performance
 

00:49:01.359 --> 00:49:04.839
used to even understand this performance
which captures an intended purpose now

00:49:04.839 --> 00:49:04.849
which captures an intended purpose now
 

00:49:04.849 --> 00:49:08.500
which captures an intended purpose now
you can imagine weaving that inference

00:49:08.500 --> 00:49:08.510
you can imagine weaving that inference
 

00:49:08.510 --> 00:49:10.450
you can imagine weaving that inference
into another system that's using it in

00:49:10.450 --> 00:49:10.460
into another system that's using it in
 

00:49:10.460 --> 00:49:13.630
into another system that's using it in
some way that still understands the

00:49:13.630 --> 00:49:13.640
some way that still understands the
 

00:49:13.640 --> 00:49:19.480
some way that still understands the
intended purpose now in the future in a

00:49:19.480 --> 00:49:19.490
intended purpose now in the future in a
 

00:49:19.490 --> 00:49:21.280
intended purpose now in the future in a
future World Wars training systems in

00:49:21.280 --> 00:49:21.290
future World Wars training systems in
 

00:49:21.290 --> 00:49:23.589
future World Wars training systems in
the in the wild to learn general things

00:49:23.589 --> 00:49:23.599
the in the wild to learn general things
 

00:49:23.599 --> 00:49:25.809
the in the wild to learn general things
about life which doesn't exist now you

00:49:25.809 --> 00:49:25.819
about life which doesn't exist now you
 

00:49:25.819 --> 00:49:27.359
about life which doesn't exist now you
can imagine also to mention questions

00:49:27.359 --> 00:49:27.369
can imagine also to mention questions
 

00:49:27.369 --> 00:49:32.500
can imagine also to mention questions
coming to the fore about does the system

00:49:32.500 --> 00:49:32.510
coming to the fore about does the system
 

00:49:32.510 --> 00:49:34.569
coming to the fore about does the system
that was trained at the factory to this

00:49:34.569 --> 00:49:34.579
that was trained at the factory to this
 

00:49:34.579 --> 00:49:36.849
that was trained at the factory to this
level and then trained at my home to

00:49:36.849 --> 00:49:36.859
level and then trained at my home to
 

00:49:36.859 --> 00:49:40.089
level and then trained at my home to
with more data to personalize it does it

00:49:40.089 --> 00:49:40.099
with more data to personalize it does it
 

00:49:40.099 --> 00:49:43.780
with more data to personalize it does it
perform well when you know my husband or

00:49:43.780 --> 00:49:43.790
perform well when you know my husband or
 

00:49:43.790 --> 00:49:45.910
perform well when you know my husband or
a wife uses a system or my kid interacts

00:49:45.910 --> 00:49:45.920
a wife uses a system or my kid interacts
 

00:49:45.920 --> 00:49:46.390
a wife uses a system or my kid interacts
with it

00:49:46.390 --> 00:49:46.400
with it
 

00:49:46.400 --> 00:49:47.710
with it
and I think these are big research

00:49:47.710 --> 00:49:47.720
and I think these are big research
 

00:49:47.720 --> 00:49:49.420
and I think these are big research
questions that we wait you don't have

00:49:49.420 --> 00:49:49.430
questions that we wait you don't have
 

00:49:49.430 --> 00:49:52.599
questions that we wait you don't have
great answers for right now a theme that

00:49:52.599 --> 00:49:52.609
great answers for right now a theme that
 

00:49:52.609 --> 00:49:54.039
great answers for right now a theme that
ran through a lot of what you were just

00:49:54.039 --> 00:49:54.049
ran through a lot of what you were just
 

00:49:54.049 --> 00:49:56.530
ran through a lot of what you were just
saying is about the importance of a

00:49:56.530 --> 00:49:56.540
saying is about the importance of a
 

00:49:56.540 --> 00:49:57.910
saying is about the importance of a
certain kind of technical literacy

00:49:57.910 --> 00:49:57.920
certain kind of technical literacy
 

00:49:57.920 --> 00:50:00.609
certain kind of technical literacy
technological literacy that is perhaps

00:50:00.609 --> 00:50:00.619
technological literacy that is perhaps
 

00:50:00.619 --> 00:50:03.099
technological literacy that is perhaps
lacking in a lot of the public and a lot

00:50:03.099 --> 00:50:03.109
lacking in a lot of the public and a lot
 

00:50:03.109 --> 00:50:05.650
lacking in a lot of the public and a lot
of even policy and decision makers in

00:50:05.650 --> 00:50:05.660
of even policy and decision makers in
 

00:50:05.660 --> 00:50:07.210
of even policy and decision makers in
terms of being able to understand the

00:50:07.210 --> 00:50:07.220
terms of being able to understand the
 

00:50:07.220 --> 00:50:10.059
terms of being able to understand the
role of training data or how to go from

00:50:10.059 --> 00:50:10.069
role of training data or how to go from
 

00:50:10.069 --> 00:50:11.380
role of training data or how to go from
it was trained for this purpose

00:50:11.380 --> 00:50:11.390
it was trained for this purpose
 

00:50:11.390 --> 00:50:13.180
it was trained for this purpose
therefore it probably shouldn't be used

00:50:13.180 --> 00:50:13.190
therefore it probably shouldn't be used
 

00:50:13.190 --> 00:50:15.730
therefore it probably shouldn't be used
for this other one right question to ask

00:50:15.730 --> 00:50:15.740
for this other one right question to ask
 

00:50:15.740 --> 00:50:19.750
for this other one right question to ask
ourselves you know it's probably gonna

00:50:19.750 --> 00:50:19.760
ourselves you know it's probably gonna
 

00:50:19.760 --> 00:50:22.450
ourselves you know it's probably gonna
be hard to assume we can train people to

00:50:22.450 --> 00:50:22.460
be hard to assume we can train people to
 

00:50:22.460 --> 00:50:24.640
be hard to assume we can train people to
understand the details of a goes on with

00:50:24.640 --> 00:50:24.650
understand the details of a goes on with
 

00:50:24.650 --> 00:50:26.650
understand the details of a goes on with
these systems but you would hope for

00:50:26.650 --> 00:50:26.660
these systems but you would hope for
 

00:50:26.660 --> 00:50:29.260
these systems but you would hope for
example someday for example when it

00:50:29.260 --> 00:50:29.270
example someday for example when it
 

00:50:29.270 --> 00:50:30.390
example someday for example when it
comes to what

00:50:30.390 --> 00:50:30.400
comes to what
 

00:50:30.400 --> 00:50:32.670
comes to what
and car will do in one of these famous

00:50:32.670 --> 00:50:32.680
and car will do in one of these famous
 

00:50:32.680 --> 00:50:36.269
and car will do in one of these famous
settings or a nuanced situation that

00:50:36.269 --> 00:50:36.279
settings or a nuanced situation that
 

00:50:36.279 --> 00:50:39.690
settings or a nuanced situation that
rather than then being concerned with

00:50:39.690 --> 00:50:39.700
rather than then being concerned with
 

00:50:39.700 --> 00:50:41.640
rather than then being concerned with
what the auto manufacturer had to say

00:50:41.640 --> 00:50:41.650
what the auto manufacturer had to say
 

00:50:41.650 --> 00:50:44.160
what the auto manufacturer had to say
about this the car is not drivable in

00:50:44.160 --> 00:50:44.170
about this the car is not drivable in
 

00:50:44.170 --> 00:50:47.099
about this the car is not drivable in
automated mode until there is clear

00:50:47.099 --> 00:50:47.109
automated mode until there is clear
 

00:50:47.109 --> 00:50:50.160
automated mode until there is clear
signals at the end user watch the

00:50:50.160 --> 00:50:50.170
signals at the end user watch the
 

00:50:50.170 --> 00:50:52.049
signals at the end user watch the
material to learn about what these

00:50:52.049 --> 00:50:52.059
material to learn about what these
 

00:50:52.059 --> 00:50:56.190
material to learn about what these
trade-offs are and then went ahead after

00:50:56.190 --> 00:50:56.200
trade-offs are and then went ahead after
 

00:50:56.200 --> 00:50:58.460
trade-offs are and then went ahead after
even taking an exam or test and and

00:50:58.460 --> 00:50:58.470
even taking an exam or test and and
 

00:50:58.470 --> 00:51:01.710
even taking an exam or test and and
assessed preferences so it's the drivers

00:51:01.710 --> 00:51:01.720
assessed preferences so it's the drivers
 

00:51:01.720 --> 00:51:03.930
assessed preferences so it's the drivers
knowledge in the system when that carbon

00:51:03.930 --> 00:51:03.940
knowledge in the system when that carbon
 

00:51:03.940 --> 00:51:06.150
knowledge in the system when that carbon
was forward or the drivers preferences

00:51:06.150 --> 00:51:06.160
was forward or the drivers preferences
 

00:51:06.160 --> 00:51:08.990
was forward or the drivers preferences
about behavior and you can imagine

00:51:08.990 --> 00:51:09.000
about behavior and you can imagine
 

00:51:09.000 --> 00:51:10.289
about behavior and you can imagine
modularizing

00:51:10.289 --> 00:51:10.299
modularizing
 

00:51:10.299 --> 00:51:12.690
modularizing
operations in a certain way where yes I

00:51:12.690 --> 00:51:12.700
operations in a certain way where yes I
 

00:51:12.700 --> 00:51:14.190
operations in a certain way where yes I
feel comfortable that I could actually

00:51:14.190 --> 00:51:14.200
feel comfortable that I could actually
 

00:51:14.200 --> 00:51:16.589
feel comfortable that I could actually
explain to an end-user even without much

00:51:16.589 --> 00:51:16.599
explain to an end-user even without much
 

00:51:16.599 --> 00:51:19.710
explain to an end-user even without much
computer science education what's gonna

00:51:19.710 --> 00:51:19.720
computer science education what's gonna
 

00:51:19.720 --> 00:51:22.500
computer science education what's gonna
happen in the setting visually and help

00:51:22.500 --> 00:51:22.510
happen in the setting visually and help
 

00:51:22.510 --> 00:51:24.299
happen in the setting visually and help
visualize and with the sets of controls

00:51:24.299 --> 00:51:24.309
visualize and with the sets of controls
 

00:51:24.309 --> 00:51:28.950
visualize and with the sets of controls
get at end-user assessments directly so

00:51:28.950 --> 00:51:28.960
get at end-user assessments directly so
 

00:51:28.960 --> 00:51:33.690
get at end-user assessments directly so
again it's up to computer scientists and

00:51:33.690 --> 00:51:33.700
again it's up to computer scientists and
 

00:51:33.700 --> 00:51:36.599
again it's up to computer scientists and
engineers and manufacturers and

00:51:36.599 --> 00:51:36.609
engineers and manufacturers and
 

00:51:36.609 --> 00:51:38.640
engineers and manufacturers and
salespeople to make sure that these

00:51:38.640 --> 00:51:38.650
salespeople to make sure that these
 

00:51:38.650 --> 00:51:40.680
salespeople to make sure that these
systems these more sophisticated systems

00:51:40.680 --> 00:51:40.690
systems these more sophisticated systems
 

00:51:40.690 --> 00:51:42.660
systems these more sophisticated systems
than we sold in the past are well

00:51:42.660 --> 00:51:42.670
than we sold in the past are well
 

00:51:42.670 --> 00:51:45.510
than we sold in the past are well
understood by the end-user and by the

00:51:45.510 --> 00:51:45.520
understood by the end-user and by the
 

00:51:45.520 --> 00:51:47.130
understood by the end-user and by the
way this end also includes maintenance

00:51:47.130 --> 00:51:47.140
way this end also includes maintenance
 

00:51:47.140 --> 00:51:48.930
way this end also includes maintenance
we never really thought about you know

00:51:48.930 --> 00:51:48.940
we never really thought about you know
 

00:51:48.940 --> 00:51:50.760
we never really thought about you know
what it takes to maintain a system in a

00:51:50.760 --> 00:51:50.770
what it takes to maintain a system in a
 

00:51:50.770 --> 00:51:53.339
what it takes to maintain a system in a
dynamic world once you ship it when we

00:51:53.339 --> 00:51:53.349
dynamic world once you ship it when we
 

00:51:53.349 --> 00:51:55.170
dynamic world once you ship it when we
shipped I'll save this out of 11 for

00:51:55.170 --> 00:51:55.180
shipped I'll save this out of 11 for
 

00:51:55.180 --> 00:51:56.640
shipped I'll save this out of 11 for
know this when we ship that medical

00:51:56.640 --> 00:51:56.650
know this when we ship that medical
 

00:51:56.650 --> 00:52:00.329
know this when we ship that medical
system that remission system years ago

00:52:00.329 --> 00:52:00.339
system that remission system years ago
 

00:52:00.339 --> 00:52:02.309
system that remission system years ago
it worked really well we trained it on

00:52:02.309 --> 00:52:02.319
it worked really well we trained it on
 

00:52:02.319 --> 00:52:04.740
it worked really well we trained it on
and tested in many hospitals what it did

00:52:04.740 --> 00:52:04.750
and tested in many hospitals what it did
 

00:52:04.750 --> 00:52:06.839
and tested in many hospitals what it did
was it trained in tests locally because

00:52:06.839 --> 00:52:06.849
was it trained in tests locally because
 

00:52:06.849 --> 00:52:09.089
was it trained in tests locally because
we found that the data set from one

00:52:09.089 --> 00:52:09.099
we found that the data set from one
 

00:52:09.099 --> 00:52:11.099
we found that the data set from one
hospital didn't do well to generalize to

00:52:11.099 --> 00:52:11.109
hospital didn't do well to generalize to
 

00:52:11.109 --> 00:52:15.289
hospital didn't do well to generalize to
another hospital it was fine right

00:52:15.289 --> 00:52:15.299
another hospital it was fine right
 

00:52:15.299 --> 00:52:17.339
another hospital it was fine right
before we shipped it I said wait a

00:52:17.339 --> 00:52:17.349
before we shipped it I said wait a
 

00:52:17.349 --> 00:52:19.440
before we shipped it I said wait a
minute maintenance we didn't think about

00:52:19.440 --> 00:52:19.450
minute maintenance we didn't think about
 

00:52:19.450 --> 00:52:21.029
minute maintenance we didn't think about
these issues in those days it's a

00:52:21.029 --> 00:52:21.039
these issues in those days it's a
 

00:52:21.039 --> 00:52:23.430
these issues in those days it's a
standalone package that runs with data

00:52:23.430 --> 00:52:23.440
standalone package that runs with data
 

00:52:23.440 --> 00:52:26.099
standalone package that runs with data
locally I said can you um have that

00:52:26.099 --> 00:52:26.109
locally I said can you um have that
 

00:52:26.109 --> 00:52:27.990
locally I said can you um have that
system whenever a tree trains anywhere

00:52:27.990 --> 00:52:28.000
system whenever a tree trains anywhere
 

00:52:28.000 --> 00:52:31.079
system whenever a tree trains anywhere
in the world send email to me with that

00:52:31.079 --> 00:52:31.089
in the world send email to me with that
 

00:52:31.089 --> 00:52:34.829
in the world send email to me with that
curve and you know but nowadays at

00:52:34.829 --> 00:52:34.839
curve and you know but nowadays at
 

00:52:34.839 --> 00:52:37.799
curve and you know but nowadays at
Microsoft we say what are the terms of

00:52:37.799 --> 00:52:37.809
Microsoft we say what are the terms of
 

00:52:37.809 --> 00:52:39.420
Microsoft we say what are the terms of
reference and the terms of usage

00:52:39.420 --> 00:52:39.430
reference and the terms of usage
 

00:52:39.430 --> 00:52:42.059
reference and the terms of usage
including required maintenance and

00:52:42.059 --> 00:52:42.069
including required maintenance and
 

00:52:42.069 --> 00:52:44.430
including required maintenance and
chickens by end-users

00:52:44.430 --> 00:52:44.440
chickens by end-users
 

00:52:44.440 --> 00:52:47.069
chickens by end-users
stand that the world moves around miss

00:52:47.069 --> 00:52:47.079
stand that the world moves around miss
 

00:52:47.079 --> 00:52:49.680
stand that the world moves around miss
dynamic and you have to really keep keep

00:52:49.680 --> 00:52:49.690
dynamic and you have to really keep keep
 

00:52:49.690 --> 00:52:51.059
dynamic and you have to really keep keep
maintaining these systems make sure they

00:52:51.059 --> 00:52:51.069
maintaining these systems make sure they
 

00:52:51.069 --> 00:52:53.220
maintaining these systems make sure they
perform well well of course when we

00:52:53.220 --> 00:52:53.230
perform well well of course when we
 

00:52:53.230 --> 00:52:54.569
perform well well of course when we
think about local learning there's a

00:52:54.569 --> 00:52:54.579
think about local learning there's a
 

00:52:54.579 --> 00:52:56.970
think about local learning there's a
rather notorious case where local

00:52:56.970 --> 00:52:56.980
rather notorious case where local
 

00:52:56.980 --> 00:52:58.230
rather notorious case where local
learning went poorly and that was the

00:52:58.230 --> 00:52:58.240
learning went poorly and that was the
 

00:52:58.240 --> 00:52:58.740
learning went poorly and that was the
chat box

00:52:58.740 --> 00:52:58.750
chat box
 

00:52:58.750 --> 00:53:02.760
chat box
Tay so how do we balance ensuring that

00:53:02.760 --> 00:53:02.770
Tay so how do we balance ensuring that
 

00:53:02.770 --> 00:53:04.859
Tay so how do we balance ensuring that
the local learning occurs in an

00:53:04.859 --> 00:53:04.869
the local learning occurs in an
 

00:53:04.869 --> 00:53:06.300
the local learning occurs in an
unsupervised way there's the human

00:53:06.300 --> 00:53:06.310
unsupervised way there's the human
 

00:53:06.310 --> 00:53:07.530
unsupervised way there's the human
they're necessarily looking over the

00:53:07.530 --> 00:53:07.540
they're necessarily looking over the
 

00:53:07.540 --> 00:53:09.180
they're necessarily looking over the
shoulder anymore but that it learns the

00:53:09.180 --> 00:53:09.190
shoulder anymore but that it learns the
 

00:53:09.190 --> 00:53:11.490
shoulder anymore but that it learns the
runt for achieving the goal that the

00:53:11.490 --> 00:53:11.500
runt for achieving the goal that the
 

00:53:11.500 --> 00:53:13.349
runt for achieving the goal that the
developer and the end user had in mind I

00:53:13.349 --> 00:53:13.359
developer and the end user had in mind I
 

00:53:13.359 --> 00:53:16.290
developer and the end user had in mind I
was going to comment on teh sorry in

00:53:16.290 --> 00:53:16.300
was going to comment on teh sorry in
 

00:53:16.300 --> 00:53:18.089
was going to comment on teh sorry in
that it's a great example of things

00:53:18.089 --> 00:53:18.099
that it's a great example of things
 

00:53:18.099 --> 00:53:21.120
that it's a great example of things
going awry but it wasn't a very deep AI

00:53:21.120 --> 00:53:21.130
going awry but it wasn't a very deep AI
 

00:53:21.130 --> 00:53:23.480
going awry but it wasn't a very deep AI
system at all and it's attacked by

00:53:23.480 --> 00:53:23.490
system at all and it's attacked by
 

00:53:23.490 --> 00:53:26.040
system at all and it's attacked by
malevolent attackers but that doesn't

00:53:26.040 --> 00:53:26.050
malevolent attackers but that doesn't
 

00:53:26.050 --> 00:53:27.569
malevolent attackers but that doesn't
mean it's not a great example to think

00:53:27.569 --> 00:53:27.579
mean it's not a great example to think
 

00:53:27.579 --> 00:53:28.950
mean it's not a great example to think
deeply about what might happen in the

00:53:28.950 --> 00:53:28.960
deeply about what might happen in the
 

00:53:28.960 --> 00:53:33.960
deeply about what might happen in the
future you know you can imagine giving

00:53:33.960 --> 00:53:33.970
future you know you can imagine giving
 

00:53:33.970 --> 00:53:36.420
future you know you can imagine giving
systems the ability to monitor their

00:53:36.420 --> 00:53:36.430
systems the ability to monitor their
 

00:53:36.430 --> 00:53:38.700
systems the ability to monitor their
performance there's lots of chatter

00:53:38.700 --> 00:53:38.710
performance there's lots of chatter
 

00:53:38.710 --> 00:53:41.220
performance there's lots of chatter
about what does it mean to to add a

00:53:41.220 --> 00:53:41.230
about what does it mean to to add a
 

00:53:41.230 --> 00:53:44.849
about what does it mean to to add a
layer of reflection in systems where the

00:53:44.849 --> 00:53:44.859
layer of reflection in systems where the
 

00:53:44.859 --> 00:53:48.089
layer of reflection in systems where the
the meta level reasoner is simpler and

00:53:48.089 --> 00:53:48.099
the meta level reasoner is simpler and
 

00:53:48.099 --> 00:53:50.339
the meta level reasoner is simpler and
it looks out for certain problems it's

00:53:50.339 --> 00:53:50.349
it looks out for certain problems it's
 

00:53:50.349 --> 00:53:52.859
it looks out for certain problems it's
not very complicated and it's there as

00:53:52.859 --> 00:53:52.869
not very complicated and it's there as
 

00:53:52.869 --> 00:53:55.410
not very complicated and it's there as
kind of a century to to make sure that

00:53:55.410 --> 00:53:55.420
kind of a century to to make sure that
 

00:53:55.420 --> 00:53:57.750
kind of a century to to make sure that
the what's going on at the base level

00:53:57.750 --> 00:53:57.760
the what's going on at the base level
 

00:53:57.760 --> 00:53:59.069
the what's going on at the base level
the complicated system trying to learn

00:53:59.069 --> 00:53:59.079
the complicated system trying to learn
 

00:53:59.079 --> 00:54:01.260
the complicated system trying to learn
about the world is not getting off the

00:54:01.260 --> 00:54:01.270
about the world is not getting off the
 

00:54:01.270 --> 00:54:01.710
about the world is not getting off the
rails

00:54:01.710 --> 00:54:01.720
rails
 

00:54:01.720 --> 00:54:04.410
rails
and reports out and actually monitors

00:54:04.410 --> 00:54:04.420
and reports out and actually monitors
 

00:54:04.420 --> 00:54:06.510
and reports out and actually monitors
what's happening so you can imagine we

00:54:06.510 --> 00:54:06.520
what's happening so you can imagine we
 

00:54:06.520 --> 00:54:08.160
what's happening so you can imagine we
get as we build more reliable software

00:54:08.160 --> 00:54:08.170
get as we build more reliable software
 

00:54:08.170 --> 00:54:10.589
get as we build more reliable software
we build into them besides having people

00:54:10.589 --> 00:54:10.599
we build into them besides having people
 

00:54:10.599 --> 00:54:13.200
we build into them besides having people
maintain them and oversee them in

00:54:13.200 --> 00:54:13.210
maintain them and oversee them in
 

00:54:13.210 --> 00:54:15.359
maintain them and oversee them in
certain situations give them the give

00:54:15.359 --> 00:54:15.369
certain situations give them the give
 

00:54:15.369 --> 00:54:18.440
certain situations give them the give
them the ability to have a layer of of

00:54:18.440 --> 00:54:18.450
them the ability to have a layer of of
 

00:54:18.450 --> 00:54:20.970
them the ability to have a layer of of
an overview or a meta view of what's

00:54:20.970 --> 00:54:20.980
an overview or a meta view of what's
 

00:54:20.980 --> 00:54:23.099
an overview or a meta view of what's
going on at the base level so they can

00:54:23.099 --> 00:54:23.109
going on at the base level so they can
 

00:54:23.109 --> 00:54:24.839
going on at the base level so they can
start to recognize that there's some

00:54:24.839 --> 00:54:24.849
start to recognize that there's some
 

00:54:24.849 --> 00:54:26.730
start to recognize that there's some
unknown unknown out there they may not

00:54:26.730 --> 00:54:26.740
unknown unknown out there they may not
 

00:54:26.740 --> 00:54:29.250
unknown unknown out there they may not
right of course and and in fact there's

00:54:29.250 --> 00:54:29.260
right of course and and in fact there's
 

00:54:29.260 --> 00:54:30.839
right of course and and in fact there's
some really fabulous work by Tom

00:54:30.839 --> 00:54:30.849
some really fabulous work by Tom
 

00:54:30.849 --> 00:54:33.000
some really fabulous work by Tom
Dietrich over at Oregon State right now

00:54:33.000 --> 00:54:33.010
Dietrich over at Oregon State right now
 

00:54:33.010 --> 00:54:36.270
Dietrich over at Oregon State right now
on systems that actually learn when

00:54:36.270 --> 00:54:36.280
on systems that actually learn when
 

00:54:36.280 --> 00:54:39.750
on systems that actually learn when
there's a another class out there in

00:54:39.750 --> 00:54:39.760
there's a another class out there in
 

00:54:39.760 --> 00:54:41.609
there's a another class out there in
this case he did it for biology but that

00:54:41.609 --> 00:54:41.619
this case he did it for biology but that
 

00:54:41.619 --> 00:54:44.280
this case he did it for biology but that
the system didn't know about and there

00:54:44.280 --> 00:54:44.290
the system didn't know about and there
 

00:54:44.290 --> 00:54:46.500
the system didn't know about and there
are certain signatures that will show up

00:54:46.500 --> 00:54:46.510
are certain signatures that will show up
 

00:54:46.510 --> 00:54:49.680
are certain signatures that will show up
if there's something you just don't know

00:54:49.680 --> 00:54:49.690
if there's something you just don't know
 

00:54:49.690 --> 00:54:51.510
if there's something you just don't know
in the world that that's doing its thing

00:54:51.510 --> 00:54:51.520
in the world that that's doing its thing
 

00:54:51.520 --> 00:54:53.460
in the world that that's doing its thing
and you have it's the the blind spot has

00:54:53.460 --> 00:54:53.470
and you have it's the the blind spot has
 

00:54:53.470 --> 00:54:55.319
and you have it's the the blind spot has
a signature and so you can imagine that

00:54:55.319 --> 00:54:55.329
a signature and so you can imagine that
 

00:54:55.329 --> 00:54:57.000
a signature and so you can imagine that
kind of method as well

00:54:57.000 --> 00:54:57.010
kind of method as well
 

00:54:57.010 --> 00:55:01.710
kind of method as well
I do think that the nature and the

00:55:01.710 --> 00:55:01.720
I do think that the nature and the
 

00:55:01.720 --> 00:55:03.840
I do think that the nature and the
sophistication of our technologies will

00:55:03.840 --> 00:55:03.850
sophistication of our technologies will
 

00:55:03.850 --> 00:55:07.860
sophistication of our technologies will
be morphing over time and that rather

00:55:07.860 --> 00:55:07.870
be morphing over time and that rather
 

00:55:07.870 --> 00:55:09.860
be morphing over time and that rather
than saying we have solved this problem

00:55:09.860 --> 00:55:09.870
than saying we have solved this problem
 

00:55:09.870 --> 00:55:13.530
than saying we have solved this problem
we need to have these continuing studies

00:55:13.530 --> 00:55:13.540
we need to have these continuing studies
 

00:55:13.540 --> 00:55:16.500
we need to have these continuing studies
like the 100 year study like the ethics

00:55:16.500 --> 00:55:16.510
like the 100 year study like the ethics
 

00:55:16.510 --> 00:55:21.060
like the 100 year study like the ethics
and AI program we have here now like the

00:55:21.060 --> 00:55:21.070
and AI program we have here now like the
 

00:55:21.070 --> 00:55:22.830
and AI program we have here now like the
new block program and so in box Center

00:55:22.830 --> 00:55:22.840
new block program and so in box Center
 

00:55:22.840 --> 00:55:24.390
new block program and so in box Center
here I think they're all gonna be

00:55:24.390 --> 00:55:24.400
here I think they're all gonna be
 

00:55:24.400 --> 00:55:26.100
here I think they're all gonna be
required over time these these

00:55:26.100 --> 00:55:26.110
required over time these these
 

00:55:26.110 --> 00:55:27.930
required over time these these
challenges and opportunities I say

00:55:27.930 --> 00:55:27.940
challenges and opportunities I say
 

00:55:27.940 --> 00:55:30.240
challenges and opportunities I say
aren't going to go away and we'll have

00:55:30.240 --> 00:55:30.250
aren't going to go away and we'll have
 

00:55:30.250 --> 00:55:32.070
aren't going to go away and we'll have
solutions that will cascade and be

00:55:32.070 --> 00:55:32.080
solutions that will cascade and be
 

00:55:32.080 --> 00:55:34.410
solutions that will cascade and be
layered and we'll build on them but I

00:55:34.410 --> 00:55:34.420
layered and we'll build on them but I
 

00:55:34.420 --> 00:55:35.790
layered and we'll build on them but I
think we need to stay on it basically

00:55:35.790 --> 00:55:35.800
think we need to stay on it basically
 

00:55:35.800 --> 00:55:37.620
think we need to stay on it basically
because of technical but because the

00:55:37.620 --> 00:55:37.630
because of technical but because the
 

00:55:37.630 --> 00:55:41.600
because of technical but because the
advances can change our best controls

00:55:41.600 --> 00:55:41.610
advances can change our best controls
 

00:55:41.610 --> 00:55:44.820
advances can change our best controls
meta-reasoning understandings d biasing

00:55:44.820 --> 00:55:44.830
meta-reasoning understandings d biasing
 

00:55:44.830 --> 00:55:46.830
meta-reasoning understandings d biasing
techniques as we go and of course those

00:55:46.830 --> 00:55:46.840
techniques as we go and of course those
 

00:55:46.840 --> 00:55:49.170
techniques as we go and of course those
same things then interact back on the

00:55:49.170 --> 00:55:49.180
same things then interact back on the
 

00:55:49.180 --> 00:55:50.940
same things then interact back on the
human biases so you think about things

00:55:50.940 --> 00:55:50.950
human biases so you think about things
 

00:55:50.950 --> 00:55:54.480
human biases so you think about things
like the people's loss of awareness when

00:55:54.480 --> 00:55:54.490
like the people's loss of awareness when
 

00:55:54.490 --> 00:55:55.740
like the people's loss of awareness when
they go on autopilot

00:55:55.740 --> 00:55:55.750
they go on autopilot
 

00:55:55.750 --> 00:55:58.950
they go on autopilot
so the Tesla takes over for you and you

00:55:58.950 --> 00:55:58.960
so the Tesla takes over for you and you
 

00:55:58.960 --> 00:56:01.290
so the Tesla takes over for you and you
know to look at the the orange cones and

00:56:01.290 --> 00:56:01.300
know to look at the the orange cones and
 

00:56:01.300 --> 00:56:03.690
know to look at the the orange cones and
be ready to take over but we also know

00:56:03.690 --> 00:56:03.700
be ready to take over but we also know
 

00:56:03.700 --> 00:56:05.640
be ready to take over but we also know
from you know aviation studies that it's

00:56:05.640 --> 00:56:05.650
from you know aviation studies that it's
 

00:56:05.650 --> 00:56:07.350
from you know aviation studies that it's
very hard to be able to take over as I'm

00:56:07.350 --> 00:56:07.360
very hard to be able to take over as I'm
 

00:56:07.360 --> 00:56:10.110
very hard to be able to take over as I'm
sure you've also had that experience yet

00:56:10.110 --> 00:56:10.120
sure you've also had that experience yet
 

00:56:10.120 --> 00:56:12.720
sure you've also had that experience yet
the the whole issue of and the

00:56:12.720 --> 00:56:12.730
the the whole issue of and the
 

00:56:12.730 --> 00:56:15.770
the the whole issue of and the
importance of social and cognitive

00:56:15.770 --> 00:56:15.780
importance of social and cognitive
 

00:56:15.780 --> 00:56:20.000
importance of social and cognitive
psychology in understanding the human AI

00:56:20.000 --> 00:56:20.010
psychology in understanding the human AI
 

00:56:20.010 --> 00:56:22.680
psychology in understanding the human AI
relationship and in human at

00:56:22.680 --> 00:56:22.690
relationship and in human at
 

00:56:22.690 --> 00:56:24.330
relationship and in human at
collaboration and interaction more

00:56:24.330 --> 00:56:24.340
collaboration and interaction more
 

00:56:24.340 --> 00:56:28.430
collaboration and interaction more
generally is is is is deep and broad

00:56:28.430 --> 00:56:28.440
generally is is is is deep and broad
 

00:56:28.440 --> 00:56:31.730
generally is is is is deep and broad
it's great that CMU has fabulous

00:56:31.730 --> 00:56:31.740
it's great that CMU has fabulous
 

00:56:31.740 --> 00:56:34.260
it's great that CMU has fabulous
expertise in in both Yuma computer

00:56:34.260 --> 00:56:34.270
expertise in in both Yuma computer
 

00:56:34.270 --> 00:56:37.020
expertise in in both Yuma computer
interaction and AI and now these these

00:56:37.020 --> 00:56:37.030
interaction and AI and now these these
 

00:56:37.030 --> 00:56:39.090
interaction and AI and now these these
ideas are coming together in a variety

00:56:39.090 --> 00:56:39.100
ideas are coming together in a variety
 

00:56:39.100 --> 00:56:41.550
ideas are coming together in a variety
of places but but here as well and just

00:56:41.550 --> 00:56:41.560
of places but but here as well and just
 

00:56:41.560 --> 00:56:44.010
of places but but here as well and just
you thinking about I have some really

00:56:44.010 --> 00:56:44.020
you thinking about I have some really
 

00:56:44.020 --> 00:56:46.940
you thinking about I have some really
great pieces of video that show

00:56:46.940 --> 00:56:46.950
great pieces of video that show
 

00:56:46.950 --> 00:56:50.010
great pieces of video that show
prototype surgeons working

00:56:50.010 --> 00:56:50.020
prototype surgeons working
 

00:56:50.020 --> 00:56:51.630
prototype surgeons working
sorry prototype of robotic surgeons

00:56:51.630 --> 00:56:51.640
sorry prototype of robotic surgeons
 

00:56:51.640 --> 00:56:54.210
sorry prototype of robotic surgeons
working with human surgeons hand in hand

00:56:54.210 --> 00:56:54.220
working with human surgeons hand in hand
 

00:56:54.220 --> 00:56:56.910
working with human surgeons hand in hand
very quickly and you can imagine that if

00:56:56.910 --> 00:56:56.920
very quickly and you can imagine that if
 

00:56:56.920 --> 00:56:58.980
very quickly and you can imagine that if
you had a robotic surgeon working with a

00:56:58.980 --> 00:56:58.990
you had a robotic surgeon working with a
 

00:56:58.990 --> 00:57:02.760
you had a robotic surgeon working with a
human surgeon you see you see literally

00:57:02.760 --> 00:57:02.770
human surgeon you see you see literally
 

00:57:02.770 --> 00:57:06.030
human surgeon you see you see literally
four hands words or you know with three

00:57:06.030 --> 00:57:06.040
four hands words or you know with three
 

00:57:06.040 --> 00:57:06.850
four hands words or you know with three
hands in

00:57:06.850 --> 00:57:06.860
hands in
 

00:57:06.860 --> 00:57:09.100
hands in
in a scene and this signaling going on

00:57:09.100 --> 00:57:09.110
in a scene and this signaling going on
 

00:57:09.110 --> 00:57:10.120
in a scene and this signaling going on
is who's doing what

00:57:10.120 --> 00:57:10.130
is who's doing what
 

00:57:10.130 --> 00:57:12.400
is who's doing what
whose turn is it the idea of a mix of

00:57:12.400 --> 00:57:12.410
whose turn is it the idea of a mix of
 

00:57:12.410 --> 00:57:14.890
whose turn is it the idea of a mix of
initiatives are back and forth how does

00:57:14.890 --> 00:57:14.900
initiatives are back and forth how does
 

00:57:14.900 --> 00:57:16.360
initiatives are back and forth how does
that work how do we do it in a fluid

00:57:16.360 --> 00:57:16.370
that work how do we do it in a fluid
 

00:57:16.370 --> 00:57:19.420
that work how do we do it in a fluid
manner we're sitting here chatting in a

00:57:19.420 --> 00:57:19.430
manner we're sitting here chatting in a
 

00:57:19.430 --> 00:57:21.130
manner we're sitting here chatting in a
fluid manner you're doing predictions

00:57:21.130 --> 00:57:21.140
fluid manner you're doing predictions
 

00:57:21.140 --> 00:57:22.450
fluid manner you're doing predictions
about what I'm gonna end the sentence

00:57:22.450 --> 00:57:22.460
about what I'm gonna end the sentence
 

00:57:22.460 --> 00:57:24.820
about what I'm gonna end the sentence
and someone went to start

00:57:24.820 --> 00:57:24.830
and someone went to start
 

00:57:24.830 --> 00:57:26.650
and someone went to start
who's making contributions there's a

00:57:26.650 --> 00:57:26.660
who's making contributions there's a
 

00:57:26.660 --> 00:57:29.380
who's making contributions there's a
whole rich literature in psychology for

00:57:29.380 --> 00:57:29.390
whole rich literature in psychology for
 

00:57:29.390 --> 00:57:31.900
whole rich literature in psychology for
mutual grounding and conversation that

00:57:31.900 --> 00:57:31.910
mutual grounding and conversation that
 

00:57:31.910 --> 00:57:33.190
mutual grounding and conversation that
we think it's going to be important for

00:57:33.190 --> 00:57:33.200
we think it's going to be important for
 

00:57:33.200 --> 00:57:36.850
we think it's going to be important for
any human AI interaction when it comes

00:57:36.850 --> 00:57:36.860
any human AI interaction when it comes
 

00:57:36.860 --> 00:57:38.950
any human AI interaction when it comes
to interactive AI and fluid

00:57:38.950 --> 00:57:38.960
to interactive AI and fluid
 

00:57:38.960 --> 00:57:41.500
to interactive AI and fluid
collaboration well and also making sure

00:57:41.500 --> 00:57:41.510
collaboration well and also making sure
 

00:57:41.510 --> 00:57:44.170
collaboration well and also making sure
that the the AIS are able to as it were

00:57:44.170 --> 00:57:44.180
that the the AIS are able to as it were
 

00:57:44.180 --> 00:57:46.420
that the the AIS are able to as it were
augmented humans rather than simply

00:57:46.420 --> 00:57:46.430
augmented humans rather than simply
 

00:57:46.430 --> 00:57:48.070
augmented humans rather than simply
replace them which I think connects with

00:57:48.070 --> 00:57:48.080
replace them which I think connects with
 

00:57:48.080 --> 00:57:50.020
replace them which I think connects with
a lot of the the workforce issues that

00:57:50.020 --> 00:57:50.030
a lot of the the workforce issues that
 

00:57:50.030 --> 00:57:51.880
a lot of the the workforce issues that
get raised about the development of AI

00:57:51.880 --> 00:57:51.890
get raised about the development of AI
 

00:57:51.890 --> 00:57:54.220
get raised about the development of AI
technologies right is how can we have

00:57:54.220 --> 00:57:54.230
technologies right is how can we have
 

00:57:54.230 --> 00:57:56.200
technologies right is how can we have
the a I helped the human rather than

00:57:56.200 --> 00:57:56.210
the a I helped the human rather than
 

00:57:56.210 --> 00:57:58.540
the a I helped the human rather than
simply replace the human thereby leaving

00:57:58.540 --> 00:57:58.550
simply replace the human thereby leaving
 

00:57:58.550 --> 00:58:00.760
simply replace the human thereby leaving
them out of the workforce what person

00:58:00.760 --> 00:58:00.770
them out of the workforce what person
 

00:58:00.770 --> 00:58:02.500
them out of the workforce what person
will think that we ever want to

00:58:02.500 --> 00:58:02.510
will think that we ever want to
 

00:58:02.510 --> 00:58:04.630
will think that we ever want to
constrain the advantage of technology to

00:58:04.630 --> 00:58:04.640
constrain the advantage of technology to
 

00:58:04.640 --> 00:58:06.790
constrain the advantage of technology to
say we don't build systems that replace

00:58:06.790 --> 00:58:06.800
say we don't build systems that replace
 

00:58:06.800 --> 00:58:09.190
say we don't build systems that replace
people you know people used to drive

00:58:09.190 --> 00:58:09.200
people you know people used to drive
 

00:58:09.200 --> 00:58:10.870
people you know people used to drive
cars around and adjust the dwell angle

00:58:10.870 --> 00:58:10.880
cars around and adjust the dwell angle
 

00:58:10.880 --> 00:58:12.940
cars around and adjust the dwell angle
of their spark gap of their sparks on

00:58:12.940 --> 00:58:12.950
of their spark gap of their sparks on
 

00:58:12.950 --> 00:58:14.530
of their spark gap of their sparks on
their debt on their steering column I

00:58:14.530 --> 00:58:14.540
their debt on their steering column I
 

00:58:14.540 --> 00:58:15.970
their debt on their steering column I
don't know who remembers that but you

00:58:15.970 --> 00:58:15.980
don't know who remembers that but you
 

00:58:15.980 --> 00:58:18.160
don't know who remembers that but you
know in now sorry and now we have

00:58:18.160 --> 00:58:18.170
know in now sorry and now we have
 

00:58:18.170 --> 00:58:20.440
know in now sorry and now we have
automatic dwell advanced for those of

00:58:20.440 --> 00:58:20.450
automatic dwell advanced for those of
 

00:58:20.450 --> 00:58:23.590
automatic dwell advanced for those of
you car people out there so some jobs

00:58:23.590 --> 00:58:23.600
you car people out there so some jobs
 

00:58:23.600 --> 00:58:25.870
you car people out there so some jobs
you know or tasks we think will be

00:58:25.870 --> 00:58:25.880
you know or tasks we think will be
 

00:58:25.880 --> 00:58:28.030
you know or tasks we think will be
automated and I'll shift the nature of

00:58:28.030 --> 00:58:28.040
automated and I'll shift the nature of
 

00:58:28.040 --> 00:58:31.150
automated and I'll shift the nature of
the human task we hear about who's the

00:58:31.150 --> 00:58:31.160
the human task we hear about who's the
 

00:58:31.160 --> 00:58:33.540
the human task we hear about who's the
radiologist I said I'm gonna go away so

00:58:33.540 --> 00:58:33.550
radiologist I said I'm gonna go away so
 

00:58:33.550 --> 00:58:37.330
radiologist I said I'm gonna go away so
you know as you know radiology is not

00:58:37.330 --> 00:58:37.340
you know as you know radiology is not
 

00:58:37.340 --> 00:58:39.310
you know as you know radiology is not
necessarily about only recognizing

00:58:39.310 --> 00:58:39.320
necessarily about only recognizing
 

00:58:39.320 --> 00:58:42.490
necessarily about only recognizing
patterns and features on on a film it's

00:58:42.490 --> 00:58:42.500
patterns and features on on a film it's
 

00:58:42.500 --> 00:58:43.960
patterns and features on on a film it's
about helping the ordering physician

00:58:43.960 --> 00:58:43.970
about helping the ordering physician
 

00:58:43.970 --> 00:58:47.860
about helping the ordering physician
with the plan the the the rule out the

00:58:47.860 --> 00:58:47.870
with the plan the the the rule out the
 

00:58:47.870 --> 00:58:50.820
with the plan the the the rule out the
strategy what tests will look the best

00:58:50.820 --> 00:58:50.830
strategy what tests will look the best
 

00:58:50.830 --> 00:58:53.230
strategy what tests will look the best
it just will essentially shift the

00:58:53.230 --> 00:58:53.240
it just will essentially shift the
 

00:58:53.240 --> 00:58:55.180
it just will essentially shift the
nature of the job and what's learned to

00:58:55.180 --> 00:58:55.190
nature of the job and what's learned to
 

00:58:55.190 --> 00:58:57.040
nature of the job and what's learned to
be an expert radiologists

00:58:57.040 --> 00:58:57.050
be an expert radiologists
 

00:58:57.050 --> 00:59:00.580
be an expert radiologists
and the solution to a certain sub task

00:59:00.580 --> 00:59:00.590
and the solution to a certain sub task
 

00:59:00.590 --> 00:59:03.370
and the solution to a certain sub task
in radiology plus the human might be

00:59:03.370 --> 00:59:03.380
in radiology plus the human might be
 

00:59:03.380 --> 00:59:05.410
in radiology plus the human might be
create a very different more powerful

00:59:05.410 --> 00:59:05.420
create a very different more powerful
 

00:59:05.420 --> 00:59:07.690
create a very different more powerful
position in the future funny ways for

00:59:07.690 --> 00:59:07.700
position in the future funny ways for
 

00:59:07.700 --> 00:59:09.250
position in the future funny ways for
the human and machine to team together

00:59:09.250 --> 00:59:09.260
the human and machine to team together
 

00:59:09.260 --> 00:59:12.010
the human and machine to team together
team to advance far beyond what either

00:59:12.010 --> 00:59:12.020
team to advance far beyond what either
 

00:59:12.020 --> 00:59:13.360
team to advance far beyond what either
could do right but there's certainly

00:59:13.360 --> 00:59:13.370
could do right but there's certainly
 

00:59:13.370 --> 00:59:15.970
could do right but there's certainly
certainly the automation of the form

00:59:15.970 --> 00:59:15.980
certainly the automation of the form
 

00:59:15.980 --> 00:59:18.140
certainly the automation of the form
that that

00:59:18.140 --> 00:59:18.150
that that
 

00:59:18.150 --> 00:59:20.690
that that
that you know ameliorates the need to

00:59:20.690 --> 00:59:20.700
that you know ameliorates the need to
 

00:59:20.700 --> 00:59:23.210
that you know ameliorates the need to
have a human being on staff Ron on hand

00:59:23.210 --> 00:59:23.220
have a human being on staff Ron on hand
 

00:59:23.220 --> 00:59:25.609
have a human being on staff Ron on hand
now one thing we were discussing

00:59:25.609 --> 00:59:25.619
now one thing we were discussing
 

00:59:25.619 --> 00:59:28.970
now one thing we were discussing
actually this brings up but earlier

00:59:28.970 --> 00:59:28.980
actually this brings up but earlier
 

00:59:28.980 --> 00:59:31.640
actually this brings up but earlier
today at lunch was whether autumn the

00:59:31.640 --> 00:59:31.650
today at lunch was whether autumn the
 

00:59:31.650 --> 00:59:33.799
today at lunch was whether autumn the
rise of automation in the world I'll

00:59:33.799 --> 00:59:33.809
rise of automation in the world I'll
 

00:59:33.809 --> 00:59:37.309
rise of automation in the world I'll
just be toes that's the audience whether

00:59:37.309 --> 00:59:37.319
just be toes that's the audience whether
 

00:59:37.319 --> 00:59:40.279
just be toes that's the audience whether
the rise of automation will will help to

00:59:40.279 --> 00:59:40.289
the rise of automation will will help to
 

00:59:40.289 --> 00:59:44.690
the rise of automation will will help to
pressure the economies into valuing

00:59:44.690 --> 00:59:44.700
pressure the economies into valuing
 

00:59:44.700 --> 00:59:49.010
pressure the economies into valuing
human touch and human contact more the

00:59:49.010 --> 00:59:49.020
human touch and human contact more the
 

00:59:49.020 --> 00:59:51.980
human touch and human contact more the
rise of a caring the economy as some of

00:59:51.980 --> 00:59:51.990
rise of a caring the economy as some of
 

00:59:51.990 --> 00:59:53.809
rise of a caring the economy as some of
the things we rely on people now become

00:59:53.809 --> 00:59:53.819
the things we rely on people now become
 

00:59:53.819 --> 00:59:56.359
the things we rely on people now become
automated if there really is more wealth

00:59:56.359 --> 00:59:56.369
automated if there really is more wealth
 

00:59:56.369 --> 01:00:00.049
automated if there really is more wealth
created by this automation won't we

01:00:00.049 --> 01:00:00.059
created by this automation won't we
 

01:00:00.059 --> 01:00:03.859
created by this automation won't we
actually want to value human experience

01:00:03.859 --> 01:00:03.869
actually want to value human experience
 

01:00:03.869 --> 01:00:07.519
actually want to value human experience
human beings more someday and you know

01:00:07.519 --> 01:00:07.529
human beings more someday and you know
 

01:00:07.529 --> 01:00:10.579
human beings more someday and you know
teachers and mentors and elder care

01:00:10.579 --> 01:00:10.589
teachers and mentors and elder care
 

01:00:10.589 --> 01:00:13.609
teachers and mentors and elder care
providers are the top paid human being

01:00:13.609 --> 01:00:13.619
providers are the top paid human being
 

01:00:13.619 --> 01:00:15.170
providers are the top paid human being
is because they do the job the best I

01:00:15.170 --> 01:00:15.180
is because they do the job the best I
 

01:00:15.180 --> 01:00:17.000
is because they do the job the best I
think educators absolutely should be the

01:00:17.000 --> 01:00:17.010
think educators absolutely should be the
 

01:00:17.010 --> 01:00:19.460
think educators absolutely should be the
type of aid it's a wonderful idea and I

01:00:19.460 --> 01:00:19.470
type of aid it's a wonderful idea and I
 

01:00:19.470 --> 01:00:20.960
type of aid it's a wonderful idea and I
think on that on that wonderful right

01:00:20.960 --> 01:00:20.970
think on that on that wonderful right
 

01:00:20.970 --> 01:00:23.040
think on that on that wonderful right
thank you so much okay thank you

01:00:23.040 --> 01:00:23.050
thank you so much okay thank you
 

01:00:23.050 --> 01:00:31.269
thank you so much okay thank you
[Applause]

