WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.367
[MUSIC PLAYING]

00:00:09.139 --> 00:00:10.703
GAL OSHRI: Hi, everyone.

00:00:10.703 --> 00:00:12.120
My name is Gal
Oshri, and I'm here

00:00:12.120 --> 00:00:14.370
to talk to you about
TensorBoard and what's

00:00:14.370 --> 00:00:17.770
new in TensorFlow's
visualization toolkit.

00:00:17.770 --> 00:00:20.820
So how many you have
used TensorBoard before?

00:00:20.820 --> 00:00:21.575
Awesome.

00:00:21.575 --> 00:00:22.950
So as you know,
TensorBoard helps

00:00:22.950 --> 00:00:26.460
you visualize a lot of the data
that comes out of TensorFlow.

00:00:26.460 --> 00:00:28.160
It has a wide
variety of features,

00:00:28.160 --> 00:00:29.993
but today we're going
to talk about a couple

00:00:29.993 --> 00:00:31.072
of new additions.

00:00:31.072 --> 00:00:33.030
So I'm actually going to
switch over to a demo.

00:00:38.040 --> 00:00:40.580
So this is Colab, a service
from Google Research that

00:00:40.580 --> 00:00:43.670
makes it really easy to get
started with TensorFlow.

00:00:43.670 --> 00:00:45.560
If you've seen the
TensorFlow documentation,

00:00:45.560 --> 00:00:48.050
you've probably used Colab.

00:00:48.050 --> 00:00:49.940
A few minutes before
the demo, I made sure

00:00:49.940 --> 00:00:51.920
that we have installed
TensorFlow 2.0

00:00:51.920 --> 00:00:54.440
Alpha and a couple
of other setup steps,

00:00:54.440 --> 00:00:56.930
so we don't have to
do them right now.

00:00:56.930 --> 00:00:58.685
We're going to use
the FashionMNIST data

00:00:58.685 --> 00:01:02.780
set, one that I'm sure
you've never seen before.

00:01:02.780 --> 00:01:05.930
And we're going to train a
really simple keras sequential

00:01:05.930 --> 00:01:08.540
model, one that just has
a few layers, including

00:01:08.540 --> 00:01:11.000
dents and dropout layers.

00:01:11.000 --> 00:01:13.400
We're going to train
it with a fit API

00:01:13.400 --> 00:01:16.280
and give it the TensorBoard
callback so that we

00:01:16.280 --> 00:01:18.420
make sure that we log the
right data to visualize

00:01:18.420 --> 00:01:19.970
in TensorBoard.

00:01:19.970 --> 00:01:22.123
Now, if you've
used Colab before,

00:01:22.123 --> 00:01:23.540
you'll know that
at this stage, we

00:01:23.540 --> 00:01:26.230
would need to download the
logs to our local machine,

00:01:26.230 --> 00:01:29.090
make sure TensorBoard is set
up, point it at those logs,

00:01:29.090 --> 00:01:32.070
and then look at them, and
then go back to Colab again.

00:01:32.070 --> 00:01:33.480
That's not very convenient.

00:01:33.480 --> 00:01:36.770
So what we've done is enabled
showing TensorBoard directly

00:01:36.770 --> 00:01:38.780
within Colab.

00:01:38.780 --> 00:01:41.870
You'll notice that the way
that we start TensorBoard here

00:01:41.870 --> 00:01:44.360
is exactly the same as
in the command line.

00:01:44.360 --> 00:01:49.010
It's the same command, just
has the magic function in front

00:01:49.010 --> 00:01:51.440
of it.

00:01:51.440 --> 00:01:54.620
The same thing will also
work in Jupyter Notebooks.

00:01:54.620 --> 00:01:56.760
So let's see what's different.

00:01:56.760 --> 00:01:59.480
Well, first of all, when
using the keras callback,

00:01:59.480 --> 00:02:03.230
we now have train validation
showing up on the same charts

00:02:03.230 --> 00:02:06.500
to make it much easier to
compare them in accuracy, loss,

00:02:06.500 --> 00:02:08.060
and other metrics.

00:02:08.060 --> 00:02:12.160
This makes it easier to detect
things like over-fitting.

00:02:12.160 --> 00:02:14.710
In the graphs
dashboard, while seeing

00:02:14.710 --> 00:02:17.140
all of the ops and
auxiliary information

00:02:17.140 --> 00:02:19.810
is useful for many
scenarios, sometimes you just

00:02:19.810 --> 00:02:22.630
want to see what is the model
that I created in keras.

00:02:22.630 --> 00:02:23.900
What are the layers in it?

00:02:23.900 --> 00:02:26.170
So you can now switch
to the keras tag

00:02:26.170 --> 00:02:28.180
and just view that model.

00:02:28.180 --> 00:02:31.570
We can expand this to see the
actual layers that we added.

00:02:34.560 --> 00:02:37.090
There's several other
APIs for using TensorBoard

00:02:37.090 --> 00:02:40.440
within Notebooks that let you
change the height of the cell

00:02:40.440 --> 00:02:42.660
as well as list the
active instances

00:02:42.660 --> 00:02:44.372
within your Colab notebook.

00:02:44.372 --> 00:02:46.080
But we're not going
to look at that today

00:02:46.080 --> 00:02:48.750
because I want to switch gears
and talk about hyperparameter

00:02:48.750 --> 00:02:50.580
tuning.

00:02:50.580 --> 00:02:53.750
Now the model that we
created was extremely simple,

00:02:53.750 --> 00:02:55.890
and we've paid a couple of
hyperparameters for it--

00:02:55.890 --> 00:02:59.040
the dropout rate, the number
of units in the dense layer,

00:02:59.040 --> 00:03:00.738
and the optimizer.

00:03:00.738 --> 00:03:03.030
Now, if we really cared about
that model's performance,

00:03:03.030 --> 00:03:05.368
we're going to want to
try out different values.

00:03:05.368 --> 00:03:07.410
We're going to want to
experiment several of them

00:03:07.410 --> 00:03:09.790
and see which one leads
to the best model.

00:03:09.790 --> 00:03:11.810
The way this would look
today in TensorBoard

00:03:11.810 --> 00:03:13.852
is that you might include
all of that information

00:03:13.852 --> 00:03:16.590
about what the values
are into the run names,

00:03:16.590 --> 00:03:19.890
so as you can see here
in the bottom left.

00:03:19.890 --> 00:03:22.080
You can then filter
using regular expressions

00:03:22.080 --> 00:03:24.750
or go to the chart
and try and hover over

00:03:24.750 --> 00:03:27.570
which line had the
best result to identify

00:03:27.570 --> 00:03:29.210
which values were good.

00:03:29.210 --> 00:03:30.960
It's not the best
experience, so let's see

00:03:30.960 --> 00:03:32.850
if we can do something better.

00:03:32.850 --> 00:03:35.310
Now, what I'm going
to show next is

00:03:35.310 --> 00:03:36.820
something that's
going to change,

00:03:36.820 --> 00:03:39.550
both in terms of
the APIs and the UI.

00:03:39.550 --> 00:03:41.910
But it is available
in the TF 2.0 Alpha

00:03:41.910 --> 00:03:45.200
today, so you can try it out.

00:03:45.200 --> 00:03:48.100
We're going to do several
additional imports

00:03:48.100 --> 00:03:52.120
and define which values of the
hyperparameters we want to try.

00:03:52.120 --> 00:03:54.520
We'll start out with just a
few so that we don't take up

00:03:54.520 --> 00:03:57.520
too much time during the demo.

00:03:57.520 --> 00:04:00.640
We're going to log a summary
that tells TensorBoard what

00:04:00.640 --> 00:04:03.572
were the hyperparameters
that we care about

00:04:03.572 --> 00:04:04.655
and what were the metrics.

00:04:08.230 --> 00:04:10.240
We then wrap the
existing training code

00:04:10.240 --> 00:04:12.310
that we had, just
to make sure that we

00:04:12.310 --> 00:04:16.000
log the accuracy at the
end on the validation set

00:04:16.000 --> 00:04:18.010
and also tell TensorBoard
that the experiment

00:04:18.010 --> 00:04:19.525
has started and finished.

00:04:23.400 --> 00:04:25.590
This time, we're going
to start TensorBoard

00:04:25.590 --> 00:04:28.030
before doing our training
because, in most cases,

00:04:28.030 --> 00:04:30.060
your training will take
longer than one minute

00:04:30.060 --> 00:04:33.420
and you want to view the
TensorBoard while your model is

00:04:33.420 --> 00:04:36.300
training to understand
its progress.

00:04:36.300 --> 00:04:37.320
So we've started it.

00:04:37.320 --> 00:04:38.890
It has no data.

00:04:38.890 --> 00:04:41.410
But once a couple of epochs
of training have finished,

00:04:41.410 --> 00:04:43.810
we can refresh it and
start to see something.

00:04:43.810 --> 00:04:45.870
Now you'll notice
in the top, we now

00:04:45.870 --> 00:04:48.950
have the HPARAMS
dashboard, which shows us,

00:04:48.950 --> 00:04:53.270
at first, a table where each
run is represented by a row,

00:04:53.270 --> 00:04:55.970
and we have columns for
each of the hyperparameters

00:04:55.970 --> 00:04:58.030
and metrics.

00:04:58.030 --> 00:05:00.940
As the runs finish, the
table will become populated

00:05:00.940 --> 00:05:02.640
with them.

00:05:02.640 --> 00:05:06.780
On the left, we have the
ability to filter and sort.

00:05:06.780 --> 00:05:08.280
So we can say that
we don't actually

00:05:08.280 --> 00:05:10.620
care about the number
of units or we only

00:05:10.620 --> 00:05:16.600
want to see experiments where
the accuracy is at least 85.

00:05:19.780 --> 00:05:21.310
So before we proceed
further, I want

00:05:21.310 --> 00:05:26.170
to actually cheat a little
bit and log and access

00:05:26.170 --> 00:05:28.620
some completed
experiments where we've

00:05:28.620 --> 00:05:32.080
run a wider range of
combinations of values

00:05:32.080 --> 00:05:34.185
for the hyperparameters.

00:05:34.185 --> 00:05:35.560
Now while this is
loading, I want

00:05:35.560 --> 00:05:38.950
to point out that I'm
pointing TensorBoard directly

00:05:38.950 --> 00:05:41.860
at a folder in my Google Drive.

00:05:41.860 --> 00:05:43.750
So I had all my
TensorBoard logs maybe

00:05:43.750 --> 00:05:47.332
on another machine, uploaded
them to my Google Drive,

00:05:47.332 --> 00:05:48.790
and then I can
access them directly

00:05:48.790 --> 00:05:51.210
within my Colab Notebook.

00:05:51.210 --> 00:05:53.160
So this takes a moment
to load, but hopefully

00:05:53.160 --> 00:05:55.830
when I refresh we now see it.

00:05:55.830 --> 00:05:58.800
And I can switch over
to the HPARAMS dashboard

00:05:58.800 --> 00:06:02.790
and now see a more complete
set of experiments.

00:06:02.790 --> 00:06:04.770
I can switch over to
the Parallel Coordinates

00:06:04.770 --> 00:06:07.620
View, which shows
me a visualization

00:06:07.620 --> 00:06:13.260
where we have an axis for each
hyperparameter and each metric.

00:06:13.260 --> 00:06:16.410
Each run is represented
by a line that

00:06:16.410 --> 00:06:19.050
passes through all
these axes at the points

00:06:19.050 --> 00:06:23.420
corresponding to its
hyperparameters and values.

00:06:23.420 --> 00:06:27.870
I can click and drag on any axis
to select a particular range.

00:06:27.870 --> 00:06:30.260
So in this case, I've
selected the experiments

00:06:30.260 --> 00:06:33.530
with a relatively high accuracy,
and they become highlighted

00:06:33.530 --> 00:06:35.790
in this visualization.

00:06:35.790 --> 00:06:38.300
I can immediately see
that all these experiments

00:06:38.300 --> 00:06:41.660
used the Adam Optimizer
as opposed to SGD

00:06:41.660 --> 00:06:45.440
and had a relatively high number
of units in the dense layer.

00:06:45.440 --> 00:06:47.570
This gives me some
great ideas about what

00:06:47.570 --> 00:06:51.400
I can experiment with next.

00:06:51.400 --> 00:06:54.022
I can also view the
scatterplot view,

00:06:54.022 --> 00:06:56.230
which shows me the correlations
between the different

00:06:56.230 --> 00:06:59.950
hyperparameters and metrics.

00:06:59.950 --> 00:07:02.290
I can, again,
select a region here

00:07:02.290 --> 00:07:04.795
to view those points
across the other charts.

00:07:09.328 --> 00:07:11.370
So just to summarize,
we've looked at TensorBoard

00:07:11.370 --> 00:07:14.430
in Colab, an easier
way to compare

00:07:14.430 --> 00:07:17.280
the train validation
runs, visualizing

00:07:17.280 --> 00:07:20.520
the keras conceptual graph,
and better hyperparameter

00:07:20.520 --> 00:07:23.430
tuning with the
HPARAMS dashboard.

00:07:23.430 --> 00:07:26.340
All of this information is
available as documentation

00:07:26.340 --> 00:07:29.190
in TensorFlow.org/TensorBoard.

00:07:29.190 --> 00:07:31.260
We also have a
demonstration upstairs,

00:07:31.260 --> 00:07:32.700
and we'd love to hear from you.

00:07:32.700 --> 00:07:35.750
[MUSIC PLAYING]

