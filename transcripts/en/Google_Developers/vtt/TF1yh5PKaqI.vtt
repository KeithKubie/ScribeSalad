WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.353
[MUSIC PLAYING]

00:00:05.748 --> 00:00:06.900
JOSH GORDON: Hey, everyone.

00:00:06.900 --> 00:00:08.400
Today I'd like to
make a quick video

00:00:08.400 --> 00:00:11.246
that I hope will be
concrete and useful for you.

00:00:11.246 --> 00:00:12.620
It's about the
very first machine

00:00:12.620 --> 00:00:14.280
learning library I ever tried.

00:00:14.280 --> 00:00:15.920
And it's called Weka.

00:00:15.920 --> 00:00:18.380
What's great is that Weka
comes with a GUI that

00:00:18.380 --> 00:00:20.480
makes it easy to
visualize your data sets

00:00:20.480 --> 00:00:23.176
and train and compare
different classifiers.

00:00:23.176 --> 00:00:24.800
And this is a really
handy tool to have

00:00:24.800 --> 00:00:26.482
while you're learning ML.

00:00:26.482 --> 00:00:28.690
I'll give you a quick
walkthrough of how to use Weka,

00:00:28.690 --> 00:00:31.280
from installation all the
way to running experiments,

00:00:31.280 --> 00:00:34.040
and show you some
of what it can do.

00:00:34.040 --> 00:00:36.866
I'll demo training models
on two different datasets.

00:00:36.866 --> 00:00:38.240
First, we'll
predict if a patient

00:00:38.240 --> 00:00:41.990
has diabetes based on attributes
like their glucose levels.

00:00:41.990 --> 00:00:43.850
And next, we'll predict
if a congressperson

00:00:43.850 --> 00:00:46.130
is a Democrat or
Republican based on how

00:00:46.130 --> 00:00:48.230
they voted on different bills.

00:00:48.230 --> 00:00:49.790
I'll also show you
how to evaluate

00:00:49.790 --> 00:00:51.770
the results of these
experiments and how

00:00:51.770 --> 00:00:53.270
to do things like
feature selection

00:00:53.270 --> 00:00:55.760
to discover which
attributes are important.

00:00:55.760 --> 00:00:57.750
OK, let's dive right in.

00:00:57.750 --> 00:01:00.527
The first thing we'll do is
download and install Weka.

00:01:00.527 --> 00:01:02.360
And what's neat is that
it comes as a nicely

00:01:02.360 --> 00:01:05.750
packaged application you can
run on Mac, Windows, or Linux.

00:01:05.750 --> 00:01:07.710
There's also a Java API.

00:01:07.710 --> 00:01:09.950
Here, I'll download
and install Weka.

00:01:09.950 --> 00:01:11.970
And now I'll start it up.

00:01:11.970 --> 00:01:15.189
There are different interfaces,
and we'll use the Explorer.

00:01:15.189 --> 00:01:17.480
There's a lot on this screen,
but don't worry about it.

00:01:17.480 --> 00:01:20.140
You'll get a feel for how
this works in a moment.

00:01:20.140 --> 00:01:22.450
The first thing to
do is open a dataset.

00:01:22.450 --> 00:01:23.480
So we'll hit Open.

00:01:23.480 --> 00:01:26.250
And now would be a good
time to download one.

00:01:26.250 --> 00:01:28.170
You can find a bunch
of prepackaged datasets

00:01:28.170 --> 00:01:29.370
on this page.

00:01:29.370 --> 00:01:32.040
And we'll start with
the UCI repository.

00:01:32.040 --> 00:01:34.480
It contains about 37 problems.

00:01:34.480 --> 00:01:36.439
And when you download
it, you'll get a JAR.

00:01:36.439 --> 00:01:37.980
Now, you might be
familiar with these

00:01:37.980 --> 00:01:39.120
if you're a Java developer.

00:01:39.120 --> 00:01:40.380
But if not, don't worry.

00:01:40.380 --> 00:01:42.470
You can treat them as a ZIP.

00:01:42.470 --> 00:01:43.400
Here I'll unzip it.

00:01:43.400 --> 00:01:46.100
And now we can see a
directory of datasets.

00:01:46.100 --> 00:01:48.470
Let's return to Weka and
open one of these up.

00:01:48.470 --> 00:01:50.730
And we'll start with diabetes.

00:01:50.730 --> 00:01:52.190
All right, what do we see here?

00:01:52.190 --> 00:01:53.900
Let me walk you through it.

00:01:53.900 --> 00:01:55.716
First, let's learn
about the dataset.

00:01:55.716 --> 00:01:57.090
At the top, you
can see there are

00:01:57.090 --> 00:02:02.570
768 examples, or instances, and
nine attributes, or features.

00:02:02.570 --> 00:02:05.360
The best attribute to start
with is class, or the label we

00:02:05.360 --> 00:02:06.650
want to predict.

00:02:06.650 --> 00:02:10.340
And usually in Weka, that's the
last attribute in a dataset.

00:02:10.340 --> 00:02:12.716
Clicking on that, we
can see a histogram.

00:02:12.716 --> 00:02:14.090
The blue column
on the left shows

00:02:14.090 --> 00:02:16.910
the number of people who
tested negative for diabetes.

00:02:16.910 --> 00:02:18.530
And the red column
on the right shows

00:02:18.530 --> 00:02:20.417
those who tested positive.

00:02:20.417 --> 00:02:22.000
Now let's look at
the attributes we'll

00:02:22.000 --> 00:02:24.910
use to predict if a
patient has the disease.

00:02:24.910 --> 00:02:26.770
The descriptions here
are pretty short.

00:02:26.770 --> 00:02:28.670
But we can open up
the dataset in Sublime

00:02:28.670 --> 00:02:30.670
or your favorite text
editor to learn more about

00:02:30.670 --> 00:02:33.880
what they mean, as well as
how the dataset was collected.

00:02:33.880 --> 00:02:36.460
Now, Weka datasets
come in an ARF format.

00:02:36.460 --> 00:02:40.740
And this is just a CSV with some
metadata included at the top.

00:02:40.740 --> 00:02:42.720
Scrolling down a bit,
we can see a description

00:02:42.720 --> 00:02:43.986
of the attributes.

00:02:43.986 --> 00:02:45.360
And the first
tells us the number

00:02:45.360 --> 00:02:47.640
of times a patient was
pregnant, and the second

00:02:47.640 --> 00:02:50.040
tells us their plasma glucose.

00:02:50.040 --> 00:02:51.690
For diabetes, I
imagine one of these

00:02:51.690 --> 00:02:53.620
is more predictive
than the other.

00:02:53.620 --> 00:02:56.610
Let's see if Weka can
tell us that, too.

00:02:56.610 --> 00:02:58.980
Back in the GUI,
let's click on plasma.

00:02:58.980 --> 00:03:01.050
And what's cool is you
can see a histogram of how

00:03:01.050 --> 00:03:04.440
different values correlate to
the class we want to predict.

00:03:04.440 --> 00:03:07.326
Recall that blue is negative,
and red is positive.

00:03:07.326 --> 00:03:08.700
And right off the
bat, we can see

00:03:08.700 --> 00:03:12.240
this is a useful attribute,
meaning that if plasma is low,

00:03:12.240 --> 00:03:15.420
say below about 100, then
it's unlikely the patient has

00:03:15.420 --> 00:03:16.780
diabetes.

00:03:16.780 --> 00:03:20.550
Most of these values are blue,
whereas as the value increases,

00:03:20.550 --> 00:03:24.260
it's increasingly likely that
a patient has the disease.

00:03:24.260 --> 00:03:25.840
Now let's look at pregnancy.

00:03:25.840 --> 00:03:29.080
And to me, this doesn't look
like a strong correlation.

00:03:29.080 --> 00:03:32.510
There may be one here,
but it's less obvious.

00:03:32.510 --> 00:03:34.671
Now here's where things
get really interesting.

00:03:34.671 --> 00:03:36.170
I've so much to
show you, but I want

00:03:36.170 --> 00:03:38.550
to jump in and
classify this data.

00:03:38.550 --> 00:03:41.180
So let's head over
to the Classify tab.

00:03:41.180 --> 00:03:43.760
There's a whole bunch
of built-in classifiers.

00:03:43.760 --> 00:03:45.850
Let's start with
the decision tree.

00:03:45.850 --> 00:03:49.530
And J48 is one type of
tree that does pruning.

00:03:49.530 --> 00:03:50.990
We'll hit Start, and bam.

00:03:50.990 --> 00:03:51.830
We're done.

00:03:51.830 --> 00:03:55.350
We just trained a decision
tree on the diabetes dataset.

00:03:55.350 --> 00:03:57.990
Let's say we also wanted to
train a linear classifier,

00:03:57.990 --> 00:03:59.660
like logistic regression.

00:03:59.660 --> 00:04:02.120
To do that, we can go
into Functions, Logistic,

00:04:02.120 --> 00:04:03.110
and hit Start.

00:04:03.110 --> 00:04:04.724
And there we go.

00:04:04.724 --> 00:04:06.140
This is great,
because we can flip

00:04:06.140 --> 00:04:09.800
back and forth between the
two and compare the results.

00:04:09.800 --> 00:04:11.630
There are many types
of classifiers in Weka

00:04:11.630 --> 00:04:13.760
if you're interested,
everything from naive

00:04:13.760 --> 00:04:16.649
Bayes to basic neural networks.

00:04:16.649 --> 00:04:19.191
Now let's head back to the
tree and see the results.

00:04:19.191 --> 00:04:21.190
And there's a lot of
information on this screen,

00:04:21.190 --> 00:04:23.310
so let me walk you through it.

00:04:23.310 --> 00:04:25.860
Let's scroll to the
top and start there.

00:04:25.860 --> 00:04:27.840
First, you can see
the trained tree.

00:04:27.840 --> 00:04:30.450
As always, you read
it from the top down.

00:04:30.450 --> 00:04:33.540
It's telling us to start by
looking at the value of plasma.

00:04:33.540 --> 00:04:35.820
And this happens to be the
most predictive attribute

00:04:35.820 --> 00:04:38.670
in the dataset, but we'll
return to that later.

00:04:38.670 --> 00:04:42.690
Scrolling down, we can see
the accuracy was about 73%.

00:04:42.690 --> 00:04:46.110
But what exactly was the
accuracy evaluated on?

00:04:46.110 --> 00:04:48.914
Well, here you can see Weka
gives you three options.

00:04:48.914 --> 00:04:51.330
The first would be to compute
the accuracy on the training

00:04:51.330 --> 00:04:52.180
set.

00:04:52.180 --> 00:04:54.570
And if we do that, of
course it will be higher.

00:04:54.570 --> 00:04:57.360
It goes up to 84%, because
we've tested the tree

00:04:57.360 --> 00:04:59.550
on data it's already seen.

00:04:59.550 --> 00:05:02.365
Of course, this isn't
useful in the real world.

00:05:02.365 --> 00:05:03.990
As always in machine
learning, our goal

00:05:03.990 --> 00:05:06.240
is to generalize from
the training data.

00:05:06.240 --> 00:05:08.340
Ideally, we want a
model that performs well

00:05:08.340 --> 00:05:10.350
on data it's never seen before.

00:05:10.350 --> 00:05:12.180
So how can we know if it does?

00:05:12.180 --> 00:05:15.690
Well, one way to simulate that
is to have a separate test set.

00:05:15.690 --> 00:05:17.880
Basically, you can divide
the diabetes ARF file

00:05:17.880 --> 00:05:21.630
into two separate files, one for
training and one for testing.

00:05:21.630 --> 00:05:23.910
Use the testing file
only rarely to see how

00:05:23.910 --> 00:05:26.360
well your algorithm performs.

00:05:26.360 --> 00:05:29.030
Another thing we can do
is use cross-validation.

00:05:29.030 --> 00:05:31.010
And this sounds
fancy, but all it does

00:05:31.010 --> 00:05:34.190
is iteratively divide the
dataset into two chunks.

00:05:34.190 --> 00:05:36.020
The larger chunk is
used for training,

00:05:36.020 --> 00:05:38.540
and the smaller one
is used for testing.

00:05:38.540 --> 00:05:40.790
We train a model,
evaluate it, and repeat

00:05:40.790 --> 00:05:44.360
this process a number of times,
then average the results.

00:05:44.360 --> 00:05:47.400
And Weka automates
this process for you.

00:05:47.400 --> 00:05:50.100
Now, let's look more
closely at the evaluation.

00:05:50.100 --> 00:05:53.030
You'll see stats like these
for any classifier you train.

00:05:53.030 --> 00:05:54.530
And importantly,
notice that there

00:05:54.530 --> 00:05:59.200
are metrics like precision and
recall in addition to accuracy.

00:05:59.200 --> 00:06:00.050
Why?

00:06:00.050 --> 00:06:01.880
Well, although accuracy
is the first thing

00:06:01.880 --> 00:06:04.370
we think of when
evaluating a classifier,

00:06:04.370 --> 00:06:06.650
it doesn't always tell
us the whole story,

00:06:06.650 --> 00:06:10.012
especially in datasets
where one class is rare.

00:06:10.012 --> 00:06:11.720
For example, let me
show you how to write

00:06:11.720 --> 00:06:14.840
a 99% accurate classifier
that doesn't really

00:06:14.840 --> 00:06:16.780
do anything at all.

00:06:16.780 --> 00:06:19.450
Imagine you're writing a program
to assist a doctor, like we're

00:06:19.450 --> 00:06:21.640
doing with this
diabetes dataset.

00:06:21.640 --> 00:06:24.850
Now imagine that the disease we
want to predict is very rare.

00:06:24.850 --> 00:06:27.820
Say only one person
in 100 is sick.

00:06:27.820 --> 00:06:31.120
So how can you train a
99% accurate classifier

00:06:31.120 --> 00:06:33.380
without using any ML at all?

00:06:33.380 --> 00:06:34.660
Well, it's simple.

00:06:34.660 --> 00:06:37.270
It turns out you can
write one line of Python.

00:06:37.270 --> 00:06:39.850
Def diagnose return healthy.

00:06:39.850 --> 00:06:41.920
Because most people
are healthy, just

00:06:41.920 --> 00:06:45.120
by predicting that everyone
is, or the majority class,

00:06:45.120 --> 00:06:48.490
we're 99% accurate,
but not useful.

00:06:48.490 --> 00:06:51.920
Our model will always be wrong
when the patient is sick.

00:06:51.920 --> 00:06:53.900
That's why when we
evaluate classifiers,

00:06:53.900 --> 00:06:56.870
we have to look at accuracy both
on the positive and negative

00:06:56.870 --> 00:06:57.920
cases.

00:06:57.920 --> 00:06:59.825
And there are different
ways to do this.

00:06:59.825 --> 00:07:02.740
A confusion matrix like we see
below is one of my favorites.

00:07:02.740 --> 00:07:04.490
And you can find a
link in the description

00:07:04.490 --> 00:07:06.410
to learn more about it.

00:07:06.410 --> 00:07:08.370
Now onto another topic.

00:07:08.370 --> 00:07:10.670
Imagine we had asked the
question, which attributes

00:07:10.670 --> 00:07:12.650
in the dataset are important?

00:07:12.650 --> 00:07:14.180
Here we don't want
to train a model.

00:07:14.180 --> 00:07:16.079
We just want to
explore the data.

00:07:16.079 --> 00:07:18.620
There's a technique we can use
called feature selection which

00:07:18.620 --> 00:07:19.790
can help.

00:07:19.790 --> 00:07:22.280
And the first thing we can
do is rank the attributes

00:07:22.280 --> 00:07:24.250
by their information gain.

00:07:24.250 --> 00:07:27.840
Let's head back to the diabetes
dataset for an example.

00:07:27.840 --> 00:07:31.760
We can hit Filters, Supervised,
Attribute, Attribute Selection,

00:07:31.760 --> 00:07:35.615
then select Info Gain
as the method and Ranker

00:07:35.615 --> 00:07:37.070
as the search.

00:07:37.070 --> 00:07:38.730
And when we run
this, the attributes

00:07:38.730 --> 00:07:41.900
will be sorted by how useful
they are to predict the label.

00:07:41.900 --> 00:07:44.180
If you could know just one
feature from the dataset,

00:07:44.180 --> 00:07:46.187
you'd probably want
to know plasma.

00:07:46.187 --> 00:07:48.020
If you could know two
things, you'd probably

00:07:48.020 --> 00:07:49.880
also want to know mass.

00:07:49.880 --> 00:07:52.212
But keep in mind we
haven't done a search.

00:07:52.212 --> 00:07:54.170
It's possible that these
two attributes are not

00:07:54.170 --> 00:07:55.640
the best combination.

00:07:55.640 --> 00:07:57.140
There are other
methods of selecting

00:07:57.140 --> 00:07:59.180
attributes, like
this, if you want

00:07:59.180 --> 00:08:02.440
to find the best subset to use.

00:08:02.440 --> 00:08:04.720
An exhaustive search can be
computationally expensive,

00:08:04.720 --> 00:08:05.830
though.

00:08:05.830 --> 00:08:07.725
Now let's look briefly
at the vote data.

00:08:07.725 --> 00:08:10.315
I'll move faster this time,
because training and evaluating

00:08:10.315 --> 00:08:13.420
a classifier uses
the same pattern.

00:08:13.420 --> 00:08:15.640
This dataset is from
the US Congress,

00:08:15.640 --> 00:08:17.770
and the goal is to predict
if a representative is

00:08:17.770 --> 00:08:20.050
a Democrat or a
Republican based on how

00:08:20.050 --> 00:08:22.230
they voted on different bills.

00:08:22.230 --> 00:08:24.450
As before, let's
start with class.

00:08:24.450 --> 00:08:27.510
Here, blue are Democrats
and red are Republicans.

00:08:27.510 --> 00:08:30.100
And this dataset was
collected back in the 1980s,

00:08:30.100 --> 00:08:33.070
so these ratios are different
than they are today.

00:08:33.070 --> 00:08:35.049
Each attribute describes
how a congressperson

00:08:35.049 --> 00:08:36.640
voted on different bills.

00:08:36.640 --> 00:08:38.104
And many are predictive.

00:08:38.104 --> 00:08:40.270
If you flip through them,
you'll see that many votes

00:08:40.270 --> 00:08:42.520
are divided along party lines.

00:08:42.520 --> 00:08:45.070
As before, you can read details
about the bills in the ARF

00:08:45.070 --> 00:08:46.660
if you're interested.

00:08:46.660 --> 00:08:48.520
And if we train a tree,
these are the rules

00:08:48.520 --> 00:08:50.590
you can use to predict
the political affiliation

00:08:50.590 --> 00:08:53.920
of a member of Congress based
on their voting history.

00:08:53.920 --> 00:08:56.260
It's still amazing to me how
easy this tool is to use.

00:08:56.260 --> 00:08:58.600
And I find it helpful
on a regular basis.

00:08:58.600 --> 00:09:00.820
I usually begin my experiments
with a decision tree

00:09:00.820 --> 00:09:02.920
to learn more about the
data and as a sanity check

00:09:02.920 --> 00:09:04.840
for a baseline
classifier before I

00:09:04.840 --> 00:09:07.750
move on to more complex
models like neural nets.

00:09:07.750 --> 00:09:08.302
OK.

00:09:08.302 --> 00:09:10.510
I hope this was helpful and
that Weka makes it easier

00:09:10.510 --> 00:09:11.860
for you to learn ML.

00:09:11.860 --> 00:09:14.020
Thanks, everyone, and
I'll see you next time.

00:09:14.020 --> 00:09:17.370
[MUSIC PLAYING]

