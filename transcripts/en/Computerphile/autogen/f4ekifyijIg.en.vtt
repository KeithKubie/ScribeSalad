WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:06.880
We were talking a few weeks ago about how we can add additional processes into a computers (sic) to do specialist tasks

00:00:06.940 --> 00:00:09.680
One of the things we talked about was floating point processors

00:00:09.680 --> 00:00:12.160
Now these days they're built directly onto your CPU

00:00:12.180 --> 00:00:18.100
But, you can still get some CPUs –some of the variants of the ARM CPU– and certainly if you go back in history

00:00:18.280 --> 00:00:24.090
Some of the other CPUs you can get didn't have a floating-point unit. They could do math but they could only do integer math

00:00:24.090 --> 00:00:28.280
So they could add 1 and 2 but they couldn't add 1.5 and 2.5

00:00:28.600 --> 00:00:34.980
Well you "could" but you had to write the software to handle the fractional part of the number an do the math

00:00:34.980 --> 00:00:37.380
and stick it back together in the right order. So I was wondering

00:00:38.050 --> 00:00:41.489
What's the difference in speed that a floating-point person would actually make?

00:00:43.930 --> 00:00:48.809
So as I said most computers we have these days have floating-point units of some sort built-in

00:00:49.690 --> 00:00:55.410
So I went back to one of my old ones and I decided to write a program to test it

00:00:55.410 --> 00:01:00.959
So I wrote a very simple program which does a 3d spinning cube. So the program is relatively straightforward

00:01:01.300 --> 00:01:06.750
It's got a series of eight points which restores representation into a series of matrix transformations on them

00:01:06.970 --> 00:01:09.659
To get them into the screen coordinates and then we draw the lines

00:01:10.000 --> 00:01:14.489
so I did this using floating point maps and the programs running here and we can see it's

00:01:15.100 --> 00:01:22.500
Reasonably quick it takes no point not for five a second to calculate where all the screen coordinates need to be for each frame

00:01:22.780 --> 00:01:28.349
sometimes varies but that's in general what it takes so I then went off onto a

00:01:28.900 --> 00:01:30.900
popular auction site

00:01:31.060 --> 00:01:33.060
beginning with a letter E and

00:01:33.490 --> 00:01:38.400
Ordered myself a floating point ship for the Falcon and I then inserted it

00:01:38.680 --> 00:01:43.979
into the machine and I recompiled the program this time to use

00:01:44.710 --> 00:01:48.449
The floating point unit. So this version it's doing floating point maps

00:01:48.450 --> 00:01:49.920
It's using the fractions

00:01:49.920 --> 00:01:54.839
But it's all being done in software this machine code instructions for the six 8030 chip in there

00:01:55.149 --> 00:01:58.589
To calculate all those different floating point numbers

00:01:59.740 --> 00:02:05.460
so then we compiled the program this time to actually use the floating point unit and

00:02:06.969 --> 00:02:11.458
This version runs about 4.5 times faster it takes no point

00:02:11.459 --> 00:02:17.819
Not one seconds rather no point naught four five seconds to do exactly the same calculations. This is exactly the same source code

00:02:17.819 --> 00:02:21.958
I just recompile a using GCC to produce a version that used the floating-point

00:02:22.390 --> 00:02:28.079
Unit and you can actually see that the graphics are slightly smoother and the time is much less

00:02:28.079 --> 00:02:31.409
So the fact we can speed it up by doing it in hardware. Perhaps isn't that surprising?

00:02:31.719 --> 00:02:36.539
There's lots of tasks where you could either implement it in software or implement it in harder

00:02:36.540 --> 00:02:38.540
And if you implement it in hard way it's often

00:02:38.560 --> 00:02:44.879
Faster to do so so we are going to try that but it's actually worth thinking about what's involved in adding up

00:02:45.280 --> 00:02:47.399
Floating-point numbers Tom did a good video

00:02:47.680 --> 00:02:53.939
Right back at the beginning computer file looking at how floating-point numbers are represented as a sort of high-level and it will say well

00:02:56.139 --> 00:02:59.999
It - naught point nine nine nine nine nine nine nine then after a while

00:03:00.760 --> 00:03:03.269
you'll stop but actually when you get down to the sort of

00:03:03.639 --> 00:03:06.509
Level of the computer's having to deal with them and see how they're stored

00:03:06.909 --> 00:03:11.249
It's quite interesting to see how they're stored and how then manipulating them

00:03:11.250 --> 00:03:14.489
Are you writing software to do something simple like adding two numbers together?

00:03:15.189 --> 00:03:20.159
Actually ends up being quite an involved task compared to adding together two binary numbers

00:03:20.229 --> 00:03:22.949
So let's look at how we add numbers together

00:03:22.949 --> 00:03:28.079
So let's let's take a number and to save time I've printed out the bits. So let's take the number

00:03:28.079 --> 00:03:34.769
Let's say 42 because why everyone who uses that so we've got here the number 42 is one zero

00:03:35.319 --> 00:03:41.219
One zero one zero and then we need to fill the rest of these with zeros. Well ignore that for now

00:03:41.220 --> 00:03:45.720
So that's bit naught over on the right hand side through two bit

00:03:45.849 --> 00:03:50.188
One two three, and these of course are the same with the powers of 2

00:03:50.189 --> 00:03:54.569
So 2 2 zeros is ones two to the one is two four eight

00:03:54.970 --> 00:03:59.939
Just like we have powers of 10 when we do decimal numbers. So let's everyone to add together 42 and

00:04:01.419 --> 00:04:05.758
23 so I've got another binary number here 23 so the same bits and

00:04:06.189 --> 00:04:10.319
We'll just basically do addition. So 0 plus 1 Shaun is

00:04:10.930 --> 00:04:12.930
1 good

00:04:13.479 --> 00:04:15.989
yeah, ok, 1 plus 1 is

00:04:17.079 --> 00:04:22.228
0 and we have to carry the 1 0 plus 1 plus 1

00:04:23.139 --> 00:04:25.949
Okay. Yeah 1 plus 0 plus 1

00:04:26.400 --> 00:04:26.850
So yeah

00:04:26.850 --> 00:04:32.579
We've run it up 42 this numbers 23 42 plus 23 is 65 and sorry produced

00:04:32.979 --> 00:04:34.889
65 in binary is a result

00:04:34.889 --> 00:04:38.129
So rubbing up in binary is a relatively straightforward thing

00:04:38.130 --> 00:04:42.570
What we do is we take from the right each pair of bits add them together

00:04:42.570 --> 00:04:45.749
We produce a sum bit and occasion

00:04:45.750 --> 00:04:49.949
We also produce a carry bit and then we add the carry on in the next column just like we do

00:04:50.290 --> 00:04:52.290
when we do decimal arithmetic

00:04:52.720 --> 00:04:55.559
And you can generate systems that represent

00:04:57.400 --> 00:04:58.919
Decimals or by symbols

00:04:58.919 --> 00:05:00.919
I guess they'd be called or fractional numbers

00:05:01.000 --> 00:05:03.299
Using this so you can use a system which is quite common

00:05:03.520 --> 00:05:08.039
Was used in Adobe Acrobat as used on the iOS for 3d graphics at one point

00:05:08.560 --> 00:05:15.000
which is fixed point numbers where you say that say about 32 bits the top 16 bits are going to represent the sort of

00:05:15.130 --> 00:05:17.639
Integer part the bottom 16 bits are going to represent

00:05:18.130 --> 00:05:22.589
the fractional part and the basic way to think about that is you multiplied every number by 6 on

00:05:24.190 --> 00:05:29.910
65,536 shifts everything along and then when you want to produce the final result you divide it all by 6

00:05:31.570 --> 00:05:35.279
65536 now the problem with fixed point numbers is that they have a fixed scale

00:05:35.830 --> 00:05:39.150
Fixed is key in the name. So for example, if we use

00:05:40.030 --> 00:05:45.329
32-bit fixed point numbers splitting into 16 bits and 16 bits. That's great. We can go up to

00:05:45.849 --> 00:05:50.729
65,000 or so on the integer part, but if we need to get to 70,000 we can't story

00:05:51.340 --> 00:05:53.340
Likewise we can go to 1

00:05:54.400 --> 00:05:57.060
65536 the other thing we'd agree to go to 1

00:05:58.120 --> 00:05:59.650
130

00:05:59.650 --> 00:06:04.799
1072 sort of a thing we can't because we don't have that resolution on occasion

00:06:04.800 --> 00:06:08.820
We need the bits down here to represent very small quantities and occasion

00:06:08.820 --> 00:06:13.109
We want them to represent very large quantities for something like 3d graphics or graphics in general

00:06:13.539 --> 00:06:17.729
Fixed point numbers can work. Well for general-purpose things. They don't work that well

00:06:18.340 --> 00:06:23.789
So what people tend to do is they use floating-point numbers, which is the right things as tom said in

00:06:24.460 --> 00:06:26.460
scientific notation so rather than writing

00:06:27.130 --> 00:06:33.779
102 4 we write it as 1 point 0 to 4 times 10 to the 3 so we're using scientific notation

00:06:33.940 --> 00:06:36.809
We can do the same in binary rather than writing

00:06:38.020 --> 00:06:39.370
101

00:06:39.370 --> 00:06:41.709
One, oh, we can write one point

00:06:42.380 --> 00:06:44.380
zero one zero

00:06:44.419 --> 00:06:51.279
One times two this time rather than 10 to the 1 2 3 4 so we can write it 2 to the 4

00:06:51.490 --> 00:06:55.540
So what floating-point numbers do is that they say okay rather than representing

00:06:56.510 --> 00:07:03.909
Numbers using a fixed number as bits for each we're going to represent them in scientific notation effectively. We're the sort of

00:07:05.270 --> 00:07:11.350
Number that were then going to multiply by 2 to the something to shift it to the right point to make things absolutely clear

00:07:11.350 --> 00:07:13.070
I'm going to use

00:07:13.070 --> 00:07:18.159
Decimal numbers here to represent the 2 times 10 to the 4 so I will cheat and use this here

00:07:18.160 --> 00:07:19.850
but of course it would be

00:07:19.850 --> 00:07:25.749
102 the 1 0 0 so I guess the question that remains is how do we represent this in a computer?

00:07:25.750 --> 00:07:31.389
We've got to change this notation, which we can write nicely on a piece of paper to represent the binary number

00:07:31.910 --> 00:07:35.739
Multiplied by a power of 2, but how do we represent that in the computer?

00:07:35.750 --> 00:07:39.970
what we need to do is take this and find an encouraging which

00:07:40.280 --> 00:07:43.179
Represents it as a series of bits that the computer can then deal with

00:07:43.700 --> 00:07:49.869
So we're going to look at 32 bit of floating point numbers mainly because the number of digits I have to fill in

00:07:50.539 --> 00:07:51.620
become

00:07:51.620 --> 00:07:55.359
Relatively smaller to deal with then rather than doing 64 bit

00:07:55.360 --> 00:07:58.240
We could have done 16 bit sign things, but they use the same thing

00:07:58.240 --> 00:08:03.250
It's just the way they break it down change your slightly how many bits are assigned to each section

00:08:03.320 --> 00:08:06.399
So we've got our 32 bits and we need to represent

00:08:06.919 --> 00:08:10.899
this number in there we start off by splitting this up into

00:08:11.539 --> 00:08:18.939
A few different things. So the first bits or the most significant bit are in the number. The one on the left over here is

00:08:19.550 --> 00:08:22.990
The sign bit and that says whether it's a positive number

00:08:23.690 --> 00:08:26.829
We just let it be zero or negative number what which case it will be what?

00:08:26.900 --> 00:08:28.630
So unlike two's complement

00:08:28.630 --> 00:08:35.559
Which David's looked at in the past two's complement is equivalent to the ones complement with one?

00:08:35.870 --> 00:08:41.830
Added to it. The sign is represented purely as being a zero means positive one means negative

00:08:41.830 --> 00:08:43.240
We just have one bit represented with that

00:08:43.240 --> 00:08:48.130
They then say we're going to have eight bits, which we're going to use to represent the exponent this bit here

00:08:48.130 --> 00:08:51.999
I what power of 2 which gives us 255 or so

00:08:52.519 --> 00:08:58.959
Different powers of two we can use we'll come back to how that's represented in a second and then the rest of it

00:08:59.749 --> 00:09:06.609
Is used to represent the mantissa as its referred to so the remaining 23 bits are at the 32 are used to represent

00:09:06.799 --> 00:09:09.129
The remaining bit of the number, okay

00:09:09.139 --> 00:09:15.579
So they've got 23 bits to represent the number which is n gonna be multiplied by the 8 bit exponent

00:09:15.579 --> 00:09:21.188
They said every single possible floating-point number you're gonna write down is going to have a 1 as its most significant digit

00:09:22.519 --> 00:09:28.808
Except 1 0 say, ok. We'll treat 0 as a special case and to represent 0 they just set all bits to be zeros

00:09:29.089 --> 00:09:34.029
So we know that this is going to be 1 what we know is 1 so we don't need to encode it

00:09:34.029 --> 00:09:36.029
It's always going to be 1 so actually these

00:09:36.290 --> 00:09:40.029
23 bits here are the bits that come after the 1 so it's one dollar

00:09:41.869 --> 00:09:44.559
So on which are all the bits that come after here

00:09:44.559 --> 00:09:49.809
So we we sort of don't encode that bit because we know it's there one way to think about floating-point numbers

00:09:49.809 --> 00:09:52.928
Is there a sort of lossy compression mechanism for?

00:09:53.569 --> 00:09:54.589
real

00:09:54.589 --> 00:09:56.589
numbers floating-point real

00:09:56.839 --> 00:10:03.368
Fractional numbers because we're taking a number which is some representation and we're compressing it into these bits

00:10:03.529 --> 00:10:06.308
But we lose some information and we can see that in a second

00:10:06.309 --> 00:10:09.939
we run a little demo and we'll see that actually it can't represent all numbers and

00:10:10.189 --> 00:10:15.849
It's surprising sometimes which numbers it can't represent and which can each it can so we can then start writing

00:10:16.399 --> 00:10:21.248
Numbers in this form and to simplify things. I've printed out a form like this

00:10:21.529 --> 00:10:27.099
So if you want to write out the number one, it's one point naught naught naught naught

00:10:27.649 --> 00:10:31.328
Times 2 to the power of 0 so it's one point

00:10:31.939 --> 00:10:33.110
na-na-na-na-na-na-na-na naught

00:10:33.110 --> 00:10:34.369
times 2 to the power of

00:10:34.369 --> 00:10:38.918
0 which is 1 so it's 1 times 1 which is 1 and of course the sign bit because it's positive

00:10:39.499 --> 00:10:43.509
would be 0 to say that so we could write that out as

00:10:44.360 --> 00:10:48.189
The number so we can start assigning these things to the different bits

00:10:48.189 --> 00:10:53.199
We put a 0 there cuz it's positive and the mantissa is all 0 so we just fill them up with

00:10:53.480 --> 00:10:56.110
Zeros, and that leaves us with this 8 bit here

00:10:56.110 --> 00:11:01.869
We've got to represent 2 to the power of 0 now they could have decided to just put 0 in there

00:11:01.869 --> 00:11:05.359
But then the number 1 would have exactly the same a bit patent

00:11:05.360 --> 00:11:08.479
There's the number zero and introduced that's potentially not a good idea

00:11:08.820 --> 00:11:14.989
so what they actually say we're going to do is we're going to take the power which will go from mind 127 through to

00:11:15.930 --> 00:11:21.500
127 and then they add 127 on it. So our exponent here. Our power of 2 is 0

00:11:22.170 --> 00:11:23.340
so 0

00:11:23.340 --> 00:11:25.170
plus

00:11:25.170 --> 00:11:28.759
127 obviously is 127 so we encode

00:11:29.400 --> 00:11:34.129
127 into these remaining bits so 0 1 1 1 1

00:11:34.860 --> 00:11:36.860
1 1 1

00:11:37.230 --> 00:11:42.589
So to encode the number 1 like that we encode it into the binary representation

00:11:43.140 --> 00:11:45.499
0 for the sign bit 0 1

00:11:46.680 --> 00:11:54.350
127 for the exponent and then because we know that one's already encoded the rest of it becomes 0 this is a lossy system

00:11:54.350 --> 00:12:01.670
We can encode some numbers, but we're only encoding 24 significant bits where they are within the number of encoding changes

00:12:01.670 --> 00:12:03.860
But we're only encoding 24 significant bits

00:12:03.860 --> 00:12:05.430
So that's just right program

00:12:05.430 --> 00:12:11.989
That takes a number 1 6 7 7 7 - 1 5 an integer number and adds one to it

00:12:12.300 --> 00:12:13.880
And we'll do this in a loop

00:12:13.880 --> 00:12:16.580
We'll add one to the result and add one to the world and print out the values

00:12:16.580 --> 00:12:22.879
So we think that one six seven seven seven two one six one six seven seven seven

00:12:23.190 --> 00:12:30.050
Two one seven and we'll do this with both for an integer variable. So a 32-bit integer and also with a

00:12:30.600 --> 00:12:35.359
32-bit float, so got to money without program written here on the computer

00:12:35.790 --> 00:12:39.739
So we set up a float why we set up the variable. I to be

00:12:40.320 --> 00:12:42.320
16 million 770

00:12:43.140 --> 00:12:48.230
7215 checking things binary there and we set Y to equal I so they both start off with the same value

00:12:48.270 --> 00:12:54.350
We're then going to print them out. Where's the decimal and the floating point? Well, I'm also going to print out the hexadecimal

00:12:54.870 --> 00:12:57.020
Representations of the bits so we can see what's going on

00:12:57.600 --> 00:13:02.659
We're then going to add 1 to the value of y and add 1 to the value of ice

00:13:02.660 --> 00:13:04.660
So we're going to increment them both

00:13:04.740 --> 00:13:06.740
So let's run this program

00:13:09.810 --> 00:13:15.440
To not million mistakes that's always a good sign and let's run it so we get

00:13:17.800 --> 00:13:19.839
And 15 and we get

00:13:22.100 --> 00:13:25.360
16777215 point normal or not? What would expect?

00:13:28.370 --> 00:13:34.630
16,777,216 and the same there. So now we had one on it again and we get for the integer value

00:13:38.839 --> 00:13:45.819
16777215 point number that's not a right. Okay, so that's not right what's going on there? Well, if we think about how we represent this

00:13:46.640 --> 00:13:50.709
Let's think about the number one six seven seven seven two one six

00:13:51.980 --> 00:13:57.969
That number is one times two to the 24 and I sort of tricked by

00:13:58.430 --> 00:14:03.070
Generating it this way at the beginning. There's one with lots of zeros after it times two to the 24

00:14:03.310 --> 00:14:07.690
we have only 23 bits to represent this bit in here if

00:14:08.300 --> 00:14:10.300
We want to add on

00:14:10.610 --> 00:14:12.610
an extra bit

00:14:13.519 --> 00:14:17.649
We would need 24 bits here. We've only got 23 we can't do it

00:14:17.899 --> 00:14:23.949
So we can't represent it. If we added up to each time, it would work fine. So actually as we get some larger numbers

00:14:24.529 --> 00:14:26.769
We still have the same number of significant bits

00:14:27.470 --> 00:14:29.470
or significant digits

00:14:29.630 --> 00:14:35.079
But we can't store certain values. Well as we can with integers, so it's a lossy compression system

00:14:35.079 --> 00:14:38.679
basically, we can store a large range of values for anything from

00:14:39.649 --> 00:14:43.089
minus 2 to the power of 127 through 2 2 to the power of

00:14:43.339 --> 00:14:49.239
127 or we can go very very low and have numbers as small as 2 to the minus 127

00:14:50.990 --> 00:14:53.589
But we only have a certain they were pursuing

00:14:53.589 --> 00:14:57.249
So if we deal with very very large numbers that we've still only got 23 bits

00:14:57.250 --> 00:15:02.859
We have the precision and if we do them very small, but numbers we can got 23 bits worth of precision, which is fine

00:15:02.899 --> 00:15:05.619
We can cope with that because often when you're dealing with big numbers

00:15:05.990 --> 00:15:13.959
You're not worried about the small fiddly decimal places in your small four significant figures if you're measuring how far it is from the Earth

00:15:14.360 --> 00:15:16.360
to Alpha Centauri in

00:15:16.579 --> 00:15:18.999
millimeters plus or minus a few

00:15:20.000 --> 00:15:24.250
Millimeters a few points of a millimeter isn't going to make much difference that sort of thing

00:15:24.250 --> 00:15:26.799
So it's a compression where it's just a lossy system

00:15:28.290 --> 00:15:30.290
you

00:15:33.540 --> 00:15:38.630
All for this videos gonna mean writing zeroes 23 times, maybe I should have done 16-bit numbers

