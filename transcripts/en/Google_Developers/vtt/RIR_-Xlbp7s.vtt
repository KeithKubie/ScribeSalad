WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.444
[MUSIC PLAYING]

00:00:03.944 --> 00:00:04.821
[CLAPPING]

00:00:04.821 --> 00:00:05.570
EUGENE BREVDO: Hi.

00:00:05.570 --> 00:00:06.800
So I'm Eugene.

00:00:06.800 --> 00:00:08.870
I'm going to talk
about the sequence

00:00:08.870 --> 00:00:12.900
models and the recurring neural
network API, Intensive Flow.

00:00:12.900 --> 00:00:15.156
First, the motivating example.

00:00:15.156 --> 00:00:15.864
Google Translate.

00:00:15.864 --> 00:00:17.820
This is an awesome product.

00:00:17.820 --> 00:00:19.920
A lot of people really love it.

00:00:19.920 --> 00:00:22.020
And how does it work?

00:00:22.020 --> 00:00:26.300
You type in or you speak in a
sentence in a source language,

00:00:26.300 --> 00:00:28.170
some magic happens
in the background,

00:00:28.170 --> 00:00:31.860
and you get the translation
in the target language.

00:00:31.860 --> 00:00:34.590
And as of a couple of
months ago, the magic

00:00:34.590 --> 00:00:37.710
got a little bit more magical.

00:00:37.710 --> 00:00:40.920
And that's because
the product is now

00:00:40.920 --> 00:00:44.580
backed by a neural
network, and specifically

00:00:44.580 --> 00:00:47.040
a Sequence-to-Sequence Model.

00:00:47.040 --> 00:00:49.310
So what is the
Sequence-to-Sequence Model?

00:00:49.310 --> 00:00:53.220
Basically, it's two
recurring neural networks.

00:00:53.220 --> 00:00:56.220
One on the left is the encoder.

00:00:56.220 --> 00:01:02.790
And the encoder reads in one
word or word piece at a time,

00:01:02.790 --> 00:01:04.239
from left to right.

00:01:04.239 --> 00:01:08.850
Then it creates some
intermediate representation.

00:01:08.850 --> 00:01:12.120
Then the decoder, which is
a second Recurring Neural

00:01:12.120 --> 00:01:17.690
Network, or RNN, it receives
its start token that says,

00:01:17.690 --> 00:01:20.280
start decoding in
this target language.

00:01:20.280 --> 00:01:22.920
And it emits one
word or word piece

00:01:22.920 --> 00:01:26.340
at a time in the
target language.

00:01:26.340 --> 00:01:29.400
And once it admits
a token, it feeds

00:01:29.400 --> 00:01:31.380
that back into
the next timestep,

00:01:31.380 --> 00:01:33.600
and that token and
the previous state

00:01:33.600 --> 00:01:36.060
are used to figure
out what to emit next.

00:01:36.060 --> 00:01:41.080
Now in practice the model
is a little complicated.

00:01:41.080 --> 00:01:43.320
You've seen this slide
a couple of times today.

00:01:43.320 --> 00:01:46.140
And let me just
focus on the fact

00:01:46.140 --> 00:01:49.330
that again here there's an
encoder RNN on the left,

00:01:49.330 --> 00:01:51.780
there's some intermediate
representation in the middle,

00:01:51.780 --> 00:01:54.550
and on the right we
have a decoder RNN.

00:01:54.550 --> 00:01:58.614
And we're going to refer back
to the slide a couple of times.

00:01:58.614 --> 00:02:00.780
But let's talk about what
we're going to talk about.

00:02:00.780 --> 00:02:02.250
First of all, we'll
talk about how

00:02:02.250 --> 00:02:06.090
to read and batch sequenced data
efficiently in a distributed

00:02:06.090 --> 00:02:07.920
setting for training.

00:02:07.920 --> 00:02:11.038
We'll talk about
the RNN API itself,

00:02:11.038 --> 00:02:12.900
and as we talk about
that, we're going

00:02:12.900 --> 00:02:15.480
to start talking about
family dynamic calculation

00:02:15.480 --> 00:02:18.720
in TensorFlow and I'll
give you some tools that

00:02:18.720 --> 00:02:20.911
allow you to do that.

00:02:20.911 --> 00:02:22.410
Then we'll switch
tacks a little bit

00:02:22.410 --> 00:02:27.570
and talk about how to trade
off flexibility for efficiency

00:02:27.570 --> 00:02:29.670
in the RNN.

00:02:29.670 --> 00:02:31.680
And finally, we'll talk
about a neat new library

00:02:31.680 --> 00:02:34.380
that we're working
on that allows you

00:02:34.380 --> 00:02:38.140
to perform dynamic decoding.

00:02:38.140 --> 00:02:40.880
So reading and
batching sequence data.

00:02:40.880 --> 00:02:47.540
So as we scale up our training
for Lexi the translation model,

00:02:47.540 --> 00:02:51.160
it's very important to be able
to feed in large mini-batches

00:02:51.160 --> 00:02:53.230
of variable-length data.

00:02:53.230 --> 00:02:55.780
And so here we're
going to be focusing

00:02:55.780 --> 00:02:59.920
on the source sentence,
which is on the lower left,

00:02:59.920 --> 00:03:03.100
and on the target sentence,
which is on the lower right

00:03:03.100 --> 00:03:04.780
and on the upper right.

00:03:04.780 --> 00:03:07.390
And we're going to focus on
how to do this efficiently

00:03:07.390 --> 00:03:09.400
in a distributed setting.

00:03:09.400 --> 00:03:15.280
So for this we have a proto
called the sequence example.

00:03:15.280 --> 00:03:18.460
And Google protobufs
are, as you've heard,

00:03:18.460 --> 00:03:21.010
a language agnostic,
architecture agnostic,

00:03:21.010 --> 00:03:23.350
data storage format.

00:03:23.350 --> 00:03:25.810
And the sequence example
format is made specifically

00:03:25.810 --> 00:03:28.210
to store variable-length
sequences.

00:03:28.210 --> 00:03:32.260
So some of the pros, it
provides efficient storage

00:03:32.260 --> 00:03:35.170
of multiple sequences
per example.

00:03:35.170 --> 00:03:37.900
It supports a variable
number of features

00:03:37.900 --> 00:03:41.140
per time stamp, which
we won't talk about

00:03:41.140 --> 00:03:43.210
but is an important feature.

00:03:43.210 --> 00:03:46.420
And also we provide
an efficient parser

00:03:46.420 --> 00:03:50.750
that reads in a serialized
proto string and emits tensors

00:03:50.750 --> 00:03:53.890
and/or sparse tensors,
according to the configuration

00:03:53.890 --> 00:03:55.630
that you provide

00:03:55.630 --> 00:03:57.850
And coming soon, well,
you've heard Noah talk

00:03:57.850 --> 00:04:00.180
about TensorFlow Serving.

00:04:00.180 --> 00:04:02.410
The sequence example
proto is going

00:04:02.410 --> 00:04:06.220
to be a first-class citizen
in TensorFlow Serving.

00:04:06.220 --> 00:04:08.990
So you'll be able to
use it for creating

00:04:08.990 --> 00:04:13.330
data both at training
time and at serving time

00:04:13.330 --> 00:04:15.910
with TensorFlow Serving.

00:04:15.910 --> 00:04:19.600
So now that you can read
in sequence one at a time,

00:04:19.600 --> 00:04:22.300
you need to be able to batch it.

00:04:22.300 --> 00:04:24.650
And there are a couple
of ways to do this.

00:04:24.650 --> 00:04:29.940
So one way is you basically
do the padding yourself.

00:04:29.940 --> 00:04:34.330
So you pick a maximum
sequence length,

00:04:34.330 --> 00:04:37.330
and each of your inputs, you
pad to that maximum sequence

00:04:37.330 --> 00:04:38.020
length.

00:04:38.020 --> 00:04:41.530
And then you pass it into the
standard TensorFlow batching

00:04:41.530 --> 00:04:44.500
mechanism, the function
tf.training.batch.

00:04:44.500 --> 00:04:47.940
So the major negatives
of this approach, one,

00:04:47.940 --> 00:04:50.980
you need to know beforehand
the longest possible sequence

00:04:50.980 --> 00:04:51.700
length.

00:04:51.700 --> 00:04:54.310
And then you'll always pad
to that maximum length,

00:04:54.310 --> 00:04:58.690
and as a result, you're
wasting both space and time

00:04:58.690 --> 00:05:00.640
during training.

00:05:00.640 --> 00:05:02.080
So can we do better?

00:05:02.080 --> 00:05:06.070
So the next best thing is to
allow the batching mechanism

00:05:06.070 --> 00:05:07.750
to perform the padding for you.

00:05:07.750 --> 00:05:11.770
So you get your variable
length sequences one at a time,

00:05:11.770 --> 00:05:15.760
and you feed them into
the special padding queue.

00:05:15.760 --> 00:05:18.220
And the queue, when it
reads off the mini-batch,

00:05:18.220 --> 00:05:20.380
it finds the longest
sequence, and it

00:05:20.380 --> 00:05:24.160
pads all the other sequences
up to that maximum length.

00:05:24.160 --> 00:05:26.060
So this is a little bit better.

00:05:26.060 --> 00:05:28.630
You don't need to know
the maximum length.

00:05:28.630 --> 00:05:30.430
When you get a mini-batch,
you're no longer

00:05:30.430 --> 00:05:33.010
wasting space or time,
because you're only

00:05:33.010 --> 00:05:35.410
padding up to the maximum
length within that batch.

00:05:35.410 --> 00:05:38.470
However, as you're trying to
scale up training, for example,

00:05:38.470 --> 00:05:45.475
and your batch sizes become
larger and larger, 32, 64, 128,

00:05:45.475 --> 00:05:47.350
you're more and more
likely that at least one

00:05:47.350 --> 00:05:50.440
of the elements in every
mini-batch is going to be long.

00:05:50.440 --> 00:05:52.300
And so as a result,
you're back to where

00:05:52.300 --> 00:05:54.010
you started in the
previous slide,

00:05:54.010 --> 00:05:57.680
wasting space and
computation time.

00:05:57.680 --> 00:05:59.300
But we can do better.

00:05:59.300 --> 00:06:05.560
So the solution here
is to actually create

00:06:05.560 --> 00:06:09.490
a number of different queues,
which we called buckets,

00:06:09.490 --> 00:06:11.060
running at the same time.

00:06:11.060 --> 00:06:14.105
And when you pass
into this mechanism

00:06:14.105 --> 00:06:16.690
a variable-length
sequence, it will put it

00:06:16.690 --> 00:06:18.190
into one of a number of queues.

00:06:18.190 --> 00:06:21.610
And each queue will
contain only sequences

00:06:21.610 --> 00:06:23.920
that are about the same length.

00:06:23.920 --> 00:06:28.030
And when a mini-batch is
available from that queue,

00:06:28.030 --> 00:06:31.420
that mini-batch is returned
to you as the next mini-batch.

00:06:31.420 --> 00:06:33.460
And all of the sequences
within that mini-batch

00:06:33.460 --> 00:06:34.780
are about the same length.

00:06:34.780 --> 00:06:36.670
There's a little bit of padding.

00:06:36.670 --> 00:06:39.340
But this still gives you
the best of both worlds,

00:06:39.340 --> 00:06:41.569
because every
mini-batch everything

00:06:41.569 --> 00:06:42.610
is about the same length.

00:06:42.610 --> 00:06:44.080
You're not padding too much.

00:06:44.080 --> 00:06:47.560
You're not wasting too much
computation time or space

00:06:47.560 --> 00:06:50.110
during training.

00:06:50.110 --> 00:06:54.190
And most of the time, you get
your short or medium-length

00:06:54.190 --> 00:06:57.640
sequences as they
come in, almost as

00:06:57.640 --> 00:06:58.780
quickly as they come in.

00:06:58.780 --> 00:07:01.690
Every once in a while
you'll get a mini-batch

00:07:01.690 --> 00:07:04.990
from your spill-over from
the very long sequences.

00:07:04.990 --> 00:07:07.510
But again, even though it
takes longer to compute

00:07:07.510 --> 00:07:10.510
and uses more
memory, at least all

00:07:10.510 --> 00:07:13.240
of the sequences within this
mini-batch are all pretty long,

00:07:13.240 --> 00:07:15.550
so you're not wasting time.

00:07:15.550 --> 00:07:17.100
And we have a function for that.

00:07:17.100 --> 00:07:19.480
And it's called
Bucket-by-Sequence Length.

00:07:19.480 --> 00:07:21.970
Finally, for those
of you who are not

00:07:21.970 --> 00:07:25.270
necessarily interested in
Sequence-to-Sequence Models,

00:07:25.270 --> 00:07:28.390
but are interested in
building language models

00:07:28.390 --> 00:07:34.420
or variational auto
encoders, for example,

00:07:34.420 --> 00:07:36.729
we have a State
Saver, which allows

00:07:36.729 --> 00:07:38.520
you to implement
something called Truncated

00:07:38.520 --> 00:07:40.210
Backpropogation Through Time.

00:07:40.210 --> 00:07:43.670
So what this means is that you
pick a fixed number of times,

00:07:43.670 --> 00:07:46.830
steps that you unroll
for, and any sequence

00:07:46.830 --> 00:07:49.680
which is longer that than
that number of time steps,

00:07:49.680 --> 00:07:52.680
get split up into
multiple segments.

00:07:52.680 --> 00:07:58.290
When you finish your mini-batch,
any sequences or segments which

00:07:58.290 --> 00:08:02.400
are not the completed sequence,
the state is saved for you.

00:08:02.400 --> 00:08:05.110
Some bookkeeping is
done in the background,

00:08:05.110 --> 00:08:09.030
and at the next
training iteration,

00:08:09.030 --> 00:08:11.100
that state is loaded.

00:08:11.100 --> 00:08:18.709
And you continue processing that
sequence with that Saved State.

00:08:18.709 --> 00:08:20.250
And we have a function
for that, it's

00:08:20.250 --> 00:08:23.260
called
batch_sequences_with_states.

00:08:23.260 --> 00:08:27.210
So now that we've talked
a little bit about how

00:08:27.210 --> 00:08:30.570
to efficiently read in
variable-length data and batch

00:08:30.570 --> 00:08:34.320
it, let's talk about RNN's.

00:08:34.320 --> 00:08:38.429
So just to get everybody on
the same page, what is an RNN?

00:08:38.429 --> 00:08:41.840
An RNN is basically
a unit of computation

00:08:41.840 --> 00:08:43.690
that you repeat over and over.

00:08:43.690 --> 00:08:46.330
It's essentially a for loop.

00:08:46.330 --> 00:08:50.460
And the way that the
forward calculation flows,

00:08:50.460 --> 00:08:52.840
you get inputs from below.

00:08:52.840 --> 00:08:55.350
So X at every timestep.

00:08:55.350 --> 00:08:58.650
And you have some inputs
which are the previous state

00:08:58.650 --> 00:09:00.870
from the previous timestep.

00:09:00.870 --> 00:09:03.030
So these are the
arrows from the left.

00:09:03.030 --> 00:09:06.000
And this intermediate
calculation

00:09:06.000 --> 00:09:10.050
emits a new output, H,
and an updated state

00:09:10.050 --> 00:09:11.880
for the next timestep.

00:09:11.880 --> 00:09:16.775
So this diagram specifically,
is an implementation of LSTM,

00:09:16.775 --> 00:09:19.950
or Long Short Term Memory,
which is a very popular RNN

00:09:19.950 --> 00:09:22.320
architecture.

00:09:22.320 --> 00:09:25.710
So as I said, you can
think of it as a for loop.

00:09:25.710 --> 00:09:29.850
And in fact, you can
implement it, say in Python,

00:09:29.850 --> 00:09:31.020
as a for loop.

00:09:31.020 --> 00:09:34.440
So let's go through this
relatively straightforward

00:09:34.440 --> 00:09:35.370
approach.

00:09:35.370 --> 00:09:37.230
So you have a function RNN.

00:09:37.230 --> 00:09:41.350
It accepts a function cell
that reads in inputs in state

00:09:41.350 --> 00:09:43.950
and emits new outputs in state.

00:09:43.950 --> 00:09:47.580
You also provide it with
a list of input tensors,

00:09:47.580 --> 00:09:51.480
and you provide it with
some initial state.

00:09:51.480 --> 00:09:53.550
So you set your state
to the initial state.

00:09:53.550 --> 00:09:56.610
You loop over your
list of input tensors,

00:09:56.610 --> 00:09:59.250
so these are dense
floating point tensors.

00:09:59.250 --> 00:10:01.950
And at each iteration,
you call cell.

00:10:01.950 --> 00:10:06.090
You emit an output
and a new state.

00:10:06.090 --> 00:10:10.470
You append this output to
your list of output tensors.

00:10:10.470 --> 00:10:16.090
And at the very end you return
this list and the final state.

00:10:16.090 --> 00:10:19.470
So as I showed you on
the previous slide,

00:10:19.470 --> 00:10:22.890
you have for an LSTM, for
example, the state is actually

00:10:22.890 --> 00:10:25.540
two tensors, not just one.

00:10:25.540 --> 00:10:29.080
For a [? GRE, ?] which is
another popular architecture,

00:10:29.080 --> 00:10:31.220
there's only one
tensor for this state.

00:10:31.220 --> 00:10:37.510
And so one of the problems
is that the cell function

00:10:37.510 --> 00:10:39.730
and the initial
state are coupled,

00:10:39.730 --> 00:10:42.100
so you need to provide
an initial state that

00:10:42.100 --> 00:10:46.180
says one or a couple of tensors,
which usually have all zero

00:10:46.180 --> 00:10:48.070
values.

00:10:48.070 --> 00:10:52.180
And if you want to
change your architecture

00:10:52.180 --> 00:10:54.230
and try another one, so
you change your cell,

00:10:54.230 --> 00:10:56.950
you need to be able
to easily generate,

00:10:56.950 --> 00:11:01.960
for example, a new initial state
with a different structure.

00:11:01.960 --> 00:11:03.790
So that problem is
actually fairly easy

00:11:03.790 --> 00:11:06.925
to solve by changing the
functions to objects,

00:11:06.925 --> 00:11:08.710
and I'll talk about that next.

00:11:08.710 --> 00:11:10.930
The harder problem
is that here you

00:11:10.930 --> 00:11:14.290
have a list of input tensors.

00:11:14.290 --> 00:11:17.740
And this is a fixed size list,
you're in Python after all.

00:11:17.740 --> 00:11:19.630
And you're building
a graph that has

00:11:19.630 --> 00:11:21.910
a fixed number of timesteps.

00:11:21.910 --> 00:11:23.650
And so as a result,
you're basically

00:11:23.650 --> 00:11:28.760
losing the ability to run a
variable number of iterations.

00:11:28.760 --> 00:11:30.560
We'll talk about that next.

00:11:30.560 --> 00:11:32.510
So first, the RNN cell.

00:11:32.510 --> 00:11:35.810
So the RNN cell is a base
class that we provide,

00:11:35.810 --> 00:11:41.270
which a different architecture
subclass, and it provides

00:11:41.270 --> 00:11:43.070
helper properties that
give you knowledge

00:11:43.070 --> 00:11:45.530
about that architecture.

00:11:45.530 --> 00:11:50.090
Now, the second important
feature of the RNN cell,

00:11:50.090 --> 00:11:56.240
class library, is that you can
treat each timestep as building

00:11:56.240 --> 00:11:58.100
a timestep as building a layer.

00:11:58.100 --> 00:12:00.710
And so you've heard
Francois talk about Keras.

00:12:00.710 --> 00:12:03.890
And for the last month
or so in the background,

00:12:03.890 --> 00:12:10.244
we've been working to make RNN
cells act like Keras layers.

00:12:10.244 --> 00:12:12.410
And so eventually we hope
to have full compatibility

00:12:12.410 --> 00:12:14.462
between the two.

00:12:14.462 --> 00:12:17.280
But let's go through an example.

00:12:17.280 --> 00:12:21.580
Suppose you want to
implement an LSTM.

00:12:21.580 --> 00:12:23.690
First, you build and
a basic LSTM class.

00:12:23.690 --> 00:12:26.800
It's a subclass of RNN cell.

00:12:26.800 --> 00:12:28.370
It has two important properties.

00:12:28.370 --> 00:12:30.640
The first one is state size.

00:12:30.640 --> 00:12:32.890
And what this state
size tells you

00:12:32.890 --> 00:12:35.860
is that my state is
composed of two tensors.

00:12:35.860 --> 00:12:38.410
And each tensor has
num units columns,

00:12:38.410 --> 00:12:41.310
so these are the
depth of the LSTM.

00:12:41.310 --> 00:12:43.270
The second property,
output size,

00:12:43.270 --> 00:12:47.310
tells you I emit at every
timestep a single tensor

00:12:47.310 --> 00:12:49.950
and has num units columns.

00:12:49.950 --> 00:12:52.610
Finally, there is
a call method that

00:12:52.610 --> 00:12:56.370
accepts an impotensor and a
state tuple, in this case,

00:12:56.370 --> 00:12:59.960
and decomposes the
state into two tensors,

00:12:59.960 --> 00:13:03.710
performs intermediate
calculations for the timestep,

00:13:03.710 --> 00:13:09.030
and emits and new output, a
new H, and then you stay tuple.

00:13:09.030 --> 00:13:13.050
So this a fairly
flexible framework.

00:13:13.050 --> 00:13:17.460
And we have a fairly
sizable library

00:13:17.460 --> 00:13:20.500
of different implementations
of different architectures,

00:13:20.500 --> 00:13:25.260
including basic RNN
cells, LSTM GRU,

00:13:25.260 --> 00:13:29.360
a bunch of other interesting
research oriented

00:13:29.360 --> 00:13:30.810
architectures.

00:13:30.810 --> 00:13:32.680
One that's particularly
interesting

00:13:32.680 --> 00:13:35.360
is the bottom one, NASL.

00:13:35.360 --> 00:13:39.270
So if you heard Megan
talk this morning

00:13:39.270 --> 00:13:42.630
about neural
architecture search,

00:13:42.630 --> 00:13:46.020
one of the fruits of
this research program

00:13:46.020 --> 00:13:48.900
is this new architecture
that was basically

00:13:48.900 --> 00:13:51.960
learned by
reinforcement learning

00:13:51.960 --> 00:13:55.620
on different possible
RNN architectures.

00:13:55.620 --> 00:13:59.520
And so we have an implementation
of that, as of this week,

00:13:59.520 --> 00:14:00.600
on TensorFlow GitHub.

00:14:00.600 --> 00:14:02.830
It's called NASL.

00:14:02.830 --> 00:14:04.960
So check it out see
if it provides you

00:14:04.960 --> 00:14:09.670
with the performance
quality boost.

00:14:09.670 --> 00:14:12.970
And within Google,
we have dozens

00:14:12.970 --> 00:14:16.180
of slightly different
or radically different

00:14:16.180 --> 00:14:19.240
RNN architectures that are
used by different teams

00:14:19.240 --> 00:14:21.630
to solve their problem.

00:14:21.630 --> 00:14:25.770
So we've talked about
solving the simple problem.

00:14:25.770 --> 00:14:28.070
Let's talk about how to
perform dynamic calculation

00:14:28.070 --> 00:14:30.510
in TensorFlow.

00:14:30.510 --> 00:14:32.551
And there are
basically two tools

00:14:32.551 --> 00:14:34.050
that I like to talk
about, and these

00:14:34.050 --> 00:14:36.600
are the main primitives
that are used

00:14:36.600 --> 00:14:39.730
to build dynamic calculations.

00:14:39.730 --> 00:14:42.420
The first one is
the tf.while_loop,

00:14:42.420 --> 00:14:46.950
and it allows you to build
dynamic loops and supports

00:14:46.950 --> 00:14:47.850
backdrop.

00:14:47.850 --> 00:14:49.830
The second one is
called TensorArray.

00:14:49.830 --> 00:14:54.210
And this is required to be able
to efficiently read and write

00:14:54.210 --> 00:14:58.640
slices of tensors and
also supports backprop.

00:14:58.640 --> 00:15:01.010
So the while_loop was
developed by Yuan,

00:15:01.010 --> 00:15:04.010
who is a research scientist
on the brain team.

00:15:04.010 --> 00:15:05.600
And it's a function
that basically

00:15:05.600 --> 00:15:07.830
accepts three arguments--

00:15:07.830 --> 00:15:12.350
the cond, lambda, the body
lambda, and loop_vars.

00:15:12.350 --> 00:15:16.010
So the cond is the condition
function that tells you,

00:15:16.010 --> 00:15:17.840
should I continue
this loop or not?

00:15:17.840 --> 00:15:22.580
The body implements the actual
computation per timestep.

00:15:22.580 --> 00:15:25.700
And the loop_vars are
the initial conditions

00:15:25.700 --> 00:15:27.120
being fed to the while_loop.

00:15:27.120 --> 00:15:28.860
So let's go through an example.

00:15:28.860 --> 00:15:30.980
We'll start with
a tuple of tuples.

00:15:30.980 --> 00:15:32.750
ijk_0.

00:15:32.750 --> 00:15:36.050
The values it takes on
are zero, one and two.

00:15:36.050 --> 00:15:39.525
The condition lambda looks
at the first component i,

00:15:39.525 --> 00:15:43.130
and says, continue looping
while i is less than 10.

00:15:43.130 --> 00:15:47.900
The body takes both of
these parameters, variables

00:15:47.900 --> 00:15:51.830
i and j, k, increments
i by 1, and updates

00:15:51.830 --> 00:15:56.000
jk with the calculation
that you see there.

00:15:56.000 --> 00:15:59.480
You pass these into the
while_loop, and at the output,

00:15:59.480 --> 00:16:02.960
you receive a tuple
of tuples of tensors.

00:16:02.960 --> 00:16:06.590
So the final value of i,
which if you look at this,

00:16:06.590 --> 00:16:09.660
basically is going
to be equal to 10.

00:16:09.660 --> 00:16:12.770
And jk_final, which is a
tuple of tensors, which

00:16:12.770 --> 00:16:15.050
contains the result
of performing

00:16:15.050 --> 00:16:18.540
the calculation there 10 times.

00:16:18.540 --> 00:16:22.320
So while_loop gets you
almost all the way there,

00:16:22.320 --> 00:16:27.540
but if you need to be able
to process slices of tensors

00:16:27.540 --> 00:16:32.190
at every timestep, you need to
be able to efficiently access

00:16:32.190 --> 00:16:33.980
them and to write new ones.

00:16:33.980 --> 00:16:36.090
So this we have a tensorarray.

00:16:36.090 --> 00:16:39.360
So let's suppose that you have
a matrix and you don't know

00:16:39.360 --> 00:16:42.180
at [? graft ?] build time how
many rows that matrix has.

00:16:42.180 --> 00:16:45.120
But you can use the
TensorFlow primitive tf.shape

00:16:45.120 --> 00:16:47.860
to access that at runtime.

00:16:47.860 --> 00:16:51.400
So let's say this is equivalent
to an unknown sequence

00:16:51.400 --> 00:16:54.950
length in your mini-batch.

00:16:54.950 --> 00:16:59.380
So first you create
a tensorarray,

00:16:59.380 --> 00:17:03.970
whose number of entries at
size is equal to matrix_rows,

00:17:03.970 --> 00:17:07.480
and then you can unpack
or unstack the matrix

00:17:07.480 --> 00:17:09.040
into that tensorarray.

00:17:09.040 --> 00:17:11.079
Then inside the
while_loop body, you

00:17:11.079 --> 00:17:15.730
can access any index that you
want and you'll get back out

00:17:15.730 --> 00:17:17.410
of vector.

00:17:17.410 --> 00:17:19.930
You perform some
processing, and then you

00:17:19.930 --> 00:17:23.800
need to take the result and
store that in the new matrix.

00:17:23.800 --> 00:17:26.589
So the way you do that is
first you create a tensorarray.

00:17:26.589 --> 00:17:29.660
Again, in this case, you know
the size that it's going to be,

00:17:29.660 --> 00:17:32.885
but in fact, you don't even need
to know the size before hand.

00:17:32.885 --> 00:17:36.170
Then within the loop,
you write an update

00:17:36.170 --> 00:17:40.022
that tensorarray at
whatever row you want.

00:17:40.022 --> 00:17:41.980
And the results of your
calculation [INAUDIBLE]

00:17:41.980 --> 00:17:44.110
vector, you pass it in there.

00:17:44.110 --> 00:17:45.730
You take that tensor
you update it.

00:17:45.730 --> 00:17:47.146
At the very end
of the while_loop,

00:17:47.146 --> 00:17:49.780
you get the finally
updated tensorarray.

00:17:49.780 --> 00:17:53.620
And then you can stack it, pack
it, and you get a new matrix.

00:17:53.620 --> 00:17:57.700
And that's the result of
your of your calculation.

00:17:57.700 --> 00:18:00.837
Between these two primitives,
you can get pretty far.

00:18:00.837 --> 00:18:03.420
In fact, we have a whole library
of functional building blocks

00:18:03.420 --> 00:18:06.180
in TensorFlow that you
can use based on these.

00:18:06.180 --> 00:18:08.520
So we have a mao function.

00:18:08.520 --> 00:18:10.080
We have TensorFlow
scan, which is

00:18:10.080 --> 00:18:14.230
somewhat similar to theano scan,
in some ways more powerful.

00:18:14.230 --> 00:18:16.710
We have the leftfold
and rightfold operators.

00:18:16.710 --> 00:18:18.580
We have the dynamic
RNN function,

00:18:18.580 --> 00:18:20.650
which I'll talk about next.

00:18:20.650 --> 00:18:24.360
We have a couple of
other variants of RNN.

00:18:24.360 --> 00:18:26.770
And also we have a
dynamic decoder function,

00:18:26.770 --> 00:18:30.020
which I'll also talk about
towards the end of this talk.

00:18:30.020 --> 00:18:32.480
Finally, there are a bunch of
other interesting algorithms

00:18:32.480 --> 00:18:36.760
that you can implement
using these primitives.

00:18:36.760 --> 00:18:41.390
So let's talk about the encoder
of the translation model.

00:18:41.390 --> 00:18:44.740
So a couple of
interesting things

00:18:44.740 --> 00:18:46.750
that we can see in this encoder.

00:18:46.750 --> 00:18:51.460
First of all, it has
eight stacked LSTM layers.

00:18:51.460 --> 00:18:55.200
And each one sits on a
different GPU device,

00:18:55.200 --> 00:18:58.110
and between each of the layers
there are residual connections.

00:18:58.110 --> 00:18:59.610
So residual connections
are actually

00:18:59.610 --> 00:19:05.160
very important in being able to
train a performant model that

00:19:05.160 --> 00:19:09.480
has such a large
stack of layers.

00:19:09.480 --> 00:19:11.910
So we'd like to
implement this by using

00:19:11.910 --> 00:19:13.530
the API I presented to you.

00:19:13.530 --> 00:19:18.050
Turns out, it's two lines
of code, fairly gnarly code,

00:19:18.050 --> 00:19:21.130
but let me [INAUDIBLE]
you through them.

00:19:21.130 --> 00:19:24.260
So first of all, you
build your LSTM cells,

00:19:24.260 --> 00:19:27.200
each one has, say, depth of 512.

00:19:27.200 --> 00:19:30.590
You then would like to
add residual connections,

00:19:30.590 --> 00:19:32.080
so you wrap each
of these objects

00:19:32.080 --> 00:19:34.280
in the residual wrapper.

00:19:34.280 --> 00:19:38.700
You'd like to put each of these
layers on a different GPU,

00:19:38.700 --> 00:19:41.510
so you wrap that in
a device wrapper,

00:19:41.510 --> 00:19:45.030
and each one goes on
GPU0 GPU1, and so on.

00:19:45.030 --> 00:19:47.330
So you make a list
of these, and then

00:19:47.330 --> 00:19:49.960
you wrap that in
a multi RNN cell.

00:19:49.960 --> 00:19:52.160
So you take this
object and you pass it

00:19:52.160 --> 00:19:54.971
to the dynamic RNN function.

00:19:54.971 --> 00:19:56.510
It takes the cell.

00:19:56.510 --> 00:19:59.990
It takes a tensor of
mini-batches of inputs.

00:19:59.990 --> 00:20:03.050
It takes a vector with
the sequence lengths

00:20:03.050 --> 00:20:04.880
of each of the
mini-batch entries,

00:20:04.880 --> 00:20:06.380
and a couple of
other arguments that

00:20:06.380 --> 00:20:13.370
control the trade between memory
consumption and performance.

00:20:13.370 --> 00:20:16.280
So what I presented
to you so far

00:20:16.280 --> 00:20:19.820
is a fairly flexible
API for customizing

00:20:19.820 --> 00:20:22.650
implementing your
own RNN architectures

00:20:22.650 --> 00:20:24.590
and dynamic decoding.

00:20:24.590 --> 00:20:30.610
And sometimes you know that
you want to use an LSTM,

00:20:30.610 --> 00:20:33.610
or you want to use a GRU, you
want to use an established RNN

00:20:33.610 --> 00:20:34.600
architecture.

00:20:34.600 --> 00:20:38.170
So in these cases,
you have the option

00:20:38.170 --> 00:20:42.310
to trade off flexibility
for performance.

00:20:42.310 --> 00:20:46.340
And in TensorFlow, basically, we
provide three ways to do that.

00:20:46.340 --> 00:20:49.810
The first one Chris and Todd
talked about earlier today

00:20:49.810 --> 00:20:51.250
is XLA.

00:20:51.250 --> 00:20:52.930
I'll talk about that one first.

00:20:52.930 --> 00:20:59.740
There are also handwritten C++
kernels that provide manually

00:20:59.740 --> 00:21:03.880
fused implementations of,
say, LSTM, per timestep.

00:21:03.880 --> 00:21:07.630
And they're also
manually designed,

00:21:07.630 --> 00:21:10.210
fully fused RNN
implementation that fuse

00:21:10.210 --> 00:21:13.660
the entire calculation
across time.

00:21:13.660 --> 00:21:16.530
So as you look at these, think
about the kinds of trade-offs

00:21:16.530 --> 00:21:18.220
that you're making.

00:21:18.220 --> 00:21:20.940
So you're trading off
flexibility for speed.

00:21:20.940 --> 00:21:23.100
Some of these implementations
don't work well

00:21:23.100 --> 00:21:27.780
on some architectures but
are faster are on others.

00:21:27.780 --> 00:21:29.580
And I'll mention these.

00:21:29.580 --> 00:21:31.950
So let's first talk about XLA.

00:21:31.950 --> 00:21:34.710
The nice thing about XLA is
that there's very minimal

00:21:34.710 --> 00:21:36.180
flexibility trade-off.

00:21:36.180 --> 00:21:40.110
You implement an RNN cell
and you get an architecture

00:21:40.110 --> 00:21:41.250
that you care about.

00:21:41.250 --> 00:21:44.910
And then it's basically one
step to get it compiled.

00:21:44.910 --> 00:21:50.280
So if before you had a stack
of eight LSTM stem cells

00:21:50.280 --> 00:21:55.020
and you want to fuse each of
them individually per layer,

00:21:55.020 --> 00:21:57.900
you simply wrap that
LSTM cell in something

00:21:57.900 --> 00:22:01.050
called a compiled wrapper,
which we recently released.

00:22:01.050 --> 00:22:03.630
And this basically performs
that builds [INAUDIBLE]

00:22:03.630 --> 00:22:06.750
compilation for you per layer.

00:22:06.750 --> 00:22:09.060
Alternatively, if you want
to be a bit more aggressive

00:22:09.060 --> 00:22:12.510
and try to fuse
across all layers,

00:22:12.510 --> 00:22:14.760
you basically move the
compilation outside

00:22:14.760 --> 00:22:18.360
of the multi-RNN cell.

00:22:18.360 --> 00:22:22.230
So pretty easy to
use and flexible.

00:22:22.230 --> 00:22:24.480
When is this a good idea?

00:22:24.480 --> 00:22:26.780
So when should you use XLA?

00:22:26.780 --> 00:22:30.530
You saw some slides about
that in previous talks.

00:22:30.530 --> 00:22:32.820
I also ran a couple
of benchmarks

00:22:32.820 --> 00:22:39.790
myself training a multi-RNN
cell with three separately fused

00:22:39.790 --> 00:22:43.680
LSTM layers running
for 50 timesteps.

00:22:43.680 --> 00:22:46.020
And I tried this on slightly
different architectures

00:22:46.020 --> 00:22:49.330
of different architecture sizes.

00:22:49.330 --> 00:22:51.010
So what are the takeaways?

00:22:51.010 --> 00:22:54.090
First of all, if you're
doing this on a GPU--

00:22:54.090 --> 00:22:58.080
say I had a Tesla
K40 available--

00:22:58.080 --> 00:23:01.230
and you're using a small
batch size and a small depth

00:23:01.230 --> 00:23:05.130
for yourselves, today it's kind
of a toss-up of whether it's

00:23:05.130 --> 00:23:06.570
actually faster.

00:23:06.570 --> 00:23:11.340
The XLA team is actually
investigating this area,

00:23:11.340 --> 00:23:13.590
and hopefully the
performance will

00:23:13.590 --> 00:23:15.870
be sped up for this regime.

00:23:15.870 --> 00:23:17.520
In more realistic
training cases where

00:23:17.520 --> 00:23:21.360
you have large
batch size, 16, 32,

00:23:21.360 --> 00:23:25.110
and so on, even with
a small cell depth

00:23:25.110 --> 00:23:28.560
or with larger cell depths,
as you increase the numbers,

00:23:28.560 --> 00:23:31.200
your performance on a
GPU is going to improve.

00:23:31.200 --> 00:23:35.130
15%, 30%, 45%.

00:23:35.130 --> 00:23:37.320
And again, this is an area
under active development,

00:23:37.320 --> 00:23:40.080
so it's going to
continue to improve.

00:23:40.080 --> 00:23:44.700
Now with CPU, there's
currently not much benefit.

00:23:44.700 --> 00:23:48.080
So don't use XLA there.

00:23:48.080 --> 00:23:52.340
Now if you're
interested in ahead

00:23:52.340 --> 00:23:54.680
of time compilation for
an embedded device--

00:23:54.680 --> 00:23:59.350
say, an Android
device or iOS device--

00:23:59.350 --> 00:24:03.320
today it's not
faster to use XLA,

00:24:03.320 --> 00:24:06.260
but there are other benefits.

00:24:06.260 --> 00:24:09.340
There is a reduction in
memory usage by the model.

00:24:09.340 --> 00:24:11.720
A reduction in the binary size.

00:24:11.720 --> 00:24:16.040
And again, while I didn't
benchmark these aspects of it,

00:24:16.040 --> 00:24:18.500
this is something worth
doing as you're looking

00:24:18.500 --> 00:24:23.000
into building embedded models.

00:24:23.000 --> 00:24:27.010
So my current
recommendation, try XLA.

00:24:27.010 --> 00:24:29.310
Benchmark your
intended use case.

00:24:29.310 --> 00:24:31.470
The benchmarks are
actually open source

00:24:31.470 --> 00:24:34.850
and they're in GitHub repo,
so you can look at them

00:24:34.850 --> 00:24:38.756
and use them to compare the
runtimes and the memory usage

00:24:38.756 --> 00:24:39.256
yourself.

00:24:42.370 --> 00:24:46.850
Moving on from XLA, we have
some handwritten C++ kernels

00:24:46.850 --> 00:24:50.330
for the LSTM and GRU
cells per timestep.

00:24:50.330 --> 00:24:53.090
And these work on
CPU's and on GPU's.

00:24:53.090 --> 00:24:56.600
They tend to be faster
for smaller batch sizes

00:24:56.600 --> 00:25:00.380
and on mobile devices.

00:25:00.380 --> 00:25:03.380
So try them out.

00:25:03.380 --> 00:25:07.740
If you're interested in
fully fused calculations,

00:25:07.740 --> 00:25:10.640
especially on mobile
devices, the LSTM block

00:25:10.640 --> 00:25:15.660
fused cell is even
faster for these cases.

00:25:15.660 --> 00:25:18.380
So look at that.

00:25:18.380 --> 00:25:21.440
Now, if you're working,
you have nvidia GPUs,

00:25:21.440 --> 00:25:28.250
you're using CUDNN,
we have CUDNN wrappers

00:25:28.250 --> 00:25:30.200
that allow you to speed
up your training up

00:25:30.200 --> 00:25:33.440
to three times, which
is awesome if you're

00:25:33.440 --> 00:25:35.120
working on these devices.

00:25:35.120 --> 00:25:36.890
And it's fairly flexible.

00:25:36.890 --> 00:25:41.835
Supports bi-directional
RNNs, stacked RNNs for LSTM,

00:25:41.835 --> 00:25:44.890
GRU, and a couple of
other architectures.

00:25:44.890 --> 00:25:48.920
And as of about a month ago,
you can take the parameters

00:25:48.920 --> 00:25:54.590
that these layers use,
and during export time,

00:25:54.590 --> 00:25:57.890
convert them from the
custom CUDNN format

00:25:57.890 --> 00:26:01.670
to canonical format, so that you
can then take these checkpoints

00:26:01.670 --> 00:26:03.770
and load them in a
different graph that

00:26:03.770 --> 00:26:07.510
uses one of the other
LSTM implementations.

00:26:07.510 --> 00:26:10.700
What that means is that
you can train on GPU

00:26:10.700 --> 00:26:12.500
and then perform
inference on a CPU

00:26:12.500 --> 00:26:17.790
or on a mobile device,
which is pretty cool.

00:26:17.790 --> 00:26:20.120
So we've talked about that.

00:26:20.120 --> 00:26:23.610
Let's look at this neat new
API that we're working on

00:26:23.610 --> 00:26:25.350
for dynamic decoding.

00:26:25.350 --> 00:26:27.390
And in fact,
decoding tends to be

00:26:27.390 --> 00:26:32.010
the most complicated part
of many Sequence-to-Sequence

00:26:32.010 --> 00:26:33.330
Models.

00:26:33.330 --> 00:26:37.350
And that's partly because you
have to attend to the encoder

00:26:37.350 --> 00:26:38.180
output.

00:26:38.180 --> 00:26:43.200
And partly because you're
emitting an example

00:26:43.200 --> 00:26:46.110
at every timestep, and then
you're feeding that back in.

00:26:46.110 --> 00:26:48.463
And decoder has to
decide when to stop.

00:26:51.620 --> 00:26:53.470
So we have this library.

00:26:53.470 --> 00:26:55.770
It's a new object-oriented API.

00:26:55.770 --> 00:26:58.950
It's under active development.

00:26:58.950 --> 00:27:04.020
We hope to release a new neuro
machine translation tutorial,

00:27:04.020 --> 00:27:06.310
which uses this library.

00:27:06.310 --> 00:27:10.390
And the code is already in
TensorFlow GitHub master branch

00:27:10.390 --> 00:27:14.940
in the t.f.contrib.seq2seq
package.

00:27:14.940 --> 00:27:18.570
The main goal of the decoder
is to make a plug-and-play

00:27:18.570 --> 00:27:22.200
and batch-friendly decoding
library that allows you

00:27:22.200 --> 00:27:28.950
to first pick a sampling method,
pick the RNN architecture,

00:27:28.950 --> 00:27:35.280
then pick, if you want to have
a fancy decoding approach-- say,

00:27:35.280 --> 00:27:38.490
using attention or an
in-graph beam search--

00:27:38.490 --> 00:27:40.510
and then decode.

00:27:40.510 --> 00:27:43.630
So let's start by first
picking a sampling method.

00:27:43.630 --> 00:27:47.350
So during decoding,
when you're training,

00:27:47.350 --> 00:27:50.320
oftentimes you want to just use
the Ground Truth as your inputs

00:27:50.320 --> 00:27:51.620
at every timestep.

00:27:51.620 --> 00:27:53.920
So in this case,
we have a helper

00:27:53.920 --> 00:27:55.390
called the training helper.

00:27:55.390 --> 00:27:58.640
And you feed it the [? tensor ?]
decoder inputs and the sequence

00:27:58.640 --> 00:28:01.180
lengths, and that's
all you need to do.

00:28:01.180 --> 00:28:05.260
However, if you want to try
something a bit more fancy--

00:28:05.260 --> 00:28:07.370
let's say you want to
use scheduled sampling,

00:28:07.370 --> 00:28:11.230
another popular approach
for training decoders.

00:28:11.230 --> 00:28:13.660
So use another helper.

00:28:13.660 --> 00:28:16.820
In addition to the Ground
Truth decoder inputs,

00:28:16.820 --> 00:28:21.640
you can also say, well, the
RNN is going to emit an output.

00:28:21.640 --> 00:28:23.830
I'll treat that as logits.

00:28:23.830 --> 00:28:29.560
And I'm going to sample from
these and get a sample ID.

00:28:29.560 --> 00:28:33.880
And I'll look up in
an embedding table

00:28:33.880 --> 00:28:36.460
and feed that in
as the next input.

00:28:36.460 --> 00:28:39.220
So here you have to provide
the embeddings, as well

00:28:39.220 --> 00:28:40.930
as the probability
with which you're

00:28:40.930 --> 00:28:44.260
going to choose whether you use
Ground Truth or you're going

00:28:44.260 --> 00:28:46.090
to sample.

00:28:46.090 --> 00:28:49.660
Finally, at inference time,
you don't have Ground Truth,

00:28:49.660 --> 00:28:51.730
but you do have your
embeddings and what

00:28:51.730 --> 00:28:54.330
you can do is you can read
the outputs of the RNN,

00:28:54.330 --> 00:28:56.170
feed them as logits.

00:28:56.170 --> 00:28:59.950
And take the arg max,
the highest probability

00:28:59.950 --> 00:29:03.110
output in your vocabulary, and
look that up in the embeddings,

00:29:03.110 --> 00:29:05.690
and feed that as
your next input.

00:29:05.690 --> 00:29:09.100
So you pick one of these.

00:29:09.100 --> 00:29:12.000
Then you pick your
RNN architecture.

00:29:12.000 --> 00:29:13.860
And in this case, it's
the same architecture

00:29:13.860 --> 00:29:16.770
as we had for the encoder.

00:29:16.770 --> 00:29:18.240
And then you build your decoder.

00:29:18.240 --> 00:29:22.350
So this is an object that
combines the cell architecture

00:29:22.350 --> 00:29:24.870
and the helper object,
the sampling approach,

00:29:24.870 --> 00:29:29.920
and possibly also
provides an initial state.

00:29:29.920 --> 00:29:32.550
Now, the simplest one
doesn't do anything extra.

00:29:32.550 --> 00:29:34.540
However, this week
and next week,

00:29:34.540 --> 00:29:36.780
we're actually
working on releasing

00:29:36.780 --> 00:29:39.660
an attentional decoder, where
you provide an attention

00:29:39.660 --> 00:29:43.330
mechanism that
describes how you,

00:29:43.330 --> 00:29:46.830
at every timestep
same translation,

00:29:46.830 --> 00:29:51.000
you look at the output
of the encoder and say,

00:29:51.000 --> 00:29:54.930
oh, maybe I should be attending
to this part of the input

00:29:54.930 --> 00:29:58.320
sentence, and that
will help me to decide

00:29:58.320 --> 00:30:01.930
what word to emit next.

00:30:01.930 --> 00:30:05.200
So finally, you
have your decoder.

00:30:05.200 --> 00:30:07.030
Just decode, call
dynamic decode on it.

00:30:07.030 --> 00:30:11.770
This will return to you the
output, the dynamic decoder

00:30:11.770 --> 00:30:15.830
output, the tensor, and
the final decoder state.

00:30:15.830 --> 00:30:21.060
So the outputs, for a simple
decoder, there are two.

00:30:21.060 --> 00:30:24.710
One is the tensor of the RNN
outputs at every time stamp.

00:30:24.710 --> 00:30:28.040
And the second one, is a
tensor with sample IDs.

00:30:28.040 --> 00:30:30.800
So if you chose a
sampling decoder,

00:30:30.800 --> 00:30:33.510
this will emit the
actual categories,

00:30:33.510 --> 00:30:36.680
the actual words that were
chosen at every timestep.

00:30:36.680 --> 00:30:39.800
And in this case, for the final
decoder state, because you

00:30:39.800 --> 00:30:42.680
have a stack of
eight LSTMs, it's

00:30:42.680 --> 00:30:45.770
going to be a list of
eight tuples containing

00:30:45.770 --> 00:30:48.500
the LSTM and states.

00:30:48.500 --> 00:30:52.640
So to combine everything,
essentially four lines of code,

00:30:52.640 --> 00:30:55.490
first you choose
your sampling method.

00:30:55.490 --> 00:30:57.800
Then you choose your
RNN architecture,

00:30:57.800 --> 00:31:00.710
you build your decoder,
and then you decode.

00:31:00.710 --> 00:31:02.600
So we hope that
this is something

00:31:02.600 --> 00:31:05.690
that will take a fairly
complicated procedure

00:31:05.690 --> 00:31:09.900
and make it as simple
and flexible as possible.

00:31:09.900 --> 00:31:14.490
So to summarize, I've talked
about a flexible sequence input

00:31:14.490 --> 00:31:17.730
pipeline that allows you
to scale up distributed

00:31:17.730 --> 00:31:19.770
training of sequence models.

00:31:19.770 --> 00:31:22.685
I've talked about the
RNN API in TensorFlow.

00:31:22.685 --> 00:31:24.900
It has hundreds of
uses within Google

00:31:24.900 --> 00:31:28.237
and many, many users
outside of Google as well.

00:31:28.237 --> 00:31:29.820
I've talked about
dynamic calculations

00:31:29.820 --> 00:31:32.760
within TensorFlow
and dynamic RNNs.

00:31:32.760 --> 00:31:36.000
These have a reduced
memory footprint compared

00:31:36.000 --> 00:31:39.630
to the basic for loop approach.

00:31:39.630 --> 00:31:41.820
They allow cast
your GPU activations

00:31:41.820 --> 00:31:45.990
to the CPU reducing
the memory usage.

00:31:45.990 --> 00:31:51.050
And they're essentially as fast
as the equivalent for loop.

00:31:51.050 --> 00:31:53.430
I've also talked
about how to improve

00:31:53.430 --> 00:31:55.680
the performance of your
RNNs by a fusion and some

00:31:55.680 --> 00:31:57.580
of the trade-offs
that you make there.

00:31:57.580 --> 00:32:01.320
And finally, I've introduced
a new decoder library

00:32:01.320 --> 00:32:05.760
that we hope you'll
all check out and try.

00:32:05.760 --> 00:32:07.620
It's available on GitHub now.

00:32:07.620 --> 00:32:09.720
So thank you very much.

00:32:09.720 --> 00:32:12.770
[MUSIC PLAYING]

