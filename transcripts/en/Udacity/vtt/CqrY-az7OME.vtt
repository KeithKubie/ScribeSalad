WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:03.550
Okay so I think that was almost it,
was there anything else?

00:00:03.550 --> 00:00:07.640
&gt;&gt; Yes, we then made a leap
to Monte Carlo tree search.

00:00:07.640 --> 00:00:08.970
&gt;&gt; I wouldn't say it was a leap.

00:00:08.970 --> 00:00:11.710
&gt;&gt; We climbed a Monte Carlo tree.

00:00:11.710 --> 00:00:13.300
&gt;&gt; Yes, and did some search, and

00:00:13.300 --> 00:00:15.430
what did we learn about
Monte Carlo tree search?

00:00:15.430 --> 00:00:19.320
&gt;&gt; So it's really important to help
with scaling because it let's you

00:00:19.320 --> 00:00:22.860
have computation that's independent
of the size of the state space

00:00:22.860 --> 00:00:27.720
Even though it depends very possibly
badly on the length of the horizon.

00:00:27.720 --> 00:00:33.650
&gt;&gt; Right so it's got a low o on
the state space practice independent.

00:00:33.650 --> 00:00:40.480
But it has a big o on the horizon
which I'll just call h here.

00:00:40.480 --> 00:00:42.150
&gt;&gt; And it's exponential with m.

00:00:42.150 --> 00:00:43.609
&gt;&gt; Yeah it's exponential with m.

00:00:43.609 --> 00:00:46.040
&gt;&gt; [LAUGH]
&gt;&gt; It looks like hydrogen now.

00:00:46.040 --> 00:00:51.260
So does MCTS and
abstraction fit together in any way?

00:00:51.260 --> 00:00:54.740
&gt;&gt; We could make it if
we were abstract enough.

00:00:56.620 --> 00:00:59.040
If you pop back up far enough

00:00:59.040 --> 00:01:03.650
you can sort of think of it as a kind of
abstraction over actions and policies.

00:01:03.650 --> 00:01:07.780
&gt;&gt; But no I was thinking in particular
that the idea that in MCTS the way

00:01:07.780 --> 00:01:12.410
we talked about it, it was talking about
specific states ground level states.

00:01:12.410 --> 00:01:15.320
Like if we have a good way of
abstracting states can we use those

00:01:15.320 --> 00:01:17.410
abstract states in an MTCS context.

00:01:17.410 --> 00:01:18.086
&gt;&gt; Sure.
In fact,

00:01:18.086 --> 00:01:20.420
what I was about to say is that all
of the stuff that we learned about

00:01:20.420 --> 00:01:24.080
generalization up here can apply
to MCTS more easily than others.

00:01:24.080 --> 00:01:27.750
Doing the function approximation
is sometimes quite hard, but

00:01:27.750 --> 00:01:32.990
not really because of the way that we're
sort of thinking about the q functions.

00:01:32.990 --> 00:01:35.680
But the temple abstraction works
very well because I can use

00:01:35.680 --> 00:01:38.818
options instead of
actions to do my MCTS.

00:01:38.818 --> 00:01:40.698
&gt;&gt; Right, you said that.

00:01:40.698 --> 00:01:42.150
&gt;&gt; Yeah I said that.

00:01:42.150 --> 00:01:44.890
And as a result
the state abstraction and

00:01:44.890 --> 00:01:47.740
the the goal abstraction can also
follow through there if you think about

00:01:47.740 --> 00:01:52.660
each of the individual modules as
doing their own version of MCTS.

00:01:52.660 --> 00:01:54.270
But MCTS is a very general notion.

00:01:54.270 --> 00:01:58.440
So if I were trying to decide between
different modules that I might take,

00:01:58.440 --> 00:02:03.705
it will look exactly like trying to
do the sequential decision making and

00:02:03.705 --> 00:02:05.610
MCTS will work either way.

00:02:05.610 --> 00:02:09.729
But the big temporal abstraction is
a huge win for MCTS because again all of

00:02:09.729 --> 00:02:14.300
its difficulty lies in the link
to the horizon, right.

00:02:14.300 --> 00:02:18.270
So if I'm able to search
far ahead in the future,

00:02:18.270 --> 00:02:20.342
that lowers my effective horizon.

00:02:20.342 --> 00:02:20.908
&gt;&gt; Nice.

00:02:20.908 --> 00:02:22.040
&gt;&gt; All right.

00:02:22.040 --> 00:02:23.650
&gt;&gt; So if these ideas are so
important for

00:02:23.650 --> 00:02:27.200
scaling up do you do you feel like
they've contributed to, I don't know,

00:02:27.200 --> 00:02:30.220
the ability of reinforcement,
learning to solve some real problems.

00:02:30.220 --> 00:02:31.180
&gt;&gt; Yeah.
Absolutely.

00:02:31.180 --> 00:02:32.700
No, these things actually
come up quite a bit.

00:02:32.700 --> 00:02:35.050
They've been used in a lot of games.

00:02:35.050 --> 00:02:38.850
MCTS has been used, well it's all in the
papers that the students have written.

00:02:38.850 --> 00:02:42.870
But MCTS has been used to solve
a bunch of game problems in just my

00:02:42.870 --> 00:02:45.090
own work as well as in
a variety of other things.

00:02:45.090 --> 00:02:48.040
You probably know a few more
off the top of your head.

00:02:48.040 --> 00:02:52.960
But really any time you have a huge
state space, or potentially a huge

00:02:52.960 --> 00:02:57.540
state space, these kinds of interactions
turn out to be extremely useful.

00:02:57.540 --> 00:02:59.940
Some of them are easier to
think about than others.

00:02:59.940 --> 00:03:02.310
Function approximation is very nice, but

00:03:02.310 --> 00:03:05.410
does have its own problem with
picking the right features.

00:03:05.410 --> 00:03:07.270
Particularly, if you are going
to be non-linear about it.

00:03:07.270 --> 00:03:09.900
Temple abstraction is
often quite useful because

00:03:09.900 --> 00:03:14.270
people can help you come up with
things like options or constraints.

00:03:14.270 --> 00:03:16.990
Goal abstraction has kind
of a similar flavor to it.

00:03:16.990 --> 00:03:19.710
And because of those things,
the state abstraction helps.

00:03:19.710 --> 00:03:22.710
Basically, anything that let's you
lower the number of states or

00:03:22.710 --> 00:03:25.762
the effective horizon has to make the
reinforcement learning problem easier.

00:03:25.762 --> 00:03:26.316
&gt;&gt; Nice.

00:03:26.316 --> 00:03:26.994
Alright.

00:03:26.994 --> 00:03:28.300
So there, scalability.

00:03:28.300 --> 00:03:30.700
These things actually work
they actually are helpful and

00:03:30.700 --> 00:03:34.250
this is a huge wide open field
in reinforcement learning.

00:03:34.250 --> 00:03:36.950
In fact, to your point I'll just sort
of ended with this that I think that

00:03:38.080 --> 00:03:41.960
reinforcement learning has gotten so
good at solving.

00:03:41.960 --> 00:03:45.640
Kind of well thought out
grid like problems that.

00:03:45.640 --> 00:03:48.690
The field is really interested in, and
increasingly more interested in figuring

00:03:48.690 --> 00:03:50.290
out how to make it
work in the real world.

00:03:50.290 --> 00:03:52.560
And these kind of techniques,
these abstraction techniques,

00:03:52.560 --> 00:03:54.010
these scaling techniques,

00:03:54.010 --> 00:03:56.910
become increasingly important as you
try to move it out into the real world.

00:03:56.910 --> 00:03:58.090
So this is a good thing.

00:03:58.090 --> 00:03:59.580
&gt;&gt; Wow, I'm glad you told me about it.

00:03:59.580 --> 00:04:00.870
&gt;&gt; I'm about helping others.

00:04:00.870 --> 00:04:01.840
So I think we're good.

00:04:01.840 --> 00:04:03.920
So I guess, goodbye, Michael.

00:04:03.920 --> 00:04:04.480
Until next time.

00:04:04.480 --> 00:04:05.470
&gt;&gt; All right.

00:04:05.470 --> 00:04:06.050
Yeah, see you then.

