WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:03.550
All right Michael, so here's boosting in pseudo code. Okay,

00:00:03.550 --> 00:00:05.180
let me read it out to you then you can tell

00:00:05.180 --> 00:00:08.480
me if it makes sense. So we're given a training set.

00:00:08.480 --> 00:00:11.720
It's made up of a bunch of xi, yr pairs. You

00:00:11.720 --> 00:00:15.180
know, x is your input and y is your output.

00:00:15.180 --> 00:00:18.350
And for reasons that'll be clear in a moment All of

00:00:18.350 --> 00:00:22.340
your labels are either minus one or plus one. Where minus

00:00:22.340 --> 00:00:25.240
one means not in class or plus one means you're in

00:00:25.240 --> 00:00:28.260
a class. So this is a binary classification task. That make sense?

00:00:28.260 --> 00:00:29.360
&gt;&gt; So far.

00:00:29.360 --> 00:00:32.140
&gt;&gt; Okay. And then what you're going to do is,

00:00:32.140 --> 00:00:34.545
you're going to loop at every time step, let's call it

00:00:34.545 --> 00:00:39.180
lower-case t. From the first time step one, to some big

00:00:39.180 --> 00:00:41.290
time in the future. We'll just call it capital T and

00:00:41.290 --> 00:00:42.980
not worry about where it comes from right now. The

00:00:42.980 --> 00:00:46.240
first thing you're going to do is you're going to construct a distribution.

00:00:46.240 --> 00:00:47.840
And I'll tell you how in a minute, Michael. Okay, so,

00:00:47.840 --> 00:00:50.260
so, don't worry about it. And we'll just call that D

00:00:50.260 --> 00:00:52.470
sub T. So, this is your distribution over your

00:00:52.470 --> 00:00:57.220
examples at some particular time T. Given that distribution, I

00:00:57.220 --> 00:00:59.350
want you to find a weak classifier. I want your

00:00:59.350 --> 00:01:03.430
weak learner to output some hypothesis. Let's call that epsilon

00:01:03.430 --> 00:01:05.450
sub T. The hypothesis that gets produced to that

00:01:05.450 --> 00:01:09.560
time step. And that hypothesis should have some small error.

00:01:09.560 --> 00:01:11.750
Let's call that error Epsilon sub T, because it's a

00:01:11.750 --> 00:01:15.310
small number. Such that it does well on the training

00:01:15.310 --> 00:01:19.000
set, given the particular distribution. So, I'm just rewriting

00:01:19.000 --> 00:01:22.060
my notion of error from, the other side of the

00:01:22.060 --> 00:01:24.750
screen. So there are times we want you to find

00:01:24.750 --> 00:01:27.190
a weak classifier. That is, we want you to call

00:01:27.190 --> 00:01:30.310
some weak learner that returns some hypothesis. let's call

00:01:30.310 --> 00:01:33.140
it h sub t that has a small error. let's

00:01:33.140 --> 00:01:36.870
call that epsilons of t. Which is to say that

00:01:36.870 --> 00:01:40.390
the probability of it being wrong that is disagreeing with

00:01:40.390 --> 00:01:44.670
the training label is small, with respect to the underlying distribution.

00:01:44.670 --> 00:01:48.230
&gt;&gt; So just to be clear there, the epsilon could

00:01:48.230 --> 00:01:51.560
be as big as slightly less than a half. Right? It

00:01:51.560 --> 00:01:54.200
doesn't have to be teeny, tiny. It could actually be,

00:01:54.200 --> 00:01:56.100
almost a half. But it can't be bigger than a half.

00:01:56.100 --> 00:01:58.870
&gt;&gt; That's right. And, and no matter what happens.

00:01:58.870 --> 00:02:01.480
Or even equal to a half. but, you know,

00:02:01.480 --> 00:02:03.390
we can assume, although it doesn't matter for the

00:02:03.390 --> 00:02:05.540
algorithm that the learner is going to return the best one

00:02:05.540 --> 00:02:08.120
that it can. With some error. But regardless, it's

00:02:08.120 --> 00:02:11.050
going to have, it's going to satisfy the requirements

00:02:11.050 --> 00:02:14.040
of a weak learner, and all I've done is

00:02:14.040 --> 00:02:16.310
copy this notion of error over to here. Ok?

00:02:17.370 --> 00:02:18.500
&gt;&gt; Awesome!

00:02:18.500 --> 00:02:21.320
&gt;&gt; Ok. So, you're going to do that and you'll do that a whole bunch of times

00:02:21.320 --> 00:02:25.315
steps, constantly finding hypothesis at each time step

00:02:25.315 --> 00:02:31.060
h sub t with small error epsilon sub t constantly making new distributions,

00:02:31.060 --> 00:02:33.020
and then eventually, you're going to output some

00:02:33.020 --> 00:02:35.930
final hypothesis. Which, I haven't told you yet how

00:02:35.930 --> 00:02:37.750
you're going to to get the final hypothesis. But that's

00:02:37.750 --> 00:02:39.770
the high level bit. Look at your training data,

00:02:39.770 --> 00:02:44.110
construct distributions, find a week classifier with low error.

00:02:44.110 --> 00:02:45.930
Keep doing that you have a bunch of them

00:02:45.930 --> 00:02:48.490
and then combine them somehow into some final hypothesis.

00:02:48.490 --> 00:02:51.250
And that's high level of algorithm for boosting, okay?

00:02:51.250 --> 00:02:53.380
&gt;&gt; Okay, but you've left out the two, two really

00:02:53.380 --> 00:02:55.880
important things, even the part from how you find we,

00:02:55.880 --> 00:02:58.430
weak classifier, which is where do we get this

00:02:58.430 --> 00:03:01.380
distribution and where do we get this, this final hypothesis?

00:03:01.380 --> 00:03:03.640
&gt;&gt; Right, so let me do that for you right now.

