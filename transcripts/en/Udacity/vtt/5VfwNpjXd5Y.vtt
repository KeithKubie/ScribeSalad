WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.570
Now that I've given some
kind of motivation for

00:00:02.570 --> 00:00:06.610
this notion of generalization, it might
be worth talking about what exactly

00:00:06.610 --> 00:00:10.670
we are trying to generalize between
different learning opportunities.

00:00:10.670 --> 00:00:13.790
So what are the functions that we're
actually making use of when we're doing

00:00:13.790 --> 00:00:14.740
reinforcement learning?

00:00:14.740 --> 00:00:15.540
&gt;&gt; What do you mean?

00:00:15.540 --> 00:00:18.430
&gt;&gt; When we're learning we're actually
generally learning some mapping right.

00:00:18.430 --> 00:00:20.280
So this is true in reinforcement
learning in general.

00:00:20.280 --> 00:00:22.800
There's some mapping
from input to output.

00:00:22.800 --> 00:00:23.480
&gt;&gt; I see what you mean.

00:00:23.480 --> 00:00:25.270
So, well, the whole goal,

00:00:25.270 --> 00:00:29.140
of course, is to learn a policy, which
is a mapping from states to action.

00:00:29.140 --> 00:00:30.320
&gt;&gt; Great.
Okay, that seems like a really good

00:00:30.320 --> 00:00:31.380
place to start.

00:00:31.380 --> 00:00:34.370
So policy maps states to actions.

00:00:34.370 --> 00:00:35.850
&gt;&gt; But that's usually hard.

00:00:35.850 --> 00:00:38.570
So we usually learn some
kind of intermediate thing,

00:00:38.570 --> 00:00:40.650
like a value function.

00:00:40.650 --> 00:00:42.950
&gt;&gt; Well, let's just stick with
policy here for a second.

00:00:42.950 --> 00:00:44.130
So in terms of the policy.

00:00:44.130 --> 00:00:47.720
Generalization would mean that to
the extent that similar states

00:00:47.720 --> 00:00:49.430
have similar actions.

00:00:49.430 --> 00:00:53.090
Then we could learn what the right
action is for some states and then

00:00:53.090 --> 00:00:56.580
guess what the right action is for other
states, using some kind of I don't know.

00:00:56.580 --> 00:00:58.730
Function approximation,
some kind of supervised learning.

00:00:58.730 --> 00:01:01.860
&gt;&gt; Right, so if you're in a state
where an anvil is about to hit you on

00:01:01.860 --> 00:01:04.500
the head,
you should probably jump out of the way.

00:01:04.500 --> 00:01:05.459
That's probably true for

00:01:05.459 --> 00:01:07.860
every state where an anvil's
about to hit you in the head.

00:01:07.860 --> 00:01:10.460
Even if you never seen the specific
details of that state before.

00:01:10.460 --> 00:01:11.850
&gt;&gt; Yeah,
I mean it could matter whether or

00:01:11.850 --> 00:01:13.760
not there also is a shark
tank right in front of you.

00:01:13.760 --> 00:01:15.630
So when you jump out of
the way of the anvil,

00:01:15.630 --> 00:01:17.060
you actually jump into the shark tank.

00:01:17.060 --> 00:01:18.340
But yeah, I think that's exactly right.

00:01:18.340 --> 00:01:21.110
That we want to be able to
say across lots and lots and

00:01:21.110 --> 00:01:24.690
lots of related states, we could
be using the same kind of action.

00:01:24.690 --> 00:01:26.820
But I think what you're pointing
out is that we don't know how to

00:01:26.820 --> 00:01:28.740
learn the policy directly.

00:01:28.740 --> 00:01:29.440
Anyway, when

00:01:29.440 --> 00:01:32.100
we're talking about these kinds of
reinforcement learning algorithms.

00:01:32.100 --> 00:01:35.440
So, we usually learn a kind
of a stand in for it.

00:01:35.440 --> 00:01:38.100
&gt;&gt; And
my favorite one is the value function,

00:01:38.100 --> 00:01:42.030
which is where you are now mapping
states to some number to r.

00:01:42.030 --> 00:01:43.750
&gt;&gt; Right.
So we can map, for example, states and

00:01:43.750 --> 00:01:45.700
actions to the estimated return.

00:01:45.700 --> 00:01:48.880
That's what the q function does and
that's another function that we could be

00:01:48.880 --> 00:01:52.230
trying to learn and this is another
function where generalization might be

00:01:52.230 --> 00:01:53.000
an option, right.

00:01:53.000 --> 00:01:57.780
So to the extent that similar state
action pairs have similar returns,

00:01:57.780 --> 00:02:00.410
again, because if an anvil's
about to hit you on the head.

00:02:00.410 --> 00:02:05.150
Probably that's a bad state,
you're in a bad situation across

00:02:05.150 --> 00:02:07.623
all the different kinds of states where
handles about to hit you in the head.

00:02:07.623 --> 00:02:08.470
&gt;&gt; There we go that you.

00:02:08.470 --> 00:02:12.570
You should draw a picture of me
standing off to the side to your left.

00:02:12.570 --> 00:02:14.480
I have the world's tallest neck.

00:02:14.480 --> 00:02:15.120
&gt;&gt; Yeah.
So

00:02:15.120 --> 00:02:17.230
say I'm in a state where enamels
about to hit me on the head,

00:02:17.230 --> 00:02:19.670
there's lots of other details that
are sort of not that important

00:02:19.670 --> 00:02:22.050
in predicting what the estimated
returns going to be.

00:02:22.050 --> 00:02:24.150
So you even look kind of concerned.

00:02:24.150 --> 00:02:25.220
&gt;&gt; That's supposed to be me?

00:02:25.220 --> 00:02:26.800
&gt;&gt; Yeah you're tall enough.

00:02:26.800 --> 00:02:27.480
&gt;&gt; Okay, fair enough.

00:02:27.480 --> 00:02:30.100
&gt;&gt; So this is another place where we
could be doing function approximation,

00:02:30.100 --> 00:02:32.780
is in the actual q function itself.

00:02:32.780 --> 00:02:36.020
And there's a third one that I think
is actually really important and

00:02:36.020 --> 00:02:38.730
interesting, and that is the model.

00:02:38.730 --> 00:02:43.010
To the extent that you can actually do
a good job of predicting next states,

00:02:43.010 --> 00:02:46.400
transitions for related states.

00:02:46.400 --> 00:02:48.400
Then you could actually be
doing function approximation or

00:02:48.400 --> 00:02:50.145
generalization in the model as well.

00:02:50.145 --> 00:02:50.696
&gt;&gt; Right.

00:02:50.696 --> 00:02:54.190
In fact, that's probably very,
very important often.

00:02:54.190 --> 00:02:55.530
&gt;&gt; Yes.
Not only is it important but

00:02:55.530 --> 00:02:59.890
it's actually often really natural place
to do it because when you're learning

00:02:59.890 --> 00:03:02.870
a model right so
what is it about learning a model?

00:03:02.870 --> 00:03:04.808
You're in some state,
you take some action, and

00:03:04.808 --> 00:03:06.570
you observe what net stage you're in.

00:03:06.570 --> 00:03:09.880
These are actually supervised
examples of transitions.

00:03:09.880 --> 00:03:11.880
So unlike learning the policy or

00:03:11.880 --> 00:03:14.890
learning the value function,
when you're trying to learn the model,

00:03:14.890 --> 00:03:18.000
you're actually getting
standard supervised examples.

00:03:18.000 --> 00:03:22.860
So that being said using function
approximation models it has been

00:03:22.860 --> 00:03:25.840
done in the field but
it's not that well understood.

00:03:25.840 --> 00:03:30.430
In particular, you need models that can
actually predict many many steps ahead

00:03:30.430 --> 00:03:33.890
to be able to be used effectively for
planning and decision making.

00:03:33.890 --> 00:03:38.470
Mostly what researchers have focused on
is function approximation in the value

00:03:38.470 --> 00:03:40.910
function, the generalization
of the value function.

00:03:40.910 --> 00:03:43.830
So that's what we're going to look
at in the rest of this lesson.

00:03:43.830 --> 00:03:44.730
&gt;&gt; Okay cool, look forward to it.

