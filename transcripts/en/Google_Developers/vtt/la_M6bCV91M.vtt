WEBVTT
Kind: captions
Language: en

00:00:03.532 --> 00:00:04.240
DEREK MURRAY: OK.

00:00:04.240 --> 00:00:05.330
Hello, everyone.

00:00:05.330 --> 00:00:06.700
I hope you all had a nice lunch.

00:00:06.700 --> 00:00:09.160
So as Zach said,
I'm Derek Murray.

00:00:09.160 --> 00:00:12.670
I've been working on TensorFlow
for a little over two years

00:00:12.670 --> 00:00:13.234
now.

00:00:13.234 --> 00:00:15.400
And in that time, one of
the most interesting things

00:00:15.400 --> 00:00:18.116
I've worked on has been
distributed TensorFlow.

00:00:18.116 --> 00:00:20.240
So that's what I'm going
to be talking about today.

00:00:20.240 --> 00:00:21.250
Before I get
started, I just want

00:00:21.250 --> 00:00:22.450
to get a sense of the room.

00:00:22.450 --> 00:00:25.235
So how many people here
have ever written or run

00:00:25.235 --> 00:00:28.980
a distributed
TensorFlow program?

00:00:28.980 --> 00:00:29.590
All right.

00:00:29.590 --> 00:00:32.307
I'm going to say that's,
like, 5% of you, maybe.

00:00:32.307 --> 00:00:34.140
I'm sorry to the folks
on the livestream who

00:00:34.140 --> 00:00:36.644
can't put your hands up and
give me this immediate feedback.

00:00:36.644 --> 00:00:38.310
But if you want to
follow me on Twitter,

00:00:38.310 --> 00:00:40.980
that would send a
very strong signal.

00:00:40.980 --> 00:00:41.770
OK.

00:00:41.770 --> 00:00:43.350
So why-- thank you.

00:00:43.350 --> 00:00:45.810
So why do I care so much
about distributed TensorFlow?

00:00:45.810 --> 00:00:48.480
Well, it's what enables
us here at Google

00:00:48.480 --> 00:00:50.700
to scale up the training
and inference of machine

00:00:50.700 --> 00:00:53.250
learning models on these
futuristic-looking machines

00:00:53.250 --> 00:00:55.189
in our data centers.

00:00:55.189 --> 00:00:57.480
And since we made it open
source just under a year ago,

00:00:57.480 --> 00:01:00.720
you've been able to run it on
your clusters at home as well.

00:01:00.720 --> 00:01:03.300
And why might you want to run
TensorFlow on lots of computers

00:01:03.300 --> 00:01:04.450
at the same time?

00:01:04.450 --> 00:01:06.804
Zach kind of gave you a
clue in the introduction.

00:01:06.804 --> 00:01:08.220
The most common
reason is that you

00:01:08.220 --> 00:01:10.580
want to increase the
throughput of training

00:01:10.580 --> 00:01:15.240
your models to train them faster
by harnessing a load of GPUs

00:01:15.240 --> 00:01:17.080
and train that
model in parallel.

00:01:17.080 --> 00:01:19.290
So this graph is taken
from our OSDI paper

00:01:19.290 --> 00:01:20.530
that we wrote last year.

00:01:20.530 --> 00:01:22.196
And it shows how the
training throughput

00:01:22.196 --> 00:01:26.920
of the Inception_V3 model
scales up using up to 200 GPUs.

00:01:26.920 --> 00:01:29.925
And actually this is, I
think, TensorFlow .8, .9,

00:01:29.925 --> 00:01:31.300
was the version
we used for this.

00:01:31.300 --> 00:01:32.883
So the numbers have
only gotten better

00:01:32.883 --> 00:01:35.670
with all the improvements
you heard about this morning.

00:01:35.670 --> 00:01:38.460
So the Inception model is
nice because it fits neatly

00:01:38.460 --> 00:01:39.840
in a single GPU.

00:01:39.840 --> 00:01:41.340
We also have more
complicated things

00:01:41.340 --> 00:01:43.340
like the neural machine
translation model, which

00:01:43.340 --> 00:01:45.150
Megan mentioned in the keynote.

00:01:45.150 --> 00:01:46.860
And this model is
so big, it benefits

00:01:46.860 --> 00:01:51.212
from being split across up to
eight GPUs in the same server.

00:01:51.212 --> 00:01:52.920
And then we use multiple
servers to train

00:01:52.920 --> 00:01:54.104
that model in parallel.

00:01:54.104 --> 00:01:55.770
If you're interested
in finding out more

00:01:55.770 --> 00:01:56.910
about models like
this, Eugene is

00:01:56.910 --> 00:01:58.650
going to be talking
about the LSTMs that

00:01:58.650 --> 00:02:03.211
make up each layer of the
translation model later on.

00:02:03.211 --> 00:02:05.460
Another reason you might
want to use lots of computers

00:02:05.460 --> 00:02:07.590
is that your model is
so big that it doesn't

00:02:07.590 --> 00:02:09.270
fit in a single machine.

00:02:09.270 --> 00:02:11.910
So this is true for a lot
of the wide and deep models

00:02:11.910 --> 00:02:14.640
that [INAUDIBLE] is going to
be talking about later today.

00:02:14.640 --> 00:02:17.060
And these use large
embedding matrices,

00:02:17.060 --> 00:02:20.290
that can be gigabytes or
tens of gigabytes in size,

00:02:20.290 --> 00:02:24.510
to map sparse data like, in
this example, pairs of apps

00:02:24.510 --> 00:02:27.049
that users have used
together to map those down

00:02:27.049 --> 00:02:28.590
to [INAUDIBLE]
representation that we

00:02:28.590 --> 00:02:32.020
can train with a traditional
deep neural network.

00:02:32.020 --> 00:02:34.370
And then, why settle
for very large models

00:02:34.370 --> 00:02:37.520
when you can train outrageously
large models as well?

00:02:37.520 --> 00:02:40.500
So our colleague Noam Shazeer
has a bit of a reputation for,

00:02:40.500 --> 00:02:43.130
like, pushing the envelope
on making large models

00:02:43.130 --> 00:02:44.510
practical to train.

00:02:44.510 --> 00:02:46.430
And he uses tricks like
distributed candidate

00:02:46.430 --> 00:02:49.820
sampling to scale up to very
large output class sets.

00:02:49.820 --> 00:02:51.680
Recently he's been
working with a team--

00:02:51.680 --> 00:02:54.740
I think they just got a
paper into ICLR 2017--

00:02:54.740 --> 00:02:58.310
to train neural networks with
tens of billions of parameters,

00:02:58.310 --> 00:03:00.260
using techniques like
conditional computation

00:03:00.260 --> 00:03:03.050
and mixtures of experts to
get even better performance

00:03:03.050 --> 00:03:07.350
on some language modeling
and translation tasks.

00:03:07.350 --> 00:03:09.570
So what all these
examples have in common

00:03:09.570 --> 00:03:11.790
is that they take advantage
of the flexibility

00:03:11.790 --> 00:03:15.760
distributed TensorFlow gives you
to train bigger models faster

00:03:15.760 --> 00:03:17.692
and to better accuracy.

00:03:17.692 --> 00:03:20.047
And this flexibility,
you know, it's

00:03:20.047 --> 00:03:21.630
great if you're an
expert user, but it

00:03:21.630 --> 00:03:24.840
can make distributed TensorFlow
look a little bit daunting.

00:03:24.840 --> 00:03:27.600
It actually is a very
simple, very minimalist core,

00:03:27.600 --> 00:03:29.940
but it wears a lot of the
complexity of distributed

00:03:29.940 --> 00:03:31.860
computing on the outside.

00:03:31.860 --> 00:03:35.100
So while we do have some nice
high-level APIs, sometimes

00:03:35.100 --> 00:03:36.600
for the really
cutting-edge stuff

00:03:36.600 --> 00:03:38.850
you have to kind of get in
there and move some tensors

00:03:38.850 --> 00:03:41.560
around using the
low-level TensorFlow API.

00:03:41.560 --> 00:03:45.021
And so my aim over the
next 20, 25 minutes or so

00:03:45.021 --> 00:03:47.270
is to give you a bottom-up
introduction to distributed

00:03:47.270 --> 00:03:48.150
TensorFlow.

00:03:48.150 --> 00:03:50.691
I'm going to show you how the
core concepts you might be used

00:03:50.691 --> 00:03:53.610
to in single-process TensorFlow
translate to the distributed

00:03:53.610 --> 00:03:54.420
world.

00:03:54.420 --> 00:03:56.970
I'll give you some ideas for
how to deal with the complexity

00:03:56.970 --> 00:03:57.505
that ensues.

00:03:59.915 --> 00:04:01.790
So I just claimed that
distributed TensorFlow

00:04:01.790 --> 00:04:03.840
has a minimalist core.

00:04:03.840 --> 00:04:05.560
What did I mean by that?

00:04:05.560 --> 00:04:07.890
Well, let's say I've
got just one computer,

00:04:07.890 --> 00:04:11.227
and it's got a GCPU device
and a GPU device in it.

00:04:11.227 --> 00:04:13.560
And if I want to write a
TensorFlow program to use these

00:04:13.560 --> 00:04:17.610
devices, I can put these little
with_tf.device annotations

00:04:17.610 --> 00:04:20.100
in my code, so that
for this example,

00:04:20.100 --> 00:04:23.660
the variables go on the CPU,
and the math goes on the GPU,

00:04:23.660 --> 00:04:26.230
where it's going to run faster.

00:04:26.230 --> 00:04:27.860
Then, when I come
to run this program,

00:04:27.860 --> 00:04:29.943
TensorFlow splits up the
graph between the devices

00:04:29.943 --> 00:04:33.640
and it puts in the necessary
DNA between the devices

00:04:33.640 --> 00:04:36.080
to run it for me.

00:04:36.080 --> 00:04:39.710
Now what happens when my
clicker runs out of battery?

00:04:39.710 --> 00:04:41.570
OK, we're moving over here.

00:04:41.570 --> 00:04:43.790
I think.

00:04:43.790 --> 00:04:48.706
So when we have multiple--
no, is it working again?

00:04:48.706 --> 00:04:50.900
Ah, OK.

00:04:50.900 --> 00:04:52.060
Just a little bit of lag.

00:04:52.060 --> 00:04:53.060
Sorry about that, folks.

00:04:53.060 --> 00:04:53.650
All right.

00:04:53.650 --> 00:04:56.170
So what happens if you
have multiple machines?

00:04:56.170 --> 00:04:58.720
Let's say, for reasons that
will become apparent later,

00:04:58.720 --> 00:05:00.910
that we want to
take those variables

00:05:00.910 --> 00:05:04.630
and put them on the CPU
device of a different process.

00:05:04.630 --> 00:05:07.870
Well, TensorFlow treats the
remote devices exactly the same

00:05:07.870 --> 00:05:08.770
as the local ones.

00:05:08.770 --> 00:05:11.380
All I have to do is add just
a little bit of information

00:05:11.380 --> 00:05:13.540
to these device
names, and the runtime

00:05:13.540 --> 00:05:15.880
will put the variables
in a different process,

00:05:15.880 --> 00:05:17.680
splitting up the graph
between the devices

00:05:17.680 --> 00:05:19.120
in the different
processes and adding

00:05:19.120 --> 00:05:20.286
the necessary communication.

00:05:20.286 --> 00:05:22.630
In this case, it
will be using GRPC

00:05:22.630 --> 00:05:27.250
to transfer tensors between
the processes instead of DNA

00:05:27.250 --> 00:05:29.680
from the GPU device.

00:05:29.680 --> 00:05:30.640
So there you have it.

00:05:30.640 --> 00:05:32.710
Using distributed
TensorFlow is just

00:05:32.710 --> 00:05:35.080
a simple matter of getting
all of your device placements

00:05:35.080 --> 00:05:36.850
exactly right.

00:05:36.850 --> 00:05:38.822
And yeah, I heard a wry chuckle.

00:05:38.822 --> 00:05:41.030
Yeah, I'm sure you know
exactly how easy that can be,

00:05:41.030 --> 00:05:43.250
if you've ever written
a TensorFlow program.

00:05:43.250 --> 00:05:43.900
All right.

00:05:43.900 --> 00:05:46.820
I should have put a sarcasm
tag in there or something.

00:05:46.820 --> 00:05:47.320
But OK.

00:05:47.320 --> 00:05:48.819
Maybe we should
back up a little bit

00:05:48.819 --> 00:05:51.310
and see how we can make
this problem a bit simpler.

00:05:51.310 --> 00:05:53.830
Well, one thing we can do
is we can take inspiration

00:05:53.830 --> 00:05:56.260
from other systems, including
our previous system, which

00:05:56.260 --> 00:05:57.940
was called DisBelief.

00:05:57.940 --> 00:06:01.270
And in DisBelief we had two
distinct kinds of processes.

00:06:01.270 --> 00:06:04.299
We had parameter servers
and worker replicas.

00:06:04.299 --> 00:06:06.340
And parameter servers are
responsible for holding

00:06:06.340 --> 00:06:09.250
onto all of the model
state, so the parameters,

00:06:09.250 --> 00:06:12.370
and updating them in response
to incoming gradients,

00:06:12.370 --> 00:06:13.850
whereas the worker
replicas, they

00:06:13.850 --> 00:06:16.130
would do all the intensive
part of the computation.

00:06:16.130 --> 00:06:17.920
So they would do the
input processing.

00:06:17.920 --> 00:06:20.120
They'd calculate the
loss for your network.

00:06:20.120 --> 00:06:22.120
And they'd do the backprop
to actually calculate

00:06:22.120 --> 00:06:24.370
those gradients.

00:06:24.370 --> 00:06:26.590
In TensorFlow, we can
mimic this architecture

00:06:26.590 --> 00:06:28.300
by designating some
of the processes

00:06:28.300 --> 00:06:30.520
to be what we call PS tasks.

00:06:30.520 --> 00:06:33.640
And we put the variable
nodes and the ops that

00:06:33.640 --> 00:06:35.902
update them on those tasks.

00:06:35.902 --> 00:06:38.110
And then we put the rest of
the graph, the nodes that

00:06:38.110 --> 00:06:40.456
do the pre-processing, the
forward, and the backprop

00:06:40.456 --> 00:06:42.580
on the worker tasks, which
tend to be more powerful

00:06:42.580 --> 00:06:46.320
and have multi-course
CPUs and GPUs.

00:06:46.320 --> 00:06:48.880
And one big difference
between how we did things

00:06:48.880 --> 00:06:50.650
in DisBelief and
in TensorFlow is

00:06:50.650 --> 00:06:54.070
that in DisBelief, the parameter
servers and the worker replicas

00:06:54.070 --> 00:06:55.780
were two completely
different programs.

00:06:55.780 --> 00:06:58.480
They were basically two
loosely-coupled distributed

00:06:58.480 --> 00:07:00.110
systems that had very
little in common.

00:07:00.110 --> 00:07:06.130
And you would program them
by very different APIs.

00:07:06.130 --> 00:07:08.640
In TensorFlow, the PS
and the worker tasks

00:07:08.640 --> 00:07:10.750
run exactly the same code.

00:07:10.750 --> 00:07:12.850
They are just
servers that you can

00:07:12.850 --> 00:07:14.814
send little bits of
TensorFlow graph to,

00:07:14.814 --> 00:07:16.480
and it will react to
those bits of graph

00:07:16.480 --> 00:07:17.855
and execute them very quickly.

00:07:17.855 --> 00:07:19.480
And this gives us a
lot of flexibility.

00:07:19.480 --> 00:07:21.880
So a PS task, for
example, we could

00:07:21.880 --> 00:07:24.640
have it use a GPU to accelerate
some of the parameter update

00:07:24.640 --> 00:07:25.840
computations.

00:07:25.840 --> 00:07:29.260
Or we could have the workers
store some state locally

00:07:29.260 --> 00:07:33.090
to avoid network traffic,
to cache some of the reads.

00:07:33.090 --> 00:07:34.960
Or we could even cut
out the PS altogether

00:07:34.960 --> 00:07:36.940
and use direct connections
between the workers,

00:07:36.940 --> 00:07:39.250
if we had a network that
was fast enough to deal

00:07:39.250 --> 00:07:41.710
with that communication.

00:07:41.710 --> 00:07:43.950
But with that said,
the old PS-worker split

00:07:43.950 --> 00:07:46.540
has been really useful for
training a lot of our models.

00:07:46.540 --> 00:07:49.210
And so we've developed a decent
amount of software support

00:07:49.210 --> 00:07:50.020
around this idea.

00:07:50.020 --> 00:07:52.750
And I'm going to show you
now some examples of how we

00:07:52.750 --> 00:07:55.461
do that in the next few slides.

00:07:55.461 --> 00:07:55.960
All right.

00:07:55.960 --> 00:07:57.640
So the first idea
that works pretty well

00:07:57.640 --> 00:07:59.290
for distributed
training, particularly

00:07:59.290 --> 00:08:02.710
when a single model will
fit in a single machine,

00:08:02.710 --> 00:08:04.400
is replication.

00:08:04.400 --> 00:08:07.540
So just like in DisBelief, we
take the compute-intensive part

00:08:07.540 --> 00:08:10.510
of the model training, the
forwards and the backprop,

00:08:10.510 --> 00:08:12.970
and we make a copy of it
in multiple worker tasks,

00:08:12.970 --> 00:08:15.906
so that each task works on a
different subset of the data.

00:08:15.906 --> 00:08:17.530
This is data parallel
training, like we

00:08:17.530 --> 00:08:20.597
were doing for that Inception
example back at the start.

00:08:20.597 --> 00:08:22.930
And the simplest thing,
simplest way we can achieve this

00:08:22.930 --> 00:08:26.620
is by doing something I'm going
to call in-graph replication.

00:08:26.620 --> 00:08:29.020
The reason for this name
will hopefully become

00:08:29.020 --> 00:08:30.790
self-explanatory when I--

00:08:30.790 --> 00:08:34.690
or maybe just explanatory when
I tell you what the code does.

00:08:34.690 --> 00:08:37.570
We start by putting the
variables on a PS task,

00:08:37.570 --> 00:08:39.190
like the earlier example.

00:08:39.190 --> 00:08:41.530
And this is just so that
they're in a central location

00:08:41.530 --> 00:08:44.729
that they can be accessed
by all of the workers.

00:08:44.729 --> 00:08:47.020
And then the easiest way to
do the in-graph replication

00:08:47.020 --> 00:08:49.330
is just to split up
a batch of input data

00:08:49.330 --> 00:08:52.810
into equal-sized chunks,
loop over the worker tasks,

00:08:52.810 --> 00:08:56.260
and use this
tf.device string here

00:08:56.260 --> 00:09:00.490
to put a subgraph on each worker
to compute a partial result.

00:09:00.490 --> 00:09:02.110
And then finally,
we combine together

00:09:02.110 --> 00:09:04.120
all of the partial
results into a single loss

00:09:04.120 --> 00:09:07.650
value that we optimize by
using a standard TensorFlow

00:09:07.650 --> 00:09:09.281
optimizer.

00:09:09.281 --> 00:09:11.530
And sure enough, when you
tell it to compute the loss,

00:09:11.530 --> 00:09:13.880
TensorFlow will split up the
graph across the workers,

00:09:13.880 --> 00:09:16.980
and it will run across these
worker tasks and the PS all in

00:09:16.980 --> 00:09:19.320
parallel.

00:09:19.320 --> 00:09:22.920
So in-graph replication
is pretty easy to achieve.

00:09:22.920 --> 00:09:25.200
It's not a big modification
to your existing programs.

00:09:25.200 --> 00:09:28.560
And it works pretty well up
to a small number of replicas.

00:09:28.560 --> 00:09:31.050
If you want to replicate
across all the GPUs

00:09:31.050 --> 00:09:33.420
in a single machine, then
maybe in-graph replication

00:09:33.420 --> 00:09:34.704
is the way to go.

00:09:34.704 --> 00:09:36.120
But one of the
things we found out

00:09:36.120 --> 00:09:39.300
when we tried to apply this
technique to a large model

00:09:39.300 --> 00:09:41.850
like Inception_V3, and
we tried to scale it up

00:09:41.850 --> 00:09:43.720
to a couple of
hundred machines, was

00:09:43.720 --> 00:09:46.950
that this graph gets really
big if you have to materialize

00:09:46.950 --> 00:09:47.960
all the replicas in it.

00:09:47.960 --> 00:09:49.543
And the client gets
bogged down trying

00:09:49.543 --> 00:09:53.379
to coordinate the computation
and to build this whole graph.

00:09:53.379 --> 00:09:55.920
And that's why we came up with
an alternative approach, which

00:09:55.920 --> 00:09:58.210
is called between-graph
replication.

00:09:58.210 --> 00:10:00.182
This is currently what
we use most internally.

00:10:00.182 --> 00:10:01.640
It's what we
recommend in Cloud ML.

00:10:01.640 --> 00:10:03.780
And it's also what
our high-level APIs

00:10:03.780 --> 00:10:05.470
are designed to do.

00:10:05.470 --> 00:10:07.650
So if you've ever
written an NPI program,

00:10:07.650 --> 00:10:10.710
between-graph replication should
be a kind of familiar concept.

00:10:10.710 --> 00:10:13.620
So instead of running one
all-powerful client program

00:10:13.620 --> 00:10:15.840
that knows about all
of the worker replicas,

00:10:15.840 --> 00:10:18.787
we run a smaller client
program on each task.

00:10:18.787 --> 00:10:20.370
And that client
program just builds up

00:10:20.370 --> 00:10:23.107
the graph for a single
replica of the model.

00:10:23.107 --> 00:10:24.690
And this client
program is essentially

00:10:24.690 --> 00:10:27.160
doing the same thing, with one
key difference in the device

00:10:27.160 --> 00:10:27.660
placement.

00:10:27.660 --> 00:10:30.840
So it takes kind of the
non-parameter part of the graph

00:10:30.840 --> 00:10:32.510
and it puts it on
the local devices,

00:10:32.510 --> 00:10:36.592
or the devices that are
local to that worker replica.

00:10:36.592 --> 00:10:38.300
And so now when you
run it, each program

00:10:38.300 --> 00:10:41.134
is running its smaller
graph independently,

00:10:41.134 --> 00:10:42.800
and they get mapped
to different subsets

00:10:42.800 --> 00:10:46.684
of the devices that intersect
on the PS task in the middle.

00:10:46.684 --> 00:10:48.350
There's a little bit
of magic here which

00:10:48.350 --> 00:10:50.100
I should probably
explain, in the interest

00:10:50.100 --> 00:10:51.260
of full disclosure.

00:10:51.260 --> 00:10:55.190
So each replica places its
variables on the same PS task.

00:10:55.190 --> 00:10:57.860
And when you're running in
distributed mode, by default,

00:10:57.860 --> 00:10:59.750
any two clients that
create a variable

00:10:59.750 --> 00:11:02.710
with the same name
on the same device

00:11:02.710 --> 00:11:05.584
will share the same backing
storage for that variable.

00:11:05.584 --> 00:11:07.250
And if you're doing
replicated training,

00:11:07.250 --> 00:11:08.840
this is exactly what you want.

00:11:08.840 --> 00:11:12.770
So the updates from task 0, when
it's applying its gradients,

00:11:12.770 --> 00:11:15.420
will be visible in
task 1, and vice versa.

00:11:15.420 --> 00:11:17.490
So you'll train faster.

00:11:17.490 --> 00:11:18.990
But this brings us
to another issue,

00:11:18.990 --> 00:11:21.320
which is how do you decide
where to put the variables?

00:11:21.320 --> 00:11:24.556
So far, I've just been
putting them on jobPS task 0

00:11:24.556 --> 00:11:26.180
and setting that as
the explicit device

00:11:26.180 --> 00:11:27.939
string for the variables.

00:11:27.939 --> 00:11:29.480
But device strings,
you know, they're

00:11:29.480 --> 00:11:31.480
all very good if you want
to put all of your ops

00:11:31.480 --> 00:11:32.992
in one particular location.

00:11:32.992 --> 00:11:34.700
But you often want to
do things like have

00:11:34.700 --> 00:11:37.220
more than one PS task, say
if you want to distribute

00:11:37.220 --> 00:11:38.690
the work of updating
the variables

00:11:38.690 --> 00:11:42.740
or distribute the network
load of fetching them

00:11:42.740 --> 00:11:44.300
to the workers.

00:11:44.300 --> 00:11:47.000
So instead of passing a
device string to tf.device,

00:11:47.000 --> 00:11:49.190
you can also pass
a device function.

00:11:49.190 --> 00:11:51.980
And we use these to build more
complicated, more sophisticated

00:11:51.980 --> 00:11:54.720
placement strategies.

00:11:54.720 --> 00:11:57.324
Device functions are pretty
general, very powerful.

00:11:57.324 --> 00:11:59.240
They let you basically
customize the placement

00:11:59.240 --> 00:12:00.920
of every single op in the graph.

00:12:00.920 --> 00:12:02.830
That makes them a little
bit tricky to use.

00:12:02.830 --> 00:12:04.520
So one of the
things we've done is

00:12:04.520 --> 00:12:06.680
to provide a few
pre-canned device functions

00:12:06.680 --> 00:12:08.222
to make your life a bit easier.

00:12:08.222 --> 00:12:09.680
And the simplest
of these is called

00:12:09.680 --> 00:12:12.080
tf_train_replica_device_setter.

00:12:12.080 --> 00:12:13.910
And its job is to
assign variables

00:12:13.910 --> 00:12:16.782
to PS tasks in a round-robin
fashion as they're created.

00:12:16.782 --> 00:12:18.490
One nice thing about
this device function

00:12:18.490 --> 00:12:20.448
is you can just wrap your
entire model building

00:12:20.448 --> 00:12:22.550
code in this with block.

00:12:22.550 --> 00:12:25.670
It only affects the variables,
puts them on PS tasks.

00:12:25.670 --> 00:12:29.520
And the rest of your ops in
the graph will go on a worker.

00:12:29.520 --> 00:12:31.400
So if you're doing
between-graph replication,

00:12:31.400 --> 00:12:33.930
this takes care
of it all for you.

00:12:33.930 --> 00:12:36.860
So for this program,
it will assign devices

00:12:36.860 --> 00:12:38.550
to the parameters
as they're created.

00:12:38.550 --> 00:12:40.730
So the first weight
matrix will go on task 0.

00:12:40.730 --> 00:12:43.000
A bias vector will go on task 1.

00:12:43.000 --> 00:12:44.510
The second weight's on task 2.

00:12:44.510 --> 00:12:47.910
And then the second
bias is back on task 0.

00:12:47.910 --> 00:12:51.270
This is obviously not
an optimal balanced load

00:12:51.270 --> 00:12:53.709
for these variables, neither
in terms of the memory usage

00:12:53.709 --> 00:12:55.500
or the amount of work
that would need to be

00:12:55.500 --> 00:12:57.557
done to update these variables.

00:12:57.557 --> 00:12:59.640
And just think, if we only
had two PS tasks here--

00:12:59.640 --> 00:13:00.990
I guess I should have
drawn a diagram--

00:13:00.990 --> 00:13:03.490
we'd end up with an even worse
case, because all the weights

00:13:03.490 --> 00:13:06.300
would be on task 0, and all the
biases would be on the other.

00:13:06.300 --> 00:13:09.840
We'd have an even bigger
imbalance between those tasks.

00:13:09.840 --> 00:13:11.370
So clearly, we can
do better here.

00:13:11.370 --> 00:13:14.250
And one of the ways that we've
provided to achieve a more

00:13:14.250 --> 00:13:16.650
balanced load is to use
something called a load

00:13:16.650 --> 00:13:18.210
balancing strategy.

00:13:18.210 --> 00:13:21.210
And this is an optional argument
to the replica device setter.

00:13:21.210 --> 00:13:23.880
Right now we only have, I think,
a simple greedy strategy that

00:13:23.880 --> 00:13:25.620
does a kind of
online bin packing

00:13:25.620 --> 00:13:28.140
based on the number of
bytes in your parameters.

00:13:28.140 --> 00:13:30.600
And it leads to this
more balanced outcome.

00:13:30.600 --> 00:13:33.480
Each of the weight matrices
is put on a separate PS task,

00:13:33.480 --> 00:13:35.940
and the biases get packed
together on task 1.

00:13:35.940 --> 00:13:38.023
It's a lot more balanced,
and this should give you

00:13:38.023 --> 00:13:40.815
a lot better performance.

00:13:40.815 --> 00:13:42.940
And so far, I've only talked
about relatively small

00:13:42.940 --> 00:13:45.820
variables that all
fit in a single task.

00:13:45.820 --> 00:13:48.370
But what about these
outrageously large model

00:13:48.370 --> 00:13:51.010
parameters, things like
the large embeddings

00:13:51.010 --> 00:13:54.020
that might be tens
of gigabytes in size?

00:13:54.020 --> 00:13:55.540
Well, to deal with
these, we have

00:13:55.540 --> 00:13:56.980
something called partitioners.

00:13:56.980 --> 00:13:59.260
They do exactly what it
sounds like they would do.

00:13:59.260 --> 00:14:01.676
If you create a variable with
a partitioner, like this one

00:14:01.676 --> 00:14:04.810
here, TensorFlow will split
the large logical variable

00:14:04.810 --> 00:14:07.420
into three concrete
parts and assign them

00:14:07.420 --> 00:14:09.944
to different PS tasks.

00:14:09.944 --> 00:14:11.860
And one nice thing about
doing things this way

00:14:11.860 --> 00:14:13.270
is if you take that
embedding variable

00:14:13.270 --> 00:14:15.478
and then you pass it to some
of the embedding-related

00:14:15.478 --> 00:14:16.940
functions, like
embedding look-up,

00:14:16.940 --> 00:14:19.090
for example, it
will take advantage

00:14:19.090 --> 00:14:21.490
of the knowledge about
the fact that it's

00:14:21.490 --> 00:14:22.990
going to be
partitioned, and it will

00:14:22.990 --> 00:14:25.630
offload some of the
embedding look-up computation

00:14:25.630 --> 00:14:27.640
and the gradients to
the parameter server

00:14:27.640 --> 00:14:30.380
devices themselves.

00:14:30.380 --> 00:14:31.000
All right.

00:14:31.000 --> 00:14:33.110
That was a lot of detail
about device placement.

00:14:33.110 --> 00:14:35.250
But it is a very big topic.

00:14:35.250 --> 00:14:37.760
I guess you could say it's a
combinatorially large problem

00:14:37.760 --> 00:14:39.860
domain that we have to solve.

00:14:39.860 --> 00:14:41.510
The main thing I
want you to take away

00:14:41.510 --> 00:14:43.700
is that the
tf_train_replica_device_setter

00:14:43.700 --> 00:14:46.730
is this simple heuristic that
works for a lot of distributed

00:14:46.730 --> 00:14:48.080
training use cases.

00:14:48.080 --> 00:14:49.580
And you can customize
it if you need

00:14:49.580 --> 00:14:52.280
to by providing these optional
strategies for things like load

00:14:52.280 --> 00:14:55.400
balancing and partitioning, or
even writing your own device

00:14:55.400 --> 00:14:58.369
functions that override the
simple policies we provide.

00:14:58.369 --> 00:15:00.410
And this is definitely an
area where we're always

00:15:00.410 --> 00:15:02.870
on the lookout for new and
better device placement

00:15:02.870 --> 00:15:03.480
policies.

00:15:03.480 --> 00:15:05.240
So if you do end up
implementing your own

00:15:05.240 --> 00:15:07.031
and you think it might
be generally useful,

00:15:07.031 --> 00:15:09.380
we really encourage you to
send it as a pull request,

00:15:09.380 --> 00:15:11.210
and we can add it
to the set of tools

00:15:11.210 --> 00:15:14.210
that people have to
make their lives easier.

00:15:14.210 --> 00:15:14.900
All right.

00:15:14.900 --> 00:15:16.610
So you've built your graph.

00:15:16.610 --> 00:15:18.290
You've got all of
the devices set.

00:15:18.290 --> 00:15:21.450
And now you want to
go ahead and run it.

00:15:21.450 --> 00:15:25.100
But when you create a TensorFlow
session using code like this,

00:15:25.100 --> 00:15:27.410
that session will only
know about the devices

00:15:27.410 --> 00:15:29.657
in a local machine.

00:15:29.657 --> 00:15:31.490
And let's say you have
200 computers sitting

00:15:31.490 --> 00:15:32.880
idle over there.

00:15:32.880 --> 00:15:35.300
How do you make it
use those computers?

00:15:35.300 --> 00:15:38.270
The answer is that you create
this thing called a TensorFlow

00:15:38.270 --> 00:15:40.470
server in each of
those machines.

00:15:40.470 --> 00:15:42.320
And you configure those
servers in a cluster

00:15:42.320 --> 00:15:45.660
so that they can communicate
over the network.

00:15:45.660 --> 00:15:47.470
Now, looking at
it in more detail,

00:15:47.470 --> 00:15:50.244
the first thing we need to do
is to provide a cluster spec.

00:15:50.244 --> 00:15:52.660
This is something that tells
TensorFlow about the machines

00:15:52.660 --> 00:15:53.810
that you want to run on.

00:15:53.810 --> 00:15:55.560
A cluster spec is
really just a dictionary

00:15:55.560 --> 00:15:57.140
that maps the names of jobs--

00:15:57.140 --> 00:16:00.500
so that's things like worker
and PS, in this example--

00:16:00.500 --> 00:16:03.390
to a list of one or more network
addresses that correspond

00:16:03.390 --> 00:16:05.655
to the tasks in each job.

00:16:05.655 --> 00:16:07.030
Now, we don't
actually expect you

00:16:07.030 --> 00:16:09.390
to type in all of these
addresses by hand.

00:16:09.390 --> 00:16:11.687
It gets kind of error prone.

00:16:11.687 --> 00:16:13.270
But in the next talk,
Jonathan's going

00:16:13.270 --> 00:16:15.760
to be showing you how a
cluster manager like Kubernetes

00:16:15.760 --> 00:16:18.000
or Mesos will do this for you.

00:16:18.000 --> 00:16:19.592
And if you're using
a cluster manager,

00:16:19.592 --> 00:16:21.550
internally we use a
cluster manager called Borg

00:16:21.550 --> 00:16:23.644
that a lot of this
stuff was inspired by.

00:16:23.644 --> 00:16:25.060
So typically, your
cluster manager

00:16:25.060 --> 00:16:27.610
will run an instance
of your program

00:16:27.610 --> 00:16:30.160
on each machine in the cluster,
giving it the same cluster

00:16:30.160 --> 00:16:31.350
spec.

00:16:31.350 --> 00:16:33.850
And then it'll start
a TensorFlow server

00:16:33.850 --> 00:16:35.210
in each program.

00:16:35.210 --> 00:16:37.210
It'll pass it a particular
job name and task

00:16:37.210 --> 00:16:40.000
index that matches the
address of the local machine

00:16:40.000 --> 00:16:41.660
in that cluster.

00:16:41.660 --> 00:16:43.750
And then finally, when
you create your session,

00:16:43.750 --> 00:16:46.540
you specify the local
server's address

00:16:46.540 --> 00:16:49.480
as the target, which is
what enables it to connect

00:16:49.480 --> 00:16:51.880
through that server to any
of the machines mentioned

00:16:51.880 --> 00:16:53.286
in the cluster spec.

00:16:53.286 --> 00:16:54.410
And then you're good to go.

00:16:54.410 --> 00:16:57.010
So your session run
call can run code

00:16:57.010 --> 00:16:59.027
on any device in
the entire cluster.

00:16:59.027 --> 00:17:00.610
And typically, what
the worker will do

00:17:00.610 --> 00:17:02.651
is it'll start a training
loop that just iterates

00:17:02.651 --> 00:17:05.290
over its partition of the
data, running a training

00:17:05.290 --> 00:17:07.560
op over and over again.

00:17:07.560 --> 00:17:08.060
OK.

00:17:08.060 --> 00:17:09.589
So that's what a
worker looks like.

00:17:09.589 --> 00:17:11.230
PS task is much simpler.

00:17:11.230 --> 00:17:12.935
So PS tasks simply--

00:17:12.935 --> 00:17:15.010
they have basically
no client code.

00:17:15.010 --> 00:17:17.770
They just respond to
incoming bits of graph

00:17:17.770 --> 00:17:19.810
that are sent to them
by other workers.

00:17:19.810 --> 00:17:21.601
So in this case, you
just build the server.

00:17:21.601 --> 00:17:23.230
You say it's job name PS.

00:17:23.230 --> 00:17:25.819
And then you call
join on the server.

00:17:25.819 --> 00:17:28.300
And all that does is it
blocks waiting for connections

00:17:28.300 --> 00:17:30.709
to come in from other
nodes in the cluster.

00:17:30.709 --> 00:17:32.000
And actually, it's a bit weird.

00:17:32.000 --> 00:17:34.300
So I deal with a lot of
questions on stack overflow.

00:17:34.300 --> 00:17:36.310
And one of the common
ones is, can you

00:17:36.310 --> 00:17:39.040
show me where in the
implementation of server.join

00:17:39.040 --> 00:17:41.410
the parameter server code lives?

00:17:41.410 --> 00:17:43.480
And if you go and look
at it, it's like, five,

00:17:43.480 --> 00:17:44.630
10 lines of C++.

00:17:44.630 --> 00:17:46.191
It just does some
error checking,

00:17:46.191 --> 00:17:47.940
and then it blocks on
a couple of threads.

00:17:47.940 --> 00:17:49.180
It joins a couple of threads.

00:17:49.180 --> 00:17:51.240
That's why it's called join.

00:17:51.240 --> 00:17:53.597
But it's a reasonable
misconception,

00:17:53.597 --> 00:17:54.805
because this is your PS task.

00:17:54.805 --> 00:17:56.110
Well, what's that
actually doing?

00:17:56.110 --> 00:17:58.484
Well, this highlights something
quite important about how

00:17:58.484 --> 00:18:00.460
distributed TensorFlow works.

00:18:00.460 --> 00:18:02.110
So parameter
servers, everything,

00:18:02.110 --> 00:18:04.630
all the behavior of
a parameter server,

00:18:04.630 --> 00:18:07.240
is not implemented at the
low level in these servers

00:18:07.240 --> 00:18:09.550
or in the execution
engine, but instead it's

00:18:09.550 --> 00:18:12.170
built out of TensorFlow
programming primitives,

00:18:12.170 --> 00:18:14.050
as these little bits
of data flow graph

00:18:14.050 --> 00:18:16.570
that a worker ships
to a server to say,

00:18:16.570 --> 00:18:19.300
manage some parameters for
me and update them this way.

00:18:19.300 --> 00:18:21.880
And that, representing the
way we program the parameter

00:18:21.880 --> 00:18:24.160
servers as little fragments
of data flow graph,

00:18:24.160 --> 00:18:26.140
is what gives us
the flexibility.

00:18:26.140 --> 00:18:29.200
So unless you change everything
about how parameter management

00:18:29.200 --> 00:18:31.870
is implemented, if you want
to customize the optimization

00:18:31.870 --> 00:18:34.360
algorithm, or maybe
change the synchronization

00:18:34.360 --> 00:18:36.420
scheme for applying
the updates, you

00:18:36.420 --> 00:18:38.260
can do that just by
changing the graph.

00:18:38.260 --> 00:18:39.801
And we've gained a
lot of performance

00:18:39.801 --> 00:18:43.480
by doing that in some
of our algorithms.

00:18:43.480 --> 00:18:43.990
All right.

00:18:43.990 --> 00:18:46.630
So now that your
servers are listening,

00:18:46.630 --> 00:18:48.910
your sessions are all created,
and your training loop

00:18:48.910 --> 00:18:51.600
is running, are we done?

00:18:51.600 --> 00:18:55.830
Well, whenever you run some
long-running training job

00:18:55.830 --> 00:18:59.580
on a set of machines, I hope you
take the wise words of Leslie

00:18:59.580 --> 00:19:03.069
Lamport to heart, and
always use a saver

00:19:03.069 --> 00:19:04.610
in any long-running
training process.

00:19:04.610 --> 00:19:06.840
So a saver, for those of you
who haven't seen that before,

00:19:06.840 --> 00:19:09.340
is just what you use to write
out a checkpoint of your model

00:19:09.340 --> 00:19:10.590
parameters to disk.

00:19:10.590 --> 00:19:14.190
The same saver can be used for
local and distributed training,

00:19:14.190 --> 00:19:16.890
but it has a couple of useful
features in distributed mode

00:19:16.890 --> 00:19:19.076
that I want to highlight.

00:19:19.076 --> 00:19:20.950
The first one is that
you'll almost certainly

00:19:20.950 --> 00:19:24.970
want to set sharded equals true
when you create your saver.

00:19:24.970 --> 00:19:27.820
So in this example, we
have three PS tasks,

00:19:27.820 --> 00:19:30.020
and so sharded equals
true tells TensorFlow,

00:19:30.020 --> 00:19:32.020
write the checkpoint
in three shards,

00:19:32.020 --> 00:19:34.330
each containing all
the variables from one

00:19:34.330 --> 00:19:37.030
particular parameter server.

00:19:37.030 --> 00:19:39.120
And that means that the
parameter servers can

00:19:39.120 --> 00:19:40.872
write directly to
the file system,

00:19:40.872 --> 00:19:43.330
and we don't have to collect
all of the values in one place

00:19:43.330 --> 00:19:44.560
in order to write them out.

00:19:44.560 --> 00:19:47.050
So actually the default
behavior is kind of pessimal,

00:19:47.050 --> 00:19:48.970
because if you set
sharded equals false,

00:19:48.970 --> 00:19:51.322
it will bring all of the
variables into one save op,

00:19:51.322 --> 00:19:53.530
meaning they have to be
materialized in memory at one

00:19:53.530 --> 00:19:55.524
process before it
writes the first byte.

00:19:55.524 --> 00:19:57.190
And if you have these
really big models,

00:19:57.190 --> 00:20:00.624
well, that's a sure way
to have a memory error.

00:20:00.624 --> 00:20:02.040
So always use
sharded equals true.

00:20:02.040 --> 00:20:04.360
That's if you
remember one thing.

00:20:04.360 --> 00:20:07.540
OK, second, if you are using
between-graph replication,

00:20:07.540 --> 00:20:12.170
you now have a choice
of several worker tasks

00:20:12.170 --> 00:20:14.710
that could be responsible
for writing this checkpoint.

00:20:14.710 --> 00:20:17.020
And by convention, we give
some extra responsibilities

00:20:17.020 --> 00:20:17.927
to worker task 0.

00:20:17.927 --> 00:20:20.260
We just picked that because
there's always a worker task

00:20:20.260 --> 00:20:24.410
0 in our jobs, because that's
where the numbering starts.

00:20:24.410 --> 00:20:25.930
And we call it the chief worker.

00:20:25.930 --> 00:20:28.630
And so the chief worker has a
few important maintenance tasks

00:20:28.630 --> 00:20:31.510
that it has to carry out,
doing things like writing

00:20:31.510 --> 00:20:32.890
checkpoints, in this case.

00:20:32.890 --> 00:20:34.960
Also things like
initializing the parameters

00:20:34.960 --> 00:20:38.510
at the start of day and logging
summaries for TensorBoard.

00:20:38.510 --> 00:20:40.534
These are all done by the chief.

00:20:40.534 --> 00:20:41.950
And then the last
thing to note is

00:20:41.950 --> 00:20:44.500
that savers now support a
variety of distributed file

00:20:44.500 --> 00:20:45.520
systems.

00:20:45.520 --> 00:20:47.380
So instead of writing
to a local file,

00:20:47.380 --> 00:20:49.270
you can write to
Google Cloud Storage,

00:20:49.270 --> 00:20:51.230
let's say if you're
running on Cloud ML.

00:20:51.230 --> 00:20:53.230
Or if you're running on
top of a Hadoop cluster,

00:20:53.230 --> 00:20:55.259
you can write straight to HDFS.

00:20:55.259 --> 00:20:56.925
And using a distributed
file system here

00:20:56.925 --> 00:20:59.410
is a smart idea, because it
gives you more durable storage.

00:20:59.410 --> 00:21:01.660
You're not going to lose
your model checkpoints if one

00:21:01.660 --> 00:21:03.187
of the machines goes on fire.

00:21:03.187 --> 00:21:05.020
It also makes it easier
to read a checkpoint

00:21:05.020 --> 00:21:05.894
from another machine.

00:21:05.894 --> 00:21:09.580
So one common pattern we use is
we have a separate evaluation

00:21:09.580 --> 00:21:11.540
task that's running
on a separate machine,

00:21:11.540 --> 00:21:13.420
and it just goes and
picks up the latest

00:21:13.420 --> 00:21:16.450
checkpoint of the model and
evaluates the test set on that.

00:21:16.450 --> 00:21:19.210
So this makes that kind of
loosely-coupled coordination

00:21:19.210 --> 00:21:22.130
a bit easier.

00:21:22.130 --> 00:21:22.630
All right.

00:21:22.630 --> 00:21:23.980
So you're writing
checkpoints now.

00:21:23.980 --> 00:21:24.771
That's a good step.

00:21:24.771 --> 00:21:26.510
That guards against failure.

00:21:26.510 --> 00:21:30.370
But what actually happens
when you experience a fault?

00:21:30.370 --> 00:21:31.120
In the best case--

00:21:31.120 --> 00:21:32.536
there are a few
cases to consider.

00:21:32.536 --> 00:21:35.155
And the best case is that one
of your non-chief workers fails.

00:21:35.155 --> 00:21:36.790
And the reason this
is the best case

00:21:36.790 --> 00:21:38.830
is because these
workers are typically

00:21:38.830 --> 00:21:41.200
sort of effectively stateless.

00:21:41.200 --> 00:21:44.440
So when a non-chief worker
comes back up, all it has to do

00:21:44.440 --> 00:21:46.780
is contact all the PS
tasks again and carry on

00:21:46.780 --> 00:21:48.186
as if nothing happened.

00:21:48.186 --> 00:21:50.590
And usually we have
a cluster manager

00:21:50.590 --> 00:21:56.640
that will restart a failed
process as soon as it crashes.

00:21:56.640 --> 00:21:58.390
If a PS task fails,
that's a little worse,

00:21:58.390 --> 00:21:59.740
because PS tasks are stateful.

00:21:59.740 --> 00:22:02.230
All the workers are relying on
them to send their gradients

00:22:02.230 --> 00:22:04.360
and get the new values
of the parameters.

00:22:04.360 --> 00:22:06.850
So in this case, the chief
is responsible for noticing

00:22:06.850 --> 00:22:07.660
the failure.

00:22:07.660 --> 00:22:09.700
It interrupts training
on all of the workers,

00:22:09.700 --> 00:22:13.535
and restores all the PS tasks
from the last checkpoint.

00:22:13.535 --> 00:22:15.910
The trickiest case is when
the chief fails, because we've

00:22:15.910 --> 00:22:17.784
given it all of these
extra responsibilities,

00:22:17.784 --> 00:22:20.510
and we need to make sure that
we get back to a good state

00:22:20.510 --> 00:22:21.457
when it comes back up.

00:22:21.457 --> 00:22:23.540
So training could continue
when the chief is down,

00:22:23.540 --> 00:22:25.190
but when it comes
back up, we don't

00:22:25.190 --> 00:22:27.140
know if, for example,
another task has failed.

00:22:27.140 --> 00:22:31.550
So it has to go and restore
bits from the last checkpoint.

00:22:31.550 --> 00:22:34.460
So the thing we do is
just to interrupt training

00:22:34.460 --> 00:22:35.607
when the chief fails.

00:22:35.607 --> 00:22:37.940
And when it comes back up,
we restore from a checkpoint,

00:22:37.940 --> 00:22:39.860
just like a parameter
server fails.

00:22:39.860 --> 00:22:44.060
This is a pretty simple and
a pretty conservative scheme.

00:22:44.060 --> 00:22:47.120
And it only really works if your
machines are reliable enough

00:22:47.120 --> 00:22:49.850
that they aren't
failing all the time.

00:22:49.850 --> 00:22:53.110
And it keeps the logic for
recovery really, really simple.

00:22:53.110 --> 00:22:55.801
But depending on how common
failures are in your cluster

00:22:55.801 --> 00:22:57.800
environment, maybe it
would be beneficial to use

00:22:57.800 --> 00:23:00.560
a different policy that tries
to keep the training job running

00:23:00.560 --> 00:23:02.180
without interruption.

00:23:02.180 --> 00:23:04.790
And I just want to sort of
provoke some thinking here.

00:23:04.790 --> 00:23:07.490
One thing you could do is maybe
use a configuration management

00:23:07.490 --> 00:23:10.640
server, something like
Apache ZooKeeper or Etcd,

00:23:10.640 --> 00:23:12.470
to choose a chief
by leader election,

00:23:12.470 --> 00:23:15.152
rather than saying it
always has to be worker 0.

00:23:15.152 --> 00:23:16.610
And then if you
did that, you could

00:23:16.610 --> 00:23:20.090
pass on the responsibility of
being a chief when one of them

00:23:20.090 --> 00:23:22.486
fails, and kind of do a
failover without interrupting

00:23:22.486 --> 00:23:23.360
the training process.

00:23:23.360 --> 00:23:25.776
If somebody wanted to try that
out and contribute it back,

00:23:25.776 --> 00:23:28.730
that would be just great.

00:23:28.730 --> 00:23:30.740
But if you do like
that simple policy,

00:23:30.740 --> 00:23:32.114
we've got something for you.

00:23:32.114 --> 00:23:33.530
So you can use
this recently added

00:23:33.530 --> 00:23:37.190
MonitoredTrainingSession
class to automate the recovery

00:23:37.190 --> 00:23:38.000
process.

00:23:38.000 --> 00:23:40.305
So going back to our simple,
single-process example,

00:23:40.305 --> 00:23:42.680
when you create a TensorFlow
session, usually what you do

00:23:42.680 --> 00:23:45.620
is you explicitly
initialize the variables,

00:23:45.620 --> 00:23:47.100
or restore them
from a checkpoint,

00:23:47.100 --> 00:23:49.490
before you start training.

00:23:49.490 --> 00:23:51.940
And MonitoredTrainingSession
looks a lot like a regular tf

00:23:51.940 --> 00:23:54.770
session, but you'll see that
it kind of takes a little bit

00:23:54.770 --> 00:23:56.710
more information in
its constructor-- needs

00:23:56.710 --> 00:23:58.220
to know if it's
the chief or not,

00:23:58.220 --> 00:24:00.080
needs to know what
server you're using.

00:24:00.080 --> 00:24:04.820
And you'll note that there's no
init op in the program anymore.

00:24:04.820 --> 00:24:08.150
Instead, what the
MonitoredTrainingSession

00:24:08.150 --> 00:24:11.630
is going to do is it
will automatically

00:24:11.630 --> 00:24:14.570
ensure that all of the variables
have been initialized either

00:24:14.570 --> 00:24:16.910
from their initial
values, like the randomly

00:24:16.910 --> 00:24:19.610
initialized variables, or by
restoring them from the latest

00:24:19.610 --> 00:24:22.760
checkpoint if one is available,
before it returns control back

00:24:22.760 --> 00:24:23.720
to the user.

00:24:23.720 --> 00:24:25.250
And it does something slightly
different depending on

00:24:25.250 --> 00:24:26.240
if you're the chief or not.

00:24:26.240 --> 00:24:27.590
So if you're the
chief, it's going

00:24:27.590 --> 00:24:29.650
to go and look and see
if there is a checkpoint,

00:24:29.650 --> 00:24:31.316
or it's going to run
the initializers.

00:24:31.316 --> 00:24:33.440
If you're not the chief,
it just sits there waiting

00:24:33.440 --> 00:24:34.766
until the chief does its work.

00:24:34.766 --> 00:24:36.140
And then as soon
as it's done, it

00:24:36.140 --> 00:24:38.640
can carry on running
the training loop.

00:24:38.640 --> 00:24:41.300
There's a lot of potential
for customization here.

00:24:41.300 --> 00:24:43.880
So we support these
installable hooks

00:24:43.880 --> 00:24:46.370
that you can attach either
when you create the session

00:24:46.370 --> 00:24:49.540
or before or after doing
a session run call.

00:24:49.540 --> 00:24:51.520
And the
MonitoredTrainingSession is

00:24:51.520 --> 00:24:54.170
sort of a tastefully
curated set of these hooks

00:24:54.170 --> 00:24:56.487
that makes distributed
training a bit easier.

00:24:56.487 --> 00:24:58.820
So it comes with standard
hooks for writing a checkpoint

00:24:58.820 --> 00:25:01.927
every 10 minutes, writing
summaries every 100 steps.

00:25:01.927 --> 00:25:04.010
And if you want to, you
can customize its behavior

00:25:04.010 --> 00:25:05.840
by adding more
hooks or replacing

00:25:05.840 --> 00:25:08.920
the ones that are there.

00:25:08.920 --> 00:25:10.920
OK, finally, by this
point, some of you

00:25:10.920 --> 00:25:14.100
might be thinking, wow,
that is a lot of effort

00:25:14.100 --> 00:25:16.470
that you have to go to
just to run training

00:25:16.470 --> 00:25:19.020
on a distributed cluster.

00:25:19.020 --> 00:25:20.044
Can it be easier?

00:25:20.044 --> 00:25:21.960
And yeah, well, if you
do things from scratch,

00:25:21.960 --> 00:25:24.090
and if you use
the low-level API,

00:25:24.090 --> 00:25:26.679
there are a lot of things
that you have to take care of.

00:25:26.679 --> 00:25:28.470
But fear not, because
as you might remember

00:25:28.470 --> 00:25:30.390
from Martin's talk
this morning, we now

00:25:30.390 --> 00:25:32.100
have these high-level
APIs that have

00:25:32.100 --> 00:25:36.380
great support for training and
evaluation on a single machine.

00:25:36.380 --> 00:25:39.450
And there's tf contrib
level support for training

00:25:39.450 --> 00:25:41.310
on a distributed cluster.

00:25:41.310 --> 00:25:42.596
So what does that look like?

00:25:42.596 --> 00:25:44.220
Well, this distributed
training support

00:25:44.220 --> 00:25:48.030
uses a recently added class
called Experiment to package up

00:25:48.030 --> 00:25:51.696
the training and evaluation of
an estimator using a cluster.

00:25:51.696 --> 00:25:53.070
So when you create
an experiment,

00:25:53.070 --> 00:25:55.528
you bundle together an estimator
with some input functions.

00:25:55.528 --> 00:25:58.350
You tell it how to read
a particular data set

00:25:58.350 --> 00:26:00.260
for training and for evaluation.

00:26:00.260 --> 00:26:02.230
You give it some more
configuration options.

00:26:02.230 --> 00:26:04.790
And here you could specify
a little canned estimator

00:26:04.790 --> 00:26:08.670
in a few lines, so this is
a pre-canned DNN classifier

00:26:08.670 --> 00:26:10.220
with two hidden layers.

00:26:10.220 --> 00:26:12.020
You could do
something more fancy

00:26:12.020 --> 00:26:13.816
by passing in a
custom model function,

00:26:13.816 --> 00:26:15.690
or you could even pass
a Keras model in there

00:26:15.690 --> 00:26:17.844
to do distributed training.

00:26:17.844 --> 00:26:20.010
And you wrap that in an
experiment function and pass

00:26:20.010 --> 00:26:23.340
that function to something
called learn_runner.run, which,

00:26:23.340 --> 00:26:26.520
if you're running on Cloud ML,
will pick up the configuration

00:26:26.520 --> 00:26:29.250
from the environment and it will
run the appropriate low-level

00:26:29.250 --> 00:26:30.246
APIs for you.

00:26:30.246 --> 00:26:32.120
So your code can be
almost as simple as this.

00:26:32.120 --> 00:26:34.036
It's probably, like,
five lines of boilerplate

00:26:34.036 --> 00:26:37.587
that you'd need at the top
to run distributed training.

00:26:37.587 --> 00:26:38.670
Actually, I've been told--

00:26:38.670 --> 00:26:41.280
Martin told me that the contrib
function learn_runner is

00:26:41.280 --> 00:26:42.600
going to change its name soon.

00:26:42.600 --> 00:26:44.610
So I know we're
1.0, but we still

00:26:44.610 --> 00:26:47.340
like changing names of things.

00:26:47.340 --> 00:26:49.530
This code example is a
little bit more aspirational

00:26:49.530 --> 00:26:51.238
than some of the
others, but hopefully it

00:26:51.238 --> 00:26:53.490
gets across the idea
that the high-level APIs

00:26:53.490 --> 00:26:56.790
can take care of a lot of
the gory details for you.

00:26:56.790 --> 00:26:57.330
And so yeah.

00:26:57.330 --> 00:27:00.046
I mean, these estimators are
great if your training process

00:27:00.046 --> 00:27:01.170
follows a standard pattern.

00:27:01.170 --> 00:27:02.544
And nine times
out of 10, they're

00:27:02.544 --> 00:27:04.950
probably what you're going
to want to use when you're

00:27:04.950 --> 00:27:06.550
running distributed TensorFlow.

00:27:06.550 --> 00:27:08.091
So you can start
getting your program

00:27:08.091 --> 00:27:11.100
running on nicely-lit data
centers like these in no time

00:27:11.100 --> 00:27:12.240
at all.

00:27:12.240 --> 00:27:14.690
But sometimes you're going
to want to bend the rules,

00:27:14.690 --> 00:27:16.648
if you have some more
complicated requirements,

00:27:16.648 --> 00:27:18.760
or you want to go
slightly outside the--

00:27:18.760 --> 00:27:20.452
think slightly outside the box.

00:27:20.452 --> 00:27:21.910
So in this talk,
I've shown you how

00:27:21.910 --> 00:27:23.910
distributed TensorFlow
gives you the flexibility

00:27:23.910 --> 00:27:25.330
to do just that.

00:27:25.330 --> 00:27:27.780
You can scale training
out to hundreds of GPUs.

00:27:27.780 --> 00:27:30.750
You can train outrageously large
models with tens of billions

00:27:30.750 --> 00:27:32.010
of parameters.

00:27:32.010 --> 00:27:34.950
And you can choose between
customizing every last detail

00:27:34.950 --> 00:27:37.650
of the execution or defining
the whole training process

00:27:37.650 --> 00:27:40.340
in just a few lines of code.

00:27:40.340 --> 00:27:43.170
If you're interested in learning
a bit more about distributed

00:27:43.170 --> 00:27:44.570
TensorFlow, I'd encourage
you to check out

00:27:44.570 --> 00:27:46.819
these links, which cover
this sort of low-level system

00:27:46.819 --> 00:27:48.920
architecture, the
low-level distributed

00:27:48.920 --> 00:27:50.742
API and the high-level API.

00:27:50.742 --> 00:27:53.200
But at this point, I'd like to
thank all of you in the room

00:27:53.200 --> 00:27:55.280
and on the livestream
for listening.

00:27:55.280 --> 00:27:56.690
Thanks, all, very much.

00:27:56.690 --> 00:28:00.040
[APPLAUSE]

