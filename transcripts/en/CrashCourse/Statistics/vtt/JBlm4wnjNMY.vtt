WEBVTT
Kind: captions
Language: en

00:00:03.160 --> 00:00:07.740
Hi, I’m Adriene Hill, and Welcome back to
Crash Course Statistics. In previous episodes

00:00:07.750 --> 00:00:11.530
we’ve talked about things like cars learning
how to drive themselves...and apps that can

00:00:11.530 --> 00:00:14.870
recognize handwriting and turn it into printed
text.

00:00:14.870 --> 00:00:20.060
A lot of these projects are done using a type
of Machine Learning called a Neural Network.

00:00:20.060 --> 00:00:25.039
The term Neural Network covers a bunch of
different--but related--methods that can take

00:00:25.039 --> 00:00:27.880
in data and spit out useful outputs.

00:00:27.880 --> 00:00:32.230
Neural networks can output everything from
the probability of someone getting a particularly

00:00:32.230 --> 00:00:38.400
nasty strain of MRSA on their next hospital
stay, to new chapters of Harry Potter...seriously.

00:00:38.400 --> 00:00:43.110
They may even be behind some of the annoying
Twitter bots that just seem to spout tweets

00:00:43.110 --> 00:00:46.070
that rile people up.
Today, we’re going to take a look at the

00:00:46.070 --> 00:00:50.220
big picture of what neural networks are, and
how they do all these things.

00:00:50.220 --> 00:00:59.500
INTRO

00:00:59.520 --> 00:01:03.320
In Crash Course Computer Science, we talked
a little bit about what a neural network is.

00:01:03.320 --> 00:01:07.920
In the simplest sense, a neural network looks
at data and tries to figure out the function--or

00:01:07.920 --> 00:01:12.880
set of calculations--that turns the input...
variables...into the output.

00:01:12.880 --> 00:01:17.159
That output could be a number, a probability,
or even something a bit more complicated.

00:01:17.159 --> 00:01:22.590
Neural networks are analogous to robots that
can learn to make things---like a toy car--not

00:01:22.590 --> 00:01:27.100
by following step by step instructions from
humans, but by looking at a bunch of toy cars

00:01:27.100 --> 00:01:32.039
and figuring out for itself how to turn inputs
(like metal and plastic) into outputs (the

00:01:32.039 --> 00:01:33.039
toy cars)!

00:01:33.039 --> 00:01:37.280
If we want to work with data instead of toy
cars we can use a neural network to predict

00:01:37.280 --> 00:01:43.039
future salary based on a number of variables
such as degree, field, age, years of experience,

00:01:43.039 --> 00:01:45.369
gender, number of promotions, and university.

00:01:45.369 --> 00:01:49.490
We feed these variables to the neural network.
These circles are called Nodes, and they just

00:01:49.490 --> 00:01:52.030
hold a value like degree or field.

00:01:52.030 --> 00:01:56.960
Eventually we want the Neural Network to output
its prediction for future salary. So we know

00:01:56.960 --> 00:02:02.200
there will be one output node at the end of
our network that tells us what it predicts

00:02:02.200 --> 00:02:03.200
the salary will be .

00:02:03.200 --> 00:02:07.380
At this point, the Neural Network looks kinda
like a regression, we have a bunch of inputs...our

00:02:07.380 --> 00:02:12.140
variables...which are combined in some way
to create an output...our predicted value.

00:02:12.140 --> 00:02:17.310
But unlike most regressions, neural networks
feed the weighted sum of age, degree, field,

00:02:17.310 --> 00:02:22.700
etc through something called an “activation
function” which takes the value and transforms

00:02:22.700 --> 00:02:24.620
it before returning an output.

00:02:24.620 --> 00:02:28.670
These activation functions improve the way
many neural networks learn, and give them

00:02:28.670 --> 00:02:33.400
more flexibility to model complex relationships
between input and output.

00:02:33.400 --> 00:02:38.350
One common activation function is called Rectified
Linear Unit (ReLU) --which turns all negative

00:02:38.350 --> 00:02:41.020
values to 0, and leaves positive ones as they
are.

00:02:41.020 --> 00:02:45.120
This makes these nodes act a little bit like
neurons in your brain--hence the name neural

00:02:45.120 --> 00:02:49.820
network-- which require a certain “threshold”
of activation before they’ll fire. So a

00:02:49.820 --> 00:02:54.320
node with 0 doesn’t fire, or contribute
to the output at all. But one with a positive

00:02:54.320 --> 00:02:55.350
value will.

00:02:55.350 --> 00:03:00.390
This Neural Network currently has two layers--input
and output. But we can add layers between

00:03:00.390 --> 00:03:02.510
them.
So now the inputs are indirectly connected

00:03:02.510 --> 00:03:04.770
to the output, through the middle layer of
nodes.

00:03:04.770 --> 00:03:08.600
It’s pretty clear what the input nodes are,
since they’re values we understand. And

00:03:08.600 --> 00:03:12.810
the output node is a salary, so we get that
too. But it can be harder to grasp exactly

00:03:12.810 --> 00:03:14.760
what the middle layers represent.

00:03:14.760 --> 00:03:18.580
You can think of all the calculations that
happen between the input nodes and output

00:03:18.580 --> 00:03:23.570
nodes as something called “feature generation”.
“Feature” is just a fancy word for a variable

00:03:23.570 --> 00:03:26.760
that can be made up of a combination of other
variables.

00:03:26.760 --> 00:03:30.560
For example we could use your grades, attendance,
and test scores to create a “Feature”

00:03:30.560 --> 00:03:34.900
called Academic Performance. Essentially the
neural network is taking the variables we

00:03:34.900 --> 00:03:40.720
give it, and performing combinations and calculations
to create new values, or “features”.

00:03:40.720 --> 00:03:44.320
Then, it combines those “features” to
create an output.

00:03:44.320 --> 00:03:48.380
When we have large amounts of complex data,
the neural network saves us a LOT of time

00:03:48.380 --> 00:03:52.890
by combining variables and figuring out which
ones are important. Neural Networks allow

00:03:52.890 --> 00:03:57.490
us to make use of data that might seem too
big and overwhelming for us to try to use

00:03:57.490 --> 00:04:02.020
on its own. They can find patterns that humans
might never be able to see.

00:04:02.020 --> 00:04:05.910
If a neural network has more than one layer,
we say that we’re using “Deep Learning”,

00:04:05.910 --> 00:04:10.760
since there are many layers of nodes. Deep
Learning has gained popularity in recent years.

00:04:10.760 --> 00:04:14.670
Neural networks and deep learning have been
used extensively to do things like recognize

00:04:14.670 --> 00:04:20.150
handwritten numbers and simulate x-ray images
so airport security can be trained to recognize

00:04:20.150 --> 00:04:21.919
items like drugs and guns.

00:04:21.919 --> 00:04:25.889
There’s a lot more math that goes into neural
networks . But in short, they learn by figuring

00:04:25.889 --> 00:04:30.960
out what they got wrong, and then working
backwards to determine what values and connections

00:04:30.960 --> 00:04:32.600
made the output incorrect.

00:04:32.600 --> 00:04:37.289
For example, if it predicted my salary and
is $10,000 off, it will take that difference

00:04:37.289 --> 00:04:42.830
and figure out which parts of the neural network
were influential in creating that $10,000

00:04:42.830 --> 00:04:46.460
error. It then tweaks them so that next time,
it’s not as wrong.

00:04:46.460 --> 00:04:51.360
You can see that in this neural network--sometimes
called a Feed Forward Neural Network--all

00:04:51.360 --> 00:04:56.569
the nodes only feed into the next layer from
input to output. Hence, they only Feed information

00:04:56.569 --> 00:04:57.569
Forward.

00:04:57.569 --> 00:05:01.650
But it is possible to feed the output of a
Neural Net back into the model as an input

00:05:01.650 --> 00:05:05.449
the next time you run it. In other words,
nodes in one layer can be connected to each

00:05:05.449 --> 00:05:10.160
other, even themselves! These types of Neural
Networks are called Recurrent Neural Networks.

00:05:10.160 --> 00:05:14.830
We can use RNNs to learn patterns. For example,
words! RNNs have been used to spell check

00:05:14.830 --> 00:05:19.979
text. The Network can learn to take in a misspelled
word like this... and correct it.

00:05:19.979 --> 00:05:24.379
Often we use this kind of network when we
have sequential data-- like stock prices over

00:05:24.379 --> 00:05:28.479
time, or the words in a sentence. If you’re
trying to predict the words in a sentence,

00:05:28.479 --> 00:05:31.029
it matters a lot what the previous word was.

00:05:31.029 --> 00:05:35.189
If the previous word was “A”, that influences
what the current word is. Usually the word

00:05:35.189 --> 00:05:39.630
“A” precedes a noun, or an adjective -- one
that starts with an consonant. A Fox. A Quick,

00:05:39.630 --> 00:05:44.819
Brown Fox. But it’s unlikely to precede
a verb. “A walked” wouldn’t make sense.

00:05:44.819 --> 00:05:50.319
But the further you get through the sentence,
the less influence the word “A” has.

00:05:50.319 --> 00:05:54.909
Unlike Feed Forward Neural Networks, Recurrent
Neural Networks “ remember “ the previous

00:05:54.909 --> 00:05:59.599
outputs. For example, if we used a Recurrent
Neural Networks to generate a melody, we would

00:05:59.599 --> 00:06:04.430
give the network some information about our
song framework, and we’d ask it for a note.

00:06:04.430 --> 00:06:08.199
Then we feed that note back into the model
along with the information about our song

00:06:08.199 --> 00:06:10.749
framework and the network would generate the
next note.

00:06:10.749 --> 00:06:15.249
In order to make a melody that sounds good,
the Recurrent Neural Network needs to “remember”

00:06:15.249 --> 00:06:19.530
what the previous notes were. Using the outputs
as inputs allows us to do that.

00:06:19.530 --> 00:06:24.159
A popular type of Recurrent Neural Network
called a Long Short-Term Memory Network has

00:06:24.159 --> 00:06:29.099
been used to generate all kinds of music.
It’s even been used to write a few new Harry

00:06:29.099 --> 00:06:30.099
Potter chapters.

00:06:30.100 --> 00:06:36.500
ahem Here is one of those chapters from a
Recurrent Neural Network trained by Max Deutsch…

00:06:36.500 --> 00:06:38.559
“The Malfoys!” said Hermione.

00:06:38.559 --> 00:06:43.279
Harry was watching him. He looked like Madame
Maxime. When she strode up the wrong staircase

00:06:43.279 --> 00:06:44.469
to visit himself.

00:06:44.469 --> 00:06:49.939
“I’m afraid I’ve definitely been suspended
from power, no chance — indeed?” said

00:06:49.939 --> 00:06:50.939
Snape.

00:06:50.939 --> 00:06:54.999
He put his head back behind them and read
groups as they crossed a corner and fluttered

00:06:54.999 --> 00:06:58.689
down onto their ink lamp, and picked up his
spoon.

00:06:58.689 --> 00:07:02.189
The doorbell rang. It was a lot cleaner down
in London.

00:07:02.189 --> 00:07:06.729
So, J.K. Rowling isn’t out of a job yet.
This excerpt doesn’t make sense within the

00:07:06.729 --> 00:07:11.919
context of the Harry Potter universe, or really
make sense at all. But it at least has the

00:07:11.919 --> 00:07:13.539
structure of a book chapter.

00:07:13.539 --> 00:07:18.339
We can also use Neural Networks to look at
another form of art: images. A lot of applications

00:07:18.339 --> 00:07:24.169
of image recognition use a type of Neural
Network called a Convolutional Neural Network.

00:07:24.169 --> 00:07:25.809
Images are made up of a grid of pixels.

00:07:25.809 --> 00:07:30.869
A very tiny grayscale image like this could
be represented by a grid like this ...where

00:07:30.869 --> 00:07:36.320
each number represents how much black is in
that pixel. 0 is complete black, 1 is complete

00:07:36.320 --> 00:07:38.410
white, and anything in between is a shade
of gray.

00:07:38.410 --> 00:07:42.550
Color images are a little more complicated,
since each pixel has a red, green, and blue

00:07:42.550 --> 00:07:44.080
value, but the idea is similar.

00:07:44.080 --> 00:07:48.689
In this case, a pixel is affected by all the
pixels surrounding it. It’s not simple sequential

00:07:48.689 --> 00:07:53.770
data. So, convolutional neural networks look
at “windows” of pixels instead of one

00:07:53.770 --> 00:07:54.849
pixel at a time.

00:07:54.849 --> 00:07:59.919
They apply a filter to these windows to create
“features”. This step is called convolution.

00:07:59.919 --> 00:08:04.639
The filters that the network uses are just
calculations that transform the pixels that

00:08:04.639 --> 00:08:09.939
are inside the window. The network uses the
data to determine which windows and filters

00:08:09.939 --> 00:08:11.020
will be used.

00:08:11.020 --> 00:08:13.610
Some filters might help detect edges in the
image

00:08:13.610 --> 00:08:18.090
Others might recognize features like curves,
horizontal lines, or even more complex objects

00:08:18.090 --> 00:08:22.279
like eyes, or faces. These features make it
so we can take an image...which has a huge

00:08:22.279 --> 00:08:25.860
number of pixels...and make a smaller number
of features.

00:08:25.860 --> 00:08:29.969
This process is called pooling. In the end,
the network will use the features generated

00:08:29.969 --> 00:08:34.750
by convolution and pooling to give us some
kind of output, like a decision about whether

00:08:34.750 --> 00:08:37.580
or not an image contains a stop sign, or a
human face.

00:08:37.580 --> 00:08:42.030
Snapchat, for example, has used variations
of convolutional neural networks in their

00:08:42.030 --> 00:08:46.690
app. And these networks are used extensively
in all kinds of image recognition.

00:08:46.690 --> 00:08:51.220
If you hate those CAPTCHAs that ask you to
click on each image that has a stop sign,

00:08:51.220 --> 00:08:54.510
you could use a convolutional neural network
to fill them out for you.

00:08:54.510 --> 00:08:57.880
And the next time you’re in another country,
you can use Google’s Translate app which

00:08:57.880 --> 00:09:03.320
uses these networks to help translate the
text from signs or menus into your language.

00:09:03.320 --> 00:09:07.470
One thing that limits our use of neural networks
of all kinds is a lack of data. The more complex

00:09:07.470 --> 00:09:10.590
these networks are, the more data they need
to perform well.

00:09:10.590 --> 00:09:14.320
But some neural networks can be trained to
generate data. These are called Generative

00:09:14.320 --> 00:09:19.160
Adversarial Networks (GANs). They use sets
of existing data to try to learn how to create

00:09:19.160 --> 00:09:24.670
new data. These networks are kinda like two
neural networks...disguised as one...by wearing

00:09:24.670 --> 00:09:25.670
a trenchcoat.

00:09:25.670 --> 00:09:29.310
We’ll illustrate how they work, with an
analogy. Let’s say you’re a counterfeiter

00:09:29.310 --> 00:09:33.830
who’s trying to make fake $100 bills. You
examine a few $100 bills create a fake

00:09:33.830 --> 00:09:38.900
and then try to use it at your local convenience
store. If the bill is rejected, you politely

00:09:38.900 --> 00:09:43.540
ask the cashier what made them realize the
bill was fake. And they’re happy to help.

00:09:43.540 --> 00:09:48.410
They tell you, you take this information back
to your counterfeiting lab, and make a new,

00:09:48.410 --> 00:09:50.230
better fake $100 bill.

00:09:50.230 --> 00:09:54.710
You repeat this process over and over--hopefully
the cashiers don’t start to recognize you...and

00:09:54.710 --> 00:09:58.720
eventually, you should have a passable fake
bill. (Assuming you aren’t already in jail.)

00:09:58.720 --> 00:10:04.350
However, since the cashiers are seeing so
many fake bills, they get better at recognizing

00:10:04.350 --> 00:10:05.820
them as time goes on.

00:10:05.820 --> 00:10:10.990
In our analogy, you are the generator. Your
job is to make fake input...in this case $100

00:10:10.990 --> 00:10:15.230
bills that are good enough to “trick”
the convenience store. The cashier is the

00:10:15.230 --> 00:10:20.670
discriminator since her job is to learn to
discriminate between real and fake $100 bills.

00:10:20.670 --> 00:10:25.990
Essentially you have two neural networks battling
it out to create better and better outputs.

00:10:25.990 --> 00:10:30.340
The generator is trying to get better and
better at making data that can trick the discriminator.

00:10:30.340 --> 00:10:34.990
And the discriminator is trying to learn how
to best discriminate between fake and real

00:10:34.990 --> 00:10:35.990
data.

00:10:35.990 --> 00:10:39.700
These networks have been used to create new
anime characters , make new Van Gogh-like

00:10:39.700 --> 00:10:41.680
art, and create new skate decks

00:10:41.680 --> 00:10:45.990
Neural Networks of all kinds help us deal
with the big, sometimes messy data that we

00:10:45.990 --> 00:10:49.930
have in real life. They help detect patterns
in data that humans can’t see.

00:10:49.930 --> 00:10:54.410
And often, they’re way better than we are
at figuring out how to turn input variables

00:10:54.410 --> 00:10:58.990
into accurate outputs. While you and I don’t
have time to look through Terabytes of data,

00:10:58.990 --> 00:11:03.450
a Neural Network does. With neural networks
we can make use of data that may have been

00:11:03.450 --> 00:11:05.230
too overwhelming without them.

00:11:05.230 --> 00:11:08.150
We’ve talked about Neural Networks being
applied to a lot of different things, like

00:11:08.150 --> 00:11:13.630
image and face recognition. That type of technology
could one day be utilized in drones on search-and-rescue

00:11:13.630 --> 00:11:14.630
missions.

00:11:14.630 --> 00:11:18.600
Natural Language Processing...often using
neural networks...is the reason we have Alexa.

00:11:18.600 --> 00:11:23.550
Also, hey Google, hey Siri,
hey Cortana, hey Bixbi. I think I got everybody.

00:11:23.550 --> 00:11:27.700
As data gets bigger and bigger, neural networks
will likely become a more common tool for

00:11:27.700 --> 00:11:31.830
making the most of our data, and understanding
how they work allows us to think of really

00:11:31.840 --> 00:11:36.640
creative ways to use them. Hopefully, less annoying ways.  Thanks for watching, I’ll see you next time.

