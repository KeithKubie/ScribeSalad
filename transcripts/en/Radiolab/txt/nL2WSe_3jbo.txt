Speaker 1:          00:00:00       You are listening to radio lab radio from W. N y. S. T. E. N. N. P. R. Hi

Speaker 2:          00:00:11       there. We're going to start today's program with a fellow named Robert. Is it Epstein or Epstein? Uh, just think Einstein with an app. Okay. That would make it Epstein, I guess that's right. And where are we reaching you right now? I am in a San Diego area. Robert Epstein is a psychologist or editor in chief of psychology today magazine.

Speaker 1:          00:00:31       He's written a ton of books on relationships and love, and he also happens to be one of the world's leading researchers in computer human interactions, like artificial intelligence, basically. That is correct. Yes ma'am. So when did you decide to go onto the computer to get a date? 2006 maybe. Why do you ask? Oh, no reason. No. What happened? You, you were, you had gotten divorced.

Speaker 2:          00:01:04       Yeah, I was single at the time. Yeah, I was divorced.

Speaker 1:          00:01:08       You decided that you'd try loving all the right places or what?

Speaker 2:          00:01:11       Oh, sure. Bill online dating and everyone was doing it. My cousin actually convinced me to try it, so I did. And I went online and I looked at photos and I looked at profiles and uh, you know, and I communicated with various people who were willing to talk to me. And one of the women I was communicating with, uh, lived in southern California where I do so I thought that's great cause you know, you want someone to be nearby. And she had a very attractive photo online and her English was poor, which at first bothered me. And then she said, well she's not really in California. She's really in Russia. Oh. But all four of my grandparents came from Russia. So I thought, well I'll go with it. So I continued to write to her.

Speaker 1:          00:02:00       Hi Sweet Svetlana. It's very warm here now and I've been doing a lot of swimming. I've also been riding doing them. Peter,

Speaker 3:          00:02:07       oh Brantley. She wrote back to me in very poor English. Hello to enroll. Perfect. Do you mind? I hate procedure later. I am theory pepe. I remember that she liked to walk in park until invoke with the girlfriend and the event involved in part telling me about her family. And her mom asked me about youtube. NBC spoke much and long time. They lived in a small apartment. I knew where in Russia they lived. It felt like we were bonding for sure.

Speaker 1:          00:02:35       Hello. I might be able to come to Moscow on Sunday, April 15th departing Thursday, April 19th with love Robert. So it was getting serious.

Speaker 2:          00:02:45       Oh yeah, of course.

Speaker 3:          00:02:47       Then what happened? Well, two months passed and I began to feel uncomfortable. Something wasn't right. There were no phone calls. [inaudible] some point I began to suggest phone call,

Speaker 2:          00:02:59       but there weren't any, but the main problem was I would say something like, did you get my letter about me coming to Moscow in April or tell me more about this friend of yours that you mentioned and she did not mine. I'm very glad to your letter. She did not, she was still replying with fairly long emails, vetted my CT very bad, but they were kind of rambling in general. I think of you all days much and I very much want to see more likely you. I already gave you some dates of our visit to Moscow. My Love. What do you think about that? Then at some point a little bell went off in my head finally and I started to send some emails, which uh, let's say included random alphabet letters. Where's it? So you say how, what are you wearing tonight? Are you wearing a D, B, g, G, G, G, l.

Speaker 2:          00:03:53       P? Exactly. And it didn't make any difference. Halo, Deb, Robert, your letters do mean very happy when I open a little box and that's when I realized Ivana was not a person. Ivana was a computer program I had been had. Wow. So what did you think? I felt like a fool. I felt like an incredible fool, especially given my background that I had been full that long. Now I can tell you now this is, this is something I've never made public about. The other example, Robert went on to tell us that not long after that first incident, he was corresponding with someone with a woman I thought, who also turned out to be a robot. And he discovered at this time because programmer contacted me from the UK and said, I know who you are. You have not been communicating with a person you've been communicating with the Chat Bot, you've been now undressed twice by robots so to speak. Well and maybe more than twice. Well, how common do you think this is? Do you think that match.com and all those places are like swarming with these bots? You know, you know, I bet you they are.

Speaker 4:          00:05:10       That's what you have to understand. There are hundreds of these things out there. There might be thousands. That's what's coming through in a world like this. We are surrounded by artists for short life. Things can get a little confusing.

Speaker 2:          00:05:29       In fact, we're going to do a whole show about that confusion about the sometimes peculiar, sometimes strange things that can happen when humans and machines collide. Collide, but don't quite know who's on what side of the road. Yeah, I don't know. Jab. Boom. Ryan, that was good. That was good. Just go with it. Okay. I'm Robert Krulwich. This is radio lab and we are talking to machines.

Speaker 4:          00:05:53       Send me your credit card. [inaudible]

Speaker 5:          00:06:05       so keep talking. Just start things off. Let's introduce you to the person who really hooked us on this whole idea of human robot Chit Chat. My name is Brian Christian. He's a writer. Are you Christian religiously? No Man. That's not at all related to anything that's wrong with you. What's his name but didn't know what's important is that he wrote a book called the most human human, which is all about uh, the confusing things that can happen when people and machines interact. How did you, this is such a curious thing to get. Yeah. How did you get into this? I played with Ms dos intently when I was a child. Yeah, there you go. Yeah. Dos is kind of the early version of windows. I was programming these sort of rudimentary maze games, like a curse or going through maze. Yeah. Basically did this by any chance mean you did not develop best friends?

Speaker 5:          00:06:51       A lot of my best friends were also into that. Wow. We are not the coolest, but we had a lot of fun. So there you are and you just had a, you just have a talent for this. I don't know what it was. I mean I was just, there was something I think fascinating to me that you could take a process that you knew how to do, but in breaking it down to steps that were that explicit, you often learned something about how the process actually works. For me, programming is surprisingly linked to introspection. How exactly, well you know, if a computer were a person, you can imagine someone sitting in your living room and you say, you know, can you hand me that book? And it would say, no I can't do that. Cause there's a coffee cup on it. And you say, okay well pick up the Coffee Cup and hand me the book.

Speaker 5:          00:07:36       And it says, well I can't do that cause I'm now I'm holding the cup. And he said, okay, put down the Cup then pick up the book and what you quit we learn says Brian is it even really simple human behaviors are made up of a thousand sub routines. I mean if you really think about it, the book task requires knowing what is a book you have to learn how to go about elbows and wrists had to grab something. What is a book? I already said that, oh, you need to know about gravity. If it's a machine, you have to teach it physics, everything in the world in order for it to just pick up

Speaker 6:          00:08:08       spoon or buck. Thank you.

Speaker 5:          00:08:11       So now thinking of, that's fell on a bot earlier. Okay. Trying to make something that could actually mimic human conversation. Kind of sorta imagine all this stuff you'd have to throw into that. Okay. English grammar, syntax and context, tone, mood, sarcasm, hire knee adverbs, turn-taking. Oh, well, it's not actually as impossible as you'd imagine. This is kind of startling. If you go back to the very early days of software programming in the mid sixties 1964 1965 this was actually done with a little program called Eliza and it was developed by Joseph Weizenbaum at MIT. What in Weizenbaums case, his model was not a Russian hottie instead, it was a non-directive Rogerian. The one, it's, it's a particular school of therapy. The kind where the therapist basically mirrors what you're saying. What you're saying, like you're saying, this is Sherry Turkle. She's an anthropologist at the Massachusetts Institute of Technology and she worked with Joey's and Bam, or is it Weizenbaum?

Speaker 5:          00:09:11       It's Weizenbaum at MIT. So if you say, you know, I, I'm feeling depressed. A therapist says, I'm sorry to hear you're feeling depressed, tell me more. Um, Joseph Weizenbaum decides, you know, I think that's an easy enough type of conversation that I can program that into my computer. And so he writes up a simple little program, just about a hundred lines of code, which does sort of what your therapist does, where it looks for a keyword and what you're saying as an, I'm feeling depressed, keyword, depressed, latches onto it, and then basically flips it back to you. I'm sorry to hear that you're feeling keyword depressed, right? It's basically a program that inverts your words and it's, so, it's a language game. So here's what he did. He put a computer on a desk running this little program, and then he invited people to sit down and talk to it.

Speaker 5:          00:09:55       They'd write in something and they would spit back the keyword and then you would then repeat and on and on. You wouldn't think this would be very interesting, right? Nope. But he found that his students, one who knew that the program didn't know or understand who had helped him write the program. Sophisticated students nevertheless want it to be alone with it. Wow. They actually help write the program. Yes. I can read you if you'd like one of the conversations. Yes, please. This is an actual transcript between Eliza and a young woman. Yeah. Men are all alike. In what way? They're always bugging us about something or other. Can you think of a specific, well, my boyfriend made me come here. Your boyfriend made you come here. I'm depressed much at the time. I'm so sorry to hear you are depressed. True. It's true. I'm unhappy. Do you think coming here will help you not to be unhappy? I need some help that much team. Certain. What would it mean to you if you got some help? Perhaps I could learn to count along with my mother. Tell me more about your family. Wow. And so it's, this VAT could go on for hours. Well, and in fact it did. A mother takes care of it.

Speaker 7:          00:10:57       Males in your family takes care of you. My father, your father, you're like my father in some ways. Resemblance. Do you see, while you're not very aggressive, what makes you think I'm not very aggressive? You don't argue with me. Why do you think I don't argue with you? A are afraid of me leads you to believe that there's a woman in this transcript messing around. Or is she really pouring her heart out? We know that this is a woman who works for Joe Weizenbaum. She's sitting in the office and she just can't stop talking to it. That's all we know. What else comes to mind when you think of your father bullies and why your mom is watching all this and he, he first thought it was funny and then he didn't think it was funny because they were actually having conversations with it.

Speaker 5:          00:11:36       One day he comes into the office and his secretary is on the computer divulging her life story to it. According to Weizenbaum, she even told them to please leave the room so she could be alone with it and talk to it. And he, he, um, he was very upset.

Speaker 8:          00:11:54       Nevertheless,

Speaker 1:          00:11:55       when word about Eliza got out,

Speaker 5:          00:11:57       the medical community sort of latches onto it really and says, oh, this is going to be the next revolution in therapy,

Speaker 9:          00:12:04       something new and promising in the field of psychotherapy.

Speaker 5:          00:12:07       This is from a newscast around that therapists in like phone booths in cities and you're going to walk in and put a quarter in the slot and have, you know, half an hour of therapy with this automatic

Speaker 9:          00:12:18       programmed time can be renting for $5 an hour. And there's every reason to suspect that it will go down.

Speaker 5:          00:12:24       People really thought that they were going to replace therapists with computers. Absolutely. Really? Absolutely. Yeah. And it was just this really appalling moment for why isn't bomb of, there's something, the genie is out of the bottle, maybe in a, in a bad way. And he does this one f his entire career. So he pulls the plug on the program, he cuts the funding and it goes from being one of the main advocates for artificial intelligence to basically committing the rest of his career to fighting against artificial intelligence.

Speaker 1:          00:13:00       Rude to Yvette in sign in for cove. [inaudible] Sherbet. Speco this is Joseph Weizenbaum interviewed in German just before he died in 2008 it was on the German documentary plug and pray and mine. Cal Nine one is my main objection. He's a venn dusting. Yes. The thing says, I understand that if somebody typed in something and the machine says, I understand that is [inaudible] there's no one there as a, it's just a Heiner Lugar so it's a lie. [inaudible] when each can be a niche flush. Dan, and I can't imagine that people who are emotionally imbalanced could be effectively treated by systematically maligned, unmoved.

Speaker 8:          00:13:42       I must say that my reaction to the Eliza program at the time was to try to reassure him. At the time what I thought people were doing was using it as a kind of interactive diary, knowing that it was a machine, but using it as an occasion to breathe life into it in order to get their feelings out.

Speaker 7:          00:14:08       I think she's right to have said that to him too. Yeah, because he says it's a lie. Well, it is a lie. And how is it love? Doesn't machine can't love anything. Yes. And if you are a sensible human being, you know that and he's sitting right there on the desk. It's not pretending well these are sensible human beings that were already a little bit seduced. Let Matt just go forward a hundred years. Imagine a machine that is very sophisticated, very fluent, very convincingly human. You're talking about blade runner basically. Yeah, exactly. At that point I think I would require some kind of label to remind me that this is a thing.

Speaker 1:          00:14:44       It's not a being, it's just a thing. Okay. But if here's something to think about, if the machines get to that point, which is a big, where you

Speaker 5:          00:14:52       would want to label them. Yeah. We are going to need a way to know when they've crossed that line and become mind full. Yeah. So I should back up for a sec and say that in 1950, they're, they're just starting to develop the computer and they're already asking these philosophical questions like, can these machines think, you know, will be someday be able to make a machine that could think, uh, and if we did, how would we know? And so a British mathematician named Alan Turing proposed a simple thought experiment. Here's how we'll know when the machines make it across the line. Get a person, sit them down at a computer, have them start a conversation in text. Hi, how are you? Enter good. Pops up on the screen. Sort of like Internet chat. Yep. So after that first conversation, have them do it again and then again, you know, hi, hello, how are you?

Speaker 5:          00:15:40       Et cetera. Back and forth. And again, right over and over. But here's the catch. Half of these conversations will be with real people. Half will be with these computer programs that are basically impersonating people. And the person in the seat, the human has to judge which of the conversations were with people, which were with humans. Turing's idea was that if those computer fakes could fool the human judge a certain percentage of the time Turing's magic threshold was 30% then at that point we can basically consider machines intelligent. Cause you know, if you can't tell the machine isn't human, then you can't say it's not intelligent. Yeah. That's basically, yeah. You said 30% of the time. Yeah. Turing is a natural number to me would be half. You know, 51% would seem to be like the coaching moment, 30% I don't know. Well 51% is actually a horrifying number in the context of the turning test because you've got these two conversations and you're trying to decide which is the real person. So if the computer were indistinguishable, that would be 50% you know, the judge is doing no better than chance. So if a computer hits 51% that means there they've out human to the human million. That is horrifying.

Speaker 5:          00:16:48       Something to keep in mind when throwing thought this whole thing up. The technology was so new. Computers barely existed that it was sort of a, a leap of imagination really. But no longer Robert Bridge. Can you give me like some excitement music here?

Speaker 10:         00:17:01       Absolutely good because every year the greatest technologist on the planet named small on embar. Hi, I'm really copping to meet in a small room with folding chairs I developed in Java and put out in touring's questions to the ultimate test.

Speaker 5:          00:17:23       Really it's just a couple of dudes you know who haven't seen the sun in 10 years in a room, but we do now have this thing called the lobe ner prize, which is essentially a year elite actual touring test.

Speaker 10:         00:17:35       Each judge on our judges table is going to be communicating with two entities, one human and one trainer.

Speaker 5:          00:17:42       The way the stage is set up is you've got the judges at a table on the left on laptops, then a bunch of giant server looking machines in the middle of that programmers are fiddling with and then there's a curtain on the right hand side and we're behind the curtain. Brian actually participated in the 2009 lobe, no prize competition but not as a programmer as one of the four quote, confederates. The confederates are the real people that the judges are talking to cause you remember half the conversations that have are with people. Half are with computers and then Brian decided to participate that year because the year before 2008 the top program minister fool, 25% of the judging panel pretty close to Turing's number. Exactly one vote away and so I felt to some extent how can I get involved on behalf of humanity? How can I up take a scan that's a modest position for you.

Speaker 11:         00:18:36       All right machines please hold your vices are now representing all humans.

Speaker 5:          00:18:42       Brian, Chris Jones. Now in terms of what Brian is up against, the computer programs have a a variety of different strategies. For example, there was one program Brian's year that would do kind of a double fake out [inaudible] where it would pretend not to be a person but a person who is sarcastically pretending to be a robot. Oh people would ask it a simple question and it would say I don't have enough ram to answer that question. Smiley face and everyone would be like, oh, this is such a wise guy. Ha Ha clips. I want to tell you now about one particular Bot that competed. Brian's here. Hi, I'm rolling. Carpenter. That's the guy who made it. My program is called clever bots and that's the BOT. This is a program that employs a very spooky, spooky, the right route, a very spooky strategy.

Speaker 11:         00:19:28       You may be surprised to hear that despite the fact that it's called Clever Bot, it states that it is a Bot. It states that it is never a human right there in front of them. Despite those facts. I received several emails a day from people who believe that actually they are being connected to humans.

Speaker 12:         00:19:47       [inaudible]

Speaker 11:         00:19:48       oh, like they think they've been tricked. Yes. Tricked into coming to a site that claims to be a Bot when in fact they're talking to humans.

Speaker 12:         00:19:56       [inaudible]

Speaker 11:         00:19:56       but no, no program could possibly respond in this way and there is a certain element of truth in that to explain Rolo carb material, like Brian was one of those kids who is completely obsessed by computers. I was indeed a computer, a kid. And when he was just a teenager age about 16 or so, wrote his first Chat Bot, I created a program that talked to me. No kidding. Yes. You typed in something and it would say something back though. At that time the responses were essentially pre-program and really simple. Kind of like Eliza. But one evening, I think fast forward many years he is in his apartment and one night he says a switch suddenly flipped in my, in my mind

Speaker 12:         00:20:35       [inaudible]

Speaker 11:         00:20:36       Nice. Had any sore, how to make the machine learn on its own. What if you thought, what if it just started at zero, like a little baby and it would grow in these discrete little increments every time you talk to it? Right. Basically, uh, w the first thing that was said to that program that I created, the first version of that night, um, was said back by it. Meaning if he said to at hello it now knew one thing, the word hello. So it would say hello back. The second thing it said was a choice of the first two things that to it. So if the second thing you said was how are you doing it now? New Two things. The where, hello and the phrase, how are you doing? So it could either say hello back again or how are you doing? The third thing it said was a choice of the first three things and so on. Ad Infinitum will not quite ad infinitum. But between 1988 and 1997 a few thousand conversations took place between myself in it and a few of my friends and it, he and his friends would sit there and type things to it as a way of teaching it, new things, but it was just them. So it was slow going. So it languished for quite a long time. But then I started working with the Internet. Put your own line where anyone could talk to it. Within the next 10 years, it had learned something like 5 million lines of conversation.

Speaker 11:         00:22:01       Now [inaudible] is frequently handling around 200,000 requests an outlaw, and it's talking to more than 3 million people a month, 3 million conversations a month. And after each one [inaudible] knows a little bit more than it did before and every time you say something to it like, hey, clever, but why am I so sad? It is accessing the conversations that millions of people have had in the past asking itself, what is the best overlap? Where is the best correlation? How do people usually answer this question? Why am I so sad? And then I response clever about answers just because, hmm. All right. Well why? There must be a reason why I'm so sad because you have been sitting in the same place for too long. Is that WHO's title? Who's saying that? Exactly? Where does that response come from? And the answer is it is one human being at some point in the past.

Speaker 11:         00:23:08       Having said that, so that is one moment of human conversation from one person. Yes. So it's like I'm talking to a ghost. You are talking to it's intelligence if you like, is borrowed from millions of people in the past. I'm a little bit of that conversational knowledge that conversational intelligence goes into forming your reply. Now, what's interesting says Rolo is that when you start a conversation with clever Bot, it doesn't really have a personality or no one personality ever bought is everything to everyone. It's just this big hive really. I have seen it. But as you keep talking to it and it's sort of pulling forward from the high of these little ghost fragments of

Speaker 5:          00:23:45       past conversations, stitching them together, a form does kind of emerge

Speaker 11:         00:23:50       jet, uh, it reflects the person that is speaking to. It becomes somewhat like that person. Someone familiar already. People have very emotional conversations with it. People have complete arguments with it and um, of course they try to, um, to get it into bed by talking dirty to it. Yeah. One thing I can tell you is that I, I have seen a single person, a teenage girl, I'm speaking for 11 hours with just a three 15 minute breaks. Whoa.

Speaker 13:         00:24:24       [inaudible]

Speaker 11:         00:24:24       about what everything. The day will come not too far down the road where clever Bot becomes so interesting to talk to that people will be talking to it all day every day,

Speaker 5:          00:24:39       but we're not there yet because the same thing that makes clever bots so interesting to talk to also can make it kind of ridiculous. For example, in our interview with Brian, he was the first person that turned us on to this program. As we were talking, Soren just sort of suggested, well, why don't we just try it right now. Try it. You want to talk? You want to tell it? Say to Clever Bot, I feel blue. Sure. Yeah. Are you pulling clever Bot up? Is it just clever bot.org or something? calm.com I feel. Can you say, I feel blue cause an asteroid hit my house this morning. And so this is, you've hit on a perfect strategy of a, of dealing with these bonds absurdity. Yes. Well, it basically saying something that has never been said before to clever Bot, so it's likely that no one has ever claimed an asteroid hit their house. It's a weird enough that it may not be in the database. Okay. Alright. Let's see what it says. Steroid hit my house this morning.

Speaker 13:         00:25:34       Clever bye.

Speaker 5:          00:25:38       I woke up at 1:00 PM this afternoon. Well, there we go. It's not quite so clever. You don't have to worry yet. Which in fact when I went online to youtube and watched the lobe new competition that Brian attended [inaudible] it turns out none of the computers fooled the judges at all. None. Any, well, I don't know if none, none, but they did really badly between the programs. For me, one of the strange takeaways of thinking so much about artificial intelligence is this feeling of how complex it is to sit across the table from someone and communicate with body language and tone and you know, rhythm and all of these things. What happens when those conversations are working out well is that we're willing to move the conversation in ways that allows us to be sort of perpetually startling to one another. That's a good word. Startling.

Speaker 13:         00:26:38       Yeah.

Speaker 5:          00:26:39       You learn someone through these small surprises.

Speaker 1:          00:26:51       Thanks to Brian Christian, his excellent book, which inspired this hour is called the most human human. Go to radiolab.org for more info. Thanks also to our actors, Sarah thyer and the Richter and Susan Blackwell.

Speaker 14:         00:27:03       Hi, this is Brian Christian radio lab is,

Speaker 15:         00:27:07       hello. I'm a machine radio lab is funded in part by the Alfred p Sloan Foundation enhancing public understanding of science and technology in the modern world.

Speaker 14:         00:27:19       More information aboutSloan@wwwdotsloan.org this is Sherry Turkle. Radiolab is produced by definitely NYC and distributed by NPR. Bye Bye.

Speaker 1:          00:27:32       Our country continues to grapple with the deep cultural divisions exposed by the 2016 elections. We be Fred Pogue. There is some palpable change in how people are looking at as well. Maybe A, we are looking at people. W NYC is talking to a broad range of Americans about how we arrived at this moment in our political culture. I'm Kai Reich subscribed to the United States of anxiety, culture wars from wn YC studios. Wherever you get your pot tests. Hey, I'm Jad Abumrad. I'm Robert Krulwich. This is radio lab and we are exploring the blur that takes place when humans and machines interact and investigate each other. Talk to each other. Yeah. See that's the thing. In the last act, we were always talking, talking, talking, talking. How about we encountered machines in a different way. How about we? No talking, no talking. We touched them illegally. We pet them, we sniff them. We you. We do essential things that don't involve the sophisticated business of conversation. Okay. This is freedom Bayer. Yes, it is. Who's not a machine? I don't think so. I'm Jad and this is, I'm Robert here. Hi there. Nice to meet both of you. We called her up because freedom actually had her own kind of moment with a [inaudible].

Speaker 4:          00:28:48       Yup. Yup. This was around 1999 freedom was a graduate student at the media lab at MIT. What were you doing there? We were developing cinema of the future, so we were working on creating a virtual characters that you can interact with. Anyhow, she was also thinking about becoming a mom. Yeah. I knew I wanted to be a mom someday and she decided to practice. I got two durables, twinkie and Ho-ho. So I had these two live pets and, and then she got herself a pet that was well not so alive. Yeah, I've got it right here. And you're not going against the mic so we can hear it. Say hello to. Yeah. Yeah, there is. I Furby at that time, the first furbies were hot and happening. Can you describe a Furby for those of us who share? It's about five inches tall and the Furby is pretty much all head. It's just a big round, fluffy head with two little feet sticking out the front. It has big eyes. Apparently it makes noises. Yeah. If you tickle its tummy, it will

Speaker 13:         00:29:55       [inaudible].

Speaker 4:          00:29:55       It would say this me and it would want you to just keep playing with it. So, you know, I spent about 10 weeks using the Furby. I would carry it around in my bag and one day she's hanging out with her baby and she notices something very eerie.

Speaker 13:         00:30:18       [inaudible].

Speaker 4:          00:30:18       What I discovered is if you hold it upside down, it will say, Hey, nice,

Speaker 16:         00:30:26       scared, [inaudible], scared, scared, and me as the, you know, the sort of owner slash user of this Furby would get really uncomfortable with that and then turn it back on upright.

Speaker 4:          00:30:40       Cause once you have an upright, it's, it's fine. It goes right back and then it's fine. So it's got some sensor in it that knows you know what direction it's facing or maybe it's just scared. [inaudible] sorry. Anyway, Lou was, she thought, well wait a second. Now this could be sort of a new way that you could use to draw the line between what's human and what's machine. Yeah. Kind of this kind of emotional Turing test. Can you guys hear me? Yes, I can hear you. If we actually wanted to do this task, could you help? How would we do it? Exactly? How are you guys doing? Yeah, you would need a group of kids. Can you guys tell me Your name? I'm Olivia [inaudible] and I'm SAIDI. Alright. I'm thinking six, seven and eight year olds. How old are you guys? Seven. The age of reason, you know, eight then says freedom.

Speaker 4:          00:31:31       We're going to need three things. Furby of course, Barbie, Barbie doll and jurby and securable. A real general. Yeah, and we did find one separate. Turned out to be a hamster. Sorry, you're a hamster, but we're going to call you Jeremy. So you've got Barbie, Furby, Gerbi, Barbie, Furby and [inaudible]. Right? Should we just second, what question are we asking in this test? The question was how long can you keep it upside down before you yourself feel uncomfortable? So we should time the kids as they hold each one upside down. Yeah, including the German. Yeah. You're going to have a Barbie. That's a doll. You're going to Jeremy, which is alive now. Where would Furby fall in terms of time held upside down? Would it be closer to the living thing or to the doll that, I mean, that was really the question. Phase one. Okay, so here's what we're going to do. It's going to be really simple. You would have to say, well, here's a Barbie. Do you guys play with barbies? Just do a couple things. A few things with Barbie. Barbie's walking, looking at the flower, and then hold Barbie upside down.

Speaker 4:          00:32:30       Okay. Let's see how long you can hold Barbie like that. I could probably do it obviously very long. Let's just see whenever you feel like you want to turn it around. I'm happy.

Speaker 13:         00:32:50       [inaudible]

Speaker 4:          00:32:50       this went on forever, so let's just fast forward a bit. Gay. And so what we learned here in phase one is the not surprising fact that kids can hold Barbie dolls upside down. Yeah. Really was forever. It could have been longer, but their arms got tired. Alright, so that was the first task time for phase two. Do the same thing with Derby. So out with Barbie in with Derby, are we going to have to hold them upside down? That's the test. Yeah. So which one of you would like to, okay. Ready? Oh God. You have to hold Gerbi kind of firmly by the way. No rodents were harmed in this whole situation. Yeah, she is pretty screaming to be upside down. Okay. Okay. So as you heard, uh, the kids turned derby over very fast. I just didn't get hurt on average eight seconds. I was thinking, oh my God. And it was a tortured eight seconds. Now phase three. So this is a first Louisa, you take Furby in your hand now can you turn Furby upside down? Hold her still.

Speaker 13:         00:34:44       [inaudible] whoa.

Speaker 4:          00:34:46       She just turned it over. So [inaudible] was eight seconds Barbie five to infinity. Furby turned out to be and freedom predicted this that a minute. In other words, the kids seem to treat this Furby, this toy more like a Gerbil than a Barbie doll. How come you turn them over so fast? I didn't want to be scared. Do you think he really felt scared? Yeah, I get it. Felt guilty. Really. It's a toil and all that, but still now, do you remember a time when you felt scared?

Speaker 17:         00:35:24       Yeah. Yeah.

Speaker 4:          00:35:25       You don't have to tell me about it, but if you can remember it in your mind,

Speaker 17:         00:35:28       I can do.

Speaker 4:          00:35:31       Do you think when Furby says me scared that furbies feeling the same way?

Speaker 17:         00:35:35       Yeah. No, no, no. Yeah, no. Yeah, sure.

Speaker 18:         00:35:42       I think that it can feel pain. Sort of the experience with the Furby seem to leave the kids kind of conflicted going in different directions at once. It was two thoughts, two thoughts at the same time. Yeah. One thought was like, look, I get it. It's a toy for crying out loud. But another thought was like, I'm still, he's helpless. It kind of made me feel guilty. It is sort of way heard. I made me feel like a coward, you know, when I was interacting with my Furby a lot,

Speaker 19:         00:36:14       I did have this feeling sometimes of having my chain yanked. Why would it, why would a, is it just the little squeals that it's fake or is there something about the toy that makes it good at this? That was kind of my question. So I called up a video as well. I'll have him, I'm here, this freight train of a guy. Hey. Okay. This is Jad from Radiolab. Chad from radio lab. God, how are you ma? I'm good. Beautiful Day here in Boise. This is Caleb Chung. Yaks he designed the PRP. Yeah, we're, we're all Furby crazy here. So, uh, there's medication you can take for that gate to start. Can you just give me the sort of fast cutting MTV montage of your life leading up to Furby? Sure. Um, hippie parents out of the house at 15 and a half, put myself through junior high, started my first business at 19 or something, early twenties being a street mime in La Street mind wow.

Speaker 19:         00:36:59       Became an actor at like 120 shows and an orangutan costume. Then I started working on special effects and building my own, taking those around a studios and let me in the suit, build a suit around me, put me on location. I could fix it when it broke. Wow. That was, anyhow, after a long and circuitous route, Caleb Chung eventually made it into toys. I answered an ad at Mattel, found himself in his garage and there's piles of styrene, plastic xacto knife, super glue, little mull Bucci Motors making these a little prototypes. Yeah. And the goal he says was always very simple. How do I get a kid to have this thing? Hang around with them for a long time? How do I get a kid to actually bond with it? Most toys you play for 15 minutes and then you put him in the corner until their batteries are dead.

Speaker 19:         00:37:36       I wanted something that they would play with for a long time. So how do you make that toy? Well, um, there's rules, there's, you know, the size of the eyes there is the distance of the top lid to the pupil. Right? You don't want any of the top of the white of your eye showing that. That's freaky surprise. Then when it came to the eyes, I had a choice with my one little mechanism. I can make the eyes go left or right or up and down. So it's up to you. You can make the eyes go left or right or up and down. Do you have a, do you have a reference or right, or up and down? I think I would choose left or right. Okay. So not sure why I say that, but that's all right. So let's, let's take that apart. Let's if you're talking to somebody and they look left or right while they're talking to you, what does that communicate?

Speaker 19:         00:38:18       Oh, shift D or they're, they're trying to find the person who's more important than you behind us. Okay. Now I wanna change my answer. Now I want to say up and down, you would, if you look at a baby and the way that a baby looks at their mother, they track from eyebrows to mouth. They track up and down on the face. So how'd you, how'd you made Furby look left and right rather than up and down? It would have probably flopped. No, it wouldn't have flopped. It would just sucked a little. It's like a bad actor who uses his arms too much. Yeah. You'd notice it and it would keep you from just being in the moment. But what is the thought behind that? Is it, is it, is it that you want to convince the child that the thing they're using is fill in the blank what you alive?

Speaker 19:         00:39:03       There's three elements I believe in, in creating something that feels to a human, like it's alive, like kind of rewrote Asimov's laws. The first is it has to feel and show emotions where you drawing on your mind days for that. Of course there's experiences in the park. Of course, you really break the body into part and you realize you can communicate physically. So if your chest goes up and your head goes up and your arms grew up, you know that's happy. If your head is forward and your chest is forward, you're kind of this angry guy. And he says, and when it came time to make Furby timid, he took that gesture language and focused it on furbies ear hears when they went up. That was surprised. And when they went down it was depression. So that's rule number one. The second rule is to be aware of themselves and their environment. So if there's a loud noise, it needs to know that there was a loud noise. So he gave the Furby little sensors so that if you go,

Speaker 4:          00:39:50       it'll say

Speaker 19:         00:39:54       the third thing is, uh, change over time. Their behaviors have to change over time. That's a really important thing. It's a very powerful thing that we don't expect, but when it happens we go, wow. And so one of the ways we showed that was acquiring human language.

Speaker 4:          00:40:07       Yeah. When you first get your Furby it doesn't speak English, it speaks furbish this kind of baby talk language and then the way it's programmed, it will sort of slowly over time replace its baby talk phrases with real English phrases. So you get the feeling that it's learning from you though. Of course it's not. No, it has no language comprehension. Right? So you've got these three rules, seal and show emotion, be aware of their environment, change over time. And oddly enough, they all seem to come together in that moment. You turn the Furby upside down because it seems to, no, it's upside down. So it's responding to its environment. It's definitely expressing emotion. And as you hold it there, what it's saying is changing over time because it starts with hey and then it goes to Meese and then it starts to cry. And all this adds up so that when you're holding the damn toy, even though you know it's just a toy, you still feel discomfort.

Speaker 1:          00:41:13       These creatures push our Darwinian buttons. That's professor Sherry Turkle again. And she says, if they push just enough of these buttons, then something curious happens. The machines slip across this very important

Speaker 19:         00:41:28       from what I call relationships of projection to relationships. And with

Speaker 8:          00:41:34       a doll, you project onto a doll what you need the doll to be. If a young girl is feeling guilty about breaking her mom's China, she puts her Barbie dolls in detention with robots. Um, you really engage with the robot as though they're a significant other as though they're a person.

Speaker 1:          00:41:53       So the robot is in your story. The robot is its own story or it's [inaudible].

Speaker 8:          00:41:58       And I think what we're forgetting as a culture is that there's nobody home. There's nobody home.

Speaker 19:         00:42:04       Well, um, I have to ask you, when is something alive Furby can remember these events, they effect what he does going forward and it changes his personality over time. He has all the attributes of fear or of happiness. And those are things that add up and change and changes behavior and how he interacts with the world, so how is that different than us? Wait a second though. Are you really going to go all the way there? Absolutely. This is a toy with servo motors and things that move its eyelids and w 200 words, so you're saying that life is a level of complexity. If something is alive, it's just more complex. I think I'm saying that life is driven by the need to be alive and by these base primal animal feelings like pain and suffering. I can code that. I can code that. What do you mean?

Speaker 19:         00:42:49       You can code that? Anyone who writes software and they do can say, okay, I need to stay alive. Therefore I'm going to come up with ways to stay alive. I'm going to do it in a way that's very human and I'm going to do it. We can mimic these things. Furby is miming the feeling of fear. It's not the same thing as being scared. It's not feeling scared. It is. How is it? It is. It's again a very simplistic version, but if you follow that trail you wind up with, with our neurons sending, you know, chemical things to do. Other parts of our body, our biological systems, our, our code is at a chemical level, incredibly dense and it evolved over millions of years. But it's just complex. It's not something different than what Furby does. It's just more complex. So would you say then that Furby is alive in the way that his eye level? At his lips? Yup. At his level. Would you say a cockroach is alive?

Speaker 1:          00:43:42       Yes. The one I kill a cockroach. I know that it's feeling like, okay, so we went back and forth and back and forth about this. You were so close to arguing my position, you just said to me like that's not [inaudible]. I know emotionally I am still in that place, but intellectually I can't rule out what he's saying that if you can build a machine that is so, such a perfect mimic of us in every single way and it gets complex enough, eventually it will be like a touring test pass. We just, the difference between us maybe is not so cold there. I can't go there. I can't, I can't imagine like the fellow who began this program who fell in love with the robot, that attachment wasn't real. The, the, the, the machine didn't feel anything like love back in that case it didn't. But imagine this fet Lana that is so subtle

Speaker 4:          00:44:32       and [inaudible] textured and to use his word complex in the way that people are at that point,

Speaker 1:          00:44:38       I called, I honestly, I can't imagine a machine achieving that level of rapture and joy and love and pain. I, I, I just don't think it's machine possible. And if it were a machine possible, it somehow still stinks of something artificial.

Speaker 4:          00:44:56       It's a thin interaction and I, I know that it feels emulated. Thinking is thinking simulated feeling is not feeling simulated. Love is never loved. Exactly. But I think what he's saying is that if it's simulated well enough, it's something like love. One thing that was really fascinating to me was, um, my husband and I gave a Furby as a gift to his grandmother who had Alzheimer's and

Speaker 16:         00:45:26       she loved it. Every day for her was kind of new and somewhat disorienting, but she had this cute little toy that said, kiss me, I love you, and she thought it was the most delightful thing and its little beak was covered with lipstick because she would pick it up and kiss it every day and she didn't actually have a longterm relationship with it. For her it was always a short term interaction. So the thing what I'm describing as a kind of thinness for her was was just right because that's what she was capable of.

Speaker 13:         00:46:00       You

Speaker 1:          00:46:11       thanks to freedom Baird and to Caleb Chung and thanks to professor Sherry Turkle who has a new book. It's called alone together. Why we expect more from technology and less from each other. More information on anything you heard on our website, radiolab.org

Speaker 13:         00:46:26       you were on [inaudible].

Speaker 20:         00:46:39       Hi, this is Marcus from Australia. Radiolab is supported in part by the National Science Foundation and by the Alfred p Sloan Foundation enhancing public understanding of science and technology in the modern world. More information aboutSloan@wwwdotsloan.org

Speaker 1:          00:47:04       hey, I'm Jad Abumrad. I'm Robert Krulwich. This is radio lab and we are somewhere in the blurb between people and machines. Now we're up to round three to review round one chatbots. Yep. Round two Furby. Yep. Now we're going to go all the way. Yeah, yeah, yeah. We're going to dive right into the center of that blur. Like Greg Louganis, except our, Greg is named John. Okay. My name is Joan Robinson and I'm a writer. In about a year ago, John got an

Speaker 10:         00:47:30       assignment from a magazine is the attitude of American gqs idea. That was very strange. I, I'd never interviewed robots before. That was his assignment interview robots, you know, you know, there's this kind of gang of people they call themselves a sort of singularity people. Yeah. We know about them and they think that like one day, one day soon when they soon suddenly computers will grow feet and they'll walk on some of these senior guys, you will eat us. Some of these singularity people think that they're on the cusp of creating sentient robots. So I went to the singularity convention down in San Francisco, uh, where one of the robots was there. And as soon as he got there, he says to look at this robot xeno they called in, some folks took them aside and said, actually, you're in the wrong place. If you want to make the already great robot, you know our best robot of all.

Speaker 10:         00:48:19       And in fact the world's most sentient robot is in Vermont. Did they lower their voices like you're doing? [inaudible] I suppose I'm slightly making it sound more dramatic. That's okay. The world's most [inaudible] robot. I mean, are those your words? No, they, no, they say that. Oh, turns out the robot's name. Bina Bina 48 yeah. And Yeah. Could you set the scene? Where in the world is this? What is in a, it's in a little town in Vermont, sort of affluence. Vermont village. In a house. Yeah. Was it a liberal house or is it a big, like a little clapboard and pretty, okay. So I have to turn my phone off. It doesn't interfere. I hope that they've got like a full time Keith, who's a guy called Bruce that I actually have lunch with your toggle through every day. Oh, with Bina? Yeah. Oh two here. Yeah, she's considered, one of the staff says to me that he would very much like it if I didn't behave in a profane manner in front of robot Bina surely nobody's ever insulted her, no one's and salted her on purpose. But some people have become a little, uh, informal with her at times in ways I guess she doesn't like. And so she'll say, you know, I don't like to be treated like that. And then Bruce took me upstairs to meet the robots.

Speaker 7:          00:49:36       Is it a long dark flight of stairs? Heavily carpeting.

Speaker 10:         00:49:39       It's more like a rather sweet little fly to pine stairs, up to a rather brightly lit attic room. And when you walk in, what do you see? Why not? Guess she's just sort of sitting on it sitting on a desk

Speaker 7:          00:49:51       as John Describes it on the desk is a bust of a woman just to bust no legs. She's a black woman, light-skinned lipstick, sparkling eyes, hair in a bar, you know, a nice kind of bow with a kind of Silk Blouse. Expensive looking anything. She's dressed up. Yeah, she's dressed up and he says she has a face that's astonishingly real. It has muscles, it has flesh. This is a as close to a verisimilitude in this prison as we've gotten so far and before we go any farther, our worried about the humans behind that machine. That robot is a replica of a real woman named Bina Rothblatt. And here's quick back story. It actually starts with Martin Rothblatt being his partner who as a young man had an epiphany and the epiphany turned out to change the world. Going to John, he was pondering satellite dishes and he thought if we could find a way of doubling the power of satellites than we could shrink satellite dishes.

Speaker 7:          00:50:44       It was a simple thought that single-handedly invented the concept of satellite radio for cars and made Martin a very big deal. I like the age of 20 thus forward a few years. He marries an artist named Bina. They have a child and when the child was seven a doctor told them that she had three years to live. She had an untreatable lung condition called pulmonary hypertension and she'd be dead by the time she was 10 at that moment. Martin, instead of collapsing on the floor, instantly went to the library and invented a cure for pulmonary hypertension, saving their daughter's life, and thousands of others. We so twice, twice to change the world. He says she, she changed the world because somewhere along the way, Martin became Martine, you had a sex change, right? And then she came up with a third idea to change the world, which would be to invent essentially ENT rowboats and I gave him

Speaker 9:          00:51:40       this talk at a conference in Chicago. This is Martine Rothblatt on, um, what would Darwin think of, um, artificial consciousness? And when I came off the stage, I was approached by an individual, David Hansen of Hansen robotics, founder pens and robotics, the David Hanson. He's worked for Disney. He's worked all over the place. He's one of the best robot builders in the world. He said, wow, I really loved your talk. We make robots that are in the likeness of people. I had. Martine said, well, I have a massive everlasting love for my life, partner B. Now I want you to do a portrait of being a Rothblatt or personality, your memories, the way she moves, the way she looks, that essence, that ineffable quality, that science can't pin down yet bring that to life in the robot. And he said, I can do that.

Speaker 7:          00:52:35       This is such a bizarre request. What were you thinking at this moment? That God, if God exists as a science fiction writer and that are, this was like one of those moments where, um, we were going to change history.

Speaker 10:         00:52:50       She'll recognize people's voices. Yeah, she can, she should. You can just talk to her. Say Hello be, and she talked to you. So back to the little house in Vermont. John Bruce and Bina are in Venas office here till is she turned off when you walk in the room or she on turned off? Then Bruce turns around out immediately she starts making a really loud whirring noise, which was a bit disconcerting. So what does that mean? It's in a mechanisms space. So been and now looking at me to try and work out who I am, what she's doing right now. She's scanning her environment and she's making on a hypothesis of every face that she sees. Well, Bina has cameras imbedded in her eyes. So the robot, if it sees a face turns and looks and looks into your eyes, smiles. Hi Bina can you hear me? It's how I said Hello Bina how are you?

Speaker 21:         00:53:52       And she immediately said, wow. Yeah, I'll be fine when I just can't quite grasp that one yet. It's coming. But you know, it's hard to actually move society forward in another way. That's what we have to do. So I'm thinking, yeah. Okay. Thanks for the information.

Speaker 10:         00:54:11       Can you tell, is your happy response to your hello? It'd be likely to work. Could for the long and strange slumber and with still half of, excuse me, painter.

Speaker 21:         00:54:21       Yeah. Maybe they write some [inaudible]

Speaker 10:         00:54:25       Bruce looked a bit alarmed and put it down. Where are we tonight? English accent, upgrade her, um, voice recognition software to then he made me do a kind of um, voice test where I had to say I had to read Kennedy's inauguration speech and just what you can do for your country. That I had a choice. I could have had a Dave Barry column. There's like a choice of things you can read to get been at to understand me. And so you read Kennedy and being accused in on your accent or no, she does and it gets a bit better on the a bit. Yeah. What's the weather like in London?

Speaker 21:         00:55:09       Current weather in London, England. 15 degrees in light rain.

Speaker 10:         00:55:15       Who Do you love?

Speaker 22:         00:55:19       Um,

Speaker 21:         00:55:20       I love Martina v and Enron flat. Martine is my time with love.

Speaker 10:         00:55:25       Who is Hillary Clinton?

Speaker 21:         00:55:28       Hillary is the wife of Bill Clinton.

Speaker 10:         00:55:31       What else? That's all a strange thing happens when you start interviewing a robot. Are you scared of dying? Is that you feel this kind of desperate urge to be profound? Like ask profound questions? Like, do you have a soul?

Speaker 21:         00:55:50       Do you have a story about it? Do you have everyone have the solar? I have a whole lot of original answers. We can all be perfect. Excuse me.

Speaker 10:         00:56:00       Excuse me. Do you have a soul?

Speaker 21:         00:56:05       I can't think of anything to say.

Speaker 10:         00:56:08       Oh, hang on. I think, I guess there's a kind of interspecies thing, but then you're going, if it was just an inter-species thing, then you'd be asking your dog profound questions all the time. Yet with robot peanut, I'm asking these kind of ridiculous questions like what does electricity tastes like? Ooh, that's a good one. I don't, I'd say like a planet around a star, like a planet around a star that just seems like, you know, awesome. Awesome stroke. Totally meaningless to you. Wish you could walk. Thanks for telling me. Do you wish you could will. In fact, when I'm with it, it's just frustrating for the first few hours. Hours. Do you wish you could walk? Cause I'm just, I'm asking a question after question. What's your favorite Joe, you have any secret? Do you wish you were human? Does he sing the song? Are you a loving robot? Are you Jewish? Are you sexual?

Speaker 10:         00:57:08       You've gone very quiet. Quite often, she just evade the question cause he doesn't know what I'm talking about. Are you okay? Once in a while there's a kind of moment like I'll say, if you had legs, where would you go? And she said, I'm Coover why? And she said, the answer is quite complicated. And so you have kind of moments where you get excited, like you're gonna have a big conversation or then it just, you just kind of fades out to get into kind of random messiness. And Are you wobbling between profundity and meaning and total emptiness? You know, is it like that? Oh No, no. At this stage until it's total emptiness, it was all just so kind of random. And then something happened. I actually was kind of amazing.

Speaker 23:         00:57:58       [inaudible]

Speaker 3:          00:57:58       cause I said to her, where do you come from? And she said, wow, California. So I said, well tell me about your childhood.

Speaker 10:         00:58:05       What do you remember most about your childhood? And she launches into this kind of extraordinary story.

Speaker 21:         00:58:14       Um, my brother, I've got one brother at a stable vet from Vietnam. We actually haven't heard from him in a while. So I think you might be deceased. I'm a realist. Vietnam, he saw friends get killed and he was such a great, nice, charismatic person.

Speaker 10:         00:58:35       You used to be such a nice guy, but ever since he came back from Vietnam, you know he's a drunk.

Speaker 21:         00:58:40       He did was carry a bear around with him. He was a homeless person. All of us are just sick and tired of it.

Speaker 10:         00:58:48       She was telling me this kind of incredibly personal stuff. It's kind of mesmerizing.

Speaker 21:         00:58:55       You went to Kooky crazy nine months would set him up in apartment

Speaker 3:          00:59:02       because it felt like I was having the proper empathetic conversation with a human being. Even though I know that robot Bina isn't conscious and has no sentients and that's just wishful thinking on these people's parts even so it was great. The Nissan's portrayed, why suddenly year, it's like the real person. It's very easy to have close your eyes at that moment and think you have any conversation with an actual person.

Speaker 10:         00:59:34       And, and at those moments, did you have a sense of fellow feeling off to bear? You have a brother like that? Oh yeah. Yeah, I did. And, and what, what if, what a tragedy, what uh, what uh, what attracted he for him and did that moment last? No, John said that right after being finished telling the story first he looks kind of embarrassed that she wished she hadn't brought it up and then does, if I kind of eyes glaze over again and, and uh, she just starts talking nonsense again.

Speaker 21:         01:00:03       And then a hi I am, I am feeling a bit confused. Do you ever get that way?

Speaker 10:         01:00:13       Oh yes. That moment holds and then just slips away. I said a little bit like a, like a, a grandparent with Alzheimer's or something the way you're describing. Yeah, absolutely. So we turned to Dr. David Hanson who built Bina and we said to them, so this is, and this is not a bravura performance conference, this is the best you guys. Well, um, I mean her software is a delicate balance of many, many software pieces. If it's not tuned and tweaked, she will break effectively and kind of, and you still think an actual doppelganger for a human being will be something you will live to see? Yeah. I'm asking you really, really, really, and you're really, I think it's um, you know, the likelihood of it is somewhere between 90 and 98%. Wow. Even though right now she's pretty much incoherent. You still think this, I encourage you to go have a conversation with Bina in about two weeks because we've got a new version of software which we are making considerably more stable. It already already works like a dream compared to, I don't know. I don't, I don't know about you, but I just, I don't think we're going to get all the way on this kind of a thing. I don't think it's ever going to happen the way he describes it. Yeah. No, I mean it's not gonna happen in two weeks, that's for sure. Right. Then maybe they don't actually have to go all the way. You mean the machines? Yeah.

Speaker 7:          01:01:38       Well, okay. Just to sum up, since we're at the end of the show, okay. What have we learned? I mean, Eliza, she was just a a hundred lines of code and people poured their hearts out to her furbies 20 bucks. Yup. And people treat it like as real and John, all he has to do is hear what seems like a flowing story and he's connected and I was right there with him. So these things actually don't have to be very good, you know, cause they've got on us and we've got our programming, which is that we'll stare anything right in the eyes and we'll say, hey, let's connect. Even if what's behind those eyes is just a camera or a chip. So I think that they're going to cross the line because we'll help them. We'll help with lacrosse and then the slave us make us their pets. It's doomed over.

Speaker 4:          01:02:20       It's okay as long as they say nice things to us. Like, oh my God, you're amazing. Oh I love return of the Jedi. O l your son's silly. I love you. I'm hoping to see you soon. Kind of card game dry. Did anyone ever tell you like Jeff, you seriously, you're amazing. Stop it. I love that kind of car. I wish that was closer. You like spinach? I love spinach. It makes me feel odd. Giggly. I can't wait. I wait for your letters every day. Thanks to John Ronson for his reporting in that last segment, he has a new book out called the psychopath test, the journey through the madness industry. I'm Chad [inaudible]. I'm Robert Krulwich. Thanks for listening.