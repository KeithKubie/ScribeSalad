WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:02.800
What about those discriminative
methods we were talking about?

00:00:04.250 --> 00:00:08.670
As we said before they find a division,
a surface between the classes.

00:00:08.670 --> 00:00:11.190
We're going to talk about
a couple of different methods.

00:00:11.190 --> 00:00:13.920
The one that I want to talk about
today is nearest neighbors.

00:00:13.920 --> 00:00:16.341
And then, we'll do separate lessons for
each of the, the other two.

00:00:17.520 --> 00:00:19.940
So the simplest method we're going to
talk about is nearest neighbor

00:00:19.940 --> 00:00:21.070
classification.

00:00:21.070 --> 00:00:24.450
And it is a discriminative method
because we are using the boundaries

00:00:24.450 --> 00:00:29.020
between I wont say classes,
I'll say examples, right.

00:00:29.020 --> 00:00:31.400
The boundary is always between
examples of the classes.

00:00:31.400 --> 00:00:33.940
But we actually don't have
to do very much training.

00:00:33.940 --> 00:00:36.789
So the idea is we have some future
space here labeled as blank.

00:00:37.940 --> 00:00:39.770
It should be x1 and x2.

00:00:39.770 --> 00:00:43.250
This picture by the way is taken
from a classic book, due to Hart.

00:00:43.250 --> 00:00:44.467
It was, the original was due to Hart,

00:00:44.467 --> 00:00:46.060
the new version is due to Hart and
Stork.

00:00:46.060 --> 00:00:50.600
And it's a good way of learning sort
of classical pattern recognition.

00:00:50.600 --> 00:00:52.910
So the idea is I've got
a bunch of examples.

00:00:52.910 --> 00:00:56.590
The negatives, so the not a's
are these little black points, here.

00:00:56.590 --> 00:00:57.870
So the, the, the black thing.

00:00:57.870 --> 00:00:59.970
And then I've got some
positive examples, right?

00:00:59.970 --> 00:01:01.660
And those are the little red ones,
there.

00:01:01.660 --> 00:01:05.000
And when I come up with a new point,
all right?

00:01:05.000 --> 00:01:06.410
So there it is in the green.

00:01:06.410 --> 00:01:09.780
Basically, that's my
new novel test example.

00:01:09.780 --> 00:01:10.982
And how am I going to label that?

00:01:10.982 --> 00:01:15.150
I'm going to find the closest
training example, so in this

00:01:15.150 --> 00:01:19.410
case the closest training example
is that one, and so I say, ah-ha.

00:01:19.410 --> 00:01:24.062
The closest one is a positive, so I'm
going to label this new one a positive.

00:01:24.062 --> 00:01:28.770
All right, now drawn on this picture
actually are the divisions themselves,

00:01:28.770 --> 00:01:32.180
and the computer scientists
among you will recognize that as

00:01:32.180 --> 00:01:33.480
a Voronoi partitioning.

00:01:33.480 --> 00:01:37.100
Right, when you partition a space
using a, a Voronoi method,

00:01:37.100 --> 00:01:41.490
you essentially carve the space
up into these little chunks.

00:01:41.490 --> 00:01:43.990
Where this chunk means that
if you're in that chunk,

00:01:43.990 --> 00:01:46.580
this black point is
the closest one to you.

00:01:46.580 --> 00:01:49.988
And so all nearest neighbor
is doing is giving you

00:01:49.988 --> 00:01:53.710
a Voronoi partitioning of the space.

00:01:53.710 --> 00:01:57.280
It's shall we say pretty
easy to implement.

00:01:57.280 --> 00:01:59.670
It has a couple problems,
one is it doesn't work very well.

00:01:59.670 --> 00:02:00.910
We'll get to that in a second.

00:02:00.910 --> 00:02:05.910
It's also, very data intensive in terms
of the memory of what you need to know.

00:02:05.910 --> 00:02:08.710
Right, so if I give you
a million trainee examples,

00:02:08.710 --> 00:02:10.710
how many of them do you
have to remember, Megan?

00:02:10.710 --> 00:02:11.420
&gt;&gt; A million.

00:02:11.420 --> 00:02:13.620
&gt;&gt; A million, that's exactly right.

00:02:13.620 --> 00:02:20.360
Furthermore, every time a new point
comes in I gotta find the closest one.

00:02:20.360 --> 00:02:21.850
So if I was really dumb,

00:02:21.850 --> 00:02:24.590
you know I don't have a compute, Masters
of Computer Science Degree from Georgia,

00:02:24.590 --> 00:02:26.580
I might list go through
all of them one at a time.

00:02:26.580 --> 00:02:29.490
If I'm a little smarter I'll use
something called a KD tree or

00:02:29.490 --> 00:02:31.680
some other hierarchical
representation but

00:02:31.680 --> 00:02:33.940
you're still searching through
a lot of these things.

00:02:33.940 --> 00:02:36.448
So at test time,
it's very painful as well so

00:02:36.448 --> 00:02:39.550
not only is it a lot of memory stored,
etc.

00:02:39.550 --> 00:02:42.700
But let's get back to that
doesn't work all that well thing.

00:02:42.700 --> 00:02:47.080
Well one of the things that might
happen is that I might have, you know,

00:02:47.080 --> 00:02:49.647
an occasional kind of spurious point.

00:02:49.647 --> 00:02:54.313
Or I might be in an area where I really
don't have too many points nearby.

00:02:54.313 --> 00:02:58.740
And what I'd like to do is be able
to make a more robust decision.

00:02:58.740 --> 00:03:02.318
Okay, and the way of doing
that is referred to as k-NN or

00:03:02.318 --> 00:03:04.190
K-Nearest Neighbor.

00:03:04.190 --> 00:03:05.770
And it's really very simple.

00:03:05.770 --> 00:03:08.950
It's basically the idea, if I've got
some new point, and it's written here as

00:03:08.950 --> 00:03:14.680
an x, so that point right there,
I don't just find the nearest point.

00:03:14.680 --> 00:03:16.810
You gotta think like
a computer scientist.

00:03:16.810 --> 00:03:17.420
You find k.

00:03:18.600 --> 00:03:21.480
So k might be one, might be three,
might be five, might be seven.

00:03:21.480 --> 00:03:24.660
Whatever choice you choose to make,
you would look for k.

00:03:24.660 --> 00:03:30.650
So, in k-NN, in 5-NN, for example,
you know, if I have this point x, okay?

00:03:30.650 --> 00:03:32.740
What I'm going to do is,
I'm going to look for

00:03:32.740 --> 00:03:34.700
the five nearest neighbors, right?

00:03:34.700 --> 00:03:38.840
And I'll just, one,
two, three, four, five.

00:03:38.840 --> 00:03:44.700
Okay, and in this particular case three
of them are black, two of them are red,

00:03:44.700 --> 00:03:48.840
black is negative, red is positive so
I would classify it as what?

00:03:48.840 --> 00:03:49.670
Negative.

00:03:49.670 --> 00:03:50.170
Okay?

00:03:51.360 --> 00:03:53.088
One of the funny things, well,

00:03:53.088 --> 00:03:57.480
one of the interesting things about
k-NN is it works really well, okay?

00:03:57.480 --> 00:04:00.810
It does have this data intensive
problem, and there are methods that we

00:04:00.810 --> 00:04:04.520
now use that are kind of,
they're sort of related to k-NN.

00:04:04.520 --> 00:04:10.062
But this idea of getting a loose
consensus is very effective.

00:04:10.062 --> 00:04:12.930
We're not actually going to talk about
a thing called random forest later,

00:04:12.930 --> 00:04:16.230
but this notion of consensus.

00:04:16.230 --> 00:04:20.970
So I don't get the support from just one
place or even one classification method

00:04:20.970 --> 00:04:24.260
little classifier, this notion
of consensus is very powerful.

