WEBVTT
Kind: captions
Language: en

00:00:06.760 --> 00:00:09.190
ROBERT CROWE: Hi and welcome
to Coding TensorFlow.

00:00:09.190 --> 00:00:11.830
I'm Robert Crowe, and today
we'll be learning how to get

00:00:11.830 --> 00:00:14.680
started using TensorFlow
to create and train

00:00:14.680 --> 00:00:18.730
a model when the result we need
is a single number, like maybe

00:00:18.730 --> 00:00:24.100
a price or a probability, or
in this case, miles per gallon.

00:00:24.100 --> 00:00:26.770
In other words, a
regression problem.

00:00:26.770 --> 00:00:27.670
Real quick.

00:00:27.670 --> 00:00:29.650
Let's talk for a moment
about a regression

00:00:29.650 --> 00:00:32.290
problem versus a
classification problem.

00:00:32.290 --> 00:00:34.480
Regression and
classification are probably

00:00:34.480 --> 00:00:37.030
the two most commonly
solved problems

00:00:37.030 --> 00:00:38.650
with machine learning today.

00:00:38.650 --> 00:00:42.050
Today, we're going to be
working on a regression problem.

00:00:42.050 --> 00:00:45.790
So we want our model to look
at an example from our data

00:00:45.790 --> 00:00:48.250
and predict a number.

00:00:48.250 --> 00:00:50.980
Usually that will be a
floating point number.

00:00:50.980 --> 00:00:53.860
Now, if we were doing a
classification problem,

00:00:53.860 --> 00:00:57.280
then we'd want our model to
look at an example from our data

00:00:57.280 --> 00:01:02.500
and tell us which class or
group it thinks this example is.

00:01:02.500 --> 00:01:03.910
So it might be
telling us that it

00:01:03.910 --> 00:01:08.470
thinks this example is a fish
or a bird or maybe a weasel.

00:01:08.470 --> 00:01:10.990
But today, we're
doing regression.

00:01:10.990 --> 00:01:14.470
Specifically, today, we're going
to look at training a model

00:01:14.470 --> 00:01:18.710
to tell us the miles per
gallon of cars from the 1970s

00:01:18.710 --> 00:01:21.670
by looking at data,
like their weight,

00:01:21.670 --> 00:01:24.580
the number of cylinders,
the horsepower.

00:01:24.580 --> 00:01:26.560
Just look at these beauties.

00:01:26.560 --> 00:01:28.700
Miles per gallon
is a single number.

00:01:28.700 --> 00:01:30.940
So regression is the
right thing here.

00:01:30.940 --> 00:01:31.540
OK.

00:01:31.540 --> 00:01:32.500
One more thing.

00:01:32.500 --> 00:01:34.540
We're going to be using Keras.

00:01:34.540 --> 00:01:36.490
If you haven't heard
of Keras before,

00:01:36.490 --> 00:01:38.560
it's a high level API
for deep learning,

00:01:38.560 --> 00:01:41.110
and it's very user friendly.

00:01:41.110 --> 00:01:43.510
And it's really powerful too.

00:01:43.510 --> 00:01:46.750
And today, we're just going
to scratch the surface.

00:01:46.750 --> 00:01:49.190
OK, let's get started.

00:01:49.190 --> 00:01:51.700
So today, we're going
to be using seaborn.

00:01:51.700 --> 00:01:54.850
And seaborn isn't installed
by default on a co-lab.

00:01:54.850 --> 00:01:57.370
So we're going to pull
it with a PIP install.

00:01:57.370 --> 00:01:58.990
You can totally do
that in a co-lab.

00:01:58.990 --> 00:02:01.180
You can install
whatever you need to.

00:02:01.180 --> 00:02:01.960
OK.

00:02:01.960 --> 00:02:02.530
That's it.

00:02:02.530 --> 00:02:03.850
Seaborn's installed.

00:02:03.850 --> 00:02:04.532
That was quick.

00:02:04.532 --> 00:02:06.490
Now we're going to go
through the other imports

00:02:06.490 --> 00:02:07.330
that we need to do.

00:02:07.330 --> 00:02:10.940
Pandas, and of course,
TensorFlow itself,

00:02:10.940 --> 00:02:12.560
including Keras.

00:02:12.560 --> 00:02:13.670
OK, that's good.

00:02:13.670 --> 00:02:18.580
So now we've got version
1.13 of TensorFlow.

00:02:18.580 --> 00:02:20.580
TensorFlow includes a
lot of great data sets.

00:02:20.580 --> 00:02:22.780
But today, we're going
to be reaching out

00:02:22.780 --> 00:02:25.600
to a data set that's at the
University of California

00:02:25.600 --> 00:02:26.560
at Irvine.

00:02:26.560 --> 00:02:27.780
It's a great resource.

00:02:27.780 --> 00:02:30.910
They have this repository
of public domain data sets.

00:02:30.910 --> 00:02:34.210
And it's a great tool when
you're looking for data.

00:02:34.210 --> 00:02:37.720
Keras makes it really easy for
us to download our data set.

00:02:37.720 --> 00:02:41.510
You can see the URL right
there of the UCI repository.

00:02:41.510 --> 00:02:44.350
Then, because it's easier
to see what we're doing,

00:02:44.350 --> 00:02:47.380
we're going to give
our columns some names.

00:02:47.380 --> 00:02:50.420
And then we want to
take a look at our data

00:02:50.420 --> 00:02:51.850
and just see what it looks like.

00:02:51.850 --> 00:02:53.780
That's always a
good thing to do.

00:02:53.780 --> 00:02:55.910
And pandas will help us do that.

00:02:55.910 --> 00:02:57.940
Data's almost never clean.

00:02:57.940 --> 00:03:01.510
Even data that you've gotten
for something like UCI.

00:03:01.510 --> 00:03:06.020
So we need to make sure that
all our data is good values.

00:03:06.020 --> 00:03:09.150
One of the things we're going
to look for is unknown values.

00:03:09.150 --> 00:03:11.770
And you can see from this
that there are six of them

00:03:11.770 --> 00:03:13.420
in the Horsepower column.

00:03:13.420 --> 00:03:16.280
In this case, there's
only six rows.

00:03:16.280 --> 00:03:18.890
So we can be safe
just dropping them.

00:03:18.890 --> 00:03:20.780
So let's go ahead and do that.

00:03:20.780 --> 00:03:22.840
So we have one column
that's different.

00:03:22.840 --> 00:03:24.460
The Origin column.

00:03:24.460 --> 00:03:25.990
It's not a numeric column.

00:03:25.990 --> 00:03:27.610
It's categorical.

00:03:27.610 --> 00:03:30.250
And usually with a
categorical column,

00:03:30.250 --> 00:03:34.790
you want to convert it to what's
known as a one hot column.

00:03:34.790 --> 00:03:38.410
So to do that, we're going to
first remove the Origin column.

00:03:38.410 --> 00:03:41.470
And then we're going to
create three new columns, one

00:03:41.470 --> 00:03:44.470
for each country that the
cars could be coming from.

00:03:44.470 --> 00:03:48.010
And each of those will
only have a value of 1

00:03:48.010 --> 00:03:50.930
when the car came
from that country.

00:03:50.930 --> 00:03:53.360
Let's take a look at that.

00:03:53.360 --> 00:03:56.590
So we're going to split
our data in two pieces.

00:03:56.590 --> 00:04:00.610
80% of it, or you can see
there, a fraction of 0.8,

00:04:00.610 --> 00:04:02.830
we're going to keep in
our training data set.

00:04:02.830 --> 00:04:05.690
And the other is going
to be our test data set.

00:04:05.690 --> 00:04:08.960
So you might be wondering,
why did we split our data?

00:04:08.960 --> 00:04:11.260
Well, we want it
to do well on data

00:04:11.260 --> 00:04:13.060
that it's never seen before.

00:04:13.060 --> 00:04:16.029
So what we're going to do
is split it into two pieces

00:04:16.029 --> 00:04:17.950
and keep one of those pieces.

00:04:17.950 --> 00:04:19.570
And we're only going
to use that when

00:04:19.570 --> 00:04:21.910
we're ready to test
our model and see

00:04:21.910 --> 00:04:24.880
how well it does for data
it's never seen before.

00:04:24.880 --> 00:04:26.860
That's called generalization.

00:04:26.860 --> 00:04:30.420
A model that does well on data
that it's never seen before

00:04:30.420 --> 00:04:32.710
generalizes well.

00:04:32.710 --> 00:04:34.720
So now let's take
another look at our data.

00:04:34.720 --> 00:04:36.430
Specifically, let's
look at the miles

00:04:36.430 --> 00:04:40.450
per gallon, the cylinders, the
displacement, and the weight.

00:04:40.450 --> 00:04:43.510
To do that, we're going
to use seaborn's pair plot

00:04:43.510 --> 00:04:48.370
utility, which gives us a nice
plot of the joint distributions

00:04:48.370 --> 00:04:52.870
of each of our features along
with kernel density estimation

00:04:52.870 --> 00:04:56.860
plots, or KDE plots,
along the diagonal.

00:04:56.860 --> 00:04:59.620
If you haven't seen
them before, KDE plots

00:04:59.620 --> 00:05:02.850
are essentially just
smooth histograms.

00:05:02.850 --> 00:05:05.230
So you can see there are
some clear relationships

00:05:05.230 --> 00:05:07.180
between some of our features.

00:05:07.180 --> 00:05:08.050
OK.

00:05:08.050 --> 00:05:11.830
Now let's look at some summary
statistics of our features

00:05:11.830 --> 00:05:15.130
like the mean, the standard
deviation, the minimum,

00:05:15.130 --> 00:05:17.380
the quartiles, and the maximum.

00:05:17.380 --> 00:05:19.330
One of the things I
want you to notice here

00:05:19.330 --> 00:05:23.290
is that the ranges of these
values are very different.

00:05:23.290 --> 00:05:25.210
And that's not a
good thing in general

00:05:25.210 --> 00:05:27.650
when you're training a
machine learning model.

00:05:27.650 --> 00:05:30.860
So we're going to be fixing
that in a little bit.

00:05:30.860 --> 00:05:33.430
The next thing we need
to do is make sure

00:05:33.430 --> 00:05:37.430
that we're not giving our
model the right answer,

00:05:37.430 --> 00:05:40.690
meaning we don't want to give
it the labels in our training

00:05:40.690 --> 00:05:41.710
data.

00:05:41.710 --> 00:05:43.910
Or for that matter,
in our test data.

00:05:43.910 --> 00:05:47.590
So we need to split those
off from our data sets.

00:05:47.590 --> 00:05:50.940
OK, so it's time to deal
with those ranges of values

00:05:50.940 --> 00:05:52.230
that were really different.

00:05:52.230 --> 00:05:55.770
We need to do something called
normalization of our features.

00:05:55.770 --> 00:05:57.870
When we do that,
all of our features

00:05:57.870 --> 00:06:01.000
will fall between 0 and 1.

00:06:01.000 --> 00:06:03.090
And the way we accomplish
that is essentially

00:06:03.090 --> 00:06:04.920
by using a z-score.

00:06:04.920 --> 00:06:07.170
We subtract the
mean and then divide

00:06:07.170 --> 00:06:09.130
by the standard deviation.

00:06:09.130 --> 00:06:11.270
So let's do that now.

00:06:11.270 --> 00:06:13.170
So now we're ready
to build our model.

00:06:13.170 --> 00:06:15.590
And Keras makes it really easy.

00:06:15.590 --> 00:06:18.290
We're going to use Keras
sequential, which gives us

00:06:18.290 --> 00:06:20.180
a fully connected model.

00:06:20.180 --> 00:06:21.980
It's going to have three layers.

00:06:21.980 --> 00:06:24.300
All three are dense layers.

00:06:24.300 --> 00:06:27.020
The first to have
a relu activation.

00:06:27.020 --> 00:06:29.030
And the first one
needs to know what

00:06:29.030 --> 00:06:31.400
the shape of the input data is.

00:06:31.400 --> 00:06:34.970
The last layer doesn't
have an activation, meaning

00:06:34.970 --> 00:06:37.160
that it's a linear activation.

00:06:37.160 --> 00:06:40.950
And for a regression
model, that's what we want.

00:06:40.950 --> 00:06:41.660
OK.

00:06:41.660 --> 00:06:44.360
We also need to give
it an optimizer.

00:06:44.360 --> 00:06:47.310
We're going to use
RMSprop for that.

00:06:47.310 --> 00:06:49.460
And we need a loss function.

00:06:49.460 --> 00:06:52.890
We're going to use mean
squared error for that.

00:06:52.890 --> 00:06:54.710
And we need to give it metrics.

00:06:54.710 --> 00:06:57.320
The metrics are what
we're going to use to see

00:06:57.320 --> 00:06:59.420
how well our model is doing.

00:06:59.420 --> 00:07:04.120
We'll give it mean squared
error and mean absolute error.

00:07:04.120 --> 00:07:04.790
OK.

00:07:04.790 --> 00:07:06.860
Let's go ahead and
create our model.

00:07:06.860 --> 00:07:08.580
You can ignore those warnings.

00:07:08.580 --> 00:07:10.400
Those things come up sometimes.

00:07:10.400 --> 00:07:13.530
Now let's take a look at
a summary of our model.

00:07:13.530 --> 00:07:16.680
Keras makes it really
easy to do that.

00:07:16.680 --> 00:07:19.130
So you can see that we
have our three layers.

00:07:19.130 --> 00:07:24.710
And notice that we have 4,865
trainable parameters, even

00:07:24.710 --> 00:07:27.080
for a very simple
model like this.

00:07:27.080 --> 00:07:28.550
That's a lot to try to train.

00:07:28.550 --> 00:07:31.768
But Keras and TensorFlow
make that easy.

00:07:31.768 --> 00:07:34.310
Now we can go ahead and try our
model, even though we haven't

00:07:34.310 --> 00:07:37.190
trained it, and just make
sure that it produces

00:07:37.190 --> 00:07:39.710
results and doesn't blow up.

00:07:39.710 --> 00:07:41.670
Let's go ahead and do that now.

00:07:41.670 --> 00:07:42.500
So there.

00:07:42.500 --> 00:07:45.110
Now we've got some
results from our model,

00:07:45.110 --> 00:07:47.390
and we haven't trained it,
so of course, those numbers

00:07:47.390 --> 00:07:48.057
are meaningless.

00:07:48.057 --> 00:07:51.930
But the shape is right and
it didn't throw any errors.

00:07:51.930 --> 00:07:53.540
So things are looking good.

00:07:53.540 --> 00:07:56.000
So now we're ready
to train our model.

00:07:56.000 --> 00:07:59.210
We're going to train
it for 1,000 epochs.

00:07:59.210 --> 00:08:02.870
Each epoch is a pass through
all of our training data.

00:08:02.870 --> 00:08:05.780
And as we do that, we
want to print a dot

00:08:05.780 --> 00:08:08.210
so that we know that our
model is still training.

00:08:08.210 --> 00:08:10.070
Sometimes this could take days.

00:08:10.070 --> 00:08:12.290
We want to make sure
it's still training.

00:08:12.290 --> 00:08:16.230
So to do that, we're going
to create a print dot class.

00:08:16.230 --> 00:08:19.580
The other thing we're going to
do is split off 20% of our data

00:08:19.580 --> 00:08:21.590
in a validation set.

00:08:21.590 --> 00:08:24.590
And we'll use that to test
our model as we're training

00:08:24.590 --> 00:08:27.260
it to see how it's doing.

00:08:27.260 --> 00:08:27.950
OK.

00:08:27.950 --> 00:08:29.780
Let's go ahead and
train our model.

00:08:29.780 --> 00:08:31.430
See those dots?

00:08:31.430 --> 00:08:33.440
We want something
like those dots.

00:08:33.440 --> 00:08:35.210
If this was taking
a week, we'd want

00:08:35.210 --> 00:08:38.150
to make sure that we knew
that it was still going.

00:08:38.150 --> 00:08:38.900
And there we go.

00:08:38.900 --> 00:08:40.400
It's done.

00:08:40.400 --> 00:08:42.679
So we saved a history
object and we can

00:08:42.679 --> 00:08:44.130
look at our training results.

00:08:44.130 --> 00:08:45.830
Let's do that now.

00:08:45.830 --> 00:08:47.240
There we go.

00:08:47.240 --> 00:08:50.510
You see we've got loss,
our mean absolute error

00:08:50.510 --> 00:08:52.020
and mean squared error.

00:08:52.020 --> 00:08:53.480
Those were our metrics.

00:08:53.480 --> 00:08:56.030
And we've got the same
things for validation.

00:08:56.030 --> 00:08:59.930
Validation loss, validation
mean absolute error.

00:08:59.930 --> 00:09:01.250
But wait a minute.

00:09:01.250 --> 00:09:03.650
Something looks wrong
with those numbers.

00:09:03.650 --> 00:09:07.350
The loss and the validation
loss are going up.

00:09:07.350 --> 00:09:08.990
Let's take another look at this.

00:09:08.990 --> 00:09:10.730
A lot of times,
plotting will really

00:09:10.730 --> 00:09:12.710
help to see what's going on.

00:09:12.710 --> 00:09:15.340
We're going to use
matplotlib to do that.

00:09:15.340 --> 00:09:17.690
And we're going to
plot learning curves,

00:09:17.690 --> 00:09:19.950
one for each of our metrics.

00:09:19.950 --> 00:09:21.430
So there are our curves.

00:09:21.430 --> 00:09:24.180
The green is our
validation error.

00:09:24.180 --> 00:09:26.180
And you'll notice,
wait a minute.

00:09:26.180 --> 00:09:28.350
The green is going up.

00:09:28.350 --> 00:09:29.870
That's not good.

00:09:29.870 --> 00:09:32.370
This is classic overfitting.

00:09:32.370 --> 00:09:34.110
So how do you fix it?

00:09:34.110 --> 00:09:36.087
Well, there's lots
of different ways.

00:09:36.087 --> 00:09:37.670
But today we're going
to use something

00:09:37.670 --> 00:09:39.830
called early stopping.

00:09:39.830 --> 00:09:42.630
Early stopping is
really very simple.

00:09:42.630 --> 00:09:44.780
It just stops trading
the model as soon

00:09:44.780 --> 00:09:47.138
as the model stops improving.

00:09:47.138 --> 00:09:48.680
One of the things
you need to specify

00:09:48.680 --> 00:09:50.630
is what should it look for?

00:09:50.630 --> 00:09:53.450
In this case, we're going
to look at validation loss.

00:09:53.450 --> 00:09:54.800
And how patient should it be?

00:09:54.800 --> 00:09:57.860
How long should it wait
before it decides that things

00:09:57.860 --> 00:09:59.537
aren't getting better?

00:09:59.537 --> 00:10:01.370
So let's go ahead and
train our model again.

00:10:01.370 --> 00:10:05.080
But this time, we
use early stopping.

00:10:05.080 --> 00:10:06.170
And look at that.

00:10:06.170 --> 00:10:08.240
Those curves are perfect.

00:10:08.240 --> 00:10:10.730
Both the validation loss
and the training loss

00:10:10.730 --> 00:10:13.180
are going down together.

00:10:13.180 --> 00:10:16.560
Let's look at how well our
model is doing overall.

00:10:16.560 --> 00:10:19.320
We'll look at the
mean absolute error.

00:10:19.320 --> 00:10:20.040
Look at that.

00:10:20.040 --> 00:10:23.940
It's 1.8 miles per gallon off.

00:10:23.940 --> 00:10:25.720
Is that good?

00:10:25.720 --> 00:10:26.640
Well, you decide.

00:10:26.640 --> 00:10:30.480
But for me, if I was trying to
predict the miles per gallon

00:10:30.480 --> 00:10:33.300
of a car by looking at the
cylinders and the weight,

00:10:33.300 --> 00:10:35.650
I don't think I could
do any better than that.

00:10:35.650 --> 00:10:38.320
So now we're ready to
make some predictions.

00:10:38.320 --> 00:10:40.828
We'll make several and
then plot them out.

00:10:40.828 --> 00:10:41.370
Look at that.

00:10:41.370 --> 00:10:43.380
That's really not too bad.

00:10:43.380 --> 00:10:46.380
If they were perfect,
they'd all be on that line.

00:10:46.380 --> 00:10:49.840
But they're all pretty close,
so really, not too bad.

00:10:49.840 --> 00:10:52.830
Let's check and see what
our error looks like.

00:10:52.830 --> 00:10:55.320
We'll use a histogram for that.

00:10:55.320 --> 00:10:58.680
So it's pretty close to a
Gaussian, but not quite.

00:10:58.680 --> 00:11:00.480
With a data set this
small, you wouldn't

00:11:00.480 --> 00:11:02.350
expect it to be perfect.

00:11:02.350 --> 00:11:03.340
So that was great.

00:11:03.340 --> 00:11:04.830
We learned a lot today.

00:11:04.830 --> 00:11:07.650
We learned about mean
squared error loss.

00:11:07.650 --> 00:11:10.110
We learned about metrics
that help us understand

00:11:10.110 --> 00:11:12.210
how well our model is training.

00:11:12.210 --> 00:11:14.730
We learned about
normalization, which

00:11:14.730 --> 00:11:16.530
helps us prepare our data.

00:11:16.530 --> 00:11:18.960
And we learned about
early stopping, which

00:11:18.960 --> 00:11:20.880
helps us deal with overfitting.

00:11:20.880 --> 00:11:22.290
Thanks for watching.

00:11:22.290 --> 00:11:24.100
Tune in next time.

00:11:24.100 --> 00:11:27.220
If you have any questions or
comments, post them below.

00:11:27.220 --> 00:11:29.790
And don't forget to subscribe
to the TensorFlow channel

00:11:29.790 --> 00:11:32.300
for more stuff like this.

