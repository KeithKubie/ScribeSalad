WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:04.970
&gt;&gt; From the Library of
Congress in Washington D.C.

00:00:16.190 --> 00:00:18.350
&gt;&gt; John Haskell: Well
thank you all for coming.

00:00:18.350 --> 00:00:22.700
I know you're busy, and for
those of you who serve Congress,

00:00:22.700 --> 00:00:24.810
which I know some folks
are from Congress.

00:00:24.810 --> 00:00:26.140
You're still in session.

00:00:26.140 --> 00:00:27.770
So we're glad to
have everybody here

00:00:27.770 --> 00:00:29.070
from the Library and elsewhere.

00:00:29.070 --> 00:00:32.610
And I'm John Haskell, director

00:00:32.610 --> 00:00:35.360
of the Kluge Center
here at the Library.

00:00:35.360 --> 00:00:37.080
And we're co-sponsoring
this event

00:00:37.080 --> 00:00:39.200
with national digital
initiatives.

00:00:39.200 --> 00:00:43.360
That's a group, its mission is
promote the Library's digital

00:00:43.360 --> 00:00:46.850
presence here in the US as
well as around the world.

00:00:46.850 --> 00:00:50.670
Abby Potter and I think Kate
Swart [phonetic] associated

00:00:50.670 --> 00:00:51.970
with that.

00:00:51.970 --> 00:00:55.560
Abby is the head of NDI,
National Digital Initiatives.

00:00:55.560 --> 00:00:56.860
Is Abby going to raise her?

00:00:56.860 --> 00:00:58.160
There she is.

00:00:58.160 --> 00:01:01.130
If you want to know more about
it, just give her a shout.

00:01:01.130 --> 00:01:03.870
Not during the, not
during this event.

00:01:03.870 --> 00:01:06.930
Let me tell you a little bit
about the Kluge Center for many

00:01:06.930 --> 00:01:08.270
of you probably don't know much.

00:01:08.270 --> 00:01:11.150
I'll keep it to about a
sentence, and I'm going to steal

00:01:11.150 --> 00:01:15.500
from our charter which says
that the Kluge Center is,

00:01:15.500 --> 00:01:18.360
its mission is to
reinvigorate the connection

00:01:18.360 --> 00:01:20.290
between thought and action.

00:01:20.290 --> 00:01:24.060
The idea of being to connect
scholars with policymakers,

00:01:24.060 --> 00:01:26.220
one of the things we're
doing today with Martin.

00:01:26.220 --> 00:01:29.630
And the idea to be part
of the conversation

00:01:29.630 --> 00:01:31.640
about addressing challenges

00:01:31.640 --> 00:01:34.630
that democracies are facing
here in the 21st century.

00:01:34.630 --> 00:01:36.450
So that's what we're about.

00:01:36.450 --> 00:01:39.400
Fundamentally, we have
scholars in residence

00:01:39.400 --> 00:01:42.050
like Martin Hilbert who's a
distinguished visiting scholar

00:01:42.050 --> 00:01:43.350
with us.

00:01:43.350 --> 00:01:44.780
And he's had, he's a recidivist.

00:01:44.780 --> 00:01:48.360
This is his second go round
at the Kluge Center, and he's,

00:01:48.360 --> 00:01:52.240
and he's actually studying
the library this time around.

00:01:52.240 --> 00:01:54.470
And we'll get more into that.

00:01:54.470 --> 00:01:57.020
Martin is on the
faculty at the University

00:01:57.020 --> 00:01:58.650
of California at Davis.

00:01:58.650 --> 00:02:01.470
Let me tell you a little bit
more about him before I launch

00:02:01.470 --> 00:02:04.190
into asking him difficult
questions.

00:02:04.190 --> 00:02:06.980
First of all, he has two PhDs.

00:02:06.980 --> 00:02:11.910
It wasn't enough to
have one in German from,

00:02:11.910 --> 00:02:15.480
the PhD is in economics
and social sciences

00:02:15.480 --> 00:02:18.870
from the Friedrich-Alexander
University in Germany.

00:02:18.870 --> 00:02:20.530
He got that in 2006.

00:02:20.530 --> 00:02:22.650
And then he has one
in communication

00:02:22.650 --> 00:02:25.930
from the University of
Southern California.

00:02:25.930 --> 00:02:29.350
And that was at the Annenberg
School of Communication there.

00:02:29.350 --> 00:02:32.330
Before joining UC
Davis, Martin created

00:02:32.330 --> 00:02:35.110
and coordinated the
Information Society Program

00:02:35.110 --> 00:02:37.390
of the UN Regional Commission

00:02:37.390 --> 00:02:39.870
for Latin America
and the Caribbean.

00:02:39.870 --> 00:02:43.710
In his 15 years as United
Nations economics officer,

00:02:43.710 --> 00:02:46.600
he performed hands-on technical
assistance in the field

00:02:46.600 --> 00:02:49.550
of digital development
to presidents,

00:02:49.550 --> 00:02:52.680
other government experts,
legislators, diplomats,

00:02:52.680 --> 00:02:56.180
NGOs and companies
in over 20 countries.

00:02:56.180 --> 00:02:59.820
Not only is he fluent
in German, he's not bad

00:02:59.820 --> 00:03:01.870
at English, as you'll see.

00:03:01.870 --> 00:03:06.420
And what, three or four
other languages, I think.

00:03:06.420 --> 00:03:10.060
To start off this conversation,
by the way, we'll leave plenty

00:03:10.060 --> 00:03:12.050
of time for questions
at the end.

00:03:12.050 --> 00:03:14.490
But to start off the
conversation, I'm going to quote

00:03:14.490 --> 00:03:17.470
from how Martin describes
himself

00:03:17.470 --> 00:03:19.580
when you fish around on the web.

00:03:19.580 --> 00:03:24.740
He says, I am pursuing a
multidisciplinary approach

00:03:24.740 --> 00:03:28.110
to understanding the role of
information, communication

00:03:28.110 --> 00:03:30.110
and knowledge in the development

00:03:30.110 --> 00:03:34.030
of complex socio-technological
systems.

00:03:34.030 --> 00:03:36.050
He also claims to be a
really good translator,

00:03:36.050 --> 00:03:38.540
so you're going to
start with that.

00:03:38.540 --> 00:03:39.840
What does it mean --

00:03:39.840 --> 00:03:42.710
&gt;&gt; Martin Hilbert: To
German or to Python?

00:03:42.710 --> 00:03:44.010
&gt;&gt; John Haskell:
We're hoping English.

00:03:44.010 --> 00:03:45.310
&gt;&gt; Martin Hilbert: Okay.

00:03:45.310 --> 00:03:49.990
The translator, what
does the sentence me?

00:03:49.990 --> 00:03:52.730
&gt;&gt; John Haskell: Yeah, what
does the sentence mean?

00:03:52.730 --> 00:03:56.070
&gt;&gt; Martin Hilbert:
Well, for me, yeah.

00:03:56.070 --> 00:03:59.380
Both of them go very
much together.

00:03:59.380 --> 00:04:01.000
I mean the fact that
it's complex systems

00:04:01.000 --> 00:04:02.760
or a more disciplinary
approach is kind

00:04:02.760 --> 00:04:04.470
of like the same thing, right.

00:04:04.470 --> 00:04:07.320
So as for me, I see myself
as a social scientist.

00:04:07.320 --> 00:04:09.660
And they are not really, I mean,

00:04:09.660 --> 00:04:11.520
there are not really a
lot of boundaries for me.

00:04:11.520 --> 00:04:14.320
So I'm interested at how digital
technology changes in society.

00:04:14.320 --> 00:04:17.280
If it might be for society
to satisfy the needs,

00:04:17.280 --> 00:04:20.340
the economics, how
society governs itself,

00:04:20.340 --> 00:04:23.830
political science, or the little
quirks that develops, you know,

00:04:23.830 --> 00:04:25.470
anthropology, sociology.

00:04:25.470 --> 00:04:28.770
So that doesn't, and there comes
a complex systems approach comes

00:04:28.770 --> 00:04:30.070
to that.

00:04:30.070 --> 00:04:31.370
That there's an underlying
mechanism

00:04:31.370 --> 00:04:33.990
that creates these emergent
phenomena on a higher level.

00:04:33.990 --> 00:04:35.290
&gt;&gt; John Haskell: Okay.

00:04:35.290 --> 00:04:36.590
&gt;&gt; Martin Hilbert: This
translator thing is more

00:04:36.590 --> 00:04:37.890
like because it came
from the United Nations,

00:04:37.890 --> 00:04:41.660
so I'm just in academia now
for three years or four years.

00:04:41.660 --> 00:04:48.290
So before that, I was for 15
years at the secretariat there.

00:04:48.290 --> 00:04:51.530
And yeah, I did research there
as well at the UN Secretariat.

00:04:51.530 --> 00:04:54.220
It was a quite successful
career.

00:04:54.220 --> 00:04:56.770
I had, at the end I
had lifelong employment

00:04:56.770 --> 00:05:01.440
with lifelong global
diplomatic immunity

00:05:01.440 --> 00:05:04.320
and what they call
golden handcuffs, right.

00:05:04.320 --> 00:05:10.330
So I found a little way to,
yeah, I stepped out of that

00:05:10.330 --> 00:05:14.840
and I retired then early in
my late 30s, in my mid-30s,

00:05:14.840 --> 00:05:16.810
and joined the University
of California.

00:05:16.810 --> 00:05:18.560
I mean that's, you
know, the most complete,

00:05:18.560 --> 00:05:23.270
most comprehensive tertiary
educational system in the world.

00:05:23.270 --> 00:05:25.160
And it's a really nice
playground because I wanted

00:05:25.160 --> 00:05:26.460
to be part of this revolution.

00:05:26.460 --> 00:05:29.370
You know, the digital
revolution, more closely to it

00:05:29.370 --> 00:05:30.670
in California certainly.

00:05:30.670 --> 00:05:32.170
&gt;&gt; John Haskell: There's
probably a lot of people

00:05:32.170 --> 00:05:35.860
in that system that think they
have golden handcuffs too.

00:05:35.860 --> 00:05:37.160
&gt;&gt; Martin Hilbert: Yes.

00:05:37.160 --> 00:05:39.020
&gt;&gt; John Haskell: So let's
get into the substance,

00:05:39.020 --> 00:05:43.370
and we're going to start
real simple, which suits me,

00:05:43.370 --> 00:05:47.430
is that I want you
to define big data,

00:05:47.430 --> 00:05:49.490
just so we make sure we know
exactly what we're talking

00:05:49.490 --> 00:05:55.200
about, particularly insofar
as big data about us.

00:05:55.200 --> 00:05:56.500
&gt;&gt; Martin Hilbert: Right.

00:05:56.500 --> 00:05:57.800
&gt;&gt; John Haskell: You
know, that affects us

00:05:57.800 --> 00:06:00.650
as citizens and as individuals.

00:06:00.650 --> 00:06:03.240
&gt;&gt; Martin Hilbert:
Yeah, so big data,

00:06:03.240 --> 00:06:06.020
for me for the social
sciences, I would, that concerns

00:06:06.020 --> 00:06:08.320
to us I think the term
is not very lucky.

00:06:08.320 --> 00:06:10.560
And it doesn't always
have to be big in order

00:06:10.560 --> 00:06:12.170
to make the difference
in the phenomena

00:06:12.170 --> 00:06:13.470
that we're talking about.

00:06:13.470 --> 00:06:15.400
It's basically, for all the
social science purposes,

00:06:15.400 --> 00:06:18.240
you can just replace the
word with digital footprint.

00:06:18.240 --> 00:06:19.540
And then if you do that,

00:06:19.540 --> 00:06:20.910
you know much better
what it's about.

00:06:20.910 --> 00:06:22.780
And there are some
other characteristics.

00:06:22.780 --> 00:06:25.080
May we just maybe start
with a digital footprint.

00:06:25.080 --> 00:06:28.130
Maybe can pull my computer up.

00:06:28.130 --> 00:06:31.400
If you have here, if you have
it on your phone as well,

00:06:31.400 --> 00:06:32.940
and you have a Google
account logged in,

00:06:32.940 --> 00:06:35.510
you can just Google
for timeline.

00:06:35.510 --> 00:06:41.330
And then your Google
timeline will come up.

00:06:41.330 --> 00:06:43.480
If you didn't change
the default settings,

00:06:43.480 --> 00:06:45.730
which I guess nobody
did really, right.

00:06:45.730 --> 00:06:47.030
Anyone looked at that stuff?

00:06:47.030 --> 00:06:48.330
No. All right.

00:06:48.330 --> 00:06:49.630
So then you can see here,
and you can see exactly

00:06:49.630 --> 00:06:52.680
where you've been during
the last three years.

00:06:52.680 --> 00:06:56.000
So this is where I have been
during the last three years,

00:06:56.000 --> 00:06:57.300
right.

00:06:57.300 --> 00:07:00.840
So you can see here, I've
been quite all over the place.

00:07:00.840 --> 00:07:04.020
You can also zoom in and
you can go very closely

00:07:04.020 --> 00:07:14.330
and you can see particular, I
don't know, let's take 2014,

00:07:14.330 --> 00:07:19.530
November, whatever, '14.

00:07:19.530 --> 00:07:21.860
Right. And you can
see here where I was.

00:07:21.860 --> 00:07:23.400
I have no idea where I was.

00:07:23.400 --> 00:07:24.700
I was in London.

00:07:24.700 --> 00:07:26.000
Check that out.

00:07:26.000 --> 00:07:29.900
And you can see exactly
where you went

00:07:29.900 --> 00:07:31.200
around and where you walk.

00:07:31.200 --> 00:07:32.540
You can even have
a little animation

00:07:32.540 --> 00:07:33.840
and see where you will.

00:07:33.840 --> 00:07:35.140
You leave this digital
footprint behind

00:07:35.140 --> 00:07:37.540
because basically you have
this tracker right, and you're,

00:07:37.540 --> 00:07:39.350
in your pocket that tracks you.

00:07:39.350 --> 00:07:42.730
And even so you don't remember,
the digital footprint remembers.

00:07:42.730 --> 00:07:45.750
For any practical purposes,
if you're not absolutely sure,

00:07:45.750 --> 00:07:50.080
don't share this trick with
your significant other.

00:07:50.080 --> 00:07:51.380
[Inaudible].

00:07:51.380 --> 00:07:54.070
&gt;&gt; John Haskell: I notice you
were in South Beach there.

00:07:54.070 --> 00:07:57.110
Exactly that was.

00:07:57.110 --> 00:07:58.410
&gt;&gt; Martin Hilbert: So, yeah.

00:07:58.410 --> 00:08:01.860
And so yeah, that
was a diversion.

00:08:01.860 --> 00:08:03.160
So that's the digital footprint.

00:08:03.160 --> 00:08:04.460
There are some other
characteristics.

00:08:04.460 --> 00:08:06.890
So use sometimes the big data
that describe with the V's

00:08:06.890 --> 00:08:09.290
with the three or four
V's that they have.

00:08:09.290 --> 00:08:10.790
So there's the volume.

00:08:10.790 --> 00:08:13.400
There's a big, I'm not
so good with the V's.

00:08:13.400 --> 00:08:15.780
I'm just going, I'm giving you
four or five characteristics.

00:08:15.780 --> 00:08:17.080
We can turn this off again.

00:08:17.080 --> 00:08:19.350
The four or five
characteristics,

00:08:19.350 --> 00:08:20.670
so one is the digital footprint.

00:08:20.670 --> 00:08:22.830
The other one is that
the digital footprint is

00:08:22.830 --> 00:08:24.300
always messy.

00:08:24.300 --> 00:08:26.400
So it's never complete, right.

00:08:26.400 --> 00:08:29.640
It's not like a traditional data
form maybe than maybe a complete

00:08:29.640 --> 00:08:32.120
and kind of like every row and
every column is filled out.

00:08:32.120 --> 00:08:34.420
So there comes this technique
that's called data fusion,

00:08:34.420 --> 00:08:36.270
which is very characteristic
for working with these kind

00:08:36.270 --> 00:08:37.570
of digital footprints.

00:08:37.570 --> 00:08:40.080
So we have different sources
that we basically mix together,

00:08:40.080 --> 00:08:41.660
and the main technology
that drove all

00:08:41.660 --> 00:08:44.660
of that is called Hadoop, is
basically based on this as well.

00:08:44.660 --> 00:08:47.380
It crosses the decentralized and
then brings it together again.

00:08:47.380 --> 00:08:49.580
And we can really nicely
mix different sources.

00:08:49.580 --> 00:08:50.880
Which we need to.

00:08:50.880 --> 00:08:53.070
Because not everybody is
going to be on Facebook.

00:08:53.070 --> 00:08:55.630
And not everybody is going
to even be on Twitter.

00:08:55.630 --> 00:08:57.190
But then you're going to have
a credit card or you're going

00:08:57.190 --> 00:08:59.570
to have a, like somehow
we're going to get you.

00:08:59.570 --> 00:09:02.490
And then we try to fill out
all these different holes

00:09:02.490 --> 00:09:05.250
and then complement
different data sources.

00:09:05.250 --> 00:09:06.870
&gt;&gt; John Haskell: So what's
the, if you had to put it

00:09:06.870 --> 00:09:10.170
in a capsule, like distill like,

00:09:10.170 --> 00:09:13.050
what are the various
ways it's used

00:09:13.050 --> 00:09:16.510
that might affect say
any individual's life?

00:09:16.510 --> 00:09:18.800
&gt;&gt; Martin Hilbert: So there's
data fusion, for example,

00:09:18.800 --> 00:09:21.300
it gives us much better
predictions because we can,

00:09:21.300 --> 00:09:24.470
for example, you always
had data about this stuff,

00:09:24.470 --> 00:09:25.770
for example, the FICO score.

00:09:25.770 --> 00:09:28.010
If you were to get a credit,
right, you had the FICO score

00:09:28.010 --> 00:09:29.660
and you were always
told, get a credit card.

00:09:29.660 --> 00:09:31.490
You have to build up your
FICO score and whatever.

00:09:31.490 --> 00:09:33.040
Nowadays, we use
this data fusion.

00:09:33.040 --> 00:09:35.510
We mix it with many
other indicators.

00:09:35.510 --> 00:09:37.820
For example, how you
pay your cellphone bill

00:09:37.820 --> 00:09:40.770
and even the way you move
the mouse when you go

00:09:40.770 --> 00:09:43.690
over the bank's webpage, right,
is a very high predictive power

00:09:43.690 --> 00:09:45.600
if you're going to pay
your loan back or not.

00:09:45.600 --> 00:09:49.390
And with that, they can
predict default rate 30% better.

00:09:49.390 --> 00:09:53.530
That means they can offer
credit 30% cheaper, right.

00:09:53.530 --> 00:09:55.890
So, which is a big gain, right,

00:09:55.890 --> 00:09:58.330
so by complementing
these different sources.

00:09:58.330 --> 00:10:00.600
But also another
characteristic of big data is

00:10:00.600 --> 00:10:03.040
that often it's often
in real time.

00:10:03.040 --> 00:10:05.480
So for example, when you call
a call center, the call center

00:10:05.480 --> 00:10:07.560
of your choice, you often
hear this message, right,

00:10:07.560 --> 00:10:09.920
this call might be recorded for
quality and blah, blah, blah.

00:10:09.920 --> 00:10:13.130
And you always think it's
the head of human resources

00:10:13.130 --> 00:10:16.000
that listens in and makes
sure you're treated right.

00:10:16.000 --> 00:10:18.980
No, it's not.

00:10:18.980 --> 00:10:22.650
In most call centers nowadays,
it's up to 10 million algorithms

00:10:22.650 --> 00:10:24.220
that actually listen
to you while you talk.

00:10:24.220 --> 00:10:27.760
And from the way you talk,
classify your personality

00:10:27.760 --> 00:10:30.720
into five different rubrics,
if you're actions-driven,

00:10:30.720 --> 00:10:32.950
emotions-driven and so forth,
and then they match you

00:10:32.950 --> 00:10:34.650
with somebody on the
call on the other side

00:10:34.650 --> 00:10:37.330
who has the same personality
as you have, right.

00:10:37.330 --> 00:10:39.800
So if you're kind of
emotions-driven, they match you,

00:10:39.800 --> 00:10:41.100
the people on the
other side also,

00:10:41.100 --> 00:10:42.400
they don't even know about it.

00:10:42.400 --> 00:10:43.700
You know, they don't
know about that.

00:10:43.700 --> 00:10:45.960
They also just have the
person already classified.

00:10:45.960 --> 00:10:47.630
But then if you're
actions-driven, for example,

00:10:47.630 --> 00:10:48.930
I am very actions-driven, right.

00:10:48.930 --> 00:10:51.020
A caller calls in, and
I just want my coupon.

00:10:51.020 --> 00:10:52.320
I don't have a lot
of time, right.

00:10:52.320 --> 00:10:55.370
They really messed up for two
days and I want correct my bill

00:10:55.370 --> 00:10:57.090
or give me something, right.

00:10:57.090 --> 00:11:01.110
So if I'm the somebody
who's also actions-driven,

00:11:01.110 --> 00:11:02.410
that's great, you know.

00:11:02.410 --> 00:11:03.710
We understand each other.

00:11:03.710 --> 00:11:06.840
My wife, on the other hand,
she's very emotions-driven.

00:11:06.840 --> 00:11:09.050
She just wants to be
really understood, right.

00:11:09.050 --> 00:11:10.710
So when she calls
in, I'm just like,

00:11:10.710 --> 00:11:12.810
and so what did they say
from the call center?

00:11:12.810 --> 00:11:15.350
And she's like, did they give
us the money back or something?

00:11:15.350 --> 00:11:17.250
And she would say like,
no, you know what?

00:11:17.250 --> 00:11:19.410
No, no, he said he
couldn't help us.

00:11:19.410 --> 00:11:20.710
But you know what?

00:11:20.710 --> 00:11:22.010
He really understood me.

00:11:22.010 --> 00:11:23.750
Like you, I told him
everything with the kids,

00:11:23.750 --> 00:11:26.470
two days without a cellphone,
the doctor, how hard it was,

00:11:26.470 --> 00:11:28.380
and he really, great company.

00:11:28.380 --> 00:11:31.670
Really great, they really
understand us, right.

00:11:31.670 --> 00:11:33.150
So, imagine I would have ended

00:11:33.150 --> 00:11:34.490
up with a person
like that, right.

00:11:34.490 --> 00:11:35.790
It would have been really --

00:11:35.790 --> 00:11:37.090
&gt;&gt; John Haskell: Aggravating.

00:11:37.090 --> 00:11:38.390
&gt;&gt; Martin Hilbert: Yeah.

00:11:38.390 --> 00:11:40.440
So, studies show that it
reduces call duration by half

00:11:40.440 --> 00:11:41.990
and doubles customer
satisfaction.

00:11:41.990 --> 00:11:43.290
&gt;&gt; John Haskell: AT&amp;T
always gives me somebody

00:11:43.290 --> 00:11:44.590
with a southern accent.

00:11:44.590 --> 00:11:45.890
And I don't know.

00:11:45.890 --> 00:11:47.190
&gt;&gt; Martin Hilbert: Maybe
there's something there.

00:11:47.190 --> 00:11:50.090
So the idea is that, you know,
in real time we can also adjust

00:11:50.090 --> 00:11:52.670
to that if we do this data
manipulation in real time.

00:11:52.670 --> 00:11:54.810
And in all of that often
done with machine learning.

00:11:54.810 --> 00:11:56.730
So these are some
characteristics, you know.

00:11:56.730 --> 00:11:59.300
So the digital footprint
basically comes for free,

00:11:59.300 --> 00:12:01.140
which we leave behind for free.

00:12:01.140 --> 00:12:04.160
The data fusion that
they're complementary sources

00:12:04.160 --> 00:12:06.000
that we can bring together.

00:12:06.000 --> 00:12:09.010
The idea of having it often
real time and then doing

00:12:09.010 --> 00:12:10.920
with machine learning, just
throwing machines at it

00:12:10.920 --> 00:12:13.370
and looking for patterns that
we don't even understand.

00:12:13.370 --> 00:12:14.750
But the machines can pick up.

00:12:14.750 --> 00:12:17.470
So machines just from
the way you talk can pick

00:12:17.470 --> 00:12:18.980
up your personality, right.

00:12:18.980 --> 00:12:20.470
By hand it would
be very difficult,

00:12:20.470 --> 00:12:23.350
but they discover
these patterns.

00:12:23.350 --> 00:12:26.370
&gt;&gt; John Haskell: So
to pivot to politics

00:12:26.370 --> 00:12:28.560
and elections, dive right in.

00:12:28.560 --> 00:12:33.760
So, those of us, you know,
some of us can remember back

00:12:33.760 --> 00:12:36.570
when there was different
ways that politicians

00:12:36.570 --> 00:12:38.600
and their consultants
would try to manipulate us

00:12:38.600 --> 00:12:40.120
and influence us
to vote for them.

00:12:40.120 --> 00:12:44.510
In the contemporary era, I think
at least some people would say,

00:12:44.510 --> 00:12:46.970
and maybe this isn't accurate,
and you can straighten me out,

00:12:46.970 --> 00:12:50.720
but the Obama campaign in
'08 made great advances,

00:12:50.720 --> 00:12:53.790
perhaps by plumbing our
social media habits.

00:12:53.790 --> 00:12:55.090
I'm not sure.

00:12:55.090 --> 00:12:59.370
But I'm curious, is, you know,
clearly in the last ten years

00:12:59.370 --> 00:13:03.140
or so there've been
great advances in efforts

00:13:03.140 --> 00:13:06.760
in campaigns to influence us.

00:13:06.760 --> 00:13:11.330
So what kind of advances
have been made since then?

00:13:11.330 --> 00:13:14.620
Like what's going on
now in campaigns to try

00:13:14.620 --> 00:13:17.460
to influence us that's
much more advanced

00:13:17.460 --> 00:13:19.080
than let's say the Obama

00:13:19.080 --> 00:13:21.570
for America campaign
was ten years ago.

00:13:21.570 --> 00:13:22.870
&gt;&gt; Martin Hilbert: Yeah.

00:13:22.870 --> 00:13:24.170
I think in two ways.

00:13:24.170 --> 00:13:25.470
One is it got a little
bit deeper,

00:13:25.470 --> 00:13:26.770
that means we've
got more indicators

00:13:26.770 --> 00:13:28.070
and can make better predictions.

00:13:28.070 --> 00:13:30.800
And second, it's more
fine-grained, the data.

00:13:30.800 --> 00:13:33.430
And second, the samples
are more [inaudible].

00:13:33.430 --> 00:13:36.090
So to pitch kind of the Obama

00:13:36.090 --> 00:13:39.300
versus the election
two years ago in 2016,

00:13:39.300 --> 00:13:43.880
so what Obama did there, he
spent a billion dollars, right,

00:13:43.880 --> 00:13:47.580
to set up this project
called Project [inaudible].

00:13:47.580 --> 00:13:51.250
And the idea was to classify
16 million swing voters.

00:13:51.250 --> 00:13:52.990
And with these 16
million swing voters,

00:13:52.990 --> 00:13:57.580
they all did the data fusion,
so they took a voter registers

00:13:57.580 --> 00:14:00.860
until TV setup boxes, whatever
they could get their hands on,

00:14:00.860 --> 00:14:03.260
right, and put them together
and created this database,

00:14:03.260 --> 00:14:05.700
kind of like patched
together these 16 million.

00:14:05.700 --> 00:14:08.350
And they run models,
and then with that,

00:14:08.350 --> 00:14:13.340
they pitched tailormade messages
to the 16 million, right.

00:14:13.340 --> 00:14:15.180
So for example, on Facebook,
if you think about that,

00:14:15.180 --> 00:14:19.250
just to give you an example,
one of the ways this is done,

00:14:19.250 --> 00:14:22.670
so on Facebook, in political
campaigns it's very easy

00:14:22.670 --> 00:14:24.240
compared to marketing.

00:14:24.240 --> 00:14:25.540
Marketing is much more difficult

00:14:25.540 --> 00:14:27.310
because marketing you want
people to buy one product.

00:14:27.310 --> 00:14:30.680
A politician has kind of like
80 campaign promises, right.

00:14:30.680 --> 00:14:31.980
So it's very easy.

00:14:31.980 --> 00:14:33.670
Like of the 80 it might be
that you're not in agreement

00:14:33.670 --> 00:14:36.100
with the 78, but two
you're going to be

00:14:36.100 --> 00:14:37.400
in agreement with, right.

00:14:37.400 --> 00:14:39.850
So it's very easy to actually
pitch because they just see,

00:14:39.850 --> 00:14:41.900
you know, what are
your interests are,

00:14:41.900 --> 00:14:43.520
and then you pick the two
you're in agreement with.

00:14:43.520 --> 00:14:45.330
Then you create these
filter bubbles, right,

00:14:45.330 --> 00:14:46.630
that's the technical terms,

00:14:46.630 --> 00:14:48.980
where you just show them
always these two messages.

00:14:48.980 --> 00:14:51.520
Or actually, some
of your friends,

00:14:51.520 --> 00:14:52.820
one of them might
have made click

00:14:52.820 --> 00:14:54.510
on the Obama campaign, right.

00:14:54.510 --> 00:14:58.320
A post on their Facebook
page, not even a message

00:14:58.320 --> 00:15:00.580
from the campaign, but like
a New York Times article

00:15:00.580 --> 00:15:02.920
that talks about how
Obama is the hero

00:15:02.920 --> 00:15:04.220
of whatever, green energy.

00:15:04.220 --> 00:15:05.940
And you, after three
months seeing that,

00:15:05.940 --> 00:15:08.430
you think like I don't like
Obama, but he really seems,

00:15:08.430 --> 00:15:11.920
all my friends say this and
that, and then, you know,

00:15:11.920 --> 00:15:15.330
you start to see the
messages that agree with you

00:15:15.330 --> 00:15:17.460
through these photo
bubbles and echo chambers.

00:15:17.460 --> 00:15:18.830
Kind of like your
friends talk about it,

00:15:18.830 --> 00:15:21.640
so there's a combined effect for
the bubbles and echo chambers.

00:15:21.640 --> 00:15:22.940
So what that then creates,

00:15:22.940 --> 00:15:24.740
what they achieved is
they changed the opinion

00:15:24.740 --> 00:15:27.160
of 78% of the voters, right.

00:15:27.160 --> 00:15:28.460
Which is big.

00:15:28.460 --> 00:15:30.870
I mean, yeah, almost 80%

00:15:30.870 --> 00:15:33.050
of the voters they
targeted they changed.

00:15:33.050 --> 00:15:36.550
Now, what happened in the
years since then, first of all,

00:15:36.550 --> 00:15:38.540
the coverage got bigger.

00:15:38.540 --> 00:15:43.800
Instead of going for 16 million,
the company is now really go

00:15:43.800 --> 00:15:46.100
for the 240 million, right.

00:15:46.100 --> 00:15:47.840
So we have a profile of them.

00:15:47.840 --> 00:15:49.140
Or Facebook.

00:15:49.140 --> 00:15:51.220
Facebook, as you heard, Mr.
Zuckerberg two weeks ago he

00:15:51.220 --> 00:15:53.450
and Congress, right, the
famous shadow profile,

00:15:53.450 --> 00:15:55.910
he goes for a profile for
everybody on planet Earth.

00:15:55.910 --> 00:15:58.910
There are these two billion
people that are on Facebook,

00:15:58.910 --> 00:16:02.520
but he has a profile potentially
for everybody on planet Earth,

00:16:02.520 --> 00:16:05.040
so 7 point something
billion, right.

00:16:05.040 --> 00:16:06.840
So that's what they
are going for.

00:16:06.840 --> 00:16:10.770
And then you have much better
coverage, first of all.

00:16:10.770 --> 00:16:15.250
And second of all,
the information

00:16:15.250 --> 00:16:17.410
that we have got deeper, so
the machine learning picked

00:16:17.410 --> 00:16:20.030
up some algorithms that allow
us to make better predictions,

00:16:20.030 --> 00:16:24.890
for example, the famous
psychological profiles

00:16:24.890 --> 00:16:27.920
that Cambridge Analytica
supposedly worked with.

00:16:27.920 --> 00:16:30.770
So what that basically
is, it came from a study

00:16:30.770 --> 00:16:33.200
that a researcher
did in Cambridge,

00:16:33.200 --> 00:16:36.860
a researcher called
Kosinski and his team.

00:16:36.860 --> 00:16:38.240
It was a great study,

00:16:38.240 --> 00:16:41.050
so basically what they
did was they took,

00:16:41.050 --> 00:16:44.930
they gave you a little
survey on Facebook

00:16:44.930 --> 00:16:46.750
that says well test
your personality

00:16:46.750 --> 00:16:48.470
in less than five minutes.

00:16:48.470 --> 00:16:49.770
All right.

00:16:49.770 --> 00:16:51.070
And then it says sponsored
by University of Cambridge.

00:16:51.070 --> 00:16:52.370
You're like great, you know.

00:16:52.370 --> 00:16:54.630
Tens of millions
people participated,

00:16:54.630 --> 00:16:56.820
filled out the survey, and then
they know they extroverted,

00:16:56.820 --> 00:16:58.800
introverted or whatever.

00:16:58.800 --> 00:17:01.040
Between these things, you
know, when you scroll down,

00:17:01.040 --> 00:17:03.840
the stuff you never read,
right, the terms of agreement,

00:17:03.840 --> 00:17:05.140
it also gave them permission

00:17:05.140 --> 00:17:06.930
to scrape all their
Facebook history.

00:17:06.930 --> 00:17:09.610
So okay, so now these
researchers they had the

00:17:09.610 --> 00:17:11.840
Facebook history and the
psychological profile

00:17:11.840 --> 00:17:13.200
because you gave both of them.

00:17:13.200 --> 00:17:15.730
Then they had a machine learning
algorithm learning that,

00:17:15.730 --> 00:17:19.290
and the idea was, how many
Facebook likes do you need

00:17:19.290 --> 00:17:22.250
in order to predict the
personality of a person?

00:17:22.250 --> 00:17:23.550
It's a machine learning
problem, right.

00:17:23.550 --> 00:17:24.850
They have the personality.

00:17:24.850 --> 00:17:26.330
They have their Facebook, and it
just says how many do you need.

00:17:26.330 --> 00:17:30.320
It turned out like with 100
likes you could predict the

00:17:30.320 --> 00:17:32.510
personality extremely
accurately.

00:17:32.510 --> 00:17:36.570
And then they ask the
significant other, right.

00:17:36.570 --> 00:17:39.020
So your spouse or your
mother or your father

00:17:39.020 --> 00:17:40.390
or your siblings, right.

00:17:40.390 --> 00:17:41.870
Your family, your best friend.

00:17:41.870 --> 00:17:43.510
Fill out this personality
profile

00:17:43.510 --> 00:17:44.870
in the name of that person.

00:17:44.870 --> 00:17:47.320
All right, I mean your mother
should know you, right.

00:17:47.320 --> 00:17:52.120
And it turns out that with 150
likes, the algorithm was better

00:17:52.120 --> 00:17:55.100
to predict your personality
than your mother, right.

00:17:55.100 --> 00:17:57.920
And then they ask you
yourself and what do you think?

00:17:57.920 --> 00:17:59.790
Are you introverted,
extraverted, what do you think

00:17:59.790 --> 00:18:01.090
about your personality?

00:18:01.090 --> 00:18:03.830
It turns out 200, 250 likes,
the algorithm was better

00:18:03.830 --> 00:18:07.100
than you yourself in predicting
your personality, right.

00:18:07.100 --> 00:18:08.460
And that was a big study.

00:18:08.460 --> 00:18:12.370
Kosinski got a tenure
track on Stanford for that,

00:18:12.370 --> 00:18:16.720
so he moved Cambridge to
Stanford and went to California.

00:18:16.720 --> 00:18:19.530
And then this method was
around in some other people

00:18:19.530 --> 00:18:22.510
in Cambridge unlinked to
Kosinski, right, started to work

00:18:22.510 --> 00:18:24.090
with it, and that's where
this company came from,

00:18:24.090 --> 00:18:25.390
Cambridge Analytica.

00:18:25.390 --> 00:18:26.760
And they tried to
do the same thing.

00:18:26.760 --> 00:18:30.720
Now, it's unclear how successful
they were in that actually,

00:18:30.720 --> 00:18:32.540
in doing the psychological
profile.

00:18:32.540 --> 00:18:34.800
But we know there have been
some attempts, for example,

00:18:34.800 --> 00:18:38.090
during the third campaign
between Clinton and Trump.

00:18:38.090 --> 00:18:41.140
We know that Trump
sent some kind

00:18:41.140 --> 00:18:43.230
of message, for example,
I think.

00:18:43.230 --> 00:18:47.710
I defend the right to
bear arms or whatever.

00:18:47.710 --> 00:18:53.280
And they sent 175,000 different
versions of this sentence out.

00:18:53.280 --> 00:18:55.150
So the sentence is
the same, but you kind

00:18:55.150 --> 00:18:58.350
of like personalize it
according to people's fears,

00:18:58.350 --> 00:18:59.960
because that's the
easiest thing, right.

00:18:59.960 --> 00:19:01.260
That's what hits home most.

00:19:01.260 --> 00:19:06.200
So if there is a single mother,
they would pitch this message

00:19:06.200 --> 00:19:07.960
with a picture of
a burglar, right.

00:19:07.960 --> 00:19:09.560
And they could even find you
in the picture of a burglar

00:19:09.560 --> 00:19:11.600
in a house that is kind
of like close by that kind

00:19:11.600 --> 00:19:13.680
of like subconsciously
rings a bell, right.

00:19:13.680 --> 00:19:18.320
And then, if there's
a father sports father

00:19:18.320 --> 00:19:20.650
with three sons they will pitch
this message with somebody

00:19:20.650 --> 00:19:21.950
who was hunting, right.

00:19:21.950 --> 00:19:24.320
So they had 175,000
different versions,

00:19:24.320 --> 00:19:27.180
and the idea is potentially you
have tailor-made messages per

00:19:27.180 --> 00:19:28.920
person and can send that out.

00:19:28.920 --> 00:19:30.230
&gt;&gt; John Haskell:
Right, because you know,

00:19:30.230 --> 00:19:32.560
I did a lot of research
on consultants

00:19:32.560 --> 00:19:34.230
who were working in the 70s.

00:19:34.230 --> 00:19:38.780
And they had it broken down
at best to 480, you know,

00:19:38.780 --> 00:19:40.900
categories of people, and
you try to pitch that way.

00:19:40.900 --> 00:19:42.200
So there's --

00:19:42.200 --> 00:19:43.500
&gt;&gt; Martin Hilbert: Right.

00:19:43.500 --> 00:19:44.800
&gt;&gt; John Haskell: It would be
interesting to know exactly,

00:19:44.800 --> 00:19:46.100
you know, whether you could
measure how much better

00:19:46.100 --> 00:19:47.400
you're doing.

00:19:47.400 --> 00:19:48.700
But I'm sure it's a
measurable amount better

00:19:48.700 --> 00:19:50.590
when you can be that refined.

00:19:50.590 --> 00:19:51.890
&gt;&gt; Martin Hilbert: Yeah.

00:19:51.890 --> 00:19:53.900
&gt;&gt; John Haskell: So it's, a
lot of what you're talking

00:19:53.900 --> 00:19:57.440
about isn't just
figuring out who we are

00:19:57.440 --> 00:20:00.030
through the digital footprint
but then coming back at us.

00:20:00.030 --> 00:20:03.240
So you've got this recursive
model where they're coming back

00:20:03.240 --> 00:20:08.050
at us and hitting
us with content.

00:20:08.050 --> 00:20:09.350
&gt;&gt; Martin Hilbert: Right.

00:20:09.350 --> 00:20:10.650
&gt;&gt; John Haskell: And
maybe even news, right,

00:20:10.650 --> 00:20:13.200
to try to have an impact
on our vote in this case.

00:20:13.200 --> 00:20:14.840
We'll talk about
institutions in a second,

00:20:14.840 --> 00:20:16.140
but our vote in this case.

00:20:16.140 --> 00:20:18.060
Is that a good way
to think about it?

00:20:18.060 --> 00:20:19.420
&gt;&gt; Martin Hilbert:
Yeah, yeah, absolutely.

00:20:19.420 --> 00:20:21.750
And that's basically what these
social media companies do,

00:20:21.750 --> 00:20:24.970
right, to keep with this
topic of Cambridge Analytica

00:20:24.970 --> 00:20:26.570
since it's fresh
on people's mind.

00:20:26.570 --> 00:20:28.870
People get very upset that
they've got, I don't know,

00:20:28.870 --> 00:20:32.940
50 million or whatever, 80
million Facebook profiles.

00:20:32.940 --> 00:20:34.370
It doesn't really matter

00:20:34.370 --> 00:20:35.760
if Cambridge Analytica
has 50 million,

00:20:35.760 --> 00:20:37.060
80 million, 100 million.

00:20:37.060 --> 00:20:38.360
That's not even the
discussion, you know.

00:20:38.360 --> 00:20:40.880
Facebook has two
billion profiles

00:20:40.880 --> 00:20:43.220
and does exactly the
same thing, you know.

00:20:43.220 --> 00:20:46.900
The Trump campaign
spent $17 million

00:20:46.900 --> 00:20:49.800
on Facebook doing the same
thing officially, right.

00:20:49.800 --> 00:20:51.910
So you don't need
Cambridge Analytica for that.

00:20:51.910 --> 00:20:53.630
So the Facebook actually
doing the election,

00:20:53.630 --> 00:20:54.930
that's what Facebook
does, right.

00:20:54.930 --> 00:20:58.220
It gets to know you, and
it's a commercial company

00:20:58.220 --> 00:21:00.050
and it tries to sell
you the ads.

00:21:00.050 --> 00:21:02.260
And the clients can also
be political parties.

00:21:02.260 --> 00:21:05.640
So actually, they set up a team
during the election and went

00:21:05.640 --> 00:21:07.410
to the presidential candidates.

00:21:07.410 --> 00:21:11.200
All presidential campaigns,
also from the primaries,

00:21:11.200 --> 00:21:15.020
spent $1.4 billion on these
kind of social media ads.

00:21:15.020 --> 00:21:16.730
So that's a very
lucrative market, right.

00:21:16.730 --> 00:21:20.840
So they send out those specific
teams that went to the parties

00:21:20.840 --> 00:21:23.160
and to the candidates and
said we going to help you.

00:21:23.160 --> 00:21:26.140
The Trump campaign was
a little less organized.

00:21:26.140 --> 00:21:30.400
That's why they spent
$70 million on it and put

00:21:30.400 --> 00:21:32.120
like six million ads out.

00:21:32.120 --> 00:21:34.210
The Clinton campaign said
no, no, we are covered.

00:21:34.210 --> 00:21:35.510
We do that ourselves.

00:21:35.510 --> 00:21:40.640
They only put 60,000 ads
out, and well people say,

00:21:40.640 --> 00:21:41.940
you know what happened.

00:21:41.940 --> 00:21:45.360
So Facebook clearly, and it's
not that Facebook went to Trump.

00:21:45.360 --> 00:21:46.920
No, they just go to
everyone who wants

00:21:46.920 --> 00:21:48.220
to do business with them, right.

00:21:48.220 --> 00:21:50.650
It's a very lucrative
market, right, $70 million.

00:21:50.650 --> 00:21:52.420
And then they did
exactly the same thing.

00:21:52.420 --> 00:21:54.040
Now the question is,
what's the difference

00:21:54.040 --> 00:21:56.120
if Cambridge Analytica does
it or Facebook does it?

00:21:56.120 --> 00:21:57.590
There's absolutely
no difference, right.

00:21:57.590 --> 00:22:01.090
The question is rather,
do we, is this kind

00:22:01.090 --> 00:22:03.170
of model that's developed
for marketing, you know,

00:22:03.170 --> 00:22:05.840
commercial marketing,
should we allow that or not

00:22:05.840 --> 00:22:07.290
for political campaigns?

00:22:07.290 --> 00:22:10.510
You know, political campaigns
in many companies are regulated,

00:22:10.510 --> 00:22:13.320
in this case, on social media
they are not regulated, right.

00:22:13.320 --> 00:22:15.910
Nobody even has to tell you
even on TV you have to say,

00:22:15.910 --> 00:22:17.330
also here in this
country you have

00:22:17.330 --> 00:22:19.170
to say it's a political
ad or not.

00:22:19.170 --> 00:22:21.370
And who sponsored
it on social media.

00:22:21.370 --> 00:22:22.700
Nothing like this happens.

00:22:22.700 --> 00:22:25.070
And they just went in and
made a whole lot of money,

00:22:25.070 --> 00:22:26.370
much more money is made there.

00:22:26.370 --> 00:22:29.780
In the Obama, even in
the Obama 2012 campaign,

00:22:29.780 --> 00:22:33.360
more money was already spent
on social media campaigning

00:22:33.360 --> 00:22:35.660
than was on TV campaigning,
right.

00:22:35.660 --> 00:22:38.180
On big data was more
spent than entire TV,

00:22:38.180 --> 00:22:39.720
as well as completely
unregulated.

00:22:39.720 --> 00:22:42.640
So the question is not does
Cambridge Analytica or not,

00:22:42.640 --> 00:22:46.260
the question is, if we
should, if we want, you know,

00:22:46.260 --> 00:22:48.480
social media with that kind of
grand analogy of information

00:22:48.480 --> 00:22:51.830
about us get into the business
of doing democratic campaigning.

00:22:51.830 --> 00:22:53.340
&gt;&gt; John Haskell:
So if you stipulate

00:22:53.340 --> 00:22:58.420
that as citizens we're
dependent on some form of media,

00:22:58.420 --> 00:23:01.430
forms of media, to get our
information to make decisions

00:23:01.430 --> 00:23:06.340
in voting or to write a letter
to a congressman or whatever.

00:23:06.340 --> 00:23:09.000
How radical is the
change compared

00:23:09.000 --> 00:23:13.510
to however many years ago in
the way we're getting news?

00:23:13.510 --> 00:23:16.420
You know, because I can tell you

00:23:16.420 --> 00:23:18.920
that my dad just got it
from Walter Cronkite.

00:23:18.920 --> 00:23:21.390
Maybe a lot of you
haven't heard of him.

00:23:21.390 --> 00:23:23.230
He was CBS News.

00:23:23.230 --> 00:23:25.360
But, and that was it.

00:23:25.360 --> 00:23:27.980
You know, maybe he read a
Cleveland newspaper, you know,

00:23:27.980 --> 00:23:31.070
it limits what you
can get out of that.

00:23:31.070 --> 00:23:32.370
And that was it.

00:23:32.370 --> 00:23:34.530
But today, obviously,
that's an extreme.

00:23:34.530 --> 00:23:36.770
But that isn't that many
years ago, you know,

00:23:36.770 --> 00:23:38.250
in the greater sweep of things.

00:23:38.250 --> 00:23:40.900
So how radical is the change
now in terms of where any

00:23:40.900 --> 00:23:44.480
of us might be receiving news
that would have an impact

00:23:44.480 --> 00:23:47.270
on our decision making
in politics.

00:23:47.270 --> 00:23:48.700
&gt;&gt; Martin Hilbert: It's
still changing a lot,

00:23:48.700 --> 00:23:50.120
so even the last year still.

00:23:50.120 --> 00:23:55.240
So last year, 60% of
Americans received news

00:23:55.240 --> 00:23:56.540
through social media.

00:23:56.540 --> 00:23:58.710
And this year it was 70%, right.

00:23:58.710 --> 00:24:00.010
&gt;&gt; John Haskell: Is it

00:24:00.010 --> 00:24:01.310
that they're getting
most of their news?

00:24:01.310 --> 00:24:03.650
&gt;&gt; Martin Hilbert: Most
of the news is about 50%.

00:24:03.650 --> 00:24:07.400
So the 20% like they
say not so much.

00:24:07.400 --> 00:24:10.440
But you know, one glimpse
of something is enough

00:24:10.440 --> 00:24:11.740
to change an opinion, right.

00:24:11.740 --> 00:24:14.440
So even if it's not so much,
you cannot get this image

00:24:14.440 --> 00:24:16.620
out of your head or this
video out of your head.

00:24:16.620 --> 00:24:18.530
This video might be fake
or not fake, you know,

00:24:18.530 --> 00:24:20.210
it might be made by AI.

00:24:20.210 --> 00:24:21.510
But you cannot get
it out of your head.

00:24:21.510 --> 00:24:22.810
So even if you don't
do it a lot,

00:24:22.810 --> 00:24:24.810
it still has a lot
of influence, right.

00:24:24.810 --> 00:24:26.960
So 70%, and it's
still increasing.

00:24:26.960 --> 00:24:30.320
And especially last year we
saw a big increase in people

00:24:30.320 --> 00:24:35.450
over 50 years of age, they
increased a lot, and nonwhites.

00:24:35.450 --> 00:24:37.880
So nonwhites it's over 75%,

00:24:37.880 --> 00:24:42.310
like 75% that get their
news on social media.

00:24:42.310 --> 00:24:43.700
And the less educated.

00:24:43.700 --> 00:24:45.910
So the ones without
a college degree

00:24:45.910 --> 00:24:49.250
that also was still increasing
or is still increasing for that.

00:24:49.250 --> 00:24:51.460
And the ones with above
a bachelor degree,

00:24:51.460 --> 00:24:52.840
that means with a
postsecondary degree,

00:24:52.840 --> 00:24:54.840
it was decreasing slightly.

00:24:54.840 --> 00:24:58.330
But it decreased, so it's
down to 62% of something.

00:24:58.330 --> 00:25:00.510
But it's all in the same.

00:25:00.510 --> 00:25:01.810
&gt;&gt; John Haskell: So it's not

00:25:01.810 --> 00:25:03.110
that they're just
getting some news,

00:25:03.110 --> 00:25:04.900
a lot of people are
getting most of their news.

00:25:04.900 --> 00:25:06.200
&gt;&gt; Martin Hilbert: Yeah,
I think 50% get most

00:25:06.200 --> 00:25:07.570
of the news from there, yeah.

00:25:07.570 --> 00:25:09.010
&gt;&gt; John Haskell:
And that's going

00:25:09.010 --> 00:25:11.970
to differ you're saying based
on kind of a socioeconomic --

00:25:11.970 --> 00:25:13.810
&gt;&gt; Martin Hilbert: Based
on socioeconomic, even so,

00:25:13.810 --> 00:25:16.150
I mean I said these are
differences, small differences.

00:25:16.150 --> 00:25:19.440
It means like if you're highly
educated and you're white,

00:25:19.440 --> 00:25:21.350
you're like 60% of them.

00:25:21.350 --> 00:25:25.010
And if you're nonwhite and
less educated, it's 70%.

00:25:25.010 --> 00:25:26.900
But it's still like
it's in this range.

00:25:26.900 --> 00:25:29.030
It's not such a significant
difference,

00:25:29.030 --> 00:25:30.400
it's a really mass phenomenon.

00:25:30.400 --> 00:25:32.070
Now, it comes through
different channels.

00:25:32.070 --> 00:25:34.220
That makes actually
more a difference.

00:25:34.220 --> 00:25:35.990
So the average American uses

00:25:35.990 --> 00:25:38.710
about eight social
media a day, right.

00:25:38.710 --> 00:25:41.950
So, and some of them are
much more news-focused.

00:25:41.950 --> 00:25:46.270
Twitter, of course, right, the
social media of the president.

00:25:46.270 --> 00:25:48.820
Then Facebook and Reddit.

00:25:48.820 --> 00:25:50.310
So these are very political.

00:25:50.310 --> 00:25:53.530
So we have a lot
of people, say 60,

00:25:53.530 --> 00:25:56.810
70% of what's happening
there is political.

00:25:56.810 --> 00:25:59.940
But even others,
20, 30%, 20, 30,

00:25:59.940 --> 00:26:02.750
40% of other social
medias is news.

00:26:02.750 --> 00:26:05.930
For example, in YouTube,

00:26:05.930 --> 00:26:08.730
they made a big effort
recently to bring up the news.

00:26:08.730 --> 00:26:13.100
And Instagram, Snapchat,
Tumblr as well.

00:26:13.100 --> 00:26:14.710
Snapchat made a big move

00:26:14.710 --> 00:26:17.510
in recent months getting
CNN and others involved.

00:26:17.510 --> 00:26:19.820
You know, they have
special news shows on there.

00:26:19.820 --> 00:26:21.120
But even WhatsApp.

00:26:21.120 --> 00:26:22.690
I mean 20% of people
say that yes,

00:26:22.690 --> 00:26:25.310
some kind of news they often
get from WhatsApp, right.

00:26:25.310 --> 00:26:27.220
So you get it through all
different kind of channels.

00:26:27.220 --> 00:26:28.820
But within these
different social media,

00:26:28.820 --> 00:26:33.380
yeah some of WhatsApp is
80% conversation, 20% news.

00:26:33.380 --> 00:26:34.680
&gt;&gt; John Haskell: So if
scholars got to the point

00:26:34.680 --> 00:26:38.670
where they're making
judgments, I mean, clearly,

00:26:38.670 --> 00:26:41.680
we're being influenced
because of our, you know,

00:26:41.680 --> 00:26:44.610
digital footprint in
a thorough-going way.

00:26:44.610 --> 00:26:46.240
Or people are trying
to at least.

00:26:46.240 --> 00:26:47.540
&gt;&gt; Martin Hilbert: Right.

00:26:47.540 --> 00:26:48.840
&gt;&gt; John Haskell: So have
scholars made any judgments

00:26:48.840 --> 00:26:52.180
about whether that's
affecting how we're governed?

00:26:52.180 --> 00:26:56.300
Or in what ways it's
affecting how we're governed.

00:26:56.300 --> 00:26:57.860
&gt;&gt; Martin Hilbert: Yeah,
I think it affects it

00:26:57.860 --> 00:27:00.150
in many different subtle ways.

00:27:00.150 --> 00:27:02.870
And it's difficult to
actually to point to anything.

00:27:02.870 --> 00:27:08.320
I mean, we saw clearly in the
aftermath of the 2016 election,

00:27:08.320 --> 00:27:10.590
even so we still don't really
understand what's going on.

00:27:10.590 --> 00:27:13.890
But yeah. Putting the question,

00:27:13.890 --> 00:27:18.950
the answer in a nutshell
I think, you know, in,

00:27:18.950 --> 00:27:20.320
it's really like this.

00:27:20.320 --> 00:27:23.410
In 500 years, you know,
historians will come back

00:27:23.410 --> 00:27:27.780
to planet Earth and look what
happens during these decades

00:27:27.780 --> 00:27:30.920
where we digitalized the
world's information stockpile

00:27:30.920 --> 00:27:33.640
in a generation, which
is our generation, right.

00:27:33.640 --> 00:27:36.220
They will find that it
profoundly changed the economy

00:27:36.220 --> 00:27:38.840
and healthcare and
education and whatever.

00:27:38.840 --> 00:27:40.690
But I think the most
profound change they will find

00:27:40.690 --> 00:27:42.900
in retrospect has been the way
that we're governing ourselves.

00:27:42.900 --> 00:27:44.440
So, to answer your
question, yes,

00:27:44.440 --> 00:27:45.740
I think the most profound way.

00:27:45.740 --> 00:27:47.700
But I think you can only see
it from this bird's eye view.

00:27:47.700 --> 00:27:50.090
Because it's like we're a
part of this process, right.

00:27:50.090 --> 00:27:53.550
So I can go in, and I
can digitalize a company,

00:27:53.550 --> 00:27:56.160
or I can digitalize a
school or a university.

00:27:56.160 --> 00:27:57.810
Or I can digitalize even
an entire government.

00:27:57.810 --> 00:27:59.310
And I've been involved
in these projects.

00:27:59.310 --> 00:28:00.730
It's basically a
digitalization project.

00:28:00.730 --> 00:28:02.030
That's okay.

00:28:02.030 --> 00:28:04.020
But digitalizing let's
say, you know, society

00:28:04.020 --> 00:28:05.320
and how it forms
its [inaudible],

00:28:05.320 --> 00:28:06.620
like we are like part of it.

00:28:06.620 --> 00:28:07.920
It's not like a project, right.

00:28:07.920 --> 00:28:09.220
It's a process in the making,

00:28:09.220 --> 00:28:10.980
and we are part of
this thing, right.

00:28:10.980 --> 00:28:14.100
So we see some things
that are happening there.

00:28:14.100 --> 00:28:17.740
And we see some things
that actually go wrong,

00:28:17.740 --> 00:28:22.270
one recent thing the many people
pointed about is exactly this,

00:28:22.270 --> 00:28:23.570
what's happening with Facebook.

00:28:23.570 --> 00:28:26.790
And that's why Mr. Zuckerberg
was mainly invited to Congress,

00:28:26.790 --> 00:28:28.090
to the Congress hearing, right.

00:28:28.090 --> 00:28:30.200
It's basically these two
companies, Facebook and Google,

00:28:30.200 --> 00:28:33.630
which are the two big
elephants in the room.

00:28:33.630 --> 00:28:36.270
Which at the beginning of
digitalization, it was almost

00:28:36.270 --> 00:28:39.680
like a left-wing socialist
vision that they have

00:28:39.680 --> 00:28:42.520
to make information free
for everybody, all right.

00:28:42.520 --> 00:28:44.930
So the idea and the vision
was, in Silicon Valley,

00:28:44.930 --> 00:28:46.930
I mean it's very,
it's very moved

00:28:46.930 --> 00:28:48.650
to the left entire discourse.

00:28:48.650 --> 00:28:52.530
So the idea was we create this
information free for everybody,

00:28:52.530 --> 00:28:55.690
and we have this brave new world
where everybody can do for free.

00:28:55.690 --> 00:28:57.320
And that was great ambition.

00:28:57.320 --> 00:28:59.880
And yeah, you can use Google
Maps and whatever for free

00:28:59.880 --> 00:29:01.270
and WhatsApp, and
it's all for free.

00:29:01.270 --> 00:29:02.570
And Facebook, it's all for free.

00:29:02.570 --> 00:29:04.050
And yeah, they implement it.

00:29:04.050 --> 00:29:05.950
They implemented that.

00:29:05.950 --> 00:29:08.350
But with the cost of
this, so they made a bond

00:29:08.350 --> 00:29:09.650
with the devil, right.

00:29:09.650 --> 00:29:11.130
They sold themselves
advertising.

00:29:11.130 --> 00:29:14.660
So actually, they had
almost this leftwing idea

00:29:14.660 --> 00:29:19.510
and they created the tightest
capitalistic machinery history

00:29:19.510 --> 00:29:20.810
has ever seen, right.

00:29:20.810 --> 00:29:23.720
So everything that's going on on
these two platforms especially,

00:29:23.720 --> 00:29:25.810
Facebook and Google, is
mediated by some kind

00:29:25.810 --> 00:29:27.210
of commercial interest.

00:29:27.210 --> 00:29:29.030
That was very different
than back in the days

00:29:29.030 --> 00:29:32.110
when we paid our
monthly fixed line bill.

00:29:32.110 --> 00:29:34.960
And I was talking to you
but nobody was in between.

00:29:34.960 --> 00:29:36.690
I was talking to you
over the phone, right.

00:29:36.690 --> 00:29:40.240
Nowadays, if I talk to you
over Facebook or over Google,

00:29:40.240 --> 00:29:42.630
there are incountable
commercial interests in between,

00:29:42.630 --> 00:29:45.150
which absolutely distorts
the message, right.

00:29:45.150 --> 00:29:48.040
So actually, by creating
these communication platforms,

00:29:48.040 --> 00:29:50.360
having the devil in the
middle kind of like, right.

00:29:50.360 --> 00:29:51.850
Many people asks
themselves, wouldn't it better

00:29:51.850 --> 00:29:53.150
if we go back to this, right.

00:29:53.150 --> 00:29:54.580
We just pay a monthly fee.

00:29:54.580 --> 00:29:56.660
There's some other
companies did, like Netflix,

00:29:56.660 --> 00:29:59.540
Amazon and whatever
with prime services.

00:29:59.540 --> 00:30:02.030
They've entered this like let's
go back how it was in the day.

00:30:02.030 --> 00:30:03.330
You just pay a monthly fee.

00:30:03.330 --> 00:30:04.880
And, you know, we're still
going to do marketing,

00:30:04.880 --> 00:30:07.240
but we don't depend 100% on it.

00:30:07.240 --> 00:30:10.010
Whereas Facebook and Google,
they are so in the corner

00:30:10.010 --> 00:30:12.260
because they 100%
depend on that.

00:30:12.260 --> 00:30:14.530
They don't have an Amazon
Prime to fall back on

00:30:14.530 --> 00:30:15.830
or whatever, you know.

00:30:15.830 --> 00:30:17.750
&gt;&gt; John Haskell:
So you started this

00:30:17.750 --> 00:30:19.690
with the whole sci-fi
looking back

00:30:19.690 --> 00:30:22.380
from 500 years in the future.

00:30:22.380 --> 00:30:25.070
So put on your futuristic
hat if you'd like.

00:30:25.070 --> 00:30:26.750
&gt;&gt; Martin Hilbert: Oh my.

00:30:26.750 --> 00:30:28.960
&gt;&gt; John Haskell: You know,
where might this be headed,

00:30:28.960 --> 00:30:32.760
particularly when you think
about, you know, the advances

00:30:32.760 --> 00:30:38.350
in artificial intelligence, in
terms of how we're governed.

00:30:41.150 --> 00:30:44.330
&gt;&gt; Martin Hilbert: Yeah,
that's a big question.

00:30:44.330 --> 00:30:46.140
It's difficult, it's
very difficult to answer.

00:30:46.140 --> 00:30:47.440
Let me put that in perspective,

00:30:47.440 --> 00:30:49.210
why it is so difficult
to answer.

00:30:49.210 --> 00:30:52.320
It's because we don't
understand the technology yet.

00:30:52.320 --> 00:30:55.560
Which is absolutely normal that
we don't understand it yet.

00:30:55.560 --> 00:30:56.860
It's always been like.

00:30:56.860 --> 00:30:58.930
So people say like oh, we don't
understand neural networks.

00:30:58.930 --> 00:31:00.540
We never understood
the technology

00:31:00.540 --> 00:31:02.430
that we were dealing
with, right.

00:31:02.430 --> 00:31:06.040
But the end result is always
we made much quick advances

00:31:06.040 --> 00:31:08.490
than we always had hoped
for even in our [inaudible].

00:31:08.490 --> 00:31:10.660
I gave you a few
examples, right.

00:31:10.660 --> 00:31:13.750
So, take the industrial
revolution,

00:31:13.750 --> 00:31:16.800
all technological
revolutions work like this.

00:31:16.800 --> 00:31:18.370
That's why all the
theories are important.

00:31:18.370 --> 00:31:20.790
So technological innovation
theory is very important

00:31:20.790 --> 00:31:23.770
if you're in this field
because the fundamental theory

00:31:23.770 --> 00:31:25.070
of innovation has not changed.

00:31:25.070 --> 00:31:28.430
So, thermodynamics, right, the
first industrial revolution

00:31:28.430 --> 00:31:30.790
or the second, depends
on how you count it.

00:31:30.790 --> 00:31:33.790
When Carnot, Carnot studied
steam engines, right.

00:31:33.790 --> 00:31:35.470
So trains were already running.

00:31:35.470 --> 00:31:39.220
We had no idea actually
how that actually works.

00:31:39.220 --> 00:31:42.480
The equations that Boltzmann
wrote done of thermodynamics,

00:31:42.480 --> 00:31:44.850
they came 50 years later, right.

00:31:44.850 --> 00:31:46.590
The same with electricity.

00:31:46.590 --> 00:31:48.800
Faraday built the
first electronic motor,

00:31:48.800 --> 00:31:52.630
but electronic motors were
already among us before Maxwell

00:31:52.630 --> 00:31:54.280
wrote down the electromagnetic
equation.

00:31:54.280 --> 00:31:56.300
That also came 50 years later.

00:31:56.300 --> 00:31:58.550
Or take the brothers Wright.

00:31:58.550 --> 00:32:01.960
So the Wright brothers, they
flew the first time for 100 feet

00:32:01.960 --> 00:32:03.820
and was killing themselves.

00:32:03.820 --> 00:32:05.120
That's nothing.

00:32:05.120 --> 00:32:07.000
That's like a large jump,
you know, like 30 meters.

00:32:07.000 --> 00:32:10.210
We had no idea what
flying actually was.

00:32:10.210 --> 00:32:12.840
We always thought it has
to do with feathers, right.

00:32:12.840 --> 00:32:15.540
At least with flapping your
wings because we saw everything

00:32:15.540 --> 00:32:17.370
that flew has feathers or
was flapping their wings.

00:32:17.370 --> 00:32:18.670
And we always thought like wow,

00:32:18.670 --> 00:32:20.070
that's how biology
came up with it.

00:32:20.070 --> 00:32:21.370
That's what it has to do.

00:32:21.370 --> 00:32:22.810
And since da Vinci we
had this confusion.

00:32:22.810 --> 00:32:26.150
Then when the brothers Wright
built the first flying machines,

00:32:26.150 --> 00:32:27.960
we understood like
oh wow, it has to do

00:32:27.960 --> 00:32:29.280
with the curvature of the wing.

00:32:29.280 --> 00:32:31.310
It kind of like sucks
you up, you know.

00:32:31.310 --> 00:32:34.650
And then we developed
aerodynamics much later.

00:32:34.650 --> 00:32:38.260
And 60 years later we
flew to the moon, right.

00:32:38.260 --> 00:32:40.580
So that's, so we don't
understand what we're doing

00:32:40.580 --> 00:32:41.880
when we have these
new technologies.

00:32:41.880 --> 00:32:43.210
It's always been like that.

00:32:43.210 --> 00:32:44.630
But we make these huge jumps,

00:32:44.630 --> 00:32:46.130
and we create all
these different kinds

00:32:46.130 --> 00:32:47.750
of alternatives then, right.

00:32:47.750 --> 00:32:50.440
So we didn't only, not only did
we discover it has nothing to do

00:32:50.440 --> 00:32:53.190
with feathers, we built
helicopters that had nothing

00:32:53.190 --> 00:32:55.940
to do with [inaudible] blades
even though they have the same,

00:32:55.940 --> 00:32:57.240
they can stand in the air.

00:32:57.240 --> 00:33:01.390
Drones, satellites, rockets,
jet planes, you know.

00:33:01.390 --> 00:33:04.220
And right now we're doing the
same thing with intelligence.

00:33:04.220 --> 00:33:06.710
So, kind of like this
information process

00:33:06.710 --> 00:33:10.330
that Mother Nature came up with,
it's kind of like the feathers.

00:33:10.330 --> 00:33:12.030
It's kind of like
the birds, right.

00:33:12.030 --> 00:33:14.980
It's one solution to the
problem of intelligence.

00:33:14.980 --> 00:33:17.710
Just like birds are one solution
to the problem of aerodynamic

00:33:17.710 --> 00:33:19.010
and how to go about it.

00:33:19.010 --> 00:33:21.820
But there are many other ways,
just as there are rockets

00:33:21.820 --> 00:33:24.580
and hovercrafts and jet planes.

00:33:24.580 --> 00:33:26.890
There are many other kinds of
intelligence, as in right now,

00:33:26.890 --> 00:33:28.190
we don't understand it.

00:33:28.190 --> 00:33:30.350
And people freak out because we
don't understand neural nets,

00:33:30.350 --> 00:33:31.800
but it's always been like that.

00:33:31.800 --> 00:33:34.720
And what we understand is
like oh, we are just one part

00:33:34.720 --> 00:33:37.750
of intelligence of
this larger picture of,

00:33:37.750 --> 00:33:40.750
let's call it not the theory of
aerodynamics or thermodynamics,

00:33:40.750 --> 00:33:42.170
the theory of intelligence.

00:33:42.170 --> 00:33:44.790
And we see where we fall
in into that, right.

00:33:44.790 --> 00:33:46.730
So the fact that artificial
intelligence comes in,

00:33:46.730 --> 00:33:48.030
that artificial intelligence,

00:33:48.030 --> 00:33:49.810
we see that these other
intelligence are much better

00:33:49.810 --> 00:33:51.110
in some things.

00:33:51.110 --> 00:33:55.680
One thing they're much
better is executing laws.

00:33:55.680 --> 00:33:58.960
Because the algorithms,
that's what it is, right.

00:33:58.960 --> 00:34:02.310
The other thing, they are
much better being impartial

00:34:02.310 --> 00:34:03.810
and being really neutral.

00:34:03.810 --> 00:34:06.240
That's what democracy
strives for, right.

00:34:06.240 --> 00:34:10.590
So the rule of law,
neutrality, democracy,

00:34:10.590 --> 00:34:12.020
the equality among each other,

00:34:12.020 --> 00:34:14.860
having the big picture
processing a lot of information,

00:34:14.860 --> 00:34:17.150
they're much better than
this little, you know,

00:34:17.150 --> 00:34:19.720
nature-like solution we
came up with, we cannot.

00:34:19.720 --> 00:34:24.030
So all these different factors
that we have, we know this kind

00:34:24.030 --> 00:34:26.200
of intelligence is
much better than that.

00:34:26.200 --> 00:34:27.540
And it will lend itself

00:34:27.540 --> 00:34:32.040
to create a much more solid
governance system that we have.

00:34:32.040 --> 00:34:34.640
It's kind of like we are the
Wright Brothers and we try

00:34:34.640 --> 00:34:37.410
to speculate about
flying to the moon.

00:34:37.410 --> 00:34:38.780
&gt;&gt; John Haskell: So we
know a lot more today

00:34:38.780 --> 00:34:40.790
than we did yesterday about
artificial intelligence

00:34:40.790 --> 00:34:45.590
and how essentially you're
saying that as an individual,

00:34:45.590 --> 00:34:49.870
we're not quite as bright as
some other way of, you know,

00:34:49.870 --> 00:34:52.850
bringing information together
and making a decision.

00:34:52.850 --> 00:34:54.150
You brought up the law.

00:34:54.150 --> 00:34:55.490
I'd like to hear you talk a
little bit more about that.

00:34:55.490 --> 00:34:59.030
But I thought as a practical
matter, do we know enough now.

00:34:59.030 --> 00:35:00.380
I think you're hinting at this.

00:35:00.380 --> 00:35:06.270
Do we know enough now
to enhance policymaking

00:35:06.270 --> 00:35:08.910
so that it would be
more evidence-based

00:35:08.910 --> 00:35:13.950
and maybe less discriminatory,
something that would, you know,

00:35:13.950 --> 00:35:17.550
be in tune with the
spirit of democracy?

00:35:17.550 --> 00:35:18.850
&gt;&gt; Martin Hilbert: Yeah.

00:35:18.850 --> 00:35:20.560
Yes, I mean there's the promise.

00:35:20.560 --> 00:35:23.650
There's not until now, because
we don't have implanted now just

00:35:23.650 --> 00:35:27.220
because most of AI has been
developed to optimize ads.

00:35:27.220 --> 00:35:29.090
Like honestly, right.

00:35:29.090 --> 00:35:33.420
So, but we know theoretically
and also practically,

00:35:33.420 --> 00:35:34.740
like academics are
working on it,

00:35:34.740 --> 00:35:36.680
some other colleagues
are working on that.

00:35:36.680 --> 00:35:37.980
And we can do that.

00:35:37.980 --> 00:35:40.020
So one thing is to say for
example, the discrimination,

00:35:40.020 --> 00:35:41.350
that's a very good point
that you mentioned.

00:35:41.350 --> 00:35:47.640
So, you know, we strive to,
for example, execute the law

00:35:47.640 --> 00:35:49.630
in a very nondiscriminatory
way, right.

00:35:49.630 --> 00:35:51.590
If the law says that, we
should actually execute the law

00:35:51.590 --> 00:35:52.890
like this.

00:35:52.890 --> 00:35:56.290
And the colleagues next door,
the Supreme Court, right,

00:35:56.290 --> 00:35:57.880
they've been trained
for 50 years

00:35:57.880 --> 00:36:00.000
of being really impartial
in doing that.

00:36:00.000 --> 00:36:02.600
Now, we can show, and studies
show that even after 50 years

00:36:02.600 --> 00:36:04.840
of training, you
will still be biased.

00:36:04.840 --> 00:36:06.180
You will still have stereotypes.

00:36:06.180 --> 00:36:07.690
You cannot get them
out of your head,

00:36:07.690 --> 00:36:09.540
because these are these
variables like race,

00:36:09.540 --> 00:36:11.310
like gender, religious belief.

00:36:11.310 --> 00:36:13.860
These are these all income
[phonetic] bracing variables

00:36:13.860 --> 00:36:16.030
that are so powerful and have
so much [inaudible] power

00:36:16.030 --> 00:36:18.690
that this little processor
that we have here loves

00:36:18.690 --> 00:36:20.170
to work with these variables.

00:36:20.170 --> 00:36:22.830
Because like a big
bat, you know.

00:36:22.830 --> 00:36:24.550
Like oh, you just
take gender and race

00:36:24.550 --> 00:36:26.530
and I can have all these little.

00:36:26.530 --> 00:36:28.490
So we cannot get
it out of our mind.

00:36:28.490 --> 00:36:30.430
Now algorithms can work

00:36:30.430 --> 00:36:32.800
with much more fine-grained
indicators, you know.

00:36:32.800 --> 00:36:34.100
So instead of working
with gender,

00:36:34.100 --> 00:36:37.080
we can work with all these
underlying indicators.

00:36:37.080 --> 00:36:39.280
Now in practice, they
are not until now.

00:36:39.280 --> 00:36:41.760
So right now, there
have been studies done.

00:36:41.760 --> 00:36:45.230
You take a neural net, and you
feed it with everything you find

00:36:45.230 --> 00:36:48.080
on social media, on Wikipedia
and all the newspaper articles,

00:36:48.080 --> 00:36:49.440
everything you find on the web.

00:36:49.440 --> 00:36:52.300
You feed it and you
ask this algorithm

00:36:52.300 --> 00:36:54.610
who to invite on
a job interview.

00:36:54.610 --> 00:36:57.350
Somebody with a male
or female name

00:36:57.350 --> 00:36:59.480
if all the qualifications
are the same.

00:36:59.480 --> 00:37:01.910
It will recommend you
a male name, with,

00:37:01.910 --> 00:37:03.350
I don't know, 60% likelihood.

00:37:03.350 --> 00:37:04.650
If you ask somebody

00:37:04.650 --> 00:37:08.380
if everything sounds the same
except the first and last name,

00:37:08.380 --> 00:37:10.150
Afro-American or European,

00:37:10.150 --> 00:37:12.930
it will recommend you
to go for European.

00:37:12.930 --> 00:37:14.960
And if you look into
[inaudible] and you can, right,

00:37:14.960 --> 00:37:16.380
this multidimensional space,

00:37:16.380 --> 00:37:18.610
and you see that
African-American names,

00:37:18.610 --> 00:37:22.400
they are in the corner
together with terms like prison

00:37:22.400 --> 00:37:23.990
and agony and violence.

00:37:23.990 --> 00:37:26.810
And European names in a
corner together with success

00:37:26.810 --> 00:37:28.940
and celery and whatever.

00:37:28.940 --> 00:37:33.940
And who taught the AI
such a racist view?

00:37:33.940 --> 00:37:35.240
Well, we did, you know.

00:37:35.240 --> 00:37:39.290
It read 250 years of
our writings and says

00:37:39.290 --> 00:37:41.140
like you guys say that
goes together with that,

00:37:41.140 --> 00:37:42.440
and that goes together
with that.

00:37:42.440 --> 00:37:43.740
That's all AI learned.

00:37:43.740 --> 00:37:45.200
It learned it from us,
from learning 300 years

00:37:45.200 --> 00:37:46.830
of our writings, right.

00:37:46.830 --> 00:37:51.180
So, now what we can do as well
with AI is say, since it be able

00:37:51.180 --> 00:37:53.480
to [inaudible] the small
indicators below these big

00:37:53.480 --> 00:37:58.250
indicators, we can tell AI, use
all the information you can get.

00:37:58.250 --> 00:38:00.220
But don't use the
variable's gender,

00:38:00.220 --> 00:38:04.860
race and religious
belief, right.

00:38:04.860 --> 00:38:06.610
Do not discriminate on that.

00:38:06.610 --> 00:38:07.910
Don't use information.

00:38:07.910 --> 00:38:10.680
You will inevitably
lose accuracy,

00:38:10.680 --> 00:38:13.750
because every variable gets
you more accuracy, right.

00:38:13.750 --> 00:38:15.310
You will lose accuracy,
but it turns

00:38:15.310 --> 00:38:17.760
out that you can design
the algorithm in such a way

00:38:17.760 --> 00:38:19.860
that you lose accuracy
very minimally.

00:38:19.860 --> 00:38:22.940
So if a person, for
example, is 70% accurate,

00:38:22.940 --> 00:38:26.570
the machine is 86.6% accurate.

00:38:26.570 --> 00:38:32.070
If you take these three,
four variables, out is 86.2.

00:38:32.070 --> 00:38:33.370
&gt;&gt; John Haskell: Still
better than the person.

00:38:33.370 --> 00:38:34.670
&gt;&gt; Martin Hilbert:
Well, much better

00:38:34.670 --> 00:38:35.970
&gt;&gt; John Haskell: By a long shot.

00:38:35.970 --> 00:38:37.270
That's the point, right.

00:38:37.270 --> 00:38:38.570
&gt;&gt; Martin Hilbert: Yeah,
much better than the person.

00:38:38.570 --> 00:38:39.870
And we know the person
will always be biased.

00:38:39.870 --> 00:38:41.170
The person cannot
get these variables

00:38:41.170 --> 00:38:42.470
out of its head, right.

00:38:42.470 --> 00:38:44.950
The machine can with
absolutely no discrimination

00:38:44.950 --> 00:38:46.740
and still be way
superior than the human.

00:38:46.740 --> 00:38:49.020
So in theory, yes, we can.

00:38:49.020 --> 00:38:51.760
Now, we haven't developed that
yet for a commercial scale

00:38:51.760 --> 00:38:53.670
or applicable scale because
nobody put the dollars

00:38:53.670 --> 00:38:54.970
on the table.

00:38:54.970 --> 00:38:59.240
I mean, we spend a lot of money,
we employ 40,000 mathematicians

00:38:59.240 --> 00:39:01.400
in the NSA to keep
Americans safe.

00:39:01.400 --> 00:39:04.600
We employ tens of thousands of
programmers in Silicon Valley

00:39:04.600 --> 00:39:07.110
in order to get people
the ads that they want.

00:39:07.110 --> 00:39:08.920
But nobody put the
money on the table

00:39:08.920 --> 00:39:11.850
to say let's develop
some AI, you know,

00:39:11.850 --> 00:39:13.290
to make democracy work.

00:39:13.290 --> 00:39:17.120
And it wouldn't be a big thing
just hire 10,000 programmers.

00:39:17.120 --> 00:39:18.810
Nobody has been doing
that until now.

00:39:18.810 --> 00:39:21.380
Now these applications,
in theory, they have a lot

00:39:21.380 --> 00:39:23.800
of potential to improve
democracy in practice.

00:39:23.800 --> 00:39:25.630
We haven't started
to look into them.

00:39:25.630 --> 00:39:29.820
&gt;&gt; John Haskell: So for, and
being a Congress-focused person,

00:39:29.820 --> 00:39:33.110
my academic career, I
think like, well members

00:39:33.110 --> 00:39:34.410
of Congress have a lot

00:39:34.410 --> 00:39:35.710
of different reasons
to introduce bills.

00:39:35.710 --> 00:39:37.010
We know that, right.

00:39:37.010 --> 00:39:38.310
You know, some of them
have nothing to do

00:39:38.310 --> 00:39:39.610
with actually passing a law.

00:39:39.610 --> 00:39:41.930
That's a news flash, I
know, for a lot of people.

00:39:41.930 --> 00:39:44.120
Because they have other
completely legitimate reasons

00:39:44.120 --> 00:39:45.420
to introduce a bill.

00:39:45.420 --> 00:39:48.050
But a lot of times they actually
do want to achieve something.

00:39:48.050 --> 00:39:51.060
&gt;&gt; Martin Hilbert:
It's good to know that.

00:39:51.060 --> 00:39:52.360
&gt;&gt; John Haskell: Right.

00:39:52.360 --> 00:39:53.820
We're trying to be
snarky about that.

00:39:53.820 --> 00:39:56.350
But seriously, I mean there
are times you introduce a bill

00:39:56.350 --> 00:40:00.750
for other purposes that aren't
about legislative product

00:40:00.750 --> 00:40:02.380
at the end of the day.

00:40:02.380 --> 00:40:04.870
But there are times
when it is about that.

00:40:04.870 --> 00:40:08.740
So would there be some way to
figure out how to craft a bill

00:40:08.740 --> 00:40:12.070
that would take advantage of an
intelligence that's better than,

00:40:12.070 --> 00:40:15.810
say I'm a congressman, and
you three were my staff

00:40:15.810 --> 00:40:17.360
and us just having
a brainstorming.

00:40:17.360 --> 00:40:19.040
There's got to be a better way,
right, based on what you said.

00:40:19.040 --> 00:40:20.340
&gt;&gt; Martin Hilbert: Yeah, right.

00:40:20.340 --> 00:40:21.890
Yeah. So I think the best,

00:40:21.890 --> 00:40:24.680
one of the things is this
amazing thing, so for example,

00:40:24.680 --> 00:40:26.060
the bills, for example,
how that works.

00:40:26.060 --> 00:40:28.260
We have a one-dimensional
scale, left and right.

00:40:28.260 --> 00:40:31.430
And usually we divide opinions
are you left or are you right?

00:40:31.430 --> 00:40:33.630
Right. In most countries
around the world.

00:40:33.630 --> 00:40:34.930
So that's one I mention.

00:40:34.930 --> 00:40:36.230
Opinion doesn't have
one dimension.

00:40:36.230 --> 00:40:37.540
Opinion has many
dimensions, right.

00:40:37.540 --> 00:40:41.030
So you have 800 dimensions,
2,000 dimensions,

00:40:41.030 --> 00:40:44.010
that you have in a big space.

00:40:44.010 --> 00:40:45.920
And you can see actually
where things hang.

00:40:45.920 --> 00:40:47.650
Actually, that's another thing.

00:40:47.650 --> 00:40:48.960
We can play around with that.

00:40:48.960 --> 00:40:52.050
If you turn on the computer
again, you can go to that.

00:40:52.050 --> 00:40:53.350
So that's an AI.

00:40:53.350 --> 00:40:55.120
Most companies actually
open up the neural nets

00:40:55.120 --> 00:40:56.500
so you can actually go there.

00:40:56.500 --> 00:40:59.640
They opened up because they're
a little scared of it, you know.

00:40:59.640 --> 00:41:00.990
We have a terminator
in the basement.

00:41:00.990 --> 00:41:02.690
We just want you to
know we have one.

00:41:02.690 --> 00:41:04.070
Have a look at it.

00:41:04.070 --> 00:41:07.940
They don't give you
like the, they give you

00:41:07.940 --> 00:41:09.570
like an empty brain, right.

00:41:09.570 --> 00:41:11.700
They don't give you the
brain that they trained

00:41:11.700 --> 00:41:13.220
for 20 years with their data.

00:41:13.220 --> 00:41:15.830
It's kind of like they have
the Ivy League graduate,

00:41:15.830 --> 00:41:17.740
and that's what they
make money with.

00:41:17.740 --> 00:41:19.040
And they give you
like the newborn baby

00:41:19.040 --> 00:41:20.340
and say here you go.

00:41:20.340 --> 00:41:21.640
But you can trade
in yourself, right.

00:41:21.640 --> 00:41:23.150
At least they give you
the architecture, right.

00:41:23.150 --> 00:41:24.450
So you can go here.

00:41:24.450 --> 00:41:27.820
One, for example, is
projector tends of flow.

00:41:27.820 --> 00:41:31.490
And that's one of
these deep neural net

00:41:31.490 --> 00:41:33.660
that allows you to
represent words.

00:41:33.660 --> 00:41:36.040
So this here is a
200-dimensional space,

00:41:36.040 --> 00:41:38.500
well shown in a
three-dimensional space.

00:41:38.500 --> 00:41:42.800
I once asked one of these
computer scientists how they

00:41:42.800 --> 00:41:46.840
imagined 200 dimensions, because
I can only imagine three.

00:41:46.840 --> 00:41:48.140
And he said well, that's easy.

00:41:48.140 --> 00:41:51.770
You close your eyes and then
you scream very loudly 200.

00:41:51.770 --> 00:41:56.580
All right, so they
also cannot, right.

00:41:56.580 --> 00:41:58.630
But yeah. So in here
you can see, you know,

00:41:58.630 --> 00:41:59.930
you can kind of like zoom in.

00:41:59.930 --> 00:42:01.600
You see it's all these
words here, right,

00:42:01.600 --> 00:42:04.490
and they hang together these
words and different corners.

00:42:04.490 --> 00:42:06.800
And then you can see,
for example, here,

00:42:06.800 --> 00:42:09.920
let's see if we have Congress.

00:42:09.920 --> 00:42:11.910
Yeah. We can see Congress here,

00:42:11.910 --> 00:42:14.730
and then we can see the 100
neighboring words of that.

00:42:14.730 --> 00:42:20.430
Jefferson, Washington, Vienna,
interesting, house, presidency,

00:42:20.430 --> 00:42:23.110
Senate, policy and so
forth, act, library.

00:42:23.110 --> 00:42:25.720
Oh look, library is here
too next to Congress.

00:42:25.720 --> 00:42:28.890
So, that depends on how
this network was trained,

00:42:28.890 --> 00:42:31.010
how this neural network
was trained.

00:42:31.010 --> 00:42:33.970
And you can project it now
onto a three-dimensional space

00:42:33.970 --> 00:42:36.710
as well and you get
these kind of shapes.

00:42:36.710 --> 00:42:40.170
These three-dimensional
shapes calculates it now

00:42:40.170 --> 00:42:43.420
and breaks these two dimensions
down into three dimensions.

00:42:43.420 --> 00:42:45.150
And you can really
shape, like the shape

00:42:45.150 --> 00:42:46.960
where I said you before where
I told you before so people

00:42:46.960 --> 00:42:49.150
with African-American
names are in the corner

00:42:49.150 --> 00:42:50.860
with these kind of concepts.

00:42:50.860 --> 00:42:52.610
And that's basically
what that is.

00:42:52.610 --> 00:42:55.010
So now, it beaks it down,

00:42:55.010 --> 00:42:56.470
and it's called tisney
[phonetic] this algorithm

00:42:56.470 --> 00:43:00.170
that people swear it's the best
dimension reduction algorithm

00:43:00.170 --> 00:43:03.140
being seen for these
kind of purposes.

00:43:03.140 --> 00:43:06.340
And you can actually see kind

00:43:06.340 --> 00:43:11.660
of like a shape forming
over time here.

00:43:11.660 --> 00:43:14.160
And then see where it hangs out.

00:43:14.160 --> 00:43:20.150
So, when you are here,
maybe we can stop that now.

00:43:20.150 --> 00:43:23.740
And you can move
this around, right.

00:43:23.740 --> 00:43:25.860
And you can actually see
this word cloud here.

00:43:25.860 --> 00:43:27.640
And then you can look at
the different corners.

00:43:27.640 --> 00:43:30.140
Let's, for example,
look at this corner.

00:43:30.140 --> 00:43:34.390
Tim, Tom, Steve, Barry, so
these all seem to be last names.

00:43:34.390 --> 00:43:37.500
Maybe let's look at
another, first names.

00:43:37.500 --> 00:43:39.050
Look at another corner.

00:43:39.050 --> 00:43:40.350
Oops, where am I?

00:43:40.350 --> 00:43:44.980
Here. MacIntosh, IBM, PC, okay.

00:43:44.980 --> 00:43:46.490
This is also.

00:43:46.490 --> 00:43:52.110
Songwriter, writer, reformed,
entrepreneurs, so these seem

00:43:52.110 --> 00:43:55.700
to be methodologists, scholar.

00:43:55.700 --> 00:43:57.980
So these names, basketball,
so these names seem to be kind

00:43:57.980 --> 00:43:59.320
of like jobs or entertainment.

00:43:59.320 --> 00:44:01.940
And you can now see like which
words kind of hang together.

00:44:01.940 --> 00:44:04.270
And whatever you
train it with, right,

00:44:04.270 --> 00:44:05.570
you will see different things

00:44:05.570 --> 00:44:08.060
that oh here is Alabama,
Mississippi, Indiana.

00:44:08.060 --> 00:44:10.530
So these seem to all be states
that have to do with each other.

00:44:10.530 --> 00:44:13.100
Now nobody told this
network what actually to do.

00:44:13.100 --> 00:44:15.450
Basically, how they work is
they do a prediction game.

00:44:15.450 --> 00:44:18.700
They read this entire text and
try to predict future words

00:44:18.700 --> 00:44:21.780
or sentences based on based on
previous words and sentences.

00:44:21.780 --> 00:44:23.080
So it's all syntax.

00:44:23.080 --> 00:44:24.380
There's no semantics.

00:44:24.380 --> 00:44:26.000
The semantic is the
result of syntax.

00:44:26.000 --> 00:44:27.650
Syntax, we always told
was different things.

00:44:27.650 --> 00:44:30.840
No, we can get meaning
just from structure.

00:44:30.840 --> 00:44:34.020
Which is an amazing thing
once we have the big data.

00:44:34.020 --> 00:44:35.640
Now imagine we do
the thing like that.

00:44:35.640 --> 00:44:37.420
I always was thinking about
if you do a thing like that

00:44:37.420 --> 00:44:39.440
with the bills, for example,
or with something like that.

00:44:39.440 --> 00:44:40.740
Just throw everything in,

00:44:40.740 --> 00:44:43.720
and you will actually see 200
dimensions how it maps out.

00:44:43.720 --> 00:44:45.630
You know, what's the
opinion structure

00:44:45.630 --> 00:44:46.930
of these different bills.

00:44:46.930 --> 00:44:49.440
You have a few thousand bills,
and you can see like actually,

00:44:49.440 --> 00:44:51.030
well these kind of
rubrics of the bills,

00:44:51.030 --> 00:44:52.330
these concepts, these
hang together.

00:44:52.330 --> 00:44:54.900
And then you can very quickly
figure out instead of having

00:44:54.900 --> 00:44:57.480
like one dimension,
you know, left, right,

00:44:57.480 --> 00:44:59.370
there are many concepts
in a bill.

00:44:59.370 --> 00:45:00.670
And with that information
process,

00:45:00.670 --> 00:45:04.120
you can actually quickly figure
out like how could I get a 50%,

00:45:04.120 --> 00:45:05.420
how could I get a majority?

00:45:05.420 --> 00:45:07.120
Like what do I have to put
together in a bill in order

00:45:07.120 --> 00:45:09.070
to get a majority to
get it through, right.

00:45:09.070 --> 00:45:12.560
Because it very quickly maps
it out on these kind of scales.

00:45:12.560 --> 00:45:13.860
&gt;&gt; John Haskell: And
then only the Senate gets

00:45:13.860 --> 00:45:15.160
in the way at that point.

00:45:15.160 --> 00:45:16.460
&gt;&gt; Martin Hilbert: Yeah.

00:45:16.460 --> 00:45:17.990
&gt;&gt; John Haskell: So, let's
continue this conversation

00:45:17.990 --> 00:45:19.640
with you all to see
what direction you'd

00:45:19.640 --> 00:45:21.330
like Martin to take this.

00:45:21.330 --> 00:45:24.730
So we've got a couple
people who have microphones

00:45:24.730 --> 00:45:29.650
so that you can, so just raise
your hand and indicate to,

00:45:29.650 --> 00:45:32.600
we've got somebody
way up in the front.

00:45:32.600 --> 00:45:34.730
Get our exercise in.

00:45:34.730 --> 00:45:36.610
This gentleman here
was the first

00:45:36.610 --> 00:45:39.830
to raise his hand
in the green shirt.

00:45:39.830 --> 00:45:41.130
&gt;&gt; Thank you.

00:45:41.130 --> 00:45:45.500
One quick clarification,
when you mentioned the 78%

00:45:45.500 --> 00:45:47.680
of the 17 million voters

00:45:47.680 --> 00:45:50.530
that Obama swayed
of the independents.

00:45:50.530 --> 00:45:51.830
I think those were the numbers.

00:45:51.830 --> 00:45:55.490
And you said changed
their minds.

00:45:55.490 --> 00:45:59.230
Did you actually mean just
impressed them in a certain way?

00:45:59.230 --> 00:46:01.160
Because they were
undecided, weren't they?

00:46:01.160 --> 00:46:02.460
&gt;&gt; Martin Hilbert: Yeah,
they were undecided.

00:46:02.460 --> 00:46:03.760
So they swayed them over, yeah.

00:46:03.760 --> 00:46:05.630
&gt;&gt; Swayed them in a way

00:46:05.630 --> 00:46:07.900
that might have been the
way they already were.

00:46:07.900 --> 00:46:12.660
So it didn't like, it
wasn't a 70% change of mind.

00:46:12.660 --> 00:46:15.360
&gt;&gt; Martin Hilbert: Well,
50% were like, undecided,

00:46:15.360 --> 00:46:17.370
an average would have
been a 50/50 call,

00:46:17.370 --> 00:46:19.420
and at the end of
this 16 million --

00:46:19.420 --> 00:46:20.780
&gt;&gt; John Haskell: They could
have been on the fence, right?

00:46:20.780 --> 00:46:22.080
They could have been
on the fence,

00:46:22.080 --> 00:46:23.380
and this tipped them over.

00:46:23.380 --> 00:46:24.680
&gt;&gt; Martin Hilbert:
Right, yeah, yah.

00:46:24.680 --> 00:46:25.980
&gt;&gt; John Haskell: I
mean that's what --

00:46:25.980 --> 00:46:27.280
&gt;&gt; Martin Hilbert: If they
would have done nothing,

00:46:27.280 --> 00:46:28.840
it would probably be again 50%.

00:46:28.840 --> 00:46:30.560
Because if they're real
swing voters, you know,

00:46:30.560 --> 00:46:31.860
it would have fallen 50/50.

00:46:31.860 --> 00:46:33.630
But it turned out it was 78,

00:46:33.630 --> 00:46:36.750
which was different
than 50/50, right.

00:46:36.750 --> 00:46:38.050
&gt;&gt; John Haskell: That's good.

00:46:38.050 --> 00:46:41.000
Who else has got something?

00:46:41.000 --> 00:46:43.390
Lucy Ann over here.

00:46:43.390 --> 00:46:46.660
&gt;&gt; Hey, this has been
very interesting.

00:46:46.660 --> 00:46:51.310
So, I would like to press
back a little bit on this idea

00:46:51.310 --> 00:46:55.110
that an algorithm can be
truly nondiscriminatory.

00:46:55.110 --> 00:46:58.840
Because you use this example,
right, of taking European

00:46:58.840 --> 00:47:02.430
versus African names, which
is very focused on, you know,

00:47:02.430 --> 00:47:06.120
one kind of learning, something
that's focused on language.

00:47:06.120 --> 00:47:08.150
But you're not touching
on the fact

00:47:08.150 --> 00:47:12.280
that language itself can
be specific to ethnicity.

00:47:12.280 --> 00:47:14.440
It can be specifically gendered.

00:47:14.440 --> 00:47:16.960
And a lot of that
is baked already

00:47:16.960 --> 00:47:18.850
into the way we use language.

00:47:18.850 --> 00:47:22.670
And that is not something that
you can easily flag as being,

00:47:22.670 --> 00:47:25.190
you know, anyone can try
this at home if you want

00:47:25.190 --> 00:47:28.740
to do a Google search and phrase
your question using, you know,

00:47:28.740 --> 00:47:30.840
hyper academic language
and then using slang,

00:47:30.840 --> 00:47:33.110
you get different
results, right.

00:47:33.110 --> 00:47:36.380
And this doesn't touch
on discrimination

00:47:36.380 --> 00:47:39.170
in like image facial
recognition algorithms.

00:47:39.170 --> 00:47:43.470
So I wonder if you can comment a
little bit on that, because it's

00:47:43.470 --> 00:47:44.910
like very utopian to say

00:47:44.910 --> 00:47:48.160
that these algorithms
can be nondiscriminatory

00:47:48.160 --> 00:47:52.780
but also whose motivation
will it be to fix them

00:47:52.780 --> 00:47:55.420
when the creators of algorithms
area already a monoculture

00:47:55.420 --> 00:47:56.720
to themselves?

00:47:56.720 --> 00:47:58.020
&gt;&gt; Martin Hilbert: Yeah, right.

00:47:58.020 --> 00:47:59.320
Now that's a very
important point.

00:47:59.320 --> 00:48:01.140
And a very good point.

00:48:01.140 --> 00:48:06.530
That it's not, yes, it can, I
think they can be, like I say,

00:48:06.530 --> 00:48:11.570
they can be as, even
less discriminatory

00:48:11.570 --> 00:48:12.870
than we are, right.

00:48:12.870 --> 00:48:14.170
If we exactly know what we do

00:48:14.170 --> 00:48:15.680
when we say we are
nondiscriminatory, right.

00:48:15.680 --> 00:48:18.140
If a supreme court judge
really does something,

00:48:18.140 --> 00:48:19.440
these are the principles,

00:48:19.440 --> 00:48:23.070
then I can right now everyone
does exactly that, number one.

00:48:23.070 --> 00:48:26.890
So it can be at least as
nondiscriminatory as we are.

00:48:26.890 --> 00:48:29.730
And second, even
more, in a sense,

00:48:29.730 --> 00:48:32.280
better because we can
take something out.

00:48:32.280 --> 00:48:34.750
It doesn't mean that we still
have to define what we take out.

00:48:34.750 --> 00:48:37.520
So when I said we make
nondiscriminatory algorithms,

00:48:37.520 --> 00:48:39.710
it's we make them
nondiscriminatory

00:48:39.710 --> 00:48:41.650
to these three variables.

00:48:41.650 --> 00:48:43.290
You have to define these
three variables, right.

00:48:43.290 --> 00:48:47.350
So I said gender, race
and religious belief.

00:48:47.350 --> 00:48:48.650
And why these three?

00:48:48.650 --> 00:48:52.830
Well, because, you know,
the law gave them to us.

00:48:52.830 --> 00:48:54.520
Right. So that's just, we define

00:48:54.520 --> 00:48:56.260
on these three we don't
want to discriminate.

00:48:56.260 --> 00:48:58.670
And then I can say
okay, make a decision,

00:48:58.670 --> 00:49:00.090
but don't discriminate
on these three.

00:49:00.090 --> 00:49:02.430
Like what are these three?

00:49:02.430 --> 00:49:04.120
That's really for
society to decide.

00:49:04.120 --> 00:49:06.540
It might be another society
just say you can discriminate

00:49:06.540 --> 00:49:09.080
according to that because
that doesn't matter to us.

00:49:09.080 --> 00:49:10.560
And then how and
what you discriminate

00:49:10.560 --> 00:49:13.300
and what you don't discriminate
and to what degree and in

00:49:13.300 --> 00:49:15.370
which sense, yes, that
is completely baked

00:49:15.370 --> 00:49:16.670
into the decision algorithm.

00:49:16.670 --> 00:49:18.670
Same as it is baked in
any decision algorithm

00:49:18.670 --> 00:49:20.260
by a supreme court justice.

00:49:20.260 --> 00:49:24.460
So, the difference is
then that if you have it

00:49:24.460 --> 00:49:27.000
in a digital algorithm,
you can open it up.

00:49:27.000 --> 00:49:28.880
And if it's open
source, also discuss it.

00:49:28.880 --> 00:49:30.440
And I think it will
be very important

00:49:30.440 --> 00:49:32.640
that these algorithms
will be open source.

00:49:32.640 --> 00:49:34.170
So they have to be
completely open source.

00:49:34.170 --> 00:49:35.900
Everybody should be
able to look into them.

00:49:35.900 --> 00:49:39.090
Everybody should be able to
exactly know what they do.

00:49:39.090 --> 00:49:41.580
And this new law in Europe that
comes into effect at the end

00:49:41.580 --> 00:49:44.840
of the month, kind of like
goes into that direction.

00:49:44.840 --> 00:49:47.290
I talked with the German
government last week,

00:49:47.290 --> 00:49:49.200
and they have no idea how
they're going to implement it.

00:49:49.200 --> 00:49:52.320
But this is this right
to know law, right.

00:49:52.320 --> 00:49:55.360
So in Europe you can go now
starting for next month,

00:49:55.360 --> 00:49:58.240
and you can say somebody,
this algorithm does something,

00:49:58.240 --> 00:49:59.540
and it took a decision.

00:49:59.540 --> 00:50:01.750
I want to exactly know how this
algorithm got to this decision.

00:50:01.750 --> 00:50:03.660
So Facebook, you need
to explain to me,

00:50:03.660 --> 00:50:05.770
or somebody who hires
you, you need to explain

00:50:05.770 --> 00:50:08.320
to me how this algorithm
made the decision not

00:50:08.320 --> 00:50:10.040
to invite me to a job interview.

00:50:10.040 --> 00:50:12.670
And the person that used the
algorithm or the provider

00:50:12.670 --> 00:50:16.570
of the algorithm then has
to lay that out to you.

00:50:16.570 --> 00:50:18.290
So it goes in this
direction of being,

00:50:18.290 --> 00:50:19.840
but you are completely right.

00:50:19.840 --> 00:50:22.550
Yeah, these algorithms, these
should be exactly transparent.

00:50:22.550 --> 00:50:24.200
Now the good thing is we
then know what they do.

00:50:24.200 --> 00:50:25.820
For example, there's this
famous study also done

00:50:25.820 --> 00:50:29.600
by the same Kosinski who started
this psychoanalytic stuff.

00:50:29.600 --> 00:50:31.710
So he took, I think that's
what you are referring to.

00:50:31.710 --> 00:50:34.540
He took images in Facebook
and trained the algorithm

00:50:34.540 --> 00:50:35.840
to detect homosexuality.

00:50:35.840 --> 00:50:39.710
And it was very successful,
just from your profile shot,

00:50:39.710 --> 00:50:42.630
the algorithm could detect
your sexual preferences.

00:50:42.630 --> 00:50:43.930
Just for looking at the image.

00:50:43.930 --> 00:50:47.220
Now the interesting thing was,
now once we had an algorithm,

00:50:47.220 --> 00:50:49.900
and he just said, you know,
just show that it's possible.

00:50:49.900 --> 00:50:51.200
And it is possible.

00:50:51.200 --> 00:50:52.500
You can detect sexual
preferences just

00:50:52.500 --> 00:50:55.580
by the picture, by
the face actually.

00:50:55.580 --> 00:50:57.920
It just, what's really
surprising to many.

00:50:57.920 --> 00:50:59.290
Now, since it's the algorithm,

00:50:59.290 --> 00:51:01.170
what came of that
is they can look

00:51:01.170 --> 00:51:03.480
at what the algorithm
actually did, right.

00:51:03.480 --> 00:51:06.440
And then HPC like what does
the algorithm actually do

00:51:06.440 --> 00:51:08.300
when it gets to this
or that decision?

00:51:08.300 --> 00:51:10.890
Like humans make the
decision all the time, right.

00:51:10.890 --> 00:51:12.190
We just don't know.

00:51:12.190 --> 00:51:13.490
It's tacit knowledge for us.

00:51:13.490 --> 00:51:15.090
But we kind of making
that implicit.

00:51:15.090 --> 00:51:16.750
Once we have an algorithm,
we can learn more

00:51:16.750 --> 00:51:18.450
about why it's discrimination.

00:51:18.450 --> 00:51:19.750
What is the discrimination.

00:51:19.750 --> 00:51:23.180
Now, the results we take from
that can be extremely ugly,

00:51:23.180 --> 00:51:24.750
or it can be very useful.

00:51:24.750 --> 00:51:26.620
And that's still, that's
a decision completely out.

00:51:26.620 --> 00:51:27.920
And that's really up to us.

00:51:27.920 --> 00:51:29.440
That's not, the technology
doesn't care

00:51:29.440 --> 00:51:30.930
about that, what you do with it.

00:51:30.930 --> 00:51:32.340
&gt;&gt; John Haskell:
So the gentleman

00:51:32.340 --> 00:51:35.750
with the yellow shirt right
next to you, Mike, was next.

00:51:35.750 --> 00:51:37.050
&gt;&gt; Okay, thank you.

00:51:37.050 --> 00:51:38.350
Thank you very much.

00:51:38.350 --> 00:51:40.980
Just a very simple, well
maybe not so simple,

00:51:40.980 --> 00:51:42.830
but very short question.

00:51:42.830 --> 00:51:46.720
Is this is a danger to
participatory democracy?

00:51:46.720 --> 00:51:53.950
Are we going in a direction
that could be construed

00:51:53.950 --> 00:51:59.850
such that a country or
a government that is not

00:51:59.850 --> 00:52:04.280
so democratically inclined
could take this information

00:52:04.280 --> 00:52:06.710
and really control
just about everything?

00:52:06.710 --> 00:52:09.900
I mean I'm looking
at news from China

00:52:09.900 --> 00:52:14.500
where they have these facial
recognition situations,

00:52:14.500 --> 00:52:15.800
and who knows?

00:52:15.800 --> 00:52:18.640
I mean, prediction
can be dangerous.

00:52:18.640 --> 00:52:24.210
And I just don't know if you've
had any thoughts or ideas

00:52:24.210 --> 00:52:28.210
on how this type of thing
can be used by governments

00:52:28.210 --> 00:52:30.090
that are not so benevolent.

00:52:30.090 --> 00:52:31.390
Thanks.

00:52:31.390 --> 00:52:32.690
&gt;&gt; John Haskell:
That's a great question.

00:52:32.690 --> 00:52:33.990
&gt;&gt; Martin Hilbert: Yes.

00:52:33.990 --> 00:52:36.210
Yeah, as I just said,

00:52:36.210 --> 00:52:39.220
the technology is always
normatively neutral.

00:52:39.220 --> 00:52:41.180
It's just that technology
is just a tool, you know.

00:52:41.180 --> 00:52:44.410
It has to be socially
constructed, just like a hammer.

00:52:44.410 --> 00:52:47.850
I mean if you want to build a
shelter to protect yourself,

00:52:47.850 --> 00:52:49.880
you need something
equivalent to a hammer.

00:52:49.880 --> 00:52:51.180
Now, everything equivalent

00:52:51.180 --> 00:52:52.580
to a hammer can be
used to kill somebody.

00:52:52.580 --> 00:52:55.180
It's not the hammer's fault.

00:52:55.180 --> 00:52:57.050
But if you want to go
on evolution, you know,

00:52:57.050 --> 00:52:58.910
if you want to be a
civilization, we need to,

00:52:58.910 --> 00:53:01.210
need something like a hammer
in order to build a shelter.

00:53:01.210 --> 00:53:03.240
You know, otherwise, you'll
be back with the animals.

00:53:03.240 --> 00:53:06.190
You know, like, but the
hammer is just, you know,

00:53:06.190 --> 00:53:08.950
and any technology is a
tool like that that has

00:53:08.950 --> 00:53:10.830
to be socially constructed.

00:53:10.830 --> 00:53:15.060
It's not technologically
deterministic in that sense.

00:53:15.060 --> 00:53:18.830
So it can be used for
participation, it can be used

00:53:18.830 --> 00:53:20.620
for participation for
a very good sense.

00:53:20.620 --> 00:53:23.320
A colleague of mine at
MIT [inaudible] he talks

00:53:23.320 --> 00:53:28.200
about this idea, he just gave a
Ted Talk last week about that.

00:53:28.200 --> 00:53:31.510
How actually, his idea would be
to kind of like have an Avatar

00:53:31.510 --> 00:53:34.970
for each one of us that
basically reads all

00:53:34.970 --> 00:53:37.790
of our digital footprints
all the time what we read,

00:53:37.790 --> 00:53:39.890
what we do on Facebook,
who we're in contact with.

00:53:39.890 --> 00:53:42.110
What our friends
think and so forth.

00:53:42.110 --> 00:53:45.770
And this Avatar then basically
internalizes our political

00:53:45.770 --> 00:53:47.070
views, right.

00:53:47.070 --> 00:53:51.390
And then we create this Congress
supplement or complementary

00:53:51.390 --> 00:53:53.060
to existing Congress, right,

00:53:53.060 --> 00:53:56.150
where we send 250
million Avatars.

00:53:56.150 --> 00:53:57.620
And if a bill goes
through, you know,

00:53:57.620 --> 00:53:59.930
these Avatars then
basically say well, I'm busy.

00:53:59.930 --> 00:54:01.230
I have other things
to do, you know.

00:54:01.230 --> 00:54:04.660
So I sent my Avatar to this
hearing, and my Avatar is

00:54:04.660 --> 00:54:06.480
like now, like my opinion
would be like this.

00:54:06.480 --> 00:54:07.780
You have to change that or that,

00:54:07.780 --> 00:54:09.080
or it doesn't fly
with me, right.

00:54:09.080 --> 00:54:10.570
So, it would be a complete
director [inaudible]

00:54:10.570 --> 00:54:12.260
because they always
update it in real time,

00:54:12.260 --> 00:54:15.200
like I update my Avatar in
real time, as all of you are.

00:54:15.200 --> 00:54:17.530
And we have this 250
million people assembly there

00:54:17.530 --> 00:54:21.370
of Avatars assembly
there, right.

00:54:21.370 --> 00:54:23.060
And, you know, these
digital footprints exist.

00:54:23.060 --> 00:54:24.760
I mean Facebook has them.

00:54:24.760 --> 00:54:26.320
Why, and Google has them.

00:54:26.320 --> 00:54:27.630
Why shouldn't my
Avatar have them?

00:54:27.630 --> 00:54:30.370
And I send them to
Congress, right.

00:54:30.370 --> 00:54:31.850
So is there a big benefit?

00:54:31.850 --> 00:54:34.150
And this is almost like
direct democracy then.

00:54:34.150 --> 00:54:37.500
Now, remind ourselves
the founding fathers

00:54:37.500 --> 00:54:39.260
of this Constitution,
of this country here,

00:54:39.260 --> 00:54:41.930
were very skeptical of tyrant
democracy, right, because,

00:54:41.930 --> 00:54:44.040
you know, the mob
killed Socrates, right.

00:54:44.040 --> 00:54:46.830
So we need a mechanism
to refine and to enlarge,

00:54:46.830 --> 00:54:49.400
and that's why a representative
of democracy was created.

00:54:49.400 --> 00:54:52.690
So going, we have a
lot of tools for that

00:54:52.690 --> 00:54:54.130
to make participation better.

00:54:54.130 --> 00:54:56.450
But also, that's not
the entire solution.

00:54:56.450 --> 00:54:58.740
We need to also like
representative democracy is more

00:54:58.740 --> 00:55:01.740
than just going back
to Athens, you know.

00:55:01.740 --> 00:55:03.400
So to direct democracy.

00:55:03.400 --> 00:55:06.800
On the other hand, on the
other extreme of the question

00:55:06.800 --> 00:55:09.860
with using abusing
that, yes, absolutely.

00:55:09.860 --> 00:55:13.510
I mean the oldest vision
of this entire scenario

00:55:13.510 --> 00:55:16.050
of what we call maybe
information society,

00:55:16.050 --> 00:55:18.030
[inaudible] society is it
doesn't come from an academic.

00:55:18.030 --> 00:55:21.070
It comes from George
Orwell in 1948, right.

00:55:21.070 --> 00:55:22.370
&gt;&gt; John Haskell: '84, right.

00:55:22.370 --> 00:55:23.670
He wrote it in '48, yeah.

00:55:23.670 --> 00:55:25.210
&gt;&gt; Martin Hilbert: Yeah,
in '48 it wrote '84, right.

00:55:25.210 --> 00:55:27.020
So, the academics
started to talk

00:55:27.020 --> 00:55:28.870
about the information
society in the '80s.

00:55:28.870 --> 00:55:31.150
He talked about that
in '48, right.

00:55:31.150 --> 00:55:33.630
And it was this kind
of vision, you know.

00:55:33.630 --> 00:55:36.610
I mean nowadays he would
probably turn in his grave

00:55:36.610 --> 00:55:38.380
if he knew what was
going on, right.

00:55:38.380 --> 00:55:39.920
Because it's much more severe

00:55:39.920 --> 00:55:43.370
than he could have ever
envisioned back then already

00:55:43.370 --> 00:55:46.840
with what, government
industrial complex,

00:55:46.840 --> 00:55:51.810
that's how he called it,
knows about us, right.

00:55:51.810 --> 00:55:53.360
&gt;&gt; John Haskell: Gentleman
two down is the next,

00:55:53.360 --> 00:55:56.010
and then we'll come over here.

00:55:56.010 --> 00:55:57.950
&gt;&gt; Excuse me.

00:55:57.950 --> 00:56:01.340
I came in late, so I don't
know if you've touched on this.

00:56:01.340 --> 00:56:05.340
But this is a question about
the, highly speculative question

00:56:05.340 --> 00:56:08.030
about the future of
artificial intelligence.

00:56:08.030 --> 00:56:13.610
This is the 50th anniversary of
2001 a Space Odyssey and how.

00:56:13.610 --> 00:56:15.730
And I think we're
mostly familiar

00:56:15.730 --> 00:56:18.980
with the outcome of that.

00:56:18.980 --> 00:56:22.380
Scientists have seriously
thought

00:56:22.380 --> 00:56:26.440
that true artificial
intelligence can be achieved,

00:56:26.440 --> 00:56:28.660
not in the far distant future.

00:56:28.660 --> 00:56:31.790
But perhaps with the
next 30, 40 years.

00:56:31.790 --> 00:56:34.810
What would happen to all
these algorithms that we now,

00:56:34.810 --> 00:56:38.630
we nicely plan and
put into computers,

00:56:38.630 --> 00:56:40.790
when the computers
themselves think

00:56:40.790 --> 00:56:43.230
and decide well,
about discrimination.

00:56:43.230 --> 00:56:46.160
Well maybe democracy
isn't such a good idea.

00:56:46.160 --> 00:56:50.100
Maybe it's more effective to
have say [inaudible] thought,

00:56:50.100 --> 00:56:52.730
you know, a philosopher king.

00:56:52.730 --> 00:56:55.560
Or us, the computers,
who know far more

00:56:55.560 --> 00:56:57.680
than these petty human lives.

00:56:57.680 --> 00:57:00.290
So I mean, I know this
sounds like science fiction,

00:57:00.290 --> 00:57:03.150
but science fiction has
gone science back rapidly

00:57:03.150 --> 00:57:04.890
in the last 50 years.

00:57:04.890 --> 00:57:06.320
&gt;&gt; Martin Hilbert: Right.

00:57:06.320 --> 00:57:08.480
Yeah, wow, that's
a big question.

00:57:08.480 --> 00:57:11.620
I mean two, three
things coming to mind.

00:57:11.620 --> 00:57:14.960
So obviously, the question is
with the singularity, right.

00:57:14.960 --> 00:57:18.830
So, I do, I think
it's complementary.

00:57:18.830 --> 00:57:20.130
So there are two things.

00:57:20.130 --> 00:57:22.820
I don't think that artificial
intelligence basically cares

00:57:22.820 --> 00:57:24.730
about replicating
us or going to,

00:57:24.730 --> 00:57:28.650
when we say true intelligence,
it's kind of like, you know,

00:57:28.650 --> 00:57:31.040
you don't have to figure
out how a Colibri flies

00:57:31.040 --> 00:57:32.890
in order to build helicopters.

00:57:32.890 --> 00:57:34.190
You know, it's just
that helicopters

00:57:34.190 --> 00:57:36.220
that Colibri cannot
do and drones can't

00:57:36.220 --> 00:57:37.520
that Colibri cannot do.

00:57:37.520 --> 00:57:39.060
So it's not, like you
don't have to like,

00:57:39.060 --> 00:57:40.560
oh how does the brain work?

00:57:40.560 --> 00:57:42.440
It doesn't really
matter how the brain,

00:57:42.440 --> 00:57:45.190
it doesn't really matter how
birds fly in order for us

00:57:45.190 --> 00:57:46.960
to fly to the moon, right.

00:57:46.960 --> 00:57:50.390
So if you have infinite human
resources, get some people

00:57:50.390 --> 00:57:51.690
to study how the brain works

00:57:51.690 --> 00:57:52.990
and maybe you can learn
something from that.

00:57:52.990 --> 00:57:54.290
But you don't really need to.

00:57:54.290 --> 00:57:57.250
Once you understand the
general concept of aerodynamics,

00:57:57.250 --> 00:57:59.140
you can build all kind
of flying machines.

00:57:59.140 --> 00:58:00.440
You don't need to study birds.

00:58:00.440 --> 00:58:03.740
You can still do, but it's just
this one solution nature came

00:58:03.740 --> 00:58:05.040
up with, right.

00:58:05.040 --> 00:58:06.340
So same with intelligence.

00:58:06.340 --> 00:58:07.670
What is true intelligence
is actually an

00:58:07.670 --> 00:58:08.970
interesting question.

00:58:08.970 --> 00:58:10.780
Because we have only
like this one thing

00:58:10.780 --> 00:58:12.720
that Mother Nature came
up with after tinkering,

00:58:12.720 --> 00:58:16.020
but that's basically it, right.

00:58:16.020 --> 00:58:19.850
So, yeah, I think it's not like,

00:58:19.850 --> 00:58:21.780
I don't think it's the
terminators and so forth,

00:58:21.780 --> 00:58:23.260
it's just a different
kind of intelligence.

00:58:23.260 --> 00:58:26.770
And that brings me to the second
point is it's the singularity.

00:58:26.770 --> 00:58:28.460
It's not like we will
have a terminator

00:58:28.460 --> 00:58:29.760
which will be like us.

00:58:29.760 --> 00:58:32.830
No, it's more like a jet
plane compared to a Colibri,

00:58:32.830 --> 00:58:34.130
or a helicopter compared
to a Colibri

00:58:34.130 --> 00:58:36.030
and a jet plane compared
to a Condor, right.

00:58:36.030 --> 00:58:38.520
It's more like this that
the difference will be there

00:58:38.520 --> 00:58:40.440
complementary doing
different things similar

00:58:40.440 --> 00:58:42.860
in the same concept, but
complementary things.

00:58:42.860 --> 00:58:44.700
And I think this
digital technology

00:58:44.700 --> 00:58:47.380
where it's very good is
it brings us together,

00:58:47.380 --> 00:58:49.320
especially talking
about governance, right.

00:58:49.320 --> 00:58:51.330
It's kind of the glue
that stitches us together.

00:58:51.330 --> 00:58:54.570
I'm in the department of
communication because, you know,

00:58:54.570 --> 00:58:56.330
it's communication
that holds us together.

00:58:56.330 --> 00:58:58.540
And that's what's
being digitalized.

00:58:58.540 --> 00:59:01.830
Laws are the same, the
communicative structures laws,

00:59:01.830 --> 00:59:03.530
algorithms, you know,
on this level.

00:59:03.530 --> 00:59:06.250
So laws are algorithms
on this bird's eye view.

00:59:06.250 --> 00:59:08.460
And then that level, we already
merged with the technology.

00:59:08.460 --> 00:59:12.080
I mean singularity
already happened.

00:59:12.080 --> 00:59:16.320
And 80% of the transaction on
the stock market are decided

00:59:16.320 --> 00:59:17.830
by artificial intelligence.

00:59:17.830 --> 00:59:20.330
Ninety-nine point nine
percent of the decisions

00:59:20.330 --> 00:59:23.670
on the electric grid are made
by artificial intelligence.

00:59:23.670 --> 00:59:28.920
And over half of marriages in
the United States are decided

00:59:28.920 --> 00:59:31.580
by matching algorithms,
artificial intelligence guided

00:59:31.580 --> 00:59:32.910
on dating sites, right.

00:59:32.910 --> 00:59:36.440
So if you tell me, hey Martin,
we found this new species,

00:59:36.440 --> 00:59:39.070
our extraterrestrial
species, right,

00:59:39.070 --> 00:59:41.140
80% of the resource
distribution,

00:59:41.140 --> 00:59:43.780
100% of the energy distribution,

00:59:43.780 --> 00:59:46.570
and 50% of the procreation
decisions,

00:59:46.570 --> 00:59:49.580
are taken by this
thing called AI.

00:59:49.580 --> 00:59:52.140
I would say like hey,
yeah, that's one, right.

00:59:52.140 --> 00:59:53.700
You guys already merged, right.

00:59:53.700 --> 00:59:56.380
You already, it's
inseparable already

00:59:56.380 --> 00:59:58.590
on a bird's eye view
societal level.

00:59:58.590 --> 01:00:00.180
And that's what I
use society, right.

01:00:00.180 --> 01:00:03.870
Now, you could turn off your
cellphones, never again be

01:00:03.870 --> 01:00:06.620
in touch with money, because
that's all digitalized, right.

01:00:06.620 --> 01:00:09.260
Go to the mountains, and you
probably will also survive

01:00:09.260 --> 01:00:11.690
without any contact
or digital whatsoever.

01:00:11.690 --> 01:00:13.910
You would be able to
survive for a few years.

01:00:13.910 --> 01:00:16.230
But under no circumstance
can you claim

01:00:16.230 --> 01:00:18.630
that you coevolve with us.

01:00:18.630 --> 01:00:21.220
Right. Human evolution
has already merged.

01:00:21.220 --> 01:00:22.900
On an evolutionary level,

01:00:22.900 --> 01:00:26.320
we already have a socially
technological system, right.

01:00:26.320 --> 01:00:29.240
So you can step out, but
you're not evolving anymore

01:00:29.240 --> 01:00:33.760
in a biologically evolutionary
sense with us as society.

01:00:33.760 --> 01:00:36.330
Right, so I think singularity
has already happened.

01:00:36.330 --> 01:00:37.910
And I think the biggest
effects are

01:00:37.910 --> 01:00:39.530
from this bird's eye view level,

01:00:39.530 --> 01:00:41.070
because who cares
about this brain.

01:00:41.070 --> 01:00:42.370
It's just one brain.

01:00:42.370 --> 01:00:45.570
It's kind of like okay, you have
birds, birds and planes, but,

01:00:45.570 --> 01:00:46.870
you know, these are
just different things

01:00:46.870 --> 01:00:48.170
that make a bigger hole.

01:00:48.170 --> 01:00:50.650
&gt;&gt; John Haskell: So we're
going to do one quick question,

01:00:50.650 --> 01:00:55.100
and then we, and the exciting
thing is there is like wine

01:00:55.100 --> 01:00:57.090
over there when we're done.

01:00:57.090 --> 01:01:00.580
The last one is the
lady in the middle here.

01:01:00.580 --> 01:01:01.880
She'll have the last question.

01:01:01.880 --> 01:01:03.960
And then people can
have at Martin.

01:01:03.960 --> 01:01:07.040
He has no bodyguard.

01:01:07.040 --> 01:01:13.490
&gt;&gt; Okay, I feel like when
I use Facebook I can get it

01:01:13.490 --> 01:01:16.840
to make me aware
of a lot of things

01:01:16.840 --> 01:01:22.520
that it might not naturally
choose to by selectively looking

01:01:22.520 --> 01:01:25.720
at other things and liking them.

01:01:25.720 --> 01:01:27.450
Like if I read the New York,

01:01:27.450 --> 01:01:29.860
if I read the Washington
Post then I'll look

01:01:29.860 --> 01:01:31.790
at the Washington Times.

01:01:31.790 --> 01:01:34.770
And if I get things

01:01:34.770 --> 01:01:38.150
about certain problems,
I'll try to look up.

01:01:38.150 --> 01:01:40.710
And once I've done that,
it just keeps feeding me

01:01:40.710 --> 01:01:42.010
from both sides.

01:01:42.010 --> 01:01:46.760
I mean, if I make a few choices,
it'll just keep feeding me.

01:01:46.760 --> 01:01:48.900
&gt;&gt; Martin Hilbert: Right.

01:01:48.900 --> 01:01:51.600
Yeah, so, that's a question
how these algorithms work

01:01:51.600 --> 01:01:52.910
and how those algorithms
are set up.

01:01:52.910 --> 01:01:55.670
So Facebook has set up
the algorithm in order to,

01:01:55.670 --> 01:01:58.010
like Facebook, what all
the social media sites try

01:01:58.010 --> 01:02:01.000
to maximize is to keep you on
the site as long as possible.

01:02:01.000 --> 01:02:03.370
All right, and they
have these maintenance

01:02:03.370 --> 01:02:04.670
optimization algorithms.

01:02:04.670 --> 01:02:07.340
So YouTube and Facebook
and whatever,

01:02:07.340 --> 01:02:10.030
they try to get every split
second out of you to stay longer

01:02:10.030 --> 01:02:11.880
at the site because they
have more interaction,

01:02:11.880 --> 01:02:14.510
more data about you, better
ads and so forth, right.

01:02:14.510 --> 01:02:15.810
That's the business model.

01:02:15.810 --> 01:02:18.940
So, the easiest way to
keep you on the site is

01:02:18.940 --> 01:02:20.490
to show you what you like.

01:02:20.490 --> 01:02:22.650
We know if we show you exactly
the opposite of what you

01:02:22.650 --> 01:02:23.980
like you will run, right.

01:02:23.980 --> 01:02:30.430
&gt;&gt; I say I like this and the
opposite thing [inaudible].

01:02:30.430 --> 01:02:32.460
&gt;&gt; Martin Hilbert: Well, you can
try to confuse the algorithm,

01:02:32.460 --> 01:02:34.180
but your digital footprint
is way too complete

01:02:34.180 --> 01:02:35.520
as to really confuse it, right.

01:02:35.520 --> 01:02:38.040
I mean bots sometimes
try to confuse that.

01:02:38.040 --> 01:02:39.400
And that's been the
solution to it.

01:02:39.400 --> 01:02:42.300
But, your digital footprint
is from all the different,

01:02:42.300 --> 01:02:43.600
from all the different ads.

01:02:43.600 --> 01:02:44.900
It's not only on Facebook.

01:02:44.900 --> 01:02:46.780
Facebook is everywhere,
right, even if you're not,

01:02:46.780 --> 01:02:48.420
you can download
your Facebook data.

01:02:48.420 --> 01:02:51.510
You will see that most of
it is actually not collected

01:02:51.510 --> 01:02:52.810
on Facebook.

01:02:52.810 --> 01:02:54.260
If you don't have
a Facebook profile,

01:02:54.260 --> 01:02:57.260
Facebook still has a
dataset about you, right.

01:02:57.260 --> 01:03:00.650
I invite everybody,
just go to your email.

01:03:00.650 --> 01:03:04.000
Go to the search function
and type in Facebook.

01:03:04.000 --> 01:03:06.760
You will see that almost
every email in your inbox,

01:03:06.760 --> 01:03:08.570
something to do with
Facebook, right.

01:03:08.570 --> 01:03:11.090
So most websites have the
Facebook pixel on them.

01:03:11.090 --> 01:03:12.390
So independent government,

01:03:12.390 --> 01:03:14.010
even other they have
the Facebook pixel.

01:03:14.010 --> 01:03:15.630
So they track you
when you get there.

01:03:15.630 --> 01:03:16.930
So it's everywhere.

01:03:16.930 --> 01:03:19.940
So they actually have a much
too complete digital footprint

01:03:19.940 --> 01:03:23.740
as to confuse them with one
or two different likes, right.

01:03:23.740 --> 01:03:26.940
Now you can completely
try to be another person,

01:03:26.940 --> 01:03:28.820
but then in a Goffmanian sense,

01:03:28.820 --> 01:03:30.470
you will become this
other person, right,

01:03:30.470 --> 01:03:32.020
because we are all actors.

01:03:32.020 --> 01:03:34.600
&gt;&gt; John Haskell: All right, we
have to go to the reception.

01:03:34.600 --> 01:03:35.900
Thank you all very much.

01:03:35.900 --> 01:03:37.200
Thank you, Martin.

01:03:37.200 --> 01:03:38.500
&gt;&gt; Martin Hilbert: Yeah.

01:03:38.500 --> 01:03:40.210
&gt;&gt; John Haskell: This is great.

01:03:40.210 --> 01:03:41.510
&gt;&gt; Martin Hilbert: Great.

01:03:41.510 --> 01:03:44.470
&gt;&gt; This has been a presentation
of the Library of Congress.

01:03:44.470 --> 01:03:46.970
Visit us at loc.gov.

