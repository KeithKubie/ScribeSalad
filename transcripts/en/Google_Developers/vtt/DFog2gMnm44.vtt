WEBVTT
Kind: captions
Language: en

00:00:03.097 --> 00:00:04.930
CHRISTIAN PLAGEMANN:
Good morning, everyone,

00:00:04.930 --> 00:00:07.190
to this morning
session on Cardboard,

00:00:07.190 --> 00:00:09.360
virtual reality for Android.

00:00:09.360 --> 00:00:11.120
I've got with me here David Coz.

00:00:11.120 --> 00:00:14.090
He's a software engineer at
Google's Cultural Institute

00:00:14.090 --> 00:00:15.290
in Paris.

00:00:15.290 --> 00:00:16.690
My name is Christian Plagemann.

00:00:16.690 --> 00:00:17.950
I'm a research
scientist and team

00:00:17.950 --> 00:00:20.491
lead for physical interaction
research here in Mountain View.

00:00:20.491 --> 00:00:23.110
And we have Boris Smus, who's
a prototyper software engineer

00:00:23.110 --> 00:00:27.900
and researcher also in our
group in Mountain View.

00:00:27.900 --> 00:00:31.770
So this talk is about
a piece of cardboard.

00:00:31.770 --> 00:00:33.590
Or let's say what it
actually is about,

00:00:33.590 --> 00:00:35.820
is about some surprising
things that your phone

00:00:35.820 --> 00:00:40.500
can do when you just add a
little bit of extra equipment.

00:00:40.500 --> 00:00:42.960
And for those of you
who've been here yesterday,

00:00:42.960 --> 00:00:44.880
after the keynote
and in the afternoon,

00:00:44.880 --> 00:00:47.396
they know of course exactly
what I'm talking about.

00:00:47.396 --> 00:00:49.020
For those who are
watching from remote,

00:00:49.020 --> 00:00:51.300
let me just a replay a
few moments of yesterday.

00:00:55.654 --> 00:00:56.320
[VIDEO PLAYBACK]

00:00:56.320 --> 00:00:59.170
-We're going to hand each and
every one of you a cardboard.

00:00:59.170 --> 00:01:00.120
-Here you go.

00:01:00.120 --> 00:01:02.226
-There's a piece
of adhesive tape.

00:01:02.226 --> 00:01:07.110
[CHATTER]

00:01:07.110 --> 00:01:08.420
-Wow, that's great.

00:01:08.420 --> 00:01:09.760
-Oh, dang.

00:01:09.760 --> 00:01:10.510
-Oh, wow.

00:01:15.460 --> 00:01:16.450
-Wow.

00:01:16.450 --> 00:01:17.935
-Oh, fancy.

00:01:17.935 --> 00:01:20.420
-I think I love this.

00:01:20.420 --> 00:01:22.190
-Wow, that's crazy.

00:01:22.190 --> 00:01:24.640
-Oh my god!

00:01:24.640 --> 00:01:26.110
-Oh, this is incredible.

00:01:26.110 --> 00:01:27.580
-Oh, that's insane.

00:01:27.580 --> 00:01:31.010
-Oh, cool.

00:01:31.010 --> 00:01:31.980
[GASP]

00:01:31.980 --> 00:01:32.480
-Wow.

00:01:32.480 --> 00:01:33.940
That's awesome.

00:01:33.940 --> 00:01:34.440
-Cool.

00:01:34.440 --> 00:01:36.400
That's fantastic.

00:01:36.400 --> 00:01:40.320
--[GASP] It says "Fly to Space,"
when you look up into the sky.

00:01:40.320 --> 00:01:43.850
[LAUGHTER]

00:01:43.850 --> 00:01:45.362
-This is so [BLEEP] clever.

00:01:45.362 --> 00:01:47.730
[LAUGHTER]

00:01:47.730 --> 00:01:50.630
[END VIDEOPLAYBACK]

00:01:50.630 --> 00:01:54.963
[APPLAUSE]

00:01:58.410 --> 00:01:59.410
CHRISTIAN PLAGEMANN: OK.

00:01:59.410 --> 00:02:01.050
Here goes my clicker.

00:02:01.050 --> 00:02:03.850
So what you got
yesterday, all of you,

00:02:03.850 --> 00:02:05.956
were these simple
cardboard boxes.

00:02:05.956 --> 00:02:08.539
And what that basically does is
you insert your phone into it,

00:02:08.539 --> 00:02:10.930
and it kind of turns your
phone into a very basic

00:02:10.930 --> 00:02:12.450
virtual reality headset.

00:02:12.450 --> 00:02:16.100
And what you can
do with that is you

00:02:16.100 --> 00:02:18.550
can look around and experience
content in another place,

00:02:18.550 --> 00:02:21.050
whether it's real or artificial,
in a totally different way.

00:02:21.050 --> 00:02:22.560
Like not on a small
postcard size,

00:02:22.560 --> 00:02:24.040
but actually all around you.

00:02:24.040 --> 00:02:26.820
And we are making
the hardware design

00:02:26.820 --> 00:02:29.534
and an open-source
software toolkit

00:02:29.534 --> 00:02:31.200
openly available to
everyone, because we

00:02:31.200 --> 00:02:33.960
want people to get started.

00:02:33.960 --> 00:02:37.270
There's a lot of development
in this area already,

00:02:37.270 --> 00:02:39.400
and we want to really
kickstart a lot more of that.

00:02:39.400 --> 00:02:41.850
And that's why we want to
get it into people's hands.

00:02:41.850 --> 00:02:45.460
And to just to add a few
other surprising things that

00:02:45.460 --> 00:02:49.670
happened yesterday, so just
two hours or three hours

00:02:49.670 --> 00:02:52.136
after we posted our
designs on the website,

00:02:52.136 --> 00:02:53.510
somebody actually
came up already

00:02:53.510 --> 00:02:55.020
with their own
construction of it.

00:02:55.020 --> 00:02:57.170
It's a little bit
rough around the edges.

00:02:57.170 --> 00:02:58.220
[LAUGHTER]

00:02:58.220 --> 00:03:00.810
But wait another two hours,
and on the first online store,

00:03:00.810 --> 00:03:03.200
there was actually, you could
order Google's Cardboard VR

00:03:03.200 --> 00:03:03.700
Tookit.

00:03:03.700 --> 00:03:05.252
[LAUGHTER AND APPLAUSE]

00:03:05.252 --> 00:03:08.280
And we actually express
ordered one yesterday.

00:03:08.280 --> 00:03:10.750
So we kind of ran
out, so maybe we

00:03:10.750 --> 00:03:13.120
can get some more to give
out at the conference,

00:03:13.120 --> 00:03:16.070
but they haven't arrived yet.

00:03:16.070 --> 00:03:19.370
We got the lenses people
can order on the store

00:03:19.370 --> 00:03:21.364
through our friends
Durovis Dive,

00:03:21.364 --> 00:03:23.280
and those actually all
sold out in 20 minutes.

00:03:23.280 --> 00:03:25.810
So this is really
what we had hoped for,

00:03:25.810 --> 00:03:29.710
just 100 times faster than
we kind of anticipated.

00:03:29.710 --> 00:03:32.036
And the goal of this
talk today, which

00:03:32.036 --> 00:03:33.910
is obviously brief for
a big topic like this,

00:03:33.910 --> 00:03:37.080
is we want to give you
a rough overview of how

00:03:37.080 --> 00:03:40.000
virtual reality on a mobile
phone works in principle.

00:03:40.000 --> 00:03:42.840
We want to explain to you
how this project came about

00:03:42.840 --> 00:03:43.880
and why we started it.

00:03:43.880 --> 00:03:45.520
It was really like
a side project.

00:03:45.520 --> 00:03:48.730
And most importantly
for you as developers,

00:03:48.730 --> 00:03:50.920
we want to walk you through
our open source software

00:03:50.920 --> 00:03:55.160
toolkit and a sample app, so
that you can start hacking

00:03:55.160 --> 00:03:56.010
right away.

00:03:56.010 --> 00:03:58.880
And with this, I hand
over to David Coz.

00:03:58.880 --> 00:04:00.630
He actually started
this as a side project

00:04:00.630 --> 00:04:01.830
six months ago in Paris.

00:04:01.830 --> 00:04:03.897
This is David.

00:04:03.897 --> 00:04:05.230
DAVID COZ: Thank you, Christian.

00:04:05.230 --> 00:04:08.520
[APPLAUSE]

00:04:08.520 --> 00:04:09.450
Hi, everybody.

00:04:09.450 --> 00:04:12.370
So the first thing to know
is that I'm a big VR fan.

00:04:12.370 --> 00:04:15.480
I just love, you know,
the sense of immersion

00:04:15.480 --> 00:04:17.950
it gives, the fact that you
can just look around you

00:04:17.950 --> 00:04:21.690
in a very natural way at
all this content, be it

00:04:21.690 --> 00:04:25.100
created content or virtual
reality, pure content.

00:04:25.100 --> 00:04:27.670
And there's been
so much progress

00:04:27.670 --> 00:04:30.660
in the space in
the past few years.

00:04:30.660 --> 00:04:34.440
We have, obviously, Oculus
Rift doing a great job

00:04:34.440 --> 00:04:36.570
at putting VR back
into the media

00:04:36.570 --> 00:04:40.020
attention with an
awesome device.

00:04:40.020 --> 00:04:44.510
And also, a trend of
mobile phone-based viewers,

00:04:44.510 --> 00:04:48.190
like the Dorovis Dive or
other do-it-yourself projects.

00:04:48.190 --> 00:04:49.880
And with my friend
Damien in Paris,

00:04:49.880 --> 00:04:52.260
about six months ago, we
were really enthusiastic

00:04:52.260 --> 00:04:54.220
about this progress,
and we're thinking,

00:04:54.220 --> 00:04:56.680
how can we accelerate
this even more,

00:04:56.680 --> 00:05:01.430
so that not just tech
addicts can experience VR,

00:05:01.430 --> 00:05:03.880
but you know, anybody
with a smartphone?

00:05:03.880 --> 00:05:09.540
So our idea was really to build
the simplest and cheapest way

00:05:09.540 --> 00:05:13.360
of turning your smartphone
into A VR viewer.

00:05:13.360 --> 00:05:16.990
And that's how we-- we
started to collaborate

00:05:16.990 --> 00:05:20.205
about six months ago in
the Cultural Institute Lab

00:05:20.205 --> 00:05:22.210
at our office in
Paris, which is,

00:05:22.210 --> 00:05:26.180
you know, a lab space where
engineers and artists,

00:05:26.180 --> 00:05:29.110
thinkers, can
collaborate on projects.

00:05:29.110 --> 00:05:31.730
And we started to
really naturally try it

00:05:31.730 --> 00:05:34.070
with cardboard,
because it's so easy.

00:05:34.070 --> 00:05:36.930
And we know we cut some
cardboard into a box,

00:05:36.930 --> 00:05:40.830
and quickly came up with
is really rough design.

00:05:40.830 --> 00:05:44.060
Looks a bit like the one you
saw earlier, with Christian.

00:05:44.060 --> 00:05:47.030
That really worked
well already, and I

00:05:47.030 --> 00:05:50.590
decided to work on this
as 20% project, which

00:05:50.590 --> 00:05:52.960
is a side project
you can do at Google.

00:05:52.960 --> 00:05:59.150
And we used the laser cutter
to build the first batch

00:05:59.150 --> 00:06:04.940
of almost identical goggles that
we showed around at the office.

00:06:04.940 --> 00:06:10.670
People got really excited, so
I decided to take this with me

00:06:10.670 --> 00:06:13.230
to our main office
here in Mountain View.

00:06:13.230 --> 00:06:16.150
And that's how I met
Christian and his team.

00:06:16.150 --> 00:06:18.800
And they got really
excited and enthusiastic

00:06:18.800 --> 00:06:20.920
about this product,
and together we

00:06:20.920 --> 00:06:24.640
decided to take it
a few steps further,

00:06:24.640 --> 00:06:28.360
in terms of hardware
design-- also, the software

00:06:28.360 --> 00:06:31.766
toolkit and the experiences.

00:06:31.766 --> 00:06:33.810
And now actually
to run some demos,

00:06:33.810 --> 00:06:39.430
I'm going to ask Boris to
build a headset for us.

00:06:39.430 --> 00:06:41.400
So Boris, everyone
is watching you.

00:06:41.400 --> 00:06:43.190
You have 30 seconds.

00:06:43.190 --> 00:06:45.990
No pressure.

00:06:45.990 --> 00:06:49.790
So while Boris is
assembling the viewer,

00:06:49.790 --> 00:06:53.090
let me emphasize why we
actually chose cardboard

00:06:53.090 --> 00:06:55.210
and we actually stick to it.

00:06:55.210 --> 00:06:57.180
The first reason is
we wanted the viewer

00:06:57.180 --> 00:06:59.090
to feel and look really simple.

00:06:59.090 --> 00:07:01.510
Everybody's familiar
with cardboard.

00:07:01.510 --> 00:07:05.240
And the idea there is
that your phone is really

00:07:05.240 --> 00:07:06.900
the piece that does
the magic here.

00:07:06.900 --> 00:07:09.680
Your phones are so
powerful, it was just

00:07:09.680 --> 00:07:12.960
great to add this simple
viewer on top of it.

00:07:12.960 --> 00:07:15.830
The second reason is we
want you guys to just take

00:07:15.830 --> 00:07:20.190
scissors, staplers, and just
go crazy at it and modify it.

00:07:20.190 --> 00:07:23.170
And it's just so natural
and easy with cardboard.

00:07:23.170 --> 00:07:26.910
We put the plans online, if
you go to our main website.

00:07:26.910 --> 00:07:28.570
And it's open for grabs.

00:07:28.570 --> 00:07:32.640
Just go crazy at it and
send us your modifications.

00:07:32.640 --> 00:07:34.060
We'll be really
happy to see them.

00:07:40.000 --> 00:07:41.991
So Boris, a bit slower
than the rehearsal.

00:07:41.991 --> 00:07:42.490
Yeah.

00:07:42.490 --> 00:07:43.440
OK, great job, Boris.

00:07:43.440 --> 00:07:45.950
[APPLAUSE]

00:07:45.950 --> 00:07:48.190
So now that we have these
viewer, the question is,

00:07:48.190 --> 00:07:50.090
what can we do with this?

00:07:50.090 --> 00:07:52.360
And the first thing that
we got really excited about

00:07:52.360 --> 00:07:55.780
is the possibility
to just go anywhere

00:07:55.780 --> 00:08:00.740
on Earth, leveraging, of course,
the geodata we have at Google.

00:08:00.740 --> 00:08:09.050
So we built this experience
with Google Earth,

00:08:09.050 --> 00:08:10.660
using geodata that we have.

00:08:10.660 --> 00:08:15.350
And Boris is going to take
you on a tour through this.

00:08:15.350 --> 00:08:20.670
So you can see here the kind of
traditional VR screen settings,

00:08:20.670 --> 00:08:24.430
with two screens,
one for each eye.

00:08:24.430 --> 00:08:27.020
And you can see how
the display adapts

00:08:27.020 --> 00:08:30.580
to the head movements in
a really reactive way.

00:08:30.580 --> 00:08:34.309
Look at how Boris is moving,
and this is adapting quite fast.

00:08:34.309 --> 00:08:38.650
So your phones are doing quite
a good job at this, actually.

00:08:38.650 --> 00:08:41.419
And then using the magnet
clicker on the side

00:08:41.419 --> 00:08:46.850
that we added, Boris can start
to fly around the mountains.

00:08:46.850 --> 00:08:50.190
And it's one thing-- I guess
if you guys will try it.

00:08:50.190 --> 00:08:51.740
It's one thing to
see it like this.

00:08:51.740 --> 00:08:53.770
It's totally another
thing to try it,

00:08:53.770 --> 00:08:56.620
because you have
this nice 3D effect.

00:08:56.620 --> 00:08:59.549
And of course, here the cables
adjust for screen mirroring.

00:08:59.549 --> 00:09:01.215
Otherwise, you can
just walk everywhere.

00:09:04.110 --> 00:09:08.970
And Boris can also start to
look up and fly to space.

00:09:11.720 --> 00:09:16.500
And from there, you can access
a selection of nice places

00:09:16.500 --> 00:09:19.630
that we selected for you
guys-- and precached, actually,

00:09:19.630 --> 00:09:23.680
so that we don't have
Wi-Fi problems at the I/O.

00:09:23.680 --> 00:09:27.780
And now he just teleports
himself too Chicago

00:09:27.780 --> 00:09:29.010
and can look around.

00:09:29.010 --> 00:09:33.930
So we really like this idea of
virtually visiting any place.

00:09:33.930 --> 00:09:37.240
And not only can
you visit a place,

00:09:37.240 --> 00:09:41.210
but you can also use VR
to learn more about them.

00:09:41.210 --> 00:09:43.370
And that was really
important for us,

00:09:43.370 --> 00:09:47.580
for my team in the Cultural
Institute in Paris.

00:09:47.580 --> 00:09:50.290
We wanted to build this
tour where you can, like,

00:09:50.290 --> 00:09:56.160
take people, take even kids, to
learn more about these places.

00:09:56.160 --> 00:09:58.490
So we used the Street
View data that we

00:09:58.490 --> 00:10:01.010
acquired for this
really iconic place.

00:10:01.010 --> 00:10:03.730
This is the Palace of
Versailles in France.

00:10:03.730 --> 00:10:05.910
And you have a
charming local guide

00:10:05.910 --> 00:10:09.160
that tells you stories
about this place,

00:10:09.160 --> 00:10:10.830
and you can just
really look around.

00:10:10.830 --> 00:10:15.770
It's a very nice way
of discovering content.

00:10:15.770 --> 00:10:18.120
Another kind of
interesting thing we tried

00:10:18.120 --> 00:10:24.190
is this YouTube integration,
where basically, it's

00:10:24.190 --> 00:10:26.830
a massive-- yes, it's
flickering a bit.

00:10:26.830 --> 00:10:27.330
Yeah.

00:10:27.330 --> 00:10:31.590
It's a massive, giant
home theater room

00:10:31.590 --> 00:10:34.910
where you have this big
screen in front of your eyes

00:10:34.910 --> 00:10:37.340
so you can play videos.

00:10:37.340 --> 00:10:39.890
And imagine it's like having
a giant screen just, like,

00:10:39.890 --> 00:10:41.810
two feet from your face.

00:10:41.810 --> 00:10:46.080
And we also kind of
made use of, you know,

00:10:46.080 --> 00:10:48.210
all this space that
you have around you.

00:10:48.210 --> 00:10:50.800
So it's a really
interesting use of VR,

00:10:50.800 --> 00:10:53.880
where we put video
thumbnails, and by looking

00:10:53.880 --> 00:10:56.680
at them-- whoops, something
went down, I think.

00:11:00.990 --> 00:11:05.760
So by looking at them and
using the magnet clicker,

00:11:05.760 --> 00:11:08.150
you can very naturally
select a video and play it.

00:11:12.890 --> 00:11:15.320
It's a bit shaky.

00:11:19.949 --> 00:11:22.240
So we're really excited about
this kind of integration.

00:11:22.240 --> 00:11:25.400
We think we can-- we
have lots of product

00:11:25.400 --> 00:11:27.010
at Google, lots of products.

00:11:27.010 --> 00:11:33.620
We can use VR to kind of
enhance existing products.

00:11:33.620 --> 00:11:38.050
And one similar such integration
we're really proud about

00:11:38.050 --> 00:11:40.510
is Google Maps integration.

00:11:40.510 --> 00:11:44.220
There's actually, right now,
as of today, integrations--

00:11:44.220 --> 00:11:47.230
we have a Street View
VR mode for Street View.

00:11:50.350 --> 00:11:53.410
There's something
wrong with the cables.

00:11:53.410 --> 00:11:55.920
So you can basically
go anywhere where

00:11:55.920 --> 00:12:00.930
we have Street View, pull
up the Street View data,

00:12:00.930 --> 00:12:02.600
and then you double tap.

00:12:06.890 --> 00:12:10.420
You double-tap on
the arrow icon,

00:12:10.420 --> 00:12:12.960
and you have a
side-by-side screen

00:12:12.960 --> 00:12:14.560
rendering of Street View.

00:12:14.560 --> 00:12:16.870
So you can put this on
your Cardboard device

00:12:16.870 --> 00:12:19.920
and get a really good
feel for the place.

00:12:19.920 --> 00:12:24.790
And this actually works on any
other phone-based VR viewer.

00:12:24.790 --> 00:12:28.220
So Durovis Dive,
Altergaze, any view.

00:12:28.220 --> 00:12:30.492
So we're really
happy about this.

00:12:30.492 --> 00:12:31.700
It's not just a demo anymore.

00:12:31.700 --> 00:12:33.033
It's actually in a live product.

00:12:35.420 --> 00:12:37.670
Another thing, as I
guess you noticed,

00:12:37.670 --> 00:12:41.180
these were all
Android-based applications.

00:12:41.180 --> 00:12:44.300
But you can also very
simply, very easily

00:12:44.300 --> 00:12:47.710
build similar applications
with a browser.

00:12:47.710 --> 00:12:53.510
And we have this set
of Chrome Experiments

00:12:53.510 --> 00:12:55.540
that you can find
out on our website.

00:12:55.540 --> 00:12:59.500
And this is just using
JavaScript and HTML5.

00:12:59.500 --> 00:13:02.170
It's very easy with
libraries like Three.js.

00:13:02.170 --> 00:13:05.260
So I encourage you to go on
our website and check out.

00:13:05.260 --> 00:13:09.280
The source is absolutely
small and nice.

00:13:09.280 --> 00:13:10.200
And it has nice music.

00:13:10.200 --> 00:13:12.616
Here we don't have sound, but
you have nice, catchy music,

00:13:12.616 --> 00:13:14.460
so try it.

00:13:14.460 --> 00:13:16.020
So we have a couple
of more demos.

00:13:16.020 --> 00:13:17.720
We don't have
really all the time

00:13:17.720 --> 00:13:19.310
in the world to present them.

00:13:19.310 --> 00:13:22.200
I wanted to
emphasize two things.

00:13:22.200 --> 00:13:24.970
The first thing is the Photo
Sphere demo is really cool.

00:13:24.970 --> 00:13:28.140
You take your Photo Sphere and
you can really naturally look

00:13:28.140 --> 00:13:30.290
around at a place
you visited before.

00:13:30.290 --> 00:13:33.420
I think it totally makes
sense with Photo Sphere.

00:13:33.420 --> 00:13:35.930
The second thing is Windy Day.

00:13:35.930 --> 00:13:38.650
They actually have
a booth right there

00:13:38.650 --> 00:13:41.980
at I/O. I encourage
you to check it out.

00:13:41.980 --> 00:13:46.450
They produced this really
nice storytelling movie

00:13:46.450 --> 00:13:48.620
in a 3D space, and you
can just look around,

00:13:48.620 --> 00:13:50.410
and depending on where
you're looking at,

00:13:50.410 --> 00:13:52.850
the story progresses.

00:13:52.850 --> 00:13:55.710
So yeah, again, we
really want to get

00:13:55.710 --> 00:13:59.500
you inspired by all these
experiences and demos.

00:13:59.500 --> 00:14:02.600
You can go on g.co/cardboard
and learn more about them,

00:14:02.600 --> 00:14:05.380
get the plans for this viewer.

00:14:05.380 --> 00:14:07.350
And more importantly,
we really want

00:14:07.350 --> 00:14:10.520
you to build on top
of what we provide,

00:14:10.520 --> 00:14:13.420
in terms of the viewer,
the toolkit, and the demos.

00:14:13.420 --> 00:14:18.716
And Christian is going to tell
you more about how you do this.

00:14:18.716 --> 00:14:23.104
[APPLAUSE]

00:14:23.104 --> 00:14:25.520
CHRISTIAN PLAGEMANN: So there's
really one important thing

00:14:25.520 --> 00:14:27.590
we want to emphasize, which
is this is just a placeholder.

00:14:27.590 --> 00:14:29.250
Like, this is a
piece of cardboard.

00:14:29.250 --> 00:14:31.820
I think TechCrunch wrote
this very nicely yesterday.

00:14:31.820 --> 00:14:32.700
Is that an Oculus?

00:14:32.700 --> 00:14:33.470
Of course not.

00:14:33.470 --> 00:14:34.420
It's cardboard.

00:14:34.420 --> 00:14:38.370
But we think it can get you
started, like, fairly quickly.

00:14:38.370 --> 00:14:40.080
So that's the point of it.

00:14:40.080 --> 00:14:41.850
And kind of the
principles behind VR

00:14:41.850 --> 00:14:44.640
are pretty uniformly the same.

00:14:44.640 --> 00:14:47.180
And I just want to spend a few
minutes on describing those

00:14:47.180 --> 00:14:51.850
to you, before Boris goes
into the coding part.

00:14:51.850 --> 00:14:54.430
So what do you
need to actually do

00:14:54.430 --> 00:14:57.810
to turn a phone
into a VR headset?

00:14:57.810 --> 00:15:01.484
I want to describe three aspects
that we put into this viewer.

00:15:01.484 --> 00:15:02.900
The one, obviously,
is the optics.

00:15:02.900 --> 00:15:04.774
That's kind of one of
the crucial components.

00:15:04.774 --> 00:15:07.410
The other one is you want to
interact with the experience,

00:15:07.410 --> 00:15:08.850
so we built this little clicker.

00:15:08.850 --> 00:15:12.820
And the NFC tag, which is a neat
little idea that lets a phone

00:15:12.820 --> 00:15:15.420
go automatically from a phone
experience into a VR experience

00:15:15.420 --> 00:15:17.010
and back, like, seamlessly.

00:15:17.010 --> 00:15:18.980
So how does it work?

00:15:18.980 --> 00:15:21.372
So imagine you rip off the
top, which you can easily

00:15:21.372 --> 00:15:25.290
do-- you just need to build
another one then-- to basically

00:15:25.290 --> 00:15:27.790
realize there are two holes,
and then at a certain distance,

00:15:27.790 --> 00:15:29.110
there's the screen.

00:15:29.110 --> 00:15:32.850
So what happens if the user
brings the device really

00:15:32.850 --> 00:15:35.430
close to the eyes, your eyes
look through these holes

00:15:35.430 --> 00:15:38.510
and you have a fairly wide field
of view through these holes.

00:15:38.510 --> 00:15:42.020
And then what happens next
is when you put lenses

00:15:42.020 --> 00:15:44.130
into these holes, that
basically concentrates

00:15:44.130 --> 00:15:47.335
your wide field of view onto a
small image area on the screen,

00:15:47.335 --> 00:15:49.400
like for the left eye
and for the right eye.

00:15:49.400 --> 00:15:52.170
And "all," in quotation
marks, that you

00:15:52.170 --> 00:15:54.140
have to do as
developers is render

00:15:54.140 --> 00:15:57.510
images on these small
areas, left and right, that

00:15:57.510 --> 00:16:00.730
make your brain believe
you're in a different place.

00:16:00.730 --> 00:16:04.290
So that has to be very fast,
accurate, as high-resolution

00:16:04.290 --> 00:16:05.160
as possible.

00:16:05.160 --> 00:16:07.510
It has to be synchronized
well with your head motion.

00:16:07.510 --> 00:16:10.070
So let's look a
little bit closer

00:16:10.070 --> 00:16:11.630
into this rendering pipeline.

00:16:11.630 --> 00:16:14.689
So imagine you want to put
the user into a virtual space.

00:16:14.689 --> 00:16:16.980
One of the critical things
you need to know, of course,

00:16:16.980 --> 00:16:19.840
is head where is
the user looking at?

00:16:19.840 --> 00:16:24.429
And what we can do is, the
mobile phones, the recent ones

00:16:24.429 --> 00:16:26.720
of the recent years, they
give you very nice, actually,

00:16:26.720 --> 00:16:27.800
orientation tracking.

00:16:27.800 --> 00:16:30.020
They have [INAUDIBLE].

00:16:30.020 --> 00:16:32.090
And Android provides
you-- and all kind

00:16:32.090 --> 00:16:33.856
of other mobile
operating systems, too,

00:16:33.856 --> 00:16:35.730
or you write your own
libraries-- provide you

00:16:35.730 --> 00:16:37.896
a way of integrating all
of these readings over time

00:16:37.896 --> 00:16:42.070
and give you a relatively
fast stream of 3D orientation

00:16:42.070 --> 00:16:45.630
estimates, like the [INAUDIBLE]
of where the device is headed.

00:16:45.630 --> 00:16:48.860
So if you have those, you
can pick your favorite kind

00:16:48.860 --> 00:16:53.250
of 3D environment, like library,
like OpenGL, for example,

00:16:53.250 --> 00:16:57.060
and you can define the center
of this virtual universe

00:16:57.060 --> 00:16:58.660
as the head of the user.

00:16:58.660 --> 00:17:01.136
And then you basically
put two cameras,

00:17:01.136 --> 00:17:03.260
for the left eye and the
right eye, at eye distance

00:17:03.260 --> 00:17:05.780
on a virtual rig, and
you orient these cameras

00:17:05.780 --> 00:17:08.770
based on these orientation
estimates from the phone.

00:17:08.770 --> 00:17:11.369
And then you do just
the regular rendering

00:17:11.369 --> 00:17:14.025
that you do anyways for
any kind of 3D application.

00:17:14.025 --> 00:17:16.150
You render one for the
left, one for the right eye,

00:17:16.150 --> 00:17:19.660
and you get these
streams of images out.

00:17:19.660 --> 00:17:21.619
And obviously,
you want to adjust

00:17:21.619 --> 00:17:24.920
the field of view of the camera
and kind of the general set-up

00:17:24.920 --> 00:17:28.045
to match the geometry
of your viewer.

00:17:28.045 --> 00:17:30.670
One last thing that you need to
do before you put that actually

00:17:30.670 --> 00:17:35.310
onto the screen is you have
to adjust for lens distortion.

00:17:35.310 --> 00:17:37.759
So because this lens kind of
widens your field of view,

00:17:37.759 --> 00:17:39.300
you get this distortion
effect, which

00:17:39.300 --> 00:17:42.310
is called the
pincushion distortion.

00:17:42.310 --> 00:17:44.420
Which basically means
that outer-image areas

00:17:44.420 --> 00:17:47.769
are enlarged more
than central ones.

00:17:47.769 --> 00:17:49.560
Luckily, what you can
do is you can come up

00:17:49.560 --> 00:17:50.300
with a model for this.

00:17:50.300 --> 00:17:52.550
You can kind of fit a
function either analytically,

00:17:52.550 --> 00:17:54.850
because you know how
this device is set up

00:17:54.850 --> 00:17:57.960
and what the lenses are,
or you just measure that.

00:17:57.960 --> 00:17:59.730
Using a camera and a
calibration pattern,

00:17:59.730 --> 00:18:01.563
you can fit a function
to this, and then you

00:18:01.563 --> 00:18:02.840
can invert this function.

00:18:02.840 --> 00:18:06.940
So this means if you write a
shader that basically takes

00:18:06.940 --> 00:18:10.030
a regularly-rendered image
and distorts with this, what

00:18:10.030 --> 00:18:12.330
is called barrel
distortion, this

00:18:12.330 --> 00:18:14.510
has the effect that if you
put that through a lens,

00:18:14.510 --> 00:18:16.660
the undistorted image
arrives in your eyes.

00:18:16.660 --> 00:18:20.980
And that's how you basically
render this entire thing.

00:18:20.980 --> 00:18:22.690
The next aspect I
want to talk about

00:18:22.690 --> 00:18:24.980
is you have these
images on your eye,

00:18:24.980 --> 00:18:26.681
and they kind of track
your head motion,

00:18:26.681 --> 00:18:27.930
but how do interact with them?

00:18:27.930 --> 00:18:30.760
You kind of hid the
touchscreen in this viewer,

00:18:30.760 --> 00:18:33.220
and you don't necessarily
want to stick your fingers in

00:18:33.220 --> 00:18:35.240
and kind of obstruct your view.

00:18:35.240 --> 00:18:38.060
So we kind of had
this neat little idea

00:18:38.060 --> 00:18:41.259
of putting magnets inside.

00:18:41.259 --> 00:18:42.800
There's a fixed
magnet on the inside,

00:18:42.800 --> 00:18:44.510
and there's this
movable magnetic ring

00:18:44.510 --> 00:18:47.600
on the outside, which
if you move this magnet,

00:18:47.600 --> 00:18:50.090
you can basically detect
the change of magnetic field

00:18:50.090 --> 00:18:51.170
with a magnetometer.

00:18:51.170 --> 00:18:53.120
So kind of what
drives the compass.

00:18:53.120 --> 00:18:54.800
And I want to show
you real quick

00:18:54.800 --> 00:18:56.220
how beautiful these signals are.

00:18:56.220 --> 00:18:58.650
We were kind of surprised
how clean they are.

00:18:58.650 --> 00:19:04.470
So what you see
here is somebody--

00:19:04.470 --> 00:19:07.609
you see three lines, which
is the X, Y, and Z direction,

00:19:07.609 --> 00:19:09.650
like three coordinates of
the magnetometer, which

00:19:09.650 --> 00:19:12.960
kind of tells you how the
magnetic field is oriented.

00:19:12.960 --> 00:19:15.230
And you see when I move
this magnet, what kind

00:19:15.230 --> 00:19:16.442
of beautiful signals we get.

00:19:16.442 --> 00:19:18.900
And so you can write a very
simple classifier that picks up

00:19:18.900 --> 00:19:23.346
the up and down and translates
that into an input event.

00:19:23.346 --> 00:19:28.177
The downside of this, of course,
is you're losing the compass.

00:19:28.177 --> 00:19:30.260
So inside this experience,
we don't know, anymore,

00:19:30.260 --> 00:19:31.540
where kind of north is.

00:19:31.540 --> 00:19:34.040
Because we always think, oh,
north is where the magnet is.

00:19:34.040 --> 00:19:35.814
But we figured
universal environments,

00:19:35.814 --> 00:19:38.230
it's much more important you
have some way of interacting.

00:19:38.230 --> 00:19:41.920
Like flying, not flying,
starting/stopping a video,

00:19:41.920 --> 00:19:44.450
opening up a box, or
whatever you want to do.

00:19:44.450 --> 00:19:45.930
So that's a
trade-off, and you're

00:19:45.930 --> 00:19:48.410
completely free to come up
with other awesome ideas

00:19:48.410 --> 00:19:50.320
to interact.

00:19:50.320 --> 00:19:54.620
The last thing I want to
mention is the NFC tag.

00:19:54.620 --> 00:19:56.650
So we have just a
regular NFC tag--

00:19:56.650 --> 00:19:58.620
we used a fairly
big one so that it

00:19:58.620 --> 00:20:01.910
fits different phones-- where
we put a special URL on it

00:20:01.910 --> 00:20:05.340
that our framework handles, that
basically tells the phone, hey,

00:20:05.340 --> 00:20:07.040
I'm now in the 3D viewer.

00:20:07.040 --> 00:20:10.150
Your app wants to maybe
switch into an immersive mode.

00:20:10.150 --> 00:20:13.282
So the long-term
vision for this is,

00:20:13.282 --> 00:20:14.740
for example, you
have a YouTube app

00:20:14.740 --> 00:20:17.272
and you're watching some kind
of talk or a sports game.

00:20:17.272 --> 00:20:19.230
It's nice, but when you
put it into the viewer,

00:20:19.230 --> 00:20:22.156
you have the exact same sports
game, but you can look around.

00:20:22.156 --> 00:20:24.030
And you can hop from,
like, player to player,

00:20:24.030 --> 00:20:25.197
or all these kind of things.

00:20:25.197 --> 00:20:27.196
So you can think about
all kinds of applications

00:20:27.196 --> 00:20:29.470
that can go seamlessly back
and forth and in and out.

00:20:29.470 --> 00:20:31.344
That's where kind of
also this mobile concept

00:20:31.344 --> 00:20:35.096
is a bit different from these
really head-mounted concepts.

00:20:35.096 --> 00:20:36.720
But with a stapler,
you can, of course,

00:20:36.720 --> 00:20:38.900
transform this
into head-mounted.

00:20:38.900 --> 00:20:41.310
And this is all nice.

00:20:41.310 --> 00:20:43.492
It's not that super
difficult to do

00:20:43.492 --> 00:20:44.950
this kind of
rendering in type end.

00:20:44.950 --> 00:20:46.824
But we wanted to make
it as easy as possible,

00:20:46.824 --> 00:20:49.770
so we packaged, basically,
the hard pieces up

00:20:49.770 --> 00:20:51.370
into one open-source
library, which

00:20:51.370 --> 00:20:53.060
you can find on our website.

00:20:53.060 --> 00:20:56.070
And Boris will walk you
through kind of the APIs

00:20:56.070 --> 00:20:57.830
and how you can start coding.

00:21:01.169 --> 00:21:06.290
[APPLAUSE]

00:21:06.290 --> 00:21:07.682
BORIS SMUS: Thanks, Christian.

00:21:07.682 --> 00:21:09.615
Check, check,
check, check, check.

00:21:09.615 --> 00:21:11.790
All right.

00:21:11.790 --> 00:21:13.840
So as you saw, we
demoed a bunch of things

00:21:13.840 --> 00:21:15.459
at the beginning of the talk.

00:21:15.459 --> 00:21:17.000
Some of them were
written in Android,

00:21:17.000 --> 00:21:20.820
but the coin collector example
was actually written in Chrome

00:21:20.820 --> 00:21:23.660
using Three.js, which
is a lightweight wrapper

00:21:23.660 --> 00:21:25.960
around WebGL to
make developing 3D

00:21:25.960 --> 00:21:28.180
applications for
the web super easy.

00:21:28.180 --> 00:21:31.540
And we used, also,
JavaScript events

00:21:31.540 --> 00:21:33.770
to track the position
of the user's head.

00:21:33.770 --> 00:21:35.634
So it's an easy
way to get started.

00:21:35.634 --> 00:21:37.800
But of course, with Android,
we can get a little bit

00:21:37.800 --> 00:21:39.240
closer to the metal.

00:21:39.240 --> 00:21:42.420
So the toolkit that we
are open-sourcing today

00:21:42.420 --> 00:21:46.600
is an Android-based VR toolkit,
as Christian introduced,

00:21:46.600 --> 00:21:49.530
designed to make developing
VR applications as

00:21:49.530 --> 00:21:51.890
easy as possible.

00:21:51.890 --> 00:21:55.390
So today, if you go
to g.co/cardboard,

00:21:55.390 --> 00:21:59.200
what you get is the
soon-to-be open-sourced--

00:21:59.200 --> 00:22:03.110
we're working on a couple
of tweaks-- VR toolkit,

00:22:03.110 --> 00:22:06.710
so this is just a JAR file, and
a sample that we'll be talking

00:22:06.710 --> 00:22:09.660
about more throughout
this session.

00:22:09.660 --> 00:22:12.570
So what is the sample
we're going to be building?

00:22:12.570 --> 00:22:16.320
It is a treasure
hunt application.

00:22:16.320 --> 00:22:19.430
So the idea here is there's
an object hidden somewhere

00:22:19.430 --> 00:22:21.960
inside your field
of view around you,

00:22:21.960 --> 00:22:24.365
and your goal is to
find it and collect it.

00:22:24.365 --> 00:22:26.740
The way you collect it is, of
course, through the magnet,

00:22:26.740 --> 00:22:30.660
and the more objects you
get, the more points you get.

00:22:30.660 --> 00:22:31.260
Very simple.

00:22:31.260 --> 00:22:34.950
So we'll start with
our favorite IDE.

00:22:34.950 --> 00:22:37.410
Get the JAR from the website.

00:22:37.410 --> 00:22:39.450
Put it in as a
regular dependency.

00:22:39.450 --> 00:22:44.210
And the first thing we do is
create our own custom activity.

00:22:44.210 --> 00:22:47.450
But instead of extending from
the default Android activity,

00:22:47.450 --> 00:22:49.410
we'll extend from the
Cardboard activity,

00:22:49.410 --> 00:22:51.205
which the toolkit provides.

00:22:51.205 --> 00:22:52.820
This does a few things.

00:22:52.820 --> 00:22:55.390
Firstly, the screen
is forced to stay on.

00:22:55.390 --> 00:22:57.740
The screen is placed
in landscape mode,

00:22:57.740 --> 00:23:00.047
and we hide all of
the UI on the top

00:23:00.047 --> 00:23:01.630
and on the bottom
of the phone screen,

00:23:01.630 --> 00:23:06.140
typically the navigation bar
and the notification bar.

00:23:06.140 --> 00:23:09.470
We also disable the volume
keys, because often you

00:23:09.470 --> 00:23:11.860
might accidentally trigger them.

00:23:11.860 --> 00:23:15.000
These are, of course, just
defaults and can be overridden,

00:23:15.000 --> 00:23:17.715
but it's a good starting point.

00:23:17.715 --> 00:23:19.590
still, we have nothing
on the screen for now.

00:23:19.590 --> 00:23:22.130
So what we'll do is
we'll create an instance

00:23:22.130 --> 00:23:24.990
of the CardboardView,
which is a view that

00:23:24.990 --> 00:23:26.940
extends from
GLSurfaceView, which

00:23:26.940 --> 00:23:31.571
is the standard way that you
render 3D content for Android.

00:23:31.571 --> 00:23:34.570
The way we'll do this
is in our activity,

00:23:34.570 --> 00:23:39.910
we override the onCreate method
with a series of simple steps.

00:23:39.910 --> 00:23:43.160
Here we find the view
in our layout hierarchy.

00:23:43.160 --> 00:23:45.190
Next, we tell the
Cardboard activity

00:23:45.190 --> 00:23:47.600
that we should be using
this particular view,

00:23:47.600 --> 00:23:50.710
set the renderer,
and we're good to go.

00:23:50.710 --> 00:23:53.570
So before I go too deep
into the renderer itself,

00:23:53.570 --> 00:23:56.000
let me quickly recap what
Christian just told you

00:23:56.000 --> 00:23:58.230
about the rendering pipeline.

00:23:58.230 --> 00:24:00.270
So there's four basic steps.

00:24:00.270 --> 00:24:02.300
First, we need to
estimate eye positions,

00:24:02.300 --> 00:24:04.030
to figure out where
to place the two

00:24:04.030 --> 00:24:05.820
virtual cameras in our scene.

00:24:05.820 --> 00:24:07.150
Next, we render the scene.

00:24:07.150 --> 00:24:09.825
So we take the world
space and project it

00:24:09.825 --> 00:24:11.660
into eye space for both eyes.

00:24:11.660 --> 00:24:16.040
Then each eye needs to have
its lens distortion corrected.

00:24:16.040 --> 00:24:18.950
And finally, we place
the two displays side

00:24:18.950 --> 00:24:24.330
by side in landscape
stereo mode.

00:24:24.330 --> 00:24:26.510
As far as what the
toolkit provides for you,

00:24:26.510 --> 00:24:28.050
it's actually quite a lot.

00:24:28.050 --> 00:24:31.430
So the first thing,
we have a head tracker

00:24:31.430 --> 00:24:32.910
that we're releasing.

00:24:32.910 --> 00:24:36.250
This also includes an eye model.

00:24:36.250 --> 00:24:39.000
So we have a basic
distance estimation

00:24:39.000 --> 00:24:41.420
between two average
human eyes, so we

00:24:41.420 --> 00:24:43.410
know where to place the cameras.

00:24:43.410 --> 00:24:46.880
Once the scene is rendered,
we correct for lens distortion

00:24:46.880 --> 00:24:50.420
based on the optical properties
of the Cardboard device.

00:24:50.420 --> 00:24:52.500
And finally, we place
the images side by side,

00:24:52.500 --> 00:24:54.320
leaving you only the rendering.

00:24:54.320 --> 00:24:58.650
The second step is the only
thing you need to worry about.

00:24:58.650 --> 00:25:01.200
So to do that, we make
a custom renderer,

00:25:01.200 --> 00:25:03.580
the TreasureHuntRenderer
here, and you

00:25:03.580 --> 00:25:05.290
can see it implements
an interface

00:25:05.290 --> 00:25:07.830
that our framework provides.

00:25:07.830 --> 00:25:10.050
And there's two methods you
need to care about here.

00:25:10.050 --> 00:25:12.310
The first one is onNewFrame.

00:25:12.310 --> 00:25:15.950
This gets called every
time a frame is rendered,

00:25:15.950 --> 00:25:18.790
and it takes in the
HeadTransform, which

00:25:18.790 --> 00:25:21.964
is roughly the heading of
where the user is looking.

00:25:21.964 --> 00:25:23.880
So here we'll save it
for later, because we're

00:25:23.880 --> 00:25:26.338
going to need to know if the
user's looking at the treasure

00:25:26.338 --> 00:25:27.430
or not.

00:25:27.430 --> 00:25:30.960
The other thing we can do in
onNewFrame is update the model.

00:25:30.960 --> 00:25:33.350
So typically there's
an update/draw cycle

00:25:33.350 --> 00:25:35.490
in many graphics applications.

00:25:35.490 --> 00:25:39.486
So we'll just increment
a time count here.

00:25:39.486 --> 00:25:40.610
So that's the first method.

00:25:40.610 --> 00:25:43.900
The next is we need to
implement onDrawEye.

00:25:43.900 --> 00:25:46.520
And this gets called
twice for each frame,

00:25:46.520 --> 00:25:48.570
once for the left eye
and once for the right.

00:25:48.570 --> 00:25:50.670
And, of course, we
pass in the position

00:25:50.670 --> 00:25:54.690
of each eye, or of the
eye order rendering,

00:25:54.690 --> 00:25:56.360
which includes a
bunch of parameters

00:25:56.360 --> 00:25:58.330
that you need to draw a scene.

00:25:58.330 --> 00:26:02.460
So as you expect in
a GL application,

00:26:02.460 --> 00:26:04.260
first we set up our matrices.

00:26:04.260 --> 00:26:06.600
So we go from View
and from Perspective

00:26:06.600 --> 00:26:11.640
to Model-View-Projection matrix,
and then we draw the scene.

00:26:11.640 --> 00:26:13.870
Now it's important that
we do this operation

00:26:13.870 --> 00:26:16.390
as quickly as possible,
because introducing any latency

00:26:16.390 --> 00:26:22.070
into the system can have
undesirable effects, let's say.

00:26:22.070 --> 00:26:23.359
I'll go more into this later.

00:26:23.359 --> 00:26:25.150
So at this point, we've
rendered our scene.

00:26:25.150 --> 00:26:28.500
Here you can see our
treasure, which is a cube.

00:26:28.500 --> 00:26:31.330
And it's, you know, it's there.

00:26:31.330 --> 00:26:35.160
But of course, once we
pass on to the framework,

00:26:35.160 --> 00:26:37.910
we take the scene, transform
it by the head orientation,

00:26:37.910 --> 00:26:40.850
by each eye-- so each eye
has a separate camera--

00:26:40.850 --> 00:26:45.050
and we apply the lens distortion
and place the results side

00:26:45.050 --> 00:26:46.670
by side.

00:26:46.670 --> 00:26:49.240
So we have our
treasure, teasing us,

00:26:49.240 --> 00:26:51.545
but so far, we haven't
figured out how to collect it.

00:26:51.545 --> 00:26:53.450
It's very simple.

00:26:53.450 --> 00:26:58.520
So as Christian mentioned, we
have these magnetometer traces.

00:26:58.520 --> 00:27:02.050
And we've provided
a way to detect

00:27:02.050 --> 00:27:04.940
this pull-and-release
gesture very easily.

00:27:04.940 --> 00:27:07.340
All you need to do
is, in the activity,

00:27:07.340 --> 00:27:10.480
implement onCardboardTrigger.

00:27:10.480 --> 00:27:12.020
And for our application
here, we can

00:27:12.020 --> 00:27:14.500
see the code is straightforward.

00:27:14.500 --> 00:27:17.920
If the user's looking at the
treasure, then we pick it up.

00:27:17.920 --> 00:27:20.405
Otherwise, we tell
them some instructions.

00:27:20.405 --> 00:27:22.630
But lastly, we want
to somehow emphasize--

00:27:22.630 --> 00:27:24.650
or I want to
emphasize-- that we want

00:27:24.650 --> 00:27:29.690
to provide user feedback
here, since we want

00:27:29.690 --> 00:27:32.750
to know if the magnet
pull was actually

00:27:32.750 --> 00:27:34.976
detected by the framework.

00:27:34.976 --> 00:27:36.600
And a good way to
provide this feedback

00:27:36.600 --> 00:27:38.960
is through a
vibration-- but make

00:27:38.960 --> 00:27:42.020
sure it's a short one, because
long vibrations with the phone

00:27:42.020 --> 00:27:44.720
so close to your face can
be a little bit jarring--

00:27:44.720 --> 00:27:50.240
or a quick AlphaBlend,
just a transparency blend.

00:27:52.830 --> 00:27:56.390
OK, the last piece of
input is the NFC tag.

00:27:56.390 --> 00:27:58.380
And as we mentioned
earlier, it's

00:27:58.380 --> 00:28:01.100
an easy way to make these
amphibious applications that

00:28:01.100 --> 00:28:07.100
can adjust to being inserted or
removed from the VR enclosure.

00:28:07.100 --> 00:28:11.350
So in our example, for the
purpose of this illustration,

00:28:11.350 --> 00:28:14.480
we may want to make our
treasure hunt app also

00:28:14.480 --> 00:28:16.990
work when you take
it out of Cardboard.

00:28:16.990 --> 00:28:20.380
In that case, we want to
present not a stereo view,

00:28:20.380 --> 00:28:21.660
but a single view.

00:28:21.660 --> 00:28:25.050
So the toolkit provides
this very easy flag

00:28:25.050 --> 00:28:28.540
that you can toggle,
which is VRModeEnabled.

00:28:28.540 --> 00:28:30.130
And all you do is
you flip that flag

00:28:30.130 --> 00:28:34.150
and go between the
stereo and regular modes.

00:28:34.150 --> 00:28:41.420
Of course, the NFC tag has
something encoded in it.

00:28:41.420 --> 00:28:43.770
So you don't need to use
our framework to detect it.

00:28:43.770 --> 00:28:45.250
You can just use
a regular intent

00:28:45.250 --> 00:28:48.500
filter, as you would in
an Android application,

00:28:48.500 --> 00:28:52.960
to know if the tag has
been scanned or not.

00:28:52.960 --> 00:28:55.920
Now another benefit of
having this tag in the device

00:28:55.920 --> 00:28:59.500
is that if you want to make
hardware modifications-- which

00:28:59.500 --> 00:29:01.970
we hope you do--
then you can encode

00:29:01.970 --> 00:29:04.520
these changes inside the NFC.

00:29:04.520 --> 00:29:06.690
So for example, you
want to increase

00:29:06.690 --> 00:29:09.540
the field of view of the lenses.

00:29:09.540 --> 00:29:12.340
You can change the
parameters of the distortion

00:29:12.340 --> 00:29:13.910
that you need to apply.

00:29:13.910 --> 00:29:16.740
Or you want to increase the
interpupillary distance, which

00:29:16.740 --> 00:29:19.810
is the technical term for the
distance between your eyes.

00:29:19.810 --> 00:29:22.420
You can also encode
that in the NFC tag.

00:29:22.420 --> 00:29:25.040
And this way the toolkit
can provide the best

00:29:25.040 --> 00:29:27.120
possible rendering even
for your modified device.

00:29:29.750 --> 00:29:35.280
OK, so we've talked a bit
about using the framework.

00:29:35.280 --> 00:29:37.170
And we've used this
framework extensively

00:29:37.170 --> 00:29:40.200
over the course of the
last couple months.

00:29:40.200 --> 00:29:44.480
So what we've learned, though,
is that quite obviously,

00:29:44.480 --> 00:29:46.290
building an Android
application is

00:29:46.290 --> 00:29:48.700
very different from
building a VR application.

00:29:48.700 --> 00:29:52.200
So I want to leave you with
three principles which we've

00:29:52.200 --> 00:29:55.050
found from this
development process.

00:29:55.050 --> 00:29:57.920
The first one is very
well-known in the VR community,

00:29:57.920 --> 00:30:02.100
and it is to keep physical
motion and visual motion

00:30:02.100 --> 00:30:03.810
closely coupled.

00:30:03.810 --> 00:30:08.370
The idea is that the brain has
two systems, roughly speaking,

00:30:08.370 --> 00:30:09.660
to detect motion.

00:30:09.660 --> 00:30:12.690
One is the vestibular, which is
sort of your sense of balance,

00:30:12.690 --> 00:30:14.760
and the other is
your visual, which

00:30:14.760 --> 00:30:16.710
is, obviously, your eyesight.

00:30:16.710 --> 00:30:19.740
And if the systems
are not in sync,

00:30:19.740 --> 00:30:23.770
then your illusion of reality
in a VR environment can go away,

00:30:23.770 --> 00:30:25.850
and you can even
get motion sick.

00:30:25.850 --> 00:30:28.020
So we've found that
it helps, for example,

00:30:28.020 --> 00:30:31.070
to relieve this
problem, to place

00:30:31.070 --> 00:30:32.870
the viewer in a fixed position.

00:30:32.870 --> 00:30:35.440
Let them look around.

00:30:35.440 --> 00:30:38.560
Or if you want to
create motion, then

00:30:38.560 --> 00:30:40.770
make them move forward
at a constant velocity

00:30:40.770 --> 00:30:42.817
in the direction of view.

00:30:42.817 --> 00:30:44.650
There are probably other
ways of doing this,

00:30:44.650 --> 00:30:47.050
but these two
things, we've found,

00:30:47.050 --> 00:30:51.540
make it a pretty good experience
that's not very sickening.

00:30:51.540 --> 00:30:53.720
So the other thing is you
need to keep latency down,

00:30:53.720 --> 00:30:56.570
as I mentioned in the
onDrawEye discussion,

00:30:56.570 --> 00:30:58.730
because having
high latency really

00:30:58.730 --> 00:31:01.640
takes away from this illusion.

00:31:01.640 --> 00:31:04.590
The second idea is to
keep the user's head free.

00:31:04.590 --> 00:31:08.630
So the YouTube VR
adaptation that you saw

00:31:08.630 --> 00:31:13.390
showed the user positioned
inside a movie theater,

00:31:13.390 --> 00:31:16.700
as opposed to placing the movies
strictly in front of their eyes

00:31:16.700 --> 00:31:18.630
and sticking to
them when they move.

00:31:18.630 --> 00:31:22.010
So the idea here is
that we want to create

00:31:22.010 --> 00:31:25.950
an immersive environment,
and this, in practice,

00:31:25.950 --> 00:31:28.415
seems to work a lot better
than just putting things

00:31:28.415 --> 00:31:29.040
in screenspace.

00:31:32.030 --> 00:31:35.830
Lastly, really take advantage
of the giant field of view

00:31:35.830 --> 00:31:37.450
that Cardboard gives you.

00:31:37.450 --> 00:31:39.110
And essentially what
we're giving you

00:31:39.110 --> 00:31:43.750
is a 80-degree FOV right off the
bat, which is like being a foot

00:31:43.750 --> 00:31:46.940
or two away from a
50-inch TV screen.

00:31:46.940 --> 00:31:50.490
Not only that, you can look
around in any direction.

00:31:50.490 --> 00:31:53.780
So you have this infinite
virtual screen around you.

00:31:53.780 --> 00:31:59.100
And it really helps to
take advantage of this.

00:31:59.100 --> 00:32:01.790
So I want to emphasize
that it's still

00:32:01.790 --> 00:32:04.550
very early days for
virtual reality.

00:32:04.550 --> 00:32:08.130
And our toolkit is
just the beginning.

00:32:08.130 --> 00:32:09.890
It's just the tip
of the iceberg,

00:32:09.890 --> 00:32:11.650
and there's a lot
more to explore.

00:32:11.650 --> 00:32:14.470
Obviously smartphones have
tons of additional sensors

00:32:14.470 --> 00:32:16.230
that we haven't tapped into.

00:32:16.230 --> 00:32:18.174
There's a microphone present.

00:32:18.174 --> 00:32:19.215
There's an accelerometer.

00:32:19.215 --> 00:32:22.400
I've Seen demos where
you can detect jumps.

00:32:22.400 --> 00:32:25.840
You can, of course,
interface with accessories

00:32:25.840 --> 00:32:29.310
like game pads and
steering wheels, et cetera.

00:32:29.310 --> 00:32:38.430
And also, I'm particularly
excited about combining this 3D

00:32:38.430 --> 00:32:43.320
immersive video experience with
a sound experience, as well.

00:32:43.320 --> 00:32:47.840
So imagine wearing a pair of
stereo headphones, in addition.

00:32:47.840 --> 00:32:52.040
So of course, also, we have
a camera in the device.

00:32:52.040 --> 00:32:53.770
So I want to call
David up to show us

00:32:53.770 --> 00:32:57.380
a demo of something that
uses the camera, as well.

00:33:00.480 --> 00:33:05.890
So David is going to fire up
a demonstration in which he

00:33:05.890 --> 00:33:08.670
can manipulate a
virtual object in front

00:33:08.670 --> 00:33:15.010
of him using the camera
inside of the phone.

00:33:15.010 --> 00:33:19.010
So this is using the
Qualcomm Vieuphoria library,

00:33:19.010 --> 00:33:20.840
which is a pretty
advanced object--

00:33:20.840 --> 00:33:21.675
DAVID COZ: Whoops.

00:33:21.675 --> 00:33:24.720
BORIS SMUS: A pretty advanced
configuration library.

00:33:24.720 --> 00:33:31.190
And we've augmented our demo.

00:33:31.190 --> 00:33:32.600
DAVID COZ: Sorry for that.

00:33:32.600 --> 00:33:33.750
BORIS SMUS: No problem.

00:33:33.750 --> 00:33:35.390
So anyway, the
idea here is David

00:33:35.390 --> 00:33:38.890
will have-- actually, your
Cardboard comes with a QR

00:33:38.890 --> 00:33:44.510
code, which this demo
will be showing you.

00:33:44.510 --> 00:33:49.180
So essentially, when
you look at the QR code,

00:33:49.180 --> 00:33:52.720
you can manipulate this piece
of cardboard in front of you.

00:33:52.720 --> 00:33:56.128
So give us a second here
as we try to get it going.

00:34:08.056 --> 00:34:10.389
So as you can see, it's just
a slightly modified version

00:34:10.389 --> 00:34:10.889
of the app.

00:34:29.130 --> 00:34:32.770
So you can see, as I move
this piece of cardboard,

00:34:32.770 --> 00:34:35.928
the Vieuphoria library is
actually tracking the marker.

00:34:35.928 --> 00:34:38.469
DAVID COZ: And I can still look
around in a very natural way.

00:34:38.469 --> 00:34:40.179
BORIS SMUS: So we
can combine-- by just

00:34:40.179 --> 00:34:42.380
having a simple
marker and a camera,

00:34:42.380 --> 00:34:44.280
we can get this
really cool effect.

00:34:44.280 --> 00:34:46.770
So imagine if you were
to combine our technology

00:34:46.770 --> 00:34:48.270
with a Tango device,
which gives you

00:34:48.270 --> 00:34:53.970
six degrees of tracking
in almost any environment.

00:34:53.970 --> 00:34:54.469
Great.

00:34:54.469 --> 00:34:56.555
Thanks, David.

00:34:56.555 --> 00:34:59.180
So with that, definitely
check out our website,

00:34:59.180 --> 00:35:01.210
where you can download
the toolkit itself.

00:35:01.210 --> 00:35:02.430
It'll be open source soon.

00:35:02.430 --> 00:35:03.566
We have docs.

00:35:03.566 --> 00:35:04.940
We have a tutorial
for you to get

00:35:04.940 --> 00:35:07.270
started that goes
through this sample.

00:35:07.270 --> 00:35:10.310
Also, tons of samples,
including Chrome ones,

00:35:10.310 --> 00:35:14.450
on chromeexperiments.com,
and Android ones, the ones

00:35:14.450 --> 00:35:18.250
that we showed you today, which
is available in the Play Store.

00:35:18.250 --> 00:35:20.350
And if you want to hack
on the physical model,

00:35:20.350 --> 00:35:23.770
then we have all the
plans there for you.

00:35:23.770 --> 00:35:26.430
So we can't wait to see
what you come up with.

00:35:26.430 --> 00:35:28.550
Thank you all for
listening, and we'll

00:35:28.550 --> 00:35:31.074
take about five
minutes of questions.

00:35:31.074 --> 00:35:36.750
[APPLAUSE]

00:35:36.750 --> 00:35:38.500
CHRISTIAN PLAGEMANN:
So for the questions,

00:35:38.500 --> 00:35:40.600
if you could walk up to
the microphones, that'd

00:35:40.600 --> 00:35:43.400
probably be easiest.

00:35:43.400 --> 00:35:44.546
BORIS SMUS: Yes, go ahead.

00:35:44.546 --> 00:35:45.087
AUDIENCE: Hi.

00:35:45.087 --> 00:35:46.245
Oh, this is so cool.

00:35:46.245 --> 00:35:47.070
I really love it.

00:35:47.070 --> 00:35:48.930
I just had a quick
question for you.

00:35:48.930 --> 00:35:50.010
I hope you can hear me.

00:35:50.010 --> 00:35:52.040
The trigger that
you had, you seem

00:35:52.040 --> 00:35:54.630
to have kind of like tied
it to the magnetic thing.

00:35:54.630 --> 00:35:56.879
So I was wondering, is
it possible to do it

00:35:56.879 --> 00:35:58.420
through double-clicks
and things just

00:35:58.420 --> 00:36:00.520
by using the one
waveform you have?

00:36:00.520 --> 00:36:03.300
Or does that need the
framework to actually support

00:36:03.300 --> 00:36:04.249
those methods?

00:36:04.249 --> 00:36:06.290
BORIS SMUS: Right, so
thank you for the question.

00:36:06.290 --> 00:36:08.430
The question was about
supporting double-clicks

00:36:08.430 --> 00:36:10.190
using the magnet clicker.

00:36:10.190 --> 00:36:13.660
And the answer is
we can do a lot.

00:36:13.660 --> 00:36:15.490
The problem is right
now we're using

00:36:15.490 --> 00:36:18.290
a calibrated magnetometer,
which is the thing that's

00:36:18.290 --> 00:36:21.504
used mostly for the compass.

00:36:21.504 --> 00:36:23.420
And the thing with the
calibrated magnetometer

00:36:23.420 --> 00:36:26.340
is it calibrates every so
often, which is this event,

00:36:26.340 --> 00:36:29.540
and it drastically
messes up the signals.

00:36:29.540 --> 00:36:32.950
So a lot of modern phones--
I think the Nexus 4 and 5--

00:36:32.950 --> 00:36:36.480
have an uncalibrated
magnetometer, which you can use

00:36:36.480 --> 00:36:38.800
and it does not have
this calibration event.

00:36:38.800 --> 00:36:43.020
So with that out of the
equation, we can do a lot more.

00:36:43.020 --> 00:36:46.015
We've even thought about,
like, having a full joypad

00:36:46.015 --> 00:36:48.250
on the side of the
cardboard device,

00:36:48.250 --> 00:36:51.180
with the magnet able to
move in any direction.

00:36:51.180 --> 00:36:52.980
So this is something
we can do once we've

00:36:52.980 --> 00:36:55.450
switched to-- once
enough phones, I guess,

00:36:55.450 --> 00:36:59.178
have uncalibrated magnetometers.

00:36:59.178 --> 00:37:02.280
AUDIENCE: Can you
elaborate-- is there

00:37:02.280 --> 00:37:07.930
any way for users to readily
generate pictorial content?

00:37:07.930 --> 00:37:10.810
For instance, if you're
taking a vacation

00:37:10.810 --> 00:37:13.607
and you want to
capture, what do you do?

00:37:13.607 --> 00:37:14.690
CHRISTIAN PLAGEMANN: Yeah.

00:37:14.690 --> 00:37:16.821
So one thing we haven't
demoed right here,

00:37:16.821 --> 00:37:19.320
but you should come up to the
Sandbox and really try it out,

00:37:19.320 --> 00:37:21.330
is the Photo Sphere viewer.

00:37:21.330 --> 00:37:24.260
So there's actually an app
that someone in our team

00:37:24.260 --> 00:37:27.020
wrote that can actually show you
photo spheres in this viewer.

00:37:27.020 --> 00:37:29.760
So you can actually take
those with your device,

00:37:29.760 --> 00:37:32.160
and it actually takes the
local files from your device,

00:37:32.160 --> 00:37:33.660
and that works well.

00:37:33.660 --> 00:37:36.180
And of course, you could
come up with a picture view

00:37:36.180 --> 00:37:38.882
just for regular pictures
that-- just very large.

00:37:38.882 --> 00:37:41.340
And there's actually, the app
has an intent filter already,

00:37:41.340 --> 00:37:44.460
so you can just click on
any photo sphere in the web,

00:37:44.460 --> 00:37:47.050
for example, and it would
launch that particular viewer,

00:37:47.050 --> 00:37:48.284
or give you the option.

00:37:48.284 --> 00:37:48.950
BORIS SMUS: Yes?

00:37:48.950 --> 00:37:50.230
Go ahead.

00:37:50.230 --> 00:37:52.460
AUDIENCE: Yeah, two
questions real quick.

00:37:52.460 --> 00:37:55.030
One is I know there's a lot
of 3D content on YouTube.

00:37:55.030 --> 00:37:57.020
Might this just
become a good way

00:37:57.020 --> 00:37:59.310
to view that
stereoscopic content?

00:37:59.310 --> 00:38:01.851
Right now, it looked
like it was 2D content

00:38:01.851 --> 00:38:02.850
in the examples you had.

00:38:02.850 --> 00:38:03.903
Is that right?

00:38:03.903 --> 00:38:05.170
DAVID COZ: Yep.

00:38:05.170 --> 00:38:07.630
Yeah, so those were 2D
videos, just regular videos.

00:38:07.630 --> 00:38:09.830
But it's true that
you have a lot

00:38:09.830 --> 00:38:11.570
of side-by-side
videos on YouTube.

00:38:11.570 --> 00:38:15.134
That's actually how
we kind of proved

00:38:15.134 --> 00:38:17.300
that this concept was
working, was putting a YouTube

00:38:17.300 --> 00:38:20.780
side-by-side video and
have the depth effect.

00:38:20.780 --> 00:38:24.690
So I yeah, the YouTube
team might want to do this

00:38:24.690 --> 00:38:27.440
in the future, but we cannot
really comment on this,

00:38:27.440 --> 00:38:28.155
I guess, now.

00:38:28.155 --> 00:38:29.049
It's a bit too early.

00:38:29.049 --> 00:38:29.590
AUDIENCE: OK.

00:38:29.590 --> 00:38:30.350
CHRISTIAN PLAGEMANN:
It's a natural thing.

00:38:30.350 --> 00:38:33.020
Like it's very clearly
possible, and there are actually

00:38:33.020 --> 00:38:34.940
apps on the store that do this.

00:38:34.940 --> 00:38:36.666
AUDIENCE: And the
other question is

00:38:36.666 --> 00:38:38.290
it seems like a
compass would be really

00:38:38.290 --> 00:38:40.039
great to have in these
kinds of situation,

00:38:40.039 --> 00:38:41.950
especially for
navigation-type tools.

00:38:41.950 --> 00:38:44.270
Might we see a future
version of this

00:38:44.270 --> 00:38:47.524
that just uses voice commands to
navigate instead of the magnet

00:38:47.524 --> 00:38:49.574
to click?

00:38:49.574 --> 00:38:51.990
BORIS SMUS: So the question
was, can we replace the magnet

00:38:51.990 --> 00:38:54.970
with voice commands in order
to free up the compass?

00:38:54.970 --> 00:38:55.880
AUDIENCE: Right.

00:38:55.880 --> 00:38:58.800
BORIS SMUS: I think--
so voice commands can

00:38:58.800 --> 00:39:00.987
be a bit unwieldy
for a quick action.

00:39:00.987 --> 00:39:02.820
I mean, it's certainly
something to explore.

00:39:02.820 --> 00:39:05.240
I mean, there's certainly
a microphone in the device.

00:39:05.240 --> 00:39:07.357
So you can do so much more.

00:39:07.357 --> 00:39:09.190
Like one of the things
that we thought about

00:39:09.190 --> 00:39:13.710
was combining a tap on the
side, the accelerometer impulse,

00:39:13.710 --> 00:39:16.750
with the microphone
signature of a tap.

00:39:16.750 --> 00:39:20.350
So you can imagine doing all
sorts of different input modes

00:39:20.350 --> 00:39:24.260
that would free up the magnet,
or free up the magnetometer,

00:39:24.260 --> 00:39:25.253
for the compass.

00:39:25.253 --> 00:39:26.211
AUDIENCE: Cool, thanks.

00:39:26.211 --> 00:39:28.010
BORIS SMUS: Yeah,
there are many ways.

00:39:28.010 --> 00:39:30.280
One particular reason why
we didn't look at speech

00:39:30.280 --> 00:39:32.650
was we give it out to
6,000 I/O attendees,

00:39:32.650 --> 00:39:34.520
and everyone tries
to speech-control it,

00:39:34.520 --> 00:39:36.750
then it gets a bit loud.

00:39:36.750 --> 00:39:39.695
AUDIENCE: Could you manipulate
objects with your hands?

00:39:39.695 --> 00:39:40.957
Have you tried that?

00:39:40.957 --> 00:39:42.665
CHRISTIAN PLAGEMANN:
Yeah, of definitely.

00:39:42.665 --> 00:39:43.180
AUDIENCE: I know you
did the QR Code, but--

00:39:43.180 --> 00:39:44.540
CHRISTIAN PLAGEMANN:
I mean, take

00:39:44.540 --> 00:39:46.740
a gesture-recognition library,
like you can recognize

00:39:46.740 --> 00:39:48.656
the finger, you could
do these kind of things.

00:39:48.656 --> 00:39:51.300
AUDIENCE: But it is
possible with the kit

00:39:51.300 --> 00:39:52.290
that you put out today?

00:39:52.290 --> 00:39:54.498
CHRISTIAN PLAGEMANN: No,
it's not in the kit already,

00:39:54.498 --> 00:39:56.819
but there are many
ways to do this.

00:39:56.819 --> 00:39:58.860
It's again-- I mean, it's
computer vision, right?

00:39:58.860 --> 00:40:01.400
So you need to detect hands,
need to detect gestures,

00:40:01.400 --> 00:40:03.280
and it's a regular
computer vision problem.

00:40:03.280 --> 00:40:05.050
But actually, the cameras
in these are really good.

00:40:05.050 --> 00:40:07.216
BORIS SMUS: There's also
some interesting prototypes

00:40:07.216 --> 00:40:10.660
with the Oculus, for example,
and a Leap attached to it,

00:40:10.660 --> 00:40:13.120
where you get a full
hand-tracker in front of you.

00:40:13.120 --> 00:40:16.120
So you could look
into that, as well.

00:40:16.120 --> 00:40:21.420
AUDIENCE: So there's no
[INAUDIBLE] reference.

00:40:21.420 --> 00:40:22.750
How do you guys compensate?

00:40:22.750 --> 00:40:25.760
Do you guys have any
compensation for the drift?

00:40:25.760 --> 00:40:29.240
Because over time, like when
you're rotating sometimes,

00:40:29.240 --> 00:40:32.710
and when you come back, it's
not in the same location.

00:40:32.710 --> 00:40:34.940
Does the experience--

00:40:34.940 --> 00:40:37.720
DAVID COZ: So the question
is about heading reference?

00:40:37.720 --> 00:40:38.840
Like, yeah--

00:40:38.840 --> 00:40:41.670
AUDIENCE: Yeah, because over
time, your [INAUDIBLE] has--

00:40:41.670 --> 00:40:43.670
DAVID COZ: Yeah, so it's
true that right now, we

00:40:43.670 --> 00:40:48.850
don't have any reference,
because the compass is

00:40:48.850 --> 00:40:51.820
kind of modified by the magnet.

00:40:51.820 --> 00:40:53.580
So you can have this problem.

00:40:53.580 --> 00:40:55.500
It depends a lot on
the phones, actually,

00:40:55.500 --> 00:40:59.660
on the kind of calibration
that you have in your sensors.

00:40:59.660 --> 00:41:03.480
It seems that on, for example,
recent phones, like Nexus 5K,

00:41:03.480 --> 00:41:04.840
it doesn't drift so much.

00:41:04.840 --> 00:41:06.520
But on all the phones
we tried, there

00:41:06.520 --> 00:41:10.350
was kind of a significant drift.

00:41:10.350 --> 00:41:13.266
So it's something that we
want to work on in the future.

00:41:13.266 --> 00:41:14.640
BORIS SMUS: Are
you guys planning

00:41:14.640 --> 00:41:17.060
on working also
position tracking

00:41:17.060 --> 00:41:19.850
using just a 2D camera?

00:41:19.850 --> 00:41:22.860
That would be a great
Tango integration, right?

00:41:22.860 --> 00:41:28.010
So Tango does 6D tracking,
just with the reference

00:41:28.010 --> 00:41:29.425
of the scene around you.

00:41:29.425 --> 00:41:32.251
DAVID COZ: We actually built
a Tango-compatible Cardboard

00:41:32.251 --> 00:41:32.750
device.

00:41:32.750 --> 00:41:33.640
It's very easy.

00:41:33.640 --> 00:41:36.592
Like you just need to increase
the width of the Cardboard.

00:41:36.592 --> 00:41:38.300
CHRISTIAN PLAGEMANN:
And actually, the 2D

00:41:38.300 --> 00:41:42.080
camera itself can provide
a pretty good, actually,

00:41:42.080 --> 00:41:43.662
drift compensation.

00:41:43.662 --> 00:41:46.120
So you can actually-- I mean,
you track features over time.

00:41:46.120 --> 00:41:48.930
And then usually, these
drifts are very, very slow,

00:41:48.930 --> 00:41:50.520
so usually they
accumulate over time.

00:41:50.520 --> 00:41:52.180
Like these are
usually not big drifts

00:41:52.180 --> 00:41:53.050
that are kind of showing--

00:41:53.050 --> 00:41:54.799
AUDIENCE: Yeah, it's
just, like, depending

00:41:54.799 --> 00:41:57.400
how fast the camera can
detect the features, then?

00:41:57.400 --> 00:41:58.840
CHRISTIAN PLAGEMANN: Uh, sure.

00:41:58.840 --> 00:42:00.734
Yes.

00:42:00.734 --> 00:42:02.020
AUDIENCE: Thank you.

00:42:02.020 --> 00:42:04.103
AUDIENCE: Windy Day is
fantastic on the Cardboard.

00:42:04.103 --> 00:42:06.190
I was wondering if
you expect to get

00:42:06.190 --> 00:42:08.960
the rights to the other
Spotlight Stories?

00:42:08.960 --> 00:42:13.659
CHRISTIAN PLAGEMANN: Oh, we were
absolutely, absolutely amazed

00:42:13.659 --> 00:42:15.950
by what the-- like we talked
directly to the Spotlights

00:42:15.950 --> 00:42:17.680
team, and it was actually
the Spotlights team, like

00:42:17.680 --> 00:42:20.000
the [INAUDIBLE] team in
the previous presentation,

00:42:20.000 --> 00:42:22.760
that integrated all this
tech into the Cardboard,

00:42:22.760 --> 00:42:23.737
and it works so well.

00:42:23.737 --> 00:42:26.320
And I'm pretty sure they would
be more than happy to integrate

00:42:26.320 --> 00:42:27.030
their others, as well.

00:42:27.030 --> 00:42:29.071
DAVID COZ: The limitation
here was just the size,

00:42:29.071 --> 00:42:31.800
because we embed all the
assets in the application.

00:42:31.800 --> 00:42:34.465
So it was just a
question of size.

00:42:34.465 --> 00:42:35.435
AUDIENCE: Thank you.

00:42:35.435 --> 00:42:36.310
BORIS SMUS: Go ahead.

00:42:36.310 --> 00:42:38.934
AUDIENCE: One of the things that
came to mind as an alternative

00:42:38.934 --> 00:42:41.290
to your magnetic button--
which I think is very clever,

00:42:41.290 --> 00:42:42.831
but I'd really like
the magnetometer.

00:42:42.831 --> 00:42:45.200
One of the things
I saw at AWE was,

00:42:45.200 --> 00:42:49.980
what you can buy online for $20,
is a little pocket Bluetooth

00:42:49.980 --> 00:42:51.007
game controller.

00:42:51.007 --> 00:42:52.590
So I think that would
be a good thing.

00:42:52.590 --> 00:42:55.430
And then I'd like to know when
you think you'll have here

00:42:55.430 --> 00:42:57.130
your Cardboard Eyetracker ready.

00:42:57.130 --> 00:42:58.630
Because I think
that's a good thing.

00:42:58.630 --> 00:42:59.250
[LAUGHTER]

00:42:59.250 --> 00:43:01.230
CHRISTIAN PLAGEMANN:
Yeah, that'd be nice, too.

00:43:01.230 --> 00:43:01.730
It's tricky.

00:43:01.730 --> 00:43:02.530
We thought about it.

00:43:02.530 --> 00:43:04.071
Like using the
inward-facing cameras.

00:43:04.071 --> 00:43:06.635
Like one of the major
design problems with this

00:43:06.635 --> 00:43:08.640
is there's so many
different phones out there.

00:43:08.640 --> 00:43:11.240
Like to basically, just to
find a hole that kind of fits

00:43:11.240 --> 00:43:15.450
all the outward-facing
cameras, and to kind of find

00:43:15.450 --> 00:43:17.450
the right form factor
that fits most,

00:43:17.450 --> 00:43:18.710
that was already challenging.

00:43:18.710 --> 00:43:20.793
And now if we want to come
up with the optics that

00:43:20.793 --> 00:43:23.780
project the inward-facing
into your eyes,

00:43:23.780 --> 00:43:25.704
it's a bit more work.

00:43:25.704 --> 00:43:26.620
Oh, but totally agree.

00:43:26.620 --> 00:43:29.690
Would be an amazing input.

00:43:29.690 --> 00:43:30.620
BORIS SMUS: OK.

00:43:30.620 --> 00:43:31.240
That's it.

00:43:31.240 --> 00:43:32.340
Thank you very much, guys.

00:43:32.340 --> 00:43:33.631
CHRISTIAN PLAGEMANN: Thank you.

00:43:33.631 --> 00:43:34.790
[APPLAUSE]

