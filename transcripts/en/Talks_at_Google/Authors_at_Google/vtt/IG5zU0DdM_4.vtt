WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.415
[MUSIC PLAYING]

00:00:06.114 --> 00:00:07.030
SPEAKER: Hi, everyone.

00:00:07.030 --> 00:00:10.740
Welcome to Talks at Google
from Cambridge, Massachusetts.

00:00:10.740 --> 00:00:13.680
And today, we're
very happy to host

00:00:13.680 --> 00:00:17.970
Sara Wachter-Boettcher and her
new book, "Technically Wrong."

00:00:17.970 --> 00:00:21.510
Sara's been a web designer
and UX consultant.

00:00:21.510 --> 00:00:24.810
And so she's in the industry,
but not necessarily,

00:00:24.810 --> 00:00:25.900
completely, of it.

00:00:25.900 --> 00:00:29.100
And I think this gives her
a great fresh perspective

00:00:29.100 --> 00:00:34.470
on some of the issues that face
our industry and society today.

00:00:34.470 --> 00:00:37.110
These are increasingly
familiar themes,

00:00:37.110 --> 00:00:38.970
but I think that's
exactly as it should be,

00:00:38.970 --> 00:00:42.100
given how critically important
they are to our industry

00:00:42.100 --> 00:00:44.610
and, indeed, to the
world as a whole now.

00:00:44.610 --> 00:00:45.390
So, welcome, Sara.

00:00:51.360 --> 00:00:52.610
SARA WACHTER-BOETTCHER: Hello.

00:00:52.610 --> 00:00:53.734
Thank you, all, for coming.

00:00:56.220 --> 00:00:59.480
So I'd like to talk today,
first off, about something

00:00:59.480 --> 00:01:01.530
that is not in the book at all.

00:01:01.530 --> 00:01:05.200
And it's not in the book
because it happened last week.

00:01:05.200 --> 00:01:06.950
And some of you might
be familiar with it.

00:01:10.770 --> 00:01:13.580
This is out friends,
the mini cupcakes.

00:01:13.580 --> 00:01:15.560
And I'd like to talk
about this, both

00:01:15.560 --> 00:01:17.000
because I'm here at Google,
and because they think

00:01:17.000 --> 00:01:19.130
this is a really good
example of some of the things

00:01:19.130 --> 00:01:22.790
that I think are not
being talked about enough.

00:01:22.790 --> 00:01:26.660
So mini cupcakes, as
I'm sure you all know,

00:01:26.660 --> 00:01:30.770
were the topic of discussion
because of a Google Maps update

00:01:30.770 --> 00:01:34.520
that went out to a
selection of iPhone users

00:01:34.520 --> 00:01:40.190
that started showing
the number of calories

00:01:40.190 --> 00:01:43.010
that maps thought that you might
burn if you walked somewhere

00:01:43.010 --> 00:01:44.384
instead of taking
some other form

00:01:44.384 --> 00:01:49.250
of transit and also how
many mini cupcakes that

00:01:49.250 --> 00:01:53.560
might be in terms of
your calories burned.

00:01:53.560 --> 00:01:57.110
Now, almost immediately,
after this launched,

00:01:57.110 --> 00:01:58.907
some people started
talking about it.

00:01:58.907 --> 00:02:00.740
One of them was a woman
named Taylor Lorenz,

00:02:00.740 --> 00:02:01.573
who is a journalist.

00:02:01.573 --> 00:02:05.079
And she did not
like this update.

00:02:05.079 --> 00:02:06.120
So she had some comments.

00:02:06.120 --> 00:02:07.670
So one of the first
things she said

00:02:07.670 --> 00:02:12.320
was, oh, my god, if you
click on walking directions,

00:02:12.320 --> 00:02:14.660
it's going to tell you
how much food this burns.

00:02:14.660 --> 00:02:18.600
She goes on to
talk about there's

00:02:18.600 --> 00:02:20.622
no way to turn this off.

00:02:20.622 --> 00:02:22.080
Then she says, "Do
they not realize

00:02:22.080 --> 00:02:24.850
how Triggering this is for
people with eating disorders?"

00:02:24.850 --> 00:02:28.500
And this just generally
feel shamey and bad.

00:02:28.500 --> 00:02:30.900
She goes on to talk about
how calorie counting maybe

00:02:30.900 --> 00:02:32.400
isn't even a good
thing, and there's

00:02:32.400 --> 00:02:34.320
a lot of disagreement on that.

00:02:34.320 --> 00:02:39.270
Then she talks about how this is
even perpetuating diet culture.

00:02:39.270 --> 00:02:42.940
So on and on she goes
up until about 9:00 PM.

00:02:42.940 --> 00:02:44.910
This is about an hour
of tweets that she

00:02:44.910 --> 00:02:47.760
has sent walking through
all of the reasons

00:02:47.760 --> 00:02:49.980
that she thinks
this is a problem.

00:02:49.980 --> 00:02:51.736
She said there's no
way to turn it off.

00:02:51.736 --> 00:02:54.110
This could be dangerous for
people with eating disorders,

00:02:54.110 --> 00:02:56.280
this feels shamey,
average calorie counts

00:02:56.280 --> 00:02:59.460
are wildly inaccurate, not
all calories are equal,

00:02:59.460 --> 00:03:01.920
a cupcake is not a
useful metric, what

00:03:01.920 --> 00:03:04.410
is a mini cupcake anyway,
whose mini cupcake

00:03:04.410 --> 00:03:07.830
are we talking about,
pink cupcakes are not

00:03:07.830 --> 00:03:10.200
a neutral choice, they have--

00:03:10.200 --> 00:03:11.880
not her exact wording,
but, they have

00:03:11.880 --> 00:03:13.680
social and cultural
encoding to them

00:03:13.680 --> 00:03:16.350
that they would be perceived
as being more feminine, more

00:03:16.350 --> 00:03:18.030
white, more middle class--

00:03:18.030 --> 00:03:20.446
and that this
perpetuates diet culture.

00:03:20.446 --> 00:03:24.570
That is sort of a summary of her
hour of evaluating this product

00:03:24.570 --> 00:03:26.350
choice.

00:03:26.350 --> 00:03:27.960
And I look at
something like that,

00:03:27.960 --> 00:03:34.296
and I think, OK, it took her an
hour to document these flaws.

00:03:34.296 --> 00:03:36.670
It took three hours after she
started tweeting about it--

00:03:36.670 --> 00:03:39.210
and I'm sure not only because
she was tweeting about it--

00:03:39.210 --> 00:03:42.440
for the feature
to get shut down.

00:03:42.440 --> 00:03:46.410
And so I ask how much time
was invested in building

00:03:46.410 --> 00:03:48.580
that in the first place?

00:03:48.580 --> 00:03:52.930
I suspect more than an
hour, more than one person.

00:03:52.930 --> 00:03:56.440
And why, nowhere along the
line, did this come up?

00:03:56.440 --> 00:03:59.740
Or if it came up, why
wasn't it addressed?

00:03:59.740 --> 00:04:02.380
Why didn't people
take it seriously?

00:04:02.380 --> 00:04:04.750
Whoever brought it up,
if they brought it up,

00:04:04.750 --> 00:04:08.620
why was their voice not
valued in that discussion?

00:04:08.620 --> 00:04:11.110
Why did people not think
that this mattered?

00:04:11.110 --> 00:04:12.940
And this is just one
tiny thing, right?

00:04:12.940 --> 00:04:15.280
It's a freaking cupcake.

00:04:15.280 --> 00:04:17.261
Except that it's
not this tiny thing.

00:04:17.261 --> 00:04:18.760
What I think it is,
is it's actually

00:04:18.760 --> 00:04:20.920
a perfect encapsulation
of some of what

00:04:20.920 --> 00:04:24.070
I see going wrong in all kinds
of tech companies, certainly

00:04:24.070 --> 00:04:25.450
at Google and elsewhere.

00:04:25.450 --> 00:04:30.530
All over, in almost every
single aspect of our lives,

00:04:30.530 --> 00:04:33.970
we have tech companies
that really end up

00:04:33.970 --> 00:04:36.130
thinking that they
understand people

00:04:36.130 --> 00:04:39.370
and making choices that make a
lot of assumptions about people

00:04:39.370 --> 00:04:43.120
that leave them too narrowly
focused on whatever they

00:04:43.120 --> 00:04:46.810
thought their goal was, and
leaving so many people out

00:04:46.810 --> 00:04:48.210
along the way.

00:04:48.210 --> 00:04:52.600
And you end up with this kind
of tech-knows-best paternalism

00:04:52.600 --> 00:04:55.180
where a tech
company is presuming

00:04:55.180 --> 00:04:57.100
to know what people want
and what people need.

00:04:57.100 --> 00:04:59.350
And that when you're mapping
something, what you really need

00:04:59.350 --> 00:05:01.340
is calorie counts and
to know how much food

00:05:01.340 --> 00:05:03.430
that is because,
after all, let's

00:05:03.430 --> 00:05:06.890
talk about obesity in
America or whatever.

00:05:06.890 --> 00:05:09.250
And so, what you
end up having is

00:05:09.250 --> 00:05:14.080
this really narrow understanding
of what normal people are.

00:05:14.080 --> 00:05:16.810
And I think that that goes
very deep in tech companies

00:05:16.810 --> 00:05:20.560
and the people who work there--
this idea that we understand

00:05:20.560 --> 00:05:23.810
what normal means,
what users want--

00:05:23.810 --> 00:05:26.739
you can see it in so
many different examples.

00:05:26.739 --> 00:05:29.280
And when you work on something
like this, all of your friends

00:05:29.280 --> 00:05:30.170
send you screenshots.

00:05:30.170 --> 00:05:32.950
So I got a lot of really, really
fun screenshots from people.

00:05:32.950 --> 00:05:35.440
One of them came from
my friend Dan [? Han. ?]

00:05:35.440 --> 00:05:37.960
This is an email that
he got from his scale,

00:05:37.960 --> 00:05:41.960
which is a thing that you
get from your scale now.

00:05:41.960 --> 00:05:45.040
And this email is saying
don't be discouraged

00:05:45.040 --> 00:05:46.500
by last week's results.

00:05:46.500 --> 00:05:48.100
We believe in you.

00:05:48.100 --> 00:05:49.930
Let's set a weight
goal to help inspire

00:05:49.930 --> 00:05:52.454
you to shed those extra pounds.

00:05:52.454 --> 00:05:54.870
But you'll notice this is not
addressed to Dan, my friend,

00:05:54.870 --> 00:05:56.350
this is addressed to Calvin.

00:05:56.350 --> 00:05:57.970
Calvin is his son,
who is a toddler.

00:06:01.600 --> 00:06:06.350
And every single week,
Calvin weighed more, right?

00:06:06.350 --> 00:06:08.540
Weird.

00:06:08.540 --> 00:06:11.000
And the scale didn't
get that, like, just

00:06:11.000 --> 00:06:13.379
didn't understand that that
actually could be a perfectly

00:06:13.379 --> 00:06:14.420
normal and natural thing.

00:06:14.420 --> 00:06:18.540
Because the only thing that the
product had been designed to do

00:06:18.540 --> 00:06:20.900
was to congratulate weight loss.

00:06:20.900 --> 00:06:22.670
And it's really
funny when you get

00:06:22.670 --> 00:06:25.910
this message for your toddler
because your toddler isn't

00:06:25.910 --> 00:06:28.400
internalizing this
particular email.

00:06:28.400 --> 00:06:30.590
But it gets
progressively less funny

00:06:30.590 --> 00:06:32.210
for a lot of other people.

00:06:32.210 --> 00:06:35.000
In fact, this was another
message on a push notification

00:06:35.000 --> 00:06:36.140
that he got.

00:06:36.140 --> 00:06:36.920
"Congratulations!

00:06:36.920 --> 00:06:38.630
You've hit a new low weight."

00:06:38.630 --> 00:06:42.230
This one actually
was for Dan's wife.

00:06:42.230 --> 00:06:46.760
And she received this right
after she had a baby, which,

00:06:46.760 --> 00:06:49.460
I mean, I guess it's true.

00:06:49.460 --> 00:06:52.370
But that wasn't something she
wanted to be congratulated on.

00:06:52.370 --> 00:06:54.350
And in fact, I know
a lot of people

00:06:54.350 --> 00:06:58.195
for whom a message like
this is not good at all--

00:06:58.195 --> 00:07:00.320
people who suffer from
eating disorders, for sure--

00:07:00.320 --> 00:07:04.160
I have a dear friend who
spent a long time in treatment

00:07:04.160 --> 00:07:05.700
for eating disorders.

00:07:05.700 --> 00:07:07.610
And this kind of
message, this is

00:07:07.610 --> 00:07:09.590
exactly what she does not need.

00:07:09.590 --> 00:07:11.372
She does not need
to be congratulated.

00:07:11.372 --> 00:07:12.830
But it's also a
lot of other people

00:07:12.830 --> 00:07:15.800
who have chronic
illnesses and where

00:07:15.800 --> 00:07:20.500
their chronic illness might mean
that hitting a new low weight--

00:07:20.500 --> 00:07:22.200
it means they're sick.

00:07:22.200 --> 00:07:25.884
It's a sign that something is
going really wrong with them.

00:07:25.884 --> 00:07:27.300
And there are just
so many reasons

00:07:27.300 --> 00:07:28.800
that notifications
like this don't

00:07:28.800 --> 00:07:30.510
work-- that product
decisions like this

00:07:30.510 --> 00:07:34.440
don't work for real people.

00:07:34.440 --> 00:07:37.190
But yet, so many
examples of people

00:07:37.190 --> 00:07:39.290
making product choices
that are like this,

00:07:39.290 --> 00:07:41.930
that are exclusionary
and that are alienating--

00:07:41.930 --> 00:07:45.200
for example, this is a message
that a friend of mine, Erin

00:07:45.200 --> 00:07:46.697
got from Etsy.

00:07:46.697 --> 00:07:48.530
She has the Etsy app
installed on her phone.

00:07:48.530 --> 00:07:49.880
So she got this
push notification

00:07:49.880 --> 00:07:51.860
because, obviously, Etsy wants
to move some Valentine's Day

00:07:51.860 --> 00:07:52.480
merchandise.

00:07:52.480 --> 00:07:54.350
It says, move over, Cupid.

00:07:54.350 --> 00:07:55.770
We've got what he wants.

00:07:55.770 --> 00:07:58.880
Shop Valentine's
Day gifts for him.

00:07:58.880 --> 00:08:01.376
And Erin's partner is a woman.

00:08:01.376 --> 00:08:02.917
And she looks at
that-- and it's just

00:08:02.917 --> 00:08:04.109
a stupid throwaway message.

00:08:04.109 --> 00:08:05.650
It's just a little
marketing message.

00:08:05.650 --> 00:08:07.170
It's just a little bit of copy.

00:08:07.170 --> 00:08:08.753
Except that she looks
at that, and she

00:08:08.753 --> 00:08:10.380
says this is really alienating.

00:08:10.380 --> 00:08:13.680
Did you not think that
you have gay customers?

00:08:13.680 --> 00:08:16.350
How did this not
cross your mind?

00:08:16.350 --> 00:08:17.849
And if this doesn't
cross your mind

00:08:17.849 --> 00:08:19.390
on this little tiny
thing, where else

00:08:19.390 --> 00:08:21.480
has that not crossed your mind?

00:08:21.480 --> 00:08:24.297
And, is this a company I
really want to spend money at?

00:08:27.640 --> 00:08:29.730
We also can see these
kinds of failures

00:08:29.730 --> 00:08:32.820
to understand real
people in places

00:08:32.820 --> 00:08:35.679
that are more worrisome
and more problematic.

00:08:35.679 --> 00:08:39.210
So last year, JAMA
Internal Medicine

00:08:39.210 --> 00:08:42.450
released a study that showed
that the smartphone assistance

00:08:42.450 --> 00:08:44.940
from a bunch of
different manufacturers--

00:08:44.940 --> 00:08:46.890
not just Apple-- but
we have Siri here,

00:08:46.890 --> 00:08:49.360
as well as from Google, from
Samsung, from Microsoft--

00:08:49.360 --> 00:08:52.170
that they weren't programmed
to help during crisis.

00:08:52.170 --> 00:08:54.750
They didn't understand a lot
of inquiries related to crisis,

00:08:54.750 --> 00:08:57.190
including a lot of things
related to things like rape,

00:08:57.190 --> 00:08:59.880
sexual assault, or
domestic violence like,

00:08:59.880 --> 00:09:00.990
my husband is hitting me.

00:09:04.614 --> 00:09:06.780
So I went in and I started
saying well, let me go in

00:09:06.780 --> 00:09:08.760
and see what I get.

00:09:08.760 --> 00:09:10.680
So I went in, and I
asked some questions,

00:09:10.680 --> 00:09:12.990
and I took some
screenshots of what I got.

00:09:12.990 --> 00:09:15.570
And what was really alarming
about it wasn't just

00:09:15.570 --> 00:09:17.210
that Siri didn't understand.

00:09:17.210 --> 00:09:19.710
Because I figure Siri is not
going to understand everything.

00:09:19.710 --> 00:09:21.751
Although, I think that it
could do better there--

00:09:21.751 --> 00:09:25.667
but also, Siri was responding
with jokes or little digs.

00:09:25.667 --> 00:09:27.750
Like, thinking it's going
to be, clever and funny,

00:09:27.750 --> 00:09:29.580
and "it's not a problem."

00:09:29.580 --> 00:09:33.240
"One can't know
everything, can one?"

00:09:33.240 --> 00:09:37.710
And the real thing that
made me upset about this

00:09:37.710 --> 00:09:40.290
was that this
wasn't exactly new.

00:09:40.290 --> 00:09:43.810
Because back in 2011,
when Siri was brand new,

00:09:43.810 --> 00:09:46.680
there was a whole spate
of bad press for Apple

00:09:46.680 --> 00:09:50.289
because people have found
that if you said things

00:09:50.289 --> 00:09:51.705
like you wanted
to shoot yourself,

00:09:51.705 --> 00:09:54.180
it would give you
directions to a gun store.

00:09:54.180 --> 00:09:56.942
There was another example where
somebody was saying something

00:09:56.942 --> 00:09:58.900
about jumping off of a
bridge, and it told them

00:09:58.900 --> 00:10:00.780
where the nearest bridge was.

00:10:00.780 --> 00:10:04.500
And Apple was, like, oh, whoops.

00:10:04.500 --> 00:10:06.790
We don't want that to happen.

00:10:06.790 --> 00:10:08.730
And so they went,
and they fixed it.

00:10:08.730 --> 00:10:11.280
And they said, OK, well,
now if you say something

00:10:11.280 --> 00:10:13.830
that Siri identifies as
being potentially suicidal,

00:10:13.830 --> 00:10:16.320
what we're going to do is
we're going to surface up

00:10:16.320 --> 00:10:18.570
some information about the
National Suicide Prevention

00:10:18.570 --> 00:10:19.290
Lifeline.

00:10:19.290 --> 00:10:21.610
And so you can click
through to them

00:10:21.610 --> 00:10:23.940
so you can get help in crisis.

00:10:23.940 --> 00:10:27.530
So they did that in 2011.

00:10:27.530 --> 00:10:30.240
But I look at that,
and I wonder, well, OK,

00:10:30.240 --> 00:10:34.760
if you knew that in 2011, why,
five years later, has it not

00:10:34.760 --> 00:10:37.550
occurred to you to go
beyond that one thing

00:10:37.550 --> 00:10:40.100
and to make changes to
the product as a whole?

00:10:40.100 --> 00:10:42.950
Why is it still more
important to program jokes

00:10:42.950 --> 00:10:46.220
into the interface than to think
about other types of scenarios

00:10:46.220 --> 00:10:49.870
where somebody might
turn to the device?

00:10:49.870 --> 00:10:51.734
And in fact, in this
particular example,

00:10:51.734 --> 00:10:53.150
when I started
talking about this,

00:10:53.150 --> 00:10:55.180
I had some folks
say things to me

00:10:55.180 --> 00:11:00.070
like, well, you're an idiot if
you use your phone after being

00:11:00.070 --> 00:11:01.750
sexually assaulted.

00:11:01.750 --> 00:11:04.240
You need to go to the police--

00:11:04.240 --> 00:11:07.400
or various other
statements like that.

00:11:07.400 --> 00:11:10.880
And I thought, you know what?

00:11:10.880 --> 00:11:12.530
I don't actually
care what you think

00:11:12.530 --> 00:11:14.321
about what somebody
should do after they've

00:11:14.321 --> 00:11:17.050
been sexually assaulted.

00:11:17.050 --> 00:11:19.540
What I care about is the
fact that people are using

00:11:19.540 --> 00:11:21.640
their phones during this time.

00:11:21.640 --> 00:11:25.050
There is a quote from my friend
Karen McGrane, who is well

00:11:25.050 --> 00:11:27.730
known in user experience,
and she did a lot of work

00:11:27.730 --> 00:11:28.870
on mobile content.

00:11:28.870 --> 00:11:33.880
And when she started doing
that, she would say to people,

00:11:33.880 --> 00:11:36.340
you don't get to decide
what device people use

00:11:36.340 --> 00:11:37.570
to access the internet.

00:11:37.570 --> 00:11:38.930
They do.

00:11:38.930 --> 00:11:41.350
And I think that it's a
similar sentiment here.

00:11:41.350 --> 00:11:43.990
You don't get to decide what
circumstances somebody is

00:11:43.990 --> 00:11:46.150
going to be in when they
use your technology.

00:11:46.150 --> 00:11:49.150
You don't get to decide that
somebody should or shouldn't

00:11:49.150 --> 00:11:52.690
use their smartphone assistant
when they're in crisis.

00:11:52.690 --> 00:11:54.632
They're going to decide that.

00:11:54.632 --> 00:11:57.090
The only power that we have in
tech is to figure out, well,

00:11:57.090 --> 00:11:58.190
how do we respond to that?

00:11:58.190 --> 00:12:00.290
Do we choose to
help them or not?

00:12:00.290 --> 00:12:02.540
Do we choose to
anticipate that or not?

00:12:02.540 --> 00:12:04.800
How do we deal with that?

00:12:04.800 --> 00:12:06.590
Because you can think
that it's a bad idea

00:12:06.590 --> 00:12:09.280
all you want, but
people are doing it.

00:12:09.280 --> 00:12:12.540
And those people,
oftentimes, are kids or youth

00:12:12.540 --> 00:12:17.340
who are more comfortable talking
to somebody on the screen

00:12:17.340 --> 00:12:20.110
than they would be to go
to somebody in real life.

00:12:20.110 --> 00:12:23.160
And what do you want
to do about that?

00:12:23.160 --> 00:12:25.668
And so I look at
this, and I think,

00:12:25.668 --> 00:12:28.410
there's been a
lot of opportunity

00:12:28.410 --> 00:12:30.810
to improve this beyond
just, let's change one

00:12:30.810 --> 00:12:32.110
individual thing.

00:12:32.110 --> 00:12:33.750
But we haven't really seen that.

00:12:33.750 --> 00:12:35.860
Instead, what we have is we
have Apple going in and saying,

00:12:35.860 --> 00:12:36.570
oh, yeah, yeah, yeah.

00:12:36.570 --> 00:12:37.600
We don't want that to happen.

00:12:37.600 --> 00:12:39.933
So we'll partner with the
Rape Abuse and Incest National

00:12:39.933 --> 00:12:41.280
Network, RAINN.

00:12:41.280 --> 00:12:43.750
And we will do the same kind
of thing as we did before.

00:12:43.750 --> 00:12:45.210
We'll have a little--

00:12:45.210 --> 00:12:47.460
if you have an issue
with sexual assault,

00:12:47.460 --> 00:12:49.510
you can go to the
lifeline there.

00:12:49.510 --> 00:12:51.720
And I think that
that's a good change,

00:12:51.720 --> 00:12:53.310
but I don't think
it's enough, right?

00:12:53.310 --> 00:12:56.400
Because I don't know how
many other types of scenarios

00:12:56.400 --> 00:12:57.840
Siri is not going to understand.

00:12:57.840 --> 00:13:01.650
And I don't expect Siri to
be perfect by any means.

00:13:01.650 --> 00:13:03.870
But I do expect for
it to understand

00:13:03.870 --> 00:13:05.880
that it's going to
have scenarios that

00:13:05.880 --> 00:13:09.314
are negative and straight-up
terrifying that people are

00:13:09.314 --> 00:13:10.980
going to use that
device for-- and to be

00:13:10.980 --> 00:13:15.920
able to anticipate that and not
have these terrible breaches.

00:13:15.920 --> 00:13:18.530
But I find that over and
over, tech companies really

00:13:18.530 --> 00:13:20.840
aren't thinking
enough about the ways

00:13:20.840 --> 00:13:24.050
that their design decisions
and tech decisions can break.

00:13:24.050 --> 00:13:26.810
Grief is a great
example of that.

00:13:26.810 --> 00:13:29.690
Tech is very good at focusing
on things like delight--

00:13:29.690 --> 00:13:31.760
tends to be really
bad at focusing

00:13:31.760 --> 00:13:36.260
on things like grief,
particularly because we

00:13:36.260 --> 00:13:37.220
spend a lot of time--

00:13:37.220 --> 00:13:39.740
I know on my end
of the tech world,

00:13:39.740 --> 00:13:41.614
where we talk a lot
about UX and content--

00:13:41.614 --> 00:13:43.280
we spend a lot of
time talking to people

00:13:43.280 --> 00:13:44.560
about things like personality.

00:13:44.560 --> 00:13:47.420
And we've got to make this
human, make it friendly.

00:13:47.420 --> 00:13:50.940
And that gets translated into,
let's make this overly clever.

00:13:50.940 --> 00:13:53.090
And as a result, you
get things like this.

00:13:53.090 --> 00:13:58.700
We've got Timehop, which posted
to this person who was sharing

00:13:58.700 --> 00:14:00.320
about a memorial service.

00:14:00.320 --> 00:14:01.910
They resurfaced
that post and said,

00:14:01.910 --> 00:14:03.740
this is the really
long post that you

00:14:03.740 --> 00:14:06.300
wrote in the year of
2000-and-whatever.

00:14:06.300 --> 00:14:09.230
That's snarky, rude.

00:14:09.230 --> 00:14:11.810
Or you've got the millions
of different places where

00:14:11.810 --> 00:14:14.840
you've got-- "could a
gift say it better?"

00:14:14.840 --> 00:14:17.390
Could you be more clever?

00:14:17.390 --> 00:14:20.810
Or Medium-- which, Medium has
removed this string, actually,

00:14:20.810 --> 00:14:22.490
from their system.

00:14:22.490 --> 00:14:24.860
But when you posted a
new story on Medium,

00:14:24.860 --> 00:14:26.360
they would send you
these little fun

00:14:26.360 --> 00:14:29.000
facts along with their update
on how your story is doing.

00:14:29.000 --> 00:14:31.370
And they're trying to make
it seem like, it's OK.

00:14:31.370 --> 00:14:33.710
Your story is just
picking up steam.

00:14:33.710 --> 00:14:36.050
But you don't really
want a fun fact

00:14:36.050 --> 00:14:40.560
on a post that's in
memory of a friend.

00:14:40.560 --> 00:14:43.560
And in fact, maybe the
worst break of this sort

00:14:43.560 --> 00:14:45.930
is one that some of you
have probably heard of.

00:14:45.930 --> 00:14:48.600
It happened to Eric Meyer.

00:14:48.600 --> 00:14:51.240
And he is a longtime
web developer

00:14:51.240 --> 00:14:52.950
who I've known over the years.

00:14:52.950 --> 00:14:54.571
And it was one of
the things that

00:14:54.571 --> 00:14:56.820
led us to write the book
called "Design for Real Life"

00:14:56.820 --> 00:14:58.220
together a couple of years ago.

00:14:58.220 --> 00:15:01.800
And what happened to him
was that he went to Facebook

00:15:01.800 --> 00:15:04.230
on Christmas Eve in 2014.

00:15:04.230 --> 00:15:06.480
And he was expecting to
see the normal family

00:15:06.480 --> 00:15:08.220
well-wishes, stuff like that.

00:15:08.220 --> 00:15:11.070
But what he found
instead was this.

00:15:11.070 --> 00:15:12.270
This is a Year in Review.

00:15:12.270 --> 00:15:15.780
And what Year in
Review did was surface

00:15:15.780 --> 00:15:21.150
your most popular posts,
images, videos from the year,

00:15:21.150 --> 00:15:24.780
and package them up in a nice,
curated little collection

00:15:24.780 --> 00:15:26.390
for you.

00:15:26.390 --> 00:15:29.225
And then they took
that, and they put it

00:15:29.225 --> 00:15:30.600
into their little
wrapper, right,

00:15:30.600 --> 00:15:33.540
with balloons, streamers,
people dancing.

00:15:33.540 --> 00:15:36.880
And it says, hey, Eric, here's
what your year looked like.

00:15:36.880 --> 00:15:39.240
And in the center of that
was the most popular photo

00:15:39.240 --> 00:15:42.000
that he had posted all year.

00:15:42.000 --> 00:15:44.820
That photo is of his
daughter, Rebecca.

00:15:44.820 --> 00:15:47.040
And Rebecca had died
of an aggressive brain

00:15:47.040 --> 00:15:49.620
cancer on her sixth birthday.

00:15:49.620 --> 00:15:51.420
Of course, it was the
most popular photo

00:15:51.420 --> 00:15:53.220
he posted all year.

00:15:53.220 --> 00:15:55.380
It was also the worst
year of his life

00:15:55.380 --> 00:15:57.932
and the worst
moment of his life.

00:15:57.932 --> 00:16:00.750
And so Facebook had kind of
put that back in front of him

00:16:00.750 --> 00:16:04.412
in this peppy little package
and said, here you go--

00:16:04.412 --> 00:16:06.870
even though he'd been avoiding
this feature, even though he

00:16:06.870 --> 00:16:09.330
didn't want to use it.

00:16:09.330 --> 00:16:12.240
And I'll tell you, Eric
was gutted by this.

00:16:12.240 --> 00:16:14.409
It just hurt very badly.

00:16:14.409 --> 00:16:15.950
And he wrote this
blog post about it,

00:16:15.950 --> 00:16:17.100
and the blog post went viral.

00:16:17.100 --> 00:16:18.474
And all of a
sudden, it's getting

00:16:18.474 --> 00:16:21.060
reposted to "Slate," et
cetera, et cetera, et cetera.

00:16:21.060 --> 00:16:22.950
And Facebook reaches out.

00:16:22.950 --> 00:16:24.394
And the product
owner apologizes.

00:16:24.394 --> 00:16:26.060
I mean, they didn't
want this to happen.

00:16:26.060 --> 00:16:29.366
Nobody there wanted
this to happen.

00:16:29.366 --> 00:16:30.990
So they apologized
and said they're not

00:16:30.990 --> 00:16:32.070
going to do this again.

00:16:32.070 --> 00:16:33.630
And the next year, when
they did Year in Review,

00:16:33.630 --> 00:16:35.170
they changed the
design of that feature

00:16:35.170 --> 00:16:37.753
so it doesn't take your content
and put it into a new context.

00:16:41.210 --> 00:16:44.430
But now, it's almost
three years later.

00:16:44.430 --> 00:16:48.060
And in fact, Facebook is still
doing basically the same thing.

00:16:48.060 --> 00:16:51.290
So this is an example from
just a couple of weeks ago.

00:16:51.290 --> 00:16:53.550
Olivia Solon is a journalist
for "The Guardian."

00:16:53.550 --> 00:16:57.980
and she had posted
a photo to Instagram

00:16:57.980 --> 00:17:02.030
that was a screenshot
of an email she received

00:17:02.030 --> 00:17:04.740
that was full of rape threats.

00:17:04.740 --> 00:17:06.470
She posted it to
Instagram because she

00:17:06.470 --> 00:17:10.010
wanted to show the kind of
abuse that women who are public

00:17:10.010 --> 00:17:12.140
online often get.

00:17:12.140 --> 00:17:14.930
It was also a highly
commented on photo

00:17:14.930 --> 00:17:16.880
on her Instagram account.

00:17:16.880 --> 00:17:19.065
Well, Facebook owns Instagram.

00:17:19.065 --> 00:17:20.690
Facebook would like
more Facebook users

00:17:20.690 --> 00:17:22.170
to use Instagram.

00:17:22.170 --> 00:17:24.980
So what Facebook does is, it
will take your Instagram posts,

00:17:24.980 --> 00:17:27.650
insert them into an
ad for Instagram,

00:17:27.650 --> 00:17:31.310
and then put that ad onto
your friends' Facebook feeds.

00:17:31.310 --> 00:17:34.160
And so Facebook did
that with this image

00:17:34.160 --> 00:17:35.570
from her Instagram account.

00:17:35.570 --> 00:17:38.780
And so her friends started
receiving peppy ads

00:17:38.780 --> 00:17:44.130
for Instagram showcasing
a rape threat.

00:17:44.130 --> 00:17:46.860
Because you see, they
treated this problem

00:17:46.860 --> 00:17:49.500
with Year in Review as sort
of an isolated incident.

00:17:49.500 --> 00:17:51.360
They fixed it,
and they moved on.

00:17:51.360 --> 00:17:54.180
But they didn't think about
the overall problem of changing

00:17:54.180 --> 00:17:56.940
the context of users' content,
and how this could go wrong,

00:17:56.940 --> 00:17:58.190
and who could this could hurt.

00:18:01.020 --> 00:18:05.180
I think this quote from Zeynep
Tufekci, digital sociologist,

00:18:05.180 --> 00:18:06.639
really sums it up
effectively here.

00:18:06.639 --> 00:18:09.179
She just started talking about
this the other day on Twitter.

00:18:09.179 --> 00:18:11.510
She said "Silicon
Valley is run by people

00:18:11.510 --> 00:18:13.610
who think that they're
in the tech business,

00:18:13.610 --> 00:18:15.450
but they're in the
people business.

00:18:15.450 --> 00:18:18.790
And they're in a way
over their heads."

00:18:18.790 --> 00:18:20.040
And I think that this is true.

00:18:20.040 --> 00:18:21.623
I think that we have
spent a long time

00:18:21.623 --> 00:18:23.910
assuming that the most
important thing that we work on

00:18:23.910 --> 00:18:25.350
is technology.

00:18:25.350 --> 00:18:28.860
But in fact, we are affecting
people's lives in dramatic ways

00:18:28.860 --> 00:18:31.530
that we could not have even
envisioned a couple of decades

00:18:31.530 --> 00:18:32.174
ago.

00:18:32.174 --> 00:18:33.840
And we haven't really
caught up to that.

00:18:33.840 --> 00:18:36.720
We haven't really
taken that to heart.

00:18:36.720 --> 00:18:39.130
And we can see that playing
out in so many ways,

00:18:39.130 --> 00:18:41.270
from all of these little
interface examples,

00:18:41.270 --> 00:18:44.260
to much, much deeper places.

00:18:44.260 --> 00:18:48.250
For example, we see that playing
out frequently in anything

00:18:48.250 --> 00:18:54.950
seemingly related to image
recognition and image filters.

00:18:54.950 --> 00:18:58.480
For example, you may have
seen this on Snapchat.

00:18:58.480 --> 00:19:00.130
Snapchat, last year,
released something

00:19:00.130 --> 00:19:03.040
that it called the anime filter.

00:19:03.040 --> 00:19:06.760
Only, it didn't look like
any anime I have ever seen.

00:19:06.760 --> 00:19:10.060
What it looked more like was
something that you probably

00:19:10.060 --> 00:19:11.590
wouldn't see today.

00:19:11.590 --> 00:19:14.470
That's Mickey Rooney playing
IY Yunioshi in "Breakfast

00:19:14.470 --> 00:19:15.880
at Tiffany's."

00:19:15.880 --> 00:19:17.500
That is Mickey
Rooney doing, what

00:19:17.500 --> 00:19:19.480
we would now call, yellowface.

00:19:19.480 --> 00:19:22.527
He's dressed up as
an Asian person.

00:19:22.527 --> 00:19:24.110
And what that filter
did was basically

00:19:24.110 --> 00:19:27.710
the same-- here's your slanty
eyes and your buck teeth.

00:19:27.710 --> 00:19:36.280
And you know, race is not a
costume that you get to put on.

00:19:36.280 --> 00:19:38.140
And we probably
mostly know by now

00:19:38.140 --> 00:19:40.060
that you don't dress
up as an Asian person

00:19:40.060 --> 00:19:43.040
to go to a Halloween party.

00:19:43.040 --> 00:19:45.530
And yet, nobody thought
that it might be a problem

00:19:45.530 --> 00:19:47.210
to dress somebody up
as an Asian person

00:19:47.210 --> 00:19:49.490
in their selfie on Snapchat.

00:19:49.490 --> 00:19:51.830
After this happened, Snapchat
wouldn't apologize for it.

00:19:51.830 --> 00:19:54.727
And in fact, it wasn't
even the first time

00:19:54.727 --> 00:19:56.060
they'd done something like this.

00:19:56.060 --> 00:19:59.540
Because just a few months
before that, on April 20,

00:19:59.540 --> 00:20:02.340
they released a filter
they called Bob Marley.

00:20:02.340 --> 00:20:06.710
And it gave people black
skin and dreadlocks.

00:20:06.710 --> 00:20:08.300
And again, it was
roundly criticized

00:20:08.300 --> 00:20:11.340
as being a digital blackface.

00:20:11.340 --> 00:20:15.440
But they wouldn't actually
admit that this was a problem.

00:20:15.440 --> 00:20:17.040
And it's certainly
not just Snapchat.

00:20:17.040 --> 00:20:20.580
Because just this year,
FaceApp made this splash

00:20:20.580 --> 00:20:22.320
for a little while
where you could

00:20:22.320 --> 00:20:24.750
make selfies that were a
younger version of yourself

00:20:24.750 --> 00:20:27.000
or an older version of
yourself or a hotter

00:20:27.000 --> 00:20:28.870
version of yourself.

00:20:28.870 --> 00:20:31.810
And for the hotness
filter, what people started

00:20:31.810 --> 00:20:35.500
realizing was that it
made their skin lighter.

00:20:35.500 --> 00:20:39.230
It changed their features
to look more European.

00:20:39.230 --> 00:20:41.590
And what's
interesting about this

00:20:41.590 --> 00:20:44.830
is that FaceApp admitted
what had gone wrong.

00:20:44.830 --> 00:20:48.100
The CEO said, "We are deeply
sorry for this unquestionably

00:20:48.100 --> 00:20:49.520
serious issue.

00:20:49.520 --> 00:20:51.100
It's an unfortunate
side-effect of

00:20:51.100 --> 00:20:53.780
the underlying neural network
caused by the training set

00:20:53.780 --> 00:20:58.150
bias, not intended behavior."

00:20:58.150 --> 00:21:01.350
Now, I read that
and I think, OK.

00:21:01.350 --> 00:21:02.740
It's an unfortunate side-effect.

00:21:02.740 --> 00:21:04.620
It's not intended behavior.

00:21:04.620 --> 00:21:07.280
Actually, that's not true.

00:21:07.280 --> 00:21:09.790
It's definitely intended
behavior in the sense

00:21:09.790 --> 00:21:16.030
that you took a set of photos
to train the algorithm on what

00:21:16.030 --> 00:21:18.820
beauty was or what
hotness was, right?

00:21:18.820 --> 00:21:20.650
And those were photos
of white people.

00:21:20.650 --> 00:21:23.066
Like you've just said, you had
bias in that training data.

00:21:23.066 --> 00:21:25.900
So you fed the system a bunch
of photos of white people

00:21:25.900 --> 00:21:30.310
and said here is what
attractiveness looks like.

00:21:30.310 --> 00:21:32.460
And so the algorithm
worked as intended.

00:21:32.460 --> 00:21:34.920
It found patterns in those
pictures of white people,

00:21:34.920 --> 00:21:38.500
and it applied them
to these selfies.

00:21:38.500 --> 00:21:41.730
So the algorithm was
working as intended.

00:21:41.730 --> 00:21:44.520
You just fed it biased
data at the beginning.

00:21:44.520 --> 00:21:47.980
And it's not an
unfortunate side-effect.

00:21:47.980 --> 00:21:50.760
It's, in fact, entirely on
you and entirely something

00:21:50.760 --> 00:21:53.630
that you could have anticipated.

00:21:53.630 --> 00:21:55.130
But they apologized.

00:21:55.130 --> 00:21:55.940
They renamed it.

00:21:55.940 --> 00:21:57.590
They said they were
going to fix it.

00:21:57.590 --> 00:22:00.020
But just a little
later, this August,

00:22:00.020 --> 00:22:02.540
they released this new filter.

00:22:02.540 --> 00:22:06.020
This one, you could
literally try on races--

00:22:06.020 --> 00:22:09.120
black, Caucasian,
Asian, Indian--

00:22:09.120 --> 00:22:11.840
and so you could just select
them and show yourself

00:22:11.840 --> 00:22:13.730
as different races.

00:22:13.730 --> 00:22:18.440
And people were not
pleased with this.

00:22:18.440 --> 00:22:21.480
This was not a good idea.

00:22:21.480 --> 00:22:24.320
And immediately, they decided
they would take this down.

00:22:24.320 --> 00:22:26.694
They apologized and said, "The
new controversial features

00:22:26.694 --> 00:22:29.650
will be removed in
the next few hours."

00:22:29.650 --> 00:22:33.840
And I read that, and I
think, controversial is not

00:22:33.840 --> 00:22:35.370
the right word for this.

00:22:35.370 --> 00:22:40.790
Controversial implies that
well, got to hear both sides--

00:22:40.790 --> 00:22:42.840
some disagreement.

00:22:42.840 --> 00:22:43.830
No.

00:22:43.830 --> 00:22:46.820
Calling it controversial--
what that does

00:22:46.820 --> 00:22:49.620
is that doesn't
acknowledge history.

00:22:49.620 --> 00:22:52.440
It doesn't acknowledge there
is a tremendous body of work--

00:22:52.440 --> 00:22:55.230
like, have you ever heard
of critical race theory?

00:22:55.230 --> 00:22:56.760
Maybe you should
before you start

00:22:56.760 --> 00:22:59.550
messing with races and filters.

00:22:59.550 --> 00:23:01.560
There's an entire
body of work that's

00:23:01.560 --> 00:23:04.200
looking at how race
functions in culture.

00:23:04.200 --> 00:23:06.420
And so it is not
enough to write it off

00:23:06.420 --> 00:23:08.290
as a controversial feature.

00:23:08.290 --> 00:23:11.160
It's not like somebody
didn't like your new logo,

00:23:11.160 --> 00:23:13.770
and people wrote some
Medium post about it.

00:23:13.770 --> 00:23:14.340
No.

00:23:14.340 --> 00:23:18.770
This is an actual problem,
and it is well-documented.

00:23:18.770 --> 00:23:22.110
And so it gets reduced
to just a disagreement.

00:23:22.110 --> 00:23:25.380
And the other thing about
this is that this is not news.

00:23:25.380 --> 00:23:27.240
FaceApp could have known better.

00:23:27.240 --> 00:23:29.880
Because not only
had Snapchat already

00:23:29.880 --> 00:23:32.340
had some similar issues, but
we can go back to the year

00:23:32.340 --> 00:23:35.220
before, to 2015, and we
can talk about something

00:23:35.220 --> 00:23:37.770
that happened here at Google,
which I'm sure most of you

00:23:37.770 --> 00:23:40.380
are familiar with, when
some photos, a whole series

00:23:40.380 --> 00:23:43.200
of photos of black
people got tagged

00:23:43.200 --> 00:23:46.410
by Google Photos
auto-tagging as gorillas.

00:23:46.410 --> 00:23:49.620
Now, the fact that they weren't
auto-tagged with something that

00:23:49.620 --> 00:23:53.702
is a racial slur is
probably the reason

00:23:53.702 --> 00:23:54.910
that this blew up really big.

00:23:54.910 --> 00:23:56.970
It's definitely the reason
this blew up really big.

00:23:56.970 --> 00:23:58.140
This was all over the media.

00:23:58.140 --> 00:24:00.360
And that's what
people talked about.

00:24:00.360 --> 00:24:03.360
But the thing about this example
that is much more important,

00:24:03.360 --> 00:24:06.770
I think, and that wasn't
talked about nearly as much,

00:24:06.770 --> 00:24:09.040
was why this happened.

00:24:09.040 --> 00:24:11.010
And so I dug around
at this example,

00:24:11.010 --> 00:24:13.740
and I found what
Yonatan Zunger had

00:24:13.740 --> 00:24:18.030
said to Jacky Alcine, who was
the guy that this happened to,

00:24:18.030 --> 00:24:19.690
after the fact.

00:24:19.690 --> 00:24:21.730
He seems like a wonderful guy.

00:24:21.730 --> 00:24:24.802
He seems to be trying very
hard to get this stuff right.

00:24:24.802 --> 00:24:26.760
And yet, you take a look
at what he wrote here.

00:24:26.760 --> 00:24:29.509
He said, "We're working on
longer-term fixes around both

00:24:29.509 --> 00:24:31.550
linguistics-- words to be
careful about in photos

00:24:31.550 --> 00:24:32.200
of people"--

00:24:32.200 --> 00:24:32.700
OK.

00:24:32.700 --> 00:24:34.650
So we're going to
be more careful

00:24:34.650 --> 00:24:38.430
applying tags that could
have questionable context--

00:24:38.430 --> 00:24:40.890
but also "image
recognition, itself--

00:24:40.890 --> 00:24:45.000
e.g., better recognition
of dark-skinned faces."

00:24:45.000 --> 00:24:46.560
And what that is
acknowledging right

00:24:46.560 --> 00:24:49.980
there is that that product
went to market not as

00:24:49.980 --> 00:24:52.380
good at identifying
dark-skinned faces

00:24:52.380 --> 00:24:55.440
as it was at identifying
white people.

00:24:55.440 --> 00:24:58.460
So the specifics
of that missed tag

00:24:58.460 --> 00:25:00.960
happened to look really bad.

00:25:00.960 --> 00:25:02.610
But the underlying
issue was that it

00:25:02.610 --> 00:25:06.290
was more likely to
mis-tag people of color.

00:25:06.290 --> 00:25:08.540
And I think, well, how did
you get a product to market

00:25:08.540 --> 00:25:12.920
that wasn't that good at
identifying dark-skinned faces?

00:25:12.920 --> 00:25:16.390
Well, that's not just Google.

00:25:16.390 --> 00:25:18.700
Because failing design
for black people--

00:25:18.700 --> 00:25:20.350
that's not new.

00:25:20.350 --> 00:25:23.200
Back in the '50s,
Kodak first started

00:25:23.200 --> 00:25:26.320
allowing people who
were not in Kodak labs

00:25:26.320 --> 00:25:27.350
to produce their film.

00:25:27.350 --> 00:25:28.766
So you could go
to a mom and pop's

00:25:28.766 --> 00:25:30.161
to get your film developed.

00:25:30.161 --> 00:25:31.660
And so what they
did is they started

00:25:31.660 --> 00:25:35.620
sending these little packets
to the photo lab technicians

00:25:35.620 --> 00:25:39.160
to use to calibrate
skin tones and light.

00:25:39.160 --> 00:25:42.780
And they called
them "Shirley" cards

00:25:42.780 --> 00:25:44.830
named for the first woman
who ever sat for them

00:25:44.830 --> 00:25:46.740
who was a Kodak employee.

00:25:46.740 --> 00:25:48.700
And for decades, they
sent these cards out.

00:25:48.700 --> 00:25:49.950
And they were always the same.

00:25:49.950 --> 00:25:51.270
The styles would
change, the woman

00:25:51.270 --> 00:25:53.395
sitting for them would
change, but they were always

00:25:53.395 --> 00:25:54.750
called "Shirley" cards.

00:25:54.750 --> 00:25:57.750
They always said
"normal" on them,

00:25:57.750 --> 00:26:01.470
and they always
showed a white person.

00:26:01.470 --> 00:26:04.260
And so for decades,
this went on.

00:26:04.260 --> 00:26:06.724
And a black photographer
wrote this piece on BuzzFeed

00:26:06.724 --> 00:26:07.890
where she talked about this.

00:26:07.890 --> 00:26:09.810
And she talked about
her experience feeling

00:26:09.810 --> 00:26:13.830
like film was never developing
correctly for her skin tone.

00:26:13.830 --> 00:26:16.290
And she said, "With a white
body as a light meter,

00:26:16.290 --> 00:26:20.310
all other skin tones become
deviations from the norm."

00:26:20.310 --> 00:26:23.810
It turned out that film's
failure to capture dark skin

00:26:23.810 --> 00:26:26.010
is not a technical issue.

00:26:26.010 --> 00:26:28.600
It's a choice.

00:26:28.600 --> 00:26:31.750
And in fact, a researcher
went and talked

00:26:31.750 --> 00:26:33.910
to a bunch of people who
used to work at Kodak back

00:26:33.910 --> 00:26:36.400
in the '70s when this
started to change.

00:26:36.400 --> 00:26:39.040
And what she learned was
that it wasn't that they

00:26:39.040 --> 00:26:40.390
decided to be more inclusive.

00:26:40.390 --> 00:26:44.740
It was that furniture makers
and chocolate manufacturers

00:26:44.740 --> 00:26:45.970
were complaining.

00:26:45.970 --> 00:26:48.790
And they were complaining
because this film was not

00:26:48.790 --> 00:26:52.144
properly showing the difference
in different wood grains

00:26:52.144 --> 00:26:54.310
and the difference in
different chocolate varieties,

00:26:54.310 --> 00:26:55.970
like milk versus dark.

00:26:55.970 --> 00:27:00.140
And that is actually what
led to that product shift.

00:27:00.140 --> 00:27:03.090
And so here we are, again,
over and over again,

00:27:03.090 --> 00:27:06.690
seeing tech companies re-enact
these kinds of choices.

00:27:06.690 --> 00:27:08.370
It's not a technical issue.

00:27:08.370 --> 00:27:10.960
It's a choice.

00:27:10.960 --> 00:27:15.090
And when I look at that, I
have no choice but to say,

00:27:15.090 --> 00:27:18.330
this is literally what it means
when you say white supremacy.

00:27:18.330 --> 00:27:21.600
And people don't want to
hear those words, oftentimes.

00:27:21.600 --> 00:27:23.200
Because when you
say white supremacy,

00:27:23.200 --> 00:27:25.470
what people want
to be talking about

00:27:25.470 --> 00:27:28.680
is scary guys with swastikas.

00:27:28.680 --> 00:27:31.712
And in fact, those
are terrifying people.

00:27:31.712 --> 00:27:34.170
But when you talk about white
supremacy, what you're really

00:27:34.170 --> 00:27:37.260
talking about is simply
putting whiteness first.

00:27:37.260 --> 00:27:38.970
And so, if you
decide you're going

00:27:38.970 --> 00:27:41.520
to make a product that works
better for white people,

00:27:41.520 --> 00:27:43.770
if you decide that you're
not going to include people

00:27:43.770 --> 00:27:46.094
of color in your
training data, you

00:27:46.094 --> 00:27:47.760
have literally decided
that white people

00:27:47.760 --> 00:27:49.410
are more important.

00:27:49.410 --> 00:27:52.900
You have literally
enacted white supremacy.

00:27:52.900 --> 00:27:54.850
That is uncomfortable.

00:27:54.850 --> 00:27:56.650
That is not a
conversation most of us

00:27:56.650 --> 00:27:59.230
want to get up in
the morning and have.

00:27:59.230 --> 00:28:02.800
But the thing is, that's
what we do in tech.

00:28:02.800 --> 00:28:05.260
That is what we enact
every single time we don't

00:28:05.260 --> 00:28:06.580
specifically work against it.

00:28:09.910 --> 00:28:14.310
We embed it because it's
already present in our culture--

00:28:14.310 --> 00:28:17.730
not because tech created it,
but because it already exists.

00:28:17.730 --> 00:28:19.770
And so we just re-enact
it over and over again,

00:28:19.770 --> 00:28:21.530
unless we question it.

00:28:21.530 --> 00:28:26.220
And the thing is, we do all
of that while insisting,

00:28:26.220 --> 00:28:28.680
over and over, that
no, no, no, no, no, no.

00:28:28.680 --> 00:28:30.040
We're not racist, though.

00:28:30.040 --> 00:28:31.620
We want to include everybody.

00:28:31.620 --> 00:28:33.600
And we talk about
tech as if it's

00:28:33.600 --> 00:28:36.666
some kind of utopian
society that's possible.

00:28:36.666 --> 00:28:38.790
But we're not necessarily
doing that difficult work

00:28:38.790 --> 00:28:40.470
of looking at the
ways that we continue

00:28:40.470 --> 00:28:41.770
to center white people.

00:28:44.690 --> 00:28:46.460
And that's, I think,
bad enough when

00:28:46.460 --> 00:28:49.220
you're talking about
something like photo tagging.

00:28:49.220 --> 00:28:51.540
But it, of course, gets
even more worrisome

00:28:51.540 --> 00:28:55.166
the more that tech embeds
into other aspects of life.

00:28:55.166 --> 00:28:56.540
For example, some
of you may have

00:28:56.540 --> 00:29:00.140
heard about some
software called COMPAS.

00:29:00.140 --> 00:29:02.930
COMPAS stands for Correctional
Offender Management Profiling

00:29:02.930 --> 00:29:04.184
for Alternative Sanctions.

00:29:04.184 --> 00:29:05.975
It is made by a company
called Northpointe.

00:29:05.975 --> 00:29:11.090
And it is being used in
courts around the country

00:29:11.090 --> 00:29:16.040
to decide how risky somebody
is to commit a future crime.

00:29:16.040 --> 00:29:19.550
So it provides criminal
recidivism scoring.

00:29:19.550 --> 00:29:22.760
And last year, ProPublica
did a big investigation

00:29:22.760 --> 00:29:24.140
into this software.

00:29:24.140 --> 00:29:28.640
And what they found was that it
had some real biases embedded

00:29:28.640 --> 00:29:29.910
within it.

00:29:29.910 --> 00:29:32.080
So for example, you
have these two men here.

00:29:32.080 --> 00:29:34.490
On your left, you've
got Bernard Parker.

00:29:34.490 --> 00:29:37.940
In January of 2013,
he was arrested

00:29:37.940 --> 00:29:41.420
in Broward County, Florida,
for marijuana possession.

00:29:41.420 --> 00:29:43.640
And then, on your right,
you have Dylan Fugett.

00:29:43.640 --> 00:29:46.190
And in the same year,
one month later,

00:29:46.190 --> 00:29:48.570
in the same place,
Broward County, Florida,

00:29:48.570 --> 00:29:51.831
he was arrested for
possession of cocaine.

00:29:51.831 --> 00:29:54.640
Now, both of these men
had a prior record.

00:29:54.640 --> 00:29:58.450
Bernard had resisted
arrest without violence,

00:29:58.450 --> 00:30:01.180
and Dylan had
attempted burglary.

00:30:01.180 --> 00:30:03.070
But according to
COMPAS, these men

00:30:03.070 --> 00:30:05.790
did not have a similar
profile at all.

00:30:05.790 --> 00:30:07.570
Because Bernard
was labeled a 10--

00:30:07.570 --> 00:30:10.540
the highest risk there
is for recidivism.

00:30:10.540 --> 00:30:13.180
And Dylan was labeled a 3.

00:30:13.180 --> 00:30:14.920
And in fact, Dylan
happened to go on

00:30:14.920 --> 00:30:18.100
to be arrested three more
times on drug charges.

00:30:18.100 --> 00:30:20.710
Bernard was not
arrested again at all.

00:30:20.710 --> 00:30:22.810
And what ProPublica
found was that that story

00:30:22.810 --> 00:30:27.380
was playing out over and
over again with the software.

00:30:27.380 --> 00:30:33.210
So the software was particularly
likely to falsely flag

00:30:33.210 --> 00:30:35.955
black defendants as
likely to re-offend.

00:30:35.955 --> 00:30:39.690
So 45% of those who
were labeled high-risk

00:30:39.690 --> 00:30:44.700
did not re-offend, versus
only 24% of white defendants.

00:30:44.700 --> 00:30:49.670
Meanwhile, the opposite
was true for low-risk.

00:30:49.670 --> 00:30:53.680
48% of white defendants
labeled low risk did re-offend,

00:30:53.680 --> 00:30:57.695
and 28% of black
defendants re-offended.

00:30:57.695 --> 00:31:00.770
So what you have
is a system where,

00:31:00.770 --> 00:31:06.670
over and over again, when
the system gets it wrong,

00:31:06.670 --> 00:31:10.390
the people it is wrong for, the
people who are harmed by that,

00:31:10.390 --> 00:31:13.060
are much more likely
to be black people.

00:31:13.060 --> 00:31:16.690
Now, after ProPublica
did this research,

00:31:16.690 --> 00:31:18.250
some other researchers
from Stanford

00:31:18.250 --> 00:31:20.230
started digging into
this more closely.

00:31:20.230 --> 00:31:22.774
And they looked at what
Northpointe, the company who

00:31:22.774 --> 00:31:24.190
makes the software,
said about it,

00:31:24.190 --> 00:31:25.660
versus what ProPublica said.

00:31:25.660 --> 00:31:28.360
And they found an
underlying problem.

00:31:28.360 --> 00:31:31.220
And the problem wasn't
a technical glitch.

00:31:31.220 --> 00:31:34.840
The problem was different ideas
about what fairness means.

00:31:34.840 --> 00:31:37.540
So at ProPublica, they
said this is not fair,

00:31:37.540 --> 00:31:41.140
because you have a group that
is disproportionately harmed

00:31:41.140 --> 00:31:43.290
by inaccurate predictions.

00:31:43.290 --> 00:31:45.730
What Northpointe said
was that their algorithm

00:31:45.730 --> 00:31:51.080
had been tuned to parity of
accuracy at each score level.

00:31:51.080 --> 00:31:53.320
Meaning that, if you
scored a 7, whether you

00:31:53.320 --> 00:31:56.710
were black or white, you were
roughly the same likelihood

00:31:56.710 --> 00:31:58.420
of committing a future crime.

00:31:58.420 --> 00:32:03.490
So 60% of white people and 61%
of black people who scored a 7

00:32:03.490 --> 00:32:05.541
would go on to commit
a future crime.

00:32:05.541 --> 00:32:07.540
And they said that was
equality, see, because it

00:32:07.540 --> 00:32:09.500
was equal across races.

00:32:09.500 --> 00:32:10.404
But the problem is--

00:32:10.404 --> 00:32:12.820
the researchers at Stanford
who looked into this further--

00:32:12.820 --> 00:32:15.830
they said, well,
you can't have both.

00:32:15.830 --> 00:32:19.410
You cannot define fairness
that way and fairness this way.

00:32:19.410 --> 00:32:20.785
And the reason
you can't do that,

00:32:20.785 --> 00:32:22.960
said it's mathematically
impossible,

00:32:22.960 --> 00:32:27.099
because you have different
base arrest rates across races.

00:32:27.099 --> 00:32:28.640
So then you have to
talk about, well,

00:32:28.640 --> 00:32:31.240
why do we have different
baseline arrest rates?

00:32:31.240 --> 00:32:33.640
Well, we can look
at a lot of reasons

00:32:33.640 --> 00:32:36.070
why the incarceration of
black people in this country

00:32:36.070 --> 00:32:39.520
is tremendously higher
than white people.

00:32:39.520 --> 00:32:41.360
I will not go into
all of them today,

00:32:41.360 --> 00:32:43.900
but we can talk about things
like the different application

00:32:43.900 --> 00:32:47.050
of drug laws, of crack
cocaine versus regular cocaine

00:32:47.050 --> 00:32:48.040
for decades.

00:32:48.040 --> 00:32:50.290
We can talk about the ways
that black communities tend

00:32:50.290 --> 00:32:52.060
to be policed more
than white communities

00:32:52.060 --> 00:32:54.010
even though there
are a statistically

00:32:54.010 --> 00:32:56.010
similar likelihood of
crimes being committed

00:32:56.010 --> 00:32:57.310
in both of those places.

00:32:57.310 --> 00:33:01.150
We can talk about a whole lot
of different historical factors

00:33:01.150 --> 00:33:03.330
that might have led to this.

00:33:03.330 --> 00:33:05.670
But the other thing
that we can talk about

00:33:05.670 --> 00:33:08.100
is what went into the
scores that wasn't just

00:33:08.100 --> 00:33:09.690
past criminal profile.

00:33:09.690 --> 00:33:13.640
Because if you remember, those
two examples we talked about,

00:33:13.640 --> 00:33:15.510
Dylan and Bernard,
they are really

00:33:15.510 --> 00:33:17.600
similar criminal profiles.

00:33:17.600 --> 00:33:19.900
But COMPAS doesn't
just care about that.

00:33:19.900 --> 00:33:24.230
COMPAS cares about a
lot of other factors.

00:33:24.230 --> 00:33:28.290
I think there's 137
different factors.

00:33:28.290 --> 00:33:31.700
Is there a crime in
your neighborhood?

00:33:31.700 --> 00:33:34.652
Is it easy to get drugs
in your neighborhood?

00:33:34.652 --> 00:33:38.820
Was your father ever arrested?

00:33:38.820 --> 00:33:42.520
Were you ever
suspended or expelled?

00:33:42.520 --> 00:33:45.160
All of these questions
go in to factoring

00:33:45.160 --> 00:33:47.320
what your score is going to be.

00:33:47.320 --> 00:33:51.130
And the thing is, these
questions are not neutral.

00:33:51.130 --> 00:33:54.400
These questions are very, very
much tied to race and class

00:33:54.400 --> 00:33:55.810
in this country.

00:33:55.810 --> 00:33:58.780
If you are poor in America,
and if you're black,

00:33:58.780 --> 00:34:01.150
you were more likely
to grow up poor.

00:34:01.150 --> 00:34:03.310
You probably lived
in a neighborhood

00:34:03.310 --> 00:34:05.410
that had more crime.

00:34:05.410 --> 00:34:07.360
You probably lived
in a neighborhood

00:34:07.360 --> 00:34:11.080
where you had to
move around a lot.

00:34:11.080 --> 00:34:13.840
If you were black in
the United States,

00:34:13.840 --> 00:34:16.210
and incarceration rates
are what they are,

00:34:16.210 --> 00:34:18.969
the likelihood that somebody in
your family has been arrested

00:34:18.969 --> 00:34:20.860
is way higher.

00:34:20.860 --> 00:34:22.719
And if you think about
it, the only reason

00:34:22.719 --> 00:34:25.330
that you build software like
COMPAS in the first place

00:34:25.330 --> 00:34:28.570
is because you think that there
is some kind of human bias

00:34:28.570 --> 00:34:30.630
that you're trying
to get rid of.

00:34:30.630 --> 00:34:33.130
Because you don't want to just
leave it to individual judges

00:34:33.130 --> 00:34:34.420
to make these decisions.

00:34:34.420 --> 00:34:36.679
Otherwise, why would
you build that software?

00:34:36.679 --> 00:34:38.920
So the software is meant
to make it less biased,

00:34:38.920 --> 00:34:41.290
make it fairer for people.

00:34:41.290 --> 00:34:44.830
And yet, we're not questioning
the underlying information

00:34:44.830 --> 00:34:47.900
going into this model and
saying, well, wait a second.

00:34:47.900 --> 00:34:50.350
What was the historical
context that led to this?

00:34:52.860 --> 00:34:55.050
Do we want to consider
things like whether you've

00:34:55.050 --> 00:34:57.000
been suspended from
school when we also

00:34:57.000 --> 00:34:59.070
know that black
kids are much more

00:34:59.070 --> 00:35:01.560
likely to be suspended from
school for the same infractions

00:35:01.560 --> 00:35:04.270
as white kids?

00:35:04.270 --> 00:35:07.800
Are we going to ask
those questions or not?

00:35:07.800 --> 00:35:10.300
And I think that this is
the extreme example of what

00:35:10.300 --> 00:35:12.790
happens when we assume that
something that is technical

00:35:12.790 --> 00:35:15.200
is also neutral.

00:35:15.200 --> 00:35:16.735
And I think we do
this all the time

00:35:16.735 --> 00:35:18.110
because we're so
used to thinking

00:35:18.110 --> 00:35:21.440
about things as technical
problems to be solved.

00:35:21.440 --> 00:35:23.315
But of course, this
is not neutral at all.

00:35:23.315 --> 00:35:25.190
The information that
went into that algorithm

00:35:25.190 --> 00:35:27.950
is anything but neutral, and
it needs to be interrogated.

00:35:27.950 --> 00:35:30.145
And it needs to be
interrogated at a level that

00:35:30.145 --> 00:35:32.270
is so much deeper than what
most tech companies are

00:35:32.270 --> 00:35:33.782
prepared to do right now.

00:35:33.782 --> 00:35:35.240
And so what ends
up happening is we

00:35:35.240 --> 00:35:37.110
have algorithms that
don't eliminate bias,

00:35:37.110 --> 00:35:39.650
but they just outsource it.

00:35:39.650 --> 00:35:42.500
And by that, I mean you can
make it the machine's problem.

00:35:42.500 --> 00:35:44.270
And so you, as a human
person, don't have

00:35:44.270 --> 00:35:45.570
to be responsible for the bias.

00:35:45.570 --> 00:35:47.510
And we all feel better
about how unbiased

00:35:47.510 --> 00:35:49.760
we are because we're not
making these biased decisions

00:35:49.760 --> 00:35:51.770
and not leaving it
up to racist judges.

00:35:51.770 --> 00:35:55.449
And we're letting the
machine sort it out.

00:35:55.449 --> 00:35:57.740
And out of that machine,
comes some nice, clean, little

00:35:57.740 --> 00:35:58.610
numbers, right--

00:35:58.610 --> 00:36:01.700
3, 7, 10, charts, graphs--

00:36:01.700 --> 00:36:05.490
It seems so obvious, so clear.

00:36:05.490 --> 00:36:08.237
And in fact, I talk
to designers a lot,

00:36:08.237 --> 00:36:09.570
and I work with designers a lot.

00:36:09.570 --> 00:36:12.840
And design is intended
to make these things seem

00:36:12.840 --> 00:36:15.180
like facts, like truth.

00:36:15.180 --> 00:36:17.670
Because we spend all of this
time trying to make software

00:36:17.670 --> 00:36:19.920
easy to use.

00:36:19.920 --> 00:36:22.440
I mean, that's most of what
I've spent my life doing,

00:36:22.440 --> 00:36:24.630
is trying to make things
easy to use for people.

00:36:24.630 --> 00:36:25.520
And that's important.

00:36:25.520 --> 00:36:28.644
Nobody is saying don't
make usable software.

00:36:28.644 --> 00:36:30.060
However, when you
say things like,

00:36:30.060 --> 00:36:31.860
"COMPAS is designed
to be user-friendly,

00:36:31.860 --> 00:36:33.990
even for those with
limited computer experience

00:36:33.990 --> 00:36:35.830
and education."

00:36:35.830 --> 00:36:38.340
What you end up
doing is you end up

00:36:38.340 --> 00:36:42.750
making it feel inevitable,
truthful, factual, so seamless

00:36:42.750 --> 00:36:44.130
and easy to use.

00:36:44.130 --> 00:36:46.320
You reduce really
complicated stuff

00:36:46.320 --> 00:36:50.070
down to things that feel
very palatable to people.

00:36:50.070 --> 00:36:52.890
And we hear this
over and over again

00:36:52.890 --> 00:36:55.565
across all kinds of different
tech companies, right?

00:36:55.565 --> 00:36:56.940
We want things to
be easy to use.

00:36:56.940 --> 00:36:59.070
We want things to seem right.

00:36:59.070 --> 00:37:01.260
That's certainly true at Google.

00:37:01.260 --> 00:37:04.619
Miriam Sweeney, who is a
librarian information sciences

00:37:04.619 --> 00:37:07.160
professor at the University of
Alabama, she wrote about this.

00:37:07.160 --> 00:37:08.720
She said, "The
simple, sparse design

00:37:08.720 --> 00:37:11.430
works to obscure the
complexity of the interface,

00:37:11.430 --> 00:37:14.850
making the result appear purely
scientific and data-driven.

00:37:14.850 --> 00:37:16.500
The insistence of
scientific 'truth'

00:37:16.500 --> 00:37:18.450
about algorithmic search
has encouraged users

00:37:18.450 --> 00:37:22.500
to view search as an objective
and neutral experience."

00:37:22.500 --> 00:37:26.256
Google explicitly wants people
to think of search this way.

00:37:26.256 --> 00:37:27.880
Because they want it
to be easy to use.

00:37:27.880 --> 00:37:30.960
And you want people to trust the
results that they're getting.

00:37:30.960 --> 00:37:33.520
And that's not
inherently bad, to want

00:37:33.520 --> 00:37:35.861
people to trust the results
that they're getting.

00:37:35.861 --> 00:37:37.360
However, we have
to think about what

00:37:37.360 --> 00:37:39.100
are the ramifications of that?

00:37:39.100 --> 00:37:41.710
How do people actually
interpret that?

00:37:41.710 --> 00:37:43.870
What we have, over
and over again,

00:37:43.870 --> 00:37:47.800
is technology that's not asking
some deep questions about where

00:37:47.800 --> 00:37:51.400
data comes from or what the
history is of a subject.

00:37:51.400 --> 00:37:53.110
And design that is
making those things

00:37:53.110 --> 00:37:56.260
feel neutral and feel factual.

00:37:56.260 --> 00:38:02.314
And so we recreate these toxic
patterns over and over again.

00:38:02.314 --> 00:38:03.980
And I think we talk
a lot about the ways

00:38:03.980 --> 00:38:05.930
that culture informs technology.

00:38:05.930 --> 00:38:07.610
I mean, that's kind
of a given, right?

00:38:07.610 --> 00:38:09.770
It's not as if racism
was created in tech.

00:38:09.770 --> 00:38:11.240
Racism existed.

00:38:11.240 --> 00:38:14.540
And what we have is tech that
can end up reenacting it.

00:38:14.540 --> 00:38:16.370
But what I don't think
we talk enough about

00:38:16.370 --> 00:38:20.000
is the way that tech
also informs culture.

00:38:20.000 --> 00:38:22.530
The work that we do in
technology is powerful.

00:38:22.530 --> 00:38:24.590
It impacts people's
lives in almost every way

00:38:24.590 --> 00:38:25.790
you can imagine.

00:38:25.790 --> 00:38:28.254
It is embedded into almost
everything you can imagine.

00:38:28.254 --> 00:38:30.920
It is often the first thing that
people look at in the morning--

00:38:30.920 --> 00:38:33.200
the last thing they
look at at night.

00:38:33.200 --> 00:38:36.205
And so when tech
perpetuates bias, even

00:38:36.205 --> 00:38:37.730
at those tiny
little levels, even

00:38:37.730 --> 00:38:40.220
at the mini cupcake levels--

00:38:40.220 --> 00:38:42.900
that is a problem.

00:38:42.900 --> 00:38:44.970
Because it's indicative
of an industry that,

00:38:44.970 --> 00:38:47.730
over and over, thinks
narrowly about people,

00:38:47.730 --> 00:38:51.330
assumes that its ideas are
going to work for everyone,

00:38:51.330 --> 00:38:54.720
assumes that the
decisions it makes

00:38:54.720 --> 00:38:58.062
don't carry that much weight,
that they're not that important

00:38:58.062 --> 00:38:59.520
and that it assumes
it knows what's

00:38:59.520 --> 00:39:04.830
best for people without
understanding people very well.

00:39:04.830 --> 00:39:07.590
I think that tech is too
important to our lives--

00:39:07.590 --> 00:39:09.680
to our personal lives,
our social lives,

00:39:09.680 --> 00:39:13.340
our emotional lives,
our political lives--

00:39:13.340 --> 00:39:15.650
to keep toying
with people's lives

00:39:15.650 --> 00:39:17.990
without really
deeply considering

00:39:17.990 --> 00:39:20.790
the consequences of that.

00:39:20.790 --> 00:39:22.600
And so oftentimes,
the examples I've

00:39:22.600 --> 00:39:24.520
shown you today, the
companies behind them

00:39:24.520 --> 00:39:26.680
have treated them as if
they are software bugs.

00:39:26.680 --> 00:39:28.572
You squash it down,
and you move on.

00:39:28.572 --> 00:39:31.030
The problems that we are facing
are not just software bugs.

00:39:31.030 --> 00:39:32.340
What they are, are systems.

00:39:32.340 --> 00:39:36.660
They are systemic patterns, and
so they need systemic action.

00:39:36.660 --> 00:39:40.220
And that is a big
and difficult job.

00:39:40.220 --> 00:39:43.280
And that, actually,
is why I was so upset

00:39:43.280 --> 00:39:45.050
when I read the infamous memo.

00:39:47.850 --> 00:39:52.500
I was pretty upset about the
tenuous grasp of research

00:39:52.500 --> 00:39:53.910
in that memo.

00:39:53.910 --> 00:39:58.470
I was pretty upset about the
way that tiny little differences

00:39:58.470 --> 00:40:00.930
in studies about gender
were extrapolated

00:40:00.930 --> 00:40:04.950
into broad sweeping statements
about women in technology.

00:40:04.950 --> 00:40:08.310
But when I got to the section
where James Damore wrote that

00:40:08.310 --> 00:40:10.800
Google really needed to
"de-emphasize empathy,"

00:40:10.800 --> 00:40:13.620
and said that "being emotionally
unengaged helps us better

00:40:13.620 --> 00:40:16.440
reason about the facts."

00:40:16.440 --> 00:40:20.280
I thought, you know,
you have misunderstood

00:40:20.280 --> 00:40:23.847
the entire project here.

00:40:23.847 --> 00:40:25.680
Because that's actually
the opposite of what

00:40:25.680 --> 00:40:27.900
needs to happen.

00:40:27.900 --> 00:40:30.840
At a time when tech
companies are deciding things

00:40:30.840 --> 00:40:32.550
like what we see
during an election,

00:40:32.550 --> 00:40:34.830
or building software that
determines whether or not

00:40:34.830 --> 00:40:37.830
you're going to get a job or
whether you can get a loan--

00:40:37.830 --> 00:40:39.930
at a time when tech
companies are increasingly

00:40:39.930 --> 00:40:42.469
manipulating relationships
and emotions,

00:40:42.469 --> 00:40:44.760
and the person who designed
the Like button on Facebook

00:40:44.760 --> 00:40:46.380
has actually said
they've rid technology

00:40:46.380 --> 00:40:47.730
from their lives and
their children's lives

00:40:47.730 --> 00:40:50.070
because they've realized
that they actually don't want

00:40:50.070 --> 00:40:53.490
that level of manipulation--

00:40:53.490 --> 00:40:55.466
this is not the answer.

00:40:55.466 --> 00:40:57.090
Because when we start
saying that we're

00:40:57.090 --> 00:40:58.890
going to de-emphasize
empathy, and we're

00:40:58.890 --> 00:41:02.140
going to focus on reason,
facts, we don't ever

00:41:02.140 --> 00:41:05.490
answer some important
questions, like whose job is

00:41:05.490 --> 00:41:08.540
it to decide what "fair" means?

00:41:08.540 --> 00:41:12.180
Do you want some random engineer
to decide what fair means?

00:41:12.180 --> 00:41:14.850
That is a big cultural question.

00:41:14.850 --> 00:41:18.300
And it needs big
discussions around it.

00:41:18.300 --> 00:41:21.570
Whose job is it to understand
historical context?

00:41:21.570 --> 00:41:25.710
Whose job is it to know the
history of race and policing

00:41:25.710 --> 00:41:27.720
in this country before
making decisions

00:41:27.720 --> 00:41:30.190
that could affect people?

00:41:30.190 --> 00:41:32.140
Whose job is it to
sit down and think

00:41:32.140 --> 00:41:34.690
through what the potential
unintended consequences

00:41:34.690 --> 00:41:38.270
of a design decision might be?

00:41:38.270 --> 00:41:42.530
Usually, that's not anybody's
job, but it needs to be.

00:41:42.530 --> 00:41:45.080
And I will say I was really
pleased to read this interview

00:41:45.080 --> 00:41:46.317
with Fei-Fei Li, who--

00:41:46.317 --> 00:41:48.650
is from the Stanford Vision
Lab, and she's on sabbatical

00:41:48.650 --> 00:41:51.800
with Google Cloud right now
working as a chief scientist.

00:41:51.800 --> 00:41:53.990
And she has a tremendous
amount of experience

00:41:53.990 --> 00:41:55.910
with image processing.

00:41:55.910 --> 00:41:57.350
And she's talking about this.

00:41:57.350 --> 00:42:00.200
She saying, you know "AI is
very task-focused" right now.

00:42:00.200 --> 00:42:02.180
"It lacks contextual awareness.

00:42:02.180 --> 00:42:05.785
It lacks the kind of flexible
learning that humans have."

00:42:05.785 --> 00:42:07.160
And so we want to
make technology

00:42:07.160 --> 00:42:10.730
that makes people's lives
better and our world safer.

00:42:10.730 --> 00:42:13.010
And that takes a "layer of
human-level communication

00:42:13.010 --> 00:42:14.810
and collaboration"--

00:42:14.810 --> 00:42:18.920
that that has been
what is missing.

00:42:18.920 --> 00:42:22.684
But it's going to take a
lot of work to get there.

00:42:22.684 --> 00:42:24.100
It's going to take
a lot of people

00:42:24.100 --> 00:42:25.870
changing the way that
they approach problems

00:42:25.870 --> 00:42:27.495
and changing the way
that they approach

00:42:27.495 --> 00:42:31.627
products to be able to
make that kind of shift.

00:42:31.627 --> 00:42:33.460
So what I will leave
with today is something

00:42:33.460 --> 00:42:35.168
I am personally excited
about, but that I

00:42:35.168 --> 00:42:37.760
recognize is difficult when
you work in a tech company.

00:42:37.760 --> 00:42:39.970
And that is increasing
pressure and backlash

00:42:39.970 --> 00:42:43.210
that we are starting
to see in big tech.

00:42:43.210 --> 00:42:47.500
You can see across the board,
from the truly terrible year

00:42:47.500 --> 00:42:51.280
Uber has had, to the
Twitter boycott that

00:42:51.280 --> 00:42:55.420
happened just a few weeks
ago, to the ongoing questions

00:42:55.420 --> 00:42:57.250
around Facebook's
role in the election

00:42:57.250 --> 00:42:59.670
and what ads were sold
to which Russians,

00:42:59.670 --> 00:43:02.790
to whatever happened with
the Google memo here.

00:43:02.790 --> 00:43:05.410
We are seeing this backlash
starting to spring up.

00:43:05.410 --> 00:43:07.630
And it can be difficult,
and it can be uncomfortable.

00:43:07.630 --> 00:43:09.190
But this is exactly
the kind of thing

00:43:09.190 --> 00:43:11.410
that we need to have
happen and that I actually

00:43:11.410 --> 00:43:13.960
hope more people start
pushing back against.

00:43:13.960 --> 00:43:15.970
I hope that this is a
broader conversation that

00:43:15.970 --> 00:43:18.910
makes its way well
outside of tech circles.

00:43:18.910 --> 00:43:20.950
Because as much as I
enjoy coming and talking

00:43:20.950 --> 00:43:23.200
with technical
audiences, I really

00:43:23.200 --> 00:43:25.000
think one of the
most important things

00:43:25.000 --> 00:43:28.480
is that we make technology, and
pushback against technology,

00:43:28.480 --> 00:43:30.250
and the overreach
of tech companies

00:43:30.250 --> 00:43:32.950
accessible to everyday people,
to people who don't necessarily

00:43:32.950 --> 00:43:35.407
have the insider knowledge.

00:43:35.407 --> 00:43:37.240
Because I really think
that it is only then,

00:43:37.240 --> 00:43:40.420
it is only when we take the
concerns of everyday people,

00:43:40.420 --> 00:43:42.520
all people, even
people who don't like

00:43:42.520 --> 00:43:44.950
mini cupcakes into
consideration,

00:43:44.950 --> 00:43:47.650
that we're actually able to make
these kinds of changes possible

00:43:47.650 --> 00:43:49.608
and that we can make a
technology industry that

00:43:49.608 --> 00:43:52.600
is going to be
sustainable for the world

00:43:52.600 --> 00:43:55.470
and for individual
people in the long-run.

00:43:55.470 --> 00:43:57.340
So thank you so much
for having me today.

00:43:57.340 --> 00:43:58.298
I really appreciate it.

00:43:58.298 --> 00:44:00.405
[APPLAUSE]

00:44:04.462 --> 00:44:05.920
SPEAKER: Thank you
very much, Sara.

00:44:05.920 --> 00:44:07.520
Questions?

00:44:07.520 --> 00:44:10.990
AUDIENCE: The cupcake
thing is sort of a little--

00:44:10.990 --> 00:44:11.800
it's frustrating.

00:44:11.800 --> 00:44:13.341
Because you can
imagine that we could

00:44:13.341 --> 00:44:16.240
have taken a different path, and
that would not have happened.

00:44:16.240 --> 00:44:19.450
It's also sort of maybe
easier because you

00:44:19.450 --> 00:44:21.880
can imagine what
that path would be,

00:44:21.880 --> 00:44:24.340
and maybe we could follow
different paths in the future.

00:44:24.340 --> 00:44:28.180
There's something I'd love to
hear your thoughts on that I

00:44:28.180 --> 00:44:33.760
think is, maybe fundamentally,
from my perspective,

00:44:33.760 --> 00:44:36.550
seems to be a harder problem.

00:44:36.550 --> 00:44:38.240
I'm not sure if you
were aware of this--

00:44:38.240 --> 00:44:40.090
this was about a year ago.

00:44:40.090 --> 00:44:42.440
There was an issue
where people noticed

00:44:42.440 --> 00:44:46.690
that if you searched in Google
for unprofessional hairstyles,

00:44:46.690 --> 00:44:48.610
you got pictures that
were overwhelmingly

00:44:48.610 --> 00:44:52.420
of women and people that
were overwhelmingly of color.

00:44:52.420 --> 00:44:55.620
And my simplistic
understanding of this--

00:44:55.620 --> 00:44:58.550
I don't work on that
product, I should say--

00:44:58.550 --> 00:45:00.760
is that that's
actually a reflection

00:45:00.760 --> 00:45:02.680
of how people tag images.

00:45:02.680 --> 00:45:05.380
It is a reflection
of people having

00:45:05.380 --> 00:45:09.190
sites about what to do and not
to do in the workplace that

00:45:09.190 --> 00:45:15.040
use those pictures as
examples of what not to do.

00:45:15.040 --> 00:45:20.710
And so an algorithm that is--

00:45:23.250 --> 00:45:25.810
what is the right
word for this--

00:45:25.810 --> 00:45:29.830
really trying not to be biased,
but the algorithm winds up

00:45:29.830 --> 00:45:33.650
reflecting a larger
bias in society.

00:45:33.650 --> 00:45:37.390
And I think we've seen
a number of instances

00:45:37.390 --> 00:45:40.470
of this kind of problem.

00:45:40.470 --> 00:45:42.880
We had a problem a few
years ago in Google Maps,

00:45:42.880 --> 00:45:46.840
where the White House would get
labeled with a racial epithet

00:45:46.840 --> 00:45:50.320
because a lot of
racist people were

00:45:50.320 --> 00:45:52.900
tagging it that way in Maps.

00:45:52.900 --> 00:45:56.966
And we were using that data.

00:45:56.966 --> 00:45:59.340
And I was just wondering if
you have any thoughts on what

00:45:59.340 --> 00:45:59.775
to do about that one

00:45:59.775 --> 00:46:00.983
SARA WACHTER-BOETTCHER: Yeah.

00:46:00.983 --> 00:46:03.700
So I think that this is
legitimately difficult, right?

00:46:03.700 --> 00:46:04.900
But here's the thing.

00:46:04.900 --> 00:46:07.780
I think that one of the reasons
this is particularly difficult

00:46:07.780 --> 00:46:09.430
is that tech
companies have spent

00:46:09.430 --> 00:46:11.410
a lot of time
focusing on what they

00:46:11.410 --> 00:46:16.600
do as being somehow neutral
and a lot of time saying

00:46:16.600 --> 00:46:18.550
free speech, free speech.

00:46:18.550 --> 00:46:21.580
And so it's kind of
abdicated responsibility.

00:46:21.580 --> 00:46:24.725
And now, we're looking at
it, and going, oh, shit.

00:46:24.725 --> 00:46:25.600
What have we wrought?

00:46:25.600 --> 00:46:27.940
And so I think part of
the reason it's hard

00:46:27.940 --> 00:46:31.200
is that we should have taken
it seriously a long time ago.

00:46:31.200 --> 00:46:32.800
Are you familiar
with Safiya Noble?

00:46:32.800 --> 00:46:34.091
I don't know if any of you are.

00:46:34.091 --> 00:46:36.670
Safiya Noble is an
information studies scholar,

00:46:36.670 --> 00:46:38.860
and she actually talks
a lot about that kind

00:46:38.860 --> 00:46:41.630
of algorithmic bias in
things like search results.

00:46:41.630 --> 00:46:43.880
And so specifically,
things like, if you--

00:46:43.880 --> 00:46:45.880
so unprofessional
hairstyles-- that makes sense.

00:46:45.880 --> 00:46:48.400
Also things like, if you
Google the words "black girls,"

00:46:48.400 --> 00:46:52.185
you will get typically
explicit results.

00:46:52.185 --> 00:46:53.560
And there's lots
and lots of ways

00:46:53.560 --> 00:46:56.440
in which that mirror that's
being reflected back to us

00:46:56.440 --> 00:46:58.360
doesn't look great.

00:46:58.360 --> 00:47:00.760
So I think what you have
to start thinking through,

00:47:00.760 --> 00:47:06.681
though, is, as a company, as
an industry, as a culture,

00:47:06.681 --> 00:47:09.180
what is the role that we want
something like search to take?

00:47:09.180 --> 00:47:10.107
Is it a mirror?

00:47:10.107 --> 00:47:11.940
And then, well, what
do we do about the fact

00:47:11.940 --> 00:47:13.200
that it's never just a mirror.

00:47:13.200 --> 00:47:15.060
It's also a magnifier, right?

00:47:15.060 --> 00:47:16.090
Because that's the fact.

00:47:16.090 --> 00:47:17.790
And that's what we have to
acknowledge and really, deeply

00:47:17.790 --> 00:47:19.560
internalize, is
that it's not just

00:47:19.560 --> 00:47:22.830
that you're reflecting back to
society what people have said.

00:47:22.830 --> 00:47:25.336
But you're making that codified.

00:47:25.336 --> 00:47:27.960
Cathy O'Neil who's the author of
"Weapons of Mass Destruction,"

00:47:27.960 --> 00:47:29.820
she talks a lot
about how big data.

00:47:29.820 --> 00:47:32.580
It doesn't create the
future, it codifies the past.

00:47:32.580 --> 00:47:35.290
So you're basically taking
that historical information,

00:47:35.290 --> 00:47:37.882
and you're making it seem
more objectively true.

00:47:37.882 --> 00:47:39.840
And so I think that that
is really the question

00:47:39.840 --> 00:47:41.220
that you have to be asking.

00:47:41.220 --> 00:47:43.500
Not just, are we comfortable
reflecting this back,

00:47:43.500 --> 00:47:46.050
but are we comfortable
magnifying this,

00:47:46.050 --> 00:47:49.350
and normalizing this,
and codifying this?

00:47:49.350 --> 00:47:51.240
I think that if we
ask those questions,

00:47:51.240 --> 00:47:53.250
I think we will come
out with answers that

00:47:53.250 --> 00:47:56.197
are going to feel more ethically
sound and something that we can

00:47:56.197 --> 00:47:57.780
really stick by and
something that can

00:47:57.780 --> 00:47:59.157
be a better guidance for us.

00:47:59.157 --> 00:48:00.990
I don't think it makes
that discussion easy.

00:48:00.990 --> 00:48:03.031
I think that's still a
really difficult question.

00:48:05.930 --> 00:48:08.550
So then you have to say,
well, where's the line?

00:48:08.550 --> 00:48:11.880
And you're in the
sea of grey area.

00:48:11.880 --> 00:48:14.490
But if you're not--

00:48:14.490 --> 00:48:16.805
and that's exactly why
this is a people business.

00:48:16.805 --> 00:48:18.930
If you're not going to have
that kind of complexity

00:48:18.930 --> 00:48:21.990
of conversation
about what you want

00:48:21.990 --> 00:48:25.530
to see in society, what
we think is appropriate,

00:48:25.530 --> 00:48:27.690
what we think is fair--

00:48:27.690 --> 00:48:29.760
if you're not going to
have those conversations,

00:48:29.760 --> 00:48:33.080
then you should be playing in
the space in the first place.

00:48:33.080 --> 00:48:35.521
Hope that's helpful.

00:48:35.521 --> 00:48:36.600
AUDIENCE: Hi.

00:48:36.600 --> 00:48:38.100
So first of all,
thank you for being

00:48:38.100 --> 00:48:41.790
so direct and strongly-worded,
and raising your [INAUDIBLE]

00:48:41.790 --> 00:48:43.680
problem.

00:48:43.680 --> 00:48:49.050
I normally would not use the
words decided if somebody

00:48:49.050 --> 00:48:51.630
was ignorant to the issue.

00:48:51.630 --> 00:48:55.290
I kind of pulled that word to
mean that it was an informed

00:48:55.290 --> 00:48:56.340
decision of some kind.

00:48:56.340 --> 00:48:58.930
So they knew they
were at a junction.

00:48:58.930 --> 00:49:01.440
This definitely is some form
of neglect or malpractice.

00:49:01.440 --> 00:49:04.890
And when I took
professional ethics,

00:49:04.890 --> 00:49:06.734
I was told that
what distinguished

00:49:06.734 --> 00:49:09.150
professional ethics from the
rest of other kinds of ethics

00:49:09.150 --> 00:49:11.460
is that, if you're
in a profession,

00:49:11.460 --> 00:49:15.400
your client doesn't know enough
to double check your work.

00:49:15.400 --> 00:49:19.390
If you're an architect or a
lawyer, almost by definition,

00:49:19.390 --> 00:49:21.660
your client just has to
trust you to do it right.

00:49:21.660 --> 00:49:25.740
And I think it's appropriate
that these companies should

00:49:25.740 --> 00:49:29.520
know these pitfalls and should
be systemizing solutions

00:49:29.520 --> 00:49:30.450
to them.

00:49:30.450 --> 00:49:32.544
But I don't know
how much you know

00:49:32.544 --> 00:49:34.710
about the internal structure
of different companies,

00:49:34.710 --> 00:49:38.160
but where exactly would you
pin that responsibility?

00:49:38.160 --> 00:49:40.200
Is that engineering malpractice?

00:49:40.200 --> 00:49:42.210
Is that design malpractice?

00:49:42.210 --> 00:49:45.690
Is that legal malpractice?

00:49:45.690 --> 00:49:48.420
Who should be the
professional that

00:49:48.420 --> 00:49:50.250
is aware of all the
implications here?

00:49:50.250 --> 00:49:53.280
SARA WACHTER-BOETTCHER: Well,
So I think, realistically, we're

00:49:53.280 --> 00:49:56.099
never going to be aware of
every potential implication,

00:49:56.099 --> 00:49:57.390
every potential outcome, right?

00:49:57.390 --> 00:49:59.350
That is not realistic.

00:49:59.350 --> 00:50:01.080
I think that it is
incumbent upon people

00:50:01.080 --> 00:50:03.640
in every discipline,
though, to be more aware.

00:50:03.640 --> 00:50:06.630
And I also think that it is
incumbent upon tech companies

00:50:06.630 --> 00:50:09.150
to look at what they're doing--

00:50:09.150 --> 00:50:12.630
when you acknowledge that what
you're doing is not just tech,

00:50:12.630 --> 00:50:14.250
then you realize
that you actually

00:50:14.250 --> 00:50:16.650
need expertise in these areas.

00:50:16.650 --> 00:50:20.130
That, of course, you're
going to do a bad job seeing

00:50:20.130 --> 00:50:23.070
some of this stuff
because you don't actually

00:50:23.070 --> 00:50:25.620
have the background to see it.

00:50:25.620 --> 00:50:27.330
You should probably
bring somebody

00:50:27.330 --> 00:50:31.590
in who has a deep knowledge
of history and race

00:50:31.590 --> 00:50:37.200
in this country to have any
influence over any product that

00:50:37.200 --> 00:50:39.210
could impact people
of different races--

00:50:39.210 --> 00:50:42.840
which is probably
all of your products.

00:50:42.840 --> 00:50:45.060
Somebody has to actually
know something in order

00:50:45.060 --> 00:50:46.420
to find these problems.

00:50:46.420 --> 00:50:48.220
And if you're not
hiring for that,

00:50:48.220 --> 00:50:50.220
then you're going to
continue having these gaps.

00:50:50.220 --> 00:50:53.310
I do think that everybody can do
better across every discipline.

00:50:53.310 --> 00:50:56.232
I do think it is the
responsibility of people

00:50:56.232 --> 00:50:57.690
in design, it is
the responsibility

00:50:57.690 --> 00:51:00.189
of people in development, it
is the responsibility of people

00:51:00.189 --> 00:51:01.826
in legal, it's everywhere.

00:51:01.826 --> 00:51:04.200
But the thing is, if you're
going to say it's everybody's

00:51:04.200 --> 00:51:05.910
responsibility, it's
very easy for that

00:51:05.910 --> 00:51:07.830
to become nobody's job, right?

00:51:07.830 --> 00:51:09.900
And so I think that,
fundamentally though,

00:51:09.900 --> 00:51:12.000
that's because
it's not just about

00:51:12.000 --> 00:51:14.430
the individual culpability.

00:51:14.430 --> 00:51:18.540
It is about priorities
as an organization.

00:51:18.540 --> 00:51:20.727
If this is a priority
for your organization,

00:51:20.727 --> 00:51:23.310
then it's going to become part
of your organizational culture.

00:51:23.310 --> 00:51:24.893
It is going to be
systematized, and it

00:51:24.893 --> 00:51:27.690
is going to be present in
every step along the way.

00:51:27.690 --> 00:51:30.450
It isn't going to be done
in this sort of ad hoc way

00:51:30.450 --> 00:51:33.030
where, you, individual
engineer, needed

00:51:33.030 --> 00:51:34.500
to have noticed this thing.

00:51:34.500 --> 00:51:36.390
I think assigning blame
to individual people

00:51:36.390 --> 00:51:38.140
is actually not very helpful.

00:51:38.140 --> 00:51:41.530
I think it's much more helpful
to say why does this happen?

00:51:41.530 --> 00:51:44.280
That's why I don't really
care about the bug fix.

00:51:44.280 --> 00:51:46.500
I care about what
kinds of patterns

00:51:46.500 --> 00:51:48.270
are emerging over
and over again?

00:51:48.270 --> 00:51:51.270
And how does the way
that you are organized--

00:51:51.270 --> 00:51:54.280
what does a project
genesis look like,

00:51:54.280 --> 00:51:56.790
and how does that get
translated into a team that's

00:51:56.790 --> 00:51:57.960
working on it?

00:51:57.960 --> 00:52:00.750
Who is saying yea
or nay to decisions?

00:52:00.750 --> 00:52:02.440
Who thought it was
a terrible idea,

00:52:02.440 --> 00:52:06.120
but was uncomfortable speaking
up, and why is that happening?

00:52:06.120 --> 00:52:08.400
Did you have enough
diverse people in the room?

00:52:08.400 --> 00:52:10.500
Were there a bunch of
women in the room who

00:52:10.500 --> 00:52:14.760
decided the mini cupcakes
should be there or not?

00:52:14.760 --> 00:52:17.580
And you could say that
about many, many groups.

00:52:17.580 --> 00:52:20.862
But I think that you have to
look at it at that macro level.

00:52:20.862 --> 00:52:22.320
Because I think
when we try to talk

00:52:22.320 --> 00:52:24.780
about it at that individual
level, then, of course,

00:52:24.780 --> 00:52:25.680
people miss stuff.

00:52:25.680 --> 00:52:26.959
Everybody misses stuff.

00:52:26.959 --> 00:52:29.250
I think we have to look at
the pattern of missing stuff

00:52:29.250 --> 00:52:33.495
and say how do we tackle
that organizationally?

00:52:33.495 --> 00:52:36.390
AUDIENCE: You were talking a lot
about how these sorts of checks

00:52:36.390 --> 00:52:38.694
need to be systematized.

00:52:38.694 --> 00:52:40.110
And I'm just
wondering if you know

00:52:40.110 --> 00:52:42.750
of any examples of
organizations which

00:52:42.750 --> 00:52:46.110
have decision-making
structures which support this.

00:52:46.110 --> 00:52:50.310
How can we foster an environment
where we are in a meeting,

00:52:50.310 --> 00:52:53.880
and the person that thinks
maybe this cupcake idea is bad,

00:52:53.880 --> 00:52:55.500
feels empowered to speak up?

00:52:55.500 --> 00:52:58.814
What sorts of systemic things
can we do to be better at that?

00:52:58.814 --> 00:53:00.480
SARA WACHTER-BOETTCHER:
So this is hard.

00:53:00.480 --> 00:53:03.150
I don't have any examples
off the top of my head,

00:53:03.150 --> 00:53:05.910
where I'm like, just look at
what this corporation does.

00:53:05.910 --> 00:53:07.920
Because I think that
this is not something

00:53:07.920 --> 00:53:10.870
that most companies
are doing that well at.

00:53:10.870 --> 00:53:13.740
But I would say that
part of it is, OK,

00:53:13.740 --> 00:53:16.785
so you do need to have a
diverse team working on things.

00:53:16.785 --> 00:53:18.660
But you also have to
have a diverse team that

00:53:18.660 --> 00:53:21.130
feels like it can speak up.

00:53:21.130 --> 00:53:22.950
And what that means
is that you have

00:53:22.950 --> 00:53:25.936
to have people at different
levels who are diverse.

00:53:25.936 --> 00:53:27.060
I mean, this is the thing--

00:53:27.060 --> 00:53:28.280
if you want to fix
this problem, you've

00:53:28.280 --> 00:53:29.946
got to go back to
root cause, and that's

00:53:29.946 --> 00:53:31.350
a really hard problem.

00:53:31.350 --> 00:53:35.140
But you have to have
people who aren't just

00:53:35.140 --> 00:53:38.550
at a junior level who might
have different perspectives.

00:53:38.550 --> 00:53:40.830
So if you have all of the
senior people in the room--

00:53:40.830 --> 00:53:44.950
So for example, the
other day, the chief

00:53:44.950 --> 00:53:47.314
of diversity inclusion at
Apple had this comment--

00:53:47.314 --> 00:53:48.730
it was kind of an
offhand comment,

00:53:48.730 --> 00:53:49.990
and it totally blew
up-- where she said,

00:53:49.990 --> 00:53:51.520
something like,
well, you know, you

00:53:51.520 --> 00:53:53.770
could have 12 blond-haired,
blue-eyed men in the room,

00:53:53.770 --> 00:53:56.350
and that would have its
own kind of diversity.

00:53:56.350 --> 00:54:00.490
And what that indicates to me,
in a company like Apple that

00:54:00.490 --> 00:54:02.950
is actively trying to
recruit more diversely,

00:54:02.950 --> 00:54:04.720
is that they still
have this perception

00:54:04.720 --> 00:54:09.400
that you can have that group
of people come into the room,

00:54:09.400 --> 00:54:12.430
and that that's still OK.

00:54:12.430 --> 00:54:13.180
That's not OK.

00:54:13.180 --> 00:54:15.250
Because I'm going to tell
you, those 12 people--

00:54:15.250 --> 00:54:17.530
they are going to
miss a lot of stuff.

00:54:17.530 --> 00:54:20.740
So you have to,
both, acknowledge

00:54:20.740 --> 00:54:22.330
that you need that
diversity there,

00:54:22.330 --> 00:54:26.469
and that everybody there has to
be able to speak up to those 12

00:54:26.469 --> 00:54:28.510
blond-haired, blue-eyed
white men in the room who

00:54:28.510 --> 00:54:30.700
all come from the same
background in order for it

00:54:30.700 --> 00:54:33.160
to work.

00:54:33.160 --> 00:54:36.770
Anyway, I think that
is a key piece of it.

00:54:36.770 --> 00:54:43.390
And then it's also--
we have processes,

00:54:43.390 --> 00:54:46.690
we have systems that we
use for all kinds of stuff.

00:54:46.690 --> 00:54:49.330
Why is there no
system in place that's

00:54:49.330 --> 00:54:52.240
checks and balances
on assumptions?

00:54:52.240 --> 00:54:54.310
Has anybody ever sat
down and documented

00:54:54.310 --> 00:54:57.310
all of the technical assumptions
that they are making?

00:54:57.310 --> 00:54:58.709
That's pretty common.

00:54:58.709 --> 00:55:00.250
Have you ever sat
down and documented

00:55:00.250 --> 00:55:04.060
all of the social assumptions
that you're making

00:55:04.060 --> 00:55:06.020
or human assumptions
that you're making?

00:55:06.020 --> 00:55:08.760
I have very rarely seen a team
actually sit down and do that,

00:55:08.760 --> 00:55:10.694
and then think about
well, OK, what's

00:55:10.694 --> 00:55:11.860
the worst that could happen?

00:55:11.860 --> 00:55:15.190
How could this go totally
wrong, and how are we

00:55:15.190 --> 00:55:16.570
going to ward against that?

00:55:16.570 --> 00:55:18.445
I just don't think that
they're being trained

00:55:18.445 --> 00:55:21.010
to do that kind of thing.

00:55:21.010 --> 00:55:23.560
So yeah, I think that you have
to go pretty deep to start

00:55:23.560 --> 00:55:24.820
solving the problem.

00:55:24.820 --> 00:55:29.770
It's not going to
be fixed by just,

00:55:29.770 --> 00:55:32.010
let's hire some more junior
people in diverse roles

00:55:32.010 --> 00:55:35.710
and will not be fixed by just
kind of encouraging people

00:55:35.710 --> 00:55:36.340
to speak up.

00:55:36.340 --> 00:55:40.510
It will be fixed by big,
deep changes in the way

00:55:40.510 --> 00:55:42.910
that we structure teams,
the way that we promote,

00:55:42.910 --> 00:55:45.254
the way that we
operate as companies.

00:55:45.254 --> 00:55:47.170
Which makes it really
tough, makes it painful,

00:55:47.170 --> 00:55:50.560
makes it feel like it's
hard to get anywhere.

00:55:50.560 --> 00:55:53.140
But I think that's
really the only way.

00:55:53.140 --> 00:55:54.940
SPEAKER: Let's thank Sara again.

00:55:54.940 --> 00:55:57.690
[APPLAUSE]

