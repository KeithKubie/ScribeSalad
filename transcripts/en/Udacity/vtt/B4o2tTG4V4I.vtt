WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:01.680
So I was thinking of talking to you more

00:00:01.680 --> 00:00:04.720
about sampling, but it seems like it might work out

00:00:04.720 --> 00:00:07.480
best to just have some hands on experience with it

00:00:07.480 --> 00:00:09.730
so we're going to put those things on the homework. So

00:00:09.730 --> 00:00:11.960
given that we're actually in a position now to, to

00:00:11.960 --> 00:00:14.530
kind of wrap up the whole Bayes net inference piece

00:00:14.530 --> 00:00:16.500
that we were talking about. So do you want to help

00:00:16.500 --> 00:00:19.260
remind me, Charles, what were the things that we covered?

00:00:19.260 --> 00:00:24.105
&gt;&gt; Sure, I can help you with that. We covered Bayesian Inference [LAUGH]

00:00:24.105 --> 00:00:26.210
I'm sorry.

00:00:26.210 --> 00:00:27.660
I'm punch drunk.

00:00:27.660 --> 00:00:30.780
&gt;&gt; I'm going to choose not to pay attention to that. Instead, write

00:00:30.780 --> 00:00:32.280
Bayesian Networks. We talked about the

00:00:32.280 --> 00:00:35.880
Bayesian Network representation of joint probability distributions.

00:00:35.880 --> 00:00:42.900
&gt;&gt; Right. We did a lot of examples of how to actually do inference with networks.

00:00:42.900 --> 00:00:44.150
You know, exactly how do we, do we

00:00:44.150 --> 00:00:48.080
compute probabilities of particular values. We mentioned sampling.

00:00:48.080 --> 00:00:48.700
&gt;&gt; That's right.

00:00:48.700 --> 00:00:51.630
&gt;&gt; And then we did a Naive Bayes.

00:00:51.630 --> 00:00:54.850
&gt;&gt; Well first we did say that, that in general it's hard

00:00:54.850 --> 00:00:58.155
to do exact inference. It's actually hard to do even approximate inference.

00:00:58.155 --> 00:00:59.193
&gt;&gt; Mm-hm.

00:00:59.193 --> 00:01:00.430
&gt;&gt; But we talked about a special

00:01:00.430 --> 00:01:03.150
case of bayesian networks, that was called naive

00:01:03.150 --> 00:01:05.379
bayes with the naive part being, that we're

00:01:05.379 --> 00:01:08.630
assuming that attributes are independent of one another.

00:01:08.630 --> 00:01:10.710
&gt;&gt; Condition on the label.

00:01:10.710 --> 00:01:12.900
&gt;&gt; Right. And this was actually helping us make a

00:01:12.900 --> 00:01:16.700
link between all this bayesian stuff. The bayesian rabbit hole we

00:01:16.700 --> 00:01:21.350
went down. And classification, which is the core machine learning

00:01:21.350 --> 00:01:23.370
topic that we've been spending a lot of time on.

00:01:23.370 --> 00:01:24.350
&gt;&gt; So the other thing that I really

00:01:24.350 --> 00:01:27.160
liked about this notion, this link to classification, Michael,

00:01:27.160 --> 00:01:29.210
is that when I was talking about Bayesian

00:01:29.210 --> 00:01:31.730
learning, what we ended up with at the end

00:01:31.730 --> 00:01:36.360
is this nice idea that we had a gold standard, right? We had a sort of way

00:01:36.360 --> 00:01:39.260
of talking about what the right hypothesis was

00:01:39.260 --> 00:01:41.790
and, ultimately, what the right classification was by computing

00:01:41.790 --> 00:01:45.120
these probabilities. And sometimes, we couldn't do it because, typically,

00:01:45.120 --> 00:01:48.880
you can't actually do the for loop that requires you compute

00:01:48.880 --> 00:01:53.290
conditional probabilities of hypothesis given data. Over say an infinite number

00:01:53.290 --> 00:01:55.110
of hypothesis, but at least we kind of knew what the

00:01:55.110 --> 00:01:57.580
right thing was and we made right assumptions we could do

00:01:57.580 --> 00:02:01.190
things like derive, oh I don't know, a sum of squared

00:02:01.190 --> 00:02:04.120
errors or various other things that you might do and that

00:02:04.120 --> 00:02:06.790
was all very cool. But what you've done here when you

00:02:06.790 --> 00:02:10.300
do inference. Is at least with a Naive Bayes case,

00:02:10.300 --> 00:02:12.460
you've shown us a way that we can do classification

00:02:12.460 --> 00:02:15.090
using these things, that actually is tractable, and is the

00:02:15.090 --> 00:02:17.480
right thing to do under certain assumptions. I really like

00:02:17.480 --> 00:02:19.790
that. And the other thing that I think is worth

00:02:19.790 --> 00:02:22.930
mentioning is that not only does it link this Bayesian

00:02:22.930 --> 00:02:27.950
learning to classification. But it connects classification back to this

00:02:27.950 --> 00:02:31.810
general notion of Bayesian learning, Bayesian inference where, you don't

00:02:31.810 --> 00:02:36.220
have to worry about just figuring out the most likely label

00:02:36.220 --> 00:02:39.100
given a bunch of attributes. But because it's a Bayes network and

00:02:39.100 --> 00:02:41.720
you can compute anything from it, you could try to ask

00:02:41.720 --> 00:02:44.560
well what's the likelihood that I see some particular attribute or set

00:02:44.560 --> 00:02:48.050
of attributes, given a label or given a subset of attributes

00:02:48.050 --> 00:02:50.740
on all those kind of things that you could do. With the

00:02:50.740 --> 00:02:53.680
Bayesian learning. So inference gives us this power to not just

00:02:53.680 --> 00:02:57.110
do classification, but to do a larger set of things beyond classification.

00:02:57.110 --> 00:02:57.820
I think that's kind of cool.

00:02:57.820 --> 00:03:00.610
&gt;&gt; Cool. Yeah, well said. The, the For, and another thing,

00:03:00.610 --> 00:03:03.080
kind of in that same space is that it handles missing attributes

00:03:03.080 --> 00:03:07.260
really well. So whereas things like, oh. You know, decision trees

00:03:07.260 --> 00:03:09.610
and so forth, if you give me an example that doesn't have

00:03:09.610 --> 00:03:12.220
one of the attribute values and you've hit that part of

00:03:12.220 --> 00:03:14.830
the decision tree where you need to know that attribute value you're

00:03:14.830 --> 00:03:18.650
stuck. Whereas in this naive base setting, you can still do the

00:03:18.650 --> 00:03:20.840
probabalistic inference over the missing attributes

00:03:20.840 --> 00:03:22.190
because all the things are linked

00:03:22.190 --> 00:03:23.630
by probabilities.

00:03:23.630 --> 00:03:24.020
&gt;&gt; Nice.

00:03:24.020 --> 00:03:25.900
&gt;&gt; All right. So I think, you know, you'll, you'll get

00:03:25.900 --> 00:03:28.570
a much stronger handle of this when you go through the,

00:03:28.570 --> 00:03:32.670
the homework problems. But I think that's enough for Bayesian inference.

00:03:32.670 --> 00:03:36.490
And I think that actually wraps up classification and regression more generally.

00:03:36.490 --> 00:03:40.460
&gt;&gt; Right. So we're done with supervised learning. Well, one's never done with

00:03:40.460 --> 00:03:42.970
supervised learning. But we're at least done with this part of the course.

00:03:42.970 --> 00:03:44.850
&gt;&gt; Because there's always more to supervise learn.

00:03:44.850 --> 00:03:47.270
&gt;&gt; That's right. And in particular you'll get

00:03:47.270 --> 00:03:50.120
a nice example of this, because you'll be taking an exam.

00:03:50.120 --> 00:03:51.369
&gt;&gt; [LAUGH]

00:03:51.369 --> 00:03:54.750
&gt;&gt; And your input will be the exam, and then we'll give you a label back.

00:03:54.750 --> 00:03:57.120
&gt;&gt; [LAUGH] I guess that's one way to think about it.

00:03:57.120 --> 00:03:58.770
&gt;&gt; Well and then they'll get to generalize beyond

00:03:58.770 --> 00:04:00.200
that for the next time they take the exam.

00:04:00.200 --> 00:04:01.997
&gt;&gt; Very good! All right. Well, well thanks

00:04:01.997 --> 00:04:03.860
very much, this has been fun. Thanks Charles.

00:04:03.860 --> 00:04:06.810
&gt;&gt; This has been fun. I will see you in the second mini course.

00:04:06.810 --> 00:04:07.510
&gt;&gt; All right.

00:04:07.510 --> 00:04:08.840
&gt;&gt; Bye.

