WEBVTT
Kind: captions
Language: en

00:00:00.089 --> 00:00:01.969
 
in the previous video you saw how you

00:00:01.969 --> 00:00:01.979
in the previous video you saw how you
 

00:00:01.979 --> 00:00:04.370
in the previous video you saw how you
can use mini-batch gradient descent to

00:00:04.370 --> 00:00:04.380
can use mini-batch gradient descent to
 

00:00:04.380 --> 00:00:06.260
can use mini-batch gradient descent to
start making progress to start taking

00:00:06.260 --> 00:00:06.270
start making progress to start taking
 

00:00:06.270 --> 00:00:08.240
start making progress to start taking
gradient descent steps even when you're

00:00:08.240 --> 00:00:08.250
gradient descent steps even when you're
 

00:00:08.250 --> 00:00:09.799
gradient descent steps even when you're
just partway through processing your

00:00:09.799 --> 00:00:09.809
just partway through processing your
 

00:00:09.809 --> 00:00:11.600
just partway through processing your
training set even for the first time in

00:00:11.600 --> 00:00:11.610
training set even for the first time in
 

00:00:11.610 --> 00:00:14.240
training set even for the first time in
this video you learn more details of how

00:00:14.240 --> 00:00:14.250
this video you learn more details of how
 

00:00:14.250 --> 00:00:16.340
this video you learn more details of how
to implement gradient descent and gain a

00:00:16.340 --> 00:00:16.350
to implement gradient descent and gain a
 

00:00:16.350 --> 00:00:18.470
to implement gradient descent and gain a
better understanding of what is doing

00:00:18.470 --> 00:00:18.480
better understanding of what is doing
 

00:00:18.480 --> 00:00:20.630
better understanding of what is doing
and why it works with batch gradient

00:00:20.630 --> 00:00:20.640
and why it works with batch gradient
 

00:00:20.640 --> 00:00:23.090
and why it works with batch gradient
descent on every iteration you go

00:00:23.090 --> 00:00:23.100
descent on every iteration you go
 

00:00:23.100 --> 00:00:25.220
descent on every iteration you go
through the entire training set and you

00:00:25.220 --> 00:00:25.230
through the entire training set and you
 

00:00:25.230 --> 00:00:27.769
through the entire training set and you
would expect the costs to go down on

00:00:27.769 --> 00:00:27.779
would expect the costs to go down on
 

00:00:27.779 --> 00:00:31.130
would expect the costs to go down on
every single iteration so if we plot the

00:00:31.130 --> 00:00:31.140
every single iteration so if we plot the
 

00:00:31.140 --> 00:00:33.709
every single iteration so if we plot the
cost function J as a function of

00:00:33.709 --> 00:00:33.719
cost function J as a function of
 

00:00:33.719 --> 00:00:35.479
cost function J as a function of
different iterations it should decrease

00:00:35.479 --> 00:00:35.489
different iterations it should decrease
 

00:00:35.489 --> 00:00:37.880
different iterations it should decrease
on every single iteration and if it ever

00:00:37.880 --> 00:00:37.890
on every single iteration and if it ever
 

00:00:37.890 --> 00:00:39.860
on every single iteration and if it ever
goes up even on one iteration then

00:00:39.860 --> 00:00:39.870
goes up even on one iteration then
 

00:00:39.870 --> 00:00:41.209
goes up even on one iteration then
something's wrong maybe the learning

00:00:41.209 --> 00:00:41.219
something's wrong maybe the learning
 

00:00:41.219 --> 00:00:43.940
something's wrong maybe the learning
rates too big on mini-batch gradient

00:00:43.940 --> 00:00:43.950
rates too big on mini-batch gradient
 

00:00:43.950 --> 00:00:46.639
rates too big on mini-batch gradient
descent though if you plot progress in

00:00:46.639 --> 00:00:46.649
descent though if you plot progress in
 

00:00:46.649 --> 00:00:48.979
descent though if you plot progress in
your cost function then it may not

00:00:48.979 --> 00:00:48.989
your cost function then it may not
 

00:00:48.989 --> 00:00:51.500
your cost function then it may not
decrease on every iteration in

00:00:51.500 --> 00:00:51.510
decrease on every iteration in
 

00:00:51.510 --> 00:00:54.229
decrease on every iteration in
particular on every iteration you're

00:00:54.229 --> 00:00:54.239
particular on every iteration you're
 

00:00:54.239 --> 00:01:01.340
particular on every iteration you're
processing some X T YT and so if you

00:01:01.340 --> 00:01:01.350
processing some X T YT and so if you
 

00:01:01.350 --> 00:01:06.530
processing some X T YT and so if you
plot the cost function J T which is

00:01:06.530 --> 00:01:06.540
plot the cost function J T which is
 

00:01:06.540 --> 00:01:12.590
plot the cost function J T which is
computed using just X T YT then it's as

00:01:12.590 --> 00:01:12.600
computed using just X T YT then it's as
 

00:01:12.600 --> 00:01:15.289
computed using just X T YT then it's as
if on every iteration you're training on

00:01:15.289 --> 00:01:15.299
if on every iteration you're training on
 

00:01:15.299 --> 00:01:17.240
if on every iteration you're training on
a different training cycle really

00:01:17.240 --> 00:01:17.250
a different training cycle really
 

00:01:17.250 --> 00:01:19.399
a different training cycle really
trading on a different meaning batch so

00:01:19.399 --> 00:01:19.409
trading on a different meaning batch so
 

00:01:19.409 --> 00:01:21.170
trading on a different meaning batch so
you plot the cost function J you're more

00:01:21.170 --> 00:01:21.180
you plot the cost function J you're more
 

00:01:21.180 --> 00:01:22.760
you plot the cost function J you're more
likely to see something that looks like

00:01:22.760 --> 00:01:22.770
likely to see something that looks like
 

00:01:22.770 --> 00:01:23.210
likely to see something that looks like
this

00:01:23.210 --> 00:01:23.220
this
 

00:01:23.220 --> 00:01:26.179
this
it should trend downwards but it is also

00:01:26.179 --> 00:01:26.189
it should trend downwards but it is also
 

00:01:26.189 --> 00:01:31.140
it should trend downwards but it is also
going to be a little bit noisier

00:01:31.140 --> 00:01:31.150
 
 

00:01:31.150 --> 00:01:34.920
 
you plot J of T has your training

00:01:34.920 --> 00:01:34.930
you plot J of T has your training
 

00:01:34.930 --> 00:01:36.300
you plot J of T has your training
mini-batch gradient descent it may be

00:01:36.300 --> 00:01:36.310
mini-batch gradient descent it may be
 

00:01:36.310 --> 00:01:39.360
mini-batch gradient descent it may be
over multiple epochs you might expect to

00:01:39.360 --> 00:01:39.370
over multiple epochs you might expect to
 

00:01:39.370 --> 00:01:41.400
over multiple epochs you might expect to
see a curve like this so as okay if it

00:01:41.400 --> 00:01:41.410
see a curve like this so as okay if it
 

00:01:41.410 --> 00:01:44.250
see a curve like this so as okay if it
doesn't go down on every iteration but

00:01:44.250 --> 00:01:44.260
doesn't go down on every iteration but
 

00:01:44.260 --> 00:01:46.800
doesn't go down on every iteration but
it should trend downwards and the reason

00:01:46.800 --> 00:01:46.810
it should trend downwards and the reason
 

00:01:46.810 --> 00:01:48.480
it should trend downwards and the reason
it'll be a little bit noisy is that

00:01:48.480 --> 00:01:48.490
it'll be a little bit noisy is that
 

00:01:48.490 --> 00:01:53.940
it'll be a little bit noisy is that
maybe x1 y1 it's just a relatively easy

00:01:53.940 --> 00:01:53.950
maybe x1 y1 it's just a relatively easy
 

00:01:53.950 --> 00:01:56.010
maybe x1 y1 it's just a relatively easy
meaning batch so your cost might be a

00:01:56.010 --> 00:01:56.020
meaning batch so your cost might be a
 

00:01:56.020 --> 00:01:58.310
meaning batch so your cost might be a
bit lower but then maybe just by chance

00:01:58.310 --> 00:01:58.320
bit lower but then maybe just by chance
 

00:01:58.320 --> 00:02:02.100
bit lower but then maybe just by chance
x2 y2 is just a harder mini batch maybe

00:02:02.100 --> 00:02:02.110
x2 y2 is just a harder mini batch maybe
 

00:02:02.110 --> 00:02:04.230
x2 y2 is just a harder mini batch maybe
even some let's label examples in it in

00:02:04.230 --> 00:02:04.240
even some let's label examples in it in
 

00:02:04.240 --> 00:02:05.370
even some let's label examples in it in
which case the cost would be a bit

00:02:05.370 --> 00:02:05.380
which case the cost would be a bit
 

00:02:05.380 --> 00:02:07.620
which case the cost would be a bit
higher and so on so that's why you get

00:02:07.620 --> 00:02:07.630
higher and so on so that's why you get
 

00:02:07.630 --> 00:02:10.380
higher and so on so that's why you get
these oscillations as you plot the cost

00:02:10.380 --> 00:02:10.390
these oscillations as you plot the cost
 

00:02:10.390 --> 00:02:12.210
these oscillations as you plot the cost
when you're running mini batch gradient

00:02:12.210 --> 00:02:12.220
when you're running mini batch gradient
 

00:02:12.220 --> 00:02:14.580
when you're running mini batch gradient
descent now one of the parameters you

00:02:14.580 --> 00:02:14.590
descent now one of the parameters you
 

00:02:14.590 --> 00:02:16.770
descent now one of the parameters you
need to choose is the size of your mini

00:02:16.770 --> 00:02:16.780
need to choose is the size of your mini
 

00:02:16.780 --> 00:02:20.520
need to choose is the size of your mini
batch so M was the training set size on

00:02:20.520 --> 00:02:20.530
batch so M was the training set size on
 

00:02:20.530 --> 00:02:25.949
batch so M was the training set size on
one extreme if the mini batch size is

00:02:25.949 --> 00:02:25.959
one extreme if the mini batch size is
 

00:02:25.959 --> 00:02:29.250
one extreme if the mini batch size is
equal to M then you just end up with

00:02:29.250 --> 00:02:29.260
equal to M then you just end up with
 

00:02:29.260 --> 00:02:36.090
equal to M then you just end up with
bosch gradient descent alright so in

00:02:36.090 --> 00:02:36.100
bosch gradient descent alright so in
 

00:02:36.100 --> 00:02:37.830
bosch gradient descent alright so in
this extreme you would just have one

00:02:37.830 --> 00:02:37.840
this extreme you would just have one
 

00:02:37.840 --> 00:02:42.270
this extreme you would just have one
mini batch x1 y1 and this mini batch is

00:02:42.270 --> 00:02:42.280
mini batch x1 y1 and this mini batch is
 

00:02:42.280 --> 00:02:45.720
mini batch x1 y1 and this mini batch is
equal to your entire training set so

00:02:45.720 --> 00:02:45.730
equal to your entire training set so
 

00:02:45.730 --> 00:02:47.759
equal to your entire training set so
setting the movie batch size M just

00:02:47.759 --> 00:02:47.769
setting the movie batch size M just
 

00:02:47.769 --> 00:02:49.830
setting the movie batch size M just
gives you back gradient descent the

00:02:49.830 --> 00:02:49.840
gives you back gradient descent the
 

00:02:49.840 --> 00:02:51.930
gives you back gradient descent the
other extreme would be if your mini

00:02:51.930 --> 00:02:51.940
other extreme would be if your mini
 

00:02:51.940 --> 00:03:00.300
other extreme would be if your mini
batch size were equal to 1 this gives

00:03:00.300 --> 00:03:00.310
batch size were equal to 1 this gives
 

00:03:00.310 --> 00:03:02.240
batch size were equal to 1 this gives
you an algorithm called stochastic

00:03:02.240 --> 00:03:02.250
you an algorithm called stochastic
 

00:03:02.250 --> 00:03:09.050
you an algorithm called stochastic
gradient descent and here every example

00:03:09.050 --> 00:03:09.060
gradient descent and here every example
 

00:03:09.060 --> 00:03:19.170
gradient descent and here every example
is his own mini batch so what

00:03:19.170 --> 00:03:19.180
is his own mini batch so what
 

00:03:19.180 --> 00:03:21.479
is his own mini batch so what
in this case as you look at you know the

00:03:21.479 --> 00:03:21.489
in this case as you look at you know the
 

00:03:21.489 --> 00:03:27.089
in this case as you look at you know the
first mini batch so x1 y1 but when you

00:03:27.089 --> 00:03:27.099
first mini batch so x1 y1 but when you
 

00:03:27.099 --> 00:03:29.640
first mini batch so x1 y1 but when you
meanie batch sizes 1 this just has you

00:03:29.640 --> 00:03:29.650
meanie batch sizes 1 this just has you
 

00:03:29.650 --> 00:03:31.349
meanie batch sizes 1 this just has you
know your first training example and you

00:03:31.349 --> 00:03:31.359
know your first training example and you
 

00:03:31.359 --> 00:03:33.089
know your first training example and you
take your it into sense that with your

00:03:33.089 --> 00:03:33.099
take your it into sense that with your
 

00:03:33.099 --> 00:03:35.910
take your it into sense that with your
first training example and then you mix

00:03:35.910 --> 00:03:35.920
first training example and then you mix
 

00:03:35.920 --> 00:03:38.429
first training example and then you mix
take a look at your second mini batch

00:03:38.429 --> 00:03:38.439
take a look at your second mini batch
 

00:03:38.439 --> 00:03:40.259
take a look at your second mini batch
which is just your second training

00:03:40.259 --> 00:03:40.269
which is just your second training
 

00:03:40.269 --> 00:03:42.179
which is just your second training
example and take your grandest and step

00:03:42.179 --> 00:03:42.189
example and take your grandest and step
 

00:03:42.189 --> 00:03:44.099
example and take your grandest and step
with that and then you do with the third

00:03:44.099 --> 00:03:44.109
with that and then you do with the third
 

00:03:44.109 --> 00:03:45.869
with that and then you do with the third
training example and so on looking at

00:03:45.869 --> 00:03:45.879
training example and so on looking at
 

00:03:45.879 --> 00:03:47.580
training example and so on looking at
just one single training example at a

00:03:47.580 --> 00:03:47.590
just one single training example at a
 

00:03:47.590 --> 00:03:52.440
just one single training example at a
time so let's look at what these two

00:03:52.440 --> 00:03:52.450
time so let's look at what these two
 

00:03:52.450 --> 00:03:55.140
time so let's look at what these two
extremes will do on optimizing this cost

00:03:55.140 --> 00:03:55.150
extremes will do on optimizing this cost
 

00:03:55.150 --> 00:03:57.240
extremes will do on optimizing this cost
function if these are the contours of a

00:03:57.240 --> 00:03:57.250
function if these are the contours of a
 

00:03:57.250 --> 00:03:59.039
function if these are the contours of a
cost function trying to minimize so the

00:03:59.039 --> 00:03:59.049
cost function trying to minimize so the
 

00:03:59.049 --> 00:04:03.330
cost function trying to minimize so the
your minimum is there then - gradient

00:04:03.330 --> 00:04:03.340
your minimum is there then - gradient
 

00:04:03.340 --> 00:04:05.940
your minimum is there then - gradient
descent might start somewhere and be

00:04:05.940 --> 00:04:05.950
descent might start somewhere and be
 

00:04:05.950 --> 00:04:08.990
descent might start somewhere and be
able to take relatively low noise

00:04:08.990 --> 00:04:09.000
able to take relatively low noise
 

00:04:09.000 --> 00:04:12.929
able to take relatively low noise
relatively large steps and you know just

00:04:12.929 --> 00:04:12.939
relatively large steps and you know just
 

00:04:12.939 --> 00:04:16.170
relatively large steps and you know just
keep marching to the minimum in contrast

00:04:16.170 --> 00:04:16.180
keep marching to the minimum in contrast
 

00:04:16.180 --> 00:04:19.409
keep marching to the minimum in contrast
with so costly gradient descent if you

00:04:19.409 --> 00:04:19.419
with so costly gradient descent if you
 

00:04:19.419 --> 00:04:20.849
with so costly gradient descent if you
start somewhere let's pick a different

00:04:20.849 --> 00:04:20.859
start somewhere let's pick a different
 

00:04:20.859 --> 00:04:23.279
start somewhere let's pick a different
starting point then on every iteration

00:04:23.279 --> 00:04:23.289
starting point then on every iteration
 

00:04:23.289 --> 00:04:25.469
starting point then on every iteration
you're taking bring descends with just a

00:04:25.469 --> 00:04:25.479
you're taking bring descends with just a
 

00:04:25.479 --> 00:04:27.719
you're taking bring descends with just a
single training example so most of the

00:04:27.719 --> 00:04:27.729
single training example so most of the
 

00:04:27.729 --> 00:04:29.909
single training example so most of the
time you hit to what the global minimum

00:04:29.909 --> 00:04:29.919
time you hit to what the global minimum
 

00:04:29.919 --> 00:04:31.800
time you hit to what the global minimum
but sometimes you hit in the wrong

00:04:31.800 --> 00:04:31.810
but sometimes you hit in the wrong
 

00:04:31.810 --> 00:04:33.600
but sometimes you hit in the wrong
direction if you know that one example

00:04:33.600 --> 00:04:33.610
direction if you know that one example
 

00:04:33.610 --> 00:04:35.689
direction if you know that one example
happens to point you in a bad direction

00:04:35.689 --> 00:04:35.699
happens to point you in a bad direction
 

00:04:35.699 --> 00:04:38.700
happens to point you in a bad direction
so stochastic great descent can be

00:04:38.700 --> 00:04:38.710
so stochastic great descent can be
 

00:04:38.710 --> 00:04:41.909
so stochastic great descent can be
extremely noisy and on average takes you

00:04:41.909 --> 00:04:41.919
extremely noisy and on average takes you
 

00:04:41.919 --> 00:04:45.360
extremely noisy and on average takes you
in a good direction but um sometimes

00:04:45.360 --> 00:04:45.370
in a good direction but um sometimes
 

00:04:45.370 --> 00:04:46.740
in a good direction but um sometimes
you're headed in the wrong direction as

00:04:46.740 --> 00:04:46.750
you're headed in the wrong direction as
 

00:04:46.750 --> 00:04:49.469
you're headed in the wrong direction as
well as the constant descent won't ever

00:04:49.469 --> 00:04:49.479
well as the constant descent won't ever
 

00:04:49.479 --> 00:04:51.360
well as the constant descent won't ever
converge you're always just kind of

00:04:51.360 --> 00:04:51.370
converge you're always just kind of
 

00:04:51.370 --> 00:04:54.029
converge you're always just kind of
oscillate and wander around the region

00:04:54.029 --> 00:04:54.039
oscillate and wander around the region
 

00:04:54.039 --> 00:04:55.620
oscillate and wander around the region
of the minimum but it won't ever just

00:04:55.620 --> 00:04:55.630
of the minimum but it won't ever just
 

00:04:55.630 --> 00:04:57.360
of the minimum but it won't ever just
head to the minimum and stay there

00:04:57.360 --> 00:04:57.370
head to the minimum and stay there
 

00:04:57.370 --> 00:05:01.010
head to the minimum and stay there
in practice the mini batch size you use

00:05:01.010 --> 00:05:01.020
in practice the mini batch size you use
 

00:05:01.020 --> 00:05:07.279
in practice the mini batch size you use
will be somewhere in between

00:05:07.279 --> 00:05:07.289
 
 

00:05:07.289 --> 00:05:11.749
 
some moves in in 1 + M + 1 nm are

00:05:11.749 --> 00:05:11.759
some moves in in 1 + M + 1 nm are
 

00:05:11.759 --> 00:05:14.610
some moves in in 1 + M + 1 nm are
respectively too small and too large and

00:05:14.610 --> 00:05:14.620
respectively too small and too large and
 

00:05:14.620 --> 00:05:17.460
respectively too small and too large and
here's why if you use batch gradient

00:05:17.460 --> 00:05:17.470
here's why if you use batch gradient
 

00:05:17.470 --> 00:05:22.900
here's why if you use batch gradient
descent

00:05:22.900 --> 00:05:22.910
 
 

00:05:22.910 --> 00:05:30.170
 
so this is your mini batch size equals M

00:05:30.170 --> 00:05:30.180
 
 

00:05:30.180 --> 00:05:33.060
 
then you're processing a huge training

00:05:33.060 --> 00:05:33.070
then you're processing a huge training
 

00:05:33.070 --> 00:05:35.550
then you're processing a huge training
set on every innovation so the main

00:05:35.550 --> 00:05:35.560
set on every innovation so the main
 

00:05:35.560 --> 00:05:37.650
set on every innovation so the main
disadvantage of this is that it takes

00:05:37.650 --> 00:05:37.660
disadvantage of this is that it takes
 

00:05:37.660 --> 00:05:41.480
disadvantage of this is that it takes
too much time too long per iteration

00:05:41.480 --> 00:05:41.490
too much time too long per iteration
 

00:05:41.490 --> 00:05:43.380
too much time too long per iteration
assuming you have a very large training

00:05:43.380 --> 00:05:43.390
assuming you have a very large training
 

00:05:43.390 --> 00:05:44.790
assuming you have a very large training
set if you have you're a small training

00:05:44.790 --> 00:05:44.800
set if you have you're a small training
 

00:05:44.800 --> 00:05:47.070
set if you have you're a small training
set then bachelor in descent is fine if

00:05:47.070 --> 00:05:47.080
set then bachelor in descent is fine if
 

00:05:47.080 --> 00:05:49.590
set then bachelor in descent is fine if
you go to the opposite if you use the

00:05:49.590 --> 00:05:49.600
you go to the opposite if you use the
 

00:05:49.600 --> 00:05:56.190
you go to the opposite if you use the
conflict-ridden you're sent then it's

00:05:56.190 --> 00:05:56.200
conflict-ridden you're sent then it's
 

00:05:56.200 --> 00:05:58.080
conflict-ridden you're sent then it's
nice that you get to make progress after

00:05:58.080 --> 00:05:58.090
nice that you get to make progress after
 

00:05:58.090 --> 00:06:00.630
nice that you get to make progress after
processing just one example that's

00:06:00.630 --> 00:06:00.640
processing just one example that's
 

00:06:00.640 --> 00:06:02.610
processing just one example that's
actually not a problem and the noisiness

00:06:02.610 --> 00:06:02.620
actually not a problem and the noisiness
 

00:06:02.620 --> 00:06:05.430
actually not a problem and the noisiness
can be ameliorated or can be reduced by

00:06:05.430 --> 00:06:05.440
can be ameliorated or can be reduced by
 

00:06:05.440 --> 00:06:07.650
can be ameliorated or can be reduced by
just using a smaller learning rate but

00:06:07.650 --> 00:06:07.660
just using a smaller learning rate but
 

00:06:07.660 --> 00:06:09.210
just using a smaller learning rate but
the huge disadvantage the stochastic

00:06:09.210 --> 00:06:09.220
the huge disadvantage the stochastic
 

00:06:09.220 --> 00:06:12.480
the huge disadvantage the stochastic
green descent is that you lose almost

00:06:12.480 --> 00:06:12.490
green descent is that you lose almost
 

00:06:12.490 --> 00:06:17.540
green descent is that you lose almost
all your speed up from vectorization

00:06:17.540 --> 00:06:17.550
all your speed up from vectorization
 

00:06:17.550 --> 00:06:20.130
all your speed up from vectorization
because here you're processing a single

00:06:20.130 --> 00:06:20.140
because here you're processing a single
 

00:06:20.140 --> 00:06:22.260
because here you're processing a single
training example at a time the way you

00:06:22.260 --> 00:06:22.270
training example at a time the way you
 

00:06:22.270 --> 00:06:24.660
training example at a time the way you
process each example is going to be very

00:06:24.660 --> 00:06:24.670
process each example is going to be very
 

00:06:24.670 --> 00:06:27.450
process each example is going to be very
inefficient so what works best in

00:06:27.450 --> 00:06:27.460
inefficient so what works best in
 

00:06:27.460 --> 00:06:31.380
inefficient so what works best in
practice is something in between where

00:06:31.380 --> 00:06:31.390
practice is something in between where
 

00:06:31.390 --> 00:06:37.590
practice is something in between where
you have some you know mini batch size

00:06:37.590 --> 00:06:37.600
you have some you know mini batch size
 

00:06:37.600 --> 00:06:44.880
you have some you know mini batch size
that not too big or too small and this

00:06:44.880 --> 00:06:44.890
that not too big or too small and this
 

00:06:44.890 --> 00:06:48.660
that not too big or too small and this
gives you impractical fastest learning

00:06:48.660 --> 00:06:48.670
gives you impractical fastest learning
 

00:06:48.670 --> 00:06:53.700
gives you impractical fastest learning
and you notice that this has two good

00:06:53.700 --> 00:06:53.710
and you notice that this has two good
 

00:06:53.710 --> 00:06:55.740
and you notice that this has two good
things going for it one is that you do

00:06:55.740 --> 00:06:55.750
things going for it one is that you do
 

00:06:55.750 --> 00:06:59.040
things going for it one is that you do
get a lot of vectorization so in the

00:06:59.040 --> 00:06:59.050
get a lot of vectorization so in the
 

00:06:59.050 --> 00:07:02.250
get a lot of vectorization so in the
example we use on the previous video if

00:07:02.250 --> 00:07:02.260
example we use on the previous video if
 

00:07:02.260 --> 00:07:04.200
example we use on the previous video if
your mini batch size was a thousand

00:07:04.200 --> 00:07:04.210
your mini batch size was a thousand
 

00:07:04.210 --> 00:07:06.270
your mini batch size was a thousand
examples then you know you might go to

00:07:06.270 --> 00:07:06.280
examples then you know you might go to
 

00:07:06.280 --> 00:07:08.460
examples then you know you might go to
vectorize across a thousand examples so

00:07:08.460 --> 00:07:08.470
vectorize across a thousand examples so
 

00:07:08.470 --> 00:07:09.810
vectorize across a thousand examples so
it's going to be much faster than

00:07:09.810 --> 00:07:09.820
it's going to be much faster than
 

00:07:09.820 --> 00:07:12.060
it's going to be much faster than
processing the examples one at a time

00:07:12.060 --> 00:07:12.070
processing the examples one at a time
 

00:07:12.070 --> 00:07:21.830
processing the examples one at a time
and second you can also make progress

00:07:21.830 --> 00:07:21.840
 
 

00:07:21.840 --> 00:07:24.719
 
without needing to wait

00:07:24.719 --> 00:07:24.729
without needing to wait
 

00:07:24.729 --> 00:07:31.710
without needing to wait
till you process the entire training set

00:07:31.710 --> 00:07:31.720
 
 

00:07:31.720 --> 00:07:34.180
 
so again using the numbers we have in

00:07:34.180 --> 00:07:34.190
so again using the numbers we have in
 

00:07:34.190 --> 00:07:36.879
so again using the numbers we have in
the previous video in epochal each path

00:07:36.879 --> 00:07:36.889
the previous video in epochal each path
 

00:07:36.889 --> 00:07:38.499
the previous video in epochal each path
to your training set allows you to take

00:07:38.499 --> 00:07:38.509
to your training set allows you to take
 

00:07:38.509 --> 00:07:41.920
to your training set allows you to take
5000 gradient descent steps so in

00:07:41.920 --> 00:07:41.930
5000 gradient descent steps so in
 

00:07:41.930 --> 00:07:43.839
5000 gradient descent steps so in
practice there be some in-between mini

00:07:43.839 --> 00:07:43.849
practice there be some in-between mini
 

00:07:43.849 --> 00:07:47.290
practice there be some in-between mini
batch size that works best and so with

00:07:47.290 --> 00:07:47.300
batch size that works best and so with
 

00:07:47.300 --> 00:07:48.879
batch size that works best and so with
mini bearing assembly to start here

00:07:48.879 --> 00:07:48.889
mini bearing assembly to start here
 

00:07:48.889 --> 00:07:51.189
mini bearing assembly to start here
maybe one iteration does this two

00:07:51.189 --> 00:07:51.199
maybe one iteration does this two
 

00:07:51.199 --> 00:07:54.490
maybe one iteration does this two
iterations three four you know and it's

00:07:54.490 --> 00:07:54.500
iterations three four you know and it's
 

00:07:54.500 --> 00:07:56.320
iterations three four you know and it's
not a guarantee to always head toward

00:07:56.320 --> 00:07:56.330
not a guarantee to always head toward
 

00:07:56.330 --> 00:08:00.309
not a guarantee to always head toward
the minimum but it tends to head more

00:08:00.309 --> 00:08:00.319
the minimum but it tends to head more
 

00:08:00.319 --> 00:08:01.870
the minimum but it tends to head more
consistently in the rational minimum

00:08:01.870 --> 00:08:01.880
consistently in the rational minimum
 

00:08:01.880 --> 00:08:03.790
consistently in the rational minimum
than stochastic during descent and then

00:08:03.790 --> 00:08:03.800
than stochastic during descent and then
 

00:08:03.800 --> 00:08:05.800
than stochastic during descent and then
it doesn't always exactly convert your

00:08:05.800 --> 00:08:05.810
it doesn't always exactly convert your
 

00:08:05.810 --> 00:08:08.260
it doesn't always exactly convert your
oscillate in a very small region if

00:08:08.260 --> 00:08:08.270
oscillate in a very small region if
 

00:08:08.270 --> 00:08:09.909
oscillate in a very small region if
that's an issue you can always reduce

00:08:09.909 --> 00:08:09.919
that's an issue you can always reduce
 

00:08:09.919 --> 00:08:11.980
that's an issue you can always reduce
the learning rate slowly we'll talk more

00:08:11.980 --> 00:08:11.990
the learning rate slowly we'll talk more
 

00:08:11.990 --> 00:08:13.809
the learning rate slowly we'll talk more
about learning rate detail how to reduce

00:08:13.809 --> 00:08:13.819
about learning rate detail how to reduce
 

00:08:13.819 --> 00:08:16.390
about learning rate detail how to reduce
our learning rate in a later video so if

00:08:16.390 --> 00:08:16.400
our learning rate in a later video so if
 

00:08:16.400 --> 00:08:18.850
our learning rate in a later video so if
the mini batch size should not be M and

00:08:18.850 --> 00:08:18.860
the mini batch size should not be M and
 

00:08:18.860 --> 00:08:20.110
the mini batch size should not be M and
should not be one but it should be

00:08:20.110 --> 00:08:20.120
should not be one but it should be
 

00:08:20.120 --> 00:08:22.360
should not be one but it should be
something in between how do you go about

00:08:22.360 --> 00:08:22.370
something in between how do you go about
 

00:08:22.370 --> 00:08:23.800
something in between how do you go about
choosing it well here are some

00:08:23.800 --> 00:08:23.810
choosing it well here are some
 

00:08:23.810 --> 00:08:26.619
choosing it well here are some
guidelines first if you have a small

00:08:26.619 --> 00:08:26.629
guidelines first if you have a small
 

00:08:26.629 --> 00:08:32.920
guidelines first if you have a small
training set just use batch gradient

00:08:32.920 --> 00:08:32.930
training set just use batch gradient
 

00:08:32.930 --> 00:08:37.480
training set just use batch gradient
descent you know if you have the small

00:08:37.480 --> 00:08:37.490
descent you know if you have the small
 

00:08:37.490 --> 00:08:40.600
descent you know if you have the small
training set then no point using the

00:08:40.600 --> 00:08:40.610
training set then no point using the
 

00:08:40.610 --> 00:08:42.190
training set then no point using the
batch render send you can process the

00:08:42.190 --> 00:08:42.200
batch render send you can process the
 

00:08:42.200 --> 00:08:43.750
batch render send you can process the
whole training site quite fast so you

00:08:43.750 --> 00:08:43.760
whole training site quite fast so you
 

00:08:43.760 --> 00:08:45.910
whole training site quite fast so you
might as well use factory innocent what

00:08:45.910 --> 00:08:45.920
might as well use factory innocent what
 

00:08:45.920 --> 00:08:47.740
might as well use factory innocent what
the small training set mean I would say

00:08:47.740 --> 00:08:47.750
the small training set mean I would say
 

00:08:47.750 --> 00:08:50.760
the small training set mean I would say
you know this less than maybe 2000 um

00:08:50.760 --> 00:08:50.770
you know this less than maybe 2000 um
 

00:08:50.770 --> 00:08:52.930
you know this less than maybe 2000 um
would be perfectly fine to just use

00:08:52.930 --> 00:08:52.940
would be perfectly fine to just use
 

00:08:52.940 --> 00:08:55.600
would be perfectly fine to just use
battery and descent otherwise if you

00:08:55.600 --> 00:08:55.610
battery and descent otherwise if you
 

00:08:55.610 --> 00:08:58.030
battery and descent otherwise if you
have a bigger training set typical mini

00:08:58.030 --> 00:08:58.040
have a bigger training set typical mini
 

00:08:58.040 --> 00:09:05.769
have a bigger training set typical mini
batch sizes would be anything from 64 up

00:09:05.769 --> 00:09:05.779
batch sizes would be anything from 64 up
 

00:09:05.779 --> 00:09:09.310
batch sizes would be anything from 64 up
to maybe 512 are quite typical and

00:09:09.310 --> 00:09:09.320
to maybe 512 are quite typical and
 

00:09:09.320 --> 00:09:11.889
to maybe 512 are quite typical and
difference of the way computer memory is

00:09:11.889 --> 00:09:11.899
difference of the way computer memory is
 

00:09:11.899 --> 00:09:14.290
difference of the way computer memory is
laid out in Access sometimes you code

00:09:14.290 --> 00:09:14.300
laid out in Access sometimes you code
 

00:09:14.300 --> 00:09:17.230
laid out in Access sometimes you code
runs faster if your mini batch size is a

00:09:17.230 --> 00:09:17.240
runs faster if your mini batch size is a
 

00:09:17.240 --> 00:09:20.410
runs faster if your mini batch size is a
lot as the power of two alright so 64 is

00:09:20.410 --> 00:09:20.420
lot as the power of two alright so 64 is
 

00:09:20.420 --> 00:09:24.250
lot as the power of two alright so 64 is
2 to the 6 to the 7 2 to the 8 2 to the

00:09:24.250 --> 00:09:24.260
2 to the 6 to the 7 2 to the 8 2 to the
 

00:09:24.260 --> 00:09:27.910
2 to the 6 to the 7 2 to the 8 2 to the
9 so often I'll implement my mini batch

00:09:27.910 --> 00:09:27.920
9 so often I'll implement my mini batch
 

00:09:27.920 --> 00:09:30.430
9 so often I'll implement my mini batch
size to be a power of 2 I know in the

00:09:30.430 --> 00:09:30.440
size to be a power of 2 I know in the
 

00:09:30.440 --> 00:09:32.350
size to be a power of 2 I know in the
previous video I use in the batch size

00:09:32.350 --> 00:09:32.360
previous video I use in the batch size
 

00:09:32.360 --> 00:09:35.050
previous video I use in the batch size
of 1000 if you really want to do that

00:09:35.050 --> 00:09:35.060
of 1000 if you really want to do that
 

00:09:35.060 --> 00:09:35.810
of 1000 if you really want to do that
work

00:09:35.810 --> 00:09:35.820
work
 

00:09:35.820 --> 00:09:38.960
work
you just use zero 1024 which is to the

00:09:38.960 --> 00:09:38.970
you just use zero 1024 which is to the
 

00:09:38.970 --> 00:09:39.680
you just use zero 1024 which is to the
power of 10

00:09:39.680 --> 00:09:39.690
power of 10
 

00:09:39.690 --> 00:09:42.710
power of 10
and you do see mean batch sizes of size

00:09:42.710 --> 00:09:42.720
and you do see mean batch sizes of size
 

00:09:42.720 --> 00:09:47.120
and you do see mean batch sizes of size
1024 it is a bit more rare this range of

00:09:47.120 --> 00:09:47.130
1024 it is a bit more rare this range of
 

00:09:47.130 --> 00:09:49.190
1024 it is a bit more rare this range of
mini batch size is a little bit more

00:09:49.190 --> 00:09:49.200
mini batch size is a little bit more
 

00:09:49.200 --> 00:09:52.930
mini batch size is a little bit more
common one last tip is to make sure that

00:09:52.930 --> 00:09:52.940
common one last tip is to make sure that
 

00:09:52.940 --> 00:10:00.280
common one last tip is to make sure that
your mini batch all of your XT comma Y T

00:10:00.280 --> 00:10:00.290
your mini batch all of your XT comma Y T
 

00:10:00.290 --> 00:10:04.520
your mini batch all of your XT comma Y T
that that fits in you know CPU GPU

00:10:04.520 --> 00:10:04.530
that that fits in you know CPU GPU
 

00:10:04.530 --> 00:10:09.860
that that fits in you know CPU GPU
memory and this really depends on your

00:10:09.860 --> 00:10:09.870
memory and this really depends on your
 

00:10:09.870 --> 00:10:11.510
memory and this really depends on your
application and how large the single

00:10:11.510 --> 00:10:11.520
application and how large the single
 

00:10:11.520 --> 00:10:14.480
application and how large the single
training example is but if you ever

00:10:14.480 --> 00:10:14.490
training example is but if you ever
 

00:10:14.490 --> 00:10:16.190
training example is but if you ever
process a mini batch that doesn't

00:10:16.190 --> 00:10:16.200
process a mini batch that doesn't
 

00:10:16.200 --> 00:10:18.260
process a mini batch that doesn't
actually fit in CPU GPU memory whatever

00:10:18.260 --> 00:10:18.270
actually fit in CPU GPU memory whatever
 

00:10:18.270 --> 00:10:21.080
actually fit in CPU GPU memory whatever
using the process the data then you find

00:10:21.080 --> 00:10:21.090
using the process the data then you find
 

00:10:21.090 --> 00:10:23.090
using the process the data then you find
that the performance suddenly falls off

00:10:23.090 --> 00:10:23.100
that the performance suddenly falls off
 

00:10:23.100 --> 00:10:26.060
that the performance suddenly falls off
a cliff and is suddenly much worse so I

00:10:26.060 --> 00:10:26.070
a cliff and is suddenly much worse so I
 

00:10:26.070 --> 00:10:27.740
a cliff and is suddenly much worse so I
hope this gives you a sense of the

00:10:27.740 --> 00:10:27.750
hope this gives you a sense of the
 

00:10:27.750 --> 00:10:30.170
hope this gives you a sense of the
typical range of mini batch sizes that

00:10:30.170 --> 00:10:30.180
typical range of mini batch sizes that
 

00:10:30.180 --> 00:10:32.900
typical range of mini batch sizes that
people use in practice of course the

00:10:32.900 --> 00:10:32.910
people use in practice of course the
 

00:10:32.910 --> 00:10:34.430
people use in practice of course the
mini batch size is actually another

00:10:34.430 --> 00:10:34.440
mini batch size is actually another
 

00:10:34.440 --> 00:10:36.740
mini batch size is actually another
hyper parameter that you might do a

00:10:36.740 --> 00:10:36.750
hyper parameter that you might do a
 

00:10:36.750 --> 00:10:39.020
hyper parameter that you might do a
quick search over to try to figure out

00:10:39.020 --> 00:10:39.030
quick search over to try to figure out
 

00:10:39.030 --> 00:10:42.290
quick search over to try to figure out
which one is most efficient at reducing

00:10:42.290 --> 00:10:42.300
which one is most efficient at reducing
 

00:10:42.300 --> 00:10:44.810
which one is most efficient at reducing
your cost function J so what I would do

00:10:44.810 --> 00:10:44.820
your cost function J so what I would do
 

00:10:44.820 --> 00:10:46.940
your cost function J so what I would do
is just try a several different values

00:10:46.940 --> 00:10:46.950
is just try a several different values
 

00:10:46.950 --> 00:10:49.250
is just try a several different values
try a few different powers of two and

00:10:49.250 --> 00:10:49.260
try a few different powers of two and
 

00:10:49.260 --> 00:10:51.500
try a few different powers of two and
then see if you could pick one that

00:10:51.500 --> 00:10:51.510
then see if you could pick one that
 

00:10:51.510 --> 00:10:54.080
then see if you could pick one that
makes your gradient descent optimization

00:10:54.080 --> 00:10:54.090
makes your gradient descent optimization
 

00:10:54.090 --> 00:10:56.450
makes your gradient descent optimization
algorithm as efficient as possible but

00:10:56.450 --> 00:10:56.460
algorithm as efficient as possible but
 

00:10:56.460 --> 00:10:58.430
algorithm as efficient as possible but
hopefully this gives you a set of

00:10:58.430 --> 00:10:58.440
hopefully this gives you a set of
 

00:10:58.440 --> 00:11:01.310
hopefully this gives you a set of
guidelines for how to get started with

00:11:01.310 --> 00:11:01.320
guidelines for how to get started with
 

00:11:01.320 --> 00:11:03.500
guidelines for how to get started with
that type of parameter search you now

00:11:03.500 --> 00:11:03.510
that type of parameter search you now
 

00:11:03.510 --> 00:11:05.600
that type of parameter search you now
know how to implement mimi bash great

00:11:05.600 --> 00:11:05.610
know how to implement mimi bash great
 

00:11:05.610 --> 00:11:07.520
know how to implement mimi bash great
descent and make your algorithm run much

00:11:07.520 --> 00:11:07.530
descent and make your algorithm run much
 

00:11:07.530 --> 00:11:09.380
descent and make your algorithm run much
faster especially when you're trading on

00:11:09.380 --> 00:11:09.390
faster especially when you're trading on
 

00:11:09.390 --> 00:11:11.420
faster especially when you're trading on
a large training set but it turns out

00:11:11.420 --> 00:11:11.430
a large training set but it turns out
 

00:11:11.430 --> 00:11:13.100
a large training set but it turns out
they're even more efficient algorithms

00:11:13.100 --> 00:11:13.110
they're even more efficient algorithms
 

00:11:13.110 --> 00:11:14.990
they're even more efficient algorithms
than gradient descent or mini battery in

00:11:14.990 --> 00:11:15.000
than gradient descent or mini battery in
 

00:11:15.000 --> 00:11:16.850
than gradient descent or mini battery in
this end let's start talking about them

00:11:16.850 --> 00:11:16.860
this end let's start talking about them
 

00:11:16.860 --> 00:11:19.670
this end let's start talking about them
in the next few videos

