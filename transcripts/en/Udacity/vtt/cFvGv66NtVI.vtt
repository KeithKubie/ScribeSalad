WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:03.790
So it seems like a good idea and
I think it's worth asking whether or

00:00:03.790 --> 00:00:05.470
not it actually works.

00:00:05.470 --> 00:00:06.220
&gt;&gt; Does it work?

00:00:06.220 --> 00:00:07.490
&gt;&gt; Well, it's complicated.

00:00:07.490 --> 00:00:09.120
&gt;&gt; Tell me more.

00:00:09.120 --> 00:00:12.370
&gt;&gt; There have been some examples where
things worked out extremely well.

00:00:12.370 --> 00:00:16.079
So let me just quickly summarize
some of them, so you have a sense.

00:00:16.079 --> 00:00:20.403
You mentioned neural networks, so neural
networks, including with non-linear

00:00:20.403 --> 00:00:24.059
connections, have been used with
backpropagation in this context.

00:00:24.059 --> 00:00:27.897
Folks like Gerry Tesauro and Tom
Dietterich have gotten some very nice

00:00:27.897 --> 00:00:30.827
results creating learning
systems that can actually

00:00:30.827 --> 00:00:34.670
do some fairly complicated things and
some difficult tasks.

00:00:34.670 --> 00:00:35.258
&gt;&gt; Like what?

00:00:35.258 --> 00:00:38.210
&gt;&gt; Well Gerry Tesauro's
work was in Backgammon, and

00:00:38.210 --> 00:00:40.970
he actually used the same
kind of approach in

00:00:40.970 --> 00:00:45.380
Jeopardy in the Watson system, for
figuring out how to do decision-making.

00:00:45.380 --> 00:00:50.360
What's the right strategy for handling
different situations in jeopardy, and

00:00:50.360 --> 00:00:53.055
the value function was being
predicted using a backprop net.

00:00:53.055 --> 00:00:57.780
In Wang/Dietterich's work, they were
actually predicting the likelihood of

00:00:57.780 --> 00:01:03.109
success of various sort of
shuttle scheduling strategies.

00:01:03.109 --> 00:01:07.970
They were actually doing for
NASA shuttle scheduling strategies.

00:01:07.970 --> 00:01:09.260
&gt;&gt; Say that three times fast.

00:01:09.260 --> 00:01:13.480
&gt;&gt; She sells seashells to
the shuttling scheduling system.

00:01:13.480 --> 00:01:14.960
&gt;&gt; Sure.
&gt;&gt; So those worked out really well.

00:01:14.960 --> 00:01:15.720
That being said,

00:01:15.720 --> 00:01:18.600
there's not that many other people
who've gotten that to work effectively.

00:01:18.600 --> 00:01:21.620
Certainly, whenever, I'd say to my
students, look these things worked so

00:01:21.620 --> 00:01:23.050
well, why don't you try it?

00:01:23.050 --> 00:01:26.240
They don't seem to be nearly as
successful as these folks have been.

00:01:26.240 --> 00:01:27.546
&gt;&gt; Hey, I got it to work for
my master's thesis.

00:01:27.546 --> 00:01:28.410
&gt;&gt; Did you really?

00:01:28.410 --> 00:01:29.100
For what problem?

00:01:30.110 --> 00:01:33.132
&gt;&gt; For tic-tac-toe and then for
a vision problem that,

00:01:33.132 --> 00:01:38.046
after I went through all this trouble,
I realized was just [INAUDIBLE] around.

00:01:38.046 --> 00:01:40.498
&gt;&gt; [LAUGH],
that's impressive, in quotes.

00:01:40.498 --> 00:01:42.738
&gt;&gt; Not impressive enough to
give me a master's thesis,

00:01:42.738 --> 00:01:44.394
which I think was the important thing.

00:01:44.394 --> 00:01:47.665
So maybe the impressive thing, is that
I got a master's thesis out of this.

00:01:47.665 --> 00:01:49.590
[LAUGH]
&gt;&gt; At MIT, of all places.

00:01:49.590 --> 00:01:50.300
&gt;&gt; Of all places.

00:01:50.300 --> 00:01:52.030
&gt;&gt; So it does sometimes work.

00:01:52.030 --> 00:01:54.850
I find that it's kind of difficult
to get it to work consistently.

00:01:54.850 --> 00:01:58.920
What happens often, when my students try
this, is they get a system that actually

00:01:58.920 --> 00:02:03.240
tends to start to learn well and then it
goes into some kind of a death spiral.

00:02:03.240 --> 00:02:05.000
Because, as I mentioned before,

00:02:05.000 --> 00:02:08.479
the bootstrapping aspect of this
means that you're really dependent on

00:02:08.479 --> 00:02:11.400
making solid predictions based
on your previous predictions.

00:02:11.400 --> 00:02:13.030
And when they start to go south,

00:02:13.030 --> 00:02:15.050
the whole thing can fall apart
actually really quickly.

00:02:15.050 --> 00:02:15.770
That being said,

00:02:15.770 --> 00:02:18.300
there's some other function
approximators that actually have been

00:02:18.300 --> 00:02:22.010
more successful that are variations
of linear function approximators.

00:02:22.010 --> 00:02:23.900
So Rich Sutton, in particular,

00:02:23.900 --> 00:02:27.948
has gotten CMAC to work on a on
a whole bunch of tough problems.

00:02:27.948 --> 00:02:32.210
CMACs are c-m-a-c, it's a particular
kind of function representation that

00:02:32.210 --> 00:02:34.980
is like a neural net, except
the first layer isn't really learned.

00:02:34.980 --> 00:02:37.360
It's kind of decided in advance
how you're going to break up

00:02:37.360 --> 00:02:38.230
the input space.

00:02:38.230 --> 00:02:41.630
And so it really is just
a generalization of straight up linear.

00:02:41.630 --> 00:02:45.930
These can be really effective, but they
don't generalize very aggressively and

00:02:45.930 --> 00:02:48.420
that's part of what makes them
a bit safer to use in this setting.

