WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.900
♪ [music] ♪

00:00:04.090 --> 00:00:12.880
- [male] We are the paradoxical ape,
bi-pedal, naked, large brain. Long the

00:00:13.060 --> 00:00:18.680
master of fire, tools and language but
still trying to understand ourselves.

00:00:18.860 --> 00:00:24.150
Aware that death is inevitable, yet filled
with optimism. We grow up slowly, we hand

00:00:24.330 --> 00:00:41.220
down knowledge, we empathize and deceive,
we shape the future from our shared

00:00:41.400 --> 00:00:45.050
understanding of the past. CARTA brings
together experts from diverse disciplines

00:00:45.230 --> 00:00:52.860
to exchange insights on who we are and how
we got here. An exploration made possible

00:00:53.040 --> 00:00:56.466
by the generosity of humans like you.

00:01:30.317 --> 00:01:32.850
♪ [music] ♪

00:02:07.090 --> 00:02:12.010
- [Edward] Okay, so I wanted to first
thank the organizers really for this kind

00:02:12.190 --> 00:02:18.270
invitation to join this cast of stellar
thinkers about the biology and behavior of

00:02:18.450 --> 00:02:21.740
language. I actually want to switch the
title a little bit to something more

00:02:21.920 --> 00:02:27.070
specific I want to talk to you about
organization in particular a kind of

00:02:27.250 --> 00:02:33.630
organization that I refer to as a
taxonomy. And the organization that I am

00:02:33.810 --> 00:02:37.560
actually really referring to is the
organization of sound and particular

00:02:37.740 --> 00:02:42.340
speech sounds and how those are actually
processed in a very important part of the

00:02:42.520 --> 00:02:47.780
brain called the superior temporal gyrus
also known as Wernicke's area.

00:02:47.960 --> 00:02:54.160
The main focus of my lab is actually to
understand the basic transformation that

00:02:54.340 --> 00:02:59.640
occurs when you have an acoustic stimulus
and how it becomes transformed into

00:02:59.820 --> 00:03:04.670
phonetic units. In other words, basically
, how do we go from the physical stimulus

00:03:04.850 --> 00:03:09.590
that enters our ears into one that's
essential a mental construct one that's a

00:03:09.770 --> 00:03:15.010
linguistic one. And to basically ask the
simple question, “What is the structure of

00:03:15.190 --> 00:03:22.310
that kind of information as it's processed
in the brain?” Now this actually turns out

00:03:22.490 --> 00:03:28.900
to be a very complex problem because it's
one that actually arises from many levels

00:03:29.080 --> 00:03:34.170
of computation that occur in the ascending
auditory system, as sounds actually come

00:03:34.350 --> 00:03:39.400
through the ears they go up through at
least seven different synaptic connections

00:03:39.580 --> 00:03:43.140
across many different parts of the brain
even bi-laterally to were they're actually

00:03:43.320 --> 00:03:47.540
processed in the non-primary auditory
cortex in the superior temporal gyrus. And

00:03:47.720 --> 00:03:51.740
what we know about his area from animal
studies and non-human primates for example

00:03:51.920 --> 00:03:58.450
is that this is an area that no longer is
tuned to basic low level sound features

00:03:58.630 --> 00:04:04.200
like pure tones, pure frequencies but in
one that is actually tuned to very broad

00:04:04.380 --> 00:04:08.730
complex sounds.
There have been very nice work in fMRI

00:04:08.910 --> 00:04:13.370
that's actually demonstrated that this
area is far more selective to complex

00:04:13.550 --> 00:04:20.540
sounds like speech over non-speech sounds.
So the basic question is not really about

00:04:20.720 --> 00:04:26.190
where is this processing going on, the
question is how. Okay, what is the

00:04:26.370 --> 00:04:30.880
structure of information in this
transformation that's going? And in

00:04:31.060 --> 00:04:36.560
particular, what kind of linkages can we
make between that physical stimulus and

00:04:36.740 --> 00:04:41.740
the internal one, which is a phonetic one?
For me, I think it's really important to

00:04:41.920 --> 00:04:46.930
acknowledge some really important
fundamental contributions that occur that

00:04:47.110 --> 00:04:51.560
give us some insight and put them in a
very important perspective.

00:04:51.740 --> 00:04:56.420
For me, I think one of the most important
pieces of work that led to this work that

00:04:56.600 --> 00:05:01.020
I'm about to describe from our lab was
actually 25 years ago using an approach

00:05:01.200 --> 00:05:04.810
that's actually far more complicated and
difficult to achieve than what we do in

00:05:04.990 --> 00:05:10.310
our own work. This is using single unit
and single neuron recordings that were

00:05:10.490 --> 00:05:14.570
recorded from patients that were
undergoing neurosurgical procedures for

00:05:14.750 --> 00:05:20.370
their clinical routine care and this very
extremely rare but precious opportunity,

00:05:20.550 --> 00:05:26.890
to actually record from certain brain
areas while someone is actually listening

00:05:27.070 --> 00:05:31.340
to speech. And these are from my close
colleague and mentor Dr. Ojemann who's in

00:05:31.520 --> 00:05:35.840
the audience today. But why I think it's
so important to acknowledge this work is a

00:05:36.020 --> 00:05:41.740
lot of the clues about what I'm about to
describe were actually seen 25 years ago.

00:05:41.920 --> 00:05:45.580
And this figure that's extracted from that
paper where they actually showed and could

00:05:45.760 --> 00:05:50.560
record from single brain cells called
neurons in the superior temporal gyrus

00:05:50.740 --> 00:05:57.460
that they were active and corresponding to
very specific sounds. But if you actually

00:05:57.640 --> 00:06:01.860
look at where those sounds are,
they're not exactly corresponding to the

00:06:02.040 --> 00:06:07.470
same exact. . . let's say phonemes or the
same exact sounds. But in fact, they are

00:06:07.650 --> 00:06:12.300
corresponding to a class of sounds. This
was an observation that was made in this

00:06:12.480 --> 00:06:17.450
paper and they thought that perhaps this
is some mention of phonetic category

00:06:17.630 --> 00:06:21.780
representation there. But it wasn't that
clear, actually, and there are a lot of

00:06:21.960 --> 00:06:27.930
other really important observations that
were made in that paper. Now from a

00:06:28.110 --> 00:06:34.310
linguistic perspective in thinking about
how behaviorally we organize this

00:06:34.490 --> 00:06:38.660
information in the brain, there's actually
a wonderful way to approach it, it's not

00:06:38.840 --> 00:06:44.850
perfect but a very wonderful way to think
about how languages across the world

00:06:45.030 --> 00:06:50.793
actually share a similar and shared
inventory of speech sounds.

00:06:50.793 --> 00:06:54.720
Not all completely the same, each language
has a different numbering, but they highly

00:06:54.900 --> 00:06:59.410
overlap and the reason why they overlap is
because they are produced by the same

00:06:59.590 --> 00:07:06.780
vocal tract. And this is essentially like
a periodic table of sound elements for

00:07:06.960 --> 00:07:12.430
human language and speech. And so, this
table actually has two really important

00:07:12.610 --> 00:07:16.020
dimensions the horizontal dimension is
actually one that we call the place for

00:07:16.200 --> 00:07:20.770
articulation, it's referencing where in
the vocal tract these sounds are made for

00:07:20.950 --> 00:07:27.150
example, bi-labial sounds the P and the B
require you to actually have a transient

00:07:27.330 --> 00:07:32.330
occlusion at the lips, 'ba.' You cannot
make those sounds without that particular

00:07:32.510 --> 00:07:38.170
articulatory movement. Some of the other
sounds like a 'Da' or a 'D' or a 'T' a

00:07:38.350 --> 00:07:43.080
'Da' a 'Ta' a 'Da' we call alveolar
because the front of the tongue tip is

00:07:43.260 --> 00:07:47.240
actually placed against the teeth. And so,
these are actually referencing where

00:07:47.420 --> 00:07:50.310
occlusions are occurring in the vocal
tract when we speak and those actually

00:07:50.490 --> 00:07:57.880
correlate necessarily to very specific
acoustic signatures. The other dimension

00:07:58.060 --> 00:08:01.750
is what we call the manner of articulation
and so the manner of articulation is

00:08:01.930 --> 00:08:06.250
actually telling you a little bit more
about the not so much where, but how in

00:08:06.430 --> 00:08:10.450
the vocal tract the constrictions are made
in order to produce those sounds.

00:08:10.630 --> 00:08:14.720
We have certain ones like plosives where
you have complete closure of the vocal

00:08:14.900 --> 00:08:20.130
tract and then transient release. Other
sounds where you have near complete like a

00:08:20.310 --> 00:08:25.720
fricative like, 'sha,' 'za' those sounds
that we call fricative. And if you

00:08:25.900 --> 00:08:30.710
actually look at vowels, they actually
have a similar structural organization,

00:08:30.890 --> 00:08:34.610
there actually is something that actually
references where in the vocal tract either

00:08:34.790 --> 00:08:39.740
front, middle or back, or the degree of
open and closure. So for both consonants

00:08:39.920 --> 00:08:44.530
and vowels there actually is a structure
that we know about linguistically and

00:08:44.710 --> 00:08:49.640
phonologically about how these things are
organized.

00:08:49.820 --> 00:08:54.780
I think the thing that interests me is
that, like I referenced before, that this

00:08:54.960 --> 00:09:00.320
is something like a periodic table is that
there is something fundamental about these

00:09:00.500 --> 00:09:06.160
units to our ability to perceive speech.
And these phonological representations are

00:09:06.340 --> 00:09:10.110
not necessarily the ones that we think of
as these letters that we call phonemes,

00:09:10.290 --> 00:09:16.220
but actually groups of phonemes that share
something in common; what we call

00:09:16.400 --> 00:09:20.510
features. And these are the members of
small categories which combine to form the

00:09:20.690 --> 00:09:25.950
speech sounds of human language. This
became very attractive to me as a model of

00:09:26.130 --> 00:09:30.730
something that look for in the brain
because of essentially why it could be so

00:09:30.910 --> 00:09:34.230
important; is that languages is actually
do not vary without limit but they

00:09:34.410 --> 00:09:39.300
actually reflect some single or limited
general pattern which is actually rooted

00:09:39.480 --> 00:09:42.740
in both the physical and cognitive
capacities of the human brain. And I would

00:09:42.920 --> 00:09:47.560
add the vocal tract. And this is a very
not a new kind of thinking but it's one

00:09:47.740 --> 00:09:53.020
that has not been clearly elucidated in
terms of its biological mechanisms.

00:09:53.200 --> 00:09:58.360
So in order for us to get this
information, it requires a very special

00:09:58.540 --> 00:10:03.160
opportunity, the one where we can actually
record directly from the brain. And in

00:10:03.340 --> 00:10:06.360
many ways, this is actually a lot more
coarse than the kind of recordings that

00:10:06.540 --> 00:10:11.890
were done almost 25 years ago.
These are ones from electrode sensors that

00:10:12.070 --> 00:10:16.010
are placed on the brain in order to
localize seizures in patients that have

00:10:16.190 --> 00:10:21.170
epilepsy. In the seven to 10 days that
they are usually waiting to be localized,

00:10:21.350 --> 00:10:26.410
we have a very again, precious opportunity
to actually have some of the participant,

00:10:26.590 --> 00:10:31.980
the patient volunteers listen to natural
continuous speech. Look at those neural

00:10:32.160 --> 00:10:37.510
responses on these electrode recordings to
see how information is distributed in the

00:10:37.690 --> 00:10:42.180
superior temporal gyrus when they're
listening to these sounds. This gives you

00:10:42.360 --> 00:10:46.890
a sense of actually what that neural
activity pattern looks like.

00:10:46.890 --> 00:10:48.263
[audio example]

00:10:48.263 --> 00:10:51.910
We're going to slow down that sentence a
lot here.

00:10:52.090 --> 00:10:57.910
- [audio example] Ready tiger, go to green
five, now.

00:10:58.090 --> 00:11:03.080
- So you can see that the information is
being processed in a very precise both

00:11:03.260 --> 00:11:08.410
spatial and temporal manner in the brain,
and this is exactly the reason why this

00:11:08.590 --> 00:11:12.070
kind of information has been elusive,
because we do not currently have a method

00:11:12.250 --> 00:11:16.100
that actually has a both spatial and
temporal resolution and at the same time

00:11:16.280 --> 00:11:23.020
covers all of these areas simultaneously.
And so, it's again in the context of these

00:11:23.200 --> 00:11:27.430
rare opportunities with a human patient
volunteers that we can conduct this kind

00:11:27.610 --> 00:11:31.870
of research. So the natural question is of
course, now that I've shown you that we

00:11:32.050 --> 00:11:36.700
can actually see a pattern in the brain
both that's temporal and spatially

00:11:36.880 --> 00:11:43.340
specific, what actually happens when we
try to deconstruct some of those sound

00:11:43.520 --> 00:11:47.230
patterns from the brain? And this just
gives you an example, again, in the

00:11:47.410 --> 00:11:53.790
superior temporal gyrus where those sounds
are activating the brain and an example of

00:11:53.970 --> 00:11:57.790
the spectrogram for a given sentence and
in this case it's, “In what eyes they

00:11:57.970 --> 00:12:01.770
were,” and the last part of that figure
basically shows you that pattern across

00:12:01.950 --> 00:12:05.120
different electrodes.
It's not all happening in the same

00:12:05.300 --> 00:12:10.070
particular way, you have very specific
evoked responses that actually occur in

00:12:10.250 --> 00:12:14.480
different parts of the superior temporal
gyrus. And I want to show you what happens

00:12:14.660 --> 00:12:18.280
when you look at just one of those
electrodes and if you look at the neural

00:12:18.460 --> 00:12:22.630
response of that one particular electrode
that's labeled e1, and you organize the

00:12:22.810 --> 00:12:29.580
neural response by different phonemes.
Okay, you can actually see again on the

00:12:29.760 --> 00:12:34.680
vertical axis starting with 'da,' 'ba,'
'ga,' 'pa,' 'ta,' 'ka,' you can see

00:12:34.860 --> 00:12:39.720
that this electrode, those hundreds of
thousands of neurons that are under this

00:12:39.900 --> 00:12:47.500
electrode are very selectively responsive
to this set of sounds that we call

00:12:47.680 --> 00:12:54.250
plosives. It's not one phoneme but a
category and they share this feature that

00:12:54.430 --> 00:12:57.650
we actually know linguistically to be
called plosive.

00:12:57.830 --> 00:13:02.640
I can show you a series of other other
electrodes, electrode two has a very

00:13:02.820 --> 00:13:06.790
different kind of sensitivity, it's
showing you that it really likes those

00:13:06.970 --> 00:13:12.480
sounds 'sha,' 'za,' 'sa,' 'fa.' This
is in electrode that is again not tuned to

00:13:12.660 --> 00:13:17.320
one phoneme but actually tuned to the
category of sibilant fricatives in

00:13:17.500 --> 00:13:24.480
linguistic jargon. We have another
electrode, e3, that is selective to low

00:13:24.660 --> 00:13:30.230
back vowels these 'ah,' based ones, other
one that is a little bit more selective to

00:13:30.410 --> 00:13:37.890
high fronted vowels 'e,' and even another
electrode e5 that is corresponding to

00:13:38.070 --> 00:13:43.970
nasal sounds. So this is a very low level
description, but it's actually the first

00:13:44.150 --> 00:13:49.470
time we've ever seen in this kind of
principled way, obtained through a very

00:13:49.650 --> 00:13:55.620
precise spatial and temporal recordings,
the ability to resolve phonetic feature

00:13:55.800 --> 00:14:00.300
selectivity at single electrodes in the
human brain.

00:14:00.480 --> 00:14:06.080
Now this is not enough, we need to really
address this issue of structure. That's

00:14:06.260 --> 00:14:11.640
one of themes here. And are all of these
things just equally distributed as

00:14:11.820 --> 00:14:15.320
features in the original. . . thinking
about these things you could have a binary

00:14:15.500 --> 00:14:20.470
list of features. And it turns out that
features in and of themselves actually

00:14:20.650 --> 00:14:26.180
have structure and have relationships with
one another. And so, what we did in order

00:14:26.360 --> 00:14:32.020
to look at that structure in the brain, we
looked at hundreds of electrodes that were

00:14:32.200 --> 00:14:37.200
recorded over a dozen patients and each
one of those columns actually corresponds

00:14:37.380 --> 00:14:43.000
to one electrode in one particular
superior temporal gyrus in someone's

00:14:43.180 --> 00:14:50.260
brain. And like I just showed you before,
the vertical axis is actually how they're

00:14:50.440 --> 00:14:55.240
organized by different phonemes.
And what we did here was we used a

00:14:55.420 --> 00:14:58.780
statistical method called hierarchical
clustering, and what hierarchical

00:14:58.960 --> 00:15:05.800
clustering is used for is finding the
patterns in this data. And what the

00:15:05.980 --> 00:15:10.280
hierarchical clustering showed us and
sorted this data was that in fact there is

00:15:10.460 --> 00:15:16.670
indeed structure in the brain's responses
to human speech sounds and it looks like

00:15:16.850 --> 00:15:22.720
this. So we've organized the hierarchical
clustering as a function of a single

00:15:22.900 --> 00:15:28.170
electrode's, again a single column's,
selectivity to different phonemes but

00:15:28.350 --> 00:15:35.050
we've also organized this clustering as a
population response across all of the

00:15:35.230 --> 00:15:40.340
electrodes and looking at that selectivity
for different phonemes. So you have two

00:15:40.520 --> 00:15:45.900
different axes that were actually looking
at the brain's large distributed response

00:15:46.080 --> 00:15:50.160
to speech sounds. And we're using this
method which is what we call

00:15:50.340 --> 00:15:54.770
'unsupervised,' meaning we're not telling
it any linguistic information or we're not

00:15:54.950 --> 00:15:59.140
organizing the data, we're just saying
tell us how the brain is organizing this

00:15:59.320 --> 00:16:02.950
information.
And what we see from this is that when we

00:16:03.130 --> 00:16:07.580
actually look at where this information is
being organized, one of the biggest

00:16:07.760 --> 00:16:11.770
divisions between different parts of the
different kinds of selectivity in the

00:16:11.950 --> 00:16:14.520
brain are what we would call the
difference between consonants and vowels

00:16:14.700 --> 00:16:22.290
are really actually between obstruents and
continuants in linguistic jargon. But

00:16:22.470 --> 00:16:26.950
within those different categories, you
actually have sub-classification. So

00:16:27.130 --> 00:16:31.480
within the consonants you actually have
subdivision between plosives and

00:16:31.660 --> 00:16:35.450
fricatives. And between the sonorants you
actually have referencing for different

00:16:35.630 --> 00:16:39.770
positions of the tongue: low back, low
front, high front different classes of

00:16:39.950 --> 00:16:44.060
vowels and in fact nasal.
So basically this is telling you that

00:16:44.240 --> 00:16:50.070
feature selectivity in the brain is
actually hierarchically structured. The

00:16:50.250 --> 00:16:55.110
second thing is that instead of using
phonemes in order to organize the

00:16:55.290 --> 00:17:02.570
responses, we actually use features. So as
an example of that term, dorsal actually

00:17:02.750 --> 00:17:09.350
refers to the tongue position when it's a
fairly back like for 'ga,' 'ka' sounds.

00:17:09.530 --> 00:17:13.130
And you can see that when we organize
things by features, you have a much

00:17:13.310 --> 00:17:20.490
cleaner delineation. The electrode
responses seem to be much more tuned to

00:17:20.670 --> 00:17:25.820
phonetic features shown below as they are
compared to when you plot them as

00:17:26.000 --> 00:17:32.390
phonemes. Okay, so this essentially
disproves any idea that there is

00:17:32.570 --> 00:17:37.430
individual phoneme representation in the
brain at least not one that's locally

00:17:37.610 --> 00:17:42.120
encoded, but tells you that the brain is
organized by its sensitivity to phonetic

00:17:42.300 --> 00:17:46.510
features.
Now relating it to a phonetic features is

00:17:46.690 --> 00:17:50.520
the first step and it's one that's really
important because it's referencing the one

00:17:50.700 --> 00:17:55.720
that we know about from linguistics and
the one that we know behaviorally. But how

00:17:55.900 --> 00:17:59.900
do we connect this to the physical
stimulus that's actually coming through

00:18:00.080 --> 00:18:04.690
our ears? And that's where we have to make
a linkage to actually something about the

00:18:04.870 --> 00:18:08.990
sound properties.
Are these things truly abstract features

00:18:09.170 --> 00:18:13.230
that are being picked up by the brain or
actually are they referencing specific

00:18:13.410 --> 00:18:19.250
sound properties? And basically, the
answer is the latter is that what we're

00:18:19.430 --> 00:18:25.580
actually seeing is sensitivity to
particular spectral temporal features. In

00:18:25.760 --> 00:18:30.250
the top row, I am showing you basically,
when we look at the average tuning curves

00:18:30.430 --> 00:18:34.410
the frequency versus time tuning curves
for each one of those different

00:18:34.590 --> 00:18:39.920
classifications for plosive fricatives,
they're very similar to the acoustic

00:18:40.100 --> 00:18:43.910
structure when you average those
particular phonemes in the brain.

00:18:44.090 --> 00:18:47.380
So what these means is the tuning that we
are seeing that's corresponding to

00:18:47.560 --> 00:18:53.560
phonetic features is in fact one that is
tuned to high order acoustic spectral

00:18:53.740 --> 00:19:00.040
temporal ones. The brain is selecting
specific kind of acoustic information and

00:19:00.220 --> 00:19:05.310
converting it into what we perceive as
phonetic. In the interest of time, I'm

00:19:05.490 --> 00:19:11.690
going to sort of skip more in depth
information about vowels and plosives and

00:19:11.870 --> 00:19:15.720
how those are specifically encoded. But in
summary, what we found is that there's

00:19:15.900 --> 00:19:19.780
actually a multidimensional feature space
actually for speech sounds in the human

00:19:19.960 --> 00:19:26.120
superior temporal gyrus and that this
feature space is organized in a way that

00:19:26.300 --> 00:19:31.760
actually shows hierarchical structure. And
that, the hierarchical structure is very

00:19:31.940 --> 00:19:36.410
strongly driven by the brain's, in
particular this auditory cortex,

00:19:36.590 --> 00:19:42.250
sensitivity to acoustic differences which
are most signified actually in the manner

00:19:42.430 --> 00:19:46.200
of articulation distinctions
linguistically. And what's interesting

00:19:46.380 --> 00:19:50.320
about this is it actually does correlate
quite well with some known perceptual

00:19:50.500 --> 00:19:56.860
behavior. And so, I would like to conclude
there and acknowledge some of the really

00:19:57.040 --> 00:20:00.790
important people from my lab: post
doctoral fellow Nima Mesgarani who did

00:20:00.970 --> 00:20:03.910
most of this work with one of our graduate
students Connie Cheung.

00:20:04.090 --> 00:20:05.281
Thank you.

00:20:05.281 --> 00:20:07.614
[applause]

00:20:09.450 --> 00:20:12.400
♪ [music] ♪

