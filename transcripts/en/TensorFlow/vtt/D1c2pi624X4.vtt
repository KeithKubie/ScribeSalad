WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.440
[MUSIC PLAYING]

00:00:07.733 --> 00:00:08.900
MARK OMERNICK: Good morning.

00:00:08.900 --> 00:00:11.108
My name is Mark Omernick,
and I'm a software engineer

00:00:11.108 --> 00:00:11.860
with Google AI.

00:00:11.860 --> 00:00:13.632
Today I'll be talking
about two projects.

00:00:13.632 --> 00:00:16.090
The first is enhanced Unicode
support across the TensorFlow

00:00:16.090 --> 00:00:16.800
code base.

00:00:16.800 --> 00:00:18.790
And the second is
a new tensor type

00:00:18.790 --> 00:00:21.670
called RaggedTensors, intended
to officially represent

00:00:21.670 --> 00:00:23.860
sequence data.

00:00:23.860 --> 00:00:27.090
First, I'll take a quick look at
we've improved Unicode support

00:00:27.090 --> 00:00:29.170
in TensorFlow.

00:00:29.170 --> 00:00:30.910
Unicode is a way of
encoding characters

00:00:30.910 --> 00:00:32.680
from nearly every
written language using

00:00:32.680 --> 00:00:34.120
sequences of bytes.

00:00:34.120 --> 00:00:36.270
Here, these four characters
can be represented

00:00:36.270 --> 00:00:37.662
as four triplets of bytes.

00:00:37.662 --> 00:00:39.370
A string containing
these four characters

00:00:39.370 --> 00:00:42.460
would be 12 bytes long in total.

00:00:42.460 --> 00:00:44.560
Previously, TensorFlow
assumed that strings

00:00:44.560 --> 00:00:47.410
were indexed by individual
bytes, ASCII style.

00:00:47.410 --> 00:00:50.110
That led to issues like this,
where strings split would split

00:00:50.110 --> 00:00:52.660
Unicode characters below
the character boundary,

00:00:52.660 --> 00:00:57.680
and substr would index by
bytes instead of characters.

00:00:57.680 --> 00:01:00.880
However, now that we've added
Unicode support to TensorFlow,

00:01:00.880 --> 00:01:03.310
we can correctly handle
multi-byte characters.

00:01:03.310 --> 00:01:07.120
Unicode_split now splits into
proper triplets, and substr,

00:01:07.120 --> 00:01:11.500
with the UTF8_CHAR tag,
indexes by UTF-8 characters.

00:01:13.902 --> 00:01:15.860
In addition to string
splitting, TensorFlow now

00:01:15.860 --> 00:01:17.630
supports many other
Unicode-aware string

00:01:17.630 --> 00:01:20.480
operations, from Unicode
encoding and decoding

00:01:20.480 --> 00:01:21.710
to string length analysis.

00:01:24.432 --> 00:01:26.140
For the second part
of this presentation,

00:01:26.140 --> 00:01:29.500
I'd like to introduce a new
tensor type, RaggedTensors,

00:01:29.500 --> 00:01:32.080
that we designed to handle
text and other variable length

00:01:32.080 --> 00:01:32.740
sequences.

00:01:35.290 --> 00:01:37.300
RaggedTensors are a
native representation

00:01:37.300 --> 00:01:39.550
for sequences of varying shape.

00:01:39.550 --> 00:01:42.490
Here you can see a RaggedTensor
containing three batch items.

00:01:42.490 --> 00:01:44.770
The first, a tensor
with two strings,

00:01:44.770 --> 00:01:47.730
the second, a tensor with
four strings, and the third,

00:01:47.730 --> 00:01:50.800
a tensor with one string
without any additional padding

00:01:50.800 --> 00:01:54.060
or user-facing logic.

00:01:54.060 --> 00:01:57.360
RaggedTensors are different from
SparseTensors in one key way.

00:01:57.360 --> 00:01:58.890
SparseTensors make
the assumption

00:01:58.890 --> 00:02:01.800
that the underlying dense
tensor is regularly shaped

00:02:01.800 --> 00:02:03.960
and unmentioned
values are missing.

00:02:03.960 --> 00:02:07.610
RaggedTensors, on the other
hand, make no such assumption.

00:02:07.610 --> 00:02:09.630
Here, for instance,
the SparseTensor

00:02:09.630 --> 00:02:13.800
interprets the first batch
element as John, null, null,

00:02:13.800 --> 00:02:16.560
while the RaggedTensor
interprets it as simply John.

00:02:19.240 --> 00:02:22.570
A RaggedTensor can contain any
number of irregular dimensions.

00:02:22.570 --> 00:02:25.450
Here, for instance, we have a
three-dimensional RaggedTensor

00:02:25.450 --> 00:02:28.930
that represents every
character in every token

00:02:28.930 --> 00:02:30.610
in a batch of three sequences.

00:02:30.610 --> 00:02:32.110
There are variable
numbers of tokens

00:02:32.110 --> 00:02:34.480
per sequence and variable
numbers of characters

00:02:34.480 --> 00:02:35.380
per token.

00:02:35.380 --> 00:02:36.820
But with RaggedTensors,
you don't

00:02:36.820 --> 00:02:39.310
need to worry about
maximum sizes, padding,

00:02:39.310 --> 00:02:42.500
or anything else.

00:02:42.500 --> 00:02:45.110
RaggedTensors are a native
TensorFlow representation

00:02:45.110 --> 00:02:46.910
for any varying length
sequence of data,

00:02:46.910 --> 00:02:49.310
from words to images and beyond.

00:02:49.310 --> 00:02:50.960
You could imagine
using RaggedTensors

00:02:50.960 --> 00:02:52.520
to contain the set
of still frames

00:02:52.520 --> 00:02:57.740
in a batch of videos, where each
video is a different length.

00:02:57.740 --> 00:02:59.930
So how do you use RaggedTensors?

00:02:59.930 --> 00:03:01.270
Let's start with building them.

00:03:01.270 --> 00:03:03.395
To create a RaggedTensor,
you'll need a flat tensor

00:03:03.395 --> 00:03:06.170
of values and some
specification on how to split

00:03:06.170 --> 00:03:07.655
those values into batch items.

00:03:10.740 --> 00:03:12.330
Once you have a
RaggedTensor, you

00:03:12.330 --> 00:03:14.010
can perform standard
tensor operations

00:03:14.010 --> 00:03:16.620
on it, like concatenation
and slicing,

00:03:16.620 --> 00:03:18.240
even within
irregular dimensions.

00:03:21.800 --> 00:03:23.540
RaggedTensors are
natively supported

00:03:23.540 --> 00:03:27.230
by over 100 TensorFlow core
ops ranging from math ops

00:03:27.230 --> 00:03:29.270
through string
handling to reductions.

00:03:33.330 --> 00:03:35.990
And if you need to operate on
each value in a RaggedTensor,

00:03:35.990 --> 00:03:37.850
we provide a native
map function.

00:03:37.850 --> 00:03:40.790
You can use this to apply
ops or even entire subgraphs

00:03:40.790 --> 00:03:42.958
to every value in
a RaggedTensor.

00:03:45.890 --> 00:03:48.030
To illustrate how to use
RaggedTensors in a model,

00:03:48.030 --> 00:03:50.840
let's consider using a bag
of character level embeddings

00:03:50.840 --> 00:03:55.060
to create a token
level embedding.

00:03:55.060 --> 00:03:57.760
We start by taking a
RaggedTensor of tokens

00:03:57.760 --> 00:04:01.630
separated by batch and applying
unicode_decode, a new op that

00:04:01.630 --> 00:04:04.390
outputs a RaggedTensor of
Unicode code points separated

00:04:04.390 --> 00:04:06.220
by batch and token.

00:04:06.220 --> 00:04:08.980
We can then use map_flat_values
to get an embedding

00:04:08.980 --> 00:04:10.540
for each of these code points.

00:04:13.930 --> 00:04:16.329
Now, char_embedding is a
four-dimensional RaggedTensor

00:04:16.329 --> 00:04:19.946
with batch, token, sentence,
and embedding dimensions.

00:04:19.946 --> 00:04:22.029
We can convert it into a
standard four-dimensional

00:04:22.029 --> 00:04:25.420
tensor, reshape it, so
that it is token_major,

00:04:25.420 --> 00:04:29.260
run a convolution over each
character in each token,

00:04:29.260 --> 00:04:32.650
then reshape it back into a
dense 40 tensor with batch,

00:04:32.650 --> 00:04:37.020
token, sentence, and
embedding dimensions.

00:04:37.020 --> 00:04:39.360
That 40 dense tensor
can be converted back

00:04:39.360 --> 00:04:42.480
into a 40 RaggedTensor,
which removes any padding.

00:04:42.480 --> 00:04:45.750
This RaggedTensor can be
reduced, via reduce_mean,

00:04:45.750 --> 00:04:47.790
to create per token embeddings.

00:04:47.790 --> 00:04:51.240
At the end, we have a tensor of
embeddings, one for each token,

00:04:51.240 --> 00:04:55.330
built from characters without
any extraneous padding.

00:04:55.330 --> 00:04:57.910
For more information, you can
take a look at the tutorials

00:04:57.910 --> 00:04:59.050
available here.

00:04:59.050 --> 00:05:01.610
Please try them out and give
your feedback on GitHub.

00:05:01.610 --> 00:05:02.110
Thank you.

00:05:02.110 --> 00:05:05.760
[MUSIC PLAYING]

