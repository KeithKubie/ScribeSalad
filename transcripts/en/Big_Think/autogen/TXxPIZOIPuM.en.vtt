WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:05.060
[Music]

00:00:05.060 --> 00:00:05.070
[Music]
 

00:00:05.070 --> 00:00:08.040
[Music]
the subtitle of our book is finding

00:00:08.040 --> 00:00:08.050
the subtitle of our book is finding
 

00:00:08.050 --> 00:00:10.520
the subtitle of our book is finding
Cassandra's to stop catastrophes

00:00:10.520 --> 00:00:10.530
Cassandra's to stop catastrophes
 

00:00:10.530 --> 00:00:14.220
Cassandra's to stop catastrophes
Cassandra in Greek mythology was someone

00:00:14.220 --> 00:00:14.230
Cassandra in Greek mythology was someone
 

00:00:14.230 --> 00:00:15.629
Cassandra in Greek mythology was someone
cursed by the gods who could accurately

00:00:15.629 --> 00:00:15.639
cursed by the gods who could accurately
 

00:00:15.639 --> 00:00:18.300
cursed by the gods who could accurately
see the future but would never be

00:00:18.300 --> 00:00:18.310
see the future but would never be
 

00:00:18.310 --> 00:00:20.760
see the future but would never be
believed when we say Cassandra's

00:00:20.760 --> 00:00:20.770
believed when we say Cassandra's
 

00:00:20.770 --> 00:00:22.800
believed when we say Cassandra's
throughout the book we're talking about

00:00:22.800 --> 00:00:22.810
throughout the book we're talking about
 

00:00:22.810 --> 00:00:25.350
throughout the book we're talking about
people who can accurately see the future

00:00:25.350 --> 00:00:25.360
people who can accurately see the future
 

00:00:25.360 --> 00:00:28.460
people who can accurately see the future
people who are right Cassandra was right

00:00:28.460 --> 00:00:28.470
people who are right Cassandra was right
 

00:00:28.470 --> 00:00:31.080
people who are right Cassandra was right
people who are right about the future

00:00:31.080 --> 00:00:31.090
people who are right about the future
 

00:00:31.090 --> 00:00:35.460
people who are right about the future
but are being ignored having derived

00:00:35.460 --> 00:00:35.470
but are being ignored having derived
 

00:00:35.470 --> 00:00:37.890
but are being ignored having derived
what we think are the lessons learned

00:00:37.890 --> 00:00:37.900
what we think are the lessons learned
 

00:00:37.900 --> 00:00:41.670
what we think are the lessons learned
from past Cassandra events we then

00:00:41.670 --> 00:00:41.680
from past Cassandra events we then
 

00:00:41.680 --> 00:00:43.710
from past Cassandra events we then
looked at people today who were

00:00:43.710 --> 00:00:43.720
looked at people today who were
 

00:00:43.720 --> 00:00:47.190
looked at people today who were
predicting things and being ignored we

00:00:47.190 --> 00:00:47.200
predicting things and being ignored we
 

00:00:47.200 --> 00:00:49.319
predicting things and being ignored we
looked at issues first and then tried to

00:00:49.319 --> 00:00:49.329
looked at issues first and then tried to
 

00:00:49.329 --> 00:00:50.880
looked at issues first and then tried to
see if there was someone warning about

00:00:50.880 --> 00:00:50.890
see if there was someone warning about
 

00:00:50.890 --> 00:00:53.040
see if there was someone warning about
them so the book is about people 14

00:00:53.040 --> 00:00:53.050
them so the book is about people 14
 

00:00:53.050 --> 00:00:55.650
them so the book is about people 14
people 7 who we know were Cassandra's

00:00:55.650 --> 00:00:55.660
people 7 who we know were Cassandra's
 

00:00:55.660 --> 00:00:58.560
people 7 who we know were Cassandra's
and 7 who we are examining to find out

00:00:58.560 --> 00:00:58.570
and 7 who we are examining to find out
 

00:00:58.570 --> 00:01:01.410
and 7 who we are examining to find out
if they are usually Cassandra's are

00:01:01.410 --> 00:01:01.420
if they are usually Cassandra's are
 

00:01:01.420 --> 00:01:04.229
if they are usually Cassandra's are
people who are not directly involved in

00:01:04.229 --> 00:01:04.239
people who are not directly involved in
 

00:01:04.239 --> 00:01:06.840
people who are not directly involved in
the thing that they worry about there

00:01:06.840 --> 00:01:06.850
the thing that they worry about there
 

00:01:06.850 --> 00:01:08.850
the thing that they worry about there
are people who observe it and there are

00:01:08.850 --> 00:01:08.860
are people who observe it and there are
 

00:01:08.860 --> 00:01:11.370
are people who observe it and there are
people who study it but in the case of

00:01:11.370 --> 00:01:11.380
people who study it but in the case of
 

00:01:11.380 --> 00:01:14.070
people who study it but in the case of
Jennifer Duden them at the University of

00:01:14.070 --> 00:01:14.080
Jennifer Duden them at the University of
 

00:01:14.080 --> 00:01:16.980
Jennifer Duden them at the University of
California Berkeley she's the person who

00:01:16.980 --> 00:01:16.990
California Berkeley she's the person who
 

00:01:16.990 --> 00:01:20.430
California Berkeley she's the person who
created it and she's also our Cassandra

00:01:20.430 --> 00:01:20.440
created it and she's also our Cassandra
 

00:01:20.440 --> 00:01:23.880
created it and she's also our Cassandra
the it in this case is crisper cast line

00:01:23.880 --> 00:01:23.890
the it in this case is crisper cast line
 

00:01:23.890 --> 00:01:27.330
the it in this case is crisper cast line
a method that she invented and I'm sure

00:01:27.330 --> 00:01:27.340
a method that she invented and I'm sure
 

00:01:27.340 --> 00:01:30.500
a method that she invented and I'm sure
someday we'll get a Nobel Prize for a

00:01:30.500 --> 00:01:30.510
someday we'll get a Nobel Prize for a
 

00:01:30.510 --> 00:01:34.410
someday we'll get a Nobel Prize for a
method of doing gene editing that allows

00:01:34.410 --> 00:01:34.420
method of doing gene editing that allows
 

00:01:34.420 --> 00:01:38.399
method of doing gene editing that allows
for removal of genetic defects in the

00:01:38.399 --> 00:01:38.409
for removal of genetic defects in the
 

00:01:38.409 --> 00:01:43.789
for removal of genetic defects in the
strain or addition into a strain of new

00:01:43.789 --> 00:01:43.799
strain or addition into a strain of new
 

00:01:43.799 --> 00:01:47.969
strain or addition into a strain of new
capabilities now this is going to

00:01:47.969 --> 00:01:47.979
capabilities now this is going to
 

00:01:47.979 --> 00:01:50.460
capabilities now this is going to
revolutionize human life it's already

00:01:50.460 --> 00:01:50.470
revolutionize human life it's already
 

00:01:50.470 --> 00:01:52.950
revolutionize human life it's already
beginning it's going to mean that all

00:01:52.950 --> 00:01:52.960
beginning it's going to mean that all
 

00:01:52.960 --> 00:01:55.740
beginning it's going to mean that all
the genetic defects that have caused so

00:01:55.740 --> 00:01:55.750
the genetic defects that have caused so
 

00:01:55.750 --> 00:01:58.160
the genetic defects that have caused so
much pain and suffering for people for

00:01:58.160 --> 00:01:58.170
much pain and suffering for people for
 

00:01:58.170 --> 00:02:01.260
much pain and suffering for people for
millions of years all of that could

00:02:01.260 --> 00:02:01.270
millions of years all of that could
 

00:02:01.270 --> 00:02:06.899
millions of years all of that could
potentially be removed so why does the

00:02:06.899 --> 00:02:06.909
potentially be removed so why does the
 

00:02:06.909 --> 00:02:10.290
potentially be removed so why does the
great woman who invented this wake up in

00:02:10.290 --> 00:02:10.300
great woman who invented this wake up in
 

00:02:10.300 --> 00:02:11.520
great woman who invented this wake up in
the middle of the night worrying about

00:02:11.520 --> 00:02:11.530
the middle of the night worrying about
 

00:02:11.530 --> 00:02:14.690
the middle of the night worrying about
it what she told us was

00:02:14.690 --> 00:02:14.700
it what she told us was
 

00:02:14.700 --> 00:02:17.000
it what she told us was
she's afraid that she might have become

00:02:17.000 --> 00:02:17.010
she's afraid that she might have become
 

00:02:17.010 --> 00:02:20.990
she's afraid that she might have become
dr. Frankenstein that the technique that

00:02:20.990 --> 00:02:21.000
dr. Frankenstein that the technique that
 

00:02:21.000 --> 00:02:23.630
dr. Frankenstein that the technique that
she developed could be misused in

00:02:23.630 --> 00:02:23.640
she developed could be misused in
 

00:02:23.640 --> 00:02:26.630
she developed could be misused in
horrible ways it could be misused for

00:02:26.630 --> 00:02:26.640
horrible ways it could be misused for
 

00:02:26.640 --> 00:02:30.559
horrible ways it could be misused for
example to create biological weapons to

00:02:30.559 --> 00:02:30.569
example to create biological weapons to
 

00:02:30.569 --> 00:02:33.620
example to create biological weapons to
create new forms of threats to human

00:02:33.620 --> 00:02:33.630
create new forms of threats to human
 

00:02:33.630 --> 00:02:36.380
create new forms of threats to human
beings threats for which we don't have

00:02:36.380 --> 00:02:36.390
beings threats for which we don't have
 

00:02:36.390 --> 00:02:40.820
beings threats for which we don't have
any known antidote or it could simply be

00:02:40.820 --> 00:02:40.830
any known antidote or it could simply be
 

00:02:40.830 --> 00:02:44.180
any known antidote or it could simply be
used to create human beings of far

00:02:44.180 --> 00:02:44.190
used to create human beings of far
 

00:02:44.190 --> 00:02:48.620
used to create human beings of far
superior capability not just taking

00:02:48.620 --> 00:02:48.630
superior capability not just taking
 

00:02:48.630 --> 00:02:52.550
superior capability not just taking
genes and removing defects but adding

00:02:52.550 --> 00:02:52.560
genes and removing defects but adding
 

00:02:52.560 --> 00:02:57.710
genes and removing defects but adding
new super capabilities so one scenario

00:02:57.710 --> 00:02:57.720
new super capabilities so one scenario
 

00:02:57.720 --> 00:03:00.830
new super capabilities so one scenario
we discussed with her was one if the

00:03:00.830 --> 00:03:00.840
we discussed with her was one if the
 

00:03:00.840 --> 00:03:03.650
we discussed with her was one if the
North Koreans or the Chinese decided

00:03:03.650 --> 00:03:03.660
North Koreans or the Chinese decided
 

00:03:03.660 --> 00:03:06.009
North Koreans or the Chinese decided
that they would create super soldiers

00:03:06.009 --> 00:03:06.019
that they would create super soldiers
 

00:03:06.019 --> 00:03:09.740
that they would create super soldiers
physically large people with great

00:03:09.740 --> 00:03:09.750
physically large people with great
 

00:03:09.750 --> 00:03:12.590
physically large people with great
athletic ability designed to be soldiers

00:03:12.590 --> 00:03:12.600
athletic ability designed to be soldiers
 

00:03:12.600 --> 00:03:15.770
athletic ability designed to be soldiers
designed to be aggressive designed to be

00:03:15.770 --> 00:03:15.780
designed to be aggressive designed to be
 

00:03:15.780 --> 00:03:18.590
designed to be aggressive designed to be
able to fight for long periods of time

00:03:18.590 --> 00:03:18.600
able to fight for long periods of time
 

00:03:18.600 --> 00:03:22.789
able to fight for long periods of time
or once they simply created people who

00:03:22.789 --> 00:03:22.799
or once they simply created people who
 

00:03:22.799 --> 00:03:24.680
or once they simply created people who
were brilliant at computer programming

00:03:24.680 --> 00:03:24.690
were brilliant at computer programming
 

00:03:24.690 --> 00:03:30.319
were brilliant at computer programming
and had IQs off the charts what if in

00:03:30.319 --> 00:03:30.329
and had IQs off the charts what if in
 

00:03:30.329 --> 00:03:33.160
and had IQs off the charts what if in
the process of that kind of gene editing

00:03:33.160 --> 00:03:33.170
the process of that kind of gene editing
 

00:03:33.170 --> 00:03:38.240
the process of that kind of gene editing
we created a caste society where some

00:03:38.240 --> 00:03:38.250
we created a caste society where some
 

00:03:38.250 --> 00:03:40.849
we created a caste society where some
people were genetically designed to do

00:03:40.849 --> 00:03:40.859
people were genetically designed to do
 

00:03:40.859 --> 00:03:44.090
people were genetically designed to do
menial tasks and didn't have the

00:03:44.090 --> 00:03:44.100
menial tasks and didn't have the
 

00:03:44.100 --> 00:03:45.710
menial tasks and didn't have the
capability of doing anything else

00:03:45.710 --> 00:03:45.720
capability of doing anything else
 

00:03:45.720 --> 00:03:49.849
capability of doing anything else
and other people were designed to be the

00:03:49.849 --> 00:03:49.859
and other people were designed to be the
 

00:03:49.859 --> 00:03:55.520
and other people were designed to be the
rulers with huge IQs and the capability

00:03:55.520 --> 00:03:55.530
rulers with huge IQs and the capability
 

00:03:55.530 --> 00:03:58.210
rulers with huge IQs and the capability
of understanding things beyond the pale

00:03:58.210 --> 00:03:58.220
of understanding things beyond the pale
 

00:03:58.220 --> 00:04:02.690
of understanding things beyond the pale
for lesser humans that's something that

00:04:02.690 --> 00:04:02.700
for lesser humans that's something that
 

00:04:02.700 --> 00:04:05.660
for lesser humans that's something that
scared the creator of Chris Burke as 9

00:04:05.660 --> 00:04:05.670
scared the creator of Chris Burke as 9
 

00:04:05.670 --> 00:04:09.099
scared the creator of Chris Burke as 9
and it scared us when we heard

00:04:09.099 --> 00:04:09.109
and it scared us when we heard
 

00:04:09.109 --> 00:04:12.710
and it scared us when we heard
Jennifer's story we asked ourselves does

00:04:12.710 --> 00:04:12.720
Jennifer's story we asked ourselves does
 

00:04:12.720 --> 00:04:17.509
Jennifer's story we asked ourselves does
she fit the template of a Cassandra that

00:04:17.509 --> 00:04:17.519
she fit the template of a Cassandra that
 

00:04:17.519 --> 00:04:18.920
she fit the template of a Cassandra that
we developed in the first half of the

00:04:18.920 --> 00:04:18.930
we developed in the first half of the
 

00:04:18.930 --> 00:04:22.640
we developed in the first half of the
book looking at the the first seven is

00:04:22.640 --> 00:04:22.650
book looking at the the first seven is
 

00:04:22.650 --> 00:04:25.339
book looking at the the first seven is
she an expert absolutely she is the

00:04:25.339 --> 00:04:25.349
she an expert absolutely she is the
 

00:04:25.349 --> 00:04:27.820
she an expert absolutely she is the
expert she created it

00:04:27.820 --> 00:04:27.830
expert she created it
 

00:04:27.830 --> 00:04:31.270
expert she created it
is she dated driven yes she has a wealth

00:04:31.270 --> 00:04:31.280
is she dated driven yes she has a wealth
 

00:04:31.280 --> 00:04:33.999
is she dated driven yes she has a wealth
of data on Christopher Castle on and

00:04:33.999 --> 00:04:34.009
of data on Christopher Castle on and
 

00:04:34.009 --> 00:04:35.110
of data on Christopher Castle on and
what it can do

00:04:35.110 --> 00:04:35.120
what it can do
 

00:04:35.120 --> 00:04:37.930
what it can do
is she predicting something that is

00:04:37.930 --> 00:04:37.940
is she predicting something that is
 

00:04:37.940 --> 00:04:39.850
is she predicting something that is
first a current syndrome something

00:04:39.850 --> 00:04:39.860
first a current syndrome something
 

00:04:39.860 --> 00:04:42.339
first a current syndrome something
that's never happened before and the

00:04:42.339 --> 00:04:42.349
that's never happened before and the
 

00:04:42.349 --> 00:04:44.800
that's never happened before and the
answer that is yes is it kind of

00:04:44.800 --> 00:04:44.810
answer that is yes is it kind of
 

00:04:44.810 --> 00:04:46.869
answer that is yes is it kind of
outlandish it's the stuff of Hollywood

00:04:46.869 --> 00:04:46.879
outlandish it's the stuff of Hollywood
 

00:04:46.879 --> 00:04:50.439
outlandish it's the stuff of Hollywood
fiction yes it is what about the

00:04:50.439 --> 00:04:50.449
fiction yes it is what about the
 

00:04:50.449 --> 00:04:53.830
fiction yes it is what about the
audience the decision-maker one of the

00:04:53.830 --> 00:04:53.840
audience the decision-maker one of the
 

00:04:53.840 --> 00:04:55.629
audience the decision-maker one of the
things we saw with the earlier

00:04:55.629 --> 00:04:55.639
things we saw with the earlier
 

00:04:55.639 --> 00:04:58.300
things we saw with the earlier
Cassandra's was wasn't always clear that

00:04:58.300 --> 00:04:58.310
Cassandra's was wasn't always clear that
 

00:04:58.310 --> 00:05:01.360
Cassandra's was wasn't always clear that
there was a decision-maker people always

00:05:01.360 --> 00:05:01.370
there was a decision-maker people always
 

00:05:01.370 --> 00:05:03.820
there was a decision-maker people always
pointed at each other saying that's your

00:05:03.820 --> 00:05:03.830
pointed at each other saying that's your
 

00:05:03.830 --> 00:05:06.820
pointed at each other saying that's your
job or at least it's not my job and in

00:05:06.820 --> 00:05:06.830
job or at least it's not my job and in
 

00:05:06.830 --> 00:05:10.029
job or at least it's not my job and in
this case making decisions about what

00:05:10.029 --> 00:05:10.039
this case making decisions about what
 

00:05:10.039 --> 00:05:13.240
this case making decisions about what
gene-editing can happen and can't happen

00:05:13.240 --> 00:05:13.250
gene-editing can happen and can't happen
 

00:05:13.250 --> 00:05:15.820
gene-editing can happen and can't happen
and enforcing that as a matter of law

00:05:15.820 --> 00:05:15.830
and enforcing that as a matter of law
 

00:05:15.830 --> 00:05:18.879
and enforcing that as a matter of law
and international law it's not all clear

00:05:18.879 --> 00:05:18.889
and international law it's not all clear
 

00:05:18.889 --> 00:05:23.020
and international law it's not all clear
whose job that is one of those issues we

00:05:23.020 --> 00:05:23.030
whose job that is one of those issues we
 

00:05:23.030 --> 00:05:25.200
whose job that is one of those issues we
looked at was artificial intelligence

00:05:25.200 --> 00:05:25.210
looked at was artificial intelligence
 

00:05:25.210 --> 00:05:28.290
looked at was artificial intelligence
now frankly my co-author RP Eddie and I

00:05:28.290 --> 00:05:28.300
now frankly my co-author RP Eddie and I
 

00:05:28.300 --> 00:05:30.279
now frankly my co-author RP Eddie and I
disagreed about whether or not to do

00:05:30.279 --> 00:05:30.289
disagreed about whether or not to do
 

00:05:30.289 --> 00:05:33.040
disagreed about whether or not to do
artificial intelligence I said I don't

00:05:33.040 --> 00:05:33.050
artificial intelligence I said I don't
 

00:05:33.050 --> 00:05:35.740
artificial intelligence I said I don't
think this is a problem after all if a

00:05:35.740 --> 00:05:35.750
think this is a problem after all if a
 

00:05:35.750 --> 00:05:38.040
think this is a problem after all if a
computer acts up you can unplug it

00:05:38.040 --> 00:05:38.050
computer acts up you can unplug it
 

00:05:38.050 --> 00:05:40.480
computer acts up you can unplug it
obviously I didn't understand the issue

00:05:40.480 --> 00:05:40.490
obviously I didn't understand the issue
 

00:05:40.490 --> 00:05:44.379
obviously I didn't understand the issue
and the way that my co-author RP Eddie

00:05:44.379 --> 00:05:44.389
and the way that my co-author RP Eddie
 

00:05:44.389 --> 00:05:46.959
and the way that my co-author RP Eddie
convinced me that we should look for

00:05:46.959 --> 00:05:46.969
convinced me that we should look for
 

00:05:46.969 --> 00:05:50.860
convinced me that we should look for
someone on this issue was by saying who

00:05:50.860 --> 00:05:50.870
someone on this issue was by saying who
 

00:05:50.870 --> 00:05:53.230
someone on this issue was by saying who
are the people who are talking about

00:05:53.230 --> 00:05:53.240
are the people who are talking about
 

00:05:53.240 --> 00:05:57.070
are the people who are talking about
this today not the experts in AI the

00:05:57.070 --> 00:05:57.080
this today not the experts in AI the
 

00:05:57.080 --> 00:05:58.839
this today not the experts in AI the
people who are generally concerned about

00:05:58.839 --> 00:05:58.849
people who are generally concerned about
 

00:05:58.849 --> 00:06:02.860
people who are generally concerned about
it and who were they Bill Gates the

00:06:02.860 --> 00:06:02.870
it and who were they Bill Gates the
 

00:06:02.870 --> 00:06:05.290
it and who were they Bill Gates the
founder of Microsoft Elon Musk the

00:06:05.290 --> 00:06:05.300
founder of Microsoft Elon Musk the
 

00:06:05.300 --> 00:06:08.290
founder of Microsoft Elon Musk the
founder of Tesla Steven Hawkings the

00:06:08.290 --> 00:06:08.300
founder of Tesla Steven Hawkings the
 

00:06:08.300 --> 00:06:12.309
founder of Tesla Steven Hawkings the
great physicist from Oxford and when I

00:06:12.309 --> 00:06:12.319
great physicist from Oxford and when I
 

00:06:12.319 --> 00:06:15.399
great physicist from Oxford and when I
heard that I said okay fine maybe those

00:06:15.399 --> 00:06:15.409
heard that I said okay fine maybe those
 

00:06:15.409 --> 00:06:17.709
heard that I said okay fine maybe those
guys think this is a problem maybe we

00:06:17.709 --> 00:06:17.719
guys think this is a problem maybe we
 

00:06:17.719 --> 00:06:19.809
guys think this is a problem maybe we
should look for the expert who is

00:06:19.809 --> 00:06:19.819
should look for the expert who is
 

00:06:19.819 --> 00:06:22.450
should look for the expert who is
predicting that this could be a future

00:06:22.450 --> 00:06:22.460
predicting that this could be a future
 

00:06:22.460 --> 00:06:26.230
predicting that this could be a future
disaster and we found Elliott Kowski who

00:06:26.230 --> 00:06:26.240
disaster and we found Elliott Kowski who
 

00:06:26.240 --> 00:06:28.600
disaster and we found Elliott Kowski who
not only thinks this could become a

00:06:28.600 --> 00:06:28.610
not only thinks this could become a
 

00:06:28.610 --> 00:06:31.719
not only thinks this could become a
disaster he's dedicated his life and all

00:06:31.719 --> 00:06:31.729
disaster he's dedicated his life and all
 

00:06:31.729 --> 00:06:34.240
disaster he's dedicated his life and all
of his work to dealing with the future

00:06:34.240 --> 00:06:34.250
of his work to dealing with the future
 

00:06:34.250 --> 00:06:35.879
of his work to dealing with the future
threat of artificial intelligence

00:06:35.879 --> 00:06:35.889
threat of artificial intelligence
 

00:06:35.889 --> 00:06:38.649
threat of artificial intelligence
because he doesn't think it's inevitable

00:06:38.649 --> 00:06:38.659
because he doesn't think it's inevitable
 

00:06:38.659 --> 00:06:41.540
because he doesn't think it's inevitable
that artificial intelligence should

00:06:41.540 --> 00:06:41.550
that artificial intelligence should
 

00:06:41.550 --> 00:06:44.240
that artificial intelligence should
a problem but he does have a scenario

00:06:44.240 --> 00:06:44.250
a problem but he does have a scenario
 

00:06:44.250 --> 00:06:47.690
a problem but he does have a scenario
whereby it could be if we don't do some

00:06:47.690 --> 00:06:47.700
whereby it could be if we don't do some
 

00:06:47.700 --> 00:06:50.360
whereby it could be if we don't do some
of the things he has in mind what's the

00:06:50.360 --> 00:06:50.370
of the things he has in mind what's the
 

00:06:50.370 --> 00:06:54.260
of the things he has in mind what's the
problem problem could be that artificial

00:06:54.260 --> 00:06:54.270
problem problem could be that artificial
 

00:06:54.270 --> 00:06:58.120
problem problem could be that artificial
intelligence starts writing software

00:06:58.120 --> 00:06:58.130
intelligence starts writing software
 

00:06:58.130 --> 00:07:01.190
intelligence starts writing software
complex software maybe even encrypted

00:07:01.190 --> 00:07:01.200
complex software maybe even encrypted
 

00:07:01.200 --> 00:07:03.890
complex software maybe even encrypted
software that human beings do not

00:07:03.890 --> 00:07:03.900
software that human beings do not
 

00:07:03.900 --> 00:07:08.780
software that human beings do not
understand and can't deal with that

00:07:08.780 --> 00:07:08.790
understand and can't deal with that
 

00:07:08.790 --> 00:07:12.380
understand and can't deal with that
future is just around the corner already

00:07:12.380 --> 00:07:12.390
future is just around the corner already
 

00:07:12.390 --> 00:07:15.430
future is just around the corner already
we have software writing software

00:07:15.430 --> 00:07:15.440
we have software writing software
 

00:07:15.440 --> 00:07:18.620
we have software writing software
already at Google we have artificial

00:07:18.620 --> 00:07:18.630
already at Google we have artificial
 

00:07:18.630 --> 00:07:20.930
already at Google we have artificial
intelligence writing software for

00:07:20.930 --> 00:07:20.940
intelligence writing software for
 

00:07:20.940 --> 00:07:24.890
intelligence writing software for
further artificial intelligence and the

00:07:24.890 --> 00:07:24.900
further artificial intelligence and the
 

00:07:24.900 --> 00:07:26.390
further artificial intelligence and the
Google program is getting to the point

00:07:26.390 --> 00:07:26.400
Google program is getting to the point
 

00:07:26.400 --> 00:07:27.950
Google program is getting to the point
where they're afraid they don't fully

00:07:27.950 --> 00:07:27.960
where they're afraid they don't fully
 

00:07:27.960 --> 00:07:31.310
where they're afraid they don't fully
understand how it's doing what it's

00:07:31.310 --> 00:07:31.320
understand how it's doing what it's
 

00:07:31.320 --> 00:07:34.160
understand how it's doing what it's
doing but only eight cows keep fears

00:07:34.160 --> 00:07:34.170
doing but only eight cows keep fears
 

00:07:34.170 --> 00:07:37.580
doing but only eight cows keep fears
most is that super intelligence will

00:07:37.580 --> 00:07:37.590
most is that super intelligence will
 

00:07:37.590 --> 00:07:38.510
most is that super intelligence will
come into existence

00:07:38.510 --> 00:07:38.520
come into existence
 

00:07:38.520 --> 00:07:41.750
come into existence
that means artificial intelligence

00:07:41.750 --> 00:07:41.760
that means artificial intelligence
 

00:07:41.760 --> 00:07:45.710
that means artificial intelligence
programs that are significantly smarter

00:07:45.710 --> 00:07:45.720
programs that are significantly smarter
 

00:07:45.720 --> 00:07:49.040
programs that are significantly smarter
than human intelligence and even human

00:07:49.040 --> 00:07:49.050
than human intelligence and even human
 

00:07:49.050 --> 00:07:50.750
than human intelligence and even human
intelligence today augmented by

00:07:50.750 --> 00:07:50.760
intelligence today augmented by
 

00:07:50.760 --> 00:07:54.320
intelligence today augmented by
computers and what he sees as possible

00:07:54.320 --> 00:07:54.330
computers and what he sees as possible
 

00:07:54.330 --> 00:07:56.720
computers and what he sees as possible
looking at the rate of advanced in

00:07:56.720 --> 00:07:56.730
looking at the rate of advanced in
 

00:07:56.730 --> 00:07:59.660
looking at the rate of advanced in
technology is that this will not be a

00:07:59.660 --> 00:07:59.670
technology is that this will not be a
 

00:07:59.670 --> 00:08:03.140
technology is that this will not be a
linear growth in the capabilities of

00:08:03.140 --> 00:08:03.150
linear growth in the capabilities of
 

00:08:03.150 --> 00:08:07.760
linear growth in the capabilities of
software but it could be overnight one

00:08:07.760 --> 00:08:07.770
software but it could be overnight one
 

00:08:07.770 --> 00:08:08.240
software but it could be overnight one
day

00:08:08.240 --> 00:08:08.250
day
 

00:08:08.250 --> 00:08:10.430
day
artificial intelligence might be under

00:08:10.430 --> 00:08:10.440
artificial intelligence might be under
 

00:08:10.440 --> 00:08:12.710
artificial intelligence might be under
the control of human beings and the next

00:08:12.710 --> 00:08:12.720
the control of human beings and the next
 

00:08:12.720 --> 00:08:15.380
the control of human beings and the next
day it might have jumped into super

00:08:15.380 --> 00:08:15.390
day it might have jumped into super
 

00:08:15.390 --> 00:08:17.840
day it might have jumped into super
intelligence far more capable than

00:08:17.840 --> 00:08:17.850
intelligence far more capable than
 

00:08:17.850 --> 00:08:20.290
intelligence far more capable than
anything we could possibly understand if

00:08:20.290 --> 00:08:20.300
anything we could possibly understand if
 

00:08:20.300 --> 00:08:23.150
anything we could possibly understand if
you then put artificial intelligence

00:08:23.150 --> 00:08:23.160
you then put artificial intelligence
 

00:08:23.160 --> 00:08:26.300
you then put artificial intelligence
onto networks that are running critical

00:08:26.300 --> 00:08:26.310
onto networks that are running critical
 

00:08:26.310 --> 00:08:29.470
onto networks that are running critical
infrastructure the Internet of Things

00:08:29.470 --> 00:08:29.480
infrastructure the Internet of Things
 

00:08:29.480 --> 00:08:32.680
infrastructure the Internet of Things
another subject we look at in the book

00:08:32.680 --> 00:08:32.690
another subject we look at in the book
 

00:08:32.690 --> 00:08:34.790
another subject we look at in the book
it's possible in the worst case scenario

00:08:34.790 --> 00:08:34.800
it's possible in the worst case scenario
 

00:08:34.800 --> 00:08:38.300
it's possible in the worst case scenario
that human beings will lose control of

00:08:38.300 --> 00:08:38.310
that human beings will lose control of
 

00:08:38.310 --> 00:08:43.280
that human beings will lose control of
the infrastructure of society in even

00:08:43.280 --> 00:08:43.290
the infrastructure of society in even
 

00:08:43.290 --> 00:08:45.790
the infrastructure of society in even
worse case scenarios than that

00:08:45.790 --> 00:08:45.800
worse case scenarios than that
 

00:08:45.800 --> 00:08:48.020
worse case scenarios than that
artificial intelligence will decide he

00:08:48.020 --> 00:08:48.030
artificial intelligence will decide he
 

00:08:48.030 --> 00:08:51.500
artificial intelligence will decide he
doesn't need humans at all and it is

00:08:51.500 --> 00:08:51.510
doesn't need humans at all and it is
 

00:08:51.510 --> 00:08:54.620
doesn't need humans at all and it is
that fear that causes him

00:08:54.620 --> 00:08:54.630
that fear that causes him
 

00:08:54.630 --> 00:08:59.150
that fear that causes him
propose that we agree now as a planet as

00:08:59.150 --> 00:08:59.160
propose that we agree now as a planet as
 

00:08:59.160 --> 00:09:01.160
propose that we agree now as a planet as
a number of different countries and

00:09:01.160 --> 00:09:01.170
a number of different countries and
 

00:09:01.170 --> 00:09:04.670
a number of different countries and
societies to put limits on the

00:09:04.670 --> 00:09:04.680
societies to put limits on the
 

00:09:04.680 --> 00:09:06.020
societies to put limits on the
development of artificial intelligence

00:09:06.020 --> 00:09:06.030
development of artificial intelligence
 

00:09:06.030 --> 00:09:08.930
development of artificial intelligence
and to do that by international treaty

00:09:08.930 --> 00:09:08.940
and to do that by international treaty
 

00:09:08.940 --> 00:09:12.050
and to do that by international treaty
and to have observation to make sure

00:09:12.050 --> 00:09:12.060
and to have observation to make sure
 

00:09:12.060 --> 00:09:14.120
and to have observation to make sure
that artificial intelligence doesn't

00:09:14.120 --> 00:09:14.130
that artificial intelligence doesn't
 

00:09:14.130 --> 00:09:18.200
that artificial intelligence doesn't
break out of predetermined limits agreed

00:09:18.200 --> 00:09:18.210
break out of predetermined limits agreed
 

00:09:18.210 --> 00:09:21.410
break out of predetermined limits agreed
by human beings and their governments

00:09:21.410 --> 00:09:21.420
by human beings and their governments
 

00:09:21.420 --> 00:09:25.190
by human beings and their governments
now you've seen that plot before you've

00:09:25.190 --> 00:09:25.200
now you've seen that plot before you've
 

00:09:25.200 --> 00:09:27.520
now you've seen that plot before you've
seen that in a Hollywood movie and

00:09:27.520 --> 00:09:27.530
seen that in a Hollywood movie and
 

00:09:27.530 --> 00:09:31.010
seen that in a Hollywood movie and
that's part of the problem with so many

00:09:31.010 --> 00:09:31.020
that's part of the problem with so many
 

00:09:31.020 --> 00:09:33.740
that's part of the problem with so many
of the possible Cassandra's then we

00:09:33.740 --> 00:09:33.750
of the possible Cassandra's then we
 

00:09:33.750 --> 00:09:37.700
of the possible Cassandra's then we
looked at today that humans have seen

00:09:37.700 --> 00:09:37.710
looked at today that humans have seen
 

00:09:37.710 --> 00:09:41.060
looked at today that humans have seen
these threats before they saw them in

00:09:41.060 --> 00:09:41.070
these threats before they saw them in
 

00:09:41.070 --> 00:09:44.030
these threats before they saw them in
science fiction so whether it's the

00:09:44.030 --> 00:09:44.040
science fiction so whether it's the
 

00:09:44.040 --> 00:09:46.550
science fiction so whether it's the
possibility of an asteroid hitting the

00:09:46.550 --> 00:09:46.560
possibility of an asteroid hitting the
 

00:09:46.560 --> 00:09:50.210
possibility of an asteroid hitting the
earth or human beings being genetically

00:09:50.210 --> 00:09:50.220
earth or human beings being genetically
 

00:09:50.220 --> 00:09:52.880
earth or human beings being genetically
engineered or artificial intelligence

00:09:52.880 --> 00:09:52.890
engineered or artificial intelligence
 

00:09:52.890 --> 00:09:56.090
engineered or artificial intelligence
taking over part of the reason we don't

00:09:56.090 --> 00:09:56.100
taking over part of the reason we don't
 

00:09:56.100 --> 00:09:58.240
taking over part of the reason we don't
take these Cassandra's seriously is

00:09:58.240 --> 00:09:58.250
take these Cassandra's seriously is
 

00:09:58.250 --> 00:10:01.940
take these Cassandra's seriously is
we've seen it in the movies we've seen

00:10:01.940 --> 00:10:01.950
we've seen it in the movies we've seen
 

00:10:01.950 --> 00:10:04.790
we've seen it in the movies we've seen
it in science fiction a corollary issue

00:10:04.790 --> 00:10:04.800
it in science fiction a corollary issue
 

00:10:04.800 --> 00:10:07.310
it in science fiction a corollary issue
to artificial intelligence is the rise

00:10:07.310 --> 00:10:07.320
to artificial intelligence is the rise
 

00:10:07.320 --> 00:10:10.220
to artificial intelligence is the rise
of robotics and already in this country

00:10:10.220 --> 00:10:10.230
of robotics and already in this country
 

00:10:10.230 --> 00:10:14.120
of robotics and already in this country
we're hearing debates about the

00:10:14.120 --> 00:10:14.130
we're hearing debates about the
 

00:10:14.130 --> 00:10:16.670
we're hearing debates about the
possibility that the next wave in

00:10:16.670 --> 00:10:16.680
possibility that the next wave in
 

00:10:16.680 --> 00:10:19.730
possibility that the next wave in
automation rather than just shifting

00:10:19.730 --> 00:10:19.740
automation rather than just shifting
 

00:10:19.740 --> 00:10:22.610
automation rather than just shifting
jobs from one function to another which

00:10:22.610 --> 00:10:22.620
jobs from one function to another which
 

00:10:22.620 --> 00:10:24.230
jobs from one function to another which
has been what's happened in the past

00:10:24.230 --> 00:10:24.240
has been what's happened in the past
 

00:10:24.240 --> 00:10:27.410
has been what's happened in the past
with automation may be the next wave of

00:10:27.410 --> 00:10:27.420
with automation may be the next wave of
 

00:10:27.420 --> 00:10:29.870
with automation may be the next wave of
automation would be far more advanced

00:10:29.870 --> 00:10:29.880
automation would be far more advanced
 

00:10:29.880 --> 00:10:32.600
automation would be far more advanced
and complex and actually throw humans

00:10:32.600 --> 00:10:32.610
and complex and actually throw humans
 

00:10:32.610 --> 00:10:34.970
and complex and actually throw humans
out of work it's a debate that's going

00:10:34.970 --> 00:10:34.980
out of work it's a debate that's going
 

00:10:34.980 --> 00:10:36.890
out of work it's a debate that's going
on and we don't know who's right

00:10:36.890 --> 00:10:36.900
on and we don't know who's right
 

00:10:36.900 --> 00:10:40.130
on and we don't know who's right
some people say people will be thrown

00:10:40.130 --> 00:10:40.140
some people say people will be thrown
 

00:10:40.140 --> 00:10:42.470
some people say people will be thrown
out of work and there will be less need

00:10:42.470 --> 00:10:42.480
out of work and there will be less need
 

00:10:42.480 --> 00:10:45.440
out of work and there will be less need
for humans to do work and we'll have to

00:10:45.440 --> 00:10:45.450
for humans to do work and we'll have to
 

00:10:45.450 --> 00:10:48.950
for humans to do work and we'll have to
pay humans for doing nothing tacks

00:10:48.950 --> 00:10:48.960
pay humans for doing nothing tacks
 

00:10:48.960 --> 00:10:52.340
pay humans for doing nothing tacks
computers is one proposal tax robots is

00:10:52.340 --> 00:10:52.350
computers is one proposal tax robots is
 

00:10:52.350 --> 00:10:55.310
computers is one proposal tax robots is
a proposal and the other theory is that

00:10:55.310 --> 00:10:55.320
a proposal and the other theory is that
 

00:10:55.320 --> 00:10:57.860
a proposal and the other theory is that
just as in the past when technology

00:10:57.860 --> 00:10:57.870
just as in the past when technology
 

00:10:57.870 --> 00:11:01.790
just as in the past when technology
advances it may displace certain jobs

00:11:01.790 --> 00:11:01.800
advances it may displace certain jobs
 

00:11:01.800 --> 00:11:04.210
advances it may displace certain jobs
but it will create new ones

00:11:04.210 --> 00:11:04.220
but it will create new ones
 

00:11:04.220 --> 00:11:07.940
but it will create new ones
we don't know who's right there but we

00:11:07.940 --> 00:11:07.950
we don't know who's right there but we
 

00:11:07.950 --> 00:11:08.569
we don't know who's right there but we
do know

00:11:08.569 --> 00:11:08.579
do know
 

00:11:08.579 --> 00:11:11.530
do know
all of our future Cassandra's are our

00:11:11.530 --> 00:11:11.540
all of our future Cassandra's are our
 

00:11:11.540 --> 00:11:13.910
all of our future Cassandra's are our
present-day Cassandra's predicting

00:11:13.910 --> 00:11:13.920
present-day Cassandra's predicting
 

00:11:13.920 --> 00:11:16.129
present-day Cassandra's predicting
things about the future that they need

00:11:16.129 --> 00:11:16.139
things about the future that they need
 

00:11:16.139 --> 00:11:18.799
things about the future that they need
to be listened to and there needs to be

00:11:18.799 --> 00:11:18.809
to be listened to and there needs to be
 

00:11:18.809 --> 00:11:22.160
to be listened to and there needs to be
examination of the theories that they're

00:11:22.160 --> 00:11:22.170
examination of the theories that they're
 

00:11:22.170 --> 00:11:23.780
examination of the theories that they're
putting forward in the data that they're

00:11:23.780 --> 00:11:23.790
putting forward in the data that they're
 

00:11:23.790 --> 00:11:27.169
putting forward in the data that they're
putting forward even if they are an

00:11:27.169 --> 00:11:27.179
putting forward even if they are an
 

00:11:27.179 --> 00:11:32.340
putting forward even if they are an
outlier a minority view among experts

00:11:32.340 --> 00:11:32.350
outlier a minority view among experts
 

00:11:32.350 --> 00:11:35.590
outlier a minority view among experts
[Music]

