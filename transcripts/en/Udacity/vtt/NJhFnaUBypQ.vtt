WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:02.790
Okay Michael so
to demonstrate this idea or

00:00:02.790 --> 00:00:05.870
to demonstrate some of these ideas
I'm going to look at grid worlds.

00:00:05.870 --> 00:00:10.280
One a very simple grid world
to try to get across a pretty

00:00:10.280 --> 00:00:12.370
straightforward idea, at least what I
hope's a pretty straightforward idea.

00:00:12.370 --> 00:00:16.160
And then a slightly more complicated and
prettier grid world in about a moment.

00:00:16.160 --> 00:00:18.550
But first let's look at
kind of a basic grid world.

00:00:18.550 --> 00:00:22.540
So this is like our famous four
by three grid world except I

00:00:22.540 --> 00:00:24.070
added a couple of details.

00:00:24.070 --> 00:00:29.600
I haven't drawn all the little
squares and all the little tiny grids.

00:00:29.600 --> 00:00:32.430
Because really, I just sort of
want to pop up a level here.

00:00:32.430 --> 00:00:35.660
But you start somewhere in this world,
it almost doesn't matter where,

00:00:35.660 --> 00:00:37.920
and you have to try to get here.

00:00:37.920 --> 00:00:41.920
To the little plus one that's where
all the reward in the world is.

00:00:41.920 --> 00:00:43.420
And you have the normal actions.

00:00:43.420 --> 00:00:45.760
You can move up, down, left, right.

00:00:45.760 --> 00:00:46.940
And you know it's slippery.

00:00:46.940 --> 00:00:49.620
So every once in a while when you
try to move up you end up moving in

00:00:49.620 --> 00:00:50.700
a perpendicular direction.

00:00:50.700 --> 00:00:53.510
And the usual sort of
thing that we normally do.

00:00:53.510 --> 00:00:57.420
Now other than that, what's different
about this world is that I've got walls,

00:00:57.420 --> 00:00:58.900
like we had before.

00:00:58.900 --> 00:01:04.099
Except these walls actually happened
to coincidentally create little rooms.

00:01:04.099 --> 00:01:07.610
So let's imagine that you're
starting over here somewhere and

00:01:07.610 --> 00:01:13.800
your goal is to eventually learn in
this world how to get to, well let's go.

00:01:13.800 --> 00:01:16.330
So I think we know how to do this.

00:01:16.330 --> 00:01:20.080
I think we've done this before many many
times we certainly did it in the four by

00:01:20.080 --> 00:01:24.580
three grid world but I'm going to ask
you a slightly different question.

00:01:24.580 --> 00:01:26.990
I want you to think about this
in a slightly different way.

00:01:26.990 --> 00:01:32.230
If you were trying to tell a person how

00:01:32.230 --> 00:01:36.430
you should get from this point to
this point what would you tell them?

00:01:36.430 --> 00:01:38.790
&gt;&gt; I'm going to say Google Maps.

00:01:38.790 --> 00:01:39.710
&gt;&gt; This is not Google Maps.

00:01:39.710 --> 00:01:41.400
This is in a GPS dead zone.

00:01:41.400 --> 00:01:42.850
&gt;&gt; What?
&gt;&gt; I know, I know it's amazing.

00:01:42.850 --> 00:01:45.630
&gt;&gt; Wow I guess that's one of the real
benefits of reinforcement learning

00:01:45.630 --> 00:01:48.690
is it can work even
even a GPS dead zone.

00:01:48.690 --> 00:01:50.580
So the question is how is this?

00:01:50.580 --> 00:01:51.670
How would you tell them to go?

00:01:51.670 --> 00:01:57.010
Well, so, you know,
we talk to agents via policies.

00:01:57.010 --> 00:01:58.790
So I could try to give a policy.

00:01:58.790 --> 00:01:59.720
But you didn't really get.

00:01:59.720 --> 00:02:01.280
So that's a mapping
from states to actions.

00:02:01.280 --> 00:02:05.090
But you don't really give me a lot of
details about what the states where.

00:02:05.090 --> 00:02:09.280
So probably if I was talking to a person
I would say something kind of high level

00:02:09.280 --> 00:02:14.150
like well pass through
the doorway to the east.

00:02:15.190 --> 00:02:19.010
Then you enter a room and
there's a doorway to the south.

00:02:19.010 --> 00:02:20.060
Go through that and

00:02:20.060 --> 00:02:24.099
then a little bit inside the doorway
you'll find a great reward.

00:02:25.210 --> 00:02:26.380
Or at least a reward.

00:02:26.380 --> 00:02:29.730
&gt;&gt; Plus one is the greatest reward
that anyone can receive Michael.

00:02:29.730 --> 00:02:30.585
You know that.

00:02:30.585 --> 00:02:32.430
&gt;&gt; [LAUGH]
&gt;&gt; So right and

00:02:32.430 --> 00:02:36.790
I like this because what you did is I
gave you the actions up down left right.

00:02:36.790 --> 00:02:40.310
And you could have said okay so
move right, right, right,

00:02:40.310 --> 00:02:44.300
right, right down right, right and then
go I don't know right, right down, down,

00:02:44.300 --> 00:02:48.660
down, down, down, down, down, down,
down, down write or something but

00:02:48.660 --> 00:02:54.490
instead you said well go to the doorway
and then go to another doorway and

00:02:54.490 --> 00:02:58.370
then once you're past the doorway, you
know you'll just take a couple steps and

00:02:58.370 --> 00:03:01.940
you'll end up where you want to
be where the great reward is.

00:03:01.940 --> 00:03:05.080
And that's perfectly reasonable in fact
it's more than perfectly reasonable it's

00:03:05.080 --> 00:03:07.960
a kind of abstraction in the same
way that function approximation is

00:03:07.960 --> 00:03:10.810
a sort of attraction of the states
which you just described

00:03:10.810 --> 00:03:13.528
is what we call temporal obstruction.

00:03:13.528 --> 00:03:14.142
&gt;&gt; I see.
&gt;&gt; And

00:03:14.142 --> 00:03:15.815
you know that has a specific meaning but

00:03:15.815 --> 00:03:18.902
I think to really kind of grasp what is
the right way to think about it is you

00:03:18.902 --> 00:03:21.520
came up with new actions
&gt;&gt; That's making the action

00:03:21.520 --> 00:03:22.340
space bigger.

00:03:22.340 --> 00:03:26.520
&gt;&gt; Making the action space bigger
which is a problem you would think but

00:03:26.520 --> 00:03:30.100
each of these actions actually
covers an enormous amount of ground.

00:03:30.100 --> 00:03:32.520
So in fact, if you had said right,
right, right, right, right, right,

00:03:32.520 --> 00:03:34.510
right, right, right, down, right, right,
right, right, right, right, right,

00:03:34.510 --> 00:03:36.610
right, right, down, down, down, down,
down, down, down, down, down, down,

00:03:36.610 --> 00:03:40.100
down, down, down, down, down, down,
down, right or something like that,

00:03:40.100 --> 00:03:43.510
that's a lot of actions to have to
reason over over a long period of time.

00:03:43.510 --> 00:03:46.970
But what you actually said was, take one
action, take another action, and then,

00:03:46.970 --> 00:03:48.470
I don't know, take one or two actions.

00:03:48.470 --> 00:03:52.020
So we managed to do with your
new actions in, I don't know,

00:03:52.020 --> 00:03:55.684
about five steps what would have
otherwise taken, I don't know,

00:03:55.684 --> 00:03:58.638
100 steps or maybe
&gt;&gt; That's cool idea.

00:03:58.638 --> 00:04:02.650
&gt;&gt; Maybe 60 steps that's right that is
where the abstraction comes from and

00:04:02.650 --> 00:04:04.800
where the temple nature comes from.

00:04:04.800 --> 00:04:08.040
Because again, one of the things that
makes reinforcement learning hard

00:04:08.040 --> 00:04:10.720
is this delayed reward, and
what's implicit in that is that you have

00:04:10.720 --> 00:04:15.170
to reason over time that you have
to take many, many steps and

00:04:15.170 --> 00:04:20.170
because all the options there are
several options at every state you're

00:04:20.170 --> 00:04:23.450
actually getting an exponential blowup
over all the paths you might take.

00:04:24.520 --> 00:04:27.490
And here what you've done is
you've traded off the price in

00:04:27.490 --> 00:04:30.870
adding new actions with
being able to skip over,

00:04:30.870 --> 00:04:33.060
having to make a bunch of
decisions along the way.

00:04:33.060 --> 00:04:35.720
And so now I can do in five steps
would have taken 100 steps and

00:04:35.720 --> 00:04:37.640
that's going to be a gigantic
exponential savings.

