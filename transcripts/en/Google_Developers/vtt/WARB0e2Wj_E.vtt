WEBVTT
Kind: captions
Language: en

00:00:11.682 --> 00:00:13.390
I'll be talking about
the work that we've

00:00:13.390 --> 00:00:16.630
been doing to make sure that Big
Data Open Source software runs

00:00:16.630 --> 00:00:20.290
smoothly and effectively
on Google Cloud Platform.

00:00:20.290 --> 00:00:23.710
Now, I want to begin with
the big questions underlying

00:00:23.710 --> 00:00:25.650
all of this, which
are what exactly it

00:00:25.650 --> 00:00:28.490
means to run Big Data Open
Source software on Google's

00:00:28.490 --> 00:00:31.950
Cloud, and secondly,
why that matters to you.

00:00:31.950 --> 00:00:36.742
Now, imagine you have an awesome
idea for a web application.

00:00:36.742 --> 00:00:39.200
Maybe you've been talking about
it for some time over lunch

00:00:39.200 --> 00:00:41.470
and you decide to go for it.

00:00:41.470 --> 00:00:44.790
You coded up over a weekend
and you hit the launch button.

00:00:44.790 --> 00:00:46.682
Well as it turns out,
the rest of the world

00:00:46.682 --> 00:00:47.890
thinks you're onto something.

00:00:47.890 --> 00:00:51.520
So, you know somebody posts
an article out to Reddit,

00:00:51.520 --> 00:00:53.780
a Slashdot article appears,
and before you know it,

00:00:53.780 --> 00:00:56.830
you have a million
new users overnight.

00:00:56.830 --> 00:00:58.880
So what happens?

00:00:58.880 --> 00:01:03.290
Well, all too often, the
story ends right there.

00:01:03.290 --> 00:01:06.130
Your server melts,
and your users quickly

00:01:06.130 --> 00:01:08.500
lose interest in your
service unavailable site.

00:01:08.500 --> 00:01:10.610
But, let's hit the
undo button, suppose

00:01:10.610 --> 00:01:13.690
you went with Google App Engine,
and your app scaled seamlessly

00:01:13.690 --> 00:01:15.090
just like you intended.

00:01:15.090 --> 00:01:18.270
So now you have a
million happy new users.

00:01:18.270 --> 00:01:20.680
So what's the next step?

00:01:20.680 --> 00:01:23.100
Well, as it turns
out, life likes

00:01:23.100 --> 00:01:24.670
to throw curve balls at you.

00:01:24.670 --> 00:01:27.380
So you see bots and
spammers show up,

00:01:27.380 --> 00:01:32.660
flooding your forums with junk
and driving your users away.

00:01:32.660 --> 00:01:34.940
Now when you only
had like 10 users,

00:01:34.940 --> 00:01:37.440
it was pretty easy to
just eyeball the logs

00:01:37.440 --> 00:01:40.720
and ban a couple IP addresses
and be done with it.

00:01:40.720 --> 00:01:44.020
But somehow as it turns
out, eyeballs just

00:01:44.020 --> 00:01:47.470
don't scale to a million
users, and your computer now

00:01:47.470 --> 00:01:51.230
refuses to load the terabytes
of log files that you now have.

00:01:51.230 --> 00:01:53.990
So you might find yourself
reinventing the wheel.

00:01:53.990 --> 00:01:55.490
You might write a
quick program just

00:01:55.490 --> 00:01:57.070
to process those
logs automatically,

00:01:57.070 --> 00:01:58.739
try to catch these spammers.

00:01:58.739 --> 00:02:01.030
Maybe you'll find you need
to run on multiple machines,

00:02:01.030 --> 00:02:05.160
so you'll write a controller
program to automate this.

00:02:05.160 --> 00:02:07.460
So what began with a
day's worth of work

00:02:07.460 --> 00:02:09.482
quickly ends up with
you just discovering

00:02:09.482 --> 00:02:10.440
a week's worth of work.

00:02:10.440 --> 00:02:13.430
And a week later, you're a month
behind, and in the meantime,

00:02:13.430 --> 00:02:16.610
your website has
become a lost cause.

00:02:16.610 --> 00:02:19.890
Well, let's hit the
undo button again.

00:02:19.890 --> 00:02:22.660
What if you didn't have
to reinvent the wheel?

00:02:22.660 --> 00:02:25.010
What if with just the
click of a button,

00:02:25.010 --> 00:02:27.660
you had all the tools you
need to solve your spam

00:02:27.660 --> 00:02:29.610
problem before
lunch and you could

00:02:29.610 --> 00:02:32.460
spend the rest of the day
building the next killer app,

00:02:32.460 --> 00:02:35.920
building the next feature for
the next 10 million users?

00:02:35.920 --> 00:02:41.220
What if you really could scale
from a garage size to Google

00:02:41.220 --> 00:02:43.850
Data Center size overnight?

00:02:43.850 --> 00:02:45.570
As it turns out,
that's exactly what

00:02:45.570 --> 00:02:48.390
running Open Source Big Data
software on Google's Cloud

00:02:48.390 --> 00:02:51.330
is all about.

00:02:51.330 --> 00:02:53.870
Now, just to get into
some of the mechanics

00:02:53.870 --> 00:02:56.130
here of actually running
on Google's Cloud,

00:02:56.130 --> 00:02:58.440
there are two big
pieces to this puzzle.

00:02:58.440 --> 00:03:00.330
On the one hand, we
have making the most

00:03:00.330 --> 00:03:02.910
of Google's
infrastructure offerings.

00:03:02.910 --> 00:03:07.520
Now traditionally, if you've
invested heavily upfront

00:03:07.520 --> 00:03:09.070
in a lot of hardware,
you'll end up

00:03:09.070 --> 00:03:11.800
with all these machines of a
certain size, a fixed shape,

00:03:11.800 --> 00:03:13.820
and all too
commonly, you'll find

00:03:13.820 --> 00:03:18.049
yourself wasting endless time
and money building the software

00:03:18.049 --> 00:03:19.840
to try to fit this
mold, trying to fit them

00:03:19.840 --> 00:03:21.800
into this fixed shape.

00:03:21.800 --> 00:03:23.510
In Cloud, we turn
this upside down.

00:03:23.510 --> 00:03:26.605
So you should write
your software however

00:03:26.605 --> 00:03:29.080
it feels natural, write your
software for your application

00:03:29.080 --> 00:03:31.120
logic, and they get
the resources that

00:03:31.120 --> 00:03:34.360
match your use case perfectly.

00:03:34.360 --> 00:03:38.270
Perhaps the most egregious
example of a fixed shape

00:03:38.270 --> 00:03:42.590
would be when you buy
hardware, the time dimension

00:03:42.590 --> 00:03:43.550
is extremely rigid.

00:03:43.550 --> 00:03:45.390
You end up with this
contiguous block

00:03:45.390 --> 00:03:48.840
of time for your
on-premise hardware that

00:03:48.840 --> 00:03:50.960
is unbroken between
the time you buy it

00:03:50.960 --> 00:03:53.020
and the time you're
ready to throw it away.

00:03:53.020 --> 00:03:57.770
Well in Cloud, why not just get
multiple disposable clusters

00:03:57.770 --> 00:03:58.697
instead?

00:03:58.697 --> 00:04:00.780
If you need to get some
data analytics done today,

00:04:00.780 --> 00:04:03.070
why not run multiple
clusters, each analyst

00:04:03.070 --> 00:04:06.060
gets their own cluster
to use by themselves,

00:04:06.060 --> 00:04:08.900
and get your workload
done now instead

00:04:08.900 --> 00:04:11.040
of having to wait in line.

00:04:11.040 --> 00:04:12.860
The second piece
of the puzzle is

00:04:12.860 --> 00:04:18.050
making most of Google's platform
rather than necessarily just

00:04:18.050 --> 00:04:20.850
having to duct tape a bunch
of little components together.

00:04:20.850 --> 00:04:22.660
And this is where
we've been doing work

00:04:22.660 --> 00:04:24.370
to make sure that you
have all the tools

00:04:24.370 --> 00:04:28.820
and libraries necessary to make
it one seamless experience.

00:04:28.820 --> 00:04:32.060
We want you to be able to use
your open source tools, as well

00:04:32.060 --> 00:04:34.420
as higher value Google
services, like BigQuery,

00:04:34.420 --> 00:04:36.830
and mix and match and have
them interoperate seamlessly

00:04:36.830 --> 00:04:38.185
on the same data.

00:04:38.185 --> 00:04:39.560
Just to give an
idea of what this

00:04:39.560 --> 00:04:41.822
looks like at a very
high level view,

00:04:41.822 --> 00:04:43.030
let's go to the demo machine.

00:04:47.200 --> 00:04:49.450
I'll explain a little more
about what's happening here

00:04:49.450 --> 00:04:52.250
under the hood, but you just
to see what it looks like.

00:04:52.250 --> 00:04:56.660
I'm going to go ahead and
deploy an 88 core cluster that's

00:04:56.660 --> 00:04:58.850
fully loaded with
all the greatest data

00:04:58.850 --> 00:05:01.070
analytics open source
software there is,

00:05:01.070 --> 00:05:10.530
and-- I should actually
type a command--

00:05:10.530 --> 00:05:12.780
and we'll see what happens
a little bit later.

00:05:12.780 --> 00:05:14.363
So let's go back
to the slides now.

00:05:20.170 --> 00:05:23.700
So, you might have
noticed Hadoop

00:05:23.700 --> 00:05:25.980
was in the title
of the last slide,

00:05:25.980 --> 00:05:29.200
and we really are talking
about all Big Data Open Source

00:05:29.200 --> 00:05:29.990
software here.

00:05:29.990 --> 00:05:35.730
But, just to give a little
bit of Hadoop context,

00:05:35.730 --> 00:05:38.060
Hadoop has played a
crucial role at being

00:05:38.060 --> 00:05:39.700
one of the very
first open source

00:05:39.700 --> 00:05:42.470
implementations of
the MapReduce paradigm

00:05:42.470 --> 00:05:45.950
and one of the first such widely
adopted software frameworks,

00:05:45.950 --> 00:05:48.700
which ultimately provided
this fault tolerance

00:05:48.700 --> 00:05:53.450
and the machinery necessary to
provide a really low barrier

00:05:53.450 --> 00:05:56.480
entry to starting to make
use of large distributed

00:05:56.480 --> 00:05:59.380
clusters of commodity hardware.

00:05:59.380 --> 00:06:02.050
And this is really what
enabled Big Data to really go

00:06:02.050 --> 00:06:07.020
mainstream and help it
mature to what it is today.

00:06:07.020 --> 00:06:09.000
More recently, we
see Hadoop serving

00:06:09.000 --> 00:06:12.880
in a crucial capacity as
a point of convergence.

00:06:12.880 --> 00:06:14.610
So what I mean by
this is not quite

00:06:14.610 --> 00:06:17.800
sitting down to
write RFCs, but what

00:06:17.800 --> 00:06:22.870
we have is some organic
community consensus driven

00:06:22.870 --> 00:06:24.850
convergence onto
certain behaviors

00:06:24.850 --> 00:06:27.840
and certain interfaces
that came out of Hadoop.

00:06:27.840 --> 00:06:29.830
And one specific
example of this,

00:06:29.830 --> 00:06:32.050
is the Hadoop
distributed file system,

00:06:32.050 --> 00:06:35.110
where we see that as
data and analytics grows,

00:06:35.110 --> 00:06:37.860
as the ecosystem grows with
a diverse set of tools,

00:06:37.860 --> 00:06:41.620
all of these tools are able
to promise interoperability

00:06:41.620 --> 00:06:44.370
with your data simply by
virtue of implementing

00:06:44.370 --> 00:06:46.880
the Hadoop distributed
file system.

00:06:46.880 --> 00:06:48.390
And that brings
us to one product

00:06:48.390 --> 00:06:52.110
that we have today available and
available for you to use now,

00:06:52.110 --> 00:06:55.970
which is the Google Cloud
Storage connector for Hadoop.

00:06:55.970 --> 00:07:00.450
This is built on top of the
aforementioned standardization

00:07:00.450 --> 00:07:04.100
by implementing the Hadoop
File System Interface.

00:07:04.100 --> 00:07:08.410
So what this allows is for
you to install this connector

00:07:08.410 --> 00:07:10.700
onto your Hadoop
clusters and to replace

00:07:10.700 --> 00:07:14.240
at least a portion, or
all, of your data access

00:07:14.240 --> 00:07:19.600
needs using Google Cloud
Storage instead of Hadoop HDFS.

00:07:19.600 --> 00:07:21.700
We provide tools for
automatically installing

00:07:21.700 --> 00:07:24.160
and configuring this,
such that you can either

00:07:24.160 --> 00:07:26.330
run in a hybrid mode,
so that naturally you

00:07:26.330 --> 00:07:29.530
can take your time migrating
what data makes sense

00:07:29.530 --> 00:07:32.670
to put on Google Cloud
Storage together with HDFS,

00:07:32.670 --> 00:07:35.660
or for new batch workloads,
it might make sense just

00:07:35.660 --> 00:07:37.900
to run without any
HDFS altogether.

00:07:41.770 --> 00:07:46.890
We have screenshots, but
let's go to the demo machine.

00:07:46.890 --> 00:07:48.980
Live is always better.

00:07:48.980 --> 00:07:56.260
So let's go ahead, and we have
a Hadoop cluster open here

00:07:56.260 --> 00:07:58.940
with the GCS
connector installed.

00:07:58.940 --> 00:08:01.490
So what we'll see
here is-- let's

00:08:01.490 --> 00:08:10.270
just create a file-- let's
echo here-- we'll just

00:08:10.270 --> 00:08:16.300
say hi to Cloud and
create "hello.txt."

00:08:16.300 --> 00:08:19.226
So we can see this file has
been created on my local laptop,

00:08:19.226 --> 00:08:20.850
and we can go over
here to Google Cloud

00:08:20.850 --> 00:08:23.930
console and-- nope,
not a new folder--

00:08:23.930 --> 00:08:29.140
we want to upload this "hello"
file through our familiar GUI

00:08:29.140 --> 00:08:29.640
interface.

00:08:32.450 --> 00:08:35.270
And as we can see, this
is the cloud portion,

00:08:35.270 --> 00:08:38.510
everything worked
just as we intended.

00:08:38.510 --> 00:08:44.810
Now what we can do is we find
that we have the Hadoop file

00:08:44.810 --> 00:08:47.650
system shell is available
in this Hadoop cluster

00:08:47.650 --> 00:08:56.930
and we can just cut out
that exact same file-- OK,

00:08:56.930 --> 00:09:01.750
I mistyped something
here-- and we

00:09:01.750 --> 00:09:05.959
see that we can access that
same file the same way we would

00:09:05.959 --> 00:09:08.000
through the Hadoop file
system shell, as we would

00:09:08.000 --> 00:09:11.840
through the GCS GUI, as we would
through the GCS command line

00:09:11.840 --> 00:09:14.630
tool, gsutil.

00:09:14.630 --> 00:09:16.520
So let's return to the slides.

00:09:19.420 --> 00:09:21.340
So to summarize
what we've seen, we

00:09:21.340 --> 00:09:24.910
have two axes of
interoperability here.

00:09:24.910 --> 00:09:26.970
On the one hand, we have
Cloud interoperability,

00:09:26.970 --> 00:09:30.110
where we have the same
access and the same behavior

00:09:30.110 --> 00:09:32.280
from all different
cloud services.

00:09:32.280 --> 00:09:35.950
Importantly, this also means
the same access for other Google

00:09:35.950 --> 00:09:37.535
Cloud services like
BigQuery, which

00:09:37.535 --> 00:09:44.020
can interoperate by sharing
GCS as the common data storage.

00:09:44.020 --> 00:09:47.200
The second axis is
Hadoop interoperability,

00:09:47.200 --> 00:09:50.980
as we talked about, where we
can unlock a really broad range

00:09:50.980 --> 00:09:54.690
of open source tools just
by virtue of implementing

00:09:54.690 --> 00:09:56.580
this standard Hadoop
file system interface,

00:09:56.580 --> 00:10:01.560
and that includes making GCS
work with Pig, Hive, Spark,

00:10:01.560 --> 00:10:05.620
Shark, and numerous other
open source technologies.

00:10:05.620 --> 00:10:09.550
What this ultimately means for
you is a significant reduction

00:10:09.550 --> 00:10:11.620
in operational
overhead because now

00:10:11.620 --> 00:10:14.490
your data is not tied to
any single location in terms

00:10:14.490 --> 00:10:18.600
of some set of
disks, nor is it tied

00:10:18.600 --> 00:10:20.710
to any particular
technological stack.

00:10:20.710 --> 00:10:22.360
So if you're tired
of MapReduce and you

00:10:22.360 --> 00:10:24.610
want to try something new,
your data's still there.

00:10:24.610 --> 00:10:26.568
You can start processing
it in a new framework.

00:10:30.550 --> 00:10:38.470
Now, when the question arises
of whether something like GCS

00:10:38.470 --> 00:10:42.590
can really be a feasible
replacement for HDFS,

00:10:42.590 --> 00:10:44.280
performance often comes up.

00:10:44.280 --> 00:10:48.890
And I think there is really two
real aspects to this question.

00:10:48.890 --> 00:10:52.070
First is the question of whether
the networking technology is

00:10:52.070 --> 00:10:55.860
really progressing in the right
direction, such that doing

00:10:55.860 --> 00:10:57.000
this would be feasible.

00:10:57.000 --> 00:10:58.560
And secondly,
there's this question

00:10:58.560 --> 00:11:02.610
of relinquishing a little bit
of that fine grain control

00:11:02.610 --> 00:11:05.140
that you might otherwise
have with HDFS.

00:11:05.140 --> 00:11:08.170
Little things like
rack locality,

00:11:08.170 --> 00:11:10.550
or replication policies.

00:11:10.550 --> 00:11:15.010
And I'm reminded of the
first time I actually

00:11:15.010 --> 00:11:17.700
had an opportunity to write
a lot of assembly code.

00:11:17.700 --> 00:11:19.880
I remember I was really
excited going into it

00:11:19.880 --> 00:11:24.410
because it was this
opportunity to try out

00:11:24.410 --> 00:11:28.270
all the clever hacks I'd always
wanted to make it run fast.

00:11:28.270 --> 00:11:31.140
And it turns out, after
the novelty wore off,

00:11:31.140 --> 00:11:34.300
it was actually fairly painful
writing all this assembly code.

00:11:34.300 --> 00:11:36.100
But I do remember
after so many hours

00:11:36.100 --> 00:11:40.330
of muttering at the screen,
getting it to finally work

00:11:40.330 --> 00:11:42.760
the way I wanted, and
getting a huge success,

00:11:42.760 --> 00:11:45.720
it was like 20% percent speedup.

00:11:45.720 --> 00:11:49.570
I later found out, sadly, that
if only I had flipped the "-03"

00:11:49.570 --> 00:11:52.870
flag on GCC, the original
program already would have ran

00:11:52.870 --> 00:11:55.100
twice as fast.

00:11:55.100 --> 00:11:57.530
Now what I learned,
aside from the fact

00:11:57.530 --> 00:11:59.970
that maybe I'm not
the assembly guru

00:11:59.970 --> 00:12:05.080
I had hoped I was, was I
learned to trust the compiler.

00:12:05.080 --> 00:12:08.570
I learned to trust the decades
of experience, of expertise

00:12:08.570 --> 00:12:11.490
from compiler experts
that have gone into it.

00:12:11.490 --> 00:12:15.330
Whenever Java happens
to be the right tool,

00:12:15.330 --> 00:12:18.300
I feel rest assured knowing
that my memory management is

00:12:18.300 --> 00:12:19.930
actually in pretty good hands.

00:12:19.930 --> 00:12:22.480
And when I use
Google Cloud Storage,

00:12:22.480 --> 00:12:26.220
I know I'm taking advantage
of an incredible amount

00:12:26.220 --> 00:12:29.360
of research and development that
has gone into Google's storage

00:12:29.360 --> 00:12:32.970
technologies over the years.

00:12:32.970 --> 00:12:36.240
And Google Cloud
Storage still might not

00:12:36.240 --> 00:12:39.270
be the right tool for every
use case, but what's important

00:12:39.270 --> 00:12:45.770
is simply having the option
to take a step back, let

00:12:45.770 --> 00:12:47.290
somebody else handle
all the dirty,

00:12:47.290 --> 00:12:50.650
grungy work, because
that's ultimately

00:12:50.650 --> 00:12:53.560
how you can let magic happen
while you focus on building

00:12:53.560 --> 00:12:57.700
the features and products
that you actually care about.

00:12:57.700 --> 00:13:01.630
Now that said, earlier this
year the technology consulting

00:13:01.630 --> 00:13:03.310
company Accenture
actually released

00:13:03.310 --> 00:13:07.270
a study of the performance
characteristics

00:13:07.270 --> 00:13:09.900
and the total cost of
ownership trade off

00:13:09.900 --> 00:13:12.540
between on-premise
deployments of Hadoop,

00:13:12.540 --> 00:13:14.650
as compared to Hadoop
on Google Cloud.

00:13:14.650 --> 00:13:17.270
I'd encourage anybody interested
to read the whole paper.

00:13:17.270 --> 00:13:19.920
It's quite in-depth with a
lot of different dimensions

00:13:19.920 --> 00:13:20.980
of analysis.

00:13:20.980 --> 00:13:24.010
But what we have here is
at least one sample out

00:13:24.010 --> 00:13:28.220
of that paper showing a
realistic real-world workload

00:13:28.220 --> 00:13:33.510
where we have the red bars
here showing a mixed data flow

00:13:33.510 --> 00:13:36.950
model, where we use the
GCS connector for the input

00:13:36.950 --> 00:13:40.800
and output of the initial and
final stages of the MapReduce,

00:13:40.800 --> 00:13:43.790
still using HDFS as
an intermediate store,

00:13:43.790 --> 00:13:45.580
as compared to the
blue bars, which

00:13:45.580 --> 00:13:48.080
are just using a
local disk HDFS.

00:13:48.080 --> 00:13:52.400
And we see that we actually
had a pretty encouraging speed

00:13:52.400 --> 00:13:58.090
up using the GSC
connector-based data flow model.

00:13:58.090 --> 00:14:01.550
The green line here at the
top is the benchmark value

00:14:01.550 --> 00:14:03.790
of the bare metal
Hadoop deployments

00:14:03.790 --> 00:14:05.821
from a total cost of
ownership analysis,

00:14:05.821 --> 00:14:07.570
and we can see that
across all dimensions,

00:14:07.570 --> 00:14:09.650
the Cloud deployments
outperformed.

00:14:09.650 --> 00:14:11.710
So lower values are better here.

00:14:15.040 --> 00:14:16.870
So I'm happy to
announce that we've also

00:14:16.870 --> 00:14:22.340
been working on two other
Hadoop connectors for connecting

00:14:22.340 --> 00:14:23.860
to Google Cloud services.

00:14:23.860 --> 00:14:26.080
We hope that the
data store connector

00:14:26.080 --> 00:14:30.540
will help you bring data
analytics through open source

00:14:30.540 --> 00:14:33.100
tools to operate
on the same data

00:14:33.100 --> 00:14:36.330
that you use in your web
serving, or that you generate.

00:14:36.330 --> 00:14:38.310
And we hope that the
BigQuery connector will

00:14:38.310 --> 00:14:41.920
help aid the seamless
interoperability between,

00:14:41.920 --> 00:14:43.874
whether you started
out just using BigQuery

00:14:43.874 --> 00:14:45.290
and want to use
open source tools,

00:14:45.290 --> 00:14:47.123
or if you started out
with open source tools

00:14:47.123 --> 00:14:49.400
and want to start
using BigQuery,

00:14:49.400 --> 00:14:51.760
you should be able to go
either way seamlessly.

00:14:51.760 --> 00:14:53.910
Both of these we hope to
make available publicly

00:14:53.910 --> 00:14:57.364
in early April.

00:14:57.364 --> 00:14:59.030
Along with all these
connectors, we also

00:14:59.030 --> 00:15:04.700
have a thin helper tool called
bdutil, which helps deploy

00:15:04.700 --> 00:15:07.240
and install all
this Hadoop software

00:15:07.240 --> 00:15:09.550
with the thin
extensibility mechanism

00:15:09.550 --> 00:15:13.140
for customizing your software
and mixing in more open source

00:15:13.140 --> 00:15:14.140
tools.

00:15:14.140 --> 00:15:19.410
And that brings us to what we'll
demo here using bdutil of Shark

00:15:19.410 --> 00:15:20.810
on Google Cloud Platform.

00:15:20.810 --> 00:15:25.530
Now Shark is a prime example
of how all this open source

00:15:25.530 --> 00:15:28.670
technology works together and
works well with Google's Cloud.

00:15:28.670 --> 00:15:33.240
Shark is ultimately-- is
basically Hive on Spark.

00:15:33.240 --> 00:15:37.190
Where Hive is the SQL on
Hadoop, so the Hive engine

00:15:37.190 --> 00:15:40.100
will translate SQL
over unstructured data

00:15:40.100 --> 00:15:42.120
into MapReduces
that run in Hadoop.

00:15:42.120 --> 00:15:47.010
Whereas Spark is an entirely
different data processing

00:15:47.010 --> 00:15:50.680
model then MapReduce,
but here we

00:15:50.680 --> 00:15:54.980
have Shark implementing
total compatibility

00:15:54.980 --> 00:15:58.840
with Hive queries, but
instead of using MapReduces

00:15:58.840 --> 00:16:00.764
under the hood,
it will use Spark.

00:16:00.764 --> 00:16:02.680
And we'll see how this
plugs into Google Cloud

00:16:02.680 --> 00:16:04.520
Platform in two ways.

00:16:04.520 --> 00:16:07.430
First, by reading your
external data from GCS,

00:16:07.430 --> 00:16:10.410
so that your data can
persist independently

00:16:10.410 --> 00:16:12.500
of persisting your
compute cluster.

00:16:12.500 --> 00:16:15.300
And secondly, by
plugging into Cloud SQL

00:16:15.300 --> 00:16:18.390
so that you can have
multiple different clusters,

00:16:18.390 --> 00:16:22.020
all plugged into Cloud SQL as
your metadata store and reading

00:16:22.020 --> 00:16:22.750
from that.

00:16:22.750 --> 00:16:24.315
So we'll go to the
demo machine now.

00:16:29.980 --> 00:16:33.300
And we see that so
far-- well, first,

00:16:33.300 --> 00:16:37.350
let's launch a cluster, bdutil.

00:16:37.350 --> 00:16:41.650
And we ran something similar
to this already, but just

00:16:41.650 --> 00:16:43.810
to elaborate a little
bit, what we have here is

00:16:43.810 --> 00:16:45.930
an extension that
I've created here

00:16:45.930 --> 00:16:49.250
that's just for installing
Shark on Cloud SQL.

00:16:49.250 --> 00:16:51.260
And all it does-- here
are some shell scripts

00:16:51.260 --> 00:16:54.360
that actually
install Shark, we'll

00:16:54.360 --> 00:16:57.770
provide some cluster
specific configuration,

00:16:57.770 --> 00:16:59.350
and we just kick it off.

00:16:59.350 --> 00:17:05.849
If we take a look at our cluster
specific configuration here,

00:17:05.849 --> 00:17:10.280
we see that we have some
basic settings like number

00:17:10.280 --> 00:17:14.330
of workers, a prefix for
the cluster name that all

00:17:14.330 --> 00:17:18.450
our instances will have, which
project to use, and so on.

00:17:18.450 --> 00:17:20.359
Since we are lucky
enough to kick off

00:17:20.359 --> 00:17:24.710
a cluster a few moments
ago in this presentation,

00:17:24.710 --> 00:17:28.470
we can go ahead and just use
that as our other cluster.

00:17:28.470 --> 00:17:30.370
Take a look inside.

00:17:30.370 --> 00:17:33.990
So we'll run as Hadoop.

00:17:36.570 --> 00:17:44.340
And let's go ahead and boot up
a Shark command line, a Hive M

00:17:44.340 --> 00:17:46.690
line, and maybe
another Hive command

00:17:46.690 --> 00:17:50.620
line on the second cluster--
figure all of these out here.

00:17:50.620 --> 00:17:55.230
So we can see we don't
have any tables yet.

00:17:55.230 --> 00:17:58.980
Now we can go ahead-- I
happen to have a data set here

00:17:58.980 --> 00:18:10.030
of a natality birth-data
data set for the year 1970

00:18:10.030 --> 00:18:12.650
through today, and it
happens to be in Avro format.

00:18:12.650 --> 00:18:15.694
So this is just some
files sitting in GCS,

00:18:15.694 --> 00:18:18.110
and what we'll do is we'll
create an external table, which

00:18:18.110 --> 00:18:19.109
is this pointed at that.

00:18:23.840 --> 00:18:26.550
So now that that
has been created,

00:18:26.550 --> 00:18:32.380
we see that it appears
in both Shark and in Hive

00:18:32.380 --> 00:18:38.600
on the same cluster and in
Hive on the other cluster.

00:18:38.600 --> 00:18:43.860
So you can see this is demo two
down here and demo one up here.

00:18:43.860 --> 00:18:46.580
And sure enough, we
can also select top 10

00:18:46.580 --> 00:18:48.945
from it from any of
these three shells.

00:18:53.260 --> 00:18:56.090
So suppose we want
to find out if there

00:18:56.090 --> 00:18:58.560
are any trends in the
number of underweight births

00:18:58.560 --> 00:19:00.430
as a fraction of the
total number of births

00:19:00.430 --> 00:19:01.470
over the years.

00:19:01.470 --> 00:19:04.250
So, here's a query that
we have prepared here.

00:19:04.250 --> 00:19:06.565
And we'll just
plug it into Shark.

00:19:13.820 --> 00:19:17.170
Now while that
kicks off, suppose

00:19:17.170 --> 00:19:19.235
we also have
another set of data.

00:19:19.235 --> 00:19:20.610
Here's a common
use case, suppose

00:19:20.610 --> 00:19:23.430
we have a bunch of
files in JSON format.

00:19:23.430 --> 00:19:28.830
So people often use JSON formats
as an initial data format,

00:19:28.830 --> 00:19:34.980
but it's usually good to get
a binary format, like Avro.

00:19:34.980 --> 00:19:40.950
So, we can go ahead and
create this JSON table

00:19:40.950 --> 00:19:42.015
in our second cluster.

00:19:46.010 --> 00:19:55.810
And then what we'll do is we'll
use Hive as our data conversion

00:19:55.810 --> 00:20:00.910
engine here simply by creating
a second table in Avro format

00:20:00.910 --> 00:20:05.470
and then just saying select
star out of our JSON table

00:20:05.470 --> 00:20:14.234
and writing it into
our Avro table instead.

00:20:14.234 --> 00:20:16.610
We'll do that in
our second cluster.

00:20:16.610 --> 00:20:19.220
And now we can take a
look at the web UIs here.

00:20:19.220 --> 00:20:27.350
So here I have a web UI open
of the built-in Spark GUI,

00:20:27.350 --> 00:20:30.540
and we can see all the
commands that we just ran.

00:20:30.540 --> 00:20:33.350
We ran select, and it looks
like it's already finished,

00:20:33.350 --> 00:20:37.680
the Shark query that we
kicked off a little while ago.

00:20:37.680 --> 00:20:41.450
And if we go into our
second cluster, demo two,

00:20:41.450 --> 00:20:45.210
and open the Hadoop
console, we can

00:20:45.210 --> 00:20:50.810
see that Hive has gone ahead and
translated that second query,

00:20:50.810 --> 00:20:54.670
that JSON creation
query, into a MapReduce.

00:20:54.670 --> 00:20:57.270
So now we have MapReduce
running on some data that's

00:20:57.270 --> 00:21:00.700
in GCS, right next to
Shark running against Spark

00:21:00.700 --> 00:21:03.780
on other data that
was also in GCS.

00:21:03.780 --> 00:21:06.630
And we can see that, sure
enough, we got the results.

00:21:06.630 --> 00:21:09.230
We see that over the
'80s, the percentage

00:21:09.230 --> 00:21:12.210
of underweight births
went down a little bit

00:21:12.210 --> 00:21:14.370
and then seemed to
go up as we came

00:21:14.370 --> 00:21:17.030
out of less prosperous
years apparently,

00:21:17.030 --> 00:21:20.340
so some very interesting data.

