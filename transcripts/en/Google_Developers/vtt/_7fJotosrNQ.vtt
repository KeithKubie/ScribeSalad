WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.140
&gt;&gt; Good afternoon, ladies and gentlemen. I'm
Mike Aizatsky, I'm working at the App Engine

00:00:05.140 --> 00:00:12.700
team and today we're going to talk about Batch
Data Processing with Google App Engine. So

00:00:12.700 --> 00:00:17.800
during the session live notes will be available
in Google Wave and you can also pass your

00:00:17.800 --> 00:00:28.050
questions. So please write down this obscure
URL which is ds6t9F. So a little bit about

00:00:28.050 --> 00:00:33.359
agenda; so first of all, we're going to spend
a little time talking about batch processing.

00:00:33.359 --> 00:00:38.819
What is it? How is it different from what
Brett was talking about? Then just to give

00:00:38.819 --> 00:00:44.579
you a little introduction to the topic, I
will talk about how was data processing done

00:00:44.579 --> 00:00:49.859
in App Engine up-to-date, then I will tell
you how we do batch processing at Google,

00:00:49.859 --> 00:00:57.010
then we're going to spend the rest of the
session about how we would like you to do

00:00:57.010 --> 00:01:02.271
your batch processing from, starting from
today. So, batch data processing is the task

00:01:02.271 --> 00:01:07.229
of off-line processing of lots and lots of
data, lots of entities, thousands of entities,

00:01:07.229 --> 00:01:13.630
a common use cases of schema migration, export
in and input in data, report generation of

00:01:13.630 --> 00:01:19.320
whatever. And the core difference is with
what Brett was talking about this is not real

00:01:19.320 --> 00:01:24.980
time tasks. So this is something that you
just sprung, take a cup of tea or go to sleep

00:01:24.980 --> 00:01:32.009
and it finishes in the morning depending on
how much data you've got. And for some, for

00:01:32.009 --> 00:01:36.730
certain reasons this is a difficult stuff
in App Engine. First of all, they're all like

00:01:36.730 --> 00:01:44.090
certain restrictions in App Engine, which
are not created because we like those, because

00:01:44.090 --> 00:01:47.240
we need those restrictions because we need
automatic scalability, we need replication,

00:01:47.240 --> 00:01:52.100
reliability, and that's why we have got a
30-second request limit, which are really

00:01:52.100 --> 00:01:59.039
hard to try, which we're really trying to
overcome. Sometimes there are transient errors

00:01:59.039 --> 00:02:06.220
in the system which happens in all distributed
systems. We have non-zero latency for Datastore.

00:02:06.220 --> 00:02:11.890
We've got occasional, pretty rare but still
occasional timeouts. And what's really important,

00:02:11.890 --> 00:02:17.400
everyone got changing datasets. So you want
a batch processor where you're changing data

00:02:17.400 --> 00:02:24.820
which complicates this stuff a little bit.
So when App Engine was launched, the only

00:02:24.820 --> 00:02:30.150
way to do data, batch data processing was
to define a batch handler. Do something which

00:02:30.150 --> 00:02:38.100
you'd be able to fit into 30 seconds and start
another process we feel be in your page continuously.

00:02:38.100 --> 00:02:43.910
It might be just a curl, it might be a browser.
And actually, Alan is sitting here, Alan did

00:02:43.910 --> 00:02:49.060
the first thing at the very first day when
App Engine was launched. Do you remember there

00:02:49.060 --> 00:02:54.460
were invitations? So we had to process them
somehow. So we created a browser page which

00:02:54.460 --> 00:03:01.870
was doing refresh and sending out all those
invitations. There are some problems with

00:03:01.870 --> 00:03:05.120
the [INDISTINCT]. First of all, we need some
external driver which might fail and like

00:03:05.120 --> 00:03:11.370
a computer which you got to have to keep your
laptop on, you have to keep it, plug it in

00:03:11.370 --> 00:03:16.830
to the network, if network fails and your
process stops. There are also difficulties

00:03:16.830 --> 00:03:22.990
of who is doing all the error handling, recoveries
when errors happen and the whole process is

00:03:22.990 --> 00:03:29.540
slow. I mean, what's the top QPS you can drive
from a single computer? Maybe 50, if you're

00:03:29.540 --> 00:03:36.800
lucky enough but I doubt that. And the whole
state management of the process is really,

00:03:36.800 --> 00:03:43.620
really complex. People created lots of improvements
over the two years when App Engine is--well,

00:03:43.620 --> 00:03:51.630
over two years. They've got--you can communicate
with the driver or you can use remote APIs

00:03:51.630 --> 00:03:56.590
for really complex scenarios, or even have
got a Bulkloader too which does exactly that.

00:03:56.590 --> 00:04:01.880
This is something which is running in your
computer and which drives the traffic to your

00:04:01.880 --> 00:04:07.400
web app to perform some batch computations.
A year ago, we have launched a really, really

00:04:07.400 --> 00:04:13.370
nice system which we like a lot and we used
throughout all apps and this is called the

00:04:13.370 --> 00:04:18.450
Task Queue. Task Queue can perform any kind
of work outside of a user driven request.

00:04:18.450 --> 00:04:24.910
You just in-queue it, it will be done later.
And it's really reliable, high performance,

00:04:24.910 --> 00:04:31.870
high throughput system, but it still got a
30-second limit. So people have created a

00:04:31.870 --> 00:04:38.460
task chaining technique, this is something
which you use to overcome the 30-second limit.

00:04:38.460 --> 00:04:44.270
Basically, it run for some time and as soon
as you see that your time is ending, when

00:04:44.270 --> 00:04:50.600
you're close to 25 seconds, you can queue
another task and finish your job. So you--effectively,

00:04:50.600 --> 00:04:58.620
it splits your job in batches and each batch
is processed by a separate task; and the code

00:04:58.620 --> 00:05:05.500
looks kind of okay. So you define the same
kind of app handler, you get the next batch

00:05:05.500 --> 00:05:12.590
from somewhere, a common place to put starting
point is in the request headers then you do

00:05:12.590 --> 00:05:18.180
some work, define where to continue working
then you queue the continuations. And it's

00:05:18.180 --> 00:05:21.931
really easy to start. The whole process of
adjusting queue the first task and it will

00:05:21.931 --> 00:05:26.270
continue to go through your Datastore. And
people have been successfully using it and

00:05:26.270 --> 00:05:34.540
there are lots of libraries for doing that
kind of stuff. And Task Queue is really, really

00:05:34.540 --> 00:05:39.500
nice for doing that kind of work. First of
all, it guarantees sequential task execution.

00:05:39.500 --> 00:05:47.110
No matter what happens, be it timeouts, problems,
schedule of maintenance, unscheduled maintenance,

00:05:47.110 --> 00:05:51.500
whatever happens, Task Queue guarantees that
your task will be executed. You do not need

00:05:51.500 --> 00:05:57.160
any kind of external drivers. You can fire
your task and go home, close your computer,

00:05:57.160 --> 00:06:02.860
leave and sleep well. Task Queue repeats task
execution if something bad has happened. If

00:06:02.860 --> 00:06:08.080
you have some kind of unavailability or you
have some transcend error or your URL page

00:06:08.080 --> 00:06:14.010
did not work through, you just return a 500
result code and Task Queue will retry and

00:06:14.010 --> 00:06:22.139
it will back off if you have too many errors.
And also most people do not think about that,

00:06:22.139 --> 00:06:27.900
but Task Queue can limit execution rate so
that you have a predictable resource consuming--consuming

00:06:27.900 --> 00:06:34.790
predictable of resource, amount of resources
because it's really easy for Task Queue to

00:06:34.790 --> 00:06:40.120
blow up through the whole budget in one hour.
It would just execute a couple million of

00:06:40.120 --> 00:06:50.050
tasks and just spend all your money for today.
So the whole App Engine idea is to take something

00:06:50.050 --> 00:06:56.350
which works for Google and to give it to external
developers. And it's true that every single

00:06:56.350 --> 00:07:02.910
App Engine runs in the same servers, in the
same datacenters which our usual day-to-day

00:07:02.910 --> 00:07:09.570
problems run. We don't have any special datacenters
for you. We don't have any special infrastructure

00:07:09.570 --> 00:07:16.370
built. At Google, MapReduce is being used
for years and years to do batch processing

00:07:16.370 --> 00:07:23.169
our Google scale. And MapReduce is a system
that was specifically created to help developers

00:07:23.169 --> 00:07:30.590
work with unreliable distributed systems,
storages, fail-overs, retries and it's really,

00:07:30.590 --> 00:07:35.580
really widely adopted. So at Google, there
are thousands and thousands of MapReduce jobs

00:07:35.580 --> 00:07:42.340
running. MapReduce is really, really a simple
thing. It just defines only two functions.

00:07:42.340 --> 00:07:48.460
Functions to define the map functions which
take your data chunk maybe entity and it produces

00:07:48.460 --> 00:07:54.530
at least pairs of key values. Then there is
a hidden magic step which is called shuffle,

00:07:54.530 --> 00:07:58.760
which groups all values with the same keys
together. And then there is a reduce function

00:07:58.760 --> 00:08:05.860
which is also supplied by developer, which
takes key and also shaded values. And it's

00:08:05.860 --> 00:08:19.800
amazing that how broad tasks can be completed
with the Task Queue. A special interest of

00:08:19.800 --> 00:08:25.470
schema migration task, which is usually an
empty reduce. So you've got only map functions

00:08:25.470 --> 00:08:30.729
which will reliably scans through your old
dataset and in your update function and in

00:08:30.729 --> 00:08:36.569
your map function you just update your Datastore
and migrate or compute something. And another

00:08:36.569 --> 00:08:44.720
one, a really a special case, is report generation.
When a step of reduce function you just generate,

00:08:44.720 --> 00:08:52.959
you report entities, which have some report
data for consuming with other pages of other

00:08:52.959 --> 00:08:59.300
apps or other tools. We would really like
to give you the same MapReduce which we are

00:08:59.300 --> 00:09:03.240
using at Google internally. We would really
like for you to use the same implementation,

00:09:03.240 --> 00:09:09.329
the same systems, but unfortunately, there
are bigger and there are some unique challenges

00:09:09.329 --> 00:09:14.379
which we have at App Engine. First of all,
we've got additional scaling dimension which

00:09:14.379 --> 00:09:19.550
is not common for Google. We've got lots of
applications which were written by you guys.

00:09:19.550 --> 00:09:26.249
There are thousands and thousands of apps
and we expect that many of them will run MapReduce

00:09:26.249 --> 00:09:32.649
at the same time. And this scaling dimension
was not anticipated by our MapReduce engineering

00:09:32.649 --> 00:09:42.139
team so MapReduce cannot do that. And another
problem is that we want to isolate all apps

00:09:42.139 --> 00:09:47.939
so that if some app is behaving badly or consuming
lots of resources, then other apps should

00:09:47.939 --> 00:09:55.320
not be hurt. So no one wants to have his MapReduce
is running slow because someone else is running

00:09:55.320 --> 00:10:01.139
his MapReduce. So we want to do lots and lots
of isolation stuff. We also want to do rate

00:10:01.139 --> 00:10:06.559
limiting, which is a surprising requirement
because at Google we want our MapReduces to

00:10:06.559 --> 00:10:13.389
continue as fast as possible. But we realize
that that is not what you guys want because

00:10:13.389 --> 00:10:20.179
we can consume thousand dollars of resources
per hour and not everyone might be happy with

00:10:20.179 --> 00:10:28.009
the rate of resource spending. So you'd probably
want to limit your resource consumption and

00:10:28.009 --> 00:10:33.550
some apps even want a very, very slow execution.
If you're a free app, you've got a certain

00:10:33.550 --> 00:10:38.519
amount of quota daily, you do not want to
consume it in five minutes and queue your

00:10:38.519 --> 00:10:44.809
app or whatever. And the last but not the
least, people want a protection from malicious

00:10:44.809 --> 00:10:50.040
App Engine users. There are some people who
tried to exploit our infrastructure to gain

00:10:50.040 --> 00:10:55.779
some advantage or just take a look how Google
operates and we have to be protected from

00:10:55.779 --> 00:11:02.160
that people. We have to protect your data
from those kinds of people. So, that means

00:11:02.160 --> 00:11:07.829
that we could not take the existing MapReduce
and give it to you. So we have started to

00:11:07.829 --> 00:11:12.751
build the whole system from scratch using
probably some building blocks which we have

00:11:12.751 --> 00:11:18.470
already available. And we realized that we
already have a really, really nice subsystem

00:11:18.470 --> 00:11:23.170
in App Engine which solves most of the problems
we're having and that system is called Task

00:11:23.170 --> 00:11:30.069
Queue. So we decided to build our MapReduce
framework on top of Task Queue, probably editing

00:11:30.069 --> 00:11:38.331
some necessary services while we go. So today,
I'm pleased to announce that we have successfully

00:11:38.331 --> 00:11:45.199
completed part one of MapReduce story. We
have implemented map, which is not old MapReduce

00:11:45.199 --> 00:11:51.769
but we decided that it's already useful enough
for you guys to work on. And it's reliable,

00:11:51.769 --> 00:11:56.889
fast and efficient way to iterate over your
Datastore or blob files or whatever other

00:11:56.889 --> 00:12:03.920
data sources possible. You can start playing
with it today and while we are extremely busy

00:12:03.920 --> 00:12:08.790
of giving you the full MapReduce, you can
already start using it, take a look at our

00:12:08.790 --> 00:12:16.220
APIs and write your code. First of all, the
code in Meta Library is completely user space.

00:12:16.220 --> 00:12:23.119
You just pull those sources into your project
and it works. It's fully open source. So you

00:12:23.119 --> 00:12:29.800
can tweak it, modify it, change to your needs,
see how it works. You can create patches,

00:12:29.800 --> 00:12:36.480
send those back to us and we'll use them and
to use them to our code base. Today, we are

00:12:36.480 --> 00:12:41.470
launching Python version. Unfortunately, Java
version was not in the state. We're happy

00:12:41.470 --> 00:12:46.360
to open right now but it will come really,
really, really soon. And we try to make all

00:12:46.360 --> 00:12:54.939
APIs very familiar to Hadoop and Dumbo users
so that it's close to be in source compatible.

00:12:54.939 --> 00:13:01.509
But put it in reality, it means if you know
what Hadoop or Dumbo is, you can start really

00:13:01.509 --> 00:13:08.699
fast. So speaking about features, we have
automatic sharding. So if you have a huge

00:13:08.699 --> 00:13:13.100
dataset, we can process in parallel lots and
lots of shards. So we will split your datasets

00:13:13.100 --> 00:13:18.119
into manageable chunks and works through them.
You can rate limit for slow execution. We

00:13:18.119 --> 00:13:23.790
have status pages so that you can see how
well your MapReduce is doing. What is it doing?

00:13:23.790 --> 00:13:29.459
What is the progress? We have counter support,
which means that you can count certain events

00:13:29.459 --> 00:13:35.179
in your dataset and use it as a reporting
tool. We can have parameterized mappers so

00:13:35.179 --> 00:13:39.410
that you can create a mapper which can be
used to its different parameters to tweak

00:13:39.410 --> 00:13:45.990
or to tune what to work on. So that engineers
create MapReduce to an administrator, run

00:13:45.990 --> 00:13:52.399
it later, or you can create re-useable MapReducers.
We have batching datastore operations so that

00:13:52.399 --> 00:13:58.649
it works much, much faster than the code you
write by hand. And you can also iterate not

00:13:58.649 --> 00:14:07.059
over the datastore only but over the blob.
So now we'll try to give you a small demo.

00:14:07.059 --> 00:14:16.799
Let's hope that Wi-Fi gets--in good mood today.
So we created a small app which basically

00:14:16.799 --> 00:14:24.239
scans the source code of the zip file which
we have uploaded and looks for certain irregular

00:14:24.239 --> 00:14:30.350
expressions. So for this demo, we have uploaded
the source code of LinuxFi, or Linux code

00:14:30.350 --> 00:14:36.470
of fi, which is approximately 100 megabytes
of a zip file which is close to one gigabyte

00:14:36.470 --> 00:14:44.769
of your source code. So there is a MapReduce
system overview page which shows you all the

00:14:44.769 --> 00:14:51.850
MapReduce which we have run recently. Here
they are, I'm going to zoom in. You can also

00:14:51.850 --> 00:14:57.310
start a new job, so let's try it. These are
just parameters. This is not like really user

00:14:57.310 --> 00:15:04.980
friendly, just some bare UI which we've got.
So, we specify the file ID. We began a look

00:15:04.980 --> 00:15:14.549
for crap in Linux and the regular expression
will be crap and build it in 20 shards. So,

00:15:14.549 --> 00:15:19.999
we're going hit run. It take sometime to analyze
the dataset and then queue all of the tasks.

00:15:19.999 --> 00:15:26.559
So it says like, "Hey, the job was successfully
started. Here is a job which is being run."

00:15:26.559 --> 00:15:33.249
We can check the Task Queue of our app. We
got 21 tasks, 59--we have already run at this

00:15:33.249 --> 00:15:41.049
minute, that we will continue running. Meanwhile,
we can check the status page of individual

00:15:41.049 --> 00:15:50.980
MapReduce which is, which will show us how
our MapReduce is doing. So, we can see it.

00:15:50.980 --> 00:15:56.569
This is like a progress of each separate shard.
How many files has it processed? We can see

00:15:56.569 --> 00:16:03.809
that by now, it had, it has done 4,000 map
recalls which is approximately 140 calls per

00:16:03.809 --> 00:16:11.790
second which means they're processing 140
files per second. And you can see, if you're

00:16:11.790 --> 00:16:17.410
interested in the status of individual shards,
how well are they doing, like how long does

00:16:17.410 --> 00:16:24.549
it take, you can refresh and check. But meanwhile,
while it that gathers, it writes over data

00:16:24.549 --> 00:16:31.899
to the Datastore so that we can already take
a look at the results. So this is all mention

00:16:31.899 --> 00:16:42.410
of crap in the Linux source code. Depending
on the lack, like yesterday, same MapReduces

00:16:42.410 --> 00:16:47.530
completed in two minutes. Today, it's took
like eight minutes which is, two minutes,

00:16:47.530 --> 00:16:53.359
I think, is quiet good for one gig of source
files to do or gets on every line of every

00:16:53.359 --> 00:17:01.730
source file in LinuxFi. So, we will probably
check later in the demo, in the presentation.

00:17:01.730 --> 00:17:07.150
So we see that there are only nine shards
left so like 11 shards have already completed

00:17:07.150 --> 00:17:17.030
in one minute 40 seconds in every single completed
eventually. So meanwhile, let's see how the

00:17:17.030 --> 00:17:35.769
API looks like. Oops, I've lost those slides'
position. So, it is really--we try to make

00:17:35.769 --> 00:17:41.029
it really, really easy for you to start using
the library. It takes exactly two steps. First

00:17:41.029 --> 00:17:47.730
step, you checkout library from subversion
directly into your source code using the [INDISTINCT]

00:17:47.730 --> 00:17:55.340
export and then you just add these three lines
to the [INDISTINCT]. You specify URL--so this

00:17:55.340 --> 00:18:01.540
is an error, it sets at duplication. It should
say MapReduce and you say that this is admin

00:18:01.540 --> 00:18:08.210
productive pages, which is probably a good
idea, you do not want your users to start

00:18:08.210 --> 00:18:12.980
MapReduce. And it is really, really easy to
define any new mapper to do some work. You

00:18:12.980 --> 00:18:17.940
define a simple function which takes only
one argument. You do some solicit argument

00:18:17.940 --> 00:18:25.919
and you create a MapReduce yuml file in your
app, and just specify your map or how is it

00:18:25.919 --> 00:18:32.660
go, it's operated over Datastore and it has
a handler that, those kind of stuff. And then

00:18:32.660 --> 00:18:39.269
it can open your app and /MapReduce and all
the MapReduces will [INDISTINCT] you can start

00:18:39.269 --> 00:18:47.230
those from the UI. In Java, it's going to
look a little bit more messy, like you have

00:18:47.230 --> 00:18:52.860
to define class extend App Engine mapper key
entity. To be honest, I have left two additional

00:18:52.860 --> 00:18:58.059
parameters, generic parameters here because
they come in from Hadoop, this is not our

00:18:58.059 --> 00:19:03.429
interface, but this is exactly how Hadoop
interface looks like. So, you can write at

00:19:03.429 --> 00:19:11.700
the same way, but it's quite simple anyway.
So, this is--first example, how you do your

00:19:11.700 --> 00:19:17.710
Datastore operations? You specify a function.
You just accept, for example, user--you do

00:19:17.710 --> 00:19:22.370
something with user. You tweak at properties,
you change its ages, we calculate something

00:19:22.370 --> 00:19:27.970
and then you yield the operations you want
to be performed. You yield an operation to

00:19:27.970 --> 00:19:33.490
be pulled from user and it’ll collect all
those operations together from multiple run

00:19:33.490 --> 00:19:38.870
and we'll do them in batches. We make sure
that you do not cross a one megabyte of request

00:19:38.870 --> 00:19:45.210
limit and all that kind of stuff. And this
boot you can also use delete or perhaps some

00:19:45.210 --> 00:19:51.880
other kinds of operations. Counters can be
really--counters is just something which keeps

00:19:51.880 --> 00:19:58.549
tracks of simple integer value and, you can
use it to build some kind of report generation.

00:19:58.549 --> 00:20:05.940
For example, if you want to know how your
audience age distributed, it just introduce

00:20:05.940 --> 00:20:10.880
lots of counters depending on the user age
and you increment a specific counter. And

00:20:10.880 --> 00:20:15.750
on the status page, you would be able to seal
the counter fellows and you can have probably

00:20:15.750 --> 00:20:22.490
the ability of migrating to a thousand counters.
So it's like we can do pretty nice reports

00:20:22.490 --> 00:20:30.170
using counters alone without any difficulties.
And this is the example how you might generate

00:20:30.170 --> 00:20:37.250
a more complex report. You, for example, if
you want to see how many goods each customer

00:20:37.250 --> 00:20:42.740
has ordered for you, you define the mapper
over your customers then inside the code,

00:20:42.740 --> 00:20:48.870
you will, all orders, for all orders by this
customer, you sum the total. You create the

00:20:48.870 --> 00:20:54.019
report line and you yield an operation to
do the report line back. So, then you'll be

00:20:54.019 --> 00:20:59.769
able to start this MapReduce. You can even
create a crone job which will run this mapper

00:20:59.769 --> 00:21:06.870
like everyday and you will have a fresh report
every morning, there's all information you

00:21:06.870 --> 00:21:14.220
need. Some implementation details is that
they’d be using Task Queue chaining to do

00:21:14.220 --> 00:21:20.559
all the stuff and there are two types of flows
inside. The first one is a controller flow.

00:21:20.559 --> 00:21:26.940
We have a separate task which oversees all
the execution, all the statuses, log messages,

00:21:26.940 --> 00:21:32.620
whatever, and gathers information together,
helps to displays status page. And they have

00:21:32.620 --> 00:21:39.059
working flow which you can have really a lot.
So, we've been running our examples, we explain

00:21:39.059 --> 00:21:44.960
to you, you can have 4,800, it depends on
to the data rate you want to achieve. And

00:21:44.960 --> 00:21:50.570
all this task use Datastore for storage and
communication. And we optimizes it, it's really

00:21:50.570 --> 00:21:56.279
hard to do like just one read, one write,
doing the whole operation. So the overhead

00:21:56.279 --> 00:22:05.740
is, there is some overhead of using this but
it’s pretty, pretty low. And we handle most

00:22:05.740 --> 00:22:11.190
of the complexities. We're making sure that
there is no [INDISTINCT] written outs, there's

00:22:11.190 --> 00:22:16.320
no infinite forking. But there is only one
thing which we would like you to keep thinking

00:22:16.320 --> 00:22:23.240
about when you write MapReduce, is idempotence.
And idempotence is a really important concept

00:22:23.240 --> 00:22:28.350
for all batch operation. So idempotence, in
mathematics, it means that if you apply a

00:22:28.350 --> 00:22:34.000
function twice, so you can have the same result;
which in our case means that your batch handler

00:22:34.000 --> 00:22:39.950
should be ready to process the same entity
twice because errors do happen and the way

00:22:39.950 --> 00:22:45.570
we recover from errors would be rerun your
code in to the same entity. And sometimes

00:22:45.570 --> 00:22:51.490
you might be unlucky that it would be rerun
twice. And this is the most important property

00:22:51.490 --> 00:22:55.580
you should think about. You should--when you
write a batch operation, you should always

00:22:55.580 --> 00:23:02.370
think, is it idempotent? Do I care about that
stuff? And let's take a look at some examples

00:23:02.370 --> 00:23:10.960
of practical idempotent operation. So this
is naïve data migration which is not idempotent

00:23:10.960 --> 00:23:17.340
because it creates a new property. It computes
it and it puts entity back, and if its run

00:23:17.340 --> 00:23:24.529
twice, it might, on the same entity, it might
give not the same results which we'd like

00:23:24.529 --> 00:23:31.850
to have. So--oops, the same code. I'm sorry
about that. So basically, the way to make

00:23:31.850 --> 00:23:36.929
it idempotent is that, you have, it have to
checks it, you have performed a migration

00:23:36.929 --> 00:23:41.790
to your entity already. If you have performed
it, so there's no need to do any additional

00:23:41.790 --> 00:23:47.740
work. If you haven’t performed it, perform
migration, flips the flag or whatever you

00:23:47.740 --> 00:23:53.330
have, and put it back. For example, if you
decide to migrate from a zero-based value

00:23:53.330 --> 00:23:57.840
to a one-based value, you should introduce
a Boolean flag of whether it performs migration

00:23:57.840 --> 00:24:04.030
or not, or otherwise you can increments it
probably twice. It's a rare event but you

00:24:04.030 --> 00:24:12.409
do not want to take chances. Another example
is doing reports. So reporting by customer,

00:24:12.409 --> 00:24:18.519
you generate report lines and you put report
lines back into database. Again, as I said,

00:24:18.519 --> 00:24:24.769
this is not good because you might process
the same customer twice. There is a way out

00:24:24.769 --> 00:24:31.480
of this. In your final UI, you can check if
you've got the same report line for twice--double

00:24:31.480 --> 00:24:36.759
report line for a customer and you can decide
that, "Hey, I'm going to drop one," but it's

00:24:36.759 --> 00:24:42.700
really, really easy to create another idempotent
version. You have to specify a unique name

00:24:42.700 --> 00:24:48.830
for report lines. Just, for example, build
it from report idea and customer idea, and

00:24:48.830 --> 00:24:54.549
Datastore will make sure that you do not have
double report lines with the same operation.

00:24:54.549 --> 00:25:01.809
And what will actually happen here if you're
going to process your customer twice is that

00:25:01.809 --> 00:25:07.269
you will try to put it once again, it fails
and the next time there's a first statement

00:25:07.269 --> 00:25:12.330
of this report generation function. You should
probably check that you already have a report

00:25:12.330 --> 00:25:18.169
line into the database and if you have it,
no need to generate another one. And the last

00:25:18.169 --> 00:25:25.919
one, which we had previously, which is obviously
not idempotent function which yields counter

00:25:25.919 --> 00:25:33.360
increments. If you process the user twice,
you'll increment it and count it twice. And

00:25:33.360 --> 00:25:38.980
frankly speaking, there is no easy way to
achieve idempotence in this case. But one

00:25:38.980 --> 00:25:45.990
might argue that you did not need idempotence
in this case. And that’s why in most of

00:25:45.990 --> 00:25:52.799
the most legal run your reports over a live
data. Data which is being constantly changes

00:25:52.799 --> 00:26:01.250
and by the whole nature reports of these kinds
like distributions of users are approximation.

00:26:01.250 --> 00:26:07.759
So if error rate is not really high, you might
have one or two users of like, you will figure

00:26:07.759 --> 00:26:16.679
out that there are not 15,000 users of aged
30, but 15,000 plus one user is sorted, so

00:26:16.679 --> 00:26:24.389
nobody cares. And these kinds of reports are
really problematic. But we do not know right

00:26:24.389 --> 00:26:33.960
now what's the really good idempotent rate
to generates those. So in summary, we've got

00:26:33.960 --> 00:26:40.100
the mapper library which is available today.
You can already download it. Play with it.

00:26:40.100 --> 00:26:47.190
It's reliable, fast and efficient. It can
iterate over Datastore, or as I have shown

00:26:47.190 --> 00:26:52.759
you, it can iterate over blob files. We have
two ways to iterate over blob files already.

00:26:52.759 --> 00:26:55.981
The first one is to iterate over zip file
and the other one is to iterate over lines

00:26:55.981 --> 00:27:04.649
in a text file so you can easily build whatever
input to your data. We've got Python now and

00:27:04.649 --> 00:27:12.200
Java really, really soon. All the code is
fully open-source. And before we go with the

00:27:12.200 --> 00:27:19.380
questions, let’s check how our MapReduce
is going on. So, I'm going to refresh the

00:27:19.380 --> 00:27:24.710
page and this is the tab job which we have
run and it took two minutes, 49 seconds to

00:27:24.710 --> 00:27:31.889
create over one kilobyte of text data which
is probably quite good. I would say that this

00:27:31.889 --> 00:27:38.249
is comparable performance with the workstation.
There's are local data and all the data is

00:27:38.249 --> 00:27:44.950
in cloud obviously. And if you care, there
is a list of all the crap in the file. So

00:27:44.950 --> 00:27:54.860
now we are going back to questions. So if
you haven’t seen, this is the URL which

00:27:54.860 --> 00:28:16.500
is bit.ly/ds6t9F. Any live questions while
it loads?

00:28:16.500 --> 00:28:21.781
&gt;&gt; This is about idempotency.
&gt;&gt; AIZATSKY: Yes.

00:28:21.781 --> 00:28:29.610
&gt;&gt; Typically, when you have idempotency, you
know that all of this data needs to be committed,

00:28:29.610 --> 00:28:31.640
right?
&gt;&gt; AIZATSKY: Yes.

00:28:31.640 --> 00:28:41.549
&gt;&gt; But let's say there is a business transaction
where there is a counter transaction involved,

00:28:41.549 --> 00:28:44.909
like, for example, you book a hotel and then
a car, and then a flight, and then when flight

00:28:44.909 --> 00:28:45.909
gets cancelled.
&gt;&gt; AIZATSKY: Yes.

00:28:45.909 --> 00:28:49.730
&gt;&gt; And then you go and redo all of them. If
you have already committed and if you're trying

00:28:49.730 --> 00:28:55.950
to do an idempotent action in that case, you
know, you need a counter transaction where

00:28:55.950 --> 00:28:58.799
idempotent will not, idempotency will not
work.

00:28:58.799 --> 00:29:04.090
&gt;&gt; AIZATSKY: Yes, so, basically, what you
need to do is to keep track of which transactions

00:29:04.090 --> 00:29:12.840
you have applied recently for example. Just
keep tombstones and just--to make sure that

00:29:12.840 --> 00:29:19.260
you haven’t done that.
&gt;&gt; Is...

00:29:19.260 --> 00:29:23.454
&gt;&gt; AIZATSKY: What?
&gt;&gt; Is there a project which has something

00:29:23.454 --> 00:29:24.454
like change something or a snapshot of a particular
transaction that you are planning to build?

00:29:24.454 --> 00:29:26.809
&gt;&gt; AIZATSKY: You see this idempotency is not
connected only to change it, life changing

00:29:26.809 --> 00:29:32.679
data. Yes, you could somehow like freeze your
dataset, redirect a live traffic to somewhere

00:29:32.679 --> 00:29:38.179
or to like my maintenance page and try to
do MapReduce. But again, the mapper staff

00:29:38.179 --> 00:29:44.570
is running on a distributed system which has
failures and it might execute itself twice

00:29:44.570 --> 00:29:49.649
on the same entity. So it's just, it's the
way our Google MapReduce work. It’s the

00:29:49.649 --> 00:29:55.990
way our MapReduce works. It’s the way Hadoop
works. It's just the way the distributed system

00:29:55.990 --> 00:29:59.880
work. Sometimes, you might do this--you have
to be expected to the same work twice. You

00:29:59.880 --> 00:30:05.861
could produce tombstones, keeping track of
what has been done or what not.

00:30:05.861 --> 00:30:18.039
&gt;&gt; This is pretty sweet, man. Nice job. I
was just wondering it seems like it would

00:30:18.039 --> 00:30:22.960
be pretty useful to do some bulk loading of
like big data files. MapReduce is not really

00:30:22.960 --> 00:30:29.759
the intention, like would that not be a good
use case for it? So, instead of bulk loading

00:30:29.759 --> 00:30:30.940
on the client just like upload a blog...
&gt;&gt; AIZATSKY: Yeah.

00:30:30.940 --> 00:30:32.460
&gt;&gt; ...and then process that, create entities.
Would that be like more like efficient?

00:30:32.460 --> 00:30:36.911
&gt;&gt; AIZATSKY: Yes, so the question is should
you use mapper staff or Bulkloader and bulk

00:30:36.911 --> 00:30:43.309
downloading data. Yes, you probably should
and we do plan to rewrite our Bulkloader and

00:30:43.309 --> 00:30:51.730
bulk downloader to users. It just so happens
that these things were developing in parallel

00:30:51.730 --> 00:30:56.629
and we do not want to introduce dependencies.
But now, they are ready, we can just rewrite

00:30:56.629 --> 00:31:01.919
it. And, in the future, you will be able to
do much more from mapper not just read something,

00:31:01.919 --> 00:31:11.450
you would be able to write somewhere and generate
blobs and all those kind of stuff. "So is

00:31:11.450 --> 00:31:16.889
it possible to use threads in the map computations?"
It is not possible to use threads in App Engine

00:31:16.889 --> 00:31:21.900
right now at all, which means that it is not
possible to use threads in map computation.

00:31:21.900 --> 00:31:26.669
"Do you have an example of using App Engine,
MapReduce and combination as Prediction API?"

00:31:26.669 --> 00:31:36.409
No, I do not. Frankly, I did not see Prediction
API working at all yet. So but it is, it should

00:31:36.409 --> 00:31:42.950
be really, really easy to build such scene,
so like the reader for bitreader of our zip

00:31:42.950 --> 00:31:46.759
files was built by one of our developer relations
guy, you know, like 30 minutes. So I would

00:31:46.759 --> 00:31:50.749
doubt it's going to be probably easy to adapt
that kind of API. "Please could you compare

00:31:50.749 --> 00:31:59.880
the pros and cons of [INDISTINCT] for MapReduce,
Batch Processing versus using the new Google

00:31:59.880 --> 00:32:04.570
Storage and the BigQuery APIs." The BigQuery
API is specifically tailored for analyzing

00:32:04.570 --> 00:32:10.340
huge datasets while with mapper you can do
much more than that. First of all, you can

00:32:10.340 --> 00:32:16.769
work on a mutable data because BigQuery requires
your data to be frozen. You upload it to the

00:32:16.769 --> 00:32:22.159
BigStore and you live there forever without
any changes. You can analyze that, it has

00:32:22.159 --> 00:32:29.149
some utility value but it's not going to work
over your live changes into Datastore. And

00:32:29.149 --> 00:32:34.029
the other thing which is really important,
you can actually change Datastore from the

00:32:34.029 --> 00:32:39.889
map library, which is what is being used a
lot like Brett was migrating lots of data,

00:32:39.889 --> 00:32:46.749
lots of Hadoop and issuing high [INDISTINCT]
library. "Can you discuss an example of Batch

00:32:46.749 --> 00:32:52.919
Processing with Google Storage." Yes, it is
possible to build a reader which will read

00:32:52.919 --> 00:32:58.769
data because Google Storage has a block reading,
you can specify offset and read from that

00:32:58.769 --> 00:33:04.299
offset. It's very similar to our blobstore
API, so it is possible and it will probably

00:33:04.299 --> 00:33:09.279
build its own reader which will iterate over
dating Google Storage. We do not care where

00:33:09.279 --> 00:33:15.230
the data comes from. As soon as you can fetch
data, as soon as you can efficiently split

00:33:15.230 --> 00:33:20.840
it, you can you can integrate over like S
three dataset if you need to. "And how can

00:33:20.840 --> 00:33:26.080
App Engine overcome bandwidth's limitation
if Batch Processing is to be done in the cloud?"

00:33:26.080 --> 00:33:33.909
Upload your data to the cloud and you won't
have bandwidth problems. So anymore questions

00:33:33.909 --> 00:33:44.330
from the audience?
&gt;&gt; So I have an application which I’d like

00:33:44.330 --> 00:33:48.740
to send say, for example, 5,000 emails pretty
rapidly and I normally would have just used

00:33:48.740 --> 00:33:53.009
the Task Queue, but it seems like it maybe
really easy to do using the mapper, right,

00:33:53.009 --> 00:33:56.320
as I understand it.
&gt;&gt; AIZATSKY: Yes, you can you can do whatever

00:33:56.320 --> 00:34:01.360
you want from map record. You can write any
kind of code inside mapper. Any legal App

00:34:01.360 --> 00:34:06.049
Engine code is legal to use inside the map
handler functions. If you want to send exemplifi

00:34:06.049 --> 00:34:11.940
messages, fine. Just send those. We just build
those yielding operations because we have

00:34:11.940 --> 00:34:14.190
provided it some special properties like we
batch them together.

00:34:14.190 --> 00:34:18.400
&gt;&gt; So you took away a lot of the complexity
of writing my own Task Queues?

00:34:18.400 --> 00:34:20.210
&gt;&gt; AIZATSKY: Yes, that’s kind of the idea,
kind of interesting.

00:34:20.210 --> 00:34:22.320
&gt;&gt; Super.
&gt;&gt; AIZATSKY: So you can send emails, notifications,

00:34:22.320 --> 00:34:24.050
whatever. And great limit, it is especially
important for sending emails because you cannot

00:34:24.050 --> 00:34:31.970
send more than certain amounts of emails per
fixed amount of time. You're not a spammer.

00:34:31.970 --> 00:34:34.450
Yes, probably.
&gt;&gt; Can I ask?

00:34:34.450 --> 00:34:36.150
&gt;&gt; AIZATSKY: Yes.
&gt;&gt; The bulk downloader...

00:34:36.150 --> 00:34:43.560
&gt;&gt; AIZATSKY: Yes.
&gt;&gt; ...is it true that by default--I mean,

00:34:43.560 --> 00:34:56.990
I have to write a class to download--I would
think that if I want to download, shouldn’t

00:34:56.990 --> 00:35:03.550
it just download everything in that entity?
You know, look, why do I have to write a class

00:35:03.550 --> 00:35:08.760
to do, in the exporter class? And the future
one will it do everything by default?

00:35:08.760 --> 00:35:13.830
&gt;&gt; AIZATSKY: Honestly, I cannot comment about
Bulk Downloader because I'm not purely on

00:35:13.830 --> 00:35:15.130
top of the recent development.
&gt;&gt; Oh, yeah.

00:35:15.130 --> 00:35:18.490
&gt;&gt; AIZATSKY: So you should probably come to
our office hours and there might be people

00:35:18.490 --> 00:35:22.010
who know about that.
&gt;&gt; And you don’t know about the future either,

00:35:22.010 --> 00:35:54.170
because you said that it's going to be part
of mapper.

00:35:54.170 --> 00:35:55.170
&gt;&gt; AIZATSKY: Yes, like Brett...
&gt;&gt; SLATKIN: I'm sorry. So, [INAUDIBLE] going

00:35:55.170 --> 00:35:56.170
to talk [INDISTINCT] going to talk [INDISTINCT]
syntax for import and export, like you download

00:35:56.170 --> 00:35:58.980
everything. And then, the idea for these things
to report together in the future so you could

00:35:58.980 --> 00:36:00.220
use mapper so you could both import and export,
to make it much more efficient. So, that's--okay.

00:36:00.220 --> 00:36:01.220
&gt;&gt; How does it compare to Adobe?
&gt;&gt; AIZATSKY: We're trying to give open sourced

00:36:01.220 --> 00:36:03.270
friendly APIs so that you are familiar with
Adobe then you can run your code in App Engine

00:36:03.270 --> 00:36:08.750
with little or maybe even no modifications.
So, in the future one, we’ll have a full

00:36:08.750 --> 00:36:15.070
MapReduce stack, this produced part which
we'd like to give you sooner than later. You'll

00:36:15.070 --> 00:36:20.190
be able to run almost Adobe code on this stuff,
but there are no connections with Adobe implementations

00:36:20.190 --> 00:36:24.290
per se.
&gt;&gt; Is it going to be faster?

00:36:24.290 --> 00:36:28.940
&gt;&gt; AIZATSKY: Well, Adobe does not work on
a Google infrastructure.

00:36:28.940 --> 00:36:33.830
&gt;&gt; Yes, but...
&gt;&gt; AIZATSKY: So, we cannot, we cannot use

00:36:33.830 --> 00:36:40.060
it.
&gt;&gt; But doing the same kind of analysis on

00:36:40.060 --> 00:36:46.980
your MapReduce and on Adobe, which one would
be faster?

00:36:46.980 --> 00:36:54.650
&gt;&gt; AIZATSKY: Sorry, I did not get the...
&gt;&gt; If you ran the same--let's say you want

00:36:54.650 --> 00:36:55.650
to analyze, you run your own crap search...
&gt;&gt; AIZATSKY: Yes.

00:36:55.650 --> 00:36:56.650
&gt;&gt; …on Google and then you run the same
on Adobe.

00:36:56.650 --> 00:36:57.650
&gt;&gt; AIZATSKY: Yeah, so, if your engineer would
careful enough then the odds are high that

00:36:57.650 --> 00:37:03.910
you would be able to run the same result modifications
in the header cluster or in the App Engine

00:37:03.910 --> 00:37:07.820
cluster.
&gt;&gt; But do you know which one will be faster

00:37:07.820 --> 00:37:10.130
to be able to...
&gt;&gt; AIZATSKY: No idea.

00:37:10.130 --> 00:37:13.530
&gt;&gt; [INDISTINCT].
&gt;&gt; AIZATSKY: Right now, though we are quite

00:37:13.530 --> 00:37:18.660
pleased with its performance, the performance
was not our goal right now. So our goal was

00:37:18.660 --> 00:37:25.790
to give you as flexible as easy to as possible.
And, if we figure out that there are customers

00:37:25.790 --> 00:37:26.790
who care about certain terabytes, we'll see
what we’re going to do next.

00:37:26.790 --> 00:37:29.150
&gt;&gt; Thanks.
&gt;&gt; Hi. You know, so once you have the APIs

00:37:29.150 --> 00:37:33.240
in place, map and reduce and they're kind
of proven, would you consider switching out

00:37:33.240 --> 00:37:38.170
the implementation to allow for a more parallel
execution? Instead of using the chaining technique

00:37:38.170 --> 00:37:44.470
internally but, you know, if that’s abstracted
away...

00:37:44.470 --> 00:37:55.740
&gt;&gt; AIZATSKY: Yes, we do consider that direction.
But right now, we realized that probably most

00:37:55.740 --> 00:38:03.710
of our customers are small to medium apps.
By medium I mean medium datasets by Google

00:38:03.710 --> 00:38:10.200
standards. So as soon as there is a customer
who has hundreds of terabytes of data, we

00:38:10.200 --> 00:38:14.950
might give him another solution.
&gt;&gt; So, if I ask the right people perhaps there's

00:38:14.950 --> 00:38:19.440
a preview, something coming up.
&gt;&gt; AIZATSKY: Well, we have nothing to announce

00:38:19.440 --> 00:38:26.740
at this point.
&gt;&gt; Okay. All right. Thank you.

00:38:26.740 --> 00:38:33.160
&gt;&gt; AIZATSKY: So, thank you all for coming.
I hope you'll try it and you'll find it useful

00:38:33.160 --> 00:38:34.100
for your apps.

