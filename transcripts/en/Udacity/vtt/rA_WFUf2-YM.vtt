WEBVTT
Kind: captions
Language: en

00:00:00.320 --> 00:00:02.910
Okay, Michael. So, I just gave you, you know, one

00:00:02.910 --> 00:00:04.680
particular way you might do filtering. But let's see if

00:00:04.680 --> 00:00:06.140
we can expand on that a little bit and come

00:00:06.140 --> 00:00:08.410
up with some similar ways we might do wrapping. Okay?

00:00:08.410 --> 00:00:09.310
&gt;&gt; Sure.

00:00:09.310 --> 00:00:12.410
&gt;&gt; Okay so, I just gave an example of

00:00:12.410 --> 00:00:15.470
a kind of filtering you might do through decision trees.

00:00:15.470 --> 00:00:18.470
But really what I'm doing there is I'm defining

00:00:18.470 --> 00:00:23.460
some criterion. And in this case it was information gain.

00:00:23.460 --> 00:00:23.940
&gt;&gt; Right.

00:00:23.940 --> 00:00:25.390
&gt;&gt; This is the way that I will, this is

00:00:25.390 --> 00:00:29.150
the function that I will use to evaluate the usefulness

00:00:29.150 --> 00:00:31.450
of a subset of features. How much information does that

00:00:31.450 --> 00:00:33.980
feature give me about a class label? Can you think

00:00:33.980 --> 00:00:36.590
of any other kind of filtering criteria we might use?

00:00:36.590 --> 00:00:41.950
&gt;&gt; So, I don't know, variance. Like so, I want features that have

00:00:41.950 --> 00:00:43.590
lots of different values. That'll do

00:00:43.590 --> 00:00:45.480
something similar to information gain, I suspect.

00:00:45.480 --> 00:00:50.080
&gt;&gt; M'kay. Yep. I like that one. So, we might call that variance.

00:00:51.630 --> 00:00:55.190
Or actually another version of that would be entropy. And, there's tons of

00:00:55.190 --> 00:00:58.640
ways to do this. Right, so, there's, there's something called the G index which

00:00:58.640 --> 00:01:02.120
is a kind of diversion of entropy. There's variance, as you telling me

00:01:02.120 --> 00:01:07.590
here that you're trying to pick features that show up a lot. Anything else?

00:01:07.590 --> 00:01:10.030
&gt;&gt; I could imagine running a neural net and

00:01:10.030 --> 00:01:12.710
then pruning away any input features that have low weight.

00:01:12.710 --> 00:01:16.910
&gt;&gt; Okay. How would we describe that? That's that's

00:01:16.910 --> 00:01:19.390
actually a kind of information gain too right?

00:01:19.390 --> 00:01:21.580
Because if the neural network doesn't need it, then

00:01:21.580 --> 00:01:24.630
somehow it doesn't seem to be terribly useful.

00:01:24.630 --> 00:01:25.870
In fact why don't we just write that down.

00:01:25.870 --> 00:01:26.610
&gt;&gt; Oh, I have another one.

00:01:26.610 --> 00:01:27.990
&gt;&gt; Mm-hm.

00:01:27.990 --> 00:01:31.490
&gt;&gt; So, if there's features that are linearly dependent

00:01:31.490 --> 00:01:34.410
on other features then maybe get rid of them.

00:01:34.410 --> 00:01:37.660
&gt;&gt; So you want independent features?

00:01:37.660 --> 00:01:38.090
&gt;&gt; Yeah.

00:01:38.090 --> 00:01:41.183
&gt;&gt; Or maybe we'll say non-redundant.

00:01:41.183 --> 00:01:41.820
&gt;&gt; Oh, but in this,

00:01:41.820 --> 00:01:43.660
in this kind of of linear algebraic sense.

00:01:43.660 --> 00:01:48.630
&gt;&gt; Right. So if I have feature. Let's say x2

00:01:48.630 --> 00:01:53.550
which turns out to be equal to, you know, x1 plus

00:01:53.550 --> 00:01:59.200
x3 then I should probably get rid of x2. Is that what you're suggesting?

00:01:59.200 --> 00:02:01.020
&gt;&gt; Sure, yeah, that's that is what I'm saying.

00:02:01.020 --> 00:02:03.830
&gt;&gt; Okay, and that makes sense because I don't need x2.

00:02:03.830 --> 00:02:06.880
&gt;&gt; Well, in principle it gives, yeah as you

00:02:06.880 --> 00:02:11.170
said redundant information. But it also is the case that, for some

00:02:11.170 --> 00:02:14.740
learners like neural nets, it shouldn't really,

00:02:14.740 --> 00:02:16.320
you'll feel the difference. For things

00:02:16.320 --> 00:02:19.740
like decision trees, it might matter though because having it, having those

00:02:19.740 --> 00:02:23.250
sums together in one place could be more useful to split on.

00:02:23.250 --> 00:02:25.190
&gt;&gt; That's true, but I do think you said

00:02:25.190 --> 00:02:27.070
something important there, which, you said it shouldn't feel

00:02:27.070 --> 00:02:29.790
the difference. You're defining whether this is good or

00:02:29.790 --> 00:02:32.380
bad in terms of whether it helps you to learn.

00:02:32.380 --> 00:02:36.430
But if we can get rid of a feature, then we save, as

00:02:36.430 --> 00:02:39.410
we get rid of features, we save exponentially on the need for data.

00:02:39.410 --> 00:02:40.840
&gt;&gt; Yes. So as long as it doesn't

00:02:40.840 --> 00:02:42.640
make the learning problem harder, that's a good thing.

00:02:42.640 --> 00:02:46.270
&gt;&gt; Right. So long as it makes the learning problem easier, in terms of the

00:02:46.270 --> 00:02:49.800
amount of data you need, it's a win, up until the point where it makes

00:02:49.800 --> 00:02:51.910
the learning problem harder because you don't

00:02:51.910 --> 00:02:54.200
have enough features to do the actual learning.

00:02:54.200 --> 00:02:55.620
And there's a bunch of these. And I

00:02:55.620 --> 00:02:57.860
think all of these are probably good answers.

00:02:57.860 --> 00:03:00.490
You could think of all kinds of statistical tests or

00:03:00.490 --> 00:03:03.130
relevance. I mean basically are built in here these are

00:03:03.130 --> 00:03:05.660
all different ways of doing the same thing which is

00:03:05.660 --> 00:03:08.370
figuring out which features you need in order to do learning

00:03:08.370 --> 00:03:11.910
by some generic criteria. I do want to point out

00:03:11.910 --> 00:03:15.560
that something like Entropy doesn't even depend on the labels.

00:03:15.560 --> 00:03:17.860
Where something like Information Gain which is a kind of

00:03:17.860 --> 00:03:22.800
conditional Entropy, doe's depend upon knowing the labels. What about wrapping?

00:03:22.800 --> 00:03:24.410
How would you go about doing the wrapping? So we

00:03:24.410 --> 00:03:27.260
got a bunch of different ways we might do filtering. What

00:03:27.260 --> 00:03:30.240
are sort of the way in which you could imagine doing

00:03:30.240 --> 00:03:32.650
a wrapping? You got some learning, don't know what it is.

00:03:32.650 --> 00:03:35.390
You got a bunch of features you want to search over those,

00:03:35.390 --> 00:03:37.540
pass those features on to the learner and see how well

00:03:37.540 --> 00:03:40.530
it does. But you don't want to pay an exponential cost,

00:03:40.530 --> 00:03:43.700
by looking at all possible subsets. What might you imagine doing?

00:03:43.700 --> 00:03:47.990
&gt;&gt; Well, so, I mean we've already talked about a bunch of algorithms that can

00:03:47.990 --> 00:03:53.370
search, that aren't necessarily exponential. So various kinds of local search

00:03:53.370 --> 00:03:56.390
and" hill climbing" could be a way of doing the search part.

00:03:56.390 --> 00:03:59.100
&gt;&gt; Hmm, I like that. Let's write that down, "Hill Climbing". Now when you

00:03:59.100 --> 00:04:00.280
say Hill Climbing, you really mean like

00:04:00.280 --> 00:04:02.630
a gradient, a regular deterministic gradient search?

00:04:02.630 --> 00:04:05.720
&gt;&gt; Hmmmmmmmmm, yeah. I mean, I guess I was thinking

00:04:05.720 --> 00:04:07.840
of something where what does the search algorithm does is

00:04:07.840 --> 00:04:10.420
it tells the learning algorithm here's the features to try

00:04:10.420 --> 00:04:13.110
to use. The learning algorithm runs and it comes back with

00:04:13.110 --> 00:04:16.160
some kind of error. Presumably, on how that validate

00:04:16.160 --> 00:04:19.310
data otherwise, we run the risk of over fitting.

00:04:19.310 --> 00:04:20.060
&gt;&gt; Fair.

00:04:20.060 --> 00:04:22.760
&gt;&gt; And it gives a number back and then the, the

00:04:22.760 --> 00:04:26.650
search algorithm uses that to decide what subset to try next.

00:04:26.650 --> 00:04:29.720
Okay so you imagine that it's going to jump in some gradient

00:04:29.720 --> 00:04:33.230
directly, or is it going to have to do something like randomize optimization.

00:04:33.230 --> 00:04:36.500
&gt;&gt; I think it would have to, yeah I mean like, randomized optimization.

00:04:36.500 --> 00:04:38.200
&gt;&gt; Okay so, in general we can

00:04:38.200 --> 00:04:41.430
take almost everything we just did,

00:04:41.430 --> 00:04:43.390
and the last lesson on randomized optimization

00:04:43.390 --> 00:04:47.020
and apply it here, so then we might just say. Just do random.

00:04:48.690 --> 00:04:52.080
&gt;&gt; Oh, I see. So could be, could be mimic.

00:04:52.080 --> 00:04:56.610
It could be, genetic algorithms. It could be simulated annealing length.

00:04:56.610 --> 00:04:59.370
&gt;&gt; Right. So you can think of, you can do randomized optimization.

00:04:59.370 --> 00:05:00.780
And that's going to be important precisely

00:05:00.780 --> 00:05:03.940
because we want to solve, this exponential problem.

00:05:03.940 --> 00:05:07.980
We don't want that to be an issue. Okay that makes sense. Now

00:05:07.980 --> 00:05:09.620
we've already thrown out exhaustive search

00:05:09.620 --> 00:05:11.100
where we look at all possible features

00:05:11.100 --> 00:05:14.400
because, again, that's exponential. Is there anything else you can think of? I'm

00:05:14.400 --> 00:05:16.040
thinking of one more, so let's see if you can read my mind.

00:05:16.040 --> 00:05:18.639
&gt;&gt; [LAUGH]

00:05:18.639 --> 00:05:20.710
&gt;&gt; And really, when I say I'm thinking of one more, I'm thinking of two more.

00:05:21.720 --> 00:05:25.130
&gt;&gt; Oh. That's, that are not randomized

00:05:25.130 --> 00:05:28.305
optimization algorithms, but they're not going to somehow search.

00:05:28.305 --> 00:05:29.600
&gt;&gt; Mm-hm.

00:05:30.895 --> 00:05:31.980
&gt;&gt; A star?

00:05:31.980 --> 00:05:33.020
&gt;&gt; Yeah, but that's not what I was thinking.

00:05:33.020 --> 00:05:34.820
&gt;&gt; Oh. I give up.

00:05:34.820 --> 00:05:39.079
&gt;&gt; I'll give you a hint. You could go forward or you could go backward.

00:05:40.210 --> 00:05:40.670
&gt;&gt; EM.

00:05:40.670 --> 00:05:45.810
&gt;&gt; [LAUGH] No. No. No. No. Although I like that.

00:05:45.810 --> 00:05:46.050
&gt;&gt; ME?

00:05:46.050 --> 00:05:49.730
&gt;&gt; [LAUGH] That wouldn't be forward or backward, would there? No,

00:05:49.730 --> 00:05:53.430
I'm thinking of exactly a forward sequential selection and backward elimination.

00:05:53.430 --> 00:05:55.700
&gt;&gt; I don't think I know what that means.

00:05:55.700 --> 00:05:56.540
&gt;&gt; Oh. So

00:05:56.540 --> 00:05:57.450
let's talk about that then.

