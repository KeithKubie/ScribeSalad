WEBVTT

1
00:00:00.150 --> 00:00:02.670
How much time have you put into artificial intelligence?

2
00:00:02.780 --> 00:00:07.440
And we do a lot of work in my lab on Ai.
What about sex robots?

3
00:00:07.470 --> 00:00:12.000
Like what rules should they give for sex robots and how much could that damage

4
00:00:12.030 --> 00:00:15.000
interpersonal relationships?
That's a great question.

5
00:00:15.030 --> 00:00:16.920
That's exactly the right question in my view.

6
00:00:17.250 --> 00:00:22.250
So our concern with sex robots from a liberty point of view should not in the

7
00:00:22.861 --> 00:00:26.940
slightest.
B,
whether you enjoy a sex robot,
it's your business,
right?

8
00:00:27.000 --> 00:00:30.750
You'd buy or Bot do what you want.
I really don't.
I see.

9
00:00:30.751 --> 00:00:35.580
I find it would be hard pressed to,
to object.
The problem is with said,
well,

10
00:00:35.700 --> 00:00:37.530
let's back up from the less provocative.
Let's come back to sex.

11
00:00:37.570 --> 00:00:39.330
But I looked like a simpler example.
First let's talk.

12
00:00:39.360 --> 00:00:42.710
Let's talk about your children talking to Alexa.
Okay,

13
00:00:42.750 --> 00:00:46.620
so the person who designs Alexa wants to make your child's experience easy and

14
00:00:46.621 --> 00:00:49.410
pleasant,
and as part of the programming of Alexa,

15
00:00:49.770 --> 00:00:52.950
because they want to make Alexa the obedient servant of your child,

16
00:00:53.310 --> 00:00:57.270
it doesn't require a child to say,
please,
Alexa,
would you play the music for me?

17
00:00:57.930 --> 00:01:00.150
Your child can be as rude as she wants to.

18
00:01:00.151 --> 00:01:03.690
Alexa and Alexa will do what she wants.
What you should be concerned about,

19
00:01:03.691 --> 00:01:04.021
however,

20
00:01:04.021 --> 00:01:07.380
is not your child's interaction with Alexa would you should be concerned about

21
00:01:07.381 --> 00:01:10.110
is what your child is learning from interacting with Alexa.

22
00:01:10.111 --> 00:01:13.620
That then she takes to the playground.
So now she's rude to other children.

23
00:01:14.730 --> 00:01:17.790
So Alexa is corroding our social fabric.
Alexa,

24
00:01:17.791 --> 00:01:20.730
in this example is making children rude to each other.

25
00:01:21.240 --> 00:01:26.160
So our concern is not so much do we make and do we make,

26
00:01:26.161 --> 00:01:28.940
you know,
like Asimov's laws of robotics?
Do we we,

27
00:01:29.010 --> 00:01:32.770
it's not that we want to program the robots so that their don't harm you.
We we,

28
00:01:33.060 --> 00:01:34.110
it's true.
The first law,

29
00:01:34.111 --> 00:01:38.120
we don't want the robot to through an act of commission or omission harmer allow

30
00:01:38.130 --> 00:01:39.330
human to come to be harmed.

31
00:01:39.870 --> 00:01:43.290
It's that we're concerned about how the robot and interacting with you might

32
00:01:43.291 --> 00:01:46.320
cause you to harm others.
The robot,

33
00:01:46.350 --> 00:01:50.250
the robotic intelligence creates these externalities,
these cascade effects.

34
00:01:50.520 --> 00:01:51.930
So in the Alexa example,

35
00:01:52.260 --> 00:01:57.260
we might want to regulate the programming of devices that speak to children.

36
00:01:58.440 --> 00:02:01.650
Not because we want to deprive your daughter,

37
00:02:01.651 --> 00:02:04.080
have the right to speak how she wants,

38
00:02:04.470 --> 00:02:07.920
but because we recognize that that robot is going to cause your daughter to be

39
00:02:07.921 --> 00:02:11.580
rude to other people.
Is it really?
Do you really think?
Yes,
the Alexa,

40
00:02:12.210 --> 00:02:15.870
what's the weather that that would make your slowly but surely?

41
00:02:15.871 --> 00:02:19.110
I think it will contribute.
So that's a,
it's an example.
It's not like,

42
00:02:19.111 --> 00:02:23.670
I'm not arguing that Alexa should become ornately think it's so novel to kids

43
00:02:23.671 --> 00:02:26.220
that they know it's not a person.
I don't think it really all right,

44
00:02:26.221 --> 00:02:27.990
but we're using these examples to build a thing.

45
00:02:27.991 --> 00:02:29.400
So let's talk about the sex robots.

46
00:02:30.360 --> 00:02:34.510
So some people believe that actually the,

47
00:02:34.520 --> 00:02:38.100
the emergence of sex robots,
which will surely appear in the next 10 or 20 years,

48
00:02:38.580 --> 00:02:43.380
um,
will,
um,
will be a fantastic boon.
They think that,

49
00:02:43.410 --> 00:02:46.790
um,
you'll people be able to experiment.
Uh,

50
00:02:46.820 --> 00:02:49.950
you'll be able to experiment with same sex relationships.
For example,

51
00:02:49.951 --> 00:02:54.540
a group sex.
Uh,
you might learn to be a better lover.

52
00:02:54.840 --> 00:02:58.500
So he could practice with the robots and therefore you'd be more experienced

53
00:02:58.501 --> 00:03:00.550
when you having sex with a real human.

54
00:03:00.910 --> 00:03:03.970
So they said you can't get venereal diseases from a sex robots.

55
00:03:04.210 --> 00:03:07.570
You can't hurt their feelings.
Uh,
so people think that,
uh,

56
00:03:07.600 --> 00:03:10.990
the argument based on ethical grounds is that this would be terrific,
uh,

57
00:03:10.991 --> 00:03:14.890
that this will be a benefit.
Other people have the opposite opinion.

58
00:03:15.250 --> 00:03:18.970
Other people think that actually having sex with a robots first of all is

59
00:03:18.971 --> 00:03:23.950
symbolically and,
and,
um,
conceptually vile.
They think that,
you know,
it,
it,

60
00:03:23.960 --> 00:03:27.660
it did take sex and converts it into a kind of a,
a machine,

61
00:03:27.760 --> 00:03:30.710
literally a machine like a,
you know,
function.
Uh,

62
00:03:30.720 --> 00:03:34.840
and they furthermore think that it would result in you in one having a kind of

63
00:03:34.841 --> 00:03:38.750
anonymous or impersonal interactions with human subsequently that you'll be in

64
00:03:38.751 --> 00:03:43.630
trained,
you know,
to let's say want an obedient,
uh,
you know,
partner,
uh,

65
00:03:43.631 --> 00:03:46.060
for example,
I don't have a stand on this.

66
00:03:46.061 --> 00:03:48.520
Like I don't know which way it's going out.
And in a way I don't have to get,

67
00:03:48.521 --> 00:03:49.331
make a stand on it.

68
00:03:49.331 --> 00:03:53.350
Because what I'm interested in recognizing is that when we talk about having,

69
00:03:53.351 --> 00:03:55.840
allowing people to have sex with sex,
robots not allowing there,

70
00:03:55.841 --> 00:03:56.674
it's going to happen.

71
00:03:57.130 --> 00:04:01.180
The focus of our concern should be not what is your experience in your bedroom

72
00:04:01.181 --> 00:04:06.130
when you have sex with a sex robot?
Our concern is a state like my interest.

73
00:04:06.570 --> 00:04:09.700
I have no stake or control over what you're doing over there,

74
00:04:10.330 --> 00:04:13.360
but my interest is in,
in once you have had that experience,

75
00:04:13.361 --> 00:04:15.880
how does that change how you interact with other people?
Right?

76
00:04:16.510 --> 00:04:19.150
And there I think just like anything else,
like you can,

77
00:04:19.510 --> 00:04:21.970
you can make all the garbage you want in your house,

78
00:04:23.050 --> 00:04:25.720
but if you start polluting the environment,
you're harming me.

79
00:04:25.840 --> 00:04:29.500
So now I have a reason for intervening in your activities on your land.

80
00:04:29.740 --> 00:04:33.370
You can't pollute your own land if that pollution runs off onto my land.

81
00:04:34.330 --> 00:04:37.870
And so the similar argument can be made or look at autonomous vehicles.

82
00:04:37.871 --> 00:04:38.704
Here's an example.

83
00:04:40.750 --> 00:04:42.970
Right now we have all roads.

84
00:04:42.971 --> 00:04:46.480
Almost all roads have just human drivers and in 20 or 30 years,

85
00:04:46.960 --> 00:04:49.600
almost all roads will probably have only nonhuman drivers.

86
00:04:49.601 --> 00:04:54.310
Machines will drive and those autonomous vehicles probably can be yoked

87
00:04:54.311 --> 00:04:57.850
together.
They can communicate with each other so that you'll have like,

88
00:04:58.330 --> 00:05:01.090
like trains of cars moving in synchrony.

89
00:05:01.240 --> 00:05:03.760
Like each of them will be communicating with the other nearby cars and you'll

90
00:05:03.761 --> 00:05:07.930
have laminar flow where all these vehicles are smoothly moving and joining the

91
00:05:07.931 --> 00:05:11.290
highway and leaving the highway and communicating on a citywide scale,

92
00:05:11.440 --> 00:05:15.490
slowing traffic down miles away because they anticipate with AI that there'll be

93
00:05:15.491 --> 00:05:19.600
a jam here if they don't do that.
And,
and I think that'll be actually great.

94
00:05:19.630 --> 00:05:21.550
I'm actually looking forward to autonomous.
I mean,

95
00:05:22.090 --> 00:05:25.690
I still like to take my car to a speedway,
but you know,
drive itself with stick,

96
00:05:25.691 --> 00:05:27.160
which I like,
but you know,

97
00:05:28.180 --> 00:05:32.710
but in between we're going to have a world of what I call hybrid systems of

98
00:05:33.040 --> 00:05:38.040
human driven cars and autonomous vehicles coexisting in an on a plane,

99
00:05:39.100 --> 00:05:39.933
on an even plane.

100
00:05:40.660 --> 00:05:44.560
And we need to be worried about that because these autonomous vehicles,

101
00:05:44.561 --> 00:05:47.710
when we interact with them are going to change how we interact with each other.

102
00:05:48.580 --> 00:05:49.480
For example,

103
00:05:50.170 --> 00:05:54.070
do we program the autonomous vehicle to drive at a constant steady speed?

104
00:05:54.100 --> 00:05:57.010
If you're the designer of the car,
you might say,
Gee,

105
00:05:57.020 --> 00:05:59.210
I don't want this car to crash a,

106
00:05:59.211 --> 00:06:01.790
I want the car to drive in a very predictable fashion.

107
00:06:02.150 --> 00:06:04.250
And that's what's best for the occupants of the car.

108
00:06:04.251 --> 00:06:05.960
That's what's gonna allow me to sell more vehicles.

109
00:06:07.640 --> 00:06:11.900
But it may be the case that actually when people are in contact with such a

110
00:06:11.901 --> 00:06:15.290
vehicle,
they get lulled into a false sense of security.
Oh,

111
00:06:15.370 --> 00:06:16.850
I'd vehicle never does anything new.

112
00:06:17.210 --> 00:06:19.160
I don't need to pay so much attention to the car in front of me.

113
00:06:19.340 --> 00:06:22.220
I just miss drive,
you know,
at a steady clip.

114
00:06:22.730 --> 00:06:25.760
And then they veer off and they go to a part of the highway where they're just

115
00:06:25.761 --> 00:06:29.690
human drivers.
And now having been lulled into a false sense of security,

116
00:06:29.980 --> 00:06:32.420
they cause more collisions is not paying attention.

117
00:06:33.890 --> 00:06:37.430
So that autonomous vehicle has changed how I drive in a way that harms other

118
00:06:37.431 --> 00:06:38.264
people.

119
00:06:38.750 --> 00:06:42.710
So maybe the programming of the vehicle should be to occasionally do erratic

120
00:06:42.711 --> 00:06:46.520
things too.
Like suddenly slow down or speed up a little bit,

121
00:06:46.790 --> 00:06:50.600
obliging me to stay vigilant and pay attention as I'm interacting with that car.

122
00:06:50.840 --> 00:06:52.820
So that then when I go to another part of the highway,

123
00:06:52.821 --> 00:06:57.200
when I interact with just humans,
I have retained that vigilance.
Once again,

124
00:06:57.201 --> 00:07:01.160
the lesson here is that it's not just about the one on one interaction between

125
00:07:01.161 --> 00:07:04.160
the robotic artificial intelligence and the human being.

126
00:07:04.460 --> 00:07:06.920
It's about how the robot's affect us.
And in my lab,

127
00:07:06.921 --> 00:07:11.240
we do many experiments in social systems where we take a group of people and we

128
00:07:11.241 --> 00:07:12.560
drop online,

129
00:07:12.561 --> 00:07:17.561
we drop a Bot or in the laboratory we have a physical robot and we watch how the

130
00:07:17.601 --> 00:07:22.601
presence of the robot doesn't just modify how the human interacts with a robot,

131
00:07:23.300 --> 00:07:26.600
but how the humans interact with each other.
So if we put a robot right,

132
00:07:26.601 --> 00:07:31.340
they're looking at us with its third,
I would,
we,
uh,
you know,

133
00:07:31.341 --> 00:07:34.520
would it change how you and I talked to each other,
make us different.

134
00:07:35.000 --> 00:07:37.040
That's the experiments we're doing.
Well,

135
00:07:37.310 --> 00:07:42.010
clearly in the sex robot realm,
that's going to be a problem.
I mean we,

136
00:07:42.011 --> 00:07:45.600
we see the difference between humans that,
uh,

137
00:07:45.680 --> 00:07:49.250
have porn addictions.
Yeah,
that's a good example.
Yeah.
Porn addictions.

138
00:07:49.251 --> 00:07:49.970
When people do,

139
00:07:49.970 --> 00:07:54.260
they developed this very impersonal way of communicating with people and they,

140
00:07:54.560 --> 00:07:58.310
they think about sex and the objectification of the opposite sex in a very

141
00:07:58.311 --> 00:08:00.530
different reason,
a very different way.

142
00:08:00.820 --> 00:08:05.210
And it flavors the way you add flavors,
your expectations.
Yes.
Yes.

143
00:08:05.211 --> 00:08:06.200
And it makes it difficult.

144
00:08:06.201 --> 00:08:09.860
It can make it difficult for you to have normal sexual relationships if you come

145
00:08:09.861 --> 00:08:14.690
to see if your expectations are,
are,
uh,

146
00:08:14.720 --> 00:08:19.720
guided by a porn and that is going to be radically magnified by some sort of

147
00:08:21.261 --> 00:08:25.760
artificial life form that you created that's indistinguishable.
Yes.
Have,

148
00:08:25.761 --> 00:08:29.690
you can have an indistinguishable sex partner that is,
you know,

149
00:08:29.691 --> 00:08:34.691
some incredibly beautiful woman that is a robot and then you or man,

150
00:08:35.031 --> 00:08:38.720
let's go,
man,
women should be quite happy to change their spouses for robots.

151
00:08:39.110 --> 00:08:43.020
I wonder if women are going to be as into it as men because I think women divine

152
00:08:43.060 --> 00:08:44.360
desire,

153
00:08:44.510 --> 00:08:49.450
more emotional intimacy and I think,
I mean,
uh,

154
00:08:49.460 --> 00:08:54.170
on a,
on a scale than men do.
I,
I think,
um,

155
00:08:54.260 --> 00:08:56.590
I think the jury is out on know what,

156
00:08:56.730 --> 00:09:00.810
what the relative balance between men and women,
we might be surprised that,
uh,

157
00:09:01.170 --> 00:09:03.020
that will be replanted males,

158
00:09:03.220 --> 00:09:07.600
especially given x societal expectations and women can to those.
And,

159
00:09:07.720 --> 00:09:10.830
and given how your pain in the ass a lot of men can be.
Sure.

160
00:09:10.860 --> 00:09:12.330
So it could go both ways.
I don't,

161
00:09:12.331 --> 00:09:15.720
I'm not prepared to make a prediction who's going to be better off in the gender

162
00:09:16.050 --> 00:09:19.530
debate with the emergence of sex robots.
And they'll maybe you way you suggest,

163
00:09:19.531 --> 00:09:20.281
I don't know.
Well,

164
00:09:20.281 --> 00:09:24.770
we're also in this weird position genetically where they're doing genetic ex,
ah,

165
00:09:24.800 --> 00:09:29.280
experiments on humans.
And with the advent of CRISPR emerging technologies,

166
00:09:29.281 --> 00:09:30.480
I talked about that in the book too.

167
00:09:30.660 --> 00:09:34.110
Entirely possible that there's not gonna be any frumpy bodies anymore.

168
00:09:34.830 --> 00:09:39.330
That that's hundreds of years away.
But is it?
Yes,
I think so.
I wonder,
I mean,

169
00:09:39.331 --> 00:09:40.320
I don't know if it is,

170
00:09:40.321 --> 00:09:43.410
I think if they start cracking them out in China and they start giving birth to

171
00:09:43.650 --> 00:09:48.030
eight foot tall superman,
yes.
12 inch Dicks,
yes.
We're going to have a real issue.

172
00:09:48.420 --> 00:09:52.380
Yes.
Uh,
so we will,
uh,
yes,
that's the least of it,

173
00:09:55.350 --> 00:09:58.890
like God,
but I mean it's really entirely possible in the future.

174
00:09:58.891 --> 00:10:02.160
They're going to have that and then we're going to likely cumins yes.

175
00:10:02.161 --> 00:10:04.620
I think that is likely the debate is how far in the future.

176
00:10:04.650 --> 00:10:09.650
So I don't think we're going to start by using these technologies to cure a

177
00:10:10.410 --> 00:10:14.130
monogenic diseases.
So,
you know,
like thalassemia for example,

178
00:10:14.340 --> 00:10:16.200
so a diseases or certain immune deficiencies,

179
00:10:16.201 --> 00:10:20.040
a disease where single gene is defective and uh,

180
00:10:20.070 --> 00:10:22.830
and those will be the initial targets.
But once we start with that,

181
00:10:22.831 --> 00:10:26.430
eventually I think there will be people who will want to genetically engineer

182
00:10:26.431 --> 00:10:29.310
other people,
uh,
their offspring for example,

183
00:10:29.311 --> 00:10:32.490
and a modified them in the ways that you suggest,
maybe not 12 inch Dick's,

184
00:10:32.491 --> 00:10:36.350
but maybe,
you know,
ability to run fast or something else.
Far Smarter.
I mean,
yes.

185
00:10:36.360 --> 00:10:39.070
Isn't that one of these side effects that they showed with the,
um,

186
00:10:39.150 --> 00:10:43.580
genetic manipulation of these Chinese babies to,
uh,
eliminate HIV there,

187
00:10:43.610 --> 00:10:46.230
that they made them smarter?
No,
I don't know if they made them smarter there.

188
00:10:46.440 --> 00:10:48.660
What's clear from the most recent findings?

189
00:10:48.661 --> 00:10:53.661
I've seen from that case is that a unsurprisingly is anyone could predict the

190
00:10:54.121 --> 00:10:58.830
technology is not good enough to restrict the mutations to one particular region

191
00:10:58.831 --> 00:10:59.664
of the genome.

192
00:11:00.000 --> 00:11:03.360
So there were other changes in the genome in these children that occurred

193
00:11:03.420 --> 00:11:05.580
elsewhere rather than the targeted region,

194
00:11:05.581 --> 00:11:10.500
which was to increase their immunity to HIV.
And we don't know what those are,

195
00:11:10.950 --> 00:11:14.700
but those could kill those kids quickly.
We could make them better in some ways.

196
00:11:14.701 --> 00:11:16.110
We have no way of knowing yet.

197
00:11:16.140 --> 00:11:19.550
But I think their conclusion was that it increased their intelligence.
I don't,

198
00:11:19.551 --> 00:11:22.680
I think it's,
I have not seen those results and I think it would be premature.

199
00:11:22.681 --> 00:11:26.230
I find that it would because they're very mature to come to their babies does.

200
00:11:26.250 --> 00:11:29.540
Yeah.
The problem is also sensationalist clickbait,

201
00:11:29.730 --> 00:11:31.410
which is that's what you want to click,
you know,

202
00:11:31.411 --> 00:11:34.230
not just that they did the HIV and they made them smarter is going to get like

203
00:11:34.231 --> 00:11:38.730
40% more clicks.
Yes.
Ooh.
Versus,
yeah.
Woo.
40%.

204
00:11:39.400 --> 00:11:43.800
Yeah.
That,
I mean,
that's,
that's just the nature of humans,
right?
Yes.
Um,

205
00:11:43.980 --> 00:11:47.730
just to be clear,
I talk about the CRISPR example in,
in uh,
in blueprint.

206
00:11:47.820 --> 00:11:50.370
I actually talk about these,
how these technologies,
again,

207
00:11:50.371 --> 00:11:55.371
my lens on it is how these technologies are going to change how we interact with

208
00:11:55.751 --> 00:11:56.081
each other.

209
00:11:56.081 --> 00:11:58.720
And it goes back to the example we were talking about at the beginning when we

210
00:11:58.721 --> 00:11:59.980
invented cities.

211
00:12:00.370 --> 00:12:02.800
That was a technology that changes how we interacted with each other.

212
00:12:03.280 --> 00:12:07.600
So human beings for a very long time had been inventing when we invented
weapons.

213
00:12:07.930 --> 00:12:10.360
That was a technology that changed how we interact with each other.

214
00:12:10.840 --> 00:12:13.050
So we have previously done this kind of thing.

215
00:12:13.180 --> 00:12:15.580
We've invented a technology that changes how we interact with each other and I'm

216
00:12:15.581 --> 00:12:20.110
very interested in,
in the and discuss some of those implications.
Yeah.
I'm,

217
00:12:20.410 --> 00:12:24.850
I'm incredibly interested in this because I love to study history and I love to

218
00:12:24.851 --> 00:12:29.851
study how crazy the world was four thousand five thousand years ago,

219
00:12:30.640 --> 00:12:33.730
a thousand years ago,
what it's going to be like in the future.

220
00:12:33.730 --> 00:12:37.000
I just think our understanding of the consequences of our actions are so

221
00:12:37.001 --> 00:12:41.590
different than anybody has ever had before.
We have just such a broader,

222
00:12:42.910 --> 00:12:43.451
first of all,

223
00:12:43.451 --> 00:12:48.040
we have examples from all over the world now that we can study very closely,

224
00:12:48.070 --> 00:12:52.390
which I don't think really was available to that many people up until fairly

225
00:12:52.391 --> 00:12:54.190
recently.
You mean?
I'm sorry.

226
00:12:54.191 --> 00:12:57.220
You're saying the examples are more numerous or capacity to discern them is

227
00:12:57.221 --> 00:13:01.680
higher.
Our capacity to discern them and just our in depth understanding of these

228
00:13:01.681 --> 00:13:03.480
various cultures all over the world.

229
00:13:03.540 --> 00:13:07.920
Like what do you've been telling you today about these,
the divers and others?

230
00:13:07.950 --> 00:13:12.390
We just have so much more data and so much more of an understanding than ever

231
00:13:12.391 --> 00:13:16.200
before.
Yes,
I love the idea that we are,

232
00:13:16.740 --> 00:13:21.180
I mean I believe that this is probably the best time ever to be alive and I

233
00:13:21.181 --> 00:13:22.800
think that it's probably,
I think that's true.

234
00:13:22.890 --> 00:13:26.250
I think there's certainly a lot of terrible things that are wrong in the world

235
00:13:26.251 --> 00:13:27.090
today also true.

236
00:13:27.300 --> 00:13:32.300
But I think that there's less of that and more good.

237
00:13:32.800 --> 00:13:35.200
Exactly that too before.
No,
I think that's right.
And,

238
00:13:35.201 --> 00:13:38.590
but one of the arguments that I make is this is a kind of Steven pinker argument

239
00:13:38.591 --> 00:13:42.400
that you're outlining,
which is,
you know,
with the emergence of,
I mean,

240
00:13:42.401 --> 00:13:44.650
people are living longer than they ever have on the whole planet.

241
00:13:44.890 --> 00:13:48.730
Fewer people in starvation.
We have less violence.
I mean,

242
00:13:48.731 --> 00:13:52.460
every indicator of human wellbeing is up.
Uh,

243
00:13:52.570 --> 00:13:57.520
and it's partly due,
are largely due in the recent last thousand years to the,
uh,

244
00:13:57.820 --> 00:14:00.590
to the emergence of the enlightenment and the Phyllis,

245
00:14:00.591 --> 00:14:04.900
the philosophy and the science that was guided that emerged about 300 years ago

246
00:14:04.901 --> 00:14:06.860
and 200 and some odd years ago.
And,

247
00:14:06.940 --> 00:14:10.450
and culminating in the present and continuing.
So I think,

248
00:14:10.510 --> 00:14:13.150
I think this is not just the kind of so-called weekish view of history.

249
00:14:13.150 --> 00:14:15.220
It's not just a progressive sort of fantasy.

250
00:14:15.730 --> 00:14:19.810
I think it's the case that these philosophical and scientific moves that our

251
00:14:19.811 --> 00:14:23.830
species made in the last few hundred years has improved our wellbeing.
However,

252
00:14:24.220 --> 00:14:25.450
as we've been discussing today,

253
00:14:26.110 --> 00:14:31.110
it's not just historical forces that are tending towards making us better off a

254
00:14:32.231 --> 00:14:36.370
deeper and more ancient and more powerful forces also at work,

255
00:14:36.580 --> 00:14:37.840
which is natural selection.

256
00:14:37.870 --> 00:14:41.680
It's evolutionary and not just historical forces that are relevant to our

257
00:14:41.681 --> 00:14:42.514
wellbeing.

258
00:14:42.700 --> 00:14:47.200
And we don't just need to look to philosophers to find the path to a good life.

259
00:14:47.770 --> 00:14:51.100
Natural selection has equipped us with these capacities for love and friendship

260
00:14:51.110 --> 00:14:53.570
and cooperation and teaching and all these good things we've been discussing

261
00:14:53.840 --> 00:14:57.890
that also tend to a good life.
So,
so yes,
I totally agree with you.

262
00:14:57.891 --> 00:15:02.480
We're better off today than we've ever been on average across the world.
However,

263
00:15:02.750 --> 00:15:05.240
it's not just that that's contributing to our wellbeing.

264
00:15:05.840 --> 00:15:10.840
This natural selection is literally why we are in this state now and why we were

265
00:15:11.871 --> 00:15:14.320
hoping this trend will continue.
Yes,

266
00:15:14.340 --> 00:15:18.770
we will be in this better place 50 years from now,
100 years from now.
Well,

267
00:15:18.771 --> 00:15:20.840
natural selection doesn't work over those time scales.

268
00:15:20.841 --> 00:15:24.710
So those are historical forces.
But the point is we are set up for success.
Yes.

269
00:15:24.920 --> 00:15:27.920
Um,
you know,
we are equipped with these,
uh,
you know,

270
00:15:28.160 --> 00:15:31.520
you're given five fingers which make it pos and an opposable thumb,

271
00:15:31.730 --> 00:15:33.230
which allows you to manipulate tools.

272
00:15:33.231 --> 00:15:35.750
So natural selection has given you an opposable thumb.

273
00:15:36.050 --> 00:15:37.760
Culture lets you use a computer.

274
00:15:38.000 --> 00:15:42.320
Do you worry about the circumventing of this natural process by artificial

275
00:15:42.321 --> 00:15:46.550
intelligence that artificial intelligence is going to introduce some new,

276
00:15:46.551 --> 00:15:51.551
incredibly powerful factor into this whole chain of events that by having sex

277
00:15:53.001 --> 00:15:57.710
robots and sex or,
or,
or,
or um,
robot workers,

278
00:15:57.740 --> 00:16:02.600
yes.
Things becoming automated.
Yes.
This I'm,
I'm concerned.
I mean,

279
00:16:02.601 --> 00:16:03.080
this is,

280
00:16:03.080 --> 00:16:07.700
I think I'm very concerned about how technology is going to affect our economy.

281
00:16:08.460 --> 00:16:11.630
These,
again,
these concerns were not the first generation to face these concerns.

282
00:16:11.631 --> 00:16:14.480
There were similar concerns with the industrial revolution that workers were

283
00:16:14.481 --> 00:16:18.440
being put out of work when machines were invented.
Nevertheless,
work persisted.

284
00:16:18.441 --> 00:16:22.610
People still had jobs to do.
Um,
there was a disruption.
There's no doubt about it.

285
00:16:22.611 --> 00:16:27.020
I think Google and the information revolution and these types of robotic

286
00:16:27.021 --> 00:16:28.880
automation are disruptive.

287
00:16:28.910 --> 00:16:33.910
They're going to affect how we allocate labor and capital and data in our

288
00:16:34.791 --> 00:16:36.320
society.
There's no doubt about all of that.

289
00:16:36.800 --> 00:16:40.070
I thought you were alluding to just to check if you were to the debate,

290
00:16:40.071 --> 00:16:43.790
which I don't know the answer to on whether AI will,
you know,

291
00:16:43.940 --> 00:16:46.340
are we gonna face like a terminator type existence where you know,

292
00:16:46.341 --> 00:16:50.570
the machine's rise up and kill us all or not,
and you know,

293
00:16:50.600 --> 00:16:52.580
very smart people on both sides of that debate.

294
00:16:52.580 --> 00:16:56.120
And I read them all and like I would like,
he's right.

295
00:16:56.540 --> 00:16:58.580
And then they read the guy that has the opposite of me.
I'm like,
no,
no,

296
00:16:58.760 --> 00:17:01.790
he's right.
And then it goes back and forth.
I don't know who's right.

297
00:17:01.791 --> 00:17:06.680
That goes back to nuance,
right?
Yes.
It is nuanced,
but it's hard to know whether,

298
00:17:06.870 --> 00:17:09.710
again,
we're not talking over our lifetimes,
right.
Over hundreds of years,

299
00:17:10.130 --> 00:17:10.490
you know?

300
00:17:10.490 --> 00:17:13.400
Is there a time a thousand years from now when the human beings will say,

301
00:17:13.401 --> 00:17:16.370
what the hell were our ancestors doing inventing artificial intelligence?

302
00:17:16.420 --> 00:17:20.300
They're wiping us out.
I don't know the answer to that question.

303
00:17:20.301 --> 00:17:24.500
I think there's an issue also with the,
the concept of artificial,

304
00:17:24.760 --> 00:17:28.940
like artificial life,
artificial intelligence.
It's,
I think it's going to be,

305
00:17:28.970 --> 00:17:32.000
life is just going to be a life that we've created.

306
00:17:32.360 --> 00:17:35.360
And I don't think it's artificial.
I just think it's a different kind of life.

307
00:17:35.390 --> 00:17:39.920
I think that we're thinking of biologically based life of sex.

308
00:17:41.060 --> 00:17:41.940
Yes.
You know,
well,

309
00:17:41.960 --> 00:17:46.430
some people should reproduction in terms of the way we've always known it as

310
00:17:46.431 --> 00:17:48.140
being the only way that life exists.

311
00:17:48.141 --> 00:17:52.890
But if we can create something and that something decides to do things,

312
00:17:52.920 --> 00:17:56.670
it's sound create,
live on its own.
Yeah.
It's silicone based life form.

313
00:17:56.671 --> 00:17:57.780
Like why not?
Why?

314
00:17:57.781 --> 00:18:01.950
Why does life have to be something that only exists through the,
you know,

315
00:18:01.951 --> 00:18:06.660
multiplication of cells.
Yes.
That's very charitable of you.
And uh,

316
00:18:07.140 --> 00:18:10.230
it's,
people make that claim.
Uh,
some people think that,
you know,

317
00:18:10.231 --> 00:18:11.940
those machines in the distant future,

318
00:18:11.941 --> 00:18:16.410
we'll look back at us as like one stage of evolution that culminated in them

319
00:18:16.920 --> 00:18:21.900
that we're,
I've always said that we are some sort of an electronic,
uh,

320
00:18:21.930 --> 00:18:25.410
Caterpillar that doesn't know that it's going to give birth to a butterfly.

321
00:18:25.550 --> 00:18:27.780
We're making your cocoon.
We don't even know what you're doing.

322
00:18:27.930 --> 00:18:29.160
That's a great metaphor.

323
00:18:29.730 --> 00:18:32.940
I have a hard time accepting that because you're a person.
Yes,

324
00:18:33.240 --> 00:18:35.520
it's against my interests were,
we're so flawed.

325
00:18:35.521 --> 00:18:38.670
All these things that we've outlined lollypops those would go away with

326
00:18:38.671 --> 00:18:41.280
artificial intelligence.
It's a deep philosophical question,
Joe.

327
00:18:41.740 --> 00:18:44.700
I don't think it's inevitable and I think if the single celled organisms are

328
00:18:44.701 --> 00:18:46.740
sitting around wondering what the future would going to be like,

329
00:18:47.040 --> 00:18:50.550
where are we going to be replaced?
Will they make antibiotics and kill us?
Yes.

330
00:18:50.730 --> 00:18:54.830
Yes.
They are going to kill us.
I mean this is,
I mean we are so flawed.

331
00:18:54.831 --> 00:18:59.040
We do the parade,
the ocean.
We do pull the fish out of it.
If you fuck up the air,

332
00:18:59.180 --> 00:19:01.830
do commit genocide.
There's all these things that are real,

333
00:19:02.280 --> 00:19:05.190
but the artificial life won't have those problems because it won't be

334
00:19:05.191 --> 00:19:09.180
emotionally based,
won't be biologically based.
It'll just exist.

335
00:19:11.010 --> 00:19:14.750
That's a really good story.
We're so flawed.
Why not?

336
00:19:14.751 --> 00:19:18.810
Not so much better.
Oh,
we're very flawed.
We are flawed,
but like I said,

337
00:19:19.630 --> 00:19:22.800
I'm not going to,
we're very flat though.
We are fly.
I think it's beautiful.

338
00:19:22.920 --> 00:19:25.470
Beautiful too.
But I think vultures probably think they're beautiful too.

339
00:19:25.471 --> 00:19:27.730
That's why they breed with each other.
Well,
they are beautiful,
but,

340
00:19:27.731 --> 00:19:29.850
but the point is I think we have a flop beauty.
I like,

341
00:19:29.970 --> 00:19:33.990
I'm going to stick to my principles that we are,
despite our flaws worth it.

342
00:19:34.260 --> 00:19:35.820
There's something wonderful about us.

343
00:19:35.821 --> 00:19:40.821
And I think that that wonderful creative quality is the reason why we created

344
00:19:40.981 --> 00:19:45.810
artificial life in the first place.
It's like this,
this creation.

345
00:19:45.840 --> 00:19:49.170
We've had that impetus.
You know,
if you look at a lot of the,
um,

346
00:19:50.040 --> 00:19:53.340
the art,
whether it's the Egyptian,
you know,
the,
the,
the,

347
00:19:53.341 --> 00:19:57.270
the pyramids or other kinds of,
um,
artistic expression,

348
00:19:57.271 --> 00:20:00.570
we seem to have had a desire to transcend death,
you know,

349
00:20:00.571 --> 00:20:05.571
to make things that sure that looked like us but weren't alive forever actually.

350
00:20:06.510 --> 00:20:10.530
So,
I mean,
I think in that regard,
I think you're quite right that,
um,

351
00:20:10.770 --> 00:20:14.220
it's not gonna stop that Tennessee is not gonna stop now.
You're,
you're very,

352
00:20:14.221 --> 00:20:14.641
as I said,

353
00:20:14.641 --> 00:20:19.641
charitable positive take on the claim and your analogy to a single celled

354
00:20:21.271 --> 00:20:23.910
organisms,
which are just,
you know,
but a fleeting,
not a fleeting,

355
00:20:23.911 --> 00:20:26.880
they're still there,
but a phase in our evolution,
you know,

356
00:20:26.881 --> 00:20:29.580
is something I'm going to have to be thinking about because it's disturbing,

357
00:20:29.581 --> 00:20:31.680
honestly.
Well it's an objective perspective.

358
00:20:31.681 --> 00:20:34.230
If I took myself out of the human race,
which I really can't,

359
00:20:34.231 --> 00:20:38.430
but if I tried to fake it,
I would say,
oh I see what's going on here.
Yeah.

360
00:20:38.520 --> 00:20:39.690
These gummies is,
yes,

361
00:20:39.691 --> 00:20:44.691
these dummies are buying iPhones and new Mac books because they know that this

362
00:20:45.121 --> 00:20:49.660
is what's going to help the of newer,
more superior technologies.

363
00:20:49.750 --> 00:20:52.750
The more we consume.
It's also based,

364
00:20:52.751 --> 00:20:57.751
I think in a lot of ways our insane desire for materialism is fueling this and

365
00:20:59.591 --> 00:21:04.510
it could be an inherent property of the human species that it is designed to

366
00:21:04.511 --> 00:21:08.710
create this artificial life and that literally is what it's here for.

367
00:21:08.950 --> 00:21:13.240
And much like an ant is creating an ant hill and doesn't exactly have some sort

368
00:21:13.241 --> 00:21:18.190
of a future plan for its kids and its four o one k plan that what we're doing is

369
00:21:18.191 --> 00:21:22.510
like this inherent property of being a human being,
our curiosity,

370
00:21:22.511 --> 00:21:25.260
our wanderlust,
our design,
our these things.
Yeah.

371
00:21:25.270 --> 00:21:30.270
All these things are built in because if you follow them far enough down the

372
00:21:30.671 --> 00:21:34.900
line a hundred years,
200 years,
it inevitably leads to artificial life.
Yes.

373
00:21:34.990 --> 00:21:38.710
I think,
uh,
I think that's possible.
Um,

374
00:21:39.070 --> 00:21:41.950
and of course we're not going to be alive to see,
to test that idea.

375
00:21:42.340 --> 00:21:46.760
There's Sun will maybe with CRISPR and all this crazy shit that's coming down.

376
00:21:46.810 --> 00:21:50.290
Oh come on.
I know there's nothing's gonna happen in the pace of innovation.

377
00:21:50.291 --> 00:21:52.870
People always had been said.
If you go back every decade,

378
00:21:52.871 --> 00:21:54.610
people say just around the corner,
just around the corner,

379
00:21:54.640 --> 00:21:56.290
these things that are take forever.
They're very hard.

380
00:21:56.291 --> 00:21:59.770
Biological systems are very hard to engineer.
Um,
I don't,
and you know,

381
00:21:59.771 --> 00:22:02.650
of course the people who do that kind of work will often,

382
00:22:03.220 --> 00:22:06.040
I think a lot of them engage in snake oil.
You know,
they'd want a fund.
There was.

383
00:22:06.041 --> 00:22:06.460
Sure,

384
00:22:06.460 --> 00:22:09.700
but I think it's entirely possible that there's a 20 year old listening to this

385
00:22:09.701 --> 00:22:13.360
podcast will be 150 yes.
That's possible.
Maybe a lot more than that.

