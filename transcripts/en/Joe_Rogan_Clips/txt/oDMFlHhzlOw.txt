Speaker 1:          00:00          That's happen. The Joe Rogan experience. What's Matt? I don't remember if we brought this up last time, but I just remembered seeing this video where you're playing guitar while you were driving. Yup. Well you shouldn't do that, dude. There's a reason why he's doing it. Why are you doing this on a test track? Oh, what kind of cars that look see at Lincoln Lincoln Mkc yes. Oh they do that. The Lincolns do that. We converted it and we, that's The r code controlling the car. Wow. And I'm done. It's crazy. Yep. So you converted this car to drive autonomously anonymously. Yeah. Wow. And what, what exactly do you have to do to a car to change like car, because that car does not have the capacity, the capacity to do anything like that. So the right my correct. No, no, no, no, absolutely not. But you are absolutely correct.

Speaker 1:          00:52          The, there's the first part is being able to control the car with a computer, which is converting it to be drive by wire so you can control the steering and the braking acceleration to, to basically be able to control with a joystick. And then you have to put laser sensors all around the cars that we do. 10 a sensor and a software. What's the best kind of sensor is that optical laser. A lot of debate on this. And this is the big, this is the throwdown between Ilan Musk and everybody else. So y'all Musk says the best sensors, camera, everybody else. Well, everybody else says that at this time. Lidar, which are these lasers? Yes, this is the best sensor. So I'm more on the side in this case on camera, on Eli Musk. So here's the difference. Lasers are more precise. They worked better. And in poor lighting conditions, they're more reliable.

Speaker 1:          01:42          You can actually build safe systems today that use lidar. The problem is that they don't have, they're much information. So we use our eyes to drive and uh, cameras, the same thing. And they have just a lot more information. So if you're going to build artificial intelligence systems, so the machine learning systems that learn from huge amounts of data cameras, the way to go because you can learn so much more, you can see so much more. So the, the richer, deeper censor his camera, but it's much harder. You have to collect a huge amount of data. It's a little bit more futuristic. So it's longer term solution. So today to build a safe vehicle, you have to go lidar tomorrow, however you define tomorrow, he almost says it's in a year. Others say it's five, 10, 20 years cameras, the way to go. So that's, that's the, the hard debate.

Speaker 1:          02:33          And there's, there's a lot of other debates, but that's one of the core ones. It's basically four camera. You, if you go camera, like you're doing the Tesla, there's seven cameras in your Tesla three looking forward, there's all around. So on one looking inside, no, you have the model s yeah, yeah. So that one doesn't have a camera that's looking inside. So it's all, all cameras plus radar and ultrasonic sensors. That approach requires collecting huge amounts of data and they're doing that. They drove now about 1.3 billion miles under autopilot. Cheeses as it's a, it's a very large amount of data. So you're talking about, uh, over 500,000 vehicles have autopilot 450. I think thousand have the new version of autopilot, autopilot to which is the one you're driving. And all of that is data. So all of those, all the edge cases, what they call them, all the difficult situations that occur is feeding the machine learning system to become better and better and better.

Speaker 1:          03:36          And the open question is how much better does need to get to get to the human level performance? And like one of the big thing, one of the, uh, big assumption, so I seen my beings is that we think that driving is actually pretty easy. And we think that humans suck a driving those two assumptions. We think like driving, you know, you stay in the lane, you stop at the stop sign, it's pretty easy to automate. And then the other one is you think like humans are terrible drivers and so there'll be easy to build a machine that outperforms humans at driving. Now there's, that's a, I think there's a lot of flaws behind that intuition we take for granted how hard it is to look at the scene. Like everything you just did picked up, moved around some objects. It's really difficult to build an artificial intelligence system that does that, to be able to perceive and understand the seen enough to understand the physics of the scene. Like all these objects that it like how to pick them up, the texture of those objects, the weight to understand glasses folded and unfolded, open water bottle, all those things is common sense knowledge that we take for granted. We think it's trivial, but there is no artificial system in the world today. Nor will there be for perhaps quite awhile that can reason do that kind of common sense. Reasoning about the physical world. Add to that, uh, pedestrians. So add some crazy people in this room right now to the whole scene.

Speaker 2:          05:07          Ryan, being able to notice like this guy is an asshole. Look on him. Was he doing, what is he doing? Get off that skateboard. Oh, Jesus isn't in traffic. Yeah. Yup.

Speaker 1:          05:13          And the considering not that he's an asshole, he's a respectable skateboarder. It that in order to make him behave a certain way, you yourself have to behave a certain way. So it's not just you have to perceive the world. You have to act in a way that you have to assert your presence in this world. You have to take risks. So in order to make the skateboard or not cross the street, you have to perhaps accelerate if you have the right away. And these are the, there's a game theoretic end game of chicken to get right. I mean what do we even know how to approach that as a, as uh, artificial intelligence sort of research community and also as a society do we want an autonomous vehicle that speeds up in order to make a pedestrian not crossed the street, which is what we do all the time.

Speaker 1:          06:04          We have to assert our presence. If there's a, if there's a person who doesn't have the right away, who could begins crossing, we're going to either maintain speed or speed up potentially if we want them to not cross so that, that game there to get that right. It's a dangerous game for a robot. It's for robot and for us to be rationally, if that, God forbid, least a fatality for us as a society to rationally reason about that. Think about that. I mean a fatality like that could basically bankrupt the company. There's a lawsuit going on right now, um, about, uh, uh, an accident in northern California with the Tesla. Yeah. And Are you aware about, yeah, that one. Yeah, this was the circumstances about that one. So there was a, I believe in mountain view, a fatality in a Tesla where it, this is a common problem for all, all link keeping systems.

Speaker 1:          07:00          Like, like that's a lot of pilot is a, those, a divider in the highway and basically the car was driving, you know, along lane and then the car in front moved to an adjacent lane and this divider appeared, right? So you have to now steer to the right and the car didn't end once straight into the divider. Oh Wow. And it's, you know, uh, the, basically what that boils down to is the car drifted out of lane, right? Or it didn't adjust properly to the lane. And those kinds of things happen. And this is because the person was allowing the autopilot to do everything that you can't. So we have to be extremely careful here. I don't know that really deep details of the case. I'm not sure exactly how many people do. So there's a judgment on what the people, the person was doing and then there's analysis of what the system did, right.

Speaker 1:          07:51          The system did, it drifted out of lane. And the question is did the person, was the person paying attention and was there enough time given for the person to take over and if they were paying attention to catch the vehicle, steer back onto the road. As far as I believe, uh, the only information they have his hands on the steering wheel and they were saying that like half the half the minute leading up to the crash, the hands weren't on the steering wheel or something like that. Basically trying to infer where the person paying attention or not, but we don't have the information exactly what wa where were their eyes? You can only make guesses as far as I know. Again, so the, the question is, this is the eyes on the road thing because I think I've heard Janet podcast saying you're attempted to sort of look off the road with your new Tesla or at least become a little bit complacent. That's the worry, the worry, the worry is that you, you just rely on the thing that you would relax too much. But what would that relaxation lead to that the problem is if something happened, if you weren't,

Speaker 2:          08:56          you know when you're driving, I mean we've discussed this many times on the podcast to the reason why people have road rage. One of the reasons is cause you're in a heightened state because cars are flying around you and your brain is prepared to make split second decisions and moves and the worry is that you would relax that because you're so comfortable with that thing driving everybody that I know that it's tried that they say you get really used to it doing that. You get really used to it just driving around for you.

Speaker 1:          09:26          So the question is what be, what happens when you get used to it? You start looking at off road, do you start texting more? Do you start watching a movie? It's that uh, that's a, that's a really an open question. And the like for example, we just did the study, uh, the published a study from the MIT on what people in our Dataset we have, we'll collect this data set of 300,000 miles and Tesla's, we instrumented all these Teslas and watch what people are actually doing and are they, uh, paying attention when they disengage the system. So there's a really important moment here. We have 18,000 of those when the person catches the car, you know, the disengage autopilot. And that's a really, Tesla uses this moment as well. That's a really important window into difficult cases. So some percentage of those, some small percentage, about 10% is we call them.

Speaker 1:          10:19          Tricky situations is situations where you have to immediately respond, like drifting out of lane if there's a stopped car in front. So on, the question is, are people paying attention during those moments? So in our Dataset, they were paying attention. There were still remaining vigilant. Now in our Dataset, the autopilot was going on quote, encountering tricky situations every 9.2 miles. So you could say it was a failing every 9.2 miles. Like that is one of the reasons we believe that people are still paying it, remaining vigilant, that it's regularly and unpredictably sort of drifting out of the lane or misbehaving so you don't overtrust it. You don't become too complacent. The open question is when it becomes better and better and better and better, will you start becoming complacent when it drives on the highway for an hour, an hour and a half, and as opposed to 9.2 miles, make that 50 miles, 60 miles. Do you start to overtrust it and that's a really open question,

Speaker 2:          11:24          do you think or do you anticipate a time in anywhere in the near future where you won't have to correct. You will allow the car to do it because the car will be perfect. The car, first of all, we'll never be perfect. No car will ever be perfect. Autonomous Vehicles will always, you think require at least some sort of manual override. Yeah, really. That's interesting that you're saying that because you work in AI, like what makes you think that that's impossible to

Speaker 1:          11:54          achieve? Well, let's, let's talk cause cause you're using the word perfection. I think. Perfection. That's a bad word. Yeah. So you're, I guess you're implying that, let me see. Will it achieve, because people are obviously not perfect yet. Will it achieve a state of competence that exceeds the human being and let's uh, put it in a dark way, uh, competence measured by fatal crashes. Yes. Uh, yes. I absolutely believe so. Uh, and perhaps in the near term, near term, like five years. Yup. For me, five, 10 years is near term for Ilan in Ilan Musk time that converted to one year. Have you met him? Yes. Interviewed him recently. Fascinating cat, right? Yup. Got a lot of weird shit bouncing around behind those eyeballs. You don't realize until you talk to them in person, you're like, oh you got a lot going on in there, man.

Speaker 1:          12:49          Yeah. There's this passion, this drive. I mean it's one of the hurricane of ideas. Yeah. And a focus and confidence. I mean, the thing is in a lot of the things he does, which I admire greatly from any man or a woman innovator, it's just boldly, fearlessly pursuing new ideas or jumping off the cliff and learning to fly in the way down that that's, I mean, well, no matter what happens, he'll be remembered as the great innovators of our time. Whatever you say may be in my book, Steve Jobs was as well. Even if you criticize, perhaps he hasn't contributed significantly to the technological development of the company or the different ideas they did. Still his brilliance was in all the products of iPhone, of the personal computer, the Mac and so on. And I think the same is true with, uh, with uh, with Ilan and yes, there's, in this space of autonomous vehicles, of, of semi autonomous vehicles, of driver assistance systems, it's a pretty tense space to operate in.

Speaker 1:          13:58          There's several communities in there that are very responsible but also aggressive in their criticism. So in driving in the automotive sector, obviously since Henry Ford and before there's been a culture of safety, of just great engineering. These are like some of the best engineers in the world in terms of large scale production. You talk about Toyota, you talk about Ford, GM, these people know how to do safety well and so care comes he along with silicon valley ideals that throws a lot of it out the window and says we're going to revolutionize the way we do automation in general. We'll go into make software updates to the car once a week, twice a week over the air. Just like that. That makes people in the safety engineers and human factors engineers really uncomfortable. Like what do you mean you're going to keep updating the software of the car without like how are you testing it?

Speaker 1:          14:56          All right, that makes people really uncomfortable. Why does it make them uncomfortable? Because the way in the automotive sector, you test the system, you come up with a design of the car, every component, and then you go through like really rigorous testing before it ever hits the road. Right? Here's an idea from an, the Tesla side is where they basically, they in shadow moe test the software, but then they just release it. So essentially the drivers become the testing and then they regularly update it to, uh, to, uh, to adjust a, if any issues arise that makes people uncomfortable because there's not a standardized testing procedure. There's not, there's not at least the feeling in the industry of rigor because the reality is we don't know how to test software in the same kind of, of with the same kind of rigor that we've tasted, the automotive system tested automotive system in the past.

Speaker 1:          15:49          So I think it's extremely exciting and powerful to make software sort of approach, uh, automotive engineering with, at least in part a software engineering perspective. So just doing what's made silicon valley successful. So updating regularly, aggressively innovating on the software side. So your Tesla over the air while we're sitting here could get a tool and you update the flip of a, uh, of a bit as Elon Musk says, uh, it can be, it can gain all new capabilities. That's really exciting, but that's also dangerous and that, that balance, we, uh, what's dangerous about it? That'd be faulty software faulty a bug. So if you're, you're the apps on your phone, you know, fail all the time. Where as a society used to software failing and we just kind of reboot the device where we start the APP a, the most complex software systems in the world today. If we think outside of nuclear engineering and so on, they're really nobody that they're too complex to really thoroughly tests. So a thorough, complete testing proving that the software is safe is nearly impossible. I'm most software systems that that's, that's uh, that's nerve wracking too. A lot of people because, uh, this, there's no way to prove that the new software update is safe.

Speaker 2:          17:19          So what is, what is the process like, do you know, like how they create software, they update it and then they tested on something? How much testing do they do and what, how much did they do before they upload it to your car?

Speaker 1:          17:34          Yeah, so I don't have any inside information, but I have a lot of sort of public available information, which is, uh, they, uh, they test the software in shadow mode, meaning they see how the new software compares to the current software by it in parallel on the cars and seeing if there's disagreements, if like, uh, seeing if there's any major disagreements and bringing those up and seeing what parallel, I'm sorry. Do you mean both programs running at the same time? One, the original opt? Yes. At the same time, the original update actually controlling the car and the new update is, uh, just making the same decision, making the same decisions without them being, without showing the actual okay. Without actually affecting the vehicles dynamics. And so that's, that's a really powerful way of testing. I think the software infrastructure that Tesla's built allows for that.

Speaker 1:          18:30          And I think other companies should do the same. That's a really exciting, powerful way to approach not just a automation, not just the timeless vehicles or send me a town's vehicles, but just safety is basically all the data that's on cars. Bring it back to a central point to where you can use the edge cases, all the weird situations in driving to improve the system, to test the system, to learn to understand where the cars used misused, how can be improved and so on. That's an extremely powerful, how many people do they have that are analyzing all this data? It's a, it's a really good question. So they have to do, the interesting thing about driving is most of it is pretty boring. Nothing interesting happens. So they have automated ways of extracting, again, what are called edge cases. So these weird moments of driving and once you have these weird moments, they have people annotate it.

Speaker 1:          19:28          I don't know what the number is, but a lot of companies are doing this. It's in the hundreds and the thousands basically have humans annotate the data to see what happened. But most of what they're trying to do is to automate that annotation. So to figure out how the da Da da, it can be automatically used to improve the system. So they, they have, they have methods for that because it's a huge amount of data. Right. I think in the recent autonomy day, a couple of weeks ago, they had this big autonomy day where they demonstrated the vehicle driving itself on a particular stretch of road. They, uh, they showed off that, you know, they're able to query the data, basically ask questions of the data is saying, uh, the example they gave is there's a bike on the back of a car, the bicycle on the back of a car.

Speaker 1:          20:14          And they're able to say, well, when the bicycles in the back of a car that's not a bicycle, that's just the part of the car. And they're able to now look back into the data and find all the other cases, the thousands of cases that happened all over the world in Europe, in Asia, in South American and North American, so on and pull all those elements and then train the train the perception system of autopilot to be able to, to, uh, better recognize those bicycles as part of the car. So every edge case like that, they go through saying, okay, the car freaked out in this moment. And let me find moments like this in the rest of the data and then improve the system. So it's, it's, uh, this kind of cycle is the way to deal with, uh, with problems with failures of the system is to say every time the car fails at something, say, is this part of a bigger set of problems?

Speaker 1:          21:11          Can I find all those problems and can I improve it with a new update? And that just keeps going. The open question is how many loops like that you have to take for the car to become really good, better than human to basically how hard is driving. How many weird situations when you manually drive do you deal with every day somebody, uh, somebody mentioned, I don't know, there's like millions of cases when you watch video, you see them, somebody mentioned, um, that they drive a truck or ups truck and passed cow pastures and they know that if there's no cows in the cow pasture, that means that they're grazing. And if they're grazing, I mean they be using the correct terms. I apologize, not called guy. Uh, that, that means that there may be cows up ahead on the road. There's just this kind of reasoning it can use to anticipate difficult situations. And we do, we do that kind of reasoning about like, everything cars today can't do that kind of reasoning. They're just perceiving what's in front of them.

Speaker 3:          22:20          [inaudible].