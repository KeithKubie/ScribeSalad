WEBVTT
Kind: captions
Language: en

00:00:01.074 --> 00:00:02.240
NICOLAS GEOFFRAY: All right.

00:00:02.240 --> 00:00:04.850
Good afternoon, everyone.

00:00:04.850 --> 00:00:07.290
Thank you for
coming to this talk.

00:00:07.290 --> 00:00:11.570
You'll hear about ART,
the Android Runtime.

00:00:11.570 --> 00:00:14.150
And all about the
new, exciting features

00:00:14.150 --> 00:00:18.070
we've worked on
for Android N. I'm

00:00:18.070 --> 00:00:21.090
Nicolas Geoffray, a software
engineer in the ART team,

00:00:21.090 --> 00:00:23.950
and I'll be presenting
along my colleagues

00:00:23.950 --> 00:00:26.090
Mathieu Chartier
and Calin Juravle.

00:00:28.990 --> 00:00:32.729
So ART is the software
layer between the operating

00:00:32.729 --> 00:00:35.840
system and the applications.

00:00:35.840 --> 00:00:38.780
It provides an
execution environment

00:00:38.780 --> 00:00:41.310
to run Android applications.

00:00:41.310 --> 00:00:44.910
That environment essentially
consists of two things.

00:00:44.910 --> 00:00:49.230
It is a huge text bytecode,
that's the internal Android

00:00:49.230 --> 00:00:54.010
format and Android applications,
through a mix of interpretation

00:00:54.010 --> 00:00:56.270
and computation.

00:00:56.270 --> 00:00:59.410
It also manages memory
of those applications,

00:00:59.410 --> 00:01:04.420
allocating it and reclaiming
it when it's no longer used.

00:01:04.420 --> 00:01:06.450
Why should you guys
care about ART?

00:01:06.450 --> 00:01:09.030
Well, it turns out ART is
at the forefront of the user

00:01:09.030 --> 00:01:11.320
experience.

00:01:11.320 --> 00:01:16.150
ART needs to be fast so that
applications can run smoothly.

00:01:16.150 --> 00:01:18.400
It needs to start
applications really quickly

00:01:18.400 --> 00:01:21.780
to give a snappy
experience to the user.

00:01:21.780 --> 00:01:24.160
It needs to ensure
the UI can render

00:01:24.160 --> 00:01:27.420
at least 60 frames per second
to make the user experience

00:01:27.420 --> 00:01:29.560
jank free.

00:01:29.560 --> 00:01:31.610
It needs to be
power-efficient and not

00:01:31.610 --> 00:01:36.380
do too much extra work besides
executing the application.

00:01:36.380 --> 00:01:38.805
And it needs to be savvy
in terms of memory use.

00:01:38.805 --> 00:01:41.540
The less memory being used
by the Runtime, the more

00:01:41.540 --> 00:01:43.065
applications you
can actually run.

00:01:45.980 --> 00:01:48.830
So if you remember Dalvik,
which was the first Android

00:01:48.830 --> 00:01:51.600
Runtime released
in Android phones,

00:01:51.600 --> 00:01:55.190
it was sort of efficient
on some of these metrics,

00:01:55.190 --> 00:01:58.620
and sort of OK on a few.

00:01:58.620 --> 00:02:03.850
At the time memory
footprint was paramount,

00:02:03.850 --> 00:02:08.520
so we constrained the compiler
to not use much memory,

00:02:08.520 --> 00:02:12.930
and that explains
not-so-great performance.

00:02:12.930 --> 00:02:17.150
Dalvik also had a relatively
unsophisticated garbage

00:02:17.150 --> 00:02:20.500
collector, and that could lead
to long application pauses,

00:02:20.500 --> 00:02:21.775
leading to a janky experience.

00:02:24.390 --> 00:02:29.170
So in 2014 we introduced ART.

00:02:29.170 --> 00:02:33.300
ART shifted the paradigm
of doing interpretation

00:02:33.300 --> 00:02:35.650
and Just-In-Time
computation at runtime

00:02:35.650 --> 00:02:38.314
to a Ahead-Of-Time computation
when the application is

00:02:38.314 --> 00:02:38.980
being installed.

00:02:41.899 --> 00:02:43.690
So when the application
is being installed,

00:02:43.690 --> 00:02:47.970
ART will compile it
directly to optimize code.

00:02:47.970 --> 00:02:51.803
That gave us a great boost on
performance and application

00:02:51.803 --> 00:02:52.303
startup.

00:02:55.190 --> 00:02:58.010
ART did not need a
Just-In-Time compiler

00:02:58.010 --> 00:03:03.920
to be warmed up before
it could execute.

00:03:03.920 --> 00:03:09.500
Applications were running out
of optimized code directly.

00:03:09.500 --> 00:03:12.430
And because this code is
directly loaded from disk,

00:03:12.430 --> 00:03:14.950
you don't need to pay the
cost of a JIT computation code

00:03:14.950 --> 00:03:17.660
cache.

00:03:17.660 --> 00:03:22.167
The garbage collector has
also been completely revamped,

00:03:22.167 --> 00:03:24.250
where we implemented a
state-of-the-art concurrent

00:03:24.250 --> 00:03:28.330
garbage collector algorithms,
and we made sure that GC pauses

00:03:28.330 --> 00:03:32.204
were to a minimal so that
the experience was jank free.

00:03:35.260 --> 00:03:39.500
Since Lollipop we've mostly
improved on performance,

00:03:39.500 --> 00:03:43.000
so we have [? evolution ?]
right on the compiler.

00:03:43.000 --> 00:03:45.440
Lollipop, when we
shipped, had a quick--

00:03:45.440 --> 00:03:48.070
when we shipped Lollipop
it had a quick compiler.

00:03:48.070 --> 00:03:50.890
That was a fast dex thanks
to machine code compiler.

00:03:50.890 --> 00:03:55.434
It was ported from the
Davik JIT at the time.

00:03:55.434 --> 00:03:57.100
At the time we were
really eager to ship

00:03:57.100 --> 00:04:00.340
ART, given all the
benefits, and we mostly

00:04:00.340 --> 00:04:05.000
focused on shipping a
well-tested and robust

00:04:05.000 --> 00:04:07.550
compiler.

00:04:07.550 --> 00:04:10.270
However, [? Quick ?]
was not structurally

00:04:10.270 --> 00:04:15.284
meant for more sophisticated
optimizations, such as inlining

00:04:15.284 --> 00:04:16.284
and register allocation.

00:04:18.890 --> 00:04:23.000
So in Marshmallow we introduced
the optimizing compiler.

00:04:23.000 --> 00:04:26.470
That's a state-of-the-art
SSA-form-based compiler

00:04:26.470 --> 00:04:28.810
infrastructure.

00:04:28.810 --> 00:04:32.962
SSA, in compiler jargon, stands
for Static Single Assignment.

00:04:32.962 --> 00:04:38.250
And that's a well-known
format for doing optimizations

00:04:38.250 --> 00:04:41.900
in the compiler.

00:04:41.900 --> 00:04:45.420
We implement all sorts of
optimizations such as inlining,

00:04:45.420 --> 00:04:47.490
[? bomcheck ?]
emulation, command

00:04:47.490 --> 00:04:52.030
super-expression
emulation, and so on.

00:04:52.030 --> 00:04:54.540
We also implemented a leaner
scan register allocator,

00:04:54.540 --> 00:04:58.442
a state-of-the-art register
allocation technique

00:04:58.442 --> 00:04:59.025
for compilers.

00:05:01.570 --> 00:05:04.580
And in N we iterated
on the compiler

00:05:04.580 --> 00:05:06.520
and made more aggressive
optimizations,

00:05:06.520 --> 00:05:09.842
like more inlining and
lots more optimization.

00:05:14.160 --> 00:05:18.370
This graph shows the
performance of Marshmallow

00:05:18.370 --> 00:05:23.180
and [INAUDIBLE] normalized
for performance in Lollipop.

00:05:23.180 --> 00:05:26.630
[? As ?] benchmarks
run on Nexus 9,

00:05:26.630 --> 00:05:29.850
a tablet where we
[? should ?] Lollipop,

00:05:29.850 --> 00:05:31.880
and the first
[? will leaves ?] of ART.

00:05:31.880 --> 00:05:34.130
And you can see we've
constantly improving performance

00:05:34.130 --> 00:05:36.510
over time.

00:05:36.510 --> 00:05:40.060
We measured on three
different kinds of benchmarks.

00:05:40.060 --> 00:05:43.930
Delta [? and ?] Richards are
well-known object-oriented

00:05:43.930 --> 00:05:47.170
benchmarks in the [? Android ?]
[? community ?] that stresses

00:05:47.170 --> 00:05:53.250
how the runtime implements
[? calls. ?] Dhrystone is about

00:05:53.250 --> 00:05:56.515
how the runtime in the compiler
generates integer computations.

00:06:00.080 --> 00:06:02.830
Reversibench,
Chessbench, and Ritz

00:06:02.830 --> 00:06:08.050
are actually adaptations
of Android applications.

00:06:08.050 --> 00:06:13.550
Reversi and Chess emulate
the AI of an actual game

00:06:13.550 --> 00:06:16.190
that you can download
from the Play Store

00:06:16.190 --> 00:06:18.390
and reads emulate
spreadsheet emulation.

00:06:21.242 --> 00:06:23.700
So we've been very pleased with
the improvements we've made

00:06:23.700 --> 00:06:27.460
here, ranging from
1.2x to 5x speed-ups.

00:06:30.180 --> 00:06:34.326
And the performance boost isn't
specific to one architecture.

00:06:34.326 --> 00:06:35.700
Because most of
our optimizations

00:06:35.700 --> 00:06:41.060
are CPU independent, all
platforms benefit from them.

00:06:41.060 --> 00:06:44.570
So running the same
benchmarks on ARM32

00:06:44.570 --> 00:06:46.400
will lead to the
same improvement

00:06:46.400 --> 00:06:48.440
trend across the board.

00:06:51.310 --> 00:06:52.727
So that's great.

00:06:52.727 --> 00:06:54.560
Improving the code
generated by the compiler

00:06:54.560 --> 00:06:59.850
is a great win for performance,
faster frame rendering,

00:06:59.850 --> 00:07:02.860
and faster start up.

00:07:02.860 --> 00:07:06.049
Great, but--

00:07:06.049 --> 00:07:09.220
[AUDIENCE CHUCKLES]

00:07:09.220 --> 00:07:12.353
It looks like you
guys run into this.

00:07:12.353 --> 00:07:14.478
So you're probably familiar
with that dialogue now.

00:07:18.070 --> 00:07:19.940
It's pretty lucky, right?

00:07:19.940 --> 00:07:23.058
Only 21 to go.

00:07:23.058 --> 00:07:25.010
[AUDIENCE LAUGHTER]

00:07:25.010 --> 00:07:27.950
So that dialogue
is what is shown

00:07:27.950 --> 00:07:30.020
when you take a system update.

00:07:30.020 --> 00:07:32.020
And what happens there,
in case you don't know,

00:07:32.020 --> 00:07:34.050
is that we are redoing
all the optimizations

00:07:34.050 --> 00:07:37.860
we've done at install
time of an application.

00:07:37.860 --> 00:07:39.054
Why?

00:07:39.054 --> 00:07:40.720
Because when you
install an application,

00:07:40.720 --> 00:07:45.410
ART will optimize it heavily
against your platform.

00:07:45.410 --> 00:07:47.500
And it will put
hard-coded dependencies

00:07:47.500 --> 00:07:50.920
in the compiled code that will
make your application run much

00:07:50.920 --> 00:07:53.470
faster.

00:07:53.470 --> 00:07:55.850
But when you get a
system update all

00:07:55.850 --> 00:07:57.930
those hard-coded
dependencies become invalid

00:07:57.930 --> 00:07:59.690
because you've got a new system.

00:07:59.690 --> 00:08:01.540
So we need to redo all the work.

00:08:04.160 --> 00:08:07.130
When we shipped ART, that
was actually a trade-off

00:08:07.130 --> 00:08:11.980
we've made because OTAs or
system updates were usually

00:08:11.980 --> 00:08:13.320
once a year.

00:08:13.320 --> 00:08:16.290
So for a once a year
updates you get yearly

00:08:16.290 --> 00:08:18.133
better Android experience.

00:08:25.090 --> 00:08:28.880
But Android ecosystem
has evolved.

00:08:28.880 --> 00:08:34.320
And security being at the heart
of Android, our security team

00:08:34.320 --> 00:08:36.120
worked hard on
making sure security

00:08:36.120 --> 00:08:42.330
fixes could be sent to Android
phones as soon as possible.

00:08:42.330 --> 00:08:46.610
So when our Lead Security
Engineer and Director

00:08:46.610 --> 00:08:49.820
of Nexus Products
announced we are now

00:08:49.820 --> 00:08:52.710
moving to monthly updates,
it was sort of clear

00:08:52.710 --> 00:08:56.230
that this initial trade-off
we've made would not work

00:08:56.230 --> 00:08:59.830
and ART needed to adapt.

00:08:59.830 --> 00:09:02.660
So we brought back a
Just-In-Time compiler in ART.

00:09:05.170 --> 00:09:08.150
So no more "optimizing
app" dialogue.

00:09:08.150 --> 00:09:09.700
Woo-hoo!

00:09:09.700 --> 00:09:13.165
[AUDIENCE APPLAUSE]

00:09:15.650 --> 00:09:16.330
Thank you.

00:09:21.439 --> 00:09:23.230
It turns out, removing
that dialogue is not

00:09:23.230 --> 00:09:26.640
the only benefit
of having a JIT.

00:09:26.640 --> 00:09:32.320
We now get much faster
installs, around 75% faster.

00:09:32.320 --> 00:09:35.520
And because AOT could not
know, when it was compiling,

00:09:35.520 --> 00:09:39.420
which parts of the
app is being executed,

00:09:39.420 --> 00:09:41.140
it turns out that
we were compiling

00:09:41.140 --> 00:09:43.849
all the code of your APK.

00:09:43.849 --> 00:09:45.640
And that is sort of a
waste of your storage

00:09:45.640 --> 00:09:49.930
if you're going to use
just 5% or 10% of the code.

00:09:53.140 --> 00:09:56.970
But I think a JIT
has some unknown.

00:09:56.970 --> 00:10:00.160
[? That ?] clearly keeping
on compiling all the time,

00:10:00.160 --> 00:10:03.475
like you started your app
with JIT, you kill the app,

00:10:03.475 --> 00:10:05.500
you start it up again, your JIT.

00:10:05.500 --> 00:10:07.940
It has some implications
on your battery,

00:10:07.940 --> 00:10:11.850
that AOT compile
code didn't have.

00:10:11.850 --> 00:10:14.710
And having a
compilation code cache

00:10:14.710 --> 00:10:17.350
could be wasteful if
not managed properly.

00:10:21.160 --> 00:10:25.890
So what we did in N is
introduce a hybrid Just-In-Time

00:10:25.890 --> 00:10:28.700
[? half-time ?] compilation
system that combines

00:10:28.700 --> 00:10:33.100
the benefits of both worlds.

00:10:33.100 --> 00:10:36.970
The idea is that, applications
start running with a JIT,

00:10:36.970 --> 00:10:40.560
and when the phone is idle
and charging ART will do

00:10:40.560 --> 00:10:42.765
profile-guided optimizations.

00:10:42.765 --> 00:10:45.710
And Ahead-Of-Time
heavily optimized

00:10:45.710 --> 00:10:49.900
the parts of the application
that JIT has executed already.

00:10:53.250 --> 00:10:56.600
So later in this talk, my
colleague Calin will go

00:10:56.600 --> 00:11:00.680
into more detail about how
this hybrid AOT JIT profile

00:11:00.680 --> 00:11:04.370
[? GAD ?] [? and ?]
compilation works.

00:11:04.370 --> 00:11:09.290
What I want to focus now
is, how is the [? unknown ?]

00:11:09.290 --> 00:11:10.317
with the JIT?

00:11:13.596 --> 00:11:14.970
Let's go back to
the five metrics

00:11:14.970 --> 00:11:18.340
I mentioned earlier in the talk.

00:11:18.340 --> 00:11:20.800
For runtime
performance the JIT is

00:11:20.800 --> 00:11:24.040
based on the same AOT
compiler that brings

00:11:24.040 --> 00:11:26.600
the same high performance.

00:11:26.600 --> 00:11:29.500
So we're covered.

00:11:29.500 --> 00:11:31.350
But the other metrics
are actually down

00:11:31.350 --> 00:11:32.838
to how we tune the JIT.

00:11:36.750 --> 00:11:38.660
A couple of things
we made when starting

00:11:38.660 --> 00:11:43.840
this project, a couple of
design decisions we made,

00:11:43.840 --> 00:11:47.080
were based on those metrics.

00:11:47.080 --> 00:11:51.320
Like, we implemented a much
faster interpreter in N

00:11:51.320 --> 00:11:53.210
compared to the
one in Marshmallow.

00:11:53.210 --> 00:11:56.740
It runs, like, up to 3x
faster than the [? App ?]

00:11:56.740 --> 00:11:59.230
[? startup ?] in Marshmallow.

00:11:59.230 --> 00:12:01.580
It's very important to
have a fast interpreter

00:12:01.580 --> 00:12:03.050
because when you
start your app you

00:12:03.050 --> 00:12:04.591
don't have any
Ahead-Of-Time compiled

00:12:04.591 --> 00:12:11.040
code its [? gentura ?] is going
to run, and later on the JIT.

00:12:11.040 --> 00:12:13.520
Second, [? we do ?]
a JIT compilation

00:12:13.520 --> 00:12:18.320
on a separate thread and not
on the application threads,

00:12:18.320 --> 00:12:20.650
some compilation
can take very long,

00:12:20.650 --> 00:12:23.630
and you don't want to
block the UI thread just

00:12:23.630 --> 00:12:24.980
for doing compilation.

00:12:27.850 --> 00:12:31.840
For saving power
what we do is mostly

00:12:31.840 --> 00:12:34.515
focusing on the hottest
methods of an application.

00:12:37.500 --> 00:12:39.840
And finally, for
memory footprint,

00:12:39.840 --> 00:12:42.250
we implemented a garbage
collection technique

00:12:42.250 --> 00:12:44.490
that ensures that only the
methods that in the end

00:12:44.490 --> 00:12:47.050
matter, over time, are kept.

00:12:47.050 --> 00:12:48.940
So if a method is being
compiled and then not

00:12:48.940 --> 00:12:54.390
being used anymore, we'll
remove it from memory.

00:12:54.390 --> 00:12:55.910
All right.

00:12:55.910 --> 00:12:59.740
So let's focus on
application startup.

00:12:59.740 --> 00:13:03.760
Now I'm going to walk you
through some Sys Trace that

00:13:03.760 --> 00:13:06.210
will explain some of the
implementation decisions

00:13:06.210 --> 00:13:07.860
we're made.

00:13:07.860 --> 00:13:10.020
If you don't know
what Sys Trace is,

00:13:10.020 --> 00:13:13.180
it's a great tool for both
app developers and platform

00:13:13.180 --> 00:13:19.360
developers to analyze what is
happening on Android system.

00:13:19.360 --> 00:13:21.280
So bear with me.

00:13:21.280 --> 00:13:23.280
There's a lot of
information on that slide,

00:13:23.280 --> 00:13:26.710
but we'll focus on the
things that matter for us.

00:13:26.710 --> 00:13:28.780
So here's how Sys Trace
looks after launching

00:13:28.780 --> 00:13:33.210
Gmail on a device that had
Gmail Ahead-Of-Time compiled.

00:13:36.810 --> 00:13:39.230
So application
startup for a user

00:13:39.230 --> 00:13:43.704
is actually when the first
frame is being rendered.

00:13:43.704 --> 00:13:45.870
And this trace here is
[? somewhat, ?] more or less,

00:13:45.870 --> 00:13:48.580
what you would get on
Marshmallow and Lollipop,

00:13:48.580 --> 00:13:50.700
is that for starting
Gmail, it would

00:13:50.700 --> 00:13:53.400
take half a second for the
first frame to be drawn.

00:13:56.610 --> 00:13:59.260
So our first implementation
or alpha implementation

00:13:59.260 --> 00:14:04.130
of the JIT, we did
the same measurements.

00:14:04.130 --> 00:14:07.050
And results were not great.

00:14:07.050 --> 00:14:10.730
You can see now the startup
is around one second,

00:14:10.730 --> 00:14:12.590
so we increased the
startup around 2x.

00:14:16.550 --> 00:14:22.890
And you can see that the
JIT thread here executing

00:14:22.890 --> 00:14:27.600
is initially idle, and
then becomes very busy.

00:14:27.600 --> 00:14:30.400
So what's happening?

00:14:30.400 --> 00:14:33.560
If you take a closer look at
what the application is doing,

00:14:33.560 --> 00:14:38.380
there's around 200 milliseconds
for doing just the APK

00:14:38.380 --> 00:14:41.220
extraction.

00:14:41.220 --> 00:14:44.930
And doing APK extraction
is blocking the operation,

00:14:44.930 --> 00:14:47.152
so you need to do it
before executing any code.

00:14:50.400 --> 00:14:54.630
Similarly, there's lots of
things happening after the APK

00:14:54.630 --> 00:14:58.323
extraction that don't have to do
with executing the application.

00:14:58.323 --> 00:15:00.680
And that's verification.

00:15:00.680 --> 00:15:02.950
ART needs to verify
the Dex code in order

00:15:02.950 --> 00:15:04.512
to run it and optimize it.

00:15:07.190 --> 00:15:09.160
So we fixed this problem.

00:15:09.160 --> 00:15:13.070
We decided to move extraction
and verification out

00:15:13.070 --> 00:15:15.900
of every application
startup and move it

00:15:15.900 --> 00:15:19.767
back to when the application
is actually being installed.

00:15:19.767 --> 00:15:21.350
So I just made the
application startup

00:15:21.350 --> 00:15:24.550
two times faster than our
initial JIT implementation,

00:15:24.550 --> 00:15:26.812
and quite on par
with compile code.

00:15:30.970 --> 00:15:33.236
So I've just talked about
application startup.

00:15:33.236 --> 00:15:33.860
How about jank?

00:15:36.940 --> 00:15:40.640
For jank we looked
at the frame rate

00:15:40.640 --> 00:15:46.420
of scrolling within the
Google Photos application.

00:15:46.420 --> 00:15:48.570
And Sys Trace gives
you this nice ist

00:15:48.570 --> 00:15:51.410
of frames that are being drawn.

00:15:51.410 --> 00:15:56.360
A green frame is when the UI
managed to render it in time.

00:15:56.360 --> 00:15:59.400
Orange and red is where
you're dropping the frame

00:15:59.400 --> 00:16:00.660
and it hasn't been rendered.

00:16:03.240 --> 00:16:06.030
Now jank can be attributed
to many factors.

00:16:06.030 --> 00:16:07.890
And ART does its
best at executing

00:16:07.890 --> 00:16:10.860
the code of the application.

00:16:10.860 --> 00:16:14.040
But if the application does
too much on the thread,

00:16:14.040 --> 00:16:17.350
obviously we're going
to miss a frame.

00:16:17.350 --> 00:16:21.850
So we have-- for that specific
experiment, we have around 4%

00:16:21.850 --> 00:16:23.470
of frames that
are being dropped.

00:16:26.400 --> 00:16:29.650
During our first [? bring ?]
[? up ?] we made the same

00:16:29.650 --> 00:16:31.870
experiment.

00:16:31.870 --> 00:16:36.940
So application wasn't compiled,
it we'd just run it with a JIT.

00:16:36.940 --> 00:16:38.890
And the results weren't great.

00:16:38.890 --> 00:16:41.330
We were dropping
around 20% of frames.

00:16:44.692 --> 00:16:46.900
If you take a closer look
at what the JIT is actually

00:16:46.900 --> 00:16:52.047
doing here, you can see those
long compilation activities

00:16:52.047 --> 00:16:53.630
where the compiler
is actually waiting

00:16:53.630 --> 00:16:58.520
for methods or for requests
to compile methods.

00:16:58.520 --> 00:17:00.760
Those methods haven't
reached the hotness threshold

00:17:00.760 --> 00:17:03.286
that we've set.

00:17:03.286 --> 00:17:04.660
And the hotness
threshold is high

00:17:04.660 --> 00:17:06.380
because we want to
save on battery.

00:17:06.380 --> 00:17:09.017
But that doesn't
save on saving jank.

00:17:11.980 --> 00:17:13.819
The UI thread--
you want the code

00:17:13.819 --> 00:17:18.124
that that thread is executing
to be hot as soon as possible.

00:17:20.849 --> 00:17:25.670
So the solution was to increase
the weight of UI thread

00:17:25.670 --> 00:17:27.710
requests for compilation.

00:17:27.710 --> 00:17:33.750
So the methods it runs would
be compiled almost instantly.

00:17:33.750 --> 00:17:35.510
So if you see
[? it's ?] [? trace ?],

00:17:35.510 --> 00:17:38.880
there's no more long pauses
of compilations and we only

00:17:38.880 --> 00:17:43.536
dropped around 4% of frames,
which was the OT level we had

00:17:43.536 --> 00:17:44.035
initially.

00:17:47.920 --> 00:17:48.420
All right.

00:17:48.420 --> 00:17:49.156
Battery usage.

00:17:52.780 --> 00:17:56.340
We've measured the power
usage of starting Gmail,

00:17:56.340 --> 00:18:01.912
pausing for 30 seconds, and
then scroll around the emails.

00:18:01.912 --> 00:18:03.370
And we can see here
that the JIT is

00:18:03.370 --> 00:18:06.260
paying a high cost at
application startup

00:18:06.260 --> 00:18:09.710
compared to Ahead-Of-Time
compilation.

00:18:09.710 --> 00:18:12.890
But once startup is
done, the scrolling

00:18:12.890 --> 00:18:18.360
actually has no difference
between AOT and JIT.

00:18:18.360 --> 00:18:22.570
The reason for this difference
is that, as they start up,

00:18:22.570 --> 00:18:26.810
the JIT is very busy
compiling a lot of methods.

00:18:26.810 --> 00:18:30.130
And Gmail seems to be very
aggressive in executing code

00:18:30.130 --> 00:18:32.530
at Startup, which
is not necessarily

00:18:32.530 --> 00:18:35.630
the behavior of all apps.

00:18:35.630 --> 00:18:40.910
So we've done the same
experiment with the other apps.

00:18:40.910 --> 00:18:44.090
We've looked at Chrome,
Camera, and Photos.

00:18:44.090 --> 00:18:47.370
Chrome and Camera are
mostly based on native code.

00:18:47.370 --> 00:18:49.040
So here JIT doesn't
really-- is not

00:18:49.040 --> 00:18:53.160
really useful for all
the things we mentioned.

00:18:53.160 --> 00:18:56.080
And the power usage
is very similar

00:18:56.080 --> 00:19:00.820
whether you're in AOT
setup or at JIT setup.

00:19:00.820 --> 00:19:04.180
Photos, on the other
hand, does have Java code,

00:19:04.180 --> 00:19:06.890
but doesn't have the
behavior that Gmail had.

00:19:06.890 --> 00:19:08.980
And you can see that the
difference is, again,

00:19:08.980 --> 00:19:09.480
very little.

00:19:13.230 --> 00:19:15.100
That was battery.

00:19:15.100 --> 00:19:17.640
Let's discuss about the final
metric, memory footprint.

00:19:20.220 --> 00:19:23.180
So we looked at the overall
usage of our beta testers

00:19:23.180 --> 00:19:27.370
within Google, and we
were quite happy to find

00:19:27.370 --> 00:19:31.830
that the memory use of the
JIT is fairly reasonable.

00:19:31.830 --> 00:19:35.320
The maximum we've seen on a
heavily-loaded system that

00:19:35.320 --> 00:19:37.590
had lots of application
being executed

00:19:37.590 --> 00:19:43.150
was around 30
megabyte, system-wide.

00:19:43.150 --> 00:19:45.090
But on average
what we've seen is

00:19:45.090 --> 00:19:46.450
that, in general, 10 megabyte.

00:19:49.050 --> 00:19:52.500
For individual applications,
big Java applications

00:19:52.500 --> 00:19:55.710
will take some memory,
like, four megabyte.

00:19:55.710 --> 00:19:58.330
But on average,
most applications

00:19:58.330 --> 00:20:00.790
have a reasonable
Java code size,

00:20:00.790 --> 00:20:03.876
and the code cache is fairly
small, around 300 kilobytes.

00:20:07.890 --> 00:20:15.270
So to wrap up on how JIT
affects these metrics when it's

00:20:15.270 --> 00:20:19.500
being executed, we've seen
that performance in jank

00:20:19.500 --> 00:20:23.310
are on par with the quality
level we had with ART

00:20:23.310 --> 00:20:26.160
Ahead-Of-Time compilation.

00:20:26.160 --> 00:20:28.400
And I have shown
you that it does

00:20:28.400 --> 00:20:32.860
have a relative impact on
application startup, battery,

00:20:32.860 --> 00:20:35.690
and memory footprint.

00:20:35.690 --> 00:20:38.946
So now I'll be handing it
over to my colleague Calin,

00:20:38.946 --> 00:20:43.370
and he's going to explain to you
how we're recovering from those

00:20:43.370 --> 00:20:46.110
small regressions
compared to AOT by doing

00:20:46.110 --> 00:20:48.192
[? Ahead-Of-Time ?]
profile-guided compilation.

00:20:50.987 --> 00:20:54.410
[AUDIENCE APPLAUSE]

00:20:55.984 --> 00:20:57.400
CALIN JURAVLE:
Thank you, Nicolas.

00:20:57.400 --> 00:20:58.170
Hello, everyone.

00:20:58.170 --> 00:21:01.400
I'm Calin and I'm
here today to give you

00:21:01.400 --> 00:21:05.390
more details on
profile-guided compilation.

00:21:05.390 --> 00:21:07.100
And this is a new
compilation strategy

00:21:07.100 --> 00:21:09.550
that we introduced in N.
And, as my colleague Nicolas

00:21:09.550 --> 00:21:11.690
mentioned, is a
combination between JIT

00:21:11.690 --> 00:21:14.620
and Ahead-Of-Time compilation.

00:21:14.620 --> 00:21:16.860
And it's mainly based
on the observation

00:21:16.860 --> 00:21:19.200
that the percentage of
the application's code,

00:21:19.200 --> 00:21:21.150
which is actually
worth optimizing,

00:21:21.150 --> 00:21:24.320
is very small in practice.

00:21:24.320 --> 00:21:26.310
And focusing on the
most important part

00:21:26.310 --> 00:21:28.330
of the application
drives a lot of benefits

00:21:28.330 --> 00:21:30.630
for the overall system
performance, and not

00:21:30.630 --> 00:21:34.470
only limited to
recovering the slightly

00:21:34.470 --> 00:21:37.690
regression that we have to
[INAUDIBLE] with this system.

00:21:37.690 --> 00:21:41.550
So the goal here was to have
a full five-star system.

00:21:41.550 --> 00:21:45.560
And this is what profile-guided
compilations help us achieve.

00:21:45.560 --> 00:21:48.280
So let's take a look
a bit on the idea.

00:21:48.280 --> 00:21:51.570
In a nutshell, we want to
combine execution profiles

00:21:51.570 --> 00:21:53.410
before Ahead-Of-Time
compilation,

00:21:53.410 --> 00:21:56.310
and that will lead to a
profile-guided compilation.

00:21:56.310 --> 00:22:00.220
What it means is that,
during application execution

00:22:00.220 --> 00:22:04.310
we'll record profile information
about how the app is executed,

00:22:04.310 --> 00:22:05.790
and you will use
that information

00:22:05.790 --> 00:22:08.440
to drive off-line optimizations,
but at a time when

00:22:08.440 --> 00:22:10.860
the device is charging and
idling so that it doesn't take

00:22:10.860 --> 00:22:12.296
resources out of our users.

00:22:18.380 --> 00:22:21.830
So let's take a look into
more details how this works,

00:22:21.830 --> 00:22:24.500
how it affects the lifecycle
of the application,

00:22:24.500 --> 00:22:26.700
and how it fits together
in the JIT system

00:22:26.700 --> 00:22:29.620
that my colleague talked about.

00:22:29.620 --> 00:22:32.320
The first time the
application is executed,

00:22:32.320 --> 00:22:34.560
the runtime will interpret it.

00:22:34.560 --> 00:22:38.520
The JIT system will kick in
and optimize the hot methods,

00:22:38.520 --> 00:22:41.170
and eventually this
cycle will repeat.

00:22:41.170 --> 00:22:43.280
[? Imperil ?] with
the interpretation

00:22:43.280 --> 00:22:45.110
and with the JIT
compilation will

00:22:45.110 --> 00:22:47.390
record the profile information.

00:22:47.390 --> 00:22:50.170
And this profile information
gets dumped to disk

00:22:50.170 --> 00:22:52.390
at regular interval of times.

00:22:52.390 --> 00:22:55.310
The next time that you execute
the app, the same process

00:22:55.310 --> 00:22:56.330
starts again.

00:22:56.330 --> 00:22:58.030
And the profile
files will eventually

00:22:58.030 --> 00:23:00.640
be expanded with
new use cases based

00:23:00.640 --> 00:23:03.800
on how the user used the app.

00:23:03.800 --> 00:23:07.670
At a later point, when the
device is not in use anymore,

00:23:07.670 --> 00:23:09.940
it's idling and charging,
and it's a state

00:23:09.940 --> 00:23:11.570
that we call "maintenance mode."

00:23:11.570 --> 00:23:13.040
We kick in the service.

00:23:13.040 --> 00:23:15.000
And what the service
does, it takes

00:23:15.000 --> 00:23:18.110
a look at the application,
it looks at the profiles,

00:23:18.110 --> 00:23:21.890
and it tries to optimize the
application based on its use.

00:23:21.890 --> 00:23:25.860
The output of the service is
actually a compiled binary,

00:23:25.860 --> 00:23:28.730
and this compiled binary will
replace the initial application

00:23:28.730 --> 00:23:30.790
in the system.

00:23:30.790 --> 00:23:32.910
So now the next
time the application

00:23:32.910 --> 00:23:36.090
is launched after this
service was executed,

00:23:36.090 --> 00:23:38.580
the application will
contain different codes,

00:23:38.580 --> 00:23:40.600
different states
of the same code.

00:23:40.600 --> 00:23:43.464
So you may have code
which is interpreted

00:23:43.464 --> 00:23:45.130
and eventually be
JIT-ed, and it'll also

00:23:45.130 --> 00:23:49.050
have code which
Ahead-Of-Time compiled.

00:23:49.050 --> 00:23:51.910
If the user, for example, uses
a new part of the application

00:23:51.910 --> 00:23:54.090
that hasn't been
explored before,

00:23:54.090 --> 00:23:55.930
that part will be
interpreted in JIT

00:23:55.930 --> 00:23:57.940
and it will generate a
new profile information.

00:23:57.940 --> 00:24:00.710
And so the cycle begins again.

00:24:00.710 --> 00:24:03.070
So what's important
here is, we'll

00:24:03.070 --> 00:24:05.410
improve the
application performance

00:24:05.410 --> 00:24:08.130
as the user executes
it with new use cases.

00:24:08.130 --> 00:24:10.040
And it will keep
recompiling it until we

00:24:10.040 --> 00:24:12.325
discover all possible cases.

00:24:15.670 --> 00:24:18.670
So let's focus a bit on
the profile collection

00:24:18.670 --> 00:24:21.080
and how that impacts
the application

00:24:21.080 --> 00:24:22.415
performance and other factors.

00:24:25.330 --> 00:24:27.080
As I mentioned,
we do collect them

00:24:27.080 --> 00:24:29.850
in parallel with the
application execution,

00:24:29.850 --> 00:24:31.630
and what it focused
on is to make sure

00:24:31.630 --> 00:24:36.040
that it has a minimal
impact on the performance.

00:24:36.040 --> 00:24:39.540
And one factor that we put
a lot of attention into

00:24:39.540 --> 00:24:41.970
is to have an efficient
caching and [? IO ?]

00:24:41.970 --> 00:24:47.140
foretelling so that we minimize
the write operation to disk.

00:24:47.140 --> 00:24:49.827
We also have a very
small file footprint,

00:24:49.827 --> 00:24:51.660
and the amount of data
that we write to disk

00:24:51.660 --> 00:24:55.280
is actually very, very small.

00:24:55.280 --> 00:24:58.550
Another point, which
I mentioned before,

00:24:58.550 --> 00:25:03.550
is that we keep expanding these
profiles as the app executes.

00:25:03.550 --> 00:25:06.230
And, obviously, it depends
on the application,

00:25:06.230 --> 00:25:08.310
it depends on the use case.

00:25:08.310 --> 00:25:10.800
Our test shows that the
largest part of the data

00:25:10.800 --> 00:25:13.740
is actually captured
during the first run.

00:25:13.740 --> 00:25:16.190
Subsequent runs add to
the profile information.

00:25:16.190 --> 00:25:19.370
It obviously depends how
the user used the app,

00:25:19.370 --> 00:25:21.680
but the largest chunk
of the information

00:25:21.680 --> 00:25:23.870
is actually captured
during the first execution

00:25:23.870 --> 00:25:29.840
and that gives us
important data to work on.

00:25:29.840 --> 00:25:31.820
A final point which is
worth mentioning here

00:25:31.820 --> 00:25:33.950
is that, all the application
and all the users

00:25:33.950 --> 00:25:35.081
get their own profiles.

00:25:38.840 --> 00:25:41.190
And with that in mind, let's
take a look on what exactly

00:25:41.190 --> 00:25:44.580
we record in this
profile information.

00:25:44.580 --> 00:25:47.240
The first things
are the hot methods.

00:25:47.240 --> 00:25:49.220
And what constitutes
a hot method?

00:25:49.220 --> 00:25:52.120
It's a metric which you have
internal to your runtime,

00:25:52.120 --> 00:25:53.730
and the factors that
contribute to it

00:25:53.730 --> 00:25:56.330
are, for example,
number of invocations,

00:25:56.330 --> 00:25:59.167
whether or not that method
is executed on the UI thread

00:25:59.167 --> 00:26:00.750
so that you can speed
up requests that

00:26:00.750 --> 00:26:03.120
will impact the users directly.

00:26:03.120 --> 00:26:05.770
We'll use this information to
drive off-line optimizations

00:26:05.770 --> 00:26:10.650
and to dedicate more time
to optimize those methods.

00:26:10.650 --> 00:26:14.770
The second data that we
record are the classes

00:26:14.770 --> 00:26:17.180
which impact the startup times.

00:26:17.180 --> 00:26:18.990
How do we know they do so?

00:26:18.990 --> 00:26:21.240
It means that they are loaded
in the first few seconds

00:26:21.240 --> 00:26:23.210
after the user launched the tab.

00:26:23.210 --> 00:26:25.840
And my colleague Mathieu
will go into more details

00:26:25.840 --> 00:26:30.800
on how we use that to improve
startup times even more.

00:26:30.800 --> 00:26:33.000
A final piece of
information that we record

00:26:33.000 --> 00:26:35.480
is whether or not the
application code is loaded

00:26:35.480 --> 00:26:38.470
in some other apps, and
that's very important

00:26:38.470 --> 00:26:40.820
to know because it means
that the application behaves

00:26:40.820 --> 00:26:43.310
more like a shared library.

00:26:43.310 --> 00:26:46.060
And when it does so we'll
use a different compilation

00:26:46.060 --> 00:26:47.710
strategy to optimize it.

00:26:50.500 --> 00:26:53.980
So let's focus now
on the compilation

00:26:53.980 --> 00:26:56.810
daemon, on the service that
actually does the compilation,

00:26:56.810 --> 00:27:00.730
and let's take a look on
what [? decision ?] makes.

00:27:00.730 --> 00:27:03.070
This is a service which
is started at boot time

00:27:03.070 --> 00:27:07.210
by the system and is
scheduled for daily run.

00:27:07.210 --> 00:27:10.250
Its main job is to iterate
through all the APKs installed

00:27:10.250 --> 00:27:13.040
in the system and figure
out whether or not

00:27:13.040 --> 00:27:14.450
we need to compile them.

00:27:14.450 --> 00:27:16.940
And if we do need to compile
them, what sort of strategy

00:27:16.940 --> 00:27:19.620
we should use.

00:27:19.620 --> 00:27:21.740
The service wakes up
when the device becomes

00:27:21.740 --> 00:27:24.030
idle in charging, and
the main reason for that

00:27:24.030 --> 00:27:28.880
is that we don't want
to use user time when

00:27:28.880 --> 00:27:30.300
the device is
active, and we don't

00:27:30.300 --> 00:27:31.970
want to waste battery time.

00:27:31.970 --> 00:27:36.810
So we delayed this until the
device is not in use anymore.

00:27:36.810 --> 00:27:38.740
When the daemon wakes
up what it does,

00:27:38.740 --> 00:27:40.200
it iterates with
the applications.

00:27:40.200 --> 00:27:42.910
And the first questions
it asks is whether or not

00:27:42.910 --> 00:27:45.350
the application code has
been used by some other apps

00:27:45.350 --> 00:27:47.016
that they thought
that I was telling you

00:27:47.016 --> 00:27:48.897
that we record in the profile.

00:27:48.897 --> 00:27:51.230
If that's the case, then they
perform a full compilation

00:27:51.230 --> 00:27:55.850
to make sure that all the users
benefit of the optimized code.

00:27:55.850 --> 00:27:59.000
If it's not-- and this is the
case probably for the largest

00:27:59.000 --> 00:28:00.390
percentage of the app.

00:28:00.390 --> 00:28:03.770
It's a regular app, you don't
get used by some other apps.

00:28:03.770 --> 00:28:06.530
We go into and perform
a much deeper analysis

00:28:06.530 --> 00:28:08.980
on the profile information.

00:28:08.980 --> 00:28:12.140
If we have enough new data, if
we collected enough information

00:28:12.140 --> 00:28:15.390
about the application, then
we will profile a guide

00:28:15.390 --> 00:28:16.940
compile that application.

00:28:16.940 --> 00:28:18.350
We'll take a look
at the profiles

00:28:18.350 --> 00:28:20.260
and then optimize
only the methods that

00:28:20.260 --> 00:28:24.070
were executed so that we focus
on what actually the user used

00:28:24.070 --> 00:28:26.172
from that application.

00:28:26.172 --> 00:28:28.630
If, by any chance, we don't
have enough information-- let's

00:28:28.630 --> 00:28:31.540
say we only know--
we only have data

00:28:31.540 --> 00:28:33.205
about one single
method from the top,

00:28:33.205 --> 00:28:35.080
then we'll just keep it
because probably it's

00:28:35.080 --> 00:28:37.540
not worth optimizing.

00:28:37.540 --> 00:28:40.050
And an important
thing here is that we

00:28:40.050 --> 00:28:42.750
do perform the profile
analysis every time

00:28:42.750 --> 00:28:43.860
that we run the daemon.

00:28:43.860 --> 00:28:46.140
And that what that means
is that we might end up

00:28:46.140 --> 00:28:49.600
recompiling the app again and
again until we no longer have

00:28:49.600 --> 00:28:52.171
any new information about it.

00:28:52.171 --> 00:28:53.670
And here you can
see that actually I

00:28:53.670 --> 00:28:55.850
was talking about
different use cases

00:28:55.850 --> 00:28:58.750
and how we apply different
compilation strategies.

00:28:58.750 --> 00:29:01.140
Shared libraries get
a full compilation,

00:29:01.140 --> 00:29:05.290
whereas regular apps prefer
a profile-guided compilation.

00:29:05.290 --> 00:29:08.100
In N we generalized on that.

00:29:08.100 --> 00:29:11.650
And different use cases from
the lifecycle of the application

00:29:11.650 --> 00:29:14.360
have different
completion strategies.

00:29:14.360 --> 00:29:18.030
For example, at install time we
don't have profile information,

00:29:18.030 --> 00:29:19.540
yet we still want
the application

00:29:19.540 --> 00:29:21.580
to start as fast as possible.

00:29:21.580 --> 00:29:23.400
And as my colleague
Nicolas mentioned,

00:29:23.400 --> 00:29:25.760
we have a strategy where
we extract and verify

00:29:25.760 --> 00:29:28.950
that app with
minimal running time,

00:29:28.950 --> 00:29:31.590
and which will ensure that
the application starts fast.

00:29:31.590 --> 00:29:34.520
When you update the app
we have the same story.

00:29:34.520 --> 00:29:36.520
We no longer have a
profile because what

00:29:36.520 --> 00:29:38.520
it recorded before was invalid.

00:29:38.520 --> 00:29:42.160
So we repeat this
[? education ?] procedure.

00:29:42.160 --> 00:29:44.320
When the compilation
daemon kicks in,

00:29:44.320 --> 00:29:48.460
we'll do a profile-guided
compilation where possible.

00:29:48.460 --> 00:29:50.140
And for system and
shared libraries

00:29:50.140 --> 00:29:51.700
we're going to do
a full compilation

00:29:51.700 --> 00:29:54.556
so that we make sure that
all their users are properly

00:29:54.556 --> 00:29:55.055
optimized.

00:29:57.940 --> 00:30:00.480
With that in mind, let's
take a more closer look

00:30:00.480 --> 00:30:04.500
on what benefits are when we
do profile-guided compilation.

00:30:04.500 --> 00:30:07.260
And all the benefits
share the same root cause.

00:30:07.260 --> 00:30:10.620
We only optimize
what is being used.

00:30:10.620 --> 00:30:12.580
And what it means?

00:30:12.580 --> 00:30:16.090
When you first start the app
after the compilation happened,

00:30:16.090 --> 00:30:18.540
previous hot code is
already optimized.

00:30:18.540 --> 00:30:21.240
We no longer have to wait
for the JIT, for the methods

00:30:21.240 --> 00:30:24.340
to become hot so that
the JIT can compile them.

00:30:24.340 --> 00:30:26.770
So the applications
will start faster.

00:30:26.770 --> 00:30:28.770
We have also less
work for the JIT.

00:30:28.770 --> 00:30:32.090
And that means we use less CPU
and we increase the battery

00:30:32.090 --> 00:30:33.930
life overall.

00:30:33.930 --> 00:30:35.970
And because we are
much more selective

00:30:35.970 --> 00:30:40.040
with what we optimize, we can
dedicate and spend more time

00:30:40.040 --> 00:30:43.710
there and apply
more optimizations.

00:30:43.710 --> 00:30:48.390
Besides that we get a smaller
size for the compiled binary.

00:30:48.390 --> 00:30:50.300
And that's a very
important thing

00:30:50.300 --> 00:30:53.110
because what we do it as
binary [? map ?] into memory.

00:30:53.110 --> 00:30:56.790
Smaller size translates to
reduced memory footprint,

00:30:56.790 --> 00:30:58.380
and the important
difference here

00:30:58.380 --> 00:31:01.970
is that, for example,
this memory that we

00:31:01.970 --> 00:31:06.200
map into-- this binary
that we map into memory

00:31:06.200 --> 00:31:08.630
will be a clean memory
compared to dirty memory

00:31:08.630 --> 00:31:10.550
that JIT will generate.

00:31:10.550 --> 00:31:14.194
We also use far less disk
space because the binaries

00:31:14.194 --> 00:31:15.610
are much smaller
now and they free

00:31:15.610 --> 00:31:18.310
a lot of space for our users.

00:31:18.310 --> 00:31:20.930
How much space?

00:31:20.930 --> 00:31:23.960
Let's take a look
at the numbers.

00:31:23.960 --> 00:31:26.410
In this chart we compare
different applications,

00:31:26.410 --> 00:31:29.630
which is Google Plus,
Play Music, and Hangouts.

00:31:29.630 --> 00:31:34.670
And we tracked how the generated
binary for the compiled code

00:31:34.670 --> 00:31:36.640
performance across
Marshmallow, which

00:31:36.640 --> 00:31:40.180
is the blue line, N preview
during the first boot

00:31:40.180 --> 00:31:43.020
which is equivalent of a fresh
install of the application,

00:31:43.020 --> 00:31:44.980
which is the orange line.

00:31:44.980 --> 00:31:46.800
And the green line
is how much it

00:31:46.800 --> 00:31:50.110
takes for the
profile-guided compilation.

00:31:50.110 --> 00:31:54.800
As you can see, the
reduction is more than 50%.

00:31:54.800 --> 00:32:00.430
And obviously, the green
line will go up over time.

00:32:00.430 --> 00:32:03.360
And our tests show that it
actually stays around 50%

00:32:03.360 --> 00:32:05.460
most of the times.

00:32:05.460 --> 00:32:08.380
And you may wonder, how come
we get so great reduction

00:32:08.380 --> 00:32:09.940
in terms of size?

00:32:09.940 --> 00:32:12.760
Well, when we
analyze the profiles,

00:32:12.760 --> 00:32:17.210
we've realized that only
around 4% to 5% of the methods

00:32:17.210 --> 00:32:19.170
actually get compiled.

00:32:19.170 --> 00:32:22.030
And as we use the app more,
obviously this percentage will

00:32:22.030 --> 00:32:24.740
go up, will generate more code.

00:32:24.740 --> 00:32:28.380
But, as I said, in
general we stay below 50%,

00:32:28.380 --> 00:32:31.060
or around that area.

00:32:31.060 --> 00:32:33.640
Now a natural question
here is, if I'm only

00:32:33.640 --> 00:32:36.740
compiling 5% of the app, how
come I reduce the space only

00:32:36.740 --> 00:32:37.820
by 50%?

00:32:37.820 --> 00:32:39.962
Why not 95?

00:32:39.962 --> 00:32:42.420
Well, those lines contains also
the application [INAUDIBLE]

00:32:42.420 --> 00:32:44.720
code, that Dex code.

00:32:44.720 --> 00:32:47.590
And that's a line below
which we cannot go,

00:32:47.590 --> 00:32:49.300
because we need
to run something.

00:32:49.300 --> 00:32:53.900
And here is how we compare
to the application size.

00:32:53.900 --> 00:32:55.770
You can see that,
in Marshmallow,

00:32:55.770 --> 00:33:00.680
we generated more than
3x in terms of code size,

00:33:00.680 --> 00:33:02.740
whereas in M we
stayed below 1.5x.

00:33:06.710 --> 00:33:10.100
And these are all cool benefits,
but it is not the only thing,

00:33:10.100 --> 00:33:13.360
actually, that we
use profiles for.

00:33:13.360 --> 00:33:17.420
We also use them to farther
speed up system updates.

00:33:17.420 --> 00:33:18.910
As my colleague
Nicolas mentioned,

00:33:18.910 --> 00:33:22.320
because of JIT, we don't need
to recompile the app again.

00:33:22.320 --> 00:33:25.110
And that basically gets
rid of the long waiting

00:33:25.110 --> 00:33:26.970
time for the optimizing app.

00:33:26.970 --> 00:33:29.130
We still want to do some
processing of the app,

00:33:29.130 --> 00:33:31.490
in particular extraction
and verification,

00:33:31.490 --> 00:33:35.000
to ensure that those apps get
executed as fast as possible

00:33:35.000 --> 00:33:38.220
when they are first launched.

00:33:38.220 --> 00:33:40.650
And in M we actually
know how those apps

00:33:40.650 --> 00:33:42.010
were executed before.

00:33:42.010 --> 00:33:46.320
So we can use profile to
guide the verification.

00:33:46.320 --> 00:33:49.970
And that saves around
40% of the time extra.

00:33:49.970 --> 00:33:53.300
We also added new,
improved usage stats.

00:33:53.300 --> 00:33:55.510
And compared to M,
we can now track

00:33:55.510 --> 00:33:57.790
precisely how the
application was used

00:33:57.790 --> 00:33:59.280
and how it was executed.

00:33:59.280 --> 00:34:03.840
And we only analyze what
actually matters for the users.

00:34:03.840 --> 00:34:04.770
What is that?

00:34:04.770 --> 00:34:07.680
Application [? has ?] a user
interface, and the users

00:34:07.680 --> 00:34:08.949
can interact with them.

00:34:08.949 --> 00:34:10.850
Those are the most
valuable for our users

00:34:10.850 --> 00:34:14.560
and we focus on them
during system updates.

00:34:14.560 --> 00:34:17.719
However, when we take the
update first update to M,

00:34:17.719 --> 00:34:19.620
we don't have access
to all the goodies.

00:34:19.620 --> 00:34:21.330
We don't have
profiles and we don't

00:34:21.330 --> 00:34:26.020
have enough accurate
user stats to realize

00:34:26.020 --> 00:34:27.429
how the app was used.

00:34:27.429 --> 00:34:29.409
And what we do, we do
a full verification

00:34:29.409 --> 00:34:31.090
of most of the apps.

00:34:31.090 --> 00:34:34.880
This is still much, much
faster than we used to do in M.

00:34:34.880 --> 00:34:35.736
And how much faster?

00:34:35.736 --> 00:34:37.110
Let's take a look
at the numbers.

00:34:39.949 --> 00:34:42.139
You can see here
three different lines.

00:34:42.139 --> 00:34:45.550
And these numbers are
obtained on the same device

00:34:45.550 --> 00:34:49.070
which have roughly 100
applications installed.

00:34:49.070 --> 00:34:51.460
And the first line,
which represents

00:34:51.460 --> 00:34:54.940
an OTA-- a system update
from M to M prime,

00:34:54.940 --> 00:34:57.530
where we took a
security update--

00:34:57.530 --> 00:35:02.360
we took around 14 minutes to
process all the applications.

00:35:02.360 --> 00:35:06.710
And there's the time that the
runtime spends processing them.

00:35:06.710 --> 00:35:09.290
When you take the update
to M, we reduce the time

00:35:09.290 --> 00:35:14.190
to about three minutes,
which is up to 5x reduction.

00:35:14.190 --> 00:35:17.790
And there we verify most
of the applications.

00:35:17.790 --> 00:35:20.160
Now, the next step
our security updates

00:35:20.160 --> 00:35:22.040
will kick in for
M. And what we do,

00:35:22.040 --> 00:35:23.540
we already have
profile information,

00:35:23.540 --> 00:35:26.610
we already have
improved usage stats,

00:35:26.610 --> 00:35:29.540
and we can use that to
drive that time even lower.

00:35:29.540 --> 00:35:31.830
And when you take a security
update on this device,

00:35:31.830 --> 00:35:33.470
we take less than a minute.

00:35:33.470 --> 00:35:37.240
And compared to M, that's
more than 12x improvement

00:35:37.240 --> 00:35:40.700
in terms of speed up.

00:35:40.700 --> 00:35:43.790
With that I'd like to
pass it to Mathieu,

00:35:43.790 --> 00:35:48.440
and he will explain how we
use profiles to speed up

00:35:48.440 --> 00:35:51.040
application even more.

00:35:51.040 --> 00:35:54.211
[AUDIENCE APPLAUSE]

00:35:56.362 --> 00:35:58.670
MATHIEU CHARTIER:
Thank you, Calin.

00:35:58.670 --> 00:36:01.475
One new feature in N that
reduces application launch time

00:36:01.475 --> 00:36:04.250
is application images.

00:36:04.250 --> 00:36:06.280
An application image is
a serialized [? seed ?]

00:36:06.280 --> 00:36:08.910
consisting of
pre-initialized classes.

00:36:08.910 --> 00:36:13.307
This image is loaded by the
application during the launch.

00:36:13.307 --> 00:36:14.890
During launch most
applications end up

00:36:14.890 --> 00:36:16.880
loading many classes
for initialization work,

00:36:16.880 --> 00:36:19.650
such as creating views
or inflating layouts.

00:36:19.650 --> 00:36:21.612
Unfortunately, loading
classes is not free

00:36:21.612 --> 00:36:23.820
and can consist of a large
portion of the application

00:36:23.820 --> 00:36:25.260
launch time.

00:36:25.260 --> 00:36:27.640
The way that application
images reduces this cost

00:36:27.640 --> 00:36:30.420
is by effectively shifting work
from application launch time

00:36:30.420 --> 00:36:32.069
to compile time.

00:36:32.069 --> 00:36:34.110
Since the classes inside
of the application image

00:36:34.110 --> 00:36:35.855
are pre-initialized,
this means they're

00:36:35.855 --> 00:36:38.230
able to be accessed right off
the bat by the application.

00:36:41.170 --> 00:36:43.450
Application images are
generated during the background

00:36:43.450 --> 00:36:44.150
compilation phase.

00:36:44.150 --> 00:36:46.483
I think it was Calin who
referred to it as a maintenance

00:36:46.483 --> 00:36:48.780
phase, by the OT compiler.

00:36:48.780 --> 00:36:51.000
Leveraging the JIT profiles,
the application images

00:36:51.000 --> 00:36:52.910
include and serialize
only the set

00:36:52.910 --> 00:36:55.070
of classes that were used
during prior launches

00:36:55.070 --> 00:36:57.582
of the application.

00:36:57.582 --> 00:37:00.040
Using the profile is also key
to having a small application

00:37:00.040 --> 00:37:03.140
image, since it only includes
a small fraction of the classes

00:37:03.140 --> 00:37:05.690
inside the actual application.

00:37:05.690 --> 00:37:07.610
Having a small application
image is important

00:37:07.610 --> 00:37:09.920
because the application
image is resident

00:37:09.920 --> 00:37:12.380
in RAM for the entire
lifetime of the application,

00:37:12.380 --> 00:37:16.860
as well as larger images
take longer to load.

00:37:16.860 --> 00:37:19.110
As you can see here,
application images

00:37:19.110 --> 00:37:20.630
have a very low
storage requirement.

00:37:20.630 --> 00:37:22.952
This is mostly due
to the profile.

00:37:22.952 --> 00:37:24.910
For the four apps here
the storage requirements

00:37:24.910 --> 00:37:27.562
were less than two
megabytes per app.

00:37:27.562 --> 00:37:29.145
As a comparison, I
put the application

00:37:29.145 --> 00:37:31.662
that I compiled the application
code with the profile.

00:37:31.662 --> 00:37:33.120
So this is already
reduced compared

00:37:33.120 --> 00:37:37.110
to what application code
sizes would have been on M.

00:37:37.110 --> 00:37:39.290
The loading process
of application images

00:37:39.290 --> 00:37:43.650
begins with the application
[? ClassLoader ?] creation.

00:37:43.650 --> 00:37:46.850
When the application
ClassLoader is created,

00:37:46.850 --> 00:37:48.600
we load the application
image from storage

00:37:48.600 --> 00:37:51.530
and decompress it into RAM.

00:37:51.530 --> 00:37:52.929
For dynamically
loaded Dex files,

00:37:52.929 --> 00:37:54.470
we also verified
that the ClassLoader

00:37:54.470 --> 00:37:56.460
is a supported type.

00:37:56.460 --> 00:37:59.490
Since there is no dependency
from the application compiled

00:37:59.490 --> 00:38:01.920
code to the image, it means
that we can reject the image.

00:38:01.920 --> 00:38:03.787
So if the ClassLoader
is not supported,

00:38:03.787 --> 00:38:05.745
I simply reject the image
and resume execution.

00:38:08.719 --> 00:38:10.510
Here are some results
for application image

00:38:10.510 --> 00:38:12.530
launch time improvements
for the four apps

00:38:12.530 --> 00:38:14.260
that I just displayed.

00:38:14.260 --> 00:38:16.717
As you can see here,
there's around a 5% to 20%

00:38:16.717 --> 00:38:18.800
improvement compared to
profile-guided compilation

00:38:18.800 --> 00:38:19.299
only.

00:38:23.030 --> 00:38:26.354
And now to the
garbage collector.

00:38:26.354 --> 00:38:28.270
The garbage collector
has not changed too much

00:38:28.270 --> 00:38:29.929
since our last IO presentation.

00:38:29.929 --> 00:38:32.220
As you can see here we are
already in pretty good shape

00:38:32.220 --> 00:38:34.180
back then.

00:38:34.180 --> 00:38:36.040
There is still only
one short GC pause

00:38:36.040 --> 00:38:37.915
and the garbage collector
has high throughput

00:38:37.915 --> 00:38:40.000
since it is generational.

00:38:40.000 --> 00:38:42.000
There have been a few GC
changes, mostly related

00:38:42.000 --> 00:38:44.480
to application images
and Class Unloading,

00:38:44.480 --> 00:38:45.990
however these
changes do not have--

00:38:45.990 --> 00:38:48.770
or, these GC changes do not
have a substantial performance

00:38:48.770 --> 00:38:51.500
impact.

00:38:51.500 --> 00:38:56.160
One thing that has improved
each release is allocation time.

00:38:56.160 --> 00:38:59.700
With L we introduced a new,
custom thread local allocator

00:38:59.700 --> 00:39:03.510
that reduced allocation
time substantially

00:39:03.510 --> 00:39:05.721
by avoiding
synchronization costs.

00:39:05.721 --> 00:39:07.970
In N we removed all of the
[? comparance ?] [? womp ?]

00:39:07.970 --> 00:39:10.789
operations in the
allocation common case.

00:39:10.789 --> 00:39:13.330
Finally, in N, the allocation
common case has a [? written ?]

00:39:13.330 --> 00:39:15.940
and [? hand-optimize ?]
assembly code.

00:39:15.940 --> 00:39:18.380
This is the largest speed
up yet, at over 200%

00:39:18.380 --> 00:39:20.342
compared to M.

00:39:20.342 --> 00:39:21.800
Combined, all of
these improvements

00:39:21.800 --> 00:39:24.312
mean that allocations are now
around 10x faster than KitKat,

00:39:24.312 --> 00:39:25.020
which was Dalvik.

00:39:28.480 --> 00:39:30.370
Class Unloading,
also introduced in N,

00:39:30.370 --> 00:39:33.330
is a way to reduce RAM
for modular applications.

00:39:33.330 --> 00:39:35.830
Basically, what this means is
that classes and class loaders

00:39:35.830 --> 00:39:39.140
can be freed by the GC when
they're no longer used.

00:39:39.140 --> 00:39:41.084
In previous Android
versions, these

00:39:41.084 --> 00:39:43.500
were held live for the entire
lifetime of the application.

00:39:46.400 --> 00:39:48.300
For classloading to
occur, all the classes

00:39:48.300 --> 00:39:49.674
in the ClassLoader
most no longer

00:39:49.674 --> 00:39:52.150
be reachable, as well as
the ClassLoader itself.

00:39:52.150 --> 00:39:55.265
When the GC notices this
it frees them, as well as

00:39:55.265 --> 00:39:57.380
the associated metadata.

00:39:57.380 --> 00:39:59.180
This chart
demonstrates just a bit

00:39:59.180 --> 00:40:01.680
of how the ClassLoader
interacts with other components

00:40:01.680 --> 00:40:02.420
and retains them.

00:40:05.740 --> 00:40:08.569
With that, off to
Nicolas for the wrap-up.

00:40:08.569 --> 00:40:09.360
[AUDIENCE APPLAUSE]

00:40:09.360 --> 00:40:10.901
NICOLAS GEOFFRAY:
Thank you, Matthew.

00:40:16.000 --> 00:40:16.500
All right.

00:40:16.500 --> 00:40:19.170
So to wrap up, we've
shown you all the features

00:40:19.170 --> 00:40:24.210
we've worked on for N, mainly
a faster compiler, up to 5x

00:40:24.210 --> 00:40:26.510
compared to previous releases.

00:40:26.510 --> 00:40:28.830
A faster interpreter,
up to 3x compared

00:40:28.830 --> 00:40:31.510
to Marshmallow, a
new JIT compiler

00:40:31.510 --> 00:40:33.680
and profile-guided compilation.

00:40:33.680 --> 00:40:36.200
They have just removed that
optimizing apps dialogue

00:40:36.200 --> 00:40:39.160
and provides faster installs.

00:40:39.160 --> 00:40:44.949
Applications image for fast
startup, and fast applications.

00:40:44.949 --> 00:40:47.240
You can actually, if you're
interested in all that sort

00:40:47.240 --> 00:40:54.270
of low-level stuff, follow
it on the AOSP website,

00:40:54.270 --> 00:40:57.470
where we actually
do all development.

00:40:57.470 --> 00:41:00.520
With that, we'll
take your questions.

00:41:00.520 --> 00:41:01.200
Thank you.

00:41:01.200 --> 00:41:03.400
[AUDIENCE APPLAUSE]

00:41:03.900 --> 00:41:07.250
[MUSIC PLAYING]

