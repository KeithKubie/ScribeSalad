WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.948
[MUSIC PLAYING]

00:00:08.290 --> 00:00:10.550
FERNANDA VIEGAS:
Thanks for coming.

00:00:10.550 --> 00:00:12.910
My name is Fernanda Viegas.

00:00:12.910 --> 00:00:15.280
I'm part of Google Brain.

00:00:15.280 --> 00:00:19.960
I'm also a leader in the
PAIR initiative, which stands

00:00:19.960 --> 00:00:23.500
for People + AI Research.

00:00:23.500 --> 00:00:26.380
And I'm very excited
to be joined today

00:00:26.380 --> 00:00:28.390
by this wonderful panel.

00:00:28.390 --> 00:00:33.250
So here to my side is Rajen
Sheth, he's director of product

00:00:33.250 --> 00:00:38.830
management at Google
Cloud AI, Daphne Luong,

00:00:38.830 --> 00:00:42.340
engineering director at
Google, and John Platt,

00:00:42.340 --> 00:00:45.860
director of applied
science in Google AI.

00:00:45.860 --> 00:00:48.830
And today, we're going to
talk about opportunities,

00:00:48.830 --> 00:00:53.450
challenges, and strategies
to develop AI for everyone.

00:00:53.450 --> 00:00:57.000
So if you could, please, I
would love for each one of you

00:00:57.000 --> 00:01:00.280
just to talk very briefly
about what do you do at Google.

00:01:03.192 --> 00:01:04.650
RAJEN SHETH: My
name is Rajen Sheth

00:01:04.650 --> 00:01:07.570
and I run the product
team for cloud AI.

00:01:07.570 --> 00:01:11.170
And so what we're doing
is within Google Cloud,

00:01:11.170 --> 00:01:13.690
we're trying to figure out how
can we bring the best of AI

00:01:13.690 --> 00:01:16.360
to developers and
to enterprises.

00:01:16.360 --> 00:01:20.740
And so how do we give developers
a great platform to build on?

00:01:20.740 --> 00:01:23.829
But then how do we take some
of the best of Google's AI

00:01:23.829 --> 00:01:25.870
and make it so that they're
available as services

00:01:25.870 --> 00:01:28.750
for developers?

00:01:28.750 --> 00:01:30.100
FERNANDA VIEGAS: Thank you.

00:01:30.100 --> 00:01:31.690
DAPHNE LUONG: My
name is Daphne Luong.

00:01:31.690 --> 00:01:35.530
I work in Google AI,
human computation, data,

00:01:35.530 --> 00:01:38.306
and natural language
understanding.

00:01:38.306 --> 00:01:39.430
JOHN PLATT: I'm John Platt.

00:01:39.430 --> 00:01:43.150
And I help run the
applied science org

00:01:43.150 --> 00:01:44.920
inside of Google AI.

00:01:44.920 --> 00:01:47.620
We're super excited by
all the opportunities

00:01:47.620 --> 00:01:52.530
in science related to things
like biology or physics.

00:01:52.530 --> 00:01:55.000
And we believe that computer
science, especially machine

00:01:55.000 --> 00:01:57.850
learning and AI, can really
help accelerate that.

00:01:57.850 --> 00:01:59.905
So that's what my org does.

00:01:59.905 --> 00:02:01.030
FERNANDA VIEGAS: Thank you.

00:02:01.030 --> 00:02:02.920
So one of the
things that is very

00:02:02.920 --> 00:02:06.850
inspiring to me about this panel
and about a lot of the themes

00:02:06.850 --> 00:02:10.820
going on at I/O this
year is AI for everyone.

00:02:10.820 --> 00:02:15.700
And so I know that in PAIR,
one of the main research

00:02:15.700 --> 00:02:20.260
themes we have is how do
you design human-centered AI

00:02:20.260 --> 00:02:21.010
technology?

00:02:21.010 --> 00:02:23.170
How do you start
with a user in mind

00:02:23.170 --> 00:02:27.350
and then design machine learning
technology that fits with that?

00:02:27.350 --> 00:02:30.290
So one of the things I'd
love to ask each one of you

00:02:30.290 --> 00:02:34.060
to talk about a little bit is
what does "AI for everyone"

00:02:34.060 --> 00:02:36.290
mean to you?

00:02:36.290 --> 00:02:38.350
RAJEN SHETH: So for
me, it means, really,

00:02:38.350 --> 00:02:41.650
how do you make
AI easy and useful

00:02:41.650 --> 00:02:44.709
for people that want to use it?

00:02:44.709 --> 00:02:46.750
And I think there are two
big problems that we're

00:02:46.750 --> 00:02:49.150
seeing right now with AI.

00:02:49.150 --> 00:02:53.440
One is that there isn't as
much skill out there or as much

00:02:53.440 --> 00:02:56.830
knowledge about how to
build AI models, especially

00:02:56.830 --> 00:02:59.960
deep learning, as
there should be.

00:02:59.960 --> 00:03:03.100
And so we're trying to
both build tools for that

00:03:03.100 --> 00:03:07.090
and to make it so that
more people can access AI,

00:03:07.090 --> 00:03:14.182
but then can also learn about AI
and figure out how to use that.

00:03:14.182 --> 00:03:15.640
What we're finding,
for example, is

00:03:15.640 --> 00:03:18.310
that there are probably
in the tens of thousands

00:03:18.310 --> 00:03:20.602
of people out there that know
how to use deep learning,

00:03:20.602 --> 00:03:22.434
probably in the order
of a couple of million

00:03:22.434 --> 00:03:23.680
of data scientists out there.

00:03:23.680 --> 00:03:25.830
But there are 21
million developers.

00:03:25.830 --> 00:03:28.690
And so our goal is
how do we get AI

00:03:28.690 --> 00:03:31.141
to be accessible by the
21 million developers?

00:03:31.141 --> 00:03:32.890
And the second part
of that is usefulness.

00:03:32.890 --> 00:03:36.700
How do we actually make
it so that AI is useful?

00:03:36.700 --> 00:03:38.560
How do we go-- especially
for businesses,

00:03:38.560 --> 00:03:41.975
beyond where they can
not only do cool things,

00:03:41.975 --> 00:03:43.600
but they can do things
that are useful,

00:03:43.600 --> 00:03:46.924
that actually are vital
to their business?

00:03:46.924 --> 00:03:48.090
FERNANDA VIEGAS: Definitely.

00:03:48.090 --> 00:03:52.050
DAPHNE LUONG: To me, getting
everyone involved and aware

00:03:52.050 --> 00:03:55.260
so that algorithms
have representation

00:03:55.260 --> 00:03:56.590
for all of our users.

00:03:56.590 --> 00:03:59.180
So on the data side,
this means that we

00:03:59.180 --> 00:04:01.350
need to have data
that's representative

00:04:01.350 --> 00:04:03.502
of all our users.

00:04:03.502 --> 00:04:04.210
JOHN PLATT: Yeah.

00:04:04.210 --> 00:04:06.400
Well, for me, AI for
everyone is trying

00:04:06.400 --> 00:04:11.000
to get the benefits of AI
to a large part of society

00:04:11.000 --> 00:04:12.520
or across the world.

00:04:12.520 --> 00:04:14.686
So what we do is we look
for leverage points.

00:04:14.686 --> 00:04:17.019
We're not trying to get
everyone in the world to use AI.

00:04:17.019 --> 00:04:21.040
We're trying to have the
fruits of what AI can provide.

00:04:21.040 --> 00:04:24.670
So if we can find these
leverage points, where

00:04:24.670 --> 00:04:28.471
a local application of AI can
really help the whole world,

00:04:28.471 --> 00:04:29.720
that's what we're looking for.

00:04:29.720 --> 00:04:31.840
That's why I'm super
excited about applying

00:04:31.840 --> 00:04:34.360
AI and machine learning
to problems in science,

00:04:34.360 --> 00:04:38.262
because science can lead to
breakthrough technologies.

00:04:38.262 --> 00:04:39.220
FERNANDA VIEGAS: Great.

00:04:39.220 --> 00:04:42.100
I think one of the
interesting things here

00:04:42.100 --> 00:04:44.830
for this panel and this
theme is that each one of you

00:04:44.830 --> 00:04:47.530
is coming from a very
different perspective.

00:04:47.530 --> 00:04:50.620
So Rajen, I think about you
as coming from the product

00:04:50.620 --> 00:04:53.200
slash business side of things.

00:04:53.200 --> 00:04:57.430
Daphne, you're helping build
some of the fundamental blocks,

00:04:57.430 --> 00:05:00.070
like the data, the
fundamental data creation

00:05:00.070 --> 00:05:03.700
blocks for being able to
do this technology at all.

00:05:03.700 --> 00:05:07.332
And then John, you're coming
from a research perspective.

00:05:07.332 --> 00:05:08.665
How do we enable the scientists?

00:05:08.665 --> 00:05:12.380
How do we enable
science at large?

00:05:12.380 --> 00:05:15.594
So one of the things that comes
up whenever we talk about AI,

00:05:15.594 --> 00:05:17.260
whenever we talk about
machine learning,

00:05:17.260 --> 00:05:20.890
is this notion of
representation.

00:05:20.890 --> 00:05:23.920
And I want to start at
the beginning with that.

00:05:23.920 --> 00:05:25.270
I want to talk about data.

00:05:25.270 --> 00:05:27.340
And so Daphne, I
was really excited

00:05:27.340 --> 00:05:29.860
when I heard that you were
going to be part of this panel

00:05:29.860 --> 00:05:32.650
because I think about you
as the data czar of Google.

00:05:32.650 --> 00:05:33.460
[LAUGHS]

00:05:33.460 --> 00:05:39.340
And so your work is a lot
about creating better ways

00:05:39.340 --> 00:05:40.330
to scale up.

00:05:40.330 --> 00:05:42.730
For instance, how
do we gather data?

00:05:42.730 --> 00:05:45.465
How do we do it in
an insightful way?

00:05:45.465 --> 00:05:46.840
And so one of the
things I'd love

00:05:46.840 --> 00:05:48.460
for you to talk
about a little bit

00:05:48.460 --> 00:05:53.770
is the representation
of data sets at Google.

00:05:53.770 --> 00:05:55.109
What does that look like today?

00:05:55.109 --> 00:05:55.900
DAPHNE LUONG: Yeah.

00:05:55.900 --> 00:06:01.900
So having semantically
meaningful and systematic--

00:06:01.900 --> 00:06:04.280
correctly calibrated,
labeled data,

00:06:04.280 --> 00:06:07.960
and scale is really crucial
for machine learning.

00:06:07.960 --> 00:06:11.650
And from a representation
perspective, some of the data

00:06:11.650 --> 00:06:14.500
sets out there are
not well balanced.

00:06:14.500 --> 00:06:18.080
For example, if you
look at Wikipedia data,

00:06:18.080 --> 00:06:21.550
"he," the pronoun "he"
is mentioned so much more

00:06:21.550 --> 00:06:23.090
than "she."

00:06:23.090 --> 00:06:26.050
So inside Google, we
have a lot of data.

00:06:26.050 --> 00:06:29.380
And an example set that
I want to talk about

00:06:29.380 --> 00:06:35.050
is the audio data set that
we open sourced last year.

00:06:35.050 --> 00:06:44.200
The data sets had 2.1 million
video with 8.5k hour of audio.

00:06:44.200 --> 00:06:48.940
But then we annotate it with
around 500 classes of sound.

00:06:48.940 --> 00:06:50.340
That's pretty
interesting, right?

00:06:50.340 --> 00:06:53.800
You have sounds from speech
and music, all the way

00:06:53.800 --> 00:06:58.870
to engine sounds or
burping or gargling.

00:06:58.870 --> 00:07:00.400
So then from that
perspective, it's

00:07:00.400 --> 00:07:03.280
really important to
have audio and sounds

00:07:03.280 --> 00:07:06.190
from all over the world,
like having sounds from kids.

00:07:06.190 --> 00:07:09.190
From India is very different
from China and the US.

00:07:09.190 --> 00:07:11.050
So that kind of
thing is what I think

00:07:11.050 --> 00:07:13.889
we should think about
whenever we make data sets.

00:07:13.889 --> 00:07:15.430
FERNANDA VIEGAS: So
that's wonderful.

00:07:15.430 --> 00:07:19.210
Yeah, when I became aware of all
the different open source data

00:07:19.210 --> 00:07:23.410
sets that we have put
out, it's really exciting

00:07:23.410 --> 00:07:25.720
to see audio, video, text.

00:07:25.720 --> 00:07:26.650
You have translation.

00:07:26.650 --> 00:07:29.550
You have all of those.

00:07:29.550 --> 00:07:32.370
But whenever you're talking
about data in machine learning,

00:07:32.370 --> 00:07:34.830
you're also talking
about challenges.

00:07:34.830 --> 00:07:37.770
So one of the things you started
touching on a little bit--

00:07:37.770 --> 00:07:40.260
and I want to pull
that out more--

00:07:40.260 --> 00:07:43.530
is this notion of
representation,

00:07:43.530 --> 00:07:48.690
making sure that the data
set has good representation

00:07:48.690 --> 00:07:51.750
from different users,
different communities.

00:07:51.750 --> 00:07:53.710
And that's a hard thing to do.

00:07:53.710 --> 00:07:57.347
So I'm curious, what are we
at Google doing about that?

00:07:57.347 --> 00:07:58.680
How are you thinking about that?

00:07:58.680 --> 00:07:59.790
Yeah.

00:07:59.790 --> 00:08:03.270
So Google users in
communities, they

00:08:03.270 --> 00:08:07.590
have been very generous,
donating data and stuff for us

00:08:07.590 --> 00:08:09.510
to make our product better.

00:08:09.510 --> 00:08:11.400
An example is Google Guide.

00:08:11.400 --> 00:08:14.040
How many have used that
Google Guide in the audience?

00:08:14.040 --> 00:08:15.090
So, yes.

00:08:15.090 --> 00:08:17.340
So on the data
front, we are also

00:08:17.340 --> 00:08:19.570
doing something really similar.

00:08:19.570 --> 00:08:24.060
We have a Crowdsource app,
which is an app and on the web.

00:08:24.060 --> 00:08:26.230
Right now, since
the last two years,

00:08:26.230 --> 00:08:28.020
we actually had
two million users

00:08:28.020 --> 00:08:30.930
donating to the app
from all over the world,

00:08:30.930 --> 00:08:33.030
actually 233 countries.

00:08:33.030 --> 00:08:35.049
That's a lot of
different people.

00:08:35.049 --> 00:08:38.380
And we had 200 million
donations from people answering

00:08:38.380 --> 00:08:40.710
questions, all
the way from, hey,

00:08:40.710 --> 00:08:45.660
can you see this sign from the
street in an Indonesian city,

00:08:45.660 --> 00:08:48.180
versus, hey, how do
you use this sentence?

00:08:48.180 --> 00:08:51.750
What's the sentiment of
the sentence in Hindi?

00:08:51.750 --> 00:08:53.310
So there's all those data.

00:08:53.310 --> 00:08:57.270
And those data actually help
us make products better--

00:08:57.270 --> 00:09:00.180
help people navigate streets
where there is no sign,

00:09:00.180 --> 00:09:05.070
from like a landmark navigation,
making the keyboard more

00:09:05.070 --> 00:09:10.310
accessible, as well as making
heavy [INAUDIBLE] detail

00:09:10.310 --> 00:09:11.550
labeled data set.

00:09:11.550 --> 00:09:13.245
So I think that's
really important.

00:09:13.245 --> 00:09:15.930
If you don't have
the Crowdsource app,

00:09:15.930 --> 00:09:18.230
please download it.

00:09:18.230 --> 00:09:19.410
FERNANDA VIEGAS: Great.

00:09:19.410 --> 00:09:20.220
Thank you.

00:09:20.220 --> 00:09:23.475
One of the things about
audio that I had never

00:09:23.475 --> 00:09:25.350
thought about until I
read this is that there

00:09:25.350 --> 00:09:29.430
is a huge discrepancy
between even voices of adults

00:09:29.430 --> 00:09:31.770
versus children.

00:09:31.770 --> 00:09:36.120
So you have to think about
many, many different dimensions

00:09:36.120 --> 00:09:36.739
of diversity.

00:09:36.739 --> 00:09:37.530
DAPHNE LUONG: Yeah.

00:09:37.530 --> 00:09:39.180
And then from the
app, we actually

00:09:39.180 --> 00:09:46.680
have user-initiated groups
that's in 35 countries.

00:09:46.680 --> 00:09:50.050
We're actually really excited
about the whole contributions.

00:09:50.050 --> 00:09:51.300
FERNANDA VIEGAS: Awesome.

00:09:51.300 --> 00:09:52.770
OK.

00:09:52.770 --> 00:09:57.660
So now, moving on a little
bit about from data.

00:09:57.660 --> 00:10:01.980
Whenever we talk about AI for
everyone, one of the, I think,

00:10:01.980 --> 00:10:04.140
first questions
that comes to mind

00:10:04.140 --> 00:10:08.970
is how do we make AI accessible
and better for our users?

00:10:08.970 --> 00:10:11.520
And Rajen, here I'm
going to turn to you.

00:10:11.520 --> 00:10:20.230
And since you are helping
lead these services on cloud,

00:10:20.230 --> 00:10:23.750
how do you think about
designing these ML tools

00:10:23.750 --> 00:10:27.790
so that they are easy
for users to use?

00:10:27.790 --> 00:10:30.600
And then a segue
question there too,

00:10:30.600 --> 00:10:33.930
if you could talk a little bit
about what are the difficulties

00:10:33.930 --> 00:10:35.130
that you're starting to see.

00:10:35.130 --> 00:10:39.210
What are the patterns
where users get stuck?

00:10:39.210 --> 00:10:40.250
RAJEN SHETH: Yeah, yeah.

00:10:40.250 --> 00:10:40.880
Great question.

00:10:40.880 --> 00:10:43.840
I think there are two
big parts of that.

00:10:43.840 --> 00:10:47.212
One is how do we make it easy
for people to create models?

00:10:47.212 --> 00:10:49.170
But then the second part
is how do we make sure

00:10:49.170 --> 00:10:54.592
that those models are fair, are
not biased, are really serving

00:10:54.592 --> 00:10:55.800
the purpose in the right way?

00:10:55.800 --> 00:10:58.200
Because it is very
easy for people

00:10:58.200 --> 00:11:01.090
to just take what they've been
doing manually and end up,

00:11:01.090 --> 00:11:04.300
encoding bias into a model.

00:11:04.300 --> 00:11:06.300
So on one part of it,
what we're doing is

00:11:06.300 --> 00:11:07.800
we're providing a
variety of tools.

00:11:07.800 --> 00:11:10.800
One of the most interesting
things that we're working on

00:11:10.800 --> 00:11:12.150
is Cloud AutoML.

00:11:12.150 --> 00:11:16.260
And so Cloud AutoML is a
way by which you can give us

00:11:16.260 --> 00:11:18.180
a data set, and then
we'll use machine

00:11:18.180 --> 00:11:21.450
learning to create a machine
learning model for that data

00:11:21.450 --> 00:11:23.010
set.

00:11:23.010 --> 00:11:24.910
The first area we're
doing this with

00:11:24.910 --> 00:11:27.330
is image recognition,
where you can give us

00:11:27.330 --> 00:11:29.670
a set of images that are
labeled and then we'll

00:11:29.670 --> 00:11:32.460
give you a highly
accurate model on top

00:11:32.460 --> 00:11:35.550
of that to predict for images.

00:11:35.550 --> 00:11:39.330
And so that has really
helped this expand.

00:11:39.330 --> 00:11:43.800
We have 15,000 people that have
signed up for access to that.

00:11:43.800 --> 00:11:45.900
And we're seeing amazing
use cases, everything

00:11:45.900 --> 00:11:49.230
from really large companies
to very small companies who

00:11:49.230 --> 00:11:51.610
are doing things
around, for example,

00:11:51.610 --> 00:11:55.710
helping farmers manage their
field or helping track litter

00:11:55.710 --> 00:11:59.115
and making sure to track
litter back to the producers.

00:11:59.115 --> 00:12:01.920
FERNANDA VIEGAS: How
much technical expertise

00:12:01.920 --> 00:12:05.114
do I, as a user, need to
have if I want to use AutoML?

00:12:05.114 --> 00:12:06.530
RAJEN SHETH: Yeah,
great question.

00:12:06.530 --> 00:12:08.100
We're trying to
orient it to make it

00:12:08.100 --> 00:12:10.730
so that you need to
have limited or no ML

00:12:10.730 --> 00:12:12.440
expertise to be able to use it.

00:12:12.440 --> 00:12:14.880
And if you look at our UI,
it's very similar to, say,

00:12:14.880 --> 00:12:15.840
Google Photos.

00:12:15.840 --> 00:12:17.640
You upload your pictures.

00:12:17.640 --> 00:12:19.050
You label them.

00:12:19.050 --> 00:12:21.930
We actually are integrating with
a lot of the services Daphne

00:12:21.930 --> 00:12:25.299
and team are using to have
us label it if you want that.

00:12:25.299 --> 00:12:27.090
And then you just hit
train, and then we'll

00:12:27.090 --> 00:12:28.710
create a model for you.

00:12:28.710 --> 00:12:31.310
However, that said,
getting the right data

00:12:31.310 --> 00:12:33.139
set is the key part there.

00:12:33.139 --> 00:12:35.180
So even if you don't have
to go create the model,

00:12:35.180 --> 00:12:37.310
you have to create
the right data set.

00:12:37.310 --> 00:12:39.890
And so there, we're trying
to provide tools to make sure

00:12:39.890 --> 00:12:42.650
that people are able to
create the right data set

00:12:42.650 --> 00:12:44.900
and figure out where there
are flaws in the data set.

00:12:44.900 --> 00:12:48.357
And then on the other side of
this, around things like bias,

00:12:48.357 --> 00:12:50.190
we're trying to figure
out tools around that

00:12:50.190 --> 00:12:53.810
and also help advise
customers with best practices

00:12:53.810 --> 00:12:56.840
to make sure that the data
set they're putting in

00:12:56.840 --> 00:12:59.600
can actually produce
the right results.

00:12:59.600 --> 00:13:00.750
FERNANDA VIEGAS: OK.

00:13:00.750 --> 00:13:04.490
And so data continues to be one
of those bottlenecks, right?

00:13:04.490 --> 00:13:06.260
RAJEN SHETH: It is the
foundation of this.

00:13:06.260 --> 00:13:08.262
And you put in the
wrong data, you end up

00:13:08.262 --> 00:13:09.220
with the wrong results.

00:13:09.220 --> 00:13:10.136
FERNANDA VIEGAS: Yeah.

00:13:10.136 --> 00:13:12.440
OK.

00:13:12.440 --> 00:13:16.940
John, of the themes, as
you all have been seeing,

00:13:16.940 --> 00:13:19.880
is diversity--
diversity in data sets.

00:13:19.880 --> 00:13:22.070
Also, how do you create
these experiences

00:13:22.070 --> 00:13:25.520
for users, who are not
themselves machine learning

00:13:25.520 --> 00:13:28.620
experts, to be able to
use this technology?

00:13:28.620 --> 00:13:31.580
I'm also curious to hear
from you about diversity

00:13:31.580 --> 00:13:36.650
and representation on the side
of machine learning, developers

00:13:36.650 --> 00:13:37.760
and researchers.

00:13:37.760 --> 00:13:39.980
The reason I'm looking
at you is because you're

00:13:39.980 --> 00:13:41.660
working with scientists.

00:13:41.660 --> 00:13:46.940
And so these are people who are
going to be potentially needing

00:13:46.940 --> 00:13:48.540
custom models.

00:13:48.540 --> 00:13:52.100
They are going to be pushing
the envelope sometimes.

00:13:52.100 --> 00:13:55.010
But they may not themselves
be machine learning experts.

00:13:55.010 --> 00:13:58.430
So I'm curious about
your thoughts there.

00:13:58.430 --> 00:14:00.170
JOHN PLATT: Well,
science is getting

00:14:00.170 --> 00:14:02.450
to be increasingly data driven.

00:14:02.450 --> 00:14:05.600
A lot of scientists
like physicists,

00:14:05.600 --> 00:14:08.930
geologists to biologists,
they do their job

00:14:08.930 --> 00:14:11.120
by gathering large
amounts of data.

00:14:11.120 --> 00:14:15.230
So they're already kind of
getting into data science.

00:14:15.230 --> 00:14:17.690
And so there's a big
opportunity right now

00:14:17.690 --> 00:14:21.200
for them to use machine learning
to make themselves be much more

00:14:21.200 --> 00:14:22.880
productive.

00:14:22.880 --> 00:14:24.350
The way I look at
it is it's kind

00:14:24.350 --> 00:14:26.627
of like giving a
scientist thousands

00:14:26.627 --> 00:14:28.460
of undergraduate
assistants, in other words,

00:14:28.460 --> 00:14:31.080
something to look at, slides--

00:14:31.080 --> 00:14:34.040
if you could examine
slides of cells,

00:14:34.040 --> 00:14:37.295
you could just have thousands of
assistants look at it for you.

00:14:37.295 --> 00:14:38.420
That's sort of one example.

00:14:38.420 --> 00:14:41.940
So I don't know if this
answers your question,

00:14:41.940 --> 00:14:44.780
but I'm really
excited that we can

00:14:44.780 --> 00:14:49.070
build models to help scientists
do specific things to help

00:14:49.070 --> 00:14:51.140
accelerate their research.

00:14:51.140 --> 00:14:53.840
And we actually
collaborate with scientists

00:14:53.840 --> 00:14:55.520
to help build these
kind of models.

00:14:55.520 --> 00:14:57.505
So I hope that
answered your question.

00:14:57.505 --> 00:14:59.570
FERNANDA VIEGAS: And
what kinds of scientists

00:14:59.570 --> 00:15:02.180
are you working with?

00:15:02.180 --> 00:15:04.550
JOHN PLATT: Well, the
applied science people,

00:15:04.550 --> 00:15:07.850
there's a bunch of us working
with biomedical research trying

00:15:07.850 --> 00:15:12.480
to understand the core
research of disease

00:15:12.480 --> 00:15:15.260
like neurodegenerative diseases.

00:15:15.260 --> 00:15:18.140
There's a bunch of us working
on climate and energy.

00:15:18.140 --> 00:15:21.920
So we're working with a company
called TAE Technologies, trying

00:15:21.920 --> 00:15:24.410
to figure out whether we
can make fusion energy,

00:15:24.410 --> 00:15:26.510
make it commercially relevant.

00:15:26.510 --> 00:15:29.450
So we span across biology,
physics, chemistry.

00:15:29.450 --> 00:15:34.170
We work with a bunch of people
who simulate quantum chemistry.

00:15:34.170 --> 00:15:36.980
FERNANDA VIEGAS: And I
guess the same question

00:15:36.980 --> 00:15:39.590
I asked Rajen, I'd
like to ask you,

00:15:39.590 --> 00:15:42.350
which is, what do you
see as the biggest

00:15:42.350 --> 00:15:46.220
difficulties for scientists
to do this kind of work today?

00:15:46.220 --> 00:15:48.830
Are there patterns?

00:15:48.830 --> 00:15:50.120
Is it data?

00:15:50.120 --> 00:15:53.530
Is it understanding how
to put together models?

00:15:53.530 --> 00:15:54.447
What do you see?

00:15:54.447 --> 00:15:56.030
JOHN PLATT: Well,
they generally don't

00:15:56.030 --> 00:15:59.120
want to build models themselves,
although some of them do.

00:15:59.120 --> 00:16:02.870
There is an increasing crossover
between scientists and computer

00:16:02.870 --> 00:16:04.040
scientists.

00:16:04.040 --> 00:16:08.890
But I think they would prefer
to have a productivity tool.

00:16:08.890 --> 00:16:13.220
In other words, can we help
them build a productivity tool

00:16:13.220 --> 00:16:15.030
for their own research?

00:16:15.030 --> 00:16:17.810
And either they could try
to build it themselves

00:16:17.810 --> 00:16:20.390
if they know machine
learning or we can help them.

00:16:20.390 --> 00:16:21.670
FERNANDA VIEGAS: OK.

00:16:21.670 --> 00:16:23.900
So along those lines,
one of the things

00:16:23.900 --> 00:16:25.730
that I've been
very excited about

00:16:25.730 --> 00:16:32.840
is a few months
ago, folks in PAIR,

00:16:32.840 --> 00:16:37.610
in collaboration with others at
Google, launched TensorFlow.js.

00:16:37.610 --> 00:16:41.805
And this is really significant,
in the sense that--

00:16:41.805 --> 00:16:43.910
what we were talking
about, in terms

00:16:43.910 --> 00:16:48.440
of democratizing the
technology and bringing

00:16:48.440 --> 00:16:50.870
machine learning to the web.

00:16:50.870 --> 00:16:55.010
TensorFlow.js speaks the
native language of the web,

00:16:55.010 --> 00:16:56.870
speaks JavaScript.

00:16:56.870 --> 00:16:59.330
And so now, you
have a whole new set

00:16:59.330 --> 00:17:03.440
of developers who
can start to take

00:17:03.440 --> 00:17:05.660
advantage of this technology.

00:17:05.660 --> 00:17:07.940
And we are already
starting to see

00:17:07.940 --> 00:17:10.400
some interesting applications.

00:17:10.400 --> 00:17:15.609
For instance, one
developer who decided

00:17:15.609 --> 00:17:20.359
to train a model based on his
web camera to make the mouse

00:17:20.359 --> 00:17:24.319
move in different directions
just by where he was looking.

00:17:24.319 --> 00:17:26.660
So if he looked up,
the mouse would go up.

00:17:26.660 --> 00:17:28.890
If he looked down, the
mouse would go down.

00:17:28.890 --> 00:17:31.670
And the reason he did that
was because he was trying

00:17:31.670 --> 00:17:34.850
to help a friend who
had suffered a stroke

00:17:34.850 --> 00:17:37.190
and became paraplegic.

00:17:37.190 --> 00:17:40.910
So that to me,
again, starts to talk

00:17:40.910 --> 00:17:42.860
about this possibility
of bringing

00:17:42.860 --> 00:17:47.850
this technology into your
personal-- what do you need?

00:17:47.850 --> 00:17:53.180
How can you train these models
for your specific purposes?

00:17:59.180 --> 00:18:02.210
As we talk about
democratizing the technology,

00:18:02.210 --> 00:18:05.310
as we talk about
bringing it to everybody,

00:18:05.310 --> 00:18:08.840
one of the things that
I think we at Google

00:18:08.840 --> 00:18:13.280
are very mindful of is that we
have to do that with insight

00:18:13.280 --> 00:18:15.620
and responsibility.

00:18:15.620 --> 00:18:18.290
You want to be thinking ahead.

00:18:18.290 --> 00:18:23.810
So one question
I have, Rajen, is

00:18:23.810 --> 00:18:28.610
since you are providing
this service, what

00:18:28.610 --> 00:18:34.190
happens if you have bad actors
trying to use your service?

00:18:34.190 --> 00:18:37.820
And what safeguards
could you provide?

00:18:37.820 --> 00:18:38.690
RAJEN SHETH: Yeah.

00:18:38.690 --> 00:18:40.106
That's a great
question, something

00:18:40.106 --> 00:18:42.620
that is very much
top of mind for us.

00:18:42.620 --> 00:18:46.460
Because as we see
more people use AI,

00:18:46.460 --> 00:18:49.730
we want to make sure that we're
providing those safeguards,

00:18:49.730 --> 00:18:54.440
whether it's safeguards
or best practices

00:18:54.440 --> 00:18:57.500
to make sure that it's
used in the right way.

00:18:57.500 --> 00:19:00.780
On one hand, when you have
things like open source

00:19:00.780 --> 00:19:02.780
technologies, it's very
hard to prevent people

00:19:02.780 --> 00:19:05.390
from doing bad things if
they want to do bad things.

00:19:05.390 --> 00:19:08.907
But one advantage we have
with cloud is that we can.

00:19:08.907 --> 00:19:12.960
We can put in terms of services
or acceptable use policies,

00:19:12.960 --> 00:19:14.600
things like that
to make sure people

00:19:14.600 --> 00:19:17.190
are using this in the right way.

00:19:17.190 --> 00:19:20.660
And I think one thing
we're discovering with AI

00:19:20.660 --> 00:19:22.490
is that we have to rethink that.

00:19:22.490 --> 00:19:25.950
It's not an infrastructure
technology anymore.

00:19:25.950 --> 00:19:27.950
We have to rethink the uses.

00:19:27.950 --> 00:19:31.590
And that's something that
we're doing quite a bit.

00:19:31.590 --> 00:19:33.230
Another thing that
worries me too

00:19:33.230 --> 00:19:36.770
is beyond just
bad actors, people

00:19:36.770 --> 00:19:40.760
unintentionally doing
things that might be bad

00:19:40.760 --> 00:19:42.380
and not even knowing
that they are.

00:19:42.380 --> 00:19:43.963
And it goes back to
some of the things

00:19:43.963 --> 00:19:45.950
we talked about with
bias and fairness.

00:19:45.950 --> 00:19:47.900
In a lot of cases,
people aren't intending

00:19:47.900 --> 00:19:49.664
to create something
that's biased,

00:19:49.664 --> 00:19:51.080
but they end up
creating something

00:19:51.080 --> 00:19:53.750
that's biased because of
the data that's coming in.

00:19:53.750 --> 00:19:55.490
And so that's another
area that we're

00:19:55.490 --> 00:19:58.610
looking at quite a bit, which
is how do we help people out

00:19:58.610 --> 00:20:02.210
there, our customers,
developers, have

00:20:02.210 --> 00:20:06.410
the tools to be able to do
this well such that they're

00:20:06.410 --> 00:20:09.820
not unintentionally
creating bad models too?

00:20:09.820 --> 00:20:12.140
DAPHNE LUONG:
Alongside of tooling,

00:20:12.140 --> 00:20:15.470
I think it's also important
to get people trained all

00:20:15.470 --> 00:20:17.210
the way from the
beginning, end to end,

00:20:17.210 --> 00:20:20.092
really mindful of
the data, diversity,

00:20:20.092 --> 00:20:21.050
what you need for data.

00:20:21.050 --> 00:20:23.570
We actually train,
even annotator,

00:20:23.570 --> 00:20:28.020
so they're aware, having
diversity along the whole path.

00:20:28.020 --> 00:20:29.630
So I think that's
important, and then

00:20:29.630 --> 00:20:31.820
for other practitioners
to understand

00:20:31.820 --> 00:20:35.960
about best data, best
practices for machine learning.

00:20:35.960 --> 00:20:38.252
FERNANDA VIEGAS: Absolutely.

00:20:38.252 --> 00:20:39.710
JOHN PLATT: I mean,
I usually think

00:20:39.710 --> 00:20:41.859
about this from the
research perspective

00:20:41.859 --> 00:20:42.900
because I'm a researcher.

00:20:42.900 --> 00:20:45.740
And we write and publish papers.

00:20:45.740 --> 00:20:47.690
And we have to
think very carefully

00:20:47.690 --> 00:20:50.070
about when we write a paper.

00:20:50.070 --> 00:20:53.570
I mean, the intent is to have
a positive impact on the world.

00:20:53.570 --> 00:20:57.140
So before we release
the paper, we say,

00:20:57.140 --> 00:21:00.170
is this going to have a positive
impact in the world or not?

00:21:00.170 --> 00:21:02.849
Now, you can't always predict.

00:21:02.849 --> 00:21:04.640
There's going to be
unintended consequences

00:21:04.640 --> 00:21:07.100
of releasing a technology.

00:21:07.100 --> 00:21:10.370
But we try to do
the best we can.

00:21:10.370 --> 00:21:13.890
And we generally have a
bias towards openness,

00:21:13.890 --> 00:21:16.970
because it's just
generally, the priority

00:21:16.970 --> 00:21:19.264
is that it's better to
share these technologies.

00:21:19.264 --> 00:21:20.180
FERNANDA VIEGAS: Yeah.

00:21:20.180 --> 00:21:23.675
I have to say as a
researcher myself,

00:21:23.675 --> 00:21:28.850
I love the ethos of just
publishing and open sourcing.

00:21:28.850 --> 00:21:32.330
And I have never been
in a field like this

00:21:32.330 --> 00:21:36.440
before where the whole
idea is you publish

00:21:36.440 --> 00:21:39.170
as soon as you have a finding.

00:21:39.170 --> 00:21:41.360
And then you submit
to a conference.

00:21:41.360 --> 00:21:43.100
I had never seen that before.

00:21:43.100 --> 00:21:44.870
And I think it's
wonderful because it

00:21:44.870 --> 00:21:48.155
means that the field
progresses much faster.

00:21:51.090 --> 00:21:54.200
I want to come back to one point
that we were talking about,

00:21:54.200 --> 00:21:56.510
which is this notion
of there might

00:21:56.510 --> 00:21:58.820
be unintended consequences.

00:21:58.820 --> 00:22:01.740
Or even with the
best of intentions,

00:22:01.740 --> 00:22:04.505
you may not know that
your data is biased

00:22:04.505 --> 00:22:07.580
or that there might be
a skew or something.

00:22:07.580 --> 00:22:15.890
And there, one thought I'd love
to put out is this notion that

00:22:15.890 --> 00:22:21.680
building tools so that
people can inspect the data,

00:22:21.680 --> 00:22:24.920
and also opening up
the community of people

00:22:24.920 --> 00:22:26.940
who inspect these things.

00:22:26.940 --> 00:22:30.430
So in PAIR, for instance, one
of the things we've been doing

00:22:30.430 --> 00:22:33.040
is building tools,
visualization tools,

00:22:33.040 --> 00:22:36.580
that visualize your
entire data set and allows

00:22:36.580 --> 00:22:39.040
you to very easily see
things like, oh, this

00:22:39.040 --> 00:22:40.400
is the shape of my data.

00:22:40.400 --> 00:22:41.990
This is the distribution.

00:22:41.990 --> 00:22:45.520
This is what female data
points look like or male data

00:22:45.520 --> 00:22:48.370
points or children data points.

00:22:48.370 --> 00:22:52.360
And what this means
is that because it's

00:22:52.360 --> 00:22:57.130
this user interface where the
experiences, the visualization,

00:22:57.130 --> 00:23:00.440
it means not only
developers can look at this.

00:23:00.440 --> 00:23:05.440
It means that product managers
can look at this, executives,

00:23:05.440 --> 00:23:06.220
whoever--

00:23:06.220 --> 00:23:10.790
different stakeholders can
start having a conversation,

00:23:10.790 --> 00:23:12.940
sit together and be
like, oh, I don't know.

00:23:12.940 --> 00:23:14.860
I saw something strange
in the data set.

00:23:14.860 --> 00:23:16.980
Or what do you think is
going to happen when we

00:23:16.980 --> 00:23:20.320
facet by this dimension here?

00:23:20.320 --> 00:23:23.110
And so I'd love to hear a
little bit of your thoughts

00:23:23.110 --> 00:23:29.420
on the importance of broadening
the conversation around doing

00:23:29.420 --> 00:23:32.500
AI for everyone so that
it's not a conversation that

00:23:32.500 --> 00:23:36.340
is happening only
amongst developers.

00:23:36.340 --> 00:23:39.762
Obviously, the developers
are super important.

00:23:39.762 --> 00:23:41.470
But I also think it's
a conversation that

00:23:41.470 --> 00:23:45.980
needs to happen with a
broader set of stakeholders.

00:23:45.980 --> 00:23:47.730
RAJEN SHETH: I guess
one thought on that--

00:23:47.730 --> 00:23:49.690
and it's interesting you--

00:23:49.690 --> 00:23:51.310
in the previous
conversation, you're

00:23:51.310 --> 00:23:54.810
talking about how
rapidly this is going.

00:23:54.810 --> 00:23:58.150
One way I usually start out
my presentations to customers

00:23:58.150 --> 00:24:01.840
is the first slide, I show a
picture of the Mosaic browser

00:24:01.840 --> 00:24:03.490
from 1994.

00:24:03.490 --> 00:24:07.270
And my point with it
is that all of us, we

00:24:07.270 --> 00:24:09.310
remember back to what that was.

00:24:09.310 --> 00:24:11.560
And before that
point, the internet

00:24:11.560 --> 00:24:14.750
had been around for a very
long time before that point.

00:24:14.750 --> 00:24:16.660
But that was when
the internet started

00:24:16.660 --> 00:24:19.420
to go out to many more
people for many more things.

00:24:19.420 --> 00:24:22.180
You started to see everything
from checking sports scores all

00:24:22.180 --> 00:24:24.580
the way to e-commerce,
all kinds of things.

00:24:24.580 --> 00:24:27.010
And it opened up many
more possibilities

00:24:27.010 --> 00:24:28.535
and opened up a
lot more questions.

00:24:28.535 --> 00:24:29.410
FERNANDA VIEGAS: Yes.

00:24:29.410 --> 00:24:31.000
RAJEN SHETH: And
also questions where

00:24:31.000 --> 00:24:33.580
there needed to be more tools,
tools to make it easier,

00:24:33.580 --> 00:24:35.200
but tools to make people safe.

00:24:35.200 --> 00:24:37.420
And I think we are at 1994.

00:24:37.420 --> 00:24:40.676
We are at that point
for AI right now.

00:24:40.676 --> 00:24:42.550
It's been used for years
and years and years,

00:24:42.550 --> 00:24:45.310
but now, it's being used
and many, many more ways.

00:24:45.310 --> 00:24:48.880
And so I think we need to think
about tooling in that way.

00:24:48.880 --> 00:24:53.320
New problems are starting
to come up every single day.

00:24:53.320 --> 00:24:57.350
And new technologies are
coming out every single day.

00:24:57.350 --> 00:25:00.490
And so we're going to need
to think about that not just

00:25:00.490 --> 00:25:02.320
as a developer
community, but then

00:25:02.320 --> 00:25:04.870
in coordination with how it's
being used and the people

00:25:04.870 --> 00:25:06.167
that it's affecting.

00:25:09.626 --> 00:25:11.000
JOHN PLATT: Rajen
and I were just

00:25:11.000 --> 00:25:13.430
talking about the
first spam detector

00:25:13.430 --> 00:25:16.760
before we came up in 1997.

00:25:16.760 --> 00:25:21.346
I remember the first spam email
that got sent in the '80s.

00:25:21.346 --> 00:25:24.240
And so as new
problems come up, you

00:25:24.240 --> 00:25:27.230
need to have certain new tools
to help fight the problem.

00:25:27.230 --> 00:25:29.480
RAJEN SHETH: That's a good
point, that for a long time

00:25:29.480 --> 00:25:31.340
you didn't need a spam detector.

00:25:31.340 --> 00:25:32.840
And then all of a
sudden, you did.

00:25:32.840 --> 00:25:36.560
And then you had to have tools
to be able to work with it.

00:25:36.560 --> 00:25:39.140
FERNANDA VIEGAS: Another
thing, another aspect

00:25:39.140 --> 00:25:41.960
of this of broadening
the conversation,

00:25:41.960 --> 00:25:44.690
especially when we're
talking about products also,

00:25:44.690 --> 00:25:47.570
is this notion of
having in the same room

00:25:47.570 --> 00:25:51.500
your technical team, your
design team, for instance,

00:25:51.500 --> 00:25:54.000
and your PMs and so forth.

00:25:54.000 --> 00:25:56.750
And so one of the things
we're doing at Google

00:25:56.750 --> 00:26:02.600
is finding ways of involving
the designers, the UXers

00:26:02.600 --> 00:26:07.040
from the beginning and
educating them around machine

00:26:07.040 --> 00:26:09.720
learning as a design material.

00:26:09.720 --> 00:26:12.620
How do you think about
that as, literally,

00:26:12.620 --> 00:26:15.230
a material you're
going to design with?

00:26:15.230 --> 00:26:18.350
And if you think about it, it's
a very challenging material

00:26:18.350 --> 00:26:21.680
because it's something that
each person in a product

00:26:21.680 --> 00:26:25.110
might have a slightly
different user experience.

00:26:25.110 --> 00:26:27.680
My experience might be
very different from yours.

00:26:27.680 --> 00:26:30.410
Certain things may be
automated in different ways

00:26:30.410 --> 00:26:32.940
for different people
at different times.

00:26:32.940 --> 00:26:37.220
And so it makes that UX,
the user interaction,

00:26:37.220 --> 00:26:38.330
much more challenging.

00:26:38.330 --> 00:26:42.050
That space becomes a lot bigger.

00:26:42.050 --> 00:26:45.290
And so again, when we are
talking about AI for everyone,

00:26:45.290 --> 00:26:48.230
it really means
this conversation

00:26:48.230 --> 00:26:54.060
that needs to happen
between multiple parties.

00:26:54.060 --> 00:26:57.920
One thing I have to come
back to before we run out

00:26:57.920 --> 00:27:02.840
of time, John, is this
notion of science.

00:27:02.840 --> 00:27:06.890
One of the things that I think
there's a lot of excitement

00:27:06.890 --> 00:27:09.860
around is that
machine learning could

00:27:09.860 --> 00:27:13.460
be a new paradigm
for scientists,

00:27:13.460 --> 00:27:17.690
that finally we could
solve certain, very

00:27:17.690 --> 00:27:22.860
complex scientific problems
using these new tools.

00:27:22.860 --> 00:27:27.080
So I'm curious about your
thoughts as why is it

00:27:27.080 --> 00:27:33.410
that AI makes such a difference
in these very complex spaces?

00:27:33.410 --> 00:27:36.020
JOHN PLATT: Well, that's
an excellent question.

00:27:36.020 --> 00:27:40.700
Right now, the current state
of AI and machine learning

00:27:40.700 --> 00:27:44.060
is still very perceptual.

00:27:44.060 --> 00:27:48.050
We're very, very good at looking
at images, listening to sounds.

00:27:48.050 --> 00:27:50.990
And so you can imagine,
as I said before,

00:27:50.990 --> 00:27:53.360
I like to talk about these
thousands of undergraduates.

00:27:53.360 --> 00:27:56.340
They would be listening
or looking at the data.

00:27:56.340 --> 00:27:59.840
And so that's a great
productivity term for science.

00:27:59.840 --> 00:28:02.790
In the long run--

00:28:02.790 --> 00:28:04.640
people are starting to
research into this--

00:28:04.640 --> 00:28:07.160
but in the long
run, what we really

00:28:07.160 --> 00:28:09.270
want is something
called causal models,

00:28:09.270 --> 00:28:11.990
where the machine learning or
AI algorithm actually tries

00:28:11.990 --> 00:28:14.470
to understand what caused what.

00:28:14.470 --> 00:28:18.020
I mean, that's still very,
very early in research.

00:28:18.020 --> 00:28:19.830
But that's almost the
essence of science,

00:28:19.830 --> 00:28:23.490
a scientific model
explains this caused that.

00:28:23.490 --> 00:28:27.440
And you can extrapolate if you
know what the causal model is.

00:28:27.440 --> 00:28:31.880
So I think that's sort of
the long-term trend for AI.

00:28:31.880 --> 00:28:35.870
But we're not there yet
because our models don't really

00:28:35.870 --> 00:28:38.900
understand causation yet.

00:28:38.900 --> 00:28:41.450
FERNANDA VIEGAS: So one of
the things along those lines,

00:28:41.450 --> 00:28:45.980
we had a visiting scientist
in our group in Cambridge.

00:28:45.980 --> 00:28:47.869
He's in earthquake science.

00:28:47.869 --> 00:28:49.160
You know who I'm talking about.

00:28:49.160 --> 00:28:50.840
He's a professor
at Harvard who came

00:28:50.840 --> 00:28:53.180
and spent half a year with us.

00:28:53.180 --> 00:28:56.750
And one of the things I
thought was incredibly

00:28:56.750 --> 00:28:59.420
inspiring about what he
was doing-- so obviously,

00:28:59.420 --> 00:29:02.810
as an earthquake scientist, you
want to predict earthquakes.

00:29:02.810 --> 00:29:04.970
It's incredibly hard.

00:29:04.970 --> 00:29:08.130
We can't do it well yet.

00:29:08.130 --> 00:29:10.220
But the thing that
he was able to do--

00:29:10.220 --> 00:29:13.790
so I did not know anything
about earthquake science--

00:29:13.790 --> 00:29:16.070
he explained to me that
this is the kind of science

00:29:16.070 --> 00:29:21.510
that lives in HPC, high
performance computing.

00:29:21.510 --> 00:29:27.530
So huge, huge computers
that will spend entire weeks

00:29:27.530 --> 00:29:31.880
doing very, very gnarly
computation, trying to

00:29:31.880 --> 00:29:35.270
simulate where
might an aftershock

00:29:35.270 --> 00:29:38.060
happen after an
earthquake, things

00:29:38.060 --> 00:29:40.970
that Californians should
care a lot about--

00:29:40.970 --> 00:29:42.870
and I'm sure you do.

00:29:42.870 --> 00:29:47.690
So imagine something
that takes a week running

00:29:47.690 --> 00:29:51.140
on huge computers.

00:29:51.140 --> 00:29:56.600
He was able to get these
kinds of simulation results

00:29:56.600 --> 00:30:02.000
that were just as good
using a simple neural net.

00:30:02.000 --> 00:30:04.850
It was so simple, the
neural net was so simple

00:30:04.850 --> 00:30:06.470
that it was the
kind of neural net

00:30:06.470 --> 00:30:08.290
where you could
count the neurons.

00:30:08.290 --> 00:30:10.970
That's how simple it was.

00:30:10.970 --> 00:30:14.510
And so one, he was stunned
and incredibly happy

00:30:14.510 --> 00:30:18.200
because it meant he didn't
need to wait a whole week

00:30:18.200 --> 00:30:21.630
and using a ton of compute
power to do the same kind

00:30:21.630 --> 00:30:24.770
of simulations he was doing.

00:30:24.770 --> 00:30:26.180
So that in itself was a win.

00:30:26.180 --> 00:30:28.490
But then the deeper question,
and one of the things

00:30:28.490 --> 00:30:30.890
that gives me a
lot of excitement,

00:30:30.890 --> 00:30:36.620
is what exactly was this
neural net figuring out

00:30:36.620 --> 00:30:40.130
about the physics of the Earth
that we haven't figured out

00:30:40.130 --> 00:30:43.020
yet?

00:30:43.020 --> 00:30:47.150
And again, if we could interpret
what this system was doing,

00:30:47.150 --> 00:30:50.570
what this neural net was doing,
if we could learn from it,

00:30:50.570 --> 00:30:54.680
could we become better
scientists ourselves?

00:30:54.680 --> 00:30:57.520
And you're starting to
see this in science.

00:30:57.520 --> 00:31:00.150
You're starting to
see this in medicine.

00:31:00.150 --> 00:31:04.730
So for instance, the brain that
work with diabetic retinopathy,

00:31:04.730 --> 00:31:08.960
where these systems are looking
at the image of the back

00:31:08.960 --> 00:31:10.850
of your eye, your fundus.

00:31:10.850 --> 00:31:16.040
And not only are they being able
to understand whether or not

00:31:16.040 --> 00:31:20.660
you have the
diabetic retinopathy,

00:31:20.660 --> 00:31:24.260
but they're also being
able to do things

00:31:24.260 --> 00:31:28.220
like understand the
gender of the patients

00:31:28.220 --> 00:31:36.140
and the cardiovascular
disease risk, things

00:31:36.140 --> 00:31:39.350
that doctors themselves
couldn't necessarily

00:31:39.350 --> 00:31:40.830
see in these images.

00:31:40.830 --> 00:31:43.280
So again, part of
the race now is

00:31:43.280 --> 00:31:45.860
to understand what are
these systems learning

00:31:45.860 --> 00:31:47.090
that we couldn't learn?

00:31:47.090 --> 00:31:52.700
So can we become better
doctors, better scientists

00:31:52.700 --> 00:31:54.140
by using this?

00:31:54.140 --> 00:31:55.160
Can we learn back?

00:31:55.160 --> 00:31:59.090
Can we listen back
from these machines?

00:31:59.090 --> 00:32:02.190
So we have a few minutes to go.

00:32:02.190 --> 00:32:04.760
But I have two more questions.

00:32:04.760 --> 00:32:06.740
One is for you, Rajen.

00:32:06.740 --> 00:32:09.920
What are you seeing
of the ability of AI

00:32:09.920 --> 00:32:12.720
to really help businesses?

00:32:12.720 --> 00:32:14.990
I guess I'm curious
about what have

00:32:14.990 --> 00:32:16.880
been some of the most
successful cases,

00:32:16.880 --> 00:32:18.560
but also some of
the most surprising?

00:32:18.560 --> 00:32:21.581
And you're like, whoa, I didn't
know we could do AI for this.

00:32:21.581 --> 00:32:22.580
RAJEN SHETH: Yeah, yeah.

00:32:22.580 --> 00:32:24.620
There are a bunch of
things that we've seen.

00:32:24.620 --> 00:32:27.740
There are definitely
certain industries

00:32:27.740 --> 00:32:29.070
that are farther ahead.

00:32:29.070 --> 00:32:31.670
Tech industry, of course,
there's a lot being used there.

00:32:31.670 --> 00:32:34.480
Financial services, there's
a lot already being used.

00:32:34.480 --> 00:32:37.280
But then there are many
emerging use cases, which

00:32:37.280 --> 00:32:38.940
I think are pretty amazing.

00:32:38.940 --> 00:32:42.590
Some of the examples we see,
like on the manufacturing line.

00:32:42.590 --> 00:32:46.580
Can you use vision to figure
out if a part is OK or not,

00:32:46.580 --> 00:32:48.650
like if a tire coming
down the factory line

00:32:48.650 --> 00:32:51.370
is going to be a good
tire or it might actually

00:32:51.370 --> 00:32:53.240
bear risk to the person.

00:32:53.240 --> 00:32:56.720
Those kinds of things are things
that we're starting to see.

00:32:56.720 --> 00:32:58.036
Retail is another case.

00:32:58.036 --> 00:32:59.660
How do you actually
make it so that you

00:32:59.660 --> 00:33:03.080
can make the experience
of the user better?

00:33:03.080 --> 00:33:04.530
So I'll give you an example.

00:33:04.530 --> 00:33:07.010
We're using AutoML with Disney.

00:33:07.010 --> 00:33:10.460
My son happens to be a
massive Lightning McQueen

00:33:10.460 --> 00:33:12.500
fan, from "Cars."

00:33:12.500 --> 00:33:15.390
And I'm sure many of you have
kids or family members that

00:33:15.390 --> 00:33:18.020
are big Disney fans.

00:33:18.020 --> 00:33:22.850
He has an insatiable
appetite for Disney stuff,

00:33:22.850 --> 00:33:24.500
and stuff with
Lightning McQueen.

00:33:24.500 --> 00:33:26.480
Now, with the technology
that we've developed,

00:33:26.480 --> 00:33:31.370
you can search on
shopDisney for everything

00:33:31.370 --> 00:33:33.707
with Lightning McQueen on
it, whether or not it says it

00:33:33.707 --> 00:33:34.790
in the description or not.

00:33:34.790 --> 00:33:36.810
It'll do it by
visual inspection.

00:33:36.810 --> 00:33:39.650
Those kinds of things
are really interesting,

00:33:39.650 --> 00:33:41.310
interesting use cases.

00:33:41.310 --> 00:33:42.950
And so we're
starting to see that.

00:33:42.950 --> 00:33:44.783
One of the things I
think that is important,

00:33:44.783 --> 00:33:47.845
though, is that we need to
get businesses to the point

00:33:47.845 --> 00:33:49.220
where they're not
thinking about,

00:33:49.220 --> 00:33:52.160
hey, I'm using AI to
do this, but where

00:33:52.160 --> 00:33:54.680
the users are feeling magic.

00:33:54.680 --> 00:33:57.402
And it doesn't matter if it's
AI behind the covers or not.

00:33:57.402 --> 00:33:59.360
And that's where I think
things like what we've

00:33:59.360 --> 00:34:00.735
done with Google
Photos and stuff

00:34:00.735 --> 00:34:03.410
like that is really incredible
because it's not about AI,

00:34:03.410 --> 00:34:05.180
it's about the user experience.

00:34:05.180 --> 00:34:07.040
FERNANDA VIEGAS: That's awesome.

00:34:07.040 --> 00:34:12.780
So we talked about
some of the challenges,

00:34:12.780 --> 00:34:14.330
some of the strategies.

00:34:14.330 --> 00:34:16.730
I want to end on
the opportunities.

00:34:16.730 --> 00:34:18.710
We talked about some of
the opportunities also,

00:34:18.710 --> 00:34:21.560
but I want to end on that
note, on the opportunities.

00:34:21.560 --> 00:34:25.820
In the last year,
we've seen AI being

00:34:25.820 --> 00:34:30.989
applied to various domains
in really interesting ways.

00:34:30.989 --> 00:34:33.889
So you have medicine, science.

00:34:33.889 --> 00:34:35.370
You have farming.

00:34:35.370 --> 00:34:37.949
We were talking about
some of those examples.

00:34:37.949 --> 00:34:42.350
And I'd love to hear from each
one of you what opportunity

00:34:42.350 --> 00:34:46.976
are you excited about for
the next however many years?

00:34:46.976 --> 00:34:48.100
John, do you want to start?

00:34:48.100 --> 00:34:48.909
JOHN PLATT: OK.

00:34:48.909 --> 00:34:51.514
Well, you mentioned
biomedical research,

00:34:51.514 --> 00:34:53.420
so now I can't talk about that.

00:34:53.420 --> 00:34:57.530
So let me talk about another
project I find really exciting,

00:34:57.530 --> 00:34:59.860
which I mentioned before,
which is we're actually

00:34:59.860 --> 00:35:04.300
working to see if we can
get fusion energy to be

00:35:04.300 --> 00:35:06.340
a real commercial
source of energy

00:35:06.340 --> 00:35:08.680
in order to displace
fossil fuels.

00:35:08.680 --> 00:35:10.870
Because right now, the
world is burning way too

00:35:10.870 --> 00:35:13.390
many fossil fuels and
flooding the atmosphere

00:35:13.390 --> 00:35:15.110
with carbon dioxide.

00:35:15.110 --> 00:35:17.440
So we're actually using
machine learning to help

00:35:17.440 --> 00:35:20.500
this company, TAE Technologies.

00:35:20.500 --> 00:35:21.750
And we're helping in two ways.

00:35:21.750 --> 00:35:24.464
We're trying to optimize
their experiments.

00:35:24.464 --> 00:35:26.380
We're actually helping
them design experiments

00:35:26.380 --> 00:35:27.780
through optimization.

00:35:27.780 --> 00:35:30.670
And we're using Bayesian
methods to actually help

00:35:30.670 --> 00:35:33.000
them debug the plasma
inside their machine.

00:35:33.000 --> 00:35:34.000
FERNANDA VIEGAS: Oh wow.

00:35:34.000 --> 00:35:35.680
JOHN PLATT: So the
goal is going to be

00:35:35.680 --> 00:35:38.260
trying to make this
plasma about as

00:35:38.260 --> 00:35:40.390
hot as the center of the sun.

00:35:40.390 --> 00:35:44.020
And if we can do that, we can
measure its heat loss rate

00:35:44.020 --> 00:35:46.150
and see if we can actually
get to a commercially

00:35:46.150 --> 00:35:48.025
relevant fusion, which
would really, I think,

00:35:48.025 --> 00:35:49.310
revolutionize the world.

00:35:49.310 --> 00:35:52.250
FERNANDA VIEGAS: When
can we expect that?

00:35:52.250 --> 00:35:55.620
JOHN PLATT: Well, I don't
want to promise anything.

00:35:55.620 --> 00:35:59.385
We'll have some good scientific
results, I think, in 2019.

00:35:59.385 --> 00:36:01.510
We won't have what they
call breakeven fusion then,

00:36:01.510 --> 00:36:04.690
but we'll have some very solid
results predicting the heat

00:36:04.690 --> 00:36:05.360
loss rate.

00:36:05.360 --> 00:36:08.810
So we'll know a lot
more in about a year.

00:36:08.810 --> 00:36:11.690
FERNANDA VIEGAS: Awesome.

00:36:11.690 --> 00:36:15.080
DAPHNE LUONG: For me, I'm
very excited about two things.

00:36:15.080 --> 00:36:17.080
The first thing is
really we'll continue

00:36:17.080 --> 00:36:21.850
to work with the Crowdsource
contributors who built really

00:36:21.850 --> 00:36:25.000
diverse, globally
diverse data set,

00:36:25.000 --> 00:36:28.720
that not a single company
couldn't build by itself,

00:36:28.720 --> 00:36:30.820
but actually leveraging
everyone who's

00:36:30.820 --> 00:36:34.990
passionate about representing
their part of the world

00:36:34.990 --> 00:36:37.090
to create data set
that we can open source

00:36:37.090 --> 00:36:40.720
to use by everyone-- and then
similarly for machine learning.

00:36:40.720 --> 00:36:43.920
Since I do a lot of
volunteering outside of work

00:36:43.920 --> 00:36:47.410
with social entrepreneurs
across the world,

00:36:47.410 --> 00:36:54.430
I really want to see that
ML available for nonprofits

00:36:54.430 --> 00:36:59.530
to do sort of air quality models
or maybe [INAUDIBLE] books

00:36:59.530 --> 00:37:04.960
they do, using Translate to make
books available for thousands

00:37:04.960 --> 00:37:07.615
of different languages
and for mother tongue.

00:37:07.615 --> 00:37:08.740
So I think that's exciting.

00:37:08.740 --> 00:37:09.954
I'm looking forward to that.

00:37:09.954 --> 00:37:10.870
FERNANDA VIEGAS: Nice.

00:37:10.870 --> 00:37:11.710
RAJEN SHETH: I
think for me, there

00:37:11.710 --> 00:37:13.210
are two things I'm
passionate about.

00:37:13.210 --> 00:37:15.340
One is health care and
the other is education.

00:37:15.340 --> 00:37:17.440
And so health care,
just this idea

00:37:17.440 --> 00:37:19.930
like you talked about,
being able to use AI

00:37:19.930 --> 00:37:21.370
to do early disease detection.

00:37:21.370 --> 00:37:24.985
You have this incredible
impact on one human being

00:37:24.985 --> 00:37:28.365
and on many human beings, that
and on families around them

00:37:28.365 --> 00:37:29.240
and things like that.

00:37:29.240 --> 00:37:31.450
And that, I think,
could be really amazing.

00:37:31.450 --> 00:37:33.040
Education, I spend
most of my time

00:37:33.040 --> 00:37:35.500
here at Google working on
our education products.

00:37:35.500 --> 00:37:39.820
And the idea of imagining an
environment where every student

00:37:39.820 --> 00:37:42.340
can learn in the way
that they want to learn--

00:37:42.340 --> 00:37:44.500
at their own pace,
in their own style--

00:37:44.500 --> 00:37:48.740
and having AI help power
that can be transformative.

00:37:48.740 --> 00:37:53.920
You can harness the potential
of every single individual

00:37:53.920 --> 00:37:57.100
in a way that was
never possible before.

00:37:57.100 --> 00:38:00.250
FERNANDA VIEGAS: I'm
going to segue from that

00:38:00.250 --> 00:38:04.420
and say, again, one of the
things that has been really

00:38:04.420 --> 00:38:09.220
inspiring to see with
things like TensorFlow.js

00:38:09.220 --> 00:38:14.500
is that we have now professors
at a number of universities

00:38:14.500 --> 00:38:19.570
who are building educational
materials around this.

00:38:19.570 --> 00:38:21.790
And I feel like that's
really important.

00:38:21.790 --> 00:38:26.290
I'm really excited about this
because it not only lowers

00:38:26.290 --> 00:38:29.480
the barrier for entry
in this technology,

00:38:29.480 --> 00:38:35.270
but it also, I think, starts
to hit a much more diverse set

00:38:35.270 --> 00:38:39.580
of developers who are
interested in using

00:38:39.580 --> 00:38:42.460
this technology for
different things

00:38:42.460 --> 00:38:45.440
that we haven't
even dreamt of yet.

00:38:45.440 --> 00:38:48.710
So I'm very excited about that.

00:38:48.710 --> 00:38:52.660
The other thing that I
think is really good to see

00:38:52.660 --> 00:38:56.020
is how the conversation
around machine learning

00:38:56.020 --> 00:39:03.220
has started to touch on things
like ethics and fairness.

00:39:03.220 --> 00:39:06.160
And I think that's
really important.

00:39:06.160 --> 00:39:10.120
I'm really happy that here at
Google, we think very deeply

00:39:10.120 --> 00:39:13.750
about these questions.

00:39:13.750 --> 00:39:16.480
And both from a
research perspective,

00:39:16.480 --> 00:39:18.880
what are research
things we can do to help

00:39:18.880 --> 00:39:20.660
address some of these problems?

00:39:20.660 --> 00:39:27.040
But also policy, product-wise,
design-wise, it's

00:39:27.040 --> 00:39:29.420
a broader conversation.

00:39:29.420 --> 00:39:33.920
So with that, I am going
to thank our panel.

00:39:33.920 --> 00:39:35.000
Thank you so much.

00:39:35.000 --> 00:39:38.380
It's been so inspiring to
hear not only your insights,

00:39:38.380 --> 00:39:40.780
but to have you share
your experience,

00:39:40.780 --> 00:39:45.130
as I said, from many different
parts of Google on this topic

00:39:45.130 --> 00:39:46.190
today with us.

00:39:46.190 --> 00:39:46.700
Thank you.

00:39:46.700 --> 00:39:47.260
JOHN PLATT: Thank you.

00:39:47.260 --> 00:39:48.385
FERNANDA VIEGAS: Thank you.

00:39:48.385 --> 00:39:51.090
[MUSIC PLAYING]

