WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.346
[MUSIC PLAYING]

00:00:06.214 --> 00:00:08.330
LAURENCE MORONEY: Hi,
and welcome to part three

00:00:08.330 --> 00:00:12.380
of this series on using Google
Colab to code, train, and test

00:00:12.380 --> 00:00:15.380
neural networks in the browser
without needing to install

00:00:15.380 --> 00:00:17.330
any kind of a runtime.

00:00:17.330 --> 00:00:18.950
In the previous
video, I showed you

00:00:18.950 --> 00:00:21.380
how you can install
TensorFlow with Colab.

00:00:21.380 --> 00:00:22.910
In this video,
I'll show you then

00:00:22.910 --> 00:00:25.250
how you can use
TensorFlow to build

00:00:25.250 --> 00:00:28.790
a neural network for breast
cancer classification.

00:00:28.790 --> 00:00:31.400
It runs completely in
the browser using Colab,

00:00:31.400 --> 00:00:33.860
and it's really quick
to train and test.

00:00:33.860 --> 00:00:35.840
The data for training
this neural network

00:00:35.840 --> 00:00:38.570
comes from the Diagnostic
Wisconsin Breast Cancer

00:00:38.570 --> 00:00:39.660
Database.

00:00:39.660 --> 00:00:41.990
You can find it at the
URL in the description.

00:00:41.990 --> 00:00:44.660
It has close to 600
samples of data,

00:00:44.660 --> 00:00:47.450
each from a cell biopsy,
where 30 features have

00:00:47.450 --> 00:00:49.580
been extracted per cell.

00:00:49.580 --> 00:00:52.940
I've pre-processed the
data into several CSV files

00:00:52.940 --> 00:00:55.910
so we can just focus on
the neural network itself.

00:00:55.910 --> 00:00:57.440
Let's now take a
look at the code

00:00:57.440 --> 00:01:00.410
for training this neural
network using this data so you

00:01:00.410 --> 00:01:02.930
can use that network to
then perform breast cancer

00:01:02.930 --> 00:01:04.849
classification yourself.

00:01:04.849 --> 00:01:07.620
Let's start with
uploading the CSV files.

00:01:07.620 --> 00:01:09.350
Now, that's a really
neat thing in Colab,

00:01:09.350 --> 00:01:12.090
that you can load
external data into it.

00:01:12.090 --> 00:01:14.060
I'm going to load
my CSVs into panda

00:01:14.060 --> 00:01:16.760
dataframes with this code.

00:01:16.760 --> 00:01:19.760
Next, using Keros in
the sequential API,

00:01:19.760 --> 00:01:21.500
I'm going to create
a neural network

00:01:21.500 --> 00:01:23.500
with an input dimension of 30.

00:01:23.500 --> 00:01:26.540
And that's because each of
these cells has 30 features.

00:01:26.540 --> 00:01:29.660
And we'll then have a layer
of 16, then 8, then 6,

00:01:29.660 --> 00:01:31.610
and then, finally, 1.

00:01:31.610 --> 00:01:33.290
The final layer
will be activated

00:01:33.290 --> 00:01:35.360
by a Sigmoid function,
which will push it

00:01:35.360 --> 00:01:37.460
towards a 1 or a 0.

00:01:37.460 --> 00:01:40.310
Now we're classifying two
features, so that's perfect.

00:01:40.310 --> 00:01:42.890
The network itself will
need to have a loss function

00:01:42.890 --> 00:01:45.260
and an optimizer defined on it.

00:01:45.260 --> 00:01:48.350
On each iteration, it measures
how well it did in training

00:01:48.350 --> 00:01:50.030
using the loss function.

00:01:50.030 --> 00:01:53.230
It then tries to improve on
that using the optimizer.

00:01:53.230 --> 00:01:55.460
And as you'll see in a
moment, the training process

00:01:55.460 --> 00:02:00.140
has 100 steps, with this
loop happening once per step.

00:02:00.140 --> 00:02:03.110
The training itself takes
place in the Fit function.

00:02:03.110 --> 00:02:06.080
Here, I pass in the
training x's and y's, and I

00:02:06.080 --> 00:02:08.479
specify how many
times it will loop,

00:02:08.479 --> 00:02:11.750
where a loop is it making
a guess at the relationship

00:02:11.750 --> 00:02:13.370
between the x and the y.

00:02:13.370 --> 00:02:16.580
It measures how well or how
badly it does using the loss

00:02:16.580 --> 00:02:19.040
function, and then it
improves on its guess

00:02:19.040 --> 00:02:20.600
using the optimizer.

00:02:20.600 --> 00:02:22.790
It's coded to do that
100 times, but you

00:02:22.790 --> 00:02:24.800
can amend that easily
and explore the results

00:02:24.800 --> 00:02:26.660
for yourself.

00:02:26.660 --> 00:02:28.880
As you'll see, once
it finishes training,

00:02:28.880 --> 00:02:34.820
the loss is 0.0595, showing
that it's about 94% accurate.

00:02:34.820 --> 00:02:37.250
We can now test that
network with data

00:02:37.250 --> 00:02:39.710
that the neural network
hasn't yet seen.

00:02:39.710 --> 00:02:41.970
This is the x-test data.

00:02:41.970 --> 00:02:45.260
So we'll get a set of y
predictions for this data.

00:02:45.260 --> 00:02:47.500
Now, these predictions are
going to be a probability.

00:02:47.500 --> 00:02:49.880
They're not a 0 or
a 1, but values that

00:02:49.880 --> 00:02:52.790
are close to 0 or close to 1.

00:02:52.790 --> 00:02:55.280
So we'll write this code that
takes all of the values that

00:02:55.280 --> 00:02:58.610
are less than 0.5 and
consider that to be 0

00:02:58.610 --> 00:03:00.950
and everything else to be a 1.

00:03:00.950 --> 00:03:02.990
And now, here's some
simple code that

00:03:02.990 --> 00:03:04.820
will compare the
predicted values

00:03:04.820 --> 00:03:07.700
for the test set
against the actual known

00:03:07.700 --> 00:03:09.440
values for the same set.

00:03:09.440 --> 00:03:12.350
Now, there were 114
values in this test set,

00:03:12.350 --> 00:03:15.710
and you'll see it
gets it 100% correct.

00:03:15.710 --> 00:03:19.700
Now, remember earlier we said
it was about 94% accurate.

00:03:19.700 --> 00:03:22.626
So why do you think it
gets it 100% correct?

00:03:22.626 --> 00:03:24.500
Well, that's a little
homework for you to do.

00:03:24.500 --> 00:03:26.124
Post your answers in
the comments below

00:03:26.124 --> 00:03:27.630
and let's see who
can get it right.

00:03:27.630 --> 00:03:29.250
And that's it for this episode.

00:03:29.250 --> 00:03:30.950
And in the next
video in this series,

00:03:30.950 --> 00:03:32.630
my colleague,
Paige, will show you

00:03:32.630 --> 00:03:35.840
about how to use different
runtimes and processors

00:03:35.840 --> 00:03:39.410
and how to use your code to
take advantage of GPUs and TPUs

00:03:39.410 --> 00:03:40.367
right in your browser.

00:03:40.367 --> 00:03:42.950
So whatever you do, don't forget
to hit that subscribe button,

00:03:42.950 --> 00:03:44.420
and you'll be able
to catch up with it.

00:03:44.420 --> 00:03:44.960
Thank you.

00:03:44.960 --> 00:03:48.310
[MUSIC PLAYING]

