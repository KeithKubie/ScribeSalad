WEBVTT
Kind: captions
Language: en

00:00:00.122 --> 00:00:03.705
Let me come back to one piece of this code that needs a little more explanation,

00:00:03.705 --> 00:00:05.307
and that's this kernel call.

00:00:05.307 --> 00:00:07.580
Here's the name of the kernel call, square,

00:00:07.580 --> 00:00:11.215
and we call it with these launch parameters with these arguments.

00:00:11.215 --> 00:00:16.087
There's a lot going on in this call so I need to explain details that we didn't need in our example

00:00:16.087 --> 00:00:18.421
that are necessary as we move forward.

00:00:18.421 --> 00:00:24.461
What I told you is that we were launching one block of 64 threads, and that's absolutely true.

00:00:24.461 --> 00:00:27.797
But let me explain what's happening under the hood in a little more detail.

00:00:27.797 --> 00:00:33.415
When you launch a kernel, you specify both the number of blocks and the number of threads per block.

00:00:33.415 --> 00:00:37.609
In our example, we only had one block of 64 threads.

00:00:37.609 --> 00:00:39.912
But we want to run bigger problems than this.

00:00:39.912 --> 00:00:43.313
What you need to know about the hardware right now is two things.

00:00:43.313 --> 00:00:47.084
One, it is capable of running many blocks at the same time.

00:00:47.084 --> 00:00:50.922
Two, each block has a maximum number of threads that it can support.

00:00:50.922 --> 00:00:56.927
Newer GPUs can support 1024 threads per block; older GPUs can only support 512.

00:00:56.927 --> 00:01:01.604
When you have lots of work to do, you'll divide that work into any number of blocks

00:01:01.604 --> 00:01:06.435
each of which had no more than 512 or possibly 1,024 threads.

00:01:06.435 --> 00:01:12.806
If we wanted to launch 128 threads and square the values in each of them instead of 64 threads,

00:01:12.806 --> 00:01:17.313
we could change this call to square of 1,128.

00:01:17.313 --> 00:01:23.116
If we wanted to launch 1280 threads instead we could call square of 10,128,

00:01:23.116 --> 00:01:26.386
launching ten blocks of 128 threads each.

00:01:26.386 --> 00:01:31.894
Or square 5,256, we're launching five blocks of 256 threads each.

00:01:31.894 --> 00:01:37.533
But we can't call square 1,1280 because that's too many threads per block.

00:01:37.533 --> 00:01:42.469
You should pick the breakdown of threads and blocks that makes the most sense for your problem.

00:01:42.469 --> 00:01:46.833
As we saw before, each thread that we launch knows its index within that block.

00:01:46.833 --> 00:01:52.438
And you won't be surprised to hear that each thread also knows the index of its block as well.

00:01:52.438 --> 00:01:55.341
We'll see how we access this information in a moment.

00:01:55.341 --> 00:01:59.812
Now how do these kernels actually map into threads and blocks?

00:01:59.812 --> 00:02:06.321
Well, when we square 10,128, we're going to launch 10 thread blocks of 128 threads each.

00:02:06.321 --> 00:02:12.825
When we square 5,256, we'll launch 5 consecutive thread blocks of 256 threads each.

00:02:12.825 --> 00:02:18.302
These diagrams are one dimensional, they only progress in one dimension, the x dimension.

00:02:18.302 --> 00:02:20.838
That's fine if your problem is one dimensional.

00:02:20.838 --> 00:02:23.168
But many problems are 2 or 3 dimensional.

00:02:23.168 --> 00:02:25.479
You'll be doing image processing in the homeworks, for instance,

00:02:25.479 --> 00:02:28.014
and that's very definitely a two dimensional problem.

00:02:28.014 --> 00:02:33.522
It makes sense that CUDA would natively support not only one dimensional layouts of blocks and threads,

00:02:33.522 --> 00:02:37.194
like we're showing here, but also 2 and 3 dimension as well.

00:02:37.194 --> 00:02:43.664
For instance, perhaps we like to process this 128 x 128 pixel image.

00:02:43.664 --> 00:02:45.867
We'd like to launch one thread per pixel.

00:02:45.867 --> 00:02:50.539
We might choose, for instance, to launch these 128 x 128 threads

00:02:50.539 --> 00:02:55.770
as 128 blocks in the y-dimension where each one of those blocks

00:02:55.770 --> 00:03:00.699
is a 128 by 1 block of threads in the x-dimension.

00:03:00.699 --> 00:03:10.377
Or we might instead choose to launch an 8 x 8 grid of blocks where each block is 16 threads by 16 threads.

00:03:10.377 --> 00:03:16.046
So CUDA supports 1, 2, or 3 dimensional thread blocks.

00:03:16.046 --> 00:03:20.684
We can also arrange thread blocks into 1, 2, or 3 dimensional grids.

00:03:20.684 --> 00:03:23.355
Now, let's return to how we launch kernels.

00:03:23.355 --> 00:03:26.391
We put two parameters in the triple chevrons.

00:03:26.391 --> 00:03:29.676
The first is the dimensionality of the grid of blocks,

00:03:29.676 --> 00:03:33.094
and the second is the dimensionality of the threads within a block.

00:03:33.094 --> 00:03:35.775
We can specify up to three dimensions for each.

00:03:35.775 --> 00:03:38.843
But if we don't specify a dimension, it defaults to one.

00:03:38.843 --> 00:03:43.582
More generally, you can specify a 3 dimensional configuration for grid of blocks

00:03:43.582 --> 00:03:49.920
or block of threads with this dim3 struct which you initialize as dim3 x,y,z.

00:03:49.920 --> 00:03:54.527
Recall that, again, if we don't specify y or z, they default to 1.

00:03:54.527 --> 00:04:01.532
When we say dim3 of w, that means the same thing as dim3 of w,1,1,

00:04:01.532 --> 00:04:05.503
and you can actually abbreviate this with simply the integer w.

00:04:05.503 --> 00:04:17.478
When we specified square of 1,64, that was equivalent to square of dim3 1,1,1, dim3 64,1,1.

