WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:04.620
SPEAKER 1: At the City
University of London

00:00:04.620 --> 00:00:07.280
in the Centre of Human
and Computer Interaction.

00:00:07.280 --> 00:00:10.500
She's done a lot of work
on end user development,

00:00:10.500 --> 00:00:15.480
and more recently on
smart homes, and also some

00:00:15.480 --> 00:00:19.960
on personal
information management.

00:00:19.960 --> 00:00:23.114
So with that, I
hand it over to her.

00:00:23.114 --> 00:00:24.030
SIMONE STUMPF: Thanks.

00:00:24.030 --> 00:00:26.009
Let's see if I can
bring up these slides,

00:00:26.009 --> 00:00:26.800
is it just forward?

00:00:29.400 --> 00:00:30.260
Great.

00:00:30.260 --> 00:00:34.140
So thanks for inviting me.

00:00:34.140 --> 00:00:35.370
My name is Simone Stumpf.

00:00:35.370 --> 00:00:37.860
I'm from the Centre
for HCI Design

00:00:37.860 --> 00:00:39.660
at City University of London.

00:00:39.660 --> 00:00:43.230
And you will be able to
tell that I'm an academic

00:00:43.230 --> 00:00:46.020
because my slides look
awful in comparison

00:00:46.020 --> 00:00:48.750
to the previous speakers.

00:00:48.750 --> 00:00:51.360
So what I'm here to
talk to you about

00:00:51.360 --> 00:00:55.140
is how to design
for intelligibility

00:00:55.140 --> 00:00:56.670
of smart systems.

00:00:56.670 --> 00:00:59.070
And just to introduce
myself, I've

00:00:59.070 --> 00:01:01.960
been thinking about this
for the last 15 years.

00:01:01.960 --> 00:01:06.550
It's not a new thing that
suddenly has sprung up.

00:01:06.550 --> 00:01:11.160
And in terms of my
bio, I've really

00:01:11.160 --> 00:01:16.950
looked at supporting
investigators, company

00:01:16.950 --> 00:01:20.910
investigators, to spot fraud.

00:01:20.910 --> 00:01:26.190
I've also been involved in the
CALO system back in the States,

00:01:26.190 --> 00:01:29.070
and that was really
groundbreaking.

00:01:29.070 --> 00:01:32.100
It actually produced Siri.

00:01:32.100 --> 00:01:37.090
And Adam Cheyer has struck it
rich and I'm still in academia,

00:01:37.090 --> 00:01:41.220
but it came out of
that same project.

00:01:41.220 --> 00:01:45.120
And really a lot of
what I'll be covering

00:01:45.120 --> 00:01:48.420
comes out of an
NSF funding project

00:01:48.420 --> 00:01:52.860
that I was part of, which
was called End User Debugging

00:01:52.860 --> 00:01:55.350
of Machine Learned Programs.

00:01:55.350 --> 00:01:59.850
And we've done a lot of
work around intelligibility,

00:01:59.850 --> 00:02:03.900
around controllability, that
I will be talking about.

00:02:03.900 --> 00:02:07.530
Other than that,
I'm also looking

00:02:07.530 --> 00:02:10.259
at smart heating systems.

00:02:10.259 --> 00:02:13.070
And so I'll cover
that very briefly.

00:02:13.070 --> 00:02:18.510
I won't be talking about the
stuff I'm doing on a smart home

00:02:18.510 --> 00:02:21.580
project, but if you
have any questions,

00:02:21.580 --> 00:02:25.470
feel free to talk
to me about that.

00:02:25.470 --> 00:02:29.240
So launching into
the meat of this,

00:02:29.240 --> 00:02:32.190
what do I think the
issues are for AI

00:02:32.190 --> 00:02:33.825
that are coming from HCI?

00:02:33.825 --> 00:02:39.720
I think there are three
big aspects to what we

00:02:39.720 --> 00:02:41.400
have to try and solve.

00:02:41.400 --> 00:02:46.470
This is intelligibility, this is
controllability, and then also

00:02:46.470 --> 00:02:52.380
how these two affect or
impact user experience.

00:02:52.380 --> 00:02:57.690
And I think there is some work
out there that says, OK, they

00:02:57.690 --> 00:03:00.480
are somehow linked,
but we really

00:03:00.480 --> 00:03:03.720
don't understand quite
how they are linked

00:03:03.720 --> 00:03:06.010
and what to do about them.

00:03:06.010 --> 00:03:10.710
So the reason that
these are important,

00:03:10.710 --> 00:03:13.380
I think, and particularly
important for AI

00:03:13.380 --> 00:03:18.330
systems and smart
systems, is that currently

00:03:18.330 --> 00:03:22.590
all those systems, all this
AI, are like black boxes.

00:03:22.590 --> 00:03:25.320
They don't really
tell the user anything

00:03:25.320 --> 00:03:28.800
about how they work, how
they make their decisions,

00:03:28.800 --> 00:03:31.710
what data they're using.

00:03:31.710 --> 00:03:37.800
So what happens is that they
are like this bit of magic,

00:03:37.800 --> 00:03:40.490
a black art, to users.

00:03:40.490 --> 00:03:44.490
So they don't even
have the opportunity

00:03:44.490 --> 00:03:51.090
to build a good mental model
of how they actually work.

00:03:51.090 --> 00:03:56.260
And that has been the focus of
a lot of UX design previously.

00:03:56.260 --> 00:04:02.220
We want users to have a good
mental model so they understand

00:04:02.220 --> 00:04:06.570
how systems work
and, particularly,

00:04:06.570 --> 00:04:10.090
what to do if they don't do
what they want them to do,

00:04:10.090 --> 00:04:13.980
how to fix them when
something goes wrong.

00:04:13.980 --> 00:04:17.040
And even though AI
system, nowadays,

00:04:17.040 --> 00:04:23.000
have an incredible
accuracy, previous speakers

00:04:23.000 --> 00:04:23.750
have covered this.

00:04:23.750 --> 00:04:27.930
Well, we're now at sort
of 95% accuracy, right?

00:04:27.930 --> 00:04:32.490
But there's still 5%
of, well, it's wrong.

00:04:32.490 --> 00:04:36.210
What do you do to fix that?

00:04:36.210 --> 00:04:41.250
So what is becoming
increasingly important is, well,

00:04:41.250 --> 00:04:47.050
how do we address this
problem of intellibility?

00:04:47.050 --> 00:04:55.050
So the way that I use that
term is, well, intelligibility

00:04:55.050 --> 00:05:00.240
is all about making
a smart system

00:05:00.240 --> 00:05:06.480
understandable by an end-user,
or the user of the system.

00:05:06.480 --> 00:05:11.280
So some people have called
it explainability, sometimes

00:05:11.280 --> 00:05:13.170
even understandability.

00:05:13.170 --> 00:05:18.750
But you know, to me
it's much of the same.

00:05:18.750 --> 00:05:22.170
So what is
intelligibility all about?

00:05:22.170 --> 00:05:30.120
Well, it's making sure that
they know what the system does,

00:05:30.120 --> 00:05:37.420
why it's making those decisions
or displaying that behavior.

00:05:37.420 --> 00:05:39.900
And by making it
more understandable,

00:05:39.900 --> 00:05:44.190
by making it more intelligible,
hopefully, we also

00:05:44.190 --> 00:05:48.000
make it more trustable, and
perhaps even more enjoyable

00:05:48.000 --> 00:05:49.680
to use.

00:05:49.680 --> 00:05:54.850
So once we have that
understanding by the end-user,

00:05:54.850 --> 00:06:00.270
then they should also be able
to fix it if it goes wrong.

00:06:00.270 --> 00:06:07.800
Now, how do we get to
intelligible systems?

00:06:07.800 --> 00:06:13.080
Well, intelligibility
is really anything

00:06:13.080 --> 00:06:16.500
that helps end-users
to make, to build,

00:06:16.500 --> 00:06:21.570
these good and
appropriate mental models.

00:06:21.570 --> 00:06:28.750
And simply, it could be just
by interacting with the system.

00:06:28.750 --> 00:06:33.240
That's quite often how we do
any kind of interaction design.

00:06:33.240 --> 00:06:36.270
We just say, OK, you use
the system for a while

00:06:36.270 --> 00:06:42.780
and you discover
these features, right?

00:06:42.780 --> 00:06:47.490
And that really depends on
the system revealing itself

00:06:47.490 --> 00:06:51.970
over time, and that
might be perfectly fine.

00:06:51.970 --> 00:06:57.700
But in a complex system
that is supported by AI,

00:06:57.700 --> 00:07:01.200
that is supported by machine
learning, that is often not

00:07:01.200 --> 00:07:02.820
enough.

00:07:02.820 --> 00:07:06.390
And you need a little bit of
a boost to that understanding.

00:07:06.390 --> 00:07:09.750
And I would argue
that explanations

00:07:09.750 --> 00:07:12.840
have been used in
the past by AI system

00:07:12.840 --> 00:07:17.970
to get that to
achieve that, and this

00:07:17.970 --> 00:07:20.880
is what I'm going to be talking
about a little bit more--

00:07:20.880 --> 00:07:27.450
how to craft explanations
so you give intelligibility

00:07:27.450 --> 00:07:28.560
a little boost.

00:07:28.560 --> 00:07:32.400
Now, when I use the
term "explanations,"

00:07:32.400 --> 00:07:36.300
I include visualizations,
I include pretty pictures,

00:07:36.300 --> 00:07:38.320
graphs, and words.

00:07:38.320 --> 00:07:43.870
So it's not just some sort of
chat bot gives you the answer,

00:07:43.870 --> 00:07:45.690
right?

00:07:45.690 --> 00:07:51.570
So let me tell you a little
bit about the history of that.

00:07:51.570 --> 00:07:57.640
So explanations have been around
since the 1980s in the old AI

00:07:57.640 --> 00:08:03.930
systems, but the sort
of modern AI systems

00:08:03.930 --> 00:08:06.330
are much more complex,
like the old ones.

00:08:06.330 --> 00:08:08.610
And so you have
that problem, well,

00:08:08.610 --> 00:08:14.160
how do you explain the really
complex machine learning

00:08:14.160 --> 00:08:17.040
algorithms that are around?

00:08:17.040 --> 00:08:20.970
So what has been
shown is that why

00:08:20.970 --> 00:08:23.260
or why not explanations
work really,

00:08:23.260 --> 00:08:28.650
really well to explain
how these systems work.

00:08:28.650 --> 00:08:30.970
And there is some
work around that.

00:08:30.970 --> 00:08:34.260
And quite often it's
sort of a mixture

00:08:34.260 --> 00:08:36.510
of asking questions
of the system

00:08:36.510 --> 00:08:40.830
and displaying some kind
of visualization, very

00:08:40.830 --> 00:08:43.919
much like the sort
of facets system

00:08:43.919 --> 00:08:47.170
that we saw in the introduction.

00:08:47.170 --> 00:08:52.850
However, sometimes
explaining these systems

00:08:52.850 --> 00:08:55.500
in terms of why or why
not is really, really

00:08:55.500 --> 00:08:58.620
tricky, because what do
you actually explain?

00:08:58.620 --> 00:09:01.890
You could explain the data,
you could explain the input,

00:09:01.890 --> 00:09:05.670
you could explain the
model that is behind,

00:09:05.670 --> 00:09:07.410
you could explain
the output as well.

00:09:07.410 --> 00:09:11.880
And really, that is
still an open question.

00:09:11.880 --> 00:09:15.090
How do you do that best?

00:09:15.090 --> 00:09:19.350
There's some recent
work by Ribeiro

00:09:19.350 --> 00:09:21.750
on model agnostic
explanations that's

00:09:21.750 --> 00:09:25.220
sort of really storming
the field at the moment.

00:09:25.220 --> 00:09:28.380
And that is really quite
interesting to look at,

00:09:28.380 --> 00:09:33.810
where it tries to explain a
deep learning system in terms

00:09:33.810 --> 00:09:36.599
of the input and output.

00:09:36.599 --> 00:09:38.140
That's really
interesting to look at.

00:09:40.760 --> 00:09:42.770
What I've been
mainly involved in

00:09:42.770 --> 00:09:46.550
is an approach called
explanatory debugging.

00:09:46.550 --> 00:09:53.000
And what that combines
is intelligibility

00:09:53.000 --> 00:09:54.350
with controllability.

00:09:54.350 --> 00:09:58.160
So the intelligibility
is via sort

00:09:58.160 --> 00:10:00.200
of an explanation-centric
approach.

00:10:00.200 --> 00:10:06.260
We display an explanation of
what their system is doing.

00:10:06.260 --> 00:10:11.120
And then the controllability is
where we use these explanations

00:10:11.120 --> 00:10:18.590
as a feedback mechanism to help
debug or improve the system.

00:10:18.590 --> 00:10:23.840
So it's kind of like
human-in-the-loop learning via

00:10:23.840 --> 00:10:26.210
explanations.

00:10:26.210 --> 00:10:33.740
We've done some work on it,
and I'll show you a little bit

00:10:33.740 --> 00:10:38.120
later, but what we've
also started to do

00:10:38.120 --> 00:10:41.660
is, we thought about
the principles that

00:10:41.660 --> 00:10:45.830
underlie our approach to
explanatory of debugging.

00:10:45.830 --> 00:10:48.680
So we have some
general principles

00:10:48.680 --> 00:10:53.750
that we try and design
into any system, right?

00:10:53.750 --> 00:10:58.080
And they are principles that
speak to intelligibility.

00:10:58.080 --> 00:11:05.240
So we always are iterative
in our explanations,

00:11:05.240 --> 00:11:14.890
so the user can actually reveal
the explanation over time

00:11:14.890 --> 00:11:16.340
as well.

00:11:16.340 --> 00:11:18.260
We are always
trying to be sound.

00:11:18.260 --> 00:11:20.510
So we're not going
to lie and explain

00:11:20.510 --> 00:11:23.270
the system in different terms.

00:11:23.270 --> 00:11:26.450
We might simplify it,
but we're definitely

00:11:26.450 --> 00:11:30.500
sticking to what the
model actually does.

00:11:30.500 --> 00:11:33.150
We're trying to be complete.

00:11:33.150 --> 00:11:38.630
So we include as much
information as possible

00:11:38.630 --> 00:11:47.660
to explain how the system
works, so while, obviously, also

00:11:47.660 --> 00:11:51.020
not trying to
overwhelm the user.

00:11:51.020 --> 00:11:59.720
So we try and explain the
system in fairly detailed,

00:11:59.720 --> 00:12:03.080
but simplified terms.

00:12:03.080 --> 00:12:06.830
So that's on the
intelligibility side.

00:12:06.830 --> 00:12:11.210
And in terms of controllability,
what we're trying to do

00:12:11.210 --> 00:12:16.820
is that, all of the things
that we are explaining

00:12:16.820 --> 00:12:19.622
are actually actionable.

00:12:19.622 --> 00:12:21.080
And quite often
what we're doing is

00:12:21.080 --> 00:12:24.170
we're using visualization,
where interacting

00:12:24.170 --> 00:12:29.630
with these visualizations feeds
back training to the machine,

00:12:29.630 --> 00:12:32.030
to the machine learning system.

00:12:32.030 --> 00:12:38.370
We're always reversible,
because the user might be wrong

00:12:38.370 --> 00:12:41.210
or what they're doing
has unintended effects.

00:12:41.210 --> 00:12:44.720
So this is actually
kind of novel

00:12:44.720 --> 00:12:47.870
in machine learning
terms, not in human terms,

00:12:47.870 --> 00:12:52.390
but, you know, where the idea
of actually moving training

00:12:52.390 --> 00:12:56.490
is kind of unheard of.

00:12:56.490 --> 00:12:58.110
We always honor feedback.

00:12:58.110 --> 00:13:00.200
So if the user
tells us something,

00:13:00.200 --> 00:13:03.080
we do something about it.

00:13:03.080 --> 00:13:07.280
We don't just go, oh, you're
wrong, we chuck it away.

00:13:07.280 --> 00:13:09.890
And then incremental
changes matter.

00:13:09.890 --> 00:13:17.600
So with every kind of example,
with every bit of training data

00:13:17.600 --> 00:13:22.610
that we get, we actually
change the model.

00:13:22.610 --> 00:13:26.340
So we don't do sort
of that batch training

00:13:26.340 --> 00:13:28.550
that is quite often around.

00:13:28.550 --> 00:13:31.290
So how do we do that?

00:13:31.290 --> 00:13:33.130
So these are the
general principles.

00:13:33.130 --> 00:13:38.210
We published a paper in
IUI in 2015 about it.

00:13:38.210 --> 00:13:39.950
And here's an
example of a system

00:13:39.950 --> 00:13:42.650
that we actually
built to engender

00:13:42.650 --> 00:13:46.610
some of these principles.

00:13:46.610 --> 00:13:47.870
We called it EluciDebug.

00:13:47.870 --> 00:13:55.100
It was kind of a classification,
a text classifier.

00:13:55.100 --> 00:14:00.950
It's using the 20 Newsgroups
data set as an example.

00:14:00.950 --> 00:14:04.070
And if you just
look in the middle,

00:14:04.070 --> 00:14:09.560
it looks very much like
an email client, right?

00:14:09.560 --> 00:14:14.930
But what we then added was
all of these explanations

00:14:14.930 --> 00:14:18.450
of how that system works.

00:14:18.450 --> 00:14:22.190
So we have things in
there about, well,

00:14:22.190 --> 00:14:27.380
what is the current training
data that it's looking at?

00:14:27.380 --> 00:14:31.490
What is the current prediction
and the confidence level,

00:14:31.490 --> 00:14:34.070
or the accuracy level?

00:14:34.070 --> 00:14:40.790
What are the features that it
tends to look at right now?

00:14:40.790 --> 00:14:45.490
And if you look at this sort of
bar graph down at the bottom,

00:14:45.490 --> 00:14:52.630
this displays the weights on
those features, on those words.

00:14:52.630 --> 00:14:57.100
And the interesting thing is
that the user can actually

00:14:57.100 --> 00:15:03.430
pick up these bars
and move them around

00:15:03.430 --> 00:15:06.040
to give the features
more or less weight.

00:15:06.040 --> 00:15:10.210
So they are really,
really interactive.

00:15:10.210 --> 00:15:19.150
So each of these decisions
is iterative, it is sound,

00:15:19.150 --> 00:15:26.230
it is complete, and yet we
think it doesn't overwhelm.

00:15:26.230 --> 00:15:30.850
So we evaluated it.

00:15:30.850 --> 00:15:32.590
I'll go over that
very, very quickly.

00:15:32.590 --> 00:15:38.770
77 participants, they were
not all undergraduates,

00:15:38.770 --> 00:15:43.465
but they didn't have any clue
about machine learning systems.

00:15:46.060 --> 00:15:49.120
As I said, it was the
20 Newsgroups data set.

00:15:49.120 --> 00:15:55.990
They had 30 minutes to make the
system as accurate as possible.

00:15:55.990 --> 00:15:59.320
And we were only training
on five messages, right,

00:15:59.320 --> 00:16:02.450
which is something that
you usually don't do.

00:16:02.450 --> 00:16:05.800
You usually train on a
much larger sample of it.

00:16:05.800 --> 00:16:09.470
So we took various measures,
and here's what we found.

00:16:09.470 --> 00:16:13.450
Well, it actually works.

00:16:13.450 --> 00:16:18.640
They could make the
system much better

00:16:18.640 --> 00:16:24.610
in those 30 minutes than
without any explanations.

00:16:24.610 --> 00:16:28.300
They did that with
less feedback, right?

00:16:28.300 --> 00:16:31.660
So they had to make fewer
changes because they knew

00:16:31.660 --> 00:16:32.980
what they needed to change.

00:16:35.590 --> 00:16:38.930
And they actually had a
much better understanding.

00:16:38.930 --> 00:16:42.580
So compared to no
explanation, they

00:16:42.580 --> 00:16:46.780
had a much larger, or
better, mental model

00:16:46.780 --> 00:16:51.430
compared to people who
didn't have an explanation.

00:16:55.050 --> 00:16:57.900
However, here's what I believe.

00:16:57.900 --> 00:17:01.020
There are no cookie
cutter explanations.

00:17:01.020 --> 00:17:05.700
So this worked, particularly
for that example.

00:17:05.700 --> 00:17:11.710
And we really spent a long time
crafting those explanations

00:17:11.710 --> 00:17:14.369
and those visualizations,
but I'm not

00:17:14.369 --> 00:17:18.690
pretending that you could
take this exact thing

00:17:18.690 --> 00:17:21.000
and plunk it onto
a different system.

00:17:21.000 --> 00:17:22.150
It wouldn't work.

00:17:22.150 --> 00:17:27.569
Because what we have is
the systems are changing,

00:17:27.569 --> 00:17:30.480
the users are changing.

00:17:30.480 --> 00:17:33.630
So you have different
smart systems--

00:17:33.630 --> 00:17:36.990
SVMs, deep learning,
perhaps even something

00:17:36.990 --> 00:17:42.120
that isn't really an AI
system, just very complex.

00:17:42.120 --> 00:17:46.500
You might have it in a
different context, right?

00:17:46.500 --> 00:17:53.940
So we had a whole desktop
interface to play with, right?

00:17:53.940 --> 00:17:57.000
If you have something
like the Nest,

00:17:57.000 --> 00:18:00.810
you don't have that space for
all of these explanations,

00:18:00.810 --> 00:18:01.840
right?

00:18:01.840 --> 00:18:05.790
So you have different
pipes, width of pipes,

00:18:05.790 --> 00:18:09.790
for these interactions that
you actually need to consider.

00:18:09.790 --> 00:18:11.650
You have different
levels of risk.

00:18:11.650 --> 00:18:14.100
I think getting the
temperature wrong on the Nest

00:18:14.100 --> 00:18:18.150
is really, really different
to getting a wrong diagnosis

00:18:18.150 --> 00:18:21.660
for a cancer patient, right?

00:18:21.660 --> 00:18:26.250
These systems have also a
range of different users

00:18:26.250 --> 00:18:29.790
that you have to consider
in your UX design, right?

00:18:29.790 --> 00:18:35.220
The way that you design for an
end-user versus a domain expert

00:18:35.220 --> 00:18:39.510
versus perhaps somebody who's
actually a machine learning

00:18:39.510 --> 00:18:42.870
expert needs to be different.

00:18:42.870 --> 00:18:45.870
And as always, when
you're doing UX design,

00:18:45.870 --> 00:18:48.210
you need to be
aware of the effect

00:18:48.210 --> 00:18:51.490
that you want to achieve.

00:18:51.490 --> 00:18:55.080
So are you after trust,
satisfaction, perhaps

00:18:55.080 --> 00:18:59.790
actually maximizing the feedback
to get that system better?

00:18:59.790 --> 00:19:04.480
And all of these need different
kinds of explanations.

00:19:04.480 --> 00:19:09.370
So in conclusion, I
think we are all aware

00:19:09.370 --> 00:19:13.150
that explainable systems,
intelligible systems,

00:19:13.150 --> 00:19:18.160
and intelligibility in general
are really, really important

00:19:18.160 --> 00:19:21.520
for research, for
industry, for UX designers,

00:19:21.520 --> 00:19:24.220
for machine learning
experts, and also

00:19:24.220 --> 00:19:27.220
for users of these systems.

00:19:27.220 --> 00:19:30.250
What we really,
really need, I think,

00:19:30.250 --> 00:19:34.810
is an increased communication
or increased communication

00:19:34.810 --> 00:19:40.630
between AI researchers
and HCI researchers

00:19:40.630 --> 00:19:43.870
between designers and engineers.

00:19:43.870 --> 00:19:47.680
I think this problem
is really, really not

00:19:47.680 --> 00:19:51.070
solvable by one
discipline on its own,

00:19:51.070 --> 00:19:53.680
without talking to anybody else.

00:19:53.680 --> 00:19:58.240
So what I really would
want is that together we

00:19:58.240 --> 00:20:03.190
develop more knowledge
to actually apply

00:20:03.190 --> 00:20:07.000
what we're sort of finding
out in research, in practice.

00:20:07.000 --> 00:20:11.770
So we end up
building systems that

00:20:11.770 --> 00:20:15.160
are intelligible,
that are controllable,

00:20:15.160 --> 00:20:17.050
and that have great UX.

00:20:17.050 --> 00:20:19.750
And just at the end,
I have a little plug

00:20:19.750 --> 00:20:23.050
for a recent workshop
that I was co-organizing,

00:20:23.050 --> 00:20:27.560
which was a workshop on
explainable smart systems.

00:20:27.560 --> 00:20:31.232
And I'll leave you with
that and take questions.

00:20:31.232 --> 00:20:33.200
[APPLAUSE]

00:20:41.067 --> 00:20:42.400
SPEAKER 2: Questions for Simone?

00:20:46.475 --> 00:20:47.350
SPEAKER 3: Thank you.

00:20:47.350 --> 00:20:50.730
That was fascinating, and
this controllability thing

00:20:50.730 --> 00:20:53.675
seems key in terms of
something like Nest

00:20:53.675 --> 00:20:55.110
or an equivalent
of those things.

00:20:55.110 --> 00:20:58.500
If you look at, as you
said, the pipe on a Nest,

00:20:58.500 --> 00:21:03.090
you can't really
explain width directly.

00:21:03.090 --> 00:21:05.700
But did you look at sort of
outsourcing that learning

00:21:05.700 --> 00:21:07.770
or controllability
to something else,

00:21:07.770 --> 00:21:14.110
as in an app alongside Nest or
some other displaced moment?

00:21:14.110 --> 00:21:17.400
Or did you see that the learning
has to happen kind of in situ

00:21:17.400 --> 00:21:19.320
at the moment of the
core interaction,

00:21:19.320 --> 00:21:21.979
as in changing the temperature?

00:21:21.979 --> 00:21:24.020
SIMONE STUMPF: That's a
really interesting point,

00:21:24.020 --> 00:21:26.750
because particularly
with the project I'm

00:21:26.750 --> 00:21:29.180
doing on a smart
heating system, we

00:21:29.180 --> 00:21:34.780
have that problem that you
can't explain everything

00:21:34.780 --> 00:21:37.550
in that display.

00:21:37.550 --> 00:21:42.110
Explaining the current
decision is possible,

00:21:42.110 --> 00:21:47.990
I think, but anything that
needs to be more in-depth,

00:21:47.990 --> 00:21:51.070
that's perhaps on
the periphery, I

00:21:51.070 --> 00:21:54.470
think we're sort of
thinking of outsourcing

00:21:54.470 --> 00:21:55.490
to a different place.

00:21:55.490 --> 00:21:58.680
But you still need to know
that you need to explain it.

00:22:06.114 --> 00:22:07.530
SPEAKER 4: Simone,
thanks so much.

00:22:07.530 --> 00:22:11.980
I was curious what other
provocations you explored other

00:22:11.980 --> 00:22:13.800
than to try to
make the system as

00:22:13.800 --> 00:22:18.900
accurate as possible
to the participants.

00:22:18.900 --> 00:22:21.890
SIMONE STUMPF: For the
explanatory debugging,

00:22:21.890 --> 00:22:29.520
we focused mainly
on intelligibility

00:22:29.520 --> 00:22:38.290
and controllability, because our
hope was that we could actually

00:22:38.290 --> 00:22:43.990
make systems that draw on
the knowledge of users much

00:22:43.990 --> 00:22:46.270
better, much quicker, right?

00:22:46.270 --> 00:22:49.060
That was the whole
thing about it.

00:22:49.060 --> 00:22:54.190
And there are plenty of systems
that operate in that fashion.

00:22:54.190 --> 00:22:57.460
So anything that's to
do with personalization,

00:22:57.460 --> 00:23:03.760
or something where you don't
have a lot of training data,

00:23:03.760 --> 00:23:06.160
right, all of these
systems, I think,

00:23:06.160 --> 00:23:10.690
are ripe for explanatory
debugging approaches.

00:23:10.690 --> 00:23:15.070
I think this again, coming back
to the smart heating system,

00:23:15.070 --> 00:23:17.950
controllability is not it.

00:23:17.950 --> 00:23:23.990
In fact, we're going
against a sort of UX mantra.

00:23:23.990 --> 00:23:29.800
We don't want users
to feedback a lot,

00:23:29.800 --> 00:23:33.280
because these systems
are highly optimized

00:23:33.280 --> 00:23:35.650
and we want to stop
users from fiddling

00:23:35.650 --> 00:23:38.440
with these damn things, right?

00:23:38.440 --> 00:23:43.360
So there, we're actually
trying to use explanation

00:23:43.360 --> 00:23:52.210
to make them not fiddle with
the Nest or its equivalent.

00:23:52.210 --> 00:23:57.490
So I think it's sort
of different approaches

00:23:57.490 --> 00:24:00.145
that need to shift from
context to context.

00:24:03.252 --> 00:24:03.835
SPEAKER 5: Hi.

00:24:03.835 --> 00:24:05.620
Thank you for the
wonderful talk.

00:24:05.620 --> 00:24:09.795
In your research, or what
you've seen in literature,

00:24:09.795 --> 00:24:12.430
have you seen
examples, or studies

00:24:12.430 --> 00:24:15.460
done, for explainable
systems and how

00:24:15.460 --> 00:24:18.460
it might relate in a
user's environment,

00:24:18.460 --> 00:24:20.320
such as complex
environments where

00:24:20.320 --> 00:24:22.420
there might be a lot
of things going on,

00:24:22.420 --> 00:24:26.940
or they're multitasking,
or things like that?

00:24:26.940 --> 00:24:31.890
SIMONE STUMPF: Yes, [LAUGHS]
I think explainable AI is

00:24:31.890 --> 00:24:34.170
really, really hot right now.

00:24:34.170 --> 00:24:38.400
There's a lot of academic
research being done,

00:24:38.400 --> 00:24:44.550
and a lot of that is in
very complex environments,

00:24:44.550 --> 00:24:48.330
mainly because it's
driven by a huge research

00:24:48.330 --> 00:24:51.930
program in the United
States led by DARPA.

00:24:51.930 --> 00:25:02.370
So a lot of it is how would you
actually help human and machine

00:25:02.370 --> 00:25:08.740
teams to operate efficiently,
to operate at an optimal level.

00:25:08.740 --> 00:25:11.480
So there's a lot of
stuff currently going on.

00:25:15.667 --> 00:25:17.250
SPEAKER 6: My question
is specifically

00:25:17.250 --> 00:25:21.360
about the smart meter heating
systems and explaining

00:25:21.360 --> 00:25:22.940
decisions that are made.

00:25:22.940 --> 00:25:27.000
Have you yet come across
the privacy implications

00:25:27.000 --> 00:25:30.930
of explaining
decisions when objects

00:25:30.930 --> 00:25:33.600
that are powered by
these kind of systems

00:25:33.600 --> 00:25:36.270
have shared ownership?

00:25:36.270 --> 00:25:39.935
So how are you thinking
about that problem?

00:25:39.935 --> 00:25:42.060
Because it's something that
preys on my mind a lot.

00:25:44.564 --> 00:25:46.980
SIMONE STUMPF: That's actually
a really interesting point,

00:25:46.980 --> 00:25:53.070
because currently these
smart heating systems just

00:25:53.070 --> 00:25:56.370
display explanations,
even though you usually

00:25:56.370 --> 00:25:59.910
have a multiple
occupancy home, right?

00:25:59.910 --> 00:26:05.400
And so quite often,
the person who's

00:26:05.400 --> 00:26:10.140
in control, who
owns the thermostat,

00:26:10.140 --> 00:26:14.310
is the one that gets to
display first, right?

00:26:14.310 --> 00:26:19.110
And so I think that problem
hasn't been solved at all.

00:26:19.110 --> 00:26:24.060
I think that is ripe
for investigation.

00:26:24.060 --> 00:26:26.950
And it's a really,
really good point.

00:26:26.950 --> 00:26:29.550
I think also, you know,
what is coming, especially

00:26:29.550 --> 00:26:33.210
with the energy networks,
is that you don't even

00:26:33.210 --> 00:26:35.670
own your heat anymore.

00:26:35.670 --> 00:26:37.350
It's controlled by the network.

00:26:37.350 --> 00:26:39.960
They're going to be
switching the heat on and off

00:26:39.960 --> 00:26:41.130
whether you like it or not.

00:26:49.240 --> 00:26:50.730
SPEAKER 7: Hi, Simone, thanks.

00:26:50.730 --> 00:26:56.616
With the editable system,
what was the system

00:26:56.616 --> 00:26:58.240
called, the research
that you're doing?

00:26:58.240 --> 00:26:59.323
SIMONE STUMPF: EluciDebug.

00:26:59.323 --> 00:27:00.950
SPEAKER 7:
EluciDebugging system,

00:27:00.950 --> 00:27:03.380
I thought some of
the insights you

00:27:03.380 --> 00:27:05.402
had around getting
users to label data

00:27:05.402 --> 00:27:06.360
was really interesting.

00:27:06.360 --> 00:27:11.390
So was it in the conclusion
that by surfacing, visualizing

00:27:11.390 --> 00:27:15.050
what the smart system was doing,
the parameters, users were

00:27:15.050 --> 00:27:17.330
more incentivized to label?

00:27:17.330 --> 00:27:22.550
And I mean, I think
right now getting users

00:27:22.550 --> 00:27:24.770
to just label data as
they're progressing

00:27:24.770 --> 00:27:28.230
during the day is a massive
issue in machine learning.

00:27:28.230 --> 00:27:31.160
So could this kind of
visualization support

00:27:31.160 --> 00:27:33.077
in that aspect?

00:27:33.077 --> 00:27:34.160
SIMONE STUMPF: Definitely.

00:27:34.160 --> 00:27:38.150
I think there's partly
something about engagement.

00:27:38.150 --> 00:27:42.860
Even without explanations,
they were able to label data.

00:27:42.860 --> 00:27:47.750
So they could just stick
a label on a message.

00:27:47.750 --> 00:27:50.360
Without explanations,
they were actually

00:27:50.360 --> 00:27:59.096
able to make these very
pinpointed and targeted

00:27:59.096 --> 00:27:59.596
changes.

00:28:04.010 --> 00:28:06.680
But they also understood
more where they

00:28:06.680 --> 00:28:08.410
needed to make these changes.

00:28:08.410 --> 00:28:11.960
So it worked hand in hand, but
there's definitely something

00:28:11.960 --> 00:28:14.830
about engagement
in there as well.

00:28:14.830 --> 00:28:17.092
SPEAKER 2: We have
time for one more.

00:28:17.092 --> 00:28:18.050
Other side of the room.

00:28:18.050 --> 00:28:20.270
I was wondering if
we were neglecting,

00:28:20.270 --> 00:28:25.295
our bias was against the
far reaches over there?

00:28:25.295 --> 00:28:26.170
SPEAKER 8: Thank you.

00:28:26.170 --> 00:28:27.350
Great talk.

00:28:27.350 --> 00:28:29.590
A question, much of
the explainability,

00:28:29.590 --> 00:28:33.550
often we are trying to
get a secondary goal.

00:28:33.550 --> 00:28:39.020
Trust, debugging, debuggability,
control, fairness.

00:28:39.020 --> 00:28:41.270
We want to show that the
model is fair share and such.

00:28:41.270 --> 00:28:45.530
Explainability in complex models
is becoming harder and harder.

00:28:45.530 --> 00:28:48.370
I'm not up to date in
all the publications,

00:28:48.370 --> 00:28:50.440
but I know it's a
very complex problem.

00:28:50.440 --> 00:28:53.560
But many of these secondary
goals, which often is not we

00:28:53.560 --> 00:28:56.840
want, those are attractable.

00:28:56.840 --> 00:28:59.710
So when should we focus
on explainability,

00:28:59.710 --> 00:29:02.230
or instead focus on the
secondary goals which

00:29:02.230 --> 00:29:05.870
are more attractable?

00:29:05.870 --> 00:29:07.730
SIMONE STUMPF: When you
say secondary goals,

00:29:07.730 --> 00:29:08.900
you mean trust?

00:29:08.900 --> 00:29:09.880
SPEAKER 8: Trust.

00:29:09.880 --> 00:29:13.860
Yes, trust you can attack by
showing results right away.

00:29:13.860 --> 00:29:15.860
There are many things we
don't know how it work,

00:29:15.860 --> 00:29:17.640
maybe airplanes or
things like that,

00:29:17.640 --> 00:29:20.120
but we trust because
they work overall.

00:29:20.120 --> 00:29:21.326
SIMONE STUMPF: Well.

00:29:21.326 --> 00:29:21.847
[LAUGHS]

00:29:21.847 --> 00:29:23.180
SPEAKER 8: Or something that's--

00:29:23.180 --> 00:29:25.200
SIMONE STUMPF: So
I think, you know--

00:29:27.694 --> 00:29:29.360
SPEAKER 8: Fairness
can be also attacked

00:29:29.360 --> 00:29:33.380
without understanding the model.

00:29:33.380 --> 00:29:35.180
SIMONE STUMPF: Yes.

00:29:35.180 --> 00:29:41.750
However, I would argue that
intelligibility is really

00:29:41.750 --> 00:29:45.540
one of the core things
that we need to solve.

00:29:45.540 --> 00:29:48.740
You can achieve trust
in some kind of way,

00:29:48.740 --> 00:29:51.980
even by just having them
interact with the system

00:29:51.980 --> 00:29:54.800
repeatedly.

00:29:54.800 --> 00:29:59.030
However, the danger is that
if they don't understand

00:29:59.030 --> 00:30:01.700
how the system works and
they encounter something

00:30:01.700 --> 00:30:06.890
that they don't like, that
trust is eroded, right?

00:30:06.890 --> 00:30:10.520
And intelligibility, I
think, and explanations,

00:30:10.520 --> 00:30:12.560
in particular, can
help to give that

00:30:12.560 --> 00:30:17.420
a little bit of a boost
to actually cause somebody

00:30:17.420 --> 00:30:19.610
to adopt the system.

00:30:19.610 --> 00:30:23.120
And then that leads
to trust, right?

00:30:23.120 --> 00:30:26.730
Because if they don't adopt
it, if they kind of have

00:30:26.730 --> 00:30:28.730
a little look and go, oh,
I don't understand it,

00:30:28.730 --> 00:30:33.570
I don't like it, and stop using
it, that can be a problem.

00:30:33.570 --> 00:30:35.360
And I think
intelligibility is one

00:30:35.360 --> 00:30:36.812
of the cornerstones of that.

00:30:40.046 --> 00:30:42.370
SPEAKER 2: I think we're
going to switch over

00:30:42.370 --> 00:30:44.357
to-- thank you, Simone.

00:30:44.357 --> 00:30:44.857
[APPLAUSE]

00:30:44.857 --> 00:30:46.606
I think we're going
to switch over to Dan.

00:30:49.132 --> 00:30:51.960
SPEAKER 1: The next
speaker is Dan Hill.

00:30:51.960 --> 00:30:56.330
Dan is a designer,
urbanist, author.

00:30:56.330 --> 00:30:58.310
Currently he's
Associate Director

00:30:58.310 --> 00:31:02.300
of the Arup Group, which is
a strategy interdisciplinary

00:31:02.300 --> 00:31:08.680
service, also working
as advisors to some

00:31:08.680 --> 00:31:11.970
of the innovation issues
of the UK government.

00:31:11.970 --> 00:31:13.637
Thank you.

00:31:13.637 --> 00:31:14.470
DAN HILL: Thank you.

00:31:17.760 --> 00:31:20.199
That's [INAUDIBLE]
Housing Co-Op, which

00:31:20.199 --> 00:31:21.490
is around the corner from here.

00:31:21.490 --> 00:31:22.885
I urge everybody to go to it.

00:31:22.885 --> 00:31:25.600
I'll explain why later.

00:31:25.600 --> 00:31:28.390
That's where I was
yesterday afternoon.

00:31:28.390 --> 00:31:29.890
So I'm a designer,
as you could see,

00:31:29.890 --> 00:31:31.405
because I left my
slides up there,

00:31:31.405 --> 00:31:32.863
and they were
exposed to condition.

00:31:32.863 --> 00:31:34.800
You could see, well,
actually it's--

