WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:02.370
A third alternative is what is called a big

00:00:02.370 --> 00:00:05.364
flip. In the big flip, what we are doing

00:00:05.364 --> 00:00:07.740
is we are bringing down half the nodes at

00:00:07.740 --> 00:00:11.470
once. So in other words, the service is available

00:00:11.470 --> 00:00:17.140
at 50% of the full capacity. So you can see that with a big flip, half the nodes

00:00:17.140 --> 00:00:20.840
are down. So we have reduced the server capacity,

00:00:20.840 --> 00:00:25.400
the total DQ available by 50% for u units

00:00:25.400 --> 00:00:29.414
of time. And the total upgrade is going to

00:00:29.414 --> 00:00:32.680
last 2 times u because we have partitioned the

00:00:32.680 --> 00:00:35.900
entire server capacity in two halves, and upgrading one

00:00:35.900 --> 00:00:39.040
half, upgrading the second half for 2 times u

00:00:39.040 --> 00:00:42.851
time units we're not going to have full capacity. And

00:00:42.851 --> 00:00:45.898
during this 2 times U time unit we get

00:00:45.898 --> 00:00:49.180
the service at 50% capacity, and at the end

00:00:49.180 --> 00:00:50.860
of this it is back up to full capacity.

00:00:50.860 --> 00:00:53.180
So these are the three different choices that

00:00:53.180 --> 00:00:56.170
you've got, the fast reboot where the upgrade

00:00:56.170 --> 00:01:00.280
time for the entire service is just equal

00:01:00.280 --> 00:01:03.430
to the upgrade time for a single node. But

00:01:03.430 --> 00:01:07.150
here, we don't have the service available for

00:01:07.150 --> 00:01:09.260
any of the users for that period of

00:01:09.260 --> 00:01:11.350
time, and this might be a, a good

00:01:11.350 --> 00:01:16.400
strategy for services that can exploit the diurnal property

00:01:16.400 --> 00:01:20.180
of user community. Rolling upgrade is the more common

00:01:20.180 --> 00:01:24.570
one, where what we are doing is we are upgrading

00:01:24.570 --> 00:01:27.194
the servers one at a time but it's going

00:01:27.194 --> 00:01:29.490
to take a long time especially when you're talking about

00:01:29.490 --> 00:01:32.830
data centers having thousands and thousands of processors. Rolling

00:01:32.830 --> 00:01:35.470
upgrade is going to take a long time and it

00:01:35.470 --> 00:01:38.170
might be done in batches for instance instead of being

00:01:38.170 --> 00:01:42.110
exactly one at a time. And the extreme of that

00:01:42.110 --> 00:01:44.735
rolling upgrade is this big flip where we are

00:01:44.735 --> 00:01:47.690
saying we'll bring down 50% of the nodes, upgrade

00:01:47.690 --> 00:01:50.650
them, and then turn them back on, and then

00:01:50.650 --> 00:01:53.570
do the upgrade for the remaining 50%. So in the

00:01:53.570 --> 00:01:56.750
third case, in the big flip, the service is

00:01:56.750 --> 00:02:01.280
always available, but at 50% capacity for a certain duration

00:02:01.280 --> 00:02:03.140
of time. But there is no way to hide

00:02:03.140 --> 00:02:06.970
the DQ loss. And you'll see that the DQ loss,

00:02:06.970 --> 00:02:09.830
which is shown by the area of this

00:02:09.830 --> 00:02:12.990
shaded rectangle in each one of these cases, the

00:02:12.990 --> 00:02:15.860
area in the shaded rectangle is exactly the

00:02:15.860 --> 00:02:19.220
same. In other words, the DQ loss is the

00:02:19.220 --> 00:02:22.070
same for all three strategies, and it is

00:02:22.070 --> 00:02:25.240
the DQ loss of an individual node, the upgrade

00:02:25.240 --> 00:02:28.050
time for an individual node, and the number of

00:02:28.050 --> 00:02:32.300
servers, n, that you have in your server farm.

00:02:32.300 --> 00:02:36.450
That's the total DQ loss for online evolution, and

00:02:36.450 --> 00:02:40.240
all that these different strategies are saying is, as a

00:02:40.240 --> 00:02:43.072
system administrator, you have a choice on how you want

00:02:43.072 --> 00:02:46.740
to dish out the DQ loss. And make it apparent

00:02:46.740 --> 00:02:48.950
and not apparent to the user community. In this

00:02:48.950 --> 00:02:51.420
case, it's going to be apparent to the entire user

00:02:51.420 --> 00:02:54.980
community that there is upgrade going on. In this case,

00:02:54.980 --> 00:02:57.910
different segments of the user community are going to notice

00:02:57.910 --> 00:03:00.200
that the service has become unavailable for a

00:03:00.200 --> 00:03:02.980
short duration of time. And here it's sort of

00:03:02.980 --> 00:03:05.900
in between these two extremes and that half

00:03:05.900 --> 00:03:08.800
the user community may see a service being unavailable

00:03:08.800 --> 00:03:10.340
for some amount of time. So, in other

00:03:10.340 --> 00:03:15.650
words, using the DQ principle, maintenance and upgrades are

00:03:15.650 --> 00:03:20.780
controlled failures that can be handled by the administrator

00:03:20.780 --> 00:03:23.670
of a system service. So what we are seeing

00:03:23.670 --> 00:03:28.640
through this concept of DQ is a way by which a system

00:03:28.640 --> 00:03:33.480
developer and a system administrator can work together in figuring out how

00:03:33.480 --> 00:03:38.430
to architect the system in terms of how the data should be

00:03:38.430 --> 00:03:43.295
partitioned or replicated and how the system should

00:03:43.295 --> 00:03:48.770
fine-tune the service by looking at the instantaneous offered

00:03:48.770 --> 00:03:55.230
load and tuning whether to keep the yield the same or the harvest the same,

00:03:55.230 --> 00:03:57.500
knowing that DQ is a constant and the

00:03:57.500 --> 00:04:01.540
server is getting saturated. And finally, when service

00:04:01.540 --> 00:04:04.630
has to evolve once again, the system administrator

00:04:04.630 --> 00:04:07.260
can make an informed decision on how to

00:04:07.260 --> 00:04:11.080
do this online evolution by controlling the DQ

00:04:11.080 --> 00:04:14.450
loss that is experienced by the user community

00:04:14.450 --> 00:04:15.350
at any point of time.

