WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.410
[MUSIC PLAYING]

00:00:09.517 --> 00:00:11.350
ANDRE SUSANO PINTO:
Hello, my name is Andre,

00:00:11.350 --> 00:00:13.180
and I'm a software
engineer in Zurich.

00:00:13.180 --> 00:00:15.685
I'm the technical lead for
the TensorFlow Hub project.

00:00:15.685 --> 00:00:18.760
TensorFlow Hub is a
library and a repository

00:00:18.760 --> 00:00:21.150
for reusing machine learning.

00:00:21.150 --> 00:00:23.430
We open-sourced it last
year, and today I'll

00:00:23.430 --> 00:00:26.750
give you an update
on the project.

00:00:26.750 --> 00:00:31.478
Let's start by talking about
when you'd want to use it.

00:00:31.478 --> 00:00:33.270
If you have problems
collecting enough data

00:00:33.270 --> 00:00:35.080
to train your
models from scratch,

00:00:35.080 --> 00:00:37.860
then transfer learning
is a technique for you.

00:00:37.860 --> 00:00:41.450
You can use Hub to make it easy
to reuse parts of models that

00:00:41.450 --> 00:00:43.140
were trained on large parts--

00:00:43.140 --> 00:00:45.230
on large amounts of data.

00:00:45.230 --> 00:00:48.100
Additionally, since it's so
easy to reuse computations

00:00:48.100 --> 00:00:51.400
and weights, it becomes
possible to leverage features

00:00:51.400 --> 00:00:54.800
without having to learn how to
fit them into neural networks.

00:00:54.800 --> 00:00:58.330
So images, text, and
videos are features

00:00:58.330 --> 00:01:01.540
you can use with a
single line of code.

00:01:01.540 --> 00:01:03.640
You might also have
encountered problems

00:01:03.640 --> 00:01:06.880
where code bases
become really coupled

00:01:06.880 --> 00:01:10.040
and experimentation
becomes slower over time.

00:01:10.040 --> 00:01:13.100
By defining an artifact that
does not depend on code,

00:01:13.100 --> 00:01:15.470
Hub allows for more
maintainable systems,

00:01:15.470 --> 00:01:19.960
similar to how libraries have
helped software engineer.

00:01:19.960 --> 00:01:23.620
So the main concept is these
pre-trained building blocks

00:01:23.620 --> 00:01:25.850
that we call modules.

00:01:25.850 --> 00:01:29.210
Typically, we start by
training a large model

00:01:29.210 --> 00:01:30.890
with the right
algorithm and data.

00:01:30.890 --> 00:01:32.510
From this large
model, we're just

00:01:32.510 --> 00:01:35.400
interested then into a part
of it, which is reusable.

00:01:35.400 --> 00:01:38.540
This is typically a bottleneck
layer or some other distributed

00:01:38.540 --> 00:01:41.260
representation.

00:01:41.260 --> 00:01:43.750
Then we can package them
into a SavedModel that

00:01:43.750 --> 00:01:46.420
defines this computation and
the weights that were trained,

00:01:46.420 --> 00:01:48.813
and no longer depends
on the original code.

00:01:48.813 --> 00:01:50.480
You can share this
artifact via the file

00:01:50.480 --> 00:01:54.480
system, web servers, cloud.

00:01:54.480 --> 00:01:58.260
Then you can bring this module
back as a piece of a new model

00:01:58.260 --> 00:02:00.670
to solve a new task.

00:02:00.670 --> 00:02:03.640
Since the model is defined
by TensorFlow primitives,

00:02:03.640 --> 00:02:05.920
you can fine tune its
weights and adjust them

00:02:05.920 --> 00:02:07.430
to your problem.

00:02:10.389 --> 00:02:11.950
So what's new?

00:02:11.950 --> 00:02:14.110
For the last few
months, we've been

00:02:14.110 --> 00:02:16.480
making it even easier to use.

00:02:16.480 --> 00:02:21.190
This concept of saving a part
of a model and then loading it

00:02:21.190 --> 00:02:25.450
back is getting integrated
right in the core of TensorFlow.

00:02:25.450 --> 00:02:27.700
We have added
SavedModel features

00:02:27.700 --> 00:02:31.540
that make it possible to share
more than just signatures.

00:02:31.540 --> 00:02:33.790
And with eager execution,
it becomes even easier

00:02:33.790 --> 00:02:37.710
to select the part of a
network that gets exported.

00:02:37.710 --> 00:02:40.395
Let's look at a few examples.

00:02:40.395 --> 00:02:45.970
In TensorFlow 2.0, we can
load a module with hub.load.

00:02:45.970 --> 00:02:48.490
We can also use
tf.savedmodel.load

00:02:48.490 --> 00:02:51.880
if the model is already
on our file system.

00:02:51.880 --> 00:02:54.490
Due to eager
execution, once loaded,

00:02:54.490 --> 00:02:57.570
we can call it right away.

00:02:57.570 --> 00:02:59.930
Additionally, due to
the new capabilities,

00:02:59.930 --> 00:03:02.210
you can now share any
object which is composed

00:03:02.210 --> 00:03:03.750
of TensorFlow primitives.

00:03:03.750 --> 00:03:08.270
So in this case, text_features
has two members--

00:03:08.270 --> 00:03:10.980
__call__, which is a TF
function, and embeddings,

00:03:10.980 --> 00:03:11.960
which is a TF variable.

00:03:14.720 --> 00:03:17.450
We're also really excited
that we added support

00:03:17.450 --> 00:03:20.480
for polymorphic functions
when we serialize TF functions

00:03:20.480 --> 00:03:21.530
on SavedModel.

00:03:21.530 --> 00:03:24.320
This will provide a
more natural interface

00:03:24.320 --> 00:03:26.750
than we had before
with signatures.

00:03:26.750 --> 00:03:30.170
For example, here we see an
image representation module

00:03:30.170 --> 00:03:34.100
being loaded and then being
used in inference mode,

00:03:34.100 --> 00:03:38.120
or being used during training
mode where batch norm is on.

00:03:38.120 --> 00:03:40.010
Or when it's on,
we can also control

00:03:40.010 --> 00:03:41.890
its-- some of its parameters.

00:03:41.890 --> 00:03:45.200
And all of this is just
baked by TF graphs,

00:03:45.200 --> 00:03:49.960
so we no longer need
to be selecting things

00:03:49.960 --> 00:03:51.190
here and there.

00:03:51.190 --> 00:03:54.230
We just get all the
API that looks very--

00:03:54.230 --> 00:03:57.230
very, like, patterned.

00:03:57.230 --> 00:03:59.370
Additionally, we have
added a new symbol--

00:03:59.370 --> 00:04:01.310
hub.KerasLayer.

00:04:01.310 --> 00:04:04.310
This makes integrating hub
modules into a Keras model

00:04:04.310 --> 00:04:05.150
easier.

00:04:05.150 --> 00:04:11.410
In this example, we see how it
is to build a text sentence--

00:04:11.410 --> 00:04:13.520
a sentence classification model.

00:04:13.520 --> 00:04:15.350
So we have three layers.

00:04:15.350 --> 00:04:19.300
The top layer-- this Keras
layer, which is the NNLM--

00:04:19.300 --> 00:04:22.150
is a layer that receives
sentences as inputs

00:04:22.150 --> 00:04:24.460
and outputs a Dense
representation.

00:04:24.460 --> 00:04:29.160
Then we have a dense layer
and a classification layer.

00:04:29.160 --> 00:04:31.860
Since the Keras layer--

00:04:31.860 --> 00:04:35.850
this NNLM layer-- includes
text preprocessing on it,

00:04:35.850 --> 00:04:38.780
we can just feed sentences
straight into our model.

00:04:38.780 --> 00:04:41.350
We never had to define
the logic for it.

00:04:41.350 --> 00:04:43.740
Additionally, if we wanted
to try other text models,

00:04:43.740 --> 00:04:45.650
we could just
change that string.

00:04:45.650 --> 00:04:47.775
They'll be as easy to try
like the latest research.

00:04:51.560 --> 00:04:54.590
So the status is we
have released a 0.3

00:04:54.590 --> 00:04:56.240
version of TensorFlow Hub.

00:04:56.240 --> 00:04:58.250
It has these two new
symbols we just saw--

00:04:58.250 --> 00:05:01.540
hub.load and hub.KerasLayer.

00:05:01.540 --> 00:05:03.820
And they are usable
in TensorFlow 2, both

00:05:03.820 --> 00:05:06.510
in eager and graph mode.

00:05:06.510 --> 00:05:08.190
To let you preview
this functionality,

00:05:08.190 --> 00:05:11.070
we have published some
modules in this format.

00:05:11.070 --> 00:05:13.350
And we-- the next
steps for us is

00:05:13.350 --> 00:05:14.790
to backport existing modules.

00:05:17.880 --> 00:05:20.160
A bit more practical now--

00:05:20.160 --> 00:05:22.440
Google AI and
DeepMind teams have

00:05:22.440 --> 00:05:25.410
been sharing their resource
with you on tfhub.dev.

00:05:25.410 --> 00:05:27.330
This was already
launched last year,

00:05:27.330 --> 00:05:28.870
and there are some new modules.

00:05:28.870 --> 00:05:32.320
And we're going to have
a look at some of those.

00:05:32.320 --> 00:05:36.010
One of the most popular was
the universal sentence encoder.

00:05:36.010 --> 00:05:39.160
This was a module that
encoded short sentences

00:05:39.160 --> 00:05:41.080
into a dimensional
vector that could

00:05:41.080 --> 00:05:43.930
be used for many
natural language tasks.

00:05:43.930 --> 00:05:47.970
Recently, the team has added a
cross lingual version of this.

00:05:47.970 --> 00:05:50.050
So sentence with
similar meaning,

00:05:50.050 --> 00:05:51.850
independent of the
language-- they'll

00:05:51.850 --> 00:05:54.040
end up in points close together.

00:05:54.040 --> 00:05:55.870
What's exciting about
this is that now you

00:05:55.870 --> 00:05:58.560
can learn a classifier
using English data,

00:05:58.560 --> 00:06:02.690
and then you can run
it on other languages.

00:06:02.690 --> 00:06:06.250
We have also added image
augmentation modules.

00:06:06.250 --> 00:06:09.740
The policies to augment images
were trained by reinforcement

00:06:09.740 --> 00:06:11.770
learning on tasks
such as ImageNet,

00:06:11.770 --> 00:06:15.040
and they have been shown
to transfer to new tasks.

00:06:15.040 --> 00:06:17.920
An interesting thing is that
you could grab this module,

00:06:17.920 --> 00:06:21.190
and you could grab one of the
image representation modules,

00:06:21.190 --> 00:06:23.148
and you could string
them together.

00:06:23.148 --> 00:06:24.940
In this case, the image
augmentation module

00:06:24.940 --> 00:06:27.280
would reduce the amount of
data by data augmentation

00:06:27.280 --> 00:06:32.970
and the image feature
vector by transfer learning.

00:06:32.970 --> 00:06:34.430
And there are many more.

00:06:34.430 --> 00:06:37.030
We have BERT module
for text tasks.

00:06:37.030 --> 00:06:39.310
We have object
detection modules,

00:06:39.310 --> 00:06:42.750
BigGAN modules for
controlled image generation,

00:06:42.750 --> 00:06:47.480
I3D kinetics for video
action recognition, and more.

00:06:47.480 --> 00:06:49.010
Some of these
models' architectures

00:06:49.010 --> 00:06:52.913
were specially designed
for low resources.

00:06:52.913 --> 00:06:54.330
Additionally, we
have been working

00:06:54.330 --> 00:06:57.330
making modules more integrated
with other pieces of TensorFlow

00:06:57.330 --> 00:06:59.490
ecosystem.

00:06:59.490 --> 00:07:02.270
We have a command line
utility to convert Hub module

00:07:02.270 --> 00:07:05.080
into a TFJS model.

00:07:05.080 --> 00:07:07.540
Hub models can be used
together with AdaNet,

00:07:07.540 --> 00:07:10.120
which is a library for AutoML.

00:07:10.120 --> 00:07:14.310
And they can also be
used inside TF transform.

00:07:14.310 --> 00:07:18.540
So if you want to try it,
you can go to tfhub.dev

00:07:18.540 --> 00:07:21.060
and you can search for modules.

00:07:21.060 --> 00:07:22.560
Most of them include
the link where

00:07:22.560 --> 00:07:25.310
you can see them in action.

00:07:25.310 --> 00:07:26.020
Thank you.

00:07:26.020 --> 00:07:28.770
[MUSIC PLAYING]

