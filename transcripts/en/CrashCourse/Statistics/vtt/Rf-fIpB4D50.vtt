WEBVTT
Kind: captions
Language: en

00:00:03.240 --> 00:00:06.000
Hi, I’m Adriene Hill and welcome back to
Crash Course Statistics.

00:00:06.010 --> 00:00:10.850
In our last episode we talked about how we
use experiments to imitate having two parallel

00:00:10.850 --> 00:00:12.690
universes to test things.

00:00:12.690 --> 00:00:16.970
But sometimes you can’t do certain experiments
without becoming an all-powerful and evil

00:00:16.970 --> 00:00:21.600
dictator, and since it’s statistically unlikely
that any of you are evil dictators, today,

00:00:21.600 --> 00:00:23.060
we’ll explore those methods.

00:00:23.060 --> 00:00:26.840
Like we mentioned at the beginning of the
series, you’re not always able to answer

00:00:26.850 --> 00:00:30.260
the questions you really want to answer using
statistics.

00:00:30.279 --> 00:00:34.820
For example, it would be great to experimentally
test whether getting married increases your

00:00:34.820 --> 00:00:39.710
lifespan, but you can’t randomly assign
some people to be married and force another

00:00:39.710 --> 00:00:41.050
group to be single.

00:00:41.050 --> 00:00:45.261
Not only would that be difficult to enforce,
it would also be pretty unethical, though

00:00:45.261 --> 00:00:49.310
I suppose you being evil takes care of that
particular concern.

00:00:49.310 --> 00:00:53.059
Similarly we can’t assign someone to be
a twin, or a Democrat, or a smoker.

00:00:53.059 --> 00:00:57.440
But that doesn’t mean we should just give
up and stop trying to find out more about

00:00:57.440 --> 00:00:58.440
these topics.

00:00:58.440 --> 00:00:59.380
Not at all.

00:00:59.380 --> 00:01:02.180
Instead we just need a different method to
collect data.

00:01:02.180 --> 00:01:04.660
Enter Non-Experimental methods.

00:01:04.660 --> 00:01:14.960
INTRO

00:01:14.969 --> 00:01:18.590
One of the most common non-experimental method
is the survey.

00:01:18.590 --> 00:01:22.979
From user experience surveys on websites,
to political polls, to health questionnaires

00:01:22.979 --> 00:01:26.849
at the doctor’s office, you’ve probably
taken hundreds of surveys in your lifetime.

00:01:26.849 --> 00:01:31.329
There are two things that can make or break a survey: the questions, and who the researcher

00:01:31.329 --> 00:01:32.790
gives the questions to.

00:01:32.790 --> 00:01:35.990
The goal of a survey is to get specific information.

00:01:35.990 --> 00:01:39.389
Say you’re walking your dog in a local park,
and someone approaches you and asks you to

00:01:39.389 --> 00:01:42.560
take a survey on local businesses in your
town.

00:01:42.560 --> 00:01:46.489
When you look at the questions you notice
that none of them are about local businesses,

00:01:46.489 --> 00:01:50.420
instead you find yourself answering questions
about your politics and religious beliefs.

00:01:50.420 --> 00:01:54.980
Unless the surveyor was lying to you about
their purposes, this is not a very good survey….It’s

00:01:54.980 --> 00:01:58.420
also not a very good lie
A survey should measure what it claims to

00:01:58.420 --> 00:01:59.420
measure.

00:01:59.429 --> 00:02:04.509
It might seem obvious that having only unrelated
questions on your survey is problematic, and

00:02:04.509 --> 00:02:07.649
there are even more subtle ways a question
can be biased.

00:02:07.649 --> 00:02:11.980
Let’s take a look at a few questions from
a health survey you might take at a doctor’s office.

00:02:11.980 --> 00:02:16.060
The first question asks you how often you
exercise: never, less than 30 minutes a week

00:02:16.060 --> 00:02:17.360
or 30 minutes a day.

00:02:17.370 --> 00:02:20.459
So what do you answer if you exercise for
half an hour twice a week?

00:02:20.460 --> 00:02:23.829
Or if you’re on swim team and exercise for
at least an hour a day?

00:02:23.829 --> 00:02:26.440
And does walking count as exercise?

00:02:26.440 --> 00:02:30.290
Multiple choice questions that don’t offer
all possible options and/or an “Other”

00:02:30.290 --> 00:02:34.510
option can cause respondents to either skip
the question, or feel forced to choose an

00:02:34.510 --> 00:02:36.349
answer that isn’t accurate.

00:02:36.349 --> 00:02:40.310
Claims made using these questions aren’t
as strong as they could be if people were

00:02:40.310 --> 00:02:42.549
offered a full range of choices.

00:02:42.549 --> 00:02:47.439
The next question asks you “Answer yes or
no: I don’t smoke because I know it’s

00:02:47.439 --> 00:02:51.430
damaging to my health” this is a leading
question since the wording leads to towards

00:02:51.430 --> 00:02:53.530
the quote “desired” answer.

00:02:53.530 --> 00:02:58.599
This is especially effective when a question
deals with sensitive issues like smoking,

00:02:58.599 --> 00:03:00.069
politics, or religion.

00:03:00.069 --> 00:03:03.019
People answering the questions want to be
seen in a positive light, and so they tend

00:03:03.019 --> 00:03:05.620
to give the answer they think is “appropriate”.

00:03:05.620 --> 00:03:10.359
While having people fill surveys out anonymously
by themselves can help, it can sometimes be

00:03:10.359 --> 00:03:14.470
the case that respondents don’t want to
admit things--even to themselves--that are

00:03:14.470 --> 00:03:15.980
socially undesirable.

00:03:15.980 --> 00:03:20.090
In general terms, good survey questions are
worded in a neutral way such as asking “how

00:03:20.090 --> 00:03:25.110
often do you exercise” or “describe your
smoking habits” instead of using wording

00:03:25.110 --> 00:03:28.639
or options that push survey takers in a certain
direction.

00:03:28.639 --> 00:03:33.639
And while your doctor wouldn’t...or shouldn’t...do
this...sometimes groups purposely use biased

00:03:33.639 --> 00:03:36.939
questions in their surveys to get the results
that they want.

00:03:36.939 --> 00:03:43.269
Apparently, back in 1972, Virginia Slims conducted
a poll asking respondents if they would agree

00:03:43.269 --> 00:03:48.370
with the statement: “There won’t be a
woman President of the United States for a

00:03:48.370 --> 00:03:51.780
long time and that’s probably just as well.”

00:03:51.780 --> 00:03:53.240
Not a well-written question.

00:03:53.240 --> 00:03:56.900
Biased questions can be more subtle...and
can lead to skewed reports of very serious

00:03:56.900 --> 00:03:59.660
things like sexual assault, or mental health
conditions.

00:03:59.660 --> 00:04:04.060
It’s important to always look for biased
questions in surveys, especially when the

00:04:04.060 --> 00:04:07.780
people giving the survey stand to benefit
from a certain response.

00:04:07.780 --> 00:04:12.280
Even when researchers have created a non-biased
survey, they still need to get it into the

00:04:12.280 --> 00:04:13.379
right hands.

00:04:13.379 --> 00:04:17.880
Ideally, a survey should go to a random sample
of the population that they’re interested

00:04:17.880 --> 00:04:18.380
in.

00:04:18.380 --> 00:04:22.000
Usually this means using a random number generator
to pick who gets the survey.

00:04:22.009 --> 00:04:27.340
We do Simple Random Sampling so that there’s
no pattern or system for selecting respondents

00:04:27.340 --> 00:04:30.050
and each respondent has an equal chance of
being selected.

00:04:30.050 --> 00:04:34.740
For example, telephone surveys often use Random
Digit Dialing which selects 7 random digits

00:04:34.740 --> 00:04:36.030
and dials them.

00:04:36.030 --> 00:04:38.120
When someone picks up, they’re asked to
take a survey.

00:04:38.120 --> 00:04:39.990
But here’s where we hit our first issue.

00:04:39.990 --> 00:04:43.729
If people aren’t forced to respond to the
survey, we might experience something called

00:04:43.729 --> 00:04:49.569
Non-Response Bias in which the people who
are most likely to complete a survey are systematically

00:04:49.569 --> 00:04:51.240
different from those who don’t.

00:04:51.240 --> 00:04:55.960
For example, people with non-traditional working
schedules like retirees, stay at home parents,

00:04:55.960 --> 00:05:00.470
or people who work from home might be more
likely to answer a middle-of-the-day phone

00:05:00.470 --> 00:05:01.470
survey.

00:05:01.470 --> 00:05:05.340
This is a huge problem if those groups are
different than the population as a whole.

00:05:05.340 --> 00:05:09.121
If your survey was on health insurance plans,
or political opinions, it’s likely that

00:05:09.121 --> 00:05:13.870
these three groups would have different opinions
than the population, but they represent the

00:05:13.870 --> 00:05:19.300
majority of survey responses, which means
your data won’t represent the total population

00:05:19.300 --> 00:05:20.300
very well.

00:05:20.300 --> 00:05:24.360
This is also related to Voluntary Response
Bias in which people who choose to respond

00:05:24.360 --> 00:05:30.009
to voluntary surveys they see on Facebook...or
Twitter... are people who again, are different

00:05:30.009 --> 00:05:31.479
than the broad population.

00:05:31.479 --> 00:05:34.980
This is especially true with things like customer
service surveys.

00:05:34.980 --> 00:05:39.840
People who respond tend to have either very
positive or very negative opinions.

00:05:39.840 --> 00:05:41.400
See the comment section below.

00:05:41.400 --> 00:05:46.780
The majority of customers with an average
experience tend not to respond because service

00:05:46.780 --> 00:05:47.780
wasn’t noteworthy.

00:05:47.780 --> 00:05:48.780
Wait.

00:05:48.780 --> 00:05:50.300
Does that mean I’m not noteworthy?

00:05:50.300 --> 00:05:53.980
Another source of bias is just plain underrepresentation.

00:05:53.980 --> 00:05:58.850
If a group of interest is a minority in the
population, random sampling paired with response

00:05:58.850 --> 00:06:03.580
biases might mean that that minority isn’t
represented at all in the sample.

00:06:03.580 --> 00:06:08.190
Let's say there is a city where 5% of the population
is single mothers, it’s entirely possible

00:06:08.190 --> 00:06:11.160
that the sample will contain no single moms.

00:06:11.160 --> 00:06:13.660
To overcome these issues, we have a couple
options.

00:06:13.660 --> 00:06:17.479
We could weight people’s responses so that
they match the population (like, counting

00:06:17.480 --> 00:06:24.320
the few single mothers who do respond multiple times so that they count for 5% of the total sample).

00:06:24.360 --> 00:06:28.220
But, this can be problematic for the same
reasons that response bias is problematic.

00:06:28.220 --> 00:06:33.120
If the few single mothers who respond don’t
represent all single mothers, our data is

00:06:33.120 --> 00:06:34.120
still biased.

00:06:34.120 --> 00:06:40.440
In a 2016 LA Times/USC political tracking
poll, a 19-year-old black man was one of 3,000

00:06:40.440 --> 00:06:44.620
panelists who was interviewed week after week
about the upcoming presidential election.

00:06:44.620 --> 00:06:49.600
Because he was a member of more than one group
that was underrepresented in this poll, his

00:06:49.600 --> 00:06:53.450
response was weighted 30x more than the average
respondent.

00:06:53.450 --> 00:06:58.470
According to the New York Times, his survey
boost his candidate’s margins by an entire

00:06:58.470 --> 00:06:59.600
percentage point.

00:06:59.600 --> 00:07:02.140
Stratified Random Sampling is another option.

00:07:02.140 --> 00:07:06.889
It splits the population into groups of interest
and randomly selects people from each of the

00:07:06.889 --> 00:07:11.100
“stratas” so that each group in the overall
sample is represented appropriately.

00:07:11.100 --> 00:07:15.280
Researchers have used stratified sampling
to study differences in the way same-sex and

00:07:15.280 --> 00:07:17.620
different-sex couples parent their kids.

00:07:17.629 --> 00:07:21.730
They randomly select people from the same-sex
parenting group and... randomly select people

00:07:21.730 --> 00:07:25.680
from a different-sex group of parents to make
sure that they’re well represented in the

00:07:25.680 --> 00:07:26.180
sample.

00:07:26.180 --> 00:07:29.060
Another issue is that getting surveys to people
can be expensive.

00:07:29.060 --> 00:07:34.180
If a cereal company wants to see how families
react to their new cereal, it would be costly

00:07:34.180 --> 00:07:37.920
to send some cereal to a random sample of
all families in the country.

00:07:37.920 --> 00:07:44.580
Instead they use Cluster Sampling which create clusters (not Honey Nut Clusters) that are naturally occuring (like

00:07:44.580 --> 00:07:49.020
schools, or cities) and randomly select a
few clusters to survey instead of randomly

00:07:49.040 --> 00:07:50.820
selecting individuals.

00:07:50.820 --> 00:07:56.300
For this to work, clusters cannot be systematically
different than the population as a whole and

00:07:56.320 --> 00:07:59.280
and they should about equally represent all groups.

00:07:59.280 --> 00:08:03.280
Issues can also arise when the population
being surveyed is very small or difficult

00:08:03.280 --> 00:08:08.100
to reach, like children with rare genetic
disorders, or people addicted to certain drugs.

00:08:08.100 --> 00:08:13.080
In this case, surveyors may choose to not
use randomness at all, and instead use Snowball Sampling.

00:08:13.200 --> 00:08:16.880
That’s when current respondents are asked
to help recruit people they know from the

00:08:16.889 --> 00:08:20.479
population of interest... since people tend
to know others in their communities and can

00:08:20.479 --> 00:08:22.669
help researchers get more responses.

00:08:22.669 --> 00:08:27.480
And note that these sampling techniques can
and are used in experiments as well as surveys.

00:08:27.480 --> 00:08:31.460
There are also non-experimental data collection
methods like a Census.

00:08:31.469 --> 00:08:35.220
A Census is a survey that samples an ENTIRE
population.

00:08:35.230 --> 00:08:39.340
The United States conducts a Census every
10 years, with the next one scheduled to be

00:08:39.340 --> 00:08:40.340
done in 2020.

00:08:40.340 --> 00:08:45.070
It attempts to collect data from every.single.resident
of the United States (even undocumented residents,

00:08:45.070 --> 00:08:46.510
and homeless residents).

00:08:46.510 --> 00:08:48.910
As you can imagine, this is hard, and
it is not without error.

00:08:48.910 --> 00:08:53.380
In Medieval Europe, William the I of England
conducted a census in order to properly tax

00:08:53.380 --> 00:08:55.230
the people he had conquered.

00:08:55.230 --> 00:08:59.890
In fact a lot of rulers tended to use censuses
to know just how much money they should be

00:08:59.890 --> 00:09:00.890
demanding.

00:09:00.890 --> 00:09:05.770
Until the widespread availability of computers,
the US census data took almost 10 years to

00:09:05.770 --> 00:09:07.560
collect and analyze.

00:09:07.560 --> 00:09:11.680
Meaning that the data from the last census
wasn’t even available until right before

00:09:11.680 --> 00:09:12.680
the next census.

00:09:12.680 --> 00:09:17.040
The length of time it took to complete the
census is part of the reason we even have

00:09:17.040 --> 00:09:19.800
computers...check out our CompSci series for
more on that.

00:09:19.800 --> 00:09:23.030
So why collect census data--instead of just
sampling the population?

00:09:23.030 --> 00:09:27.200
In the US--the Census could cost more than
15 Billion dollars in 2020.

00:09:27.200 --> 00:09:28.350
There are a lot of reasons.

00:09:28.350 --> 00:09:32.680
The constitution says we have to, but also
the census provides the truest measure of

00:09:32.680 --> 00:09:34.280
the population we can get.

00:09:34.280 --> 00:09:35.610
It minimizes sampling error.

00:09:35.610 --> 00:09:38.260
It also functions as a benchmark for future
studies.

00:09:38.260 --> 00:09:44.920
And a census can give researchers really specific
information about small groups of the population--information

00:09:44.920 --> 00:09:48.200
that might be hard to gather with regular
sampling methods.

00:09:48.200 --> 00:09:52.480
Doing statistics on Census data is different,
because most statistical inference aims to

00:09:52.480 --> 00:09:56.290
take a small sample and use it to make guesses
about that population.

00:09:56.290 --> 00:10:01.070
But with a census we already have data from
the entire population, we don’t need to

00:10:01.070 --> 00:10:03.410
guess if there are differences, we can just
see them.

00:10:03.410 --> 00:10:07.710
Analysis on Census data is usually more concerned
with whether differences we see are large

00:10:07.710 --> 00:10:11.840
enough to make a difference in everyday life,
rather than guessing IF there is a relationship.

00:10:11.840 --> 00:10:13.720
The census as we said can take years.

00:10:13.720 --> 00:10:15.240
And entire countries to fund.

00:10:15.240 --> 00:10:17.810
That doesn’t discount the value of sampling.

00:10:17.810 --> 00:10:23.140
But we should be cautious...Badly worded polls,
fake polls, and biased polls are common.

00:10:23.140 --> 00:10:24.660
So are the results of those polls.

00:10:24.660 --> 00:10:29.370
The statistics-friendly website FiveThirtyEight
put together a great list of advice on how

00:10:29.370 --> 00:10:31.190
not-to-fall for a fake poll.

00:10:31.190 --> 00:10:34.300
Among its advice--Ask yourself if it seems
professional.

00:10:34.300 --> 00:10:37.480
Check to see who conducted the poll--and if
you trust them.

00:10:37.480 --> 00:10:39.230
See how the poll was conducted.

00:10:39.230 --> 00:10:42.190
Check out the questions they asked...and who
they asked.

00:10:42.190 --> 00:10:43.550
If it seems fishy.

00:10:43.550 --> 00:10:44.880
It probably is fishy.

00:10:44.880 --> 00:10:47.640
That said, well done surveys are essential.

00:10:47.650 --> 00:10:51.540
They allow us to get information without all
the trouble of doing an experiment, and since

00:10:51.540 --> 00:10:56.100
they’re comparatively easy, they’re popular
ways for businesses, countries, and even Youtube

00:10:56.100 --> 00:10:57.680
channels to collect information.

00:10:57.680 --> 00:11:02.240
In fact Crash Course Statistics has its own
survey! The Link is in the description.

00:11:02.240 --> 00:11:05.500
And it takes way less time than 
the Nerdfighteria one. I promise.

00:11:05.500 --> 00:11:07.300
Thanks for watching. I'll see you next time.

