WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.472
[MUSIC PLAYING]

00:00:04.870 --> 00:00:06.620
MUSTAFA SULEYMAN: Good
morning, everybody.

00:00:06.620 --> 00:00:08.720
Welcome to the session.

00:00:08.720 --> 00:00:10.170
My name is Mustafa.

00:00:10.170 --> 00:00:12.733
I'm co-founder of DeepMind.

00:00:12.733 --> 00:00:15.400
And this morning, I want to tell
you a little bit about the work

00:00:15.400 --> 00:00:17.500
that we've been doing
to use machine learning

00:00:17.500 --> 00:00:22.180
and AI to tackle climate change.

00:00:22.180 --> 00:00:25.410
So you may be familiar
with our mission.

00:00:25.410 --> 00:00:28.560
Our mission is to try
to solve intelligence.

00:00:28.560 --> 00:00:32.820
And the core motivation that led
us to set up this mission when

00:00:32.820 --> 00:00:35.490
we started the
company in 2010 was

00:00:35.490 --> 00:00:38.760
to try to address the
key question "What if we

00:00:38.760 --> 00:00:42.630
could distill what makes
us unique and exceptional

00:00:42.630 --> 00:00:43.500
as a species--

00:00:43.500 --> 00:00:48.060
our minds, our intelligences,
our capacity to be creative,

00:00:48.060 --> 00:00:50.640
to plan over
long-term sequences--

00:00:50.640 --> 00:00:53.340
to have really
incredible intuition

00:00:53.340 --> 00:00:57.180
for discovering insight in
complex social systems?"

00:00:57.180 --> 00:01:00.060
What if we could extract
the essence of that

00:01:00.060 --> 00:01:02.040
into an algorithmic construct?

00:01:02.040 --> 00:01:04.980
Wouldn't it be incredible if
we could use that and take

00:01:04.980 --> 00:01:07.680
advantage of parallel
compute, access

00:01:07.680 --> 00:01:09.840
to vast amounts
of training data,

00:01:09.840 --> 00:01:14.320
and use it to do really
important things in the world?

00:01:14.320 --> 00:01:16.230
And so was the
motivation that led us

00:01:16.230 --> 00:01:18.870
to architect
solving intelligence

00:01:18.870 --> 00:01:21.068
as our core mission.

00:01:21.068 --> 00:01:22.860
And I think what's at
stake in the world is

00:01:22.860 --> 00:01:25.680
that many of our most
challenging problems

00:01:25.680 --> 00:01:28.890
are actually intractably
complex, from science

00:01:28.890 --> 00:01:32.040
to macroeconomics
to weather modeling.

00:01:32.040 --> 00:01:35.250
We're overwhelmed by the
complexity of the systems

00:01:35.250 --> 00:01:35.910
around us.

00:01:35.910 --> 00:01:38.580
We've got tons and tons
of data, but trying

00:01:38.580 --> 00:01:41.100
to extract insight
from that data

00:01:41.100 --> 00:01:43.980
and learn the relationship
between cause and effect

00:01:43.980 --> 00:01:47.100
well enough to be able to
make meaningful predictions

00:01:47.100 --> 00:01:48.930
in these environments
is becoming

00:01:48.930 --> 00:01:51.743
more and more challenging.

00:01:51.743 --> 00:01:53.160
So the key question
is "What if we

00:01:53.160 --> 00:01:55.410
were able to use
the sorts of systems

00:01:55.410 --> 00:01:59.070
that we've been developing and
try and extract new insight,

00:01:59.070 --> 00:02:02.790
but actually use that to turn
insight into real action,

00:02:02.790 --> 00:02:06.870
to actually affect these
stock social challenges?"

00:02:06.870 --> 00:02:09.000
That was our core motivation.

00:02:09.000 --> 00:02:13.650
We really need new tools, new
tools that humans control,

00:02:13.650 --> 00:02:16.050
that we design to
try and help us

00:02:16.050 --> 00:02:20.940
to make sense of the complexity
of the world around us.

00:02:20.940 --> 00:02:23.550
And we began quite
a few years ago now

00:02:23.550 --> 00:02:25.980
on the old-school
challenge of Atari.

00:02:25.980 --> 00:02:31.155
Our first major
breakthrough came in 2015.

00:02:31.155 --> 00:02:32.530
For those of you
who don't know--

00:02:32.530 --> 00:02:33.780
I'm sure everyone does.

00:02:33.780 --> 00:02:38.820
The Atari set of games was the
sort of 100 or so environments

00:02:38.820 --> 00:02:41.370
from around the '80s
and '90s, many of which

00:02:41.370 --> 00:02:47.280
you'll be familiar with,
from Pong to Space Invaders.

00:02:47.280 --> 00:02:48.810
And the challenge
for us is could we

00:02:48.810 --> 00:02:51.000
actually take just
the raw environment,

00:02:51.000 --> 00:02:54.210
provide purely the
pixels as inputs,

00:02:54.210 --> 00:02:57.360
and give them to an agent that
isn't told anything about what

00:02:57.360 --> 00:02:59.640
they could do, isn't
given any prior knowledge,

00:02:59.640 --> 00:03:02.250
no heuristics,
everything the agent

00:03:02.250 --> 00:03:05.070
should learn purely
from scratch.

00:03:05.070 --> 00:03:07.440
And all we provide
is simply the goal.

00:03:07.440 --> 00:03:11.190
In this case, the
optimization target was score.

00:03:11.190 --> 00:03:13.530
Could a single
agent learn to play

00:03:13.530 --> 00:03:15.880
all of these games to a
human-level performance

00:03:15.880 --> 00:03:19.830
or even above simply
through correlating score

00:03:19.830 --> 00:03:23.760
and a rewarding outcome with
the preceding state that

00:03:23.760 --> 00:03:26.790
had taken place over
the past interactions

00:03:26.790 --> 00:03:28.840
that it had had
playing the game?

00:03:28.840 --> 00:03:33.060
And so the key intuition here
is imagine a robot standing

00:03:33.060 --> 00:03:35.670
in an arcade with no
sense of what's actually

00:03:35.670 --> 00:03:38.530
going on behind the scenes,
no additional information,

00:03:38.530 --> 00:03:40.260
but just controlling
the joystick

00:03:40.260 --> 00:03:43.840
and looking at the screen.

00:03:43.840 --> 00:03:45.420
So here's a little
video to show you

00:03:45.420 --> 00:03:47.443
what it was like when
we first set our agent

00:03:47.443 --> 00:03:48.610
to play in this environment.

00:03:48.610 --> 00:03:51.810
You can see it's randomly
moving left and right to control

00:03:51.810 --> 00:03:53.250
the paddle at the bottom.

00:03:53.250 --> 00:03:55.230
And most of the
time, it's completely

00:03:55.230 --> 00:03:58.360
missing any of
the bricks at all.

00:03:58.360 --> 00:04:01.140
After 300 or so
games, the agent seems

00:04:01.140 --> 00:04:02.820
to learn this
correlation between score

00:04:02.820 --> 00:04:05.940
going up and moving the paddle
preemptively to the right place

00:04:05.940 --> 00:04:08.940
to tap the ball into the blocks.

00:04:08.940 --> 00:04:14.010
What was really interesting--
after about 500 or so games,

00:04:14.010 --> 00:04:16.860
really unexpectedly and very
surprising to many of us

00:04:16.860 --> 00:04:18.750
engineers, was
that it discovered

00:04:18.750 --> 00:04:19.904
a strategy of tunneling.

00:04:19.904 --> 00:04:21.779
If it could just pummel
the ball up the back,

00:04:21.779 --> 00:04:25.020
it could get maximum
score with minimum effort.

00:04:25.020 --> 00:04:27.270
And this is really interesting
because it was actually

00:04:27.270 --> 00:04:30.840
the first time that we got
really concrete intuition

00:04:30.840 --> 00:04:32.820
that we were onto something.

00:04:32.820 --> 00:04:35.940
Nothing in the system
had been pre-programmed.

00:04:35.940 --> 00:04:37.140
There were no heuristics.

00:04:37.140 --> 00:04:40.110
None of the engineers were
able to hand-code little tricks

00:04:40.110 --> 00:04:41.100
like this one.

00:04:41.100 --> 00:04:43.680
But purely through
self-play, the system

00:04:43.680 --> 00:04:45.420
discovered new knowledge.

00:04:45.420 --> 00:04:48.570
And that is the core
quest of DeepMind.

00:04:48.570 --> 00:04:53.140
How can we discover new
insights and new knowledge?

00:04:53.140 --> 00:04:54.900
We extended this
a few years later

00:04:54.900 --> 00:04:58.320
to tackle the
ancient game of Go.

00:04:58.320 --> 00:05:00.480
The incredible thing
about the game of Go

00:05:00.480 --> 00:05:04.620
is that enormous complexity
arises from very, very

00:05:04.620 --> 00:05:06.160
stunning simplicity.

00:05:06.160 --> 00:05:11.180
There are very few rules and
restrictions in the game.

00:05:11.180 --> 00:05:14.240
On a 19-by-19 professional
board like this one,

00:05:14.240 --> 00:05:16.520
each player of black
and white stones

00:05:16.520 --> 00:05:19.550
takes turns to place a single
stone anywhere on the board

00:05:19.550 --> 00:05:21.410
where there's no other stone.

00:05:21.410 --> 00:05:24.200
And, over time, the objective
is to surround your opponent's

00:05:24.200 --> 00:05:29.210
stones and conquer territory in
the way that you can see here.

00:05:29.210 --> 00:05:30.680
The incredible
thing about the game

00:05:30.680 --> 00:05:33.320
is just with these simple rules,
there are something like 10

00:05:33.320 --> 00:05:37.860
to the power of 170 possible
configurations of the board.

00:05:37.860 --> 00:05:40.430
So what you're seeing
here is an intuition

00:05:40.430 --> 00:05:41.840
of the branching factor.

00:05:41.840 --> 00:05:45.040
At every moment, there's a
further few hundred or so

00:05:45.040 --> 00:05:45.540
positions.

00:05:45.540 --> 00:05:47.690
And that goes on and on and on.

00:05:47.690 --> 00:05:50.930
And to try and place that
in some kind of context--

00:05:50.930 --> 00:05:53.960
10 to the power of
170 is estimated

00:05:53.960 --> 00:05:57.080
to be more atoms than there
are in the known universe,

00:05:57.080 --> 00:06:00.260
in every liquid and solid
and gas in this room,

00:06:00.260 --> 00:06:01.940
all around us on
our entire planet,

00:06:01.940 --> 00:06:03.110
and in the known universe.

00:06:03.110 --> 00:06:05.810
More atoms-- or
there are fewer atoms

00:06:05.810 --> 00:06:07.880
in the universe than
there are positions that

00:06:07.880 --> 00:06:09.680
are possible in the game of Go.

00:06:09.680 --> 00:06:12.710
So the traditional methods
of writing handcrafted rules

00:06:12.710 --> 00:06:15.750
and heuristics are
clearly not scalable.

00:06:15.750 --> 00:06:18.710
Really important intuition,
because many of our world's

00:06:18.710 --> 00:06:20.300
most complex challenges
that we would

00:06:20.300 --> 00:06:22.670
like to make progress
with have the same kind

00:06:22.670 --> 00:06:24.950
of characteristics.

00:06:24.950 --> 00:06:28.460
And so, thankfully, we were
successful in playing the world

00:06:28.460 --> 00:06:30.140
champion at the time
at the game of Go.

00:06:30.140 --> 00:06:33.080
And this was really recognized
as a milestone moment

00:06:33.080 --> 00:06:35.420
in the development of AI.

00:06:35.420 --> 00:06:37.220
But really
interestingly, Lee Sedol,

00:06:37.220 --> 00:06:40.940
who was one of the legends
of the game who we played

00:06:40.940 --> 00:06:43.700
originally in Korea
a few years back,

00:06:43.700 --> 00:06:46.760
said something very
interesting after the game.

00:06:46.760 --> 00:06:49.490
His initial assessment
going in was

00:06:49.490 --> 00:06:52.730
that he thought AlphaGo
was based on a probability

00:06:52.730 --> 00:06:55.760
calculation and it
was simply a machine.

00:06:55.760 --> 00:06:58.520
But when he saw some of
the spectacular moves

00:06:58.520 --> 00:07:02.390
that AlphaGo was able to
play, his mind was changed.

00:07:02.390 --> 00:07:05.170
"Surely," he said,
"AlphaGo is creative."

00:07:05.170 --> 00:07:07.190
The specific move in
reference, he said,

00:07:07.190 --> 00:07:09.840
was "creative and beautiful."

00:07:09.840 --> 00:07:11.420
And this was really
exciting for us

00:07:11.420 --> 00:07:13.760
because, again, it
demonstrated to us

00:07:13.760 --> 00:07:16.640
that a system through
self-play, through interacting

00:07:16.640 --> 00:07:19.010
with an environment in the
way that we had designed,

00:07:19.010 --> 00:07:20.960
could discover new
strategies that

00:07:20.960 --> 00:07:25.580
were surprising to even the
very best players in the world.

00:07:25.580 --> 00:07:29.840
After AlphaGo, we extended it
to what we called AlphaZero.

00:07:29.840 --> 00:07:32.090
Really interesting that
we're able to generalize

00:07:32.090 --> 00:07:36.090
AlphaGo to be able to play
any other two-player game.

00:07:36.090 --> 00:07:39.950
So we really wanted to resist
the temptation that systems got

00:07:39.950 --> 00:07:42.080
really good at specific games.

00:07:42.080 --> 00:07:45.350
Clearly, as humans, what
makes us really impressive

00:07:45.350 --> 00:07:48.680
is that we can learn
new skills quite quickly

00:07:48.680 --> 00:07:51.200
based on our experience
of performing well

00:07:51.200 --> 00:07:52.440
in other domains.

00:07:52.440 --> 00:07:54.725
So, for example, if you
know how to ride a bicycle,

00:07:54.725 --> 00:07:56.600
you're probably going
to do a bit of a better

00:07:56.600 --> 00:07:58.220
job on a motorbike,
and you bring

00:07:58.220 --> 00:08:00.290
that prior knowledge to bear.

00:08:00.290 --> 00:08:03.740
That transfer learning
approach is exactly what

00:08:03.740 --> 00:08:05.660
we were hungering after.

00:08:05.660 --> 00:08:09.290
And so we tested
AlphaGo to extend it

00:08:09.290 --> 00:08:11.330
to a whole bunch of other games.

00:08:11.330 --> 00:08:14.990
Again, learning completely from
scratch in a fully general way,

00:08:14.990 --> 00:08:17.810
with no opening book,
no end-game database,

00:08:17.810 --> 00:08:19.730
no heuristics.

00:08:19.730 --> 00:08:23.270
Its self-play starts completely
from random and no reference

00:08:23.270 --> 00:08:28.070
to any past human games,
so purely organically.

00:08:28.070 --> 00:08:30.140
And the interesting thing
is that the improvement

00:08:30.140 --> 00:08:32.360
rate of the algorithm
was actually phenomenal.

00:08:32.360 --> 00:08:35.299
So AlphaGo Zero had
no prior knowledge

00:08:35.299 --> 00:08:38.000
of the game and only
the very basic rules

00:08:38.000 --> 00:08:39.740
at its input at 0 days.

00:08:39.740 --> 00:08:43.492
Three days later, AlphaGo
Zero surpassed the abilities

00:08:43.492 --> 00:08:45.200
of AlphaGo Lee, which
is the version that

00:08:45.200 --> 00:08:48.290
beat the world champion, Lee
Sedol, four games out of five

00:08:48.290 --> 00:08:49.970
in 2015.

00:08:49.970 --> 00:08:52.010
And three weeks after
that, AlphaGo Zero

00:08:52.010 --> 00:08:54.380
reaches the level of
an AlphaGo Master,

00:08:54.380 --> 00:08:57.380
the version that defeated
60 top professionals online

00:08:57.380 --> 00:08:59.810
and the world champion,
Ke Jie, in three

00:08:59.810 --> 00:09:02.620
out of three games in 2017.

00:09:02.620 --> 00:09:04.640
And then after 40
days, AlphaGo Zero

00:09:04.640 --> 00:09:08.120
surpassed all other versions
of AlphaGo and arguably

00:09:08.120 --> 00:09:11.160
became the best Go
programmer in the world.

00:09:11.160 --> 00:09:12.860
And, again, the
key intuition here

00:09:12.860 --> 00:09:16.460
is that more training
time, more self-play,

00:09:16.460 --> 00:09:18.770
provides the algorithm
and the agent

00:09:18.770 --> 00:09:22.370
with more experience, more
examples to learn from,

00:09:22.370 --> 00:09:26.090
a greater distribution
of possible state spaces.

00:09:26.090 --> 00:09:28.880
And in doing so, it's able
to produce much more general,

00:09:28.880 --> 00:09:31.610
much more flexible insights.

00:09:31.610 --> 00:09:33.710
So this single system,
this general model

00:09:33.710 --> 00:09:37.100
was able to beat the
best systems out there

00:09:37.100 --> 00:09:39.620
in Shogi, in chess
in under four hours,

00:09:39.620 --> 00:09:42.042
and, ultimately, in Go
in under eight hours.

00:09:42.042 --> 00:09:44.000
It's really interesting
here to look at a quote

00:09:44.000 --> 00:09:47.780
from Garry Kasparov, one of the
former world chess champions.

00:09:47.780 --> 00:09:49.460
He said, "The
implications go far

00:09:49.460 --> 00:09:52.580
belong beyond my
beloved chess board.

00:09:52.580 --> 00:09:55.010
Not only do these
self-taught expert machines

00:09:55.010 --> 00:09:57.860
perform incredibly well,
but we can actually

00:09:57.860 --> 00:10:01.010
learn from the new
knowledge they produce."

00:10:01.010 --> 00:10:05.150
And, again, this is precisely
the symbiotic man and machine

00:10:05.150 --> 00:10:07.460
relationship that we
were hungering after.

00:10:07.460 --> 00:10:09.860
How could agent
systems provide us

00:10:09.860 --> 00:10:12.420
humans with new insights
and new knowledge

00:10:12.420 --> 00:10:14.370
that we could then
use to take actions

00:10:14.370 --> 00:10:17.970
in our own complex environments
and, again, train systems

00:10:17.970 --> 00:10:21.625
to improve in the process?

00:10:21.625 --> 00:10:23.250
And I really want to
sort take a moment

00:10:23.250 --> 00:10:26.010
to step back, because
this is really

00:10:26.010 --> 00:10:30.210
the real purpose behind our core
mission to solve intelligence.

00:10:30.210 --> 00:10:33.690
How can we enable
machines to help teach us

00:10:33.690 --> 00:10:37.140
new insights, new
strategies, and new knowledge

00:10:37.140 --> 00:10:40.080
to focus first and foremost
on the problems that actually

00:10:40.080 --> 00:10:42.230
matter in the world today?

00:10:42.230 --> 00:10:47.040
And I think what's really at
heart here is our free choice

00:10:47.040 --> 00:10:49.860
to work on the problems that
really matter and really

00:10:49.860 --> 00:10:52.170
address the core
question of what is

00:10:52.170 --> 00:10:54.870
our purpose in the world today?

00:10:54.870 --> 00:10:57.900
And there couldn't be
any greater moment for us

00:10:57.900 --> 00:11:02.410
to address these real,
stark social challenges.

00:11:02.410 --> 00:11:06.370
For example, take the
challenge of climate change.

00:11:06.370 --> 00:11:09.990
We know that global mean surface
temperature over the last 150

00:11:09.990 --> 00:11:13.170
years or so has
been rapidly rising.

00:11:13.170 --> 00:11:15.540
We know that we're
potentially on a trajectory

00:11:15.540 --> 00:11:18.780
to hit 2 degrees of
rising temperature, which

00:11:18.780 --> 00:11:21.990
could cause irreversible
consequences for our world

00:11:21.990 --> 00:11:24.960
and have a massive
impact on our ecosystems.

00:11:24.960 --> 00:11:27.960
According to NASA, the planet's
average surface temperature

00:11:27.960 --> 00:11:33.575
has already risen by about 0.9
degrees since the 19th century.

00:11:33.575 --> 00:11:36.180
I mean, this is scientific
fact and is clearly

00:11:36.180 --> 00:11:38.400
one of the most urgent and
pressing problems for us

00:11:38.400 --> 00:11:41.070
to focus on because
the trajectory looks

00:11:41.070 --> 00:11:42.630
really worrying.

00:11:42.630 --> 00:11:45.960
A 1.5 degrees
centigrade average rise

00:11:45.960 --> 00:11:50.610
could put up to 30% of
species at risk of extinction,

00:11:50.610 --> 00:11:53.230
according to the IPCC.

00:11:53.230 --> 00:11:55.470
And at 2 degrees
centigrade, most

00:11:55.470 --> 00:11:59.978
think ecosystems will struggle
to survive altogether.

00:11:59.978 --> 00:12:01.520
But this temperature
rise, of course,

00:12:01.520 --> 00:12:04.270
doesn't just affect
animals and ecosystems.

00:12:04.270 --> 00:12:06.970
Global temperature
increases of 3 degrees

00:12:06.970 --> 00:12:11.080
are estimated to result in 330
million people being displaced

00:12:11.080 --> 00:12:14.530
by flooding alone,
according to the UN.

00:12:14.530 --> 00:12:17.890
And, in fact, this actually
affects all of us right now.

00:12:17.890 --> 00:12:21.940
Where we're stood here today,
sea levels could rise by 3 foot

00:12:21.940 --> 00:12:25.050
by 2100.

00:12:25.050 --> 00:12:28.740
And so take a look at what
would happen right here at IO.

00:12:28.740 --> 00:12:32.700
This would leave us underwater
right where we're stood--

00:12:32.700 --> 00:12:34.810
pretty remarkable.

00:12:34.810 --> 00:12:37.920
So it's clear that climate is
one of our greatest challenges.

00:12:37.920 --> 00:12:40.200
And so three or
four years ago, we

00:12:40.200 --> 00:12:43.320
decided to ask ourselves the
core question "How could we

00:12:43.320 --> 00:12:46.200
as a team start to
focus significant amount

00:12:46.200 --> 00:12:49.290
of our efforts on this
really important problem?"

00:12:49.290 --> 00:12:51.240
And, of course,
energy consumption

00:12:51.240 --> 00:12:53.910
is one of the largest
contributors to climate change

00:12:53.910 --> 00:12:56.330
itself.

00:12:56.330 --> 00:13:01.647
So we gave ourselves two core
pillars to work on at Google.

00:13:01.647 --> 00:13:03.980
The first is the question of
whether we can dramatically

00:13:03.980 --> 00:13:06.960
increase the efficiency
of existing systems,

00:13:06.960 --> 00:13:09.140
both on the consumer
side and also

00:13:09.140 --> 00:13:12.540
on the large-scale industrial
systems side of things.

00:13:12.540 --> 00:13:15.230
Secondly, could we rapidly
accelerate the introduction

00:13:15.230 --> 00:13:16.220
of renewables?

00:13:16.220 --> 00:13:18.440
We know that this
technology is possible.

00:13:18.440 --> 00:13:19.847
Prices are plummeting.

00:13:19.847 --> 00:13:21.680
But there are some very
significant barriers

00:13:21.680 --> 00:13:23.600
to adoption that we
think machine learning

00:13:23.600 --> 00:13:24.500
models can help with.

00:13:24.500 --> 00:13:28.470
So I'm going to talk to
you about these two goals.

00:13:28.470 --> 00:13:30.802
So, first of all, what did
we do on the consumer side?

00:13:30.802 --> 00:13:33.260
Well, we've had an excellent
collaboration with the Android

00:13:33.260 --> 00:13:35.520
team for over three years now.

00:13:35.520 --> 00:13:38.480
And what we've been trying to
do is extend the battery life

00:13:38.480 --> 00:13:41.450
that you get from your
phone, simply by improving

00:13:41.450 --> 00:13:44.510
the way that your phone
interacts with you in a very

00:13:44.510 --> 00:13:46.010
personalized way.

00:13:46.010 --> 00:13:49.820
We managed to deliver a 30%
reduction in CPU wake-ups.

00:13:49.820 --> 00:13:51.830
This is now rolled
out to Android Pie,

00:13:51.830 --> 00:13:56.480
which is already hitting
about 2.5 billion users.

00:13:56.480 --> 00:13:58.490
This had an enormous
contribution

00:13:58.490 --> 00:14:00.110
to increasing the
overall battery

00:14:00.110 --> 00:14:05.360
life from a single charge on a
Pixel to around 30 hours or so.

00:14:05.360 --> 00:14:07.850
So this is a little
bit on how it worked.

00:14:07.850 --> 00:14:10.040
We basically, along
with the Android team,

00:14:10.040 --> 00:14:14.020
built a two-layer deep
convolutional neural network

00:14:14.020 --> 00:14:16.250
with a neural network
on top of that

00:14:16.250 --> 00:14:18.440
to predict the probability
that an app would

00:14:18.440 --> 00:14:22.440
be opened at a given interval.

00:14:22.440 --> 00:14:25.130
So the key thing here is
that we abstracted out

00:14:25.130 --> 00:14:28.010
an anonymized sequences
of app interactions

00:14:28.010 --> 00:14:31.130
that you had and then tried
to predict when you were

00:14:31.130 --> 00:14:33.433
likely to use an app in
relation to the other apps

00:14:33.433 --> 00:14:34.850
that you were using
in the future.

00:14:34.850 --> 00:14:36.767
So we know, for example,
that some people tend

00:14:36.767 --> 00:14:38.618
to use certain apps
in the morning,

00:14:38.618 --> 00:14:40.910
say, when you're on your
commute or when you're quickly

00:14:40.910 --> 00:14:42.470
accessing your news.

00:14:42.470 --> 00:14:45.502
Other apps tend to be
used more at the weekends.

00:14:45.502 --> 00:14:47.210
And this relationship
is really important

00:14:47.210 --> 00:14:49.120
because if we can
optimize which apps

00:14:49.120 --> 00:14:51.110
they open in the
background, we can obviously

00:14:51.110 --> 00:14:55.320
significantly reduce the
cost on battery life.

00:14:55.320 --> 00:14:57.650
And, of course, as with
all of the applications

00:14:57.650 --> 00:14:59.750
and deployments and launches
that we try to make,

00:14:59.750 --> 00:15:03.320
we pay a great deal of attention
to fairness and privacy.

00:15:03.320 --> 00:15:05.690
So there's no favoritism
of one app over another.

00:15:05.690 --> 00:15:08.420
All of that was
completely identified.

00:15:08.420 --> 00:15:11.840
You may use a particular app in
a very different way to the way

00:15:11.840 --> 00:15:14.180
I use another app, and
that doesn't actually

00:15:14.180 --> 00:15:15.260
get factored in at all.

00:15:15.260 --> 00:15:19.160
It's entirely personalized,
and your individual usage

00:15:19.160 --> 00:15:20.610
is what really matters.

00:15:20.610 --> 00:15:23.660
But, of course, we personalize
in a privacy-preserving way,

00:15:23.660 --> 00:15:26.540
as well, so all of the
personally identifiable data

00:15:26.540 --> 00:15:29.810
is removed before the
actual model is trained.

00:15:29.810 --> 00:15:32.600
And then the model is retrained
on your local device, which

00:15:32.600 --> 00:15:35.690
I think is a really exciting
and promising proof point of how

00:15:35.690 --> 00:15:38.990
we're going to get closer
and closer to locally trained

00:15:38.990 --> 00:15:42.870
models over the next
five years or so.

00:15:42.870 --> 00:15:45.110
So moving now from
processes in your pocket

00:15:45.110 --> 00:15:47.300
to processes in the cloud--

00:15:47.300 --> 00:15:49.100
of course, at
Google, we have some

00:15:49.100 --> 00:15:51.080
of the largest and
actually already

00:15:51.080 --> 00:15:53.120
most well-engineered
and efficient

00:15:53.120 --> 00:15:55.520
industrial systems in the world.

00:15:55.520 --> 00:15:58.650
When we decided to take a
look at Google data centers,

00:15:58.650 --> 00:16:00.980
I can tell you that some
of the expert engineers who

00:16:00.980 --> 00:16:04.190
had been working on these
systems for almost two decades

00:16:04.190 --> 00:16:06.170
now were a little
bit surprised that we

00:16:06.170 --> 00:16:09.173
thought we could make them
significantly more efficient.

00:16:09.173 --> 00:16:11.090
They were very collaborative,
and we set about

00:16:11.090 --> 00:16:13.190
on a three-year
journey to see if we

00:16:13.190 --> 00:16:15.740
could try to use machine
learning systems to improve

00:16:15.740 --> 00:16:18.990
the efficiency of how power
is managed in the Google data

00:16:18.990 --> 00:16:20.558
center fleet.

00:16:20.558 --> 00:16:22.100
And this is a super
important problem

00:16:22.100 --> 00:16:24.650
because we're all
consuming a hell of a lot

00:16:24.650 --> 00:16:29.490
more of data centers'
energy and storage,

00:16:29.490 --> 00:16:31.850
and we're creating
vast amounts more data.

00:16:31.850 --> 00:16:35.090
Data centers across the
world actually use around 3%

00:16:35.090 --> 00:16:36.620
of the world's electricity.

00:16:36.620 --> 00:16:38.660
And, of course, it's
growing rapidly,

00:16:38.660 --> 00:16:40.580
so there's enormous
opportunity for us

00:16:40.580 --> 00:16:44.570
here to try and use the existing
data and the existing systems

00:16:44.570 --> 00:16:47.370
and run them much
more efficiently.

00:16:47.370 --> 00:16:49.670
And it turns out that the
cooling energy is actually

00:16:49.670 --> 00:16:52.860
one of the biggest
consumers of electricity

00:16:52.860 --> 00:16:56.228
after the non-server
load in a data center.

00:16:56.228 --> 00:16:57.770
In fact, cooling
can actually make up

00:16:57.770 --> 00:17:03.070
around 40% of the total energy
used in the data centers.

00:17:03.070 --> 00:17:04.130
So how did we do this?

00:17:04.130 --> 00:17:06.430
Well, obviously, everything
starts with a large amount

00:17:06.430 --> 00:17:09.190
of historic training
data to give the agent,

00:17:09.190 --> 00:17:11.810
to give the system
some visibility on how

00:17:11.810 --> 00:17:13.480
the system has
operated in the past

00:17:13.480 --> 00:17:16.940
and try and learn from that
and replicate it going forward.

00:17:16.940 --> 00:17:19.300
So there's over
2,500 data inputs,

00:17:19.300 --> 00:17:22.060
again, showing one of the
many benefits of collecting

00:17:22.060 --> 00:17:25.119
really rich, really accurate,
ready well-labeled data

00:17:25.119 --> 00:17:26.980
over extended periods of time.

00:17:26.980 --> 00:17:29.440
We were able to look at
things like the incoming IT

00:17:29.440 --> 00:17:33.370
load, power meters, pressure
sensors, water flow meters,

00:17:33.370 --> 00:17:37.570
pump and fan speeds, alarms,
external weather conditions--

00:17:37.570 --> 00:17:40.540
loads of very rich,
contextual data

00:17:40.540 --> 00:17:43.150
that provides the agent
with a lot of information

00:17:43.150 --> 00:17:45.970
about how the
system has operated.

00:17:45.970 --> 00:17:47.650
And then we asked
the agent to take

00:17:47.650 --> 00:17:50.770
control of 20 or so actions.

00:17:50.770 --> 00:17:54.160
So it could adjust which
cooling towers are activated.

00:17:54.160 --> 00:17:56.110
It could adjust
how many chillers

00:17:56.110 --> 00:17:58.120
are being used at what time.

00:17:58.120 --> 00:18:01.300
It could adjust pressure set
points, temperature set points,

00:18:01.300 --> 00:18:02.680
flow set points--

00:18:02.680 --> 00:18:06.430
a whole range of other
control space actions.

00:18:06.430 --> 00:18:09.430
And so the way to think about
it a little bit like Atari

00:18:09.430 --> 00:18:11.110
is that the system
is trying to learn

00:18:11.110 --> 00:18:15.160
the correlation between
data at a particular state

00:18:15.160 --> 00:18:17.140
and a desired action
that it's trying

00:18:17.140 --> 00:18:20.170
to optimize with respect
to a specific goal.

00:18:20.170 --> 00:18:25.230
And, of course, that goal was
can we run the existing system

00:18:25.230 --> 00:18:26.730
with the same level
of performance--

00:18:26.730 --> 00:18:28.240
we obviously don't
want any drop out

00:18:28.240 --> 00:18:32.060
in DC uptime and availability.

00:18:32.060 --> 00:18:35.830
But can we do so with
less energy consumed?

00:18:35.830 --> 00:18:38.740
And so the way the initial
version of the system worked

00:18:38.740 --> 00:18:41.620
is that every five minutes, the
cloud-based agent pulls data

00:18:41.620 --> 00:18:43.990
from some of these
thousands of sensors,

00:18:43.990 --> 00:18:47.170
does a bunch of cleaning
and processing in the cloud,

00:18:47.170 --> 00:18:49.630
and then spits back a
set of recommendations

00:18:49.630 --> 00:18:53.470
to a human operator who examines
them and then implements them.

00:18:53.470 --> 00:18:55.750
And this adjusts controls
for all the set points

00:18:55.750 --> 00:18:57.520
that I mentioned.

00:18:57.520 --> 00:19:01.950
And remarkably, this was able to
deliver a 40% reduction in data

00:19:01.950 --> 00:19:03.430
center cooling energy.

00:19:03.430 --> 00:19:06.370
This is a graph that
we first produced

00:19:06.370 --> 00:19:09.220
when we turned on the
machine learning model

00:19:09.220 --> 00:19:12.350
for about 48 hours and then
turned it off afterwards.

00:19:12.350 --> 00:19:14.883
And again, this was an
incredibly significant moment

00:19:14.883 --> 00:19:17.050
for us a couple of years
ago because it was actually

00:19:17.050 --> 00:19:20.410
demonstrating that we can
do what we attempted to do,

00:19:20.410 --> 00:19:23.140
what we wanted to do
in 2010 when we founded

00:19:23.140 --> 00:19:27.100
the company, which is
train in a petri dish, toy,

00:19:27.100 --> 00:19:29.140
game-like environment,
and actually

00:19:29.140 --> 00:19:31.210
extract the lessons
from that environment

00:19:31.210 --> 00:19:33.655
and deploy them in a real
world-- in this case,

00:19:33.655 --> 00:19:37.690
in an enormous, very valuable
industrial system like Google's

00:19:37.690 --> 00:19:39.610
data center fleet.

00:19:39.610 --> 00:19:41.470
But the really
exciting thing is some

00:19:41.470 --> 00:19:43.655
of the kinds of
lessons or knowledge

00:19:43.655 --> 00:19:45.280
that was discovered
in the process that

00:19:45.280 --> 00:19:48.700
was surprising to many of
the data center engineers.

00:19:48.700 --> 00:19:52.330
The first is that it is actually
more efficient to spread load

00:19:52.330 --> 00:19:54.280
across more equipment.

00:19:54.280 --> 00:19:56.830
And so if you think about it
as a data center engineer,

00:19:56.830 --> 00:20:01.030
you're having to review
thousands of data sensor inputs

00:20:01.030 --> 00:20:03.670
over time, and you've
got all sorts of controls

00:20:03.670 --> 00:20:04.840
that you can adapt.

00:20:04.840 --> 00:20:06.640
And finding the
optimal relationship

00:20:06.640 --> 00:20:09.460
between different pieces
of cooling equipment

00:20:09.460 --> 00:20:12.070
at different times given
different incoming IT loads

00:20:12.070 --> 00:20:13.510
and different
weather environments

00:20:13.510 --> 00:20:16.540
is incredibly difficult, even
for the most experienced data

00:20:16.540 --> 00:20:17.695
center engineers.

00:20:17.695 --> 00:20:19.570
And so it turned out
that spreading that load

00:20:19.570 --> 00:20:22.940
across more equipment was
actually more effective.

00:20:22.940 --> 00:20:26.050
Another surprising intuition
that higher flow rate

00:20:26.050 --> 00:20:28.690
through the chillers was
actually not always better.

00:20:28.690 --> 00:20:30.400
In some cases, it
remained better.

00:20:30.400 --> 00:20:32.410
But in many cases, it
was actually better

00:20:32.410 --> 00:20:36.230
to reduce flow to the chillers
given some weather conditions.

00:20:36.230 --> 00:20:39.250
And, finally, adapting the
loads to different pieces

00:20:39.250 --> 00:20:41.590
of equipment across
the year turned out

00:20:41.590 --> 00:20:46.158
to also be a very valuable
way to drive efficiencies.

00:20:46.158 --> 00:20:48.700
So, then, how did we scale this
up and get it into production

00:20:48.700 --> 00:20:49.880
across the fleet?

00:20:49.880 --> 00:20:51.550
Well, it was obviously
really important

00:20:51.550 --> 00:20:55.195
that we took a safety-first
automation approach.

00:20:55.195 --> 00:20:57.280
So let me tell you a
little bit about that.

00:20:57.280 --> 00:21:00.190
The first part of the process
looked exactly the same.

00:21:00.190 --> 00:21:03.670
The data center produces a
bunch of sensor information

00:21:03.670 --> 00:21:06.370
that describes state
at any given moment.

00:21:06.370 --> 00:21:09.400
And then the model produces
a set of recommendations.

00:21:09.400 --> 00:21:12.280
But in this case, the model
sent those recommendations

00:21:12.280 --> 00:21:16.060
to a local data center control
system that automatically

00:21:16.060 --> 00:21:19.390
implemented those without
the human necessarily

00:21:19.390 --> 00:21:20.800
being in the loop.

00:21:20.800 --> 00:21:23.680
And this in itself was the
first autonomous application

00:21:23.680 --> 00:21:27.740
that we're aware of in a
large-scale industrial system.

00:21:27.740 --> 00:21:30.010
So some of those key
safety features-- firstly,

00:21:30.010 --> 00:21:33.250
there was continuous monitoring
across the entire fleet.

00:21:33.250 --> 00:21:35.260
There was automatic failover.

00:21:35.260 --> 00:21:36.760
Just in case
anything went wrong,

00:21:36.760 --> 00:21:39.610
there were a set of smooth
transition heuristics

00:21:39.610 --> 00:21:42.460
that allowed the system
to gracefully fail rather

00:21:42.460 --> 00:21:45.430
than abruptly shift to
a set of new parameters.

00:21:45.430 --> 00:21:47.230
There was two-layer
verification, so

00:21:47.230 --> 00:21:49.770
both at the local level
and at the cloud level.

00:21:49.770 --> 00:21:52.600
There was verification of
the inputs at both levels.

00:21:52.600 --> 00:21:55.810
Constant communication between
the data center operator,

00:21:55.810 --> 00:21:59.050
the model, the cloud team, and
the local data center team,

00:21:59.050 --> 00:22:00.070
too.

00:22:00.070 --> 00:22:02.550
And then, crucially, there
was uncertainty estimation.

00:22:02.550 --> 00:22:04.380
And I think this
is very exciting.

00:22:04.380 --> 00:22:07.210
It's the beginnings of
interpretability of our machine

00:22:07.210 --> 00:22:08.440
learning algorithms.

00:22:08.440 --> 00:22:10.750
You want your model
to accurately give you

00:22:10.750 --> 00:22:13.420
a confidence indicator
of how sure it

00:22:13.420 --> 00:22:16.090
is that a particular
set of recommendations

00:22:16.090 --> 00:22:18.280
are going to lead to a
particular outcome that

00:22:18.280 --> 00:22:19.382
is desirable.

00:22:19.382 --> 00:22:21.340
And sometimes you might
have a bit more courage

00:22:21.340 --> 00:22:24.462
to move that confidence
estimation further

00:22:24.462 --> 00:22:26.170
towards the objective
that you're looking

00:22:26.170 --> 00:22:29.240
for if you choose to do that.

00:22:29.240 --> 00:22:31.360
Or you might want to
keep it more constrained,

00:22:31.360 --> 00:22:33.730
depending on what
you're optimizing for

00:22:33.730 --> 00:22:37.413
and how risky the state
is at that moment.

00:22:37.413 --> 00:22:39.580
And, of course, there's
always rules and heuristics,

00:22:39.580 --> 00:22:43.210
and the human is in the
loop all of the time.

00:22:43.210 --> 00:22:46.440
The interesting thing here is
that over the 12-month period

00:22:46.440 --> 00:22:49.170
that our autonomous
system was in deployment,

00:22:49.170 --> 00:22:52.380
as more and more data or
training examples were

00:22:52.380 --> 00:22:54.275
collected, the
performance of the system

00:22:54.275 --> 00:22:55.650
got better and
better and better.

00:22:55.650 --> 00:22:58.380
In this case, going down
is actually a good thing

00:22:58.380 --> 00:23:00.600
because it's improving the
efficiency of the system

00:23:00.600 --> 00:23:04.910
and using less energy to deliver
the same performance output.

00:23:07.900 --> 00:23:10.290
So it's really cool to
see one of the key data

00:23:10.290 --> 00:23:12.950
center engineers in the
team say the following.

00:23:12.950 --> 00:23:14.530
"It was amazing to
see the AI learn

00:23:14.530 --> 00:23:16.390
to take advantage
of winter conditions

00:23:16.390 --> 00:23:19.060
and produce colder than
normal water, which

00:23:19.060 --> 00:23:22.390
reduces the energy required for
cooling with the data center."

00:23:22.390 --> 00:23:25.640
So really helpful to
get that feedback.

00:23:25.640 --> 00:23:28.210
So now moving on to what we've
been doing on the wind farm

00:23:28.210 --> 00:23:29.060
side of things.

00:23:29.060 --> 00:23:31.090
This was the second
pillar of our motivation

00:23:31.090 --> 00:23:33.280
to try to figure out how
we can use machine learning

00:23:33.280 --> 00:23:37.390
models to make wind
much more competitive.

00:23:37.390 --> 00:23:39.370
The challenge with
wind energy is

00:23:39.370 --> 00:23:41.620
that although the
cost of production

00:23:41.620 --> 00:23:45.050
can by unit be
lower, over a year,

00:23:45.050 --> 00:23:47.200
it's really, really
difficult, principally

00:23:47.200 --> 00:23:50.620
because it's very unlike
fossil fuel energy

00:23:50.620 --> 00:23:52.660
and it's very
difficult to dispatch.

00:23:52.660 --> 00:23:54.530
It's very unreliable.

00:23:54.530 --> 00:23:57.760
So grid operators
really prioritize

00:23:57.760 --> 00:23:59.780
knowing exactly how
much energy they're

00:23:59.780 --> 00:24:01.930
going to receive from
different producers

00:24:01.930 --> 00:24:04.090
and when they need
to dispatch that

00:24:04.090 --> 00:24:06.160
to the various
different consumers

00:24:06.160 --> 00:24:08.530
who demand it at
a particular time.

00:24:08.530 --> 00:24:11.470
And this has really significant
economic consequences,

00:24:11.470 --> 00:24:14.740
because that certainty
is incredibly valuable.

00:24:14.740 --> 00:24:17.440
The unpredictability
of renewable energy

00:24:17.440 --> 00:24:20.620
makes it much, much less
valuable than fossil fuels

00:24:20.620 --> 00:24:22.850
because you can't
guarantee the exact amount.

00:24:22.850 --> 00:24:23.950
Sometimes it's windy.

00:24:23.950 --> 00:24:24.790
Sometimes it's not.

00:24:24.790 --> 00:24:27.380
There's huge variability.

00:24:27.380 --> 00:24:29.770
So our challenge was
to try to improve

00:24:29.770 --> 00:24:33.070
the quality of our predictions
so that we could accurately

00:24:33.070 --> 00:24:37.420
schedule when we were going
to have surplus wind energy

00:24:37.420 --> 00:24:39.830
to provide to the grid.

00:24:39.830 --> 00:24:41.830
And that makes it
much more competitive

00:24:41.830 --> 00:24:45.620
with traditional fossil
fuel energy production.

00:24:45.620 --> 00:24:50.450
We started working on about 700
megawatts of Google's wind farm

00:24:50.450 --> 00:24:50.990
portfolio.

00:24:50.990 --> 00:24:54.290
This is about a quarter of
the entire energy produced

00:24:54.290 --> 00:24:56.750
by Google at any given time.

00:24:56.750 --> 00:24:58.830
And just to put
that in perspective,

00:24:58.830 --> 00:25:00.380
it's about the same
amount of energy

00:25:00.380 --> 00:25:03.480
that's consumed in
all of San Francisco,

00:25:03.480 --> 00:25:06.563
so it's really material.

00:25:06.563 --> 00:25:07.980
And, again, the
way that it worked

00:25:07.980 --> 00:25:10.060
is that we took a
whole set of inputs,

00:25:10.060 --> 00:25:12.720
including a whole range
of weather forecasts,

00:25:12.720 --> 00:25:16.600
local weather observations, and
all sorts of other data inputs,

00:25:16.600 --> 00:25:18.000
and we trained
the neural network

00:25:18.000 --> 00:25:21.900
to try to predict with some
probability distribution what

00:25:21.900 --> 00:25:25.410
power generation will
look like in 36 hours.

00:25:25.410 --> 00:25:28.500
And that allowed the team
to then provide the grid

00:25:28.500 --> 00:25:30.990
with much more accurate,
much more reliable

00:25:30.990 --> 00:25:36.750
information about when the farms
were able to provide energy

00:25:36.750 --> 00:25:38.310
back to the grid,
and therefore keep

00:25:38.310 --> 00:25:42.067
it competitive with the
rest of fossil fuels.

00:25:42.067 --> 00:25:44.400
Just to give you a bit of an
intuition for how difficult

00:25:44.400 --> 00:25:49.680
this is, this is production
from 0 to 250 megawatts,

00:25:49.680 --> 00:25:51.910
as you can see on
the y-axis here,

00:25:51.910 --> 00:25:54.460
and that's just across
a 16-day period.

00:25:54.460 --> 00:25:57.920
So the scale of the variation
is absolutely phenomenal.

00:25:57.920 --> 00:26:01.080
So it's really difficult to
make this a competitive product.

00:26:01.080 --> 00:26:04.650
And being able to tell the grid
that we're very confident in 36

00:26:04.650 --> 00:26:07.110
hours that we're going
to be able to supply you

00:26:07.110 --> 00:26:09.270
with the right amount
of energy that you need,

00:26:09.270 --> 00:26:10.770
again, makes it competitive.

00:26:10.770 --> 00:26:13.290
And you can see here that
our predictions are tracking

00:26:13.290 --> 00:26:15.750
the ground truth over time.

00:26:15.750 --> 00:26:19.920
Overall, this actually made
the value of the wind energy

00:26:19.920 --> 00:26:23.340
that's developed at
Google 20% more valuable

00:26:23.340 --> 00:26:25.020
in economic terms.

00:26:25.020 --> 00:26:27.630
And so, again, this is a
step change improvement,

00:26:27.630 --> 00:26:30.360
a really significant
step up that

00:26:30.360 --> 00:26:34.590
helps us to eliminate fossil
fuels in the grid over time

00:26:34.590 --> 00:26:37.410
and increase the amount
of non-carbon-based energy

00:26:37.410 --> 00:26:40.270
that we're producing.

00:26:40.270 --> 00:26:42.620
So, finally, just to sort
of wrap up and summarize,

00:26:42.620 --> 00:26:44.920
the intuition I want to
leave everybody with today

00:26:44.920 --> 00:26:48.100
is that we actually
have enormous potential

00:26:48.100 --> 00:26:51.460
to deliver radical improvement
to existing systems.

00:26:51.460 --> 00:26:53.170
Of course, we
would love a moment

00:26:53.170 --> 00:26:55.520
where we can rebuild
everything from scratch.

00:26:55.520 --> 00:26:58.420
But the reality is, we have
to engage over the next couple

00:26:58.420 --> 00:27:01.510
decades with old-school,
industrial, established

00:27:01.510 --> 00:27:05.920
systems, where we have to work
with existing data and existing

00:27:05.920 --> 00:27:08.440
hardware and existing
infrastructure.

00:27:08.440 --> 00:27:10.630
There were no new phones
used in the production

00:27:10.630 --> 00:27:12.340
of these algorithms,
no new cooling

00:27:12.340 --> 00:27:14.080
systems, no new turbines.

00:27:14.080 --> 00:27:16.840
This was existing hardware.

00:27:16.840 --> 00:27:19.450
But when we collect
the right data,

00:27:19.450 --> 00:27:22.600
and it's stored and processed
and well labeled accurately,

00:27:22.600 --> 00:27:24.730
we actually can deploy
machine learning models

00:27:24.730 --> 00:27:27.580
to generate enormous
step-function-like

00:27:27.580 --> 00:27:28.730
efficiencies.

00:27:28.730 --> 00:27:31.510
And I think this is a
very modest step forward

00:27:31.510 --> 00:27:33.340
in the right
direction and gives us

00:27:33.340 --> 00:27:35.630
a good sign that
over the next decade,

00:27:35.630 --> 00:27:39.100
I think there could be really
remarkable breakthroughs

00:27:39.100 --> 00:27:42.790
and advances to come from these
kinds of energy efficiencies

00:27:42.790 --> 00:27:44.867
using machine learning models.

00:27:44.867 --> 00:27:46.450
So with that, I just
want to leave you

00:27:46.450 --> 00:27:47.830
with a final closing thought.

00:27:47.830 --> 00:27:51.040
This, to me, is the
real power of AI--

00:27:51.040 --> 00:27:54.370
to help us find really
deeply practical,

00:27:54.370 --> 00:27:57.820
real-world solutions
to try and tame some

00:27:57.820 --> 00:28:00.940
of the complexity of our most
difficult social challenges.

00:28:00.940 --> 00:28:02.138
This is our core purpose.

00:28:02.138 --> 00:28:04.180
And, to me, this is what
makes life worth living.

00:28:04.180 --> 00:28:05.972
Thanks very much for
listening to our talk.

00:28:05.972 --> 00:28:07.380
[APPLAUSE]

00:28:07.380 --> 00:28:10.730
[MUSIC PLAYING]

