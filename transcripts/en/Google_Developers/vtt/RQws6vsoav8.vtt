WEBVTT
Kind: captions
Language: en

00:00:00.880 --> 00:00:02.940
JAMES DONG: Good afternoon.

00:00:02.940 --> 00:00:04.370
My name is James Dong.

00:00:04.370 --> 00:00:07.250
I'm working on the Android
Media Team.

00:00:07.250 --> 00:00:12.850
Today, I'm here to present you
a set of new low-level media

00:00:12.850 --> 00:00:19.620
APIs we published in Android
that are being released.

00:00:19.620 --> 00:00:23.370
This set of low-level media
APIs allows the Java

00:00:23.370 --> 00:00:27.770
applications to directly access
the low-level decoding

00:00:27.770 --> 00:00:31.540
and the encoding capabilities of
the media codecs available

00:00:31.540 --> 00:00:35.300
on a specific Android device.

00:00:35.300 --> 00:00:41.830
With this set of new low-level
media APIs, as Java

00:00:41.830 --> 00:00:48.660
developers, you are given much
more flexibility and finer

00:00:48.660 --> 00:00:52.110
granularity control on how your

00:00:52.110 --> 00:00:55.770
media application behaves.

00:00:55.770 --> 00:00:58.660
Today my talk is partitioned
into two parts.

00:00:58.660 --> 00:01:02.350
In the first part, I will
briefly talk about the

00:01:02.350 --> 00:01:08.130
existing Android media
architecture and the existing

00:01:08.130 --> 00:01:12.755
high-level media APIs prior
to the Jelly Bean release.

00:01:12.755 --> 00:01:17.160
In the second part, I will focus
in my talk on the new

00:01:17.160 --> 00:01:20.790
low-level media APIs
we published in

00:01:20.790 --> 00:01:22.890
the Jelly Bean release.

00:01:22.890 --> 00:01:28.770
So let's start off with the
brief introduction to the

00:01:28.770 --> 00:01:30.480
media framework architecture.

00:01:33.120 --> 00:01:37.970
On Android-powered devices,
typically you'll see many

00:01:37.970 --> 00:01:44.110
media-related applications, such
as a camera, music app,

00:01:44.110 --> 00:01:52.030
gallery, YouTube, a movie
player, et cetera.

00:01:52.030 --> 00:01:55.170
All of these applications
basically are using the same

00:01:55.170 --> 00:01:56.800
architecture.

00:01:56.800 --> 00:01:59.650
Each application is partitioned
into two separate

00:01:59.650 --> 00:02:02.370
address spaces--

00:02:02.370 --> 00:02:07.560
the application process and
the media server process.

00:02:07.560 --> 00:02:11.240
The communication between the
application process and the

00:02:11.240 --> 00:02:15.430
media server process is using
the Android remote procedure

00:02:15.430 --> 00:02:17.880
call, or binder call.

00:02:17.880 --> 00:02:23.970
In the media server process, a
set of commonly used media

00:02:23.970 --> 00:02:27.220
services such as the
playback recording,

00:02:27.220 --> 00:02:31.070
audio routing, provided.

00:02:31.070 --> 00:02:36.340
The media server process then
talks to the drivers in the

00:02:36.340 --> 00:02:40.770
kernel through a hardware
abstract layer.

00:02:40.770 --> 00:02:44.520
For decoding and encoding
purposes, the hardware

00:02:44.520 --> 00:02:48.940
abstraction layer is the OpenMAX
integration layer.

00:02:48.940 --> 00:02:53.220
You can find out a detailed
reference URL basically at the

00:02:53.220 --> 00:02:56.340
end of my slides if you
want to find out

00:02:56.340 --> 00:02:57.590
details about OpenMAX.

00:03:03.140 --> 00:03:06.660
So prior to the Jelly Bean
release, we already had a set

00:03:06.660 --> 00:03:10.210
of high-level media APIs.

00:03:10.210 --> 00:03:14.890
With these high-level media
APIs, you are able to write

00:03:14.890 --> 00:03:19.220
applications quickly.

00:03:19.220 --> 00:03:23.090
On this slide, I show you a
really simple example on how

00:03:23.090 --> 00:03:27.770
to write a playback or streaming
application using a

00:03:27.770 --> 00:03:35.390
high-level media API, which is a
media player dot Java class.

00:03:35.390 --> 00:03:37.750
At this point, I would assume
you are pretty familiar with

00:03:37.750 --> 00:03:41.920
the media player dot Java class,
and I will quickly walk

00:03:41.920 --> 00:03:45.630
through the example
on this slide.

00:03:45.630 --> 00:03:49.300
First, what you need to do is
to create a media player

00:03:49.300 --> 00:03:55.080
object, and then set its data
source by passing it a URL.

00:03:55.080 --> 00:04:00.510
The URL can be a local file on
your Android device, or it can

00:04:00.510 --> 00:04:04.540
be pointing to a media data
source that is located on a

00:04:04.540 --> 00:04:07.920
remote server.

00:04:07.920 --> 00:04:10.850
So after you complete the media
player object, that you

00:04:10.850 --> 00:04:14.310
can send a prepared message
message to the media object,

00:04:14.310 --> 00:04:18.180
which basically will validate
the data source, see whether

00:04:18.180 --> 00:04:22.820
this file format is supported
by the parser.

00:04:22.820 --> 00:04:30.050
And if the parsing succeeded,
then you can use the common

00:04:30.050 --> 00:04:38.390
set of media controls like
start, pause, and to control

00:04:38.390 --> 00:04:40.880
the playback or streaming.

00:04:40.880 --> 00:04:45.810
You can also jump to a specific
time position by

00:04:45.810 --> 00:04:52.100
calling seek to, if the data
source is seekable.

00:04:52.100 --> 00:04:55.960
So it depends on the interaction
between users and

00:04:55.960 --> 00:04:56.680
the application.

00:04:56.680 --> 00:05:00.850
This process may be to iterated
multiple times.

00:05:00.850 --> 00:05:05.330
And then at the end of a
playback session, you want to

00:05:05.330 --> 00:05:10.780
terminate the playback session
by calling stop.

00:05:10.780 --> 00:05:13.890
Immediately after the
termination of the playback

00:05:13.890 --> 00:05:20.550
session, you want to release all
the associated resources

00:05:20.550 --> 00:05:25.140
with the media player object
by calling release.

00:05:25.140 --> 00:05:29.870
So as you can see from this
extremely simplified example,

00:05:29.870 --> 00:05:34.610
you do not need to write too
many lines of code, and it

00:05:34.610 --> 00:05:39.660
provides you with a high
abstraction level on how you

00:05:39.660 --> 00:05:45.530
can control a media playback
or streaming application.

00:05:45.530 --> 00:05:50.515
So with this existing high-level
media API prior to

00:05:50.515 --> 00:05:56.830
the Jelly Bean release, we are
able to meet the needs of most

00:05:56.830 --> 00:06:00.810
commonly used media application
use cases, and we

00:06:00.810 --> 00:06:07.360
are able to support commonly
used file formats.

00:06:07.360 --> 00:06:12.810
In addition, we are able to
support most commonly used

00:06:12.810 --> 00:06:18.510
streaming format protocols like
HTTP, RTSP, [? HERS. ?]

00:06:21.040 --> 00:06:23.410
However, as a [INAUDIBLE]

00:06:23.410 --> 00:06:28.730
Java developer, you are
often given the

00:06:28.730 --> 00:06:31.740
challenge to do more.

00:06:31.740 --> 00:06:36.520
So what if you are given a task
to write an application

00:06:36.520 --> 00:06:40.370
to support a file format that
is not yet supported by the

00:06:40.370 --> 00:06:42.100
Android platform?

00:06:42.100 --> 00:06:47.090
What if you are given a task
to write a streaming

00:06:47.090 --> 00:06:51.640
application to use a streaming
protocol that is not yet

00:06:51.640 --> 00:06:55.240
supported by the Android
platform?

00:06:55.240 --> 00:07:00.090
What if you want to do some
image processing on the output

00:07:00.090 --> 00:07:05.230
of the decoder, and then you
send the [INAUDIBLE]

00:07:05.230 --> 00:07:06.730
for rendering?

00:07:06.730 --> 00:07:12.130
So what if, after the image
processing, you want to send

00:07:12.130 --> 00:07:17.820
the output to be used as input
to an encoder, and then encode

00:07:17.820 --> 00:07:21.130
it, and then streaming
the result out?

00:07:21.130 --> 00:07:25.660
For these kinds of advanced
challenges, with the existing

00:07:25.660 --> 00:07:26.600
[INAUDIBLE]

00:07:26.600 --> 00:07:32.590
media APIs, to be honest, you
probably cannot do too much.

00:07:32.590 --> 00:07:39.640
So that's why we publish a set
of new, low-level media APIs

00:07:39.640 --> 00:07:43.260
in the Jelly Bean release
to address this set of

00:07:43.260 --> 00:07:45.810
challenges.

00:07:45.810 --> 00:07:51.410
So this new set of low-level
APIs mainly

00:07:51.410 --> 00:07:53.540
consists of three parts.

00:07:53.540 --> 00:07:57.830
The first part is
a file parser.

00:07:57.830 --> 00:08:03.240
Basically it is a new class
called the media extractor.

00:08:03.240 --> 00:08:07.890
It allows you to parse all the
currently supported file

00:08:07.890 --> 00:08:12.600
formats on the Android
platform.

00:08:12.600 --> 00:08:18.450
The second part of the new
low-level API is the ability

00:08:18.450 --> 00:08:24.350
to query the abilities of the
media codecs available on an

00:08:24.350 --> 00:08:26.120
Android device.

00:08:26.120 --> 00:08:29.480
Why do you need this
capability?

00:08:29.480 --> 00:08:33.159
You need to find out what's are
the available codecs so

00:08:33.159 --> 00:08:38.130
that you can create the specific
media codec to do

00:08:38.130 --> 00:08:42.789
decoding and encoding directly
from your application process.

00:08:42.789 --> 00:08:48.190
So that's why you need the third
part of the API, which

00:08:48.190 --> 00:08:53.730
is the MediaCodec
dot Java class.

00:08:53.730 --> 00:08:59.190
So is this class provides you
the direct access of decoding

00:08:59.190 --> 00:09:05.060
and the encoding functions of
the available media codecs on

00:09:05.060 --> 00:09:07.670
an Android device.

00:09:07.670 --> 00:09:11.850
So I will start with the first
part, the file parser.

00:09:15.130 --> 00:09:18.770
Before I want to talk about the
details of the APIs in the

00:09:18.770 --> 00:09:24.260
media extractor class, a basic
understanding on a general

00:09:24.260 --> 00:09:30.380
file structure for a typical
media data source might help.

00:09:30.380 --> 00:09:37.110
So a typical media data source
consists of one or more

00:09:37.110 --> 00:09:41.670
elementary streams, which we
call "tracks." Each track

00:09:41.670 --> 00:09:45.880
[INAUDIBLE] belong to a specific
type, like audio or

00:09:45.880 --> 00:09:48.680
video or text.

00:09:48.680 --> 00:09:57.730
So for each specific track, it
has one or more sample data

00:09:57.730 --> 00:09:59.600
associated with it.

00:09:59.600 --> 00:10:05.820
For each sample data, it can be
a decoder-specific data, or

00:10:05.820 --> 00:10:13.150
it can be the actual compressed
access unit of

00:10:13.150 --> 00:10:15.530
audioframe or videoframe.

00:10:15.530 --> 00:10:20.630
So it can have the data itself,
and then also provide

00:10:20.630 --> 00:10:23.910
the information about the media
data, such as the time

00:10:23.910 --> 00:10:27.190
stamp you need to present
the data.

00:10:31.500 --> 00:10:35.640
For a video purpose, it will
also tell you whether the

00:10:35.640 --> 00:10:39.400
current sample is
a sync frame.

00:10:39.400 --> 00:10:44.350
So with this basic understanding
on the file

00:10:44.350 --> 00:10:49.770
structure of a typical media
source, then I will introduce

00:10:49.770 --> 00:10:55.160
to you the first low-level
media API, which

00:10:55.160 --> 00:10:58.000
is the media extractor.

00:10:58.000 --> 00:11:01.990
So if you look at the media
extractor, we provide a

00:11:01.990 --> 00:11:07.875
default constructor for you to
create an instance of a media

00:11:07.875 --> 00:11:11.110
extractor, and then you
can use [? a set of ?]

00:11:11.110 --> 00:11:15.170
data source which I assume you
are pretty familiar with, a

00:11:15.170 --> 00:11:20.810
set of data source methods
similar to what media player

00:11:20.810 --> 00:11:25.130
has to configure your
data source.

00:11:25.130 --> 00:11:29.080
So once the data source is
configured, you can find out

00:11:29.080 --> 00:11:32.860
the total number of available
tracks from your data source

00:11:32.860 --> 00:11:35.045
by calling Get Track Count.

00:11:38.010 --> 00:11:41.995
And then for each individual
track count, you can find out

00:11:41.995 --> 00:11:48.560
a detailed format about it by
calling Get Track Format.

00:11:48.560 --> 00:11:53.580
So Get Track Format will retain
you an object of a

00:11:53.580 --> 00:12:00.300
media format, which is another
API we released in Android

00:12:00.300 --> 00:12:01.450
Jelly Bean.

00:12:01.450 --> 00:12:06.630
I'll talk about the media
format in next slide.

00:12:06.630 --> 00:12:14.230
So in order to use the parser,
you need to select a specific

00:12:14.230 --> 00:12:17.090
media track to use to
read data from.

00:12:20.180 --> 00:12:24.260
In order to do that, you can
call Select a Track.

00:12:24.260 --> 00:12:28.470
So for instance, a media data
source may have multiple audio

00:12:28.470 --> 00:12:32.530
tracks for different languages,
so you want to

00:12:32.530 --> 00:12:37.020
select a specific language for
your application to use.

00:12:37.020 --> 00:12:40.100
Then, you can use
Select Track.

00:12:40.100 --> 00:12:44.340
You can also deselect this track
so that you won't parse

00:12:44.340 --> 00:12:46.700
data from this specific
media track.

00:12:53.390 --> 00:12:59.120
You can also do a random seek
within the data source by

00:12:59.120 --> 00:13:03.250
calling Seek To, which
takes two arguments.

00:13:03.250 --> 00:13:06.180
First one is the time position
you want to seek to.

00:13:06.180 --> 00:13:10.870
The second argument is basically
the seek mode.

00:13:10.870 --> 00:13:14.800
So for example, if you want to
seek to a seek frame, or you

00:13:14.800 --> 00:13:18.515
want to seek to a time position
like that is after

00:13:18.515 --> 00:13:23.690
but the closest to
a seek frame.

00:13:23.690 --> 00:13:29.080
So there are a number of sync
modes you can use to affect

00:13:29.080 --> 00:13:35.580
the behavior, how you seek to
inside the data source.

00:13:35.580 --> 00:13:40.440
If you don't want to do a random
seek, you just want to

00:13:40.440 --> 00:13:43.130
move to next sample
position, you can

00:13:43.130 --> 00:13:45.190
just simply call advance.

00:13:45.190 --> 00:13:50.950
So which will move to the next
sample sequentially.

00:13:50.950 --> 00:13:55.870
So once you reach a specific
time position, then you can

00:13:55.870 --> 00:14:01.950
read media data or decode
specific information data

00:14:01.950 --> 00:14:05.980
using read sample data method.

00:14:05.980 --> 00:14:10.030
So this read sample data method
takes two arguments.

00:14:10.030 --> 00:14:14.690
The first one is to the buffer
that you want to read the

00:14:14.690 --> 00:14:16.690
sample data into.

00:14:16.690 --> 00:14:21.980
The second argument basically
is to provide an offset that

00:14:21.980 --> 00:14:25.250
you'll start to put
your data in.

00:14:25.250 --> 00:14:30.520
It also retains an integer to
indicate whether your read is

00:14:30.520 --> 00:14:33.730
successful, or whether
it reaches the end

00:14:33.730 --> 00:14:37.660
of your data source.

00:14:37.660 --> 00:14:42.000
So if it's a non-active value,
it indicates the read is

00:14:42.000 --> 00:14:43.690
successful.

00:14:43.690 --> 00:14:47.710
If it is a negative value,
that indicates the read

00:14:47.710 --> 00:14:51.580
reaches the end of the stream.

00:14:51.580 --> 00:14:54.570
So for each example, as I just
mentioned in the previous

00:14:54.570 --> 00:14:59.380
slide, you can find a bunch
of metadata associated.

00:14:59.380 --> 00:15:04.180
So one of the important pieces
of information is the

00:15:04.180 --> 00:15:06.010
presentation time.

00:15:06.010 --> 00:15:08.670
So if you wanted to find the
presentation time for that

00:15:08.670 --> 00:15:13.230
sample, you can call Get Sample
Time, which will return

00:15:13.230 --> 00:15:15.480
the presentation time
in microseconds.

00:15:18.160 --> 00:15:21.510
You can also find out the flags
associated with this

00:15:21.510 --> 00:15:25.750
particular sample, such as
whether this sample is a sync

00:15:25.750 --> 00:15:34.610
frame, whether this sample is
decoder-specific information.

00:15:34.610 --> 00:15:41.810
If your data source is not a
local file source, then you

00:15:41.810 --> 00:15:45.900
need to do a cache the data.

00:15:45.900 --> 00:15:50.640
So in order to find out how many
cached data in a RAM or

00:15:50.640 --> 00:15:57.010
in a memory, you can use Get
Cached Duration to find out

00:15:57.010 --> 00:16:00.900
how much amount of data is
actually cached in memory.

00:16:00.900 --> 00:16:06.530
And you can also use Has Cache
Reached End of Stream to find

00:16:06.530 --> 00:16:12.110
out whether the data cached in
memory already reached the end

00:16:12.110 --> 00:16:13.360
of the stream or not.

00:16:16.740 --> 00:16:19.960
As I mentioned earlier, in
order to find out details

00:16:19.960 --> 00:16:28.390
about a specific media track,
you need to use a new class

00:16:28.390 --> 00:16:30.510
called media format.

00:16:30.510 --> 00:16:36.190
Media format dot Java class
basically encapsulates a set

00:16:36.190 --> 00:16:37.890
of key value pairs.

00:16:41.020 --> 00:16:45.880
And in addition to that, it also
makes available a bunch

00:16:45.880 --> 00:16:49.970
of setters and getters methods,
so that you can find

00:16:49.970 --> 00:16:54.120
out the value or modify
the value associated

00:16:54.120 --> 00:16:56.380
with a given key.

00:16:56.380 --> 00:17:01.810
For example, if you want to
find out an audio track's

00:17:01.810 --> 00:17:07.390
number of audio channels, you
can just provide a key channel

00:17:07.390 --> 00:17:12.930
count to find out the value
of the number of channels.

00:17:12.930 --> 00:17:19.420
So to give you an example on
how these two APIs work, I

00:17:19.420 --> 00:17:21.300
will show you two examples.

00:17:21.300 --> 00:17:25.339
The first example basically
shows you a typical use of the

00:17:25.339 --> 00:17:30.340
media extractor API we provided
So first off, you

00:17:30.340 --> 00:17:34.860
need to create a media extractor
instance by calling

00:17:34.860 --> 00:17:39.955
its default constructor, and
then just like your media

00:17:39.955 --> 00:17:43.600
player object, you'll
set its data source.

00:17:43.600 --> 00:17:49.100
So once its data source is set,
then you can find the

00:17:49.100 --> 00:17:52.850
total available tracks from that
data source by calling

00:17:52.850 --> 00:17:54.850
Get Track Count.

00:17:54.850 --> 00:18:02.270
So by iterating through all the
available tracks, so you

00:18:02.270 --> 00:18:06.230
find out the media tracks you
are interested in, and then

00:18:06.230 --> 00:18:09.240
you'll select that specific
immediate track by calling

00:18:09.240 --> 00:18:11.050
Select Track.

00:18:11.050 --> 00:18:15.870
So once you select the
particular the media track you

00:18:15.870 --> 00:18:20.840
are interested in, then you
are able to read data from

00:18:20.840 --> 00:18:26.580
that track by calling
Read Sampled Data.

00:18:26.580 --> 00:18:28.370
So Read Sample Data--

00:18:28.370 --> 00:18:31.200
basically we will read a sample
from the given time

00:18:31.200 --> 00:18:33.930
position into the buffer
[INAUDIBLE]

00:18:33.930 --> 00:18:36.610
provider.

00:18:36.610 --> 00:18:41.040
If the Read Sample Data returns
a negative value, that

00:18:41.040 --> 00:18:44.520
means you already reached
the end of the stream.

00:18:44.520 --> 00:18:47.740
If it is a positive number,
that means the read is

00:18:47.740 --> 00:18:52.630
successful, and in addition to
the media data, you can find

00:18:52.630 --> 00:18:55.730
out if metadata like
presentation time, sample

00:18:55.730 --> 00:19:00.240
time, and the flags associated
with this sample.

00:19:00.240 --> 00:19:03.190
And you can decide whether you
want to move to the next

00:19:03.190 --> 00:19:09.000
sample by calling Advanced
Method, or you want to jump to

00:19:09.000 --> 00:19:13.605
a random time position by
calling Seek To So once you

00:19:13.605 --> 00:19:16.850
are done with this process,
you can iterate until you

00:19:16.850 --> 00:19:19.180
reach the end of stream.

00:19:19.180 --> 00:19:22.320
Once you are done with the media
extractor, then you just

00:19:22.320 --> 00:19:28.050
call Release Method to release
all the resources associated

00:19:28.050 --> 00:19:29.300
with this object.

00:19:31.550 --> 00:19:35.960
So how to find out a media
track that you

00:19:35.960 --> 00:19:36.650
are interested in.

00:19:36.650 --> 00:19:38.760
You'll need to know the
details of the track

00:19:38.760 --> 00:19:40.220
information.

00:19:40.220 --> 00:19:48.650
That's why you'll need to use
this media format class.

00:19:48.650 --> 00:19:53.430
So with this media format class,
you can find out the

00:19:53.430 --> 00:19:56.890
MIME stream for a specific
media track.

00:19:56.890 --> 00:20:00.250
And from the MIME stream, you
are able to tell whether this

00:20:00.250 --> 00:20:05.620
media track is audio or video
or any other media type.

00:20:05.620 --> 00:20:09.190
And if it is audio, you can find
out number of channels,

00:20:09.190 --> 00:20:13.795
sample rate, by using the getter
methods I mentioned.

00:20:13.795 --> 00:20:19.450
If it is video, you can use the
getter method, like Get

00:20:19.450 --> 00:20:23.280
Integer to find out the
video resolution.

00:20:23.280 --> 00:20:27.470
So the use for media format
class is really

00:20:27.470 --> 00:20:28.720
straightforward.

00:20:31.720 --> 00:20:36.070
I want to emphasize the file
parser, media [INAUDIBLE]

00:20:36.070 --> 00:20:42.260
class, basically supports all
the existing formats currently

00:20:42.260 --> 00:20:44.970
we are supporting on
Android platform.

00:20:44.970 --> 00:20:51.280
However, if you ask to support
a file format that is not

00:20:51.280 --> 00:20:54.520
currently supported, you
need to implement

00:20:54.520 --> 00:20:56.860
your own file parser.

00:20:56.860 --> 00:20:58.460
So this media [? extractor ?]

00:20:58.460 --> 00:21:02.130
basically just provides you
an example of how you can

00:21:02.130 --> 00:21:06.180
implement your own media
extractor to support a new

00:21:06.180 --> 00:21:10.270
file format that is
not supported.

00:21:10.270 --> 00:21:14.700
So that's the first part of
the low-level media API.

00:21:14.700 --> 00:21:18.660
Now let me move on to the second
part of the media API.

00:21:18.660 --> 00:21:25.620
Basically, it is the ability to
query the capabilities of

00:21:25.620 --> 00:21:31.070
media codecs available
on an Android device.

00:21:31.070 --> 00:21:35.230
So we published the
two new APIs--

00:21:35.230 --> 00:21:39.510
MediaCodecList and
MediaCodecInfo.

00:21:39.510 --> 00:21:46.160
So let me show you how
these two APIs work.

00:21:46.160 --> 00:21:48.380
For MediaCodecList,
it's very simple.

00:21:48.380 --> 00:21:52.270
There are only two public
APIs that you can call.

00:21:52.270 --> 00:21:56.317
The first one is that you'll
find out the total number of

00:21:56.317 --> 00:22:00.960
available codec accounts
on an Android device.

00:22:00.960 --> 00:22:03.900
The second method--

00:22:03.900 --> 00:22:08.310
the first one basically is the
Get Codec Count method.

00:22:08.310 --> 00:22:12.690
The second method is the Get
Codec Info At method, which

00:22:12.690 --> 00:22:17.340
allows you to find out the
detailed capabilities of a

00:22:17.340 --> 00:22:20.010
specific media codec.

00:22:20.010 --> 00:22:26.030
So you need to use the new API,
MediaCodecInfo in order

00:22:26.030 --> 00:22:30.640
to find out detailed
capabilities of a media codec.

00:22:30.640 --> 00:22:35.350
So the MediaCodec info class
tells you what's the codec

00:22:35.350 --> 00:22:39.240
name for this media codec,
whether it is a

00:22:39.240 --> 00:22:42.090
decoder or an encoder.

00:22:42.090 --> 00:22:47.090
And you can also find out the
number of MIME types that this

00:22:47.090 --> 00:22:49.390
specific media codec supports.

00:22:49.390 --> 00:22:54.680
So just for your information,
the same media codec may be

00:22:54.680 --> 00:22:57.300
able to support multiple
MIME types.

00:22:57.300 --> 00:23:01.340
So that's why the Get Supported
Types returns an

00:23:01.340 --> 00:23:02.380
[? array ?]

00:23:02.380 --> 00:23:06.610
of a string.

00:23:06.610 --> 00:23:11.910
So basically, it tells you all
the MIME types it supports.

00:23:11.910 --> 00:23:15.720
By iterating through the
supported the types, you are

00:23:15.720 --> 00:23:21.590
able to find out the
capabilities for a specific

00:23:21.590 --> 00:23:25.335
MIME type for this given
media codec.

00:23:28.000 --> 00:23:29.790
So you need this [INAUDIBLE]

00:23:29.790 --> 00:23:33.020
class called Codec Capabilities
in order to find

00:23:33.020 --> 00:23:35.970
out their specific capability.

00:23:35.970 --> 00:23:39.390
So what other capabilities
exposed through this API?

00:23:39.390 --> 00:23:43.010
Basically, two pieces
of information.

00:23:43.010 --> 00:23:47.190
The first one is a list of
color formats that is

00:23:47.190 --> 00:23:50.110
supported by this codec.

00:23:50.110 --> 00:23:53.940
The second piece of information
basically is a

00:23:53.940 --> 00:24:00.240
list of profiles and levels
supported by this media codec.

00:24:00.240 --> 00:24:11.720
So to give you an example on
how this MediaCodecList and

00:24:11.720 --> 00:24:18.790
MediaCodecInfo works, here
is a very simple example.

00:24:18.790 --> 00:24:22.510
So first, you call
MediaCodecList dot Get Codec

00:24:22.510 --> 00:24:26.660
Count to find out all
the available codecs

00:24:26.660 --> 00:24:29.380
on a specific device.

00:24:29.380 --> 00:24:32.650
So once you have the count, then
you can iterate through

00:24:32.650 --> 00:24:40.360
it by calling Get Codec Info
At to find out the detailed

00:24:40.360 --> 00:24:44.760
capability of a specific
media codec.

00:24:44.760 --> 00:24:50.660
So this method returns you a
object of media codec info.

00:24:50.660 --> 00:24:56.630
So using that info object, you
can find out whether this

00:24:56.630 --> 00:25:00.700
codec is an encoder or decoder,
and then you can find

00:25:00.700 --> 00:25:04.440
out all the MIME types it
supports by coding Get

00:25:04.440 --> 00:25:05.690
Supported Types.

00:25:08.040 --> 00:25:12.730
With the array of supported
types, then you can iterate

00:25:12.730 --> 00:25:18.590
through all of them, and find
out the capability for each

00:25:18.590 --> 00:25:24.170
MIME type that this media
codec can do.

00:25:24.170 --> 00:25:30.700
So in this particular example, I
would like to find out which

00:25:30.700 --> 00:25:39.950
media codec is capable of
decoding an ABC video that is

00:25:39.950 --> 00:25:43.370
encoded in high profile
level 4.0.

00:25:43.370 --> 00:25:45.280
So how to find out that.

00:25:45.280 --> 00:25:49.760
And first, you can check
the type, [INAUDIBLE]

00:25:49.760 --> 00:25:55.730
the MIME type supported by
this MediaCodec is ABC.

00:25:55.730 --> 00:26:00.690
So basically, just check the
type against video slash ABC.

00:26:00.690 --> 00:26:04.410
You can find out whether this
media codec supports ABC.

00:26:04.410 --> 00:26:09.200
So if this video codec supports
ABC, then you can

00:26:09.200 --> 00:26:16.720
find out the capabilities
of this media codec.

00:26:16.720 --> 00:26:19.300
So there are two pieces
of information,

00:26:19.300 --> 00:26:20.690
as I mentioned earlier.

00:26:20.690 --> 00:26:24.940
One is the list of output color
format and the second

00:26:24.940 --> 00:26:28.620
one is the list of profiles
and levels [INAUDIBLE].

00:26:28.620 --> 00:26:31.570
So you can just iterate through
the list of profiles

00:26:31.570 --> 00:26:35.790
and the levels exposed by the
capability object, and then

00:26:35.790 --> 00:26:39.830
compare to see whether this
media codec is supporting

00:26:39.830 --> 00:26:41.510
high-profile level four.

00:26:46.770 --> 00:26:50.230
So the use of this
MediaCodecList and the

00:26:50.230 --> 00:26:55.250
MediaCodecInfo API is pretty
much straightforward.

00:26:58.940 --> 00:27:02.680
Behind the scenes, how
this API works--

00:27:02.680 --> 00:27:11.630
for each Jelly Bean Android
device, we require a registry

00:27:11.630 --> 00:27:17.720
XML file to be put on the
device, which actually tells

00:27:17.720 --> 00:27:22.090
you all the available
media codecs.

00:27:22.090 --> 00:27:29.390
So on this slide, I show you
a simple example of this

00:27:29.390 --> 00:27:33.110
media_codecs.xml file.

00:27:33.110 --> 00:27:37.690
So in this XML file, you can
see there are two lists of

00:27:37.690 --> 00:27:38.890
media codec.

00:27:38.890 --> 00:27:44.750
One list is for encoder, the
second list is for decoder.

00:27:44.750 --> 00:27:49.760
Regardless of whether it is a
decoder instance or an encoder

00:27:49.760 --> 00:27:58.490
instance, each media codec
entry has a single

00:27:58.490 --> 00:28:00.130
attribute for name.

00:28:00.130 --> 00:28:04.390
So this name tells you the media
codec name that you got

00:28:04.390 --> 00:28:08.870
from the MediaCodec
info class.

00:28:08.870 --> 00:28:13.580
So that's the Get Name method.

00:28:13.580 --> 00:28:17.160
It will return you the name
attribute of the MediaCodec..

00:28:20.300 --> 00:28:26.460
Any media codec with the
same name can support

00:28:26.460 --> 00:28:28.700
multiple MIME types.

00:28:28.700 --> 00:28:34.200
So if you have multiple MIME
type support for a single

00:28:34.200 --> 00:28:38.880
media codec, you can have
multiple attributes for type.

00:28:38.880 --> 00:28:46.010
That's why if you look at the
MediaCodecInfo class, it has a

00:28:46.010 --> 00:28:49.990
method called Get Supported
Types, which will return all

00:28:49.990 --> 00:28:57.610
the MIME types available in this
media codecs XML file for

00:28:57.610 --> 00:28:59.510
a specific media codec entry.

00:29:03.720 --> 00:29:10.580
So a media codec list, and
the MediaCodec info class

00:29:10.580 --> 00:29:14.180
addresses the need.

00:29:14.180 --> 00:29:19.530
Like, what are the available
media codecs on a specific

00:29:19.530 --> 00:29:20.990
Android device?

00:29:20.990 --> 00:29:25.840
So once you know the available
codecs, you can use a

00:29:25.840 --> 00:29:33.930
MediaCodec class to create a
instance of that codec to

00:29:33.930 --> 00:29:38.740
directly access the low-level
decoding or encoding

00:29:38.740 --> 00:29:40.620
capabilities.

00:29:40.620 --> 00:29:42.850
So how do you do that?

00:29:42.850 --> 00:29:49.230
First, let me show you the API
available in this media codec

00:29:49.230 --> 00:29:51.960
dot Java class.

00:29:51.960 --> 00:29:56.670
The first three methods in this
class allow you to create

00:29:56.670 --> 00:29:58.340
a media codec.

00:29:58.340 --> 00:30:02.090
So the media codec can be
created either by the MIME

00:30:02.090 --> 00:30:03.860
type you support.

00:30:03.860 --> 00:30:07.970
Or if you know the specific name
for the media codec, you

00:30:07.970 --> 00:30:10.610
can just pass the name.

00:30:10.610 --> 00:30:13.040
You can create a decoding
instance or an encoding

00:30:13.040 --> 00:30:14.460
instance this way.

00:30:16.960 --> 00:30:20.480
Once you have a media codec
instance, then you need to

00:30:20.480 --> 00:30:27.640
configure this media codec for
decoding or encoding purpose

00:30:27.640 --> 00:30:30.440
using the Config method.

00:30:30.440 --> 00:30:34.480
This Config method requires
four arguments.

00:30:34.480 --> 00:30:37.790
The first argument
is media format.

00:30:37.790 --> 00:30:43.190
So this media format, usually
you can get the return value

00:30:43.190 --> 00:30:45.330
from your media extractor.

00:30:45.330 --> 00:30:50.720
So let me go back a few
slides to show you--

00:30:50.720 --> 00:30:54.090
see, for example, the Get Track
Format retains you the

00:30:54.090 --> 00:30:56.000
media format object.

00:30:56.000 --> 00:31:06.560
So you can just pass that media
format object to the

00:31:06.560 --> 00:31:08.800
config method.

00:31:08.800 --> 00:31:15.180
So if it's a video track, if you
decide to render the video

00:31:15.180 --> 00:31:20.120
to a surface, you pass
the second argument--

00:31:20.120 --> 00:31:22.050
a surface argument.

00:31:22.050 --> 00:31:25.900
If your data source is
encrypted, then you need to

00:31:25.900 --> 00:31:28.405
provide a media crypto-object.

00:31:30.940 --> 00:31:35.270
The last flags basically
indicate to the API whether

00:31:35.270 --> 00:31:37.840
this is an encoder or
decoder instance.

00:31:37.840 --> 00:31:41.480
So why you need that is because
if the instance is

00:31:41.480 --> 00:31:47.350
created by the third create
method, Create By Codec Name,

00:31:47.350 --> 00:31:49.860
you are not be able to tell
whether this is for encoding

00:31:49.860 --> 00:31:50.800
or decoding.

00:31:50.800 --> 00:31:53.300
So that's why you need
a flag to tell--

00:31:53.300 --> 00:31:56.250
so this config is for
decoding purpose or

00:31:56.250 --> 00:31:59.770
for encoding purpose.

00:31:59.770 --> 00:32:02.950
The next set of APIs basically
is a standard

00:32:02.950 --> 00:32:05.370
set of media control.

00:32:05.370 --> 00:32:07.110
You can do a stop [INAUDIBLE]

00:32:07.110 --> 00:32:08.770
you can do flash.

00:32:08.770 --> 00:32:12.230
I have a list of flash here.

00:32:12.230 --> 00:32:16.910
I'll skip those APIs, basically
just standard media

00:32:16.910 --> 00:32:18.660
control APIs.

00:32:18.660 --> 00:32:27.110
In a [INAUDIBLE] media codec
API, it's basically how we

00:32:27.110 --> 00:32:31.860
pass data back and forth between
the media process and

00:32:31.860 --> 00:32:35.170
the application process and
the media server process.

00:32:35.170 --> 00:32:39.610
We are using an array of byte
buffers for this purpose.

00:32:39.610 --> 00:32:43.310
There are two arrays
of byte buffers--

00:32:43.310 --> 00:32:49.080
one is used for input, the
second one is used for output.

00:32:49.080 --> 00:32:52.400
The actual communication between
the your application

00:32:52.400 --> 00:32:55.760
process and the media server
process actually is not

00:32:55.760 --> 00:32:58.760
bypassing the byte
buffers directly.

00:32:58.760 --> 00:33:07.280
Instead, it is passing the index
into the array, rather

00:33:07.280 --> 00:33:11.350
than just past the
buffer directly.

00:33:11.350 --> 00:33:19.000
So the next set of APIs
basically tells you how to use

00:33:19.000 --> 00:33:22.220
these buffers.

00:33:22.220 --> 00:33:24.900
So for input, you have
three methods.

00:33:24.900 --> 00:33:29.380
The first one is Get
Input Buffers.

00:33:29.380 --> 00:33:37.310
You only can call this method
after you start, after you

00:33:37.310 --> 00:33:39.680
call Start.

00:33:39.680 --> 00:33:44.610
So this method will tell you the
array of byte buffers used

00:33:44.610 --> 00:33:47.160
for this codecs session--

00:33:47.160 --> 00:33:52.110
either encoding session
or a decoding section.

00:33:52.110 --> 00:33:57.940
Once you know the array of byte
buffers, then you can

00:33:57.940 --> 00:34:03.780
find out whether there are any
input buffers available for

00:34:03.780 --> 00:34:08.520
you to use by coding Dequeue
Input Buffer.

00:34:08.520 --> 00:34:10.800
This Dequeue Input
Buffer takes an

00:34:10.800 --> 00:34:13.020
argument with Timeout.

00:34:13.020 --> 00:34:17.989
So if you give a timeout value
a negative value, basically

00:34:17.989 --> 00:34:24.020
you just wait forever until an
input buffer is available.

00:34:24.020 --> 00:34:29.690
Otherwise, you just wait for the
specified timeout period.

00:34:29.690 --> 00:34:34.909
And if there is no buffer
available, it will return a

00:34:34.909 --> 00:34:39.840
negative integer to indicate its
timeout, and there is no

00:34:39.840 --> 00:34:42.270
input buffer available.

00:34:42.270 --> 00:34:46.670
So if there is an available
input buffer before the

00:34:46.670 --> 00:34:51.760
timeout period, then it will
return a valid [? binary ?]

00:34:51.760 --> 00:34:55.400
array index, which
is basically is

00:34:55.400 --> 00:34:57.730
a non-active value.

00:34:57.730 --> 00:35:05.840
So using this index, you can
find out the right byte buffer

00:35:05.840 --> 00:35:12.060
using the input array
of byte buffers.

00:35:12.060 --> 00:35:16.700
So once you will find out the
right buffer to use, then you

00:35:16.700 --> 00:35:22.240
can put valid input data inside
this byte buffer.

00:35:22.240 --> 00:35:25.200
So you can either do some image
processing before you

00:35:25.200 --> 00:35:29.720
put your data in-- that's
fine, it's up to you.

00:35:29.720 --> 00:35:35.840
But after you prepare your data
and put it into the byte

00:35:35.840 --> 00:35:38.720
buffer, then you can use the
next method, Queue Input

00:35:38.720 --> 00:35:45.490
Buffer, to send this input
buffer to the codec.

00:35:45.490 --> 00:35:50.170
So this Queue Input Buffer takes
a bunch of arguments.

00:35:50.170 --> 00:35:52.740
Those are pretty obvious.

00:35:52.740 --> 00:35:55.850
But I want to talk about
the first argument.

00:35:55.850 --> 00:35:58.490
As you can see, it is indexed.

00:35:58.490 --> 00:36:00.590
It is not the byte buffer.

00:36:00.590 --> 00:36:05.370
So you need to pass a valid
index to this queue input

00:36:05.370 --> 00:36:11.810
buffer, which usually you got
from the return value of

00:36:11.810 --> 00:36:13.060
Dequeue Input Buffer.

00:36:16.550 --> 00:36:20.060
The next argument basically is
offset, basically indicated

00:36:20.060 --> 00:36:23.660
the starting position of your
valid data inside that byte

00:36:23.660 --> 00:36:27.600
buffer, and the size basically
is the number of bytes of data

00:36:27.600 --> 00:36:30.868
you put in your byte buffer.

00:36:30.868 --> 00:36:35.250
The CDS time, US, argument
basically tells you what is

00:36:35.250 --> 00:36:38.730
the presentation time for
this input buffer.

00:36:38.730 --> 00:36:45.150
And the flags, basically a bunch
of flags associated with

00:36:45.150 --> 00:36:49.780
this byte buffer flag, whether
this is a key frame.

00:36:49.780 --> 00:36:51.750
Something like that.

00:36:51.750 --> 00:36:56.620
So similarly for output buffer,
you can find out the

00:36:56.620 --> 00:37:02.350
array of byte buffers by calling
Get Output Buffers.

00:37:02.350 --> 00:37:06.560
Similarly, we are not using
the byte buffer directly

00:37:06.560 --> 00:37:12.620
between your application and
the media server process.

00:37:12.620 --> 00:37:20.090
We are passing the index of
the output byte buffer.

00:37:20.090 --> 00:37:23.840
So in order to find out whether
an output buffer is

00:37:23.840 --> 00:37:28.430
available, you'll call Dequeue
Output Buffer.

00:37:28.430 --> 00:37:33.290
The [INAUDIBLE] value will
indicate whether you get a

00:37:33.290 --> 00:37:36.230
valid output buffer.

00:37:36.230 --> 00:37:40.810
If it is a negative value,
there are three cases.

00:37:40.810 --> 00:37:44.380
It can be timed out
to indicate there

00:37:44.380 --> 00:37:46.650
is no buffer available.

00:37:46.650 --> 00:37:52.170
It can be indicated something
like the output format changed

00:37:52.170 --> 00:37:53.840
for decoding applications.

00:37:53.840 --> 00:37:55.240
Usually it happens.

00:37:55.240 --> 00:37:59.360
So you start to decode, and then
you find out the output

00:37:59.360 --> 00:38:01.080
buffer changed.

00:38:01.080 --> 00:38:04.190
So the return value, a negative
return value, from

00:38:04.190 --> 00:38:07.700
Dequeue Output Buffer will tell
you the output buffer

00:38:07.700 --> 00:38:11.590
format changed, the output
format changed.

00:38:11.590 --> 00:38:15.520
The third negative value that
can return from Dequeue Output

00:38:15.520 --> 00:38:24.500
Buffer is an indication of
output buffer itself changed.

00:38:24.500 --> 00:38:29.140
So after you receive that return
value, you need to call

00:38:29.140 --> 00:38:35.950
the Get Output Buffer method
again to get a refresh on the

00:38:35.950 --> 00:38:38.020
array of output buffers.

00:38:40.970 --> 00:38:45.450
If Dequeue Output Buffer returns
a non-negative value,

00:38:45.450 --> 00:38:51.560
meaning a valid buffer index,
then you can use that for the

00:38:51.560 --> 00:38:56.320
subsequent call, Release
Output Buffer.

00:38:56.320 --> 00:39:00.760
So before you call Release
Output Buffer, you may also do

00:39:00.760 --> 00:39:05.880
some image processing on the
buffer, and using the valid

00:39:05.880 --> 00:39:13.390
index to find out the correct
byte buffer that hosts the

00:39:13.390 --> 00:39:15.420
data, and you can do some image

00:39:15.420 --> 00:39:17.980
processing with that data.

00:39:17.980 --> 00:39:21.750
And then you call this Release
Output Buffer.

00:39:21.750 --> 00:39:24.050
Release Output Buffer,
basically,

00:39:24.050 --> 00:39:25.280
there are two arguments.

00:39:25.280 --> 00:39:29.160
The first one, same thing,
is an index into the

00:39:29.160 --> 00:39:31.470
rate of byte buffers.

00:39:31.470 --> 00:39:33.670
The second is a Boolean value.

00:39:33.670 --> 00:39:37.560
It basically indicates whether
you want to do rendering for

00:39:37.560 --> 00:39:39.070
this output buffer.

00:39:39.070 --> 00:39:42.900
For video, rendering means you
want to show the video frame.

00:39:42.900 --> 00:39:44.370
For audio, many of you
probably want to

00:39:44.370 --> 00:39:47.310
send to audio [? sync ?]

00:39:47.310 --> 00:39:48.560
for decoding purposes.

00:39:51.030 --> 00:39:56.000
So for each buffer, for each
byte buffer, regardless of

00:39:56.000 --> 00:40:01.760
whether it's input or output,
we provide you a set of

00:40:01.760 --> 00:40:04.120
metadata associated with it.

00:40:04.120 --> 00:40:07.290
If you look at the Dequeue
Output Buffer, the first

00:40:07.290 --> 00:40:12.650
argument is Byte Buffer Info.

00:40:12.650 --> 00:40:18.330
The Buffer Info object tells
you the metadata associated

00:40:18.330 --> 00:40:20.160
with a byte buffer.

00:40:20.160 --> 00:40:25.610
So you can find out information
like, what's the

00:40:25.610 --> 00:40:30.840
number of bytes that this buffer
holds the valid data?

00:40:30.840 --> 00:40:35.830
And what are the starting
positions, like the valid data

00:40:35.830 --> 00:40:39.400
begins, inside this
byte buffer?

00:40:39.400 --> 00:40:45.360
What is the presentation time
for this data inside the byte

00:40:45.360 --> 00:40:51.000
buffer, and also the flags,
such as whether this byte

00:40:51.000 --> 00:40:55.870
buffer holds a sync frame data,
or whether this holds

00:40:55.870 --> 00:40:58.650
the decoder-specific information
or codec config

00:40:58.650 --> 00:41:03.130
flag, or whether this actually
indicates an

00:41:03.130 --> 00:41:07.800
end-of-stream sample.

00:41:07.800 --> 00:41:12.740
So to show you how to
use the media codec

00:41:12.740 --> 00:41:17.320
API, I have two slides.

00:41:17.320 --> 00:41:20.350
The first slide basically shows
you a typical use of the

00:41:20.350 --> 00:41:23.150
low-level media codec APIs.

00:41:23.150 --> 00:41:27.990
So you start off by creating
a media codec instance, by

00:41:27.990 --> 00:41:31.560
calling one of the three
Create methods.

00:41:31.560 --> 00:41:36.790
So in this particular case, you
create a decoding instance

00:41:36.790 --> 00:41:40.460
using the supported MIME type.

00:41:40.460 --> 00:41:44.310
And then, you need to configure
this codec by

00:41:44.310 --> 00:41:47.410
calling Config Method.

00:41:47.410 --> 00:41:54.100
So the Config Method basically
needs to know the format that

00:41:54.100 --> 00:41:56.340
this codec deals with.

00:41:56.340 --> 00:42:00.680
That format can come from your
media extractor, if you

00:42:00.680 --> 00:42:04.490
provide a customized media
extractor implementation, or

00:42:04.490 --> 00:42:09.090
you use the media extractor
class we provide for you, and

00:42:09.090 --> 00:42:11.220
by calling Get Track Format.

00:42:11.220 --> 00:42:15.010
So that returns the right format
for you so that your

00:42:15.010 --> 00:42:19.690
codec can deal with that
specific media track.

00:42:19.690 --> 00:42:24.335
So once this media codec is
configured, you can just start

00:42:24.335 --> 00:42:30.300
the decoding process
by calling Start.

00:42:30.300 --> 00:42:35.190
After you call in Start, then it
is time to get the array of

00:42:35.190 --> 00:42:40.710
input byte buffers and the
array of output buffers.

00:42:40.710 --> 00:42:44.190
And you can also find out
the output format

00:42:44.190 --> 00:42:46.380
for this media track.

00:42:48.960 --> 00:42:51.430
Then following that,
basically, is

00:42:51.430 --> 00:42:53.130
an iterative process--

00:42:53.130 --> 00:42:57.110
basically you just send the
input of data to the codec to

00:42:57.110 --> 00:43:00.170
do the decoding or encoding, and
then you get output from

00:43:00.170 --> 00:43:02.850
the decoder or encoder.

00:43:02.850 --> 00:43:07.680
So that's an iterative process
in the for loop, as I've shown

00:43:07.680 --> 00:43:08.970
on the slide.

00:43:08.970 --> 00:43:15.800
So after you are done with this
process, you stop the

00:43:15.800 --> 00:43:20.940
decoding process, and then you
release all the resources

00:43:20.940 --> 00:43:22.290
associated with this
media codec.

00:43:28.610 --> 00:43:33.050
Let me go to the next slide to
show you how to actually send

00:43:33.050 --> 00:43:37.890
input data to the decoder for
decoding, and how to get

00:43:37.890 --> 00:43:40.710
output data from the decoder.

00:43:40.710 --> 00:43:44.700
So this slide shows
you the details.

00:43:44.700 --> 00:43:49.720
The first part shows you how
to send encoded data to the

00:43:49.720 --> 00:43:51.600
decoder instance.

00:43:51.600 --> 00:43:57.620
So firstly, you call the
Dequeue Input Buffer.

00:43:57.620 --> 00:43:59.670
You check its return value.

00:43:59.670 --> 00:44:02.770
If the return value is
non-negative, meaning greater

00:44:02.770 --> 00:44:10.810
than or equal to zero, then it
is a valid buffer index,

00:44:10.810 --> 00:44:13.780
meaning there is
an input buffer

00:44:13.780 --> 00:44:15.800
available for you to use.

00:44:15.800 --> 00:44:20.240
So at this point, you know which
buffer is available by

00:44:20.240 --> 00:44:25.710
using the index, which you can
find out along with the array

00:44:25.710 --> 00:44:28.570
of input buffers.

00:44:28.570 --> 00:44:31.840
And you can do image processing

00:44:31.840 --> 00:44:33.590
on the input buffer.

00:44:33.590 --> 00:44:37.370
And once you're done with your
image processing, you can call

00:44:37.370 --> 00:44:43.160
Queue Input Buffer with the
right buffer index.

00:44:43.160 --> 00:44:47.760
And then that API will send
the input buffer to the

00:44:47.760 --> 00:44:49.250
decoding instance
for decoding.

00:44:51.840 --> 00:44:54.665
If the return value is negative,
that means probably

00:44:54.665 --> 00:44:57.630
it's timed out, so you
need to retry.

00:45:01.340 --> 00:45:06.960
Next, I'll show you how to get
the output from the decoder.

00:45:06.960 --> 00:45:14.990
So you call codec dot dequeue
output buffer with a timeout.

00:45:14.990 --> 00:45:15.990
Same thing.

00:45:15.990 --> 00:45:19.990
It will return to you an integer
value, if it is a

00:45:19.990 --> 00:45:27.160
non-negative value, meaning you
find basically that there

00:45:27.160 --> 00:45:29.460
is an output buffer available.

00:45:29.460 --> 00:45:34.490
Meaning, the decoder already
generates the output data, and

00:45:34.490 --> 00:45:38.100
it's already stored in one
of the output buffers.

00:45:38.100 --> 00:45:41.480
So how to find out
that byte buffer?

00:45:41.480 --> 00:45:45.420
You use the return value from
Dequeue Output Buffer, use

00:45:45.420 --> 00:45:52.390
that as an index into the array
of output byte buffers.

00:45:52.390 --> 00:45:55.760
And of course, you can also
decide to do some image

00:45:55.760 --> 00:45:57.130
processing with it.

00:45:57.130 --> 00:45:59.100
It's up to you.

00:45:59.100 --> 00:46:00.340
After you're done with it--

00:46:00.340 --> 00:46:04.090
and then you can release this
output buffer back to the

00:46:04.090 --> 00:46:05.670
output buffer array pool.

00:46:08.750 --> 00:46:11.850
If the return value from Dequeue
Output Buffer is

00:46:11.850 --> 00:46:16.060
negative, there are three
cases I have mentioned.

00:46:16.060 --> 00:46:18.220
The slides only show
two cases.

00:46:18.220 --> 00:46:20.430
So the first case is
timeout, basically.

00:46:20.430 --> 00:46:22.010
You can check whether
it's timeout.

00:46:22.010 --> 00:46:27.470
If it's timeout, you can retry
until you find an output

00:46:27.470 --> 00:46:29.390
buffer available.

00:46:29.390 --> 00:46:33.140
Otherwise, if the
return value--

00:46:33.140 --> 00:46:37.280
if info output buffer
has changed.

00:46:37.280 --> 00:46:40.690
That means the buffer size or
number of buffers may change.

00:46:40.690 --> 00:46:47.100
So you need to get a refresh on
the array of byte buffers

00:46:47.100 --> 00:46:48.190
for output.

00:46:48.190 --> 00:46:52.430
So you call Get Output Buffers
for that purpose.

00:46:52.430 --> 00:46:57.370
For the third case, if the
return value is info

00:46:57.370 --> 00:47:02.500
underscore output underscore
format change, that means the

00:47:02.500 --> 00:47:05.730
decoder will find out the
codec format basically

00:47:05.730 --> 00:47:12.080
changed, or the subsequent
sample output will conform to

00:47:12.080 --> 00:47:13.280
the new format.

00:47:13.280 --> 00:47:16.550
So in order to find out the new
output format, you can use

00:47:16.550 --> 00:47:19.150
Get Output Format for
that purpose.

00:47:23.980 --> 00:47:32.520
So to wrap up the API for
MediaCodec, I want to mention

00:47:32.520 --> 00:47:34.540
a couple of things more.

00:47:34.540 --> 00:47:38.020
First, although the example
is given using a decoding

00:47:38.020 --> 00:47:44.960
instance, most of the APIs
in this MediaCodec class

00:47:44.960 --> 00:47:49.090
basically is the same for
encoder instance and decoder

00:47:49.090 --> 00:47:53.170
instance so that you don't need
to learn two sides of the

00:47:53.170 --> 00:47:57.470
different APIs.

00:47:57.470 --> 00:48:02.320
Second thing I want to point
out is, if you're using

00:48:02.320 --> 00:48:07.390
MediaCodec API, it is up to
the applications to do the

00:48:07.390 --> 00:48:09.960
audio and video synchronization.

00:48:09.960 --> 00:48:17.460
Because we return the buffer
info to tell you what's the

00:48:17.460 --> 00:48:18.950
presentation time.

00:48:18.950 --> 00:48:24.620
Now the opposition has the
information to do proper audio

00:48:24.620 --> 00:48:27.370
and video synchronization.

00:48:27.370 --> 00:48:32.550
Well, with the release of this
low-level MediaCodec API, now

00:48:32.550 --> 00:48:38.330
it is up to you to write more
sophisticated media

00:48:38.330 --> 00:48:42.360
applications to your
imagination.

00:48:45.780 --> 00:48:50.840
Before I want to open the floor
for questions from the

00:48:50.840 --> 00:48:56.490
audience, I want to thank
Andreas, in particular, for

00:48:56.490 --> 00:48:59.930
developing the low-level
media APIs for

00:48:59.930 --> 00:49:01.230
the Jelly Bean release.

00:49:01.230 --> 00:49:05.880
Also, I want to point out, if
you want to find out more

00:49:05.880 --> 00:49:10.820
information about OpenMAX spec,
you can use that URL on

00:49:10.820 --> 00:49:14.100
the Q&amp;A slide.

00:49:14.100 --> 00:49:20.260
Now let me the floor for all
questions from the audience,

00:49:20.260 --> 00:49:24.160
if there are any questions.

00:49:24.160 --> 00:49:25.410
Thank you.

00:49:34.080 --> 00:49:35.040
AUDIENCE: Thank you for
your presentation.

00:49:35.040 --> 00:49:36.520
So two questions.

00:49:36.520 --> 00:49:39.850
One is, is the actual decoding
and encoding happening inside

00:49:39.850 --> 00:49:42.210
of the media server
process still?

00:49:42.210 --> 00:49:43.030
JAMES DONG: Yes, that's true.

00:49:43.030 --> 00:49:43.800
AUDIENCE: OK.

00:49:43.800 --> 00:49:47.080
And so in terms of a battery
consumption, if you were to

00:49:47.080 --> 00:49:50.180
write a media player application
using these APIs

00:49:50.180 --> 00:49:53.730
versus a natively written one,
how would they compare?

00:49:53.730 --> 00:49:56.620
JAMES DONG: Yeah, that's
a really good question.

00:49:56.620 --> 00:50:02.660
So we did some experiments,
although our test app is not

00:50:02.660 --> 00:50:04.020
instrumented.

00:50:04.020 --> 00:50:09.230
And the findings, basically, the
usual findings using the

00:50:09.230 --> 00:50:18.140
MediaCodec API, we find out the
total power consumption

00:50:18.140 --> 00:50:22.510
for the application basically
is roughly 10 to 20

00:50:22.510 --> 00:50:23.510
[? milliamps ?]

00:50:23.510 --> 00:50:30.250
above what we have using
media player class.

00:50:30.250 --> 00:50:34.150
So a little bit more, but
I think it's acceptable.

00:50:34.150 --> 00:50:34.820
AUDIENCE: Thank you.

00:50:34.820 --> 00:50:35.240
JAMES DONG: Sure.

00:50:35.240 --> 00:50:36.490
Thanks.

00:50:38.160 --> 00:50:39.440
AUDIENCE: Thank you for
the presentation.

00:50:39.440 --> 00:50:45.700
I have a question about
low-level codec APIs.

00:50:45.700 --> 00:50:50.310
So does that means that I can
write an application to do the

00:50:50.310 --> 00:50:54.970
realtime video chat
with those APIs?

00:50:54.970 --> 00:51:02.090
My understanding is that those
codecs will enable leveraging

00:51:02.090 --> 00:51:05.210
the hardware capabilities
through OpenMAX protocol

00:51:05.210 --> 00:51:06.010
[INAUDIBLE]

00:51:06.010 --> 00:51:07.340
JAMES DONG: That's true.

00:51:07.340 --> 00:51:12.060
The MediaCodec API actually
allows you to access the

00:51:12.060 --> 00:51:15.410
low-level decoding-encoding
capabilities, regardless of

00:51:15.410 --> 00:51:19.390
whether the decoder is a
software decoder or is a

00:51:19.390 --> 00:51:22.160
hardware decoder, or regardless
whether encoder is

00:51:22.160 --> 00:51:26.950
an encoder or hardware
[INAUDIBLE] encoder.

00:51:26.950 --> 00:51:30.410
So you can leverage
the hardware,

00:51:30.410 --> 00:51:34.350
accelerate media codecs.

00:51:34.350 --> 00:51:38.570
AUDIENCE: And my understanding
is that prior to the Jelly

00:51:38.570 --> 00:51:42.260
Bean, those are not
exposed, right?

00:51:42.260 --> 00:51:44.840
At least for encoding.

00:51:44.840 --> 00:51:46.500
For the hardware [INAUDIBLE].

00:51:46.500 --> 00:51:47.140
JAMES DONG: That's true.

00:51:47.140 --> 00:51:50.680
That's why I said, this is a
new set of low-level media

00:51:50.680 --> 00:51:56.910
APIs, only available in
Jelly Bean and after.

00:51:56.910 --> 00:51:57.596
AUDIENCE: OK.

00:51:57.596 --> 00:51:59.840
Thank you for the answer.

00:51:59.840 --> 00:52:02.200
JAMES DONG: Thank you.

00:52:02.200 --> 00:52:03.780
AUDIENCE: I have
two questions.

00:52:03.780 --> 00:52:08.270
One, is there an X264
hardware-based encoder that

00:52:08.270 --> 00:52:11.790
goes with the 4.1 that is
accessible through the

00:52:11.790 --> 00:52:15.300
low-level APIs?

00:52:15.300 --> 00:52:17.320
JAMES DONG: To answer
your question, yes.

00:52:17.320 --> 00:52:22.060
For example, the current
device, for example,

00:52:22.060 --> 00:52:22.790
[INAUDIBLE]

00:52:22.790 --> 00:52:25.570
and the prime [INAUDIBLE]
provide you the

00:52:25.570 --> 00:52:30.580
hardware-accelerated ABC
decoder, which are able to

00:52:30.580 --> 00:52:36.250
decode ABC high-profile
level four videos.

00:52:36.250 --> 00:52:40.550
So really, it depends on
the device capability.

00:52:40.550 --> 00:52:45.850
So if you have a low-end device,
you may not be able to

00:52:45.850 --> 00:52:48.980
have the hardware to do that.

00:52:48.980 --> 00:52:51.440
So that's why you need
that capability

00:52:51.440 --> 00:52:54.540
query API to find out.

00:52:54.540 --> 00:52:57.290
AUDIENCE: And my second
question, is there an RTSP

00:52:57.290 --> 00:53:01.990
streaming component that could
integrate with the encoders?

00:53:05.540 --> 00:53:07.880
That work with the new
media [INAUDIBLE]?

00:53:07.880 --> 00:53:10.690
JAMES DONG: RTSP streaming--?

00:53:10.690 --> 00:53:12.870
AUDIENCE: Servers.

00:53:12.870 --> 00:53:15.160
JAMES DONG: Servers
we do not have.

00:53:15.160 --> 00:53:17.200
We have a client
implementation.

00:53:17.200 --> 00:53:17.646
AUDIENCE: OK.

00:53:17.646 --> 00:53:18.896
Thank you.

00:53:21.220 --> 00:53:22.130
AUDIENCE: Two questions.

00:53:22.130 --> 00:53:24.540
First of all, with the release
of Jelly Bean are you adding

00:53:24.540 --> 00:53:29.740
any extra codecs that kind of
implement these APIs that

00:53:29.740 --> 00:53:32.880
would serve as good examples?

00:53:32.880 --> 00:53:34.720
JAMES DONG: Your question
basically asked whether we

00:53:34.720 --> 00:53:38.710
have additional media codecs
to support the--?

00:53:38.710 --> 00:53:41.590
AUDIENCE: That were added in
Jelly Bean versus [INAUDIBLE].

00:53:41.590 --> 00:53:42.280
JAMES DONG: Jelly Bean?

00:53:42.280 --> 00:53:47.610
Well, this really depends
on the device you have.

00:53:47.610 --> 00:53:53.000
But the commonly available
codecs are basically, for

00:53:53.000 --> 00:53:56.720
video, you have ABC decoder,
ABC encoder.

00:53:56.720 --> 00:53:59.440
You have a MPEG-4 video
encoder-decoder.

00:53:59.440 --> 00:54:03.420
You have a H263 encoder-decoder
you would

00:54:03.420 --> 00:54:06.000
expect it to have on a commonly

00:54:06.000 --> 00:54:07.660
available Android device.

00:54:07.660 --> 00:54:11.200
For audio, you have a
bunch of formats to

00:54:11.200 --> 00:54:14.180
support decoder for MP3.

00:54:14.180 --> 00:54:22.160
For encoder, you have an AAC,
AMR, narrow band, wider band.

00:54:22.160 --> 00:54:25.150
So all of those are supported.

00:54:25.150 --> 00:54:30.460
AUDIENCE: Second, you kind of
mentioned the media crypto, I

00:54:30.460 --> 00:54:35.690
believe it was, for the actual
DRM, I would assume,

00:54:35.690 --> 00:54:36.010
production.

00:54:36.010 --> 00:54:37.820
Can you go into a little more
detail on what would be

00:54:37.820 --> 00:54:40.160
involved in using that
in your app?

00:54:40.160 --> 00:54:43.640
JAMES DONG: Yeah, that's
unfortunately out of the scope

00:54:43.640 --> 00:54:46.980
of this presentation,
which is another,

00:54:46.980 --> 00:54:49.710
basically, talk in itself.

00:54:49.710 --> 00:54:53.950
But basically, we published
a new API called the media

00:54:53.950 --> 00:54:56.700
crypto cast.

00:54:56.700 --> 00:55:03.040
You can do a decoding of
encrypted content.

00:55:03.040 --> 00:55:06.980
So currently, we implement
this and verify it with

00:55:06.980 --> 00:55:11.023
Widevine DRM, Widevine
encrypted content.

00:55:14.160 --> 00:55:16.870
Does that answer
your question?

00:55:16.870 --> 00:55:20.580
Otherwise, it's another long
talk, basically, about media

00:55:20.580 --> 00:55:21.780
crypto API.

00:55:21.780 --> 00:55:24.660
So once the API is published,
you can look at it.

00:55:24.660 --> 00:55:27.490
It's pretty simple, also.

00:55:27.490 --> 00:55:29.080
AUDIENCE: The APIs
are going to be--

00:55:29.080 --> 00:55:31.780
JAMES DONG: For media crypto.

00:55:31.780 --> 00:55:33.910
AUDIENCE: When is the actual
documentation of that going to

00:55:33.910 --> 00:55:35.850
be totally available?

00:55:35.850 --> 00:55:38.382
JAMES DONG: It's going to be
released with the SDK.

00:55:38.382 --> 00:55:39.346
AUDIENCE: OK.

00:55:39.346 --> 00:55:40.310
Great, thanks.

00:55:40.310 --> 00:55:40.590
JAMES DONG: OK.

00:55:40.590 --> 00:55:41.600
Thank you.

00:55:41.600 --> 00:55:43.500
AUDIENCE: So you mentioned
in the configuration

00:55:43.500 --> 00:55:45.150
that it uses surface.

00:55:45.150 --> 00:55:48.010
What does it use that
for, that value?

00:55:48.010 --> 00:55:48.790
JAMES DONG: OK.

00:55:48.790 --> 00:55:50.720
The second argument
for configured

00:55:50.720 --> 00:55:52.340
method is the surface.

00:55:52.340 --> 00:55:59.290
So if you want to render your
video output, for example,

00:55:59.290 --> 00:56:03.030
then you want to pass a surface
so that you can render

00:56:03.030 --> 00:56:06.240
that video output for
that purpose.

00:56:06.240 --> 00:56:06.590
AUDIENCE: OK.

00:56:06.590 --> 00:56:10.610
And how does that work with
the syncing of the audio?

00:56:10.610 --> 00:56:13.200
Would you render the movie and
then have another thread

00:56:13.200 --> 00:56:15.190
that's parsing your audio?

00:56:15.190 --> 00:56:15.960
JAMES DONG: Yeah.

00:56:15.960 --> 00:56:17.850
It's up to the application,
basically.

00:56:17.850 --> 00:56:22.390
You probably need to write
multiple threads.

00:56:22.390 --> 00:56:25.540
One thread is handling the
video, the other thread is

00:56:25.540 --> 00:56:26.620
handling the audio.

00:56:26.620 --> 00:56:29.210
But it's up to the application
to do that.

00:56:29.210 --> 00:56:29.460
AUDIENCE: OK.

00:56:29.460 --> 00:56:32.090
So you just passed the surface
for us to use.

00:56:32.090 --> 00:56:32.520
JAMES DONG: Yes.

00:56:32.520 --> 00:56:35.420
We just passed the surface
as the video sync.

00:56:35.420 --> 00:56:35.750
AUDIENCE: All right.

00:56:35.750 --> 00:56:36.630
Thank you.

00:56:36.630 --> 00:56:38.390
JAMES DONG: Thank you.

00:56:38.390 --> 00:56:38.775
AUDIENCE: Hi.

00:56:38.775 --> 00:56:41.750
In Ice Cream Sandwich, you
released a new low-level

00:56:41.750 --> 00:56:43.680
streaming API.

00:56:43.680 --> 00:56:49.760
Will Jelly Bean build upon
or replace that one?

00:56:49.760 --> 00:56:51.565
JAMES DONG: Sorry, let
me turn this one off.

00:56:57.580 --> 00:56:59.110
Sorry, what was your
question again?

00:56:59.110 --> 00:57:01.000
AUDIENCE: In Ice Cream Sandwich,
you released the new

00:57:01.000 --> 00:57:04.560
OpenMAX AL streaming
low-level library.

00:57:04.560 --> 00:57:09.110
Will Jelly Bean replace
or build upon that?

00:57:09.110 --> 00:57:10.500
JAMES DONG: Well,
it's used for a

00:57:10.500 --> 00:57:11.905
different purpose, basically.

00:57:18.600 --> 00:57:24.230
And I think from Jelly Bean and
on, we should to focus on

00:57:24.230 --> 00:57:26.850
MediaCodecs API.

00:57:26.850 --> 00:57:28.760
AUDIENCE: Will the ICS1 still
be supported, then?

00:57:28.760 --> 00:57:31.320
JAMES DONG: It will be
still supported.

00:57:31.320 --> 00:57:32.090
AUDIENCE: OK.

00:57:32.090 --> 00:57:33.830
And then you mentioned
media crypto.

00:57:33.830 --> 00:57:38.420
Is there a way to protect the
decrypted yet still encoded

00:57:38.420 --> 00:57:40.030
data using media crypto?

00:57:40.030 --> 00:57:41.360
JAMES DONG: Yes.

00:57:41.360 --> 00:57:45.920
For encrypted content, actually,
you are not able to

00:57:45.920 --> 00:57:50.660
actually access the output
by the buffer.

00:57:50.660 --> 00:57:53.740
So it's opaque data,
basically, to

00:57:53.740 --> 00:57:56.900
the application process.

00:57:56.900 --> 00:58:01.870
So if you have hardware-assisted
protection

00:58:01.870 --> 00:58:07.040
for your video pass, then the
output buffer actually is

00:58:07.040 --> 00:58:08.490
hardware-protected.

00:58:08.490 --> 00:58:11.130
AUDIENCE: Does it just support
Widevine, or do custom DRMs

00:58:11.130 --> 00:58:12.180
support it, too?

00:58:12.180 --> 00:58:14.600
JAMES DONG: Currently, the only
verified implementation

00:58:14.600 --> 00:58:15.820
is with Widevine.

00:58:15.820 --> 00:58:18.670
But I can imagine you're
also able to

00:58:18.670 --> 00:58:21.195
work with other schemes.

00:58:21.195 --> 00:58:21.540
AUDIENCE: OK.

00:58:21.540 --> 00:58:22.620
Thank you.

00:58:22.620 --> 00:58:23.870
JAMES DONG: Thanks.

00:58:28.760 --> 00:58:33.340
AUDIENCE: My first question is,
is there any constraint

00:58:33.340 --> 00:58:37.390
for threading access
of these buffers?

00:58:37.390 --> 00:58:39.050
Say [INAUDIBLE]

00:58:39.050 --> 00:58:42.980
different threads for collecting
data and maybe

00:58:42.980 --> 00:58:46.530
another to call a codec.

00:58:46.530 --> 00:58:49.870
So is there any constraint
on thread [? parts ?]

00:58:49.870 --> 00:58:53.990
for accessing the input
buffer or the buffer?

00:58:56.610 --> 00:58:59.230
JAMES DONG: Sorry,
can you repeat--?

00:58:59.230 --> 00:59:00.460
AUDIENCE: I'll make it simple.

00:59:00.460 --> 00:59:05.980
When I access the input buffer
and output buffer, can I use

00:59:05.980 --> 00:59:07.220
[INAUDIBLE]

00:59:07.220 --> 00:59:07.750
thread?

00:59:07.750 --> 00:59:10.320
Is there any constraint
for it?

00:59:10.320 --> 00:59:11.630
JAMES DONG: No, there
shouldn't be.

00:59:11.630 --> 00:59:15.790
As long as you have a valid
input buffer or output buffer,

00:59:15.790 --> 00:59:18.516
you can use it for
your own purpose.

00:59:18.516 --> 00:59:19.940
AUDIENCE: OK.

00:59:19.940 --> 00:59:23.200
My second question is, you
mentioned about the

00:59:23.200 --> 00:59:24.800
[INAUDIBLE]

00:59:24.800 --> 00:59:25.850
decoder.

00:59:25.850 --> 00:59:29.750
Is there any way to get to the
hardware color converging?

00:59:29.750 --> 00:59:32.550
Like, YUV to RGB?

00:59:32.550 --> 00:59:35.580
JAMES DONG: So, there was no API
we published in Jelly Bean

00:59:35.580 --> 00:59:37.810
for that purpose, for
color conversion.

00:59:37.810 --> 00:59:39.160
AUDIENCE: OK.

00:59:39.160 --> 00:59:44.820
And you mentioned the codec
information can tell the

00:59:44.820 --> 00:59:47.560
profile and the level
[? supported. ?]

00:59:47.560 --> 00:59:51.147
Is there any way to
know a profile or

00:59:51.147 --> 00:59:52.540
level of a media source?

00:59:56.710 --> 00:59:58.300
JAMES DONG: The profile
and the levels

00:59:58.300 --> 01:00:00.100
for the media source.

01:00:03.060 --> 01:00:06.100
No, currently, we are
not supporting that.

01:00:06.100 --> 01:00:08.930
So as an application developer,
you need to know

01:00:08.930 --> 01:00:11.060
the source, basically.

01:00:11.060 --> 01:00:12.250
AUDIENCE: Maybe not.

01:00:12.250 --> 01:00:16.440
JAMES DONG: If you don't know
it, then the codec API will

01:00:16.440 --> 01:00:22.160
indicate, it is not able to
handle if the codec is not

01:00:22.160 --> 01:00:23.220
available to handle.

01:00:23.220 --> 01:00:24.700
AUDIENCE: I haven't
read the document.

01:00:24.700 --> 01:00:27.700
Is there any arrow
codec since--?

01:00:30.490 --> 01:00:31.740
JAMES DONG: There is--

01:00:33.670 --> 01:00:38.900
well, for example, the Set
Data Source and Get Track

01:00:38.900 --> 01:00:44.660
Count, you may not get the
right number if the file

01:00:44.660 --> 01:00:46.500
format is not supported,
for example.

01:00:46.500 --> 01:00:49.340
AUDIENCE: Say for the audio
is supported, video is not

01:00:49.340 --> 01:00:53.060
supported, so maybe a check
account would be

01:00:53.060 --> 01:00:54.770
one instead of two?

01:00:54.770 --> 01:00:56.520
JAMES DONG: Yes.

01:00:56.520 --> 01:00:57.680
AUDIENCE: OK.

01:00:57.680 --> 01:01:02.180
And my last minor question is,
in your code showing in the

01:01:02.180 --> 01:01:03.100
[? spring, ?]

01:01:03.100 --> 01:01:07.900
you used two equal to compare
the codec [INAUDIBLE].

01:01:07.900 --> 01:01:10.460
Is it safe?

01:01:10.460 --> 01:01:11.190
JAMES DONG: Two equals?

01:01:11.190 --> 01:01:11.410
Sorry.

01:01:11.410 --> 01:01:12.830
AUDIENCE: Two equals.

01:01:12.830 --> 01:01:16.170
Instead of a color stream,
you closed two--

01:01:16.170 --> 01:01:18.090
JAMES DONG: Yeah, that's
the probably my--

01:01:18.090 --> 01:01:18.960
yeah.

01:01:18.960 --> 01:01:22.270
But basically, just to compare
the MIME type against the

01:01:22.270 --> 01:01:25.980
supported MIME type.

01:01:25.980 --> 01:01:26.150
AUDIENCE: OK.

01:01:26.150 --> 01:01:26.390
Thank you.

01:01:26.390 --> 01:01:27.640
JAMES DONG: Sure.

01:01:30.730 --> 01:01:30.940
AUDIENCE: Hello.

01:01:30.940 --> 01:01:33.670
You haven't mentioned the
Android indicator.

01:01:33.670 --> 01:01:36.520
So are the same set of API
changes also available from

01:01:36.520 --> 01:01:38.470
the native side, like
from the NDK side?

01:01:38.470 --> 01:01:39.360
JAMES DONG: No.

01:01:39.360 --> 01:01:41.050
This is all Java API.

01:01:41.050 --> 01:01:41.310
AUDIENCE: OK.

01:01:41.310 --> 01:01:43.820
And have there been any changes
to the NDK exposed

01:01:43.820 --> 01:01:46.980
APIs at all for Jelly Bean?

01:01:46.980 --> 01:01:49.380
JAMES DONG: Not as
far as I know.

01:01:49.380 --> 01:01:54.650
This is just Java APIs,
not related to NDKs.

01:01:54.650 --> 01:01:56.500
AUDIENCE: From that looks of
your sample code, it's really

01:01:56.500 --> 01:01:59.090
just the thin wraparound from
some native code, like most

01:01:59.090 --> 01:01:59.365
[? of the ?]

01:01:59.365 --> 01:01:59.900
[? classes ?]?

01:01:59.900 --> 01:02:00.160
JAMES DONG: Yes.

01:02:00.160 --> 01:02:03.130
Those will be available at the
native layer, but it's not a

01:02:03.130 --> 01:02:04.390
part of NDK.

01:02:04.390 --> 01:02:06.550
AUDIENCE: So you can't actually
call these from--

01:02:06.550 --> 01:02:08.940
JAMES DONG: No, we are not
recommending you to call

01:02:08.940 --> 01:02:11.120
directly into the native code.

01:02:11.120 --> 01:02:13.340
AUDIENCE: Because some audio
APIs are actually

01:02:13.340 --> 01:02:14.920
exposed in the NDK.

01:02:14.920 --> 01:02:16.090
JAMES DONG: Yeah.

01:02:16.090 --> 01:02:19.950
Firstly, in the NDK, you
have the guarantee we

01:02:19.950 --> 01:02:21.510
will maintain it.

01:02:21.510 --> 01:02:27.840
But if it's not a part of the
NDK, then we may change it.

01:02:27.840 --> 01:02:29.840
AUDIENCE: If you want to write
some really low-latency codes,

01:02:29.840 --> 01:02:32.000
for example, and you have,
you're calling a native code

01:02:32.000 --> 01:02:35.990
anyway, so it kind of makes
sense to actually expose these

01:02:35.990 --> 01:02:38.300
codecs natively.

01:02:38.300 --> 01:02:38.800
JAMES DONG: Yeah.

01:02:38.800 --> 01:02:41.170
But for the Jelly Bean
release, we are

01:02:41.170 --> 01:02:42.270
not supporting it.

01:02:42.270 --> 01:02:42.394
AUDIENCE: OK.

01:02:42.394 --> 01:02:42.880
Thank you.

01:02:42.880 --> 01:02:44.400
JAMES DONG: Thanks.

01:02:44.400 --> 01:02:44.646
AUDIENCE: Hi.

01:02:44.646 --> 01:02:47.900
So you mentioned about we need
to do the way we do RGB

01:02:47.900 --> 01:02:48.620
conversion.

01:02:48.620 --> 01:02:51.310
So can the surface be backed
by a surface texture?

01:02:51.310 --> 01:02:54.300
In that case, would we get
something as a [INAUDIBLE]

01:02:54.300 --> 01:02:56.740
texture, and we can
use a [? GL ?]

01:02:56.740 --> 01:02:59.300
to do the color conversion?

01:02:59.300 --> 01:03:02.170
JAMES DONG: Sorry, I haven't
mentioned that.

01:03:02.170 --> 01:03:06.310
But it's up to you to do
the image processing.

01:03:06.310 --> 01:03:09.450
Not necessary to do
color conversion.

01:03:09.450 --> 01:03:14.440
So the media API basically gives
you the raw data, like

01:03:14.440 --> 01:03:19.400
the output of uncompressed
video frame, for example.

01:03:19.400 --> 01:03:21.730
You can do image processing
on it,

01:03:21.730 --> 01:03:22.750
AUDIENCE: It is in
what format?

01:03:22.750 --> 01:03:23.700
It is RGB?

01:03:23.700 --> 01:03:25.580
JAMES DONG: Well, the format,
it will tell you, basically,

01:03:25.580 --> 01:03:27.085
you can call Get
Audible Format.

01:03:27.085 --> 01:03:29.640
You can find out what
the format there.

01:03:29.640 --> 01:03:32.130
AUDIENCE: And it is expected
to be in an RGB format, and

01:03:32.130 --> 01:03:34.213
not in an intermediate
[INAUDIBLE] format, is it?

01:03:37.320 --> 01:03:37.680
JAMES DONG: No.

01:03:37.680 --> 01:03:41.480
It's a decode output, so it
could be, mostly it is,

01:03:41.480 --> 01:03:42.490
[INAUDIBLE]

01:03:42.490 --> 01:03:43.350
output format.

01:03:43.350 --> 01:03:44.590
AUDIENCE: So that means we
need to do the color

01:03:44.590 --> 01:03:45.850
conversion.

01:03:45.850 --> 01:03:48.470
JAMES DONG: Well, it depends
on how you do your image

01:03:48.470 --> 01:03:49.770
processing, yes.

01:03:49.770 --> 01:03:51.370
AUDIENCE: So if you don't do
image processing, and we just

01:03:51.370 --> 01:03:54.800
say, Boolean rendered to be
true, the underlying platform

01:03:54.800 --> 01:03:57.530
will do the color conversion and
render it to the surface?

01:03:57.530 --> 01:03:58.490
JAMES DONG: Yes.

01:03:58.490 --> 01:04:00.580
AUDIENCE: And are these
buffers getting

01:04:00.580 --> 01:04:01.510
copied in any way?

01:04:01.510 --> 01:04:05.330
Like, the decoder could use the
decoder resources to push

01:04:05.330 --> 01:04:05.940
the decoded buffers.

01:04:05.940 --> 01:04:11.770
Is it getting copied from the
GPU memory to CPU memory?

01:04:11.770 --> 01:04:15.970
JAMES DONG: Well, we try to
avoid mem copies as much as we

01:04:15.970 --> 01:04:20.090
can, and as far as I know, there
is no copy at all at the

01:04:20.090 --> 01:04:21.100
media level.

01:04:21.100 --> 01:04:24.750
And the only traffic
communication added for this

01:04:24.750 --> 01:04:29.350
media API is the indices of this
input buffer and output

01:04:29.350 --> 01:04:30.330
buffer arrays.

01:04:30.330 --> 01:04:36.170
Basically, that's the added
communication traffic between

01:04:36.170 --> 01:04:38.980
the application and the
media server process.

01:04:38.980 --> 01:04:42.210
AUDIENCE: But then, if the
decoder is using GPU memory,

01:04:42.210 --> 01:04:44.150
and if it has to be passed
as a [INAUDIBLE]

01:04:44.150 --> 01:04:47.920
into Java, wouldn't it have to
be a non-GPU memory for the

01:04:47.920 --> 01:04:51.650
application to do any kind
of post-processing?

01:04:51.650 --> 01:04:55.670
JAMES DONG: Well, the byte
buffer array is basically just

01:04:55.670 --> 01:04:57.570
a wrap-around of the
native buffers.

01:04:57.570 --> 01:05:04.600
You may not necessarily see a
specific format for that.

01:05:04.600 --> 01:05:06.100
AUDIENCE: So it could
be textures, too?

01:05:06.100 --> 01:05:07.596
JAMES DONG: It could be.

01:05:07.596 --> 01:05:07.904
AUDIENCE: OK.

01:05:07.904 --> 01:05:09.916
And how do we know if it's
a texture or not?

01:05:09.916 --> 01:05:11.950
JAMES DONG: You don't
know at this point.

01:05:11.950 --> 01:05:14.020
There is no API to help
you about it.

01:05:14.020 --> 01:05:15.590
AUDIENCE: But if you have to do
post-processing, we should

01:05:15.590 --> 01:05:18.655
know how it is represented
inside the byte buffer, right,

01:05:18.655 --> 01:05:20.120
in order to read those datas?

01:05:20.120 --> 01:05:22.960
JAMES DONG: Well, that data
itself actually is arranged in

01:05:22.960 --> 01:05:27.690
a way that you can access it,
basically find out the output,

01:05:27.690 --> 01:05:28.490
the [INAUDIBLE]

01:05:28.490 --> 01:05:32.860
format, and the size of it.

01:05:32.860 --> 01:05:33.470
AUDIENCE: I see.

01:05:33.470 --> 01:05:36.450
And the encoded data is passed
as like elementary strings?

01:05:36.450 --> 01:05:40.780
JAMES DONG: Sorry, I think we
are running out of time, and

01:05:40.780 --> 01:05:43.480
we can take this offline.

01:05:43.480 --> 01:05:45.200
All right?

01:05:45.200 --> 01:05:46.180
OK.

01:05:46.180 --> 01:05:47.430
Thank you.

