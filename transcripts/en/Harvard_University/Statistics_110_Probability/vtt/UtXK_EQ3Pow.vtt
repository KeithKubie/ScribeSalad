WEBVTT
Kind: captions
Language: en

00:00:00.620 --> 00:00:04.662
Okay, so we've been talking about
conditional expectation, right?

00:00:04.662 --> 00:00:12.174
And I want to do one more example of
conditional expectation, if I can.

00:00:12.174 --> 00:00:16.894
Okay, so one more example
of conditional expectation,

00:00:16.894 --> 00:00:23.399
then the main topic for today is
inequalities, as statistics inequalities.

00:00:23.399 --> 00:00:29.325
So okay, all right, so here's one
more conditional expectation problem.

00:00:29.325 --> 00:00:32.118
So all right, so
suppose you have a store and

00:00:32.118 --> 00:00:37.266
different customers show up at your store
and spend different amounts of money.

00:00:37.266 --> 00:00:40.279
Doesn't have to be a store and customers,

00:00:40.279 --> 00:00:44.226
just to have a concrete example,
I'll say it that way.

00:00:44.226 --> 00:00:45.667
That's clearly a very,

00:00:45.667 --> 00:00:49.382
very general type of setting that
would come up in a lot of cases.

00:00:49.382 --> 00:00:54.455
So I'll just say store with
a random number of customers.

00:00:54.455 --> 00:00:59.236
But you can make up your own story for
it, but just have something concrete.

00:00:59.236 --> 00:01:01.321
That's what I'm thinking of right now,

00:01:01.321 --> 00:01:06.099
random number of customers,
Which is just pretty realistic, right?

00:01:06.099 --> 00:01:09.008
You dont know how many customers
you're gonna have, and

00:01:09.008 --> 00:01:12.718
then each customers chooses to spend
some amount of money, maybe zero.

00:01:12.718 --> 00:01:15.827
And then you wanna know
how much money you got or

00:01:15.827 --> 00:01:20.067
how much profit or whatever, so
it's a very natural problem.

00:01:20.067 --> 00:01:28.182
And so let's let Xj,
Be the amount that the jth customer,

00:01:31.386 --> 00:01:35.614
Spends, Okay?

00:01:35.614 --> 00:01:42.721
And then let's assume that there are,
why did I say topics?

00:01:42.721 --> 00:01:45.298
&gt;&gt; [LAUGH]
&gt;&gt; I have no idea.

00:01:45.298 --> 00:01:48.279
Random number of customers.

00:01:48.279 --> 00:01:50.509
The number of topics in
this class is fixed,

00:01:50.509 --> 00:01:52.936
which is like I'll say
we'll do these topics.

00:01:52.936 --> 00:01:57.464
Actually it's not entirely fixed because
I might start rambling or something and

00:01:57.464 --> 00:01:58.721
not cover something.

00:01:58.721 --> 00:02:02.792
But actually it's been pretty much fixed,
we covered exactly what I want to cover

00:02:02.792 --> 00:02:05.625
for the most part since I've
been teaching this course.

00:02:05.625 --> 00:02:11.493
Okay, so random number of customers,
now let's say N,

00:02:11.493 --> 00:02:17.741
N = # of customers in a day or
in a week or in some time period.

00:02:17.741 --> 00:02:22.764
N cuts a random variable
as the number of customers.

00:02:22.764 --> 00:02:26.223
So maybe it's Poisson,
would be a reasonable.

00:02:26.223 --> 00:02:29.090
If you had to guess the distribution
maybe you might use a Poisson.

00:02:29.090 --> 00:02:32.788
I don't need to specify
the distribution right now,

00:02:32.788 --> 00:02:37.322
I'll just say that's N and
Xj is the amount jth customer spends.

00:02:37.322 --> 00:02:42.289
And let's assume Xj has mean mu and

00:02:42.289 --> 00:02:46.061
variance sigma squared.

00:02:46.061 --> 00:02:50.474
So I'm assuming that they all have
the same mean and variance for

00:02:50.474 --> 00:02:52.608
how much the customers spend.

00:02:52.608 --> 00:02:55.664
Of course, you can generalize
this problem in different ways.

00:02:55.664 --> 00:02:59.222
But for now, let's assume they
all spend the same average.

00:02:59.222 --> 00:03:02.879
They may spend different actual amounts,
but they spend the same average amount and

00:03:02.879 --> 00:03:04.179
they have the same variance.

00:03:04.179 --> 00:03:08.548
And let's assume that, N, and

00:03:08.548 --> 00:03:14.835
this sequence of expenditures
are independent.

00:03:18.999 --> 00:03:23.770
Okay, so we're not necessarily
assuming that the Xs are iid,

00:03:23.770 --> 00:03:27.469
but we are assuming that
they're independent.

00:03:27.469 --> 00:03:32.089
So [LAUGH] it's not like the second
customer sees how much the first

00:03:32.089 --> 00:03:33.355
customer spent.

00:03:33.355 --> 00:03:37.086
And wants to spend more than that
person or something like that or

00:03:37.086 --> 00:03:41.595
they come in groups and families and
decide together or something like that.

00:03:41.595 --> 00:03:46.731
They're just independent customers and
the other important assumption here is,

00:03:46.731 --> 00:03:50.986
the number of customers is independent
of the individual choices for

00:03:50.986 --> 00:03:52.175
how much to spend.

00:03:52.175 --> 00:03:54.624
So I mean that sounds like
it maybe plusable, and

00:03:54.624 --> 00:03:58.291
I'm sure you can think of examples
where this would break down, right?

00:03:58.291 --> 00:04:02.488
And it's so large that you can't
fit everyone in the store, and

00:04:02.488 --> 00:04:06.153
then maybe people start
leaving cuz it's too crowded.

00:04:06.153 --> 00:04:09.978
Or maybe then they're more
determined to buy a lot of stuff,

00:04:09.978 --> 00:04:14.785
cuz then they think this is a really good,
all kinds of things could happen.

00:04:14.785 --> 00:04:18.976
Or maybe N being very large
is an indication that they're

00:04:18.976 --> 00:04:22.480
having some really good sale that day and
so on.

00:04:22.480 --> 00:04:24.974
But anyway,
we're assuming they're independent.

00:04:24.974 --> 00:04:30.885
Okay, and then we wanna find, The mean and

00:04:30.885 --> 00:04:36.626
variance of just the total expenditure,
right?

00:04:36.626 --> 00:04:39.438
That's how much revenue would you take in?

00:04:39.438 --> 00:04:42.405
So let's call that X,

00:04:42.405 --> 00:04:47.720
X is just the sum Xj, j = 1 to capital N.

00:04:47.720 --> 00:04:49.405
So it's just a sum, right?

00:04:49.405 --> 00:04:54.434
Just total amount, but
what's unusual about this sum,

00:04:54.434 --> 00:05:00.405
compared to what we've seen generally
is that in upper index here,

00:05:00.405 --> 00:05:03.250
capital N is a random variable.

00:05:03.250 --> 00:05:07.020
So we're adding up a random
number of random variables, okay?

00:05:07.020 --> 00:05:12.969
So that's the setup, and
if we just tried to use linearity,

00:05:12.969 --> 00:05:18.583
your first thought may be about
linearity is just to sum.

00:05:18.583 --> 00:05:25.513
Then you might write E(x) =,
well there's N terms,

00:05:25.513 --> 00:05:32.162
and each one has mean mu's,
so you just go N times mu.

00:05:32.162 --> 00:05:36.472
However if you did that, how should
you immediately know this is wrong?

00:05:38.984 --> 00:05:45.047
Yeah, the right hand side is
a random variable that's a number,

00:05:45.047 --> 00:05:47.701
that's a random variable.

00:05:47.701 --> 00:05:50.300
So that would be a category error,

00:05:54.145 --> 00:05:57.590
E(x) is supposed to be a number, okay?

00:05:57.590 --> 00:06:00.604
It can be based on the various
constants we have, but

00:06:00.604 --> 00:06:02.739
it can't involve random variable.

00:06:02.739 --> 00:06:08.230
Capital N is a random variable, so that's
completely wrong, it's a category error.

00:06:08.230 --> 00:06:14.014
However that category error actually
suggests something useful to us,

00:06:14.014 --> 00:06:18.099
which is that we kind of
wish that N were a constant.

00:06:18.099 --> 00:06:21.129
Because if N is a constant,
this is not a category error anymore,

00:06:21.129 --> 00:06:23.458
just saying a constant
equals another constant.

00:06:23.458 --> 00:06:27.761
It might be true or false, but at least
it's not a category error anymore.

00:06:27.761 --> 00:06:31.995
And if N is a constant,
then really that is just linearity, okay?

00:06:31.995 --> 00:06:36.311
So this terrible blunder actually
tells us what we should do,

00:06:36.311 --> 00:06:39.060
that we wish that we knew the value of N.

00:06:39.060 --> 00:06:42.426
So we could treat it like a constant,
therefore let's just condition on N.

00:06:44.370 --> 00:06:47.928
So condition on N, that just means,
well we write E(x),

00:06:47.928 --> 00:06:51.858
I'll do this in two different notations,
it's the same thing.

00:06:51.858 --> 00:06:58.610
We'll do E(x) equals conditioning, so this
analogous to the law of total probability.

00:06:58.610 --> 00:07:03.163
The expected value of X
given big N = little n times

00:07:03.163 --> 00:07:07.507
the probability that big N = little n,
right?

00:07:07.507 --> 00:07:10.402
Just condition on the value of N.

00:07:10.402 --> 00:07:15.021
Now this is the sum, n = 0 to infinity.

00:07:15.021 --> 00:07:19.811
This conditional expectation means
we get to treat N that is known to

00:07:19.811 --> 00:07:24.023
equal little N, so
we know we have little N customers know.

00:07:24.023 --> 00:07:29.373
In that case,
we really can just apply linearity, right?

00:07:29.373 --> 00:07:32.217
So we're gonna plug in
big N = little N and

00:07:32.217 --> 00:07:35.398
here's where this assumption is important.

00:07:35.398 --> 00:07:41.105
Cuz if we didn't have this assumption what
we would do, is plug in big N = little n.

00:07:41.105 --> 00:07:44.904
But just like in earlier examples we saw,
like the two envelope paradox,

00:07:44.904 --> 00:07:47.673
we can't then forget
the condition big N = little n.

00:07:47.673 --> 00:07:51.346
In this case, though we plug in big
N = little n, they're independent.

00:07:51.346 --> 00:07:55.199
Big N is independent of the Xs,
so we can forget the condition.

00:07:55.199 --> 00:08:01.710
And so then by linearity,
we can just write down that that is,

00:08:01.710 --> 00:08:06.253
You're adding up little n things each
with mean mu, so that's mu times n.

00:08:10.161 --> 00:08:15.276
So mu is just a constant that comes out,
and what left in the sum with just

00:08:15.276 --> 00:08:20.761
left the sum of n times the PMF by
definition that's the expected value of n.

00:08:20.761 --> 00:08:24.380
So we know that that's
just mu times E of N.

00:08:26.740 --> 00:08:34.430
So the correction here is that this N
should be the expected value of N, not N.

00:08:34.430 --> 00:08:37.180
So let's also do this using Adam's Law.

00:08:39.580 --> 00:08:40.890
Which is the same thing.

00:08:40.890 --> 00:08:42.980
It's just a more complex notation.

00:08:45.080 --> 00:08:46.450
So we want E of X,

00:08:46.450 --> 00:08:51.050
we want a condition on n because then
we can just apply linearity, right?

00:08:51.050 --> 00:08:54.190
It's more familiar to deal with
a fixed number of terms rather

00:08:54.190 --> 00:08:56.310
than a random number of terms.

00:08:56.310 --> 00:09:04.281
So Adam's law, or iterated expectation
just says we can do E of E of X given N.

00:09:06.517 --> 00:09:11.344
Okay, to get E of X given N,
we just take E of X

00:09:11.344 --> 00:09:16.930
given N = little n,
which was mu times little n.

00:09:16.930 --> 00:09:19.300
And we replaced little n by big N.

00:09:19.300 --> 00:09:23.157
So that's mu times N.

00:09:23.157 --> 00:09:27.770
That is-- we treat, this notation
means treat N as a known constant.

00:09:27.770 --> 00:09:30.291
Then by linearity would just be mu N.

00:09:30.291 --> 00:09:34.231
And so again, this is mu times E of N,
so we get the same answer.

00:09:34.231 --> 00:09:37.610
So you can see that this
is more compact than that.

00:09:37.610 --> 00:09:40.542
Less writing, it's shorter and nicer.

00:09:40.542 --> 00:09:46.060
But in terms of the meaning and intuition,
both of these mean the same thing.

00:09:46.060 --> 00:09:50.110
So it's good to be comfortable kind of
writing out longhand like this, and

00:09:50.110 --> 00:09:52.170
shorthand like that are both useful.

00:09:53.820 --> 00:09:56.525
Okay, so that's the mean.

00:09:56.525 --> 00:09:57.720
Let's get the variance.

00:10:01.280 --> 00:10:03.900
So for
the variance we're gonna use Eve's Law.

00:10:08.160 --> 00:10:12.800
So Var of X = same idea, right?

00:10:12.800 --> 00:10:17.714
Condition on N, so the expected

00:10:17.714 --> 00:10:22.992
value of the variance of X given N +

00:10:22.992 --> 00:10:28.636
the variance of E of X given N, okay?

00:10:28.636 --> 00:10:36.268
Now, let's just evaluate these two
terms,the variance of X given N.

00:10:36.268 --> 00:10:41.160
To define that, we really just have to
understand what that notation means.

00:10:41.160 --> 00:10:45.020
We're treating n as known, and
then I say, what's the variance of that?

00:10:45.020 --> 00:10:47.670
Well, we know that the variance

00:10:47.670 --> 00:10:51.190
of the sum of a fixed number of
independent random variables.

00:10:51.190 --> 00:10:52.980
You just add up their variances, right?

00:10:52.980 --> 00:10:54.155
We've proved that before.

00:10:54.155 --> 00:10:57.015
There's no covariance terms because
I assume they're independent.

00:10:57.015 --> 00:10:58.923
So if we're treating n as a constant,

00:10:58.923 --> 00:11:02.080
the variance is just n times
the variance of one term.

00:11:02.080 --> 00:11:05.492
So that's just N sigma squared +,

00:11:05.492 --> 00:11:10.738
we need the variance of
the conditional expectation.

00:11:10.738 --> 00:11:14.530
But we already found that
the conditional expectation,

00:11:14.530 --> 00:11:17.058
E of X given N was mu times N, as above.

00:11:20.723 --> 00:11:24.490
Okay, then to simplify that a little bit,
we can just take out the sigma squared.

00:11:24.490 --> 00:11:29.460
So this is,
let's write it as sigma squared times

00:11:29.460 --> 00:11:33.700
E of N +, and

00:11:33.700 --> 00:11:39.196
then the mu comes out squared, so it's
mu squared times the variance of that.

00:11:39.196 --> 00:11:44.478
So that's the variance,
in terms of the min and variance of N,

00:11:44.478 --> 00:11:49.762
of course I could have said N is
Poisson or something like that,

00:11:49.762 --> 00:11:53.974
and Poisson lambda and
then it would just plug in.

00:11:53.974 --> 00:11:57.744
Lambda and lambda but
this is more general.

00:11:57.744 --> 00:12:04.935
All right, so let's quickly check whether
these answers make intuitive sense.

00:12:04.935 --> 00:12:09.207
So or the mean, I think this result
is pretty intuitive cuz it says

00:12:09.207 --> 00:12:14.102
the average amount of money that the store
will take in is the average number

00:12:14.102 --> 00:12:18.770
of customers times the average
amount that each customers spends.

00:12:18.770 --> 00:12:20.070
So that's pretty intuitive.

00:12:20.070 --> 00:12:23.730
As we've seen many times, intuition can
be wrong in this class, but in this case,

00:12:23.730 --> 00:12:25.130
I think this is pretty intuitive.

00:12:26.220 --> 00:12:29.560
And then for this one, well,

00:12:29.560 --> 00:12:34.430
let's just do a quick check that this
even makes sense in terms of the units.

00:12:34.430 --> 00:12:38.854
Now, capital N or little n we're
just talking about number of people.

00:12:38.854 --> 00:12:42.831
It doesn't really have units
that's just counting people, okay?

00:12:42.831 --> 00:12:47.735
People are not units in that sense,
so like meters,

00:12:47.735 --> 00:12:51.309
and inches, and seconds, or units.

00:12:51.309 --> 00:12:56.704
Know in the other hand, mu,
well we're measuring in some dollar or

00:12:56.704 --> 00:13:00.216
euro, or whatever let's assume dollars.

00:13:00.216 --> 00:13:04.890
So mu is in dollars, sigma squared is
dollar squared which is we like to work

00:13:04.890 --> 00:13:08.457
with standard deviation rather
than variance when we try

00:13:08.457 --> 00:13:13.383
to interpret things cuz I would rather
work with dollars than dollar squared.

00:13:13.383 --> 00:13:15.511
So this,
if we want the standard deviation,

00:13:15.511 --> 00:13:17.490
we just take the square root of that.

00:13:17.490 --> 00:13:20.720
Notice if we take the square root of this,
we're gonna get dollars.

00:13:20.720 --> 00:13:24.110
And if this were mu to the fourth or
sigma cubed or something,

00:13:24.110 --> 00:13:25.280
it wouldn't make any sense.

00:13:25.280 --> 00:13:29.400
You'd be trying to add dollars cubed to
dollars to the 4th or something like that,

00:13:29.400 --> 00:13:31.750
wouldn't make much sense, okay?

00:13:31.750 --> 00:13:36.040
So it makes sense in terms of the units,

00:13:36.040 --> 00:13:40.467
okay so similarly if you wanted the MGF.

00:13:40.467 --> 00:13:46.243
If you want the MGF of X well
again just condition on N and

00:13:46.243 --> 00:13:49.020
if we knew that N is five.

00:13:49.020 --> 00:13:51.870
Then we're just adding up five
random variables independent so

00:13:51.870 --> 00:13:55.520
we know that the MGF just
multiply those five MGFs right?

00:13:55.520 --> 00:13:57.360
So I'll be very, very straight forward.

00:13:57.360 --> 00:14:00.010
Assuming that we know
the MGF of each XJ it

00:14:00.010 --> 00:14:04.540
will be very straight forward to get
the MGF of this if n is a constant.

00:14:04.540 --> 00:14:08.730
Okay, but that tells us we can get
the MGF in general by conditioning on N.

00:14:08.730 --> 00:14:13.778
Same idea, so it can work that one out for
your self the same idea.

00:14:13.778 --> 00:14:19.205
okay, all right, so now we move

00:14:19.205 --> 00:14:24.230
on to inequalities, right,

00:14:24.230 --> 00:14:29.255
statistical inequalities,

00:14:29.255 --> 00:14:37.730
there are four of them
that we need in Stat 110.

00:14:37.730 --> 00:14:42.420
So there's a sense in which inequalities

00:14:42.420 --> 00:14:46.450
deserve a lot more attention than
they usually get in most courses.

00:14:48.620 --> 00:14:52.955
And so, there are different
ways to explain that, but

00:14:52.955 --> 00:14:58.141
one I particularly like was I had
a conversation recently with one

00:14:58.141 --> 00:15:04.850
of the leading experts in the world on the
interface between statistics and the law.

00:15:04.850 --> 00:15:12.266
And he was making a point that if you're
in court as a statistical expert witness,

00:15:12.266 --> 00:15:16.684
which is a common thing for
statisticians to do.

00:15:16.684 --> 00:15:20.100
It's a lot easier if you have
an inequality than if you have

00:15:20.100 --> 00:15:21.296
an approximation.

00:15:21.296 --> 00:15:26.084
And I know it's a common mistake in
the past in this course has been to

00:15:26.084 --> 00:15:29.620
kinda confuse approximations
with inequality.

00:15:29.620 --> 00:15:31.710
So I wanna make sure that
distinction is clear,

00:15:31.710 --> 00:15:33.900
then we'll go through the inequalities.

00:15:33.900 --> 00:15:38.166
The distinction is just that,
we did the Poisson approximation, right?

00:15:38.166 --> 00:15:39.958
That is under certain conditions,

00:15:39.958 --> 00:15:43.601
you can say that a certain distribution's
approximately Poisson, and

00:15:43.601 --> 00:15:46.959
that's gonna be a good approximation
under certain conditions.

00:15:46.959 --> 00:15:51.604
That's extremely useful because there are
a lot of problems where it's just too hard

00:15:51.604 --> 00:15:55.796
to do it exactly, but we can get a good
approximation without that much effort

00:15:55.796 --> 00:15:57.765
using Poisson approximation.

00:15:57.765 --> 00:15:58.815
For example,

00:15:58.815 --> 00:16:02.953
later in the course we'll do normal
approximation under some conditions.

00:16:02.953 --> 00:16:07.006
A lot of conditions that are pretty
realistic that normal distributions give

00:16:07.006 --> 00:16:09.290
us good approximations, I think.

00:16:09.290 --> 00:16:10.930
Those are approximations.

00:16:10.930 --> 00:16:12.450
Right now we're talking
about inequalities.

00:16:12.450 --> 00:16:14.940
Now, of course, they're related.

00:16:14.940 --> 00:16:18.780
If I prove that a certain probability
is between .36 and .38, right?

00:16:18.780 --> 00:16:25.460
So then I have bother upper and
lower bounds, right?

00:16:25.460 --> 00:16:29.292
And then I can say well, the probability
is somewhere between .36 and

00:16:29.292 --> 00:16:30.910
.38 so I would guess .37.

00:16:30.910 --> 00:16:35.840
But at least I have bounds
in both directions.

00:16:35.840 --> 00:16:40.918
But if all I say is that the probability
is less than .38, well, it could be

00:16:40.918 --> 00:16:47.189
.004 is less than .38, so that's not an
approximation, that's just a bound, okay?

00:16:47.189 --> 00:16:49.160
So that's the distinction.

00:16:49.160 --> 00:16:53.802
And the reason that this guy who I was
talking to was saying that you're much

00:16:53.802 --> 00:16:58.741
happier in court if you have an inequality
is that basically [COUGH] you can kind of

00:16:58.741 --> 00:17:00.530
imagine what would happen.

00:17:00.530 --> 00:17:05.552
But let's say I'm the expert witness and
I use my Poisson approximation on

00:17:05.552 --> 00:17:10.855
something and then you can just imagine
kind of being cross examined, right?

00:17:10.855 --> 00:17:15.826
Dr. Blitzstein,
you claim that this approximation is good,

00:17:15.826 --> 00:17:20.159
can you explain what you mean
by a good approximation.

00:17:20.159 --> 00:17:25.540
And then I'd say well,
good means that it's close to the truth.

00:17:25.540 --> 00:17:28.940
And then the lawyer could say, well,

00:17:28.940 --> 00:17:33.790
is there an accepted standard
about how close, close is?

00:17:33.790 --> 00:17:36.000
And do you know how close it is, right?

00:17:36.000 --> 00:17:38.550
And I'd have to say,
well if I knew exactly how close it is,

00:17:38.550 --> 00:17:40.555
then I'd actually know the answer, right?

00:17:40.555 --> 00:17:43.323
&gt;&gt; [LAUGH]
&gt;&gt; And there is not a standard for

00:17:43.323 --> 00:17:45.170
what does good mean.

00:17:45.170 --> 00:17:47.310
So what one person says
is a good approximation,

00:17:47.310 --> 00:17:51.210
another person could say is
a lousy approximation, right?

00:17:51.210 --> 00:17:53.620
You don't wanna get into that, right?

00:17:53.620 --> 00:17:56.245
And lawyers are good at kind of
tripping you up in that way.

00:17:56.245 --> 00:17:59.385
&gt;&gt; [LAUGH]
&gt;&gt; However, if I had an inequality,

00:17:59.385 --> 00:18:03.757
then I can just say the probability,
I've proven, is less that 0.37.

00:18:03.757 --> 00:18:09.990
And then there's basically not much
that can be said about that, right?

00:18:09.990 --> 00:18:12.887
I actually proved a theorem that says
the probability is less than 0.37, okay?

00:18:12.887 --> 00:18:17.636
It's kind of interesting, right,
because there's still randomness and

00:18:17.636 --> 00:18:21.198
uncertainty that's why we're
using probability, but

00:18:21.198 --> 00:18:25.000
we've proven a definite fact
about something random.

00:18:25.000 --> 00:18:29.600
So it's very advantageous a lot
of times to have inequalities.

00:18:29.600 --> 00:18:34.195
All right, so we're gonna talk about
the four most important inequalities

00:18:34.195 --> 00:18:35.790
arguably in statistics.

00:18:36.930 --> 00:18:43.407
The first one, we've already seen in
some forms, that's Cauchy-Schwarz.

00:18:46.330 --> 00:18:47.857
So for random variable,

00:18:47.857 --> 00:18:53.340
a lot of you have seen Cauchy-Schwarz
in the linear algebra or math class.

00:18:53.340 --> 00:18:58.210
For random variables,
Cauchy–Schwarz says that the expected

00:18:58.210 --> 00:19:03.190
value of X times Y is less than or
equal to

00:19:03.190 --> 00:19:08.372
the square root of E(X)
squared E(Y) squared.

00:19:10.152 --> 00:19:11.380
That's true.

00:19:11.380 --> 00:19:14.270
You can put absolute values
around it also if you want.

00:19:15.290 --> 00:19:18.871
Still true.
When we're talking about that geometric

00:19:18.871 --> 00:19:22.737
interpretation of conditional expectation,

00:19:22.737 --> 00:19:29.030
I mentioned the fact that this E(XY)
is playing the role of the dot product.

00:19:29.030 --> 00:19:32.502
That is, you're familiar with
the dot product of vectors and

00:19:32.502 --> 00:19:35.000
this is kind of the analog
of a dot product.

00:19:35.000 --> 00:19:38.341
So those of you familiar with
Cauchy-Schwarz just in general in your

00:19:38.341 --> 00:19:41.349
algebra, this looks the same
once you interpret it that way.

00:19:41.349 --> 00:19:44.109
But even if you've never seen
Cauchy-Schwarz before you can,

00:19:44.109 --> 00:19:46.408
you can just think about what
this inequality says and

00:19:46.408 --> 00:19:48.520
understand this just has
it's own inequality.

00:19:50.060 --> 00:19:54.980
Notice that if x and y are uncorrelated,

00:19:58.640 --> 00:20:05.710
Then by definition of uncorrelated,
then E(XY) equals E(X) E(Y).

00:20:05.710 --> 00:20:09.240
That's just the definition of
what it means to be uncorrelated.

00:20:10.740 --> 00:20:15.010
So in that case,
it would be crazy to use this inequality

00:20:15.010 --> 00:20:19.100
because we have an exact equality, right?

00:20:19.100 --> 00:20:24.722
And this is just an inequality, you can
see the direction makes sense, right?

00:20:24.722 --> 00:20:30.520
Because E(X) squared is bigger than or
equal to E(X) squared the other way,

00:20:30.520 --> 00:20:33.530
so it's true, but there will be no point.

00:20:33.530 --> 00:20:35.460
Just this equals this, okay?

00:20:35.460 --> 00:20:39.560
So where this is useful is
the case where they're correlated.

00:20:39.560 --> 00:20:40.898
I mean, it's good that it's true anyway.

00:20:40.898 --> 00:20:44.452
Just so we don't have to break things down
into separate cases, correlated case,

00:20:44.452 --> 00:20:45.385
uncorrelated case.

00:20:45.385 --> 00:20:47.065
It is always true.

00:20:47.065 --> 00:20:52.745
Okay, but to see why this is telling us
something interesting in the correlated

00:20:52.745 --> 00:20:58.223
case, kind of the cool thing about
this is if we want to compute

00:20:58.223 --> 00:21:03.950
E(XY) in general,
we'd have to use the 2D LOTUS, right?

00:21:03.950 --> 00:21:07.057
That is X and Y,
there's some joint distribution.

00:21:07.057 --> 00:21:09.961
Well, either we could do a Jacobean and

00:21:09.961 --> 00:21:14.283
find the distribution of X
times Y in the continuous case.

00:21:14.283 --> 00:21:20.080
And then so find the PDF of this or
we could use the 2D LOTUS.

00:21:20.080 --> 00:21:24.232
And that could be very,
very messy and difficult.

00:21:24.232 --> 00:21:26.316
So this is based on
the joint distribution.

00:21:26.316 --> 00:21:29.451
This is separating it out into
this is a marginal thing, and

00:21:29.451 --> 00:21:30.840
this is a marginal thing.

00:21:30.840 --> 00:21:33.700
That is,
this is the marginal second moment of X.

00:21:33.700 --> 00:21:38.270
That is, this is just the expected
value of X square, there's no Y in this

00:21:38.270 --> 00:21:41.220
expectation and there's no X in this one,
so it separates them out.

00:21:41.220 --> 00:21:42.180
So that's nice.

00:21:44.370 --> 00:21:49.492
Okay, and the interpretation,
the statistical interpretation

00:21:49.492 --> 00:21:54.140
is easiest to see in the case
where they have 0 mean.

00:21:54.140 --> 00:21:56.280
And this is the case we've
talked about before.

00:21:56.280 --> 00:22:00.292
Because if X and Y have means 0,

00:22:01.422 --> 00:22:06.560
Then the correlation between x and y.

00:22:08.380 --> 00:22:09.850
Well, in general to get the correlation,

00:22:09.850 --> 00:22:14.180
we take the covariance divided by
the product of standard deviations.

00:22:15.790 --> 00:22:20.710
The covariance is E(XY) minus E(Y),
but I assuming mean 0.

00:22:20.710 --> 00:22:23.460
So this is the covariance.

00:22:23.460 --> 00:22:27.271
And I divide by the product
of standard deviations, but

00:22:27.271 --> 00:22:31.578
the variance of X is just E(X)
squared because it has mean 0.

00:22:31.578 --> 00:22:35.364
So we just do E(X) squared
E(Y) squared square root.

00:22:38.659 --> 00:22:40.400
That would be the correlation.

00:22:42.035 --> 00:22:48.060
And let's take the absolute
value of the correlation.

00:22:48.060 --> 00:22:54.180
When we introduce correlation, we prove
that it's always between -1 and 1, right?

00:22:54.180 --> 00:22:59.010
So we already showed that
correlation is between -1 and 1.

00:22:59.010 --> 00:23:03.015
But notice that this statement
is exactly the same as

00:23:03.015 --> 00:23:06.392
the statement of Cauchy-Schwarz, okay?

00:23:06.392 --> 00:23:08.342
So it's the same thing.

00:23:08.342 --> 00:23:13.340
So in statistics Cauchy-Schwarz means
the correlation is between -1 and 1.

00:23:13.340 --> 00:23:16.839
So I'm not gonna go through a different
proof of this cuz we already proved

00:23:16.839 --> 00:23:17.690
this fact.

00:23:17.690 --> 00:23:22.821
And this is just a small extension
that says this is still true even if

00:23:22.821 --> 00:23:28.241
they don't have mean 0, and
that's just a fact from linear algebra.

00:23:28.241 --> 00:23:35.160
But this is a very nice interpretation for
our purposes, okay?

00:23:35.160 --> 00:23:38.680
So that's Cauchy-Schwarz.

00:23:38.680 --> 00:23:42.720
And you can see why it would be kind
of nice, this thing, this joint thing,

00:23:42.720 --> 00:23:45.250
and this thing may be much easier.

00:23:45.250 --> 00:23:46.220
This is an upper bound.

00:23:46.220 --> 00:23:48.827
It may not be a good approximation, right?

00:23:48.827 --> 00:23:53.140
It's probably a pretty bad if you
try to use it as an approximation.

00:23:53.140 --> 00:23:57.510
It's an upper bound, and
the strengths are simplicity and

00:23:57.510 --> 00:24:00.220
generality, not that it
gives you an approximation.

00:24:01.790 --> 00:24:09.434
Okay, so our second inequality Second
famous inequality is Jensen's inequality.

00:24:09.434 --> 00:24:12.238
Which we've already seen versions of, but

00:24:12.238 --> 00:24:17.000
we have stated it in general or
talked about it as its own topic.

00:24:17.000 --> 00:24:18.823
So Jensen's inequality,

00:24:22.988 --> 00:24:29.892
Says that if lower case
g is a convex function,

00:24:29.892 --> 00:24:35.379
and I'll remind you of what that is,

00:24:35.379 --> 00:24:40.500
then for any random variable x.

00:24:40.500 --> 00:24:47.039
Expected value of g of x is greater
than or equal to g expected value of x.

00:24:49.070 --> 00:24:50.127
So it's pretty nice.

00:24:50.127 --> 00:24:52.638
It tells you when you have convexity,

00:24:52.638 --> 00:24:56.020
it tells you which way
the inequality's gonna go.

00:24:56.020 --> 00:25:01.261
Right, one of the biggest blunders in
probability's is to move the e here.

00:25:01.261 --> 00:25:05.034
Move the E everywhere,
you can't do things like that and

00:25:05.034 --> 00:25:09.605
this tells you specifically which
way it goes for convex functions.

00:25:09.605 --> 00:25:15.260
So, okay, just to make sure everyone
knows what convex function means.

00:25:16.550 --> 00:25:18.928
If the second derivative exists,

00:25:18.928 --> 00:25:23.200
it means that just the g''(x)
greater than or equal to 0.

00:25:23.200 --> 00:25:26.807
That's usually the easiest way to
determine if a function is convex,

00:25:26.807 --> 00:25:28.590
just take the second derivative.

00:25:30.030 --> 00:25:35.268
So a simple example would be,
y equals x squared,

00:25:35.268 --> 00:25:39.413
and you can draw this U-shaped thing.

00:25:39.413 --> 00:25:43.504
Y = x squared, then the derivative is 2,

00:25:43.504 --> 00:25:47.269
which is positive, so this is convex.

00:25:47.269 --> 00:25:51.285
So at least when I took ap calculus
this was not called convex,

00:25:51.285 --> 00:25:55.456
it was called concave up which
was kind of a stupid terminology,

00:25:55.456 --> 00:25:59.645
at least no one actually uses that
once you get past ap calculus.

00:25:59.645 --> 00:26:03.334
This is convex and
we also had mnemonic, so

00:26:03.334 --> 00:26:08.970
concave is the opposite,
if the second derivative is negative,

00:26:08.970 --> 00:26:13.485
or less than or equal to 0,
then we say its concave.

00:26:13.485 --> 00:26:17.688
But we don't really need
to study that separately,

00:26:17.688 --> 00:26:23.431
because if we have a concave function,
let's just say if h is concave.

00:26:23.431 --> 00:26:29.172
I'll write, it just means the inequality
flips, and you can see that right away,

00:26:29.172 --> 00:26:34.506
because if it's concave just take
the negative of it, and that's gonna flip

00:26:34.506 --> 00:26:40.460
the second derivative from being less than
or equal 0 to being greater than equal 0.

00:26:40.460 --> 00:26:44.320
Apply Jensen's Inequality but because
of a minus sign the inequality flips.

00:26:44.320 --> 00:26:47.642
So it just says it goes the other way for
a concave.

00:26:47.642 --> 00:26:51.526
So, anyway,
we used to have a mnemonic for this,

00:26:51.526 --> 00:26:54.583
which was that concave up holds water.

00:26:54.583 --> 00:26:58.260
And have any of you heard
that mnemonic before?

00:26:58.260 --> 00:27:03.190
Would be nice of it, if it died out, so I
guess I shouldn't be repeating it, anyway,

00:27:03.190 --> 00:27:04.911
that's a very bad mnemonic.

00:27:04.911 --> 00:27:09.707
Because, first of all I don't really
see why concave up holds water is more

00:27:09.707 --> 00:27:12.750
memorable than concave down holds water.

00:27:12.750 --> 00:27:16.420
So doesn't actually tells you which
way it goes, and secondly it's wrong.

00:27:16.420 --> 00:27:21.079
So like there was, it was worth
having this numeric just so that some

00:27:21.079 --> 00:27:27.260
mathematicians can write a paper called
does concave up holds water, hold water.

00:27:27.260 --> 00:27:32.269
And the answer was no and it gave some
examples where that doesn't actually work.

00:27:32.269 --> 00:27:39.320
So the way I remember it is just
remember that this is convex.

00:27:39.320 --> 00:27:43.718
You just have to remember one simple
example of a function that's convex, okay?

00:27:43.718 --> 00:27:45.770
And then go back to this picture.

00:27:45.770 --> 00:27:50.710
And this one is an especially good example
to think about because we already knew

00:27:51.940 --> 00:27:56.760
that E of x squared is greater than or
equal to E of x squared the other way.

00:27:57.840 --> 00:28:01.244
We already knew that fact,
because variance is non-negative.

00:28:01.244 --> 00:28:05.670
So if you ever forget which
direction this inequality goes,

00:28:05.670 --> 00:28:10.616
just think back to your friendly old
parabola x squared is convex and

00:28:10.616 --> 00:28:14.809
inequality goes this way,
we already knew that, okay?

00:28:14.809 --> 00:28:17.880
So you shouldn't get confused about
which way the inequality goes.

00:28:19.400 --> 00:28:22.610
So that's an example, and we're gonna
prove that this inequality is true,

00:28:22.610 --> 00:28:24.870
just doing a couple of examples first.

00:28:26.210 --> 00:28:28.130
As I should say what's the definition,

00:28:28.130 --> 00:28:31.450
this is usually the easiest way to figure
out whether our function is convex, but

00:28:31.450 --> 00:28:34.150
the definition of convex is
a little bit more general.

00:28:34.150 --> 00:28:36.940
For example if we had
an absolute value function,

00:28:36.940 --> 00:28:42.570
you know it looks like a v shape and
that's y equals absolute value of x.

00:28:43.860 --> 00:28:48.117
So the derivative does not exist at 0,
because it has a sharp corner,

00:28:48.117 --> 00:28:50.658
that's still a convex function though.

00:28:50.658 --> 00:28:55.247
So the definition is that if you
take any two points on the curve and

00:28:55.247 --> 00:29:00.460
connect them, let's say I just pick
two points and connected them.

00:29:00.460 --> 00:29:06.193
This line segment is above the curve,
that's what it means geometrically.

00:29:06.193 --> 00:29:10.402
Pick any two points you want,
you go like that and it's above the curve,

00:29:10.402 --> 00:29:13.000
it doesn't cross below the curve.

00:29:13.000 --> 00:29:16.142
That's what it means geometrically, so

00:29:16.142 --> 00:29:20.283
that's true for
the absolute value, as well, okay.

00:29:20.283 --> 00:29:22.531
So that's the geometric interpretation,
but

00:29:22.531 --> 00:29:25.229
if the second derivative
exists it's usually easiest to

00:29:25.229 --> 00:29:28.786
just take the second derivative and
see if that's non-negative, okay.

00:29:28.786 --> 00:29:33.540
So to do a couple other quick examples,
then we'll prove this theorem.

00:29:35.830 --> 00:29:41.330
What if we have the expected value of
one over x and let's let x be positive.

00:29:45.490 --> 00:29:47.716
Positive random variable for this part.

00:29:47.716 --> 00:29:53.079
I don't have to worry about dividing by 0
or negative numbers and stuff like that.

00:29:53.079 --> 00:29:57.902
So that's x to the negative 1, so
the first derivative is minus x to

00:29:57.902 --> 00:30:02.850
the minus 2, and
the second derivative is 2 over x cubed.

00:30:02.850 --> 00:30:06.870
If x is positive,
then 2 over x cubed is positive.

00:30:06.870 --> 00:30:10.660
So this is convex as
long as x is positive.

00:30:10.660 --> 00:30:15.926
It's convex and this is greater than or
equal to 1 over E of x.

00:30:21.486 --> 00:30:25.030
So let's let X be positive for
a couple of examples.

00:30:25.030 --> 00:30:30.241
Okay, so that's true and then, what about
expected value of log x, again, I'm

00:30:30.241 --> 00:30:36.520
assuming x is positive, so I don't have to
worry about the log of a negative number.

00:30:36.520 --> 00:30:38.390
The derivative of log x is 1 over x, and

00:30:38.390 --> 00:30:42.860
then the second derivative is minus 1 over
x squared is negative, so it's concave.

00:30:42.860 --> 00:30:47.151
So we know this is gonna be less than or
equal to ln E(x).

00:30:50.519 --> 00:30:51.599
Okay, and so on.

00:30:51.599 --> 00:30:54.710
So it's pretty straight forward.

00:30:54.710 --> 00:30:58.130
So, okay,
let's prove that this is true now.

00:31:01.749 --> 00:31:08.010
And we should also discuss
when does equality hold here.

00:31:08.010 --> 00:31:09.080
In this case,

00:31:09.080 --> 00:31:13.996
we know this only equals this only
in the case when x is a constant.

00:31:13.996 --> 00:31:18.230
Right, because the variance is 0,
which means you have a constant, okay?

00:31:18.230 --> 00:31:24.310
So let's talk about that,
all right, so proof of Jensen.

00:31:27.194 --> 00:31:34.560
So let's draw a little picture again.

00:31:34.560 --> 00:31:38.277
All right, well I could think of
a more creative convex function, but

00:31:38.277 --> 00:31:40.930
I'm just gonna draw our
familiar one again.

00:31:40.930 --> 00:31:43.260
That's what a convex function looks like.

00:31:43.260 --> 00:31:48.367
Now, kind of a geometric fact about
convex functions is that what you can

00:31:48.367 --> 00:31:54.076
see in the picture draw, you would prove
this formally in an analysis of course.

00:31:54.076 --> 00:31:59.389
But just to see it geometrically, just
imagine we have this convex function and,

00:31:59.389 --> 00:32:04.960
take any point, let's say here,
and draw a tangent line And

00:32:04.960 --> 00:32:07.860
that was a pretty bad tangent line.

00:32:07.860 --> 00:32:09.380
But anyway, it looked too thick.

00:32:09.380 --> 00:32:15.200
This is supposed to be tangent here,
and then it's below the curve, right.

00:32:15.200 --> 00:32:18.940
So, or try it over here.

00:32:18.940 --> 00:32:22.780
Take a point here,
draw a tangent line, and go like that.

00:32:22.780 --> 00:32:25.390
And the point is that any of our,

00:32:25.390 --> 00:32:31.206
draw it at zero where it takes its
minimum, then we just have the x axis.

00:32:31.206 --> 00:32:37.700
And any of these tangent lines you draw,
it's gonna stay below the curve, right.

00:32:38.890 --> 00:32:44.270
So that's the whole, that's the only
fact essentially that we need for

00:32:44.270 --> 00:32:46.870
Jensen's inequality, for the proof.

00:32:46.870 --> 00:32:49.560
That if you draw this line, so
let's actually draw this line.

00:32:49.560 --> 00:32:53.670
Say this is the point mu g of mu, okay.

00:32:53.670 --> 00:32:55.680
So that's a point on the curve.

00:32:55.680 --> 00:32:58.970
And supposedly draw a tangent line there.

00:32:58.970 --> 00:33:00.910
So it goes through there.

00:33:00.910 --> 00:33:06.923
Then what we're asserting is
that g(x) is greater than or

00:33:06.923 --> 00:33:13.930
equal to, lets say, a+bx, or
that's the equation of the line.

00:33:13.930 --> 00:33:18.645
So suppose that this
line is the line y=a+bx.

00:33:20.061 --> 00:33:24.575
And the statement that this curve
stays above the line is just

00:33:24.575 --> 00:33:28.660
the statement that the curve
is above the line, okay.

00:33:28.660 --> 00:33:32.950
Once you've studied the geometry
enough to write down this inequality,

00:33:32.950 --> 00:33:38.550
then Jensen's inequality follows
very easily because this is true for

00:33:38.550 --> 00:33:44.280
every number little x, yeah,
in the domain that we're looking at.

00:33:44.280 --> 00:33:48.977
So that's also true as an inequality for
random variables, that is no matter what

00:33:48.977 --> 00:33:53.144
value x takes, here we're talking
about comparing random variables.

00:33:53.144 --> 00:33:56.783
I'm saying that this event, that this
random variable, is bigger than or

00:33:56.783 --> 00:33:58.530
equal to this one, always occurs.

00:33:58.530 --> 00:34:02.297
So we know for
sure that this is true for capital X,

00:34:02.297 --> 00:34:05.721
then just put the expectation
on both sides.

00:34:08.643 --> 00:34:16.470
And then we know that E(a+bX) is a+bE(x).

00:34:16.470 --> 00:34:20.210
I'm letting mu equal E(x),
that's the notation.

00:34:21.846 --> 00:34:27.550
So that's a+b mu, but we chose this line

00:34:27.550 --> 00:34:32.770
such that the line intersects
the curve at that point.

00:34:32.770 --> 00:34:37.950
So at that point, where x equals mu,
this is the x-axis here.

00:34:37.950 --> 00:34:42.980
At this point x equal mu,
that's the same thing as g(mu),

00:34:42.980 --> 00:34:45.010
which by definition is g(EX).

00:34:45.010 --> 00:34:46.488
So that's Jensen's inequality.

00:34:46.488 --> 00:34:51.210
Okay, so
it's a pretty short proof once you have

00:34:51.210 --> 00:34:55.900
the geometric picture in mind.

00:34:55.900 --> 00:35:00.631
You can also prove this by doing
a Taylor expansion argument, but

00:35:00.631 --> 00:35:03.134
you can look in the books for that.

00:35:03.134 --> 00:35:07.530
But I kind of like having a more geometric
perspective on it for various reasons.

00:35:09.140 --> 00:35:14.130
Okay, so
that just leaves two more inequalities.

00:35:15.860 --> 00:35:19.324
There are a lot of other inequalities
in statistics, but this is what I

00:35:19.324 --> 00:35:23.040
consider the top four, and these
are the only ones we need for this course.

00:35:24.720 --> 00:35:30.083
And you'll see why, later in this semester
you'll see why we need these ones,

00:35:30.083 --> 00:35:34.650
aside from the fact that they're
interesting in their own right.

00:35:34.650 --> 00:35:39.071
Okay, so the third one is
called Markov's inequality.

00:35:42.248 --> 00:35:46.070
The very last topic in Stat
110 is gonna be Markov chains.

00:35:46.070 --> 00:35:48.210
Same Markov, different idea.

00:35:49.960 --> 00:35:56.415
Markov's inequality says that the
probability that any random variable x,

00:35:56.415 --> 00:36:01.376
let's say absolute value of x
is greater than or equal to a,

00:36:01.376 --> 00:36:06.238
is less than or equal to expected
value of absolute value of

00:36:06.238 --> 00:36:10.333
x divided by a for
any constant a greater than 0.

00:36:16.482 --> 00:36:21.329
So, we're gonna prove this in a minute,
that the strength of this inequality is

00:36:21.329 --> 00:36:24.770
not that it gives a good
enough approximation.

00:36:24.770 --> 00:36:26.690
Its simplicity and

00:36:26.690 --> 00:36:31.640
generality, that this is completely
general for any random variable.

00:36:33.710 --> 00:36:37.920
Of course, you have a random variable
where this is infinity, and that's

00:36:37.920 --> 00:36:42.130
a pretty bad inequality, a probability
less than or equal to infinity.

00:36:42.130 --> 00:36:44.440
Okay, but it's still true.

00:36:44.440 --> 00:36:49.567
And in fact, in some cases,
the right-hand side is bigger than one.

00:36:49.567 --> 00:36:54.000
In which case this is true but
tells us absolutely nothing, okay.

00:36:54.000 --> 00:37:01.350
So this is a simple crude inequality,
and so let's prove it.

00:37:02.570 --> 00:37:07.740
Well the proof is basically
to use the fundamental

00:37:07.740 --> 00:37:12.410
bridge that I'm gonna convert
this probability of an event.

00:37:12.410 --> 00:37:17.990
It's the same as the expected value of
the indicator of that event, right.

00:37:17.990 --> 00:37:23.414
So that's the same thing
as the expected value of

00:37:23.414 --> 00:37:28.450
the indicator of x greater than or
equal to a.

00:37:28.450 --> 00:37:33.820
I'm just using this as notation for a one
if this event occurs, zero otherwise.

00:37:33.820 --> 00:37:37.520
That's the same thing as this,
right, fundamental bridge.

00:37:37.520 --> 00:37:40.787
And let's multiply by a.

00:37:42.986 --> 00:37:45.681
I'm just thinking of the same inequality
but with an a on the left, so

00:37:45.681 --> 00:37:48.880
I'm rewriting the left-hand side,
except put the a over there.

00:37:48.880 --> 00:37:55.540
And then let's see, how does this thing
compare with the absolute value of x?

00:38:01.218 --> 00:38:06.440
Okay, this inequality is always true,
let's just think why.

00:38:08.930 --> 00:38:12.830
There are, I shouldn't have an x,
do this without expectation first,

00:38:12.830 --> 00:38:14.140
then we'll bring in the expectation.

00:38:15.860 --> 00:38:19.332
Okay, so I say that this
inequality is always true because,

00:38:19.332 --> 00:38:21.300
there are only two cases to consider,
right.

00:38:21.300 --> 00:38:25.343
Anytime you have an indicator of
a random variable, either it's zero or

00:38:25.343 --> 00:38:27.240
it's one, right.

00:38:27.240 --> 00:38:33.020
If it's zero, that just says zero less
than or equal to the absolute value of x.

00:38:33.020 --> 00:38:35.060
So of course that's true, all right.

00:38:35.060 --> 00:38:36.200
That's one case.

00:38:36.200 --> 00:38:39.320
Other case is the indicator is one.

00:38:39.320 --> 00:38:41.620
So this I sub whatever is one.

00:38:41.620 --> 00:38:43.787
So left-hand side becomes a.

00:38:43.787 --> 00:38:47.526
Now in that case,
if this equaling one says that

00:38:47.526 --> 00:38:51.370
absolute value of x is greater than or
equal to a.

00:38:52.390 --> 00:38:53.650
But that's what I just said, all right.

00:38:54.870 --> 00:38:56.340
Replace this by one,

00:38:56.340 --> 00:38:59.170
it says absolute value of x square
greater than or equal to a.

00:38:59.170 --> 00:39:00.940
That's what we just said.

00:39:00.940 --> 00:39:01.925
So this is always true.

00:39:01.925 --> 00:39:05.776
So I'll just write,
note that this is true.

00:39:10.884 --> 00:39:12.409
These are random variables, but

00:39:12.409 --> 00:39:15.470
this relationship always holds
between those random variables.

00:39:17.560 --> 00:39:21.720
Once you recognize that this
is less than or equal to this,

00:39:21.720 --> 00:39:26.500
then Markov's inequality just follows
just by putting E on both sides.

00:39:28.200 --> 00:39:32.684
So the expected value of this indicator,

00:39:32.684 --> 00:39:39.870
take out the a which is a constant,
is less than or equal to E(x).

00:39:39.870 --> 00:39:43.274
And by the fundamental bridge, that's
the same thing as Markov's inequality.

00:39:43.274 --> 00:39:45.049
So that proves Markov's inequality.

00:39:48.205 --> 00:39:54.088
Okay, so if you want a little
bit of intuition on Markov's

00:39:54.088 --> 00:39:59.020
inequality, let's think
of a simple example.

00:40:00.150 --> 00:40:03.130
So, then we'll do the last inequality.

00:40:05.785 --> 00:40:10.410
All right, so here's a simple
little example to think about.

00:40:10.410 --> 00:40:14.430
Suppose that we have 100 people.

00:40:16.970 --> 00:40:20.997
Okay, and let's just think intuitively
about a couple simple questions.

00:40:20.997 --> 00:40:23.678
And we just proved this, but

00:40:23.678 --> 00:40:29.268
that doesn't make it intuitively
obvious to most people,

00:40:29.268 --> 00:40:33.540
so we should think also
about the intuition.

00:40:33.540 --> 00:40:38.204
Okay, suppose we have 100 people and

00:40:38.204 --> 00:40:42.319
suppose we ask is it possible that,

00:40:42.319 --> 00:40:47.804
let's say 95% of the people, I'll even say

00:40:47.804 --> 00:40:53.288
at least 95% of the people are, let's say,

00:40:53.288 --> 00:40:59.570
younger than the average
person in the group.

00:40:59.570 --> 00:41:01.430
Average meaning mean.

00:41:03.980 --> 00:41:05.070
Is that possible?

00:41:06.650 --> 00:41:07.240
Yes.

00:41:07.240 --> 00:41:07.740
Why?

00:41:12.808 --> 00:41:16.010
You have 100 people,
they all have different ages.

00:41:16.010 --> 00:41:18.510
Usually I do income here, but
I'm trying to avoid that.

00:41:18.510 --> 00:41:21.200
Yeah?
&gt;&gt; [INAUDIBLE]

00:41:21.200 --> 00:41:23.420
&gt;&gt; Older, one person's much older.

00:41:23.420 --> 00:41:27.380
So one of these 100 people is really,
really, really old.

00:41:27.380 --> 00:41:30.900
That one person is gonna pull
up the average a lot, right?

00:41:30.900 --> 00:41:36.380
And so then it's easily possible
that 95 people could be younger

00:41:36.380 --> 00:41:39.490
than the average right, cuz one person
could pull up the average a lot, okay?

00:41:39.490 --> 00:41:42.250
So that's pretty intuitive,
but this is possible.

00:41:42.250 --> 00:41:44.360
If we talk about median
that's a different thing.

00:41:44.360 --> 00:41:46.350
But here I just mean mean.

00:41:47.760 --> 00:41:49.480
Okay, so that's possible.

00:41:49.480 --> 00:41:50.560
The answer is yes.

00:41:52.200 --> 00:41:54.400
Now, let me ask you a similar question.

00:41:54.400 --> 00:41:59.181
Is it possible, same question,

00:41:59.181 --> 00:42:03.962
that at least 50% of the people

00:42:03.962 --> 00:42:09.439
are older than twice the average age?

00:42:17.489 --> 00:42:19.221
Take the average age, double it,

00:42:19.221 --> 00:42:21.800
can more than half of
the people be older than that?

00:42:23.240 --> 00:42:24.260
No, why not?

00:42:33.055 --> 00:42:38.565
Yeah, so, because just taking
those 50% who are more than double

00:42:38.565 --> 00:42:43.600
the average age, you just compute
their average or the total.

00:42:43.600 --> 00:42:44.980
Let's think about the total,

00:42:44.980 --> 00:42:50.060
cuz if the average is mu for
100 people, then the total is 100 mu.

00:42:50.060 --> 00:42:54.925
Now, suppose you had 50 people who
are all bigger than double the average,

00:42:54.925 --> 00:42:58.245
those people alone have already made
the average bigger than what it is,

00:42:58.245 --> 00:43:00.225
which is impossible, right?

00:43:00.225 --> 00:43:04.255
Just those people already pulled
up the average from what it was,

00:43:04.255 --> 00:43:05.905
which doesn't make sense.

00:43:05.905 --> 00:43:06.985
Okay, so that's impossible.

00:43:08.345 --> 00:43:12.590
Similarly, you can't have more
than one-third of the people.

00:43:12.590 --> 00:43:17.698
You can't have more than one third be more
than triple the average age, and so on.

00:43:17.698 --> 00:43:18.503
Right?

00:43:18.503 --> 00:43:20.910
It's impossible.

00:43:20.910 --> 00:43:23.480
That is exactly what
Markov's inequality says.

00:43:23.480 --> 00:43:24.830
So, that is the intuition.

00:43:26.200 --> 00:43:26.730
All right.
So,

00:43:26.730 --> 00:43:31.850
our last inequality is Chebyshev's,
which is another famous inequality.

00:43:35.342 --> 00:43:40.130
Chebyshev's inequality follows almost
immediately from Markov's inequality.

00:43:40.130 --> 00:43:46.100
Which is kind of ironic because in real
life Chebyshev was Markov's adviser.

00:43:46.100 --> 00:43:50.000
But the inequality, and the both of
them are famous mathematicians for

00:43:50.000 --> 00:43:57.520
other reasons, but these inequalities
are very useful but very simple.

00:43:57.520 --> 00:44:00.170
That's a crude general upper bound.

00:44:01.320 --> 00:44:08.390
Chebyshev is basically says that, well,
let me write down the inequality.

00:44:08.390 --> 00:44:13.330
It says that the probability that x minus

00:44:13.330 --> 00:44:18.570
its mean,
we're just letting mu equal e of x.

00:44:22.617 --> 00:44:24.790
Is greater than something.

00:44:24.790 --> 00:44:29.626
So here, we're just looking at
differences from the mean, greater than

00:44:29.626 --> 00:44:34.315
some number a is less than or equal
to the variance divided by a squared.

00:44:34.315 --> 00:44:36.935
So mu is the mean.

00:44:36.935 --> 00:44:38.965
And a is just, again, any positive number.

00:44:40.585 --> 00:44:42.395
Okay?
So it's kind of similar in spirit.

00:44:42.395 --> 00:44:44.665
Except we're looking at
the difference from the mean.

00:44:44.665 --> 00:44:47.765
And we get variance and
a squared thing up here.

00:44:49.235 --> 00:44:53.160
Okay?
And the other way to write this is that x

00:44:53.160 --> 00:44:58.260
minus mu greater than, let's say,
c times the standard deviation,

00:45:02.908 --> 00:45:06.620
Is less than or equal to 1 over c squared.

00:45:08.890 --> 00:45:10.220
Where, again, C is greater than 0.

00:45:10.220 --> 00:45:15.070
So this says that the probability
that x minus it's mean is more,

00:45:15.070 --> 00:45:17.750
that the probability that x is more than

00:45:17.750 --> 00:45:22.300
two standard deviations away from
it's mean is at most one quarter.

00:45:22.300 --> 00:45:22.955
Right?

00:45:22.955 --> 00:45:24.460
0.25.

00:45:24.460 --> 00:45:27.720
So you can see why kind of

00:45:27.720 --> 00:45:32.450
cool like in the normal case we have
the 68, 95, 99.7% rule, remember?

00:45:32.450 --> 00:45:37.490
Which part of it says that the probability
that a normal random variable is

00:45:37.490 --> 00:45:43.090
more than two standard deviations
away from it's mean is about 0.05.

00:45:43.090 --> 00:45:47.701
And Chebyshev's inequality says that that
would always be true, except with 0.25,

00:45:47.701 --> 00:45:53.560
that's 1 over 2 squared, rather than 0.05,
so it's a crude upper bound.

00:45:53.560 --> 00:45:59.350
And the proof is very easy once
we have Markov's inequality.

00:45:59.350 --> 00:46:03.170
And this is equivalent to this
just by letting a equal c,

00:46:03.170 --> 00:46:06.070
standard deviation,
then those are the same thing.

00:46:07.100 --> 00:46:13.030
So to prove this first line here,
let's just use Markov's inequality and

00:46:13.030 --> 00:46:18.390
just do one step first,
which is to square both sides, right?

00:46:18.390 --> 00:46:19.800
So, let's square both sides.

00:46:20.920 --> 00:46:23.840
And since we're dealing with this
is a non negative random variable.

00:46:23.840 --> 00:46:25.560
This is a positive number.

00:46:25.560 --> 00:46:29.010
It's an equivalent event if
we just square both sides.

00:46:29.010 --> 00:46:30.500
So that's squared.

00:46:30.500 --> 00:46:33.460
We can drop the absolute
value cuz we squared it.

00:46:33.460 --> 00:46:38.060
Greater than a squared, now let's use
Markov's inequality on this term.

00:46:39.950 --> 00:46:42.830
So my Markov's inequality,
this is less than or

00:46:42.830 --> 00:46:46.278
equal to the expected value
of this divided by this.

00:46:46.278 --> 00:46:53.090
Which is E(x-mu) squared,
divided by a squared.

00:46:56.239 --> 00:46:59.072
So that's just Markov,
we can put greater than or equal here,

00:46:59.072 --> 00:47:02.500
it's still true either way, but
I'll write it with greater than or equal.

00:47:04.480 --> 00:47:07.000
Markov's inequality,
so this is less than or

00:47:07.000 --> 00:47:11.280
equal to this, right, just immediately
applying Markov's inequality there.

00:47:12.280 --> 00:47:16.550
But the numerator, that's just
the definition of variance, right?

00:47:16.550 --> 00:47:22.129
So that's the variance of x divided by
a squared, which is what we wanted.

00:47:23.310 --> 00:47:26.410
Okay, so
that proves Chebyshev's inequality.

00:47:26.410 --> 00:47:28.540
And that's all for today,
so see you next time.

