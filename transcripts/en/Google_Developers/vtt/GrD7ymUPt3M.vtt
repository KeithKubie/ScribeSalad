WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:12.067
[MUSIC PLAYING]

00:00:12.067 --> 00:00:14.150
WILLIAM VAMBENEPE: This
is one of the two big data

00:00:14.150 --> 00:00:15.480
sessions for today.

00:00:15.480 --> 00:00:16.610
We're starting now.

00:00:16.610 --> 00:00:17.760
I'm William Vambenepe.

00:00:17.760 --> 00:00:20.760
I'm a product manager on
Google Cloud Platform.

00:00:20.760 --> 00:00:22.790
More specifically, I'm
responsible for big data

00:00:22.790 --> 00:00:24.510
services on the platform.

00:00:24.510 --> 00:00:26.080
And with me today is Jim Caputo.

00:00:26.080 --> 00:00:27.830
He's the engineering
manager for BigQuery.

00:00:31.510 --> 00:00:33.970
So speaking of
big data, when you

00:00:33.970 --> 00:00:36.330
talk with industry
analysts about big data,

00:00:36.330 --> 00:00:39.030
there's two factors which
are most often listed

00:00:39.030 --> 00:00:41.820
as explaining the
growth of big data.

00:00:41.820 --> 00:00:43.973
We believe there's a
third one, which is just

00:00:43.973 --> 00:00:46.306
as important [? and getting ?]
more important every day.

00:00:46.306 --> 00:00:47.681
And we'd like to
talk about that.

00:00:47.681 --> 00:00:50.454
So the two which
are usually agreed

00:00:50.454 --> 00:00:54.200
are, first, the increase in
the availability of data.

00:00:54.200 --> 00:00:57.062
And that takes many forms--
one, of course, being the fact

00:00:57.062 --> 00:00:59.249
that most business
transactions now

00:00:59.249 --> 00:01:02.408
go through [INAUDIBLE]
program and generate data.

00:01:02.408 --> 00:01:03.866
Another [INAUDIBLE] transaction.

00:01:03.866 --> 00:01:06.296
A huge number of
[INAUDIBLE] times

00:01:06.296 --> 00:01:09.212
that include [INAUDIBLE]
go through [? computer ?]

00:01:09.212 --> 00:01:11.642
programs, and a lot
of data is generated

00:01:11.642 --> 00:01:15.130
that can be [INAUDIBLE] and
help document what happens.

00:01:15.130 --> 00:01:17.790
At the same time, there is
an ever-increasing number

00:01:17.790 --> 00:01:21.240
of devices, from
cellphones to cars,

00:01:21.240 --> 00:01:24.970
and then things like sensors,
objects whose only goal in life

00:01:24.970 --> 00:01:25.762
is to produce data.

00:01:25.762 --> 00:01:27.928
That's the only thing they
do, is just pump out data

00:01:27.928 --> 00:01:30.350
all the time, and there's more
and more of them every day,

00:01:30.350 --> 00:01:31.940
every month, every year.

00:01:31.940 --> 00:01:35.580
Fortunately, the cost of
collecting and storing

00:01:35.580 --> 00:01:37.980
all these data is coming
down dramatically as well.

00:01:37.980 --> 00:01:40.600
So all those three
factors together,

00:01:40.600 --> 00:01:45.070
that explains the much
larger availability of data.

00:01:45.070 --> 00:01:47.530
The second widely
recognized reason

00:01:47.530 --> 00:01:49.580
and factor explaining
the growth of big data

00:01:49.580 --> 00:01:53.810
is the increase in the ability
to process data at this scale.

00:01:53.810 --> 00:01:56.620
And this, too, comes
in three main aspects.

00:01:56.620 --> 00:02:00.040
One of them is the invention
of new programming models, more

00:02:00.040 --> 00:02:03.460
famously MapReduce, and
now some newer large scale

00:02:03.460 --> 00:02:06.920
[INAUDIBLE] programming models,
which were built specifically

00:02:06.920 --> 00:02:10.210
to allow people to
process data at a very

00:02:10.210 --> 00:02:13.180
large scale in parallel easily.

00:02:13.180 --> 00:02:14.990
The flip side of
that is, in addition

00:02:14.990 --> 00:02:16.880
to inventing new
programming models,

00:02:16.880 --> 00:02:21.300
we've taken existing well known
familiar, popular methods,

00:02:21.300 --> 00:02:25.220
SQL chiefly, and
created services

00:02:25.220 --> 00:02:30.520
that allow SQL to work
at a much larger scale,

00:02:30.520 --> 00:02:35.070
and much faster than it was
designed to do originally.

00:02:35.070 --> 00:02:38.010
And then finally, there's
been a huge explosion

00:02:38.010 --> 00:02:40.970
in the availability of open
source software in general,

00:02:40.970 --> 00:02:42.760
but specifically for big data.

00:02:42.760 --> 00:02:45.710
And that has really
given everybody

00:02:45.710 --> 00:02:48.750
a wide portfolio of
products they can use,

00:02:48.750 --> 00:02:50.430
high quality
portfolio, something

00:02:50.430 --> 00:02:54.242
that with a constant innovation,
very, very dynamic environment,

00:02:54.242 --> 00:02:56.200
with products like Hadoop
and Spark and others.

00:02:56.200 --> 00:02:58.470
And that has really
democratized and brought

00:02:58.470 --> 00:03:00.090
down the cost of big data.

00:03:00.090 --> 00:03:03.300
So all those together, that
has really improved the ability

00:03:03.300 --> 00:03:05.090
to process data at large scale.

00:03:05.090 --> 00:03:07.490
Combining that with the
availability of data,

00:03:07.490 --> 00:03:09.310
these are the two well
understood reasons

00:03:09.310 --> 00:03:12.240
why big data is
really picking up.

00:03:12.240 --> 00:03:15.630
The third one, and the one
which I would submit is just

00:03:15.630 --> 00:03:17.560
as important and becoming
even more important

00:03:17.560 --> 00:03:20.930
and today's [INAUDIBLE]--
and the keynote this morning,

00:03:20.930 --> 00:03:23.110
I think, has made
that very clear--

00:03:23.110 --> 00:03:28.170
is the emergence of the cloud
consumption model for big data.

00:03:28.170 --> 00:03:31.150
One aspect of that is how
easily it lets you get started.

00:03:31.150 --> 00:03:33.450
You can wake up one
morning with an idea,

00:03:33.450 --> 00:03:35.880
and by the end of the day,
even if the idea involves

00:03:35.880 --> 00:03:38.299
processing terabytes of
data, you've tried it.

00:03:38.299 --> 00:03:39.090
You've iterated it.

00:03:39.090 --> 00:03:40.040
You've tested it.

00:03:40.040 --> 00:03:42.960
Maybe you've [INAUDIBLE]
or you've refined it.

00:03:42.960 --> 00:03:45.620
You've already been
able to do work with it

00:03:45.620 --> 00:03:49.000
and make progress without having
to start by thinking about,

00:03:49.000 --> 00:03:49.680
can I do it?

00:03:49.680 --> 00:03:50.370
What do I need?

00:03:50.370 --> 00:03:52.010
How do I get infrastructure?

00:03:52.010 --> 00:03:52.810
You just go do it.

00:03:52.810 --> 00:03:55.020
Whether it's a one-day
personal project

00:03:55.020 --> 00:03:57.490
or a one-week, two-week
proof of concept,

00:03:57.490 --> 00:04:00.600
you can right away
start using systems,

00:04:00.600 --> 00:04:03.330
and do that at a
very, very low cost.

00:04:03.330 --> 00:04:05.870
And then, for those
ideas and projects

00:04:05.870 --> 00:04:09.400
that actually prove to be useful
that you want to take forward,

00:04:09.400 --> 00:04:11.150
where you are
already in the cloud,

00:04:11.150 --> 00:04:13.890
you can scale those
pretty much infinitely

00:04:13.890 --> 00:04:16.279
and keep doing that
at a very, very, very

00:04:16.279 --> 00:04:17.910
low cost of ownership.

00:04:17.910 --> 00:04:19.285
I think [INAUDIBLE]
hopefully you

00:04:19.285 --> 00:04:20.779
were all here early
in the morning

00:04:20.779 --> 00:04:22.370
when Urs talked about
the price drops.

00:04:22.370 --> 00:04:25.460
And he made it clear, this
is not a one-time price drop.

00:04:25.460 --> 00:04:30.940
This is a continuous effort to
really bring the cost on cloud

00:04:30.940 --> 00:04:33.680
as low as allowed by the
declining cost of hardware.

00:04:33.680 --> 00:04:37.407
So cloud is the cheapest
place for big data,

00:04:37.407 --> 00:04:39.990
and it will remain the cheapest
place and most effective place

00:04:39.990 --> 00:04:41.810
for big data processing.

00:04:41.810 --> 00:04:44.890
And finally, the third aspect
of the cloud [INAUDIBLE]

00:04:44.890 --> 00:04:47.990
for big data is that on
a platform like Google,

00:04:47.990 --> 00:04:52.990
you have a chance of using a mix
of platform-provided services,

00:04:52.990 --> 00:04:56.870
fully hosted, fully managed, as
well as open source software.

00:04:56.870 --> 00:04:58.880
And it's not one or the other.

00:04:58.880 --> 00:05:00.210
You use them together.

00:05:00.210 --> 00:05:01.550
They share the same data.

00:05:01.550 --> 00:05:03.750
They share the same
authentication system.

00:05:03.750 --> 00:05:06.720
It's one environment,
where you can seamlessly

00:05:06.720 --> 00:05:09.800
make use of one or the other,
use them together, transition

00:05:09.800 --> 00:05:10.742
between them.

00:05:10.742 --> 00:05:13.200
And that's one aspect that
we're going to illustrate today.

00:05:13.200 --> 00:05:15.170
So we have two
sessions on big data.

00:05:15.170 --> 00:05:17.910
In this session, we'll
focus on BigQuery.

00:05:17.910 --> 00:05:22.060
And Jim is going to describe
new features, new scalability

00:05:22.060 --> 00:05:26.320
achievements, more detail
about a new pricing model.

00:05:26.320 --> 00:05:29.210
And as he describes all that,
keep in mind, everything

00:05:29.210 --> 00:05:31.820
he's describing, you get
that with no installation,

00:05:31.820 --> 00:05:34.030
no configuration, no monitoring.

00:05:34.030 --> 00:05:36.151
You just come, send your
query, get your result.

00:05:36.151 --> 00:05:36.650
That's it.

00:05:36.650 --> 00:05:38.670
This is the best
of a fully hosted,

00:05:38.670 --> 00:05:42.010
the easiest to use, most
productive services.

00:05:42.010 --> 00:05:44.210
And then, at the
same time, we'll

00:05:44.210 --> 00:05:48.250
have at 3:30 here in the
same room another session all

00:05:48.250 --> 00:05:50.930
dedicated to running
open source big data

00:05:50.930 --> 00:05:53.680
processing software on
Google Cloud Platform.

00:05:53.680 --> 00:05:55.620
And there, we're
going to show how

00:05:55.620 --> 00:05:57.660
the quality of the
infrastructure,

00:05:57.660 --> 00:06:00.990
the compute, the
storage, the network,

00:06:00.990 --> 00:06:03.390
give you excellent
performance and excellent cost

00:06:03.390 --> 00:06:05.649
performance running open
[? source beta ?] software.

00:06:05.649 --> 00:06:07.190
We'll talk about
something that we've

00:06:07.190 --> 00:06:10.010
done to implement-- [INAUDIBLE]
to implement open source

00:06:10.010 --> 00:06:12.670
APIs to natively connect
with the sources.

00:06:12.670 --> 00:06:14.970
We'll talk about some tooling
that we've put in place.

00:06:14.970 --> 00:06:17.440
We'll describe in
detail how much value

00:06:17.440 --> 00:06:20.470
you can get from running open
source software for big data

00:06:20.470 --> 00:06:21.410
at Google.

00:06:21.410 --> 00:06:23.743
And keep in mind, between
these two sessions-- hopefully

00:06:23.743 --> 00:06:26.270
you'll be able to attend both--
this is really one platform.

00:06:26.270 --> 00:06:27.978
We've broken it in
two sessions, but it's

00:06:27.978 --> 00:06:30.230
one dataset, one
environment for you

00:06:30.230 --> 00:06:31.960
to bring your
[? big data ?] processing.

00:06:31.960 --> 00:06:34.250
So for now, I'll
yield to Jim, and he's

00:06:34.250 --> 00:06:38.329
going to give us all the
goodies about BigQuery.

00:06:38.329 --> 00:06:39.429
JIM CAPUTO: Thanks.

00:06:39.429 --> 00:06:40.970
Thanks, William,
and thanks everybody

00:06:40.970 --> 00:06:41.960
for coming this morning.

00:06:41.960 --> 00:06:44.376
I'm really excited to be here
to tell you about the latest

00:06:44.376 --> 00:06:46.990
innovations we've got
going on at BigQuery.

00:06:46.990 --> 00:06:49.400
So we launched BigQuery a
little under two years ago,

00:06:49.400 --> 00:06:51.410
May of 2012.

00:06:51.410 --> 00:06:53.620
And what we brought to
market for the first time

00:06:53.620 --> 00:06:56.710
was the ability to scan
multi-terabyte datasets

00:06:56.710 --> 00:06:58.331
in just seconds.

00:06:58.331 --> 00:07:01.010
This capability was
previously unheard of.

00:07:01.010 --> 00:07:04.620
This unlocked a whole new
set of use cases, the ability

00:07:04.620 --> 00:07:09.230
to finally do interactive
analysis over huge datasets.

00:07:09.230 --> 00:07:12.724
We combine that with
tremendous ease of use.

00:07:12.724 --> 00:07:14.640
No provisioning, as
William was talking about.

00:07:14.640 --> 00:07:16.931
No figuring out how many
machines do I need to do this,

00:07:16.931 --> 00:07:19.030
how many cores would
it take to process

00:07:19.030 --> 00:07:20.995
a terabyte of data in seconds.

00:07:20.995 --> 00:07:22.370
And we make it
convenient, right?

00:07:22.370 --> 00:07:26.290
The convenience of SQL and open
interfaces like a REST API,

00:07:26.290 --> 00:07:31.560
web user experience, SQL-92
compatible, ODBC driver.

00:07:31.560 --> 00:07:35.240
And finally, we introduced this
notion of big data storage.

00:07:35.240 --> 00:07:36.771
We wanted to improve
the experience

00:07:36.771 --> 00:07:39.020
around how you actually
interact and manage your data.

00:07:39.020 --> 00:07:41.130
Most of the large data
processing, big data

00:07:41.130 --> 00:07:43.610
processing, is done
on large files,

00:07:43.610 --> 00:07:45.370
large quantities of files.

00:07:45.370 --> 00:07:47.339
We felt like a familiar
database structure

00:07:47.339 --> 00:07:48.880
is what people really
wanted, though.

00:07:48.880 --> 00:07:50.339
They wanted easier
data management.

00:07:50.339 --> 00:07:52.463
They wanted to be able to
ACL and share their data.

00:07:52.463 --> 00:07:54.830
After all, you're moving all
this data into the cloud.

00:07:54.830 --> 00:07:56.371
Shouldn't it be
easier to collaborate

00:07:56.371 --> 00:07:57.690
with your coworkers?

00:07:57.690 --> 00:08:01.695
And we made it easy and fast to
get your data into the system.

00:08:01.695 --> 00:08:03.820
So we've been hard at work
over the last two years,

00:08:03.820 --> 00:08:06.970
and we've added quite a few
pieces of new functionality

00:08:06.970 --> 00:08:09.140
along the way.

00:08:09.140 --> 00:08:11.862
Shortly after launch, we
introduced batch processing.

00:08:11.862 --> 00:08:14.070
So you have the ability to
queue up all of your jobs,

00:08:14.070 --> 00:08:15.570
and then we would
just run them when

00:08:15.570 --> 00:08:17.090
idle resources were available.

00:08:17.090 --> 00:08:18.760
Clearly not every
use case requires

00:08:18.760 --> 00:08:20.580
interactive performance.

00:08:20.580 --> 00:08:22.790
The Excel connector made
it easy to use Excel

00:08:22.790 --> 00:08:24.570
as your primary front
end, if you desired,

00:08:24.570 --> 00:08:27.514
and then use BigQuery as your
primary big data back end.

00:08:27.514 --> 00:08:29.180
I've got a lot more
in terms of partners

00:08:29.180 --> 00:08:31.980
to talk about in a moment.

00:08:31.980 --> 00:08:34.000
Next, we added JSON
import, in addition

00:08:34.000 --> 00:08:36.419
to sort of standard CSV imports.

00:08:36.419 --> 00:08:38.740
JSON's obviously valuable,
pretty ubiquitous standard

00:08:38.740 --> 00:08:40.360
in terms of moving data around.

00:08:40.360 --> 00:08:41.840
But more importantly,
it unlocked

00:08:41.840 --> 00:08:44.020
nested and repeated
fields for us,

00:08:44.020 --> 00:08:45.990
which is a pretty powerful,
although admittedly

00:08:45.990 --> 00:08:49.644
a little bit complex, data
structure to make use of.

00:08:49.644 --> 00:08:51.560
We also made it possible
to bring data stores.

00:08:51.560 --> 00:08:53.630
If you have a Cloud
Datastore instance,

00:08:53.630 --> 00:08:56.230
you can back that up, and then
bring it right into BigQuery

00:08:56.230 --> 00:08:58.160
for analysis.

00:08:58.160 --> 00:09:00.860
Or as mentioned earlier, at
the beginning of last year,

00:09:00.860 --> 00:09:04.240
Big JOIN, the ability to join
two arbitrarily large datasets

00:09:04.240 --> 00:09:08.459
together, regardless of JOIN
key that you're operating on.

00:09:08.459 --> 00:09:10.500
Big aggregates, the ability
to group on something

00:09:10.500 --> 00:09:13.615
that has potentially
millions of distinct values.

00:09:13.615 --> 00:09:15.490
So for instance, you're
grouping on something

00:09:15.490 --> 00:09:16.948
like user ID, that's
going to scale

00:09:16.948 --> 00:09:18.436
with the size of your system.

00:09:18.436 --> 00:09:19.310
And then time stamps.

00:09:19.310 --> 00:09:22.880
So we're furthering that goal of
making this more database-like.

00:09:22.880 --> 00:09:26.190
A standard timestamp data type,
and the company functions that

00:09:26.190 --> 00:09:28.630
would go with that.

00:09:28.630 --> 00:09:30.510
After that, large query results.

00:09:30.510 --> 00:09:33.520
So not only were you able to
query data of unlimited size,

00:09:33.520 --> 00:09:36.129
you can now generate
data of unlimited size.

00:09:36.129 --> 00:09:37.670
So all of a sudden,
we started seeing

00:09:37.670 --> 00:09:39.044
more and more use
cases emerging.

00:09:39.044 --> 00:09:41.890
We started with that interactive
analysis, started seeing people

00:09:41.890 --> 00:09:45.587
doing more ETL processing, for
instance, on top of BigQuery.

00:09:45.587 --> 00:09:47.920
Query caching, just to help
out application development,

00:09:47.920 --> 00:09:49.929
reduce costs,
speed things along.

00:09:49.929 --> 00:09:50.720
Analytic functions.

00:09:50.720 --> 00:09:54.550
We continue to invest
in our SQL engine.

00:09:54.550 --> 00:09:56.435
Near the end of last
year, the streaming API.

00:09:56.435 --> 00:09:58.530
It actually launched
with 100 rows per second

00:09:58.530 --> 00:10:00.620
initially, combined
with that feature you

00:10:00.620 --> 00:10:02.800
saw that Eric demonstrated,
with table decorators.

00:10:02.800 --> 00:10:05.180
So the ability to
just slice into-- I

00:10:05.180 --> 00:10:08.980
only want to see the last two
hours of data, for instance.

00:10:08.980 --> 00:10:11.630
At the end of last year,
Google Analytics integration.

00:10:11.630 --> 00:10:14.490
I'll talk more about
this in a moment.

00:10:14.490 --> 00:10:17.020
And finally, today, a bunch
of new features coming out.

00:10:17.020 --> 00:10:19.020
We've dramatically increased
the streaming rate,

00:10:19.020 --> 00:10:22.510
as Eric showed you, up to
100,000 rows per table now.

00:10:22.510 --> 00:10:25.430
Table views, again, extending on
that goal of advanced big data

00:10:25.430 --> 00:10:26.120
storage.

00:10:26.120 --> 00:10:27.310
We want this easy to use.

00:10:27.310 --> 00:10:29.059
We want you to be able
to reuse this stuff

00:10:29.059 --> 00:10:33.480
and share large queries across
your organization more easily.

00:10:33.480 --> 00:10:34.440
Table wild cards.

00:10:34.440 --> 00:10:36.690
So also mentioned this
morning, the ability

00:10:36.690 --> 00:10:39.550
to very easily shard your
data across many tables.

00:10:39.550 --> 00:10:42.180
We want to make that easy to
use from a query perspective.

00:10:42.180 --> 00:10:43.650
So for instance,
if you've chosen

00:10:43.650 --> 00:10:45.790
to shard your data
across days, you

00:10:45.790 --> 00:10:48.230
may want to use a wild card
that says give me that table,

00:10:48.230 --> 00:10:50.310
and here's the date
range I want to apply.

00:10:50.310 --> 00:10:52.120
Automatically just
works for you.

00:10:52.120 --> 00:10:53.070
JSON functions.

00:10:53.070 --> 00:10:55.410
So if you've loaded up a bunch
of JSON in a given field,

00:10:55.410 --> 00:10:57.900
you can use a JSON function
to extract out values

00:10:57.900 --> 00:10:59.370
automatically.

00:10:59.370 --> 00:11:02.500
And finally, a whole bunch
of other SQL improvements,

00:11:02.500 --> 00:11:04.390
trying to make us much
more closely aligned

00:11:04.390 --> 00:11:06.520
with the standard
SQL-92 spec, as well

00:11:06.520 --> 00:11:07.790
as a whole bunch of
additional improvements,

00:11:07.790 --> 00:11:09.456
like some of those
analytical functions.

00:11:14.340 --> 00:11:15.560
So it's not just us.

00:11:15.560 --> 00:11:18.320
The ecosystem around us has been
going at an incredible speed.

00:11:18.320 --> 00:11:19.920
We've really been
impressed by this.

00:11:19.920 --> 00:11:21.980
We've got a great partnership,
for instance, with Tableau.

00:11:21.980 --> 00:11:23.200
We've spent quite a
bit of time working

00:11:23.200 --> 00:11:25.000
on the technical
integration there.

00:11:25.000 --> 00:11:26.720
We've been able to
see whole new use

00:11:26.720 --> 00:11:28.390
cases emerging in
that space as well.

00:11:28.390 --> 00:11:30.098
Actually bringing
interactive performance

00:11:30.098 --> 00:11:32.380
to a visualization and data
analysis tool like that

00:11:32.380 --> 00:11:33.520
has been tremendous.

00:11:33.520 --> 00:11:35.115
I saw a great
example of pandas--

00:11:35.115 --> 00:11:38.170
so Python and
analytical functions

00:11:38.170 --> 00:11:40.041
done there in the
open source community.

00:11:40.041 --> 00:11:42.040
As you can see, a whole
bunch of other providers

00:11:42.040 --> 00:11:43.112
all working around us.

00:11:43.112 --> 00:11:45.320
So thanks for all your
efforts out on the open source

00:11:45.320 --> 00:11:46.384
and partner community.

00:11:46.384 --> 00:11:48.550
We're sort of excited to
see where this can go next,

00:11:48.550 --> 00:11:52.180
building on top of
the BigQuery platform.

00:11:52.180 --> 00:11:53.100
So BigQuery streaming.

00:11:53.100 --> 00:11:54.849
Let's dive into this
a little bit further.

00:11:54.849 --> 00:11:58.240
As demoed this morning, we
can do 100,000 rows per table

00:11:58.240 --> 00:12:01.990
at a low cost of just one penny
per 100,000 rows in real time

00:12:01.990 --> 00:12:04.090
availability of that data.

00:12:04.090 --> 00:12:06.970
This dramatically simplifies
the amount of infrastructure

00:12:06.970 --> 00:12:09.430
that you have to build
and maintain on your side.

00:12:09.430 --> 00:12:11.272
No longer do you need
to batch up records,

00:12:11.272 --> 00:12:12.980
figure out when you
want to submit those,

00:12:12.980 --> 00:12:14.646
and at what latency
or frequency they're

00:12:14.646 --> 00:12:16.430
going to come into the system.

00:12:16.430 --> 00:12:18.380
And the use cases
are endless here.

00:12:18.380 --> 00:12:21.580
So for starters, you can see an
example like that distributed

00:12:21.580 --> 00:12:23.200
application that Eric built.

00:12:23.200 --> 00:12:25.880
But we're starting to see
other examples emerging.

00:12:25.880 --> 00:12:27.690
And working with a
customer, Streak,

00:12:27.690 --> 00:12:30.110
one of our early adopters,
they were impressed by this.

00:12:30.110 --> 00:12:32.254
They were able to use
streaming to reduce

00:12:32.254 --> 00:12:33.920
the amount of
infrastructure, of course,

00:12:33.920 --> 00:12:35.990
to simplify the
infrastructure to bring

00:12:35.990 --> 00:12:37.574
their data into the system.

00:12:37.574 --> 00:12:39.240
And so great, they
were able to continue

00:12:39.240 --> 00:12:40.920
using their standard reporting.

00:12:40.920 --> 00:12:43.950
What they found were new
cases, new use cases emerging.

00:12:43.950 --> 00:12:46.619
All of a sudden, they had
user support scenarios

00:12:46.619 --> 00:12:47.410
that would come in.

00:12:47.410 --> 00:12:49.243
And rather than have
to go to one system end

00:12:49.243 --> 00:12:51.064
to figure out the
debug logs, they

00:12:51.064 --> 00:12:52.980
could go to a single
system, look at the debug

00:12:52.980 --> 00:12:56.670
logs right alongside what
was the historical behavior

00:12:56.670 --> 00:12:57.500
for this customer.

00:12:57.500 --> 00:13:01.150
What was their user experience
in the last month or so?

00:13:01.150 --> 00:13:03.360
So we think this is the
best of both worlds.

00:13:03.360 --> 00:13:06.906
It gives you the flexibility
to do that real time processing

00:13:06.906 --> 00:13:08.280
that you desire
when you need it,

00:13:08.280 --> 00:13:11.830
that low latency processing,
while also bringing that data

00:13:11.830 --> 00:13:14.750
into a storage system
that's capable of doing

00:13:14.750 --> 00:13:17.090
your full historical analysis.

00:13:17.090 --> 00:13:18.705
So there's nothing
disjoint here.

00:13:18.705 --> 00:13:20.330
Everything is all
done in one location.

00:13:23.427 --> 00:13:25.010
Moving on, next, I'd
like to deep dive

00:13:25.010 --> 00:13:27.690
for sure into Google Analytics.

00:13:27.690 --> 00:13:30.520
For the first time,
Google Analytics customers

00:13:30.520 --> 00:13:34.400
are now capable of bringing
in completely unsampled,

00:13:34.400 --> 00:13:36.515
row-by-row detail of
their analytics data.

00:13:36.515 --> 00:13:38.060
This was previously unavailable.

00:13:38.060 --> 00:13:41.634
And the volume of data
here is tremendous.

00:13:41.634 --> 00:13:43.050
Bringing this data
into the system

00:13:43.050 --> 00:13:45.270
is done completely
for you automatically

00:13:45.270 --> 00:13:48.950
via managed pipeline
behind the scenes.

00:13:48.950 --> 00:13:51.212
Bringing this data in couldn't
be any easier for you.

00:13:51.212 --> 00:13:53.670
All you need to do is sign up
for Google Analytics Premium.

00:13:53.670 --> 00:13:56.650
The data automatically
arrives on a daily basis.

00:13:56.650 --> 00:13:58.400
So we've already seen
quick adoption here.

00:13:58.400 --> 00:14:00.920
The number of use cases
are essentially endless.

00:14:00.920 --> 00:14:03.130
And talking with some
of our early customers

00:14:03.130 --> 00:14:05.554
and website owners, the first
thing they were amazed by

00:14:05.554 --> 00:14:07.470
is the volume of data
that they could actually

00:14:07.470 --> 00:14:09.230
analyze inside of BigQuery.

00:14:09.230 --> 00:14:11.780
No longer were they looking
at month over month changes.

00:14:11.780 --> 00:14:13.613
They were looking at
year over year changes,

00:14:13.613 --> 00:14:15.700
over many years,
something just previously

00:14:15.700 --> 00:14:19.670
they couldn't actually
endeavor to do.

00:14:19.670 --> 00:14:22.550
Secondarily, we've seen
quickly customers are actually

00:14:22.550 --> 00:14:24.809
uploading their transactional
information, which

00:14:24.809 --> 00:14:25.600
is pretty exciting.

00:14:25.600 --> 00:14:28.700
Because now they can do those
big joins, those data mashups.

00:14:28.700 --> 00:14:30.780
They can actually
look at, in this case,

00:14:30.780 --> 00:14:32.450
the customer is interested
in trying to figure out

00:14:32.450 --> 00:14:34.533
why are some users spending
a fair amount of money

00:14:34.533 --> 00:14:36.567
on their site, they come
back, but they're not

00:14:36.567 --> 00:14:37.650
repeating their purchases.

00:14:37.650 --> 00:14:38.810
What changed?

00:14:38.810 --> 00:14:40.880
And then there's
another example as well.

00:14:40.880 --> 00:14:42.970
So the value here
is incredibly high.

00:14:42.970 --> 00:14:44.510
We've seen tremendous adoption.

00:14:44.510 --> 00:14:47.830
In just a few months, a
whole bunch of huge customers

00:14:47.830 --> 00:14:49.310
have been coming
online, huge data

00:14:49.310 --> 00:14:50.875
volumes and
interesting use cases.

00:14:56.877 --> 00:14:58.460
So I wanted to look
at a few other use

00:14:58.460 --> 00:15:01.460
cases I found interesting.

00:15:01.460 --> 00:15:02.640
Interactions marketing.

00:15:02.640 --> 00:15:05.347
Interactions does
on-site retail marketing.

00:15:05.347 --> 00:15:06.930
So they're actually
out at storefronts

00:15:06.930 --> 00:15:08.740
doing marketing campaigns.

00:15:08.740 --> 00:15:10.800
So actually, maybe a
sample of some sort

00:15:10.800 --> 00:15:11.984
that they're distributing.

00:15:11.984 --> 00:15:13.650
And they wanted to
understand the answer

00:15:13.650 --> 00:15:15.660
to what I thought was a pretty
straightforward question.

00:15:15.660 --> 00:15:17.950
They wanted to understand
what's the impact of weather

00:15:17.950 --> 00:15:21.470
and weather events on these
retail marketing campaigns.

00:15:21.470 --> 00:15:23.052
As it turns out,
their analysts had

00:15:23.052 --> 00:15:24.760
struggled to do this
for quite some time.

00:15:24.760 --> 00:15:26.301
The data volumes
were just too large.

00:15:26.301 --> 00:15:28.600
They couldn't perform
that data mashup.

00:15:28.600 --> 00:15:30.310
But I love this quote
here from Giovanni.

00:15:30.310 --> 00:15:34.080
He says, "Analyses would require
hours or days to complete.

00:15:34.080 --> 00:15:36.920
With Google, it
took just minutes."

00:15:36.920 --> 00:15:40.290
This timed insight was
previously impossible.

00:15:40.290 --> 00:15:43.150
So using the power of Tableau
as their front end and BigQuery

00:15:43.150 --> 00:15:46.220
as their big data back
end, deriving insights

00:15:46.220 --> 00:15:48.120
that were previously
not possible to generate

00:15:48.120 --> 00:15:49.145
in the past.

00:15:51.670 --> 00:15:53.580
This next one came from
Shine Technologies.

00:15:53.580 --> 00:15:55.360
I was really interested
to learn more about this.

00:15:55.360 --> 00:15:56.818
In fact, they're
here today, so I'm

00:15:56.818 --> 00:15:59.090
excited to have them
in the audience here.

00:15:59.090 --> 00:16:02.420
So Graham actually posted a
tweet a couple of months ago

00:16:02.420 --> 00:16:04.760
and said, hey, we've
been really successful

00:16:04.760 --> 00:16:07.010
with this BigQuery product.

00:16:07.010 --> 00:16:09.140
What it turned out
was, they were actually

00:16:09.140 --> 00:16:11.970
struggling in the past to do
the data analysis they needed

00:16:11.970 --> 00:16:14.980
to do on running on just a
small sample of the dataset

00:16:14.980 --> 00:16:16.600
that they had in question.

00:16:16.600 --> 00:16:19.170
They're using ad serving logs,
so huge numbers of impressions

00:16:19.170 --> 00:16:22.120
and counts that they were trying
to push through the system.

00:16:22.120 --> 00:16:23.160
And they were amazed.

00:16:23.160 --> 00:16:25.220
In a two hour proof
of concept effort,

00:16:25.220 --> 00:16:28.240
they went from getting the data
loaded to immediately being

00:16:28.240 --> 00:16:31.700
able to answer questions that
were previously impossible.

00:16:31.700 --> 00:16:32.810
I love the last line here.

00:16:32.810 --> 00:16:35.450
He says, "The SQL-like query
language is an easy transition

00:16:35.450 --> 00:16:38.480
to make for any engineer
and much quicker and easier

00:16:38.480 --> 00:16:40.420
than using a MapReduce model."

00:16:40.420 --> 00:16:42.012
Again, simplicity
along with scale.

00:16:42.012 --> 00:16:44.220
We're trying to drive both
of these at the same time.

00:16:46.990 --> 00:16:50.940
So I mentioned we started with
this arguably relatively narrow

00:16:50.940 --> 00:16:51.500
offering.

00:16:51.500 --> 00:16:53.119
It was high performance,
really fast,

00:16:53.119 --> 00:16:55.410
but it wasn't really intended
to solve for something as

00:16:55.410 --> 00:16:56.990
big as data warehousing.

00:16:56.990 --> 00:16:59.570
Well, that's definitely changed
over the last two years.

00:16:59.570 --> 00:17:01.020
Our friends over
at Motorola have

00:17:01.020 --> 00:17:03.810
made huge use of the
system and are using it

00:17:03.810 --> 00:17:05.880
as their primary data
warehouse for some

00:17:05.880 --> 00:17:08.890
of their most important data.

00:17:08.890 --> 00:17:11.099
So to walk through this,
their most important data

00:17:11.099 --> 00:17:13.069
is their telemetry data
that they're collecting off

00:17:13.069 --> 00:17:14.777
all of their devices
out there, obviously

00:17:14.777 --> 00:17:17.089
hundreds of millions
of Motorola phones.

00:17:17.089 --> 00:17:19.810
They ship this data up through
App Engine multiple times

00:17:19.810 --> 00:17:21.200
per day.

00:17:21.200 --> 00:17:23.260
App Engine does some
lightweight preprocessing,

00:17:23.260 --> 00:17:26.426
and then drops it into cloud
storage for archival purposes.

00:17:26.426 --> 00:17:28.550
Which is fine, but it's
not particularly actionable

00:17:28.550 --> 00:17:29.440
sitting there.

00:17:29.440 --> 00:17:31.770
They instead, as
quickly as possible,

00:17:31.770 --> 00:17:33.580
move that data over
to BigQuery storage,

00:17:33.580 --> 00:17:35.940
where they can begin
to take action on it.

00:17:35.940 --> 00:17:37.970
Well, Motorola was
running the majority

00:17:37.970 --> 00:17:40.570
of their data processing
in-house previously.

00:17:40.570 --> 00:17:42.200
They had a rather
large Hadoop instance

00:17:42.200 --> 00:17:44.340
on their own private cluster.

00:17:44.340 --> 00:17:46.130
And they came to us
less than a year ago

00:17:46.130 --> 00:17:49.400
and really started investing
in the cloud platform.

00:17:49.400 --> 00:17:51.770
As it turns out, they went
from basically 0 bytes

00:17:51.770 --> 00:17:54.460
in BigQuery a little
over a year ago to now

00:17:54.460 --> 00:17:57.620
having many petabytes
of data in storage.

00:17:57.620 --> 00:18:00.220
And they actually are still
using those same Hadoop

00:18:00.220 --> 00:18:00.880
MapReduces.

00:18:00.880 --> 00:18:02.549
So they're running
those MapReduces now

00:18:02.549 --> 00:18:04.840
on top of the logs that have
been brought into BigQuery

00:18:04.840 --> 00:18:07.970
storage and generating
the downstream tables

00:18:07.970 --> 00:18:10.437
they need for
analysts to consume.

00:18:10.437 --> 00:18:12.520
On top of that, we've seen
them actually migrating

00:18:12.520 --> 00:18:14.990
a decent number of their
workflows over to BigQuery.

00:18:14.990 --> 00:18:17.510
So MapReduce is great
in some instances,

00:18:17.510 --> 00:18:19.380
SQL solves for other cases.

00:18:19.380 --> 00:18:21.492
Might be simpler to use
in some circumstances.

00:18:21.492 --> 00:18:23.950
And then finally, once that
data is all processed and ready

00:18:23.950 --> 00:18:26.740
for use, it's moved
downstream in general for all

00:18:26.740 --> 00:18:28.410
of their analysts
to take advantage

00:18:28.410 --> 00:18:30.490
of, hundreds of analysts
around the company

00:18:30.490 --> 00:18:32.370
running queries all
day long, building

00:18:32.370 --> 00:18:35.060
applications,
visualizations, dashboards.

00:18:35.060 --> 00:18:37.740
So whereas previously we assumed
that BigQuery was primarily

00:18:37.740 --> 00:18:39.650
intended as an
analytics engine, now

00:18:39.650 --> 00:18:41.130
we're seeing it
expanding, seeing

00:18:41.130 --> 00:18:44.596
it going into this
data warehousing role.

00:18:44.596 --> 00:18:46.220
So I've harped on
this quite a bit now.

00:18:46.220 --> 00:18:47.060
BigQuery storage.

00:18:47.060 --> 00:18:48.726
It's more than
just file storage.

00:18:48.726 --> 00:18:50.100
We've invested in
it quite a bit,

00:18:50.100 --> 00:18:52.039
and we continue to
make improvements.

00:18:52.039 --> 00:18:53.580
It's, of course,
everything you would

00:18:53.580 --> 00:18:55.204
expect from cloud-based
storage, right?

00:18:55.204 --> 00:18:57.420
It's highly available,
highly durable.

00:18:57.420 --> 00:19:00.450
It uses an optimized
columnar format underneath,

00:19:00.450 --> 00:19:02.540
something we've built
proprietary inside Google

00:19:02.540 --> 00:19:06.010
specifically for this workload.

00:19:06.010 --> 00:19:08.010
And we take those files
that have been imported,

00:19:08.010 --> 00:19:09.360
and we actually do quite
a bit of management

00:19:09.360 --> 00:19:10.990
on top of those for
you on your behalf.

00:19:10.990 --> 00:19:12.531
So for instance, if
you have happened

00:19:12.531 --> 00:19:14.540
to import a whole
bunch of small files,

00:19:14.540 --> 00:19:16.910
we'll actually automatically
coalesce those over time

00:19:16.910 --> 00:19:19.080
to ensure that we get the
highest possible query

00:19:19.080 --> 00:19:20.250
throughput.

00:19:20.250 --> 00:19:22.820
And we offer fast table
reads, the ability to read

00:19:22.820 --> 00:19:27.050
data out of that storage
layer without running a query.

00:19:27.050 --> 00:19:27.930
It's rich metadata.

00:19:27.930 --> 00:19:30.180
As I said, a familiar
database-like structure,

00:19:30.180 --> 00:19:34.099
datasets, tables, full fields
and schemas, as well as

00:19:34.099 --> 00:19:36.140
descriptions we just added
in this latest release

00:19:36.140 --> 00:19:37.330
for each one of those.

00:19:37.330 --> 00:19:38.430
A time to live on tables.

00:19:38.430 --> 00:19:40.810
Maybe you only want to store
90 days' worth of data.

00:19:40.810 --> 00:19:43.490
You can just expire
tables after 90 days.

00:19:43.490 --> 00:19:47.550
Project and dataset level ACLs,
so very fine-grained controls.

00:19:47.550 --> 00:19:48.511
And table decorators.

00:19:48.511 --> 00:19:50.760
Maybe another interesting
use case of table decorators

00:19:50.760 --> 00:19:53.162
to highlight-- imagine,
over the last two days,

00:19:53.162 --> 00:19:54.620
you discover that
your imports were

00:19:54.620 --> 00:19:56.340
bringing in corrupted data.

00:19:56.340 --> 00:19:59.750
You could actually reference the
table as it was two days ago,

00:19:59.750 --> 00:20:02.370
make a copy of it, rerun the
imports with your corrected

00:20:02.370 --> 00:20:03.460
data, and off you go.

00:20:03.460 --> 00:20:05.911
Very easy to interact with.

00:20:05.911 --> 00:20:07.690
And finally, data imports.

00:20:07.690 --> 00:20:10.960
We've invested heavily in the
standard, high-frequency bulk

00:20:10.960 --> 00:20:11.510
imports.

00:20:11.510 --> 00:20:13.343
We've obviously got
this new improvement now

00:20:13.343 --> 00:20:14.740
with the streaming API.

00:20:14.740 --> 00:20:17.729
And we couldn't make it any
easier with respect to data

00:20:17.729 --> 00:20:18.895
feeds like Google Analytics.

00:20:21.880 --> 00:20:23.450
So BigQuery storage.

00:20:23.450 --> 00:20:25.160
It's, of course,
optimized for SQL.

00:20:25.160 --> 00:20:26.965
That's the primary interface
that we've exposed to you

00:20:26.965 --> 00:20:28.740
all, and we continue
to make investments

00:20:28.740 --> 00:20:30.580
in terms of new functions.

00:20:30.580 --> 00:20:32.760
JSON_EXTRACT is an
interesting example here.

00:20:32.760 --> 00:20:35.160
So this might open up a
whole new set of use cases.

00:20:35.160 --> 00:20:36.710
You have your JSON
data somewhere.

00:20:36.710 --> 00:20:38.760
You just bring it
directly into BigQuery.

00:20:38.760 --> 00:20:41.440
You can preprocess it and then
generate downstream tables

00:20:41.440 --> 00:20:43.720
that you want to run
your analytics on.

00:20:43.720 --> 00:20:45.920
So it's a structured
data repository,

00:20:45.920 --> 00:20:47.610
but a very flexible one.

00:20:47.610 --> 00:20:49.140
With this JSON
processing, you can

00:20:49.140 --> 00:20:50.931
add a lot of value to
the data right there.

00:20:50.931 --> 00:20:52.880
You can do these
large, arbitrary joins.

00:20:52.880 --> 00:20:54.650
And you have nested
and repeated fields.

00:20:54.650 --> 00:20:59.020
Complex, as I said
earlier, but very powerful.

00:20:59.020 --> 00:21:00.520
So let's take a
closer look at that.

00:21:00.520 --> 00:21:01.660
I'm going to use the
analytics data here

00:21:01.660 --> 00:21:03.243
to look at nested
and repeated fields.

00:21:05.650 --> 00:21:08.170
So analytics applies quite
a bit of business logic

00:21:08.170 --> 00:21:10.710
to all of the hits that are
uploaded throughout the day.

00:21:10.710 --> 00:21:13.126
It's collecting every single
hit, of course, that somebody

00:21:13.126 --> 00:21:16.120
has on a given website,
with the general goal being

00:21:16.120 --> 00:21:18.871
they want to generate this
notion of a visit or a session.

00:21:18.871 --> 00:21:20.245
So all of these
hits encapsulated

00:21:20.245 --> 00:21:23.310
for a given visitor that's
arrived at the system.

00:21:23.310 --> 00:21:25.070
So their schema looks
something like this.

00:21:25.070 --> 00:21:28.460
They have at the top
level of a row or a record

00:21:28.460 --> 00:21:31.090
this notion of a visit ID,
and a visit start time,

00:21:31.090 --> 00:21:32.610
and a couple of other fields.

00:21:32.610 --> 00:21:37.570
And then, embedded within
that, an array of hits.

00:21:37.570 --> 00:21:40.280
And within each one of
those is a hit's time,

00:21:40.280 --> 00:21:42.404
so the offset from
the start time,

00:21:42.404 --> 00:21:44.070
and the path, the
actual URL of the hit,

00:21:44.070 --> 00:21:46.205
and a whole bunch of
other fields as well.

00:21:46.205 --> 00:21:48.580
And then just thought I'd note
here, yet another repeated

00:21:48.580 --> 00:21:50.454
field embedded in there
for custom variables,

00:21:50.454 --> 00:21:52.780
if the website
owner set those up.

00:21:52.780 --> 00:21:57.260
And those would hang
off of the hits array.

00:21:57.260 --> 00:21:59.010
So what does this
actually look like here?

00:21:59.010 --> 00:22:00.710
So we actually run
Google Analytics

00:22:00.710 --> 00:22:03.150
right on top of that
BigQuery user experience

00:22:03.150 --> 00:22:04.370
that you're able to use.

00:22:04.370 --> 00:22:07.924
So I pulled out a single visit
here from early February.

00:22:07.924 --> 00:22:09.590
Looking at a few here,
I notice the user

00:22:09.590 --> 00:22:12.840
went from github_nested, one
of our public data samples,

00:22:12.840 --> 00:22:15.350
and then moved to the
timeline version of that,

00:22:15.350 --> 00:22:16.800
and then kind of
proceeded around

00:22:16.800 --> 00:22:18.216
to various different
tables there.

00:22:18.216 --> 00:22:20.260
So they're sort of exploring,
clearly, and trying

00:22:20.260 --> 00:22:22.551
to get a sense of what do
those different tables-- what

00:22:22.551 --> 00:22:23.850
can they do with those.

00:22:23.850 --> 00:22:26.016
So I started thinking about
an interesting use case.

00:22:26.016 --> 00:22:27.990
I was kind of wondering,
how many people

00:22:27.990 --> 00:22:30.614
have done the same thing, or how
many people have transitioned,

00:22:30.614 --> 00:22:33.649
for instance, from github_nested
to github_timeline?

00:22:33.649 --> 00:22:35.440
How would I go about
solving that question?

00:22:35.440 --> 00:22:36.815
And maybe that's
a bit contrived,

00:22:36.815 --> 00:22:39.700
but you can imagine this in a
given e-commerce site-- maybe

00:22:39.700 --> 00:22:40.920
a travel website.

00:22:40.920 --> 00:22:43.035
You're looking at a
hotel details page,

00:22:43.035 --> 00:22:44.410
and you want to
see how long does

00:22:44.410 --> 00:22:46.880
it take for people to
transition from the details page

00:22:46.880 --> 00:22:48.449
over to the purchase page.

00:22:48.449 --> 00:22:50.990
OK, that might be a little more
interesting in that scenario.

00:22:50.990 --> 00:22:52.730
Maybe something's
changed, or maybe it

00:22:52.730 --> 00:22:54.620
varies by region or by
hotel, and maybe you

00:22:54.620 --> 00:22:57.280
need to change your
marketing on those pages.

00:22:57.280 --> 00:22:59.480
So I set out to write this.

00:22:59.480 --> 00:23:03.070
I wanted to find the average
duration spent transitioning

00:23:03.070 --> 00:23:07.020
from page A to page B, in this
case, those two GitHub tables.

00:23:07.020 --> 00:23:09.510
And as it turns out,
the SQL for this

00:23:09.510 --> 00:23:11.300
was significantly
more complicated

00:23:11.300 --> 00:23:14.310
than I was actually expecting.

00:23:14.310 --> 00:23:15.060
So I apologize.

00:23:15.060 --> 00:23:15.890
This is a bit of an eye chart.

00:23:15.890 --> 00:23:16.440
I realize that.

00:23:16.440 --> 00:23:18.023
But I want to go
through the specifics

00:23:18.023 --> 00:23:20.110
here and explain how
this actually works.

00:23:20.110 --> 00:23:21.640
So in the innermost
query here, we

00:23:21.640 --> 00:23:25.030
need to pull out the
visit ID, the hit time,

00:23:25.030 --> 00:23:28.690
and the actual path
that was navigated to.

00:23:28.690 --> 00:23:31.046
The next bit of syntax
is OMIT RECORD IF.

00:23:31.046 --> 00:23:33.420
It's actually something I
wasn't even familiar with prior

00:23:33.420 --> 00:23:35.520
to trying to work
on this example.

00:23:35.520 --> 00:23:39.310
What it does is rather than just
apply a standard WHERE clause,

00:23:39.310 --> 00:23:43.580
OMIT IF allows me to find
rows in which, embedded

00:23:43.580 --> 00:23:46.770
in that repeated field of
hits, both of those conditions

00:23:46.770 --> 00:23:47.790
have been met.

00:23:47.790 --> 00:23:49.770
In this case, the
user navigated to

00:23:49.770 --> 00:23:52.280
both github_nested
and github_timeline.

00:23:52.280 --> 00:23:53.834
So now that I have
at least the rows

00:23:53.834 --> 00:23:55.250
that I'm interested
in processing,

00:23:55.250 --> 00:23:57.260
I need to find those
cases where they actually

00:23:57.260 --> 00:24:01.160
did transition between
those two specific tables.

00:24:01.160 --> 00:24:04.240
So I come here, and I use one
of those analytic functions,

00:24:04.240 --> 00:24:06.810
a combination of
partition by, ordering

00:24:06.810 --> 00:24:09.234
on hit time, and
this notion of lead.

00:24:09.234 --> 00:24:11.400
And what it's essentially
doing is it's transposing.

00:24:11.400 --> 00:24:14.580
It's taking hit number one, and
it's transposing hit number two

00:24:14.580 --> 00:24:17.686
alongside of it, and
so on and so forth.

00:24:17.686 --> 00:24:19.185
Once I have that
created, using path

00:24:19.185 --> 00:24:22.700
and now path lead as the two
fields as I've named them,

00:24:22.700 --> 00:24:25.982
I can now apply this
final outer select.

00:24:25.982 --> 00:24:27.690
First it's filtering,
of course, and it's

00:24:27.690 --> 00:24:30.710
looking for a path as
github_nested and path lead

00:24:30.710 --> 00:24:32.150
as github_timeline.

00:24:32.150 --> 00:24:36.160
And then I can do the
count and the average.

00:24:36.160 --> 00:24:37.760
So this works.

00:24:37.760 --> 00:24:38.707
This definitely works.

00:24:38.707 --> 00:24:39.790
There's no doubt about it.

00:24:39.790 --> 00:24:41.873
And you can construct all
sorts of amazing things,

00:24:41.873 --> 00:24:44.560
especially if you've
been using SQL for years.

00:24:44.560 --> 00:24:47.060
But I would argue, clearly
SQL has some challenges.

00:24:47.060 --> 00:24:50.950
I mean, these limits, if
not nearly impossible,

00:24:50.950 --> 00:24:53.690
in some cases are
truly impossible.

00:24:53.690 --> 00:24:55.440
And so I don't think
this is anything new.

00:24:55.440 --> 00:24:57.606
I mean, other databases
have solved for this, right?

00:24:57.606 --> 00:24:59.100
There's PL/SQL, there's T-SQL.

00:24:59.100 --> 00:25:01.090
This is pretty standard
in the industry.

00:25:01.090 --> 00:25:04.230
People have accepted that
sometimes a procedural language

00:25:04.230 --> 00:25:06.830
makes life 1,000 times easier.

00:25:06.830 --> 00:25:08.832
Even if you can
express it in SQL,

00:25:08.832 --> 00:25:11.040
it might be that much easier
to express your business

00:25:11.040 --> 00:25:14.401
logic in a procedural language.

00:25:14.401 --> 00:25:16.150
So what would this one
look like if I just

00:25:16.150 --> 00:25:17.280
want to find those hits?

00:25:17.280 --> 00:25:19.586
Let's isolate in on that
most important part.

00:25:19.586 --> 00:25:20.710
It's super straightforward.

00:25:20.710 --> 00:25:23.770
For each row here, I
have an array of hits.

00:25:23.770 --> 00:25:27.620
I just need to check if hits
sub i is equal to github_nested,

00:25:27.620 --> 00:25:30.870
and hits sub i plus 1 is equal
to github_timeline, then do

00:25:30.870 --> 00:25:31.380
something.

00:25:31.380 --> 00:25:34.030
In this case, I'm adding to
an array and omitting it.

00:25:34.030 --> 00:25:35.530
So it's clearly a
lot easier, right?

00:25:35.530 --> 00:25:36.980
I need this escape hatch.

00:25:36.980 --> 00:25:39.350
I need this ability to
be able to go operate

00:25:39.350 --> 00:25:40.880
in more arbitrary
ways than I can

00:25:40.880 --> 00:25:43.300
express in SQL in some cases.

00:25:43.300 --> 00:25:46.260
And I mentioned earlier
that we make this possible.

00:25:46.260 --> 00:25:49.476
You can do reads directly
from BigQuery storage

00:25:49.476 --> 00:25:50.475
without running a query.

00:25:50.475 --> 00:25:52.530
In fact, I noted that
Motorola was actually

00:25:52.530 --> 00:25:55.460
doing MapReduces over
BigQuery storage.

00:25:55.460 --> 00:25:58.352
So what we're announcing as
well today is not only can

00:25:58.352 --> 00:25:59.810
you do those reads,
but you can now

00:25:59.810 --> 00:26:02.450
do them at incredibly
high throughput.

00:26:02.450 --> 00:26:04.720
We're making it possible
to read in parallel

00:26:04.720 --> 00:26:09.660
across a huge BigQuery table
via Hadoop, via whatever

00:26:09.660 --> 00:26:12.580
parallel processing tool
you have of your choosing.

00:26:12.580 --> 00:26:14.190
The pseudocode
here I've included,

00:26:14.190 --> 00:26:15.480
it's trivial to set this up.

00:26:15.480 --> 00:26:18.310
You first need to instruct
BigQuery via an API call

00:26:18.310 --> 00:26:21.179
that you want to read in
parallel across a given table.

00:26:21.179 --> 00:26:23.470
You tell us the number of
workers you're going to have.

00:26:23.470 --> 00:26:25.490
Maybe you're going to run
100 MapReduce workers.

00:26:25.490 --> 00:26:27.170
You then wait for
that job to start.

00:26:27.170 --> 00:26:29.096
Should take no more
than a few seconds.

00:26:29.096 --> 00:26:30.220
And then you would iterate.

00:26:30.220 --> 00:26:34.530
Each partition could be consumed
in parallel across that table.

00:26:34.530 --> 00:26:38.120
So this unlocks a whole
new set of use cases.

00:26:38.120 --> 00:26:39.940
But maybe there's
an even easier way.

00:26:39.940 --> 00:26:42.035
And if you can switch
to the demo, please.

00:27:00.730 --> 00:27:01.230
Apologies.

00:27:01.230 --> 00:27:03.220
It's pulling up
the wrong desktop.

00:27:13.001 --> 00:27:14.498
The question is which one.

00:27:27.971 --> 00:27:28.907
A little help.

00:27:28.907 --> 00:27:29.740
I'm on desktop four.

00:27:29.740 --> 00:27:32.200
How do I get to desktop four?

00:27:45.933 --> 00:27:46.700
A-ha.

00:27:46.700 --> 00:27:47.200
Much better.

00:28:04.640 --> 00:28:07.050
Hopefully you all can read
that from the back row.

00:28:07.050 --> 00:28:08.975
It might be a little bit small.

00:28:08.975 --> 00:28:10.350
So what you're
looking at here, I

00:28:10.350 --> 00:28:12.850
mentioned we have the
github_nested table

00:28:12.850 --> 00:28:14.460
alongside the
github_timeline table.

00:28:14.460 --> 00:28:15.975
So here I just
induce this behavior.

00:28:15.975 --> 00:28:19.040
I've navigated between the two.

00:28:19.040 --> 00:28:21.230
And I've pulled in all
of our analytics data

00:28:21.230 --> 00:28:22.441
into this test dataset here.

00:28:22.441 --> 00:28:24.190
So we can take a quick
look there as well.

00:28:24.190 --> 00:28:26.279
So we have visit ID
and visit start time,

00:28:26.279 --> 00:28:27.320
like I was talking about.

00:28:27.320 --> 00:28:28.778
A whole bunch of
other fields here,

00:28:28.778 --> 00:28:33.260
totals, traffic source,
the device that you've

00:28:33.260 --> 00:28:36.250
navigated from, as well as
that repeated field of hits.

00:28:36.250 --> 00:28:39.996
So hits, and the hit
time, and the page path.

00:28:39.996 --> 00:28:41.870
And this table isn't
particularly large here.

00:28:41.870 --> 00:28:44.617
It's about 18 gigs,
about 11 million rows.

00:28:44.617 --> 00:28:46.450
So I mentioned there
might be an easier way.

00:28:53.058 --> 00:28:54.480
Well, indeed, there is.

00:28:58.060 --> 00:29:01.410
We are combining the power of
that procedural language right

00:29:01.410 --> 00:29:04.150
inside the power of BigQuery,
the full scalability

00:29:04.150 --> 00:29:06.545
of BigQuery, allowing you
to merge right together

00:29:06.545 --> 00:29:10.010
both SQL and JavaScript
at the same time.

00:29:10.010 --> 00:29:12.140
And like that, I've run
that exact same query,

00:29:12.140 --> 00:29:14.812
now directly inside
BigQuery, without any effort

00:29:14.812 --> 00:29:16.770
in terms of spinning up
resources, provisioning

00:29:16.770 --> 00:29:19.311
hardware, or doing any kind of
software provisioning as well.

00:29:27.144 --> 00:29:29.560
So taking a closer look here,
it's pretty straightforward.

00:29:29.560 --> 00:29:31.980
We've got a standard SQL
select at the top here

00:29:31.980 --> 00:29:34.070
pulling from that given table.

00:29:34.070 --> 00:29:36.889
The next row, I'm
instructing BigQuery

00:29:36.889 --> 00:29:39.180
what is going to be the input
schema I'm interested in.

00:29:39.180 --> 00:29:41.346
So which fields do I want
to pull out of that table.

00:29:41.346 --> 00:29:43.440
The following row is
what's the schema that's

00:29:43.440 --> 00:29:45.400
going to be omitted
from the function.

00:29:45.400 --> 00:29:47.540
And then, as you can
see, the function itself.

00:29:47.540 --> 00:29:49.460
And I would argue, going
back to that can you

00:29:49.460 --> 00:29:52.564
push the limits with SQL,
what are the boundaries there.

00:29:52.564 --> 00:29:54.230
Let's take a slightly
different example.

00:29:54.230 --> 00:29:55.550
Maybe I don't want
to look specifically

00:29:55.550 --> 00:29:57.220
at that transition
from github_nested

00:29:57.220 --> 00:29:59.370
to timeline immediately,
but I'd like

00:29:59.370 --> 00:30:02.450
to see if they skipped a
page or two in between.

00:30:02.450 --> 00:30:06.350
So changing that is as simple
as coming through here,

00:30:06.350 --> 00:30:08.130
making a quick update.

00:30:08.130 --> 00:30:10.200
It's very easy at least
for me to understand this

00:30:10.200 --> 00:30:11.630
relative to the
challenge I think

00:30:11.630 --> 00:30:13.213
I would endure in
trying to figure out

00:30:13.213 --> 00:30:14.480
how to do this in SQL.

00:30:14.480 --> 00:30:15.970
So off it runs.

00:30:15.970 --> 00:30:17.050
This is coming soon.

00:30:17.050 --> 00:30:20.240
We're going to make it
available to you-- excuse me.

00:30:20.240 --> 00:30:22.020
Please contact a sales
rep to get started.

00:30:24.870 --> 00:30:26.920
Hopefully this will
finish here in a moment.

00:30:26.920 --> 00:30:27.710
[INAUDIBLE] indeed it does.

00:30:27.710 --> 00:30:29.335
Please, let's switch
back to the slide.

00:30:33.690 --> 00:30:37.140
So with BigQuery and the
combination of both SQL and now

00:30:37.140 --> 00:30:39.810
JavaScript UDFs, we
think we're unlocking

00:30:39.810 --> 00:30:41.230
a whole set of new use cases.

00:30:41.230 --> 00:30:43.355
We're very excited to see
what you guys can come up

00:30:43.355 --> 00:30:45.197
with next on this.

00:30:45.197 --> 00:30:46.030
So BigQuery storage.

00:30:46.030 --> 00:30:48.417
In summary, it's optimized
for SQL, of course.

00:30:48.417 --> 00:30:50.250
And we're going to
continue to invest there.

00:30:50.250 --> 00:30:52.458
We have a whole bunch of
new improvements coming down

00:30:52.458 --> 00:30:53.140
the road.

00:30:53.140 --> 00:30:55.410
Fast table reads,
the new ability

00:30:55.410 --> 00:30:58.540
to do huge scale MapReduce
directly against a BigQuery

00:30:58.540 --> 00:31:02.330
table, not having to necessarily
do any operations in between.

00:31:02.330 --> 00:31:04.300
And finally, coming
soon, JavaScript

00:31:04.300 --> 00:31:07.120
UDFs on top of
BigQuery data directly.

00:31:09.994 --> 00:31:11.410
So the last area
I wanted to cover

00:31:11.410 --> 00:31:12.530
are those pricing announcements.

00:31:12.530 --> 00:31:14.310
We talked about those this
morning in the keynote.

00:31:14.310 --> 00:31:16.768
We're definitely excited about
opening this up more broadly

00:31:16.768 --> 00:31:18.459
to more users.

00:31:18.459 --> 00:31:19.500
So why are we doing this?

00:31:19.500 --> 00:31:21.875
Well, of course, to reduce
price, lower barrier to entry.

00:31:21.875 --> 00:31:22.720
We want more people.

00:31:22.720 --> 00:31:26.100
We want everyone to have
access to big data processing.

00:31:26.100 --> 00:31:28.770
We've also heard from the
larger customers we have,

00:31:28.770 --> 00:31:30.300
they really wanted
predictability

00:31:30.300 --> 00:31:32.312
in terms of cost.

00:31:32.312 --> 00:31:34.270
Additionally, we've seen
these new usage models

00:31:34.270 --> 00:31:35.380
emerging since we started.

00:31:35.380 --> 00:31:37.470
So data warehousing,
now streaming ingestion,

00:31:37.470 --> 00:31:40.420
doing real time capabilities,
and UDFs coming along the way

00:31:40.420 --> 00:31:42.530
soon.

00:31:42.530 --> 00:31:45.910
So lower across the board,
everything is coming down.

00:31:45.910 --> 00:31:48.540
So lower query pricing,
$5 per terabyte,

00:31:48.540 --> 00:31:53.100
an 85% reduction off of our
current $35 price point.

00:31:53.100 --> 00:31:56.630
Lower storage pricing, $26
per terabyte per month,

00:31:56.630 --> 00:31:58.374
65% reduction.

00:31:58.374 --> 00:32:00.040
And what's really
important to note here

00:32:00.040 --> 00:32:02.590
is that your storage
pricing is completely

00:32:02.590 --> 00:32:05.200
independent from
your query pricing.

00:32:05.200 --> 00:32:06.710
So you can park as
much data as you

00:32:06.710 --> 00:32:09.180
want with this at
bare bones pricing

00:32:09.180 --> 00:32:11.574
and not necessarily have to
pay a dime in query costs.

00:32:11.574 --> 00:32:13.990
Use it as much or as little
as you want in terms of query.

00:32:13.990 --> 00:32:15.260
It's immediately available.

00:32:15.260 --> 00:32:17.390
Only Google has
the infrastructure,

00:32:17.390 --> 00:32:20.160
in terms of storage, network,
and query capability,

00:32:20.160 --> 00:32:23.000
to give you these
things independently.

00:32:23.000 --> 00:32:24.800
And finally, lower
streaming costs,

00:32:24.800 --> 00:32:26.390
one penny per 100,000 rows.

00:32:26.390 --> 00:32:28.740
We've actually deferred
pricing on this until July

00:32:28.740 --> 00:32:31.760
to let you get started now.

00:32:31.760 --> 00:32:33.970
So on demand query pricing,
a closer look at this.

00:32:33.970 --> 00:32:36.140
This is essentially our
existing usage model.

00:32:36.140 --> 00:32:39.100
It's ideal for interactive
ad hoc analysis.

00:32:39.100 --> 00:32:42.580
It leverages a very large
shared pool of resources.

00:32:42.580 --> 00:32:45.100
Because it is a shared pool,
we do impose these limits

00:32:45.100 --> 00:32:46.830
in terms of how much
a single user can

00:32:46.830 --> 00:32:49.310
use, making sure everybody
gets their fair share

00:32:49.310 --> 00:32:50.500
of those resources.

00:32:50.500 --> 00:32:53.490
Pricing is simple, $5
per terabyte processed,

00:32:53.490 --> 00:32:55.610
no contracts, pay as you go.

00:32:55.610 --> 00:32:57.060
Doesn't get any easier.

00:32:57.060 --> 00:32:58.580
But Urs also
mentioned this morning

00:32:58.580 --> 00:33:01.440
we have a whole new pricing
model coming online.

00:33:01.440 --> 00:33:03.250
It's reserve capacity.

00:33:03.250 --> 00:33:05.210
It gives predictable
costs for those larger

00:33:05.210 --> 00:33:07.900
workloads, consistent
performance,

00:33:07.900 --> 00:33:09.850
and we remove those
concurring query quotas.

00:33:09.850 --> 00:33:12.920
You choose how much capacity
you need to send as many queries

00:33:12.920 --> 00:33:14.790
as you like.

00:33:14.790 --> 00:33:17.780
So capacity reservations, the
purchase in increments of 5

00:33:17.780 --> 00:33:19.049
gigabytes per second.

00:33:19.049 --> 00:33:21.090
Something that's relatively
easy to get your head

00:33:21.090 --> 00:33:21.715
wrapped around.

00:33:21.715 --> 00:33:23.730
How much do I want in
terms of throughput?

00:33:23.730 --> 00:33:26.290
Not how many machines do I need
to buy, how many cores do I

00:33:26.290 --> 00:33:28.790
need, do I need SSD
versus regular disks,

00:33:28.790 --> 00:33:30.390
what do I need to do?

00:33:30.390 --> 00:33:31.880
Larger, more
consistent workloads

00:33:31.880 --> 00:33:33.570
is what this is
after, obviously,

00:33:33.570 --> 00:33:36.100
with a monthly commitment
attached to it.

00:33:36.100 --> 00:33:38.520
Each 5 gigabyte per
second increment

00:33:38.520 --> 00:33:42.590
costs $20,000 per month and
enables 13,000 terabytes

00:33:42.590 --> 00:33:44.180
of processing per month.

00:33:44.180 --> 00:33:45.620
If you were to
fully utilize that,

00:33:45.620 --> 00:33:48.180
that's actually a
95% price reduction

00:33:48.180 --> 00:33:51.040
off our current $35 price point.

00:33:51.040 --> 00:33:53.252
So what is 13,000
terabytes, though?

00:33:53.252 --> 00:33:54.710
That sounds big,
but I'm not really

00:33:54.710 --> 00:33:56.990
sure what it means in reality.

00:33:56.990 --> 00:34:00.690
So I put together a quick
example customer scenario here.

00:34:00.690 --> 00:34:04.450
Let's imagine you have 50
terabytes of data stored,

00:34:04.450 --> 00:34:07.047
and your average query size
is about 2 and 1/2 terabytes.

00:34:07.047 --> 00:34:08.630
Right off the gate,
you might question

00:34:08.630 --> 00:34:10.469
why just 5% of my data?

00:34:10.469 --> 00:34:11.696
Why is it so small?

00:34:11.696 --> 00:34:13.654
Well, I mentioned earlier
columnar data format.

00:34:13.654 --> 00:34:15.070
It's very highly optimized.

00:34:15.070 --> 00:34:16.929
We reduce the amount
of data scanned simply

00:34:16.929 --> 00:34:18.710
based on the columns referenced.

00:34:18.710 --> 00:34:20.480
So if you had 50
columns in the table,

00:34:20.480 --> 00:34:23.630
and you're only referencing five
of those, probably about 10%

00:34:23.630 --> 00:34:26.230
of your data is actually going
to be scanned in that query.

00:34:26.230 --> 00:34:27.813
But there's other
places you can save.

00:34:27.813 --> 00:34:30.679
So table partitioning and
using Table Wildcards.

00:34:30.679 --> 00:34:33.114
If you have daily tables, and
you only need to query over

00:34:33.114 --> 00:34:35.530
seven days, then only reference
seven days' worth of data.

00:34:35.530 --> 00:34:37.190
You don't need to
reference the whole year.

00:34:37.190 --> 00:34:38.570
Or additionally,
Table Decorators

00:34:38.570 --> 00:34:39.729
if you're doing more
recent information

00:34:39.729 --> 00:34:40.780
that you want to pull out.

00:34:40.780 --> 00:34:42.170
Maybe you have
the streaming API.

00:34:42.170 --> 00:34:43.650
So there's many ways in
which you can get there.

00:34:43.650 --> 00:34:45.941
In fact, a lot of our customers
see a lot lower than 5%

00:34:45.941 --> 00:34:47.826
in some of their use cases.

00:34:47.826 --> 00:34:49.909
So I'm also going to imagine
that this company has

00:34:49.909 --> 00:34:51.440
two analysts that they
want to have operating

00:34:51.440 --> 00:34:54.070
on this particular dataset,
and those analysts are going

00:34:54.070 --> 00:34:56.949
to run about 75 queries per day.

00:34:56.949 --> 00:34:59.410
So in total, that's
2.5 terabytes

00:34:59.410 --> 00:35:02.830
times 150 queries, about
375 terabytes of processing

00:35:02.830 --> 00:35:03.940
per day.

00:35:03.940 --> 00:35:07.260
As it turns out, our
smallest reservation

00:35:07.260 --> 00:35:10.530
can accommodate a
50 terabyte dataset.

00:35:10.530 --> 00:35:13.190
Because a single reservation
of 5 gigabytes per second

00:35:13.190 --> 00:35:17.470
enables you to do 432 terabytes
of processing per day.

00:35:17.470 --> 00:35:18.990
So quite a bit of
capability just

00:35:18.990 --> 00:35:22.320
from the smallest
reservation increment.

00:35:22.320 --> 00:35:24.560
The other thing that
we enable, because

00:35:24.560 --> 00:35:27.230
of BigQuery's
multi-tenant architecture,

00:35:27.230 --> 00:35:30.390
is the ability to not only
use your reserve capacity,

00:35:30.390 --> 00:35:33.850
but optionally burst into
our on demand resource pool.

00:35:33.850 --> 00:35:36.890
This is something nobody
else is capable of doing.

00:35:36.890 --> 00:35:38.500
The scenarios here are endless.

00:35:38.500 --> 00:35:41.389
So it might be just during
the course of a given day,

00:35:41.389 --> 00:35:42.930
there's volatility
in your workloads.

00:35:42.930 --> 00:35:45.610
And you want to have the ability
to spike, and not necessarily

00:35:45.610 --> 00:35:47.750
have all of your users
backed up and waiting.

00:35:47.750 --> 00:35:49.380
Or maybe the finance
team is struggling

00:35:49.380 --> 00:35:52.280
to get financials done at
the end of the quarter.

00:35:52.280 --> 00:35:54.910
Or maybe literally you're
just increasing your volumes

00:35:54.910 --> 00:35:55.940
that dramatically,
and you haven't

00:35:55.940 --> 00:35:57.939
figured out how much more
you want to provision,

00:35:57.939 --> 00:36:00.820
but you need the capability
to run more queries.

00:36:00.820 --> 00:36:03.570
So it offers that
predictability in cost

00:36:03.570 --> 00:36:06.920
with guaranteed capacity
and additional resources

00:36:06.920 --> 00:36:08.859
when you need them.

00:36:08.859 --> 00:36:10.900
So what does this actually
look like in practice?

00:36:10.900 --> 00:36:13.170
This is going to get a little
in depth with some charts here,

00:36:13.170 --> 00:36:14.128
but this should be fun.

00:36:14.128 --> 00:36:16.217
So here's a given
day's worth of volume.

00:36:16.217 --> 00:36:18.550
And you can imagine it wouldn't
be a perfect bell curve,

00:36:18.550 --> 00:36:20.307
but as you might guess,
the volume's going

00:36:20.307 --> 00:36:22.890
to increase during the middle
of the day for a given work day,

00:36:22.890 --> 00:36:23.430
right?

00:36:23.430 --> 00:36:26.184
And so this curve is measuring
query throughput, essentially

00:36:26.184 --> 00:36:28.350
the number of queries you're
pushing into the system

00:36:28.350 --> 00:36:31.780
at any given time
for your reservation.

00:36:31.780 --> 00:36:33.580
In this case, using
that customer example,

00:36:33.580 --> 00:36:35.330
the total query
processing during that day

00:36:35.330 --> 00:36:39.170
was 375 terabytes.

00:36:39.170 --> 00:36:41.620
So that customer reserved
5 gigabytes per second.

00:36:41.620 --> 00:36:43.601
And they've chosen
not to actually burst.

00:36:43.601 --> 00:36:44.850
That's where they started out.

00:36:44.850 --> 00:36:46.900
They felt like that was
the appropriate approach.

00:36:46.900 --> 00:36:49.340
And this is what you would
get if you were provisioning

00:36:49.340 --> 00:36:51.290
your own hardware or
buying or reserving

00:36:51.290 --> 00:36:52.686
n number of instances.

00:36:52.686 --> 00:36:54.560
You would get something
that looks like this.

00:36:54.560 --> 00:36:56.940
Rather than the
smooth bell curve,

00:36:56.940 --> 00:36:58.780
you would end up
capping your capacity.

00:36:58.780 --> 00:37:01.090
All of your queries
would start slowing down.

00:37:01.090 --> 00:37:03.030
Your users would get
a worse experience.

00:37:03.030 --> 00:37:05.446
And then hopefully, at some
point near the end of the day,

00:37:05.446 --> 00:37:06.980
the system catches
up and recovers

00:37:06.980 --> 00:37:08.210
in the middle of the night,
and then eventually you

00:37:08.210 --> 00:37:10.070
come back, and the
same experience again.

00:37:10.070 --> 00:37:11.240
We've all experienced this.

00:37:11.240 --> 00:37:12.950
Oh, the SQL server
is running slow today

00:37:12.950 --> 00:37:15.040
because there's a lot of
workload going on, right?

00:37:15.040 --> 00:37:17.498
But with BigQuery, you don't
need to run into this problem.

00:37:19.620 --> 00:37:22.280
In our case, we're
able to actually burst

00:37:22.280 --> 00:37:26.300
that additional need above and
beyond your reserved instance.

00:37:26.300 --> 00:37:29.100
Ironically, the query
capacity requires the same.

00:37:29.100 --> 00:37:32.800
We still have 375 terabytes of
processing to do in that day.

00:37:32.800 --> 00:37:34.400
The difference is
your users didn't

00:37:34.400 --> 00:37:37.180
have to bump into those
painful slowdowns.

00:37:37.180 --> 00:37:38.647
So this is burst with cap.

00:37:38.647 --> 00:37:39.480
There was no change.

00:37:39.480 --> 00:37:41.530
And I'll explain
cap in a moment.

00:37:41.530 --> 00:37:45.050
In the next example, they've
also chosen burst with cap,

00:37:45.050 --> 00:37:46.925
but over time, their
data volumes have grown.

00:37:46.925 --> 00:37:48.924
The number of queries
they're running is higher.

00:37:48.924 --> 00:37:50.520
So they're spending
the vast majority

00:37:50.520 --> 00:37:52.900
of their time above
their reservation,

00:37:52.900 --> 00:37:56.800
more than they've reserved
in terms of query throughput.

00:37:56.800 --> 00:37:59.050
So what happens in this case,
because they've told us,

00:37:59.050 --> 00:38:02.161
they said we can't go beyond
our $20,000 per month budget,

00:38:02.161 --> 00:38:04.160
we're actually going to
stop them at some point.

00:38:04.160 --> 00:38:05.659
We're going to stop
them technically

00:38:05.659 --> 00:38:08.239
at 432 terabytes
in that given day.

00:38:08.239 --> 00:38:10.280
They could, of course, go
outside the reservation

00:38:10.280 --> 00:38:11.610
and still run on demand queries.

00:38:11.610 --> 00:38:12.570
I want to be clear about that.

00:38:12.570 --> 00:38:14.486
But they have told us--
they've instructed us,

00:38:14.486 --> 00:38:16.140
they don't want to
go beyond that cap.

00:38:16.140 --> 00:38:18.700
And then finally,
the last example.

00:38:18.700 --> 00:38:21.416
This case, they haven't figured
out exactly how much more they

00:38:21.416 --> 00:38:23.290
want to reserve, but
the last thing they want

00:38:23.290 --> 00:38:24.685
is their queries to get blocked.

00:38:24.685 --> 00:38:26.310
So they're allowing
this to go through.

00:38:26.310 --> 00:38:30.840
They're going to process, in
this example, 510 terabytes.

00:38:30.840 --> 00:38:32.190
So a number of scenarios there.

00:38:32.190 --> 00:38:33.660
And I realize that might
be a bit complicated,

00:38:33.660 --> 00:38:35.250
but I'm trying to highlight
the tremendous amount

00:38:35.250 --> 00:38:36.900
of flexibility we're bringing.

00:38:36.900 --> 00:38:39.480
The first scenario is
perhaps the most obvious one.

00:38:39.480 --> 00:38:41.580
This is what most people
face on a given day.

00:38:41.580 --> 00:38:43.290
They buy a fixed
amount of capacity.

00:38:43.290 --> 00:38:45.390
They're capped at that capacity.

00:38:45.390 --> 00:38:47.290
Perhaps there's
some benefit here

00:38:47.290 --> 00:38:49.019
you would avoid if
you were truly capped

00:38:49.019 --> 00:38:49.810
in terms of budget.

00:38:49.810 --> 00:38:51.643
You would avoid consuming
too many resources

00:38:51.643 --> 00:38:53.560
too early on in the day.

00:38:53.560 --> 00:38:55.540
The second scenario,
they ran over the top

00:38:55.540 --> 00:38:57.480
of the reservation, but
they still [INAUDIBLE]

00:38:57.480 --> 00:39:00.510
consume the exact same number
of terabytes in processing.

00:39:00.510 --> 00:39:03.170
So there was no additional cost.

00:39:03.170 --> 00:39:05.102
Third scenario,
they did a burst,

00:39:05.102 --> 00:39:06.810
and they actually
burst it too far along.

00:39:06.810 --> 00:39:07.559
But they got more.

00:39:07.559 --> 00:39:09.740
They actually got 430 terabytes.

00:39:09.740 --> 00:39:12.850
They told us the cap, so the
additional cost, again, is $0.

00:39:12.850 --> 00:39:14.570
And then finally, in
the last scenario,

00:39:14.570 --> 00:39:16.490
they ran a total
of 510 terabytes,

00:39:16.490 --> 00:39:18.180
roughly 80 terabytes more.

00:39:18.180 --> 00:39:20.800
We multiply that by the
standard on demand rate

00:39:20.800 --> 00:39:23.690
for an extra $400 that day.

00:39:23.690 --> 00:39:25.770
So we believe that
BigQuery is finally

00:39:25.770 --> 00:39:29.542
realizing the true benefits
of cloud here-- simplicity,

00:39:29.542 --> 00:39:32.890
the performance you desire,
and the scalability you

00:39:32.890 --> 00:39:36.770
need whenever you need it.

00:39:36.770 --> 00:39:38.662
So in conclusion,
you've seen a lot today,

00:39:38.662 --> 00:39:41.120
both in the keynote this morning
and some of these examples

00:39:41.120 --> 00:39:41.941
here.

00:39:41.941 --> 00:39:44.440
We're trying to make it simpler
for you in terms of storage.

00:39:44.440 --> 00:39:46.320
We're trying to unlock
all the use cases,

00:39:46.320 --> 00:39:48.520
whether it be something
you want to power with SQL

00:39:48.520 --> 00:39:50.640
or something you want to
power with a MapReduce,

00:39:50.640 --> 00:39:52.920
or coming soon, SQL
UDFs, all merged right

00:39:52.920 --> 00:39:54.230
inside of BigQuery.

00:39:54.230 --> 00:39:56.250
And we've made it
tremendously easier

00:39:56.250 --> 00:39:59.330
in terms of both cost and
provisioning, simplifying

00:39:59.330 --> 00:40:01.329
the overall use case.

00:40:01.329 --> 00:40:03.120
With that, thank you
so much for your time.

00:40:03.120 --> 00:40:05.120
We look forward to seeing
more of your use cases

00:40:05.120 --> 00:40:07.590
and working with you guys
on new business problems.

00:40:07.590 --> 00:40:09.330
Come visit us, cloud.google.com.

00:40:09.330 --> 00:40:10.880
Thanks.

