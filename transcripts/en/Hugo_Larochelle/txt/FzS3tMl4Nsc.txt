Speaker 1:          00:00          In this video will introduce a new type of neuron network for unsupervised learning known as the Ottonian quarter.

Speaker 2:          00:08          Okay.

Speaker 1:          00:08          So in the previous videos we've talked about the restrictive Bolsa machine. Uh, we said that it's a neural network for unsupervised learning that is learning in the case where inputs only take the form of an input. Vectors would have the, we don't have an associated target or we don't use it throughout learning. So we were only trying to learn on the basis of only the input vector x. Uh, so this can be useful either to extract the meaningful features, uh, or to leverage, uh, somehow the, uh, some large amount of unlabeled data that we might have in order perhaps to improve our performance on some particular task for which we have a little amount of labeled data. And, uh, we'll also talk when we talk about deep learning, uh, how we can use, uh, unsupervised learning as a data dependent, regular riser for training. And now in the, uh, following videos with will consider is another type of unsupervised neural network known as the auto in quarter.

Speaker 1:          01:14          So the auto encoder is actually a very simple type of neural network. It's feed forward, much like the neural network we've seen for a classification. And the main difference is that it's going to be trained not to produce a, uh, class, uh, at the outlet. So not to define the distribution over a class at the output, but instead to reproduce the input that was fed to the neural network. So, uh, essentially it's going to be a, so we look at a photo in color with a single hidden layer. Uh, it's output layer, which I'm now calling x hat is going to be of the same size as the input layer. And when we're going to train it were somehow going to compare the values at the output produced by the neural network deal doing color with the values at the input. And we're going to encourage the neural net to reproduce as perfectly as possible at the output layer, the value that was fed in the input.

Speaker 1:          02:16          Um, so, uh, and otherwise this is just a regular neural network, uh, in the terminology of tone colors. We'll refer to the part of the model that computes the hidden layer that compute compute h of x as the end quarter. So it's going to encode our input into a latent representation and a one form it could take, which is very popular and people use Iti. Usually use is just a sigmoid of a linear transformation, much like in a regular neural network. And then we're going to have a decoder, which is going to take the a latent representation h of x, the output of the quarter and it's going again, linear transform it and pass it through nonlinearity. So if we have inputs that are binary between zero and one, then we use a sigmoid not only in the already to squash everything at the output between zero and one.

Speaker 1:          03:11          So it has the same range as the values we can have at the input if we had other types of inputs. Uh, so if we have really validated and would use something else, but so this output, we, we're now going to a node x hat is going to be the decoded, uh, input based on the representation, the latent representation extracted by the autumn quarter. Uh, and other, uh, frequent practice is to actually tie the weights between w and what I'm calling w star, which is the weights between the hidden layer and the reconstruction output layer. So, uh, by ty I mean that, uh, w star would be set as the transpose of the, uh, connections. I go into the hidden layer. So in other words, the connections going into their Hinden layers are going to be the same as the connections that go out of the hidden layer.

Speaker 1:          04:09          And a one motivation for doing this is that this starts looking a bit more like a restricted poster machine. So like in a restricted Boltzmann machine where we're going from the hidden layer to a space which corresponds to the same space as the input space, then we're usually multiplying by the transpose. And so in this sense, each hidden unit here extracts a particular feature. And then when we're trying to reconstruct with just putting back that feature into the input. Okay, so often we'll use that, but that's not necessary. A, if you read, if you see in the paper that tide waits were used, that's what it means. That the transpose connections were the same as December. So when we'll do grain in the sand, for instance, the gradient on double you will, there will be a great incoming from the hidden layer and the great and that goes directly, uh, uh, through w transpose that we'll have, of course I have an impact on what w should be.

Speaker 1:          05:08          All right? So that's the main definition of in the top quarter. It's pretty simple fit for neural network that is trying to reproduce it's, uh, input at the output. And one motivation for training a neural net like this is that essentially we're training it such that the imp, the hidden representation maintains all the information about the input. And for instance, if we use a head in there that's much smaller than the input layer, that this means that the automaker is sort of going to compress the information. So, uh, ignore part of the input that is not useful for reconstruct, reconstructing it and just focus on what makes the input special and, uh, what, um, is important to, uh, extract from it if we were to be able to reconstruct afterwards. Okay. So you could think that a tone color like this could be used for, uh, Francis compressing the data. Um, but we'll talk about more, uh, how we can use it to extract some meaningful features, uh, for some given problem.