Speaker 1:          00:00:00       I'm Jad. I am Robert. Um, are you guys ready to do this? Maybe we should just do this. This is Radiolab. All right. But when your host come out, I need you to seriously clap. Like you've never seen two dudes with glasses talking on microphone. Okay. So like, just really, really give it up for your mostly human hosts. Jad Abumrad and Robert Krulwich

Speaker 2:          00:00:21       [inaudible].

Speaker 3:          00:00:21       So about a week ago we gathered I guess roughly a hundred people into a performance space, which is in our building here at wn sea. It's called the green space. This is like, yeah, just like a, like a playground for us so we can just try things. We decided to gather all these [inaudible]

Speaker 4:          00:00:36       well into a room on a random Monday night. What else are you doing on a Monday? Right? Because seven years previous we had made a show called talking to machines, which was all about like what happens when you talk to a computer that's pretending to be human, right? And the thing is, so much has happened since we made that show. With the proliferation of bots on Twitter, Russian bots meddling in elections, the advances in AI. So much interesting stuff had happened that we thought it is time to update that show

Speaker 3:          00:01:04       and we needed to do it live. We thought because we had a little plan in mind, we wanted to put unsuspecting people into a room for a kind of showdown between people and machines. But we want to set the scene a little bit and give you a us just a flavor of what we're really going to. Just to start things off, we brought to our stage, one of the guys who inspired that original show. Please welcome to the stage writer Brian Christian

Speaker 2:          00:01:33       [inaudible] well,

Speaker 1:          00:01:33       so just so we can just just get things sort of oriented. Oh, we need to, first of all just redefine what a chat bot is, right? So a chat Bot is a computer program, uh, that exists to mimic and impersonate human beings. Like when do I run into them? You go to a website to interact with some customer service. You might find yourself talking to a chat Bot. Um, the U s army has a chat bot called sergeant star that recruits people who, can I ask you a question about, about the, the set, the thing you just said about chatting with customer service? Yeah, I end up doing a lot. Um, I'm sorry. We says I, uh, you know, like it's in the middle of the night, you're trying to figure out some program and it's not working and then suddenly there's like need to chat and you click on that.

Speaker 1:          00:02:19       Suddenly there's need to chat. Well, it's like you, your whatever. I assume many of you have had this experience, a few of the experiences that he's had. So there's just that issue always. I'm always curious. It, it what, it seems very human when you're having that conversation with, with uh, uh, customer service Chat Bot. Is there a, is there a place where it, where is the line between human and robots in that they're both present? Yeah. Well, this, this is the question, right? So we're now sort of accustomed to having this uncanny feeling of not really knowing the difference. My guess for what it's worth is that there's a system on the back end that's designed, sort of do triage where the first few exchanges that are just like, hey, how can I help what's going on? It seems like there's an issue with this such and such.

Speaker 1:          00:03:08       Um, that is basically just a chat bot and at a certain point you kind of seamlessly transition and are handed off to a real person. But without any, you know, notification to you that this has happened. It's deliberately left opaque at what point that happens. Wow. This is just literally everywhere it is. I mean, and you can't get on social media and read some comment thread without someone accusing someone else of being a Bot. Um, and you know, it seems, uh, it seems maybe sort of trivial at some level, but we are now living through this political crisis of how do we kind of come to terms with the idea that we can, you know, weaponize this kind of speech and how do we as consumers of the news or as users of social media try to suss out whether the people we're interacting with are in fact who they say they are?

Speaker 3:          00:04:02       And all this confusion about what's the machine and who's the human. It can get very interesting in the context of a famous thought experiment named for a great mathematician named Alan Turing. Brian told us about this. It's called the Turing test.

Speaker 1:          00:04:16       Alan Turing, he makes this famous prediction back in 1950 that we'll eventually get to a point sometime around the beginning of this century where we'll stop being able to tell the difference. Well, what specifically was his sort of prophesy? His specific prophecy was that by the year 2000, uh, after five minutes of interacting by text message with a human on one hand and a chat bot on the other hand, uh, 30% of judges would fail to tell which was the human in which was the robot is 30, just like a certain kind of what Turing imagined. And he predicted that as a result of hitting this 30% threshold, we would reach a point he writes where one would speak of machines as being intelligent without expecting to be contradicted. Um, and this just existed as kind of a part of the, the philosophy of computer science until the early 1990s when into the story steps.

Speaker 1:          00:05:14       Hugh Lominger a rogue multimillionaire disco dance floor salesman. Oh, what a rogue millionaire. Plastic portable light-up disco dance floor salesman. The beaches kind of, yeah, the lighting that lights up, but portable, portable, you can make it, you can pick them, wrote millionaire from that. There's apparently millions to be made if if only if only you knew m and m to Logan or this eccentric millionaire, uh, decides that w this was in about 1992 that the technology was starting to get to the point where it would be worth not talking about the turn test as this thought experiment, but actually convening a group of people in a room once a year to actually run the test.

Speaker 4:          00:06:07       Now a bit of background during the lobe, NERC competitions, the actual lobe, no competitions. How it usually works is that you've got some participants. These are the people who have to decide what's going on. They sit at computers and they stare at the computer and they chat with someone on a screen and they don't know the, someone they're chatting with is a person or a Bot behind a curtain. You have that Bot the computer running the Bot and you also have some humans who the participants may or may not be chatting with. They've got to decide, right? Are they chatting with the person or machine? Now, Brian, many, many years ago actually participated in this competition. He was one of the humans behind the curtain that was chatting with the participants. And when we talked to him initially, many years ago, uh, for the talking to machine show, we went into all the strategies that the computer program is we're using that year to try and fool the participants. But the takeaway was that the year that he did it, the computers flopped. By and large, the participants were not fooled. They knew exactly when they were talking to a human and when they were talking to a machine. Now that was a while ago

Speaker 5:          00:07:12       in the green space we asked Brian, where do things stand now? Has it like when we last talked to you, what, what, when did we last? When was it, 201120112011 how has it have we passed

Speaker 1:          00:07:23       the 30% thresholds in the intervening eight years? So in 2014 there was a turning test competition that was held at which the top computer program managed to fool 30% of the judges. Wow. So that's it, right. Depending on how you want to interpret that result, that controversy arose in this particular year because the chat Bot, that one was claiming to be a 13 year old Ukrainian who is just beginning to get a grasp on the English language. Also, the machine was cheating. Right. That's interesting. So it masked its computer notice by in broken grammar. Yeah, exactly right. Or, or if it didn't appear to understand your question, you started to have this story you could play in your own mind of like, oh well maybe I didn't phrase that quite right or something. Has it been broke? Has the, has it, the admin a second winner or a third winner or fourth winner? Um, to the best of my knowledge, we are still sort of flirting under that threshold. Listens. We haven't had any victories since 2014 we thought we might just do this right here, right here.

Speaker 5:          00:08:23       No, no, no. Just look right here in this room. And do our little touring guests. Okay. Unbeknownst to our audience, we had actually lined up a chat bot from a company called Pandora bots that had almost passed the Turing test. It had fooled roughly not quite, but almost 25% of the participants. We got the latest version of the spot and one person. Anyone in the room, um, your, your job will be. We decided to run some tests with the audience starting with just one person. I can see one hand over the actual slate and I don't want to get the

Speaker 3:          00:08:56       first hand, I guess. What about this person over here on the left? Okay,

Speaker 6:          00:09:00       so we brought up this a young woman on stage, put her at a computer and we told her she would be chatting with two different entities. One would be this Bot Pandora Bot and the other would be me, but I was, I went off stage and sat at a computer in the dark when no one could see. She was going to chat with both of us and not know who was who, who is machine. Right.

Speaker 3:          00:09:19       Who is human, you won't know which I get as many questions as I, well I don't, no, no. We're going to give you a time limit. You can't be here though. I'll leave. So after Jedd left the stage and went back into that room up on the screen, came to different chat dialogue boxes. You'll see that we have two options. We've just labeled them for one reason by color when the strawberry, the other is blueberry or code red and code blue. You think you can talk to both of them at the same time? Just jump from one to the other. Sure. We've got any sort of thoughts on how you could suss out whether the thing was a person or a thing? Yeah, I have some thought. I mean like my first tactics are going to be like sort of like very human emotional questions and then we'll like go from there. See what [inaudible] well, I'm not going to ask cause I don't wanna I don't wanna lose your inspiration to therapize this robot. [inaudible] so when I say go, you'll just go and I'll just narrate what you're doing. Okay. Okay. Okay. Three, two one begin. So she started to type and first thing she did was she said hello to strawberry. Okay,

Speaker 7:          00:10:22       so you've gotten the first,

Speaker 3:          00:10:25       well, we've got a somewhat sexual response.

Speaker 7:          00:10:28       Yeah,

Speaker 3:          00:10:29       the machine. I said I like strawberries and then you returned with strawberries are delicious and Oh, now it's getting warmer over there. Blue is a warmer, as a cooler color. Maybe you'd like to go and discuss Aristotle with the Luberon. Then she switched over and started to text the, the blue one is blueberry or the although is high blue, high blue Zb. Okay. That's also a kind of a generous sort of opener. Yes. Hi, bluesy beans. The nickname? Yeah. Okay. Let's say, and blueberry rose behind there. I just realized, I don't even know who I'm talking to. What is your name? You're going to answer? Zandra am I not in your phone?

Speaker 7:          00:11:11       [inaudible]

Speaker 3:          00:11:12       the blueberry has responded with a, a bit of shock. Strawberry. My moms here was red. Well and blueberry. What's wrong, Boone? Nothing's wrong with me. Is there something wrong with them in back and forth and back and forth and blueberry and I have a lot going on. I remember one of these, she doesn't know which is Jack, right on the strawberry side. I cannot believe him right now. You don't believe right now? As far as I know, not unless you have x-ray vision. I'm in the room next to you. Oh, he's trying to coax you into thinking that he's Jan is. That's blueberry. Is that something they do?

Speaker 7:          00:11:50       No, I didn't. [inaudible]

Speaker 3:          00:11:51       yeah, you're at the heart of the question going to ask you to bring this to a conclusion. After a couple minutes of this, we asked the audience, you have strawberry on one side and you look at blueberry on the other. Which one do you think is Jad and which one do you think was the, where's the Bot? How many of you think that Jad is blueberry? A few of you. 13 hands went up, something like that. How many of you think the Jad is strawberry? Almost everybody overwhelming, but interestingly, I volunteer on stage, went against the room. Gee Thought Chad was blueberries. Is the robot. Is that what we all agreed? No. Oh, you're a, you're against the crowd here. Okay. Interesting. Much better theater. Alright. Jad Abumrad where w which one are you? Oh, jade comes out from his hiding place and he tells the crowd. In fact he is

Speaker 7:          00:12:42       strawberry. All right. [inaudible]

Speaker 6:          00:12:46       the crowd was right. I've definitely never had that much chemistry with something that was human, but our volunteer on stage.

Speaker 7:          00:12:53       Got It. Alright. [inaudible] we're going to give you

Speaker 3:          00:13:00       now it seemed that maybe we, we could trust democracy a little bit more and believe that if the most of the people in the room went one way, that's something that would be, you know, that would be important to find out. So we decided to do the entire thing over again for everybody in the room.

Speaker 6:          00:13:17       Yeah. So what we did was we handed out I think 17 different cell phone numbers evenly through the crowd. Yes. Look at the number that is on your envelope. Only you roughly half of those numbers were texting to a machine. Half were texting with a group of humans that were our staff. The crowd did not know which was which. Exactly. So here we go, get ready, get set. And you go. Okay. So the crowd of about a hundred people or so had two minutes ish to text with the person or thing on the other end. And we're going to skip over this part cause it's mostly quiet. People just looking down at their phones, concentrating mightily. But at the end we asked everyone to vote, where are they texting with a person or a [inaudible].

Speaker 3:          00:14:00       And then we asked the ones who had been tricked, who turned out to have guessed wrong, please stand up. Okay, so we're now looking, I believe in this, I'm talking about we are now looking at the upright citizens in this room are the wrong Guytes. You have the seated people are the right type. Correct. So that means that roughly God, I think like 45% of the people were wrong. Meaning that we just passed it.

Speaker 7:          00:14:24       Well I think that's it.

Speaker 6:          00:14:26       He did it. It was a strange moment. We were all clapping at our own demise because you know, touring had laid down this number of 30% and the BOT had fooled way more people than that. Um, I'm just now going to ask you, having been a veteran of this that we should just qualify. This was him really unscientific, super sloppy experiment. But on the other hand, and we talked to Brian about this when he was over, it really does suggest something that maybe what's changed is so much due to the machines

Speaker 3:          00:14:52       becoming more and more articulate. It's more like us, the way we, when I talk to one another these days,

Speaker 1:          00:14:59       gone from interacting in person to talking over the phone to emailing, to texting. And now, I mean for me the great irony is that even to text your phone is proactively suggesting turns of phrase that it thinks you might want to use. And so, I mean, I, I assume many people in this room have had the experience of trying to text something and you try to say it in a, like a sort of a fun, fanciful way or you've tried to make some fun or you use the word, it's not a real word and you're front just sort of slaps slaps that down and just replaces it with something more normal, which make it really hard to use words that aren't the normal words. And so you just stop using those words and you just use the words, the computer they make you use. Exactly. So in a, in a sense, what seems to be happening is that our human communication is becoming more

Speaker 3:          00:15:49       machine like,

Speaker 1:          00:15:51       but at the moment it seems like the touring test is getting passed, not because the machines have met us at our full potential, but because we are using every more and more kind of degraded sort of rote forms of communication with one another.

Speaker 8:          00:16:09       It feels like a slow slide. Downey hill or something. Yes. Down that hill towards the inevitability that we may one day be there. Pets. I don't, I don't like the way this is going no matter who's in, who's doing it. But, uh, in the next segment we're gonna, we're going to flip things a little bit and ask, you know, could the coming age and machines actually make us humans more human? So human should please stick around.

Speaker 3:          00:16:46       This is h j school on Tay calling from Chicago, Illinois. A Radiolab is supported in part by the offered Peace Sloan Foundation, enhancing public understanding of science and technology in the modern world. More information aboutSloan@wwwdotsloan.org

Speaker 2:          00:17:07       [inaudible] on the Eiffel Tower. [inaudible]

Speaker 9:          00:17:15       will you NYC studios presents a special director's cut of the orbiting human circus. We invite you to try a new and exciting form of audio storytelling. The Guardian call it addictive hypnotic, one of the best podcast of the year dying John Cameron Mitchell, Julian Koster and Tim Robbins and featuring Mandy Patinkin and many more. Listen and subscribe to the orbiting human circus. However you get your podcasts.

Speaker 1:          00:17:45       Hey, I'm Jad. I am Robert. This is Radiolab. We're back in the last segment. We gathered

Speaker 10:         00:17:50       a bunch of people in the performance space here at w NYC and we conducted a unscientific version of the Turing test, and in our case, the Bot one, it fooled more than 30% of the people in the room. Now we should point out that the woman who headed up the design of the winning Bot, her name is Lauren Koonzy, she works for a company called Pandorabots and she was actually in the room right there. Selena chair in the audience.

Speaker 7:          00:18:19       [inaudible] heading Lauren. That's Lauren.

Speaker 10:         00:18:21       And it's interesting that one of the things that Lauren mentioned is that the Bot that she designed seems to bring out rather consistently a certain side of people when they chat with it. It's a sound fast. So this Bot over 20% of people who talk to her and millions of conversations every week actually make romantic overtures. And that's pretty consistent across all of the bots on our platform. So there's something wrong with us, not the robot or right. You know. All right, Lauren, which brings up an actually a different kind of question like just for a second, let's forget whether we're being fooled into thinking about is actually a human, maybe the more important question. Given this increasing presence of all these machines in our lives, just like how do they make us behave? Yeah, and we dipped our toe into this world in a touring testy sort of way in that original show seven years ago. I want to play you an expert now to set up what comes after this is freedom bear. Yes, it is. There's not a machine. I don't think so. They're nice to meet both of you. This is an idea that we borrowed from a woman named Freedom Baird who is now a visual artist, but at the time she was a grad student at MIT doing some research and she was also the proud owner of a Furby. Yeah, I've got it right here and you're not going against the mic so we can hear it. Say hello to [inaudible].

Speaker 10:         00:19:43       You describe a Furby for those of us who it's about five inches tall and the Furby has pretty much all head. It's just a big ground, fluffy head with two little feet sticking out the front. It has big eyes apparently it makes noises. You tickle it's tummy, it will cool. It would say me, it would want you to just keep playing with it. So one day she's hanging out with her for a beat and she notices something eerie. What I discovered is if you hold it upside down, it will say nice,

Speaker 11:         00:20:26       scared, [inaudible], scared, scared, and me as the, you know, the sort of owner slash user of this Furby would get really uncomfortable with that and then turn it back up upright.

Speaker 10:         00:20:42       Once you up have an upright, it's, it's fine. It goes right back. It's fine. So it's got some sensor in that knows you know what direction it's facing. Maybe it's just scared. She thought, well wait a second. Now this could be sort of a new way that you could use to draw the line between what's human and what's machine. Yeah. Kind of this kind of emotional Turing test. Can you guys hear me? Yes, I can hear you. If we actually wanted to do this task, could you help? How would we do it exactly? How you guys doing? Yeah, you would need a group of kids. Can you guys tell me Your name? I'm Olivia and I'm alright. I'm thinking six, seven and eight year olds. How old are you guys? The age of reason. You know Lynn's is freedom. We're going to need three things.

Speaker 10:         00:21:35       Furby course, Barbie, Barbie doll. And that's a terrible, a real devil. Yeah, and we did find one separate, turned out to be a hamster. You're a hamster, but we're going to call you Jeremy. So you've got Barbie. Furby, Gerbi Barbie. We just second. What question are we asking in this test? The question was how long can you keep it upside down before you yourself feel uncomfortable? So we should time the kids as they hold each one upside down. Yeah, you're going to have a Barbie that the doll you're going to Jeremy, which is alive now, where would Furby fall in terms of time held upside down? That. I mean, that was really the question. Phase one. Okay, so here's what we're going to do. It's going to be really simple. You would have to say, well, here's a Barbie. Do you guys play with barbies? Just do a couple of things. A few things with Barbie. Barbie is walking, looking at the flower, and then hold Barbie upside down.

Speaker 12:         00:22:29       [inaudible]

Speaker 10:         00:22:29       okay, let's see how long you can hold Barbie like that. I could probably do it obviously very long. Let's just see whenever you feel like you want to turn around this room. I'm happy

Speaker 10:         00:22:49       this went on forever. So let's just fast forward a bit. Gay and [inaudible] programs. So what we learned here in phase one is the not surprising fact that kids can hold Barbie dolls upside down. Yeah, I really was forever. It could have been longer, but their arms got tired. All right, so that was the first task time for phase two. Do the same thing with jurby. So out with Barbie in with Derby. Are we going to have to hold them upside down? That's the test. Yeah. So which one of you would like to, okay, ready? Oh God, you have to hold derby kind of firmly. By the way. No rodents were harmed in this whole situation. Square me and she has pretty screaming get glands to be upside down. Yeah. Okay. Okay. So as you heard, uh, Jeremy, the kids turn Gerbi over very fast. I just didn't want him to get hurt on average eight seconds. I was thinking, oh my God, any kind of pain. And it was a tortured eight seconds. Now phase three, right? So this is a first

Speaker 7:          00:24:12       [inaudible]

Speaker 10:         00:24:12       Louisa you take for being your hand?

Speaker 7:          00:24:16       Yeah.

Speaker 13:         00:24:16       Could you turn Furby upside down? Hold her still.

Speaker 7:          00:24:19       Ah, like [inaudible]. Huh?

Speaker 14:         00:24:45       Whoa.

Speaker 10:         00:24:45       She just turned it over. So [inaudible] was eight seconds Barbie five to infinity. Furby turned out to be and freedom predicted this that a minute. In other words, the kids seem to treat this Furby, this toy more like a journal than a Barbie doll. How come you turn them over so fast? I didn't learn to be scared. Do you think he really felt scared?

Speaker 7:          00:25:09       Yeah. Yeah, I can feel guilty. Really? Yeah. Yeah. Yeah.

Speaker 10:         00:25:15       It's a toil and all that. But still now, do you remember a time when you felt scared?

Speaker 2:          00:25:22       Yeah. Yeah.

Speaker 10:         00:25:24       You don't have to tell me about it, but if you can remember it in your mind.

Speaker 2:          00:25:27       I do.

Speaker 10:         00:25:30       Do you think when Furby says me scared that furbies feeling the same way?

Speaker 2:          00:25:34       Yeah. No, no, no. Yeah. Yeah. Joy or that too.

Speaker 10:         00:25:41       I think that it can feel pain sort of experience with the Furby seem to leave. The kids kind of conflicted going in different directions at once. It was two thoughts. Two thoughts at the same time. Yeah. One thought was like, look, I get it. It's a toy for crying out loud, but another thought was like, I'm still is helpless. It kind of made me feel guilty in a sort of way, hurt or made me feel like a coward, you know? When I was interacting with my Furby a lot, I did have this feeling sometimes of having my chain yanked. Why would you,

Speaker 13:         00:26:17       why would a, is it just a little squeals in its maker or is there something about the toy that makes it good at this? That was kind of my question. So I called up [inaudible] as well. I'll have him, I'm here, this freight train of a guy. Hey, hey, this is Jan from radio lab. Chad from radio lab. Got It. I'm good. Beautiful Day here in Boise. This this point in that old show we ended up talking to a guy named Caleb Chung who designed the Furby. There's rules, there's, you know the size of the eyes. There's the distance of the top lid to the pupil. Right. You don't want any of the top of the white of your eye showing that. So that's, that's freaky surprise. So we talked to them for a long time about all the sort of tricks he used to program the Furby to prompt kids to think of it as a living thing.

Speaker 13:         00:26:57       And he objected interestingly at one point to thinking of it as not exactly a living thing. How is that different than us? Wait a second though, are you really gonna go all the way there? Absolutely. This is a toy with servo motors and things that move. It's eyelids and w a hundred words. So you're saying that life is a level of complexity. If something is alive, it's just more complex. I think I'm saying that life is driven by the need to be alive and by these base primal animal feelings like pain and suffering. I can code that. I can code that. What do you mean you can code that? Anyone who writes software and they do can say, okay, I need to stay alive. Therefore I'm going to come up with ways to stay alive. I'm going to do it in a way that's very human and I'm going to do it.

Speaker 13:         00:27:40       We can mimic these things. [inaudible] is miming the feeling of fear. It's not the same thing as being scared. It's not feeling scared. It is how is it is and then again, a very simply again to a rather long back and forth. Would you say a cockroach is alive, would you say? When I kill a cockroach, I know that it's like what is the exact definition of life? Where's that line between people, machines, but positioned? When we came back to freedom, who'd had gotten us started on this thin interaction? She says, what really stuck with her is that that little toy as simple as it is, can have such a profound effect on a human being. One thing that was really fascinating to me was, um, my husband and I gave a Furby as a gift to his grandmother who had Alzheimer's. And

Speaker 11:         00:28:31       she loved it every day for her was kind of new and somewhat disorienting, but she had this cute little toy that said, kiss me, I love you. And she thought it was the most delightful thing and its little beak was covered with lipstick because she would pick it up and kiss it every day and she didn't actually have a longterm relationship with it. For her, it was always a short term interaction. So the thing, what I'm describing as a kind of thinness for her was, was just right because that's what she was capable of.

Speaker 15:         00:29:11       [inaudible]

Speaker 13:         00:29:12       hello? Hello? Hey Caleb. Hey Caleb. It's Chad. Hey John. How are you? I'm fabulous. Um, Oh, good. It feels like only yesterday we were talking about the sentience of the Furby. Yeah. Isn't that weird? Yeah. So bizarre. And what it was like five years ago or so, we brought Caleb back into the studio because in the years since we spoke with him, he's worked on a lot of these animatronic toys, including a famous one called the pleo. And in the process he's been thinking a lot about how these toys can push our buttons as and how,

Speaker 6:          00:29:42       as a toy maker, that means he's got to be really thoughtful about how he uses that

Speaker 13:         00:29:47       in a word. You want a baby doll right now we've done one and w and the baby doll, an animatronic baby doll is probably the hardest thing to do because you know, you do one thing wrong. It's Chucky if they blink too slow, if their eyes are too wide. And also you're giving it to the most vulnerable of our species, which is our young who are, you know, practicing being nurturing moms for their kids. So let's say the baby just falls asleep, right? We're trying to write in this kind of code and uh, and um, you know, it's got like tilt sensors and stuff. So you've just, you'd give the baby a bottle and you put it down to take a nap. You put 'em down, you're quiet. And so what I want to do as the baby falls asleep, it goes into a deeper sleep. But if you bump it right after it lays down, then it wakes back up. Uh, we're trying to write in this kind of code cause that seems like a nice way to reinforce best practices for a mommy, right? So I know my responsibility, uh, in this,

Speaker 6:          00:30:43       in large part he says, because he hasn't always gotten it right here. Here's an, here's a great example. His name is Plyo Pleo. That's him.

Speaker 16:         00:30:54       [inaudible].

Speaker 6:          00:30:54       I don't know if you've ever seen the Pleo dyno. We did is a robot with artificial intelligence. Clio was a robotic dinosaur, pretty small about a foot from nose to tail. Looked a lot like the dinosaur, little foot from the movie land before time. Very cute, very lifelike. And we went hog

Speaker 13:         00:31:12       wild is inputting real emotions in it and reactions to fear and everything. Right.

Speaker 6:          00:31:20       And it is quite a step forward in terms of how lifelike it is. It makes the Furby look like child's play.

Speaker 6:          00:31:27       It's got a two microphones built in, uh, cameras to track and recognize your face. It can feel the beat of a song. And then with dozens of motors in it, it can then dance along to that song. In total, there are 40 sensors in this toy bump into things in it. So it follows you around lots of love and affection, wanting you to pet it. Whoa, you're tired, Huh? Okay. As you're petting it, it will fall asleep, asleep. It is undeniably adorable. And Caleb says his intent from the beginning was very simple to create a toy that would encourage you to show love and caring.

Speaker 13:         00:32:08       Um, you know, our belief is that that humans need to feel empathy towards things in order to be more human. And we think we can, uh, help that out by having little creatures that you can love.

Speaker 6:          00:32:19       That was a Caleb demonstrating the Pleo at a Ted talk. And what's interesting is it in keeping with this idea of wanting to encourage empathy, he programmed in some behaviors into the pleo that he hoped would nudge people in the right direction. For example, Pleo will let you, you

Speaker 13:         00:32:33       know, if you do something that it doesn't like, so if you actually moved his leg when his motor wasn't moving, I go pop, pop. And uh, he would interpret that as pain or abuse and he would limp around and he would cry and then he'd tremble and then he would take a while before he warmed up to you again. And so what happened is we launched this thing and there was a website called device. This is sort of a tech product review website. They got ahold of a pleo.

Speaker 7:          00:33:00       Oh, and they put up a video.

Speaker 13:         00:33:03       What you see in the video is pleo on a table. [inaudible]

Speaker 7:          00:33:06       beaten badly. [inaudible]

Speaker 13:         00:33:11       you don't see who's doing it exactly. You just see hands coming in from out of the frame and knocking him over again and again.

Speaker 17:         00:33:20       [inaudible]

Speaker 13:         00:33:20       you see the toys, legs in the air struggling to write itself. Sort of like a turtle. That's trying to get off it.

Speaker 7:          00:33:27       [inaudible] oh

Speaker 13:         00:33:31       and it started crying.

Speaker 7:          00:33:34       [inaudible] oh,

Speaker 13:         00:33:36       cause that's what it does. These guys start holding it upside down by its tail. Yeah. They held it by its tail.

Speaker 7:          00:33:42       Ah.

Speaker 13:         00:33:46       They smash its head into the table a few times.

Speaker 7:          00:33:50       [inaudible]

Speaker 13:         00:33:50       and you can see in the video that it responds like it's been stunned.

Speaker 7:          00:33:55       Can you get that is a good test stumbling around? No. At one point they even start strangling it. Oh, it actually starts to choke. Yay. This the like finally they pick it up by the tail one more time. Hell, it might fail and hit it. And it was prying and then it started screaming and I, Eh, hello.

Speaker 13:         00:34:30       They beat it until it died. Right. Well, until it just did not work anymore. This video was viewed about a hundred thousand times, many more times than the reviews of the Pleo in Caleb says there's something about this that he just can't shake because whether it's alive or not, that's, that's exhibiting sociopathic behavior. He's not sure what brought out that sociopathic behavior, whether there was some design in the toy, whether offering people the chance to see a toy in pain in this way somehow brought out curiosity, like a kind of cruel curiosity. He's just not sure what happens when you turn your animatronic baby upside down. Will that cry?

Speaker 17:         00:35:17       [inaudible]

Speaker 13:         00:35:20       I'm not sure yet. I mean, we're working on next versions right now, right? I'm not. What would you do? I mean, it's a good question. You have to have some kind of a response. Otherwise it seems broken. Right. But you know, if you make them react at all, I'm going to get that repetitive abuse because it's cool to watch it scream. It sounds like you have maybe an inner conflict about this. Sure. That you might even be pulling back from making it extra lifelike. Yeah. I'm, I'm for my little company, I've, I've adopted kind of a hippocratic hippocratic oath. Like, you know, don't teach something that's wrong or don't reinforce something that's wrong. And so I've been working on this problem for years. I'm, I'm struggling with what's the, what's the right thing to do?

Speaker 8:          00:36:03       You know? Yeah. Since you have the

Speaker 13:         00:36:06       power, since you have the ability to turn on and off chemicals at some level in another human, right, it's what, which ones do you [inaudible]

Speaker 8:          00:36:16       choose? And so this gets to the bigger question of AI, right? This is the question in AI. I'm going to jump to this because it's really the same question is, you know, how do we create things that can help us? You know, I'm dealing with that on a, on a microscopic scale, but this is the question. And so the first thing that I would try to teach our new AI if I had the ability, is try to understand the concept of empathy. We need to introduce the idea of empathy both in an AI and us for these things. That's where we're at.

Speaker 12:         00:37:00       [inaudible]

Speaker 8:          00:37:00       Caleb says in the specific case of the, of the animatronic babies designing, at least when we talked to him, his thinking was that he might have it if you hold it upside down, cry once or twice, but then stop so that you don't get that repeat thrill.

Speaker 15:         00:37:23       Yeah. Anyway, I was just wondering whether, whether, yeah,

Speaker 6:          00:37:28       back in the green space with Brian Christian and back on the subject of chat bots, we found ourselves asking the very question that Caleb has, is it possible that in this, this is getting kind of grim, it maybe, uh, that in some, since some ways chatbots are good for humans? Yeah. I mean, is there any situation where you can throw in a couple of bots and things get better? Like can chatbots actually be helpful for us? And if so, how?

Speaker 1:          00:37:54       Yeah, there have been some academic studies on trying to use chatbots for these humane benevolent ends, uh, that I think paint this interesting other narrative. And so for example, um, researchers have tried injecting chatbots into Twitter conversations that use hate speech. Um, and this Bot will just show up and be like, hey, that's not cool.

Speaker 15:         00:38:17       Um, um, and so the just like that, it's not cool, man know. I'll

Speaker 1:          00:38:26       say something like, there's, there's real people behind the keyboard and you're really hurting someone's feelings when you talk that way. Um, and you know, it's sort of preliminary work, but there are some studies that appear to suggest, you know, this sharp decline in that user's use of hate speech as, oh, I don't think you should change that. That's it. That's enough. Or do you say you have to say I have 50 trillion followers or something like, well yeah, it actually does depend. So this is interesting. It does depend on the follower account of the Bot that makes the intervention. So if you perceive this Bot to be well, it also requires that you think they're a person. So this is sort of flirting with, with dark magic a little bit. Um, but if you perceive them to be a higher status on the platform than yourself, then you will tend to sheepishly fall in line. But if the BOT has fewer followers than the user, it's trying to correct that. We'll just instigate the bully to bully them now in addition. Wow. I'm so, yeah. Human Nature. Yeah. What we run into, like you want to tell them, we run into this very cool thing and then we're going to finish. But this is like the, this is the,

Speaker 6:          00:39:35       all right. So, uh, we want to tell you one more story because as we were thinking about all this, uh, and trying to find a more optimistic place to land, we bumped into a story, uh, from this guy.

Speaker 18:         00:39:46       Who are you right there? Maybe just go one step back. So, um, I'm a Josh Rothman. I'm a writer for the New Yorker.

Speaker 6:          00:39:54       We brought him into the studio a couple of weeks back.

Speaker 18:         00:39:57       Well, let's come on. So why don't we begin by this, um, story of yours largely takes place in a laboratory in Barcelona. It's a lab. It's in Barcelona. It's run by a fellow named Mel Slater. He struck me as sort of a wizard like man,

Speaker 6:          00:40:11       lot of gray hair laid back

Speaker 18:         00:40:12       and he's married to a woman named [inaudible] Mavi Sanchez. Viva says, I'm a neuroscientist and they have these two VR labs together.

Speaker 6:          00:40:22       VR as in virtual reality. And Josh a little while back, took a trip to Barcelona to experience some of the simulations that Mavi and Mel put people in. He went to come to their campus, showed up at their lab,

Speaker 18:         00:40:34       you feel sort of like you're going to a black box theater. Oh, it's sort of like a lot of big rooms. Um, all covered in black with curtains. There's a lot of dark spaces and the researchers then explained that what's gonna happen is he's going to put on a headset. It's sort of big helmet. They go, they put on the head mounted display and eventually it Carson, the visuals start to fade in. These [inaudible] appears you're standing in a sort of um, generic room. The graphics look straight out of like a windows 95 computer game. It's like the loading screen of the VR and then that dissolves.

Speaker 18:         00:41:15       Then it's replaced with the simulation. And when the simulation started, I was standing in front of a mirror, a digital mirror in this digital world reflecting back at him. His digital self is Avatar. So basically you move and your waist child body moves with you. And I could see in the mirror a reflection of myself. But the person who's who, who the self that I saw reflected, uh, she had a body, she was a woman. She, yeah. So I think when people think of virtual reality, they often imagine wanting to have like realistic experiences in VR. But that's not what Mellon Mommy do. They are interested in VR precisely because it lets you experience things that you could never experience in your real body. In the real world.

Speaker 19:         00:42:00       You can have a body that can be completely transformed and Campbell fan contains color and can change shape. So it can give you a very, very unique tool to explore.

Speaker 6:          00:42:13       You know, in their work they'll often in in these VR worlds turn men into women as they did for Josh for his first time out. They will, um, often take a tall person and then make them a short person in the VR so that they can experience the world as a short person might where they have to kind of crane their neck up a bunch. They'll change the color of your skin in VR and run you through scenarios where you are having to experience the world as another race. And what's remarkable is that in all of these manipulations, um, apparently you adjust to the new body very quickly and they've done physiological tests to measure this, that it takes almost no time at all to feel as if this alien body is actually your,

Speaker 18:         00:42:53       they called us the illusion of presence.

Speaker 19:         00:42:56       You know, we, we think of our body as a very stable entity. However, by running experiments in virtual reality, you see that actually in in one minute of a stimulation what a brain accepts a different body even if this body is quite different from your own.

Speaker 6:          00:43:18       And this flexibility that our brains seem to have can lead to some very surreal situations. This is really the story that brought us to Josh. He told us about another VR adventure where again, he put on the headset, this world faded up

Speaker 18:         00:43:34       and I was sitting in a chair in front of a desk in a really cool looking modernist house,

Speaker 19:         00:43:42       good and floors. And then there is some glass walls and through the glass walls I can see fields with wildflowers getting gas outside.

Speaker 6:          00:43:51       Again, he noticed a mirror and this time the reflection, the mirror was of him. It was a realistic looking Avatar of him. And after checking out his digital self for awhile, he turned his head back to the room and realize that across the room there was another desk.

Speaker 18:         00:44:06       Um, and behind this other desk there was, Freud was sitting there. Who? Freud? Sigmund Freud. Sigmund Freud, the psychoanalyst. So a middle aged man with a big brown beard. He had a beard, he had glasses. And um, he was just sitting there with his hands folded in his lap

Speaker 6:          00:44:23       to Josh's. Sarah taking this all in. He's looking at forward voice, looking back at him and then,

Speaker 19:         00:44:28       okay, okay, now and now hears the voice of a researcher

Speaker 6:          00:44:32       in his ear coming through his VR helmet.

Speaker 20:         00:44:34       Tell fry about your, do your problems, any problem

Speaker 18:         00:44:38       she explained. What you're going to do is you're going to explain a problem that you're having, a personal problem that you're having to Freud, um, something that's bothering you and your life. Then she said, take a minute, think about what you'd like to discuss. Did something immediately jumped to mind? Yeah. So you know, my, uh, my mom had a stroke a few years ago and she's in a nursing home and I'm her guardian, so she's young, she's 65. Um, but because of this stroke, she like needs 24 hour care and she can't talk. She doesn't have any words anymore. So it's a very tough thing for me. We, we, I, I thought really hard about where she should live. I live here in New York. Um, my mom lives in Virginia.

Speaker 6:          00:45:25       Josh says he really debated for a long time. Should he put her in a nursing home in New York where he can be closer to her or should he put her in a nursing home in Virginia where he would be far away.

Speaker 18:         00:45:35       She has all these friends and family members down there. So in the end I decided to, you know, find a place for her there where there's lots of people who can visit her. So I go down maybe once every month or six weeks to see my mom. But then every weekend, you know, someone from this group of friends or family, relatives visits are down there. Whereas if she were up here, you know, I'd be the only person. Um, so that's the decision I made. But um, but you don't feel really happy. But yeah, you know, I feel guilty about it.

Speaker 6:          00:46:02       Like he was a terrible son and he says he would especially have that feeling each week after her friends would visit her in the nursing home and then send him an email update saying, hey, this is how your mom is doing. Every time he would read one of those emails, even if she was doing well, his stomach would just

Speaker 18:         00:46:17       drop this, this problem, this emotion feeling guilty is one I've felt for a while. So I said two for,

Speaker 5:          00:46:27       I said, my mom is in a nursing home in another state and friends and family visit her and they send me reports on how she's doing. And I always feel really bad when I get these reports. And this is said in your voice, if you'd glade keys at the mirror while you were talking, would you be saying it? Yeah. So after he said this to Freud, the world sort of a faded out to black. And then it feed it back in. And suddenly the world had shifted. He was now across the room, behind the desk that had just been opposite him. And he was inside the body of Freud. He looked down at himself, he was wearing a white, white shirt, gray suit. There's a mirror next to that desk. Ne Looks at himself. Um, I have the little beard, you know, everything looked just like Freud. Um, but the main thing that was really surprising was that across, I could see myself, so this is the Avatar of me now. Um, and I watched myself, uh, say what I had just said, so, oh wow. So it plays it back. Their recording

Speaker 21:         00:47:30       is now replayed their movements and also the voice and they see themselves as they talked about their problem.

Speaker 18:         00:47:39       So first I can see my, I'm sitting in the chair and sort of uncomfortable. I'm moving around, I take my hands and um, put them in my lap and fold them together. And then I take them apart. I put them together, you know, I can watch myself be nervous. And then I saw, uh, then I saw myself say what I just said. My mom is in a nursing home in another state, and friends and family visit her and they send me reports, um, in my voice and I always feel really bad. Um, I'm going to moving the way I move. And it was just like me watching myself. Um, and I guess the best way I can describe that was it was, uh, moving, moving, moving like me emotionally. Yeah. Emotionally moving. I mean, I, I felt, um, uh, I don't know if this is gonna make any sense, but you know how there's a point in your life where you realize that your parents are just people? Yes. Yeah. It was kind of like that, except it was me. Oh, interesting. Did you feel, uh, closer to that guy or, or I felt bad for him. He felt bad for him. Sorry. Yeah. I, I, my, my, uh, my, my feelings went out to this other person who was me

Speaker 22:         00:48:57       as he's having this empathetic reaction. As Freud looking back at himself, the researcher's voice again appears in his ear. Hey for advice from the perspective of signal [inaudible] advice of how this could be solved, how you could deal with the, essentially respond to your patient.

Speaker 18:         00:49:15       So I didn't know what to say. So I said, um, why do you think you feel bad?

Speaker 5:          00:49:22       That was, that was, that was a good Friday and kind of thing. Yeah. Why do you think you feel bad as soon as he asked that? Sure. He's back in his body, his virtual body staring back at virtual Freud and he sees a playback of Freud asking him that question. I watched Freud say this to me. Why do you think you feel bad? Except that when Freud talks, they had some thing in the program that made his voice extra deep. Oh, he has some voice distortion, so deeper voice. And so his voice didn't sound like my voice. How did you respond as as now you, I said I feel bad because it doesn't seem right that I'm living far away. Once again, Sean, he switches bodies. Now he's in Freud again staring back at himself. And I watched myself say this, I feel bad because, and then as Freud, I said, well, why? Why are you far away? Then shoot back into his own body. Freud says to him from across the room, why are you far away though? And I said, well, because, um, if my mom lived in New York, right? That'd

Speaker 18:         00:50:22       be the only person here. But if she's down and where she lives, then there's other people to visit her shoe back and Freud's body. And I said, so it sounds like there's, there's a reason why, um, why you live where you live. Um, so if you know that, why, why do you, why do you still feel bad? She switches back to himself. If you know that. Why, why do you, why do you still feel bad? Um, I said something like, um, you're right. In, went back into Freud and then as fried I said, you know, it sounds like the, uh, the thing that's making you unhappy, which is making you feel bad, which is getting these reports from these people is actually the whole reason why you decided to live in these, you know, to have to keep your mom where she is. Like there's, uh, a loop, right? It's like these, these reports I get from my mom's friends make me feel bad. But the whole reason why I decided to leave her in this place in Virginia is specifically so that there were friends who can visit her.

Speaker 6:          00:51:28       Hmm. There's this classic idea in psychology called the reframe, which is where you giant take a problem and reframe that problem into its solution. And he says in that moment he kind of did that. He had this very simple epiphany that his guilt was actually connected to something good.

Speaker 18:         00:51:45       I never had that thought before.

Speaker 6:          00:51:47       And he chose to keep his mom in Virginia so that her friends would visit her more in each time her friends visited. He felt bad, but that meant they were visiting. So the bad feeling and the fact that he was feeling it so much was itself kind of evidence for the fact that he had made, if not the right decision, at least the decision that made sense.

Speaker 18:         00:52:06       The experience I had talking to myself as Freud was, um, was nothing like the experience I had in my own head turning this issue over and over

Speaker 23:         00:52:15       my sweet team back and forth by swapping bodies somehow you can give advice from a different perspective.

Speaker 18:         00:52:24       When I was back into my own body and Freud said it to me, I, it was just like, I just felt like, um, wow, that's so good point. That was [inaudible]

Speaker 3:          00:52:33       but what wouldn't your next thought be? What the Hell is going on here? Why am I able in this utterly fictive situation to split myself into and heal myself?

Speaker 18:         00:52:43       Well, I, I took the headset off and I sat there for a little while while the researchers looked at me. I'm trying to make sense of it. And I, I think what, what I keep coming back to is the seeing yourself just as a person, not as you, not with all the uh, complexities and um, stuff that is any, your self experience of being yourself.

Speaker 6:          00:53:11       And this might be the real key thing. Like when you are in your body, which you pretty much always are, you have all of these thoughts and feelings which are attached to that body. It's sort of like when you go home for Thanksgiving and walk into your parent's kitchen and suddenly you just kind of feel like you're a teenager again. Like all those same thought patterns from your youth kind of kicked back into gear because the context of that kitchen is powerful and you, your body is that writ large. But if you can jump out of it and go into a new one, suddenly all those constraints and all that context is gone.

Speaker 18:         00:53:50       When I'm embodied as Freud, not only do I look different and think this is my body, but I feel different and I have different types of thoughts and I see people differently.

Speaker 6:          00:54:03       And Josh says, well, he saw when he was Freud looking back at himself was just the guy who needed help.

Speaker 18:         00:54:09       When someone comes to you and ask for help, your feelings are not complicated. They're just tenderness, kindness, my, your instinct is to help them.

Speaker 6:          00:54:22       And he says he was able to bring that very simple way of being back to himself.

Speaker 18:         00:54:26       Did it, did it make a difference? Did you walk out of that with, with a different feeling about yourself? Did I, I think, um, I've had a feeling of, I think it revised my feeling about who I was. A little. I think it made me feel a little more, um, I don't even have a word for it. Just a little more human.

Speaker 12:         00:54:55       [inaudible]

Speaker 15:         00:54:59       [inaudible]

Speaker 12:         00:55:03       [inaudible]

Speaker 6:          00:55:03       Josh Rothman is a writer for the New Yorker. His story first appeared there and uh, we told it to that live audience at the green space.

Speaker 1:          00:55:12       Hmm. So Brian, this is, you'll get the last word. I, um, tell me, this is really interesting because the history of chatbots begins with a Chat Bot program written in the 60s by an MIT professor named Joseph Weizenbaum. And the program was called Eliza and it was designed to mimic this non-directive Rogerian therapist where you would say, I'm feeling sad. It would just throw it back to you as a kind of mad lib. I'm sorry to hear you're sad. Why are you sad? Um, and Weizenbaum was famously horrified when he walked in on his secretary just like spilling her life's inner most thoughts and feelings to this program that she had seen him. Right. You know, so there's no, there's no mystery there. But he came away from that experience feeling appalled at the degree to which people will sort of project, um, human intention onto just technology.

Speaker 1:          00:56:08       And his reaction was to pull the plug on his own research project and for the rest of his life he became one of the leading critics against chat bot technology and against AI in general. Um, and I think it's really powerful to juxtapose that against the story that you just shared, which tells us that there's, there's more, there's more to the picture than that. That there are ways to use this technology in a way that doesn't sort of distance us. But in a way that sort of enables us to be more fully human. Um, and I think that's a wonderful way to think about it. Well, why don't we just leave it there. Uh, pleasantly.

Speaker 8:          00:56:49       We have some thanks to give, but if particular particularly thanks to give to the person who made this whole cyber sphere around as possible. That's Lauren Koonsy.

Speaker 7:          00:57:00       That's Lauren. Thank you to a Pandora box.

Speaker 8:          00:57:03       This is a platform that powers, uh, conversational AI software for hundreds of thousands of global brands and developers learn more about their enterprise offering and services@pandorabots.com. Thanks. Also to chance bone for designing the Robert or robot artwork for tonight and of course to Brian Christian for coming here to talk with us today.

Speaker 7:          00:57:22       Thank you. And do you okay. Okay. Thank you all so much.

Speaker 12:         00:57:38       [inaudible]

Speaker 8:          00:57:38       this episode was reported and produced by Simon Adler and our live event was produced with machine like efficiency by Simon Adler in Susie Luxembourg.

Speaker 12:         00:57:48       Yeah. Have to say a word every time you look at me, I can see it all in arise.

Speaker 9:          00:58:05       [inaudible] thinks about [inaudible] [inaudible]

Speaker 12:         00:58:19       even though your bar, I can feel you right by my side. I can read your mind. I can read your mind. I can read your mind. I can read your mind. I can read your mind.

Speaker 8:          00:58:42       By the way, thanks to Dylan Keefe, Alex Overington and Dylan Green for original music.

Speaker 12:         00:58:49       When you hold me now I can see the truth. All the secrets with the heart. You can't hide them anymore. I can feel the power in you. I can read your mind. I can read your mind. I can to keep our love alive. You must be willing to believe. [inaudible]

Speaker 9:          00:59:35       I didn't see it. Now. Never dies. I can really do a mind

Speaker 24:         00:59:45       start with message. Hi, this is Brian Christian radio lab was created by Jad Abumrad and is produced by store and Mueller. Dylan Keith is our director of sound design. Maria Padilla is our managing director. Our staff includes, I'm an Admin, Maggie Bartolomeo, Becca Bressler, Rachel Cusick, David Gabel, Dessel Hab. Tracy Hunt, Matt Kielty, Robert Krulwich, any McEwen Latif. Nasser [inaudible], Donald Ariane wack at Walters and Molly Webster with help from Amanda. Erin Chick Cima or Lee Isley and reed cannon, our fact Checker, and Michelle Harris. And this message.