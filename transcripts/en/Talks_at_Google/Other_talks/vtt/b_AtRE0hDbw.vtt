WEBVTT
Kind: captions
Language: en

00:00:02.160 --> 00:00:04.080
MING: Good afternoon,
my friends.

00:00:04.080 --> 00:00:06.180
My mane is Ming, and
I'm delighted today

00:00:06.180 --> 00:00:10.090
to introduce my friend the
renowned social psychologist

00:00:10.090 --> 00:00:13.550
David DeSteno and the author
of this book, "The Truth About

00:00:13.550 --> 00:00:15.950
Trust."

00:00:15.950 --> 00:00:19.139
I first got to know
David-- thank you, Dana.

00:00:19.139 --> 00:00:21.430
I first got to know David
through his highly innovative

00:00:21.430 --> 00:00:25.840
work in studying the
science of compassion, which

00:00:25.840 --> 00:00:28.120
is a topic I'm very
passionate about.

00:00:28.120 --> 00:00:31.700
David and his lab, they
are renowned for devising

00:00:31.700 --> 00:00:34.030
very creative
methods in studying

00:00:34.030 --> 00:00:37.380
how emotional states
affect behavior.

00:00:37.380 --> 00:00:40.510
And they are also known
for studying moral behavior

00:00:40.510 --> 00:00:42.220
in real time.

00:00:42.220 --> 00:00:47.750
Real time, not fake time like
the other labs, real time.

00:00:47.750 --> 00:00:50.640
David is most
interested to figure out

00:00:50.640 --> 00:00:55.580
how to foster prosocial
behavior all around the world.

00:00:55.580 --> 00:00:58.910
And he told me that
in summary his work

00:00:58.910 --> 00:01:03.630
can be summarised in three
words, vice and virtue.

00:01:03.630 --> 00:01:07.726
Vice and virtue, there's one
that I prefer over the other.

00:01:07.726 --> 00:01:10.100
And with that, my friends,
let's please welcome my friend

00:01:10.100 --> 00:01:10.683
David DeSteno.

00:01:15.939 --> 00:01:17.730
DAVID DESTENO: And
thank you for having me.

00:01:17.730 --> 00:01:19.930
It's a real honor to be
with you here and share

00:01:19.930 --> 00:01:20.849
this work with you.

00:01:20.849 --> 00:01:22.890
And as you can probably
guess by the title today,

00:01:22.890 --> 00:01:25.040
I'm going to talk about trust.

00:01:25.040 --> 00:01:27.460
I think it probably comes
as no surprise to you

00:01:27.460 --> 00:01:31.050
that issues and dilemmas
of trust pervade our lives.

00:01:31.050 --> 00:01:35.260
Trust determines who we want
to work with, who we love

00:01:35.260 --> 00:01:39.360
and who would marry, who
we trust to learn from,

00:01:39.360 --> 00:01:42.010
who we'll go to for support.

00:01:42.010 --> 00:01:43.635
Now, we all can
remember the big stuff,

00:01:43.635 --> 00:01:45.040
the times trust really matters.

00:01:45.040 --> 00:01:48.665
Is a new business partner
going to be trustworthy,

00:01:48.665 --> 00:01:50.820
or is he going to skim profits?

00:01:50.820 --> 00:01:54.210
Is a spouse being
faithful or unfaithful?

00:01:54.210 --> 00:01:57.960
Is a child using drugs even
when she swears, trust me.

00:01:57.960 --> 00:01:58.460
Trust me.

00:01:58.460 --> 00:01:58.959
Trust me.

00:01:58.959 --> 00:01:59.750
I'm not.

00:01:59.750 --> 00:02:02.830
But issues of trust aren't
just about those potentially

00:02:02.830 --> 00:02:04.350
momentous situations.

00:02:04.350 --> 00:02:07.710
Issues of trust pervade
our common daily life.

00:02:07.710 --> 00:02:11.690
Will my neighbor remember and I
trust him to really feed my dog

00:02:11.690 --> 00:02:14.540
while I'm away, or am I going to
come home and the dog's hungry?

00:02:14.540 --> 00:02:16.700
The mechanic, is he
really being honest when

00:02:16.700 --> 00:02:19.110
he says my car need
a new transmission?

00:02:19.110 --> 00:02:21.880
Was the salesperson when I
bought this suit really honest

00:02:21.880 --> 00:02:24.340
when he told me it
makes me look thin?

00:02:24.340 --> 00:02:25.150
You can tell me.

00:02:25.150 --> 00:02:25.950
I don't know.

00:02:25.950 --> 00:02:26.960
I won't comment on that.

00:02:26.960 --> 00:02:29.180
But whether it's big
or whether it's small,

00:02:29.180 --> 00:02:32.160
what all of these issues have
in common is a simple dynamic.

00:02:32.160 --> 00:02:34.960
They really depend on trust.

00:02:34.960 --> 00:02:38.960
And we know the more
we trust individuals,

00:02:38.960 --> 00:02:40.980
we can gain a lot
more by working

00:02:40.980 --> 00:02:42.540
together and cooperating.

00:02:42.540 --> 00:02:44.380
But in reality, as you
probably can guess,

00:02:44.380 --> 00:02:46.360
trust is a double-edged sword.

00:02:46.360 --> 00:02:48.360
Yes, we can gain more
by working together.

00:02:48.360 --> 00:02:50.480
That's why we have trust
in the first place.

00:02:50.480 --> 00:02:55.040
But trusting somebody also makes
us vulnerable to that person.

00:02:55.040 --> 00:02:57.990
It means that our outcomes
are dependent on them being

00:02:57.990 --> 00:02:59.630
competent, on them
having integrity,

00:02:59.630 --> 00:03:02.570
on them working with us.

00:03:02.570 --> 00:03:06.670
And so given that trust is
so central to human life,

00:03:06.670 --> 00:03:09.380
you would hope, you would
like to think that we really

00:03:09.380 --> 00:03:12.340
understand how it works, that we
can make really good decisions

00:03:12.340 --> 00:03:14.090
about who we should
trust or whether we're

00:03:14.090 --> 00:03:16.164
going to be
trustworthy ourselves.

00:03:16.164 --> 00:03:18.580
But I'm here to tell you we're
not really good about that.

00:03:18.580 --> 00:03:20.830
And until recently, the
science underlying that

00:03:20.830 --> 00:03:22.750
hasn't been really
good about it.

00:03:22.750 --> 00:03:26.160
And so in some ways, that's
what led me to write this book.

00:03:26.160 --> 00:03:27.790
As a scientist, I
really wanted to work

00:03:27.790 --> 00:03:30.640
on correcting a lot of
the misconceptions that

00:03:30.640 --> 00:03:33.770
are out there to empower
people to make better

00:03:33.770 --> 00:03:37.290
decisions but also so
that we can work together

00:03:37.290 --> 00:03:40.220
to nudge us all to nudge
society to become more

00:03:40.220 --> 00:03:43.160
trustworthy and
cooperative overall.

00:03:43.160 --> 00:03:45.099
So to do that, you
have to start like you

00:03:45.099 --> 00:03:46.140
would with anything else.

00:03:46.140 --> 00:03:47.848
You have to get rid
of the misconceptions

00:03:47.848 --> 00:03:50.690
and figure out how
trust really works.

00:03:50.690 --> 00:03:53.110
And so that's what I want to
talk about today in general.

00:03:53.110 --> 00:03:54.460
In the book, I talk
about lots of issues

00:03:54.460 --> 00:03:55.510
that I'm not going
to talk about today.

00:03:55.510 --> 00:03:57.450
We talk about
issues of how trust

00:03:57.450 --> 00:03:59.830
affects learning and
academic success.

00:03:59.830 --> 00:04:03.360
One of the best predictors
of a child's academic success

00:04:03.360 --> 00:04:05.496
isn't how much they
like their teacher.

00:04:05.496 --> 00:04:07.120
It's how much they
trust their teacher.

00:04:07.120 --> 00:04:08.536
And they trust
that the teacher is

00:04:08.536 --> 00:04:11.490
competent in telling them
and giving them information.

00:04:11.490 --> 00:04:14.680
We talk about how trust
affects our relationships

00:04:14.680 --> 00:04:16.930
and especially
romantic ones and how

00:04:16.930 --> 00:04:19.880
it can function to smooth out
the bumps in those in ways that

00:04:19.880 --> 00:04:21.630
operate even below our
conscious awareness

00:04:21.630 --> 00:04:24.392
to keep harmony
with those we love.

00:04:24.392 --> 00:04:25.850
In the book, I talk
about how trust

00:04:25.850 --> 00:04:27.180
is affected by power and money.

00:04:27.180 --> 00:04:28.679
There's great work
out there showing

00:04:28.679 --> 00:04:32.021
that people's trustworthiness
tracks socioeconomic status.

00:04:32.021 --> 00:04:33.520
This is work by my
friend Paul Piff.

00:04:33.520 --> 00:04:35.710
He's a psychologist
at Berkeley where

00:04:35.710 --> 00:04:37.480
he shows that higher
SES correlates

00:04:37.480 --> 00:04:40.220
to increased untrustworthiness.

00:04:40.220 --> 00:04:43.199
But really it's not
about being in the 1%.

00:04:43.199 --> 00:04:45.740
It's not a birthright of the 1%
that makes you untrustworthy.

00:04:45.740 --> 00:04:48.050
It's simply about
money and power

00:04:48.050 --> 00:04:50.100
relative to those around you.

00:04:50.100 --> 00:04:52.370
And so any of you, if
we put you in a position

00:04:52.370 --> 00:04:55.730
even for 10 minutes where you
feel elevated sense of power,

00:04:55.730 --> 00:04:58.540
it becomes a lot more
difficult in some ways

00:04:58.540 --> 00:05:00.330
to actually be trustworthy.

00:05:00.330 --> 00:05:02.690
And also how and when
can you trust yourself?

00:05:02.690 --> 00:05:03.660
It's already February.

00:05:03.660 --> 00:05:06.034
A lot of New Year's resolutions
have gone by the wayside,

00:05:06.034 --> 00:05:08.180
so if it's a good thing to know.

00:05:08.180 --> 00:05:11.590
But today what I want to talk
about is three broader themes.

00:05:11.590 --> 00:05:13.980
And the first is what does
it mean to be trustworthy

00:05:13.980 --> 00:05:16.680
and how can we understand how
trust operates within ourselves

00:05:16.680 --> 00:05:18.330
and our own trustworthiness?

00:05:18.330 --> 00:05:21.249
The second is can
we actually detect

00:05:21.249 --> 00:05:23.290
whether somebody else is
going to be trustworthy?

00:05:23.290 --> 00:05:25.180
In some ways, this has
been the holy grail

00:05:25.180 --> 00:05:28.007
of governmental research
and security research.

00:05:28.007 --> 00:05:29.340
And we've been pretty bad at it.

00:05:29.340 --> 00:05:31.006
But I have some new
data I want to share

00:05:31.006 --> 00:05:33.334
with you that
suggests we can do it.

00:05:33.334 --> 00:05:35.250
And then finally, the
question that's probably

00:05:35.250 --> 00:05:37.630
closest to my heart in the
work that I normally do,

00:05:37.630 --> 00:05:40.710
which is, how can we increase
trustworthiness and thereby

00:05:40.710 --> 00:05:44.400
increase our own and each
other's resilience around us?

00:05:44.400 --> 00:05:48.050
So let's start with
the first question.

00:05:48.050 --> 00:05:49.920
Most of us, when we
think about trust,

00:05:49.920 --> 00:05:53.100
we think about it as
this stable trait.

00:05:53.100 --> 00:05:56.860
A person's trustworthy
or they're not.

00:05:56.860 --> 00:05:59.100
But I want to convince
you that that's probably

00:05:59.100 --> 00:06:01.310
not the best way
to think about it.

00:06:01.310 --> 00:06:03.090
That's not how it really works.

00:06:03.090 --> 00:06:06.800
Growing up, we have this idea
that it's a typical motif,

00:06:06.800 --> 00:06:07.300
right?

00:06:07.300 --> 00:06:08.710
You see it in
cartoons all the time.

00:06:08.710 --> 00:06:11.126
There's an angel on one shoulder
and a devil on the other,

00:06:11.126 --> 00:06:12.900
and they whisper into your ears.

00:06:12.900 --> 00:06:14.850
And if you grow up
listening to the angel,

00:06:14.850 --> 00:06:16.400
well then you're going
to be a good person.

00:06:16.400 --> 00:06:17.290
You're going to be trustworthy.

00:06:17.290 --> 00:06:18.070
Everybody is going to love you.

00:06:18.070 --> 00:06:19.680
Everything's going to be good.

00:06:19.680 --> 00:06:22.460
There's just one
problem with that.

00:06:22.460 --> 00:06:27.120
And that is if you actually
look at the scientific data,

00:06:27.120 --> 00:06:28.430
it doesn't really hold up.

00:06:28.430 --> 00:06:31.110
What we've learned over
the past decade especially

00:06:31.110 --> 00:06:33.720
in psychological science is
that people's moral behavior

00:06:33.720 --> 00:06:36.750
is a lot more variable than
any of us would have expected.

00:06:36.750 --> 00:06:39.130
And it's a lot more
influenced by the situation.

00:06:39.130 --> 00:06:41.100
And so if you want to
control your own behavior

00:06:41.100 --> 00:06:42.974
and predict the behavior
of those around you,

00:06:42.974 --> 00:06:45.480
you need to realize that
it's not a stable trait.

00:06:45.480 --> 00:06:49.060
You need to understand how
it's affected by the situation.

00:06:49.060 --> 00:06:53.430
And so my model for
understanding trustworthiness,

00:06:53.430 --> 00:06:56.710
it's better to think of it
as a scale, the old school

00:06:56.710 --> 00:06:58.920
type of type with
the plates that go up

00:06:58.920 --> 00:07:00.710
and down as opposed
to a digital one.

00:07:00.710 --> 00:07:04.530
In any one moment, your mind,
whether you know it or not,

00:07:04.530 --> 00:07:06.650
is weighing two types of cost.

00:07:06.650 --> 00:07:10.210
It's weighing costs and
benefits in the short term

00:07:10.210 --> 00:07:12.370
versus costs and benefits
in the long term.

00:07:12.370 --> 00:07:14.790
And those usually correlate
with what's good for me

00:07:14.790 --> 00:07:18.090
in an expedient fashion right
now versus what's good for me

00:07:18.090 --> 00:07:22.050
to do even if it costs me in
the moment to built a reputation

00:07:22.050 --> 00:07:25.170
and to build social
bonds in the long term.

00:07:25.170 --> 00:07:29.340
And depending upon the
situation, which decision you

00:07:29.340 --> 00:07:32.692
choose can change
from moment to moment.

00:07:32.692 --> 00:07:33.650
You can think about it.

00:07:33.650 --> 00:07:36.590
If my friend Ming
loans me money,

00:07:36.590 --> 00:07:39.080
in the moment if I don't pay
him back, well, I'm ahead.

00:07:39.080 --> 00:07:40.750
I've profited in the short term.

00:07:40.750 --> 00:07:42.690
But long term, it's
probably a poor decision

00:07:42.690 --> 00:07:44.230
because he's not going
to give me money again.

00:07:44.230 --> 00:07:46.271
I'm going to get a reputation
as being a cheater.

00:07:46.271 --> 00:07:48.650
But if I can get away
with it, my mind,

00:07:48.650 --> 00:07:51.940
unbeknownst to me and my own
moral codes that I endorse,

00:07:51.940 --> 00:07:55.942
will try to push me to
be a bit untrustworthy.

00:07:55.942 --> 00:07:57.525
And so I want to
suggest to all of you

00:07:57.525 --> 00:07:59.410
who think this
can't happen to me

00:07:59.410 --> 00:08:01.410
and that you are
completely honest

00:08:01.410 --> 00:08:05.290
and trustworthy and wonderful,
it can happen to any of us.

00:08:05.290 --> 00:08:07.730
And let me show you an
example of how it happens

00:08:07.730 --> 00:08:10.610
and also why you probably
don't think it's true of you

00:08:10.610 --> 00:08:12.570
even though it is.

00:08:12.570 --> 00:08:15.220
So the first issue is how do
you study trustworthiness?

00:08:15.220 --> 00:08:17.930
I can't really walk around with
a clipboard and say, Cindy,

00:08:17.930 --> 00:08:19.780
are you a trustworthy person?

00:08:19.780 --> 00:08:21.850
Because what people will
probably say is they'll

00:08:21.850 --> 00:08:22.310
do one of two things.

00:08:22.310 --> 00:08:24.616
Either they know they're
not and they'll, yes I am.

00:08:24.616 --> 00:08:25.990
Because who wants
to say I'm not?

00:08:25.990 --> 00:08:29.070
But what happens
more frequently is

00:08:29.070 --> 00:08:31.250
they think they are, and
they predict they will be,

00:08:31.250 --> 00:08:33.870
but when push comes to shove,
time and again our behavior

00:08:33.870 --> 00:08:35.380
isn't what we expect.

00:08:35.380 --> 00:08:38.539
And so the way that we have to
study trustworthiness is not

00:08:38.539 --> 00:08:41.230
by asking people or looking
at their past reputations

00:08:41.230 --> 00:08:44.400
but by staging events
in real time as opposed

00:08:44.400 --> 00:08:48.440
to fake time where we can
actually see when push comes

00:08:48.440 --> 00:08:51.055
to shove, what will
people actually do

00:08:51.055 --> 00:08:53.550
when real rewards
are on the line?

00:08:53.550 --> 00:08:56.040
So let me give you an
example of how we do this.

00:08:56.040 --> 00:08:58.360
So we set up an experiment
in our lab to look at this,

00:08:58.360 --> 00:09:00.300
and it's rather simple.

00:09:00.300 --> 00:09:01.910
We bring people into the lab.

00:09:01.910 --> 00:09:04.240
These are normal
community members

00:09:04.240 --> 00:09:07.910
or even undergraduates
from the Boston community

00:09:07.910 --> 00:09:09.620
all known to be
trustworthy people.

00:09:09.620 --> 00:09:11.330
We bring them in
and say, look, we've

00:09:11.330 --> 00:09:13.580
got two tasks that
need to be done.

00:09:13.580 --> 00:09:16.460
One is really long
and onerous and it's

00:09:16.460 --> 00:09:21.290
these terrible logic
problems, and circling letters

00:09:21.290 --> 00:09:23.680
E, and random digit
strings, and all the things

00:09:23.680 --> 00:09:26.930
that you would feel like is
a big waste of your time.

00:09:26.930 --> 00:09:32.350
Or you can do a fun photo
hunt on the computer.

00:09:32.350 --> 00:09:33.880
Here's a coin.

00:09:33.880 --> 00:09:37.710
I want you to flip the coin,
and whatever one you get,

00:09:37.710 --> 00:09:40.210
it will determine whether you
do the photo hunt or the logic

00:09:40.210 --> 00:09:40.710
problems.

00:09:40.710 --> 00:09:42.920
And whichever one you
don't do, the person

00:09:42.920 --> 00:09:46.620
sitting in the next
room is going to get.

00:09:46.620 --> 00:09:49.280
And we're going to trust you
to do this the right way.

00:09:49.280 --> 00:09:49.780
Is that OK?

00:09:49.780 --> 00:09:51.290
They say sure.

00:09:51.290 --> 00:09:52.657
And then we let them go.

00:09:52.657 --> 00:09:53.740
What do you think happens?

00:09:56.330 --> 00:10:00.710
A lot of people just assign
themselves to the good task.

00:10:00.710 --> 00:10:03.930
Any guesses for how many?

00:10:03.930 --> 00:10:05.130
80%?

00:10:05.130 --> 00:10:07.200
Close, 90%.

00:10:07.200 --> 00:10:09.374
We've done this
many, many times.

00:10:09.374 --> 00:10:10.540
So it's not a fluke finding.

00:10:10.540 --> 00:10:11.581
We've done it in our lab.

00:10:11.581 --> 00:10:13.300
Other people have
copied the methodology.

00:10:13.300 --> 00:10:15.652
90% of people-- well
they do one other thing.

00:10:15.652 --> 00:10:17.360
Some of them don't
flip the coin and just

00:10:17.360 --> 00:10:19.440
say, oh, I got the good
task when they come out.

00:10:19.440 --> 00:10:21.690
Or some of them, because we
have them on hidden video,

00:10:21.690 --> 00:10:25.512
flip the coin repeatedly until
they get the answer they want,

00:10:25.512 --> 00:10:27.220
which is the same as
not flipping at all.

00:10:27.220 --> 00:10:29.520
But they feel better
about themselves.

00:10:29.520 --> 00:10:31.700
And so these are people
who we asked them

00:10:31.700 --> 00:10:34.490
before, if you don't flip the
coin, is that untrustworthy?

00:10:34.490 --> 00:10:36.690
They said, oh, it would
be terribly untrustworthy.

00:10:36.690 --> 00:10:37.930
But they do it.

00:10:37.930 --> 00:10:39.840
And if you ask them
when they come out,

00:10:39.840 --> 00:10:43.200
we have them rate on a
computer how trustworthily they

00:10:43.200 --> 00:10:43.930
just acted.

00:10:43.930 --> 00:10:45.980
So here higher numbers
mean higher trust

00:10:45.980 --> 00:10:47.720
on a one to seven scale.

00:10:47.720 --> 00:10:50.200
So when they're judging
themselves doing this,

00:10:50.200 --> 00:10:51.580
they're above the midpoint.

00:10:51.580 --> 00:10:53.760
So they say, yeah, it was OK.

00:10:53.760 --> 00:10:55.050
I was trustworthy.

00:10:55.050 --> 00:10:57.710
If you take those same
people and you now

00:10:57.710 --> 00:11:00.810
have them watch
somebody else do this,

00:11:00.810 --> 00:11:02.350
they condemn the person for it.

00:11:02.350 --> 00:11:04.370
That person was not trustworthy.

00:11:04.370 --> 00:11:05.670
When I did it, it was OK.

00:11:05.670 --> 00:11:09.190
When that person did it, they're
definitely not trustworthy.

00:11:09.190 --> 00:11:12.797
Now, the interesting
thing about this

00:11:12.797 --> 00:11:17.230
is these were people
who are normal people.

00:11:17.230 --> 00:11:19.770
And so when we see people
like Lance Armstrong or Bernie

00:11:19.770 --> 00:11:21.890
Madoff, you think, oh,
it's something wrong.

00:11:21.890 --> 00:11:26.540
Those people are morally
corrupt and untrustworthy.

00:11:26.540 --> 00:11:29.120
No, well, yes what they
did was untrustworthy.

00:11:29.120 --> 00:11:31.510
But the same process-- on
a smaller scale, of course,

00:11:31.510 --> 00:11:34.670
and we can only study on a
smaller scale in the lab--

00:11:34.670 --> 00:11:35.550
happens with us.

00:11:35.550 --> 00:11:36.730
It happens with any of you.

00:11:36.730 --> 00:11:40.620
Now, the question is, well,
why don't we realize this?

00:11:40.620 --> 00:11:42.540
Why don't we learn to
stop trusting ourselves?

00:11:42.540 --> 00:11:47.730
Well, the reason why is our mind
whitewashes our own behavior.

00:11:47.730 --> 00:11:52.739
So if you ask these subjects,
why did you not flip the coin?

00:11:52.739 --> 00:11:53.530
They'll say things.

00:11:53.530 --> 00:11:56.240
They'll create stores like,
well, yeah, I should have,

00:11:56.240 --> 00:11:58.672
but today I was late
for an appointment.

00:11:58.672 --> 00:12:00.630
And if I'm not there,
somebody depending on me.

00:12:00.630 --> 00:12:01.338
And so it was OK.

00:12:01.338 --> 00:12:03.840
So they'll create all
kinds of justifications

00:12:03.840 --> 00:12:05.847
for why it was OK for
them in the situation

00:12:05.847 --> 00:12:07.430
and how it doesn't
reflect on the fact

00:12:07.430 --> 00:12:08.930
that they're an
untrustworthy person

00:12:08.930 --> 00:12:10.450
but they can be untrustworthy.

00:12:10.450 --> 00:12:12.430
Now, in some way
that's a good thing.

00:12:12.430 --> 00:12:15.120
It has to be adaptive,
because if any of us

00:12:15.120 --> 00:12:18.050
felt like we couldn't
trust ourselves,

00:12:18.050 --> 00:12:19.580
that alternative is much worse.

00:12:19.580 --> 00:12:22.320
Because it means we're not going
to save money for the future

00:12:22.320 --> 00:12:25.260
because we know future us is
going to go blow it a casino.

00:12:25.260 --> 00:12:28.129
We're not going to diet
and take care of our health

00:12:28.129 --> 00:12:29.670
because we assume
three days from now

00:12:29.670 --> 00:12:32.175
I'm going to gorge on ice
cream or chocolate cake.

00:12:32.175 --> 00:12:33.300
We're stuck with ourselves.

00:12:33.300 --> 00:12:35.545
If somebody else
is untrustworthy,

00:12:35.545 --> 00:12:37.300
we can stop
interacting with them.

00:12:37.300 --> 00:12:39.330
We can't stop
interacting with ourself,

00:12:39.330 --> 00:12:41.570
and so we need to
trust ourselves even

00:12:41.570 --> 00:12:43.129
when we make mistakes.

00:12:43.129 --> 00:12:44.670
So that's OK, but
what I'm here to do

00:12:44.670 --> 00:12:45.890
is to help you
try and learn that

00:12:45.890 --> 00:12:48.056
so that you can decrease
the probability that you're

00:12:48.056 --> 00:12:49.800
actually going to
make those mistakes.

00:12:49.800 --> 00:12:51.540
But what I haven't
told you yet is

00:12:51.540 --> 00:12:53.456
that there's any evidence
that people actually

00:12:53.456 --> 00:12:56.360
recognize what
they did was wrong.

00:12:56.360 --> 00:12:58.160
So let me give you an example.

00:12:58.160 --> 00:13:00.050
So in psychology
we had this method

00:13:00.050 --> 00:13:02.020
which is called cognitive load.

00:13:02.020 --> 00:13:04.760
And it's a way to kind of tie
up people's mental resources

00:13:04.760 --> 00:13:07.030
so they can engage
in rationalization.

00:13:07.030 --> 00:13:08.800
And the way it works
is we give them

00:13:08.800 --> 00:13:11.867
random digit strings of
numbers, say like seven digits.

00:13:11.867 --> 00:13:13.450
And you have to
remember these digits.

00:13:13.450 --> 00:13:15.750
So what we're doing is you'll
get a string of numbers,

00:13:15.750 --> 00:13:18.250
and you'll have to say
7-6-5-4-1-0, 7-6-5-4-1-0,

00:13:18.250 --> 00:13:20.000
and then you'll have
to answer a question,

00:13:20.000 --> 00:13:22.140
how trustworthily
did you just act?

00:13:22.140 --> 00:13:23.320
And you have to remember
these numbers because I'm

00:13:23.320 --> 00:13:24.730
going to have you
enter them in a minute,

00:13:24.730 --> 00:13:26.290
and you've got to
get them right.

00:13:26.290 --> 00:13:28.610
And so what this does
is it ties up your mind.

00:13:28.610 --> 00:13:31.789
It prevents your mind from
engaging in rationalization.

00:13:31.789 --> 00:13:33.330
So when we did this
experiment again,

00:13:33.330 --> 00:13:37.680
and we have 90% of people who
did cheat even though they said

00:13:37.680 --> 00:13:42.070
they wouldn't, what you
find is on the white bars

00:13:42.070 --> 00:13:44.980
on the bottom, those who
were under cognitive load,

00:13:44.980 --> 00:13:47.161
there's no difference in
how you judge yourself

00:13:47.161 --> 00:13:48.160
or how you judge others.

00:13:48.160 --> 00:13:49.690
And those are
significantly less.

00:13:49.690 --> 00:13:51.189
You see yourself
as less trustworthy

00:13:51.189 --> 00:13:53.720
than when you have the
time to rationalize.

00:13:53.720 --> 00:13:55.810
So the second, the
moment that you're

00:13:55.810 --> 00:13:58.940
committing the transgression,
your mind knows it.

00:13:58.940 --> 00:14:01.370
You feel in your gut.

00:14:01.370 --> 00:14:03.244
You feel that pang of guilt.

00:14:03.244 --> 00:14:04.660
But what happens
is you don't want

00:14:04.660 --> 00:14:06.820
to think of yourself
as untrustworthy.

00:14:06.820 --> 00:14:10.200
And so your mind engages
in this rationalization.

00:14:10.200 --> 00:14:13.700
The good you tamps away,
tamps down the guilt so

00:14:13.700 --> 00:14:15.300
that it can create
a view of you.

00:14:15.300 --> 00:14:18.200
Well, I had a
reason, and it's OK.

00:14:18.200 --> 00:14:20.910
And I am trustworthy.

00:14:20.910 --> 00:14:22.702
So the point is to
remember that all of us,

00:14:22.702 --> 00:14:24.576
even if we think of
ourselves as trustworthy,

00:14:24.576 --> 00:14:26.669
I'm sure most of you in
general are trustworthy,

00:14:26.669 --> 00:14:28.460
but your mind is making
these calculations.

00:14:28.460 --> 00:14:30.680
Here when we gave them
anonymity-- or at least

00:14:30.680 --> 00:14:32.055
they thought they
were anonymous.

00:14:32.055 --> 00:14:34.620
They didn't know we have them
on hidden video-- their mind's

00:14:34.620 --> 00:14:37.847
impulses for short-term
gain created a story.

00:14:37.847 --> 00:14:40.180
It pushed them to say, well,
I can get away with it now.

00:14:40.180 --> 00:14:41.679
Even not consciously,
it just pushes

00:14:41.679 --> 00:14:44.090
them to make this decision
as an impulsive way.

00:14:44.090 --> 00:14:47.060
And then they justify it because
the long-term consequences

00:14:47.060 --> 00:14:49.310
they believe are not
there, because they

00:14:49.310 --> 00:14:50.690
believe they're anonymous.

00:14:53.320 --> 00:14:56.320
Let's turn to the
second question.

00:14:56.320 --> 00:15:00.620
The second question
is, can I trust you?

00:15:00.620 --> 00:15:03.100
How do you figure,
how do you determine

00:15:03.100 --> 00:15:04.860
that question about somebody?

00:15:04.860 --> 00:15:07.990
Now, as we all
know, human society

00:15:07.990 --> 00:15:11.210
flourishes when we
cooperate with each other

00:15:11.210 --> 00:15:13.420
and when we trust each other.

00:15:13.420 --> 00:15:15.940
The problem is if
one person doesn't

00:15:15.940 --> 00:15:19.000
uphold his or her
end of the bargain,

00:15:19.000 --> 00:15:21.550
that person can gain
at the other's expense.

00:15:21.550 --> 00:15:24.650
And so what you have is a very
dynamic yet delicate balance

00:15:24.650 --> 00:15:28.300
that we every day have to
navigate through and optimize

00:15:28.300 --> 00:15:29.870
our outcomes.

00:15:29.870 --> 00:15:33.811
If we make the wrong
decision over and over again,

00:15:33.811 --> 00:15:35.060
we're going to have a problem.

00:15:35.060 --> 00:15:39.645
So here what we try to do is we
try to use people's reputation.

00:15:39.645 --> 00:15:42.240
Now, as I just told
you, reputation

00:15:42.240 --> 00:15:45.660
isn't a great predictor,
and so often we're wrong.

00:15:45.660 --> 00:15:47.775
But the problem that
confronts us other times

00:15:47.775 --> 00:15:49.400
is sometimes we have
to decide if we're

00:15:49.400 --> 00:15:52.282
going to trust somebody new who
we don't know anything about.

00:15:52.282 --> 00:15:53.740
And we don't know
their reputation,

00:15:53.740 --> 00:15:55.380
yet we're negotiating with them.

00:15:55.380 --> 00:15:56.400
What do you do there?

00:15:56.400 --> 00:15:58.120
You have the opportunity
for establishing

00:15:58.120 --> 00:16:00.640
a long-term relationship
or you have the opportunity

00:16:00.640 --> 00:16:04.080
for being screwed over in a
way that you couldn't predict.

00:16:04.080 --> 00:16:07.630
And if you're wrong, well,
time and time again that's

00:16:07.630 --> 00:16:10.060
going to cause you
a lot of problems.

00:16:10.060 --> 00:16:15.260
It's a very non-optimal
outcome to be wrong.

00:16:15.260 --> 00:16:17.480
So given all that,
it would be nice

00:16:17.480 --> 00:16:19.900
if we could actually
detect if somebody else was

00:16:19.900 --> 00:16:21.226
going to be trustworthy.

00:16:21.226 --> 00:16:23.100
Now, as I said at the
beginning of this talk,

00:16:23.100 --> 00:16:25.240
people have been looking
for the Holy Grail of what

00:16:25.240 --> 00:16:27.490
signifies deception
or untrustworthiness

00:16:27.490 --> 00:16:29.210
for a long time.

00:16:29.210 --> 00:16:31.620
Is it a true smile?

00:16:31.620 --> 00:16:32.990
Does that mean I can trust you?

00:16:32.990 --> 00:16:33.900
Is it shifty eyes?

00:16:33.900 --> 00:16:36.020
Does that mean I
can't trust you?

00:16:36.020 --> 00:16:39.700
And the TSA spent $40
million on this program

00:16:39.700 --> 00:16:42.410
to look for these
single microexpressions

00:16:42.410 --> 00:16:45.900
that in GAO testimony
before Congress

00:16:45.900 --> 00:16:48.720
has been shown to
be utterly useless.

00:16:48.720 --> 00:16:51.370
And the problem is I think the
reason why we haven't found how

00:16:51.370 --> 00:16:54.060
we can detect trustworthiness
is we've been going about it

00:16:54.060 --> 00:16:56.690
in really the wrong way.

00:16:56.690 --> 00:16:58.990
There is not going
to be one marker.

00:16:58.990 --> 00:17:01.512
There is not going
to be one golden cue.

00:17:01.512 --> 00:17:03.970
Cues to trustworthiness are
going to be subtle and dynamic.

00:17:03.970 --> 00:17:05.920
Why is that the case?

00:17:05.920 --> 00:17:09.832
Well, it's very adaptive
if I'm standing here

00:17:09.832 --> 00:17:11.790
and you're looking and
me and all of a sudden I

00:17:11.790 --> 00:17:14.940
see a major threat
behind you to show fear.

00:17:14.940 --> 00:17:17.190
Because that lets you know
even without turning around

00:17:17.190 --> 00:17:19.560
very quickly, there's
something dangerous there.

00:17:19.560 --> 00:17:21.030
But trust isn't
something that you

00:17:21.030 --> 00:17:24.490
want to communicate very
easily or untrustworthiness.

00:17:24.490 --> 00:17:24.990
Why?

00:17:24.990 --> 00:17:27.750
I mean, imagine if
you're trustworthy person

00:17:27.750 --> 00:17:29.891
and you had a clear tell.

00:17:29.891 --> 00:17:32.140
It's like walking around
with a big T on your forehead

00:17:32.140 --> 00:17:33.970
that says, I'm trustworthy.

00:17:33.970 --> 00:17:36.160
What would happen?

00:17:36.160 --> 00:17:38.225
Everybody would want
to cooperate with you,

00:17:38.225 --> 00:17:40.600
more of them so they could
probably take advantage of you

00:17:40.600 --> 00:17:41.826
because they know they could.

00:17:41.826 --> 00:17:44.450
Or if you were untrustworthy and
you walked around with a big U

00:17:44.450 --> 00:17:46.200
on your forehead, well,
everybody would ignore you.

00:17:46.200 --> 00:17:47.700
And nobody would
cooperate with you.

00:17:47.700 --> 00:17:49.630
And your outcomes would be poor.

00:17:49.630 --> 00:17:52.600
And so trust signals have to
be played close to the vest.

00:17:52.600 --> 00:17:54.110
We have to interact
with each other.

00:17:54.110 --> 00:17:55.110
I can get a feeling for you.

00:17:55.110 --> 00:17:56.320
You can get a feeling for me.

00:17:56.320 --> 00:18:00.130
And then we can decide and
reveal our cards very slowly.

00:18:00.130 --> 00:18:01.992
So they're going to
be subtle and dynamic.

00:18:01.992 --> 00:18:03.950
They're also going to be
the context dependent.

00:18:03.950 --> 00:18:07.760
What signals trust in any one
specific culture may vary.

00:18:07.760 --> 00:18:10.145
What signals trust in any
one situation may vary.

00:18:10.145 --> 00:18:10.770
Think about it.

00:18:10.770 --> 00:18:12.450
There's different
kinds of trust.

00:18:12.450 --> 00:18:13.790
There's integrity.

00:18:13.790 --> 00:18:16.350
So can I trust that
you're going to do

00:18:16.350 --> 00:18:18.260
the best job you can to help me?

00:18:18.260 --> 00:18:20.710
Are you meaning well toward me?

00:18:20.710 --> 00:18:23.437
That's different than
trusting your competence.

00:18:23.437 --> 00:18:25.270
If you don't have the
competence to help me,

00:18:25.270 --> 00:18:29.240
all the intention in the
world is going to be useless.

00:18:29.240 --> 00:18:32.000
And so the cues I look for for
competence versus integrity

00:18:32.000 --> 00:18:34.320
may be very different, and
we have to think about that.

00:18:34.320 --> 00:18:36.070
But the main reason
why I think we haven't

00:18:36.070 --> 00:18:39.910
found the cues to trust is that
they're going to occur in sets.

00:18:39.910 --> 00:18:42.040
I mean, think about it, right?

00:18:42.040 --> 00:18:45.800
If touching my face means I'm
going to be untrustworthy,

00:18:45.800 --> 00:18:49.090
if I do this, am I doing
that because I've got an itch

00:18:49.090 --> 00:18:51.520
or because I'm
going to cheat you?

00:18:51.520 --> 00:18:52.960
Don't know by one thing.

00:18:52.960 --> 00:18:54.150
You can't tell.

00:18:54.150 --> 00:18:57.090
The only way you can begin to
read cues to trustworthiness

00:18:57.090 --> 00:18:59.900
is to look for them occurring
in sets so you can disambiguate

00:18:59.900 --> 00:19:02.160
the meaning of any single one.

00:19:02.160 --> 00:19:04.220
And that's what the field
typically doesn't do.

00:19:04.220 --> 00:19:07.250
And so I'm going to quickly
tell you about two experiments

00:19:07.250 --> 00:19:09.929
that we did to show
how trust can be read.

00:19:09.929 --> 00:19:11.470
The first one is
kind of exploratory.

00:19:11.470 --> 00:19:13.820
We threw out everything
that we had known before,

00:19:13.820 --> 00:19:16.920
and we simply started to try
and identify what cues actually

00:19:16.920 --> 00:19:20.530
predict real monetary
trustworthy behavior

00:19:20.530 --> 00:19:23.310
and to demonstrate that they
do this in an accurate way.

00:19:23.310 --> 00:19:27.830
And the second part was
designed to actually confirm

00:19:27.830 --> 00:19:31.470
in a very tightly controlled,
highly precise way

00:19:31.470 --> 00:19:33.957
that these are the
cues that matter.

00:19:33.957 --> 00:19:36.040
And I'll show you what I
mean by that in a second.

00:19:36.040 --> 00:19:40.090
We have an exploratory phase
and a confirmatory phase.

00:19:40.090 --> 00:19:42.860
So how did we do
this I will start

00:19:42.860 --> 00:19:44.960
with the exploratory phase.

00:19:44.960 --> 00:19:48.250
What are candidates for
signals related to trust?

00:19:48.250 --> 00:19:52.330
Well, we brought 86
people into the lab

00:19:52.330 --> 00:19:54.980
and we put them into dyads,
which are groups of two.

00:19:54.980 --> 00:19:57.380
The only requirement is you
couldn't know the person

00:19:57.380 --> 00:20:00.440
with whom you were
now going to interact.

00:20:00.440 --> 00:20:03.240
We gave them five minutes
to have a get-to-know-you

00:20:03.240 --> 00:20:03.800
conversation.

00:20:03.800 --> 00:20:06.140
You could talk about
anything you want.

00:20:06.140 --> 00:20:08.475
We gave them a list
of topics, but they

00:20:08.475 --> 00:20:10.058
could talk about
anything they wanted.

00:20:10.058 --> 00:20:13.140
And you're going to play a game
for real money, a game that

00:20:13.140 --> 00:20:17.140
pits self-interest versus being
trustworthy, communal interest.

00:20:17.140 --> 00:20:19.397
And I'll show you how the
game works in a second.

00:20:19.397 --> 00:20:20.980
And then we gave
them topics to start,

00:20:20.980 --> 00:20:23.514
but they could talk about
anything that they wanted.

00:20:23.514 --> 00:20:24.430
So we brought them in.

00:20:24.430 --> 00:20:26.990
They simply sat
across from each other

00:20:26.990 --> 00:20:28.780
at a table, half the subjects.

00:20:28.780 --> 00:20:30.950
And we had three cameras
on them that were time

00:20:30.950 --> 00:20:34.100
locked so we could record
every single gesture,

00:20:34.100 --> 00:20:36.570
every single cue they made.

00:20:36.570 --> 00:20:39.190
Now, we also had another
group of subjects

00:20:39.190 --> 00:20:41.720
who conversed in
their get to know you

00:20:41.720 --> 00:20:44.890
in separate rooms
over Google Chat

00:20:44.890 --> 00:20:47.760
or Gchat-- any type
of internet chat.

00:20:47.760 --> 00:20:51.240
And the logic for this is the
same amount of information

00:20:51.240 --> 00:20:52.880
is being exchanged
in the conversation,

00:20:52.880 --> 00:20:54.940
but in one condition
you have access

00:20:54.940 --> 00:20:57.280
to the person's nonverbal cues.

00:20:57.280 --> 00:20:59.610
In the other you don't.

00:20:59.610 --> 00:21:01.444
And then we brought
them into separate rooms

00:21:01.444 --> 00:21:03.193
if they weren't in
separate rooms already.

00:21:03.193 --> 00:21:05.140
And we said, you're
going to play this game.

00:21:05.140 --> 00:21:08.640
We gave each of
them four tokens.

00:21:08.640 --> 00:21:11.670
And the tokens are
worth $1 to each of them

00:21:11.670 --> 00:21:14.360
but $2 to their partner.

00:21:14.360 --> 00:21:16.580
And so this game is
called the give some game.

00:21:16.580 --> 00:21:18.589
And it's a nice analog
for self-interest

00:21:18.589 --> 00:21:19.630
versus communal interest.

00:21:19.630 --> 00:21:21.220
Because if you
want to be selfish,

00:21:21.220 --> 00:21:23.810
you can try and get the other
person to give you all of his

00:21:23.810 --> 00:21:24.600
and give nothing.

00:21:24.600 --> 00:21:28.437
And that means you'll have
$12 and he'll have nothing.

00:21:28.437 --> 00:21:30.520
But the most trustworthy
thing to do if you really

00:21:30.520 --> 00:21:32.600
implicitly trust each other
and want to benefit each other

00:21:32.600 --> 00:21:34.660
is to exchange all you
have at the same time,

00:21:34.660 --> 00:21:36.490
because then you all
started with four

00:21:36.490 --> 00:21:37.890
and now you have eight.

00:21:37.890 --> 00:21:40.770
And so we had people
making real decisions

00:21:40.770 --> 00:21:42.060
and we paid them accordingly.

00:21:42.060 --> 00:21:43.470
And we also had
them tell us what

00:21:43.470 --> 00:21:46.900
they thought their
partner was going to do.

00:21:46.900 --> 00:21:49.390
Now, the nice thing about
it was whether or not

00:21:49.390 --> 00:21:53.730
you talked to your partner
over an internet chat

00:21:53.730 --> 00:21:56.860
or face to face, the amount
of trustworthy behavior

00:21:56.860 --> 00:21:58.270
didn't change, which is nice.

00:21:58.270 --> 00:22:00.520
I think it's because people
are now becoming very used

00:22:00.520 --> 00:22:04.236
to communicating over
internet mediated platforms.

00:22:04.236 --> 00:22:05.860
And so it's not like
being face to face

00:22:05.860 --> 00:22:07.260
made people more trustworthy.

00:22:07.260 --> 00:22:09.300
There was people who
were cheating and being

00:22:09.300 --> 00:22:11.920
cooperative at equal
levels in both cases.

00:22:11.920 --> 00:22:17.330
But here the axis is error,
the amount that you were off.

00:22:17.330 --> 00:22:21.000
And so lower bars mean accuracy
in terms of absolute value.

00:22:21.000 --> 00:22:24.020
If you were in the presence
of the other person,

00:22:24.020 --> 00:22:26.330
your guess for how
much that person

00:22:26.330 --> 00:22:29.395
was going to be trustworthy or
cheat you in absolute dollars

00:22:29.395 --> 00:22:31.410
was significantly greater.

00:22:31.410 --> 00:22:35.390
So what this tells us is that
people are picking up on a cue.

00:22:35.390 --> 00:22:38.110
There is some information there
that your mind is gleaning

00:22:38.110 --> 00:22:41.140
from body language,
whether you know it or not.

00:22:41.140 --> 00:22:43.290
And so what we did
next was we ran

00:22:43.290 --> 00:22:46.650
models of all these possible
combinations of cues

00:22:46.650 --> 00:22:47.940
to see what would matter.

00:22:47.940 --> 00:22:51.360
And the model that
predicted untrustworthiness

00:22:51.360 --> 00:22:55.090
the best consisted of four
cues, touching your hands,

00:22:55.090 --> 00:22:58.627
touching your face, crossing
your arms, and leaning away.

00:22:58.627 --> 00:23:00.710
If you think about it,
what does this really mean?

00:23:00.710 --> 00:23:02.501
Well, we know from the
nonverbal literature

00:23:02.501 --> 00:23:04.680
that fidgeting with
your hands and touching

00:23:04.680 --> 00:23:08.080
your face repeatedly is
usually a marker of anxiety

00:23:08.080 --> 00:23:10.420
and not feeling comfortable.

00:23:10.420 --> 00:23:12.090
Crossing your arms
and leaning away

00:23:12.090 --> 00:23:14.430
is a marker of I don't
want to affiliate with you.

00:23:14.430 --> 00:23:16.010
Put them together,
what does it mean?

00:23:16.010 --> 00:23:18.990
It means, I don't really
want to be with you.

00:23:18.990 --> 00:23:19.830
I don't like you.

00:23:19.830 --> 00:23:21.204
And I'm nervous
because I'm going

00:23:21.204 --> 00:23:22.850
to screw you over in a minute.

00:23:22.850 --> 00:23:26.020
And so none of these cues
predict it on their own,

00:23:26.020 --> 00:23:28.220
but together they did.

00:23:28.220 --> 00:23:33.470
So the more often you saw a
partner show this set of cues,

00:23:33.470 --> 00:23:35.679
the smaller number of tokens
you expected that person

00:23:35.679 --> 00:23:38.136
to share with you, which meant
the more selfish you thought

00:23:38.136 --> 00:23:39.820
that person was going to be.

00:23:39.820 --> 00:23:42.280
And the more often you
yourself or any subject

00:23:42.280 --> 00:23:45.430
emitted this four set of
cues, the less trustworthy you

00:23:45.430 --> 00:23:48.080
actually were, the
more tokens you

00:23:48.080 --> 00:23:50.677
kept and tried to get from the
other person without sharing.

00:23:50.677 --> 00:23:53.260
And so in some sense, what we're
showing is ground truth here.

00:23:53.260 --> 00:23:56.290
These cues are predicting
actual financial cheating

00:23:56.290 --> 00:23:57.966
versus cooperative behavior.

00:23:57.966 --> 00:23:59.590
Now, the most
interesting part about it

00:23:59.590 --> 00:24:03.480
was if you asked our
subjects, so what did you use?

00:24:03.480 --> 00:24:04.470
They had no idea.

00:24:04.470 --> 00:24:06.320
Or they would suggest
it was other cues that

00:24:06.320 --> 00:24:07.710
didn't predict anything.

00:24:07.710 --> 00:24:09.668
Yeah, I showed you they
were more accurate when

00:24:09.668 --> 00:24:10.520
they saw the person.

00:24:10.520 --> 00:24:14.130
And they adjusted their numbers
and guesses accordingly.

00:24:14.130 --> 00:24:17.590
So what this means is your
mind is using these cues

00:24:17.590 --> 00:24:20.660
even though you're not
aware of what they are.

00:24:20.660 --> 00:24:23.666
It's still building
intuitions with them.

00:24:23.666 --> 00:24:25.540
But how do you know
those are the right cues?

00:24:25.540 --> 00:24:26.600
People are doing lots of things.

00:24:26.600 --> 00:24:28.475
How do I know that when
I'm crossing my arms,

00:24:28.475 --> 00:24:30.380
it's not that my left
pupil is dilating

00:24:30.380 --> 00:24:32.700
and that's the magic cue.

00:24:32.700 --> 00:24:35.470
Well, being scientists, we
needed to have precise control.

00:24:35.470 --> 00:24:37.190
No matter what actor
I had, I couldn't

00:24:37.190 --> 00:24:40.010
get them to have
exceedingly precise control

00:24:40.010 --> 00:24:42.400
of every expression
they're emitting.

00:24:42.400 --> 00:24:43.840
So what do you do?

00:24:43.840 --> 00:24:45.580
You need a robot.

00:24:45.580 --> 00:24:48.100
So this robot, her name is Nexi.

00:24:48.100 --> 00:24:50.700
She was designed and created
by my collaborator Cynthia

00:24:50.700 --> 00:24:52.830
Breazeal at MIT's Media Lab.

00:24:52.830 --> 00:24:53.995
And so we simply used Nexi.

00:24:53.995 --> 00:24:56.310
And I'll show you a
video of it in a moment.

00:24:56.310 --> 00:24:57.770
But the experiment was simple.

00:24:57.770 --> 00:25:00.040
We repeated the same
thing we did before,

00:25:00.040 --> 00:25:04.250
except we replaced one of
the people with the robot.

00:25:04.250 --> 00:25:06.390
So now you're talking
to this robot.

00:25:06.390 --> 00:25:10.560
And the robot will emit the
cues and express the cues

00:25:10.560 --> 00:25:14.340
that we think signify
untrustworthiness or not

00:25:14.340 --> 00:25:16.610
in a very, very precisely
controllable way.

00:25:16.610 --> 00:25:18.470
The robot was controlled
by two people, one

00:25:18.470 --> 00:25:21.670
who was the voice of the robot,
the other who would control

00:25:21.670 --> 00:25:23.190
whether or not
she made the cues.

00:25:23.190 --> 00:25:25.190
Because you don't want
the same person doing it,

00:25:25.190 --> 00:25:27.744
because they might give
cues in their vocal tone.

00:25:27.744 --> 00:25:29.660
And because I know a lot
of you are engineers,

00:25:29.660 --> 00:25:31.576
I'll give you a quick
idea of how this worked.

00:25:31.576 --> 00:25:33.630
The one person here
you can see who

00:25:33.630 --> 00:25:35.130
is sitting in front
of the computer,

00:25:35.130 --> 00:25:36.740
there's a webcam on her face.

00:25:36.740 --> 00:25:40.120
As she moves her head,
it's gotten by the webcam.

00:25:40.120 --> 00:25:41.660
The robot's head
moves in real time.

00:25:41.660 --> 00:25:43.570
She's wearing a mic
here, so as she speaks,

00:25:43.570 --> 00:25:45.660
it picks up the phonemes
and the robot's mouth

00:25:45.660 --> 00:25:47.001
moves in real time.

00:25:47.001 --> 00:25:48.750
The next person controls
whether the robot

00:25:48.750 --> 00:25:51.880
gives these untrustworthy
cues or other similar cues.

00:25:51.880 --> 00:25:55.360
And the third person is
our robot mischief person

00:25:55.360 --> 00:25:58.060
who basically controls
and monitors the system.

00:25:58.060 --> 00:26:00.510
Because every once in
awhile, it would go haywire

00:26:00.510 --> 00:26:04.510
and the robot would like
it's doing something crazy,

00:26:04.510 --> 00:26:06.997
like it's possessed
or something.

00:26:06.997 --> 00:26:07.830
It would break down.

00:26:07.830 --> 00:26:10.720
But normally it worked
wonderfully and just fine.

00:26:10.720 --> 00:26:12.719
And so we had participants,
65 participants

00:26:12.719 --> 00:26:13.510
from the community.

00:26:13.510 --> 00:26:15.740
We brought in 31 of them.

00:26:15.740 --> 00:26:17.250
We showed that we
had seen the cues

00:26:17.250 --> 00:26:20.435
that Nexi meant untrustworthy.

00:26:20.435 --> 00:26:22.340
The others didn't.

00:26:22.340 --> 00:26:24.410
Here's a picture of
it crossing its arms.

00:26:24.410 --> 00:26:26.500
Here's a picture of
it touching its face.

00:26:26.500 --> 00:26:27.390
I'll show you a
video in a second.

00:26:27.390 --> 00:26:29.460
So the first part is people
have to get used to the fact

00:26:29.460 --> 00:26:30.793
that they're talking to a robot.

00:26:30.793 --> 00:26:32.860
So we had this kind of
part where they just

00:26:32.860 --> 00:26:33.957
acclimated to it.

00:26:33.957 --> 00:26:35.290
And here's what that looks like.

00:26:35.290 --> 00:26:35.956
[VIDEO PLAYBACK]

00:26:35.956 --> 00:26:37.690
NEXI: So my name's Nexi.

00:26:37.690 --> 00:26:39.130
What's your name?

00:26:39.130 --> 00:26:40.550
KIM: My name's Kim.

00:26:40.550 --> 00:26:41.050
NEXI: Kim?

00:26:41.050 --> 00:26:42.490
It's very nice to meet you.

00:26:42.490 --> 00:26:44.070
KIM: You too.

00:26:44.070 --> 00:26:46.515
NEXI: To get started
today, why don't I

00:26:46.515 --> 00:26:47.982
tell you a little
bit about myself?

00:26:47.982 --> 00:26:49.449
KIM: OK.

00:26:49.449 --> 00:26:52.138
NEXI: I was born and
built at the MIT Media

00:26:52.138 --> 00:26:54.828
Lab two years ago.

00:26:54.828 --> 00:26:58.590
So I guess in human
years, I'm pretty young.

00:26:58.590 --> 00:27:02.077
But in robot years,
that's more like being 20.

00:27:02.077 --> 00:27:03.360
KIM: (NERVOUS LAUGH).

00:27:03.360 --> 00:27:06.445
DAVID DESTENO: So you can see
she's a little uncomfortable.

00:27:06.445 --> 00:27:08.980
In fact, we had to put that
black barrier on the bottom

00:27:08.980 --> 00:27:10.355
because people
were afraid it was

00:27:10.355 --> 00:27:12.796
going to go Terminator
on them and kill them.

00:27:12.796 --> 00:27:14.170
So we needed that
little barrier.

00:27:14.170 --> 00:27:17.102
But they quickly acclimated
to this, as you'll see.

00:27:17.102 --> 00:27:17.768
[VIDEO PLAYBACK]

00:27:17.768 --> 00:27:19.684
KIM: That's basically
all I do for fun though.

00:27:19.684 --> 00:27:21.980
I don't have a lot of time.

00:27:21.980 --> 00:27:25.070
NEXI: Did you grow up
in Upstate New York?

00:27:25.070 --> 00:27:30.350
KIM: Yeah, I did until I was
18, when I moved out here.

00:27:30.350 --> 00:27:31.850
NEXI: It seems
like that must have

00:27:31.850 --> 00:27:34.571
been a big transition for you.

00:27:34.571 --> 00:27:35.553
KIM: It was.

00:27:35.553 --> 00:27:38.008
It was a really big transition.

00:27:38.008 --> 00:27:43.389
But I kind of decided that it
wasn't the life that I wanted.

00:27:43.389 --> 00:27:45.180
DAVID DESTENO: So they
would self disclose.

00:27:45.180 --> 00:27:47.530
We heard about pets dying
and all these things.

00:27:47.530 --> 00:27:51.260
One person kept asking the
robot if it believed in God.

00:27:51.260 --> 00:27:54.130
That person was hard.

00:27:54.130 --> 00:27:55.814
But for the most
part, people behave.

00:27:55.814 --> 00:27:57.980
And just so you can tell
what it looks like face on,

00:27:57.980 --> 00:27:58.650
I'll just show you
a 10-second clip.

00:27:58.650 --> 00:27:59.316
[VIDEO PLAYBACK]

00:27:59.316 --> 00:28:01.350
NEXI: We all share
a big, open room.

00:28:01.350 --> 00:28:04.766
There are lots of
cords and gadgets.

00:28:04.766 --> 00:28:08.670
So it's probably
not like your house.

00:28:08.670 --> 00:28:11.350
But it's home for me.

00:28:11.350 --> 00:28:14.710
Why don't you tell me
about where you're from?

00:28:14.710 --> 00:28:17.210
MAN: Well, I was born in
Lawrence, Massachusetts,

00:28:17.210 --> 00:28:21.210
and I had a residency
in Somerville right now.

00:28:21.210 --> 00:28:24.190
I've been doing residential
the past four months.

00:28:24.190 --> 00:28:26.710
DAVID DESTENO:
And so we then had

00:28:26.710 --> 00:28:28.580
them play this game
with the robot.

00:28:28.580 --> 00:28:30.620
We told them, look, the robot's
got an artificial intelligence

00:28:30.620 --> 00:28:33.120
algorithm that it's going to
decide how much money it wants

00:28:33.120 --> 00:28:35.320
to give you and how
much money it thinks

00:28:35.320 --> 00:28:38.980
you're going to give it based
on how the interaction went.

00:28:38.980 --> 00:28:40.930
It didn't, but that's
what we told them.

00:28:40.930 --> 00:28:42.660
And then we asked them
questions about how much they

00:28:42.660 --> 00:28:43.920
trusted the robot, et cetera.

00:28:43.920 --> 00:28:45.190
So what happened?

00:28:45.190 --> 00:28:50.240
So to make a long story short,
what happened is these are,

00:28:50.240 --> 00:28:52.370
for those of you who are
mathematically inclined,

00:28:52.370 --> 00:28:55.510
these are standardized
regression coefficients.

00:28:55.510 --> 00:28:58.670
When Nexi made the cues that
signaled untrustworthiness

00:28:58.670 --> 00:29:00.250
in the human to
human interactions,

00:29:00.250 --> 00:29:01.720
people reported
trusting it less.

00:29:01.720 --> 00:29:03.400
Now, the important
thing is they didn't

00:29:03.400 --> 00:29:05.380
report liking it less,
because I was worried,

00:29:05.380 --> 00:29:07.660
oh, they just might think
it's doing something weird.

00:29:07.660 --> 00:29:10.157
No, they liked it equally,
but they trusted it less.

00:29:10.157 --> 00:29:12.490
Now, that's important, because
to me that makes it real.

00:29:12.490 --> 00:29:14.130
Because we all have
friends that we

00:29:14.130 --> 00:29:17.440
like who we wouldn't
trust with our money.

00:29:17.440 --> 00:29:19.750
And so OK, and the
less they trusted

00:29:19.750 --> 00:29:23.035
it, the fewer tokens they
predicted Nexi would give them,

00:29:23.035 --> 00:29:24.910
basically meaning they
thought Nexi was going

00:29:24.910 --> 00:29:28.200
to be selfish and cheat them,
and the smaller amount of money

00:29:28.200 --> 00:29:30.110
in that game they
actually gave it.

00:29:30.110 --> 00:29:33.080
And so what this
tells us is that we

00:29:33.080 --> 00:29:35.580
know these are the cues
because we manipulated them

00:29:35.580 --> 00:29:38.265
with exact precision here while
nothing else was happening

00:29:38.265 --> 00:29:40.230
or things were happening
that we could control.

00:29:40.230 --> 00:29:41.700
And so cues to
trustworthiness can

00:29:41.700 --> 00:29:45.680
be imperfectly assessed,
but better than chance.

00:29:45.680 --> 00:29:48.770
And so the TSA starts
to need to look

00:29:48.770 --> 00:29:51.440
for cues in sets in a
context-dependent way.

00:29:51.440 --> 00:29:53.950
But in some ways, the more
interesting part of this

00:29:53.950 --> 00:29:57.400
is that what it suggests is that
technology is now good enough

00:29:57.400 --> 00:29:59.560
that the mind will now
use these cues to ascribe

00:29:59.560 --> 00:30:05.390
moral intent to robots, or to
avatars, or to virtual agents.

00:30:05.390 --> 00:30:09.140
So while you may not
get it from R2-D2,

00:30:09.140 --> 00:30:13.880
you will probably get it from
Wall-E. See, I'll hear aww.

00:30:13.880 --> 00:30:17.210
Wall-E is not
human in the least,

00:30:17.210 --> 00:30:20.470
but he has enough human
characteristics in the eyes

00:30:20.470 --> 00:30:22.900
and in the hands
that he can move them

00:30:22.900 --> 00:30:28.000
in a way that pings our
mind's mental machinery

00:30:28.000 --> 00:30:30.250
to make us feel
trust, or warmth,

00:30:30.250 --> 00:30:31.890
or compassion toward him.

00:30:31.890 --> 00:30:32.920
So what does this mean?

00:30:32.920 --> 00:30:35.780
It's a whole Pandora's box,
because in some ways it's good.

00:30:35.780 --> 00:30:38.590
So for people like Cynthia who
want to design these robots

00:30:38.590 --> 00:30:40.970
and she's working
on the smaller ones

00:30:40.970 --> 00:30:42.915
so that they can
actually accompany kids

00:30:42.915 --> 00:30:44.790
for medical treatment
where parents can't go.

00:30:44.790 --> 00:30:46.790
Think radiation treatments
for kids with cancer.

00:30:46.790 --> 00:30:49.810
They can go with the children.

00:30:49.810 --> 00:30:53.590
They'll seem more
trustworthy, more comforting.

00:30:53.590 --> 00:30:55.985
But like any other science,
it's not good or bad.

00:30:55.985 --> 00:30:58.110
It depends on the uses of
the people who want them.

00:30:58.110 --> 00:30:59.610
We all know trust
sells, so if I'm

00:30:59.610 --> 00:31:01.250
a marketer, what does this mean?

00:31:01.250 --> 00:31:04.096
It means that I have the perfect
trustworthy or untrustworthy

00:31:04.096 --> 00:31:05.220
person that I can show you.

00:31:05.220 --> 00:31:07.980
Because in a human, stuff leaks.

00:31:07.980 --> 00:31:10.466
No matter how much we're
going to try and control it,

00:31:10.466 --> 00:31:12.590
which is why we could pick
up on untrustworthiness.

00:31:12.590 --> 00:31:14.507
There is no leaking here.

00:31:14.507 --> 00:31:15.590
We can control everything.

00:31:15.590 --> 00:31:18.420
And so as we're conversing more
and more with automated agents

00:31:18.420 --> 00:31:22.860
and avatars, our trust is
going to be manipulated in ways

00:31:22.860 --> 00:31:25.390
that we could never
have known before

00:31:25.390 --> 00:31:29.860
or that our mind is not
ready to defend against.

00:31:29.860 --> 00:31:32.040
OK, finally, the last
part of the talk,

00:31:32.040 --> 00:31:36.900
how do we go about enhancing
trustworthiness and enhancing

00:31:36.900 --> 00:31:40.670
the compassion and
resilience of each other?

00:31:40.670 --> 00:31:43.690
To let you know just
how powerful this can be

00:31:43.690 --> 00:31:46.550
and how quickly trust can change
and compassion can change,

00:31:46.550 --> 00:31:48.437
let me give you of
my favorite examples.

00:31:48.437 --> 00:31:49.770
Some of you may know this story.

00:31:49.770 --> 00:31:54.010
It's called the Christmas
Eve truce of World War I.

00:31:54.010 --> 00:31:55.960
So it was 1914, and
the British were

00:31:55.960 --> 00:31:59.879
fighting the Germans
outside of Ypres, Belgium.

00:31:59.879 --> 00:32:01.670
And it had been a long
and a bloody battle.

00:32:01.670 --> 00:32:03.586
And they were each in
their trenches separated

00:32:03.586 --> 00:32:06.090
by the no-man's land in between.

00:32:06.090 --> 00:32:07.840
And on Christmas
Eve, as the Brits

00:32:07.840 --> 00:32:10.430
looked across the
no-man's land, they

00:32:10.430 --> 00:32:12.540
started to see lights appear.

00:32:12.540 --> 00:32:14.310
And then they started
to hear songs.

00:32:14.310 --> 00:32:15.090
And at first, they
didn't know what

00:32:15.090 --> 00:32:16.565
they were because
they were in German,

00:32:16.565 --> 00:32:17.773
and they didn't speak German.

00:32:17.773 --> 00:32:19.960
But then they soon
recognized the melodies.

00:32:19.960 --> 00:32:22.070
And what they were
were Christmas carols.

00:32:22.070 --> 00:32:24.940
And what happened
next was amazing.

00:32:24.940 --> 00:32:26.930
The men came out
of their trenches,

00:32:26.930 --> 00:32:28.970
and they started
celebrating together.

00:32:28.970 --> 00:32:30.530
They started
exchanging trinkets.

00:32:30.530 --> 00:32:33.170
They started talking about their
families, showing pictures,

00:32:33.170 --> 00:32:33.980
celebrating.

00:32:33.980 --> 00:32:37.500
Now, these were
men who hours ago

00:32:37.500 --> 00:32:38.830
were trying to kill each other.

00:32:38.830 --> 00:32:41.810
And no one would have ever
trusted if I walked out,

00:32:41.810 --> 00:32:43.227
was an open shot,
I couldn't trust

00:32:43.227 --> 00:32:44.684
that you weren't
going to shoot me.

00:32:44.684 --> 00:32:46.420
They always had shot
each other before,

00:32:46.420 --> 00:32:48.550
but here they were
celebrating with each other

00:32:48.550 --> 00:32:51.934
in a very communal way, by
their own words, very amazing.

00:32:51.934 --> 00:32:53.350
Here we were
laughing and chatting

00:32:53.350 --> 00:32:56.620
to men who only a few hours
before we were trying to kill.

00:32:56.620 --> 00:32:59.800
Now, if that's not a big change
in how trustworthy somebody can

00:32:59.800 --> 00:33:02.870
be, I don't know what is.

00:33:02.870 --> 00:33:04.870
So the question is, how
do we display such trust

00:33:04.870 --> 00:33:08.084
and compassion in one moment
and such cruelty the next?

00:33:08.084 --> 00:33:09.500
Because if we can
understand that,

00:33:09.500 --> 00:33:11.130
then we can do
something about it.

00:33:11.130 --> 00:33:14.430
But to answer that question,
what we have to realize first

00:33:14.430 --> 00:33:16.540
is how do we address
a different one?

00:33:16.540 --> 00:33:19.330
How do we identify
who is worthy to help?

00:33:19.330 --> 00:33:22.410
The world is full of more people
than we could possibly help.

00:33:22.410 --> 00:33:23.910
Not that we don't
want to help them,

00:33:23.910 --> 00:33:25.860
but it could be overwhelming.

00:33:25.860 --> 00:33:28.810
And there's this phenomenon that
we know of in psychology called

00:33:28.810 --> 00:33:30.935
compassion fatigue, which
is when you're confronted

00:33:30.935 --> 00:33:33.240
with people over and over
and over again who need help,

00:33:33.240 --> 00:33:35.490
you begin to dial it down.

00:33:35.490 --> 00:33:36.950
I have this experience
that I'm not

00:33:36.950 --> 00:33:39.561
proud of when I go with
my daughter to New York,

00:33:39.561 --> 00:33:41.810
and we're walking by, and
there was a homeless person.

00:33:41.810 --> 00:33:43.860
She was like, daddy,
help this person.

00:33:43.860 --> 00:33:45.990
And then I realized
that in that moment,

00:33:45.990 --> 00:33:47.860
I'm completely
ignoring this person,

00:33:47.860 --> 00:33:50.340
because it's a common thing
that I face all the time.

00:33:50.340 --> 00:33:52.256
And if I stopped to try
and help every person,

00:33:52.256 --> 00:33:53.390
it would be overwhelming.

00:33:53.390 --> 00:33:55.720
And so we have to
understand how our mind goes

00:33:55.720 --> 00:33:58.880
about deciding whose
pain is worthy to feel,

00:33:58.880 --> 00:34:01.200
who it's worth to help,
and who it's worth

00:34:01.200 --> 00:34:02.810
be trustworthy toward.

00:34:02.810 --> 00:34:05.180
And once we understand that,
then we can figure out,

00:34:05.180 --> 00:34:07.910
OK, how do we increase
the number of people

00:34:07.910 --> 00:34:09.464
to whom we should feel that?

00:34:09.464 --> 00:34:12.139
Well, one way that I think
our mind does it is it

00:34:12.139 --> 00:34:13.320
uses a simple metric.

00:34:13.320 --> 00:34:16.670
And that metric is similarity.

00:34:16.670 --> 00:34:20.030
So it comes back to this
is Robert Trivers, who

00:34:20.030 --> 00:34:22.830
was the discoverer of reciprocal
altruism, which is basically

00:34:22.830 --> 00:34:25.163
the idea, why do we help
people in the biological sense?

00:34:25.163 --> 00:34:27.062
It's I scratch your back today.

00:34:27.062 --> 00:34:28.270
You'll scratch mine tomorrow.

00:34:28.270 --> 00:34:30.639
In some ways, that's
what similarity is.

00:34:30.639 --> 00:34:34.239
When there's a lot of people
who need my help, going back

00:34:34.239 --> 00:34:39.040
to that equation of short-term
versus long-term gain,

00:34:39.040 --> 00:34:39.969
who should I help?

00:34:39.969 --> 00:34:40.880
Who is it worth
it for me to help?

00:34:40.880 --> 00:34:42.820
What your mind does
shaped by evolution is it

00:34:42.820 --> 00:34:46.186
decides the person who is more
similar to me is the person

00:34:46.186 --> 00:34:48.310
that it's worth helping,
because that's more likely

00:34:48.310 --> 00:34:51.290
the person who's going to pay
me back and be around later.

00:34:51.290 --> 00:34:53.910
At least initially
that's how it works.

00:34:53.910 --> 00:34:55.590
And so what we wanted
to do was to see

00:34:55.590 --> 00:34:58.230
how deeply embedded
this bias is.

00:34:58.230 --> 00:35:01.590
If I said to you, if an American
soldier's on the battlefield

00:35:01.590 --> 00:35:03.920
and he comes across an
American soldier and a member

00:35:03.920 --> 00:35:07.240
of the Taliban and both of them
are suffering the same wounds,

00:35:07.240 --> 00:35:09.400
who is he or she going to
feel more compassion for?

00:35:09.400 --> 00:35:10.230
And if I said the
American soldier,

00:35:10.230 --> 00:35:12.070
you might not find
that surprising.

00:35:12.070 --> 00:35:13.778
But what I want to
argue is that it's not

00:35:13.778 --> 00:35:15.490
dependent on
longstanding conflict.

00:35:15.490 --> 00:35:17.070
It's this unconscious
computation

00:35:17.070 --> 00:35:18.000
that your mind makes.

00:35:18.000 --> 00:35:21.980
And so we tried to strip
that down to as basic a level

00:35:21.980 --> 00:35:22.500
as we could.

00:35:22.500 --> 00:35:23.916
And we did that
by using something

00:35:23.916 --> 00:35:26.390
called motor synchrony, which
is people basically moving

00:35:26.390 --> 00:35:27.530
together in time.

00:35:27.530 --> 00:35:29.760
You see it in the military.

00:35:29.760 --> 00:35:31.710
You see it in conga lines.

00:35:31.710 --> 00:35:33.570
You see it lots of places.

00:35:33.570 --> 00:35:35.080
You see it in lots of rituals.

00:35:35.080 --> 00:35:39.060
And the idea is that if two
people are moving together,

00:35:39.060 --> 00:35:42.940
that's a marker that for
here and now, their outcomes,

00:35:42.940 --> 00:35:45.437
their purposes, their
goals are joined.

00:35:45.437 --> 00:35:47.270
And so we wanted to see
if we could actually

00:35:47.270 --> 00:35:49.222
show this effect at that level.

00:35:49.222 --> 00:35:50.930
So we brought people
into a lab, and they

00:35:50.930 --> 00:35:52.822
thought it was a music
perception study.

00:35:52.822 --> 00:35:54.280
So they sat across
from each other.

00:35:54.280 --> 00:35:55.807
And there was
sensors on the table,

00:35:55.807 --> 00:35:56.890
and they had earphones on.

00:35:56.890 --> 00:35:58.120
They didn't talk.

00:35:58.120 --> 00:36:00.330
All they had to
do was as you hear

00:36:00.330 --> 00:36:03.810
the tones in your
earphone, tap the sensor.

00:36:03.810 --> 00:36:06.310
And so it was constructed
so that the two people would

00:36:06.310 --> 00:36:08.870
either be tapping in
time or completely

00:36:08.870 --> 00:36:10.390
randomly and out of time.

00:36:10.390 --> 00:36:12.190
That was it.

00:36:12.190 --> 00:36:14.432
Then what happens is
they see the person

00:36:14.432 --> 00:36:15.890
they were tapping
with get cheated.

00:36:15.890 --> 00:36:17.450
This party is staged,
but they don't know it.

00:36:17.450 --> 00:36:18.408
They believe it's real.

00:36:18.408 --> 00:36:20.890
They see this person get
cheated in a way that

00:36:20.890 --> 00:36:24.175
makes that person have
to do a lot of extra work

00:36:24.175 --> 00:36:26.609
that they shouldn't
have had to do.

00:36:26.609 --> 00:36:28.150
And then what happens
is we give them

00:36:28.150 --> 00:36:31.480
a chance to decide if they
want to go and help that person

00:36:31.480 --> 00:36:34.610
and relieve that
person's burden.

00:36:34.610 --> 00:36:37.130
And that's what we look at.

00:36:37.130 --> 00:36:38.990
So what happens?

00:36:38.990 --> 00:36:41.890
We asked the people,
how similar were you

00:36:41.890 --> 00:36:43.820
to that person in
the experiment?

00:36:43.820 --> 00:36:46.750
The simple act of tapping
your hands-- they didn't talk.

00:36:46.750 --> 00:36:48.492
They didn't do
anything-- made them

00:36:48.492 --> 00:36:50.700
feel that they were more
similar to the other person.

00:36:50.700 --> 00:36:53.790
Now, if you ask them why,
they'll create a story.

00:36:53.790 --> 00:36:57.170
They'll say, oh, I think
we were in the same class.

00:36:57.170 --> 00:37:00.050
Or I think I've met
this person somewhere

00:37:00.050 --> 00:37:02.250
or we share the same goals.

00:37:02.250 --> 00:37:03.354
They don't know.

00:37:03.354 --> 00:37:05.020
They never talked to
this person before.

00:37:05.020 --> 00:37:05.936
None of that was true.

00:37:05.936 --> 00:37:07.510
But because they
had this intuition

00:37:07.510 --> 00:37:08.885
that they felt
more similar, they

00:37:08.885 --> 00:37:10.770
had to create a story for it.

00:37:10.770 --> 00:37:13.461
How much compassion did
you feel for this person

00:37:13.461 --> 00:37:15.710
when they got cheated and
got stuck doing this onerous

00:37:15.710 --> 00:37:18.480
work that they weren't
supposed to do?

00:37:18.480 --> 00:37:20.750
Remember, in both cases,
the amount of suffering

00:37:20.750 --> 00:37:22.320
is exactly the same.

00:37:22.320 --> 00:37:25.680
Yet they feel more
compassion for this person

00:37:25.680 --> 00:37:29.530
if they were just tapping
in time with them.

00:37:29.530 --> 00:37:31.460
How many wanted to
go help this person?

00:37:31.460 --> 00:37:32.950
This I found truly amazing.

00:37:32.950 --> 00:37:35.490
6 out of 34 people
would say, oh,

00:37:35.490 --> 00:37:37.240
I'll go help that
person who was harmed

00:37:37.240 --> 00:37:42.390
and cheated versus 17 of 35.

00:37:42.390 --> 00:37:44.524
We had a threefold difference.

00:37:44.524 --> 00:37:46.690
When you tapped your hands
in time with this person,

00:37:46.690 --> 00:37:48.960
50% of them said,
I want to go help

00:37:48.960 --> 00:37:51.200
this person who was wronged.

00:37:51.200 --> 00:37:53.780
That's a huge effect
if it's scalable.

00:37:53.780 --> 00:37:57.750
How much time did
they spend helping?

00:37:57.750 --> 00:37:58.600
These are seconds.

00:37:58.600 --> 00:38:01.660
So if you tapped your
hands with this person,

00:38:01.660 --> 00:38:05.000
you spent a lot more time
knowing that everything you did

00:38:05.000 --> 00:38:07.840
would relieve that
person's burden.

00:38:07.840 --> 00:38:09.520
And if you look at
it-- again these

00:38:09.520 --> 00:38:12.710
are regression coefficients--
if you tapped your hand in time

00:38:12.710 --> 00:38:15.980
with this person, yes, you
felt more similar to them.

00:38:15.980 --> 00:38:17.730
And yes you like them more.

00:38:17.730 --> 00:38:20.690
But what actually predicted
the compassion you feel?

00:38:20.690 --> 00:38:24.992
Not how much you like them but
how similar you felt to them

00:38:24.992 --> 00:38:26.950
If you tapped with them,
you felt more similar.

00:38:26.950 --> 00:38:28.550
That predicted how
much compassion

00:38:28.550 --> 00:38:30.841
you felt toward them even
though the level of suffering

00:38:30.841 --> 00:38:33.040
was the same objectively.

00:38:33.040 --> 00:38:35.390
And the amount of compassion
you felt for them directly

00:38:35.390 --> 00:38:38.340
predicted how much time,
how much effort you

00:38:38.340 --> 00:38:40.660
put into relieving their pain.

00:38:40.660 --> 00:38:44.150
Now, what this suggests is that
compassion and trustworthiness

00:38:44.150 --> 00:38:44.710
are flexible.

00:38:44.710 --> 00:38:46.360
Because if you're going
to be trustworthy to me,

00:38:46.360 --> 00:38:47.859
that means you're
going to sacrifice

00:38:47.859 --> 00:38:52.650
your own immediate outcomes to
benefit me like these people

00:38:52.650 --> 00:38:53.280
did here.

00:38:53.280 --> 00:38:54.690
Can I trust you to help me?

00:38:54.690 --> 00:38:58.020
Can trust you not
to shoot me back

00:38:58.020 --> 00:38:59.480
with the Brits and the Germans.

00:39:02.500 --> 00:39:05.180
Where I live, what
this means is trying

00:39:05.180 --> 00:39:08.070
to solve some of the
more contentious things

00:39:08.070 --> 00:39:10.240
we have in Boston, which
is Yankees versus Red Sox.

00:39:10.240 --> 00:39:11.929
But what that means
basically is not

00:39:11.929 --> 00:39:13.720
thinking about your
new neighbor as the guy

00:39:13.720 --> 00:39:15.450
who hates the dreaded Yankees.

00:39:15.450 --> 00:39:17.560
Think about him as the
guy who likes Starbucks

00:39:17.560 --> 00:39:19.090
as much as you do.

00:39:19.090 --> 00:39:23.280
If you can actually retrain
your mind to find similarities

00:39:23.280 --> 00:39:24.710
that you have with
people, it will

00:39:24.710 --> 00:39:28.250
increase your trustworthiness
toward them and the compassion

00:39:28.250 --> 00:39:31.340
that you feel toward them.

00:39:31.340 --> 00:39:34.190
When you think
about social media,

00:39:34.190 --> 00:39:36.590
there are tremendous
ways to do this.

00:39:36.590 --> 00:39:39.152
We can use the computational
power of social media

00:39:39.152 --> 00:39:40.860
in ways to connect
people that have never

00:39:40.860 --> 00:39:41.890
been connected before.

00:39:41.890 --> 00:39:46.940
Think about things like profiles
on Facebook or other things.

00:39:46.940 --> 00:39:50.940
We have vast knowledge of what
people like and don't like,

00:39:50.940 --> 00:39:54.000
what they've done
or haven't done.

00:39:54.000 --> 00:39:56.920
Perhaps what you can do is
find what people in conflict

00:39:56.920 --> 00:40:01.220
have in common very
rapidly in the background

00:40:01.220 --> 00:40:03.627
and surface that
information to them.

00:40:03.627 --> 00:40:05.210
And if you do, then
it should function

00:40:05.210 --> 00:40:06.780
in just the same way
as tapping your hands.

00:40:06.780 --> 00:40:08.260
There's nothing magic
about tapping your hands.

00:40:08.260 --> 00:40:10.468
We've done it with wearing
the same wristband colors,

00:40:10.468 --> 00:40:11.070
et cetera.

00:40:11.070 --> 00:40:13.420
Anything that you can do
to highlight similarity

00:40:13.420 --> 00:40:15.650
with someone will
make your goals

00:40:15.650 --> 00:40:18.364
seems more joined, which will
increase the compassion you

00:40:18.364 --> 00:40:19.780
feel to them if
they're suffering,

00:40:19.780 --> 00:40:21.480
which will increase
how trustworthy you

00:40:21.480 --> 00:40:26.440
are toward them even in ways
that don't involve compassion.

00:40:26.440 --> 00:40:29.185
So at Google, I'm really
interested and open

00:40:29.185 --> 00:40:30.810
to talking with any
of you about if you

00:40:30.810 --> 00:40:32.780
have ideas about using
the computational power

00:40:32.780 --> 00:40:36.360
that you all have to kind
of nudge trustworthiness

00:40:36.360 --> 00:40:39.432
and compassion in the world.

00:40:39.432 --> 00:40:41.762
But that's all kind
of a top-down way.

00:40:41.762 --> 00:40:43.720
This is a way that we
have to remind ourselves,

00:40:43.720 --> 00:40:46.899
OK, think about this person
as similar to me or not.

00:40:46.899 --> 00:40:49.440
It will be nice if we had a way
that could make it automatic,

00:40:49.440 --> 00:40:51.606
a way that works from the
bottom up so that we don't

00:40:51.606 --> 00:40:54.546
have to stop and
remind ourselves.

00:40:54.546 --> 00:40:55.920
And one way that
we can do this--

00:40:55.920 --> 00:40:58.770
and I know it's an idea
close to Ming's heart,

00:40:58.770 --> 00:41:00.290
and I'm really
honored to be here

00:41:00.290 --> 00:41:05.630
to be able to talk with him
about it-- is mindfulness.

00:41:05.630 --> 00:41:07.670
If you read the paper
or know anything

00:41:07.670 --> 00:41:09.150
about mindfulness,
what you'll know

00:41:09.150 --> 00:41:12.780
is that it is enjoying
a renaissance.

00:41:12.780 --> 00:41:15.196
And we know it does all
kinds of wonderful things.

00:41:15.196 --> 00:41:17.070
And probably many of
you have more experience

00:41:17.070 --> 00:41:21.260
with it here thanks
to Ming's course.

00:41:21.260 --> 00:41:23.100
It will increase
your creativity.

00:41:23.100 --> 00:41:25.137
It will increase
your productivity.

00:41:25.137 --> 00:41:26.220
It's good for your health.

00:41:26.220 --> 00:41:28.370
It'll lower your blood pressure.

00:41:28.370 --> 00:41:31.360
It'll even increase your
scores on standardized tests.

00:41:31.360 --> 00:41:33.060
These are all good things.

00:41:33.060 --> 00:41:34.520
But if you think
about it, it's not

00:41:34.520 --> 00:41:37.360
what it was originally
designed for.

00:41:37.360 --> 00:41:39.116
If you look at what
Buddha said or many

00:41:39.116 --> 00:41:40.820
of the other ancient
meditation teachers-- well

00:41:40.820 --> 00:41:41.920
this is a quote by Buddha.

00:41:41.920 --> 00:41:44.380
I teach one thing and one only.

00:41:44.380 --> 00:41:47.720
That is suffering and
the end of suffering.

00:41:47.720 --> 00:41:50.440
There weren't LSATs
and GMATs back then.

00:41:50.440 --> 00:41:54.820
So all these other things that
meditation does are wonderful,

00:41:54.820 --> 00:41:55.570
and they're great.

00:41:55.570 --> 00:41:58.780
But one of the main purposes
was to foster compassion and end

00:41:58.780 --> 00:42:01.070
suffering and to increase
our being good to each other

00:42:01.070 --> 00:42:02.635
and being trustworthy
to each other.

00:42:02.635 --> 00:42:04.343
And so what we decided
to do was actually

00:42:04.343 --> 00:42:06.080
put that idea to a test.

00:42:06.080 --> 00:42:08.429
And so we brought
people into the lab.

00:42:08.429 --> 00:42:10.470
These were people who had
never meditated before.

00:42:10.470 --> 00:42:12.240
They were members of
the Boston community.

00:42:12.240 --> 00:42:16.011
And they were all equally
interested in meditation,

00:42:16.011 --> 00:42:17.260
doing a class for eight weeks.

00:42:17.260 --> 00:42:19.430
We assigned half of
them to actually take

00:42:19.430 --> 00:42:23.550
a mindfulness class
led by a Buddhist lama.

00:42:23.550 --> 00:42:26.090
And they also would go
home during the week

00:42:26.090 --> 00:42:29.766
with MP3s created by the lama
that they would practice.

00:42:29.766 --> 00:42:31.390
The other half were
put on a wait list.

00:42:31.390 --> 00:42:33.180
So this way we had groups
that were equally interested

00:42:33.180 --> 00:42:35.650
in medication, because you
might imagine that if we just

00:42:35.650 --> 00:42:37.400
recruited people who
wanted to meditate,

00:42:37.400 --> 00:42:39.020
they might have been
different types of people

00:42:39.020 --> 00:42:39.811
in the first place.

00:42:39.811 --> 00:42:42.250
So both groups were equally
interested, but only half

00:42:42.250 --> 00:42:43.630
of them actually got the course.

00:42:43.630 --> 00:42:46.605
The other half got it
after we did the measure.

00:42:46.605 --> 00:42:48.730
After eight weeks, we
brought them back to the lab.

00:42:48.730 --> 00:42:49.570
Now, they thought
they were coming

00:42:49.570 --> 00:42:51.653
to have their memory, and
their executive control,

00:42:51.653 --> 00:42:54.220
and all these cognitive
measures tested, which we did.

00:42:54.220 --> 00:42:56.830
But before we did those, what
we really were interested in

00:42:56.830 --> 00:42:58.980
is what was going to
happen in the waiting room.

00:42:58.980 --> 00:43:02.430
And so in our waiting
room, we had three chairs.

00:43:02.430 --> 00:43:05.565
Two were filled by actors and
the third was for the subject.

00:43:05.565 --> 00:43:07.940
And so when the subject arrived,
what did the subject do?

00:43:07.940 --> 00:43:09.360
Well, all them
except one sat down.

00:43:09.360 --> 00:43:11.776
We couldn't get that other guy
to sit down no matter what.

00:43:11.776 --> 00:43:13.750
But most of them sat down.

00:43:13.750 --> 00:43:17.660
And then a third actor
would enter the room.

00:43:17.660 --> 00:43:20.160
This person was on crutches,
had one of those foot

00:43:20.160 --> 00:43:21.882
boots you wear when
your ankle is broken.

00:43:21.882 --> 00:43:23.965
And as she'd walk down the
hall entering the room,

00:43:23.965 --> 00:43:25.715
she would kind of wince
in pain and looked

00:43:25.715 --> 00:43:27.640
noticeably uncomfortable.

00:43:27.640 --> 00:43:30.350
And she'd enter the room,
and there weren't any chairs.

00:43:30.350 --> 00:43:31.809
And so she'd lean
against the wall.

00:43:31.809 --> 00:43:33.766
And the question was,
what would the person do?

00:43:33.766 --> 00:43:36.440
The actors were told to busy
themselves in their iPhone

00:43:36.440 --> 00:43:37.770
and to not pay attention.

00:43:37.770 --> 00:43:40.080
Now, in psychology we call
this a bystander effect

00:43:40.080 --> 00:43:41.710
where this really
limits helping.

00:43:41.710 --> 00:43:43.450
If you're in a
situation where you

00:43:43.450 --> 00:43:45.940
see somebody in pain and
other people aren't helping,

00:43:45.940 --> 00:43:48.015
that tends to decrease
anybody's odds of helping,

00:43:48.015 --> 00:43:49.090
because you say, oh,
it's not a big deal

00:43:49.090 --> 00:43:50.350
or maybe I shouldn't help.

00:43:50.350 --> 00:43:52.240
And so this
situation is one that

00:43:52.240 --> 00:43:55.620
makes being counted on that
you're trustworthy to come

00:43:55.620 --> 00:43:59.200
and help the lowest
it possibly can.

00:43:59.200 --> 00:44:00.090
So what happens?

00:44:02.660 --> 00:44:09.820
So people who were in the
control group, only very

00:44:09.820 --> 00:44:14.480
small percentage of
them helped, like 16%.

00:44:14.480 --> 00:44:17.890
Among those who meditated,
50% of them helped.

00:44:17.890 --> 00:44:19.542
That's a threefold increase.

00:44:19.542 --> 00:44:21.750
And that's a threefold
increase in the situation that

00:44:21.750 --> 00:44:25.900
is designed to
actually work the most

00:44:25.900 --> 00:44:28.612
against your willing to help.

00:44:28.612 --> 00:44:30.320
Now, if that can happen
after eight weeks

00:44:30.320 --> 00:44:35.752
and if that is scalable,
that is a huge, huge effect

00:44:35.752 --> 00:44:37.210
that you can count
on other people.

00:44:37.210 --> 00:44:40.260
You can trust them that
they're going to help you.

00:44:40.260 --> 00:44:44.660
Now, why does it work that way?

00:44:44.660 --> 00:44:47.930
It works that way because
one part of mindfulness

00:44:47.930 --> 00:44:50.020
is this idea of equanimity.

00:44:50.020 --> 00:44:52.507
And that means realizing
that I am similar to you,

00:44:52.507 --> 00:44:53.590
and you are similar to me.

00:44:53.590 --> 00:44:56.480
Friends are enemies, are
enemies can become friends.

00:44:56.480 --> 00:44:58.210
And what that does
is it trains the mind

00:44:58.210 --> 00:45:00.590
to see us all as
valuable and interlinked.

00:45:00.590 --> 00:45:02.520
And it breaks down
the categories

00:45:02.520 --> 00:45:04.924
that we put on each other of
we're different in religion.

00:45:04.924 --> 00:45:07.090
We're different in sports
teams you like, et cetera.

00:45:07.090 --> 00:45:08.040
And I think that's why it works.

00:45:08.040 --> 00:45:09.289
And then it becomes automatic.

00:45:09.289 --> 00:45:11.960
It does the same thing that
my little tapping example

00:45:11.960 --> 00:45:13.604
was doing.

00:45:13.604 --> 00:45:16.020
And so when it comes down to
it, really what I want to say

00:45:16.020 --> 00:45:19.280
is that in the end,
it's trust or it's dust.

00:45:19.280 --> 00:45:22.790
And what I mean by
that is without trust,

00:45:22.790 --> 00:45:27.540
our ability to be resilient as
a society is exceedingly low.

00:45:27.540 --> 00:45:30.090
And so how can we build it up?

00:45:30.090 --> 00:45:32.000
Anything we can
do to nudge it up

00:45:32.000 --> 00:45:36.464
is important to being resilient.

00:45:36.464 --> 00:45:38.880
In the fall of 2012, I don't
know how many of you remember

00:45:38.880 --> 00:45:42.380
out here, but on the East Coast,
super storm Sandy hit New York.

00:45:42.380 --> 00:45:45.370
And it was a devastating storm.

00:45:45.370 --> 00:45:48.210
And there are neighborhoods
that still aren't recovered.

00:45:48.210 --> 00:45:51.450
But the AP did a great study.

00:45:51.450 --> 00:45:53.800
Controlling for the amount
of damage that occurred,

00:45:53.800 --> 00:45:57.320
they looked at what was the
single most important predictor

00:45:57.320 --> 00:46:00.660
of a neighborhood's resilience.

00:46:00.660 --> 00:46:02.170
The single most
important predictor

00:46:02.170 --> 00:46:05.220
was how much neighbors
trusted each other.

00:46:05.220 --> 00:46:07.340
How much they know
that the other person

00:46:07.340 --> 00:46:08.710
they could count on them,
that that person was

00:46:08.710 --> 00:46:10.126
going to have
compassion for them,

00:46:10.126 --> 00:46:12.040
that they were going
to work together.

00:46:12.040 --> 00:46:14.210
The neighborhoods that
were higher in trust

00:46:14.210 --> 00:46:16.150
were the neighborhoods
that got up and running

00:46:16.150 --> 00:46:18.120
in terms of commerce,
and support,

00:46:18.120 --> 00:46:20.241
and social services
faster than anything.

00:46:20.241 --> 00:46:22.740
And that's why I say in the
end, it really is trust or dust.

00:46:22.740 --> 00:46:24.984
If we don't trust,
we're harming everybody.

00:46:24.984 --> 00:46:27.400
But of course there were people
in neighborhoods who price

00:46:27.400 --> 00:46:29.880
gouged and who did
things they shouldn't.

00:46:29.880 --> 00:46:34.090
And so really my message in the
book is, yes, trusting is good.

00:46:34.090 --> 00:46:35.770
We should all trust.

00:46:35.770 --> 00:46:38.520
But trusting wisely is better.

00:46:38.520 --> 00:46:41.950
And so it's my hope that any
of you who read this or come

00:46:41.950 --> 00:46:45.870
into contact with this work,
it will empower you to think

00:46:45.870 --> 00:46:48.100
about the way trust
actually works

00:46:48.100 --> 00:46:49.731
and the forces
that impinge on it

00:46:49.731 --> 00:46:51.730
to make better decisions
about who you can trust

00:46:51.730 --> 00:46:56.330
but also how to foster
trustworthiness in yourself.

00:46:56.330 --> 00:46:58.355
And I thank you so much
for listening to me.

00:47:03.075 --> 00:47:04.200
MING: Thank you, my friend.

00:47:04.200 --> 00:47:06.236
We have time for questions.

00:47:06.236 --> 00:47:07.360
Anybody have any questions?

00:47:07.360 --> 00:47:10.260
AUDIENCE: Two fairly
related questions,

00:47:10.260 --> 00:47:13.890
in the earlier test
about looking for cues,

00:47:13.890 --> 00:47:16.450
so I'm just wondering
how you came

00:47:16.450 --> 00:47:18.789
to the domain of
different things

00:47:18.789 --> 00:47:20.580
you were looking for
that could conceivably

00:47:20.580 --> 00:47:23.010
be a cue in your analysis.

00:47:23.010 --> 00:47:25.520
Because you could say whether
the pinkie is touching

00:47:25.520 --> 00:47:28.350
the hand and the hand's
touching is or not.

00:47:28.350 --> 00:47:31.590
And then along with
that, with whether you

00:47:31.590 --> 00:47:35.910
were going into it starting
off focused on physical cues

00:47:35.910 --> 00:47:39.930
or if you were also
considering the types of issues

00:47:39.930 --> 00:47:43.600
that were brought up in
conversation, which could then

00:47:43.600 --> 00:47:46.100
play into the
talking with a robot

00:47:46.100 --> 00:47:49.099
or talking over the internet.

00:47:49.099 --> 00:47:51.140
DAVID DESTENO: So let me
do the second one first.

00:47:51.140 --> 00:47:54.397
We were primarily
interested in physical cues.

00:47:54.397 --> 00:47:55.980
There's lots of work
out there as well

00:47:55.980 --> 00:47:58.510
on linguistics and
the type of phrasing

00:47:58.510 --> 00:48:01.440
that people use as
well as vocal tone.

00:48:01.440 --> 00:48:02.920
We weren't looking for those.

00:48:02.920 --> 00:48:04.720
It doesn't mean that
those don't matter.

00:48:04.720 --> 00:48:06.390
And so I'm not saying these
are the only cues that matter,

00:48:06.390 --> 00:48:07.890
but these are
sufficient to predict.

00:48:07.890 --> 00:48:11.140
The more that we know
about, our accuracy will go.

00:48:11.140 --> 00:48:14.090
But we were interested in the
actual physical, biological

00:48:14.090 --> 00:48:15.520
motion cues.

00:48:15.520 --> 00:48:16.500
How did we get them?

00:48:16.500 --> 00:48:19.710
We simply started with
the brute force method,

00:48:19.710 --> 00:48:21.430
was looking for
individual cues that

00:48:21.430 --> 00:48:23.460
had some predictive
ability at all.

00:48:23.460 --> 00:48:26.562
Because even if they're
not predictive greatly

00:48:26.562 --> 00:48:28.770
on their own, they have to
have some predictive power

00:48:28.770 --> 00:48:29.360
on their own.

00:48:29.360 --> 00:48:31.960
And then we would begin
to do is to assemble

00:48:31.960 --> 00:48:34.695
different subsets just trying to
maximize the amount of accuracy

00:48:34.695 --> 00:48:35.620
that we can predict.

00:48:35.620 --> 00:48:37.340
So it was a very
bottom-up approach.

00:48:37.340 --> 00:48:38.923
And then once we
have those four sets,

00:48:38.923 --> 00:48:41.930
those predicted the
greatest amount of variance

00:48:41.930 --> 00:48:44.335
in people's selfish
monetary behavior.

00:48:44.335 --> 00:48:45.960
Which is then why,
again, it was really

00:48:45.960 --> 00:48:46.890
important to use the robot.

00:48:46.890 --> 00:48:47.490
Because you're right.

00:48:47.490 --> 00:48:48.580
This was a correlational method.

00:48:48.580 --> 00:48:49.540
Who knows what we
could be picking up.

00:48:49.540 --> 00:48:52.140
Maybe on every time I cross
my arm, it was my pinkie.

00:48:52.140 --> 00:48:54.348
And so we could actually
manipulate it with precision

00:48:54.348 --> 00:48:55.951
with the robot to validate it.

00:48:55.951 --> 00:48:57.700
MING: If you could put
a pinkie down here,

00:48:57.700 --> 00:48:58.575
it's not trustworthy.

00:49:02.060 --> 00:49:03.519
That's what I
learned from a movie.

00:49:03.519 --> 00:49:05.726
AUDIENCE: Let's see if I
can word this the right way.

00:49:05.726 --> 00:49:07.380
It seems like the
general conclusion

00:49:07.380 --> 00:49:09.750
from the research
or your conclusion

00:49:09.750 --> 00:49:13.030
is you're saying we should
be more trusting of others,

00:49:13.030 --> 00:49:15.290
like sort of the hope
for the better world.

00:49:15.290 --> 00:49:19.530
And the question is, is there
also then some drive for people

00:49:19.530 --> 00:49:21.705
themselves to be trustworthy?

00:49:21.705 --> 00:49:23.580
Like in the example you
gave in the beginning

00:49:23.580 --> 00:49:27.480
about people in a marriage,
one cheated on the other,

00:49:27.480 --> 00:49:29.010
is it to say, put that aside.

00:49:29.010 --> 00:49:30.540
Trust that person.

00:49:30.540 --> 00:49:32.680
Or is there some other
conclusion in that sense?

00:49:32.680 --> 00:49:34.180
DAVID DESTENO: It's
a good question.

00:49:34.180 --> 00:49:35.750
What we know from
all the-- so people

00:49:35.750 --> 00:49:38.490
like Martin Nowak at Harvard
is a straight mathematician,

00:49:38.490 --> 00:49:39.470
evolutionary biologist.

00:49:39.470 --> 00:49:41.095
And so they run these
fantastic models.

00:49:41.095 --> 00:49:42.750
And what we know
is that if you are

00:49:42.750 --> 00:49:46.360
untrustworthy in the short
run, you will profit immensely.

00:49:46.360 --> 00:49:48.610
But over time, that profit
then starts to go down.

00:49:48.610 --> 00:49:50.760
And so in the long run,
people who are trustworthy

00:49:50.760 --> 00:49:54.400
profit the most in terms of
everything and even as a group.

00:49:54.400 --> 00:49:56.330
And so we know that's
the better outcome.

00:49:56.330 --> 00:50:00.060
But if you can be untrustworthy
and not get caught,

00:50:00.060 --> 00:50:01.480
you're going to profit.

00:50:01.480 --> 00:50:03.070
So how do we try
to balance those?

00:50:03.070 --> 00:50:04.450
And so what we're
trying to do is

00:50:04.450 --> 00:50:05.825
to make everybody
want to be more

00:50:05.825 --> 00:50:08.324
trustworthy but at the same
time also make better decisions.

00:50:08.324 --> 00:50:10.250
It will be impossible
to have a world where

00:50:10.250 --> 00:50:12.202
everybody is trustworthy.

00:50:12.202 --> 00:50:13.660
Because if everybody's
trustworthy,

00:50:13.660 --> 00:50:15.034
you stop even
looking and caring.

00:50:15.034 --> 00:50:17.070
You just automatically
say, yes, I'll trust you.

00:50:17.070 --> 00:50:19.940
And then if there's a mutation
or whatever that causes people

00:50:19.940 --> 00:50:22.020
to be more
untrustworthy, they're

00:50:22.020 --> 00:50:24.042
going to profit like crazy.

00:50:24.042 --> 00:50:25.500
Until then everybody
starts caring,

00:50:25.500 --> 00:50:27.541
and so it's always going
to be in an equilibrium.

00:50:27.541 --> 00:50:29.570
The question is, can we
increase the set point

00:50:29.570 --> 00:50:32.090
for trustworthiness
to a higher level?

00:50:32.090 --> 00:50:34.310
And so it's about increasing
your own trustworthiness

00:50:34.310 --> 00:50:37.350
but about deciding if you can
trust somebody else wisely.

00:50:37.350 --> 00:50:39.720
So yes, if you know
absolutely nothing,

00:50:39.720 --> 00:50:42.700
it's better to trust than
not trust in the long run

00:50:42.700 --> 00:50:45.430
in terms of quantifying the
benefits that can happen.

00:50:45.430 --> 00:50:47.220
But it's certainly
not as good as making

00:50:47.220 --> 00:50:48.940
an informed correct decision.

00:50:48.940 --> 00:50:51.110
And so my hope is to try
and open people's eyes

00:50:51.110 --> 00:50:52.651
to how trust really
works so that you

00:50:52.651 --> 00:50:54.570
can make better decisions.

00:50:54.570 --> 00:50:58.490
AUDIENCE: So similar to that
question, trust over time,

00:50:58.490 --> 00:51:03.010
have you done any research
into how analysis of trust

00:51:03.010 --> 00:51:04.730
has to change, how
much it needs to be

00:51:04.730 --> 00:51:07.440
dependent on data changing?

00:51:07.440 --> 00:51:11.540
Like the first study was on
the initial get to know you,

00:51:11.540 --> 00:51:13.280
how much I trust
you as a person.

00:51:13.280 --> 00:51:18.430
And then the question is
later on, something happens.

00:51:18.430 --> 00:51:21.920
How much should future events
be added into that evaluation?

00:51:21.920 --> 00:51:24.530
DAVID DESTENO: You mean
at what point will I

00:51:24.530 --> 00:51:27.190
change my judgment of
whether I can trust you?

00:51:27.190 --> 00:51:28.800
It's dependent on
a lot of things.

00:51:28.800 --> 00:51:32.970
It's often dependent on the
magnitude of what you have held

00:51:32.970 --> 00:51:35.010
up your end for
what you haven't.

00:51:35.010 --> 00:51:37.790
But I guess my
argument is that you

00:51:37.790 --> 00:51:41.770
need to look at each situation
if it's important and new.

00:51:41.770 --> 00:51:44.020
Because even somebody who
has been always trustworthy,

00:51:44.020 --> 00:51:45.709
if the costs and
benefits change--

00:51:45.709 --> 00:51:47.750
the reason they're always
trustworthy is the cost

00:51:47.750 --> 00:51:49.200
and benefits are rather stable.

00:51:49.200 --> 00:51:51.241
Take that person and change
the cost and benefits

00:51:51.241 --> 00:51:55.030
either by dangling a reward
that is immense in front of them

00:51:55.030 --> 00:51:57.190
or giving them anonymity
so they won't get caught,

00:51:57.190 --> 00:51:58.690
like our people,
and they'll change.

00:51:58.690 --> 00:52:02.840
So I think there's not
a clear time frame.

00:52:02.840 --> 00:52:04.810
I think we all adjust
at different rates

00:52:04.810 --> 00:52:06.150
depending upon the magnitude.

00:52:06.150 --> 00:52:09.600
But my message is no matter
what you think, consider

00:52:09.600 --> 00:52:10.260
the situation.

00:52:10.260 --> 00:52:11.759
If it's somebody
you always trusted,

00:52:11.759 --> 00:52:13.940
consider has their
power changed?

00:52:13.940 --> 00:52:15.190
Has anything else changed?

00:52:15.190 --> 00:52:17.280
Because they may not want
to be untrustworthy just

00:52:17.280 --> 00:52:18.779
like our subjects
did, but they will

00:52:18.779 --> 00:52:20.230
be and construct
a story for why.

00:52:20.230 --> 00:52:22.370
AUDIENCE: Hi and
thank you for coming.

00:52:22.370 --> 00:52:24.550
I have a question actually
kind of related to that.

00:52:24.550 --> 00:52:28.320
So have you done any study
in how people's relationship

00:52:28.320 --> 00:52:32.510
long term potentially changes
how sensitive or not sensitive

00:52:32.510 --> 00:52:33.700
they are to social cues?

00:52:33.700 --> 00:52:35.914
I can imagine
someone who perhaps

00:52:35.914 --> 00:52:37.830
does all of the social
cues that you mentioned

00:52:37.830 --> 00:52:40.020
in terms of lying, but perhaps
like a brother and sister,

00:52:40.020 --> 00:52:40.480
for example.

00:52:40.480 --> 00:52:41.930
They've gotten numb
to it over time,

00:52:41.930 --> 00:52:43.721
and maybe they can't
pick up on it anymore.

00:52:43.721 --> 00:52:47.420
Or do you have a sort of sense
of how long-term relationships

00:52:47.420 --> 00:52:49.004
can change how people
pick up on that?

00:52:49.004 --> 00:52:50.420
DAVID DESTENO: Two
things on that,

00:52:50.420 --> 00:52:51.960
we haven't done
that work, but what

00:52:51.960 --> 00:52:54.280
we know from the nonverbal
literature in general

00:52:54.280 --> 00:52:58.050
is that people have a what
are often termed accents.

00:52:58.050 --> 00:53:01.560
So there's a kind of
panhuman way of doing it.

00:53:01.560 --> 00:53:03.990
But then different cultures
or even different families

00:53:03.990 --> 00:53:06.920
or individuals will have
modifications of that.

00:53:06.920 --> 00:53:08.590
And so the longer
you are with someone,

00:53:08.590 --> 00:53:10.370
you can learn that
for this person,

00:53:10.370 --> 00:53:13.500
this is that person's
tell in some ways.

00:53:13.500 --> 00:53:15.440
And it will be some
combination of these.

00:53:15.440 --> 00:53:17.856
But other things added to that
will increase your accuracy

00:53:17.856 --> 00:53:18.740
for that person.

00:53:18.740 --> 00:53:21.073
But another thing in terms
of long term, what does trust

00:53:21.073 --> 00:53:22.020
do, it's beneficial.

00:53:22.020 --> 00:53:24.111
So there's great work
done by Sandra Murray.

00:53:24.111 --> 00:53:26.360
She's psychologist who studies
romantic relationships.

00:53:26.360 --> 00:53:28.610
And one thing that the trust
does in our relationships

00:53:28.610 --> 00:53:32.174
is it smooths out
the bumps, as I said.

00:53:32.174 --> 00:53:34.590
So we've probably all had times
when our significant other

00:53:34.590 --> 00:53:37.725
does something and
it makes us go hmm,

00:53:37.725 --> 00:53:40.100
whether it's you think the
person's flirting with someone

00:53:40.100 --> 00:53:41.683
or they're working
late or what is it.

00:53:41.683 --> 00:53:44.430
Well, if you inherently
trust this person,

00:53:44.430 --> 00:53:49.851
at a very nonconscious level,
that trust erases that hmm.

00:53:49.851 --> 00:53:52.100
It just gives you an intuition
that everything's fine.

00:53:52.100 --> 00:53:54.300
And if you trust that
intuition, that's good.

00:53:54.300 --> 00:53:56.150
Because lots of times
we'll do something

00:53:56.150 --> 00:53:58.170
that we're not trying
to be untrustworthy.

00:53:58.170 --> 00:53:59.600
It's just an inadvertent thing.

00:53:59.600 --> 00:54:02.141
But if a person interprets that
as, oh, you're untrustworthy,

00:54:02.141 --> 00:54:04.540
it can start you into
kind of a death spiral.

00:54:04.540 --> 00:54:06.631
And so the good thing
about trusting someone

00:54:06.631 --> 00:54:08.130
over the long time
in a relationship

00:54:08.130 --> 00:54:09.754
is that it helps
smooth out those bumps

00:54:09.754 --> 00:54:11.580
so there aren't mistakes made.

00:54:11.580 --> 00:54:13.120
So that one person doesn't
interpret the other person

00:54:13.120 --> 00:54:14.965
as flirting or doing
something with somebody else

00:54:14.965 --> 00:54:16.130
that they shouldn't have.

00:54:16.130 --> 00:54:17.170
Now, if they keep doing
it, well, then you're

00:54:17.170 --> 00:54:18.480
going to know it's real.

00:54:18.480 --> 00:54:21.916
But that's a benefit
of trust long term.

00:54:21.916 --> 00:54:23.040
MING: Thank you, my friend.

00:54:23.040 --> 00:54:25.280
So the book is "The
Truth About Trust"

00:54:25.280 --> 00:54:26.780
available where
books are sold, also

00:54:26.780 --> 00:54:28.238
available at the
back of this room.

00:54:28.238 --> 00:54:29.930
And David will be
around to sign books.

00:54:29.930 --> 00:54:31.620
And my friends, David DeSteno.

00:54:31.620 --> 00:54:33.650
DAVID DESTENO: Thank you all.

