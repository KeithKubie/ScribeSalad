WEBVTT
Kind: captions
Language: en

00:00:00.570 --> 00:00:03.860
We hopefully now have a
respectable working knowledge

00:00:03.860 --> 00:00:06.472
of the sampling distribution
of the sample mean.

00:00:06.472 --> 00:00:08.930
And what I want to do in this
video is explore a little bit

00:00:08.930 --> 00:00:13.410
more on how that distribution
changes as we change our sample

00:00:13.410 --> 00:00:14.930
size, n.

00:00:14.930 --> 00:00:16.200
I'll write n down right here.

00:00:16.200 --> 00:00:17.310
Our sample size n.

00:00:17.310 --> 00:00:19.900
So just as a bit of review,
we saw before, we could just

00:00:19.900 --> 00:00:22.080
start off with any
crazy distribution.

00:00:22.080 --> 00:00:24.230
Maybe it looks
something like this.

00:00:24.230 --> 00:00:26.072
I'll do a discrete distribution.

00:00:26.072 --> 00:00:27.780
Really, to model
anything, at some point,

00:00:27.780 --> 00:00:29.097
you have to make it discrete.

00:00:29.097 --> 00:00:31.180
It could be a very granular
discrete distribution,

00:00:31.180 --> 00:00:33.520
but let's say something
crazy that looks like this.

00:00:33.520 --> 00:00:36.810
This is clearly not a
normal distribution.

00:00:36.810 --> 00:00:39.170
But we saw in the first
video, if you take, let's say,

00:00:39.170 --> 00:00:40.930
sample sizes of four.

00:00:40.930 --> 00:00:43.610
So if you took four numbers
from this distribution, four

00:00:43.610 --> 00:00:45.570
random numbers where,
let's say, this

00:00:45.570 --> 00:00:51.920
is the probability of a
1, 2, 3, 4, 5, 6, 7, 8, 9.

00:00:51.920 --> 00:00:54.360
If you took four numbers
at a time and averaged

00:00:54.360 --> 00:00:56.662
them-- let me do that here.

00:00:56.662 --> 00:00:58.120
If you took four
numbers at a time,

00:00:58.120 --> 00:01:01.010
let's say we use this
distribution to generate

00:01:01.010 --> 00:01:02.030
four random numbers.

00:01:02.030 --> 00:01:02.530
Right?

00:01:02.530 --> 00:01:04.010
We're very likely to get a 9.

00:01:04.010 --> 00:01:05.740
We're definitely not going
to get any 7's or 8's.

00:01:05.740 --> 00:01:07.323
We're definitely not
going to get a 4.

00:01:07.323 --> 00:01:08.600
We might get a 1 or 2.

00:01:08.600 --> 00:01:09.820
3 is also very likely.

00:01:09.820 --> 00:01:10.920
5 is very likely.

00:01:10.920 --> 00:01:13.710
So we use this function
to essentially generate

00:01:13.710 --> 00:01:15.040
random numbers for us.

00:01:15.040 --> 00:01:17.980
And we take samples of four,
and then we average them up.

00:01:17.980 --> 00:01:20.550
So let's say our first
average is, I don't know,

00:01:20.550 --> 00:01:24.760
let's say it's a 9, it's
a 5, it's another 9,

00:01:24.760 --> 00:01:26.104
and then it's a 1.

00:01:26.104 --> 00:01:26.770
So what is that?

00:01:26.770 --> 00:01:29.490
That's 14 plus 10,
24 divided by 4.

00:01:29.490 --> 00:01:32.330
The average for
this first trial,

00:01:32.330 --> 00:01:35.400
for this first sample of
four, is going to be 6.

00:01:35.400 --> 00:01:37.109
They add up to 24 divided by 4.

00:01:37.109 --> 00:01:38.400
So we would plot it right here.

00:01:38.400 --> 00:01:40.955
Our average was 6 that time.

00:01:40.955 --> 00:01:41.580
Just like that.

00:01:41.580 --> 00:01:43.010
And we'll just keep doing it.

00:01:43.010 --> 00:01:45.924
And we've seen in the past that,
if you just keep doing this,

00:01:45.924 --> 00:01:47.590
this is going to start
looking something

00:01:47.590 --> 00:01:49.060
like a normal distribution.

00:01:49.060 --> 00:01:51.300
So maybe we do it again,
the average 6 again.

00:01:51.300 --> 00:01:53.270
Maybe we do it again,
the average is 5.

00:01:53.270 --> 00:01:54.690
We do it again,
the average is 7.

00:01:54.690 --> 00:01:56.550
We do it again,
the average is 6.

00:01:56.550 --> 00:01:59.560
And then if you just do
this a ton, a ton of times,

00:01:59.560 --> 00:02:01.200
your distribution
might look something

00:02:01.200 --> 00:02:03.070
that looks very much like
a normal distribution.

00:02:03.070 --> 00:02:04.403
So these boxes are really small.

00:02:04.403 --> 00:02:06.140
So we just do a bunch
of these trials.

00:02:06.140 --> 00:02:09.289
At some point, it might look a
lot like a normal distribution.

00:02:09.289 --> 00:02:11.160
Obviously, there are
some average values.

00:02:11.160 --> 00:02:12.910
It won't be a perfect
normal distribution,

00:02:12.910 --> 00:02:16.160
because you can never
get anything less than 0,

00:02:16.160 --> 00:02:18.640
or anything less than 1,
really, as an average.

00:02:18.640 --> 00:02:20.080
You can't get 0 as an average.

00:02:20.080 --> 00:02:21.704
And you can't get
anything more than 9.

00:02:21.704 --> 00:02:23.690
So it's not going to have
infinitely long tails

00:02:23.690 --> 00:02:25.430
but, at least for the
middle part of it,

00:02:25.430 --> 00:02:27.942
a normal distribution might
be good approximation.

00:02:27.942 --> 00:02:29.400
In this video, what
I want to think

00:02:29.400 --> 00:02:34.470
about is what happens
as we change n.

00:02:34.470 --> 00:02:37.090
So in this case, n was 4.

00:02:37.090 --> 00:02:39.020
n is our sample size.

00:02:39.020 --> 00:02:40.850
Every time we do a
trial, we took four

00:02:40.850 --> 00:02:42.730
and we took their average,
and we plotted it.

00:02:42.730 --> 00:02:44.530
We could have had n equal 10.

00:02:44.530 --> 00:02:49.250
We could've taken 10 samples
from this population,

00:02:49.250 --> 00:02:51.740
you could say, or from
this random variable,

00:02:51.740 --> 00:02:54.186
averaged them, and
then plotted them here.

00:02:54.186 --> 00:02:56.060
And in the last video,
we ran the simulation.

00:02:56.060 --> 00:02:58.226
I'm going to go back to
that simulation in a second.

00:02:58.226 --> 00:02:59.880
We saw a couple of things.

00:02:59.880 --> 00:03:02.170
And I'll show it to you
in a little bit more depth

00:03:02.170 --> 00:03:03.510
this time.

00:03:03.510 --> 00:03:05.610
When n is pretty
small, it doesn't

00:03:05.610 --> 00:03:08.090
approach a normal
distribution that well.

00:03:08.090 --> 00:03:11.310
So when n is small-- let's
take the extreme case.

00:03:11.310 --> 00:03:14.090
What happens when
n is equal to 1?

00:03:14.090 --> 00:03:15.910
And that literally
just means I take

00:03:15.910 --> 00:03:18.670
one instance of this random
variable and average it.

00:03:18.670 --> 00:03:20.315
Well, it's just going
to be that thing.

00:03:20.315 --> 00:03:22.440
So if I just take a bunch
of trials from this thing

00:03:22.440 --> 00:03:24.232
and plot it over time,
what's it look like?

00:03:24.232 --> 00:03:25.856
Well, it's definitely
not going to look

00:03:25.856 --> 00:03:27.049
like a normal distribution.

00:03:27.049 --> 00:03:28.590
You're going to have
a couple of 1's.

00:03:28.590 --> 00:03:30.131
You're going to have
a couple of 2's.

00:03:30.131 --> 00:03:32.630
You're going to have
more 3's like that.

00:03:32.630 --> 00:03:33.880
You're going to have no 4's.

00:03:33.880 --> 00:03:35.660
You're going to
have a bunch of 5's.

00:03:35.660 --> 00:03:37.743
You're going to have some
6's that look like that.

00:03:37.743 --> 00:03:39.470
And you're going to
have a bunch of 9's.

00:03:39.470 --> 00:03:41.470
So there, your sampling
distribution of the sample

00:03:41.470 --> 00:03:43.030
mean for an n of 1
is going to look--

00:03:43.030 --> 00:03:44.930
I don't care how
many trials you do,

00:03:44.930 --> 00:03:47.090
it's not going to look
like a normal distribution.

00:03:47.090 --> 00:03:48.771
So the central limit
theorem, although I

00:03:48.771 --> 00:03:50.270
said you do a bunch
of trials, it'll

00:03:50.270 --> 00:03:51.420
look like a normal
distribution, definitely

00:03:51.420 --> 00:03:53.020
doesn't work for n equals 1.

00:03:53.020 --> 00:03:56.020
As n gets larger, though,
it starts to make sense.

00:03:56.020 --> 00:03:58.480
Let's see, if we've
got n equals 2--

00:03:58.480 --> 00:04:00.120
and I'm all just
doing this in my head.

00:04:00.120 --> 00:04:02.578
I don't know what the actual
distributions would look like.

00:04:02.578 --> 00:04:04.130
But then, it still
would be difficult

00:04:04.130 --> 00:04:06.317
for it to become an exact
normal distribution.

00:04:06.317 --> 00:04:07.900
But then you could
get more instances,

00:04:07.900 --> 00:04:09.834
you could get more--
you might get things

00:04:09.834 --> 00:04:10.750
from all of the above.

00:04:10.750 --> 00:04:12.972
But in each of your baskets
that you're averaging,

00:04:12.972 --> 00:04:14.513
you're only going
to get two numbers.

00:04:17.057 --> 00:04:19.140
For example, you're never
going to get a 7 and 1/2

00:04:19.140 --> 00:04:22.207
in your sampling
distribution of the sample

00:04:22.207 --> 00:04:24.540
mean for n is equal to 2,
because it's impossible to get

00:04:24.540 --> 00:04:26.740
a 7, and it's
impossible to get an 8.

00:04:26.740 --> 00:04:30.350
So you're never going to get
7 and 1/2 as-- so maybe when

00:04:30.350 --> 00:04:35.580
you plot it, maybe
it looks like this.

00:04:35.580 --> 00:04:38.190
But there'll be a gap at 7 and
1/2 because that's impossible.

00:04:38.190 --> 00:04:40.910
And maybe it looks
something like that.

00:04:40.910 --> 00:04:42.800
So it still won't be
a normal distribution

00:04:42.800 --> 00:04:44.310
when n is equal to 2.

00:04:44.310 --> 00:04:46.620
So there's a couple of
interesting things here.

00:04:46.620 --> 00:04:48.600
So one thing-- and I didn't
mention this the first time,

00:04:48.600 --> 00:04:50.120
just because I really wanted
you to get the gut sense of what

00:04:50.120 --> 00:04:52.090
the central limit theorem is.

00:04:52.090 --> 00:04:55.190
The central limit theorem
says as n approaches, really

00:04:55.190 --> 00:04:57.370
as it approaches
infinity, then is

00:04:57.370 --> 00:04:59.205
when you get the real
normal distribution.

00:05:02.260 --> 00:05:04.190
But in kind of
everyday practice,

00:05:04.190 --> 00:05:06.390
you don't have to get that
much beyond n equals 2.

00:05:06.390 --> 00:05:08.425
If you get to n equals
10 or n equals 15,

00:05:08.425 --> 00:05:10.550
you're getting very close
to a normal distribution.

00:05:10.550 --> 00:05:14.839
So this converges to a normal
distribution very quickly.

00:05:14.839 --> 00:05:16.380
Now, the other thing
is you obviously

00:05:16.380 --> 00:05:17.990
want many, many trials.

00:05:17.990 --> 00:05:20.250
So this is your sample size.

00:05:20.250 --> 00:05:21.400
That is your sample size.

00:05:21.400 --> 00:05:23.660
That's the size of
each of your baskets.

00:05:23.660 --> 00:05:25.430
In the very first
video I did on this,

00:05:25.430 --> 00:05:27.250
I took a sample size of 4.

00:05:27.250 --> 00:05:30.150
In the simulation I
did in the last video,

00:05:30.150 --> 00:05:32.505
we did sample sizes of 4
and 10 and whatever else.

00:05:32.505 --> 00:05:33.630
This is a sample size of 1.

00:05:33.630 --> 00:05:34.713
So that's our sample size.

00:05:34.713 --> 00:05:39.280
So as that approaches
infinity, your actual sampling

00:05:39.280 --> 00:05:43.160
distribution of the
sample mean will

00:05:43.160 --> 00:05:46.240
approach a normal distribution.

00:05:46.240 --> 00:05:49.050
Now, in order to actually
see that normal distribution,

00:05:49.050 --> 00:05:50.790
and actually to
prove it to yourself,

00:05:50.790 --> 00:05:52.680
you would have to
do this many, many--

00:05:52.680 --> 00:05:55.400
remember the normal
distribution happens--

00:05:55.400 --> 00:05:58.450
this is kind of the population,
or this is the random variable.

00:05:58.450 --> 00:06:00.420
That tells you all
of the possibilities.

00:06:00.420 --> 00:06:02.850
In real life, we seldom know
all of the possibilities.

00:06:02.850 --> 00:06:07.017
In fact, in real life, we
seldom know the pure probability

00:06:07.017 --> 00:06:07.850
generating function.

00:06:07.850 --> 00:06:08.960
Only if we're kind
of writing it,

00:06:08.960 --> 00:06:10.460
if we're writing a
computer program.

00:06:10.460 --> 00:06:12.570
Normally we're doing
samples, and we're

00:06:12.570 --> 00:06:13.780
trying to estimate things.

00:06:13.780 --> 00:06:16.960
So normally, there's
some random variable.

00:06:16.960 --> 00:06:20.357
And then maybe we take
a bunch of samples.

00:06:20.357 --> 00:06:21.940
We take their means,
and we plot them.

00:06:21.940 --> 00:06:23.870
And then we're going to get some
type of normal distribution.

00:06:23.870 --> 00:06:26.290
Let's say we take samples
of 100 and we average them.

00:06:26.290 --> 00:06:28.380
We're going to get some
normal distribution.

00:06:28.380 --> 00:06:32.690
And in theory, as we take those
averages hundreds or thousands

00:06:32.690 --> 00:06:37.990
of times, our data set is going
to more closely approximate

00:06:37.990 --> 00:06:49.140
that pure sampling distribution
of the sample mean.

00:06:49.140 --> 00:06:51.590
This thing is a
real distribution.

00:06:51.590 --> 00:06:54.850
It's a real distribution
with a real mean.

00:06:54.850 --> 00:06:57.970
It has a pure mean.

00:06:57.970 --> 00:07:04.460
So the mean of the sampling
distribution of the sample

00:07:04.460 --> 00:07:06.370
mean, we'll write it like that.

00:07:06.370 --> 00:07:10.470
Notice I didn't write it is
as just the x with-- this

00:07:10.470 --> 00:07:14.590
is actually saying this
is a real population mean.

00:07:14.590 --> 00:07:17.150
This is a real
random variable mean.

00:07:17.150 --> 00:07:19.206
If you looked at every
possibility of all

00:07:19.206 --> 00:07:21.330
of the samples that you
can take from your original

00:07:21.330 --> 00:07:24.970
distribution, from some other
random original distribution,

00:07:24.970 --> 00:07:27.930
and you just took all
of the possibilities of,

00:07:27.930 --> 00:07:28.970
let's say, sample size.

00:07:28.970 --> 00:07:30.470
Let's say we're
dealing with a world

00:07:30.470 --> 00:07:31.740
where a sample size is 10.

00:07:31.740 --> 00:07:34.050
If you took all of the
combinations of 10 samples

00:07:34.050 --> 00:07:38.100
from some original distribution
and you averaged them out,

00:07:38.100 --> 00:07:39.672
this would describe
that function.

00:07:39.672 --> 00:07:41.130
Of course, in
reality, if you don't

00:07:41.130 --> 00:07:42.860
know the original
distribution, you

00:07:42.860 --> 00:07:44.610
can't take infinite
samples from it.

00:07:44.610 --> 00:07:46.500
So you won't know
every combination.

00:07:46.500 --> 00:07:53.160
But if you did it with 1,000, if
you did the trial 1,000 times--

00:07:53.160 --> 00:07:56.200
so 1,000 times you took 10
samples from some distribution,

00:07:56.200 --> 00:07:58.400
and took 1,000 averages
and then plotted them,

00:07:58.400 --> 00:08:01.830
you're going to
get pretty close.

00:08:01.830 --> 00:08:03.710
Now, the next thing
I want to touch on

00:08:03.710 --> 00:08:06.970
is what happens as n-- we know
as n approaches infinity, it

00:08:06.970 --> 00:08:08.830
becomes more of a
normal distribution.

00:08:08.830 --> 00:08:11.750
But as I said already, n
equals 10 is pretty good.

00:08:11.750 --> 00:08:13.617
And n equals 20 is even better.

00:08:13.617 --> 00:08:15.200
But we saw something
in the last video

00:08:15.200 --> 00:08:17.800
that at least I find
pretty interesting.

00:08:17.800 --> 00:08:20.134
Let's say we start with this
crazy distribution up here.

00:08:20.134 --> 00:08:21.883
It really doesn't
matter what distribution

00:08:21.883 --> 00:08:22.960
we're starting with.

00:08:22.960 --> 00:08:26.330
We saw in the simulation
that when n is equal--

00:08:26.330 --> 00:08:28.910
let's say n is equal to 5.

00:08:28.910 --> 00:08:32.721
Our graph, after we take
samples of five, average them,

00:08:32.721 --> 00:08:34.220
and we do it 10,000
times, our graph

00:08:34.220 --> 00:08:36.700
looks something like this.

00:08:36.700 --> 00:08:38.480
It's kind of wide like that.

00:08:38.480 --> 00:08:41.750
And then when we did
n is equal to 10,

00:08:41.750 --> 00:08:43.970
our graph looked a little
bit-- it was actually

00:08:43.970 --> 00:08:46.770
a little bit squeezed in,
like that, a little bit more.

00:08:46.770 --> 00:08:49.824
So not only was it
more normal-- that's

00:08:49.824 --> 00:08:51.490
what the central limit
theorem tells us,

00:08:51.490 --> 00:08:53.620
because we're taking
larger sample sizes--

00:08:53.620 --> 00:08:56.900
but it had a smaller standard
deviation or a smaller

00:08:56.900 --> 00:08:57.800
variance.

00:08:57.800 --> 00:09:00.160
The mean is going to be
the same, either case.

00:09:00.160 --> 00:09:02.160
But when our sample
size was larger,

00:09:02.160 --> 00:09:04.900
our standard deviation
became smaller.

00:09:04.900 --> 00:09:06.880
In fact, our standard
deviation became smaller

00:09:06.880 --> 00:09:09.990
than our original
population distribution,

00:09:09.990 --> 00:09:12.880
or our original probability
density function.

00:09:12.880 --> 00:09:14.960
Let me show you that
with a simulation.

00:09:14.960 --> 00:09:16.507
So let me clear everything.

00:09:16.507 --> 00:09:18.090
And this simulation
is as good as any.

00:09:18.090 --> 00:09:20.423
So the first thing I want to
show-- or this distribution

00:09:20.423 --> 00:09:21.259
is as good as any.

00:09:21.259 --> 00:09:23.300
The first thing I want to
show you is that n of 2

00:09:23.300 --> 00:09:25.040
is really not that good.

00:09:25.040 --> 00:09:29.490
So let's compare an n of 2
to, let's say, an n of 16.

00:09:29.490 --> 00:09:33.445
So when you compare an n of 2 to
an n of 16-- let's do it once.

00:09:33.445 --> 00:09:34.900
So you get one, two trials.

00:09:34.900 --> 00:09:37.620
You average them, and
then it's going to do 16.

00:09:37.620 --> 00:09:40.660
And then it's going to plot it
down here and average there.

00:09:40.660 --> 00:09:43.780
Let's do that 10,000 times.

00:09:43.780 --> 00:09:45.930
So notice when you
took an n of 2,

00:09:45.930 --> 00:09:48.480
even though we did
it 10,000 times,

00:09:48.480 --> 00:09:50.400
this is not approaching
a normal distribution.

00:09:50.400 --> 00:09:53.430
And you can actually see it in
the skew and kurtosis numbers.

00:09:53.430 --> 00:09:55.730
It has a rightward
positive skew,

00:09:55.730 --> 00:09:58.190
means it has longer tail
to the right than to left.

00:09:58.190 --> 00:10:00.070
And then it has a
negative kurtosis,

00:10:00.070 --> 00:10:04.330
which means that it has
shorter tails and smaller

00:10:04.330 --> 00:10:08.410
peaks than a standard
normal distribution.

00:10:08.410 --> 00:10:11.480
Now, when n is equal to 16 and
you do the same-- so every time

00:10:11.480 --> 00:10:15.530
we took 16 samples from this
distribution function up here

00:10:15.530 --> 00:10:17.690
and averaged them--
and each of these dots

00:10:17.690 --> 00:10:18.810
represent an average.

00:10:18.810 --> 00:10:21.590
We did it 10,001 times.

00:10:21.590 --> 00:10:23.590
Now notice, the mean is
the same in both places.

00:10:23.590 --> 00:10:26.970
But here, all of a sudden,
our kurtosis is much smaller,

00:10:26.970 --> 00:10:28.380
and our skew is much smaller.

00:10:28.380 --> 00:10:32.240
So we are more normal
in this situation.

00:10:32.240 --> 00:10:34.720
But even a more interesting
thing is our standard deviation

00:10:34.720 --> 00:10:35.530
is smaller.

00:10:35.530 --> 00:10:37.340
This is more squeezed
in than that is.

00:10:37.340 --> 00:10:38.970
And it's definitely
more squeezed

00:10:38.970 --> 00:10:41.790
in than our original
distribution.

00:10:41.790 --> 00:10:45.260
Now, let me do it with two--
let me clear everything again.

00:10:45.260 --> 00:10:48.990
I like this distribution
because it's

00:10:48.990 --> 00:10:50.430
a very non-normal distribution.

00:10:50.430 --> 00:10:53.330
It looks like a bimodal
distribution of some kind.

00:10:53.330 --> 00:10:56.540
And let's take a scenario where
I take an n of-- let's take two

00:10:56.540 --> 00:10:57.040
good n's.

00:10:57.040 --> 00:10:58.009
Let's take an n of 16.

00:10:58.009 --> 00:10:59.050
That's a nice, healthy n.

00:10:59.050 --> 00:11:00.721
And let's take an n of 25.

00:11:00.721 --> 00:11:02.220
And let's compare
them a little bit.

00:11:06.680 --> 00:11:09.156
I'll do one trial animated
just to-- it's always

00:11:09.156 --> 00:11:09.780
nice to see it.

00:11:09.780 --> 00:11:12.170
So first, it's going to do 16 of
these trials and average them.

00:11:12.170 --> 00:11:12.890
And there we go.

00:11:12.890 --> 00:11:16.020
Then it's going to
do 25 of these trials

00:11:16.020 --> 00:11:18.180
and then average them.

00:11:18.180 --> 00:11:19.430
And then, there we go.

00:11:19.430 --> 00:11:21.410
Now let's do that, what
I just did animated,

00:11:21.410 --> 00:11:24.630
let's do it 10,000 times.

00:11:24.630 --> 00:11:26.264
Miracles of computers.

00:11:26.264 --> 00:11:27.180
Now, notice something.

00:11:27.180 --> 00:11:28.410
Now this is 10,000 times.

00:11:28.410 --> 00:11:30.430
These are both pretty
good approximations

00:11:30.430 --> 00:11:32.000
of normal distributions.

00:11:32.000 --> 00:11:34.750
The n is equal to
25 is more normal.

00:11:34.750 --> 00:11:38.370
It has less skew, slightly less
skew than n is equal to 16.

00:11:38.370 --> 00:11:40.700
It has a slightly
less kurtosis, which

00:11:40.700 --> 00:11:44.310
means it's closer to being
a normal distribution

00:11:44.310 --> 00:11:45.229
than n is equal to 16.

00:11:45.229 --> 00:11:47.270
But even more interesting,
it's more squeezed in.

00:11:47.270 --> 00:11:49.370
It has a lower
standard deviation.

00:11:49.370 --> 00:11:52.590
The standard
deviation here is 2.1,

00:11:52.590 --> 00:11:56.200
and the standard
deviation here is 2.64.

00:11:56.200 --> 00:11:59.090
So that's another-- I
mean, I kind of touched

00:11:59.090 --> 00:12:01.690
on that in the last video,
and it kind makes sense.

00:12:01.690 --> 00:12:03.720
For every sample you
do for your average,

00:12:03.720 --> 00:12:07.070
the more you put in that sample,
the less standard deviation.

00:12:07.070 --> 00:12:08.530
Think of the extreme case.

00:12:08.530 --> 00:12:11.100
If instead of taking 16
samples from our distribution

00:12:11.100 --> 00:12:13.192
every time, or
instead of taking 25,

00:12:13.192 --> 00:12:15.840
if I were to take a million
samples from this distribution

00:12:15.840 --> 00:12:18.310
every time-- if I were
to take a million samples

00:12:18.310 --> 00:12:21.600
from this distribution
every time, that sample mean

00:12:21.600 --> 00:12:24.160
is always going to be pretty
darn close to my mean.

00:12:24.160 --> 00:12:25.910
If I take a million
samples of everything,

00:12:25.910 --> 00:12:28.942
if I essentially try to
estimate a mean by taking

00:12:28.942 --> 00:12:30.400
a million samples,
I'm going to get

00:12:30.400 --> 00:12:31.900
a pretty good
estimate of that mean.

00:12:31.900 --> 00:12:35.090
The probability that a
bunch of-- a million numbers

00:12:35.090 --> 00:12:37.050
are all out here is very low.

00:12:37.050 --> 00:12:41.225
So if n is a million, of
course, all of my sample

00:12:41.225 --> 00:12:42.600
means when I
average them are all

00:12:42.600 --> 00:12:47.070
going to be really tightly
focused around the mean itself.

00:12:47.070 --> 00:12:50.532
And so hopefully that kind of
makes sense to you as well.

00:12:50.532 --> 00:12:51.990
If it doesn't, just
think about it.

00:12:51.990 --> 00:12:54.750
Or even use this tool
and experiment with it

00:12:54.750 --> 00:12:57.317
just so you can trust that
that is really the case.

00:12:57.317 --> 00:12:58.900
And it actually turns
out that there's

00:12:58.900 --> 00:13:00.960
a very clean
formula that relates

00:13:00.960 --> 00:13:04.460
to standard deviation of
the original probability

00:13:04.460 --> 00:13:07.770
distribution function to
the standard deviation

00:13:07.770 --> 00:13:10.250
of the sampling distribution
of the sample mean.

00:13:10.250 --> 00:13:12.690
And as you can imagine,
it is a function

00:13:12.690 --> 00:13:14.930
of your sample size,
of how many samples

00:13:14.930 --> 00:13:17.200
you take out in every basket
before you average them.

00:13:17.200 --> 00:13:19.403
And I'll go over that
in the next video.

