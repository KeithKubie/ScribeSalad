WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.473
[MUSIC PLAYING]

00:00:08.360 --> 00:00:10.310
MAGGIE JACKSON:
Well, good morning

00:00:10.310 --> 00:00:14.210
and welcome to Building Healthy
Technology, an inspiration

00:00:14.210 --> 00:00:17.510
session on digital well-being.

00:00:17.510 --> 00:00:18.860
I'm Maggie Jackson.

00:00:18.860 --> 00:00:21.260
I'm the author of
"Distracted," a book

00:00:21.260 --> 00:00:26.975
about the fragmentation of focus
and the ways we can recapture

00:00:26.975 --> 00:00:28.435
our attention today.

00:00:28.435 --> 00:00:31.700
I'm a journalist-- my background
and a former "Boston Globe"

00:00:31.700 --> 00:00:32.960
columnist.

00:00:32.960 --> 00:00:36.530
And joining me on my
left is Adam Alter.

00:00:36.530 --> 00:00:40.250
He is a NYU professor in
marketing and psychology,

00:00:40.250 --> 00:00:44.510
and writes widely for the media,
and has written most recently

00:00:44.510 --> 00:00:48.050
a book called "Irresistible,"
about behavioral addiction

00:00:48.050 --> 00:00:51.860
in our society and
tech influences.

00:00:51.860 --> 00:00:55.290
As well, joining me on
the stage is Glen Murphy.

00:00:55.290 --> 00:00:58.790
He's the director of UX Chrome.

00:00:58.790 --> 00:01:04.310
And he has been a creative force
behind many Android products

00:01:04.310 --> 00:01:09.260
from Android Auto and TV,
and behind a creative force

00:01:09.260 --> 00:01:12.170
in the digital well-being
front, of which you've

00:01:12.170 --> 00:01:13.660
heard quite a bit this week.

00:01:13.660 --> 00:01:16.190
So we'll talk about that.

00:01:16.190 --> 00:01:19.610
And I'd also like to thank
Helen Hutton and Nancy Baker

00:01:19.610 --> 00:01:22.400
for putting this
together so well.

00:01:22.400 --> 00:01:27.290
Just for a minute, let's start
by stepping back in history,

00:01:27.290 --> 00:01:29.270
stepping back in time.

00:01:29.270 --> 00:01:33.560
The year is 1905
and picture Freud.

00:01:33.560 --> 00:01:39.020
Yes, Freud standing on
the Piazza Colonna in Rome

00:01:39.020 --> 00:01:41.570
on a warm September evening.

00:01:41.570 --> 00:01:43.850
He's watching a
lantern slide show.

00:01:43.850 --> 00:01:47.600
And it's a medley of short
films and still photographs

00:01:47.600 --> 00:01:51.860
projected on the roofs of
the houses in the Piazza.

00:01:51.860 --> 00:01:56.210
And as he writes later, every
time he tries to turn away,

00:01:56.210 --> 00:02:00.380
a certain tension in the
crowd keeps him riveted.

00:02:00.380 --> 00:02:04.010
This great thinker
far from home was

00:02:04.010 --> 00:02:06.800
entranced by the media,
the precursors of the media

00:02:06.800 --> 00:02:08.229
that envelop us today.

00:02:08.229 --> 00:02:12.170
He was conflicted,
and so are we.

00:02:12.170 --> 00:02:15.440
Fast-forward to our
century and we're

00:02:15.440 --> 00:02:19.370
wrestling, still, with the
promise and the drawbacks

00:02:19.370 --> 00:02:21.122
of technology.

00:02:21.122 --> 00:02:23.330
So today, we're going to
talk a little bit about some

00:02:23.330 --> 00:02:26.270
of the challenges,
some of the ways

00:02:26.270 --> 00:02:28.520
in which we can
harness the benefits,

00:02:28.520 --> 00:02:32.840
and also, begin to take more
responsibility-- all of us

00:02:32.840 --> 00:02:35.720
as makers, developers,
citizens, and parents

00:02:35.720 --> 00:02:38.420
for what we're creating.

00:02:38.420 --> 00:02:40.760
So to start, of course,
it's really important

00:02:40.760 --> 00:02:44.480
to understand the
landscape, understand

00:02:44.480 --> 00:02:46.820
what we're experiencing.

00:02:46.820 --> 00:02:49.540
So, Adam, maybe
we'll start with you

00:02:49.540 --> 00:02:53.030
to talk a little bit about
the history of technology.

00:02:53.030 --> 00:02:57.590
And for millennia, people
have been associating

00:02:57.590 --> 00:03:00.050
new technologies with
something fearful,

00:03:00.050 --> 00:03:02.300
something challenging,
ever since Plato

00:03:02.300 --> 00:03:07.130
complained that the
written word would

00:03:07.130 --> 00:03:11.420
undermine our memories that were
so important in oral culture.

00:03:11.420 --> 00:03:15.060
So how serious are the
problems of our time?

00:03:15.060 --> 00:03:16.825
Why should we be
paying attention?

00:03:16.825 --> 00:03:19.550
ADAM ALTER: Well, I mean, if
you think about human evolution,

00:03:19.550 --> 00:03:22.010
things that are novel
are obviously very scary,

00:03:22.010 --> 00:03:23.690
because we haven't
yet tested them out.

00:03:23.690 --> 00:03:25.310
We don't know if they're
there to help us, or hurt us,

00:03:25.310 --> 00:03:26.515
or if they're neutral.

00:03:26.515 --> 00:03:28.700
And so any time something
new comes along,

00:03:28.700 --> 00:03:30.932
there's an instinctive
panic in some sense.

00:03:30.932 --> 00:03:32.390
And that's where
we are here again.

00:03:32.390 --> 00:03:34.370
And we've had that
all through time.

00:03:34.370 --> 00:03:36.942
In the 20th century, it was
TV, and pinball machines,

00:03:36.942 --> 00:03:38.900
and video games, and all
sorts of other things.

00:03:38.900 --> 00:03:41.180
And one of the questions
people ask all the time,

00:03:41.180 --> 00:03:43.234
I think quite validly
is, is this different?

00:03:43.234 --> 00:03:44.900
Are we talking about
something different

00:03:44.900 --> 00:03:46.580
now in the last 10, 15 years?

00:03:46.580 --> 00:03:48.410
Or is this just the
latest moral panic

00:03:48.410 --> 00:03:50.000
that we're all dealing with?

00:03:50.000 --> 00:03:52.680
I think there are some things
that make this different.

00:03:52.680 --> 00:03:56.690
I think for sheer scope, we're
talking about experiences

00:03:56.690 --> 00:03:59.690
that billions of us
enjoy, or sometimes don't

00:03:59.690 --> 00:04:02.850
enjoy the hours, and hours,
and hours of the day.

00:04:02.850 --> 00:04:07.700
So the average American adult
spends four hours every day

00:04:07.700 --> 00:04:09.820
glued to a phone
screen, so that's

00:04:09.820 --> 00:04:11.570
taking up a huge amount
of the waking day.

00:04:11.570 --> 00:04:14.390
So we should understand
that, I think, quite well.

00:04:14.390 --> 00:04:16.130
Another thing that's
different, I think,

00:04:16.130 --> 00:04:18.410
is that if you think about
these past technologies,

00:04:18.410 --> 00:04:20.209
we've left most of them at home.

00:04:20.209 --> 00:04:22.640
So even if you
watched a lot of TV,

00:04:22.640 --> 00:04:24.380
that was bound to
a particular place.

00:04:24.380 --> 00:04:26.240
But now, we wear our devices.

00:04:26.240 --> 00:04:29.480
And so if you ask
people-- if you ask,

00:04:29.480 --> 00:04:32.492
again, adults in the developed
world, how much time do you

00:04:32.492 --> 00:04:33.450
spend with your phones?

00:04:33.450 --> 00:04:36.609
75% of adults will say
that 24 hours a day they

00:04:36.609 --> 00:04:38.150
can reach their
phones without having

00:04:38.150 --> 00:04:40.734
to move their feet, which
is not hugely surprising.

00:04:40.734 --> 00:04:42.650
But again, it suggests
there's something a bit

00:04:42.650 --> 00:04:44.483
different about what
we're experiencing now.

00:04:44.483 --> 00:04:46.754
It's not the pinball
machine at the arcade.

00:04:46.754 --> 00:04:48.920
It's the phone that's
actually almost a part of you.

00:04:48.920 --> 00:04:49.972
It's an extension of you.

00:04:49.972 --> 00:04:51.680
So I think there are
a number of features

00:04:51.680 --> 00:04:52.920
that make this different.

00:04:52.920 --> 00:04:54.920
We're also much more
sophisticated in developing

00:04:54.920 --> 00:04:56.240
technologies than we ever were.

00:04:56.240 --> 00:04:58.460
We understand human
psychology better.

00:04:58.460 --> 00:05:00.620
As a psychologist
who's consulted

00:05:00.620 --> 00:05:03.920
on a number of these projects,
I know how much we know,

00:05:03.920 --> 00:05:05.986
how much we understand
about what drives humans.

00:05:05.986 --> 00:05:07.610
So I think there are
some features that

00:05:07.610 --> 00:05:08.770
make this a bit
different from what

00:05:08.770 --> 00:05:10.150
we've experienced in the past.

00:05:10.150 --> 00:05:11.066
MAGGIE JACKSON: Right.

00:05:11.066 --> 00:05:13.570
The degree of
penetration, the fact

00:05:13.570 --> 00:05:16.290
that we're inhabiting
this in our--

00:05:16.290 --> 00:05:18.945
it's total as well
as the rapid change.

00:05:18.945 --> 00:05:20.900
I think that's a
really important point.

00:05:20.900 --> 00:05:26.200
So, Glen, in order to come out
with all of the new changes,

00:05:26.200 --> 00:05:28.720
you have to keep your
finger on the pulse.

00:05:28.720 --> 00:05:31.060
And tell us a little
bit about the research

00:05:31.060 --> 00:05:33.430
that, for instance, has
gone into the announcements

00:05:33.430 --> 00:05:37.320
this week related to
digital well-being.

00:05:37.320 --> 00:05:42.490
Are people feeling helpless in
their devices, or empowered,

00:05:42.490 --> 00:05:43.040
or both?

00:05:43.040 --> 00:05:43.790
GLEN MURPHY: Yeah.

00:05:43.790 --> 00:05:45.929
So it's a really
exciting time for us

00:05:45.929 --> 00:05:47.470
You probably saw
Sundar the other day

00:05:47.470 --> 00:05:50.830
talk about fear of missing out
and the joy of missing out.

00:05:50.830 --> 00:05:52.360
The joy of missing out--

00:05:52.360 --> 00:05:54.850
the title of the research
that we had done,

00:05:54.850 --> 00:05:57.320
and it was really interesting.

00:05:57.320 --> 00:05:59.725
We went out and we saw how
people engaged with technology

00:05:59.725 --> 00:06:00.266
in the world.

00:06:00.266 --> 00:06:03.022
And we saw many of the
things that we see,

00:06:03.022 --> 00:06:05.230
which is sort of once your
head is down in the phone,

00:06:05.230 --> 00:06:07.330
it's very easy to
sort of stay there.

00:06:07.330 --> 00:06:10.360
There are many things, many
exciting things in there

00:06:10.360 --> 00:06:11.410
that keep you in there.

00:06:11.410 --> 00:06:13.000
But I think the
other part that we

00:06:13.000 --> 00:06:14.920
saw that was really
interesting was just

00:06:14.920 --> 00:06:18.400
how social factors became
a large part of this.

00:06:18.400 --> 00:06:22.930
Unlike TV or even
desktop computing,

00:06:22.930 --> 00:06:26.530
which would generally--
relatively solitary things--

00:06:26.530 --> 00:06:29.440
a phone is an inherently
social object.

00:06:29.440 --> 00:06:31.480
And so one of the key
things that we saw

00:06:31.480 --> 00:06:33.250
was that even if
there was nothing

00:06:33.250 --> 00:06:36.520
for you to do on your phone,
you still kept it nearby,

00:06:36.520 --> 00:06:39.310
because there were other people
who were trying to reach you.

00:06:39.310 --> 00:06:43.014
And so we all became part
of this big mesh of humans

00:06:43.014 --> 00:06:43.930
contacting each other.

00:06:43.930 --> 00:06:46.480
And I think that was something
that we had underestimated

00:06:46.480 --> 00:06:51.079
in all of this-- the role
of each other in this area.

00:06:51.079 --> 00:06:52.370
So, yeah, it's really exciting.

00:06:52.370 --> 00:06:53.410
MAGGIE JACKSON: And I think--

00:06:53.410 --> 00:06:55.420
well, actually, when cell
phones first came out,

00:06:55.420 --> 00:06:58.930
I used to joke that
people should--

00:06:58.930 --> 00:07:00.460
who are buying them--

00:07:00.460 --> 00:07:02.070
maybe you haven't
heard of RadioShack.

00:07:02.070 --> 00:07:06.490
But we used to buy phones
at RadioShack that people--

00:07:06.490 --> 00:07:09.070
the guy at the store
should take you in the back

00:07:09.070 --> 00:07:11.620
and give you counseling,
just like the minister

00:07:11.620 --> 00:07:14.050
might when you get married.

00:07:14.050 --> 00:07:16.191
Here's what you need to know.

00:07:16.191 --> 00:07:17.940
So I think, yeah,
that's really important.

00:07:17.940 --> 00:07:21.610
And what about the
emotional factors

00:07:21.610 --> 00:07:23.380
related to these feedback loops?

00:07:23.380 --> 00:07:25.510
Can you just elaborate
a little bit about--

00:07:25.510 --> 00:07:28.150
because I know it's
really complex,

00:07:28.150 --> 00:07:30.140
and we are right in
the middle of it.

00:07:30.140 --> 00:07:32.320
And it reminds me also of books.

00:07:32.320 --> 00:07:35.080
When you're reading
a book, the content

00:07:35.080 --> 00:07:37.300
is where your attention lies.

00:07:37.300 --> 00:07:41.740
But there is a technology
around it that affects you.

00:07:41.740 --> 00:07:43.790
It's a lot less static
than people think.

00:07:43.790 --> 00:07:47.320
So what are we not
aware of in terms

00:07:47.320 --> 00:07:49.180
of our emotional responses?

00:07:49.180 --> 00:07:54.220
And were you surprised
at the myriad responses

00:07:54.220 --> 00:07:55.190
that people had?

00:07:55.190 --> 00:07:56.740
GLEN MURPHY: Yeah.

00:07:56.740 --> 00:07:58.750
This whole space is
really interesting,

00:07:58.750 --> 00:08:01.690
because there are some deeply
individual responses in here.

00:08:01.690 --> 00:08:05.020
I think it's really hard to
draw broad generalizations,

00:08:05.020 --> 00:08:08.020
but the feedback thing, I
think, is really interesting.

00:08:08.020 --> 00:08:11.170
Adam talks a lot about
this-- actually in your book

00:08:11.170 --> 00:08:14.170
about how we use these
devices, and they

00:08:14.170 --> 00:08:16.300
are immensely useful to us.

00:08:16.300 --> 00:08:17.356
They've replaced maps.

00:08:17.356 --> 00:08:18.355
They've replaced clocks.

00:08:18.355 --> 00:08:20.140
They've replaced
writing letters.

00:08:20.140 --> 00:08:21.650
They've replaced
all these things.

00:08:21.650 --> 00:08:26.100
And you do those actions,
and you feel good.

00:08:26.100 --> 00:08:29.230
They're performing an
essential utility in your life.

00:08:29.230 --> 00:08:31.720
But then it's sort of very
easy for those interactions

00:08:31.720 --> 00:08:34.510
to then bleed into
other behaviors that

00:08:34.510 --> 00:08:37.059
are maybe less useful, or
maybe they're entertaining.

00:08:37.059 --> 00:08:38.956
And then you can
kind of keep going,

00:08:38.956 --> 00:08:40.789
because there is this
feedback loop of like,

00:08:40.789 --> 00:08:41.830
oh, just one more thing.

00:08:41.830 --> 00:08:45.940
And so that's why we think
about how a lot of that

00:08:45.940 --> 00:08:50.350
is very invisible to the user.

00:08:50.350 --> 00:08:52.330
I think you've talked
elsewhere about how

00:08:52.330 --> 00:08:53.950
sometimes our own
use of technology

00:08:53.950 --> 00:08:55.810
can be very invisible to us.

00:08:55.810 --> 00:08:58.960
And so a key thing for us is
how we can provide awareness

00:08:58.960 --> 00:08:59.950
to people.

00:08:59.950 --> 00:09:03.180
I don't think it's a total
number of hours thing--

00:09:03.180 --> 00:09:04.500
or that's a good indicator.

00:09:04.500 --> 00:09:07.510
It's how you felt
about that time.

00:09:07.510 --> 00:09:10.630
And I think, often we
find when we show people

00:09:10.630 --> 00:09:12.700
how much they were
actually engaging,

00:09:12.700 --> 00:09:13.840
they're often surprised.

00:09:13.840 --> 00:09:16.105
And I think that's a really
interesting component.

00:09:16.105 --> 00:09:17.980
MAGGIE JACKSON: Yeah,
a reflective component.

00:09:17.980 --> 00:09:19.120
ADAM ALTER: It's
also interesting.

00:09:19.120 --> 00:09:21.130
There are sort of scripts
that different people follow.

00:09:21.130 --> 00:09:22.120
I know I have one.

00:09:22.120 --> 00:09:24.490
When I open my phone, there's
a certain set of apps.

00:09:24.490 --> 00:09:27.197
And I'll go in this loop,
back to the same one again,

00:09:27.197 --> 00:09:28.030
and then I'll start.

00:09:28.030 --> 00:09:29.860
I'll go through five or
six of them and go back.

00:09:29.860 --> 00:09:31.234
And if there's
something, you get

00:09:31.234 --> 00:09:33.902
into a sort of lulled sense
of calm when you do that.

00:09:33.902 --> 00:09:35.485
And that's, I think,
a really big part

00:09:35.485 --> 00:09:37.955
of driving you to keep
engaging over and over again.

00:09:37.955 --> 00:09:38.830
MAGGIE JACKSON: Yeah.

00:09:38.830 --> 00:09:40.360
I think backstage
we were talking

00:09:40.360 --> 00:09:43.180
about the automaticity
of the human being

00:09:43.180 --> 00:09:46.510
as being such a crucial
factor in everything we do.

00:09:46.510 --> 00:09:47.830
It underlays expertise.

00:09:47.830 --> 00:09:50.660
We have to do things without
really thinking about them.

00:09:50.660 --> 00:09:53.770
On the other hand,
we have to be aware.

00:09:53.770 --> 00:09:58.270
We evolve to be as reflective
as we did to be automatic,

00:09:58.270 --> 00:09:59.690
and that's really important.

00:09:59.690 --> 00:10:02.080
So we're kind of
learning as we go, and we

00:10:02.080 --> 00:10:04.240
have these conflicted senses.

00:10:04.240 --> 00:10:07.120
And I think we don't have
the option to get it wrong.

00:10:07.120 --> 00:10:10.850
Honestly, I think we need to
have technology and humanity

00:10:10.850 --> 00:10:15.800
come together in better ways
to make us more compassionate,

00:10:15.800 --> 00:10:17.660
better discerning thinkers.

00:10:17.660 --> 00:10:20.840
So let's turn to the
crucial first steps.

00:10:20.840 --> 00:10:22.610
Again, Glen, maybe
you can kick it off

00:10:22.610 --> 00:10:26.060
by talking about the
Android announcements.

00:10:26.060 --> 00:10:29.990
And I thought it was interesting
that some keywords that you

00:10:29.990 --> 00:10:31.880
had mentioned in our
preliminary talks

00:10:31.880 --> 00:10:33.650
were control and awareness.

00:10:33.650 --> 00:10:34.760
You mentioned awareness.

00:10:34.760 --> 00:10:40.460
But how are some of the
wind down and shush going

00:10:40.460 --> 00:10:42.600
to give people more control?

00:10:42.600 --> 00:10:46.340
And do you have a sense that,
how effective will this be?

00:10:46.340 --> 00:10:48.090
Or is it a question mark?

00:10:48.090 --> 00:10:49.760
GLEN MURPHY: Yeah.

00:10:49.760 --> 00:10:53.980
So the Android announcements
that we announced the other day

00:10:53.980 --> 00:10:55.910
were oriented around
awareness and control,

00:10:55.910 --> 00:11:00.410
giving people visibility into
what they're actually doing.

00:11:00.410 --> 00:11:03.260
So they can be aware
of what's happening.

00:11:03.260 --> 00:11:05.120
And then giving
them the tools to be

00:11:05.120 --> 00:11:07.190
able to change that
behavior if they so choose.

00:11:07.190 --> 00:11:09.080
And I think it's
really been interesting

00:11:09.080 --> 00:11:11.163
for us as we've approached
this space and thinking

00:11:11.163 --> 00:11:15.290
about the right ways to handle
the responsibility that we

00:11:15.290 --> 00:11:16.790
have.

00:11:16.790 --> 00:11:19.130
In some ways, we have
been thinking about,

00:11:19.130 --> 00:11:21.476
where should that
regulation lie?

00:11:21.476 --> 00:11:23.100
Should it be
technologically regulated?

00:11:23.100 --> 00:11:24.830
And we sort of came down to--

00:11:24.830 --> 00:11:27.380
many of these things start
with self-regulation.

00:11:27.380 --> 00:11:30.920
I think in many ways
that feeling of control

00:11:30.920 --> 00:11:32.820
is really important to people.

00:11:32.820 --> 00:11:36.632
And so we haven't provided that
in the platform historically.

00:11:36.632 --> 00:11:38.090
There's little bits
here and there.

00:11:38.090 --> 00:11:40.215
But we want to take that
further and to give people

00:11:40.215 --> 00:11:42.189
that feeling of
awareness and control,

00:11:42.189 --> 00:11:44.480
so that as people are having
these feelings about using

00:11:44.480 --> 00:11:46.670
their phones, they
have the ability

00:11:46.670 --> 00:11:49.010
to change how they're using it.

00:11:49.010 --> 00:11:51.620
And to your question about
whether all of these things

00:11:51.620 --> 00:11:56.180
will work, I think this
is an ongoing thing.

00:11:56.180 --> 00:11:59.910
We were talking backstage
about how many of the things

00:11:59.910 --> 00:12:02.540
that we all use in our
life, like to-do lists

00:12:02.540 --> 00:12:03.920
and other things.

00:12:03.920 --> 00:12:06.260
Often they are effective
for some amount of time,

00:12:06.260 --> 00:12:09.110
and then we kind of
adapt, and then we sort of

00:12:09.110 --> 00:12:10.455
find other ways to do things.

00:12:10.455 --> 00:12:13.880
And so I expect this to be a
long-running effort for us.

00:12:13.880 --> 00:12:17.420
As we learn, it might
be that we encounter

00:12:17.420 --> 00:12:20.270
sort of different cultures
that have different attitudes

00:12:20.270 --> 00:12:22.010
towards the phone.

00:12:22.010 --> 00:12:24.890
And how we deal with that
is really interesting.

00:12:24.890 --> 00:12:27.800
Even going back to the
pre-smartphone era,

00:12:27.800 --> 00:12:30.020
we found that there
were many places where

00:12:30.020 --> 00:12:33.320
the attitude to dealing
with a ringing phone

00:12:33.320 --> 00:12:34.266
was very different.

00:12:34.266 --> 00:12:36.140
In some places, you
always have to answer it,

00:12:36.140 --> 00:12:38.098
regardless of what you
were doing, and others--

00:12:38.098 --> 00:12:40.440
it was OK to let it ring out.

00:12:40.440 --> 00:12:42.500
And so that sort
of responsibility

00:12:42.500 --> 00:12:45.380
of respecting the cultural
norms and helping people

00:12:45.380 --> 00:12:47.950
think about those cultural
norms is sort of where we're at.

00:12:47.950 --> 00:12:48.825
MAGGIE JACKSON: Yeah.

00:12:48.825 --> 00:12:50.600
Well, I used to think
about attention,

00:12:50.600 --> 00:12:53.480
which is a multi-faceted--

00:12:53.480 --> 00:12:54.905
scientists used
to ask me, when I

00:12:54.905 --> 00:12:57.620
was first studying attention,
the trick question of,

00:12:57.620 --> 00:12:59.270
what kind of
attention do you mean?

00:12:59.270 --> 00:13:01.840
Because it's a messy, big thing.

00:13:01.840 --> 00:13:03.310
But I used to think of it--

00:13:03.310 --> 00:13:06.530
I think of it now as
arrows in one's quiver.

00:13:06.530 --> 00:13:07.880
You can use focus.

00:13:07.880 --> 00:13:09.170
You can use awareness.

00:13:09.170 --> 00:13:10.880
You can use executive attention.

00:13:10.880 --> 00:13:13.940
But they're tools just
as perhaps these tools.

00:13:13.940 --> 00:13:15.650
And I think that
your announcements

00:13:15.650 --> 00:13:19.640
are a very, very, big deal.

00:13:19.640 --> 00:13:21.860
I do have to say that--

00:13:21.860 --> 00:13:28.220
I mean, the mantra or the
overarching societal push

00:13:28.220 --> 00:13:31.160
has been towards seamlessness.

00:13:31.160 --> 00:13:33.600
I mean, that's been
an unbroken thing.

00:13:33.600 --> 00:13:38.810
You must engage, or it's
beneficial to engage.

00:13:38.810 --> 00:13:42.890
To break that at all
is really momentous.

00:13:42.890 --> 00:13:45.920
And I think that that's
a huge step forward.

00:13:45.920 --> 00:13:50.270
As a very critical
person about technology,

00:13:50.270 --> 00:13:51.790
I think it's a big step forward.

00:13:51.790 --> 00:13:52.706
GLEN MURPHY: I think--

00:13:52.706 --> 00:13:53.570
sorry, just on that.

00:13:53.570 --> 00:13:56.870
I think it is really something
that's interesting for everyone

00:13:56.870 --> 00:13:59.690
here who is a developer to
think about-- is that we

00:13:59.690 --> 00:14:02.960
have been focused on
immediacy and seamlessness

00:14:02.960 --> 00:14:03.770
for a long time.

00:14:03.770 --> 00:14:06.500
We clearly know that
that's a key aspect

00:14:06.500 --> 00:14:08.480
to sort of empowering people.

00:14:08.480 --> 00:14:12.890
But thinking about that
in a social context,

00:14:12.890 --> 00:14:16.850
creating something that is
immediate for you as a sender

00:14:16.850 --> 00:14:19.490
also creates something on the
other side for the receiver.

00:14:19.490 --> 00:14:22.340
And so as we've developed
all of these things that

00:14:22.340 --> 00:14:25.970
are very immediate, there is
another side of the equation

00:14:25.970 --> 00:14:27.405
that we've got to think about.

00:14:27.405 --> 00:14:28.280
MAGGIE JACKSON: Yeah.

00:14:28.280 --> 00:14:30.330
And I think that's important.

00:14:30.330 --> 00:14:30.860
Go ahead.

00:14:30.860 --> 00:14:31.060
ADAM ALTER: Yeah.

00:14:31.060 --> 00:14:32.730
No, I totally agree with that.

00:14:32.730 --> 00:14:35.740
I think there's always
going to be this sort of--

00:14:35.740 --> 00:14:38.052
not the backlash, but
the other side of that,

00:14:38.052 --> 00:14:41.150
that the quicker the transfer
becomes in the one direction,

00:14:41.150 --> 00:14:43.287
the quicker you'll
expect to get the reply.

00:14:43.287 --> 00:14:44.370
GLEN MURPHY: That's right.

00:14:44.370 --> 00:14:47.180
ADAM ALTER: It's a
really interesting idea.

00:14:47.180 --> 00:14:52.100
And also, this general idea of
the role of frictionlessness

00:14:52.100 --> 00:14:54.200
and the absence of pain points.

00:14:54.200 --> 00:14:56.034
We talk about this all
the time in business.

00:14:56.034 --> 00:14:57.575
I'm a professor in
a business school.

00:14:57.575 --> 00:14:59.060
And one of the
mantras in business

00:14:59.060 --> 00:15:00.655
is make everything as
simple as possible.

00:15:00.655 --> 00:15:02.330
One of the best ways
to spend your money

00:15:02.330 --> 00:15:05.060
is to remove pain points,
so friction points.

00:15:05.060 --> 00:15:07.190
And that makes total
sense, because that's

00:15:07.190 --> 00:15:09.830
the best way to ensure not just
engagement for five minutes,

00:15:09.830 --> 00:15:13.830
but for sometimes weeks,
years, even decades.

00:15:13.830 --> 00:15:17.240
And I think we've now
pushed that to a point where

00:15:17.240 --> 00:15:20.220
we're starting to get the
initial backlash, and that's--

00:15:20.220 --> 00:15:23.720
it's great that
these new introduced

00:15:23.720 --> 00:15:26.716
tools allow people to manage
that a little bit, to introduce

00:15:26.716 --> 00:15:28.340
frictions where they
might be valuable.

00:15:28.340 --> 00:15:29.150
GLEN MURPHY: That's right.

00:15:29.150 --> 00:15:30.025
MAGGIE JACKSON: Yeah.

00:15:30.025 --> 00:15:33.710
And I also think that
it's really important

00:15:33.710 --> 00:15:38.870
to think about the idea
of disconnection, which

00:15:38.870 --> 00:15:42.020
might be a change in
the tempo or the rhythm

00:15:42.020 --> 00:15:44.610
that allows you
to gain friction,

00:15:44.610 --> 00:15:47.960
but to think about that
in a way that's not just

00:15:47.960 --> 00:15:49.190
in an on and off way.

00:15:49.190 --> 00:15:52.610
So in other words, maybe
we can open up the space

00:15:52.610 --> 00:15:55.100
to these questions, so
we're not just thinking

00:15:55.100 --> 00:16:00.680
about the little pause points
as being away from technology,

00:16:00.680 --> 00:16:05.585
but importing that discussion
into the very digital space.

00:16:05.585 --> 00:16:08.510
And I think about that
currently in relation

00:16:08.510 --> 00:16:11.360
to the idea of technology--

00:16:11.360 --> 00:16:15.590
I mean, I'm sorry, in relation
to the idea of uncertainty.

00:16:15.590 --> 00:16:18.200
Uncertainty seems like
a very negative thing,

00:16:18.200 --> 00:16:24.680
but just consider one of
the unintended consequences

00:16:24.680 --> 00:16:28.340
of having answers at your
fingertips all the time--

00:16:28.340 --> 00:16:30.500
this instantaneity,
which actually came in,

00:16:30.500 --> 00:16:32.390
in the 17th, 18th, 19th century.

00:16:32.390 --> 00:16:34.910
I mean, this has just
been rolling along

00:16:34.910 --> 00:16:35.980
for hundreds of years.

00:16:35.980 --> 00:16:37.430
Efficiency is it.

00:16:37.430 --> 00:16:39.440
But now, there are
studies that are showing

00:16:39.440 --> 00:16:41.630
a little bit of
hints here and there

00:16:41.630 --> 00:16:45.120
about what it means to have
that instantaneity all the time.

00:16:45.120 --> 00:16:47.900
For instance, after
briefly searching online,

00:16:47.900 --> 00:16:52.730
people are seen to be
less willing to solve--

00:16:52.730 --> 00:16:56.270
to tackle a complex
problem later.

00:16:56.270 --> 00:16:58.190
After briefly
searching online, they

00:16:58.190 --> 00:17:00.880
become a little bit
more confident--

00:17:00.880 --> 00:17:03.740
overconfident, I mean,
about what they know.

00:17:03.740 --> 00:17:06.890
They don't really know what
they know and don't know.

00:17:06.890 --> 00:17:09.990
And I think these are really
important hints, because--

00:17:09.990 --> 00:17:11.599
and why does this matter?

00:17:11.599 --> 00:17:14.030
I think it matters
because there is

00:17:14.030 --> 00:17:18.200
that side of ourselves that's
automatic-- the quick mind,

00:17:18.200 --> 00:17:22.579
thrives in the routine,
needs instant answers,

00:17:22.579 --> 00:17:24.290
does really well
in the familiar.

00:17:24.290 --> 00:17:29.270
But then if we exist on
a steady diet of the meat

00:17:29.270 --> 00:17:31.580
in the package, then the
downloadable and the instant

00:17:31.580 --> 00:17:35.630
all the time, we're
beginning to possibly forget

00:17:35.630 --> 00:17:37.460
to tap into the other
side of ourselves--

00:17:37.460 --> 00:17:40.640
the reflective, the
questioning, the skeptical,

00:17:40.640 --> 00:17:42.890
the so-called slow mind.

00:17:42.890 --> 00:17:46.430
And in fact, there's a
really exciting new explosion

00:17:46.430 --> 00:17:50.900
of research into various forms
of uncertainty as actually

00:17:50.900 --> 00:17:53.060
connected to good thinking.

00:17:53.060 --> 00:17:55.670
For instance-- and this
really does, I think,

00:17:55.670 --> 00:17:57.890
connect to technology
and well-being.

00:17:57.890 --> 00:18:01.850
But ambivalence in
CEOs in a crisis

00:18:01.850 --> 00:18:05.120
is shown to induce them
to do deeper research,

00:18:05.120 --> 00:18:07.040
to look at multiple
perspectives,

00:18:07.040 --> 00:18:10.670
to include more voices, and
then to take bolder action.

00:18:10.670 --> 00:18:11.390
Wow.

00:18:11.390 --> 00:18:14.540
And then confusion in students--

00:18:14.540 --> 00:18:17.600
again, people have always
thought the easy lesson--

00:18:17.600 --> 00:18:21.320
that what's good, easy to
digest is better for learning.

00:18:21.320 --> 00:18:24.260
But it's shown in
complex situations

00:18:24.260 --> 00:18:28.430
that confusion relates--

00:18:28.430 --> 00:18:31.820
leads to deeper
processing in students.

00:18:31.820 --> 00:18:36.260
So these sort of factors
fly against a lot

00:18:36.260 --> 00:18:38.430
of what our society values.

00:18:38.430 --> 00:18:43.730
And I think that, why not
develop a digital world where

00:18:43.730 --> 00:18:47.510
the human is more inspired
to say, oh, I don't know?

00:18:47.510 --> 00:18:53.360
Or to pause and then jack into
that kind of discerning self

00:18:53.360 --> 00:18:54.200
that they have.

00:18:54.200 --> 00:18:58.520
And I'll just leave this
point with one other piece

00:18:58.520 --> 00:19:01.970
of research, which is that when
people in the United States

00:19:01.970 --> 00:19:05.540
are surveyed and asked, what's
positive about technology?

00:19:05.540 --> 00:19:11.450
60% say speed, instant access
to information, convenience.

00:19:11.450 --> 00:19:16.120
But just 10% say a
positive about technology

00:19:16.120 --> 00:19:20.570
is that it promotes knowledge,
understanding, and curiosity.

00:19:20.570 --> 00:19:23.210
So that's a really
big disconnect.

00:19:23.210 --> 00:19:27.320
And if we could propel our
other side of ourselves

00:19:27.320 --> 00:19:29.960
into that space with
developers' help,

00:19:29.960 --> 00:19:31.670
I think that would
be really powerful.

00:19:31.670 --> 00:19:35.806
And it has everything to do with
all the hardship, et cetera.

00:19:35.806 --> 00:19:36.760
ADAM ALTER: Right.

00:19:36.760 --> 00:19:38.540
Yeah.

00:19:38.540 --> 00:19:42.280
If you think about the
way we stored information,

00:19:42.280 --> 00:19:45.440
say, in the early '80s
when I was growing up,

00:19:45.440 --> 00:19:48.370
I probably knew 100
phone numbers by heart,

00:19:48.370 --> 00:19:51.080
and that seemed straightforward.

00:19:51.080 --> 00:19:52.830
And I think all kids
at that time did.

00:19:52.830 --> 00:19:54.500
They knew the-- you
knew your friend's.

00:19:54.500 --> 00:19:56.333
You knew your family
members' phone numbers.

00:19:56.333 --> 00:19:57.770
You knew some other ones.

00:19:57.770 --> 00:20:00.380
And what that did was it meant
that certain memory faculties

00:20:00.380 --> 00:20:02.810
were always being used,
and they were being trained

00:20:02.810 --> 00:20:04.190
without your even realizing it.

00:20:04.190 --> 00:20:06.530
And there's a lot of
research now extending

00:20:06.530 --> 00:20:09.130
what you're suggesting
into a concept known

00:20:09.130 --> 00:20:10.600
as hardship inoculation.

00:20:10.600 --> 00:20:14.680
And it's the idea that if you
use your mind in certain ways,

00:20:14.680 --> 00:20:17.585
if you think about things in
certain ways in small doses,

00:20:17.585 --> 00:20:19.210
you become much better
prepared to deal

00:20:19.210 --> 00:20:23.110
with major tackles, major
problems, major creativity

00:20:23.110 --> 00:20:23.610
hurdles.

00:20:23.610 --> 00:20:25.776
Times when the way you've
been thinking about things

00:20:25.776 --> 00:20:27.490
in a traditional
sense are not going

00:20:27.490 --> 00:20:29.410
to work-- how do
you get around that?

00:20:29.410 --> 00:20:31.480
Turns out, having those
100 numbers in your head,

00:20:31.480 --> 00:20:32.710
training your memory--

00:20:32.710 --> 00:20:35.290
that also applies to how we
use our minds in other ways.

00:20:35.290 --> 00:20:38.440
And so when everything
is absolutely seamless

00:20:38.440 --> 00:20:40.300
and we don't have
to memorize, we

00:20:40.300 --> 00:20:42.320
don't have to try to think about
what we're going to do next.

00:20:42.320 --> 00:20:43.861
Because we have a
device that's going

00:20:43.861 --> 00:20:46.600
to deliver entertainment
in the next three seconds

00:20:46.600 --> 00:20:47.980
if that's what we want.

00:20:47.980 --> 00:20:50.380
I think those faculties
wither away a little bit.

00:20:50.380 --> 00:20:54.670
And so it's important to have
occasions when we do use them,

00:20:54.670 --> 00:20:57.280
even in a low-grade sense, just
so that they're kept alive.

00:20:57.280 --> 00:20:59.060
It's like a muscle
that's withering away.

00:20:59.060 --> 00:21:00.476
So you want to
make sure that life

00:21:00.476 --> 00:21:02.890
isn't so straightforward and
seamless that we're always

00:21:02.890 --> 00:21:05.962
our automatic selves and
never our reflective selves.

00:21:05.962 --> 00:21:08.170
GLEN MURPHY: That reminds
me a lot of the discussions

00:21:08.170 --> 00:21:12.130
about discipline and how that
sort of leads to developments

00:21:12.130 --> 00:21:13.480
later in people's lives.

00:21:13.480 --> 00:21:15.190
But I think a key
takeaway for me

00:21:15.190 --> 00:21:19.960
is thinking about how technology
in many ways can be used to--

00:21:19.960 --> 00:21:22.580
I think the ideal is-- and
in many ways it's true.

00:21:22.580 --> 00:21:24.790
It helps us become more
active in other areas.

00:21:24.790 --> 00:21:26.830
As we make some
things easier, that

00:21:26.830 --> 00:21:28.990
allows us to devote
time and attention

00:21:28.990 --> 00:21:30.560
to the things that matter.

00:21:30.560 --> 00:21:32.230
And I think the danger is that--

00:21:32.230 --> 00:21:35.020
where we don't elevate
ourselves in that way.

00:21:35.020 --> 00:21:38.200
And we can end up
becoming purely passive.

00:21:38.200 --> 00:21:39.220
There's plenty of time.

00:21:39.220 --> 00:21:41.170
There are times for
that in people's lives.

00:21:41.170 --> 00:21:42.670
I was in-- hospital
for a week once.

00:21:42.670 --> 00:21:44.440
And I was very passive
for the entire--

00:21:44.440 --> 00:21:46.490
and we can end up
becoming [AUDIO OUT]..

00:22:14.480 --> 00:22:16.850
MAGGIE JACKSON: So I
think we're exploring

00:22:16.850 --> 00:22:19.730
how to get a grip
on this landscape.

00:22:19.730 --> 00:22:24.200
We're also talking about
well-being, which is a very--

00:22:24.200 --> 00:22:26.240
also, messy, ill-defined topic.

00:22:26.240 --> 00:22:28.170
But we've talked about control.

00:22:28.170 --> 00:22:29.420
We're talking about awareness.

00:22:29.420 --> 00:22:32.510
We're talking about
hardship and uncertainty.

00:22:32.510 --> 00:22:37.080
I mean, these motifs
or notions plug

00:22:37.080 --> 00:22:45.410
into psychological ideas about
well-being, mastery, control,

00:22:45.410 --> 00:22:50.930
accomplishment as well as social
relationships, and health.

00:22:50.930 --> 00:22:53.140
Those are really
important components.

00:22:53.140 --> 00:22:58.630
I mean, what would you
think digital well-being--

00:22:58.630 --> 00:23:02.060
what would it look like if the
United Nations in five years

00:23:02.060 --> 00:23:02.680
was--

00:23:02.680 --> 00:23:07.970
or this-- the kingdom of Bhutan
came out with the new GDP

00:23:07.970 --> 00:23:10.530
for digital--

00:23:10.530 --> 00:23:11.320
well, DWB?

00:23:14.960 --> 00:23:18.120
We can all scale mountains there
and see what it feels like.

00:23:18.120 --> 00:23:21.081
But what do you think it
should look like, Glen?

00:23:21.081 --> 00:23:21.830
GLEN MURPHY: Yeah.

00:23:21.830 --> 00:23:26.180
That's a super deep question.

00:23:26.180 --> 00:23:31.220
I do think that there
may not be a global view.

00:23:31.220 --> 00:23:31.970
There may not be--

00:23:31.970 --> 00:23:32.170
MAGGIE JACKSON:
That's a good point.

00:23:32.170 --> 00:23:33.650
GLEN MURPHY: --a
country-wide view.

00:23:33.650 --> 00:23:39.505
And I think that the thing we
grapple with is, at what level

00:23:39.505 --> 00:23:41.510
is there a shared view?

00:23:41.510 --> 00:23:44.120
Is it even at the
friends' level?

00:23:44.120 --> 00:23:45.640
Is it at the family level?

00:23:45.640 --> 00:23:49.490
Is it different within
different social contexts?

00:23:49.490 --> 00:23:51.590
We talk about use of technology.

00:23:51.590 --> 00:23:53.930
And it can mean--
the exact same usage

00:23:53.930 --> 00:23:55.900
can mean different things
to different people.

00:23:55.900 --> 00:24:01.660
It can lead to a different
sense of satisfaction.

00:24:01.660 --> 00:24:05.990
I think there are key metrics
about whether people feel

00:24:05.990 --> 00:24:10.340
that their use of technology
was overall beneficial to them,

00:24:10.340 --> 00:24:12.320
whether they derived
value from it.

00:24:12.320 --> 00:24:14.090
And I think it's--

00:24:14.090 --> 00:24:17.180
there's a lot of debate about
what that value should be.

00:24:17.180 --> 00:24:19.436
I think we all impose our
own values upon the world

00:24:19.436 --> 00:24:20.810
as we think through
these things.

00:24:20.810 --> 00:24:23.000
And I think it is--

00:24:23.000 --> 00:24:26.060
for some people, the value
is you produced something

00:24:26.060 --> 00:24:26.770
for others.

00:24:26.770 --> 00:24:28.760
It's you develop
social relations.

00:24:28.760 --> 00:24:31.280
For others the value can
just be pure entertainment,

00:24:31.280 --> 00:24:32.210
and I think that's OK.

00:24:35.000 --> 00:24:39.460
But as long as you are artful
and intentional about it

00:24:39.460 --> 00:24:41.870
and you feel that
you're in control of it,

00:24:41.870 --> 00:24:43.962
I think that's what
well-being looks like.

00:24:43.962 --> 00:24:45.170
MAGGIE JACKSON: That's great.

00:24:45.170 --> 00:24:46.337
Do you have any flourishing?

00:24:46.337 --> 00:24:47.295
What does it look like?

00:24:47.295 --> 00:24:48.060
ADAM ALTER: Yeah.

00:24:48.060 --> 00:24:50.690
No, I think it's interesting,
because science generally

00:24:50.690 --> 00:24:51.980
resists subjectivity.

00:24:51.980 --> 00:24:52.925
You want a test.

00:24:52.925 --> 00:24:54.050
You can give people a test.

00:24:54.050 --> 00:24:54.884
There are 12 items.

00:24:54.884 --> 00:24:55.550
You answer them.

00:24:55.550 --> 00:24:57.758
You add up the score, and
you can say your well-being

00:24:57.758 --> 00:24:59.270
level is average.

00:24:59.270 --> 00:25:00.729
MAGGIE JACKSON: 4.2?

00:25:00.729 --> 00:25:01.770
ADAM ALTER: You're a 4.2.

00:25:01.770 --> 00:25:02.930
Yeah, exactly.

00:25:02.930 --> 00:25:05.092
And I think that
what's been interesting

00:25:05.092 --> 00:25:06.800
for me in studying
this topic for a while

00:25:06.800 --> 00:25:09.080
is that people have a really
good sense subjectively, when

00:25:09.080 --> 00:25:11.580
they introspect about how they
feel about their relationship

00:25:11.580 --> 00:25:12.440
to technology.

00:25:12.440 --> 00:25:15.260
And they're all using very
different metrics as Glen said.

00:25:15.260 --> 00:25:18.140
So for some, it's about
social relationships

00:25:18.140 --> 00:25:19.120
and facilitating that.

00:25:19.120 --> 00:25:20.453
And that, for me, is a big part.

00:25:20.453 --> 00:25:23.330
I live, as I imagine
you do, a long way

00:25:23.330 --> 00:25:25.127
from a big part of my family.

00:25:25.127 --> 00:25:27.710
And so technology has allowed
me to connect with them in a way

00:25:27.710 --> 00:25:29.970
that is very, very enriching.

00:25:29.970 --> 00:25:31.324
So that for me is a key factor.

00:25:31.324 --> 00:25:33.490
For other people, that might
not be a factor at all.

00:25:33.490 --> 00:25:35.906
Either you live near the people
that really matter to you,

00:25:35.906 --> 00:25:39.230
or it's more important to you
to create, or to automatize,

00:25:39.230 --> 00:25:40.525
or whatever it may be.

00:25:40.525 --> 00:25:43.190
And so I think there
is no one definition.

00:25:43.190 --> 00:25:46.070
And I think it'll be very hard
to find just one definition.

00:25:46.070 --> 00:25:47.780
But I think the best
thing we can all do

00:25:47.780 --> 00:25:50.600
is have a list of
questions to ask ourselves.

00:25:50.600 --> 00:25:52.340
And the answers
to that, I think,

00:25:52.340 --> 00:25:54.830
will illuminate for
each of us individually,

00:25:54.830 --> 00:25:58.220
whether we've got a high or low
level of digital well-being.

00:25:58.220 --> 00:26:00.080
And often for me,
digital well-being

00:26:00.080 --> 00:26:04.240
comes from augmenting our
real non-digital experiences.

00:26:04.240 --> 00:26:05.570
That's much of it for me.

00:26:05.570 --> 00:26:07.310
So I like social connection.

00:26:07.310 --> 00:26:09.320
I get more of that social
connection and better

00:26:09.320 --> 00:26:11.660
social connection with people
I couldn't have it with,

00:26:11.660 --> 00:26:13.317
using digital tools.

00:26:13.317 --> 00:26:14.400
I want to know where I am.

00:26:14.400 --> 00:26:16.460
I can do it better
with a map on a device.

00:26:16.460 --> 00:26:18.502
All of that, I think, is
a really big part of it.

00:26:18.502 --> 00:26:19.418
MAGGIE JACKSON: Right.

00:26:19.418 --> 00:26:20.930
And there will be trade-offs.

00:26:20.930 --> 00:26:22.790
In New York City, it
used to be that you'd

00:26:22.790 --> 00:26:25.090
turn to someone on the
street and say, where am I?

00:26:25.090 --> 00:26:27.160
And I'm very
directionally challenged.

00:26:27.160 --> 00:26:28.950
Sometimes I'm even
math challenged.

00:26:28.950 --> 00:26:30.334
So I turn to human beings.

00:26:30.334 --> 00:26:32.000
And they look at me
as though I'm crazy.

00:26:32.000 --> 00:26:32.905
Where's your phone?

00:26:32.905 --> 00:26:35.636
They really look, and they
don't know where they are.

00:26:35.636 --> 00:26:37.010
They have no idea
where they are.

00:26:37.010 --> 00:26:39.350
So there are really good--

00:26:39.350 --> 00:26:40.700
there are so many trade-offs.

00:26:40.700 --> 00:26:43.490
It'll keep us guessing
for a long, long time.

00:26:43.490 --> 00:26:47.767
Well, I think that I had
an interesting situation

00:26:47.767 --> 00:26:49.850
with a friend that I'm
still trying to figure out,

00:26:49.850 --> 00:26:52.760
where I sent him an article,
but instead of answering

00:26:52.760 --> 00:26:54.322
that he had gotten it--

00:26:54.322 --> 00:26:56.030
it was something very
close to my heart--

00:26:56.030 --> 00:27:00.050
an article I had written about
tolerance in today's culture.

00:27:00.050 --> 00:27:03.080
But he waited, and
waited, and waited.

00:27:03.080 --> 00:27:08.251
And he was naturally going
to read it and then get back

00:27:08.251 --> 00:27:08.750
to me.

00:27:08.750 --> 00:27:13.970
But I felt like, wow, can't
you just say you got it?

00:27:13.970 --> 00:27:16.254
And so it was really--

00:27:16.254 --> 00:27:17.420
I just throw that out there.

00:27:17.420 --> 00:27:18.410
There are no answers.

00:27:18.410 --> 00:27:21.560
But I think that
acknowledgement,

00:27:21.560 --> 00:27:23.930
communications-- maybe
this will keep us all--

00:27:23.930 --> 00:27:28.310
it should keep us on our toes
in a better and better way.

00:27:28.310 --> 00:27:31.190
GLEN MURPHY: I think that
exact case was something

00:27:31.190 --> 00:27:34.400
that we saw in our own research
that was deeply surprising.

00:27:34.400 --> 00:27:38.030
Going back to the discussion of
technology being a social tool,

00:27:38.030 --> 00:27:40.820
I think one of the
aspects of it that we

00:27:40.820 --> 00:27:44.330
found really surprising
was expectations

00:27:44.330 --> 00:27:48.530
of how we use it with each
other haven't really been set.

00:27:48.530 --> 00:27:50.780
I think with
telephones, we kind of

00:27:50.780 --> 00:27:53.311
created a culture, like
don't call after this time,

00:27:53.311 --> 00:27:54.560
because we're probably asleep.

00:27:54.560 --> 00:27:56.210
It's going to ring in the house.

00:27:56.210 --> 00:27:58.760
I feel that-- and
we saw this that--

00:27:58.760 --> 00:28:00.620
we saw many of those
situations where

00:28:00.620 --> 00:28:05.840
someone would send a message,
and they would not get a reply,

00:28:05.840 --> 00:28:08.430
and they didn't know
what that meant.

00:28:08.430 --> 00:28:14.030
They didn't know if the other
person was hurt or hated them.

00:28:14.030 --> 00:28:17.360
We saw someone who-- it took her
three hours to get back to her

00:28:17.360 --> 00:28:19.700
friend--

00:28:19.700 --> 00:28:22.040
some amount of time,
because she was busy.

00:28:22.040 --> 00:28:25.970
And when they finally
got in contact,

00:28:25.970 --> 00:28:28.580
the friend was very upset
that it had taken three hours

00:28:28.580 --> 00:28:29.660
to get back to someone.

00:28:29.660 --> 00:28:33.275
And I feel like that is a sign
that we haven't developed quite

00:28:33.275 --> 00:28:37.700
that shared understanding of
what use of these communication

00:28:37.700 --> 00:28:39.290
technologies really means.

00:28:39.290 --> 00:28:41.000
And I feel that
because we haven't

00:28:41.000 --> 00:28:44.120
set that, we often
pressure ourselves

00:28:44.120 --> 00:28:46.617
into being polite about it--

00:28:46.617 --> 00:28:48.200
say, oh, if someone
sent me a message,

00:28:48.200 --> 00:28:50.720
I have to respond right now,
because otherwise, they'll

00:28:50.720 --> 00:28:51.800
worry.

00:28:51.800 --> 00:28:55.100
And that social dynamic and
that cultural dynamic, I think,

00:28:55.100 --> 00:28:56.730
is really interesting for us.

00:28:56.730 --> 00:28:58.595
And so I think that's
something that we're

00:28:58.595 --> 00:29:00.386
going to spend a lot
of time thinking about

00:29:00.386 --> 00:29:03.650
and how we can help
through technology, nudge

00:29:03.650 --> 00:29:05.220
things in the right direction.

00:29:05.220 --> 00:29:07.172
ADAM ALTER: I think with
letters and emails--

00:29:07.172 --> 00:29:09.130
so with letters, you only
got a certain number.

00:29:09.130 --> 00:29:11.690
But emails-- some people
get hundreds a day.

00:29:11.690 --> 00:29:13.970
And so we've tried
to apply the grammar

00:29:13.970 --> 00:29:16.820
that we use to determine how
to deal socially with letters,

00:29:16.820 --> 00:29:18.080
which is you always reply.

00:29:18.080 --> 00:29:21.410
It's the polite thing to
do to email or to text.

00:29:21.410 --> 00:29:23.640
And that doesn't work if
you get a certain number.

00:29:23.640 --> 00:29:23.720
GLEN MURPHY: That's right.

00:29:23.720 --> 00:29:24.970
ADAM ALTER: You get inundated.

00:29:24.970 --> 00:29:27.410
And I was having this discussion
with my wife recently.

00:29:27.410 --> 00:29:30.112
If I get, say, 100
emails a day and I

00:29:30.112 --> 00:29:32.570
don't respond to one of them,
I make an active decision not

00:29:32.570 --> 00:29:34.430
to reply, that
seems really rude.

00:29:34.430 --> 00:29:37.370
But also, I didn't
invite conversation

00:29:37.370 --> 00:29:39.982
with everyone who's emailing,
so you just never know

00:29:39.982 --> 00:29:40.940
where to draw the line.

00:29:40.940 --> 00:29:41.640
GLEN MURPHY: That's right.

00:29:41.640 --> 00:29:41.900
Yeah.

00:29:41.900 --> 00:29:42.920
ADAM ALTER: It's really tricky.

00:29:42.920 --> 00:29:43.910
And I don't even
know if there is

00:29:43.910 --> 00:29:45.500
a set of rules you
could draw up that would

00:29:45.500 --> 00:29:46.540
allow you to deal with that.

00:29:46.540 --> 00:29:48.740
GLEN MURPHY: No, especially for
the different types of people

00:29:48.740 --> 00:29:49.880
that are emailing you--

00:29:49.880 --> 00:29:52.310
work emails versus
emails from your parents,

00:29:52.310 --> 00:29:54.669
different expectations, but
your habits may be the same.

00:29:54.669 --> 00:29:56.210
MAGGIE JACKSON:
Different generations

00:29:56.210 --> 00:29:58.820
have different
expectations, certainly.

00:29:58.820 --> 00:30:02.840
But I think that maybe
we have to welcome

00:30:02.840 --> 00:30:04.250
that type of uncertainty.

00:30:04.250 --> 00:30:06.620
We'll keep working at it,
because it took a thousand

00:30:06.620 --> 00:30:10.430
years for the book
to be hammered out

00:30:10.430 --> 00:30:11.990
as the technology we know today.

00:30:11.990 --> 00:30:15.320
In Shakespeare's time, there
were 150 different versions

00:30:15.320 --> 00:30:17.090
of "Twelfth Night,"
and you could

00:30:17.090 --> 00:30:22.400
buy all sorts of different plot
lines in London's bookstores.

00:30:22.400 --> 00:30:27.770
In our remaining minutes, let's
try to look ahead, perhaps.

00:30:27.770 --> 00:30:30.290
We've guessed a little
bit about, perhaps,

00:30:30.290 --> 00:30:31.970
what digital
well-being might be.

00:30:31.970 --> 00:30:33.500
But what's exciting?

00:30:33.500 --> 00:30:35.390
What's coming up
in the pipeline,

00:30:35.390 --> 00:30:38.690
either as a sort of
throwing our minds forward

00:30:38.690 --> 00:30:42.410
to the future in terms
of technological digital

00:30:42.410 --> 00:30:43.010
well-being?

00:30:43.010 --> 00:30:47.510
Or what's really
exciting now for you?

00:30:47.510 --> 00:30:48.879
Adam?

00:30:48.879 --> 00:30:51.170
ADAM ALTER: I'm excited that
there's a conversation now

00:30:51.170 --> 00:30:51.800
at this level.

00:30:51.800 --> 00:30:53.996
The fact that I was
invited to speak here

00:30:53.996 --> 00:30:56.120
is very exciting for me,
because I've been thinking

00:30:56.120 --> 00:30:57.411
about these issues for a while.

00:30:57.411 --> 00:31:00.561
And I used to spend the first
10 minutes of any conversation,

00:31:00.561 --> 00:31:02.810
or any talk, convincing
people that this was something

00:31:02.810 --> 00:31:04.369
we should even be discussing.

00:31:04.369 --> 00:31:05.910
And we don't have
to do that anymore.

00:31:05.910 --> 00:31:07.240
I think that's a
really good first step,

00:31:07.240 --> 00:31:08.630
that when you look
people in the eye,

00:31:08.630 --> 00:31:10.255
we all recognize that
this is something

00:31:10.255 --> 00:31:11.630
we should be focusing on.

00:31:11.630 --> 00:31:14.512
And that goes all the way up
to a big company like Google

00:31:14.512 --> 00:31:16.220
and all the way down
to individual people

00:31:16.220 --> 00:31:17.386
that you meet on the street.

00:31:17.386 --> 00:31:18.832
So I think that's very exciting.

00:31:18.832 --> 00:31:20.790
And then the question is
where we go from here.

00:31:20.790 --> 00:31:23.120
And I think we're
really at the beginning.

00:31:23.120 --> 00:31:25.700
And I wish there were better
scientific research out there

00:31:25.700 --> 00:31:29.629
that suggested this is
what the damage will be.

00:31:29.629 --> 00:31:30.920
This is what we need to change.

00:31:30.920 --> 00:31:32.180
If we don't, this
is what'll happen,

00:31:32.180 --> 00:31:33.513
and this is how we deal with it.

00:31:33.513 --> 00:31:36.594
And it turns out, I'm at a very
exciting point, because I'm

00:31:36.594 --> 00:31:37.760
doing some of this research.

00:31:37.760 --> 00:31:40.070
But I think it's some of
the earliest research.

00:31:40.070 --> 00:31:41.960
There isn't a huge
amount out there now.

00:31:41.960 --> 00:31:44.750
So we're at the bottom of a
very tall, steep mountain,

00:31:44.750 --> 00:31:46.775
and we will learn a
lot along the way.

00:31:46.775 --> 00:31:47.900
But not much has been done.

00:31:47.900 --> 00:31:49.610
There's a huge amount
of low-hanging fruit.

00:31:49.610 --> 00:31:51.943
And that's what I'll be doing
with the next few years is

00:31:51.943 --> 00:31:54.530
trying to work out, what should
we be really concerned about?

00:31:54.530 --> 00:31:55.821
What are we just talking about?

00:31:55.821 --> 00:31:57.746
Because there is
some degree of panic.

00:31:57.746 --> 00:31:59.120
And then, how do
we deal with it?

00:31:59.120 --> 00:32:02.150
How do we fix it, not just
as individual consumers who

00:32:02.150 --> 00:32:05.120
are managing our own
use or our kids' use,

00:32:05.120 --> 00:32:07.750
but also, how do
companies like Google

00:32:07.750 --> 00:32:10.120
introduce new options,
new technologies?

00:32:10.120 --> 00:32:12.580
And obviously, that's
already starting in a very--

00:32:12.580 --> 00:32:13.840
I think encouraging way.

00:32:16.540 --> 00:32:22.910
GLEN MURPHY: I think what I'm
sort of excited about is I

00:32:22.910 --> 00:32:25.970
and I think many people here--

00:32:25.970 --> 00:32:27.800
the reason why we
work in this space

00:32:27.800 --> 00:32:32.180
is that we want to build
experiences that people love.

00:32:32.180 --> 00:32:35.390
I think just delivering
something to someone

00:32:35.390 --> 00:32:37.070
and seeing the joy
on their face--

00:32:37.070 --> 00:32:41.390
I think that's a deeply
motivating factor for me

00:32:41.390 --> 00:32:43.460
and I'm sure many of you.

00:32:43.460 --> 00:32:46.730
And I feel like part
of this discussion

00:32:46.730 --> 00:32:51.860
is an understanding
that we don't quite

00:32:51.860 --> 00:32:54.680
know yet what it means for
people to love software.

00:32:54.680 --> 00:32:57.380
I think, historically, we've
sort of measured that very

00:32:57.380 --> 00:33:01.430
closely in terms of, oh, if
people are spending more time

00:33:01.430 --> 00:33:03.800
using the thing that
I've built, clearly that

00:33:03.800 --> 00:33:05.230
must mean that they love it.

00:33:05.230 --> 00:33:07.490
And I feel like
this whole space is

00:33:07.490 --> 00:33:10.195
about understanding that
that's not necessarily true.

00:33:10.195 --> 00:33:12.170
And so what I'm
really excited about

00:33:12.170 --> 00:33:15.110
is finding more of the evidence
that you talk about and more

00:33:15.110 --> 00:33:18.290
of those tools and means, so
that we can all go and build

00:33:18.290 --> 00:33:19.842
more lovable software.

00:33:19.842 --> 00:33:21.050
MAGGIE JACKSON: That's great.

00:33:21.050 --> 00:33:23.030
And I, too, would
second the idea

00:33:23.030 --> 00:33:26.930
that it's so exciting, having
been in the space of writing

00:33:26.930 --> 00:33:32.200
about technology for nearly
20 years now as a humanist,

00:33:32.200 --> 00:33:36.230
to see the maturation of
discussion and the unease.

00:33:36.230 --> 00:33:39.140
I think that's really
important, too.

00:33:39.140 --> 00:33:40.590
I would say that--

00:33:40.590 --> 00:33:42.140
and just to throw out--

00:33:42.140 --> 00:33:44.970
I think that it's a
little bit different,

00:33:44.970 --> 00:33:48.450
but it's part of well-being--
this issue of tolerance.

00:33:48.450 --> 00:33:53.330
And I have seen some
very early evidence

00:33:53.330 --> 00:33:55.490
that in this space
of the digital world,

00:33:55.490 --> 00:33:58.790
more can be done
than one would think,

00:33:58.790 --> 00:34:03.680
to kind of push back on the
hatred that's out there online.

00:34:03.680 --> 00:34:07.460
There is a professor at
NYU, actually a PhD student,

00:34:07.460 --> 00:34:12.620
who's done some amazing
work in the space of helping

00:34:12.620 --> 00:34:13.969
people speak up.

00:34:13.969 --> 00:34:16.920
And on Twitter,
he's found that when

00:34:16.920 --> 00:34:19.520
there's a hater,
someone who's really

00:34:19.520 --> 00:34:25.429
espousing horrific
things online and then

00:34:25.429 --> 00:34:27.500
a person from their in-group.

00:34:27.500 --> 00:34:32.000
So if there's a
white male espousing

00:34:32.000 --> 00:34:36.440
hatred and a white male
who's influential--

00:34:36.440 --> 00:34:38.530
has followers on Twitter--

00:34:38.530 --> 00:34:42.350
makes one comment about,
hey, man, think again,

00:34:42.350 --> 00:34:44.289
or these people are human.

00:34:44.289 --> 00:34:45.080
What are you doing?

00:34:45.080 --> 00:34:49.760
One comment lowers dramatically
the amount of hatred

00:34:49.760 --> 00:34:52.980
from that person for two months.

00:34:52.980 --> 00:34:55.889
And that's really
a remarkable thing.

00:34:55.889 --> 00:34:58.380
So it speaks to a lot
of research right now.

00:34:58.380 --> 00:35:01.880
There's an exciting
research on prejudice on

00:35:01.880 --> 00:35:04.030
and offline on
combating prejudice,

00:35:04.030 --> 00:35:06.570
that shows little points of
speaking up are important.

00:35:06.570 --> 00:35:09.800
So technology that could
help us learn that,

00:35:09.800 --> 00:35:12.650
or help us speak
up, or understand

00:35:12.650 --> 00:35:16.040
when we're needed to say
something in the realm,

00:35:16.040 --> 00:35:18.270
I think, is really important.

00:35:18.270 --> 00:35:22.160
Another study out
of Europe showed

00:35:22.160 --> 00:35:25.520
that if people are given
a kind of adventure game

00:35:25.520 --> 00:35:29.090
and perspective, taking a
day in the life of Roma--

00:35:29.090 --> 00:35:31.040
a person who's a Roma--

00:35:31.040 --> 00:35:33.560
and the kinds of troubles
that they go through--

00:35:33.560 --> 00:35:37.220
very short game has
extraordinary effects.

00:35:37.220 --> 00:35:41.660
It lowers their likelihood to
vote for a far-right party.

00:35:41.660 --> 00:35:46.910
It lowers their prejudice
against the Roma.

00:35:46.910 --> 00:35:51.230
It also, for up to a year,
lowers their prejudice

00:35:51.230 --> 00:35:55.790
against other minorities,
or other pariah people,

00:35:55.790 --> 00:35:58.820
like people with a lot
of negative sentiment

00:35:58.820 --> 00:35:59.750
about-- like refugees.

00:35:59.750 --> 00:36:02.390
So I think these ripple
effects are really

00:36:02.390 --> 00:36:07.310
important to tap in
the virtual world.

00:36:07.310 --> 00:36:11.310
Ripple effects are there,
and that's really important.

00:36:11.310 --> 00:36:13.730
So I think we've
covered a lot of ground.

00:36:13.730 --> 00:36:16.580
But I hope we've raised some
good questions, something

00:36:16.580 --> 00:36:20.210
to take away, to think
about, to chew over.

00:36:20.210 --> 00:36:23.420
And I'll just say, to wrap
up, that we've certainly come

00:36:23.420 --> 00:36:25.940
a long way from the days
of lantern slide shows

00:36:25.940 --> 00:36:29.030
and the telegraph as the
state-of-the-art communications

00:36:29.030 --> 00:36:29.880
tool.

00:36:29.880 --> 00:36:33.200
And yet, we're still wrestling
with the tensions and promises

00:36:33.200 --> 00:36:35.630
of technology.

00:36:35.630 --> 00:36:41.360
All of what we're facing demands
creative spirits, open minds,

00:36:41.360 --> 00:36:44.540
and a lot of questioning.

00:36:44.540 --> 00:36:49.570
And questioning is the
opposite of complacency.

00:36:49.570 --> 00:36:54.900
And unease is really a central
route to progress in a society.

00:36:54.900 --> 00:36:58.460
So I think we have reason to be
optimistic that with your help,

00:36:58.460 --> 00:37:01.550
we can create a more
healthy digital world.

00:37:01.550 --> 00:37:02.785
Thank you very much.

00:37:02.785 --> 00:37:05.272
[MUSIC PLAYING]

