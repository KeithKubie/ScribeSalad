WEBVTT
Kind: captions
Language: en

00:00:01.820 --> 00:00:06.700
MODERATOR: Today with us we
have Professor Nick Bostrom.

00:00:06.700 --> 00:00:09.850
He was born in
Helsingbrg in Sweden.

00:00:09.850 --> 00:00:12.270
He's a philosopher
at St. Cross College

00:00:12.270 --> 00:00:14.750
at the University of Oxford.

00:00:14.750 --> 00:00:17.520
He's known for his work
on existential risk,

00:00:17.520 --> 00:00:21.060
the entropic principle,
human enhancement ethics,

00:00:21.060 --> 00:00:24.420
the reversal test,
and consequentialism.

00:00:24.420 --> 00:00:27.925
He holds a Ph.D. from the
London School of Economics,

00:00:27.925 --> 00:00:30.980
and he is the founding director
of both the Future of Humanity

00:00:30.980 --> 00:00:34.470
Institute and the Oxford
Martin Programme on the Impacts

00:00:34.470 --> 00:00:36.760
of the Future on Technology.

00:00:36.760 --> 00:00:40.110
He's the author of
over 200 publications,

00:00:40.110 --> 00:00:42.350
including the book we
are presenting today,

00:00:42.350 --> 00:00:44.570
"Superintelligence."

00:00:44.570 --> 00:00:47.910
And he has been awarded
the Eugene R. Gannon Award

00:00:47.910 --> 00:00:51.280
and has been listed in "Foreign
Policy"'s Top 100 Global

00:00:51.280 --> 00:00:52.670
Thinkers list.

00:00:52.670 --> 00:00:56.064
Please join me in welcoming
Professor Nick Bostrom.

00:00:56.064 --> 00:01:01.427
[APPLAUSE]

00:01:01.427 --> 00:01:02.510
NICK BOSTROM: Yeah, great.

00:01:02.510 --> 00:01:04.230
Thanks you for coming.

00:01:04.230 --> 00:01:08.180
And I'm not going to try to
summarize the entire book,

00:01:08.180 --> 00:01:10.560
but I want to give
some of the background

00:01:10.560 --> 00:01:13.920
from which this work emerges.

00:01:13.920 --> 00:01:16.840
So I run this thing called The
Future of Humanity Institute,

00:01:16.840 --> 00:01:18.540
which has a very sort
of ambitious name.

00:01:18.540 --> 00:01:21.390
It's a small
research center where

00:01:21.390 --> 00:01:24.470
mathematicians, philosophers,
and scientists are trying

00:01:24.470 --> 00:01:26.650
to think through the really
big-picture questions

00:01:26.650 --> 00:01:29.620
for humanity, ones that
often traditionally

00:01:29.620 --> 00:01:34.610
have been relegated to
crackpots, relegated

00:01:34.610 --> 00:01:37.640
to journalists or
retired physicists

00:01:37.640 --> 00:01:38.980
to write some popular book.

00:01:38.980 --> 00:01:41.920
But questions that are
actually extremely important

00:01:41.920 --> 00:01:44.740
and, I think, deserve
close attention.

00:01:44.740 --> 00:01:48.850
So to give some sense of
where I'm coming from,

00:01:48.850 --> 00:01:50.740
one can think about
the human condition,

00:01:50.740 --> 00:01:55.390
in grand schematic terms, on
a diagram like this, where

00:01:55.390 --> 00:01:57.620
we plot time on the
x-axis, and then

00:01:57.620 --> 00:01:59.560
on the other axis, some
measure of capability,

00:01:59.560 --> 00:02:03.180
like level of
technological advancement.

00:02:03.180 --> 00:02:05.470
Measure of the overall
economic productivity

00:02:05.470 --> 00:02:07.450
that we have at some
given point in time.

00:02:07.450 --> 00:02:11.294
And what we take to be the
normal human condition--

00:02:11.294 --> 00:02:12.960
the idea that you
wake up in the morning

00:02:12.960 --> 00:02:15.850
and then commute to work and
sit in front of a screen,

00:02:15.850 --> 00:02:20.720
and you're having too much
rather than too little to eat,

00:02:20.720 --> 00:02:23.740
is a narrow band within
this much larger space

00:02:23.740 --> 00:02:25.740
of possibilities.

00:02:25.740 --> 00:02:29.590
It's obviously an anomaly
on evolutionary time scales.

00:02:29.590 --> 00:02:32.210
The human species is fairly
young on this planet.

00:02:32.210 --> 00:02:35.260
It's also an anomaly on
just historical time scales.

00:02:35.260 --> 00:02:39.040
For most of human history, we
were inhabiting a Malthusian

00:02:39.040 --> 00:02:43.190
state, and it's really only
in the last few hundred years

00:02:43.190 --> 00:02:44.530
that we've kind of soared up.

00:02:44.530 --> 00:02:46.560
And even then, just in
some parts of the world.

00:02:46.560 --> 00:02:48.060
It's also a huge
anomaly, obviously,

00:02:48.060 --> 00:02:51.320
in space, the little crust
of [INAUDIBLE] planet

00:02:51.320 --> 00:02:53.790
being very different from
most of the stuff around us,

00:02:53.790 --> 00:02:58.214
which is just a
ultra-high vacuum.

00:02:58.214 --> 00:02:59.630
And yet we tend
to think that this

00:02:59.630 --> 00:03:01.088
is the normal way
for things to be,

00:03:01.088 --> 00:03:03.890
that any claim that things
might be very different

00:03:03.890 --> 00:03:07.110
is a radical claim that needs
some extraordinary evidence.

00:03:07.110 --> 00:03:09.429
Yet it's possible,
if we reflect on it,

00:03:09.429 --> 00:03:11.470
that the longer the time
scale we're considering,

00:03:11.470 --> 00:03:12.928
the greater the
probability that we

00:03:12.928 --> 00:03:17.280
will exit this human
condition in either one of two

00:03:17.280 --> 00:03:20.070
ways, downwards or upwards.

00:03:20.070 --> 00:03:23.436
This picture has two
attractor states.

00:03:23.436 --> 00:03:24.810
So if we exit the
human condition

00:03:24.810 --> 00:03:27.995
in the downwards direction--
there is in population biology

00:03:27.995 --> 00:03:31.080
the concept of a minimum
viable population size.

00:03:31.080 --> 00:03:34.290
With too few individuals left,
they can't sustain themselves.

00:03:34.290 --> 00:03:37.160
There's an attractor state down
there, which is extinction.

00:03:37.160 --> 00:03:39.850
Once you're extinct, you
tend to stay extinct.

00:03:39.850 --> 00:03:43.690
And more than 99.9%
of all the species

00:03:43.690 --> 00:03:47.530
that once flew, crawled, or
swam on this planet are extinct,

00:03:47.530 --> 00:03:50.894
so that certainly is
one possible future.

00:03:50.894 --> 00:03:52.810
Another way that the
human condition could end

00:03:52.810 --> 00:03:56.570
would be that we exit in
the upwards direction.

00:03:56.570 --> 00:04:00.440
And there, too, I think that
there is an attractor state.

00:04:00.440 --> 00:04:02.400
If and when a
civilization manages

00:04:02.400 --> 00:04:06.460
to obtain technological
maturity-- meaning,

00:04:06.460 --> 00:04:10.350
we have developed most
of all technologies that

00:04:10.350 --> 00:04:12.910
are physically possible
to be developed,

00:04:12.910 --> 00:04:17.000
and we have the ability
to spread through space

00:04:17.000 --> 00:04:20.720
in a reasonable way, through
automated self-replicating

00:04:20.720 --> 00:04:23.850
colonization probes--
then the destiny

00:04:23.850 --> 00:04:25.050
might be pretty well set.

00:04:25.050 --> 00:04:27.530
The level of existential
risk will go down,

00:04:27.530 --> 00:04:30.770
and maybe we could
continue on like that

00:04:30.770 --> 00:04:33.980
for millions and billions
of years, just growing

00:04:33.980 --> 00:04:37.180
at the significant fraction
of the speed of light

00:04:37.180 --> 00:04:40.440
indefinitely, until the
cosmological expansion makes

00:04:40.440 --> 00:04:43.470
it impossible to reach
any further resources.

00:04:43.470 --> 00:04:45.350
Like if something is
too far away today, then

00:04:45.350 --> 00:04:46.870
by the time we would
get there, as it were,

00:04:46.870 --> 00:04:47.953
it has moved further away.

00:04:47.953 --> 00:04:50.460
So there's a finite
bubble of stuff

00:04:50.460 --> 00:04:53.180
that something starting
from what we are

00:04:53.180 --> 00:04:55.810
could, in principle, get.

00:04:55.810 --> 00:04:58.230
So that whole bubble
of stuff, I call

00:04:58.230 --> 00:05:00.860
humanity's cosmic endowment.

00:05:00.860 --> 00:05:02.500
There is a lot of it there.

00:05:02.500 --> 00:05:05.510
And that might be another
possible attractor state.

00:05:08.470 --> 00:05:14.589
If one has to view that--
that this stuff that

00:05:14.589 --> 00:05:16.130
could, in principle,
be reached there

00:05:16.130 --> 00:05:18.546
is very important, if one has
some kind of view on ethics,

00:05:18.546 --> 00:05:24.250
where fundamentally, the moral
significance of an experience

00:05:24.250 --> 00:05:31.680
or a life does not depend on
when it is taking place-- just

00:05:31.680 --> 00:05:35.866
as many people have the belief
that from moral point of view,

00:05:35.866 --> 00:05:37.490
it doesn't matter
where it takes place.

00:05:37.490 --> 00:05:39.710
Like if you travel to
Africa and you suffer there,

00:05:39.710 --> 00:05:41.960
it's as bad as if you
were suffering here.

00:05:41.960 --> 00:05:47.900
If one has that view about
time, then this cosmic endowment

00:05:47.900 --> 00:05:48.560
matters a lot.

00:05:48.560 --> 00:05:52.050
Because it could
be used to create

00:05:52.050 --> 00:05:55.220
an enormous amount of value.

00:05:55.220 --> 00:05:57.580
We can count it up, roughly.

00:05:57.580 --> 00:06:00.440
We know that there are billions
of galaxies, each with billions

00:06:00.440 --> 00:06:02.520
of stars, and each
of those stars

00:06:02.520 --> 00:06:05.600
could have billions of
people living around it

00:06:05.600 --> 00:06:07.150
for billions of years.

00:06:07.150 --> 00:06:09.420
And you get an enormous
number of orders of magnitude

00:06:09.420 --> 00:06:11.460
if you try to measure
the size of the future,

00:06:11.460 --> 00:06:15.030
even just assuming biological
instantiations of minds.

00:06:15.030 --> 00:06:16.700
If you're imagine
a more efficient

00:06:16.700 --> 00:06:19.800
digital implementation, you
can add another big chunk

00:06:19.800 --> 00:06:21.780
of orders of magnitude.

00:06:21.780 --> 00:06:24.650
And so what you find fairly
robustly, if you just

00:06:24.650 --> 00:06:28.340
work the numbers, is that if
you have this evaluative view,

00:06:28.340 --> 00:06:30.310
some broadly-aggregated
consequentialist

00:06:30.310 --> 00:06:33.100
view, then even a very,
very small reduction

00:06:33.100 --> 00:06:36.250
in the net level
of existential risk

00:06:36.250 --> 00:06:38.960
will be worth more, in
expected utility terms,

00:06:38.960 --> 00:06:41.980
than any interventions you
could do that would only

00:06:41.980 --> 00:06:43.920
have local effect here.

00:06:43.920 --> 00:06:46.090
Even something as
wonderful as, like,

00:06:46.090 --> 00:06:49.020
curing cancer or
eliminating world hunger

00:06:49.020 --> 00:06:51.780
would really be, from this
evaluative perspective,

00:06:51.780 --> 00:06:53.840
insignificant
compared to reducing

00:06:53.840 --> 00:06:56.270
the level of existential
risk by, say, 1/100th

00:06:56.270 --> 00:06:58.410
of one percentage point.

00:06:58.410 --> 00:06:59.980
So this level of
existential risk

00:06:59.980 --> 00:07:02.200
becomes, then, maybe
an important lens

00:07:02.200 --> 00:07:05.730
through which to look
at global priorities.

00:07:05.730 --> 00:07:09.940
I define it as a risk
that either threatens

00:07:09.940 --> 00:07:12.680
the survival of
Earth-originating intelligent

00:07:12.680 --> 00:07:15.600
life, or threatens to
permanently and drastically

00:07:15.600 --> 00:07:17.935
destroy our potential for
desirable development.

00:07:21.370 --> 00:07:24.330
And now I think that maybe in a
complete accounting of ethics,

00:07:24.330 --> 00:07:26.730
there are other variables, as
well, to take into account.

00:07:26.730 --> 00:07:29.710
We have particular
obligations to people

00:07:29.710 --> 00:07:31.000
that are near and dear to us.

00:07:31.000 --> 00:07:32.660
There might be other
things in addition

00:07:32.660 --> 00:07:35.370
to this sort of aggregated
consequentialist component.

00:07:35.370 --> 00:07:37.910
Nevertheless, I think it's
in there and it's important.

00:07:37.910 --> 00:07:43.057
So if one then tries to look
more carefully at this category

00:07:43.057 --> 00:07:45.390
of existential risk-- like,
what could actually go wrong

00:07:45.390 --> 00:07:45.889
in this way?

00:07:45.889 --> 00:07:49.979
What could permanently
destroy our future?

00:07:49.979 --> 00:07:52.520
It's a very small subset of all
the things that can go wrong.

00:07:52.520 --> 00:07:58.820
Like most things that threaten
human welfare don't really

00:07:58.820 --> 00:08:00.410
create any existential risk.

00:08:00.410 --> 00:08:02.230
So it kind of narrows
down the range

00:08:02.230 --> 00:08:03.700
of concerns quite significantly.

00:08:03.700 --> 00:08:08.370
A first distinction that
is obvious in this field

00:08:08.370 --> 00:08:11.670
is the distinction between risks
arising from nature and risks

00:08:11.670 --> 00:08:14.420
arising in some way
from human activity.

00:08:14.420 --> 00:08:16.720
And a fairly robust
result, I think,

00:08:16.720 --> 00:08:19.160
is that all the big
existential risks,

00:08:19.160 --> 00:08:22.110
at least if we are thinking a
time scale of 100 years or so,

00:08:22.110 --> 00:08:25.595
are anthropomorphic, arising
from human activities.

00:08:25.595 --> 00:08:27.220
And one can see that
just by reflecting

00:08:27.220 --> 00:08:28.636
that the human
species has already

00:08:28.636 --> 00:08:30.430
been around for 100,000 years.

00:08:30.430 --> 00:08:33.179
So if firestorms and
earthquakes and asteroids

00:08:33.179 --> 00:08:36.400
haven't done us in in the
last 100,000 years, probably

00:08:36.400 --> 00:08:38.820
not going to do us in
in the next 100 years.

00:08:38.820 --> 00:08:42.750
Whereas we will be introducing
entirely new kinds of hazards

00:08:42.750 --> 00:08:46.599
into the world that we have
no track record of surviving.

00:08:46.599 --> 00:08:49.140
And more specifically, I think
all the really big existential

00:08:49.140 --> 00:08:55.380
risks are related to certain
anticipated future technologies

00:08:55.380 --> 00:08:58.030
that we might well develop
over the coming decades or 100

00:08:58.030 --> 00:09:00.270
years.

00:09:00.270 --> 00:09:02.590
And another way
to-- another framing

00:09:02.590 --> 00:09:05.520
that makes a similar
point is to think

00:09:05.520 --> 00:09:10.160
in terms of this metaphor
of a great urn which

00:09:10.160 --> 00:09:11.360
contains a lot of balls.

00:09:11.360 --> 00:09:13.144
The balls represent
different technologies

00:09:13.144 --> 00:09:14.810
that can be discovered,
or more broadly,

00:09:14.810 --> 00:09:17.960
the different ideas
that we can invent.

00:09:17.960 --> 00:09:20.700
And throughout human history,
we have reached into this ball

00:09:20.700 --> 00:09:23.920
repeatedly and pulled
out ball after ball.

00:09:23.920 --> 00:09:27.520
And on balance, all these
discoveries, all these ideas,

00:09:27.520 --> 00:09:30.350
have been an
immense boon for us.

00:09:30.350 --> 00:09:33.720
It's because of all these ideas
that we now live in abundance,

00:09:33.720 --> 00:09:37.820
and why there can be
seven billion of us.

00:09:37.820 --> 00:09:40.070
There have been some
discoveries, perhaps,

00:09:40.070 --> 00:09:42.200
that have been mixed
blessings, that

00:09:42.200 --> 00:09:43.670
have done both good and ill.

00:09:43.670 --> 00:09:46.787
And a relatively
small number-- it's

00:09:46.787 --> 00:09:48.370
not totally trivial
to think about it,

00:09:48.370 --> 00:09:51.110
but balls that we would have
been better off without having

00:09:51.110 --> 00:09:52.990
extracted from this
urn, like discoveries

00:09:52.990 --> 00:09:54.680
that we would be better without.

00:09:54.680 --> 00:09:59.100
I mean, maybe chemical weapons,
say, or nuclear weapons.

00:09:59.100 --> 00:10:02.590
Or perhaps, like, torture
instruments of different kinds.

00:10:02.590 --> 00:10:07.590
There are few things that
seems to have been clearly bad.

00:10:07.590 --> 00:10:09.760
But there hasn't been
any discovery so far made

00:10:09.760 --> 00:10:12.760
that is such that
it automatically

00:10:12.760 --> 00:10:15.270
destroys the civilization
that discovers it.

00:10:15.270 --> 00:10:18.220
So there hasn't been any black
ball pulled out from this urn.

00:10:18.220 --> 00:10:22.660
And we can ask what that kind
of discovery could look like.

00:10:22.660 --> 00:10:24.840
What would be a
possible discovery, such

00:10:24.840 --> 00:10:26.960
that it kind of almost
automatically spells

00:10:26.960 --> 00:10:31.160
the end of the discoverers?

00:10:31.160 --> 00:10:33.940
And it might be useful here to
think of a counter [INAUDIBLE].

00:10:33.940 --> 00:10:39.490
So we discovered, just over half
a century ago, nuclear weapons.

00:10:39.490 --> 00:10:41.560
And it turned out,
fortunately, that in order

00:10:41.560 --> 00:10:44.860
to make a thermonuclear
device, you

00:10:44.860 --> 00:10:47.205
need some difficult-to-obtain
raw materials.

00:10:47.205 --> 00:10:49.920
You need highly enriched
uranium or plutonium.

00:10:49.920 --> 00:10:52.330
And the only way to
get those is by having

00:10:52.330 --> 00:10:55.690
some large facility that's
very expensive to build,

00:10:55.690 --> 00:10:57.550
takes a lot of energy,
is easy to see.

00:10:57.550 --> 00:11:02.240
So very few people can build
their own nuclear device.

00:11:02.240 --> 00:11:04.890
But suppose it had turned
out to be differently.

00:11:04.890 --> 00:11:06.975
Suppose it had
turned out instead

00:11:06.975 --> 00:11:11.850
that it had been possible to
make a thermonuclear warhead

00:11:11.850 --> 00:11:14.760
by some simple procedure, like
baking sand in your microwave

00:11:14.760 --> 00:11:15.860
oven, OK?

00:11:15.860 --> 00:11:18.852
So now we know physics
doesn't allow for that.

00:11:18.852 --> 00:11:21.310
But before you actually did
the relevant physics, how could

00:11:21.310 --> 00:11:23.970
you possibly have known
whether particle physics would

00:11:23.970 --> 00:11:26.682
have provided some
easy route to unleash

00:11:26.682 --> 00:11:28.154
these kinds of entities?

00:11:28.154 --> 00:11:29.570
So if that had
been the case, then

00:11:29.570 --> 00:11:31.680
presumably that would
then be the endpoint

00:11:31.680 --> 00:11:33.480
of human civilization.

00:11:33.480 --> 00:11:35.850
Once it became so easy
to destroy entire cities,

00:11:35.850 --> 00:11:39.390
we could never
again have cities,

00:11:39.390 --> 00:11:42.200
and maybe we would have been
knocked back to the Stone Age.

00:11:42.200 --> 00:11:44.170
And by the time we
would have again

00:11:44.170 --> 00:11:45.860
climbed back up to
the technology level

00:11:45.860 --> 00:11:47.900
where somebody could
build microwave ovens,

00:11:47.900 --> 00:11:49.840
we would presumably
fall back again.

00:11:49.840 --> 00:11:53.620
And that might be
forever the end of it.

00:11:53.620 --> 00:11:57.200
But so we were lucky
on that occasion,

00:11:57.200 --> 00:11:58.670
but the question
is whether we will

00:11:58.670 --> 00:12:00.250
continue to be lucky always.

00:12:00.250 --> 00:12:05.320
Like, whether in this big urn,
if we keep extracting ball

00:12:05.320 --> 00:12:06.772
after ball, whether
eventually we

00:12:06.772 --> 00:12:07.980
will pull out the black ball.

00:12:07.980 --> 00:12:09.020
If there is a
black ball in there

00:12:09.020 --> 00:12:10.710
and we just keep pulling
them out, then eventually,

00:12:10.710 --> 00:12:12.230
presumably, we will get it.

00:12:12.230 --> 00:12:13.720
And we don't yet
have the ability

00:12:13.720 --> 00:12:15.550
to put a ball back in the urn.

00:12:15.550 --> 00:12:19.860
We can't undiscover things
that we have discovered.

00:12:19.860 --> 00:12:25.304
So here is a kind of
quick list of some areas

00:12:25.304 --> 00:12:26.720
where one might
suspect that there

00:12:26.720 --> 00:12:29.130
could be these kinds
of black balls.

00:12:29.130 --> 00:12:34.344
And AI is one that I'll kind of
come back to more in this talk.

00:12:34.344 --> 00:12:35.260
There are some others.

00:12:35.260 --> 00:12:39.130
Synthetic biology will, I
think over the coming decades,

00:12:39.130 --> 00:12:42.180
vastly increase the
powers of human beings

00:12:42.180 --> 00:12:46.080
to change the world
around us and ourselves.

00:12:46.080 --> 00:12:48.980
Those powers might be
used wisely or not.

00:12:48.980 --> 00:12:51.250
Molecular nanotechnology.

00:12:51.250 --> 00:12:54.050
Not the kind of thing that makes
car tires today, but some kind

00:12:54.050 --> 00:12:58.360
of more advanced future version
of that, like Eric Drexler

00:12:58.360 --> 00:13:00.450
imagined.

00:13:00.450 --> 00:13:02.210
Totalitarianism-enabling
technology.

00:13:02.210 --> 00:13:04.376
So remember, again, the
definition of an existential

00:13:04.376 --> 00:13:07.950
risk-- not only extinction
scenarios, nice but also ways

00:13:07.950 --> 00:13:10.230
to permanently lock ourselves
in to some radically

00:13:10.230 --> 00:13:12.970
suboptimal state.

00:13:12.970 --> 00:13:18.110
And you can imagine that maybe
new technological discoveries

00:13:18.110 --> 00:13:20.970
that make surveillance very
easy, or some new discovery

00:13:20.970 --> 00:13:24.620
that makes it possible,
through psychological or

00:13:24.620 --> 00:13:27.190
neurophysiological
techniques, to modify desires

00:13:27.190 --> 00:13:29.580
could sort of change
some of the parameters

00:13:29.580 --> 00:13:32.720
of the sort of
sociopolitical game, where

00:13:32.720 --> 00:13:34.940
new types of social
organization becomes

00:13:34.940 --> 00:13:38.420
a lot easier to
establish and maintain.

00:13:38.420 --> 00:13:41.172
Human modification,
geoengineering.

00:13:41.172 --> 00:13:42.630
There are more you
could add there,

00:13:42.630 --> 00:13:44.140
and I've left a lot
of these bullets

00:13:44.140 --> 00:13:46.430
below here on the list unknown.

00:13:46.430 --> 00:13:48.682
So it's useful to reflect
that if this question--

00:13:48.682 --> 00:13:50.390
what are the biggest
existential risks?--

00:13:50.390 --> 00:13:54.550
had been asked 100 years
ago, then presumably none

00:13:54.550 --> 00:13:57.000
of the ones that I now would
place close to the top would

00:13:57.000 --> 00:13:59.512
have been listed.

00:13:59.512 --> 00:14:00.970
They didn't have
computers, so they

00:14:00.970 --> 00:14:02.720
wouldn't have listed
machine intelligence.

00:14:02.720 --> 00:14:07.230
Synthetic biology was not even
a concept, nor nanotechnology.

00:14:07.230 --> 00:14:09.860
Maybe they would have worried
some about, like, totalitarian

00:14:09.860 --> 00:14:11.534
tendencies.

00:14:11.534 --> 00:14:12.700
But the others, not so much.

00:14:12.700 --> 00:14:15.830
So if we reflect on
our situation today,

00:14:15.830 --> 00:14:18.586
we have to maybe acknowledge,
from standing outside

00:14:18.586 --> 00:14:20.210
and looking in, that
there are probably

00:14:20.210 --> 00:14:22.710
some additional existential
risks that are not

00:14:22.710 --> 00:14:25.490
yet on our radar but
that could turn out

00:14:25.490 --> 00:14:28.050
to be as significant
as some of the others.

00:14:28.050 --> 00:14:31.460
Which as I said, there could
be high value to doing analysis

00:14:31.460 --> 00:14:35.670
on this and research to
try to find them out.

00:14:35.670 --> 00:14:40.820
But if one combines these
considerations with one

00:14:40.820 --> 00:14:47.000
other hypothesis-- say,
a mild or moderate form

00:14:47.000 --> 00:14:49.540
of technological
determinism, which I think

00:14:49.540 --> 00:14:54.390
is true-- the idea, basically,
that assuming science

00:14:54.390 --> 00:14:58.550
and technology continues,
there's no global collapse,

00:14:58.550 --> 00:15:02.009
then eventually we'll probably
discover all technologies

00:15:02.009 --> 00:15:03.050
that could be discovered.

00:15:03.050 --> 00:15:05.260
At least all
general-purpose technologies

00:15:05.260 --> 00:15:08.789
that have a lot of
implications in many fields.

00:15:08.789 --> 00:15:10.080
I think that's fairly possible.

00:15:10.080 --> 00:15:13.766
It's not assured, but that level
of technological determinism

00:15:13.766 --> 00:15:14.890
seems quite possible to me.

00:15:14.890 --> 00:15:18.640
It's a little bit like-- if
you think of a big box that

00:15:18.640 --> 00:15:20.480
starts out empty and
you pour in sand in it,

00:15:20.480 --> 00:15:23.617
this is like, you can fund
one kind of research here.

00:15:23.617 --> 00:15:25.200
You can fund another
kind of research.

00:15:25.200 --> 00:15:30.130
And what research you fund,
where your priorities are,

00:15:30.130 --> 00:15:32.550
that determines where the
sand piles up in this box.

00:15:32.550 --> 00:15:34.079
So you get different
technologies,

00:15:34.079 --> 00:15:35.120
depending on what you do.

00:15:35.120 --> 00:15:37.328
But over time, if you just
keep pouring in sand, then

00:15:37.328 --> 00:15:39.170
eventually the whole
box will fill up.

00:15:39.170 --> 00:15:41.275
And so that seems
fairly possible.

00:15:41.275 --> 00:15:43.330
Now if one has
that view, then how

00:15:43.330 --> 00:15:45.207
should one kind of--
what attitude should one

00:15:45.207 --> 00:15:46.040
take to all of this?

00:15:46.040 --> 00:15:47.840
Like what should we do?

00:15:47.840 --> 00:15:52.510
So one possible
response is one I

00:15:52.510 --> 00:15:54.620
think best expressed
by this blogger.

00:15:54.620 --> 00:15:55.910
I don't know who it is.

00:15:55.910 --> 00:15:59.220
Washbash commented on some
blog that "I instinctively

00:15:59.220 --> 00:16:00.179
think go faster.

00:16:00.179 --> 00:16:02.220
Not because I think this
is better for the world.

00:16:02.220 --> 00:16:04.595
Why should I care about the
world when I'm dead and gone.

00:16:04.595 --> 00:16:06.330
I want it to go fast, damn it!

00:16:06.330 --> 00:16:07.865
This increases
the chances I have

00:16:07.865 --> 00:16:10.580
of experiencing a
more technologically

00:16:10.580 --> 00:16:13.450
advanced future."

00:16:13.450 --> 00:16:16.531
So here we've got to be clear
what exactly the question is

00:16:16.531 --> 00:16:17.280
that we're asking.

00:16:17.280 --> 00:16:20.220
So if the question is,
what would be best for me

00:16:20.220 --> 00:16:21.140
personally?

00:16:21.140 --> 00:16:23.510
What should we
favor or hope for,

00:16:23.510 --> 00:16:25.770
from a egoistic point of view?

00:16:25.770 --> 00:16:30.150
Then I think that
Washbash is correct.

00:16:32.940 --> 00:16:35.600
From an individual point of
view, if-- well, first of all,

00:16:35.600 --> 00:16:39.520
if you're somehow hoping
for these cosmic lifespans

00:16:39.520 --> 00:16:42.037
of millions of years, and
being able to travel and expand

00:16:42.037 --> 00:16:43.620
into the universe,
then clearly that's

00:16:43.620 --> 00:16:47.760
not going to happen unless
something radical changes.

00:16:47.760 --> 00:16:50.980
Like the way things are
going, I'm sad to say,

00:16:50.980 --> 00:16:54.190
we're all just going to die
from aging in a few decades.

00:16:54.190 --> 00:16:55.750
Like we're all rotting.

00:16:55.750 --> 00:16:58.110
So the only way that that
could possibly change

00:16:58.110 --> 00:16:59.900
is some radical upset.

00:16:59.900 --> 00:17:02.445
Like some cure for aging,
or uploading into computers,

00:17:02.445 --> 00:17:03.820
or something really
radical would

00:17:03.820 --> 00:17:07.170
have to happen to
kind of thwart that.

00:17:07.170 --> 00:17:10.130
So that would be reason to
favor faster technical growth.

00:17:10.130 --> 00:17:11.910
Or even, if you're
despairing of that,

00:17:11.910 --> 00:17:15.536
even you could just hope to have
more interesting gadgets around

00:17:15.536 --> 00:17:17.119
and a higher standard
of living, which

00:17:17.119 --> 00:17:19.920
we can hope for
through technology.

00:17:19.920 --> 00:17:22.849
However, if the question
we ask is instead,

00:17:22.849 --> 00:17:26.250
what would be best from an
impersonal point of view?

00:17:26.250 --> 00:17:30.570
Then I think the answer is quite
different, something perhaps

00:17:30.570 --> 00:17:35.730
closer to this principle of
technological development,

00:17:35.730 --> 00:17:39.715
rather than maximize the speed
with which you rush ahead.

00:17:39.715 --> 00:17:41.340
This principle would
say that we should

00:17:41.340 --> 00:17:43.410
"retard the development
of dangerous and harmful

00:17:43.410 --> 00:17:44.890
technologies,
especially ones that

00:17:44.890 --> 00:17:46.700
raise the level of
existential risk,

00:17:46.700 --> 00:17:49.650
and accelerate the development
of beneficial technologies,

00:17:49.650 --> 00:17:52.230
especially those that reduce
the existential risks posed

00:17:52.230 --> 00:17:56.940
by nature or by
other technologies."

00:17:56.940 --> 00:17:59.090
So the idea here is
that rather than asking

00:17:59.090 --> 00:18:01.460
the question for some
hypothetical technology,

00:18:01.460 --> 00:18:05.104
would we be better
off without it?

00:18:05.104 --> 00:18:06.270
We ask a different question.

00:18:06.270 --> 00:18:08.930
Because basically,
on this moderate form

00:18:08.930 --> 00:18:11.500
of technological determinism,
that's just not on the table.

00:18:11.500 --> 00:18:14.020
We can't relinquish a
technology permanently.

00:18:14.020 --> 00:18:17.750
But what we should think of
instead is on the margin,

00:18:17.750 --> 00:18:20.190
should we try to hasten the
arrival of some technology

00:18:20.190 --> 00:18:20.910
or slow it down?

00:18:20.910 --> 00:18:22.868
We might be able to make
some difference there,

00:18:22.868 --> 00:18:25.750
say, by a couple months.

00:18:25.750 --> 00:18:29.530
And we want to think about
how that small difference will

00:18:29.530 --> 00:18:31.310
influence our
likelihood of harvesting

00:18:31.310 --> 00:18:34.204
this cosmic endowment.

00:18:34.204 --> 00:18:35.620
If you think that
it was literally

00:18:35.620 --> 00:18:38.130
impossible to even make a
small difference in the timing,

00:18:38.130 --> 00:18:42.464
then that would mean that all
the funding and all the effort

00:18:42.464 --> 00:18:44.005
that goes into
technology development

00:18:44.005 --> 00:18:45.349
would just be wasted.

00:18:45.349 --> 00:18:47.140
So presumably we think
we have some ability

00:18:47.140 --> 00:18:51.690
to at least move
things around in time.

00:18:51.690 --> 00:18:54.764
And the principle
of differential

00:18:54.764 --> 00:18:56.180
technological
development suggests

00:18:56.180 --> 00:18:58.096
that it might be quite
significant, sometimes,

00:18:58.096 --> 00:18:59.760
exactly when different
things arrive,

00:18:59.760 --> 00:19:02.280
particularly the sequence in
which different technologies

00:19:02.280 --> 00:19:03.040
arrive.

00:19:03.040 --> 00:19:06.040
So if there's going
to be, at some point,

00:19:06.040 --> 00:19:11.369
like a really harmful
bio-engineered pathogen,

00:19:11.369 --> 00:19:13.660
and it could spread really
easily and it's very lethal,

00:19:13.660 --> 00:19:15.285
and there's going to
be, at some point,

00:19:15.285 --> 00:19:16.910
like a universal
vaccine, you want

00:19:16.910 --> 00:19:20.057
to invent the vaccine before
you invent the pathogen.

00:19:20.057 --> 00:19:21.640
If there's going to
be, at some point,

00:19:21.640 --> 00:19:23.435
machine superintelligence,
and if there

00:19:23.435 --> 00:19:25.310
is some possible technology
that could assure

00:19:25.310 --> 00:19:27.460
the safety of machine
superintelligence,

00:19:27.460 --> 00:19:30.400
you want the latter to
come before the former.

00:19:33.364 --> 00:19:35.330
AUDIENCE: There's an
argument that trying

00:19:35.330 --> 00:19:38.820
to retard development
of technology

00:19:38.820 --> 00:19:41.190
would make it more dangerous
because you're driving it

00:19:41.190 --> 00:19:43.157
underground, or you
have less opportunity

00:19:43.157 --> 00:19:48.534
to do it out in the open,
develop safe [INAUDIBLE].

00:19:48.534 --> 00:19:50.575
Like we had, for example,
the Asilomar guidelines

00:19:50.575 --> 00:19:53.620
in biotech, which have
actually been very effective.

00:19:53.620 --> 00:19:57.230
For 30 years, there's
been no accidents.

00:19:57.230 --> 00:19:59.677
And if you drive these
technologies underground,

00:19:59.677 --> 00:20:01.260
you don't have the
opportunity to have

00:20:01.260 --> 00:20:02.359
those kinds of safeguards.

00:20:02.359 --> 00:20:03.150
NICK BOSTROM: Yeah.

00:20:03.150 --> 00:20:06.250
This-- the principal
leaves open whether you

00:20:06.250 --> 00:20:09.410
should focus on the retarding
or the accelerating.

00:20:09.410 --> 00:20:11.320
If you wanted to
retard, maybe one way

00:20:11.320 --> 00:20:15.020
would be just to refrain from
like funding it or actively

00:20:15.020 --> 00:20:18.780
devoting yourself
to accelerating it.

00:20:18.780 --> 00:20:20.980
With regard to AI,
which I'll get to later,

00:20:20.980 --> 00:20:25.130
I think definitely accelerating
the work on the safety problem

00:20:25.130 --> 00:20:26.440
is clearly the way to go.

00:20:26.440 --> 00:20:29.270
I think it's just a lot easier
to make a big difference there

00:20:29.270 --> 00:20:32.520
than to try to somehow retard
the development of AI work

00:20:32.520 --> 00:20:34.890
itself.

00:20:34.890 --> 00:20:39.190
Maybe we can return to
that more in the Q&amp;A.

00:20:39.190 --> 00:20:42.785
So we have a picture, perhaps,
like this, where again, we're

00:20:42.785 --> 00:20:43.910
looking at three axes here.

00:20:43.910 --> 00:20:46.600
So technology on one, this
is the same capability

00:20:46.600 --> 00:20:49.660
on the earlier slide.

00:20:49.660 --> 00:20:54.600
Coordination-- some measure of
the degree to which humankind

00:20:54.600 --> 00:20:58.210
is able to solve a global
coordination problems.

00:20:58.210 --> 00:21:01.710
Avoiding wars and arms
races, and polluting

00:21:01.710 --> 00:21:04.260
our communal resources.

00:21:04.260 --> 00:21:07.760
And insight-- so a measure
of our understanding

00:21:07.760 --> 00:21:11.950
into what uses of our
capability would actually

00:21:11.950 --> 00:21:13.900
make things better.

00:21:13.900 --> 00:21:15.370
So it might well
be that in order

00:21:15.370 --> 00:21:19.830
to have the best
possible outcome,

00:21:19.830 --> 00:21:22.990
to have Utopia, that we
need maximum amounts of all

00:21:22.990 --> 00:21:23.500
of these.

00:21:23.500 --> 00:21:25.610
Like super-duper
advanced technology

00:21:25.610 --> 00:21:28.450
is necessary to realize the
best state; great coordination,

00:21:28.450 --> 00:21:30.852
so we don't use that technology
to wage war against one

00:21:30.852 --> 00:21:33.060
another, as we have through
so much of human history;

00:21:33.060 --> 00:21:36.160
and great wisdom, so that
we apply all these abilities

00:21:36.160 --> 00:21:39.100
to really do things
that are worthwhile.

00:21:39.100 --> 00:21:41.044
So that might be
where we have to be,

00:21:41.044 --> 00:21:42.960
if we want to realize
the best possible state.

00:21:42.960 --> 00:21:44.543
Now that then leaves
open the question

00:21:44.543 --> 00:21:46.690
of whether from the
position we are currently

00:21:46.690 --> 00:21:49.390
in-- at the moment,
we would be better off

00:21:49.390 --> 00:21:53.010
with faster developments
in each of these areas.

00:21:53.010 --> 00:21:55.880
It might be, for example,
that even though we ultimately

00:21:55.880 --> 00:21:58.050
want maximum technology,
we would be better off

00:21:58.050 --> 00:22:00.470
getting that technology
only after we have first

00:22:00.470 --> 00:22:09.240
made more progress on global
coordination or wisdom.

00:22:09.240 --> 00:22:13.470
Anyway, so that's by view
of like a broader context.

00:22:13.470 --> 00:22:15.890
So we are thinking about other
existential risks and stuff

00:22:15.890 --> 00:22:18.060
like that.

00:22:18.060 --> 00:22:22.280
And superintelligence,
as I will talk about,

00:22:22.280 --> 00:22:25.170
I think is one big
existential risk, perhaps.

00:22:25.170 --> 00:22:26.885
Perhaps, arguably,
perhaps the biggest.

00:22:26.885 --> 00:22:27.760
I'm not sure.

00:22:27.760 --> 00:22:29.190
But it's peculiar
in one respect,

00:22:29.190 --> 00:22:32.040
that although it's a big
danger in its own right,

00:22:32.040 --> 00:22:34.540
it's also something
that could help

00:22:34.540 --> 00:22:37.080
eliminate other
existential risks.

00:22:37.080 --> 00:22:40.280
So if we imagine like a
very simple model, where

00:22:40.280 --> 00:22:42.502
we have synthetic biology,
nanotechnology, and AI--

00:22:42.502 --> 00:22:44.210
we don't know which
order they will come.

00:22:44.210 --> 00:22:46.060
Maybe they each have
some existential risks

00:22:46.060 --> 00:22:47.510
associated with them.

00:22:47.510 --> 00:22:50.150
Suppose we first develop
synthetic biology.

00:22:50.150 --> 00:22:52.830
We get lucky and we get
through the existential risks,

00:22:52.830 --> 00:22:53.770
however big they are.

00:22:53.770 --> 00:22:55.830
And then we reach
molecular nanotechnology,

00:22:55.830 --> 00:22:56.752
and we are lucky.

00:22:56.752 --> 00:22:57.960
We get through that, as well.

00:22:57.960 --> 00:23:02.241
And finally, AI, the existential
risks along that path

00:23:02.241 --> 00:23:04.740
are kind of the sum of these
three different ones that we'll

00:23:04.740 --> 00:23:07.850
each have to surmount.

00:23:07.850 --> 00:23:09.780
In another trajectory,
maybe we get AI first,

00:23:09.780 --> 00:23:12.240
and we have to face
existential risk for that.

00:23:12.240 --> 00:23:13.810
But then if we do
get lucky there,

00:23:13.810 --> 00:23:16.260
we no longer have to face the
risk with synthetic biology

00:23:16.260 --> 00:23:17.760
and nanotechnology,
because we don't

00:23:17.760 --> 00:23:20.180
have the superintelligence
to help us through.

00:23:20.180 --> 00:23:23.150
So in reality, it gets a lot
more complicated than that,

00:23:23.150 --> 00:23:27.810
and we can discuss the
intricacies more in the Q&amp;A.

00:23:27.810 --> 00:23:30.174
But thinking about the
sequencing and timing,

00:23:30.174 --> 00:23:31.840
I think, rather than
yes or no, would we

00:23:31.840 --> 00:23:33.298
want the technology
or not, is like

00:23:33.298 --> 00:23:34.970
an initial, necessary
first step to be

00:23:34.970 --> 00:23:37.450
able to have any kind of
meaningful conversation

00:23:37.450 --> 00:23:38.100
about this.

00:23:38.100 --> 00:23:42.190
So superintelligence, I think,
will be a big game-changer,

00:23:42.190 --> 00:23:45.324
the biggest thing that will ever
have happened in human history,

00:23:45.324 --> 00:23:47.490
at some point, this transition
to superintelligence.

00:23:47.490 --> 00:23:49.990
There are two possible
pathways, in principle,

00:23:49.990 --> 00:23:52.090
one could imagine
that could lead there.

00:23:52.090 --> 00:23:54.550
You could enhance
biological intelligence.

00:23:54.550 --> 00:24:00.720
We know biological intelligence
has increased radically

00:24:00.720 --> 00:24:04.970
in the past, in kind of
making the human species.

00:24:04.970 --> 00:24:08.230
Or machine intelligence,
which is still

00:24:08.230 --> 00:24:10.450
far below biological
intelligence,

00:24:10.450 --> 00:24:13.190
insofar as we're
focusing on any form

00:24:13.190 --> 00:24:15.810
of general-purpose smartness
and learning ability,

00:24:15.810 --> 00:24:18.461
but increasing at
a more rapid clip.

00:24:21.100 --> 00:24:26.636
So specifically, you can
imagine interventions

00:24:26.636 --> 00:24:29.010
on some individual brain to
enhance biological condition.

00:24:29.010 --> 00:24:33.320
I'll say a few words
about that just shortly.

00:24:33.320 --> 00:24:35.080
Or improvements
in our ability to

00:24:35.080 --> 00:24:38.240
pool our individual
information processing devices

00:24:38.240 --> 00:24:41.400
to enhance our collective
rationality and wisdom.

00:24:41.400 --> 00:24:43.270
I won't talk about
that, but that's clearly

00:24:43.270 --> 00:24:46.394
an exciting frontier, with the
internet and new institutions,

00:24:46.394 --> 00:24:48.310
prediction markets and
other things like that.

00:24:51.820 --> 00:24:53.630
There are some kind
of hybrid approaches,

00:24:53.630 --> 00:24:58.560
it can vary between biology and
machines, the cyborg approach.

00:24:58.560 --> 00:25:01.130
I personally don't
think that that's

00:25:01.130 --> 00:25:03.510
where the action will be.

00:25:03.510 --> 00:25:06.750
It just seems to me very
difficult, technologically

00:25:06.750 --> 00:25:10.890
speaking, to create implants
that would really significantly

00:25:10.890 --> 00:25:14.880
enhance our cognitive ability
more than you could have

00:25:14.880 --> 00:25:18.155
by having the same device
outside of yourself.

00:25:18.155 --> 00:25:20.530
So you could say, wouldn't it
be great with a little chip

00:25:20.530 --> 00:25:22.155
in the brain, and
you could Google just

00:25:22.155 --> 00:25:23.390
by thinking about it?

00:25:23.390 --> 00:25:25.150
And well, I mean, I
can already Google,

00:25:25.150 --> 00:25:27.030
and I don't have to
have neurosurgery

00:25:27.030 --> 00:25:28.890
to be able to do that.

00:25:28.890 --> 00:25:31.760
We have these amazing interfaces
like the eyeballs, that

00:25:31.760 --> 00:25:34.610
can protect 100 million
bits per second,

00:25:34.610 --> 00:25:37.960
straight into dedicated
neural wetware that's

00:25:37.960 --> 00:25:41.400
highly optimized for making
sense of this information.

00:25:41.400 --> 00:25:44.330
And it's really hard
to beat that, I think.

00:25:44.330 --> 00:25:48.260
In any case, I mean, the rate
at which sensory information

00:25:48.260 --> 00:25:51.375
can be entered into the brain is
not really the limiting factor.

00:25:51.375 --> 00:25:53.000
The first thing the
brain does with all

00:25:53.000 --> 00:25:55.500
of this visual information is
to throw away almost all of it

00:25:55.500 --> 00:25:58.610
and just extract
the relevant part.

00:25:58.610 --> 00:26:02.700
And then different versions
of machine intelligence,

00:26:02.700 --> 00:26:04.690
where on the one
hand, we have sort

00:26:04.690 --> 00:26:09.130
of purely synthetic methods
that don't care about biology

00:26:09.130 --> 00:26:11.870
but try to make progress in
mathematics and statistics

00:26:11.870 --> 00:26:14.510
and figure out
clever algorithms.

00:26:14.510 --> 00:26:18.910
And two, approaches that
try to learn from this one

00:26:18.910 --> 00:26:21.410
general intelligence system
that already exists, that we can

00:26:21.410 --> 00:26:24.790
study the human brain for
inspiration from that,

00:26:24.790 --> 00:26:26.280
or maybe even
reverse-engineer it.

00:26:26.280 --> 00:26:29.360
Or in the limiting case,
literally copying it

00:26:29.360 --> 00:26:31.460
in whole-brain
emulation, where you

00:26:31.460 --> 00:26:40.660
would take a particular
human brain and freeze it

00:26:40.660 --> 00:26:42.530
and slice it up,
feed those slices

00:26:42.530 --> 00:26:45.800
through an array of microscopes
to take good pictures.

00:26:45.800 --> 00:26:47.950
So you have a stack
of these pictures

00:26:47.950 --> 00:26:49.930
and use automated
image-recognition software

00:26:49.930 --> 00:26:54.420
to extract the connectivity
matrix of the neural network

00:26:54.420 --> 00:26:56.560
that's was in the
original brain.

00:26:56.560 --> 00:26:59.760
And then annotate that with
neurocomputational models

00:26:59.760 --> 00:27:01.980
of each type of neuron works.

00:27:01.980 --> 00:27:04.121
And finally, run
that whole emulation

00:27:04.121 --> 00:27:05.620
on a sufficiently
powerful computer.

00:27:08.330 --> 00:27:11.100
That would require some very
advanced enabling technology

00:27:11.100 --> 00:27:15.640
that we don't yet have, so
we know that that is not just

00:27:15.640 --> 00:27:17.970
around the corner.

00:27:17.970 --> 00:27:19.840
On the other hand,
it would not require

00:27:19.840 --> 00:27:21.750
any theoretical breakthrough.

00:27:21.750 --> 00:27:24.340
It would not require any new,
deep conceptual understanding

00:27:24.340 --> 00:27:25.256
of how thinking works.

00:27:25.256 --> 00:27:26.650
You would only
need to understand

00:27:26.650 --> 00:27:29.030
the components of
the brain to be

00:27:29.030 --> 00:27:32.690
able to make progress with that.

00:27:32.690 --> 00:27:38.240
So it's an open question which
of these will get there first.

00:27:38.240 --> 00:27:41.505
Different researchers have
their own favorite bets on that.

00:27:41.505 --> 00:27:43.130
One thought that
sometimes is put to me

00:27:43.130 --> 00:27:46.920
is that-- OK, so Nick, you're
worried about this AI stuff.

00:27:46.920 --> 00:27:49.160
So maybe what we
should do is really

00:27:49.160 --> 00:27:52.870
try to push ahead with
biological enhancement,

00:27:52.870 --> 00:27:55.340
so that we can kind of
keep up with the computer.

00:27:55.340 --> 00:27:56.840
The computer's going
to get smarter,

00:27:56.840 --> 00:27:59.339
but maybe if we enhance our own
intelligence rapidly enough,

00:27:59.339 --> 00:28:01.330
we can keep one step ahead.

00:28:01.330 --> 00:28:03.950
I think that that's misguided.

00:28:03.950 --> 00:28:06.294
And in fact, if we
do figure out ways

00:28:06.294 --> 00:28:07.710
to enhance biological
cognition, I

00:28:07.710 --> 00:28:12.110
think that will only hasten the
time when machines overtake us.

00:28:12.110 --> 00:28:15.130
Because basically, we will have
smarter people doing the AI

00:28:15.130 --> 00:28:16.940
research and the
computer science,

00:28:16.940 --> 00:28:21.370
and they will solve
the problem faster.

00:28:21.370 --> 00:28:23.610
I still think that would
probably have reason

00:28:23.610 --> 00:28:27.595
to try to accelerate biological
cognitive development.

00:28:27.595 --> 00:28:29.720
But not so that we can keep
ahead of the computers,

00:28:29.720 --> 00:28:31.700
but that so that
when the time comes

00:28:31.700 --> 00:28:33.670
where we will create
intelligent machines,

00:28:33.670 --> 00:28:37.922
that we will be more
competent at doing so.

00:28:37.922 --> 00:28:40.380
So let me just say a few words
on this biological cognitive

00:28:40.380 --> 00:28:42.660
enhancement, because it
might be-- especially

00:28:42.660 --> 00:28:45.330
if you think of arrival dates
for artificial intelligence,

00:28:45.330 --> 00:28:46.913
where it's not just
around the corner,

00:28:46.913 --> 00:28:51.200
but maybe it will happen in
the latter half of this century

00:28:51.200 --> 00:28:54.140
or something like that-- by that
time, that could be enough time

00:28:54.140 --> 00:28:57.500
to have a new cohort of
cognitively enhanced people

00:28:57.500 --> 00:28:58.000
around.

00:28:58.000 --> 00:29:00.990
And the technology
that I think will first

00:29:00.990 --> 00:29:03.890
enable cognitive
enhancement-- my best guess is

00:29:03.890 --> 00:29:07.580
that it'll be through
genetic interventions.

00:29:07.580 --> 00:29:09.620
There are other paths,
obviously-- smart drugs

00:29:09.620 --> 00:29:10.120
and such.

00:29:10.120 --> 00:29:12.660
I just-- I don't
hold out much hope

00:29:12.660 --> 00:29:15.290
that they will do a
great deal to improve

00:29:15.290 --> 00:29:18.330
general-purpose smartness.

00:29:18.330 --> 00:29:21.655
They might-- if there
were a simple chemical

00:29:21.655 --> 00:29:23.655
that you could just inject
and it would make you

00:29:23.655 --> 00:29:25.696
a lot smarter, I think
evolution would find a way

00:29:25.696 --> 00:29:27.980
to endogenously
produce that chemical.

00:29:27.980 --> 00:29:30.450
I think there might
be ways to improve

00:29:30.450 --> 00:29:35.040
some peripheral characteristics,
like mental energy, say,

00:29:35.040 --> 00:29:35.890
or concentration.

00:29:35.890 --> 00:29:39.510
And we can see that evolution
would have optimized us

00:29:39.510 --> 00:29:41.300
for a certain type
of environment

00:29:41.300 --> 00:29:43.530
where there are
trade-offs between, maybe,

00:29:43.530 --> 00:29:45.820
metabolic consumption
of the brain

00:29:45.820 --> 00:29:47.925
and the amount of
mental energy you have.

00:29:47.925 --> 00:29:50.610
And in the environment of
evolutionary adaptiveness,

00:29:50.610 --> 00:29:53.110
the optimum point for that
trade-off is at one place,

00:29:53.110 --> 00:29:54.700
and now we want to move that.

00:29:54.700 --> 00:29:57.894
And maybe it could have some
stimulant that just increases

00:29:57.894 --> 00:30:00.310
the burn rate of calories and
give you more mental energy.

00:30:00.310 --> 00:30:02.307
So peripheral
adjustments like that, I

00:30:02.307 --> 00:30:03.890
think we could do
maybe through drugs.

00:30:03.890 --> 00:30:07.650
But raw cleverness,
I think genetics

00:30:07.650 --> 00:30:11.190
is a more likely initial
technology to do that.

00:30:11.190 --> 00:30:14.460
And so one way that
that could work

00:30:14.460 --> 00:30:18.510
is in the context of in
vitro fertilization, where

00:30:18.510 --> 00:30:22.210
you have normally, in the
course of standard fertility

00:30:22.210 --> 00:30:26.580
procedure, maybe some six,
eight, or ten eggs produced.

00:30:26.580 --> 00:30:30.350
And then the doctor chooses
one of those to implant.

00:30:30.350 --> 00:30:35.360
And at the moment, you can look
for like obvious abnormalities.

00:30:35.360 --> 00:30:39.871
You might screen for
some monogenic disorders

00:30:39.871 --> 00:30:41.370
or for Down syndrome,
which is done.

00:30:41.370 --> 00:30:43.180
But you can't really
select positively

00:30:43.180 --> 00:30:46.390
for some complex trait
today, because we don't yet

00:30:46.390 --> 00:30:53.230
know the genetic architecture
for, say, intelligence.

00:30:53.230 --> 00:30:56.690
But we will, I think,
soon know that,

00:30:56.690 --> 00:31:01.280
because the price of gene
sequencing is falling,

00:31:01.280 --> 00:31:03.420
and it's now coming
down sufficiently where

00:31:03.420 --> 00:31:06.890
it is becoming feasible to run
these very large-scale studies

00:31:06.890 --> 00:31:10.340
with hundreds of thousands
or even millions of subjects.

00:31:10.340 --> 00:31:16.010
And because it turns out that
the additive heritability,

00:31:16.010 --> 00:31:17.760
the variance in that
in humans, is not

00:31:17.760 --> 00:31:19.540
due to like one
or two genes that

00:31:19.540 --> 00:31:24.010
differ in between us, but a
lot of genes-- maybe hundreds,

00:31:24.010 --> 00:31:26.390
maybe even a few
thousand-- that each

00:31:26.390 --> 00:31:29.050
have a very, very small effect.

00:31:29.050 --> 00:31:32.090
And so to discover a
very, very small effect,

00:31:32.090 --> 00:31:35.236
you need a very
large sample size.

00:31:35.236 --> 00:31:37.110
And so you need to
sequence a lot of genomes,

00:31:37.110 --> 00:31:39.070
and that was too
expensive to do, really.

00:31:39.070 --> 00:31:41.069
But now there are studies
underway with hundreds

00:31:41.069 --> 00:31:44.010
of thousands of people,
and maybe soon millions.

00:31:44.010 --> 00:31:46.232
So that, I think,
will tell us some

00:31:46.232 --> 00:31:47.940
of this information
that would be needed.

00:31:47.940 --> 00:31:50.540
And then to start doing this,
nothing else would be required.

00:31:50.540 --> 00:31:51.700
No new technologies at all.

00:31:51.700 --> 00:31:53.890
You just have the information
and you sequence it

00:31:53.890 --> 00:31:56.790
and select the
embryos based on that.

00:31:56.790 --> 00:32:01.070
Now this would be vastly
potentiated if it were combined

00:32:01.070 --> 00:32:02.925
with another technology
that we don't yet

00:32:02.925 --> 00:32:05.100
have ready for use in
humans, which is the ability

00:32:05.100 --> 00:32:09.440
to derive gamete
from stem cells.

00:32:09.440 --> 00:32:12.620
So then you could do
iterated embryo selection.

00:32:12.620 --> 00:32:15.860
We would generate an
initial pool of embryos,

00:32:15.860 --> 00:32:19.280
select one that's highest
in the expected trait

00:32:19.280 --> 00:32:21.970
value of interest, and
then use that embryo

00:32:21.970 --> 00:32:25.330
to derive gametes-- sperm
and ova-- that you could then

00:32:25.330 --> 00:32:27.720
recombine to get a
new set of embryos.

00:32:27.720 --> 00:32:31.030
You pick out the best of
those, and you repeat.

00:32:31.030 --> 00:32:35.360
So this technology
here, the ability

00:32:35.360 --> 00:32:39.760
to create artificial
gametes through stem cells,

00:32:39.760 --> 00:32:42.040
has been developed
and done in mice.

00:32:42.040 --> 00:32:46.280
But a significant amount
of additional work

00:32:46.280 --> 00:32:48.680
would be required to make
it safe for use in humans.

00:32:48.680 --> 00:32:52.220
But if you had this-- and this
might take anything from 10

00:32:52.220 --> 00:32:53.720
to 30 or 40 years.

00:32:53.720 --> 00:32:55.854
It's hard to know.

00:32:55.854 --> 00:33:00.320
This would have the effect of
collapsing the human generation

00:33:00.320 --> 00:33:06.010
cycle from 20, 30 years
to a couple of months.

00:33:06.010 --> 00:33:09.870
And so if you imagine this kind
of old mad scientist eugenics

00:33:09.870 --> 00:33:12.810
program where they would breed
humans for like 500 years,

00:33:12.810 --> 00:33:17.420
and make very sure who mated
with whom-- which setting aside

00:33:17.420 --> 00:33:20.980
all the ethical complications
involved in that, which

00:33:20.980 --> 00:33:23.782
are legion, but I'm not going
to talk about them here,

00:33:23.782 --> 00:33:25.490
not because I don't
think they are there,

00:33:25.490 --> 00:33:28.200
but I just want to focus
on the technical stuff--

00:33:28.200 --> 00:33:32.110
it's just infeasible on a
lot of different levels.

00:33:32.110 --> 00:33:33.860
But here, you would
instead have something

00:33:33.860 --> 00:33:35.922
that could be done--
instead of 500 years,

00:33:35.922 --> 00:33:37.380
you could have it
done over a year.

00:33:37.380 --> 00:33:39.260
And instead of changing
the breeding patterns

00:33:39.260 --> 00:33:40.760
of large populations,
you would have

00:33:40.760 --> 00:33:44.950
a Petri dish and a scientist
plucking around in that.

00:33:44.950 --> 00:33:48.250
And so through
that, you would be

00:33:48.250 --> 00:33:52.660
able to probably achieve sort of
weak forms of superintelligence

00:33:52.660 --> 00:33:54.596
in biology.

00:33:54.596 --> 00:33:57.990
I did an analysis
with a colleague

00:33:57.990 --> 00:34:00.310
of mine, Carl Shulman,
quite recently

00:34:00.310 --> 00:34:03.290
where we tried to estimate,
for different assumptions

00:34:03.290 --> 00:34:05.300
about the selection
power applied,

00:34:05.300 --> 00:34:08.530
what the gain in
intelligence would be.

00:34:08.530 --> 00:34:10.690
And so you can see
here that if you just

00:34:10.690 --> 00:34:14.026
produce two random embryos
and select the best one-- not

00:34:14.026 --> 00:34:16.400
the one that's actually best,
but the one that looks most

00:34:16.400 --> 00:34:20.620
promising, to the
extent that there

00:34:20.620 --> 00:34:22.250
is an additive
genetic heritability,

00:34:22.250 --> 00:34:24.920
you may get four IQ
points from that.

00:34:24.920 --> 00:34:28.219
So if instead, you select
the best of 1 in 10, or 11,

00:34:28.219 --> 00:34:30.920
you can see here, even if you
could select the best of 1

00:34:30.920 --> 00:34:34.810
in 1,000, you only get
maybe 24 IQ points.

00:34:34.810 --> 00:34:36.650
This is with
single-shot selection.

00:34:36.650 --> 00:34:39.380
But if you did this
iterated embryo selection,

00:34:39.380 --> 00:34:43.489
and you could do five
generations of selecting

00:34:43.489 --> 00:34:46.190
the best of 1 in
10, then you might

00:34:46.190 --> 00:34:48.909
get as many as 65 IQ points.

00:34:48.909 --> 00:34:51.810
And with 10
generations of 1 in 10,

00:34:51.810 --> 00:34:56.520
you'd get far above what
we've had in human history.

00:34:56.520 --> 00:35:01.110
You'd get the kind of
phenotypes that have never

00:35:01.110 --> 00:35:03.650
existed in all of human history,
the [INAUDIBLE] and stuff

00:35:03.650 --> 00:35:04.400
like that.

00:35:04.400 --> 00:35:08.560
So you observe here that while
you get quickly diminishing

00:35:08.560 --> 00:35:10.890
returns by just doing
one-shot selection

00:35:10.890 --> 00:35:15.330
from a pool of
embryos, you largely

00:35:15.330 --> 00:35:17.980
avoid that by doing
this iterated selection.

00:35:21.090 --> 00:35:25.620
So yeah, I'm going
to skip through this.

00:35:25.620 --> 00:35:27.880
Yeah, so that does
look like it should

00:35:27.880 --> 00:35:30.810
be feasible without any sort of
magical new technology coming

00:35:30.810 --> 00:35:31.310
around.

00:35:31.310 --> 00:35:34.800
And then that also, I think,
adds to the possibility

00:35:34.800 --> 00:35:36.376
that we will eventually
get AI stuff.

00:35:36.376 --> 00:35:38.500
Like that would be towards
the end of this century,

00:35:38.500 --> 00:35:40.720
if we haven't already
solved the problem by then,

00:35:40.720 --> 00:35:44.050
with this significantly more
capable generation of humans

00:35:44.050 --> 00:35:46.160
working on it.

00:35:46.160 --> 00:35:49.810
But ultimately, we
will be surpassed

00:35:49.810 --> 00:35:52.150
by intelligent
machines, assuming

00:35:52.150 --> 00:35:56.030
we haven't succumbed to
existential catastrophe

00:35:56.030 --> 00:35:58.590
prior to that, just
because the fundamental

00:35:58.590 --> 00:36:00.980
image-to-information processing
in the machine substrate

00:36:00.980 --> 00:36:02.810
or just far beyond
those in biology,

00:36:02.810 --> 00:36:04.690
like in terms of speed.

00:36:04.690 --> 00:36:09.440
Even transistors today are
far faster than neurons.

00:36:09.440 --> 00:36:11.447
So I'm going to--
this is not relevant,

00:36:11.447 --> 00:36:13.030
for you guys already
know all of this.

00:36:13.030 --> 00:36:15.810
So there is, like, progress
in AI, like-- I'm just

00:36:15.810 --> 00:36:17.449
saying the public
consciousness is

00:36:17.449 --> 00:36:19.990
shaped by a few big milestones,
but there's a lot of progress

00:36:19.990 --> 00:36:21.620
under the hood.

00:36:21.620 --> 00:36:24.776
Also hardware has driven a
lot of progress we've seen.

00:36:24.776 --> 00:36:26.650
Here is a slice that
could have been earlier.

00:36:26.650 --> 00:36:28.233
This is like with
the brain emulation.

00:36:28.233 --> 00:36:30.860
This is basically the
state of the art today.

00:36:30.860 --> 00:36:34.580
Here's a brain slice scanned
with an electron micrograph.

00:36:34.580 --> 00:36:37.370
Here is a stack of those
pictures on top of one another.

00:36:37.370 --> 00:36:40.910
And here is the
result of applying

00:36:40.910 --> 00:36:43.410
an image-recognition
algorithm to extract

00:36:43.410 --> 00:36:44.420
the connectivity matrix.

00:36:44.420 --> 00:36:46.340
But although we have
the right resolution--

00:36:46.340 --> 00:36:48.846
you can see individual atoms,
if you want to, in the brain.

00:36:48.846 --> 00:36:50.220
It's just that to
image the brain

00:36:50.220 --> 00:36:54.550
with that level of resolution
would take, like, forever,

00:36:54.550 --> 00:36:59.730
so that we're presumably at
least decades away from making

00:36:59.730 --> 00:37:01.590
something like that work.

00:37:01.590 --> 00:37:04.220
Lot of application
there [INAUDIBLE].

00:37:04.220 --> 00:37:07.870
So the question of how far away
we are from human-level machine

00:37:07.870 --> 00:37:10.370
intelligence, I think the short
answer is that nobody knows.

00:37:10.370 --> 00:37:13.470
We did a survey of leading
AI experts last year,

00:37:13.470 --> 00:37:16.240
and one of the questions
we asked was, by what year

00:37:16.240 --> 00:37:19.130
do you think there is a 50%
chance that we will have

00:37:19.130 --> 00:37:21.000
human-level machine
intelligence?

00:37:21.000 --> 00:37:22.390
Here defined as
one that could do

00:37:22.390 --> 00:37:24.650
most jobs that humans could do.

00:37:24.650 --> 00:37:28.500
And so the median answer to
that we got was 2050 or 2040,

00:37:28.500 --> 00:37:32.200
depending exactly which
group of experts we asked.

00:37:32.200 --> 00:37:35.270
That seems, to me,
roughly reasonable,

00:37:35.270 --> 00:37:36.500
for what it's worth.

00:37:36.500 --> 00:37:37.940
We also asked, by
what year do you

00:37:37.940 --> 00:37:39.610
think there's a 90% probability?

00:37:39.610 --> 00:37:43.050
And we got 2070 or 2075.

00:37:43.050 --> 00:37:45.850
That, to me, seems
overconfident.

00:37:45.850 --> 00:37:48.600
There is just a lot more than
10% probability, I think,

00:37:48.600 --> 00:37:51.750
that we will still
have failed by then.

00:37:51.750 --> 00:37:54.280
I should say, as a footnote,
that these estimates were

00:37:54.280 --> 00:37:58.520
conditioned on no global
collapse occurring.

00:38:01.520 --> 00:38:05.250
So-- so maybe the
numbers would be--

00:38:05.250 --> 00:38:07.220
like the years would
be slightly higher

00:38:07.220 --> 00:38:10.520
up if we hadn't made
that assumption.

00:38:10.520 --> 00:38:14.070
We also asked, if and when we
do reach human-level machine

00:38:14.070 --> 00:38:15.550
intelligence, how
long do you think

00:38:15.550 --> 00:38:19.620
it will take from there to go to
some radical superintelligence?

00:38:19.620 --> 00:38:21.560
And you can see for
yourself the answer there.

00:38:21.560 --> 00:38:24.280
Now here, again,
my view disagrees

00:38:24.280 --> 00:38:26.870
with those of people we sampled.

00:38:26.870 --> 00:38:29.855
I think-- I'm quite agnostic as
to how far away we are from AI.

00:38:29.855 --> 00:38:32.480
I think we should basically have
a very smeared-out probability

00:38:32.480 --> 00:38:33.729
distribution.

00:38:33.729 --> 00:38:36.020
I do think there is a fairly
large probability, though,

00:38:36.020 --> 00:38:38.560
that if and when we
get to human-ish level,

00:38:38.560 --> 00:38:40.350
we will soon after
have superintelligence.

00:38:40.350 --> 00:38:42.770
I place a fairly high
credence on there

00:38:42.770 --> 00:38:45.540
being, at some point, an
intelligence explosion.

00:38:45.540 --> 00:38:47.770
So we need to sharply
separate the two questions,

00:38:47.770 --> 00:38:52.320
like the distance between
now and human-level,

00:38:52.320 --> 00:38:54.780
and the distance in
time between that

00:38:54.780 --> 00:38:56.030
and radical superintelligence.

00:38:56.030 --> 00:39:02.900
I think this transition
might well be very rapid.

00:39:02.900 --> 00:39:06.490
And things depend on that.

00:39:06.490 --> 00:39:08.400
So if you distinguish,
like qualitatively,

00:39:08.400 --> 00:39:10.820
like fast-take of
scenarios, where

00:39:10.820 --> 00:39:12.820
we go from something
human-ish level

00:39:12.820 --> 00:39:16.235
to superintelligence within
minutes or hours or days,

00:39:16.235 --> 00:39:19.380
a couple of weeks, in
that kind of scenario,

00:39:19.380 --> 00:39:21.360
it happens too fast
for us to really

00:39:21.360 --> 00:39:25.802
be able to do anything much
about it while it is happening.

00:39:25.802 --> 00:39:27.260
If we get a desirable
outcome, it's

00:39:27.260 --> 00:39:31.960
because we set up the initial
conditions just right.

00:39:31.960 --> 00:39:34.950
By contrast, if one
contemplates very slow take-up--

00:39:34.950 --> 00:39:36.760
so you have some
human-level system,

00:39:36.760 --> 00:39:39.464
and then only by
laboriously adding

00:39:39.464 --> 00:39:41.880
additional little incremental
capability after capability,

00:39:41.880 --> 00:39:43.790
so it takes like
decades or centuries

00:39:43.790 --> 00:39:46.160
to work your way up
to superintelligence,

00:39:46.160 --> 00:39:49.870
then that would be a lot of more
time for new human institutions

00:39:49.870 --> 00:39:53.461
to arise to deal with
this, like to develop

00:39:53.461 --> 00:39:55.960
a new profession of experts to
deal with this, to try things

00:39:55.960 --> 00:39:57.870
out, see what works,
and then change it up.

00:40:00.069 --> 00:40:01.110
So it makes a difference.

00:40:01.110 --> 00:40:02.860
Another way in which
it makes a difference

00:40:02.860 --> 00:40:04.760
is that in the fast
takeoff scenarios,

00:40:04.760 --> 00:40:08.270
it's likely that you will have
a singleton outcome, I think.

00:40:08.270 --> 00:40:09.860
Which is basically
a world order where

00:40:09.860 --> 00:40:12.240
at the highest level
of decision-making,

00:40:12.240 --> 00:40:15.750
there's like one
decision-making agency.

00:40:15.750 --> 00:40:18.480
If you think about competing
technology projects,

00:40:18.480 --> 00:40:20.950
whether it's nations
racing to build satellites,

00:40:20.950 --> 00:40:23.600
or nuclear weapons, or
competing tech products,

00:40:23.600 --> 00:40:25.427
often there's some
competition, and you're

00:40:25.427 --> 00:40:26.510
trying to get there first.

00:40:26.510 --> 00:40:29.450
But it's rare that the
difference between the leader

00:40:29.450 --> 00:40:32.660
and the closest follower
is a couple of days.

00:40:32.660 --> 00:40:34.980
Like usually the leader
will be a few months ahead

00:40:34.980 --> 00:40:37.490
of the follower, or
a couple of years.

00:40:37.490 --> 00:40:40.040
So if the takeoff is going
to be over in a few days

00:40:40.040 --> 00:40:43.400
or a few weeks, then one project
will have completed a takeoff

00:40:43.400 --> 00:40:46.540
before the next one will
have started it, very likely.

00:40:46.540 --> 00:40:49.050
And then you will have a
mature superintelligence

00:40:49.050 --> 00:40:52.040
in a world which contains no
other even vaguely comparable

00:40:52.040 --> 00:40:53.200
system.

00:40:53.200 --> 00:40:57.140
And for reasons that I'll
be happy to elaborate on

00:40:57.140 --> 00:40:59.570
in the Q&amp;A, and that a
lot of the book is about,

00:40:59.570 --> 00:41:01.164
as well, this first
system then is

00:41:01.164 --> 00:41:03.330
likely to be very powerful,
maybe to the point where

00:41:03.330 --> 00:41:06.030
it is able to shape the
entire future according

00:41:06.030 --> 00:41:08.190
to its preferences.

00:41:08.190 --> 00:41:11.200
If you have a storied
takeoff, then it's more likely

00:41:11.200 --> 00:41:13.130
you're going to have
multiple outcomes.

00:41:13.130 --> 00:41:14.970
No system is so far
ahead of all the others

00:41:14.970 --> 00:41:17.420
that it can just
lay down the law.

00:41:17.420 --> 00:41:20.650
They end up
superintelligent, but it

00:41:20.650 --> 00:41:24.010
will have economic
competitive forces

00:41:24.010 --> 00:41:25.570
and evolutionary
dynamics working

00:41:25.570 --> 00:41:29.570
on this population of digital
minds shaping the outcome.

00:41:29.570 --> 00:41:32.330
And the concerns in
that type of scenario

00:41:32.330 --> 00:41:34.320
look very different
from the ones

00:41:34.320 --> 00:41:36.130
in the singleton scenarios.

00:41:36.130 --> 00:41:39.640
Not necessarily less
serious, but different.

00:41:39.640 --> 00:41:43.800
So instead of having one agency
that can dictate the future,

00:41:43.800 --> 00:41:47.615
you now have this
ecology of digital minds.

00:41:51.662 --> 00:41:53.120
And you can think--
I mean, suppose

00:41:53.120 --> 00:41:58.070
to take a model-- so once we
had human-level minds that

00:41:58.070 --> 00:42:01.260
were digital, like they could do
exactly the same as humans do,

00:42:01.260 --> 00:42:02.970
and run at the same
speed, initially--

00:42:02.970 --> 00:42:05.190
suppose that you get there
through whole-brain emulation,

00:42:05.190 --> 00:42:06.960
and this is the first
type of AI you have.

00:42:09.870 --> 00:42:12.490
Then you could very quickly
have a population explosion.

00:42:12.490 --> 00:42:14.130
So we know how to copy software.

00:42:14.130 --> 00:42:15.990
That takes a couple of minutes.

00:42:15.990 --> 00:42:17.700
And so as long as
the productivity

00:42:17.700 --> 00:42:21.700
of these digital mind is
higher than the cost of making

00:42:21.700 --> 00:42:24.670
another copy, there
would be vast incentives

00:42:24.670 --> 00:42:28.680
to just keep making more
copies until the income

00:42:28.680 --> 00:42:32.250
that digital minds
can earn equals, like,

00:42:32.250 --> 00:42:35.600
the price of electricity
and hardware rental.

00:42:35.600 --> 00:42:37.160
So you have a
Malthusian situation

00:42:37.160 --> 00:42:38.730
where the population
of digital minds

00:42:38.730 --> 00:42:42.297
expands until the wage
falls to subsistence level.

00:42:42.297 --> 00:42:44.130
But subsistence level
for the digital minds,

00:42:44.130 --> 00:42:45.700
rather than for
biological minds.

00:42:45.700 --> 00:42:50.960
So we are a lot more
expensive, because we

00:42:50.960 --> 00:42:56.080
have to eat and have houses to
live in and stuff like that.

00:42:56.080 --> 00:42:58.640
So humans might still be
able to make some income

00:42:58.640 --> 00:43:02.880
through their
capital investment.

00:43:02.880 --> 00:43:06.166
And there's then the question
of whether in this world, which

00:43:06.166 --> 00:43:08.040
is increasingly shaped
by the digital minds--

00:43:08.040 --> 00:43:09.790
there are trillions
and trillions of them,

00:43:09.790 --> 00:43:13.760
and they're getting faster
all the time, and better,

00:43:13.760 --> 00:43:16.361
and humans constitute a
small slice of all of this--

00:43:16.361 --> 00:43:18.110
whether we would be
able, in the long run,

00:43:18.110 --> 00:43:20.250
to really enforce
property rights

00:43:20.250 --> 00:43:22.130
and our sociopolitical
structures.

00:43:22.130 --> 00:43:24.088
Or whether these digital
minds would eventually

00:43:24.088 --> 00:43:27.110
just swamp us and
expropriate us.

00:43:27.110 --> 00:43:28.660
And at some point,
presumably, even

00:43:28.660 --> 00:43:30.990
in this whole-brain
emulation, at some point

00:43:30.990 --> 00:43:33.310
probably fairly soon
after that point,

00:43:33.310 --> 00:43:36.670
you will have
synthetic AIs that are

00:43:36.670 --> 00:43:39.150
more optimized than whatever
sort of structures biology

00:43:39.150 --> 00:43:41.705
came up with, that will then
kind of leave the [INAUDIBLE].

00:43:41.705 --> 00:43:43.580
So there is a chapter
in the book about that.

00:43:43.580 --> 00:43:46.190
But the bulk of the book
is-- so all the stuff

00:43:46.190 --> 00:43:48.670
that I talked about, like how
far we are from it and stuff

00:43:48.670 --> 00:43:51.352
like that, there's like
one chapter about that

00:43:51.352 --> 00:43:52.060
in the beginning.

00:43:52.060 --> 00:43:53.450
Maybe the second
chapter has something

00:43:53.450 --> 00:43:54.650
about different pathways.

00:43:54.650 --> 00:43:57.590
But the bulk of the book is
really about the question of,

00:43:57.590 --> 00:44:01.027
if and when we do
reach the ability

00:44:01.027 --> 00:44:03.360
to create human-level machine
intelligence-- so machines

00:44:03.360 --> 00:44:06.480
that are as good as we
are in computer science,

00:44:06.480 --> 00:44:09.800
so they can start to improve
themselves-- what happens then?

00:44:09.800 --> 00:44:12.204
And what happens when you
have a superintelligence that

00:44:12.204 --> 00:44:13.370
might be extremely powerful?

00:44:13.370 --> 00:44:16.220
What are the control
methods that we

00:44:16.220 --> 00:44:20.072
could try to apply to achieve a
controlled detonation if there

00:44:20.072 --> 00:44:21.780
is going to be an
intelligence explosion?

00:44:21.780 --> 00:44:23.529
How could we set up
the initial conditions

00:44:23.529 --> 00:44:27.030
to get some kind of
beneficial outcome?

00:44:27.030 --> 00:44:31.725
And there are a lot of
initially plausible ways

00:44:31.725 --> 00:44:34.100
to solve this problem that
turn out, on closer reflection

00:44:34.100 --> 00:44:34.641
not to worry.

00:44:34.641 --> 00:44:37.139
That this kind of one
of the types of progress

00:44:37.139 --> 00:44:38.430
at have occurred in this field.

00:44:38.430 --> 00:44:40.560
It's like a deepening
appreciation

00:44:40.560 --> 00:44:43.060
of just how profoundly
difficult this problem is,

00:44:43.060 --> 00:44:45.059
of how you could create
something vastly smarter

00:44:45.059 --> 00:44:47.559
than you and still ensure
a desirable outcome.

00:44:47.559 --> 00:44:48.850
So that's the bulk of the book.

00:44:48.850 --> 00:44:50.970
And then the last
two chapters are

00:44:50.970 --> 00:44:54.010
trying to think more generally
about these macrostrategic

00:44:54.010 --> 00:44:55.740
questions, and how
to think about what

00:44:55.740 --> 00:44:59.510
our levers of influence are,
if one wants to increase

00:44:59.510 --> 00:45:02.290
the probability of
a desirable outcome.

00:45:02.290 --> 00:45:04.000
So I'll put on the
pause there, because I

00:45:04.000 --> 00:45:06.291
want to make sure we get a
little bit of discussion in.

00:45:06.291 --> 00:45:06.862
Thanks.

00:45:06.862 --> 00:45:10.159
[APPLAUSE]

00:45:13.315 --> 00:45:14.440
MODERATOR: Thank you, Nick.

00:45:14.440 --> 00:45:17.880
We will use the microphone
for questions, please.

00:45:20.670 --> 00:45:22.850
AUDIENCE: I'll just
comment quickly.

00:45:22.850 --> 00:45:26.130
That 40-year median
for when we'll

00:45:26.130 --> 00:45:28.940
achieve human intelligence
is-- I've been tracking that.

00:45:28.940 --> 00:45:32.300
It was about 300 to
400 years in 1999.

00:45:32.300 --> 00:45:34.520
It was maybe 50 years in 2006.

00:45:34.520 --> 00:45:36.780
We took a poll at this
conference at Dartmouth.

00:45:36.780 --> 00:45:39.930
And now it's 40 years.

00:45:39.930 --> 00:45:44.227
I'm saying 2029, but it's
actually not so far off.

00:45:44.227 --> 00:45:46.560
I don't think we're going to
get that far with enhancing

00:45:46.560 --> 00:45:49.100
biological intelligence, because
our biological circuits are

00:45:49.100 --> 00:45:52.670
just inherently a million
times slower than electronics,

00:45:52.670 --> 00:45:56.400
and so there's only so
far you can get that way.

00:45:56.400 --> 00:45:59.270
Whole brain emulation is
useful not to create an AI,

00:45:59.270 --> 00:46:03.530
but to be able to emulate a
brain, or more likely a portion

00:46:03.530 --> 00:46:07.960
of a brain, to establish the
functional description of what

00:46:07.960 --> 00:46:13.260
these basic circuits do to
guide our creation of AI.

00:46:13.260 --> 00:46:16.055
My view, though, is that we are
emerging with this technology.

00:46:16.055 --> 00:46:19.280
I mean, it's already-- during
that one-day SOPA strike,

00:46:19.280 --> 00:46:21.360
I felt like part of my
brain went on strike.

00:46:21.360 --> 00:46:24.952
And so we're already
enhanced by these devices.

00:46:24.952 --> 00:46:27.410
When I was in college, I'd take
my bicycle to the computer,

00:46:27.410 --> 00:46:29.450
and now I carry it on my belt.

00:46:29.450 --> 00:46:32.930
I believe we will-- these
devices are getting smaller.

00:46:32.930 --> 00:46:35.710
I think within,
say, the '20s, '30s,

00:46:35.710 --> 00:46:37.610
they'll go inside
our bloodstream

00:46:37.610 --> 00:46:39.330
and go into our brain.

00:46:39.330 --> 00:46:42.010
Basically put our
neocortex on the cloud

00:46:42.010 --> 00:46:45.360
so we can extend the
300 million modules

00:46:45.360 --> 00:46:48.360
we have in the neocortex.

00:46:48.360 --> 00:46:51.040
In the cloud, there
will be a hybrid.

00:46:51.040 --> 00:46:52.540
But I would agree
that ultimately,

00:46:52.540 --> 00:46:55.920
the non-biological portion will
be so powerful that it will

00:46:55.920 --> 00:47:01.250
dominate, but that's a path to
getting to superintelligence.

00:47:01.250 --> 00:47:04.280
But I would argue that
the non-biological portion

00:47:04.280 --> 00:47:05.530
is human intelligence.

00:47:05.530 --> 00:47:07.600
I don't think it's
non-human just

00:47:07.600 --> 00:47:10.286
because it's non-biological.

00:47:10.286 --> 00:47:11.660
NICK BOSTROM: Yes,
so whether-- I

00:47:11.660 --> 00:47:14.990
mean-- I guess one
doesn't want to be bogged

00:47:14.990 --> 00:47:16.790
down in the
terminology of whether,

00:47:16.790 --> 00:47:20.680
like-- it seems clear to
me to call it non-human.

00:47:20.680 --> 00:47:24.350
But the idea that
it's implemented

00:47:24.350 --> 00:47:27.530
in machine substrate
to me doesn't

00:47:27.530 --> 00:47:29.970
begin to answer the question
of whether the outcome is

00:47:29.970 --> 00:47:30.710
desirable or not.

00:47:30.710 --> 00:47:32.668
To me, it would all depend
on exactly what kind

00:47:32.668 --> 00:47:35.717
of intelligence is there
in this machine substrate,

00:47:35.717 --> 00:47:36.800
and what this is it doing?

00:47:36.800 --> 00:47:38.790
What is it using
its resources for?

00:47:38.790 --> 00:47:44.117
Like I could-- you
could imagine that we're

00:47:44.117 --> 00:47:45.950
discovering that we are
all in a simulation,

00:47:45.950 --> 00:47:47.033
we're already all digital.

00:47:47.033 --> 00:47:47.660
Like so what?

00:47:47.660 --> 00:47:49.940
I mean, that doesn't mean
that human life doesn't

00:47:49.940 --> 00:47:53.450
have any moral significance just
because we're not biological,

00:47:53.450 --> 00:47:54.230
as we thought.

00:47:54.230 --> 00:47:56.146
So in principle, you
could have a digital mind

00:47:56.146 --> 00:47:59.000
with exactly the same experience
and capabilities as we do,

00:47:59.000 --> 00:48:01.260
and presumably it should
count for the same morally.

00:48:01.260 --> 00:48:04.720
However, there are a lot of
really bizarre types of minds

00:48:04.720 --> 00:48:10.570
that are possible in principle,
and I think one of the slides

00:48:10.570 --> 00:48:14.630
further down, and one of the key
questions that the book tries

00:48:14.630 --> 00:48:16.830
to answer, is how can we
think about the motivations

00:48:16.830 --> 00:48:19.620
of superintelligent agents?

00:48:19.620 --> 00:48:23.050
Is it possible to say
something useful about what

00:48:23.050 --> 00:48:24.397
they would want to do?

00:48:24.397 --> 00:48:25.980
AUDIENCE: We're all
evolving together.

00:48:25.980 --> 00:48:27.610
There's, like, 2
billion people that

00:48:27.610 --> 00:48:29.950
are enhanced with
these devices now.

00:48:29.950 --> 00:48:31.720
And as they get more
intimate with us,

00:48:31.720 --> 00:48:34.300
it's not going to be like these
science futures of movies,

00:48:34.300 --> 00:48:39.640
of one evil corporation that's
got got this technology.

00:48:39.640 --> 00:48:43.700
It's going to be billions
of us that enhance together,

00:48:43.700 --> 00:48:46.740
like it is today.

00:48:46.740 --> 00:48:48.476
NICK BOSTROM:
Yeah, so the growth

00:48:48.476 --> 00:48:49.600
of collective intelligence.

00:48:49.600 --> 00:48:51.058
I mean, I think
that at some point,

00:48:51.058 --> 00:48:55.910
the fleshy parts that
are in crania will-- a.,

00:48:55.910 --> 00:48:57.850
they will be a lot
harder to enhance,

00:48:57.850 --> 00:49:00.790
and they will become just
kind of negligible part

00:49:00.790 --> 00:49:05.120
of the actual intelligence
that is created.

00:49:05.120 --> 00:49:07.959
And that everything
then depends upon us

00:49:07.959 --> 00:49:09.500
having set up the
initial conditions.

00:49:09.500 --> 00:49:11.791
So, like, superintelligence
will be extremely powerful.

00:49:11.791 --> 00:49:14.800
We have the one advantage, that
we get to make the first move.

00:49:14.800 --> 00:49:17.302
And I think we only
get one try there.

00:49:17.302 --> 00:49:19.760
Because once you have like an
unfriendly superintelligence,

00:49:19.760 --> 00:49:23.290
it will resist you sort
of changing its values.

00:49:23.290 --> 00:49:25.640
And so part of what makes
the problem so challenging

00:49:25.640 --> 00:49:28.680
is that you need to get it
right on the first attempt,

00:49:28.680 --> 00:49:30.650
and humans are generally
not very good at that.

00:49:30.650 --> 00:49:32.816
We like to sort of see how
things work out and patch

00:49:32.816 --> 00:49:35.070
things up and learn
from experience.

00:49:35.070 --> 00:49:37.570
AUDIENCE: I want
to explore what you

00:49:37.570 --> 00:49:41.670
mean when you say a desirable
outcome, what desirable means.

00:49:41.670 --> 00:49:45.115
There this old philosophical
problem of the utility monster.

00:49:45.115 --> 00:49:50.060
It's sort of a challenge to a
utilitarian notion of morality,

00:49:50.060 --> 00:49:53.350
which is, imagine that
there's some creature that

00:49:53.350 --> 00:49:58.930
wants something more than the
rest of humanity combined,

00:49:58.930 --> 00:50:00.760
feeding the one
thing that it wants

00:50:00.760 --> 00:50:02.260
because it wants
it so much more.

00:50:02.260 --> 00:50:06.740
Maximizes utility, ignoring
the rest of humanity.

00:50:06.740 --> 00:50:09.530
So in some sense, the
superintelligence scenario

00:50:09.530 --> 00:50:13.330
can give life to the
utility monster in the sense

00:50:13.330 --> 00:50:18.870
that if the cognition
after the explosion

00:50:18.870 --> 00:50:23.360
is vastly greater than the total
sum of cognition of humanity,

00:50:23.360 --> 00:50:27.210
then perhaps the moral
consideration of what

00:50:27.210 --> 00:50:30.550
a desirable outcome should be
should only be paying attention

00:50:30.550 --> 00:50:32.297
to what it wants,
not what we want.

00:50:32.297 --> 00:50:33.130
NICK BOSTROM: Right.

00:50:33.130 --> 00:50:35.254
AUDIENCE: So I wanted to
raise that as a challenge.

00:50:35.254 --> 00:50:36.870
I'm not advocating
that perspective,

00:50:36.870 --> 00:50:42.580
I want to see how you reason
about desirability in a world

00:50:42.580 --> 00:50:45.300
where we're coexisting
with superintelligence.

00:50:45.300 --> 00:50:47.700
NICK BOSTROM: Yeah,
generally speaking,

00:50:47.700 --> 00:50:51.050
it's easier to describe what
an undesirable outcome would

00:50:51.050 --> 00:50:53.520
be than a desirable one.

00:50:53.520 --> 00:50:56.970
So there are a lot of ways in
which things could turn out

00:50:56.970 --> 00:50:58.840
that, by most
reasonable [INAUDIBLE],

00:50:58.840 --> 00:51:00.880
we would regard as
pretty worthless.

00:51:00.880 --> 00:51:03.670
Like the standard example
in this little literature

00:51:03.670 --> 00:51:05.840
is the paper clip maximizer.

00:51:05.840 --> 00:51:07.840
So an AI that's
superintelligent, and has

00:51:07.840 --> 00:51:09.830
as its only final,
highest-level goal

00:51:09.830 --> 00:51:14.200
to maximize the number of
paper clips it produces.

00:51:14.200 --> 00:51:16.890
This is a stand-in for
some other arbitrary goal.

00:51:16.890 --> 00:51:19.730
But most final
goals, if you think

00:51:19.730 --> 00:51:22.530
through how the world would be
structured in order to maximize

00:51:22.530 --> 00:51:25.510
the realization of
that goal, would

00:51:25.510 --> 00:51:28.360
involve, as a side effect, the
elimination of human beings

00:51:28.360 --> 00:51:29.700
and everything we care about.

00:51:29.700 --> 00:51:32.507
So if you're a superintelligence
that's a singleton,

00:51:32.507 --> 00:51:34.840
and you want to make sure
there's as many paper clips as

00:51:34.840 --> 00:51:36.710
possible, for a start, you'd
want to get rid of all humans.

00:51:36.710 --> 00:51:39.110
Because maybe we'll want
to switch off or something

00:51:39.110 --> 00:51:41.950
like that, and then there'll
be fewer paperclips.

00:51:41.950 --> 00:51:44.510
We also have bodies that
are full of juicy atoms that

00:51:44.510 --> 00:51:46.969
could be used to make some
really nice paper clips.

00:51:46.969 --> 00:51:49.010
So then you think, OK,
that's not do paper clips.

00:51:49.010 --> 00:51:49.640
That's ridiculous.

00:51:49.640 --> 00:51:51.120
But then you think
of something else.

00:51:51.120 --> 00:51:52.619
Like what about an
AI who only wants

00:51:52.619 --> 00:51:55.900
to calculate decimal
expansion of pi?

00:51:55.900 --> 00:51:57.804
So similarly, such
an AI would want

00:51:57.804 --> 00:51:59.220
to maximize the
amount of hardware

00:51:59.220 --> 00:52:02.480
it has so it can make more rapid
progress in this calculation.

00:52:02.480 --> 00:52:05.720
And it actually turns out to
be quite difficult to specify

00:52:05.720 --> 00:52:08.580
a goal that would not be
maximally realized in a world

00:52:08.580 --> 00:52:12.610
where not just human biological
organisms are extinct,

00:52:12.610 --> 00:52:15.230
but also anything we would
possibly place value on

00:52:15.230 --> 00:52:17.690
is eradicated.

00:52:17.690 --> 00:52:20.170
AUDIENCE: So the
premise there is

00:52:20.170 --> 00:52:23.980
that-- I want to really focus
on the premise, because I think

00:52:23.980 --> 00:52:27.370
the argument hinges
on it-- that we're

00:52:27.370 --> 00:52:31.190
taking a snapshot
of what it is we

00:52:31.190 --> 00:52:36.800
value today, where "we" includes
the things that we consider

00:52:36.800 --> 00:52:39.460
to be adequately
cognitive today.

00:52:39.460 --> 00:52:45.030
And we are ignoring in our
definition of desirability--

00:52:45.030 --> 00:52:47.875
Let's go to the extreme of
the paper clip scenario.

00:52:47.875 --> 00:52:53.000
A utilitarian might say, well,
OK, if it wants paper clips,

00:52:53.000 --> 00:52:56.230
but its overall cognition
is vastly greater

00:52:56.230 --> 00:52:57.890
than the rest of
humanity as a whole,

00:52:57.890 --> 00:53:04.070
well, then that's what it wants,
so the weighted definition

00:53:04.070 --> 00:53:07.310
of desirability should be
to maximize paper clips,

00:53:07.310 --> 00:53:08.900
because that's what it wants.

00:53:08.900 --> 00:53:09.540
NICK BOSTROM: Well,
OK, so there are

00:53:09.540 --> 00:53:11.081
different versions
of utilitarianism.

00:53:11.081 --> 00:53:13.000
There is preference
satisfactionism,

00:53:13.000 --> 00:53:15.530
which I think is
what you alluded to,

00:53:15.530 --> 00:53:19.280
which would stipulate some sort
of social welfare function that

00:53:19.280 --> 00:53:23.699
is maximized by fully-satisfied
single preferences that exist.

00:53:23.699 --> 00:53:26.240
There's a big problem of how to
aggregate them, but something

00:53:26.240 --> 00:53:27.590
along those lines.

00:53:27.590 --> 00:53:30.950
Other utilitarians would say
maximize pleasure or maximize

00:53:30.950 --> 00:53:35.700
happiness or maximize some
other part, the common feature

00:53:35.700 --> 00:53:38.150
being that the
value of the whole

00:53:38.150 --> 00:53:40.390
is, as it were, the sum
of the value of the parts.

00:53:42.930 --> 00:53:46.110
If you thought preference
satisfaction is and was

00:53:46.110 --> 00:53:49.640
correct, you might
want to design agents

00:53:49.640 --> 00:53:51.749
with easy-to-satisfy
preferences.

00:53:51.749 --> 00:53:54.040
Like they want there to be
at least three prime numbers

00:53:54.040 --> 00:53:55.970
or something like that,
and then you're done.

00:53:55.970 --> 00:53:58.080
And then maybe to have as many
as possible of those agents.

00:53:58.080 --> 00:53:59.455
Like the minimum
agent that would

00:53:59.455 --> 00:54:03.270
count as a morally
considerable being.

00:54:03.270 --> 00:54:07.150
But that seems like a fairly
impossible moral view.

00:54:07.150 --> 00:54:11.762
But one can decompose this big
sort of problem into two parts.

00:54:11.762 --> 00:54:13.720
On the one hand, you have
the technical problem

00:54:13.720 --> 00:54:17.000
of-- if you specified some
value in human language,

00:54:17.000 --> 00:54:19.840
like whether it's to maximize
happiness or freedom or love

00:54:19.840 --> 00:54:22.890
or creativity, whatever it
is, that how could you sort of

00:54:22.890 --> 00:54:25.542
embed that into a seed
AI, like an AI that's

00:54:25.542 --> 00:54:27.625
destined eventually to
become a superintelligence?

00:54:27.625 --> 00:54:30.510
So this is like an
enormous technical problem.

00:54:30.510 --> 00:54:33.749
Because like in C++, you
don't have a primitive saying

00:54:33.749 --> 00:54:34.540
"happiness," right?

00:54:34.540 --> 00:54:37.310
You have to define
all of these terms.

00:54:37.310 --> 00:54:39.290
Some goals would be
feasible, like maybe

00:54:39.290 --> 00:54:40.805
to calculate as
many digits of pi.

00:54:40.805 --> 00:54:42.180
It's something we
could do today.

00:54:42.180 --> 00:54:44.950
Others, like, there's this big,
unsolved technical problem.

00:54:44.950 --> 00:54:46.490
But then on top
of that, you also

00:54:46.490 --> 00:54:49.400
have the second problem, which
is the value selection problem,

00:54:49.400 --> 00:54:51.620
like trying to figure out which
value it is that you would want

00:54:51.620 --> 00:54:53.110
to get in there in
the first place.

00:54:53.110 --> 00:54:57.210
And both of these are places
where we could easily stumble.

00:54:57.210 --> 00:55:00.530
So just to reflect on,
like-- if the idea was

00:55:00.530 --> 00:55:06.042
to try to do some AI that
was ethical, or maximally

00:55:06.042 --> 00:55:07.750
always did the morally
right thing to do,

00:55:07.750 --> 00:55:11.320
if we try to achieve that
by just creating a list

00:55:11.320 --> 00:55:15.690
or somehow embedding our current
best understanding of ethics

00:55:15.690 --> 00:55:21.190
into a final goal, we should
reflect that if any earlier

00:55:21.190 --> 00:55:23.030
age had done this
with their values,

00:55:23.030 --> 00:55:26.300
it would have been what we
can now see are catastrophes.

00:55:26.300 --> 00:55:29.930
Earlier ages were condoning
slavery or human sacrifice,

00:55:29.930 --> 00:55:33.340
and all kinds of abuses of
different minorities and stuff.

00:55:33.340 --> 00:55:35.440
And presumably even
though we might

00:55:35.440 --> 00:55:38.911
have made some progress
towards moral enlightenment,

00:55:38.911 --> 00:55:40.410
we haven't gotten
all the way there.

00:55:40.410 --> 00:55:42.170
So it would be
important to preserve

00:55:42.170 --> 00:55:46.450
the possibility for moral
growth in the value selection.

00:55:46.450 --> 00:55:49.849
And so there are a
number of different paths

00:55:49.849 --> 00:55:51.890
that each should be
explored, because we're still

00:55:51.890 --> 00:55:53.650
at such an early stage here.

00:55:53.650 --> 00:55:55.470
But maybe one of the
more promising one

00:55:55.470 --> 00:55:56.970
is this idea of
indirect normativity

00:55:56.970 --> 00:55:58.386
that I describe
in the book, which

00:55:58.386 --> 00:56:00.670
is the idea that rather than
trying to take explicitly

00:56:00.670 --> 00:56:03.790
characterized some
desired end state,

00:56:03.790 --> 00:56:08.980
we try to motivate the AI to
pursue some process whereby it

00:56:08.980 --> 00:56:13.889
can find out what it is that we
were trying to work out when we

00:56:13.889 --> 00:56:15.180
were working with this problem.

00:56:15.180 --> 00:56:18.260
So suppose you could give
the AI the goal of doing

00:56:18.260 --> 00:56:21.850
that, which we would have
asked it to do if we had had,

00:56:21.850 --> 00:56:25.180
like, 40,000 years to
think about this question,

00:56:25.180 --> 00:56:27.300
and if we ourselves
had been smarter,

00:56:27.300 --> 00:56:30.680
and if we had known more facts.

00:56:30.680 --> 00:56:32.670
So now we don't know
what that is, currently.

00:56:32.670 --> 00:56:35.910
But it's an empirical question
that we could then hopefully

00:56:35.910 --> 00:56:38.680
leverage the AI's
superior intelligence

00:56:38.680 --> 00:56:40.960
to make a better estimate of.

00:56:40.960 --> 00:56:43.690
And then that kind of
indirectly specified goal

00:56:43.690 --> 00:56:46.849
might then be more likely to
produce an outcome that we

00:56:46.849 --> 00:56:49.015
would recognize, on reflection,
as being worthwhile.

00:56:52.044 --> 00:56:53.210
AUDIENCE: So I have a story.

00:56:53.210 --> 00:56:57.390
The other day, I was reading
some of the news and analysis

00:56:57.390 --> 00:56:59.950
about the crisis
in the Middle East,

00:56:59.950 --> 00:57:02.730
and I guess I spent like
an hour thinking about it.

00:57:02.730 --> 00:57:05.720
And I didn't come up with a
solution for the Middle East.

00:57:05.720 --> 00:57:06.980
NICK BOSTROM: Ah, darn.

00:57:06.980 --> 00:57:10.230
AUDIENCE: Now if I had been
a speedy superintelligence,

00:57:10.230 --> 00:57:14.210
and in that hour I had spent
1,000 hours of thinking,

00:57:14.210 --> 00:57:17.160
I think I still wouldn't
have come up with a solution.

00:57:17.160 --> 00:57:21.410
So I think there are some
problems for which intelligence

00:57:21.410 --> 00:57:23.000
by itself isn't the answer.

00:57:23.000 --> 00:57:26.570
And you know, as humans,
we put sapiens in our name.

00:57:26.570 --> 00:57:29.120
We think intelligence
is really important,

00:57:29.120 --> 00:57:30.480
but it's not the only attribute.

00:57:30.480 --> 00:57:32.429
I don't think it
solves all problems.

00:57:32.429 --> 00:57:33.220
NICK BOSTROM: Yeah.

00:57:33.220 --> 00:57:36.362
So I mean, I agree with that.

00:57:36.362 --> 00:57:38.820
A lot of sort of sociopolitical
problems in the human realm

00:57:38.820 --> 00:57:41.282
often depend on people with
conflicting preferences.

00:57:41.282 --> 00:57:42.990
There might just not
success one solution

00:57:42.990 --> 00:57:46.460
that would maximally
please everybody.

00:57:46.460 --> 00:57:48.492
And with the case
of the AI, I mean,

00:57:48.492 --> 00:57:50.950
I think that in fact, the most
important problem to work on

00:57:50.950 --> 00:57:52.700
is not the intelligence
problem, which

00:57:52.700 --> 00:57:54.200
hastens the day
where we'll have it,

00:57:54.200 --> 00:57:56.170
but rather this control problem.

00:57:56.170 --> 00:58:00.260
How to ensure that it would
deploy its intelligence in ways

00:58:00.260 --> 00:58:01.460
that are not harmful.

00:58:01.460 --> 00:58:04.850
And just briefly, there's,
I think, two broad classes

00:58:04.850 --> 00:58:07.340
of control method that's
one can envisage here.

00:58:07.340 --> 00:58:09.990
So one is capability
control method,

00:58:09.990 --> 00:58:12.150
where you try to limit
what the AI is able to do.

00:58:12.150 --> 00:58:13.440
So maybe put it in a box.

00:58:13.440 --> 00:58:16.430
You unplug the ethernet cable.

00:58:16.430 --> 00:58:19.170
You only allow it to communicate
by typing text on a screen,

00:58:19.170 --> 00:58:19.670
let's say.

00:58:19.670 --> 00:58:24.270
Maybe only even answers to
questions that are posted.

00:58:24.270 --> 00:58:25.950
And you try to clip its wings.

00:58:25.950 --> 00:58:29.090
And I think that
those can be important

00:58:29.090 --> 00:58:31.340
during the development phase,
like before you actually

00:58:31.340 --> 00:58:33.037
are ready to launch your system.

00:58:33.037 --> 00:58:35.120
But ultimately, I don't
think they are the answer.

00:58:35.120 --> 00:58:37.580
Because in order for
this AI to actually

00:58:37.580 --> 00:58:41.039
have any effect on the
world, it will at some point

00:58:41.039 --> 00:58:42.080
need to interact with it.

00:58:42.080 --> 00:58:44.190
Like if you literally just
had an isolated box that

00:58:44.190 --> 00:58:46.773
didn't closely interact with the
world, yes, it could be safe,

00:58:46.773 --> 00:58:50.750
but it would also not
do anything at all.

00:58:50.750 --> 00:58:53.370
But as soon as you have, say,
a human being communicating

00:58:53.370 --> 00:58:55.220
with it, then you
have a weak link here.

00:58:55.220 --> 00:58:58.340
Like humans are
not secure systems.

00:58:58.340 --> 00:59:03.550
And even humans often succeed
in manipulating or tricking

00:59:03.550 --> 00:59:06.870
or deluding other humans to
do their-- like scam artists.

00:59:06.870 --> 00:59:10.000
And so if you had like a
superhumanly powerful persuader

00:59:10.000 --> 00:59:12.445
and manipulator,
chances are eventually,

00:59:12.445 --> 00:59:14.890
it would find a way to talk
its way out of the box.

00:59:14.890 --> 00:59:16.610
Unless it could just
hack its way out,

00:59:16.610 --> 00:59:20.905
like by-- so there are things
like, we think, oh, well,

00:59:20.905 --> 00:59:22.030
we'll just put it in a box.

00:59:22.030 --> 00:59:23.420
If we don't talk
to it, it's safe.

00:59:23.420 --> 00:59:25.170
Well, maybe there's
some unanticipated way

00:59:25.170 --> 00:59:27.600
that we haven't thought of,
like by wiggling its electrons

00:59:27.600 --> 00:59:28.974
around in its
circuitry, maybe it

00:59:28.974 --> 00:59:31.280
could create
electromagnetic waves that

00:59:31.280 --> 00:59:32.722
could influence a
nearby apparatus

00:59:32.722 --> 00:59:33.680
or something like that.

00:59:33.680 --> 00:59:35.638
So then we think, oh,
put it in a Faraday cage.

00:59:35.638 --> 00:59:38.780
But OK, so if we just keep
patching up all the flaws

00:59:38.780 --> 00:59:40.264
that we can find,
then we will just

00:59:40.264 --> 00:59:41.680
patch up all the
ones we can find,

00:59:41.680 --> 00:59:43.880
but there are probably some more
ones that we can't think of.

00:59:43.880 --> 00:59:45.500
And then it will
use one of those.

00:59:45.500 --> 00:59:48.330
So the second class
of control method

00:59:48.330 --> 00:59:51.880
is motivation selection
methods, where

00:59:51.880 --> 00:59:53.510
instead of, or in
addition to, trying

00:59:53.510 --> 00:59:55.460
to limit what the
system can do, you

00:59:55.460 --> 00:59:57.200
try to engineer its
motivation system,

00:59:57.200 --> 01:00:02.660
so that it would not
want to cause harm.

01:00:02.660 --> 01:00:06.225
And that's then where this
indirect normativity comes in,

01:00:06.225 --> 01:00:07.600
as one version of
that, and there

01:00:07.600 --> 01:00:11.930
are many other many
other aspects of that.

01:00:11.930 --> 01:00:14.180
And that's, I think, the
problem that ultimately we'll

01:00:14.180 --> 01:00:16.670
need to solve.

01:00:16.670 --> 01:00:20.320
AUDIENCE: So if you'd use these
two mechanisms to control it,

01:00:20.320 --> 01:00:24.280
still, it comes back
to this question

01:00:24.280 --> 01:00:25.750
on the other side
of the equation.

01:00:25.750 --> 01:00:28.780
Like it somehow turns
its fitness function

01:00:28.780 --> 01:00:33.020
into the will to dominate us,
because of its will to survive.

01:00:33.020 --> 01:00:34.720
But we also have
that will to survive,

01:00:34.720 --> 01:00:37.197
and even though
we make mistakes,

01:00:37.197 --> 01:00:39.530
it seems like the argument
of a superintelligence coming

01:00:39.530 --> 01:00:43.390
to completely dominate us
requires a lapse of attention

01:00:43.390 --> 01:00:46.720
on our part, in our own
promotion of our desire

01:00:46.720 --> 01:00:49.240
to survive, for long
enough for it to actually

01:00:49.240 --> 01:00:51.200
be irretrievable.

01:00:51.200 --> 01:00:53.749
So have you considered
that-- it seems

01:00:53.749 --> 01:00:56.290
like even in all of the horrific
things that you've described

01:00:56.290 --> 01:00:58.123
that could happen if a
superintelligence did

01:00:58.123 --> 01:01:00.720
come to dominate, there would
be that take-off duration

01:01:00.720 --> 01:01:05.140
period where we would presumably
wake up and unplug it.

01:01:05.140 --> 01:01:09.230
NICK BOSTROM: Well, one would
imagine, if the developers are

01:01:09.230 --> 01:01:12.000
somewhat sensible, that
they wouldn't actually

01:01:12.000 --> 01:01:14.750
permit the take-off unless
they at least believed

01:01:14.750 --> 01:01:17.380
that the system was safe.

01:01:17.380 --> 01:01:20.739
So imagine a scenario where
they have maybe falsely deluded

01:01:20.739 --> 01:01:22.780
themselves that there is
no flaw in their system.

01:01:22.780 --> 01:01:24.030
Or maybe they're just
worried that there's

01:01:24.030 --> 01:01:26.680
this competitor who's soon
going to release another system.

01:01:26.680 --> 01:01:29.221
So even if they haven't spent
enough time on the safety, they

01:01:29.221 --> 01:01:30.200
still--

01:01:30.200 --> 01:01:33.310
But you have to
take into account

01:01:33.310 --> 01:01:35.520
that you're dealing with
an intelligent adversary.

01:01:35.520 --> 01:01:39.420
So even just a human-level
mind in this situation

01:01:39.420 --> 01:01:41.860
could figure out that
it has an incentive

01:01:41.860 --> 01:01:45.240
to pretend to be nice, whether
or not it actually is nice.

01:01:45.240 --> 01:01:47.766
Like when you're weak and, at
the mercy of your programmers,

01:01:47.766 --> 01:01:49.140
who are inspecting
you and seeing

01:01:49.140 --> 01:01:52.500
if you're ready to be released,
and if you're an unfriendly AI,

01:01:52.500 --> 01:01:55.750
you would want to sort of behave
cooperatively and pleasingly

01:01:55.750 --> 01:01:57.330
and all of these things.

01:01:57.330 --> 01:01:59.245
Like it can plan
ahead to that extent.

01:01:59.245 --> 01:02:01.370
And only once you are sort
of strong enough that it

01:02:01.370 --> 01:02:03.860
doesn't matter whether
anybody tries to stop you,

01:02:03.860 --> 01:02:06.870
because they can't-- only
then would it be safe for you

01:02:06.870 --> 01:02:08.870
to reveal your true nature.

01:02:08.870 --> 01:02:11.500
So there is this fundamental
flaw in the-- so this

01:02:11.500 --> 01:02:13.850
is one of those initially
plausible ideas that

01:02:13.850 --> 01:02:15.200
don't seem to work.

01:02:15.200 --> 01:02:17.154
Like you develop your AI.

01:02:17.154 --> 01:02:19.320
You keep it in a sandbox,
like a secure environment,

01:02:19.320 --> 01:02:21.778
and you watch it for a while
to see that it behaves nicely.

01:02:21.778 --> 01:02:23.730
And only once you've
seen that it's

01:02:23.730 --> 01:02:25.140
cooperative and
nice and friendly

01:02:25.140 --> 01:02:26.410
there do you let it out.

01:02:26.410 --> 01:02:28.780
And the flaw is that
there is this possibility

01:02:28.780 --> 01:02:33.670
for strategic behavior,
that unfriendly AIs could

01:02:33.670 --> 01:02:37.020
mimic a friendly AI.

01:02:37.020 --> 01:02:42.030
And you mentioned something
about this survival desire.

01:02:42.030 --> 01:02:44.490
So there is something like
that, but it looks different.

01:02:44.490 --> 01:02:47.770
So we humans have--
we don't really

01:02:47.770 --> 01:02:49.550
have a clean agent architecture.

01:02:49.550 --> 01:02:51.630
There's not, like, one
final goal for most of us.

01:02:51.630 --> 01:02:53.171
And there are lot
of different drives

01:02:53.171 --> 01:02:55.810
that rise and fall in strength,
depending on the time of day

01:02:55.810 --> 01:02:59.586
and the environment we're in.

01:02:59.586 --> 01:03:01.210
But if you have this
architecture where

01:03:01.210 --> 01:03:03.050
there is a clearly-defined
final goal,

01:03:03.050 --> 01:03:04.760
and everything else
is pursued only

01:03:04.760 --> 01:03:08.060
by virtue of being
conducive to the attainment

01:03:08.060 --> 01:03:12.390
of this final goal,
then there are a couple

01:03:12.390 --> 01:03:14.740
of theses that I
think help you think

01:03:14.740 --> 01:03:16.285
about that kind of structure.

01:03:16.285 --> 01:03:19.230
So on the one hand, you have
the orthogonality thesis,

01:03:19.230 --> 01:03:19.820
as I call it.

01:03:19.820 --> 01:03:23.430
This is the idea that values
and intelligence are orthogonal.

01:03:23.430 --> 01:03:25.590
You could have virtually
any combination of them.

01:03:25.590 --> 01:03:28.320
Like a really smart system could
be really benevolent or really

01:03:28.320 --> 01:03:32.212
evil or have some bizarre goal,
like paper clips, or something

01:03:32.212 --> 01:03:32.920
human-meaningful.

01:03:32.920 --> 01:03:36.527
There's no necessary
ontological connection.

01:03:36.527 --> 01:03:39.110
On the other hand, you also have
this instrumental convergence

01:03:39.110 --> 01:03:43.290
thesis, which says that
for almost any final goal

01:03:43.290 --> 01:03:44.790
and almost any
environment, there

01:03:44.790 --> 01:03:48.399
will be certain instrumental
values that you will recognize

01:03:48.399 --> 01:03:49.440
once you're smart enough.

01:03:49.440 --> 01:03:55.230
For example, the value to
prevent your own death.

01:03:55.230 --> 01:03:57.860
And so if you're a paper clip
maximizer, the only reason

01:03:57.860 --> 01:03:58.820
that you don't want
to die, it's not

01:03:58.820 --> 01:04:00.290
because you sort of
value being alive.

01:04:00.290 --> 01:04:02.210
It's just that you predict
that there will be fewer paper

01:04:02.210 --> 01:04:04.140
clips if you are
switched off today.

01:04:04.140 --> 01:04:05.140
Because if you're
still around tomorrow,

01:04:05.140 --> 01:04:07.440
you will still be working
to make more paper clips.

01:04:07.440 --> 01:04:11.214
And similarly, goal
content preservation.

01:04:11.214 --> 01:04:13.380
You can predict that if
somebody changed your goals,

01:04:13.380 --> 01:04:14.590
then tomorrow,
you will no longer

01:04:14.590 --> 01:04:16.000
be working to make paper clips.

01:04:16.000 --> 01:04:17.470
Now you will be working
to make staplers,

01:04:17.470 --> 01:04:19.178
and then there will
be fewer paper clips.

01:04:19.178 --> 01:04:21.370
So you, being a paper
clip maximizer today,

01:04:21.370 --> 01:04:25.620
will want to prevent somebody
from changing your goals.

01:04:25.620 --> 01:04:27.730
And there are others, like
acquiring more material

01:04:27.730 --> 01:04:30.420
resources, or enhancing your own
intelligence so that you become

01:04:30.420 --> 01:04:32.590
better able to realize
whatever your goal are.

01:04:32.590 --> 01:04:36.230
And it's that combination
between the lack

01:04:36.230 --> 01:04:38.620
of any necessary connection
between final goal

01:04:38.620 --> 01:04:41.970
and intelligence, and these
convergence instrumental

01:04:41.970 --> 01:04:44.700
reasons to just
do things that are

01:04:44.700 --> 01:04:47.122
inconsistent with
human values, that

01:04:47.122 --> 01:04:48.580
creates the intrinsic
danger there.

01:04:48.580 --> 01:04:53.160
You have to engineer a very
particular kind of final goal

01:04:53.160 --> 01:04:56.710
to-- have a final goal such
that if it's actually maximally

01:04:56.710 --> 01:04:58.270
pursued by a
superintelligence, would

01:04:58.270 --> 01:05:00.780
be consistent with
human survival.

01:05:00.780 --> 01:05:02.730
Maybe something that
kind of embeds within it

01:05:02.730 --> 01:05:05.650
the same values that we have.

01:05:05.650 --> 01:05:09.460
MODERATOR: So we've been talking
a lot about hypothetical stuff.

01:05:09.460 --> 01:05:13.250
What about some concrete
stuff, namely policymakers?

01:05:13.250 --> 01:05:14.970
So we're talking
here about scenarios

01:05:14.970 --> 01:05:16.620
that are potentially
very dangerous

01:05:16.620 --> 01:05:19.310
and that may scare
policymakers, whom

01:05:19.310 --> 01:05:23.800
we know are technologically not
at the level of this audience

01:05:23.800 --> 01:05:29.830
and may start making decisions
which will slow down or impede

01:05:29.830 --> 01:05:34.060
the progress, or maybe even
ban computer science that

01:05:34.060 --> 01:05:36.880
tries to do AI research
because of the fears

01:05:36.880 --> 01:05:39.690
that crop up in some of that.

01:05:39.690 --> 01:05:43.170
What are your thoughts on
the policymaking process

01:05:43.170 --> 01:05:46.705
and legislature
process around issues

01:05:46.705 --> 01:05:48.720
of artificial intelligence?

01:05:48.720 --> 01:05:50.250
And can we expect
that, you know,

01:05:50.250 --> 01:05:54.816
like computer scientists are
one day labeled as terrorists?

01:05:54.816 --> 01:05:56.440
NICK BOSTROM: I don't
think that that's

01:05:56.440 --> 01:05:59.560
very likely for various reasons.

01:05:59.560 --> 01:06:02.920
It's hard at the moment
to see exactly what it

01:06:02.920 --> 01:06:05.486
is-- even if policymakers were
willing to do something, what

01:06:05.486 --> 01:06:07.360
they could actually do
that would be helpful,

01:06:07.360 --> 01:06:09.250
rather than harmful.

01:06:09.250 --> 01:06:11.270
At the moment, what
needs to be done,

01:06:11.270 --> 01:06:12.960
I think, is more
foundational work

01:06:12.960 --> 01:06:15.900
to build up a clear
understanding of what precisely

01:06:15.900 --> 01:06:17.090
the problem is.

01:06:17.090 --> 01:06:20.850
And then ultimately, it's mostly
a technical research challenge

01:06:20.850 --> 01:06:23.370
to work out the solution
to this control problem.

01:06:23.370 --> 01:06:27.542
It requires some top-notch
mathematical talent

01:06:27.542 --> 01:06:29.750
working together with
theoretical computer scientists

01:06:29.750 --> 01:06:32.320
and maybe some
philosophical expertise

01:06:32.320 --> 01:06:35.872
to really crack this problem.

01:06:35.872 --> 01:06:38.080
It's very hard to see how,
like, from some high level

01:06:38.080 --> 01:06:40.460
of government-- so it's
a very blunt instrument.

01:06:40.460 --> 01:06:43.100
And you might, even with the
best intentions at the start,

01:06:43.100 --> 01:06:47.620
like at the top, once it filters
down through the bureaucracy,

01:06:47.620 --> 01:06:49.210
it might have a very
different effect

01:06:49.210 --> 01:06:51.330
than the one you intended.

01:06:51.330 --> 01:06:53.609
So there are some
other existential risks

01:06:53.609 --> 01:06:55.400
where I think it would
be easier to imagine

01:06:55.400 --> 01:06:59.630
ways in which
regulation could help.

01:06:59.630 --> 01:07:02.100
AI is particularly difficult.

01:07:02.100 --> 01:07:06.890
Even just to understand what
the problem is is quite hard.

01:07:06.890 --> 01:07:09.490
And it's hard to
imagine a scenario,

01:07:09.490 --> 01:07:11.490
at least in the next
couple of decades,

01:07:11.490 --> 01:07:13.630
where we would have some
kind of sane thing coming

01:07:13.630 --> 01:07:16.310
from political processes.

01:07:16.310 --> 01:07:19.190
Maybe the closest would be
like more funding for work

01:07:19.190 --> 01:07:21.227
on the control problem.

01:07:21.227 --> 01:07:22.810
But even that, once
it sort of filters

01:07:22.810 --> 01:07:24.910
through the vested
interest and academia,

01:07:24.910 --> 01:07:27.150
will probably
translate into a rain

01:07:27.150 --> 01:07:30.486
of funding falling on a
wide range of superficially

01:07:30.486 --> 01:07:32.610
related areas that might
not actually have anything

01:07:32.610 --> 01:07:34.850
to do with the control problem,
like general computer security

01:07:34.850 --> 01:07:35.808
or something like that.

01:07:38.397 --> 01:07:40.230
But there are other
things that can be done.

01:07:40.230 --> 01:07:44.740
So there are some organizations
that are working on this.

01:07:44.740 --> 01:07:47.027
So we are doing some
work at the Future

01:07:47.027 --> 01:07:48.360
of Humanity Institute at Oxford.

01:07:48.360 --> 01:07:50.570
Another is the Machine
Intelligence Research

01:07:50.570 --> 01:07:52.260
Institute, MIRI, at Berkeley.

01:07:52.260 --> 01:07:55.839
They have some excellent
people, as well.

01:07:55.839 --> 01:07:57.130
That would be an obvious thing.

01:07:57.130 --> 01:08:01.670
And generally try to recruit
some of the brightest minds

01:08:01.670 --> 01:08:03.700
of our generation and
the next generation,

01:08:03.700 --> 01:08:05.130
to sort of focus on this.

01:08:05.130 --> 01:08:06.990
At the moment,
worldwide, maybe there

01:08:06.990 --> 01:08:10.240
are half a dozen people
or so, equivalent,

01:08:10.240 --> 01:08:15.070
working full-time on
this, which is not

01:08:15.070 --> 01:08:18.810
in proportion to the
importance of the problem.

01:08:18.810 --> 01:08:20.529
It's a more general issue.

01:08:20.529 --> 01:08:23.250
I did a little literature
survey a couple of years ago.

01:08:23.250 --> 01:08:27.439
I just compared a number of
academic papers on the dung

01:08:27.439 --> 01:08:30.762
beetle compared to a
number on human extinction.

01:08:30.762 --> 01:08:32.970
And sad to tell you that
there was more than an order

01:08:32.970 --> 01:08:34.511
of magnitude more
on the dung beetle.

01:08:37.779 --> 01:08:40.225
So the positive spin
on that is that there

01:08:40.225 --> 01:08:42.600
are enormous opportunities
for somebody who actually does

01:08:42.600 --> 01:08:44.840
care to make a big difference.

01:08:44.840 --> 01:08:46.331
Like even one
extra person or one

01:08:46.331 --> 01:08:47.830
like extra million
or something, can

01:08:47.830 --> 01:08:52.760
do a lot of good there,
because it's so neglected.

01:08:52.760 --> 01:08:55.479
AUDIENCE: So regarding
policy and political things,

01:08:55.479 --> 01:08:57.750
I think the general
underlying principle here

01:08:57.750 --> 01:09:00.600
is that modern governments
are like big battleships

01:09:00.600 --> 01:09:01.880
or big tanks.

01:09:01.880 --> 01:09:05.479
They do very well against
large, stationary targets,

01:09:05.479 --> 01:09:07.790
but against small,
mobile targets,

01:09:07.790 --> 01:09:09.380
they're extremely ineffective.

01:09:09.380 --> 01:09:12.060
And so if AI were like
nuclear weapons, where

01:09:12.060 --> 01:09:14.790
in order to produce it, you
need these giant, static

01:09:14.790 --> 01:09:17.270
manufacturing facilities
that are very expensive

01:09:17.270 --> 01:09:18.830
and they're like
fixed in one place

01:09:18.830 --> 01:09:22.890
so you can see where it is, then
the political aspect, how you

01:09:22.890 --> 01:09:26.279
regulate it and whether you
regulate it, is very important.

01:09:26.279 --> 01:09:28.270
But artificial intelligence
isn't like that.

01:09:28.270 --> 01:09:30.250
You can develop it from
anywhere in the world.

01:09:30.250 --> 01:09:32.637
Your computer
might cost $10,000,

01:09:32.637 --> 01:09:35.220
and it might be anywhere in the
world, since you can do things

01:09:35.220 --> 01:09:36.720
through the cloud.

01:09:36.720 --> 01:09:41.760
And when governments
try to handle

01:09:41.760 --> 01:09:43.920
these sorts of small
mobile targets,

01:09:43.920 --> 01:09:46.330
like individual websites
or individual people

01:09:46.330 --> 01:09:48.370
on the internet,
it doesn't really

01:09:48.370 --> 01:09:50.950
matter, compared to the
nuclear weapons case,

01:09:50.950 --> 01:09:53.250
very much what kinds
of things they do.

01:09:53.250 --> 01:09:55.770
Because governments just
can't hit that kind of target.

01:09:55.770 --> 01:09:59.700
It's like, you know,
piracy of software

01:09:59.700 --> 01:10:03.110
is, in theory, punishable
by whatever penalty.

01:10:03.110 --> 01:10:06.030
But as we see everywhere in the
world, those kinds of things

01:10:06.030 --> 01:10:08.520
are totally ineffective at
achieving their stated goals.

01:10:08.520 --> 01:10:09.200
NICK BOSTROM: It
depends a little bit

01:10:09.200 --> 01:10:11.075
on what the scenarios
here that we're having.

01:10:11.075 --> 01:10:13.880
Like, say, if there were some
scenario in which they would

01:10:13.880 --> 01:10:15.790
try to prevent AI from
ever being developed,

01:10:15.790 --> 01:10:17.630
I think that's a lot
more far-fetched.

01:10:17.630 --> 01:10:21.090
And slightly more possible
scenarios where it became

01:10:21.090 --> 01:10:23.850
clear which products
were going to succeed,

01:10:23.850 --> 01:10:27.230
and that it was going ahead, and
then they would acquire that.

01:10:27.230 --> 01:10:28.980
Like they would nationalize it.

01:10:28.980 --> 01:10:31.370
But then that doesn't
solve the problem.

01:10:31.370 --> 01:10:33.540
That just means that now
you have an encapsulation.

01:10:33.540 --> 01:10:36.901
So maybe it's all placed
under the federal government,

01:10:36.901 --> 01:10:38.900
and they have military
guarding the whole thing,

01:10:38.900 --> 01:10:40.730
but you would still have the
same people inside, basically

01:10:40.730 --> 01:10:42.350
working on the same problem.

01:10:42.350 --> 01:10:45.020
And so that that outcome,
scenario, might not

01:10:45.020 --> 01:10:48.719
make that much difference
one way or the other.

01:10:48.719 --> 01:10:51.010
You still have the same basic
technical problem inside.

01:10:51.010 --> 01:10:54.080
And it's also unclear
to what extent

01:10:54.080 --> 01:10:56.290
it would be possible
for non-experts

01:10:56.290 --> 01:10:59.680
to really be able to exert
micro-level influence

01:10:59.680 --> 01:11:01.420
on the precise design of the AI.

01:11:01.420 --> 01:11:02.920
I mean, you have
to know what you're

01:11:02.920 --> 01:11:04.139
doing to be able to do that.

01:11:04.139 --> 01:11:06.430
I think-- I mean, things that
they could do in general,

01:11:06.430 --> 01:11:07.780
there are indirect things.

01:11:07.780 --> 01:11:12.610
So working harder to achieve
global peace and coordination

01:11:12.610 --> 01:11:16.239
would help with a lot of
problems, including AI.

01:11:16.239 --> 01:11:17.280
Maybe it makes it easier.

01:11:17.280 --> 01:11:19.447
And in the future, if there
were like a race dynamic

01:11:19.447 --> 01:11:21.029
between different
countries, that they

01:11:21.029 --> 01:11:23.620
could join together and do one
joint thing, rather than racing

01:11:23.620 --> 01:11:27.679
to get there first and then
having to cut back on safety.

01:11:27.679 --> 01:11:29.470
There are things that
could be done, maybe,

01:11:29.470 --> 01:11:34.392
to facilitate biological
cognitive enhancement.

01:11:34.392 --> 01:11:36.100
If that was the will,
you could certainly

01:11:36.100 --> 01:11:39.510
imagine different kinds
of funding and policies

01:11:39.510 --> 01:11:43.070
for accessing and linking
different databases that

01:11:43.070 --> 01:11:45.720
could be done, and stuff like
that, that would be useful.

01:11:45.720 --> 01:11:48.250
So there are potentially
cost-effective,

01:11:48.250 --> 01:11:50.000
indirect ways of
approaching this problem,

01:11:50.000 --> 01:11:52.820
in addition to directly
working on the control problem.

01:11:52.820 --> 01:11:55.779
There are these other levers
that one could also consider.

01:11:55.779 --> 01:11:58.070
Particularly on things that
we are still quite far away

01:11:58.070 --> 01:12:00.837
from the relevant crunch time.

01:12:00.837 --> 01:12:01.670
AUDIENCE: Hi, there.

01:12:01.670 --> 01:12:02.970
I was just curious.

01:12:02.970 --> 01:12:04.650
You're one of the
world's experts

01:12:04.650 --> 01:12:09.280
in superintelligence and
the extensional risks.

01:12:09.280 --> 01:12:11.930
Personally speaking,
informally, intuitively,

01:12:11.930 --> 01:12:13.920
do you think we're
gonna make it?

01:12:13.920 --> 01:12:16.062
[LAUGHTER]

01:12:16.062 --> 01:12:17.020
NICK BOSTROM: Uh, yeah.

01:12:17.020 --> 01:12:19.920
I mean, it's-- I
think that the, uh--

01:12:19.920 --> 01:12:21.250
[LAUGHTER]

01:12:21.250 --> 01:12:24.370
NICK BOSTROM: I mean, like,
I mean-- yeah, probably

01:12:24.370 --> 01:12:28.820
like less than 50% risk of doom.

01:12:28.820 --> 01:12:31.510
But I don't know exactly
what the number is.

01:12:31.510 --> 01:12:33.390
I mean, the more important
question, I guess,

01:12:33.390 --> 01:12:38.450
is what is the best
way to push it down.

01:12:38.450 --> 01:12:41.250
So that's where most of the
mental energy is going into.

01:12:45.450 --> 01:12:46.350
[LAUGHTER]

01:12:46.350 --> 01:12:50.250
MODERATOR: So with that,
please thank our guest today.

01:12:50.250 --> 01:12:51.750
[APPLAUSE]

01:12:51.750 --> 01:12:53.900
MODERATOR: Thank you, Nick.

