WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.444
[MUSIC PLAYING]

00:00:08.218 --> 00:00:09.510
MARTIN WICKE: I'm Martin Wicke.

00:00:09.510 --> 00:00:11.410
I'm the engineer lead
for a TensorFlow 2,

00:00:11.410 --> 00:00:13.968
and I am going to,
unsurprisingly, talk

00:00:13.968 --> 00:00:14.760
about TensorFlow 2.

00:00:17.290 --> 00:00:19.520
TensorFlow has been
extremely successful.

00:00:19.520 --> 00:00:22.040
And inside of Google,
outside of Google,

00:00:22.040 --> 00:00:23.620
it has grown into
a vibrant community

00:00:23.620 --> 00:00:27.910
that really when we started it
we were not able to imagine.

00:00:27.910 --> 00:00:32.080
And our users are building
these ingenious, clever, elegant

00:00:32.080 --> 00:00:36.220
things everyday
from art and music,

00:00:36.220 --> 00:00:41.680
from what you just heard about
medieval manuscripts, science,

00:00:41.680 --> 00:00:42.730
and medicine.

00:00:42.730 --> 00:00:46.820
And the things people created
were unexpected and beautiful.

00:00:46.820 --> 00:00:48.190
And we learned a lot from that.

00:00:48.190 --> 00:00:51.100
And we learned a lot
from how people did it.

00:00:51.100 --> 00:00:54.130
And TensorFlow flow has
enabled all this creativity

00:00:54.130 --> 00:00:59.500
and really jumpstarted this
whole AI democratization,

00:00:59.500 --> 00:01:00.830
which is great.

00:01:00.830 --> 00:01:03.250
But it has been a little
bit hard to use sometimes

00:01:03.250 --> 00:01:04.484
And we know.

00:01:04.484 --> 00:01:07.360
At sometimes it's
been a little painful.

00:01:07.360 --> 00:01:10.420
So using sessions wasn't
the most natural thing

00:01:10.420 --> 00:01:13.270
to do when coming
from regular Python.

00:01:13.270 --> 00:01:16.680
And the TensorFlow API
has grown over time

00:01:16.680 --> 00:01:20.187
and got a little bit cluttered,
a little bit confusing.

00:01:20.187 --> 00:01:22.270
In the end, you can do
everything with TensorFlow,

00:01:22.270 --> 00:01:24.070
but it wasn't always
clear what's the best

00:01:24.070 --> 00:01:25.930
way to do it with TensorFlow.

00:01:25.930 --> 00:01:29.770
And so we've learned a
lot since we started this,

00:01:29.770 --> 00:01:32.960
and we realized that you
need rapid prototyping,

00:01:32.960 --> 00:01:36.380
you need easier debugging,
there's a lot of clutter.

00:01:36.380 --> 00:01:38.120
So we're fixing this.

00:01:38.120 --> 00:01:40.120
We're addressing these
issues with a new version

00:01:40.120 --> 00:01:41.620
of TensorFlow--

00:01:41.620 --> 00:01:45.340
today, releasing an
alpha of TensorFlow 2.

00:01:45.340 --> 00:01:47.693
Many of you have participated
in the design reviews

00:01:47.693 --> 00:01:49.360
that went into this
for all the features

00:01:49.360 --> 00:01:51.663
that we have implemented.

00:01:51.663 --> 00:01:53.080
Those of you who
are really living

00:01:53.080 --> 00:01:54.538
on the bleeding
edge of development

00:01:54.538 --> 00:01:56.230
have probably played
with a nightly.

00:01:56.230 --> 00:01:57.147
You have installed it.

00:01:57.147 --> 00:01:58.090
You have tested it.

00:01:58.090 --> 00:01:59.103
You have found issues.

00:01:59.103 --> 00:02:00.020
You have fixed issues.

00:02:00.020 --> 00:02:03.490
So thank you very
much for your help.

00:02:03.490 --> 00:02:06.670
You can now install the
alpha release of TensorFlow 2

00:02:06.670 --> 00:02:09.940
using just pip install
--pre tenserflow.

00:02:09.940 --> 00:02:12.530
And you can go and play with it.

00:02:12.530 --> 00:02:15.480
So what has changed?

00:02:15.480 --> 00:02:18.990
When we create TensorFlow
2, really the biggest focus

00:02:18.990 --> 00:02:20.490
is on usability.

00:02:20.490 --> 00:02:23.700
We've adopted Keras as the
high level API for TensorFlow.

00:02:23.700 --> 00:02:26.400
We've integrated it very
deeply with TensorFlow.

00:02:26.400 --> 00:02:29.170
Keras gives you a clear path
of how to develop models,

00:02:29.170 --> 00:02:31.260
how to deployment models.

00:02:31.260 --> 00:02:34.390
It has a [INAUDIBLE] API
that makes people productive,

00:02:34.390 --> 00:02:38.250
and we know that it gets
you started quickly.

00:02:38.250 --> 00:02:41.070
TensorFlow also includes
a complete implementation

00:02:41.070 --> 00:02:42.190
of Keras, of course.

00:02:42.190 --> 00:02:45.150
But we went further.

00:02:45.150 --> 00:02:46.770
We have extended
Keras so that you

00:02:46.770 --> 00:02:49.020
can use all of the advanced
features in TensorFlow

00:02:49.020 --> 00:02:51.350
directly from tf.keras.

00:02:51.350 --> 00:02:53.150
The other big change
in TensorFlow 2

00:02:53.150 --> 00:02:56.870
is that it is Eager execution
by default. Traditional 1.x

00:02:56.870 --> 00:02:58.370
TensorFlow used a
declarative style,

00:02:58.370 --> 00:03:00.870
and it was often a little bit
dissonant with the surrounding

00:03:00.870 --> 00:03:01.830
Python.

00:03:01.830 --> 00:03:03.860
And in TensorFlow 2,
TensorFlow behaves just

00:03:03.860 --> 00:03:06.660
like the surrounding code.

00:03:06.660 --> 00:03:08.570
So if you add up
two numbers, you

00:03:08.570 --> 00:03:13.080
get a result back immediately,
which is amazing, really.

00:03:13.080 --> 00:03:14.850
[LAUGHTER]

00:03:14.850 --> 00:03:17.030
But you still get
all the benefits

00:03:17.030 --> 00:03:18.650
of working with A graph--

00:03:18.650 --> 00:03:21.458
robust program serializating,
easy distribution

00:03:21.458 --> 00:03:23.000
of programs,
distributed computation,

00:03:23.000 --> 00:03:24.710
optimizations on the graph.

00:03:24.710 --> 00:03:27.650
All of that stuff is not going
away, it's just getting easier.

00:03:30.930 --> 00:03:33.093
So TensorFlow 2 is
a major release,

00:03:33.093 --> 00:03:35.260
which means that we have
the ability to clean it up.

00:03:35.260 --> 00:03:37.320
And we did-- like, a lot.

00:03:37.320 --> 00:03:40.210
So we had a lot of
duplicate functionality

00:03:40.210 --> 00:03:42.110
that we have removed.

00:03:42.110 --> 00:03:43.270
We've consolidated the API.

00:03:43.270 --> 00:03:44.950
We've organized the API.

00:03:44.950 --> 00:03:48.155
We've made sure that the API
looks and feels consistent.

00:03:48.155 --> 00:03:50.030
And this is not only
about TensorFlow itself,

00:03:50.030 --> 00:03:52.150
this is about the whole
ecosystem of tools that

00:03:52.150 --> 00:03:55.780
has grown around TensorFlow.

00:03:55.780 --> 00:03:58.150
We've spent a lot of
effort to try and align

00:03:58.150 --> 00:03:59.520
all of these interfaces.

00:03:59.520 --> 00:04:02.470
And we've defined
exchange formats

00:04:02.470 --> 00:04:07.780
that'll allow you to move
all throughout this ecosystem

00:04:07.780 --> 00:04:09.040
without hitting any barriers.

00:04:12.342 --> 00:04:14.300
We have removed a lot of
stuff from TensorFlow,

00:04:14.300 --> 00:04:17.839
but that doesn't
mean that it's not--

00:04:17.839 --> 00:04:20.430
it has gotten more
flexible, not less.

00:04:20.430 --> 00:04:22.850
It retains and expands
on the flexibility

00:04:22.850 --> 00:04:24.875
we had that enabled all
of this development.

00:04:24.875 --> 00:04:27.110
And we've created a
more complete low level

00:04:27.110 --> 00:04:30.140
API that now exposes all
of the internally use ops

00:04:30.140 --> 00:04:32.905
to the users via
the robs module.

00:04:32.905 --> 00:04:34.280
We provide
inheritable interfaces

00:04:34.280 --> 00:04:38.120
for all of the crucial
concepts in TensorFlow,

00:04:38.120 --> 00:04:40.100
like variables and checkpoints.

00:04:40.100 --> 00:04:41.840
And this allows
framework authors

00:04:41.840 --> 00:04:43.910
to build on top of
TensorFlow while maintaining

00:04:43.910 --> 00:04:47.175
interoperability with the
rest of the ecosystem.

00:04:47.175 --> 00:04:51.080
And you'll hear a lot
of these examples later.

00:04:51.080 --> 00:04:54.270
There'll be talk about
TF agents later today.

00:04:54.270 --> 00:04:57.410
There'll be a talk about
Sonnet by DeepMind tomorrow.

00:04:57.410 --> 00:05:02.550
So you'll see examples of
how this works in practice.

00:05:02.550 --> 00:05:04.680
Now, the real question for
you is, of course, what

00:05:04.680 --> 00:05:08.420
do I have to do to be a
part of this great new era

00:05:08.420 --> 00:05:09.920
of TensorFlow?

00:05:09.920 --> 00:05:13.760
And we know it's always
hard to upgrade to anything.

00:05:13.760 --> 00:05:16.760
And that's especially true
for major version upgrades.

00:05:16.760 --> 00:05:19.310
At Google, we will
now start the process

00:05:19.310 --> 00:05:22.460
of converting one of the
largest codebases in the world

00:05:22.460 --> 00:05:23.785
to TensorFlow 2.

00:05:23.785 --> 00:05:25.160
And while we're
doing that, we'll

00:05:25.160 --> 00:05:26.570
be writing a lot of
migration guides.

00:05:26.570 --> 00:05:28.487
we have started that
already, and some of them

00:05:28.487 --> 00:05:32.600
are online and will provide a
lot of best practices for you.

00:05:32.600 --> 00:05:37.640
And if we can do it at Google,
you can probably do it too.

00:05:37.640 --> 00:05:41.240
So we're giving you
and us a lot of tools

00:05:41.240 --> 00:05:42.685
to make this transition easier.

00:05:42.685 --> 00:05:44.840
And first of all, as
part of TensorFlow 2,

00:05:44.840 --> 00:05:47.690
we're shipping a
compatibility module--

00:05:47.690 --> 00:05:50.030
we call it tf.compat.v1--

00:05:50.030 --> 00:05:53.270
which contains all
of the 1.x API.

00:05:53.270 --> 00:05:56.270
So if you rely on a specific
deprecated function,

00:05:56.270 --> 00:05:59.090
you can still use it, and
you can still find it there,

00:05:59.090 --> 00:06:01.370
except for t
tf.contrib, which is not

00:06:01.370 --> 00:06:02.630
going to be included at all.

00:06:02.630 --> 00:06:05.780
But we've created a
community-supported alternative

00:06:05.780 --> 00:06:10.890
that support your use case if
you rely on something in there.

00:06:10.890 --> 00:06:13.220
We're also publishing a script
which will automatically

00:06:13.220 --> 00:06:15.650
update your code so it
runs on TensorFlow 2.

00:06:15.650 --> 00:06:17.720
Let me show you how that works.

00:06:17.720 --> 00:06:21.380
Let's say I want to
convert this program, which

00:06:21.380 --> 00:06:24.800
is a simple example that we
have online of a simple language

00:06:24.800 --> 00:06:27.090
model trained on Shakespeare.

00:06:27.090 --> 00:06:29.457
So what I do is I simply
run tf_upgrade_v2,

00:06:29.457 --> 00:06:31.790
which is a utility that's
included with any installation

00:06:31.790 --> 00:06:33.310
of TensorFlow 2.

00:06:33.310 --> 00:06:34.310
It tells me what it did.

00:06:34.310 --> 00:06:36.685
And in this case, there's
really just a handful of things

00:06:36.685 --> 00:06:39.330
that it changed.

00:06:39.330 --> 00:06:41.730
And you can see, if you
look at what changed,

00:06:41.730 --> 00:06:44.025
there's some functions
that it renamed.

00:06:44.025 --> 00:06:46.400
The reorganization of the API
leads to renamed functions.

00:06:46.400 --> 00:06:50.510
So multinomial was actually the
wrong name for this function.

00:06:50.510 --> 00:06:53.300
So it renamed it to
random.categorical.

00:06:53.300 --> 00:06:55.160
That's something the
script will do for you,

00:06:55.160 --> 00:06:57.710
so you don't have to
worry too much about it.

00:06:57.710 --> 00:07:01.040
A lot of functions had arguments
renamed, their order changed,

00:07:01.040 --> 00:07:03.290
or some arguments
added or deleted.

00:07:03.290 --> 00:07:05.870
As far as possible, the
script will make those changes

00:07:05.870 --> 00:07:07.350
for you.

00:07:07.350 --> 00:07:10.340
And then, if all else
fails, sometimes there

00:07:10.340 --> 00:07:13.273
isn't really a perfect
equivalent in TensorFlow 2

00:07:13.273 --> 00:07:14.940
to a symbol that
existed in TensorFlow 1

00:07:14.940 --> 00:07:17.060
And then, we'll use the
compatibility module

00:07:17.060 --> 00:07:19.832
that I talked about earlier.

00:07:19.832 --> 00:07:21.290
If there is no
perfect replacement,

00:07:21.290 --> 00:07:23.030
we'll use that in
order to make sure

00:07:23.030 --> 00:07:26.490
that your code still works as
expected after the conversion.

00:07:26.490 --> 00:07:27.956
So it's pretty conservative.

00:07:27.956 --> 00:07:30.020
So for instance,
the AdamOptimizer

00:07:30.020 --> 00:07:32.450
is a very subtle behavior
change that mostly probably

00:07:32.450 --> 00:07:33.560
won't affect you.

00:07:33.560 --> 00:07:36.350
But just in case it
might, we will convert it

00:07:36.350 --> 00:07:38.104
to compat.v1 AdamOptimizer.

00:07:41.070 --> 00:07:44.650
Once the conversion is complete
and there were no errors,

00:07:44.650 --> 00:07:47.450
then you can run the new
program in TensorFlow 2,

00:07:47.450 --> 00:07:48.510
and it'll work.

00:07:48.510 --> 00:07:49.860
That's the idea.

00:07:49.860 --> 00:07:52.085
So it should be
pretty easy for you.

00:07:52.085 --> 00:07:53.460
Hopefully, you
won't get trouble.

00:07:53.460 --> 00:07:56.270
Note that this
automatic conversion,

00:07:56.270 --> 00:07:59.400
it will fix it so it works,
but it won't fix your style

00:07:59.400 --> 00:08:02.553
to the new TensorFlow 2 style.

00:08:02.553 --> 00:08:03.970
That cannot be
done automatically.

00:08:03.970 --> 00:08:05.470
And there's a lot to
learn about this process,

00:08:05.470 --> 00:08:08.060
so if you want to know more,
check out the talk by Anna

00:08:08.060 --> 00:08:11.070
and Tomer tomorrow
at 10:30 about 2.0

00:08:11.070 --> 00:08:13.110
and porting your models.

00:08:13.110 --> 00:08:14.040
That'll be worthwhile.

00:08:14.040 --> 00:08:16.257
Of course, as we go through
this process at Google,

00:08:16.257 --> 00:08:18.090
we'll also publish a
lot more documentation.

00:08:18.090 --> 00:08:21.830
But this talk is the best start.

00:08:21.830 --> 00:08:22.330
All right.

00:08:22.330 --> 00:08:27.130
So let me give you an idea about
the timeline of all of this.

00:08:27.130 --> 00:08:31.495
We have cut 2.0 alpha
today or yesterday--

00:08:31.495 --> 00:08:34.483
it took some time building it.

00:08:34.483 --> 00:08:36.400
We're now working on
implementing some missing

00:08:36.400 --> 00:08:38.065
features that we
already know about.

00:08:38.065 --> 00:08:39.190
We're converting libraries.

00:08:39.190 --> 00:08:41.080
We're converting Google.

00:08:41.080 --> 00:08:43.150
There's lots of testing,
lots of optimization

00:08:43.150 --> 00:08:45.970
that's going to happen over
the next couple of months.

00:08:45.970 --> 00:08:49.937
We're targeting to have a
release candidate in spring.

00:08:49.937 --> 00:08:51.520
Spring is a sort of
flexible concept--

00:08:51.520 --> 00:08:53.950
[LAUGHTER]

00:08:53.950 --> 00:08:54.850
But in spring.

00:08:57.520 --> 00:08:59.260
After we have this
release candidate,

00:08:59.260 --> 00:09:02.080
it still has to go through
release testing and integration

00:09:02.080 --> 00:09:05.020
testing, so I expect that to
take a little bit longer than a

00:09:05.020 --> 00:09:06.460
for our regular release chains.

00:09:06.460 --> 00:09:10.570
But that's when you can
see an RC, and then,

00:09:10.570 --> 00:09:13.360
eventually, a final.

00:09:13.360 --> 00:09:16.720
So if you want to follow along
with the process, please do so.

00:09:16.720 --> 00:09:18.100
All of this is tracked online.

00:09:18.100 --> 00:09:21.142
It's all on the GitHub
TensorFlow 2 project tracker.

00:09:21.142 --> 00:09:22.600
So I would just go
and look at that

00:09:22.600 --> 00:09:24.183
to stay up to date
if you were looking

00:09:24.183 --> 00:09:26.770
for a particular feature or just
want to know what's going on.

00:09:26.770 --> 00:09:29.890
If you file any bugs, those end
up there as well so everybody

00:09:29.890 --> 00:09:32.890
knows what's going on.

00:09:32.890 --> 00:09:35.280
So TensorFlow 2 really wouldn't
be possible without you.

00:09:35.280 --> 00:09:36.810
That has been the case already.

00:09:36.810 --> 00:09:40.510
But in the future, even more
so, we'll need you to test this.

00:09:40.510 --> 00:09:43.840
We'll need you to tell us
what works and what doesn't.

00:09:43.840 --> 00:09:45.480
So please install
the alpha today.

00:09:45.480 --> 00:09:48.760
We're extremely excited to see
what you can create with us.

00:09:48.760 --> 00:09:51.470
So please go, install the
alpha, check out the docs.

00:09:51.470 --> 00:09:55.620
They're at tensorflow.org/r2.0.

00:09:55.620 --> 00:09:58.980
And with that, I think
you should probably

00:09:58.980 --> 00:10:02.670
hear about what it's like to
be working with TensorFlow 2.

00:10:02.670 --> 00:10:05.070
We'll have plenty of content
over the next couple of days

00:10:05.070 --> 00:10:08.820
to tell you exactly about all
the different aspects of it.

00:10:08.820 --> 00:10:12.450
But we'll start with Karmel,
who will speak about high level

00:10:12.450 --> 00:10:14.190
APIs in TensorFlow 2.0.

00:10:14.190 --> 00:10:15.003
Thank you

00:10:15.003 --> 00:10:18.384
[APPLAUSE]

00:10:21.548 --> 00:10:22.340
KARMEL ALLISON: Hi.

00:10:22.340 --> 00:10:24.740
My name is Karmel Allison,
and I'm an engineering manager

00:10:24.740 --> 00:10:25.970
for TensorFlow.

00:10:25.970 --> 00:10:28.910
Martin just told you
about TensorFlow 2.0

00:10:28.910 --> 00:10:30.368
and how to get
there. and I'm going

00:10:30.368 --> 00:10:32.452
to tell you a little bit
about what we're bringing

00:10:32.452 --> 00:10:34.070
in our high level
API and what you can

00:10:34.070 --> 00:10:37.560
expect to find when you arrive.

00:10:37.560 --> 00:10:40.650
But first, what do I mean
when I say high level APIs?

00:10:40.650 --> 00:10:42.420
There are many common
pieces and routines

00:10:42.420 --> 00:10:44.128
in building machine
learning models, just

00:10:44.128 --> 00:10:45.350
like building houses.

00:10:45.350 --> 00:10:47.880
And with our high level
APIs, we bring your tools

00:10:47.880 --> 00:10:51.180
to help you more easily and
reproducibly build your models

00:10:51.180 --> 00:10:53.767
and scale them.

00:10:53.767 --> 00:10:55.350
As Martin mentioned,
a couple of years

00:10:55.350 --> 00:10:58.340
ago, TensorFlow adopted Keras
as one of the high level

00:10:58.340 --> 00:10:59.670
APIs we offered.

00:10:59.670 --> 00:11:01.950
Keras is, at its
heart, a specification

00:11:01.950 --> 00:11:03.150
for model building.

00:11:03.150 --> 00:11:05.290
It works with multiple
machine learning frameworks,

00:11:05.290 --> 00:11:06.810
and it provides
a shared language

00:11:06.810 --> 00:11:11.580
for defining layers, models,
losses, optimizers, and so on.

00:11:11.580 --> 00:11:13.470
We implemented a
version of Keras

00:11:13.470 --> 00:11:15.060
that has been optimized
for TensorFlow

00:11:15.060 --> 00:11:17.100
inside of core
TensorFlow, and we

00:11:17.100 --> 00:11:20.640
called it TF Keras, one
of our high level APIs.

00:11:20.640 --> 00:11:23.890
Raise your hand if you
used TF Keras already?

00:11:23.890 --> 00:11:24.830
OK, a lot of you.

00:11:24.830 --> 00:11:28.280
That's good, because this
next slide will seem familiar.

00:11:28.280 --> 00:11:31.508
In its simplest form,
Keras is just that, simple.

00:11:31.508 --> 00:11:33.050
It was built from
the ground up to be

00:11:33.050 --> 00:11:35.010
Pythonic and easy to learn.

00:11:35.010 --> 00:11:37.370
And as such, it has been
instrumental in inviting people

00:11:37.370 --> 00:11:39.260
in to the machine
learning world.

00:11:39.260 --> 00:11:41.630
The code here represents
an entire model definition

00:11:41.630 --> 00:11:44.030
and training loop, meaning
it's really easy for you

00:11:44.030 --> 00:11:47.750
to design and modify your model
architectures without needing

00:11:47.750 --> 00:11:51.760
to write tons of boilerplate.

00:11:51.760 --> 00:11:55.180
At the same time, by relying
on inheritance and interfaces,

00:11:55.180 --> 00:11:57.850
Keras is extremely
flexible and customizable,

00:11:57.850 --> 00:12:00.530
which is critical for machine
learning applications.

00:12:00.530 --> 00:12:01.990
Here we have a subclass model.

00:12:01.990 --> 00:12:04.570
I can define arbitrary
model architectures,

00:12:04.570 --> 00:12:06.760
modify what happens
at each training step,

00:12:06.760 --> 00:12:11.200
and even change the whole
training loop if I want to.

00:12:11.200 --> 00:12:14.410
Which is to say, Keras
is simple and effective.

00:12:14.410 --> 00:12:16.610
Anyone can figure
out how to use it.

00:12:16.610 --> 00:12:19.990
But we had a problem, TF Keras
was built for smaller models.

00:12:19.990 --> 00:12:21.470
And many machine
learning problems,

00:12:21.470 --> 00:12:23.710
including the ones we
see internally at Google,

00:12:23.710 --> 00:12:28.050
need to operate at
a much larger scale.

00:12:28.050 --> 00:12:29.260
And so we have estimators.

00:12:29.260 --> 00:12:32.420
Estimators were built from
the ground up to distribution

00:12:32.420 --> 00:12:35.930
and scale with fault tolerance
across hundreds of machines,

00:12:35.930 --> 00:12:38.480
no questions asked.

00:12:38.480 --> 00:12:40.550
This here is the much-loved
wide and deep model,

00:12:40.550 --> 00:12:42.008
a workhorse of the
machine learning

00:12:42.008 --> 00:12:44.270
world that took many years
of research to define

00:12:44.270 --> 00:12:49.860
but is available as a built-in
for training and deployment.

00:12:49.860 --> 00:12:52.760
Which is to say, estimators
are powerful machines.

00:12:52.760 --> 00:12:55.220
But you've told us that
there's a steep learning curve,

00:12:55.220 --> 00:12:56.990
and it's not always
easy to figure out

00:12:56.990 --> 00:12:58.510
which parts to connect where.

00:13:01.390 --> 00:13:03.480
And we learned a lot
in the past two years

00:13:03.480 --> 00:13:05.472
building out
estimator in TF Keras

00:13:05.472 --> 00:13:06.930
And we've reached
the point that we

00:13:06.930 --> 00:13:08.472
don't think you
should have to choose

00:13:08.472 --> 00:13:11.460
between a simple API
and a scalable API.

00:13:11.460 --> 00:13:13.410
We want a higher
level API that takes

00:13:13.410 --> 00:13:16.140
you all the way from
[INAUDIBLE] to planet scale,

00:13:16.140 --> 00:13:19.420
no questions asked.

00:13:19.420 --> 00:13:23.820
So in TensorFlow 2.0, we are
standardizing on the Keras API

00:13:23.820 --> 00:13:25.470
for building layers and models.

00:13:25.470 --> 00:13:27.740
but we are bringing all
the power of estimators

00:13:27.740 --> 00:13:30.660
into TF Keras, so that you
can move from prototype

00:13:30.660 --> 00:13:33.780
to distributed training to
production serving in one go.

00:13:39.340 --> 00:13:40.910
OK, so brass tacks--

00:13:40.910 --> 00:13:42.270
how are we doing this?

00:13:42.270 --> 00:13:49.000
Well, this is a TF Keras model
definition in TensorFlow 1.13.

00:13:49.000 --> 00:13:51.460
And this is the same
model definition in 2.0.

00:13:51.460 --> 00:13:52.600
[LAUGHTER]

00:13:52.600 --> 00:13:55.400
Some of you may notice the
code is exactly the same.

00:13:55.400 --> 00:13:57.400
OK, so what actually
is different?

00:13:57.400 --> 00:13:59.620
Well, we've done a lot of
work to integrate Keras

00:13:59.620 --> 00:14:02.230
with all of the other features
that TensorFlow 2.0 brings

00:14:02.230 --> 00:14:03.280
to the table.

00:14:03.280 --> 00:14:06.610
For example, in 1.13 this
built a graph-based model that

00:14:06.610 --> 00:14:08.580
ran a session under the hood.

00:14:08.580 --> 00:14:10.790
In 2.0, the same
model definition

00:14:10.790 --> 00:14:15.170
will run in Eager mode
without any modification.

00:14:15.170 --> 00:14:18.280
This lets you take advantage of
everything Eager can do for us.

00:14:18.280 --> 00:14:20.710
Here we see that our data
set pipeline behaves just

00:14:20.710 --> 00:14:23.080
like a numpy array,
easy to debug

00:14:23.080 --> 00:14:25.600
and flows natively
into our Keras model.

00:14:25.600 --> 00:14:27.100
But at the same
time, data sets have

00:14:27.100 --> 00:14:29.110
been optimized for
performance so that you

00:14:29.110 --> 00:14:31.343
can iterate over and
train with a data set

00:14:31.343 --> 00:14:32.760
with minimal
performance overhead.

00:14:35.350 --> 00:14:37.090
We're able to achieve
this performance

00:14:37.090 --> 00:14:40.220
with data sets in Keras by
taking advantage of graphs,

00:14:40.220 --> 00:14:42.070
even in an Eager context.

00:14:42.070 --> 00:14:44.230
Eager makes debugging
and prototyping easy,

00:14:44.230 --> 00:14:47.800
and all the while Keras builds
an Eager-friendly function,

00:14:47.800 --> 00:14:50.230
under the hood, that
tracks your model,

00:14:50.230 --> 00:14:52.000
and the TensorFlow
runtime then takes

00:14:52.000 --> 00:14:54.880
care of optimizing for
performance and scaling.

00:14:54.880 --> 00:14:56.830
That said, you can
also explicitly run

00:14:56.830 --> 00:15:01.330
your model step-by-step in Eager
mode with the flag run_eagerly.

00:15:01.330 --> 00:15:03.682
Even though for this example,
it might be overkill,

00:15:03.682 --> 00:15:05.140
you can see that
run_eagerly allows

00:15:05.140 --> 00:15:07.540
you to use Python control flow
and Eager mode for debugging

00:15:07.540 --> 00:15:09.040
while you're
prototyping your model.

00:15:12.800 --> 00:15:14.680
Another big change
coming in 2.0 is

00:15:14.680 --> 00:15:17.350
that we have consolidated
many APIs across TensorFlow

00:15:17.350 --> 00:15:20.260
under the Keras heading,
reducing duplicative classes

00:15:20.260 --> 00:15:25.780
and making it easier to know
what you should use and when.

00:15:25.780 --> 00:15:28.180
We now have one
set of optimizers.

00:15:28.180 --> 00:15:30.430
These optimizers work
across TensorFlow

00:15:30.430 --> 00:15:33.650
in and out of eager mode, on
one machine or distributed.

00:15:33.650 --> 00:15:35.560
You can retrieve and
set hyperparamters

00:15:35.560 --> 00:15:38.140
like normal Python attributes,
and these optimizers

00:15:38.140 --> 00:15:39.610
fully serializable.

00:15:39.610 --> 00:15:40.990
The weights and
configuration can

00:15:40.990 --> 00:15:43.270
be saved both in the
TensorFlow checkpoint

00:15:43.270 --> 00:15:48.350
format and the backend
diagnostic Keras format.

00:15:48.350 --> 00:15:51.230
We have one set of metrics that
encompasses all the former TF

00:15:51.230 --> 00:15:55.010
metrics and Keras metrics and
allow for easy subclassing

00:15:55.010 --> 00:15:58.410
if you want even more.

00:15:58.410 --> 00:16:01.860
Losses, similarly, have been
consolidated into a single set

00:16:01.860 --> 00:16:04.320
with a number of frequently
used built-ins and an easily

00:16:04.320 --> 00:16:06.255
customizable interface
if you so choose.

00:16:09.910 --> 00:16:12.190
And we have one set
of layers, built

00:16:12.190 --> 00:16:13.918
in the style of Keras layers.

00:16:13.918 --> 00:16:15.460
Which means that
they are class-based

00:16:15.460 --> 00:16:16.930
and fully configurable.

00:16:16.930 --> 00:16:19.180
There are a great number of
built in layers, including

00:16:19.180 --> 00:16:21.970
all of the ones that come with
the Keras API specification.

00:16:24.535 --> 00:16:26.910
One set of layers that we took
particular care with where

00:16:26.910 --> 00:16:28.980
the RNN layers in TensorFlow.

00:16:28.980 --> 00:16:32.216
Raise your hand if you've
used RNN layers in TensorFlow?

00:16:32.216 --> 00:16:34.372
OK, these slides are for you.

00:16:34.372 --> 00:16:35.700
[LAUGHTER]

00:16:35.700 --> 00:16:38.280
In TensorFlow V1 we had
several different versions

00:16:38.280 --> 00:16:40.770
of LSTm and GRUs, and
you had to know ahead

00:16:40.770 --> 00:16:43.050
of time what device you
were optimizing for in order

00:16:43.050 --> 00:16:47.060
to get peak performance
with cuDNN kernels.

00:16:47.060 --> 00:16:50.510
In 2.0, there is one
version of the LSTM

00:16:50.510 --> 00:16:52.070
and one version
of the GRU layer.

00:16:52.070 --> 00:16:53.690
And they select
the write operation

00:16:53.690 --> 00:16:55.190
for the available
device at runtime

00:16:55.190 --> 00:16:56.780
so that you don't have to.

00:16:56.780 --> 00:16:58.790
This code here runs
the cuDNN kernel

00:16:58.790 --> 00:17:01.130
if you have GPUs
available, but also

00:17:01.130 --> 00:17:04.471
falls back to CPU ops if you
don't have GPUs available.

00:17:07.192 --> 00:17:08.650
In addition to all
of those, if you

00:17:08.650 --> 00:17:11.440
need layers that are not
included in the built-in set,

00:17:11.440 --> 00:17:14.859
TF Keras provides an API that is
easy to subclass and customize

00:17:14.859 --> 00:17:17.859
so that you can innovate on
top of the existing layers.

00:17:17.859 --> 00:17:20.359
This, in fact, is how
the community repository

00:17:20.359 --> 00:17:22.060
TensorFlow add-ons operates.

00:17:22.060 --> 00:17:25.270
They provide specialized and
particularly complex layers--

00:17:25.270 --> 00:17:27.275
metrics, optimizers, and so on--

00:17:27.275 --> 00:17:28.900
to the TensorFlow
community by building

00:17:28.900 --> 00:17:30.455
on top of the
Keras base classes.

00:17:33.410 --> 00:17:33.920
All right.

00:17:33.920 --> 00:17:35.120
So we streamlined API--

00:17:35.120 --> 00:17:36.022
that's a start.

00:17:36.022 --> 00:17:37.730
The next thing we did
was integrate Keras

00:17:37.730 --> 00:17:39.530
across all of the
tools and capabilities

00:17:39.530 --> 00:17:42.682
that TensorFlow has.

00:17:42.682 --> 00:17:44.890
One of the tools we found
critical to the development

00:17:44.890 --> 00:17:47.410
of estimators was easily
configurable structured data

00:17:47.410 --> 00:17:49.120
passing with feature columns.

00:17:49.120 --> 00:17:51.280
In TensorFlow 2.0 you
can use feature columns

00:17:51.280 --> 00:17:53.260
to parse your data
and feed it directly

00:17:53.260 --> 00:17:55.150
into downstream Keras layers.

00:17:55.150 --> 00:17:58.150
These feature columns work
both with Keras and estimators,

00:17:58.150 --> 00:18:00.790
so you can mix and match to
create reusable data input

00:18:00.790 --> 00:18:03.330
pipelines.

00:18:03.330 --> 00:18:03.830
OK.

00:18:03.830 --> 00:18:06.500
So you have data in your model,
and you're ready to train.

00:18:06.500 --> 00:18:09.440
One of the most loved tools we
have for running your training

00:18:09.440 --> 00:18:11.030
jobs is TensorBoard.

00:18:11.030 --> 00:18:13.160
And I'm pleased to
say in TensorFlow 2.0,

00:18:13.160 --> 00:18:16.760
TensorBoard integration with
Keras is as simple as one line.

00:18:16.760 --> 00:18:18.980
Here we add a TensorBoard
callback to our model

00:18:18.980 --> 00:18:20.360
when training.

00:18:20.360 --> 00:18:23.510
And this gets us both
our training progress--

00:18:23.510 --> 00:18:25.580
here we see accuracy and loss--

00:18:25.580 --> 00:18:28.040
and a conceptual graph
representing the model we

00:18:28.040 --> 00:18:31.340
built layer-by-layer.

00:18:31.340 --> 00:18:33.770
As an added bonus,
this same callback even

00:18:33.770 --> 00:18:35.942
includes full profiling
of your model,

00:18:35.942 --> 00:18:38.150
so you can better understand
your model's performance

00:18:38.150 --> 00:18:40.430
and device placement, and
you can more readily find

00:18:40.430 --> 00:18:42.200
ways to minimize bottlenecks.

00:18:44.960 --> 00:18:47.420
Speaking of performance,
one of the exciting pieces

00:18:47.420 --> 00:18:49.430
we're bringing to
TensorFlow 2.0 is

00:18:49.430 --> 00:18:51.800
the tf.distribute.Strategy API.

00:18:51.800 --> 00:18:53.450
In TensorFlow 2.0
we will have a set

00:18:53.450 --> 00:18:55.880
of built in strategies for
distributing your training

00:18:55.880 --> 00:18:58.190
workflows that work
natively with Keras.

00:18:58.190 --> 00:19:00.560
These APIs have been
designed to be easy to use,

00:19:00.560 --> 00:19:02.720
to have great
out-of-the-box performance,

00:19:02.720 --> 00:19:04.850
and to be versatile enough
to handle many different

00:19:04.850 --> 00:19:07.200
distribution
architectures and devices.

00:19:07.200 --> 00:19:08.460
So how do they work?

00:19:08.460 --> 00:19:12.910
Well, with Keras you can add and
switch distribution strategies

00:19:12.910 --> 00:19:14.750
with only a few lines of code.

00:19:14.750 --> 00:19:16.750
Here we are distributing
the same model you just

00:19:16.750 --> 00:19:19.930
saw across multiple GPUs by
defining the model network,

00:19:19.930 --> 00:19:22.480
compiling, and fitting within
the distribution strategy

00:19:22.480 --> 00:19:23.470
scope.

00:19:23.470 --> 00:19:25.710
Because we've integrated
distribution strategies

00:19:25.710 --> 00:19:28.090
throughout Keras in
TensorFlow, you get a lot

00:19:28.090 --> 00:19:29.500
with these few lines of code.

00:19:29.500 --> 00:19:32.440
Data will be prefetched
to GPUs batch-by-batch,

00:19:32.440 --> 00:19:34.060
variables will be
mirrored in sync

00:19:34.060 --> 00:19:36.880
across all available
devices using allreduce,

00:19:36.880 --> 00:19:40.870
and we see greater than
90% over multiple GPUs.

00:19:40.870 --> 00:19:44.380
This means you can scale your
model up without changing code

00:19:44.380 --> 00:19:46.645
and without losing any of
the conveniences of Keras.

00:19:49.237 --> 00:19:50.820
Once you've trained
your model, you're

00:19:50.820 --> 00:19:53.237
likely going to want to package
it for use with production

00:19:53.237 --> 00:19:56.000
systems, mobile phones, or
other programming languages,

00:19:56.000 --> 00:19:57.810
Keras models can
now export directly

00:19:57.810 --> 00:19:59.340
to saved_model,
the serialization

00:19:59.340 --> 00:20:02.290
format that works across
the TensorFlow ecosystem.

00:20:02.290 --> 00:20:03.720
This functionality
is experimental

00:20:03.720 --> 00:20:06.600
while we iron out a few
API details, but today,

00:20:06.600 --> 00:20:08.760
and going into
2.0, you can easily

00:20:08.760 --> 00:20:13.150
export your models for use with
TF Serving, TF Lite, and more.

00:20:13.150 --> 00:20:15.090
You can also easily
reload these models back

00:20:15.090 --> 00:20:17.045
into Python to continue
training and using

00:20:17.045 --> 00:20:18.045
in your normal workflow.

00:20:21.340 --> 00:20:23.400
So that's where we are
today in the alpha.

00:20:23.400 --> 00:20:24.840
But we're obviously
not done yet,

00:20:24.840 --> 00:20:27.090
so let me tell you a little
bit about what's coming up

00:20:27.090 --> 00:20:28.048
in the next few months.

00:20:30.610 --> 00:20:32.560
We talked about
distributing over multi-GPU,

00:20:32.560 --> 00:20:33.910
but that's just the start.

00:20:33.910 --> 00:20:36.850
Using the same model code,
we can swap out our strategy

00:20:36.850 --> 00:20:38.740
and scale up to multiple nodes.

00:20:38.740 --> 00:20:40.210
Here we take the
same Keras model

00:20:40.210 --> 00:20:43.000
we just saw and distribute
it across multiple machines,

00:20:43.000 --> 00:20:44.890
all working in sync
using collective ops

00:20:44.890 --> 00:20:47.560
to train your model
faster than ever before.

00:20:47.560 --> 00:20:50.290
Multi-worker mirrored strategy
currently supports ring

00:20:50.290 --> 00:20:52.990
and nickel for allreduce, which
allows us to achieve great

00:20:52.990 --> 00:20:54.590
out-of-the-box performance.

00:20:54.590 --> 00:20:56.210
This API is still
being developed,

00:20:56.210 --> 00:20:58.420
but you can try it out
today in the nightlies

00:20:58.420 --> 00:21:00.670
if you are interested in a
TensorFlow-native solution

00:21:00.670 --> 00:21:02.260
to synchronous multi
worker training.

00:21:04.810 --> 00:21:06.900
And we are very excited
to say that using Keras

00:21:06.900 --> 00:21:08.820
and distribute
strategies you will also

00:21:08.820 --> 00:21:11.780
be able to use the same code to
distribute your model on TPUs.

00:21:11.780 --> 00:21:14.280
You'll have to wait for the
next TensorFlow release for this

00:21:14.280 --> 00:21:15.648
to actually work on TPUs.

00:21:15.648 --> 00:21:17.190
But when it's
released, this strategy

00:21:17.190 --> 00:21:19.920
will still work on
Google Cloud TPUs

00:21:19.920 --> 00:21:22.575
and with Colab, which can
now access TPUs directly.

00:21:25.680 --> 00:21:28.450
And as we approach
the final 2.0 release,

00:21:28.450 --> 00:21:31.120
we will continue bringing
scalability into Keras.

00:21:31.120 --> 00:21:33.670
We will be enabling distribution
with parameter service

00:21:33.670 --> 00:21:36.160
strategy, which is the
multi-node asynchronous

00:21:36.160 --> 00:21:38.260
training that
estimator does today.

00:21:38.260 --> 00:21:41.110
We will also be exposing canned
estimator like wide and deep

00:21:41.110 --> 00:21:43.480
directly from the Keras
API for those of you

00:21:43.480 --> 00:21:46.290
who want a higher
level higher level API.

00:21:46.290 --> 00:21:48.700
And we will be adding support
for partitioned variables

00:21:48.700 --> 00:21:51.340
across many machines for some
of the largest scale models,

00:21:51.340 --> 00:21:52.690
like those we run at Google.

00:21:55.840 --> 00:21:57.550
And that's just a sneak peak.

00:21:57.550 --> 00:21:59.740
So please do check
out the 2.0 preview

00:21:59.740 --> 00:22:02.013
and try out TF Keras
if you haven't already.

00:22:02.013 --> 00:22:03.430
If you have more
questions, you'll

00:22:03.430 --> 00:22:06.580
be hearing more throughout the
day about Keras and high level

00:22:06.580 --> 00:22:08.922
API and how they integrate
across TensorFlow.

00:22:08.922 --> 00:22:11.380
And we also have a workshop
tomorrow that Martin mentioned.

00:22:11.380 --> 00:22:13.530
[MUSIC PLAYING]

