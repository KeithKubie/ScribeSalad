WEBVTT
Kind: captions
Language: en

00:00:01.514 --> 00:00:02.680
SIR DERMOT TURING: Good day.

00:00:02.680 --> 00:00:03.440
It's not morning.

00:00:03.440 --> 00:00:06.175
It's not afternoon it's kind
of lunchtime, so good day.

00:00:06.175 --> 00:00:08.780
And if you're Australian,
you'll understand that.

00:00:08.780 --> 00:00:13.194
And I'm going to talk about
computers, because that's

00:00:13.194 --> 00:00:14.485
what Alan Turing is famous for.

00:00:14.485 --> 00:00:16.780
So I'm going to skip the
whole story of his life.

00:00:16.780 --> 00:00:21.620
And I just thought we'd
home in on some machines.

00:00:21.620 --> 00:00:26.990
And I like this stuff, because
unlike modern computers,

00:00:26.990 --> 00:00:31.170
they sort of have moving parts
and they're hot and smelly

00:00:31.170 --> 00:00:35.040
and they're actually
fun to have around.

00:00:35.040 --> 00:00:37.670
And when they go wrong,
you can pick up a spanner

00:00:37.670 --> 00:00:40.440
and try and fix them.

00:00:40.440 --> 00:00:44.490
And that's kind of where
Alan Turing was on machines.

00:00:44.490 --> 00:00:48.230
You can trace this right back to
his childhood and the carpentry

00:00:48.230 --> 00:00:52.800
lessons that he took at a
school called Hazelhurst, where

00:00:52.800 --> 00:00:55.290
he and my father were sent.

00:00:55.290 --> 00:00:58.814
And the emphasis-- very
interesting for the early part

00:00:58.814 --> 00:01:00.230
of the 20th century--
the emphasis

00:01:00.230 --> 00:01:03.630
was on practical and being
able to do it yourself,

00:01:03.630 --> 00:01:07.300
not depending on other
people to fix stuff for you.

00:01:07.300 --> 00:01:09.300
And I think that's
really quite interesting.

00:01:09.300 --> 00:01:13.260
So he was completely
terrible in the workshop.

00:01:13.260 --> 00:01:17.780
His soldering technique
defied belief.

00:01:17.780 --> 00:01:20.700
But nevertheless, it
was sort of somehow

00:01:20.700 --> 00:01:22.260
a personal failing
if you couldn't

00:01:22.260 --> 00:01:23.630
try and fix it yourself.

00:01:23.630 --> 00:01:27.930
So he's an interesting
hands-on kind of guy.

00:01:27.930 --> 00:01:33.340
And you can trace throughout
the whole of his career

00:01:33.340 --> 00:01:36.930
the importance of moving
from the theoretical

00:01:36.930 --> 00:01:39.840
into the practical sphere.

00:01:39.840 --> 00:01:43.430
So we can start with the
extremely theoretical,

00:01:43.430 --> 00:01:46.104
the whole idea that came
out of this famous paper

00:01:46.104 --> 00:01:48.020
on computable numbers
that I'm sure you've all

00:01:48.020 --> 00:01:49.970
heard of, this
thing that set out

00:01:49.970 --> 00:01:54.184
the blueprint for a
universal computing machine.

00:01:54.184 --> 00:01:55.600
But this is all
quite interesting,

00:01:55.600 --> 00:01:57.270
because this wasn't
about machinery.

00:01:57.270 --> 00:02:01.050
This paper was intended
to be about solving

00:02:01.050 --> 00:02:03.590
the fundamental problem in
mathematics, which is about

00:02:03.590 --> 00:02:07.320
whether there are
rules underpinning

00:02:07.320 --> 00:02:12.010
the whole of mathematics and
in particular the question

00:02:12.010 --> 00:02:16.280
whether you could have a sort
of process that would enable you

00:02:16.280 --> 00:02:20.860
to tell ab initio whether any
particular mathematical theorem

00:02:20.860 --> 00:02:22.820
is provable or not.

00:02:22.820 --> 00:02:25.080
And the reason I
put this picture

00:02:25.080 --> 00:02:27.420
of this slightly balding
guy on the left hand

00:02:27.420 --> 00:02:30.150
side of this slide is
because that's Max Newman.

00:02:30.150 --> 00:02:33.790
Max Newman, as you all
know, very important

00:02:33.790 --> 00:02:35.510
person in the
development of computers,

00:02:35.510 --> 00:02:39.000
but also Alan Turing's mentor
for the whole of his life.

00:02:39.000 --> 00:02:41.580
And it was when attending
Max Newman's lectures

00:02:41.580 --> 00:02:45.515
on the problem of decidability,
the Entscheidungsproblem,

00:02:45.515 --> 00:02:48.490
that Max Newman explains
the problem in terms

00:02:48.490 --> 00:02:50.690
of a mechanical process.

00:02:50.690 --> 00:02:52.830
Is there a mechanical
process that you

00:02:52.830 --> 00:02:57.030
can apply to know whether any
particular theorem is provable

00:02:57.030 --> 00:02:59.360
or not?

00:02:59.360 --> 00:03:00.960
And so Alan Turing
took away this idea

00:03:00.960 --> 00:03:04.000
of a mechanical
process and came up

00:03:04.000 --> 00:03:08.330
with his idea of the universal
Turing machine, which

00:03:08.330 --> 00:03:09.744
you're all familiar with.

00:03:09.744 --> 00:03:11.660
So there's Alan Turing
on the right hand side,

00:03:11.660 --> 00:03:13.510
and that's a picture
taken at about the time

00:03:13.510 --> 00:03:14.760
he wrote this paper.

00:03:14.760 --> 00:03:18.630
That's him in his
hometown of Guilford.

00:03:18.630 --> 00:03:20.860
And apparently, he's
walking to the station

00:03:20.860 --> 00:03:22.650
to get away from
his ghastly parents

00:03:22.650 --> 00:03:28.247
and to go back to Cambridge
to work on his paper.

00:03:28.247 --> 00:03:29.580
I mentioned his ghastly parents.

00:03:29.580 --> 00:03:32.250
Actually it's kind of unfair to
the parents, who weren't really

00:03:32.250 --> 00:03:35.390
that ghastly, but he didn't get
on particularly well with them

00:03:35.390 --> 00:03:38.380
and that's probably down
to having been separated

00:03:38.380 --> 00:03:40.260
from them in his childhood.

00:03:40.260 --> 00:03:42.470
He was getting on much
better with this lady, who

00:03:42.470 --> 00:03:46.172
was the mother of his childhood
friend Christopher Morcom.

00:03:46.172 --> 00:03:48.509
And there's a lot of
mythology around Alan Turing's

00:03:48.509 --> 00:03:50.050
relationship with
Christopher Morcom.

00:03:50.050 --> 00:03:52.910
Actually, the real relationship
was with Christopher Morcom's

00:03:52.910 --> 00:03:53.780
mother.

00:03:53.780 --> 00:03:55.740
And it started after
Christopher had died.

00:03:55.740 --> 00:03:59.770
And these two found
complementary relationships

00:03:59.770 --> 00:04:00.750
with each other.

00:04:00.750 --> 00:04:05.940
Alan Turing found a maternal
figure who was basically

00:04:05.940 --> 00:04:08.110
looking for her lost son.

00:04:08.110 --> 00:04:12.962
But also, she came from a very
important scientific family.

00:04:12.962 --> 00:04:14.420
Her father invented
the light bulb.

00:04:14.420 --> 00:04:15.680
And I know I've
offended all of you

00:04:15.680 --> 00:04:17.450
here in the audience
who are Americans

00:04:17.450 --> 00:04:19.074
who believe the light
bulb was invented

00:04:19.074 --> 00:04:20.454
by some guy called Edison.

00:04:20.454 --> 00:04:22.120
But if you're British,
you don't believe

00:04:22.120 --> 00:04:24.560
[INAUDIBLE] was invented
by Sir Joseph Swan.

00:04:24.560 --> 00:04:26.450
So her father was
Sir Joseph Swan.

00:04:26.450 --> 00:04:27.940
He invented the light bulb.

00:04:27.940 --> 00:04:31.880
And she had-- not only was she
a sculptress and an artist,

00:04:31.880 --> 00:04:35.440
but she liked her kids to get
stuck in on scientific stuff.

00:04:35.440 --> 00:04:38.130
They had a laboratory at
their place at Bromsgrove

00:04:38.130 --> 00:04:39.290
and they did experiments.

00:04:39.290 --> 00:04:41.270
And this was very interesting,
because my grandmother

00:04:41.270 --> 00:04:43.561
would never have tolerated
that kind of thing going on.

00:04:43.561 --> 00:04:45.130
It would have been
messy and smelly.

00:04:45.130 --> 00:04:49.110
So Alan fitted in really
well with the Morcom way

00:04:49.110 --> 00:04:51.510
of doing things.

00:04:51.510 --> 00:04:54.160
And this is interesting because
this is in the mid-1930s.

00:04:54.160 --> 00:04:57.690
Chris was long dead and he's
still going and visiting

00:04:57.690 --> 00:04:58.230
Mrs. Morcom.

00:04:58.230 --> 00:05:01.230
And he's talking to
her about algorithms.

00:05:01.230 --> 00:05:02.560
This is the most bizarre thing.

00:05:02.560 --> 00:05:04.855
You go and talk to
somebody about algorithms.

00:05:07.337 --> 00:05:09.420
If you've ever tried it--
and I'm sure most of you

00:05:09.420 --> 00:05:12.770
have-- it kind of doesn't
work as a social gambit.

00:05:12.770 --> 00:05:15.790
So he's talking to
her about an algorithm

00:05:15.790 --> 00:05:17.055
for playing the game of Go.

00:05:17.055 --> 00:05:18.430
And these are the
drawings he had

00:05:18.430 --> 00:05:20.570
made for her, to sort of
set out the rules of Go.

00:05:20.570 --> 00:05:22.450
He's already starting
to think about how

00:05:22.450 --> 00:05:26.887
you'd program that universal
computing machine that he's

00:05:26.887 --> 00:05:29.220
trying to explain to her about
the Entscheidungsprogram.

00:05:29.220 --> 00:05:32.240
That conversation probably
didn't go too well either.

00:05:32.240 --> 00:05:34.510
But she is interested
in the game of Go,

00:05:34.510 --> 00:05:38.500
and he's willing to have
a go at trying to set out

00:05:38.500 --> 00:05:40.589
the rules for how
you'd program a machine

00:05:40.589 --> 00:05:41.630
to do that sort of thing.

00:05:41.630 --> 00:05:43.970
This is happening
before World War II,

00:05:43.970 --> 00:05:47.210
before anybody's
even thought of how

00:05:47.210 --> 00:05:50.370
you might go about
creating, in physical form,

00:05:50.370 --> 00:05:53.290
a universal computing machine.

00:05:53.290 --> 00:05:56.210
And then he goes
off to Princeton,

00:05:56.210 --> 00:05:59.860
and he spends a lot of time
thinking about machinery.

00:05:59.860 --> 00:06:02.190
And he goes down
to the machine shop

00:06:02.190 --> 00:06:06.560
and he starts making some
sort of mechanical multiplier.

00:06:06.560 --> 00:06:07.900
I have no idea how that worked.

00:06:07.900 --> 00:06:09.985
There are no drawings that
survive of that thing,

00:06:09.985 --> 00:06:11.360
not to my knowledge, anyway.

00:06:11.360 --> 00:06:14.450
But he comes back
and when he gets back

00:06:14.450 --> 00:06:17.330
to Cambridge in the
late 1930s-- this

00:06:17.330 --> 00:06:21.000
is about 1938--
he started to work

00:06:21.000 --> 00:06:25.570
on a physical
embodiment of a machine

00:06:25.570 --> 00:06:29.760
to try and solve
Riemann's Zeta function.

00:06:29.760 --> 00:06:32.300
And this is an analog machine.

00:06:32.300 --> 00:06:34.310
It's got tooth to gear wheels.

00:06:34.310 --> 00:06:36.530
Each of the gear
wheels has a number

00:06:36.530 --> 00:06:38.320
of teeth that is a prime number.

00:06:38.320 --> 00:06:40.860
And you put them together
and it will actually

00:06:40.860 --> 00:06:44.090
approximates to something
that will solve Riemann's Zeta

00:06:44.090 --> 00:06:44.590
function.

00:06:44.590 --> 00:06:48.740
And this is very
beautifully drawn

00:06:48.740 --> 00:06:51.860
blueprint that's in the library
at King's College, Cambridge.

00:06:51.860 --> 00:06:53.310
Alan didn't draw it.

00:06:53.310 --> 00:06:56.900
That's not what he was good at.

00:06:56.900 --> 00:06:59.256
But it was his
conception for a machine.

00:06:59.256 --> 00:07:00.880
And then of course
you know about this.

00:07:00.880 --> 00:07:01.910
This is a very famous machine.

00:07:01.910 --> 00:07:04.201
You can go and see a replica
of this at Bletchley Park.

00:07:04.201 --> 00:07:05.828
This the bomb.

00:07:05.828 --> 00:07:07.950
This is the machine
that was designed

00:07:07.950 --> 00:07:12.640
to find the settings
of the Enigma machine

00:07:12.640 --> 00:07:16.110
that the Germans were using to
encode their secret messages.

00:07:16.110 --> 00:07:19.580
So all we're seeing through
Alan Turing is not just theory.

00:07:19.580 --> 00:07:24.831
We're seeing physical
embodiments of his ideas.

00:07:24.831 --> 00:07:26.080
And this is quite interesting.

00:07:26.080 --> 00:07:28.126
This is a digital machine.

00:07:28.126 --> 00:07:30.500
It's not a universal machine,
but it's a digital machine.

00:07:30.500 --> 00:07:32.820
It gives you yes or no answer.

00:07:32.820 --> 00:07:39.620
And the wiring that is used
to-- and the way that you could

00:07:39.620 --> 00:07:43.990
reset the wiring around
the back of the machine

00:07:43.990 --> 00:07:47.270
in order to try
and solve today's

00:07:47.270 --> 00:07:49.890
settings-- very
interesting idea.

00:07:49.890 --> 00:07:51.690
So we can see the
evolution of things

00:07:51.690 --> 00:07:54.410
from these analog machines,
like the Riemann's Zeta function

00:07:54.410 --> 00:07:57.140
machine into something
that's more digital.

00:07:57.140 --> 00:07:58.700
OK, this is still
electromechanical.

00:07:58.700 --> 00:08:00.560
It's still single purpose.

00:08:00.560 --> 00:08:02.780
But it's re-wirable.

00:08:02.780 --> 00:08:06.095
It solves a different problem
every day-- interesting.

00:08:06.095 --> 00:08:07.220
Then of course, this thing.

00:08:07.220 --> 00:08:09.759
Now there's another piece
of mythology around this.

00:08:09.759 --> 00:08:11.050
This, as you know, is Colossus.

00:08:11.050 --> 00:08:12.580
That's where we are today.

00:08:12.580 --> 00:08:16.200
The room-- never mind.

00:08:16.200 --> 00:08:17.760
And this is Colossus.

00:08:17.760 --> 00:08:20.450
So this is not anything to
do with Enigma-- myth number

00:08:20.450 --> 00:08:21.610
one exploded.

00:08:21.610 --> 00:08:26.900
It is to do with the
Lorenz teleprinter cipher.

00:08:26.900 --> 00:08:29.470
And no, Alan Turing didn't
design this machine.

00:08:29.470 --> 00:08:31.920
So myth number two
exploded as well.

00:08:31.920 --> 00:08:36.090
Max Newman was in charge of the
team that built the Colossus.

00:08:36.090 --> 00:08:38.669
But Alan Turing was a
consultant to the project.

00:08:38.669 --> 00:08:41.179
So he knew about
electronic valves.

00:08:41.179 --> 00:08:45.320
He knew about the weird
experiments they were doing.

00:08:45.320 --> 00:08:47.870
When the machine was
having down time,

00:08:47.870 --> 00:08:49.850
the mathematicians were
programming this thing

00:08:49.850 --> 00:08:53.020
to do long division.

00:08:53.020 --> 00:08:54.150
That's interesting.

00:08:54.150 --> 00:08:57.420
That feels more like a
universal computing machine.

00:08:57.420 --> 00:09:01.260
It's designed for doing
statistical analysis

00:09:01.260 --> 00:09:02.779
on these teleprinter codes.

00:09:02.779 --> 00:09:05.320
They're actually using it for
something completely different.

00:09:05.320 --> 00:09:07.930
[INAUDIBLE] did his head in
because the mathematicians were

00:09:07.930 --> 00:09:11.360
trying to play with
it rather than use it

00:09:11.360 --> 00:09:13.390
for what it was meant for.

00:09:13.390 --> 00:09:15.175
So after World War
II, Alan Turing

00:09:15.175 --> 00:09:17.050
goes off to the National
Physical Laboratory.

00:09:17.050 --> 00:09:19.770
And here we have
an overt program

00:09:19.770 --> 00:09:23.297
to design a universal
electronic computing machine.

00:09:23.297 --> 00:09:25.130
And they got into all
sorts of difficulties,

00:09:25.130 --> 00:09:27.296
because as you can imagine,
this is a civil service.

00:09:27.296 --> 00:09:29.780
This is the austerity
years after the war.

00:09:29.780 --> 00:09:32.290
Resources are very
difficult to get hold of.

00:09:32.290 --> 00:09:34.850
And it took forever to build it.

00:09:34.850 --> 00:09:42.620
This photograph shows it in
a state of half-built nudity.

00:09:42.620 --> 00:09:45.780
And this photograph's
taken in about '56,

00:09:45.780 --> 00:09:47.500
'57, roundabout then.

00:09:47.500 --> 00:09:50.180
The machine still isn't
built. Alan Turing

00:09:50.180 --> 00:09:53.060
is already dead by the time
this photograph was taken.

00:09:53.060 --> 00:09:55.120
So you can imagine
how frustrated

00:09:55.120 --> 00:10:00.510
he'd got when he finished doing
his design and his programming

00:10:00.510 --> 00:10:01.810
manual for this machine.

00:10:01.810 --> 00:10:08.190
He'd done all the
work before 1946.

00:10:08.190 --> 00:10:11.090
And it's going to take another
10 years before this thing is

00:10:11.090 --> 00:10:14.120
actually live.

00:10:14.120 --> 00:10:18.470
So he's going
absolutely spare not

00:10:18.470 --> 00:10:20.660
being able to use
the machine that he's

00:10:20.660 --> 00:10:22.620
designed and can program.

00:10:22.620 --> 00:10:25.280
It doesn't exist yet, so he's
looking for somewhere else

00:10:25.280 --> 00:10:25.780
to go.

00:10:25.780 --> 00:10:28.770
And of course, he's rescued
by our friend Max Newman

00:10:28.770 --> 00:10:30.420
and he's taken to
Manchester, where

00:10:30.420 --> 00:10:33.700
they have something that was
called a marvel of our time.

00:10:33.700 --> 00:10:34.443
And this is it.

00:10:34.443 --> 00:10:37.070
Doesn't it look dreadful?

00:10:37.070 --> 00:10:40.980
Max Newman had managed
to rescue two leftover

00:10:40.980 --> 00:10:45.040
Colossi and a seven-ton
truck full of spare parts

00:10:45.040 --> 00:10:46.219
from Bletchely Park.

00:10:46.219 --> 00:10:48.260
And there's records in
the National Archives here

00:10:48.260 --> 00:10:50.370
that show he's asked
for permission to take

00:10:50.370 --> 00:10:52.980
these bits away with
him, because he's

00:10:52.980 --> 00:10:56.250
founding the computing
laboratory at Manchester

00:10:56.250 --> 00:10:57.920
University.

00:10:57.920 --> 00:11:01.075
And that's what they built.
This is the Manchester baby.

00:11:01.075 --> 00:11:04.980
It had a memory of 1024 bits.

00:11:04.980 --> 00:11:07.330
Wasn't very easy
to design programs

00:11:07.330 --> 00:11:09.625
that were run on a very
small memory like that.

00:11:09.625 --> 00:11:12.370
Max Newman was extremely
clever, and he came up

00:11:12.370 --> 00:11:17.700
with this thing, which
is a problem to find out

00:11:17.700 --> 00:11:18.999
about mess and primes.

00:11:18.999 --> 00:11:21.540
I won't go into the detail of
it because time doesn't permit.

00:11:21.540 --> 00:11:23.309
But The Times got
a hold of this.

00:11:23.309 --> 00:11:25.100
And they thought it
was absolutely amazing.

00:11:25.100 --> 00:11:27.520
There was this guy who
built this electronic brain.

00:11:27.520 --> 00:11:29.125
So you can ses--
it's very faint.

00:11:29.125 --> 00:11:29.750
You can see it.

00:11:29.750 --> 00:11:33.460
It says, "The Mechanical
Brain" and an answer

00:11:33.460 --> 00:11:35.140
to 300-year-old sum.

00:11:35.140 --> 00:11:37.630
They'd found this problem
that had been left unsolved

00:11:37.630 --> 00:11:38.660
for 300 years.

00:11:38.660 --> 00:11:40.660
So of course, it
grabs the headlines.

00:11:40.660 --> 00:11:43.060
This is the Dan Dare era.

00:11:43.060 --> 00:11:46.730
You've got rockets and
computers and robots

00:11:46.730 --> 00:11:48.060
and all that kind of stuff.

00:11:48.060 --> 00:11:51.090
So Max Newman
invented this program

00:11:51.090 --> 00:11:53.830
that you could actually
solve on the thing.

00:11:53.830 --> 00:11:55.870
And then of course, they
phoned up Alan Turing

00:11:55.870 --> 00:11:58.197
and Alan Turing goes on
about whether computers

00:11:58.197 --> 00:12:00.030
can write sonnets and
all this kind of stuff

00:12:00.030 --> 00:12:02.140
and gets into all sorts
of terrible trouble

00:12:02.140 --> 00:12:04.200
with journalists.

00:12:04.200 --> 00:12:07.910
It sort of got a ticking
off from Max Newman.

00:12:07.910 --> 00:12:11.350
There's lots of fun around that.

00:12:11.350 --> 00:12:14.390
As you can tell, that computer
really was not very useful.

00:12:14.390 --> 00:12:16.550
And it did look rather scrappy.

00:12:16.550 --> 00:12:18.542
But it was kind of like
a proof of concept.

00:12:18.542 --> 00:12:20.250
And it enable them to
go out to a company

00:12:20.250 --> 00:12:23.490
called Ferranti, who made
them a proper computer,

00:12:23.490 --> 00:12:28.480
this rather clean-looking young
lady with a very clean-looking

00:12:28.480 --> 00:12:29.770
computer.

00:12:29.770 --> 00:12:33.130
So this was delivered
to Manchester University

00:12:33.130 --> 00:12:36.680
probably about 1949, 1950.

00:12:36.680 --> 00:12:40.500
And it looks a bit more
professional, doesn't isn't it.

00:12:40.500 --> 00:12:43.140
You can't see the wires.

00:12:43.140 --> 00:12:45.710
But the reality is, on the
right hand side of the slide--

00:12:45.710 --> 00:12:49.840
and if you look carefully just
above the bit of printout,

00:12:49.840 --> 00:12:51.930
you'll see Alan Turing's
handwriting which says,

00:12:51.930 --> 00:12:53.430
how did this happen?

00:12:53.430 --> 00:12:55.830
These things were fiendishly
difficult to program.

00:12:55.830 --> 00:12:58.490
And everything went
horribly, horribly wrong.

00:12:58.490 --> 00:13:03.180
But despite the photograph,
I think the reality

00:13:03.180 --> 00:13:07.760
is much more challenging.

00:13:07.760 --> 00:13:09.770
But you can see what he
was starting to work on.

00:13:09.770 --> 00:13:11.750
He wasn't working on
mathematical problems,

00:13:11.750 --> 00:13:12.020
particularly.

00:13:12.020 --> 00:13:13.440
He was working on
biological ones.

00:13:13.440 --> 00:13:17.800
He was maths and the computer
to help him solve new problems

00:13:17.800 --> 00:13:22.160
that he was interested in, in
relation to growth and form--

00:13:22.160 --> 00:13:26.050
how does a horse embryo
turn from something

00:13:26.050 --> 00:13:28.900
that's a tennis ball shape into
something that's a horse shape?

00:13:28.900 --> 00:13:32.270
As Alan Turing said, a horse is
not a spherically symmetrical

00:13:32.270 --> 00:13:33.070
object.

00:13:33.070 --> 00:13:36.210
So he had some
mathematical formulae

00:13:36.210 --> 00:13:37.850
that would explain this.

00:13:37.850 --> 00:13:39.180
But he needed to prove it.

00:13:39.180 --> 00:13:41.330
And so what he did was
he's got the computer

00:13:41.330 --> 00:13:43.110
here to print out--
well, actually

00:13:43.110 --> 00:13:44.740
he's written it out in longhand.

00:13:44.740 --> 00:13:46.430
But it's a contour map.

00:13:46.430 --> 00:13:49.770
So each of these two-digit
hexadecimal numbers

00:13:49.770 --> 00:13:52.950
is showing you whether it's a
large number or a small number.

00:13:52.950 --> 00:13:57.520
And so what he's done is he's
shaded the contour map in three

00:13:57.520 --> 00:13:59.890
different shadings
so that you can

00:13:59.890 --> 00:14:02.830
see blotches appearing like
you might on a Frisian cow,

00:14:02.830 --> 00:14:03.570
for example.

00:14:03.570 --> 00:14:05.700
That's what his paper did.

00:14:05.700 --> 00:14:08.450
And you can see him here
standing at the console.

00:14:08.450 --> 00:14:10.710
He's the guy on
the right standing

00:14:10.710 --> 00:14:17.810
at the console of the Ferranti
1 computer in Manchester.

00:14:17.810 --> 00:14:21.562
So that's the stuff that he was
working on at the time he died.

00:14:21.562 --> 00:14:23.895
We all know the story about
what happened to Alan Turing

00:14:23.895 --> 00:14:24.853
at the end of his life.

00:14:24.853 --> 00:14:26.360
So I'm not going
to dwell on that.

00:14:26.360 --> 00:14:27.860
But I thought you
might like to have

00:14:27.860 --> 00:14:30.190
a look at some of
these press cuttings

00:14:30.190 --> 00:14:32.600
from the time of his death.

00:14:32.600 --> 00:14:34.690
I suspect that the
print's a bit small.

00:14:34.690 --> 00:14:38.380
But some of the headlines
are quite-- perhaps

00:14:38.380 --> 00:14:40.089
one is not supposed
to find this amusing,

00:14:40.089 --> 00:14:42.046
but I just think it's
quite interesting the way

00:14:42.046 --> 00:14:44.370
the tabloid press pick up
what Alan Turing was doing.

00:14:44.370 --> 00:14:46.870
And so we have,
"Scientist Is Found Dead."

00:14:46.870 --> 00:14:49.760
but we also get a thing
that says, "Robot Brain

00:14:49.760 --> 00:14:51.340
Man" on one of these things.

00:14:51.340 --> 00:14:54.490
And underneath the
photograph there,

00:14:54.490 --> 00:14:56.240
you probably can't
read that, but it says,

00:14:56.240 --> 00:14:59.900
"Dr. Turing, He Fed The Brain."

00:14:59.900 --> 00:15:02.810
I mean, it's just
terrific 1950s stuff.

00:15:02.810 --> 00:15:06.380
So anyway, that's enough for me.

00:15:06.380 --> 00:15:08.300
I'm happy to stay here.

00:15:08.300 --> 00:15:09.960
Do I stay and answer
questions now?

00:15:09.960 --> 00:15:11.280
Is that the plan?

00:15:11.280 --> 00:15:11.950
Very good.

00:15:11.950 --> 00:15:13.580
So I'll take questions
and Christina

00:15:13.580 --> 00:15:15.610
will tell me when time's up.

00:15:15.610 --> 00:15:18.532
FEMALE SPEAKER: [INAUDIBLE].

00:15:18.532 --> 00:15:20.480
I have some questions to ask.

00:15:20.480 --> 00:15:22.915
I'm sure everyone
else does as well.

00:15:22.915 --> 00:15:25.350
So who'd like to start?

00:15:31.320 --> 00:15:33.581
AUDIENCE: Do you have
any personal memories

00:15:33.581 --> 00:15:34.206
of Alan Turing?

00:15:34.206 --> 00:15:41.020
SIR DERMOT TURING: I know to
you guys I look 1,000 years old.

00:15:41.020 --> 00:15:42.850
I'm sure I'm the
oldest person who's

00:15:42.850 --> 00:15:44.240
ever stood behind this lectern.

00:15:44.240 --> 00:15:46.320
But no, I'm afraid not.

00:15:46.320 --> 00:15:47.480
He died in 1954.

00:15:47.480 --> 00:15:49.280
I was born in 1961.

00:15:49.280 --> 00:15:53.074
All you guys can do the math.

00:15:53.074 --> 00:15:54.240
FEMALE SPEAKER: [INAUDIBLE].

00:15:57.640 --> 00:15:59.560
AUDIENCE: How would
you suggest he should

00:15:59.560 --> 00:16:02.482
be taught about in schools?

00:16:02.482 --> 00:16:05.450
SIR DERMOT TURING: There are as
many answers to that question

00:16:05.450 --> 00:16:07.800
as there are people
in the room, but I

00:16:07.800 --> 00:16:10.460
think he's a very
inspiring character.

00:16:10.460 --> 00:16:15.530
And you can use him as a model
for things like diversity

00:16:15.530 --> 00:16:18.760
and sort of social
awareness and those kinds

00:16:18.760 --> 00:16:20.570
of things, which is very good.

00:16:20.570 --> 00:16:23.200
I also think he's very
inspiring in terms of,

00:16:23.200 --> 00:16:24.880
it's OK to be a geek.

00:16:24.880 --> 00:16:29.250
And let me say one of the
things I'm very passionate about

00:16:29.250 --> 00:16:33.510
is that we shouldn't
dumb stuff that you

00:16:33.510 --> 00:16:34.970
get taught in schools down.

00:16:34.970 --> 00:16:36.860
You shouldn't make it
too easy for people.

00:16:36.860 --> 00:16:38.620
You shouldn't give
people the answers.

00:16:38.620 --> 00:16:40.820
The idea that the answer
is in the back of the book

00:16:40.820 --> 00:16:43.319
is shocking enough, but the
idea that the answer is actually

00:16:43.319 --> 00:16:45.810
printed under the
question is deplorable.

00:16:45.810 --> 00:16:48.520
So the only way you
get to be a geek

00:16:48.520 --> 00:16:50.460
is actually to find
out for yourself

00:16:50.460 --> 00:16:52.780
and actually do the hard
work and teach yourself.

00:16:52.780 --> 00:16:55.302
And so I think he's a great
model for that as well.

00:16:55.302 --> 00:16:57.510
Sorry, I could go on for
another 45 minutes answering

00:16:57.510 --> 00:16:59.780
your questions, so I'm
going to stop there and take

00:16:59.780 --> 00:17:00.370
somebody else.

00:17:03.807 --> 00:17:06.270
AUDIENCE: Sorry [INAUDIBLE].

00:17:06.270 --> 00:17:08.998
Have you seen and do you have
any opinions on "The Imitation

00:17:08.998 --> 00:17:09.497
Game"?

00:17:09.497 --> 00:17:11.240
SIR DERMOT TURING: I have.

00:17:11.240 --> 00:17:12.471
Yes, all right.

00:17:12.471 --> 00:17:13.970
Has anybody in this
room-- I'm going

00:17:13.970 --> 00:17:15.678
to ask the question
the wrong way around.

00:17:15.678 --> 00:17:18.030
Has anyone in this room not
seen "The Imitation Game"?

00:17:18.030 --> 00:17:18.775
You're all fired.

00:17:22.034 --> 00:17:23.200
Go see "The Imitation Game."

00:17:23.200 --> 00:17:23.990
It's a great movie.

00:17:23.990 --> 00:17:24.740
Well, get the DVD.

00:17:24.740 --> 00:17:25.859
It's a great movie.

00:17:25.859 --> 00:17:28.030
Benedict Cumberbatch
is a brilliant actor.

00:17:28.030 --> 00:17:30.475
So you will enjoy it.

00:17:30.475 --> 00:17:31.320
It's a great movie.

00:17:31.320 --> 00:17:33.970
If you think you're going
to go watch a documentary,

00:17:33.970 --> 00:17:35.410
you will be disappointed.

00:17:35.410 --> 00:17:35.910
You're not.

00:17:35.910 --> 00:17:38.370
You're going to watch a
Hollywood movie and enjoy it.

00:17:38.370 --> 00:17:41.640
And so that's the spirit in
which you have to take it.

00:17:41.640 --> 00:17:46.530
There is a root of
truth in every scene.

00:17:46.530 --> 00:17:48.280
That does not mean
that any scene

00:17:48.280 --> 00:17:51.610
is a truthful representation
of what actually happened.

00:17:51.610 --> 00:17:52.400
But just enjoy it.

00:17:52.400 --> 00:17:54.490
It's a great movie.

00:17:54.490 --> 00:17:56.360
And it will tell
you stuff or make

00:17:56.360 --> 00:17:59.040
you think about stuff about Alan
Turing or about Bletchley Park

00:17:59.040 --> 00:18:01.206
that you hadn't really sort
of thought about before.

00:18:01.206 --> 00:18:02.760
So I encourage you
to go and see it.

00:18:02.760 --> 00:18:03.260
Enjoy.

00:18:11.560 --> 00:18:16.556
AUDIENCE: So basically the
perhaps two most famous things

00:18:16.556 --> 00:18:20.370
that [INAUDIBLE] Alan
Turing is the Turing

00:18:20.370 --> 00:18:21.536
machine and the Turing test.

00:18:21.536 --> 00:18:24.026
And what I always
have trouble with

00:18:24.026 --> 00:18:27.014
is somehow finding [INAUDIBLE].

00:18:27.014 --> 00:18:31.994
So I'm not sure
if we can believe

00:18:31.994 --> 00:18:36.476
that these were two completely
unrelated things that Turing

00:18:36.476 --> 00:18:40.460
worked on or if his
ultimate goal was actually

00:18:40.460 --> 00:18:43.946
for the Turing machine to be
some predecessor to something

00:18:43.946 --> 00:18:47.930
much bigger that would also
include artificial intelligence

00:18:47.930 --> 00:18:50.252
[INAUDIBLE] Turing test.

00:18:50.252 --> 00:18:52.210
SIR DERMOT TURING: OK,
well, these two concepts

00:18:52.210 --> 00:18:53.590
are 13 years apart.

00:18:53.590 --> 00:18:58.642
So it's not really
surprising that they are only

00:18:58.642 --> 00:18:59.420
loosely linked.

00:19:02.050 --> 00:19:05.130
So I don't think
you can sort of ever

00:19:05.130 --> 00:19:08.380
imagine Alan Turing went
straight from designing

00:19:08.380 --> 00:19:11.330
a very theoretical idea
of the universal machine

00:19:11.330 --> 00:19:14.220
and the Entscheidungsproblem
paper to his paper

00:19:14.220 --> 00:19:17.800
on the Turing test, the paper
that's published in mind.

00:19:17.800 --> 00:19:20.410
So it's not published in a-- it
was published in a psychology

00:19:20.410 --> 00:19:20.910
journal.

00:19:20.910 --> 00:19:23.270
It was not published in
a mathematical journal.

00:19:23.270 --> 00:19:27.870
So his best friend Robin Gandy
said that Alan Turing really

00:19:27.870 --> 00:19:30.110
enjoyed writing the paper
about the Turing test,

00:19:30.110 --> 00:19:32.220
because this was completely
different from any

00:19:32.220 --> 00:19:34.065
of his mathematical papers.

00:19:34.065 --> 00:19:37.410
There's no equations
in it, for starters.

00:19:37.410 --> 00:19:40.710
And it was all a bit
of a joke, really.

00:19:40.710 --> 00:19:43.980
And it all came out of this
whole business, the businessman

00:19:43.980 --> 00:19:46.870
writing sonnets
and the robot brain

00:19:46.870 --> 00:19:48.410
and all that kind of thing.

00:19:48.410 --> 00:19:50.140
There was this
whole debate going

00:19:50.140 --> 00:19:54.665
on in the late 1940s about
whether machines could think.

00:19:54.665 --> 00:19:56.290
And so that was really
his contribution

00:19:56.290 --> 00:19:59.380
to this was to say,
look, actually,

00:19:59.380 --> 00:20:02.502
let's get away from this
slightly daft question

00:20:02.502 --> 00:20:03.960
about whether
machines could think.

00:20:03.960 --> 00:20:08.056
Let's put the problem in
a slightly different way.

00:20:08.056 --> 00:20:09.430
Because actually,
it's a question

00:20:09.430 --> 00:20:14.250
about whether you think that
you're dealing with a human

00:20:14.250 --> 00:20:15.490
being or not.

00:20:15.490 --> 00:20:16.970
I think-- this is
very interesting.

00:20:16.970 --> 00:20:18.550
Last week, somebody
pointed out to me

00:20:18.550 --> 00:20:22.160
that when you go online
and the website you visit

00:20:22.160 --> 00:20:24.434
wants to check that
you're not an algorithm,

00:20:24.434 --> 00:20:26.850
it wants to check whether a
human being-- and it gives you

00:20:26.850 --> 00:20:29.750
a captcha and you have to
just type in whatever the four

00:20:29.750 --> 00:20:32.530
letters are that you
see on the screen.

00:20:32.530 --> 00:20:34.280
It's the Turing test
the other way around.

00:20:34.280 --> 00:20:37.170
It's the computer testing
whether you're a human.

00:20:37.170 --> 00:20:37.925
I like that.

00:20:41.565 --> 00:20:45.960
AUDIENCE: The name captcha
is an acronym [INAUDIBLE]

00:20:45.960 --> 00:20:47.101
Turing test.

00:20:47.101 --> 00:20:48.017
SIR DERMOT TURING: OK.

00:20:53.861 --> 00:20:55.910
AUDIENCE: Well, I will
ask the final question.

00:20:55.910 --> 00:20:57.395
SIR DERMOT TURING: Very good.

00:20:57.395 --> 00:20:59.942
AUDIENCE: What do you
think Alan would have hoped

00:20:59.942 --> 00:21:01.310
his legacy would have been?

00:21:01.310 --> 00:21:03.550
Would it have been the
Enigma or the [INAUDIBLE]?

00:21:03.550 --> 00:21:07.250
[INAUDIBLE] later
work on biology?

00:21:07.250 --> 00:21:09.970
SIR DERMOT TURING: It's
really difficult to know.

00:21:09.970 --> 00:21:15.430
My personal take on it-- and
that's no more valid than

00:21:15.430 --> 00:21:19.200
anybody else's-- is that I think
his work at Bletchley Park,

00:21:19.200 --> 00:21:24.560
although it was still a secret,
was I think highly valued

00:21:24.560 --> 00:21:27.260
by him and also
by his colleagues.

00:21:27.260 --> 00:21:31.860
And I think that's probably,
if the word "proud of"

00:21:31.860 --> 00:21:33.690
is the right
expression, I suspect

00:21:33.690 --> 00:21:35.710
that's what he would
have been proudest of.

00:21:35.710 --> 00:21:38.540
And actually, I think there's
a link there to the secrecy

00:21:38.540 --> 00:21:39.480
as well.

00:21:39.480 --> 00:21:43.470
He wasn't a man who wanted
to be in the spotlight

00:21:43.470 --> 00:21:44.960
nor wanted to be famous.

00:21:44.960 --> 00:21:50.090
So actually, the secret work was
probably fits the model best.

00:21:50.090 --> 00:21:53.140
Because nobody knew about it.

00:21:53.140 --> 00:21:57.320
All right, well, thank you
very much, and time for tea.

