WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.425
[MUSIC PLAYING]

00:00:05.677 --> 00:00:07.510
LAURENCE MORONEY: So
Josh, welcome to Coffee

00:00:07.510 --> 00:00:08.170
With A Googler.

00:00:08.170 --> 00:00:08.750
JOSH DILLON: Thank you.

00:00:08.750 --> 00:00:09.800
Thanks for having me.

00:00:09.800 --> 00:00:10.690
LAURENCE MORONEY: Now
you work on something

00:00:10.690 --> 00:00:12.610
called the Distributions
API, and that

00:00:12.610 --> 00:00:14.620
brings about
probabilistic programming.

00:00:14.620 --> 00:00:15.740
What's that all about?

00:00:15.740 --> 00:00:17.500
JOSH DILLON: Yeah,
so this is something

00:00:17.500 --> 00:00:20.360
that myself and my team
were really excited about.

00:00:20.360 --> 00:00:22.860
So I think a little
history is in order.

00:00:22.860 --> 00:00:27.730
So TensorFlow did a
number of amazing things.

00:00:27.730 --> 00:00:29.830
Maybe one of the
most amazing is how

00:00:29.830 --> 00:00:35.099
they made training deep neural
nets easy-- or relatively easy.

00:00:35.099 --> 00:00:37.390
LAURENCE MORONEY: It's a lot
easier than it used to be.

00:00:37.390 --> 00:00:38.560
JOSH DILLON: It's a lot
easier than it used to be.

00:00:38.560 --> 00:00:40.780
And this has resulted
in a flurry of research

00:00:40.780 --> 00:00:44.380
and exciting results
and solving problems

00:00:44.380 --> 00:00:46.810
that otherwise seemed
impossible, or at least,

00:00:46.810 --> 00:00:47.920
very difficult to solve.

00:00:47.920 --> 00:00:48.919
LAURENCE MORONEY: Right.

00:00:48.919 --> 00:00:50.650
JOSH DILLON: So
before neural nets

00:00:50.650 --> 00:00:53.510
were all the hotness, many
machine learning folks,

00:00:53.510 --> 00:00:55.900
including myself,
sort of consider

00:00:55.900 --> 00:00:58.880
ourselves probabilistic
machine learning as, like,

00:00:58.880 --> 00:01:00.220
a specialization.

00:01:00.220 --> 00:01:04.810
And we want to be a
part of the new hotness.

00:01:04.810 --> 00:01:06.060
LAURENCE MORONEY: Who doesn't?

00:01:06.060 --> 00:01:07.101
JOSH DILLON: Who doesn't?

00:01:07.101 --> 00:01:08.520
LAURENCE MORONEY:
I got the logo.

00:01:08.520 --> 00:01:11.019
JOSH DILLON: So that's where
Distributions enters the story.

00:01:11.019 --> 00:01:14.270
We think that
TensorFlow doesn't have

00:01:14.270 --> 00:01:17.360
to be just a language
for fitting neural nets.

00:01:17.360 --> 00:01:19.430
It can be a language
for statisticians.

00:01:19.430 --> 00:01:23.060
It can be a language for doing
probability first machine

00:01:23.060 --> 00:01:27.111
learning, for fusing neural
nets with sort of older ideas

00:01:27.111 --> 00:01:28.610
and getting something
that's greater

00:01:28.610 --> 00:01:30.000
than the sum of its parts.

00:01:30.000 --> 00:01:32.450
So Distributions is
a tiny Lego piece

00:01:32.450 --> 00:01:34.787
that tries to make that
just a little bit easier.

00:01:34.787 --> 00:01:36.620
LAURENCE MORONEY: Now
I noticed you recently

00:01:36.620 --> 00:01:38.900
published a paper on
the Distributions API

00:01:38.900 --> 00:01:40.770
explaining what it's all about.

00:01:40.770 --> 00:01:42.270
Could you give us
the brief summary?

00:01:42.270 --> 00:01:43.020
JOSH DILLON: Yeah.

00:01:43.020 --> 00:01:44.690
So we wrote just
a white paper sort

00:01:44.690 --> 00:01:46.970
of showing all of
the cool things that

00:01:46.970 --> 00:01:50.540
can be done when you mix
TensorFlow intrinsics

00:01:50.540 --> 00:01:53.330
and this Distributions API.

00:01:53.330 --> 00:01:55.640
It's actually pretty simple.

00:01:55.640 --> 00:01:59.990
But I think the real power comes
from the breadth of the API,

00:01:59.990 --> 00:02:01.700
in terms of the number
of Distributions

00:02:01.700 --> 00:02:05.720
that are included, as well
as the way they sort of plug

00:02:05.720 --> 00:02:07.822
and play with
TensorFlow workflows.

00:02:07.822 --> 00:02:08.780
LAURENCE MORONEY: Cool.

00:02:08.780 --> 00:02:09.350
JOSH DILLON: Let's call it.

00:02:09.350 --> 00:02:10.759
LAURENCE MORONEY: Now
I'll be honest with you.

00:02:10.759 --> 00:02:12.592
When I first started
seeing something called

00:02:12.592 --> 00:02:15.080
a Distributions API, I was
like, hey cool, something

00:02:15.080 --> 00:02:16.830
I can use to
distribute my models.

00:02:16.830 --> 00:02:17.555
But--

00:02:17.555 --> 00:02:20.106
JOSH DILLON: Yeah, maybe
we should have renamed it.

00:02:20.106 --> 00:02:21.980
LAURENCE MORONEY: Shall
we start refactoring?

00:02:21.980 --> 00:02:25.540
JOSH DILLON: If you want
to refactor, by all means.

00:02:25.540 --> 00:02:27.720
LAURENCE MORONEY: It's
going to be a bit of work.

00:02:27.720 --> 00:02:29.650
But it's not about
distributing apps, right?

00:02:29.650 --> 00:02:31.733
JOSH DILLON: It's not about
distributing apps, no.

00:02:31.733 --> 00:02:36.980
It's about encoding
properties of random variables

00:02:36.980 --> 00:02:39.110
in an easy to use framework.

00:02:39.110 --> 00:02:44.310
So a random variable is a
way to represent uncertainty,

00:02:44.310 --> 00:02:45.110
let's say.

00:02:45.110 --> 00:02:47.690
And there are several useful
properties that it has.

00:02:47.690 --> 00:02:50.420
You may ask, what's the average
under this random variable?

00:02:50.420 --> 00:02:53.010
You may ask, what's the variance
under this random variable?

00:02:53.010 --> 00:02:55.580
So distribution just
collects these properties.

00:02:55.580 --> 00:02:57.180
That's not super exciting.

00:02:57.180 --> 00:02:58.790
I think what's
slightly more exciting

00:02:58.790 --> 00:03:01.760
is the way we organize the
library automatically takes

00:03:01.760 --> 00:03:03.525
advantage of modern hardware.

00:03:03.525 --> 00:03:04.400
LAURENCE MORONEY: OK.

00:03:04.400 --> 00:03:05.774
JOSH DILLON: So
we have something

00:03:05.774 --> 00:03:08.810
we call "batch semantics,"
which is a TensorFlow idea.

00:03:08.810 --> 00:03:11.630
And it means that
you can parameterize

00:03:11.630 --> 00:03:15.260
one distribution in
many different ways

00:03:15.260 --> 00:03:18.431
and take advantage
of SIMD instructions.

00:03:18.431 --> 00:03:19.430
LAURENCE MORONEY: Right.

00:03:19.430 --> 00:03:21.710
So I noticed in your paper,
you spoke a little bit

00:03:21.710 --> 00:03:22.700
about Distributions.

00:03:22.700 --> 00:03:25.150
But you also mentioned these
things called "bijectors."

00:03:25.150 --> 00:03:25.550
JOSH DILLON: Right.

00:03:25.550 --> 00:03:26.630
LAURENCE MORONEY:
That's a fabulous name,

00:03:26.630 --> 00:03:28.400
but what exactly is a bijector?

00:03:28.400 --> 00:03:30.740
JOSH DILLON: So we actually
named it a bijector so,

00:03:30.740 --> 00:03:32.930
that way, if you
Google searched for it,

00:03:32.930 --> 00:03:35.030
you would only find that word.

00:03:35.030 --> 00:03:37.350
A bijector is a
very simple idea.

00:03:37.350 --> 00:03:40.880
It is a function of a tensor.

00:03:40.880 --> 00:03:42.800
And where this fits
into the Distribution

00:03:42.800 --> 00:03:46.010
API is, presumably,
that tensor is itself

00:03:46.010 --> 00:03:47.690
a sample from some distribution.

00:03:47.690 --> 00:03:50.930
So a bijector is a way of
transforming one distribution

00:03:50.930 --> 00:03:52.065
into another distribution.

00:03:52.065 --> 00:03:52.940
LAURENCE MORONEY: OK.

00:03:52.940 --> 00:03:54.710
JOSH DILLON: That's
useful, because if we

00:03:54.710 --> 00:03:57.980
didn't have that concept, we
have like 60 distributions now.

00:03:57.980 --> 00:03:59.690
We may have 600
distributions if we

00:03:59.690 --> 00:04:01.580
tried to implement
all transformations

00:04:01.580 --> 00:04:03.119
of all distributions.

00:04:03.119 --> 00:04:04.910
There's something called
a lognormal, which

00:04:04.910 --> 00:04:08.390
is E to the normal
distribution, let's say.

00:04:08.390 --> 00:04:12.110
And we don't actually implement
that as a first class citizen.

00:04:12.110 --> 00:04:16.220
To achieve that, you
would transform a Gaussian

00:04:16.220 --> 00:04:19.029
distribution with the
exponential bijector.

00:04:19.029 --> 00:04:20.540
And now you have lognormal.

00:04:20.540 --> 00:04:20.990
LAURENCE MORONEY: I see.

00:04:20.990 --> 00:04:23.200
So it's a way that you can
combine and chop and change

00:04:23.200 --> 00:04:23.570
distributions.

00:04:23.570 --> 00:04:24.080
JOSH DILLON: Exactly.

00:04:24.080 --> 00:04:24.700
Yeah.

00:04:24.700 --> 00:04:25.340
Exactly.

00:04:25.340 --> 00:04:28.440
The idea is, distributions
are sources of stochasticity

00:04:28.440 --> 00:04:32.000
or randomness, and bijectors are
deterministic transformations

00:04:32.000 --> 00:04:33.490
of those sources of randomness.

00:04:33.490 --> 00:04:35.630
So there is a kind
of dichotomy there.

00:04:35.630 --> 00:04:36.880
LAURENCE MORONEY: Interesting.

00:04:36.880 --> 00:04:40.190
So just taking a step back
for a second and just think,

00:04:40.190 --> 00:04:42.980
why is probabilistic
programming, in your view--

00:04:42.980 --> 00:04:45.074
why is it necessary,
and what does it

00:04:45.074 --> 00:04:46.490
bring to the table
for developers?

00:04:46.490 --> 00:04:48.406
JOSH DILLON: Yeah that's
an exciting question.

00:04:48.406 --> 00:04:51.110
So I like to think of
probabilistic programming

00:04:51.110 --> 00:04:56.060
as being a formalism for
reasoning under uncertainty.

00:04:56.060 --> 00:04:57.020
LAURENCE MORONEY: OK.

00:04:57.020 --> 00:04:57.860
JOSH DILLON: I like
that definition,

00:04:57.860 --> 00:04:59.818
because I can't think of
something that doesn't

00:04:59.818 --> 00:05:01.520
fit into that definition.

00:05:01.520 --> 00:05:02.480
LAURENCE MORONEY: Those
are the best definitions.

00:05:02.480 --> 00:05:04.313
JOSH DILLON: Those are
the best definitions.

00:05:04.313 --> 00:05:07.490
Science is you
make a hypothesis,

00:05:07.490 --> 00:05:10.730
you verify that hypothesis
with some empirical study.

00:05:10.730 --> 00:05:13.220
But we might imagine
that the third time

00:05:13.220 --> 00:05:15.820
you ran your experiment, you
got a totally different result.

00:05:15.820 --> 00:05:17.730
It doesn't mean your
hypothesis is wrong.

00:05:17.730 --> 00:05:21.240
It just means that, guess
what, nature has randomness.

00:05:21.240 --> 00:05:27.620
So having a way to make it
easy to specify uncertainty

00:05:27.620 --> 00:05:30.680
and to perform inference
and reasoning on it

00:05:30.680 --> 00:05:34.370
means that you can sort of tease
out the signal from the noise.

00:05:34.370 --> 00:05:38.180
And doing so in a formal
way keeps you honest.

00:05:38.180 --> 00:05:42.350
It means that you can't claim
"I have a cure for cancer,"

00:05:42.350 --> 00:05:45.389
when in fact, it works
only one out of a million.

00:05:45.389 --> 00:05:46.430
LAURENCE MORONEY: Got it.

00:05:46.430 --> 00:05:47.090
Got it.

00:05:47.090 --> 00:05:48.965
And also for safety
reasons, right, if you're

00:05:48.965 --> 00:05:50.220
doing something like--

00:05:50.220 --> 00:05:50.570
JOSH DILLON: Absolutely.

00:05:50.570 --> 00:05:51.390
LAURENCE MORONEY:
This is a green light.

00:05:51.390 --> 00:05:53.090
JOSH DILLON: Absolutely.

00:05:53.090 --> 00:05:54.770
Suppose you have
a machine learning

00:05:54.770 --> 00:05:57.980
algorithm trying to detect
if it's a green light.

00:05:57.980 --> 00:06:01.100
If you were only 2% sure
that it's a green light,

00:06:01.100 --> 00:06:02.395
you probably should stop.

00:06:02.395 --> 00:06:04.020
LAURENCE MORONEY:
Exactly, or don't go.

00:06:04.020 --> 00:06:05.190
JOSH DILLON: Don't go, yeah.

00:06:05.190 --> 00:06:06.189
LAURENCE MORONEY: Right.

00:06:06.189 --> 00:06:08.260
But then if it's
97% or if it's 98%--

00:06:08.260 --> 00:06:08.955
JOSH DILLON: So
that's the question.

00:06:08.955 --> 00:06:10.260
LAURENCE MORONEY:
--that's when you start.

00:06:10.260 --> 00:06:11.240
JOSH DILLON: The nice
thing about having

00:06:11.240 --> 00:06:13.980
some notion of uncertainty
in your predictive system

00:06:13.980 --> 00:06:18.260
is you can weigh the opportunity
cost of action or inaction.

00:06:18.260 --> 00:06:20.510
LAURENCE MORONEY: And it
might seem like a lot of this

00:06:20.510 --> 00:06:22.430
is very theoretical
and very mathematical,

00:06:22.430 --> 00:06:24.010
but it's also very
concrete, right.

00:06:24.010 --> 00:06:25.968
As soon as you build your
first neural network,

00:06:25.968 --> 00:06:27.800
as soon as you build
a neuron, that's

00:06:27.800 --> 00:06:29.360
entirely based on probability.

00:06:29.360 --> 00:06:30.985
JOSH DILLON: Yeah,
it's interesting you

00:06:30.985 --> 00:06:33.530
say it's theoretical, because
machine learning likes to boil

00:06:33.530 --> 00:06:35.960
things down to accuracy.

00:06:35.960 --> 00:06:39.530
I have some algorithm, and
it achieves 97% accuracy

00:06:39.530 --> 00:06:41.900
on detecting handwritten digits.

00:06:41.900 --> 00:06:44.000
That's not how the real
world actually works.

00:06:44.000 --> 00:06:46.220
The whole point of having
a machine-learned model

00:06:46.220 --> 00:06:48.170
is you're going to
make some choice.

00:06:48.170 --> 00:06:51.470
And so I think it's immensely
practical to have a system that

00:06:51.470 --> 00:06:54.110
tells you, look, this is the
choice I think you should make,

00:06:54.110 --> 00:06:55.930
but this is how much
you should trust me.

00:06:55.930 --> 00:06:58.690
That seems like actually
not theoretical.

00:06:58.690 --> 00:07:02.720
That seems like what we all
really want or maybe even need.

00:07:02.720 --> 00:07:05.450
And so if you take a
probabilistic first viewpoint,

00:07:05.450 --> 00:07:08.350
you can start to design systems
that have this capability.

00:07:08.350 --> 00:07:09.230
LAURENCE MORONEY:
Right, and to be

00:07:09.230 --> 00:07:10.580
able to get that
probability, you

00:07:10.580 --> 00:07:12.121
need like things
like loss functions,

00:07:12.121 --> 00:07:13.970
and you plug a
distribution into that.

00:07:13.970 --> 00:07:15.950
So by having tons
of distributions,

00:07:15.950 --> 00:07:17.810
it gives you the ability
to chop and change

00:07:17.810 --> 00:07:18.980
to see what works best for you.

00:07:18.980 --> 00:07:21.355
JOSH DILLON: And not just tons
of distributions, but ways

00:07:21.355 --> 00:07:24.479
to change them while preserving
the computational properties.

00:07:24.479 --> 00:07:26.270
LAURENCE MORONEY: And
that's the bijectors.

00:07:26.270 --> 00:07:27.800
JOSH DILLON: And that's
the bijector, exactly.

00:07:27.800 --> 00:07:28.758
LAURENCE MORONEY: Cool.

00:07:28.758 --> 00:07:31.010
So say I'm getting started
in machine learning,

00:07:31.010 --> 00:07:33.740
and even getting started
in machine learning

00:07:33.740 --> 00:07:34.880
is a hard enough thing.

00:07:34.880 --> 00:07:36.710
But what advice
would you give me

00:07:36.710 --> 00:07:39.272
to be able to optimize
what I'm doing?

00:07:39.272 --> 00:07:41.730
Any good pointers that you
would have from your background?

00:07:41.730 --> 00:07:46.970
JOSH DILLON: So I do not have
neural network as a background.

00:07:46.970 --> 00:07:49.460
But I like to think
of neural nets--

00:07:49.460 --> 00:07:52.400
although this may be
heretical-- as very fancy forms

00:07:52.400 --> 00:07:53.630
of regression.

00:07:53.630 --> 00:07:56.680
Regression is a relatively old--

00:07:56.680 --> 00:07:58.880
actually, an old
idea in statistics,

00:07:58.880 --> 00:08:00.920
and relatively old
in machine learning,

00:08:00.920 --> 00:08:05.762
in which you try to predict a
response, given some evidence.

00:08:05.762 --> 00:08:07.220
That's what a neural
net does, too.

00:08:07.220 --> 00:08:09.860
So I think to get started,
we have this thing called

00:08:09.860 --> 00:08:12.980
trainable distributions, which
basically take a tensor that

00:08:12.980 --> 00:08:15.980
is your evidence, and it
gives you back a distribution

00:08:15.980 --> 00:08:19.550
from which you could plug in,
say, minus distribution dot log

00:08:19.550 --> 00:08:23.710
prob of your labels as a
loss into an optimizer.

00:08:23.710 --> 00:08:29.570
So the examples we have there
show just how easy it is

00:08:29.570 --> 00:08:32.000
to do regression in TensorFlow.

00:08:32.000 --> 00:08:34.970
And the nice part about
that is, as you add more

00:08:34.970 --> 00:08:38.120
sophistication-- different
layers, in TensorFlow parlance,

00:08:38.120 --> 00:08:40.549
with different activations,
nonlinear functions

00:08:40.549 --> 00:08:42.030
of the output of those layers--

00:08:42.030 --> 00:08:44.120
now you can get state of
the art machine learning

00:08:44.120 --> 00:08:44.960
sophistication.

00:08:44.960 --> 00:08:46.220
LAURENCE MORONEY: Cool, cool.

00:08:46.220 --> 00:08:48.270
And obviously, if I'm
going beyond beginning,

00:08:48.270 --> 00:08:50.570
and I want to start getting
into Distributions what--

00:08:50.570 --> 00:08:52.900
I mean, I guess read your
paper would be good advice.

00:08:52.900 --> 00:08:54.483
JOSH DILLON: Yeah,
so, the white paper

00:08:54.483 --> 00:08:58.550
has a lot of examples that show
just the power of this really

00:08:58.550 --> 00:09:00.220
simple idea.

00:09:00.220 --> 00:09:02.120
And those are my
favorite kinds of ideas,

00:09:02.120 --> 00:09:03.992
because I actually get them.

00:09:03.992 --> 00:09:06.200
LAURENCE MORONEY: That's
always the best kind, right?

00:09:06.200 --> 00:09:07.100
JOSH DILLON: That's
the best kind of idea.

00:09:07.100 --> 00:09:08.090
LAURENCE MORONEY: No
point having an idea

00:09:08.090 --> 00:09:09.090
that nobody understands.

00:09:09.090 --> 00:09:10.530
JOSH DILLON: Especially me.

00:09:10.530 --> 00:09:12.530
LAURENCE MORONEY: Well,
thank you so much, Josh.

00:09:12.530 --> 00:09:15.170
This has been so much fun, and
it's really, really digging

00:09:15.170 --> 00:09:17.750
down into stuff that I formerly
really didn't understand.

00:09:17.750 --> 00:09:19.550
But having read your
paper and having

00:09:19.550 --> 00:09:20.570
played with some
of this stuff, I

00:09:20.570 --> 00:09:22.010
can really begin to see
where this is going.

00:09:22.010 --> 00:09:22.970
JOSH DILLON: I'm
happy to hear that.

00:09:22.970 --> 00:09:23.990
LAURENCE MORONEY: So
thanks very much, Josh.

00:09:23.990 --> 00:09:24.870
That was fascinating.

00:09:24.870 --> 00:09:27.175
And thank you, too, for
watching this episode of Coffee

00:09:27.175 --> 00:09:27.800
With A Googler.

00:09:27.800 --> 00:09:28.900
If you have any
questions for me,

00:09:28.900 --> 00:09:30.500
or if you have any
questions for Josh,

00:09:30.500 --> 00:09:32.260
please leave them in
the comments below.

00:09:32.260 --> 00:09:34.190
And don't forget to hit
that subscribe button.

00:09:34.190 --> 00:09:36.290
Oh, and by the way, be
sure to check out our brand

00:09:36.290 --> 00:09:38.150
new TensorFlow
channel on YouTube.

00:09:38.150 --> 00:09:39.880
Thank you.

