WEBVTT
Kind: captions
Language: en

00:00:00.110 --> 00:00:01.830
FEMALE SPEAKER:
We are absolutely

00:00:01.830 --> 00:00:04.820
delighted to have Walter
Isaacson here with us today

00:00:04.820 --> 00:00:07.900
to talk about his latest
book, "The Innovators:

00:00:07.900 --> 00:00:11.020
How a Group of Hackers,
Geniuses, and Geeks

00:00:11.020 --> 00:00:12.852
Created the Digital Revolution."

00:00:12.852 --> 00:00:14.310
I'm going to turn
it over to Walter

00:00:14.310 --> 00:00:17.050
to say a few things
about his book.

00:00:17.050 --> 00:00:18.580
But we have two
microphones and we'd

00:00:18.580 --> 00:00:21.047
love for folks to really
interact with Walter.

00:00:21.047 --> 00:00:23.380
WALTER ISAACSON: I hope we
can make it more interactive,

00:00:23.380 --> 00:00:25.910
but I will say a few
words, one of which

00:00:25.910 --> 00:00:27.880
is you're very lucky
to be at Google.

00:00:27.880 --> 00:00:30.030
The book ends with
Google but in some ways,

00:00:30.030 --> 00:00:33.410
the whole revolution begins
with Google, as well.

00:00:33.410 --> 00:00:36.730
Because Vint Cerf is one of
the great heroes in this book.

00:00:36.730 --> 00:00:41.240
In fact, I did a Google
meeting in Washington last week

00:00:41.240 --> 00:00:44.920
with Vint and Bob Kahn about
the creation of the Internet

00:00:44.920 --> 00:00:46.140
Protocols.

00:00:46.140 --> 00:00:49.000
And one of the things that
has sort of surprised me

00:00:49.000 --> 00:00:52.430
about the digital
revolution is most of us

00:00:52.430 --> 00:00:55.230
have some sense of the
history of many revolutions.

00:00:55.230 --> 00:00:58.880
I mean, we know of George
Washington and Ben Franklin

00:00:58.880 --> 00:01:01.730
in the American Revolution,
or the scientific revolution--

00:01:01.730 --> 00:01:05.910
the role Galileo plays-- or
the Industrial Revolution.

00:01:05.910 --> 00:01:09.120
But the history of
the digital revolution

00:01:09.120 --> 00:01:10.730
hasn't really been written.

00:01:10.730 --> 00:01:13.450
And it's got all these
unbelievably fascinating

00:01:13.450 --> 00:01:14.740
characters in it.

00:01:14.740 --> 00:01:18.730
Most of them you would have
heard of, but people you know

00:01:18.730 --> 00:01:21.420
wouldn't have heard of-- people
like Doug Engelbart, Alan

00:01:21.420 --> 00:01:24.390
Kay, Vint Cerf.

00:01:24.390 --> 00:01:29.530
And each one of them builds on
the creativity of the others.

00:01:29.530 --> 00:01:31.720
And they create teams
to do creativity.

00:01:31.720 --> 00:01:35.590
There's a collaborative
DNA infused

00:01:35.590 --> 00:01:38.140
in the digital
revolution, partly

00:01:38.140 --> 00:01:40.250
because the tools of
the digital revolution

00:01:40.250 --> 00:01:42.570
were invented collaboratively--
like the ARPANET

00:01:42.570 --> 00:01:46.120
and the Internet Protocols
as well as the computer--

00:01:46.120 --> 00:01:50.820
and partly because the digital
revolution is about networking

00:01:50.820 --> 00:01:53.620
and connectivity,
and thus it allows

00:01:53.620 --> 00:01:55.780
for a collaborative process.

00:01:55.780 --> 00:01:58.000
In fact, when I was
doing this book,

00:01:58.000 --> 00:02:02.210
I was writing about how
the genetic code of the DNA

00:02:02.210 --> 00:02:05.920
has infused into it this
collaborative nature, partly

00:02:05.920 --> 00:02:08.960
because it was originally
created as an ARPANET

00:02:08.960 --> 00:02:14.410
to allow time-sharing
of computers,

00:02:14.410 --> 00:02:19.190
as well as sharing of research
and collaborating on research.

00:02:19.190 --> 00:02:21.190
And I said, well, you
know, it's strange

00:02:21.190 --> 00:02:25.090
that since Gutenberg invented
the movable-type printing

00:02:25.090 --> 00:02:29.790
press, books have been sort
of handed down by one author.

00:02:29.790 --> 00:02:31.800
Let me see if I can
use the internet even

00:02:31.800 --> 00:02:33.680
in doing this book.

00:02:33.680 --> 00:02:37.990
So I took chapters of this
book and posted it a year ago,

00:02:37.990 --> 00:02:40.550
year and a half ago,
on places like Medium.

00:02:40.550 --> 00:02:42.260
I'd been working
with Ev Williams,

00:02:42.260 --> 00:02:45.240
because before Twitter, he
had invented Blogger, which

00:02:45.240 --> 00:02:48.540
was an important step in
taking the World Wide Web away

00:02:48.540 --> 00:02:51.090
from being a publishing medium
and being into a community

00:02:51.090 --> 00:02:52.220
medium.

00:02:52.220 --> 00:02:54.700
And he had invented Medium.

00:02:54.700 --> 00:02:58.820
And I put it up and put it
on various wikified sites,

00:02:58.820 --> 00:03:00.860
so people can make notes on it.

00:03:00.860 --> 00:03:04.190
Now we've done
that for centuries.

00:03:04.190 --> 00:03:07.000
That's why Ben Franklin
invented the postal service,

00:03:07.000 --> 00:03:10.550
so that his scientific and
philosophical papers could

00:03:10.550 --> 00:03:13.640
be circulated and people
could comment on them.

00:03:13.640 --> 00:03:16.460
And whenever I wrote a previous
book, or wrote for "Time,"

00:03:16.460 --> 00:03:18.010
you'd send your
article or book out

00:03:18.010 --> 00:03:19.843
for what we called
comments and corrections.

00:03:19.843 --> 00:03:22.440
And you'd send it
to 20, 40 people.

00:03:22.440 --> 00:03:25.710
The first day I put the chapter
on the personal computer

00:03:25.710 --> 00:03:31.630
and the California
cauldron of tribes

00:03:31.630 --> 00:03:34.930
in the early '70s in which the
personal computer was born,

00:03:34.930 --> 00:03:39.240
I got 7,800 comments--
which is obviously much more

00:03:39.240 --> 00:03:41.710
than I'd ever gotten
before by circulating

00:03:41.710 --> 00:03:46.540
my papers to my friends--
including people who were there

00:03:46.540 --> 00:03:49.080
at the revolution.

00:03:49.080 --> 00:03:51.420
I had talked about the
"Whole Earth Catalog,"

00:03:51.420 --> 00:03:53.920
and the Demise Party for
the "Whole Earth Catalog,"

00:03:53.920 --> 00:03:57.090
and how that money from
that funded these community

00:03:57.090 --> 00:04:00.350
computer centers,
and how that was

00:04:00.350 --> 00:04:03.870
part of this cultural
stew that helped

00:04:03.870 --> 00:04:06.902
create the personal computer,
taking back access to tools.

00:04:06.902 --> 00:04:08.860
All of the sudden there's
Stewart Brand saying,

00:04:08.860 --> 00:04:11.040
well you know, you
got it kind of wrong.

00:04:11.040 --> 00:04:15.130
We were dropping acid at the
Whole Earth Demise Party.

00:04:15.130 --> 00:04:18.480
And so we had this rock
band and they kept playing.

00:04:18.480 --> 00:04:23.150
But in between, we had
this stack of $100 bills

00:04:23.150 --> 00:04:25.830
and we were trying to figure
out how to divvy it up.

00:04:25.830 --> 00:04:29.230
And you know, he gave
me the color on that.

00:04:29.230 --> 00:04:31.460
Liza Loop, who I'd
never heard of,

00:04:31.460 --> 00:04:32.840
said wait, do you
not understand.

00:04:32.840 --> 00:04:34.840
It wasn't just hippies
like Stewart Brand.

00:04:34.840 --> 00:04:37.080
There were those of us who
were community organizers.

00:04:37.080 --> 00:04:39.310
We were creating the
Community Memory Project.

00:04:39.310 --> 00:04:45.390
That's how online community
bulletin boards really started.

00:04:45.390 --> 00:04:46.600
So now she's in the book.

00:04:46.600 --> 00:04:48.470
And now I put all this stuff in.

00:04:48.470 --> 00:04:52.995
I even put in a whole
section on Dan Bricklin.

00:04:52.995 --> 00:04:55.370
You know, one of the things
that's a problem in this book

00:04:55.370 --> 00:04:57.600
is there's so much
you've got to leave out.

00:04:57.600 --> 00:05:00.810
But I realized that the
creation of application software

00:05:00.810 --> 00:05:02.560
was something I hadn't
delved into enough.

00:05:02.560 --> 00:05:04.010
And Bricklin had done VisiCalc.

00:05:04.010 --> 00:05:05.510
And all of the
sudden he's giving me

00:05:05.510 --> 00:05:08.180
all of his papers, when he was
sitting at the Harvard Business

00:05:08.180 --> 00:05:11.000
School trying to visualize
how he'd do VisiCalc.

00:05:11.000 --> 00:05:12.200
And so that section's in.

00:05:12.200 --> 00:05:15.880
So the notion of
collaboratively creating a book

00:05:15.880 --> 00:05:18.550
seemed to come
naturally in a book

00:05:18.550 --> 00:05:20.370
about the digital revolution.

00:05:20.370 --> 00:05:24.720
And I hope someday-- I mean,
Google Docs is a first step.

00:05:24.720 --> 00:05:26.930
Medium is a first step.

00:05:26.930 --> 00:05:28.960
Wikis are a step.

00:05:28.960 --> 00:05:32.860
I hope we will have ways
and tools in which narrative

00:05:32.860 --> 00:05:36.690
history can be curated
by somebody like myself

00:05:36.690 --> 00:05:40.840
but not written by me, where
every one of you who invents

00:05:40.840 --> 00:05:43.900
some particular
product at Google

00:05:43.900 --> 00:05:45.270
can say, OK, I was in on this.

00:05:45.270 --> 00:05:49.090
Let me upload my oral history,
my documents, my sketches,

00:05:49.090 --> 00:05:50.400
whatever it may be.

00:05:50.400 --> 00:05:53.890
And we'll have living history
books that are collaboratively

00:05:53.890 --> 00:05:55.160
produced.

00:05:55.160 --> 00:05:57.430
But mainly, that
type of projection

00:05:57.430 --> 00:06:00.540
into the future-- and it
comes from Alan Kay and Xerox

00:06:00.540 --> 00:06:03.070
PARC, who said, the best
way to predict the future

00:06:03.070 --> 00:06:04.440
is to invent it.

00:06:04.440 --> 00:06:08.320
But the best way to
invent the future

00:06:08.320 --> 00:06:10.680
is to realize that each
new invention is built

00:06:10.680 --> 00:06:14.130
upon the trajectory
from whence we came.

00:06:14.130 --> 00:06:18.600
And I really wanted to do that
for the digital revolution.

00:06:18.600 --> 00:06:21.000
It starts-- my book,
at least-- starts

00:06:21.000 --> 00:06:23.910
with Ada Lovelace, who
comes up with the concept

00:06:23.910 --> 00:06:26.060
of the computer algorithm,
but also the concept

00:06:26.060 --> 00:06:28.840
of the general interest
computer, and more importantly,

00:06:28.840 --> 00:06:32.355
the concept of connecting
the humanities to technology.

00:06:35.190 --> 00:06:37.944
She's Lord Byron's
daughter, a poet.

00:06:37.944 --> 00:06:39.610
But her mother, who
doesn't particularly

00:06:39.610 --> 00:06:42.427
like Lord Byron when
Ada was growing up,

00:06:42.427 --> 00:06:44.760
wants to make sure she doesn't
become a poet, so has her

00:06:44.760 --> 00:06:49.210
tutored in mathematics, mainly--
as if being tutored in math

00:06:49.210 --> 00:06:51.800
were an antidote to
becoming poetical.

00:06:51.800 --> 00:06:53.900
So she develops what she
calls poetical science.

00:06:53.900 --> 00:06:57.670
She combines the two, and looks
at how punch cards are being

00:06:57.670 --> 00:07:02.370
used in the looms of industrial
England to create tapestries.

00:07:02.370 --> 00:07:05.197
And her father was a Luddite.

00:07:05.197 --> 00:07:06.280
And I mean that literally.

00:07:06.280 --> 00:07:09.410
Lord Byron's only speech
in the House of Lords

00:07:09.410 --> 00:07:14.600
was defending the followers
of Ned Ludd, the person who

00:07:14.600 --> 00:07:18.250
was smashing the mechanical
looms in England, because he

00:07:18.250 --> 00:07:21.420
thought they were putting
people out of work.

00:07:21.420 --> 00:07:23.520
But Ada loved the
looms, and the beauty

00:07:23.520 --> 00:07:29.370
of connecting the technology
and humanities together.

00:07:29.370 --> 00:07:32.170
And so she looked at her
friend Charles Babbage's

00:07:32.170 --> 00:07:35.810
analytical engine, which was a
numbers calculator, basically

00:07:35.810 --> 00:07:37.440
to do differential equations.

00:07:37.440 --> 00:07:39.950
And she said with
punch cards, it

00:07:39.950 --> 00:07:41.960
can do not just
numbers, but anything

00:07:41.960 --> 00:07:45.560
that can be notated in symbols.

00:07:45.560 --> 00:07:48.850
Meaning, as she put
it, it can do art.

00:07:48.850 --> 00:07:50.070
It can do patterns.

00:07:50.070 --> 00:07:51.220
It can do words.

00:07:51.220 --> 00:07:53.730
It could even make musical
composition, something

00:07:53.730 --> 00:07:58.130
that would've made
Lord Byron blanch.

00:07:58.130 --> 00:08:00.350
And so that notion of a
general interest computer

00:08:00.350 --> 00:08:01.850
comes from her, but
also that notion

00:08:01.850 --> 00:08:05.970
of connecting the arts and
humanities side of our brain

00:08:05.970 --> 00:08:08.870
with the engineering and
tech side of our brain.

00:08:08.870 --> 00:08:12.410
And to me, that's a thing
you see all the way through.

00:08:12.410 --> 00:08:15.600
And I'll leap forward to
how it ends with Google,

00:08:15.600 --> 00:08:18.830
and then we can open
up for discussion.

00:08:18.830 --> 00:08:22.040
I really see that there
are two branches of thought

00:08:22.040 --> 00:08:24.150
in the digital revolution.

00:08:24.150 --> 00:08:27.830
There's what I would call the
Ada Lovelace strand, which

00:08:27.830 --> 00:08:31.100
is the connection of
humans to their technology

00:08:31.100 --> 00:08:35.549
more intimately-- connecting
humanities and technology.

00:08:35.549 --> 00:08:39.090
When she said, a machine will
be able to do everything,

00:08:39.090 --> 00:08:40.750
she then adds the caveat.

00:08:40.750 --> 00:08:43.850
She says, except, it will
never be able to think.

00:08:43.850 --> 00:08:47.350
It will not originate
thought or have imagination.

00:08:47.350 --> 00:08:49.210
Only humans will do that.

00:08:49.210 --> 00:08:51.680
And our machines will
amplify our imaginations

00:08:51.680 --> 00:08:53.000
and our creativity.

00:08:53.000 --> 00:08:57.000
And as partners in symbiosis
technology and humans

00:08:57.000 --> 00:08:58.980
will work together.

00:08:58.980 --> 00:09:02.450
Exactly 100 years after that,
Alan Turing comes along.

00:09:02.450 --> 00:09:03.950
You'll see the movie
in a few weeks,

00:09:03.950 --> 00:09:07.480
if you haven't seen it already,
called "The Imitation Game."

00:09:07.480 --> 00:09:11.700
He's at Bletchley Park breaking
the German wartime codes.

00:09:11.700 --> 00:09:14.410
But he's also, as he's
using this machine

00:09:14.410 --> 00:09:18.650
to break human language
codes, thinking

00:09:18.650 --> 00:09:21.290
about the difference between
artificial intelligence

00:09:21.290 --> 00:09:24.570
and human intelligence.

00:09:24.570 --> 00:09:29.620
So he, like a good
historian, building

00:09:29.620 --> 00:09:33.040
on the inventions of others,
knows of Ada Lovelace.

00:09:33.040 --> 00:09:36.910
He's read Ada's notes on
Babbage's analytical engine.

00:09:36.910 --> 00:09:43.290
And so he coins a phrase,
"Lady Lovelace's objection,"

00:09:43.290 --> 00:09:49.100
to describe Ada Lovelace's view
that machines will never think.

00:09:49.100 --> 00:09:50.870
And he says, how
do we know that?

00:09:50.870 --> 00:09:53.740
How would we know a
machine isn't thinking?

00:09:53.740 --> 00:09:56.320
And he comes up with the
Imitation Game-- which we now

00:09:56.320 --> 00:09:58.010
call the Turing
Test-- which is, you

00:09:58.010 --> 00:10:00.110
put a machine in a room
and a human in a room,

00:10:00.110 --> 00:10:01.632
and you send in questions.

00:10:01.632 --> 00:10:03.090
And if after a
while you can't tell

00:10:03.090 --> 00:10:05.131
the difference between
the machine and the human,

00:10:05.131 --> 00:10:08.230
then it makes no
logical, sensible reason

00:10:08.230 --> 00:10:10.130
to say the machine's
not thinking.

00:10:10.130 --> 00:10:14.180
You have no empirical data to
say the machine's not thinking.

00:10:14.180 --> 00:10:16.190
Those of you who
did philosophy can

00:10:16.190 --> 00:10:18.347
try to knock down that
argument about consciousness

00:10:18.347 --> 00:10:19.180
and everything else.

00:10:19.180 --> 00:10:23.080
But leaving that aside,
that's the other strand

00:10:23.080 --> 00:10:25.519
of the digital
revolution, which is

00:10:25.519 --> 00:10:27.060
those who believe
in machine learning

00:10:27.060 --> 00:10:28.670
and artificial intelligence.

00:10:28.670 --> 00:10:32.880
In your company, amongst your
three or four top people, Larry

00:10:32.880 --> 00:10:34.790
and Sergey and
Eric, there's still

00:10:34.790 --> 00:10:37.792
that debate on how fast we
are doing machine learning.

00:10:37.792 --> 00:10:40.000
How are we going to create
robotics and machines that

00:10:40.000 --> 00:10:41.390
think without us?

00:10:41.390 --> 00:10:47.660
Versus, how do we intimately
connect humans to computers?

00:10:47.660 --> 00:10:49.729
I argue in the book--
not forcefully,

00:10:49.729 --> 00:10:51.020
because the book's a narrative.

00:10:51.020 --> 00:10:52.060
It's not preaching.

00:10:52.060 --> 00:10:53.490
It's just a tale.

00:10:53.490 --> 00:10:55.520
But I just tell the
tale, which happens

00:10:55.520 --> 00:11:00.630
to tell you that ever since Alan
Turing came up with the Turing

00:11:00.630 --> 00:11:04.190
test, it's always been,
20 years in the future

00:11:04.190 --> 00:11:06.660
there'll be machines that can
think and leave us behind.

00:11:06.660 --> 00:11:08.610
The singularity will happen.

00:11:08.610 --> 00:11:10.520
I even go through
it-- I tried not

00:11:10.520 --> 00:11:12.520
to embarrass some of my
colleagues in the media.

00:11:12.520 --> 00:11:14.050
Some are still around.

00:11:14.050 --> 00:11:18.174
But you can start in 1957
with the perceptron, one

00:11:18.174 --> 00:11:19.590
of the machines
that were supposed

00:11:19.590 --> 00:11:22.810
to mimic the neural
networks of the human brain.

00:11:22.810 --> 00:11:25.340
And it always says,
in 20 years, there'll

00:11:25.340 --> 00:11:27.500
be machines that can
think without us.

00:11:27.500 --> 00:11:30.200
And every 20 years comes
along, and it's always still

00:11:30.200 --> 00:11:31.770
another 20 years.

00:11:31.770 --> 00:11:33.370
You can read Ray Kurzweil.

00:11:33.370 --> 00:11:34.520
It's always 20 years.

00:11:34.520 --> 00:11:36.569
Whether he's telling you
in 1990 it will happen,

00:11:36.569 --> 00:11:38.610
or now it's going to
happen, they always tell you

00:11:38.610 --> 00:11:39.984
it's going to
happen in 20 years.

00:11:39.984 --> 00:11:42.065
There's a wonderful
guy in the book,

00:11:42.065 --> 00:11:45.170
Lick Licklider, JCR Licklider.

00:11:45.170 --> 00:11:48.374
He comes up with the notion
of interactive computing,

00:11:48.374 --> 00:11:50.040
because he's doing
an air defense system

00:11:50.040 --> 00:11:52.950
and you can't batch process
when a missile may be coming.

00:11:52.950 --> 00:11:55.050
He also comes up with
graphical user interfaces,

00:11:55.050 --> 00:11:58.622
because you can't make a mistake
and shoot down the wrong thing.

00:11:58.622 --> 00:12:00.830
He also comes up with what
he calls the Intergalactic

00:12:00.830 --> 00:12:03.121
Computer Network, because he
had a good sense of humor,

00:12:03.121 --> 00:12:05.140
and he had 23 air
defense stations

00:12:05.140 --> 00:12:06.420
that had to be connected.

00:12:06.420 --> 00:12:08.420
So when he goes to the
Pentagon and he funds it,

00:12:08.420 --> 00:12:10.919
it becomes the ARPANET,
then the internet.

00:12:10.919 --> 00:12:12.460
One of the things
Lick Licklider said

00:12:12.460 --> 00:12:13.900
is everybody keeps
telling me machines

00:12:13.900 --> 00:12:15.608
are going to think
without us, that we're

00:12:15.608 --> 00:12:17.646
going to have robots and
artificial intelligence

00:12:17.646 --> 00:12:18.520
and machine learning.

00:12:18.520 --> 00:12:20.570
They always say it's
happening in the future.

00:12:20.570 --> 00:12:22.435
And I say, in the
meantime, let's just

00:12:22.435 --> 00:12:24.060
become more intimate
with our machines,

00:12:24.060 --> 00:12:26.860
connected more, have graphical
interfaces so we feel

00:12:26.860 --> 00:12:29.990
comfortable with them,
that sort of thing.

00:12:29.990 --> 00:12:31.864
Sometimes when I
say that the history

00:12:31.864 --> 00:12:33.280
of the digital
revolution has been

00:12:33.280 --> 00:12:36.360
the awesome success of the
Lady Lovelace strand, which

00:12:36.360 --> 00:12:39.310
is becoming more intimate--
and I get Google Glass

00:12:39.310 --> 00:12:41.470
this afternoon,
which is my next step

00:12:41.470 --> 00:12:43.990
to being more intimate
with my machines.

00:12:43.990 --> 00:12:46.250
I keep saying, that's
been awesomely successful.

00:12:46.250 --> 00:12:49.000
And artificial
intelligence has been--

00:12:49.000 --> 00:12:51.740
there's a wonderful line, not
about artificial intelligence

00:12:51.740 --> 00:12:54.170
but I use it about
that, which is,

00:12:54.170 --> 00:12:58.460
after decades of rampant growth,
artificial intelligence is now

00:12:58.460 --> 00:13:00.430
entering its infancy.

00:13:00.430 --> 00:13:02.220
It's always 20 years away.

00:13:02.220 --> 00:13:04.330
And people say,
well, look at Google.

00:13:04.330 --> 00:13:07.270
Google is artificial-- you
know, you can type in anything.

00:13:07.270 --> 00:13:08.397
And it'll answer.

00:13:08.397 --> 00:13:09.230
It's like a machine.

00:13:09.230 --> 00:13:11.000
It can pass, virtually,
the Turing test.

00:13:11.000 --> 00:13:12.276
First of all, it can't.

00:13:12.276 --> 00:13:15.790
You can ask Google a
really difficult question

00:13:15.790 --> 00:13:17.750
that none of you
know the answer to,

00:13:17.750 --> 00:13:20.801
like what is the
depth of the Red Sea.

00:13:20.801 --> 00:13:21.300
Bing.

00:13:21.300 --> 00:13:22.540
Right up top.

00:13:22.540 --> 00:13:24.260
5,267 feet.

00:13:24.260 --> 00:13:25.380
It knows it.

00:13:25.380 --> 00:13:27.030
Your smartest friends
don't know that.

00:13:27.030 --> 00:13:29.910
But if you ask Google--
with all due respect--

00:13:29.910 --> 00:13:34.360
some question like can a
crocodile play basketball,

00:13:34.360 --> 00:13:36.790
you're going to end up
with the Florida Gators

00:13:36.790 --> 00:13:37.910
schedule or something.

00:13:37.910 --> 00:13:39.890
But you ain't going to
end up with an answer

00:13:39.890 --> 00:13:41.860
to that question.

00:13:41.860 --> 00:13:44.490
And those of us from Louisiana
know a crocodile's not a gator.

00:13:47.230 --> 00:13:50.660
And so I say no.

00:13:50.660 --> 00:13:55.590
First of all, we haven't gotten
to the point where our machine

00:13:55.590 --> 00:14:01.280
learning replicates, or tries
to replicate, the analog, messy,

00:14:01.280 --> 00:14:04.650
wetware of the human
brain in digital form.

00:14:04.650 --> 00:14:08.790
Secondly-- and this is where
the book wraps it all together,

00:14:08.790 --> 00:14:14.830
with Larry and Sergey at
graduate school in Stanford.

00:14:14.830 --> 00:14:17.450
And when they decide to
create the algorithm,

00:14:17.450 --> 00:14:19.470
as you all know full
well, but most people

00:14:19.470 --> 00:14:22.420
don't focus on-- it's
not a web crawler

00:14:22.420 --> 00:14:25.130
that goes around and decides
how to answer questions

00:14:25.130 --> 00:14:27.770
by some algorithm that does it.

00:14:27.770 --> 00:14:32.950
It crawls around and
collates, and brings together,

00:14:32.950 --> 00:14:36.300
billions of human judgments
made by real people

00:14:36.300 --> 00:14:38.900
every day when they put
a link on their website.

00:14:38.900 --> 00:14:43.630
It is the ultimate Ada Lovelace
integration partnership

00:14:43.630 --> 00:14:48.230
and symbiosis of human judgments
with machine algorithms.

00:14:48.230 --> 00:14:52.800
And so that is why Google
is so central to my book.

00:14:52.800 --> 00:14:58.150
And also why, 20 years from
now, either we will have robots

00:14:58.150 --> 00:15:01.170
and a singularity that
will have left us behind--

00:15:01.170 --> 00:15:04.530
in which case I will
apologize-- or we won't.

00:15:04.530 --> 00:15:06.850
And we'll still be
following the trajectory

00:15:06.850 --> 00:15:08.690
of the digital revolution.

00:15:08.690 --> 00:15:11.070
And I'll come back here
and say, I told you so.

00:15:11.070 --> 00:15:12.070
Anyway, thank you all.

00:15:12.070 --> 00:15:13.580
Let's open it up, if we could.

00:15:16.129 --> 00:15:18.420
I want to tell you that Google
colleagues in Washington

00:15:18.420 --> 00:15:19.400
were not shy.

00:15:19.400 --> 00:15:21.115
They were peppering
me with questions.

00:15:21.115 --> 00:15:23.530
So do not let them down, please.

00:15:23.530 --> 00:15:24.030
Yes.

00:15:24.030 --> 00:15:28.944
Are you leaning
forward, or-- Yeah.

00:15:28.944 --> 00:15:30.619
Oh, OK.

00:15:30.619 --> 00:15:31.160
AUDIENCE: Hi.

00:15:31.160 --> 00:15:33.750
So, what's the biggest
surprise that you

00:15:33.750 --> 00:15:35.578
did when you were
researching this book?

00:15:39.880 --> 00:15:41.820
WALTER ISAACSON: I'll
say this-- not simply

00:15:41.820 --> 00:15:47.160
because you're a woman--
but the biggest surprise was

00:15:47.160 --> 00:15:50.350
the pioneering role
of women in software.

00:15:50.350 --> 00:15:52.105
I knew about Ada.

00:15:52.105 --> 00:15:54.480
Actually, there's a "New York
Times" piece by Nick Bilton

00:15:54.480 --> 00:15:58.720
a few weeks ago about my book,
in which I tell the story--

00:15:58.720 --> 00:16:01.590
and got it slightly
wrong, I'm sorry-- which

00:16:01.590 --> 00:16:04.755
was that I learned about
Ada because my daughter, who

00:16:04.755 --> 00:16:06.880
was applying to college,
decided to do her entrance

00:16:06.880 --> 00:16:08.670
essay on Ada Lovelace.

00:16:08.670 --> 00:16:09.740
And I said, who's Ada?

00:16:09.740 --> 00:16:14.690
I actually knew who Ada was, but
I couldn't remember, frankly,

00:16:14.690 --> 00:16:17.520
what she had done, exactly.

00:16:17.520 --> 00:16:20.910
And so I got interested
in Ada Lovelace.

00:16:20.910 --> 00:16:23.770
But then I became
interested in the six women

00:16:23.770 --> 00:16:25.600
who programmed ENIAC.

00:16:25.600 --> 00:16:27.240
You know, boys with
their toys believed

00:16:27.240 --> 00:16:30.120
the hardware was the only thing.

00:16:30.120 --> 00:16:32.970
And so they left the
task that they thought

00:16:32.970 --> 00:16:35.030
was not quite as
important of programming

00:16:35.030 --> 00:16:39.360
it-- [? replugging ?] the cables
and all-- to six great women

00:16:39.360 --> 00:16:42.260
mathematicians.

00:16:42.260 --> 00:16:46.780
And what surprised me-- to get
more granular in the surprise--

00:16:46.780 --> 00:16:50.910
so I'm reading about Jean
Jennings Bartik and Frances

00:16:50.910 --> 00:16:53.950
Bilas, but also Grace
Hopper up at Harvard.

00:16:53.950 --> 00:16:57.720
And they all have PhDs
in math from the 1930s.

00:16:57.720 --> 00:17:00.560
One little surprise
was more women

00:17:00.560 --> 00:17:04.859
got PhDs in math-- in absolute
number and in proportion--

00:17:04.859 --> 00:17:07.690
in the '30s than during
the '50s or '60s.

00:17:07.690 --> 00:17:12.599
In other words,
women back then were

00:17:12.599 --> 00:17:15.970
much more advanced in
the world of mathematics

00:17:15.970 --> 00:17:17.599
than they later became.

00:17:17.599 --> 00:17:19.589
And I don't know why the
backsliding happened.

00:17:19.589 --> 00:17:22.069
But one problem, I think, is
there were no role models.

00:17:22.069 --> 00:17:25.609
I mean, these six women who
programmed ENIAC didn't become

00:17:25.609 --> 00:17:28.329
famous because partly,
it was wartime secrecy.

00:17:28.329 --> 00:17:33.310
And partly, they just-- people
got written out of history.

00:17:33.310 --> 00:17:38.580
More women got degrees in
computer science in 1980

00:17:38.580 --> 00:17:41.210
than got degrees in
computer science last year.

00:17:41.210 --> 00:17:43.650
In fact, the proportion
has been cut in half,

00:17:43.650 --> 00:17:46.790
from 38% to just over 17%.

00:17:46.790 --> 00:17:48.530
So all of that was weird to me.

00:17:48.530 --> 00:17:53.790
And I wanted to at least
explain how COBOL-- all

00:17:53.790 --> 00:17:56.660
the great programming
languages done collaboratively

00:17:56.660 --> 00:18:00.060
by people like Grace Hopper.

00:18:00.060 --> 00:18:02.273
And I think it's kind
of useful, if you're

00:18:02.273 --> 00:18:06.180
going to have a revolution, to
make sure 100% of the people

00:18:06.180 --> 00:18:09.820
have the chance to be included.

00:18:09.820 --> 00:18:12.210
AUDIENCE: Having studied so
many of the great innovators

00:18:12.210 --> 00:18:16.300
of humanity, have you noticed
any common threads in how

00:18:16.300 --> 00:18:18.310
their personalities,
environments, techniques

00:18:18.310 --> 00:18:20.400
work to kind of result
in those innovations?

00:18:20.400 --> 00:18:22.150
WALTER ISAACSON: Well,
first off, I always

00:18:22.150 --> 00:18:24.550
say this isn't a how-to book.

00:18:24.550 --> 00:18:27.880
Steve Jobs was, don't
try this at home.

00:18:27.880 --> 00:18:28.780
Read Steve Jobs.

00:18:28.780 --> 00:18:32.380
OK, I'm going to--
likewise, this book.

00:18:32.380 --> 00:18:34.950
There are a lot of people who
write how-to books-- you know,

00:18:34.950 --> 00:18:40.830
"Seven Secrets to Innovation"
or "12 Steps to Being a Leader."

00:18:40.830 --> 00:18:42.800
A, I think those books suck.

00:18:42.800 --> 00:18:46.600
And secondly, it doesn't
leave room for biographies.

00:18:46.600 --> 00:18:49.400
And biographies tell you,
people are more complex--

00:18:49.400 --> 00:18:53.420
that Bob Noyce is totally
different from Steve Jobs.

00:18:53.420 --> 00:18:56.560
And yet, Noyce was
a mentor to Jobs.

00:18:56.560 --> 00:18:59.280
And they both were creative.

00:18:59.280 --> 00:19:01.860
So I try to do it
through real people.

00:19:01.860 --> 00:19:04.850
I think the word "innovation"
has been so overused,

00:19:04.850 --> 00:19:06.840
it's sapped of most
of its meaning.

00:19:06.840 --> 00:19:07.922
It's become a buzzword.

00:19:07.922 --> 00:19:08.880
So I wanted to say, OK.

00:19:08.880 --> 00:19:09.930
Here are real people.

00:19:09.930 --> 00:19:12.380
You don't know Engelbart that
well, but let me show you.

00:19:12.380 --> 00:19:14.180
Here's how he made his leap.

00:19:14.180 --> 00:19:17.520
There are, having said
that, a few common threads.

00:19:17.520 --> 00:19:20.470
One of which is everybody
who's a great genius--

00:19:20.470 --> 00:19:24.050
and this is a bad story, so
don't take this to heart--

00:19:24.050 --> 00:19:25.340
dropped out of school.

00:19:25.340 --> 00:19:26.740
It's like, forget it.

00:19:26.740 --> 00:19:29.030
You know, whether it's Ben
Franklin or Mark Zuckerberg

00:19:29.030 --> 00:19:31.810
or Bill Gates or Steve
Jobs or Einstein, even,

00:19:31.810 --> 00:19:33.970
who runs away-- this is
why I don't get asked

00:19:33.970 --> 00:19:35.400
to speak at college graduations.

00:19:38.310 --> 00:19:40.430
But it's not really that
they drop out of school.

00:19:40.430 --> 00:19:42.150
What they are is rebellious.

00:19:42.150 --> 00:19:45.340
They don't like received wisdom.

00:19:45.340 --> 00:19:48.320
You can teach Einstein
the first paragraph

00:19:48.320 --> 00:19:52.020
of the Principia, which says
time marches along irrespective

00:19:52.020 --> 00:19:55.820
of how we observe it, and he
says, how do we know that?

00:19:55.820 --> 00:19:57.440
How would we test that?

00:19:57.440 --> 00:19:59.610
I mean, no received
wisdom is taken

00:19:59.610 --> 00:20:02.420
without pushing back
against authority.

00:20:02.420 --> 00:20:04.920
If you look at the people who
did the internet, part of them

00:20:04.920 --> 00:20:06.600
were pushing back
against authority

00:20:06.600 --> 00:20:08.580
because they were
avoiding the Vietnam War.

00:20:08.580 --> 00:20:11.790
And that's where perpetual
graduate students-- and they

00:20:11.790 --> 00:20:15.690
were just rebellious, as was
Steve, as was Bill Gates.

00:20:15.690 --> 00:20:19.350
So that ability to
question authority,

00:20:19.350 --> 00:20:22.500
think different,
as Steve would say.

00:20:22.500 --> 00:20:24.860
Think out of the box.

00:20:24.860 --> 00:20:31.100
That's the common trait from
Ben Franklin to Einstein, to Bob

00:20:31.100 --> 00:20:36.348
Noyce, and Steve Jobs, and
Larry and Sergey, and others.

00:20:36.348 --> 00:20:36.847
Yeah.

00:20:36.847 --> 00:20:37.388
AUDIENCE: Hi.

00:20:37.388 --> 00:20:38.305
Thanks for being here.

00:20:38.305 --> 00:20:39.304
WALTER ISAACSON: Thanks.

00:20:39.304 --> 00:20:39.880
My pleasure.

00:20:39.880 --> 00:20:41.240
AUDIENCE: So I have a
question, not about this work,

00:20:41.240 --> 00:20:43.448
but about the fruition of
one of your previous works,

00:20:43.448 --> 00:20:48.940
which is what the process was
like working with Aaron Sorkin

00:20:48.940 --> 00:20:52.720
and the Steve Jobs notes
and things like that.

00:20:52.720 --> 00:20:57.700
WALTER ISAACSON: Well actually,
I admire Aaron Sorkin hugely,

00:20:57.700 --> 00:20:59.549
but I haven't really
worked with him.

00:20:59.549 --> 00:21:00.840
I've met him a couple of times.

00:21:03.009 --> 00:21:04.550
You have to know
what you're good at.

00:21:04.550 --> 00:21:05.950
I've proven on
the national stage

00:21:05.950 --> 00:21:07.590
I don't know cable TV that well.

00:21:07.590 --> 00:21:10.810
When I was at CNN, it was
like-- it's just not my medium.

00:21:10.810 --> 00:21:12.130
And the same is true of movies.

00:21:12.130 --> 00:21:14.470
And so when they
bought the rights

00:21:14.470 --> 00:21:16.090
to the book, and
Aaron Sorkin-- I

00:21:16.090 --> 00:21:17.660
said whoa, the
great screenwriter.

00:21:17.660 --> 00:21:19.660
They said, well, do you
want to be a consultant?

00:21:19.660 --> 00:21:21.510
Do you want to be part of it?

00:21:21.510 --> 00:21:23.310
I said no, actually, I don't.

00:21:23.310 --> 00:21:26.170
Because I'm going
to be too literal.

00:21:26.170 --> 00:21:28.820
I'm not the best
person to do it.

00:21:28.820 --> 00:21:31.270
So I didn't really--
I've kind of got

00:21:31.270 --> 00:21:36.240
to have a meal or two with
him, but I have not helped him,

00:21:36.240 --> 00:21:38.800
alas, write that screenplay.

00:21:38.800 --> 00:21:42.010
I wish I had the talent to
write screenplays and stuff,

00:21:42.010 --> 00:21:42.739
but I don't.

00:21:42.739 --> 00:21:43.864
AUDIENCE: Have you read it?

00:21:43.864 --> 00:21:44.300
WALTER ISAACSON: Huh?

00:21:44.300 --> 00:21:45.740
AUDIENCE: Have you read it?

00:21:45.740 --> 00:21:47.010
WALTER ISAACSON: There
are different parts

00:21:47.010 --> 00:21:49.590
and different versions, and I
probably ought not go there,

00:21:49.590 --> 00:21:51.297
but yeah.

00:21:51.297 --> 00:21:52.005
AUDIENCE: Thanks.

00:21:54.620 --> 00:21:55.740
AUDIENCE: Hi.

00:21:55.740 --> 00:21:58.486
What other modern day
inventors, and innovators--

00:21:58.486 --> 00:22:00.610
even though you don't like
the word-- interest you?

00:22:00.610 --> 00:22:01.090
Who else do you--

00:22:01.090 --> 00:22:03.214
WALTER ISAACSON: Well I
love the word "innovators,"

00:22:03.214 --> 00:22:04.970
meaning people actually do it.

00:22:04.970 --> 00:22:07.490
I just don't like the
concept of innovation

00:22:07.490 --> 00:22:09.530
as an abstract
concept that you can

00:22:09.530 --> 00:22:10.977
learn in seven easy lessons.

00:22:10.977 --> 00:22:11.560
It's like, no.

00:22:11.560 --> 00:22:14.460
You have to see how
real innovators did

00:22:14.460 --> 00:22:17.150
it, which is why I
did-- I love Elon Musk.

00:22:17.150 --> 00:22:19.280
I just was with him last
week in San Francisco.

00:22:19.280 --> 00:22:23.730
I got to interview
him and do some stuff.

00:22:23.730 --> 00:22:29.450
And I think he's in a tougher
place than we in this room,

00:22:29.450 --> 00:22:33.580
meaning, it's harder to
innovate in physical industries

00:22:33.580 --> 00:22:35.920
like transportation and cars.

00:22:35.920 --> 00:22:37.970
We have an
over-regulated society.

00:22:37.970 --> 00:22:43.900
We have-- you know, you cannot
easily, in the garage, with,

00:22:43.900 --> 00:22:47.070
you know, the kid named
Raj from down the street,

00:22:47.070 --> 00:22:48.890
invent an auto company.

00:22:48.890 --> 00:22:52.330
But you can invent Apple.

00:22:52.330 --> 00:22:55.709
I also think he just thinks--
I mean, talk about rebellious,

00:22:55.709 --> 00:22:57.500
talk about questioning
authority, whatever.

00:22:57.500 --> 00:22:59.360
I think he's very good.

00:22:59.360 --> 00:23:01.770
I'm not just blowing
smoke to think that I just

00:23:01.770 --> 00:23:04.190
get awed by Google at
every step of the way,

00:23:04.190 --> 00:23:06.940
and the new things they're
doing, including cars,

00:23:06.940 --> 00:23:08.440
driverless cars and stuff.

00:23:08.440 --> 00:23:11.670
I watch Larry Page.

00:23:11.670 --> 00:23:15.070
I mean, I'm sure you all have
met him or listened to him.

00:23:15.070 --> 00:23:21.740
Larry's mind is so fast that
the biggest mistake you can do

00:23:21.740 --> 00:23:25.100
is have anything that
hints at a premise

00:23:25.100 --> 00:23:27.550
when you're about to
ask him something.

00:23:27.550 --> 00:23:30.740
Because then he decides to drill
down and question the premise.

00:23:30.740 --> 00:23:33.710
Like, if you said,
because it's sunny today,

00:23:33.710 --> 00:23:36.790
I want to ask you-- and
then all of a sudden,

00:23:36.790 --> 00:23:40.170
it would be questioning
the whole question of sun.

00:23:40.170 --> 00:23:43.010
I mean, I'm exaggerating
there, but that's

00:23:43.010 --> 00:23:48.540
a mark of a very questioning,
fertile mind, who

00:23:48.540 --> 00:23:53.530
is going to take nothing
as received wisdom.

00:23:53.530 --> 00:24:01.060
And his ability to tell the
tale of how he created Google is

00:24:01.060 --> 00:24:03.950
better than Steve Jobs's.

00:24:03.950 --> 00:24:07.570
Steve was very intuitive.

00:24:07.570 --> 00:24:10.380
And I spent a lot
of time with him.

00:24:10.380 --> 00:24:14.500
But if I tried to drill
down with him on especially,

00:24:14.500 --> 00:24:18.140
say, software-- like, OK,
there's a Darwin kernel

00:24:18.140 --> 00:24:20.140
that you had in the
[? next ?] operating system,

00:24:20.140 --> 00:24:24.490
and to what extent did you
make this decision involving

00:24:24.490 --> 00:24:27.720
how it was going to be part
of the Apple operating system

00:24:27.720 --> 00:24:29.306
when you're back at Apple?

00:24:29.306 --> 00:24:29.930
There's no way.

00:24:29.930 --> 00:24:31.930
I'd peel back and you
know, I'd get [? blank. ?]

00:24:31.930 --> 00:24:37.810
So he was not deeply reflective
of how decisions are made.

00:24:37.810 --> 00:24:39.930
Now he would talk
intuitively about why

00:24:39.930 --> 00:24:42.150
the iPad had to
have a curved thing,

00:24:42.150 --> 00:24:44.410
or what he did in
Jony Ive's studio.

00:24:44.410 --> 00:24:49.870
But he was not
self-analytic in terms

00:24:49.870 --> 00:24:51.840
of his decision-making process.

00:24:51.840 --> 00:24:56.270
I found Larry Page
in particular to be

00:24:56.270 --> 00:24:59.790
very smart about
understanding how

00:24:59.790 --> 00:25:01.750
his creative process worked.

00:25:01.750 --> 00:25:03.780
And I'm sorry I put it
at the end of the book,

00:25:03.780 --> 00:25:06.030
but you can skip forward and
skip all the middle parts

00:25:06.030 --> 00:25:07.130
if you want.

00:25:07.130 --> 00:25:10.859
But I find his interview
quite interesting.

00:25:10.859 --> 00:25:11.400
AUDIENCE: Hi.

00:25:11.400 --> 00:25:13.660
I had a question about your
methods as a biographer.

00:25:13.660 --> 00:25:16.130
Because writing about people
is one skill, but actually

00:25:16.130 --> 00:25:18.910
understanding them is
a very different one.

00:25:18.910 --> 00:25:21.010
So what is your approach
when you come across

00:25:21.010 --> 00:25:22.520
these very different,
unique people?

00:25:22.520 --> 00:25:24.686
Because if you come with
preconceived notions of how

00:25:24.686 --> 00:25:26.790
to analyze the life and
creativity of someone,

00:25:26.790 --> 00:25:27.810
you end up losing a lot.

00:25:27.810 --> 00:25:28.500
So what's your method?

00:25:28.500 --> 00:25:29.600
Is it spending time with them?

00:25:29.600 --> 00:25:30.516
WALTER ISAACSON: Yeah.

00:25:30.516 --> 00:25:32.160
Narrative tends to
distort history,

00:25:32.160 --> 00:25:34.370
because you're sort of
trying to push things

00:25:34.370 --> 00:25:37.170
into whatever arc you created.

00:25:37.170 --> 00:25:40.270
So you avoid trying to have
a narrative arc before you

00:25:40.270 --> 00:25:42.890
have the data
points that plot it.

00:25:42.890 --> 00:25:46.120
Secondly, when you're
talking about people,

00:25:46.120 --> 00:25:47.710
there's the simplest
of all things

00:25:47.710 --> 00:25:49.860
to do that I think most
biographers and journalists

00:25:49.860 --> 00:25:53.500
don't do enough of, which
is, just think about it

00:25:53.500 --> 00:25:55.030
and look in yourself a bit.

00:25:55.030 --> 00:25:58.710
Like if Steve Jobs
did something--

00:25:58.710 --> 00:26:02.480
whether it was a personal thing,
like dealing with his firstborn

00:26:02.480 --> 00:26:07.170
child born out of wedlock,
or dealing with being ousted

00:26:07.170 --> 00:26:09.940
from Apple, and what he said
to somebody on the board

00:26:09.940 --> 00:26:13.060
and what happened-- I
sometimes think, OK.

00:26:13.060 --> 00:26:15.470
Let me put myself
in that position.

00:26:15.470 --> 00:26:16.950
Let me try to feel it.

00:26:16.950 --> 00:26:19.580
Instead of playing gotcha,
where you kind of say--

00:26:19.580 --> 00:26:22.370
whether you're writing
about a politician

00:26:22.370 --> 00:26:25.370
or you're writing about the
head of the Centers for Disease

00:26:25.370 --> 00:26:30.370
Control or the head of Apple
or the Secretary of Health

00:26:30.370 --> 00:26:33.730
and Human Services-- you
sort of say, wait a minute.

00:26:33.730 --> 00:26:35.750
I've been in those
situations before.

00:26:35.750 --> 00:26:37.870
I've made mistakes.

00:26:37.870 --> 00:26:40.920
I've done it not for evil
motives or bad motives,

00:26:40.920 --> 00:26:43.320
but because this happened.

00:26:43.320 --> 00:26:47.880
And you try very hard to
put yourself in their shoes

00:26:47.880 --> 00:26:50.820
and to see it as they
would at that moment,

00:26:50.820 --> 00:26:53.910
rather than imposing
what's sometimes

00:26:53.910 --> 00:26:58.080
called presentism-- a very bad
word-- where a biographer will

00:26:58.080 --> 00:27:05.140
impose what we know at the
present on the subject who only

00:27:05.140 --> 00:27:07.700
knew what that subject
knew at that point.

00:27:07.700 --> 00:27:09.480
And sometimes people
say, well, how

00:27:09.480 --> 00:27:11.440
can Steve Jobs have done this?

00:27:11.440 --> 00:27:17.775
Or how could Bill
Gates have done this?

00:27:17.775 --> 00:27:18.650
I say, wait a minute.

00:27:18.650 --> 00:27:20.191
Have you ever been
in that situation?

00:27:20.191 --> 00:27:21.250
Think about it.

00:27:21.250 --> 00:27:22.230
How would you react?

00:27:22.230 --> 00:27:23.380
How would you feel?

00:27:23.380 --> 00:27:26.500
So I try to empathize more.

00:27:26.500 --> 00:27:29.624
And it sometimes means
sugarcoating things.

00:27:29.624 --> 00:27:31.540
I mean, there's things
in the Steve Jobs book,

00:27:31.540 --> 00:27:34.270
if you'll read, you'll say,
wow, that was pretty bad.

00:27:34.270 --> 00:27:36.690
He yelled at somebody.

00:27:36.690 --> 00:27:38.880
And well, yeah, I yelled
at people occasionally.

00:27:38.880 --> 00:27:42.450
And it got them to do better
work-- not when I did it,

00:27:42.450 --> 00:27:44.270
but when Steve did it.

00:27:44.270 --> 00:27:47.160
And so I try to put
that into a context.

00:27:47.160 --> 00:27:50.070
So you say, all
right, he was tough

00:27:50.070 --> 00:27:56.290
but I can now put myself
in his head, or her head.

00:27:56.290 --> 00:28:00.210
And it makes for
a more empathetic,

00:28:00.210 --> 00:28:05.290
thus a more sympathetic,
biography, which in some ways

00:28:05.290 --> 00:28:08.600
is bad, because you're
not as tough of a writer.

00:28:08.600 --> 00:28:11.255
But I look at a Bob
Woodward and know

00:28:11.255 --> 00:28:14.710
I could never be a Bob Woodward,
because he's much tougher.

00:28:14.710 --> 00:28:16.300
And I would be
saying, well, I can

00:28:16.300 --> 00:28:17.620
understand why Nixon did that.

00:28:17.620 --> 00:28:20.030
I probably would have
bugged-- not really,

00:28:20.030 --> 00:28:22.880
but you know what I mean.

00:28:22.880 --> 00:28:24.510
You get two types
of biographers.

00:28:24.510 --> 00:28:27.050
Those that really
want to expose things,

00:28:27.050 --> 00:28:30.840
and those that somehow go soft,
as we're sometimes accused of.

00:28:30.840 --> 00:28:33.780
But I want to say, yeah,
but understand-- I mean,

00:28:33.780 --> 00:28:38.050
I was just re-reading
a part of my book

00:28:38.050 --> 00:28:40.370
because I was dealing
with Bill Gates,

00:28:40.370 --> 00:28:44.330
and I wanted to-- There's a
part of it in which he and Paul

00:28:44.330 --> 00:28:46.892
Allen have split it 50/50.

00:28:46.892 --> 00:28:48.350
And then Bill's
doing all the work.

00:28:48.350 --> 00:28:50.710
And they're in Albuquerque,
and he's coding all night long.

00:28:50.710 --> 00:28:52.959
And he's also hiring and
firing, going on sales calls.

00:28:52.959 --> 00:28:56.010
And he finally tells Paul
Allen, no, 60/40-- and then I

00:28:56.010 --> 00:28:59.690
think 66, whatever, 33.

00:28:59.690 --> 00:29:01.900
And people have written
about that, including Paul,

00:29:01.900 --> 00:29:03.130
and say isn't that horrible?

00:29:03.130 --> 00:29:04.440
You know, he jammed me.

00:29:07.220 --> 00:29:09.307
I was trying to remember
how I had handled it.

00:29:09.307 --> 00:29:11.140
And there's a paragraph
I write that begins,

00:29:11.140 --> 00:29:15.840
to be fair to Bill Gates
comma-- and then I explain,

00:29:15.840 --> 00:29:17.440
he was running the show.

00:29:17.440 --> 00:29:23.590
And so I try-- is this
answering your question?

00:29:23.590 --> 00:29:26.190
I try to feel what
the person was feeling

00:29:26.190 --> 00:29:29.833
and maybe try to understand
the motives more.

00:29:29.833 --> 00:29:31.333
AUDIENCE: Yeah,
that's very helpful.

00:29:31.333 --> 00:29:31.833
Thank you.

00:29:31.833 --> 00:29:34.270
WALTER ISAACSON: OK.

00:29:34.270 --> 00:29:35.590
AUDIENCE: Hello.

00:29:35.590 --> 00:29:38.820
So the Jobs biography
was a fascinating read.

00:29:38.820 --> 00:29:40.632
Thank you for that book.

00:29:40.632 --> 00:29:42.590
At the start of the book,
there is a little bit

00:29:42.590 --> 00:29:44.670
of a section in the
book where you mentioned

00:29:44.670 --> 00:29:48.080
that when Jobs asked
you to write about him,

00:29:48.080 --> 00:29:49.820
you were not very
certain about it.

00:29:49.820 --> 00:29:52.030
You had to take
some time to decide.

00:29:52.030 --> 00:29:55.550
So I was curious about, how
did you make that decision?

00:29:55.550 --> 00:29:57.930
I mean, aside from
what's told in the book,

00:29:57.930 --> 00:30:00.510
more in terms of--
it's probably easier

00:30:00.510 --> 00:30:02.970
to write about people who
are way back in history,

00:30:02.970 --> 00:30:06.370
like somebody like
Einstein, because you're not

00:30:06.370 --> 00:30:08.110
interacting with them
on a daily basis.

00:30:08.110 --> 00:30:10.390
You have a lot of published
material available.

00:30:10.390 --> 00:30:12.655
So how do you decide
for start for Jobs

00:30:12.655 --> 00:30:13.655
and what's your process?

00:30:13.655 --> 00:30:15.770
WALTER ISAACSON: Well,
I do go back and forth.

00:30:15.770 --> 00:30:18.530
Meaning the first
big book I did--

00:30:18.530 --> 00:30:20.380
well, it was something
a friend and I did,

00:30:20.380 --> 00:30:23.540
something called "The Wise Men,"
but then I did "Kissinger."

00:30:23.540 --> 00:30:26.080
And trust me, after dealing
with Kissinger a whole lot,

00:30:26.080 --> 00:30:28.670
and then dealing with his
reaction to the book, I said,

00:30:28.670 --> 00:30:31.450
man, I'm going to do somebody
who's been dead for 200 years.

00:30:31.450 --> 00:30:32.990
So I go back to Ben Franklin.

00:30:32.990 --> 00:30:33.490
Right?

00:30:36.820 --> 00:30:39.700
I don't think I'm the
best historian you'll ever

00:30:39.700 --> 00:30:40.930
have on this stage, even.

00:30:40.930 --> 00:30:43.600
I mean you can get the
Doris Kearnses up the kazoo.

00:30:43.600 --> 00:30:45.980
I'm also probably not
the best reporter.

00:30:45.980 --> 00:30:49.010
As I say, Bob Woodward
probably is a little bit more

00:30:49.010 --> 00:30:50.540
dogged than I am.

00:30:50.540 --> 00:30:52.940
But if you do the
Venn diagram, I'm

00:30:52.940 --> 00:30:57.990
pretty good at combining
archival historical research

00:30:57.990 --> 00:30:59.780
with interviews.

00:30:59.780 --> 00:31:02.700
And I happen to have one
lucky thing in my life, which

00:31:02.700 --> 00:31:05.970
is having been the editor of
"Time," having written books.

00:31:05.970 --> 00:31:09.270
If I call Larry
Page, he says yes.

00:31:09.270 --> 00:31:10.310
Come on by.

00:31:10.310 --> 00:31:12.490
Let's spend the
morning going over it.

00:31:12.490 --> 00:31:16.120
Whereas if the kid
from down the street

00:31:16.120 --> 00:31:18.310
trying to write a book
about it calls Larry Page,

00:31:18.310 --> 00:31:21.050
he'll never get past-- so I have
a little bit more [? entry ?],

00:31:21.050 --> 00:31:22.810
which I don't want to screw up.

00:31:22.810 --> 00:31:24.610
So I try to do more interviews.

00:31:24.610 --> 00:31:27.240
I mean, I drive up to
Gordon Moore's house.

00:31:27.240 --> 00:31:29.850
I think that's a problem
with journalism today

00:31:29.850 --> 00:31:32.902
is that it's so easy for
people to be journalists

00:31:32.902 --> 00:31:35.110
and to write their own stuff,
that that notion of let

00:31:35.110 --> 00:31:37.900
me rent a car and drive
to Gordon Moore's house

00:31:37.900 --> 00:31:41.410
and hear him tell the
story-- there's not

00:31:41.410 --> 00:31:42.670
as much of that done.

00:31:42.670 --> 00:31:44.560
And there's more,
let me tell you

00:31:44.560 --> 00:31:46.925
why Moore's law doesn't
work or does work

00:31:46.925 --> 00:31:49.190
or give you my opinion on it.

00:31:49.190 --> 00:31:52.600
Totally understandable,
because A,

00:31:52.600 --> 00:31:54.870
you have to be somewhat
privileged to be

00:31:54.870 --> 00:31:59.330
able to get through and get
an appointment with some

00:31:59.330 --> 00:32:00.110
of these people.

00:32:00.110 --> 00:32:03.889
And B, it takes some resources
to fly out, rent cars,

00:32:03.889 --> 00:32:04.680
that sort of thing.

00:32:04.680 --> 00:32:06.410
So that's something
I bring to the party.

00:32:06.410 --> 00:32:09.045
I'm just lucky that I've
had a couple of books that

00:32:09.045 --> 00:32:11.490
were successful,
so I can do that.

00:32:11.490 --> 00:32:14.660
So that's what I
like to combine.

00:32:14.660 --> 00:32:17.130
Which gets to your
question of where

00:32:17.130 --> 00:32:22.100
do you pick, the Wayback Machine
versus somebody [INAUDIBLE].

00:32:22.100 --> 00:32:25.590
When Steve first talked to
me about it, and I think

00:32:25.590 --> 00:32:27.729
I put this in the introduction.

00:32:27.729 --> 00:32:30.020
You know, I had done Ben
Franklin, I had done Einstein.

00:32:30.020 --> 00:32:31.800
And my first reaction
was like, OK, yeah.

00:32:31.800 --> 00:32:33.780
Franklin, Einstein, you?

00:32:33.780 --> 00:32:35.830
I had known him
since '84 when he

00:32:35.830 --> 00:32:40.670
came to plug the Mac
at "Time" magazine.

00:32:40.670 --> 00:32:44.500
And I knew he was a genius and
wonderful, but he's my age.

00:32:44.500 --> 00:32:46.800
And I'm thinking,
wait, I don't want

00:32:46.800 --> 00:32:48.470
to write a biography
of somebody my age

00:32:48.470 --> 00:32:50.540
who's still in the
middle of his career.

00:32:50.540 --> 00:32:53.580
What I actually said to him was,
yeah, that'd be really cool.

00:32:53.580 --> 00:32:56.500
But let's wait 30
years until you retire.

00:32:56.500 --> 00:33:00.390
And then, you know, I was
just saying, put it off.

00:33:00.390 --> 00:33:03.640
And then, of course, I
realized he was sick,

00:33:03.640 --> 00:33:06.120
and that he had
called me, I think,

00:33:06.120 --> 00:33:08.400
right after he was diagnosed.

00:33:08.400 --> 00:33:10.125
So that put it in
a new perspective.

00:33:14.560 --> 00:33:17.100
And I also realize,
OK, I'm going

00:33:17.100 --> 00:33:20.620
to have a chance that
nobody else does.

00:33:20.620 --> 00:33:22.890
There are people who
know software engineering

00:33:22.890 --> 00:33:24.010
in this room.

00:33:24.010 --> 00:33:26.940
Every one of you probably
knows it a little bit.

00:33:26.940 --> 00:33:28.240
I mean, I try hard.

00:33:28.240 --> 00:33:29.150
I used to code.

00:33:29.150 --> 00:33:34.020
But I'm not-- but
I have the ability

00:33:34.020 --> 00:33:39.440
to get a Steve Jobs to want
to spend 40 days of me just

00:33:39.440 --> 00:33:42.710
sitting in his backyard, taking
walks, and talking to him.

00:33:42.710 --> 00:33:46.950
And so I figured, you don't
get that chance that often.

00:33:46.950 --> 00:33:47.880
I shouldn't blow it.

00:33:47.880 --> 00:33:51.850
I shouldn't deflect
it, especially when

00:33:51.850 --> 00:33:54.400
I knew he was sick.

00:33:54.400 --> 00:33:59.300
And very rarely does a
biographer or a journalist

00:33:59.300 --> 00:34:02.230
get to get that
close to a hugely

00:34:02.230 --> 00:34:05.070
brilliant, amazing subject.

00:34:05.070 --> 00:34:09.270
Obviously Boswell
does to Dr. Johnson.

00:34:09.270 --> 00:34:15.330
But you can list, probably on
one hand, the number of people

00:34:15.330 --> 00:34:18.880
who've gotten to spend an
enormous amount of time

00:34:18.880 --> 00:34:22.909
with a subject as
interesting as a Steve Jobs.

00:34:22.909 --> 00:34:24.850
So obviously, I
was going to do it.

00:34:24.850 --> 00:34:28.100
It was just a
question of timing.

00:34:28.100 --> 00:34:31.590
I had been working on this book,
"The Innovators," for really 15

00:34:31.590 --> 00:34:34.643
years, sort of off
and on, not knowing.

00:34:34.643 --> 00:34:38.872
I'd make Andy Grove Man
of the Year at "Time,"

00:34:38.872 --> 00:34:40.080
and spent some time with him.

00:34:40.080 --> 00:34:41.620
And then I'd
collect all my notes

00:34:41.620 --> 00:34:43.969
on how did Intel get formed.

00:34:43.969 --> 00:34:45.716
And I knew someday
I was going to try

00:34:45.716 --> 00:34:47.465
to do a history of the
digital revolution.

00:34:50.030 --> 00:34:54.590
And to me, this book is
the best of both worlds.

00:34:54.590 --> 00:34:56.929
Approximately half
the characters--

00:34:56.929 --> 00:35:02.040
there's about 80 characters
in the book, 40 done deeply.

00:35:02.040 --> 00:35:03.990
And about half of
them were alive,

00:35:03.990 --> 00:35:05.270
and I could interview them.

00:35:05.270 --> 00:35:08.820
All of them who were alive,
I think I went to interview.

00:35:08.820 --> 00:35:13.200
And there are great archives,
oral histories, documents,

00:35:13.200 --> 00:35:16.015
and nobody has written a history
of the digital revolution.

00:35:16.015 --> 00:35:18.390
So it's really the best of
both worlds, being a historian

00:35:18.390 --> 00:35:21.500
and being a journalist.

00:35:21.500 --> 00:35:24.820
AUDIENCE: So you, I think,
framed the book around people

00:35:24.820 --> 00:35:27.980
who are innovative or
innovators, as you call them.

00:35:27.980 --> 00:35:30.970
I was just curious to hear if
you had any thoughts about what

00:35:30.970 --> 00:35:32.830
it means for a company
to be innovative,

00:35:32.830 --> 00:35:34.990
if that's even possible,
or if that's just

00:35:34.990 --> 00:35:38.100
the product of a group of
innovators coming together.

00:35:38.100 --> 00:35:40.730
WALTER ISAACSON: Yeah,
it's very difficult

00:35:40.730 --> 00:35:43.540
to be innovative
once you get big

00:35:43.540 --> 00:35:46.920
and get to become a company.

00:35:46.920 --> 00:35:49.290
I did ask Steve, what
was his best product?

00:35:49.290 --> 00:35:52.330
What was he most proud
of, his best innovation?

00:35:52.330 --> 00:35:56.430
I thought he'd say the
Mac, or maybe the iPhone.

00:35:56.430 --> 00:35:58.780
He said, no, creating
a product like the Mac

00:35:58.780 --> 00:36:00.580
or the iPhone is pretty hard.

00:36:00.580 --> 00:36:02.590
But what's really
hard is creating

00:36:02.590 --> 00:36:05.370
a company that remains creative.

00:36:05.370 --> 00:36:07.340
And so the best thing
I created was Apple,

00:36:07.340 --> 00:36:09.080
because it's able
to remain creative.

00:36:09.080 --> 00:36:11.660
Why has Apple been
able to do it?

00:36:11.660 --> 00:36:13.160
Take the iPod.

00:36:13.160 --> 00:36:14.900
I mean, just this
huge success-- they're

00:36:14.900 --> 00:36:18.090
making money hand over fist
in the early 2000s when

00:36:18.090 --> 00:36:19.220
this thing comes out.

00:36:19.220 --> 00:36:21.576
And it's out of whole cloth.

00:36:21.576 --> 00:36:23.450
Who knew we needed 1,000
songs in our pocket?

00:36:23.450 --> 00:36:24.040
But we did.

00:36:26.780 --> 00:36:29.270
So then instead of being
happy, all of a sudden

00:36:29.270 --> 00:36:30.740
he's bummed out.

00:36:30.740 --> 00:36:34.720
And it's because he realizes
somebody could cannibalize it.

00:36:34.720 --> 00:36:37.930
That if the braindead
people who make cellphones

00:36:37.930 --> 00:36:41.570
figured out that they could put
music and create a smartphone,

00:36:41.570 --> 00:36:43.760
that would put iPods
out of business.

00:36:43.760 --> 00:36:46.126
So he creates the iPhone.

00:36:46.126 --> 00:36:47.750
And the people at
Apple say, well wait,

00:36:47.750 --> 00:36:51.460
that will cannibalize
our iPod business, right?

00:36:51.460 --> 00:36:54.410
And he says, yeah, but if
we don't, somebody else

00:36:54.410 --> 00:36:56.770
will eat us for lunch.

00:36:56.770 --> 00:37:00.410
So that's what it takes to avoid
what Clayton Christensen calls

00:37:00.410 --> 00:37:01.730
the Innovator's Dilemma.

00:37:01.730 --> 00:37:05.450
It's the willingness
to destroy in order

00:37:05.450 --> 00:37:08.130
to create something new.

00:37:08.130 --> 00:37:10.660
I'm a little woozy
because I was at 5:00 AM

00:37:10.660 --> 00:37:15.550
on Bloomberg financial
TV, God knows why.

00:37:15.550 --> 00:37:22.660
But a person kept asking
me, but isn't Apple

00:37:22.660 --> 00:37:26.370
going to hurt the iPad?

00:37:26.370 --> 00:37:28.082
Aren't they in
trouble with the iPad

00:37:28.082 --> 00:37:29.790
because their phones
are becoming bigger?

00:37:29.790 --> 00:37:30.560
And it's true.

00:37:30.560 --> 00:37:34.250
I no longer take my iPad around
town with me, or even on trips

00:37:34.250 --> 00:37:37.800
sometimes, because
I've got my smartphone.

00:37:37.800 --> 00:37:40.630
And I said yes, but the
whole point I'm making

00:37:40.630 --> 00:37:45.700
is Apple's not afraid to
say yes, a new iPhone will

00:37:45.700 --> 00:37:47.710
hurt the iPad market.

00:37:47.710 --> 00:37:49.240
But we've got to go for it.

00:37:49.240 --> 00:37:51.070
In fact, Apple was reticent.

00:37:51.070 --> 00:37:52.860
And they didn't
make a big iPhone

00:37:52.860 --> 00:37:57.040
until, as you know quite well,
after Samsung, Google, Androids

00:37:57.040 --> 00:37:59.450
all were coming out.

00:37:59.450 --> 00:38:01.230
But it remained innovative.

00:38:01.230 --> 00:38:02.870
Now obviously, Google does that.

00:38:02.870 --> 00:38:05.790
Google all the time
is shooting the moon.

00:38:05.790 --> 00:38:08.320
Sergey in particular,
doing GoogleGoogle[x].

00:38:08.320 --> 00:38:09.680
And that's really cool.

00:38:09.680 --> 00:38:13.890
I look at more
staid corporations.

00:38:13.890 --> 00:38:17.900
IBM is a really
interesting example.

00:38:17.900 --> 00:38:19.990
It's been around for
more than 100 years.

00:38:19.990 --> 00:38:22.520
So it kind of gets it.

00:38:22.520 --> 00:38:26.240
But you always think they're
about to just jump the shark

00:38:26.240 --> 00:38:29.390
and be over.

00:38:29.390 --> 00:38:32.880
That was the way, back in my
book, when the PC comes along,

00:38:32.880 --> 00:38:35.070
and Apple comes along.

00:38:35.070 --> 00:38:38.970
And then suddenly IBM says, no,
we can actually create a PC.

00:38:38.970 --> 00:38:40.620
And they get away
from headquarters,

00:38:40.620 --> 00:38:42.760
and they do it in
Orlando, and boom.

00:38:42.760 --> 00:38:44.060
IBM PC comes out.

00:38:44.060 --> 00:38:45.660
Not bad.

00:38:45.660 --> 00:38:49.170
Likewise, Xerox decides, we
can't just be a copier company.

00:38:49.170 --> 00:38:50.520
And they create Xerox PARC.

00:38:50.520 --> 00:38:51.950
But they don't capitalize on it.

00:38:51.950 --> 00:38:53.600
They do the graphical
user interface.

00:38:53.600 --> 00:38:55.391
They do all the things
you find in the Mac,

00:38:55.391 --> 00:38:58.260
but they don't-- in the
Xerox Star, and the Alto--

00:38:58.260 --> 00:39:00.470
it just doesn't get it,
and it doesn't work.

00:39:00.470 --> 00:39:05.410
So most big companies
can't innovate well.

00:39:05.410 --> 00:39:10.110
I think it takes a
visionary set of leaders

00:39:10.110 --> 00:39:14.120
who are willing to
break china-- i.e.,

00:39:14.120 --> 00:39:17.445
destroy the iPad to
make the bigger iPhone.

00:39:20.020 --> 00:39:23.040
And you know, I
watch Ginni Rometty,

00:39:23.040 --> 00:39:25.050
and I think OK, this
will be interesting.

00:39:25.050 --> 00:39:28.530
Can she remake
IBM one more time?

00:39:28.530 --> 00:39:32.290
She's doing it a bit to get
back to machine learning

00:39:32.290 --> 00:39:36.910
by taking their cognitive
computing division-- whatever

00:39:36.910 --> 00:39:39.200
you want to call
it-- and making it

00:39:39.200 --> 00:39:41.000
so it's a collaborative
division, where

00:39:41.000 --> 00:39:43.720
cognitive computing
is done with people.

00:39:43.720 --> 00:39:46.100
You take Watson and
pair it with doctors.

00:39:46.100 --> 00:39:49.010
You take Deep Blue
and it plays chess

00:39:49.010 --> 00:39:50.890
better when it
plays in partnership

00:39:50.890 --> 00:39:52.520
with people than
when it-- you know.

00:39:52.520 --> 00:39:56.930
And so I think that she's
trying to do this thing.

00:39:56.930 --> 00:39:59.510
I don't know whether the
Watson division of IBM

00:39:59.510 --> 00:40:02.550
will be its saving grace.

00:40:02.550 --> 00:40:05.030
But I watch industries
that get disrupted,

00:40:05.030 --> 00:40:07.050
and it's usually
because people are

00:40:07.050 --> 00:40:10.471
trying to protect what
they've invented before.

00:40:10.471 --> 00:40:11.012
AUDIENCE: Hi.

00:40:11.012 --> 00:40:13.580
I'm curious about
your relationship

00:40:13.580 --> 00:40:14.490
with your subjects.

00:40:14.490 --> 00:40:17.470
And how do you create a
context in which they're

00:40:17.470 --> 00:40:19.180
going to reveal
themselves to you.

00:40:19.180 --> 00:40:22.324
Are there lessons
learned in your career?

00:40:22.324 --> 00:40:23.740
WALTER ISAACSON:
Yeah, one is just

00:40:23.740 --> 00:40:26.710
a simple one, which is listen.

00:40:26.710 --> 00:40:29.746
And the second is silence
is the best question.

00:40:29.746 --> 00:40:32.370
There were times, whether it was
Henry Kissinger or Steve Jobs,

00:40:32.370 --> 00:40:33.690
I'd say, OK.

00:40:33.690 --> 00:40:34.510
How'd you do this?

00:40:34.510 --> 00:40:36.450
And they'd sort of blow it off.

00:40:36.450 --> 00:40:37.820
I'd just sit there.

00:40:37.820 --> 00:40:39.680
People hate silence.

00:40:39.680 --> 00:40:44.315
Eventually, they will
start talking again.

00:40:44.315 --> 00:40:47.020
I don't fully know--
I mean, I don't

00:40:47.020 --> 00:40:50.620
want to be bragging, because
I know all my weaknesses.

00:40:50.620 --> 00:40:56.226
But I do have a particular
ability at times--

00:40:56.226 --> 00:40:58.600
and I think I learned it as
a police reporter for the New

00:40:58.600 --> 00:41:03.020
Orleans "Times-Picayune"-- to
just go up to people and say,

00:41:03.020 --> 00:41:05.200
tell me the story.

00:41:05.200 --> 00:41:06.140
And you just wait.

00:41:06.140 --> 00:41:07.800
And you say, tell me the story.

00:41:07.800 --> 00:41:08.801
Everybody's got a story.

00:41:08.801 --> 00:41:10.091
And everybody wants to tell it.

00:41:10.091 --> 00:41:11.840
And if you ask them
to do it as a story,

00:41:11.840 --> 00:41:15.250
they're going to
tell you the story.

00:41:15.250 --> 00:41:18.319
I'll tell a story
I haven't told.

00:41:18.319 --> 00:41:20.110
I'm almost embarrassed
to say it in public.

00:41:20.110 --> 00:41:22.350
First day I ever
was a journalist.

00:41:22.350 --> 00:41:25.860
Summer job, "Times-Picayune,"
New Orleans.

00:41:25.860 --> 00:41:28.240
I'm on the 5:00 AM police beat.

00:41:28.240 --> 00:41:30.230
I get sent out.

00:41:30.230 --> 00:41:32.860
A young woman, a
young girl, has been

00:41:32.860 --> 00:41:35.820
killed on Carrollton
Avenue in New Orleans,

00:41:35.820 --> 00:41:38.152
and I'm sent there,
to the crime scene.

00:41:38.152 --> 00:41:41.720
I go, and interview the police.

00:41:41.720 --> 00:41:44.640
And back days before
cellphones or anything else,

00:41:44.640 --> 00:41:46.900
I go to a payphone,
phone in the story.

00:41:46.900 --> 00:41:49.744
And the early morning
rewrite man says,

00:41:49.744 --> 00:41:50.910
did you talk to her parents?

00:41:50.910 --> 00:41:51.970
I said of course not.

00:41:51.970 --> 00:41:53.511
I mean, the parents
are in the house.

00:41:53.511 --> 00:41:56.500
I'm not-- She said go
talk to the parents.

00:41:56.500 --> 00:41:58.390
So I knock on the door.

00:41:58.390 --> 00:42:00.160
And I'm like, holy shit.

00:42:00.160 --> 00:42:02.560
Their daughter just got killed.

00:42:02.560 --> 00:42:04.755
And they talk to
me for a good hour.

00:42:04.755 --> 00:42:06.630
And after I finished,
I call the rewrite man.

00:42:06.630 --> 00:42:09.040
He says, did you get a picture?

00:42:09.040 --> 00:42:09.790
I said, no.

00:42:09.790 --> 00:42:11.600
He said, go back and
get the yearbook.

00:42:11.600 --> 00:42:13.350
And they give me the yearbook.

00:42:13.350 --> 00:42:18.680
So I learned a lesson, which
is just sit there and listen.

00:42:18.680 --> 00:42:21.020
People want to tell
you their story.

00:42:21.020 --> 00:42:24.380
And that has been lost a
little bit in journalism

00:42:24.380 --> 00:42:31.150
these days, which is people
want to engage on opinions.

00:42:31.150 --> 00:42:35.879
But they don't just
say, tell me the story.

00:42:35.879 --> 00:42:36.420
AUDIENCE: Hi.

00:42:36.420 --> 00:42:38.850
Just interested to hear
your top list of people

00:42:38.850 --> 00:42:41.120
you would like to interview
throughout history.

00:42:41.120 --> 00:42:43.136
WALTER ISAACSON: Interview
throughout history.

00:42:43.136 --> 00:42:45.010
Well obviously the people
I've written about.

00:42:45.010 --> 00:42:46.690
I mean, if I could
have a beer tonight,

00:42:46.690 --> 00:42:48.620
it would be with Ben Franklin.

00:42:48.620 --> 00:42:50.030
And I think all of us would.

00:42:50.030 --> 00:42:52.600
I mean, he would be
totally blown away

00:42:52.600 --> 00:42:56.640
by the question of whether an
open Android system or a closed

00:42:56.640 --> 00:42:58.670
integrated system of
hardware and software

00:42:58.670 --> 00:43:00.330
made the most sense.

00:43:00.330 --> 00:43:03.930
He would love every-- I mean,
he invented more devices

00:43:03.930 --> 00:43:06.710
than anybody in this room has
ever thought of inventing.

00:43:06.710 --> 00:43:09.730
He invented gadgets to take
books down from shelves.

00:43:09.730 --> 00:43:11.110
And he invented the pedometer.

00:43:11.110 --> 00:43:15.940
He invented, obviously,
bifocals and lightning rods.

00:43:15.940 --> 00:43:18.290
So this guy is the
most inventive person,

00:43:18.290 --> 00:43:21.450
and he loves and
embraces technology.

00:43:21.450 --> 00:43:24.620
I also like him because,
unlike Lord Byron, who

00:43:24.620 --> 00:43:29.530
was a Luddite, as I
said, he's an optimist.

00:43:29.530 --> 00:43:33.650
One of the weird things I get
in this book-- in interviews

00:43:33.650 --> 00:43:36.360
about it all the time--
is, isn't technology bad?

00:43:36.360 --> 00:43:37.850
Isn't it hurting us?

00:43:37.850 --> 00:43:39.920
Isn't the NSA now spying on us?

00:43:39.920 --> 00:43:41.190
Isn't life horrible?

00:43:41.190 --> 00:43:45.130
Isn't Google, you know, keeping
track-- I go, wait a minute.

00:43:45.130 --> 00:43:49.540
We own this technology.

00:43:49.540 --> 00:43:52.450
And isn't it putting
us all out of work?

00:43:52.450 --> 00:43:52.950
Well no.

00:43:52.950 --> 00:43:54.380
Show me the data points.

00:43:54.380 --> 00:43:57.330
It's creating-- as it did in
the Industrial Revolution,

00:43:57.330 --> 00:43:59.230
despite what Lord
Byron thought--

00:43:59.230 --> 00:44:02.320
it's creating all
sorts of new economies.

00:44:02.320 --> 00:44:05.500
So I'm very optimistic.

00:44:05.500 --> 00:44:10.060
And Ben Franklin was the
most optimistic innovator

00:44:10.060 --> 00:44:11.940
I've ever written about.

00:44:11.940 --> 00:44:15.050
He loved the concept
that's at the heart

00:44:15.050 --> 00:44:17.070
of the digital
revolution, which is

00:44:17.070 --> 00:44:20.590
that the free flow of
peer-to-peer information

00:44:20.590 --> 00:44:24.160
will be the most empowering
way to create a new society,

00:44:24.160 --> 00:44:27.930
to create a democracy,
to transform the world.

00:44:27.930 --> 00:44:29.750
That's what you do at Google.

00:44:29.750 --> 00:44:31.530
That's what this
digital revolution

00:44:31.530 --> 00:44:34.500
has been about-- personal
computers, distributed

00:44:34.500 --> 00:44:37.170
networks, peer-to-peer.

00:44:37.170 --> 00:44:41.740
And I would love, not
to go back and have

00:44:41.740 --> 00:44:44.590
dinner with Ben Franklin, but
to bring him forward and put him

00:44:44.590 --> 00:44:49.180
here, and to let him walk, and
to fit him with Google Glass.

00:44:49.180 --> 00:44:50.845
I mean shit!

00:44:50.845 --> 00:44:53.940
That would be so cool.

00:44:53.940 --> 00:44:56.600
Yes.

00:44:56.600 --> 00:44:57.230
AUDIENCE: Hi.

00:44:57.230 --> 00:44:58.775
Thanks for coming over.

00:44:58.775 --> 00:45:01.210
So I read your book
about Steve Jobs,

00:45:01.210 --> 00:45:03.870
and it was fascinating because
he was a very interesting

00:45:03.870 --> 00:45:07.900
character, and I enjoyed
learning about him

00:45:07.900 --> 00:45:10.160
from your perspective.

00:45:10.160 --> 00:45:13.820
The one thing I want to ask
about your memory with Steve

00:45:13.820 --> 00:45:15.490
Jobs, maybe that
wasn't in the book,

00:45:15.490 --> 00:45:18.910
while you were working with
him, that you can share with us.

00:45:18.910 --> 00:45:20.736
WALTER ISAACSON: Memories?

00:45:20.736 --> 00:45:22.236
Yeah, I mean, there's
a particular--

00:45:22.236 --> 00:45:24.750
and I tried to convey
it in the book.

00:45:24.750 --> 00:45:26.870
But the most striking
thing about him

00:45:26.870 --> 00:45:29.790
was how emotional he was.

00:45:29.790 --> 00:45:31.710
I mean, he would cry at times.

00:45:31.710 --> 00:45:33.850
He'd get worked up.

00:45:33.850 --> 00:45:38.170
And that intensity of emotion,
I think-- somebody asked me,

00:45:38.170 --> 00:45:41.860
what are the keys to-- I think
you've got to be passionate.

00:45:41.860 --> 00:45:45.840
But more than just passionate,
you've got to be emotional.

00:45:45.840 --> 00:45:52.200
Early on in the interviewing
process with him,

00:45:52.200 --> 00:45:55.030
I asked him what
makes a good-- we

00:45:55.030 --> 00:45:58.140
got into this you got to be
a rebel, question authority

00:45:58.140 --> 00:45:59.590
thing that I said earlier.

00:45:59.590 --> 00:46:01.970
And he said, but you
know my manifesto.

00:46:01.970 --> 00:46:04.130
And it was the
"Think Different" ad

00:46:04.130 --> 00:46:07.830
from 1999 or so, which
you're too young to remember.

00:46:07.830 --> 00:46:10.670
But it was this awesome ad
that had a print campaign which

00:46:10.670 --> 00:46:13.310
was Gandhi and Einstein and
it just said, think different.

00:46:13.310 --> 00:46:15.981
But there was a TV ad
that Steve helped write.

00:46:15.981 --> 00:46:17.480
So he's sitting
there in his garden,

00:46:17.480 --> 00:46:20.340
and he recites the
entire 60-second ad.

00:46:20.340 --> 00:46:23.010
"Here's to the crazy ones,
the misfits, the rebels,

00:46:23.010 --> 00:46:25.710
the round pegs in
the square holes."

00:46:25.710 --> 00:46:27.077
And he goes on.

00:46:27.077 --> 00:46:29.410
And then he gets to the end,
and you know, "we at Apple,

00:46:29.410 --> 00:46:32.140
we celebrate them, because the
people who are crazy enough

00:46:32.140 --> 00:46:35.590
to think they can change the
world are the ones who do."

00:46:35.590 --> 00:46:38.130
And I got a little
choked up just saying it,

00:46:38.130 --> 00:46:40.090
because I remember
when he said it to me.

00:46:40.090 --> 00:46:42.110
By the end, he's crying.

00:46:42.110 --> 00:46:42.810
He's choked up.

00:46:42.810 --> 00:46:44.520
And I'm going, what
just happened here?

00:46:44.520 --> 00:46:47.000
Did he get something in his eye?

00:46:47.000 --> 00:46:48.660
We were sitting in the garden.

00:46:48.660 --> 00:46:50.372
And he said, you just
have to excuse me.

00:46:50.372 --> 00:46:53.500
There are things that
make me so emotional,

00:46:53.500 --> 00:46:56.720
because they're so beautiful
to me, and so meaningful to me,

00:46:56.720 --> 00:46:58.520
that I get choked up.

00:46:58.520 --> 00:46:59.430
And I go OK.

00:46:59.430 --> 00:47:02.430
That's why he's a bit
different from you and me.

00:47:02.430 --> 00:47:04.310
But that's why he was Steve.

00:47:07.070 --> 00:47:07.750
AUDIENCE: Great.

00:47:07.750 --> 00:47:11.230
Walter, you seem so
optimistic about the future,

00:47:11.230 --> 00:47:13.900
about this digital age,
compared to other people

00:47:13.900 --> 00:47:17.697
from what we would
call old media.

00:47:17.697 --> 00:47:20.280
WALTER ISAACSON: That's because
I got out in the nick of time,

00:47:20.280 --> 00:47:23.784
one step ahead of
the [? disruptors. ?]

00:47:23.784 --> 00:47:26.200
AUDIENCE: And you alluded a
bit to the art of storytelling

00:47:26.200 --> 00:47:28.720
and listening being lost
from journalism today.

00:47:28.720 --> 00:47:30.390
I mean, what would
your advice be

00:47:30.390 --> 00:47:33.810
to those who continue-- it was
just on CNBC this morning--

00:47:33.810 --> 00:47:36.780
hand wringing over the
demise of journalism,

00:47:36.780 --> 00:47:37.614
the demise of media.

00:47:37.614 --> 00:47:39.113
WALTER ISAACSON:
Actually, they were

00:47:39.113 --> 00:47:40.690
trying to get me to hand wring.

00:47:40.690 --> 00:47:41.919
I was on CNBC yesterday.

00:47:41.919 --> 00:47:42.710
I was on Bloomberg.

00:47:42.710 --> 00:47:45.000
Ah, journalism, demise, demise.

00:47:45.000 --> 00:47:46.240
There are a couple things.

00:47:46.240 --> 00:47:49.350
Journalism is not
in demise at all.

00:47:49.350 --> 00:47:53.010
This is the best era
ever for journalism.

00:47:53.010 --> 00:47:55.750
There's journalists all over
the world doing amazing things.

00:47:55.750 --> 00:47:58.720
This is a very bad
period for the business

00:47:58.720 --> 00:48:00.440
model for journalism.

00:48:00.440 --> 00:48:08.300
Which is different--
I mean, it's related.

00:48:08.300 --> 00:48:10.060
And the business
model got screwed up.

00:48:10.060 --> 00:48:11.500
And I was one of the
people who screwed it up.

00:48:11.500 --> 00:48:13.740
I was in charge of digital
media for Time, Inc.

00:48:13.740 --> 00:48:15.490
And right when the
web came along,

00:48:15.490 --> 00:48:18.000
we were being paid
$1 million a year

00:48:18.000 --> 00:48:20.670
for AOL and CompuServe and
others to put up our news

00:48:20.670 --> 00:48:21.837
and run the bulletin boards.

00:48:21.837 --> 00:48:23.753
And the web comes along,
and it's like, great.

00:48:23.753 --> 00:48:24.820
We can publish our own.

00:48:24.820 --> 00:48:26.440
We don't need AOL.

00:48:26.440 --> 00:48:29.840
And we were going to do exactly
what we did with AOL, which

00:48:29.840 --> 00:48:32.810
is put up advertising,
and we created banner ads,

00:48:32.810 --> 00:48:36.170
and charge the user a
certain amount-- metered,

00:48:36.170 --> 00:48:38.640
or whatever it would be.

00:48:38.640 --> 00:48:41.660
From Madison Avenue, you could
look out of the Time Life

00:48:41.660 --> 00:48:44.220
building, and people were
carrying bags of money.

00:48:44.220 --> 00:48:45.680
They want to buy
ads, because they

00:48:45.680 --> 00:48:47.430
want to get with
it, this new medium.

00:48:47.430 --> 00:48:49.529
So we ended up not
charging for content,

00:48:49.529 --> 00:48:51.320
not trying to get
subscriptions, not having

00:48:51.320 --> 00:48:54.550
a micropayment system,
which Ted Nelson

00:48:54.550 --> 00:48:59.080
wanted to put in the hypertext,
which had been discussed

00:48:59.080 --> 00:49:02.290
at the Web Consortium
by Tim Berners-Lee.

00:49:02.290 --> 00:49:04.570
And everybody said, oh,
it wants to be free.

00:49:04.570 --> 00:49:06.070
And you don't get it.

00:49:06.070 --> 00:49:08.270
You're clueless if you
don't make everything free,

00:49:08.270 --> 00:49:13.070
because we can all do-- there is
no way advertising will support

00:49:13.070 --> 00:49:14.860
great journalism alone.

00:49:14.860 --> 00:49:17.540
Even Henry Luce
70 years ago said,

00:49:17.540 --> 00:49:21.760
if you're depending on what
he called giveaway journalism

00:49:21.760 --> 00:49:23.900
to collect eyeballs
for advertisers,

00:49:23.900 --> 00:49:26.750
it's not only morally
abhorrent, it's

00:49:26.750 --> 00:49:28.250
economically self-defeating.

00:49:28.250 --> 00:49:32.100
I think the latter
part was worse to him.

00:49:32.100 --> 00:49:33.230
And it's true.

00:49:33.230 --> 00:49:36.980
There are always going to
be more websites created

00:49:36.980 --> 00:49:40.020
than ad dollars added
in any given year.

00:49:40.020 --> 00:49:42.720
Chevrolet's only going to
launch a certain number of cars.

00:49:42.720 --> 00:49:44.960
And even with all
sorts of new products,

00:49:44.960 --> 00:49:49.640
ad dollars go up at
best on a small slope,

00:49:49.640 --> 00:49:54.170
whereas the number of web
avails goes up exponentially.

00:49:54.170 --> 00:49:55.960
So that was a bad
business model.

00:49:55.960 --> 00:49:58.510
And it's got a lot
of people screwed up.

00:49:58.510 --> 00:50:01.640
I think you could get
back to a good business

00:50:01.640 --> 00:50:02.840
model in some ways.

00:50:02.840 --> 00:50:05.190
First of all, the
genie is slowly

00:50:05.190 --> 00:50:07.410
being put back in the bottle.

00:50:07.410 --> 00:50:09.000
I subscribe to "Wall
Street Journal,"

00:50:09.000 --> 00:50:10.550
I subscribe to
"The Economist," I

00:50:10.550 --> 00:50:12.700
subscribe to the "New
York Times," or whatever.

00:50:12.700 --> 00:50:14.950
And believe it or
not, 800,000 people

00:50:14.950 --> 00:50:19.470
pay serious money to subscribe
to the "New York Times."

00:50:19.470 --> 00:50:22.835
I don't think subscriptions
is a great idea.

00:50:22.835 --> 00:50:24.550
I mean that's fine,
but I don't think

00:50:24.550 --> 00:50:26.010
it's the way for all journalism.

00:50:26.010 --> 00:50:28.120
I don't think bloggers
can get subscribers.

00:50:28.120 --> 00:50:30.611
I don't think people who
write musicals or LARPs

00:50:30.611 --> 00:50:32.360
or whatever are going
to have subscribers.

00:50:32.360 --> 00:50:35.640
I think you want to be able
to pay per drink, or newsstand

00:50:35.640 --> 00:50:36.630
pay.

00:50:36.630 --> 00:50:39.820
And our financial system
is so [? undisrupted, ?]

00:50:39.820 --> 00:50:40.649
and so screwed up.

00:50:40.649 --> 00:50:42.440
If there's something
I'm pessimistic about,

00:50:42.440 --> 00:50:44.250
it's the banking system.

00:50:44.250 --> 00:50:50.585
I mean, if I'm trying to
give Betsy, my daughter, $50,

00:50:50.585 --> 00:50:52.840
sort of Popmoney kind of works.

00:50:52.840 --> 00:50:55.890
I end up with an Akimbo
Card because I can put money

00:50:55.890 --> 00:50:57.750
on an Akimbo Card
and designate it

00:50:57.750 --> 00:50:59.540
to whoever else has
that Akimbo Card.

00:50:59.540 --> 00:51:01.190
But I mean, this is nutty.

00:51:01.190 --> 00:51:06.810
So Bitcoin, I think, and other
cyber- and crypto-currencies

00:51:06.810 --> 00:51:09.184
could be the saving
grace, which is

00:51:09.184 --> 00:51:10.600
you don't need the
banking system.

00:51:10.600 --> 00:51:13.950
You don't need Paypal's
clunky kludgy interface.

00:51:13.950 --> 00:51:16.570
You don't need Visa
card passwords.

00:51:16.570 --> 00:51:18.680
You just say, oh.

00:51:18.680 --> 00:51:21.740
That article looks
really interesting.

00:51:21.740 --> 00:51:24.850
It's $0.25.

00:51:24.850 --> 00:51:27.620
My browser will have
bitcoins or something in it.

00:51:27.620 --> 00:51:30.930
There's about eight new
companies popping up doing it.

00:51:30.930 --> 00:51:32.980
I don't even have
to authorize it.

00:51:32.980 --> 00:51:34.580
I can sort of set a level.

00:51:34.580 --> 00:51:38.120
Like if it asks me for
anything less than $5,

00:51:38.120 --> 00:51:41.990
don't even ask, just
if I click on it,

00:51:41.990 --> 00:51:44.630
send out the cyber-currency.

00:51:44.630 --> 00:51:47.770
That will bring back a
golden age of journalism,

00:51:47.770 --> 00:51:51.580
not simply because it would
bring back a revenue stream,

00:51:51.580 --> 00:51:54.670
but because it will
do something to get

00:51:54.670 --> 00:51:58.640
to the morally abhorrent part
of Henry Luce's quote-- which

00:51:58.640 --> 00:52:04.150
is if our goal is mainly
to aggregate eyeballs

00:52:04.150 --> 00:52:06.760
for advertisers
and marketers, then

00:52:06.760 --> 00:52:09.780
we will produce clickbait
that can do that.

00:52:09.780 --> 00:52:13.890
If our goal is to be beholden
to giving something of value

00:52:13.890 --> 00:52:17.100
to the reader that that
reader or user or viewer

00:52:17.100 --> 00:52:18.720
is not going to get
somewhere else--

00:52:18.720 --> 00:52:22.810
that isn't just a commodity,
headlines, or clickbait,

00:52:22.810 --> 00:52:26.940
but something that's a true
story that they don't have

00:52:26.940 --> 00:52:33.080
elsewhere-- the only way that
that becomes economically

00:52:33.080 --> 00:52:37.350
feasible is if you
are incented every day

00:52:37.350 --> 00:52:40.900
to produce something of such
value that the person will say,

00:52:40.900 --> 00:52:41.650
$0.25?

00:52:41.650 --> 00:52:45.400
Fine, fine-- and click on it.

00:52:45.400 --> 00:52:49.360
And that will give us the
non-perverse incentive

00:52:49.360 --> 00:52:52.890
to avoid just doing
clickbait and saying,

00:52:52.890 --> 00:52:56.700
I'm going to do something
that somebody will value.

00:52:56.700 --> 00:52:59.710
And the only way to know
somebody values something

00:52:59.710 --> 00:53:01.210
is that they will
put a value on it,

00:53:01.210 --> 00:53:02.830
that they will pay
something for it.

00:53:02.830 --> 00:53:05.192
So I have this argument
with my daughter, who

00:53:05.192 --> 00:53:06.650
believes everything
should be free.

00:53:06.650 --> 00:53:09.120
And I explained to her,
since she's a writer,

00:53:09.120 --> 00:53:11.230
and since she's doing a
book or whatever, what

00:53:11.230 --> 00:53:14.710
do you think-- you know,
how does this work?

00:53:14.710 --> 00:53:17.170
And don't you want
to be incented

00:53:17.170 --> 00:53:21.150
to provide something so valuable
that it's at least worth

00:53:21.150 --> 00:53:24.090
a buck to somebody
who'll click on it?

00:53:24.090 --> 00:53:25.950
And that will help.

00:53:25.950 --> 00:53:29.630
And two other tech-- I'm
sorry to go deep into this,

00:53:29.630 --> 00:53:32.400
but I worry about it-- two
other technological things.

00:53:32.400 --> 00:53:35.090
I do think the internet and
the digital age in general

00:53:35.090 --> 00:53:36.620
tends to Balkanize.

00:53:36.620 --> 00:53:39.100
It used to be, I'm old
enough to remember growing up

00:53:39.100 --> 00:53:41.250
when Walter Cronkite said,
that's the way it is.

00:53:41.250 --> 00:53:43.780
Because you could not
start a TV network.

00:53:43.780 --> 00:53:46.310
I mean, maybe Sarnoff did
when there were two of them

00:53:46.310 --> 00:53:47.500
and he started the third.

00:53:47.500 --> 00:53:49.380
But after that,
it's kind of hard,

00:53:49.380 --> 00:53:53.200
before cable and digital comes
along to start a new network.

00:53:53.200 --> 00:53:57.129
So you had to
gather a broad base.

00:53:57.129 --> 00:53:59.170
You had to get a third of
the audience, at least,

00:53:59.170 --> 00:54:00.270
or you were failing.

00:54:00.270 --> 00:54:03.970
Nowadays, to win in
the digital world--

00:54:03.970 --> 00:54:10.280
whether you're Fox Cable News or
a web-- "Talking Points Memo"--

00:54:10.280 --> 00:54:13.500
you have to get like 1% of the
audience and be passionate,

00:54:13.500 --> 00:54:14.450
and you win.

00:54:14.450 --> 00:54:18.590
So it causes people to go for
passionate niche audiences,

00:54:18.590 --> 00:54:20.990
rather than the
mass audience, which

00:54:20.990 --> 00:54:24.910
allows it to be somewhat more
politically and ideologically

00:54:24.910 --> 00:54:26.080
Balkanized.

00:54:26.080 --> 00:54:29.390
And secondly, the
web inherently is

00:54:29.390 --> 00:54:34.130
better for information gathering
than it is for narrative.

00:54:34.130 --> 00:54:37.540
Narrative, especially
long-form narrative-- and I

00:54:37.540 --> 00:54:40.590
don't mean this because
technology makes us ADD.

00:54:40.590 --> 00:54:45.100
I just mean when you're on the
web and you're clicking around,

00:54:45.100 --> 00:54:47.580
that's because you're
putting together the thing.

00:54:47.580 --> 00:54:49.710
But if you want somebody
to tell you a whole story,

00:54:49.710 --> 00:54:53.830
if you want that "New
Yorker" essay on you know,

00:54:53.830 --> 00:54:57.760
Remnick in Moscow on Putin,
that demands-- and you

00:54:57.760 --> 00:54:58.980
can do it online.

00:54:58.980 --> 00:55:02.300
It's not like print
is better than online.

00:55:02.300 --> 00:55:08.190
But the web has all sorts of
landmines which we call links.

00:55:08.190 --> 00:55:10.140
And you know, you're
into the third paragraph

00:55:10.140 --> 00:55:11.848
and all of the sudden
you're floating off

00:55:11.848 --> 00:55:14.660
and looking at something else.

00:55:14.660 --> 00:55:18.330
So I think sustained
narrative is something

00:55:18.330 --> 00:55:20.980
we have to try to bring
back to journalism, which

00:55:20.980 --> 00:55:23.380
is, as I said, the
simplest six words.

00:55:23.380 --> 00:55:25.795
Let me tell you a story.

00:55:25.795 --> 00:55:27.670
AUDIENCE: Hopefully a
quick closing question.

00:55:27.670 --> 00:55:29.070
WALTER ISAACSON:
Yeah, it's 12:59:27.

00:55:29.070 --> 00:55:29.280
AUDIENCE: Yeah.

00:55:29.280 --> 00:55:30.071
I'll go real quick.

00:55:30.071 --> 00:55:31.707
As a fan of your
work, just curious

00:55:31.707 --> 00:55:33.790
if you know what your next
book or your next topic

00:55:33.790 --> 00:55:34.810
is going to be.

00:55:34.810 --> 00:55:38.270
WALTER ISAACSON: Yeah,
the ultimate connection

00:55:38.270 --> 00:55:42.240
between art and technology.

00:55:42.240 --> 00:55:44.460
Does anybody have
a copy of my book?

00:55:44.460 --> 00:55:46.640
Do you have it?

00:55:46.640 --> 00:55:48.630
I thought I saw
somebody with it.

00:55:48.630 --> 00:55:49.280
Yeah.

00:55:49.280 --> 00:55:53.360
If you'll open to the very
last page of the narrative,

00:55:53.360 --> 00:55:57.290
I don't announce it,
but I give a hint.

00:55:57.290 --> 00:55:59.490
Because if you had to
pick the ultimate symbol

00:55:59.490 --> 00:56:02.660
of the connection of the
sciences to the humanities,

00:56:02.660 --> 00:56:06.000
it would be "Vitruvian
Man," the amazing drawing.

00:56:06.000 --> 00:56:09.020
And nobody has
fully done Leonardo

00:56:09.020 --> 00:56:11.880
with the science
connected to the art.

00:56:11.880 --> 00:56:13.714
And I want to go back
in the WayBack Machine

00:56:13.714 --> 00:56:15.546
so I don't have to deal
with people anymore.

00:56:15.546 --> 00:56:16.860
And I want to go to Florence.

00:56:16.860 --> 00:56:19.550
So I'm going to go do Leonardo.

00:56:19.550 --> 00:56:20.050
Thank you.

00:56:20.050 --> 00:56:21.600
[APPLAUSE]

