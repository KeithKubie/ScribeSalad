WEBVTT
Kind: captions
Language: en

00:00:04.455 --> 00:00:05.750
MALE SPEAKER: Welcome.

00:00:05.750 --> 00:00:07.620
Let's start with a pop quiz.

00:00:08.840 --> 00:00:13.650
What do Benjamin Franklin,
Karl Marx, and the philosopher

00:00:13.650 --> 00:00:15.210
Hannah Arendt have in common?

00:00:16.570 --> 00:00:17.070
Anybody?

00:00:19.010 --> 00:00:22.880
So they all proposed this
notion of calling humans

00:00:22.880 --> 00:00:24.860
Homo faber-- man, the toolmaker.

00:00:26.350 --> 00:00:27.950
So we make tools.

00:00:27.950 --> 00:00:28.820
We make tools.

00:00:28.820 --> 00:00:33.040
We alter the environment,
and then the tools alter us.

00:00:33.040 --> 00:00:35.720
And sometimes we lament that.

00:00:35.720 --> 00:00:40.120
And sometimes these tools
have big effects-- clothing,

00:00:40.120 --> 00:00:44.626
cooking, fire, automobiles,
computers, and so on.

00:00:44.626 --> 00:00:46.125
Sometimes they have
smaller effects.

00:00:47.055 --> 00:00:48.430
But it looks like
right now we're

00:00:48.430 --> 00:00:50.310
in a period where
we're going to start

00:00:50.310 --> 00:00:51.860
using more and more tools.

00:00:51.860 --> 00:00:54.890
They're going to be ubiquitous
throughout our life.

00:00:54.890 --> 00:00:58.230
And our speaker
today, Nicholas Carr,

00:00:58.230 --> 00:01:01.160
has taken upon himself
to investigate this.

00:01:01.160 --> 00:01:02.960
How do these tools change us?

00:01:05.080 --> 00:01:07.260
What's for the better,
what's for the worse,

00:01:07.260 --> 00:01:09.730
and can we figure out
a way to design them

00:01:09.730 --> 00:01:11.940
so that we'll live
better with them?

00:01:11.940 --> 00:01:13.581
Welcome to Google,
Nicholas Carr.

00:01:13.581 --> 00:01:14.080
[APPLAUSE]

00:01:14.080 --> 00:01:15.121
NICHOLAS CARR: Thank you.

00:01:17.215 --> 00:01:17.714
Thank you.

00:01:19.050 --> 00:01:20.050
Thanks very much, Peter.

00:01:21.390 --> 00:01:26.250
And thanks to [? Anne ?]
Farmer for shepherding me

00:01:26.250 --> 00:01:28.619
through the process
and bringing me here.

00:01:28.619 --> 00:01:30.535
And thanks to Google for
hosting these events.

00:01:32.110 --> 00:01:34.550
I've been to a couple
of other Google offices,

00:01:34.550 --> 00:01:37.760
but this is the first time
I've been to the headquarters.

00:01:37.760 --> 00:01:39.440
So it's exciting.

00:01:39.440 --> 00:01:42.020
The Googleplex has kind of
played a role in my fantasy

00:01:42.020 --> 00:01:44.010
life for a long
time, I realized.

00:01:44.010 --> 00:01:47.600
Not a weird role; it's kind of a
dull fantasy life, but what can

00:01:47.600 --> 00:01:48.340
I say?

00:01:48.340 --> 00:01:50.885
So it's good to
be here in person.

00:01:52.100 --> 00:01:55.260
I started writing
about technology

00:01:55.260 --> 00:01:59.370
about 15 years ago
or so, more or less

00:01:59.370 --> 00:02:02.650
the same time that Google
appeared on the scene.

00:02:02.650 --> 00:02:06.660
And I think it was
good timing for Google.

00:02:06.660 --> 00:02:08.610
And it was also
good timing for me,

00:02:08.610 --> 00:02:12.200
because there's been plenty,
obviously, to write about.

00:02:12.200 --> 00:02:15.650
And like, I think, most
technology writers, I

00:02:15.650 --> 00:02:21.560
started off writing about the
technology itself-- features,

00:02:21.560 --> 00:02:24.100
design, stuff like
that-- and also about

00:02:24.100 --> 00:02:27.520
kind of the economic and
financial side of the business,

00:02:27.520 --> 00:02:30.810
so competition between
technology companies

00:02:30.810 --> 00:02:32.020
and so forth.

00:02:32.020 --> 00:02:36.200
But over the years, I
became kind of frustrated

00:02:36.200 --> 00:02:40.620
by what I saw as the narrowness
of that view, that just looks

00:02:40.620 --> 00:02:45.260
at technology as technology
or as a economic factor.

00:02:45.260 --> 00:02:47.080
Because what was
becoming clearer

00:02:47.080 --> 00:02:52.170
was that computers, as they
became smaller and smaller

00:02:52.170 --> 00:02:54.560
and more powerful
and more connected,

00:02:54.560 --> 00:02:58.950
and as programmers became
more adept at their work,

00:02:58.950 --> 00:03:04.210
computing, computation digital
connectivity and everything

00:03:04.210 --> 00:03:08.270
was infusing more and more
aspects of everybody's life--

00:03:08.270 --> 00:03:11.425
at work, during
their leisure time.

00:03:12.610 --> 00:03:16.510
And so it struck me
that, as is always true,

00:03:16.510 --> 00:03:18.090
and as Peter said,
with technology

00:03:18.090 --> 00:03:21.760
we kind of-- technology
frames, in many ways,

00:03:21.760 --> 00:03:23.160
the context in which we live.

00:03:24.209 --> 00:03:25.750
And it seemed to me
important to look

00:03:25.750 --> 00:03:29.150
at this phenomenon, the
rise of the computer as kind

00:03:29.150 --> 00:03:31.110
of a central component
of our lives,

00:03:31.110 --> 00:03:32.360
from many different angles.

00:03:32.360 --> 00:03:35.180
So, see what
sociology could tell

00:03:35.180 --> 00:03:39.810
us, what philosophy
could tell us,

00:03:39.810 --> 00:03:42.080
and all these different
ways we can approach

00:03:42.080 --> 00:03:45.250
an important phenomenon
that's influencing our life.

00:03:47.020 --> 00:03:49.520
So four or five
years ago I wrote

00:03:49.520 --> 00:03:52.070
a book called "The
Shallows" that examined

00:03:52.070 --> 00:03:56.760
how the use of the internet
as an informational medium

00:03:56.760 --> 00:04:02.250
is influencing the way we think,
and how we're adapting to this,

00:04:02.250 --> 00:04:05.430
not only availability of
vast amounts of information,

00:04:05.430 --> 00:04:09.722
but more and more an actual
active barrage of it,

00:04:09.722 --> 00:04:11.180
and what that meant
for our ability

00:04:11.180 --> 00:04:13.520
to tune out the flow
when we needed to,

00:04:13.520 --> 00:04:17.320
and really engage
attentively in one task,

00:04:17.320 --> 00:04:19.029
or one train of thought.

00:04:19.029 --> 00:04:23.190
And as I was writing
"The Shallows,"

00:04:23.190 --> 00:04:28.090
I also started becoming aware
of this other realm of research

00:04:28.090 --> 00:04:31.920
into computers that struck me
as dealing with an even broader

00:04:31.920 --> 00:04:35.950
question, which is what happens
to people and their talents

00:04:35.950 --> 00:04:38.000
and their engagement
with the world

00:04:38.000 --> 00:04:41.360
when they become
reliant on computers

00:04:41.360 --> 00:04:44.125
in their various forms to
do more and more things?

00:04:45.220 --> 00:04:48.760
So what happens when we automate
not just factory work, but lots

00:04:48.760 --> 00:04:51.370
of white-collar,
professional thinking,

00:04:51.370 --> 00:04:54.830
and what happens when we begin
to automate a lot of just

00:04:54.830 --> 00:04:56.770
the day to day
activities that we do?

00:04:56.770 --> 00:05:00.526
We've become more and
more reliant on computers,

00:05:00.526 --> 00:05:02.400
not necessarily to take
over all of the work,

00:05:02.400 --> 00:05:07.030
but to become our aid to help
shepherd us through our days.

00:05:07.030 --> 00:05:10.280
And that was the spark
that led to "The Glass

00:05:10.280 --> 00:05:14.940
Cage," my new book, which
tries to look broadly

00:05:14.940 --> 00:05:19.170
at the repercussions
of our dependence

00:05:19.170 --> 00:05:22.090
on computers and
automation in general,

00:05:22.090 --> 00:05:23.980
but also looks at
the question of,

00:05:23.980 --> 00:05:27.060
are we designing this stuff
in an optimal fashion?

00:05:28.460 --> 00:05:33.300
If we want a world in which we
get the benefits of computers

00:05:33.300 --> 00:05:38.700
but we also want people to live
full, meaningful lives; develop

00:05:38.700 --> 00:05:42.420
rich talents; interact with
the world in diverse ways,

00:05:42.420 --> 00:05:48.320
are we designing all of
these tools-- everything

00:05:48.320 --> 00:05:52.100
from robots to simple
smartphone apps-- in a way that

00:05:52.100 --> 00:05:54.500
accomplishes both those things?

00:05:54.500 --> 00:05:58.550
And what I'd like to do is
just read a short section

00:05:58.550 --> 00:06:01.220
from the book that,
to me, provides

00:06:01.220 --> 00:06:04.421
both an example of a lot of
the things I'm talking a lot,

00:06:04.421 --> 00:06:05.920
a lot of tensions
I'm talking about,

00:06:05.920 --> 00:06:09.000
but also provides sort of
a metaphor for, I think,

00:06:09.000 --> 00:06:12.550
the circumstances we're in
and the challenges we face.

00:06:12.550 --> 00:06:17.270
And this section, which comes
in the middle of the book,

00:06:17.270 --> 00:06:21.470
is about the use of computers
and automation, not in a city

00:06:21.470 --> 00:06:26.050
or even in a kind
of Western country,

00:06:26.050 --> 00:06:28.920
where there's tons of
it, but in a place that

00:06:28.920 --> 00:06:32.460
looks like this-- up
in the Arctic Circle,

00:06:32.460 --> 00:06:35.910
far, far away, where
you might think

00:06:35.910 --> 00:06:40.830
is shielded from computers and
automation but in fact is not.

00:06:40.830 --> 00:06:43.190
So let me just read this to you.

00:06:45.350 --> 00:06:48.250
"The small island of
Igloolik, lying off

00:06:48.250 --> 00:06:50.280
the coast of the
Melville peninsula

00:06:50.280 --> 00:06:53.550
in the Nunavut territory
of the Canadian north,

00:06:53.550 --> 00:06:56.490
is a bewildering
place in the winter.

00:06:56.490 --> 00:07:00.480
The average temperature hovers
around 20 degrees below zero.

00:07:00.480 --> 00:07:04.090
Thick sheets of sea ice
cover the surrounding waters.

00:07:04.090 --> 00:07:05.120
The sun is absent.

00:07:06.200 --> 00:07:08.810
Despite the brutal
conditions, Inuit hunters

00:07:08.810 --> 00:07:12.410
have for some 4,000 years
ventured out from their homes

00:07:12.410 --> 00:07:15.820
on the island and traversed
miles of ice and tundra

00:07:15.820 --> 00:07:18.410
in search of caribou
and other game.

00:07:18.410 --> 00:07:21.870
The hunters' ability to
navigate vast stretches

00:07:21.870 --> 00:07:25.420
of barren, Arctic terrain,
where landmarks are few,

00:07:25.420 --> 00:07:28.110
snow formations are
in constant flux,

00:07:28.110 --> 00:07:30.610
and trails disappear
overnight, has

00:07:30.610 --> 00:07:33.405
amazed voyagers and
scientists for centuries.

00:07:34.670 --> 00:07:37.450
The Inuits' extraordinary
wayfinding skills

00:07:37.450 --> 00:07:40.750
are born not of technological
prowess-- they've

00:07:40.750 --> 00:07:44.070
eschewed maps, compasses,
and other instruments--

00:07:44.070 --> 00:07:47.630
but of a profound understanding
of winds, snow drift

00:07:47.630 --> 00:07:51.685
patterns, animal behavior,
stars, tides, and currents.

00:07:51.685 --> 00:07:53.795
The Inuit are masters
of perception.

00:07:55.270 --> 00:07:57.200
Or at least they used to be.

00:07:57.200 --> 00:07:59.020
Something changed
in Inuit culture

00:07:59.020 --> 00:08:00.890
at the turn of the millennium.

00:08:00.890 --> 00:08:02.930
In the year 2000,
the US government

00:08:02.930 --> 00:08:05.720
lifted many of the restrictions
on the civilian use

00:08:05.720 --> 00:08:07.135
of the global
positioning system.

00:08:08.250 --> 00:08:11.460
The Igloolik hunters, who had
already swapped their dog sleds

00:08:11.460 --> 00:08:15.290
for snowmobiles, began to rely
on computer-generated maps

00:08:15.290 --> 00:08:17.390
and directions to get around.

00:08:17.390 --> 00:08:19.730
Younger Inuit were
particularly eager to use

00:08:19.730 --> 00:08:21.270
the new technology.

00:08:21.270 --> 00:08:24.880
In the past, a young hunter had
to endure a long apprenticeship

00:08:24.880 --> 00:08:27.130
with his elders,
developing his wayfinding

00:08:27.130 --> 00:08:29.700
talents over many years.

00:08:29.700 --> 00:08:32.220
By purchasing a
cheap GPS receiver,

00:08:32.220 --> 00:08:35.500
he could skip the training
and offload responsibility

00:08:35.500 --> 00:08:37.230
for navigation to the device.

00:08:38.480 --> 00:08:40.470
The ease, convenience,
and precision

00:08:40.470 --> 00:08:44.210
of automated navigation made the
Inuits' traditional techniques

00:08:44.210 --> 00:08:46.495
seem antiquated and
cumbersome by comparison.

00:08:47.950 --> 00:08:51.150
But as GPS devices
proliferated on the island,

00:08:51.150 --> 00:08:53.550
reports began to spread
of serious accidents

00:08:53.550 --> 00:08:57.520
during hunts, some resulting
in injuries and even deaths.

00:08:57.520 --> 00:09:00.755
The cause was often traced to
an overreliance on satellites.

00:09:01.760 --> 00:09:05.120
When a receiver breaks
or its batteries freeze,

00:09:05.120 --> 00:09:08.370
a hunter who hasn't developed
strong wayfinding skills

00:09:08.370 --> 00:09:11.100
can easily become lost
in the featureless waste

00:09:11.100 --> 00:09:12.460
and fall victim to exposure.

00:09:13.620 --> 00:09:17.650
Even when the devices operate
properly, they present hazards.

00:09:17.650 --> 00:09:20.740
The route, so meticulously
plotted on satellite maps,

00:09:20.740 --> 00:09:23.220
can give hunters a
form of tunnel vision.

00:09:23.220 --> 00:09:25.370
Trusting the GPS
instructions, they'll

00:09:25.370 --> 00:09:28.120
speed onto dangerously
thin ice, or

00:09:28.120 --> 00:09:31.450
into other environmental perils
that a skilled navigator would

00:09:31.450 --> 00:09:33.345
have had the sense and
foresight to avoid.

00:09:34.510 --> 00:09:36.310
Some of these problems
may eventually

00:09:36.310 --> 00:09:39.720
be mitigated by improvements
in navigational devices,

00:09:39.720 --> 00:09:42.110
or by better instruction
in their use.

00:09:42.110 --> 00:09:44.480
What won't be
mitigated is the loss

00:09:44.480 --> 00:09:47.080
of what one tribal
elder describes

00:09:47.080 --> 00:09:50.440
as "the wisdom and
knowledge of the Inuit."

00:09:50.440 --> 00:09:52.820
The anthropologist
Claudio Aporta,

00:09:52.820 --> 00:09:55.010
of Carleton
University in Ottawa,

00:09:55.010 --> 00:09:58.240
has been studying Inuit
hunters for years.

00:09:58.240 --> 00:10:00.800
He reports that while
satellite navigation offers

00:10:00.800 --> 00:10:03.740
attractive advantages,
its adoption has already

00:10:03.740 --> 00:10:06.890
brought a deterioration
in wayfinding abilities,

00:10:06.890 --> 00:10:10.540
and more generally, a
weakened feel for the land.

00:10:10.540 --> 00:10:13.510
As a hunter on a
GPS-equipped snowmobile

00:10:13.510 --> 00:10:15.850
devotes his attention to
the instructions coming

00:10:15.850 --> 00:10:19.330
from the computer, he loses
sight of his surroundings.

00:10:19.330 --> 00:10:22.710
He travels blindfolded,
as Aporta puts it.

00:10:22.710 --> 00:10:25.700
A singular talent that has
defined and distinguished

00:10:25.700 --> 00:10:29.440
a people for thousands of
years may well evaporate over

00:10:29.440 --> 00:10:33.170
the course of a
generation or two."

00:10:33.170 --> 00:10:37.770
When I relate that
story to people,

00:10:37.770 --> 00:10:39.460
they tend to have
one of two reactions.

00:10:39.460 --> 00:10:42.000
And my guess is both
of those reactions

00:10:42.000 --> 00:10:46.080
are probably represented
in this room.

00:10:46.080 --> 00:10:50.070
One of the reactions
is a feeling

00:10:50.070 --> 00:10:51.400
that this is a poignant story.

00:10:52.960 --> 00:10:55.359
It's a troubling story,
story about loss,

00:10:55.359 --> 00:10:57.400
about something essential
to the human condition.

00:10:58.560 --> 00:11:01.929
And that tends to be the
reaction I have to it.

00:11:01.929 --> 00:11:03.720
But then there's a very
different reaction,

00:11:03.720 --> 00:11:06.480
which is, well, welcome
to the modern world.

00:11:09.040 --> 00:11:13.700
Progress goes on, we adapt, and
in the end, things get better.

00:11:13.700 --> 00:11:17.390
And so if you think
about it, most of us,

00:11:17.390 --> 00:11:21.220
probably all human beings, once
had a much more sophisticated

00:11:21.220 --> 00:11:23.730
navigational sense,
inner navigational sense,

00:11:23.730 --> 00:11:25.420
much more sophisticated
perception

00:11:25.420 --> 00:11:27.090
of the world, the landscape.

00:11:27.090 --> 00:11:30.860
And for most of us, we've
lost almost all of that.

00:11:30.860 --> 00:11:32.850
And yet, we didn't go extinct.

00:11:32.850 --> 00:11:33.860
We're still here.

00:11:33.860 --> 00:11:35.310
By most measures,
we're thriving.

00:11:36.660 --> 00:11:42.580
And I think that is also a
completely valid point of view.

00:11:42.580 --> 00:11:46.470
It's true that we lose
lots of skills over time,

00:11:46.470 --> 00:11:49.230
and we gain new ones
and things go on.

00:11:49.230 --> 00:11:51.940
So in some ways,
your reaction to this

00:11:51.940 --> 00:11:56.520
is a value judgment about
what's meaningful in human life.

00:11:56.520 --> 00:11:58.590
But beyond those
value judgments,

00:11:58.590 --> 00:12:00.660
I think one thing,
or a couple things

00:12:00.660 --> 00:12:03.350
that this story, this
experience tells us,

00:12:03.350 --> 00:12:06.880
is how powerful
a new tool can be

00:12:06.880 --> 00:12:08.655
when introduced into a culture.

00:12:09.920 --> 00:12:12.900
It can change the
way people work,

00:12:12.900 --> 00:12:15.050
the way people
operate, the way they

00:12:15.050 --> 00:12:17.040
think about what's
important, the way they

00:12:17.040 --> 00:12:20.880
go about their lives
in many different ways.

00:12:20.880 --> 00:12:22.730
And it can do this
very, very quickly,

00:12:22.730 --> 00:12:26.030
overturning some
skill or some talent

00:12:26.030 --> 00:12:28.710
or some way of life that's been
around for thousands of years,

00:12:28.710 --> 00:12:30.740
just in the course
of a year or two.

00:12:30.740 --> 00:12:34.870
So introducing computer
tools, introducing automation,

00:12:34.870 --> 00:12:39.370
any kind of technology that
redefines what human beings do,

00:12:39.370 --> 00:12:41.860
and redefines what we do
versus what we hand off

00:12:41.860 --> 00:12:45.850
to machines or computers can
have very, very deep and very,

00:12:45.850 --> 00:12:47.550
very powerful effects.

00:12:47.550 --> 00:12:52.245
And a lot of these effects are
very difficult to anticipate.

00:12:53.390 --> 00:12:55.530
So the Inuit hunters,
the young hunters,

00:12:55.530 --> 00:12:57.984
didn't go out and
buy GPS systems

00:12:57.984 --> 00:13:00.150
because they wanted to
increase the odds that they'd

00:13:00.150 --> 00:13:01.320
get lost and die.

00:13:02.350 --> 00:13:03.930
And they probably
weren't thinking

00:13:03.930 --> 00:13:07.380
about eroding some
fundamental aspect of culture.

00:13:07.380 --> 00:13:09.440
They wanted to get
the convenience,

00:13:09.440 --> 00:13:13.330
the ease of the system,
which is what many of us

00:13:13.330 --> 00:13:17.460
are motivated by when
we decide to adopt

00:13:17.460 --> 00:13:22.670
some kind of new form of
automation in our lives.

00:13:22.670 --> 00:13:27.080
And when you look at all
these unanticipated effects,

00:13:27.080 --> 00:13:30.210
you can see a very common
theme that comes out

00:13:30.210 --> 00:13:32.662
in research about
automation, and particularly

00:13:32.662 --> 00:13:33.745
about computer automation.

00:13:35.000 --> 00:13:38.830
And it's something that's been
documented over and over again

00:13:38.830 --> 00:13:42.850
by human factors, scientists
and researchers, the people who

00:13:42.850 --> 00:13:46.770
study how people interact with
computers and other machines.

00:13:46.770 --> 00:13:50.890
And the concept is referred
to as "the substitution myth."

00:13:50.890 --> 00:13:52.450
And it's very simple.

00:13:52.450 --> 00:13:56.690
It says that
whenever you automate

00:13:56.690 --> 00:14:00.260
any part of an activity,
you fundamentally

00:14:00.260 --> 00:14:01.900
change the activity.

00:14:01.900 --> 00:14:05.110
And that's very different
from what we anticipate.

00:14:05.110 --> 00:14:07.605
Most people, either
users of software

00:14:07.605 --> 00:14:11.010
or other automated systems
or the designers, the makers,

00:14:11.010 --> 00:14:12.810
they assume that
actually you can

00:14:12.810 --> 00:14:15.990
take bits and pieces
of what people do.

00:14:15.990 --> 00:14:18.340
You can automate them.

00:14:18.340 --> 00:14:21.600
You can turn them over to
software or something else.

00:14:21.600 --> 00:14:23.570
And you'll make those
parts of the process

00:14:23.570 --> 00:14:27.130
more efficient or more
convenient or faster

00:14:27.130 --> 00:14:28.030
or cheaper.

00:14:28.030 --> 00:14:30.900
But you won't fundamentally
change the way people

00:14:30.900 --> 00:14:32.620
go about doing their work.

00:14:32.620 --> 00:14:34.350
You won't change their behavior.

00:14:34.350 --> 00:14:36.110
In fact, over and
over again we see

00:14:36.110 --> 00:14:40.430
that even small changes,
small shifts of responsibility

00:14:40.430 --> 00:14:42.550
from people to
technology, can have

00:14:42.550 --> 00:14:45.260
very big effects on
the way people behave,

00:14:45.260 --> 00:14:47.490
the way they learn, the way
they approach their jobs.

00:14:48.520 --> 00:14:54.020
We've seen this recently with
the increasing automation

00:14:54.020 --> 00:14:57.370
of medical record keeping.

00:14:57.370 --> 00:15:01.250
As you probably know,
we've moved fairly quickly

00:15:01.250 --> 00:15:03.750
over the last 10
years from doctors

00:15:03.750 --> 00:15:07.110
taking patient notes on paper,
either writing them by hand

00:15:07.110 --> 00:15:11.420
or dictating them,
to digital records.

00:15:11.420 --> 00:15:15.730
So doctors, usually as
they're going through an exam,

00:15:15.730 --> 00:15:18.870
will take notes, usually
going through a template

00:15:18.870 --> 00:15:20.660
on a computer or on a tablet.

00:15:20.660 --> 00:15:23.014
And for most of us,
our initial reaction

00:15:23.014 --> 00:15:24.180
is, thank goodness for that.

00:15:24.180 --> 00:15:29.060
Because having records on
paper was a pain in the neck.

00:15:29.060 --> 00:15:30.970
You'd have to enter
the same information,

00:15:30.970 --> 00:15:33.310
depending on when you
went to different doctors.

00:15:33.310 --> 00:15:38.630
And God forbid you got sick
somewhere else in the country,

00:15:38.630 --> 00:15:41.400
or something, and doctors
couldn't exchange,

00:15:41.400 --> 00:15:44.020
had no way to share
your old records.

00:15:44.020 --> 00:15:47.730
So it makes all sorts of
sense to automate this

00:15:47.730 --> 00:15:49.800
and to have digital records.

00:15:49.800 --> 00:15:52.820
And indeed, 10 years ago when
we started down this path,

00:15:52.820 --> 00:15:54.320
the US started down
this path, there

00:15:54.320 --> 00:15:56.800
were all sorts of studies
that said, oh, we're

00:15:56.800 --> 00:15:59.210
going to save enormous
amounts of money.

00:15:59.210 --> 00:16:03.350
We're going to increase patient
care, quality of health care,

00:16:03.350 --> 00:16:07.290
as well as make it easier
to share information.

00:16:07.290 --> 00:16:09.770
And there was a big study
by the Rand Corporation

00:16:09.770 --> 00:16:11.180
that documented all this.

00:16:11.180 --> 00:16:12.840
They had modeled the
entire health care

00:16:12.840 --> 00:16:16.449
system in a computer,
output various things.

00:16:16.449 --> 00:16:17.990
This was going to
be all to the good.

00:16:17.990 --> 00:16:21.210
Well, the government
went on to subsidize

00:16:21.210 --> 00:16:23.590
the adoption of
electronic medical records

00:16:23.590 --> 00:16:26.560
to the tune of something
like $30 billion since then.

00:16:26.560 --> 00:16:29.200
And now we have a lot
of information about

00:16:29.200 --> 00:16:30.360
what's really happened.

00:16:30.360 --> 00:16:34.890
And nothing that was expected
has actually played out.

00:16:34.890 --> 00:16:37.490
And all sorts of things
that weren't expected, have.

00:16:38.880 --> 00:16:42.010
For instance, the cost
savings have not materialized.

00:16:42.010 --> 00:16:43.736
Cost has continued to go up.

00:16:43.736 --> 00:16:45.110
And there's even
some indications

00:16:45.110 --> 00:16:49.100
that beyond the expense required
for the systems themselves,

00:16:49.100 --> 00:16:51.680
this shift may increase
health care costs

00:16:51.680 --> 00:16:53.360
rather than decrease them.

00:16:53.360 --> 00:16:57.360
The evidence on quality of
care is very, very mixed.

00:16:57.360 --> 00:16:59.950
There seems to be no doubt
that for some patients, those

00:16:59.950 --> 00:17:01.740
with chronic
diseases that require

00:17:01.740 --> 00:17:04.294
a lot of different
doctors, quality goes up.

00:17:04.294 --> 00:17:06.990
But for a lot of patients,
there hasn't been a change.

00:17:06.990 --> 00:17:09.760
And there may even have
been an erosion of quality,

00:17:09.760 --> 00:17:11.109
in some instances.

00:17:11.109 --> 00:17:13.170
And finally, we're not
even getting the benefits

00:17:13.170 --> 00:17:18.080
of broad sharing of the records,
because a lot of the systems

00:17:18.080 --> 00:17:18.746
are proprietary.

00:17:20.040 --> 00:17:24.693
And so you can't transfer
the records quickly or easily

00:17:24.693 --> 00:17:27.109
from one hospital to the next
or one practice to the next.

00:17:27.109 --> 00:17:30.940
And now some of these problems
just come from the fact

00:17:30.940 --> 00:17:32.920
that a lot of
software is crappy.

00:17:32.920 --> 00:17:35.412
And we've rushed to spend
huge amounts of money on it.

00:17:35.412 --> 00:17:37.370
Lots of big software
companies that supply this

00:17:37.370 --> 00:17:40.110
have gotten very wealthy.

00:17:40.110 --> 00:17:41.680
And doctors are
struggling with it.

00:17:41.680 --> 00:17:43.180
Patients are struggling with it.

00:17:43.180 --> 00:17:44.670
And so some of
those things will be

00:17:44.670 --> 00:17:47.640
fixed at more expense over time.

00:17:47.640 --> 00:17:51.000
But if you look down lower,
you see changes in behavior

00:17:51.000 --> 00:17:53.270
that are much more subtle
and much more interesting

00:17:53.270 --> 00:17:56.730
and go beyond the quality
of the software itself.

00:17:56.730 --> 00:17:59.490
So for instance, one of the
reasons that everybody expected

00:17:59.490 --> 00:18:02.890
that health care costs would
go down-- the assumption

00:18:02.890 --> 00:18:06.840
was that as soon as doctors can
call up images and other test

00:18:06.840 --> 00:18:11.050
results on their computers
when they're in with a patient,

00:18:11.050 --> 00:18:12.580
they wouldn't order more tests.

00:18:12.580 --> 00:18:16.270
So we'd see fewer diagnostic
tests and fewer costs

00:18:16.270 --> 00:18:17.980
from those diagnostic
tests-- big part

00:18:17.980 --> 00:18:20.280
of the health care
system's costs.

00:18:20.280 --> 00:18:23.320
Actually, exactly the opposite
seems to be happening.

00:18:23.320 --> 00:18:27.200
You give the doctor an
ability to quickly order

00:18:27.200 --> 00:18:29.210
tests and quickly
pull up the results,

00:18:29.210 --> 00:18:30.890
doctors actually
order more of them

00:18:30.890 --> 00:18:33.460
because they know it's
going to be easier for them.

00:18:33.460 --> 00:18:35.640
And so the quality of the
outcomes doesn't go up.

00:18:35.640 --> 00:18:38.760
We're just seeing more
diagnostic tests and more

00:18:38.760 --> 00:18:42.300
costs, exactly the opposite
of what we expected.

00:18:42.300 --> 00:18:45.160
You see changes in the
doctor/patient relationship.

00:18:45.160 --> 00:18:49.107
If you've had the experience,
if you've been around for awhile

00:18:49.107 --> 00:18:50.940
and had the experience
of going from a world

00:18:50.940 --> 00:18:53.231
in which you went into a
doctor's office for a physical

00:18:53.231 --> 00:18:56.970
or whatever and the doctor
paid his or her whole attention

00:18:56.970 --> 00:19:00.227
to you, to the world of
electronic medical records,

00:19:00.227 --> 00:19:01.685
when the doctor
has a computer, you

00:19:01.685 --> 00:19:04.960
know that it intrudes in the
doctor/patient relationship.

00:19:04.960 --> 00:19:07.020
Studies show that
doctors now spend,

00:19:07.020 --> 00:19:10.110
if they have a computer
with them, about 25% to 50%

00:19:10.110 --> 00:19:13.302
of the time during an exam
looking at the computer, rather

00:19:13.302 --> 00:19:14.010
than the patient.

00:19:14.010 --> 00:19:15.780
And doctors aren't
happy about that.

00:19:15.780 --> 00:19:17.760
Patients don't tend
to be happy about it.

00:19:17.760 --> 00:19:20.610
But it's kind of a
necessary consequence,

00:19:20.610 --> 00:19:22.655
at least how we've
designed these systems,

00:19:22.655 --> 00:19:23.900
of this transfer.

00:19:25.180 --> 00:19:28.040
The most interesting--
I'm just going

00:19:28.040 --> 00:19:31.450
to give you three examples
of unexpected results--

00:19:31.450 --> 00:19:33.560
but the most interesting
to me is the fact

00:19:33.560 --> 00:19:37.210
that the quality of the records
themselves has gone down.

00:19:38.290 --> 00:19:41.230
And the reason is that
first of all, now doctors

00:19:41.230 --> 00:19:44.540
use templates,
checkboxes lots of times.

00:19:44.540 --> 00:19:46.780
And then when they
have to put in text

00:19:46.780 --> 00:19:49.490
describing the
patient's condition,

00:19:49.490 --> 00:19:53.140
rather than dictating it from
what they've just experienced

00:19:53.140 --> 00:19:55.310
or hand writing it,
they cut and paste.

00:19:55.310 --> 00:19:57.190
They cut and paste
paragraphs and other stuff

00:19:57.190 --> 00:20:00.130
from other visits that
the patient has had,

00:20:00.130 --> 00:20:04.120
or from visits by other patients
that have similar conditions.

00:20:04.120 --> 00:20:06.880
And this is referred to
as "the cloning of text."

00:20:06.880 --> 00:20:10.290
And more and more of
personal medical records

00:20:10.290 --> 00:20:13.100
consist of cloned
text these days,

00:20:13.100 --> 00:20:16.700
which makes the records
less useful for doctors,

00:20:16.700 --> 00:20:19.730
because it has less rich
and subtle information.

00:20:19.730 --> 00:20:23.010
And it also undermines
an important role

00:20:23.010 --> 00:20:26.720
that records used to play in
the exchange of information

00:20:26.720 --> 00:20:27.800
and knowledge.

00:20:27.800 --> 00:20:30.420
A primary care physician used
to get a lot of information,

00:20:30.420 --> 00:20:33.720
a lot of knowledge by
reading rich descriptions

00:20:33.720 --> 00:20:34.980
from specialists.

00:20:34.980 --> 00:20:37.360
And now, more and
more, as doctors say,

00:20:37.360 --> 00:20:40.250
this is just boilerplate,
just cloned text.

00:20:40.250 --> 00:20:43.980
So we've created this system
that eventually will probably

00:20:43.980 --> 00:20:48.210
have the very important benefit
of allowing us to exchange

00:20:48.210 --> 00:20:51.420
information more and more
quickly, more and more easily.

00:20:51.420 --> 00:20:53.380
But at the same
time, we're reducing

00:20:53.380 --> 00:20:55.640
the quality of the
information itself

00:20:55.640 --> 00:20:59.130
and making what's
exchanged less valuable.

00:20:59.130 --> 00:21:02.890
Now those are three examples of
how the substitution myth has

00:21:02.890 --> 00:21:06.500
played out in this particular
area of automation.

00:21:06.500 --> 00:21:08.180
And they're very
specialized, and you

00:21:08.180 --> 00:21:10.640
see all sorts of these
things anywhere you look.

00:21:10.640 --> 00:21:13.460
But there are a couple
of bigger themes that

00:21:13.460 --> 00:21:17.170
tend to cross all
aspects of automation

00:21:17.170 --> 00:21:20.240
when you introduce software
to make jobs easier

00:21:20.240 --> 00:21:21.500
or to take over jobs.

00:21:21.500 --> 00:21:24.650
What you tend to get, in
addition to the benefits,

00:21:24.650 --> 00:21:28.475
are a couple of big
negative developments.

00:21:30.840 --> 00:21:33.340
Human factors, experts,
researchers on this

00:21:33.340 --> 00:21:36.880
refer to these as "automation
complacency" and "automation

00:21:36.880 --> 00:21:38.430
bias."

00:21:38.430 --> 00:21:41.830
Automation complacency means
exactly what you would expect.

00:21:43.170 --> 00:21:46.020
When people turn over
big aspects of their job

00:21:46.020 --> 00:21:49.570
to computers, to software,
to robots, they tune out.

00:21:50.900 --> 00:21:53.610
We're very good at trusting
a machine, and certainly

00:21:53.610 --> 00:21:58.750
a computerized machine, to
handle our job, to handle

00:21:58.750 --> 00:22:00.830
any challenge that might arise.

00:22:00.830 --> 00:22:03.000
And so we become complacent.

00:22:03.000 --> 00:22:03.500
We tune out.

00:22:03.500 --> 00:22:05.300
We space out.

00:22:05.300 --> 00:22:07.660
And that might be fine
until something bad happens,

00:22:07.660 --> 00:22:09.160
and we suddenly
have to re-engage

00:22:09.160 --> 00:22:11.701
with what we're doing, and then
you see people make mistakes.

00:22:13.120 --> 00:22:17.290
Everybody experiences automation
complacency in using computers.

00:22:17.290 --> 00:22:22.350
A very simple example is
autocorrect for spelling.

00:22:24.210 --> 00:22:25.870
When people have
autocorrect going,

00:22:25.870 --> 00:22:29.130
when they're texting or using
a word processor or whatever,

00:22:29.130 --> 00:22:31.890
they become much more
complacent about their spelling.

00:22:31.890 --> 00:22:32.920
They don't check things.

00:22:32.920 --> 00:22:34.620
They let it go.

00:22:34.620 --> 00:22:38.920
And then most people have had
the experience of sending out

00:22:38.920 --> 00:22:42.700
a text or an email
or a report that

00:22:42.700 --> 00:22:44.640
has some really
stupid typo in it,

00:22:44.640 --> 00:22:49.040
because the computer
misunderstood your intent.

00:22:49.040 --> 00:22:52.000
And that causes maybe a
moment of embarrassment.

00:22:52.000 --> 00:22:54.600
But you take that same
phenomenon of complacency

00:22:54.600 --> 00:23:00.000
and put it into an industrial
control room, into a cockpit,

00:23:00.000 --> 00:23:02.240
into a battlefield,
and you sometimes

00:23:02.240 --> 00:23:04.510
get very, very
dangerous situations.

00:23:05.650 --> 00:23:08.890
One of the classic examples
of automation complacency

00:23:08.890 --> 00:23:12.310
comes in the cruise
line business.

00:23:12.310 --> 00:23:15.210
A few years ago, a
cruise ship called

00:23:15.210 --> 00:23:19.040
the Royal Majesty was on
the last leg of a cruise

00:23:19.040 --> 00:23:20.329
off New England.

00:23:20.329 --> 00:23:22.245
It was going from Bermuda,
I think, to Boston.

00:23:23.562 --> 00:23:28.510
It had a GPS antenna that
was connected to an automated

00:23:28.510 --> 00:23:30.180
navigation system.

00:23:30.180 --> 00:23:32.890
The crew turned on the
automated navigation system,

00:23:32.890 --> 00:23:35.790
and kind of became
totally complacent--

00:23:35.790 --> 00:23:38.000
just assumed, OK,
everything's going fine.

00:23:38.000 --> 00:23:41.150
Hey, the computer's
plotting our course,

00:23:41.150 --> 00:23:42.360
don't have to worry about it.

00:23:42.360 --> 00:23:46.140
And at some point the antenna,
the line to the GPS antenna

00:23:46.140 --> 00:23:47.000
broke.

00:23:47.000 --> 00:23:49.240
And this was way up
somewhere and nobody saw it.

00:23:49.240 --> 00:23:50.335
And nobody noticed.

00:23:51.440 --> 00:23:54.950
There were increasing
environmental clues

00:23:54.950 --> 00:23:57.900
that the ship was
drifting off course.

00:23:57.900 --> 00:23:59.270
Nobody saw it.

00:23:59.270 --> 00:24:01.590
At one point, a
mate whose job it

00:24:01.590 --> 00:24:05.060
was to watch for
a locational buoy

00:24:05.060 --> 00:24:08.850
and report back to the bridge
that, yeah, we passed this

00:24:08.850 --> 00:24:11.066
as we should have, he was
out there watching for it

00:24:11.066 --> 00:24:12.610
and he didn't see it.

00:24:12.610 --> 00:24:15.010
And he said, well,
it must be there

00:24:15.010 --> 00:24:17.315
because the computer's
in charge here.

00:24:17.315 --> 00:24:18.440
I just must have missed it.

00:24:18.440 --> 00:24:20.064
So he didn't bother
to tell the bridge.

00:24:20.064 --> 00:24:22.350
He was embarrassed
that he had missed

00:24:22.350 --> 00:24:23.440
what must have been there.

00:24:23.440 --> 00:24:25.960
Well, hours go by, and
ultimately the ship

00:24:25.960 --> 00:24:29.591
crashes into a sandbar off
Nantucket Island many miles

00:24:29.591 --> 00:24:30.090
off course.

00:24:31.330 --> 00:24:35.590
Fortunately, no one was
killed or injured that bad,

00:24:35.590 --> 00:24:38.110
but there was millions
of dollars of damage.

00:24:38.110 --> 00:24:42.050
It kind of shows
how easily, if you

00:24:42.050 --> 00:24:44.560
give too much responsibility
to the computer,

00:24:44.560 --> 00:24:46.840
the people will tune out.

00:24:46.840 --> 00:24:50.230
And they won't notice
things are going wrong,

00:24:50.230 --> 00:24:53.500
or if they do notice, they might
make mistakes in responding.

00:24:53.500 --> 00:24:56.520
Automation bias is
closely related.

00:24:57.666 --> 00:24:58.900
to automation complacency.

00:24:59.930 --> 00:25:03.060
And it just means that
you place too much trust

00:25:03.060 --> 00:25:06.330
in the information coming
from your computer,

00:25:06.330 --> 00:25:09.160
to the point where
you begin to assume

00:25:09.160 --> 00:25:12.330
that the computer is infallible.

00:25:12.330 --> 00:25:14.040
And so you don't
have to pay attention

00:25:14.040 --> 00:25:16.820
to other sources of information,
including your own eyes

00:25:16.820 --> 00:25:17.730
and ears.

00:25:17.730 --> 00:25:21.320
And this too is something we
see over and over again when

00:25:21.320 --> 00:25:22.920
you automate any
kind of activity.

00:25:24.170 --> 00:25:27.900
A good example is the use
of GPS by truck drivers.

00:25:29.446 --> 00:25:34.100
A truck driver starts to
listen to the automated voice

00:25:34.100 --> 00:25:39.990
of the GPS woman telling them
where to go and whatever.

00:25:39.990 --> 00:25:44.190
And he or she begins to ignore
other sources of information

00:25:44.190 --> 00:25:45.800
like road signs.

00:25:45.800 --> 00:25:49.220
So we've seen an increase in
the incidence of trucks crashing

00:25:49.220 --> 00:25:53.580
into low overpasses as we've
increased the use of GPS.

00:25:53.580 --> 00:25:55.940
And in Seattle a
few years ago, there

00:25:55.940 --> 00:25:59.950
was a bus driver carrying
a load of high school

00:25:59.950 --> 00:26:02.300
athletes to a game somewhere.

00:26:02.300 --> 00:26:06.340
12-foot-high bus approached
a nine-foot-high overpass.

00:26:06.340 --> 00:26:09.380
And there were all these signs
along the way-- "Danger-- Low

00:26:09.380 --> 00:26:12.460
Overpass" or even signs that
had blinking lights around them.

00:26:12.460 --> 00:26:13.970
He smashes right into it.

00:26:15.905 --> 00:26:16.780
Luckily, no one died.

00:26:16.780 --> 00:26:18.571
A bunch of students
had to go the hospital.

00:26:18.571 --> 00:26:20.620
The police said, what
were you thinking?

00:26:20.620 --> 00:26:22.470
And he said, well,
I had my GPS on

00:26:22.470 --> 00:26:23.945
and I just didn't see the signs.

00:26:25.100 --> 00:26:29.255
So we ignore, or don't even see,
other sources of information.

00:26:30.320 --> 00:26:35.370
In another very different
area, back to health care,

00:26:35.370 --> 00:26:40.000
if you look at how radiologists
read diagnostic images today,

00:26:40.000 --> 00:26:43.020
most of them read them as
digital images, of course.

00:26:43.020 --> 00:26:45.360
But also there's
now software that

00:26:45.360 --> 00:26:49.300
is designed as a decision
support aid and analytical aid.

00:26:49.300 --> 00:26:52.590
And what it does is it gives
the radiologist prompts.

00:26:52.590 --> 00:26:57.200
It highlights particular regions
of the image that the data

00:26:57.200 --> 00:26:59.970
analysis, past data,
suggests are suspicious.

00:27:01.080 --> 00:27:03.280
In many cases, this
has very good results.

00:27:03.280 --> 00:27:08.060
The doctor focuses attention
on those particular highlighted

00:27:08.060 --> 00:27:11.340
areas, finds a cancer
or other abnormality

00:27:11.340 --> 00:27:13.150
that the doctor may have missed.

00:27:13.150 --> 00:27:14.390
And that's fine.

00:27:14.390 --> 00:27:18.650
But research shows that it also
has the exact opposite effect.

00:27:18.650 --> 00:27:21.410
Doctors become so focused
on the highlighted areas

00:27:21.410 --> 00:27:24.970
that they only pay cursory
attention to other areas,

00:27:24.970 --> 00:27:29.970
and often miss abnormalities
or cancers that

00:27:29.970 --> 00:27:31.290
aren't highlighted.

00:27:31.290 --> 00:27:35.670
And the latest research suggests
that these prompt systems,

00:27:35.670 --> 00:27:39.400
which, as you know, are
very, very common in software

00:27:39.400 --> 00:27:41.970
in general, these
prompt systems seem

00:27:41.970 --> 00:27:49.200
to improve the performance
of less expert image readers

00:27:49.200 --> 00:27:53.110
on simpler challenges,
but decrease

00:27:53.110 --> 00:27:54.710
the performance
of expert readers

00:27:54.710 --> 00:27:56.275
on very, very hard challenges.

00:28:00.690 --> 00:28:05.940
The phenomena of automation
complacency and automation bias

00:28:05.940 --> 00:28:10.210
points to, I think, an even
deeper and more insidious

00:28:10.210 --> 00:28:15.180
problem that poorly designed
software or poorly designed

00:28:15.180 --> 00:28:17.470
automated systems
often triggers.

00:28:17.470 --> 00:28:19.850
And that is that in
both of those cases,

00:28:19.850 --> 00:28:23.670
with complacency and
bias, you see a person

00:28:23.670 --> 00:28:28.200
disengaging from the
world, disengaging

00:28:28.200 --> 00:28:30.500
from his or her
circumstances, disengaging

00:28:30.500 --> 00:28:33.840
from the task at
hand, simply assuming

00:28:33.840 --> 00:28:36.010
that the computer
will handle it.

00:28:36.010 --> 00:28:38.474
And indeed, the computer
has been designed-- whatever

00:28:38.474 --> 00:28:41.140
system we're talking about-- has
been designed to handle as much

00:28:41.140 --> 00:28:42.810
of the chore as possible.

00:28:42.810 --> 00:28:47.420
And what happens then is
we see an erosion of talent

00:28:47.420 --> 00:28:48.920
on the part of the person.

00:28:48.920 --> 00:28:52.420
Either the person isn't
developing strong, rich

00:28:52.420 --> 00:28:54.340
talents, or their
existing talents

00:28:54.340 --> 00:28:55.870
are beginning to get rusty.

00:28:55.870 --> 00:28:58.790
And the reason of
is pretty obvious.

00:28:58.790 --> 00:29:01.970
We all know, either intuitively
or if you've read anything

00:29:01.970 --> 00:29:06.082
about this, how we develop rich
talents, sophisticated talents.

00:29:06.082 --> 00:29:06.790
It's by practice.

00:29:06.790 --> 00:29:09.510
It's by doing things over
and over again, facing

00:29:09.510 --> 00:29:11.740
lots of different
challenges in lots

00:29:11.740 --> 00:29:13.430
of different
circumstances, figuring out

00:29:13.430 --> 00:29:15.600
how to overcome them.

00:29:15.600 --> 00:29:18.400
That's how we build the
most sophisticated skills

00:29:18.400 --> 00:29:20.270
and how we continue
to refine them.

00:29:21.300 --> 00:29:25.130
And this element,
this crucial element

00:29:25.130 --> 00:29:28.740
in learning, in
all sorts of forms,

00:29:28.740 --> 00:29:31.690
is often referred to as
"the generation effect."

00:29:31.690 --> 00:29:34.560
And what that means
is, if you're actively

00:29:34.560 --> 00:29:37.640
engaged in some task,
in some form of work,

00:29:37.640 --> 00:29:40.510
you're going to not only
perform better, but learn more

00:29:40.510 --> 00:29:43.130
and become more expert
than if you're simply

00:29:43.130 --> 00:29:46.710
an observer, simply passively
watching as things progress.

00:29:47.790 --> 00:29:50.190
And the generation
effect was first

00:29:50.190 --> 00:29:55.150
observed in this very
simple experiment involving

00:29:55.150 --> 00:29:59.030
people's ability to
expand vocabulary, learn

00:29:59.030 --> 00:30:01.300
vocabulary, remember vocabulary.

00:30:01.300 --> 00:30:04.050
And what the researchers
did back in the '70s

00:30:04.050 --> 00:30:07.290
is they got two groups
of people to try

00:30:07.290 --> 00:30:09.300
to memorize lots of
pairs of antonyms,

00:30:09.300 --> 00:30:10.830
lots of pairs of opposites.

00:30:10.830 --> 00:30:13.680
And the only difference
between the two groups

00:30:13.680 --> 00:30:17.030
was that one group
used flash cards that

00:30:17.030 --> 00:30:19.670
had both words spelled
out entirely-- hot,

00:30:19.670 --> 00:30:22.920
cold-- the other had flashcards
that just had the first word,

00:30:22.920 --> 00:30:25.790
"hot," but then provided
only the first letter

00:30:25.790 --> 00:30:28.100
of the second word, so "c."

00:30:28.100 --> 00:30:30.210
And what they found
was that indeed,

00:30:30.210 --> 00:30:32.880
the people who
used the full words

00:30:32.880 --> 00:30:36.170
remembered much
fewer of the antonyms

00:30:36.170 --> 00:30:39.180
than the people who had to
fill in the second word.

00:30:40.930 --> 00:30:42.100
The reason?

00:30:42.100 --> 00:30:44.770
There's a little bit more
brain activity involved here.

00:30:44.770 --> 00:30:48.550
You actually have to call
to mind what this word is.

00:30:48.550 --> 00:30:50.050
You have to generate it.

00:30:50.050 --> 00:30:52.959
And just that small
difference gives you

00:30:52.959 --> 00:30:54.375
better learning,
better retention.

00:30:56.230 --> 00:30:59.270
A few years later,
some other researchers,

00:30:59.270 --> 00:31:01.350
some other professors
in this area,

00:31:01.350 --> 00:31:04.530
realized that actually, this is
kind of a form of automation.

00:31:05.880 --> 00:31:09.670
What this does, giving
the full word in essence

00:31:09.670 --> 00:31:13.820
automates the work of
filling in the word.

00:31:13.820 --> 00:31:17.080
And they explained this as,
in fact, a phenomenon related

00:31:17.080 --> 00:31:19.040
to automation complacency.

00:31:19.040 --> 00:31:21.360
You might be completely
unconscious of it,

00:31:21.360 --> 00:31:24.550
but your brain is a
little more complacent.

00:31:24.550 --> 00:31:27.990
It doesn't have to work
as hard in this mode.

00:31:27.990 --> 00:31:29.700
And that makes a big difference.

00:31:29.700 --> 00:31:33.510
And it turns out that
the generation effect

00:31:33.510 --> 00:31:36.270
kind of explains
a whole lot about

00:31:36.270 --> 00:31:39.510
how we learn and develop
skill in all sorts of places.

00:31:39.510 --> 00:31:43.720
It's definitely not just
restricted to studies

00:31:43.720 --> 00:31:45.120
of vocabulary.

00:31:45.120 --> 00:31:46.210
You see it everywhere.

00:31:46.210 --> 00:31:49.210
If you're actively
involved, you learn more.

00:31:49.210 --> 00:31:50.450
You become more expertise.

00:31:50.450 --> 00:31:52.340
If you're not, you don't.

00:31:52.340 --> 00:31:57.530
And unfortunately, with
software, more and more,

00:31:57.530 --> 00:32:00.670
the programmer, the
designer, actually

00:32:00.670 --> 00:32:02.640
gets in the way of
the generation effect.

00:32:02.640 --> 00:32:04.590
And not by accident,
but on purpose.

00:32:04.590 --> 00:32:07.510
Because of course, the
things we tend to automate,

00:32:07.510 --> 00:32:10.420
the things we tend to
simplify for people,

00:32:10.420 --> 00:32:12.160
are the things that
are challenging.

00:32:12.160 --> 00:32:13.230
You look at a process.

00:32:13.230 --> 00:32:14.771
You look where people
are struggling.

00:32:15.530 --> 00:32:19.990
And that is both often the most
interesting thing to automate

00:32:19.990 --> 00:32:23.870
but also the place
that whoever's

00:32:23.870 --> 00:32:27.020
paying you to write the software
is encouraging you to do,

00:32:27.020 --> 00:32:29.500
because it seems to
create efficiency.

00:32:29.500 --> 00:32:31.460
It seems to create productivity.

00:32:31.460 --> 00:32:35.350
But what we're doing is
designing lots of systems,

00:32:35.350 --> 00:32:39.240
lots of software that
actually deliberately--

00:32:39.240 --> 00:32:40.880
if you look at it
in that sense--

00:32:40.880 --> 00:32:42.800
gets in the way of
people's ability

00:32:42.800 --> 00:32:45.700
to learn and create expertise.

00:32:45.700 --> 00:32:48.070
There was a series
of experiments

00:32:48.070 --> 00:32:54.150
done beginning about 10 years
ago by this young cognitive

00:32:54.150 --> 00:32:56.810
psychologist in Holland
named Christof van Nimwegen.

00:32:58.520 --> 00:33:01.050
And he did something
very interesting.

00:33:01.050 --> 00:33:03.690
He got a series of
different tasks.

00:33:03.690 --> 00:33:06.610
One of them was solving a
difficult logic problem.

00:33:06.610 --> 00:33:09.420
One of them was organizing
a conference where

00:33:09.420 --> 00:33:11.962
you had a large number
of conference rooms,

00:33:11.962 --> 00:33:14.170
large number of speakers,
large number of time slots,

00:33:14.170 --> 00:33:19.520
and you had to optimize how you
put all those things together.

00:33:19.520 --> 00:33:22.120
So a number of tasks that
had lots of components.

00:33:22.120 --> 00:33:24.550
Required a certain
amount of smarts,

00:33:24.550 --> 00:33:28.180
and required you to work through
a hard problem over time.

00:33:28.180 --> 00:33:31.730
And in each case, he got
groups of people, divided them

00:33:31.730 --> 00:33:36.920
into two, created two
different applications

00:33:36.920 --> 00:33:38.200
for doing each of these.

00:33:38.200 --> 00:33:40.510
One application was
very bare bones.

00:33:40.510 --> 00:33:43.981
It just provided you
with the scenario

00:33:43.981 --> 00:33:45.480
and then you had
to work through it.

00:33:45.480 --> 00:33:46.860
The other was very helpful.

00:33:46.860 --> 00:33:47.730
It had prompts.

00:33:47.730 --> 00:33:48.610
It had highlights.

00:33:48.610 --> 00:33:50.930
It had advice, on-screen advice.

00:33:50.930 --> 00:33:54.730
When you got to a point where
you could do some moves but you

00:33:54.730 --> 00:33:57.230
couldn't do others, it would
highlight the ones you could do

00:33:57.230 --> 00:33:59.240
and gray out the
ones you couldn't.

00:33:59.240 --> 00:34:01.440
And then he let them go
and watched what happened.

00:34:01.440 --> 00:34:03.350
Well, as you might
expect, the people

00:34:03.350 --> 00:34:07.200
with the more helpful software
got off to a great start.

00:34:07.200 --> 00:34:10.300
The software was
guiding them, helping

00:34:10.300 --> 00:34:12.889
them make their initial
decisions and moves.

00:34:12.889 --> 00:34:16.070
They jumped out
to a lead in terms

00:34:16.070 --> 00:34:18.120
of solving the challenges.

00:34:18.120 --> 00:34:22.770
But over time, the people
using the bare bones software,

00:34:22.770 --> 00:34:27.139
the unhelpful software, not
only caught up but actually,

00:34:27.139 --> 00:34:31.610
in all the cases, ended up
completing the assignment much

00:34:31.610 --> 00:34:34.880
more efficiently,
made much fewer

00:34:34.880 --> 00:34:36.660
incorrect moves,
much fewer mistakes.

00:34:37.969 --> 00:34:40.644
They also seemed to have
a much clearer strategy,

00:34:40.644 --> 00:34:43.060
whereas the people using the
helpful software kind of just

00:34:43.060 --> 00:34:44.159
clicked around.

00:34:44.159 --> 00:34:49.360
And finally, van Nimwegen
gave them tests afterwards

00:34:49.360 --> 00:34:52.730
to measure their conceptual
understanding of what

00:34:52.730 --> 00:34:53.553
they had done.

00:34:53.553 --> 00:34:54.969
People with the
unhelpful software

00:34:54.969 --> 00:34:56.802
had a much clearer
conceptual understanding.

00:34:57.930 --> 00:35:02.500
Then eight months later, he
invited just the logic puzzle

00:35:02.500 --> 00:35:03.000
group.

00:35:03.000 --> 00:35:04.374
He invited all
the people who did

00:35:04.374 --> 00:35:07.480
that back, had them
solve the problem again.

00:35:07.480 --> 00:35:10.390
The people who had, eight months
earlier, used the unhelpful

00:35:10.390 --> 00:35:13.580
software solved the
puzzle twice as fast

00:35:13.580 --> 00:35:15.830
as the people who used
the unhelpful software.

00:35:16.860 --> 00:35:20.830
The more helpful the software,
the less learning, the weaker

00:35:20.830 --> 00:35:23.730
performance, the less strategic
thinking of the people

00:35:23.730 --> 00:35:25.700
who used it.

00:35:25.700 --> 00:35:29.620
Again, this underscores
a fundamental paradox

00:35:29.620 --> 00:35:34.110
that people face, people
who develop these programs

00:35:34.110 --> 00:35:38.270
and people who use them, where
our instinct to make things

00:35:38.270 --> 00:35:40.760
easier, to find the
places of friction

00:35:40.760 --> 00:35:43.310
and remove the
friction, can actually

00:35:43.310 --> 00:35:47.470
lead to counterproductive
results, where you're

00:35:47.470 --> 00:35:50.350
eroding performance
and eroding learning.

00:35:52.300 --> 00:35:55.390
So if you look at all
the psychological studies

00:35:55.390 --> 00:35:57.800
and the human factor
studies of how

00:35:57.800 --> 00:36:00.830
people interact with machines
and technology and computers,

00:36:00.830 --> 00:36:05.440
and you also combine it with
psychological understanding

00:36:05.440 --> 00:36:08.240
of how we learn, what
you see is that there's

00:36:08.240 --> 00:36:10.665
a very complex cycle involved.

00:36:11.910 --> 00:36:15.100
If you have a high
degree of engagement

00:36:15.100 --> 00:36:18.700
with people, if
they're really pushed

00:36:18.700 --> 00:36:22.060
to engage with
challenges, work hard,

00:36:22.060 --> 00:36:25.490
maintain their awareness
of their circumstances,

00:36:25.490 --> 00:36:28.930
you provoke a state of flow.

00:36:28.930 --> 00:36:31.750
If you've read Mihaly
Csikszentmihalyi's book "Flow"

00:36:31.750 --> 00:36:34.694
or are familiar with
it, we perform optimally

00:36:34.694 --> 00:36:37.110
when we're really immersed in
a hard challenge, when we're

00:36:37.110 --> 00:36:39.960
stretching our talents,
learning new talents.

00:36:41.160 --> 00:36:43.210
That's the optimal
state to be in.

00:36:43.210 --> 00:36:45.990
It gives us more skills,
pushes us to new talents,

00:36:45.990 --> 00:36:48.120
and it also happens to be
the state in which we're

00:36:48.120 --> 00:36:49.755
most fulfilled and
most satisfied.

00:36:51.590 --> 00:36:55.090
Often, people have this feeling
that if they were relieved

00:36:55.090 --> 00:36:56.630
of work, relieved
of effort, they'll

00:36:56.630 --> 00:36:58.360
be happier-- turns
out they're not.

00:36:58.360 --> 00:37:00.150
They're more miserable.

00:37:00.150 --> 00:37:02.820
They're actually happier
when they are working hard,

00:37:02.820 --> 00:37:04.030
facing a challenge.

00:37:04.030 --> 00:37:07.330
And so this sense of
fulfillment prolongs

00:37:07.330 --> 00:37:09.290
your sense of engagement,
intensifies it.

00:37:09.290 --> 00:37:10.980
And you get this
very nice cycle.

00:37:10.980 --> 00:37:13.240
People are performing
at a high level.

00:37:13.240 --> 00:37:15.040
They're learning talents.

00:37:15.040 --> 00:37:16.100
And they're fulfilled.

00:37:16.100 --> 00:37:17.150
They're happy,
they're satisfied,

00:37:17.150 --> 00:37:18.275
they like their experience.

00:37:19.260 --> 00:37:23.080
All too often, you stick
automation into here,

00:37:23.080 --> 00:37:24.540
particularly if
you haven't thought

00:37:24.540 --> 00:37:26.610
through all of the implications.

00:37:26.610 --> 00:37:28.700
And you break this cycle.

00:37:28.700 --> 00:37:30.390
Suddenly you
decrease engagement,

00:37:30.390 --> 00:37:33.430
and all the other
things go down as well.

00:37:33.430 --> 00:37:35.840
You see this today in
all sorts of places.

00:37:35.840 --> 00:37:37.750
You see it with
pilots, whose jobs

00:37:37.750 --> 00:37:40.230
have been highly,
highly automated.

00:37:40.230 --> 00:37:45.880
Automation has been a very
good, very positive development

00:37:45.880 --> 00:37:48.460
for 100 years in aviation.

00:37:48.460 --> 00:37:52.090
But recently, as
pilots' role in control

00:37:52.090 --> 00:37:54.340
of the aircraft, manual
control, has gone down

00:37:54.340 --> 00:37:56.798
to the point where they may be
in control for three minutes

00:37:56.798 --> 00:37:58.830
during a flight,
you see problems

00:37:58.830 --> 00:38:00.970
with the erosion of
engagement, the erosion

00:38:00.970 --> 00:38:03.780
of situational awareness,
and the erosion of talent.

00:38:03.780 --> 00:38:05.770
And unfortunately, on
those rare occasions

00:38:05.770 --> 00:38:09.040
when the autopilot fails
for whatever reason,

00:38:09.040 --> 00:38:11.940
or there's very
weird circumstances,

00:38:11.940 --> 00:38:15.110
you increase the odds that
the pilots will make mistakes

00:38:15.110 --> 00:38:17.260
sometimes, with
dangerous implications.

00:38:18.990 --> 00:38:24.680
So why do we go down
this path so often?

00:38:24.680 --> 00:38:30.640
Why do we create computer
programs, robotic systems,

00:38:30.640 --> 00:38:36.960
other automated systems, that
instead of raising people up

00:38:36.960 --> 00:38:38.470
to their highest
level of talent,

00:38:38.470 --> 00:38:40.750
highest level of awareness
and satisfaction,

00:38:40.750 --> 00:38:42.530
has the opposite effect?

00:38:42.530 --> 00:38:44.830
I think much of the
blame can be placed

00:38:44.830 --> 00:38:48.880
on what I would argue is the
dominant design philosophy

00:38:48.880 --> 00:38:53.360
or ethic that governs the people
who are making these programs

00:38:53.360 --> 00:38:55.660
and making these machines.

00:38:55.660 --> 00:38:58.470
And it's what's often referred
to as "technology-centered

00:38:58.470 --> 00:38:59.520
design."

00:38:59.520 --> 00:39:01.460
And basically what
that means is,

00:39:01.460 --> 00:39:03.860
the engineer or the
programmer or whatever

00:39:03.860 --> 00:39:06.710
starts by asking, what
can the computer do?

00:39:06.710 --> 00:39:08.560
What can the technology do?

00:39:08.560 --> 00:39:12.090
And then anything that the
computer or the technology

00:39:12.090 --> 00:39:17.110
can do, they give that
responsibility to the computer.

00:39:17.110 --> 00:39:21.510
And you can see why this is what
engineers and programmers would

00:39:21.510 --> 00:39:24.370
want to do, because that's
their job-- to give interesting,

00:39:24.370 --> 00:39:29.750
to simulate or automate
interesting work with software

00:39:29.750 --> 00:39:30.760
or with robots.

00:39:30.760 --> 00:39:32.420
So it's a very
natural thing to do.

00:39:32.420 --> 00:39:35.940
But what happens then is
what the human being gets

00:39:35.940 --> 00:39:37.460
is just what the
computer can't do,

00:39:37.460 --> 00:39:42.380
or what we haven't yet figured
out how the computer can do it.

00:39:42.380 --> 00:39:46.890
And that tends to be things
like monitoring screens

00:39:46.890 --> 00:39:50.780
for anomalies, entering
data, and, oh by the way,

00:39:50.780 --> 00:39:52.370
you're also the last
line of defense.

00:39:52.370 --> 00:39:54.160
So if everything
goes to hell, you've

00:39:54.160 --> 00:39:57.980
got to take over and
get us out of the fix.

00:39:57.980 --> 00:40:01.490
Those are things that people
are actually pretty bad at.

00:40:03.040 --> 00:40:07.190
We're terrible at monitoring
things, waiting for an anomaly.

00:40:07.190 --> 00:40:09.870
You can't focus on it for more
than about a half an hour.

00:40:11.230 --> 00:40:15.200
Entering data, becoming the
kind of sensor for the computer,

00:40:15.200 --> 00:40:18.150
is a pretty dull
job in most cases.

00:40:18.150 --> 00:40:23.250
And if you set up a system that
ensures that the operator is

00:40:23.250 --> 00:40:25.390
going to have a low level
of situational awareness,

00:40:25.390 --> 00:40:27.390
then that is not
the person you want

00:40:27.390 --> 00:40:29.550
to be having as the
last line of defense.

00:40:31.210 --> 00:40:34.590
The alternative is something
called-- surprise--

00:40:34.590 --> 00:40:37.550
"human-centered design,"
where you start by saying,

00:40:37.550 --> 00:40:39.520
what are human beings good at?

00:40:39.520 --> 00:40:41.290
And you look at the
fact that there's

00:40:41.290 --> 00:40:43.040
lots of important
things we're actually

00:40:43.040 --> 00:40:45.710
still much better
than computers at.

00:40:45.710 --> 00:40:46.565
We're creative.

00:40:47.850 --> 00:40:49.390
We have imagination.

00:40:49.390 --> 00:40:50.690
We can think conceptually.

00:40:50.690 --> 00:40:52.330
We have an understanding
of the world.

00:40:52.330 --> 00:40:53.680
We can think critically.

00:40:53.680 --> 00:40:55.200
We can think skeptically.

00:40:55.200 --> 00:40:57.610
And then you bring
in the software,

00:40:57.610 --> 00:41:03.130
you bring in the automation,
first to aid the person

00:41:03.130 --> 00:41:05.450
in exploiting
those capabilities,

00:41:05.450 --> 00:41:08.650
but also to fill in
the gaps and the flaws

00:41:08.650 --> 00:41:11.160
that we all have
as human beings.

00:41:11.160 --> 00:41:14.640
So we're not great at processing
huge amounts of information

00:41:14.640 --> 00:41:15.370
quickly.

00:41:15.370 --> 00:41:17.680
We're subject to
biases in our thinking.

00:41:17.680 --> 00:41:20.300
You can use software
to counteract these,

00:41:20.300 --> 00:41:25.550
or to provide an additional
set of capabilities.

00:41:25.550 --> 00:41:29.330
And if you go that path, you
get both the best of the human

00:41:29.330 --> 00:41:31.990
and the best of the machine,
or the best of the technology.

00:41:33.520 --> 00:41:36.500
And some of the ideas here
are very, very simple.

00:41:36.500 --> 00:41:38.590
For instance, with
pilots, instead

00:41:38.590 --> 00:41:42.850
of allowing them to turn
on total flight automation

00:41:42.850 --> 00:41:46.804
once they're off the ground and
then not bother to turn it off

00:41:46.804 --> 00:41:48.220
until they're about
ready to land,

00:41:48.220 --> 00:41:53.950
you can design the software to
give control back to the pilot

00:41:53.950 --> 00:41:56.740
every once in awhile
at random instances.

00:41:56.740 --> 00:41:59.450
And just when you
know that you're

00:41:59.450 --> 00:42:02.360
going to be called upon
at some random time

00:42:02.360 --> 00:42:06.400
to take back control, that
improves people's awareness

00:42:06.400 --> 00:42:07.880
and concentration immeasurably.

00:42:09.250 --> 00:42:10.750
It makes it less
likely that they're

00:42:10.750 --> 00:42:12.460
going to completely space out.

00:42:12.460 --> 00:42:15.490
Or in the example of the
radiologist-- and this

00:42:15.490 --> 00:42:19.120
goes for examples of
decision support or expert

00:42:19.120 --> 00:42:23.920
system or analytical programs in
general-- one thing you can do

00:42:23.920 --> 00:42:26.620
is instead of bringing
in the software prompts

00:42:26.620 --> 00:42:29.160
and the software advice
right at the outset,

00:42:29.160 --> 00:42:31.550
you can first encourage
the human being

00:42:31.550 --> 00:42:33.850
to deal with the
problem, to look

00:42:33.850 --> 00:42:36.830
at the image on his
or her own, or to do

00:42:36.830 --> 00:42:38.730
whatever analytical
chore is there.

00:42:38.730 --> 00:42:41.770
And then bring in the
software afterwards,

00:42:41.770 --> 00:42:45.500
as kind of a further aid,
bringing new information

00:42:45.500 --> 00:42:46.260
to bear.

00:42:46.260 --> 00:42:49.960
And that too means you get
the best of both people.

00:42:49.960 --> 00:42:54.650
Unfortunately, we don't do that,
or at least not very often.

00:42:54.650 --> 00:42:57.070
We don't pursue
human-centered design.

00:42:57.070 --> 00:42:59.130
And I think it's for
a couple of reasons.

00:42:59.130 --> 00:43:04.560
One is that we human
beings, as I said before,

00:43:04.560 --> 00:43:07.290
are very eager to hand
off any kind of work

00:43:07.290 --> 00:43:10.570
to machines, to software,
to other people,

00:43:10.570 --> 00:43:13.750
because we are afflicted
by what psychologists

00:43:13.750 --> 00:43:15.410
term "miswanting."

00:43:15.410 --> 00:43:19.040
We think we want to be freed
of labor, freed of hard work,

00:43:19.040 --> 00:43:20.510
freed of challenge.

00:43:20.510 --> 00:43:22.780
And when we are freed of
it, we feel miserable,

00:43:22.780 --> 00:43:25.170
we feel anxious, we
get self-absorbed.

00:43:25.170 --> 00:43:27.100
And actually, our
optimal experience

00:43:27.100 --> 00:43:29.390
comes when we are
working hard at things.

00:43:29.390 --> 00:43:30.950
So there's something
inside of us

00:43:30.950 --> 00:43:32.770
that is very eager
to get rid of stuff,

00:43:32.770 --> 00:43:34.880
even if it's get
rid of effort, even

00:43:34.880 --> 00:43:37.100
if it's not in our own benefit.

00:43:37.100 --> 00:43:38.670
And then the other
reason, which I

00:43:38.670 --> 00:43:42.810
think is one that's even
harder to deal with,

00:43:42.810 --> 00:43:46.430
is the pursuit of
efficiency and productivity

00:43:46.430 --> 00:43:49.850
above all other goals.

00:43:49.850 --> 00:43:51.140
And you can certainly see why.

00:43:51.140 --> 00:43:54.650
Hospitals who want the
highest productivity

00:43:54.650 --> 00:43:57.850
possible from radiologists
would be averse to saying,

00:43:57.850 --> 00:44:00.370
well, we'll let the
radiologist look at the image

00:44:00.370 --> 00:44:01.940
and then we'll bring
in the software.

00:44:01.940 --> 00:44:04.729
Because that extends the
time that a radiologist

00:44:04.729 --> 00:44:05.770
is going to look at this.

00:44:05.770 --> 00:44:09.280
And that's true of any of these
kind of analytical chores.

00:44:09.280 --> 00:44:13.390
And so there's this
tension between the pursuit

00:44:13.390 --> 00:44:16.270
of efficiency above all other
things and productivity,

00:44:16.270 --> 00:44:19.620
and the development of skill,
the development of talent,

00:44:19.620 --> 00:44:22.610
the development of high
levels of human performance,

00:44:22.610 --> 00:44:26.570
and ultimately the sense of
satisfaction that people get.

00:44:26.570 --> 00:44:29.240
And I think in the
long run, you see signs

00:44:29.240 --> 00:44:31.550
that that begins to backfire.

00:44:31.550 --> 00:44:33.360
Toyota earlier
this year announced

00:44:33.360 --> 00:44:36.300
that it was replacing
some of its robots

00:44:36.300 --> 00:44:38.900
in its Japanese factories
with human beings,

00:44:38.900 --> 00:44:41.530
because even though the
robots are more efficient,

00:44:41.530 --> 00:44:44.000
the company has struggled
with quality problems.

00:44:44.000 --> 00:44:47.700
It's had to recall 20
million cars in recent years.

00:44:47.700 --> 00:44:49.660
And not only is that
bad for business,

00:44:49.660 --> 00:44:53.130
but Toyota's entire culture
is built around quality

00:44:53.130 --> 00:44:55.620
manufacturing, so it
erodes its culture.

00:44:55.620 --> 00:44:57.810
So by bringing
back human beings,

00:44:57.810 --> 00:45:01.830
it wants to bring back both
the spirit and the reality

00:45:01.830 --> 00:45:05.200
of human craftsmanship,
of people who can actually

00:45:05.200 --> 00:45:07.470
think critically about
what they're doing.

00:45:07.470 --> 00:45:09.790
And one of the benefits
it believes it will get

00:45:09.790 --> 00:45:13.510
is that it will be smarter about
how it programs its robots.

00:45:13.510 --> 00:45:15.980
It will be able to
continually take

00:45:15.980 --> 00:45:18.930
new human thinking, new
human talent in insight,

00:45:18.930 --> 00:45:21.380
and then incorporate
that into the processes

00:45:21.380 --> 00:45:22.800
that even the robots are doing.

00:45:24.820 --> 00:45:26.550
That's a good news example.

00:45:26.550 --> 00:45:30.765
But I'm not going to
oversimplify this or lie

00:45:30.765 --> 00:45:31.280
to you.

00:45:31.280 --> 00:45:34.890
I think this tension
between placing efficiency

00:45:34.890 --> 00:45:37.990
above all other things,
immediate efficiency,

00:45:37.990 --> 00:45:44.010
is a very hard instinct,
very hard economic imperative

00:45:44.010 --> 00:45:45.420
to overcome.

00:45:45.420 --> 00:45:49.530
But nevertheless, I think
it's absolutely imperative

00:45:49.530 --> 00:45:55.000
that everyone who designs
software and robotics and all

00:45:55.000 --> 00:45:57.600
of us who use them are
conscious of the fact

00:45:57.600 --> 00:46:02.510
that there is this trade-off,
in that technology isn't just

00:46:02.510 --> 00:46:06.070
a means of production, as we
often tend to think of it.

00:46:06.070 --> 00:46:08.670
It really is a
means of experience.

00:46:08.670 --> 00:46:11.370
And it always has been, since
the first technologies were

00:46:11.370 --> 00:46:14.430
developed by our
distant ancestors.

00:46:14.430 --> 00:46:17.380
Technology at its best,
tools at their best

00:46:17.380 --> 00:46:22.160
bring us out into the
world, expand our skills

00:46:22.160 --> 00:46:25.325
and our talents, make the
world a more interesting place.

00:46:26.400 --> 00:46:29.020
And we shouldn't forget
that about ourselves

00:46:29.020 --> 00:46:33.720
as we continue at high
speed into a future where

00:46:33.720 --> 00:46:37.010
more and more aspects
of human experience

00:46:37.010 --> 00:46:40.710
are going to be offloaded to
computers and to machines.

00:46:40.710 --> 00:46:43.717
So thank you very much
for your attention.

00:46:43.717 --> 00:46:45.652
[APPLAUSE]

00:46:45.652 --> 00:46:46.152
Thank you.

00:46:49.561 --> 00:46:51.390
AUDIENCE: One thing
that immediately

00:46:51.390 --> 00:46:53.320
sprung to mind with
most of your examples

00:46:53.320 --> 00:46:57.050
is that they seem like
examples of poor automation.

00:46:57.050 --> 00:47:00.680
And I'm wondering
if you could say

00:47:00.680 --> 00:47:02.730
whether you feel
that there could be

00:47:02.730 --> 00:47:07.740
or are already any sufficiently
flawless technologies

00:47:07.740 --> 00:47:10.490
that we don't have to worry
about the sort of problems

00:47:10.490 --> 00:47:11.420
you're describing.

00:47:11.420 --> 00:47:13.250
NICHOLAS CARR: I think
in all instances,

00:47:13.250 --> 00:47:14.790
you have to worry about them.

00:47:14.790 --> 00:47:19.260
I agree with you that
a lot of these problems

00:47:19.260 --> 00:47:21.700
are not problems about
automation per se.

00:47:21.700 --> 00:47:24.180
Nobody's going to stop
the course of automation.

00:47:24.180 --> 00:47:26.100
You can argue that the
invention of the wheel

00:47:26.100 --> 00:47:28.440
was an example of
automating something,

00:47:28.440 --> 00:47:31.040
and I don't think any
of us regrets that.

00:47:31.040 --> 00:47:36.119
But I do think it's often unwise
design decisions, or unwise

00:47:36.119 --> 00:47:37.160
assumptions that come in.

00:47:37.160 --> 00:47:40.090
But as to the
question of whether we

00:47:40.090 --> 00:47:44.050
will create infallible
automation, I don't think so.

00:47:44.050 --> 00:47:47.330
I mean often, you get
this point of view,

00:47:47.330 --> 00:47:50.010
and it seems to be quite
common in Silicon Valley,

00:47:50.010 --> 00:47:55.429
if I can say that--
oh, people are only

00:47:55.429 --> 00:47:57.970
going to be a temporary nuisance
in a lot of these processes.

00:47:57.970 --> 00:47:59.570
We're going to have
fully self-driving cars.

00:47:59.570 --> 00:48:01.445
We're going to have
fully self-flying planes.

00:48:02.810 --> 00:48:08.160
We're going to have fully
analytical systems, big data

00:48:08.160 --> 00:48:10.240
systems that can pump
out the right answer

00:48:10.240 --> 00:48:12.020
and we won't have
to worry about it.

00:48:12.020 --> 00:48:14.730
I don't think that that's
actually going to happen.

00:48:14.730 --> 00:48:16.540
I mean, it might
happen eventually.

00:48:16.540 --> 00:48:20.241
It's very, very difficult
to remove the human being

00:48:20.241 --> 00:48:20.740
altogether.

00:48:23.510 --> 00:48:29.210
And so to me, what that
means is, OK, fine.

00:48:29.210 --> 00:48:33.020
You can pursue that as
some ideal, total flawless

00:48:33.020 --> 00:48:33.820
automation.

00:48:33.820 --> 00:48:37.540
But in the meantime, we live
and work in the present,

00:48:37.540 --> 00:48:38.630
not in the future.

00:48:38.630 --> 00:48:41.900
And for the foreseeable future,
in all of these processes

00:48:41.900 --> 00:48:44.140
there are going to
be people involved.

00:48:44.140 --> 00:48:46.340
And there are going to
be computers involved.

00:48:46.340 --> 00:48:50.130
And instead of
just saying, let's

00:48:50.130 --> 00:48:52.950
put the computer's interests
before the person's,

00:48:52.950 --> 00:48:56.100
I think the wise way is, as
I said, to go with a more

00:48:56.100 --> 00:49:00.259
human-centered design
that realizes and starts

00:49:00.259 --> 00:49:02.300
with the assumption that
the human being is going

00:49:02.300 --> 00:49:05.480
to play an essential
role in these things,

00:49:05.480 --> 00:49:10.020
for as long as we can imagine,
or as long as we could foresee.

00:49:11.070 --> 00:49:14.529
And so we better design
them to get the most out

00:49:14.529 --> 00:49:16.195
of the person as well
as the technology.

00:49:18.010 --> 00:49:18.895
Peter?

00:49:18.895 --> 00:49:19.770
MALE SPEAKER: Thanks.

00:49:19.770 --> 00:49:21.500
That was great, and
I certainly agree

00:49:21.500 --> 00:49:25.670
with the idea of focusing on
the human-centered design.

00:49:25.670 --> 00:49:28.210
I want to make one quick
comment, and then a question.

00:49:28.210 --> 00:49:30.460
I noticed on your
hot/cold thing,

00:49:30.460 --> 00:49:34.170
there was another researcher
who took passages from books

00:49:34.170 --> 00:49:36.610
and presented them, and
then multiple choice quiz

00:49:36.610 --> 00:49:37.480
or whatever.

00:49:37.480 --> 00:49:39.200
And then they took
the same passage

00:49:39.200 --> 00:49:44.050
and deleted a key sentence,
and people did better

00:49:44.050 --> 00:49:46.250
understanding the point then.

00:49:46.250 --> 00:49:48.680
But somehow no authors
are willing to have

00:49:48.680 --> 00:49:49.650
the guts to do that.

00:49:49.650 --> 00:49:54.200
So will you be that author to
delete the important sentences

00:49:54.200 --> 00:49:57.420
from your book and make
the reader engaged more,

00:49:57.420 --> 00:49:59.496
and therefore learn better?

00:49:59.496 --> 00:50:01.470
NICHOLAS CARR: If any
of you buy my book,

00:50:01.470 --> 00:50:02.950
I would be happy
to take a Sharpie

00:50:02.950 --> 00:50:05.780
and erase certain sentences and
you can get the full benefit.

00:50:05.780 --> 00:50:09.529
But I will take that under
advisement for future books.

00:50:09.529 --> 00:50:10.945
MALE SPEAKER: And
then a question.

00:50:12.040 --> 00:50:15.880
I'm interested in the difference
between automation complacency

00:50:15.880 --> 00:50:18.100
and authority complacency.

00:50:18.100 --> 00:50:21.500
So you see a lot of
these incidence reports,

00:50:21.500 --> 00:50:23.910
and they'll be some
underling who said, you know,

00:50:23.910 --> 00:50:26.400
I kind of noticed
something was going wrong.

00:50:26.400 --> 00:50:29.660
But the surgeon or
the pilot or the CEO

00:50:29.660 --> 00:50:32.779
seemed so sure that I
didn't want to say anything.

00:50:32.779 --> 00:50:34.570
And that has nothing
to do with automation.

00:50:34.570 --> 00:50:36.910
It's just authority.

00:50:38.610 --> 00:50:41.460
NICHOLAS CARR: I think
that's probably pretty much

00:50:41.460 --> 00:50:43.682
exactly the same phenomenon.

00:50:43.682 --> 00:50:45.140
And I actually do
think it probably

00:50:45.140 --> 00:50:48.870
has something to do
automation, because you could

00:50:48.870 --> 00:50:51.930
say that automation
complacency comes when

00:50:51.930 --> 00:50:56.900
the computer or the machine
takes the role of authority.

00:50:58.040 --> 00:51:00.220
So the person
defers to it, and I

00:51:00.220 --> 00:51:02.910
think that's certainly
one way to interpret

00:51:02.910 --> 00:51:06.266
a lot of the findings-- that
you don't question the machine.

00:51:06.266 --> 00:51:07.640
You don't question
the automation

00:51:07.640 --> 00:51:09.880
in a way that would be wise.

00:51:09.880 --> 00:51:12.990
So I think they're probably--
I think complacency has been

00:51:12.990 --> 00:51:15.290
a problem since long before
computers came around

00:51:15.290 --> 00:51:17.670
for people, for those
reasons and others.

00:51:17.670 --> 00:51:21.860
But we've created a new way to
generate the same phenomenon.

00:51:22.970 --> 00:51:24.256
Yeah?

00:51:24.256 --> 00:51:28.070
AUDIENCE: I'm
curious to ask if you

00:51:28.070 --> 00:51:33.320
know much research about
what percentage of time

00:51:33.320 --> 00:51:36.700
or experiences we need to keep
manual in order to make sure

00:51:36.700 --> 00:51:39.760
that the skills don't
fade away, and if you have

00:51:39.760 --> 00:51:41.890
any thoughts on how
much this transfers

00:51:41.890 --> 00:51:43.340
from domain to domain.

00:51:43.340 --> 00:51:47.610
So in the wayfaring example
of the Inuit, you might say,

00:51:47.610 --> 00:51:51.470
you could use GPS
80% of the time.

00:51:51.470 --> 00:51:53.100
But you've got to do
20% of it manually

00:51:53.100 --> 00:51:54.058
to keep your skills up.

00:51:54.058 --> 00:51:58.140
And maybe airline pilots
have the other examples

00:51:58.140 --> 00:51:59.060
that you cited.

00:51:59.060 --> 00:52:01.050
Do you know how much
this has been studied

00:52:01.050 --> 00:52:03.910
and how much it might vary
from domain to domain?

00:52:03.910 --> 00:52:06.700
NICHOLAS CARR: As far as the
second question, I don't know.

00:52:06.700 --> 00:52:11.300
I mean, I don't know
any rules of thumb,

00:52:11.300 --> 00:52:14.820
either in specific domains
or that cross domains.

00:52:14.820 --> 00:52:18.310
I can say, though, that there's
enormous amounts of research

00:52:18.310 --> 00:52:22.760
that's been done in
aviation because of the fact

00:52:22.760 --> 00:52:26.370
that the risk is so high,
and lots of people can die

00:52:26.370 --> 00:52:28.220
and lots of money can be lost.

00:52:28.220 --> 00:52:32.670
Ever since computerization of
flight began back in the '70s,

00:52:32.670 --> 00:52:36.737
there's been, whether it's NASA
or the FAA or universities,

00:52:36.737 --> 00:52:38.070
there has been tons of research.

00:52:38.070 --> 00:52:41.810
So my guess is that there
has been an examination.

00:52:41.810 --> 00:52:43.310
There probably have
been tests where

00:52:43.310 --> 00:52:47.220
you have different levels of
automation and manual control

00:52:47.220 --> 00:52:50.275
in comparing different
levels of performance.

00:52:51.330 --> 00:52:56.660
I didn't come across those
specific studies in my work,

00:52:56.660 --> 00:52:59.870
but my guess is that that
would be an obvious thing that

00:52:59.870 --> 00:53:01.070
would have been done.

00:53:01.070 --> 00:53:04.980
So I'm saying that in
aviation, there's probably

00:53:04.980 --> 00:53:09.940
at least some sense of
how much-- at what point

00:53:09.940 --> 00:53:12.360
does performance start to
drop off or start to drop off

00:53:12.360 --> 00:53:14.290
dramatically because
you've turned over

00:53:14.290 --> 00:53:16.720
too much responsibility
to the machine.

00:53:16.720 --> 00:53:20.190
Whether that would
also translate

00:53:20.190 --> 00:53:21.630
into the same kind
of percentages

00:53:21.630 --> 00:53:23.474
in different domains,
I don't know.

00:53:24.740 --> 00:53:25.594
Yeah?

00:53:25.594 --> 00:53:26.760
AUDIENCE: Thanks for coming.

00:53:26.760 --> 00:53:28.500
The talk was really interesting.

00:53:28.500 --> 00:53:31.710
In your talk, you pointed
out that technology always

00:53:31.710 --> 00:53:33.390
comes with trade-offs.

00:53:33.390 --> 00:53:35.000
And it's hard to
disagree with that.

00:53:36.052 --> 00:53:38.010
But I'm wondering about
the title of your book.

00:53:38.010 --> 00:53:39.890
The title is "The Glass Cage."

00:53:39.890 --> 00:53:42.820
And to call technology
a glass cage

00:53:42.820 --> 00:53:46.440
seems like a much more negative
assessment than merely saying

00:53:46.440 --> 00:53:47.920
that it comes with trade-offs.

00:53:47.920 --> 00:53:51.055
So I'm wondering if you could
say what motivates this title.

00:53:51.055 --> 00:53:52.930
NICHOLAS CARR: Well,
the title is a reference

00:53:52.930 --> 00:53:56.620
to-- back to pilots' experience.

00:53:56.620 --> 00:54:01.100
Since the '70s, pilots and
others in the aviation business

00:54:01.100 --> 00:54:04.220
have referred to cockpits
as glass cockpits.

00:54:04.220 --> 00:54:06.520
And it's because
increasingly, they're

00:54:06.520 --> 00:54:08.820
wrapped with computer screens.

00:54:08.820 --> 00:54:14.260
If you look at a
modern passenger jet,

00:54:14.260 --> 00:54:17.750
it's insane amounts
of computer screens--

00:54:17.750 --> 00:54:20.090
all sorts of input
devices and stuff.

00:54:22.150 --> 00:54:26.200
One aviation expert
refers to the cockpit

00:54:26.200 --> 00:54:28.270
now as a flying computer.

00:54:28.270 --> 00:54:32.220
So in one sense, it's just
kind of a play on that,

00:54:32.220 --> 00:54:37.290
because what I argue is that
we can learn a lot from pilots'

00:54:37.290 --> 00:54:41.562
experience as we enter into
a world that essentially,

00:54:41.562 --> 00:54:43.020
more and more of
us are going to be

00:54:43.020 --> 00:54:44.960
living inside a glass cockpit.

00:54:44.960 --> 00:54:46.502
We're going to be
looking at monitors

00:54:46.502 --> 00:54:47.626
to do more and more things.

00:54:47.626 --> 00:54:49.320
We're already there,
some would argue.

00:54:50.560 --> 00:54:54.860
And I do think that what a
lot of examples of computer

00:54:54.860 --> 00:54:58.230
automation tell us is that
the glass cockpit can become

00:54:58.230 --> 00:55:02.430
a glass cage, that
if we design it

00:55:02.430 --> 00:55:06.200
to be the primary
or the essential way

00:55:06.200 --> 00:55:08.410
we interact with the
world, then it cuts us off

00:55:08.410 --> 00:55:13.550
from other sources of
learning and information

00:55:13.550 --> 00:55:15.390
that might be
absolutely essential.

00:55:15.390 --> 00:55:17.640
But we're so focused on what
the computer's telling us

00:55:17.640 --> 00:55:18.470
we lose that.

00:55:18.470 --> 00:55:21.440
So I mean, it is intended
to be a little bit ominous,

00:55:21.440 --> 00:55:25.520
that we can either get
trapped in this glass cage,

00:55:25.520 --> 00:55:28.700
or we can use technology
in a more, what

00:55:28.700 --> 00:55:32.552
I think is a more humane
and more balanced way.

00:55:32.552 --> 00:55:34.260
FEMALE SPEAKER: Thanks
for that question.

00:55:34.260 --> 00:55:36.290
And on that note, please
join me in thanking

00:55:36.290 --> 00:55:38.111
Nicholas Carr for
coming to Google.

00:55:38.111 --> 00:55:38.610
[APPLAUSE]

00:55:38.610 --> 00:55:40.790
NICHOLAS CARR: Thank you.

