WEBVTT
Kind: captions
Language: en

00:00:02.280 --> 00:00:06.910
&gt;&gt;Lars Bak: Good morning, and welcome to the
second day of Google I/O. I hope you all enjoyed

00:00:06.910 --> 00:00:10.250
the party yesterday with Billy Idol.
[ Laughter ]

00:00:10.250 --> 00:00:14.040
&gt;&gt;Lars Bak: Yeah. From my youth.
[ Laughter ]

00:00:14.040 --> 00:00:22.540
&gt;&gt;Lars Bak: So we want to talk about web programming
languages and virtual machines and we're trying

00:00:22.540 --> 00:00:29.290
to argue why it's important to always make
it faster so that you guys can make better

00:00:29.290 --> 00:00:33.090
applications.
I'm Lars Bak. I'm the blue guy. And this is

00:00:33.090 --> 00:00:35.260
Kasper Lund --
&gt;&gt;Kasper Lund: I'm the red guy.

00:00:35.260 --> 00:00:36.260
&gt;&gt;Lars Bak: -- the red guy.
[ Laughter ]

00:00:36.260 --> 00:00:44.710
&gt;&gt;Lars Bak: And we are from Google, from the
Danish engineering office, so if we look tired,

00:00:44.710 --> 00:00:51.030
it's jet lag.
All right. So just to know -- so you know

00:00:51.030 --> 00:00:58.370
who we are, we've been working on virtual
machines for too long. I started in '86, and

00:00:58.370 --> 00:01:02.510
in '86 working on virtual machines for their
programming languages was sort of a little

00:01:02.510 --> 00:01:07.659
bit niche, and it was only research communities
that had an interest in it.

00:01:07.659 --> 00:01:12.681
But given today we have several hundred people
here, I'm pleased that there's more interest

00:01:12.681 --> 00:01:16.860
for it.
So I started working with Kasper 13 years

00:01:16.860 --> 00:01:24.670
ago and we've been doing Java virtual machines,
Smalltalk virtual machines, JavaScript virtual

00:01:24.670 --> 00:01:29.220
machines, and now Dart, so we sort of have
a little bit of experience in doing compilers

00:01:29.220 --> 00:01:35.829
and garbage collectors, so hopefully we can
tell you about some of our findings over the

00:01:35.829 --> 00:01:42.759
last many years.
So why are we here? We're here because we

00:01:42.759 --> 00:01:48.409
truly believe that speed in web browsers fuel
innovations for application developers, right?

00:01:48.409 --> 00:01:53.700
The more speed we can give you, the more interesting
applications you can do. And we strongly believe

00:01:53.700 --> 00:01:58.799
in the fly-by web, right? Developers do one
application -- it runs on mobile phones and

00:01:58.799 --> 00:02:05.320
desktop systems -- and this is just fantastic.
And the source that makes it work is the virtual

00:02:05.320 --> 00:02:09.429
machine inside the browser.
And if we can make it faster, you can take

00:02:09.429 --> 00:02:14.140
advantage of it.
The question is, we have seen a lot of speedup

00:02:14.140 --> 00:02:20.900
in browsers over the last, I guess, five years.
Is it fast enough or do we want more speed?

00:02:20.900 --> 00:02:26.200
So we are of the opinion that we need to do
even better, and that is one of the reasons

00:02:26.200 --> 00:02:31.200
why we have started on this new venture to
do a Dart platform.

00:02:31.200 --> 00:02:38.560
&gt;&gt;Kasper Lund: So before we dive into the
technical details of virtual machines and

00:02:38.560 --> 00:02:44.780
programming for the web, I just wanted to
take you back to 2006 when we started developing

00:02:44.780 --> 00:02:51.650
V8. At that point, Web pages had evolved from
being fairly static to being a little bit

00:02:51.650 --> 00:02:56.849
more rich, and we were seeing sort of the
first Web applications appear. At the point

00:02:56.849 --> 00:03:02.290
in time, you were probably running Firefox
2.0, Internet Explorer 7.0, or some other

00:03:02.290 --> 00:03:08.000
browsers there, but it was very clear that
these browsers were not really designed for

00:03:08.000 --> 00:03:12.200
long-running, heavy Web applications. They
were designed for the web of two thousand-

00:03:12.200 --> 00:03:16.530
-- well, probably 2000, yeah.
And so it was an interesting at that point

00:03:16.530 --> 00:03:21.560
in time, 2006, it was just before we were
seeing these applications really come to life

00:03:21.560 --> 00:03:26.580
on the web. And even if you -- if you look
at the Web sites I've depicted up here, you

00:03:26.580 --> 00:03:30.959
can tell the difference, if you can actually
see it from the back of the room, that in

00:03:30.959 --> 00:03:34.670
2008, already then we were seeing pages like
BBC becoming a little bit more interactive

00:03:34.670 --> 00:03:40.060
and more rich.
So in 2006 when we set out to implement a

00:03:40.060 --> 00:03:44.260
new virtual machine for JavaScript for the
web, people believed that browsers were essentially

00:03:44.260 --> 00:03:48.939
fast enough. Web applications like Gmail and
Google Maps ran fine and people were very

00:03:48.939 --> 00:03:54.150
impressed with them. People also believed
that JavaScript was just inherently too slow

00:03:54.150 --> 00:04:00.230
for writing heavy client-side computations.
And even so, people didn't really expect the

00:04:00.230 --> 00:04:04.930
JavaScript to improve that much so they didn't
think that the execution of JavaScript was

00:04:04.930 --> 00:04:09.969
holding things back. They were okay with having
more code running on the server side and having

00:04:09.969 --> 00:04:15.270
fairly limited amounts of logic on the client
side.

00:04:15.270 --> 00:04:19.690
Another key thing for us when we started was
that performance at that point was evaluated

00:04:19.690 --> 00:04:24.630
using very simple micro-benchmarks, where
all the emphasis was put on simple loops with

00:04:24.630 --> 00:04:30.940
very simple arithmetic and nobody paid any
attention to the overhead for calling methods

00:04:30.940 --> 00:04:35.720
or how large applications would perform when
it comes to memory management.

00:04:35.720 --> 00:04:40.060
So people were really spending all their time
looking at small, simple loops and trying

00:04:40.060 --> 00:04:44.520
to make them run a little bit faster.
So here's an example of a benchmark that I

00:04:44.520 --> 00:04:47.210
think represents that line of thought pretty
well.

00:04:47.210 --> 00:04:51.800
This is a benchmark, a JavaScript benchmark,
from the SunSpider benchmark collection. It's

00:04:51.800 --> 00:04:57.130
known as "bit-wise and" and that's all it
does. It computes the bit-wise and for a series

00:04:57.130 --> 00:05:01.550
of values here, starting out with a fairly
large number and then it runs through a loop

00:05:01.550 --> 00:05:08.080
and you'll probably notice that even on the
iteration, I is zero so when you "and" that

00:05:08.080 --> 00:05:14.289
with anything, you'll get zero out. So this
is just a fancy way of computing zero.

00:05:14.289 --> 00:05:17.740
Not exactly something you want to spend a
lot of time on optimizing for, I guess, and

00:05:17.740 --> 00:05:21.540
certainly not something that would, if you
actually did optimize this, it would turn

00:05:21.540 --> 00:05:26.600
your application any faster.
So SunSpider is a benchmark that sort of represents

00:05:26.600 --> 00:05:32.150
some of the things and some of the problems
from sort of the earlier web, and they -- it

00:05:32.150 --> 00:05:36.090
is being updated every now and then but not
with any significant differences. This is

00:05:36.090 --> 00:05:41.600
from SunSpider 1.0 released a couple of weeks
ago. It's very nice to see that they've added

00:05:41.600 --> 00:05:46.280
testing that actually does compute zero but
it's still a really bad benchmark. Making

00:05:46.280 --> 00:05:50.560
this faster and tuning V8 for these kind of
things would not make your applications run

00:05:50.560 --> 00:05:55.710
any faster, so...
So when we started designing V8, we had to

00:05:55.710 --> 00:05:58.819
make a choice. Like what do we want to optimize
for?

00:05:58.819 --> 00:06:02.960
And there are basically two approaches.
You can optimize for the current apps and

00:06:02.960 --> 00:06:07.710
the current benchmarks, and that's a nice
and simple incremental approach, and there's

00:06:07.710 --> 00:06:13.539
a good chance you can make things 10%, maybe
20% faster that way. And we chose to optimize

00:06:13.539 --> 00:06:18.020
for the apps of the future at that point so
we decided that we wanted to support much

00:06:18.020 --> 00:06:21.910
heavier side client-side computations and
we wanted to turn the browser into a scalable

00:06:21.910 --> 00:06:27.830
application platform, sort of thus enabling
a new class of Webapps that could be written.

00:06:27.830 --> 00:06:33.169
So optimizing a brand-new virtual machine
for some applications that you cannot write

00:06:33.169 --> 00:06:39.770
-- or you couldn't write in 2006 -- required
us to come up with some benchmarks and some

00:06:39.770 --> 00:06:42.949
metrics that would allow us to make progress
on that problem.

00:06:42.949 --> 00:06:47.280
Which is where the V8 benchmark suite entered
the game.

00:06:47.280 --> 00:06:51.590
So the V8 benchmark suite is a collection
of benchmarks that are structured and mostly

00:06:51.590 --> 00:06:58.710
object-oriented, and they're designed to push
the limits of browsers in 2006 but also today,

00:06:58.710 --> 00:07:03.660
and it's benchmarks that are mostly proven
valuable in the context of lots of other languages.

00:07:03.660 --> 00:07:07.700
So it's benchmarks that we know that if we
optimize for those, it will actually make

00:07:07.700 --> 00:07:11.599
it possible for you guys to write much larger
and much more interesting applications.

00:07:11.599 --> 00:07:17.319
It measures the performance of dynamic message
calls and property access and it also measures

00:07:17.319 --> 00:07:21.069
the performance of the memory management system,
and closure creation and location, to a much

00:07:21.069 --> 00:07:23.970
higher degree than any of the existing benchmarks
in 2006.

00:07:23.970 --> 00:07:29.539
&gt;&gt;Lars Bak: But Kasper, can I get the mic?
Thank you. Let's try to run it so we can see

00:07:29.539 --> 00:07:33.759
how it looks.
So here, let's search for the V8 benchmark

00:07:33.759 --> 00:07:39.580
suite and we can run Version 7 of it, and
it will start running and you'll see the results

00:07:39.580 --> 00:07:43.660
trickling in on the right-hand side and you
can see the collection of benchmarks we are

00:07:43.660 --> 00:07:47.600
running. It's eight different benchmarks.
They're not big, but they are certainly not

00:07:47.600 --> 00:07:53.580
micro-benchmarks.
And we should soon have the results.

00:07:53.580 --> 00:08:01.039
The interesting part is that the score you
see is computed as the geometric mean between

00:08:01.039 --> 00:08:09.669
individual scores but it's calibrated for
Firefox 2 on a high-end desktop machine in

00:08:09.669 --> 00:08:17.280
2008.
So we are pretty much running 160 times faster

00:08:17.280 --> 00:08:20.810
than one of the browsers in 2008, so this
is pretty amazing.

00:08:20.810 --> 00:08:25.889
&gt;&gt;Kasper Lund: Yeah. The score was 100 on
Firefox 2 on my beefy desktop machine in 2008.

00:08:25.889 --> 00:08:31.289
It's a little bit better now. Which is nice.
[ Laughter ]

00:08:31.289 --> 00:08:39.170
&gt;&gt;Kasper Lund: So I think it's fair to conclude
that the performance improvements for JavaScript

00:08:39.170 --> 00:08:43.630
over the last eight years have been pretty
impressive. JavaScript itself executes a hundred

00:08:43.630 --> 00:08:48.840
times faster than it did before, at least,
and we see people being able to write applications

00:08:48.840 --> 00:08:54.160
that use way bigger object heaps, and the
GC pauses are even lower than they were at

00:08:54.160 --> 00:08:58.020
that point.
Another key thing for us as compilers and

00:08:58.020 --> 00:09:02.170
VM engineers is that people pay attention
now to performance of their Webapps and it's

00:09:02.170 --> 00:09:07.570
very, very common to see benchmark numbers
reported in the press, so it's a -- there's

00:09:07.570 --> 00:09:12.690
a nice friendly competition going on to improve
the web platform performance.

00:09:12.690 --> 00:09:16.160
Another thing that's very nice to see is that
it's possible to write really large Webapps

00:09:16.160 --> 00:09:22.680
these days. We see common pages like Amazon,
CNN, and ESPN have -- they're serving fairly

00:09:22.680 --> 00:09:28.160
large amounts of JavaScript codes to their
customers all day, so it's not uncommon to

00:09:28.160 --> 00:09:34.770
see like a megabyte of JavaScript code, minified
JavaScript code, being sent to clients. And

00:09:34.770 --> 00:09:38.900
the browsers are generally capable of working
with that. So things have certainly changed

00:09:38.900 --> 00:09:44.750
since 2006.
One thing that is important for us is to pay

00:09:44.750 --> 00:09:49.500
attention to where the time is actually spent
in your browser, and what you have here is

00:09:49.500 --> 00:09:57.450
a couple of diagrams that show that we're
spending around 60% of the time in JavaScript

00:09:57.450 --> 00:10:01.570
execution when you're running things like
Gmail, Google Docs, Google Search, and things

00:10:01.570 --> 00:10:06.610
like that, and the rest is spent in the general
browser infrastructure.

00:10:06.610 --> 00:10:09.750
On other sites like Twitter and Facebook,
the percentage in JavaScript is a little bit

00:10:09.750 --> 00:10:15.590
lower than that. And these measurements are
all done by the Chrome team using Chrome,

00:10:15.590 --> 00:10:20.070
so we don't have the exact same numbers for
the other browsers. But it shows that no matter

00:10:20.070 --> 00:10:25.580
how fast we make the engines, the application
developers will adapt to that and add new

00:10:25.580 --> 00:10:30.530
functionality and essentially make their applications
better by using this thing.

00:10:30.530 --> 00:10:36.190
So imagine how it would feel like running
modern Gmail on a browser from 2006. I don't

00:10:36.190 --> 00:10:42.670
think you would like that at all.
So today, web developers are really pushing

00:10:42.670 --> 00:10:46.880
the limits of the platform and they really
want and demand predictable high performance.

00:10:46.880 --> 00:10:49.970
Higher and higher, actually.
They want consistent frame rates for their

00:10:49.970 --> 00:10:54.000
games. And they really want to support large-scale
application development.

00:10:54.000 --> 00:10:58.210
So people's expectations for this platform
have really changed. They really demand these

00:10:58.210 --> 00:11:02.870
things now, and all it took is 100X performance
improvement. It makes a really, really big

00:11:02.870 --> 00:11:06.120
change to the technology stack when you do
these things.

00:11:06.120 --> 00:11:12.690
So the question is, of course, are the web
developers getting predictable and high performance,

00:11:12.690 --> 00:11:16.120
and can they actually support really large-scale
application development on the current web

00:11:16.120 --> 00:11:18.790
stack?
So before we answer that question, I think

00:11:18.790 --> 00:11:22.610
we should dive into the -- the current web
stack and take a look at how we can improve

00:11:22.610 --> 00:11:29.300
the parts.
&gt;&gt;Lars Bak: All right. Now we'll dive into

00:11:29.300 --> 00:11:32.930
what actually happens inside the virtual machine
and the browser.

00:11:32.930 --> 00:11:37.730
So I'll be talking about a few different techniques
we've been using to make V8 fast and I'll

00:11:37.730 --> 00:11:45.460
talk about the history behind these technologies.
Firstly, let's look at a browser. So a browser

00:11:45.460 --> 00:11:52.150
has several parts. But if we look at the performance-critical
part for a Webapp, it actually consists of

00:11:52.150 --> 00:11:56.060
two segments.
At the lower part, we have the JavaScript

00:11:56.060 --> 00:12:02.690
engine where you have all the code and the
objects and the WebObjects that point to the

00:12:02.690 --> 00:12:08.711
DOM. The DOM consists of the -- the nodes
that you can see on the screen. And when you

00:12:08.711 --> 00:12:15.060
execute your Web application, you manipulate
the DOM by executing JavaScript.

00:12:15.060 --> 00:12:21.430
We all hope that when you develop a Web application,
you have low latency so it comes up fast and

00:12:21.430 --> 00:12:27.920
there's no pauses, it has high performance,
and it also has low memory usage so you don't

00:12:27.920 --> 00:12:32.230
end up swapping when you run your application.
And then, of course, more pauses if you do

00:12:32.230 --> 00:12:37.110
animation. But there are still big fears when
you run a Web application, right?

00:12:37.110 --> 00:12:43.140
If the object heap is too big, you can get
very large GC pauses, and you can have memory

00:12:43.140 --> 00:12:49.180
leaks. So having a Web application that runs
for weeks and suddenly it runs out of memory

00:12:49.180 --> 00:12:53.750
is not very pleasant for the user.
And sometimes you'll get erratic performance

00:12:53.750 --> 00:12:59.450
behavior for no good reasons.
And this is sort of some of the issues people

00:12:59.450 --> 00:13:08.210
are concerned about when writing for the web.
Here is my picture of how a virtual machine

00:13:08.210 --> 00:13:11.730
or a JavaScript engine looked like in the
year 2006.

00:13:11.730 --> 00:13:17.790
It had a simple parser that would take the
source code and convert into an abstract syntax

00:13:17.790 --> 00:13:22.270
tree or into byte codes and then it would
have a simplistic interpreter that would run

00:13:22.270 --> 00:13:29.120
over the AST, abstract syntax tree, or the
byte codes and execute the program.

00:13:29.120 --> 00:13:33.480
And then it would have a simple memory management
system to clean out unused memory.

00:13:33.480 --> 00:13:39.010
Now, in 2006 you would not create many objects
because you knew that if you created a lot

00:13:39.010 --> 00:13:44.000
of objects, your program would be slow, so
you actually didn't use many of the object-oriented

00:13:44.000 --> 00:13:49.040
qualities of the system.
So the conclusion is, it was very simple and

00:13:49.040 --> 00:13:53.890
that's nice that you can write this kind of
VM in a few thousand lines, but the downside

00:13:53.890 --> 00:13:59.810
was it was really slow.
Today, we have a sports car and I can guarantee

00:13:59.810 --> 00:14:06.060
you we have put in all kinds of fancy turbo
engines in it to make it fast, right? We still

00:14:06.060 --> 00:14:12.560
have a parser, but instead of an interpreter,
we have a multi-tier adaptive compilation

00:14:12.560 --> 00:14:18.600
system. It's basically a -- it's a series
of compilers that learns from the behavior

00:14:18.600 --> 00:14:24.450
of the program and tries to optimize the execution
based on behavior.

00:14:24.450 --> 00:14:30.330
We have a de-optimization system that allows
the system to back out of optimization if

00:14:30.330 --> 00:14:36.220
the behavior of the program changes over time.
And then we have a system that handles large

00:14:36.220 --> 00:14:42.890
object heaps. And in the V8 case, we have
a generation-based garbage collector that

00:14:42.890 --> 00:14:47.720
allows us to have heaps up to a gigabyte,
at least, where you can still have reasonable

00:14:47.720 --> 00:14:54.870
pauses when running your program.
Native code is generated on the fly and is

00:14:54.870 --> 00:15:00.200
generated by the buckets, I guarantee you,
and what will happen when it's not used anymore,

00:15:00.200 --> 00:15:05.410
it will be removed by the garbage collector.
So even return addresses on the execution

00:15:05.410 --> 00:15:11.390
stack will function like pointers into the
code, and if they are not used anymore, the

00:15:11.390 --> 00:15:16.310
code will be eliminated.
And then of course there's all this tool sport

00:15:16.310 --> 00:15:19.529
with debugging and profiling.
Because when you do optimizations -- right?

00:15:19.529 --> 00:15:25.180
-- you sort of lose the understanding of what
the source code is doing, so you cannot single-step

00:15:25.180 --> 00:15:28.080
an optimized code. It will give no meaning
to you.

00:15:28.080 --> 00:15:33.630
So what we try to do is we try to de-optimize
the code. If we try to single-step, it will

00:15:33.630 --> 00:15:39.100
look like you do single-stepping in the normal
source code. And that's great for a programmer,

00:15:39.100 --> 00:15:43.570
because then you can debug on an odd-based
system or an intra-based system and get exactly

00:15:43.570 --> 00:15:50.020
the same behavior.
It's really complex, but the good thing is,

00:15:50.020 --> 00:15:55.200
it's fast.
Let's go beyond the hood and see what's going

00:15:55.200 --> 00:15:58.830
on inside the V8 engine when it comes to a
few things.

00:15:58.830 --> 00:16:04.820
I'll briefly mention how we make the code
really fast, and this is this multi-tier adaptive

00:16:04.820 --> 00:16:09.220
compilation system.
I'll talk about how we handle large object

00:16:09.220 --> 00:16:14.860
heaps by doing this generation-based garbage
collection with a twist.

00:16:14.860 --> 00:16:22.260
And then I'll talk about how we bind JavaScript
objects to the DOM nodes.

00:16:22.260 --> 00:16:28.940
And this is -- this is tricky, so I'm calling
it the tangoing between the tracing GC and

00:16:28.940 --> 00:16:36.530
reference counting.
So listen up. This might be a little bit tricky

00:16:36.530 --> 00:16:41.870
here.
When you parse the code, the code is blue.

00:16:41.870 --> 00:16:47.370
You have a sea of JavaScript methods. Nothing
has been executed yet. When you start executing,

00:16:47.370 --> 00:16:52.640
the first simplistic compiler will take the
source code and convert into native code,

00:16:52.640 --> 00:16:55.570
right?
It doesn't know much about the behavior of

00:16:55.570 --> 00:17:01.670
the program, so what it's doing is putting
in inline cache stops that tracks the behavior

00:17:01.670 --> 00:17:06.160
of the program. So in essence, what it's doing
at one call site in the program, it tries

00:17:06.160 --> 00:17:11.660
to figure out exactly what kind of type of
objects comes by.

00:17:11.660 --> 00:17:16.690
At the same time, we have this monitoring
system in the system that detects when a method

00:17:16.690 --> 00:17:21.850
is used frequently. If it's used frequently,
we said, "Well, let's try to optimize it even

00:17:21.850 --> 00:17:27.430
further." And the optimizing native compiler,
what it will do, it will take the collected

00:17:27.430 --> 00:17:34.000
type information from the inline caches and
use that to do aggressive inlining.

00:17:34.000 --> 00:17:38.920
So it basically assumes that the behavior
of the program you had before you optimized

00:17:38.920 --> 00:17:45.380
will be the same in all future. It's not always
the case, but we're optimistic here.

00:17:45.380 --> 00:17:50.660
So we do these aggressive inlining decisions,
but we put in the optimization hook so that

00:17:50.660 --> 00:17:56.350
if our assumptions are wrong in the optimized
code, we can go back again to the naive, simple

00:17:56.350 --> 00:17:59.630
code.
In most cases, we don't have to do that and

00:17:59.630 --> 00:18:05.360
you'll get the predictable performance, which
is great.

00:18:05.360 --> 00:18:12.360
It gets even harder -- right? -- because as
you all know, there's no types in JavaScript,

00:18:12.360 --> 00:18:17.720
and that means we have to do funky stuff.
One funky thing we are doing is that if you

00:18:17.720 --> 00:18:24.480
use an array and you put doubles in it, at
some point at runtime we decide to optimize

00:18:24.480 --> 00:18:30.570
it by converting the representation of an
object from an array that can have any kind

00:18:30.570 --> 00:18:36.050
of objects to a representation that can only
have double values. Right?

00:18:36.050 --> 00:18:40.350
So we assume that moving forward, we will
only operate on double values in this array.

00:18:40.350 --> 00:18:44.380
That will certainly speed up programs that
continue to have that behavior, but you still

00:18:44.380 --> 00:18:49.050
need the hooks if something from the side
would put in an integer. Or not an integer.

00:18:49.050 --> 00:18:55.980
Let's say a -- a string in that array.
Then we have to back out of that optimization

00:18:55.980 --> 00:19:00.620
and go back to the standard representation
of an object array. Very complicated.

00:19:00.620 --> 00:19:09.310
But anyways, you have a system that's always
changing when you run it.

00:19:09.310 --> 00:19:14.910
Did we invent this multi-tier adaptive compilation?
No, we are not that smart. There is actually

00:19:14.910 --> 00:19:24.830
a long history of implementing dynamic languages,
started with interpretation in the '50s with

00:19:24.830 --> 00:19:29.930
Lisp. In the '70s, we have dynamic compilation
being added to Smalltalk. And they even had

00:19:29.930 --> 00:19:34.990
inline caching. It was not at the call side,
but they put it at the callee. So instead

00:19:34.990 --> 00:19:38.750
of having it -- the method calling it, it
was actually at the prologue of the method

00:19:38.750 --> 00:19:43.570
you called.
And then the adaptive compilation was invented

00:19:43.570 --> 00:19:48.890
in the '90s in the sales project where you
have mixed mode execution, runtime-type feedback

00:19:48.890 --> 00:19:53.840
and all this kind of stuff.
And then they also came up with deoptimization.

00:19:53.840 --> 00:19:58.160
So let me try to explain what deoptimization
is. It is actually very simple.

00:19:58.160 --> 00:20:03.540
When you have the optimized code and you have
activations on the stack that's executing

00:20:03.540 --> 00:20:08.820
and you figure out that something is wrong,
you need to revert. What you do is you take

00:20:08.820 --> 00:20:17.810
that optimized activation and convert it into
a series of unoptimized activity on the stack,

00:20:17.810 --> 00:20:24.660
flush the optimized code, and start using
the simple code again.

00:20:24.660 --> 00:20:29.650
This first appeared commercially in the HotSpot
JVM. I assume you all know about what that

00:20:29.650 --> 00:20:34.800
is.
The other thing we did in V8 to basically

00:20:34.800 --> 00:20:41.480
use all this technology was introducing behind-the-scenes
classes. You know that JavaScript is prototype-based,

00:20:41.480 --> 00:20:46.600
but we faked them behind the scenes in event
classes. Then you can apply all this technology

00:20:46.600 --> 00:20:57.870
on the slide, and that made it fast.
The garbage collector in V8 is here. It might

00:20:57.870 --> 00:21:03.590
look complicated. We basically have two generations.
And the way it works is, as you are allocating

00:21:03.590 --> 00:21:10.360
optics in your program, we will fill up the
allocation space. It is called "from" here.

00:21:10.360 --> 00:21:16.160
When it is full, we will start a garbage collection
process. And we have a tracing garbage collector.

00:21:16.160 --> 00:21:20.650
That means that we have well-defined routes,
and we start following these routes through

00:21:20.650 --> 00:21:28.240
the optics, through its references until we
have computed the complete live optigraph.

00:21:28.240 --> 00:21:33.730
And memory that's not occupied by the live
optigraph is garbage, and it can be reclaimed

00:21:33.730 --> 00:21:38.170
and used for further objects.
Let's see what happens when you do a use-space

00:21:38.170 --> 00:21:47.440
collection in V8. We migrate the optics to
the "to" space. And you can see it is smaller,

00:21:47.440 --> 00:21:51.820
and that is because most optics die young.
And this is great.

00:21:51.820 --> 00:21:59.040
Then we can start allocating and filling up
the "to" space and repeat the same story.

00:21:59.040 --> 00:22:06.530
The garbage collector in V8 is a precise garbage
collector. It stops the world and processes

00:22:06.530 --> 00:22:13.240
the garbage and then continues on.
You can see you have red pointers in the graph.

00:22:13.240 --> 00:22:18.830
It denotes that the system has a stop buffer
that tracks pointers from old generation to

00:22:18.830 --> 00:22:24.210
the new generation. And you need that if you
want to do fast use-space garbage collection.

00:22:24.210 --> 00:22:29.560
You don't have to scan the whole heap in order
to do a small garbage collection.

00:22:29.560 --> 00:22:38.010
The old space in V8 has been segmented into
normal optics. That's the optics that's promoted

00:22:38.010 --> 00:22:44.100
from use-space when they have survived for
a while, and then native code where you have

00:22:44.100 --> 00:22:50.390
native instructions, and then a segment where
you have atomic data. Atomic data is data

00:22:50.390 --> 00:22:56.600
that does not have pointers out again. So
it could be typed arrays or double optics,

00:22:56.600 --> 00:23:00.320
something like that. That is easier to handle
in a separate space because they don't have

00:23:00.320 --> 00:23:06.900
pointers out.
Anyways, we didn't invent that either. Sorry.

00:23:06.900 --> 00:23:14.181
There's a long history of automatic memory
management done in Lisp in the '50s. Incremental

00:23:14.181 --> 00:23:19.770
garbage collection done in the '70s in Lisp
as well. And then you have generational-based

00:23:19.770 --> 00:23:26.240
garbage collection that got invented by the
one guy in the '80s and generationally scavenged

00:23:26.240 --> 00:23:31.320
any use of Smalltalk.
So all these techniques for doing garbage

00:23:31.320 --> 00:23:35.970
collection is fairly simple in my mind because
you have all the pointers to the optics and

00:23:35.970 --> 00:23:40.530
you can just process them.
The hard part is actually dancing with the

00:23:40.530 --> 00:23:49.880
DOM. And I will try to tell you how that works.
So nodes in DOM is going by what we call reference

00:23:49.880 --> 00:23:55.500
counting. Reference counting is a simplistic
garbage collection technology that keeps a

00:23:55.500 --> 00:24:02.610
pointer in each optic that tells how many
pointers point to it. When that pointer goes

00:24:02.610 --> 00:24:08.510
to zero, you can de-allocate the optic.
The problem is you don't know who points to

00:24:08.510 --> 00:24:14.380
it. You just know how many. That means that
if you want to move the optic memory, you

00:24:14.380 --> 00:24:17.900
cannot do that because you cannot find how
to point to it.

00:24:17.900 --> 00:24:24.670
And another deficiency of that mechanism is
also if you have a cycle, which can happen,

00:24:24.670 --> 00:24:32.230
then it's a memory leak because you will never
get down to zero in the reference count.

00:24:32.230 --> 00:24:37.330
And the problem is JavaScript V8 has optics
that are traced and we have to get all this

00:24:37.330 --> 00:24:44.630
to work.
So, here we go. Bear with me. This is complicated.

00:24:44.630 --> 00:24:50.550
On the top you see the DOM. You have two nodes.
The first node has a reference count of 1.

00:24:50.550 --> 00:24:57.940
The second one has a count of 2. And you have
wrapper optics that point to these DOM nodes

00:24:57.940 --> 00:25:06.070
from beneath, and that is the V8 heap.
Now, in order to make sure you can collect

00:25:06.070 --> 00:25:10.850
these optics, you have to make tricks. And
you can see you have this pairing between

00:25:10.850 --> 00:25:17.430
JavaScript optics and DOM nodes by using repointers.
So, in essence, you have a JavaScript wrapper

00:25:17.430 --> 00:25:22.440
that points to a DOM node that in turn points
to a persistent handle -- that's the black

00:25:22.440 --> 00:25:28.200
ones -- that, in turn, will use a repointer
pointing back. So it is sort of a semicycle,

00:25:28.200 --> 00:25:35.470
and you break the cycle by using repointers.
So that is simple.

00:25:35.470 --> 00:25:41.130
But it gets even more complicated. When V8
is running out of memory and has to perform

00:25:41.130 --> 00:25:47.040
a garbage collection, it has to call the DOM
and ask the DOM to group all objects that

00:25:47.040 --> 00:25:53.930
belong together because that's needed in order
to only collect -- you basically have to collect

00:25:53.930 --> 00:26:02.290
all grouped optics at the same time. That's
needed to avoid resurrection of wrapper optics.

00:26:02.290 --> 00:26:07.840
That might seem complicated, but it is needed
because if you add a property to one of the

00:26:07.840 --> 00:26:13.260
JavaScript nodes at the bottom and it suddenly
disappears and reappears again, the property

00:26:13.260 --> 00:26:19.350
is gone and the application will get a little
bit confused. So we need to do all this in

00:26:19.350 --> 00:26:27.790
order to make sure garbage collection works.
Did we invent this? Actually, that's the only

00:26:27.790 --> 00:26:38.800
thing we did. It works. Chrome is great. But
that part we are not proud of. It needs a

00:26:38.800 --> 00:26:44.890
lot more work in order to make sure that we
can guarantee that all memory is collected

00:26:44.890 --> 00:26:48.770
the right way. But we will get back to this
later in this talk, what we are doing about

00:26:48.770 --> 00:26:57.840
it.
Does this mean that V8 is as good as it gets?

00:26:57.840 --> 00:27:02.460
Right? We have taken all these techniques
from the past, advanced codebase techniques

00:27:02.460 --> 00:27:07.110
with hidden classes and sophisticated memory
management.

00:27:07.110 --> 00:27:14.520
Is this what we have, or do we want more performance?
Well, from the feedback we're getting, there's

00:27:14.520 --> 00:27:23.390
still a lot of bottlenecks in the browser.
Performance is not still at par with real

00:27:23.390 --> 00:27:27.870
languages. You can define what "real" is your
own way. But I would sort of at least say

00:27:27.870 --> 00:27:37.220
Java and C# as contenders.
Performance is still unpredictable. And you

00:27:37.220 --> 00:27:41.500
can see if you do animation in the browser,
sometimes you skip frames. And part of it

00:27:41.500 --> 00:27:47.100
is because you do too much compilation. Part
of it is because you do garbage collections.

00:27:47.100 --> 00:27:51.480
Part of it is because you are deoptimized
because suddenly the assumption changed in

00:27:51.480 --> 00:27:55.630
the program.
And in JavaScript, that's very easy to do

00:27:55.630 --> 00:28:01.550
because just jamming a lot of property on
a hot object, bam, performance changes.

00:28:01.550 --> 00:28:07.390
Startup is still slow. And the reason for
that is that you still have to read in the

00:28:07.390 --> 00:28:10.610
source code whenever you have to start up
the program.

00:28:10.610 --> 00:28:18.420
And, of course, JavaScript is still JavaScript.
There's a lot of implicit conversions of values

00:28:18.420 --> 00:28:27.470
when you run JavaScript. It is just complicated.
So having said that, the V8 project is still

00:28:27.470 --> 00:28:34.500
doing excellent. They are still improving
performance quarter after quarter. But we

00:28:34.500 --> 00:28:42.059
think that if we really want to push the performance
to the next level, something else is needed.

00:28:42.059 --> 00:28:47.430
Let's look at performance over time when it
comes to V8. This is a graph that shows the

00:28:47.430 --> 00:28:54.190
V8 benchmark score from the different versions
of Chrome, all the way up to today.

00:28:54.190 --> 00:29:04.630
And it's gone from 3,800 all the way up to
14,000. So it certainly has improved a lot.

00:29:04.630 --> 00:29:14.130
So this is great. But there's a downside.
Here's the complexity over time of V8. And

00:29:14.130 --> 00:29:19.840
we measure complexity by number of lines of
code in the VM.

00:29:19.840 --> 00:29:25.970
When we came out of V8, it was roughly 100,000
lines of code. Now it's half a million lines

00:29:25.970 --> 00:29:31.790
of code. And I don't know -- as a programmer,
you know that there's trouble when it gets

00:29:31.790 --> 00:29:39.360
so big. It takes way more effort in order
to improve performance. So we question whether

00:29:39.360 --> 00:29:45.220
that's possible to get a factor of 2 in V8
in the near time. You can follow the graph.

00:29:45.220 --> 00:29:51.170
Factor of 2 will take a long time, unless
it is rewritten, of course. This also would

00:29:51.170 --> 00:29:56.830
be a fun project.
&gt;&gt;Kasper Lund: So we've gone through a lot

00:29:56.830 --> 00:30:03.780
of Web technology stuff here and looked into
how V8 works and why it is as fast as it is.

00:30:03.780 --> 00:30:12.870
Let's dive into the Dart side and look at
how Dart improves this. So Dart is a simple

00:30:12.870 --> 00:30:18.720
class-based unsurprising programming language.
It is designed to be very familiar so that

00:30:18.720 --> 00:30:23.690
Web developers and application developers
can pick it up and learn to use it in no time.

00:30:23.690 --> 00:30:27.410
We've got excellent feedback from users of
the system that this really works. You can

00:30:27.410 --> 00:30:32.860
sit down and code in Dart. And within a few
hours, you feel like you are very productive

00:30:32.860 --> 00:30:39.670
and it actually works out really well.
To hit that target of being very familiar,

00:30:39.670 --> 00:30:44.860
we've designed it based on principles and
concepts found in other languages. So the

00:30:44.860 --> 00:30:49.191
syntax is very much inspired by JavaScript,
Java, C#, those kind of languages. And we

00:30:49.191 --> 00:30:54.929
get a lot of other value from essentially
standing on the shoulders of other good programming

00:30:54.929 --> 00:30:58.681
languages out there and picking the things
that we feel are good fits for a language

00:30:58.681 --> 00:31:02.290
that needs to be really efficient and at the
same time very productive.

00:31:02.290 --> 00:31:07.150
One thing that we've added to Dart, which
is a little bit uncommon is support for optional

00:31:07.150 --> 00:31:11.480
static types. That means that you can write
static-type notations where you want them,

00:31:11.480 --> 00:31:15.740
where you feel like they convey the intent
that you want to convey. But you don't have

00:31:15.740 --> 00:31:19.960
to put them in. The system is very flexible
that way. It doesn't require you, and it is

00:31:19.960 --> 00:31:24.830
a fully dynamically typed system. It is hard
to describe this in one slide without showing

00:31:24.830 --> 00:31:30.080
some code. So I will use two.
Here is a little bit of a taste of Dart. I

00:31:30.080 --> 00:31:33.270
don't know how many of you are familiar with
Dart, tried it out. But for those of you who

00:31:33.270 --> 00:31:37.140
are seeing it for the first time, I hope you
will find that this is reasonably readable.

00:31:37.140 --> 00:31:43.220
It is a very simple example. It shows that
all Dart applications start with a main function.

00:31:43.220 --> 00:31:48.110
And in this case, I just added a little bit
of an HTML interaction here. I am creating

00:31:48.110 --> 00:31:54.300
a new button element, adding that to the document
body, and giving it a bit of text.

00:31:54.300 --> 00:31:59.179
Hopefully, you can read this code if you are
familiar with JavaScript, C#, Java, those

00:31:59.179 --> 00:32:02.530
kind of languages and it doesn't feel too
foreign for you. That's exactly what we wanted

00:32:02.530 --> 00:32:05.850
to try to get at.
One thing that might look a little bit different

00:32:05.850 --> 00:32:12.170
here is the import, and it just shows that
Dart has support for modularity. So you can

00:32:12.170 --> 00:32:17.800
import libraries of functionality and code
and use that. So we do have a namespacing

00:32:17.800 --> 00:32:23.780
mechanism and a way of carving up your application
in multiple independent bits. So I hope this

00:32:23.780 --> 00:32:30.110
seems reasonably easy to read for you guys.
I think a good question to ask at this point

00:32:30.110 --> 00:32:35.179
is: What are we trying to achieve with Dart?
And we feel that there is a need for a much

00:32:35.179 --> 00:32:40.010
more scalable development platform for Web
apps. We feel like it is too costly and too

00:32:40.010 --> 00:32:44.581
difficult to write great Web apps. You can
achieve amazing things on the Web today, but

00:32:44.581 --> 00:32:47.660
you really have to invest a lot of time in
it. Things are getting better, but we feel

00:32:47.660 --> 00:32:51.710
like we need something like Dart to push the
limits of this and make it much, much easier

00:32:51.710 --> 00:32:57.250
to write large, well-functioning applications.
We think there is a real need for higher performance

00:32:57.250 --> 00:33:02.929
and much faster startup. And we certainly
feel like having predictable performance where

00:33:02.929 --> 00:33:08.670
it's harder to write applications that really
perform poorly because of somewhat weird semantics

00:33:08.670 --> 00:33:14.460
in the core of the system that you're building
on is a must.

00:33:14.460 --> 00:33:19.190
Another thing that we consistently hear from
users of Dart and other systems is that having

00:33:19.190 --> 00:33:23.799
great toolability, being able to write tools
and use tools on your codebases for refactoring,

00:33:23.799 --> 00:33:30.190
editing, and just maneuvering around, navigating
your code is a great big help. And the static

00:33:30.190 --> 00:33:34.660
types that we have, even though they are optional,
they are a big help here, to just document

00:33:34.660 --> 00:33:39.400
your intent of your code and make use of it.
And, finally, I think it is very important

00:33:39.400 --> 00:33:46.890
for us that we make it easy and nice to use
a set of consistent libraries and make it

00:33:46.890 --> 00:33:51.351
easy for the community to create new libraries
and share them in the same way. Certainly

00:33:51.351 --> 00:33:56.080
an area where the Web suffered a little bit.
We see great things out there written for

00:33:56.080 --> 00:33:59.990
JavaScript, but it is not necessarily trivial
to make things fit together if you have multiple

00:33:59.990 --> 00:34:03.559
independent pieces of JavaScript functionality
you want to bring together. We are trying

00:34:03.559 --> 00:34:06.580
to make that better with Dart, so that's what
we are trying to achieve.

00:34:06.580 --> 00:34:12.710
Clearly, we want Dart to be useful in all
browsers, all modern browsers. So we've written

00:34:12.710 --> 00:34:18.419
a translator that translates Dart code to
JavaScript that runs in all modern browsers.

00:34:18.419 --> 00:34:21.840
That translator itself is actually written
in Dart, so we are trying to dogfood our own

00:34:21.840 --> 00:34:26.369
product here. And that has been great experience
for us, to write a fairly significant amount

00:34:26.369 --> 00:34:31.310
of code in Dart, translate -- in a translator
that translates Dart to JavaScript can actually

00:34:31.310 --> 00:34:36.720
translate itself so you can take that big
piece of code and translate it to JavaScript

00:34:36.720 --> 00:34:42.090
and then you have a Dart-to-JavaScript translator
in JavaScript. That's kind of neat. It runs

00:34:42.090 --> 00:34:47.350
across all modern browsers. That's a big one
here.

00:34:47.350 --> 00:34:53.250
So the users that actually try using Dart
are fairly happy with some of the decisions

00:34:53.250 --> 00:35:00.800
we've made. And Thomas from an Austrian startup,
Blossom, describes this fairly accurately

00:35:00.800 --> 00:35:05.140
in one of his quotes here. He says that Dart
is exactly what he needs to be productive

00:35:05.140 --> 00:35:11.160
on the Web. So the feedback that we're getting
on the productivity gains of using something

00:35:11.160 --> 00:35:13.770
like Dart for your Web applications is very,
very positive.

00:35:13.770 --> 00:35:19.869
I will let this slide stay up for two more
seconds so you can read it.

00:35:19.869 --> 00:35:32.160
&gt;&gt;Lars Bak: All right. Well, for me it is
good just to have a choice of another system,

00:35:32.160 --> 00:35:37.180
if it fits your needs better.
But let's go a little bit deeper when it comes

00:35:37.180 --> 00:35:44.870
to performance. I'd like to talk about why
we can make Dart faster than JavaScript. In

00:35:44.870 --> 00:35:50.720
some cases, it already is.
Right. We are VM engineers. We have done that

00:35:50.720 --> 00:35:57.870
for most of our professional lives. And we
designed Dart to make sure we could optimize

00:35:57.870 --> 00:36:03.410
it. Maybe it is a little bit selfish, but
it is basically so we can get better performance.

00:36:03.410 --> 00:36:09.610
So the language model, the language semantics,
is very simple in Dart. It has a much simpler

00:36:09.610 --> 00:36:13.060
optic model.
It means that when you first have allocated

00:36:13.060 --> 00:36:18.240
an optic with fields, you cannot change it
after the allocation point. That's it. And

00:36:18.240 --> 00:36:22.830
it allows us to have much faster access to
fields, right? Just like in C# or in Java,

00:36:22.830 --> 00:36:28.070
you can actually access the nth element because
you know that that is exactly where the foo

00:36:28.070 --> 00:36:32.570
property is.
Programmers are declared so you don't have

00:36:32.570 --> 00:36:39.330
to run Dart in order to set up the program
in contrast to JavaScript. That allows us

00:36:39.330 --> 00:36:46.080
to use snapshots for having fast startup of
applications. We like that feature.

00:36:46.080 --> 00:36:52.100
And then we just have fewer special corner
cases to worry about. As an example of a corner

00:36:52.100 --> 00:36:56.350
case in JavaScript is that, well, you might
actually like it as a programmer that you

00:36:56.350 --> 00:37:02.490
can call a method with too many parameters
or too few. Well, this is sort of nice but

00:37:02.490 --> 00:37:07.100
in the implementation, somebody has to figure
out what you called it with and that costs,

00:37:07.100 --> 00:37:11.030
right? And if you don't provide enough argument,
somebody has to provide them for you, the

00:37:11.030 --> 00:37:17.180
underlying system. And stuff like that makes
the code more complicated to generate and

00:37:17.180 --> 00:37:24.260
the resultant code is also more loaded.
Let's give an example. Here I tried to make

00:37:24.260 --> 00:37:29.960
a side-by-side comparison between Dart and
JavaScript and what code it generates and

00:37:29.960 --> 00:37:36.600
how it works. On the right side, you can see
that we have two classes. We have a class

00:37:36.600 --> 00:37:43.780
A. It has a method called foo. It prints out
foo. And then we have a subclass B that inherits

00:37:43.780 --> 00:37:51.369
from -- or extends A. It has nothing. And
you create a new B -- it is called b, lowercase

00:37:51.369 --> 00:37:57.630
b. And the main function on the right, you
call foo. Very, very simple. We try to mimic

00:37:57.630 --> 00:38:02.869
the same in JavaScript. You have to go to
the left of the screen. The way we model inheritance

00:38:02.869 --> 00:38:09.369
in JavaScript is by using prototypes, so we
make a function A. And in the prototype of

00:38:09.369 --> 00:38:13.490
A, we put in the foo function, just like on
the right.

00:38:13.490 --> 00:38:22.240
And then we make a function B that calls constructor
A. And in the B prototype, we don't do anything.

00:38:22.240 --> 00:38:28.090
We just set it to a new A and then we create
a new B and we call B. So it is sort of the

00:38:28.090 --> 00:38:34.780
same. No difference.
The problem in JavaScript when it comes to

00:38:34.780 --> 00:38:42.660
speed is you can actually on the fly change
the code. So in the inserter box you can see

00:38:42.660 --> 00:38:48.060
that right there we actually are extending
the B pro type with a new foo that prints

00:38:48.060 --> 00:38:54.330
in foo. You can call B foo again. And the
semantics is you have to print out new foo

00:38:54.330 --> 00:39:01.120
now. We don't have that in Dart. We think
this is a plus. It is certainly a plus for

00:39:01.120 --> 00:39:08.330
the implementation. So let's look at it. B
is the object created. That's the object we

00:39:08.330 --> 00:39:15.250
wanted to execute foo on. It points to the
B prototype in JavaScript that, again, points

00:39:15.250 --> 00:39:21.930
to the A prototype. When you start running
the program, you don't have the middle foo

00:39:21.930 --> 00:39:28.100
so you have to execute the one up in A the
problem is, you don't know if somebody is

00:39:28.100 --> 00:39:35.370
going to insert a foo going forward. The system
has to cope with it. So in V8 you have a choice.

00:39:35.370 --> 00:39:41.080
Either you have to always validate. There
is no foo in the B prototype before you can

00:39:41.080 --> 00:39:54.619
execute the one up in A. Then it will invalidate
the code, how it generated, with these assumptions.

00:39:54.619 --> 00:40:00.740
The problem is, the B prototype is an ordinary
object. You do not want to put in a general

00:40:00.740 --> 00:40:09.760
dependency system for normal objects. So in
Dart -- no, sorry, in JavaScript V8, we actually

00:40:09.760 --> 00:40:17.670
validate that there's no foo in the middle
here before we execute foo in the A prototype.

00:40:17.670 --> 00:40:27.950
That is expensive on the implementation side.
Let me try to show you what it cost in generator

00:40:27.950 --> 00:40:33.901
code. So given the example we had on the previous
slide, we now have a small benchmark. It's

00:40:33.901 --> 00:40:40.550
the micro benchmark. We don't like them, yes,
but I'm trying to make a point here. It's

00:40:40.550 --> 00:40:49.010
a loop that just called B.foo repeatedly.
And I warm up that program and I look at what

00:40:49.010 --> 00:40:54.620
the various system generates. On the left
side, you can see the result of what V8 generates.

00:40:54.620 --> 00:41:03.530
It generates nearly 300 bytes of common code
and 239 bytes of stop code. The stop code

00:41:03.530 --> 00:41:10.020
is the source. You need in order to optimization,
if something goes wrong, for instance, you

00:41:10.020 --> 00:41:17.630
add the extra code in the middle. On the Dart
side, it's somewhat simpler. You can see the

00:41:17.630 --> 00:41:25.330
optimized code here. It's much smaller. In
fact, it's only a third of the V8 code being

00:41:25.330 --> 00:41:31.050
generated. So the simple semantics in the
programming language just makes sense, right?

00:41:31.050 --> 00:41:39.280
It's much easier to make fast. Optimizing
a little bit of code is much easier than optimizing

00:41:39.280 --> 00:41:45.060
a big piece of code. Generally, less code
is great. Better memory performance. You also

00:41:45.060 --> 00:41:52.810
get predictable performance because you cannot
change the code after you start it up. So

00:41:52.810 --> 00:41:58.650
keep it simple. It's good for us, V8.
&gt;&gt;Kasper Lund: Hopefully it's very good for

00:41:58.650 --> 00:42:04.200
application developers that get more predictable
performance and just generally better performance.

00:42:04.200 --> 00:42:12.440
So let's benchmark this thing with a non-micro
brashing benchmark suite. To benchmark the

00:42:12.440 --> 00:42:25.350
Dart VM. We have used multiple. A couple that
are interesting are Richards and DeltaBlue.

00:42:25.350 --> 00:42:31.290
That means that if we make Richards and DeltaBlue
faster, it will have an impact on a real web

00:42:31.290 --> 00:42:38.330
application. So using benchmark that you -- that
have proven valuable in the context of other

00:42:38.330 --> 00:42:44.700
languages makes a lot of sense, especially
if you want to try to bring a new programming

00:42:44.700 --> 00:42:48.780
language implementation to the web and make
sure it has nice performance properties that

00:42:48.780 --> 00:42:55.590
match existing real languages. So we've used
in the past for self, Strongtalk, V8, and

00:42:55.590 --> 00:43:02.440
now we're using these two benchmarks for tuning
Dart. It's important to stress that these

00:43:02.440 --> 00:43:07.120
benchmarks really measure the performance
of calling methods, memory allocation, and

00:43:07.120 --> 00:43:12.500
all the things applications tend to spend
a lot of time in. So Richards is an interesting

00:43:12.500 --> 00:43:16.660
benchmark. It's a kernel simulating benchmark
that spends a lot of time in calling small

00:43:16.660 --> 00:43:22.290
methods and dispatching between different
objects in here. What you see on the graph

00:43:22.290 --> 00:43:29.840
here is the Dart VM is on the graph. V8 is
there and DART to JS, which is the generated

00:43:29.840 --> 00:43:36.930
code we get from compiling the Richards version
in Dart to JavaScript with our own Compiler

00:43:36.930 --> 00:43:41.550
and running that on V8. So you have three
different runtime systems here in play that

00:43:41.550 --> 00:43:46.920
execute the same benchmark. So the Dart VM
is the fastest, which is a nice thing. You

00:43:46.920 --> 00:43:52.251
can see the V8 performance is getting better
but not at the same pace as the Dart VM. Bigger

00:43:52.251 --> 00:43:57.300
is better here. V8 has tuned for this benchmark
since its inception. We started out with a

00:43:57.300 --> 00:44:01.890
Richards benchmark and we have achieved a
lot of really cool speedups on these benchmarks

00:44:01.890 --> 00:44:09.380
over the years. It means that V8 is fairly
good at optimizing for this kind of application

00:44:09.380 --> 00:44:17.770
code and benchmark. The Dart VM is already
1.7 times faster than V8 generates way less

00:44:17.770 --> 00:44:22.930
code when running the benchmark and much better
at executing that kind of code. The Dart to

00:44:22.930 --> 00:44:28.120
JS code is a little bit behind. The handwritten
version of JavaScript that we use to tune

00:44:28.120 --> 00:44:33.230
V8 and there's some extra checks going on
in there. Something we're improving over time

00:44:33.230 --> 00:44:38.670
as you can see by the graph. But it's really
nice to see that over the last year, we have

00:44:38.670 --> 00:44:43.670
been able to take the Dart VM from being a
fairly simple implementation of a fairly simple

00:44:43.670 --> 00:44:50.609
language to being a really efficient implementation
of a fairly simple language. This also shows

00:44:50.609 --> 00:44:54.359
on the DeltaBlue benchmark numbers. Here the
difference is even bigger. It's a factor of

00:44:54.359 --> 00:45:00.850
2. DeltaBlue is a one-way constraint solver.
It spends time in allocating objects, constraints

00:45:00.850 --> 00:45:06.510
and dispatcher on them. Again, it's a benchmark
that has been used for many different languages

00:45:06.510 --> 00:45:12.120
and proven valuable in those context as well.
It's great to see that here the same story

00:45:12.120 --> 00:45:17.240
repeats itself. The Dart VM is just a lot
faster than V8. Of course, it's also faster

00:45:17.240 --> 00:45:22.570
than a version we compiled to JavaScript.
It's interesting to see that the Dart to JS

00:45:22.570 --> 00:45:26.200
numbers here are actually faster than the
handwritten JavaScript. So the comparison

00:45:26.200 --> 00:45:33.360
here is between handwritten JavaScript that
V8 executes, where we have tried to implement

00:45:33.360 --> 00:45:39.890
DeltaBlue in a reasonable way in JavaScript.
And then the generated code, this is also

00:45:39.890 --> 00:45:44.470
JavaScript, that we compiled from Dart code
and the reason why it's a little bit faster

00:45:44.470 --> 00:45:49.349
here is because we can do some analysis while
compiling. We can do some inlining to help

00:45:49.349 --> 00:45:55.990
V8 execute this a little bit more quickly.
We do expect this to be able to improve the

00:45:55.990 --> 00:46:01.880
quality of the Dart to JS Compiler over time.
We're aiming for trying to be as fast as the

00:46:01.880 --> 00:46:06.770
code you would have written by hand in JavaScript.
So but clearly, the Dart VM is faster in this

00:46:06.770 --> 00:46:11.820
kind of thing.
It's important to stress that performance

00:46:11.820 --> 00:46:16.210
is super important to us. It really, really
matters that you get a really scalable application

00:46:16.210 --> 00:46:23.360
platform out of using Dart. Having said that,
I think it's important to point to the fact

00:46:23.360 --> 00:46:27.360
that users of Dart are really finding that
in addition to getting good performance, they

00:46:27.360 --> 00:46:35.040
also feel that their productivity increased.
Ali has written a really large font atlas

00:46:35.040 --> 00:46:42.710
generation tool, Glyph3D. We get this feedback
fairly consistently, that people that use

00:46:42.710 --> 00:46:46.790
Dart for building bigger things are very,
very happy with it.

00:46:46.790 --> 00:46:57.930
&gt;&gt;Lars Bak: Thank you. Just to follow up on
what Kasper said, performance is fantastic

00:46:57.930 --> 00:47:03.280
in Dart right now and it's getting better
over time. We have seen on the web the last

00:47:03.280 --> 00:47:09.140
few weeks some examples where Dart is out
performing Java. We are super excited about

00:47:09.140 --> 00:47:16.150
that. We hope to get up to that level for
almost all applications basically.

00:47:16.150 --> 00:47:28.410
So the ultimate goal is to get the Dart VM
into Chrome. I hope you all agree. Exactly.

00:47:28.410 --> 00:47:29.880
At least one.
[ Laughter ]

00:47:29.880 --> 00:47:36.310
But we have a little bit of a problem here.
I sort of mentioned that the garbage collection

00:47:36.310 --> 00:47:41.000
is story where the DOM was a little bit complicated
with these reference counts and stuff like

00:47:41.000 --> 00:47:49.980
that. And putting Dart VM into the pit doesn't
make it easier. And we cannot convince ourselves

00:47:49.980 --> 00:47:57.960
with reference counting that we can reclaim
all unused cycles or memory data structures

00:47:57.960 --> 00:48:05.030
in the browser. That means memory leaks. We
really do not want that. So that's a problem.

00:48:05.030 --> 00:48:10.210
But we like to do something about it. The
ohm way we can make sure that we do something

00:48:10.210 --> 00:48:17.010
about it is to actually change the reference
counted nature of the DOM. And we create a

00:48:17.010 --> 00:48:24.720
new project, started last month. It's called
Oilpan. It makes sure that it handles garbage

00:48:24.720 --> 00:48:30.250
collection between the difference segments
inside Chrome and Blink. Which is JavaScript,

00:48:30.250 --> 00:48:40.030
DOM, eventually Dart. We want to convert the
reference count in DOM into being traced so

00:48:40.030 --> 00:48:45.380
we can trace through JavaScript ons, through
DOM nodes and through Dart nodes and make

00:48:45.380 --> 00:48:51.880
sure we know what we have. The cool thing
about tracing is you can find all pointers

00:48:51.880 --> 00:48:57.250
that point to an object. That means you can
move the object if you choose to do so. You

00:48:57.250 --> 00:49:10.640
can even do [indiscernible] and start from
that point on. And if you are really brave,

00:49:10.640 --> 00:49:17.000
we can start doing concurrent manipulation
of the DOM. So we hope this is all appealing

00:49:17.000 --> 00:49:22.000
for us guys because it basically means we
get a much faster browser out of it and a

00:49:22.000 --> 00:49:29.820
browser use less memory. One thing about the
con currency when you have reference counting

00:49:29.820 --> 00:49:35.250
in the objects, you actually have to put in
a lock around it if you do concurrent access.

00:49:35.250 --> 00:49:41.240
That's expensive. When you get a pointer to
an object, you have to lock, increment the

00:49:41.240 --> 00:49:45.170
pointer, unlock again. That's not practical
when using reference counting. We can do all

00:49:45.170 --> 00:49:52.500
this if unify the memory manager for playing.
We are excited about this project.

00:49:52.500 --> 00:49:59.220
&gt;&gt;Kasper Lund: We feel like no presentation
is really done without a demo of some sort.

00:49:59.220 --> 00:50:05.880
So lately we have been working on making Dart
even faster. No surprise there. A lot of modern

00:50:05.880 --> 00:50:10.660
CPUs today support what is known as SIMD,
single instruction multi-data instructions,

00:50:10.660 --> 00:50:16.980
where you have instructions on the CPU that
can operate on four floating point values

00:50:16.980 --> 00:50:21.800
in parallel making it much, much faster. So
the Dart VM has been enhanced for support

00:50:21.800 --> 00:50:27.099
for using those instructions and no other
web language has the support yet. It's very

00:50:27.099 --> 00:50:32.680
nice to see this in action in a browser, just
making things faster. It's really useful for

00:50:32.680 --> 00:50:39.020
3D calculations, image processing, audio processing.
We would like to show you a little demo of

00:50:39.020 --> 00:50:45.840
Google Chrome with the Dart VM put in there
running a 3D animation thing with and without

00:50:45.840 --> 00:50:50.880
the SIMD support to show you the significance
of this kind of work. So let's switch to the

00:50:50.880 --> 00:51:01.960
other.
&gt;&gt;Lars Bak: So here we are running Dart VM

00:51:01.960 --> 00:51:09.550
with a Blink with Dart VM. We have a bunch
of monsters. They all handle inside Dart.

00:51:09.550 --> 00:51:15.160
So that means that the animation, the skeleton
positions and all that is happening inside

00:51:15.160 --> 00:51:25.500
Dart. It's not on the GPU. And this is the
-- somebody is beeping. This benchmark has

00:51:25.500 --> 00:51:31.670
been created so it is trying to always have
60 frames per second. If it runs too fast,

00:51:31.670 --> 00:51:45.880
it will throw in more monsters. We have 34.
We can turn it on with using a flag down here.

00:51:45.880 --> 00:52:00.970
Let's try it. SIMD. So this is using SIMD
in animation in Dart.

00:52:00.970 --> 00:52:05.390
[ Applause ]
So a factor of three and a half is not too

00:52:05.390 --> 00:52:12.430
bad, I guess. I'm really excited about this
and it actually gives you a lot more power

00:52:12.430 --> 00:52:20.040
to do computation inside JavaScript. So this
is coming to you and I'm excited.

00:52:20.040 --> 00:52:33.330
&gt;&gt;Kasper Lund: So let's conclude this presentation
so we have time for some questions. So we

00:52:33.330 --> 00:52:39.670
are VM guys. We want to have a job moving
forward. We think performance is always interesting.

00:52:39.670 --> 00:52:46.210
I hope you like that, too. And we hope we
have convinced you that if you really want

00:52:46.210 --> 00:52:53.280
to spark more performance into the browser,
we need probably a different option than JavaScript.

00:52:53.280 --> 00:53:01.880
I think Dart is one contender for that. The
Dart VM is already faster than JavaScript

00:53:01.880 --> 00:53:08.500
and is approaching other programming languages
out there. Again, just to point it out again

00:53:08.500 --> 00:53:13.540
that higher performance is great for application
developers. That's where you actually can

00:53:13.540 --> 00:53:21.260
get head room to do more interesting stuff.
Right now, the core Dart platform is stable.

00:53:21.260 --> 00:53:29.480
We will start using the SDK. That doesn't
mean it's inside Chrome. After we send out

00:53:29.480 --> 00:53:36.760
1.0, that will be our main focus. Right now
you can see people on the web using Dart.

00:53:36.760 --> 00:53:43.420
Some have written fairly large bodies of code
and they are happy with it. And inside Google

00:53:43.420 --> 00:53:49.570
people have started using it. There's critical
projects start using Dart, hopefully we will

00:53:49.570 --> 00:53:56.030
see some of these applications coming to you
soon. This will conclude our presentation.

00:53:56.030 --> 00:54:01.720
We should go to questions.
[ Applause ]

00:54:01.720 --> 00:54:13.920
&gt;&gt;Kasper Lund: If we are unable to answer
your questions here. We have other Dart questions

00:54:13.920 --> 00:54:18.960
at Google I/O today. So if you have found
this inspiring and you want to see more on

00:54:18.960 --> 00:54:24.480
more specific details, here's a list of things
in room -- mostly in room 6, I guess, where

00:54:24.480 --> 00:54:28.060
you can see other Dart presentations here.
Yes?

00:54:28.060 --> 00:54:33.930
&gt;&gt;Lars Bak: Let's get the first question.
&gt;&gt;&gt; My question is slightly related to that

00:54:33.930 --> 00:54:39.560
first one, coming from a GWT development background,
wondering if you have done any benchmarks

00:54:39.560 --> 00:54:46.170
in comparison from the Dart to JavaScript
Compiler to the Java to JavaScript Compiler.

00:54:46.170 --> 00:54:54.770
&gt;&gt;Lars Bak: I don't have any numbers for you.
It is easy to try out. DeltaBlue, it is free

00:54:54.770 --> 00:55:08.119
to try out for yourself. One thing I would
like to see is that we are trying to go beyond

00:55:08.119 --> 00:55:15.500
grit in that we are doing a native Dart VM
that can boost performance and reduce startup

00:55:15.500 --> 00:55:20.770
time. This is really what we want to get at.
Especially for mobile platforms where loading

00:55:20.770 --> 00:55:26.470
an application can be a dog and take many
resources. Faster VM, faster startup means

00:55:26.470 --> 00:55:33.140
less battery. Next question, please.
&gt;&gt;&gt; Hi. Have there been discussions of including

00:55:33.140 --> 00:55:39.370
Dart in other browsers and if not, would it
be possible to add it through plug-ins?

00:55:39.370 --> 00:55:51.510
&gt;&gt;Lars Bak: It will be fantastic to have all
browsers using Dart. Clearly people have strong

00:55:51.510 --> 00:55:55.430
opinions when it comes to programming languages.
I don't really understand why but that's how

00:55:55.430 --> 00:55:58.000
it is.
[ Laughter ]

00:55:58.000 --> 00:56:03.349
I just want to make sure that people the right
tools to build the applications with. Our

00:56:03.349 --> 00:56:08.400
system is completely open source. It has a
well-defined API. So when other browsers think

00:56:08.400 --> 00:56:12.960
that this extra boost of performance will
be good for them, they can take it up and

00:56:12.960 --> 00:56:17.570
we will be happy to help them out, basically.
So we are open for collaboration.

00:56:17.570 --> 00:56:23.780
&gt;&gt;Kasper Lund: We have a few questions from
our online audience and maybe just try to

00:56:23.780 --> 00:56:28.300
answer a few of them as well. I can cover
the second one. I think you covered the grit

00:56:28.300 --> 00:56:34.220
question already. This question is about Android
studio. And if we are planning on releasing

00:56:34.220 --> 00:56:39.619
something similar for Dart. In a sense we
already are releasing something similar for

00:56:39.619 --> 00:56:45.750
Dart. We have a fully-featured Dart editor.
But in addition to that, and maybe this is

00:56:45.750 --> 00:56:49.579
what the question is really about, we do have
support for using and working with Dart from

00:56:49.579 --> 00:56:56.500
IntelliJ. It is here. If you are interested
in this kind of thing, definitely come talk

00:56:56.500 --> 00:57:01.620
to them at the booth. So the short answer
is really that we are looking into releasing

00:57:01.620 --> 00:57:06.099
all sorts of nice tools for Dart being based
both on Eclipse and IntelliJ. Definitely.

00:57:06.099 --> 00:57:09.450
&gt;&gt;Lars Bak: Let's take the next question from
the audience.

00:57:09.450 --> 00:57:15.829
&gt;&gt;&gt; Hi. I have a bit of long -- seemingly
longstanding wisdom I got a few years ago

00:57:15.829 --> 00:57:21.079
that occurs to me might be out of date which
is the idea -- this is about V8, that JavaScript

00:57:21.079 --> 00:57:26.410
objects are going to be more performant if
you define prototype rather than defining

00:57:26.410 --> 00:57:32.359
methods dynamically in a constructor. It occurs
to me with hidden, you have optimized it so

00:57:32.359 --> 00:57:39.310
it wouldn't make a difference.
&gt;&gt;Lars Bak: We certainly try to. We tried

00:57:39.310 --> 00:57:46.230
very hard to optimize in V8. But it's still
the case that you have to create a new class

00:57:46.230 --> 00:57:52.920
whenever you add a property in a way you haven't
seen before. So you get a forest of classes

00:57:52.920 --> 00:57:59.880
even for the same constructor. And in Dart,
you only have one format for one class whereas

00:57:59.880 --> 00:58:06.160
in V8, right, depending on how you add properties
to an object from a given constructor, you

00:58:06.160 --> 00:58:10.921
can get a sea of them. We have to cut off
at some point. We have limits inside V8. When

00:58:10.921 --> 00:58:16.549
you reach a certain number of hidden classes
for constructer. We say, we actually just

00:58:16.549 --> 00:58:24.369
-- we de-optimize now and we make sure that
objects will only be treated as maps and get

00:58:24.369 --> 00:58:27.010
slow.
&gt;&gt;&gt; So is it true that, then, prototype would

00:58:27.010 --> 00:58:32.940
be more performant in V8 than using construct-defining
properties, even if you define them the same

00:58:32.940 --> 00:58:36.970
way every time in constructor.
&gt;&gt;Lars Bak: I think it depends on exactly

00:58:36.970 --> 00:58:41.560
the setup. You can make it so they perform
exactly the same. It really depends on how

00:58:41.560 --> 00:58:46.380
the application adds properties to your object
on the fly.

00:58:46.380 --> 00:58:52.590
&gt;&gt;Kasper Lund: Little bit of explanation.
V8 does try to move functions to the class

00:58:52.590 --> 00:58:57.020
side so to share them behind the scenes anyway.
So you can say adding on the objects is sort

00:58:57.020 --> 00:59:02.070
of optimize too, but it really depends on
the application if it makes a difference.

00:59:02.070 --> 00:59:06.619
&gt;&gt;Lars Bak: Next question.
&gt;&gt;&gt; One of the arguments for no JS is that

00:59:06.619 --> 00:59:11.210
you have JavaScript everywhere, service side
even. Are you looking at Dart service side

00:59:11.210 --> 00:59:16.560
or is there more momentum behind go.
&gt;&gt;Lars Bak: That's a great question. Of course,

00:59:16.560 --> 00:59:25.641
we have that. So we have a service side library
you can use. You can use the Dart VM stand

00:59:25.641 --> 00:59:30.920
alone if you want to. You can use the same
kind of ace synchronize size style as you

00:59:30.920 --> 00:59:38.800
do in no JS. You saw the performance numbers
on the slide. This is a performance measurement

00:59:38.800 --> 00:59:45.300
system we are using internally, all written
in Dart. It controls a lot of machines at

00:59:45.300 --> 00:59:54.450
the same time we use exactly that server side
system. So, yes, we have a libraries for supporting

00:59:54.450 --> 01:00:00.190
service side Dart execution but our main focus
right now is the client side and getting it

01:00:00.190 --> 01:00:02.530
to Chrome.
&gt;&gt;Kasper Lund: I think we have time for one

01:00:02.530 --> 01:00:08.340
more question. If you have more questions,
the large line, it will be at the office hours

01:00:08.340 --> 01:00:14.170
booth so come to us and ask those questions
there. But let's take the final question.

01:00:14.170 --> 01:00:19.290
&gt;&gt;&gt; My question is there's a lot of JavaScript
code already there. Is there a tool, or even

01:00:19.290 --> 01:00:22.369
if it is possible, to convert JavaScript into
Dart.

01:00:22.369 --> 01:00:27.940
&gt;&gt;Kasper Lund: Generally we find that people
that try to rewrite existing code basis from

01:00:27.940 --> 01:00:32.260
JavaScript or from other languages to Dart
are generally fairly successful. It's also

01:00:32.260 --> 01:00:36.940
possible to use the JavaScript code from Dart
by wrapping it a little bit and using what

01:00:36.940 --> 01:00:43.260
we called JavaScript interrupt. But generally
translating from JavaScript, unless you use

01:00:43.260 --> 01:00:48.810
very, very dynamic features of JavaScript
a lot, it's actually recently easy. A lot

01:00:48.810 --> 01:00:51.960
of people have good success with that.
&gt;&gt;&gt; You guys have a tool.

01:00:51.960 --> 01:00:57.859
&gt;&gt;Kasper Lund: There is no tool, no.
&gt;&gt;Lars Bak: Thank you very much.

01:00:57.859 --> 01:00:58.130
[ Applause ]

