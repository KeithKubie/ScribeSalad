WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:01.309
 
when you train your neural network is

00:00:01.309 --> 00:00:01.319
when you train your neural network is
 

00:00:01.319 --> 00:00:02.530
when you train your neural network is
important to initialize the weights

00:00:02.530 --> 00:00:02.540
important to initialize the weights
 

00:00:02.540 --> 00:00:05.539
important to initialize the weights
randomly for logistic regression it was

00:00:05.539 --> 00:00:05.549
randomly for logistic regression it was
 

00:00:05.549 --> 00:00:07.430
randomly for logistic regression it was
okay to initialize the weights to zero

00:00:07.430 --> 00:00:07.440
okay to initialize the weights to zero
 

00:00:07.440 --> 00:00:09.799
okay to initialize the weights to zero
but for a neural network of initializer

00:00:09.799 --> 00:00:09.809
but for a neural network of initializer
 

00:00:09.809 --> 00:00:12.169
but for a neural network of initializer
wastes the parameters to all 0 and then

00:00:12.169 --> 00:00:12.179
wastes the parameters to all 0 and then
 

00:00:12.179 --> 00:00:14.450
wastes the parameters to all 0 and then
apply gradient descent it won't work

00:00:14.450 --> 00:00:14.460
apply gradient descent it won't work
 

00:00:14.460 --> 00:00:19.880
apply gradient descent it won't work
let's see why so you have here two input

00:00:19.880 --> 00:00:19.890
let's see why so you have here two input
 

00:00:19.890 --> 00:00:22.720
let's see why so you have here two input
features so n 0 is equal to 2 and 2

00:00:22.720 --> 00:00:22.730
features so n 0 is equal to 2 and 2
 

00:00:22.730 --> 00:00:27.320
features so n 0 is equal to 2 and 2
hidden units so N 1 is equal to 2 and so

00:00:27.320 --> 00:00:27.330
hidden units so N 1 is equal to 2 and so
 

00:00:27.330 --> 00:00:30.650
hidden units so N 1 is equal to 2 and so
the matrix associated with a hidden

00:00:30.650 --> 00:00:30.660
the matrix associated with a hidden
 

00:00:30.660 --> 00:00:35.690
the matrix associated with a hidden
layer or W 1 is going to be 2 by 2 let's

00:00:35.690 --> 00:00:35.700
layer or W 1 is going to be 2 by 2 let's
 

00:00:35.700 --> 00:00:38.500
layer or W 1 is going to be 2 by 2 let's
say that you initialize it to all zeros

00:00:38.500 --> 00:00:38.510
say that you initialize it to all zeros
 

00:00:38.510 --> 00:00:42.799
say that you initialize it to all zeros
0 0 0 0 2 by 2 matrix and let's say P 1

00:00:42.799 --> 00:00:42.809
0 0 0 0 2 by 2 matrix and let's say P 1
 

00:00:42.809 --> 00:00:46.459
0 0 0 0 2 by 2 matrix and let's say P 1
is also equal to 0 0 turns out

00:00:46.459 --> 00:00:46.469
is also equal to 0 0 turns out
 

00:00:46.469 --> 00:00:49.700
is also equal to 0 0 turns out
initializing the bias terms B to 0 is

00:00:49.700 --> 00:00:49.710
initializing the bias terms B to 0 is
 

00:00:49.710 --> 00:00:52.549
initializing the bias terms B to 0 is
actually okay but initializing W to all

00:00:52.549 --> 00:00:52.559
actually okay but initializing W to all
 

00:00:52.559 --> 00:00:55.670
actually okay but initializing W to all
zeros is a problem so the problem with

00:00:55.670 --> 00:00:55.680
zeros is a problem so the problem with
 

00:00:55.680 --> 00:00:58.010
zeros is a problem so the problem with
this formal initialization is that for

00:00:58.010 --> 00:00:58.020
this formal initialization is that for
 

00:00:58.020 --> 00:01:02.139
this formal initialization is that for
any example you give it you have that a

00:01:02.139 --> 00:01:02.149
any example you give it you have that a
 

00:01:02.149 --> 00:01:09.590
any example you give it you have that a
1 1 and a 1 2 will be equal all right so

00:01:09.590 --> 00:01:09.600
1 1 and a 1 2 will be equal all right so
 

00:01:09.600 --> 00:01:11.750
1 1 and a 1 2 will be equal all right so
this activation and this activation

00:01:11.750 --> 00:01:11.760
this activation and this activation
 

00:01:11.760 --> 00:01:13.640
this activation and this activation
would be sync because both of these

00:01:13.640 --> 00:01:13.650
would be sync because both of these
 

00:01:13.650 --> 00:01:15.530
would be sync because both of these
hidden units are computing exactly the

00:01:15.530 --> 00:01:15.540
hidden units are computing exactly the
 

00:01:15.540 --> 00:01:18.590
hidden units are computing exactly the
same function and then when you compute

00:01:18.590 --> 00:01:18.600
same function and then when you compute
 

00:01:18.600 --> 00:01:22.880
same function and then when you compute
back propagation it turns out that D Z 1

00:01:22.880 --> 00:01:22.890
back propagation it turns out that D Z 1
 

00:01:22.890 --> 00:01:28.999
back propagation it turns out that D Z 1
1 and D Z 1 2 will also be the same

00:01:28.999 --> 00:01:29.009
1 and D Z 1 2 will also be the same
 

00:01:29.009 --> 00:01:30.649
1 and D Z 1 2 will also be the same
color by symmetry all right both of

00:01:30.649 --> 00:01:30.659
color by symmetry all right both of
 

00:01:30.659 --> 00:01:32.149
color by symmetry all right both of
these settings units will initialize the

00:01:32.149 --> 00:01:32.159
these settings units will initialize the
 

00:01:32.159 --> 00:01:36.050
these settings units will initialize the
same way technically for what I'm saying

00:01:36.050 --> 00:01:36.060
same way technically for what I'm saying
 

00:01:36.060 --> 00:01:38.030
same way technically for what I'm saying
I'm assuming that the outgoing weights

00:01:38.030 --> 00:01:38.040
I'm assuming that the outgoing weights
 

00:01:38.040 --> 00:01:42.740
I'm assuming that the outgoing weights
are also identical so that W 2 is equal

00:01:42.740 --> 00:01:42.750
are also identical so that W 2 is equal
 

00:01:42.750 --> 00:01:48.139
are also identical so that W 2 is equal
to 0 0 but if you initialize the neural

00:01:48.139 --> 00:01:48.149
to 0 0 but if you initialize the neural
 

00:01:48.149 --> 00:01:51.469
to 0 0 but if you initialize the neural
network this way then this hidden unit

00:01:51.469 --> 00:01:51.479
network this way then this hidden unit
 

00:01:51.479 --> 00:01:53.389
network this way then this hidden unit
and this hidden unit are completely

00:01:53.389 --> 00:01:53.399
and this hidden unit are completely
 

00:01:53.399 --> 00:01:54.770
and this hidden unit are completely
identical so they're completely

00:01:54.770 --> 00:01:54.780
identical so they're completely
 

00:01:54.780 --> 00:01:56.510
identical so they're completely
sometimes you say they're completely

00:01:56.510 --> 00:01:56.520
sometimes you say they're completely
 

00:01:56.520 --> 00:01:57.920
sometimes you say they're completely
symmetric which just means that the

00:01:57.920 --> 00:01:57.930
symmetric which just means that the
 

00:01:57.930 --> 00:02:00.440
symmetric which just means that the
computing exactly the same function and

00:02:00.440 --> 00:02:00.450
computing exactly the same function and
 

00:02:00.450 --> 00:02:04.580
computing exactly the same function and
by kind of a proof by induction it turns

00:02:04.580 --> 00:02:04.590
by kind of a proof by induction it turns
 

00:02:04.590 --> 00:02:07.100
by kind of a proof by induction it turns
out that after every single iteration of

00:02:07.100 --> 00:02:07.110
out that after every single iteration of
 

00:02:07.110 --> 00:02:09.050
out that after every single iteration of
training you're two hidden units are

00:02:09.050 --> 00:02:09.060
training you're two hidden units are
 

00:02:09.060 --> 00:02:10.729
training you're two hidden units are
still confusing exactly the same

00:02:10.729 --> 00:02:10.739
still confusing exactly the same
 

00:02:10.739 --> 00:02:11.660
still confusing exactly the same
function

00:02:11.660 --> 00:02:11.670
function
 

00:02:11.670 --> 00:02:15.230
function
since also the show that DW will be a

00:02:15.230 --> 00:02:15.240
since also the show that DW will be a
 

00:02:15.240 --> 00:02:18.140
since also the show that DW will be a
matrix that looks like this where every

00:02:18.140 --> 00:02:18.150
matrix that looks like this where every
 

00:02:18.150 --> 00:02:21.290
matrix that looks like this where every
row takes on the same value so we

00:02:21.290 --> 00:02:21.300
row takes on the same value so we
 

00:02:21.300 --> 00:02:23.540
row takes on the same value so we
perform a weight update so you perform

00:02:23.540 --> 00:02:23.550
perform a weight update so you perform
 

00:02:23.550 --> 00:02:26.420
perform a weight update so you perform
when you perform a weight update w1 gets

00:02:26.420 --> 00:02:26.430
when you perform a weight update w1 gets
 

00:02:26.430 --> 00:02:30.410
when you perform a weight update w1 gets
updated as w1 minus alpha times DW you

00:02:30.410 --> 00:02:30.420
updated as w1 minus alpha times DW you
 

00:02:30.420 --> 00:02:33.650
updated as w1 minus alpha times DW you
find that w1 after every iteration will

00:02:33.650 --> 00:02:33.660
find that w1 after every iteration will
 

00:02:33.660 --> 00:02:36.980
find that w1 after every iteration will
have you know the first row equal to the

00:02:36.980 --> 00:02:36.990
have you know the first row equal to the
 

00:02:36.990 --> 00:02:39.110
have you know the first row equal to the
second row so it's possible to construct

00:02:39.110 --> 00:02:39.120
second row so it's possible to construct
 

00:02:39.120 --> 00:02:41.180
second row so it's possible to construct
a proof by induction that if you

00:02:41.180 --> 00:02:41.190
a proof by induction that if you
 

00:02:41.190 --> 00:02:43.580
a proof by induction that if you
initialize all the ways all the values

00:02:43.580 --> 00:02:43.590
initialize all the ways all the values
 

00:02:43.590 --> 00:02:47.480
initialize all the ways all the values
of W to 0 then because both hidden units

00:02:47.480 --> 00:02:47.490
of W to 0 then because both hidden units
 

00:02:47.490 --> 00:02:49.430
of W to 0 then because both hidden units
start off computing the same function

00:02:49.430 --> 00:02:49.440
start off computing the same function
 

00:02:49.440 --> 00:02:51.560
start off computing the same function
and both hidden units have the same

00:02:51.560 --> 00:02:51.570
and both hidden units have the same
 

00:02:51.570 --> 00:02:55.040
and both hidden units have the same
influence on the output unit then after

00:02:55.040 --> 00:02:55.050
influence on the output unit then after
 

00:02:55.050 --> 00:02:57.590
influence on the output unit then after
one iteration that same statement is

00:02:57.590 --> 00:02:57.600
one iteration that same statement is
 

00:02:57.600 --> 00:02:59.330
one iteration that same statement is
still true the two hidden units are

00:02:59.330 --> 00:02:59.340
still true the two hidden units are
 

00:02:59.340 --> 00:03:01.160
still true the two hidden units are
still symmetric and therefore by

00:03:01.160 --> 00:03:01.170
still symmetric and therefore by
 

00:03:01.170 --> 00:03:02.990
still symmetric and therefore by
induction after two iterations three

00:03:02.990 --> 00:03:03.000
induction after two iterations three
 

00:03:03.000 --> 00:03:05.090
induction after two iterations three
iterations and so on no matter how long

00:03:05.090 --> 00:03:05.100
iterations and so on no matter how long
 

00:03:05.100 --> 00:03:07.250
iterations and so on no matter how long
you train in your network both hidden

00:03:07.250 --> 00:03:07.260
you train in your network both hidden
 

00:03:07.260 --> 00:03:09.500
you train in your network both hidden
units are still computing exactly the

00:03:09.500 --> 00:03:09.510
units are still computing exactly the
 

00:03:09.510 --> 00:03:11.750
units are still computing exactly the
same function and so in this case

00:03:11.750 --> 00:03:11.760
same function and so in this case
 

00:03:11.760 --> 00:03:14.090
same function and so in this case
there's really no point to having more

00:03:14.090 --> 00:03:14.100
there's really no point to having more
 

00:03:14.100 --> 00:03:15.650
there's really no point to having more
than one hidden unit because they're all

00:03:15.650 --> 00:03:15.660
than one hidden unit because they're all
 

00:03:15.660 --> 00:03:17.870
than one hidden unit because they're all
computing the same thing and of course

00:03:17.870 --> 00:03:17.880
computing the same thing and of course
 

00:03:17.880 --> 00:03:20.449
computing the same thing and of course
for larger neural networks less you have

00:03:20.449 --> 00:03:20.459
for larger neural networks less you have
 

00:03:20.459 --> 00:03:22.640
for larger neural networks less you have
three features and maybe a very large

00:03:22.640 --> 00:03:22.650
three features and maybe a very large
 

00:03:22.650 --> 00:03:24.800
three features and maybe a very large
number of hidden units a similar

00:03:24.800 --> 00:03:24.810
number of hidden units a similar
 

00:03:24.810 --> 00:03:28.280
number of hidden units a similar
argument works to show that we're gonna

00:03:28.280 --> 00:03:28.290
argument works to show that we're gonna
 

00:03:28.290 --> 00:03:31.430
argument works to show that we're gonna
network like this because I won't drawn

00:03:31.430 --> 00:03:31.440
network like this because I won't drawn
 

00:03:31.440 --> 00:03:33.860
network like this because I won't drawn
all the edges if you initialize the ways

00:03:33.860 --> 00:03:33.870
all the edges if you initialize the ways
 

00:03:33.870 --> 00:03:35.900
all the edges if you initialize the ways
to zero then all of your hidden units

00:03:35.900 --> 00:03:35.910
to zero then all of your hidden units
 

00:03:35.910 --> 00:03:38.750
to zero then all of your hidden units
are symmetric and no matter how long you

00:03:38.750 --> 00:03:38.760
are symmetric and no matter how long you
 

00:03:38.760 --> 00:03:40.039
are symmetric and no matter how long you
run gradient descent they'll all

00:03:40.039 --> 00:03:40.049
run gradient descent they'll all
 

00:03:40.049 --> 00:03:42.500
run gradient descent they'll all
continue to compute exactly the same

00:03:42.500 --> 00:03:42.510
continue to compute exactly the same
 

00:03:42.510 --> 00:03:45.710
continue to compute exactly the same
function so that's not helpful because

00:03:45.710 --> 00:03:45.720
function so that's not helpful because
 

00:03:45.720 --> 00:03:47.660
function so that's not helpful because
you want two different hidden units to

00:03:47.660 --> 00:03:47.670
you want two different hidden units to
 

00:03:47.670 --> 00:03:50.240
you want two different hidden units to
compute different functions the solution

00:03:50.240 --> 00:03:50.250
compute different functions the solution
 

00:03:50.250 --> 00:03:52.460
compute different functions the solution
to this is to initialize your parameters

00:03:52.460 --> 00:03:52.470
to this is to initialize your parameters
 

00:03:52.470 --> 00:03:56.360
to this is to initialize your parameters
randomly so here's what you do you can

00:03:56.360 --> 00:03:56.370
randomly so here's what you do you can
 

00:03:56.370 --> 00:04:02.180
randomly so here's what you do you can
set W 1 equals NP about random dot R and

00:04:02.180 --> 00:04:02.190
set W 1 equals NP about random dot R and
 

00:04:02.190 --> 00:04:04.850
set W 1 equals NP about random dot R and
n this generates a Gaussian random

00:04:04.850 --> 00:04:04.860
n this generates a Gaussian random
 

00:04:04.860 --> 00:04:08.900
n this generates a Gaussian random
variable to 2 and then usually you

00:04:08.900 --> 00:04:08.910
variable to 2 and then usually you
 

00:04:08.910 --> 00:04:10.699
variable to 2 and then usually you
multiply this by a very small number

00:04:10.699 --> 00:04:10.709
multiply this by a very small number
 

00:04:10.709 --> 00:04:13.520
multiply this by a very small number
such as 0.01 so you initialize it to

00:04:13.520 --> 00:04:13.530
such as 0.01 so you initialize it to
 

00:04:13.530 --> 00:04:17.170
such as 0.01 so you initialize it to
very small random values and then be um

00:04:17.170 --> 00:04:17.180
very small random values and then be um
 

00:04:17.180 --> 00:04:20.659
very small random values and then be um
it turns out that B does not have this

00:04:20.659 --> 00:04:20.669
it turns out that B does not have this
 

00:04:20.669 --> 00:04:22.610
it turns out that B does not have this
symmetry problem what's called the

00:04:22.610 --> 00:04:22.620
symmetry problem what's called the
 

00:04:22.620 --> 00:04:24.620
symmetry problem what's called the
symmetry breaking problem

00:04:24.620 --> 00:04:24.630
symmetry breaking problem
 

00:04:24.630 --> 00:04:29.540
symmetry breaking problem
so is okay to initialize B to just zeros

00:04:29.540 --> 00:04:29.550
so is okay to initialize B to just zeros
 

00:04:29.550 --> 00:04:31.670
so is okay to initialize B to just zeros
because so long as WS initialize

00:04:31.670 --> 00:04:31.680
because so long as WS initialize
 

00:04:31.680 --> 00:04:33.950
because so long as WS initialize
randomly you start top of the different

00:04:33.950 --> 00:04:33.960
randomly you start top of the different
 

00:04:33.960 --> 00:04:36.050
randomly you start top of the different
hidden units computing different things

00:04:36.050 --> 00:04:36.060
hidden units computing different things
 

00:04:36.060 --> 00:04:38.030
hidden units computing different things
and so you no longer have this some

00:04:38.030 --> 00:04:38.040
and so you no longer have this some
 

00:04:38.040 --> 00:04:40.490
and so you no longer have this some
symmetry breaking problem and then

00:04:40.490 --> 00:04:40.500
symmetry breaking problem and then
 

00:04:40.500 --> 00:04:43.340
symmetry breaking problem and then
similarly for w2 you can initialize that

00:04:43.340 --> 00:04:43.350
similarly for w2 you can initialize that
 

00:04:43.350 --> 00:04:48.320
similarly for w2 you can initialize that
randomly and b2 you can initialize that

00:04:48.320 --> 00:04:48.330
randomly and b2 you can initialize that
 

00:04:48.330 --> 00:04:51.950
randomly and b2 you can initialize that
to zero so you might be wondering you

00:04:51.950 --> 00:04:51.960
to zero so you might be wondering you
 

00:04:51.960 --> 00:04:53.570
to zero so you might be wondering you
know where did this constant come from

00:04:53.570 --> 00:04:53.580
know where did this constant come from
 

00:04:53.580 --> 00:04:56.570
know where did this constant come from
and why is it 0.01 why not put the

00:04:56.570 --> 00:04:56.580
and why is it 0.01 why not put the
 

00:04:56.580 --> 00:04:59.990
and why is it 0.01 why not put the
number 100 or 1000 turns out that we

00:04:59.990 --> 00:05:00.000
number 100 or 1000 turns out that we
 

00:05:00.000 --> 00:05:02.420
number 100 or 1000 turns out that we
usually prefer to initialize the ways to

00:05:02.420 --> 00:05:02.430
usually prefer to initialize the ways to
 

00:05:02.430 --> 00:05:06.470
usually prefer to initialize the ways to
get very small random values because um

00:05:06.470 --> 00:05:06.480
get very small random values because um
 

00:05:06.480 --> 00:05:09.800
get very small random values because um
if you're using a satanic or sigmoid

00:05:09.800 --> 00:05:09.810
if you're using a satanic or sigmoid
 

00:05:09.810 --> 00:05:11.360
if you're using a satanic or sigmoid
activation function or if you have a

00:05:11.360 --> 00:05:11.370
activation function or if you have a
 

00:05:11.370 --> 00:05:14.060
activation function or if you have a
sigmoid even just at the output layer if

00:05:14.060 --> 00:05:14.070
sigmoid even just at the output layer if
 

00:05:14.070 --> 00:05:17.810
sigmoid even just at the output layer if
the weights are too large then when you

00:05:17.810 --> 00:05:17.820
the weights are too large then when you
 

00:05:17.820 --> 00:05:20.180
the weights are too large then when you
compute the activation values remember

00:05:20.180 --> 00:05:20.190
compute the activation values remember
 

00:05:20.190 --> 00:05:28.000
compute the activation values remember
that Z 1 is equal to W 1 X plus B and

00:05:28.000 --> 00:05:28.010
that Z 1 is equal to W 1 X plus B and
 

00:05:28.010 --> 00:05:31.670
that Z 1 is equal to W 1 X plus B and
then on a 1 is the activation function

00:05:31.670 --> 00:05:31.680
then on a 1 is the activation function
 

00:05:31.680 --> 00:05:37.670
then on a 1 is the activation function
applied to Z 1 so if W is very big Z

00:05:37.670 --> 00:05:37.680
applied to Z 1 so if W is very big Z
 

00:05:37.680 --> 00:05:40.010
applied to Z 1 so if W is very big Z
will be very big or at least some values

00:05:40.010 --> 00:05:40.020
will be very big or at least some values
 

00:05:40.020 --> 00:05:42.520
will be very big or at least some values
of Z will be either very large or very

00:05:42.520 --> 00:05:42.530
of Z will be either very large or very
 

00:05:42.530 --> 00:05:45.860
of Z will be either very large or very
small and so in that case you're more

00:05:45.860 --> 00:05:45.870
small and so in that case you're more
 

00:05:45.870 --> 00:05:48.770
small and so in that case you're more
likely to end up at these flat parts of

00:05:48.770 --> 00:05:48.780
likely to end up at these flat parts of
 

00:05:48.780 --> 00:05:52.160
likely to end up at these flat parts of
the 10h function or the sigmoid function

00:05:52.160 --> 00:05:52.170
the 10h function or the sigmoid function
 

00:05:52.170 --> 00:05:54.530
the 10h function or the sigmoid function
where the slope of the gradient is very

00:05:54.530 --> 00:05:54.540
where the slope of the gradient is very
 

00:05:54.540 --> 00:05:57.200
where the slope of the gradient is very
small meaning that gradient descent

00:05:57.200 --> 00:05:57.210
small meaning that gradient descent
 

00:05:57.210 --> 00:05:59.180
small meaning that gradient descent
would be very slow and so learning was

00:05:59.180 --> 00:05:59.190
would be very slow and so learning was
 

00:05:59.190 --> 00:06:02.960
would be very slow and so learning was
very slow so just a recap if W is too

00:06:02.960 --> 00:06:02.970
very slow so just a recap if W is too
 

00:06:02.970 --> 00:06:05.510
very slow so just a recap if W is too
large you're more likely to end up even

00:06:05.510 --> 00:06:05.520
large you're more likely to end up even
 

00:06:05.520 --> 00:06:07.100
large you're more likely to end up even
at the very start of training with very

00:06:07.100 --> 00:06:07.110
at the very start of training with very
 

00:06:07.110 --> 00:06:10.130
at the very start of training with very
large values of Z which causes your 10 a

00:06:10.130 --> 00:06:10.140
large values of Z which causes your 10 a
 

00:06:10.140 --> 00:06:12.050
large values of Z which causes your 10 a
to a sigmoid activation function to be

00:06:12.050 --> 00:06:12.060
to a sigmoid activation function to be
 

00:06:12.060 --> 00:06:15.140
to a sigmoid activation function to be
saturated on the slowing down learning

00:06:15.140 --> 00:06:15.150
saturated on the slowing down learning
 

00:06:15.150 --> 00:06:17.720
saturated on the slowing down learning
if you don't have any Sigma naught NH

00:06:17.720 --> 00:06:17.730
if you don't have any Sigma naught NH
 

00:06:17.730 --> 00:06:19.130
if you don't have any Sigma naught NH
activation functions throughout your

00:06:19.130 --> 00:06:19.140
activation functions throughout your
 

00:06:19.140 --> 00:06:21.440
activation functions throughout your
neural network this is less of an issue

00:06:21.440 --> 00:06:21.450
neural network this is less of an issue
 

00:06:21.450 --> 00:06:23.690
neural network this is less of an issue
but if you're doing binary consultation

00:06:23.690 --> 00:06:23.700
but if you're doing binary consultation
 

00:06:23.700 --> 00:06:25.640
but if you're doing binary consultation
and your output unit is a sigmoid

00:06:25.640 --> 00:06:25.650
and your output unit is a sigmoid
 

00:06:25.650 --> 00:06:27.620
and your output unit is a sigmoid
function then you know you just don't

00:06:27.620 --> 00:06:27.630
function then you know you just don't
 

00:06:27.630 --> 00:06:29.600
function then you know you just don't
want the initial parameters to be too

00:06:29.600 --> 00:06:29.610
want the initial parameters to be too
 

00:06:29.610 --> 00:06:33.200
want the initial parameters to be too
large so that's why multiplying by 0.01

00:06:33.200 --> 00:06:33.210
large so that's why multiplying by 0.01
 

00:06:33.210 --> 00:06:35.719
large so that's why multiplying by 0.01
would be something reasonable to try or

00:06:35.719 --> 00:06:35.729
would be something reasonable to try or
 

00:06:35.729 --> 00:06:38.780
would be something reasonable to try or
any other small number and same for W 2

00:06:38.780 --> 00:06:38.790
any other small number and same for W 2
 

00:06:38.790 --> 00:06:44.000
any other small number and same for W 2
it can be a random random I guess this

00:06:44.000 --> 00:06:44.010
it can be a random random I guess this
 

00:06:44.010 --> 00:06:47.390
it can be a random random I guess this
would be 1 by 2 in this example times

00:06:47.390 --> 00:06:47.400
would be 1 by 2 in this example times
 

00:06:47.400 --> 00:06:54.530
would be 1 by 2 in this example times
0.01 Sigma s there so finally it turns

00:06:54.530 --> 00:06:54.540
0.01 Sigma s there so finally it turns
 

00:06:54.540 --> 00:06:56.900
0.01 Sigma s there so finally it turns
out that sometimes they can be better

00:06:56.900 --> 00:06:56.910
out that sometimes they can be better
 

00:06:56.910 --> 00:07:01.430
out that sometimes they can be better
constants than 0.01 when you're training

00:07:01.430 --> 00:07:01.440
constants than 0.01 when you're training
 

00:07:01.440 --> 00:07:04.460
constants than 0.01 when you're training
a neural network with just one hidden

00:07:04.460 --> 00:07:04.470
a neural network with just one hidden
 

00:07:04.470 --> 00:07:06.890
a neural network with just one hidden
layer this is a relatively shallow you

00:07:06.890 --> 00:07:06.900
layer this is a relatively shallow you
 

00:07:06.900 --> 00:07:08.570
layer this is a relatively shallow you
know network without too many hidden

00:07:08.570 --> 00:07:08.580
know network without too many hidden
 

00:07:08.580 --> 00:07:11.690
know network without too many hidden
layers set into 0.01 will probably work

00:07:11.690 --> 00:07:11.700
layers set into 0.01 will probably work
 

00:07:11.700 --> 00:07:14.240
layers set into 0.01 will probably work
ok but when you're training a very very

00:07:14.240 --> 00:07:14.250
ok but when you're training a very very
 

00:07:14.250 --> 00:07:16.700
ok but when you're training a very very
deep neural network then you might want

00:07:16.700 --> 00:07:16.710
deep neural network then you might want
 

00:07:16.710 --> 00:07:19.010
deep neural network then you might want
to pick a different constant in 0.001

00:07:19.010 --> 00:07:19.020
to pick a different constant in 0.001
 

00:07:19.020 --> 00:07:21.950
to pick a different constant in 0.001
and in next week's material we'll talk a

00:07:21.950 --> 00:07:21.960
and in next week's material we'll talk a
 

00:07:21.960 --> 00:07:24.320
and in next week's material we'll talk a
little bit about how and when you might

00:07:24.320 --> 00:07:24.330
little bit about how and when you might
 

00:07:24.330 --> 00:07:26.210
little bit about how and when you might
want to choose a different constant than

00:07:26.210 --> 00:07:26.220
want to choose a different constant than
 

00:07:26.220 --> 00:07:29.840
want to choose a different constant than
0.01 but either way it will usually end

00:07:29.840 --> 00:07:29.850
0.01 but either way it will usually end
 

00:07:29.850 --> 00:07:32.630
0.01 but either way it will usually end
up being a relatively small number so

00:07:32.630 --> 00:07:32.640
up being a relatively small number so
 

00:07:32.640 --> 00:07:34.850
up being a relatively small number so
that's it so this week videos you now

00:07:34.850 --> 00:07:34.860
that's it so this week videos you now
 

00:07:34.860 --> 00:07:38.000
that's it so this week videos you now
know how to set up a neural network of a

00:07:38.000 --> 00:07:38.010
know how to set up a neural network of a
 

00:07:38.010 --> 00:07:38.540
know how to set up a neural network of a
hidden layer

00:07:38.540 --> 00:07:38.550
hidden layer
 

00:07:38.550 --> 00:07:40.610
hidden layer
initialize the parameters make

00:07:40.610 --> 00:07:40.620
initialize the parameters make
 

00:07:40.620 --> 00:07:42.620
initialize the parameters make
predictions using forward prop as well

00:07:42.620 --> 00:07:42.630
predictions using forward prop as well
 

00:07:42.630 --> 00:07:44.150
predictions using forward prop as well
as compute derivatives and it's been

00:07:44.150 --> 00:07:44.160
as compute derivatives and it's been
 

00:07:44.160 --> 00:07:46.670
as compute derivatives and it's been
gradient descent using back prop so that

00:07:46.670 --> 00:07:46.680
gradient descent using back prop so that
 

00:07:46.680 --> 00:07:49.220
gradient descent using back prop so that
you should be able to do the quizzes as

00:07:49.220 --> 00:07:49.230
you should be able to do the quizzes as
 

00:07:49.230 --> 00:07:50.920
you should be able to do the quizzes as
well as disease programming exercises

00:07:50.920 --> 00:07:50.930
well as disease programming exercises
 

00:07:50.930 --> 00:07:53.210
well as disease programming exercises
best of luck with that I hope you have

00:07:53.210 --> 00:07:53.220
best of luck with that I hope you have
 

00:07:53.220 --> 00:07:55.490
best of luck with that I hope you have
fun with pro exercise and look forward

00:07:55.490 --> 00:07:55.500
fun with pro exercise and look forward
 

00:07:55.500 --> 00:07:56.870
fun with pro exercise and look forward
to seeing you in the week school

00:07:56.870 --> 00:07:56.880
to seeing you in the week school
 

00:07:56.880 --> 00:07:59.420
to seeing you in the week school
materials

