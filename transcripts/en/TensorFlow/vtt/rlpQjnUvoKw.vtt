WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.850
[MUSIC PLAYING]

00:00:07.240 --> 00:00:08.740
MALCOLM REYNOLDS:
My name is Malcolm

00:00:08.740 --> 00:00:10.530
and I'm a research
engineer at DeepMind.

00:00:10.530 --> 00:00:12.030
TAMARA NORMAN: Hi,
my name is Tamara

00:00:12.030 --> 00:00:13.853
and I'm a software
engineer at DeepMind.

00:00:13.853 --> 00:00:16.270
MALCOLM REYNOLDS: So we're
here today to talk about Sonnet

00:00:16.270 --> 00:00:18.103
and hopefully answer
some questions for you,

00:00:18.103 --> 00:00:20.800
so what is Sonnet, why
DeepMind has found it useful,

00:00:20.800 --> 00:00:22.282
and why it might
be useful to you,

00:00:22.282 --> 00:00:23.740
and then talk a
bit about our plans

00:00:23.740 --> 00:00:26.210
for the future of the library.

00:00:26.210 --> 00:00:28.310
So Sonnet is DeepMind's
library for constructing

00:00:28.310 --> 00:00:29.870
neural networks in TensorFlow.

00:00:29.870 --> 00:00:31.610
We've been working
on it since 2016

00:00:31.610 --> 00:00:34.110
and it's been in open source
for the last couple of years.

00:00:34.110 --> 00:00:37.400
And you can try the TensorFlow
1.0 version with this command

00:00:37.400 --> 00:00:38.610
line here.

00:00:38.610 --> 00:00:41.000
But you might be thinking,
sounds interesting.

00:00:41.000 --> 00:00:42.375
Why does this exist?

00:00:42.375 --> 00:00:44.000
And that is a very
legitimate question.

00:00:44.000 --> 00:00:46.220
So to answer that, we
have to take a little trip

00:00:46.220 --> 00:00:47.070
down memory lane.

00:00:47.070 --> 00:00:50.160
So 2014, TensorFlow is
in development at Brain,

00:00:50.160 --> 00:00:52.340
but there's no
open-source release yet.

00:00:52.340 --> 00:00:55.850
And unrelatedly I guess,
DeepMind is acquired by Google.

00:00:55.850 --> 00:00:58.220
So a little bit of
code archeology here.

00:00:58.220 --> 00:01:02.720
So does anybody recognize what
system this is written in?

00:01:02.720 --> 00:01:06.188
Hands up if it looks
vaguely familiar.

00:01:06.188 --> 00:01:08.622
OK, not too many hands.

00:01:08.622 --> 00:01:11.080
So I censored some of this just
to make it slightly harder.

00:01:11.080 --> 00:01:14.830
So now it may be clearer
that this is actually

00:01:14.830 --> 00:01:17.650
Torch7 code, or Lua Torch.

00:01:17.650 --> 00:01:20.200
And specifically, we're
defining a module here.

00:01:20.200 --> 00:01:22.810
That's what the red
arrow is pointing to.

00:01:22.810 --> 00:01:25.300
So DeepMind was an
entirely Torch7 operation,

00:01:25.300 --> 00:01:28.450
at least in the research
side when I joined.

00:01:28.450 --> 00:01:30.860
And it's a really great
piece of software.

00:01:30.860 --> 00:01:33.880
If you're interested in the
sort of history of deep learning

00:01:33.880 --> 00:01:36.340
abstractions, you know,
the abstractions in Torch

00:01:36.340 --> 00:01:39.460
really go back to stuff
from the '90s, like Lush.

00:01:39.460 --> 00:01:41.987
So fast forward a
few years, we have

00:01:41.987 --> 00:01:44.320
dozens of research projects
going on, hundreds of people

00:01:44.320 --> 00:01:47.050
involved, and being able to
kind of quickly share code

00:01:47.050 --> 00:01:48.980
between the projects
was really critical.

00:01:48.980 --> 00:01:51.310
So somebody comes
up with a new RNN,

00:01:51.310 --> 00:01:53.710
everyone else can try
that out with minimal fuss

00:01:53.710 --> 00:01:55.897
and no changing of the code.

00:01:55.897 --> 00:01:57.730
And we thought we had
a pretty good workflow

00:01:57.730 --> 00:01:59.105
and we were pretty
happy with it,

00:01:59.105 --> 00:02:01.090
and then we decided
to just get rid of it

00:02:01.090 --> 00:02:02.740
all and start from scratch.

00:02:02.740 --> 00:02:04.780
Seems questionable.

00:02:04.780 --> 00:02:08.801
But the reason is we were
transitioning to TensorFlow.

00:02:08.801 --> 00:02:10.509
And obviously, there's
many great reasons

00:02:10.509 --> 00:02:11.259
to use TensorFlow.

00:02:11.259 --> 00:02:13.780
We've been hearing about
them for the last few days.

00:02:13.780 --> 00:02:15.940
Top of the list for us
was better alignment

00:02:15.940 --> 00:02:18.760
and collaboration with
our colleagues at Google.

00:02:18.760 --> 00:02:21.610
It was designed for distributed
training from day one, which

00:02:21.610 --> 00:02:23.890
Torch really wasn't, and
that was becoming a problem

00:02:23.890 --> 00:02:25.290
in some of our research.

00:02:25.290 --> 00:02:29.210
And we wanted to really take
the best advantage of TPUs,

00:02:29.210 --> 00:02:32.040
and obviously, TensorFlow
is the best way to use them.

00:02:32.040 --> 00:02:33.740
And we knew from
initial assessments

00:02:33.740 --> 00:02:35.615
this was going to be
flexible enough to build

00:02:35.615 --> 00:02:37.443
whatever we need on top of.

00:02:37.443 --> 00:02:39.360
But we weren't actually
starting from scratch.

00:02:39.360 --> 00:02:41.820
We had some stuff we
really wanted to preserve.

00:02:41.820 --> 00:02:44.917
And it's not that we want
the same APIs as Lua Torch.

00:02:44.917 --> 00:02:46.750
That would not have
been the right decision.

00:02:46.750 --> 00:02:49.040
But we wanted the same
engineering philosophy.

00:02:49.040 --> 00:02:51.250
So what we mean by this
is like hopefully we're

00:02:51.250 --> 00:02:53.860
going to state
through this talk.

00:02:53.860 --> 00:02:55.380
So first of all,
kind of composable

00:02:55.380 --> 00:02:57.250
modules using
object-oriented programming.

00:02:57.250 --> 00:02:59.020
Now it's definitely
totally valid

00:02:59.020 --> 00:03:02.050
to have a kind of purely
functional library.

00:03:02.050 --> 00:03:05.590
But we found that when you start
having lots of variable reuse

00:03:05.590 --> 00:03:08.650
RNNs, those kind of
features, objects really

00:03:08.650 --> 00:03:09.950
made a lot more sense.

00:03:09.950 --> 00:03:11.820
So we definitely
want to keep that.

00:03:11.820 --> 00:03:13.768
We wanted to decouple
the model definition

00:03:13.768 --> 00:03:15.060
from how it's actually trained.

00:03:15.060 --> 00:03:17.680
And this is really key,
and I will comment on this

00:03:17.680 --> 00:03:18.950
again shortly.

00:03:18.950 --> 00:03:20.780
Hackability is really crucial.

00:03:20.780 --> 00:03:23.170
We don't think we can anticipate
everything our research

00:03:23.170 --> 00:03:26.140
scientists might want, like
years or even months ahead

00:03:26.140 --> 00:03:26.890
of time.

00:03:26.890 --> 00:03:28.840
But what we can do is
have code that they

00:03:28.840 --> 00:03:32.290
are able to dig down,
hopefully not too far,

00:03:32.290 --> 00:03:34.750
make the changes they need to,
maybe by forking the module,

00:03:34.750 --> 00:03:37.090
and we're kind of
comfortable with that,

00:03:37.090 --> 00:03:39.397
and then continue
with their research.

00:03:39.397 --> 00:03:41.230
And it's important to
emphasize we're really

00:03:41.230 --> 00:03:42.490
optimizing for research.

00:03:42.490 --> 00:03:44.532
You've heard a lot about
a lot of great solutions

00:03:44.532 --> 00:03:46.780
for kind of moving
things into production.

00:03:46.780 --> 00:03:49.630
And that's really important,
but for DeepMind research, that

00:03:49.630 --> 00:03:51.250
really happens very rarely.

00:03:51.250 --> 00:03:53.410
So we prefer to
optimize the libraries

00:03:53.410 --> 00:03:57.255
based on research progress,
next paper, that kind of thing.

00:03:57.255 --> 00:03:59.380
And overall, we're looking
for a standard interface

00:03:59.380 --> 00:04:01.713
with really minimal assumptions
that doesn't prevent you

00:04:01.713 --> 00:04:03.720
from doing anything.

00:04:03.720 --> 00:04:05.022
So composability.

00:04:05.022 --> 00:04:05.980
What do I mean by this?

00:04:05.980 --> 00:04:08.890
Hopefully, anyone who's
used an LSTM would be like,

00:04:08.890 --> 00:04:11.380
yes, you can implement this
out of some Linear modules,

00:04:11.380 --> 00:04:13.210
and they contain the
variables and LSTM

00:04:13.210 --> 00:04:15.340
is a thing around all of that.

00:04:15.340 --> 00:04:17.079
But you can go many
levels up from there.

00:04:17.079 --> 00:04:18.920
So the differentiable
neural computer,

00:04:18.920 --> 00:04:21.100
which was the first project
I worked on at DeepMind,

00:04:21.100 --> 00:04:22.930
you know, that's a
module that contains

00:04:22.930 --> 00:04:25.580
an LSTM plus some other stuff.

00:04:25.580 --> 00:04:27.590
And we worked on that
and got the paper out,

00:04:27.590 --> 00:04:29.500
and then the code
is sitting there,

00:04:29.500 --> 00:04:31.870
and then another team could
come along and be like, hey,

00:04:31.870 --> 00:04:34.280
we have some system
which requires an RNN.

00:04:34.280 --> 00:04:35.950
It's currently an
LSTM, but maybe we'll

00:04:35.950 --> 00:04:37.877
just try this instead.

00:04:37.877 --> 00:04:39.710
And you know, it's been
used in many places,

00:04:39.710 --> 00:04:42.340
but I think the most significant
maybe was the Capture the Flag

00:04:42.340 --> 00:04:44.590
work, which had some really
cool results, specifically

00:04:44.590 --> 00:04:45.930
with the DNC controller.

00:04:45.930 --> 00:04:47.740
And you know, if they
could reuse the code,

00:04:47.740 --> 00:04:50.065
like they didn't need to
ask us about how it worked.

00:04:50.065 --> 00:04:51.690
They didn't even need
to read the code.

00:04:51.690 --> 00:04:54.840
They're just like, this is an
RNN that conforms to some API,

00:04:54.840 --> 00:04:58.273
we'll just drop it
in, and it works.

00:04:58.273 --> 00:05:00.190
Orthogonality to the
training setup dimension.

00:05:00.190 --> 00:05:02.680
So I did a quick survey.

00:05:02.680 --> 00:05:06.590
We have roughly two setups
for unsupervised learning,

00:05:06.590 --> 00:05:09.310
four for reinforcement
learning, and many more setups

00:05:09.310 --> 00:05:10.150
for single projects.

00:05:10.150 --> 00:05:11.380
And to be clear,
what I mean here,

00:05:11.380 --> 00:05:13.713
this is kind of everything
that goes on above the model.

00:05:13.713 --> 00:05:15.610
Like how do you
feed it the data,

00:05:15.610 --> 00:05:17.860
how do you launch experiments,
monitor them, like

00:05:17.860 --> 00:05:19.983
what configurability is
there for the researchers.

00:05:19.983 --> 00:05:21.400
All this kind of
stuff is what I'm

00:05:21.400 --> 00:05:23.900
referring to as training setup.

00:05:23.900 --> 00:05:25.750
And this might seem
like a ton of, you know,

00:05:25.750 --> 00:05:27.810
repeated effort and
duplicated code,

00:05:27.810 --> 00:05:30.160
but we don't actually
view this as a problem.

00:05:30.160 --> 00:05:33.560
These different setups exist
for different research goals.

00:05:33.560 --> 00:05:36.790
And trying to push everything
into a single training setup,

00:05:36.790 --> 00:05:40.030
even just for RL,
it's been tried.

00:05:40.030 --> 00:05:41.390
We don't think it works.

00:05:41.390 --> 00:05:43.690
So we're kind of happy with
lots of different setups

00:05:43.690 --> 00:05:44.973
which kind of coexist.

00:05:44.973 --> 00:05:46.390
And the reason
it's not a problem,

00:05:46.390 --> 00:05:48.430
apart from the serving
different research goals,

00:05:48.430 --> 00:05:51.470
is that we can reuse Sonnet
modules between all of them.

00:05:51.470 --> 00:05:54.730
And that means we're not like
redoing the DNC from scratch.

00:05:57.480 --> 00:05:59.660
So I've talked a bunch about
what we've already done

00:05:59.660 --> 00:06:01.190
with Sonnet and TensorFlow 1.

00:06:01.190 --> 00:06:03.782
Tamara is going to talk
a bit about the future.

00:06:03.782 --> 00:06:04.740
TAMARA NORMAN: So yeah.

00:06:04.740 --> 00:06:05.835
Cheers, Malcolm.

00:06:05.835 --> 00:06:07.730
So what will Sonnet
look like in TF 2?

00:06:07.730 --> 00:06:09.340
DeepMind are really
excited for TF 2,

00:06:09.340 --> 00:06:11.580
and are planning
once again to invest

00:06:11.580 --> 00:06:14.820
and changing our workflow
in adopting TF 2.

00:06:14.820 --> 00:06:18.060
Eager execution has already been
widely tried by researchers,

00:06:18.060 --> 00:06:20.580
and they really enjoy
the improved flexibility,

00:06:20.580 --> 00:06:23.168
debugging, and simplicity
that come from this.

00:06:23.168 --> 00:06:24.960
And they're looking
forward to the benefits

00:06:24.960 --> 00:06:27.630
of this being part of TF 2.

00:06:27.630 --> 00:06:30.150
But one thing we've noticed
is that Sonnet 1 was

00:06:30.150 --> 00:06:34.230
based on features, like
variable scripts quite heavily,

00:06:34.230 --> 00:06:35.790
and these are going away.

00:06:35.790 --> 00:06:37.900
This isn't something
that we're worried about.

00:06:37.900 --> 00:06:40.440
We're really excited
and follow TF

00:06:40.440 --> 00:06:43.740
into this much more
natural and Pythonic world.

00:06:43.740 --> 00:06:47.770
So how do we plan on making
Sonnet in this new world?

00:06:47.770 --> 00:06:51.480
So we've built tf.Module
as the base of Sonnet 2.

00:06:51.480 --> 00:06:53.940
This is a stateful container,
which provides both variable

00:06:53.940 --> 00:06:55.230
and model tracking.

00:06:55.230 --> 00:06:57.060
It's been designed by
engineers at DeepMind

00:06:57.060 --> 00:06:59.490
in collaboration between
many individuals at DeepMind

00:06:59.490 --> 00:07:03.270
and Brain, learning lessons from
both Sonnet 1 and Lua Torch.

00:07:03.270 --> 00:07:05.460
It's now been upstreamed
into TensorFlow.

00:07:05.460 --> 00:07:07.080
And you can try it
out in the alpha

00:07:07.080 --> 00:07:10.080
that's been released over
the course of Dev Summit.

00:07:10.080 --> 00:07:13.170
It will soon form the
basis of many higher level

00:07:13.170 --> 00:07:17.280
components in TF, including
things like TF Keras.

00:07:17.280 --> 00:07:18.650
So what about modules?

00:07:18.650 --> 00:07:20.750
Modules can have
multiple forward methods,

00:07:20.750 --> 00:07:23.780
addressing a limitation that
we found within Sonnet 1.

00:07:23.780 --> 00:07:26.330
They can be nested
arbitrarily, allowing

00:07:26.330 --> 00:07:27.980
for the creation
of complex models,

00:07:27.980 --> 00:07:32.750
like the [INAUDIBLE] that
was talked about earlier.

00:07:32.750 --> 00:07:34.880
And they have automatic
name scoping, which

00:07:34.880 --> 00:07:38.310
allows for easy debugging.

00:07:38.310 --> 00:07:40.177
So what are we trying
to do with Sonnet 2?

00:07:40.177 --> 00:07:41.760
We're aiming to
create a library which

00:07:41.760 --> 00:07:44.340
makes very few assumptions
about what users want to do,

00:07:44.340 --> 00:07:46.860
both with their networks
and their training loops.

00:07:46.860 --> 00:07:49.830
It's been designed in close
collaboration with researchers.

00:07:49.830 --> 00:07:52.620
And we think it's a lightweight,
simple abstraction that's

00:07:52.620 --> 00:07:55.950
as close to the
maths as it can be.

00:07:55.950 --> 00:07:57.517
So what the features of Sonnet?

00:07:57.517 --> 00:07:59.850
Sonnet has the ability to
have multiple forward methods,

00:07:59.850 --> 00:08:01.840
something enabled by tf.Module.

00:08:01.840 --> 00:08:03.212
But what does that really mean?

00:08:03.212 --> 00:08:05.670
There will be an easy way to
create [? excessive ?] methods

00:08:05.670 --> 00:08:07.890
on an object that
can have access

00:08:07.890 --> 00:08:09.750
to the same state and variables.

00:08:09.750 --> 00:08:11.430
An example of where
this is useful

00:08:11.430 --> 00:08:14.070
will be in a few slides time.

00:08:14.070 --> 00:08:15.900
One thing that we've
found that's really

00:08:15.900 --> 00:08:19.710
crucial with Sonnet 1 is that
it's really easy to compose it

00:08:19.710 --> 00:08:21.290
with other systems.

00:08:21.290 --> 00:08:23.850
Sonnet 1 works out of
the box with Replicator,

00:08:23.850 --> 00:08:26.700
which is our internal solution
to distributed training,

00:08:26.700 --> 00:08:28.950
even though these libraries
were never designed

00:08:28.950 --> 00:08:30.240
with each other in mind.

00:08:30.240 --> 00:08:33.250
They don't even have a
dependency on each other.

00:08:33.250 --> 00:08:35.850
They both just use
TensorFlow under the hood.

00:08:35.850 --> 00:08:37.559
It's up to users
to define how they

00:08:37.559 --> 00:08:41.190
want to compose their system
together and decide where they

00:08:41.190 --> 00:08:43.013
want to put things
like that [INAUDIBLE]

00:08:43.013 --> 00:08:45.180
to ensure that the gradients
are the same across all

00:08:45.180 --> 00:08:46.710
the devices.

00:08:46.710 --> 00:08:49.940
We plan for the same philosophy
to be implemented in Sonnet 2.

00:08:49.940 --> 00:08:52.840
Replicator has been merged
with Distribution Strategy,

00:08:52.840 --> 00:08:55.163
so you'll all be
able to try this out.

00:08:55.163 --> 00:08:57.830
And you can find more details on
Replicator on the DeepMind blog

00:08:57.830 --> 00:08:59.100
post.

00:08:59.100 --> 00:09:01.200
So what do we want
to do within Sonnet?

00:09:01.200 --> 00:09:04.350
We also want to continue
to provide the flexibility

00:09:04.350 --> 00:09:06.180
and composability
that we've already

00:09:06.180 --> 00:09:09.300
heard about that they have in
v1, such as things that models

00:09:09.300 --> 00:09:11.590
can be composed
in arbitrary ways,

00:09:11.590 --> 00:09:13.800
and that no training
loop is provided.

00:09:13.800 --> 00:09:15.480
We still don't want
to aim to predict

00:09:15.480 --> 00:09:17.740
where the research will go.

00:09:17.740 --> 00:09:20.090
So what does the code look like?

00:09:20.090 --> 00:09:21.830
This is the linear layer.

00:09:21.830 --> 00:09:23.620
It's pretty much the
implementation, just

00:09:23.620 --> 00:09:25.500
without some of the comments.

00:09:25.500 --> 00:09:27.310
And one thing we've
been really excited by

00:09:27.310 --> 00:09:29.470
is how simple an
implementation we've

00:09:29.470 --> 00:09:32.850
been able to create on top
of tf.Module and TF 2 is.

00:09:32.850 --> 00:09:35.350
Research is by nature
about pushing boundaries.

00:09:35.350 --> 00:09:38.200
And we think these simple
layers with minimal boilerplate

00:09:38.200 --> 00:09:40.810
will encourage forking and
inheritance as a way of trying

00:09:40.810 --> 00:09:43.630
out variations.

00:09:43.630 --> 00:09:45.730
Another thing I just
wanted to highlight

00:09:45.730 --> 00:09:48.190
is that modules don't
necessarily have to be layers

00:09:48.190 --> 00:09:50.650
or even have state.

00:09:50.650 --> 00:09:54.220
They just inherit from tf.Module
and get the name scoping

00:09:54.220 --> 00:09:55.160
from that.

00:09:55.160 --> 00:09:57.890
In Sonnet, we also implement
optimizers of modules.

00:09:57.890 --> 00:09:59.980
They are simply nodes
on a graph and benefit

00:09:59.980 --> 00:10:03.250
with the name scoping from
tf.Module, which greatly adds

00:10:03.250 --> 00:10:05.200
to the visualization
of graphs and debugging

00:10:05.200 --> 00:10:07.000
of operations later on.

00:10:07.000 --> 00:10:09.940
The forward method on optimizers
isn't even [INAUDIBLE]..

00:10:09.940 --> 00:10:13.000
It's the supply update, which
doesn't return anything.

00:10:13.000 --> 00:10:15.700
It just takes your
gradients and your variables

00:10:15.700 --> 00:10:17.860
and applies them together.

00:10:17.860 --> 00:10:19.690
And as promised,
here's the example

00:10:19.690 --> 00:10:21.530
of using multiple
forward methods.

00:10:21.530 --> 00:10:23.740
Making something like a
variation [INAUDIBLE] encoder

00:10:23.740 --> 00:10:26.350
or encapsulate both an
encoder and a decoder,

00:10:26.350 --> 00:10:28.780
so they can be trained
together whilst exposing both

00:10:28.780 --> 00:10:31.330
the forward methods on them
both as a way for testing

00:10:31.330 --> 00:10:33.830
and using them later on.

00:10:33.830 --> 00:10:36.700
So what have we found when
implementing Sonnet in TF 2?

00:10:36.700 --> 00:10:38.200
We've been doing
it for a while now,

00:10:38.200 --> 00:10:40.200
and we found that we've
had a much simpler, more

00:10:40.200 --> 00:10:41.710
understandable code than before.

00:10:41.710 --> 00:10:44.140
The debugging has been vastly
improved and less painful

00:10:44.140 --> 00:10:45.042
with Eager mode.

00:10:45.042 --> 00:10:47.250
And we have initially seen
some significant speed ups

00:10:47.250 --> 00:10:48.550
with TF function.

00:10:48.550 --> 00:10:52.010
I think this will be a hugely
powerful tool going forwards.

00:10:52.010 --> 00:10:53.710
So when can you try this out?

00:10:53.710 --> 00:10:56.410
The road map for
Sonnet 2 is on the way.

00:10:56.410 --> 00:10:57.580
It's on the way.

00:10:57.580 --> 00:11:00.430
tf.Module is already
available in the alpha release

00:11:00.430 --> 00:11:01.530
in TF Core.

00:11:01.530 --> 00:11:03.680
We're iterating currently
on basic modules

00:11:03.680 --> 00:11:07.103
now with researchers with the
alpha release on GitHub soon.

00:11:07.103 --> 00:11:09.770
And hopefully, we'll have a beta
release sometime in the summer.

00:11:09.770 --> 00:11:10.770
And now over to Malcolm.

00:11:10.770 --> 00:11:12.270
He's going to talk
to you about some

00:11:12.270 --> 00:11:14.510
of the projects, some of
which you may have heard of,

00:11:14.510 --> 00:11:17.110
which have used and been
enabled by features of Sonnet 1

00:11:17.110 --> 00:11:19.880
that we're taking
forward into Sonnet 2.

00:11:19.880 --> 00:11:21.270
MALCOLM REYNOLDS:
Thanks, Tamara.

00:11:21.270 --> 00:11:24.630
So just a few samples
of stuff that's

00:11:24.630 --> 00:11:26.240
been done over
the past few years

00:11:26.240 --> 00:11:28.440
in Sonnet and TensorFlow 1.

00:11:28.440 --> 00:11:30.480
So this is the
generative query network.

00:11:30.480 --> 00:11:34.710
So it's kind of receiving,
like, 2D rendering observations

00:11:34.710 --> 00:11:35.970
at the top.

00:11:35.970 --> 00:11:39.870
And then it's able to infer
what the 3D environment looks

00:11:39.870 --> 00:11:42.640
like and put the camera
in any new position.

00:11:42.640 --> 00:11:46.110
And you can see here the
different probabilistic views

00:11:46.110 --> 00:11:48.060
of what the whole
environment could look like.

00:11:48.060 --> 00:11:49.590
And this is, like,
a classic example

00:11:49.590 --> 00:11:52.357
of several papers
worth of research,

00:11:52.357 --> 00:11:53.940
which are all like
modules, which then

00:11:53.940 --> 00:11:55.260
get kind of built on top of.

00:11:55.260 --> 00:11:58.170
So the original paper was draw
back in the Lua Torch days.

00:11:58.170 --> 00:12:00.450
And then we expanded on
that with convolutional draw

00:12:00.450 --> 00:12:03.560
and then the generative
query network.

00:12:03.560 --> 00:12:05.430
AlphaStar, something
quite recent.

00:12:05.430 --> 00:12:07.500
And Sonnet was really
key here because there's

00:12:07.500 --> 00:12:10.530
no assumptions on the
shape of what's coming

00:12:10.530 --> 00:12:12.180
in and out of each module.

00:12:12.180 --> 00:12:14.220
Both the observations
and actions

00:12:14.220 --> 00:12:16.770
for the StarCraft environment
are extremely complex

00:12:16.770 --> 00:12:18.143
hierarchical structures.

00:12:18.143 --> 00:12:19.560
The action space,
you need to have

00:12:19.560 --> 00:12:22.212
a kind of autoregressive
model to pick

00:12:22.212 --> 00:12:23.670
whether you're
going to move a unit

00:12:23.670 --> 00:12:26.370
and then where you're
going to move the unit to.

00:12:26.370 --> 00:12:27.775
And the fact that
Sonnet had been

00:12:27.775 --> 00:12:30.150
built with minimal assumptions
about what goes in and out

00:12:30.150 --> 00:12:32.420
meant that this was
actually quite easy.

00:12:32.420 --> 00:12:36.780
Another interesting aspect here
is how the model is actually

00:12:36.780 --> 00:12:37.560
trained.

00:12:37.560 --> 00:12:40.600
So, the details are on a blog
post on the DeepMind website.

00:12:40.600 --> 00:12:43.770
But the AlphaStar League is
quite a complicated set up

00:12:43.770 --> 00:12:45.480
with lots of different agents.

00:12:45.480 --> 00:12:48.660
And they train against
frozen versions of themselves

00:12:48.660 --> 00:12:50.673
from the past and try and
learn to exploit them.

00:12:50.673 --> 00:12:52.090
And then you make
a new agent that

00:12:52.090 --> 00:12:54.470
tries to learn to exploit
all the previous ones.

00:12:54.470 --> 00:12:57.612
And this kind of custom setup is
a good reason why we don't just

00:12:57.612 --> 00:12:59.820
try and stick with one
training style for everything,

00:12:59.820 --> 00:13:03.067
because even a year ago, I don't
think we could have anticipated

00:13:03.067 --> 00:13:04.650
what kind of elaborate
training scheme

00:13:04.650 --> 00:13:07.890
would have been necessary
to make this work.

00:13:07.890 --> 00:13:09.210
And finally, BigGAN.

00:13:09.210 --> 00:13:10.930
I suspect a lot of people
are quite familiar with this.

00:13:10.930 --> 00:13:12.305
But just in case
you're not, this

00:13:12.305 --> 00:13:14.130
is not a real dog,
not a real photo.

00:13:14.130 --> 00:13:16.390
This is randomly generated.

00:13:16.390 --> 00:13:18.570
And I think the key
aspects of Sonnet

00:13:18.570 --> 00:13:21.480
here was that the underlying
GAN architecture was, like,

00:13:21.480 --> 00:13:23.745
maybe relatively similar
to things that had already

00:13:23.745 --> 00:13:25.080
been in the literature.

00:13:25.080 --> 00:13:26.580
There was a few key
components which

00:13:26.580 --> 00:13:28.258
made it really a lot better.

00:13:28.258 --> 00:13:30.800
One of them was being able to
train it on an entire [? TPE ?]

00:13:30.800 --> 00:13:31.300
pod.

00:13:31.300 --> 00:13:34.800
And a key aspect here was
cross replica [INAUDIBLE]..

00:13:34.800 --> 00:13:37.750
So the researchers
involved could just say,

00:13:37.750 --> 00:13:40.320
here's a normal GAN made
out of normal modules.

00:13:40.320 --> 00:13:42.420
We need to, like,
hack the batch norm,

00:13:42.420 --> 00:13:44.250
like, completely
replace it to do kind

00:13:44.250 --> 00:13:46.540
of interesting [? TPE ?] stuff.

00:13:46.540 --> 00:13:48.630
And then everything
else is just kind of--

00:13:48.630 --> 00:13:50.312
just works as normal.

00:13:50.312 --> 00:13:52.020
And obviously, we
can't talk about BigGAN

00:13:52.020 --> 00:13:54.600
without briefly having
a look at dogball.

00:13:54.600 --> 00:13:56.640
So here's a little a of dogball.

00:13:56.640 --> 00:13:58.600
And then, that's
probably enough.

00:13:58.600 --> 00:14:01.055
So in conclusion,
Sonnet is a library

00:14:01.055 --> 00:14:03.180
that's been designed in
collaboration with DeepMind

00:14:03.180 --> 00:14:04.140
research scientists.

00:14:04.140 --> 00:14:06.473
We've been using it very
happily for the last few years.

00:14:06.473 --> 00:14:10.230
We're looking forward to using
it in the TensorFlow 2 version.

00:14:10.230 --> 00:14:12.340
It works for us and
it might work for you.

00:14:12.340 --> 00:14:13.470
Thank you very much.

00:14:13.470 --> 00:14:14.970
[APPLAUSE]

00:14:17.370 --> 00:14:20.120
[MUSIC PLAYING]

