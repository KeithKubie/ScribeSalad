WEBVTT
Kind: captions
Language: en

00:00:00.700 --> 00:00:01.270
All right.

00:00:01.270 --> 00:00:02.061
Quick introduction.

00:00:02.061 --> 00:00:03.680
My name is Trevor Norris.

00:00:03.680 --> 00:00:05.410
I now work at the NodeSource.

00:00:05.410 --> 00:00:07.750
Previously worked at Mozilla,
and then before that,

00:00:07.750 --> 00:00:12.299
worked with companies.

00:00:12.299 --> 00:00:13.340
I did a lot of front-end.

00:00:16.930 --> 00:00:23.120
I worked on doing a lot of
enterprise-level front-end.

00:00:23.120 --> 00:00:24.940
My first language
was JavaScript.

00:00:24.940 --> 00:00:28.890
My only language was JavaScript
for about six years, honestly,

00:00:28.890 --> 00:00:32.380
until I became a Node core
maintainer, at which time,

00:00:32.380 --> 00:00:35.150
I was forced to learn C++.

00:00:35.150 --> 00:00:39.710
And ever since then, I just
started moving down the stack.

00:00:39.710 --> 00:00:42.860
Currently, I'm learning
C, and aspiring

00:00:42.860 --> 00:00:45.880
to become one of the libuv
maintainers, as well.

00:00:45.880 --> 00:00:49.330
So I'm just going completely
the opposite direction,

00:00:49.330 --> 00:00:53.460
which is really odd.

00:00:53.460 --> 00:00:59.590
But I do have to say that
coming from client side

00:00:59.590 --> 00:01:02.560
and going to server side, and
then going all the way down

00:01:02.560 --> 00:01:05.920
to systems-level development
has given me a really, I'd

00:01:05.920 --> 00:01:11.290
say, unique insight
into how things work.

00:01:11.290 --> 00:01:16.110
Tonight, my talk is
going to be about dealing

00:01:16.110 --> 00:01:19.540
with a lot of requests.

00:01:19.540 --> 00:01:20.520
Let's just get kicking.

00:01:24.090 --> 00:01:28.590
So I did a very
unscientific analysis

00:01:28.590 --> 00:01:30.950
of just going to a bunch--
I just went to-- what is it,

00:01:30.950 --> 00:01:31.450
Alexis?

00:01:31.450 --> 00:01:34.800
I just went to the top
100 or something websites,

00:01:34.800 --> 00:01:39.910
and just viewed how many
calls were required just

00:01:39.910 --> 00:01:41.490
to load the homepage.

00:01:41.490 --> 00:01:44.800
And the majority of them
were 50 or more requests just

00:01:44.800 --> 00:01:46.194
for the homepage.

00:01:46.194 --> 00:01:47.610
I remember a time
where this would

00:01:47.610 --> 00:01:50.040
have been insane or
ridiculous, and people

00:01:50.040 --> 00:01:51.900
would have never allowed for it.

00:01:51.900 --> 00:01:55.390
But it's very common now.

00:01:55.390 --> 00:01:58.420
And it's a new way of
dealing with things.

00:01:58.420 --> 00:02:06.800
It's a new way of looking about
how we write our applications.

00:02:06.800 --> 00:02:09.919
Now, as it says
here, many of those

00:02:09.919 --> 00:02:15.850
are static content, images,
JavaScript, and CSS.

00:02:15.850 --> 00:02:18.480
I'll get to why I
air-quoted those later.

00:02:18.480 --> 00:02:23.220
But a lot of that is
static, but also, there's

00:02:23.220 --> 00:02:27.300
a good amount of
dynamically-generated content.

00:02:27.300 --> 00:02:29.520
And that is where
Node is sitting today,

00:02:29.520 --> 00:02:34.400
at that level where your
dynamically-generated content

00:02:34.400 --> 00:02:38.370
is being processed and
delivered through Node.

00:02:38.370 --> 00:02:41.230
Whether it's doing a
back-end call and delivering

00:02:41.230 --> 00:02:45.970
JSON down to the client, so that
the client can render the DOM

00:02:45.970 --> 00:02:48.530
on the fly, or
whether templates are

00:02:48.530 --> 00:02:51.070
being generated on
the server, and being

00:02:51.070 --> 00:02:52.070
delivered to the client.

00:02:54.660 --> 00:02:59.730
So here are several of
the most popular pages.

00:02:59.730 --> 00:03:02.800
And that number is
the number of requests

00:03:02.800 --> 00:03:05.040
just to load the home page.

00:03:05.040 --> 00:03:08.650
And then, I broke it up into a
number of HTML, and JavaScript,

00:03:08.650 --> 00:03:12.030
and CSS files, specifically
for each of those.

00:03:12.030 --> 00:03:16.690
And I'll get into later why
those three are important.

00:03:16.690 --> 00:03:19.670
The rest of them are
images and whatnot.

00:03:19.670 --> 00:03:23.660
But as you can see,
that's borderline insane.

00:03:23.660 --> 00:03:25.420
I mean, I'm sorry.

00:03:25.420 --> 00:03:30.430
Over 250 individual requests
just to visit the home page?

00:03:30.430 --> 00:03:31.750
That's a lot.

00:03:31.750 --> 00:03:37.110
I have a really fast connection,
but-- fortunately, amazon.com

00:03:37.110 --> 00:03:40.600
has a mobile site, so
they don't do that.

00:03:40.600 --> 00:03:42.140
But a lot of people don't.

00:03:42.140 --> 00:03:44.310
And so when you're
doing crap like this,

00:03:44.310 --> 00:03:47.510
and then you want mobile users
to try and use your website,

00:03:47.510 --> 00:03:48.530
you're crap out of luck.

00:03:48.530 --> 00:03:51.170
That's just not going to work.

00:03:51.170 --> 00:03:54.690
So common uses for
Node I've seen today

00:03:54.690 --> 00:04:00.210
are for template generation,
REST APIs, static content

00:04:00.210 --> 00:04:03.630
concatenation, and
metrics reporting.

00:04:03.630 --> 00:04:06.810
The reason I place the template
generation and the REST

00:04:06.810 --> 00:04:10.410
APIs separately
instead of together

00:04:10.410 --> 00:04:17.570
is-- excuse me-- is because the
template generation will make

00:04:17.570 --> 00:04:22.019
your application CPU-bound,
where REST APIs generally make

00:04:22.019 --> 00:04:25.820
calls out to other resources.

00:04:25.820 --> 00:04:28.050
And so your application
then becomes

00:04:28.050 --> 00:04:31.930
bound by whatever external
resource you're calling to.

00:04:31.930 --> 00:04:35.120
A lot of times, it'll make
a call out to a resource,

00:04:35.120 --> 00:04:36.550
perhaps in JSON.

00:04:36.550 --> 00:04:39.700
This is usually the case
where you're sending JSON down

00:04:39.700 --> 00:04:41.800
to the client, where the
client will dynamically

00:04:41.800 --> 00:04:42.590
render the DOM.

00:04:46.040 --> 00:04:50.550
But template generation
will make your application

00:04:50.550 --> 00:04:55.040
completely CPU-bound,
which is to be honest,

00:04:55.040 --> 00:04:56.910
I think the oddball
case for Node,

00:04:56.910 --> 00:05:03.480
because Node is touted as
being very friendly to I/O.

00:05:03.480 --> 00:05:06.170
But template generation
is an important thing.

00:05:06.170 --> 00:05:10.190
Its use, as I've seen it,
has increased in popularity

00:05:10.190 --> 00:05:12.420
in generating
templates to generate

00:05:12.420 --> 00:05:15.520
the actual HTML for the page.

00:05:15.520 --> 00:05:17.450
Static content concatenation.

00:05:17.450 --> 00:05:18.910
We'll get more into that later.

00:05:18.910 --> 00:05:20.470
And then, metrics reporting.

00:05:20.470 --> 00:05:25.750
This one is interesting,
because say, for example,

00:05:25.750 --> 00:05:32.360
you wanted to keep track of
how well the ping backed.

00:05:32.360 --> 00:05:35.970
If you want live metrics
on how many people

00:05:35.970 --> 00:05:38.650
are visiting a specific page.

00:05:38.650 --> 00:05:42.510
So that means however
many connections you have,

00:05:42.510 --> 00:05:46.110
you know that you will be
getting that number of requests

00:05:46.110 --> 00:05:49.370
per five seconds or
10 seconds at minimum,

00:05:49.370 --> 00:05:51.710
just for the pingback,
plus any additionals.

00:05:51.710 --> 00:05:54.810
I know there are some that keep
a ridiculous number of metrics

00:05:54.810 --> 00:05:56.560
on everything that's
going on on the page,

00:05:56.560 --> 00:06:00.770
and it's constantly making
requests back to the server.

00:06:00.770 --> 00:06:05.900
And this can put a lot
of load on your servers.

00:06:05.900 --> 00:06:08.470
Part of the reason
for this talk is

00:06:08.470 --> 00:06:11.509
to instruct how to get
around these things.

00:06:11.509 --> 00:06:13.300
A few common problems
I'd like to point out

00:06:13.300 --> 00:06:19.080
are Node applications are
becoming more CPU-bound

00:06:19.080 --> 00:06:21.980
as more temple
generation is happening.

00:06:21.980 --> 00:06:27.740
So the number of requests
expected are dropping.

00:06:27.740 --> 00:06:29.530
Excuse me.

00:06:29.530 --> 00:06:32.080
People delivering static
content through Node,

00:06:32.080 --> 00:06:34.950
which is just crazy.

00:06:34.950 --> 00:06:36.980
Don't deliver static
content through Node.

00:06:36.980 --> 00:06:38.530
You have CDNs.

00:06:38.530 --> 00:06:42.810
You have a huge number
of other resources

00:06:42.810 --> 00:06:45.870
to deliver your truly,
truly static content.

00:06:45.870 --> 00:06:47.000
Don't use Node.

00:06:47.000 --> 00:06:48.260
Make sure it's filtered out.

00:06:48.260 --> 00:06:52.870
One very common one I see
is when the browser hits

00:06:52.870 --> 00:06:56.150
the service, and then
it makes the request.

00:06:56.150 --> 00:07:00.320
But then, straight up, it
makes a Fave icon request.

00:07:00.320 --> 00:07:04.760
Now, the HTTP parser
in Node is not cheap.

00:07:04.760 --> 00:07:06.600
We're going to improve that.

00:07:06.600 --> 00:07:08.150
But currently, as
it was designed

00:07:08.150 --> 00:07:09.760
and for backwards
compatibility, we

00:07:09.760 --> 00:07:12.270
haven't been able to
make as many improvements

00:07:12.270 --> 00:07:13.430
as we've liked.

00:07:13.430 --> 00:07:15.550
And it is not cheap.

00:07:15.550 --> 00:07:18.840
And so that one extra
request-- imagine every request

00:07:18.840 --> 00:07:22.080
automatically generating
one more request,

00:07:22.080 --> 00:07:26.450
just to have it immediately
dumped is just a waste of CPU.

00:07:26.450 --> 00:07:28.970
Make sure those things
are filtered out.

00:07:28.970 --> 00:07:31.320
Now, external API calls
being a bottleneck.

00:07:31.320 --> 00:07:33.580
Well, then you know
Node is doing its job.

00:07:33.580 --> 00:07:38.830
If Node is sitting there
at 5%, maybe 10% CPU usage,

00:07:38.830 --> 00:07:42.650
but you see thousands
of connections

00:07:42.650 --> 00:07:44.216
on your application,
then you know

00:07:44.216 --> 00:07:46.980
you're OK, because you're
waiting for some other service

00:07:46.980 --> 00:07:48.650
to return new information.

00:07:48.650 --> 00:07:51.920
And there's nothing you can do
about that at the Node layer.

00:07:51.920 --> 00:08:00.090
And then, another thing I see
is people allowing connections

00:08:00.090 --> 00:08:02.410
to be received by
a Node process,

00:08:02.410 --> 00:08:04.620
until the process falters.

00:08:04.620 --> 00:08:08.600
What we should be doing is
doing performance testing,

00:08:08.600 --> 00:08:11.030
keeping metrics on our
production systems,

00:08:11.030 --> 00:08:13.410
and finding that sweet spot.

00:08:13.410 --> 00:08:19.560
Finding the arch of where
that begins to decline,

00:08:19.560 --> 00:08:22.720
and don't allow
it to exceed that.

00:08:22.720 --> 00:08:27.550
Shoot to maintain that
as best as possible.

00:08:27.550 --> 00:08:31.180
So these bullet points we'll hit
in more detail, so we'll just

00:08:31.180 --> 00:08:34.340
address them quickly.

00:08:34.340 --> 00:08:38.580
So don't immediately
write small packets.

00:08:38.580 --> 00:08:40.580
Reduce the number of
requests in total.

00:08:40.580 --> 00:08:43.190
As you saw, there are a
ridiculous number of requests.

00:08:43.190 --> 00:08:45.670
For the ones like
images and whatnot,

00:08:45.670 --> 00:08:48.490
that's not such a big thing.

00:08:48.490 --> 00:08:52.060
Not a lot, in my mind,
we can do about that.

00:08:52.060 --> 00:08:55.980
But as far as the numbers you
saw for the JavaScript, CSS,

00:08:55.980 --> 00:09:02.110
and HTML, there is a solution
for those that we can address.

00:09:02.110 --> 00:09:03.900
And then, also
remember that Node

00:09:03.900 --> 00:09:06.570
doesn't gzip your
content by default.

00:09:06.570 --> 00:09:10.520
So a lot of times, you're
streaming out content,

00:09:10.520 --> 00:09:14.470
and you're forgetting that
your rendered template is

00:09:14.470 --> 00:09:19.110
50 to 100 kilobytes,
but you totally

00:09:19.110 --> 00:09:24.020
didn't set your gzip encoding,
and actually gzip your data.

00:09:24.020 --> 00:09:25.574
Node doesn't do
that automatically.

00:09:25.574 --> 00:09:26.990
So you need to
remember to do that

00:09:26.990 --> 00:09:29.250
within your own applications.

00:09:29.250 --> 00:09:29.750
All right.

00:09:29.750 --> 00:09:35.200
So hitting the first bullet
point, don't immediately write.

00:09:35.200 --> 00:09:39.550
I've seen some type of modules.

00:09:39.550 --> 00:09:43.750
And honestly, the more
popular ones default to what's

00:09:43.750 --> 00:09:46.740
called "streaming mode,"
where every time a segment

00:09:46.740 --> 00:09:49.600
of template is
generated, it calls

00:09:49.600 --> 00:09:52.790
res.write for that segment.

00:09:52.790 --> 00:09:55.710
But instead, what
you should be doing

00:09:55.710 --> 00:10:01.400
is aggregating the results, and
sending them out all at once

00:10:01.400 --> 00:10:03.560
in a single request.

00:10:03.560 --> 00:10:05.540
This has two advantages.

00:10:05.540 --> 00:10:10.110
One is it allows you to set
the Content-Length header,

00:10:10.110 --> 00:10:11.560
and that is important.

00:10:11.560 --> 00:10:13.770
As a general rule
of thumb in Node,

00:10:13.770 --> 00:10:16.530
if you don't set the
Content-Length header,

00:10:16.530 --> 00:10:22.110
you will get approximately
50% fewer requests per second

00:10:22.110 --> 00:10:25.990
then if you are to set
the Content-Length.

00:10:25.990 --> 00:10:27.890
If you can set the
Content-Length,

00:10:27.890 --> 00:10:32.270
you will do much better,
in terms of performance.

00:10:32.270 --> 00:10:36.120
Now, some people
might say, well,

00:10:36.120 --> 00:10:38.680
I want to get data out
as quickly as possible.

00:10:38.680 --> 00:10:41.520
Truth be told, that's
actually not the case.

00:10:41.520 --> 00:10:44.290
If you're streaming out lots
of small requests, what's

00:10:44.290 --> 00:10:47.420
happening is you're
incurring assist call time,

00:10:47.420 --> 00:10:52.030
and you're incurring
a-- excuse me--

00:10:52.030 --> 00:10:56.280
you're incurring
garbage collection time.

00:10:56.280 --> 00:10:58.360
There are lots of
small things that

00:10:58.360 --> 00:11:01.400
accumulate with lots
of small writes.

00:11:01.400 --> 00:11:03.850
And hopefully, your
template generation

00:11:03.850 --> 00:11:06.470
isn't lasting more
than 10 milliseconds.

00:11:06.470 --> 00:11:08.250
If it takes you more
than 10 milliseconds

00:11:08.250 --> 00:11:11.560
to do whatever synchronous
operation you need to do,

00:11:11.560 --> 00:11:15.690
you need to rethink your
design, because that's not

00:11:15.690 --> 00:11:18.610
something Node should be
doing, in my honest opinion.

00:11:18.610 --> 00:11:22.190
And so waiting 10
milliseconds for sending out

00:11:22.190 --> 00:11:24.792
the beginning of the content
with the end of the content

00:11:24.792 --> 00:11:25.750
should not be an issue.

00:11:31.260 --> 00:11:37.260
So a note about 0.12, which we
currently have a branch for.

00:11:37.260 --> 00:11:39.990
Right now, it's pretty
much in feature freeze,

00:11:39.990 --> 00:11:43.190
and we are working
on fixing bugs.

00:11:48.140 --> 00:11:52.090
Now that native V8
Promises are in,

00:11:52.090 --> 00:11:54.590
there are some bugs around that,
and other things that we're

00:11:54.590 --> 00:11:56.760
finishing up to
make sure that it's

00:11:56.760 --> 00:12:00.930
usable and fully
working and stable.

00:12:00.930 --> 00:12:04.710
But one feature that was added,
which I believe is important,

00:12:04.710 --> 00:12:11.490
is that in 0,10, every time
you call Socket.write, or just

00:12:11.490 --> 00:12:17.280
resp.write, in terms of the HTTP
module, that makes a syscall.

00:12:17.280 --> 00:12:24.830
But coming up with TCP is the
ability to call the cork API.

00:12:24.830 --> 00:12:29.820
And the cork API uses what's
called the write.VSS call.

00:12:29.820 --> 00:12:35.060
And what happens is when
you cork, it will then halt.

00:12:35.060 --> 00:12:38.600
You can call write as
many times as you want,

00:12:38.600 --> 00:12:43.040
but internally, it will
aggregate all those.

00:12:43.040 --> 00:12:45.530
And then, when you
either call uncork

00:12:45.530 --> 00:12:51.740
or you end-- like at the end of
an HTTP call, you call res.end,

00:12:51.740 --> 00:12:58.090
it will send all the data down,
and make one single syscall

00:12:58.090 --> 00:13:02.140
to the kernel, and say,
read all of these out as one

00:13:02.140 --> 00:13:03.740
giant buffer.

00:13:03.740 --> 00:13:06.250
The write syscall,
you have to write out

00:13:06.250 --> 00:13:09.030
each individual buffer.

00:13:09.030 --> 00:13:13.690
But with this new API, you
can treat many buffers as one.

00:13:13.690 --> 00:13:17.430
We've seen very high
throughput increases

00:13:17.430 --> 00:13:23.070
in writing out small requests,
because of this new ability.

00:13:23.070 --> 00:13:27.640
And it will be enabled by
default for the HTTP module

00:13:27.640 --> 00:13:28.770
in 0.12.

00:13:28.770 --> 00:13:36.390
I need to clarify that, and say
by enabled by default means,

00:13:36.390 --> 00:13:41.300
when you start
writing HTTP data,

00:13:41.300 --> 00:13:44.170
it automatically
corks at the beginning

00:13:44.170 --> 00:13:47.130
of the synchronous code.

00:13:47.130 --> 00:13:51.220
And then, it will allow you to
call res.write as many times

00:13:51.220 --> 00:13:52.560
as you want.

00:13:52.560 --> 00:13:55.340
And then at the end,
it will automatically

00:13:55.340 --> 00:13:59.020
uncork it for you, if you
haven't called res.end.

00:13:59.020 --> 00:14:02.710
Or if you have called
res.end, it will be sent down.

00:14:02.710 --> 00:14:08.770
So in 0.12, if you write
out something immediately,

00:14:08.770 --> 00:14:10.520
it won't be sent
out immediately.

00:14:10.520 --> 00:14:14.490
It will be sent out when
your synchronous code

00:14:14.490 --> 00:14:16.420
execution is complete.

00:14:16.420 --> 00:14:19.280
Again, hopefully, your
synchronous code execution

00:14:19.280 --> 00:14:23.220
isn't taking more
than 10 milliseconds.

00:14:23.220 --> 00:14:25.220
But yeah, that will
be enabled by default

00:14:25.220 --> 00:14:28.260
on 0.12 for the HTTP module.

00:14:28.260 --> 00:14:30.280
I believe it only
works on Linux,

00:14:30.280 --> 00:14:34.010
but since the
majority of servers

00:14:34.010 --> 00:14:37.330
we've seen run on
Linux, we figured

00:14:37.330 --> 00:14:38.710
that would be a big gain.

00:14:38.710 --> 00:14:41.810
Now, there's an idea of
a well-formatted request.

00:14:41.810 --> 00:14:46.790
And that is, when you
have an internal API,

00:14:46.790 --> 00:14:48.930
you have your own
JavaScript libraries

00:14:48.930 --> 00:14:54.130
that you are programming, and it
is making a call to the server.

00:14:54.130 --> 00:14:57.670
The format of the request
should be well-formed.

00:14:57.670 --> 00:15:01.200
It should have a
very specific format.

00:15:01.200 --> 00:15:05.850
Your REST APIs, requesting
content, all of these things

00:15:05.850 --> 00:15:10.080
should have a very specific
set of headers or a specific

00:15:10.080 --> 00:15:11.400
get request.

00:15:11.400 --> 00:15:16.765
And with these, you can
actually bypass the HTTP module

00:15:16.765 --> 00:15:17.265
entirely.

00:15:20.670 --> 00:15:23.470
We'll show some code
here in a second.

00:15:23.470 --> 00:15:28.270
But what you can actually
do is open up a TCP socket,

00:15:28.270 --> 00:15:33.870
and then allow HTTP connections
to be made to that TCP socket,

00:15:33.870 --> 00:15:40.010
manually extract the data you're
expecting from that API call,

00:15:40.010 --> 00:15:43.040
and make the response
as quick as possible.

00:15:43.040 --> 00:15:46.950
Say for example, you're
doing a restful API,

00:15:46.950 --> 00:15:51.030
and the only portion of the
message you need to know

00:15:51.030 --> 00:15:53.220
is the get request.

00:15:53.220 --> 00:15:55.670
But say for whatever
reason, there

00:15:55.670 --> 00:16:00.772
are a large number of
cookies or something.

00:16:00.772 --> 00:16:02.230
There are a large
number of headers

00:16:02.230 --> 00:16:03.900
that will require to be parsed.

00:16:03.900 --> 00:16:09.494
You can bypass that entire
segment by doing that manually.

00:16:09.494 --> 00:16:11.160
Now, this is on the
more advanced level.

00:16:11.160 --> 00:16:13.420
This is on the high
performance level,

00:16:13.420 --> 00:16:16.531
but that's generally what I do.

00:16:16.531 --> 00:16:18.780
To be honest, I get a lot
of people that come up to me

00:16:18.780 --> 00:16:22.340
and say, our application's
just fast enough.

00:16:22.340 --> 00:16:26.670
We'll talk more about that
later, but I don't-- to me,

00:16:26.670 --> 00:16:28.320
there is no fast enough.

00:16:28.320 --> 00:16:29.960
I measure performance
degradation

00:16:29.960 --> 00:16:31.910
in nanoseconds and clock cycles.

00:16:31.910 --> 00:16:35.670
I dump out the assembly code of
the JavaScript that I create.

00:16:35.670 --> 00:16:37.140
That's my level of performance.

00:16:37.140 --> 00:16:40.600
I know that isn't the
performance of 99.99% of Node

00:16:40.600 --> 00:16:42.712
developers out there,
but that's where I live,

00:16:42.712 --> 00:16:44.170
and so that's what
I like to share.

00:16:48.710 --> 00:16:52.060
So here's an example.

00:16:52.060 --> 00:16:57.320
So we're just creating a server,
and we're listening on a port.

00:16:57.320 --> 00:17:00.620
On connection, first
thing I'm doing here

00:17:00.620 --> 00:17:05.940
is setting Node delay to
disable the-- I believe

00:17:05.940 --> 00:17:07.849
you pronounce that
Nagle algorithm.

00:17:07.849 --> 00:17:12.410
Now as a quick reminder of what
the Nagle algorithm actually

00:17:12.410 --> 00:17:18.440
is, when you make a TCP
request, those packets

00:17:18.440 --> 00:17:23.140
are broken up into
sizes, your MTU,

00:17:23.140 --> 00:17:28.720
which is usually
around 1,500 bytes.

00:17:28.720 --> 00:17:31.920
Now, the Nagle algorithm
says, don't send out

00:17:31.920 --> 00:17:36.240
that small packet until
the MTU is either full,

00:17:36.240 --> 00:17:39.210
or 200 milliseconds
have passed by.

00:17:39.210 --> 00:17:41.610
And so if you're
writing out data,

00:17:41.610 --> 00:17:43.520
and the very end
of your response

00:17:43.520 --> 00:17:46.130
only takes half
the size of an MTU,

00:17:46.130 --> 00:17:48.850
and you haven't disabled
the Nagle algorithm,

00:17:48.850 --> 00:17:52.730
it will wait 200 milliseconds
before sending that out.

00:17:52.730 --> 00:17:56.550
Also, in this code, I'm
making the assumption

00:17:56.550 --> 00:18:03.980
that we're going
to set keepalive,

00:18:03.980 --> 00:18:05.420
that this is a web page.

00:18:05.420 --> 00:18:09.420
Now, if this was a
restful API, where

00:18:09.420 --> 00:18:12.740
people make one-shot
requests, then I

00:18:12.740 --> 00:18:14.630
wouldn't be setting keepalive.

00:18:14.630 --> 00:18:19.020
Instead, I would be expecting
for them to make a connection,

00:18:19.020 --> 00:18:23.060
make a request, and
then end the connection.

00:18:23.060 --> 00:18:26.100
So that really depends
on what you're doing.

00:18:26.100 --> 00:18:28.550
Now, here you can
see that I'm setting

00:18:28.550 --> 00:18:30.270
the three callbacks
for data error.

00:18:30.270 --> 00:18:33.830
And now, it does
actually work this way,

00:18:33.830 --> 00:18:35.442
where I can hoist
the functions out.

00:18:35.442 --> 00:18:38.270
This is a performance thing.

00:18:38.270 --> 00:18:39.600
I can hoist the functions out.

00:18:39.600 --> 00:18:43.600
Now on data, when you
get the chunk of data

00:18:43.600 --> 00:18:47.870
that will contain the raw
headers and body exactly as

00:18:47.870 --> 00:18:50.210
sent by the client,
and here's where

00:18:50.210 --> 00:18:54.030
you're going to do your
parsing of the raw data.

00:18:54.030 --> 00:18:58.660
Now remember, in Node that
the "this" of these functions

00:18:58.660 --> 00:19:00.555
is the actual connection
object instance.

00:19:04.520 --> 00:19:08.870
So as we'll see, you can just
call this.write, this.end

00:19:08.870 --> 00:19:12.000
within those hoisted functions,
and it'll work just fine.

00:19:12.000 --> 00:19:16.150
You don't need to embed them
in the on-connection callback,

00:19:16.150 --> 00:19:21.360
so that you can reference the C
variable there in on-connection

00:19:21.360 --> 00:19:24.080
Now on-air, those are
important to handle.

00:19:24.080 --> 00:19:28.130
There are a lot of things that
happen, like handling econ

00:19:28.130 --> 00:19:32.410
reset, where in the middle
of a connection response,

00:19:32.410 --> 00:19:34.970
the connection just hard
closes on the other end.

00:19:34.970 --> 00:19:36.050
That will throw an error.

00:19:36.050 --> 00:19:38.450
Those need to be handled.

00:19:38.450 --> 00:19:40.930
And then on end, make sure you
clean up all your resources

00:19:40.930 --> 00:19:42.672
and close down
your site, or else

00:19:42.672 --> 00:19:44.380
you're just going to
have an open socket,

00:19:44.380 --> 00:19:45.796
and you're wasting
your resources.

00:19:50.040 --> 00:19:53.570
Now, here's a small graph.

00:19:53.570 --> 00:19:56.490
This is a single
Node instance running

00:19:56.490 --> 00:20:01.850
on a 512-megabyte
digital ocean droplet.

00:20:01.850 --> 00:20:05.510
So it's pretty
low-powered, but I

00:20:05.510 --> 00:20:09.100
tried to simulate
as close as I could

00:20:09.100 --> 00:20:15.110
real-world throughput
on a restful API call.

00:20:15.110 --> 00:20:17.160
And as we can see,
at the beginning,

00:20:17.160 --> 00:20:18.760
they're pretty much the same.

00:20:18.760 --> 00:20:23.310
But the bottom there, you
have simultaneous requests.

00:20:23.310 --> 00:20:27.970
And as you can see, when
you get over 200, 250,

00:20:27.970 --> 00:20:31.390
you're pushing around 800
additional requests per second

00:20:31.390 --> 00:20:33.910
by manually parsing
those yourself.

00:20:33.910 --> 00:20:35.370
x

00:20:35.370 --> 00:20:37.940
Now, for high-traffic
websites, that

00:20:37.940 --> 00:20:41.950
can mean two, three,
four less servers.

00:20:41.950 --> 00:20:44.650
And those servers
can cost a lot.

00:20:44.650 --> 00:20:49.754
So a matter of savings and
making me happy, because I

00:20:49.754 --> 00:20:51.420
know that you have
made your application

00:20:51.420 --> 00:20:54.220
as fast as possible.

00:20:54.220 --> 00:21:00.730
So also, there's this idea of
reducing the number of requests

00:21:00.730 --> 00:21:04.820
by concatenating sets
of dynamic content.

00:21:04.820 --> 00:21:06.540
You can do that by
basically creating

00:21:06.540 --> 00:21:13.320
a hashmap of segmented
pieces of data.

00:21:13.320 --> 00:21:15.440
Like for example,
we saw earlier,

00:21:15.440 --> 00:21:21.360
there were 17, 20 JavaScript
files, and 10 CSS files.

00:21:21.360 --> 00:21:25.550
Those could technically be
concatenated, and be sent down

00:21:25.550 --> 00:21:28.390
to the client in one request.

00:21:28.390 --> 00:21:36.215
So here is a very basic example,
just get the point across,

00:21:36.215 --> 00:21:42.240
of using this idea
of concatenating

00:21:42.240 --> 00:21:45.600
sets of static content, as
requested by the client,

00:21:45.600 --> 00:21:46.910
on the fly.

00:21:46.910 --> 00:21:50.110
Basically, when your
application starts up,

00:21:50.110 --> 00:21:53.150
you read in all the files
from your resource directory.

00:21:53.150 --> 00:21:57.170
And say, for each
template you have,

00:21:57.170 --> 00:22:01.410
you have a specific CSS
file and/or JavaScript file.

00:22:01.410 --> 00:22:03.470
And so you can read those in.

00:22:03.470 --> 00:22:09.570
And remember, those will be
read in as a Node buffer,

00:22:09.570 --> 00:22:13.230
and buffers lives
outside of the V8 heap.

00:22:13.230 --> 00:22:16.250
So if you have a machine
that has plenty of memory,

00:22:16.250 --> 00:22:20.470
you can go as high as you want.

00:22:20.470 --> 00:22:21.870
It's really not an issue.

00:22:21.870 --> 00:22:25.940
Now, the on data, this is
from the TCP sock example.

00:22:25.940 --> 00:22:29.150
First thing we're going
to do is use the cork API

00:22:29.150 --> 00:22:31.270
that's going to be
coming out in 0.12.

00:22:31.270 --> 00:22:35.330
And then, say where that second
comment is, basically, we

00:22:35.330 --> 00:22:36.380
do some quick parsing.

00:22:36.380 --> 00:22:39.640
We make an API call, we figure
out what resources are needed,

00:22:39.640 --> 00:22:43.980
and then, we just loop
through and do a write

00:22:43.980 --> 00:22:47.060
on all the segments of content.

00:22:47.060 --> 00:22:48.710
And then, we do an
end at the very end,

00:22:48.710 --> 00:22:54.210
so they're all written out
as if they were one big file.

00:22:54.210 --> 00:22:56.050
And so each request
could technically

00:22:56.050 --> 00:23:03.950
have a unique set of
pieces of static data,

00:23:03.950 --> 00:23:08.390
but they're written out
as a single large file

00:23:08.390 --> 00:23:09.715
to the client.

00:23:09.715 --> 00:23:12.490
And if you hook up a
CDN in front of that,

00:23:12.490 --> 00:23:19.500
your performance will
drastically increase,

00:23:19.500 --> 00:23:24.250
instead of trying to either
push down the full 1.5-megabyte

00:23:24.250 --> 00:23:26.250
JavaScript file that
contains all the JavaScript

00:23:26.250 --> 00:23:34.564
for your entire site, or
returning 17 JavaScript files

00:23:34.564 --> 00:23:35.105
individually.

00:23:40.120 --> 00:23:42.770
Now, addressing the fact
there's no default compression,

00:23:42.770 --> 00:23:44.970
here's a very simple example.

00:23:44.970 --> 00:23:50.840
So we're going to say that
the object variable is a JSON

00:23:50.840 --> 00:23:52.510
object from a restful
API call that we

00:23:52.510 --> 00:23:54.210
made to some other service.

00:23:54.210 --> 00:23:58.470
We're going to go ahead
and gzip that sucker.

00:23:58.470 --> 00:24:01.330
We're going to set the
proper headings or headers,

00:24:01.330 --> 00:24:03.550
and then, we're going to
send that out over the pipe.

00:24:08.210 --> 00:24:10.020
That will allow us
to properly compress

00:24:10.020 --> 00:24:11.890
our content before
sending it down.

00:24:11.890 --> 00:24:18.040
In my test, I was showing
around 50% decrease in size.

00:24:18.040 --> 00:24:21.050
But unfortunately, it
does have quite a big hit.

00:24:21.050 --> 00:24:24.245
So I just ran this for 250
simultaneous connections

00:24:24.245 --> 00:24:26.256
at approximately one
kilobyte response.

00:24:26.256 --> 00:24:27.630
I made it very
small in response,

00:24:27.630 --> 00:24:32.760
because I was more interested
in the CPU hit overall.

00:24:32.760 --> 00:24:35.130
And so when I gzipped
it, I was only

00:24:35.130 --> 00:24:38.120
getting 980 requests
per second on average.

00:24:38.120 --> 00:24:40.870
And when I just did it
plain text streaming,

00:24:40.870 --> 00:24:42.250
I got about 1400.

00:24:42.250 --> 00:24:48.210
So to be honest, probably the
most performant way to do this

00:24:48.210 --> 00:24:52.990
is to use an external method
of gzipping your data.

00:24:52.990 --> 00:24:55.540
A lot of times, you'll have
a CDN or something in front,

00:24:55.540 --> 00:25:00.670
or any resource in
front, that can just

00:25:00.670 --> 00:25:02.430
gzip that data as
it's going out.

00:25:02.430 --> 00:25:03.510
Node is good.

00:25:03.510 --> 00:25:05.250
This is one area
where it alters,

00:25:05.250 --> 00:25:08.470
in terms of what other
libraries can do.

00:25:08.470 --> 00:25:12.140
And I would say, take
advantage of that.

00:25:12.140 --> 00:25:15.500
Get that data compressed
as it's going out.

00:25:15.500 --> 00:25:18.210
And final thoughts.

00:25:18.210 --> 00:25:22.520
I've addressed a
very few things that

00:25:22.520 --> 00:25:26.050
can be done to optimize
your applications.

00:25:26.050 --> 00:25:28.520
Node has many ways
that I haven't

00:25:28.520 --> 00:25:31.230
addressed that can be optimized.

00:25:31.230 --> 00:25:36.560
And I never want "it's fast
enough to get in your way."

00:25:36.560 --> 00:25:40.040
Yes, making sure it works
properly, that your tests pass,

00:25:40.040 --> 00:25:43.640
that it's stable,
those are important.

00:25:43.640 --> 00:25:48.630
But saying, "well,
it's fast enough,"

00:25:48.630 --> 00:25:53.230
is like saying, "well, I
don't want my site to grow."

00:25:53.230 --> 00:25:55.450
Because yes, I can
handle all the requests

00:25:55.450 --> 00:25:58.210
that I want right now.

00:25:58.210 --> 00:25:59.800
But then, say suddenly,
you're running

00:25:59.800 --> 00:26:03.920
a site that gets put on
a reddit or a slashdot.

00:26:03.920 --> 00:26:06.380
Plenty of websites
have been slashdotted,

00:26:06.380 --> 00:26:10.210
where it's up one second
and down 10 minutes later,

00:26:10.210 --> 00:26:14.810
because it gets a huge
flurry of requests.

00:26:14.810 --> 00:26:17.710
And then, my last thing would
be, while programming in Node,

00:26:17.710 --> 00:26:20.020
be creative.

00:26:20.020 --> 00:26:22.950
Many of my best optimizations,
especially in Node core

00:26:22.950 --> 00:26:25.494
have began with even
the Node core team

00:26:25.494 --> 00:26:26.660
saying, that's not possible.

00:26:26.660 --> 00:26:27.440
You're insane.

00:26:27.440 --> 00:26:28.960
There's no way that can work.

00:26:28.960 --> 00:26:31.930
And then, I go off in my own
little corner for a few days,

00:26:31.930 --> 00:26:35.240
and I fram it out, and
then show everybody and be,

00:26:35.240 --> 00:26:36.930
no, actually it will work.

00:26:36.930 --> 00:26:40.240
We can do this, and it's
reliable and stable.

00:26:40.240 --> 00:26:42.520
It just took a
little creativity.

00:26:42.520 --> 00:26:43.980
But be creative with Node.

00:26:43.980 --> 00:26:45.200
JavaScript is a fun language.

00:26:45.200 --> 00:26:49.110
You can do things in a
lot of different ways.

00:26:49.110 --> 00:26:54.970
And that will allow you
to figure out not only

00:26:54.970 --> 00:26:58.240
your most comfortable
way of doing things,

00:26:58.240 --> 00:27:01.690
but also figure out new ways of
doing things, things that you

00:27:01.690 --> 00:27:03.920
might not be comfortable
with, but yet are

00:27:03.920 --> 00:27:06.220
fun and exciting and different.

00:27:06.220 --> 00:27:09.040
So thank you very much.

00:27:09.040 --> 00:27:14.260
I hope that some of this
was helpful to some of you.

