WEBVTT
Kind: captions
Language: en

00:00:00.279 --> 00:00:02.785
So there's a couple more
important distributions.

00:00:02.785 --> 00:00:04.343
That's the main topic for today.

00:00:04.343 --> 00:00:07.521
Then we'll start Markov chains next time.

00:00:07.521 --> 00:00:12.123
All the remaining distributions are what
I would call off shoots of the normal.

00:00:12.123 --> 00:00:17.367
That is their importance is inherited from
the fact that the normal is important,

00:00:17.367 --> 00:00:21.091
they're all defined in terms
of normal distributions and

00:00:21.091 --> 00:00:24.437
their stories are tied to
the stories of the normal.

00:00:24.437 --> 00:00:25.365
Cuz they're just,

00:00:25.365 --> 00:00:28.375
basically we do different things
with normal random variables.

00:00:28.375 --> 00:00:31.580
And the normal is so important,
we saw the central limit theorem last

00:00:31.580 --> 00:00:34.122
time which is one reason that
the normal is important,

00:00:34.122 --> 00:00:36.955
but there are a lot of other
reasons the normal is important.

00:00:36.955 --> 00:00:38.651
And so you do things with the normal,

00:00:38.651 --> 00:00:41.677
then there's a good chance that
that's also important, okay?

00:00:41.677 --> 00:00:45.827
So the first one is called
the chi-squared distribution.

00:00:45.827 --> 00:00:49.570
That has one parameter, so it's chi,

00:00:49.570 --> 00:00:54.340
the Greek letter chi looks
like a fancy x squared.

00:00:54.340 --> 00:00:55.738
Chi-square of n.

00:00:55.738 --> 00:01:00.282
There's one parameter, n,
which is called the degrees of freedom, or

00:01:00.282 --> 00:01:02.450
in words it's just chi-square.

00:01:06.066 --> 00:01:09.293
It's an extremely famous
distribution in statistics.

00:01:09.293 --> 00:01:11.718
Some of you have probably
heard of chi-squared tests or

00:01:11.718 --> 00:01:14.203
maybe even used chi-squared tests,
things like that.

00:01:14.203 --> 00:01:17.531
So we're not gonna derive
chi-squared tests in this class.

00:01:17.531 --> 00:01:21.250
But you would see that in a lot
of other stat courses and

00:01:21.250 --> 00:01:23.490
other non-state courses too.

00:01:23.490 --> 00:01:28.700
The chi-square test has become one of
the most famous statistical methods

00:01:28.700 --> 00:01:34.085
in psychology and sociology and
all over the place, it's used everywhere.

00:01:34.085 --> 00:01:38.875
I mean, you can get a computer
to do a chi-square test for you.

00:01:38.875 --> 00:01:41.539
And in some sense they're
really over used for

00:01:41.539 --> 00:01:46.115
reasons that are not important to get into
in this class but to really understand

00:01:46.115 --> 00:01:50.298
what it's doing, you have to
understand chi-squared distribution.

00:01:50.298 --> 00:01:54.248
So we're talking about as
a distribution probability but

00:01:54.248 --> 00:01:57.320
it's used all over
the place in statistics.

00:01:57.320 --> 00:02:01.687
So the definition,rather than
defining it by writing down the PDF,

00:02:01.687 --> 00:02:05.222
which would tell you nothing
about where it comes from,

00:02:05.222 --> 00:02:10.439
I wanna define it in terms of how it's
related to the normal distribution, okay?

00:02:10.439 --> 00:02:13.745
So let's say, V equal,

00:02:13.745 --> 00:02:19.255
let's say Z1 squared plus Z2 squared,

00:02:19.255 --> 00:02:25.082
plus blah, blah, blah, plus Zn squared,

00:02:25.082 --> 00:02:30.600
where the Zj's are iid standard normal.

00:02:30.600 --> 00:02:35.784
That's the definition of how we
get a chi-squared distribution.

00:02:35.784 --> 00:02:40.410
So then, by definition,
just to write that down, by definition,

00:02:40.410 --> 00:02:45.212
this is chi-squared, and
we would just write V is chi-squared of n.

00:02:45.212 --> 00:02:49.528
So sometimes the n gets subscripted,
sometimes it's written like that,

00:02:49.528 --> 00:02:50.837
it's the same thing.

00:02:50.837 --> 00:02:57.035
So all it is is the sum of squares
of N iid standard normals, okay?

00:02:57.035 --> 00:03:01.504
And this comes up a lot in statistics
because a lot of statistical methods

00:03:01.504 --> 00:03:04.887
somehow involve adding up
squares of things, right?

00:03:04.887 --> 00:03:09.570
And if those things happens to be iid
standard normal then you're gonna get

00:03:09.570 --> 00:03:10.621
chi-squared.

00:03:10.621 --> 00:03:13.537
So it comes up everywhere.

00:03:13.537 --> 00:03:17.447
All right, well,
that's all as far as the definition.

00:03:17.447 --> 00:03:19.099
But I wanted to show you,

00:03:19.099 --> 00:03:23.045
how does it connect with other
distributions we've done.

00:03:23.045 --> 00:03:27.474
So I'll just call this
fact chi-squared of 1,

00:03:27.474 --> 00:03:34.077
chi-squared of 1 means we just take
one standard normal and square it.

00:03:34.077 --> 00:03:40.189
Chi squared of 1 is the same
thing as gamma of 1/2, 1/2.

00:03:45.543 --> 00:03:49.863
That's not something that you
can easily see in your head.

00:03:49.863 --> 00:03:54.375
But it's a calculation that you
could all do it at this point.

00:03:54.375 --> 00:03:56.487
It's good practice, so
I'll let you do this.

00:03:56.487 --> 00:04:00.477
There's just a change of variables thing.

00:04:00.477 --> 00:04:03.909
The reason that I'm not gonna through
the algebra of this is because you had

00:04:03.909 --> 00:04:06.469
a homework problem,
where we took a standard normal and

00:04:06.469 --> 00:04:09.103
raised to the fourth power and
you found the PDF, right?

00:04:09.103 --> 00:04:10.991
Now this is like an easier
version of that problem.

00:04:10.991 --> 00:04:12.292
It's the same idea.

00:04:12.292 --> 00:04:17.002
The only thing you have to be careful
about is because it's squared then you

00:04:17.002 --> 00:04:21.112
have to deal with the fact that
the function, y equals x squared,

00:04:21.112 --> 00:04:23.986
is decreasing and then increasing, right?

00:04:23.986 --> 00:04:26.155
So you can't just plug into
the change of variance formula.

00:04:26.155 --> 00:04:27.961
As long as you're careful
about that fact though,

00:04:27.961 --> 00:04:29.942
it's just an easier version
of that homework problem.

00:04:29.942 --> 00:04:32.353
You can just do the calculation,
get the PDF.

00:04:32.353 --> 00:04:35.992
Same as on the homework, so
if you understood that homework problem,

00:04:35.992 --> 00:04:37.419
then this is easy to check.

00:04:37.419 --> 00:04:38.823
And you just get the PDF.

00:04:38.823 --> 00:04:43.158
And you'll see,
that's the same as the Gamma 1/2, 1/2.

00:04:43.158 --> 00:04:47.329
So that's a useful fact about
the chi-squared because we

00:04:47.329 --> 00:04:50.915
already know a lot of nice
things about the Gamma.

00:04:50.915 --> 00:04:52.643
We know how it relates to the beta.

00:04:52.643 --> 00:04:55.309
We know how to add up Gammas.

00:04:55.309 --> 00:04:59.671
And we know how to get moments for
the Gamma, things like that.

00:04:59.671 --> 00:05:02.835
We can borrow all the stuff we know
about the Gamma distribution and

00:05:02.835 --> 00:05:05.960
just use that for free without
having to re-derive everything.

00:05:05.960 --> 00:05:10.319
So that's very helpful
in studying chi-square.

00:05:10.319 --> 00:05:13.160
So that's Chi-Square of 1.

00:05:13.160 --> 00:05:17.777
So assuming this fact which you can just
check, like on the homework problem.

00:05:17.777 --> 00:05:23.315
Now if you want Chi-Squared
of n that just says we're

00:05:23.315 --> 00:05:28.498
adding up n, iid Gamma 1/2, 1/2, right?

00:05:28.498 --> 00:05:33.279
And remember, we also talked about the
fact that if you have a gamma a lambda and

00:05:33.279 --> 00:05:36.828
gamma b lambda and
they're independent and you add them,

00:05:36.828 --> 00:05:39.160
you get gamma a plus b lambda, right?

00:05:39.160 --> 00:05:44.410
So therefore if you had and
n of them, and the lambda is 1/2,

00:05:44.410 --> 00:05:48.285
that's the same for
all of them, same scale.

00:05:48.285 --> 00:05:53.650
Just adding 1/2, so
it immediately follows from this that

00:05:53.650 --> 00:05:59.025
chi-square of n is the same
thing as Gamma n over 2, 1/2.

00:05:59.025 --> 00:06:02.161
So this is really actually
not a new distribution.

00:06:02.161 --> 00:06:07.088
It's a new name for something we
already knew, which is a gamma, okay?

00:06:07.088 --> 00:06:11.978
So in a sense I don't really need to list
it separately on the table of famous

00:06:11.978 --> 00:06:16.657
distributions, things like that,
it's a special case of the Gamma.

00:06:16.657 --> 00:06:21.378
But it's used so much in statistics
because of adding up sums of squares,

00:06:21.378 --> 00:06:25.655
it gets its own name, and
it gets its own entry in the table, okay?

00:06:25.655 --> 00:06:29.732
So that is the chi-squared distribution,

00:06:29.732 --> 00:06:35.578
that's our penultimate,
famous univariate distribution.

00:06:35.578 --> 00:06:38.447
That univariate,
as in it's not multivariate.

00:06:38.447 --> 00:06:42.074
We're gonna also do
the multivariate normal today.

00:06:42.074 --> 00:06:46.418
This is just foreshadowing that, we talked
about the multinomial distribution, right?

00:06:46.418 --> 00:06:49.584
Which was the generalization of
the binomial when you have more than two

00:06:49.584 --> 00:06:50.226
categories.

00:06:50.226 --> 00:06:54.379
So the multinomial is the most important
multivariate discreet distribution,

00:06:54.379 --> 00:06:54.995
all right?

00:06:54.995 --> 00:06:57.501
It's discreet,
it's a generalization of the binomial.

00:06:57.501 --> 00:07:01.620
Multivariate normal which we're gonna
do later today is the most important

00:07:01.620 --> 00:07:03.947
multivariate continuous distribution.

00:07:03.947 --> 00:07:08.052
So the question is what's
the natural way to extend normal

00:07:08.052 --> 00:07:12.829
distribution into higher dimensions
where you have a vector rather

00:07:12.829 --> 00:07:17.709
than you have just a random variable
you have a random vector, okay?

00:07:17.709 --> 00:07:20.387
And so we'll talk about that later.

00:07:20.387 --> 00:07:25.091
But first, we just need one
more univariate distribution

00:07:25.091 --> 00:07:28.745
which is the famous
student t distribution.

00:07:37.278 --> 00:07:42.237
A lot of students have wondered
why it's called student and

00:07:42.237 --> 00:07:48.501
the reason is that it was discovered and
first used by a guy called Gossett.

00:07:52.565 --> 00:07:57.217
Well, the fact that his name is Gosset
doesn't tell you about why it's called

00:07:57.217 --> 00:07:58.144
Student then.

00:07:58.144 --> 00:08:02.510
The reason,
this was around 1908 That he first

00:08:02.510 --> 00:08:07.170
introduced this in a very
influential paper.

00:08:07.170 --> 00:08:11.670
Gosset was a statistician who was working

00:08:11.670 --> 00:08:17.240
as a master brewer for
Guinness, that beer company.

00:08:17.240 --> 00:08:20.870
And he was an excellent,

00:08:20.870 --> 00:08:24.775
excellent statistician working for
Guinness a little over 100 years ago.

00:08:24.775 --> 00:08:27.306
And a lot of people think that, and so

00:08:27.306 --> 00:08:31.812
when we was publishing his papers
he used the pseudonym student.

00:08:31.812 --> 00:08:36.406
And a lot of people think that was because
he didn't want Guinness to know, but

00:08:36.406 --> 00:08:40.360
actually Guinness was very
supportive of him as far as I know.

00:08:40.360 --> 00:08:44.579
But Guinness did not want the other beer
companies to know they had a statistician

00:08:44.579 --> 00:08:45.580
working for them.

00:08:45.580 --> 00:08:47.785
That was like their secret weapon.

00:08:47.785 --> 00:08:50.290
So he published his papers
using the name student,

00:08:50.290 --> 00:08:52.330
was just a pseudonym he came up with.

00:08:53.580 --> 00:08:59.168
The letter t just because it became
standard to use the letter t for

00:08:59.168 --> 00:09:00.250
a certain statistics.

00:09:00.250 --> 00:09:02.600
Those of you that have
seen t statistics or

00:09:02.600 --> 00:09:08.240
t tests before it just got called t for
some reason and that caught on.

00:09:09.530 --> 00:09:13.710
Okay, so
what he introduced with the t test,

00:09:13.710 --> 00:09:16.400
which is based on the t distribution,
which we're about to do.

00:09:16.400 --> 00:09:21.377
100 years ago,
just over 100 years ago, but

00:09:21.377 --> 00:09:26.702
since then, since it caught on,
it's been extremely

00:09:26.702 --> 00:09:31.937
widely used in statistics and
it still is to this day.

00:09:31.937 --> 00:09:36.723
So I just wanna say a little, we're not
gonna go through a lot of the statistical

00:09:36.723 --> 00:09:39.722
side of how do you use t test and
one sample t test and

00:09:39.722 --> 00:09:42.420
two sample t tests and things like that.

00:09:42.420 --> 00:09:45.770
Some of you have see that but
I'm not assuming that you have.

00:09:45.770 --> 00:09:51.230
But the reason, the mathematical basis
behind those things is the t distribution.

00:09:51.230 --> 00:09:53.483
So we're gonna talk about
the t distribution, okay.

00:09:53.483 --> 00:09:59.610
So again, you could write down the PDF of
this thing, but it's just this complicated

00:09:59.610 --> 00:10:04.660
looking thing and it would give you
no idea of where does it come from.

00:10:04.660 --> 00:10:10.680
So again we're gonna define it by relating
it back to normal distribution, okay.

00:10:10.680 --> 00:10:14.480
So we're going to say that let Let T,

00:10:14.480 --> 00:10:20.630
call it T because it's
gonna be a T distribution.

00:10:20.630 --> 00:10:26.020
Suppose we had a random variable
T that's of the form Z over

00:10:26.020 --> 00:10:31.965
the square root of V over N,
where Z is against standard normal.

00:10:35.221 --> 00:10:40.218
And V is chi square of N,
and they're independent.

00:10:40.218 --> 00:10:43.620
So Z is independent of V, all right?

00:10:43.620 --> 00:10:47.180
So if we have a random variable
of this form, we say that,

00:10:47.180 --> 00:10:49.480
that follows a T distribution.

00:10:49.480 --> 00:10:55.370
And we'd write that as T,
distributed as t sub n.

00:10:58.300 --> 00:11:00.410
Lowercase t is the name
of the distribution,

00:11:00.410 --> 00:11:02.610
we don't necessarily
always write the student.

00:11:02.610 --> 00:11:07.320
We just say t distribution and
degrees of freedom.

00:11:07.320 --> 00:11:11.190
Degrees of freedom is just mysterious
at first sounding name for

00:11:11.190 --> 00:11:12.540
what the parameter is.

00:11:12.540 --> 00:11:14.070
It's kind of a deep question like,

00:11:14.070 --> 00:11:18.410
what do degrees of freedom really mean,
when people talk about degrees of freedom.

00:11:18.410 --> 00:11:22.060
But for now you can just think
about it as the parameter

00:11:22.060 --> 00:11:24.330
which goes back to the chi-square.

00:11:24.330 --> 00:11:28.740
It's just,
how many normals squared did we add up?

00:11:29.810 --> 00:11:32.470
We call that the degrees of freedom,
that's the parameter.

00:11:32.470 --> 00:11:34.740
On top it's just one standard normal.

00:11:34.740 --> 00:11:36.920
But here we have a chi square of n.

00:11:36.920 --> 00:11:38.910
So that's where the parameter comes in.

00:11:40.090 --> 00:11:42.700
All right, so that's the t distribution.

00:11:42.700 --> 00:11:44.821
Let's just mention a few properties.

00:11:47.643 --> 00:11:52.549
That we can see,
we could get the PDF of this thing by

00:11:52.549 --> 00:11:56.899
doing an ugly Jacobian
type of calculation.

00:11:56.899 --> 00:12:01.050
But there's no need for
that for our purposes here.

00:12:01.050 --> 00:12:03.350
I mean, I'm not saying you would
never need the PDF of this thing.

00:12:03.350 --> 00:12:07.060
But for the things that make it
interesting for our purposes

00:12:07.060 --> 00:12:11.840
we can just think of it in terms of this
representation, in terms of normals.

00:12:11.840 --> 00:12:16.940
Okay, so first of all, let's just stare at
this and see, what can we say about it.

00:12:18.940 --> 00:12:20.769
First of all,
it's a symmetric distribution.

00:12:26.254 --> 00:12:27.142
That is.

00:12:29.752 --> 00:12:33.420
If we multiply by minus 1 it
has the same distribution.

00:12:33.420 --> 00:12:35.270
So it's metric about zero.

00:12:35.270 --> 00:12:38.290
And you can see that just
my multiplying by minus 1.

00:12:38.290 --> 00:12:41.550
And we know that the Z is symmetric,
right?

00:12:41.550 --> 00:12:44.340
Normal 01 is symmetric without 0.

00:12:44.340 --> 00:12:48.360
We put a minus up here, that doesn't
change the distribution on top and

00:12:48.360 --> 00:12:52.070
the bottom is just some independent
thing that hasn't changed.

00:12:52.070 --> 00:12:59.413
So, it symmetric,
let's look at the case, n equals 1.

00:13:04.109 --> 00:13:08.051
If n = 1 then we just have the square
root of a squared normal, so

00:13:08.051 --> 00:13:10.560
that's the absolute value of a normal.

00:13:10.560 --> 00:13:15.270
So what we have in that case is a ratio
of independent standard normals

00:13:15.270 --> 00:13:17.210
with an absolute value sign on the bottom.

00:13:18.390 --> 00:13:22.210
But the absolute value sign on the bottom
does not affect the distribution.

00:13:22.210 --> 00:13:23.850
Again because of symmetry.

00:13:23.850 --> 00:13:28.100
So, if n = 1, that's really just the ratio
of two independent standard normals and

00:13:28.100 --> 00:13:30.010
that's the distribution
we saw before the Cauchy.

00:13:30.010 --> 00:13:33.320
So, this is a generalization
of the Cauchy.

00:13:33.320 --> 00:13:35.420
The t one is the same as Cauchy.

00:13:35.420 --> 00:13:37.470
Remember we did that
Cauchy interview problem,

00:13:37.470 --> 00:13:40.560
we derived of the PDF of the Cauchy
if you think about that.

00:13:40.560 --> 00:13:46.524
So, this generalizes that.

00:13:46.524 --> 00:13:49.870
And in particular, the mean doesn't exist.

00:13:49.870 --> 00:13:52.230
We were talking about
the evil Cauchy before.

00:13:52.230 --> 00:13:55.130
Cauchy does not have a finite expectation.

00:13:55.130 --> 00:13:59.930
So, the t distribution with one degree of
freedom does not have a finite meaning,

00:13:59.930 --> 00:14:01.110
the mean doesn't exist.

00:14:02.540 --> 00:14:05.146
But on the other hand
if n is at least two,

00:14:05.146 --> 00:14:11.370
then by symmetry,
I'll expect a value of equal zero.

00:14:13.800 --> 00:14:15.670
Symmetry is one way to see it, but

00:14:15.670 --> 00:14:20.080
let's also do just a quick calculation for
why the mean is zero.

00:14:20.080 --> 00:14:23.480
So you had another homework problem
pointing out that in general you can't say

00:14:23.480 --> 00:14:28.870
E of this ratio, in general that's
gonna be E of this over E of that.

00:14:28.870 --> 00:14:33.576
Right, but
it is true that we can write this is E

00:14:33.576 --> 00:14:38.060
(Z) times E of 1 over
square root V over n.

00:14:38.060 --> 00:14:42.560
That's valid because this poisso and
this poisso are independent.

00:14:42.560 --> 00:14:44.710
I assumed that they're independent, right?

00:14:44.710 --> 00:14:47.553
They're independent, therefore,
they aren't correlated, so we can do that.

00:14:50.694 --> 00:14:51.738
And we got zero.

00:14:54.260 --> 00:14:59.231
The reason this would break down in
the case of n = 1 is that this term

00:14:59.231 --> 00:15:00.640
wouldn't exist.

00:15:00.640 --> 00:15:04.715
And you can't say that something that
doesn't exist times 0 we can't say that,

00:15:04.715 --> 00:15:05.790
that's 0, right?

00:15:07.060 --> 00:15:09.810
So that's the problem, okay?

00:15:09.810 --> 00:15:15.320
But as long as this part exist
which it will if n leads to then 0.

00:15:15.320 --> 00:15:21.980
You can also just see that had to be
true by symmetry the mean is 0, okay?

00:15:21.980 --> 00:15:28.030
And so, it doesn't have an MGF it
doesn't have all of its moments.

00:15:28.030 --> 00:15:32.100
Like the T1 doesn't have a first moment.

00:15:32.100 --> 00:15:34.750
The T2 won't have a second moment.

00:15:34.750 --> 00:15:36.750
The T3 won't have a third moment.

00:15:36.750 --> 00:15:39.860
It goes like that, so there's a limited
number of moments you can take.

00:15:41.220 --> 00:15:46.831
And actually, if you want to get,
so I did this for, I did this for

00:15:46.831 --> 00:15:51.421
a first moment, but
if you wanna get a higher moment,

00:15:51.421 --> 00:15:56.850
if it exists, all you have to do
is put this T to a power, right?

00:15:56.850 --> 00:15:59.560
And then just put the power here,
power here.

00:15:59.560 --> 00:16:05.730
We get all that from things we know,
then, because we know

00:16:05.730 --> 00:16:11.210
all the moments of the normal, and then
this one here, you raise that to a power.

00:16:11.210 --> 00:16:15.630
It looks ugly the way I wrote it, but
if you just imagine raising this thing to

00:16:15.630 --> 00:16:22.380
a power that's just a power,
there's some constant you can take out,

00:16:22.380 --> 00:16:26.760
aside from that it's just a power
of a Gamma distribution, right?

00:16:26.760 --> 00:16:31.760
And powers of Gamma distributions,
it's easy to get the mean using LOTUS,

00:16:31.760 --> 00:16:36.710
so that means we can find
the moments that exist.

00:16:36.710 --> 00:16:38.550
It's not true that all
of its moments exist.

00:16:38.550 --> 00:16:42.490
But the ones that do,
we can find it in an easy way using this.

00:16:43.700 --> 00:16:47.570
The odd moments are gonna be
0 if they exist by symmetry.

00:16:47.570 --> 00:16:50.400
If you put Z cubed here,
then that's gonna be 0.

00:16:50.400 --> 00:16:52.210
But for the even ones,

00:16:52.210 --> 00:16:57.655
we talked before about the even
moments of the normal and then this.

00:16:57.655 --> 00:17:00.282
Actually, here's another way to get the,

00:17:00.282 --> 00:17:03.700
this is just a reminder of
the even moments of the normal.

00:17:03.700 --> 00:17:10.380
E of Z squared is 1, because that's
the variance if Z is standard normal.

00:17:10.380 --> 00:17:15.787
E of Z to the fourth is 3,
E of Z to sixth is 3 times 5,

00:17:15.787 --> 00:17:18.810
and then it goes up like that.

00:17:19.970 --> 00:17:21.950
Where your multiplying,
this is 1, 1 times 3,

00:17:21.950 --> 00:17:25.440
1 times 3 times 5 it's a skip factorial.

00:17:25.440 --> 00:17:31.239
We proved that before using MGF, right?

00:17:31.239 --> 00:17:34.380
But here's another way
to look at this problem.

00:17:34.380 --> 00:17:40.094
If we wanted, so we used MGFs before.

00:17:40.094 --> 00:17:44.604
Another way would be
to relate it to Gamma,

00:17:44.604 --> 00:17:48.150
by relating it to chi-squared.

00:17:48.150 --> 00:17:51.770
So if we have the expected
value of Z to the 2n,

00:17:52.920 --> 00:17:57.840
that is we want the 2n

00:17:57.840 --> 00:18:03.140
moment of a standard normal where
n is a positive integer, well,

00:18:03.140 --> 00:18:07.060
we could just think of that
as E of Z squared to the n.

00:18:07.060 --> 00:18:08.940
That doesn't take much algebra.

00:18:08.940 --> 00:18:13.974
But the point of writing it
this way is now we recognize,

00:18:13.974 --> 00:18:18.533
Z squared,
that's just chi-squared of 1, right?

00:18:18.533 --> 00:18:25.348
But chi-squared of 1 is the same
thing as Gamma of 1/2, 1/2.

00:18:29.096 --> 00:18:33.776
So all we really need is the nth
moment of a Gamma of 1/2, 1/2,

00:18:33.776 --> 00:18:36.640
then you can just use LOTUS.

00:18:36.640 --> 00:18:41.135
So we did a calculation before using LOTUS
to get the moments of the Gamma, and for

00:18:41.135 --> 00:18:44.924
practice, doesn't matter if you
remember that or memorize that.

00:18:44.924 --> 00:18:46.960
But you should be able to do the LOTUS and

00:18:46.960 --> 00:18:50.280
the Gamma function it's really
easy to get moments, right?

00:18:50.280 --> 00:18:53.787
The Gamma distribution it's easy to get
the moments because you just go X to

00:18:53.787 --> 00:18:57.420
a power times this Gamma thing, but
then it still looks like a Gamma, right?

00:18:57.420 --> 00:19:01.400
So as long as you have the pattern
recognition of the Gamma integral and

00:19:01.400 --> 00:19:03.724
you write down LOTUS you can get this.

00:19:03.724 --> 00:19:05.634
Okay, so if you do it this way,

00:19:05.634 --> 00:19:09.246
you're gonna get something in
terms of a Gamma function.

00:19:09.246 --> 00:19:14.012
But you can show using properties
of a Gamma function that is in

00:19:14.012 --> 00:19:18.548
fact equivalent to just
multiplying odd numbers, okay?

00:19:18.548 --> 00:19:23.430
So, all right, that's how we can get
the moments of the T distribution

00:19:23.430 --> 00:19:25.219
up to the point where they stop existing.

00:19:26.600 --> 00:19:29.030
Okay, and other properties.

00:19:31.470 --> 00:19:37.630
Well, one reason the T is famous is that
it looks like the normal distribution,

00:19:37.630 --> 00:19:42.080
approximately, but it's heavier tailed and

00:19:42.080 --> 00:19:48.400
the easiest way to see that is
just to see it by trying it out.

00:19:48.400 --> 00:19:51.700
Plot what the density looks like,
or generate some values.

00:19:51.700 --> 00:19:56.255
So heavier tails meaning that more
extreme values are relatively

00:19:56.255 --> 00:20:00.250
more likely than they would be for
the normal.

00:20:00.250 --> 00:20:01.904
And heavier tails than normal.

00:20:07.051 --> 00:20:10.525
You can measure that in terms
of kurtosis for example,

00:20:10.525 --> 00:20:15.370
which is on the homework, you've just
turned in, or just by looking at it.

00:20:16.890 --> 00:20:23.486
That's especially true if n is small,
like in the Cauchy has very heavy tales,

00:20:23.486 --> 00:20:28.716
the Cauchy density was a constant
times 1 over 1 + X squared.

00:20:28.716 --> 00:20:34.609
As you let X go to infinity, the Cauchy
is decaying like 1 over X squared, right?

00:20:34.609 --> 00:20:36.380
Try comparing that with the normal.

00:20:36.380 --> 00:20:39.052
The normal you have E to
the minus X squared over 2.

00:20:39.052 --> 00:20:43.035
And the normal is decaying much, much,
much, much faster than a Cauchy.

00:20:45.140 --> 00:20:51.160
But on the other hand to
relate it more to the normal

00:20:51.160 --> 00:20:58.500
if n is large like let's say n is
you know, 30 or 40 or 50 or larger.

00:20:58.500 --> 00:21:04.257
Then, it really just look like a normal so

00:21:04.257 --> 00:21:11.498
for n large tn looks very
much like standard normal.

00:21:11.498 --> 00:21:16.090
And when I say looks very much like,
I'm using that kind of intuitively.

00:21:16.090 --> 00:21:19.540
But if you want a mathematical statement
of it, let's just take the limit.

00:21:22.030 --> 00:21:28.367
So math statement would be that
the distribution, Either the CDF or

00:21:28.367 --> 00:21:33.653
the PDF, either way, will converge to
the distribution of standard normal,

00:21:33.653 --> 00:21:35.510
if you let N go to infinity.

00:21:44.733 --> 00:21:51.420
And to see that, well, you could do a big
calculation, but it's not necessary.

00:21:51.420 --> 00:21:56.290
But the best way to see that is
to use the Law of Large Numbers.

00:21:56.290 --> 00:21:59.850
So let's actually check
that fact over here.

00:22:04.172 --> 00:22:07.060
Okay, so now let's imagine.

00:22:07.060 --> 00:22:10.770
To make it clear, I didn't
indicate the dependence on N here.

00:22:10.770 --> 00:22:15.824
To make that clear because we're taking a
limit, let's consider a sequence of them.

00:22:15.824 --> 00:22:21.762
So let's let Tn = z/square root of Vn / n,

00:22:21.762 --> 00:22:26.576
where let's say we have an infinite

00:22:26.576 --> 00:22:31.880
sequence of i.i.d standard normal.

00:22:34.610 --> 00:22:40.971
Okay, so we have all these i.i.d standard
normal and what do we do with them?

00:22:40.971 --> 00:22:46.839
Well, that's supposed to
be a chi-squared of n,

00:22:46.839 --> 00:22:52.310
so Vn is the sum of squaroots of normals,
okay?

00:22:52.310 --> 00:22:56.010
So I'm just generating a T distribution
with n degrees of freedom.

00:22:57.180 --> 00:23:00.550
May as well just use the same Z for
all of them, right?

00:23:02.280 --> 00:23:08.260
Those are independent of, so
Z is also standard normal.

00:23:08.260 --> 00:23:10.300
And it's independent of the Zj's.

00:23:13.773 --> 00:23:17.530
So this is a way to construct
a t-distribution with n degrees of freedom

00:23:17.530 --> 00:23:19.420
just by the definition over there.

00:23:19.420 --> 00:23:23.270
I took a normal,
divide by this kind of squared thing.

00:23:23.270 --> 00:23:25.010
Okay, but I chose to define it,

00:23:25.010 --> 00:23:28.626
because I'm just looking at what
happens with the distribution.

00:23:28.626 --> 00:23:32.710
I'm saying, what happens to the
distribution of this as goes to infinity?

00:23:32.710 --> 00:23:35.300
We can choose whatever T
random variable we want,

00:23:35.300 --> 00:23:38.010
as long as it has the correct
distribution, right?

00:23:39.730 --> 00:23:42.000
Okay, so let's choose it this way.

00:23:42.000 --> 00:23:44.230
Using the same Z each time.

00:23:44.230 --> 00:23:45.440
That makes it easier, right?

00:23:45.440 --> 00:23:46.580
Same Z.

00:23:46.580 --> 00:23:47.872
Then what happens?

00:23:53.567 --> 00:23:56.064
Let's look at this thing Vn/n,

00:23:56.064 --> 00:24:00.530
what do you think happens to that
as you let n go to infinity?

00:24:03.469 --> 00:24:07.271
It goes to a constant, which constant?

00:24:10.615 --> 00:24:15.838
Converges, well,
if I took a sample mean of these,

00:24:15.838 --> 00:24:20.821
and I look at the, yeah, so
you can think of this as

00:24:20.821 --> 00:24:25.540
the sample mean of these squared normals.

00:24:25.540 --> 00:24:27.700
So its distribution.

00:24:27.700 --> 00:24:30.250
If we standardize this,

00:24:30.250 --> 00:24:34.430
then distribution will go to normal zero,
one after we standardize it.

00:24:34.430 --> 00:24:36.380
But I just want something
simpler than that.

00:24:36.380 --> 00:24:39.230
What does it converge to point wise?

00:24:40.900 --> 00:24:41.570
It converges to one.

00:24:41.570 --> 00:24:42.750
Why'd you say one?

00:24:45.372 --> 00:24:46.250
From this?

00:24:47.410 --> 00:24:48.235
Using what theorem?

00:24:48.235 --> 00:24:53.050
&gt;&gt; [INAUDIBLE]
&gt;&gt; Law of large numbers.

00:24:53.050 --> 00:24:55.780
CLT will let us get
the distribution of this.

00:24:55.780 --> 00:24:59.430
But I just wanna say what
happens to this point wise?

00:24:59.430 --> 00:25:05.059
That is if we evaluate this at
each point in the sample space.

00:25:06.710 --> 00:25:09.385
So this goes to one with probability one.

00:25:11.267 --> 00:25:16.642
By the law of large numbers,
because we're taking

00:25:16.642 --> 00:25:22.780
these NIID squared normals and
we averaged them, right?

00:25:22.780 --> 00:25:26.500
And the law of large numbers says that
the average of the sample converges

00:25:26.500 --> 00:25:28.860
to the true theoretical average.

00:25:28.860 --> 00:25:33.240
The true theoretical average is just
the expected value of one of these, right?

00:25:33.240 --> 00:25:34.230
They're IID.

00:25:34.230 --> 00:25:40.460
Numbers says this will converge to
the theoretical value e of z one squared.

00:25:40.460 --> 00:25:41.540
Which is just one.

00:25:43.050 --> 00:25:45.123
That's the variance of z one.

00:25:47.192 --> 00:25:51.170
So this thing goes to 1,
the probability 1.

00:25:51.170 --> 00:25:57.030
There is a square root there but because
this is just a point wise statement,

00:25:57.030 --> 00:26:02.175
this converges to 1 if take square
roots it still converges to 1.

00:26:02.175 --> 00:26:06.207
The square root of 1 is 1,
with probability 1.

00:26:09.455 --> 00:26:10.282
Okay?

00:26:10.282 --> 00:26:14.476
So now we're letting n go to infinity, but

00:26:14.476 --> 00:26:19.954
the denominator is just going to 1,
so this sequence,

00:26:19.954 --> 00:26:24.865
Tn, is just converging
to z with probability 1.

00:26:27.793 --> 00:26:31.289
That's telling us that these random
variables converge because I constructed

00:26:31.289 --> 00:26:31.870
it this way.

00:26:31.870 --> 00:26:34.850
I could have constructed the t
distribution some other way.

00:26:34.850 --> 00:26:38.710
I didn't have to use the same Z and
the same Z1 squared, and

00:26:38.710 --> 00:26:40.600
keep recycling things this way.

00:26:40.600 --> 00:26:43.430
We could have chosen some
other t distributions, but

00:26:43.430 --> 00:26:47.410
the distribution, so this point wise
statement might not be true anymore, but

00:26:47.410 --> 00:26:52.040
the distributions are still
behaving this way.

00:26:52.040 --> 00:26:56.780
So what that says is that the t
distribution with n degrees of freedom

00:26:56.780 --> 00:26:58.590
converges.

00:26:58.590 --> 00:27:01.410
Now I'm talking about the distribution,
not the random variable.

00:27:01.410 --> 00:27:03.903
Converges to a standard
normal distribution.

00:27:08.296 --> 00:27:12.699
So if N is very large, then you're just
thinking of this denominator as being

00:27:12.699 --> 00:27:15.220
essentially 1 by the law of large numbers.

00:27:15.220 --> 00:27:19.540
And so then what matters
is the normal on top, okay?

00:27:19.540 --> 00:27:21.390
So if you have a large number
of degrees of freedom,

00:27:21.390 --> 00:27:23.780
it just looks like
the normal distribution.

00:27:23.780 --> 00:27:27.530
Okay, but with the small number
degrees of freedom then, you know,

00:27:27.530 --> 00:27:32.610
it looks sort of like the normal but
it'll have a much heavier details, okay.

00:27:33.750 --> 00:27:37.760
All right, so
that's really all you need to know as for

00:27:37.760 --> 00:27:43.120
univariate famous named distributions,
okay?

00:27:43.120 --> 00:27:45.410
So last topic for today, and

00:27:45.410 --> 00:27:48.940
our last famous distribution
is the multi-variant normal.

00:27:48.940 --> 00:27:54.680
So what we want to do is extend some of
the nice stuff for the uni-variant normal.

00:27:54.680 --> 00:27:58.680
Right, normal has a lot of nice
properties, very, very useful.

00:27:58.680 --> 00:28:00.910
But what if we have a random vector,
not a random variable?

00:28:02.060 --> 00:28:06.370
Okay, so, there's one case where it

00:28:06.370 --> 00:28:11.280
would be really easy to construct
a multivariate distribution,

00:28:12.450 --> 00:28:16.240
which is if we have independent
random variables, right?

00:28:16.240 --> 00:28:19.360
If we have independent,
let's say we have IID random variables

00:28:19.360 --> 00:28:23.100
they stick them together into a vector and
they have a joint distribution.

00:28:23.100 --> 00:28:27.170
But that's the simple case where
the joint, and if it's continuous,

00:28:27.170 --> 00:28:32.540
the joint PDF is the product of
the marginal PDFs and you know.

00:28:32.540 --> 00:28:36.540
The more interesting cases as far as
a multivariate perspective is when

00:28:36.540 --> 00:28:38.700
there's some correlation, okay?

00:28:38.700 --> 00:28:43.600
And it turns out that for
the normal there's a very,

00:28:43.600 --> 00:28:47.680
very nice standard way to
extend it to higher dimensions.

00:28:47.680 --> 00:28:50.910
And there's different
equivalent ways to define this.

00:28:50.910 --> 00:28:52.861
So we want the multivariate normal.

00:28:54.658 --> 00:28:57.278
Which I sometimes abbreviate to MVN.

00:29:00.229 --> 00:29:02.292
Well, we have to start by defining it.

00:29:05.079 --> 00:29:07.123
And there is a different
ways to define it.

00:29:07.123 --> 00:29:14.521
But I think the nicest way for our purpose
is defining terms of linear combinations.

00:29:14.521 --> 00:29:17.720
That is every distribution
we're talking about today.

00:29:17.720 --> 00:29:21.263
The way we're thinking of it is
by reducing it back to stuff we

00:29:21.263 --> 00:29:24.468
already know about the normal
distribution, right?

00:29:24.468 --> 00:29:26.700
Chi-squared we defined
in terms of the normal.

00:29:26.700 --> 00:29:31.440
T was defined in terms of normal and
chi-squared, but

00:29:31.440 --> 00:29:32.930
in turn,
that's defined in terms of normal.

00:29:32.930 --> 00:29:36.540
And we're going to find multi-variant
normal in terms of just normal, right.

00:29:36.540 --> 00:29:40.260
Everything is based on the normal
distribution today, okay?

00:29:40.260 --> 00:29:45.258
So if we have a vector,
a random vector just means that we have

00:29:45.258 --> 00:29:50.761
a bunch of random variables that
we strung together into a vector.

00:29:50.761 --> 00:29:53.180
So let's say we have a random vector.

00:29:53.180 --> 00:29:56.134
Let's say x1, x2, xk,

00:29:56.134 --> 00:30:01.671
which maybe we'll just call that vector x,
okay?

00:30:01.671 --> 00:30:06.715
And the definition is
that this is multivariate

00:30:06.715 --> 00:30:10.255
normal if the following is true.

00:30:14.505 --> 00:30:18.520
If I want to reduce this back
down to one dimension, alright?

00:30:18.520 --> 00:30:20.930
Reduce it back down to
the case we're familiar with,

00:30:20.930 --> 00:30:23.360
which is the one dimensional case.

00:30:23.360 --> 00:30:27.750
A natural way to do that is just to
take a combination of the components.

00:30:27.750 --> 00:30:30.004
So if every linear combination.

00:30:35.229 --> 00:30:39.271
A linear combination,
if you haven't had linear algebra,

00:30:39.271 --> 00:30:44.870
linear combination just means add up these
variables with any constants in front.

00:30:44.870 --> 00:30:49.611
So a linear combination just looks like,
let's say t1 x1+ t2 x2,

00:30:49.611 --> 00:30:54.683
that's just called a linear combination
if you haven't seen that before.

00:30:56.884 --> 00:30:59.655
So I took a combination
of the random variables,

00:30:59.655 --> 00:31:04.070
t1 through tk are just arbitrary
constants, any constants at all, okay?

00:31:04.070 --> 00:31:06.547
If this is always normal.

00:31:09.514 --> 00:31:11.727
Every linear combination.

00:31:11.727 --> 00:31:15.270
So if you can choose even one choice for
the T's for

00:31:15.270 --> 00:31:19.850
which this fails to be normal,
then it would not be multi variant normal.

00:31:19.850 --> 00:31:22.190
But if no matter how you
choose these constants T,

00:31:22.190 --> 00:31:26.340
this always has a normal distribution
then we say this is multi variant normal.

00:31:26.340 --> 00:31:29.550
Okay, so see how this reduced
it back down to one dimension?

00:31:29.550 --> 00:31:31.550
Now this is just a random variable.

00:31:31.550 --> 00:31:35.580
Okay, so
we've reduced it back to familiar things.

00:31:35.580 --> 00:31:41.020
So for a couple quick examples, let's do a
quick example that is multivariate normal.

00:31:41.020 --> 00:31:45.123
And a quick example that
is not multivariate normal.

00:31:45.123 --> 00:31:49.338
So suppose that we started with,

00:31:49.338 --> 00:31:52.805
let's say, Z and W be IID.

00:31:54.564 --> 00:31:59.243
Standard normal,
then of course it's gonna be true that Z,

00:31:59.243 --> 00:32:04.610
W is multi-variant normal because
they're just independent.

00:32:04.610 --> 00:32:10.220
But it's more interesting if there's
some correlation between them.

00:32:10.220 --> 00:32:16.240
So for example, what if we did, Z plus 2W.

00:32:16.240 --> 00:32:17.920
I'm just making up some numbers here.

00:32:17.920 --> 00:32:19.590
You can put whatever constants you want.

00:32:19.590 --> 00:32:21.780
I'm just wanna doing a concrete example.

00:32:21.780 --> 00:32:24.988
3Z, let's say we did 3 Z + 5W.

00:32:24.988 --> 00:32:30.943
I just made up some constants here,
is multi variant normal.

00:32:30.943 --> 00:32:34.343
So this is a vector now,
I took these IID standard normals and

00:32:34.343 --> 00:32:37.700
I just added them with some
constants I just made up.

00:32:37.700 --> 00:32:41.432
Of course I can put ABCD
here that will be true,

00:32:41.432 --> 00:32:45.181
soon I wanted a little more concrete.

00:32:45.181 --> 00:32:47.510
Okay so, and why is that true?

00:32:48.520 --> 00:32:52.680
Well, to verify that this is true all
we need to do is to say if we take

00:32:55.030 --> 00:32:58.276
any constant times this plus any
constant times this it is normal.

00:32:58.276 --> 00:33:02.280
So if I take,
let's call the constants S and T.

00:33:02.280 --> 00:33:07.210
S times Z+2W, + t(3Z + 5W)

00:33:07.210 --> 00:33:12.140
I just took two arbitrary constant

00:33:12.140 --> 00:33:17.240
in front s and t or any constant, and

00:33:17.240 --> 00:33:21.830
rewrite this in terms of Z and W.

00:33:21.830 --> 00:33:28.552
That is something times
Z + something times W,

00:33:28.552 --> 00:33:35.784
well that's just (s + 3t) Z + (2s + 5t)W.

00:33:38.283 --> 00:33:40.080
And that's normal.

00:33:40.080 --> 00:33:43.122
Because we already knew that if we add up,
right,

00:33:43.122 --> 00:33:46.880
we already knew using MGFs and
previous stuff we did,

00:33:46.880 --> 00:33:51.060
we already knew that if we add up
independent normals it's normal, right?

00:33:51.060 --> 00:33:53.690
So this is one independent normal plus,

00:33:53.690 --> 00:33:55.940
this is the sum of two
independent normals.

00:33:55.940 --> 00:33:57.230
So we know that, that's normal.

00:33:59.170 --> 00:34:03.460
So that's just an example, but that's kind
of an important general class of examples.

00:34:03.460 --> 00:34:05.380
That we start with IID normals,

00:34:05.380 --> 00:34:09.750
and then just put together a vector
of different linear combinations.

00:34:09.750 --> 00:34:12.834
That will always be multivariate normal.

00:34:12.834 --> 00:34:16.743
All right, so that's an example,
let's do a non-example.

00:34:18.520 --> 00:34:20.330
I don't know if non-example
is the right word.

00:34:20.330 --> 00:34:25.142
I wanna go as far away from
that example as possible.

00:34:25.142 --> 00:34:28.959
I'm not gonna say counter example, because
it's not a counter example to anything,

00:34:28.959 --> 00:34:31.720
it's just an example that's
not a multivariate norm.

00:34:31.720 --> 00:34:35.470
Well sort of a counter example,
but I'll call it a non example.

00:34:35.470 --> 00:34:43.580
Non example would be let
Z be standard normal and

00:34:43.580 --> 00:34:49.460
let's let S be a random sign.

00:34:49.460 --> 00:34:54.822
That is S is plus or minus 1 with equal
probabilities, and it's independent of Z.

00:34:58.470 --> 00:35:03.821
Okay, so, Then Z and

00:35:03.821 --> 00:35:08.072
SZ, S times Z is related very
closely to one of the strategic

00:35:08.072 --> 00:35:13.090
practice problems from before,
where you can check for yourself.

00:35:13.090 --> 00:35:16.560
Multiple by a random sign just
through the symmetry of the normal,

00:35:16.560 --> 00:35:18.310
that's still standard normal.

00:35:18.310 --> 00:35:22.970
So marginally,
these are our standard normal.

00:35:22.970 --> 00:35:25.680
Marginally meaning you
look at Z on it's own,

00:35:25.680 --> 00:35:28.720
look at S times Z on it's own,
they are just standard normal.

00:35:31.260 --> 00:35:36.283
But the pairs Z, SZ is not

00:35:36.283 --> 00:35:41.073
multi variant normal.

00:35:43.465 --> 00:35:48.650
So put them together, on their own their
normal, put them together it's not.

00:35:52.560 --> 00:35:58.043
The reason is that we could

00:35:58.043 --> 00:36:04.279
just look at, look at Z + SZ.

00:36:04.279 --> 00:36:07.009
If you want to test whether this
is multi-variant and normal,

00:36:07.009 --> 00:36:08.349
according to the definition,

00:36:08.349 --> 00:36:11.814
you need to check what if I take something
times this plus something times this.

00:36:11.814 --> 00:36:15.837
Well, the easiest case of that
would be to just simply add them.

00:36:15.837 --> 00:36:20.280
Z + SZ, there's no possible
way that that could be normal.

00:36:20.280 --> 00:36:24.870
Because if you look at what happens,
half the time, S is -1 and you get 0.

00:36:24.870 --> 00:36:28.610
The other half the time,
you get some continuous thing.

00:36:28.610 --> 00:36:33.227
So this is actually a mixture
of discrete and continuous.

00:36:33.227 --> 00:36:36.178
You're never gonna find a normal
distribution that equals

00:36:36.178 --> 00:36:38.840
zero with probability equal to one half,
all right?

00:36:38.840 --> 00:36:41.280
That is not a property of
the normal distribution.

00:36:41.280 --> 00:36:43.530
So that is not multivariate normal.

00:36:43.530 --> 00:36:46.120
And you can see that this definition,
if in

00:36:46.120 --> 00:36:49.430
our definition if we had decided to just
say well multivariate normal just means

00:36:49.430 --> 00:36:53.190
that you string together some normals
then we'd be including this example.

00:36:53.190 --> 00:36:55.460
But this is a pretty nasty example right,

00:36:55.460 --> 00:36:59.480
so we're avoiding things like
that by defining it this way.

00:36:59.480 --> 00:37:01.085
So that's much better.

00:37:01.085 --> 00:37:06.270
Okay, so while we haven't done the PDF or
the MGF,

00:37:06.270 --> 00:37:11.380
the MGF of the multivariate normal,
we don't really need for this course.

00:37:11.380 --> 00:37:14.240
I mean you can do some Jacobean and
work it out.

00:37:15.830 --> 00:37:19.010
MGFs are much more useful for
our purpose, much simpler.

00:37:19.010 --> 00:37:23.690
So let's derive the MGF,
this is a joint, the MGF.

00:37:26.000 --> 00:37:30.137
So this is the MGF of
the joint distribution Of X.

00:37:31.773 --> 00:37:38.288
So let X be multivariate normal, Okay,

00:37:38.288 --> 00:37:44.351
now since this is multivariate,
when you define the MGF,

00:37:44.351 --> 00:37:49.640
you do it like with
a constant times each term.

00:37:49.640 --> 00:37:53.120
That is, in the univariate case,
we'd just do e to the tx.

00:37:53.120 --> 00:37:56.890
In the multivariate case,
we have a linear combination.

00:37:58.670 --> 00:38:03.680
Either the t, I'm gonna write it this way,
t prime x where

00:38:03.680 --> 00:38:09.845
this just means take the vector of t and
do a dot product with the vector of x.

00:38:09.845 --> 00:38:12.790
Another words to write
that out more long hand.

00:38:12.790 --> 00:38:15.500
This is just the definition of MGF,
is that we need a constant for

00:38:15.500 --> 00:38:17.140
each component.

00:38:17.140 --> 00:38:21.826
So it's e to the t1 x1 + blah,

00:38:21.826 --> 00:38:26.521
blah, blah, + tk xk, right.

00:38:29.161 --> 00:38:35.750
Right, okay so, that looks like
it might be a complicated thing.

00:38:35.750 --> 00:38:37.490
We're assuming we have
a multivariate normal.

00:38:37.490 --> 00:38:39.520
And I'll say what is the MGF look like?

00:38:40.920 --> 00:38:44.030
Well, it would be this, but
what can we say about this?

00:38:44.030 --> 00:38:46.590
Well, it looks like this may be
very concrete until we think

00:38:46.590 --> 00:38:49.000
back again to the definition we just did.

00:38:49.000 --> 00:38:55.200
The definition said that, if we
are requiring to see multivariate normal,

00:38:55.200 --> 00:38:59.270
then the thing up in
the exponent is normal, right?

00:38:59.270 --> 00:39:05.350
So all we really have is the expected of
E to a certain normal random variable.

00:39:05.350 --> 00:39:11.800
But that we can get from the MGF of the
normal in the one-dimensional case, right?

00:39:11.800 --> 00:39:18.065
So we've reduced it back
down to one dimension, And

00:39:18.065 --> 00:39:25.879
in particular, If you look up
the MGF of the standard normal,

00:39:25.879 --> 00:39:32.250
let me just write it here for
reference's sake.

00:39:32.250 --> 00:39:36.590
If, let's say, x is N mu sigma squared,

00:39:38.220 --> 00:39:41.560
I'll just remind you of the MGF,
which we've done before.

00:39:41.560 --> 00:39:46.747
The MGF of x,
the way I remember it is that it's

00:39:46.747 --> 00:39:53.264
e to the power of the mean of
what's up here, which is t mu.

00:39:53.264 --> 00:39:56.680
I just took the mean of this,
plus 1/2 the variance, so

00:39:56.680 --> 00:39:59.530
that's just what the MGF works out to.

00:39:59.530 --> 00:40:02.970
So the variance of this is
t squared sigma squared.

00:40:02.970 --> 00:40:07.300
So that's the MGF of a standard normal,
would be the case if mu equals 0,

00:40:07.300 --> 00:40:08.140
sigma equals 1.

00:40:08.140 --> 00:40:10.740
That's the MGF of any univariate normal.

00:40:11.800 --> 00:40:16.850
But if we know that fact, which we
did earlier, then we just know right

00:40:16.850 --> 00:40:22.320
away what this is, by definition,
that's just e to a normal,

00:40:22.320 --> 00:40:26.910
that's a special case of the MGA,
in particular if let, if you just,

00:40:26.910 --> 00:40:31.380
instead you have an e to a normal,
you can just let T equal one, if you want.

00:40:31.380 --> 00:40:33.660
Right?
You can let T be anything.

00:40:33.660 --> 00:40:38.698
Okay, so to sum that up, we already know

00:40:38.698 --> 00:40:43.593
without doing anymore calculations,

00:40:43.593 --> 00:40:49.523
we already know this is
gonna be e to the expected

00:40:49.523 --> 00:40:54.564
value of what's up here which is lets say

00:40:54.564 --> 00:41:00.346
t 1 miu1 plus blah blah
blah plus tk miu k where

00:41:00.346 --> 00:41:06.190
miu j is Let's just let
E of EX j equal mu j.

00:41:06.190 --> 00:41:07.940
That's the mean.

00:41:07.940 --> 00:41:11.780
So mu J is the mean of the J [INAUDIBLE]
component just viewed on it's own.

00:41:11.780 --> 00:41:13.100
All right?

00:41:13.100 --> 00:41:18.385
And then plus half the variants, so
that's E to this whole big thing,

00:41:18.385 --> 00:41:21.175
one half the variants of this thing.

00:41:24.774 --> 00:41:26.477
I'm just gonna write it as
variance of that thing.

00:41:34.869 --> 00:41:37.140
So that's all up in the exponent.

00:41:37.140 --> 00:41:41.230
Let me just write it as exp,
that means e to the power of this thing.

00:41:42.400 --> 00:41:48.140
Okay, just the mean of what's up here,
plus 1/2 the variance and then to get

00:41:48.140 --> 00:41:53.230
this variance thing, if they're dependent,
then we can add up these variances.

00:41:53.230 --> 00:41:57.330
But it might be that there's some
covariances between them and

00:41:57.330 --> 00:42:01.360
then we'll just expand this out in
a way we usually get variances.

00:42:02.710 --> 00:42:06.470
So that's the MGF and from multivariate
distributions its true just like for

00:42:06.470 --> 00:42:11.380
univariate that the MGF
determines the distribution.

00:42:13.390 --> 00:42:18.090
Okay, more so
one more property of multivariate normal,

00:42:20.500 --> 00:42:24.790
last fact about it is, very useful fact

00:42:26.600 --> 00:42:29.470
about independence and correlation.

00:42:30.850 --> 00:42:38.630
So, that's the fact that,
Within, within an MVN.

00:42:38.630 --> 00:42:42.650
That is, if you have this multivariate
normal vector, and you kind of look inside

00:42:42.650 --> 00:42:48.790
that vector, then it's true that
uncorrelated implies independent.

00:42:49.830 --> 00:42:52.300
We saw before that, in general.

00:42:52.300 --> 00:42:54.110
Independent implies uncorrelated, but

00:42:54.110 --> 00:42:56.420
uncorrelated it does
not imply independent.

00:42:56.420 --> 00:43:00.860
But a very important special
case where the converse is true,

00:43:00.860 --> 00:43:07.871
is when you're working within a
multi-variant normal, implies independent

00:43:11.040 --> 00:43:15.440
in other words just to write this out,
if we let X equal, so

00:43:15.440 --> 00:43:20.270
that's our vector, but let's say we split
it up into like X 1 and X 2 MVN, so

00:43:20.270 --> 00:43:24.160
we have multi variant normal,

00:43:24.160 --> 00:43:28.360
and we split it up into two vectors,
then the statement is that

00:43:31.527 --> 00:43:37.617
If every component of x1 is uncorrelated

00:43:37.617 --> 00:43:41.970
with every component of x2,

00:43:45.606 --> 00:43:47.780
Then they're independent.

00:43:50.010 --> 00:43:54.420
So take any component of X1 and
look at the covariant between that and

00:43:54.420 --> 00:43:56.770
any components of X2.

00:43:56.770 --> 00:44:05.500
And if they're all zero then X1 and
X2 are independent.

00:44:08.902 --> 00:44:11.350
So x1 is independent of x2.

00:44:15.751 --> 00:44:21.069
So that would not be, if we allowed
weird examples like this one,

00:44:21.069 --> 00:44:25.131
then that statement would
not be true because Z and

00:44:25.131 --> 00:44:30.352
s times x z are clearly not
independent but they are uncorrelated

00:44:30.352 --> 00:44:35.510
cuz if you compute the covariance
of this and this you get zero.

00:44:35.510 --> 00:44:40.130
So this are normal and they are
uncorrelated that doesn't imply that they

00:44:40.130 --> 00:44:44.460
are independent, but
with multi-variance normal that is true.

00:44:44.460 --> 00:44:49.491
So for a quick example of this,

00:44:49.491 --> 00:44:54.715
suppose we had x+y and x-y, so

00:44:54.715 --> 00:45:01.101
let's let x,y be iid*N(0,1),

00:45:01.101 --> 00:45:06.130
okay, and consider the sum and

00:45:06.130 --> 00:45:09.344
the difference.

00:45:09.344 --> 00:45:14.387
Okay so X+Y and X-Y,

00:45:14.387 --> 00:45:18.822
taken together as a vector, that's

00:45:18.822 --> 00:45:25.860
multi-variant normal We'd probably
call it bivariate normal in this case.

00:45:25.860 --> 00:45:28.640
When it's two dimensional,
it's often called bivariate normal.

00:45:28.640 --> 00:45:31.880
That's multivariate normal for
the same reason we just saw.

00:45:31.880 --> 00:45:34.330
That is, if you take any consonant
times this plus consonant times this,

00:45:35.480 --> 00:45:36.080
it's normal.

00:45:36.080 --> 00:45:40.310
So therefore it's multivariate normal and
they are uncorrelated.

00:45:43.943 --> 00:45:50.407
Because if we take the covariance of
x+y and x-y and just expand that out,

00:45:50.407 --> 00:45:55.859
is the covariance of x with itself,
that's the variance of x.

00:45:56.920 --> 00:46:02.880
And then it's plus covariance (x,y) but
from this one, but

00:46:02.880 --> 00:46:08.400
there is also a minus covariance x and
y from those two so those cancel and

00:46:08.400 --> 00:46:14.280
then there is minus covariance(y) with
itself, so that is minus the variance of y

00:46:15.920 --> 00:46:20.360
so this cancels, and
because they both have variants one.

00:46:20.360 --> 00:46:24.380
Or in general, if they both had the same
variance here, the variances cancel.

00:46:24.380 --> 00:46:25.080
We get zero.

00:46:25.080 --> 00:46:27.940
So they're uncorrelated, the sum and

00:46:27.940 --> 00:46:31.380
difference of IOD standard
normal are uncorrelated.

00:46:31.380 --> 00:46:34.170
In general uncorrelated would
not tell us independent, but

00:46:34.170 --> 00:46:39.270
here because they're multi-variant normal,
we now know that X plus Y,

00:46:39.270 --> 00:46:41.867
and X minus Y are independent.

00:46:43.673 --> 00:46:47.530
That's another very,
very special fact about the normal,

00:46:47.530 --> 00:46:52.517
it's very difficult to prove, but it turns
out to be true that if we did here I,

00:46:52.517 --> 00:46:56.069
I, D Anything other than
a normal distribution here and

00:46:56.069 --> 00:46:59.470
if it were true that this
isn't independent of that,

00:46:59.470 --> 00:47:03.170
then actually it would have
to be a normal distribution.

00:47:03.170 --> 00:47:06.500
So this is a very special property
of the normal distribution and

00:47:06.500 --> 00:47:09.530
the proof is just from this
fact about multivariate normal.

00:47:09.530 --> 00:47:10.940
Okay.
So to prove this fact,

00:47:10.940 --> 00:47:15.210
you could just do a calculation with
MGFs that we don't have time for and

00:47:15.210 --> 00:47:16.490
that you don't need to worry about.

00:47:16.490 --> 00:47:20.010
Although you could do it for
practice with the MGF if you want.

00:47:20.010 --> 00:47:23.330
But anyway,
knowing this result is very very useful.

00:47:23.330 --> 00:47:26.250
Because it's often easier to compute
the covariants than to directly show

00:47:26.250 --> 00:47:26.870
they're independent.

