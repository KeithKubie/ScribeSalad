WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:04.200
So let's get started training
a logistic classifier.

00:00:04.200 --> 00:00:07.830
A logistic classifier is what's
called the linear classifier.

00:00:07.830 --> 00:00:12.130
It takes the input, for example,
the pixels in an image, and

00:00:12.130 --> 00:00:15.559
applies a linear function to them
to generate its predictions.

00:00:16.690 --> 00:00:20.130
A linear function is just
a giant matrix multiply.

00:00:20.130 --> 00:00:25.100
It take all the inputs as a big vector,
that will denote X, and multiplies

00:00:25.100 --> 00:00:30.390
them with a matrix to generate its
predictions, one per output class.

00:00:30.390 --> 00:00:35.280
Throughout, we'll denote the inputs
by X, the weights by W, and

00:00:35.280 --> 00:00:36.800
the biased term by b.

00:00:37.960 --> 00:00:41.830
The weights of the matrix and the bias,
is where the machine learning comes in.

00:00:41.830 --> 00:00:43.750
We're going to train that model.

00:00:43.750 --> 00:00:46.520
That means we're going to try to
find the values for the weights and

00:00:46.520 --> 00:00:50.480
bias, which are good at
performing those predictions.

00:00:50.480 --> 00:00:54.840
How are we going to use the scores
to perform the classification?

00:00:54.840 --> 00:00:56.620
Well, let's recap our task.

00:00:56.620 --> 00:01:03.430
Each image, that we have as an input can
have one and only one possible label.

00:01:03.430 --> 00:01:06.520
So, we're going to turn
the scores into probabilities.

00:01:06.520 --> 00:01:09.890
We're going to want the probability of
the correct class to be very close to

00:01:09.890 --> 00:01:14.490
one and the probability for
every other class to be close to zero.

00:01:14.490 --> 00:01:17.070
The way to turn scores
into probabilities is to

00:01:17.070 --> 00:01:20.990
use a softmax function,
which I'll denote here by S.

00:01:20.990 --> 00:01:22.500
This is what it looks like.

00:01:22.500 --> 00:01:25.170
But beyond the formula,
what's important to know about it

00:01:25.170 --> 00:01:30.800
is that it can take any kind of scores
and turn them into proper probabilities.

00:01:30.800 --> 00:01:35.320
Proper probabilities, sum to 1,
and they will be large

00:01:35.320 --> 00:01:39.820
when the scores are large and small when
the scores are comparatively smaller.

00:01:40.990 --> 00:01:45.502
Scores in the context of logistic
regression are often also called logits.

