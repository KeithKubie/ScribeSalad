WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.000
So in lecture, we spent a lot of time learning about context-free grammars and languages.

00:00:05.000 --> 00:00:07.000
Can context-free grammars be used

00:00:07.000 --> 00:00:10.000
to do anything other than parsing a programming language?

00:00:10.000 --> 00:00:14.000
That's a good point, Peter, and the answer is yes.

00:00:14.000 --> 00:00:18.000
In fact, there are a large number of applications for language formalisms

00:00:18.000 --> 00:00:20.000
or context-free grammars.

00:00:20.000 --> 00:00:22.000
I'll just list 4 or 5 of them.

00:00:22.000 --> 00:00:25.000
One of the first is security.

00:00:25.000 --> 00:00:29.000
We saw last time that regular expressions can be used in virus checkers.

00:00:29.000 --> 00:00:32.000
And in fact, that's how the vast majority of virus checkers work.

00:00:32.000 --> 00:00:34.000
They're like a big lexer.

00:00:34.000 --> 00:00:38.000
It turns out that these days the most common attacks or exploits

00:00:38.000 --> 00:00:42.000
or vulnerabilities reported involve cross-site scripting

00:00:42.000 --> 00:00:46.000
or SQL--database--code injection vulnerabilities.

00:00:46.000 --> 00:00:52.000
Both of these involve an application, a web application like Udacity's servers,

00:00:52.000 --> 00:00:57.000
reading in a string from the user and treating it as if it were part of another important language

00:00:57.000 --> 00:01:01.000
like HTML or SQL, the language of database queries.

00:01:01.000 --> 00:01:04.000
It turns out that there a number of techniques available

00:01:04.000 --> 00:01:11.000
for using context-free grammars to detect if a string from a user is normal

00:01:11.000 --> 00:01:15.000
or if it's trying to take advantage of some special system structure

00:01:15.000 --> 00:01:17.000
if it's one of these attacks.

00:01:17.000 --> 00:01:22.000
And essentially, the way to do that is we pretend to parse the final string

00:01:22.000 --> 00:01:24.000
given the context that the user has provided.

00:01:24.000 --> 00:01:28.000
And if the user's changes, the string provided by the user

00:01:28.000 --> 00:01:33.000
ends up influencing a large part of the parse tree, of the abstract syntax tree,

00:01:33.000 --> 00:01:37.000
in essence, if the high watermark of the changes or the tainted information from the user

00:01:37.000 --> 00:01:41.000
reaches very, very high, then we know it's a SQL code injection

00:01:41.000 --> 00:01:45.000
or cross-site scripting attack because those attacks are based on destroying

00:01:45.000 --> 00:01:50.000
or conflicting or deforming the essential structure of the output language.

00:01:50.000 --> 00:01:53.000
So 1 possible use for them is in security.

00:01:53.000 --> 00:01:56.000
Some of the best ways to figure out if something is SQL code injection

00:01:56.000 --> 00:01:59.000
or cross-site scripting, let's say, before you actually run it and find out,

00:01:59.000 --> 00:02:02.000
involve the use of grammars.

00:02:02.000 --> 00:02:05.000
Another is in the world of optimization.

00:02:05.000 --> 00:02:11.000
A production compiler or interpreter is often very interested in getting the same results

00:02:11.000 --> 00:02:15.000
but faster or using less memory or--and this is a big one these days--

00:02:15.000 --> 00:02:17.000
using less power.

00:02:17.000 --> 00:02:20.000
If you're running a program on your smartphone or some other mobile device,

00:02:20.000 --> 00:02:24.000
you really want it to last as long as possible.

00:02:24.000 --> 00:02:28.000
Later on in the class, around Unit 6, we'll actually cover some basic optimizations.

00:02:28.000 --> 00:02:31.000
I'll teach some of them to you, and you'll get a chance to use them

00:02:31.000 --> 00:02:34.000
in your JavaScript interpreter for the web browser.

00:02:34.000 --> 00:02:36.000
But for now, just take my word for it.

00:02:36.000 --> 00:02:41.000
In order to do optimizations, we have to figure out what the values and variables are.

00:02:41.000 --> 00:02:45.000
For example, if I can figure out that X is always 0

00:02:45.000 --> 00:02:48.000
and you have a line in your program that says Y gets Y + X,

00:02:48.000 --> 00:02:51.000
I don't have to bother interpreting that line

00:02:51.000 --> 00:02:54.000
because I know that adding 0 to something doesn't change its value.

00:02:54.000 --> 00:02:58.000
That sort of thing is called a data flow analysis or an optimization,

00:02:58.000 --> 00:03:02.000
because if I can reason about how numbers flow through your program,

00:03:02.000 --> 00:03:06.000
then I can make them faster, higher, stronger, use less power.

00:03:06.000 --> 00:03:08.000
It's a really nice idea.

00:03:08.000 --> 00:03:12.000
It's going to turn out that the best known research idea for figuring out values

00:03:12.000 --> 00:03:16.000
as they flow through various functions in your program

00:03:16.000 --> 00:03:21.000
is related to context-free grammars or, more formally, context-free language reachability.

00:03:21.000 --> 00:03:26.000
It turns out that interprocedural data flow analysis is the same problem as,

00:03:26.000 --> 00:03:29.000
in a theoretical sense, context-free language reachability,

00:03:29.000 --> 00:03:34.000
which is the same problem as could I generate a string in a context-free grammar.

00:03:34.000 --> 00:03:37.000
It turns out all of you are experts at figuring out if we could generate a string

00:03:37.000 --> 00:03:39.000
in the language of a context-free grammar,

00:03:39.000 --> 00:03:43.000
and exactly that sort of knowledge is one of the things program optimizers use

00:03:43.000 --> 00:03:47.000
to squeeze every last drop of goodness out of a program.

00:03:47.000 --> 00:03:51.000
Third possibility is in linguistics or computational linguistics.

00:03:51.000 --> 00:03:54.000
There are a number of references to these underneath the lecture videos,

00:03:54.000 --> 00:03:57.000
but it turns out that a lot of these notions of context-free grammars

00:03:57.000 --> 00:04:01.000
or generative grammars or universal grammars actually came out of psychology

00:04:01.000 --> 00:04:04.000
or linguistics or computational linguistics

00:04:04.000 --> 00:04:06.000
to help us understand human and natural language.

00:04:06.000 --> 00:04:08.000
And it's a really important use.

00:04:08.000 --> 00:04:10.000
But let me skip past that one since it's too easy in some sense.

00:04:10.000 --> 00:04:12.000
I'll give you 2 more.

00:04:12.000 --> 00:04:17.000
One more--near and dear to my heart--is specification mining.

00:04:17.000 --> 00:04:21.000
It turns out that often we want to figure out what a program should be doing

00:04:21.000 --> 00:04:23.000
just by looking at its source code.

00:04:23.000 --> 00:04:25.000
And this is super tricky.

00:04:25.000 --> 00:04:27.000
It's like trying to learn the rules of English grammar

00:04:27.000 --> 00:04:29.000
by reading a bunch of high school student essays

00:04:29.000 --> 00:04:32.000
and looking at what they get right and get wrong in common.

00:04:32.000 --> 00:04:35.000
As you can imagine, it's very difficult to get this perfectly

00:04:35.000 --> 00:04:38.000
because just because a number of students say something

00:04:38.000 --> 00:04:40.000
doesn't necessarily mean that they're following the rules.

00:04:40.000 --> 00:04:42.000
They might all get it wrong the same way.

00:04:42.000 --> 00:04:44.000
And if you've seen people post on forums,

00:04:44.000 --> 00:04:46.000
maybe not everyone has perfect grammar.

00:04:46.000 --> 00:04:49.000
So just taking samples might not be the right way to do it.

00:04:49.000 --> 00:04:55.000
It turns out that the output of specification mining often takes the form of a formal grammar

00:04:55.000 --> 00:04:58.000
or some sort of state machine: You should always call open

00:04:58.000 --> 00:05:00.000
and then you should call close.

00:05:00.000 --> 00:05:03.000
A lot of important security or software engineering policies

00:05:03.000 --> 00:05:06.000
can be described in terms of some sort of grammar.

00:05:06.000 --> 00:05:10.000
It turns out that learning grammars from examples is really, really difficult.

00:05:10.000 --> 00:05:15.000
Both that and this notion I mentioned earlier of interprocedural data flow analysis

00:05:15.000 --> 00:05:19.000
are the sorts of things that one might get into in a subsequent course after this one

00:05:19.000 --> 00:05:22.000
but where you'd build on the knowledge you learned here.

00:05:22.000 --> 00:05:25.000
Here's my last example, and this one is perhaps the farthest from the norm,

00:05:25.000 --> 00:05:29.000
but I think students may appreciate it.

00:05:29.000 --> 00:05:32.000
It turns out that if you're using a cellular phone,

00:05:32.000 --> 00:05:37.000
people might be able to intercept what you're saying if they just listen very hard.

00:05:37.000 --> 00:05:42.000
So these days, modern cell phones or mobile phones might try to encrypt the data packets

00:05:42.000 --> 00:05:45.000
that they send containing your vocal information so that, in theory,

00:05:45.000 --> 00:05:48.000
eavesdroppers can't figure out what's going on.

00:05:48.000 --> 00:05:51.000
However, if I've used a lot of that computational linguistic information,

00:05:51.000 --> 00:05:55.000
if I've studied the world, it turns out that in different spoken languages

00:05:55.000 --> 00:06:01.000
or with different accents we have different patterns of speech and then pauses.

00:06:01.000 --> 00:06:04.000
And some of the initial implementations of phone encryption tried to be smart

00:06:04.000 --> 00:06:06.000
and save power.

00:06:06.000 --> 00:06:11.000
They would only send information when you actually said something.

00:06:11.000 --> 00:06:16.000
So this meant that attackers could listen in and, using linguistic analysis information,

00:06:16.000 --> 00:06:20.000
even if they couldn't tell what you were saying, even if it was all scrambled,

00:06:20.000 --> 00:06:24.000
just by doing a statistical analysis of how long you said and then how long you paused

00:06:24.000 --> 00:06:29.000
and then how long you said again, we can reliably identify what language you're speaking

00:06:29.000 --> 00:06:33.000
and, worse than that, regional accents--say you're speaking English--

00:06:33.000 --> 00:06:36.000
whether you're speaking with some sort of New York/Brooklyn accent or from the South

00:06:36.000 --> 00:06:38.000
or from California.

00:06:38.000 --> 00:06:42.000
We can tell based just on the patterns of how long you speak and how long the pauses are.

00:06:42.000 --> 00:06:48.000
That has very unfortunate Orwellian implications for some sort of surveillance state.

00:06:48.000 --> 00:06:53.000
Conveniently, it is more or less totally defeated by encrypting the silence as well--

00:06:53.000 --> 00:06:56.000
using more power to just get rid of that side channel,

00:06:56.000 --> 00:06:59.000
prevent people from figuring out when you're speaking and when you're not.

00:06:59.000 --> 00:07:02.000
But that's an example of an application that also uses this sort of linguistic analysis--

00:07:02.000 --> 00:07:06.000
perhaps a little less related to context-free grammars but still in the same vein.

00:07:06.000 --> 00:07:10.000
All in all, context-free grammars, context-free languages

00:07:10.000 --> 09:59:59.000
come up in significantly more of computer science than just parsing.

