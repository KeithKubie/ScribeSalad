WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.330
[MUSIC PLAYING]

00:00:04.216 --> 00:00:05.340
NOAH FIEDEL: Hi, everybody.

00:00:05.340 --> 00:00:07.420
I'm Noah Fidel, and I'm
going to share with you today

00:00:07.420 --> 00:00:09.211
how you can serve your
models in production

00:00:09.211 --> 00:00:12.275
with TensorFlow Serving.

00:00:12.275 --> 00:00:15.190
So we're going to start with
that image on the bottom right,

00:00:15.190 --> 00:00:17.607
and really this is just
showing how, as an industry,

00:00:17.607 --> 00:00:19.690
software engineering is
quite mature and advanced.

00:00:19.690 --> 00:00:22.065
And machine learning is still
early on in the early days.

00:00:22.065 --> 00:00:24.064
So if you think about all
the great things we're

00:00:24.064 --> 00:00:26.410
used to with soft engineering
and best practices

00:00:26.410 --> 00:00:28.089
that we've all
come to understand,

00:00:28.089 --> 00:00:30.130
things like source control
and continuous builds,

00:00:30.130 --> 00:00:31.312
we might take for granted.

00:00:31.312 --> 00:00:32.770
And machine learning,
we don't even

00:00:32.770 --> 00:00:34.210
know what some of
those things are yet,

00:00:34.210 --> 00:00:37.000
and that motivates a lot of what
we do with TensorFlow Serving.

00:00:37.000 --> 00:00:38.820
So starting with a few stories--

00:00:38.820 --> 00:00:42.730
who has taken a commercial
airline in the last 10 years?

00:00:42.730 --> 00:00:45.620
OK, so half of us are awake
and the other half probably

00:00:45.620 --> 00:00:47.360
have as well.

00:00:47.360 --> 00:00:48.860
So going back
about 12 years ago,

00:00:48.860 --> 00:00:51.170
I was working on a system
that did the flight planning

00:00:51.170 --> 00:00:54.050
software for the majority of the
world's commercial air flights.

00:00:54.050 --> 00:00:55.758
This included things
you might care about

00:00:55.758 --> 00:00:58.610
like how much fuel to take on
your plane to get somewhere.

00:00:58.610 --> 00:01:00.620
It didn't have source
control when I joined.

00:01:00.620 --> 00:01:02.310
It did when I left.

00:01:02.310 --> 00:01:04.110
And I was helping do
that, but this kind

00:01:04.110 --> 00:01:06.110
of motivates that even
though source control was

00:01:06.110 --> 00:01:08.610
an understood thing of the time,
just because we have a best

00:01:08.610 --> 00:01:10.700
practice doesn't mean
everybody uses it yet.

00:01:10.700 --> 00:01:11.830
And that's something
with TensorFlow Serving.

00:01:11.830 --> 00:01:13.640
We want to make best
practices that you all

00:01:13.640 --> 00:01:16.250
can use and make them easy
enough that you got them

00:01:16.250 --> 00:01:17.610
kind of by default.

00:01:17.610 --> 00:01:21.050
So in 2010, I was at a small
startup doing photo sharing

00:01:21.050 --> 00:01:23.217
on mobile apps.

00:01:23.217 --> 00:01:24.050
It was really great.

00:01:24.050 --> 00:01:25.130
We had some great
engineering, we

00:01:25.130 --> 00:01:26.588
tried to do all
the best practices,

00:01:26.588 --> 00:01:29.312
we had source control and
continuous builds, and so on.

00:01:29.312 --> 00:01:31.520
And one day we decided to
add a machine-learned model

00:01:31.520 --> 00:01:33.230
to do face detection
and auto cropping,

00:01:33.230 --> 00:01:34.230
and it was really great.

00:01:34.230 --> 00:01:38.150
We loved it, our users loved it,
our investors really loved it,

00:01:38.150 --> 00:01:41.084
but we had no continuous
build from machine learning.

00:01:41.084 --> 00:01:43.250
And at the time, the tools
are kind of ad hoc enough

00:01:43.250 --> 00:01:45.800
that we checked in the weights
to the model in the execution

00:01:45.800 --> 00:01:48.250
environment, but you couldn't
retrain them model, right?

00:01:48.250 --> 00:01:50.250
And now it's a kind of
understood best practice.

00:01:50.250 --> 00:01:52.210
You want to be able to do that.

00:01:52.210 --> 00:01:54.580
So here we are today in 2017.

00:01:54.580 --> 00:01:57.356
We have lots and lots of great
tools, but really, you know,

00:01:57.356 --> 00:01:59.230
we're getting started,
and we have a long way

00:01:59.230 --> 00:02:00.646
to go from machine learning.

00:02:00.646 --> 00:02:02.020
And hopefully,
all of us together

00:02:02.020 --> 00:02:04.144
can build a great ecosystem
around machine learning

00:02:04.144 --> 00:02:07.460
and best practices, and
so on, and share those.

00:02:07.460 --> 00:02:09.276
So here's the agenda
for today's talk.

00:02:09.276 --> 00:02:11.650
To answer the first question,
what is TensorFlow Serving?

00:02:11.650 --> 00:02:13.390
I'm actually going to
introduce what is serving.

00:02:13.390 --> 00:02:15.056
So some of you probably
know what it is.

00:02:15.056 --> 00:02:16.870
This might be, kind
of, the basics for you,

00:02:16.870 --> 00:02:18.578
but for those who
aren't familiar with it

00:02:18.578 --> 00:02:21.272
yet, if you ever want to
deploy your machine learning

00:02:21.272 --> 00:02:23.230
into production, you'll
want to become familiar

00:02:23.230 --> 00:02:24.700
with these concepts.

00:02:24.700 --> 00:02:28.030
So serving is how you apply
a machine learning model

00:02:28.030 --> 00:02:30.004
after you've trained it.

00:02:30.004 --> 00:02:31.670
Seems pretty simple
and straightforward.

00:02:31.670 --> 00:02:33.336
It has some attributes
though that might

00:02:33.336 --> 00:02:36.400
be unexpected or new to you.

00:02:36.400 --> 00:02:38.291
And, you know, taking
another view on this.

00:02:38.291 --> 00:02:40.540
Hopefully, everybody is
familiar with the orange boxes

00:02:40.540 --> 00:02:41.860
on the left side of the slide.

00:02:41.860 --> 00:02:42.880
So you have some data.

00:02:42.880 --> 00:02:45.702
You're going to use that to
train a model in TensorFlow.

00:02:45.702 --> 00:02:48.160
And then you have that model,
and you have your application

00:02:48.160 --> 00:02:49.094
on the right.

00:02:49.094 --> 00:02:51.010
Let's say you're showing
video recommendations

00:02:51.010 --> 00:02:53.466
to users in your application.

00:02:53.466 --> 00:02:55.090
Somehow you need to
get that model such

00:02:55.090 --> 00:02:57.410
that it's usable inside
your application.

00:02:57.410 --> 00:03:01.210
So the most common way to do
this is using an RPC server.

00:03:01.210 --> 00:03:04.030
And TensorFlow serving can
be used both as an RPC server

00:03:04.030 --> 00:03:06.850
and as a set of libraries inside
your app on embedded devices

00:03:06.850 --> 00:03:09.830
or in mobile.

00:03:09.830 --> 00:03:11.720
So a key attribute
for serving is

00:03:11.720 --> 00:03:13.340
that it's online
and low latency,

00:03:13.340 --> 00:03:15.740
and this differs quite
a bit from training

00:03:15.740 --> 00:03:17.990
and other big data
pipelines you might run.

00:03:17.990 --> 00:03:20.610
Your users don't want to
wait for the recommendations.

00:03:20.610 --> 00:03:21.380
They're not going
to wait a minute.

00:03:21.380 --> 00:03:23.296
They're probably not
going to wait 10 seconds.

00:03:23.296 --> 00:03:26.591
You really need to be fast
and consistently fast.

00:03:26.591 --> 00:03:29.090
Another thing that's, you know,
different from many big data

00:03:29.090 --> 00:03:31.940
tools out there is that
you'll have many models

00:03:31.940 --> 00:03:32.850
in a single process.

00:03:32.850 --> 00:03:34.766
So let's say you had a
great production model,

00:03:34.766 --> 00:03:36.830
it's serving great
recommendations to your users,

00:03:36.830 --> 00:03:39.087
and then you have a new
experiment you want to run.

00:03:39.087 --> 00:03:40.670
The most common
pattern that people do

00:03:40.670 --> 00:03:43.610
is they want to load, say,
a second experimental model

00:03:43.610 --> 00:03:46.510
in their server alongside
their main production model.

00:03:46.510 --> 00:03:47.960
So it's really common.

00:03:47.960 --> 00:03:50.150
Another thing is an
emerging best practice

00:03:50.150 --> 00:03:52.100
that we're seeing a
lot inside Google,

00:03:52.100 --> 00:03:54.500
and we want to bring
more to industry--

00:03:54.500 --> 00:03:56.000
is that you'll
have many versions

00:03:56.000 --> 00:03:57.502
of your model over time.

00:03:57.502 --> 00:03:59.210
So the data that you
trained on last week

00:03:59.210 --> 00:04:01.921
might not be as relevant to the
data you gathered yesterday,

00:04:01.921 --> 00:04:03.920
and so you want to
continually train your models

00:04:03.920 --> 00:04:06.620
and continually deploy them.

00:04:06.620 --> 00:04:08.450
Last but not least,
I'm just going

00:04:08.450 --> 00:04:10.070
mention the last bullet here.

00:04:10.070 --> 00:04:12.871
Is anybody familiar
with mini-batching?

00:04:12.871 --> 00:04:13.370
OK.

00:04:13.370 --> 00:04:15.450
Just a f-- oh, a good number.

00:04:15.450 --> 00:04:18.240
So mini-batching is where
you take a bunch of examples

00:04:18.240 --> 00:04:20.233
at training time,
maybe cue them up,

00:04:20.233 --> 00:04:21.899
and then you run them
through your graph

00:04:21.899 --> 00:04:24.639
together to get more throughput
and higher efficiency.

00:04:24.639 --> 00:04:25.680
And this is really great.

00:04:25.680 --> 00:04:28.200
It lets you train models
more quickly, process a lot

00:04:28.200 --> 00:04:30.534
more training data, and
so the challenge here

00:04:30.534 --> 00:04:32.700
is how can you do that in
a production setting where

00:04:32.700 --> 00:04:34.590
all of your requests
arrive asynchronously

00:04:34.590 --> 00:04:36.560
and you want to keep a
nice bound on latency.

00:04:36.560 --> 00:04:38.810
And we have some good tools,
both at the library level

00:04:38.810 --> 00:04:41.700
and binaries for you to do that.

00:04:41.700 --> 00:04:44.260
All right, so moving on to
what is TensorFlow Serving.

00:04:44.260 --> 00:04:47.096
So it's a flexible,
high-performance serving system

00:04:47.096 --> 00:04:49.470
for machine models, and it's
designed for your production

00:04:49.470 --> 00:04:51.970
environments.

00:04:51.970 --> 00:04:54.980
TensorFlow Serving has
three major pillars.

00:04:54.980 --> 00:04:56.890
The first are C++ libraries.

00:04:56.890 --> 00:05:00.060
These are what we use internally
to build our binaries,

00:05:00.060 --> 00:05:02.620
and they're, you
know, all open source.

00:05:02.620 --> 00:05:03.820
And they're very low levels.

00:05:03.820 --> 00:05:06.790
They include things like how to
save and restore a TensorFlow

00:05:06.790 --> 00:05:10.720
model as well as how to load
new model versions over time.

00:05:10.720 --> 00:05:12.310
We have binaries
that incorporate

00:05:12.310 --> 00:05:14.800
all of the best practices
we've learned thus far

00:05:14.800 --> 00:05:16.660
out of the box, and
for the emerging ones

00:05:16.660 --> 00:05:18.760
that we're not sure if
it's a best practice yet,

00:05:18.760 --> 00:05:21.440
you know, flag so that you
can enable and try them.

00:05:21.440 --> 00:05:24.340
We also have Docker
containers, and tutorials,

00:05:24.340 --> 00:05:27.100
and code labs that let you
auto scale our binaries

00:05:27.100 --> 00:05:29.510
on Kubernetes and so on.

00:05:29.510 --> 00:05:32.920
And finally, we have a hosted
service with Google Cloud ML

00:05:32.920 --> 00:05:36.190
as well as an internal
version of that.

00:05:36.190 --> 00:05:39.640
OK, so diving into
our libraries.

00:05:39.640 --> 00:05:42.252
Our core platform is
completely generic,

00:05:42.252 --> 00:05:44.710
and what this means is let's
say you have a current system.

00:05:44.710 --> 00:05:48.160
Maybe you're serving on a legacy
or in-house built ML system

00:05:48.160 --> 00:05:49.870
and you want to
adopt TensorFlow,

00:05:49.870 --> 00:05:51.520
but for some transition
period of time

00:05:51.520 --> 00:05:54.280
you might want to mix that
in with your legacy system.

00:05:54.280 --> 00:05:58.030
So our core platform lets you
include any C++ class you want

00:05:58.030 --> 00:06:00.400
as a servable--

00:06:00.400 --> 00:06:02.800
The components of the
libraries are all a la carte,

00:06:02.800 --> 00:06:04.390
so every component--
if you go look

00:06:04.390 --> 00:06:06.370
at our page, every
single class we

00:06:06.370 --> 00:06:09.640
have is used by some customer
inside Google on its own

00:06:09.640 --> 00:06:11.410
and in just about
any combination

00:06:11.410 --> 00:06:12.610
that you can imagine.

00:06:12.610 --> 00:06:14.890
They're also used and deployed.

00:06:14.890 --> 00:06:16.930
And taking the last
three bullets together,

00:06:16.930 --> 00:06:20.980
the highlight here is
that all of our APIs,

00:06:20.980 --> 00:06:24.040
by-and-large, allow you to plug
in your own implementations.

00:06:24.040 --> 00:06:26.110
So you can, you
know, add support

00:06:26.110 --> 00:06:27.370
for a different model storage.

00:06:27.370 --> 00:06:29.730
Maybe you're already
an existing user of ML.

00:06:29.730 --> 00:06:32.270
Maybe you have hundreds
of models in a database.

00:06:32.270 --> 00:06:34.540
If we provide a way to get
models off a file system,

00:06:34.540 --> 00:06:36.850
you can easily extend that
to support a custom database

00:06:36.850 --> 00:06:39.800
or a custom data store.

00:06:39.800 --> 00:06:41.610
OK, so I'm going to
walk you through.

00:06:41.610 --> 00:06:43.830
Hopefully, this is
not too complex.

00:06:43.830 --> 00:06:46.680
This is our libraries at a
high level inside TensorFlow

00:06:46.680 --> 00:06:49.500
Serving, and they're also
arranged here exactly

00:06:49.500 --> 00:06:52.780
as they are inside our binaries
that we pre-build for you.

00:06:52.780 --> 00:06:55.620
So the green boxes are
our standard, kind of,

00:06:55.620 --> 00:06:58.710
abstractions with APIs,
and the yellow boxes

00:06:58.710 --> 00:07:01.200
are plug-ins into
those abstractions.

00:07:01.200 --> 00:07:03.775
So let's imagine that
this is a server,

00:07:03.775 --> 00:07:05.400
and you're doing
video recommendations,

00:07:05.400 --> 00:07:07.760
and you have version one
of your model loaded.

00:07:07.760 --> 00:07:09.330
And it's called My Model.

00:07:09.330 --> 00:07:11.235
So you have the
source, and the source

00:07:11.235 --> 00:07:14.070
is responsible for
identifying new models

00:07:14.070 --> 00:07:15.944
and-- that should be loaded.

00:07:15.944 --> 00:07:17.860
So we have a file system
plug into the source.

00:07:17.860 --> 00:07:19.360
It does exactly
what it sounds like.

00:07:19.360 --> 00:07:20.736
It monitors the
file system, sees

00:07:20.736 --> 00:07:22.235
that there's a new
version that say,

00:07:22.235 --> 00:07:23.980
ah-ha, version two
arrived, right?

00:07:23.980 --> 00:07:26.710
And now we want to load version
two because it has-- you know,

00:07:26.710 --> 00:07:28.168
was train on fresher
data, and it's

00:07:28.168 --> 00:07:31.270
going to provide better
recommendations to your users.

00:07:31.270 --> 00:07:35.440
So it's going to create a loader
of a TensorFlow SavedModel.

00:07:35.440 --> 00:07:38.350
And it's important to note
that the loader of a SavedModel

00:07:38.350 --> 00:07:41.080
knows how to load the
SavedModel and it knows

00:07:41.080 --> 00:07:44.517
how to estimate the resources
such as RAM or GPU storage

00:07:44.517 --> 00:07:45.850
that will be used by that model.

00:07:45.850 --> 00:07:47.500
It doesn't actually load it yet.

00:07:47.500 --> 00:07:49.120
That's the job of the manager.

00:07:49.120 --> 00:07:52.372
So this loader's admitted to the
manager as an inspired version,

00:07:52.372 --> 00:07:53.830
and it's actually
up to the manager

00:07:53.830 --> 00:07:57.160
to figure out when it's ready,
when it has threads scheduling

00:07:57.160 --> 00:07:59.620
available, and when it
has enough resources

00:07:59.620 --> 00:08:02.640
to load that new model
version, it'll do so.

00:08:02.640 --> 00:08:04.850
And this is where one
really key plug-in

00:08:04.850 --> 00:08:07.740
comes into play, which
is the version policy.

00:08:07.740 --> 00:08:10.650
And it turns out that in
almost all scenarios, if you're

00:08:10.650 --> 00:08:13.530
serving something like video
recommendations to users,

00:08:13.530 --> 00:08:16.110
you want that serving system
to always be available, right?

00:08:16.110 --> 00:08:18.330
You never want have a downtime.

00:08:18.330 --> 00:08:20.760
On the other hand, there
are use cases out there

00:08:20.760 --> 00:08:23.760
where let's say you have
an offline, big batch

00:08:23.760 --> 00:08:27.240
pipeline that's maybe annotating
these videos in batch, right?

00:08:27.240 --> 00:08:29.880
And it's not
directly user facing.

00:08:29.880 --> 00:08:31.650
And let's say that
your model is very big.

00:08:31.650 --> 00:08:35.970
You might prefer to have a
little bit of unavailability

00:08:35.970 --> 00:08:38.700
in that pipeline and save
a bunch of memory, right?

00:08:38.700 --> 00:08:41.130
So instead of loading both
versions of that model at once,

00:08:41.130 --> 00:08:43.830
you can actually delete the old
one and then load the new one

00:08:43.830 --> 00:08:46.050
and just have a little
hiccup in your serving.

00:08:46.050 --> 00:08:48.900
So the version policies lets
you preserve availability

00:08:48.900 --> 00:08:50.380
or preserve resources.

00:08:50.380 --> 00:08:53.150
So it's a nice thing there.

00:08:53.150 --> 00:08:56.375
OK, so onto some strengths
of the libraries.

00:08:56.375 --> 00:08:58.000
First and foremost
is they're optimized

00:08:58.000 --> 00:08:59.920
for very high performance
and robustness.

00:08:59.920 --> 00:09:02.170
So these are used in some
of the largest serving

00:09:02.170 --> 00:09:06.580
systems at Google at, you
know, pretty large scale.

00:09:06.580 --> 00:09:11.590
And we do things like, you
know, ref-count accesses

00:09:11.590 --> 00:09:12.430
to your models.

00:09:12.430 --> 00:09:14.864
So from the previous
slide, let's imagine

00:09:14.864 --> 00:09:16.780
that version two of the
models load-- and this

00:09:16.780 --> 00:09:18.280
is kind of getting
into the details,

00:09:18.280 --> 00:09:19.790
but version two is loaded.

00:09:19.790 --> 00:09:22.292
You can, actually,
immediately unload version one

00:09:22.292 --> 00:09:24.500
because there might still
be a request pending to it.

00:09:24.500 --> 00:09:26.770
So we actually keep
track, via ref-counting,

00:09:26.770 --> 00:09:30.220
of the requests in-flight on
each version of your models,

00:09:30.220 --> 00:09:32.380
and only after all the
requests have quiesced can

00:09:32.380 --> 00:09:35.530
we delete a version
that's no longer needed.

00:09:35.530 --> 00:09:38.200
And last but not least,
just kind of reemphasizing,

00:09:38.200 --> 00:09:40.000
we have these nice
plug-in APIs, so you

00:09:40.000 --> 00:09:42.040
can build your own
sources to get models

00:09:42.040 --> 00:09:45.670
from a database, an RPC
layer, a pub subsystem, kind

00:09:45.670 --> 00:09:48.800
of whatever you like.

00:09:48.800 --> 00:09:51.260
OK, so last show of hands.

00:09:51.260 --> 00:09:55.295
Who's used TensorFlow
Serving libraries so far?

00:09:55.295 --> 00:09:55.795
Great.

00:09:55.795 --> 00:09:56.336
A few of you.

00:09:56.336 --> 00:09:59.220
So for anyone who's
thinking of using this,

00:09:59.220 --> 00:10:00.882
you would just see
this by default.

00:10:00.882 --> 00:10:03.090
This is in our tutorials,
but for those of you who've

00:10:03.090 --> 00:10:05.280
used the current
libraries, what we found,

00:10:05.280 --> 00:10:07.612
internally and externally,
is that you had this--

00:10:07.612 --> 00:10:09.570
setting up all of those
libraries with the best

00:10:09.570 --> 00:10:11.760
practices took a bunch of
boilerplate and, kind of,

00:10:11.760 --> 00:10:12.840
common code.

00:10:12.840 --> 00:10:16.180
So we made a new class called
ServerCore, and what this does

00:10:16.180 --> 00:10:18.870
is it lets you declare the
set of models you want loaded

00:10:18.870 --> 00:10:20.781
and pass them to
ServerCore and say, just

00:10:20.781 --> 00:10:22.530
give me a manager of
these models, please.

00:10:22.530 --> 00:10:24.113
I don't really care
about the details.

00:10:24.113 --> 00:10:26.304
Give me all the best
practices out of the box.

00:10:26.304 --> 00:10:27.720
And so, this should
let you delete

00:10:27.720 --> 00:10:29.580
about 200 to 300 lines of code.

00:10:29.580 --> 00:10:31.350
So give that a try.

00:10:31.350 --> 00:10:34.200
Send comments and
feature requests.

00:10:34.200 --> 00:10:37.590
OK, so this slide is
intentionally short.

00:10:37.590 --> 00:10:39.490
The binaries are
very, very simple.

00:10:39.490 --> 00:10:41.260
They take all of
the best practices

00:10:41.260 --> 00:10:43.240
from the libraries
of TensorFlow Serving

00:10:43.240 --> 00:10:47.106
and just wrap them GRPC
layer along with some flags,

00:10:47.106 --> 00:10:49.230
and configuration, and
monitoring, and other things

00:10:49.230 --> 00:10:50.640
you would need.

00:10:50.640 --> 00:10:52.870
Our binaries are
based on GRPC, which

00:10:52.870 --> 00:10:55.990
is Google's high performance,
open-source universal RPC

00:10:55.990 --> 00:10:57.420
framework.

00:10:57.420 --> 00:10:59.440
And you can extend this
as well, but this is

00:10:59.440 --> 00:11:01.870
what we provide out of the box.

00:11:01.870 --> 00:11:04.030
In terms of specific APIs--

00:11:04.030 --> 00:11:07.120
currently, open source complete
with the API implementation.

00:11:07.120 --> 00:11:10.780
We have a low-level,
Tensor-oriented predict API,

00:11:10.780 --> 00:11:13.870
so this should be usable for
any kind of modeling inference

00:11:13.870 --> 00:11:15.550
that you would need to do.

00:11:15.550 --> 00:11:19.300
Coming soon we have regression
and classification interfaces.

00:11:19.300 --> 00:11:21.760
The APIs are already on
GitHub, and the implementation

00:11:21.760 --> 00:11:22.510
is out for review.

00:11:22.510 --> 00:11:25.990
So it should land very soon.

00:11:25.990 --> 00:11:27.990
All right, now I'm to
move on to some challenges

00:11:27.990 --> 00:11:30.360
that we've seen along with
the best practices for how

00:11:30.360 --> 00:11:31.780
to solve them.

00:11:31.780 --> 00:11:33.660
So the first one,
this was a great story

00:11:33.660 --> 00:11:35.650
around isolation and latency.

00:11:35.650 --> 00:11:38.610
So this is a graph
of latency with--

00:11:38.610 --> 00:11:40.710
each of those spikes
is approximately daily,

00:11:40.710 --> 00:11:42.417
Mountain View time at noon.

00:11:42.417 --> 00:11:44.000
So this is for a
large Google customer

00:11:44.000 --> 00:11:46.620
serving several thousand
queries per second.

00:11:46.620 --> 00:11:48.390
And right around,
give or take, noon,

00:11:48.390 --> 00:11:52.020
there was this latency spike
at the 99.9th percentile.

00:11:52.020 --> 00:11:54.794
That put that serving
customer outside of their SLA.

00:11:54.794 --> 00:11:56.460
So we looked into
this and it turned out

00:11:56.460 --> 00:11:58.126
that there was a
fantastic engineer we'd

00:11:58.126 --> 00:11:59.820
been working
closely with, really

00:11:59.820 --> 00:12:02.520
liked to push new experimental
models right before

00:12:02.520 --> 00:12:05.410
or after he went to lunch.

00:12:05.410 --> 00:12:05.910
Who knew?

00:12:05.910 --> 00:12:07.618
And it-- actually, it
threw us for a loop

00:12:07.618 --> 00:12:10.080
because it did move forward
and backward by an hour or two

00:12:10.080 --> 00:12:10.580
every day.

00:12:10.580 --> 00:12:13.500
And so we didn't find any
automated jobs doing this,

00:12:13.500 --> 00:12:14.910
but we did find the problem.

00:12:14.910 --> 00:12:17.460
It was loading a new model--
could have a pretty big latency

00:12:17.460 --> 00:12:20.350
impact on your existing
models running inference.

00:12:20.350 --> 00:12:22.339
So if you dive in a
little bit deeper,

00:12:22.339 --> 00:12:23.880
it turns out that
the default-- and I

00:12:23.880 --> 00:12:26.340
think it's a very good
default for a TensorFlow-- is

00:12:26.340 --> 00:12:27.480
to optimize for throughput.

00:12:27.480 --> 00:12:29.560
That's what most of us
want in most situations.

00:12:29.560 --> 00:12:32.060
So you don't have to go set a
flag that says, please give me

00:12:32.060 --> 00:12:32.670
throughput.

00:12:32.670 --> 00:12:35.490
But a side effect of that
is, by default, optimizing

00:12:35.490 --> 00:12:38.192
for throughput pretty much
all the models and all

00:12:38.192 --> 00:12:39.900
the sessions on your
task will get access

00:12:39.900 --> 00:12:41.370
to all of the threads.

00:12:41.370 --> 00:12:44.094
So again, very good default,
but for the specific case of you

00:12:44.094 --> 00:12:45.510
have many models
and many versions

00:12:45.510 --> 00:12:47.460
floating over time, what
you really want to do

00:12:47.460 --> 00:12:48.930
is isolate the
loading threads away

00:12:48.930 --> 00:12:50.130
from your inference threads.

00:12:50.130 --> 00:12:52.530
So we added-- you know,
it's pretty fancy slide,

00:12:52.530 --> 00:12:54.950
but we added just one flag.

00:12:54.950 --> 00:12:56.700
You can set the number
of loading threads,

00:12:56.700 --> 00:12:59.730
and we typically set it to
somewhere around one or two.

00:12:59.730 --> 00:13:02.670
And I'll show you
the after slide.

00:13:02.670 --> 00:13:06.570
So you can see much more detail,
but the main point from here

00:13:06.570 --> 00:13:08.730
is that the y-axis
dropped by 10x.

00:13:08.730 --> 00:13:11.640
So the latency spikes now are
roughly a hundred milliseconds.

00:13:11.640 --> 00:13:14.250
This is completely inside
the customer's SLA.

00:13:14.250 --> 00:13:15.114
Customer's happy.

00:13:15.114 --> 00:13:17.280
We have a best practice now
that all of you can use.

00:13:20.420 --> 00:13:22.670
The next challenge-- and
I hinted at this earlier

00:13:22.670 --> 00:13:25.070
in the talk-- is about
batching, and handling

00:13:25.070 --> 00:13:27.320
asynchronous requests,
and getting the efficiency

00:13:27.320 --> 00:13:29.210
of mini-batching at serve time.

00:13:29.210 --> 00:13:31.370
So if you look at the
little traffic on the right,

00:13:31.370 --> 00:13:33.740
you have the green,
blue, and orange boxes.

00:13:33.740 --> 00:13:35.450
And these represent
your user queries

00:13:35.450 --> 00:13:38.450
arriving at different
times, kind of falling down

00:13:38.450 --> 00:13:39.680
towards your server.

00:13:39.680 --> 00:13:41.990
And what you want
to do, ideally,

00:13:41.990 --> 00:13:44.000
to get that
mini-batching-like performance

00:13:44.000 --> 00:13:45.950
is kind of wait for
some period of time.

00:13:45.950 --> 00:13:48.050
Take a few requests,
put them together,

00:13:48.050 --> 00:13:50.216
and run one graph computation.

00:13:50.216 --> 00:13:51.590
At the same time,
you really want

00:13:51.590 --> 00:13:54.440
to keep a strict upper
bound on the latency you're

00:13:54.440 --> 00:13:56.780
willing to wait.

00:13:56.780 --> 00:13:59.680
So we've done quite a
bit of investment here.

00:13:59.680 --> 00:14:01.940
Our batching is
available as a library

00:14:01.940 --> 00:14:05.120
as well as via flag
for the server.

00:14:05.120 --> 00:14:07.310
And there are a lot
of interesting nuances

00:14:07.310 --> 00:14:10.040
here that we didn't even realize
what we're getting to when

00:14:10.040 --> 00:14:11.270
we first started doing this.

00:14:11.270 --> 00:14:13.400
And as just one
example, let's imagine

00:14:13.400 --> 00:14:15.914
that you had two models
on the same machine.

00:14:15.914 --> 00:14:17.330
What you might
want to actually do

00:14:17.330 --> 00:14:19.220
is schedule their
batches back to back

00:14:19.220 --> 00:14:21.865
instead of having them overlap
and contend for threads.

00:14:21.865 --> 00:14:23.990
So we have the concept of
a shared-batch scheduler.

00:14:23.990 --> 00:14:25.460
So each model has
its own, and they

00:14:25.460 --> 00:14:29.310
cooperate via shared scheduler.

00:14:29.310 --> 00:14:31.560
As a teaser for
later, sequence models

00:14:31.560 --> 00:14:34.980
make this really interesting
given that they're typically

00:14:34.980 --> 00:14:36.959
not just a fee-forward network.

00:14:36.959 --> 00:14:38.750
And the sequences can
be of varying length,

00:14:38.750 --> 00:14:40.350
so it gets very tricky
and challenging for how

00:14:40.350 --> 00:14:41.550
to batch these together.

00:14:41.550 --> 00:14:43.392
And Eugene is up in
a couple of talks.

00:14:43.392 --> 00:14:45.600
He's going to go into much
more detail about batching

00:14:45.600 --> 00:14:48.530
and sequences.

00:14:48.530 --> 00:14:50.240
A few areas of emerging tech.

00:14:50.240 --> 00:14:52.460
So Jonathan was
mentioning a SavedModel.

00:14:52.460 --> 00:14:55.850
We're quite excited to announce
this and encourage everybody

00:14:55.850 --> 00:14:56.750
to adopt it.

00:14:56.750 --> 00:14:59.510
So SavedModel is the
universal serialization format

00:14:59.510 --> 00:15:00.980
for TensorFlow Models.

00:15:00.980 --> 00:15:04.010
It has-- it's included
in terms of flow 1.0

00:15:04.010 --> 00:15:06.200
and also in TensorFlow
Serving as of right

00:15:06.200 --> 00:15:09.260
now, live on the repo.

00:15:09.260 --> 00:15:11.820
There are two marquee
features of SavedModel.

00:15:11.820 --> 00:15:14.630
So the first one is support
for multiple MetaGraphs.

00:15:14.630 --> 00:15:17.420
And for folks who have been,
you know, mostly doing training,

00:15:17.420 --> 00:15:19.336
this-- you might not
know why you'd want this,

00:15:19.336 --> 00:15:21.830
but once you hear more details
you'll see at serve time

00:15:21.830 --> 00:15:23.990
it's really important to
have different MetaGraphs.

00:15:23.990 --> 00:15:27.170
And the use case here is let's
say you were training a model,

00:15:27.170 --> 00:15:28.670
and you went to
save it for serving.

00:15:28.670 --> 00:15:31.003
So you typically want to
remove things that are training

00:15:31.003 --> 00:15:32.690
specific like input queues.

00:15:32.690 --> 00:15:33.710
And drop out layers.

00:15:33.710 --> 00:15:36.461
You really don't want those in
your production serving models.

00:15:36.461 --> 00:15:37.460
A few people have tried.

00:15:37.460 --> 00:15:40.220
You get really bad
results or no results

00:15:40.220 --> 00:15:42.265
at all if they
just hang on a cue.

00:15:42.265 --> 00:15:43.640
Other things you
might want to do

00:15:43.640 --> 00:15:45.350
is you might want to
transform your serving graphs.

00:15:45.350 --> 00:15:47.141
So you might want to
have a quantized graph

00:15:47.141 --> 00:15:49.100
for serving on a GPU or a TPU.

00:15:49.100 --> 00:15:51.340
And what the multiple
MetaGraph support does

00:15:51.340 --> 00:15:53.810
is it lets you have as
MetaGraphs as you want,

00:15:53.810 --> 00:15:57.140
and you can store and
access them by simple tags.

00:15:57.140 --> 00:15:59.630
So you can tag one with
Serve, one with Train,

00:15:59.630 --> 00:16:03.730
one with Serve and
GPU or TPU, and so on.

00:16:03.730 --> 00:16:07.160
The next concept
is SignatureDefs.

00:16:07.160 --> 00:16:09.100
So a SignatureDef
defines the signature

00:16:09.100 --> 00:16:12.040
of a computation supported
by a TensorFlow graph.

00:16:12.040 --> 00:16:13.520
And this is really important.

00:16:13.520 --> 00:16:15.370
So if you look at the
slide, most people

00:16:15.370 --> 00:16:17.410
probably can right away--
especially for humans.

00:16:17.410 --> 00:16:17.950
We're good at this.

00:16:17.950 --> 00:16:19.660
You can read input
classes and scores

00:16:19.660 --> 00:16:22.610
and probably figure out what's
going on with this graph.

00:16:22.610 --> 00:16:24.652
But if you handed this
graph without those labels

00:16:24.652 --> 00:16:26.234
to a serving system,
it would probably

00:16:26.234 --> 00:16:27.880
have no idea what to
do with it, right?

00:16:27.880 --> 00:16:30.220
With these graphs, you can
feed or fetch from just

00:16:30.220 --> 00:16:31.940
about any node in most graphs.

00:16:31.940 --> 00:16:34.120
So how would you identify
where your input goes

00:16:34.120 --> 00:16:36.850
and where your output goes, and
that's what SignatureDefs do.

00:16:36.850 --> 00:16:39.340
So in this case, they specify
that that middle node,

00:16:39.340 --> 00:16:41.715
on the left, is where you want
to feed in your TensorFlow

00:16:41.715 --> 00:16:44.470
example, and the top right
node is your string Classes,

00:16:44.470 --> 00:16:47.080
and the bottom right is
your floating point Scores

00:16:47.080 --> 00:16:50.840
for, say, a classification.

00:16:50.840 --> 00:16:52.421
Building onto
SignatureDefs, we're

00:16:52.421 --> 00:16:54.170
adding support for
Multi-headed Inference.

00:16:54.170 --> 00:16:55.919
So this is one of the
top feature requests

00:16:55.919 --> 00:16:57.740
we've had from the community.

00:16:57.740 --> 00:17:01.140
Is anyone familiar with
Multi-headed inference?

00:17:01.140 --> 00:17:01.670
OK.

00:17:01.670 --> 00:17:02.639
Very few.

00:17:02.639 --> 00:17:04.430
This is going to be
really popular and more

00:17:04.430 --> 00:17:07.118
common as people productionize
and deploy machine learning

00:17:07.118 --> 00:17:08.060
in their products.

00:17:08.060 --> 00:17:10.520
So it's really,
really common that you

00:17:10.520 --> 00:17:13.010
might start with one model,
maybe a click-through model,

00:17:13.010 --> 00:17:15.550
to do predictions on, say,
your video recommendations.

00:17:15.550 --> 00:17:17.300
But over time, you
might decide, you know,

00:17:17.300 --> 00:17:20.390
we'd also like to train a model
for conversions or maybe people

00:17:20.390 --> 00:17:22.608
who watch the whole video.

00:17:22.608 --> 00:17:25.040
And quite often people
would go about this

00:17:25.040 --> 00:17:26.540
by actually training
a second model,

00:17:26.540 --> 00:17:28.040
but for starters,
you're training on

00:17:28.040 --> 00:17:29.420
almost all the same data.

00:17:29.420 --> 00:17:30.920
All of your data,
pre-processing,

00:17:30.920 --> 00:17:34.970
all of your hidden layers,
quite often, are the same,

00:17:34.970 --> 00:17:37.099
and then at serve
time you're doing

00:17:37.099 --> 00:17:38.390
a lot of redundant computation.

00:17:38.390 --> 00:17:40.517
So you're parsing your
input, preprocessing it,

00:17:40.517 --> 00:17:43.100
doing the same, you know, hidden
layer computation, and really

00:17:43.100 --> 00:17:44.660
only the output layers change.

00:17:44.660 --> 00:17:48.110
So our Multi-headed Inference
support builds on signatures,

00:17:48.110 --> 00:17:50.720
and lets you specify
multiple signatures together.

00:17:50.720 --> 00:17:53.790
And say, run these
together in one request,

00:17:53.790 --> 00:17:55.735
and it will do that.

00:17:55.735 --> 00:17:57.110
Performing
Multi-headed Inference

00:17:57.110 --> 00:17:59.960
will save you operational
overhead and complexity

00:17:59.960 --> 00:18:01.730
of deploying multiple models.

00:18:01.730 --> 00:18:05.120
It'll save you bandwidth
CPU latency, and so on.

00:18:05.120 --> 00:18:08.240
So we're really excited, and
that should land sometime

00:18:08.240 --> 00:18:10.840
in the next week or two.

00:18:10.840 --> 00:18:12.160
All right, so wrapping up.

00:18:12.160 --> 00:18:14.620
And again, Eugene is going
to go into much more depth

00:18:14.620 --> 00:18:15.539
on sequence models.

00:18:15.539 --> 00:18:17.080
I just wanted to
highlight that there

00:18:17.080 --> 00:18:19.560
are many flavors
of sequence models.

00:18:19.560 --> 00:18:21.760
They are generally
very expensive to serve

00:18:21.760 --> 00:18:23.950
both in compute
cost and latency,

00:18:23.950 --> 00:18:26.590
and we're investigating,
specifically,

00:18:26.590 --> 00:18:28.540
batching, padding, and
unrolling strategies

00:18:28.540 --> 00:18:30.970
to make them more efficient
and effective for all of you

00:18:30.970 --> 00:18:33.890
to serve.

00:18:33.890 --> 00:18:37.350
All right, so for
anybody who's interested,

00:18:37.350 --> 00:18:40.560
we really warmly welcome
collaboration, pull requests,

00:18:40.560 --> 00:18:43.680
feature requests,
bugs, and so on.

00:18:43.680 --> 00:18:46.410
We sync all of our
code to Github weekly.

00:18:46.410 --> 00:18:47.910
We have a developer
on call, and it

00:18:47.910 --> 00:18:51.450
includes facilitating your pull
requests, answering questions,

00:18:51.450 --> 00:18:54.040
comments, and ideas, and so on.

00:18:54.040 --> 00:18:58.670
To get in touch, you can reach
us at discuss@tensorflow.org.

00:18:58.670 --> 00:19:01.160
So here are a bunch of links
for how you can get started.

00:19:01.160 --> 00:19:03.680
I'll let you read these
offline, and please

00:19:03.680 --> 00:19:07.430
do get in touch with your
questions and feedback.

00:19:07.430 --> 00:19:10.410
All right, so thanks very
much and up next is Ashish.

00:19:10.410 --> 00:19:11.010
[APPLAUSE]

00:19:11.010 --> 00:19:13.760
[MUSIC PLAYING]

