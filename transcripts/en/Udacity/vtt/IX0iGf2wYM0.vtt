WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:02.130
So now, we have an intuition of best, and how we

00:00:02.130 --> 00:00:07.210
want to split. We've, we've looked over, Michael's proposed, the high-level

00:00:07.210 --> 00:00:09.550
algorithm for how we would build a decision tree. And I

00:00:09.550 --> 00:00:12.600
think we have enough information now that we can actually do,

00:00:12.600 --> 00:00:15.770
a real specific algorithm. So, let's write that down. And the

00:00:15.770 --> 00:00:19.530
particular algorithm that Michael proposed is a kind of generic version

00:00:19.530 --> 00:00:22.270
of something that's called ID3. So let me write down what

00:00:22.270 --> 00:00:25.110
that algorithm is, and we can talk about it. Okay, so here's

00:00:25.110 --> 00:00:28.750
the ID3 algorithm. You're simply going to keep looping forever until you've

00:00:28.750 --> 00:00:33.440
solved the problem. At each step, you're going to pick the best attribute,

00:00:33.440 --> 00:00:35.650
and we're going to define what we mean by best. There are

00:00:35.650 --> 00:00:37.970
a couple of different ways we might, we might define best in

00:00:37.970 --> 00:00:40.970
a moment. And then, given the best attribute that splits the

00:00:40.970 --> 00:00:42.660
data the way that we want, it does all those things that

00:00:42.660 --> 00:00:46.860
we talked about, assign that as a decision attribute for node.

00:00:46.860 --> 00:00:50.130
And then for each value that the attribute A can take on,

00:00:50.130 --> 00:00:53.760
create a descendent of node. Sort the training examples

00:00:53.760 --> 00:00:56.840
to those leaves based upon exactly what values they

00:00:56.840 --> 00:01:00.310
take on, and if you've perfectly classified your training

00:01:00.310 --> 00:01:03.750
set, then you stop. Otherwise, you iterate over each

00:01:03.750 --> 00:01:07.140
of those leaves, picking the best attribute in turn

00:01:07.140 --> 00:01:09.380
for the training examples that were sorted into that

00:01:09.380 --> 00:01:11.460
leaf, and you keep doing that. Building up the

00:01:11.460 --> 00:01:15.310
tree until you're done. So that's the ID3 algorithm.

00:01:15.310 --> 00:01:18.290
And the key bit that we have to expand upon

00:01:18.290 --> 00:01:21.840
in this case, is exactly what it means to have a

00:01:21.840 --> 00:01:25.320
best attribute. All right so, what is exactly, what exactly

00:01:25.320 --> 00:01:27.730
is it that we mean by best attribute? So, there are

00:01:27.730 --> 00:01:30.090
lots of possibilities, that you can come up with. The

00:01:30.090 --> 00:01:32.570
one that is most common, and the one I want you

00:01:32.570 --> 00:01:36.080
to think about the most, is what's called information gain.

00:01:36.080 --> 00:01:41.070
So information gain is simply a mathematical way to capture the

00:01:41.070 --> 00:01:43.020
amount of information that i want to gain by

00:01:43.020 --> 00:01:46.540
picking particular attribute, funnily enough. But what it

00:01:46.540 --> 00:01:49.000
really talks about is the reduction in the

00:01:49.000 --> 00:01:52.490
randomness, over the labels that you have with

00:01:52.490 --> 00:01:55.190
set of data, based upon the knowing the

00:01:55.190 --> 00:01:58.270
value of particular attribute. So the formula's simply

00:01:58.270 --> 00:02:02.610
this. The information gain over S and A

00:02:02.610 --> 00:02:06.410
where S is the collection of training examples

00:02:06.410 --> 00:02:08.050
that you're looking at. And A, as

00:02:08.050 --> 00:02:11.490
a particular attribute, is simply defined as the

00:02:11.490 --> 00:02:14.970
entropy, with respect to the labels, of

00:02:14.970 --> 00:02:17.320
the set of training examples, you have S,

00:02:17.320 --> 00:02:21.605
minus, sort of, the expected or average

00:02:21.605 --> 00:02:24.950
entropy that you would have over each set

00:02:24.950 --> 00:02:28.020
of examples that you have with a particular

00:02:28.020 --> 00:02:30.150
value. Does that make sense to you, Michael?

00:02:30.150 --> 00:02:31.900
&gt;&gt; So what we're doing, we're picking an attribute and

00:02:31.900 --> 00:02:33.250
that attribute could have a bunch of different

00:02:33.250 --> 00:02:36.840
values, like true or false, or short, medium, tall?

00:02:36.840 --> 00:02:37.010
&gt;&gt; Right.

00:02:37.010 --> 00:02:37.170
&gt;&gt; And.

00:02:37.170 --> 00:02:38.320
&gt;&gt; And that's represented by v.

00:02:38.320 --> 00:02:41.670
&gt;&gt; Okay, each of those is a different v. And then we're

00:02:41.670 --> 00:02:45.470
saying okay, for over those leaves, we're going to do this entropy thing again.

00:02:45.470 --> 00:02:46.443
&gt;&gt; Mm-hm.

00:02:46.443 --> 00:02:52.420
&gt;&gt; And we right. So what, and what is entropy?

00:02:53.470 --> 00:02:57.140
&gt;&gt; entropy. So, we'll talk about entropy later on in the class

00:02:57.140 --> 00:03:00.820
in some detail and define it exactly and mathematically. And

00:03:00.820 --> 00:03:03.080
some of you probably already know what, what entropy is,

00:03:03.080 --> 00:03:05.070
but for those of you who don't, it's exactly a

00:03:05.070 --> 00:03:08.406
measure of randomness. So if I have a coin, let's

00:03:08.406 --> 00:03:11.030
say a two-headed coin. It can be heads or tails,

00:03:11.030 --> 00:03:14.070
and I don't know anything about the coin except that

00:03:14.070 --> 00:03:17.790
it's probably fair. If I were to flip the coin,

00:03:17.790 --> 00:03:21.170
what's the probability that it would end up heads versus tails?

00:03:21.170 --> 00:03:21.840
&gt;&gt; A half.

00:03:21.840 --> 00:03:22.600
&gt;&gt; It's

00:03:22.600 --> 00:03:24.900
a half, exactly, if it's a fair coin it's a half.

00:03:24.900 --> 00:03:28.530
Which means that I have no basis, going into flipping the coin,

00:03:28.530 --> 00:03:32.410
to guess either way whether it's heads or it's tails. And so

00:03:32.410 --> 00:03:34.730
that has a lot of entropy. In fact it has exactly what's

00:03:34.730 --> 00:03:38.220
called one bit of entropy. On the other hand, let's imagine

00:03:38.220 --> 00:03:41.930
that I have a coin that has heads on both sides. Then,

00:03:41.930 --> 00:03:44.890
before I even flip the coin, I already know what the outcome's

00:03:44.890 --> 00:03:47.620
going to be. It's going to come up heads. So what's the

00:03:47.620 --> 00:03:49.610
probability of it coming up with heads? It's.

00:03:49.610 --> 00:03:50.730
&gt;&gt; One.

00:03:50.730 --> 00:03:53.160
&gt;&gt; One. So that actually has no

00:03:53.160 --> 00:03:56.998
information, no randomness, no entropy whatsoever. And has

00:03:56.998 --> 00:04:01.710
zero bits of entropy. So, when I look at this set of examples that I

00:04:01.710 --> 00:04:06.870
have, and the set of labels I have, I can count the number that are coming up,

00:04:06.870 --> 00:04:13.050
let's say, red x's. Versus the ones that are coming up green o's.

00:04:13.050 --> 00:04:16.209
And if those are evenly split, then the entropy of them

00:04:16.209 --> 00:04:18.839
is maximal, because if I were to close my eyes and

00:04:18.839 --> 00:04:21.769
reach for an instance, I have no way of knowing beforehand

00:04:21.769 --> 00:04:24.660
whether I'm more likely to get an x or I'm more likely

00:04:24.660 --> 00:04:28.620
to get an o. On the other hand, if I have

00:04:28.620 --> 00:04:31.970
all the x's in together, then I already know before I even

00:04:31.970 --> 00:04:34.640
reach in that I'm going to end up with an x. So

00:04:34.640 --> 00:04:38.160
as I have more of one label than the other the amount

00:04:38.160 --> 00:04:40.810
of entropy goes down. That is, I have

00:04:40.810 --> 00:04:43.000
more information going in. Does that make sense, Michael?

00:04:43.000 --> 00:04:45.392
&gt;&gt; I think so. So, is there, can, maybe we

00:04:45.392 --> 00:04:48.968
can say what the formula is for this, or, or?

00:04:48.968 --> 00:04:51.560
&gt;&gt; Sure. What is the formula for it? You should remember.

00:04:52.570 --> 00:04:56.412
&gt;&gt; It's, if we have, well, I'm not sure what the notation ought to be with

00:04:56.412 --> 00:05:01.650
these S's but it has something to do with Log P log, no wait, it's P(log)P.

00:05:01.650 --> 00:05:03.160
&gt;&gt; Mm-hm. So the

00:05:03.160 --> 00:05:07.054
actual formula for entropy, using the same notation that we're

00:05:07.054 --> 00:05:10.090
using for information gain is simply the sum, over all

00:05:10.090 --> 00:05:13.390
the possible values that you might see, of the probability

00:05:13.390 --> 00:05:16.624
of you seeing that value, times the log of the probability

00:05:16.624 --> 00:05:19.800
of you seeing that value, times minus one. And I

00:05:19.800 --> 00:05:21.590
don't want to get into the details here. We're going to go

00:05:21.590 --> 00:05:24.080
into a lot more details about this later when we

00:05:24.080 --> 00:05:28.330
get further on in the class with randomize optimization, where entropy's

00:05:28.330 --> 00:05:30.440
going to matter a lot. But for now, I just, you

00:05:30.440 --> 00:05:33.230
have, I want you to have the intuition that this

00:05:33.230 --> 00:05:36.250
is a measure of information. This is the measure of

00:05:36.250 --> 00:05:39.690
randomness in some variable that you haven't seen. It's the likelihood

00:05:39.690 --> 00:05:42.090
of you already knowing what you're going to get if you

00:05:42.090 --> 00:05:45.450
close your eyes and pick one of the training examples, versus

00:05:45.450 --> 00:05:47.770
you not knowing what you're going to get. If you close

00:05:47.770 --> 00:05:50.500
your eyes and you picked one of the training examples. Okay?

00:05:50.500 --> 00:05:53.120
&gt;&gt; Alright. So, well, so, okay, so then in the practice,

00:05:54.300 --> 00:05:56.780
trees that you had given us before, it was the

00:05:56.780 --> 00:06:00.740
case that we worked, we wanted to prefer splits that I

00:06:00.740 --> 00:06:02.810
guess, made things less random, right? So if things were

00:06:02.810 --> 00:06:06.820
all mixed together, the reds and the greens, after the split

00:06:06.820 --> 00:06:08.300
if it was all reds on one side and all

00:06:08.300 --> 00:06:10.610
greens on the other. Then each of those two sides would

00:06:10.610 --> 00:06:13.660
have very, what? They would have very low entropy, even though

00:06:13.660 --> 00:06:17.050
when we started out before the split we had high entropy.

00:06:17.050 --> 00:06:19.187
&gt;&gt; Right, that's exactly right. So if you, if you remember the,

00:06:19.187 --> 00:06:22.320
the, three examples before. One of them, it was the case

00:06:22.320 --> 00:06:25.450
that all of the samples went down the left side of

00:06:25.450 --> 00:06:27.850
the tree. So the amount of entropy that we had, didn't

00:06:27.850 --> 00:06:31.460
change at all. So there was no gain in using that attribute.

00:06:31.460 --> 00:06:34.590
In another case, we split the data in half. But in

00:06:34.590 --> 00:06:37.510
each case, we had half of the x's and half of the

00:06:37.510 --> 00:06:40.820
o's together, on both sides of the split. Which means that

00:06:40.820 --> 00:06:44.430
the total amount of entropy actually didn't change at all. Even though

00:06:44.430 --> 00:06:49.290
we split the data. And in the final case, the best one, we still split

00:06:49.290 --> 00:06:52.330
the data in half, but since all of the x's ended up on one side

00:06:52.330 --> 00:06:55.730
and all of the o's ended up on the other side, we had to entropy

00:06:55.730 --> 00:06:57.940
or no randomness left whatsoever. And that

00:06:57.940 --> 00:06:59.710
gave us the maximum amount of information gain.

00:06:59.710 --> 00:07:01.010
&gt;&gt; So is that how we're choosing the

00:07:01.010 --> 00:07:03.050
best attribute? The one with the maximum gain?

00:07:03.050 --> 00:07:06.880
&gt;&gt; Exactly. So the goal is to maximize over the entropy gain.

00:07:09.160 --> 00:07:10.360
And that's the best attribute.

