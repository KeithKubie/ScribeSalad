WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:02.410
All right, so let's get started.

00:00:02.410 --> 00:00:06.990
So today,
we're gonna talk about what are probably

00:00:06.990 --> 00:00:11.490
the two most famous theorems in
the entire history of probably.

00:00:11.490 --> 00:00:15.890
They're called the law of large
numbers and the central limit theorem.

00:00:15.890 --> 00:00:20.330
They're closely related, so makes sense
to do them together, kind of compare and

00:00:20.330 --> 00:00:21.100
contrast them.

00:00:22.320 --> 00:00:27.640
I don't, I can't think of a more famous
probability theorem than these two.

00:00:27.640 --> 00:00:34.690
So the setup for today is that
we have i.i.d random variables.

00:00:34.690 --> 00:00:39.400
Let's just call them X1, X2 i.i.d.

00:00:39.400 --> 00:00:44.920
Since they're i.i.d they have
the same mean and variance.

00:00:44.920 --> 00:00:49.060
If the mean and variance exists but
we'll assume they do.

00:00:49.060 --> 00:00:50.600
So the mean, we'll just call it Mu.

00:00:50.600 --> 00:00:53.670
And the variants, sigma squared.

00:00:56.460 --> 00:00:59.200
So we're assuming that
these are finite for now.

00:00:59.200 --> 00:01:00.760
The mean and variants exist.

00:01:00.760 --> 00:01:05.420
And both of these theorems tell us

00:01:05.420 --> 00:01:09.200
what happens to the sample
mean as n gets large.

00:01:09.200 --> 00:01:12.470
So, the sample mean is
just defined as Xn bar.

00:01:14.090 --> 00:01:17.960
Standard notation in statistics
is put a bar to mean averages and

00:01:17.960 --> 00:01:20.000
that's just the average of the first n.

00:01:21.130 --> 00:01:24.718
So to take the first n random variables,
and average them, so

00:01:24.718 --> 00:01:26.934
that's just called the sample mean.

00:01:32.491 --> 00:01:37.980
So the question is, what can we
say about Xn bar as n gets large?

00:01:37.980 --> 00:01:44.180
So the way we would interpret this or
use this is we get to observe.

00:01:44.180 --> 00:01:47.540
These Xs, they're random variables but
after we observe them they become data.

00:01:47.540 --> 00:01:51.230
We're never going to have
an infinite amount of data so

00:01:51.230 --> 00:01:52.870
at some point we stop it at n.

00:01:52.870 --> 00:01:57.440
We can think of that as the sample size
and hopefully we get a large sample size.

00:01:57.440 --> 00:01:58.700
Of course, it depends on the problem.

00:01:58.700 --> 00:02:01.440
Some problems,
you may not be able to get large n.

00:02:01.440 --> 00:02:03.230
Well, we assume n is large, and

00:02:03.230 --> 00:02:07.110
just take the average,
question is just, what can we say?

00:02:07.110 --> 00:02:12.217
All right, so first,
here's what the law of large numbers says.

00:02:16.689 --> 00:02:17.860
It's a very simple statement.

00:02:19.870 --> 00:02:22.140
And hopefully pretty intuitive, too.

00:02:22.140 --> 00:02:27.510
Law of Large Numbers says that Xn bar

00:02:27.510 --> 00:02:33.600
converges to mu, as n goes to infinity.

00:02:36.217 --> 00:02:37.726
With probability 1.

00:02:39.745 --> 00:02:44.152
That's the fine print, probability 1.

00:02:44.152 --> 00:02:47.380
With probability 0, so
something really crazy could happen.

00:02:47.380 --> 00:02:50.170
But we don't worry too much about it,
because it has probability 0.

00:02:50.170 --> 00:02:54.568
With probability 1,
this is the sample mean, and

00:02:54.568 --> 00:02:59.394
it says that the sample mean
converges to the true mean.

00:03:03.673 --> 00:03:11.750
So, that is a pretty nice,
intuitive, easy to remember result.

00:03:11.750 --> 00:03:14.920
That is,
by true I mean the theoretical mean.

00:03:14.920 --> 00:03:21.410
That is the expected value of Xj for
any j is the true expected value.

00:03:21.410 --> 00:03:24.070
Whereas this, is a random variable.

00:03:24.070 --> 00:03:25.200
Right?
We're taking an average of

00:03:25.200 --> 00:03:25.850
random variables.

00:03:25.850 --> 00:03:26.940
That's a random variable.

00:03:26.940 --> 00:03:30.790
So this is just a constant but
this is a random variable.

00:03:30.790 --> 00:03:35.600
But it's gonna converge and
I should say a little bit more,

00:03:35.600 --> 00:03:39.000
what is this convergence
statement actually mean.

00:03:39.000 --> 00:03:43.690
You've all seen limit of sequences, but
when we are talking about limits of random

00:03:43.690 --> 00:03:45.290
variables we have to be
a little more careful.

00:03:45.290 --> 00:03:47.690
How do we actually define this.

00:03:47.690 --> 00:03:53.030
The definition of this statement
is just pointwise which means,

00:03:54.150 --> 00:03:56.050
remember Xn bar is a random variable.

00:03:56.050 --> 00:03:58.390
Random variable mathematically
speaking is a function.

00:03:58.390 --> 00:04:02.260
So it's say for each possible,
if you evaluate this at some

00:04:02.260 --> 00:04:06.140
specific outcome of the experiment,
then you'll get a sequence of numbers.

00:04:06.140 --> 00:04:11.030
That is if you actually observed the
values and this kind of crystallizes into

00:04:11.030 --> 00:04:14.810
numbers when you evaluate it at
the outcome of the experiment.

00:04:14.810 --> 00:04:20.730
And so those numbers converge to mu.

00:04:20.730 --> 00:04:23.430
In other words, this is an event.

00:04:23.430 --> 00:04:27.720
Either these random variables converge or
they don't.

00:04:27.720 --> 00:04:31.375
And we say that event has probability 1.

00:04:31.375 --> 00:04:34.900
That' what the statement
of the theorem is.

00:04:34.900 --> 00:04:38.470
So to just give a simple example.

00:04:41.160 --> 00:04:45.326
Let's think about what happens
if we have Bernoulli p.

00:04:45.326 --> 00:04:50.429
So if Xj is Bernoulli p,
then intuitively we're

00:04:50.429 --> 00:04:56.500
just imagining a infinite
sequence of coin tosses.

00:04:56.500 --> 00:05:00.367
Where the probability of heads is p, and

00:05:00.367 --> 00:05:06.279
then this says that if we add up
all of these Bernoullis up to n,

00:05:06.279 --> 00:05:13.899
that it's just in the first coin flips,
how many times did the coin land heads,

00:05:13.899 --> 00:05:20.778
divided by the number of flips should
convert to p with probability 1.

00:05:25.294 --> 00:05:28.800
So for example, so
this is a very intuitive statement.

00:05:28.800 --> 00:05:33.730
If it's a fair coin and
you flip the coin a million times, well,

00:05:33.730 --> 00:05:39.130
you're not really expecting that it
will be 500,000 heads and 500,00 tails.

00:05:39.130 --> 00:05:44.000
But you do think that,
in the long run, it should be the case

00:05:44.000 --> 00:05:48.670
that it's going to be essentially
half heads, half tails.

00:05:48.670 --> 00:05:50.930
Not exactly, but essentially.

00:05:50.930 --> 00:05:54.570
And the proportion should get closer and
closer to the true value.

00:05:56.170 --> 00:05:59.730
This qualification would probably 1 is
needed because mathematically speaking

00:05:59.730 --> 00:06:04.750
even if you have a fair coin,
there's nothing in the math that says

00:06:04.750 --> 00:06:09.340
it's impossible that the coin would land
heads, heads, heads, heads, heads forever.

00:06:09.340 --> 00:06:12.580
You know that that's never
actually gonna happen in reality.

00:06:13.810 --> 00:06:15.170
It's just not gonna happen.

00:06:15.170 --> 00:06:16.710
It's a fair coin.

00:06:16.710 --> 00:06:20.740
It might land heads, heads, heads for
a time if you're very lucky or

00:06:20.740 --> 00:06:21.920
unlucky or whatever.

00:06:21.920 --> 00:06:25.180
But it's not gonna be heads,
heads, heads forever.

00:06:26.400 --> 00:06:30.040
But there's nothing in the math that
says that's an invalid sequence.

00:06:31.660 --> 00:06:35.570
So there's some weird
pathological cases like that.

00:06:35.570 --> 00:06:39.050
But with probability one,
we get what we expect.

00:06:39.050 --> 00:06:44.650
If we didn't have this result,
how we would ever even estimate p?

00:06:45.820 --> 00:06:48.460
You might imagine if you
didn't know what p was,

00:06:48.460 --> 00:06:51.040
kind of the obvious thing to do is
flip the coin a lot of times and

00:06:51.040 --> 00:06:54.950
take the proportion of heads and
use that as your approximation for p.

00:06:54.950 --> 00:06:57.130
But what justification could you have for

00:06:57.130 --> 00:07:00.070
doing that approximation
if you didn't have this.

00:07:00.070 --> 00:07:02.880
So this is a very, very necessary result.

00:07:06.140 --> 00:07:09.890
But I guess to comment a little bit
more about what does it actually say for

00:07:09.890 --> 00:07:13.650
the coin, because this is kind of
related to gambler's fallacy, and

00:07:13.650 --> 00:07:14.350
things like that.

00:07:15.660 --> 00:07:19.880
The gambler's fallacy is the idea
that like let's say your gambling and

00:07:19.880 --> 00:07:25.840
you lose like ten times in a row and then
it's the feeling that your due to win.

00:07:27.040 --> 00:07:32.110
You lost all these times then and
you might try to justify that using a lot

00:07:32.110 --> 00:07:36.910
of large numbers and say well you
know the coin might landed let's say,

00:07:36.910 --> 00:07:41.510
heads you win money, tails you lose money,
you just lost money ten times in a row.

00:07:41.510 --> 00:07:44.330
But the law of large numbers says,
in the long run,

00:07:44.330 --> 00:07:47.030
it's gonna go back to
one-half if it's fair.

00:07:47.030 --> 00:07:50.160
So somehow you need to start
winning a lot to compensate.

00:07:51.560 --> 00:07:53.000
That's not the way it works.

00:07:54.540 --> 00:07:56.000
The coin is memoryless.

00:07:56.000 --> 00:08:00.290
The coin does not care how many failures
or how many losses you had before.

00:08:00.290 --> 00:08:04.370
So the way it works is not through If
you're unlucky at the beginning that

00:08:04.370 --> 00:08:09.900
somehow it gets offset later
by an increase in heads.

00:08:09.900 --> 00:08:14.415
The way it works is through
what we might call swamping.

00:08:14.415 --> 00:08:19.577
And let's say the coin landed
tails a 100 times in a row.

00:08:19.577 --> 00:08:24.050
It doesn't mean that the probability
has changed for 101st flip.

00:08:24.050 --> 00:08:29.920
What it means though, is that we're
letting n go to infinity here, okay?

00:08:29.920 --> 00:08:32.570
So no matter how unlucky you
were in the first 100 or

00:08:32.570 --> 00:08:37.620
the first million trials, that's
nothing compared to infinity, right?

00:08:37.620 --> 00:08:42.992
So those first million just get swamped
out by the entire infinite future,

00:08:42.992 --> 00:08:47.960
so that what's going on here.

00:08:50.875 --> 00:08:55.950
Yeah, so
to tell you one little story about the law

00:08:55.950 --> 00:09:01.130
of large numbers,
a colleague of mine told me this story.

00:09:01.130 --> 00:09:05.450
He had a student once who
said he hated statistics.

00:09:06.460 --> 00:09:08.770
And of course,
my colleague was very shocked,

00:09:08.770 --> 00:09:11.520
like how can anyone hate statistics?

00:09:11.520 --> 00:09:12.653
And so he asked, why?

00:09:12.653 --> 00:09:15.465
How is it possible that
you hate statistics?

00:09:15.465 --> 00:09:19.744
And then the student who was an athlete,
and he was training everyday and

00:09:19.744 --> 00:09:22.425
he had just learned
the law of large numbers.

00:09:22.425 --> 00:09:26.962
And he was very, very depressed by this
because he said, the law of large numbers

00:09:26.962 --> 00:09:30.917
says in the long run, I'm gonna only
be average and I can't improve.

00:09:30.917 --> 00:09:38.390
So well, of course the fallacy there,
we assumed iid right now.

00:09:38.390 --> 00:09:41.574
Now there are generalizations
of this theorem beyond iid, but

00:09:41.574 --> 00:09:43.084
we can't just get rid of iid.

00:09:43.084 --> 00:09:48.435
So the iid is saying that the distribution
is not changing with time.

00:09:48.435 --> 00:09:53.093
That doesn't mean that you can't actually
improve your own distribution then it

00:09:53.093 --> 00:09:54.140
would not be iid.

00:09:54.140 --> 00:09:59.227
So don't be depressed by this,
and in fact this theorem

00:09:59.227 --> 00:10:05.640
I think is crucial in order for
science to actually be possible.

00:10:05.640 --> 00:10:08.850
Because if you kind of
imagine kind of hypothetical

00:10:08.850 --> 00:10:13.800
counter factual world where this
theorem was actually false.

00:10:13.800 --> 00:10:18.682
That would be really depressing to try
to ever learn about the world, right?

00:10:18.682 --> 00:10:21.481
Cuz this is saying,
you're collecting more and more data.

00:10:21.481 --> 00:10:24.183
You're letting your sample
size go to infinity.

00:10:24.183 --> 00:10:28.032
And this says,
you converged to the truth, right?

00:10:28.032 --> 00:10:31.908
And it would be some weird setting, where
you get more and more data, and more and

00:10:31.908 --> 00:10:35.283
more data, and yet you're not able
to converge to the truth, right?

00:10:35.283 --> 00:10:36.760
So that would be really bad.

00:10:36.760 --> 00:10:39.070
So this is very intuitive, very important.

00:10:40.610 --> 00:10:47.390
Okay, so let's prove this
at least a similar version.

00:10:47.390 --> 00:10:51.480
So this is actually sometimes called
the strong law of large numbers.

00:10:53.280 --> 00:10:56.240
And we're actually gonna
prove what's sometimes called

00:10:56.240 --> 00:10:57.620
the weak law of large numbers.

00:10:57.620 --> 00:11:02.309
I don't really like the terminology
strong and weak here, but

00:11:02.309 --> 00:11:04.444
that's kind of a standard.

00:11:04.444 --> 00:11:07.407
Strong law of large numbers
is what I just said,

00:11:07.407 --> 00:11:11.125
where it's converging
point-wise with probability 1.

00:11:13.583 --> 00:11:19.235
That is just these random variables
converged to this constant,

00:11:19.235 --> 00:11:23.563
except on some bad event
that has probability 0.

00:11:23.563 --> 00:11:26.303
The weak law of large
numbers says that for

00:11:26.303 --> 00:11:32.337
any, C greater than 0,

00:11:32.337 --> 00:11:37.097
the probability that Xn bar minus

00:11:37.097 --> 00:11:42.367
the mean is greater than c goes to 0.

00:11:42.367 --> 00:11:46.290
So it's a very similar looking statement.

00:11:47.830 --> 00:11:49.410
It's not exactly equivalent.

00:11:49.410 --> 00:11:53.330
It's possible to show, you have to
go through some real analysis for

00:11:53.330 --> 00:11:55.600
this that is not necessary for
our purposes.

00:11:55.600 --> 00:11:57.970
But it turns out that, this statement,

00:11:57.970 --> 00:12:01.940
once you've proven this thing it
implies this form of convergence.

00:12:01.940 --> 00:12:06.676
This is called convergence in probability,
but

00:12:06.676 --> 00:12:09.801
the intuition is very similar.

00:12:09.801 --> 00:12:14.257
So just to interpret this statement
in words it says, so we can chose,

00:12:14.257 --> 00:12:17.439
we should interpret c as
being some small number.

00:12:17.439 --> 00:12:21.409
So let's say we chose c to be 0.001, okay?

00:12:21.409 --> 00:12:26.110
And then it says that this thing
goes to 0, so in other words, this,

00:12:26.110 --> 00:12:28.070
as n goes to infinity again.

00:12:29.100 --> 00:12:33.020
So this says that if n is large enough,
then

00:12:34.090 --> 00:12:38.860
it's extremely unlikely that
these are more than 0.001 apart.

00:12:38.860 --> 00:12:41.150
In other words, if n is large,

00:12:41.150 --> 00:12:46.900
it's extremely likely that this is
extremely close to this, right?

00:12:46.900 --> 00:12:49.510
So it's a very similar statement,
n is large,

00:12:49.510 --> 00:12:53.200
it's extremely likely that the sample
mean is very close to the true mean.

00:12:54.490 --> 00:12:55.880
Okay, so that's what it says.

00:12:55.880 --> 00:12:58.107
So we'll prove this one,

00:12:58.107 --> 00:13:03.279
because to prove this one takes
a lot of work and a lot of time.

00:13:03.279 --> 00:13:06.450
This one,
it looks like it's a nice-looking theorem.

00:13:06.450 --> 00:13:07.915
And it is a nice theorem, but

00:13:07.915 --> 00:13:11.040
we can prove it very easily
using Chebyshev's inequality.

00:13:15.355 --> 00:13:18.528
Okay, so
let's prove the weak law of large numbers.

00:13:23.507 --> 00:13:26.610
So all we need to do is show
that this goes to 0, right?

00:13:26.610 --> 00:13:28.460
That's what the statement is.

00:13:28.460 --> 00:13:32.690
So let's just bound it using, this looks
pretty similar to what we were doing last

00:13:32.690 --> 00:13:36.400
time, where we did Markov's inequality,
Chebyshev's inequality.

00:13:36.400 --> 00:13:39.761
This looks similar to that
kind of stuff from last time,

00:13:39.761 --> 00:13:42.790
which is why I did that, well,
one reason for doing that last time.

00:13:42.790 --> 00:13:46.604
We need the inequalities anyway,
but it's especially useful here.

00:13:46.604 --> 00:13:48.818
So we just need to show
this thing goes to 0.

00:13:48.818 --> 00:13:53.206
Xn bar minus mu greater than c, goes to 0,

00:13:55.062 --> 00:13:59.316
By Chebyshev's inequality,
this is less than or

00:13:59.316 --> 00:14:03.773
equal to the variance of Xn
bar divided by c squared,

00:14:03.773 --> 00:14:07.950
that's just exactly
Chebyshev from last time.

00:14:09.650 --> 00:14:15.040
Now we just need the variance of Xn bar,
variance of Xn bar,

00:14:15.040 --> 00:14:18.910
well, just stare at the definition
of Xn bar for a second.

00:14:18.910 --> 00:14:23.220
There's a 1 over n in front,
that comes out as 1 over n squared.

00:14:25.080 --> 00:14:28.450
And then since I'm assuming
they're iid an then dependent,

00:14:28.450 --> 00:14:32.030
the variance of the sum is just n
times the variance of one term.

00:14:32.030 --> 00:14:36.508
So that's n sigma squared
divided by c squared,

00:14:36.508 --> 00:14:40.230
which is sigma squared over nc squared.

00:14:41.870 --> 00:14:48.070
Sigma is a constant, c is a constant,
n goes to infinity, so this goes to 0.

00:14:48.070 --> 00:14:54.360
So that proved the weak law of large
numbers, just only a one line thing.

00:14:59.819 --> 00:15:06.566
Okay, so that tells us what happens
point-wise when we average a bunch

00:15:06.566 --> 00:15:11.860
of iid random variables, and
it converges to the mean.

00:15:11.860 --> 00:15:14.190
So let me just rewrite that statement.

00:15:14.190 --> 00:15:17.800
Then we'll write the central limit
theorem and kind of compare them.

00:15:17.800 --> 00:15:22.821
So another way to write
what we just showed

00:15:22.821 --> 00:15:27.842
is that Xn bar minus mu
goes to 0 as n goes to

00:15:27.842 --> 00:15:32.870
infinity, which is a good thing to know.

00:15:33.910 --> 00:15:40.360
However, it doesn't tell us what
the distribution of Xn bar looks like.

00:15:40.360 --> 00:15:48.300
So this is true with probability one,
but what is the distribution?

00:15:52.000 --> 00:15:57.579
What is the distribution
of Xn bar look like?

00:16:00.753 --> 00:16:05.086
So this says it's getting closer,
Xn bar is getting closer and

00:16:05.086 --> 00:16:07.700
closer to this constant mu.

00:16:07.700 --> 00:16:10.438
Okay, but that's not really
telling us the shape, and

00:16:10.438 --> 00:16:12.310
it's not really telling us the rate.

00:16:12.310 --> 00:16:16.183
This goes to 0, but at what rate?

00:16:16.183 --> 00:16:22.826
So one way to think about problems like
that, when you have something going to 0,

00:16:22.826 --> 00:16:27.949
and you wanna study something about,
how fast does it go to 0?

00:16:27.949 --> 00:16:30.094
Then one might, not just in here, but

00:16:30.094 --> 00:16:33.360
just as a general approach
to that kind of problem.

00:16:33.360 --> 00:16:37.380
We know this goes to 0, but
we don't know how fast.

00:16:37.380 --> 00:16:42.461
One way to study that would be multiply it
by something that goes to infinity, right.

00:16:42.461 --> 00:16:47.240
Now, if we multiply it by
something that goes to infinity,

00:16:47.240 --> 00:16:50.789
such that this times
this goes to infinity.

00:16:50.789 --> 00:16:55.080
Then we know that this part that blows
up is dominating over this part.

00:16:55.080 --> 00:16:58.475
And if we multiply by something
that goes to infinity, but

00:16:58.475 --> 00:17:02.930
this whole thing still goes to 0,
then that's more informative, right?

00:17:02.930 --> 00:17:08.004
So what's gonna happen is that we
can imagine multiplying here by

00:17:08.004 --> 00:17:12.985
n to some power and we're gonna
show that there's a power here,

00:17:12.985 --> 00:17:15.958
and to some power, fill in the blank.

00:17:15.958 --> 00:17:18.812
What we're gonna show is that,

00:17:18.812 --> 00:17:24.414
if the power here is above some
threshold and to the big powers,

00:17:24.414 --> 00:17:29.820
its gonna go to infinity fast,
this thing will just blow up.

00:17:29.820 --> 00:17:34.566
And if we put a smaller power than the
threshold here, then this is still going

00:17:34.566 --> 00:17:39.021
to infinity as long as this is a positive
power of n, this is still going to

00:17:39.021 --> 00:17:43.641
infinity, this parts going to 0,
but this part's dominating, right?

00:17:43.641 --> 00:17:46.728
So this term is competing with this term.

00:17:46.728 --> 00:17:49.520
This one goes to infinity,
this one goes to 0, okay?

00:17:49.520 --> 00:17:53.548
So then the question is what's
that magic threshold value?

00:17:53.548 --> 00:17:57.343
And the answer is one-half.

00:17:57.343 --> 00:17:58.797
So that's what we're
gonna study right now.

00:17:58.797 --> 00:18:04.287
So we're gonna take the square
root of n times xn bar minus mu.

00:18:04.287 --> 00:18:06.363
This is kind of the happy medium,

00:18:06.363 --> 00:18:11.705
where we're gonna get a non-degenerate
distribution, that this is gonna converge

00:18:11.705 --> 00:18:16.896
in distribution to an actual distribution,
it's not gonna just get killed to 0 or

00:18:16.896 --> 00:18:22.180
blow up to infinity, it's actually
gonna give us a nice distribution.

00:18:22.180 --> 00:18:28.313
Okay, and I'm also gonna divide by the
sigma here, makes it a little bit cleaner.

00:18:28.313 --> 00:18:31.169
So this is the central limit theorem now.

00:18:31.169 --> 00:18:33.375
I'm stating it, then we'll prove it.

00:18:37.266 --> 00:18:40.452
Central limit theorem says,
if you take this and

00:18:40.452 --> 00:18:43.260
look at what happens
as n goes to infinity.

00:18:47.910 --> 00:18:55.075
Converges to standard
normal in distribution.

00:18:55.075 --> 00:19:00.344
[SOUND] By convergence and
distribution, what we mean is that

00:19:00.344 --> 00:19:06.870
the distribution of this converges
to the standard normal distribution.

00:19:06.870 --> 00:19:09.811
In other words, you could take the CDF.

00:19:09.811 --> 00:19:14.890
I mean these may be discrete or continuous
or a mixture of discreet and continuous.

00:19:14.890 --> 00:19:20.000
So it doesn't necessarily have a PDF,
but every random variable has a CDF.

00:19:20.000 --> 00:19:22.959
So it says if you take the CDF of this,

00:19:22.959 --> 00:19:27.742
it's gonna converge to capital 5,
the standard normal.

00:19:27.742 --> 00:19:33.070
So I think this is kind of an amazing
result that this holds in such generality,

00:19:33.070 --> 00:19:38.480
right, because I mean the normal is just
this one, standard normal is just this

00:19:38.480 --> 00:19:44.484
one particular, it's a nice looking bell
curve, but that's just one distribution.

00:19:44.484 --> 00:19:48.757
And those x's they could be discrete,
they could be continuous,

00:19:48.757 --> 00:19:52.889
they could be extremely nasty
looking distributions, right?

00:19:52.889 --> 00:19:54.506
It could look like anything,

00:19:54.506 --> 00:19:57.940
the only thing we assumed was
that there was a finite variance.

00:19:59.050 --> 00:20:03.470
Other than that,
they could have an incredibly complicated,

00:20:03.470 --> 00:20:06.150
messy distribution.

00:20:06.150 --> 00:20:08.460
But it's always gonna
go to standard normal.

00:20:09.510 --> 00:20:14.416
So this is one of the reasons why
the standard normal distribution is so

00:20:14.416 --> 00:20:19.489
important on the one hand and so,
widely used, because this is a theorem

00:20:19.489 --> 00:20:24.894
as n goes to infinity is what it says,
but the way it's used in practice is then

00:20:24.894 --> 00:20:30.465
people use normal approximations all the
time and a lot of the justification for

00:20:30.465 --> 00:20:36.206
normal approximations is coming from this,
because this says that if n is large,

00:20:36.206 --> 00:20:41.164
then the sample mean will approximately
have a normal distribution.

00:20:44.016 --> 00:20:50.034
Even if the original data did not look
like they came from a normal distribution,

00:20:50.034 --> 00:20:55.060
when you average lots and
lots of them, it looks normal, okay.

00:20:55.060 --> 00:20:59.570
So this is in a sense is a better
theorem than the law of large numbers,

00:20:59.570 --> 00:21:03.550
but because it's kind of more
informative to know the distribution,

00:21:03.550 --> 00:21:07.000
know something about the rate, and
you know it's interesting that it's,

00:21:07.000 --> 00:21:11.340
square root of n is kind of the power
of n that's just right, right?

00:21:11.340 --> 00:21:14.600
A larger power it's gonna blow up,
a smaller power it's gonna go to 0.

00:21:15.880 --> 00:21:20.410
N to the one-half is the compromise,
then you always get a normal distribution.

00:21:20.410 --> 00:21:22.627
It's more informative in some sense, but

00:21:22.627 --> 00:21:27.070
you should also keep in mind,
it is a different sense of convergence.

00:21:27.070 --> 00:21:32.160
Up here, we're talking about the random
variables actually converging,

00:21:32.160 --> 00:21:36.692
literally the random variables
converge the sample mean converges

00:21:36.692 --> 00:21:41.163
literally to point-wise with
probability 1, to the true mean.

00:21:41.163 --> 00:21:43.787
Here, we're talking about
convergence in distribution.

00:21:43.787 --> 00:21:47.350
So we're not talking about
convergence of random variables.

00:21:47.350 --> 00:21:52.604
We're just saying the distribution of this
converges to the normal 0, 1 distribution.

00:21:52.604 --> 00:21:57.704
So that's a different sense
of convergence, but anyway,

00:21:57.704 --> 00:22:04.570
both of them are telling us what's gonna
happen to Xn bar when n is large, okay?

00:22:04.570 --> 00:22:07.807
So well, let's prove this theorem.

00:22:07.807 --> 00:22:11.676
Here's another way to write this,
by the way,

00:22:11.676 --> 00:22:15.080
it's good to be familiar with both ways.

00:22:15.080 --> 00:22:18.361
It's just algebra to go
from one to the other, but

00:22:18.361 --> 00:22:21.890
they're both useful enough
to be worth mentioning.

00:22:21.890 --> 00:22:26.497
Let's just write the central limit
theorem in terms of the sum of X's

00:22:26.497 --> 00:22:29.760
rather than in terms of the sample mean.

00:22:29.760 --> 00:22:34.444
So I'm just gonna take the sum of Xj,
j equals 1 to n.

00:22:34.444 --> 00:22:38.019
And so, we can either think of
the central limit theorem as,

00:22:38.019 --> 00:22:41.944
either think of it as telling us what
happens to the sample mean or we

00:22:41.944 --> 00:22:46.733
can think of it as telling us what happens
to the sum, or the convolution, okay?

00:22:46.733 --> 00:22:50.108
It's equivalent because
they're just a factor of,

00:22:50.108 --> 00:22:53.558
we just have to be careful not
to mess up the factor of n,

00:22:53.558 --> 00:22:57.386
b ut we can go from one to the other
cuz it's just a factor of n.

00:22:57.386 --> 00:23:02.980
So the claim is that this is
approximately normal when n is large,

00:23:02.980 --> 00:23:08.384
but if we just have this thing,
this could easily just blow up.

00:23:08.384 --> 00:23:10.510
You're just adding more and more terms.

00:23:10.510 --> 00:23:15.250
But somehow we wanna
standardize this first.

00:23:15.250 --> 00:23:20.104
So if we take this thing,
because this thing has mean and

00:23:20.104 --> 00:23:23.425
mu, right, so let's subtract n mu.

00:23:26.889 --> 00:23:30.042
Because then it has zero mean,
because I just want to match.

00:23:30.042 --> 00:23:32.773
I wanna make the mean 0 and
the variance 1, so

00:23:32.773 --> 00:23:37.128
that it kind of matches up with that,
rather than just letting it blow up.

00:23:37.128 --> 00:23:41.267
So this is called centering,
we just subtracted by linearity,

00:23:41.267 --> 00:23:43.961
the mean is n mu, so
just subtract it n mu.

00:23:43.961 --> 00:23:47.005
And then let's divide by
the standard deviation,

00:23:47.005 --> 00:23:50.060
this is just how we did
standard deviation before.

00:23:50.060 --> 00:23:57.534
So over there we showed that the variants
of Xn bar is sigma-squared over n.

00:23:57.534 --> 00:24:02.050
And the variance of this sum
is just n sigma squared.

00:24:02.050 --> 00:24:07.520
So let's just divide by
the standard deviation, right,

00:24:07.520 --> 00:24:12.517
which is square root of n Times sigma,
okay?

00:24:12.517 --> 00:24:15.110
Cuz the variance is n sigma squared.

00:24:15.110 --> 00:24:17.520
So that's just the standardized version.

00:24:17.520 --> 00:24:22.110
And the statement is again that this
converges to the standard normal

00:24:22.110 --> 00:24:23.500
in distribution.

00:24:23.500 --> 00:24:28.101
So if we take this sum and standardize it,
then it's gonna go standard normal.

00:24:33.722 --> 00:24:39.418
Okay, so, all right, so
now we're ready to prove this theorem.

00:24:41.797 --> 00:24:46.316
And, sort of just a calculation,
but it's kind of a nice

00:24:46.316 --> 00:24:50.940
calculation in some ways,
we're gonna prove it, well.

00:24:53.287 --> 00:24:57.276
This theorem is always true as
long as the variance exist.

00:24:57.276 --> 00:25:01.770
We don't need to assume that, the third
moment or the fourth moment exist.

00:25:01.770 --> 00:25:05.691
But the proof is much more complicated
to do it in that generality.

00:25:05.691 --> 00:25:11.231
So we're gonna assume that the MGF exists,
then we can actually work with the MGFs.

00:25:11.231 --> 00:25:15.806
Because when you see this thing,
sum of independent random variables,

00:25:15.806 --> 00:25:20.450
then we know the MGF is gonna be
something useful if it exists.

00:25:20.450 --> 00:25:23.870
And there's ways to extend this proof
to cases where the MGF doesn't exist.

00:25:23.870 --> 00:25:30.065
But for our purposes,
we may as well just assume MGF exists.

00:25:30.065 --> 00:25:34.812
So assuming MGF, let's call it M(t).

00:25:38.193 --> 00:25:44.760
Of Xj, they're iid, so if one of them
has an MGF, they all have the same MGF.

00:25:44.760 --> 00:25:46.563
We'll just assume that that exists.

00:25:54.744 --> 00:26:01.139
Once we have MGFs, then our strategy
is to show that the MGFs converge.

00:26:01.139 --> 00:26:07.557
So that's a theorem about MGFs, that
if the MGFs converge to some other MGF,

00:26:07.557 --> 00:26:12.980
then the random variables
converge in distribution, right?

00:26:12.980 --> 00:26:18.040
We had a homework problem related to that,
where you found that the MGFs converged

00:26:18.040 --> 00:26:22.678
to some MGF, and that implies
convergence of the distributions, right?

00:26:22.678 --> 00:26:24.490
Okay, so that's the whole strategy.

00:26:24.490 --> 00:26:28.470
So that means all we need to
do is find the MGF of this and

00:26:28.470 --> 00:26:30.603
then take the limit, okay?

00:26:30.603 --> 00:26:35.796
So basically at this point,
it's just like, write down the MGF,

00:26:35.796 --> 00:26:40.086
take the limit, and
use a few facts about MGFs, okay?

00:26:40.086 --> 00:26:46.091
So first of all, we can assume.

00:26:50.208 --> 00:26:54.610
That, let's just assume mu = 0 and

00:26:54.610 --> 00:27:00.015
sigma = 1, just to simplify the notation.

00:27:00.015 --> 00:27:04.000
This is without loss of generality,

00:27:04.000 --> 00:27:10.689
because we could write this as,
all we have to do is consider.

00:27:10.689 --> 00:27:14.049
I wrote the standardized thing this way,
but

00:27:14.049 --> 00:27:19.170
I could've just written it as
standardizing each X separately.

00:27:19.170 --> 00:27:24.835
I could've written Xj- mu over sigma.

00:27:24.835 --> 00:27:29.140
So this would be standardizing each
of them separately, j = 1 to n, and

00:27:29.140 --> 00:27:30.848
then we have a 1 over root n.

00:27:34.304 --> 00:27:36.530
That will be the same thing
that we're looking at.

00:27:36.530 --> 00:27:39.630
This just says standardize
them separately first.

00:27:39.630 --> 00:27:43.394
But then you could just, I mean if
you want, just call this thing Yj.

00:27:43.394 --> 00:27:47.980
And once you have the central limit term
for Yj, then you know that that's true.

00:27:47.980 --> 00:27:50.450
So you might as well just assume that
they've already been standardized.

00:27:51.660 --> 00:27:57.340
And so just to have some notation,
let's just let Sn equal the sum,

00:27:57.340 --> 00:28:00.350
S for sum, of the first n terms.

00:28:00.350 --> 00:28:03.080
And what we wanna show is that the MGF

00:28:04.530 --> 00:28:08.975
of Sn over root n,
that's what we're looking at, right?

00:28:08.975 --> 00:28:12.395
That let mu equal zero, sigma equals one,
so we're looking at Sn over root n.

00:28:12.395 --> 00:28:17.942
And we wanna show that that goes
to the standard normal MGF.

00:28:22.166 --> 00:28:25.390
Right, so we just need to find this MGF,
take a limit.

00:28:27.060 --> 00:28:30.977
Okay, so let's just find the MGF.

00:28:30.977 --> 00:28:38.060
So by definition, that's the expected
value of e to the t times Sn over root n.

00:28:42.177 --> 00:28:44.993
And Sn is just the sum.

00:28:44.993 --> 00:28:50.205
So, and we're assuming independence,
which means that these, you can

00:28:50.205 --> 00:28:56.188
write this as e to the t x1 over root n, e
to the t x2 over root n, blah, blah, blah.

00:28:56.188 --> 00:29:02.350
All of those factors are independent,
therefore, they're uncorrelated.

00:29:02.350 --> 00:29:07.389
So we can just split it up as a product,
X1/ over root n.

00:29:09.814 --> 00:29:13.125
Blah, blah, blah, same thing,

00:29:13.125 --> 00:29:18.390
just e to the Xj over root n
is the general term, right?

00:29:18.390 --> 00:29:23.346
I'm just using the fact that
those are uncorrelated, so

00:29:23.346 --> 00:29:28.590
we can write e of the product
of the expectations.

00:29:28.590 --> 00:29:30.480
But since these X's are iid,

00:29:30.480 --> 00:29:33.720
these are really just the same
thing written, n times.

00:29:33.720 --> 00:29:40.340
So really,
this is just this thing to the nth power.

00:29:40.340 --> 00:29:44.520
And this thing,
that should remind you of an MGF, right?

00:29:44.520 --> 00:29:46.410
That's just the MGF of X1,

00:29:46.410 --> 00:29:50.140
except that instead of evaluated at t,
it's evaluated at t over root n.

00:29:51.300 --> 00:29:54.453
So really, that's just the MGF,

00:29:54.453 --> 00:29:59.033
evaluated at t over root n
raised to the nth power.

00:30:00.853 --> 00:30:02.297
So that's what we have.

00:30:04.516 --> 00:30:06.925
Now we need to take the limit
as n goes to infinity.

00:30:06.925 --> 00:30:10.820
So let's just look at what's gonna
happen here, n is going to infinity.

00:30:11.830 --> 00:30:16.048
This thing on the inside becomes M of 0.

00:30:16.048 --> 00:30:19.950
M of 0 is 1 for any MGF, right?

00:30:19.950 --> 00:30:21.650
Cuz e to the 0 is 1.

00:30:21.650 --> 00:30:28.840
So this is of the form 1 to the infinity
which is in indeterminate form, right?

00:30:28.840 --> 00:30:31.670
It could evaluate to anything.

00:30:31.670 --> 00:30:35.620
So going back to calculus,
how do you deal with 1 to the infinity,

00:30:35.620 --> 00:30:37.230
or 0 over 0, or whatever.

00:30:37.230 --> 00:30:41.156
Usually we try to reduce it to something
where we can use L'Hopital's Rule for

00:30:41.156 --> 00:30:42.473
those problems, right?

00:30:42.473 --> 00:30:44.390
Or we can use a Taylor
series type of thing.

00:30:45.980 --> 00:30:48.621
So, how do we get into that form?

00:30:51.640 --> 00:30:56.240
Take the log,
because this looks like 1 to infinity.

00:30:56.240 --> 00:31:00.725
If we take the log,
it'll look like infinity times log of 1.

00:31:00.725 --> 00:31:04.710
So it'll look like infinity times 0,
take logs.

00:31:04.710 --> 00:31:10.110
Then we just have to remember to
exponentiate at the end to undo the log.

00:31:10.110 --> 00:31:13.581
Okay, so
let's write down then what we have.

00:31:18.477 --> 00:31:22.130
After taking the log, and
we're trying to do a limit, so

00:31:22.130 --> 00:31:26.331
we're doing the limit as n goes
to infinity, and we take the log.

00:31:26.331 --> 00:31:31.401
It's n log M(t

00:31:31.401 --> 00:31:36.472
over root n).

00:31:36.472 --> 00:31:41.208
So that's of the form infinity times 0.

00:31:41.208 --> 00:31:44.568
If we want 0 over 0 or
infinity over infinity,

00:31:44.568 --> 00:31:48.187
we can just write it as 1
over n in the denominator.

00:31:54.437 --> 00:31:57.140
Okay, and now it's of the form 0 over 0.

00:31:57.140 --> 00:32:00.812
So we can almost use L'Hopital's Rule,
but not quite.

00:32:00.812 --> 00:32:02.007
We have to be a little bit careful.

00:32:02.007 --> 00:32:05.717
Because first of all,
I'm assuming n is an integer,

00:32:05.717 --> 00:32:08.400
and you can't do calculus on integers.

00:32:10.000 --> 00:32:14.610
Secondly, it's just kind of, even if we
pretended that n is a real number and

00:32:14.610 --> 00:32:18.600
then the derivative of n would
be- 1 over n squared and

00:32:18.600 --> 00:32:20.570
that's kind of annoying to deal with.

00:32:20.570 --> 00:32:23.290
And it's kind of annoying to
deal with this square root here.

00:32:23.290 --> 00:32:25.250
So let's first make a change of variables.

00:32:26.970 --> 00:32:33.963
Let's just let y = 1 over root n and
also let y be real, not necessarily,

00:32:37.524 --> 00:32:42.356
Not necessarily of the form 1 over
square root of an integer, okay?

00:32:42.356 --> 00:32:46.980
So it's the same limit, just written
in terms of y instead of in terms of n.

00:32:48.290 --> 00:32:54.212
So as n goes to infinity y goes to 0 and
1 over n is y squared,

00:32:54.212 --> 00:32:58.246
so it's denominator is just y squared.

00:32:58.246 --> 00:33:02.856
The reason I do it this way is
that 1 over root n is just y

00:33:02.856 --> 00:33:07.689
by definition but
then the numerator is just log m of yt.

00:33:07.689 --> 00:33:11.109
That's a lot easier to deal with
because we got rid of the square roots.

00:33:13.540 --> 00:33:16.920
So it's still of the form 0 over 0.

00:33:16.920 --> 00:33:21.582
So we're gonna use L'Hospital's Rule.

00:33:21.582 --> 00:33:23.457
So limit, y goes to 0.

00:33:23.457 --> 00:33:28.262
Take the derivative of the numerator and
the denominator separately.

00:33:28.262 --> 00:33:31.834
The derivative of the denominator is 2y.

00:33:31.834 --> 00:33:32.907
The derivative of the numerator,

00:33:32.907 --> 00:33:35.020
well we're just going to
have to use the chain rule.

00:33:35.020 --> 00:33:39.740
Derivative of log something
is 1 over that thing.

00:33:39.740 --> 00:33:44.750
So that's M of yt hence the derivative
of that thing which again

00:33:44.750 --> 00:33:50.610
by the chain rule is M prime of
yt times the derivative of yt.

00:33:50.610 --> 00:33:55.540
We're treating t as constant,
we're differentiating with respect to y.

00:33:55.540 --> 00:33:56.840
So t comes out.

00:34:00.178 --> 00:34:02.880
And now let's see what we have.

00:34:02.880 --> 00:34:06.850
Let's just summarize
a couple facts about MGFs.

00:34:08.110 --> 00:34:14.470
So M of t is the expected
value of E to the tX1.

00:34:14.470 --> 00:34:22.520
So M of 0 = 1 Okay.

00:34:22.520 --> 00:34:26.510
And when we first started doing MGF we
said that we take derivatives of the MGF

00:34:26.510 --> 00:34:28.200
and evaluate it at 0.

00:34:28.200 --> 00:34:31.120
We get the moments, that is why it's
called the moment generating function.

00:34:31.120 --> 00:34:36.380
So the first derivative at 0 is the mean,
but we assume that mu is 0.

00:34:36.380 --> 00:34:38.524
So this is 0, here.

00:34:38.524 --> 00:34:42.740
And the second derivative,
while we're doing this.

00:34:42.740 --> 00:34:46.295
Secondary derivative is the second moment,
but since we assumed that the variance is

00:34:46.295 --> 00:34:51.240
1 and the mean is 0,
the second moment is 1, okay?

00:34:51.240 --> 00:34:56.720
So over here, as we let y go to 0,
denominator's still going to 0.

00:34:56.720 --> 00:35:01.750
Numerator's also going to 0,
because M prime of 0 is 0,

00:35:01.750 --> 00:35:06.560
so its still on the form 0 over 0, so
let's just do what we were told again.

00:35:08.980 --> 00:35:13.430
So first I can simplify it a little bit,
this t can come out,

00:35:13.430 --> 00:35:17.658
because that's acting as a constant,
and the 2 can come out.

00:35:17.658 --> 00:35:24.067
And limit y goes to 0 and
this M of yt part,

00:35:24.067 --> 00:35:28.900
that's just going to 1.

00:35:28.900 --> 00:35:31.910
So we can write that as part
of a separate limit, but

00:35:31.910 --> 00:35:34.087
that other limit is just going to 1.

00:35:34.087 --> 00:35:36.614
You can think of it as just
the limit of this part times

00:35:36.614 --> 00:35:37.944
the limit of the rest of it.

00:35:37.944 --> 00:35:41.991
But that part's just going to 1,
so we can get rid of that.

00:35:41.991 --> 00:35:47.905
So really is just, what's left is just

00:35:47.905 --> 00:35:53.289
the limit of M prime yt divided by y.

00:35:53.289 --> 00:35:59.149
Everything else is gone, so
it's actually pretty nicely simplified.

00:35:59.149 --> 00:36:02.967
Now, using L'Hospital's Rule
a second time,

00:36:02.967 --> 00:36:07.446
now the derivative of
the denominator is just 1, okay?

00:36:07.446 --> 00:36:12.136
And for the numerator,
chain rule, M double prime of yt.

00:36:16.067 --> 00:36:19.737
That was a t not a t squared,
but now it's a t squared,

00:36:19.737 --> 00:36:25.221
because by the chain rule, derivative of
yt is t, so we have a t squared over 2.

00:36:25.221 --> 00:36:29.601
Now when we let y go to 0,
now it's just M double prime 0 is 1, so

00:36:29.601 --> 00:36:31.378
now this limit is just 1.

00:36:31.378 --> 00:36:35.555
So we get t squared over 2,

00:36:35.555 --> 00:36:39.371
that's what we wanted,

00:36:39.371 --> 00:36:45.017
because t squared over 2 is the log.

00:36:45.017 --> 00:36:48.678
Of e to the t squared over 2, but

00:36:48.678 --> 00:36:55.199
e to the t square over 2 is
exactly the normal 0,1 MGF.

00:37:02.523 --> 00:37:03.131
Okay so,

00:37:03.131 --> 00:37:08.234
to prove that theorem that's the end of
the proof of the central limit theorem.

00:37:08.234 --> 00:37:14.467
All we had to do was just basic facts
out MGF, use, L'Hospital's Rule twice.

00:37:14.467 --> 00:37:20.207
And there we have one of the most famous
important theorems in statistics.

00:37:20.207 --> 00:37:23.796
Now so
there are more general versions of this,

00:37:23.796 --> 00:37:28.552
like you can extend this in various
ways where it's not an IID,

00:37:28.552 --> 00:37:32.699
but it still has to satisfy
some assumptions, right.

00:37:32.699 --> 00:37:36.910
But anyway,
this is the basic central limit theorem.

00:37:36.910 --> 00:37:40.007
Okay, so that's pretty good.

00:37:40.007 --> 00:37:45.340
Let's do an example,
like how do we actually use this,

00:37:45.340 --> 00:37:50.341
for the sake of approximations,
things like that.

00:37:50.341 --> 00:37:53.414
Last time I was talking about
the difference between inequalities and

00:37:53.414 --> 00:37:54.621
approximations, right?

00:37:54.621 --> 00:37:57.218
And we talked about Poisson
approximation before.

00:37:57.218 --> 00:37:59.760
We haven't really talked
about normal approximation.

00:38:01.270 --> 00:38:06.756
This result is giving us the ability
to use normal approximations

00:38:06.756 --> 00:38:11.354
when we're studying sample mean and
is large, okay?

00:38:11.354 --> 00:38:16.326
So historically, though,
the first version of

00:38:16.326 --> 00:38:21.299
the central limit theorem
that was ever proven,

00:38:21.299 --> 00:38:24.830
I think was for binomials, okay?

00:38:24.830 --> 00:38:28.900
So what we're saying is that

00:38:28.900 --> 00:38:33.380
binomial np under some conditions
will be approximately normal.

00:38:33.380 --> 00:38:38.400
And well in the old days that was
incredibly important fact because

00:38:38.400 --> 00:38:42.840
they didn't have computers to
binomials how to deal with

00:38:42.840 --> 00:38:47.180
like n choose k, and n is large, and
k, you have all these factorials.

00:38:47.180 --> 00:38:49.182
You can't do these things by hand.

00:38:49.182 --> 00:38:53.150
Now we have fast computers,
so it's a little bit better.

00:38:53.150 --> 00:38:57.100
But it's still a lot easier working
with normal distributions than

00:38:57.100 --> 00:39:00.080
binomial distributions most of the time,
right?

00:39:00.080 --> 00:39:05.020
And even now factorials still grow so
fast that even with

00:39:05.020 --> 00:39:09.320
a fast computer with large memory and
everything, you may quickly

00:39:09.320 --> 00:39:13.760
exceed its ability when you're doing
some big complicated binomial problem.

00:39:13.760 --> 00:39:18.040
And normals have a lot of nice properties,
as we've seen, okay?

00:39:18.040 --> 00:39:24.132
The question is,
when can we approximate a binomial

00:39:24.132 --> 00:39:29.397
using a normal, and
how do we do that, okay?

00:39:29.397 --> 00:39:34.466
So this is just the binomial approximation

00:39:34.466 --> 00:39:38.557
to the normal, other way around.

00:39:38.557 --> 00:39:41.338
Normal approximation,
I'll say binomial approximated by normal,

00:39:41.338 --> 00:39:43.090
the normal approximation to the binomial.

00:39:48.151 --> 00:39:48.990
When is that valid?

00:39:52.320 --> 00:39:55.320
To contrast it with
the Poisson approximation,

00:39:55.320 --> 00:39:58.270
that we've seen before, okay?

00:39:58.270 --> 00:40:03.065
So, if x is, let's x be binomial np

00:40:05.060 --> 00:40:10.070
And as we've done many times before

00:40:10.070 --> 00:40:15.680
we can represent x as
a sum of iid Bernoulli.

00:40:18.116 --> 00:40:23.284
Right?
Well these are just 1, if success on the J

00:40:23.284 --> 00:40:29.232
trials 0 otherwise, so
the XJ are iid Bernoulli P.

00:40:33.460 --> 00:40:37.096
So this does fit into
the framework of the central limit

00:40:37.096 --> 00:40:40.742
theorem that is we are adding
up iid random variables.

00:40:40.742 --> 00:40:45.603
So the central limit theorem says that,
if the N is large this will be

00:40:45.603 --> 00:40:50.828
approximately normal, at least after
we have standardized it, okay?

00:40:50.828 --> 00:40:55.944
So suppose we wanted to approximate,
suppose we're

00:40:55.944 --> 00:41:01.300
interested in the probability
that x is between A and B.

00:41:04.996 --> 00:41:07.061
And I want to approximate that,

00:41:07.061 --> 00:41:10.738
first we'll do equality then
we're approximating it.

00:41:10.738 --> 00:41:15.695
So, I mean if you had to do this on
a computer what you would do or by hand,

00:41:15.695 --> 00:41:19.580
which you wouldn't want to,
would be to take the PMF and

00:41:19.580 --> 00:41:23.073
sum up all the values of
the PMF from A to B, right.

00:41:23.073 --> 00:41:28.220
So okay, you would not want to do
that by hand most of the time.

00:41:28.220 --> 00:41:32.338
But suppose we just want an approximation
for this, not the exact thing.

00:41:32.338 --> 00:41:38.627
So first, the strategy is just gonna
be to take x and standardize it first.

00:41:38.627 --> 00:41:44.130
So we're gonna subtract the mean,
so we know that the mean is NP,

00:41:44.130 --> 00:41:48.535
and we're gonna divide by
the standard deviation,

00:41:48.535 --> 00:41:53.156
which we know as the square root of NPQ or
Q is 1 minus P.

00:41:53.156 --> 00:41:55.320
So, I'm just standardizing it right now.

00:41:55.320 --> 00:41:58.253
So this is still equal,
we haven't done any approximations yet.

00:42:02.622 --> 00:42:05.559
And then, now that we've standardized it,

00:42:05.559 --> 00:42:10.135
we can apply the central limit theorem,
if N is large enough, right?

00:42:10.135 --> 00:42:12.932
If N is, if central limit
theorem said N goes to infinity,

00:42:12.932 --> 00:42:16.030
that doesn't answer the question
of how large does N have to be.

00:42:16.030 --> 00:42:20.190
And for that, there's various theorems and
various rules of thumb.

00:42:20.190 --> 00:42:23.597
A lot of books will say,
how large does N have to be?

00:42:23.597 --> 00:42:30.391
And some books at least will say 30,
and that's just a rule of thumb.

00:42:30.391 --> 00:42:36.322
That's not always gonna work for all,
there's separate rules of thumb for

00:42:36.322 --> 00:42:41.232
the binomial, like you want N
times P to be reasonably large and

00:42:41.232 --> 00:42:46.780
N times 1 minus P to be large,
there are different rules of thumb.

00:42:46.780 --> 00:42:49.149
But anyway, if N is large enough,

00:42:49.149 --> 00:42:53.563
then what we've just proven is that
this is gonna look like it has

00:42:53.563 --> 00:42:57.673
a normal distribution because
that's a sum of IID things.

00:42:57.673 --> 00:43:02.237
And we standardized it correctly, because
we already knew the mean and the variance,

00:43:02.237 --> 00:43:03.748
so we just standardized it.

00:43:03.748 --> 00:43:05.794
Okay, so this is approximately.

00:43:08.166 --> 00:43:10.388
Now we're going to use
the normal approximation,

00:43:10.388 --> 00:43:13.280
we're going to say this
is approximately normal.

00:43:13.280 --> 00:43:17.898
And if I want the probability that
the normal is between something and

00:43:17.898 --> 00:43:22.368
something, that's just the CDF
here minus the CDF here, right?

00:43:22.368 --> 00:43:27.748
Because for the normal, I mean this
is discrete but we're approximating

00:43:27.748 --> 00:43:34.170
using something continuous and we just
say, integrate the PDF from here to here.

00:43:34.170 --> 00:43:37.922
But fundamental theorem calculus,
that just says take the CDF and go, okay.

00:43:37.922 --> 00:43:43.142
So we're just gonna do Phi of B minus

00:43:43.142 --> 00:43:48.188
NP over square root of NPQ minus Phi

00:43:48.188 --> 00:43:53.700
of A minus NP over square root of NPQ.

00:43:53.700 --> 00:43:57.362
So that would be the basic
normal approximation,

00:43:57.362 --> 00:44:02.171
I'll talk a little bit about how
to improve this approximation.

00:44:02.171 --> 00:44:06.620
But to contrast it with
the Poisson approximation.

00:44:11.483 --> 00:44:16.490
We talked before about the fact that,
and we proved the fact

00:44:16.490 --> 00:44:22.520
that if N goes to infinity, and
P goes to 0, and N times P is fixed.

00:44:22.520 --> 00:44:26.790
Then the binomial distribution
converts to the Poisson distribution,

00:44:26.790 --> 00:44:28.196
we proved that before.

00:44:28.196 --> 00:44:31.570
So in the Poisson approximation, so for

00:44:31.570 --> 00:44:38.329
the Poisson approximation what we had was
N is large but P was very small, right?

00:44:38.329 --> 00:44:42.540
And we let lambda equal NP and
x as moderate.

00:44:46.084 --> 00:44:51.340
And most important thing is that
P is small here, P is close to 0.

00:44:51.340 --> 00:44:55.911
We proved it in the case where this goes
to infinity and this goes to 0, okay?

00:44:55.911 --> 00:45:00.801
So Poisson is relevant when we're
dealing with a large number of very

00:45:00.801 --> 00:45:02.880
rare unlikely things.

00:45:02.880 --> 00:45:06.497
That's really in contrast to this,

00:45:06.497 --> 00:45:10.823
in this case for the normal approximation.

00:45:10.823 --> 00:45:14.196
Then, while we still want N to be large,
but

00:45:14.196 --> 00:45:19.396
if you kind of think intuitively
about when is this gonna work well,

00:45:19.396 --> 00:45:22.613
we actually want P to
be close to one half.

00:45:25.131 --> 00:45:30.506
Because think about the symmetry, if you
have a binomial of P equals one half,

00:45:30.506 --> 00:45:33.510
that's a symmetric distribution.

00:45:33.510 --> 00:45:39.035
The normal is symmetric, no matter,
every normal distribution is symmetric.

00:45:39.035 --> 00:45:44.965
If P is far from one half, then the
binomial is very, very skewed, and in that

00:45:44.965 --> 00:45:51.110
case it's kind of doesn't make that much
sense to approximate using a normal.

00:45:52.780 --> 00:45:58.829
So this is gonna work as an approximation,
that's normal approximation,

00:45:58.829 --> 00:46:04.981
as an approximation if P is very small,
this makes a lot more sense than this.

00:46:04.981 --> 00:46:08.912
However, think about the statement
of the central limit theorem.

00:46:08.912 --> 00:46:12.112
In that theorem I never said
P was close to one half,

00:46:12.112 --> 00:46:17.025
in fact that was just a general theorem,
we didn't even have P in the statement

00:46:17.025 --> 00:46:22.470
of the central limit theorem, but
somehow this still has to eventually work.

00:46:22.470 --> 00:46:26.020
But as a practical matter
as an approximation,

00:46:26.020 --> 00:46:30.188
if P is close to one half this
is going to work quite well,

00:46:30.188 --> 00:46:34.204
if N is like 30 or 50 or
100, it will work fine.

00:46:34.204 --> 00:46:38.449
But if P is .001,
the central limit theorem is still true,

00:46:38.449 --> 00:46:41.930
that as N goes to infinity
it's gonna work, okay.

00:46:41.930 --> 00:46:46.090
But if N is kind of not
that enormous of a number,

00:46:46.090 --> 00:46:50.354
then it's gonna be a pretty
bad approximation.

00:46:50.354 --> 00:46:58.430
And let's just try to reconcile these
statements though, is there a case?

00:46:58.430 --> 00:47:02.247
If we let N go to infinity and
P be very small,

00:47:02.247 --> 00:47:05.754
I still said, if N is going to infinity,

00:47:05.754 --> 00:47:11.349
it's still gonna converge to
normal just much slower, right?

00:47:11.349 --> 00:47:18.180
So, how could the binomial
look both normal and Poisson?

00:47:18.180 --> 00:47:21.408
Well, the answer is that
the Poisson also looks normal.

00:47:21.408 --> 00:47:24.916
So if you've Poisson lambda
where lambda's very large,

00:47:24.916 --> 00:47:30.450
that's also gonna look normal, so
there is a case where those come together.

00:47:30.450 --> 00:47:35.468
Okay, one last thing about this
is that there is something kind

00:47:35.468 --> 00:47:40.107
of weird about this in the sense
that we're approximating

00:47:40.107 --> 00:47:44.572
a discrete distribution
using something continuous.

00:47:44.572 --> 00:47:47.285
And if we wanted to get,

00:47:47.285 --> 00:47:53.270
what if we wanted to just
approximate same problem?

00:47:53.270 --> 00:47:54.887
I just wanna add something to this.

00:47:54.887 --> 00:47:59.145
Well, let's just look at that just to see
what more of like what could go wrong

00:47:59.145 --> 00:47:59.809
with this.

00:47:59.809 --> 00:48:02.159
What if we look at the case A equals B?

00:48:02.159 --> 00:48:06.634
So then we're just saying
the probability that x equals A,

00:48:06.634 --> 00:48:10.060
that is approximate the Binomial PMF.

00:48:10.060 --> 00:48:14.547
And one kind of weird thing about this is,
this thing would change if

00:48:14.547 --> 00:48:18.728
we changed these to strict inequality but
this part would not.

00:48:18.728 --> 00:48:22.237
As soon as we say that this is
approximately normal than we don't care

00:48:22.237 --> 00:48:24.030
about that anymore.

00:48:24.030 --> 00:48:27.642
So there's something called the continuity
correction which I just wanted to

00:48:27.642 --> 00:48:28.524
briefly mention.

00:48:28.524 --> 00:48:31.858
Which is an improvement to deal with
the fact that you're using something

00:48:31.858 --> 00:48:34.900
continuous to approximate
something discrete.

00:48:34.900 --> 00:48:39.755
And it's often not explained very well but
if you understand what

00:48:39.755 --> 00:48:44.534
it does in this simple case,
then it's not hard to see the idea.

00:48:44.534 --> 00:48:49.127
The idea is that if you just said this is
approximately normal then you would just

00:48:49.127 --> 00:48:50.670
say zero, right?

00:48:50.670 --> 00:48:53.800
Because it would be zero for continuous,
that's not very useful, right?

00:48:53.800 --> 00:48:56.278
We want something more useful than zero.

00:48:56.278 --> 00:49:00.327
So the idea is just
simply to write this as,

00:49:00.327 --> 00:49:05.389
here let's assume A is
an integer x is discreet well,

00:49:05.389 --> 00:49:10.337
x equals A is the same thing
as saying that x is between

00:49:10.337 --> 00:49:14.179
A plus one-half and A minus one-half.

00:49:17.740 --> 00:49:19.489
Right?

00:49:19.489 --> 00:49:21.359
So just use this first.

00:49:24.220 --> 00:49:26.368
So for each value in this range,

00:49:26.368 --> 00:49:30.029
replace it by an interval
of length 1 centered there,

00:49:30.029 --> 00:49:35.457
that's exactly the same thing because x
is an integer anyway, so that's true.

00:49:35.457 --> 00:49:40.188
But here at least we're giving it
an interval to work with instead of

00:49:40.188 --> 00:49:44.261
just saying zero, so
that improves this approximation.

00:49:44.261 --> 00:49:45.790
Anyway, it's just central limit theorem.

00:49:45.790 --> 00:49:47.370
All right, so see you next time.

