WEBVTT
Kind: captions
Language: en

00:00:11.316 --> 00:00:12.990
JENNY MURPHY: --a question and
answer show, so if any of you

00:00:12.990 --> 00:00:15.055
out there would like
to ask questions.

00:00:20.850 --> 00:00:22.990
BILL LUAN: Shanghai
GDG is a very

00:00:22.990 --> 00:00:24.780
interesting developer community.

00:00:24.780 --> 00:00:25.910
SUSANNAH RAUB: I'm
glad somebody

00:00:25.910 --> 00:00:26.930
has asked this question.

00:00:26.930 --> 00:00:28.412
RETO MEIER: This is where
the magic happens.

00:00:28.412 --> 00:00:29.585
JENNY MURPHY: This is primarily
a question and

00:00:29.585 --> 00:00:31.470
answer show, so if any
of you out there

00:00:31.470 --> 00:00:32.720
would like to ask questions.

00:00:39.780 --> 00:00:41.320
RYAN BOYD: Hello, everyone.

00:00:41.320 --> 00:00:42.490
My name is Ryan Boyd.

00:00:42.490 --> 00:00:44.090
MICHAEL MANOOCHEHRI: And I'm
Michael Manoochehri.

00:00:44.090 --> 00:00:47.320
RYAN BOYD: And today, we're here
live from Campus London

00:00:47.320 --> 00:00:50.950
in London, the United Kingdom,
away from our home base of San

00:00:50.950 --> 00:00:54.720
Francisco to talk with you about
what's been exciting

00:00:54.720 --> 00:00:58.150
this week in the land
of BigQuery.

00:00:58.150 --> 00:01:00.990
So this week, we've actually
done a lot of things.

00:01:00.990 --> 00:01:05.080
We've been out amongst the
community talking with the

00:01:05.080 --> 00:01:07.950
community and gone to the Strata
Conference and some

00:01:07.950 --> 00:01:10.390
other events.

00:01:10.390 --> 00:01:12.600
But the real important thing,
which we're here to talk with

00:01:12.600 --> 00:01:17.380
you about today, around the
world is the launch that

00:01:17.380 --> 00:01:18.490
happened this week.

00:01:18.490 --> 00:01:21.390
We had a major launch
for BigQuery.

00:01:21.390 --> 00:01:23.660
I think our largest launch,
probably, since we went

00:01:23.660 --> 00:01:25.730
general availability.

00:01:25.730 --> 00:01:29.780
And that was on JSON support,
nested and repeated support,

00:01:29.780 --> 00:01:32.780
and other improvements to
BigQuery import, which makes

00:01:32.780 --> 00:01:36.440
it a lot easier for developers
to use BigQuery.

00:01:36.440 --> 00:01:39.690
So I want to talk with you about
some of those things.

00:01:39.690 --> 00:01:43.030
And we're not going into the
presentation quite yet.

00:01:43.030 --> 00:01:45.160
We're just going to have a
little bit of discussion on

00:01:45.160 --> 00:01:47.750
what we've been up
to this week.

00:01:47.750 --> 00:01:51.580
And that started off this week
with the Strata Conference.

00:01:51.580 --> 00:01:55.660
So on Monday, we went to the
Strata London Conference.

00:01:55.660 --> 00:01:58.670
And this is one of the premiere
big data conferences

00:01:58.670 --> 00:02:00.020
run by O'Reilly.

00:02:00.020 --> 00:02:04.490
And I actually gave a talk
on BigQuery at Strata.

00:02:04.490 --> 00:02:06.600
And it went over really,
really well.

00:02:06.600 --> 00:02:09.149
I only think I had a short
20 minutes, I believe.

00:02:09.149 --> 00:02:11.050
MICHAEL MANOOCHEHRI: It was a
lot to pack in a short time.

00:02:11.050 --> 00:02:13.400
RYAN BOYD: It was a lot to
pack in a short time.

00:02:13.400 --> 00:02:16.110
But we announced a lot of these
new features at the

00:02:16.110 --> 00:02:19.960
Strata Conference a few hours
before they went live on our

00:02:19.960 --> 00:02:22.920
blogs, based out of
Mountain View.

00:02:22.920 --> 00:02:25.420
And it was really exciting
talking with all the

00:02:25.420 --> 00:02:26.510
developers.

00:02:26.510 --> 00:02:29.980
I heard afterwards the audience
was abuzz and

00:02:29.980 --> 00:02:33.240
chatting amongst themselves
about my query of one terabyte

00:02:33.240 --> 00:02:36.990
of data, running regular
expressions on 13 billion rows

00:02:36.990 --> 00:02:41.390
in, I think, 23 seconds is what
it took during that talk.

00:02:41.390 --> 00:02:44.090
And so there was a lot of
buzz and energy around

00:02:44.090 --> 00:02:46.000
BigQuery as a product.

00:02:46.000 --> 00:02:48.400
And it was super exciting
to talk with all those

00:02:48.400 --> 00:02:50.260
developers.

00:02:50.260 --> 00:02:51.670
We also did an Office Hours.

00:02:51.670 --> 00:02:55.230
And a number of people stopped
by with various use cases,

00:02:55.230 --> 00:02:59.520
from travel agencies to
marketing analytics firms.

00:02:59.520 --> 00:03:02.550
A lot of people stopping by and
chatting with us about how

00:03:02.550 --> 00:03:04.780
they are currently using
BigQuery, or how they're

00:03:04.780 --> 00:03:06.180
planning on using BigQuery.

00:03:06.180 --> 00:03:07.880
It was great to hear
this stuff.

00:03:07.880 --> 00:03:10.570
If you're not out at one of
these events with us, feel

00:03:10.570 --> 00:03:14.370
free to send that information
to us online on Google+.

00:03:14.370 --> 00:03:16.300
Let us know how you're
using BigQuery.

00:03:16.300 --> 00:03:20.400
It's just really fascinating
for us to hear all the

00:03:20.400 --> 00:03:23.880
different use cases that you
guys are using BigQuery for.

00:03:23.880 --> 00:03:26.320
And I'm trying to think what
else at Strata Conference.

00:03:26.320 --> 00:03:26.853
There was a number
of other talks--

00:03:26.853 --> 00:03:27.760
MICHAEL MANOOCHEHRI: Yeah,
we did the Data

00:03:27.760 --> 00:03:28.370
Science Meetup as well.

00:03:28.370 --> 00:03:28.920
RYAN BOYD: Yeah.

00:03:28.920 --> 00:03:32.540
We had the London big Data
Science Meetup where we got to

00:03:32.540 --> 00:03:36.560
talk about BigQuery
to their audience.

00:03:36.560 --> 00:03:40.430
That was a very short,
five-minute talk.

00:03:40.430 --> 00:03:43.260
We basically crammed our
50-minute talk into 20 minutes

00:03:43.260 --> 00:03:45.580
for the main Strata Conference,
and then moved it

00:03:45.580 --> 00:03:48.150
down to five minutes for the
Data Science Meetup.

00:03:48.150 --> 00:03:48.500
MICHAEL MANOOCHEHRI: Yeah.

00:03:48.500 --> 00:03:49.840
That was a very well-attended
talk.

00:03:49.840 --> 00:03:51.570
And we did say we'd be
back sometime to

00:03:51.570 --> 00:03:52.300
talk to them again.

00:03:52.300 --> 00:03:52.680
RYAN BOYD: Yeah, exactly.

00:03:52.680 --> 00:03:52.996
MICHAEL MANOOCHEHRI: It
was a huge Meetup.

00:03:52.996 --> 00:03:55.760
RYAN BOYD: We plan to keep on
coming out here to the UK to

00:03:55.760 --> 00:03:57.485
talk with developers.

00:03:57.485 --> 00:03:59.350
Do you want to talk, Michael,
about some of the other stuff

00:03:59.350 --> 00:04:00.175
outside of the conference?

00:04:00.175 --> 00:04:02.190
MICHAEL MANOOCHEHRI: So yeah,
we did a little tour of some

00:04:02.190 --> 00:04:03.310
of the incubators around.

00:04:03.310 --> 00:04:05.910
One was TechHub Manchester,
which is opening soon.

00:04:05.910 --> 00:04:08.090
And I got to meet with some of
the startups there, some of

00:04:08.090 --> 00:04:09.870
the people in Manchester who
are doing really good work

00:04:09.870 --> 00:04:11.530
around big data and
the big data area.

00:04:11.530 --> 00:04:13.970
I met with a business
intelligence startup, who was

00:04:13.970 --> 00:04:17.300
interested in using BigQuery
to have 100% cloud-based

00:04:17.300 --> 00:04:18.620
enterprise analytic software.

00:04:18.620 --> 00:04:19.950
This is a very small team.

00:04:19.950 --> 00:04:21.730
And it just goes to show what
you can do with BigQuery.

00:04:21.730 --> 00:04:23.920
It's so powerful, and how
fast you can build.

00:04:23.920 --> 00:04:26.450
RYAN BOYD: Their original plan
was just to use a relational

00:04:26.450 --> 00:04:28.130
database that they were
familiar with.

00:04:28.130 --> 00:04:28.700
MICHAEL MANOOCHEHRI: Right.

00:04:28.700 --> 00:04:30.610
RYAN BOYD: But then they
heard about BigQuery.

00:04:30.610 --> 00:04:34.770
And you could just see the
excitement in their face about

00:04:34.770 --> 00:04:36.850
the amount of data that could
be crunched with BigQuery.

00:04:36.850 --> 00:04:38.480
MICHAEL MANOOCHEHRI: We always
say it enables you to do

00:04:38.480 --> 00:04:39.520
things that you couldn't
do before.

00:04:39.520 --> 00:04:40.630
And I think that was the
theme of some of the

00:04:40.630 --> 00:04:41.970
startups that I visited.

00:04:41.970 --> 00:04:43.200
I also talked to several social

00:04:43.200 --> 00:04:44.930
media analytics startups.

00:04:44.930 --> 00:04:47.740
And these are also small two-,
three-person teams who are

00:04:47.740 --> 00:04:49.870
thinking about using BigQuery
to do some of the data

00:04:49.870 --> 00:04:52.050
crunching and data mining that
it takes to really get value

00:04:52.050 --> 00:04:53.300
out of social media.

00:04:53.300 --> 00:04:55.010
And we also talked
to some ISVs.

00:04:55.010 --> 00:04:58.600
We talked to a software vendor
nearby that was using BigQuery

00:04:58.600 --> 00:05:01.850
in an end-to-end pipeline,
using some of our partner

00:05:01.850 --> 00:05:05.030
tools such as Talon for ETL
and for transformation of

00:05:05.030 --> 00:05:08.320
data, and QlikView, which is
another one of our partners,

00:05:08.320 --> 00:05:10.670
for visualization of BigQuery
query results.

00:05:10.670 --> 00:05:12.210
It was pretty exciting to
see people putting these

00:05:12.210 --> 00:05:13.700
technologies together and
building these pipelines.

00:05:13.700 --> 00:05:14.460
RYAN BOYD: Yeah, absolutely.

00:05:14.460 --> 00:05:15.050
MICHAEL MANOOCHEHRI:
Cool stuff.

00:05:15.050 --> 00:05:17.660
RYAN BOYD: And we're actually
out here with a bunch of the

00:05:17.660 --> 00:05:21.090
other folks from our Cloud
Team, some folks from

00:05:21.090 --> 00:05:24.000
AppEngine and folks from
Compute Engine.

00:05:24.000 --> 00:05:26.900
And we've all talked to
a bunch of startups.

00:05:26.900 --> 00:05:31.220
I actually dived back into my
old world of working on Google

00:05:31.220 --> 00:05:33.650
Apps to talk with a number of
folks who are doing some

00:05:33.650 --> 00:05:35.380
innovative things around
Google Apps too.

00:05:35.380 --> 00:05:39.260
So it was all around a very
exciting event or series of

00:05:39.260 --> 00:05:41.520
events around the UK.

00:05:41.520 --> 00:05:43.860
And I think we should emphasize
that-- you said

00:05:43.860 --> 00:05:45.920
Manchester, but Manchester
is a little

00:05:45.920 --> 00:05:46.860
bit outside of London.

00:05:46.860 --> 00:05:50.150
But it's exciting to really
see the energy that's

00:05:50.150 --> 00:05:51.400
happening throughout the UK.

00:05:51.400 --> 00:05:51.650
MICHAEL MANOOCHEHRI: Yeah.

00:05:51.650 --> 00:05:53.820
RYAN BOYD: It's not just in
London, as we might have

00:05:53.820 --> 00:05:55.390
expected coming into this.

00:05:55.390 --> 00:05:55.950
MICHAEL MANOOCHEHRI: Yeah,
it's all over.

00:05:55.950 --> 00:05:57.360
I was really impressed with
all the startups and

00:05:57.360 --> 00:05:58.230
incubators I visited.

00:05:58.230 --> 00:05:58.950
I thought it was great.

00:05:58.950 --> 00:06:00.430
And I think there's a really
big scene around--

00:06:00.430 --> 00:06:02.640
not just big data-- but startups
in general in London,

00:06:02.640 --> 00:06:03.850
Manchester, all over the UK.

00:06:03.850 --> 00:06:04.620
It's pretty impressive.

00:06:04.620 --> 00:06:05.160
RYAN BOYD: Yeah.

00:06:05.160 --> 00:06:09.330
So hopefully, we'll actually
come out here more to London

00:06:09.330 --> 00:06:13.870
and the UK areas in general and
hear more of this and see

00:06:13.870 --> 00:06:16.010
more of this.

00:06:16.010 --> 00:06:19.520
The Data Science Meetup group
invited us to come back and

00:06:19.520 --> 00:06:21.140
give a fuller-length talk.

00:06:21.140 --> 00:06:23.070
And we'd love to do that.

00:06:23.070 --> 00:06:24.300
So really exciting.

00:06:24.300 --> 00:06:28.850
And we're headed off tonight
to Sydney, actually.

00:06:28.850 --> 00:06:33.020
We're headed out to Australia
to do a similar type of tour

00:06:33.020 --> 00:06:36.540
with a bunch of incubators
and startups.

00:06:36.540 --> 00:06:38.670
And we're excited about
that as well.

00:06:38.670 --> 00:06:42.530
And I'm trying to think of
what's all in our tour here.

00:06:42.530 --> 00:06:44.060
But we're meeting with Google
developer groups.

00:06:44.060 --> 00:06:45.990
We did a Google developer
group in London.

00:06:45.990 --> 00:06:47.440
I think there's eight events,
basically, we're

00:06:47.440 --> 00:06:49.330
packing into two weeks.

00:06:49.330 --> 00:06:52.360
So it's great to be out there
amongst the community hearing

00:06:52.360 --> 00:06:53.880
what everyone's up to.

00:06:53.880 --> 00:06:56.825
And actually, today's
my birthday too.

00:06:56.825 --> 00:06:57.705
MICHAEL MANOOCHEHRI: Happy
birthday, Ryan.

00:06:57.705 --> 00:06:58.000
[LAUGHS]

00:06:58.000 --> 00:06:59.470
RYAN BOYD: So I get
to actually go

00:06:59.470 --> 00:07:00.490
celebrate my birthday.

00:07:00.490 --> 00:07:00.720
MICHAEL MANOOCHEHRI: Woo!

00:07:00.720 --> 00:07:01.195
[LAUGHS]

00:07:01.195 --> 00:07:04.350
RYAN BOYD: We're headed
to Singapore.

00:07:04.350 --> 00:07:07.040
I think we have a six-hour
layover is Singapore.

00:07:07.040 --> 00:07:09.110
So we're going to get out and
celebrate my birthday.

00:07:09.110 --> 00:07:09.470
MICHAEL MANOOCHEHRI:
That's right.

00:07:09.470 --> 00:07:12.820
RYAN BOYD: If you want to come
hunt us down in Singapore and

00:07:12.820 --> 00:07:15.960
celebrate my birthday we
me, drop me a line.

00:07:15.960 --> 00:07:18.520
And I think we're going to a
crab restaurant or something.

00:07:18.520 --> 00:07:19.640
MICHAEL MANOOCHEHRI: That's for
the few hours we're there,

00:07:19.640 --> 00:07:20.260
before we get back
on the plane.

00:07:20.260 --> 00:07:20.955
RYAN BOYD: Exactly.

00:07:20.955 --> 00:07:21.730
MICHAEL MANOOCHEHRI:
Sounds good.

00:07:21.730 --> 00:07:24.780
RYAN BOYD: But all around,
having a good time, but also

00:07:24.780 --> 00:07:27.910
hearing a lot about what
developers are up to.

00:07:27.910 --> 00:07:30.590
So now let's talk about the
real business, instead of

00:07:30.590 --> 00:07:31.355
talking about a party.

00:07:31.355 --> 00:07:32.605
MICHAEL MANOOCHEHRI:
[INAUDIBLE]

00:07:44.480 --> 00:07:47.000
up to a terabyte of
data at once.

00:07:49.520 --> 00:07:52.020
So a terabyte of data in one
load job, which is an enormous

00:07:52.020 --> 00:07:53.280
amount of data.

00:07:53.280 --> 00:07:56.170
And also, individual files can
be up to 100 gigabytes per

00:07:56.170 --> 00:07:59.350
file, depending on what
type of file it is.

00:07:59.350 --> 00:08:01.240
So this is a lot of data.

00:08:01.240 --> 00:08:03.190
So Ryan, you were doing some
testing on this earlier.

00:08:03.190 --> 00:08:03.680
What was your experience?

00:08:03.680 --> 00:08:03.856
RYAN BOYD: Yeah.

00:08:03.856 --> 00:08:06.980
Actually, one of the big things
about the 100 gigabyte

00:08:06.980 --> 00:08:10.720
per file is that's actually
uncompressed files.

00:08:10.720 --> 00:08:14.310
So uncompressed files, we can
actually shard and do parallel

00:08:14.310 --> 00:08:17.190
ingestion on a lot better
than compressed files.

00:08:17.190 --> 00:08:19.790
Happy to chat with you about
the reasons behind that in

00:08:19.790 --> 00:08:21.790
another venue.

00:08:21.790 --> 00:08:24.780
So 100 gigabytes of uncompressed
files.

00:08:24.780 --> 00:08:27.740
And the real important thing is
that you're actually able

00:08:27.740 --> 00:08:30.630
to send all your files
in at once.

00:08:30.630 --> 00:08:33.220
And we just queue them
up for ingestion.

00:08:33.220 --> 00:08:36.010
And this makes it much
easier for a

00:08:36.010 --> 00:08:37.200
developer to code against.

00:08:37.200 --> 00:08:39.440
So previously, what you had is
you had to make sure there

00:08:39.440 --> 00:08:42.690
were only, I think, two
ingestion jobs per minute that

00:08:42.690 --> 00:08:45.520
you were sending in for
the entire project.

00:08:45.520 --> 00:08:49.210
And this meant you really had to
queue up your jobs and send

00:08:49.210 --> 00:08:52.170
them to us, do a little
sleeping, do a little

00:08:52.170 --> 00:08:53.160
[INAUDIBLE], and
queue them up.

00:08:53.160 --> 00:08:53.400
MICHAEL MANOOCHEHRI: Yeah.

00:08:53.400 --> 00:08:54.735
It put the responsibility
on the developer.

00:08:54.735 --> 00:08:56.444
But we're here to
help you out.

00:08:56.444 --> 00:08:58.630
RYAN BOYD: --to really make
this easier and put the

00:08:58.630 --> 00:09:01.830
responsibility on us to allow
you to ingest a lot of data

00:09:01.830 --> 00:09:02.850
really quickly.

00:09:02.850 --> 00:09:05.720
So I've been playing
around with this.

00:09:05.720 --> 00:09:09.650
And I've been able to ingest
a lot of data.

00:09:09.650 --> 00:09:13.660
So previously I had three
terabytes of data that I was

00:09:13.660 --> 00:09:15.390
trying to load in.

00:09:15.390 --> 00:09:18.950
And that three terabytes of
data, I think, at absolute

00:09:18.950 --> 00:09:22.055
optimum, was it 12 hours, I
think, or something like that.

00:09:22.055 --> 00:09:23.050
I believe it's 12 hours.

00:09:23.050 --> 00:09:23.900
MICHAEL MANOOCHEHRI: Many,
many hours to get

00:09:23.900 --> 00:09:24.380
that much data in.

00:09:24.380 --> 00:09:27.110
RYAN BOYD: Yeah, at absolute
optimum speed, 12 hours.

00:09:27.110 --> 00:09:29.400
And in reality, it took a little
bit longer, because I

00:09:29.400 --> 00:09:32.540
didn't shard it perfectly
and that sort of thing.

00:09:32.540 --> 00:09:34.850
But now I've actually taken--

00:09:34.850 --> 00:09:36.760
sorry, I missed the bullet
point here on that.

00:09:36.760 --> 00:09:40.030
But I've actually taken that
data, that three terabytes of

00:09:40.030 --> 00:09:44.190
data, and loaded it in in 36
minutes into BigQuery, because

00:09:44.190 --> 00:09:47.180
BigQuery doesn't actually
build any indexes.

00:09:47.180 --> 00:09:51.330
BigQuery embraces the concept
of a full table scan.

00:09:51.330 --> 00:09:54.200
There's actually some comments
from Strata Conference about

00:09:54.200 --> 00:09:58.290
how embracing the concept of a
full table scan gave some DBAs

00:09:58.290 --> 00:09:58.810
some heart palpitations.

00:09:58.810 --> 00:09:59.230
MICHAEL MANOOCHEHRI: [LAUGHS]

00:09:59.230 --> 00:09:59.650
Yeah.

00:09:59.650 --> 00:10:01.620
People gasped at the London
GDG when I said that too.

00:10:01.620 --> 00:10:02.080
What?

00:10:02.080 --> 00:10:03.550
RYAN BOYD: Yeah, exactly.

00:10:03.550 --> 00:10:06.870
So all you DBAs out there,
please, please don't get heart

00:10:06.870 --> 00:10:07.650
palpitations.

00:10:07.650 --> 00:10:10.350
But we embrace the concept
of full table scan.

00:10:10.350 --> 00:10:12.640
And so once you're able
to load this three

00:10:12.640 --> 00:10:14.430
terabytes of data in--

00:10:14.430 --> 00:10:16.950
and in my example,
in 36 minutes--

00:10:16.950 --> 00:10:19.220
you can actually immediately
start querying.

00:10:19.220 --> 00:10:22.720
And that's about a 20x
improvement over what it was

00:10:22.720 --> 00:10:25.170
before this week, in terms of
how quickly you can start

00:10:25.170 --> 00:10:25.910
querying your data.

00:10:25.910 --> 00:10:28.780
So really, really awesome
for developers.

00:10:28.780 --> 00:10:32.030
Check it out, if you have
a large amount of data.

00:10:32.030 --> 00:10:33.700
It makes it a lot easier.

00:10:33.700 --> 00:10:35.440
MICHAEL MANOOCHEHRI:
Good stuff.

00:10:35.440 --> 00:10:38.430
So let's talk about one of my
favorite subjects, getting

00:10:38.430 --> 00:10:41.145
AppEngine Datastore data
into BigQuery.

00:10:41.145 --> 00:10:43.890
RYAN BOYD: Before we dive into
this, I do want to mention

00:10:43.890 --> 00:10:46.920
that we do have a Google
Moderator.

00:10:46.920 --> 00:10:49.560
So for those of you that are
watching this in your Google+

00:10:49.560 --> 00:10:55.140
Stream, head over to
developers.google.com/live and

00:10:55.140 --> 00:10:56.290
click on this event.

00:10:56.290 --> 00:10:59.090
And you'll see embedded in that
page a Google Moderator

00:10:59.090 --> 00:11:01.710
that you can ask any questions
that you have as you're

00:11:01.710 --> 00:11:03.080
listening to this event.

00:11:03.080 --> 00:11:06.640
And we'll visit that Moderator
at the end of this event and

00:11:06.640 --> 00:11:09.370
just go through some of those
questions that you have.

00:11:09.370 --> 00:11:12.870
We're not doing a live hangout
here today, but we are

00:11:12.870 --> 00:11:15.680
answering a lot of the questions
after we have our

00:11:15.680 --> 00:11:17.290
little pontificating here.

00:11:17.290 --> 00:11:19.540
We're going to answer a lot of
those questions that you have,

00:11:19.540 --> 00:11:22.690
either about these features or
about BigQuery in general.

00:11:22.690 --> 00:11:23.540
Sorry for the interruption.

00:11:23.540 --> 00:11:24.540
MICHAEL MANOOCHEHRI: Oh,
yeah, no problem.

00:11:24.540 --> 00:11:24.970
So yeah.

00:11:24.970 --> 00:11:27.470
So one of my favorite subjects
is getting AppEngine Datastore

00:11:27.470 --> 00:11:30.420
data into BigQuery, combining
these two technologies to have

00:11:30.420 --> 00:11:33.190
a high-availability
datastore--

00:11:33.190 --> 00:11:34.140
RYAN BOYD: You're still
jet-lagged.

00:11:34.140 --> 00:11:35.340
MICHAEL MANOOCHEHRI: I'm
still jet-lagged--

00:11:35.340 --> 00:11:38.040
and putting that data into
Google BigQuery.

00:11:38.040 --> 00:11:40.170
If you've seen my Google I/O
talk about this, you know we

00:11:40.170 --> 00:11:41.990
also have some documentation
on our web site.

00:11:41.990 --> 00:11:45.030
But now it's even easier to get
AppEngine Datastore data

00:11:45.030 --> 00:11:45.700
into BigQuery.

00:11:45.700 --> 00:11:48.140
We've launched a new Trusted
Tester program of a new

00:11:48.140 --> 00:11:52.930
feature to put Datastore Admin
backup data into BigQuery.

00:11:52.930 --> 00:11:55.540
So let's just give you a
little look at that.

00:11:55.540 --> 00:11:58.280
If you enable the AppEngine
Datastore Admin function,

00:11:58.280 --> 00:12:01.180
there's a backup feature which
lets you dump Datastore data

00:12:01.180 --> 00:12:02.520
into Google Cloud Storage.

00:12:02.520 --> 00:12:05.140
And now you can kick off
BigQuery ingestion jobs on

00:12:05.140 --> 00:12:07.580
those Datastore data backups.

00:12:07.580 --> 00:12:09.000
This is a Trusted
Tester program.

00:12:09.000 --> 00:12:11.640
So we're actually getting people
to sign up now, if

00:12:11.640 --> 00:12:12.750
you're interested.

00:12:12.750 --> 00:12:14.590
Just go to this link
right here.

00:12:14.590 --> 00:12:19.490
It's a Google Short Link, qhts,
capital H, to get there.

00:12:19.490 --> 00:12:21.760
Or look for it on Ryan's recent
blog post on the Google

00:12:21.760 --> 00:12:22.460
Developers blog.

00:12:22.460 --> 00:12:24.220
There's a link to the Trusted
Tester program as well.

00:12:24.220 --> 00:12:26.900
We're really excited to see what
people do with this and

00:12:26.900 --> 00:12:27.610
how you feel about it.

00:12:27.610 --> 00:12:28.730
So try it out.

00:12:28.730 --> 00:12:32.240
RYAN BOYD: So this actually
shows how you can take data

00:12:32.240 --> 00:12:36.770
from a high-availability,
read-write datastore like

00:12:36.770 --> 00:12:40.010
AppEngine and bring it into
BigQuery for analysis.

00:12:40.010 --> 00:12:43.600
So the datastore is great, if
you need to maintain your

00:12:43.600 --> 00:12:48.810
point of record for your data,
high-availability rights, and

00:12:48.810 --> 00:12:51.960
quick lookups of your data,
when things match keys and

00:12:51.960 --> 00:12:56.110
indexes and that sort of thing,
but then load it into

00:12:56.110 --> 00:12:59.340
BigQuery for doing that
aggregate analysis that things

00:12:59.340 --> 00:13:01.600
like Datastore aren't really
as fast at doing.

00:13:01.600 --> 00:13:03.170
If you're trying to do an
aggregate analysis on

00:13:03.170 --> 00:13:05.590
Datastore, you might have to
use, say, AppEngine MapReduce

00:13:05.590 --> 00:13:07.780
or something like that to really
loop through all of

00:13:07.780 --> 00:13:11.540
your records and do
your analysis.

00:13:11.540 --> 00:13:16.510
And that same thing holds true
for other online databases

00:13:16.510 --> 00:13:20.050
like NoSQL-type style,
document-style databases.

00:13:20.050 --> 00:13:24.720
And we've actually talked with
some of those folks outside of

00:13:24.720 --> 00:13:27.670
Google at some of these events
at the AngelHack event that we

00:13:27.670 --> 00:13:28.770
were at a couple of weeks ago.

00:13:28.770 --> 00:13:33.620
We talked with the FireBase
folks about doing this style

00:13:33.620 --> 00:13:39.480
of, basically, get data out of
FireBase and into BigQuery for

00:13:39.480 --> 00:13:42.160
analysis in a similar way that
we're doing with AppEngine.

00:13:42.160 --> 00:13:42.600
MICHAEL MANOOCHEHRI: Right.

00:13:42.600 --> 00:13:45.750
RYAN BOYD: And it'll be
fantastic to have that level

00:13:45.750 --> 00:13:46.560
of integration.

00:13:46.560 --> 00:13:48.030
I believe, Michael, you were
actually doing some

00:13:48.030 --> 00:13:48.710
experiments too.

00:13:48.710 --> 00:13:49.030
MICHAEL MANOOCHEHRI: Yeah.

00:13:49.030 --> 00:13:49.690
RYAN BOYD: I think you were
planning on doing

00:13:49.690 --> 00:13:50.730
some MongoDB work.

00:13:50.730 --> 00:13:50.890
MICHAEL MANOOCHEHRI: Yeah.

00:13:50.890 --> 00:13:51.580
I was playing with MongoDB.

00:13:51.580 --> 00:13:53.650
I know a lot of web
devs use MongoDB.

00:13:53.650 --> 00:13:55.380
And I think it's a good synergy
between the two

00:13:55.380 --> 00:13:56.060
technologies, right?

00:13:56.060 --> 00:13:59.450
Use MongoDB for the web
interface back end, and stick

00:13:59.450 --> 00:14:00.070
that data into BigQuery.

00:14:00.070 --> 00:14:00.650
RYAN BOYD: Did you say the
word synergy, Michael?

00:14:00.650 --> 00:14:00.760
MICHAEL MANOOCHEHRI: What?

00:14:00.760 --> 00:14:01.520
Did I say synergy?

00:14:01.520 --> 00:14:01.980
No.

00:14:01.980 --> 00:14:02.420
Impossible.

00:14:02.420 --> 00:14:02.647
[LAUGHS]

00:14:02.647 --> 00:14:02.875
RYAN BOYD: Michael.

00:14:02.875 --> 00:14:03.330
Michael.

00:14:03.330 --> 00:14:05.605
[LAUGHTER]

00:14:05.605 --> 00:14:06.505
RYAN BOYD: Anyway--

00:14:06.505 --> 00:14:07.725
MICHAEL MANOOCHEHRI: We'll
synergize on this later.

00:14:07.725 --> 00:14:07.950
[LAUGHS]

00:14:07.950 --> 00:14:08.431
RYAN BOYD: Yeah.

00:14:08.431 --> 00:14:10.355
It sounds great, Michael.

00:14:10.355 --> 00:14:14.750
Anyway, the word "synergy"
seems to come up in some

00:14:14.750 --> 00:14:15.660
business meetings.

00:14:15.660 --> 00:14:18.880
And Michael and I, as engineers,
laugh at it now.

00:14:18.880 --> 00:14:19.356
MICHAEL MANOOCHEHRI: Exactly.

00:14:19.356 --> 00:14:21.740
[LAUGHTER]

00:14:21.740 --> 00:14:23.100
RYAN BOYD: Yeah, so it
should be fantastic.

00:14:23.100 --> 00:14:24.770
So we can do it with
AppEngine today.

00:14:24.770 --> 00:14:26.530
Sign up for the Trusted
Tester program.

00:14:26.530 --> 00:14:29.180
We're interested to know what
your real business use cases

00:14:29.180 --> 00:14:32.130
are, what types of value you're
looking to get out of

00:14:32.130 --> 00:14:35.330
the data that you currently
are storing in AppEngine.

00:14:35.330 --> 00:14:39.460
And then, later on, we'll
definitely be exploring some

00:14:39.460 --> 00:14:42.610
of these other NoSQL databases
and document stores and figure

00:14:42.610 --> 00:14:45.220
out how we get those integrated
into BigQuery.

00:14:45.220 --> 00:14:48.990
If you guys have ideas, or if
you have open source projects,

00:14:48.990 --> 00:14:52.100
definitely let us know what
you guys are building.

00:14:52.100 --> 00:14:54.220
There's been, actually, a number
of open source projects

00:14:54.220 --> 00:14:54.770
around BigQuery recently.

00:14:54.770 --> 00:14:55.110
MICHAEL MANOOCHEHRI: Right.

00:14:55.110 --> 00:14:56.620
Yeah, definitely.

00:14:56.620 --> 00:14:57.860
Like Ruby Libraries,
there are people

00:14:57.860 --> 00:15:01.280
doing logs into BigQuery.

00:15:01.280 --> 00:15:03.860
So a good place to follow this--
what we're doing, is

00:15:03.860 --> 00:15:07.310
Google Cloud Developers Plus
page, or follow us on our

00:15:07.310 --> 00:15:08.150
private pages as well.

00:15:08.150 --> 00:15:10.250
And we will share some of these
technologies with you.

00:15:10.250 --> 00:15:13.380
RYAN BOYD: And we're actually
thinking about an upcoming

00:15:13.380 --> 00:15:16.110
hangout here, upcoming Google
Developers Live session, to

00:15:16.110 --> 00:15:20.240
talk about a lot of the open
source efforts around BigQuery

00:15:20.240 --> 00:15:21.880
and what some other developers
are doing.

00:15:21.880 --> 00:15:24.530
And maybe we can even invite
some of those developers to

00:15:24.530 --> 00:15:26.350
the Google Developers Live.

00:15:26.350 --> 00:15:28.540
So look forward to that.

00:15:28.540 --> 00:15:33.650
And let us know, in the
meantime, what you're doing.

00:15:33.650 --> 00:15:35.080
So preparing your data.

00:15:35.080 --> 00:15:37.220
We just want to talk a
little bit about--

00:15:37.220 --> 00:15:39.980
we talked about JSON and
nested/repeated as one of the

00:15:39.980 --> 00:15:42.830
things that we launched, I
believe, on October 1, at the

00:15:42.830 --> 00:15:44.140
beginning of this week.

00:15:44.140 --> 00:15:46.930
And we just wanted to compare
the different styles of data.

00:15:46.930 --> 00:15:50.630
Sometimes it's helpful to see it
in a graphic form what the

00:15:50.630 --> 00:15:53.740
different styles of data looks
like and how you can work with

00:15:53.740 --> 00:15:54.580
it in BigQuery.

00:15:54.580 --> 00:15:58.970
So typically, a lot of our
developers are familiar with

00:15:58.970 --> 00:16:00.230
relational databases.

00:16:00.230 --> 00:16:03.740
So this is a relational database
schema where you have

00:16:03.740 --> 00:16:07.560
two separate tables and you've
done normalization--

00:16:07.560 --> 00:16:10.040
which are taught a lot in
college database classes--

00:16:10.040 --> 00:16:14.330
to basically separate out the
data so that, for instance, if

00:16:14.330 --> 00:16:17.840
you have people here, and each
person has maybe lived in

00:16:17.840 --> 00:16:21.530
multiple cities, that you
basically only have one

00:16:21.530 --> 00:16:23.610
definitive record per person.

00:16:23.610 --> 00:16:26.120
And then you have separate
records for the cities that

00:16:26.120 --> 00:16:26.950
they've lived in.

00:16:26.950 --> 00:16:30.150
And you link the two
up on a key.

00:16:30.150 --> 00:16:33.750
And that's really useful if
you are trying to maintain

00:16:33.750 --> 00:16:38.090
consistency in a database
environment, which is often a

00:16:38.090 --> 00:16:39.950
read-write database environment
and you want to

00:16:39.950 --> 00:16:42.550
make sure that you have
perfect consistency.

00:16:42.550 --> 00:16:45.850
But you don't necessarily need
that relational schema when

00:16:45.850 --> 00:16:46.490
you're trying to do

00:16:46.490 --> 00:16:48.230
large-scale aggregate analysis.

00:16:48.230 --> 00:16:51.360
And in fact, at times, it's a
lot faster to do large-scale

00:16:51.360 --> 00:16:53.020
aggregate analysis when
you don't have

00:16:53.020 --> 00:16:54.190
that relational schema.

00:16:54.190 --> 00:16:59.940
So BigQuery has always taught
you to do a denormalized form

00:16:59.940 --> 00:17:03.630
where you take and make every
single one of the rows.

00:17:03.630 --> 00:17:05.910
It's essentially a join.

00:17:05.910 --> 00:17:08.540
And you're doing a pre-join on
your data when you're loading

00:17:08.540 --> 00:17:10.270
it into BigQuery.

00:17:10.270 --> 00:17:14.990
And you could represent that
in a CSV format quite well.

00:17:14.990 --> 00:17:18.390
That denormalized form worked
well in a CSV format.

00:17:18.390 --> 00:17:21.099
And that's the primary data
format that BigQuery did

00:17:21.099 --> 00:17:24.270
support previously, and the
only format we supported

00:17:24.270 --> 00:17:24.859
previously.

00:17:24.859 --> 00:17:28.530
But we've now looked and said,
all right, well, what about

00:17:28.530 --> 00:17:30.300
JSON as a data format?

00:17:30.300 --> 00:17:34.210
And we can represent the data
that you would represent in a

00:17:34.210 --> 00:17:38.440
CSV in a JSON object
fairly easily.

00:17:38.440 --> 00:17:41.460
It looks a little heavier
weight, as you see here,

00:17:41.460 --> 00:17:43.490
because you're repeating the
names of the fields over and

00:17:43.490 --> 00:17:44.270
over again.

00:17:44.270 --> 00:17:47.560
But as soon as you compress this
data, it compresses down

00:17:47.560 --> 00:17:48.680
pretty small.

00:17:48.680 --> 00:17:52.520
We did use a Newline-delimited
JSON format.

00:17:52.520 --> 00:17:54.110
I want to stress that.

00:17:54.110 --> 00:17:56.970
You can see the backslash
ends at the end of

00:17:56.970 --> 00:17:58.940
each of these lines.

00:17:58.940 --> 00:18:01.030
In reality, this would only
be two lines of text.

00:18:01.030 --> 00:18:02.370
But on the slide, it's
a lot easier to

00:18:02.370 --> 00:18:06.160
communicate it as it is here.

00:18:06.160 --> 00:18:09.240
And the Newline-delimited format
allows us to really

00:18:09.240 --> 00:18:13.430
efficiently do things like
parallel ingestion of the data

00:18:13.430 --> 00:18:16.920
that you're putting into
the JSON format.

00:18:16.920 --> 00:18:19.420
But you can see here, we've
just, basically, taken that

00:18:19.420 --> 00:18:21.750
CSV format and put
it into JSON.

00:18:21.750 --> 00:18:25.380
It's a little more expressive,
a little easier to read, but

00:18:25.380 --> 00:18:28.990
doesn't add a huge amount of
value, other than potentially

00:18:28.990 --> 00:18:34.550
supporting some existing
JSON-based document stores and

00:18:34.550 --> 00:18:35.490
that sort of thing.

00:18:35.490 --> 00:18:37.710
But that's not all we've done.

00:18:37.710 --> 00:18:42.320
What we've done that adds a lot
of value is giving you the

00:18:42.320 --> 00:18:44.290
ability to do nested data.

00:18:44.290 --> 00:18:49.220
So you can see here, we went
from the normalized form to

00:18:49.220 --> 00:18:50.490
the denormalized form.

00:18:50.490 --> 00:18:53.550
And now we've given you the
ability to represent structure

00:18:53.550 --> 00:18:56.830
in your data through nested
and repeated values.

00:18:56.830 --> 00:18:59.730
And that structure really helps
make it much easier for

00:18:59.730 --> 00:19:03.400
people who are looking at your
data, developing against your

00:19:03.400 --> 00:19:05.600
data, and also as you're
loading your data, to

00:19:05.600 --> 00:19:10.080
understand the underlying
structure of your data.

00:19:10.080 --> 00:19:12.490
In terms of the actual querying
support that it

00:19:12.490 --> 00:19:17.020
provides, we can jump into
the queries here

00:19:17.020 --> 00:19:17.810
a little bit later.

00:19:17.810 --> 00:19:20.300
But the queries aren't that
much more expressive.

00:19:20.300 --> 00:19:23.710
You're not able to do that many
more different things

00:19:23.710 --> 00:19:26.490
with the nested data, but it
makes it a lot easier to

00:19:26.490 --> 00:19:30.070
understand and a lot easier to
read and basically represents

00:19:30.070 --> 00:19:33.700
how humans think a lot better,
in sort of an object-oriented

00:19:33.700 --> 00:19:34.590
nested format.

00:19:34.590 --> 00:19:39.260
So you can see what that might
look here for a table of

00:19:39.260 --> 00:19:41.990
people versus where they lived,
various cities that

00:19:41.990 --> 00:19:43.140
they lived in.

00:19:43.140 --> 00:19:46.940
And then we support that now in
JSON and be able to express

00:19:46.940 --> 00:19:49.370
that data in JSON format.

00:19:49.370 --> 00:19:52.000
And you can see here this
represents something like a

00:19:52.000 --> 00:19:55.420
birth record in this particular
record here.

00:19:55.420 --> 00:19:58.630
And a birth record, it's one of
our example data sets, the

00:19:58.630 --> 00:20:00.930
CDC birth statistics data.

00:20:00.930 --> 00:20:05.920
And you have each baby born
in the US represents

00:20:05.920 --> 00:20:07.890
a row of that data.

00:20:07.890 --> 00:20:10.600
And in here, you have
the states that the

00:20:10.600 --> 00:20:12.940
mother has lived in.

00:20:12.940 --> 00:20:14.900
The baby was born in 1979.

00:20:14.900 --> 00:20:17.010
The mother's lived in three
different states.

00:20:17.010 --> 00:20:20.480
That's showing the repeated
structure of BigQuery.

00:20:20.480 --> 00:20:22.970
And then, in terms of the nested
and repeated structure,

00:20:22.970 --> 00:20:24.560
we have the health
scores here.

00:20:24.560 --> 00:20:28.380
So they take a score, a
measurement of a baby's health

00:20:28.380 --> 00:20:30.610
after they're born, a couple of
minutes after they're born,

00:20:30.610 --> 00:20:33.310
maybe 10 minutes after
they're born.

00:20:33.310 --> 00:20:36.050
Back in 1979, maybe they only
took two measurements.

00:20:36.050 --> 00:20:39.470
So you would have the time at
five minutes and the time at

00:20:39.470 --> 00:20:41.920
10 minutes, and what the baby's
health was at each of

00:20:41.920 --> 00:20:43.390
those times.

00:20:43.390 --> 00:20:45.730
But let's say, nowadays, they
want to start collecting

00:20:45.730 --> 00:20:46.710
additional measurements.

00:20:46.710 --> 00:20:50.030
You can use the same schema,
and just add an additional

00:20:50.030 --> 00:20:52.570
time as you load new records
into BigQuery.

00:20:52.570 --> 00:20:56.330
So it's really powerful in
a way that allows you to

00:20:56.330 --> 00:20:58.590
structure your data a lot
better, makes it a lot more

00:20:58.590 --> 00:21:00.800
readable, and a lot more
workable when you're writing

00:21:00.800 --> 00:21:02.050
your queries.

00:21:04.000 --> 00:21:07.250
So let's give you a few examples
of queries here.

00:21:07.250 --> 00:21:10.125
Here's one example of querying
nested and repeated data, a

00:21:10.125 --> 00:21:13.050
very simple example where
we can say that the

00:21:13.050 --> 00:21:17.420
mother_substance_used.alcohol
equals true.

00:21:17.420 --> 00:21:19.630
So you could have a bunch of
different substance_used

00:21:19.630 --> 00:21:21.240
properties.

00:21:21.240 --> 00:21:23.160
I probably shouldn't go through
the whole list of

00:21:23.160 --> 00:21:25.650
various substances that a mother
could use that would

00:21:25.650 --> 00:21:27.750
affect the health
of their baby.

00:21:27.750 --> 00:21:30.730
But
mother_substance_used.alcohol

00:21:30.730 --> 00:21:32.720
here equals true.

00:21:32.720 --> 00:21:36.010
And really, that dot syntax
allows you to just express

00:21:36.010 --> 00:21:39.480
what the nested value is.

00:21:39.480 --> 00:21:42.770
And then where you get a little
bit more complicated is

00:21:42.770 --> 00:21:44.790
when you're actually--

00:21:44.790 --> 00:21:47.460
well, complicated might
not be the right term.

00:21:47.460 --> 00:21:50.400
Where you're able to be a little
more expressive here is

00:21:50.400 --> 00:21:53.740
when you're trying to look
at the nested data.

00:21:53.740 --> 00:21:55.950
And for instance, in this case,
we're trying to figure

00:21:55.950 --> 00:21:59.070
out the average number of states
a mother has lived in

00:21:59.070 --> 00:22:00.800
before giving birth
to their child.

00:22:00.800 --> 00:22:03.360
And maybe you want to correlate
that, versus the

00:22:03.360 --> 00:22:05.790
health scores or the substances
used or something.

00:22:05.790 --> 00:22:06.970
MICHAEL MANOOCHEHRI: This is
an interesting query too,

00:22:06.970 --> 00:22:09.490
because the amount of states a
mother has lived in can be a

00:22:09.490 --> 00:22:11.000
variable between
records, right?

00:22:11.000 --> 00:22:13.080
So you could have somebody can
be living in five states over

00:22:13.080 --> 00:22:15.170
their lifetime, maybe two
states in another one.

00:22:15.170 --> 00:22:15.840
RYAN BOYD: Yeah, exactly.

00:22:15.840 --> 00:22:17.310
And you're trying to
figure out what the

00:22:17.310 --> 00:22:18.850
average number is.

00:22:18.850 --> 00:22:21.450
And that's what this WITHIN
statement, WITHIN RECORD, can

00:22:21.450 --> 00:22:21.970
give you a COUNT.

00:22:21.970 --> 00:22:22.390
MICHAEL MANOOCHEHRI: Right.

00:22:22.390 --> 00:22:25.570
RYAN BOYD: And then you can do
an aggregate on the outside of

00:22:25.570 --> 00:22:31.700
that, as a SuperSelect of the
Subselect statement here, and

00:22:31.700 --> 00:22:33.310
get the average number
of states that a

00:22:33.310 --> 00:22:33.746
mother has lived in.

00:22:33.746 --> 00:22:33.940
MICHAEL MANOOCHEHRI:
That's great.

00:22:33.940 --> 00:22:35.400
So it's a very natural way
to model the data.

00:22:35.400 --> 00:22:38.350
If this was in a flat record,
it would just be a lot of

00:22:38.350 --> 00:22:39.990
repeated values over and
over in the records.

00:22:39.990 --> 00:22:41.120
RYAN BOYD: Yeah, exactly.

00:22:41.120 --> 00:22:45.440
So it really allows you to
repeat your data a lot less.

00:22:45.440 --> 00:22:49.240
And the underlying storage is
a lot less, when you have

00:22:49.240 --> 00:22:51.880
these nested/repeated records.

00:22:51.880 --> 00:22:54.540
It's kind of ironic that
repeated records allow you to

00:22:54.540 --> 00:22:55.120
repeat your data a lot less.

00:22:55.120 --> 00:22:55.274
MICHAEL MANOOCHEHRI: Yeah.

00:22:55.274 --> 00:22:55.351
[LAUGHS]

00:22:55.351 --> 00:22:59.450
RYAN BOYD: But anyway during
the ingestion process,

00:22:59.450 --> 00:23:04.210
basically, you don't have to
repeat the values for every

00:23:04.210 --> 00:23:05.860
single row as you flatten
out the data.

00:23:05.860 --> 00:23:09.000
So super valuable.

00:23:09.000 --> 00:23:11.380
And I think that this release
really represents what we're

00:23:11.380 --> 00:23:14.590
trying to do with BigQuery,
trying to make Google's

00:23:14.590 --> 00:23:17.270
underlying technologies
available to you.

00:23:17.270 --> 00:23:20.550
A lot easier to use than even
some of the functionality that

00:23:20.550 --> 00:23:22.190
Google has internally.

00:23:22.190 --> 00:23:26.010
We're adding that layer of
ease on top of it for the

00:23:26.010 --> 00:23:30.090
developers to be able to get
their data in quickly in a

00:23:30.090 --> 00:23:36.340
format that they're accustomed
to using with other NoSQL

00:23:36.340 --> 00:23:39.230
databases and other document
stores and using elsewhere on

00:23:39.230 --> 00:23:42.690
the web in various APIs
the JSON format.

00:23:42.690 --> 00:23:45.970
And in the nested-repeated
structure, that's a lot easier

00:23:45.970 --> 00:23:50.430
for developers to understand and
then eventually query on.

00:23:50.430 --> 00:23:54.150
So I think that really sums up
this release is it just makes

00:23:54.150 --> 00:23:56.840
it a lot easier for developers
to import data in a structure

00:23:56.840 --> 00:24:00.300
they understand and then
query upon it.

00:24:00.300 --> 00:24:03.900
Anything else, before we head
off to the Office Hours?

00:24:03.900 --> 00:24:04.006
MICHAEL MANOOCHEHRI: No.

00:24:04.006 --> 00:24:04.330
Let's do it.

00:24:04.330 --> 00:24:05.800
Let's check out what
people are asking.

00:24:05.800 --> 00:24:08.460
RYAN BOYD: All right, so we're
going to ask questions from

00:24:08.460 --> 00:24:09.590
the audience.

00:24:09.590 --> 00:24:11.880
But you can express
these questions

00:24:11.880 --> 00:24:13.350
over in Google Moderator.

00:24:13.350 --> 00:24:18.280
Again, just going to
developers.google.com/life and

00:24:18.280 --> 00:24:19.920
click on the event here today.

00:24:19.920 --> 00:24:23.270
And you'll see a Moderator
embedded there.

00:24:23.270 --> 00:24:24.350
You can ask your questions.

00:24:24.350 --> 00:24:26.310
I think there's already some
questions over there, so we're

00:24:26.310 --> 00:24:29.470
going to head over and check out
on my browser here, check

00:24:29.470 --> 00:24:36.600
out what questions there are, if
I can learn how to operate

00:24:36.600 --> 00:24:38.720
my computer.

00:24:38.720 --> 00:24:42.640
That's always a challenge
for me.

00:24:42.640 --> 00:24:44.440
So let's see here.

00:24:44.440 --> 00:24:45.690
Here's our Moderator.

00:24:48.430 --> 00:24:52.110
So it looks like we have five
questions from six people.

00:24:52.110 --> 00:24:54.002
And we'll just go
through the--

00:24:56.840 --> 00:24:57.700
let's see here.

00:24:57.700 --> 00:24:58.970
Does running queries--

00:24:58.970 --> 00:25:00.810
so let's go through the
top to bottom here.

00:25:00.810 --> 00:25:01.980
MICHAEL MANOOCHEHRI:
All right.

00:25:01.980 --> 00:25:03.490
RYAN BOYD: So you want to talk
about the first question.

00:25:03.490 --> 00:25:03.810
MICHAEL MANOOCHEHRI: Yeah.

00:25:03.810 --> 00:25:04.470
So this one here?

00:25:04.470 --> 00:25:04.850
RYAN BOYD: Yeah.

00:25:04.850 --> 00:25:06.790
MICHAEL MANOOCHEHRI: So does
running queries with the

00:25:06.790 --> 00:25:09.210
unions on lots of tables affect
the performance in

00:25:09.210 --> 00:25:11.550
terms of the time it takes to
return the query result?

00:25:11.550 --> 00:25:13.850
So what this question is
referring to is it's possible

00:25:13.850 --> 00:25:17.640
to run queries over tables with
the same schema by doing

00:25:17.640 --> 00:25:20.770
a union query, basically listing
the table names in the

00:25:20.770 --> 00:25:23.710
query, running the query over
multiple tables at once.

00:25:23.710 --> 00:25:25.300
RYAN BOYD: Because, oftentimes,
you'll actually

00:25:25.300 --> 00:25:27.790
shard your data in BigQuery.

00:25:27.790 --> 00:25:31.690
For instance, I was looking at
the Wikipedia page views.

00:25:31.690 --> 00:25:32.600
And I sharded that data.

00:25:32.600 --> 00:25:34.500
It's huge amounts of data.

00:25:34.500 --> 00:25:36.960
I sharded that data by month.

00:25:36.960 --> 00:25:40.410
So I could look and only embrace
the full table scan on

00:25:40.410 --> 00:25:42.810
a month of data at a time,
because often you'll only need

00:25:42.810 --> 00:25:45.060
to actually query the
most recent month or

00:25:45.060 --> 00:25:45.900
something like that.

00:25:45.900 --> 00:25:47.950
So you'll shard your data.

00:25:47.950 --> 00:25:50.490
But then, at times, you
do want to query over

00:25:50.490 --> 00:25:51.520
many months of data.

00:25:51.520 --> 00:25:55.440
And actually the 13 billion rows
thing that I talked about

00:25:55.440 --> 00:25:58.290
was created over many months of
data in a union query like

00:25:58.290 --> 00:25:59.110
this talks about.

00:25:59.110 --> 00:25:59.810
MICHAEL MANOOCHEHRI:
Yeah, definitely.

00:25:59.810 --> 00:26:01.910
By the way, for more information
about sharding and

00:26:01.910 --> 00:26:04.870
table design, Ryan and I have
written a new ingestion

00:26:04.870 --> 00:26:08.520
cookbook on our web site, which
is best practices for

00:26:08.520 --> 00:26:10.810
data ingestion and schemas
and sharding,

00:26:10.810 --> 00:26:12.090
and things like that.

00:26:12.090 --> 00:26:14.920
To get back to this question,
because of the design of

00:26:14.920 --> 00:26:18.680
BigQuery, actually there's a
very negligible performance

00:26:18.680 --> 00:26:20.170
hit when you do these
kind of things.

00:26:20.170 --> 00:26:23.450
Actually, the queries are
returned very quickly.

00:26:23.450 --> 00:26:24.720
And that's due to the

00:26:24.720 --> 00:26:26.440
distributed design of BigQuery.

00:26:26.440 --> 00:26:28.460
So you shouldn't see any
performance hit when you run

00:26:28.460 --> 00:26:29.375
over multiple tables.

00:26:29.375 --> 00:26:30.160
RYAN BOYD: Yeah.

00:26:30.160 --> 00:26:34.730
If you look at our Google I/O
video that I did along with

00:26:34.730 --> 00:26:39.180
one of the core engineers on
the BigQuery back end, we

00:26:39.180 --> 00:26:42.020
actually explain a lot of the
underlying BigQuery structure

00:26:42.020 --> 00:26:43.520
and how it processes queries.

00:26:43.520 --> 00:26:46.650
So if you're a true query geek,
a true database geek,

00:26:46.650 --> 00:26:47.890
you might want to
check that out.

00:26:47.890 --> 00:26:51.840
And you can see how each leaf
node only processes a small

00:26:51.840 --> 00:26:52.210
chunk of data.

00:26:52.210 --> 00:26:55.100
It really doesn't matter what
table that chunk of data is

00:26:55.100 --> 00:26:55.560
coming from.

00:26:55.560 --> 00:26:58.740
It doesn't change the
performance characteristics.

00:26:58.740 --> 00:27:02.460
So the next one here, I
guess I'll take this.

00:27:02.460 --> 00:27:05.530
The data I'm dealing with
is all time series data.

00:27:05.530 --> 00:27:08.270
And all the queries
are time base.

00:27:08.270 --> 00:27:11.070
Will queries become more
expensive as the size of the

00:27:11.070 --> 00:27:12.710
database increases?

00:27:12.710 --> 00:27:14.770
Is it a good idea to shard
the database--

00:27:14.770 --> 00:27:15.950
actually, this is
quite relevant--

00:27:15.950 --> 00:27:19.870
shard the database into
monthly/biweekly tables to

00:27:19.870 --> 00:27:20.910
optimize cost?

00:27:20.910 --> 00:27:24.470
And the short answer is yes.

00:27:24.470 --> 00:27:28.830
So yes, it is helpful to shard
the database, both for

00:27:28.830 --> 00:27:31.040
performance as well
as for the cost.

00:27:31.040 --> 00:27:34.400
So it really depends on what
types of queries that you're

00:27:34.400 --> 00:27:35.230
going to run.

00:27:35.230 --> 00:27:38.120
Are you running queries over
all of your data usually?

00:27:38.120 --> 00:27:40.410
Or are you running queries
just over a specific

00:27:40.410 --> 00:27:41.770
part of your data?

00:27:41.770 --> 00:27:44.270
Most people with time-based
data, like I was saying

00:27:44.270 --> 00:27:47.690
earlier, oftentimes, they're
just running queries over the

00:27:47.690 --> 00:27:49.650
last week or the last month.

00:27:49.650 --> 00:27:52.000
We even have people, some of
our developers, that are

00:27:52.000 --> 00:27:56.030
sharding their data by hour,
because that's the type of

00:27:56.030 --> 00:27:58.730
access that they often need,
is just looking at it on an

00:27:58.730 --> 00:28:00.720
hourly by hourly basis.

00:28:00.720 --> 00:28:02.040
And then you can even
do things--

00:28:02.040 --> 00:28:04.920
I believe we have a way that you
can express that the table

00:28:04.920 --> 00:28:07.470
should expire.

00:28:07.470 --> 00:28:10.880
So as your data gets old and
know you don't actually need

00:28:10.880 --> 00:28:13.980
to have data, maybe, from
five years ago, you can

00:28:13.980 --> 00:28:17.050
automatically expire those
tables as it goes on.

00:28:17.050 --> 00:28:19.450
So that's very helpful.

00:28:19.450 --> 00:28:22.320
And sharding, because we are
embracing the concept of the

00:28:22.320 --> 00:28:26.560
full table scan, you're
basically charged for the size

00:28:26.560 --> 00:28:29.400
of all the columns that you're
querying times the number of

00:28:29.400 --> 00:28:31.840
rows that are in your table.

00:28:31.840 --> 00:28:34.390
And it's that complete table,
because we're doing a full

00:28:34.390 --> 00:28:35.380
table scan each time.

00:28:35.380 --> 00:28:38.100
So shard your data, definitely,
if you're often

00:28:38.100 --> 00:28:42.310
doing queries that are over just
a segment or sub-part of

00:28:42.310 --> 00:28:43.100
your overall data.

00:28:43.100 --> 00:28:43.990
MICHAEL MANOOCHEHRI: These
are really good

00:28:43.990 --> 00:28:45.040
questions, by the way.

00:28:45.040 --> 00:28:45.510
RYAN BOYD: Yeah, absolutely.

00:28:45.510 --> 00:28:47.740
MICHAEL MANOOCHEHRI: So it says
the next question is--

00:28:47.740 --> 00:28:48.720
RYAN BOYD: That's the one
we just answered.

00:28:48.720 --> 00:28:49.410
MICHAEL MANOOCHEHRI: We
answered that one.

00:28:49.410 --> 00:28:51.960
According to the BigQuery web
site, we are charged on the

00:28:51.960 --> 00:28:54.580
data processed in each column
selected in the query.

00:28:54.580 --> 00:28:56.210
What exactly does data
process mean?

00:28:56.210 --> 00:28:58.080
Is this the amount of data
being accessed to

00:28:58.080 --> 00:28:59.400
return a data set?

00:28:59.400 --> 00:29:01.870
Well, it's the amount of data
being accessed to return your

00:29:01.870 --> 00:29:04.850
query result, so yes, that
is how we charge.

00:29:04.850 --> 00:29:06.590
This is actually a very
economical way to charge

00:29:06.590 --> 00:29:10.270
because, if you only select a
certain set of columns to

00:29:10.270 --> 00:29:12.890
return your query result, you're
only charged for the

00:29:12.890 --> 00:29:15.180
columns that the query touches,
not the rest of the

00:29:15.180 --> 00:29:16.710
columns in your table.

00:29:16.710 --> 00:29:19.410
So that makes the queries as
economical as possible.

00:29:19.410 --> 00:29:22.070
RYAN BOYD: And actually, from
a performance perspective as

00:29:22.070 --> 00:29:26.250
well, we store this data in
a column-based format.

00:29:26.250 --> 00:29:29.030
So we're only actually reading
data off of disk that

00:29:29.030 --> 00:29:31.450
represents the columns that
you're accessing.

00:29:31.450 --> 00:29:34.970
So, basically, the way that
we're charging you directly,

00:29:34.970 --> 00:29:39.895
maps onto our underlying cost
and, basically, the time that

00:29:39.895 --> 00:29:42.630
it takes to read that
data off of disk.

00:29:42.630 --> 00:29:47.150
So that's an important part of
BigQuery, because most of our

00:29:47.150 --> 00:29:50.320
customers are actually not
needing to query across every

00:29:50.320 --> 00:29:52.060
single one of the columns
in their tables.

00:29:52.060 --> 00:29:55.000
So the row-based structure
doesn't work well for them,

00:29:55.000 --> 00:29:59.490
but the column-based structure
of BigQuery does work well.

00:29:59.490 --> 00:30:03.630
And in terms of what exactly
does data process mean, it's

00:30:03.630 --> 00:30:08.060
really just the number of
bytes in the column, the

00:30:08.060 --> 00:30:11.420
columns that you're accessing,
so the size of each of the

00:30:11.420 --> 00:30:17.080
columns times the number of
rows that we talked about.

00:30:17.080 --> 00:30:20.760
And I think one more question
here on quantile queries.

00:30:20.760 --> 00:30:22.192
You've done a little bit
more of that, so why

00:30:22.192 --> 00:30:22.490
don't you answer it.

00:30:22.490 --> 00:30:23.770
MICHAEL MANOOCHEHRI: Yeah, I'm
really interested in this one.

00:30:23.770 --> 00:30:26.290
So quantile queries do not allow
any kind of grouping by

00:30:26.290 --> 00:30:27.270
another column.

00:30:27.270 --> 00:30:29.560
Do you have plans to allow
this in the future?

00:30:29.560 --> 00:30:31.980
And this person is going to
group by medians and 99th

00:30:31.980 --> 00:30:33.310
percentile the time.

00:30:33.310 --> 00:30:34.820
This is a really interesting
question.

00:30:34.820 --> 00:30:37.700
So currently, BigQuery's
Quantile function merely

00:30:37.700 --> 00:30:42.330
buckets data into any number of
buckets that you want, but

00:30:42.330 --> 00:30:44.680
doesn't allow for a further
group-by function.

00:30:44.680 --> 00:30:45.770
We've been talking about this.

00:30:45.770 --> 00:30:47.590
We don't have any news to
report, but we're looking into

00:30:47.590 --> 00:30:50.770
ways to improve the Quantile
function to provide additional

00:30:50.770 --> 00:30:54.240
functionality, like grouping the
buckets by a certain other

00:30:54.240 --> 00:30:55.390
value in another column.

00:30:55.390 --> 00:30:57.230
So that doesn't exist yet, but
we're looking into it.

00:30:57.230 --> 00:30:59.045
And we're glad that you have
some use cases for it.

00:30:59.045 --> 00:31:01.120
It's interesting to see that
people are looking to BigQuery

00:31:01.120 --> 00:31:02.160
to do things like this.

00:31:02.160 --> 00:31:03.830
Another thing you could do, by
the way, is you could take the

00:31:03.830 --> 00:31:06.620
result set and feed that into
another program that does

00:31:06.620 --> 00:31:08.330
additional statistical
analysis.

00:31:08.330 --> 00:31:12.260
It's easy to get the results
back as a CSV form where you

00:31:12.260 --> 00:31:16.160
can pipe that into something
like R, into a data frame with

00:31:16.160 --> 00:31:17.430
R or some other statistical
packages.

00:31:17.430 --> 00:31:18.180
So you can also do that.

00:31:18.180 --> 00:31:20.290
RYAN BOYD: And oftentimes, for
those statistical type

00:31:20.290 --> 00:31:23.780
queries, people will use the
Hash function in BigQuery to

00:31:23.780 --> 00:31:26.980
select a random subset
of their data.

00:31:26.980 --> 00:31:31.270
So they'll select 1/10 of the
data or 1/100 of their data.

00:31:31.270 --> 00:31:33.910
And that allows you to do those
statistical analyses on

00:31:33.910 --> 00:31:35.560
what was big data.

00:31:35.560 --> 00:31:39.040
But you're selecting the
random subset to do the

00:31:39.040 --> 00:31:39.690
analysis on.

00:31:39.690 --> 00:31:42.630
And then you can verify your
results by then selecting a

00:31:42.630 --> 00:31:45.200
different random subset,
et cetera.

00:31:45.200 --> 00:31:46.710
So what I'm going to do here
is I'm going to quickly

00:31:46.710 --> 00:31:48.480
refresh, because I'm not sure
if this is actually

00:31:48.480 --> 00:31:49.590
refreshing on us.

00:31:49.590 --> 00:31:51.910
So I'm going to quickly refresh
the screen here, and

00:31:51.910 --> 00:31:55.310
we'll see if there's any other
questions that you guys have

00:31:55.310 --> 00:31:58.290
put into the Moderator here.

00:31:58.290 --> 00:32:02.160
And if there are any other
questions, we'll answer them.

00:32:02.160 --> 00:32:04.340
If not-- it doesn't look like
there are any other questions.

00:32:04.340 --> 00:32:07.350
So maybe we should thank
you all for joining

00:32:07.350 --> 00:32:09.220
our show here today.

00:32:09.220 --> 00:32:11.770
And I do want to say that
these types of questions

00:32:11.770 --> 00:32:15.450
provide great value for us
to understand what you're

00:32:15.450 --> 00:32:17.140
thinking about and
us to understand

00:32:17.140 --> 00:32:18.060
your feature requests.

00:32:18.060 --> 00:32:21.110
So although, for instance, we
don't support what was just

00:32:21.110 --> 00:32:23.730
asked about the Quantile
functions that people are

00:32:23.730 --> 00:32:27.760
looking for, just asking them
here today allows us to know

00:32:27.760 --> 00:32:30.690
that you're looking for that
type of functionality and

00:32:30.690 --> 00:32:33.050
allows us to prioritize
it better in our

00:32:33.050 --> 00:32:33.880
upcoming road map.

00:32:33.880 --> 00:32:35.330
So thank you.

00:32:35.330 --> 00:32:36.990
My name is Ryan Boyd.

00:32:36.990 --> 00:32:37.970
MICHAEL MANOOCHEHRI: And I'm
Michael Manoochehri.

00:32:37.970 --> 00:32:39.690
RYAN BOYD: And this
is the Ryan and

00:32:39.690 --> 00:32:41.380
Michael show, or the--

00:32:41.380 --> 00:32:42.632
MICHAEL MANOOCHEHRI: Michael
and Ryan show.

00:32:42.632 --> 00:32:43.200
Live from London.

00:32:43.200 --> 00:32:43.400
[LAUGHS]

00:32:43.400 --> 00:32:44.650
RYAN BOYD: Take care.

