WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:07.808
[MUSIC PLAYING]

00:00:07.808 --> 00:00:08.784
[APPLAUSE]

00:00:08.784 --> 00:00:10.570
JUAN SILVEIRA: Hello, everyone.

00:00:10.570 --> 00:00:11.660
My name is Juan Silveira.

00:00:11.660 --> 00:00:15.040
I'm a software engineer in
the DeepMind Health team,

00:00:15.040 --> 00:00:17.890
and I'm here to tell you a
little bit about DeepMind

00:00:17.890 --> 00:00:21.780
and the work that
we've been doing.

00:00:21.780 --> 00:00:25.340
So let me start by
telling you who we are.

00:00:25.340 --> 00:00:29.350
So DeepMind is a British
artificial intelligence company

00:00:29.350 --> 00:00:31.900
that was founded in
2010 and joined forces

00:00:31.900 --> 00:00:34.390
with Google in 2014.

00:00:34.390 --> 00:00:36.520
We're a bit of an unusual
company in that we

00:00:36.520 --> 00:00:39.250
have a two-part mission.

00:00:39.250 --> 00:00:41.320
The first part of
that mission is

00:00:41.320 --> 00:00:44.440
to create general
artificial intelligence.

00:00:44.440 --> 00:00:47.770
But what do we mean by general
artificial intelligence?

00:00:47.770 --> 00:00:52.600
And the key here is being able
to create agents' algorithms

00:00:52.600 --> 00:00:57.370
that can work and do well
in a variety of tasks

00:00:57.370 --> 00:00:58.600
and environments.

00:00:58.600 --> 00:01:00.160
And the key to
that is to get them

00:01:00.160 --> 00:01:02.980
to learn from their
own experience,

00:01:02.980 --> 00:01:05.230
so not to preprogram
them with behaviors

00:01:05.230 --> 00:01:08.720
but to actually allow them
to learn from what they

00:01:08.720 --> 00:01:11.104
can see in their environment.

00:01:11.104 --> 00:01:12.770
So that's the first
part of our mission.

00:01:12.770 --> 00:01:14.890
But we also have a
second part, which

00:01:14.890 --> 00:01:20.110
is to use artificial general
intelligence to address

00:01:20.110 --> 00:01:22.010
pressing social challenges.

00:01:22.010 --> 00:01:27.399
So we recognize the
potential of AGI

00:01:27.399 --> 00:01:28.940
to really make a
change in the world.

00:01:28.940 --> 00:01:32.620
We want to make sure that the
benefits of that reach everyone

00:01:32.620 --> 00:01:35.350
and that we can use
it to address things

00:01:35.350 --> 00:01:39.710
that we all, as a society,
have to deal with.

00:01:39.710 --> 00:01:42.160
So how are we doing that?

00:01:42.160 --> 00:01:44.140
So there's our mission.

00:01:44.140 --> 00:01:45.645
How are we actually formed?

00:01:45.645 --> 00:01:48.640
We're at heart a
research organization.

00:01:48.640 --> 00:01:51.250
We're based in London,
in Kings Cross,

00:01:51.250 --> 00:01:55.450
and we have more than 300
of the best AI research

00:01:55.450 --> 00:01:58.210
scientists in the world,
all here in Europe.

00:02:00.720 --> 00:02:06.230
Like any research organization,
we do our research in the open.

00:02:06.230 --> 00:02:10.660
So we actually publish over 100
papers in different research

00:02:10.660 --> 00:02:12.130
organizations.

00:02:12.130 --> 00:02:15.610
And we joined forces
with Google in 2014,

00:02:15.610 --> 00:02:19.510
which means that we could
really take our speed up

00:02:19.510 --> 00:02:20.800
to the next level.

00:02:20.800 --> 00:02:23.950
And the mixture of
these two things

00:02:23.950 --> 00:02:27.910
means that we're a bit
of a hybrid organization.

00:02:27.910 --> 00:02:31.300
On the one hand, we have
the very long-term focus

00:02:31.300 --> 00:02:34.060
of academia because AGI
is not something that's

00:02:34.060 --> 00:02:37.300
going to happen anytime soon,
but we're very, very focused

00:02:37.300 --> 00:02:38.660
on making that happen.

00:02:38.660 --> 00:02:43.600
But at the same time, we
have the pace, the scale,

00:02:43.600 --> 00:02:46.734
and the agility of a
very well-funded startup.

00:02:46.734 --> 00:02:48.650
And because of the second
part of our mission,

00:02:48.650 --> 00:02:51.460
we also have a lot of
people in our organization

00:02:51.460 --> 00:02:55.150
that have the kind of
social impact values

00:02:55.150 --> 00:02:59.850
that you only find generally
in the public sector.

00:02:59.850 --> 00:03:04.230
Now, I talked about
AGI, artificial general

00:03:04.230 --> 00:03:04.890
intelligence.

00:03:04.890 --> 00:03:07.680
But how is it that we're
trying to resolve this?

00:03:07.680 --> 00:03:11.910
And we're trying to resolve
this by building general purpose

00:03:11.910 --> 00:03:14.100
learning systems.

00:03:14.100 --> 00:03:16.290
And this, just to
give you an idea,

00:03:16.290 --> 00:03:20.860
involves creating agents that
interact with an environment.

00:03:20.860 --> 00:03:25.410
So these agents get signals
and make observations

00:03:25.410 --> 00:03:28.530
of the environment
and decide on actions

00:03:28.530 --> 00:03:32.520
to take that then get
reflected in that environment.

00:03:32.520 --> 00:03:34.410
And once they've
taken those actions,

00:03:34.410 --> 00:03:38.770
they can actually observe again
how it affects the environment.

00:03:38.770 --> 00:03:42.390
But the key thing is that they
have a goal, so a function that

00:03:42.390 --> 00:03:44.700
tells them whether
they're closer

00:03:44.700 --> 00:03:47.460
to actually achieving what it
is that they're trying to do.

00:03:47.460 --> 00:03:51.705
And this whole process is called
Deep Reinforcement Learning.

00:03:51.705 --> 00:03:53.980
I'm sure some of you
have heard about it.

00:03:53.980 --> 00:03:55.916
And it's key to what
we're doing at DeepMind.

00:03:58.832 --> 00:04:02.760
The environment that
we use for this is key,

00:04:02.760 --> 00:04:04.910
and we've actually
had quite a bit

00:04:04.910 --> 00:04:08.876
of success using the old
Atari games as a testbed,

00:04:08.876 --> 00:04:11.570
as an environment in
which to train agents.

00:04:11.570 --> 00:04:15.950
So we've taken over a hundred
of the classic Atari games,

00:04:15.950 --> 00:04:19.640
so this includes things like
Pong, Frogger, Montezuma's

00:04:19.640 --> 00:04:21.560
Revenge Breakout--

00:04:21.560 --> 00:04:24.050
some games that I'm sure
many of you have played.

00:04:24.050 --> 00:04:27.280
And we've created an environment
in which what the agent sees,

00:04:27.280 --> 00:04:31.160
the inputs are just the
pixels on the screen.

00:04:31.160 --> 00:04:33.020
So it just gets that
matrix of pixels.

00:04:33.020 --> 00:04:36.350
For RGB values, that is what
you would see on the screen.

00:04:36.350 --> 00:04:39.279
And the outputs are the
controllers in the Atari game,

00:04:39.279 --> 00:04:40.570
which are very simple controls.

00:04:40.570 --> 00:04:45.260
So you could go up, down, left,
right, and that trigger button.

00:04:45.260 --> 00:04:47.900
But the really important
thing is that the agent is not

00:04:47.900 --> 00:04:49.430
told what to do.

00:04:49.430 --> 00:04:51.950
So it's not told the rules
of the different games.

00:04:51.950 --> 00:04:59.270
It's not told how it works,
the internal functioning

00:04:59.270 --> 00:05:00.570
of the Atari game.

00:05:00.570 --> 00:05:04.130
So any knowledge needs to
be learned from scratch

00:05:04.130 --> 00:05:06.920
with serial preprogramming,
like no strategies for how

00:05:06.920 --> 00:05:08.480
to play with different games.

00:05:08.480 --> 00:05:13.040
What we give it is the goal, and
that is to maximize the score.

00:05:13.040 --> 00:05:17.610
But the key thing that
I think is fantastic

00:05:17.610 --> 00:05:20.390
when I first learned about
this was that one agent plays

00:05:20.390 --> 00:05:21.370
all of the games.

00:05:21.370 --> 00:05:23.450
So you don't train one
agent for each game.

00:05:23.450 --> 00:05:25.930
You train one agent
in all of the games

00:05:25.930 --> 00:05:28.490
and have it play
all of the games.

00:05:28.490 --> 00:05:30.560
So a good analogy
of how this works

00:05:30.560 --> 00:05:33.530
is that you could have a
little robot in an arcade

00:05:33.530 --> 00:05:36.000
sitting in front of the
game, looking at the screen,

00:05:36.000 --> 00:05:37.790
and moving the controller,
and that's all.

00:05:37.790 --> 00:05:41.220
It doesn't have any knowledge of
the internals of how it works.

00:05:41.220 --> 00:05:45.830
So let me show
you how this looks

00:05:45.830 --> 00:05:51.020
like as the agent is being
trained and how it evolves.

00:05:51.020 --> 00:05:52.657
So this is the game of Breakout.

00:05:52.657 --> 00:05:54.740
I'm sure many of you have
played-- how many of you

00:05:54.740 --> 00:05:56.491
have played Breakout before?

00:05:56.491 --> 00:05:56.990
Great.

00:05:56.990 --> 00:05:57.990
I won't explain it then.

00:06:00.450 --> 00:06:03.260
So the agent starts training.

00:06:03.260 --> 00:06:07.280
After half an hour, it doesn't
really know what's happening,

00:06:07.280 --> 00:06:08.630
so it just loses quite quickly.

00:06:08.630 --> 00:06:11.260
Sometimes it manages to
bounce the ball back,

00:06:11.260 --> 00:06:14.330
but mostly just
loses quite quickly.

00:06:14.330 --> 00:06:17.490
After an hour of training,
gets a little bit better.

00:06:17.490 --> 00:06:21.770
It maybe survives two
bounces, but that's about it.

00:06:21.770 --> 00:06:25.490
It actually still
loses quite quickly.

00:06:25.490 --> 00:06:28.565
But after two hours,
it's now doing

00:06:28.565 --> 00:06:30.940
how you would do probably if
you started playing Breakout

00:06:30.940 --> 00:06:31.440
right now.

00:06:31.440 --> 00:06:35.780
So it runs around,
bounces a ball back,

00:06:35.780 --> 00:06:38.929
and manages a basic
level of competence.

00:06:38.929 --> 00:06:40.970
But the great thing is
what happened unexpectedly

00:06:40.970 --> 00:06:44.270
after we let it
play for four hours,

00:06:44.270 --> 00:06:45.770
and the agent
started doing this.

00:06:52.330 --> 00:06:54.010
So for those of you
who have actually

00:06:54.010 --> 00:06:56.530
played Breakout
for some time, you

00:06:56.530 --> 00:06:58.162
know that this is
the winning strategy.

00:06:58.162 --> 00:07:00.370
You break the sides and you
push the ball at the top,

00:07:00.370 --> 00:07:02.710
and it just does
the work for you.

00:07:02.710 --> 00:07:05.230
But this is not
something that was

00:07:05.230 --> 00:07:07.990
preprogrammed on the system.

00:07:07.990 --> 00:07:10.380
It's not something that
we told the agent about.

00:07:10.380 --> 00:07:13.330
It's something that the agent
discovered by playing the game

00:07:13.330 --> 00:07:16.480
thousands and thousands of
times with this goal function

00:07:16.480 --> 00:07:18.652
of improving the score.

00:07:18.652 --> 00:07:20.860
And this is the power of
Deep Reinforcement Learning,

00:07:20.860 --> 00:07:24.580
that it allows this kind
of behavior so much.

00:07:24.580 --> 00:07:30.235
So we used the Atari games
for a long time for research

00:07:30.235 --> 00:07:33.450
and we're still
using them today.

00:07:33.450 --> 00:07:35.310
And we published papers
about this research,

00:07:35.310 --> 00:07:37.300
and it was quite successful.

00:07:37.300 --> 00:07:39.860
But a few years ago, there
is a team on DeepMind

00:07:39.860 --> 00:07:41.800
that started looking
for a new challenge.

00:07:41.800 --> 00:07:45.740
And that challenge
was the game of Go.

00:07:45.740 --> 00:07:49.320
Let's see if I can get it.

00:07:49.320 --> 00:07:51.790
So for those of you
that don't know Go,

00:07:51.790 --> 00:07:55.680
it's a very ancient
game, originated in China

00:07:55.680 --> 00:07:57.990
about 3,000 years ago.

00:07:57.990 --> 00:08:02.820
And it's considered by many
to be more than just a game.

00:08:02.820 --> 00:08:06.300
People think about it as
a spiritual dimension,

00:08:06.300 --> 00:08:08.790
as poetry or art.

00:08:08.790 --> 00:08:11.340
It was considered to
be one of the four arts

00:08:11.340 --> 00:08:16.440
that were key to be
mastered by a true scholar.

00:08:16.440 --> 00:08:20.190
It is hugely popular in many
places, especially in Asia.

00:08:20.190 --> 00:08:23.980
There's 40 million players,
2,000 professional players,

00:08:23.980 --> 00:08:26.830
and there's Go schools
throughout Japan, South Korea,

00:08:26.830 --> 00:08:28.350
and China.

00:08:28.350 --> 00:08:31.000
And the thing about Go,
why it's a great challenge

00:08:31.000 --> 00:08:33.360
is because it has
very simple rules.

00:08:33.360 --> 00:08:35.770
You could learn the rules of
Go in a couple of minutes,

00:08:35.770 --> 00:08:38.730
but from those rules
actually arises

00:08:38.730 --> 00:08:40.950
a huge, profound complexity.

00:08:40.950 --> 00:08:43.940
Just to give you an
idea of the scale,

00:08:43.940 --> 00:08:46.110
there's an estimate
that there's about 10

00:08:46.110 --> 00:08:49.290
to the 80 number of
atoms in the universe,

00:08:49.290 --> 00:08:53.160
but there are 10 to
the 170 possible board

00:08:53.160 --> 00:08:54.720
configurations in Go.

00:08:54.720 --> 00:08:58.170
So it's impossible to
play Go by brute forcing

00:08:58.170 --> 00:09:01.700
or by trying to explore all
of that possibility space

00:09:01.700 --> 00:09:03.642
with no good strategy.

00:09:03.642 --> 00:09:05.100
And that means that
Go has been one

00:09:05.100 --> 00:09:09.510
of the great challenges
for AI because no computer

00:09:09.510 --> 00:09:14.759
program could beat a human
professional Go player.

00:09:14.759 --> 00:09:16.800
If you compared this to
chess, which is something

00:09:16.800 --> 00:09:20.124
that computer programs have been
doing very well for a while,

00:09:20.124 --> 00:09:21.540
the branching
factor of the number

00:09:21.540 --> 00:09:25.080
of possible moves at any
given time in chess is 20,

00:09:25.080 --> 00:09:28.710
and in Go it's 200.

00:09:28.710 --> 00:09:34.210
So a team in DeepMind
started creating

00:09:34.210 --> 00:09:38.040
an artificial intelligence
program that's called AlphaGo.

00:09:38.040 --> 00:09:42.030
And AlphaGo is actually
composed of two neural networks.

00:09:42.030 --> 00:09:45.630
So you have the Policy
Network and the Value Network.

00:09:45.630 --> 00:09:48.360
And the Policy
Network was initially

00:09:48.360 --> 00:09:55.170
trained using human games,
so we took thousands of games

00:09:55.170 --> 00:09:58.230
from an online Go Server
that we had access to,

00:09:58.230 --> 00:10:01.950
and trained this network
by watching those games

00:10:01.950 --> 00:10:05.630
and then trained it further
by playing against itself.

00:10:05.630 --> 00:10:07.800
And the objective of
the Policy Network

00:10:07.800 --> 00:10:10.230
is to give any
particular position,

00:10:10.230 --> 00:10:12.360
a particular
configuration of the board

00:10:12.360 --> 00:10:16.350
to say which positions are
most likely to be played next,

00:10:16.350 --> 00:10:19.560
and that allows you to narrow
down that branching factor.

00:10:19.560 --> 00:10:23.130
The other network, the Value
Network takes the configuration

00:10:23.130 --> 00:10:26.160
of the board, and what
it tries to estimate

00:10:26.160 --> 00:10:29.880
is how likely is
white or black to win.

00:10:29.880 --> 00:10:32.070
So instead of having
to actually play out

00:10:32.070 --> 00:10:34.120
the entirety of the game
to try to figure out

00:10:34.120 --> 00:10:36.330
who is most likely
to win, this allows

00:10:36.330 --> 00:10:39.960
us to cut down how deep you
need to go in order to explore

00:10:39.960 --> 00:10:42.840
the possibilities of the game.

00:10:42.840 --> 00:10:44.540
So a combination of
these two networks

00:10:44.540 --> 00:10:47.120
is what made
AlphaGo so powerful.

00:10:47.120 --> 00:10:49.850
But let me tell you a little
bit how the progression of this

00:10:49.850 --> 00:10:52.130
went.

00:10:52.130 --> 00:10:54.560
After creating this
network and training it,

00:10:54.560 --> 00:10:56.090
the AlphaGo team
actually managed

00:10:56.090 --> 00:10:59.870
to get a 100% success rate
against the best computer

00:10:59.870 --> 00:11:02.300
programs, but we know
that they were not

00:11:02.300 --> 00:11:07.850
as strong as even the level-one
professional Go players.

00:11:07.850 --> 00:11:11.570
So we got a professional
Go player, Fun Hui,

00:11:11.570 --> 00:11:15.590
who is the three-time European
Go champion and a two dan

00:11:15.590 --> 00:11:17.390
professional player.

00:11:17.390 --> 00:11:19.790
And he came to the
DeepMind office

00:11:19.790 --> 00:11:23.900
to play a five-game
match against AlphaGo.

00:11:23.900 --> 00:11:27.470
And AlphaGo actually won 5-0,
which was completely unexpected

00:11:27.470 --> 00:11:30.230
because it was the first
time a computer program could

00:11:30.230 --> 00:11:32.990
beat a professional player.

00:11:32.990 --> 00:11:37.070
And after that, we wanted
to see how much further it

00:11:37.070 --> 00:11:38.630
could actually go.

00:11:38.630 --> 00:11:43.970
So we actually scheduled a
match in Korea, in Seoul,

00:11:43.970 --> 00:11:44.810
against Lee Sedol.

00:11:44.810 --> 00:11:48.440
And Lee Sedol, for those
that don't know Go,

00:11:48.440 --> 00:11:50.900
is actually a legend
in the world of Go.

00:11:50.900 --> 00:11:53.285
He has won more
international competitions

00:11:53.285 --> 00:11:55.650
than I think only
one other person

00:11:55.650 --> 00:11:57.650
and is considered by
many to be the best

00:11:57.650 --> 00:12:00.500
Go player of the last decade.

00:12:00.500 --> 00:12:04.940
So the AlphaGo
team went to Korea,

00:12:04.940 --> 00:12:09.080
played on this match,
which was a huge event.

00:12:09.080 --> 00:12:11.810
It was everywhere in the
newspapers and the TV.

00:12:11.810 --> 00:12:14.670
It was watched by more people
than actually watched the Super

00:12:14.670 --> 00:12:16.760
Bowl in the US.

00:12:16.760 --> 00:12:19.730
And the fantastic thing to
me about these matches--

00:12:19.730 --> 00:12:21.530
once again, five matches--

00:12:21.530 --> 00:12:26.090
it's not just the end
result, but the fact

00:12:26.090 --> 00:12:32.300
that on the second game, AlphaGo
made a move, Move 37, that

00:12:32.300 --> 00:12:36.230
had a [? one ?] in 10,000
chance of being played,

00:12:36.230 --> 00:12:38.090
according to the Policy Network.

00:12:38.090 --> 00:12:40.670
So it was a very, very
super unlikely move, took

00:12:40.670 --> 00:12:42.290
the commentators by surprise.

00:12:42.290 --> 00:12:44.250
Many thought it was
actually a mistake.

00:12:44.250 --> 00:12:47.150
But actually turned
the game, and it was--

00:12:47.150 --> 00:12:49.670
Fan Hui, commentating
on that game

00:12:49.670 --> 00:12:53.960
said that it was actually a
beautiful, a beautiful move.

00:12:53.960 --> 00:12:57.810
And then in game 4, the
same thing happened,

00:12:57.810 --> 00:13:01.100
but Lee Sedol made
a move, once again,

00:13:01.100 --> 00:13:03.590
that AlphaGo didn't expect
because the Policy Network will

00:13:03.590 --> 00:13:07.670
say they had a 1 in 10,000
chance of being played.

00:13:07.670 --> 00:13:09.970
And people were
talking about that move

00:13:09.970 --> 00:13:13.610
as a touch of God, a beautiful,
beautiful move again.

00:13:13.610 --> 00:13:16.130
So the fantastic
thing about these

00:13:16.130 --> 00:13:19.010
matches to me is not so much
the result, but the fact

00:13:19.010 --> 00:13:23.450
that the computer and the human
professional Go player playing

00:13:23.450 --> 00:13:27.230
together actually reach
new levels of play

00:13:27.230 --> 00:13:29.630
and could actually come out
with these beautiful games

00:13:29.630 --> 00:13:31.430
by playing together.

00:13:31.430 --> 00:13:33.890
After this, we
actually had further Go

00:13:33.890 --> 00:13:37.610
summits, the Future of Go Summit
precisely to explore this,

00:13:37.610 --> 00:13:41.930
to explore how AlphaGo and human
players could play together,

00:13:41.930 --> 00:13:47.160
to bring the art of
Go to the next level.

00:13:47.160 --> 00:13:51.300
So they're both--

00:13:51.300 --> 00:13:54.310
I work with Atari and
I work with AlphaGo.

00:13:54.310 --> 00:13:56.060
It was done in the
open, like I mentioned,

00:13:56.060 --> 00:14:00.380
so we publish papers
for both for these

00:14:00.380 --> 00:14:01.670
and many other things.

00:14:01.670 --> 00:14:05.360
We're very lucky that
those two papers actually

00:14:05.360 --> 00:14:08.870
got onto the cover of "Nature,"
which within 18 months,

00:14:08.870 --> 00:14:11.570
it was the first time for
an artificial intelligence

00:14:11.570 --> 00:14:12.707
research organization.

00:14:12.707 --> 00:14:14.540
And let me tell you
about some of the things

00:14:14.540 --> 00:14:16.760
that we've done since then.

00:14:16.760 --> 00:14:19.059
So these are two
projects I think

00:14:19.059 --> 00:14:20.350
I'm particularly excited about.

00:14:20.350 --> 00:14:25.140
So one of them was
lip reading sentences,

00:14:25.140 --> 00:14:28.790
which, once again, managed to
push the boundary from being

00:14:28.790 --> 00:14:31.100
able to recognize
particular words,

00:14:31.100 --> 00:14:35.570
like a limited vocabulary to
just being able to recognize

00:14:35.570 --> 00:14:39.540
and have good results on
unbounded natural language,

00:14:39.540 --> 00:14:43.310
so being able to
recognize any speech.

00:14:43.310 --> 00:14:45.840
And there's also WaveNet.

00:14:45.840 --> 00:14:47.960
WaveNet, if you
never heard about it,

00:14:47.960 --> 00:14:49.460
I really encourage
you to go and see

00:14:49.460 --> 00:14:51.530
the blog post on
the DeepMind website

00:14:51.530 --> 00:14:54.110
where you can
actually hear samples

00:14:54.110 --> 00:14:58.850
that were generated by WaveNet
and are incredibly natural.

00:14:58.850 --> 00:15:01.670
It really changed the way
that we do text to speech

00:15:01.670 --> 00:15:03.680
because up to now
text to speech will

00:15:03.680 --> 00:15:06.860
stand by taking recordings
of a professional actor,

00:15:06.860 --> 00:15:09.170
of particular sounds
and then putting them

00:15:09.170 --> 00:15:11.110
back together to form a phrase.

00:15:11.110 --> 00:15:13.510
But WaveNet actually generates--

00:15:13.510 --> 00:15:15.860
if you know about how
digital audio encode those--

00:15:15.860 --> 00:15:18.354
16,000 samples per second.

00:15:18.354 --> 00:15:20.270
And what WaveNet does,
the neural network does

00:15:20.270 --> 00:15:22.820
is generates each
of those samples

00:15:22.820 --> 00:15:24.530
a particular step,
so it actually

00:15:24.530 --> 00:15:26.690
generates the audio wave.

00:15:26.690 --> 00:15:29.330
And the amazing thing
is that I managed

00:15:29.330 --> 00:15:33.890
to cut the gap between the
best text to speech systems

00:15:33.890 --> 00:15:37.130
available and a
human voice by 50%.

00:15:37.130 --> 00:15:41.360
So we're halfway
closer to reaching

00:15:41.360 --> 00:15:45.750
the level of a human speaking,
generating natural speech.

00:15:45.750 --> 00:15:48.800
And the great
thing about this is

00:15:48.800 --> 00:15:52.880
that I think it really shows
how these kind of algorithms--

00:15:52.880 --> 00:15:58.890
how reinforcement learning can
be adapted to new environments,

00:15:58.890 --> 00:16:02.600
new domains really easily
because they are more generic,

00:16:02.600 --> 00:16:08.300
because they're a step closer
to general intelligence.

00:16:08.300 --> 00:16:13.140
So that is the work that our
research arm has been doing,

00:16:13.140 --> 00:16:16.520
but let me tell you also
about how we're applying this.

00:16:16.520 --> 00:16:19.610
So this is a photo of
our Google Data Center,

00:16:19.610 --> 00:16:23.400
and in particular it's the
cooling area of the Google Data

00:16:23.400 --> 00:16:23.900
Center.

00:16:23.900 --> 00:16:26.230
They're very colorful
pipes actually coded

00:16:26.230 --> 00:16:30.036
for different water
flows to different areas.

00:16:30.036 --> 00:16:31.910
So as you can imagine,
our Google Data Center

00:16:31.910 --> 00:16:35.060
is already a very, very
optimized data center.

00:16:35.060 --> 00:16:39.500
It's some of the most efficient
data centers in the industry.

00:16:39.500 --> 00:16:42.797
And after DeepMind joined
forces with Google,

00:16:42.797 --> 00:16:45.380
it was natural to start working
with some of the Google teams.

00:16:45.380 --> 00:16:49.160
So a team from DeepMind started
working with the Google Data

00:16:49.160 --> 00:16:51.955
Center team to
create an agent that

00:16:51.955 --> 00:16:55.640
would have as inputs the sensor
readings from the data center,

00:16:55.640 --> 00:16:59.360
so things like the temperature,
the weather outside,

00:16:59.360 --> 00:17:02.480
the pressure in the cooling
system, how much power

00:17:02.480 --> 00:17:05.510
was being drawn by the servers.

00:17:05.510 --> 00:17:09.260
And the output was the
configuration parameters

00:17:09.260 --> 00:17:12.950
for the cooling system, things
like the water flow pressure,

00:17:12.950 --> 00:17:14.780
refined pressure.

00:17:14.780 --> 00:17:16.170
And the result of this--

00:17:16.170 --> 00:17:21.230
of applying this agent to this
very optimized data center

00:17:21.230 --> 00:17:26.989
was a 50% improved overall
efficiency over the entire data

00:17:26.989 --> 00:17:29.330
center, which was a
fantastic improvement

00:17:29.330 --> 00:17:32.510
in that system that was already
being heavily optimized.

00:17:32.510 --> 00:17:35.150
So we think this
shows how AI can

00:17:35.150 --> 00:17:37.160
be applied to
real-world problems,

00:17:37.160 --> 00:17:39.260
for example, to
reduce energy usage

00:17:39.260 --> 00:17:41.152
and reduce carbon emissions.

00:17:44.050 --> 00:17:48.160
The other applied project that
I'd like to tell you a little

00:17:48.160 --> 00:17:50.570
about is DeepMind Health.

00:17:50.570 --> 00:17:53.460
DeepMind Health is
a project I work on.

00:17:53.460 --> 00:17:56.620
And it was launched
in February of 2016

00:17:56.620 --> 00:18:01.390
as our first external
facing applied project,

00:18:01.390 --> 00:18:05.200
and we're really
excited about Health

00:18:05.200 --> 00:18:07.930
because we're excited
about the potential

00:18:07.930 --> 00:18:11.290
for digital technology
combined with better data

00:18:11.290 --> 00:18:13.810
to support clinicians
in transforming

00:18:13.810 --> 00:18:16.660
the way they do patient care.

00:18:16.660 --> 00:18:20.290
We've grown and we're now a team
of 70 clinicians and engineers

00:18:20.290 --> 00:18:22.520
and designers.

00:18:22.520 --> 00:18:27.230
But as we started to go
into the health space,

00:18:27.230 --> 00:18:30.600
the first thing we did
is we spent time on wards

00:18:30.600 --> 00:18:33.820
in hospitals, and
we realized how much

00:18:33.820 --> 00:18:36.250
was needed still to
be done to deliver

00:18:36.250 --> 00:18:39.850
the dream of the
paperless hospital.

00:18:39.850 --> 00:18:44.170
So things like pagers
or paper or pen

00:18:44.170 --> 00:18:47.500
were still very much part
of the day-to-day life

00:18:47.500 --> 00:18:49.105
for nurses and doctors.

00:18:51.790 --> 00:18:55.520
And you can contrast this what
your day-to-day life is like.

00:18:55.520 --> 00:18:58.910
So because of the services
that all of us creates,

00:18:58.910 --> 00:19:02.150
you have the information that
you need when you need it

00:19:02.150 --> 00:19:04.460
at any time in your pocket.

00:19:04.460 --> 00:19:06.980
So the first step
towards delivering

00:19:06.980 --> 00:19:10.660
AI-enabled technology is to
make sure the doctors and nurses

00:19:10.660 --> 00:19:15.100
have the latest digital tools.

00:19:15.100 --> 00:19:18.010
So we created this
project called Streams.

00:19:18.010 --> 00:19:20.770
And Streams is a
mobile app that doesn't

00:19:20.770 --> 00:19:23.620
have artificial
intelligence built in.

00:19:23.620 --> 00:19:27.370
And what we aim to do with
it is to support clinicians

00:19:27.370 --> 00:19:30.290
in their existing
workflows by, for example,

00:19:30.290 --> 00:19:32.710
sending alerts to
a specialist when

00:19:32.710 --> 00:19:35.392
we detect that a
patient is deteriorating

00:19:35.392 --> 00:19:36.850
and then making
sure that they have

00:19:36.850 --> 00:19:41.500
access to the relevant
information when they need it.

00:19:41.500 --> 00:19:46.180
So the idea here is
to help clinicians

00:19:46.180 --> 00:19:50.050
get patients from their tests
to their treatment in a matter

00:19:50.050 --> 00:19:52.720
of minutes instead of hours.

00:19:52.720 --> 00:19:54.880
And at the same time,
in parallel to this,

00:19:54.880 --> 00:19:57.590
we're also using
artificial intelligence

00:19:57.590 --> 00:19:59.879
to help with cutting-edge
medical research.

00:19:59.879 --> 00:20:01.670
And there's a couple
of projects that we're

00:20:01.670 --> 00:20:05.030
working on here, one of
them with the Moorfields Eye

00:20:05.030 --> 00:20:09.230
Hospital, which we're
working on retinal scans that

00:20:09.230 --> 00:20:14.320
can detect diabetes related
or macular degeneration.

00:20:14.320 --> 00:20:16.217
And the thing about
this area, why

00:20:16.217 --> 00:20:17.800
it's so exciting to
be working on this

00:20:17.800 --> 00:20:20.630
is even though
there's a 10 to 5--

00:20:20.630 --> 00:20:23.570
if you have diabetes, 10
to 5 times more likely

00:20:23.570 --> 00:20:27.350
to suffer sight loss,
98% of the sight loss

00:20:27.350 --> 00:20:31.330
is preventable if it's
detected early enough.

00:20:31.330 --> 00:20:34.780
And we hope that we can
help with that detection.

00:20:34.780 --> 00:20:37.460
We're also working with the
University College London

00:20:37.460 --> 00:20:41.420
Hospital on radiotherapy
planning for head and neck

00:20:41.420 --> 00:20:42.420
cancers.

00:20:42.420 --> 00:20:43.950
So head and neck
cancer treatment

00:20:43.950 --> 00:20:46.910
are very tricky because there's
a lot of things going on there

00:20:46.910 --> 00:20:49.760
and the treatment needs to be
planned very, very accurately.

00:20:49.760 --> 00:20:52.080
That means that in order
to treat one patient,

00:20:52.080 --> 00:20:55.490
a radiotherapist needs
to spend about four hours

00:20:55.490 --> 00:20:57.690
to plan each of the treatments.

00:20:57.690 --> 00:21:00.260
And there's already
a huge shortage

00:21:00.260 --> 00:21:02.570
of radiotherapists in the UK.

00:21:02.570 --> 00:21:05.570
So we hope that
we can reduce this

00:21:05.570 --> 00:21:10.100
from a very, very lengthy task
to a much shorter verification

00:21:10.100 --> 00:21:12.460
task.

00:21:12.460 --> 00:21:15.250
We have a lot more information
about this on our web site,

00:21:15.250 --> 00:21:17.570
deepmind.com/health.

00:21:17.570 --> 00:21:19.920
And if you have any
questions about all this,

00:21:19.920 --> 00:21:22.480
send us an email,
sayhi@deepmindhealth.com.

00:21:25.990 --> 00:21:27.911
And that's all I had for today.

00:21:27.911 --> 00:21:29.410
If you have any
questions about this

00:21:29.410 --> 00:21:31.700
and you want to talk
about any of that,

00:21:31.700 --> 00:21:35.720
find me after the talk,
send me a message.

00:21:35.720 --> 00:21:38.450
And if you're interested
in what we're doing,

00:21:38.450 --> 00:21:42.210
also check out our website,
deepmind.com/careers.

00:21:42.210 --> 00:21:43.190
Thank you.

00:21:43.190 --> 00:21:44.390
[APPLAUSE]

00:21:44.390 --> 00:21:46.540
[MUSIC PLAYING]

