WEBVTT

1
00:00:00.140 --> 00:00:03.510
That's happen.
The Joe Rogan experience.
What's Matt?

2
00:00:04.290 --> 00:00:05.910
I don't remember if we brought this up last time,

3
00:00:05.911 --> 00:00:08.550
but I just remembered seeing this video where you're playing guitar while you

4
00:00:08.551 --> 00:00:10.800
were driving.
Yup.
Well you shouldn't do that,
dude.

5
00:00:11.600 --> 00:00:16.080
There's a reason why he's doing it.
Why are you doing this on a test track?
Oh,

6
00:00:16.290 --> 00:00:20.310
what kind of cars that look see at Lincoln Lincoln Mkc yes.
Oh they do that.

7
00:00:20.370 --> 00:00:23.670
The Lincolns do that.
We converted it and we,

8
00:00:23.671 --> 00:00:28.500
that's The r code controlling the car.
Wow.
And I'm done.
It's crazy.

9
00:00:28.670 --> 00:00:33.570
Yep.
So you converted this car to drive autonomously anonymously.

10
00:00:33.571 --> 00:00:36.270
Yeah.
Wow.
And what,

11
00:00:36.450 --> 00:00:41.450
what exactly do you have to do to a car to change like car,

12
00:00:41.850 --> 00:00:43.800
because that car does not have the capacity,

13
00:00:43.830 --> 00:00:48.790
the capacity to do anything like that.
So the right my correct.
No,

14
00:00:48.850 --> 00:00:53.400
no,
no,
no,
absolutely not.
But you are absolutely correct.
The,

15
00:00:53.520 --> 00:00:56.310
there's the first part is being able to control the car with a computer,

16
00:00:56.340 --> 00:01:00.250
which is converting it to be drive by wire so you can control the steering and

17
00:01:00.251 --> 00:01:04.380
the braking acceleration to,
to basically be able to control with a joystick.

18
00:01:04.440 --> 00:01:08.190
And then you have to put laser sensors all around the cars that we do.

19
00:01:08.210 --> 00:01:10.480
10 a sensor and a software.

20
00:01:10.690 --> 00:01:14.760
What's the best kind of sensor is that optical laser.
A lot of debate on this.

21
00:01:14.761 --> 00:01:18.600
And this is the big,
this is the throwdown between Ilan Musk and everybody else.

22
00:01:18.920 --> 00:01:22.980
So y'all Musk says the best sensors,
camera,
everybody else.
Well,

23
00:01:23.370 --> 00:01:27.710
everybody else says that at this time.
Lidar,
which are these lasers?
Yes,

24
00:01:27.770 --> 00:01:28.680
this is the best sensor.

25
00:01:28.980 --> 00:01:33.980
So I'm more on the side in this case on camera,

26
00:01:34.140 --> 00:01:38.590
on Eli Musk.
So here's the difference.
Lasers are more precise.
They worked better.

27
00:01:38.591 --> 00:01:42.170
And in poor lighting conditions,
they're more reliable.

28
00:01:42.330 --> 00:01:45.540
You can actually build safe systems today that use lidar.

29
00:01:46.560 --> 00:01:50.520
The problem is that they don't have,
they're much information.

30
00:01:50.670 --> 00:01:55.440
So we use our eyes to drive and uh,
cameras,
the same thing.

31
00:01:55.830 --> 00:01:58.110
And they have just a lot more information.

32
00:01:58.320 --> 00:02:00.840
So if you're going to build artificial intelligence systems,

33
00:02:00.841 --> 00:02:04.740
so the machine learning systems that learn from huge amounts of data cameras,

34
00:02:04.741 --> 00:02:08.100
the way to go because you can learn so much more,
you can see so much more.

35
00:02:08.490 --> 00:02:12.180
So the,
the richer,
deeper censor his camera,

36
00:02:12.330 --> 00:02:16.110
but it's much harder.
You have to collect a huge amount of data.

37
00:02:16.290 --> 00:02:19.320
It's a little bit more futuristic.
So it's longer term solution.

38
00:02:19.530 --> 00:02:23.550
So today to build a safe vehicle,
you have to go lidar tomorrow,

39
00:02:24.180 --> 00:02:28.530
however you define tomorrow,
he almost says it's in a year.
Others say it's five,

40
00:02:28.531 --> 00:02:33.340
10,
20 years cameras,
the way to go.
So that's,
that's the,
the hard debate.

41
00:02:33.340 --> 00:02:36.150
And there's,
there's a lot of other debates,
but that's one of the core ones.

42
00:02:36.440 --> 00:02:40.950
It's basically four camera.
You,
if you go camera,
like you're doing the Tesla,

43
00:02:40.980 --> 00:02:45.330
there's seven cameras in your Tesla three looking forward,

44
00:02:45.331 --> 00:02:49.630
there's all around.
So on one looking inside,
no,
you have the model s yeah,
yeah.

45
00:02:49.650 --> 00:02:53.220
So that one doesn't have a camera that's looking inside.
So it's all,

46
00:02:53.310 --> 00:02:56.790
all cameras plus radar and ultrasonic sensors.

47
00:02:57.810 --> 00:03:01.420
That approach requires collecting huge amounts of data and they're doing that.

48
00:03:01.480 --> 00:03:06.480
They drove now about 1.3 billion miles under autopilot.

49
00:03:07.150 --> 00:03:11.320
Cheeses as it's a,
it's a very large amount of data.

50
00:03:11.321 --> 00:03:12.770
So you're talking about,
uh,

51
00:03:12.940 --> 00:03:17.940
over 500,000 vehicles have autopilot 450.

52
00:03:18.911 --> 00:03:21.760
I think thousand have the new version of autopilot,

53
00:03:21.790 --> 00:03:25.870
autopilot to which is the one you're driving.
And all of that is data.

54
00:03:26.110 --> 00:03:28.870
So all of those,
all the edge cases,
what they call them,

55
00:03:28.871 --> 00:03:33.790
all the difficult situations that occur is feeding the machine learning system

56
00:03:34.180 --> 00:03:36.400
to become better and better and better.

57
00:03:36.640 --> 00:03:41.470
And the open question is how much better does need to get to get to the human

58
00:03:41.471 --> 00:03:45.550
level performance?
And like one of the big thing,

59
00:03:46.000 --> 00:03:48.500
one of the,
uh,
big assumption,

60
00:03:48.501 --> 00:03:52.210
so I seen my beings is that we think that driving is actually pretty easy.

61
00:03:52.870 --> 00:03:56.710
And we think that humans suck a driving those two assumptions.

62
00:03:56.711 --> 00:04:00.430
We think like driving,
you know,
you stay in the lane,
you stop at the stop sign,

63
00:04:00.760 --> 00:04:02.350
it's pretty easy to automate.

64
00:04:02.620 --> 00:04:07.620
And then the other one is you think like humans are terrible drivers and so

65
00:04:07.960 --> 00:04:10.960
there'll be easy to build a machine that outperforms humans at driving.

66
00:04:11.440 --> 00:04:13.210
Now there's,
that's a,

67
00:04:13.240 --> 00:04:18.240
I think there's a lot of flaws behind that intuition we take for granted how

68
00:04:18.251 --> 00:04:22.660
hard it is to look at the scene.
Like everything you just did picked up,

69
00:04:22.661 --> 00:04:23.920
moved around some objects.

70
00:04:24.670 --> 00:04:28.600
It's really difficult to build an artificial intelligence system that does that,

71
00:04:28.840 --> 00:04:32.230
to be able to perceive and understand the seen enough to understand the physics

72
00:04:32.231 --> 00:04:36.790
of the scene.
Like all these objects that it like how to pick them up,

73
00:04:36.850 --> 00:04:38.080
the texture of those objects,

74
00:04:38.081 --> 00:04:43.060
the weight to understand glasses folded and unfolded,
open water bottle,

75
00:04:43.150 --> 00:04:47.110
all those things is common sense knowledge that we take for granted.

76
00:04:47.380 --> 00:04:48.520
We think it's trivial,

77
00:04:48.790 --> 00:04:52.390
but there is no artificial system in the world today.

78
00:04:52.660 --> 00:04:57.660
Nor will there be for perhaps quite awhile that can reason do that kind of

79
00:04:57.791 --> 00:05:02.230
common sense.
Reasoning about the physical world.
Add to that,
uh,

80
00:05:02.920 --> 00:05:07.450
pedestrians.
So add some crazy people in this room right now to the whole scene.

81
00:05:07.640 --> 00:05:10.820
<v 1>Ryan,
being able to notice like this guy is an asshole.
Look on him.
Was he doing,</v>

82
00:05:10.850 --> 00:05:13.640
what is he doing?
Get off that skateboard.
Oh,
Jesus isn't in traffic.
Yeah.
Yup.

83
00:05:13.820 --> 00:05:18.320
<v 0>And the considering not that he's an asshole,
he's a respectable skateboarder.</v>

84
00:05:20.660 --> 00:05:25.430
It that in order to make him behave a certain way,

85
00:05:25.610 --> 00:05:27.380
you yourself have to behave a certain way.

86
00:05:27.381 --> 00:05:29.840
So it's not just you have to perceive the world.

87
00:05:30.140 --> 00:05:34.100
You have to act in a way that you have to assert your presence in this world.

88
00:05:34.220 --> 00:05:35.900
You have to take risks.

89
00:05:36.320 --> 00:05:38.900
So in order to make the skateboard or not cross the street,

90
00:05:38.901 --> 00:05:42.830
you have to perhaps accelerate if you have the right away.
And these are the,

91
00:05:42.831 --> 00:05:45.980
there's a game theoretic end game of chicken to get right.

92
00:05:46.120 --> 00:05:49.970
I mean what do we even know how to approach that as a,
as uh,

93
00:05:50.030 --> 00:05:54.800
artificial intelligence sort of research community and also as a society do we

94
00:05:54.801 --> 00:05:59.801
want an autonomous vehicle that speeds up in order to make a pedestrian not

95
00:06:00.351 --> 00:06:03.620
crossed the street,
which is what we do all the time.

96
00:06:04.010 --> 00:06:07.940
We have to assert our presence.
If there's a,

97
00:06:07.941 --> 00:06:10.760
if there's a person who doesn't have the right away,
who could begins crossing,

98
00:06:11.030 --> 00:06:15.020
we're going to either maintain speed or speed up potentially if we want them to

99
00:06:15.021 --> 00:06:19.160
not cross so that,
that game there to get that right.

100
00:06:19.190 --> 00:06:20.990
It's a dangerous game for a robot.

101
00:06:21.110 --> 00:06:24.830
It's for robot and for us to be rationally,

102
00:06:25.640 --> 00:06:27.860
if that,
God forbid,

103
00:06:27.861 --> 00:06:32.200
least a fatality for us as a society to rationally reason about that.

104
00:06:32.210 --> 00:06:33.043
Think about that.

105
00:06:33.140 --> 00:06:36.740
I mean a fatality like that could basically bankrupt the company.

106
00:06:36.830 --> 00:06:40.680
There's a lawsuit going on right now,
um,
about,
uh,
uh,

107
00:06:40.940 --> 00:06:44.930
an accident in northern California with the Tesla.
Yeah.

108
00:06:45.800 --> 00:06:47.720
And Are you aware about,
yeah,
that one.
Yeah,

109
00:06:47.721 --> 00:06:51.020
this was the circumstances about that one.
So there was a,

110
00:06:51.021 --> 00:06:53.270
I believe in mountain view,

111
00:06:53.620 --> 00:06:56.450
a fatality in a Tesla where it,

112
00:06:57.320 --> 00:07:01.310
this is a common problem for all,
all link keeping systems.
Like,

113
00:07:01.670 --> 00:07:03.980
like that's a lot of pilot is a,
those,

114
00:07:04.010 --> 00:07:08.720
a divider in the highway and basically the car was driving,
you know,

115
00:07:08.730 --> 00:07:13.160
along lane and then the car in front moved to an adjacent lane and this divider

116
00:07:13.161 --> 00:07:14.270
appeared,
right?

117
00:07:14.630 --> 00:07:19.190
So you have to now steer to the right and the car didn't end once straight into

118
00:07:19.191 --> 00:07:23.270
the divider.
Oh Wow.
And it's,
you know,
uh,
the,

119
00:07:23.271 --> 00:07:27.740
basically what that boils down to is the car drifted out of lane,
right?

120
00:07:28.010 --> 00:07:31.970
Or it didn't adjust properly to the lane.
And those kinds of things happen.

121
00:07:32.630 --> 00:07:36.530
And this is because the person was allowing the autopilot to do everything

122
00:07:38.320 --> 00:07:41.120
that you can't.
So we have to be extremely careful here.

123
00:07:41.121 --> 00:07:43.700
I don't know that really deep details of the case.

124
00:07:43.701 --> 00:07:46.070
I'm not sure exactly how many people do.

125
00:07:46.071 --> 00:07:47.580
So there's a judgment on what the people,

126
00:07:47.630 --> 00:07:51.380
the person was doing and then there's analysis of what the system did,
right.

127
00:07:51.500 --> 00:07:56.090
The system did,
it drifted out of lane.
And the question is did the person,

128
00:07:56.120 --> 00:07:59.900
was the person paying attention and was there enough time given for the person

129
00:07:59.901 --> 00:08:03.560
to take over and if they were paying attention to catch the vehicle,

130
00:08:03.650 --> 00:08:08.480
steer back onto the road.
As far as I believe,

131
00:08:08.850 --> 00:08:13.610
uh,
the only information they have his hands on the steering wheel and they were

132
00:08:13.611 --> 00:08:17.930
saying that like half the half the minute leading up to the crash,

133
00:08:18.260 --> 00:08:20.720
the hands weren't on the steering wheel or something like that.

134
00:08:21.020 --> 00:08:24.110
Basically trying to infer where the person paying attention or not,

135
00:08:24.111 --> 00:08:28.280
but we don't have the information exactly what wa where were their eyes?

136
00:08:29.000 --> 00:08:33.400
You can only make guesses as far as I know.
Again,
so the,

137
00:08:33.470 --> 00:08:34.580
the question is,

138
00:08:34.880 --> 00:08:38.290
this is the eyes on the road thing because I think I've heard Janet podcast

139
00:08:38.291 --> 00:08:43.130
saying you're attempted to sort of look off the road with your new Tesla or at

140
00:08:43.131 --> 00:08:46.150
least become a little bit complacent.
That's the worry,
the worry,

141
00:08:46.160 --> 00:08:50.570
the worry is that you,
you just rely on the thing that you would relax too much.

142
00:08:51.290 --> 00:08:55.250
But what would that relaxation lead to that the problem is if something
happened,

143
00:08:55.500 --> 00:08:56.333
if you weren't,

144
00:08:56.670 --> 00:08:58.680
<v 1>you know when you're driving,</v>

145
00:08:58.770 --> 00:09:03.180
I mean we've discussed this many times on the podcast to the reason why people

146
00:09:03.181 --> 00:09:04.014
have road rage.

147
00:09:04.320 --> 00:09:07.800
One of the reasons is cause you're in a heightened state because cars are flying

148
00:09:07.801 --> 00:09:12.630
around you and your brain is prepared to make split second decisions and moves

149
00:09:13.410 --> 00:09:18.240
and the worry is that you would relax that because you're so comfortable with

150
00:09:18.241 --> 00:09:21.600
that thing driving everybody that I know that it's tried that they say you get

151
00:09:21.601 --> 00:09:23.010
really used to it doing that.

152
00:09:23.210 --> 00:09:25.470
You get really used to it just driving around for you.

153
00:09:26.030 --> 00:09:29.210
<v 0>So the question is what be,
what happens when you get used to it?</v>

154
00:09:29.430 --> 00:09:31.820
You start looking at off road,
do you start texting more?

155
00:09:31.821 --> 00:09:34.760
Do you start watching a movie?
It's that uh,
that's a,

156
00:09:34.761 --> 00:09:39.620
that's a really an open question.
And the like for example,

157
00:09:39.621 --> 00:09:41.440
we just did the study,
uh,

158
00:09:41.630 --> 00:09:46.630
the published a study from the MIT on what people in our Dataset we have,

159
00:09:47.570 --> 00:09:50.920
we'll collect this data set of 300,000 miles and Tesla's,

160
00:09:50.921 --> 00:09:54.530
we instrumented all these Teslas and watch what people are actually doing and

161
00:09:54.531 --> 00:09:57.830
are they,
uh,
paying attention when they disengage the system.

162
00:09:57.920 --> 00:10:00.080
So there's a really important moment here.

163
00:10:00.110 --> 00:10:04.730
We have 18,000 of those when the person catches the car,

164
00:10:05.240 --> 00:10:08.150
you know,
the disengage autopilot.
And that's a really,

165
00:10:08.570 --> 00:10:10.310
Tesla uses this moment as well.

166
00:10:10.490 --> 00:10:14.030
That's a really important window into difficult cases.

167
00:10:14.210 --> 00:10:16.490
So some percentage of those,
some small percentage,

168
00:10:16.510 --> 00:10:19.220
about 10% is we call them.

169
00:10:19.580 --> 00:10:23.750
Tricky situations is situations where you have to immediately respond,

170
00:10:23.810 --> 00:10:27.140
like drifting out of lane if there's a stopped car in front.
So on,

171
00:10:27.830 --> 00:10:30.800
the question is,
are people paying attention during those moments?

172
00:10:30.950 --> 00:10:33.230
So in our Dataset,
they were paying attention.

173
00:10:33.380 --> 00:10:36.800
There were still remaining vigilant.
Now in our Dataset,

174
00:10:37.190 --> 00:10:39.590
the autopilot was going on quote,

175
00:10:39.920 --> 00:10:44.210
encountering tricky situations every 9.2 miles.

176
00:10:44.240 --> 00:10:48.890
So you could say it was a failing every 9.2 miles.

177
00:10:49.060 --> 00:10:53.510
Like that is one of the reasons we believe that people are still paying it,

178
00:10:54.410 --> 00:10:55.430
remaining vigilant,

179
00:10:55.670 --> 00:11:00.670
that it's regularly and unpredictably sort of drifting out of the lane or

180
00:11:01.160 --> 00:11:05.810
misbehaving so you don't overtrust it.
You don't become too complacent.

181
00:11:06.440 --> 00:11:10.160
The open question is when it becomes better and better and better and better,

182
00:11:10.550 --> 00:11:14.960
will you start becoming complacent when it drives on the highway for an hour,

183
00:11:14.970 --> 00:11:19.550
an hour and a half,
and as opposed to 9.2 miles,
make that 50 miles,

184
00:11:19.551 --> 00:11:23.990
60 miles.
Do you start to overtrust it and that's a really open question,

185
00:11:24.830 --> 00:11:29.180
<v 1>do you think or do you anticipate a time in anywhere in the near future where</v>

186
00:11:29.181 --> 00:11:31.100
you won't have to correct.

187
00:11:31.340 --> 00:11:36.050
You will allow the car to do it because the car will be perfect.
The car,

188
00:11:36.140 --> 00:11:39.140
first of all,
we'll never be perfect.
No car will ever be perfect.

189
00:11:39.170 --> 00:11:41.150
Autonomous Vehicles will always,

190
00:11:41.151 --> 00:11:46.130
you think require at least some sort of manual override.
Yeah,

191
00:11:46.580 --> 00:11:50.030
really.
That's interesting that you're saying that because you work in AI,

192
00:11:50.360 --> 00:11:54.040
like what makes you think that that's impossible to

193
00:11:54.040 --> 00:11:58.870
<v 0>achieve?
Well,
let's,
let's talk cause cause you're using the word perfection.</v>

194
00:11:58.871 --> 00:12:02.210
I think.
Perfection.
That's a bad word.
Yeah.
So you're,

195
00:12:02.290 --> 00:12:04.690
I guess you're implying that,
let me see.
Will it achieve,

196
00:12:04.691 --> 00:12:06.400
because people are obviously not perfect yet.

197
00:12:06.520 --> 00:12:11.520
Will it achieve a state of competence that exceeds the human being and let's uh,

198
00:12:15.410 --> 00:12:20.240
put it in a dark way,
uh,
competence measured by fatal crashes.
Yes.

199
00:12:20.930 --> 00:12:25.220
Uh,
yes.
I absolutely believe so.
Uh,
and perhaps in the near term,

200
00:12:25.580 --> 00:12:29.390
near term,
like five years.
Yup.
For me,
five,

201
00:12:29.391 --> 00:12:34.391
10 years is near term for Ilan in Ilan Musk time that converted to one year.

202
00:12:36.710 --> 00:12:40.640
Have you met him?
Yes.
Interviewed him recently.
Fascinating cat,
right?

203
00:12:41.450 --> 00:12:44.170
Yup.
Got a lot of weird shit bouncing around behind those eyeballs.

204
00:12:45.280 --> 00:12:47.380
You don't realize until you talk to them in person,
you're like,

205
00:12:47.440 --> 00:12:52.060
oh you got a lot going on in there,
man.
Yeah.
There's this passion,
this drive.

206
00:12:52.090 --> 00:12:55.980
I mean it's one of the hurricane of ideas.
Yeah.

207
00:12:56.820 --> 00:13:01.320
And a focus and confidence.
I mean,

208
00:13:01.560 --> 00:13:04.410
the thing is in a lot of the things he does,

209
00:13:04.440 --> 00:13:08.640
which I admire greatly from any man or a woman innovator,

210
00:13:09.090 --> 00:13:10.560
it's just boldly,

211
00:13:10.980 --> 00:13:15.510
fearlessly pursuing new ideas or jumping off the cliff and learning to fly in

212
00:13:15.511 --> 00:13:20.280
the way down that that's,
I mean,
well,
no matter what happens,

213
00:13:20.281 --> 00:13:22.590
he'll be remembered as the great innovators of our time.

214
00:13:23.490 --> 00:13:27.600
Whatever you say may be in my book,
Steve Jobs was as well.

215
00:13:27.750 --> 00:13:28.890
Even if you criticize,

216
00:13:28.891 --> 00:13:32.880
perhaps he hasn't contributed significantly to the technological development of

217
00:13:32.881 --> 00:13:34.830
the company or the different ideas they did.

218
00:13:35.160 --> 00:13:38.610
Still his brilliance was in all the products of iPhone,

219
00:13:39.090 --> 00:13:43.980
of the personal computer,
the Mac and so on.
And I think the same is true with,
uh,

220
00:13:44.460 --> 00:13:47.340
with uh,
with Ilan and yes,
there's,

221
00:13:47.700 --> 00:13:52.500
in this space of autonomous vehicles,
of,
of semi autonomous vehicles,

222
00:13:52.501 --> 00:13:53.940
of driver assistance systems,

223
00:13:54.870 --> 00:13:57.810
it's a pretty tense space to operate in.

224
00:13:58.560 --> 00:14:03.120
There's several communities in there that are very responsible but also

225
00:14:03.121 --> 00:14:07.710
aggressive in their criticism.
So in driving in the automotive sector,

226
00:14:07.770 --> 00:14:12.770
obviously since Henry Ford and before there's been a culture of safety,

227
00:14:13.800 --> 00:14:15.420
of just great engineering.

228
00:14:15.960 --> 00:14:19.020
These are like some of the best engineers in the world in terms of large scale

229
00:14:19.021 --> 00:14:22.380
production.
You talk about Toyota,
you talk about Ford,
GM,

230
00:14:22.980 --> 00:14:27.780
these people know how to do safety well and so care comes he along with silicon

231
00:14:27.781 --> 00:14:32.781
valley ideals that throws a lot of it out the window and says we're going to

232
00:14:33.150 --> 00:14:36.660
revolutionize the way we do automation in general.

233
00:14:36.810 --> 00:14:41.370
We'll go into make software updates to the car once a week,

234
00:14:41.371 --> 00:14:43.920
twice a week over the air.
Just like that.

235
00:14:44.670 --> 00:14:48.750
That makes people in the safety engineers and human factors engineers really

236
00:14:48.751 --> 00:14:49.584
uncomfortable.

237
00:14:49.860 --> 00:14:53.180
Like what do you mean you're going to keep updating the software of the car

238
00:14:53.810 --> 00:14:56.510
without like how are you testing it?
All right,

239
00:14:56.540 --> 00:14:59.270
that makes people really uncomfortable.
Why does it make them uncomfortable?

240
00:14:59.570 --> 00:15:02.930
Because the way in the automotive sector,
you test the system,

241
00:15:03.170 --> 00:15:06.320
you come up with a design of the car,
every component,

242
00:15:06.710 --> 00:15:10.100
and then you go through like really rigorous testing before it ever hits the

243
00:15:10.101 --> 00:15:13.200
road.
Right?
Here's an idea from an,

244
00:15:13.750 --> 00:15:16.220
the Tesla side is where they basically,

245
00:15:17.000 --> 00:15:20.180
they in shadow moe test the software,
but then they just release it.

246
00:15:20.390 --> 00:15:25.390
So essentially the drivers become the testing and then they regularly update it

247
00:15:26.180 --> 00:15:28.910
to,
uh,
to,
uh,
to adjust a,

248
00:15:28.911 --> 00:15:32.840
if any issues arise that makes people uncomfortable because there's not a

249
00:15:32.841 --> 00:15:35.900
standardized testing procedure.
There's not,

250
00:15:36.140 --> 00:15:41.060
there's not at least the feeling in the industry of rigor because the reality is

251
00:15:41.061 --> 00:15:44.320
we don't know how to test software in the same kind of,

252
00:15:44.321 --> 00:15:46.520
of with the same kind of rigor that we've tasted,

253
00:15:46.521 --> 00:15:49.260
the automotive system tested automotive system in the past.

254
00:15:49.550 --> 00:15:54.550
So I think it's extremely exciting and powerful to make software sort of

255
00:15:56.271 --> 00:16:01.070
approach,
uh,
automotive engineering with,

256
00:16:01.220 --> 00:16:03.590
at least in part a software engineering perspective.

257
00:16:03.770 --> 00:16:07.220
So just doing what's made silicon valley successful.

258
00:16:07.670 --> 00:16:11.390
So updating regularly,
aggressively innovating on the software side.

259
00:16:11.720 --> 00:16:15.160
So your Tesla over the air while we're sitting here could get a tool and you

260
00:16:15.161 --> 00:16:20.090
update the flip of a,
uh,
of a bit as Elon Musk says,

261
00:16:20.390 --> 00:16:24.200
uh,
it can be,
it can gain all new capabilities.

262
00:16:24.560 --> 00:16:29.210
That's really exciting,
but that's also dangerous and that,
that balance,

263
00:16:29.420 --> 00:16:34.040
we,
uh,
what's dangerous about it?
That'd be faulty software faulty a bug.

264
00:16:34.340 --> 00:16:38.540
So if you're,
you're the apps on your phone,
you know,

265
00:16:38.541 --> 00:16:39.470
fail all the time.

266
00:16:39.710 --> 00:16:44.600
Where as a society used to software failing and we just kind of reboot the

267
00:16:44.601 --> 00:16:46.760
device where we start the APP a,

268
00:16:46.770 --> 00:16:51.200
the most complex software systems in the world today.

269
00:16:51.830 --> 00:16:54.500
If we think outside of nuclear engineering and so on,

270
00:16:54.950 --> 00:16:59.030
they're really nobody that they're too complex to really thoroughly tests.

271
00:16:59.060 --> 00:17:01.910
So a thorough,

272
00:17:01.911 --> 00:17:06.290
complete testing proving that the software is safe is nearly impossible.

273
00:17:06.291 --> 00:17:10.730
I'm most software systems that that's,
that's uh,

274
00:17:10.760 --> 00:17:15.190
that's nerve wracking too.
A lot of people because,
uh,
this,

275
00:17:15.250 --> 00:17:19.130
there's no way to prove that the new software update is safe.

276
00:17:19.320 --> 00:17:22.920
<v 1>So what is,
what is the process like,
do you know,</v>

277
00:17:22.950 --> 00:17:25.620
like how they create software,

278
00:17:25.860 --> 00:17:28.950
they update it and then they tested on something?

279
00:17:29.040 --> 00:17:30.990
How much testing do they do and what,

280
00:17:30.991 --> 00:17:34.020
how much did they do before they upload it to your car?

281
00:17:34.450 --> 00:17:36.290
<v 0>Yeah,
so I don't have any inside information,</v>

282
00:17:36.291 --> 00:17:40.580
but I have a lot of sort of public available information,
which is,
uh,

283
00:17:40.940 --> 00:17:43.340
they,
uh,
they test the software in shadow mode,

284
00:17:43.670 --> 00:17:48.670
meaning they see how the new software compares to the current software by it in

285
00:17:49.861 --> 00:17:54.260
parallel on the cars and seeing if there's disagreements,
if like,
uh,

286
00:17:54.300 --> 00:17:58.410
seeing if there's any major disagreements and bringing those up and seeing what

287
00:17:58.830 --> 00:18:02.880
parallel,
I'm sorry.
Do you mean both programs running at the same time?

288
00:18:04.500 --> 00:18:07.320
One,
the original opt?
Yes.
At the same time,

289
00:18:07.321 --> 00:18:12.321
the original update actually controlling the car and the new update is,

290
00:18:12.901 --> 00:18:15.810
uh,
just making the same decision,

291
00:18:15.811 --> 00:18:20.040
making the same decisions without them being,
without showing the actual okay.

292
00:18:20.370 --> 00:18:23.610
Without actually affecting the vehicles dynamics.
And so that's,

293
00:18:23.640 --> 00:18:25.410
that's a really powerful way of testing.

294
00:18:25.680 --> 00:18:30.270
I think the software infrastructure that Tesla's built allows for that.

295
00:18:30.690 --> 00:18:34.410
And I think other companies should do the same.
That's a really exciting,

296
00:18:34.411 --> 00:18:38.460
powerful way to approach not just a automation,

297
00:18:38.490 --> 00:18:40.650
not just the timeless vehicles or send me a town's vehicles,

298
00:18:40.651 --> 00:18:45.210
but just safety is basically all the data that's on cars.

299
00:18:45.720 --> 00:18:50.720
Bring it back to a central point to where you can use the edge cases,

300
00:18:50.761 --> 00:18:55.290
all the weird situations in driving to improve the system,
to test the system,

301
00:18:55.620 --> 00:19:00.180
to learn to understand where the cars used misused,

302
00:19:00.450 --> 00:19:03.570
how can be improved and so on.
That's an extremely powerful,

303
00:19:03.660 --> 00:19:08.300
how many people do they have that are analyzing all this data?
It's a,

304
00:19:08.310 --> 00:19:11.240
it's a really good question.
So they have to do,

305
00:19:11.520 --> 00:19:14.850
the interesting thing about driving is most of it is pretty boring.

306
00:19:14.910 --> 00:19:19.530
Nothing interesting happens.
So they have automated ways of extracting,

307
00:19:20.130 --> 00:19:21.630
again,
what are called edge cases.

308
00:19:21.631 --> 00:19:25.620
So these weird moments of driving and once you have these weird moments,

309
00:19:25.621 --> 00:19:29.490
they have people annotate it.
I don't know what the number is,

310
00:19:29.491 --> 00:19:31.050
but a lot of companies are doing this.

311
00:19:31.110 --> 00:19:35.520
It's in the hundreds and the thousands basically have humans annotate the data

312
00:19:35.521 --> 00:19:36.420
to see what happened.

313
00:19:36.720 --> 00:19:40.860
But most of what they're trying to do is to automate that annotation.

314
00:19:41.490 --> 00:19:44.670
So to figure out how the da Da da,

315
00:19:44.690 --> 00:19:48.060
it can be automatically used to improve the system.
So they,
they have,

316
00:19:48.150 --> 00:19:51.230
they have methods for that because it's a huge amount of data.
Right.

317
00:19:51.840 --> 00:19:54.510
I think in the recent autonomy day,
a couple of weeks ago,

318
00:19:54.560 --> 00:19:58.890
they had this big autonomy day where they demonstrated the vehicle driving

319
00:19:58.891 --> 00:20:03.090
itself on a particular stretch of road.
They,
uh,

320
00:20:03.120 --> 00:20:05.940
they showed off that,
you know,
they're able to query the data,

321
00:20:06.360 --> 00:20:09.450
basically ask questions of the data is saying,
uh,

322
00:20:09.840 --> 00:20:12.270
the example they gave is there's a bike on the back of a car,

323
00:20:13.060 --> 00:20:16.590
the bicycle on the back of a car.
And they're able to say,
well,

324
00:20:16.620 --> 00:20:19.530
when the bicycles in the back of a car that's not a bicycle,

325
00:20:19.650 --> 00:20:20.970
that's just the part of the car.

326
00:20:21.750 --> 00:20:25.800
And they're able to now look back into the data and find all the other cases,

327
00:20:25.980 --> 00:20:29.820
the thousands of cases that happened all over the world in Europe,
in Asia,

328
00:20:30.540 --> 00:20:32.610
in South American and North American,

329
00:20:32.611 --> 00:20:37.611
so on and pull all those elements and then train the train the perception system

330
00:20:38.550 --> 00:20:41.190
of autopilot to be able to,
to,
uh,

331
00:20:41.220 --> 00:20:45.990
better recognize those bicycles as part of the car.
So every edge case like that,

332
00:20:46.020 --> 00:20:49.540
they go through saying,
okay,
the car freaked out in this moment.

333
00:20:49.810 --> 00:20:54.370
And let me find moments like this in the rest of the data and then improve the

334
00:20:54.371 --> 00:20:58.030
system.
So it's,
it's,
uh,

335
00:20:58.510 --> 00:21:02.410
this kind of cycle is the way to deal with,
uh,

336
00:21:02.920 --> 00:21:07.870
with problems with failures of the system is to say every time the car fails at

337
00:21:07.871 --> 00:21:11.530
something,
say,
is this part of a bigger set of problems?

338
00:21:11.530 --> 00:21:15.100
Can I find all those problems and can I improve it with a new update?

339
00:21:15.730 --> 00:21:16.960
And that just keeps going.

340
00:21:17.380 --> 00:21:21.520
The open question is how many loops like that you have to take for the car to

341
00:21:21.521 --> 00:21:23.140
become really good,

342
00:21:23.470 --> 00:21:27.400
better than human to basically how hard is driving.

343
00:21:27.550 --> 00:21:32.380
How many weird situations when you manually drive do you deal with every day

344
00:21:32.680 --> 00:21:35.230
somebody,
uh,
somebody mentioned,
I don't know,

345
00:21:35.270 --> 00:21:38.410
there's like millions of cases when you watch video,
you see them,

346
00:21:38.650 --> 00:21:40.230
somebody mentioned,
um,

347
00:21:40.810 --> 00:21:45.810
that they drive a truck or ups truck and passed cow pastures and they know that

348
00:21:48.760 --> 00:21:52.480
if there's no cows in the cow pasture,
that means that they're grazing.

349
00:21:53.440 --> 00:21:57.790
And if they're grazing,
I mean they be using the correct terms.
I apologize,

350
00:21:57.820 --> 00:22:02.530
not called guy.
Uh,
that,
that means that there may be cows up ahead on the road.

351
00:22:03.070 --> 00:22:06.520
There's just this kind of reasoning it can use to anticipate difficult

352
00:22:06.521 --> 00:22:09.700
situations.
And we do,
we do that kind of reasoning about like,

353
00:22:09.730 --> 00:22:13.120
everything cars today can't do that kind of reasoning.

354
00:22:13.840 --> 00:22:15.610
They're just perceiving what's in front of them.

355
00:22:20.230 --> 00:22:23.450
<v 2>[inaudible].</v>

