WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.838
[MUSIC PLAYING]

00:00:05.203 --> 00:00:06.600
JOSH GORDON: Hey, everyone.

00:00:06.600 --> 00:00:08.612
Thank you very much for coming.

00:00:08.612 --> 00:00:09.570
My name is Josh Gordon.

00:00:09.570 --> 00:00:11.880
I work in TensorFlow
developer relations,

00:00:11.880 --> 00:00:14.910
and I'm really excited to
share with you some thoughts

00:00:14.910 --> 00:00:16.644
on deep learning today.

00:00:16.644 --> 00:00:18.060
A whole bunch of
resources you can

00:00:18.060 --> 00:00:21.420
use to learn much more
about it and a pair

00:00:21.420 --> 00:00:23.430
of my favorite
open source models

00:00:23.430 --> 00:00:25.770
for image classification,
natural language

00:00:25.770 --> 00:00:28.660
parsing, and some neat
tricks you can do with these.

00:00:28.660 --> 00:00:31.560
And I'll also show you some cool
things and how you can actually

00:00:31.560 --> 00:00:33.960
use a pre-trained
image classification

00:00:33.960 --> 00:00:37.020
network to generate
your own artwork, which

00:00:37.020 --> 00:00:40.340
I find really interesting.

00:00:40.340 --> 00:00:41.980
So before we get
started, I just wanted

00:00:41.980 --> 00:00:44.050
to take a moment to
reflect on this idea

00:00:44.050 --> 00:00:46.330
of reproducible research.

00:00:46.330 --> 00:00:48.250
So we can do a lot
of things today

00:00:48.250 --> 00:00:52.060
with deep learning that I
think we take for granted.

00:00:52.060 --> 00:00:56.290
So in 2005, I spent
about six months

00:00:56.290 --> 00:00:59.260
trying to do basic
image classification.

00:00:59.260 --> 00:01:01.390
I was working in a
medical lab, and we

00:01:01.390 --> 00:01:05.500
had a slide containing some
solution with some cells on it.

00:01:05.500 --> 00:01:07.990
And my goal was to
use a neural network

00:01:07.990 --> 00:01:10.450
to identify cells that were
diseased and cells that

00:01:10.450 --> 00:01:11.440
were healthy.

00:01:11.440 --> 00:01:14.680
So I was literally just
tagging regions of an image.

00:01:14.680 --> 00:01:17.387
And there were many machine
learning libraries in 2005,

00:01:17.387 --> 00:01:18.970
including some great
ones that I still

00:01:18.970 --> 00:01:21.553
use today in a teaching setting,
like Weka, which I absolutely

00:01:21.553 --> 00:01:22.644
love.

00:01:22.644 --> 00:01:25.060
But we were still writing a
lot of our neural network code

00:01:25.060 --> 00:01:26.200
by hand.

00:01:26.200 --> 00:01:28.450
And it took us about
six months to get

00:01:28.450 --> 00:01:31.450
a network that worked well just
for this binary classification

00:01:31.450 --> 00:01:32.560
task.

00:01:32.560 --> 00:01:34.121
And it was really
hard to experiment

00:01:34.121 --> 00:01:35.620
with different
network architectures

00:01:35.620 --> 00:01:37.660
and different optimizers.

00:01:37.660 --> 00:01:40.600
And what's interesting-- we had
a lot of really good engineers

00:01:40.600 --> 00:01:42.100
working with us.

00:01:42.100 --> 00:01:46.430
And doing this type of
image classification today

00:01:46.430 --> 00:01:48.690
looks very different.

00:01:48.690 --> 00:01:52.130
So today, a strong
Python developer

00:01:52.130 --> 00:01:54.560
with some basic
background in TensorFlow

00:01:54.560 --> 00:01:56.870
and willingness to try
different open source models

00:01:56.870 --> 00:02:00.080
and read some literature, could
do a much, much, much better

00:02:00.080 --> 00:02:05.720
job than I could have done in
six months, like full time,

00:02:05.720 --> 00:02:07.370
in just a few days.

00:02:07.370 --> 00:02:10.889
And so there's really been
a lot of value created.

00:02:10.889 --> 00:02:13.700
And the reason that we can do
all these great things is that

00:02:13.700 --> 00:02:16.110
a lot of researchers,
academics, and developers,

00:02:16.110 --> 00:02:17.600
both from universities,
and Google,

00:02:17.600 --> 00:02:19.590
and other large
companies-- and small--

00:02:19.590 --> 00:02:22.280
have been, I think,
very generous in sharing

00:02:22.280 --> 00:02:25.102
their tools and their
ideas and their code,

00:02:25.102 --> 00:02:26.810
so that the next
generation of developers

00:02:26.810 --> 00:02:29.370
can iterate and
refine on their ideas.

00:02:29.370 --> 00:02:32.270
So we have a lot of
value at our fingertips.

00:02:32.270 --> 00:02:35.012
And because I started my machine
learning career in medicine,

00:02:35.012 --> 00:02:36.470
I just also wanted
to take a moment

00:02:36.470 --> 00:02:38.951
to reflect on some value
that's been created

00:02:38.951 --> 00:02:41.450
by a lot of this reproducible
research and these open source

00:02:41.450 --> 00:02:42.380
models.

00:02:42.380 --> 00:02:44.870
And so here are
three applications

00:02:44.870 --> 00:02:48.950
of medical imaging from the
last eight months or so.

00:02:48.950 --> 00:02:51.620
And they all rely on an
open source TensorFlow model

00:02:51.620 --> 00:02:53.570
that I'll show you
today, called Inception.

00:02:53.570 --> 00:02:56.030
And Inception is one of the
world's most accurate image

00:02:56.030 --> 00:02:57.044
classifiers.

00:02:57.044 --> 00:02:58.460
It's completely
open source, which

00:02:58.460 --> 00:03:01.160
means you can check it out
and train it on your own data

00:03:01.160 --> 00:03:02.980
to classify things
you care about.

00:03:02.980 --> 00:03:05.680
Here we're looking at work
in diabetic retinopathy.

00:03:05.680 --> 00:03:07.850
I'm going clockwise
from the top left.

00:03:07.850 --> 00:03:09.620
Diabetic retinopathy--
and the basic idea

00:03:09.620 --> 00:03:11.660
here is there's a eye disease.

00:03:11.660 --> 00:03:13.850
And if it's caught
early, it's treatable.

00:03:13.850 --> 00:03:16.340
And if it's caught late,
it can lead to blindness.

00:03:16.340 --> 00:03:19.160
And so in this work, we're
using image recognition

00:03:19.160 --> 00:03:22.100
to identify patients
in need of care.

00:03:22.100 --> 00:03:26.030
The next image here
was a "Nature" paper

00:03:26.030 --> 00:03:27.590
from Stanford University.

00:03:27.590 --> 00:03:30.050
And this was using TensorFlow
and image classification--

00:03:30.050 --> 00:03:31.520
actually on Android--

00:03:31.520 --> 00:03:33.290
to detect skin cancer.

00:03:33.290 --> 00:03:35.875
And the cool thing
about this is that--

00:03:35.875 --> 00:03:37.250
I don't mind going
to the doctor.

00:03:37.250 --> 00:03:38.540
But I'm kind of a lazy guy.

00:03:38.540 --> 00:03:41.240
But if I had a Android app
that I could just quickly scan

00:03:41.240 --> 00:03:44.000
my arm to find out if
something was malignant or not,

00:03:44.000 --> 00:03:46.760
I'm much more likely to
do advance screening.

00:03:46.760 --> 00:03:49.280
And then in the
lower left, we have

00:03:49.280 --> 00:03:52.760
work using image classification
in medical pathology.

00:03:52.760 --> 00:03:55.370
And the idea here is
you have a pathologist,

00:03:55.370 --> 00:03:57.440
and their job is to
take a look at a slide

00:03:57.440 --> 00:03:59.660
and find cells
that are cancerous.

00:03:59.660 --> 00:04:01.940
The problem is these
slides can be enormous.

00:04:01.940 --> 00:04:05.900
They can be 100,000
pixels by 100,000 pixels.

00:04:05.900 --> 00:04:09.290
So we're talking about billions
of pixels on this slide.

00:04:09.290 --> 00:04:11.540
And they might be looking
for a small region that

00:04:11.540 --> 00:04:13.490
contains cancerous cells.

00:04:13.490 --> 00:04:16.910
And so you can imagine that
scanning these slides by hand

00:04:16.910 --> 00:04:18.369
can be incredibly demanding.

00:04:18.369 --> 00:04:20.660
But, of course, if you have
an image recognition model,

00:04:20.660 --> 00:04:22.580
you can analyze
every single region

00:04:22.580 --> 00:04:24.835
on that image with
a fine tooth comb.

00:04:24.835 --> 00:04:26.960
And you can really build
tools to assist physicians

00:04:26.960 --> 00:04:28.460
in their diagnostic work.

00:04:28.460 --> 00:04:30.800
So we can also do awesome,
fun things with, like,

00:04:30.800 --> 00:04:32.219
art, and music, and sound.

00:04:32.219 --> 00:04:34.760
But there's also, like, I think,
real humanitarian value here

00:04:34.760 --> 00:04:37.100
that's becoming
increasingly accessible.

00:04:37.100 --> 00:04:40.010
And the reason I really
like this medical stuff is--

00:04:40.010 --> 00:04:42.470
this is a field that is
not computer science.

00:04:42.470 --> 00:04:44.360
But because of a lot
of reproducible work

00:04:44.360 --> 00:04:45.901
and deep learning,
we're seeing a lot

00:04:45.901 --> 00:04:47.760
of value being created
in many disciplines,

00:04:47.760 --> 00:04:49.343
including things you
wouldn't expect--

00:04:49.343 --> 00:04:50.930
like artwork.

00:04:50.930 --> 00:04:54.060
So I mentioned the word deep
learning a couple of times.

00:04:54.060 --> 00:04:56.960
And so here I'd like to show
you a cartoon diagram of a fully

00:04:56.960 --> 00:04:58.760
connected deep neural network.

00:04:58.760 --> 00:05:01.280
And this particular network
is classifying this image

00:05:01.280 --> 00:05:03.200
as a cat or a dog.

00:05:03.200 --> 00:05:04.970
And I wanted to show
you this to highlight

00:05:04.970 --> 00:05:07.553
one of the differences with deep
learning and some of the work

00:05:07.553 --> 00:05:10.070
that I was doing in, say, 2005.

00:05:10.070 --> 00:05:13.697
So the way I would have written
an image classifier in 2005

00:05:13.697 --> 00:05:15.530
is I would have written
a lot of Python code

00:05:15.530 --> 00:05:17.674
to extract features
from the image.

00:05:17.674 --> 00:05:19.340
I would have thought
about what features

00:05:19.340 --> 00:05:21.050
do I want to feed
into my classifier.

00:05:21.050 --> 00:05:23.390
And these would be lines,
and shapes, colors.

00:05:23.390 --> 00:05:25.280
I might have used a
library, like openCV,

00:05:25.280 --> 00:05:26.491
to do face detection.

00:05:26.491 --> 00:05:28.490
But in deep learning, we
can give the classifier

00:05:28.490 --> 00:05:30.740
the raw pixels from the image.

00:05:30.740 --> 00:05:33.440
And we'll let the network
find good features on its own.

00:05:33.440 --> 00:05:36.320
And we can actually visualize
what it learns later.

00:05:36.320 --> 00:05:38.630
The reason this is called
a deep neural network

00:05:38.630 --> 00:05:40.610
is there is more than one layer.

00:05:40.610 --> 00:05:43.340
We have a stack of layers,
it's a deep neural network.

00:05:43.340 --> 00:05:45.350
That's where the term
deep learning comes from.

00:05:45.350 --> 00:05:46.725
And at the very
top, this network

00:05:46.725 --> 00:05:50.729
happens to be classifying
that image as a cat or a dog.

00:05:50.729 --> 00:05:52.770
There are many wonderful
deep learning libraries,

00:05:52.770 --> 00:05:54.540
but, of course, my
favorite is TensorFlow.

00:05:54.540 --> 00:05:56.540
It's from Google, and as
you guys probably know,

00:05:56.540 --> 00:05:58.712
it's used both for
research and production.

00:05:58.712 --> 00:06:00.420
This means that
TensorFlow has everything

00:06:00.420 --> 00:06:03.330
you need to build products with
hundreds of millions of users,

00:06:03.330 --> 00:06:06.420
like Google Photos, as
well as do actual research,

00:06:06.420 --> 00:06:08.190
like the work you saw
in medical imaging.

00:06:08.190 --> 00:06:10.148
And having the same code
base for both of these

00:06:10.148 --> 00:06:11.430
is very valuable.

00:06:11.430 --> 00:06:12.530
So these are the--

00:06:12.530 --> 00:06:13.950
I'm going to have to fly
through this, by the way.

00:06:13.950 --> 00:06:16.300
I have a lot of content that
I want to share with you.

00:06:16.300 --> 00:06:19.120
So I'm going to have to
accelerate a little bit.

00:06:19.120 --> 00:06:22.230
So here are some of the papers--

00:06:22.230 --> 00:06:23.730
well, these are
actually blog posts.

00:06:23.730 --> 00:06:25.355
But all of them point
to paper that I'd

00:06:25.355 --> 00:06:27.300
like to share with
you today and show you

00:06:27.300 --> 00:06:30.120
how you do practical and
fun things with them.

00:06:30.120 --> 00:06:32.370
And, of course, there's
much, much, much more.

00:06:32.370 --> 00:06:34.290
Google Research, in the
last couple of years,

00:06:34.290 --> 00:06:38.340
has been prolific in
publishing papers.

00:06:38.340 --> 00:06:40.770
And many of these
are state of the art.

00:06:40.770 --> 00:06:42.690
And often, in
addition to the paper,

00:06:42.690 --> 00:06:45.850
they publish everything you need
to try this out on your own.

00:06:45.850 --> 00:06:47.920
And I'm going to show
you what that means.

00:06:47.920 --> 00:06:51.480
So one question that comes up--

00:06:51.480 --> 00:06:55.290
if you have a, say, state of
the art natural language parser,

00:06:55.290 --> 00:06:56.880
that has a lot of
business value.

00:06:56.880 --> 00:06:58.740
And one question
that's asked me a lot

00:06:58.740 --> 00:07:00.540
is, well, why-- why
open source that?

00:07:00.540 --> 00:07:01.740
Why share it?

00:07:01.740 --> 00:07:03.870
And one really
important reason is

00:07:03.870 --> 00:07:06.780
that it empowers others
to continue your ideas.

00:07:06.780 --> 00:07:08.850
So I'm not doing research now.

00:07:08.850 --> 00:07:12.480
But even as just a hacker,
I have many more projects

00:07:12.480 --> 00:07:14.064
than I will ever
finish in my lifetime

00:07:14.064 --> 00:07:15.313
because you guys are all busy.

00:07:15.313 --> 00:07:16.840
There's not enough
hours in the day.

00:07:16.840 --> 00:07:18.390
But by open sourcing
your code, you

00:07:18.390 --> 00:07:20.056
make it possible for
the next generation

00:07:20.056 --> 00:07:23.160
to continue your
line of thought.

00:07:23.160 --> 00:07:25.890
And then, another huge benefit
is that it reduces barriers

00:07:25.890 --> 00:07:27.000
to entry.

00:07:27.000 --> 00:07:29.580
So rather than having to design
your own image classifier

00:07:29.580 --> 00:07:31.110
from scratch, you
can immediately

00:07:31.110 --> 00:07:32.580
go to this state of
the art and open source

00:07:32.580 --> 00:07:33.900
and get started right
away on the problems

00:07:33.900 --> 00:07:35.100
that you care about.

00:07:35.100 --> 00:07:38.700
So, briefly, here's what makes a
good, reproducible, open source

00:07:38.700 --> 00:07:39.600
release.

00:07:39.600 --> 00:07:41.502
Of course you want
to share your code.

00:07:41.502 --> 00:07:43.960
But in machine learning, you
want to share all of the code.

00:07:43.960 --> 00:07:47.250
That means the code you need
for inference, for training, as

00:07:47.250 --> 00:07:48.580
well as for evaluation.

00:07:48.580 --> 00:07:50.130
So you can't have
any hidden pieces--

00:07:50.130 --> 00:07:51.859
otherwise it's not reproducible.

00:07:51.859 --> 00:07:53.775
You always want to share
the data set that you

00:07:53.775 --> 00:07:54.797
used to train the model.

00:07:54.797 --> 00:07:56.880
I know sometimes there's
lots of copyright issues,

00:07:56.880 --> 00:07:58.140
especially if you're
working with images.

00:07:58.140 --> 00:08:00.180
But you should at least
make it reproducible.

00:08:00.180 --> 00:08:02.672
It's also really helpful
to include a toy data set

00:08:02.672 --> 00:08:03.630
when you share a model.

00:08:03.630 --> 00:08:06.379
And by a toy data set,
I mean a small piece

00:08:06.379 --> 00:08:07.920
that makes it really
easy for someone

00:08:07.920 --> 00:08:12.000
to test if their end to end
pipeline is working properly.

00:08:12.000 --> 00:08:14.640
A pre-trained checkpoint
is super valuable.

00:08:14.640 --> 00:08:16.920
So I believe the first
implementation of Inception--

00:08:16.920 --> 00:08:18.420
and this was about a year ago--

00:08:18.420 --> 00:08:21.870
took something like a week to
train on a machine with 8 GPUs.

00:08:21.870 --> 00:08:25.030
Of course, not everyone happens
to have 8 GPUs sitting around.

00:08:25.030 --> 00:08:27.210
So by sharing a
pre-trained checkpoint,

00:08:27.210 --> 00:08:29.910
that means the model works out
of the box to classify images

00:08:29.910 --> 00:08:32.740
and it makes it much
faster to explore with.

00:08:32.740 --> 00:08:36.360
And then something that I'm a
recent convert to is Docker.

00:08:36.360 --> 00:08:39.127
So oftentimes, you have
a lot of dependencies

00:08:39.127 --> 00:08:39.960
in your environment.

00:08:39.960 --> 00:08:41.850
And so by sharing
a Docker container,

00:08:41.850 --> 00:08:44.940
it makes it much faster for
people to try your ideas.

00:08:44.940 --> 00:08:46.755
So this talk is about
open source models,

00:08:46.755 --> 00:08:48.630
but of course, you can
also develop with deep

00:08:48.630 --> 00:08:49.980
learning by writing code.

00:08:49.980 --> 00:08:52.660
And briefly, I want to show
you what that looks like.

00:08:52.660 --> 00:08:55.350
So the point I want to
make is that developing

00:08:55.350 --> 00:08:58.110
with deep learning does
not have to be hard.

00:08:58.110 --> 00:09:01.080
With basic Python skills,
you can write neural networks

00:09:01.080 --> 00:09:03.360
in something like 15
lines of code now.

00:09:03.360 --> 00:09:05.040
So this is code from
a library called

00:09:05.040 --> 00:09:07.369
Keras, which is a high
level machine learning API

00:09:07.369 --> 00:09:08.160
that I really like.

00:09:08.160 --> 00:09:08.970
It's very concise.

00:09:08.970 --> 00:09:11.030
And it's fully
supported in TensorFlow.

00:09:11.030 --> 00:09:12.780
And the idea is that
in TensorFlow, you'll

00:09:12.780 --> 00:09:14.400
get the best of both worlds.

00:09:14.400 --> 00:09:16.560
And here is almost
the entire code

00:09:16.560 --> 00:09:19.440
to define a network very
similar to that cartoon

00:09:19.440 --> 00:09:21.017
diagram we saw earlier.

00:09:21.017 --> 00:09:23.100
I'm adding a stack of
layers, and most of the work

00:09:23.100 --> 00:09:25.410
here is going into making sure
that the weights and the sizes

00:09:25.410 --> 00:09:25.970
all line up.

00:09:28.910 --> 00:09:31.020
I've got to pick up
the pace of this.

00:09:31.020 --> 00:09:32.742
Here's code to
compile the model.

00:09:32.742 --> 00:09:34.700
And so, in deep learning,
if you're learning it

00:09:34.700 --> 00:09:36.650
for the first time, one
thing to be aware of

00:09:36.650 --> 00:09:39.120
is you'll run into a whole
lot of new terminology.

00:09:39.120 --> 00:09:41.860
And oftentimes, the
terminology you encounter

00:09:41.860 --> 00:09:42.699
can be quite scary.

00:09:42.699 --> 00:09:44.240
Like here, we're
looking at something

00:09:44.240 --> 00:09:46.802
that says loss equals
categorical cross entropy.

00:09:46.802 --> 00:09:49.010
And the first time I saw
that, I was like, oh my god.

00:09:49.010 --> 00:09:51.050
This is incredibly different
from a lot of stuff

00:09:51.050 --> 00:09:52.460
I've worked with in the past.

00:09:52.460 --> 00:09:54.517
But oftentimes, when
you look at these terms,

00:09:54.517 --> 00:09:55.850
they're simpler than they sound.

00:09:55.850 --> 00:09:58.220
So loss in deep
learning just means air.

00:09:58.220 --> 00:10:00.080
And it's just a number.

00:10:00.080 --> 00:10:01.640
And categorical
cross entropy is just

00:10:01.640 --> 00:10:03.899
a fancy word for comparing
two distributions.

00:10:03.899 --> 00:10:05.690
So there's a distribution
that you predict,

00:10:05.690 --> 00:10:07.760
and there's a distribution
you should have predicted.

00:10:07.760 --> 00:10:09.468
And that will just
give you a number that

00:10:09.468 --> 00:10:10.895
says how different they were.

00:10:10.895 --> 00:10:12.770
I also wanted to let
you know that TensorFlow

00:10:12.770 --> 00:10:17.570
comes with a nice visualization
tool, called TensorBoard.

00:10:17.570 --> 00:10:19.760
And as you can see from
this animation that

00:10:19.760 --> 00:10:21.410
has failed
spectacularly, you can

00:10:21.410 --> 00:10:23.900
use TensorBoard to actually
visualize the neural network

00:10:23.900 --> 00:10:26.108
that you defined and actually
poke inside the layers.

00:10:28.105 --> 00:10:30.230
So the second way to get
started with deep learning

00:10:30.230 --> 00:10:32.540
is to leverage open
source, which is

00:10:32.540 --> 00:10:33.860
what I want to focus on today.

00:10:33.860 --> 00:10:36.230
And the first model I want
to talk about is Inception.

00:10:36.230 --> 00:10:37.605
And I'll show you
how can you use

00:10:37.605 --> 00:10:39.740
how you can use Inception
for image classification

00:10:39.740 --> 00:10:42.920
as well as Deep Dream
and Style Transfer.

00:10:42.920 --> 00:10:44.810
So before, we saw a
cartoon neural network.

00:10:44.810 --> 00:10:46.520
But here's what Inception
actually looks like.

00:10:46.520 --> 00:10:48.770
You can see it's much larger,
it has many more layers.

00:10:48.770 --> 00:10:50.947
And inside the layers,
there's more moving parts.

00:10:50.947 --> 00:10:53.030
And when you're defining
a network like Inception,

00:10:53.030 --> 00:10:54.738
this is somewhere
where TensorFlow really

00:10:54.738 --> 00:10:55.974
begins to shine.

00:10:55.974 --> 00:10:57.890
And tools like TensorBoard
make it really easy

00:10:57.890 --> 00:11:01.606
to spot any errors that you
have in your architecture.

00:11:01.606 --> 00:11:03.730
So first I want to talk
about image classification.

00:11:03.730 --> 00:11:04.750
So I'm from New York.

00:11:04.750 --> 00:11:07.870
So here's a picture
I snapped last week,

00:11:07.870 --> 00:11:11.260
looking north towards
Central Park from the office.

00:11:11.260 --> 00:11:13.150
And when you're doing
image classification,

00:11:13.150 --> 00:11:15.979
the easiest way to get
started is to use an API.

00:11:15.979 --> 00:11:18.020
And I just want to show
you what this looks like.

00:11:18.020 --> 00:11:20.150
So this is the Google
Cloud Vision API.

00:11:20.150 --> 00:11:22.690
It's drag and drop, and there's
also a RESTful interface.

00:11:22.690 --> 00:11:24.220
And basically, you can
classify the image,

00:11:24.220 --> 00:11:25.636
and you'll get a
set of tags back.

00:11:25.636 --> 00:11:28.870
So here, this is a skyline,
city, metropolis, whatever.

00:11:28.870 --> 00:11:31.510
But you can also do
this in open source.

00:11:31.510 --> 00:11:33.770
And you can do this,
actually, quite easily.

00:11:33.770 --> 00:11:36.460
So here's code for Keras,
running inside TensorFlow.

00:11:36.460 --> 00:11:39.610
And this is actually the
complete code on this slide.

00:11:39.610 --> 00:11:41.080
It's complete.

00:11:41.080 --> 00:11:43.506
And this code will
actually import Inception,

00:11:43.506 --> 00:11:45.130
which is a network
architecture that we

00:11:45.130 --> 00:11:46.540
saw in the previous slide.

00:11:46.540 --> 00:11:48.220
It will load the
pre-trained weights

00:11:48.220 --> 00:11:50.500
from a checkpoint
and pre-process

00:11:50.500 --> 00:11:52.610
and classify an image.

00:11:52.610 --> 00:11:56.740
And so this stuff is much,
much, more accessible

00:11:56.740 --> 00:11:57.449
than often seems.

00:11:57.449 --> 00:11:59.448
And this right here-- in
these, like, whatever--

00:11:59.448 --> 00:12:00.290
five lines of code--

00:12:00.290 --> 00:12:01.770
is much, much, much
better than what

00:12:01.770 --> 00:12:04.145
I could have done just a few
years ago with a huge amount

00:12:04.145 --> 00:12:05.510
of effort.

00:12:05.510 --> 00:12:07.970
And the funny thing about
Inception out of the box

00:12:07.970 --> 00:12:10.931
is if you classify this image,
you get nonsensical tags back.

00:12:10.931 --> 00:12:12.680
So here, Inception is
telling me that this

00:12:12.680 --> 00:12:15.320
is a valley and
possibly a doormat.

00:12:15.320 --> 00:12:19.490
And the reason why that is--
and it's not a problem--

00:12:19.490 --> 00:12:22.040
the reason why that is
is the pre-trained model

00:12:22.040 --> 00:12:24.440
that comes with Inception
is from a data set called

00:12:24.440 --> 00:12:25.490
ImageNet.

00:12:25.490 --> 00:12:27.830
And ImageNet has a
million different images,

00:12:27.830 --> 00:12:29.810
from 1,000 different categories.

00:12:29.810 --> 00:12:35.120
But most of them are pictures
of dogs, cats, flowers, artwork.

00:12:35.120 --> 00:12:37.490
And so, of course,
image classifiers

00:12:37.490 --> 00:12:39.860
only can recognize the things
they've been trained on.

00:12:39.860 --> 00:12:42.641
So Inception doesn't know any
better when you get started.

00:12:42.641 --> 00:12:45.140
And in fact, the blog post that
announced the latest version

00:12:45.140 --> 00:12:47.720
of Inception is very proud
that, out of the box,

00:12:47.720 --> 00:12:49.384
Inception is actually
sensitive enough

00:12:49.384 --> 00:12:51.050
to tell the difference
between these two

00:12:51.050 --> 00:12:52.550
different species of dogs.

00:12:52.550 --> 00:12:54.444
One's a malamute
and one's a husky.

00:12:54.444 --> 00:12:56.860
And it's this sensitivity that,
when you retrain Inception

00:12:56.860 --> 00:12:59.443
on different data, makes it very
good in the medical diagnosis

00:12:59.443 --> 00:13:00.360
domain.

00:13:00.360 --> 00:13:03.140
And in fact, there's a good
expression-- don't trust,

00:13:03.140 --> 00:13:04.160
but verify--

00:13:04.160 --> 00:13:05.820
if you run inception
on this image,

00:13:05.820 --> 00:13:08.420
you'll actually get output
that's more sensible.

00:13:08.420 --> 00:13:11.470
So it will identify
it correctly as a dog.

00:13:11.470 --> 00:13:15.150
So probably you don't care
about recognizing dogs.

00:13:15.150 --> 00:13:19.500
So how can you take Inception
and retrain it to recognize

00:13:19.500 --> 00:13:21.360
images that you care about?

00:13:21.360 --> 00:13:23.802
And the good news is
there's this concept

00:13:23.802 --> 00:13:26.010
in machine learning, called
transfer learning, that I

00:13:26.010 --> 00:13:27.801
want to briefly introduce
and then show you

00:13:27.801 --> 00:13:29.220
how accessible it is.

00:13:29.220 --> 00:13:31.710
So the idea in transfer
learning is simple.

00:13:31.710 --> 00:13:34.170
And that's-- if you have learned
a neural network to, say,

00:13:34.170 --> 00:13:36.330
recognize dogs, and
now you want to recog--

00:13:36.330 --> 00:13:38.220
recognize, say,
cities, do you really

00:13:38.220 --> 00:13:40.140
need to start from scratch?

00:13:40.140 --> 00:13:42.005
While you're learning
to recognize dogs,

00:13:42.005 --> 00:13:44.130
the network has probably
identified useful features

00:13:44.130 --> 00:13:46.660
in images that might be
applicable to other things.

00:13:46.660 --> 00:13:48.450
So instead of throwing
away the network

00:13:48.450 --> 00:13:51.330
that you've previously learned
and retraining from scratch,

00:13:51.330 --> 00:13:53.910
instead you can do
surgery on the network.

00:13:53.910 --> 00:13:57.000
You can chop off the very
last layer of weights

00:13:57.000 --> 00:14:00.344
and create output nodes just
for the classes you care about.

00:14:00.344 --> 00:14:02.760
And then, instead of re-learning
the entire network, which

00:14:02.760 --> 00:14:05.010
might take a week
and a million images,

00:14:05.010 --> 00:14:07.830
you can re-learn just
the very last layer.

00:14:07.830 --> 00:14:09.600
And you can do
that in 30 seconds,

00:14:09.600 --> 00:14:11.630
and you might only
need 100 images.

00:14:11.630 --> 00:14:13.710
So with a very small
amount of data,

00:14:13.710 --> 00:14:15.865
you can actually create
your own image classifier.

00:14:15.865 --> 00:14:18.240
And I wanted to let you know
we have a code lab for this.

00:14:18.240 --> 00:14:19.380
And just a couple
of days ago, we

00:14:19.380 --> 00:14:20.754
published a second
version of it,

00:14:20.754 --> 00:14:22.620
called TensorFlow for Poets.

00:14:22.620 --> 00:14:25.100
And TensorFlow for
Poets is dead simple.

00:14:25.100 --> 00:14:27.350
It's basically going to run
a bunch of scripts for you

00:14:27.350 --> 00:14:29.340
that do that surgery
on the network.

00:14:29.340 --> 00:14:33.030
And the only thing you need
to train a very high accuracy

00:14:33.030 --> 00:14:36.030
custom classifier is literally
to collect directories

00:14:36.030 --> 00:14:37.766
of images that you care about.

00:14:37.766 --> 00:14:39.390
So what we're looking
at here is an app

00:14:39.390 --> 00:14:42.219
that I built at the Metropolitan
Museum of Art in New York.

00:14:42.219 --> 00:14:44.010
And I can point at
paintings and sculptures

00:14:44.010 --> 00:14:45.780
and it will tell
me which one it is.

00:14:45.780 --> 00:14:47.782
And Poets 1 and 2
will actually walk you

00:14:47.782 --> 00:14:49.740
through training a
classifier and installing it

00:14:49.740 --> 00:14:50.460
onto an Android.

00:14:53.810 --> 00:14:56.530
A hilarious piece of criticism
that I got about TensorFlow

00:14:56.530 --> 00:15:00.840
for Poets was some developers
felt that it was too easy.

00:15:00.840 --> 00:15:03.210
And TensorFlow for Poets
is a really easy code lab.

00:15:03.210 --> 00:15:05.400
It's not meant to go
into the details of how

00:15:05.400 --> 00:15:06.480
transfer learning works.

00:15:06.480 --> 00:15:09.160
But we have TensorFlow tutorials
for that if you're interested.

00:15:09.160 --> 00:15:10.743
But there's something
really important

00:15:10.743 --> 00:15:12.120
about TensorFlow for Poets.

00:15:12.120 --> 00:15:15.000
A lot of using machine
learning and practice

00:15:15.000 --> 00:15:17.070
is not about thinking
really carefully

00:15:17.070 --> 00:15:19.080
about how to define
models, and, like,

00:15:19.080 --> 00:15:21.576
using sophisticated
algorithms and lots of math.

00:15:21.576 --> 00:15:22.950
A lot of machine
learning is just

00:15:22.950 --> 00:15:24.330
like development in general.

00:15:24.330 --> 00:15:26.310
It's-- if you're
building an app,

00:15:26.310 --> 00:15:29.220
like this one at the museum, you
have to think carefully about

00:15:29.220 --> 00:15:32.140
how is this app going to
be used in production.

00:15:32.140 --> 00:15:33.996
So if a user picks it
up and they point it

00:15:33.996 --> 00:15:35.370
at a painting or
a sculpture, you

00:15:35.370 --> 00:15:37.980
have to think about things
like, what if it's dark?

00:15:37.980 --> 00:15:39.780
What if there's someone
in the background?

00:15:39.780 --> 00:15:41.072
What if there's an obstruction?

00:15:41.072 --> 00:15:43.071
And so a lot of the
thinking in machine learning

00:15:43.071 --> 00:15:44.730
is how can you
collect training data

00:15:44.730 --> 00:15:46.454
to cover these different cases.

00:15:46.454 --> 00:15:48.120
And if you do that
properly, then you'll

00:15:48.120 --> 00:15:50.314
have an app that works
very well in practice.

00:15:50.314 --> 00:15:51.730
And for Android
developers here, I

00:15:51.730 --> 00:15:53.563
wanted to let you know
we have three Android

00:15:53.563 --> 00:15:54.830
samples for TensorFlow.

00:15:54.830 --> 00:15:56.580
The one on the far
left is doing something

00:15:56.580 --> 00:15:59.760
called style transfer,
which I'll show in a sec.

00:15:59.760 --> 00:16:00.870
It's beautiful.

00:16:00.870 --> 00:16:03.180
The one in the middle
is an image classifier

00:16:03.180 --> 00:16:04.144
which runs Inception.

00:16:04.144 --> 00:16:06.060
And the one on the right
is a object detector,

00:16:06.060 --> 00:16:08.601
which can help you identify the
location of things in images.

00:16:12.378 --> 00:16:14.900
So one really, really
fascinating thing

00:16:14.900 --> 00:16:16.339
about deep learning--

00:16:16.339 --> 00:16:18.380
that I would just want to
give you a preview of--

00:16:18.380 --> 00:16:21.890
is that when you learn an
image classifier, in addition

00:16:21.890 --> 00:16:24.230
to being able to
classify images,

00:16:24.230 --> 00:16:26.690
that classifier actually
has gained some knowledge

00:16:26.690 --> 00:16:28.430
about images in general.

00:16:28.430 --> 00:16:31.514
And you can use that knowledge,
surprisingly, to generate art.

00:16:31.514 --> 00:16:32.930
I'm going to-- I'm
running behind,

00:16:32.930 --> 00:16:34.679
so I'm going to skip
that, and we're going

00:16:34.679 --> 00:16:36.110
to go straight into Deep Dream.

00:16:36.110 --> 00:16:39.320
So question-- how many
people have heard of

00:16:39.320 --> 00:16:41.810
or tried Deep Dream?

00:16:41.810 --> 00:16:42.691
So, like, half?

00:16:42.691 --> 00:16:43.190
Right.

00:16:43.190 --> 00:16:47.400
How many people know
why it's important?

00:16:47.400 --> 00:16:48.510
Much fewer.

00:16:48.510 --> 00:16:51.390
So, to me, the
point of Deep Dream

00:16:51.390 --> 00:16:54.600
is not that we can generate
beautiful psychedelic artwork.

00:16:54.600 --> 00:16:56.910
Like, that's really
wonderful, but that's not

00:16:56.910 --> 00:16:58.410
why Deep Dream is
fascinating to me.

00:17:01.470 --> 00:17:03.480
The reason that I find
Deep Dream interesting

00:17:03.480 --> 00:17:06.839
is it really begins to
investigate the question-- why

00:17:06.839 --> 00:17:08.760
is it that deep
neural networks are

00:17:08.760 --> 00:17:11.600
so good at recognizing images?

00:17:11.600 --> 00:17:14.099
And there's things we know about
deep learning and general--

00:17:14.099 --> 00:17:15.150
why they're so effective.

00:17:15.150 --> 00:17:17.190
Of course, with,
like, GPUs and cloud,

00:17:17.190 --> 00:17:19.569
we can train larger
networks on much more data.

00:17:19.569 --> 00:17:21.590
So that's one reason why
networks are so good.

00:17:21.590 --> 00:17:24.089
Another reason is there's been
many algorithmic improvements

00:17:24.089 --> 00:17:26.520
over the last few years that
enable you to train networks

00:17:26.520 --> 00:17:27.359
more efficiently.

00:17:27.359 --> 00:17:29.775
These are things like different
initialization strategies,

00:17:29.775 --> 00:17:34.040
or optimizers, or batch norm,
but the reason, in my view,

00:17:34.040 --> 00:17:34.770
that--

00:17:34.770 --> 00:17:37.470
why deep neural networks are
so good at recognizing images

00:17:37.470 --> 00:17:40.000
is they automatically
learn features.

00:17:40.000 --> 00:17:43.070
So I mentioned-- with that
cartoon diagram at the start--

00:17:43.070 --> 00:17:45.330
that the network will find
useful features from images

00:17:45.330 --> 00:17:46.500
automatically.

00:17:46.500 --> 00:17:49.050
And in addition to
finding useful features,

00:17:49.050 --> 00:17:52.700
networks actually learn a
hierarchy of those features.

00:17:52.700 --> 00:17:55.030
So in most image
recognition networks,

00:17:55.030 --> 00:17:57.190
you're using something
called convolution.

00:17:57.190 --> 00:17:58.720
And I wanted to
give you a preview

00:17:58.720 --> 00:18:01.190
of what convolution looks like.

00:18:01.190 --> 00:18:03.400
So when I say convolution,
what I really mean

00:18:03.400 --> 00:18:04.780
is using a filter.

00:18:04.780 --> 00:18:06.790
And so on the left, we
have a gray scale picture

00:18:06.790 --> 00:18:07.780
of Manhattan.

00:18:07.780 --> 00:18:10.270
In the center we have a
three by three filter.

00:18:10.270 --> 00:18:12.430
And on the right, we
have another image

00:18:12.430 --> 00:18:14.890
that shows very roughly
the edges of where

00:18:14.890 --> 00:18:16.930
the buildings are.

00:18:16.930 --> 00:18:19.300
This kernel, or filter,
is a little edge detector

00:18:19.300 --> 00:18:20.230
that I wrote.

00:18:20.230 --> 00:18:22.000
It's very scrappy,
but if you write

00:18:22.000 --> 00:18:25.289
Python code to slide this
filter along the image

00:18:25.289 --> 00:18:26.830
and compute the dot
product of what's

00:18:26.830 --> 00:18:29.320
beneath it at each location,
it will produce that image

00:18:29.320 --> 00:18:31.690
which roughly shows the edges.

00:18:31.690 --> 00:18:35.140
So this filter is
hand engineered.

00:18:35.140 --> 00:18:37.540
But in convolutional
neural networks,

00:18:37.540 --> 00:18:40.040
we're going to learn a
hierarchy of these things.

00:18:40.040 --> 00:18:41.920
So here I've added
a line of keras

00:18:41.920 --> 00:18:44.380
that's going to learn
32 of these filters.

00:18:44.380 --> 00:18:46.360
And each of these
filters is going

00:18:46.360 --> 00:18:50.380
to be learned as a function
of the raw input image--

00:18:50.380 --> 00:18:52.090
so as a function of the pixels.

00:18:52.090 --> 00:18:56.850
But if you have a stack of
these filters, what you're doing

00:18:56.850 --> 00:18:57.930
is you have--

00:18:57.930 --> 00:19:01.560
the first layer is finding
features of the raw image.

00:19:01.560 --> 00:19:05.280
The second layer of filters is
learning features of features.

00:19:05.280 --> 00:19:07.140
And the third layer
is learning features

00:19:07.140 --> 00:19:08.710
of features of features.

00:19:08.710 --> 00:19:11.880
So the filters that you learn
become increasingly abstract.

00:19:11.880 --> 00:19:12.840
And you get this--

00:19:12.840 --> 00:19:15.720
for free, by virtue of
training an image classifier--

00:19:15.720 --> 00:19:19.290
you get this learned hierarchy
of feature-- of filters.

00:19:19.290 --> 00:19:22.110
And what I've done here is--
and you can repeat this with

00:19:22.110 --> 00:19:23.370
the code at the GooLink--

00:19:23.370 --> 00:19:25.570
is I've started
from random noise,

00:19:25.570 --> 00:19:28.650
and I've optimized the noise
to increasingly excite filters

00:19:28.650 --> 00:19:30.644
at different layers
of the network.

00:19:30.644 --> 00:19:32.060
And the really
interesting thing--

00:19:32.060 --> 00:19:33.870
and I think there's really
good science to be done here,

00:19:33.870 --> 00:19:36.060
and really understanding
this rigorously--

00:19:36.060 --> 00:19:38.790
is that the lowest layer
filters generally learns

00:19:38.790 --> 00:19:41.070
to detect edges and colors.

00:19:41.070 --> 00:19:42.960
The next layer is
learning textures--

00:19:42.960 --> 00:19:44.586
those are features
of edges and colors.

00:19:44.586 --> 00:19:46.376
And then as you move
higher up the network,

00:19:46.376 --> 00:19:48.057
you begin to learn
things like patterns.

00:19:48.057 --> 00:19:49.890
And if you go way high
up the network, which

00:19:49.890 --> 00:19:52.980
I'll show you later, that's
when you get psychedelic stuff.

00:19:52.980 --> 00:19:54.480
And so Deep Dream
doesn't have to be

00:19:54.480 --> 00:19:56.295
used to create trippy artwork.

00:19:56.295 --> 00:19:58.170
As you'll see in a
moment, I'm not an artist,

00:19:58.170 --> 00:20:00.390
but what I tried to
do is use Deep Dream

00:20:00.390 --> 00:20:03.030
to turn this image into
something like a firework

00:20:03.030 --> 00:20:04.110
display.

00:20:04.110 --> 00:20:05.910
And to do this, you
can actually select

00:20:05.910 --> 00:20:07.710
which filter you
want to optimize for.

00:20:10.204 --> 00:20:12.620
If you look at the Deep Dream
notebook at the GooLink link

00:20:12.620 --> 00:20:15.140
on the top, you're going
to see a whole lot of code.

00:20:15.140 --> 00:20:17.060
But the interesting
thing is that something

00:20:17.060 --> 00:20:19.850
like 90% of that code is fine
tuning, just to make sure

00:20:19.850 --> 00:20:22.250
that the images that are
produced are really beautiful.

00:20:22.250 --> 00:20:25.040
This is basically the core
of the Deep Dream algorithm,

00:20:25.040 --> 00:20:27.680
and it's really, really concise.

00:20:27.680 --> 00:20:29.450
So the basic idea
in TensorFlow here

00:20:29.450 --> 00:20:31.610
is that we're going
to pick some filter

00:20:31.610 --> 00:20:33.870
from some layer of the network.

00:20:33.870 --> 00:20:36.380
And we're going to
define a loss function.

00:20:36.380 --> 00:20:38.146
The loss is just a number.

00:20:38.146 --> 00:20:39.770
And here, the loss
is going to tell us,

00:20:39.770 --> 00:20:43.650
on average, how excited is
this filter by some image.

00:20:43.650 --> 00:20:45.770
So if we classify
any random image,

00:20:45.770 --> 00:20:47.881
the filter is going to
get some activation.

00:20:47.881 --> 00:20:49.880
And in Deep Dream, what
we can do is we can say,

00:20:49.880 --> 00:20:52.550
TensorFlow, please tell me
the gradient of this loss

00:20:52.550 --> 00:20:54.260
with respect to my image.

00:20:54.260 --> 00:20:56.690
And you're asking
TensorFlow, given an image,

00:20:56.690 --> 00:20:59.600
how should I modify that
to increasingly excite

00:20:59.600 --> 00:21:01.697
some filter in the network?

00:21:01.697 --> 00:21:04.280
And then what you can do is you
can literally modify the image

00:21:04.280 --> 00:21:05.630
according to the gradient.

00:21:05.630 --> 00:21:08.090
And here, I can actually
add it directly to the image

00:21:08.090 --> 00:21:08.720
and repeat.

00:21:08.720 --> 00:21:10.660
And so your image will
progressively incite--

00:21:10.660 --> 00:21:11.840
excite a filter.

00:21:11.840 --> 00:21:15.020
And that's where Deep
Dream comes from.

00:21:15.020 --> 00:21:18.920
So here's another picture I took
from the reservoir in New York.

00:21:18.920 --> 00:21:22.760
And we're looking
south from 90th street.

00:21:22.760 --> 00:21:24.740
And I ran Deep
Dream on top of it

00:21:24.740 --> 00:21:28.490
using a filter at a very
high level of the network.

00:21:28.490 --> 00:21:30.950
And this particular filter--
actually, I'm sorry,

00:21:30.950 --> 00:21:33.560
I ran it on a whole layer
of the network, the second

00:21:33.560 --> 00:21:34.830
to last layer.

00:21:34.830 --> 00:21:37.370
And if you run Deep Dream on
an entire layer, what you're

00:21:37.370 --> 00:21:39.650
really saying is TensorFlow,
please progressively

00:21:39.650 --> 00:21:43.560
modify the image to
excite all the filters.

00:21:43.560 --> 00:21:45.830
So you're basically super
saturating it with things

00:21:45.830 --> 00:21:47.630
that it knows how to recognize.

00:21:47.630 --> 00:21:50.030
And that's why you see, if
you use Inception, that's

00:21:50.030 --> 00:21:52.130
why you see dogs,
and cats, and snakes,

00:21:52.130 --> 00:21:53.630
and stuff like that everywhere.

00:21:53.630 --> 00:21:56.900
But if you took Inception
and you retrained it--

00:21:56.900 --> 00:21:58.460
say on artwork--

00:21:58.460 --> 00:22:00.950
and then you ran Deep
Dream on top of that,

00:22:00.950 --> 00:22:02.450
you would no longer
have these dogs.

00:22:02.450 --> 00:22:04.280
Instead, you might
modify an image

00:22:04.280 --> 00:22:06.830
to contain elements of,
like, Monet and Picasso.

00:22:06.830 --> 00:22:10.140
So it's cool, but I think
there's good science there.

00:22:10.140 --> 00:22:11.640
I didn't want to
show you this here,

00:22:11.640 --> 00:22:13.264
but the notebook
actually contains code

00:22:13.264 --> 00:22:16.670
to produce your own
animated dreams.

00:22:16.670 --> 00:22:18.700
But it was borderline
not safe for work.

00:22:18.700 --> 00:22:20.780
Not in like a strange
sense, it was just--

00:22:20.780 --> 00:22:24.560
was too psychedelic
and seizure-inducing.

00:22:24.560 --> 00:22:26.960
So there's another
absolutely beautiful thing

00:22:26.960 --> 00:22:29.270
you can do with
pre-trained image networks.

00:22:29.270 --> 00:22:30.830
And this is called
style transfer.

00:22:30.830 --> 00:22:32.450
And for people
new to it, I can't

00:22:32.450 --> 00:22:33.980
recommend this strongly enough.

00:22:33.980 --> 00:22:37.490
It is my favorite way to produce
really beautiful artwork.

00:22:37.490 --> 00:22:40.190
So what I want to
show you is something

00:22:40.190 --> 00:22:42.590
from a project called Magenta.

00:22:42.590 --> 00:22:44.450
There's going to be
a talk on Magenta

00:22:44.450 --> 00:22:47.480
tomorrow by Douglas Eck, who's
one of the project leads.

00:22:47.480 --> 00:22:48.889
Magenta is a wonderful project.

00:22:48.889 --> 00:22:50.930
And it investigates how
you can use deep learning

00:22:50.930 --> 00:22:52.340
to generate art and music.

00:22:52.340 --> 00:22:54.260
And they've done a great job.

00:22:54.260 --> 00:22:56.045
So here, in Style
Transfer, the idea

00:22:56.045 --> 00:22:57.740
is that you begin
with a painting.

00:22:57.740 --> 00:23:01.970
And here's one from the Museum
of Modern Art, in New York.

00:23:01.970 --> 00:23:03.680
And then you take a photograph.

00:23:03.680 --> 00:23:05.750
So here's one of the
Golden Gate Bridge.

00:23:05.750 --> 00:23:07.280
And the goal in
Style Transfer is

00:23:07.280 --> 00:23:09.590
to produce a new
piece of artwork that

00:23:09.590 --> 00:23:13.280
combines the style of the
painting with the content

00:23:13.280 --> 00:23:14.510
of the photograph.

00:23:14.510 --> 00:23:17.840
And by style, I mean things like
edges and shapes and colors.

00:23:17.840 --> 00:23:20.390
And by content, I mean things
that are semantic-- like here,

00:23:20.390 --> 00:23:21.980
I see a bridge.

00:23:21.980 --> 00:23:24.200
And so if you try
to do this by hand,

00:23:24.200 --> 00:23:26.500
or in Photoshop or whatever,
to produce a new painting,

00:23:26.500 --> 00:23:28.250
the result would
probably not be very good

00:23:28.250 --> 00:23:30.470
unless you're much better
at Photoshop than I am.

00:23:30.470 --> 00:23:32.480
But using Style Transfer,
you can actually

00:23:32.480 --> 00:23:35.840
generate images like this,
which are astoundingly good.

00:23:35.840 --> 00:23:37.910
And the reason I find
this so interesting

00:23:37.910 --> 00:23:41.300
is you're able to produce
images like this almost for free

00:23:41.300 --> 00:23:44.960
as an artifact of training
an image classifier.

00:23:44.960 --> 00:23:46.850
And in Deep Dream,
what we were doing

00:23:46.850 --> 00:23:49.430
is finding an image
that maximally excites

00:23:49.430 --> 00:23:51.500
some filter of the network.

00:23:51.500 --> 00:23:55.610
But in Style Transfer, we need
to somehow caption the notion

00:23:55.610 --> 00:23:57.590
of content and style.

00:23:57.590 --> 00:24:00.510
And here's a high level
diagram of how that works.

00:24:00.510 --> 00:24:02.660
We also get this for free.

00:24:02.660 --> 00:24:05.240
If you take, say, the
painting on the top

00:24:05.240 --> 00:24:07.200
and you use some
existing image classifier

00:24:07.200 --> 00:24:10.010
and you just classify the
painting, what you can do

00:24:10.010 --> 00:24:13.370
is look at the activations of
the lower half of the layers,

00:24:13.370 --> 00:24:14.920
roughly.

00:24:14.920 --> 00:24:17.320
Earlier, we said-- we saw
that the first layer detected

00:24:17.320 --> 00:24:19.050
edges, shapes, and
colors, and textures,

00:24:19.050 --> 00:24:20.440
say in the first three.

00:24:20.440 --> 00:24:22.139
And so when you
classify the image

00:24:22.139 --> 00:24:24.680
by looking at the activations
in those layers, in some sense,

00:24:24.680 --> 00:24:27.460
you can capture what
it means to have style.

00:24:27.460 --> 00:24:29.200
And then if you
classify the photograph,

00:24:29.200 --> 00:24:31.870
and you look at the activations
towards the top of the network,

00:24:31.870 --> 00:24:35.950
you have some sense of what
it means to have content.

00:24:35.950 --> 00:24:37.540
And then if you
combine those, you

00:24:37.540 --> 00:24:38.956
can actually come
up with a number

00:24:38.956 --> 00:24:40.809
that you can optimize
in TensorFlow.

00:24:40.809 --> 00:24:43.100
And that's how you actually
can start with random noise

00:24:43.100 --> 00:24:46.450
and progressively optimize
it to minimize a loss that

00:24:46.450 --> 00:24:48.100
combines content and style.

00:24:48.100 --> 00:24:50.140
And so at a high level
that's how it works,

00:24:50.140 --> 00:24:52.660
but I think the science
behind it is mind blowing.

00:24:52.660 --> 00:24:55.060
So this is a good example in
Deep Learning of something

00:24:55.060 --> 00:24:58.690
that never in 100 years would
I have thought that, ever, I

00:24:58.690 --> 00:25:01.480
could write a Python
program to produce images

00:25:01.480 --> 00:25:02.710
like that stylized bridge.

00:25:02.710 --> 00:25:04.210
It just never would
have happened.

00:25:04.210 --> 00:25:06.975
But it turns out that this
is something we get for free.

00:25:06.975 --> 00:25:08.350
And you'll see in
Douglas's talk,

00:25:08.350 --> 00:25:10.090
if you go-- which I
highly recommend--

00:25:10.090 --> 00:25:12.760
you can actually do style
transfer in real time now.

00:25:12.760 --> 00:25:15.389
And here, what
they're doing is they

00:25:15.389 --> 00:25:16.680
have a collection of paintings.

00:25:16.680 --> 00:25:18.340
And they're saying, I'd like
to apply some more "Starry

00:25:18.340 --> 00:25:19.540
Night" to this dog.

00:25:23.410 --> 00:25:26.140
So that's Inception.

00:25:26.140 --> 00:25:28.960
Language is also,
obviously, really important.

00:25:28.960 --> 00:25:31.180
People that work in, like,
robotics or, like, video

00:25:31.180 --> 00:25:34.540
or images or games
have the best demos.

00:25:34.540 --> 00:25:37.030
So their work is
usually more well known,

00:25:37.030 --> 00:25:39.860
but obviously, almost all the
apps we write work with text.

00:25:39.860 --> 00:25:41.776
So here, I want to show
you a couple of models

00:25:41.776 --> 00:25:42.910
from the syntax net family.

00:25:42.910 --> 00:25:44.770
Actually just one, ParseySaurus.

00:25:44.770 --> 00:25:47.290
And ParseySaurus is currently
the world's most accurate

00:25:47.290 --> 00:25:48.795
natural language processor.

00:25:48.795 --> 00:25:52.280
And it's completely open
source in TensorFlow.

00:25:52.280 --> 00:25:55.187
So here's how we
can work with text.

00:25:55.187 --> 00:25:57.020
So imagine you just
have a simple sentence--

00:25:57.020 --> 00:25:59.760
I love NYC.

00:25:59.760 --> 00:26:02.040
One thing you can do,
if you go the API route,

00:26:02.040 --> 00:26:04.830
is you can use something like
the cloud Natural Language API

00:26:04.830 --> 00:26:06.960
to analyze the sentence.

00:26:06.960 --> 00:26:07.800
It's drag and drop.

00:26:07.800 --> 00:26:09.900
There's a web demo, which
is actually really fun.

00:26:09.900 --> 00:26:11.608
And it's also RESTful,
if you write code.

00:26:16.280 --> 00:26:18.370
So here, we're
seeing the cloud API

00:26:18.370 --> 00:26:20.160
telling us that love
is a verb and it's

00:26:20.160 --> 00:26:21.410
looking at a dependency parse.

00:26:21.410 --> 00:26:23.470
So we can find out that
NYC is the direct object

00:26:23.470 --> 00:26:24.820
of a sentence.

00:26:24.820 --> 00:26:27.910
But you can also do this
in open source TensorFlow.

00:26:27.910 --> 00:26:29.590
And so here is a
single line of code

00:26:29.590 --> 00:26:32.140
to start up a Docker
container for syntax net.

00:26:32.140 --> 00:26:35.110
And this will contain the
ParseySaurus model as well as

00:26:35.110 --> 00:26:37.930
code for over 40
different languages.

00:26:37.930 --> 00:26:40.660
And using this, you can actually
do some really interesting

00:26:40.660 --> 00:26:41.710
things.

00:26:41.710 --> 00:26:45.650
So Parsi McParseface--
from last year--

00:26:45.650 --> 00:26:48.080
McParseface was previously
the world's most accurate

00:26:48.080 --> 00:26:49.810
natural language parser.

00:26:49.810 --> 00:26:52.686
And McParseface has been
superseded by ParseySaurus.

00:26:52.686 --> 00:26:55.310
The main difference-- well, one
of the differences between them

00:26:55.310 --> 00:26:58.550
is that Parsi McParseface
operated on words.

00:26:58.550 --> 00:27:00.450
So the tokens that it
would analyse would be,

00:27:00.450 --> 00:27:02.590
the gostak distims.

00:27:02.590 --> 00:27:05.150
But ParseySaurus
operates on characters,

00:27:05.150 --> 00:27:07.010
so it's much finer grained.

00:27:07.010 --> 00:27:10.040
This is a quote from a
book called "The Meaning Of

00:27:10.040 --> 00:27:11.280
Meaning".

00:27:11.280 --> 00:27:14.027
And if you look at it, the
sentence is nonsensical.

00:27:14.027 --> 00:27:15.860
But what's really funny
is, for some reason,

00:27:15.860 --> 00:27:18.840
our brain actually knows a
whole lot about this sentence.

00:27:18.840 --> 00:27:21.950
So for instance, I could ask
you some questions about it.

00:27:21.950 --> 00:27:26.630
So what do you think the
part of speeches for distims?

00:27:26.630 --> 00:27:29.310
Is distims a noun?

00:27:29.310 --> 00:27:31.820
Is distims a verb?

00:27:31.820 --> 00:27:32.710
It's a verb.

00:27:32.710 --> 00:27:35.410
And it's funny that we know
that, because it's not a word.

00:27:35.410 --> 00:27:37.180
And the really
interesting thing is

00:27:37.180 --> 00:27:38.740
you can actually
use ParseySaurus

00:27:38.740 --> 00:27:40.700
to tell you that as well.

00:27:40.700 --> 00:27:42.642
So if you fire up
that Docker container,

00:27:42.642 --> 00:27:45.100
you'll find an ipython notebook
called the interactive text

00:27:45.100 --> 00:27:46.120
analyzer.

00:27:46.120 --> 00:27:47.770
And with just a
few lines of code,

00:27:47.770 --> 00:27:50.110
you can feed ParseySaurus
that sentence.

00:27:50.110 --> 00:27:51.700
You can get back
a bunch of tokens,

00:27:51.700 --> 00:27:53.658
which are protobuffs that
contain a whole bunch

00:27:53.658 --> 00:27:55.180
of information about the parse.

00:27:55.180 --> 00:27:57.599
You can iterate over it,
find the distims word,

00:27:57.599 --> 00:27:59.140
and print out the
part of speech tag.

00:27:59.140 --> 00:28:01.330
And ParseySaurus will tell
you that that's distims.

00:28:04.510 --> 00:28:05.890
You can also ask
the question who

00:28:05.890 --> 00:28:08.920
or what is doing the distimming?

00:28:08.920 --> 00:28:12.820
And is it the doshes that are
distimming or is it the gostak?

00:28:12.820 --> 00:28:15.040
And when I read this, I
see it-- it's the gostak--

00:28:15.040 --> 00:28:17.230
is the thing that's
doing the distimming.

00:28:17.230 --> 00:28:19.700
And ParseySaurus can
tell you that as well.

00:28:19.700 --> 00:28:21.220
So if you iterate
over the tokens,

00:28:21.220 --> 00:28:23.027
we can find the
subject of a sentence.

00:28:23.027 --> 00:28:24.610
And you'll find out
that's the gostak.

00:28:28.312 --> 00:28:30.020
ParseySaurus can also
tell you a lot more

00:28:30.020 --> 00:28:32.550
about a sentence like this.

00:28:32.550 --> 00:28:36.026
So if you just print out the raw
protobuffs from all the tokens,

00:28:36.026 --> 00:28:37.650
you'll get a whole
slew of information.

00:28:37.650 --> 00:28:40.270
And only some of it is
actually on this slide.

00:28:40.270 --> 00:28:42.660
So it actually can give you
pretty fine-grained stuff.

00:28:42.660 --> 00:28:44.118
Like, for instance,
it can tell you

00:28:44.118 --> 00:28:45.900
that distims is in
the present tense.

00:28:45.900 --> 00:28:48.480
It can tell you that
the gostak is singular.

00:28:48.480 --> 00:28:51.080
And it can tell you that
the doshes are plural.

00:28:54.840 --> 00:28:56.730
And then the
Natural Language API

00:28:56.730 --> 00:28:58.560
has these beautiful
dependency trees,

00:28:58.560 --> 00:29:00.469
but you can actually
produce that as well.

00:29:00.469 --> 00:29:02.010
And so the notebook
actually contains

00:29:02.010 --> 00:29:04.980
code, using good old
graphviz, to produce something

00:29:04.980 --> 00:29:06.630
that looks exactly like this.

00:29:06.630 --> 00:29:08.340
And there you can
see that the doshes

00:29:08.340 --> 00:29:11.570
is the object of the sentence.

00:29:11.570 --> 00:29:15.530
So there's a lot of
value in open source.

00:29:15.530 --> 00:29:17.750
And the cool thing
about ParseySaurus

00:29:17.750 --> 00:29:20.810
is that multi-lingual support
is obviously really important.

00:29:20.810 --> 00:29:24.200
And I'm really, really impressed
by the syntax net team, as well

00:29:24.200 --> 00:29:26.109
as the Magenta team
and the Inception team,

00:29:26.109 --> 00:29:27.650
because out of the
box, they actually

00:29:27.650 --> 00:29:30.200
have a link where you can
download pre-trained models

00:29:30.200 --> 00:29:31.700
for 40 different languages.

00:29:31.700 --> 00:29:33.200
Or more than 40, actually.

00:29:33.200 --> 00:29:35.090
Loading these languages,
it's a little bit--

00:29:35.090 --> 00:29:37.006
there's a few more
parameters, but it's really

00:29:37.006 --> 00:29:38.270
one or two lines of code.

00:29:38.270 --> 00:29:41.190
And so you can switch the
model to German just like that,

00:29:41.190 --> 00:29:44.410
and hear the languages
that I saw out of the box.

00:29:44.410 --> 00:29:48.630
So this is actually really
cool, because if you're

00:29:48.630 --> 00:29:51.750
building any kind of
application that works with text

00:29:51.750 --> 00:29:54.062
and is doing something
machine learning-y,

00:29:54.062 --> 00:29:55.770
you can think about
multi-lingual support

00:29:55.770 --> 00:29:56.394
from the start.

00:29:59.660 --> 00:30:02.700
There's two really good
use cases for this.

00:30:02.700 --> 00:30:04.710
The first is an easy
way to get started.

00:30:04.710 --> 00:30:06.470
And if you have an
existing system which

00:30:06.470 --> 00:30:08.330
is doing some sort of
text classification--

00:30:08.330 --> 00:30:10.330
this could be
sentiment analysis,

00:30:10.330 --> 00:30:12.530
or you could be doing spam
detection, or whatever.

00:30:12.530 --> 00:30:14.630
And you're using part of
speech tags or dependency tags,

00:30:14.630 --> 00:30:16.171
you can use a model
like ParseySaurus

00:30:16.171 --> 00:30:18.115
to get a slight boost
in your accuracy.

00:30:18.115 --> 00:30:19.490
But I think more
importantly, you

00:30:19.490 --> 00:30:21.960
can use this as a
foundation for a new system.

00:30:21.960 --> 00:30:24.230
So anytime you're training
a model in Deep Learning,

00:30:24.230 --> 00:30:25.970
you're going to
need lots of data.

00:30:25.970 --> 00:30:29.485
And I think, by using these
features as additional input,

00:30:29.485 --> 00:30:30.860
you can reduce
the amount of data

00:30:30.860 --> 00:30:31.970
that you need because
you're not going

00:30:31.970 --> 00:30:34.428
to have to learn notions like
part of speech and dependency

00:30:34.428 --> 00:30:35.210
tags from scratch.

00:30:35.210 --> 00:30:36.460
Instead you get them for free.

00:30:41.010 --> 00:30:45.570
There's a really awesome term
that I found on Magenta's blog,

00:30:45.570 --> 00:30:47.474
and it's called Release++.

00:30:47.474 --> 00:30:49.140
And so this is some
text that I actually

00:30:49.140 --> 00:30:51.764
stole from their article, where
they're talking about something

00:30:51.764 --> 00:30:54.960
called Ensign, which is
actually a program that

00:30:54.960 --> 00:30:57.450
can generate sounds.

00:30:57.450 --> 00:30:59.670
And I thought this was a
really beautiful passage.

00:30:59.670 --> 00:31:02.790
So when they shared the code
for Ensign, they actually--

00:31:02.790 --> 00:31:04.331
they explicitly called out--

00:31:04.331 --> 00:31:06.330
sure, they're sharing the
code, but they're also

00:31:06.330 --> 00:31:09.900
sharing pre-trained models
and a Docker container.

00:31:09.900 --> 00:31:12.390
And because the team has done
such a wonderful job of doing

00:31:12.390 --> 00:31:15.030
this, it means that if you
want to try out their work,

00:31:15.030 --> 00:31:18.180
especially in style transfer,
you can do it out of the box,

00:31:18.180 --> 00:31:19.682
and it works very, very well.

00:31:19.682 --> 00:31:21.390
And it sounds simple,
but that's actually

00:31:21.390 --> 00:31:23.460
something really hard
to achieve in practice.

00:31:23.460 --> 00:31:24.660
Like, I can't tell
you how much time

00:31:24.660 --> 00:31:27.120
I spend playing with open source
software machine learning,

00:31:27.120 --> 00:31:28.230
whether it's written
in TensorFlow

00:31:28.230 --> 00:31:29.430
or a different toolkit.

00:31:29.430 --> 00:31:31.920
And it is so hard to
actually replicate --

00:31:31.920 --> 00:31:32.880
results.

00:31:32.880 --> 00:31:35.970
And so I think some of the
work that I showed you today

00:31:35.970 --> 00:31:38.430
is good examples of work
that sets the bar on really

00:31:38.430 --> 00:31:40.020
how to share ideas.

00:31:40.020 --> 00:31:42.459
I think I've accelerated
too much in my speed--

00:31:42.459 --> 00:31:44.250
in my rush to get
through all this content.

00:31:44.250 --> 00:31:46.710
But anyway, a
blessing and a curse

00:31:46.710 --> 00:31:48.240
of learning deep
learning today is

00:31:48.240 --> 00:31:52.540
that there are a huge number
of educational resources.

00:31:52.540 --> 00:31:55.880
There's actually an
entire medium blog post,

00:31:55.880 --> 00:31:57.630
where somebody--
actually very helpfully--

00:31:57.630 --> 00:32:00.005
went through something like
30 different machine learning

00:32:00.005 --> 00:32:01.890
courses and looked
at all the reviews.

00:32:01.890 --> 00:32:04.740
So here just a couple
that I'd recommend.

00:32:04.740 --> 00:32:06.630
Stanford has a great
course-- it's actually

00:32:06.630 --> 00:32:09.770
student initiated, it's really
impressive-- on TensorFlow.

00:32:09.770 --> 00:32:12.201
CS20si-- it has an
intimidating title.

00:32:12.201 --> 00:32:14.700
The title is, like, "TensorFlow
for Deep Learning Research".

00:32:14.700 --> 00:32:16.575
But really a lot of this
course will actually

00:32:16.575 --> 00:32:19.260
walk you through the basics of
writing TensorFlow programs.

00:32:19.260 --> 00:32:20.880
If you're new to
neural networks,

00:32:20.880 --> 00:32:24.480
the course notes from
CS231n are outstanding.

00:32:24.480 --> 00:32:26.700
That course also has a
slightly intimidating title,

00:32:26.700 --> 00:32:29.040
its "Introduction to
Convolutional Neural Networks",

00:32:29.040 --> 00:32:30.450
but the beginning of
the class will actually

00:32:30.450 --> 00:32:31.350
walk you through--

00:32:31.350 --> 00:32:33.660
just in Python, it doesn't
use TensorFlow at all--

00:32:33.660 --> 00:32:35.370
really, what is
a classifier, how

00:32:35.370 --> 00:32:36.750
do you write one from scratch.

00:32:36.750 --> 00:32:37.890
And then it will take
you from something

00:32:37.890 --> 00:32:39.514
like k nearest
neighbors all the way up

00:32:39.514 --> 00:32:40.994
to a convolutional
neural network.

00:32:40.994 --> 00:32:42.660
If you like natural
language processing,

00:32:42.660 --> 00:32:44.784
although it's not on the
slides, Stanford also has,

00:32:44.784 --> 00:32:46.314
I believe, CS224n,
which is actually

00:32:46.314 --> 00:32:47.730
a course in doing
natural language

00:32:47.730 --> 00:32:49.440
processing with TensorFlow.

00:32:49.440 --> 00:32:51.390
And then there's
a wonderful blog.

00:32:51.390 --> 00:32:54.507
If you'd like to get some
intuition for how networks

00:32:54.507 --> 00:32:56.340
actually work under the
hood and why they're

00:32:56.340 --> 00:32:58.890
so good at recognizing images
and doing other things,

00:32:58.890 --> 00:33:01.080
Chris Ola's blog is
absolutely outstanding,

00:33:01.080 --> 00:33:03.920
and I highly recommend it.

00:33:03.920 --> 00:33:05.902
So thanks very much.

00:33:05.902 --> 00:33:07.360
One of my favorite
things about I/O

00:33:07.360 --> 00:33:08.943
is just actually
looking at the crowd.

00:33:08.943 --> 00:33:11.750
I see a bunch of folks that I
haven't seen for about a year.

00:33:11.750 --> 00:33:13.662
So I like this conference a lot.

00:33:13.662 --> 00:33:15.370
I'll be around if you
have any questions.

00:33:15.370 --> 00:33:17.436
And I'm happy to
chat afterwards.

00:33:17.436 --> 00:33:19.060
So thank you very
much guys, and I hope

00:33:19.060 --> 00:33:20.185
these resources are useful.

00:33:20.185 --> 00:33:22.250
[APPLAUSE]

00:33:22.250 --> 00:33:26.240
[MUSIC PLAYING]

