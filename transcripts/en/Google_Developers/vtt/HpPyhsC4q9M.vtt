WEBVTT
Kind: captions
Language: en

00:00:02.320 --> 00:00:03.380
KAZ SATO: Hello.

00:00:03.380 --> 00:00:05.575
Thank you for taking your
time on our session, How

00:00:05.575 --> 00:00:09.610
to build a smart RasPi bot with
Cloud Vision and Speech API.

00:00:09.610 --> 00:00:10.980
I'm Kaz Sato.

00:00:10.980 --> 00:00:13.524
I'm a developer out of gate
for Google Cloud platform.

00:00:13.524 --> 00:00:14.185
And--

00:00:14.185 --> 00:00:15.560
GLEN SHIRES: And
I'm Glen Shires.

00:00:15.560 --> 00:00:20.800
I'm a software engineer
on the Cloud Speech API.

00:00:20.800 --> 00:00:22.096
KAZ SATO: OK.

00:00:22.096 --> 00:00:26.730
I'd like to show a demonstration
video of the Cloud Vision Bot,

00:00:26.730 --> 00:00:31.270
at first, So let
me share the video.

00:00:31.270 --> 00:00:33.770
[VIDEO PLAYBACK]

00:00:36.510 --> 00:00:40.170
-Cloud Vision provides powerful
image analytics capabilities

00:00:40.170 --> 00:00:42.130
as easy-to-use APIs.

00:00:42.130 --> 00:00:44.360
It enables
application developers

00:00:44.360 --> 00:00:46.820
to build the next
generation of application

00:00:46.820 --> 00:00:50.580
that can see and understand
the content within the images.

00:00:50.580 --> 00:00:53.300
The service is built on
powerful computer vision

00:00:53.300 --> 00:00:57.670
models that power several
different Google services.

00:00:57.670 --> 00:00:59.960
The service enables
developers to detect

00:00:59.960 --> 00:01:01.990
a broad set of entities
within an image,

00:01:01.990 --> 00:01:05.810
from everyday objects to
faces and product logos.

00:01:05.810 --> 00:01:08.110
The service is so easy to use.

00:01:08.110 --> 00:01:10.470
As one example of
the use cases, you

00:01:10.470 --> 00:01:14.330
can have any Raspberry
PI robot, like GoPiGo,

00:01:14.330 --> 00:01:16.910
calling the Cloud
Vision API directly.

00:01:16.910 --> 00:01:19.580
So the bot can send the
images taken by its camera

00:01:19.580 --> 00:01:23.520
to the cloud, and can get the
analysis result in real time.

00:01:23.520 --> 00:01:26.850
It detect faces in the image,
along with the associated

00:01:26.850 --> 00:01:27.820
emotions.

00:01:27.820 --> 00:01:31.290
The Cloud Vision API is
also able to detect entities

00:01:31.290 --> 00:01:33.220
within the image.

00:01:33.220 --> 00:01:36.036
Now, let's see how
facial detection works.

00:01:36.036 --> 00:01:38.055
Cloud Vision detects
faces on the picture,

00:01:38.055 --> 00:01:42.020
and returns the positions
of eyes, nose and mouth.

00:01:42.020 --> 00:01:45.080
So you can program the
bot to follow the face.

00:01:45.080 --> 00:01:50.240
[ROBOT BEEPING]

00:01:50.240 --> 00:01:54.480
It also detects emotions
such as joy, anger, surprise

00:01:54.480 --> 00:01:55.920
and sorrow.

00:01:55.920 --> 00:01:58.530
So the bot can move
toward smiling faces,

00:01:58.530 --> 00:02:02.070
or avoid anger or
surprised face.

00:02:02.070 --> 00:02:05.315
One of the very interesting
features of Cloud Vision API

00:02:05.315 --> 00:02:07.330
is the entity detection.

00:02:07.330 --> 00:02:10.374
That means it detects
any abject you like.

00:02:10.374 --> 00:02:11.580
Let's see how it works.

00:02:16.480 --> 00:02:17.250
-It's glasses.

00:02:19.824 --> 00:02:22.101
It's banana.

00:02:22.101 --> 00:02:22.770
It's automobile.

00:02:25.290 --> 00:02:26.930
It's money.

00:02:26.930 --> 00:02:28.190
-You see?

00:02:28.190 --> 00:02:30.800
Cloud Vision lets
developers to take advantage

00:02:30.800 --> 00:02:33.050
of Google's latest machine
learning technologies

00:02:33.050 --> 00:02:34.330
quite easily.

00:02:34.330 --> 00:02:37.010
Please go to
cloud.google.com/vision

00:02:37.010 --> 00:02:40.040
to learn more.

00:02:40.040 --> 00:02:41.116
[END PLAYBACK]

00:02:41.116 --> 00:02:42.990
KAZ SATO: So that was
the demonstration video

00:02:42.990 --> 00:02:44.150
for Cloud Vision bots.

00:02:44.150 --> 00:02:46.700
So I'd like to
discuss how to build

00:02:46.700 --> 00:02:50.270
this bot by using the Vision
API in these sessions.

00:02:50.270 --> 00:02:53.030
But I'd like to start
briefly discussing

00:02:53.030 --> 00:02:57.956
about the machine intelligence
working behind the bot.

00:02:57.956 --> 00:02:59.330
GLEN SHIRES: Do
you want to get--

00:02:59.330 --> 00:03:00.330
KAZ SATO: OK.

00:03:00.330 --> 00:03:01.390
Thank you.

00:03:01.390 --> 00:03:06.180
So we are using a technology
called neural networks.

00:03:06.180 --> 00:03:09.640
So what is neural network?

00:03:09.640 --> 00:03:11.440
Neural network is
a function that

00:03:11.440 --> 00:03:13.840
can learn from the
training data set.

00:03:13.840 --> 00:03:17.090
So it's designed to mimic
the behavior of neurons

00:03:17.090 --> 00:03:21.900
inside human brain by using
the matrix operations.

00:03:21.900 --> 00:03:25.730
For example, if you want to
do the image recognitions

00:03:25.730 --> 00:03:27.760
with neural networks,
then you can

00:03:27.760 --> 00:03:30.780
convert your input image,
such as cat images,

00:03:30.780 --> 00:03:34.940
into a Raj vector, and
then put that vector

00:03:34.940 --> 00:03:37.640
to the neural
networks, where it does

00:03:37.640 --> 00:03:39.990
the massive amount of
the matrix operations,

00:03:39.990 --> 00:03:44.300
such as the match applications
or additions between vectors

00:03:44.300 --> 00:03:46.480
and matrices.

00:03:46.480 --> 00:03:49.180
And then, eventually, you'd
have another large vector

00:03:49.180 --> 00:03:53.830
as an output, which represents
the levels of the objects

00:03:53.830 --> 00:03:57.640
detected in an image, such
as the cat, or automobile,

00:03:57.640 --> 00:04:00.170
or the human face.

00:04:00.170 --> 00:04:04.200
So let's take a look
at another example, how

00:04:04.200 --> 00:04:08.280
neural network works, by
using the dataset called

00:04:08.280 --> 00:04:09.520
double spiral.

00:04:09.520 --> 00:04:11.910
In this double
spiral data set, we

00:04:11.910 --> 00:04:14.530
have two groups of
the data points.

00:04:14.530 --> 00:04:18.620
One is the orange group,
then another is blue group.

00:04:18.620 --> 00:04:20.550
If you are a
programmer, and if you

00:04:20.550 --> 00:04:24.920
are asked to classify those data
points, what kind of a program

00:04:24.920 --> 00:04:26.700
code you would write?

00:04:26.700 --> 00:04:29.200
Do you want to write
many if statements

00:04:29.200 --> 00:04:34.154
or [INAUDIBLE]
statements to classify

00:04:34.154 --> 00:04:36.320
each location with the
points by using the condition

00:04:36.320 --> 00:04:37.330
and the thresholds?

00:04:37.330 --> 00:04:38.970
Do you want to write that?

00:04:38.970 --> 00:04:41.040
I don't want you to
write that kind of code.

00:04:41.040 --> 00:04:44.340
Instead, I would be using
machine learning or neural

00:04:44.340 --> 00:04:48.570
networks, so that I can let
the computer try to find a way

00:04:48.570 --> 00:04:49.661
to solve this problem.

00:04:49.661 --> 00:04:51.410
So let's take a look
at the demonstration.

00:04:58.080 --> 00:05:00.870
This is a demonstration
called the Playground, where

00:05:00.870 --> 00:05:05.979
you can just, actually, running
the neural networks to solve

00:05:05.979 --> 00:05:06.520
this problem.

00:05:09.120 --> 00:05:10.820
Now you are seeing,
the computer is

00:05:10.820 --> 00:05:13.280
trying to find the
optimal combination

00:05:13.280 --> 00:05:16.820
of the parameters inside neural
networks to solve this problem.

00:05:16.820 --> 00:05:21.020
Actually, it's not working
right, so let's try it again.

00:05:21.020 --> 00:05:23.390
Sometimes the machine
learning fails,

00:05:23.390 --> 00:05:25.760
so you have to try
multiple times.

00:05:25.760 --> 00:05:27.460
Now here you go.

00:05:27.460 --> 00:05:31.080
The computer found
a way to combine

00:05:31.080 --> 00:05:34.030
the parameters in
an optimal way to do

00:05:34.030 --> 00:05:36.940
the classification with
the double spiral datasets.

00:05:36.940 --> 00:05:40.770
So this is how neural networks
works to solve your problem,

00:05:40.770 --> 00:05:42.720
rather than instructing
the computers how

00:05:42.720 --> 00:05:44.920
to solve the problem by humans.

00:05:48.756 --> 00:05:52.010
OK, go back to this slide.

00:05:52.010 --> 00:05:55.630
And you can apply these
neural network technologies

00:05:55.630 --> 00:05:59.150
to solve much more complex
problems, such as recognizing

00:05:59.150 --> 00:06:02.610
a cat in an image or
recognizing pedestrians

00:06:02.610 --> 00:06:03.820
walking around the street.

00:06:03.820 --> 00:06:07.850
You can do that, but you have
to have many more hidden layers,

00:06:07.850 --> 00:06:12.320
the layers inside the input
vector and output vector.

00:06:12.320 --> 00:06:16.470
So it takes a long time
to finish training.

00:06:16.470 --> 00:06:19.440
That is called deep neural
networks, or deep learnings.

00:06:19.440 --> 00:06:22.470
But the largest problems right
now for deep neural networks

00:06:22.470 --> 00:06:24.580
is the computational resource.

00:06:24.580 --> 00:06:29.060
It usually takes a few
days or a few weeks,

00:06:29.060 --> 00:06:30.810
sometimes, to finish
your trainings

00:06:30.810 --> 00:06:32.950
with deep neural network.

00:06:32.950 --> 00:06:34.680
So that's the reason
why Google has

00:06:34.680 --> 00:06:37.670
been researching on
distributed training

00:06:37.670 --> 00:06:39.300
by using the Google Cloud.

00:06:39.300 --> 00:06:43.970
By using the GPUs or TPUs, that
can shorten the training times

00:06:43.970 --> 00:06:49.650
in order of 1/10 or 1/100.

00:06:49.650 --> 00:06:54.420
And that is the reason why
Google has been so successful

00:06:54.420 --> 00:06:56.770
applying the deep neural
network technologies

00:06:56.770 --> 00:06:59.670
to the many consumer
services, such as the voice

00:06:59.670 --> 00:07:01.930
recognitions with
Android devices,

00:07:01.930 --> 00:07:04.910
or the image recognitions
with Google [INAUDIBLE],

00:07:04.910 --> 00:07:09.390
or the ranking in the
Google search services.

00:07:09.390 --> 00:07:12.540
Now, we have over 20 products
and services at Google that

00:07:12.540 --> 00:07:15.280
have been using the deep
learning [? technologies ?]

00:07:15.280 --> 00:07:17.540
[? underlying. ?]

00:07:17.540 --> 00:07:20.230
And now, we have
started to externalize

00:07:20.230 --> 00:07:22.590
the power of the neural
networks running on Google

00:07:22.590 --> 00:07:25.820
Cloud to external developers.

00:07:25.820 --> 00:07:30.460
The first product is
called Cloud Vision API,

00:07:30.460 --> 00:07:36.050
and the second product is
called Cloud Speech API.

00:07:36.050 --> 00:07:38.122
What is Cloud Vision API?

00:07:38.122 --> 00:07:42.790
Cloud Vision API is an
image analysis service that

00:07:42.790 --> 00:07:44.180
provides the pre-trained model.

00:07:44.180 --> 00:07:46.620
So you don't have to train
your own neural networks

00:07:46.620 --> 00:07:48.990
or machine learning model.

00:07:48.990 --> 00:07:51.680
Rather than that, you can
just use the REST API,

00:07:51.680 --> 00:07:55.540
just sending your own
images uploaded to the API.

00:07:55.540 --> 00:07:57.560
Then, you are going to
be receiving the analysis

00:07:57.560 --> 00:08:01.450
result in a JSON format
in a few seconds.

00:08:01.450 --> 00:08:04.410
So you don't have to have any
machine learning skill set

00:08:04.410 --> 00:08:08.430
or experience, and
it's so inexpensive.

00:08:08.430 --> 00:08:12.950
It only costs $2.50 per
1000 units or images,

00:08:12.950 --> 00:08:18.370
and it takes no charge to
start trying out the API.

00:08:18.370 --> 00:08:21.651
So let's look at another
demonstration for Cloud Vision

00:08:21.651 --> 00:08:22.151
API.

00:08:26.780 --> 00:08:29.380
Here, I'm launching a
demonstration called

00:08:29.380 --> 00:08:33.919
Cloud Vision Explorer, where
we have imported over 80,000

00:08:33.919 --> 00:08:37.690
images from Wikimedia commons,
uploaded on the Google Cloud

00:08:37.690 --> 00:08:41.720
storage, and applied the
Vision API analysis already.

00:08:41.720 --> 00:08:46.130
And by using the result
of the Vision API,

00:08:46.130 --> 00:08:48.330
we have done
clustering analysis,

00:08:48.330 --> 00:08:51.730
so that you are seeing
the cluster of the images,

00:08:51.730 --> 00:08:55.130
such as sea cluster, or
snow cluster, or estate

00:08:55.130 --> 00:08:58.020
or residential area cluster.

00:08:58.020 --> 00:09:00.890
And if you take a look
at the cluster for cats,

00:09:00.890 --> 00:09:04.960
then you'll be seeing the
many images that is classified

00:09:04.960 --> 00:09:08.520
as a cat, like these.

00:09:08.520 --> 00:09:10.450
If you put these
images through the API,

00:09:10.450 --> 00:09:14.480
the API will be sending back
the results, such as the cat,

00:09:14.480 --> 00:09:17.670
or pet, or this must
be a British shorthair.

00:09:17.670 --> 00:09:21.700
All those results will be
returned in the JSON format

00:09:21.700 --> 00:09:23.480
like this.

00:09:23.480 --> 00:09:29.770
But in these demonstrations, you
can see them in a graphical UI.

00:09:29.770 --> 00:09:33.870
Also, if your image
has text inside it,

00:09:33.870 --> 00:09:37.800
then the API can convert
the text inside the images

00:09:37.800 --> 00:09:41.290
to a string, like this,
with tree kangaroos crossing

00:09:41.290 --> 00:09:42.990
next two kilometers.

00:09:42.990 --> 00:09:46.650
You can get it as a string.

00:09:46.650 --> 00:09:50.210
Or if you have the
faces in your images,

00:09:50.210 --> 00:09:53.440
then the API can detect
the faces and the locations

00:09:53.440 --> 00:09:57.390
of each face, and also the
emotions of each faces,

00:09:57.390 --> 00:10:01.490
such as joy, or sorrow,
anger, or surprise.

00:10:01.490 --> 00:10:05.092
So you can easily find which
face is smiling or not.

00:10:07.870 --> 00:10:12.620
If your image contains
very popular locations,

00:10:12.620 --> 00:10:17.530
then it can provide the name
of the popular location,

00:10:17.530 --> 00:10:20.870
such as the Citi Field stadium
in New York City with longitude

00:10:20.870 --> 00:10:22.200
and latitude.

00:10:22.200 --> 00:10:26.370
You can even put a
marker on Google Maps.

00:10:26.370 --> 00:10:28.510
Oh, sorry.

00:10:32.980 --> 00:10:36.770
You can even use the product
logo detection features

00:10:36.770 --> 00:10:39.520
so that you can
easily understand

00:10:39.520 --> 00:10:42.128
the image has the product
logo or corporate logo.

00:10:44.820 --> 00:10:48.310
So there was the demonstrations
of the Vision API.

00:10:48.310 --> 00:10:51.350
So let's take a look at how
you can bring this machine

00:10:51.350 --> 00:10:54.810
intelligence into details
the Raspberry Pi bot.

00:10:54.810 --> 00:10:57.930
So Cloud Vision bot is
based on a Raspberry Pi

00:10:57.930 --> 00:11:01.870
robot called GoPiGo, which is
produced by Dexter Industries.

00:11:01.870 --> 00:11:04.420
So you can go to
Dexter's website

00:11:04.420 --> 00:11:07.110
to buy GoPiGo at around $200.

00:11:07.110 --> 00:11:10.150
And also, you may want
to buy a fisheye camera

00:11:10.150 --> 00:11:14.140
to capture the wider range
of views surrounding the bot.

00:11:14.140 --> 00:11:17.210
And we have written a few of
handwritten lines of Python

00:11:17.210 --> 00:11:20.600
code to capture the
image from my camera

00:11:20.600 --> 00:11:22.165
and send the image to the API.

00:11:25.140 --> 00:11:26.960
It's really easy
to start getting

00:11:26.960 --> 00:11:28.320
started with the Vision API.

00:11:28.320 --> 00:11:32.500
You can just go to
cloud.google.com/vision

00:11:32.500 --> 00:11:34.280
to getting started.

00:11:34.280 --> 00:11:38.270
You can try the
Quickstart tutorial,

00:11:38.270 --> 00:11:42.180
and that should be
finished within 30 minutes.

00:11:42.180 --> 00:11:45.530
This is a sample of Python
code to send your image

00:11:45.530 --> 00:11:47.180
data to the API.

00:11:47.180 --> 00:11:52.930
You have to convert the image
binaries into the base 64 text,

00:11:52.930 --> 00:11:55.000
and embed that text
into the content

00:11:55.000 --> 00:11:56.590
property of the request.

00:11:56.590 --> 00:12:00.180
And also, you have to specify
the types of the features you

00:12:00.180 --> 00:12:01.060
want to detect.

00:12:01.060 --> 00:12:03.540
In this case, it's specified
the label detections

00:12:03.540 --> 00:12:05.810
as the features,
so that you will

00:12:05.810 --> 00:12:09.890
be receiving the labels as a
result of the API analysis.

00:12:09.890 --> 00:12:12.670
And you can make
the call to the API.

00:12:12.670 --> 00:12:16.490
Then, you will be receiving the
result, JSON, in a few seconds,

00:12:16.490 --> 00:12:19.070
so that you can easily
dig into the JSON result

00:12:19.070 --> 00:12:22.640
to getting out the labels.

00:12:22.640 --> 00:12:26.020
If you specify face
detection, then you'll

00:12:26.020 --> 00:12:29.770
be receiving the positions
of the face, and landmarks,

00:12:29.770 --> 00:12:33.290
such as the boundingPoly
properties, where you would

00:12:33.290 --> 00:12:36.540
have the x and y positions.

00:12:36.540 --> 00:12:39.900
And also, you will have
the joylikelihood property,

00:12:39.900 --> 00:12:43.260
where you can find each
face is smiling or not.

00:12:43.260 --> 00:12:46.680
So it's really easy to
write a Python code to turn

00:12:46.680 --> 00:12:49.650
the bot into the
direction of the face,

00:12:49.650 --> 00:12:51.730
and also if it's
smiling, then you

00:12:51.730 --> 00:12:57.730
can run the motors of the
bot to follow the person.

00:12:57.730 --> 00:13:01.750
So let's take a look at the real
demonstration of the Vision API

00:13:01.750 --> 00:13:02.250
bot.

00:13:08.260 --> 00:13:10.140
So I'm showing the console.

00:13:14.370 --> 00:13:17.280
So this is the user interface
web console for the bot.

00:13:17.280 --> 00:13:24.550
So actually, it's showing the
vision it is taking right now.

00:13:24.550 --> 00:13:27.110
Not sure it's working or not.

00:13:27.110 --> 00:13:28.317
Must be working.

00:13:28.317 --> 00:13:29.150
I hope it's working.

00:13:32.450 --> 00:13:34.666
Is it working?

00:13:34.666 --> 00:13:35.750
Not sure.

00:13:35.750 --> 00:13:36.950
Let me try.

00:13:36.950 --> 00:13:40.380
So if you put the flowers--
it's not saying anything.

00:13:55.200 --> 00:13:57.061
So it looks like it's
not working anymore.

00:14:00.150 --> 00:14:01.690
Maybe I can try
fixing this stuff.

00:14:01.690 --> 00:14:04.344
So while doing that, maybe
I can pass the stage to him.

00:14:04.344 --> 00:14:05.010
GLEN SHIRES: OK.

00:14:08.100 --> 00:14:10.510
So you want me to
jump into my slides?

00:14:10.510 --> 00:14:11.614
KAZ SATO: Yeah.

00:14:11.614 --> 00:14:12.280
GLEN SHIRES: OK.

00:14:15.106 --> 00:14:15.605
Yeah.

00:14:15.605 --> 00:14:16.104
Hi.

00:14:16.104 --> 00:14:20.730
So I'd also like to talk about
the Cloud Speech API, which

00:14:20.730 --> 00:14:22.340
is actually a rather new API.

00:14:22.340 --> 00:14:24.860
It was released about
a month and a half ago,

00:14:24.860 --> 00:14:28.100
and it joins a
number of speech APIs

00:14:28.100 --> 00:14:30.020
that Google has had for
quite some time, quite

00:14:30.020 --> 00:14:31.550
a number of years.

00:14:31.550 --> 00:14:34.140
You're probably familiar with
the Android Speech API, which

00:14:34.140 --> 00:14:37.950
allows you to do speech-to-text
and text-to-speech on Android

00:14:37.950 --> 00:14:42.850
devices, phones,
tablets, autos, TV, etc.

00:14:42.850 --> 00:14:44.910
So that's in Java.

00:14:44.910 --> 00:14:47.240
That's a Java-based API.

00:14:47.240 --> 00:14:49.090
There's also the Web
Speech API, which

00:14:49.090 --> 00:14:53.040
is in Chrome, which is a
JavaScript-based API that

00:14:53.040 --> 00:14:55.130
allows you to do speech.

00:14:55.130 --> 00:14:57.870
And the Cloud Speech
API is a new API

00:14:57.870 --> 00:15:01.990
that we've released that allows
you to put this on any device.

00:15:01.990 --> 00:15:06.100
So whatever device or server
you'd like to put speech into,

00:15:06.100 --> 00:15:07.810
we've made it very
easy, and we support

00:15:07.810 --> 00:15:09.920
quite a number of languages.

00:15:09.920 --> 00:15:12.720
So we've made it very
easy to integrate this

00:15:12.720 --> 00:15:16.720
into all sorts of different
clients and servers.

00:15:16.720 --> 00:15:20.600
The Cloud Speech API is powered
by Google's machine learning.

00:15:20.600 --> 00:15:23.310
So we've got a lot of
experience with speech,

00:15:23.310 --> 00:15:25.905
and we've built that
all into the API.

00:15:25.905 --> 00:15:29.470
So the same powerful engine
that you have on Android,

00:15:29.470 --> 00:15:33.200
you have on Chrome, you now have
available for whatever project

00:15:33.200 --> 00:15:34.810
you'd like to use it on.

00:15:34.810 --> 00:15:38.440
The models are
pre-trained, so you do not

00:15:38.440 --> 00:15:41.770
have to learn machine learning
to specifically use it.

00:15:41.770 --> 00:15:45.520
You can just get up and
running immediately.

00:15:45.520 --> 00:15:48.080
And it supports over 80
languages and variants

00:15:48.080 --> 00:15:49.780
of languages.

00:15:49.780 --> 00:15:51.850
It's got real-time streaming.

00:15:51.850 --> 00:15:56.270
And so, what real-time streaming
means is that, as I'm talking,

00:15:56.270 --> 00:15:59.260
the text is actually coming
out while I'm talking.

00:15:59.260 --> 00:16:02.390
And what I'd like to do is
give you a quick demo of that.

00:16:06.510 --> 00:16:09.470
So I'm sure you're familiar
with this page, which has speech

00:16:09.470 --> 00:16:13.240
on it, Google search page.

00:16:13.240 --> 00:16:16.640
Web Speech API.

00:16:16.640 --> 00:16:21.150
What I'm doing here is pulling
up a demonstration page

00:16:21.150 --> 00:16:23.540
for the Chrome Web Speech API.

00:16:23.540 --> 00:16:27.589
And make it a little
bit bigger here.

00:16:27.589 --> 00:16:29.380
So what I'm going to
do is, as I'm talking,

00:16:29.380 --> 00:16:31.590
I want you to notice that
the words are coming out.

00:16:31.590 --> 00:16:33.470
And actually, first,
they'll be gray words,

00:16:33.470 --> 00:16:34.845
because it's not
quite sure, then

00:16:34.845 --> 00:16:39.930
when it's very confident of the
words, it turns them to black.

00:16:39.930 --> 00:16:43.570
So you can see, as I'm speaking,
the words are coming out

00:16:43.570 --> 00:16:47.000
and appearing on the screen.

00:16:47.000 --> 00:16:51.034
They turn black after it's
very confident of what I said.

00:16:56.404 --> 00:16:57.612
And where's the presentation?

00:17:00.640 --> 00:17:02.500
OK.

00:17:02.500 --> 00:17:05.440
So that's an example of the
real-time streaming that's

00:17:05.440 --> 00:17:06.510
built in.

00:17:06.510 --> 00:17:09.200
And we have a limited
preview right now,

00:17:09.200 --> 00:17:11.119
which you can sign
up and join and start

00:17:11.119 --> 00:17:12.440
using the Cloud Speech API.

00:17:16.670 --> 00:17:19.089
Cloud Speech API is
actually two different

00:17:19.089 --> 00:17:23.135
API, or at least two
versions of the same API,

00:17:23.135 --> 00:17:25.240
is probably a better
way to put it.

00:17:25.240 --> 00:17:27.890
There's a REST API, that's
a very, very simple way

00:17:27.890 --> 00:17:29.250
to use it.

00:17:29.250 --> 00:17:30.880
You can get started immediately.

00:17:30.880 --> 00:17:34.400
It's as simple as writing a
cURL command and some JSON.

00:17:34.400 --> 00:17:36.320
And then there's
Remote Procedure Calls,

00:17:36.320 --> 00:17:39.290
which gives you more power.

00:17:39.290 --> 00:17:41.560
Let me show you the REST API.

00:17:41.560 --> 00:17:44.060
And this is literally
everything you

00:17:44.060 --> 00:17:48.220
need to know to do the
REST API, on one slide.

00:17:48.220 --> 00:17:51.420
You see on the left,
there's a JSON request,

00:17:51.420 --> 00:17:52.600
and you can formulate that.

00:17:52.600 --> 00:17:54.516
You can make it more
complicated, if you want,

00:17:54.516 --> 00:17:56.870
add languages, add
different types

00:17:56.870 --> 00:18:00.670
of ways you want to process it.

00:18:00.670 --> 00:18:02.230
You can even add context.

00:18:02.230 --> 00:18:03.780
A new thing we
released is the fact

00:18:03.780 --> 00:18:08.572
that you can add new words to
the vocabulary, new phrases,

00:18:08.572 --> 00:18:10.030
and so that's coming
out this week.

00:18:10.030 --> 00:18:11.946
So you can make it as
complicated as you want.

00:18:11.946 --> 00:18:15.180
But the simplest requests
would be exactly that,

00:18:15.180 --> 00:18:19.200
those couple lines of JSON
with the content, where

00:18:19.200 --> 00:18:22.660
you insert your audio
file or your audio data.

00:18:22.660 --> 00:18:26.460
If you look at the bottom,
there's a cURL command.

00:18:26.460 --> 00:18:28.500
Kind of long, but
basically, all it's posting

00:18:28.500 --> 00:18:32.200
is a content type
and posting to a URL.

00:18:32.200 --> 00:18:34.770
So it's taking that JSON
and posting to the URL,

00:18:34.770 --> 00:18:38.140
and what you get back is that
response that's on your right.

00:18:38.140 --> 00:18:40.430
And again, this is the
simplest type of response.

00:18:40.430 --> 00:18:42.760
If all you want is
one alternative,

00:18:42.760 --> 00:18:44.680
you don't want to see
multiple alternatives

00:18:44.680 --> 00:18:47.130
and interim results,
you'll get something

00:18:47.130 --> 00:18:48.226
that looks just like this.

00:18:53.760 --> 00:18:56.730
So as I said, there's
actually two types of APIs.

00:18:56.730 --> 00:19:00.180
The other one is the
Remote Procedure Call API.

00:19:00.180 --> 00:19:04.150
And what that means is, you can
do everything by simply calling

00:19:04.150 --> 00:19:06.000
methods in your
favorite language,

00:19:06.000 --> 00:19:09.820
in either Java or C++, or the
10 different languages that are

00:19:09.820 --> 00:19:11.960
supported.

00:19:11.960 --> 00:19:13.880
So you don't have to
worry about the network.

00:19:13.880 --> 00:19:16.550
And it also supports the
bi-directional streaming

00:19:16.550 --> 00:19:18.660
that I mentioned, so
that as you're talking,

00:19:18.660 --> 00:19:21.208
you're getting the data back.

00:19:24.510 --> 00:19:28.760
So we support quite
a number of languages

00:19:28.760 --> 00:19:33.120
for the Remote Procedure
Call, and this is actually

00:19:33.120 --> 00:19:33.734
open-source.

00:19:33.734 --> 00:19:35.400
It's free and
open-source, so if there's

00:19:35.400 --> 00:19:37.450
a language that
doesn't appear here,

00:19:37.450 --> 00:19:39.880
you can certainly build the
source for that language,

00:19:39.880 --> 00:19:42.730
and actually use
it on any language.

00:19:42.730 --> 00:19:48.300
It also uses HTTPS/2,
HTTP/2 secure,

00:19:48.300 --> 00:19:50.760
which allows you to
have some very robust

00:19:50.760 --> 00:19:51.940
bi-directional streaming.

00:19:57.080 --> 00:19:59.460
So what I'd like to do
is demonstrate this.

00:20:11.620 --> 00:20:12.250
Go straight.

00:20:30.590 --> 00:20:31.140
Stand still.

00:20:31.140 --> 00:20:31.640
Thank you.

00:20:31.640 --> 00:20:32.950
I like that command.

00:20:32.950 --> 00:20:35.780
[LAUGHTER]

00:20:35.780 --> 00:20:36.770
I'm going to work it.

00:20:36.770 --> 00:20:39.250
Sit.

00:20:39.250 --> 00:20:39.953
Go to sleep.

00:20:51.640 --> 00:20:55.700
So I'm going to start
that over, and we'll see.

00:20:58.570 --> 00:21:01.370
Let me move forward.

00:21:01.370 --> 00:21:03.210
So let me show you,
while we're waiting

00:21:03.210 --> 00:21:08.180
for that to reboot,
exactly what we're

00:21:08.180 --> 00:21:11.650
sending with the RPC calls.

00:21:11.650 --> 00:21:14.085
We're sending an
initial request.

00:21:14.085 --> 00:21:15.890
Now, this is similar
to the JSON that I

00:21:15.890 --> 00:21:18.480
showed in the last
slide, but in here, we're

00:21:18.480 --> 00:21:23.190
actually using proto
buffers to send this.

00:21:23.190 --> 00:21:25.054
So it's a very compact format.

00:21:25.054 --> 00:21:26.470
In other words,
you're not sending

00:21:26.470 --> 00:21:29.180
extra bytes over the wire,
the way you are with JSON.

00:21:31.950 --> 00:21:34.640
So you send your request.

00:21:34.640 --> 00:21:36.830
You capture audio
from the microphone.

00:21:36.830 --> 00:21:39.380
And in this case, what
we're doing is, on a thread,

00:21:39.380 --> 00:21:42.280
we're reading a
buffer full of audio

00:21:42.280 --> 00:21:45.200
and sending that
buffer of audio.

00:21:45.200 --> 00:21:49.280
You will see, what we're
calling here is onNext.

00:21:49.280 --> 00:21:51.890
So this request
observer, onNext,

00:21:51.890 --> 00:21:55.820
is code that's automatically
supplied in any of 10 languages

00:21:55.820 --> 00:21:58.880
so that you can keep
passing new buffers to it.

00:21:58.880 --> 00:22:00.532
And finally, you've
got the response,

00:22:00.532 --> 00:22:02.490
which you can be running
on a different thread,

00:22:02.490 --> 00:22:04.880
because you're sending
audio and receiving data

00:22:04.880 --> 00:22:06.660
at the same time.

00:22:06.660 --> 00:22:09.900
And again, there's an
onNext command here

00:22:09.900 --> 00:22:12.120
that's provided
automatically for you,

00:22:12.120 --> 00:22:13.570
which provides the data.

00:22:13.570 --> 00:22:15.590
And in this case, we're
printing out the results

00:22:15.590 --> 00:22:16.589
of what we're receiving.

00:22:25.140 --> 00:22:25.800
Go forward.

00:22:33.590 --> 00:22:36.430
Spin left.

00:22:36.430 --> 00:22:37.345
Spin right.

00:22:40.600 --> 00:22:43.450
Go backwards.

00:22:43.450 --> 00:22:44.025
Do a dance.

00:22:48.820 --> 00:22:51.871
Go to sleep.

00:22:51.871 --> 00:22:52.370
Play dead.

00:22:52.370 --> 00:22:53.766
I like that one, too.

00:22:53.766 --> 00:22:55.960
[APPLAUSE]

00:22:55.960 --> 00:22:56.712
So there we are.

00:22:59.962 --> 00:23:01.720
So one thing I
wanted to point out

00:23:01.720 --> 00:23:03.550
is, it's responding
very quickly,

00:23:03.550 --> 00:23:06.100
and that's because it's
streaming the speech as I'm

00:23:06.100 --> 00:23:06.760
speaking.

00:23:06.760 --> 00:23:09.218
So it's not going to capture
all the speech, and then wait,

00:23:09.218 --> 00:23:11.340
and then send a big
chunk of data up.

00:23:11.340 --> 00:23:13.900
As I'm speaking, it's going
bi-directional, and so, that's

00:23:13.900 --> 00:23:16.340
why I can do it so quickly.

00:23:16.340 --> 00:23:18.196
And my clicker is here.

00:23:22.280 --> 00:23:26.480
So what I'd like to
do is-- this is easier

00:23:26.480 --> 00:23:27.950
to show than to talk about.

00:23:27.950 --> 00:23:28.980
So let me show this.

00:23:31.800 --> 00:23:34.460
These are experimental
features, I want to say.

00:23:34.460 --> 00:23:38.380
What I've showed you today
is what's available right now

00:23:38.380 --> 00:23:39.550
on the Cloud Speech API.

00:23:39.550 --> 00:23:43.031
These are experimental features
that will be available soon.

00:23:49.710 --> 00:23:50.955
What time is it in Tokyo?

00:23:53.980 --> 00:23:59.870
ROBOT: The time in
Tokyo, Japan is 9:29 AM.

00:23:59.870 --> 00:24:02.955
GLEN SHIRES: How do you say,
"when is the next train?"

00:24:02.955 --> 00:24:03.635
in French.

00:24:06.600 --> 00:24:09.749
ROBOT: [SPEAKING FRENCH]

00:24:09.749 --> 00:24:15.128
[APPLAUSE]

00:24:22.970 --> 00:24:26.900
GLEN SHIRES: Turn
on the table lamp.

00:24:26.900 --> 00:24:30.598
ROBOT: Not sure how to help
with "turn on the table lamp."

00:24:30.598 --> 00:24:31.639
GLEN SHIRES: There we go.

00:24:35.391 --> 00:24:38.240
Go to sleep.

00:24:38.240 --> 00:24:40.870
So as you can see,
I've demonstrated two

00:24:40.870 --> 00:24:42.420
different things in that demo.

00:24:42.420 --> 00:24:45.540
The first one is spoken answers,
and what that's doing is,

00:24:45.540 --> 00:24:50.150
as I'm asking questions,
it's providing answers.

00:24:50.150 --> 00:24:52.400
And the second one is
something that we've actually

00:24:52.400 --> 00:24:57.700
integrated with If This Then
That, which is a way that you

00:24:57.700 --> 00:25:00.150
can integrate with all
sorts of different devices--

00:25:00.150 --> 00:25:02.830
in this case, I've integrated
with a light module

00:25:02.830 --> 00:25:07.390
controller-- so that
you can actually set up

00:25:07.390 --> 00:25:10.350
your own triggers to do this.

00:25:10.350 --> 00:25:13.660
And this is exactly what I did.

00:25:13.660 --> 00:25:16.140
There's a web page
that I went to,

00:25:16.140 --> 00:25:18.340
and I typed in what
I wanted to say

00:25:18.340 --> 00:25:22.910
as triggers, voice triggers,
various ways I wanted

00:25:22.910 --> 00:25:28.080
to say it, and what it's going
to respond, when I do say that.

00:25:28.080 --> 00:25:31.270
And what it does is, it goes
out and configures this.

00:25:31.270 --> 00:25:33.830
And now, when I speak
that phrase, or one

00:25:33.830 --> 00:25:38.080
of those phrases, it
goes out and triggers

00:25:38.080 --> 00:25:40.450
whatever IFTTT action you
would like to trigger.

00:25:50.180 --> 00:25:51.795
So do you want to
go back to yours?

00:25:51.795 --> 00:25:52.420
KAZ SATO: Yeah.

00:25:52.420 --> 00:25:57.610
Let me try the-- I'm not
sure if it's working or not.

00:25:57.610 --> 00:25:59.240
Maybe I can show the console.

00:25:59.240 --> 00:25:59.740
Yeah.

00:25:59.740 --> 00:26:03.760
Let me try again with the bot.

00:26:03.760 --> 00:26:05.463
Oh, looks like it's working.

00:26:11.790 --> 00:26:12.290
Yeah.

00:26:12.290 --> 00:26:13.660
It looks like it's working.

00:26:18.410 --> 00:26:21.202
ROBOT: It's the goose.

00:26:21.202 --> 00:26:21.910
It's [INAUDIBLE].

00:26:33.840 --> 00:26:34.640
It's currency.

00:26:34.640 --> 00:26:36.187
Are you smiling?

00:26:36.187 --> 00:26:36.770
KAZ SATO: Yes.

00:26:36.770 --> 00:26:38.430
ROBOT: How are you?

00:26:38.430 --> 00:26:39.530
KAZ SATO: I'm fine.

00:26:39.530 --> 00:26:40.030
Thank you.

00:26:40.030 --> 00:26:41.196
ROBOT: It's not [INAUDIBLE].

00:26:41.196 --> 00:26:43.830
[LAUGHTER] It's [INAUDIBLE].

00:26:43.830 --> 00:26:46.747
KAZ SATO: Yes, it is.

00:26:46.747 --> 00:26:47.580
ROBOT: It's goggles.

00:26:47.580 --> 00:26:48.144
Hello.

00:26:48.144 --> 00:26:49.060
KAZ SATO: Not goggles.

00:26:51.600 --> 00:26:54.700
ROBOT: It's [INAUDIBLE].

00:26:54.700 --> 00:26:55.602
It's eyewear.

00:26:55.602 --> 00:26:56.810
KAZ SATO: Yeah, it's eyewear.

00:27:00.240 --> 00:27:01.220
How about this?

00:27:01.220 --> 00:27:04.240
Yeah, if you keep smiling, then
it tries to follow towards me.

00:27:07.790 --> 00:27:08.921
What is this?

00:27:08.921 --> 00:27:10.364
Can you see this?

00:27:13.980 --> 00:27:14.480
Ah.

00:27:25.940 --> 00:27:27.780
Thank you.

00:27:27.780 --> 00:27:30.297
So yeah, it works, finally.

00:27:30.297 --> 00:27:31.380
ROBOT: It was [INAUDIBLE].

00:27:31.380 --> 00:27:32.180
KAZ SATO: Yeah, thank you.

00:27:32.180 --> 00:27:33.620
So that was our demonstration.

00:27:33.620 --> 00:27:35.564
[LAUGHTER]

00:27:35.564 --> 00:27:39.452
[APPLAUSE]

00:27:39.659 --> 00:27:40.950
KAZ SATO: OK, shall we go back?

00:27:40.950 --> 00:27:41.700
GLEN SHIRES: Yeah.

00:27:46.030 --> 00:27:48.230
OK.

00:27:48.230 --> 00:27:52.000
One thing I failed to mention,
with the IFTTT triggers,

00:27:52.000 --> 00:27:54.700
you can actually add
parameters to those triggers.

00:27:54.700 --> 00:27:56.820
So you could add a number
or a string parameter,

00:27:56.820 --> 00:27:59.470
and you could, for
example, tell the robot

00:27:59.470 --> 00:28:03.440
to turn left a certain
number of steps or degrees.

00:28:03.440 --> 00:28:06.515
So you can make the triggers,
actually, quite interesting.

00:28:09.170 --> 00:28:12.270
So we have a number of
resources that you should

00:28:12.270 --> 00:28:14.380
go out and take a look at.

00:28:14.380 --> 00:28:18.990
The Cloud Vision API
and the Cloud Speech API

00:28:18.990 --> 00:28:20.590
are ready for sign up.

00:28:20.590 --> 00:28:24.740
And we also have-- well,
first of all, thank you--

00:28:24.740 --> 00:28:27.310
but we also have a
number of other sessions

00:28:27.310 --> 00:28:31.320
that are coming up that
you might be interested in.

00:28:31.320 --> 00:28:35.040
We have code labs that talk
about machine learning.

00:28:35.040 --> 00:28:38.270
There's several machine learning
presentations coming up.

00:28:38.270 --> 00:28:39.740
There's Cloud
office hours, if you

00:28:39.740 --> 00:28:42.670
want to learn more about Cloud
and integrating with the Cloud

00:28:42.670 --> 00:28:43.850
APIs.

00:28:43.850 --> 00:28:47.550
We have office hours
throughout the next few days,

00:28:47.550 --> 00:28:50.180
and there's the Sandbox.

00:28:50.180 --> 00:28:52.440
So thank you very much.

00:28:52.440 --> 00:28:55.440
[APPLAUSE]

00:28:55.440 --> 00:28:58.490
[MUSIC PLAYING]

