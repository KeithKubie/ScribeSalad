WEBVTT

1
00:00:00.080 --> 00:00:03.330
[inaudible].

2
00:00:05.020 --> 00:00:07.450
<v 1>So let's talk about some of the specifics here.</v>

3
00:00:07.720 --> 00:00:10.720
So I wanted to go to one that I think we disagree and I thought I want you to,

4
00:00:10.721 --> 00:00:13.850
I want you to make the most compelling case that you can,
uh,

5
00:00:14.000 --> 00:00:16.850
this open borders stuff.
Yes.
So you're not,
you're not a,

6
00:00:16.920 --> 00:00:19.360
I'm not an open borders guy.
This is a,

7
00:00:19.361 --> 00:00:24.190
I'm not a crazy borders person or whatever that means.
I don't,
I don't think,

8
00:00:24.280 --> 00:00:27.430
I don't think there should be no immigration.
We are a nation of immigrants.
Uh,

9
00:00:27.431 --> 00:00:28.750
that's what's made this melting pot.

10
00:00:28.751 --> 00:00:30.530
I think the greatest nation in the history of the world,

11
00:00:30.531 --> 00:00:34.750
that's given more freedom to more people than,
than anyone ever.
Um,

12
00:00:34.780 --> 00:00:38.410
but the open borders situation,
especially now in 2018,

13
00:00:38.770 --> 00:00:42.290
2018 strikes me as particularly tenuous,
but take it away.

14
00:00:42.880 --> 00:00:46.530
So I think one thing to keep in mind is what exactly,
uh,

15
00:00:46.600 --> 00:00:49.630
an advocate of open borders is arguing for.

16
00:00:49.750 --> 00:00:53.110
So I think most so open borders is maybe a bit of a misnomer.

17
00:00:53.410 --> 00:00:57.340
It's more light.
Borders are porous borders or something like that where,

18
00:00:57.341 --> 00:01:01.960
so the idea wouldn't be that there's perhaps no checkpoints are no restrictions

19
00:01:01.961 --> 00:01:03.460
on immigration,
just very few.

20
00:01:03.610 --> 00:01:08.380
So I think most open border folks would be happy saying that if you are a wanton

21
00:01:08.381 --> 00:01:09.161
violent criminal,

22
00:01:09.161 --> 00:01:13.780
that might exclude you from integrating if you have some particularly deadly

23
00:01:13.781 --> 00:01:18.310
contagious disease and so forth.
But other than that,
uh,
sort of ordinary,

24
00:01:18.311 --> 00:01:22.000
peaceful migrants should be able to come to the United States.
So as far as the,

25
00:01:22.001 --> 00:01:25.950
the argument goes,
I think they're really two strands.
One is just that the,

26
00:01:25.951 --> 00:01:29.050
the economic benefits of immigration are huge.

27
00:01:29.170 --> 00:01:33.940
So the most optimistic estimates say that we could potentially double worlds GDP

28
00:01:34.120 --> 00:01:37.870
by opening borders.
So we've done a pretty good job,
uh,
opening up trade.

29
00:01:37.900 --> 00:01:39.940
We've done a much worse job opening up borders,

30
00:01:40.090 --> 00:01:43.210
but in terms of the productivity gains that it would be incredible better than

31
00:01:43.211 --> 00:01:45.190
anything else we could do a,

32
00:01:45.191 --> 00:01:48.730
and this goes back to what I was discussing earlier with our obligations to the

33
00:01:48.731 --> 00:01:51.940
global poor foreign aid doesn't have a good track record and military

34
00:01:51.941 --> 00:01:53.620
intervention doesn't have a good track record.

35
00:01:53.890 --> 00:01:58.890
But allowing people to move from places with low wages to higher wages is by far

36
00:01:59.441 --> 00:02:01.720
the best antipoverty tool that we have.

37
00:02:01.780 --> 00:02:04.360
So if we were going to get into the nitty gritty,
it's never that,

38
00:02:04.390 --> 00:02:08.320
what do you actually have to do then to secure the borders,
right?

39
00:02:08.321 --> 00:02:09.940
Cause that's really where there's about.
So even if,

40
00:02:10.390 --> 00:02:12.760
if you make every economic argument that I'm like,
all right,

41
00:02:12.761 --> 00:02:15.130
I can't wiggle my way out of that.
Right?

42
00:02:15.131 --> 00:02:19.150
Like if I'm on board that you still have to do something to make sure that

43
00:02:19.360 --> 00:02:20.500
you're doing those things,

44
00:02:20.501 --> 00:02:22.780
making sure that murderers are coming in and all of that.

45
00:02:22.780 --> 00:02:24.190
And we don't seem very good at that.
Right.

46
00:02:24.310 --> 00:02:28.660
And I think maybe that's the leap that gets me to where I can't make the

47
00:02:29.070 --> 00:02:31.420
secondary part that you're talking about here.
Yeah.

48
00:02:31.421 --> 00:02:33.580
So I'm not sure about the policy specifics.
I mean,

49
00:02:33.581 --> 00:02:36.550
something like the old Ellis island system seems pretty good.
I mean,

50
00:02:36.551 --> 00:02:40.510
we had a ton of immigration,
um,
back then,

51
00:02:40.511 --> 00:02:45.280
but there still was some kind of a checking mechanism.

52
00:02:45.400 --> 00:02:49.300
So as far so,
I doubt that that involves building a wall.
Um,

53
00:02:49.330 --> 00:02:51.890
but having some kind of,
uh,
you know,

54
00:02:51.940 --> 00:02:55.180
centralized location where immigrants can come and get documents and things like

55
00:02:55.181 --> 00:02:57.610
that seems,
seems pretty reasonable.
Uh,

56
00:02:57.640 --> 00:03:00.940
but that's probably the most I could give you on policy ideas.
Yeah.

57
00:03:01.090 --> 00:03:04.000
<v 2>Right.
So then when you go over these borders,
then your gut,</v>

58
00:03:04.030 --> 00:03:07.450
you would want these people to be governed by the laws of the place that they

59
00:03:07.451 --> 00:03:07.840
were in,

60
00:03:07.840 --> 00:03:12.330
<v 1>right?
That's right.
Yeah.
So,
so I would be happy to write,
uh,</v>

61
00:03:12.350 --> 00:03:15.430
an allow ordinary,
peaceful migrants to come in,
uh,

62
00:03:15.470 --> 00:03:18.620
and then apply for US citizenship.
Just,
you know,
uh,

63
00:03:19.400 --> 00:03:21.020
make it pretty easy to do that.
Right.

64
00:03:21.020 --> 00:03:24.410
<v 2>Is this still a tough one for a guy that doesn't trust the government that much?</v>

65
00:03:24.560 --> 00:03:27.440
Cause that's what's ringing with me here.
And when I've heard this argument,

66
00:03:27.441 --> 00:03:29.990
it's like,
all right,
wait,
if you don't trust the government,

67
00:03:30.020 --> 00:03:33.380
well then why do you think the government would be able to vet people properly?

68
00:03:33.500 --> 00:03:37.250
Especially because we now see what's going on in Europe where they've vetted

69
00:03:37.251 --> 00:03:40.160
people very poorly and I've had very porous borders.

70
00:03:40.230 --> 00:03:42.930
<v 1>Yeah.
So this might,
yeah.
So I,
I dunno,</v>

71
00:03:42.960 --> 00:03:45.030
I think that will fear a skeptic of the government.

72
00:03:45.090 --> 00:03:48.330
So presumably you're a free trade advocate.
You like to get across borders.

73
00:03:48.540 --> 00:03:51.660
You wouldn't be happy with governments,

74
00:03:51.661 --> 00:03:55.710
micro managing trade across borders because you're a skeptic about how well the

75
00:03:55.711 --> 00:03:59.370
government works as am I.
And for the same sorts of reasons I am skeptical.

76
00:03:59.380 --> 00:04:01.740
So I think if you're worried about the government working well,

77
00:04:01.830 --> 00:04:06.570
that's all the more reason to get it out of the immigration regulation business.

78
00:04:06.690 --> 00:04:11.340
So it might,
I think I'd probably rights,
there's a bad job of a lot of stuff,

79
00:04:11.341 --> 00:04:15.570
almost everything.
Um,
and so the standard here is not going to be perfection,

80
00:04:15.571 --> 00:04:17.580
but just what's the best alternative?

81
00:04:17.581 --> 00:04:20.790
Heavily regulated immigration or lightly immigrated regulation.

82
00:04:21.000 --> 00:04:23.190
And I think all things equal,
lightly,

83
00:04:23.280 --> 00:04:25.980
lightly regulated immigration will probably be better.

84
00:04:26.420 --> 00:04:27.840
And let me give you another argument too.

85
00:04:27.960 --> 00:04:30.570
So this again is maybe a more libertarian,

86
00:04:30.571 --> 00:04:33.180
classical liberal argument for a immigration,

87
00:04:33.300 --> 00:04:38.220
but it just seems like people have a right to,
to move across borders.

88
00:04:38.221 --> 00:04:42.540
So,
for example,
if,
uh,
you know,
I dunno,
uh,

89
00:04:42.570 --> 00:04:47.100
this table for example,
we're divided by some kind of border and I said,

90
00:04:47.101 --> 00:04:47.760
well look,

91
00:04:47.760 --> 00:04:52.760
I want you to come over to my side and rent my house from me or work in my

92
00:04:52.921 --> 00:04:56.850
business or join my religious congregation.
And you said,
okay,

93
00:04:56.880 --> 00:05:00.720
I do want to do those things.
And we both agree on this.
We,
we agree on the terms.

94
00:05:00.930 --> 00:05:04.800
And then the state sort of intervenes is a third party in this.
Um,

95
00:05:04.980 --> 00:05:08.280
what this philosopher Robert knows it would call a capitalist act between

96
00:05:08.281 --> 00:05:09.150
consenting adults.

97
00:05:09.330 --> 00:05:14.100
It seems like they would be violating our rights to associate with one another

98
00:05:14.101 --> 00:05:15.240
on free terms.

99
00:05:15.241 --> 00:05:19.890
And so I think there's a reason to think that a border controls violate people's

100
00:05:19.980 --> 00:05:20.820
natural rights.

101
00:05:21.000 --> 00:05:23.490
<v 2>Is there also some inherent problem though that,</v>

102
00:05:24.030 --> 00:05:26.670
let's say most of the western societies,

103
00:05:26.671 --> 00:05:29.490
the freest societies got on board this?

104
00:05:30.150 --> 00:05:34.260
Well then they would end up being flooded with people where maybe the economics

105
00:05:34.261 --> 00:05:37.980
don't work out.
Because if we just say,
I mean if you think about it,

106
00:05:37.981 --> 00:05:41.170
like if tomorrow we were dressed like all right everybody can come and you just,

107
00:05:41.400 --> 00:05:45.060
as long as you're not a murderer,
you know drug dealer,
you're good to go.

108
00:05:45.360 --> 00:05:48.690
That there must be so mechanistic theory here where we're going to let in a

109
00:05:48.691 --> 00:05:50.730
certain amount of people that your economics are going to work out and then

110
00:05:50.731 --> 00:05:54.450
we're going to get to some odd tipping point where because freedom is pretty

111
00:05:54.451 --> 00:05:56.010
good,
people are going to want in on it.

112
00:05:56.180 --> 00:05:59.630
<v 1>Yeah.
So I,
there are a couple of things to say.
So one is just if we,</v>

113
00:05:59.631 --> 00:06:02.360
I think that problem to some extent would be self correcting.

114
00:06:02.510 --> 00:06:05.780
So if you think that a lot of the pressure to migrate is economics.

115
00:06:05.781 --> 00:06:08.570
So people are moving from low wage countries to higher wage countries,

116
00:06:08.840 --> 00:06:12.440
all as you have more and more people enter the high wage,
higher wage countries,

117
00:06:12.441 --> 00:06:14.480
this would increase the total number of workers,

118
00:06:14.481 --> 00:06:17.810
which would probably start dropping down their wages until you would probably

119
00:06:17.811 --> 00:06:21.590
see less motivation to move.
I think that's one thing,
but as a,

120
00:06:21.680 --> 00:06:25.310
as a concession to this worry,
what I would say is,
well,
we can do it slowly.

121
00:06:25.460 --> 00:06:26.300
So for example,

122
00:06:26.301 --> 00:06:31.160
if we're really worried about these are facts of dramatically increasing

123
00:06:31.190 --> 00:06:34.850
immigration overnight,
what we could do is just say increase,

124
00:06:34.880 --> 00:06:37.040
see the limits on immigration by,

125
00:06:37.320 --> 00:06:40.940
I don't know if 10% every year or something like that and take it slowly and see

126
00:06:40.941 --> 00:06:42.770
where it goes.
We wouldn't have to push the button,

127
00:06:42.800 --> 00:06:45.920
open the borders overnight if we have this worry.
Yeah.

128
00:06:46.040 --> 00:06:47.980
What do you think is philosophically the,

129
00:06:48.070 --> 00:06:52.640
the soundest way to deal with the people that are already here?
Uh,
I,
yeah,

130
00:06:52.641 --> 00:06:57.641
so I am an advocate of the view that you don't have an obligation to obey unjust

131
00:06:57.771 --> 00:06:58.550
laws.

132
00:06:58.550 --> 00:07:02.060
And so I think that our immigration laws are on just so I think people who came

133
00:07:02.061 --> 00:07:05.390
here,
uh,
even against current immigration law,

134
00:07:05.391 --> 00:07:09.580
I think they should just MSD full citizenship that,

135
00:07:09.610 --> 00:07:13.670
that would be my approach to that,
Huh?
Yeah.
All right.
We're not quite there.

136
00:07:13.680 --> 00:07:18.170
[inaudible] that's what it's all about.
So let me ask you this.
So your eye,

137
00:07:18.171 --> 00:07:20.810
it's a,
you're not a fan of the drug war?
No.
No.
Okay.

138
00:07:21.050 --> 00:07:23.640
So suppose we have somebody who,
uh,
I don't know,

139
00:07:23.690 --> 00:07:26.990
is in jail for selling marijuana or something like this.
Um,

140
00:07:27.410 --> 00:07:29.750
and then suppose that,
uh,
yeah,

141
00:07:29.751 --> 00:07:32.070
so they're breaking was a citizen of the United as a citizen,

142
00:07:32.410 --> 00:07:35.930
but who broke say current immigration earn not immigration current drug laws.

143
00:07:35.931 --> 00:07:40.730
Yeah.
Uh,
do you think that they violated some moral obligation that they had,
uh,

144
00:07:40.760 --> 00:07:43.610
not to break us drug law or do you say,

145
00:07:43.611 --> 00:07:46.220
well what good was it bad loss or they didn't do anything?
Well,

146
00:07:46.280 --> 00:07:48.420
partly I think it would depend on the specific offense.
So,

147
00:07:48.530 --> 00:07:50.720
so whether they were using drugs or selling drugs,

148
00:07:50.721 --> 00:07:53.090
I would see a distinction there.
So if there's someone that,

149
00:07:53.091 --> 00:07:57.200
and there are people that are in jail for using drugs right now for using pot,

150
00:07:57.740 --> 00:07:58.580
I think those,

151
00:07:58.610 --> 00:08:02.150
those laws are unjust and I would want to do everything I could to reverse those

152
00:08:02.151 --> 00:08:06.770
laws and reversed the,
the,
uh,
prison system,
the justice system and all that.

153
00:08:06.771 --> 00:08:11.210
As far as the dealers,
yeah.
You can't until,

154
00:08:11.211 --> 00:08:12.590
until we fix these laws.

155
00:08:12.591 --> 00:08:15.860
And figure out what actually is fair and changed some classifications on

156
00:08:15.861 --> 00:08:19.700
marijuana and a bunch of other things.
It's tough.
I think it's tough.

157
00:08:19.701 --> 00:08:23.540
I don't think you can free drug dealers per se,
even though you're,
again,

158
00:08:23.570 --> 00:08:25.760
you don't think they did something that should be against the law.

159
00:08:25.910 --> 00:08:27.560
The people that are doing the drugs.

160
00:08:27.561 --> 00:08:31.640
I think you have a right to do with your body as you see fit.
But again,

161
00:08:31.641 --> 00:08:33.890
but again,
if you break a law you have to like,

162
00:08:33.891 --> 00:08:35.780
I would change the laws is what I'm saying.

163
00:08:35.930 --> 00:08:40.520
But if you break a law then you have to suffer the consequences of that.

164
00:08:40.730 --> 00:08:43.580
So does that do that all again?
Well,
the,
I'm curious,
I don't know.

165
00:08:43.581 --> 00:08:45.440
Forgive me for that.
No,
no,
no.
On this.
But,

166
00:08:45.441 --> 00:08:48.710
so suppose you have a drug user who's convicted of this offense and they're,

167
00:08:48.720 --> 00:08:53.630
they're in jail,
but suppose they can escape from prison.
Harmlessly um,

168
00:08:54.020 --> 00:08:56.760
do you think they would be doing something morally wrong by escaping from
prison?

169
00:08:57.280 --> 00:08:58.730
All right,
so this is a good little,
all right.

170
00:08:58.750 --> 00:09:02.940
We're doing a little [inaudible] always my students.
No,
this is great.
I mean,

171
00:09:02.941 --> 00:09:04.110
this is what it's about,
right?
So,
okay,

172
00:09:04.111 --> 00:09:08.400
so let's say someone was smoking weed at home.
Cops bust in there,

173
00:09:08.401 --> 00:09:11.560
thrown in jail for three years,
year or two,

174
00:09:11.880 --> 00:09:14.490
they're going to pull the Shawshank and get out of there.
Right?

175
00:09:14.940 --> 00:09:16.890
Do I think they're morally,
the question is,

176
00:09:16.891 --> 00:09:19.980
do I think they're morally right for doing it or can that be morally justified?

177
00:09:20.100 --> 00:09:21.690
That's the question,
right?

178
00:09:21.691 --> 00:09:26.340
So would we morally criticize them if they pull the Shawshank where they escaped

179
00:09:26.341 --> 00:09:29.310
to the wall or something like that and they don't harm anyone.
They just,

180
00:09:29.370 --> 00:09:30.870
they just leave because they say,

181
00:09:31.170 --> 00:09:33.690
I was convicted of something that ought not be a crime.

182
00:09:33.810 --> 00:09:37.470
So I don't owe any morally moral allegiance to that law.

183
00:09:37.471 --> 00:09:40.500
And so I'm just going to escape.
But you hear about this story,
do you say,

184
00:09:40.530 --> 00:09:43.340
I'd be rooting for that guy.
I wouldn't be rooting for that.

185
00:09:43.341 --> 00:09:45.720
So that's kind of my perspective on immigration,

186
00:09:45.721 --> 00:09:48.180
that I think that it's an unjust law.

187
00:09:48.181 --> 00:09:51.450
And so p justice people aren't under an obligation to obey the state with

188
00:09:51.451 --> 00:09:52.770
respect to the drug war.
Yeah.

189
00:09:52.771 --> 00:09:55.500
I don't think people are under an obligation to obey the state with respect to

190
00:09:55.501 --> 00:09:56.161
immigration.
Right.

191
00:09:56.161 --> 00:10:00.120
So I guess our sort of disconnected on this is just the unjustice of immigration

192
00:10:00.121 --> 00:10:02.790
laws versus drug laws.
Right?
Right.

193
00:10:02.791 --> 00:10:05.100
So if you think that the law itself is not unjust,

194
00:10:05.130 --> 00:10:08.860
then my argument won't have any kind of grip on you.
Yeah,
yeah.

195
00:10:09.120 --> 00:10:11.700
I'm not saying our immigration laws are just,
by the way,

196
00:10:12.090 --> 00:10:14.400
I just think that especially now,
uh,

197
00:10:14.430 --> 00:10:18.480
also just because of terrorism and people move and just when you see what's

198
00:10:18.481 --> 00:10:23.280
going on with Europe and,
and cause it hasn't just been,
uh,

199
00:10:23.580 --> 00:10:26.210
people trying to get over for humanitarian purposes there.

200
00:10:26.310 --> 00:10:29.580
We know that there's a lot of migrants and all sorts of other people and then

201
00:10:29.581 --> 00:10:32.550
assimilation problems and then welfare state problems.

202
00:10:32.551 --> 00:10:36.210
Like it just seems like it's become this massive thing that most of Europe would

203
00:10:36.211 --> 00:10:40.530
probably do it very differently if they could look back eight years ago.
Right.

204
00:10:40.860 --> 00:10:43.110
Yeah.
So,
as far as the terrorism point goes,
I mean,

205
00:10:43.111 --> 00:10:46.220
I think if somebody is a one to a terrorist or a suspected terrorist,

206
00:10:46.221 --> 00:10:49.910
that's a perfectly legitimate reason not to,
not to allow them,
man,
I'm,

207
00:10:49.940 --> 00:10:53.730
I'm less concerned about the assimilation concern,

208
00:10:53.731 --> 00:10:57.540
the welfare state concern.
So as,
as far as the,
the assimilation where it goes,

209
00:10:57.541 --> 00:11:02.310
I mean,
a lot of the evidence I've seen a suggest that the political values of

210
00:11:02.311 --> 00:11:04.860
immigrants and especially second generation American immigrants,

211
00:11:05.010 --> 00:11:08.240
it's almost identical to the political,
uh,

212
00:11:08.340 --> 00:11:13.340
political values held by a native born Americans.
Um,
and as far as the walls,

213
00:11:13.380 --> 00:11:15.810
welfare state concern goes,
um,

214
00:11:15.870 --> 00:11:20.250
I would say we have lots of other freedoms that can potentially increase the

215
00:11:20.251 --> 00:11:23.580
cost of the welfare state.
And we're okay with people exercising them.

216
00:11:23.581 --> 00:11:26.070
So for example,
uh,
you know,
I dunno,

217
00:11:26.310 --> 00:11:29.610
we allow people to choose don't own profession,
for example,
in the United States.

218
00:11:29.790 --> 00:11:31.680
And so suppose you have a student who says,

219
00:11:32.020 --> 00:11:35.210
here's what I want to do with my life.
I want to become a philosopher.
He said,

220
00:11:35.211 --> 00:11:38.310
well,
that's,
you know,
that's a dicey career choice.
Not a lot of money in it.

221
00:11:38.610 --> 00:11:39.031
Who knows,

222
00:11:39.031 --> 00:11:42.960
maybe you'll end up on unemployment and this will in fact increase the cost of

223
00:11:42.961 --> 00:11:45.690
the welfare state because now you have this person on unemployment because they

224
00:11:45.691 --> 00:11:50.480
chose to become a philosopher with our life.
Um,
we say,
well,
uh,
we,

225
00:11:50.481 --> 00:11:51.720
we let them do that.

226
00:11:51.720 --> 00:11:54.910
We let them exercise that freedom to choose their occupation,

227
00:11:55.150 --> 00:11:58.990
even knowing that this might increase the costs of the welfare state.

228
00:11:59.230 --> 00:12:01.630
And so I think you can make an analogous argument about immigration.

229
00:12:01.660 --> 00:12:05.080
If you think it's a right,
an important human freedom,
then you say,
well,

230
00:12:05.081 --> 00:12:08.320
this is sort of the price of respecting rights and respecting freedom.

231
00:12:08.321 --> 00:12:10.900
They sometimes can impose costs on third parties.

232
00:12:11.230 --> 00:12:12.940
So if all these people come here,
then what,

233
00:12:13.000 --> 00:12:18.000
what duty does the state have to make sure that they don't just get here and

234
00:12:18.071 --> 00:12:22.600
can't do anything and then then more crime comes and drugs and the rest of it.

235
00:12:22.780 --> 00:12:25.150
Yeah,
their responsibility for the state at that point,

236
00:12:25.180 --> 00:12:28.240
I'm not sure that the responsibility of the state to immigrants is different

237
00:12:28.241 --> 00:12:32.350
from their responsibility to citizens.
So I think part,
so,
uh,

238
00:12:32.710 --> 00:12:36.010
I mean there's also a question of what we will do in the ideal world versus the

239
00:12:36.011 --> 00:12:39.460
real world.
So we say in the ideal world of somebody like Milton Friedman,

240
00:12:39.461 --> 00:12:42.550
the welfare state is much smaller and maybe you have something like the negative

241
00:12:42.551 --> 00:12:44.830
income tax.
This isn't the world we live in.

242
00:12:44.831 --> 00:12:49.710
We have over $1 trillion of redistributive spending every year.
I mean,
I don't.

243
00:12:49.711 --> 00:12:51.550
So,
so there are two separate issues.

244
00:12:51.551 --> 00:12:56.230
So one is this empirical claim about whether immigrants tend to consume more in

245
00:12:56.231 --> 00:12:58.330
government services than they pay in taxes.

246
00:12:58.570 --> 00:13:01.870
And it seems like the fiscal effect is pretty moderate.

247
00:13:01.871 --> 00:13:06.340
So some estimates say they do in fact raise fiscal costs a little bit.

248
00:13:06.460 --> 00:13:10.210
Others say no.
In fact,
actually there in that benefit,
because a lot of,
uh,

249
00:13:10.330 --> 00:13:14.830
a lot of new immigrants aren't consuming a lot of government services and so

250
00:13:14.831 --> 00:13:18.850
forth and they're paying sales tax and things like this.
Um,
but I think that,

251
00:13:19.060 --> 00:13:22.320
again,
this is like a concession.
If you really have this worry about the welfare,

252
00:13:22.321 --> 00:13:26.600
say we could say,
well maybe this is not my view,
but if,
if,

253
00:13:26.620 --> 00:13:29.510
if this is what it took for me to get you over to my side,
I'll,

254
00:13:29.511 --> 00:13:32.590
I'll give you this,
I'll give you this concession.
We could say something like,

255
00:13:32.770 --> 00:13:37.770
you have a five year waiting period before you have access to unemployment

256
00:13:38.411 --> 00:13:39.880
benefits and whatnot.

257
00:13:40.120 --> 00:13:43.930
I would take that over border closure if those were the only two options on the

258
00:13:43.931 --> 00:13:46.510
table.
It's tough being intellectually honest,
isn't it?
Cause you have to,

259
00:13:46.570 --> 00:13:48.790
you have to concede things every now and again,

260
00:13:48.791 --> 00:13:53.140
which you just don't see people doing anymore.
So it seems like doubly,

261
00:13:53.170 --> 00:13:56.740
that's just,
it's just a ploy to get you and your viewers to the Lord on my side.

262
00:13:56.741 --> 00:13:58.720
So yeah,
I don't care so much about the honesty.

263
00:13:58.721 --> 00:14:00.610
Just get people on the open borders side.
All right,

264
00:14:00.611 --> 00:14:05.080
so you've argued that inequality of income is in the real issue,

265
00:14:05.110 --> 00:14:09.910
but poverty is the real issue.
Right.
Let's get into the weeds on that.
Okay,
good.

266
00:14:09.940 --> 00:14:12.700
So sometimes you'll hear,
this is sometimes philosophers,

267
00:14:12.701 --> 00:14:14.200
but oftentimes politicians,

268
00:14:14.440 --> 00:14:18.400
they'll make statements like 2% of,
uh,

269
00:14:18.401 --> 00:14:22.510
American earners have 40% of the country's wealth or something like that.

270
00:14:22.870 --> 00:14:27.490
And from here we're supposed to infer that some injustice has occurred because

271
00:14:27.491 --> 00:14:31.150
there's this,
this large inequality.
And I think,
uh,

272
00:14:31.240 --> 00:14:33.580
I mean there might be an injustice there,
uh,

273
00:14:33.581 --> 00:14:36.700
but we can't tell a strictly on the basis of the inequality.

274
00:14:36.850 --> 00:14:39.910
So there's a philosopher named Robert Nozick who famously said,

275
00:14:40.060 --> 00:14:43.630
what manners isn't so much the income distribution that we end up with,

276
00:14:43.720 --> 00:14:44.920
but how we got there.

277
00:14:45.130 --> 00:14:50.130
So if somebody has a huge amount of wealth but they got it through theft or they

278
00:14:50.291 --> 00:14:54.260
got it through lobbying the government for special privileges,
this is very bad.

279
00:14:54.261 --> 00:14:56.510
This is something that we shouldn't encourage.
On the other hand,

280
00:14:56.511 --> 00:15:00.770
if we have somebody who has a lot of wealth because they invented the iPhone and

281
00:15:00.800 --> 00:15:03.980
there are tens,
if not hundreds of millions of people who want to buy this,

282
00:15:04.100 --> 00:15:05.180
that's totally fine.

283
00:15:05.360 --> 00:15:08.780
So we can't just look at the pie and how it's carved up and say whether it's

284
00:15:08.781 --> 00:15:10.790
just or unjust.
We have to see,

285
00:15:10.910 --> 00:15:15.050
did people make their money by making other people better off by giving them

286
00:15:15.051 --> 00:15:18.470
things that they wanted or did they take it through fraudulent means?

287
00:15:18.471 --> 00:15:19.460
Coercive means?

288
00:15:19.850 --> 00:15:24.850
And I think that the talk about inequality oftentimes confuses a quality with

289
00:15:25.160 --> 00:15:25.970
poverty.

290
00:15:25.970 --> 00:15:30.410
So it's what we're really concerned with equalizing income or are we concerned

291
00:15:30.411 --> 00:15:32.120
about making the poor are better off.

292
00:15:32.150 --> 00:15:34.340
And I think it's the latter that we really should care about.

293
00:15:34.610 --> 00:15:37.820
So how do we go ahead and do that?
Well,

294
00:15:37.821 --> 00:15:41.120
so opening borders I think is the first step in terms of alleviating global

295
00:15:41.121 --> 00:15:44.600
inequality or,
I'm sorry,
global poverty I think is the best thing that we can do.

296
00:15:44.960 --> 00:15:49.960
I think domestically there are a lot of government policies that really do harm

297
00:15:50.350 --> 00:15:52.580
to the most disadvantaged groups.

298
00:15:52.730 --> 00:15:56.570
So I think opening up school choice would be a great idea.

299
00:15:56.750 --> 00:15:58.790
I think I knew the drug war would be a great idea.

300
00:15:59.030 --> 00:16:03.140
I think ending occupational licensing would be a very good idea.

301
00:16:03.141 --> 00:16:08.000
So enabling people to work in certain industries or start their own business,
uh,

302
00:16:08.001 --> 00:16:09.800
with a lot less red tape is a very good idea.

303
00:16:09.801 --> 00:16:13.680
And would in barber's be stabbing people with scissors?
Well,
this is what I say.

304
00:16:13.690 --> 00:16:15.360
There are some states where you have to,
uh,

305
00:16:15.400 --> 00:16:19.100
you don't have to get a license to shampoo people.
But I shampooed my,
I'm,
I,

306
00:16:19.170 --> 00:16:23.340
I'm going to sample my hair every morning and doing this and I know it's,

307
00:16:23.410 --> 00:16:28.150
and I probably shouldn't say that on this.
Yeah,
do I do,
I showed myself like,

308
00:16:28.210 --> 00:16:29.900
you know,
call my hair without a license.

309
00:16:29.901 --> 00:16:32.960
And so far the government hasn't come after me and I haven't,
you know,

310
00:16:32.961 --> 00:16:36.170
poked my eye out.
So what's your,
what's your best argument then when,

311
00:16:36.200 --> 00:16:38.150
when the people that don't buy into this and say,
well,

312
00:16:38.151 --> 00:16:41.960
we need these regulations because you're going to have people that don't know

313
00:16:41.961 --> 00:16:46.450
how to dye hair are gonna be scalding people's heads and you know,
uh,

314
00:16:46.490 --> 00:16:49.760
Mr. Burns is going to be dumping nuclear waste into the Springfield river and

315
00:16:49.761 --> 00:16:51.780
all of those things.
What's the best argument against it?

316
00:16:51.810 --> 00:16:54.980
So while argument is just that,
I don't think regulation works very well,

317
00:16:54.981 --> 00:16:55.980
and I don't think so.

318
00:16:56.120 --> 00:16:59.780
I think the way that the public views regulation is probably my sky did.

319
00:16:59.990 --> 00:17:02.840
So I don't think the regulators are bad people.

320
00:17:02.960 --> 00:17:07.910
But I think if you examine the ways that regulations are formed and operate in

321
00:17:07.911 --> 00:17:10.060
the real world,
it's oftentimes to protect the,

322
00:17:10.061 --> 00:17:12.620
in the interest of the industries that they're regulating.

323
00:17:12.830 --> 00:17:14.840
So this is what's known as regulatory capture,

324
00:17:15.050 --> 00:17:18.350
where regulators aren't always looking out for the public interest or actually

325
00:17:18.560 --> 00:17:21.770
looking out for the interest of people in that industry.

326
00:17:21.860 --> 00:17:24.140
So financial regulation is a case of this.

327
00:17:24.320 --> 00:17:27.680
You have lobbyists who might be friends with people in the financial sector.

328
00:17:27.800 --> 00:17:30.740
They want to work in the financial sector later or vice versa.

329
00:17:30.980 --> 00:17:34.040
And so they're often very friendly to the very people that they're regulating.

330
00:17:34.460 --> 00:17:37.190
Um,
and as far as,
you know,
the,
the worry about,
you know,

331
00:17:37.191 --> 00:17:39.590
scolding people and so forth.
Um,

332
00:17:40.040 --> 00:17:43.760
I think that competition itself is a kind of regulation.
So in the case of,

333
00:17:43.820 --> 00:17:48.140
of school choice,
it's true.
You might have very bad charter schools,

334
00:17:48.141 --> 00:17:51.570
for example.
But again,
the standard here is not perfection.

335
00:17:51.571 --> 00:17:53.100
The standard is what's the alternative.

336
00:17:53.190 --> 00:17:55.020
So we have public schools that are terrible,

337
00:17:55.380 --> 00:17:59.970
but charter schools have this advantage of enabling people to choose.

338
00:18:00.030 --> 00:18:02.190
So if you go to a charter school that's failing,

339
00:18:02.220 --> 00:18:04.530
you can pull your kids out and put them somewhere else.

340
00:18:04.680 --> 00:18:08.730
And that's a kind of regulation in the sense that it provides incentives to the

341
00:18:08.731 --> 00:18:11.250
providers to supply good service too.

342
00:18:11.251 --> 00:18:16.251
It's almost like we should rely on self regulation or not regulation,

343
00:18:17.040 --> 00:18:20.550
not even self regulation in a sense.
So this is something that comes up when I,

344
00:18:20.580 --> 00:18:25.050
when I argue with people is they think that I have this view of private

345
00:18:25.051 --> 00:18:26.820
business,
uh,
as,

346
00:18:26.850 --> 00:18:30.210
as being sort of benevolent where they'll just take care of themselves.

347
00:18:30.210 --> 00:18:33.240
I have a sense of the public interest.
I think maybe to some extent that's true,

348
00:18:33.420 --> 00:18:37.890
but it's more the kind of Smithian insight where if you have providers who are

349
00:18:37.891 --> 00:18:39.390
competing for my business,

350
00:18:39.420 --> 00:18:43.290
they don't have to care about my welfare directly or the welfare of my family,

351
00:18:43.410 --> 00:18:44.640
but they might just want my money.

352
00:18:44.850 --> 00:18:48.600
And that itself is an incentive for them to provide good,
safe service.

353
00:18:48.601 --> 00:18:51.390
So in a sense it's competition for dollars,

354
00:18:51.391 --> 00:18:54.100
competition for customers that acts as the regulation.

355
00:18:54.101 --> 00:18:58.570
So I think the market as a whole is self regulating herself.
Correcting.
Yeah.

356
00:18:58.620 --> 00:19:02.450
So you're also a universal basic income supporter.
Yeah.

357
00:19:02.520 --> 00:19:07.520
Now usually this is a idea that comes from economists on the left.

358
00:19:08.650 --> 00:19:12.720
Uh,
I've heard some interesting sort of libertarian arguments from it,
uh,

359
00:19:12.750 --> 00:19:14.730
from people.
But what is your take on,

360
00:19:15.360 --> 00:19:18.540
so this goes back to the idea that it is important to take care of people who

361
00:19:18.541 --> 00:19:22.040
are in poverty or who might have fallen on hard lock and uh,

362
00:19:22.050 --> 00:19:24.600
it needs some help getting back on their feet.
I,
I,
my,

363
00:19:24.620 --> 00:19:27.300
my short pitch for the universal basic income would just be,

364
00:19:27.450 --> 00:19:30.670
suppose you could take,
I forget what the number is,
but you know,

365
00:19:30.720 --> 00:19:34.980
over $1 trillion of redistributive spending that the United States government

366
00:19:34.981 --> 00:19:39.390
does,
uh,
social security,
Medicare,
medicaid,
unemployment,

367
00:19:39.420 --> 00:19:41.640
all of these things.
Suppose we said,
okay,

368
00:19:41.641 --> 00:19:46.641
you can remove all that and just get some kind of cash payout if you fall below

369
00:19:47.761 --> 00:19:51.450
a certain level of poverty.
So this is kind of the Milton Friedman idea,
uh,

370
00:19:51.600 --> 00:19:54.720
where you get sort of more and more money the further below the poverty line you

371
00:19:54.721 --> 00:19:54.930
are.

372
00:19:54.930 --> 00:19:58.590
So you can take the whole apparatus of the welfare state with its bureaucracy

373
00:19:58.591 --> 00:20:01.860
and inefficiency and just replace that with a universal basic income.

374
00:20:01.890 --> 00:20:05.030
I make that trade every day of the week.
Yeah.
Do,

375
00:20:05.070 --> 00:20:08.510
do you think that that would disincentivize people to actually,

376
00:20:08.520 --> 00:20:11.040
the people that are right above that marker?
I mean,

377
00:20:11.041 --> 00:20:13.590
do you know where you'd put that market?
So I don't know.

378
00:20:13.591 --> 00:20:16.890
I don't have the specifics of where I would put it by the,
the idea is that we,

379
00:20:16.980 --> 00:20:18.660
uh,
so the way that,
uh,
uh,

380
00:20:18.710 --> 00:20:23.710
Friedman structured the idea of the negative income tax is that you would still

381
00:20:23.791 --> 00:20:26.970
get more total income by working and making more money.

382
00:20:26.971 --> 00:20:30.180
So you would see some decrease in the cash grants that you get from the

383
00:20:30.181 --> 00:20:33.780
government.
Um,
but it wouldn't be so massive as to disincentivize.

384
00:20:33.781 --> 00:20:35.100
So they would be trend,

385
00:20:35.340 --> 00:20:39.090
but they wouldn't be trimmed so much that you have no incentive to go to work.

386
00:20:39.100 --> 00:20:39.540
Yeah.

387
00:20:39.540 --> 00:20:43.320
Do you see that as just like some sort of inconsistency with the way that you

388
00:20:43.321 --> 00:20:47.200
generally view government?
Cause it's,
you're obviously not a big government guy.

389
00:20:47.230 --> 00:20:48.010
Right.
And yet this

390
00:20:48.010 --> 00:20:49.900
<v 2>is something that's really like,</v>

391
00:20:49.901 --> 00:20:54.040
I would view this as something that this is like government should have no place

392
00:20:54.041 --> 00:20:57.400
in giving that much to people.
Like it's kind of shitty.

393
00:20:57.401 --> 00:21:01.990
Like I'd love to figure out better ways to help people who need it the most.
Um,

394
00:21:02.020 --> 00:21:05.380
but the idea of giving more,
I just don't know that there's evidence that

395
00:21:05.460 --> 00:21:10.320
<v 1>works.
Yeah.
So there,
so I think though that if that objection I successful,</v>

396
00:21:10.321 --> 00:21:13.470
it would also be successful about against something like private charity for

397
00:21:13.471 --> 00:21:17.190
example.
So I think there always is this worry that,
um,

398
00:21:17.310 --> 00:21:21.810
when you give some kind of assistance that this will have negative effects on

399
00:21:21.811 --> 00:21:23.730
people's incentives to work.
I think that's,

400
00:21:23.790 --> 00:21:25.590
I think that's probably true to some extent,

401
00:21:25.890 --> 00:21:28.920
but that might just be a cost that we have to live with.
So I think,
again,

402
00:21:28.950 --> 00:21:31.690
even if you're doing some sort of private charity,
uh,

403
00:21:31.710 --> 00:21:34.500
that might lessen at the margin,
people's willingness to go to work.

404
00:21:34.501 --> 00:21:38.010
But I think that trade off is worth it if it means say that,
uh,

405
00:21:38.220 --> 00:21:43.220
we have people who aren't starving who aren't a very sick without help.

406
00:21:44.160 --> 00:21:47.730
But I do,
I,
so again,
a concession that I will make,
cause I think by and large,

407
00:21:47.731 --> 00:21:52.500
private charity will work much better than sort of state,
bureaucratic charity,

408
00:21:52.880 --> 00:21:55.080
um,
or,
or transfer program

409
00:21:55.680 --> 00:21:58.990
<v 2>some philosophical level.
Doesn't that also make people feel better?</v>

410
00:21:59.290 --> 00:22:02.500
Like the idea that like right now the government just does things in it either

411
00:22:02.501 --> 00:22:05.200
does them inefficiently or not and you don't really know where your money's

412
00:22:05.201 --> 00:22:08.560
going and you kind of just are like,
you can just easily be like,
yeah,

413
00:22:08.561 --> 00:22:11.250
I'm for poor people because the government's doing it right.

414
00:22:11.410 --> 00:22:15.580
Where when you actually go ahead and do things and go ahead and volunteer and go

415
00:22:15.581 --> 00:22:17.740
into head and give charity and all that,

416
00:22:17.741 --> 00:22:22.030
that just philosophically for your own goodness and happiness and things that

417
00:22:22.031 --> 00:22:24.520
we've talked about that that's probably much more rewarding.

418
00:22:24.760 --> 00:22:26.770
<v 1>I think so.
Uh,
right.
It's very,</v>

419
00:22:26.771 --> 00:22:31.480
it's very easy and low cost to just cast a vote for some policy or politician

420
00:22:31.481 --> 00:22:33.940
that happens to align with your values.
That doesn't really,
you know,

421
00:22:33.941 --> 00:22:38.200
the price of that is very low but right.
Actually say doing some research,

422
00:22:38.440 --> 00:22:42.490
figuring out what causes you support,
putting money into those things.
Right.

423
00:22:42.491 --> 00:22:43.780
I think that's much more gratifying.

424
00:22:43.781 --> 00:22:47.050
I think you should get more and moral credit for doing that than just voting for

425
00:22:47.051 --> 00:22:49.110
policies that you like to is that sort of thing.

426
00:22:49.150 --> 00:22:51.940
<v 2>Basic disconnect between Democrats and Republicans in this point.</v>

427
00:22:51.941 --> 00:22:55.270
Like I view it as sort of Democrats are kind of like we'll just do everything

428
00:22:55.271 --> 00:22:57.640
for her.
We're going to say all the nice things and do it for you.

429
00:22:57.970 --> 00:23:00.170
And Republicans are just like,
we're not we,

430
00:23:00.280 --> 00:23:02.960
well they ended up doing it anyway because once they were in power,

431
00:23:02.961 --> 00:23:04.570
they're always spending the money anyway.

432
00:23:04.571 --> 00:23:07.780
But they seem like the evil guys cause they don't want to do it and then they do

433
00:23:07.781 --> 00:23:10.030
it anyway.
But that's what it really comes down to.

434
00:23:10.031 --> 00:23:12.700
It's like one set of people who are saying we're going to do all the nice things

435
00:23:13.060 --> 00:23:15.700
and then doing them even when the results are in Nice and another set,

436
00:23:15.701 --> 00:23:18.030
who doesn't it don't say they want to do the nice things.
Right.

437
00:23:18.180 --> 00:23:20.830
<v 1>They end up doing it and the results still aren't great.
Yeah.
Well so,</v>

438
00:23:20.831 --> 00:23:23.450
so I'm not sure,
although I do remember this book,

439
00:23:23.451 --> 00:23:27.560
I was probably a little old by now called who really cares by Arthur Brooks.

440
00:23:27.730 --> 00:23:29.480
Have you ever come across though?
So He's,

441
00:23:29.550 --> 00:23:32.450
I think now he's the president of the a right.

442
00:23:32.870 --> 00:23:36.500
And what he did find that there were differences in charitable giving between

443
00:23:36.501 --> 00:23:38.110
Democrats and Republicans are,

444
00:23:38.270 --> 00:23:41.690
I'm not sure if it was along party lines or ideological lines more broadly.

445
00:23:42.080 --> 00:23:44.540
And he did find evidence that,
uh,

446
00:23:44.600 --> 00:23:49.060
conservatives to give more privately than Democrats.
Uh,

447
00:23:49.340 --> 00:23:51.140
there might be a variety of explanations for that.

448
00:23:51.141 --> 00:23:54.950
So one explanation might just be that conservatives tend to be more skeptical of

449
00:23:54.980 --> 00:23:58.190
this,
the efficacy of these large scale bureaucracies.

450
00:23:58.191 --> 00:24:01.310
Like how well do they actually work.
I think there's maybe also this,

451
00:24:01.311 --> 00:24:06.050
this idea that it's the role of something like the family or the church or civil

452
00:24:06.051 --> 00:24:09.530
society to take care of people who are in poverty rather than the states.

453
00:24:09.530 --> 00:24:13.340
And so maybe that's why you see conservatives giving more charitable dollars

454
00:24:13.341 --> 00:24:17.870
than,
um,
people on the left.
Yeah.
All right.
So I want to finish up with a couple,

455
00:24:17.930 --> 00:24:22.070
uh,
scientific experiments and you've,
you've written about,
oh wow.

456
00:24:22.071 --> 00:24:23.250
I got a couple here.
All right.

457
00:24:23.330 --> 00:24:26.660
Let's see how sharp you are and the previous work you've done.
All right.

458
00:24:26.661 --> 00:24:31.550
Talk to me about the Stanford prison experiment.
Okay.
Yeah.
So,
oh man,
that's,

459
00:24:31.551 --> 00:24:34.310
yeah,
that's gone back yet.
Right?
So this was,
uh,

460
00:24:34.610 --> 00:24:37.730
this was like the research around here.
I don't just sit down with people and,

461
00:24:37.731 --> 00:24:40.400
you know,
I think you know,
my work better than that at this point.

462
00:24:40.550 --> 00:24:43.400
So that was an experiment that was conducted,

463
00:24:43.430 --> 00:24:47.060
I want to say in the early seventies by a Stanford psychologist.

464
00:24:47.360 --> 00:24:49.140
And he recruited,
uh,

465
00:24:49.220 --> 00:24:53.830
more or less ordinary young men to serve in this experiment.
And,
uh,

466
00:24:53.870 --> 00:24:58.160
some of the subjects were cast as prisoners in this fake prison.

467
00:24:58.460 --> 00:25:02.810
And other,
uh,
subjects were cast as the prison guards in this experiment.

468
00:25:02.811 --> 00:25:07.160
And so they were enforcing the prison rules and feeding the prisoners and,
uh,

469
00:25:07.190 --> 00:25:09.740
having them go to sleep at specified times and so forth.

470
00:25:10.040 --> 00:25:11.270
And to make a long story short,

471
00:25:11.271 --> 00:25:16.271
what he found was that the power given to the students who are the prison guards

472
00:25:18.170 --> 00:25:21.740
did have this corrupt in effect to this idea that power corrupts and absolute

473
00:25:21.741 --> 00:25:26.200
power corrupts absolutely,
um,
seem to be supported by his experiments.

474
00:25:26.201 --> 00:25:28.430
So very quickly,
I think in a matter of days,

475
00:25:28.431 --> 00:25:31.310
what you found were the guards who were again,

476
00:25:31.311 --> 00:25:35.180
were drawn from the same demographic pool is the,
as the prisoners were abusing,

477
00:25:35.181 --> 00:25:38.210
the prisoners were mocking them.
Uh,

478
00:25:38.240 --> 00:25:41.570
and he had to cut it short because essentially the wheels came off of this

479
00:25:41.571 --> 00:25:45.890
experiment and the prisoners got so abuse.
The prison guards got so abusive.

480
00:25:46.280 --> 00:25:49.290
And so this I think,
confirms this idea that,
uh,

481
00:25:49.370 --> 00:25:54.370
you don't have to be a bad person to allow power to have this corrupting effect

482
00:25:54.531 --> 00:25:57.890
on their behavior.
Yeah.
There must be plenty of others studies proving that.

483
00:25:57.891 --> 00:25:59.670
Right.
Even through just the prison system.
I mean,

484
00:25:59.671 --> 00:26:01.790
when I feel like every prison movie I've ever arrived,

485
00:26:01.791 --> 00:26:02.750
or orange is the new black,

486
00:26:02.751 --> 00:26:05.660
I mean the people that are in charge of the prisons usually are pretty bad

487
00:26:05.661 --> 00:26:09.050
people.
Yeah.
Yeah.
And,
and like I said,
they could be perfectly ordinary.

488
00:26:09.200 --> 00:26:13.010
And there's also the,
the Milgram experiments were,
which are kind of the,
uh,

489
00:26:13.011 --> 00:26:17.480
the classic case of this where you just have people who are in essence willing

490
00:26:17.481 --> 00:26:18.560
to torture,

491
00:26:18.590 --> 00:26:22.520
give electric shocks to complete strangers because somebody in authority told

492
00:26:22.521 --> 00:26:23.420
them to do so.

493
00:26:23.720 --> 00:26:28.610
And the subjects figure out ways to rationalize why these electric shocks are

494
00:26:28.611 --> 00:26:29.690
justified.
You know,

495
00:26:29.691 --> 00:26:33.200
the person should have behaved differently and they wouldn't have gotten the

496
00:26:33.201 --> 00:26:34.034
shocks.

497
00:26:34.100 --> 00:26:37.430
And what's really frightened about this is it does show that ordinary people,

498
00:26:37.431 --> 00:26:38.900
people like,
uh,
you know,

499
00:26:38.901 --> 00:26:42.740
you and I could easily see our behavior corrupted given the right sort of

500
00:26:42.750 --> 00:26:46.850
institutions.
Yeah.
What do you do as,
as a human knowing that,
what do you do to,

501
00:26:46.870 --> 00:26:48.630
to insulate yourself?
Yeah.
Well,

502
00:26:48.631 --> 00:26:53.220
so one thing I think is to be very skeptical of these sorts of institutions,

503
00:26:53.221 --> 00:26:55.530
which give people lots of power over others.

504
00:26:55.531 --> 00:26:57.770
I think that's one of the big picture,
um,

505
00:26:57.930 --> 00:27:01.630
lessons from these sorts of experiments.
The experiments is just be wary of,

506
00:27:01.660 --> 00:27:04.320
of giving people power on an individual level.

507
00:27:04.500 --> 00:27:06.570
Knowing about it I think helps a little bit,

508
00:27:06.780 --> 00:27:10.680
but even more than just knowing about it,
I think,
um,
practicing it.

509
00:27:10.681 --> 00:27:15.150
So when you'll know that you're in a position say to do something wrong because

510
00:27:15.151 --> 00:27:19.650
you feel the social pressure to do it,
uh,
on a small level,
um,

511
00:27:19.680 --> 00:27:22.230
just try to resist that impulse and actually do the right thing.

512
00:27:22.230 --> 00:27:25.500
So I don't think it's the sort of thing that you can condition yourself to do

513
00:27:25.501 --> 00:27:29.280
overnight to resist my level into authority or resist becoming a malevolent

514
00:27:29.281 --> 00:27:31.950
authority.
But you can take small steps,

515
00:27:32.140 --> 00:27:34.950
but kind of it's like practicing an athletic skill or something like that.

516
00:27:34.951 --> 00:27:37.350
You don't try Dung on the basketball the first time you pick it up,

517
00:27:37.560 --> 00:27:40.860
but you take these small incremental steps and you'll get better and better.

518
00:27:41.340 --> 00:27:43.590
Hopefully you build character traits,
um,

519
00:27:43.620 --> 00:27:47.010
and then maybe resistant when the situation becomes more dire.

520
00:27:47.190 --> 00:27:50.400
It's interesting because that sort of reminds me of the way that we see virtue

521
00:27:50.401 --> 00:27:52.590
signaling happening on social media these days.

522
00:27:52.591 --> 00:27:55.800
It's not that we're all deciding we have this power and we can shock somebody,

523
00:27:56.040 --> 00:27:59.950
but it's like you find someone halfway across the world who says something you

524
00:27:59.951 --> 00:28:03.030
slightly disagree with,
even though you've never heard of them before.

525
00:28:03.240 --> 00:28:06.450
You start seeing the mob go with them and the next thing you know,

526
00:28:06.451 --> 00:28:09.420
you ratchet up more and then someone else ratcheted up more aware to that word,

527
00:28:09.540 --> 00:28:12.240
and it's all done in the name of being good,
actually,

528
00:28:12.241 --> 00:28:15.090
the way you're trying to shame and destroy this person.
Yeah,
they're in there.

529
00:28:15.091 --> 00:28:18.480
Right?
There's something kind of psychologically gratifying about being part of a

530
00:28:18.481 --> 00:28:20.010
moralizing mob,

531
00:28:20.040 --> 00:28:22.820
and I think that's an impulse that there does have to be resisted.
Right?

532
00:28:22.840 --> 00:28:25.830
So it's virtue signaling.
It's not as cool.
That's your right.
Exactly.

533
00:28:26.010 --> 00:28:29.100
Actual virtual would be saying something like,
well,
I,
I,
you know,

534
00:28:29.190 --> 00:28:32.700
I think everybody here is wrong about what the right thing to do is here.

535
00:28:33.090 --> 00:28:36.810
That's usually a pretty lonely guy.
That's true.
That's true.
All right,
one more.

536
00:28:36.840 --> 00:28:39.770
Sure.
The asch conformity experiment.
Yeah.

537
00:28:39.810 --> 00:28:43.200
So that's along the same line as the Stanford prison experiment and the Milgram

538
00:28:43.201 --> 00:28:46.920
experiments in this,
I think this was the one of the originals.

539
00:28:47.130 --> 00:28:50.580
It was in the 1950s and,
uh,

540
00:28:50.640 --> 00:28:53.460
subjects were given,
uh,

541
00:28:54.070 --> 00:28:57.150
a board which had lines of different lengths on them.

542
00:28:57.390 --> 00:29:00.270
So like there was one that was very long when it was moderately long,

543
00:29:00.271 --> 00:29:02.070
when it was short,
for example.
Uh,

544
00:29:02.071 --> 00:29:07.020
and then they were given another line that they were told was the same length as

545
00:29:07.021 --> 00:29:09.810
one of the lines on the other board.
And it was very obvious.
So it was like,

546
00:29:09.960 --> 00:29:12.540
this was a,
like a,
a very long line over here.

547
00:29:12.750 --> 00:29:15.570
And there was a very long line over here.
There are clearly the same length,

548
00:29:15.571 --> 00:29:18.930
but they were surrounded by other lines of various link,
but it was just,

549
00:29:18.931 --> 00:29:21.180
is very easy test until they asked subjects.
Well,

550
00:29:21.181 --> 00:29:24.240
which line over here is the same length as this line over here?

551
00:29:24.270 --> 00:29:27.210
And everybody knew what the correct answer was.

552
00:29:27.690 --> 00:29:30.350
But the twist of the experiment was,
uh,

553
00:29:30.380 --> 00:29:34.620
the experimenter would bring in confederates who would lie and give the

554
00:29:34.650 --> 00:29:38.130
obviously false answers.
So they would say,
uh,
oh.
In fact,

555
00:29:38.131 --> 00:29:41.620
it turns out that the short line over here is the same length as this line over

556
00:29:41.621 --> 00:29:42.190
here.

557
00:29:42.190 --> 00:29:46.690
And what they wanted to test was whether the genuine subject would conform to

558
00:29:46.691 --> 00:29:49.300
the mistaken group judgment,
which they need to be clearly wrong,

559
00:29:49.450 --> 00:29:52.870
or would they stick to their guns and say,
no,
everybody in this room is wrong.

560
00:29:53.230 --> 00:29:56.440
It's clearly,
you know,
this line here and everybody else made a mistake.

561
00:29:56.800 --> 00:30:00.710
And the depressing results is that many people,
uh,

562
00:30:00.760 --> 00:30:02.320
would not just conform once,

563
00:30:02.321 --> 00:30:06.400
but would conform over and over again to the judgment that they knew was wrong.

564
00:30:06.670 --> 00:30:10.450
And even people who sometimes broke from the group would have at least a couple

565
00:30:10.451 --> 00:30:12.550
of answers where they conform to the group answer.

566
00:30:12.600 --> 00:30:17.600
<v 2>So is that just sort of basic built in DNA that just certain people are going to</v>

567
00:30:18.121 --> 00:30:20.910
stand up for what they believe in what's right and with true and,

568
00:30:20.970 --> 00:30:24.780
and just most of us aren't.
Or is that,
can you,
can you really?

569
00:30:25.010 --> 00:30:29.630
<v 1>Oh yeah.
That's a good pitch.
Like Steven pinker requests.
That's it.
Yeah.
Right.</v>

570
00:30:29.670 --> 00:30:32.610
Well I believe pinker would say something about a blank slate.
Yeah.
Right.
So,

571
00:30:32.611 --> 00:30:36.210
so I'm not sure.
Right.
He might,
he might say it's something close to our DNA.

572
00:30:36.450 --> 00:30:37.283
I'm not sure.

573
00:30:37.440 --> 00:30:41.010
It's interesting when you read testimony from people in the Milgram experiments,

574
00:30:41.011 --> 00:30:45.000
there were some people who just refused to deliver these shocks to people even

575
00:30:45.001 --> 00:30:46.260
though there was a lot of pressure on them.

576
00:30:46.680 --> 00:30:51.680
And it seems like some of them had life experiences that really discourage them

577
00:30:52.591 --> 00:30:56.610
from obeying the authority.
And it wouldn't shock me,
no pun intended,

578
00:30:56.730 --> 00:30:59.970
if there was something like that going on in these conformity experiments where

579
00:31:00.150 --> 00:31:04.620
maybe people have seen the malevolent effects of blind conformity in this,

580
00:31:04.621 --> 00:31:06.990
motivated them to do it.
Uh,
so yeah.

581
00:31:06.991 --> 00:31:09.540
So I'm not sure whether it's nature or nurture.
I don't know.

582
00:31:09.541 --> 00:31:10.890
That's probably above my pay grade.

583
00:31:11.780 --> 00:31:13.200
<v 2>All right.
As long as you mentioned pinker,</v>

584
00:31:13.201 --> 00:31:16.650
this I think will be a good way to wrap this all up.
Are you hopeful as,

585
00:31:16.651 --> 00:31:21.650
as a philosopher,
are you hopeful for free thought?
I mean a,

586
00:31:21.651 --> 00:31:25.740
a lot of people that care about the conversations that we're having are really

587
00:31:25.741 --> 00:31:29.310
worried and I find that the people that I'm most closely associated with,

588
00:31:29.311 --> 00:31:30.870
that I have these conversations with most,

589
00:31:31.170 --> 00:31:34.200
I would say most of us are sort of world weary optimists.

590
00:31:34.201 --> 00:31:37.080
That's how I would describe myself.
Like if I wasn't hopeful,

591
00:31:37.081 --> 00:31:38.970
I don't know how the hell I could do this every day.
Right?

592
00:31:38.971 --> 00:31:43.820
Like I'm hopeful that we can make things better and yet at the same time there's

593
00:31:43.830 --> 00:31:48.690
no doubt we have an uphill battle and that there's so much hysteria all the time

594
00:31:48.691 --> 00:31:50.730
and all of the forces that,

595
00:31:50.770 --> 00:31:54.540
that we're not into seemed to be on the march all the time.
Are you hopeful?

596
00:31:54.700 --> 00:31:56.940
<v 1>I like that term world.
Very optimistic.</v>

597
00:31:56.941 --> 00:31:59.190
I think that probably describes me pretty well.

598
00:31:59.191 --> 00:32:04.191
And part of the reason I'm optimistic is because oftentimes when I think when

599
00:32:04.771 --> 00:32:07.140
you're interacting with people on a one on one level,

600
00:32:07.470 --> 00:32:11.820
you're more likely to have a civil productive discussion about controversial

601
00:32:11.821 --> 00:32:14.070
topics.
Then when it's like these huge,

602
00:32:14.640 --> 00:32:19.640
you just gave this talk I think recently at a college and you've got [inaudible]

603
00:32:20.550 --> 00:32:23.440
give me an hour of philosophical.
That's right.
That's right.

604
00:32:23.460 --> 00:32:26.730
Down at the University of New Hampshire.
Okay.
Right.
Yeah.

605
00:32:26.731 --> 00:32:27.820
And so I think in those cities,

606
00:32:27.870 --> 00:32:30.180
so talking about like virtue signaling in a mob mentality,

607
00:32:30.181 --> 00:32:33.600
I think those sorts of settings are conducive to that.

608
00:32:33.840 --> 00:32:38.220
I think oftentimes when you're just talking with a person over a beer,

609
00:32:38.370 --> 00:32:42.840
you might have different politics than they do,
but it's kind of a,

610
00:32:42.890 --> 00:32:46.220
a non threatening sort of friendly situation.

611
00:32:46.520 --> 00:32:49.250
I think people's guards go down a little bit,
uh,

612
00:32:49.280 --> 00:32:50.810
more in those sorts of situations.

613
00:32:50.810 --> 00:32:54.740
And you can actually have a conversation about controversial topics in good

614
00:32:54.741 --> 00:32:59.030
faith.
And so that I,
so I think people want that.
I think people,
uh,

615
00:32:59.031 --> 00:33:02.580
want to have these kinds of discussions and arguments.
Um,

616
00:33:02.630 --> 00:33:07.630
and it might be the people who are least friendly to that sort of thing.

617
00:33:07.760 --> 00:33:11.600
Uh,
get the most visibility or get the most publicity.
But I think,
you know,

618
00:33:11.601 --> 00:33:13.940
if you took,
took an average person off the street and he said,
hey,

619
00:33:13.941 --> 00:33:18.560
let's grab a beer and talk about politics,
it wouldn't be nearly as bad as,

620
00:33:18.590 --> 00:33:21.620
you know,
uh,
doing it in front of 50,000 people.
I agree.

621
00:33:21.621 --> 00:33:25.070
And that's why I said the thing about the 80% before,
and I think in summation,

622
00:33:25.071 --> 00:33:29.600
you believe that beer is the great philosophical equalizer.
Maybe Bourbon.

623
00:33:29.601 --> 00:33:34.250
I'm not really [inaudible] beer.
There's no,
I've got this great thing over here.

624
00:33:34.251 --> 00:33:37.580
So yeah.
So Bourbon is Bourbon whiskey that,
yeah.
On that note,

625
00:33:37.581 --> 00:33:40.160
we should wrap this up and let's see what we can do with the Bourbon.
All right,

626
00:33:40.161 --> 00:33:44.960
for more on Chris Checkout,
see fryman.com which we'll link to write down below.

