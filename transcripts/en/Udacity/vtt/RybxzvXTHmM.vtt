WEBVTT
Kind: captions
Language: en

00:00:00.650 --> 00:00:01.550
Policy shaping.

00:00:01.550 --> 00:00:02.540
&gt;&gt; Policy shaping.

00:00:02.540 --> 00:00:04.689
&gt;&gt; Did you make up that word because
I'm not sure I understand what that

00:00:04.689 --> 00:00:05.840
word means.

00:00:05.840 --> 00:00:09.770
&gt;&gt; So I did not make up the word policy
and I did not make up the word shaping.

00:00:09.770 --> 00:00:10.562
&gt;&gt; No I know that.

00:00:10.562 --> 00:00:15.040
[LAUGH] No, I'm sorry that
the combination of those two words.

00:00:15.040 --> 00:00:18.010
&gt;&gt; Well I'm not going to say I
made it up although maybe I did,

00:00:18.010 --> 00:00:21.970
one of the papers that we're
going to read is actually a paper

00:00:21.970 --> 00:00:24.230
that I was a part of the author team on.

00:00:24.230 --> 00:00:26.970
And it's all about introducing
the notion of policy shaping.

00:00:26.970 --> 00:00:29.958
And it's really a very simple idea,
it's sort of what I just described, that

00:00:29.958 --> 00:00:32.786
what human beings are actually doing
is giving you direct policy advice.

00:00:32.786 --> 00:00:37.298
And so they're trying to shape what
the policy should be directly,

00:00:37.298 --> 00:00:42.680
as opposed to trying to say something
about what rewards ought to be.

00:00:42.680 --> 00:00:43.600
Does that make sense?

00:00:43.600 --> 00:00:44.120
&gt;&gt; Yeah I think so.

00:00:44.120 --> 00:00:47.590
I mean I guess now, before I was
thinking about shaping differently.

00:00:47.590 --> 00:00:50.110
But now it's sort of feeling
like sculpting to me,

00:00:50.110 --> 00:00:54.470
like you have a policy out there and
you're going to apply pressure and

00:00:54.470 --> 00:00:57.700
smoothing in different places to
have it be a little bit different.

00:00:57.700 --> 00:00:59.720
&gt;&gt; Right, so think about it this way.

00:00:59.720 --> 00:01:01.350
There's actually I think
a direct way of doing this.

00:01:01.350 --> 00:01:04.010
When you do policy shaping,
what have you actually done?

00:01:04.010 --> 00:01:06.954
What you've actually done is you've
taken whatever the reward function was

00:01:06.954 --> 00:01:11.250
in say some particular state or
some particular action and

00:01:11.250 --> 00:01:12.020
let's say was zero.

00:01:12.020 --> 00:01:15.550
And what you said is no, this is
the wrong answer, the wrong value,

00:01:15.550 --> 00:01:17.020
it should not be zero here.

00:01:17.020 --> 00:01:19.940
It should actually be something else,
it should be, well I don't know,

00:01:19.940 --> 00:01:22.590
three so you've actually replace
the original reward function

00:01:22.590 --> 00:01:27.370
R with another reward function R prime
instead this is actually the reward

00:01:27.370 --> 00:01:29.130
function that you should
be learning over.

00:01:29.130 --> 00:01:31.620
And that's really what reward
shaping does for you, right?

00:01:31.620 --> 00:01:34.250
I mean, if you think back to the
examples that you were giving with all

00:01:34.250 --> 00:01:37.530
the reward shaping, you were
replacing reward values, agreed?

00:01:37.530 --> 00:01:40.920
&gt;&gt; Yeah, though in the case where we
were doing potential base shaping,

00:01:40.920 --> 00:01:44.130
we also had to make sure that whenever
we sprinkled some positive reward in,

00:01:44.130 --> 00:01:47.380
there was negative reward someplace
else to balance it out, so

00:01:47.380 --> 00:01:50.080
you didn't get new positive
reward cycles happening.

00:01:50.080 --> 00:01:53.870
&gt;&gt; Right now that's true but regardless
of where you sprinkle them and

00:01:53.870 --> 00:01:57.630
what kind of constraints you put up you
still took what was originally a reward

00:01:57.630 --> 00:02:00.580
value of something like zero and
replaced it with another reward value

00:02:00.580 --> 00:02:01.080
&gt;&gt; I agree with that.

00:02:01.080 --> 00:02:03.020
&gt;&gt; Plus three, minus three,
seven, doesn't matter.

00:02:03.020 --> 00:02:04.810
So you change the reward function.

00:02:04.810 --> 00:02:08.860
So in exactly the same way policy
shaping is about changing the policy.

00:02:08.860 --> 00:02:12.420
There's some policy that
the agent currently has and

00:02:12.420 --> 00:02:15.830
you're saying actually in this state,
rather than whatever it was you

00:02:15.830 --> 00:02:19.740
think you should do,
you should do in fact this.

00:02:19.740 --> 00:02:23.140
So maybe I'm confirming what you already
had, maybe I'm telling you to do

00:02:23.140 --> 00:02:27.690
something different, but regardless
I'm overriding your policy function.

00:02:27.690 --> 00:02:31.340
In the same way that in reward shaping
you're overriding the reward function.

00:02:31.340 --> 00:02:32.440
&gt;&gt; Okay, that makes sense.

00:02:32.440 --> 00:02:35.740
&gt;&gt; Right, so this is what
policy shaping is all about and

00:02:35.740 --> 00:02:38.360
it happens to line up with,
it turns out in practice,

00:02:38.360 --> 00:02:40.910
the way people are actually
trying to convey information.

00:02:40.910 --> 00:02:43.830
And so then the real question is,
what do you do with this?

00:02:43.830 --> 00:02:46.080
How do I take this information
that's being given,

00:02:46.080 --> 00:02:49.130
if it's not being given in the form of
rewards, we know how to do that, but

00:02:49.130 --> 00:02:52.300
it's actually being given
in the form of a policy.

00:02:52.300 --> 00:02:54.220
What should I actually do about it?

00:02:54.220 --> 00:02:57.340
&gt;&gt; So I was interpreting what
you described as saying,

00:02:57.340 --> 00:03:01.470
if a person gives positive feedback for
an action in a state and it's not

00:03:01.470 --> 00:03:06.050
the action that the agent would have
taken that state, then just replace it.

00:03:06.050 --> 00:03:10.300
Just make a modification of the policy
so that, that's the action that's taken.

00:03:10.300 --> 00:03:11.520
Is that the way to do it?

00:03:11.520 --> 00:03:13.230
&gt;&gt; Well, ideally that is
what you would do, right?

00:03:13.230 --> 00:03:16.270
If a person tells you, look,
you should go up here, or

00:03:16.270 --> 00:03:19.150
you should eat a ghost here,
you should just do it.

00:03:19.150 --> 00:03:24.240
But that actually requires something be
true, which as the lawyers would say,

00:03:24.240 --> 00:03:25.980
assumes facts not in evidence.

00:03:25.980 --> 00:03:28.530
And that is,
that the person's actually right.

00:03:28.530 --> 00:03:29.580
What if the person's wrong?

00:03:29.580 --> 00:03:33.570
&gt;&gt; Yeah, but certainly we should at
least have that as our assumption.

00:03:33.570 --> 00:03:36.140
I mean, if we don't think the person
is right then this is a really

00:03:36.140 --> 00:03:37.550
easy thing to do policy keeping.

00:03:37.550 --> 00:03:38.880
Just ignore the person.

00:03:38.880 --> 00:03:41.160
&gt;&gt; But what if the person is
right most of the time, but

00:03:41.160 --> 00:03:42.630
can occasionally be wrong?

00:03:42.630 --> 00:03:44.690
&gt;&gt; I see.
&gt;&gt; But what if the person's right but

00:03:44.690 --> 00:03:47.430
actually meant to say at
the time step before?

00:03:47.430 --> 00:03:50.680
&gt;&gt; I see, so maybe there's a way to
use all these different things as

00:03:50.680 --> 00:03:53.330
evidence and
then to combine them together to get,

00:03:53.330 --> 00:03:57.640
I don't know kind of a best guess
as to what the right answer is.

00:03:57.640 --> 00:04:01.630
&gt;&gt; Exactly and that's why it's
policy shaping and not policy doing.

00:04:01.630 --> 00:04:06.910
So I'm going to try to give you a little
quiz and see if we can kind of work out

00:04:06.910 --> 00:04:10.610
what the right thing to do is under
the circumstances, where a person

00:04:10.610 --> 00:04:13.210
might tell you to do something and
might not always be right about it.

00:04:13.210 --> 00:04:13.920
Seem fair?

00:04:13.920 --> 00:04:14.420
&gt;&gt; Yeah.

