WEBVTT

1
00:00:00.580 --> 00:00:00.770
Okay.

2
00:00:00.770 --> 00:00:05.240
<v 1>In this video will introduce another type of task which is quite useful and</v>

3
00:00:05.241 --> 00:00:08.960
popular in natural language processing.
The task is word tagging.

4
00:00:11.060 --> 00:00:11.180
<v 0>Okay.</v>

5
00:00:11.180 --> 00:00:13.700
<v 1>So in many NLP applications or system,</v>

6
00:00:13.730 --> 00:00:18.650
it's actually going to be useful to take the raw text and actually augmented

7
00:00:18.651 --> 00:00:23.651
with some additional higher level data of syntactic or semantic nature.

8
00:00:25.160 --> 00:00:26.150
And,
uh,

9
00:00:26.180 --> 00:00:31.180
specifically we would like to add semantic or syntactic labels to each word in

10
00:00:32.961 --> 00:00:36.710
the,
in the,
in a document or in a sentence.
Um,

11
00:00:36.770 --> 00:00:41.770
so this problem can be tackled using some sequential classification problem.

12
00:00:42.520 --> 00:00:46.430
Uh,
and in particular we could use a conditional random fields approach,

13
00:00:46.760 --> 00:00:49.730
which you've seen earlier in the course.
Um,

14
00:00:49.850 --> 00:00:54.050
and then we could use a neural net to model the urinary potentials,
uh,

15
00:00:54.051 --> 00:00:58.340
where the neural net would take as input a window for words and these words

16
00:00:58.341 --> 00:01:01.670
would be mapped to work representations and so on.
Uh,

17
00:01:01.700 --> 00:01:06.700
what will actually describe in the subsequent videos is something that's a bit

18
00:01:08.120 --> 00:01:12.830
more involved,
a model that some,
a bit more complicated than that.

19
00:01:13.160 --> 00:01:16.220
It's a very original model which performs really well.

20
00:01:16.400 --> 00:01:19.910
It was designed by [inaudible] and Jason Weston.

21
00:01:20.090 --> 00:01:22.930
In this paper.
And,
uh,

22
00:01:23.120 --> 00:01:25.790
before we actually look at what this model is,

23
00:01:26.210 --> 00:01:30.740
we'll first spend some time looking at different word tagging tasks that are

24
00:01:30.741 --> 00:01:32.690
popular in the NLP community.

25
00:01:35.510 --> 00:01:36.220
<v 0>Yeah.</v>

26
00:01:36.220 --> 00:01:40.740
<v 1>One of the simplest tasks,
probably four,
we're tagging is a part,</v>

27
00:01:40.750 --> 00:01:42.810
a part of speech tagging a,

28
00:01:42.820 --> 00:01:47.470
so the idea here is that we want to take each word and identify what is the part

29
00:01:47.471 --> 00:01:51.730
of speech category of that work.
So in other words,
is it a noun,

30
00:01:51.760 --> 00:01:55.270
is it a verb,
sit in that verb,
an adjective,
and so on.
Uh,

31
00:01:55.690 --> 00:01:59.650
you could determine different types of tax sets.
Uh,

32
00:01:59.651 --> 00:02:01.600
so different types of categories.

33
00:02:01.780 --> 00:02:06.190
You might want categories that distinguished between singular and plural words

34
00:02:06.220 --> 00:02:10.480
or between the tense present or past tense of verbs.

35
00:02:10.481 --> 00:02:11.830
And so on.
Um,

36
00:02:11.920 --> 00:02:16.510
one of the perhaps most popular tax set is the Pantry Bang,
pause,

37
00:02:16.511 --> 00:02:18.070
tax set,
uh,

38
00:02:18.190 --> 00:02:21.760
to get a more detailed description of what it is I encourage you to look on on,

39
00:02:21.820 --> 00:02:25.440
uh,
the web.
Uh,
I won't describe it further.
It.

40
00:02:25.441 --> 00:02:26.890
Usually when you get a Dataset,

41
00:02:26.950 --> 00:02:30.820
you essentially have the tax set that is a defined,

42
00:02:30.910 --> 00:02:35.870
uh,
by the,
uh,
either by reference to the Pantry Bank,
pass tag,

43
00:02:35.871 --> 00:02:39.010
set a or will be defined for this particular data set.

44
00:02:39.011 --> 00:02:42.580
But usually you don't really get a choice when you just take a data set from the

45
00:02:42.581 --> 00:02:45.160
web and to train a model.
Um,

46
00:02:45.310 --> 00:02:49.390
and so here we have an example where a,
for instance in this sentence,

47
00:02:49.391 --> 00:02:53.680
the little yellow dog,
uh,
barked at the cat.
Well,

48
00:02:53.681 --> 00:02:57.390
we've identified that Bart was a verb.
That dog is a noun,

49
00:02:57.530 --> 00:02:59.680
yellow is in Niger active and so on.

50
00:03:03.020 --> 00:03:03.550
<v 2>Yeah.</v>

51
00:03:03.550 --> 00:03:06.490
<v 1>And other were tagging.
Problem is chunking.</v>

52
00:03:06.640 --> 00:03:11.640
So the idea is to segment the sentence into phrases that have a particular

53
00:03:12.100 --> 00:03:16.110
syntactic,
former,
particularly syntactic unit,
a,
uh,

54
00:03:16.440 --> 00:03:20.410
particular,
uh,
phrase.
Um,
so,
uh,

55
00:03:20.411 --> 00:03:25.150
for instance,
in the sentence he reckons the current account deficit,

56
00:03:25.510 --> 00:03:30.310
uh,
then,
uh,
we could distinguish that we have he here,

57
00:03:30.311 --> 00:03:34.570
which is a noun phrase reckons,
which is a verb frames.

58
00:03:34.571 --> 00:03:36.740
So these are simple because there,

59
00:03:36.850 --> 00:03:39.700
there is a segment that correspond to just an individual word,

60
00:03:39.880 --> 00:03:44.140
but then we could chunk together the current account deficit as forming a

61
00:03:44.141 --> 00:03:47.380
particular meaningful syntactic unit in that sentence.

62
00:03:48.520 --> 00:03:50.860
And um,
so,
uh,

63
00:03:50.861 --> 00:03:55.861
so it means that we have to somehow turn this problem of chunking or segmenting

64
00:03:56.351 --> 00:03:59.830
the sentence into a individual word tagging problem.

65
00:04:00.310 --> 00:04:04.150
And the way this is done usually is by using some sort of encoding,

66
00:04:04.330 --> 00:04:08.900
identifying the beginning,
the middle,
and the end of the segments.
Uh,

67
00:04:08.950 --> 00:04:13.150
so one type of encoding is the IOB s encoding.

68
00:04:13.540 --> 00:04:14.230
Um,

69
00:04:14.230 --> 00:04:18.100
specifically what it does is that if we have a phrase that corresponds to just a

70
00:04:18.101 --> 00:04:18.934
single word,

71
00:04:19.060 --> 00:04:23.590
then we'll just a prefix s dash a and then,
uh,

72
00:04:23.830 --> 00:04:27.880
right before the type of phrase that we have.
So if you have a noun phrase,

73
00:04:28.150 --> 00:04:32.860
then we would have s dash right in front to identify that this word constitutes

74
00:04:33.190 --> 00:04:37.360
a noun phrase like,
uh,
so we have an example right here.

75
00:04:37.420 --> 00:04:41.020
He is just a noun phrase.
The word itself is a noun phrase.

76
00:04:41.830 --> 00:04:43.690
And now if we have multiple word phrases,

77
00:04:44.410 --> 00:04:47.050
then we'll identify the beginning with B,

78
00:04:47.290 --> 00:04:52.090
the end word with e in the,
uh,
in the phrase.
And,

79
00:04:52.120 --> 00:04:52.690
uh,

80
00:04:52.690 --> 00:04:57.340
I will use I for all of the words that fall between the beginning and DM.

81
00:04:57.940 --> 00:05:01.980
So if you had the word phrase that contained forwards,
we'd have

82
00:05:02.130 --> 00:05:05.790
<v 2>Bvp,
Ivp,
Ivp and EVP.</v>

83
00:05:06.450 --> 00:05:08.290
<v 1>And so I realizing now this isn't a beat.</v>

84
00:05:08.310 --> 00:05:10.380
<v 2>It should be,
it should be a p.
Okay.</v>

85
00:05:10.620 --> 00:05:15.150
<v 1>And so in our example,
the current account deficit would be,</v>

86
00:05:15.750 --> 00:05:17.430
each of its word would be labeled with

87
00:05:17.710 --> 00:05:21.160
<v 2>BNP,
I,
N,
p,
I,
n,
p,
and e and p.</v>

88
00:05:22.040 --> 00:05:27.040
<v 1>And for any other words that do not fall in the category of trying to recognize</v>

89
00:05:27.561 --> 00:05:30.620
the segments that we're trying to identify in a sentence,

90
00:05:30.800 --> 00:05:34.730
we will use the other label.
Oh yeah.

91
00:05:34.760 --> 00:05:39.760
To identify that particular work doesn't belong to any particular phrase we're

92
00:05:40.400 --> 00:05:43.880
trying to,
or segment we're trying to identify in the sentence.

93
00:05:46.870 --> 00:05:47.460
<v 2>Yeah.</v>

94
00:05:47.460 --> 00:05:51.390
<v 1>Named entity recognition is in other type of word tagging problem.</v>

95
00:05:52.250 --> 00:05:55.220
Um,
this is also a,
uh,

96
00:05:55.240 --> 00:05:59.660
a case where we might have to segments.
So we might have,
for instance,

97
00:06:00.110 --> 00:06:04.430
a to identify persons are locations which would be described by more than a

98
00:06:04.431 --> 00:06:07.430
single word.
So named entity recognition.

99
00:06:07.431 --> 00:06:11.900
We are interested in finding whether a words in a sentence correspond to the

100
00:06:11.901 --> 00:06:15.410
identification of something that's named.
So a person,

101
00:06:15.470 --> 00:06:19.630
a vocation and organization.
So I have an example here,
UN officials,
uh,

102
00:06:19.910 --> 00:06:24.050
each kiss probably saying that wrong,
but a heads for Baghdad.

103
00:06:24.740 --> 00:06:27.900
And in this case we only have single word named entity.
Uh,

104
00:06:27.920 --> 00:06:31.730
we have UN just an organizations.
We have s dash org,

105
00:06:33.010 --> 00:06:37.220
uh,
eat kiss or [inaudible],
uh,
which is s dash,
uh,

106
00:06:37.440 --> 00:06:40.970
per person.
And then Baghdad,
which is a location,

107
00:06:40.971 --> 00:06:45.330
so as dash location.
And then all the other words are not named to.

108
00:06:45.400 --> 00:06:48.350
So they're tagged with the tag.
Oh.

109
00:06:51.350 --> 00:06:52.900
And finally another task,

110
00:06:52.910 --> 00:06:56.690
which is a much more complicated than the previous ones,

111
00:06:56.691 --> 00:07:00.650
and it's also a richer and corresponds to higher level information about words

112
00:07:00.830 --> 00:07:02.780
is semantic role labeling.

113
00:07:03.350 --> 00:07:06.920
So now the idea is that for the words in the sentence,

114
00:07:06.921 --> 00:07:11.450
we want to identify what roles they're playing in the cemented meaning of the

115
00:07:11.451 --> 00:07:12.284
sentence.

116
00:07:12.500 --> 00:07:16.490
And with respect to each verb that is contained in the sentence.

117
00:07:16.491 --> 00:07:20.000
So it means that if a sentence contains,
say,
two words,

118
00:07:20.001 --> 00:07:24.080
then we'll have two types of segmentations.
One for each verb.

119
00:07:24.530 --> 00:07:29.490
And the segmentation will identify the roles played with respect to each verb,

120
00:07:29.910 --> 00:07:34.190
uh,
the roles that these other words,
our plane.
So,

121
00:07:34.230 --> 00:07:37.580
so let's look at it and an example,
whether it's a single Ferb,

122
00:07:37.640 --> 00:07:40.910
you wouldn't accept anything of value.
Uh,

123
00:07:40.940 --> 00:07:45.080
well first we'd identified the verb or the tag fee,

124
00:07:45.710 --> 00:07:49.910
and then,
um,
then we'd identified the Seminole,

125
00:07:49.911 --> 00:07:54.560
the phrase in the sentence that corresponds to the actor of the verbs so far,

126
00:07:54.561 --> 00:07:57.810
except it would be the acceptor.
And,
uh,
uh,

127
00:07:57.830 --> 00:08:01.670
when we have labeling that is with the label ac row.
So in this case,

128
00:08:01.671 --> 00:08:06.560
it's he who is accepting.
So we have a single word phrase,

129
00:08:06.920 --> 00:08:09.230
a corresponding to the type a zero.

130
00:08:09.980 --> 00:08:14.750
And then there's the object of the verb.
So in this case,
four,

131
00:08:14.751 --> 00:08:18.170
except it's the thing that is accepted,
uh,

132
00:08:18.171 --> 00:08:21.830
we'll label that as a one.
And in this case,

133
00:08:21.831 --> 00:08:25.190
the thing that is accepted is anything of value.
And now it's a,

134
00:08:25.250 --> 00:08:29.660
it's a phrase with three words.
We have ba one I won and he won.

135
00:08:30.620 --> 00:08:34.870
And then the other tags.
So it could be,
uh,

136
00:08:34.910 --> 00:08:39.090
it's accepted from what,
uh,
attributes of the,
uh,

137
00:08:39.190 --> 00:08:41.000
accept action.
Uh,

138
00:08:41.001 --> 00:08:46.001
there can be a verbs that influenced the mode or that correspond to negotiation

139
00:08:47.200 --> 00:08:52.050
formation.
Uh,
again,
I won't go into detail of how exactly,
um,

140
00:08:52.250 --> 00:08:55.260
what are the different types of roles that,
uh,

141
00:08:55.480 --> 00:08:58.100
where it's can play with respect to a verb a,

142
00:08:58.110 --> 00:09:01.980
but I'll let you look at the literature on Semantic relabeling to learn more

143
00:09:01.981 --> 00:09:04.970
about that.
But again,
usually if we take a data set from the webs,

144
00:09:04.980 --> 00:09:07.410
the words are of course already labeled for you.

145
00:09:07.760 --> 00:09:12.760
And so you can just look at the categories that are in the training set to know

146
00:09:13.021 --> 00:09:15.630
which categories you should be identifying.

147
00:09:18.420 --> 00:09:19.120
<v 3>Okay.</v>

148
00:09:19.120 --> 00:09:23.920
<v 1>And,
uh,
whatever the data sets are distributed,
uh,
with we're tagging information,</v>

149
00:09:24.100 --> 00:09:29.050
the raw data usually looks like this.
It's a text file.
And,
uh,

150
00:09:29.110 --> 00:09:33.400
essentially each line in the fog responds to one verse,
one word,

151
00:09:33.550 --> 00:09:37.420
and all of its a tags.
So in this example we have,

152
00:09:37.970 --> 00:09:41.920
and the different tags are arranged into different columns.
So for instance,

153
00:09:41.980 --> 00:09:46.210
this column would be the Pos stag.
This would be chunking tags.

154
00:09:46.480 --> 00:09:50.270
These would be named entity tag.
So for instance,
we only have one name,

155
00:09:50.271 --> 00:09:54.120
Ntt Jupiter,
and it's,
uh,
uh,
it was,
uh,

156
00:09:54.130 --> 00:09:58.030
identify as an organization.
Um,

157
00:09:58.660 --> 00:10:02.050
and then,
uh,
I'm not sure exactly if that's the correct label,

158
00:10:02.051 --> 00:10:07.030
but that's what is in this data.
Uh,
then,
uh,
what do we have?
Uh,

159
00:10:07.031 --> 00:10:07.900
we have,
uh,

160
00:10:07.930 --> 00:10:12.930
here the roles played by the other words for this verb here,

161
00:10:13.541 --> 00:10:18.220
faces and then the roles played by the same words,

162
00:10:18.221 --> 00:10:21.880
but with respect now to the verb explore.
Okay.

163
00:10:21.881 --> 00:10:26.200
So we see now that we have tags for each verb in the sentence.

164
00:10:27.100 --> 00:10:31.810
And so now what we'd like to have is an or learning algorithm to,
for instance,

165
00:10:31.811 --> 00:10:35.320
and neural network learning algorithm that would take all of this data and be

166
00:10:35.321 --> 00:10:40.321
able to reproduce it and produce these stacks for a new sentences.

167
00:10:40.900 --> 00:10:44.860
And so that's what we'll look at at the,
uh,
in the next videos.

