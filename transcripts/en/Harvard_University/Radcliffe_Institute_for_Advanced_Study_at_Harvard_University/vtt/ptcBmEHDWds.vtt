WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.325
[MUSIC PLAYING]

00:00:06.520 --> 00:00:08.270
- So good afternoon, everybody.

00:00:08.270 --> 00:00:11.530
I'm very glad to see here so
many friendly and familiar

00:00:11.530 --> 00:00:12.490
faces.

00:00:12.490 --> 00:00:15.580
Thank you for coming
here this hour.

00:00:15.580 --> 00:00:19.060
It's really a great honor for
me to be here on the stage

00:00:19.060 --> 00:00:22.630
and I would like to think the
Radcliffe Institute for giving

00:00:22.630 --> 00:00:25.360
me this opportunity for
the amazing fellowship

00:00:25.360 --> 00:00:28.570
program which I'm part of.

00:00:28.570 --> 00:00:32.150
And to thank Dean Cohen
and Professor Vichniac,

00:00:32.150 --> 00:00:35.470
Rebecca, Sharon, all the
Radcliffe staff working

00:00:35.470 --> 00:00:38.350
tirelessly both in front
and behind the scenes

00:00:38.350 --> 00:00:39.380
to make this all happen.

00:00:39.380 --> 00:00:41.290
Thank you very much.

00:00:41.290 --> 00:00:43.930
I guess I'm also the first
one to talk about science

00:00:43.930 --> 00:00:47.800
in these series of
Radcliffe public talks

00:00:47.800 --> 00:00:51.320
and I must say that I feel a
little bit nervous and excited

00:00:51.320 --> 00:00:53.650
after the wonderful
lecture of Professor Brian

00:00:53.650 --> 00:00:57.410
Greene that all of you
attended maybe two weeks ago.

00:00:57.410 --> 00:01:00.880
And I think the bar is set so
unachievably high that it's

00:01:00.880 --> 00:01:02.340
probably impossible to make.

00:01:02.340 --> 00:01:06.040
His ability to explain
very complex notions

00:01:06.040 --> 00:01:09.220
in modern physics and string
theory that probably requires

00:01:09.220 --> 00:01:11.140
the most complicated
math that has ever

00:01:11.140 --> 00:01:14.020
been invented in very
simple layman terms

00:01:14.020 --> 00:01:16.790
is really extraordinary.

00:01:16.790 --> 00:01:19.400
So I was actually, as Judy
mentioned, two years ago,

00:01:19.400 --> 00:01:22.120
I was invited to speak at
the World Economic Forum

00:01:22.120 --> 00:01:24.970
in all these events
that gathers all

00:01:24.970 --> 00:01:27.190
the mighty people of the world.

00:01:27.190 --> 00:01:28.990
And I prepared a
wonderful presentation

00:01:28.990 --> 00:01:30.520
about the latest
and greatest stuff

00:01:30.520 --> 00:01:32.860
that we're doing
in our research,

00:01:32.860 --> 00:01:35.560
but their reaction was
oh, actually our audience,

00:01:35.560 --> 00:01:38.350
maybe in the best case, would
read Scientific American.

00:01:38.350 --> 00:01:42.000
So they want to hear a nice
story and see cool pictures.

00:01:42.000 --> 00:01:45.470
And so I hope I have prepared
an interesting story for you

00:01:45.470 --> 00:01:46.480
tonight.

00:01:46.480 --> 00:01:48.100
Or actually, three stories--

00:01:48.100 --> 00:01:51.220
one about the past, my
personal account how

00:01:51.220 --> 00:01:54.040
I got interested and involved
in this field and basically

00:01:54.040 --> 00:01:56.950
got into a career in research.

00:01:56.950 --> 00:01:59.170
The present, the current,
state of the earth

00:01:59.170 --> 00:02:02.560
and the cool technology
that exists today.

00:02:02.560 --> 00:02:06.240
And maybe a bit more
ambitiously, and maybe a little

00:02:06.240 --> 00:02:08.880
bit also more
arrogantly, the future.

00:02:08.880 --> 00:02:11.470
Or at least what I believe
to be the future or at least

00:02:11.470 --> 00:02:13.600
potentially interesting
and promising research

00:02:13.600 --> 00:02:17.300
directions on which I will
be working during this year

00:02:17.300 --> 00:02:21.410
here at Radcliffe and
of course, our intent

00:02:21.410 --> 00:02:22.970
to show plenty of pictures.

00:02:22.970 --> 00:02:24.875
And in fact, it's not
the case that there

00:02:24.875 --> 00:02:28.790
is a saying that the picture
is worth a thousand words.

00:02:28.790 --> 00:02:32.180
And if we look at the amount of
the information that our brain

00:02:32.180 --> 00:02:36.260
receives from different senses,
most of it-- about 90%--

00:02:36.260 --> 00:02:38.270
comes from vision.

00:02:38.270 --> 00:02:41.970
And the site is arguably the
most important of our senses,

00:02:41.970 --> 00:02:44.960
and we use it continuously
in our everyday life.

00:02:44.960 --> 00:02:48.860
We start learning to recognize
visual objects, basically,

00:02:48.860 --> 00:02:51.740
from age zero-- the moment
we open our eyes way

00:02:51.740 --> 00:02:54.200
before we learn how to speak.

00:02:54.200 --> 00:02:56.600
And therefore, we take
these amazing capability

00:02:56.600 --> 00:02:58.670
of our own brain of
visual perception

00:02:58.670 --> 00:03:01.790
almost for granted
without actually realizing

00:03:01.790 --> 00:03:05.600
how complex the
process that is going

00:03:05.600 --> 00:03:09.200
behind the scenes in our
brain that makes it work.

00:03:09.200 --> 00:03:12.290
My main field of research
is computer vision-- a field

00:03:12.290 --> 00:03:14.660
where we try to
give the machines

00:03:14.660 --> 00:03:19.010
the remarkable capability to
see and understand and perceive

00:03:19.010 --> 00:03:21.312
the world around it.

00:03:21.312 --> 00:03:23.270
Anecdotally, the birth
of computer vision as we

00:03:23.270 --> 00:03:26.720
know it is credited to a
summer project at the MIT

00:03:26.720 --> 00:03:31.579
just next door here in
Cambridge in the 1960s.

00:03:31.579 --> 00:03:33.620
And there, a group of
researchers-- three people,

00:03:33.620 --> 00:03:36.680
unfortunately, two of them
are not with us anymore--

00:03:36.680 --> 00:03:38.450
they got a television
camera that could

00:03:38.450 --> 00:03:40.020
be connected to a computer.

00:03:40.020 --> 00:03:42.830
And if you think of the
technology in the '60s,

00:03:42.830 --> 00:03:45.080
that was super high technology.

00:03:45.080 --> 00:03:47.750
They assigned graduate
students with the task

00:03:47.750 --> 00:03:50.240
of making the
computer describe what

00:03:50.240 --> 00:03:53.310
it sees basically, parse the
visual objects in the scene.

00:03:53.310 --> 00:03:56.150
And though I'm told that
MIT students are very good,

00:03:56.150 --> 00:03:58.160
probably among the
best in the world,

00:03:58.160 --> 00:04:00.350
even they failed to
complete this project.

00:04:00.350 --> 00:04:01.970
So it didn't work
well at the end.

00:04:01.970 --> 00:04:03.928
Probably otherwise, I
wouldn't be standing here

00:04:03.928 --> 00:04:05.480
to speak about computer vision.

00:04:05.480 --> 00:04:08.380
And even 50 years
since, the problem is,

00:04:08.380 --> 00:04:11.870
I would say, quite
far from being solved.

00:04:11.870 --> 00:04:14.110
So speaking seriously, one
of the first applications

00:04:14.110 --> 00:04:17.070
that computer vision
researchers considered

00:04:17.070 --> 00:04:19.519
was to recognize human faces.

00:04:19.519 --> 00:04:21.529
And for us, it's very
important, of course,

00:04:21.529 --> 00:04:23.680
for social interactions
when we talk

00:04:23.680 --> 00:04:28.150
to people we know from the
faces to whom we are talking.

00:04:28.150 --> 00:04:30.940
And you can also think of other
applications like security

00:04:30.940 --> 00:04:33.430
applications when
the computer needs

00:04:33.430 --> 00:04:37.360
to know who is the
person in front of it.

00:04:37.360 --> 00:04:40.540
The human face appears to
be very structured object.

00:04:40.540 --> 00:04:43.750
We all have a nose, a
mouth, pair of eyes.

00:04:43.750 --> 00:04:46.690
And what makes us
different from each other

00:04:46.690 --> 00:04:49.780
is how these different
facial parts look like

00:04:49.780 --> 00:04:52.100
and how they are related.

00:04:52.100 --> 00:04:54.290
What are the spatial
relations between them.

00:04:54.290 --> 00:04:56.350
So the first works
on facial recognition

00:04:56.350 --> 00:05:00.670
tired to automatically detect
these features of the face

00:05:00.670 --> 00:05:03.100
and compute their geometric
relations, in this way,

00:05:03.100 --> 00:05:06.220
creating a kind of
geometric descriptor

00:05:06.220 --> 00:05:11.020
that would able to
identify the individual.

00:05:11.020 --> 00:05:14.500
And at the first glance, it
may sound like an easy task.

00:05:14.500 --> 00:05:17.374
But the problem is that we
are looking at an image.

00:05:17.374 --> 00:05:18.790
Basically, a two
dimensional image

00:05:18.790 --> 00:05:21.370
that comes from a
projection of three

00:05:21.370 --> 00:05:25.870
dimensional world by means of
a camera that mixes together

00:05:25.870 --> 00:05:28.330
a lot of different
information-- the initial color

00:05:28.330 --> 00:05:31.420
of the object, its reflected
properties, its geometry,

00:05:31.420 --> 00:05:34.000
its position in space,
the way it is illuminated.

00:05:34.000 --> 00:05:36.910
Basically, we get a single image
that has all this information

00:05:36.910 --> 00:05:40.420
mixed together and extracting
different components

00:05:40.420 --> 00:05:42.290
is a very challenging task.

00:05:42.290 --> 00:05:44.860
So if you look at these
pictures of a human face,

00:05:44.860 --> 00:05:47.380
it is even hard to tell
that that's the same person

00:05:47.380 --> 00:05:49.810
so differently it
is illuminated.

00:05:49.810 --> 00:05:51.310
And for a computer,
it would be very

00:05:51.310 --> 00:05:54.890
hard to distinguish between what
constitutes a facial feature

00:05:54.890 --> 00:05:57.130
and what is a kind
of false feature--

00:05:57.130 --> 00:06:01.740
what comes from external
factors like light and shadow.

00:06:01.740 --> 00:06:03.340
And that's why many
face recognition,

00:06:03.340 --> 00:06:05.590
of the past at least,
that worked very well

00:06:05.590 --> 00:06:08.410
in the lab in very
controlled conditions

00:06:08.410 --> 00:06:12.670
failed miserably when taken
into outdoor environment.

00:06:12.670 --> 00:06:14.740
Lighting is just one
of different factors

00:06:14.740 --> 00:06:16.810
that can affect the
appearance of the face.

00:06:16.810 --> 00:06:19.540
You can also think of, for
example, head orientation

00:06:19.540 --> 00:06:22.739
or maybe even
facial expressions.

00:06:22.739 --> 00:06:25.030
So talking about illumination,
probably the most famous

00:06:25.030 --> 00:06:28.900
example of the jokes-- so
to say that light can play--

00:06:28.900 --> 00:06:32.110
is this image taken by
the NASA Viking mission

00:06:32.110 --> 00:06:34.000
on Mars in the '70s.

00:06:34.000 --> 00:06:36.790
This bizarre
face-looking structure

00:06:36.790 --> 00:06:39.490
baptized by the journalist
as the Martian Spinx,

00:06:39.490 --> 00:06:43.510
or face on Mars, gave rise to
numerous conspiracy theories

00:06:43.510 --> 00:06:46.900
and ideas about intelligent
extraterrestrial life.

00:06:46.900 --> 00:06:50.170
But subsequent photos show that
it's just a rock formation that

00:06:50.170 --> 00:06:53.110
appeared in this weird form
because of particularly

00:06:53.110 --> 00:06:54.620
wacky illumination.

00:06:54.620 --> 00:06:58.450
So if you were impatient to
get signs of intelligent life

00:06:58.450 --> 00:06:59.980
on Mars, at least not that time.

00:06:59.980 --> 00:07:02.750
It didn't happen.

00:07:02.750 --> 00:07:04.820
Let me show you another example.

00:07:04.820 --> 00:07:08.740
You can look at this drawing
and tell me if it frightens you.

00:07:08.740 --> 00:07:09.880
Does it frighten you?

00:07:09.880 --> 00:07:13.420
You certainly recall the
little Antoine de Saint-Exupery

00:07:13.420 --> 00:07:15.160
posing this question, right?

00:07:15.160 --> 00:07:19.850
And people replied, why should
anyone be frightened by a hat?

00:07:19.850 --> 00:07:20.560
It's a hat.

00:07:20.560 --> 00:07:22.570
However, of course,
what he meant

00:07:22.570 --> 00:07:24.400
was not a hat but a huge snake.

00:07:24.400 --> 00:07:26.770
A boa constrictor
digesting an elephant

00:07:26.770 --> 00:07:27.910
that would look like this.

00:07:27.910 --> 00:07:31.210
Again, this is another
example of how ambiguous

00:07:31.210 --> 00:07:34.420
a two dimensional projection
of three dimensional objects

00:07:34.420 --> 00:07:36.500
could be.

00:07:36.500 --> 00:07:40.460
So since we humans we don't have
the ability to explicitly sense

00:07:40.460 --> 00:07:43.250
depth, we just get
a rough idea of it

00:07:43.250 --> 00:07:45.770
through our binocular vision
or other implicit cues

00:07:45.770 --> 00:07:47.750
such as shading and shadows.

00:07:47.750 --> 00:07:51.050
We attribute way more
importance to the appearance

00:07:51.050 --> 00:07:52.970
than to the 3D geometry.

00:07:52.970 --> 00:07:54.440
Let's do the
following experiment.

00:07:54.440 --> 00:07:57.110
That's an image that
appeared in a book

00:07:57.110 --> 00:07:59.550
that we wrote
about 10 years ago.

00:07:59.550 --> 00:08:01.690
This is a 3D scan
of a human face.

00:08:01.690 --> 00:08:04.190
You can see the three
dimensional surface.

00:08:04.190 --> 00:08:06.860
Now imagine that we
had major make up

00:08:06.860 --> 00:08:09.770
that would allow us to
draw anything on this face.

00:08:09.770 --> 00:08:13.700
We can do it, of course, with
computer graphic tools using

00:08:13.700 --> 00:08:15.480
texture-mapping techniques.

00:08:15.480 --> 00:08:18.570
So if you were able
to paint something,

00:08:18.570 --> 00:08:20.510
let's paint the face
of another person.

00:08:20.510 --> 00:08:22.610
Let's paint the
face of George W.

00:08:22.610 --> 00:08:25.670
Bush who was the President of
the United States at that time.

00:08:25.670 --> 00:08:28.100
Or if you don't like for
some reason George W. Bush,

00:08:28.100 --> 00:08:31.670
let's paint Osama bin Laden,
who was that number one wanted

00:08:31.670 --> 00:08:33.620
terrorist at some point.

00:08:33.620 --> 00:08:36.950
And it's exactly the same
three dimensional surface

00:08:36.950 --> 00:08:38.419
just painted differently.

00:08:38.419 --> 00:08:40.590
And we perceive it
as different people.

00:08:40.590 --> 00:08:44.480
Whereas, if we could explicitly
sense the three dimensional

00:08:44.480 --> 00:08:46.700
structure, if you will
could analyze the geometry,

00:08:46.700 --> 00:08:48.830
we'll be able to tell
immediately that that's not

00:08:48.830 --> 00:08:49.820
Bush or bin Laden.

00:08:49.820 --> 00:08:53.990
It's just an imposter
that painted himself

00:08:53.990 --> 00:08:56.280
in Bush or bin Laden's style.

00:08:56.280 --> 00:08:58.220
So this 3D structure
is not affected

00:08:58.220 --> 00:09:01.100
by makeup, illumination
or head orientation.

00:09:01.100 --> 00:09:03.920
And it contains a lot of useful
information, potentially,

00:09:03.920 --> 00:09:06.110
that could make face
recognition better.

00:09:06.110 --> 00:09:09.830
So we looked into ways of
using 3D geometry to make

00:09:09.830 --> 00:09:11.910
face recognition more reliable.

00:09:11.910 --> 00:09:13.236
And this was 15 years ago.

00:09:13.236 --> 00:09:14.735
I was still an
undergraduate student

00:09:14.735 --> 00:09:18.140
at [INAUDIBLE] as Judy
mentioned in her introduction.

00:09:18.140 --> 00:09:21.290
And this was one of my first
two years research projects

00:09:21.290 --> 00:09:23.900
during my undergraduate
studies that I

00:09:23.900 --> 00:09:25.640
would say now, to
a large extent,

00:09:25.640 --> 00:09:28.980
has shaped my research
interest and also

00:09:28.980 --> 00:09:31.400
the future academic career.

00:09:31.400 --> 00:09:33.950
And maybe on a
more personal note,

00:09:33.950 --> 00:09:36.560
also has created a wonderful
bond and friendship

00:09:36.560 --> 00:09:39.320
with my former PhD advisor,
Professor [INAUDIBLE]

00:09:39.320 --> 00:09:40.790
from the [INAUDIBLE].

00:09:40.790 --> 00:09:44.930
Long friend and collaborator,
not only in size, but also

00:09:44.930 --> 00:09:48.510
in commercial endeavors.

00:09:48.510 --> 00:09:50.120
So in particular,
we're interested

00:09:50.120 --> 00:09:53.270
in facial expressions because
our face is non rigid.

00:09:53.270 --> 00:09:56.330
It can deform and the
deformations of the face

00:09:56.330 --> 00:09:59.150
make the life of facial
recognition systems

00:09:59.150 --> 00:10:01.700
that are based on these
measurements of distances

00:10:01.700 --> 00:10:05.010
between facial
features very hard.

00:10:05.010 --> 00:10:06.510
Talking about
distances on the face,

00:10:06.510 --> 00:10:09.227
there are several ways
of measuring distances.

00:10:09.227 --> 00:10:11.060
We can just take two
points and connect them

00:10:11.060 --> 00:10:12.000
with a straight line.

00:10:12.000 --> 00:10:15.080
This is what we call
the Euclidean distance.

00:10:15.080 --> 00:10:18.560
And as I said, such distances
can change significantly

00:10:18.560 --> 00:10:21.690
when the face
undergoes deformations

00:10:21.690 --> 00:10:23.780
due to facial expressions.

00:10:23.780 --> 00:10:26.120
Another option is to look at
the lengths of the shortest

00:10:26.120 --> 00:10:27.440
curve connecting two points.

00:10:27.440 --> 00:10:31.080
In technical jargon, this is
called the geodesic distances.

00:10:31.080 --> 00:10:34.610
And we found out that such
distances undergo much less

00:10:34.610 --> 00:10:37.640
variation, or
approximately invariant,

00:10:37.640 --> 00:10:39.340
under facial expressions.

00:10:39.340 --> 00:10:41.730
And now we can measure
geodesic distances on surfaces.

00:10:41.730 --> 00:10:44.510
There are efficient numerical
algorithms to do it.

00:10:44.510 --> 00:10:50.520
But we didn't want to use any of
landmarks such as eyes or nose

00:10:50.520 --> 00:10:51.200
tip.

00:10:51.200 --> 00:10:53.300
First, because it's hard
to reliably detect them.

00:10:53.300 --> 00:10:57.620
And second, because there are
very few stable facial points

00:10:57.620 --> 00:11:00.620
that you can detect on the
face, while many other parts

00:11:00.620 --> 00:11:03.500
of the facial surface-- like
the forehead for example--

00:11:03.500 --> 00:11:06.800
it's impossible to detect
anything meaningful there.

00:11:06.800 --> 00:11:09.920
So basically, we end
up with a collection

00:11:09.920 --> 00:11:13.200
of points, geodesic distances
measured between them.

00:11:13.200 --> 00:11:15.530
And now we need to
compare them because we

00:11:15.530 --> 00:11:18.920
want then to tell whether two
people are similar or not.

00:11:18.920 --> 00:11:21.590
So it would be nice if
we would have some way

00:11:21.590 --> 00:11:24.680
to represent these
distances in some space that

00:11:24.680 --> 00:11:26.370
is convenient to work with.

00:11:26.370 --> 00:11:29.300
For example, if we could use
a Euclidean space or, even

00:11:29.300 --> 00:11:32.620
better, two dimensional
Euclidean space-- the plane.

00:11:32.620 --> 00:11:35.290
And these kind of problems
arise in cartography.

00:11:35.290 --> 00:11:38.330
Assume that you are given
a selection of cities

00:11:38.330 --> 00:11:40.310
with distances between them.

00:11:40.310 --> 00:11:44.000
And, of course, here we're
talking about geodesic distance

00:11:44.000 --> 00:11:45.980
because when we travel
on earth, we don't

00:11:45.980 --> 00:11:49.610
dig a tunnel under the ground.

00:11:49.610 --> 00:11:53.827
We actually fly over
the spherical surface,

00:11:53.827 --> 00:11:55.160
so it cannot be a straight line.

00:11:55.160 --> 00:11:56.870
It must be a geodesic distance.

00:11:56.870 --> 00:11:59.270
For example, here
the geodesic distance

00:11:59.270 --> 00:12:04.824
between San Francisco and New
York would be 2,500 miles.

00:12:04.824 --> 00:12:06.240
So we would like
to create a plane

00:12:06.240 --> 00:12:09.530
or map of this vertical
surface of the Earth, such

00:12:09.530 --> 00:12:14.450
that the Euclidean distances
between these cities

00:12:14.450 --> 00:12:18.050
are basically represented by
Euclidean distances on the map

00:12:18.050 --> 00:12:20.570
that you can measure
with a ruler.

00:12:20.570 --> 00:12:23.210
And these new
distances are equal

00:12:23.210 --> 00:12:25.250
to the original geodesic ones.

00:12:25.250 --> 00:12:27.170
And this is called an
isometric embedding

00:12:27.170 --> 00:12:31.790
where the term "isometric" means
distance preserving in Greek.

00:12:31.790 --> 00:12:33.890
So how should we do such a map?

00:12:33.890 --> 00:12:35.500
If you ask a
mathematician, you know

00:12:35.500 --> 00:12:37.900
that mathematicians are
especially annoying type

00:12:37.900 --> 00:12:38.790
of people.

00:12:38.790 --> 00:12:40.810
And when you ask them
how to do something,

00:12:40.810 --> 00:12:42.560
they will not tell
you the answer,

00:12:42.560 --> 00:12:46.720
they will ask you whether
it's at all possible.

00:12:46.720 --> 00:12:49.480
So is it possible to
create such a map?

00:12:49.480 --> 00:12:52.960
And is it indeed possible to
find an isometric embedding

00:12:52.960 --> 00:12:55.210
of a curved surface--

00:12:55.210 --> 00:12:57.070
for example, they've
given the sphere--

00:12:57.070 --> 00:12:57.814
into the plate?

00:12:57.814 --> 00:12:59.355
And that's out of
this question which

00:12:59.355 --> 00:13:02.710
has to do with another
apparently unrelated question.

00:13:02.710 --> 00:13:06.970
Is the circle perimeter
always equal to 2 pi r?

00:13:06.970 --> 00:13:08.440
So it sounds like
a silly question.

00:13:08.440 --> 00:13:11.020
This is a formula we
all learned in school.

00:13:11.020 --> 00:13:15.040
And indeed, it has been an
undisputed fact in geometry

00:13:15.040 --> 00:13:17.320
since the ancient Greeks.

00:13:17.320 --> 00:13:21.080
And they took about 2,000 years
until Reimann and Lobachevsky

00:13:21.080 --> 00:13:23.620
formulated what we call
today non-Euclidean geometry.

00:13:23.620 --> 00:13:26.800
It's where circles can be
longer or shorter than flat ones

00:13:26.800 --> 00:13:30.610
or, maybe more famously, whether
parallel lines can intersect

00:13:30.610 --> 00:13:32.320
or not.

00:13:32.320 --> 00:13:33.820
So the degree to
which the perimeter

00:13:33.820 --> 00:13:36.850
of a circle on a surface is
different from the school

00:13:36.850 --> 00:13:40.120
formula 2pir is
called curvature.

00:13:40.120 --> 00:13:42.310
And I'm not as good
as Brian Greene

00:13:42.310 --> 00:13:45.580
in visualizing 11 dimensional
Calabi-Yau manifolds.

00:13:45.580 --> 00:13:48.790
Fortunately, we're talking here
about two dimensional manifolds

00:13:48.790 --> 00:13:50.440
and I brought some paper models.

00:13:50.440 --> 00:13:53.300
So I will show you what
curvature means on paper.

00:13:53.300 --> 00:13:55.360
So this is a flat circle, right?

00:13:55.360 --> 00:13:57.150
So basically, here
you can see it

00:13:57.150 --> 00:14:02.350
has radius, r, and the perimeter
is equal exactly to 2 pi r.

00:14:02.350 --> 00:14:06.640
Now if I wanted to create
positively curved surface,

00:14:06.640 --> 00:14:08.650
by this definition,
I need to create what

00:14:08.650 --> 00:14:10.450
is called the perimeter defect.

00:14:10.450 --> 00:14:12.370
I need to make the
perimeter shorter.

00:14:12.370 --> 00:14:16.030
And by making the perimeter
shorter, I get curvature.

00:14:16.030 --> 00:14:20.350
Luckily, around this point, the
surface looks like a sphere.

00:14:20.350 --> 00:14:21.850
So it's positively curved.

00:14:21.850 --> 00:14:24.280
If you want to make a
negatively curved surface,

00:14:24.280 --> 00:14:26.392
you need to actually
increase the perimeter

00:14:26.392 --> 00:14:28.600
and you get a surface that
looks like a [INAUDIBLE]..

00:14:28.600 --> 00:14:32.290
In geometric terms, this is
called a hyperbolic surface.

00:14:32.290 --> 00:14:33.220
So this is curvature.

00:14:33.220 --> 00:14:35.830
It's really a fundamental
concept in geometry

00:14:35.830 --> 00:14:37.120
that has many definitions.

00:14:37.120 --> 00:14:39.760
For example, you can also
consider the definition

00:14:39.760 --> 00:14:42.550
of the curvature by just
taking a piece of paper,

00:14:42.550 --> 00:14:46.600
bending it, and looking at
different directions of curves

00:14:46.600 --> 00:14:48.040
that pass for a point.

00:14:48.040 --> 00:14:50.470
And the smallest and
the largest curvature,

00:14:50.470 --> 00:14:53.320
what are called the principal
curvature, as their product

00:14:53.320 --> 00:14:57.930
defines exactly the same
notion that we've seen before.

00:14:57.930 --> 00:14:59.540
This notion of
curvature dates back

00:14:59.540 --> 00:15:02.310
to Karl Friedrich Gauss,
probably the most prominent

00:15:02.310 --> 00:15:04.369
mathematician that
has ever lived.

00:15:04.369 --> 00:15:05.910
And when he wrote
down the expression

00:15:05.910 --> 00:15:08.520
for the curvature that
bears his name today--

00:15:08.520 --> 00:15:11.550
the Gaussian curvature-- he
was surprised to find out

00:15:11.550 --> 00:15:13.620
that it can be
expressed entirely

00:15:13.620 --> 00:15:16.400
in terms of the intrinsic
structure of the surface, what

00:15:16.400 --> 00:15:17.617
we call the metric.

00:15:17.617 --> 00:15:19.950
Meaning that if we deform the
surface without stretching

00:15:19.950 --> 00:15:22.230
or tearing it, such
deformation will not

00:15:22.230 --> 00:15:24.100
change the curvature
of the surface.

00:15:24.100 --> 00:15:26.160
And again, piece of
paper is a good model.

00:15:26.160 --> 00:15:28.550
You can bend it, but
you cannot stretch it.

00:15:28.550 --> 00:15:30.900
It is not elastic.

00:15:30.900 --> 00:15:33.036
Gauss was so surprised
with his observation

00:15:33.036 --> 00:15:34.410
that he named the
result-- and he

00:15:34.410 --> 00:15:36.720
was writing in Latin--
theorema egregium

00:15:36.720 --> 00:15:38.530
or the remarkable theorem.

00:15:38.530 --> 00:15:41.640
And Gauss was not only a
great mathematician, but also

00:15:41.640 --> 00:15:42.960
a very modest man.

00:15:42.960 --> 00:15:45.030
And if he thought of
something as remarkable,

00:15:45.030 --> 00:15:46.750
it must really be.

00:15:46.750 --> 00:15:48.630
And of course, you
may think, this

00:15:48.630 --> 00:15:50.190
is just some weird
geometric idea.

00:15:50.190 --> 00:15:52.690
Why on earth should you care?

00:15:52.690 --> 00:15:54.330
Well, I believe that you should.

00:15:54.330 --> 00:15:56.610
It appears that curvature
plays a fundamental role

00:15:56.610 --> 00:15:59.010
in the modern description
of our universe

00:15:59.010 --> 00:16:03.090
Einstein's general relativity
theory models gravitation

00:16:03.090 --> 00:16:05.550
by curvature of the space-time.

00:16:05.550 --> 00:16:08.350
It is slightly more complicated
because the spaces there

00:16:08.350 --> 00:16:11.820
are higher dimensional, but
the basic idea is the same.

00:16:11.820 --> 00:16:14.940
And again, here I cannot refrain
from referring to the lecture

00:16:14.940 --> 00:16:18.900
of Professor Greene where he
masterfully showed the meaning

00:16:18.900 --> 00:16:21.580
of these concepts.

00:16:21.580 --> 00:16:24.640
But not only on
cosmic scales do you

00:16:24.640 --> 00:16:27.190
encounter the notion of
curvature and manifestation

00:16:27.190 --> 00:16:28.870
of the remarkable
theorem of Gauss,

00:16:28.870 --> 00:16:31.910
but in much more
prosaic situations when,

00:16:31.910 --> 00:16:33.040
for example, you eat pizza.

00:16:33.040 --> 00:16:34.720
So again, I have
here a paper model.

00:16:34.720 --> 00:16:37.240
And I beg the forgiveness
of my Italian wife

00:16:37.240 --> 00:16:40.770
for calling this piece
of paper a pizza.

00:16:40.770 --> 00:16:43.774
Think of it as playing
with zero curvature.

00:16:43.774 --> 00:16:45.190
So when you grab
a piece of pizza,

00:16:45.190 --> 00:16:47.530
you instinctively tend
to bend it like, right?

00:16:47.530 --> 00:16:50.590
By bending it like this,
you create here a direction

00:16:50.590 --> 00:16:51.910
in which it is curved.

00:16:51.910 --> 00:16:53.975
Now remember that the
Gaussian curvature

00:16:53.975 --> 00:16:55.850
is defined as product
of principle curvature.

00:16:55.850 --> 00:16:58.600
So it has a curvature
here that is non-zero,

00:16:58.600 --> 00:17:01.869
so you must create
a reach, a direction

00:17:01.869 --> 00:17:03.910
in which the curvature
would be zero-- basically,

00:17:03.910 --> 00:17:04.880
a straight line.

00:17:04.880 --> 00:17:07.180
And this gives rigidity
to this piece of pizza

00:17:07.180 --> 00:17:10.150
that prevents the topping
from falling and spotting

00:17:10.150 --> 00:17:12.250
your shirt or pants.

00:17:12.250 --> 00:17:14.569
And of course, this is just
a mathematical abstraction,

00:17:14.569 --> 00:17:17.435
so in practice, real pizza
might have some elasticity.

00:17:17.435 --> 00:17:19.810
So if disaster happens sometime,
don't blame it on Gauss.

00:17:23.480 --> 00:17:25.339
And of course, the
remarkable theorem

00:17:25.339 --> 00:17:28.420
had an impact in cartography
which brings us back

00:17:28.420 --> 00:17:30.220
to our original question.

00:17:30.220 --> 00:17:33.230
So now you know the
answer that the sphere

00:17:33.230 --> 00:17:36.250
has constant positive
curvature and the plane

00:17:36.250 --> 00:17:37.640
has zero curvature.

00:17:37.640 --> 00:17:41.650
So they cannot be
isometric and, even worse,

00:17:41.650 --> 00:17:45.730
by different arguments, usually
such isometric embedding v will

00:17:45.730 --> 00:17:48.700
not be possible even to
Euclidean spaces of higher

00:17:48.700 --> 00:17:50.760
dimension.

00:17:50.760 --> 00:17:53.050
So what we can
still do is to find

00:17:53.050 --> 00:17:55.372
representation that distorts
the distances the least.

00:17:55.372 --> 00:17:56.830
So it will not be
a true isometric,

00:17:56.830 --> 00:17:58.414
it will be
approximate isometric.

00:17:58.414 --> 00:18:00.580
In this way, we can get the
Euclidean representation

00:18:00.580 --> 00:18:03.940
of the face that allows to
undo the facial expressions.

00:18:03.940 --> 00:18:07.360
And this way, making the
comparison of two facial

00:18:07.360 --> 00:18:09.190
surfaces much easier.

00:18:09.190 --> 00:18:11.470
During my masters
and PhD studies,

00:18:11.470 --> 00:18:14.770
I worked on ways to efficiently
compute such minimal distortion

00:18:14.770 --> 00:18:16.850
embeddings and
their applications

00:18:16.850 --> 00:18:19.570
to problems in computer
vision and computer graphics,

00:18:19.570 --> 00:18:23.170
in particular analysis of
deformable three-dimensional

00:18:23.170 --> 00:18:25.220
objects.

00:18:25.220 --> 00:18:28.210
And we also built a
face recognition system.

00:18:28.210 --> 00:18:30.340
It was one of the first
3D face recognition

00:18:30.340 --> 00:18:33.010
systems that also
allowed to cope

00:18:33.010 --> 00:18:34.540
with facial
expressions, which is

00:18:34.540 --> 00:18:37.480
one of the difficult
problems in this domain, that

00:18:37.480 --> 00:18:40.040
was based exactly on
these geometric ideas.

00:18:40.040 --> 00:18:43.570
And here we can see a
screenshot from these systems.

00:18:43.570 --> 00:18:46.090
again, that was many years ago.

00:18:46.090 --> 00:18:47.620
That shows the
recognition process--

00:18:47.620 --> 00:18:50.740
the creation of these
four-dimensional Euclidean

00:18:50.740 --> 00:18:52.150
representation.

00:18:52.150 --> 00:18:54.910
And this, as Judy mentioned,
was joint work with my brother

00:18:54.910 --> 00:18:56.300
who is also a
computer scientist.

00:18:56.300 --> 00:18:59.410
He works in Israel and he works
exactly in the same field.

00:18:59.410 --> 00:19:02.040
We've published a lot
of papers together.

00:19:02.040 --> 00:19:06.910
And it's been a successful
collaboration already 37 years.

00:19:06.910 --> 00:19:09.220
And again, as Judy
mentioned, what

00:19:09.220 --> 00:19:11.710
makes the story especially
interesting and curious

00:19:11.710 --> 00:19:14.860
that my brother and I
are identical twins.

00:19:14.860 --> 00:19:18.460
And this is a composite
portrait of us from that period.

00:19:18.460 --> 00:19:23.130
So you can see my picture on
the left in this portrait.

00:19:23.130 --> 00:19:24.580
No actually, I'm on the right.

00:19:24.580 --> 00:19:27.040
Sorry.

00:19:27.040 --> 00:19:30.320
This is yet another proof
that we looked similar

00:19:30.320 --> 00:19:32.459
and we are still similar,
but of course there

00:19:32.459 --> 00:19:34.250
are some differences
in geometric structure

00:19:34.250 --> 00:19:35.365
of our faces.

00:19:35.365 --> 00:19:37.570
The 3D geometry
of our face is not

00:19:37.570 --> 00:19:39.500
totally under genetic control.

00:19:39.500 --> 00:19:42.610
So our system was
apparently able to detect

00:19:42.610 --> 00:19:44.950
these differences
and tell us apart.

00:19:44.950 --> 00:19:46.930
And the media
picked up this story

00:19:46.930 --> 00:19:50.170
of face recognition
system able to distinguish

00:19:50.170 --> 00:19:51.550
between identical twins.

00:19:51.550 --> 00:19:54.640
This way I got my
15 minutes of glory.

00:19:54.640 --> 00:19:57.520
Our research was featured on
CNN with quite an improbable

00:19:57.520 --> 00:20:01.480
title "Twins Rock Face
Recognition Puzzle."

00:20:01.480 --> 00:20:03.010
And of course, we
tried our system

00:20:03.010 --> 00:20:04.720
on other pairs of twins--

00:20:04.720 --> 00:20:07.629
identical twins- also
quite successfully.

00:20:07.629 --> 00:20:09.170
So this looked like
a cool technology

00:20:09.170 --> 00:20:11.020
and we tried to
commercialize it.

00:20:11.020 --> 00:20:14.090
That was a period of booms
of start-ups in Israel.

00:20:14.090 --> 00:20:16.240
In all these startup
nations, everybody

00:20:16.240 --> 00:20:18.040
was trying to make
money out of technology.

00:20:18.040 --> 00:20:21.170
So we thought why not to
try, but quickly found out,

00:20:21.170 --> 00:20:23.860
to our great disappointment,
that there was really

00:20:23.860 --> 00:20:27.110
no market for it, or at least
there was no market for it

00:20:27.110 --> 00:20:28.490
15 years ago.

00:20:28.490 --> 00:20:31.430
And one of the reasons is
being three-dimensional face

00:20:31.430 --> 00:20:34.480
recognition system requires
a three-dimensional input--

00:20:34.480 --> 00:20:36.610
a three-dimensional scanner.

00:20:36.610 --> 00:20:39.600
And 15 years ago,
if you wanted to buy

00:20:39.600 --> 00:20:41.860
three-dimensional
scanner, you would just

00:20:41.860 --> 00:20:47.110
have a handful of devices
that you could really count

00:20:47.110 --> 00:20:48.977
on the fingers of one hand.

00:20:48.977 --> 00:20:50.560
And most of them
would look like this.

00:20:50.560 --> 00:20:53.620
It's a big box of the
size weighing probably

00:20:53.620 --> 00:20:57.400
several kilograms and
it costs like a new car.

00:20:57.400 --> 00:20:59.860
So definitely a showstopper
for any serious commercial

00:20:59.860 --> 00:21:01.640
application.

00:21:01.640 --> 00:21:03.610
And in fact, the 3D
sensor at the time

00:21:03.610 --> 00:21:06.810
were so prohibitively expensive
that for our 3D facing

00:21:06.810 --> 00:21:09.790
commission system, we had
to develop our own 3D sensor

00:21:09.790 --> 00:21:12.460
and you can see it
here in this picture.

00:21:12.460 --> 00:21:14.050
It was a structured
light sensor.

00:21:14.050 --> 00:21:16.300
We used a digital
slide projector

00:21:16.300 --> 00:21:19.270
to illuminate the face with
a sequence of patterns that--

00:21:19.270 --> 00:21:21.160
captured by digital camera--

00:21:21.160 --> 00:21:25.040
allowed to infer the 3D
structure of the face.

00:21:25.040 --> 00:21:27.580
I remind you this was
15 years ago in 2002.

00:21:27.580 --> 00:21:30.190
The year when the
movie "Minority Report"

00:21:30.190 --> 00:21:33.520
hit the screens and for those
of you who have not seen it--

00:21:33.520 --> 00:21:34.900
just one second spoiler--

00:21:34.900 --> 00:21:37.720
it's a dystopian vision
of how our world might

00:21:37.720 --> 00:21:39.940
look like in 2050.

00:21:39.940 --> 00:21:41.980
And one of the most famous
scenes in this movie

00:21:41.980 --> 00:21:44.740
is where Tom Cruise
uses his hand gestures

00:21:44.740 --> 00:21:48.540
to manipulate virtual objects
on a huge holographic screen--

00:21:48.540 --> 00:21:51.970
a vision of how we
might be interacting

00:21:51.970 --> 00:21:56.620
with our future intelligence
machines in 50 years from now.

00:21:56.620 --> 00:21:59.470
And in fact, the idea of
gesture-based interfaces

00:21:59.470 --> 00:22:02.450
has been around for a while.

00:22:02.450 --> 00:22:05.560
An interface like this that
would be based only on vision

00:22:05.560 --> 00:22:07.690
requires the ability
to recognize and track

00:22:07.690 --> 00:22:09.700
different parts of
the human hand, which

00:22:09.700 --> 00:22:12.580
is a difficult problem because
our hand is a 3D object

00:22:12.580 --> 00:22:15.580
and because of two
dimensional projection,

00:22:15.580 --> 00:22:19.476
there are occlusions and
ambiguities so it's not easy.

00:22:19.476 --> 00:22:21.350
Apparently, the creators
of "Minority Report"

00:22:21.350 --> 00:22:24.370
were so skeptical about the
ability of computer vision

00:22:24.370 --> 00:22:27.070
researchers to solve
this hand tracking

00:22:27.070 --> 00:22:30.510
problem 50 years from now
in the very distant future

00:22:30.510 --> 00:22:33.430
that they equipped Tom Cruise
with these light gloves

00:22:33.430 --> 00:22:36.190
that you can see in
this movie scene where--

00:22:36.190 --> 00:22:38.440
which, of course,
makes the life of hand

00:22:38.440 --> 00:22:42.100
tracking much easier than
tracking simply bare hands.

00:22:42.100 --> 00:22:43.990
So at least this part
of "Minority Report"

00:22:43.990 --> 00:22:47.060
remains science fiction for
a very brief moment of time.

00:22:47.060 --> 00:22:50.230
I hope the rest remains
science fiction forever.

00:22:50.230 --> 00:22:52.510
In the matter of a
few years, Microsoft

00:22:52.510 --> 00:22:55.120
released the highly
successful Kinect product.

00:22:55.120 --> 00:22:59.230
It was a plug-in for their
Xbox gaming consoles that

00:22:59.230 --> 00:23:03.340
allowed users to control
their games with bare hands

00:23:03.340 --> 00:23:04.370
and gestures.

00:23:04.370 --> 00:23:05.620
No gloves were required.

00:23:05.620 --> 00:23:07.570
So it was even better
than envisioned

00:23:07.570 --> 00:23:09.010
in "Minority Report."

00:23:09.010 --> 00:23:10.960
And this ability
was made possible

00:23:10.960 --> 00:23:14.050
by a novel 3D sensor
that was developed

00:23:14.050 --> 00:23:16.840
by an Israeli company
called PrimeSense that

00:23:16.840 --> 00:23:21.580
projected invisible infrared
light pattern on the objects

00:23:21.580 --> 00:23:24.190
in front of it, and by simple
triangulation techniques

00:23:24.190 --> 00:23:25.960
recovered the geometric
structure that was

00:23:25.960 --> 00:23:28.090
accurate to a few millimeters.

00:23:28.090 --> 00:23:30.670
And this gave a lot
more useful information

00:23:30.670 --> 00:23:33.130
than standard two
dimensional image

00:23:33.130 --> 00:23:37.630
because in 3D, the hand tracking
becomes much easier to solve

00:23:37.630 --> 00:23:39.040
many of the ambiguities.

00:23:39.040 --> 00:23:41.080
Microsoft, using
this 3D information,

00:23:41.080 --> 00:23:44.560
we're able to use simple
computer vision and machine

00:23:44.560 --> 00:23:48.640
learning techniques to do
efficient hand tracking.

00:23:48.640 --> 00:23:51.760
So Kinect was by all means
a revolutionary product,

00:23:51.760 --> 00:23:53.530
the first of its kind.

00:23:53.530 --> 00:23:55.690
And at that point,
we again returned

00:23:55.690 --> 00:23:59.380
to our old ideas of
trying to make the 3D face

00:23:59.380 --> 00:24:01.480
recognition into a
commercial product,

00:24:01.480 --> 00:24:06.190
but it became very quickly
apparent that the sensor itself

00:24:06.190 --> 00:24:08.860
is much more interesting than
the particular application

00:24:08.860 --> 00:24:10.660
of 3D face recognition.

00:24:10.660 --> 00:24:12.880
The investors were
betting big time

00:24:12.880 --> 00:24:15.220
on the future of 3D sensors.

00:24:15.220 --> 00:24:17.800
Kinect, however
revolutionary it was,

00:24:17.800 --> 00:24:18.970
was still large and clunky.

00:24:18.970 --> 00:24:23.110
It required an external power
supply so you cannot simply put

00:24:23.110 --> 00:24:27.620
such a device into a
mobile laptop, tablet,

00:24:27.620 --> 00:24:29.020
or even cellular phone.

00:24:29.020 --> 00:24:30.640
And we thought
that we could make

00:24:30.640 --> 00:24:32.650
a much more cheaper device.

00:24:32.650 --> 00:24:35.830
So it became a startup called
Invision based in Israel.

00:24:35.830 --> 00:24:40.014
And in 2012, we were
acquired by Intel.

00:24:40.014 --> 00:24:41.680
And somehow surprisingly,
even though it

00:24:41.680 --> 00:24:44.230
was pretty dramatic
moment in life for me

00:24:44.230 --> 00:24:46.240
and for my colleagues,
I don't have

00:24:46.240 --> 00:24:47.530
any pictures from that moment.

00:24:47.530 --> 00:24:51.430
Just this picture where the logo
of Invision on the office door

00:24:51.430 --> 00:24:53.590
is replaced by the
logo for Intel.

00:24:53.590 --> 00:24:56.530
And of course, it took
another several years

00:24:56.530 --> 00:24:59.110
and the huge effort of
many teams inside Intel

00:24:59.110 --> 00:25:01.000
before it became a
commercial product.

00:25:01.000 --> 00:25:03.100
Actually, the first 3D
camera that could be mass

00:25:03.100 --> 00:25:05.170
manufactured into
the form factor

00:25:05.170 --> 00:25:08.230
that would allow to fit
it into mobile devices--

00:25:08.230 --> 00:25:10.360
our technology got
the name Real Sense

00:25:10.360 --> 00:25:12.880
and it was presented
by the Intel CEO

00:25:12.880 --> 00:25:15.400
at the Consumer Electronics
Show in Las Vegas

00:25:15.400 --> 00:25:17.470
in the beginning of 2015.

00:25:17.470 --> 00:25:20.740
So I have here one of the
first samples of the camera

00:25:20.740 --> 00:25:21.820
from a few years back.

00:25:21.820 --> 00:25:25.390
So you can see that it's
a tiny, thin device that

00:25:25.390 --> 00:25:28.840
easily goes into
[INAUDIBLE] left of display.

00:25:28.840 --> 00:25:30.460
And for the launch
of the product,

00:25:30.460 --> 00:25:32.140
Intel shot this
funny commercial that

00:25:32.140 --> 00:25:34.510
was aired on Christmas, 2014.

00:25:34.510 --> 00:25:37.720
For those of you who are fans
of the "Big Bang Theory,"

00:25:37.720 --> 00:25:39.590
you can recognize the character.

00:25:43.090 --> 00:25:45.400
- What's that?

00:25:45.400 --> 00:25:48.160
Wow!

00:25:48.160 --> 00:25:49.185
Hello, humans.

00:25:52.810 --> 00:25:54.010
Focus on me.

00:25:54.010 --> 00:25:56.869
- Focus on the guard.

00:25:56.869 --> 00:25:58.660
Don't tell anyone what
you've seen in here.

00:26:01.660 --> 00:26:03.400
- Have you seen what's in there?

00:26:03.400 --> 00:26:04.510
- They have Intel.

00:26:04.510 --> 00:26:05.960
This is where it all changes.

00:26:08.819 --> 00:26:11.110
- So first of all, the Intel
lab doesn't look like this

00:26:11.110 --> 00:26:12.340
obviously.

00:26:12.340 --> 00:26:14.440
And here it says coming soon.

00:26:14.440 --> 00:26:16.570
Of course now it is
a successful product

00:26:16.570 --> 00:26:18.550
with several generations
and multiple millions

00:26:18.550 --> 00:26:21.470
of units shipped worldwide.

00:26:21.470 --> 00:26:24.760
And so if Kinect made
3D sensing affordable,

00:26:24.760 --> 00:26:26.890
RealSense allowed to
make it ubiquitous,

00:26:26.890 --> 00:26:29.980
and today, you can have a
3D sensor in your laptop

00:26:29.980 --> 00:26:31.090
or tablet.

00:26:31.090 --> 00:26:33.970
And this opens the door to
many interesting and important

00:26:33.970 --> 00:26:35.030
applications.

00:26:35.030 --> 00:26:37.720
Let me show just one example
of an application developed

00:26:37.720 --> 00:26:40.990
by a Swiss company
called FaceShift.

00:26:40.990 --> 00:26:42.610
- He name's Mandy.

00:26:42.610 --> 00:26:46.640
The profile says 6'4",
good with an axe,

00:26:46.640 --> 00:26:49.620
and likes the music
of Lana Del Rey.

00:26:49.620 --> 00:26:55.049
Sultry, sexy, and when
necessary, extremely violent.

00:26:55.049 --> 00:26:57.340
- So what you see here is a
computer graphics character

00:26:57.340 --> 00:27:00.970
that is animated in real time
by a human actor that performs

00:27:00.970 --> 00:27:02.740
in front of a 3D sensor.

00:27:02.740 --> 00:27:05.530
And this kind of technology
is called motion capture.

00:27:05.530 --> 00:27:08.290
Nowadays, it's widely-used
by Hollywood studios

00:27:08.290 --> 00:27:11.010
to produce special effects
in movies like "Avatar."

00:27:11.010 --> 00:27:15.310
But a typical motion capture
setup as big as this room

00:27:15.310 --> 00:27:18.010
would cost millions of
dollars and requires to places

00:27:18.010 --> 00:27:20.110
special markers on the
face and other body

00:27:20.110 --> 00:27:23.290
parts of the actor,
while FaceShift did it

00:27:23.290 --> 00:27:24.400
without any markers.

00:27:24.400 --> 00:27:29.530
They marketed it as
markerless motion capture

00:27:29.530 --> 00:27:31.390
and could do
practically the same

00:27:31.390 --> 00:27:35.470
with just a small 3D sensor
that costs maybe $20 or $30.

00:27:35.470 --> 00:27:37.630
So it was really remarkable.

00:27:37.630 --> 00:27:40.392
FaceShift was acquired
by Apple two years ago.

00:27:40.392 --> 00:27:42.600
And if you've seen the
presentation of the new iPhone

00:27:42.600 --> 00:27:45.240
10 a few weeks ago,
Apple has really taken it

00:27:45.240 --> 00:27:47.160
to completely next level.

00:27:47.160 --> 00:27:50.650
There are emoji-animated
emoticons are, presumably

00:27:50.650 --> 00:27:53.680
at least, powered by the
FaceShift technology.

00:27:53.680 --> 00:27:56.770
And the piece of hardware
that enables this capability

00:27:56.770 --> 00:27:58.750
is a tiny 3D sensor
that is built

00:27:58.750 --> 00:28:02.050
into the phone, what Apple
calls the TrueDepth camera.

00:28:02.050 --> 00:28:05.710
And again, Apple also acquired
PrimeSense that provided that

00:28:05.710 --> 00:28:09.470
technology for the first version
of Microsoft Kinect and again,

00:28:09.470 --> 00:28:12.430
presumably-- nobody really knows
except for people that work

00:28:12.430 --> 00:28:15.550
at Apple that build these
products what exactly they do--

00:28:15.550 --> 00:28:19.270
but presumably, that's a
very miniaturized version

00:28:19.270 --> 00:28:22.630
of the same technology
that powered Kinect.

00:28:22.630 --> 00:28:26.890
And you could also notice that
the new iPhone 10 abandoned

00:28:26.890 --> 00:28:30.250
the fingerprint scanner
and instead they

00:28:30.250 --> 00:28:33.190
do face recognition in
order to unlock the phone.

00:28:33.190 --> 00:28:36.430
So interestingly, and maybe
a little bit ironically,

00:28:36.430 --> 00:28:38.920
brings us back to the
good old face recognition

00:28:38.920 --> 00:28:41.260
problem from which we started.

00:28:41.260 --> 00:28:43.960
And now a lot of
things have changed.

00:28:43.960 --> 00:28:46.927
Until the past five years,
the predominant way face

00:28:46.927 --> 00:28:49.510
recognition-- but in general, I
would say most of the problems

00:28:49.510 --> 00:28:50.680
in computer vision--

00:28:50.680 --> 00:28:54.610
were approached was by designing
so-called feature descriptors--

00:28:54.610 --> 00:28:56.890
a smart and carefully
engineered way

00:28:56.890 --> 00:28:59.350
of capturing the local
content of an image.

00:28:59.350 --> 00:29:02.530
For example, computing
histograms of local derivatives

00:29:02.530 --> 00:29:04.120
or image gradients.

00:29:04.120 --> 00:29:07.309
And some of these descriptors
had become extremely popular,

00:29:07.309 --> 00:29:09.600
for example the paper describing
the famous [INAUDIBLE]

00:29:09.600 --> 00:29:12.220
descriptor is arguably
one of the most

00:29:12.220 --> 00:29:14.440
cited papers, not only
in computer vision,

00:29:14.440 --> 00:29:16.540
but in the field
of computer science

00:29:16.540 --> 00:29:18.370
with over 40,000 citations.

00:29:18.370 --> 00:29:21.650
I checked Google Scholar
just a few days ago.

00:29:21.650 --> 00:29:25.240
So the problem though with
such hand-crafted features

00:29:25.240 --> 00:29:29.890
is that it's very hard to
decide a priori what constitutes

00:29:29.890 --> 00:29:33.130
meaningful information and
what is the result of noise

00:29:33.130 --> 00:29:37.090
or external factors such
as different illumination.

00:29:37.090 --> 00:29:40.660
A paradigm shift that happened
recently in computer vision is

00:29:40.660 --> 00:29:43.820
instead of trying to engineer
a local descriptor basically

00:29:43.820 --> 00:29:46.810
hand-crafted by determining
what makes a good feature,

00:29:46.810 --> 00:29:49.570
let's assume that we have a
template that we can pass over

00:29:49.570 --> 00:29:53.290
the image and detect if it
correlates well with image

00:29:53.290 --> 00:29:55.750
content depending on the
application for example,

00:29:55.750 --> 00:29:57.550
if we're looking for--

00:29:57.550 --> 00:29:59.310
human eyes or etc.

00:29:59.310 --> 00:30:01.820
And we can use many such
templates, basically

00:30:01.820 --> 00:30:05.600
a bank of filters, which
are combined together

00:30:05.600 --> 00:30:07.477
through some nonlinear function.

00:30:07.477 --> 00:30:09.310
And we can repeat this
process several times

00:30:09.310 --> 00:30:12.970
applying filters to the output
of the previous layer again

00:30:12.970 --> 00:30:13.750
and again.

00:30:13.750 --> 00:30:16.690
And the key here is that you can
use these filters more or less

00:30:16.690 --> 00:30:17.800
the way you want.

00:30:17.800 --> 00:30:19.600
Basically, there is
no prior assumption

00:30:19.600 --> 00:30:21.460
about how they look like.

00:30:21.460 --> 00:30:24.280
The only thing that you care
is to select these filters

00:30:24.280 --> 00:30:26.920
in a way that will
make your final goal,

00:30:26.920 --> 00:30:28.360
your task work the best.

00:30:28.360 --> 00:30:31.690
In this case, for example,
tell apart different people.

00:30:31.690 --> 00:30:35.260
And it turns out
that in applications

00:30:35.260 --> 00:30:38.440
like this in image analysis and
face recognition, the filters

00:30:38.440 --> 00:30:42.310
that will be learned in the
first layers of the system

00:30:42.310 --> 00:30:44.290
will be very simple
visual features,

00:30:44.290 --> 00:30:46.340
oriented edges, corners.

00:30:46.340 --> 00:30:48.010
Features in the
second layer will

00:30:48.010 --> 00:30:51.310
capture more complicated
structures such as eyes

00:30:51.310 --> 00:30:54.310
or noses, and in
the very deep layers

00:30:54.310 --> 00:30:56.440
you will find units
that may be activated

00:30:56.440 --> 00:31:00.910
by just that specific face
of that specific person.

00:31:00.910 --> 00:31:02.650
You know that among
neuroscientists there

00:31:02.650 --> 00:31:05.000
is a joke about the
grandmother neuron--

00:31:05.000 --> 00:31:07.780
a hypothetical,
imaginary brain cell that

00:31:07.780 --> 00:31:12.779
is activated when you see just
the face of your grandmother.

00:31:12.779 --> 00:31:14.320
And the reference
to neurons in brain

00:31:14.320 --> 00:31:17.470
here is not the case because
such systems are called

00:31:17.470 --> 00:31:19.310
artificial neural networks.

00:31:19.310 --> 00:31:21.700
And this particular
architecture that I described

00:31:21.700 --> 00:31:23.680
is a convolutional
neural network

00:31:23.680 --> 00:31:25.870
and they originated
as simplified

00:31:25.870 --> 00:31:29.500
mathematical descriptions
of the human brain.

00:31:29.500 --> 00:31:34.060
And the most important advantage
of artificial neural networks

00:31:34.060 --> 00:31:36.580
is that they can
learn from data.

00:31:36.580 --> 00:31:40.030
And roughly speaking, if we
cannot build an axiomatic model

00:31:40.030 --> 00:31:42.970
of what makes a face
look like a face,

00:31:42.970 --> 00:31:45.730
we can get many examples of
faces and, in particular,

00:31:45.730 --> 00:31:49.510
faces of similar people
versus different people,

00:31:49.510 --> 00:31:54.010
and we can train a neural
network to tell them apart.

00:31:54.010 --> 00:31:56.590
in fact, artificial neural
networks are not new at all.

00:31:56.590 --> 00:31:58.390
The first works in
this domain date back

00:31:58.390 --> 00:32:01.210
to the '50s and the '60s
of the past century,

00:32:01.210 --> 00:32:03.730
mainly by neuroscientists
attempting to model

00:32:03.730 --> 00:32:06.145
how the human brain learns.

00:32:06.145 --> 00:32:08.270
And in particular, the
convolutional neural network

00:32:08.270 --> 00:32:12.310
that I showed in my example
was invented in the '80s

00:32:12.310 --> 00:32:17.140
by Yann LeCun who is currently
the director of AI research

00:32:17.140 --> 00:32:18.490
at Facebook.

00:32:18.490 --> 00:32:20.470
But only recently the
computational power

00:32:20.470 --> 00:32:23.530
of modern processors has
become sufficient to make

00:32:23.530 --> 00:32:27.430
it possible to design very deep
and complex neural networks

00:32:27.430 --> 00:32:32.260
that can adequately model the
complexity of visual objects.

00:32:32.260 --> 00:32:35.560
So only recently, the
computational power

00:32:35.560 --> 00:32:39.670
of modern processors have
become sufficient to make

00:32:39.670 --> 00:32:42.940
sufficiently complex
models that can adequately

00:32:42.940 --> 00:32:46.420
cope with the task of
visual object recognition.

00:32:46.420 --> 00:32:50.500
We also have efficient methods
to train such networks and we

00:32:50.500 --> 00:32:52.730
also-- probably that's
the most important part--

00:32:52.730 --> 00:32:54.250
we also have very
large collections

00:32:54.250 --> 00:32:57.040
of images on which these
networks can train.

00:32:57.040 --> 00:32:59.300
And neural networks
in these settings

00:32:59.300 --> 00:33:01.090
what is called
supervised learning

00:33:01.090 --> 00:33:02.630
are very hungry for data.

00:33:02.630 --> 00:33:05.200
And in order to learn
to recognize images,

00:33:05.200 --> 00:33:09.301
they literally require millions
of examples to learn from.

00:33:09.301 --> 00:33:11.050
So the convergence of
these three trends--

00:33:11.050 --> 00:33:14.650
the computational power,
the efficient architectures

00:33:14.650 --> 00:33:20.150
and methods, and also the
availability of big data,

00:33:20.150 --> 00:33:24.520
and last but not least, the
persistence of a few research

00:33:24.520 --> 00:33:27.280
teams in the world that
continued working on these

00:33:27.280 --> 00:33:30.970
methods for the dark ages of
AI where neural networks became

00:33:30.970 --> 00:33:33.850
almost a bad word and
extremely unpopular--

00:33:33.850 --> 00:33:37.540
so people willing to risk
their academic career to push

00:33:37.540 --> 00:33:38.980
for these ideas--

00:33:38.980 --> 00:33:42.400
all has led to the
breakthrough or a household

00:33:42.400 --> 00:33:45.070
name, a phenomenon that we
called today deep learning.

00:33:45.070 --> 00:33:47.380
And I'm sure that you have
heard about deep learning

00:33:47.380 --> 00:33:49.070
in the past couple of years.

00:33:49.070 --> 00:33:51.880
Even if not, it is
very likely that you

00:33:51.880 --> 00:33:55.150
are deep neural networks even
without being aware of it.

00:33:55.150 --> 00:33:58.630
For example, the Siri voice
recognition in your iPhone

00:33:58.630 --> 00:34:01.420
or if you have a Tesla car,
the autopilot feature--

00:34:01.420 --> 00:34:04.094
they all are based
on deep learning.

00:34:04.094 --> 00:34:06.010
In particular in the
field of computer vision,

00:34:06.010 --> 00:34:09.880
deep learning had a
groundbreaking effect

00:34:09.880 --> 00:34:13.510
amounting to a little
revolution that nearly wiped out

00:34:13.510 --> 00:34:17.230
previous approaches in a
matter of just five years.

00:34:17.230 --> 00:34:22.929
And I must admit that somehow
I have pity for the fact

00:34:22.929 --> 00:34:25.670
that what turns out
to work the best

00:34:25.670 --> 00:34:27.460
are brute force
methods and black box,

00:34:27.460 --> 00:34:28.876
because we don't
really understand

00:34:28.876 --> 00:34:32.469
how these neural networks
work and why they work.

00:34:32.469 --> 00:34:35.060
These are now the best methods.

00:34:35.060 --> 00:34:38.397
And especially when dealing
with geometry with 3D objects,

00:34:38.397 --> 00:34:40.480
we have a lot of understanding
how geometry works.

00:34:40.480 --> 00:34:43.360
We have very beautiful
differential geometric models,

00:34:43.360 --> 00:34:46.150
and throwing these models
away is really a pity.

00:34:46.150 --> 00:34:47.560
So the question
is can we combine

00:34:47.560 --> 00:34:49.239
the best of these two worlds?

00:34:49.239 --> 00:34:52.570
Can we treat the data correctly
from the geometric perspective,

00:34:52.570 --> 00:34:56.560
but also take advantage
of the powerful tools

00:34:56.560 --> 00:34:59.290
available today in the
field of machine learning?

00:34:59.290 --> 00:35:01.600
Or can we build
some prior knowledge

00:35:01.600 --> 00:35:04.120
or reasonable geometric
model of our data

00:35:04.120 --> 00:35:06.070
into the neural
network architecture

00:35:06.070 --> 00:35:09.790
and learn only what doesn't
fit well into this model?

00:35:09.790 --> 00:35:11.770
This is what we call
geometric deep learning.

00:35:11.770 --> 00:35:13.270
It's a new trend
in machine learning

00:35:13.270 --> 00:35:16.660
that is followed by
several research groups.

00:35:16.660 --> 00:35:18.720
Basically, it's an
attempt to generalize

00:35:18.720 --> 00:35:21.160
successful deep
learning methods to data

00:35:21.160 --> 00:35:25.720
that has non-euclidean
underlying structure.

00:35:25.720 --> 00:35:27.940
One of the main
challenges in generalizing

00:35:27.940 --> 00:35:30.130
deep neural networks to
geometric non-Euclidean

00:35:30.130 --> 00:35:34.330
structures is that we need
to reinvent all the building

00:35:34.330 --> 00:35:36.470
blocks of neural networks.

00:35:36.470 --> 00:35:38.410
For example, take the
convolution operation

00:35:38.410 --> 00:35:40.630
that is used in convolutional
neural networks.

00:35:40.630 --> 00:35:43.220
In an image, we can simply slide
a template across the image,

00:35:43.220 --> 00:35:43.720
right?

00:35:43.720 --> 00:35:45.790
The way it works is you
take a block of pixels,

00:35:45.790 --> 00:35:48.380
you multiply it by template,
you sum up the results

00:35:48.380 --> 00:35:51.650
and that's the output
of the convolution.

00:35:51.650 --> 00:35:54.840
Then you move your
template to a new location.

00:35:54.840 --> 00:35:56.864
So on the surface,
because of the curvature,

00:35:56.864 --> 00:35:58.280
the local structure
of the surface

00:35:58.280 --> 00:36:01.310
will differ at different
points as you can see here.

00:36:01.310 --> 00:36:07.130
So this notion of page changes
throughout the surface,

00:36:07.130 --> 00:36:09.860
or in technical terms, we don't
have shift invariance which

00:36:09.860 --> 00:36:12.920
determines a lot of important
characteristics of convolution

00:36:12.920 --> 00:36:15.150
in the Euclidean setting.

00:36:15.150 --> 00:36:16.920
So there are several
ways of defining

00:36:16.920 --> 00:36:18.920
filtering operations that
look like convolutions

00:36:18.920 --> 00:36:23.546
on non-Euclidean spaces.

00:36:23.546 --> 00:36:24.920
The one I would
like to show here

00:36:24.920 --> 00:36:28.880
is based on modeling heat
propagation on surfaces.

00:36:28.880 --> 00:36:31.100
The first systematic
study of heat

00:36:31.100 --> 00:36:34.630
was done by Joseph Fourier,
French mathematician, actually

00:36:34.630 --> 00:36:36.590
a contemporary of Gauss.

00:36:36.590 --> 00:36:38.910
And besides being a
talented mathematician,

00:36:38.910 --> 00:36:42.100
he was also a gifted
administrator and this cost

00:36:42.100 --> 00:36:43.970
Fourier a part of
his scientific career

00:36:43.970 --> 00:36:47.030
as he received from Napoleon
an offer that he could not

00:36:47.030 --> 00:36:47.660
refuse--

00:36:47.660 --> 00:36:50.870
an appointment as the prefect
of the city of Grenoble.

00:36:50.870 --> 00:36:53.690
And in his work "Analytic
Theory of Heat,"

00:36:53.690 --> 00:36:56.240
he formulated some of the
very basic equations that

00:36:56.240 --> 00:36:59.990
are taught today in all
mathematical physics

00:36:59.990 --> 00:37:01.730
courses throughout the world.

00:37:01.730 --> 00:37:03.350
And also introduce
the basics of what

00:37:03.350 --> 00:37:05.600
we call today free analysis.

00:37:05.600 --> 00:37:09.590
So his book was published
almost 200 years ago,

00:37:09.590 --> 00:37:12.712
so we are using
pretty old stuff here.

00:37:12.712 --> 00:37:14.420
This is the partial
differential equation

00:37:14.420 --> 00:37:16.800
that governs the heat
diffusion on surfaces.

00:37:16.800 --> 00:37:19.674
It's exactly the same equation
Fourier derived in his work.

00:37:19.674 --> 00:37:22.340
The only difference here is that
the operators on the right hand

00:37:22.340 --> 00:37:25.520
side are defined
intrinsically on the surface.

00:37:25.520 --> 00:37:28.730
We call this equation isotropic
because the speed of the heat

00:37:28.730 --> 00:37:30.590
propagation on the
surface here is

00:37:30.590 --> 00:37:33.350
assumed to be constant and
uniform in every direction

00:37:33.350 --> 00:37:34.800
in every point.

00:37:34.800 --> 00:37:38.270
So the way of thinking of it is
that you pierce this poor horse

00:37:38.270 --> 00:37:41.900
with an infinitely hot
needle at the point,

00:37:41.900 --> 00:37:45.290
and then you allow as
time goes for the heat

00:37:45.290 --> 00:37:47.750
to spread throughout
the entire surface.

00:37:47.750 --> 00:37:49.520
And basically, the
heat propagation

00:37:49.520 --> 00:37:51.290
follows the structure
of the surface.

00:37:51.290 --> 00:37:53.750
In particular, it
depends on the curvature.

00:37:53.750 --> 00:37:55.400
And the great thing
about this equation

00:37:55.400 --> 00:37:57.980
is that it is intrinsic
so the result will not

00:37:57.980 --> 00:38:01.310
change no matter how we
deform this horse shape as

00:38:01.310 --> 00:38:05.070
long as the deformations remain
isometric, or in other words,

00:38:05.070 --> 00:38:09.500
they don't stretch or they
don't tear the surface.

00:38:09.500 --> 00:38:12.650
So far, we assumed that the heat
flows equally fast everywhere.

00:38:12.650 --> 00:38:15.410
We can consider a more
complicated and isotropic heat

00:38:15.410 --> 00:38:18.500
diffusion where the heat
propagation speed depends

00:38:18.500 --> 00:38:20.090
on position and direction.

00:38:20.090 --> 00:38:22.310
It is modeled if you're
interested in math,

00:38:22.310 --> 00:38:25.620
by this matrix A in the right
hand side of the equation.

00:38:25.620 --> 00:38:28.340
And in this example, you can
see that the heat flows faster

00:38:28.340 --> 00:38:30.950
in the horizontal direction.

00:38:30.950 --> 00:38:33.620
Actually the term horizontal
here is totally appropriate

00:38:33.620 --> 00:38:37.190
because the diffusion process
is intrinsic and defined

00:38:37.190 --> 00:38:38.480
on the surface itself.

00:38:38.480 --> 00:38:41.480
So on the surface, you can talk
only about local directions

00:38:41.480 --> 00:38:45.172
with respect to some local
system of coordinates.

00:38:45.172 --> 00:38:46.880
we can design an
isotropic heat diffusion

00:38:46.880 --> 00:38:49.550
that flows in a
prescribed direction,

00:38:49.550 --> 00:38:52.560
and controlling the direction of
the diffusion and the diffusion

00:38:52.560 --> 00:38:53.150
time--

00:38:53.150 --> 00:38:55.370
the size of this heat
blob on the surface--

00:38:55.370 --> 00:38:58.580
we can construct the locals
system of coordinates that

00:38:58.580 --> 00:39:00.380
is intrinsic by definition.

00:39:00.380 --> 00:39:03.320
And these coordinates can be
used to define a local page

00:39:03.320 --> 00:39:05.547
and using which you
can define operations

00:39:05.547 --> 00:39:07.130
similar to the
template matching we've

00:39:07.130 --> 00:39:10.035
seen in convolutional
neural networks.

00:39:10.035 --> 00:39:11.660
We used these and
similar constructions

00:39:11.660 --> 00:39:14.780
to develop the first deep neural
networks in manifolds where

00:39:14.780 --> 00:39:16.400
the architecture
counts correctly

00:39:16.400 --> 00:39:18.960
for the structure of the
underlying geometric object

00:39:18.960 --> 00:39:21.644
and has a built-in
invariance to deformation.

00:39:21.644 --> 00:39:23.060
And this is
particularly important

00:39:23.060 --> 00:39:26.354
when dealing with deformable
3D shapes like our body.

00:39:26.354 --> 00:39:27.770
The deformations
of the human body

00:39:27.770 --> 00:39:31.540
can be approximated
reasonably well as isometries,

00:39:31.540 --> 00:39:33.320
but where they deviate
from this model,

00:39:33.320 --> 00:39:35.220
we can compensate by learning.

00:39:35.220 --> 00:39:38.240
So in other words, we just
learn what cannot be modeled

00:39:38.240 --> 00:39:43.190
axiomatically and as a result,
end up with way simpler network

00:39:43.190 --> 00:39:45.110
that requires
orders of magnitude,

00:39:45.110 --> 00:39:48.620
less parameters than traditional
Euclidean architectures,

00:39:48.620 --> 00:39:50.750
and also require much
less data to train.

00:39:50.750 --> 00:39:54.679
So it all reduces to
much simpler things.

00:39:54.679 --> 00:39:56.970
When we applied intrinsic
convolutional neural networks

00:39:56.970 --> 00:39:59.580
to learn correspondence
between deformable shapes,

00:39:59.580 --> 00:40:02.060
this is one of the
cornerstone problems

00:40:02.060 --> 00:40:04.880
in many applications in
computer vision and graphics.

00:40:04.880 --> 00:40:08.227
For example, it's a crucial
step in the markerless motion

00:40:08.227 --> 00:40:09.560
capture that I mentioned before.

00:40:09.560 --> 00:40:11.930
Here, you can see a few
examples of correspondence

00:40:11.930 --> 00:40:15.620
between human shapes visualized
by color texture, independent

00:40:15.620 --> 00:40:18.190
of the poles and
individual characteristics

00:40:18.190 --> 00:40:20.224
of the human body.

00:40:20.224 --> 00:40:21.920
Geometric deep
learning currently

00:40:21.920 --> 00:40:25.160
holds the record of the best
quality 3D correspondence

00:40:25.160 --> 00:40:28.130
on several standard benchmarks.

00:40:28.130 --> 00:40:30.280
If so far we are
talking about surfaces

00:40:30.280 --> 00:40:32.012
as examples of
geometric objects,

00:40:32.012 --> 00:40:33.470
there are other
structures that can

00:40:33.470 --> 00:40:35.660
be approached by geometric
deep learning methods,

00:40:35.660 --> 00:40:37.094
such as graphs and networks.

00:40:37.094 --> 00:40:38.510
And in the remaining
time, I would

00:40:38.510 --> 00:40:41.330
like to show a few results,
and maybe, mostly ideas

00:40:41.330 --> 00:40:45.289
and promising research
directions in that regard.

00:40:45.289 --> 00:40:47.330
In social networks are
probably the first example

00:40:47.330 --> 00:40:49.580
that comes to my mind.

00:40:49.580 --> 00:40:51.440
Mathematically
speaking, it's a graph

00:40:51.440 --> 00:40:55.820
where the nodes or the vertices
represent users and the edges

00:40:55.820 --> 00:40:57.980
represent the social relations.

00:40:57.980 --> 00:41:00.440
So you mentioned that we are
given a social graph, say

00:41:00.440 --> 00:41:02.630
from Facebook, and
we have information

00:41:02.630 --> 00:41:05.840
about the users such as
their gender, age, education,

00:41:05.840 --> 00:41:08.340
background, religion, whatever.

00:41:08.340 --> 00:41:09.890
And assume that
for some users, we

00:41:09.890 --> 00:41:12.924
know how they voted in
the recent elections.

00:41:12.924 --> 00:41:14.840
So geometric deep learning
on the social graph

00:41:14.840 --> 00:41:18.950
can use this data to predict
whether a particular user voted

00:41:18.950 --> 00:41:21.620
for Clinton or for Trump
from the information

00:41:21.620 --> 00:41:24.080
of his friends and
their voting patterns

00:41:24.080 --> 00:41:25.440
and the structure of the graph.

00:41:25.440 --> 00:41:27.800
So in a sense, the
saying, tell me

00:41:27.800 --> 00:41:30.890
who your friends are and I will
tell you who you are has never

00:41:30.890 --> 00:41:33.005
been more true.

00:41:33.005 --> 00:41:34.130
Here's another application.

00:41:34.130 --> 00:41:37.880
I'm sure that many of you
watch movies on Netflix.

00:41:37.880 --> 00:41:40.310
An important problem
for Netflix is

00:41:40.310 --> 00:41:43.580
how to recommend new movies to
users predicting whether they

00:41:43.580 --> 00:41:45.330
might like them or not.

00:41:45.330 --> 00:41:47.150
And this is called
recommendation system

00:41:47.150 --> 00:41:50.630
and you can imagine what kind
of commercial value or impact

00:41:50.630 --> 00:41:54.860
it might have for online
retail platforms such as Amazon

00:41:54.860 --> 00:41:56.230
or Google.

00:41:56.230 --> 00:41:59.300
10 years ago, Netflix even
announced the competition

00:41:59.300 --> 00:42:03.260
bearing a $1 million prize for
a search team that would improve

00:42:03.260 --> 00:42:05.340
on their prediction results.

00:42:05.340 --> 00:42:08.420
So assume that users can give
scores to movies on a scale

00:42:08.420 --> 00:42:10.070
from 1 to 10.

00:42:10.070 --> 00:42:14.000
Mathematically, we can think
of them as a huge matrix.

00:42:14.000 --> 00:42:17.480
Netflix has tens of millions
of users and hundreds

00:42:17.480 --> 00:42:19.460
of thousands of movie titles.

00:42:19.460 --> 00:42:22.700
And the matrix is only
very, very sparsely sampled

00:42:22.700 --> 00:42:25.340
because a single person
can see that many movies

00:42:25.340 --> 00:42:27.530
in his or her lifetime.

00:42:27.530 --> 00:42:29.570
So the recommendation
problem consists

00:42:29.570 --> 00:42:34.280
of predicting and filling in the
missing values of this matrix.

00:42:34.280 --> 00:42:36.710
Standard approaches use
algebraic techniques

00:42:36.710 --> 00:42:39.140
trying to find some low
dimensional model that

00:42:39.140 --> 00:42:42.230
explains the data,
basically by minimizing

00:42:42.230 --> 00:42:44.750
what is called matrix
rank, or more correctly,

00:42:44.750 --> 00:42:49.540
a convex proxy of the rank,
usually the [INAUDIBLE]..

00:42:49.540 --> 00:42:51.887
Now assume that we know the
relations between the users.

00:42:51.887 --> 00:42:54.470
For example, in the form of the
social network that we've seen

00:42:54.470 --> 00:42:57.910
before, we can use the geometric
deep learning to learn how

00:42:57.910 --> 00:43:01.460
to aggregate scores of the
users social neighborhood--

00:43:01.460 --> 00:43:03.339
friends and friends of friends--

00:43:03.339 --> 00:43:05.630
in order to predict whether
he or she will like a movie

00:43:05.630 --> 00:43:06.840
or not.

00:43:06.840 --> 00:43:09.230
And we're able to show a
significant improvement

00:43:09.230 --> 00:43:11.870
over standard approaches using
these type of techniques.

00:43:11.870 --> 00:43:14.270
So geometric deep learning
seems to have also

00:43:14.270 --> 00:43:17.100
big promise for recommender
systems as well.

00:43:19.790 --> 00:43:21.710
The final example I
would like to mention

00:43:21.710 --> 00:43:25.250
is from the domain of chemistry
and this is work actually done

00:43:25.250 --> 00:43:26.840
here at Harvard.

00:43:26.840 --> 00:43:30.080
One of the hard problems
in material or drug design

00:43:30.080 --> 00:43:32.000
is that you need to
search over a huge space

00:43:32.000 --> 00:43:34.280
of potential
candidates before you

00:43:34.280 --> 00:43:37.790
discover that right
molecule that does the job.

00:43:37.790 --> 00:43:41.510
And today, this is still done
either by very complicated

00:43:41.510 --> 00:43:44.660
simulation on supercomputers--
molecular dynamics--

00:43:44.660 --> 00:43:46.550
or by experiment in the lab.

00:43:46.550 --> 00:43:48.440
And if you can think
of molecules as graphs

00:43:48.440 --> 00:43:53.100
where the nodes model atoms
and edges are chemical bonds,

00:43:53.100 --> 00:43:54.770
we can employ
geometric deep learning

00:43:54.770 --> 00:43:56.870
for future
rescreening a molecule

00:43:56.870 --> 00:43:59.065
to determine whether it
has certain properties.

00:43:59.065 --> 00:44:00.440
For example, in
drug design, this

00:44:00.440 --> 00:44:03.590
could be efficiency
against certain pathogen

00:44:03.590 --> 00:44:05.780
or its toxicity.

00:44:05.780 --> 00:44:08.000
So these methods
have already been

00:44:08.000 --> 00:44:10.200
tested in several real
life applications,

00:44:10.200 --> 00:44:14.030
for example, for the
design of new materials.

00:44:14.030 --> 00:44:15.830
And the promise is
huge, potentially

00:44:15.830 --> 00:44:18.920
cutting times of molecular
discovery and design

00:44:18.920 --> 00:44:21.660
by orders of magnitude.

00:44:21.660 --> 00:44:24.080
So in general, graphs appear
to be a very natural model

00:44:24.080 --> 00:44:28.490
of relation or interaction
between different objects

00:44:28.490 --> 00:44:29.540
or things.

00:44:29.540 --> 00:44:32.060
Graphs can model relations
between different regions

00:44:32.060 --> 00:44:34.970
in the human brain in
functional magnetic resonance

00:44:34.970 --> 00:44:37.970
imaging in order to better
understand how the brain works

00:44:37.970 --> 00:44:41.480
or how to diagnose and
cure neurological diseases,

00:44:41.480 --> 00:44:44.840
or interactions between
particles in a large Hadron

00:44:44.840 --> 00:44:48.680
Collider where energetic beams
of protons are banged together.

00:44:48.680 --> 00:44:52.220
And the hope is to discover
new physics or maybe

00:44:52.220 --> 00:44:54.800
spatial relations, spatial
structures of proteins

00:44:54.800 --> 00:44:58.670
that constituents the cells
of every living thing.

00:44:58.670 --> 00:45:01.220
And my hope is that harnessing
the power of machine

00:45:01.220 --> 00:45:05.750
learning and combining it with
meaningful geometric structures

00:45:05.750 --> 00:45:08.270
for this kind of data and
these kinds of problems

00:45:08.270 --> 00:45:11.570
would bringing both quantitative
and qualitative breakthroughs.

00:45:11.570 --> 00:45:15.080
And perhaps we'll see some new
exciting science and technology

00:45:15.080 --> 00:45:16.917
coming out of this research.

00:45:16.917 --> 00:45:17.750
Thank you very much.

00:45:17.750 --> 00:45:20.500
[MUSIC PLAYING]

