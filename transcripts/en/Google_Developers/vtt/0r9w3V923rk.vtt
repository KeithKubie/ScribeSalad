WEBVTT
Kind: captions
Language: en

00:00:04.130 --> 00:00:05.720
PETE WARDEN: Hi.

00:00:05.720 --> 00:00:09.200
So I'm Pete Warden, and I'm the
only thing standing between you

00:00:09.200 --> 00:00:13.490
and lunch, so I'm going to try
and talk as fast as possible.

00:00:13.490 --> 00:00:18.110
So I'm here to talk about
mobile and embedded TensorFlow,

00:00:18.110 --> 00:00:20.780
using TensorFlow on mobile
and embedded devices.

00:00:20.780 --> 00:00:25.460
And the first question is,
why would you want to do that?

00:00:25.460 --> 00:00:27.140
Isn't TensorFlow
and deep learning

00:00:27.140 --> 00:00:30.290
something that you do on
massive clusters of GPU machines

00:00:30.290 --> 00:00:32.119
in big data centers
in the cloud?

00:00:32.119 --> 00:00:38.600
Well, yes it is, but it's
also an incredible way

00:00:38.600 --> 00:00:42.530
of offering unique user
experiences that we've never

00:00:42.530 --> 00:00:46.220
been able to offer people
before by running interactively

00:00:46.220 --> 00:00:50.270
on devices that host to data and
the people that are working on.

00:00:50.270 --> 00:00:53.600
Everything from real
time translation,

00:00:53.600 --> 00:00:56.570
to predicting next
words on keyboards,

00:00:56.570 --> 00:00:59.200
to helping people scan
old photos with photo

00:00:59.200 --> 00:01:01.450
scan, to all of
the amazing stuff

00:01:01.450 --> 00:01:04.129
that you know Snapchat
is doing in real time,

00:01:04.129 --> 00:01:07.460
and as you just saw,
actually trying to help

00:01:07.460 --> 00:01:13.250
detect diseases running
real time on device.

00:01:13.250 --> 00:01:17.490
So, because it's so important
to us, and because we think

00:01:17.490 --> 00:01:20.731
it's such an important
area to be working on.

00:01:20.731 --> 00:01:22.230
As you saw in the
keynote we've been

00:01:22.230 --> 00:01:24.900
working with people
like Qualcomm

00:01:24.900 --> 00:01:27.510
and all sorts of other
hardware manufacturers

00:01:27.510 --> 00:01:29.550
to make sure that
TensorFlow runs

00:01:29.550 --> 00:01:32.741
as fast and as
smoothly as possible

00:01:32.741 --> 00:01:34.365
on a whole bunch of
different hardware.

00:01:41.700 --> 00:01:42.780
Once my clicker works.

00:01:49.398 --> 00:01:54.390
Yeah, the joke around the
office is we may be on the way

00:01:54.390 --> 00:02:01.650
to solving AI, but AV is
still well out of our reach.

00:02:01.650 --> 00:02:06.690
So, we are going to talk a lot
more depth about all the sample

00:02:06.690 --> 00:02:10.440
code, and documentation,
and build support

00:02:10.440 --> 00:02:13.020
we have for Android
iOS and Raspberry Pi,

00:02:13.020 --> 00:02:15.760
but if you have something
that's not listed here,

00:02:15.760 --> 00:02:20.400
we really care about all
the diversity of platforms

00:02:20.400 --> 00:02:21.740
that are actually emerging.

00:02:21.740 --> 00:02:24.430
So please do get in touch.

00:02:24.430 --> 00:02:26.280
We're really keen to
see TensorFlow running

00:02:26.280 --> 00:02:27.600
as many places as possible.

00:02:31.590 --> 00:02:33.820
Yeah, still nothing.

00:02:33.820 --> 00:02:35.070
OK.

00:02:35.070 --> 00:02:36.460
We're going to do this manually.

00:02:36.460 --> 00:02:41.430
So, how can you actually
get running on Android?

00:02:41.430 --> 00:02:48.910
Well, if you're an
Android developer,

00:02:48.910 --> 00:02:51.840
you're probably already
using Android Studio

00:02:51.840 --> 00:02:56.540
and TensorFlow can
actually be built

00:02:56.540 --> 00:02:59.702
with just a few simple steps
using Android Studio directly.

00:03:02.240 --> 00:03:09.850
Oh yeah, so if we
go back one more.

00:03:09.850 --> 00:03:11.530
So back.

00:03:11.530 --> 00:03:12.540
Yes.

00:03:12.540 --> 00:03:16.260
So here we actually show
you how you can actually

00:03:16.260 --> 00:03:19.290
alter the build script to let
it know where Bazel, which

00:03:19.290 --> 00:03:22.200
is the underlying build system
we use to build for Android,

00:03:22.200 --> 00:03:24.930
is located in the
build.gradle file

00:03:24.930 --> 00:03:28.860
that we supply as part
of the sample code,

00:03:28.860 --> 00:03:31.710
and then you just
add the sample folder

00:03:31.710 --> 00:03:33.480
as a new project
in Android Studio

00:03:33.480 --> 00:03:35.430
and go ahead and build it.

00:03:35.430 --> 00:03:39.450
Now, if you actually want
to get in a bit more depth

00:03:39.450 --> 00:03:42.520
and make alterations
to TensorFlow

00:03:42.520 --> 00:03:45.280
and do more complex
rebuilding, you

00:03:45.280 --> 00:03:49.960
use Bazel, which is Google's
open source built tool.

00:03:49.960 --> 00:03:52.190
You grab the latest version.

00:03:52.190 --> 00:03:58.420
The URLs listed here, and
you need to grab the SDKs

00:03:58.420 --> 00:04:02.860
for Android, both the SDK
and the NDK for the c++ part

00:04:02.860 --> 00:04:05.992
of the building, and I like
to use Android Studio's SDK

00:04:05.992 --> 00:04:08.200
manager because it makes it
a lot simpler than having

00:04:08.200 --> 00:04:12.190
to figure out all the URLs, and
it makes it really easy to have

00:04:12.190 --> 00:04:14.290
multiple versions
lying around too.

00:04:14.290 --> 00:04:19.029
So make a note of what version
of the SDK you've actually got

00:04:19.029 --> 00:04:22.300
and also where it's
located on disk.

00:04:22.300 --> 00:04:26.450
And you need to add
to this incantation

00:04:26.450 --> 00:04:30.470
to the end of the workspace
file in the source

00:04:30.470 --> 00:04:33.720
tree of TensorFlow at the root.

00:04:33.720 --> 00:04:36.950
And what this is
saying is basically

00:04:36.950 --> 00:04:41.330
which API you've installed
and where about it's actually

00:04:41.330 --> 00:04:42.560
living.

00:04:42.560 --> 00:04:47.300
And because I actually end
up doing this a lot as part

00:04:47.300 --> 00:04:49.700
of my job, I like to keep
this information around

00:04:49.700 --> 00:04:52.430
as a little file
snippet, which I can just

00:04:52.430 --> 00:04:54.740
do a one line append
to that WORKSPACE file

00:04:54.740 --> 00:04:58.620
whenever I set up
a new source tree.

00:04:58.620 --> 00:05:02.130
And then to build
the demo apps, all

00:05:02.130 --> 00:05:06.560
you have to do is call
this one Bazel build line,

00:05:06.560 --> 00:05:12.990
and just as a note, the
build file here is actually

00:05:12.990 --> 00:05:15.660
only set up for Android, so
you don't have to specify it.

00:05:15.660 --> 00:05:17.820
But for other tools
like the benchmark

00:05:17.820 --> 00:05:20.400
and things that can be built
for multiple platforms,

00:05:20.400 --> 00:05:24.030
you'll need to specify minus
minus config equals Android

00:05:24.030 --> 00:05:27.590
as part of the build process.

00:05:27.590 --> 00:05:30.770
So, what you get after that?

00:05:30.770 --> 00:05:33.200
The Android examples
are designed

00:05:33.200 --> 00:05:35.780
to help you think about
some of the different things

00:05:35.780 --> 00:05:39.710
that you can actually do
using TensorFlow on device.

00:05:39.710 --> 00:05:42.800
So if you're interested in
ideas on improving your apps,

00:05:42.800 --> 00:05:46.650
or new things you can build,
they're a great place to start.

00:05:46.650 --> 00:05:50.350
And unfortunately, this
doesn't come with cake.

00:05:50.350 --> 00:05:53.300
I could do with some
right now, but this

00:05:53.300 --> 00:05:57.710
is the classify example,
and what this is doing

00:05:57.710 --> 00:06:01.580
is it taking a live camera
feed, and it's actually

00:06:01.580 --> 00:06:07.550
trying to label images using
the Inception v3 network,

00:06:07.550 --> 00:06:11.300
the same one that we saw in
the previous presentation,

00:06:11.300 --> 00:06:18.140
but for the objects that are
known in the image net class.

00:06:18.140 --> 00:06:22.370
And one key thing about
this is you can actually

00:06:22.370 --> 00:06:25.370
use the TensorFlow
for Poets tutorial

00:06:25.370 --> 00:06:28.460
which is a code lab
which you can Google,

00:06:28.460 --> 00:06:33.690
and that lets you train your
own model using Inception

00:06:33.690 --> 00:06:37.610
v3 with no coding
required, on images

00:06:37.610 --> 00:06:39.950
that you actually
care about, and then

00:06:39.950 --> 00:06:42.860
you can take that model and
you can drop it directly

00:06:42.860 --> 00:06:46.580
into this sample, and have your
own custom image recognition

00:06:46.580 --> 00:06:51.380
app with very, very
little coding involved.

00:06:51.380 --> 00:06:53.780
And a couple of
other details, if you

00:06:53.780 --> 00:06:55.250
play with the
volume button you'll

00:06:55.250 --> 00:06:58.070
get to see statistics about
where the time is going

00:06:58.070 --> 00:07:01.040
and other debug
information, and you

00:07:01.040 --> 00:07:04.250
might notice with the
default image net model,

00:07:04.250 --> 00:07:06.830
if you point it at
people it will come up

00:07:06.830 --> 00:07:11.210
with results that are
confusing because there's

00:07:11.210 --> 00:07:15.740
no person label in the
image net class set.

00:07:15.740 --> 00:07:18.140
So it will show you
things that it often

00:07:18.140 --> 00:07:22.880
sees together with people like
oxygen masks and seat belts.

00:07:22.880 --> 00:07:26.290
But if you do want
to recognize people,

00:07:26.290 --> 00:07:28.100
we have the app for you.

00:07:28.100 --> 00:07:31.930
And this is TF Detect, and
what this is actually doing

00:07:31.930 --> 00:07:35.920
is drawing bounding
boxes around any people

00:07:35.920 --> 00:07:39.990
that it actually detects
in the live camera view.

00:07:39.990 --> 00:07:42.370
And here's a
screenshot, but just

00:07:42.370 --> 00:07:45.460
to live really
dangerously, I'm actually

00:07:45.460 --> 00:07:50.020
going to try and do a live demo
that we haven't even rehearsed.

00:07:50.020 --> 00:07:54.850
So I'm going to give some
offerings to the demo gods

00:07:54.850 --> 00:07:57.640
and hopefully they
will smile on us.

00:07:57.640 --> 00:08:01.830
I am ready to switch over.

00:08:01.830 --> 00:08:04.922
So can you-- do we
need more light?

00:08:10.090 --> 00:08:15.410
Well, hopefully you can see.

00:08:15.410 --> 00:08:17.270
Yeah, that actually helps.

00:08:17.270 --> 00:08:20.630
You can see my little nephew
there is being detected

00:08:20.630 --> 00:08:25.340
as a person in this
picture, and if you

00:08:25.340 --> 00:08:28.040
look at the bounding
box, you'll see

00:08:28.040 --> 00:08:29.660
that it sort of
follows him around

00:08:29.660 --> 00:08:33.150
as you sort of move the camera.

00:08:33.150 --> 00:08:34.890
So before anything
goes wrong with that,

00:08:34.890 --> 00:08:38.890
I'm going to skip quickly past.

00:08:38.890 --> 00:08:40.621
Since I think I
got away with that.

00:08:44.620 --> 00:08:47.200
Yeah.

00:08:47.200 --> 00:08:49.306
OK, let's see if this--

00:08:49.306 --> 00:08:52.150
no, clicker's still-- oh, OK.

00:08:54.820 --> 00:08:58.960
So it uses tracking, which is
really important when you're

00:08:58.960 --> 00:09:01.630
doing object detection because
that means you can actually

00:09:01.630 --> 00:09:05.560
know which objects are the
same from frame to fame.

00:09:05.560 --> 00:09:09.170
So we include a full tracking
solution as part of this demo,

00:09:09.170 --> 00:09:11.170
and that makes it really
useful for doing things

00:09:11.170 --> 00:09:13.525
like counting
objects because you

00:09:13.525 --> 00:09:16.000
know when a new one appears.

00:09:16.000 --> 00:09:18.190
There isn't training for
the person detection yet,

00:09:18.190 --> 00:09:21.550
but if you look in the read me,
you'll see that the grey open

00:09:21.550 --> 00:09:27.490
source project called Yolo,
which actually supports object

00:09:27.490 --> 00:09:32.380
recognition, we also support
that as part of the demo.

00:09:32.380 --> 00:09:35.140
And finally, you've
probably seen

00:09:35.140 --> 00:09:39.670
the Magenta Group talking about
stylisation and style transfer.

00:09:39.670 --> 00:09:43.420
You can actually run
this live on the phone

00:09:43.420 --> 00:09:46.990
using the stylize example.

00:09:46.990 --> 00:09:50.590
And because it's part of
the Magenta model set,

00:09:50.590 --> 00:09:53.330
you can just grab the
model directly from there

00:09:53.330 --> 00:09:58.770
and train it on any stars
that you actually want to do.

00:09:58.770 --> 00:10:02.480
So, if you're
developing for Android,

00:10:02.480 --> 00:10:05.330
you're almost certainly
writing your code in Java.

00:10:05.330 --> 00:10:09.590
And the bulk of TensorFlow's
core that you're actually going

00:10:09.590 --> 00:10:11.510
to be using is written in c++.

00:10:11.510 --> 00:10:13.830
So how do you deal with that?

00:10:13.830 --> 00:10:18.340
How do you actually call
TensorFlow from your app?

00:10:18.340 --> 00:10:24.010
The answer is a thing called
the Android Inference Library,

00:10:24.010 --> 00:10:26.710
and if you don't want to
understand what it does,

00:10:26.710 --> 00:10:31.750
you don't need to, but basically
it acts as a bridge between

00:10:31.750 --> 00:10:35.920
the Android Java
level and the c++.

00:10:35.920 --> 00:10:41.380
And you can actually grab
a pre-built binary of this

00:10:41.380 --> 00:10:45.910
and an APK of all of
the demos that I've just

00:10:45.910 --> 00:10:51.861
shown you at this URL, where
the nightly builds are actually

00:10:51.861 --> 00:10:52.360
put.

00:10:52.360 --> 00:10:54.635
So if you don't want to
compile the library yourself,

00:10:54.635 --> 00:10:56.510
you don't want to compile
the demos yourself,

00:10:56.510 --> 00:11:02.230
but you want to give them a try,
this is the URL to check how.

00:11:02.230 --> 00:11:05.800
And even better-- go back.

00:11:05.800 --> 00:11:10.030
Even better, we actually
have a full Java API

00:11:10.030 --> 00:11:14.020
that has just been
created, which

00:11:14.020 --> 00:11:18.220
is going to offer a whole range
of access, much beyond just

00:11:18.220 --> 00:11:20.800
like loading and
running a model, which

00:11:20.800 --> 00:11:22.990
is what the Inference
Library focuses on.

00:11:22.990 --> 00:11:28.290
So also check out
the full Java API.

00:11:28.290 --> 00:11:30.440
So, if you're
building for mobile,

00:11:30.440 --> 00:11:33.210
you almost certainly
care about iOS as well.

00:11:33.210 --> 00:11:36.280
So what can you do there?

00:11:36.280 --> 00:11:41.770
We have full support
for building for iOS.

00:11:41.770 --> 00:11:45.820
We actually use a
makefile to help

00:11:45.820 --> 00:11:48.860
us get this running
because we call out

00:11:48.860 --> 00:11:52.070
to Xcode's command line tools.

00:11:52.070 --> 00:11:53.470
So you need to
make sure that you

00:11:53.470 --> 00:11:56.290
have a recent version
of Xcode running,

00:11:56.290 --> 00:11:58.750
and also install a
couple of dependencies

00:11:58.750 --> 00:12:01.900
you need to help make
files actually run.

00:12:01.900 --> 00:12:04.810
And I like to use brew
because it makes life simpler.

00:12:04.810 --> 00:12:06.970
But whatever use, just
try to make sure you've

00:12:06.970 --> 00:12:10.220
got automate and libtool.

00:12:10.220 --> 00:12:15.890
And then all you have to do is
run the build or iOS.sh script

00:12:15.890 --> 00:12:19.680
and it usually takes
somewhere around 20 minutes,

00:12:19.680 --> 00:12:23.240
and at the end of it you'll
have a universal library

00:12:23.240 --> 00:12:27.560
implementing TensorFlow in all
of the architectures plus bit

00:12:27.560 --> 00:12:31.375
code, and just in case you
need to dive a bit deeper

00:12:31.375 --> 00:12:34.250
and you need to make iterative
changes to the TensorFlow

00:12:34.250 --> 00:12:38.290
library, you can run
the individual steps

00:12:38.290 --> 00:12:43.040
and they will only rebuild
things that have changed.

00:12:43.040 --> 00:12:46.790
And to make sure that things run
as fast as possible, by default

00:12:46.790 --> 00:12:49.040
we use the minus
OS flag, optimizing

00:12:49.040 --> 00:12:56.090
for size, which we also find
gives a good speed profile.

00:12:56.090 --> 00:12:59.270
And we love using Apple's
Accelerate framework

00:12:59.270 --> 00:13:03.740
because they offer really fast
linear algebra matrix multiply

00:13:03.740 --> 00:13:09.060
operations on iOS devices.

00:13:09.060 --> 00:13:13.230
And when you're ready to
use this in your own app,

00:13:13.230 --> 00:13:16.110
you need to link against
the TensorFlow library

00:13:16.110 --> 00:13:17.610
that you've just created.

00:13:17.610 --> 00:13:19.500
You also need to
set up your project

00:13:19.500 --> 00:13:22.980
to link against the protobuf
libraries that were created

00:13:22.980 --> 00:13:26.340
as part of the previous steps.

00:13:26.340 --> 00:13:30.270
You need to add include parts
to the TensorFlow headers

00:13:30.270 --> 00:13:35.840
as part of your project so it
can find the dot H's, and you

00:13:35.840 --> 00:13:38.720
need to use this
minus force load

00:13:38.720 --> 00:13:43.340
flag as part of your
linker ops when you're

00:13:43.340 --> 00:13:45.170
building your final executable.

00:13:45.170 --> 00:13:49.520
And this is mysterious,
and I'm going

00:13:49.520 --> 00:13:52.070
to be explaining why
you need to do this

00:13:52.070 --> 00:13:55.200
in more depth in a
little bit later,

00:13:55.200 --> 00:14:01.330
but for now, just remember do
this or bad things will happen.

00:14:01.330 --> 00:14:02.850
So what can you do on iOS?

00:14:02.850 --> 00:14:07.050
Well, you can create some
really, really ugly UI, which

00:14:07.050 --> 00:14:10.950
is the demo app that
I first created,

00:14:10.950 --> 00:14:14.370
and all this does is has
a single button, which you

00:14:14.370 --> 00:14:16.920
tap the button to run a model.

00:14:16.920 --> 00:14:23.520
It loads a picture of Admiral
Grace Hopper from the assets

00:14:23.520 --> 00:14:28.320
and runs it through an inception
model and prints out the label.

00:14:28.320 --> 00:14:30.480
So why does this exist?

00:14:30.480 --> 00:14:33.450
It's very simple
and it's very small.

00:14:33.450 --> 00:14:34.920
So if you're looking
for something

00:14:34.920 --> 00:14:37.980
to start with to look
at how to integrate this

00:14:37.980 --> 00:14:40.260
into your own project,
it's a very small amount

00:14:40.260 --> 00:14:42.660
of code to look at and
you'll get a good idea

00:14:42.660 --> 00:14:45.570
of how to work with it.

00:14:45.570 --> 00:14:48.420
But for something more
fun, these are actually

00:14:48.420 --> 00:14:54.180
my two cats from home, and
much like the Classify app

00:14:54.180 --> 00:14:58.200
that I talked about earlier,
this takes a live camera feed

00:14:58.200 --> 00:15:02.430
and runs an inception
network on every frame,

00:15:02.430 --> 00:15:05.730
and prints out the
labels on screen

00:15:05.730 --> 00:15:09.180
to show what it
thinks it's found.

00:15:09.180 --> 00:15:14.640
And just like the
Classify app on Android,

00:15:14.640 --> 00:15:18.450
you can take a TensorFlow
for Poets trained model

00:15:18.450 --> 00:15:21.510
and drop it into the
example, and have it work

00:15:21.510 --> 00:15:23.470
with very, very few changes.

00:15:23.470 --> 00:15:27.510
So again, it's really easy to
build your own custom image

00:15:27.510 --> 00:15:30.850
recognition app with
almost no coding required.

00:15:33.410 --> 00:15:35.420
And I love the Raspberry Pi.

00:15:35.420 --> 00:15:36.980
It's a massive amount of fun.

00:15:36.980 --> 00:15:40.250
I have so many of them
cluttering up my house,

00:15:40.250 --> 00:15:43.490
but it's also a really
powerful platform

00:15:43.490 --> 00:15:46.880
for prototyping all
sorts of devices

00:15:46.880 --> 00:15:49.490
sort of in the internet
of things world.

00:15:49.490 --> 00:15:52.190
So it's a really
important platform for us

00:15:52.190 --> 00:15:54.200
to be supporting.

00:15:54.200 --> 00:15:58.670
And the good news is
that building for it,

00:15:58.670 --> 00:16:01.010
it looks a lot like a
normal Linux machine

00:16:01.010 --> 00:16:04.160
that you might have in your
development environment.

00:16:04.160 --> 00:16:07.580
The bad news is that it's
an incredibly slow Linux

00:16:07.580 --> 00:16:12.090
machine compared to any desktop
or laptop that you're used to.

00:16:12.090 --> 00:16:16.010
So the temptation might
be to compile everything

00:16:16.010 --> 00:16:20.720
on your fast machine,
cross-compiling for arm

00:16:20.720 --> 00:16:25.760
and for the Raspberry Pi,
but I've gone down that route

00:16:25.760 --> 00:16:29.030
and I actually found I
hit a whole bunch of very

00:16:29.030 --> 00:16:31.670
hard to diagnose problems.

00:16:31.670 --> 00:16:34.370
So you might have
better luck, but I

00:16:34.370 --> 00:16:36.959
recommend actually
building everything locally

00:16:36.959 --> 00:16:38.750
on the Raspberry Pi,
even though it's going

00:16:38.750 --> 00:16:42.320
to take a few hours to run.

00:16:42.320 --> 00:16:48.260
And to build the c++ core of
TensorFlow on the Raspberry Pi

00:16:48.260 --> 00:16:50.752
you need to install a
few dependencies again,

00:16:50.752 --> 00:16:52.460
and we're going to be
using the makefile.

00:16:52.460 --> 00:16:56.180
So you need a few makefile
dependencies, and also

00:16:56.180 --> 00:16:59.540
if you're running on a
recent version of Raspian,

00:16:59.540 --> 00:17:04.160
make sure you install GCC
4.8 because TensorFlow

00:17:04.160 --> 00:17:08.810
has had some issues with
the arm version of 4.9.

00:17:08.810 --> 00:17:11.450
So we found
switching back to 4.8

00:17:11.450 --> 00:17:14.240
makes things a bit smoother.

00:17:14.240 --> 00:17:15.920
And just as another
note as well,

00:17:15.920 --> 00:17:19.200
there's a fantastic
tutorial out there,

00:17:19.200 --> 00:17:22.640
if you look for it, for
building the whole of TensorFlow

00:17:22.640 --> 00:17:26.089
on your Raspberry Pi,
including the Python parts.

00:17:26.089 --> 00:17:30.530
That requires a
swap drive and about

00:17:30.530 --> 00:17:32.870
a day of compiling
last time I tried it.

00:17:32.870 --> 00:17:37.340
So I like to just start
off with the makefile

00:17:37.340 --> 00:17:40.900
so that you can run
models and do inference.

00:17:40.900 --> 00:17:45.760
And then to build it you just
run this makefile script.

00:17:45.760 --> 00:17:49.960
Again, specifying GCC 4.8
so that you use the version

00:17:49.960 --> 00:17:52.320
that you've just installed.

00:17:52.320 --> 00:17:55.680
And one fantastic thing
about recent versions

00:17:55.680 --> 00:17:58.800
of the Raspberry Pi
is that they actually

00:17:58.800 --> 00:18:00.420
have SIM-D acceleration.

00:18:00.420 --> 00:18:03.060
They have neon
acceleration instructions,

00:18:03.060 --> 00:18:04.980
and so if you want
to take advantage

00:18:04.980 --> 00:18:07.860
of that, which you really should
if you can avoid deploying

00:18:07.860 --> 00:18:13.410
on Raspberry Pi 1's, you need
to specify these optimization

00:18:13.410 --> 00:18:14.580
flags.

00:18:14.580 --> 00:18:17.370
And you'll get a
many times speed up

00:18:17.370 --> 00:18:21.350
compared to the
normal scalar code.

00:18:21.350 --> 00:18:25.160
So what can you do
on the Raspberry Pi?

00:18:25.160 --> 00:18:27.260
As is traditional
at this point, we

00:18:27.260 --> 00:18:31.190
have a very small kind of boring
example to get you started,

00:18:31.190 --> 00:18:34.370
called label_image, and this
is a small amount of code

00:18:34.370 --> 00:18:38.630
that loads an image
net inception model,

00:18:38.630 --> 00:18:41.990
and it will take an
image file as an input,

00:18:41.990 --> 00:18:44.120
and it will run the image
through and print out

00:18:44.120 --> 00:18:46.460
what the labels are.

00:18:46.460 --> 00:18:50.120
Now, for more fun,
the camera example

00:18:50.120 --> 00:18:53.750
actually takes a live feed
from the Raspberry Pi camera

00:18:53.750 --> 00:18:55.820
accessory, if you
have one attached,

00:18:55.820 --> 00:19:00.020
and it runs through
an inception network,

00:19:00.020 --> 00:19:03.740
and it prints out the labels
that finds continuously

00:19:03.740 --> 00:19:04.880
to the console.

00:19:04.880 --> 00:19:08.060
And again, just
to emphasize this,

00:19:08.060 --> 00:19:10.670
you can take a model that
you've trained with TensorFlow

00:19:10.670 --> 00:19:13.740
for Poets and drop
it into this example,

00:19:13.740 --> 00:19:16.130
and have something
running that's

00:19:16.130 --> 00:19:20.030
running your custom image model
for things that you actually

00:19:20.030 --> 00:19:21.980
care about.

00:19:21.980 --> 00:19:25.790
And just for fun, I've
actually tried to set things up

00:19:25.790 --> 00:19:28.460
so you can just
pipe on the command

00:19:28.460 --> 00:19:30.950
line, the output of
this tool into flight,

00:19:30.950 --> 00:19:33.020
and have your Raspberry
Pi sort of saying

00:19:33.020 --> 00:19:37.420
what it's seeing as you
point it around the room.

00:19:37.420 --> 00:19:38.130
OK.

00:19:38.130 --> 00:19:40.230
Do you remember I
mentioned minus minus force

00:19:40.230 --> 00:19:42.660
load with lots of mystery?

00:19:42.660 --> 00:19:46.350
Well, you'll need
to know about it

00:19:46.350 --> 00:19:47.970
when you run into this error.

00:19:47.970 --> 00:19:50.280
You've built your
app successfully.

00:19:50.280 --> 00:19:52.620
You've managed to link in,
get all of the header files

00:19:52.620 --> 00:19:54.570
and everything else set up.

00:19:54.570 --> 00:19:55.610
You've written the code.

00:19:55.610 --> 00:19:57.147
Everything's compiling.

00:19:57.147 --> 00:19:58.980
You've run your app
trying to use TensorFlow

00:19:58.980 --> 00:20:01.340
for the first time, and you
get this "No session factory

00:20:01.340 --> 00:20:04.770
registered" error, and you're
just left scratching your head.

00:20:04.770 --> 00:20:06.510
Like what's going on?

00:20:06.510 --> 00:20:09.000
Well the punchline is--

00:20:09.000 --> 00:20:10.860
if you don't want to
know the details--

00:20:10.860 --> 00:20:14.520
use minus force load, or
on Linux, minus minus whole

00:20:14.520 --> 00:20:18.060
archive to link in the
TensorFlow library.

00:20:18.060 --> 00:20:20.760
But why do you need to do that?

00:20:20.760 --> 00:20:23.540
So to understand that,
you have to jump into how

00:20:23.540 --> 00:20:25.760
TensorFlow is built a bit more.

00:20:25.760 --> 00:20:27.440
There are actually
hundreds and hundreds

00:20:27.440 --> 00:20:31.760
of components in TensorFlow,
and there's no single source

00:20:31.760 --> 00:20:34.310
file which lists all of them.

00:20:34.310 --> 00:20:40.400
Instead, we have a pattern where
each object, each component

00:20:40.400 --> 00:20:42.860
in the system, each
module, actually

00:20:42.860 --> 00:20:46.220
advertises for the rest of
the system what it can do.

00:20:46.220 --> 00:20:49.460
It registers itself
to say hey, here I am.

00:20:49.460 --> 00:20:51.620
I can implement
this thing for you,

00:20:51.620 --> 00:20:54.410
and then TensorFlow
as a system, actually

00:20:54.410 --> 00:20:57.560
gets that full list
at runtime and figures

00:20:57.560 --> 00:20:59.990
out what's actually available.

00:20:59.990 --> 00:21:03.740
And this error is a sign
that the session factory,

00:21:03.740 --> 00:21:06.530
the sessions, which
are some of the most

00:21:06.530 --> 00:21:10.970
basic things, the first thing
that TensorFlow tries to grab,

00:21:10.970 --> 00:21:14.300
the registration
mechanism hasn't worked.

00:21:14.300 --> 00:21:18.230
So what do I mean by a
registration mechanism?

00:21:18.230 --> 00:21:20.240
This is a super
simplified example

00:21:20.240 --> 00:21:24.980
of an implementation of
the multiply operation

00:21:24.980 --> 00:21:26.180
in TensorFlow.

00:21:26.180 --> 00:21:30.290
It's a c++ class that
effectively has a few

00:21:30.290 --> 00:21:33.980
functions, probably the most
important of which is compute,

00:21:33.980 --> 00:21:37.380
which actually takes some
inputs, processes them,

00:21:37.380 --> 00:21:41.320
and puts the results
into an output tensor.

00:21:41.320 --> 00:21:45.520
And in order to let the system
let the whole of TensorFlow

00:21:45.520 --> 00:21:49.620
know that this is
available, there

00:21:49.620 --> 00:21:51.700
you'll see if you look
in the dot cc file,

00:21:51.700 --> 00:21:55.870
there's something like
this, REGISTER_KERNEL line.

00:21:55.870 --> 00:21:58.130
And the meaning of
this is saying hey,

00:21:58.130 --> 00:22:01.450
this class, Mul
Kernel, is available

00:22:01.450 --> 00:22:05.710
if you ever run across a
graph that has a Mul op in it,

00:22:05.710 --> 00:22:08.770
you can use this class
to actually implement

00:22:08.770 --> 00:22:12.230
that functionality,
and that's fantastic.

00:22:12.230 --> 00:22:15.730
That means that we can easily
build very complex systems

00:22:15.730 --> 00:22:18.914
with no sort of
single global list

00:22:18.914 --> 00:22:20.080
that we have to worry about.

00:22:20.080 --> 00:22:24.660
It's very flexible, as you
can swap modules in and out.

00:22:24.660 --> 00:22:32.170
But it's implemented
using a bit of c++ magic,

00:22:32.170 --> 00:22:37.270
and the way that line
REGISTER_KERNEL actually works

00:22:37.270 --> 00:22:41.410
is it expands into something
that's kind of a more complex

00:22:41.410 --> 00:22:42.610
version of this.

00:22:42.610 --> 00:22:46.870
And what this is doing is it's
defining a class that pretty

00:22:46.870 --> 00:22:49.450
much just contains
a constructor,

00:22:49.450 --> 00:22:55.480
and the constructor
calls the registry method

00:22:55.480 --> 00:23:00.610
to tell the rest of TensorFlow,
hey, if you've got a Mul op,

00:23:00.610 --> 00:23:02.840
here's a factory method.

00:23:02.840 --> 00:23:04.600
Here's a small
function that will

00:23:04.600 --> 00:23:06.400
return classes
that will actually

00:23:06.400 --> 00:23:07.840
implement that for you.

00:23:07.840 --> 00:23:10.300
And then, the real
magic comes in

00:23:10.300 --> 00:23:14.500
because a global
variable of that type

00:23:14.500 --> 00:23:17.350
is actually declared
immediately after.

00:23:17.350 --> 00:23:22.360
So when that global
variable is created,

00:23:22.360 --> 00:23:25.300
which should happen before the
rest of the program is run,

00:23:25.300 --> 00:23:30.370
before main is actually run,
then that registration core

00:23:30.370 --> 00:23:33.670
we made, so by the time
any user code is run,

00:23:33.670 --> 00:23:36.370
all of the objects which uses
pattern will have actually

00:23:36.370 --> 00:23:39.070
called into the registry
and let the registry

00:23:39.070 --> 00:23:41.720
know about them existing.

00:23:41.720 --> 00:23:48.270
But, linkers are stock in
1978 and have no idea that c++

00:23:48.270 --> 00:23:51.840
exists, and they're extremely
keen to get rid of any

00:23:51.840 --> 00:23:54.240
variables that aren't
actually being used.

00:23:54.240 --> 00:23:59.160
And they look at that global
that you just created.

00:23:59.160 --> 00:24:03.330
They see nobody's reading from
it, nobody is writing to it,

00:24:03.330 --> 00:24:07.290
so they say oh great, I
can just get rid of this.

00:24:07.290 --> 00:24:09.990
They don't know that
it actually has a side

00:24:09.990 --> 00:24:12.810
effect in its constructor.

00:24:12.810 --> 00:24:16.530
So what happens is that
all of the register

00:24:16.530 --> 00:24:21.450
calls get stripped out and
the first problem it hits

00:24:21.450 --> 00:24:24.120
is that sessions themselves
aren't registered.

00:24:24.120 --> 00:24:26.500
So what do you do?

00:24:26.500 --> 00:24:31.360
So as I mentioned at the start,
minus minus whole archive

00:24:31.360 --> 00:24:36.670
on Linux or minus force
load on iOS or OS X

00:24:36.670 --> 00:24:40.270
is a flag that you have to
put into your linker flags

00:24:40.270 --> 00:24:42.940
when you're building
your executable,

00:24:42.940 --> 00:24:45.220
and this is the tricky
part because this is not

00:24:45.220 --> 00:24:48.610
something we can put into the
library compilation scripts

00:24:48.610 --> 00:24:51.130
because it's something you
need to do when you're adding

00:24:51.130 --> 00:24:54.580
the TensorFlow to your binary.

00:24:54.580 --> 00:24:58.510
So just to reiterate,
you ever see

00:24:58.510 --> 00:25:02.010
the no session found
error, first thing to do

00:25:02.010 --> 00:25:05.450
is go check your linker options
and try putting these in

00:25:05.450 --> 00:25:07.420
to make sure you're
loading TensorFlow

00:25:07.420 --> 00:25:11.470
without the linker
trying to get clever.

00:25:11.470 --> 00:25:14.290
So you've got something
up and running,

00:25:14.290 --> 00:25:19.190
and you're getting ready to put
your app out into the world.

00:25:19.190 --> 00:25:23.150
But you really, really
care as a mobile developer

00:25:23.150 --> 00:25:27.740
about the size of your
app, and Inception v3

00:25:27.740 --> 00:25:30.890
is almost 100 megs,
which is appalling

00:25:30.890 --> 00:25:32.360
if you're a mobile developer.

00:25:32.360 --> 00:25:35.810
Like that's just mind blowing
that people would be shipping

00:25:35.810 --> 00:25:37.940
that sort of stuff around.

00:25:37.940 --> 00:25:39.470
But do not worry.

00:25:39.470 --> 00:25:41.840
We actually have a
whole bunch of solutions

00:25:41.840 --> 00:25:46.670
to help you shrink the model
size down by a massive amount.

00:25:46.670 --> 00:25:52.100
So to understand how that
works, to get started,

00:25:52.100 --> 00:25:55.190
you need to actually run
freeze graph script to take

00:25:55.190 --> 00:25:58.940
a model form the
normal checkpoint

00:25:58.940 --> 00:26:01.730
format into something that's
just a single graph def

00:26:01.730 --> 00:26:05.650
so you can do more
easy processing after.

00:26:05.650 --> 00:26:09.410
Then you use the
graph transform tool

00:26:09.410 --> 00:26:13.430
to actually do a whole
bunch of optimizations.

00:26:13.430 --> 00:26:15.980
This is a big toolbox
full of useful things

00:26:15.980 --> 00:26:18.990
helping to speed up
and tidy up your graph.

00:26:18.990 --> 00:26:22.040
But the most important
one for this problem

00:26:22.040 --> 00:26:24.650
is the quantize_weights
transform,

00:26:24.650 --> 00:26:29.540
and by default
TensorFlow will store

00:26:29.540 --> 00:26:34.790
the weight for most models as
32-bit floating point values,

00:26:34.790 --> 00:26:37.790
and models contain millions,
and millions, and millions

00:26:37.790 --> 00:26:39.980
of these weights, typically.

00:26:39.980 --> 00:26:45.720
So the Inception v3 has
over 20 million whites,

00:26:45.720 --> 00:26:50.060
which means that you end up
with this very large, almost 100

00:26:50.060 --> 00:26:52.500
meg model file in the end.

00:26:52.500 --> 00:26:55.980
But those numbers
are actually usually

00:26:55.980 --> 00:26:59.640
within fairly small
ranges, so you can actually

00:26:59.640 --> 00:27:03.660
compress them down to 8-bit
using this quantized_weights

00:27:03.660 --> 00:27:05.520
transform.

00:27:05.520 --> 00:27:09.600
And you will get a very
surprisingly small loss

00:27:09.600 --> 00:27:14.010
of accuracy, in most cases,
for a very, very big reduction

00:27:14.010 --> 00:27:16.020
in file size.

00:27:16.020 --> 00:27:18.600
And if you want to
get even more fancy,

00:27:18.600 --> 00:27:21.180
this is still a very
experimental feature,

00:27:21.180 --> 00:27:23.010
but we do have
quantized nodes which

00:27:23.010 --> 00:27:27.250
actually does the
calculations in 8-bit as well.

00:27:27.250 --> 00:27:30.870
And there's lots of
asterisks next to this,

00:27:30.870 --> 00:27:35.190
depending on which model you're
using, but we're working hard,

00:27:35.190 --> 00:27:38.220
and it's actually how we're
doing the Qualcomm integration

00:27:38.220 --> 00:27:40.011
for example, on their DSP.

00:27:42.580 --> 00:27:46.720
And if you're actually running
into problems with RAM usage

00:27:46.720 --> 00:27:48.530
when you're actually
running your app.

00:27:48.530 --> 00:27:50.920
For example, on iOS,
on older devices,

00:27:50.920 --> 00:27:53.680
it can be very easy to
have your app killed

00:27:53.680 --> 00:27:56.830
if you're using more than
100 megs of memory at a time.

00:27:56.830 --> 00:27:59.380
You can actually
use memory mapping

00:27:59.380 --> 00:28:04.120
to avoid loading the
weights explicitly

00:28:04.120 --> 00:28:06.940
into an allocated
buffer in RAM, which

00:28:06.940 --> 00:28:10.090
will help a lot with
both loading speed

00:28:10.090 --> 00:28:12.630
and avoiding getting killed.

00:28:12.630 --> 00:28:15.480
And at the end of it, you end
up with a quantized version

00:28:15.480 --> 00:28:18.810
of Inception v3 that's
just 24 megs, and if that's

00:28:18.810 --> 00:28:22.530
too big you can actually look
at older versions of Inception

00:28:22.530 --> 00:28:26.350
like v1, which still gives
pretty good accuracy,

00:28:26.350 --> 00:28:30.080
and that can be just seven megs.

00:28:30.080 --> 00:28:33.530
So, you've actually managed
to get your model size down.

00:28:33.530 --> 00:28:35.990
The other side of your
application bundle

00:28:35.990 --> 00:28:38.030
is how big is your binary?

00:28:38.030 --> 00:28:40.850
And another scary number.

00:28:40.850 --> 00:28:45.500
TensorFlow will increase your
binary size by 12 megabytes

00:28:45.500 --> 00:28:48.380
if you just add it
before you do any tuning,

00:28:48.380 --> 00:28:50.690
but do not be scared.

00:28:50.690 --> 00:28:52.580
We actually have
again, a whole bunch

00:28:52.580 --> 00:28:57.740
of ways to reduce that size
by taking a few simple steps.

00:28:57.740 --> 00:29:01.170
And to understand why
TensorFlow is so big,

00:29:01.170 --> 00:29:06.200
it includes hundreds and 100 of
ops, and each one of those ops

00:29:06.200 --> 00:29:11.870
has an implementation that
takes up a bunch of code space.

00:29:11.870 --> 00:29:16.520
And to actually keep the
size even down to 12 megs,

00:29:16.520 --> 00:29:19.610
we actually don't
include some data types

00:29:19.610 --> 00:29:22.610
and some ops by
default. So if you

00:29:22.610 --> 00:29:27.110
run into the no op
kernel found ever,

00:29:27.110 --> 00:29:32.120
you may actually want to add
some dot cc to either the build

00:29:32.120 --> 00:29:35.750
files or the makefile listing
of files you need to include,

00:29:35.750 --> 00:29:39.480
depending on which system
you're actually using.

00:29:39.480 --> 00:29:42.650
But if you're trying to
keep the binary size down,

00:29:42.650 --> 00:29:45.740
you can run this print
selective registration header

00:29:45.740 --> 00:29:49.130
script on your model,
and what this does

00:29:49.130 --> 00:29:52.580
is it actually looks at what
ops your model is using,

00:29:52.580 --> 00:29:55.650
and then creates a header
file that you can put it

00:29:55.650 --> 00:29:57.590
in the root of your
source tree, and then

00:29:57.590 --> 00:30:00.560
you specify the selective
registration macro,

00:30:00.560 --> 00:30:03.170
and TensorFlow will be
compiled to only include

00:30:03.170 --> 00:30:07.340
the implementations of
the ops that you actually

00:30:07.340 --> 00:30:10.340
are using in your model.

00:30:10.340 --> 00:30:13.550
For example, with
inception v3 set of ops,

00:30:13.550 --> 00:30:17.980
you can get the binary size
increased to under 2 megabytes.

00:30:17.980 --> 00:30:21.320
So, thanks all for listening.

00:30:21.320 --> 00:30:23.080
I know I've been
keeping you from lunch,

00:30:23.080 --> 00:30:24.145
so I'll wrap up now.

00:30:24.145 --> 00:30:26.440
But I'm really excited
to see everything

00:30:26.440 --> 00:30:28.180
that you guys are
going to be creating,

00:30:28.180 --> 00:30:30.400
and please do get
in touch, and let me

00:30:30.400 --> 00:30:32.632
know how you're getting
on, and how we can help.

00:30:32.632 --> 00:30:33.340
Thanks, everyone.

00:30:33.340 --> 00:30:35.790
[APPLAUSE]

