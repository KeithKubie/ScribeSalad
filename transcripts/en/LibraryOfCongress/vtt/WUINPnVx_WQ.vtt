WEBVTT
Kind: captions
Language: en

00:00:01.120 --> 00:00:04.970
&gt;&gt; From the Library of
Congress in Washington D.C.

00:00:15.970 --> 00:00:17.270
&gt;&gt; John Haskell: So
welcome everybody

00:00:17.270 --> 00:00:19.270
to the Coolidge Auditorium
here at the Library

00:00:19.270 --> 00:00:22.280
and to conversation
about disinformation

00:00:22.280 --> 00:00:24.000
and the threat to democracy.

00:00:24.000 --> 00:00:27.920
This event is hosted by
the Kluge Center here

00:00:27.920 --> 00:00:29.570
at the Library of Congress.

00:00:29.570 --> 00:00:32.320
And many of you may not know
much about the Kluge Center,

00:00:32.320 --> 00:00:34.920
so I'm going to give you
a one-seven synopsis.

00:00:34.920 --> 00:00:39.340
It was created to reinvigorate
in the words of its charter,

00:00:39.340 --> 00:00:41.630
to reinvigorate the inner
connection between thought

00:00:41.630 --> 00:00:43.640
and action at a high level.

00:00:43.640 --> 00:00:46.780
Conversations like these
addressing challenges facing

00:00:46.780 --> 00:00:50.960
democracies in the 21st century
are a key part of that effort.

00:00:50.960 --> 00:00:55.310
I'm going to introduce our
guest today, Anne Applebaum,

00:00:55.310 --> 00:00:56.770
who as many of you
know is a columnist

00:00:56.770 --> 00:01:00.140
for the Washington Post and
a prize-winning historian.

00:01:00.140 --> 00:01:03.040
Her degrees are from at
the bachelor's level Yale.

00:01:03.040 --> 00:01:05.330
She has a master's degree
in international relations

00:01:05.330 --> 00:01:07.270
from the London School
of Economics,

00:01:07.270 --> 00:01:10.770
and Georgetown granted
her an honorary doctorate.

00:01:10.770 --> 00:01:14.570
She's a professor of practice at
the London School of Economics

00:01:14.570 --> 00:01:18.790
where she runs Arena, a research
project on disinformation,

00:01:18.790 --> 00:01:23.400
and she's the author of
Gulag, a History in 2004,

00:01:23.400 --> 00:01:26.610
which won the Pulitzer's
Prize for nonfiction.

00:01:26.610 --> 00:01:31.060
She won the Cundill Hisotry
Prize in 2013 for Iron Curtain,

00:01:31.060 --> 00:01:35.150
the Crushing of Eastern
Europe 1944 to 1956.

00:01:35.150 --> 00:01:39.540
And just this year she won
the Lionel Gelber Prize

00:01:39.540 --> 00:01:42.570
for Red Famine, Stalin's
War on Ukraine.

00:01:42.570 --> 00:01:44.950
We'll have a book signing
afterward next door

00:01:44.950 --> 00:01:47.740
in the Woodall Pavilion
for Red Famine.

00:01:47.740 --> 00:01:49.300
And I wanted to start out
to give you an opportunity

00:01:49.300 --> 00:01:51.570
to tell us what that
book's about

00:01:51.570 --> 00:01:54.760
and why it would
be relevant to us.

00:01:54.760 --> 00:01:56.060
&gt;&gt; Anne Applebaum: Well
first of all, before that,

00:01:56.060 --> 00:01:57.360
I should say thank you.

00:01:57.360 --> 00:02:00.330
I'm really delighted to be
at the Library of Congress.

00:02:00.330 --> 00:02:03.140
It's a place where I've
done research myself

00:02:03.140 --> 00:02:05.200
for my book on the Gulag.

00:02:05.200 --> 00:02:08.460
You have a pretty extensive
collection of dissident material

00:02:08.460 --> 00:02:11.690
that I was able to use,
Russian dissident, I should say,

00:02:11.690 --> 00:02:12.990
not American dissident.

00:02:12.990 --> 00:02:14.870
Although you probably
have that too somewhere.

00:02:14.870 --> 00:02:16.660
&gt;&gt; John Haskell: I think so.

00:02:16.660 --> 00:02:19.350
&gt;&gt; Anne Applebaum:
Somewhere, American samizdat.

00:02:19.350 --> 00:02:21.870
So thank you for
inviting me here.

00:02:21.870 --> 00:02:28.320
Red Famine is a history
of the Ukrainian famine.

00:02:28.320 --> 00:02:30.680
It's, and it's also an argument

00:02:30.680 --> 00:02:32.830
about the Ukrainian
famine and why it happened.

00:02:32.830 --> 00:02:39.570
And this was a famine that took
place in the years 1932-33,

00:02:39.570 --> 00:02:42.670
and the book argues
that it was, well,

00:02:42.670 --> 00:02:46.260
we've known for a long time that
it was not an accidental famine.

00:02:46.260 --> 00:02:47.770
So it was not caused
by the weather.

00:02:47.770 --> 00:02:50.790
It was not caused by bad crops.

00:02:50.790 --> 00:02:52.150
It was not caused by insects.

00:02:52.150 --> 00:02:54.980
It was caused by a deliberate
set of decisions taken

00:02:54.980 --> 00:02:59.370
by Stalin designed to weaken
the Ukrainian peasantry,

00:02:59.370 --> 00:03:01.920
and it result, and it
was literally caused

00:03:01.920 --> 00:03:04.230
by confiscation of food.

00:03:04.230 --> 00:03:05.900
So when you confiscate
people's food,

00:03:05.900 --> 00:03:07.200
when you take their food away,

00:03:07.200 --> 00:03:10.280
and then as the Soviet
Union did, you block roads

00:03:10.280 --> 00:03:11.960
so that peasants can't
get to the cities,

00:03:11.960 --> 00:03:13.840
and people couldn't leave
the Ukrainian public.

00:03:13.840 --> 00:03:16.130
Then you have a lot
of people die.

00:03:16.130 --> 00:03:18.340
So the book is an argument
about why that happened,

00:03:18.340 --> 00:03:21.970
why Stalin did that, why
Ukraine, it's a little bit

00:03:21.970 --> 00:03:23.700
of a potted history of Ukraine.

00:03:23.700 --> 00:03:26.170
It explains what
Ukrainian nationalism was

00:03:26.170 --> 00:03:27.620
with the Ukrainian
national movement was,

00:03:27.620 --> 00:03:29.490
why Stalin disliked it.

00:03:29.490 --> 00:03:35.110
And I suppose it leads into our,
into our current subject now

00:03:35.110 --> 00:03:38.750
in ways too because one of the
reasons why it was possible,

00:03:38.750 --> 00:03:41.290
one of the reasons why it
was possible to persuade

00:03:41.290 --> 00:03:43.840
to confiscate the food
of starving people,

00:03:43.840 --> 00:03:47.540
because of course, it's
required a massive operation,

00:03:47.540 --> 00:03:52.230
was that it followed
a decade-long campaign

00:03:52.230 --> 00:03:55.810
against both the so-called
Kulaks, which presence and also

00:03:55.810 --> 00:03:58.310
against Ukraine and Ukrainians.

00:03:58.310 --> 00:04:02.170
And one of the things you
learn when you study Stalinism,

00:04:02.170 --> 00:04:04.930
I've now written three books
about, essentially about Stalin

00:04:04.930 --> 00:04:08.560
and his way of thinking and
his way of occupying countries,

00:04:08.560 --> 00:04:10.360
is that none of the violence

00:04:10.360 --> 00:04:13.490
that is possible would have been
possible without the propaganda

00:04:13.490 --> 00:04:15.130
that prepared the violence.

00:04:15.130 --> 00:04:17.420
And so you immediately begin
to think about how it is

00:04:17.420 --> 00:04:19.740
that you change people's
minds, how you prepare them

00:04:19.740 --> 00:04:25.800
to do terrible things, how you
convince them to accept certain,

00:04:25.800 --> 00:04:28.320
you know, a Stalinist
totalitarian structure

00:04:28.320 --> 00:04:29.620
of the country.

00:04:29.620 --> 00:04:33.680
And that gets you thinking a
lot about political language

00:04:33.680 --> 00:04:35.180
and how it's used
and who's using it.

00:04:35.180 --> 00:04:38.680
And that's, in my head,
one of the parallels

00:04:38.680 --> 00:04:40.870
between studying the history
of the past and then trying

00:04:40.870 --> 00:04:43.080
to understand the
politics of the present.

00:04:43.080 --> 00:04:46.400
&gt;&gt; John Haskell: So, you
grew up in Washington D.C.

00:04:46.400 --> 00:04:49.490
and became an expert
on the Soviet Union

00:04:49.490 --> 00:04:51.350
and Russia and Eastern Europe.

00:04:51.350 --> 00:04:52.650
How did that happen?

00:04:52.650 --> 00:04:55.190
How did you get interested
in that, and how did --

00:04:55.190 --> 00:04:56.490
&gt;&gt; Anne Applebaum: Bad luck.

00:04:56.490 --> 00:04:59.430
I mean there are different
ways to answer that.

00:04:59.430 --> 00:05:02.980
So I was at university
in the 1980s,

00:05:02.980 --> 00:05:04.760
early 80s when it was
kind of the height

00:05:04.760 --> 00:05:08.240
of what we call the new Cold
War as Reagan was president.

00:05:08.240 --> 00:05:11.040
And the Soviet Union seemed
like this urgent problem

00:05:11.040 --> 00:05:12.340
that had to be solved.

00:05:12.340 --> 00:05:14.390
So like lots of people,
I studied Russian.

00:05:14.390 --> 00:05:16.680
I had another idea which is was,

00:05:16.680 --> 00:05:19.100
I had been a fairly
pretentious teenager,

00:05:19.100 --> 00:05:21.930
and my favorite writer
was Nabokov,

00:05:21.930 --> 00:05:24.890
and I had this idea I would be
able to read him in Russian.

00:05:24.890 --> 00:05:26.710
And one of the horrible
ironies is

00:05:26.710 --> 00:05:28.190
that although I do
speak Russian, I read it

00:05:28.190 --> 00:05:30.560
and use it all the time,
and I can read Tolstoy.

00:05:30.560 --> 00:05:31.930
He's pretty, he's a
very clear writer.

00:05:31.930 --> 00:05:34.170
I have a lot of trouble
reading Nabokov.

00:05:34.170 --> 00:05:37.010
So that was all, he
writes in a very --

00:05:37.010 --> 00:05:38.310
&gt;&gt; John Haskell: You have
something to aspire to then.

00:05:38.310 --> 00:05:40.010
&gt;&gt; Anne Applebaum: I can
still aspire to that.

00:05:40.010 --> 00:05:42.880
But so that was another
side reason.

00:05:42.880 --> 00:05:44.810
But no, I was drawn to it.

00:05:44.810 --> 00:05:46.110
I studied it.

00:05:46.110 --> 00:05:50.770
I was lucky enough to be a
student in Leningrad in 1984.

00:05:50.770 --> 00:05:54.040
I spent a summer there when
it was still Leningrad.

00:05:54.040 --> 00:05:57.240
And so I saw the end
of the Soviet Union

00:05:57.240 --> 00:05:58.860
when it was still
the Soviet Union.

00:05:58.860 --> 00:06:01.360
And I realized only
years late, I've written

00:06:01.360 --> 00:06:04.370
about this recently, that
that was a stroke of luck

00:06:04.370 --> 00:06:08.730
because a couple of years
later, if you went to Leningrad,

00:06:08.730 --> 00:06:10.350
you would have been
there during [inaudible].

00:06:10.350 --> 00:06:11.650
It all looked different.

00:06:11.650 --> 00:06:14.650
So I was almost the last
generation of American students

00:06:14.650 --> 00:06:17.830
who saw it when it
was still the USSR

00:06:17.830 --> 00:06:19.970
and was still a totalitarian
state or aspired

00:06:19.970 --> 00:06:21.520
to be a totalitarian state.

00:06:21.520 --> 00:06:25.410
&gt;&gt; John Haskell: You ended
up in, or you were in Poland

00:06:25.410 --> 00:06:28.970
when the Berlin Wall fell,
so you'd been working there.

00:06:28.970 --> 00:06:31.360
&gt;&gt; Anne Applebaum: Right, so
then the second piece of luck,

00:06:31.360 --> 00:06:35.450
good luck, bad luck, is
that I was in the interest

00:06:35.450 --> 00:06:38.970
in the region led me to
become a freelance writer.

00:06:38.970 --> 00:06:43.860
I was a stringer in Poland
in 1988-89, and I was there

00:06:43.860 --> 00:06:46.310
when communism fell and
when the Berlin Wall fell.

00:06:46.310 --> 00:06:50.620
And after that, you know, I
was stuck with that region.

00:06:50.620 --> 00:06:51.920
I also married a Pole.

00:06:51.920 --> 00:06:53.220
&gt;&gt; John Haskell: There we go.

00:06:53.220 --> 00:06:54.870
&gt;&gt; Anne Applebaum: That was
another reason to stay there.

00:06:54.870 --> 00:07:00.380
And I've actually been living in
Central Europe on and off since,

00:07:00.380 --> 00:07:01.860
you know, for 20 years.

00:07:01.860 --> 00:07:04.860
&gt;&gt; John Haskell: So at LSE you
have this research project,

00:07:04.860 --> 00:07:06.160
Arena.

00:07:06.160 --> 00:07:07.460
Tell us a little
bit more about that.

00:07:07.460 --> 00:07:08.760
What are you doing with that?

00:07:08.760 --> 00:07:11.000
&gt;&gt; Anne Applebaum: So
Arena evolved out of,

00:07:11.000 --> 00:07:14.370
I run it with a colleague,
Peter Pomerantsev,

00:07:14.370 --> 00:07:17.710
who is a British television
producer and journalist

00:07:17.710 --> 00:07:19.130
who worked for ten
years for Russia

00:07:19.130 --> 00:07:22.980
and wrote a very brilliant book
about the Russian, the creation

00:07:22.980 --> 00:07:24.760
of the Putinist [phonetic]
propaganda state

00:07:24.760 --> 00:07:26.810
and has a wonderful title.

00:07:26.810 --> 00:07:30.270
The title is Nothing is True
and Everything is Possible.

00:07:30.270 --> 00:07:36.400
So if you want background on
how, on the change in Russia,

00:07:36.400 --> 00:07:37.800
it's a very good book to read.

00:07:37.800 --> 00:07:40.000
He and I both at the same time

00:07:40.000 --> 00:07:45.100
for slightly different reasons
got interested in the question

00:07:45.100 --> 00:07:51.330
of how Russian attempts to use
the qualities of the internet

00:07:51.330 --> 00:07:54.410
and the quality of
social media in order

00:07:54.410 --> 00:07:59.000
to promote both true and false.

00:07:59.000 --> 00:08:04.360
But pro-Russian narratives
was beginning to take off.

00:08:04.360 --> 00:08:08.750
I was interested in it from
my perch in Central Europe.

00:08:08.750 --> 00:08:11.900
I was watching Russian
political influence campaigns,

00:08:11.900 --> 00:08:13.490
the attempt to influence
elections.

00:08:13.490 --> 00:08:17.020
This, of course, all burst
into the open in 2014

00:08:17.020 --> 00:08:19.140
when Russia invaded
Ukraine, and then,

00:08:19.140 --> 00:08:23.830
I think that was the moment when
many people under, you know,

00:08:23.830 --> 00:08:25.130
some more clearly
what they were doing.

00:08:25.130 --> 00:08:26.930
Because, of course, the invasion
of Crimea, if you can remember,

00:08:26.930 --> 00:08:31.800
that was accompanied by a
massive denial, you know.

00:08:31.800 --> 00:08:33.880
We don't know who
these masked men are

00:08:33.880 --> 00:08:35.610
who are walking through Crimea.

00:08:35.610 --> 00:08:38.150
We have no idea where
they got their weapons.

00:08:38.150 --> 00:08:40.260
I think the president
of Russia even said,

00:08:40.260 --> 00:08:41.970
oh maybe they bought
them in a shop, you know.

00:08:41.970 --> 00:08:44.580
You can buy, you know,
armed personnel carriers,

00:08:44.580 --> 00:08:46.120
you know, in the safe way.

00:08:46.120 --> 00:08:51.610
But it was actually a
very effective campaign,

00:08:51.610 --> 00:08:54.190
because it confused,
certainly for long enough

00:08:54.190 --> 00:08:55.860
for them to occupy Crimea.

00:08:55.860 --> 00:08:57.650
And then it was followed

00:08:57.650 --> 00:09:00.800
by a similar campaign
attempting to divide Ukraine.

00:09:00.800 --> 00:09:03.060
And I think this was the moment
where people were oriented.

00:09:03.060 --> 00:09:05.410
We saw it a little
bit earlier than that.

00:09:05.410 --> 00:09:08.170
So we began by being interested
in this problem in trying

00:09:08.170 --> 00:09:12.420
to define how the
authoritarian states

00:09:12.420 --> 00:09:13.860
in the modern world
use language,

00:09:13.860 --> 00:09:15.620
how they're using the internet.

00:09:15.620 --> 00:09:20.280
I think one of the things
that happens when you study,

00:09:20.280 --> 00:09:23.330
you know, the Russian
disinformation campaigns online

00:09:23.330 --> 00:09:25.510
is pretty quickly you
begin to understand

00:09:25.510 --> 00:09:27.900
that the problem
isn't really Russia.

00:09:27.900 --> 00:09:30.500
The Russian state, for
historical reasons,

00:09:30.500 --> 00:09:31.800
because they've been doing this

00:09:31.800 --> 00:09:33.720
for many years is what the KGB
famously did, I mean this is

00:09:33.720 --> 00:09:37.880
in all my books, was interested
in how to use language

00:09:37.880 --> 00:09:39.580
to manipulate people
and was interested also

00:09:39.580 --> 00:09:40.880
in trying to penetrate.

00:09:40.880 --> 00:09:43.530
I mean we can remember,
you know, Soviet support

00:09:43.530 --> 00:09:45.750
for the communist
parties all over the west.

00:09:45.750 --> 00:09:47.830
Was interested in trying to get
political influence in the West.

00:09:47.830 --> 00:09:50.420
This is a very old, you know.

00:09:50.420 --> 00:09:53.810
But pretty, you know,
they were good at this

00:09:53.810 --> 00:09:55.380
because they've been
thinking about it longer

00:09:55.380 --> 00:09:57.360
because they saw the
possibilities first.

00:09:57.360 --> 00:09:59.150
But really, anything
that they do now,

00:09:59.150 --> 00:10:00.450
almost anybody else could do.

00:10:00.450 --> 00:10:03.920
And there's no, you know,
the creation of a bot farm

00:10:03.920 --> 00:10:06.310
or a trolling campaign, we
can talk about what that is

00:10:06.310 --> 00:10:09.980
in a second if you want,
is something anyone can do.

00:10:09.980 --> 00:10:11.280
You can do it in Russia.

00:10:11.280 --> 00:10:12.580
You can do it in China.

00:10:12.580 --> 00:10:13.880
You can do it in Texas.

00:10:13.880 --> 00:10:16.360
I mean, it's not a, it
doesn't require any special

00:10:16.360 --> 00:10:17.660
technological knowledge.

00:10:17.660 --> 00:10:20.020
A little bit maybe, but
not that much investment.

00:10:20.020 --> 00:10:23.030
And so it's really that, you get
really quickly to understanding

00:10:23.030 --> 00:10:25.920
that this is to do with
the nature of how we get

00:10:25.920 --> 00:10:28.960
and receive information
now more generally.

00:10:28.960 --> 00:10:31.900
And so our project
does two things.

00:10:31.900 --> 00:10:34.900
We study, we have done some
projects very specifically

00:10:34.900 --> 00:10:36.890
on Russia, looking
at Russian attempts

00:10:36.890 --> 00:10:40.390
to do political influence,
and we did one in Germany.

00:10:40.390 --> 00:10:42.630
But also looking at, you know,

00:10:42.630 --> 00:10:45.250
how might we rethink
what media does in order

00:10:45.250 --> 00:10:46.890
to reach alienated audiences.

00:10:46.890 --> 00:10:52.550
Are there ways to counter the
spread of falsehood online.

00:10:52.550 --> 00:10:55.160
We've done these experiments.

00:10:55.160 --> 00:10:58.200
We've only actually been in
existence for a year and a half.

00:10:58.200 --> 00:11:00.470
Because we, we were both working

00:11:00.470 --> 00:11:01.830
on the subject and
writing about it.

00:11:01.830 --> 00:11:04.130
And then we decided it
deserved its little seed

00:11:04.130 --> 00:11:05.430
at the university.

00:11:05.430 --> 00:11:08.150
And we were, we made an
arrangement with LSE.

00:11:08.150 --> 00:11:11.320
&gt;&gt; John Haskell: Okay, so
specifically to the topic

00:11:11.320 --> 00:11:14.070
of the conversation here today.

00:11:14.070 --> 00:11:19.240
How do you describe the threat
from disinformation broadly,

00:11:19.240 --> 00:11:22.200
and does it constitute
a paradigm shift?

00:11:22.200 --> 00:11:26.660
&gt;&gt; Anne Applebaum: So I think
the, I think that the way,

00:11:26.660 --> 00:11:33.620
again, the way in which
we get and receive

00:11:33.620 --> 00:11:37.230
and issue political information
has changed very fundamentally,

00:11:37.230 --> 00:11:40.200
and I would say that
it is a paradigm shift.

00:11:40.200 --> 00:11:46.430
And a good comparison is the
invention of the printing press.

00:11:46.430 --> 00:11:49.890
You know, it seems like a long
time ago, but if you think

00:11:49.890 --> 00:11:53.200
of what happened when, you know,

00:11:53.200 --> 00:11:56.840
instead of language being
controlled by, you know,

00:11:56.840 --> 00:12:00.520
monks and monasteries who copied
out manuscripts and handed them

00:12:00.520 --> 00:12:02.420
out to specific people,
you know,

00:12:02.420 --> 00:12:05.410
who then passed them
on more broadly.

00:12:05.410 --> 00:12:08.220
You know, there was a very
severe system of gatekeeping

00:12:08.220 --> 00:12:09.940
in terms of who controlled
information

00:12:09.940 --> 00:12:15.280
up until the printing
press was invented.

00:12:15.280 --> 00:12:17.360
And then once you had
the printing press,

00:12:17.360 --> 00:12:19.700
you had this multiple, you had
exactly the same effect you have

00:12:19.700 --> 00:12:22.750
now, which is suddenly all
kinds of people can read

00:12:22.750 --> 00:12:24.610
and get access to information.

00:12:24.610 --> 00:12:27.070
They can question what
the monks are doing.

00:12:27.070 --> 00:12:30.480
And this has both positive
and negative effects.

00:12:30.480 --> 00:12:31.780
It means that suddenly,

00:12:31.780 --> 00:12:33.870
I mean it's what led
to the reformation.

00:12:33.870 --> 00:12:35.810
It's what led to
the Protestantism.

00:12:35.810 --> 00:12:39.290
And actually, I've talked
about this recently in Austria

00:12:39.290 --> 00:12:41.160
and I could say, well, people

00:12:41.160 --> 00:12:43.910
in this room might not
like Protestantism.

00:12:43.910 --> 00:12:45.820
But maybe people in the
United States think it's okay.

00:12:45.820 --> 00:12:48.770
Anyway, there's different
views about that.

00:12:48.770 --> 00:12:51.390
But, you know, and there
are positive and negative.

00:12:51.390 --> 00:12:52.700
But one of the things
that it did do is

00:12:52.700 --> 00:12:55.460
that it then created
enormous political conflicts.

00:12:55.460 --> 00:12:58.470
And there were religious
wars in Europe.

00:12:58.470 --> 00:13:01.570
You know, well into
the next century.

00:13:01.570 --> 00:13:05.620
And, you know, it created real
political division and change.

00:13:05.620 --> 00:13:07.520
And again, in both a
positive and negative sense.

00:13:07.520 --> 00:13:10.620
And I think that's what we're
seeing social media doing is

00:13:10.620 --> 00:13:13.620
that it's given suddenly really
all of us are writers and all

00:13:13.620 --> 00:13:15.470
of us are journalists and
all of us are publishers.

00:13:15.470 --> 00:13:17.810
Anybody who writes something
on Facebook or Twitter

00:13:17.810 --> 00:13:21.200
and then passes it on is
now functionally publishing.

00:13:21.200 --> 00:13:24.850
And that means that
the, you know,

00:13:24.850 --> 00:13:28.450
the way in which people
see things, what they trust

00:13:28.450 --> 00:13:34.190
and don't trust, what they,
you know, the way they look

00:13:34.190 --> 00:13:35.700
at news has changed
very fundamentally.

00:13:35.700 --> 00:13:38.710
And really every time this has
happened in recent history,

00:13:38.710 --> 00:13:40.770
it's been accompanied by
major political changes.

00:13:40.770 --> 00:13:42.630
I mean you don't, you know,
you had religious wars

00:13:42.630 --> 00:13:45.740
in the 16th and 17th century.

00:13:45.740 --> 00:13:48.330
But actually after the
invention of radio,

00:13:48.330 --> 00:13:50.990
who were the first real
beneficiaries of radio?

00:13:50.990 --> 00:13:53.230
Who understood its power
better than anybody else,

00:13:53.230 --> 00:13:54.800
and the answer is
Hitler and Stalin,

00:13:54.800 --> 00:13:56.290
who were both obsessed
with the radio.

00:13:56.290 --> 00:13:57.690
They both used it.

00:13:57.690 --> 00:14:02.560
I mean, just as an example,
when Stalin arrived in,

00:14:02.560 --> 00:14:06.660
when he invaded, when he
arrived in Berlin in May 1945,

00:14:06.660 --> 00:14:09.470
the absolute first thing he did
before he did anything else was

00:14:09.470 --> 00:14:10.770
take over the radio station.

00:14:10.770 --> 00:14:12.230
I mean even before they
really finished fighting

00:14:12.230 --> 00:14:13.530
in other parts of the city.

00:14:13.530 --> 00:14:14.830
&gt;&gt; John Haskell: And we
had somebody who didn't

00:14:14.830 --> 00:14:16.130
like the reformation,
Father Coughlin,

00:14:16.130 --> 00:14:17.700
was also, he was a radio guy.

00:14:17.700 --> 00:14:19.000
&gt;&gt; Anne Applebaum: Yes.

00:14:19.000 --> 00:14:20.300
Yes, no, no,

00:14:20.300 --> 00:14:21.600
I mean understanding
how to use radio was.

00:14:21.600 --> 00:14:23.880
But then of course,
what was the reaction?

00:14:23.880 --> 00:14:25.650
Well one was that Franklin
Roosevelt also learned

00:14:25.650 --> 00:14:26.950
to use the radio.

00:14:26.950 --> 00:14:30.780
The BBC in Britain is a
very interesting reaction

00:14:30.780 --> 00:14:33.010
to the radio, because the
British state suddenly said

00:14:33.010 --> 00:14:36.110
right, how do we channel
this in a positive way?

00:14:36.110 --> 00:14:40.420
How do we bring people who
live in the Shetland Islands

00:14:40.420 --> 00:14:43.110
and then Cornwall into
a national conversation?

00:14:43.110 --> 00:14:45.690
And then they just, you know,
the BBC was partly a creation

00:14:45.690 --> 00:14:51.210
of this fear that, of a social
breakdown that would be created

00:14:51.210 --> 00:14:54.440
if we don't think, if we don't
find ways to use it positively.

00:14:54.440 --> 00:14:55.940
And, you know, different
countries came

00:14:55.940 --> 00:14:59.310
up with different answers to
the disorganization of radio.

00:14:59.310 --> 00:15:01.010
But that's all a segue.

00:15:01.010 --> 00:15:02.490
I didn't want to get too
much into the history

00:15:02.490 --> 00:15:04.200
of saying I think we're
living through exactly

00:15:04.200 --> 00:15:06.970
that same kind of moment.

00:15:06.970 --> 00:15:11.080
And it has, again,
both very positive

00:15:11.080 --> 00:15:12.380
and very negative effects.

00:15:12.380 --> 00:15:14.910
I mean technology, the
technology itself is neutral.

00:15:14.910 --> 00:15:18.840
I mean I'm not anti-internet
or anti-social media.

00:15:18.840 --> 00:15:22.820
But neither am I a Utopian
who thinks that, you know,

00:15:22.820 --> 00:15:25.240
now that we have social
media we're all connected.

00:15:25.240 --> 00:15:26.540
Everything will be better

00:15:26.540 --> 00:15:27.840
because clearly,
that's not the case.

00:15:27.840 --> 00:15:30.240
I mean one of the
effects, for example,

00:15:30.240 --> 00:15:34.210
is that we now have effectively
a global media space.

00:15:34.210 --> 00:15:39.640
So whereas in the past, in
the Soviet era, if, you know,

00:15:39.640 --> 00:15:43.840
the KGB wanted to try and
create a rumor, for example,

00:15:43.840 --> 00:15:46.680
there's a wonderful
example in the 1980s.

00:15:46.680 --> 00:15:49.690
The KGB staged this huge
operation designed to create

00:15:49.690 --> 00:15:53.550
and push the rumor that
the CIA had created AIDS.

00:15:53.550 --> 00:15:55.090
It was a famous story.

00:15:55.090 --> 00:15:56.920
And they tried to plant
it in different presses,

00:15:56.920 --> 00:15:58.430
and they found sympathetic
journalists

00:15:58.430 --> 00:15:59.730
who would write about it.

00:15:59.730 --> 00:16:01.030
And, you know, they
had different,

00:16:01.030 --> 00:16:03.670
but this was very difficult
to spread this rumor.

00:16:03.670 --> 00:16:04.970
They did it actually.

00:16:04.970 --> 00:16:07.920
They succeeded in convincing
people in Malaysia or wherever.

00:16:07.920 --> 00:16:10.210
But, you know, but it
was a tedious process,

00:16:10.210 --> 00:16:11.550
and it took many months.

00:16:11.550 --> 00:16:14.480
So now, you want
to create a rumor?

00:16:14.480 --> 00:16:18.030
You know, you create, you know,

00:16:18.030 --> 00:16:20.800
ten fake websites
that support it.

00:16:20.800 --> 00:16:23.060
You, you know, have
them echo one another.

00:16:23.060 --> 00:16:25.800
You create, you know, a bot farm

00:16:25.800 --> 00:16:28.570
or a troll farm that
can push it out.

00:16:28.570 --> 00:16:30.450
And you can do it in an hour.

00:16:30.450 --> 00:16:31.810
&gt;&gt; John Haskell: Because
there are rumors about Zika

00:16:31.810 --> 00:16:33.840
that could be analogous
to the one about --

00:16:33.840 --> 00:16:35.140
&gt;&gt; Anne Applebaum: Right, I
mean there are lots of it.

00:16:35.140 --> 00:16:38.230
I mean now that, now the
amount of rumor and falsehood

00:16:38.230 --> 00:16:42.300
that we see, you know,
every day, is extraordinary.

00:16:42.300 --> 00:16:44.360
This is not to say that
there were no problems

00:16:44.360 --> 00:16:45.660
with the previous media model.

00:16:45.660 --> 00:16:48.140
And I'm not here to, even though
I work for the Washington Post,

00:16:48.140 --> 00:16:49.680
I don't advocate
returning to it.

00:16:49.680 --> 00:16:51.550
I don't think we're going to
put it back together again.

00:16:51.550 --> 00:16:54.650
I don't have any nostalgia
for Walter Cronkite.

00:16:54.650 --> 00:16:58.090
But nevertheless, we
live in this new world,

00:16:58.090 --> 00:17:00.030
and it's important to
understand, you know,

00:17:00.030 --> 00:17:01.560
the positive and negative.

00:17:01.560 --> 00:17:04.590
&gt;&gt; John Haskell: So do
the major media sources,

00:17:04.590 --> 00:17:05.980
some people call them
mainstream media,

00:17:05.980 --> 00:17:10.010
but the major media
sources, they're,

00:17:10.010 --> 00:17:12.120
I don't know whether I want
to put intention into this,

00:17:12.120 --> 00:17:15.610
but aren't they meant to be
a bull work against this?

00:17:15.610 --> 00:17:17.830
Why aren't they serving

00:17:17.830 --> 00:17:21.320
as a bull work against,
you know, fake news?

00:17:21.320 --> 00:17:22.620
Or are they?

00:17:22.620 --> 00:17:24.230
&gt;&gt; Anne Applebaum:
They try, you know.

00:17:24.230 --> 00:17:26.050
There are lots of
people have experimented

00:17:26.050 --> 00:17:28.700
with doing factchecking,
creating factchecking websites

00:17:28.700 --> 00:17:30.860
or having special
factcheckers on the pages.

00:17:30.860 --> 00:17:35.550
You know, but that, and
that works up to a point.

00:17:35.550 --> 00:17:38.830
It works with those audiences
who trust the fact checkers

00:17:38.830 --> 00:17:41.600
and who trust, I don't
know, the Washington Post.

00:17:41.600 --> 00:17:43.240
But it does not work
with audiences

00:17:43.240 --> 00:17:44.770
who don't trust the
Washington Post.

00:17:44.770 --> 00:17:47.510
Which there are, you
know, very large numbers.

00:17:47.510 --> 00:17:53.110
And so the problem can't really
be solved by institutions

00:17:53.110 --> 00:17:57.360
that don't, you know, lots of
people don't read or don't see

00:17:57.360 --> 00:17:58.960
or don't believe actually.

00:17:58.960 --> 00:18:03.690
So, I don't think it's a problem
that the mainstream media,

00:18:03.690 --> 00:18:06.040
so-called mainstream
media, can solve by itself.

00:18:06.040 --> 00:18:08.910
&gt;&gt; John Haskell: And so is it,
are we at a point then where,

00:18:08.910 --> 00:18:12.910
I don't know, objectivity is
a loaded term in some ways,

00:18:12.910 --> 00:18:15.110
but where the concept
of objectivity

00:18:15.110 --> 00:18:19.650
or at least evenhandedness, you
know, doesn't hold any sway?

00:18:19.650 --> 00:18:23.640
Is it at that level
do you think?

00:18:23.640 --> 00:18:24.940
&gt;&gt; Anne Applebaum: You know,

00:18:24.940 --> 00:18:27.000
I think it's a little
bit different than that.

00:18:27.000 --> 00:18:29.590
The problem is that there
is no public agreement

00:18:29.590 --> 00:18:32.730
on what objectivity and
evenhandedness means.

00:18:32.730 --> 00:18:36.380
And you know, there are
certainly still people working

00:18:36.380 --> 00:18:39.700
in journalism who believe very
much that they can do that.

00:18:39.700 --> 00:18:42.590
I mean I have colleagues
who, I'm myself a columnist.

00:18:42.590 --> 00:18:45.900
And so this isn't my problem
as much as it is others.

00:18:45.900 --> 00:18:47.200
But yes, certainly
there are people

00:18:47.200 --> 00:18:50.110
that believe absolutely that,
you know, their function

00:18:50.110 --> 00:18:54.410
in life is not to promote a
political, you know, narrative

00:18:54.410 --> 00:18:56.060
or a political candidate.

00:18:56.060 --> 00:18:57.970
But what they're supposed
to do is, you know,

00:18:57.970 --> 00:18:59.940
tell you what happened
yesterday.

00:18:59.940 --> 00:19:03.910
And that's very much for a
large piece of the press,

00:19:03.910 --> 00:19:06.290
it motivates a lot of people to
go into journalism, you know.

00:19:06.290 --> 00:19:08.940
People who want to look
for the truth and try

00:19:08.940 --> 00:19:10.510
and bring honesty
into public life.

00:19:10.510 --> 00:19:13.690
And if you ask people what their
motives are, that's often it.

00:19:13.690 --> 00:19:16.210
So I don't think it's
dead as an ideal,

00:19:16.210 --> 00:19:19.230
but I think you would find,
certainly in this country,

00:19:19.230 --> 00:19:21.000
but also across Europe,
you know,

00:19:21.000 --> 00:19:24.770
you would find differing views
about whether that's possible.

00:19:24.770 --> 00:19:27.760
I actually had a conversation,

00:19:27.760 --> 00:19:29.640
to me it was quite a
shocking conversation,

00:19:29.640 --> 00:19:32.620
but it was with a Hungarian sort

00:19:32.620 --> 00:19:36.570
of former colleague a few
weeks ago who just said

00:19:36.570 --> 00:19:40.990
to me point blank, oh, you, you
know, you Americans who believe

00:19:40.990 --> 00:19:43.930
in this nonsense about
objective journalism.

00:19:43.930 --> 00:19:46.480
You know, we all know that
it's all one side or the other.

00:19:46.480 --> 00:19:48.640
And, you know, stop pretending.

00:19:48.640 --> 00:19:50.890
Which is interesting
because the, you know,

00:19:50.890 --> 00:19:53.780
the attack on the
possibility of good journalism.

00:19:53.780 --> 00:19:57.100
The attack on the
independence of the judiciary.

00:19:57.100 --> 00:20:02.040
The attack on the, you know,
neutrality of civil servants.

00:20:02.040 --> 00:20:04.950
All these kinds of attacks
are we have heard before,

00:20:04.950 --> 00:20:06.250
and this is exactly.

00:20:06.250 --> 00:20:08.990
I mean when you look at the
history of Soviet communism,

00:20:08.990 --> 00:20:11.720
one of the, you know, you
would hear Lenin talk a lot

00:20:11.720 --> 00:20:14.340
about bourgeois, so-called
bourgeois democracy

00:20:14.340 --> 00:20:17.460
and its fake institutions
and its pretend media,

00:20:17.460 --> 00:20:19.800
which is really in the
service of somebody or another.

00:20:19.800 --> 00:20:22.520
And it's really all a reflection
of the power structure.

00:20:22.520 --> 00:20:26.830
You know, and on the right,
you would have, you know,

00:20:26.830 --> 00:20:29.410
this is very much how
fascists spoke as well.

00:20:29.410 --> 00:20:31.540
You know, we don't believe
in these fake institutions.

00:20:31.540 --> 00:20:36.850
So, these kinds of, this dislike
of any possibility of neutrality

00:20:36.850 --> 00:20:41.290
or honesty in public life is
not new, and it certainly has,

00:20:41.290 --> 00:20:43.510
you know, I do not believe
that we are now living

00:20:43.510 --> 00:20:46.270
through the 1930s by any means.

00:20:46.270 --> 00:20:48.630
But, you know, you can
certainly see the possibility,

00:20:48.630 --> 00:20:50.770
the negative possibilities
that could come from this.

00:20:50.770 --> 00:20:53.430
&gt;&gt; John Haskell: So I want
to move on to some of the,

00:20:53.430 --> 00:20:55.150
a few of the mechanics
of disinformation.

00:20:55.150 --> 00:21:00.590
Before we get there, I just
want to, is it possible

00:21:00.590 --> 00:21:04.090
that we're overreacting, that
the discourse isn't really

00:21:04.090 --> 00:21:07.720
as poisoned as what you
might be suggesting?

00:21:07.720 --> 00:21:10.600
&gt;&gt; Anne Applebaum: I mean,
you know, it's always possible

00:21:10.600 --> 00:21:12.550
that we're overreacting.

00:21:12.550 --> 00:21:17.480
But it's, you know,
maybe better to see

00:21:17.480 --> 00:21:20.180
and acknowledge the problems and
try and deal with them rather

00:21:20.180 --> 00:21:22.050
than ignore them and
pretend they don't exist.

00:21:22.050 --> 00:21:24.860
I mean I actually had
just with the narrow point

00:21:24.860 --> 00:21:27.860
of Russian disinformation, I had
for a long time people saying

00:21:27.860 --> 00:21:31.350
to me, oh, maybe this is
a problem for some people

00:21:31.350 --> 00:21:34.110
in Poland or Ukraine, but
this isn't a real problem.

00:21:34.110 --> 00:21:36.290
It doesn't really
exist anywhere else.

00:21:36.290 --> 00:21:38.770
And it was a kind of, even
in this city when you talked

00:21:38.770 --> 00:21:41.650
to people in Congress about
it, and you got a kind of well,

00:21:41.650 --> 00:21:43.290
this is a sort of
third-rate issue.

00:21:43.290 --> 00:21:45.730
And we're sort of interested
in it but not really.

00:21:45.730 --> 00:21:49.880
I did find after the
last US election,

00:21:49.880 --> 00:21:52.690
a major change in
how people see it.

00:21:52.690 --> 00:21:54.110
And suddenly, people
understood okay,

00:21:54.110 --> 00:21:55.930
this is what the Russians do.

00:21:55.930 --> 00:21:58.740
They create Facebook
pages designed that we,

00:21:58.740 --> 00:22:02.990
most people will have seen the
description of how that works.

00:22:02.990 --> 00:22:06.600
And they create in
their, you know,

00:22:06.600 --> 00:22:09.300
Black Lives Matter's
Facebook pages on the one hand

00:22:09.300 --> 00:22:10.950
and anti-immigrant on the other.

00:22:10.950 --> 00:22:13.140
And they try and stage
and create conflict.

00:22:13.140 --> 00:22:16.550
And now that people have seen oh
yeah, that's how it worked here,

00:22:16.550 --> 00:22:17.850
there's more interest

00:22:17.850 --> 00:22:19.240
in understanding how it
works around the world.

00:22:19.240 --> 00:22:22.840
But you, it's almost as if
people here had to experience it

00:22:22.840 --> 00:22:25.070
or see it for themselves
before they believed

00:22:25.070 --> 00:22:26.370
that it was a real problem.

00:22:26.370 --> 00:22:28.680
&gt;&gt; John Haskell: But the Soviet
Union always propagandized.

00:22:28.680 --> 00:22:29.980
&gt;&gt; Anne Applebaum: Sure.

00:22:29.980 --> 00:22:31.280
&gt;&gt; John Haskell: Is this
different in some way?

00:22:31.280 --> 00:22:34.260
&gt;&gt; Anne Applebaum: So, it's
only different in that the ease

00:22:34.260 --> 00:22:36.870
and speed with which it
can be done is different.

00:22:36.870 --> 00:22:40.350
It's different in that the
kinds of language they're using

00:22:40.350 --> 00:22:42.110
and what they're working
with is different.

00:22:42.110 --> 00:22:44.230
So when you look at, I
mean this is really not

00:22:44.230 --> 00:22:45.880
about disinformation but more

00:22:45.880 --> 00:22:48.240
about Russian political
influence campaigns.

00:22:48.240 --> 00:22:51.040
For example, in Europe,
there, you know,

00:22:51.040 --> 00:22:54.810
in the olden days the Soviet
Union looked for communists

00:22:54.810 --> 00:22:56.540
and fellow travelers
that it could work

00:22:56.540 --> 00:22:57.940
with in different countries.

00:22:57.940 --> 00:23:01.450
And actually, the
modern Russian state is

00:23:01.450 --> 00:23:04.430
in a certain sense
much less ideological.

00:23:04.430 --> 00:23:06.880
They're not, you know,
they're not bound

00:23:06.880 --> 00:23:08.520
to any particular ideology.

00:23:08.520 --> 00:23:11.850
What they're looking for is
anybody on the far right,

00:23:11.850 --> 00:23:14.490
anybody on the far left,
but even, you know,

00:23:14.490 --> 00:23:16.360
they look for others as well

00:23:16.360 --> 00:23:19.260
who serve their broader
strategy in Europe.

00:23:19.260 --> 00:23:22.210
Which is kind of anti-European,
anti-European Union,

00:23:22.210 --> 00:23:24.450
anti-NATO, anti-American.

00:23:24.450 --> 00:23:27.330
I mean they seek to undermine
Western institutions.

00:23:27.330 --> 00:23:31.830
And you know, what they do,
they don't actually create.

00:23:32.900 --> 00:23:35.610
They don't, you know, they
didn't create Marine Le Pen,

00:23:35.610 --> 00:23:37.540
for example, who's
the far-right leader

00:23:37.540 --> 00:23:38.840
of the National Front in France.

00:23:38.840 --> 00:23:40.430
She's an old, she's
been in politics

00:23:40.430 --> 00:23:42.790
for decades, so was her father.

00:23:42.790 --> 00:23:45.530
They didn't create her,
invent her, but they sought

00:23:45.530 --> 00:23:47.310
to amplify her message.

00:23:47.310 --> 00:23:50.010
And that's just something that's
easier to do than it used to be.

00:23:50.010 --> 00:23:53.810
You can do it with, you
can do it by, you know,

00:23:53.810 --> 00:23:55.110
you can do it with money.

00:23:55.110 --> 00:23:56.550
They funded her political
campaign.

00:23:56.550 --> 00:24:01.130
Or you can do it online with,
you know, with fake, you know,

00:24:01.130 --> 00:24:03.870
internet users and fake
social media campaigns.

00:24:03.870 --> 00:24:06.840
So it's simply working

00:24:06.840 --> 00:24:10.030
with existing cleavages
in Western societies.

00:24:10.030 --> 00:24:11.330
They can do quite a lot.

00:24:11.330 --> 00:24:13.660
So it's not, no I don't think
it's fundamentally different,

00:24:13.660 --> 00:24:18.380
but the ground is fertile,
and it's much easier

00:24:18.380 --> 00:24:19.680
and much cheaper than it ever --

00:24:19.680 --> 00:24:20.980
&gt;&gt; John Haskell: And they're
not promoting themselves.

00:24:20.980 --> 00:24:22.280
I mean they're not
promoting communism.

00:24:22.280 --> 00:24:23.910
&gt;&gt; Anne Applebaum: No, they
don't really care if we

00:24:23.910 --> 00:24:26.370
like them or admire
them at all actually.

00:24:26.370 --> 00:24:28.270
I mean, they don't, for
example, it's very different

00:24:28.270 --> 00:24:30.160
from the Chinese who do
care, you know, whether --

00:24:30.160 --> 00:24:31.460
&gt;&gt; John Haskell: So
are they doing any

00:24:31.460 --> 00:24:32.760
self-promotion internally.

00:24:32.760 --> 00:24:34.060
I mean, if they?

00:24:34.060 --> 00:24:35.360
&gt;&gt; Anne Applebaum:
So that's different.

00:24:35.360 --> 00:24:37.870
I mean Russian propaganda
campaigns inside Russia are

00:24:37.870 --> 00:24:41.100
much, you know, really
are much more important.

00:24:41.100 --> 00:24:43.890
And those are also, by the way,
very, the language that's used

00:24:43.890 --> 00:24:46.770
in the Russian media, there was
recently a very good study done

00:24:46.770 --> 00:24:49.650
of the three main Russian
television channels,

00:24:49.650 --> 00:24:54.470
looking at what they,
you know, what they,

00:24:54.470 --> 00:24:58.810
the stories that they wrote
and publicized about Europe.

00:24:58.810 --> 00:25:00.890
And this was, and by the
way, the United States,

00:25:00.890 --> 00:25:02.490
it's a very similar story.

00:25:02.490 --> 00:25:05.300
And I can't remember off the
top of my head the numbers.

00:25:05.300 --> 00:25:07.470
I just wrote about this
a couple of days ago.

00:25:07.470 --> 00:25:11.690
But it's something like
17 times a day, you know,

00:25:11.690 --> 00:25:15.310
certainly multiple times a
day you get negative stories

00:25:15.310 --> 00:25:16.820
on Russian television
of all kinds.

00:25:16.820 --> 00:25:18.460
And they fit into
particular narratives.

00:25:18.460 --> 00:25:19.760
You know, Europe is weak.

00:25:19.760 --> 00:25:21.060
Europe is disorganized.

00:25:21.060 --> 00:25:25.230
Also, Europeans are, you
know, terrified by terrorism,

00:25:25.230 --> 00:25:27.530
and they live at home in fear.

00:25:27.530 --> 00:25:32.070
The European, you know, there's
a repeated stories, for example,

00:25:32.070 --> 00:25:34.940
you know, if you live in Europe,
your children can be taken away

00:25:34.940 --> 00:25:36.940
and given to gay families.

00:25:36.940 --> 00:25:39.130
Because there's this terror,

00:25:39.130 --> 00:25:41.400
they terrorize you
with homosexuality.

00:25:41.400 --> 00:25:43.900
I mean I know it sounds absurd,
but that is actually stories

00:25:43.900 --> 00:25:46.390
that you get on Russian
television.

00:25:46.390 --> 00:25:50.210
And at the same time, Europe is
very aggressive and Russophobic.

00:25:50.210 --> 00:25:52.810
And they're anti, yeah,
so it tries to show both.

00:25:52.810 --> 00:25:57.830
I mean the purpose of that is to
make sure that no Russians think

00:25:57.830 --> 00:26:00.720
that democracy or
European values are better,

00:26:00.720 --> 00:26:02.370
that they don't find
it attractive

00:26:02.370 --> 00:26:06.830
so that they don't try to
revolt against the authoritarian

00:26:06.830 --> 00:26:09.940
and oligarchic system
that they have.

00:26:09.940 --> 00:26:13.550
And also maybe to prepare,
you know, get people prepared

00:26:13.550 --> 00:26:15.900
for some kind of conflict.

00:26:15.900 --> 00:26:17.250
&gt;&gt; John Haskell: So
on to the mechanics,

00:26:17.250 --> 00:26:19.880
what exactly are bots,
and for those of us

00:26:19.880 --> 00:26:21.990
who are less technologically
savvy?

00:26:21.990 --> 00:26:27.510
And what other specific methods
are being used to hack or troll?

00:26:27.510 --> 00:26:33.310
&gt;&gt; Anne Applebaum: So somebody,
this is not my original,

00:26:33.310 --> 00:26:36.680
I didn't make up this breakdown.

00:26:36.680 --> 00:26:41.500
But one of the ways to think
about, you know, fakeness,

00:26:41.500 --> 00:26:43.740
I really hate the
expression fake news

00:26:43.740 --> 00:26:46.880
for all the obvious reasons,
one of the ways to think

00:26:46.880 --> 00:26:50.780
about falsity or falsehood
on the internet is to think

00:26:50.780 --> 00:26:52.320
about it in different
categories.

00:26:52.320 --> 00:26:56.030
And so first of all, there's
the category fake identities.

00:26:56.030 --> 00:26:57.900
There are, you know, people

00:26:57.900 --> 00:26:59.900
who are pretending
to be someone else.

00:26:59.900 --> 00:27:02.450
There are bots, which
are little pieces,

00:27:02.450 --> 00:27:05.420
for those who don't do these
things, these are little pieces

00:27:05.420 --> 00:27:08.970
of computer code that imitate
human social media behavior

00:27:08.970 --> 00:27:11.620
so they can be automated.

00:27:11.620 --> 00:27:15.210
You can create bot farms so that
you can create, I don't know,

00:27:15.210 --> 00:27:18.190
10,000 bots, which
will retweet certain

00:27:18.190 --> 00:27:21.210
or repost certain
kinds of messages.

00:27:21.210 --> 00:27:22.600
And some of these
are pretty crude,

00:27:22.600 --> 00:27:26.110
and you can actually
detect them pretty easily.

00:27:26.110 --> 00:27:28.160
And some of them are quite
sophisticated, you know,

00:27:28.160 --> 00:27:31.130
they'll respond to a particular
word or a particular idea,

00:27:31.130 --> 00:27:33.730
and they'll immediately
create a response.

00:27:33.730 --> 00:27:37.140
And so you can essentially
automate reaction.

00:27:37.140 --> 00:27:39.970
So you have fake people,
fake websites, you know,

00:27:39.970 --> 00:27:42.630
which pretend to be one thing
but are really something else.

00:27:42.630 --> 00:27:44.310
And that's, by the
way, very easy to do.

00:27:44.310 --> 00:27:46.530
And you know, lots of, anybody

00:27:46.530 --> 00:27:48.560
in this room could have
a fake Facebook page

00:27:48.560 --> 00:27:51.320
or a fake Twitter
page, Twitter account.

00:27:51.320 --> 00:27:52.640
It's not difficult.

00:27:52.640 --> 00:27:56.090
You might have to have some
technological capability

00:27:56.090 --> 00:28:01.700
to create a bot, but even I,
apparently not even so hard.

00:28:01.700 --> 00:28:03.000
Maybe for you and me, but --

00:28:03.000 --> 00:28:04.300
&gt;&gt; John Haskell: So you're --

00:28:04.300 --> 00:28:05.600
&gt;&gt; Anne Applebaum: No so anyway,

00:28:05.600 --> 00:28:06.900
there's this level
of fake identity.

00:28:06.900 --> 00:28:08.200
That's one thing.

00:28:08.200 --> 00:28:09.500
And then there's a second
level which is fake audiences.

00:28:09.500 --> 00:28:11.450
So you can create fake people.

00:28:11.450 --> 00:28:13.580
And then you can
give the impression

00:28:13.580 --> 00:28:15.560
that you have more
followers than you do.

00:28:15.560 --> 00:28:19.500
Or you can, you know,
seek to show

00:28:19.500 --> 00:28:20.860
that something is more popular.

00:28:20.860 --> 00:28:23.330
You can seek to make something
more popular, as I say,

00:28:23.330 --> 00:28:27.380
using automation or using
trolls, which are people

00:28:27.380 --> 00:28:29.410
who are professionally
posting things on the internet.

00:28:29.410 --> 00:28:31.610
And this is something
the Russians did before

00:28:31.610 --> 00:28:32.910
anybody else.

00:28:32.910 --> 00:28:34.210
They understood the
possibilities of that.

00:28:34.210 --> 00:28:38.060
Then they created these troll
farms in Saint Petersburg,

00:28:38.060 --> 00:28:39.990
which is actually a
wonderful image if you think

00:28:39.990 --> 00:28:42.610
about little trolls,
you know, in a farm.

00:28:42.610 --> 00:28:43.910
&gt;&gt; John Haskell: There's
no famine there right

00:28:43.910 --> 00:28:45.210
now apparently.

00:28:45.210 --> 00:28:46.510
&gt;&gt; Anne Applebaum: Right.

00:28:46.510 --> 00:28:47.810
So these, and these
are different ways.

00:28:47.810 --> 00:28:50.150
Then you have a third level,
which is actual fake stories,

00:28:50.150 --> 00:28:53.710
you know, invented stories,
which can be then promoted

00:28:53.710 --> 00:28:58.680
by these, you know, by the fake
people and the fake amplifiers.

00:28:58.680 --> 00:29:00.940
And then there's a
fourth level which is sort

00:29:00.940 --> 00:29:02.420
of entirely fake narratives.

00:29:02.420 --> 00:29:05.900
So for example, this narrative
that I just mentioned in Russia

00:29:05.900 --> 00:29:07.750
about the terrorism, you know,

00:29:07.750 --> 00:29:09.510
the government is
terrorizing people

00:29:09.510 --> 00:29:11.920
who are insufficiently pro-gay.

00:29:11.920 --> 00:29:16.990
And I'm sorry to use that, but
that's part of the narrative.

00:29:16.990 --> 00:29:19.090
So, and you know, that's
repeated again and again.

00:29:19.090 --> 00:29:20.810
And it's shown in
different kind of stories.

00:29:20.810 --> 00:29:22.660
And there are different ways
in which it's, you know,

00:29:22.660 --> 00:29:24.750
there's a version of this
that's used quite a lot

00:29:24.750 --> 00:29:28.160
in Western Europe which has to
do with immigrants, you know,

00:29:28.160 --> 00:29:29.460
immigrants are coming.

00:29:29.460 --> 00:29:31.350
They're going to
rape your daughters.

00:29:31.350 --> 00:29:32.980
You know, pictures

00:29:32.980 --> 00:29:34.890
and photographs are
shown in different ways.

00:29:34.890 --> 00:29:36.490
During the German
election it was a big,

00:29:36.490 --> 00:29:41.800
this was a big Russian theme
was an attempt to scare people

00:29:41.800 --> 00:29:43.100
and worry people and make them,

00:29:43.100 --> 00:29:45.040
so it's an attempt
to create fear.

00:29:45.040 --> 00:29:48.040
Anyway, these are
different levels of ways

00:29:48.040 --> 00:29:50.930
in which falsehood
exists on the internet.

00:29:50.930 --> 00:29:52.230
And you know, it may be

00:29:52.230 --> 00:29:56.080
that through controlling even
the problem of fake identities,

00:29:56.080 --> 00:29:59.120
you could do quite a lot to
eliminate the problem of fake.

00:29:59.120 --> 00:30:02.480
We need to begin breaking down
this problem, and if we want

00:30:02.480 --> 00:30:06.820
to really think about stopping
it, start, one wants to start

00:30:06.820 --> 00:30:08.120
with the narrative problem.

00:30:08.120 --> 00:30:09.430
So how do we fight
back against Russia?

00:30:09.430 --> 00:30:11.660
But maybe actually we should
go back further down and look

00:30:11.660 --> 00:30:14.900
at how anonymity
works on the internet.

00:30:14.900 --> 00:30:19.380
Ask whether we want there to be
that much anonymity and so on.

00:30:19.380 --> 00:30:22.370
&gt;&gt; John Haskell: So do you know
of any specific examples of any

00:30:22.370 --> 00:30:24.570
of those levels,
but say for example,

00:30:24.570 --> 00:30:27.540
at the creating a
fake audience level

00:30:27.540 --> 00:30:32.500
that Russia might be
doing now in the US?

00:30:32.500 --> 00:30:33.800
&gt;&gt; Anne Applebaum: So
actually I haven't,

00:30:33.800 --> 00:30:36.740
I mean we know what they were
doing during the US election.

00:30:36.740 --> 00:30:38.040
&gt;&gt; John Haskell: Right.

00:30:38.040 --> 00:30:40.400
&gt;&gt; Anne Applebaum: Because
that's now been published.

00:30:40.400 --> 00:30:45.350
So for example, they
created fake,

00:30:45.350 --> 00:30:48.340
I mean this was one
I was shown actually.

00:30:48.340 --> 00:30:49.640
So they created, for example,

00:30:49.640 --> 00:30:53.950
a fake Black lives Matter
Twitter account that looked

00:30:53.950 --> 00:30:58.770
like a real one and
sought to obtain audiences.

00:30:58.770 --> 00:31:01.560
And, you know, what is
the purpose of doing that?

00:31:01.560 --> 00:31:05.450
Well, the purpose was that at
some point to use the trust

00:31:05.450 --> 00:31:07.850
that people had in
that account in order

00:31:07.850 --> 00:31:10.570
to get past a message
to that audience.

00:31:10.570 --> 00:31:13.680
So they seek to create
false audiences.

00:31:13.680 --> 00:31:16.990
And again, sometimes
using real causes.

00:31:16.990 --> 00:31:18.610
I mean, of course, there's
a real Black Lives Matter.

00:31:18.610 --> 00:31:20.310
It doesn't mean it's not real.

00:31:20.310 --> 00:31:22.870
But in addition to that,
they seek to create copycat

00:31:22.870 --> 00:31:25.780
or imitation ones that they can
get followers or get audiences

00:31:25.780 --> 00:31:28.010
and then use that, the trust

00:31:28.010 --> 00:31:29.470
that they gained
to pass messages.

00:31:29.470 --> 00:31:31.070
Because one of the
important things now,

00:31:31.070 --> 00:31:34.700
if you ask what's different
about now and the past,

00:31:34.700 --> 00:31:38.750
is we have much more divided
audiences than we once did.

00:31:38.750 --> 00:31:43.150
And people now are much more
likely to get their information,

00:31:43.150 --> 00:31:45.250
they get it from
people they trust.

00:31:45.250 --> 00:31:47.390
They get it from whatever
their friends on Facebook

00:31:47.390 --> 00:31:49.990
or their cousins
who they follow.

00:31:49.990 --> 00:31:54.410
And so the, you know, the
game is to build audiences

00:31:54.410 --> 00:32:00.380
that trust you since, as we've
said, the markers of quality

00:32:00.380 --> 00:32:03.010
or the markers of trust
that used to exist are gone.

00:32:03.010 --> 00:32:04.870
I mean even somebody was
saying to me the other day,

00:32:04.870 --> 00:32:08.090
if you think about even when
you used to read, for example,

00:32:08.090 --> 00:32:10.890
mainstream newspaper, you
know, you had the front page,

00:32:10.890 --> 00:32:12.320
you know, and then you
had the sports section.

00:32:12.320 --> 00:32:15.470
And then you had the, you
know, the comics, you know,

00:32:15.470 --> 00:32:17.310
and then you had
the opinion pages.

00:32:17.310 --> 00:32:20.380
And even you would see those
visually and you would think

00:32:20.380 --> 00:32:21.680
of them all a little
bit different.

00:32:21.680 --> 00:32:23.540
Like you didn't think the
comic page was news, right.

00:32:23.540 --> 00:32:25.990
I mean, presumably.

00:32:25.990 --> 00:32:27.290
&gt;&gt; John Haskell: Right.

00:32:27.290 --> 00:32:28.590
&gt;&gt; Anne Applebaum: And when
you looked at the opinion page,

00:32:28.590 --> 00:32:30.590
the way it was structured,
I mean, for example --

00:32:30.590 --> 00:32:31.990
&gt;&gt; John Haskell: That's before
Doonesbury is what you're

00:32:31.990 --> 00:32:33.290
talking about pre-Doonesbury.

00:32:33.290 --> 00:32:34.610
&gt;&gt; Anne Applebaum: Before
Doonesbury, that's right.

00:32:34.610 --> 00:32:37.010
But if you looked
at the op ed page,

00:32:37.010 --> 00:32:38.640
you knew these were
op eds, right.

00:32:38.640 --> 00:32:40.640
There's the editorials
on the left

00:32:40.640 --> 00:32:41.940
and then the opinions
on the right.

00:32:41.940 --> 00:32:43.480
And this is different
from the news.

00:32:43.480 --> 00:32:45.020
And so here you're
reading people's opinions

00:32:45.020 --> 00:32:47.270
and interpretations and the
news is meant to be the news.

00:32:47.270 --> 00:32:48.960
This is all completely
broken down.

00:32:48.960 --> 00:32:51.080
You know, when you look at
the, you look at something

00:32:51.080 --> 00:32:52.740
that says Washington
Post on the top,

00:32:52.740 --> 00:32:55.300
you don't have any sense
immediately whether that's an op

00:32:55.300 --> 00:32:59.060
ed or it's a news story or
it's a joke or it's a parody.

00:32:59.060 --> 00:33:06.430
And so, you know, the
hierarchies are broken down,

00:33:06.430 --> 00:33:11.370
and so people, you know, when
people decide what they're going

00:33:11.370 --> 00:33:13.990
to trust, you know, they
often rely on their friends

00:33:13.990 --> 00:33:17.130
or they rely on, you know,
certain kinds of, you know,

00:33:17.130 --> 00:33:18.430
language that they trust

00:33:18.430 --> 00:33:20.400
or certain kinds of
people and so on.

00:33:20.400 --> 00:33:22.720
And this is what the
Russians tried to use.

00:33:22.720 --> 00:33:24.130
And again, we're talking
about the Russians,

00:33:24.130 --> 00:33:25.430
but really anybody could do it,

00:33:25.430 --> 00:33:27.840
in order to build
particular audiences

00:33:27.840 --> 00:33:29.210
that they could then message.

00:33:29.210 --> 00:33:31.400
&gt;&gt; John Haskell: Is
China in this space then?

00:33:31.400 --> 00:33:33.540
&gt;&gt; Anne Applebaum:
So yes and no.

00:33:33.540 --> 00:33:35.880
They have a different
set of tactics.

00:33:35.880 --> 00:33:39.100
I don't think the
Chinese are interested in,

00:33:39.100 --> 00:33:43.490
for the most part, certainly in
European and American politics,

00:33:43.490 --> 00:33:47.440
they're not seeking to, you
know, undermine democracy

00:33:47.440 --> 00:33:51.870
or increase extremism or
elect particular candidates.

00:33:51.870 --> 00:33:53.370
They don't have that
interest, and they don't seem

00:33:53.370 --> 00:33:54.670
to be playing that at all.

00:33:54.670 --> 00:33:55.970
In fact, I mean the
Chinese are not interested

00:33:55.970 --> 00:33:57.270
in extremism at all.

00:33:57.270 --> 00:33:58.570
They like the world
to be very stable.

00:33:58.570 --> 00:34:00.360
They're actually
very happy with NATO.

00:34:00.360 --> 00:34:01.660
They don't want it
to fall apart.

00:34:01.660 --> 00:34:02.960
They don't want the
EU to fall apart.

00:34:02.960 --> 00:34:06.200
They like dealing with, you
know, they have, you know,

00:34:06.200 --> 00:34:07.690
the status quo suits
them in that way.

00:34:07.690 --> 00:34:09.910
Russia is a revisionist
power that doesn't

00:34:09.910 --> 00:34:12.140
like the way the
international system

00:34:12.140 --> 00:34:13.440
and wants to undermine it.

00:34:13.440 --> 00:34:14.740
So they have a different,

00:34:14.740 --> 00:34:16.060
the Chinese are doing
different things though.

00:34:16.060 --> 00:34:21.920
They are, they do seek to not
so much online but they seek

00:34:21.920 --> 00:34:23.280
to use institution,

00:34:23.280 --> 00:34:26.060
like Confucius centers
that they fund.

00:34:26.060 --> 00:34:29.600
They seek to get influence
in American universities.

00:34:29.600 --> 00:34:31.990
They have, you know,
through scholars.

00:34:31.990 --> 00:34:35.710
There are some, in some European
countries, I mean, oddly,

00:34:35.710 --> 00:34:37.240
you know, in some Central
European countries,

00:34:37.240 --> 00:34:40.860
they've made kind of targeted
investments that they seek

00:34:40.860 --> 00:34:42.160
to then use to get some.

00:34:42.160 --> 00:34:44.070
But when they have
political influence,

00:34:44.070 --> 00:34:47.390
what they're interested
in is, for example,

00:34:47.390 --> 00:34:49.430
discouraging country X or Y

00:34:49.430 --> 00:34:51.130
from having a relationship
with the Dalai Lama.

00:34:51.130 --> 00:34:54.690
I mean they have particular
political goals that they care

00:34:54.690 --> 00:34:57.180
about that are more
to do with them.

00:34:57.180 --> 00:34:58.480
So it's more.

00:34:58.480 --> 00:34:59.780
&gt;&gt; John Haskell: Right.

00:34:59.780 --> 00:35:01.480
&gt;&gt; Anne Applebaum: And it's,
they don't have this, you know,

00:35:01.480 --> 00:35:03.380
this Russian style
interest in kind

00:35:03.380 --> 00:35:04.680
of upsetting the apple cart.

00:35:04.680 --> 00:35:07.070
&gt;&gt; John Haskell: So as a
practical policy matter,

00:35:07.070 --> 00:35:08.370
is the --

00:35:08.370 --> 00:35:09.670
&gt;&gt; Anne Applebaum: I
think they're different.

00:35:09.670 --> 00:35:10.970
&gt;&gt; John Haskell: Yeah,

00:35:10.970 --> 00:35:15.320
but is the US government
tracking disinformation efforts

00:35:15.320 --> 00:35:17.380
in our elections yet?

00:35:17.380 --> 00:35:18.680
&gt;&gt; Anne Applebaum: No.

00:35:18.680 --> 00:35:22.570
I mean, there are pieces of the
US government, I mean, actually,

00:35:22.570 --> 00:35:24.630
a lot of pieces of the US
government understand this issue

00:35:24.630 --> 00:35:26.640
quite well.

00:35:26.640 --> 00:35:30.720
Certainly, people, you
know, at the Pentagon

00:35:30.720 --> 00:35:32.020
and at the State Department,

00:35:32.020 --> 00:35:33.520
they understand particularly
this Russian problem.

00:35:33.520 --> 00:35:36.400
They, you know, for, you
know, if you're worried about,

00:35:36.400 --> 00:35:38.120
for example, American
troops that are based

00:35:38.120 --> 00:35:43.500
in the Baltic states, you think
about this problem every day.

00:35:43.500 --> 00:35:47.620
You know, you're worried
about, you know, people trying

00:35:47.620 --> 00:35:49.540
to hack your soldiers,
and at the same time,

00:35:49.540 --> 00:35:52.590
you're worried about, I don't
know, a fake story saying

00:35:52.590 --> 00:35:55.710
that American soldiers has
raped a Lithuanian girl,

00:35:55.710 --> 00:35:57.010
and what are you going
to do about that?

00:35:57.010 --> 00:35:59.280
So people are thinking about it
all the time, so very, very big

00:35:59.280 --> 00:36:02.130
and important problem.

00:36:03.200 --> 00:36:07.600
What we don't have yet is
a center in the government

00:36:07.600 --> 00:36:12.210
or a place where you could
track and monitor these things

00:36:12.210 --> 00:36:15.470
in a daily way and in
a comprehensive way.

00:36:15.470 --> 00:36:16.770
Let me put it differently.

00:36:16.770 --> 00:36:21.000
It isn't anybody's full-time
job to worry about this.

00:36:21.000 --> 00:36:24.290
There are pockets of people
who are interested and care

00:36:24.290 --> 00:36:26.650
about it, but I don't
know there's a center.

00:36:26.650 --> 00:36:29.440
I mean, in terms of our
elections, there is,

00:36:29.440 --> 00:36:33.600
I know that the state, kind of
state level, this is more to do

00:36:33.600 --> 00:36:37.020
with the mechanics of elections.

00:36:37.020 --> 00:36:40.570
State level, you know, election
offices, election commissions,

00:36:40.570 --> 00:36:42.190
are worried about
it and do think

00:36:42.190 --> 00:36:43.490
about it and talk about it.

00:36:43.490 --> 00:36:45.470
There was recently
an interesting kind

00:36:45.470 --> 00:36:49.210
of training exercises at
Harvard that was partly funded

00:36:49.210 --> 00:36:52.580
by the media tech
companies which,

00:36:52.580 --> 00:36:54.540
they did a kind of war game.

00:36:54.540 --> 00:37:01.260
So imagine it's election day
and, you know, here, you know,

00:37:01.260 --> 00:37:04.300
here are some attacks on
the system that are coming.

00:37:04.300 --> 00:37:06.170
Someone's trying to
hack your system.

00:37:06.170 --> 00:37:07.470
How do you react?

00:37:07.470 --> 00:37:09.500
And so it was a kind of training
that they run like a war game

00:37:09.500 --> 00:37:10.800
where they had, I think people

00:37:10.800 --> 00:37:12.800
from all 50 states
participating.

00:37:12.800 --> 00:37:14.250
&gt;&gt; John Haskell: Are
you aware of the status

00:37:14.250 --> 00:37:15.960
of what academic
research is on this?

00:37:15.960 --> 00:37:17.760
I know you're doing some
of this at LSE, but --

00:37:17.760 --> 00:37:20.090
&gt;&gt; Anne Applebaum: So
there is academic research.

00:37:20.090 --> 00:37:21.940
It's, again, it's
still pretty scattered.

00:37:21.940 --> 00:37:23.970
This is a very new
subject and issue.

00:37:23.970 --> 00:37:26.700
But there are some very good
people both in this country

00:37:26.700 --> 00:37:30.610
and elsewhere who have begun
trying to analyze social media.

00:37:30.610 --> 00:37:35.330
One of the problems
is that the some is,

00:37:35.330 --> 00:37:37.380
so Twitter is quite
easy to analyze.

00:37:37.380 --> 00:37:39.180
Facebook is very difficult.

00:37:39.180 --> 00:37:41.900
Facebook has not made its
data accessible to academics.

00:37:41.900 --> 00:37:45.130
I think they're under,
after the political pressure

00:37:45.130 --> 00:37:48.070
of the last couple of
months they're talking

00:37:48.070 --> 00:37:51.240
about in very controlled
ways, you know,

00:37:51.240 --> 00:37:54.620
in ways so that your people,
academics, researchers are blind

00:37:54.620 --> 00:37:58.650
to individuals and details and
so on, making data available

00:37:58.650 --> 00:38:01.510
so that people can understand
how some of this works.

00:38:01.510 --> 00:38:04.480
But the, you know,
look, it's a new field.

00:38:04.480 --> 00:38:06.530
&gt;&gt; John Haskell: Right, yeah.

00:38:06.530 --> 00:38:07.830
So ripe in other words.

00:38:07.830 --> 00:38:11.500
So, I mean who could or
should be fighting back

00:38:11.500 --> 00:38:12.800
against it then?

00:38:12.800 --> 00:38:15.310
I mean the government's
not involved in it except

00:38:15.310 --> 00:38:18.560
at the intel level perhaps.

00:38:18.560 --> 00:38:21.990
You know, should it be, should
tech companies be relied upon?

00:38:21.990 --> 00:38:26.220
I mean how are we, what would
make sense as a strategy,

00:38:26.220 --> 00:38:27.980
as a society to fight
back, for instance.

00:38:27.980 --> 00:38:29.280
&gt;&gt; Anne Applebaum: If
a difficult question.

00:38:29.280 --> 00:38:30.580
I've been thinking about
this for a while now.

00:38:30.580 --> 00:38:34.430
There's a, you can't, I think
it's impossible to look for sort

00:38:34.430 --> 00:38:35.990
of a silver bullet strategy.

00:38:35.990 --> 00:38:37.290
Now there's going
to be one answer,

00:38:37.290 --> 00:38:39.310
and if we could just find this
then we're going to fix it.

00:38:39.310 --> 00:38:41.340
I think there's going to
be a range of answers.

00:38:41.340 --> 00:38:45.980
I mean some of the answers could
be found by the tech companies

00:38:45.980 --> 00:38:47.280
if they wanted to find it.

00:38:47.280 --> 00:38:48.580
And if they don't
want to find it,

00:38:48.580 --> 00:38:49.880
we might have to
make them find it.

00:38:49.880 --> 00:38:52.210
And this, again, this is this
problem of fake identities

00:38:52.210 --> 00:38:54.210
and fake amplification.

00:38:54.210 --> 00:38:55.510
I mean this is something

00:38:55.510 --> 00:38:56.860
that could be fixed
technically or technologically.

00:38:56.860 --> 00:39:00.270
So there's that piece of it.

00:39:00.270 --> 00:39:05.080
There is a, you know, there's
clearly a role for media

00:39:05.080 --> 00:39:06.770
and for journalists
to think differently.

00:39:06.770 --> 00:39:08.710
How do we reach the
people who don't read us?

00:39:08.710 --> 00:39:10.980
This is an interesting problem.

00:39:10.980 --> 00:39:17.120
There is a role for civic
organizations for people who,

00:39:17.120 --> 00:39:19.930
you know, who do online
investigations of this

00:39:19.930 --> 00:39:21.510
and who do fact checking.

00:39:21.510 --> 00:39:24.640
And funding for that
and activity

00:39:24.640 --> 00:39:26.230
in that space has
bumped up a lot.

00:39:26.230 --> 00:39:29.110
Even in the last year their
foundations are now interested

00:39:29.110 --> 00:39:30.920
in that kind of problem.

00:39:30.920 --> 00:39:33.620
And I think there's obviously
there's a role for education

00:39:33.620 --> 00:39:37.530
at many levels, not
just kind of in schools.

00:39:37.530 --> 00:39:40.720
But you know, adults should
learn how to, you know,

00:39:40.720 --> 00:39:43.500
in this question of detecting
what's true and what's false.

00:39:43.500 --> 00:39:45.080
I mean we were just
talking before

00:39:45.080 --> 00:39:49.800
about how students don't
necessarily see anymore what's a

00:39:49.800 --> 00:39:51.720
good source and what's
a bad source online.

00:39:51.720 --> 00:39:53.260
I mean I think there's
same for adult,

00:39:53.260 --> 00:39:54.560
you know, almost everybody.

00:39:54.560 --> 00:39:56.470
&gt;&gt; John Haskell: Because you
and I when we were in college,

00:39:56.470 --> 00:39:59.470
we were, you go to the card
catalog, and everything,

00:39:59.470 --> 00:40:02.260
virtually everything
was a major publisher --

00:40:02.260 --> 00:40:03.560
&gt;&gt; Anne Applebaum: Right.

00:40:03.560 --> 00:40:04.860
&gt;&gt; John Haskell: or
university press.

00:40:04.860 --> 00:40:08.130
So it was relatively reputable,
and then you follow cites

00:40:08.130 --> 00:40:09.860
and book bibliographies.

00:40:09.860 --> 00:40:12.310
But today it's, you know,
and we had actually to get

00:40:12.310 --> 00:40:14.850
out of our dorm room to
go there and do that.

00:40:14.850 --> 00:40:16.150
&gt;&gt; Anne Applebaum: Right.

00:40:16.150 --> 00:40:17.450
&gt;&gt; John Haskell: As opposed

00:40:17.450 --> 00:40:18.750
to being bombarded
with all this stuff.

00:40:18.750 --> 00:40:21.500
So that's, I mean, we're the
suckers ultimately, right.

00:40:21.500 --> 00:40:24.650
And so, it's our weakness,
and that's what you.

00:40:24.650 --> 00:40:26.580
Is that what we have to address?

00:40:26.580 --> 00:40:27.880
&gt;&gt; Anne Applebaum:
That's, one of the issues

00:40:27.880 --> 00:40:29.180
that I haven't resolved

00:40:29.180 --> 00:40:31.760
in my head actually is the
question do people want

00:40:31.760 --> 00:40:33.060
to know what's true?

00:40:33.060 --> 00:40:34.990
Do they want good information?

00:40:34.990 --> 00:40:37.720
And we all think we want
it, and if you ask people,

00:40:37.720 --> 00:40:39.420
they say they want it.

00:40:39.420 --> 00:40:42.120
But how much effort are
people willing to put

00:40:42.120 --> 00:40:44.010
in in order to get it.

00:40:44.010 --> 00:40:45.860
And this then becomes
a political problem.

00:40:45.860 --> 00:40:48.980
Because if they aren't
willing to do it, I don't know,

00:40:48.980 --> 00:40:52.550
can you have democracy if people
don't care anymore whether

00:40:52.550 --> 00:40:53.850
things are true or false?

00:40:53.850 --> 00:40:58.510
It begins to be, you
know, a real challenge.

00:40:58.510 --> 00:41:01.520
And the second problem then
is can you have democracy

00:41:01.520 --> 00:41:06.060
if you don't have, so one of
the effects of social media and,

00:41:06.060 --> 00:41:10.970
you know, online media, is that,
as I said, people are now siloed

00:41:10.970 --> 00:41:12.270
in their echo chambers

00:41:12.270 --> 00:41:15.090
where different people trust
different kinds of news.

00:41:15.090 --> 00:41:17.910
Okay, but if you don't, if
there's no shared public space,

00:41:17.910 --> 00:41:20.090
if we're not all having
the same debate anymore.

00:41:20.090 --> 00:41:23.110
And this isn't about opinion,
you know, left wing-right wing.

00:41:23.110 --> 00:41:25.920
This is like, do we all agree
what happened yesterday?

00:41:25.920 --> 00:41:28.820
Right. If we don't all agree
what happened yesterday,

00:41:28.820 --> 00:41:30.580
how do we make a
policy to deal with it?

00:41:30.580 --> 00:41:32.730
Or how do we debate
it or talk about it?

00:41:32.730 --> 00:41:35.670
And finding ways to
bring back some kind

00:41:35.670 --> 00:41:37.360
of shared public space.

00:41:37.360 --> 00:41:40.220
I think that's a real crisis,
particularly in this country.

00:41:40.220 --> 00:41:41.520
I mean in some European
countries

00:41:41.520 --> 00:41:43.130
where you still have
public broadcasters,

00:41:43.130 --> 00:41:47.160
there still is the BBC, you
know, if not everybody likes it.

00:41:47.160 --> 00:41:49.450
It exists, and it's,
everyone agrees

00:41:49.450 --> 00:41:52.000
that it's a legitimate
news organization.

00:41:52.000 --> 00:41:54.710
But I mean look, in this country
we have people who, you know,

00:41:54.710 --> 00:41:56.420
I don't think there is
a national agreement

00:41:56.420 --> 00:41:59.020
about who's legitimate
television and who's not.

00:41:59.020 --> 00:42:01.870
&gt;&gt; John Haskell: So, one of
Russia's targets, of course,

00:42:01.870 --> 00:42:04.340
is Ukraine, and you've
written a good bit about it.

00:42:04.340 --> 00:42:07.370
Are they trying to do anything?

00:42:07.370 --> 00:42:09.060
&gt;&gt; Anne Applebaum: So Ukraine
is fascinating actually,

00:42:09.060 --> 00:42:11.780
because Ukraine is a
kind of petri dish.

00:42:11.780 --> 00:42:14.510
Ukraine is where all
kinds of, you know,

00:42:14.510 --> 00:42:16.890
where Russian political
influence campaigns have been

00:42:16.890 --> 00:42:19.820
tried and practiced, and almost
everything you see everywhere

00:42:19.820 --> 00:42:22.360
else has been tried at
least once in Ukraine.

00:42:22.360 --> 00:42:27.020
Whatever hacking people's
private email or creating,

00:42:27.020 --> 00:42:30.170
you know, bot nets that
will, that was all done

00:42:30.170 --> 00:42:32.280
over the last decade in Ukraine.

00:42:32.280 --> 00:42:34.690
Ii think it's, and
it's also been a kind

00:42:34.690 --> 00:42:37.750
of petri dish for responses.

00:42:37.750 --> 00:42:42.650
The first really good
and interesting kind

00:42:42.650 --> 00:42:47.830
of anti-disinformation NGO or
civil society group was created

00:42:47.830 --> 00:42:50.150
in Ukraine called Stop Fake.

00:42:50.150 --> 00:42:54.720
And Stop Fake began
using sort of techniques

00:42:54.720 --> 00:42:58.210
of online journalism to
identify when a picture was fake

00:42:58.210 --> 00:43:01.630
or when a video was fake or
when, you know, and they create,

00:43:01.630 --> 00:43:05.300
again, they created, they
sought to reach journalists,

00:43:05.300 --> 00:43:07.800
and they began looking
at identifying.

00:43:07.800 --> 00:43:11.190
This is not so much
factchecking as verification.

00:43:11.190 --> 00:43:13.270
So they're in, it's
an interesting group,

00:43:13.270 --> 00:43:14.570
and they've tried, you know,

00:43:14.570 --> 00:43:16.230
they've tried different
experiments and trying to figure

00:43:16.230 --> 00:43:17.530
out how to reach
people and how to try

00:43:17.530 --> 00:43:19.840
in different languages
and so on.

00:43:19.840 --> 00:43:21.990
You know, the Ukrainian
government has also tried some

00:43:21.990 --> 00:43:23.490
things that I think
are negative.

00:43:23.490 --> 00:43:26.900
You know, they've tried to
ban, they have banned actually.

00:43:26.900 --> 00:43:31.810
There's a Russian social
media platform called VK,

00:43:31.810 --> 00:43:34.640
which is now banned in Ukraine.

00:43:34.640 --> 00:43:38.530
And there's just, you know,
they reckon that's manipulated.

00:43:38.530 --> 00:43:40.500
So they try and do
it by banning things,

00:43:40.500 --> 00:43:41.800
and we'll see if that works.

00:43:41.800 --> 00:43:44.320
I think, you know,
probably it won't.

00:43:44.320 --> 00:43:47.780
You know, one of the
answers sometimes is okay,

00:43:47.780 --> 00:43:50.840
you need to create,
if you're being,

00:43:50.840 --> 00:43:53.920
if your society is being
undermined by, you know,

00:43:53.920 --> 00:43:55.250
negative, you know, one

00:43:55.250 --> 00:43:57.700
of the correct responses
should be well,

00:43:57.700 --> 00:44:00.520
you need a positive narrative
that attracts people.

00:44:00.520 --> 00:44:02.850
They've been maybe less
good at creating that.

00:44:02.850 --> 00:44:06.740
&gt;&gt; John Haskell: So you
have a good megaphone

00:44:06.740 --> 00:44:08.800
with writing a column
a couple times a week

00:44:08.800 --> 00:44:11.770
for the Post and other outlets.

00:44:11.770 --> 00:44:14.960
What have you told
policymakers to think about

00:44:14.960 --> 00:44:16.260
or what would you like to?

00:44:16.260 --> 00:44:18.820
We have some in the room that
we have an opportunity you

00:44:18.820 --> 00:44:20.120
can tell.

00:44:20.120 --> 00:44:21.420
&gt;&gt; Anne Applebaum:
Yeah, raise your hands.

00:44:21.420 --> 00:44:23.740
&gt;&gt; John Haskell: Maybe this
is what might be a good thing

00:44:23.740 --> 00:44:25.040
to think about.

00:44:25.040 --> 00:44:26.490
You know, when you have
an opportunity to talk

00:44:26.490 --> 00:44:27.790
to a member of Congress.

00:44:27.790 --> 00:44:29.510
What do you tell them?

00:44:29.510 --> 00:44:31.720
What would be a useful
way to think

00:44:31.720 --> 00:44:33.420
about what to do about this?

00:44:33.420 --> 00:44:35.780
&gt;&gt; Anne Applebaum: So first
of all, it would be useful

00:44:35.780 --> 00:44:39.590
to have some piece of the US
government doing this full time

00:44:39.590 --> 00:44:41.060
as its only job.

00:44:41.060 --> 00:44:44.760
And it shouldn't just be
people who do public diplomacy

00:44:44.760 --> 00:44:47.700
or press communications,
which is what it often is now.

00:44:47.700 --> 00:44:51.030
And who can begin to think full
time about the aspects of it.

00:44:51.030 --> 00:44:54.130
And then, and also beginning
to fund research into it.

00:44:54.130 --> 00:44:57.090
Which is happening a
kind of scattered way.

00:44:57.090 --> 00:45:01.170
There are foundations doing
it, but there should be more.

00:45:01.170 --> 00:45:05.260
Also, I think, I
think it's, you know,

00:45:05.260 --> 00:45:11.880
the lesson of the recent
hearings with Mark Zuckerberg is

00:45:11.880 --> 00:45:14.340
that Congress really
needs to up its game

00:45:14.340 --> 00:45:17.570
on understanding what
this is and how it works.

00:45:17.570 --> 00:45:19.160
Yeah. It's not --

00:45:19.160 --> 00:45:21.360
&gt;&gt; John Haskell: They're
disagreeing with you.

00:45:21.360 --> 00:45:25.410
&gt;&gt; Anne Applebaum: Yeah,
it's not actually that funny.

00:45:25.410 --> 00:45:26.710
You know --

00:45:26.710 --> 00:45:28.010
&gt;&gt; John Haskell:
[Inaudible] humor.

00:45:28.010 --> 00:45:29.310
&gt;&gt; Anne Applebaum: The
level of knowledge,

00:45:29.310 --> 00:45:30.780
you know, is incredibly low.

00:45:30.780 --> 00:45:35.560
You know, I mean this is
such an important and urgent

00:45:35.560 --> 00:45:38.130
and interesting problem with all
kinds of facets, both domestic

00:45:38.130 --> 00:45:43.050
and foreign policy, affecting
education, affecting research.

00:45:43.050 --> 00:45:44.470
You know, shouldn't
there be a, you know,

00:45:44.470 --> 00:45:46.050
congressional committee
devoted to this?

00:45:46.050 --> 00:45:49.270
Shouldn't we begin to think
harder about which pieces

00:45:49.270 --> 00:45:50.800
of the US government
should be doing,

00:45:50.800 --> 00:45:53.230
which pieces of the legislature.

00:45:53.230 --> 00:45:55.600
I mean I'd like to see, you
know, members of congress

00:45:55.600 --> 00:45:57.300
who do this, you know,
just like we have some

00:45:57.300 --> 00:45:58.610
who do the arms services
committee,

00:45:58.610 --> 00:46:00.420
and that's what they
think about full time.

00:46:00.420 --> 00:46:02.870
I'd like someone to be thinking
about this all the time.

00:46:02.870 --> 00:46:06.880
Because there may be, there
may have to be some regulatory

00:46:06.880 --> 00:46:08.610
or legislative pieces
of the solution.

00:46:08.610 --> 00:46:10.470
I mean certainly, there will
be, the Europeans are going

00:46:10.470 --> 00:46:11.770
to do that if we don't.

00:46:11.770 --> 00:46:14.510
So, it's time to
get on the ball.

00:46:14.510 --> 00:46:15.810
&gt;&gt; John Haskell: Well, you know,

00:46:15.810 --> 00:46:17.730
we appreciate very
much your being a part

00:46:17.730 --> 00:46:19.330
of addressing that
knowledge gap.

00:46:19.330 --> 00:46:20.980
I mean that's what
we're trying to do here.

00:46:20.980 --> 00:46:23.570
We appreciate that a
lot here at the library.

00:46:23.570 --> 00:46:25.610
So the last thing I want
to ask is, you know,

00:46:25.610 --> 00:46:28.530
you have a good streak on
winning prizes for your books.

00:46:28.530 --> 00:46:33.070
So, not to jinx anything, but
what is your next project?

00:46:33.070 --> 00:46:34.370
&gt;&gt; Anne Applebaum:
I thought you were

00:46:34.370 --> 00:46:38.040
about to say what is my prize?

00:46:38.040 --> 00:46:40.610
I don't know.

00:46:40.610 --> 00:46:41.910
I don't know.

00:46:41.910 --> 00:46:45.140
I am, I have, I've been
very involved in this issue

00:46:45.140 --> 00:46:48.530
and trying to do research on it
and also inspire a conversation

00:46:48.530 --> 00:46:50.060
about it, which is something
I hadn't really done before.

00:46:50.060 --> 00:46:52.450
I've always worked as a
journalist or as a historian,

00:46:52.450 --> 00:46:55.250
not as somebody kind
of involved in policy.

00:46:55.250 --> 00:46:56.810
So, it's been new for me.

00:46:56.810 --> 00:47:00.500
I would very much like to write
a book about the year 1989

00:47:00.500 --> 00:47:02.170
and what happened afterwards.

00:47:02.170 --> 00:47:04.360
Because we've made
a lot of assumptions

00:47:04.360 --> 00:47:08.330
about what happened then and
what happened in the 1990s.

00:47:08.330 --> 00:47:10.800
And actually, the 1990s is
a really interesting decade

00:47:10.800 --> 00:47:13.350
when you had huge transformation
in Central and Eastern Europe.

00:47:13.350 --> 00:47:16.040
And although it's been written
about, you know, by journalists

00:47:16.040 --> 00:47:18.460
at the time and in some
scattered way, I think it,

00:47:18.460 --> 00:47:21.810
it's one of those things where
the 1990s weren't interesting

00:47:21.810 --> 00:47:24.180
for a long time because it was
just kind of old news and stuff

00:47:24.180 --> 00:47:26.660
that happened that was
boring and yesterday.

00:47:26.660 --> 00:47:28.110
And now suddenly, I think we're

00:47:28.110 --> 00:47:29.850
at the moment where
it's history.

00:47:29.850 --> 00:47:32.290
And so oh now, it's time

00:47:32.290 --> 00:47:35.330
to reassess what happened
after communism fell.

00:47:35.330 --> 00:47:39.080
As you know, most of my three
larger history books have all

00:47:39.080 --> 00:47:40.380
been about Stalinism.

00:47:40.380 --> 00:47:43.680
And I think I won't
write about Stalin again.

00:47:43.680 --> 00:47:44.980
Done with Stalin.

00:47:44.980 --> 00:47:47.080
&gt;&gt; John Haskell: So we're
having a book signing

00:47:47.080 --> 00:47:49.920
on the most recent book next
door at Woodall Pavilion,

00:47:49.920 --> 00:47:52.790
so I hope you all will come
join Anne in a few minutes.

00:47:52.790 --> 00:47:54.090
But thank you very much.

00:47:54.090 --> 00:47:55.390
&gt;&gt; Anne Applebaum: Thank you.

00:47:55.390 --> 00:47:56.690
&gt;&gt; John Haskell: This
was very informative.

00:47:56.690 --> 00:47:58.080
&gt;&gt; Anne Applebaum: Thanks.

00:47:58.080 --> 00:48:00.170
[ Applause ]

00:48:00.170 --> 00:48:03.850
&gt;&gt; This has been a presentation
of the Library of Congress.

00:48:03.850 --> 00:48:06.360
Visit us at loc.gov.

