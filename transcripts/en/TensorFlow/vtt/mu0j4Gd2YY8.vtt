WEBVTT
Kind: captions
Language: en

00:00:01.290 --> 00:00:02.890
RICHARD WEI: Good
afternoon, everyone.

00:00:02.890 --> 00:00:05.891
My name's Richard Wei, and I'm
from the Google Brain Team.

00:00:05.891 --> 00:00:07.140
Today, let's talk about Swift.

00:00:10.590 --> 00:00:12.630
So traditionally,
there are two ways

00:00:12.630 --> 00:00:14.550
of building models in Python.

00:00:14.550 --> 00:00:16.610
One way is graph building.

00:00:16.610 --> 00:00:19.710
The users are required to
explicitly build a graph

00:00:19.710 --> 00:00:25.000
and run it and pass it
to a session at runtime.

00:00:25.000 --> 00:00:27.520
And the other way
is eager execution.

00:00:27.520 --> 00:00:32.640
Eager execution allows
you to write everything

00:00:32.640 --> 00:00:36.750
in the source code, and you
interpret it line by line.

00:00:36.750 --> 00:00:38.530
And it's really easy to use.

00:00:38.530 --> 00:00:40.470
But there's a tension
between graph execution

00:00:40.470 --> 00:00:41.700
and eager execution.

00:00:41.700 --> 00:00:43.660
Graph execution has
great performance,

00:00:43.660 --> 00:00:46.680
but it's not very easy
to use, especially when

00:00:46.680 --> 00:00:48.750
it comes to control flow.

00:00:48.750 --> 00:00:50.460
Eager execution is
super easy to use

00:00:50.460 --> 00:00:53.520
but can be challenging to
achieve high performance,

00:00:53.520 --> 00:00:54.570
like graph execution.

00:00:57.680 --> 00:01:02.080
Well, can we combine
usability and performance?

00:01:02.080 --> 00:01:03.710
Well, what we are
building is something

00:01:03.710 --> 00:01:06.710
that combines the ease of
use with high performance

00:01:06.710 --> 00:01:08.450
for your machine learning code.

00:01:08.450 --> 00:01:11.210
But to do that, we have to
do something more than just

00:01:11.210 --> 00:01:13.670
writing a new library.

00:01:13.670 --> 00:01:17.840
We had to innovate in
the programming language.

00:01:17.840 --> 00:01:19.430
Machine learning is
so important to us

00:01:19.430 --> 00:01:23.230
that we're willing to adopt
a programming language

00:01:23.230 --> 00:01:25.730
and entire software stack.

00:01:25.730 --> 00:01:27.800
Since we need
first-class machine

00:01:27.800 --> 00:01:32.840
learning capabilities
and primitives,

00:01:32.840 --> 00:01:35.550
not only do we need the ease
of use and high performance,

00:01:35.550 --> 00:01:37.520
we also need an
open design process

00:01:37.520 --> 00:01:40.910
driven by an established
community for the language.

00:01:40.910 --> 00:01:44.200
This brings us to the
Swift programming language.

00:01:44.200 --> 00:01:47.480
Swift is a fast, modern
programming language

00:01:47.480 --> 00:01:49.230
and is super easy to use.

00:01:49.230 --> 00:01:52.610
It's cross-platform, has a
clean syntax like Python,

00:01:52.610 --> 00:01:54.500
and it has all the
powerful capabilities,

00:01:54.500 --> 00:01:57.020
like type inference,
optionals, and supports

00:01:57.020 --> 00:01:59.225
functional programming and
all the great features.

00:02:01.830 --> 00:02:04.370
So what exactly is
Swift for TensorFlow?

00:02:04.370 --> 00:02:06.650
Well, the key components
of Swift for TensorFlow

00:02:06.650 --> 00:02:10.240
is just a magical tensor type.

00:02:10.240 --> 00:02:13.120
You can write anything
using tensors.

00:02:13.120 --> 00:02:16.570
As you would expect, you can
initialize a tensor like this,

00:02:16.570 --> 00:02:18.340
like Python.

00:02:18.340 --> 00:02:20.620
Or you can use a full
power generics to power

00:02:20.620 --> 00:02:24.360
your models, libraries,
and applications.

00:02:24.360 --> 00:02:26.530
Well, math operators,
like sigmoid and matmul

00:02:26.530 --> 00:02:29.140
are type saved generic
functions in Swift,

00:02:29.140 --> 00:02:32.080
without the need of name
spacing when you use them.

00:02:32.080 --> 00:02:35.320
When you combine things
together, it's nice and clean.

00:02:35.320 --> 00:02:41.190
The programming style looks just
like Python's eager execution.

00:02:41.190 --> 00:02:43.530
Well, as you see,
tensor just works

00:02:43.530 --> 00:02:47.250
and is part of the TensorFlow
library, written in Swift.

00:02:47.250 --> 00:02:49.740
However, it works very
differently from other language

00:02:49.740 --> 00:02:53.160
bindings because
TensorFlow, while tensor

00:02:53.160 --> 00:02:55.830
is supported as a first-class
citizen in the programming

00:02:55.830 --> 00:02:56.910
language.

00:02:56.910 --> 00:03:00.270
For this, we built a technology
called graph program extraction

00:03:00.270 --> 00:03:03.750
in the Swift compiler.

00:03:03.750 --> 00:03:09.570
In addition to the
ability to treat tensors

00:03:09.570 --> 00:03:12.330
as first-class
citizens, we also need

00:03:12.330 --> 00:03:14.820
the ability to
train models using

00:03:14.820 --> 00:03:16.880
automatic differentiation.

00:03:16.880 --> 00:03:21.900
It's graph program extraction,
using automatic differentiation

00:03:21.900 --> 00:03:26.350
and do interesting
things even with Python

00:03:26.350 --> 00:03:28.500
through interoperability.

00:03:28.500 --> 00:03:31.550
Well, all of these components
are integrated natively

00:03:31.550 --> 00:03:33.420
in the Swift
programming language,

00:03:33.420 --> 00:03:36.166
providing great productivity
for a developer.

00:03:39.570 --> 00:03:43.530
So let's start with
a TensorFlow library.

00:03:43.530 --> 00:03:48.960
The TensorFlow library has all
the common APIs like the Tensor

00:03:48.960 --> 00:03:54.341
API and data API.

00:03:54.341 --> 00:03:58.250
In data API, you can
use dataset initializer.

00:03:58.250 --> 00:04:02.000
You can use functional zip
and batch operations just

00:04:02.000 --> 00:04:06.740
like TF data in Python.

00:04:06.740 --> 00:04:10.000
Well in addition to nice-looking
TensorFlow APIs defined

00:04:10.000 --> 00:04:13.420
in Swift, you also have access
to all the raw TensorFlow

00:04:13.420 --> 00:04:14.980
APIs like this.

00:04:17.880 --> 00:04:20.850
Tensor code sits alongside
the rest of the Swift program,

00:04:20.850 --> 00:04:22.940
just like eager
execution, and it

00:04:22.940 --> 00:04:26.340
interacts with arbitrary
user-defined data structures

00:04:26.340 --> 00:04:27.780
in algorithms.

00:04:27.780 --> 00:04:30.480
But will it actually
execute line

00:04:30.480 --> 00:04:34.090
by line, one
statement at a time,

00:04:34.090 --> 00:04:36.190
just like eager execution?

00:04:36.190 --> 00:04:38.280
Well, the answer is no.

00:04:38.280 --> 00:04:40.410
The TensorFlow library
works quite differently

00:04:40.410 --> 00:04:42.690
from all the language bindings.

00:04:42.690 --> 00:04:45.720
Swift for TensorFlow is
a full-fledged compiler,

00:04:45.720 --> 00:04:48.540
and it makes our code run
fast without sacrificing

00:04:48.540 --> 00:04:51.800
the ease of use.

00:04:51.800 --> 00:04:55.580
The technology is called
graph program extraction.

00:04:55.580 --> 00:04:56.450
So how does it work?

00:05:01.370 --> 00:05:06.020
Well, we want users to write
code like eager execution,

00:05:06.020 --> 00:05:10.610
but we also want it to
graph level performance.

00:05:10.610 --> 00:05:13.460
And we want to support
native language control flow

00:05:13.460 --> 00:05:15.530
so your control flow
like ifs and whiles

00:05:15.530 --> 00:05:19.400
can be compiled directly
to a TensorFlow graph.

00:05:23.930 --> 00:05:26.600
We want the compiler
to handle data transfer

00:05:26.600 --> 00:05:29.360
between the device
or between devices,

00:05:29.360 --> 00:05:32.670
between the device and the host.

00:05:32.670 --> 00:05:35.316
So the burden is now on the
compiler, not on the developer.

00:05:35.316 --> 00:05:37.190
Developers don't even
have to think about it.

00:05:40.090 --> 00:05:41.710
So this is how it works.

00:05:41.710 --> 00:05:46.240
Well, dataset operations such as
zip and batch work as well as--

00:05:46.240 --> 00:05:52.780
well, they work in a graph.

00:05:52.780 --> 00:05:55.900
Tensor operations such
as matmul can also

00:05:55.900 --> 00:05:58.120
be represented as a graph.

00:05:58.120 --> 00:06:00.980
When Swift compiler
sees the code,

00:06:00.980 --> 00:06:07.510
the Swift compiler automatically
turns this into a graph

00:06:07.510 --> 00:06:11.530
by identifying all the
graph-extractable operations.

00:06:15.520 --> 00:06:18.510
So Swift compiler
compiles this code,

00:06:18.510 --> 00:06:21.390
compiles the
graph-extractable operations

00:06:21.390 --> 00:06:23.520
into a TensorFlow graph.

00:06:23.520 --> 00:06:27.120
And that will be
part of the binary

00:06:27.120 --> 00:06:30.600
that Swift compiler
produces so it's one graph.

00:06:34.040 --> 00:06:37.630
At the same time since
it's compiling the graph,

00:06:37.630 --> 00:06:41.875
it produces error messages
before even running your code

00:06:41.875 --> 00:06:47.110
and catches some shape errors
and all the type errors.

00:06:51.800 --> 00:06:53.710
Now with graph
program extraction,

00:06:53.710 --> 00:06:57.160
we can combine
performance and usability.

00:06:57.160 --> 00:07:01.030
But often in machine
learning code,

00:07:01.030 --> 00:07:04.190
we also need to be able
to differentiate code

00:07:04.190 --> 00:07:06.094
in train models.

00:07:06.094 --> 00:07:07.510
Well, that's one
of the key things

00:07:07.510 --> 00:07:09.320
in machine learning algorithm.

00:07:09.320 --> 00:07:11.350
A common way to do that
is to take advantage

00:07:11.350 --> 00:07:13.840
of automatic differentiation.

00:07:13.840 --> 00:07:17.020
Although most automatic
differentiation tools

00:07:17.020 --> 00:07:19.540
are an implement
as a library, we

00:07:19.540 --> 00:07:22.210
want to take full advantage
of being able to improve

00:07:22.210 --> 00:07:24.710
a programming language.

00:07:24.710 --> 00:07:26.847
So we build automatic
differentiation

00:07:26.847 --> 00:07:27.805
also into the compiler.

00:07:31.410 --> 00:07:33.000
So automatic
differentiating in Swift

00:07:33.000 --> 00:07:35.950
works on custom data structures.

00:07:35.950 --> 00:07:39.360
It works on
TensorFlow, as well as

00:07:39.360 --> 00:07:43.770
lots of standard library
functions and standard library

00:07:43.770 --> 00:07:45.040
types.

00:07:45.040 --> 00:07:47.010
And as a user, you can
define your own type

00:07:47.010 --> 00:07:48.510
to be differentiable.

00:07:48.510 --> 00:07:52.710
For example, you can define
arbitrary structs, enums,

00:07:52.710 --> 00:07:56.310
and other data types and make
them represent a vector space.

00:07:56.310 --> 00:08:02.770
And they suddenly
become differentiable.

00:08:02.770 --> 00:08:08.080
So the core of the system
is a differential operator.

00:08:08.080 --> 00:08:11.530
This is actually
a keyword in Swift

00:08:11.530 --> 00:08:14.440
because we've built
that into the language.

00:08:14.440 --> 00:08:20.440
For example, we have a 32-bit
floating point operation.

00:08:20.440 --> 00:08:21.910
It's a function.

00:08:21.910 --> 00:08:24.780
And to differentiate
its function,

00:08:24.780 --> 00:08:27.450
we just throw a
differential operator at it.

00:08:27.450 --> 00:08:30.270
And we get a gradient,
and we can call it.

00:08:30.270 --> 00:08:32.429
It's all functional.

00:08:32.429 --> 00:08:34.620
The same thing
applies to tensors.

00:08:34.620 --> 00:08:36.720
I have a tensor
inference function

00:08:36.720 --> 00:08:39.059
that computes a prediction.

00:08:39.059 --> 00:08:41.309
I can throw a differential
operator at it.

00:08:41.309 --> 00:08:44.150
I can get the prediction
by calling it,

00:08:44.150 --> 00:08:49.210
and I can also get the gradient
of it and apply the gradient.

00:08:49.210 --> 00:08:51.670
And I can use a code
like this for training.

00:08:55.510 --> 00:08:57.680
Swift also supports
custom gradients.

00:08:57.680 --> 00:09:00.890
This is a commonly
requested feature.

00:09:00.890 --> 00:09:05.370
Well for example, we
have a times operator,

00:09:05.370 --> 00:09:09.180
and we want to apply some
custom gradient on it.

00:09:09.180 --> 00:09:12.270
All we have to do is to use
a differentiable attribute

00:09:12.270 --> 00:09:17.500
to specify the reverse mode
adjoint for this function.

00:09:17.500 --> 00:09:20.400
So when automatic
differentiation sees the code,

00:09:20.400 --> 00:09:23.760
it'll automatically
call into the adjoint

00:09:23.760 --> 00:09:25.260
when it's differentiating
data flow.

00:09:28.470 --> 00:09:30.120
Well, because automatic
differentiation

00:09:30.120 --> 00:09:33.530
is language-integrated, it
shows the errors at compile time

00:09:33.530 --> 00:09:36.150
when you're differentiating
some non-differentiable function

00:09:36.150 --> 00:09:39.400
with cursors pointing exactly
at each non-differentiable

00:09:39.400 --> 00:09:41.480
operation in the call stack.

00:09:41.480 --> 00:09:42.900
It's really useful.

00:09:42.900 --> 00:09:47.640
And also in the future, we plan
to support showing warnings

00:09:47.640 --> 00:09:51.770
for numeric stability.

00:09:51.770 --> 00:09:56.210
So all of this is
enabled by having

00:09:56.210 --> 00:09:59.630
automatic differentiation
built into the compiler.

00:09:59.630 --> 00:10:01.510
But since advanced
automatic differentiation

00:10:01.510 --> 00:10:04.510
is immensely useful for
machine learning research,

00:10:04.510 --> 00:10:06.650
we have support for
arbitrary control flow,

00:10:06.650 --> 00:10:08.190
including loops and recursion.

00:10:08.190 --> 00:10:12.190
And control flow can also
enable differentiating

00:10:12.190 --> 00:10:14.560
through all the tree
data structures defined

00:10:14.560 --> 00:10:18.080
using the algebraic
data type in Swift.

00:10:18.080 --> 00:10:20.910
And we'll also support
APIs from manipulating

00:10:20.910 --> 00:10:24.520
gradient of some
variables, compile time

00:10:24.520 --> 00:10:28.000
warning for numeric
ability, and the ability

00:10:28.000 --> 00:10:29.980
to compute forward-mode
Jacobian vector

00:10:29.980 --> 00:10:31.510
products and full Jacobians.

00:10:34.400 --> 00:10:36.160
Well, with all the
features providing

00:10:36.160 --> 00:10:39.400
a super easy-to-use
programming interface in Swift,

00:10:39.400 --> 00:10:41.590
we also care a lot
about the ecosystem.

00:10:41.590 --> 00:10:43.420
Since a machine
learning ecosystem

00:10:43.420 --> 00:10:46.300
relies on a lot of
Python libraries,

00:10:46.300 --> 00:10:47.710
we don't want to
lose the ability

00:10:47.710 --> 00:10:51.370
to call into Python libraries
and use them in Swift.

00:10:51.370 --> 00:10:54.280
For that, we built Python
interoperability in Swift.

00:10:58.490 --> 00:11:03.620
You can import a Python
library using Python import.

00:11:03.620 --> 00:11:04.940
It's like Python.

00:11:04.940 --> 00:11:09.530
And you can use numpy
using Python syntax.

00:11:09.530 --> 00:11:12.220
Users can directly import their
favorite libraries directly

00:11:12.220 --> 00:11:15.730
in Swift and use them as if
they were writing Python.

00:11:15.730 --> 00:11:18.800
This gives great compatibility
with the entire machine

00:11:18.800 --> 00:11:20.880
learning ecosystem.

00:11:20.880 --> 00:11:24.660
So for Python, you can use
your favorite libraries

00:11:24.660 --> 00:11:30.390
like numpy or pickle or
gzip, load some images.

00:11:30.390 --> 00:11:33.590
It's that simple.

00:11:33.590 --> 00:11:38.530
So Swift supports interpreted
mode, scripting mode,

00:11:38.530 --> 00:11:42.110
and Jupyter notebooks.

00:11:42.110 --> 00:11:45.744
And you can write
interactive code

00:11:45.744 --> 00:11:46.910
as if you're writing Python.

00:11:50.080 --> 00:11:54.280
And we are releasing an Iris
tutorial on our website,

00:11:54.280 --> 00:11:58.720
and you can follow the
tutorial and try it out.

00:12:05.480 --> 00:12:08.114
If you want to participate
in the development,

00:12:08.114 --> 00:12:10.530
you can download a development
toolchain from our website.

00:12:13.730 --> 00:12:15.620
Everything is open source.

00:12:15.620 --> 00:12:18.290
On GitHub/tensorflow/swift,
you can

00:12:18.290 --> 00:12:21.980
find our technical
documentation, white papers,

00:12:21.980 --> 00:12:27.130
and everything, and we have an
open design process as well.

00:12:27.130 --> 00:12:30.340
So this is Swift for TensorFlow.

00:12:30.340 --> 00:12:31.780
Thank you everyone.

00:12:31.780 --> 00:12:34.217
[APPLAUSE]

