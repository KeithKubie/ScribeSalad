WEBVTT
Kind: captions
Language: en

00:00:02.410 --> 00:00:04.820
[APPLAUSE]

00:00:12.050 --> 00:00:13.310
JASON SILVA: Fun.

00:00:13.310 --> 00:00:15.430
So Doug, I want to
ask you a little bit

00:00:15.430 --> 00:00:17.850
about the themes in "Cyberia."

00:00:17.850 --> 00:00:19.750
I'm particularly
fascinated, and I've

00:00:19.750 --> 00:00:23.640
been fascinated with this
term "cyberdelics," which

00:00:23.640 --> 00:00:26.430
connects the idea of cybernetic
systems and computers

00:00:26.430 --> 00:00:28.890
and high technology with
psychedelic substances,

00:00:28.890 --> 00:00:29.935
expanding your mind.

00:00:29.935 --> 00:00:31.310
There's a lot of
people out there

00:00:31.310 --> 00:00:34.000
that think that the
computer revolution is

00:00:34.000 --> 00:00:37.290
the literalization of the
psychedelic dream of mind

00:00:37.290 --> 00:00:38.647
expansion.

00:00:38.647 --> 00:00:41.230
And that the confluence of the
counterculture and the computer

00:00:41.230 --> 00:00:43.230
culture in the early
'60s, like, in a sense

00:00:43.230 --> 00:00:44.940
sparked this world,
where it was like,

00:00:44.940 --> 00:00:46.660
these computers can
extend our minds.

00:00:46.660 --> 00:00:49.790
Computers are the new LSD,
and so on and so forth.

00:00:49.790 --> 00:00:51.591
You know this world very well.

00:00:51.591 --> 00:00:53.840
Do you want to just riff a
little bit on your thoughts

00:00:53.840 --> 00:00:54.910
on that?

00:00:54.910 --> 00:00:57.000
DOUGLAS RUSHKOFF:
Well, I don't know.

00:00:57.000 --> 00:00:59.510
For me it was kind of
a surprise, you know.

00:00:59.510 --> 00:01:04.986
I went to Princeton in the early
'80s when there weren't many,

00:01:04.986 --> 00:01:07.800
I don't know, there weren't that
many interesting people there,

00:01:07.800 --> 00:01:09.050
to use the word "interesting."

00:01:09.050 --> 00:01:11.170
And there certainly
weren't many psychedelic,

00:01:11.170 --> 00:01:13.480
counter culturally,
theater people there.

00:01:13.480 --> 00:01:20.600
And after I graduated, I
found that the other nine

00:01:20.600 --> 00:01:24.270
of the other 12 psychedelic
Princeton people

00:01:24.270 --> 00:01:28.680
had all ended up moving
to California to work

00:01:28.680 --> 00:01:30.150
in the high-tech industry.

00:01:30.150 --> 00:01:32.830
And that seemed really weird
to me because when I grew up,

00:01:32.830 --> 00:01:35.170
people who used computers
were the kids who

00:01:35.170 --> 00:01:37.720
kind of cut right angles in the
corridors of the high school

00:01:37.720 --> 00:01:38.570
and stuff.

00:01:38.570 --> 00:01:41.080
So I wanted to
find out why people

00:01:41.080 --> 00:01:47.600
who were for the most part
more stoned than I was,

00:01:47.600 --> 00:01:50.590
the true deadhead people,
would go out there.

00:01:50.590 --> 00:01:54.140
So I went out and spent
time with these people who

00:01:54.140 --> 00:01:58.280
were working at
Intel during the day

00:01:58.280 --> 00:02:02.390
and going home scraping the buds
off peyote cactuses at night.

00:02:02.390 --> 00:02:02.890
And--

00:02:02.890 --> 00:02:03.180
JASON SILVA: Wow.

00:02:03.180 --> 00:02:04.846
DOUGLAS RUSHKOFF:
--trying to figure out

00:02:04.846 --> 00:02:07.910
why were these people
attracted to it.

00:02:07.910 --> 00:02:11.310
And then why were these people
the ones being hired to do it.

00:02:11.310 --> 00:02:13.400
And what I figured
out in the early '90s

00:02:13.400 --> 00:02:18.370
was that people who had
hallucinatory experience

00:02:18.370 --> 00:02:20.680
were really the only
people who weren't

00:02:20.680 --> 00:02:25.530
afraid to build these platforms,
to build this reality.

00:02:25.530 --> 00:02:28.820
These were people who were
accustomed to imagining

00:02:28.820 --> 00:02:31.749
something that then appeared.

00:02:31.749 --> 00:02:32.540
JASON SILVA: Right.

00:02:32.540 --> 00:02:33.540
DOUGLAS RUSHKOFF: Right.

00:02:33.540 --> 00:02:37.195
So these are people who
then had to go and do,

00:02:37.195 --> 00:02:38.500
that is their job.

00:02:38.500 --> 00:02:39.210
JASON SILVA: Right
off the bat, I mean,

00:02:39.210 --> 00:02:40.250
it makes me think of this idea.

00:02:40.250 --> 00:02:42.300
You have all these companies
that are talking about

00:02:42.300 --> 00:02:44.840
creativity and out-of-the-box
and we got to get our employees

00:02:44.840 --> 00:02:45.710
to think differently.

00:02:45.710 --> 00:02:47.918
And don't look at what is
but think of what could be.

00:02:47.918 --> 00:02:50.390
Steven Johnson and his
book "Where Good Ideas

00:02:50.390 --> 00:02:53.220
Come From" talks about
the adjacent possible.

00:02:53.220 --> 00:02:56.127
And he says the adjacent
possible is a shadow future.

00:02:56.127 --> 00:02:57.710
It's this thing
that's not quite there

00:02:57.710 --> 00:02:59.662
and it hovers over the present.

00:02:59.662 --> 00:03:01.120
And so how do you
get there, right?

00:03:01.120 --> 00:03:02.911
DOUGLAS RUSHKOFF: The
problem here, though,

00:03:02.911 --> 00:03:05.720
is that deliberate and
disciplined psychedelics

00:03:05.720 --> 00:03:09.610
users bring back what they've
learned back to terra firma.

00:03:09.610 --> 00:03:10.610
JASON SILVA: Absolutely.

00:03:10.610 --> 00:03:15.190
DOUGLAS RUSHKOFF: And implement
it in real life in a real way.

00:03:15.190 --> 00:03:20.590
And there's a kind of an
implicit morality in that.

00:03:20.590 --> 00:03:26.710
And the kinds of people who
were doing computers then

00:03:26.710 --> 00:03:28.920
were the kinds of
people who shared

00:03:28.920 --> 00:03:32.440
a kind of a common
sense of values

00:03:32.440 --> 00:03:38.880
about people and weirdness and
paradox and the arts and all

00:03:38.880 --> 00:03:39.580
that.

00:03:39.580 --> 00:03:42.100
And when I look
at the people who

00:03:42.100 --> 00:03:44.840
are programming
our reality today,

00:03:44.840 --> 00:03:47.160
I see kids graduating
from Stanford,

00:03:47.160 --> 00:03:51.200
going to work for Goldman
Sachs to build algorithms

00:03:51.200 --> 00:03:55.610
that work on the stock market
to outsmart human traders.

00:03:55.610 --> 00:03:58.060
So I don't feel that that value
system's implicit anymore,

00:03:58.060 --> 00:04:05.360
that the adjacent possible is
no longer necessarily connected

00:04:05.360 --> 00:04:08.760
to the real, but is an
alternative reality.

00:04:08.760 --> 00:04:10.850
Let's build this platform.

00:04:10.850 --> 00:04:13.160
And it harkens back
to stuff McLuhan

00:04:13.160 --> 00:04:15.250
was talking about back
in his dissertation

00:04:15.250 --> 00:04:16.970
when he wrote about the trivium.

00:04:16.970 --> 00:04:19.980
That there were these
kind of two sects

00:04:19.980 --> 00:04:21.200
sort of throughout history.

00:04:21.200 --> 00:04:25.600
The grammarians who were very
concerned with sort of what is.

00:04:25.600 --> 00:04:29.320
And the dialecticians,
the constructivists,

00:04:29.320 --> 00:04:32.760
who were sort of out
there on, what can we do?

00:04:32.760 --> 00:04:33.740
What can we construct?

00:04:33.740 --> 00:04:36.080
What else can we--

00:04:36.080 --> 00:04:38.750
The constructivists,
it was very consonant

00:04:38.750 --> 00:04:41.080
with industrialization and
the Renaissance and progress

00:04:41.080 --> 00:04:42.830
and where are we going
to go, where are we

00:04:42.830 --> 00:04:44.160
going to take this thing?

00:04:44.160 --> 00:04:48.110
But you know, that eyes
on the prize mentality

00:04:48.110 --> 00:04:53.305
is kind of an abstraction when
it's not counterbalanced by--

00:04:53.305 --> 00:04:54.430
JASON SILVA: Being present?

00:04:54.430 --> 00:04:55.915
DOUGLAS RUSHKOFF:
Being present, right.

00:04:55.915 --> 00:04:56.860
Being [? by the ?] real.

00:04:56.860 --> 00:04:58.776
JASON SILVA: Yeah, and
I understand everything

00:04:58.776 --> 00:04:59.818
you're referring to here.

00:04:59.818 --> 00:05:01.776
And so it almost sounds
like what you're saying

00:05:01.776 --> 00:05:03.270
is that dream that
existed, maybe,

00:05:03.270 --> 00:05:05.230
when you wrote "Cyberia."

00:05:05.230 --> 00:05:07.140
Or the ideas that
John Markoff explores.

00:05:07.140 --> 00:05:09.640
And what the Dormouse says
when he says that this 1960s

00:05:09.640 --> 00:05:11.820
moment, this counterculture,
these computers,

00:05:11.820 --> 00:05:15.470
this dream that was
conceived and potentially

00:05:15.470 --> 00:05:17.002
to be literalized now.

00:05:17.002 --> 00:05:19.210
You're saying that the value
system that exists today

00:05:19.210 --> 00:05:19.780
is not the same.

00:05:19.780 --> 00:05:22.310
So instead of people creating
an app to transform the world,

00:05:22.310 --> 00:05:24.100
they want to create
some little app

00:05:24.100 --> 00:05:26.210
that they can, like,
put on the stock market

00:05:26.210 --> 00:05:26.860
and sell for $1 million.

00:05:26.860 --> 00:05:29.151
DOUGLAS RUSHKOFF: It became
more the quest, well, yeah.

00:05:29.151 --> 00:05:31.080
Or even in the best
case, it's the quest

00:05:31.080 --> 00:05:35.470
to make a second life, a second
life rather than this life,

00:05:35.470 --> 00:05:37.410
where we are alive.

00:05:37.410 --> 00:05:41.610
And we talk a lot
about the singularity.

00:05:41.610 --> 00:05:43.720
I get where the
singularity to me

00:05:43.720 --> 00:05:46.340
seems to be kind of
a self-loathing, kind

00:05:46.340 --> 00:05:50.530
of antihuman, zombie
apocalypse fantasy.

00:05:50.530 --> 00:05:53.360
You know, where
the story they tell

00:05:53.360 --> 00:05:59.220
is that the history of evolution
is information itself striving

00:05:59.220 --> 00:06:00.940
towards greater
states of complexity.

00:06:00.940 --> 00:06:02.650
Human beings are really
good, and culture

00:06:02.650 --> 00:06:04.750
is really good for doing that
for the last 3,000 years.

00:06:04.750 --> 00:06:06.840
But now computers are
even better at doing that.

00:06:06.840 --> 00:06:09.670
So people are only going
to be necessary insofar

00:06:09.670 --> 00:06:11.330
as we can help
computers manifest

00:06:11.330 --> 00:06:13.240
the next stage of evolution.

00:06:13.240 --> 00:06:18.090
And to argue, as I do, that
no humans matter is hubris.

00:06:18.090 --> 00:06:19.310
Who says humans matter?

00:06:19.310 --> 00:06:21.870
And I say humans matter
because I'm on team human.

00:06:21.870 --> 00:06:23.500
JASON SILVA: Well,
it's interesting

00:06:23.500 --> 00:06:26.540
when Kurzweil talks
about the singularity,

00:06:26.540 --> 00:06:28.410
and he addresses
the criticism where

00:06:28.410 --> 00:06:30.220
people say, oh,
the AIs are going

00:06:30.220 --> 00:06:31.740
to take over and displace us.

00:06:31.740 --> 00:06:35.220
He says you keep talking
about the AI as if it's them,

00:06:35.220 --> 00:06:37.260
but that's really
going to be us.

00:06:37.260 --> 00:06:39.200
I mean, he echoes
some of the ideas

00:06:39.200 --> 00:06:40.830
of David Chalmers
and Andy Clark,

00:06:40.830 --> 00:06:43.630
who says we need to get
over our skin bag bias.

00:06:43.630 --> 00:06:45.610
And that these technologies,
if you zoom out

00:06:45.610 --> 00:06:46.680
and you look at it
from the outside,

00:06:46.680 --> 00:06:48.554
these technologies are
actually a part of us.

00:06:48.554 --> 00:06:49.850
They are our second skin.

00:06:49.850 --> 00:06:51.350
Just like the
spider's web is a part

00:06:51.350 --> 00:06:53.590
of the spider, our
smartphones, our technology,

00:06:53.590 --> 00:06:56.830
our computers are like
a scaffolding of mind.

00:06:56.830 --> 00:06:59.380
I mean, when you look at
the Mars Rover on Mars,

00:06:59.380 --> 00:07:02.590
that's the human mind
is crawling Mars.

00:07:02.590 --> 00:07:05.770
The Hubble Space Telescope
gives us, allows us to mainline

00:07:05.770 --> 00:07:07.520
the whole of time
through our optic nerve.

00:07:07.520 --> 00:07:10.580
So I don't think that
we're being replaced

00:07:10.580 --> 00:07:11.670
as far as being augmented.

00:07:11.670 --> 00:07:12.940
But this is my interpretation,
I mean, we are [INAUDIBLE].

00:07:12.940 --> 00:07:13.920
DOUGLAS RUSHKOFF:
Oh, yeah, it depends.

00:07:13.920 --> 00:07:15.461
If you're part of
the camp that wants

00:07:15.461 --> 00:07:20.200
to upload human consciousness to
a silicon chip and watch it go,

00:07:20.200 --> 00:07:24.050
then you've got no problem
with skin bag bias,

00:07:24.050 --> 00:07:25.434
I'll tell you that much.

00:07:25.434 --> 00:07:26.850
But I think there's
a danger there

00:07:26.850 --> 00:07:29.840
that the problem
with constructivism,

00:07:29.840 --> 00:07:32.140
while it's great
to plan, is it's

00:07:32.140 --> 00:07:36.360
very hard to remember that
we have to keep testing it

00:07:36.360 --> 00:07:38.440
against reality.

00:07:38.440 --> 00:07:42.475
What might be has to continually
be tested against what is.

00:07:42.475 --> 00:07:45.100
It goes all the way back to the
invention of the first meeting,

00:07:45.100 --> 00:07:46.190
the invention of text.

00:07:46.190 --> 00:07:48.648
Once we could write things
down, once we could write things

00:07:48.648 --> 00:07:50.000
down, we got time travel.

00:07:50.000 --> 00:07:52.550
We could write about what
happened in the past.

00:07:52.550 --> 00:07:53.550
We got history.

00:07:53.550 --> 00:07:54.790
And we got the future.

00:07:54.790 --> 00:07:57.800
So then we got the messianism
and Moshiach and all that.

00:07:57.800 --> 00:08:00.130
And Moshiach and the future
and where we're going

00:08:00.130 --> 00:08:00.800
and tikkun olam.

00:08:00.800 --> 00:08:03.150
That's all beautiful as long
as it's constantly tested

00:08:03.150 --> 00:08:04.492
against what's happening.

00:08:04.492 --> 00:08:06.950
JASON SILVA: Well, don't you
think that language allowed us

00:08:06.950 --> 00:08:07.680
to dream?

00:08:07.680 --> 00:08:10.140
Because by conceiving of
what might be is sort of--

00:08:10.140 --> 00:08:10.480
DOUGLAS RUSHKOFF: It did.

00:08:10.480 --> 00:08:12.226
JASON SILVA: --the catalyst
for human imagination.

00:08:12.226 --> 00:08:12.640
DOUGLAS RUSHKOFF: And progress.

00:08:12.640 --> 00:08:13.820
JASON SILVA: To
imagine what could be.

00:08:13.820 --> 00:08:14.940
DOUGLAS RUSHKOFF: And
industralization and all that.

00:08:14.940 --> 00:08:16.940
JASON SILVA: To yearn,
to dream, to sort of take

00:08:16.940 --> 00:08:20.500
the protoplasmic
yearning of biology

00:08:20.500 --> 00:08:22.126
and articulate it
into this beautiful--

00:08:22.126 --> 00:08:23.166
DOUGLAS RUSHKOFF: It did.

00:08:23.166 --> 00:08:24.840
And dreaming is
beautiful as long as,

00:08:24.840 --> 00:08:26.340
just like with the
psychedelic trip,

00:08:26.340 --> 00:08:31.610
as long as that dream is then
integrated back into reality

00:08:31.610 --> 00:08:36.074
rather than engineered as a
way to get out of reality.

00:08:36.074 --> 00:08:37.490
JASON SILVA: Well,
OK, definitely.

00:08:37.490 --> 00:08:38.989
And one of the
things you talk about

00:08:38.989 --> 00:08:42.890
is the comparison between
this digital future

00:08:42.890 --> 00:08:46.210
and bringing it back to the
human spectrum of experience.

00:08:46.210 --> 00:08:48.180
So I'm really interested
in experience design.

00:08:48.180 --> 00:08:51.190
And I'm interested in
technologically mediating

00:08:51.190 --> 00:08:52.310
experience, right?

00:08:52.310 --> 00:08:54.101
So this is something
I'm obsessed with just

00:08:54.101 --> 00:08:57.850
because for my own interest
in, like, being blissed out.

00:08:57.850 --> 00:09:00.640
When I watch movies, when I
listen to beautiful music,

00:09:00.640 --> 00:09:02.450
when I experience
aesthetic arrest.

00:09:02.450 --> 00:09:04.712
These are all
technologies, right?

00:09:04.712 --> 00:09:06.170
Cinema, truth, 24
times per second.

00:09:06.170 --> 00:09:06.650
DOUGLAS RUSHKOFF:
Since the beginning.

00:09:06.650 --> 00:09:07.890
Since the cave paintings.

00:09:07.890 --> 00:09:10.350
And all we've been experimenting
with from the beginning

00:09:10.350 --> 00:09:12.912
is kind of challenging
our proprioception,

00:09:12.912 --> 00:09:15.370
challenging, where does my body
end and the rest of reality

00:09:15.370 --> 00:09:16.140
begin?

00:09:16.140 --> 00:09:17.110
I mean, the museums.

00:09:17.110 --> 00:09:19.470
If you look at the history
of museums, you see dioramas.

00:09:19.470 --> 00:09:20.200
What were they?

00:09:20.200 --> 00:09:20.700
Whoa.

00:09:20.700 --> 00:09:22.510
And then holograms, whoa!

00:09:22.510 --> 00:09:24.440
Then virtual reality, whoa!

00:09:24.440 --> 00:09:25.940
JASON SILVA: Faulkner
says, a writer

00:09:25.940 --> 00:09:27.930
wants to fit the
world into a sentence.

00:09:27.930 --> 00:09:30.534
A filmmaker wants to fit
the world into that film.

00:09:30.534 --> 00:09:31.950
You know, when you
go to a museum,

00:09:31.950 --> 00:09:35.290
you want to fit history
into an experience that

00:09:35.290 --> 00:09:36.310
fits into your now.

00:09:36.310 --> 00:09:37.810
It's almost like
you want to process

00:09:37.810 --> 00:09:39.430
the world into this moment.

00:09:39.430 --> 00:09:41.970
See the world in a grain of
sand and then a wild flower.

00:09:41.970 --> 00:09:45.920
So Diana Slattery has that
amazing article about,

00:09:45.920 --> 00:09:48.660
she talks about the
term "the trope high."

00:09:48.660 --> 00:09:51.440
And she says, whether
it's drug-induced highs

00:09:51.440 --> 00:09:55.980
or technological highs, high
resolution, high fidelity,

00:09:55.980 --> 00:10:00.339
our desire to just chase these
blissful mind-body states that

00:10:00.339 --> 00:10:02.130
seem to be integral to
the human condition.

00:10:02.130 --> 00:10:04.550
And us using our tools and
folding those tools back

00:10:04.550 --> 00:10:07.710
into the self to essentially
hack perception, hack

00:10:07.710 --> 00:10:09.040
experience, hack consciousness.

00:10:09.040 --> 00:10:11.540
I mean, don't you think that's
an amazing opportunity for us

00:10:11.540 --> 00:10:12.040
to play?

00:10:12.040 --> 00:10:12.900
Does that excite you?

00:10:12.900 --> 00:10:14.320
DOUGLAS RUSHKOFF: It's an
amazing opportunity for us

00:10:14.320 --> 00:10:15.330
to play, for sure.

00:10:15.330 --> 00:10:20.560
And although I've seen
many acid casualties

00:10:20.560 --> 00:10:22.420
along the way of the
people that chased just

00:10:22.420 --> 00:10:25.460
a few too many highs, I'd
hate for our civilization

00:10:25.460 --> 00:10:27.090
to become an acid casualty.

00:10:27.090 --> 00:10:29.600
But the other problem
for me is, who's

00:10:29.600 --> 00:10:31.510
building these technologies?

00:10:31.510 --> 00:10:32.910
Who are we in concert with?

00:10:32.910 --> 00:10:35.547
When I say, OK, so Google's
building these technologies.

00:10:35.547 --> 00:10:37.130
And yeah, they're
also building robots

00:10:37.130 --> 00:10:39.590
that really are good at chasing
and shooting down people.

00:10:39.590 --> 00:10:42.250
And while they're donating it
to the World Wildlife Federation

00:10:42.250 --> 00:10:44.560
to chase poachers, who we
all know are really bad,

00:10:44.560 --> 00:10:46.500
so that's a very
socially acceptable way

00:10:46.500 --> 00:10:50.470
to build robot drones
that chase down people.

00:10:50.470 --> 00:10:52.310
And we know Google
will do no evil,

00:10:52.310 --> 00:10:54.110
so we don't have to
worry about that.

00:10:54.110 --> 00:10:55.818
And the government
will never get a hold.

00:10:58.000 --> 00:11:00.260
While we're chasing
our highs, we

00:11:00.260 --> 00:11:06.250
are supporting the creation
of a technological structure

00:11:06.250 --> 00:11:10.670
that we're not
really in charge of.

00:11:10.670 --> 00:11:12.480
JASON SILVA: So
does that scare you?

00:11:12.480 --> 00:11:15.690
I mean, do you anticipate, like,
some totalitarian information

00:11:15.690 --> 00:11:17.520
state where we're not free?

00:11:17.520 --> 00:11:22.700
Or McKenna talking about us
each moving into universes

00:11:22.700 --> 00:11:25.440
of our own construction,
where Big Data is used

00:11:25.440 --> 00:11:27.589
to create engineered
serendipities

00:11:27.589 --> 00:11:29.130
where everything
that's always around

00:11:29.130 --> 00:11:31.860
is custom for us and our tastes.

00:11:31.860 --> 00:11:34.960
And the smart
systems that start.

00:11:34.960 --> 00:11:36.791
We dovetail our
minds to our tools,

00:11:36.791 --> 00:11:38.540
but when the tools
start dovetailing back,

00:11:38.540 --> 00:11:41.225
the distinction
between self and world

00:11:41.225 --> 00:11:42.600
is going to become
really flimsy.

00:11:42.600 --> 00:11:44.730
Because everything is
going to have mind in it.

00:11:44.730 --> 00:11:46.480
Everything is going
to have my mind in it.

00:11:46.480 --> 00:11:47.855
Because everything
is going to be

00:11:47.855 --> 00:11:50.050
responding to my tastes and
my desires and my needs.

00:11:50.050 --> 00:11:52.570
I mean, I imagine a
sort of playground,

00:11:52.570 --> 00:11:56.190
a kind of wake-walking lucid
dream mediated by technology.

00:11:56.190 --> 00:11:58.180
Kind of like the end
of "Vanilla Sky," when

00:11:58.180 --> 00:12:00.126
he's in that lucid dream.

00:12:00.126 --> 00:12:01.250
DOUGLAS RUSHKOFF: Could be.

00:12:01.250 --> 00:12:02.140
JASON SILVA: That's
what I fantasize about.

00:12:02.140 --> 00:12:02.970
DOUGLAS RUSHKOFF: It could be.

00:12:02.970 --> 00:12:03.600
It could be.

00:12:03.600 --> 00:12:05.380
But for me, the
best part of a trip

00:12:05.380 --> 00:12:07.980
is always waking
up in the morning.

00:12:07.980 --> 00:12:10.430
I mean, coming down
is the good part.

00:12:10.430 --> 00:12:11.430
It should be, anyway.

00:12:11.430 --> 00:12:13.395
Coming down is the part
where you're, like, OK.

00:12:13.395 --> 00:12:14.770
JASON SILVA: What
have I learned?

00:12:14.770 --> 00:12:15.250
DOUGLAS RUSHKOFF: Now what?

00:12:15.250 --> 00:12:15.620
Yeah.

00:12:15.620 --> 00:12:18.036
What did I learn and how do I
apply this and what is this?

00:12:18.036 --> 00:12:21.260
And then you go out in the
woods and you [SNIFFING]

00:12:21.260 --> 00:12:24.399
smell reality again
from that new place.

00:12:24.399 --> 00:12:26.190
And you realize, oh,
my god, I've only tip.

00:12:26.190 --> 00:12:28.172
I'm only 1% of what I could be.

00:12:28.172 --> 00:12:29.630
And I think we've
both shared that.

00:12:29.630 --> 00:12:31.430
We're only 1% of
what we could be.

00:12:31.430 --> 00:12:33.460
The question is, where
is that other 99%?

00:12:33.460 --> 00:12:36.670
Is that other 99% by
building these technologies

00:12:36.670 --> 00:12:39.240
that can then do this stuff?

00:12:39.240 --> 00:12:41.810
Or are we leaving
that 99% behind

00:12:41.810 --> 00:12:44.590
and building technologies
that can house the 1% that we

00:12:44.590 --> 00:12:46.520
already know about?

00:12:46.520 --> 00:12:48.020
JASON SILVA: You
just said something

00:12:48.020 --> 00:12:50.910
that really got me excited.

00:12:50.910 --> 00:12:54.030
You talked about what
happens after the trip.

00:12:54.030 --> 00:12:59.020
This idea that ego death results
in a kind of reset of the self.

00:12:59.020 --> 00:13:01.590
And you go and smell the
roses and smell reality.

00:13:01.590 --> 00:13:03.120
And you've been
changed in some way.

00:13:03.120 --> 00:13:04.500
You've been conditioned
by the experience.

00:13:04.500 --> 00:13:06.417
And you want to go out
and be a better person.

00:13:06.417 --> 00:13:08.875
And you want to exercise new
possibilities and so on and so

00:13:08.875 --> 00:13:09.460
forth.

00:13:09.460 --> 00:13:11.770
So I want to ask you--

00:13:11.770 --> 00:13:14.270
DOUGLAS RUSHKOFF: And the very
best thing about psychedelics

00:13:14.270 --> 00:13:16.310
is you come back and
you see the structures

00:13:16.310 --> 00:13:18.470
that you've been
accepting at face value,

00:13:18.470 --> 00:13:20.460
as given circumstances
of your reality,

00:13:20.460 --> 00:13:22.470
and you see them as
social constructions

00:13:22.470 --> 00:13:26.220
of people who may or may
not have had our best

00:13:26.220 --> 00:13:27.610
interests at heart.

00:13:27.610 --> 00:13:33.300
So when our very
best digital people

00:13:33.300 --> 00:13:39.330
are, say, building
Twitter and disrupting

00:13:39.330 --> 00:13:40.810
this industry or that.

00:13:40.810 --> 00:13:42.570
But then what do they do?

00:13:42.570 --> 00:13:45.640
They go to daddy at
Goldman Sachs and do an IPO

00:13:45.640 --> 00:13:49.200
and reify the worst
shit of our society.

00:13:49.200 --> 00:13:52.550
They basically turn it into
another prop for Wall Street.

00:13:52.550 --> 00:13:55.430
The opportunity of
moving into a digital age

00:13:55.430 --> 00:14:01.110
is not to build upon the
mistakes of the Industrial Age,

00:14:01.110 --> 00:14:03.360
but to challenge
the Industrial Age

00:14:03.360 --> 00:14:05.890
and retrieve all of the
values that got repressed

00:14:05.890 --> 00:14:06.480
the last time.

00:14:06.480 --> 00:14:08.720
That's McKenna's
archaic revival.

00:14:08.720 --> 00:14:12.280
That's McLuhan's
retrieval of values.

00:14:12.280 --> 00:14:16.330
So what I saw on my
technological trip,

00:14:16.330 --> 00:14:17.847
when I saw it was, oh, my gosh.

00:14:17.847 --> 00:14:19.930
You mean we can transact
in a peer-to-peer fashion

00:14:19.930 --> 00:14:21.200
without corporations.

00:14:21.200 --> 00:14:23.980
Oh, we can invent
local currencies.

00:14:23.980 --> 00:14:26.835
And we can use our iPhones
to do authentication.

00:14:29.435 --> 00:14:33.740
We can look back at
reality again rather than

00:14:33.740 --> 00:14:36.620
just build on the
artificial structures

00:14:36.620 --> 00:14:37.780
that we've had before.

00:14:37.780 --> 00:14:40.570
And my concern is, when we
just go headlong for the high,

00:14:40.570 --> 00:14:42.497
there's a ton of
powers that be that

00:14:42.497 --> 00:14:43.830
are more than happy to get you--

00:14:43.830 --> 00:14:44.480
JASON SILVA: To
serve you that high.

00:14:44.480 --> 00:14:46.396
DOUGLAS RUSHKOFF: Yeah,
to get you and me high

00:14:46.396 --> 00:14:48.870
as long as they get the Big
Data that they need in order

00:14:48.870 --> 00:14:52.495
to predict or influence
our upcoming behaviors.

00:14:52.495 --> 00:14:54.420
JASON SILVA: So its
revolution or hedonism.

00:14:54.420 --> 00:14:55.409
DOUGLAS RUSHKOFF: Yeah.

00:14:55.409 --> 00:14:56.950
JASON SILVA: It's
interesting though,

00:14:56.950 --> 00:15:01.960
but sometimes I feel like,
OK, so when I watch a movie,

00:15:01.960 --> 00:15:05.790
I'm a consumer who is paying
money, feeding an existing

00:15:05.790 --> 00:15:09.320
system, a business that
is a thriving business,

00:15:09.320 --> 00:15:12.760
a corporate business that
is, those that criticize film

00:15:12.760 --> 00:15:16.380
say that it's like a soma
that's keeping us all placid

00:15:16.380 --> 00:15:17.960
and mindlessly entertained.

00:15:17.960 --> 00:15:19.800
But when I see a
movie that I love,

00:15:19.800 --> 00:15:23.030
I don't necessarily want
to have a revolution

00:15:23.030 --> 00:15:23.920
against that system.

00:15:23.920 --> 00:15:26.295
I actually enjoy the film,
and I'm glad I live in a world

00:15:26.295 --> 00:15:28.040
where these expert
filmmakers can amass

00:15:28.040 --> 00:15:30.656
these resources to make these
amazing cultural technologies

00:15:30.656 --> 00:15:31.530
for me to experience.

00:15:31.530 --> 00:15:31.880
But I don't know if
that just makes me--

00:15:31.880 --> 00:15:33.630
DOUGLAS RUSHKOFF: But
we still have those.

00:15:33.630 --> 00:15:35.400
And they're still great movies.

00:15:35.400 --> 00:15:38.170
Go to a Chris Nolan movie, and
it'll blow your little mind.

00:15:38.170 --> 00:15:41.175
I mean, that's a good
thing, not a bad thing.

00:15:41.175 --> 00:15:43.300
JASON SILVA: Yeah, but I
guess what I'm saying is--

00:15:43.300 --> 00:15:44.570
DOUGLAS RUSHKOFF: It's
when we're not conscious.

00:15:44.570 --> 00:15:47.490
So then it's like, OK, now I
can make my own movies, right?

00:15:47.490 --> 00:15:49.260
And you're one of
the lucky ones.

00:15:49.260 --> 00:15:51.650
You can buy a Sony camera
and buy an Apple computer

00:15:51.650 --> 00:15:55.800
and buy some Time Warner
Road Runner uplink speed.

00:15:55.800 --> 00:15:58.180
And you can now, instead
of paying to watch a movie,

00:15:58.180 --> 00:15:59.440
you can pay to make a movie.

00:15:59.440 --> 00:16:01.880
And upload it to YouTube,
where Google will own it.

00:16:01.880 --> 00:16:04.290
And if you're one
out of 3 million,

00:16:04.290 --> 00:16:07.611
you can be one of the few that
becomes luckily you or me.

00:16:07.611 --> 00:16:10.110
JASON SILVA: Well, Google will
own it, et cetera, et cetera.

00:16:10.110 --> 00:16:13.320
But the point is, technology has
lowered the barriers of entry.

00:16:13.320 --> 00:16:15.340
There's been a
flooding of new talent.

00:16:15.340 --> 00:16:17.756
DOUGLAS RUSHKOFF: There has
been a flooding of new talent.

00:16:17.756 --> 00:16:20.500
But the number of people
that have actually made it

00:16:20.500 --> 00:16:24.610
in the DIY system, if you
look at music or filmmaking,

00:16:24.610 --> 00:16:27.480
is less than made it
in the old system.

00:16:27.480 --> 00:16:29.530
The disparity between
the stars of music.

00:16:29.530 --> 00:16:31.280
JASON SILVA: And why
do you think that is?

00:16:31.280 --> 00:16:32.196
DOUGLAS RUSHKOFF: Why?

00:16:32.196 --> 00:16:35.140
Because I think it's actually
because they're not really

00:16:35.140 --> 00:16:35.931
selling the music.

00:16:35.931 --> 00:16:37.430
Because what we're
really selling in

00:16:37.430 --> 00:16:40.350
this system are networks.

00:16:40.350 --> 00:16:43.130
And I was just talking to Oliver
Luckett, who runs theAudience,

00:16:43.130 --> 00:16:44.530
whom you know.

00:16:44.530 --> 00:16:48.840
It's a social media agency
for rock stars and pop stars.

00:16:48.840 --> 00:16:51.964
And what they realized is, if
you're going to be a pop star,

00:16:51.964 --> 00:16:54.130
you're not going to make
money selling your records.

00:16:54.130 --> 00:16:56.980
What you can make money doing is
selling your Twitter followers,

00:16:56.980 --> 00:16:59.850
is selling your Facebook likes.

00:16:59.850 --> 00:17:01.310
So what they do is
help you amass--

00:17:01.310 --> 00:17:03.200
JASON SILVA: It's a currency
of attention, right?

00:17:03.200 --> 00:17:04.730
Attention's the new
limited resource.

00:17:04.730 --> 00:17:06.329
There's an infinite amount
of signals competing

00:17:06.329 --> 00:17:07.079
for our attention.

00:17:07.079 --> 00:17:09.079
We're all suffering
from bandwidth anxiety.

00:17:09.079 --> 00:17:10.079
DOUGLAS RUSHKOFF: Right.

00:17:10.079 --> 00:17:11.630
You could take
Adderall for that.

00:17:11.630 --> 00:17:14.160
JASON SILVA: But those
with the loudest microphone

00:17:14.160 --> 00:17:17.250
or amplifier in the
age of information,

00:17:17.250 --> 00:17:18.800
that's where the
wealth is, right?

00:17:18.800 --> 00:17:21.089
You can control attention,
you can control the world.

00:17:21.089 --> 00:17:22.780
It's interesting
because in the book

00:17:22.780 --> 00:17:25.550
"The Mating Mind," the
guy talks about how

00:17:25.550 --> 00:17:27.680
the human capacity for
creativity, you know,

00:17:27.680 --> 00:17:30.510
language, art, poetry, is
really just the human version

00:17:30.510 --> 00:17:31.840
of the peacock feather.

00:17:31.840 --> 00:17:34.900
And that we went from trading
in genes to trading in memes.

00:17:34.900 --> 00:17:37.870
So today, evolutionary
success is really

00:17:37.870 --> 00:17:40.060
those that are controlling
the Twitter followers.

00:17:40.060 --> 00:17:41.060
DOUGLAS RUSHKOFF: Right.

00:17:41.060 --> 00:17:42.420
I mean, and people
like me were the ones--

00:17:42.420 --> 00:17:43.590
JASON SILVA: They're mating
their memes the widest

00:17:43.590 --> 00:17:44.440
and farthest.

00:17:44.440 --> 00:17:45.565
DOUGLAS RUSHKOFF: Honestly.

00:17:45.565 --> 00:17:47.370
And it was little
schlemiels like me

00:17:47.370 --> 00:17:50.392
who liked the trading
with memes thing

00:17:50.392 --> 00:17:52.100
better than the trading
with genes thing.

00:17:52.100 --> 00:17:53.849
That's why I couldn't
live in Los Angeles.

00:17:53.849 --> 00:17:55.620
I had to come to
New York where I

00:17:55.620 --> 00:17:59.076
can get currency with the things
I say rather than my flesh.

00:17:59.076 --> 00:18:00.700
JASON SILVA: Rather
than your six pack.

00:18:00.700 --> 00:18:02.135
DOUGLAS RUSHKOFF: Yeah.

00:18:02.135 --> 00:18:04.060
And you got both.

00:18:04.060 --> 00:18:05.674
[LAUGHTER]

00:18:08.626 --> 00:18:10.000
JASON SILVA: So
I want to go back

00:18:10.000 --> 00:18:13.920
to what you were saying before
about resetting the self,

00:18:13.920 --> 00:18:17.110
coming down from the trip,
whatever that trip was,

00:18:17.110 --> 00:18:19.610
technologically mediated
or drug mediated.

00:18:19.610 --> 00:18:22.270
And the therapeutic aspects
of these experiences.

00:18:22.270 --> 00:18:23.916
Because one of the
things that I've

00:18:23.916 --> 00:18:26.540
been obsessing with lately,
and I've done videos about it,

00:18:26.540 --> 00:18:28.410
is the subject of awe.

00:18:28.410 --> 00:18:31.130
And I tell people
that awe is, to me,

00:18:31.130 --> 00:18:34.590
it's a sort of an antidote
to existential despair.

00:18:34.590 --> 00:18:36.630
You know that I'm really
into Ernest Becker.

00:18:36.630 --> 00:18:39.350
He says the human condition
is defined by the fact

00:18:39.350 --> 00:18:41.900
that we know that we
are mortal beings.

00:18:41.900 --> 00:18:45.060
So we lose sleep over the fact
that one day in the future

00:18:45.060 --> 00:18:46.180
we're going to die.

00:18:46.180 --> 00:18:48.220
And that imbues the
human experience

00:18:48.220 --> 00:18:50.260
with a kind of absurdity, right?

00:18:50.260 --> 00:18:53.080
We are simultaneously
gods and worms.

00:18:53.080 --> 00:18:57.000
We can ponder the infinite,
yet we watch those

00:18:57.000 --> 00:19:00.300
that we love get old, ourselves
get old, and eventually die.

00:19:00.300 --> 00:19:04.145
And so I find that whether
it's psychedelic therapy

00:19:04.145 --> 00:19:07.210
or whatever, inspiration,
awe, aesthetic arrest

00:19:07.210 --> 00:19:09.450
are the antidotes
to that experience.

00:19:09.450 --> 00:19:13.390
So ego death, being
blissed out arrests time,

00:19:13.390 --> 00:19:16.710
temporarily pushes aside
thoughts of impending doom,

00:19:16.710 --> 00:19:18.860
takes us off that people
mover that's carrying

00:19:18.860 --> 00:19:20.330
everyone else towards death.

00:19:20.330 --> 00:19:24.420
And then there was a study
that came out of Stanford where

00:19:24.420 --> 00:19:27.600
they basically exposed
people to experiences of awe,

00:19:27.600 --> 00:19:29.160
which they defined
as experiences

00:19:29.160 --> 00:19:31.280
of such perceptual
expansion that people

00:19:31.280 --> 00:19:33.410
had to upgrade their
mental schemata

00:19:33.410 --> 00:19:35.220
to accommodate the experience.

00:19:35.220 --> 00:19:36.740
And what they found
is that people

00:19:36.740 --> 00:19:38.570
who have regular
incidences of awe

00:19:38.570 --> 00:19:41.370
are left with increased
altruism, increased well-being,

00:19:41.370 --> 00:19:43.770
increased feelings of
compassion towards other people,

00:19:43.770 --> 00:19:44.690
et cetera, et cetera.

00:19:44.690 --> 00:19:48.462
Like there's all these takeaways
after blowing your own mind.

00:19:48.462 --> 00:19:49.420
The therapeutic aspect.

00:19:49.420 --> 00:19:50.586
DOUGLAS RUSHKOFF: There are.

00:19:50.586 --> 00:19:56.067
And awe was
systematically extracted

00:19:56.067 --> 00:19:58.150
from the human experience
over the last 600 years.

00:19:58.150 --> 00:20:00.040
Because people in awe, people
who have awe experiences,

00:20:00.040 --> 00:20:00.623
are dangerous.

00:20:00.623 --> 00:20:01.480
We're unpredictable.

00:20:01.480 --> 00:20:02.320
JASON SILVA: They
get hit by a car

00:20:02.320 --> 00:20:03.819
because they can't
cross the street.

00:20:03.819 --> 00:20:07.500
DOUGLAS RUSHKOFF: Which is why
we have psychedelic medicine.

00:20:07.500 --> 00:20:09.967
But psychedelic drugs,
they're still drugs.

00:20:09.967 --> 00:20:11.300
They're for people who are sick.

00:20:11.300 --> 00:20:13.330
They're for a society
that's lost the ability

00:20:13.330 --> 00:20:14.640
to encounter awe.

00:20:14.640 --> 00:20:17.180
And they reacquaint
us with awe, not so

00:20:17.180 --> 00:20:19.180
that we have to do it
again and again and again.

00:20:19.180 --> 00:20:22.210
As Alan Watts said, once you get
the message, hang up the phone.

00:20:22.210 --> 00:20:24.815
But rather to be able
to experience awe,

00:20:24.815 --> 00:20:26.870
as I am right now
just sitting with you,

00:20:26.870 --> 00:20:29.030
or looking at my
daughter, or having sex,

00:20:29.030 --> 00:20:30.020
or looking at a sunset.

00:20:30.020 --> 00:20:31.770
It's like awe, awe, awe.

00:20:31.770 --> 00:20:32.840
The awe is everywhere.

00:20:32.840 --> 00:20:33.960
Do I need that medicine?

00:20:33.960 --> 00:20:36.510
Now the technology is also
good because you can't

00:20:36.510 --> 00:20:37.640
get everybody high on acid.

00:20:37.640 --> 00:20:38.723
We found that in the '60s.

00:20:38.723 --> 00:20:39.870
People are afraid.

00:20:39.870 --> 00:20:41.790
But the computer, great.

00:20:41.790 --> 00:20:45.510
All right, so let's give
people some awe experiences.

00:20:45.510 --> 00:20:46.090
Great.

00:20:46.090 --> 00:20:49.080
Not so that they can then
travel into the computer

00:20:49.080 --> 00:20:50.820
into the land of awe.

00:20:50.820 --> 00:20:52.510
But so that now
that they recognize

00:20:52.510 --> 00:20:55.150
awe, or a simulation
of awe in this case,

00:20:55.150 --> 00:20:57.920
they can come back to the real
world and go, oh, my gosh.

00:20:57.920 --> 00:20:58.330
JASON SILVA: But it's true.

00:20:58.330 --> 00:20:59.810
But the existentialist
philosophers

00:20:59.810 --> 00:21:00.590
were always saying it.

00:21:00.590 --> 00:21:02.180
I mean, Camus used
to say, life should

00:21:02.180 --> 00:21:03.750
be lived to the point of tears.

00:21:03.750 --> 00:21:04.830
I mean, you read
the romantic poets.

00:21:04.830 --> 00:21:07.371
DOUGLAS RUSHKOFF: Life should
be lived to the point of tears.

00:21:07.371 --> 00:21:10.100
We should go to a movie
to experience a simulation

00:21:10.100 --> 00:21:11.154
of someone else's life.

00:21:11.154 --> 00:21:12.195
You can go into virtual--

00:21:12.195 --> 00:21:14.527
JASON SILVA: Because we can't
have it in our own lives.

00:21:14.527 --> 00:21:15.610
Perhaps that is a tragedy.

00:21:15.610 --> 00:21:16.430
But you know what
the problem is--

00:21:16.430 --> 00:21:17.800
DOUGLAS RUSHKOFF: We can
have it in our own lives.

00:21:17.800 --> 00:21:18.160
It's a thing.

00:21:18.160 --> 00:21:18.660
It's here.

00:21:18.660 --> 00:21:19.650
It's available.

00:21:19.650 --> 00:21:21.960
We don't need to pay
Google for the awe.

00:21:24.940 --> 00:21:28.234
This is a way for white,
Western, patriarchal--

00:21:28.234 --> 00:21:30.400
JASON SILVA: I still want
to have my iTunes playlist

00:21:30.400 --> 00:21:33.100
with the custom
music that I picked

00:21:33.100 --> 00:21:36.317
to be playing conditioning
my real experience.

00:21:36.317 --> 00:21:37.900
So I still want to
use the technology.

00:21:37.900 --> 00:21:39.330
I still want to
borrow from the--

00:21:39.330 --> 00:21:40.730
DOUGLAS RUSHKOFF: But also
there's a reality out there

00:21:40.730 --> 00:21:41.970
you can listen to, too.

00:21:41.970 --> 00:21:43.320
JASON SILVA: Like just
the trees rustling?

00:21:43.320 --> 00:21:44.278
DOUGLAS RUSHKOFF: Yeah.

00:21:44.278 --> 00:21:45.060
[LAUGHTER]

00:21:45.060 --> 00:21:46.680
The trees rustling.

00:21:46.680 --> 00:21:49.052
I mean, gosh, my
favorite thing when

00:21:49.052 --> 00:21:51.510
I lived in the West Village,
back when that was affordable,

00:21:51.510 --> 00:21:53.290
was there was a
public school there.

00:21:53.290 --> 00:21:57.330
And I would stand at the chain
link fence when they had recess

00:21:57.330 --> 00:22:01.560
and listen in perfect stereo
to all the kids playing

00:22:01.560 --> 00:22:02.250
in this thing.

00:22:02.250 --> 00:22:03.590
And that was awe.

00:22:03.590 --> 00:22:04.270
But it was awe.

00:22:04.270 --> 00:22:05.427
It was right there.

00:22:05.427 --> 00:22:07.010
JASON SILVA: I've
always envied people

00:22:07.010 --> 00:22:08.900
that just are naturally in awe.

00:22:08.900 --> 00:22:10.450
My mother's one of those people.

00:22:10.450 --> 00:22:13.150
She's just one of those people
who says everything moves her.

00:22:13.150 --> 00:22:16.130
She finds beauty
almost in everything.

00:22:16.130 --> 00:22:19.090
And then some of us that have to
find ways to mediate and induce

00:22:19.090 --> 00:22:22.340
an occasion that experience.

00:22:22.340 --> 00:22:24.340
So it's just something
I'm really interested in.

00:22:24.340 --> 00:22:26.900
DOUGLAS RUSHKOFF: Now
your stuff, your videos

00:22:26.900 --> 00:22:28.470
are medicine in that sense.

00:22:28.470 --> 00:22:30.790
Because people are, you're
at work, you're doing this,

00:22:30.790 --> 00:22:33.120
you hear about your stocks,
and does it go up or down,

00:22:33.120 --> 00:22:35.584
and private school, my kid.

00:22:35.584 --> 00:22:37.250
And now I can watch
a three-minute video

00:22:37.250 --> 00:22:39.543
and go whoa, fuck!

00:22:39.543 --> 00:22:42.370
[LAUGHTER]

00:22:42.370 --> 00:22:43.812
And that's the point, right?

00:22:43.812 --> 00:22:44.520
That's the point.

00:22:44.520 --> 00:22:45.600
JASON SILVA: Well,
I mean, the 100%.

00:22:45.600 --> 00:22:46.700
The videos, I try to
explain to people.

00:22:46.700 --> 00:22:46.880
DOUGLAS RUSHKOFF: Brain games.

00:22:46.880 --> 00:22:47.250
I mean, what do you do?

00:22:47.250 --> 00:22:48.990
You're teaching people
to play with their brain.

00:22:48.990 --> 00:22:50.280
Play with your brain,
it's here for that.

00:22:50.280 --> 00:22:51.780
JASON SILVA: Yeah,
it's interesting.

00:22:51.780 --> 00:22:54.419
Because the reason I
did those videos was,

00:22:54.419 --> 00:22:55.960
thank you for using
the term medicine

00:22:55.960 --> 00:22:59.640
because I did them
as self-medication.

00:22:59.640 --> 00:23:05.120
It was like if I was in any kind
of stupor, if I was afflicted

00:23:05.120 --> 00:23:09.400
by the banality of the every
day, the been theres and done

00:23:09.400 --> 00:23:12.360
thats of the adult mind,
as Michael Pollan writes,

00:23:12.360 --> 00:23:14.370
I needed to be reset.

00:23:14.370 --> 00:23:18.200
And not every month
does Christopher Nolan

00:23:18.200 --> 00:23:19.170
release a film.

00:23:19.170 --> 00:23:20.870
So you've got to wait for them.

00:23:20.870 --> 00:23:23.500
So when there's
nothing I can purchase

00:23:23.500 --> 00:23:24.970
to give me that
experience, I want

00:23:24.970 --> 00:23:26.640
to go and make that experience.

00:23:26.640 --> 00:23:28.820
And finally I have
the tools, right?

00:23:28.820 --> 00:23:30.910
And so I have limited resources.

00:23:30.910 --> 00:23:33.720
And perhaps kind of
a limited attention

00:23:33.720 --> 00:23:37.180
span, and kind of impatience and
a desire to just make it now.

00:23:37.180 --> 00:23:40.002
So rather than spending six
months doing a feature doc,

00:23:40.002 --> 00:23:42.210
I was like, I want to make
these three-minute videos.

00:23:42.210 --> 00:23:44.150
Because I want to take
a three-minute hit.

00:23:44.150 --> 00:23:45.330
Like a hit of DMT.

00:23:45.330 --> 00:23:46.580
It's just like a three minute.

00:23:46.580 --> 00:23:49.180
But I want it to have all the
energy of the most amazing

00:23:49.180 --> 00:23:50.700
movie trailer you've ever seen.

00:23:50.700 --> 00:23:53.120
And all the ideas
packed in there densely.

00:23:53.120 --> 00:23:54.780
And I want it to be
like an inception.

00:23:54.780 --> 00:23:56.770
You watch it, and then
it goes and percolates,

00:23:56.770 --> 00:23:59.360
and it stays with you,
and this and that.

00:23:59.360 --> 00:24:02.430
Yeah, I mean, for me
it's been medicine.

00:24:02.430 --> 00:24:03.930
DOUGLAS RUSHKOFF:
No, and it's cool.

00:24:03.930 --> 00:24:06.388
The trick, though, is to help
people understand that you're

00:24:06.388 --> 00:24:08.290
not necessarily
describing reality,

00:24:08.290 --> 00:24:12.690
but you're giving metaphorical
ways of understanding reality.

00:24:12.690 --> 00:24:16.120
So the mind is like a processor.

00:24:16.120 --> 00:24:18.130
You know, we are like computers.

00:24:18.130 --> 00:24:20.590
We are uploading stuff.

00:24:20.590 --> 00:24:22.010
Memes are like genes.

00:24:22.010 --> 00:24:23.960
And we are infecting
one another with ideas.

00:24:23.960 --> 00:24:25.459
JASON SILVA: I love
those metaphors.

00:24:25.459 --> 00:24:27.890
They help me understand
the world a lot more.

00:24:27.890 --> 00:24:29.540
It makes everything make sense.

00:24:29.540 --> 00:24:30.520
That's the thing about
those kinds of metaphors.

00:24:30.520 --> 00:24:31.770
DOUGLAS RUSHKOFF: No, it does.

00:24:31.770 --> 00:24:32.700
They're just prone.

00:24:32.700 --> 00:24:34.770
In our society
those metaphors are

00:24:34.770 --> 00:24:37.320
prone to the sort
of Margaret Mead,

00:24:37.320 --> 00:24:43.410
Gregory Bateson abstracted
understanding of reality

00:24:43.410 --> 00:24:49.768
as this kind of participatory
mediated spectacle.

00:24:49.768 --> 00:24:52.050
JASON SILVA: I love that term.

00:24:52.050 --> 00:24:55.210
OK, so let's talk about
cultural operating systems

00:24:55.210 --> 00:24:56.550
and the consensus trance.

00:24:56.550 --> 00:24:58.440
You were talking
about that before,

00:24:58.440 --> 00:25:00.810
that when people have these
psychedelic experiences,

00:25:00.810 --> 00:25:01.920
they come out of
it and they realize

00:25:01.920 --> 00:25:03.680
that these social,
rigid frameworks that

00:25:03.680 --> 00:25:05.842
seemed so solid and
so real are much more

00:25:05.842 --> 00:25:07.050
fluid than we think they are.

00:25:07.050 --> 00:25:08.760
They've been created
by other people.

00:25:08.760 --> 00:25:10.300
What do you think--

00:25:10.300 --> 00:25:10.860
DOUGLAS RUSHKOFF:
It's the same thing

00:25:10.860 --> 00:25:13.370
that happens when you watch a
Shakespeare play, or a Brecht

00:25:13.370 --> 00:25:17.730
play, or have good sex, or go
to a good religious ritual, too.

00:25:17.730 --> 00:25:19.320
JASON SILVA: So
what do you think,

00:25:19.320 --> 00:25:22.570
how can we upgrade our
social operating systems?

00:25:22.570 --> 00:25:25.787
Do you ever speculate of
what a better society?

00:25:25.787 --> 00:25:27.870
DOUGLAS RUSHKOFF: Yeah, I
mean for me, it's funny.

00:25:27.870 --> 00:25:29.440
JASON SILVA: Do we
all run around naked?

00:25:29.440 --> 00:25:31.231
DOUGLAS RUSHKOFF: For
me, the digital thing

00:25:31.231 --> 00:25:37.800
is so key because we have
migrated from an industrial age

00:25:37.800 --> 00:25:39.350
society to a digital one.

00:25:39.350 --> 00:25:42.380
But for me, the
digital is the opposite

00:25:42.380 --> 00:25:46.080
of the kind of global,
Google, meta network thing.

00:25:46.080 --> 00:25:48.937
For me, digital was
always the digits.

00:25:48.937 --> 00:25:49.770
It was about return.

00:25:49.770 --> 00:25:50.780
These are the digits.

00:25:50.780 --> 00:25:51.863
That's what digital means.

00:25:51.863 --> 00:25:53.810
A return to the fingers.

00:25:53.810 --> 00:25:56.220
Not to individuality as
individual consumers,

00:25:56.220 --> 00:26:01.570
but to local means
of production.

00:26:01.570 --> 00:26:07.610
And for me, though, the whole
local thing, the Etsy thing,

00:26:07.610 --> 00:26:09.600
the retrieval of
medieval values,

00:26:09.600 --> 00:26:15.036
and Burning Man, and Occupy
are all about the same thing.

00:26:15.036 --> 00:26:16.660
I mean, I love Occupy,
not in the terms

00:26:16.660 --> 00:26:20.840
of the political occupation but
the idea of occupying reality.

00:26:20.840 --> 00:26:21.460
Let's Occupy.

00:26:21.460 --> 00:26:23.240
JASON SILVA: Own this space.

00:26:23.240 --> 00:26:24.160
Be here now.

00:26:24.160 --> 00:26:25.660
DOUGLAS RUSHKOFF:
Digital does that,

00:26:25.660 --> 00:26:29.040
because I was a kid who went
from the television mass media

00:26:29.040 --> 00:26:32.800
experience to the digital
experience, which was

00:26:32.800 --> 00:26:34.334
so peer-to-peer and connected.

00:26:34.334 --> 00:26:36.250
JASON SILVA: It's
interesting to hear you talk

00:26:36.250 --> 00:26:38.820
about occupy this, be here now.

00:26:38.820 --> 00:26:41.540
I haven't been to Burning
Man, I'm embarrassed to say.

00:26:41.540 --> 00:26:43.960
But I need to make
it over there.

00:26:43.960 --> 00:26:46.146
I read Eric Davis's
essay on Burning Man.

00:26:46.146 --> 00:26:47.812
DOUGLAS RUSHKOFF: And
you've been there.

00:26:47.812 --> 00:26:49.290
JASON SILVA: Oh, my god.

00:26:49.290 --> 00:26:50.336
But one of the things--

00:26:50.336 --> 00:26:52.294
DOUGLAS RUSHKOFF: And
now it's not Burning Man,

00:26:52.294 --> 00:26:53.400
it's not Burning Man
anymore than the internet's

00:26:53.400 --> 00:26:53.950
the internet.

00:26:53.950 --> 00:26:55.200
I mean, Burning Man
is where, you know,

00:26:55.200 --> 00:26:57.510
if you're a young Google
executive, you go and prove

00:26:57.510 --> 00:26:58.969
you're cool by
living in a trailer.

00:26:58.969 --> 00:27:01.051
JASON SILVA: But the people
that have transcendent

00:27:01.051 --> 00:27:02.430
experiences there,
to me it seems

00:27:02.430 --> 00:27:06.930
like the place creates this
kind of radical novelty

00:27:06.930 --> 00:27:09.600
that by virtue of
just being bombarded

00:27:09.600 --> 00:27:13.194
by the juxtaposition
of so much originality,

00:27:13.194 --> 00:27:15.110
it's so different than
what you see every day.

00:27:15.110 --> 00:27:17.630
You don't have mental references
for what you're seeing,

00:27:17.630 --> 00:27:24.410
so it induces that wonder,
that imagination explosion.

00:27:24.410 --> 00:27:28.320
And what that does is
it's like a drug, right?

00:27:28.320 --> 00:27:31.620
Serves the purpose of
transforming your perception

00:27:31.620 --> 00:27:33.890
of the present moment
and the passing of time,

00:27:33.890 --> 00:27:38.280
the unfolding of time, and
just experience changes.

00:27:38.280 --> 00:27:41.590
Subjectivity is transformed
That's interesting

00:27:41.590 --> 00:27:43.920
because the hacking of
subjectivity is like--

00:27:43.920 --> 00:27:44.878
DOUGLAS RUSHKOFF: Yeah.

00:27:44.878 --> 00:27:47.810
But to the purpose of then being
able to recognize and foster

00:27:47.810 --> 00:27:49.750
the things that you
see in real life.

00:27:49.750 --> 00:27:54.140
I was at a tree-trimming party
with a pinata the other day.

00:27:54.140 --> 00:27:55.940
And the pinata fell,
and all the boys

00:27:55.940 --> 00:27:56.900
are shoving and
kicking and trying

00:27:56.900 --> 00:27:58.873
and everybody gets their
candy and their little toys

00:27:58.873 --> 00:27:59.440
and all that.

00:27:59.440 --> 00:28:01.200
And we're all like, oh, my god.

00:28:01.200 --> 00:28:02.430
This is all so--

00:28:02.430 --> 00:28:04.970
And then 10 minutes
later, nobody's

00:28:04.970 --> 00:28:06.580
looking because then
the kids are off.

00:28:06.580 --> 00:28:08.801
I see, like, five
girls in a circle,

00:28:08.801 --> 00:28:10.050
and my daughter's one of them.

00:28:10.050 --> 00:28:12.020
These are eight-year-old
girls in a circle.

00:28:12.020 --> 00:28:15.860
And they're trading
candy with each other.

00:28:15.860 --> 00:28:19.120
And it's this very
sophisticated sort

00:28:19.120 --> 00:28:21.270
of modalities of what
different things are worth.

00:28:21.270 --> 00:28:24.650
There's a piece of plastic, it's
like a toy, and that's worth--

00:28:24.650 --> 00:28:27.850
And so immediately
a peer-to-peer

00:28:27.850 --> 00:28:30.210
bazaar in the medieval sense.

00:28:30.210 --> 00:28:31.650
A bazaar assembled.

00:28:31.650 --> 00:28:33.856
It was social, it
was commercial.

00:28:36.850 --> 00:28:38.700
And I was, like, it happens.

00:28:38.700 --> 00:28:39.785
It does happen.

00:28:42.430 --> 00:28:43.430
That was Burning Man.

00:28:43.430 --> 00:28:46.895
It happened there
without the drugs

00:28:46.895 --> 00:28:48.030
and without the computers.

00:28:48.030 --> 00:28:49.230
JASON SILVA: Right, right.

00:28:49.230 --> 00:28:53.120
So OK, I want to ask
you about creativity.

00:28:53.120 --> 00:28:54.309
Just like big word.

00:28:54.309 --> 00:28:56.600
DOUGLAS RUSHKOFF: I should
ask you as the creative one.

00:28:56.600 --> 00:28:57.930
JASON SILVA: Well, we're
both creative guys.

00:28:57.930 --> 00:28:59.304
DOUGLAS RUSHKOFF:
I'm the author.

00:28:59.304 --> 00:29:01.460
JASON SILVA: How do
you just randomly just

00:29:01.460 --> 00:29:03.690
define creativity?

00:29:03.690 --> 00:29:04.710
Moments of aha.

00:29:04.710 --> 00:29:08.510
What happens when
human beings step out

00:29:08.510 --> 00:29:10.910
of their usual
mental frameworks?

00:29:10.910 --> 00:29:11.630
What happens?

00:29:11.630 --> 00:29:13.400
What happens in the
brain, what results?

00:29:13.400 --> 00:29:14.340
How do you define creativity?

00:29:14.340 --> 00:29:15.839
DOUGLAS RUSHKOFF:
For me, creativity

00:29:15.839 --> 00:29:19.020
is, almost a fun way to
describe what creativity

00:29:19.020 --> 00:29:22.270
is, and no one has a great
definition for it, especially

00:29:22.270 --> 00:29:23.480
in a market society.

00:29:23.480 --> 00:29:28.030
But creativity is when humans
do what only humans can do.

00:29:28.030 --> 00:29:34.510
Creativity is the
expression of the paradox,

00:29:34.510 --> 00:29:39.830
you know, the ambiguity, the
queerness, the weirdness.

00:29:39.830 --> 00:29:42.660
It's these unique,
novel possibilities.

00:29:42.660 --> 00:29:43.410
JASON SILVA: Yeah.

00:29:43.410 --> 00:29:44.130
Well, it's interesting
because I've

00:29:44.130 --> 00:29:46.005
been thinking a lot
about what you just said,

00:29:46.005 --> 00:29:47.710
that ambiguity
and this and that.

00:29:47.710 --> 00:29:51.590
The word "liminality" has
to do with the in-between.

00:29:51.590 --> 00:29:55.120
So I've become really
interested in these immersive,

00:29:55.120 --> 00:29:57.495
site-specific interactive
theater experiences.

00:29:57.495 --> 00:29:59.870
I was talking to you about
"Punchdrunk," "Sleep No More."

00:29:59.870 --> 00:30:02.161
There's another one in Brooklyn
called "Then She Fell,"

00:30:02.161 --> 00:30:05.260
which is a reference to Alice
tumbling down the rabbit hole.

00:30:05.260 --> 00:30:07.350
And in these theater
experiences they

00:30:07.350 --> 00:30:09.720
create this space where you
can pretty much go anywhere

00:30:09.720 --> 00:30:11.380
you want.

00:30:11.380 --> 00:30:14.090
And it's this nonlinear,
interactive experience

00:30:14.090 --> 00:30:14.930
of the narrative.

00:30:14.930 --> 00:30:17.030
So you don't
experience it in order,

00:30:17.030 --> 00:30:19.950
and you kind of are connecting
the dots as you experience it.

00:30:19.950 --> 00:30:21.490
And figuring out
what's going on is

00:30:21.490 --> 00:30:24.540
kind of the purpose of
what you're supposed to do.

00:30:24.540 --> 00:30:28.380
But I describe the
experience as being

00:30:28.380 --> 00:30:30.270
inside of someone else's dream.

00:30:30.270 --> 00:30:33.742
And so it is this liminal space
between dreams and reality,

00:30:33.742 --> 00:30:35.700
kind of like when you're
watching a movie, kind

00:30:35.700 --> 00:30:37.050
of like when you're
watching a piece of theater.

00:30:37.050 --> 00:30:39.140
Time, space, they
kind of collapse.

00:30:39.140 --> 00:30:42.390
You can criss-cross and
cross-cut to anywhere,

00:30:42.390 --> 00:30:44.780
experiencing anything,
and you believe it.

00:30:44.780 --> 00:30:47.450
So that's the landscape
of mind and the landscape

00:30:47.450 --> 00:30:48.490
of imagination.

00:30:48.490 --> 00:30:52.370
Do you think we'll ever be
able to create virtual reality

00:30:52.370 --> 00:30:55.440
technologies that allow us to
essentially live in that space

00:30:55.440 --> 00:30:56.540
all the time?

00:30:56.540 --> 00:30:58.700
To live in landscapes of mind?

00:30:58.700 --> 00:31:01.740
To just completely leave
behind the sort of gravity

00:31:01.740 --> 00:31:04.880
of the body and its
rigid, have to feed it,

00:31:04.880 --> 00:31:06.340
have to go to
sleep at this time?

00:31:06.340 --> 00:31:08.350
Just live in this
wonderland all the time?

00:31:08.350 --> 00:31:12.540
DOUGLAS RUSHKOFF: My sense
is if we do it, it will be,

00:31:12.540 --> 00:31:17.690
that the price will be we
leave behind a few billion

00:31:17.690 --> 00:31:23.210
of our fellows to be in
nonmediated, wonderful, virtual

00:31:23.210 --> 00:31:23.710
things.

00:31:23.710 --> 00:31:26.670
I mean, it's the great way
to get the city on the cloud,

00:31:26.670 --> 00:31:28.210
so we can all be
in virtual reality

00:31:28.210 --> 00:31:32.650
as long as we have little
brown people in subcontinents

00:31:32.650 --> 00:31:36.069
feeding the coal into the
whatever, into the processors.

00:31:36.069 --> 00:31:38.610
JASON SILVA: That's assuming we
wouldn't have, like, nanotech

00:31:38.610 --> 00:31:40.690
or solar technologies
that could make

00:31:40.690 --> 00:31:42.850
this perfectly sustainable.

00:31:42.850 --> 00:31:44.460
DOUGLAS RUSHKOFF: Yeah, it is.

00:31:44.460 --> 00:31:45.590
It is.

00:31:45.590 --> 00:31:50.210
Because I don't think that
we would have the drive

00:31:50.210 --> 00:31:55.230
to inhabit an alternative
reality unless there

00:31:55.230 --> 00:31:57.800
was something we were
trying to get away from.

00:31:57.800 --> 00:32:04.010
Be it some Philip K. Dick in
apocalyptic, dry desert future.

00:32:04.010 --> 00:32:06.150
It's like, oh, shit, I'm
going into Second Life.

00:32:06.150 --> 00:32:08.130
See you later, honey.

00:32:08.130 --> 00:32:10.130
JASON SILVA: So you always
think that the appeal

00:32:10.130 --> 00:32:13.230
is that this is bad and
that is good, rather

00:32:13.230 --> 00:32:14.677
than this is pretty good.

00:32:14.677 --> 00:32:16.260
Humanity's come to
this certain point.

00:32:16.260 --> 00:32:18.020
But now there's
this mental universe

00:32:18.020 --> 00:32:20.320
we can go to that's
infinite and boundless.

00:32:20.320 --> 00:32:22.060
That was the dream
of cyberspace.

00:32:22.060 --> 00:32:24.530
There were corridors
of the mind.

00:32:24.530 --> 00:32:25.420
William Gibson.

00:32:25.420 --> 00:32:29.800
DOUGLAS RUSHKOFF: I think the
better the virtual realities

00:32:29.800 --> 00:32:32.190
we build, the
better I would think

00:32:32.190 --> 00:32:33.800
people will get
at distinguishing

00:32:33.800 --> 00:32:37.104
between the virtual
and the real.

00:32:37.104 --> 00:32:38.520
JASON SILVA: Kevin
Kelly says real

00:32:38.520 --> 00:32:41.156
is going to be a really
relative term in the future.

00:32:41.156 --> 00:32:42.780
DOUGLAS RUSHKOFF:
Yeah, but Kevin Kelly

00:32:42.780 --> 00:32:47.510
is a person who as a
certain kind of Christian

00:32:47.510 --> 00:32:49.460
imagines the end of the world.

00:32:49.460 --> 00:32:51.770
I mean, there are
people that are still

00:32:51.770 --> 00:32:53.810
stuck in what I
would consider to be

00:32:53.810 --> 00:32:58.690
a Renaissance-era linear arc
of sort of beginnings, middles,

00:32:58.690 --> 00:32:59.260
and ends.

00:32:59.260 --> 00:33:01.090
They're not playing
the infinite game

00:33:01.090 --> 00:33:03.940
where the object of the game
is to keep the game going.

00:33:03.940 --> 00:33:06.940
They're still playing
a Renaissance-era game,

00:33:06.940 --> 00:33:09.540
where the object of the
game is to win the game.

00:33:09.540 --> 00:33:11.650
They're stuck in the
culture of the book.

00:33:11.650 --> 00:33:12.491
And I write books.

00:33:12.491 --> 00:33:12.990
I love them.

00:33:12.990 --> 00:33:15.073
But books are about
beginnings, middles, and ends.

00:33:15.073 --> 00:33:18.310
Whereas games, video games, are
about keeping the game going.

00:33:18.310 --> 00:33:18.486
JASON SILVA: When are
you going to write

00:33:18.486 --> 00:33:18.600
a book that never ends?

00:33:18.600 --> 00:33:19.600
DOUGLAS RUSHKOFF: Can't.

00:33:19.600 --> 00:33:21.026
Can't do it.

00:33:21.026 --> 00:33:21.525
Luckily.

00:33:25.015 --> 00:33:26.752
But no, I mean, it's--

00:33:26.752 --> 00:33:28.210
JASON SILVA: When
Kevin Kelly talks

00:33:28.210 --> 00:33:30.980
about the technium, when
he describes technology

00:33:30.980 --> 00:33:32.530
as the seventh kingdom of life.

00:33:32.530 --> 00:33:34.130
And he says that
it's a living thing

00:33:34.130 --> 00:33:37.740
subject to evolutionary forces,
that has wants and needs.

00:33:37.740 --> 00:33:40.570
Do you think that
this is just crap?

00:33:40.570 --> 00:33:43.180
Or do you think that there's
a truth to what he's saying?

00:33:43.180 --> 00:33:46.810
DOUGLAS RUSHKOFF: I think
it is like a living thing.

00:33:46.810 --> 00:33:52.390
I think it's
algorithmic in nature.

00:33:52.390 --> 00:33:54.450
It's a program.

00:33:54.450 --> 00:33:56.675
And while people are very
algorithmic at times.

00:33:56.675 --> 00:33:58.300
JASON SILVA: Is
biology a program, too?

00:33:58.300 --> 00:33:59.550
DOUGLAS RUSHKOFF:
It's like a program.

00:33:59.550 --> 00:34:01.870
And there's many things that
are programmatic about it.

00:34:01.870 --> 00:34:05.440
But the more you hone
in on that program,

00:34:05.440 --> 00:34:08.620
it squirts away from you like a
tomato seed under your finger.

00:34:08.620 --> 00:34:10.610
And you realize, oh,
it's not the DNA,

00:34:10.610 --> 00:34:12.100
it's the proteins
around the DNA.

00:34:12.100 --> 00:34:12.780
Well, of course.

00:34:12.780 --> 00:34:13.630
Well, now we know.

00:34:13.630 --> 00:34:14.699
Oh, it's the proteins.

00:34:14.699 --> 00:34:15.782
Oh, it's not the proteins.

00:34:15.782 --> 00:34:17.780
It's like something else.

00:34:17.780 --> 00:34:20.110
JASON SILVA: Well, a lot of
complexity for sure, yes.

00:34:20.110 --> 00:34:21.570
DOUGLAS RUSHKOFF: It's not
just complexity, though.

00:34:21.570 --> 00:34:22.370
It's not.

00:34:22.370 --> 00:34:25.179
I'm not, in the end,
I'm not a materialist.

00:34:25.179 --> 00:34:28.880
I think that there's something
going on here that we don't yet

00:34:28.880 --> 00:34:29.770
know about.

00:34:29.770 --> 00:34:35.380
I guess it's Kantian or
something, that we barely

00:34:35.380 --> 00:34:40.540
even perceive reality, much
less are we able-- We can't even

00:34:40.540 --> 00:34:44.310
recreate a simulation
of what we perceive,

00:34:44.310 --> 00:34:48.692
much less the 99.9% of reality
that we don't perceive.

00:34:48.692 --> 00:34:50.900
JASON SILVA: So are you
talking about the limitations

00:34:50.900 --> 00:34:52.691
in understanding our
own consciousness, not

00:34:52.691 --> 00:34:55.214
to mention our limitations
in creating instruments

00:34:55.214 --> 00:34:56.130
to perceive the world?

00:34:56.130 --> 00:34:57.260
Because we can't
see the very small,

00:34:57.260 --> 00:34:58.440
but we create a microscope.

00:34:58.440 --> 00:35:01.460
We can't see the very large,
but we create a telescope.

00:35:01.460 --> 00:35:04.840
We can peer into the, like,
the universe pretty vividly.

00:35:04.840 --> 00:35:05.770
You're saying what?

00:35:05.770 --> 00:35:06.050
You're saying--

00:35:06.050 --> 00:35:08.008
DOUGLAS RUSHKOFF: These
guys buy a model vague,

00:35:08.008 --> 00:35:09.820
buy a model of reality,
and it may be true

00:35:09.820 --> 00:35:13.159
where there was this
matter, there was a boom,

00:35:13.159 --> 00:35:15.200
and then all these things
banged into each other.

00:35:15.200 --> 00:35:16.240
And they got more complex.

00:35:16.240 --> 00:35:18.073
And they got so complex
and eventually there

00:35:18.073 --> 00:35:20.050
was emerging
consciousness from it.

00:35:20.050 --> 00:35:22.350
That there was the Big Bang,
and then there was time.

00:35:22.350 --> 00:35:23.749
Well, what if they're wrong?

00:35:23.749 --> 00:35:25.540
Because they still
don't think that at all.

00:35:25.540 --> 00:35:25.913
JASON SILVA: So
you think there's

00:35:25.913 --> 00:35:26.955
a more satisfying answer?

00:35:26.955 --> 00:35:29.288
DOUGLAS RUSHKOFF: What if
consciousness preceded the Big

00:35:29.288 --> 00:35:29.834
Bang?

00:35:29.834 --> 00:35:31.500
What of consciousness
was a prerequisite

00:35:31.500 --> 00:35:34.210
for the Big Bang rather than
the result of the Big Bang?

00:35:34.210 --> 00:35:36.610
Then if it was, then,
well, wait a minute, now

00:35:36.610 --> 00:35:38.100
what are we playing with?

00:35:38.100 --> 00:35:40.640
So if we don't understand the
sort of very basic building

00:35:40.640 --> 00:35:45.620
blocks of reality, I'm
concerned about ditching reality

00:35:45.620 --> 00:35:46.620
for another one.

00:35:46.620 --> 00:35:50.780
I love Playland, I love
Disneyland, I love movies,

00:35:50.780 --> 00:35:51.920
I love all these things.

00:35:51.920 --> 00:35:54.610
But I would never
mistake them for reality.

00:35:54.610 --> 00:35:55.830
Disneyland is a great idea.

00:35:55.830 --> 00:35:58.995
I mean, a world where the
trains worked on time,

00:35:58.995 --> 00:36:00.495
I feel like Disneyland
is what would

00:36:00.495 --> 00:36:03.230
have happened if the Nazis
had won and weren't evil.

00:36:03.230 --> 00:36:03.970
[LAUGHTER]

00:36:03.970 --> 00:36:05.370
JASON SILVA: Oh, god.

00:36:05.370 --> 00:36:07.930
DOUGLAS RUSHKOFF:
And that's fine.

00:36:07.930 --> 00:36:08.790
That's fine.

00:36:08.790 --> 00:36:11.020
But it's not the world.

00:36:11.020 --> 00:36:12.770
JASON SILVA: But let
me ask you something.

00:36:12.770 --> 00:36:16.560
You talked about how,
OK, you like movies,

00:36:16.560 --> 00:36:19.940
you like video games,
but they're not reality.

00:36:19.940 --> 00:36:22.790
One of the interesting ideas in
Chris Nolan's film "Inception"

00:36:22.790 --> 00:36:26.070
is this notion that the dream
is real when you're in it.

00:36:26.070 --> 00:36:29.410
So if the simulation,
if the representation,

00:36:29.410 --> 00:36:33.650
can become immersive enough,
can you ever essentially trick,

00:36:33.650 --> 00:36:36.300
hack, or convince
your brain to forget

00:36:36.300 --> 00:36:39.300
to remember that it's subjecting
itself to a simulation?

00:36:39.300 --> 00:36:40.216
DOUGLAS RUSHKOFF: Yes.

00:36:40.216 --> 00:36:40.300
Absolutely.

00:36:40.300 --> 00:36:41.841
JASON SILVA: For
example, when I went

00:36:41.841 --> 00:36:44.830
to see "Gravity" on IMAX
in 3D, which I thought

00:36:44.830 --> 00:36:47.910
was a technical achievement film
I'd never seen before in terms

00:36:47.910 --> 00:36:49.720
of the places where
the camera could go,

00:36:49.720 --> 00:36:51.610
it just didn't make sense.

00:36:51.610 --> 00:36:52.950
You couldn't place it.

00:36:52.950 --> 00:36:56.580
And it was so immersive that
during certain sequences,

00:36:56.580 --> 00:37:00.160
my body would jolt with
the movements on screen

00:37:00.160 --> 00:37:02.640
as if it was happening to me.

00:37:02.640 --> 00:37:05.240
It was going primordial,
deeper than my ability

00:37:05.240 --> 00:37:07.080
to remember this is a movie.

00:37:07.080 --> 00:37:09.620
And I think that
eventually, more and more

00:37:09.620 --> 00:37:12.210
of our simulated experiences
are going to be like that.

00:37:12.210 --> 00:37:16.030
So it won't matter
if they're not real.

00:37:16.030 --> 00:37:18.600
When they're done, because
when you're in them,

00:37:18.600 --> 00:37:20.190
they will become real.

00:37:20.190 --> 00:37:22.517
The dream will be real
while you're in it.

00:37:22.517 --> 00:37:23.600
What do you think of that?

00:37:23.600 --> 00:37:24.410
[LAUGHTER]

00:37:24.410 --> 00:37:26.822
It's a cool idea, right?

00:37:26.822 --> 00:37:27.780
DOUGLAS RUSHKOFF: Yeah.

00:37:27.780 --> 00:37:31.294
JASON SILVA: I'm just trying
to convince you to like this.

00:37:31.294 --> 00:37:32.460
DOUGLAS RUSHKOFF: I like it.

00:37:32.460 --> 00:37:36.810
I just don't like it
more than what I got.

00:37:36.810 --> 00:37:40.450
I like it, sure, it's
great, it's great,

00:37:40.450 --> 00:37:42.330
cool, go for it, wonderful.

00:37:42.330 --> 00:37:46.340
But if we are going to build
these things, let's go back

00:37:46.340 --> 00:37:48.700
and say, OK, what's the
purpose of these things?

00:37:48.700 --> 00:37:51.270
The purpose could be just
because I want to get high.

00:37:51.270 --> 00:37:54.360
Just like the kid taking
acid in the AC/DC parking lot

00:37:54.360 --> 00:37:57.250
instead of in the dorm room,
trying to figure out the world.

00:37:57.250 --> 00:37:59.750
So you're going to do it, and
you're just going to get high.

00:37:59.750 --> 00:38:01.390
JASON SILVA: The
purpose is catharsis.

00:38:01.390 --> 00:38:03.390
Well, we would talk about
psychedelic therapies.

00:38:03.390 --> 00:38:06.240
But it feels like
human beings, we

00:38:06.240 --> 00:38:09.890
need to have regular
experiences that are cathartic.

00:38:09.890 --> 00:38:14.060
These ego death,
rebirth simulations.

00:38:14.060 --> 00:38:16.380
I was talking about Alan
Harrington, who wrote

00:38:16.380 --> 00:38:17.890
the book "The Immortalists."

00:38:17.890 --> 00:38:19.920
That even our lovers,
even when we're in love,

00:38:19.920 --> 00:38:23.710
our lovers act as stand-ins
for various dream figures

00:38:23.710 --> 00:38:25.750
in a stage-managed resurrection.

00:38:25.750 --> 00:38:28.130
Where the pilgrim without
faith can die and live again.

00:38:28.130 --> 00:38:29.430
DOUGLAS RUSHKOFF: So why
not just do it in a dream?

00:38:29.430 --> 00:38:31.650
JASON SILVA: We need
these archetype spaces,

00:38:31.650 --> 00:38:34.690
these landscapes of mind,
where we sort shit out.

00:38:34.690 --> 00:38:39.000
Like that's the world, the world
of the self, the world inside.

00:38:39.000 --> 00:38:42.190
And that's what these
movies connect to, I think.

00:38:42.190 --> 00:38:43.450
DOUGLAS RUSHKOFF: They do.

00:38:43.450 --> 00:38:44.950
They do, and that's fine.

00:38:44.950 --> 00:38:48.280
It's just the world of
the self can be ultimately

00:38:48.280 --> 00:38:49.950
a lonely place.

00:38:49.950 --> 00:38:52.780
It's not a social place.

00:38:52.780 --> 00:38:54.020
Work on yourself.

00:38:54.020 --> 00:38:55.110
Sure, you know.

00:38:55.110 --> 00:38:56.120
And this is New York.

00:38:56.120 --> 00:38:59.390
Everyone goes to therapy for 45
minutes once a week or twice.

00:38:59.390 --> 00:39:02.009
I mean, they all know what
that inner work is about

00:39:02.009 --> 00:39:02.800
and why it's great.

00:39:02.800 --> 00:39:05.425
But the reason we do inner work
is that we go out of the office

00:39:05.425 --> 00:39:07.180
and then engage
with people and deal

00:39:07.180 --> 00:39:09.760
with the real problems
of distributing

00:39:09.760 --> 00:39:13.700
the surplus of stuff that
we have in America somehow

00:39:13.700 --> 00:39:16.530
to all the people that need
it, instead of burning food

00:39:16.530 --> 00:39:18.630
every week and ripping
down houses in California

00:39:18.630 --> 00:39:20.213
because the prices
aren't high enough.

00:39:20.213 --> 00:39:26.402
It's like the practical
concerns are so real.

00:39:26.402 --> 00:39:28.110
JASON SILVA: I completely
agree with you.

00:39:28.110 --> 00:39:30.764
But the practical concerns are
also, they are being addressed.

00:39:30.764 --> 00:39:32.930
I mean, there was an article
in "The New York Times"

00:39:32.930 --> 00:39:36.690
recently that said that the
lowest level of poverty,

00:39:36.690 --> 00:39:38.260
the most extreme
form of poverty,

00:39:38.260 --> 00:39:40.736
has almost been eliminated
across the world.

00:39:40.736 --> 00:39:42.610
You have Steven Pinker,
who wrote "The Better

00:39:42.610 --> 00:39:44.265
Angels of Our Nature"
and his TED talk,

00:39:44.265 --> 00:39:45.640
"The Myth of
Violence," that says

00:39:45.640 --> 00:39:47.410
that, contrary to what
you see on the news,

00:39:47.410 --> 00:39:49.659
the chances of a man dying
at the hands of another man

00:39:49.659 --> 00:39:51.801
are the lowest than they've
ever been in history.

00:39:51.801 --> 00:39:53.300
I mean, there's all
these indicators

00:39:53.300 --> 00:39:55.367
that things are getting better.

00:39:55.367 --> 00:39:56.950
But the media feeds
us doom and gloom.

00:39:56.950 --> 00:39:59.241
So I mean, obviously, everything
you're saying is true.

00:39:59.241 --> 00:40:02.250
But it's not like there's not
an active effort right now

00:40:02.250 --> 00:40:05.300
at addressing the fundamental
challenges of humanity.

00:40:05.300 --> 00:40:07.260
Do you have optimism
at all in that space?

00:40:07.260 --> 00:40:09.135
DOUGLAS RUSHKOFF: Oh,
I got lots of optimism.

00:40:09.135 --> 00:40:10.700
I just want to
make sure we don't

00:40:10.700 --> 00:40:13.770
use these medicines the
same way that, say, America

00:40:13.770 --> 00:40:15.870
uses Prozac, which is a way--

00:40:15.870 --> 00:40:17.110
JASON SILVA: Fair enough.

00:40:17.110 --> 00:40:18.550
DOUGLAS RUSHKOFF: --to--

00:40:18.550 --> 00:40:19.470
JASON SILVA: To numb.

00:40:19.470 --> 00:40:21.261
DOUGLAS RUSHKOFF: --to
numb, yeah, exactly.

00:40:21.261 --> 00:40:25.210
To help us cope
with reality as it

00:40:25.210 --> 00:40:29.950
is rather than engage reality
to make it less painful.

00:40:29.950 --> 00:40:32.410
JASON SILVA: And on
that note, I think

00:40:32.410 --> 00:40:35.487
it's time for our
Q&amp;A. It's about 9:00.

00:40:35.487 --> 00:40:37.320
DOUGLAS RUSHKOFF: And
don't assume that this

00:40:37.320 --> 00:40:39.430
is like pro-anti here.

00:40:39.430 --> 00:40:42.370
We're both pro and
anti and all that.

00:40:42.370 --> 00:40:44.980
It's sort of the rules
of brain jazz, in a way.

00:40:44.980 --> 00:40:47.010
You kind of stake out positions.

00:40:47.010 --> 00:40:49.780
On another night, under
different medications,

00:40:49.780 --> 00:40:54.400
we could as easily be
taking opposite stands.

00:40:54.400 --> 00:40:59.160
It's just sort of
where I'm at right now.

00:40:59.160 --> 00:41:02.860
Really it all for me, it's
my disappointment in Bitcoin.

00:41:02.860 --> 00:41:04.860
JASON SILVA: I read an
article about that today.

00:41:04.860 --> 00:41:05.460
DOUGLAS RUSHKOFF: It really is.

00:41:05.460 --> 00:41:06.660
I'm disappointed.

00:41:06.660 --> 00:41:10.640
We have the opportunity to
invent entirely new currencies

00:41:10.640 --> 00:41:14.420
rather than just
remake an investor's

00:41:14.420 --> 00:41:17.044
speculative central currency
in a digital format.

00:41:17.044 --> 00:41:17.960
It's, like, so stupid.

00:41:22.099 --> 00:41:23.140
INTERVIEWER 1: Mic check.

00:41:23.140 --> 00:41:24.900
Hi.

00:41:24.900 --> 00:41:27.140
You guys talked at
different points

00:41:27.140 --> 00:41:31.740
throughout the brain jam
about these value shifts.

00:41:31.740 --> 00:41:35.530
I think you talked about
how in the Industrial Age,

00:41:35.530 --> 00:41:38.240
we got stripped of some
pre-existing values,

00:41:38.240 --> 00:41:41.070
and we got some new ones.

00:41:41.070 --> 00:41:45.390
And I've heard other people
refer to the digital age as

00:41:45.390 --> 00:41:47.544
also a knowledge age.

00:41:47.544 --> 00:41:49.710
Some of the values are
already kind of like embedded

00:41:49.710 --> 00:41:50.626
in what you're saying.

00:41:50.626 --> 00:41:54.080
So like the
peer-to-peer network.

00:41:54.080 --> 00:41:58.260
That's obviously kind of
like a value that rises up,

00:41:58.260 --> 00:42:02.080
post-Industrial Age,
whatever we're in now.

00:42:02.080 --> 00:42:05.750
Looking into, like,
let's say 2050,

00:42:05.750 --> 00:42:10.290
where you do have this kind
of radical transformation,

00:42:10.290 --> 00:42:14.380
radical kind of integration
of organic matter

00:42:14.380 --> 00:42:18.260
and artificial solar
technology, all this stuff

00:42:18.260 --> 00:42:20.385
just like it's at
a mature stage.

00:42:20.385 --> 00:42:22.510
I wouldn't say it's
stabilized, but it's definitely

00:42:22.510 --> 00:42:23.570
like at a mature stage.

00:42:23.570 --> 00:42:29.010
What are the values
that humanity will hold

00:42:29.010 --> 00:42:32.060
and prioritize in the future?

00:42:34.482 --> 00:42:35.440
JASON SILVA: Who knows?

00:42:35.440 --> 00:42:36.590
DOUGLAS RUSHKOFF:
You're going to be here.

00:42:36.590 --> 00:42:38.167
I mean, I think
right now that's part

00:42:38.167 --> 00:42:39.750
of why this moment's
so interesting is

00:42:39.750 --> 00:42:41.290
it's up for grabs.

00:42:41.290 --> 00:42:41.790
You know?

00:42:41.790 --> 00:42:44.360
Right now we are
choosing whether we're

00:42:44.360 --> 00:42:47.190
going to reify the values
of the Industrial Age, which

00:42:47.190 --> 00:42:49.890
are efficiency over time,
extracting value out of people

00:42:49.890 --> 00:42:51.330
and resources, and all that.

00:42:51.330 --> 00:42:53.200
In order to push
through somehow,

00:42:53.200 --> 00:42:55.310
do the Industrial
Age on steroids.

00:42:55.310 --> 00:42:58.360
Or are we going to
use this opportunity

00:42:58.360 --> 00:43:02.600
to do some cultural resetting
and retrieve the values that

00:43:02.600 --> 00:43:04.430
were repressed the
last time out, which is

00:43:04.430 --> 00:43:05.745
when the Renaissance happened?

00:43:05.745 --> 00:43:07.120
When the Renaissance
happened, we

00:43:07.120 --> 00:43:11.390
got the invention of the
individual, and centralization,

00:43:11.390 --> 00:43:13.560
and perspective.

00:43:13.560 --> 00:43:15.530
We got a whole lot of values.

00:43:15.530 --> 00:43:23.020
And other ones were
oppressed, like women, women,

00:43:23.020 --> 00:43:27.550
and peer-to-peer,
local, the bazaar,

00:43:27.550 --> 00:43:29.420
all those kinds of things.

00:43:29.420 --> 00:43:33.020
I would love to say
in 2050 we end up

00:43:33.020 --> 00:43:37.610
in a reality where
80%, 85% of our needs

00:43:37.610 --> 00:43:40.830
are met locally through
peer-to-peer networks of people

00:43:40.830 --> 00:43:43.660
we're engaged with in
the places that we live.

00:43:43.660 --> 00:43:47.090
And 10% or 15% of
our weird, cool stuff

00:43:47.090 --> 00:43:50.650
comes from big international
high-tech companies that

00:43:50.650 --> 00:43:54.520
make us phones that have
very big supply chains, that

00:43:54.520 --> 00:43:59.000
will move into either
some kind of a world with

00:43:59.000 --> 00:44:02.100
and many cottage industries
that are all networked

00:44:02.100 --> 00:44:05.620
and it would be
vastly more efficient.

00:44:05.620 --> 00:44:09.230
Or if the Big Data
people have their way,

00:44:09.230 --> 00:44:12.140
we'll end up entirely
more predictable

00:44:12.140 --> 00:44:14.020
than we've been before.

00:44:14.020 --> 00:44:16.140
And we'll be easily
influenced to just purchase

00:44:16.140 --> 00:44:17.575
whatever the next thing is.

00:44:17.575 --> 00:44:19.200
JASON SILVA: There
was a recent article

00:44:19.200 --> 00:44:21.520
that said that physicists
discovered basically

00:44:21.520 --> 00:44:24.810
a law of physics that can
predict the growth of cities.

00:44:24.810 --> 00:44:26.760
So even the behavior
of human beings

00:44:26.760 --> 00:44:28.530
and the unpredictable
changes that

00:44:28.530 --> 00:44:31.310
happen, that it can all be
described by an equation.

00:44:31.310 --> 00:44:33.196
That cities are like
organisms and they have

00:44:33.196 --> 00:44:34.320
all these metabolic rights.

00:44:34.320 --> 00:44:35.290
So who knows?

00:44:35.290 --> 00:44:36.930
Maybe what Big Data
is going to reveal

00:44:36.930 --> 00:44:39.460
is something that it
scares us to admit,

00:44:39.460 --> 00:44:41.710
which is that we're not
really these free, autonomous,

00:44:41.710 --> 00:44:42.530
creative beings.

00:44:42.530 --> 00:44:44.650
Even though it feels
that way subjectively.

00:44:44.650 --> 00:44:47.030
But at a macro scale and
looking at all the data

00:44:47.030 --> 00:44:51.520
points, that we're just
algorithmic cascades after all.

00:44:51.520 --> 00:44:54.170
And we have to maybe
live with that duality.

00:44:54.170 --> 00:44:56.569
Like sure, the Big
Data AIs can predict

00:44:56.569 --> 00:44:57.860
everything we're going to want.

00:44:57.860 --> 00:44:59.480
But it doesn't feel
that way to me.

00:44:59.480 --> 00:45:01.640
For us it'll feel like
we live in this carnival

00:45:01.640 --> 00:45:04.277
land of novelty, where
everything is interesting

00:45:04.277 --> 00:45:05.360
and everything is awesome.

00:45:05.360 --> 00:45:08.140
And people in the
imagination business in 2050

00:45:08.140 --> 00:45:09.805
have storytelling
technologies that

00:45:09.805 --> 00:45:12.570
are dazzling in their
capacity to wow us.

00:45:12.570 --> 00:45:16.689
And the movies like "Gravity" in
2050 will be that much cooler.

00:45:16.689 --> 00:45:18.730
DOUGLAS RUSHKOFF: And the
materialists, you know,

00:45:18.730 --> 00:45:21.280
dismiss this sensibility.

00:45:21.280 --> 00:45:22.500
They call it essentialism.

00:45:22.500 --> 00:45:23.640
Oh, it's essentialism.

00:45:23.640 --> 00:45:24.760
You're a sophist.

00:45:24.760 --> 00:45:27.080
You know, it's some
weird, romantic,

00:45:27.080 --> 00:45:31.130
melancholy for this idea
that human beings are weird

00:45:31.130 --> 00:45:35.800
and that your perception matters
so that you have free will.

00:45:35.800 --> 00:45:39.340
This illusion is
good enough for me.

00:45:39.340 --> 00:45:40.630
I'm going to work with it.

00:45:40.630 --> 00:45:42.080
I'm going to act
as if, I'm going

00:45:42.080 --> 00:45:44.360
to behave as if this is real.

00:45:44.360 --> 00:45:47.980
And as if they are the ones
who haven't figured things out.

00:45:47.980 --> 00:45:51.290
And at the very
least, there's to be

00:45:51.290 --> 00:45:54.595
a balance between
materialism and whatever

00:45:54.595 --> 00:45:55.470
we want to call this.

00:45:55.470 --> 00:45:57.620
Spirituality, for
lack of a better word.

00:45:57.620 --> 00:46:00.300
Rather than having
to fall all the way

00:46:00.300 --> 00:46:02.270
into one camp or another.

00:46:02.270 --> 00:46:06.110
And throughout history they've
generally moved hand in hand.

00:46:06.110 --> 00:46:08.130
And it was really only
after the invention

00:46:08.130 --> 00:46:12.110
of sort of rational science
and that this stuff that we're

00:46:12.110 --> 00:46:17.140
talking about got pooh-poohed as
being some kind of whoo, weird,

00:46:17.140 --> 00:46:22.210
womanly Fritjof Capra,
Wu Li Master nonsense.

00:46:22.210 --> 00:46:24.610
Where I think it's real.

00:46:24.610 --> 00:46:26.920
It's as real as your
perception of reality.

00:46:26.920 --> 00:46:31.970
And your perception of reality
is more, I would argue.

00:46:31.970 --> 00:46:36.130
All we really have to go
on more than the metaphors

00:46:36.130 --> 00:46:38.401
we have to describe it.

00:46:38.401 --> 00:46:39.442
INTERVIEWER 2: Thank you.

00:46:39.442 --> 00:46:40.980
Hey, guys.

00:46:40.980 --> 00:46:41.840
JASON SILVA: Hey.

00:46:41.840 --> 00:46:43.200
INTERVIEWER 2: Quick question.

00:46:43.200 --> 00:46:44.350
JASON SILVA: Where are you?

00:46:44.350 --> 00:46:46.120
INTERVIEWER 2: Here.

00:46:46.120 --> 00:46:50.037
So do you think drugs and
psychedelics are still

00:46:50.037 --> 00:46:51.870
going to be around in
the age of singularity

00:46:51.870 --> 00:46:53.490
or 50 or 100 years from now?

00:46:53.490 --> 00:46:55.190
Like, how are people
going to get high?

00:46:55.190 --> 00:46:57.564
Is it going to be in the form
of a hack or computer code,

00:46:57.564 --> 00:47:00.110
or how are they going to open
up those doors of perception?

00:47:00.110 --> 00:47:01.690
JASON SILVA: Well,
this is the thing.

00:47:01.690 --> 00:47:04.700
Even the way that people
describe drugs, in most cases,

00:47:04.700 --> 00:47:06.547
is not as interesting
to me as some people.

00:47:06.547 --> 00:47:08.130
I don't know if you
talked about this.

00:47:08.130 --> 00:47:11.210
But this guy, Rich Doyle,
whom I really enjoy as well.

00:47:11.210 --> 00:47:14.380
He says that drugs are actually
information technologies.

00:47:14.380 --> 00:47:18.460
And the philosopher Eliade calls
them technologies of ecstasy.

00:47:18.460 --> 00:47:21.890
And he talks about
humanity's cognitive toolkit

00:47:21.890 --> 00:47:25.410
as essentially stuff
that we found in nature,

00:47:25.410 --> 00:47:28.580
but we've been using it as
part of our mental toolkit

00:47:28.580 --> 00:47:31.120
for tens of thousands of years.

00:47:31.120 --> 00:47:35.130
So I imagine we're going to have
a new generation of smart drugs

00:47:35.130 --> 00:47:37.640
because, just because
it's chemistry doesn't

00:47:37.640 --> 00:47:39.140
mean it's not technology.

00:47:39.140 --> 00:47:40.870
These are still
going to be tools

00:47:40.870 --> 00:47:46.270
that we use to play with
the essence of self.

00:47:46.270 --> 00:47:50.010
And I think that the
best is yet to come.

00:47:50.010 --> 00:47:51.760
People read a book
like "Brave New World"

00:47:51.760 --> 00:47:53.159
and they're like, oh, the soma.

00:47:53.159 --> 00:47:53.700
I don't know.

00:47:53.700 --> 00:47:56.600
I mean, there's a guy
called David Pearce.

00:47:56.600 --> 00:47:59.680
He wrote "The Hedonistic
Imperative" on the internet.

00:47:59.680 --> 00:48:02.630
It's a wonderful treatise about
how we're going to use nanotech

00:48:02.630 --> 00:48:06.860
and biotech to create dazzling
new spectrums of subjectivity,

00:48:06.860 --> 00:48:10.300
like realms of mind
that are unfathomable.

00:48:10.300 --> 00:48:15.230
As unfathomable to us as
the nuances of Shakespeare

00:48:15.230 --> 00:48:16.970
are to a great ape.

00:48:16.970 --> 00:48:20.810
And so it's useless to
even speculate, in a way.

00:48:20.810 --> 00:48:22.400
DOUGLAS RUSHKOFF: Yeah.

00:48:22.400 --> 00:48:28.670
And if we get really,
really good at fine tuning

00:48:28.670 --> 00:48:35.880
our shamanic alchemy, and we
can play the brain exactly

00:48:35.880 --> 00:48:41.120
as we want it, you know, cool.

00:48:41.120 --> 00:48:47.530
But there's something that the
mushrooms tell you that that

00:48:47.530 --> 00:48:51.640
synthetic perfectly
modulated chemical won't.

00:48:51.640 --> 00:48:53.150
The mushrooms are
going to tell you

00:48:53.150 --> 00:48:55.940
some stuff you may not
want to really hear

00:48:55.940 --> 00:48:59.420
when you're high, like you're
fucking up the planet here.

00:48:59.420 --> 00:49:02.090
Hey, you're cheating on
your girlfriend there.

00:49:02.090 --> 00:49:04.884
Why haven't you talked to
me in the last three years?

00:49:04.884 --> 00:49:06.800
I have something to
tell-- I mean, it's like--

00:49:06.800 --> 00:49:08.030
JASON SILVA: It's not
a guilt-free high.

00:49:08.030 --> 00:49:10.160
DOUGLAS RUSHKOFF: It's
not, it's not guilt.

00:49:10.160 --> 00:49:15.130
I mean it's not, it has, the
plants, the sacred plants,

00:49:15.130 --> 00:49:16.390
have agendas.

00:49:16.390 --> 00:49:18.190
We are in relationship
with ayahuasca,

00:49:18.190 --> 00:49:20.300
we're in relationship
with the Torah.

00:49:20.300 --> 00:49:23.520
And the ones that we make
we're not in relationship with.

00:49:23.520 --> 00:49:24.500
Then we're just going.

00:49:24.500 --> 00:49:27.809
So again, I think it's
a bit about balance.

00:49:27.809 --> 00:49:29.350
Rupert Sheldrake
talked about there's

00:49:29.350 --> 00:49:31.750
a morphic genetic field
of our whole species'

00:49:31.750 --> 00:49:33.080
memory with the plant.

00:49:33.080 --> 00:49:35.500
Maybe, but I think
it's the plant.

00:49:35.500 --> 00:49:37.840
I think it is the
plants that we've

00:49:37.840 --> 00:49:40.950
been in relationship with
for however many millennia.

00:49:40.950 --> 00:49:44.000
Understand, and they can
help balance us in ways

00:49:44.000 --> 00:49:47.540
that we might not choose
to balance ourselves

00:49:47.540 --> 00:49:50.715
if we moved exclusively toward
designer chemicals and designer

00:49:50.715 --> 00:49:51.715
states of consciousness.

00:49:55.500 --> 00:49:56.670
INTERVIEWER 3: Hey, guys.

00:49:56.670 --> 00:49:59.350
I just want to say thank you
for just blowing our mind,

00:49:59.350 --> 00:50:01.270
and that was way too fast.

00:50:01.270 --> 00:50:05.300
But bear with me while
I work this question.

00:50:05.300 --> 00:50:08.480
So first and
foremost, in a society

00:50:08.480 --> 00:50:12.530
where the only socially
condition and accepted plane

00:50:12.530 --> 00:50:16.000
of consciousness is kind
of that producer/consumer

00:50:16.000 --> 00:50:19.520
plane of consciousness, where
the only way that we can afford

00:50:19.520 --> 00:50:23.300
ourselves the opportunity
to explore the world of art

00:50:23.300 --> 00:50:28.130
and spirit is exactly to do
so, is to afford to do that.

00:50:28.130 --> 00:50:30.580
And create a company
where maybe we

00:50:30.580 --> 00:50:34.100
can unfurl through our
creative mind in that company.

00:50:34.100 --> 00:50:35.740
But of course, a
company or a business

00:50:35.740 --> 00:50:41.450
is there to create financial
security, to create money.

00:50:41.450 --> 00:50:44.025
I just want to know, how can
we afford the opportunity

00:50:44.025 --> 00:50:47.710
to stay in this world, the
world of art and spirit

00:50:47.710 --> 00:50:49.400
and mind the soul?

00:50:49.400 --> 00:50:51.320
But what I'm really
interested in

00:50:51.320 --> 00:50:53.880
is, what was your
answer to this question

00:50:53.880 --> 00:50:57.020
before you were Douglas
Rushkoff, media theorist

00:50:57.020 --> 00:50:57.890
and author?

00:50:57.890 --> 00:51:00.580
Or Jason Silva,
futurist techno-optimist

00:51:00.580 --> 00:51:01.890
and philosopher?

00:51:01.890 --> 00:51:04.250
How did you live
into this answer,

00:51:04.250 --> 00:51:08.317
and how has that changed now
that you're on the other side?

00:51:08.317 --> 00:51:09.150
Because you made it.

00:51:12.930 --> 00:51:14.725
DOUGLAS RUSHKOFF: I
mean, I was lucky.

00:51:17.960 --> 00:51:21.130
For the first eight or
10 years of my career,

00:51:21.130 --> 00:51:25.760
I made $8,000 to $10,000 a year.

00:51:25.760 --> 00:51:29.640
And I just didn't mind because I
was having fun while the yuppie

00:51:29.640 --> 00:51:32.140
scum were all going to work.

00:51:32.140 --> 00:51:32.850
You know?

00:51:32.850 --> 00:51:36.370
So that was the beauty of
coming out as a slacker.

00:51:36.370 --> 00:51:40.670
Because it was celebrated
that with me and my friends,

00:51:40.670 --> 00:51:42.500
the idea was not
to have to work.

00:51:42.500 --> 00:51:45.000
That's why now when I look at
Obama talking about job, let's

00:51:45.000 --> 00:51:47.625
create jobs for everybody, it's,
like, who wants a fucking job?

00:51:47.625 --> 00:51:48.811
[LAUGHTER]

00:51:48.811 --> 00:51:49.920
I don't want a job.

00:51:49.920 --> 00:51:52.610
I want stuff and
access and all that.

00:51:52.610 --> 00:51:56.400
And I'd like to create things
and have meaningful engagement.

00:51:56.400 --> 00:51:57.614
But I don't want a job.

00:51:57.614 --> 00:51:59.280
A job is an artifact
of the Renaissance.

00:51:59.280 --> 00:52:01.160
People didn't have
jobs until 1300.

00:52:01.160 --> 00:52:03.117
They used to make
stuff and trade.

00:52:03.117 --> 00:52:05.700
They only got jobs when it was
illegal to make stuff yourself.

00:52:05.700 --> 00:52:07.370
You had to go work for
a chartered corporation.

00:52:07.370 --> 00:52:09.670
So when I hear questions
like that, it's, like oh,

00:52:09.670 --> 00:52:12.270
because we're accepting the
premise of this BS Industrial

00:52:12.270 --> 00:52:13.910
Age business model
that was there

00:52:13.910 --> 00:52:18.629
for kings and bourgeoisie,
who have long since left

00:52:18.629 --> 00:52:19.170
the building.

00:52:19.170 --> 00:52:21.480
And we don't even remember
it's their operating system

00:52:21.480 --> 00:52:22.890
that we're keeping going.

00:52:22.890 --> 00:52:25.190
So there's that.

00:52:25.190 --> 00:52:27.950
Lentils and rice is another--

00:52:27.950 --> 00:52:29.650
JASON SILVA: I love
lentils and rice.

00:52:29.650 --> 00:52:30.400
DOUGLAS RUSHKOFF: Yeah,
but you can live--

00:52:30.400 --> 00:52:31.320
JASON SILVA: I still
eat lentils and rice.

00:52:31.320 --> 00:52:33.070
DOUGLAS RUSHKOFF: You
can live a long time

00:52:33.070 --> 00:52:34.580
on lentils, rice, and water.

00:52:34.580 --> 00:52:35.490
You can.

00:52:35.490 --> 00:52:39.160
And fine spices and stuff,
without even being freegan,

00:52:39.160 --> 00:52:40.050
you can survive.

00:52:40.050 --> 00:52:44.480
And I did that for
a good long time.

00:52:44.480 --> 00:52:49.860
And I tutored kids
on their SATs.

00:52:49.860 --> 00:52:52.740
I mean, that taught me a lot
because I was in Beverly Hills.

00:52:52.740 --> 00:52:57.080
I would drive to these Beverly
Hills mansions in my used Ford

00:52:57.080 --> 00:52:57.900
Escort.

00:52:57.900 --> 00:53:01.700
And I'd park between the
gardener and the maid

00:53:01.700 --> 00:53:04.150
and then go tutor
some super-rich kid

00:53:04.150 --> 00:53:06.700
for three hours for $35
an hour or something,

00:53:06.700 --> 00:53:09.680
which is enough to live a
week if you did it frugally.

00:53:09.680 --> 00:53:12.510
So no, it was--

00:53:12.510 --> 00:53:15.560
Maybe those were easier
times than now on some level.

00:53:15.560 --> 00:53:19.440
But I was lucky, and
frankly, the fact

00:53:19.440 --> 00:53:24.510
that I was shoved out the chute
of an Ivy League education

00:53:24.510 --> 00:53:35.990
after Westchester High
School childhood, I mean,

00:53:35.990 --> 00:53:38.510
you've got such an
advantage already.

00:53:38.510 --> 00:53:41.750
You're not being put
out on the ground.

00:53:41.750 --> 00:53:45.860
At that level of wealth, of
an upper middle class parent,

00:53:45.860 --> 00:53:46.870
you're being shot out.

00:53:46.870 --> 00:53:51.230
You've got a long way
to go down before.

00:53:51.230 --> 00:53:53.590
So I'm a child of
privilege, you know.

00:53:53.590 --> 00:53:55.075
I see it that way.

00:53:55.075 --> 00:53:57.450
My fellows at Princeton, who
are all these private school

00:53:57.450 --> 00:54:00.101
rich kids, saw me as the
poor kid is the funny thing.

00:54:00.101 --> 00:54:00.600
You know?

00:54:04.530 --> 00:54:05.160
It was luck.

00:54:05.160 --> 00:54:07.420
And not getting off the bus.

00:54:07.420 --> 00:54:09.260
I think the big
thing is, I've never

00:54:09.260 --> 00:54:11.830
worked for some other thing.

00:54:11.830 --> 00:54:15.170
So everything I did was
investing in my own,

00:54:15.170 --> 00:54:17.820
I hate to use the word, like,
brand, but in my own franchise.

00:54:17.820 --> 00:54:19.790
In me.

00:54:19.790 --> 00:54:21.090
So it builds.

00:54:21.090 --> 00:54:25.944
The freelance lifestyle is more
secure than having a real job.

00:54:25.944 --> 00:54:27.110
Because they can't fire you.

00:54:27.110 --> 00:54:30.234
When you lose this contract,
you've got five more.

00:54:30.234 --> 00:54:32.650
JASON SILVA: Well, it's funny
you say that because there's

00:54:32.650 --> 00:54:35.381
some school of thought that
says that in the future,

00:54:35.381 --> 00:54:37.380
everything is going to
be essentially freelance.

00:54:37.380 --> 00:54:39.546
And people talk about the
rise of the creative class

00:54:39.546 --> 00:54:42.509
and these people
empowered by technologies.

00:54:42.509 --> 00:54:44.550
It was some article I read
that was unbelievable.

00:54:44.550 --> 00:54:47.210
It was saying that there's a
sort of mentality of people

00:54:47.210 --> 00:54:49.510
living in places like
Berlin or Williamsburg,

00:54:49.510 --> 00:54:51.590
Brooklyn, that think
it abominable to have

00:54:51.590 --> 00:54:53.110
to work for somebody else.

00:54:53.110 --> 00:54:54.776
And these are like
these creative people

00:54:54.776 --> 00:54:57.340
that have the education
of the upper class

00:54:57.340 --> 00:54:59.870
but maybe the income of,
like, the lower middle class.

00:54:59.870 --> 00:55:02.860
And so they're like these super
intellectually affluent people

00:55:02.860 --> 00:55:04.530
that don't necessarily
need things.

00:55:04.530 --> 00:55:06.030
So they're not as
dependent on money

00:55:06.030 --> 00:55:07.700
because they don't need
to buy as much stuff

00:55:07.700 --> 00:55:08.908
because they have everything.

00:55:08.908 --> 00:55:10.940
Just their laptop and
their iPhone, that's it.

00:55:10.940 --> 00:55:11.730
So that's really interesting.

00:55:11.730 --> 00:55:12.900
But to answer your question.

00:55:12.900 --> 00:55:17.062
I mean, I think for me, Steven
Johnson has a great line.

00:55:17.062 --> 00:55:18.520
He says, the world
is full of clues

00:55:18.520 --> 00:55:20.400
and you can read
your way through it.

00:55:20.400 --> 00:55:23.100
So I've always kind of
felt that, like, the signs

00:55:23.100 --> 00:55:26.610
are there, connecting the
dots on how I can do my thing

00:55:26.610 --> 00:55:27.780
and make it work.

00:55:27.780 --> 00:55:30.660
I was lucky enough to
grow up in a household

00:55:30.660 --> 00:55:33.280
where my parents, like,
encouraged me to kind of find

00:55:33.280 --> 00:55:36.020
my bliss, just make it happen.

00:55:36.020 --> 00:55:39.520
But also I think there's
little techniques.

00:55:39.520 --> 00:55:41.830
George Carlin, actually,
was interviewed recently

00:55:41.830 --> 00:55:42.785
about his career.

00:55:42.785 --> 00:55:45.460
And he was talking about
psycho cybernetics.

00:55:45.460 --> 00:55:47.606
And he discovered this
book on psycho cybernetics.

00:55:47.606 --> 00:55:48.730
Are you familiar with this?

00:55:48.730 --> 00:55:50.810
And he was saying that
the brain is essentially

00:55:50.810 --> 00:55:52.800
a goal-seeking mechanism.

00:55:52.800 --> 00:55:55.496
And that there's these
devices, these tricks

00:55:55.496 --> 00:55:56.870
you can use to
create impressions

00:55:56.870 --> 00:55:59.100
in your brain that then
teach your brain what

00:55:59.100 --> 00:56:00.933
to look for in the world
to connect the dots

00:56:00.933 --> 00:56:01.890
to achieve your goals.

00:56:01.890 --> 00:56:04.420
So he says the first impression
is, OK, you get an idea

00:56:04.420 --> 00:56:06.270
or you notice something
that inspires you.

00:56:06.270 --> 00:56:08.186
And you're like, oh,
that's what I want to do.

00:56:08.186 --> 00:56:09.439
Or that's what moves me.

00:56:09.439 --> 00:56:10.480
So that's impression one.

00:56:10.480 --> 00:56:12.396
Impression two is
you write it down.

00:56:12.396 --> 00:56:13.270
So you write it down.

00:56:13.270 --> 00:56:14.670
You're already making
a second impression.

00:56:14.670 --> 00:56:16.030
You're acknowledging it,
you're writing it down.

00:56:16.030 --> 00:56:18.330
Impression three is looking
at your notes later.

00:56:18.330 --> 00:56:23.150
And then by that point you've
kind of primed your brain

00:56:23.150 --> 00:56:26.937
with this trajectory of this
network of ideas or concepts

00:56:26.937 --> 00:56:28.520
or whatever things
are turning you on.

00:56:28.520 --> 00:56:30.100
And then what happens
is a few days later

00:56:30.100 --> 00:56:31.430
you're walking down the
street, and all of a sudden you

00:56:31.430 --> 00:56:33.110
start noticing all these
serendipitous signs that

00:56:33.110 --> 00:56:35.235
are connected to what
you were thinking about.

00:56:35.235 --> 00:56:37.110
And people always call
that magical thinking.

00:56:37.110 --> 00:56:38.910
They're like, oh, it's the
secret, I'm attracting it.

00:56:38.910 --> 00:56:39.630
It's not that.

00:56:39.630 --> 00:56:41.046
It's that you've
primed your brain

00:56:41.046 --> 00:56:43.500
to connect the dots using
that meta pattern, right?

00:56:43.500 --> 00:56:45.260
Because the signs
are always there.

00:56:45.260 --> 00:56:46.824
You can read
anything in any way.

00:56:46.824 --> 00:56:48.490
But in a way you're
kind of like hacking

00:56:48.490 --> 00:56:50.829
your brain to pay
attention to the clues that

00:56:50.829 --> 00:56:53.120
are going to help you achieve
what you want to achieve.

00:56:53.120 --> 00:56:55.320
I feel like I didn't know
about psycho cybernetics.

00:56:55.320 --> 00:56:58.430
But it actually echoes a
lot of the things I've done.

00:56:58.430 --> 00:57:00.800
Like, if I look at
my notes from, like,

00:57:00.800 --> 00:57:03.520
three or four years ago, I
have little, these scribblings

00:57:03.520 --> 00:57:08.100
and these lists, describing
exactly what ended up

00:57:08.100 --> 00:57:09.340
happening in my life.

00:57:09.340 --> 00:57:11.576
Like exactly like
where I was going to be

00:57:11.576 --> 00:57:13.450
and what I was going to
do and describing it.

00:57:13.450 --> 00:57:17.347
It's just crazy how then you end
up having those patterns just

00:57:17.347 --> 00:57:18.180
marked in your head.

00:57:18.180 --> 00:57:22.790
And then you kind
of, it's a thought.

00:57:22.790 --> 00:57:24.760
INTERVIEWER 4: Is it on?

00:57:24.760 --> 00:57:25.260
Wow.

00:57:25.260 --> 00:57:25.926
What a question.

00:57:28.730 --> 00:57:33.600
So I've been reading a
lot of history recently.

00:57:33.600 --> 00:57:36.375
And it seems to me that
in the large scale,

00:57:36.375 --> 00:57:39.064
all of human history's
kind of like humanity

00:57:39.064 --> 00:57:41.480
trying to get up on its knees
and reach up for this higher

00:57:41.480 --> 00:57:44.340
ideal, and falling again,
and then reaching up again,

00:57:44.340 --> 00:57:46.640
and then falling again.

00:57:46.640 --> 00:57:50.560
And Kierkegaard said
that the good is slow,

00:57:50.560 --> 00:57:51.950
the evolution of good is slow.

00:57:51.950 --> 00:57:53.360
Because when you
have revolution,

00:57:53.360 --> 00:57:55.680
it destabilizes
what's already there.

00:57:55.680 --> 00:57:57.300
And that just causes
more revolution,

00:57:57.300 --> 00:57:59.850
whereas when the good
is slow, it's evolution.

00:57:59.850 --> 00:58:02.724
It doesn't leave anybody behind.

00:58:02.724 --> 00:58:04.890
And that was just a comment,
that wasn't a question.

00:58:04.890 --> 00:58:08.760
But I recently read
in a Russian magazine

00:58:08.760 --> 00:58:13.920
an article that questioned
several spiritual leaders,

00:58:13.920 --> 00:58:18.700
Christian, Buddhist,
just [INAUDIBLE] people,

00:58:18.700 --> 00:58:22.100
some professors, about their
psychedelic experiences

00:58:22.100 --> 00:58:24.950
and what that had to do
with their spiritual growth.

00:58:24.950 --> 00:58:26.870
And they all
unanimously said that it

00:58:26.870 --> 00:58:29.160
had nothing to do with
their spiritual growth.

00:58:29.160 --> 00:58:31.830
It did cognitively, and it did
with their experiential growth,

00:58:31.830 --> 00:58:34.120
but nothing to do
with spiritual growth.

00:58:34.120 --> 00:58:36.877
And in my personal opinion, I
believe that the whole point

00:58:36.877 --> 00:58:37.710
is spiritual growth.

00:58:40.600 --> 00:58:44.020
Well, their amazing psychedelic
experiences, they said,

00:58:44.020 --> 00:58:46.380
really corresponded
to their state of mind

00:58:46.380 --> 00:58:49.380
and what they were primed
to find in the first place.

00:58:49.380 --> 00:58:52.100
So wouldn't the intention,
whether of a good trip

00:58:52.100 --> 00:58:56.270
or building up this amazing
alternative reality, world

00:58:56.270 --> 00:58:58.650
first, wouldn't the intention
be a lot more important

00:58:58.650 --> 00:59:02.580
than the actual trip
or the experience?

00:59:02.580 --> 00:59:09.180
DOUGLAS RUSHKOFF: I mean, for
me psychedelics seem much closer

00:59:09.180 --> 00:59:12.950
to psychotherapy
than spirituality.

00:59:12.950 --> 00:59:15.150
And part of that
is because there's

00:59:15.150 --> 00:59:18.230
a duality implicit in
taking a drug, right?

00:59:18.230 --> 00:59:20.240
There's before
you take the drug,

00:59:20.240 --> 00:59:22.510
and while you're on
the drug, and after.

00:59:22.510 --> 00:59:24.560
You're getting high.

00:59:24.560 --> 00:59:26.460
So high means, and
then you're low?

00:59:26.460 --> 00:59:28.960
So there's this sort
of duality in it.

00:59:28.960 --> 00:59:32.565
And for me, I got
really into this idea

00:59:32.565 --> 00:59:35.860
of birthing another
human species

00:59:35.860 --> 00:59:37.280
through collective whatever.

00:59:37.280 --> 00:59:40.300
It was sort of when I was back
in the rave psychedelic mode.

00:59:40.300 --> 00:59:43.790
And I thought that the
ultimate spiritual progression

00:59:43.790 --> 00:59:46.160
of our species would be
to achieve meta organism

00:59:46.160 --> 00:59:48.730
and then give birth
to some other thing.

00:59:48.730 --> 00:59:50.950
And then so I had that.

00:59:50.950 --> 00:59:52.965
Then I get married and
have a kid and went,

00:59:52.965 --> 00:59:55.790
oh, that was easy.

00:59:55.790 --> 00:59:57.620
Well, hard, but you
know what I mean.

00:59:57.620 --> 01:00:00.640
But that was like, it's like,
the real tools of evolution

01:00:00.640 --> 01:00:02.900
have been staring me in
the face all the time.

01:00:02.900 --> 01:00:03.980
And they are more subtle.

01:00:03.980 --> 01:00:05.230
They are more slow.

01:00:05.230 --> 01:00:10.250
They're not these giant,
system-wide, revolutionary

01:00:10.250 --> 01:00:10.750
leaps.

01:00:10.750 --> 01:00:11.800
Revolutions are what?

01:00:11.800 --> 01:00:13.260
Revolutions are circles.

01:00:13.260 --> 01:00:14.150
That's what they do.

01:00:14.150 --> 01:00:15.650
I'm into Renaissance,
which is what?

01:00:15.650 --> 01:00:16.200
Rebirth.

01:00:16.200 --> 01:00:18.200
Rebirth of old ideas
in a new context.

01:00:18.200 --> 01:00:20.840
So you get a new technology, a
new media environment, and now

01:00:20.840 --> 01:00:24.377
we can retrieve
things and bring all

01:00:24.377 --> 01:00:26.710
those who didn't come along
with us and go, oh, my gosh,

01:00:26.710 --> 01:00:28.170
those aboriginal
people that we thought

01:00:28.170 --> 01:00:29.280
were all doing nonsense.

01:00:29.280 --> 01:00:30.655
They actually have
the technology

01:00:30.655 --> 01:00:36.385
that we need in order to
get incrementally forward.

01:00:36.385 --> 01:00:37.301
JASON SILVA: I concur.

01:00:37.301 --> 01:00:39.265
[LAUGHTER]

01:00:47.200 --> 01:00:48.120
INTERVIEWER 5: Hi.

01:00:48.120 --> 01:00:49.860
JASON SILVA: Hi.

01:00:49.860 --> 01:00:52.670
INTERVIEWER 5: So
Douglas Rushkoff

01:00:52.670 --> 01:00:54.270
touched on this
before on the fact

01:00:54.270 --> 01:00:58.660
that the agendas
matter, like the agendas

01:00:58.660 --> 01:00:59.610
that we have matter.

01:00:59.610 --> 01:01:05.950
And so I guess what I'm trying
to ask is that, to Jason Silva.

01:01:05.950 --> 01:01:11.140
When you talk about this future
where our technologies advance

01:01:11.140 --> 01:01:13.560
so much, and
singularity, and what's

01:01:13.560 --> 01:01:15.540
going to happen
after that, don't you

01:01:15.540 --> 01:01:19.680
ever feel afraid that things
will advance so quickly

01:01:19.680 --> 01:01:24.150
without necessarily thinking
about what we want to do,

01:01:24.150 --> 01:01:28.210
what our agendas are
when we have this future?

01:01:28.210 --> 01:01:31.230
I feel like, what happens
if these things do end up

01:01:31.230 --> 01:01:32.759
in the wrong hands?

01:01:32.759 --> 01:01:35.050
Or if it's in the right hands,
but we don't necessarily

01:01:35.050 --> 01:01:37.710
know what to do because things
are moving too fast for us?

01:01:37.710 --> 01:01:38.460
JASON SILVA: Sure.

01:01:38.460 --> 01:01:41.810
I mean, there's a great article
in the website for The Edge

01:01:41.810 --> 01:01:45.600
Foundation that was saying
that when the astronauts first

01:01:45.600 --> 01:01:50.600
turned around and took a
picture of the Earth from space,

01:01:50.600 --> 01:01:54.410
that changed the story
of who and what we were.

01:01:54.410 --> 01:01:56.970
That picture provided
a different context

01:01:56.970 --> 01:01:59.420
and a different way of seeing
and understanding ourselves

01:01:59.420 --> 01:02:00.930
and in turn changed the story.

01:02:00.930 --> 01:02:02.555
Changed the context,
changed the story,

01:02:02.555 --> 01:02:05.190
changed reality of what we are.

01:02:05.190 --> 01:02:07.330
And that now in the
face of these emerging

01:02:07.330 --> 01:02:11.250
technologies, that we
are due for a new story.

01:02:11.250 --> 01:02:12.470
And I'm not a scientist.

01:02:12.470 --> 01:02:15.340
I tell people, I was hearing
this filmmaker once saying,

01:02:15.340 --> 01:02:16.870
I'm in the feelings
business, or I'm

01:02:16.870 --> 01:02:18.170
in the imagination business.

01:02:18.170 --> 01:02:20.870
But I feel like my
small contribution

01:02:20.870 --> 01:02:24.090
to that is the exploration
in the media that I make.

01:02:24.090 --> 01:02:26.790
So my short videos
are speculative,

01:02:26.790 --> 01:02:28.020
and they're about technology.

01:02:28.020 --> 01:02:29.270
But they're also
about metaphysics,

01:02:29.270 --> 01:02:30.185
and they're about
the human condition,

01:02:30.185 --> 01:02:32.460
and they're about love,
and they're about loss.

01:02:32.460 --> 01:02:35.770
And they're meant to provoke a
conversation and a discussion,

01:02:35.770 --> 01:02:36.430
you know?

01:02:36.430 --> 01:02:39.869
And I always say, I just
want a seat at the table.

01:02:39.869 --> 01:02:40.910
You know what I'm saying?

01:02:40.910 --> 01:02:42.600
Because I agree with you.

01:02:42.600 --> 01:02:47.430
I think that we need to be
talking about these agendas.

01:02:47.430 --> 01:02:50.890
And I want to be part of
that conversation, for sure.

01:02:50.890 --> 01:02:52.474
That is an ambition
of mine, for sure.

01:02:52.474 --> 01:02:53.764
DOUGLAS RUSHKOFF: That's funny.

01:02:53.764 --> 01:02:56.260
I remember the first few
times I was interviewed

01:02:56.260 --> 01:02:57.629
when I did my first book.

01:02:57.629 --> 01:02:58.420
What are you doing?

01:02:58.420 --> 01:02:59.620
I just want a seat at the table.

01:02:59.620 --> 01:03:00.400
I used to say that all the time.

01:03:00.400 --> 01:03:02.855
I just want to be a part of
that conversation, a real part

01:03:02.855 --> 01:03:03.730
of that conversation.

01:03:03.730 --> 01:03:06.950
But the thing is, then you
get your seat at the table.

01:03:06.950 --> 01:03:09.970
And now, which is sort of where
she's going, it's like, well,

01:03:09.970 --> 01:03:10.470
now what?

01:03:10.470 --> 01:03:12.053
Now you've got your
seat at the table,

01:03:12.053 --> 01:03:14.200
and then you realize,
oh, I'm not a journalist.

01:03:14.200 --> 01:03:15.890
I'm a propagandist.

01:03:15.890 --> 01:03:16.540
Right?

01:03:16.540 --> 01:03:17.550
And that's OK.

01:03:17.550 --> 01:03:18.550
So what am I?

01:03:18.550 --> 01:03:19.270
What am I?

01:03:19.270 --> 01:03:20.050
Because we're all propagandists.

01:03:20.050 --> 01:03:21.924
There's no such thing
as balanced journalism.

01:03:21.924 --> 01:03:24.240
That was such a stupid
idea, that we'd be balanced.

01:03:24.240 --> 01:03:25.360
It's like, where do
you put the fulcrum?

01:03:25.360 --> 01:03:26.280
JASON SILVA: Stop
pretending, exactly.

01:03:26.280 --> 01:03:27.640
DOUGLAS RUSHKOFF: Yeah.

01:03:27.640 --> 01:03:29.340
So instead we're
arguing for something.

01:03:29.340 --> 01:03:30.220
We're fighting for something.

01:03:30.220 --> 01:03:32.095
We're trying to persuade
people of something.

01:03:32.095 --> 01:03:36.380
We're trying to at least get
our mirror neurons activated

01:03:36.380 --> 01:03:38.900
by people agreeing with us.

01:03:38.900 --> 01:03:40.850
But then what do we
want to throw out there,

01:03:40.850 --> 01:03:43.370
is where you go with it.

01:03:43.370 --> 01:03:44.890
And it's funny.

01:03:44.890 --> 01:03:47.930
For me it started, I used to
say, oh, what I'm trying to do

01:03:47.930 --> 01:03:49.342
is get people off
their bad trip.

01:03:49.342 --> 01:03:51.050
I felt like I'd go in
front of audiences,

01:03:51.050 --> 01:03:53.674
and everybody's, oh, technology
and this and, oh, the internet.

01:03:53.674 --> 01:03:54.510
I'm going to change.

01:03:54.510 --> 01:03:55.210
I'm going to have
to learn something.

01:03:55.210 --> 01:03:56.460
JASON SILVA: You
told me that that's

01:03:56.460 --> 01:03:57.820
the phase I'm in right now.

01:03:57.820 --> 01:03:59.580
I'm getting people
off their bad trip.

01:03:59.580 --> 01:04:00.300
DOUGLAS RUSHKOFF: Get
people off the bad trip.

01:04:00.300 --> 01:04:00.380
JASON SILVA: That's my phase.

01:04:00.380 --> 01:04:00.720
DOUGLAS RUSHKOFF: To say no, no.

01:04:00.720 --> 01:04:01.890
It's going to be OK.

01:04:01.890 --> 01:04:03.570
Waking up is not
going to be so hard.

01:04:03.570 --> 01:04:04.520
It's going to be OK.

01:04:04.520 --> 01:04:05.330
Everything is OK.

01:04:05.330 --> 01:04:06.913
Don't worry, don't
worry, don't worry.

01:04:06.913 --> 01:04:08.040
It's all right, have fun.

01:04:08.040 --> 01:04:11.050
And follow them
down their corridor

01:04:11.050 --> 01:04:14.190
so that they really believe
that you understand the terror.

01:04:14.190 --> 01:04:16.620
So that you can take
them out of there.

01:04:16.620 --> 01:04:18.810
But then it's
like, so then what?

01:04:18.810 --> 01:04:19.460
Then what?

01:04:19.460 --> 01:04:22.930
I mean, and you can do that
because the phase of life

01:04:22.930 --> 01:04:27.110
that you're in, you
still have the exuberance

01:04:27.110 --> 01:04:33.026
to make people feel comforted,
you know, and OK and excited.

01:04:33.026 --> 01:04:35.400
But then it's like, OK, so
then when they're on the trip,

01:04:35.400 --> 01:04:37.022
now where?

01:04:37.022 --> 01:04:38.730
JASON SILVA: Who's
steering the Starship?

01:04:38.730 --> 01:04:40.170
DOUGLAS RUSHKOFF: Yeah,
what do we do with that?

01:04:40.170 --> 01:04:40.740
And I tell you--

01:04:40.740 --> 01:04:41.790
JASON SILVA: It's
a work in progress.

01:04:41.790 --> 01:04:43.290
DOUGLAS RUSHKOFF:
Yeah, but the ones

01:04:43.290 --> 01:04:46.010
who have been steering the
Starship, I don't trust them.

01:04:46.010 --> 01:04:48.930
I mean, whether it's Mark
Zuckerberg or Sergey Brin.

01:04:48.930 --> 01:04:50.100
They're at that side.

01:04:50.100 --> 01:04:53.237
Or whether it's Ray Kurzweil
or Kevin Kelly and those guys.

01:04:53.237 --> 01:04:54.320
I don't trust them either.

01:04:54.320 --> 01:04:57.295
I feel like they are
still in the story.

01:04:57.295 --> 01:04:58.170
You know what I mean?

01:04:58.170 --> 01:05:00.300
They're still trying
to figure out a story.

01:05:00.300 --> 01:05:01.800
And what story are
we going to tell.

01:05:01.800 --> 01:05:05.070
And I think it's time,
stories are for children.

01:05:05.070 --> 01:05:09.290
You tell children stories at
night so they go to sleep.

01:05:09.290 --> 01:05:12.230
And you try to dose it with some
programming about the morals

01:05:12.230 --> 01:05:14.270
that you want them to
have when they grow up.

01:05:14.270 --> 01:05:15.850
But I think we're
past stories now.

01:05:15.850 --> 01:05:19.690
We're in something more like
a game, or more an adventure,

01:05:19.690 --> 01:05:22.500
or more an experiential
something like the theater

01:05:22.500 --> 01:05:26.370
experiments you
were talking about.

01:05:26.370 --> 01:05:28.590
It's a different way
of taking people.

01:05:31.410 --> 01:05:32.810
INTERVIEWER 6: Hey, guys.

01:05:32.810 --> 01:05:34.180
This side.

01:05:34.180 --> 01:05:36.330
And kind of related to that.

01:05:36.330 --> 01:05:39.730
In this age where
technology and social media

01:05:39.730 --> 01:05:44.820
are kind of like this limitless
extension of our individual

01:05:44.820 --> 01:05:48.470
being, can it be kind
of counterproductive

01:05:48.470 --> 01:05:54.350
in interpersonal communications
where, especially young people,

01:05:54.350 --> 01:05:56.080
there is no delete key?

01:05:56.080 --> 01:06:00.350
And it's kind of taken them
away from improvisational

01:06:00.350 --> 01:06:02.370
face-to-face conversation,
where they're

01:06:02.370 --> 01:06:04.370
uncomfortable
interacting with people

01:06:04.370 --> 01:06:07.910
where they can't go back and
delete what they've said.

01:06:07.910 --> 01:06:12.850
Where now there's this kind
of insecurity in just talking

01:06:12.850 --> 01:06:13.499
face to face.

01:06:13.499 --> 01:06:15.290
DOUGLAS RUSHKOFF: And
then throw the market

01:06:15.290 --> 01:06:17.934
on top of that
and a value system

01:06:17.934 --> 01:06:20.100
where you actually have a
number of how many friends

01:06:20.100 --> 01:06:21.380
you have as a way of judging.

01:06:21.380 --> 01:06:23.950
You have metrics for your
popularity and success.

01:06:23.950 --> 01:06:25.240
It's tricky.

01:06:25.240 --> 01:06:28.330
94% of human communication
happens nonverbally.

01:06:28.330 --> 01:06:30.880
So what happens when you grow
up in an environment where

01:06:30.880 --> 01:06:33.940
you're using text to engage?

01:06:33.940 --> 01:06:38.490
You end up with a kind of an
induced Asperger syndrome,

01:06:38.490 --> 01:06:40.600
where you don't
make eye contact,

01:06:40.600 --> 01:06:42.090
you're afraid to
make eye contact,

01:06:42.090 --> 01:06:44.460
you don't know how
to establish rapport.

01:06:44.460 --> 01:06:46.170
You don't know how
to not just stand up

01:06:46.170 --> 01:06:48.750
in front of a group of
people, but to engage live

01:06:48.750 --> 01:06:49.700
with one other person.

01:06:49.700 --> 01:06:51.720
You resort to this.

01:06:51.720 --> 01:06:56.780
Or you relate through the selfie
rather than through the self.

01:06:59.484 --> 01:07:01.900
JASON SILVA: I agree with what
you're saying to an extent.

01:07:01.900 --> 01:07:04.120
But I also think that
that is a criticism that

01:07:04.120 --> 01:07:07.906
is biased in favor of
the communication style

01:07:07.906 --> 01:07:10.030
that we're comfortable with
and that we're used to.

01:07:10.030 --> 01:07:13.200
And this assumption that
this new one is not as good,

01:07:13.200 --> 01:07:14.330
et cetera, et cetera.

01:07:14.330 --> 01:07:17.089
But Kurzweil, who is a
friend of mine, he says,

01:07:17.089 --> 01:07:19.380
look, we're not going to be
interacting with each other

01:07:19.380 --> 01:07:21.060
through these
square-shaped devices.

01:07:21.060 --> 01:07:22.810
I mean, these are
temporary growing pains.

01:07:22.810 --> 01:07:24.650
These devices are going
to be shrinking down

01:07:24.650 --> 01:07:25.774
to the size of blood cells.

01:07:25.774 --> 01:07:27.310
They're going to
be in our brains.

01:07:27.310 --> 01:07:31.420
Sending a text message is going
to be very different than going

01:07:31.420 --> 01:07:32.860
down into a screen like that.

01:07:32.860 --> 01:07:35.220
We'll be able to just send
our thoughts to one another.

01:07:35.220 --> 01:07:37.350
Maybe we'll transcend
verbal communication.

01:07:37.350 --> 01:07:39.150
Maybe we'll transcend words.

01:07:39.150 --> 01:07:40.920
We'll be able to
become each other

01:07:40.920 --> 01:07:42.790
and merge our consciousnesses.

01:07:42.790 --> 01:07:45.950
I mean, there are all these
speculative, exotic things

01:07:45.950 --> 01:07:48.450
on the horizon, potentially.

01:07:48.450 --> 01:07:51.000
But I think to criticize
it now as if it

01:07:51.000 --> 01:07:54.670
was the end of the way humans
communicate, I'm like, no.

01:07:54.670 --> 01:07:56.140
It is what it is.

01:07:56.140 --> 01:07:58.450
And if you are not
a fan of it, then

01:07:58.450 --> 01:08:00.610
make sure you practice
being present.

01:08:00.610 --> 01:08:04.160
But I wouldn't label it,
like, the end of the world.

01:08:04.160 --> 01:08:05.580
It just is what it is.

01:08:05.580 --> 01:08:07.996
DOUGLAS RUSHKOFF: I don't think
it's the end of the world.

01:08:07.996 --> 01:08:09.960
But I think it has to
be balanced, is all,

01:08:09.960 --> 01:08:12.057
by some real-world engagement.

01:08:12.057 --> 01:08:14.640
I get concerned when I see kids
in a classroom with a teacher,

01:08:14.640 --> 01:08:16.450
and they're all looking
at their tablets.

01:08:16.450 --> 01:08:17.399
You know, it's like,
no, when you're

01:08:17.399 --> 01:08:18.516
in the classroom with a
teacher, you shouldn't even

01:08:18.516 --> 01:08:19.474
be looking at the book.

01:08:19.474 --> 01:08:21.130
I mean, remember
"Captain, My Captain?"

01:08:21.130 --> 01:08:22.870
They put away the
poetry book, and they

01:08:22.870 --> 01:08:25.439
engaged with each other.

01:08:25.439 --> 01:08:28.660
Our face-to-face
time is so minimal.

01:08:28.660 --> 01:08:32.490
And how are we going to
know what the nanos should

01:08:32.490 --> 01:08:39.260
do if we aren't experiencing the
full breadth of what we already

01:08:39.260 --> 01:08:40.575
have?

01:08:40.575 --> 01:08:41.950
JASON SILVA: I
think we have time

01:08:41.950 --> 01:08:43.890
for one or two more questions.

01:08:43.890 --> 01:08:47.399
And then we have some vodka
for you guys if you're over 21.

01:08:49.938 --> 01:08:51.979
Thank you to Russian
Standard Vodka for providing

01:08:51.979 --> 01:08:55.029
the free vodka bar after this.

01:08:55.029 --> 01:08:57.130
INTERVIEWER 7: OK,
so you guys obviously

01:08:57.130 --> 01:08:59.279
have a lot of views on
psychedelics and stuff.

01:08:59.279 --> 01:09:03.050
But what do you think
the most legitimate

01:09:03.050 --> 01:09:07.979
and, I guess, the most efficient
way to reintroduce psychedelics

01:09:07.979 --> 01:09:09.700
as a legitimate tool?

01:09:09.700 --> 01:09:11.200
JASON SILVA: It's
already happening.

01:09:11.200 --> 01:09:13.950
I mean, the majority
of Americans

01:09:13.950 --> 01:09:17.140
now in the latest polls support
full marijuana legalization.

01:09:17.140 --> 01:09:18.560
The majority of Americans.

01:09:18.560 --> 01:09:20.370
So that's not even
taboo anymore.

01:09:20.370 --> 01:09:23.859
And as that becomes more
mainstream and more legalized,

01:09:23.859 --> 01:09:27.189
you're going to start to see the
emergence of a class of citizen

01:09:27.189 --> 01:09:30.609
that is not the stereotypical
stoner with the bong hit

01:09:30.609 --> 01:09:31.597
playing video games.

01:09:31.597 --> 01:09:33.680
But what's going to happen
when these, oh, my god,

01:09:33.680 --> 01:09:35.514
all these screenwriters
like to get high?

01:09:35.514 --> 01:09:37.430
All these brilliant
thinkers like to get high?

01:09:37.430 --> 01:09:37.930
Wow.

01:09:37.930 --> 01:09:41.710
Oh, not everybody uses it in
a destructive, addicting way.

01:09:41.710 --> 01:09:44.720
Oh, so this is a tool
for your creativity?

01:09:44.720 --> 01:09:46.920
Oh, that's interesting.

01:09:46.920 --> 01:09:50.660
A recent study that looked at
semantic priming in marijuana

01:09:50.660 --> 01:09:52.750
found that marijuana
actually induces

01:09:52.750 --> 01:09:54.790
a state of hyperpriming,
which means it

01:09:54.790 --> 01:09:56.330
expands your associative net.

01:09:56.330 --> 01:09:57.800
It expands your
capacity to connect

01:09:57.800 --> 01:10:00.610
the dots between seemingly
disparate ideas, a sign

01:10:00.610 --> 01:10:02.180
of creativity right there.

01:10:02.180 --> 01:10:03.840
Or at Johns Hopkins
University, where

01:10:03.840 --> 01:10:06.410
they're doing psilocybin
mushroom studies with people

01:10:06.410 --> 01:10:08.240
who have terminal
illnesses and making

01:10:08.240 --> 01:10:12.474
them process their own mortality
in a radically different way.

01:10:12.474 --> 01:10:13.640
I think even the government.

01:10:13.640 --> 01:10:15.970
I mean, there is going to be
a psychedelic renaissance.

01:10:15.970 --> 01:10:18.690
And I think we're already
seeing it a little bit, right?

01:10:18.690 --> 01:10:20.150
DOUGLAS RUSHKOFF: Yeah.

01:10:20.150 --> 01:10:21.750
[LAUGHTER]

01:10:21.750 --> 01:10:26.950
I'm certainly more
interested in that

01:10:26.950 --> 01:10:29.990
than watching Big
Pharma neutralize

01:10:29.990 --> 01:10:32.720
the inside elements
of these drugs

01:10:32.720 --> 01:10:39.050
and turn them into coping
neurotransmitter modulators.

01:10:39.050 --> 01:10:44.760
Which is where they went
with it in 30 years.

01:10:44.760 --> 01:10:50.460
And luckily, I mean, the
plants are still there.

01:10:50.460 --> 01:10:53.230
They're still, well, it's
not illegal yet, I guess.

01:10:53.230 --> 01:10:54.600
A lot of seeds are illegal.

01:10:54.600 --> 01:10:57.020
So the whole thing
with seeds and Monsanto

01:10:57.020 --> 01:10:59.750
and copyrighting
your genes and stuff.

01:10:59.750 --> 01:11:01.800
That gets really tricky, too.

01:11:01.800 --> 01:11:05.970
Because if we take that,
it's like right now it

01:11:05.970 --> 01:11:08.190
feels to me like
the technosphere

01:11:08.190 --> 01:11:11.910
and the marketplace
are equivalent.

01:11:11.910 --> 01:11:16.230
And if we can somehow
parse them from each other,

01:11:16.230 --> 01:11:18.390
I'm going to like the
technosphere a lot more.

01:11:18.390 --> 01:11:20.556
Because then the agenda,
then it's going to be like,

01:11:20.556 --> 01:11:21.590
what's technology for?

01:11:21.590 --> 01:11:22.540
To make us more human.

01:11:22.540 --> 01:11:24.706
It's to realize our
full potential, rather

01:11:24.706 --> 01:11:25.830
than what's technology for?

01:11:25.830 --> 01:11:26.610
Well, it's to enhance the--

01:11:26.610 --> 01:11:27.580
JASON SILVA: What we can sell.

01:11:27.580 --> 01:11:27.690
DOUGLAS RUSHKOFF: Yeah.

01:11:27.690 --> 01:11:29.160
What we can sell.

01:11:29.160 --> 01:11:30.879
Or how we can extract data.

01:11:30.879 --> 01:11:31.920
JASON SILVA: Fair enough.

01:11:31.920 --> 01:11:33.302
OK, one more question.

01:11:33.302 --> 01:11:34.510
INTERVIEWER 8: OK, that's me.

01:11:34.510 --> 01:11:37.854
I'm the lucky lady right here.

01:11:37.854 --> 01:11:39.520
Thank you so much for
this conversation.

01:11:39.520 --> 01:11:41.380
Incredibly invigorating.

01:11:41.380 --> 01:11:45.030
Jason, you are like an
electrifying optimist.

01:11:45.030 --> 01:11:48.390
And Douglas, you're a very
reassuring practical person

01:11:48.390 --> 01:11:49.200
for me.

01:11:49.200 --> 01:11:53.310
"Shots of Awe" is like a
lightning bolt of inspiration.

01:11:53.310 --> 01:11:54.990
And I got super high on it.

01:11:54.990 --> 01:11:57.480
And then I was like, crash.

01:11:57.480 --> 01:11:59.960
Wait, there are some things
he's just not talking about,

01:11:59.960 --> 01:12:02.700
and Douglas brings them
into the conversation.

01:12:02.700 --> 01:12:06.690
So what would be incredibly
awesome for me in my world

01:12:06.690 --> 01:12:10.460
is like a super organism the
way that a flock of swallows

01:12:10.460 --> 01:12:11.600
flies through the sky.

01:12:11.600 --> 01:12:14.110
Like if humanity could
take that metaphor

01:12:14.110 --> 01:12:16.010
and, like you say,
use technology

01:12:16.010 --> 01:12:19.060
to become more human and more
telepathic and more in touch

01:12:19.060 --> 01:12:23.200
with each other, that we're
like aesthetically, peacefully

01:12:23.200 --> 01:12:24.720
in harmony through the sky.

01:12:24.720 --> 01:12:25.900
That's what I want.

01:12:25.900 --> 01:12:28.280
Now what I see as
the barrier there

01:12:28.280 --> 01:12:31.010
is the digital
divide and education.

01:12:31.010 --> 01:12:34.120
For me, education is that,
like, exponential wormhole

01:12:34.120 --> 01:12:37.222
that could take us to this
utopic future that you talk of.

01:12:37.222 --> 01:12:39.180
And I would love to hear
what your thoughts are

01:12:39.180 --> 01:12:44.390
from a systemic perspective on
how we can improve and enhance

01:12:44.390 --> 01:12:45.770
the system of education.

01:12:45.770 --> 01:12:47.930
Because I've worked in
education for a long time.

01:12:47.930 --> 01:12:50.310
And I'll tell you the
statistics, the facts.

01:12:50.310 --> 01:12:52.660
In New York City,
40% of our kids

01:12:52.660 --> 01:12:54.520
are not graduating
in four years.

01:12:54.520 --> 01:12:57.080
That's more than
500,000 students right

01:12:57.080 --> 01:12:58.750
now, snapshot,
that are not going

01:12:58.750 --> 01:13:00.170
to graduate in four years.

01:13:00.170 --> 01:13:02.430
That's the size of
the city of Atlanta.

01:13:02.430 --> 01:13:05.680
So it's like a whole city
subsidy within our city

01:13:05.680 --> 01:13:08.000
that are not going
to be literate, let

01:13:08.000 --> 01:13:10.680
alone digital literacy, which is
all these amazing things you're

01:13:10.680 --> 01:13:12.740
talking about, which
I'm so excited about.

01:13:12.740 --> 01:13:14.770
But I'm at a loss.

01:13:14.770 --> 01:13:17.262
And so I'd love to hear the
conversation and debate,

01:13:17.262 --> 01:13:18.720
like, what's going
to get us there?

01:13:18.720 --> 01:13:20.511
Because I believe we're
going to get there.

01:13:20.511 --> 01:13:22.670
But I just need a little
bit more to hold onto.

01:13:22.670 --> 01:13:23.420
JASON SILVA: Yeah.

01:13:23.420 --> 01:13:27.197
Well, I mean, just briefly,
I agree with the problems

01:13:27.197 --> 01:13:28.280
that you're talking about.

01:13:28.280 --> 01:13:31.020
But I think education is
due for a massive upgrade.

01:13:31.020 --> 01:13:33.970
I think the whole idea of how
we teach, and how we learn,

01:13:33.970 --> 01:13:36.280
and whether it's
doing more fMRI scans,

01:13:36.280 --> 01:13:39.450
and introducing
crazy psychotropics

01:13:39.450 --> 01:13:40.810
into the classroom.

01:13:40.810 --> 01:13:41.990
Maybe after you're 18.

01:13:41.990 --> 01:13:43.690
No, but I'm just saying,
like, I think we need to--

01:13:43.690 --> 01:13:44.450
DOUGLAS RUSHKOFF: You
just lost the job as--

01:13:44.450 --> 01:13:45.380
JASON SILVA: --completely--

01:13:45.380 --> 01:13:47.890
DOUGLAS RUSHKOFF: --chancellor
of the schools in New York.

01:13:47.890 --> 01:13:49.700
JASON SILVA: I
just think that we

01:13:49.700 --> 01:13:53.960
need to not have an educational
system that is still of, like,

01:13:53.960 --> 01:13:55.990
the Industrial Revolution Age.

01:13:55.990 --> 01:13:57.650
I mean, I went to a
Montessori school,

01:13:57.650 --> 01:13:59.620
which has a kind of different
philosophical agenda.

01:13:59.620 --> 01:14:01.620
But at the same time, I
think these technologies

01:14:01.620 --> 01:14:04.320
and these tools are giving
us radical new opportunities.

01:14:04.320 --> 01:14:06.900
I think there was a Bill Clinton
article in "Time" magazine

01:14:06.900 --> 01:14:08.441
called "The Case
for Optimism," where

01:14:08.441 --> 01:14:11.474
he says, forget about the
haves and the have-nots

01:14:11.474 --> 01:14:13.390
because the cell phone
was one of the greatest

01:14:13.390 --> 01:14:15.556
inventions in history to
pull people out of poverty,

01:14:15.556 --> 01:14:16.184
for example.

01:14:16.184 --> 01:14:17.600
And the idea of
the digital divide

01:14:17.600 --> 01:14:19.992
is actually not
what we thought it

01:14:19.992 --> 01:14:21.700
was going to be because
people are wired,

01:14:21.700 --> 01:14:23.075
people are going
on the internet.

01:14:23.075 --> 01:14:25.590
Whether or not they're using
that to watch LOL cat videos

01:14:25.590 --> 01:14:29.150
or actually learn stuff
online, that's something else.

01:14:29.150 --> 01:14:30.716
I'm a big fan of
Sir Ken Robinson

01:14:30.716 --> 01:14:32.340
and a lot of his
ideas about education.

01:14:32.340 --> 01:14:35.390
And kind of like, not try to
turn everybody into a robot

01:14:35.390 --> 01:14:36.450
and so on and so forth.

01:14:36.450 --> 01:14:40.060
But my mom also taught for
five years at the South Bronx.

01:14:40.060 --> 01:14:42.540
So I'm also privy
to a broken system

01:14:42.540 --> 01:14:44.940
and understand how
that works as well.

01:14:44.940 --> 01:14:48.160
But I just think, throw
away what's not working.

01:14:48.160 --> 01:14:50.430
Be willing to upgrade
the whole thing.

01:14:50.430 --> 01:14:53.539
Put everything into
question if we have to.

01:14:53.539 --> 01:14:54.580
Start from the ground up.

01:14:54.580 --> 01:14:56.371
DOUGLAS RUSHKOFF: I
mean, yeah, it's funny.

01:14:56.371 --> 01:14:58.510
This question brings up
everything sort of I've

01:14:58.510 --> 01:14:59.720
been thinking about tonight.

01:14:59.720 --> 01:15:03.120
I mean, on the one
hand, the thing

01:15:03.120 --> 01:15:09.760
I did after my decade of awe
was look and basically said,

01:15:09.760 --> 01:15:15.720
I'm going to go to the darkest
places of mind and of reality

01:15:15.720 --> 01:15:17.280
and test my awe.

01:15:17.280 --> 01:15:20.420
And I'm going to go as deep
as I can, without losing it,

01:15:20.420 --> 01:15:23.890
and then pull back and figure
out how to work with it.

01:15:23.890 --> 01:15:25.740
So the first thing, I
went to South Africa

01:15:25.740 --> 01:15:28.370
before the elections and go
around Soweto and see basically

01:15:28.370 --> 01:15:30.470
test myself in the
darkest possible places

01:15:30.470 --> 01:15:33.360
to then look, what do we do?

01:15:33.360 --> 01:15:37.260
As a way of challenging
my awe and earning

01:15:37.260 --> 01:15:39.820
my awe by going dark.

01:15:39.820 --> 01:15:41.930
So when I go dark,
I'm not a pessimist.

01:15:41.930 --> 01:15:46.520
But rather than just talking
people down off their bad trips

01:15:46.520 --> 01:15:49.580
now from my happy young place,
what I do is I go into the,

01:15:49.580 --> 01:15:51.280
the only good trip
is a bad trip.

01:15:51.280 --> 01:15:54.260
I go into the bad trip so that
I can then find my way back.

01:15:54.260 --> 01:15:56.580
I get as lost as I can in
the despair of, oh, my god,

01:15:56.580 --> 01:15:58.980
and money and blah, blah,
blah, and then come back.

01:15:58.980 --> 01:16:01.210
Education is definitely
one of those places.

01:16:01.210 --> 01:16:03.490
And education is the place
where we can say, OK,

01:16:03.490 --> 01:16:08.290
we can use digital technology to
build on our education system.

01:16:08.290 --> 01:16:09.502
And make kids more job ready.

01:16:09.502 --> 01:16:11.710
And teach them code, and do
this, and give them that,

01:16:11.710 --> 01:16:13.180
and so they can get into IBM.

01:16:13.180 --> 01:16:17.272
In other words, and that would
be the constructivist approach,

01:16:17.272 --> 01:16:18.230
the dialectic approach.

01:16:18.230 --> 01:16:19.730
We're going to build
on top of that.

01:16:19.730 --> 01:16:22.390
Or we can say,
digital technology

01:16:22.390 --> 01:16:26.477
and the big digital changes and
opportunity to go back and say,

01:16:26.477 --> 01:16:28.060
and this is what the
sophist would do.

01:16:28.060 --> 01:16:36.060
This is what the grammarian
would do in this situation, is

01:16:36.060 --> 01:16:39.870
go, what's education for?

01:16:39.870 --> 01:16:41.430
What is it for?

01:16:41.430 --> 01:16:45.390
Right now what it's for, if you
look at Ellwood Cubberley, who

01:16:45.390 --> 01:16:48.900
created the modern American,
New York education system.

01:16:48.900 --> 01:16:53.110
It was to create, we all know,
workers for American factories.

01:16:53.110 --> 01:16:55.250
So that's why they have
bells after every class.

01:16:55.250 --> 01:16:56.360
This is why your
school is like that.

01:16:56.360 --> 01:16:57.720
They're trying to make
you a good factory person.

01:16:57.720 --> 01:16:59.210
The teacher is the foreman.

01:16:59.210 --> 01:17:00.719
And you students
are the workers.

01:17:00.719 --> 01:17:01.510
And the bell rings.

01:17:01.510 --> 01:17:02.650
You go to the next place.

01:17:02.650 --> 01:17:03.890
You learn, you memorize this.

01:17:03.890 --> 01:17:04.390
You do that.

01:17:04.390 --> 01:17:05.450
Follow my orders and go.

01:17:05.450 --> 01:17:06.450
Do you have a hall pass?

01:17:06.450 --> 01:17:07.811
No, good.

01:17:07.811 --> 01:17:08.685
That's what it's for.

01:17:08.685 --> 01:17:10.300
It was constructed to do that.

01:17:10.300 --> 01:17:13.110
And now, we're like, well,
what is education for?

01:17:13.110 --> 01:17:13.945
And it depends.

01:17:13.945 --> 01:17:16.320
It depends on what we think
the future of the economy is.

01:17:16.320 --> 01:17:18.120
I mean, if education
is to create

01:17:18.120 --> 01:17:22.640
a new peer-to-peer
society of creative people

01:17:22.640 --> 01:17:25.130
who are engaging, if we
accept the fact that we only

01:17:25.130 --> 01:17:26.750
need people working
10% of the time

01:17:26.750 --> 01:17:29.416
in order to give everyone enough
food and shelter and stuff they

01:17:29.416 --> 01:17:32.976
need, what are we doing with
the other 90% of human activity?

01:17:32.976 --> 01:17:35.100
So then what you have to
do is go back and you say,

01:17:35.100 --> 01:17:38.160
oh, education is to imbue
people with the ability

01:17:38.160 --> 01:17:39.160
to do critical thinking.

01:17:41.670 --> 01:17:43.980
The need for the
liberal arts education

01:17:43.980 --> 01:17:47.780
is actually more important
now than it was before,

01:17:47.780 --> 01:17:49.410
because now we need
a generation who's

01:17:49.410 --> 01:17:51.079
going to ask the big questions.

01:17:51.079 --> 01:17:52.620
If anything, the
purpose of education

01:17:52.620 --> 01:17:54.780
now is to create the people who
can develop the next education

01:17:54.780 --> 01:17:55.310
system.

01:17:55.310 --> 01:17:56.785
JASON SILVA: There you go.

01:17:56.785 --> 01:17:57.410
It's very meta.

01:17:57.410 --> 01:17:59.201
DOUGLAS RUSHKOFF: Go
meta on it, but, yeah.

01:17:59.201 --> 01:17:59.970
JASON SILVA: Cool.

01:17:59.970 --> 01:18:01.137
Well, thank you, guys.

01:18:01.137 --> 01:18:01.970
Join us for a drink.

01:18:01.970 --> 01:18:03.386
DOUGLAS RUSHKOFF:
Thank you, yeah.

01:18:03.386 --> 01:18:05.620
[APPLAUSE]

