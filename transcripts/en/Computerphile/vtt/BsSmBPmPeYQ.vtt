WEBVTT
Kind: captions
Language: en-GB

00:00:00.000 --> 00:00:01.340
Yeah, we've been late to the party

00:00:01.340 --> 00:00:03.500
Google Deep Dream has been out for a while, but...

00:00:03.500 --> 00:00:05.600
it follows so nicely from our neural network talks.

00:00:05.600 --> 00:00:07.180
Let's talk about how it works.

00:00:07.300 --> 00:00:10.120
Google Deep Dream is a strange computer program

00:00:10.280 --> 00:00:14.100
that outputs...
kinda psychodelic, trippy images.

00:00:14.220 --> 00:00:17.900
This is one of the Google's gallery,
and it's some kind of...

00:00:17.980 --> 00:00:19.680
strange... I mean...
what is it? We don't know...

00:00:19.980 --> 00:00:20.980
Kinda strange...

00:00:21.360 --> 00:00:22.580
There's sort of a viaduct here,

00:00:22.580 --> 00:00:25.640
and that looks like a fountain...
and some grass...

00:00:25.640 --> 00:00:29.840
Weird, sort of artistic image, 
but generated entirely by computer.

00:00:29.840 --> 00:00:31.160
Now, at the time this came out,

00:00:31.260 --> 00:00:33.020
most people where having lots of fun
playing with this,

00:00:33.140 --> 00:00:34.860
and playing with online generators...

00:00:34.880 --> 00:00:37.640
But no one, you know, really
talked about, deep down, how it worked.

00:00:40.440 --> 00:00:42.700
Kinda looks like sort of digital Salvador DalÃ­, doesn't it?

00:00:42.820 --> 00:00:43.680
It does a bit, yeah.

00:00:43.680 --> 00:00:45.680
And I think we quite instinctively
quite like the idea

00:00:45.680 --> 00:00:47.680
of computers can do art,
in some way...

00:00:47.680 --> 00:00:49.220
For what it's worth that's not
quite art yet

00:00:49.400 --> 00:00:50.940
I don't think, but we...
we... you know,

00:00:50.940 --> 00:00:51.440
it's a bit of fun.

00:00:51.440 --> 00:00:53.340
It also has an insteresting
implications as to

00:00:53.340 --> 00:00:55.120
what a neural network is doing underneath.

00:00:55.640 --> 00:00:57.460
But we talked about, a bit,

00:00:57.560 --> 00:00:59.720
doing the video where we
looked into a network,

00:00:59.720 --> 00:01:00.940
and so it classified digits.

00:01:01.220 --> 00:01:03.220
This is a similar kind of thing
where you can see

00:01:03.420 --> 00:01:05.620
that the lower level layers
are doing some things

00:01:05.860 --> 00:01:07.380
and the higher level layers are
doing other things.

00:01:07.860 --> 00:01:10.160
So let's try and break down
what it's doing

00:01:10.160 --> 00:01:12.160
and then we can see
some funny images

00:01:12.160 --> 00:01:13.340
I've run on myself.

00:01:13.340 --> 00:01:14.700
Now, Google's "GoogLeNet"

00:01:14.700 --> 00:01:16.700
(which is the name for Google's networks

00:01:16.700 --> 00:01:17.780
that they released,
I think in... 2012,

00:01:17.920 --> 00:01:20.120
as part of the ImageNet competition)

00:01:20.320 --> 00:01:21.740
is quite complicated, right?

00:01:21.940 --> 00:01:24.260
They have these modules of
groups of convolutional layers

00:01:24.400 --> 00:01:25.780
called "Inception modules"

00:01:25.780 --> 00:01:27.220
which is a very cool name

00:01:27.320 --> 00:01:28.280
for something which is...

00:01:28.640 --> 00:01:30.420
probably not quite as cool as that

00:01:30.600 --> 00:01:31.540
but... it's a cool name.

00:01:31.700 --> 00:01:33.540
The idea is you go
deeper and deeper into

00:01:33.620 --> 00:01:35.500
the network and you get
more and more powerful

00:01:35.700 --> 00:01:37.080
in classification out
of it, ok?

00:01:37.420 --> 00:01:39.920
But, at its core,
it's still a classification network.

00:01:39.920 --> 00:01:41.920
So it's saying:
"What is this a picture of?"

00:01:42.200 --> 00:01:43.500
"It's a picture of a cat."
Right?

00:01:43.500 --> 00:01:45.500
"A hundred percent confident:
picture of a cat, definitely..."

00:01:45.500 --> 00:01:48.160
"Oh, no, it could be a dog."
Right? So...

00:01:49.240 --> 00:01:50.660
What I'm do is:
I'm gonna draw a network

00:01:50.660 --> 00:01:52.660
But I'm gonna draw my
sort of standard...

00:01:52.660 --> 00:01:53.540
multi-layer network

00:01:53.680 --> 00:01:54.920
(but it's got nothing
to do with convolutions

00:01:55.080 --> 00:01:57.040
and nothing to do with
GoogLeNet)

00:01:57.040 --> 00:01:58.600
because it's easier to visualize.

00:01:58.780 --> 00:02:01.280
So I'm over-simplifying it,
but on the other hand...

00:02:02.040 --> 00:02:04.600
...the same things apply to this
small network I'm going to draw,

00:02:04.840 --> 00:02:05.700
as to the big network.

00:02:05.700 --> 00:02:08.480
So, remember that we have
some input neurons here

00:02:08.480 --> 00:02:10.480
and then we have some intermediate neurons

00:02:10.760 --> 00:02:12.760
and then, finally, our output neurons.

00:02:12.760 --> 00:02:15.420
Now, if people've been back to
the videos we did on this,

00:02:15.760 --> 00:02:19.680
this neuron here calculates
a weighted sum of all these neurons,

00:02:19.980 --> 00:02:22.120
and this one calculates
a weighted sum of all these neurons.

00:02:22.360 --> 00:02:24.360
So, if we were going back to our house analogy, all right?

00:02:24.360 --> 00:02:27.400
This one here could be number of windows,

00:02:27.400 --> 00:02:29.400
this could be square footage,

00:02:29.400 --> 00:02:31.280
this could be if it's got a pool or not, right?

00:02:31.280 --> 00:02:33.460
And this is taking some combinations of those things

00:02:33.580 --> 00:02:35.580
and trying to start to work towards
the price of the house.

00:02:36.080 --> 00:02:38.080
And this one takes some combination of those combinations

00:02:38.080 --> 00:02:40.080
and starts working towards the price of the house.

00:02:40.080 --> 00:02:42.940
Now, when we talked about
convolutional networks

00:02:42.940 --> 00:02:45.440
these neurons where replaced with
image convolutions,

00:02:45.440 --> 00:02:47.880
like Sobel edge detedections,
and other things, right?

00:02:47.980 --> 00:02:50.440
Where the actual convolutions them selves have learned.

00:02:50.440 --> 00:02:54.300
So, the early layers are going to be finding
lines and corners and things like these.

00:02:54.580 --> 00:02:58.080
Later on we are gonna start to find
objects, boxes, circles,

00:02:58.080 --> 00:03:00.980
things that have multiple lines
and corners as part of them.

00:03:01.120 --> 00:03:03.720
And finally, as a top, we start to move
towards actual objects

00:03:03.720 --> 00:03:05.000
we are trying to classify.

00:03:05.000 --> 00:03:06.900
Cats and dogs and bikes.

00:03:06.900 --> 00:03:10.500
And then finally we get an ouptut
that lights up if it's a cat, right?

00:03:10.660 --> 00:03:11.500
That's the key.

00:03:11.700 --> 00:03:13.460
I mentioned this very briefly in a video,

00:03:13.740 --> 00:03:15.320
and I'm gonna mention it here again,
very briefly

00:03:15.320 --> 00:03:18.040
because backpropagation is not
for a computerphile video,

00:03:18.160 --> 00:03:21.260
there's a lot of detailed analysis of
backpropagation online

00:03:21.380 --> 00:03:23.000
for people who is interested in it, right?

00:03:23.020 --> 00:03:24.480
It's fairly mathematically complex,

00:03:24.480 --> 00:03:26.140
It talks a lot about partial derivatives,

00:03:26.140 --> 00:03:28.420
and multivariable calculus and things like this.

00:03:28.580 --> 00:03:30.040
We won't be doing any of that in this video,

00:03:30.040 --> 00:03:31.520
so please don't turn off..., right?

00:03:31.720 --> 00:03:32.800
But the only theory is that

00:03:32.800 --> 00:03:34.940
if we put an image in it at this point

00:03:34.940 --> 00:03:36.940
we can calculate these weighted sums

00:03:36.940 --> 00:03:38.940
and we can propagate it through
and get a value out that says

00:03:39.120 --> 00:03:41.220
how much of a cat is in this image, right?

00:03:41.380 --> 00:03:43.380
It's essentially what we are doing.

00:03:43.640 --> 00:03:46.100
When we actually want to train this network to do something.

00:03:46.460 --> 00:03:47.280
What we do is

00:03:47.460 --> 00:03:48.740
we know we are looking for a cat

00:03:48.740 --> 00:03:50.320
so we try and change these weights

00:03:50.320 --> 00:03:51.460
to better predict it.

00:03:51.460 --> 00:03:54.160
So we have something called
a "cost function" here, C,

00:03:54.260 --> 00:03:55.340
and what we're trying to do,

00:03:55.480 --> 00:03:57.800
is we're trying to work out
how we affect C

00:03:58.660 --> 00:04:00.520
by changing this particular weight here...

00:04:00.520 --> 00:04:03.480
So, some relationship between
the weights and the cost function.

00:04:03.480 --> 00:04:04.260
Now...

00:04:04.980 --> 00:04:06.740
when we train a neural network what we do

00:04:06.740 --> 00:04:08.360
is we try and minimize this cost function.

00:04:08.520 --> 00:04:10.480
So the cost function might be something
like prediction accuracy,

00:04:10.520 --> 00:04:12.080
or euclidean distance, or

00:04:12.160 --> 00:04:13.760
some sort of softmax, right?

00:04:14.220 --> 00:04:17.780
But the point is that this gives us a value 
of how good our guess is,

00:04:18.120 --> 00:04:20.280
and then we alter all or weights going backwards

00:04:20.800 --> 00:04:22.440
to say, let's change these weights a bit

00:04:22.440 --> 00:04:24.020
so that that error goes down

00:04:24.020 --> 00:04:26.040
and we get a little bit closer to do our prediction,

00:04:26.040 --> 00:04:26.780
right?

00:04:27.520 --> 00:04:29.940
So we go forward to get our prediction,

00:04:30.140 --> 00:04:31.460
we calculate the error

00:04:31.460 --> 00:04:32.880
and we propagate the error backwards.

00:04:33.020 --> 00:04:34.540
So, that's all the background you're gonna need

00:04:34.560 --> 00:04:35.820
to know how Google Deep Dream works.

00:04:35.820 --> 00:04:37.720
What Google Deep Dream does is...

00:04:37.720 --> 00:04:39.140
forget the cost function completely.

00:04:39.140 --> 00:04:41.140
We've already trained the network

00:04:41.140 --> 00:04:42.360
What we want to do

00:04:42.360 --> 00:04:43.960
is maximize

00:04:44.160 --> 00:04:45.640
these values here,

00:04:45.640 --> 00:04:47.640
or these values here. So...

00:04:47.760 --> 00:04:49.480
Think about it...
if this is a picture of a cat, right?

00:04:49.480 --> 00:04:51.480
So, I'm putting in a picture of a cat here.

00:04:51.480 --> 00:04:53.980
Then, what's gonna happen is it's gonna
crossweight the sum,

00:04:53.980 --> 00:04:55.980
and then, one of the cat neurons is gonna light up.
Right?

00:04:56.900 --> 00:04:58.600
But also, if you think about this layer,

00:04:58.600 --> 00:04:59.860
if we are working on it backwards,

00:04:59.860 --> 00:05:01.740
it might be because this one lit up,

00:05:01.880 --> 00:05:03.920
which is maybe "there's ears",

00:05:03.920 --> 00:05:05.060
and this one lit up, which is maybe "paws".

00:05:05.060 --> 00:05:07.140
And maybe this one lit up because here

00:05:07.320 --> 00:05:08.640
was "a few lines in a row",

00:05:08.640 --> 00:05:11.420
and this one is sort of "fury texture",
or something, you know.

00:05:11.460 --> 00:05:13.580
And we're getting lower and lower levels,
we go through,

00:05:13.580 --> 00:05:16.100
and in the end it's because this one lit up,

00:05:16.100 --> 00:05:17.020
which is "edges"

00:05:17.020 --> 00:05:19.020
and this one lit up which is "corners in a certain place".

00:05:19.580 --> 00:05:22.200
The values in here influence the values
 here, and here, and here, and here...

00:05:22.200 --> 00:05:24.200
and then end up converging on our thing.

00:05:24.200 --> 00:05:27.300
So, what we want to do to make
our Google Deep Dream images

00:05:27.300 --> 00:05:31.640
is change the image to make these bigger, right?

00:05:31.640 --> 00:05:35.020
So this is the amount of "ear" in our image.

00:05:35.020 --> 00:05:37.960
If we can just make that as big as possible,
we can say "more ears please!"...

00:05:38.100 --> 00:05:40.040
- "There's a bit of ears going on there..."
"Well I want more."

00:05:40.040 --> 00:05:43.100
"I want more ears, I want more paws, 
I want more bits of cat."

00:05:43.100 --> 00:05:44.960
So instead of minimizing this cost function,

00:05:45.060 --> 00:05:47.820
we're maximizing the sum of these,
or the squared sum of these.

00:05:47.820 --> 00:05:49.820
Let's not do anymore maths, right?

00:05:49.820 --> 00:05:50.980
Let's look at some pictures.

00:05:50.980 --> 00:05:52.980
I have my landscape image.

00:05:53.400 --> 00:05:54.680
Now, if it's looking very boring to you,

00:05:54.680 --> 00:05:56.820
it's because I havn't passed this through
Google Deep Dream yet, right?

00:05:56.820 --> 00:06:00.100
But what I'll do is:
I'll pass this input into the Google Deep Dream

00:06:00.100 --> 00:06:03.260
and for every area in the image
it starts to light up some of these neurons.

00:06:03.300 --> 00:06:05.980
Because maybe, although this isn't the picture of a cat,

00:06:05.980 --> 00:06:08.480
maybe there are some kind of "catty" features in it.

00:06:08.760 --> 00:06:11.880
You know, like the edge of a leaf might
be kind of the same shape as an ear,

00:06:12.140 --> 00:06:15.200
or this texture, this grass,
kind of the same texture as fur.

00:06:15.200 --> 00:06:17.200
So, some of the same neurons are gonna light up,

00:06:17.820 --> 00:06:18.320
right?

00:06:18.320 --> 00:06:21.240
So what we do is we've been trying 
to make those bigger

00:06:21.240 --> 00:06:22.960
by altering our input image, ok?

00:06:23.360 --> 00:06:26.580
So just like we were trying...
train our network to get better,

00:06:26.780 --> 00:06:28.780
we train our image to get better,

00:06:28.880 --> 00:06:30.440
to be more of these features.

00:06:30.440 --> 00:06:31.320
Now, of course...

00:06:31.500 --> 00:06:33.880
this network is trained on lots of things
other than cats,

00:06:33.880 --> 00:06:35.880
so anything that looks at all plausible

00:06:35.880 --> 00:06:37.420
it's gonna try and maximize that effect.

00:06:37.420 --> 00:06:40.580
So, this is a picture I run through it,
here, ok?

00:06:40.580 --> 00:06:41.980
We've done some strange things.

00:06:41.980 --> 00:06:45.020
In the sky here we've got what kinda looks 
like buildings appearing

00:06:45.020 --> 00:06:47.020
And then down here we've got
some animals appearing,

00:06:47.220 --> 00:06:48.000
And s...

00:06:48.000 --> 00:06:49.680
I don't know what that is!

00:06:49.840 --> 00:06:50.780
Some sort of... dalek

00:06:50.780 --> 00:06:52.320
And then this weird animal here,

00:06:52.320 --> 00:06:54.080
if we zoom in on that...

00:06:54.080 --> 00:06:56.080
I mean... it's anybody's guess
what kind of animal that is.

00:06:56.220 --> 00:06:57.680
But this is what's so cool about Google Deep Dream:

00:06:57.680 --> 00:06:58.700
it's you don't know what you're going to get

00:06:58.700 --> 00:07:00.900
and it's going to depend on your input image.

00:07:00.900 --> 00:07:01.680
So... you know...

00:07:01.780 --> 00:07:03.280
The features that it found in the input image...

00:07:03.280 --> 00:07:07.320
"Oh, that feature looks a bit like a bunch of lines,
which in turn look a bit like...

00:07:07.320 --> 00:07:09.200
the edge of a cat's head...

00:07:09.800 --> 00:07:10.820
...make it look more like that."

00:07:10.820 --> 00:07:12.040
And if you keep doing this process

00:07:12.040 --> 00:07:14.340
it starts to converge on weird animals

00:07:14.340 --> 00:07:15.740
that have interesting features.

00:07:15.740 --> 00:07:17.740
- So is that multiple iterations?

00:07:17.740 --> 00:07:19.740
Yes! I think it does about...

00:07:20.380 --> 00:07:22.380
40 iterations by default.

00:07:22.660 --> 00:07:26.960
So it... tweaks the weights of the image
40 times.

00:07:27.200 --> 00:07:30.560
An actual fact, Google Deep Dream
does it at different scales, as well.

00:07:30.740 --> 00:07:32.820
But we've... I sort of blush over that
because it's not hugely important,

00:07:33.000 --> 00:07:34.900
but it runs a small version of the image first,

00:07:35.180 --> 00:07:37.980
makes it a bit bigger and runs it again,
makes it a bit bigger and runs it again.

00:07:39.220 --> 00:07:41.580
So you could then take this image,
and put it back into the fun

00:07:42.040 --> 00:07:45.480
and make it more: "I want more
of this weird shapes and weird animals".

00:07:45.480 --> 00:07:47.020
So I take this image and I put it in,

00:07:47.020 --> 00:07:48.920
and I get something that's really weird.

00:07:48.920 --> 00:07:51.380
So, it's the same, but just more of it.

00:07:51.540 --> 00:07:54.540
It bears no ressemblance with it,
I mean, there's a tiny bit of tree left here...

00:07:54.700 --> 00:07:55.200
But this...

00:07:55.600 --> 00:07:57.580
it bears very little ressemblance 
to our original image,

00:07:57.580 --> 00:08:00.600
apart from this generic area of ground and the sky...

00:08:00.740 --> 00:08:02.280
But on the other hand, we've got all kinds of...

00:08:02.320 --> 00:08:04.320
there's a weird car appearing here

00:08:04.320 --> 00:08:07.140
and some actual full own buildings
starting to appear.

00:08:07.340 --> 00:08:09.200
Because, later on in this network,

00:08:09.200 --> 00:08:10.400
some of these neurons

00:08:10.400 --> 00:08:12.380
are going to be representing building shapes.

00:08:12.380 --> 00:08:14.380
And so it's trying to make it "more building shape".

00:08:14.860 --> 00:08:17.220
What's this for? Right?

00:08:17.220 --> 00:08:18.420
Why are we doing this?

00:08:18.640 --> 00:08:21.140
I mean... that's a question you've got to ask Google,
cause I'm not entirely sure...

00:08:21.140 --> 00:08:22.280
But no, it's...

00:08:22.280 --> 00:08:23.080
There are two things...

00:08:23.220 --> 00:08:25.220
It's fun, right? so...
Mostly it's fun.

00:08:25.380 --> 00:08:27.580
Most people aren't interested in what neural
network is doing underneath,

00:08:27.700 --> 00:08:29.700
they like cool, trippy images.

00:08:30.700 --> 00:08:32.700
One of the probles with neural networks is
they are a black box.

00:08:33.160 --> 00:08:35.400
So we... design them with an architecture

00:08:35.540 --> 00:08:36.960
and then we run them,
and they get... I don't know...

00:08:36.960 --> 00:08:38.760
80% accuracy on some task,

00:08:38.840 --> 00:08:39.960
and that's very good,

00:08:40.020 --> 00:08:41.400
and then we say no more about it.

00:08:41.400 --> 00:08:45.960
We now have a program that can
classify these things at 80% accuracy.

00:08:45.960 --> 00:08:49.320
In many ways we don't really care about how it did it,

00:08:49.320 --> 00:08:51.320
(if it does it).

00:08:51.540 --> 00:08:54.040
But, if we want to improve these things
beyond the 80%,

00:08:54.040 --> 00:08:56.040
and beyond 90%,
and getting better and better...

00:08:56.880 --> 00:08:59.060
It's a good idea to try and understand

00:08:59.060 --> 00:09:01.060
what's going on underneath.

00:09:01.360 --> 00:09:02.840
So there are some papers out there,

00:09:03.280 --> 00:09:05.280
Google working on it developed papers as well,

00:09:05.440 --> 00:09:06.480
that are trying to understand

00:09:06.480 --> 00:09:08.640
what it is that the lowest
layers of the network are doing

00:09:08.940 --> 00:09:10.480
and the highest layers of the network are doing

00:09:10.480 --> 00:09:11.240
for different tasks.

00:09:11.780 --> 00:09:14.020
Intuitively the lowest levels are edges in things...

00:09:14.020 --> 00:09:16.580
and, with we go up, hierarchical group of these things,

00:09:16.580 --> 00:09:18.580
so... buildings and so on.

00:09:19.960 --> 00:09:21.740
One thing you could do... is you could...

00:09:21.740 --> 00:09:23.500
instead of maximizing this layer,

00:09:23.500 --> 00:09:25.500
which represents very high level objects,

00:09:25.500 --> 00:09:27.500
we can maximize one of the layers down here

00:09:27.500 --> 00:09:29.500
which maximizes edges and things.

00:09:29.500 --> 00:09:31.320
So here's another picture

00:09:31.320 --> 00:09:34.540
of Google Deep Dream that I've maximized
a lower layer

00:09:34.840 --> 00:09:37.380
So you can see that instead of
starting to form objects,

00:09:37.380 --> 00:09:39.380
it's now just starting to form

00:09:39.380 --> 00:09:41.380
patterns of lines and textures.

00:09:42.120 --> 00:09:44.120
And that's because that's the only thing

00:09:44.120 --> 00:09:46.120
that's described at this lower level of a network.

00:09:46.120 --> 00:09:47.260
- So now we are on Van Gogh

00:09:47.260 --> 00:09:49.440
Yeah, right?
So yeah... impressionism, huh?

00:09:49.440 --> 00:09:51.440
This is much better I can paint, as well.

00:09:51.440 --> 00:09:54.280
The idea is the lower levels
of a network are doing things like this....

00:09:54.280 --> 00:09:57.480
And the higher layers of a network
are looking for more complex objects.

00:09:57.720 --> 00:09:59.220
That's basically what neural network does.

00:09:59.640 --> 00:10:01.820
This network has been trained on
somewhere around defined and closed objects

00:10:01.940 --> 00:10:05.560
so cats, dogs, bikes, people, buildings and so on.

00:10:05.900 --> 00:10:07.900
This network that I showed at the beginning

00:10:07.900 --> 00:10:08.980
is trained only on buildings,

00:10:09.140 --> 00:10:11.940
which is why many of the things
that have been generated in it

00:10:12.120 --> 00:10:13.160
look like buildings.

00:10:13.520 --> 00:10:14.020
Often...

00:10:14.560 --> 00:10:15.440
some of the objects you see

00:10:15.460 --> 00:10:16.900
start to look very similar

00:10:16.900 --> 00:10:18.900
So you've got a building here that looks like a building,

00:10:18.900 --> 00:10:20.720
And that one that looks kinda similar

00:10:20.720 --> 00:10:21.840
with this spike on.

00:10:22.060 --> 00:10:24.060
And that's because the network's been 
trained on certain objects

00:10:24.640 --> 00:10:26.640
and these objects get a good response

00:10:26.640 --> 00:10:28.480
and then it maximizes those things.

00:10:28.480 --> 00:10:29.580
So the question was then:

00:10:29.580 --> 00:10:32.780
What if I want to generate an image
that makes it look more like a cat?

00:10:32.780 --> 00:10:34.780
Specifically a cat, rather than just

00:10:34.780 --> 00:10:37.860
cats and dogs and buildings
and bikes, and all this different things.

00:10:38.460 --> 00:10:40.460
So what we do is we put a cat image into it,

00:10:40.460 --> 00:10:41.540
into the network

00:10:41.700 --> 00:10:43.700
and we find out which of these light up,

00:10:43.700 --> 00:10:44.740
for a cat,

00:10:44.740 --> 00:10:46.740
specifically a cat, right?

00:10:47.740 --> 00:10:48.580
And then we...

00:10:48.580 --> 00:10:50.580
instead of maximizing all of them,

00:10:50.580 --> 00:10:52.180
we maximize only those ones, ok?

00:10:52.180 --> 00:10:53.780
So we're basically saing "now...

00:10:53.780 --> 00:10:54.900
"...more of it, please..."

00:10:55.100 --> 00:10:57.760
"...but more of only the specific
interesting cat ones."

00:10:59.500 --> 00:11:02.680
I chose cats because people on the
Internet have a lot of pictures of cats

00:11:03.160 --> 00:11:04.120
They're very easy to obtain...

00:11:04.120 --> 00:11:05.720
So I put in a picture.
Here's some pictures,

00:11:05.720 --> 00:11:06.780
Some cats I put in.

00:11:06.780 --> 00:11:08.780
So, when I put this into the image,

00:11:08.780 --> 00:11:09.580
into the neural network,

00:11:09.660 --> 00:11:11.880
it's going to classify this as a cat, or multiple cats.

00:11:11.880 --> 00:11:16.380
And it's going to that by finding combinations 
of features that look like cats.

00:11:16.380 --> 00:11:19.280
So if I pin down the learning to do this,

00:11:19.280 --> 00:11:22.140
I can start to make my image look more like a cat.

00:11:22.400 --> 00:11:24.220
So you can see that some eyes have appeared,

00:11:24.220 --> 00:11:25.380
there's a kinda... nose here

00:11:25.560 --> 00:11:27.640
That looks... let's face it,
it's not really a cat,

00:11:27.640 --> 00:11:29.640
but it's more a cat than the landscape was.

00:11:29.640 --> 00:11:32.500
It's a pretty weird image,
all things considered.

00:11:32.500 --> 00:11:33.940
And this is a high level.

00:11:33.940 --> 00:11:35.680
If we do the same thing for a lower layer,

00:11:35.680 --> 00:11:37.680
we can't get all that hierarchical

00:11:38.500 --> 00:11:40.500
sort of ears plus eyes plus nose.

00:11:40.500 --> 00:11:42.500
We can only get the low level things.

00:11:42.500 --> 00:11:44.500
So we can say do this one,

00:11:44.500 --> 00:11:47.240
which is almost entirely fur and eyes.

00:11:47.300 --> 00:11:48.260
Right? so...

00:11:48.260 --> 00:11:50.260
you can see that the clouds kinda look a bit like fur,

00:11:50.260 --> 00:11:52.260
so it's made them look more like that

00:11:52.260 --> 00:11:54.260
and the eyes... all down here.

00:11:54.260 --> 00:11:56.860
So, this is a different kind of image that we produced,

00:11:56.860 --> 00:11:58.860
by trying to make it look a bit like a cat,

00:11:58.860 --> 00:12:00.860
but only at low level,

00:12:00.860 --> 00:12:03.360
so, you know, what are the low level
features that make a cat.

00:12:03.860 --> 00:12:05.860
Ok? How strange!

00:12:06.800 --> 00:12:08.220
And finally, you can do it to people,

00:12:08.500 --> 00:12:10.500
so I put in a picture with some people's faces.

00:12:11.000 --> 00:12:13.000
And... out we get

00:12:13.000 --> 00:12:14.660
this incredibly weird...

00:12:14.780 --> 00:12:15.980
picture of sort of weird...

00:12:16.060 --> 00:12:17.140
harpy baby things...

00:12:17.140 --> 00:12:18.840
that kinda...
gives me nightmares.

00:12:19.140 --> 00:12:21.360
You could argue, in some ways
we're gaining in intuition about

00:12:21.360 --> 00:12:23.360
what the lower levels do,
what the higher levels do.

00:12:23.560 --> 00:12:25.560
Predominantly, it's just for fun.

00:12:25.560 --> 00:12:27.320
There are other papers that do...

00:12:27.320 --> 00:12:30.400
an outputting of network layers and trying
to work out what it is that each layer is doing

00:12:30.400 --> 00:12:34.000
but in this way it just...
generates cool images.

00:12:34.000 --> 00:12:36.000
So, if you want to use Google Deep Dream,

00:12:36.000 --> 00:12:37.240
you just need an input image

00:12:37.240 --> 00:12:38.540
and maybe a reference image

00:12:38.540 --> 00:12:39.900
like a cat to target it towards,

00:12:40.020 --> 00:12:42.020
but really you don't need much more than that.

00:12:42.020 --> 00:12:43.540
And then you can just get going on it.

00:12:43.680 --> 00:12:45.200
- Is there a website or something?

00:12:45.200 --> 00:12:47.200
So... no, so, actually, it's, it's

00:12:47.200 --> 00:12:49.440
python code which goes into caffe

00:12:49.520 --> 00:12:51.520
which is a deep learning library.

00:12:52.200 --> 00:12:54.200
So that's how I generated these images.

00:12:54.580 --> 00:12:56.580
People have obviously put a website frontend on this,

00:12:56.580 --> 00:12:59.100
it's very easy to find websites that do the same thing.

00:12:59.100 --> 00:13:02.220
But in actual fact, they would just
be running the source code back behing the scenes.

00:13:02.660 --> 00:13:04.400
If you look for the source code,
it's actually not very long.

00:13:04.940 --> 00:13:06.480
because this process of

00:13:06.480 --> 00:13:09.500
backpropagation is already
coded up in these libraries.

00:13:09.780 --> 00:13:11.440
So what we need to do is telling...

00:13:11.440 --> 00:13:13.660
instead of maximi... minimizing this C

00:13:13.660 --> 00:13:15.160
we wanna maximize the value of these things.

00:13:15.260 --> 00:13:16.460
And you just change a few numbers around,

00:13:16.580 --> 00:13:18.580
and send it backwards through the network.

00:13:19.020 --> 00:13:20.420
It's, you know, not...

00:13:20.420 --> 00:13:23.320
It sounds complicated, but really actually isn't that complicated

00:13:23.520 --> 00:13:25.380
once you actually look at the code.

00:13:26.060 --> 00:13:34.680
(other video reproducing)

