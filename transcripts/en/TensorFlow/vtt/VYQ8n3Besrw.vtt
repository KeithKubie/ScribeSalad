WEBVTT
Kind: captions
Language: en

00:00:01.930 --> 00:00:05.140
LUCASZ KAISER: Hi, my
name is Lucasz Kaiser,

00:00:05.140 --> 00:00:09.340
and I want to tell you
in this final session

00:00:09.340 --> 00:00:12.100
about Tensor2Tensor,
which is a library we've

00:00:12.100 --> 00:00:16.149
built on top of TensorFlow to
organize the world's models

00:00:16.149 --> 00:00:17.920
and data sets.

00:00:17.920 --> 00:00:21.370
So I want to tell you
about the motivation,

00:00:21.370 --> 00:00:24.820
and how it came together,
and what you can do with it.

00:00:24.820 --> 00:00:26.830
But also if you
have any questions

00:00:26.830 --> 00:00:30.280
in the meantime
anytime, just ask.

00:00:30.280 --> 00:00:33.290
And only if you've already used
Tensor2Tensor, in that case,

00:00:33.290 --> 00:00:36.070
you might have even
more questions.

00:00:36.070 --> 00:00:38.680
But the motivation
behind this library

00:00:38.680 --> 00:00:42.080
is-- so I am a researcher
in machine learning.

00:00:42.080 --> 00:00:44.410
I also worked from production
[INAUDIBLE] models,

00:00:44.410 --> 00:00:46.870
and research can
be very annoying.

00:00:46.870 --> 00:00:48.940
It can be very annoying
to researchers,

00:00:48.940 --> 00:00:51.130
and it's even more
annoying to people

00:00:51.130 --> 00:00:56.110
who put it into production,
because the research works

00:00:56.110 --> 00:00:56.810
like this.

00:00:56.810 --> 00:00:57.790
You have an idea.

00:00:57.790 --> 00:00:59.210
You want to try it out.

00:00:59.210 --> 00:01:00.970
It's machine learning,
and you think,

00:01:00.970 --> 00:01:04.180
well, I will change something
in the model, it will be great.

00:01:04.180 --> 00:01:10.420
It will solve physics problems,
or translation, or whatever.

00:01:10.420 --> 00:01:13.540
So we have this idea, and
you're like, it's so simple.

00:01:13.540 --> 00:01:16.770
I just need to change one
tweak, but then, OK, I

00:01:16.770 --> 00:01:17.890
need to get the data.

00:01:17.890 --> 00:01:18.550
Where was it?

00:01:18.550 --> 00:01:21.510
So we search it
online, you find it,

00:01:21.510 --> 00:01:25.360
and it's like, well, so
I need to preprocess it.

00:01:25.360 --> 00:01:27.040
You implement some data reading.

00:01:27.040 --> 00:01:30.280
You download the model
that someone else did.

00:01:30.280 --> 00:01:32.860
And it doesn't give
the result at all

00:01:32.860 --> 00:01:34.820
that someone else
wrote in the paper.

00:01:34.820 --> 00:01:36.400
It's worse.

00:01:36.400 --> 00:01:37.930
It works 10 times slower.

00:01:37.930 --> 00:01:39.830
It doesn't train at all.

00:01:39.830 --> 00:01:41.560
So then you start tweaking it.

00:01:41.560 --> 00:01:44.080
Turns out, someone else
had this postscript

00:01:44.080 --> 00:01:46.930
that preprocessed the
data in a certain way that

00:01:46.930 --> 00:01:49.120
improved the model 10 times.

00:01:49.120 --> 00:01:50.210
So you add that.

00:01:50.210 --> 00:01:52.840
Then it turns out your input
pipeline is not performing,

00:01:52.840 --> 00:01:57.670
because it doesn't put data
on GPU or CPU or whatever.

00:01:57.670 --> 00:01:59.500
So you tweak that.

00:01:59.500 --> 00:02:01.570
Before you start with
your research idea,

00:02:01.570 --> 00:02:05.170
you've spent half a
year on just reproducing

00:02:05.170 --> 00:02:07.200
what's been done before.

00:02:07.200 --> 00:02:07.840
So then great.

00:02:07.840 --> 00:02:09.830
Then you do your idea.

00:02:09.830 --> 00:02:10.330
It works.

00:02:13.070 --> 00:02:14.330
You write the paper.

00:02:14.330 --> 00:02:15.410
You submit it.

00:02:15.410 --> 00:02:18.050
You put it in the
repo on GitHub,

00:02:18.050 --> 00:02:20.160
which has a README
file that says,

00:02:20.160 --> 00:02:22.190
well, I downloaded
the data from there,

00:02:22.190 --> 00:02:24.680
but this link has
already gone by two days

00:02:24.680 --> 00:02:26.210
after he made the repo.

00:02:26.210 --> 00:02:27.290
And then I applied.

00:02:27.290 --> 00:02:29.570
And you describe
all these 17 tweaks,

00:02:29.570 --> 00:02:32.669
but maybe you forgot one
option that was crucial.

00:02:32.669 --> 00:02:35.210
Well, and then there is the next
paper and the next research,

00:02:35.210 --> 00:02:38.000
and the next person
comes and does the same.

00:02:38.000 --> 00:02:44.330
So it's all great except the
production team, at some point,

00:02:44.330 --> 00:02:46.610
they get like, well, we
should put it into production.

00:02:46.610 --> 00:02:49.640
It's a great result.
And then they

00:02:49.640 --> 00:02:52.990
need to track this whole
path, redo all of it,

00:02:52.990 --> 00:02:55.180
and try to get the same.

00:02:55.180 --> 00:03:01.140
So it's a very difficult
state of the world.

00:03:01.140 --> 00:03:03.830
And it's even worse because
there are different hardware

00:03:03.830 --> 00:03:04.730
configurations.

00:03:04.730 --> 00:03:07.370
So maybe something that
trained well on a CPU

00:03:07.370 --> 00:03:11.780
does not train on a GPU, or
maybe you need an 8 GPU setup,

00:03:11.780 --> 00:03:14.420
and so on and so forth.

00:03:14.420 --> 00:03:17.210
So the idea behind
Tensor2Tensor was,

00:03:17.210 --> 00:03:19.370
let's make a library
that has at least a bunch

00:03:19.370 --> 00:03:22.520
of standard models for
standard tasks that includes

00:03:22.520 --> 00:03:24.860
the data and the preprocessing.

00:03:24.860 --> 00:03:27.690
So you really can, on a
command line, just say,

00:03:27.690 --> 00:03:31.640
please get me this data set
and this model, and train it,

00:03:31.640 --> 00:03:34.910
and make it so that we can have
regression tests and actually

00:03:34.910 --> 00:03:38.750
know that it will train, and
that it will not break with

00:03:38.750 --> 00:03:40.250
TensorFlow 1.10.

00:03:40.250 --> 00:03:43.350
And that it will train both
on the GPU and on a TPU,

00:03:43.350 --> 00:03:44.990
and on a CPU--

00:03:44.990 --> 00:03:47.780
to have it in a more
organized fashion.

00:03:47.780 --> 00:03:51.710
And the thing that
prompted Tensor2Tensor,

00:03:51.710 --> 00:03:55.680
the thing why I started it,
was machine translation.

00:03:55.680 --> 00:03:57.870
So I worked with the
Google Translate team

00:03:57.870 --> 00:04:01.150
on launching neural
networks for translation.

00:04:01.150 --> 00:04:04.960
And this was two years ago,
and this was amazing work.

00:04:04.960 --> 00:04:08.440
Because before that,
machine translation

00:04:08.440 --> 00:04:11.320
was done in this way like--

00:04:11.320 --> 00:04:13.550
it was called phrase-based
machine translation.

00:04:13.550 --> 00:04:16.060
So if you find some
alignments of phrases,

00:04:16.060 --> 00:04:18.130
then you translate the
phrases, and then you

00:04:18.130 --> 00:04:21.399
try to realign the
sentences to make them work.

00:04:21.399 --> 00:04:23.800
And the results in
machine translation

00:04:23.800 --> 00:04:26.050
are normally measured
in terms of something

00:04:26.050 --> 00:04:28.120
called the BLEU score.

00:04:28.120 --> 00:04:30.040
I will not go into the
details of what it was.

00:04:30.040 --> 00:04:32.320
It's like the higher the better.

00:04:32.320 --> 00:04:35.930
So for example, for
English-German translation,

00:04:35.930 --> 00:04:40.810
the BLEU score that human
translators get is about 30.

00:04:40.810 --> 00:04:44.820
And the best phrase-based--
so non-neural network,

00:04:44.820 --> 00:04:48.280
non-deep-learning--
systems were about 20, 21.

00:04:48.280 --> 00:04:53.140
And it's been, really, a
decade of research at least,

00:04:53.140 --> 00:04:54.400
maybe more.

00:04:54.400 --> 00:04:58.300
So when I was doing a PhD,
if you got one BLEU score up,

00:04:58.300 --> 00:04:59.590
you would be a star.

00:04:59.590 --> 00:05:01.720
It was good PhD.

00:05:01.720 --> 00:05:06.650
If you went from 21 to
22, it would be amazing.

00:05:06.650 --> 00:05:08.710
So then the neural
networks came.

00:05:08.710 --> 00:05:15.730
And the early LSTMs in 2015,
they were like 19.5, 20.

00:05:15.730 --> 00:05:18.490
And we talked to
the Translate team,

00:05:18.490 --> 00:05:22.090
and they were like, you
know, guys, it's fun.

00:05:22.090 --> 00:05:24.100
It's interesting, because
it's simpler in a way.

00:05:24.100 --> 00:05:25.760
You just train the
network on the data.

00:05:25.760 --> 00:05:27.750
You don't have all the--

00:05:27.750 --> 00:05:30.730
no language-specific stuff.

00:05:30.730 --> 00:05:32.890
It's a simpler system.

00:05:32.890 --> 00:05:35.920
But it gets worse
results, and who knows

00:05:35.920 --> 00:05:37.045
if it will ever get better.

00:05:40.030 --> 00:05:43.890
But then the neural
network research moved on,

00:05:43.890 --> 00:05:46.960
and people started
getting 21, 22.

00:05:46.960 --> 00:05:49.560
So the Translate team, together
with Brain, where I work,

00:05:49.560 --> 00:05:53.440
made the big effort to try
to make a really large LSTM

00:05:53.440 --> 00:05:56.580
model, which is called the
GNMT, the Google Neural Machine

00:05:56.580 --> 00:05:58.410
Translation.

00:05:58.410 --> 00:06:01.350
And indeed it was
a huge improvement.

00:06:01.350 --> 00:06:03.180
It got to 25.

00:06:03.180 --> 00:06:07.510
BLEU, later-- we added mixtures
of experts, it even got to 26.

00:06:07.510 --> 00:06:09.490
So they were amazed.

00:06:09.490 --> 00:06:12.300
It launched in
production, and well, it

00:06:12.300 --> 00:06:17.820
was like a two-year
effort to take the papers,

00:06:17.820 --> 00:06:19.860
scale them up, launch it.

00:06:19.860 --> 00:06:21.570
And to get these
really good results,

00:06:21.570 --> 00:06:23.190
you really needed
a large network.

00:06:23.190 --> 00:06:28.590
So as an example why
this is important,

00:06:28.590 --> 00:06:31.500
or why this was
important for Google is--

00:06:31.500 --> 00:06:36.054
so you have a sentence
in German here,

00:06:36.054 --> 00:06:37.470
which is like,
"problems can never

00:06:37.470 --> 00:06:41.370
be solved with the same way
of thinking that caused them."

00:06:41.370 --> 00:06:45.810
And this neural translator
translates the sentence kind

00:06:45.810 --> 00:06:47.340
of the way it should--

00:06:47.340 --> 00:06:50.100
I doubt there is a much
better translation--

00:06:50.100 --> 00:06:52.470
while the phrase-based
translators, you can see,

00:06:52.470 --> 00:06:55.080
"no problem can be solved
from the same consciousness

00:06:55.080 --> 00:06:56.850
that they have arisen."

00:06:56.850 --> 00:07:00.600
It kind of shows how the
phrase-based method works.

00:07:00.600 --> 00:07:04.150
Every word or phrase is
translated correctly,

00:07:04.150 --> 00:07:10.080
but the whole thing
does not exactly add up.

00:07:10.080 --> 00:07:13.710
You can see it's a
very machiney way,

00:07:13.710 --> 00:07:17.250
and it's not so clear what
it is supposed to say.

00:07:17.250 --> 00:07:19.200
So the big advantage
of neural networks

00:07:19.200 --> 00:07:21.480
is they train on
whole sentences.

00:07:21.480 --> 00:07:23.670
They can even train
on paragraphs.

00:07:23.670 --> 00:07:25.100
They can be very fluent.

00:07:25.100 --> 00:07:30.720
Since they take into account
the whole context at once,

00:07:30.720 --> 00:07:32.050
it's a really big improvement.

00:07:32.050 --> 00:07:34.920
And if you ask people
to score translations,

00:07:34.920 --> 00:07:36.780
this really starts
coming close--

00:07:36.780 --> 00:07:42.600
or at least 80% of the distance
to what human translators do,

00:07:42.600 --> 00:07:45.870
at least on newspaper
language-- not poetry.

00:07:45.870 --> 00:07:47.150
[CHUCKLING]

00:07:47.150 --> 00:07:49.430
We're nowhere near that.

00:07:49.430 --> 00:07:52.510
So it was great.

00:07:52.510 --> 00:07:55.420
We got the high BLEU scores.

00:07:55.420 --> 00:07:59.200
We reduced the distance
to human translators.

00:07:59.200 --> 00:08:01.780
It turned out the
one system can handle

00:08:01.780 --> 00:08:03.550
different languages,
and sometimes even

00:08:03.550 --> 00:08:06.260
multilingual translations.

00:08:06.260 --> 00:08:07.510
But there were problems.

00:08:07.510 --> 00:08:09.440
So one problem is
the training time.

00:08:09.440 --> 00:08:14.980
It took about a week on a
setup of 64 to 128 GPUs.

00:08:14.980 --> 00:08:21.420
And all the code for that
was done specifically

00:08:21.420 --> 00:08:23.010
for this hardware setup.

00:08:23.010 --> 00:08:25.320
So it was distributed
training where

00:08:25.320 --> 00:08:27.300
everything in the
machine learning pipeline

00:08:27.300 --> 00:08:30.140
was tuned for the hardware.

00:08:30.140 --> 00:08:32.140
Well, because we knew we
will train on this data

00:08:32.140 --> 00:08:33.350
center on this hardware.

00:08:33.350 --> 00:08:35.350
So why not?

00:08:35.350 --> 00:08:40.559
Well, the problem is batch
sizes and learning rates,

00:08:40.559 --> 00:08:41.890
they come together.

00:08:41.890 --> 00:08:44.010
You can not tune
them separately.

00:08:44.010 --> 00:08:45.930
And then you add tricks.

00:08:45.930 --> 00:08:47.930
Then you tweak some
things in the model

00:08:47.930 --> 00:08:50.910
that are really good
for this specific setup,

00:08:50.910 --> 00:08:54.360
for this specific learning
grade or batch size.

00:08:54.360 --> 00:08:58.240
This distributed setup was
training asynchronously.

00:08:58.240 --> 00:09:00.180
So there were delayed gradients.

00:09:00.180 --> 00:09:04.050
It's a regular [? ISO, ?]
so you decrease dropout.

00:09:04.050 --> 00:09:07.020
You start doing
parts of the model

00:09:07.020 --> 00:09:09.840
specifically for
a hardware setup.

00:09:09.840 --> 00:09:12.780
And then you write the paper.

00:09:12.780 --> 00:09:13.800
We did write a paper.

00:09:13.800 --> 00:09:15.720
It was cited.

00:09:15.720 --> 00:09:17.640
But nobody ever
outside of Google

00:09:17.640 --> 00:09:20.490
managed to reproduce
this, get the same result

00:09:20.490 --> 00:09:23.730
with the same network,
because we can give you

00:09:23.730 --> 00:09:26.520
our hyperparameters, but you're
running on a different hardware

00:09:26.520 --> 00:09:27.690
setup.

00:09:27.690 --> 00:09:30.100
You will not get
the same result.

00:09:30.100 --> 00:09:32.520
And then, in addition to
the machine learning setup,

00:09:32.520 --> 00:09:35.070
there is the whole will
tokenization pipeline, data

00:09:35.070 --> 00:09:36.810
preparation pipeline.

00:09:36.810 --> 00:09:40.090
And even though these results
are on the public data,

00:09:40.090 --> 00:09:43.910
the whole pre-processing
is also partially Google.

00:09:43.910 --> 00:09:45.420
It doesn't matter much.

00:09:45.420 --> 00:09:50.610
But it really did not
allow other people

00:09:50.610 --> 00:09:52.770
to build on top of this work.

00:09:52.770 --> 00:09:56.220
So it launched, it
was a success for us,

00:09:56.220 --> 00:09:58.680
but in the research
sense, we felt that it

00:09:58.680 --> 00:10:00.540
came short a little bit.

00:10:00.540 --> 00:10:03.900
Because for one, I mean, you'd
need a huge hardware setup

00:10:03.900 --> 00:10:05.100
to train it.

00:10:05.100 --> 00:10:09.660
And on the other hand, even
if you had the hardware setup,

00:10:09.660 --> 00:10:14.450
or if you got it on cloud
and wanted to invest in it,

00:10:14.450 --> 00:10:18.590
there would still be no
way for you to just do it.

00:10:18.590 --> 00:10:22.070
And that was the
prompt, why I thought,

00:10:22.070 --> 00:10:24.800
OK, we need to make a
library for the next time

00:10:24.800 --> 00:10:27.250
we build a model.

00:10:27.250 --> 00:10:31.370
So the LSTMs were like the
first wave of sequence models

00:10:31.370 --> 00:10:33.030
with the first great results.

00:10:33.030 --> 00:10:36.540
But I thought, OK, the next
time when we come build a model,

00:10:36.540 --> 00:10:39.530
we need to have a library that
will ensure it works at Google

00:10:39.530 --> 00:10:43.850
and outside, that will make
sure when you train on one GPU,

00:10:43.850 --> 00:10:46.452
you get a worse result,
but we know what it is.

00:10:46.452 --> 00:10:48.410
We can tell you, yes,
you're on the same setup.

00:10:48.410 --> 00:10:49.550
Just scale up.

00:10:49.550 --> 00:10:53.720
And it should work on
cloud so you can just,

00:10:53.720 --> 00:10:56.210
if you want better
result, get some money,

00:10:56.210 --> 00:10:58.460
pay for larger hardware.

00:10:58.460 --> 00:11:04.060
But it should be tested, done,
and reproducible outside.

00:11:04.060 --> 00:11:08.120
And the need-- so the
Tensor2Tensor library started

00:11:08.120 --> 00:11:10.940
with the model
called Transformer,

00:11:10.940 --> 00:11:13.820
which is the next generation
of sequence models.

00:11:13.820 --> 00:11:15.605
It's based on
self-attentional layers.

00:11:18.240 --> 00:11:20.760
And we designed this model.

00:11:20.760 --> 00:11:22.680
It got even better results.

00:11:22.680 --> 00:11:25.470
It got 28.4 BLEU.

00:11:25.470 --> 00:11:28.680
Now we are on par with BLEU
with human translators.

00:11:28.680 --> 00:11:31.540
So this metric is
not good anymore.

00:11:31.540 --> 00:11:35.340
It just means that we
need better metrics.

00:11:35.340 --> 00:11:42.550
But this thing, it can train
in one day on an 8 GPU machine.

00:11:42.550 --> 00:11:45.340
So you can just get it.

00:11:45.340 --> 00:11:46.700
Get an 8 GPU machine.

00:11:46.700 --> 00:11:50.270
It can be your machine,
it can be in the cloud.

00:11:50.270 --> 00:11:53.440
Train, get the results.

00:11:53.440 --> 00:11:56.870
And it's not just
reproducible in principle.

00:11:56.870 --> 00:11:59.410
There's been a number of
groups that reproduced it, got

00:11:59.410 --> 00:12:02.950
the same results,
wrote follow-up papers,

00:12:02.950 --> 00:12:04.250
changed the architecture.

00:12:04.250 --> 00:12:07.360
It went up to 29,
it went up to 30.

00:12:07.360 --> 00:12:09.760
There are companies
that use this code.

00:12:09.760 --> 00:12:12.310
They launched competition
to Google Translate.

00:12:12.310 --> 00:12:13.510
Well, that happens.

00:12:13.510 --> 00:12:16.870
And Google Translate
improved again.

00:12:16.870 --> 00:12:20.320
But in a sense, I feel like it's
been a larger success in terms

00:12:20.320 --> 00:12:24.950
of community and research, and
it raised the bar for everyone.

00:12:24.950 --> 00:12:27.440
It raised our quality as well.

00:12:27.440 --> 00:12:30.790
So that's how it
came to be that we

00:12:30.790 --> 00:12:35.890
feel that it's really important
to make things reproducible,

00:12:35.890 --> 00:12:41.890
open, and test them on
different configurations

00:12:41.890 --> 00:12:43.420
and different hardwares.

00:12:43.420 --> 00:12:46.120
Because then we can isolate
what parts are really

00:12:46.120 --> 00:12:49.510
good fundamentally from
the parts that are just

00:12:49.510 --> 00:12:51.550
tweaks that work in
one configuration

00:12:51.550 --> 00:12:52.660
and fail in the other.

00:12:55.240 --> 00:13:00.740
So that's our solution to this
annoying research problem.

00:13:00.740 --> 00:13:04.300
It's a solution that
requires a lot of work,

00:13:04.300 --> 00:13:07.450
and it's based on many layers.

00:13:07.450 --> 00:13:10.540
So the bottom layer
is TensorFlow.

00:13:10.540 --> 00:13:14.230
And TensorFlow, in the meantime,
has also evolved a lot.

00:13:14.230 --> 00:13:18.700
So we have TF Data, which is the
TensorFlow data input pipeline.

00:13:18.700 --> 00:13:20.890
It was also not
there a year ago.

00:13:20.890 --> 00:13:22.930
It's in the newer releases.

00:13:22.930 --> 00:13:24.880
It helps to build
input pipelines that

00:13:24.880 --> 00:13:28.670
are performant across
different hardware.

00:13:28.670 --> 00:13:31.600
There is TF Layers
and Keras, which

00:13:31.600 --> 00:13:33.060
are higher-level libraries.

00:13:33.060 --> 00:13:35.980
So you don't need to
write, in small TensorFlow

00:13:35.980 --> 00:13:36.820
Ops, everything.

00:13:36.820 --> 00:13:41.110
You can write things on a
higher level of abstraction.

00:13:41.110 --> 00:13:44.390
There is the new
distribution strategy,

00:13:44.390 --> 00:13:46.330
which allows you to
have an estimator

00:13:46.330 --> 00:13:47.830
and say, OK, train
on eight GPUs,

00:13:47.830 --> 00:13:50.050
train on one GPU, train
on a distributed setup,

00:13:50.050 --> 00:13:51.000
train on TPU.

00:13:51.000 --> 00:13:55.660
You don't need rewrite handlers
for everything on your own.

00:13:55.660 --> 00:13:58.420
But that's just the basics.

00:13:58.420 --> 00:14:04.510
And then comes the Tensor2Tensor
part, which is like, OK,

00:14:04.510 --> 00:14:08.900
I want a good translation model,
where do I get the data from?

00:14:08.900 --> 00:14:10.720
It's somewhere on the
internet, but where?

00:14:13.920 --> 00:14:16.830
How do I download it?

00:14:16.830 --> 00:14:18.390
How do I pre-process it?

00:14:18.390 --> 00:14:19.830
Which model should I use?

00:14:19.830 --> 00:14:23.130
Which hyperparameters
of the model?

00:14:23.130 --> 00:14:25.110
What if I want to
change a model?

00:14:25.110 --> 00:14:29.160
I just want to try my
own, but on the same data.

00:14:29.160 --> 00:14:30.930
What do I need to change?

00:14:30.930 --> 00:14:32.520
How can it be done?

00:14:32.520 --> 00:14:35.250
What if I want to use the same
model, but on my own data?

00:14:35.250 --> 00:14:36.600
I have a translation company.

00:14:36.600 --> 00:14:38.340
I have some data.

00:14:38.340 --> 00:14:42.200
I want to check how that works.

00:14:42.200 --> 00:14:43.200
What if I want to share?

00:14:43.200 --> 00:14:44.491
What if I want to share a part?

00:14:44.491 --> 00:14:46.590
What if I want to
share everything?

00:14:46.590 --> 00:14:50.530
That's what Tensor2Tensor does.

00:14:50.530 --> 00:14:53.170
So it's a library.

00:14:53.170 --> 00:14:57.420
It's a library that has
a lot of data sets--

00:14:57.420 --> 00:15:00.750
I think it's more
than 100 by now--

00:15:00.750 --> 00:15:05.090
all the standard ones, images,
ImageNet, CIFAR, MNIST,

00:15:05.090 --> 00:15:07.680
image captionings,
Coco, translations

00:15:07.680 --> 00:15:11.280
for a number of languages,
just pure language

00:15:11.280 --> 00:15:18.700
modeling data sets, speech to
text, music, video data sets.

00:15:18.700 --> 00:15:20.035
It's recently very active.

00:15:22.660 --> 00:15:26.110
If you're into research, you
can either probably find it here

00:15:26.110 --> 00:15:29.650
or there is a very easy
tutorial on how to add it.

00:15:29.650 --> 00:15:32.380
And then with the data
sets come the models.

00:15:32.380 --> 00:15:34.800
There is the transformer,
as I said you--

00:15:34.800 --> 00:15:37.780
told you-- that's
how it started.

00:15:37.780 --> 00:15:43.210
But then the standard things,
ResNet, then more fancy image

00:15:43.210 --> 00:15:50.890
models like RevNet, ShakeShake,
Xception, Sequence Model,

00:15:50.890 --> 00:15:52.920
also a bunch of them,
SliceNet, ByteNet,

00:15:52.920 --> 00:15:55.770
that's subversion of WaveNet.

00:15:55.770 --> 00:16:03.320
LSTMs then algorithmic
models like Neural GPUs.

00:16:03.320 --> 00:16:05.890
There was a bunch
of recent papers.

00:16:05.890 --> 00:16:10.900
So it's a selection of
models and data sets,

00:16:10.900 --> 00:16:13.240
but also the framework.

00:16:13.240 --> 00:16:18.394
So if you want to train a model,
there is one way to do it.

00:16:18.394 --> 00:16:19.310
There are many models.

00:16:19.310 --> 00:16:20.690
You need to specify which one.

00:16:20.690 --> 00:16:21.856
And there are many datasets.

00:16:21.856 --> 00:16:23.590
You need to specify which one.

00:16:23.590 --> 00:16:25.990
But there is one
training binary.

00:16:25.990 --> 00:16:27.460
So it's always the same.

00:16:27.460 --> 00:16:31.480
No two page read me,
please run these commands

00:16:31.480 --> 00:16:34.240
and for another run
different comments.

00:16:34.240 --> 00:16:35.530
Same for decoding.

00:16:35.530 --> 00:16:37.510
You want to get your the
outputs of your model

00:16:37.510 --> 00:16:38.740
on the new data set?

00:16:38.740 --> 00:16:40.810
One command, t2t decoder.

00:16:40.810 --> 00:16:44.140
You want to export it to
make a server or a website?

00:16:44.140 --> 00:16:45.550
One command.

00:16:45.550 --> 00:16:48.430
And then you want to
train, train locally,

00:16:48.430 --> 00:16:49.570
you just run the binary.

00:16:49.570 --> 00:16:53.500
If you want to train
on Google Cloud,

00:16:53.500 --> 00:16:56.740
just give your cloud project ID.

00:16:56.740 --> 00:17:01.080
You want to train on cloud
TPU, just say dash dash use TPU

00:17:01.080 --> 00:17:04.450
and give the ID.

00:17:04.450 --> 00:17:07.950
You need to tune
hyper parameters.

00:17:07.950 --> 00:17:10.800
There is support for
it on Google Cloud.

00:17:10.800 --> 00:17:11.910
We have ranges.

00:17:11.910 --> 00:17:17.160
Just specify the
hyperparameter range and tune.

00:17:17.160 --> 00:17:20.329
You want to train distributed
on multiple machines,

00:17:20.329 --> 00:17:23.940
there is a script for that.

00:17:23.940 --> 00:17:29.440
So Tensor2Tensor are data
sets, models, and everything

00:17:29.440 --> 00:17:33.940
around that's needed
to train them.

00:17:33.940 --> 00:17:38.890
Now, this project, due to our
experience with translation,

00:17:38.890 --> 00:17:42.940
we decided it's open
by default. And open

00:17:42.940 --> 00:17:47.050
by default, in a similar
way as TensorFlow,

00:17:47.050 --> 00:17:50.950
means every internal
code change we push gets

00:17:50.950 --> 00:17:53.260
immediately pushed to GitHub.

00:17:53.260 --> 00:17:57.920
And every PR from GitHub, we
import internally and merge.

00:17:57.920 --> 00:17:59.530
So there is just one code base.

00:17:59.530 --> 00:18:04.120
And since this project is
pure Python, there's no magic.

00:18:04.120 --> 00:18:07.180
It's the same code at
Google and outside.

00:18:07.180 --> 00:18:12.550
And it's like internally we have
dozens of code changes a day.

00:18:12.550 --> 00:18:15.580
They get pushed out
to GitHub immediately.

00:18:15.580 --> 00:18:20.650
And since a lot of brain
researchers use this daily,

00:18:20.650 --> 00:18:21.950
there are things like this.

00:18:21.950 --> 00:18:25.600
So there was a tweet about
research and optimizers.

00:18:25.600 --> 00:18:29.320
And it was like, there
are optimizers like AMS

00:18:29.320 --> 00:18:32.170
grad, adaptive learning
create methods.

00:18:32.170 --> 00:18:34.220
And then James
Bradbury at Facebook

00:18:34.220 --> 00:18:38.830
at that time tweeted, well,
it's not the latest optimizer.

00:18:38.830 --> 00:18:42.040
The latest optimizer is in
Tensor2Tensor encode with a

00:18:42.040 --> 00:18:43.300
to do to write a paper.

00:18:46.609 --> 00:18:47.650
The paper is written now.

00:18:47.650 --> 00:18:51.040
It's a very good optimizer
[INAUDIBLE] factor.

00:18:51.040 --> 00:18:54.960
But yeah, the code,
it just appears there.

00:18:54.960 --> 00:18:58.200
The papers come later.

00:18:58.200 --> 00:19:00.060
But it makes no sense to wait.

00:19:00.060 --> 00:19:05.880
I mean, it's an open
research community.

00:19:05.880 --> 00:19:10.770
These ideas, sometimes they
work, sometimes they don't.

00:19:10.770 --> 00:19:12.510
But that's how we work.

00:19:12.510 --> 00:19:14.710
We push things out.

00:19:14.710 --> 00:19:17.210
And then we train and see.

00:19:17.210 --> 00:19:19.760
Actually, by the time
the paper appeared,

00:19:19.760 --> 00:19:21.590
some people in the
open source community

00:19:21.590 --> 00:19:23.220
have already trained
models with it.

00:19:23.220 --> 00:19:25.930
So we added the results.

00:19:25.930 --> 00:19:27.120
They were happy to.

00:19:27.120 --> 00:19:32.540
It's a very good optimizer,
saves a lot of memory.

00:19:32.540 --> 00:19:33.770
It's a big collaboration.

00:19:33.770 --> 00:19:37.790
So as I said, this is
just one list of names.

00:19:37.790 --> 00:19:41.990
It should probably
be longer by now.

00:19:41.990 --> 00:19:46.790
It's a collaboration between
Google Brain, DeepMind.

00:19:46.790 --> 00:19:50.230
Currently there are researchers
from the Czech Republic

00:19:50.230 --> 00:19:59.210
on GitHub and Germany, so
over 100 contributors by now,

00:19:59.210 --> 00:20:00.800
over 100,000 downloads.

00:20:00.800 --> 00:20:05.810
I was surprised, because Ryan
got this number for this talk.

00:20:05.810 --> 00:20:08.870
And I was like, how comes
there are 100,000 people using

00:20:08.870 --> 00:20:09.440
this thing?

00:20:09.440 --> 00:20:10.760
It's for ML researchers.

00:20:10.760 --> 00:20:14.120
But whatever, they are.

00:20:17.040 --> 00:20:22.544
And there are a lot
of papers that use it.

00:20:22.544 --> 00:20:24.460
So these are just the
papers that have already

00:20:24.460 --> 00:20:26.200
been published and accepted.

00:20:29.870 --> 00:20:33.310
There is a long
pipeline of other papers

00:20:33.310 --> 00:20:35.710
and possibly some
we don't know about.

00:20:39.640 --> 00:20:43.780
So as I told you, it's a
unified framework for models.

00:20:43.780 --> 00:20:45.620
So how does it work?

00:20:45.620 --> 00:20:50.890
Well, the main script of the
whole library is t2t-trainer.

00:20:50.890 --> 00:20:55.180
It's the one binary where you
tell what model, what data set,

00:20:55.180 --> 00:20:58.912
what hyperparameters, go train.

00:20:58.912 --> 00:21:06.260
So that's the basic command
line-- install tensor2tensor

00:21:06.260 --> 00:21:09.130
and then call t2t-trainer.

00:21:09.130 --> 00:21:11.620
The problem is the
name of the dataset.

00:21:11.620 --> 00:21:14.080
And it also includes
all details like how

00:21:14.080 --> 00:21:19.270
to pre-process, how to resize
images, and so on and so forth.

00:21:19.270 --> 00:21:22.100
Model is the name of the
model and hyperparameter set

00:21:22.100 --> 00:21:24.490
is which configuration,
which hyperparameters

00:21:24.490 --> 00:21:29.290
of the model, which learning
grades, and so on, to use.

00:21:29.290 --> 00:21:31.480
And then, of course,
you need to specify

00:21:31.480 --> 00:21:34.330
where to store the
data, where to store

00:21:34.330 --> 00:21:38.200
the model checkpoints for how
many steps to train and so on.

00:21:38.200 --> 00:21:40.390
But that's the full command.

00:21:40.390 --> 00:21:46.540
And for example, you want
a summarization model.

00:21:46.540 --> 00:21:48.220
There is a
summarization data set

00:21:48.220 --> 00:21:49.600
that's been used in academia.

00:21:49.600 --> 00:21:52.750
It's from CNN and Daily Mail.

00:21:52.750 --> 00:21:54.160
You say you want
the transformer,

00:21:54.160 --> 00:21:55.840
and there is a
hyperparameter set that

00:21:55.840 --> 00:21:58.920
does well on summarization.

00:21:58.920 --> 00:22:00.720
You want to image
classification,

00:22:00.720 --> 00:22:05.430
like CIFAR10 is quite a
standard benchmark for papers.

00:22:05.430 --> 00:22:08.550
You say, I want image CIFAR10.

00:22:08.550 --> 00:22:11.970
ShakeShake model, this was
state of the art a year

00:22:11.970 --> 00:22:13.200
or a year ago.

00:22:13.200 --> 00:22:15.600
This changes quickly.

00:22:15.600 --> 00:22:18.720
You want the big
model, you go train it.

00:22:18.720 --> 00:22:22.140
And the important thing
is we know this result.

00:22:22.140 --> 00:22:26.710
This gives less than 3% error
on CIFAR, which is, as I said,

00:22:26.710 --> 00:22:28.670
was state of the art a year ago.

00:22:28.670 --> 00:22:30.090
Now it's down to two.

00:22:30.090 --> 00:22:34.170
But we can be
certain that when you

00:22:34.170 --> 00:22:38.530
run this command for the
specified number of training

00:22:38.530 --> 00:22:42.000
steps, you will actually
get this state of the art,

00:22:42.000 --> 00:22:44.970
because internally we
run regression tests that

00:22:44.970 --> 00:22:49.600
start this every day
and tell us if it fails.

00:22:49.600 --> 00:22:53.550
So the usefulness of this
framework is not just in--

00:22:53.550 --> 00:22:55.950
well, we have it grouped
into one command.

00:22:55.950 --> 00:22:59.040
But because it's automated,
we can start testing it.

00:22:59.040 --> 00:23:01.080
If there is a new change
in TensorFlow that

00:23:01.080 --> 00:23:03.780
will break some kernel
and it doesn't come out

00:23:03.780 --> 00:23:06.210
in the unit test,
it often comes out

00:23:06.210 --> 00:23:09.880
in the regression
tests of these models.

00:23:09.880 --> 00:23:14.160
And we found at least three
bugs in the recent two versions

00:23:14.160 --> 00:23:17.940
of TensorFlow, because some
things in machine learning only

00:23:17.940 --> 00:23:19.320
appear--

00:23:19.320 --> 00:23:21.990
like, things still run,
things still train,

00:23:21.990 --> 00:23:24.540
but they give you 2% less.

00:23:24.540 --> 00:23:26.370
These are very
tricky bugs to find,

00:23:26.370 --> 00:23:29.370
but if you know which
day it started failing,

00:23:29.370 --> 00:23:32.430
it's much easier.

00:23:32.430 --> 00:23:37.320
Translation, as I said, it
started with transformer.

00:23:37.320 --> 00:23:39.450
We added more changes.

00:23:39.450 --> 00:23:42.450
Nowadays, it trains
to over 29 BLEU.

00:23:42.450 --> 00:23:45.300
It's a very good
translation model.

00:23:45.300 --> 00:23:48.030
Just run this command
on an 8 GPU machine.

00:23:48.030 --> 00:23:49.230
Wait.

00:23:49.230 --> 00:23:52.680
You will get a really
good translator.

00:23:52.680 --> 00:23:59.120
Speech recognition, there is
the open librispeech data set.

00:23:59.120 --> 00:24:03.090
Transformer model without
any language model

00:24:03.090 --> 00:24:05.010
gets a really good
word error rate.

00:24:08.230 --> 00:24:11.320
Some more fancy things, like
if you want to generate images,

00:24:11.320 --> 00:24:14.980
it's recently popular, have a
model that just generates you.

00:24:14.980 --> 00:24:20.770
Either phases or landscapes,
there are different datasets.

00:24:20.770 --> 00:24:23.350
So this is a model
that you train just

00:24:23.350 --> 00:24:25.830
on CIFAR 10 reversed.

00:24:25.830 --> 00:24:28.090
Every data set in
tensor2tensor you

00:24:28.090 --> 00:24:32.920
can add this underscore rev.
It reverses inputs and targets.

00:24:32.920 --> 00:24:37.270
And generative models, they
can take it and generate it.

00:24:37.270 --> 00:24:38.944
For translation,
it's very useful

00:24:38.944 --> 00:24:41.485
if you want, instead of English,
German, and German, English,

00:24:41.485 --> 00:24:45.040
you just do underscore
rev. It reverses

00:24:45.040 --> 00:24:46.695
the ordering of the dataset.

00:24:49.830 --> 00:24:53.320
So yeah, so they're
the commands.

00:24:53.320 --> 00:24:55.810
But so for example, on
an image transformer,

00:24:55.810 --> 00:24:59.870
if you try to train
this on a single GPU

00:24:59.870 --> 00:25:02.650
to get to this 2.9
beats per dimension,

00:25:02.650 --> 00:25:05.210
you'd probably have
to wait half a year.

00:25:05.210 --> 00:25:08.800
So that's not very practical.

00:25:08.800 --> 00:25:09.850
But that's the point.

00:25:09.850 --> 00:25:13.060
Currently it's a very hard
task to do a very good image

00:25:13.060 --> 00:25:16.310
generative model.

00:25:16.310 --> 00:25:20.170
One GPU might not be enough
for state of the art.

00:25:20.170 --> 00:25:23.910
So if you want to really push
it, you need to train at scale.

00:25:23.910 --> 00:25:25.720
You need to train multi GPU.

00:25:25.720 --> 00:25:27.075
You need to go to TPUs.

00:25:30.320 --> 00:25:34.650
Well, this is the command
you've seen before.

00:25:34.650 --> 00:25:40.650
To make it multi GPU, you
just say worker GPU equals 8.

00:25:40.650 --> 00:25:43.500
This will use eight
GPUs on your machine.

00:25:43.500 --> 00:25:46.770
Just make batches
eight times larger.

00:25:46.770 --> 00:25:54.990
Run the eight GPUs in
parallel, and there it trains.

00:25:54.990 --> 00:25:58.280
Want to train on a cloud TPU?

00:25:58.280 --> 00:26:02.360
Use TPU, and you need to specify
the master of the TPU instance

00:26:02.360 --> 00:26:05.540
that you booked on cloud.

00:26:05.540 --> 00:26:08.840
It trains the same.

00:26:08.840 --> 00:26:11.160
Want to train on
a cloud TPU pod?

00:26:11.160 --> 00:26:13.620
I don't know, I guess
you've heard today,

00:26:13.620 --> 00:26:18.650
Google is opening up to public
the pods which go up to 256,

00:26:18.650 --> 00:26:21.500
I think, TPU cores.

00:26:21.500 --> 00:26:29.090
Just say, oh, maybe up to 512,
what I see from this command.

00:26:29.090 --> 00:26:30.890
Just say do it.

00:26:30.890 --> 00:26:32.450
Train.

00:26:32.450 --> 00:26:34.730
It will train much faster.

00:26:34.730 --> 00:26:35.790
How much faster?

00:26:35.790 --> 00:26:40.790
Well, we've observed
nearly linear scaling up

00:26:40.790 --> 00:26:47.580
to half a pod, and I think,
like, 10% loss on a full pod.

00:26:47.580 --> 00:26:50.180
So these models, the
translation models,

00:26:50.180 --> 00:26:53.160
they can train on
a pod for an hour,

00:26:53.160 --> 00:26:56.340
and you get state of
the art performance.

00:26:56.340 --> 00:27:00.050
So this can really make
you train very fast.

00:27:00.050 --> 00:27:01.542
Same for ImageNet.

00:27:01.542 --> 00:27:03.500
Well, I say an hour,
there's now a competition.

00:27:03.500 --> 00:27:06.780
Can we get down to half
an hour, 18 minutes.

00:27:06.780 --> 00:27:12.760
I'm not sure how important
that is, but it's really fast.

00:27:12.760 --> 00:27:16.020
Now, maybe you don't
just care about training

00:27:16.020 --> 00:27:17.490
one set of hyperparameters.

00:27:17.490 --> 00:27:19.230
Maybe you have your
own data set and you

00:27:19.230 --> 00:27:22.110
need to tune hyperparameters,
find a really good model

00:27:22.110 --> 00:27:25.270
for your application.

00:27:25.270 --> 00:27:29.690
Say cloud ML engine auto tune.

00:27:29.690 --> 00:27:32.390
You need to say what
metric to optimize--

00:27:32.390 --> 00:27:36.050
so accuracy, perplexity,
these are the standard metrics

00:27:36.050 --> 00:27:38.840
that people tune the models for.

00:27:38.840 --> 00:27:42.950
Say how many trials, how many
of them to run in parallel.

00:27:42.950 --> 00:27:45.410
And the final line is a range.

00:27:45.410 --> 00:27:47.780
So a range says, well,
try learning grades

00:27:47.780 --> 00:27:52.640
from 0.1 to 0.3,
logarithmically or uniformly.

00:27:52.640 --> 00:27:54.120
These are the
things you specify.

00:27:54.120 --> 00:27:57.980
So you can specify continuous
things in an interval

00:27:57.980 --> 00:27:59.810
and you can specify
discrete things.

00:27:59.810 --> 00:28:04.760
Just try two, three,
four, five layers.

00:28:04.760 --> 00:28:08.970
And the tuner, it starts the
number of parallel trials,

00:28:08.970 --> 00:28:11.370
so 20 in this command.

00:28:11.370 --> 00:28:13.850
The first one is random,
and then the next one,

00:28:13.850 --> 00:28:19.010
it has a quite sophisticated
non-differential optimizing

00:28:19.010 --> 00:28:22.430
model which is Bayesian
mixed with CMAES.

00:28:22.430 --> 00:28:26.060
What to try next, it will
try another 20 trials.

00:28:26.060 --> 00:28:30.620
Usually after, like, 60
or so it starts getting

00:28:30.620 --> 00:28:33.360
to a good parameter space.

00:28:33.360 --> 00:28:37.830
So if you need to optimize,
that's how you do it.

00:28:37.830 --> 00:28:40.230
And, like, if you're wondering
what range to optimize,

00:28:40.230 --> 00:28:42.450
we have a few ranges
in code that we usually

00:28:42.450 --> 00:28:45.790
optimize for when we
start with new data.

00:28:48.790 --> 00:28:53.730
On a TPU pod, if you want
a model that doesn't just

00:28:53.730 --> 00:28:55.650
do training on
large batches, data

00:28:55.650 --> 00:28:57.990
parallel, but model parallel.

00:28:57.990 --> 00:29:00.630
If you want to have a model with
a huge number of parameters,

00:29:00.630 --> 00:29:03.750
more than one
billion, you can use

00:29:03.750 --> 00:29:07.620
something we call mesh
TensorFlow that we also

00:29:07.620 --> 00:29:10.320
have started developing
in tensor2tensor,

00:29:10.320 --> 00:29:13.410
which allows to do model
parallelism in an easy way.

00:29:13.410 --> 00:29:18.310
It just say, split my
tensor into the cores,

00:29:18.310 --> 00:29:20.010
how many cores you have.

00:29:20.010 --> 00:29:22.980
Or split it eight-wise on
this dimension and four-wise

00:29:22.980 --> 00:29:24.120
on this dimension.

00:29:24.120 --> 00:29:26.910
I'll tell a bit more
later about that.

00:29:26.910 --> 00:29:30.902
It allows you to train really
large models if you want this.

00:29:30.902 --> 00:29:32.360
And that gives
really good results.

00:29:35.230 --> 00:29:38.040
So that's how the library works.

00:29:38.040 --> 00:29:40.350
You can go and use it
with the models and data

00:29:40.350 --> 00:29:42.400
sets that are there.

00:29:42.400 --> 00:29:49.090
But what if you want to just
get the data from the data set

00:29:49.090 --> 00:29:51.600
or to add your own data set?

00:29:51.600 --> 00:29:53.380
Well, it's still
a Python library.

00:29:53.380 --> 00:29:55.370
You can just import it.

00:29:55.370 --> 00:30:00.680
And there is this
problem class, which

00:30:00.680 --> 00:30:05.610
you can use without any
other part of the library.

00:30:05.610 --> 00:30:06.410
So you can just--

00:30:06.410 --> 00:30:09.650
you get an instance
of the problem class

00:30:09.650 --> 00:30:11.660
either by-- so we
have this registry

00:30:11.660 --> 00:30:13.160
to call things by name.

00:30:13.160 --> 00:30:17.820
So you can say registry
dot problem and the name.

00:30:17.820 --> 00:30:20.330
You can say problems
dot available to get

00:30:20.330 --> 00:30:22.460
all the available names.

00:30:22.460 --> 00:30:24.470
Or you can instantiate
it directly.

00:30:24.470 --> 00:30:27.170
If you look into the code where
the class is, you can say,

00:30:27.170 --> 00:30:29.200
give me this class.

00:30:29.200 --> 00:30:31.510
And then generate data.

00:30:31.510 --> 00:30:35.770
The problem class knows where
on the internet to find the data

00:30:35.770 --> 00:30:37.810
and how to pre-process it.

00:30:37.810 --> 00:30:40.380
So the generate data
will go to this place,

00:30:40.380 --> 00:30:44.890
download it from the internet,
and pre-process into TF example

00:30:44.890 --> 00:30:48.100
files in the same
way that we use it

00:30:48.100 --> 00:30:50.820
or that the authors
of this data set

00:30:50.820 --> 00:30:53.740
decide it is good
for their models.

00:30:53.740 --> 00:30:57.160
And then you call a problem
dot data set, which reads it

00:30:57.160 --> 00:31:00.220
from this, can gives you
this queue of tensors

00:31:00.220 --> 00:31:01.530
in the form of a data set.

00:31:05.780 --> 00:31:07.840
So that's for data sets.

00:31:07.840 --> 00:31:11.110
For a model, all our
models are a subclass

00:31:11.110 --> 00:31:16.250
of this t2t model class,
which itself is a Keras layer.

00:31:16.250 --> 00:31:18.700
So if you want to
take one model,

00:31:18.700 --> 00:31:22.330
plug it together with another
one, same as with layers.

00:31:25.290 --> 00:31:26.480
You get a model.

00:31:26.480 --> 00:31:31.960
You can get it again either
by registry or by class name.

00:31:31.960 --> 00:31:36.710
Call the model on a
dictionary of tensors.

00:31:36.710 --> 00:31:39.970
And you get the outputs
and the losses if you need.

00:31:42.590 --> 00:31:44.220
So you can add your own.

00:31:44.220 --> 00:31:47.930
You can subclass the
base problem class,

00:31:47.930 --> 00:31:52.490
or for text to text or
image to class problems,

00:31:52.490 --> 00:31:54.770
there are subclasses that
are easier to subclass.

00:31:54.770 --> 00:31:59.540
You just basically point where
your images are and get them

00:31:59.540 --> 00:32:03.830
from any format to this.

00:32:03.830 --> 00:32:06.890
And for your own model,
you can subclass t2t model.

00:32:09.750 --> 00:32:13.180
If you want to share
it, it's on GitHub.

00:32:13.180 --> 00:32:15.230
Make a PR.

00:32:15.230 --> 00:32:18.390
Under models, there is
a research sub directory

00:32:18.390 --> 00:32:20.490
where there are models
that we don't consider,

00:32:20.490 --> 00:32:22.290
that we don't regression test.

00:32:22.290 --> 00:32:24.840
We allow them to be free.

00:32:24.840 --> 00:32:28.590
If you have an idea, want
to share, put it there.

00:32:28.590 --> 00:32:31.230
People might come, run
it, tell you it's great.

00:32:33.840 --> 00:32:36.690
So yeah, Tensor2Tensor,
it's a set of data sets,

00:32:36.690 --> 00:32:40.540
models, and scripts
to run it everywhere.

00:32:40.540 --> 00:32:46.270
And yeah, looking
ahead, it's growing.

00:32:46.270 --> 00:32:48.610
So we are happy to
have more data sets.

00:32:48.610 --> 00:32:51.300
We are happy to
have more models.

00:32:51.300 --> 00:32:54.770
We are ramping up on
regression testing.

00:32:54.770 --> 00:32:56.260
We're moving models
out of research

00:32:56.260 --> 00:32:58.780
to the more official
part to have them

00:32:58.780 --> 00:33:01.870
tested and stabilized.

00:33:01.870 --> 00:33:04.930
On the technical side,
we are on to simplifying

00:33:04.930 --> 00:33:05.860
the infrastructure.

00:33:05.860 --> 00:33:09.460
So TensorFlow 2 is coming.

00:33:09.460 --> 00:33:14.620
The code base-- well, it's
started more than a year ago.

00:33:14.620 --> 00:33:16.150
It's based on estimators.

00:33:16.150 --> 00:33:18.220
We are moving it to Keras.

00:33:18.220 --> 00:33:22.570
We had our own
scripts and binaries

00:33:22.570 --> 00:33:26.020
for running on
TPUs and multi-GPUs

00:33:26.020 --> 00:33:28.990
or moving to a
distribution strategy.

00:33:28.990 --> 00:33:31.420
We are allowing
experts to TF hub.

00:33:31.420 --> 00:33:34.490
So this is a library for
training your own models.

00:33:34.490 --> 00:33:36.160
The main thing is the trainer.

00:33:36.160 --> 00:33:38.750
Once it's trained and you want
to share a pre-trained model,

00:33:38.750 --> 00:33:41.570
TF hub is the right place.

00:33:41.570 --> 00:33:45.640
You can export it with one line.

00:33:45.640 --> 00:33:49.840
And the mesh TensorFlow
allows to train huge models

00:33:49.840 --> 00:33:51.250
on cloud pods.

00:33:51.250 --> 00:33:54.820
I will tell you a little bit
more about it in a moment.

00:33:54.820 --> 00:33:58.180
On the research side, there's
been a lot of research

00:33:58.180 --> 00:34:01.690
in video models recently.

00:34:01.690 --> 00:34:04.840
We have a ton of them
in Tensor2Tensor.

00:34:04.840 --> 00:34:06.470
And they're getting
better and better.

00:34:06.470 --> 00:34:10.690
And it's a fun thing to
generate your own videos.

00:34:10.690 --> 00:34:14.590
There is-- the new thing
in machine translation

00:34:14.590 --> 00:34:16.690
is using back translation.

00:34:16.690 --> 00:34:20.710
So it uses an unsupervised--
you have a corpus of English

00:34:20.710 --> 00:34:23.219
and a corpus of German,
but no matching.

00:34:23.219 --> 00:34:26.860
And to use a model you have
to generate data and then back

00:34:26.860 --> 00:34:30.489
translate and it
shows improvements.

00:34:30.489 --> 00:34:35.199
And in general, well,
hyperparameter tuning

00:34:35.199 --> 00:34:37.960
is an important thing
in research, too.

00:34:37.960 --> 00:34:40.510
So it's integrated
now, and we're

00:34:40.510 --> 00:34:42.429
doing more and more of it.

00:34:42.429 --> 00:34:48.070
Reinforcement learning,
guns, well, as I said,

00:34:48.070 --> 00:34:49.790
there are a lot of
researchers using it.

00:34:49.790 --> 00:34:51.159
So there's a lot going on.

00:34:53.780 --> 00:34:58.520
One of the things,
Mesh TensorFlow,

00:34:58.520 --> 00:35:04.360
it's a tool for training huge
models, meaning really huge.

00:35:04.360 --> 00:35:07.450
Like, you can have one
model that uses a whole TPU

00:35:07.450 --> 00:35:12.420
pod, 4 terabytes of RAM.

00:35:12.420 --> 00:35:15.840
That's how many
parameters you can do.

00:35:15.840 --> 00:35:21.540
It's by Noam, Youlong, Niki,
Ashish, and many people.

00:35:21.540 --> 00:35:25.380
So what if you want to
train an image generation

00:35:25.380 --> 00:35:30.840
models on high definition
videos or process data that's

00:35:30.840 --> 00:35:34.090
huge even at batch size 1?

00:35:34.090 --> 00:35:37.030
So you cannot just say, oh,
I'll do one thing on one core,

00:35:37.030 --> 00:35:39.640
another on one core,
just split it by data.

00:35:39.640 --> 00:35:43.280
One data example has to
go on the whole machine.

00:35:43.280 --> 00:35:46.750
And then there needs to be a
convolution that applies to it,

00:35:46.750 --> 00:35:49.930
or a matrix multiplication.

00:35:49.930 --> 00:35:54.010
So how can we do
this and not drown

00:35:54.010 --> 00:35:57.350
into writing manually,
OK, on this core,

00:35:57.350 --> 00:36:00.820
do this, and then slice back?

00:36:00.820 --> 00:36:10.150
So the idea is build every
tensor, every dimension

00:36:10.150 --> 00:36:12.765
it has needs to be named.

00:36:12.765 --> 00:36:14.890
For example, you name the
first dimension is batch.

00:36:14.890 --> 00:36:16.570
The second is length.

00:36:16.570 --> 00:36:20.320
And the third is just
the hidden vector.

00:36:20.320 --> 00:36:22.870
And for every dimension,
you specify how it

00:36:22.870 --> 00:36:25.460
will be laid out on a device.

00:36:25.460 --> 00:36:26.920
So you say, OK, batches--

00:36:26.920 --> 00:36:30.490
for example, modern
devices, they have 2D--

00:36:30.490 --> 00:36:32.140
they're like a 2D mesh of chips.

00:36:32.140 --> 00:36:34.600
So the communication is
fast to nearby chips,

00:36:34.600 --> 00:36:38.080
but not so fast across.

00:36:38.080 --> 00:36:42.710
So you can say if it's a grid of
chips in hardware, you can say,

00:36:42.710 --> 00:36:46.457
OK, the batch dimension will
be on the horizontal chips

00:36:46.457 --> 00:36:48.290
and the length will be
on the vertical ones.

00:36:48.290 --> 00:36:53.470
So we define how to split the
tensor on the hardware mesh.

00:36:53.470 --> 00:36:57.160
And then the
operations are already

00:36:57.160 --> 00:37:03.640
optimized to use the
processing of the hardware

00:37:03.640 --> 00:37:07.900
to do fast communication
and operate on these tensors

00:37:07.900 --> 00:37:10.310
as if they were single sensors.

00:37:10.310 --> 00:37:12.040
So you specify the
dimensions by name.

00:37:12.040 --> 00:37:14.110
You specify their layout.

00:37:14.110 --> 00:37:16.990
And then you write your model
as if it was a single GPU model.

00:37:19.780 --> 00:37:26.690
And so everything stays simple
except for this layout thing,

00:37:26.690 --> 00:37:28.730
which you need to think
a little bit about.

00:37:31.390 --> 00:37:33.490
We did a transformer on it.

00:37:33.490 --> 00:37:36.160
We did an image transformer.

00:37:36.160 --> 00:37:41.380
We can train models with 5
million parameters on TPU pods

00:37:41.380 --> 00:37:44.620
with over 50% utilization.

00:37:44.620 --> 00:37:46.850
So this paper, it's
also a to do paper,

00:37:46.850 --> 00:37:49.670
it should be coming
out in a few weeks.

00:37:49.670 --> 00:37:53.950
Not yet there, but it's new
state-of-the-art on translation

00:37:53.950 --> 00:37:56.050
language modeling.

00:37:56.050 --> 00:37:59.200
It's the next step in
really good models.

00:37:59.200 --> 00:38:02.950
It also generates nice images.

00:38:02.950 --> 00:38:05.350
So big models are good.

00:38:05.350 --> 00:38:07.330
They give great results.

00:38:07.330 --> 00:38:12.040
And this is a way of
writing them simply.

00:38:12.040 --> 00:38:17.050
So yeah, that's the
Mesh TensorFlow.

00:38:17.050 --> 00:38:19.990
And we try to make it--
so it runs on TPU pods,

00:38:19.990 --> 00:38:23.410
but it also runs on
clusters of GPUs,

00:38:23.410 --> 00:38:26.170
because we tried to not make the
mistake again to do something

00:38:26.170 --> 00:38:28.440
that just runs on one hardware.

00:38:28.440 --> 00:38:30.200
And with the
Tensor2Tensor library,

00:38:30.200 --> 00:38:33.880
you're welcome to be part of it.

00:38:33.880 --> 00:38:35.440
Give it a try.

00:38:35.440 --> 00:38:37.400
Use it.

00:38:37.400 --> 00:38:40.370
We are on GitHub.

00:38:40.370 --> 00:38:42.420
There is a GitHub chat.

00:38:42.420 --> 00:38:46.240
There is an active lobby for
Tensor2Tensor, where we also

00:38:46.240 --> 00:38:48.685
try to be everyday day to help.

00:38:51.830 --> 00:38:54.270
And yep, that's it.

00:38:54.270 --> 00:38:55.640
Thank you very much.

00:38:55.640 --> 00:38:58.690
[APPLAUSE]

