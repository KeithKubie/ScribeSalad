WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:03.469
&gt;&gt; In theory you take a DAG and
you scheduled it on a PRAM.

00:00:03.469 --> 00:00:07.260
But here a question
where do DAGs come from?

00:00:07.260 --> 00:00:09.290
The answer is from you.

00:00:09.290 --> 00:00:12.870
But you need a programming
model to generate the DAGs.

00:00:12.870 --> 00:00:16.615
Let me show you one that,
I think is especially clean and elegant.

00:00:16.615 --> 00:00:19.860
Here's the DAG for a divide and
conquer algorithm to computer reduction.

00:00:19.860 --> 00:00:24.060
You have argued that it's work
optimal and it has logarithmic span.

00:00:24.060 --> 00:00:28.120
Now, I want to give you an algorithm
that produces this DAG.

00:00:28.120 --> 00:00:31.200
Let's start with just a sequential
algorithm that implements the divide and

00:00:31.200 --> 00:00:31.890
conquer scheme.

00:00:32.960 --> 00:00:35.360
So here is the pseudo code for
such an algorithm.

00:00:35.360 --> 00:00:38.110
It takes an array A of length n, and

00:00:38.110 --> 00:00:40.870
if there are at least two elements
it does divide and conquer.

00:00:40.870 --> 00:00:46.440
So recursively calls itself on each
half of the array, and returns the sum.

00:00:46.440 --> 00:00:50.410
Otherwise the array is just of size
one and it returns that element.

00:00:50.410 --> 00:00:53.220
Now what you can observe
is that the two recursive

00:00:53.220 --> 00:00:56.110
calls are independent of one another.

00:00:56.110 --> 00:00:59.270
Now since the two recursive
function calls are independent.

00:00:59.270 --> 00:01:02.920
I'm going to give you a special
keyword to mark that fact.

00:01:02.920 --> 00:01:04.720
That keyword is called a spawn.

00:01:05.830 --> 00:01:10.140
The target of a spawn is always either
a function call or a procedure call.

00:01:10.140 --> 00:01:12.650
In this case I'm showing
a function call.

00:01:12.650 --> 00:01:16.050
Now the spawn keyword is a signal
to either the compiler or

00:01:16.050 --> 00:01:20.000
the runtime system that the target
is an independent unit of work.

00:01:21.010 --> 00:01:23.750
By inserting the spawn you're
effectively saying that the target

00:01:23.750 --> 00:01:27.080
may be executed asynchronously
from the collar

00:01:27.080 --> 00:01:29.570
any time a processor is available.

00:01:29.570 --> 00:01:32.740
Now even though these two calls
are independent units of work,

00:01:32.740 --> 00:01:37.210
notice that they produce intermediate
results that then have to be combined.

00:01:37.210 --> 00:01:42.590
In other words there's a dependence
from a and b and the return statement.

00:01:42.590 --> 00:01:44.550
That means in addition to spawns,

00:01:44.550 --> 00:01:48.100
we also need a way to indicate
these kinds of dependences.

00:01:48.100 --> 00:01:51.940
For that, I will give you a second
special keyword called sync.

00:01:51.940 --> 00:01:52.980
So, one question.

00:01:52.980 --> 00:01:56.880
To which spawns does a given sync apply?

00:01:56.880 --> 00:02:00.900
So we will use a particular convention
which is that the sync waits for

00:02:00.900 --> 00:02:04.520
any spawn that has occurred so
far within the same stack frame.

00:02:04.520 --> 00:02:07.790
If you need a refresher on
call stacks and stack frames,

00:02:07.790 --> 00:02:09.119
please see the instructor's notes.

00:02:10.320 --> 00:02:13.670
Let me make one final remark
about spawn and sync.

00:02:13.670 --> 00:02:16.600
Now, suppose you leave the sync out.

00:02:16.600 --> 00:02:21.485
Even if you leave sync out, there will
always be an implicit sync at the return

00:02:21.485 --> 00:02:24.430
immediately before going
back to the caller.

00:02:24.430 --> 00:02:28.196
Let me show you what that means by
transforming this code to match

00:02:28.196 --> 00:02:29.800
the note.

00:02:29.800 --> 00:02:35.020
So instead of return of a+b,
we'll generate three statements.

00:02:35.020 --> 00:02:37.550
The first will evaluate the operand a+b.

00:02:37.550 --> 00:02:40.950
The second will perform the sync.

00:02:40.950 --> 00:02:43.840
And then finally will return
the value to the caller.

00:02:43.840 --> 00:02:46.200
And notice where the sync appears, and

00:02:46.200 --> 00:02:49.050
you transform the other
return in the same way.

00:02:49.050 --> 00:02:50.440
Now, an important point.

00:02:50.440 --> 00:02:54.460
Even with this transformation,
the program is still wrong.

00:02:54.460 --> 00:02:56.210
Can you see why?

00:02:56.210 --> 00:02:58.500
Notice that the sync
appears after the sum.

00:02:59.540 --> 00:03:03.981
Now the two spawned calls are only
guaranteed to be complete at the sync.

00:03:03.981 --> 00:03:08.940
Therefore the values of a and
b might not yet be valid at that point.

00:03:08.940 --> 00:03:13.070
Now the fact of Implicit syncs will
constrain the kinds of DAGs that this

00:03:13.070 --> 00:03:15.480
programming model can produce.

00:03:15.480 --> 00:03:20.490
The style of parallelism in such DAGs
is sometimes called nested parallelism.

00:03:20.490 --> 00:03:24.140
Now once you see how this pseudo code
gives rise to a DAG you'll understand

00:03:24.140 --> 00:03:27.140
where the term nested
parallelism comes from.

00:03:27.140 --> 00:03:29.540
Let's go back to the correct code.

00:03:29.540 --> 00:03:32.910
Now what I want you to do is try
simulating the algorithm to see how

00:03:32.910 --> 00:03:34.920
the DAG unfolds.

00:03:34.920 --> 00:03:38.480
So in particular let's say we start
to execute a reduce on an array of

00:03:38.480 --> 00:03:39.900
size four.

00:03:39.900 --> 00:03:42.740
The first step is to
enter the reduce call.

00:03:42.740 --> 00:03:46.320
That creates the first unit
of work within the DAG.

00:03:46.320 --> 00:03:48.030
Then there's the conditional test.

00:03:48.030 --> 00:03:49.270
Then you encounter the spawn.

00:03:50.310 --> 00:03:52.330
Here's where things get interesting.

00:03:52.330 --> 00:03:56.060
The spawn creates a new
branch in the DAG.

00:03:56.060 --> 00:03:59.300
Essentially the spawn signals that
there's a new independent of work

00:03:59.300 --> 00:04:00.690
that's ready to go.

00:04:00.690 --> 00:04:03.110
And it creates a new path as well.

00:04:03.110 --> 00:04:06.870
Now in the mean time,
the current path will continue.

00:04:06.870 --> 00:04:10.230
So this is an extremely important
point to that the spawn creates

00:04:10.230 --> 00:04:12.970
essentially two independent paths.

00:04:12.970 --> 00:04:16.750
One path carries the new work and
the other path is the path

00:04:16.750 --> 00:04:20.290
that continues executing
immediately after the spawn.

00:04:20.290 --> 00:04:23.490
Now while the main path is
happily going about its business,

00:04:23.490 --> 00:04:26.730
the newly spawned path
is a recursive call.

00:04:26.730 --> 00:04:30.020
And therefore it has to
generate its own subgraph.

00:04:30.020 --> 00:04:32.100
Now you are travelling
along the main path and

00:04:32.100 --> 00:04:34.500
so you'll encounter the second spawn.

00:04:34.500 --> 00:04:36.550
This branching will happen again.

00:04:36.550 --> 00:04:38.760
Next, you'll reach the sync.

00:04:38.760 --> 00:04:41.340
The sync waits for
the previous spawns to complete.

00:04:42.360 --> 00:04:45.625
In terms of the DAG, that should create
some dependence edges between these

00:04:45.625 --> 00:04:47.990
subgraphs and the sync point.

00:04:47.990 --> 00:04:50.960
Control then goes to
the return statement, and

00:04:50.960 --> 00:04:54.030
you'll evaluate a+b and
then finally return that value.

