WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:01.610
 
you've not heard a lot about how to

00:00:01.610 --> 00:00:01.620
you've not heard a lot about how to
 

00:00:01.620 --> 00:00:04.400
you've not heard a lot about how to
search for good hyper parameters before

00:00:04.400 --> 00:00:04.410
search for good hyper parameters before
 

00:00:04.410 --> 00:00:06.740
search for good hyper parameters before
wrapping up our discussion on hyper

00:00:06.740 --> 00:00:06.750
wrapping up our discussion on hyper
 

00:00:06.750 --> 00:00:08.600
wrapping up our discussion on hyper
parameter search I want to share with

00:00:08.600 --> 00:00:08.610
parameter search I want to share with
 

00:00:08.610 --> 00:00:11.089
parameter search I want to share with
you just a couple final tips and tricks

00:00:11.089 --> 00:00:11.099
you just a couple final tips and tricks
 

00:00:11.099 --> 00:00:13.730
you just a couple final tips and tricks
for how to organize your hyper Frances

00:00:13.730 --> 00:00:13.740
for how to organize your hyper Frances
 

00:00:13.740 --> 00:00:16.099
for how to organize your hyper Frances
search process deep learning today is

00:00:16.099 --> 00:00:16.109
search process deep learning today is
 

00:00:16.109 --> 00:00:18.200
search process deep learning today is
applied to many different application

00:00:18.200 --> 00:00:18.210
applied to many different application
 

00:00:18.210 --> 00:00:21.290
applied to many different application
areas and that intuitions about hyper

00:00:21.290 --> 00:00:21.300
areas and that intuitions about hyper
 

00:00:21.300 --> 00:00:22.910
areas and that intuitions about hyper
parameter settings from one application

00:00:22.910 --> 00:00:22.920
parameter settings from one application
 

00:00:22.920 --> 00:00:25.370
parameter settings from one application
area may or may not transfer to a

00:00:25.370 --> 00:00:25.380
area may or may not transfer to a
 

00:00:25.380 --> 00:00:27.410
area may or may not transfer to a
different one there is a lot of

00:00:27.410 --> 00:00:27.420
different one there is a lot of
 

00:00:27.420 --> 00:00:28.580
different one there is a lot of
cross-fertilization

00:00:28.580 --> 00:00:28.590
cross-fertilization
 

00:00:28.590 --> 00:00:30.290
cross-fertilization
a lot of different applications domain

00:00:30.290 --> 00:00:30.300
a lot of different applications domain
 

00:00:30.300 --> 00:00:33.049
a lot of different applications domain
so for example I've seen ideas 2000 the

00:00:33.049 --> 00:00:33.059
so for example I've seen ideas 2000 the
 

00:00:33.059 --> 00:00:35.240
so for example I've seen ideas 2000 the
computer vision community such as

00:00:35.240 --> 00:00:35.250
computer vision community such as
 

00:00:35.250 --> 00:00:37.580
computer vision community such as
confidence or res Nets which we'll talk

00:00:37.580 --> 00:00:37.590
confidence or res Nets which we'll talk
 

00:00:37.590 --> 00:00:40.760
confidence or res Nets which we'll talk
about in a later course successfully

00:00:40.760 --> 00:00:40.770
about in a later course successfully
 

00:00:40.770 --> 00:00:43.280
about in a later course successfully
apply to speech and I've seen ideas that

00:00:43.280 --> 00:00:43.290
apply to speech and I've seen ideas that
 

00:00:43.290 --> 00:00:44.330
apply to speech and I've seen ideas that
were first developed in speech

00:00:44.330 --> 00:00:44.340
were first developed in speech
 

00:00:44.340 --> 00:00:47.810
were first developed in speech
successfully applied in NLP and so on so

00:00:47.810 --> 00:00:47.820
successfully applied in NLP and so on so
 

00:00:47.820 --> 00:00:49.850
successfully applied in NLP and so on so
one nice development in deep learning is

00:00:49.850 --> 00:00:49.860
one nice development in deep learning is
 

00:00:49.860 --> 00:00:51.889
one nice development in deep learning is
that people from different application

00:00:51.889 --> 00:00:51.899
that people from different application
 

00:00:51.899 --> 00:00:54.709
that people from different application
domains do read increasingly research

00:00:54.709 --> 00:00:54.719
domains do read increasingly research
 

00:00:54.719 --> 00:00:56.840
domains do read increasingly research
papers from other application domains to

00:00:56.840 --> 00:00:56.850
papers from other application domains to
 

00:00:56.850 --> 00:00:58.760
papers from other application domains to
look for inspiration for cross

00:00:58.760 --> 00:00:58.770
look for inspiration for cross
 

00:00:58.770 --> 00:01:01.279
look for inspiration for cross
fertilization in terms of your setting

00:01:01.279 --> 00:01:01.289
fertilization in terms of your setting
 

00:01:01.289 --> 00:01:03.020
fertilization in terms of your setting
for the hyper parameters though I've

00:01:03.020 --> 00:01:03.030
for the hyper parameters though I've
 

00:01:03.030 --> 00:01:06.890
for the hyper parameters though I've
seen that intuitions do get fail so even

00:01:06.890 --> 00:01:06.900
seen that intuitions do get fail so even
 

00:01:06.900 --> 00:01:08.570
seen that intuitions do get fail so even
if you work on just one problem

00:01:08.570 --> 00:01:08.580
if you work on just one problem
 

00:01:08.580 --> 00:01:11.120
if you work on just one problem
say logistics you might have found a

00:01:11.120 --> 00:01:11.130
say logistics you might have found a
 

00:01:11.130 --> 00:01:12.560
say logistics you might have found a
good setting for the hyper parameters

00:01:12.560 --> 00:01:12.570
good setting for the hyper parameters
 

00:01:12.570 --> 00:01:15.950
good setting for the hyper parameters
and kept on developing your algorithm or

00:01:15.950 --> 00:01:15.960
and kept on developing your algorithm or
 

00:01:15.960 --> 00:01:17.840
and kept on developing your algorithm or
maybe seen your data

00:01:17.840 --> 00:01:17.850
maybe seen your data
 

00:01:17.850 --> 00:01:19.700
maybe seen your data
gradually change over the course of

00:01:19.700 --> 00:01:19.710
gradually change over the course of
 

00:01:19.710 --> 00:01:22.249
gradually change over the course of
several months or maybe just upgraded

00:01:22.249 --> 00:01:22.259
several months or maybe just upgraded
 

00:01:22.259 --> 00:01:24.530
several months or maybe just upgraded
the servers in your data center and

00:01:24.530 --> 00:01:24.540
the servers in your data center and
 

00:01:24.540 --> 00:01:26.600
the servers in your data center and
because of those changes the best

00:01:26.600 --> 00:01:26.610
because of those changes the best
 

00:01:26.610 --> 00:01:28.760
because of those changes the best
setting of your hyper parameters can get

00:01:28.760 --> 00:01:28.770
setting of your hyper parameters can get
 

00:01:28.770 --> 00:01:31.219
setting of your hyper parameters can get
stale so I recommend maybe just re

00:01:31.219 --> 00:01:31.229
stale so I recommend maybe just re
 

00:01:31.229 --> 00:01:33.560
stale so I recommend maybe just re
testing or reevaluating your hyper

00:01:33.560 --> 00:01:33.570
testing or reevaluating your hyper
 

00:01:33.570 --> 00:01:35.390
testing or reevaluating your hyper
parameters at least once every several

00:01:35.390 --> 00:01:35.400
parameters at least once every several
 

00:01:35.400 --> 00:01:36.679
parameters at least once every several
months to make sure that you're still

00:01:36.679 --> 00:01:36.689
months to make sure that you're still
 

00:01:36.689 --> 00:01:39.770
months to make sure that you're still
happy with the values you have finally

00:01:39.770 --> 00:01:39.780
happy with the values you have finally
 

00:01:39.780 --> 00:01:41.210
happy with the values you have finally
in terms of how people go about

00:01:41.210 --> 00:01:41.220
in terms of how people go about
 

00:01:41.220 --> 00:01:43.399
in terms of how people go about
searching for hyper parameters I see

00:01:43.399 --> 00:01:43.409
searching for hyper parameters I see
 

00:01:43.409 --> 00:01:46.190
searching for hyper parameters I see
maybe two major schools of thought or

00:01:46.190 --> 00:01:46.200
maybe two major schools of thought or
 

00:01:46.200 --> 00:01:48.649
maybe two major schools of thought or
maybe two major different ways in which

00:01:48.649 --> 00:01:48.659
maybe two major different ways in which
 

00:01:48.659 --> 00:01:51.170
maybe two major different ways in which
people go about it one way is if you

00:01:51.170 --> 00:01:51.180
people go about it one way is if you
 

00:01:51.180 --> 00:01:53.539
people go about it one way is if you
babysit one model and usually you do

00:01:53.539 --> 00:01:53.549
babysit one model and usually you do
 

00:01:53.549 --> 00:01:56.179
babysit one model and usually you do
this if you have maybe a huge data set

00:01:56.179 --> 00:01:56.189
this if you have maybe a huge data set
 

00:01:56.189 --> 00:01:58.459
this if you have maybe a huge data set
but not a lot of computational resources

00:01:58.459 --> 00:01:58.469
but not a lot of computational resources
 

00:01:58.469 --> 00:02:00.679
but not a lot of computational resources
not allow CPUs and GPUs so you can

00:02:00.679 --> 00:02:00.689
not allow CPUs and GPUs so you can
 

00:02:00.689 --> 00:02:02.600
not allow CPUs and GPUs so you can
basically afford to Train only one model

00:02:02.600 --> 00:02:02.610
basically afford to Train only one model
 

00:02:02.610 --> 00:02:04.130
basically afford to Train only one model
or a very small number of bottles at a

00:02:04.130 --> 00:02:04.140
or a very small number of bottles at a
 

00:02:04.140 --> 00:02:07.630
or a very small number of bottles at a
time in that case you might gradually

00:02:07.630 --> 00:02:07.640
time in that case you might gradually
 

00:02:07.640 --> 00:02:10.339
time in that case you might gradually
babysit that model even as its trained

00:02:10.339 --> 00:02:10.349
babysit that model even as its trained
 

00:02:10.349 --> 00:02:13.459
babysit that model even as its trained
so for example on day zero you might

00:02:13.459 --> 00:02:13.469
so for example on day zero you might
 

00:02:13.469 --> 00:02:15.199
so for example on day zero you might
initialize the parameters random and

00:02:15.199 --> 00:02:15.209
initialize the parameters random and
 

00:02:15.209 --> 00:02:17.000
initialize the parameters random and
then start training and you gradually

00:02:17.000 --> 00:02:17.010
then start training and you gradually
 

00:02:17.010 --> 00:02:20.300
then start training and you gradually
watch you know your learning curve may

00:02:20.300 --> 00:02:20.310
watch you know your learning curve may
 

00:02:20.310 --> 00:02:22.729
watch you know your learning curve may
be the cost function J or your death set

00:02:22.729 --> 00:02:22.739
be the cost function J or your death set
 

00:02:22.739 --> 00:02:24.830
be the cost function J or your death set
error or something else gradually

00:02:24.830 --> 00:02:24.840
error or something else gradually
 

00:02:24.840 --> 00:02:27.979
error or something else gradually
decrease over the first day then at the

00:02:27.979 --> 00:02:27.989
decrease over the first day then at the
 

00:02:27.989 --> 00:02:30.199
decrease over the first day then at the
end of day one you might say gee looks

00:02:30.199 --> 00:02:30.209
end of day one you might say gee looks
 

00:02:30.209 --> 00:02:31.849
end of day one you might say gee looks
looks learn quite well I'm going to try

00:02:31.849 --> 00:02:31.859
looks learn quite well I'm going to try
 

00:02:31.859 --> 00:02:33.860
looks learn quite well I'm going to try
increasing the learning rate robits and

00:02:33.860 --> 00:02:33.870
increasing the learning rate robits and
 

00:02:33.870 --> 00:02:35.959
increasing the learning rate robits and
see how it does and then maybe does

00:02:35.959 --> 00:02:35.969
see how it does and then maybe does
 

00:02:35.969 --> 00:02:38.000
see how it does and then maybe does
better and then bless your day to

00:02:38.000 --> 00:02:38.010
better and then bless your day to
 

00:02:38.010 --> 00:02:40.670
better and then bless your day to
performance and after 2 days you say ok

00:02:40.670 --> 00:02:40.680
performance and after 2 days you say ok
 

00:02:40.680 --> 00:02:42.709
performance and after 2 days you say ok
still doing quite well maybe after the

00:02:42.709 --> 00:02:42.719
still doing quite well maybe after the
 

00:02:42.719 --> 00:02:44.569
still doing quite well maybe after the
momentum term a bit or decrease of

00:02:44.569 --> 00:02:44.579
momentum term a bit or decrease of
 

00:02:44.579 --> 00:02:46.490
momentum term a bit or decrease of
learning regular bit now and then you

00:02:46.490 --> 00:02:46.500
learning regular bit now and then you
 

00:02:46.500 --> 00:02:48.619
learning regular bit now and then you
know in two days three and every day you

00:02:48.619 --> 00:02:48.629
know in two days three and every day you
 

00:02:48.629 --> 00:02:50.929
know in two days three and every day you
kind of look at it and you know try

00:02:50.929 --> 00:02:50.939
kind of look at it and you know try
 

00:02:50.939 --> 00:02:52.520
kind of look at it and you know try
edging up and down your parameters and

00:02:52.520 --> 00:02:52.530
edging up and down your parameters and
 

00:02:52.530 --> 00:02:54.679
edging up and down your parameters and
maybe on one day you found your learning

00:02:54.679 --> 00:02:54.689
maybe on one day you found your learning
 

00:02:54.689 --> 00:02:56.330
maybe on one day you found your learning
rate was too big so you might go back to

00:02:56.330 --> 00:02:56.340
rate was too big so you might go back to
 

00:02:56.340 --> 00:02:58.759
rate was too big so you might go back to
the previous days model and so on you're

00:02:58.759 --> 00:02:58.769
the previous days model and so on you're
 

00:02:58.769 --> 00:03:02.449
the previous days model and so on you're
kind of babysitting the model one day at

00:03:02.449 --> 00:03:02.459
kind of babysitting the model one day at
 

00:03:02.459 --> 00:03:04.550
kind of babysitting the model one day at
a time even as a training over a course

00:03:04.550 --> 00:03:04.560
a time even as a training over a course
 

00:03:04.560 --> 00:03:06.679
a time even as a training over a course
of many days or over the course of

00:03:06.679 --> 00:03:06.689
of many days or over the course of
 

00:03:06.689 --> 00:03:08.750
of many days or over the course of
several different weeks so that's one

00:03:08.750 --> 00:03:08.760
several different weeks so that's one
 

00:03:08.760 --> 00:03:11.330
several different weeks so that's one
approach and people that babysit one

00:03:11.330 --> 00:03:11.340
approach and people that babysit one
 

00:03:11.340 --> 00:03:13.580
approach and people that babysit one
model that is watching a performance and

00:03:13.580 --> 00:03:13.590
model that is watching a performance and
 

00:03:13.590 --> 00:03:16.039
model that is watching a performance and
you know patiently nudging the learning

00:03:16.039 --> 00:03:16.049
you know patiently nudging the learning
 

00:03:16.049 --> 00:03:18.050
you know patiently nudging the learning
rate up or down that's usually what

00:03:18.050 --> 00:03:18.060
rate up or down that's usually what
 

00:03:18.060 --> 00:03:19.490
rate up or down that's usually what
happens if you don't have enough

00:03:19.490 --> 00:03:19.500
happens if you don't have enough
 

00:03:19.500 --> 00:03:22.520
happens if you don't have enough
computational capacity to train a lot of

00:03:22.520 --> 00:03:22.530
computational capacity to train a lot of
 

00:03:22.530 --> 00:03:24.379
computational capacity to train a lot of
models at the same time the other

00:03:24.379 --> 00:03:24.389
models at the same time the other
 

00:03:24.389 --> 00:03:26.599
models at the same time the other
approach would be if you train many

00:03:26.599 --> 00:03:26.609
approach would be if you train many
 

00:03:26.609 --> 00:03:30.140
approach would be if you train many
models in parallel so you might have

00:03:30.140 --> 00:03:30.150
models in parallel so you might have
 

00:03:30.150 --> 00:03:31.909
models in parallel so you might have
some setting of the hyper browsers and

00:03:31.909 --> 00:03:31.919
some setting of the hyper browsers and
 

00:03:31.919 --> 00:03:34.550
some setting of the hyper browsers and
just let it run by itself either for a

00:03:34.550 --> 00:03:34.560
just let it run by itself either for a
 

00:03:34.560 --> 00:03:36.140
just let it run by itself either for a
day or even for multiple days and then

00:03:36.140 --> 00:03:36.150
day or even for multiple days and then
 

00:03:36.150 --> 00:03:37.640
day or even for multiple days and then
give some learning curve like that and

00:03:37.640 --> 00:03:37.650
give some learning curve like that and
 

00:03:37.650 --> 00:03:39.740
give some learning curve like that and
this could be a plot of the cost

00:03:39.740 --> 00:03:39.750
this could be a plot of the cost
 

00:03:39.750 --> 00:03:41.629
this could be a plot of the cost
function J or cost of your training

00:03:41.629 --> 00:03:41.639
function J or cost of your training
 

00:03:41.639 --> 00:03:43.610
function J or cost of your training
error or cause you're just an error but

00:03:43.610 --> 00:03:43.620
error or cause you're just an error but
 

00:03:43.620 --> 00:03:45.800
error or cause you're just an error but
some measures in your tracking and then

00:03:45.800 --> 00:03:45.810
some measures in your tracking and then
 

00:03:45.810 --> 00:03:47.270
some measures in your tracking and then
at the same time you might start up a

00:03:47.270 --> 00:03:47.280
at the same time you might start up a
 

00:03:47.280 --> 00:03:48.770
at the same time you might start up a
different model with a different setting

00:03:48.770 --> 00:03:48.780
different model with a different setting
 

00:03:48.780 --> 00:03:51.110
different model with a different setting
on the high preferences and so your

00:03:51.110 --> 00:03:51.120
on the high preferences and so your
 

00:03:51.120 --> 00:03:52.699
on the high preferences and so your
second model might generate a different

00:03:52.699 --> 00:03:52.709
second model might generate a different
 

00:03:52.709 --> 00:03:55.309
second model might generate a different
learning curve maybe one that looks like

00:03:55.309 --> 00:03:55.319
learning curve maybe one that looks like
 

00:03:55.319 --> 00:03:56.780
learning curve maybe one that looks like
that looks like that one looks better

00:03:56.780 --> 00:03:56.790
that looks like that one looks better
 

00:03:56.790 --> 00:03:58.789
that looks like that one looks better
and at the same time you might train a

00:03:58.789 --> 00:03:58.799
and at the same time you might train a
 

00:03:58.799 --> 00:04:00.679
and at the same time you might train a
third model which my genuine learning

00:04:00.679 --> 00:04:00.689
third model which my genuine learning
 

00:04:00.689 --> 00:04:03.409
third model which my genuine learning
curve does all that and another one that

00:04:03.409 --> 00:04:03.419
curve does all that and another one that
 

00:04:03.419 --> 00:04:05.569
curve does all that and another one that
maybe this one diverges that looks like

00:04:05.569 --> 00:04:05.579
maybe this one diverges that looks like
 

00:04:05.579 --> 00:04:08.659
maybe this one diverges that looks like
that and so on but you might train many

00:04:08.659 --> 00:04:08.669
that and so on but you might train many
 

00:04:08.669 --> 00:04:10.610
that and so on but you might train many
different models in parallel where these

00:04:10.610 --> 00:04:10.620
different models in parallel where these
 

00:04:10.620 --> 00:04:12.979
different models in parallel where these
orange lines are different models right

00:04:12.979 --> 00:04:12.989
orange lines are different models right
 

00:04:12.989 --> 00:04:14.719
orange lines are different models right
then so this way you can try a lot of

00:04:14.719 --> 00:04:14.729
then so this way you can try a lot of
 

00:04:14.729 --> 00:04:16.550
then so this way you can try a lot of
different hyper parameter settings and

00:04:16.550 --> 00:04:16.560
different hyper parameter settings and
 

00:04:16.560 --> 00:04:18.890
different hyper parameter settings and
then just maybe quickly at the end

00:04:18.890 --> 00:04:18.900
then just maybe quickly at the end
 

00:04:18.900 --> 00:04:21.319
then just maybe quickly at the end
pick the one that works best look like

00:04:21.319 --> 00:04:21.329
pick the one that works best look like
 

00:04:21.329 --> 00:04:22.890
pick the one that works best look like
in this example it was

00:04:22.890 --> 00:04:22.900
in this example it was
 

00:04:22.900 --> 00:04:25.740
in this example it was
maybe describe their little pest so to

00:04:25.740 --> 00:04:25.750
maybe describe their little pest so to
 

00:04:25.750 --> 00:04:27.990
maybe describe their little pest so to
make an analogy I'm going to call the

00:04:27.990 --> 00:04:28.000
make an analogy I'm going to call the
 

00:04:28.000 --> 00:04:30.420
make an analogy I'm going to call the
approach on the left the pander approach

00:04:30.420 --> 00:04:30.430
approach on the left the pander approach
 

00:04:30.430 --> 00:04:32.460
approach on the left the pander approach
you know when pandas have children they

00:04:32.460 --> 00:04:32.470
you know when pandas have children they
 

00:04:32.470 --> 00:04:35.129
you know when pandas have children they
have very few children usually one child

00:04:35.129 --> 00:04:35.139
have very few children usually one child
 

00:04:35.139 --> 00:04:36.840
have very few children usually one child
at a time and then they really put a lot

00:04:36.840 --> 00:04:36.850
at a time and then they really put a lot
 

00:04:36.850 --> 00:04:38.550
at a time and then they really put a lot
of effort into making sure that the baby

00:04:38.550 --> 00:04:38.560
of effort into making sure that the baby
 

00:04:38.560 --> 00:04:40.800
of effort into making sure that the baby
pandas device so that's really

00:04:40.800 --> 00:04:40.810
pandas device so that's really
 

00:04:40.810 --> 00:04:42.900
pandas device so that's really
babysitting you know one model or one

00:04:42.900 --> 00:04:42.910
babysitting you know one model or one
 

00:04:42.910 --> 00:04:45.390
babysitting you know one model or one
baby panda words the personal rate is

00:04:45.390 --> 00:04:45.400
baby panda words the personal rate is
 

00:04:45.400 --> 00:04:48.180
baby panda words the personal rate is
more like what fish do and commonly

00:04:48.180 --> 00:04:48.190
more like what fish do and commonly
 

00:04:48.190 --> 00:04:50.430
more like what fish do and commonly
called it the caviar strategy there's

00:04:50.430 --> 00:04:50.440
called it the caviar strategy there's
 

00:04:50.440 --> 00:04:52.439
called it the caviar strategy there's
some fish that lay over a hundred

00:04:52.439 --> 00:04:52.449
some fish that lay over a hundred
 

00:04:52.449 --> 00:04:54.930
some fish that lay over a hundred
million eggs in one season in one mating

00:04:54.930 --> 00:04:54.940
million eggs in one season in one mating
 

00:04:54.940 --> 00:04:57.450
million eggs in one season in one mating
season but the way fish reproduces they

00:04:57.450 --> 00:04:57.460
season but the way fish reproduces they
 

00:04:57.460 --> 00:04:59.460
season but the way fish reproduces they
lay a lot of eggs and don't pay too much

00:04:59.460 --> 00:04:59.470
lay a lot of eggs and don't pay too much
 

00:04:59.470 --> 00:05:01.439
lay a lot of eggs and don't pay too much
attention to any one of them but you

00:05:01.439 --> 00:05:01.449
attention to any one of them but you
 

00:05:01.449 --> 00:05:03.330
attention to any one of them but you
know just see that hopefully one of them

00:05:03.330 --> 00:05:03.340
know just see that hopefully one of them
 

00:05:03.340 --> 00:05:06.029
know just see that hopefully one of them
or maybe a bunch of them will do well so

00:05:06.029 --> 00:05:06.039
or maybe a bunch of them will do well so
 

00:05:06.039 --> 00:05:08.129
or maybe a bunch of them will do well so
I guess this is really the difference

00:05:08.129 --> 00:05:08.139
I guess this is really the difference
 

00:05:08.139 --> 00:05:11.370
I guess this is really the difference
between how mammals reproduce versus how

00:05:11.370 --> 00:05:11.380
between how mammals reproduce versus how
 

00:05:11.380 --> 00:05:15.000
between how mammals reproduce versus how
fish and a lot of reptiles reproduce but

00:05:15.000 --> 00:05:15.010
fish and a lot of reptiles reproduce but
 

00:05:15.010 --> 00:05:16.500
fish and a lot of reptiles reproduce but
I'm going to call it the pander approach

00:05:16.500 --> 00:05:16.510
I'm going to call it the pander approach
 

00:05:16.510 --> 00:05:18.300
I'm going to call it the pander approach
versus the caviar approach since there's

00:05:18.300 --> 00:05:18.310
versus the caviar approach since there's
 

00:05:18.310 --> 00:05:20.520
versus the caviar approach since there's
more fun and memorable so the way to

00:05:20.520 --> 00:05:20.530
more fun and memorable so the way to
 

00:05:20.530 --> 00:05:22.320
more fun and memorable so the way to
choose between these two approaches is

00:05:22.320 --> 00:05:22.330
choose between these two approaches is
 

00:05:22.330 --> 00:05:24.029
choose between these two approaches is
really a function of how much

00:05:24.029 --> 00:05:24.039
really a function of how much
 

00:05:24.039 --> 00:05:26.670
really a function of how much
computational resources you have if you

00:05:26.670 --> 00:05:26.680
computational resources you have if you
 

00:05:26.680 --> 00:05:29.159
computational resources you have if you
have enough computers they train a lot

00:05:29.159 --> 00:05:29.169
have enough computers they train a lot
 

00:05:29.169 --> 00:05:32.400
have enough computers they train a lot
of models in parallel then by all means

00:05:32.400 --> 00:05:32.410
of models in parallel then by all means
 

00:05:32.410 --> 00:05:35.129
of models in parallel then by all means
take the caviar approach and try a lot

00:05:35.129 --> 00:05:35.139
take the caviar approach and try a lot
 

00:05:35.139 --> 00:05:36.629
take the caviar approach and try a lot
of different Hydra parameters and see

00:05:36.629 --> 00:05:36.639
of different Hydra parameters and see
 

00:05:36.639 --> 00:05:38.760
of different Hydra parameters and see
what worlds but in some application

00:05:38.760 --> 00:05:38.770
what worlds but in some application
 

00:05:38.770 --> 00:05:41.010
what worlds but in some application
domains I see this in some online

00:05:41.010 --> 00:05:41.020
domains I see this in some online
 

00:05:41.020 --> 00:05:43.260
domains I see this in some online
advertising settings as well as in some

00:05:43.260 --> 00:05:43.270
advertising settings as well as in some
 

00:05:43.270 --> 00:05:45.240
advertising settings as well as in some
computer vision applications where

00:05:45.240 --> 00:05:45.250
computer vision applications where
 

00:05:45.250 --> 00:05:47.760
computer vision applications where
there's just so much data and the models

00:05:47.760 --> 00:05:47.770
there's just so much data and the models
 

00:05:47.770 --> 00:05:49.080
there's just so much data and the models
you want to train are so big that's

00:05:49.080 --> 00:05:49.090
you want to train are so big that's
 

00:05:49.090 --> 00:05:51.510
you want to train are so big that's
difficult to train a lot of models at

00:05:51.510 --> 00:05:51.520
difficult to train a lot of models at
 

00:05:51.520 --> 00:05:53.909
difficult to train a lot of models at
the same time it's really application

00:05:53.909 --> 00:05:53.919
the same time it's really application
 

00:05:53.919 --> 00:05:55.710
the same time it's really application
dependent of course that are but I've

00:05:55.710 --> 00:05:55.720
dependent of course that are but I've
 

00:05:55.720 --> 00:05:58.620
dependent of course that are but I've
seen those communities use the Pandora

00:05:58.620 --> 00:05:58.630
seen those communities use the Pandora
 

00:05:58.630 --> 00:06:00.360
seen those communities use the Pandora
approach a little bit more where you are

00:06:00.360 --> 00:06:00.370
approach a little bit more where you are
 

00:06:00.370 --> 00:06:03.060
approach a little bit more where you are
kind of babying a single model long and

00:06:03.060 --> 00:06:03.070
kind of babying a single model long and
 

00:06:03.070 --> 00:06:04.800
kind of babying a single model long and
nudging the parameters up and down and

00:06:04.800 --> 00:06:04.810
nudging the parameters up and down and
 

00:06:04.810 --> 00:06:07.140
nudging the parameters up and down and
trying to make this one model you know

00:06:07.140 --> 00:06:07.150
trying to make this one model you know
 

00:06:07.150 --> 00:06:09.270
trying to make this one model you know
work although of course you know the

00:06:09.270 --> 00:06:09.280
work although of course you know the
 

00:06:09.280 --> 00:06:11.580
work although of course you know the
Pandora approach having trained one

00:06:11.580 --> 00:06:11.590
Pandora approach having trained one
 

00:06:11.590 --> 00:06:13.080
Pandora approach having trained one
model and seeing it work or not work

00:06:13.080 --> 00:06:13.090
model and seeing it work or not work
 

00:06:13.090 --> 00:06:15.180
model and seeing it work or not work
maybe in the second week of the third

00:06:15.180 --> 00:06:15.190
maybe in the second week of the third
 

00:06:15.190 --> 00:06:16.589
maybe in the second week of the third
week maybe you actually initialize a

00:06:16.589 --> 00:06:16.599
week maybe you actually initialize a
 

00:06:16.599 --> 00:06:18.839
week maybe you actually initialize a
different model and then you know baby

00:06:18.839 --> 00:06:18.849
different model and then you know baby
 

00:06:18.849 --> 00:06:21.570
different model and then you know baby
that went along just like even pandas I

00:06:21.570 --> 00:06:21.580
that went along just like even pandas I
 

00:06:21.580 --> 00:06:23.100
that went along just like even pandas I
guess can have multiple children in

00:06:23.100 --> 00:06:23.110
guess can have multiple children in
 

00:06:23.110 --> 00:06:24.779
guess can have multiple children in
their lifetime even if they have only

00:06:24.779 --> 00:06:24.789
their lifetime even if they have only
 

00:06:24.789 --> 00:06:26.969
their lifetime even if they have only
one or a very small number of children

00:06:26.969 --> 00:06:26.979
one or a very small number of children
 

00:06:26.979 --> 00:06:29.670
one or a very small number of children
at any one time so hopefully this gives

00:06:29.670 --> 00:06:29.680
at any one time so hopefully this gives
 

00:06:29.680 --> 00:06:31.350
at any one time so hopefully this gives
you a good sense of how to go about the

00:06:31.350 --> 00:06:31.360
you a good sense of how to go about the
 

00:06:31.360 --> 00:06:33.460
you a good sense of how to go about the
hyper parameter search process

00:06:33.460 --> 00:06:33.470
hyper parameter search process
 

00:06:33.470 --> 00:06:36.010
hyper parameter search process
now it turns out that there's one other

00:06:36.010 --> 00:06:36.020
now it turns out that there's one other
 

00:06:36.020 --> 00:06:37.750
now it turns out that there's one other
technique that can make your neural

00:06:37.750 --> 00:06:37.760
technique that can make your neural
 

00:06:37.760 --> 00:06:39.910
technique that can make your neural
network much more robust to the choice

00:06:39.910 --> 00:06:39.920
network much more robust to the choice
 

00:06:39.920 --> 00:06:41.080
network much more robust to the choice
of hyper parameters

00:06:41.080 --> 00:06:41.090
of hyper parameters
 

00:06:41.090 --> 00:06:43.000
of hyper parameters
doesn't work for all neural networks but

00:06:43.000 --> 00:06:43.010
doesn't work for all neural networks but
 

00:06:43.010 --> 00:06:44.500
doesn't work for all neural networks but
when it does it can make the hyper

00:06:44.500 --> 00:06:44.510
when it does it can make the hyper
 

00:06:44.510 --> 00:06:46.600
when it does it can make the hyper
parent to search much easier and also

00:06:46.600 --> 00:06:46.610
parent to search much easier and also
 

00:06:46.610 --> 00:06:48.880
parent to search much easier and also
make training go much faster let's talk

00:06:48.880 --> 00:06:48.890
make training go much faster let's talk
 

00:06:48.890 --> 00:06:52.690
make training go much faster let's talk
about this technique in the next video

