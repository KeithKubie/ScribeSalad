WEBVTT
Kind: captions
Language: en-GB

00:00:00.000 --> 00:00:03.520
(The Alpha Go has been using Deep Learning to make this happen then?)

00:00:03.880 --> 00:00:04.800
Yes, yes.

00:00:04.800 --> 00:00:08.520
That's... I mean the company that did that, well, now is Google, right?

00:00:08.520 --> 00:00:13.900
But it's Deep Mind company that was a startup in London and Google acquired them at some point,

00:00:13.900 --> 00:00:16.200
before actually anyone knew what they were doing, right?

00:00:16.200 --> 00:00:18.680
So they were smelling that it was something nice, right?

00:00:19.100 --> 00:00:21.340
And, um, their, um -

00:00:21.340 --> 00:00:23.620
Their best thing is that they're doing Deep Learning

00:00:23.620 --> 00:00:26.080
with Reinforcement Learning.  Right?

00:00:26.080 --> 00:00:28.820
So Reinforcement Learning is the type

00:00:28.820 --> 00:00:31.360
of way you try to teach,

00:00:31.360 --> 00:00:33.360
and Deep Learning is the type of algorithm

00:00:33.360 --> 00:00:35.360
that you use to try to learn that.

00:00:35.360 --> 00:00:37.040
Right?  So -

00:00:37.040 --> 00:00:39.360
Reinforcement Learning is exactly this thing of,

00:00:39.360 --> 00:00:41.360
I'm not gonna tell you exactly

00:00:41.360 --> 00:00:43.020
what the output is.

00:00:43.020 --> 00:00:45.080
I'm just gonna tell you, What is the task,

00:00:45.080 --> 00:00:47.660
and I'm gonna tell you whether you're doing the task

00:00:47.660 --> 00:00:50.880
well or bad.  And it's gonna be your, uh,

00:00:50.880 --> 00:00:54.800
your job, as an algorithm, to find the best way of doing this task.

00:00:58.420 --> 00:01:01.120
The first idea that we have to understand is, what -

00:01:01.440 --> 00:01:04.820
what we're trying to do with it, right, which is basically machine learning.

00:01:04.820 --> 00:01:06.080
It's, ah -

00:01:06.340 --> 00:01:09.580
It's trying to automatically

00:01:09.580 --> 00:01:12.280
do some task that -

00:01:12.280 --> 00:01:14.860
Well, we can do it by counting, usually, right?

00:01:14.860 --> 00:01:16.860
We can do, basically, statistics.

00:01:16.860 --> 00:01:19.680
But if we have really a lot of numbers,

00:01:19.680 --> 00:01:21.400
we can end up

00:01:21.400 --> 00:01:23.400
doing some heuristics,

00:01:23.400 --> 00:01:25.140
what we can come up with.

00:01:25.140 --> 00:01:27.300
Then we might want to use computers

00:01:27.300 --> 00:01:29.300
to actually learn

00:01:29.300 --> 00:01:31.300
from these statistics in an optimum manner -

00:01:31.300 --> 00:01:33.300
Say, that the best way of

00:01:33.300 --> 00:01:35.300
finding this information out of

00:01:35.300 --> 00:01:38.300
these data that you have is following these rules, right?

00:01:38.300 --> 00:01:39.900
So the computer does this for you.

00:01:39.900 --> 00:01:42.660
This is the basic concept of machine learning.

00:01:42.660 --> 00:01:46.200
One typical example is if you want to invest in, ah -

00:01:47.180 --> 00:01:49.180
either an exchange market, in the stock market -

00:01:49.180 --> 00:01:51.180
something like that; and, uh,

00:01:51.180 --> 00:01:53.180
then, I guess the people that invest, they have

00:01:53.180 --> 00:01:55.420
certain rules.  Right?  If you have

00:01:55.420 --> 00:01:58.460
the last five weeks going up in value,

00:01:58.460 --> 00:02:01.320
then, overall, you expect that

00:02:01.320 --> 00:02:03.580
this week will go down, because it's being,

00:02:03.580 --> 00:02:05.580
I don't know, like reaching a maximum;

00:02:05.580 --> 00:02:07.580
or this kind of idea, right?

00:02:07.580 --> 00:02:09.580
That, uh, you compute the

00:02:09.580 --> 00:02:11.580
median of the last ten values, and

00:02:11.580 --> 00:02:13.580
this gives you an idea of the *real* value,

00:02:13.580 --> 00:02:15.580
and if it's above that, it will go down;

00:02:15.580 --> 00:02:17.860
and so forth.  Right?  But this is just basically

00:02:17.860 --> 00:02:19.860
heuristics that we *think* might work, right?

00:02:19.860 --> 00:02:21.860
Then we can even, through experience,

00:02:21.860 --> 00:02:23.860
think that some are better than others,

00:02:23.860 --> 00:02:25.860
and so forth, right?  But when we have, like,

00:02:25.860 --> 00:02:29.260
50 values on the - of different stocks in the market,

00:02:29.260 --> 00:02:31.260
and we have, you know, like

00:02:31.260 --> 00:02:33.500
the currency exchange,

00:02:33.500 --> 00:02:35.500
and we have many other factors, right?

00:02:35.500 --> 00:02:37.840
How to - How many rules can we find there?

00:02:37.840 --> 00:02:39.840
How many heuristics?  Right?  This is just, almost

00:02:39.840 --> 00:02:41.840
infinite number.  So, uh -

00:02:41.840 --> 00:02:43.840
What you would like is,

00:02:43.840 --> 00:02:45.840
ideally, a way of choosing the best.

00:02:45.840 --> 00:02:51.480
And the computer has a capacity of checking them all.

00:02:51.480 --> 00:02:53.480
Right?  You can do that, pen and paper.

00:02:53.480 --> 00:02:56.120
You'd spend, like, 5 lifetimes doing it.

00:02:56.120 --> 00:02:58.120
And, maybe, you know - by the time

00:02:58.120 --> 00:02:59.960
you finish, there's no stock market.

00:02:59.960 --> 00:03:01.580
But the computer does it, everything for you,

00:03:01.580 --> 00:03:03.580
very quickly.  Right?

00:03:03.580 --> 00:03:05.580
The only thing is, it will not do something better

00:03:05.580 --> 00:03:07.580
than you can do, given enough time;

00:03:07.580 --> 00:03:09.580
but it does it so much quicker

00:03:09.580 --> 00:03:11.580
that you will never get there, right?

00:03:11.580 --> 00:03:13.580
So now we begin to see cases in which

00:03:13.580 --> 00:03:15.580
machine learning is actually doing

00:03:15.580 --> 00:03:17.580
something better than humans, especially

00:03:17.580 --> 00:03:19.580
with very specific tasks

00:03:19.580 --> 00:03:21.580
and maybe

00:03:21.580 --> 00:03:24.840
better than humans that are not experts

00:03:24.840 --> 00:03:26.840
on that task.  Right?

00:03:26.840 --> 00:03:28.840
But, historically,

00:03:28.840 --> 00:03:31.760
machine learning was always worse than

00:03:31.760 --> 00:03:34.560
an expert human doing something.  Right?

00:03:35.560 --> 00:03:37.560
So, one of the cases is this,

00:03:37.560 --> 00:03:39.560
like, now-famous AlphaGo.  Right?

00:03:40.400 --> 00:03:42.400
So, this is one of the few cases

00:03:42.400 --> 00:03:45.020
in which machine learning has managed to beat -

00:03:45.780 --> 00:03:47.780
to beat an expert.  I mean, like

00:03:47.780 --> 00:03:51.700
the world expert on one specific problem.  Right?

00:03:51.700 --> 00:03:53.700
And there's been this - all this

00:03:53.700 --> 00:03:56.160
talking about why this is different from chess, right?

00:03:56.160 --> 00:03:58.160
The main difference is that, for chess,

00:03:58.160 --> 00:04:00.940
you have a limited number of options, right?

00:04:00.940 --> 00:04:03.400
And, once you move one piece,

00:04:03.400 --> 00:04:05.860
then you have, I don't know how many options, right -

00:04:05.860 --> 00:04:07.260
but again, a limited number.

00:04:07.260 --> 00:04:09.400
So, you can organize that in a tree,

00:04:09.400 --> 00:04:12.080
and, more or less, check exhaustively.

00:04:12.080 --> 00:04:14.600
You tell the program, This is what you want to do.

00:04:14.600 --> 00:04:16.600
Check all the options, and just

00:04:16.600 --> 00:04:18.500
do what is best according to

00:04:18.500 --> 00:04:20.700
what I told you that is good.  Right?

00:04:21.840 --> 00:04:24.880
So, this is just a sheer competition of power

00:04:24.880 --> 00:04:27.100
in order to check the options.  Right?

00:04:27.100 --> 00:04:30.220
This tells you, Our computers are really powerful.

00:04:30.220 --> 00:04:33.380
Right?  But it doesn't say anything about the algorithm itself.

00:04:33.640 --> 00:04:36.020
Um, with Go, it's totally different

00:04:36.020 --> 00:04:38.020
because there are so many options, that you cannot

00:04:38.020 --> 00:04:39.940
really check them.  Right?

00:04:39.940 --> 00:04:41.940
A lot of people are repeating this fact, that there's

00:04:41.940 --> 00:04:44.460
more Go positions than particles in the universe.

00:04:44.700 --> 00:04:46.700
The people in Deep Mind, which is the people

00:04:46.700 --> 00:04:48.700
that did this, have said that

00:04:48.700 --> 00:04:50.700
they really don't know what kind of

00:04:50.700 --> 00:04:52.700
tactic the computer is following.  Right?

00:04:52.700 --> 00:04:54.960
They didn't really hard-code any tactic.

00:04:54.960 --> 00:04:56.960
So, what they did is, um,

00:04:56.960 --> 00:04:59.460
they created this machine learning algorithm

00:04:59.460 --> 00:05:01.940
that would play Go,

00:05:01.940 --> 00:05:04.880
right, and would play terribly.

00:05:04.880 --> 00:05:08.380
It's just, you know, some random rules.

00:05:08.380 --> 00:05:10.760
They didn't really have a clue how to implement that,

00:05:10.760 --> 00:05:12.760
so they started at some random

00:05:12.760 --> 00:05:15.620
algorithm that would play Go terribly.

00:05:15.620 --> 00:05:17.620
And the point is that they

00:05:17.620 --> 00:05:19.620
pitted this, uh, this -

00:05:19.620 --> 00:05:21.620
this machine, this algorithm,

00:05:21.620 --> 00:05:23.280
against some other algorithm that would have

00:05:23.280 --> 00:05:25.460
different parameters.  Right?  Would play Go differently.

00:05:26.120 --> 00:05:28.120
But you still wouldn't hard-code anything.

00:05:28.120 --> 00:05:29.920
You just would use some

00:05:29.920 --> 00:05:31.780
parameters that would define how it would play,

00:05:31.780 --> 00:05:33.640
but you really don't know what the meaning of

00:05:33.640 --> 00:05:36.300
these parameters are.  Right?  And they would play against each other.

00:05:36.300 --> 00:05:38.520
And at some point, after, I don't know, 100 matches,

00:05:38.520 --> 00:05:40.080
one would win.  Right?

00:05:40.080 --> 00:05:41.700
And then you say, OK, this one is better.

00:05:41.700 --> 00:05:44.740
And then, you keep these parameters, you change them somehow,

00:05:44.740 --> 00:05:48.100
and you keep on doing that for a very, very long time.

00:05:48.100 --> 00:05:50.420
It's a bit like evolution.  There is a part of

00:05:50.420 --> 00:05:53.260
computer science that is called evolutionary computing.

00:05:53.260 --> 00:05:55.600
So I don't want to - this is -

00:05:55.600 --> 00:05:57.600
this is slightly different thing.  Right?

00:05:57.600 --> 00:05:59.600
But the concept is, you have

00:05:59.600 --> 00:06:01.240
a parameter space, right?

00:06:01.240 --> 00:06:04.160
And this will tell you all the possible ways

00:06:04.160 --> 00:06:06.480
to play Go, according to

00:06:06.480 --> 00:06:08.480
your certain machine learning algorithm.

00:06:08.480 --> 00:06:11.220
It will have certain possibilities

00:06:11.220 --> 00:06:13.220
that are specific to that algorithm.

00:06:13.220 --> 00:06:14.720
But, you have

00:06:14.720 --> 00:06:16.720
a lot of parameters, and each

00:06:16.720 --> 00:06:18.720
parameter can take a real value,

00:06:18.720 --> 00:06:20.720
so, you have infinte possibilities.

00:06:20.720 --> 00:06:23.160
And the question is how to find the best parameters

00:06:23.160 --> 00:06:25.800
to play Go.  Most machine learning algorithms

00:06:25.800 --> 00:06:28.140
they have, you search for the optimum parameters

00:06:28.140 --> 00:06:29.960
in some other way.

00:06:29.960 --> 00:06:31.960
This - This was a specific of a

00:06:31.960 --> 00:06:33.260
type of problems that are called

00:06:33.260 --> 00:06:35.260
Reinforcement Learning, where you -

00:06:35.260 --> 00:06:37.260
ah, you basically tell them

00:06:37.260 --> 00:06:39.500
what you want to do, but, ah,

00:06:39.500 --> 00:06:41.360
you are not sure what is the

00:06:41.360 --> 00:06:43.360
correct way of doing it.  Right?  And,

00:06:43.360 --> 00:06:45.780
that's why Deep Mind says,

00:06:45.780 --> 00:06:47.780
We don't really know.  Because they didn't

00:06:47.780 --> 00:06:50.300
give the examples for the machine to imitate.

00:06:50.300 --> 00:06:52.300
They said, OK, try to learn by yourself.  Right?

00:06:52.300 --> 00:06:54.300
This is what you want to do,

00:06:54.300 --> 00:06:57.020
what we want you to do.  Just learn how to do it.

00:06:57.020 --> 00:06:59.020
Every time that the machine learning

00:06:59.020 --> 00:07:00.880
decides a move, right?

00:07:00.880 --> 00:07:02.740
So, this is what the algorithm is learning.

00:07:02.740 --> 00:07:04.740
We have this configuration; what is my

00:07:04.740 --> 00:07:07.380
next move?  You are not specifying,

00:07:07.380 --> 00:07:09.380
"The best move is *this*" because no one knows.

00:07:09.380 --> 00:07:11.700
Right?  I mean, the idea is that they beat

00:07:11.700 --> 00:07:13.700
the world champion.  So, you know, maybe

00:07:13.700 --> 00:07:16.000
now, the algorithm was able

00:07:16.000 --> 00:07:18.340
to figure out "Which next  move is better?"

00:07:18.340 --> 00:07:21.320
better than the world champion.  Right?

00:07:21.320 --> 00:07:24.380
So, there's no [golden standard to imitate here.]  [unsure]  Right?

00:07:24.380 --> 00:07:27.700
That's the difference, in essence.

00:07:27.880 --> 00:07:29.880
(I suppose I don't really understand the game Go,)

00:07:29.880 --> 00:07:31.880
(so that doesn't help much.)  Yes.
(But I suppose, and the biggest thing)

00:07:31.880 --> 00:07:34.780
(here is, um, feel free to kind of correct me,)

00:07:34.780 --> 00:07:37.200
(but, in Chess, you can brute-force it.)

00:07:37.200 --> 00:07:40.020
Yeah.  Yeah.
(And in Go, you can't.)

00:07:40.020 --> 00:07:41.500
Yeah.
(Would that be fair?)

00:07:41.500 --> 00:07:43.120
That is - That is very fair.  Yeah.

00:07:43.120 --> 00:07:45.280
I mean, you still have to do it smartly,

00:07:45.280 --> 00:07:47.280
to brute-force it for Chess, because you

00:07:47.280 --> 00:07:49.280
have to find a way to tell the computer

00:07:49.280 --> 00:07:51.280
whether a move is a smart

00:07:51.280 --> 00:07:52.780
move or not, down the line.  Right?

00:07:52.780 --> 00:07:54.880
But you still would check all the consequences

00:07:54.880 --> 00:07:56.880
of your move, right?  But this is

00:07:56.880 --> 00:07:58.880
kind of the standard.  You put a bit

00:07:58.880 --> 00:08:01.620
of domain knowledge, and you put brute force.  Right?

00:08:01.620 --> 00:08:04.360
The Go is a totally different game.

00:08:05.160 --> 00:08:08.260
So, this is really fair assessment of it.

00:08:08.260 --> 00:08:11.760
(So, basically, there were two sets of algorithms)

00:08:11.760 --> 00:08:14.320
(working against each other; and every time)

00:08:14.320 --> 00:08:16.380
(one did a bit better than the other, they thought,)

00:08:16.380 --> 00:08:18.540
(Something about that is better.)
Yes.

00:08:18.540 --> 00:08:20.760
So, this, uh - There were two different

00:08:20.760 --> 00:08:23.120
set of parameters, right,

00:08:23.120 --> 00:08:25.320
of the same algorithm.  Right?  So the decisions

00:08:25.320 --> 00:08:27.320
that it would take would be different.

00:08:27.320 --> 00:08:29.820
The algorithm was the same.  It was a neural network.

00:08:29.820 --> 00:08:31.620
And, uh, basically,

00:08:31.620 --> 00:08:33.620
what you tell is, the input is,

00:08:33.620 --> 00:08:35.500
What is the configuration.  Right?

00:08:35.500 --> 00:08:37.700
And then you have a certain structure that will

00:08:37.700 --> 00:08:39.700
you know, like, decide on the move.  Right?

00:08:39.700 --> 00:08:42.140
But this structure is, uh -

00:08:42.140 --> 00:08:43.860
You need some parameters.

00:08:43.860 --> 00:08:47.140
That is what, in the end, decides.

00:08:47.140 --> 00:08:49.140
Right?  What - So -

00:08:49.140 --> 00:08:51.980
To see the difference, for example -

00:08:52.900 --> 00:08:55.700
You can - You can see that you have a function

00:08:55.700 --> 00:08:58.760
that will take the input and decide on the output.

00:08:58.760 --> 00:09:01.680
This function, you can decide that it's a linear function.

00:09:01.680 --> 00:09:04.700
Right?  So, it's just a line, or a plane.  Right?

00:09:04.700 --> 00:09:06.060
This, uh -

00:09:07.300 --> 00:09:10.820
The traditional, you know, like, regressing a line.  Right?

00:09:10.820 --> 00:09:13.140
And, um, this is,

00:09:13.140 --> 00:09:16.380
when you modify the parameters of the line,

00:09:16.380 --> 00:09:18.280
this is more or less the same.

00:09:18.280 --> 00:09:20.280
You change the slope of the line.  You change

00:09:20.280 --> 00:09:21.600
the height of the line.  Right?

00:09:21.600 --> 00:09:23.380
And this gives you different lines, right?

00:09:23.380 --> 00:09:25.380
So this is exactly the same, but

00:09:25.380 --> 00:09:29.180
instead of having a line, it was, like, much more complex a function.

00:09:29.180 --> 00:09:30.800
(So that would be in two dimensions.)

00:09:30.800 --> 00:09:32.800
(But there are just so many more dimensions in it.)

00:09:32.800 --> 00:09:34.240
Yeah, it was not only about the dimensions.

00:09:34.240 --> 00:09:36.240
It's also about the way that

00:09:36.240 --> 00:09:38.920
the parameters are structured.  Right?

00:09:38.920 --> 00:09:42.480
And this is why we use the word "deep" on it, right?

00:09:42.480 --> 00:09:44.980
So, a line is what we call a

00:09:44.980 --> 00:09:46.980
shallow structure

00:09:46.980 --> 00:09:48.980
of the variables.  Because you just have

00:09:48.980 --> 00:09:50.980
some inputs, and you multiply

00:09:50.980 --> 00:09:52.980
by some values, and you get the output.  Right?

00:09:52.980 --> 00:09:55.940
So, in a difference with deep learning,

00:09:55.940 --> 00:09:58.400
or, well, deep algorithms in general,

00:09:58.400 --> 00:10:00.640
is you have a certain input,

00:10:00.640 --> 00:10:03.820
then you get some intermediate variables

00:10:03.820 --> 00:10:05.820
by doing the same.  It's like if you

00:10:05.820 --> 00:10:07.600
fit a line, and then you fit another line,

00:10:07.600 --> 00:10:09.600
and another line, and another line,

00:10:09.600 --> 00:10:12.260
and then, when you input some values,

00:10:12.260 --> 00:10:14.000
it will give you certain values.  Right?

00:10:14.000 --> 00:10:16.000
So you have, like, predictions.  Right?

00:10:16.000 --> 00:10:18.000
These predictions are not the ultimate thing that you

00:10:18.000 --> 00:10:20.220
need; they are intermediate values

00:10:20.220 --> 00:10:23.060
that, at the same time, input to another

00:10:23.060 --> 00:10:25.060
layer of prediction.  Right?  And you

00:10:25.060 --> 00:10:27.060
go doing like that.  Right?  So -

00:10:27.060 --> 00:10:31.160
The input to one layer is the output of the previous layer.  Right?

00:10:31.160 --> 00:10:35.160
And this hierarchical structure is much more powerful

00:10:35.160 --> 00:10:37.940
in the sense of what kind of functions,

00:10:37.940 --> 00:10:40.540
what kind of - the flexibility of the functions.  Right?

00:10:40.540 --> 00:10:43.080
You can model much more complex things

00:10:43.080 --> 00:10:44.900
with many less variables.  Right?

00:10:44.900 --> 00:10:48.200
And this is - This is a magnificent trade-off that gives it a lot of power.

00:10:51.540 --> 00:10:53.900
You can go back and find historical examples;

00:10:54.160 --> 00:10:55.640
endless historical examples,

00:10:55.940 --> 00:10:58.720
of people claiming that something fantastic is right around the corner,

00:10:58.720 --> 00:11:00.980
when in fact it isn't.
[voices overlapping]
We should get a shell.  OK?

00:11:01.720 --> 00:11:03.200
And we did.  OK?

00:11:03.660 --> 00:11:05.700
So that's a good start, right?  We know our program works.

