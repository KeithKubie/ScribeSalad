WEBVTT
Kind: captions
Language: en

00:00:00.410 --> 00:00:04.649
&gt;&gt; DEAN: Yes, Flight of the Conchords.

00:00:04.649 --> 00:00:07.229
Cool.

00:00:07.229 --> 00:00:09.809
Okay.

00:00:09.809 --> 00:00:15.209
So let's get started.

00:00:15.209 --> 00:00:20.990
So many of you have probably used some of
Google's products and to a user, it sort of

00:00:20.990 --> 00:00:25.560
looks like a bunch of different tools and
services that you can use that help you accomplish

00:00:25.560 --> 00:00:31.090
various kinds of things; looking for information,
getting maps, checking your email, these kinds

00:00:31.090 --> 00:00:32.090
of things.

00:00:32.090 --> 00:00:35.980
What I'm going to talk about is a slightly
different view of Google which is more of

00:00:35.980 --> 00:00:39.570
a computer scientist's view and the sort of
range of problems that we work on.

00:00:39.570 --> 00:00:44.880
I'll talk about some of the computing systems
we've built behind the scenes that are used

00:00:44.880 --> 00:00:53.330
for serving all these different products,
building different products on top of them.

00:00:53.330 --> 00:00:57.719
One of the things that I think is interesting
about Google is that there's a really wide

00:00:57.719 --> 00:01:02.840
range of areas that people work on and many
of the products that we work on cross lots

00:01:02.840 --> 00:01:03.840
of these different areas.

00:01:03.840 --> 00:01:07.150
So from that prospective, it's a really fun
and interesting place to be because you may

00:01:07.150 --> 00:01:12.070
be an expert or good at one area but not know
much about another and you can work with colleagues

00:01:12.070 --> 00:01:17.180
who know a lot about of different areas and
in that way, you can sort of learn more which

00:01:17.180 --> 00:01:18.270
is always good.

00:01:18.270 --> 00:01:23.480
So, I'm going to start at the very bottom
which is basically what our computing hardware

00:01:23.480 --> 00:01:25.910
platform looks like.

00:01:25.910 --> 00:01:31.410
One of our philosophies from very early on
in the company was that we were going to build

00:01:31.410 --> 00:01:35.700
all of our systems on top of very low-end
commodity PCs.

00:01:35.700 --> 00:01:39.840
Essentially, that's where all the volume is
in the market, that's where you get the most

00:01:39.840 --> 00:01:41.100
bang for your buck.

00:01:41.100 --> 00:01:45.960
In any case, you could buy very large machines
but most of our problems are large enough

00:01:45.960 --> 00:01:49.200
that you'd have to parallelize things anyway
across lots of machines.

00:01:49.200 --> 00:01:53.930
And given that you have to do that anyway,
you might as well go with a platform where

00:01:53.930 --> 00:01:56.620
you get the most performance per dollar.

00:01:56.620 --> 00:01:59.850
So, lots of commodity machines is what we
do.

00:01:59.850 --> 00:02:06.640
In the very beginning when our two founders
were at Stanford, they started out with a

00:02:06.640 --> 00:02:11.370
kind of a motley collection of machines they
found from other research groups.

00:02:11.370 --> 00:02:15.450
They would head down to the loading back and
volunteer to set up machines for other research

00:02:15.450 --> 00:02:19.730
groups and then they kind of used them for
three or four months before actually delivering

00:02:19.730 --> 00:02:22.500
them.

00:02:22.500 --> 00:02:26.700
And one problem with that approach is you
end up with a very heterogeneous collection

00:02:26.700 --> 00:02:31.010
of machines with all kinds of different CPUs
and operating systems.

00:02:31.010 --> 00:02:36.800
So we--and once the company was founded, we
ended up going with a more homogeneous set-up

00:02:36.800 --> 00:02:38.310
made out of commodity components.

00:02:38.310 --> 00:02:42.750
We actually were pretty bold in deciding we
would just assemble them ourselves by buying

00:02:42.750 --> 00:02:46.020
a bunch of motherboards and discs and power
supplies.

00:02:46.020 --> 00:02:48.420
So this is a very early design.

00:02:48.420 --> 00:02:53.000
What you probably can't see is that there's
a tray and on top of that tray, there's four

00:02:53.000 --> 00:02:56.330
machines all sharing a power supply.

00:02:56.330 --> 00:02:59.930
Turns out not to be the best idea actually.

00:02:59.930 --> 00:03:04.310
There's also a thin layer of cork insulating
the machines from the cookie tray that they're

00:03:04.310 --> 00:03:06.660
sitting on.

00:03:06.660 --> 00:03:12.470
And eventually, we decided that perhaps cork
was not an integral part of our computer architecture

00:03:12.470 --> 00:03:17.030
and went with a slightly more traditional-looking
machine set up.

00:03:17.030 --> 00:03:21.340
We tried to pack our machines pretty densely
just because you use your space more efficiently

00:03:21.340 --> 00:03:27.819
and at this time, we were renting space in
data centers that we didn't know and so we

00:03:27.819 --> 00:03:31.440
were leasing space in lots of different data
center providers.

00:03:31.440 --> 00:03:35.190
And at that time, the data center providers
seemed to have a pricing model that charged

00:03:35.190 --> 00:03:43.660
only by the square foot, so we had an incentive
to pack as many machines in as we could.

00:03:43.660 --> 00:03:49.400
So we often had to help them a little bit
with cooling with a small fan we purchased

00:03:49.400 --> 00:03:51.260
at Target.

00:03:51.260 --> 00:03:54.550
The other thing we learned is that it's good
to have all your connectors on the front of

00:03:54.550 --> 00:03:55.580
the machines.

00:03:55.580 --> 00:03:59.160
The previous design did not have that property
which was kind of [INDISTINCT] and you'd end

00:03:59.160 --> 00:04:02.860
up with crimped ethernet tables.

00:04:02.860 --> 00:04:11.070
So based on a pricing model, we also got very
adept at moving out of bankrupt data centers.

00:04:11.070 --> 00:04:17.471
So we ended up being able to move into new
data centers pretty quickly at that time.

00:04:17.471 --> 00:04:18.850
We sort of had all the racks pre-assembled.

00:04:18.850 --> 00:04:24.660
We just wheel them in and then you cable together
the rack switches to a central switch for

00:04:24.660 --> 00:04:28.200
the whole cluster and away you go.

00:04:28.200 --> 00:04:29.840
This is our current design.

00:04:29.840 --> 00:04:33.150
We've kind of gone back and forth on whether
cases are a good idea.

00:04:33.150 --> 00:04:37.730
We've now settled on no sort of cases because
it gives you better air flow.

00:04:37.730 --> 00:04:41.080
But cables are a lot neater than that initial
design.

00:04:41.080 --> 00:04:47.460
They're all basically commodity class PCs
that run Linux and a bunch of our own in-house

00:04:47.460 --> 00:04:50.660
software that I'll describe, basically PC
class motherboards.

00:04:50.660 --> 00:04:56.650
We kind of have a small number of different
configurations that we use basically not many

00:04:56.650 --> 00:04:59.460
discs and a lot of discs.

00:04:59.460 --> 00:05:03.310
That's kind of the extent of the different
configurations.

00:05:03.310 --> 00:05:07.560
Everything within one data center tends to
be like the same generation of hardware and

00:05:07.560 --> 00:05:11.540
then if we build the new data center, that
will be kind of whatever the best sweet spot

00:05:11.540 --> 00:05:14.780
is in terms of price performance for that
when we build that data center.

00:05:14.780 --> 00:05:18.680
So we have heterogeneity across different
data centers but typically not within the

00:05:18.680 --> 00:05:21.640
same data center.

00:05:21.640 --> 00:05:25.210
And this is kind of our current idea of what
multicore computing is.

00:05:25.210 --> 00:05:30.640
This is our facility in Oregon that we opened
pretty recently.

00:05:30.640 --> 00:05:35.400
It's a pretty large facility with lots of
machines in it.

00:05:35.400 --> 00:05:43.550
And that's kind of the scale of machine, if
you like, that we're dealing with today.

00:05:43.550 --> 00:05:50.169
Now, one problem with these commodity machines
is that if you have a lot of them, you end

00:05:50.169 --> 00:05:55.790
up with kind of bizarre problems that are--just
happen because you have so many machines that

00:05:55.790 --> 00:05:58.699
you are bound to see almost any kind of problem.

00:05:58.699 --> 00:06:02.150
And this is thanks to one of our operations
folk--people.

00:06:02.150 --> 00:06:07.389
They--he put together a list of the kinds
of things that tend to go wrong in the first

00:06:07.389 --> 00:06:09.060
year of a cluster.

00:06:09.060 --> 00:06:11.840
After that first year, though it gets a little
bit better.

00:06:11.840 --> 00:06:17.229
So--but you see all kinds of things that you
wouldn't necessarily expect and that you really

00:06:17.229 --> 00:06:22.070
have to be prepared to deal with in your software
in various ways like a whole rack going missing.

00:06:22.070 --> 00:06:27.310
So 40 machines kind of drop off the face of
the earth and maybe just for a little while,

00:06:27.310 --> 00:06:28.580
maybe for a long time.

00:06:28.580 --> 00:06:29.580
Those actually aren't so bad.

00:06:29.580 --> 00:06:34.949
The really sort of persnickety kinds of failures
are the ones that are not quite bad enough

00:06:34.949 --> 00:06:41.440
to have the machine look dead but just kind
of look very slow or you get corrupted traffic

00:06:41.440 --> 00:06:46.410
through network switches that sort of survive
TCB checks on errors and that kind of thing.

00:06:46.410 --> 00:06:53.419
So those are kind of the more severe errors
where you have kind of Byzantine failures.

00:06:53.419 --> 00:06:54.419
Okay.

00:06:54.419 --> 00:07:03.710
So, given that that's the bedrock on which
we're building, the main issue is that you

00:07:03.710 --> 00:07:07.090
have to really provide reliability at the
software level.

00:07:07.090 --> 00:07:11.651
Anything you're running on a given machine,
you have to expect to be able to--that machine

00:07:11.651 --> 00:07:12.651
might fail.

00:07:12.651 --> 00:07:17.320
If you have, you know, one server, you may
not really have to worry about that server

00:07:17.320 --> 00:07:18.320
failing.

00:07:18.320 --> 00:07:21.479
But if you're running something on 10,000
machines, some of them are going to die everyday

00:07:21.479 --> 00:07:25.100
and you have to really be prepared to deal
with that.

00:07:25.100 --> 00:07:28.980
The other thing to notice is that if you spent
lots of money and bought ultra-reliable hardware

00:07:28.980 --> 00:07:34.340
with redundant power supplies or rated discs,
that would help a little but you still have

00:07:34.340 --> 00:07:39.340
lots of failures per day that you have to
deal with and those fancy features cost you

00:07:39.340 --> 00:07:42.370
a lot in terms of dollars for the hardware.

00:07:42.370 --> 00:07:48.229
So our view--our view is it's better twice
as much hardware that's much reliable than

00:07:48.229 --> 00:07:52.120
half as much that's not as reliable.

00:07:52.120 --> 00:07:53.130
Okay.

00:07:53.130 --> 00:07:58.419
So the rest of this talk, I'm going to talk
a little bit about different pieces of infrastructures

00:07:58.419 --> 00:08:04.770
we've--that we've built over the years to
take a big blob of commodity machines and

00:08:04.770 --> 00:08:09.889
make them more useable as sort of a coherent
system as opposed to a bunch of individual

00:08:09.889 --> 00:08:14.919
machines that you care about in an individual
manner.

00:08:14.919 --> 00:08:20.300
I'll take a quick peek at how sort of when
a query comes in, it gets served on Google.com.

00:08:20.300 --> 00:08:25.110
I'll look a little bit at our machine translation
system because I think that's kind of a fun

00:08:25.110 --> 00:08:31.289
problem, talk about some interesting data
and then give you some thoughts about our

00:08:31.289 --> 00:08:35.339
engineering style and philosophy and some
of the approaches we've developed to deal

00:08:35.339 --> 00:08:38.800
with our source code base and so on.

00:08:38.800 --> 00:08:39.800
Okay.

00:08:39.800 --> 00:08:43.749
So given that you have a big pile of machines,
one of the first things you want to be able

00:08:43.749 --> 00:08:47.700
to do is store data on them and you'd like
to be able to do that reliably given that

00:08:47.700 --> 00:08:49.190
machines are failing.

00:08:49.190 --> 00:08:55.449
So one of the first systems that we put together
at Google was a sort of large scale distributive

00:08:55.449 --> 00:09:00.620
file system and the particular--this is called
the Google File System, GFS.

00:09:00.620 --> 00:09:07.519
In this design, basically, we just take chunks
of files, 64 megabyte chunks, we're going

00:09:07.519 --> 00:09:12.279
break files into those chunks, and each of
the chunks is going to have many copies of

00:09:12.279 --> 00:09:16.170
it stored across different machines, typically
three copies per chunk.

00:09:16.170 --> 00:09:21.579
So that if a machine goes bad, you have two
other copies of each chunk of data that was

00:09:21.579 --> 00:09:27.470
managed by that machine that you can recover
that data from or use to make a third copy

00:09:27.470 --> 00:09:30.160
now that you're down to two.

00:09:30.160 --> 00:09:34.189
We have a centralized master in this file
system that is responsible for all the metadata

00:09:34.189 --> 00:09:35.189
operations.

00:09:35.189 --> 00:09:38.670
So when you open a file, the client goes and
talks to the master and says, okay, I'm going

00:09:38.670 --> 00:09:39.670
to open this file.

00:09:39.670 --> 00:09:42.589
Tell me where the chunks are, how big the
file is, et cetera.

00:09:42.589 --> 00:09:46.839
And then the clients can talk directly to
chunkserver processes that are running on

00:09:46.839 --> 00:09:53.110
all the machines and those chunkservers sort
of are responsible for reading data from chunks

00:09:53.110 --> 00:09:56.350
off of local discs, sending it across the
network to clients.

00:09:56.350 --> 00:10:01.249
One of the real advantages of this approach
is all the real bulk data transfer happens

00:10:01.249 --> 00:10:04.680
between clients and the chunkservers directly.

00:10:04.680 --> 00:10:08.370
The master is not really involved in that
once the files have been opened.

00:10:08.370 --> 00:10:10.980
And so, you can get a very aggregate bandwidth
from this system.

00:10:10.980 --> 00:10:15.069
You can have, you know, thousands of clients
talking to thousands of chunkservers and reading

00:10:15.069 --> 00:10:21.550
at many tens of gigabytes per second out of
that file system.

00:10:21.550 --> 00:10:26.850
So that's sort of our lowest level of systems
infrastructure system that runs across a bunch

00:10:26.850 --> 00:10:31.720
of machines is a file system where you can
store lots of data.

00:10:31.720 --> 00:10:36.540
It's sort of optimized for larger files than
a traditional file system.

00:10:36.540 --> 00:10:41.730
We tend to--you know, if you crawl a bunch
of pages of the web, we don't put one file

00:10:41.730 --> 00:10:47.470
per page, we just dump of bunch of, you know,
millions of pages into a single file and then

00:10:47.470 --> 00:10:52.550
we process it from there.

00:10:52.550 --> 00:10:54.879
You know, so today basically, GFS runs on
almost all of our machines.

00:10:54.879 --> 00:10:58.059
We have a chunkserver running on most of our
machines.

00:10:58.059 --> 00:11:05.809
The, you know--we have file systems that are
many petabytes in size and can support pretty

00:11:05.809 --> 00:11:08.069
aggressive read and write leads.

00:11:08.069 --> 00:11:13.529
And the master is responsible also for making
backup like monitoring the health of all the

00:11:13.529 --> 00:11:16.459
chunkservers and making sure that the copies
of the chunks that are supposed to be there

00:11:16.459 --> 00:11:17.920
are still there.

00:11:17.920 --> 00:11:22.450
And if it notices a machine has fail or that
a chuck, a disc has gone bad or something,

00:11:22.450 --> 00:11:27.230
it will go ahead and start making--basically
cloning copies of that chunk again so--to

00:11:27.230 --> 00:11:29.399
get it back up to the desired replication
level.

00:11:29.399 --> 00:11:36.040
So, essentially, what machine failures are
handle entirely by the GFS system at least

00:11:36.040 --> 00:11:38.189
at the storage level.

00:11:38.189 --> 00:11:39.189
Okay.

00:11:39.189 --> 00:11:43.319
The next thing I'm going to talk about is
MapReduce.

00:11:43.319 --> 00:11:47.309
Given that you have a file system, a pretty
natural thing to want to be able to do is

00:11:47.309 --> 00:11:52.429
to run computations over data that you've
stored in a file system and perhaps compute

00:11:52.429 --> 00:11:54.499
derived data.

00:11:54.499 --> 00:12:01.660
One example might be, you have a bunch crawled
pages stored in a bunch files and you want

00:12:01.660 --> 00:12:10.040
to process those pages in order to, I don't
know, count how often every word occurs or

00:12:10.040 --> 00:12:14.189
maybe you want to compute an inverted index
where you take--you want to build for every

00:12:14.189 --> 00:12:17.189
word a list of documents where that word occurs.

00:12:17.189 --> 00:12:22.459
It's a pretty useful structure for the kinds
of things we do when we're serving queries

00:12:22.459 --> 00:12:26.649
is you want to be able to offline, build an
index like that so you can tell for the query

00:12:26.649 --> 00:12:30.679
New York restaurants what are the pages you
need to consider that have the word New and

00:12:30.679 --> 00:12:33.170
York and restaurants in it.

00:12:33.170 --> 00:12:37.889
So MapReduce is a way of taking large computations
like that that are actually pretty simple

00:12:37.889 --> 00:12:42.040
to describe--you know, it's not that hard
to say, "I want to count how often every word

00:12:42.040 --> 00:12:43.040
occurs."

00:12:43.040 --> 00:12:47.189
The problems come because you need to be able
to run this computation across lots of thousands

00:12:47.189 --> 00:12:52.259
of machines in order for it to compute--complete
in a reasonable amount of time.

00:12:52.259 --> 00:12:56.860
And what we found was that over time, we were
ending up writing similar kinds of code all

00:12:56.860 --> 00:13:02.510
the time that dealt with a lot of the issues
that MapReduce now deals with, things like,

00:13:02.510 --> 00:13:05.970
"How am I going to take this computation and
spread it across thousands of machines?

00:13:05.970 --> 00:13:10.009
How am I going to get load balancing across
the different machines so that, you know,

00:13:10.009 --> 00:13:15.470
I don't end up with one machine that's got
a lot of the work towards the end of the job?

00:13:15.470 --> 00:13:20.690
How can I ideally move computation close to
the data rather than having to transfer lots

00:13:20.690 --> 00:13:25.470
and lots of data across the network and deal--how
can I deal with machine failures?"

00:13:25.470 --> 00:13:30.679
And so MapReduce is an attempt to basically
abstract the programming model a little bit

00:13:30.679 --> 00:13:39.290
and allow you to write fairly simple--a couple
of fairly simple functions and have the underlying

00:13:39.290 --> 00:13:43.920
library take care of a lot of the details
for running those functions across your data

00:13:43.920 --> 00:13:47.239
and on thousands of machines.

00:13:47.239 --> 00:13:52.600
So the typical outline of something is you
want to read a bunch of input data, you want

00:13:52.600 --> 00:13:57.910
to take each record which might be a crawled
page or something like that, extract something

00:13:57.910 --> 00:13:59.839
you care about from every record.

00:13:59.839 --> 00:14:04.110
You want to shuffle and sort the data and
then you want to reduce the data you extracted

00:14:04.110 --> 00:14:05.110
in some way.

00:14:05.110 --> 00:14:08.279
Maybe you want to [INDISTINCT] it all together,
maybe you want to add it up--add up counts

00:14:08.279 --> 00:14:14.369
of how often every word occurs, that kind
of thing and then you want to produce your

00:14:14.369 --> 00:14:15.369
output.

00:14:15.369 --> 00:14:19.609
So let's looks at it in the context of processing
geographic data, for example, to produce the

00:14:19.609 --> 00:14:25.569
kinds of map tiles that show up on Google
Maps where every image is kind of all the

00:14:25.569 --> 00:14:29.519
roads in that square and rendered as map.

00:14:29.519 --> 00:14:36.149
What you actually get to start with is a input
that is a list of features that you license

00:14:36.149 --> 00:14:38.329
from, say, a third party map provider.

00:14:38.329 --> 00:14:43.189
And they say, okay, here's a road, it intersects
with intersection number three and here's

00:14:43.189 --> 00:14:46.170
the path the road takes.

00:14:46.170 --> 00:14:50.160
Here's some intersection that intersects roads
one, two and five but I don't have any data

00:14:50.160 --> 00:14:51.299
about each of the roads.

00:14:51.299 --> 00:14:55.489
So one of the first things that I want to
be able to do is build a data structure that

00:14:55.489 --> 00:14:59.839
for every intersection, I can tell--I can
basically join it with all the roads that

00:14:59.839 --> 00:15:05.549
are entering that intersection so that I have
the directions of the roads.

00:15:05.549 --> 00:15:08.860
So for example, intersection three, I want
to have the actual data for roads one, two

00:15:08.860 --> 00:15:12.870
and five and for intersection six, I want
to have the actual data for roads four, five

00:15:12.870 --> 00:15:16.220
and seven, so that I can eventually render
the map.

00:15:16.220 --> 00:15:22.149
So this is pretty simple to describe; it's
a lot of--it's a fair amount of data.

00:15:22.149 --> 00:15:25.160
In MapReduce, you would basically just read
the input.

00:15:25.160 --> 00:15:30.169
If it's a road, you would figure out what
intersections it intersects with and emit

00:15:30.169 --> 00:15:32.300
a key value pair for each of the intersections.

00:15:32.300 --> 00:15:36.369
If it's an intersection, you would just propagate
the data there.

00:15:36.369 --> 00:15:41.019
And then you would essentially, in the reduce
phase, have all the data for each intersection

00:15:41.019 --> 00:15:46.560
and be able to apply whatever computation
you wanted to join together; the complete

00:15:46.560 --> 00:15:51.279
details of the intersection including all
the road information.

00:15:51.279 --> 00:15:53.300
And so that's pretty easy to describe.

00:15:53.300 --> 00:15:54.300
It's not much code.

00:15:54.300 --> 00:15:58.029
I don't want you to actually read the code,
but give you a sense of, you know, how much

00:15:58.029 --> 00:16:04.329
code you might have to write in order to able
to run this.

00:16:04.329 --> 00:16:07.859
And let's look at one more example which is
now once, I've got my intersections, I want

00:16:07.859 --> 00:16:09.859
to be able to render map tiles.

00:16:09.859 --> 00:16:16.670
So the second example is, in the map phase,
I'm going to have a key that I'm going to

00:16:16.670 --> 00:16:18.690
designate for each map tile I want to render.

00:16:18.690 --> 00:16:24.519
So each square on the earth, I'm going to
have a square, and then in the map phase,

00:16:24.519 --> 00:16:29.550
I'm going to go through all the intersections,
all the roads and if any road intersects a

00:16:29.550 --> 00:16:33.989
particular map tile, I'm going to emit it
to that map tile's key.

00:16:33.989 --> 00:16:39.209
And in the reduce phase, it's going to join
together all the data for a particular map

00:16:39.209 --> 00:16:41.989
tile and I'm going to get a list of all the
roads that have any intersection with that

00:16:41.989 --> 00:16:46.680
map tile and I can then clip to the map tile
boundary and then render that road.

00:16:46.680 --> 00:16:52.509
So for example, if you know Seattle I-5 runs
north-south, so we'll emit I-5 to both of

00:16:52.509 --> 00:16:53.699
these two map tiles.

00:16:53.699 --> 00:16:58.519
Uh, the 520 Bridge run east-west and it only
interests with the top tile, so we'll just

00:16:58.519 --> 00:16:59.989
emit it to the top one.

00:16:59.989 --> 00:17:04.660
We'll emit I-90 to the bottom one and then
so on with all the other roads.

00:17:04.660 --> 00:17:10.020
And you could then end up being able to render
all those tiles.

00:17:10.020 --> 00:17:13.750
It turns out that this programming model is
actually pretty easy to describe a pretty

00:17:13.750 --> 00:17:17.120
broad and diverse set of problems in.

00:17:17.120 --> 00:17:20.360
And there are a lot of uses that we make of
it inside Google.

00:17:20.360 --> 00:17:24.380
I'll show you some data about, you know, number
of map reductions people have written over

00:17:24.380 --> 00:17:25.380
time.

00:17:25.380 --> 00:17:27.000
But it's used in a really wide range of areas.

00:17:27.000 --> 00:17:32.551
Our original motivation was to write a system
for building our indexing pipeline where you

00:17:32.551 --> 00:17:37.990
start with a bunch of raw pages on disc and
you ultimately want to end up with sort of

00:17:37.990 --> 00:17:41.780
an inverted index of, you know, words to list
the documents and some other data about the

00:17:41.780 --> 00:17:43.430
documents.

00:17:43.430 --> 00:17:44.810
But there's a bunch of phases in between.

00:17:44.810 --> 00:17:47.120
You want to eliminate duplicate content.

00:17:47.120 --> 00:17:53.701
You want to, you know, maybe compute page
rank for the--based on the graph structure

00:17:53.701 --> 00:17:55.950
of the pages that you've crawled and so on.

00:17:55.950 --> 00:18:01.000
So we originally wrote it for that purpose
and it's been used in a lot of other contexts

00:18:01.000 --> 00:18:02.520
as well.

00:18:02.520 --> 00:18:07.080
Underneath the cover, what really happens
is that we--when you run a map reduction,

00:18:07.080 --> 00:18:12.030
you set up a little structure that says, okay,
here's my map function, here's my reduce function,

00:18:12.030 --> 00:18:18.450
here's the number of machines I want to use
and here's the input files I want to run my

00:18:18.450 --> 00:18:20.370
map reduction over.

00:18:20.370 --> 00:18:25.060
And then there's a master that's started up
and a bunch of worker machines, you know,

00:18:25.060 --> 00:18:27.840
could be ten, could be several thousand.

00:18:27.840 --> 00:18:30.800
And the master is going to start assigning
work to the worker machines that check in

00:18:30.800 --> 00:18:31.800
with it.

00:18:31.800 --> 00:18:37.480
And one of the things it does is it tries
to direct workers to read pieces of the input

00:18:37.480 --> 00:18:42.050
data that they have on local desk, so it finds
out where the different chunks are of the

00:18:42.050 --> 00:18:46.540
files are located on GFS and it's going to
tell this work--the, you know, the upper right

00:18:46.540 --> 00:18:50.910
hand worker to read data that is ideally on
that same machine or at least on the same

00:18:50.910 --> 00:18:54.920
rack, so that you don't need to use a lot
of aggregate network bandwidth to read input

00:18:54.920 --> 00:19:01.710
data across, say, from a different rack in
your same data center.

00:19:01.710 --> 00:19:05.920
One of our precious commodities is network
bandwidth, so we try to do optimizations like

00:19:05.920 --> 00:19:09.560
that that help preserve that.

00:19:09.560 --> 00:19:14.520
And each map task is going to be started which
is going to--a map task is basically going

00:19:14.520 --> 00:19:21.280
to read a chunk of the input data, apply the
map function to each of the chunks and then

00:19:21.280 --> 00:19:27.900
emit the data that's produced by the map function
to a local file on the local disk.

00:19:27.900 --> 00:19:32.700
And then the master is also going to assign
a reduce task to a bunch of reduce workers

00:19:32.700 --> 00:19:37.330
and each of those reduce workers is going
to ask the map workers that have data for

00:19:37.330 --> 00:19:39.460
it to send their data over the network.

00:19:39.460 --> 00:19:43.370
So basically, there's a bunch of RPCs that
happen to shuffle the data across the network.

00:19:43.370 --> 00:19:47.910
And then the reduce workers sort things to
combine together all the data for the same

00:19:47.910 --> 00:19:53.530
key and then apply the reduce function too
so that you could say, fine, I have this map

00:19:53.530 --> 00:19:58.070
tile, here's all the data that was emitted
with that map tile key and you can apply the

00:19:58.070 --> 00:20:00.430
reduce function to do that.

00:20:00.430 --> 00:20:05.630
And then the output of the reduce function
is emitted to a bunch of files.

00:20:05.630 --> 00:20:08.030
So that's kind of an overview of the operation.

00:20:08.030 --> 00:20:14.170
When a machine fails, the master nodes which
map tasks that machine had accomplished or

00:20:14.170 --> 00:20:19.980
which reduce task that machine was assigned,
and will direct all the other machines to

00:20:19.980 --> 00:20:27.220
recover--to take up the map tasks that are
now--that were on a now dead machine.

00:20:27.220 --> 00:20:30.710
One of the nice things about this is we try
to make the map tasks fairly fine grained.

00:20:30.710 --> 00:20:35.180
So a given worker might do a hundred map tasks
in the course of a map reduction.

00:20:35.180 --> 00:20:40.170
And so if a machine fails, you end up losing
the hundred map tasks that have been completed

00:20:40.170 --> 00:20:45.270
but you can have each of a hundred other machines
each pick up one of those map tasks.

00:20:45.270 --> 00:20:48.190
And so recovery from failure is very, very
quick.

00:20:48.190 --> 00:20:54.070
We actually were--at one point, we were running
a map reduction and our operations folks,

00:20:54.070 --> 00:20:57.170
unbeknownst to us, were doing maintenance
on our clusters.

00:20:57.170 --> 00:21:02.740
So we were running a MapReduce on 1,800 machines
and they were unplugging all the networks

00:21:02.740 --> 00:21:05.480
switches from groups of, like, 80 machines
at a time.

00:21:05.480 --> 00:21:09.180
So they would unplug 80 machines, they'd drop
off the network, we'd lose the work and it

00:21:09.180 --> 00:21:12.550
would get redone by , you know, the other
1,720 machines.

00:21:12.550 --> 00:21:16.000
And then they'd unplugged the next group as
we were progressing.

00:21:16.000 --> 00:21:21.110
It ran a little slowly but it actually all
completed which is kind of a testament to

00:21:21.110 --> 00:21:23.270
how robust this stuff can be.

00:21:23.270 --> 00:21:24.620
All right.

00:21:24.620 --> 00:21:28.990
The other thing you got by using this sort
of centralized library is you got kind of

00:21:28.990 --> 00:21:32.450
a common status report about how your job
is progressing.

00:21:32.450 --> 00:21:37.540
You know, early on, the green is kind of assigned
map tasks but not completed.

00:21:37.540 --> 00:21:42.920
The red is now completed map tasks so there's
a bunch of completed map task in the bottom

00:21:42.920 --> 00:21:43.920
left there.

00:21:43.920 --> 00:21:45.800
And some map tasks being worked on.

00:21:45.800 --> 00:21:51.410
The reduced tasks are sort of in blue as the
reduced work progresses for each of the different

00:21:51.410 --> 00:21:53.240
reduced partitions.

00:21:53.240 --> 00:21:59.330
The blue kind of builds up and that's kind
of very close to the end of the job.

00:21:59.330 --> 00:22:04.900
So you got this centralized status reporting
and monitoring and so on.

00:22:04.900 --> 00:22:10.520
So we originally wrote the first version of
MapReduce in early '03 and then wrote another

00:22:10.520 --> 00:22:16.270
version that had much higher performance in
kind of end of '03.

00:22:16.270 --> 00:22:20.580
And it's had a bunch of other optimizations
and iterations done in the design as we've

00:22:20.580 --> 00:22:26.700
scaled the kinds of map, you know, the sizes
of map reductions we've been trying to do.

00:22:26.700 --> 00:22:32.890
We now have, you know, upwards of 10,000 different
MapReduce programs in our source tree.

00:22:32.890 --> 00:22:37.150
If you take the derivative of that graph,
you see that the number of new MapReduce programs

00:22:37.150 --> 00:22:40.130
per month has been growing fairly steadily.

00:22:40.130 --> 00:22:44.350
One of the things you might notice is that
every summer, we have a bunch of summer interns

00:22:44.350 --> 00:22:50.130
kind of show up and they're actually--you
know, they work really hard for the whole

00:22:50.130 --> 00:22:51.130
summer.

00:22:51.130 --> 00:22:53.570
And then at the end of the summer, they try
to check in all their code and so we end up

00:22:53.570 --> 00:22:58.570
with a big spike towards the end of the summer
when they're going to check in all their code.

00:22:58.570 --> 00:23:02.710
And it's kind of--most of these interns don't
have a lot of experience in running, writing

00:23:02.710 --> 00:23:07.660
large, you know, scale high performance distributed
computations.

00:23:07.660 --> 00:23:13.010
And they are still able to sort of make use
of MapReduce to run jobs on lots of machines

00:23:13.010 --> 00:23:17.300
and get work accomplished which is kind of
the whole goal is you want to be able to have

00:23:17.300 --> 00:23:21.120
this infrastructure that takes care of a lot
of these details for you so you can concentrate

00:23:21.120 --> 00:23:25.530
on what is it you're really trying to do rather
than dealing with machine failures.

00:23:25.530 --> 00:23:32.350
It's just some statistics over time of, you
know, how much we've been using MapReduce

00:23:32.350 --> 00:23:35.130
for various things.

00:23:35.130 --> 00:23:39.930
You know, we now run about a hundred thousand
MapReduce jobs per day.

00:23:39.930 --> 00:23:43.100
And they use, you know, an average 400 machines
each roughly.

00:23:43.100 --> 00:23:48.930
You know, a lot of machines years of used
in September.

00:23:48.930 --> 00:23:53.570
The job is complete pretty quickly, you know,
about 5 or 10 minutes or something even though

00:23:53.570 --> 00:23:56.150
they're running on lots of machines.

00:23:56.150 --> 00:24:02.790
So it's been pretty heavily used and has grown
in utility and sort of it's integration into

00:24:02.790 --> 00:24:05.200
our source base over time.

00:24:05.200 --> 00:24:06.290
Okay.

00:24:06.290 --> 00:24:11.510
Let's switch gears a bit one more time and,
well, the first of many.

00:24:11.510 --> 00:24:14.230
Let's talk about a system called Big Table.

00:24:14.230 --> 00:24:20.960
So Big Table is a system that we started working
on a few years ago to allow us to have a slightly

00:24:20.960 --> 00:24:24.140
higher level view of data than a file system.

00:24:24.140 --> 00:24:29.380
It's kind of like a database except it doesn't
do sort of relations or drawings.

00:24:29.380 --> 00:24:33.460
It's really designed for semi-structured data.

00:24:33.460 --> 00:24:37.730
We have a lot of that kind of data at Google,
so we have, you know, for our crawling system,

00:24:37.730 --> 00:24:42.020
it goes out and fetches pages and we have
different kinds of data about every URL.

00:24:42.020 --> 00:24:46.080
We have sort of meta-information like when
did we last crawl this page?

00:24:46.080 --> 00:24:47.760
What happened when we tried to crawl it?

00:24:47.760 --> 00:24:52.101
Did we, you know, get a success code back
or did we get a 404?

00:24:52.101 --> 00:24:53.450
That kind of thing.

00:24:53.450 --> 00:24:56.340
We have the actual contents of the page which
are pretty big.

00:24:56.340 --> 00:24:58.920
We have information about the links from each
page.

00:24:58.920 --> 00:25:04.230
We have kind of the inversion of that which
is what are the links that point to this page?

00:25:04.230 --> 00:25:08.100
We have page rank which is kind of this offline
graph computation we run and want to be able

00:25:08.100 --> 00:25:10.280
to update and get data.

00:25:10.280 --> 00:25:14.100
Similarly, we have per-user data where users
want to be able to set their preferences of

00:25:14.100 --> 00:25:18.270
what their preferred language is, what are
their recent queries and search results so

00:25:18.270 --> 00:25:20.870
that we can show them their history.

00:25:20.870 --> 00:25:25.580
And more recently, we've been dealing with
a lot of geographic kinds of data and in particular,

00:25:25.580 --> 00:25:30.330
satellite imagery is a fairly large, bulky
kind of thing.

00:25:30.330 --> 00:25:35.950
So in all these cases, the scale is pretty
large and, you know, many, mane terabytes

00:25:35.950 --> 00:25:39.370
or petabytes in many cases.

00:25:39.370 --> 00:25:42.430
And we want to be able to have pretty high
volume access to it.

00:25:42.430 --> 00:25:47.360
So, one approach would be to use a commercial
database of some sort.

00:25:47.360 --> 00:25:52.560
One problem with that approach is that the
scale is pretty large even for the pretty

00:25:52.560 --> 00:25:54.500
high-end commercial databases.

00:25:54.500 --> 00:26:01.120
And even if it weren't, when you have a really
large scale database that you need to use,

00:26:01.120 --> 00:26:04.450
the price that you pay is often quite high
for commercial databases.

00:26:04.450 --> 00:26:10.520
So by building this system internally that
was more suited to our needs and a little

00:26:10.520 --> 00:26:15.240
bit more tailored to our needs, we can also
then amortize the cost of building it across

00:26:15.240 --> 00:26:16.390
lots and lots of different projects.

00:26:16.390 --> 00:26:22.330
And once you've built it and can use it for
10 projects, the 11th one is pretty easy to

00:26:22.330 --> 00:26:27.080
use it for and has a very low marginal cost.

00:26:27.080 --> 00:26:33.440
The other thing is we can integrate a lot
of low level storage optimizations into our

00:26:33.440 --> 00:26:37.270
system that would be hard to do if we were
running on top of a commercial database layer.

00:26:37.270 --> 00:26:41.740
We do a lot of compression inside our system
even across different rows.

00:26:41.740 --> 00:26:46.480
We're integrated pretty tightly with GFS so
that we can give hints about placement of

00:26:46.480 --> 00:26:50.170
where a chunk should go so that we can get
locality.

00:26:50.170 --> 00:26:55.280
And another reason we decided to build it
is it was kind of fun to build things like

00:26:55.280 --> 00:26:56.280
this.

00:26:56.280 --> 00:27:02.110
So the basic data model that we provide for
big table is essentially a distributed multi-dimensional

00:27:02.110 --> 00:27:03.110
sparse map.

00:27:03.110 --> 00:27:05.280
So what do I mean by that?

00:27:05.280 --> 00:27:10.850
There--you can think of it as a triple that
maps from a row and a column name and a timestamp

00:27:10.850 --> 00:27:12.130
to some cell contents.

00:27:12.130 --> 00:27:19.730
And the cell contents we just treat as a uninterpreted
binary stream of bytes.

00:27:19.730 --> 00:27:24.770
So the rows typically and, you know, in one
example, if you wanted to maintain data about

00:27:24.770 --> 00:27:27.300
URLs, the row name would be the URL.

00:27:27.300 --> 00:27:30.710
We might have a column for the contents, we
might have another column for, you know, when

00:27:30.710 --> 00:27:36.500
was the last time we crawled it, another one
for the page rank of the page.

00:27:36.500 --> 00:27:42.180
We found it useful in our systems to maintain
historical data, to keep multiple versions

00:27:42.180 --> 00:27:46.600
of a particular row and column so that we
can look at how things have changed over time

00:27:46.600 --> 00:27:50.990
or look at various trends.

00:27:50.990 --> 00:27:55.470
And that three dimensional model seems to
be a pretty good match for a lot of applications

00:27:55.470 --> 00:27:58.790
that we want to build.

00:27:58.790 --> 00:28:00.400
Let's see.

00:28:00.400 --> 00:28:06.780
So the way we actually implement this system
is the row names are sorted and so when you

00:28:06.780 --> 00:28:10.240
create a new table, it essentially starts
out with a bunch of empty rows and as you

00:28:10.240 --> 00:28:14.309
add data to it, you have a bunch of rows and
columns.

00:28:14.309 --> 00:28:19.560
A column doesn't have to exist and so it doesn't
exist until you actually write data into that

00:28:19.560 --> 00:28:20.560
column.

00:28:20.560 --> 00:28:24.670
Over time, you'll end up filling up the data
and one of the important properties we wanted

00:28:24.670 --> 00:28:27.230
was the system should be largely self-managing.

00:28:27.230 --> 00:28:30.880
So if I start adding more and more data to
the system, I don't want to have to have some

00:28:30.880 --> 00:28:36.090
heavyweight sort of manual process whereby
I say, "Oh, gee, now, I need 200 machines

00:28:36.090 --> 00:28:37.830
instead of 100 to maintain my data."

00:28:37.830 --> 00:28:44.330
I want the system to be able to sort of automatically
adjust to more data or maybe I can throw more

00:28:44.330 --> 00:28:49.760
machines at it and have it automatically adjust
to more machines even if I haven't added more

00:28:49.760 --> 00:28:50.760
data to the system.

00:28:50.760 --> 00:28:56.150
So the main way we deal with that is the unit
of, uh, serving is something called the tablet,

00:28:56.150 --> 00:29:00.870
which is essentially a continuous range of
rows in this large table.

00:29:00.870 --> 00:29:05.260
And a given table may be made up of lots and
lots of tablets.

00:29:05.260 --> 00:29:11.320
When you add more data to a tablet or to a
table, eventually, a tablet will up and will

00:29:11.320 --> 00:29:15.100
decide it's too large and we want to split
it into multiple pieces.

00:29:15.100 --> 00:29:17.390
Two, actually.

00:29:17.390 --> 00:29:24.120
And so we'll pick a row that's roughly in
the middle of the size profile of the different

00:29:24.120 --> 00:29:28.550
rows in the tablet and we'll just say fine,
we're going to now spit apart the state that

00:29:28.550 --> 00:29:33.940
was maintained in that system and now we have
two independent tablets and those tablets

00:29:33.940 --> 00:29:38.860
can now be moved around the system and responsibility
for serving those tablets can be moved from

00:29:38.860 --> 00:29:43.130
one machine to another.

00:29:43.130 --> 00:29:49.920
So the representation of a tablet is basically
a--you can think of it as a infinite length

00:29:49.920 --> 00:29:56.640
log that we truncate periodically by check-pointing
the state into a special file format we call

00:29:56.640 --> 00:29:58.010
an SS table.

00:29:58.010 --> 00:30:02.770
It's basically just a key to value map where
the keys are basically combinations of row,

00:30:02.770 --> 00:30:06.180
column, and timestamp and the value is the
actual bytes there.

00:30:06.180 --> 00:30:10.240
So when you got to write, you append to a
log that's stored in GFS and you also represent

00:30:10.240 --> 00:30:14.600
that write in memory for a little while so
that you have fast random access to recently

00:30:14.600 --> 00:30:15.970
written data.

00:30:15.970 --> 00:30:21.780
Eventually, your memory fills up and you decide
to dump it to one of these files on disk.

00:30:21.780 --> 00:30:26.880
So you take all the--you find the table that
has the most buffered updates, you dump it

00:30:26.880 --> 00:30:32.690
to an SS table on disk that's going to represent
a range of that log, and then you can then

00:30:32.690 --> 00:30:37.010
truncate the log and remove the prefix of
the log because you now have representation

00:30:37.010 --> 00:30:41.910
of the data in the SS table file.

00:30:41.910 --> 00:30:46.450
When a read comes in, you essentially look
in your in-memory buffer and you then also

00:30:46.450 --> 00:30:53.010
maybe look on the on-disk SS tables seek--doing
a seek for that particular key that you're

00:30:53.010 --> 00:30:55.500
looking for.

00:30:55.500 --> 00:30:57.560
You may need to look in multiple SS tables.

00:30:57.560 --> 00:31:01.620
We also allow you to specify that certain
data--certain columns essentially should be

00:31:01.620 --> 00:31:03.240
mapped into memory.

00:31:03.240 --> 00:31:09.059
And so, those data--those SS table get mapped--and
mapped and you can do very fast high volume

00:31:09.059 --> 00:31:12.630
operations on--data mapped in the memory.

00:31:12.630 --> 00:31:20.070
Maybe you go directly to memory in some cases,
skipping the on disk ones.

00:31:20.070 --> 00:31:24.830
So, like a lot of our systems, the structure
of Big Table is that we have a centralized

00:31:24.830 --> 00:31:29.000
master and then we have a bunch of serves
that are going to be responsible for actually

00:31:29.000 --> 00:31:30.340
serving data.

00:31:30.340 --> 00:31:36.680
The--underneath the covers, we have--well,
sitting underneath this, we have GFS where

00:31:36.680 --> 00:31:39.210
all the state for Big Table is stored.

00:31:39.210 --> 00:31:44.350
So, all the SS Table files and the log files
are written into GFS and when a tablet migrates

00:31:44.350 --> 00:31:50.340
from one machine to another, that other tablet--that
other machine will read whatever state is

00:31:50.340 --> 00:31:53.770
needed to recover out of GFS.

00:31:53.770 --> 00:31:57.179
We have a clustered scheduling system where
we can say, "I want to schedule a job that

00:31:57.179 --> 00:32:03.520
is my tablet servers with 400 instances for
those, because I want to run 400 tablet servers,"

00:32:03.520 --> 00:32:08.740
or you can change it to be 500 and then the
master will notice that now there are 500

00:32:08.740 --> 00:32:16.150
tablet serves and sort of rebalance the serving
state.

00:32:16.150 --> 00:32:19.490
We also have a lock service that we use to
do master elections, although we only have

00:32:19.490 --> 00:32:24.330
a single master running that is making decisions
on the cluster.

00:32:24.330 --> 00:32:26.650
I'm not going to talk much about that.

00:32:26.650 --> 00:32:31.800
And then we have a client library that we
link into our applications that, when it wants

00:32:31.800 --> 00:32:35.270
to open a file, it talks to the lock service
to find out where the master is.

00:32:35.270 --> 00:32:39.880
When it wants to read and write data, it talks
directly to the tablet server and metadata

00:32:39.880 --> 00:32:43.720
operations like creating a new table go to
the master.

00:32:43.720 --> 00:32:45.590
Pretty straightforward.

00:32:45.590 --> 00:32:52.670
The status of this is that, over time, kind
of like MapReduce, we started out with a few

00:32:52.670 --> 00:32:57.809
applications in mind, some that were fairly
latency sensitive, so we were actually serving

00:32:57.809 --> 00:33:02.710
real user requests behind some of those applications
and other ones were more batch style crawling

00:33:02.710 --> 00:33:06.610
system kinds of things where you just want
to have a larger repository of data and have

00:33:06.610 --> 00:33:08.850
it in a slightly more structured format.

00:33:08.850 --> 00:33:14.710
And over time, the number of applications
that are using this--that using this system

00:33:14.710 --> 00:33:19.830
has grown a lot and we now do satellite imagery
processing in Big Table.

00:33:19.830 --> 00:33:24.560
We do or [INDISTINCT] is served heavily off
of Big Table.

00:33:24.560 --> 00:33:26.800
All kinds of things.

00:33:26.800 --> 00:33:32.400
Use this kind of as a low-level building block
for building your applications.

00:33:32.400 --> 00:33:37.580
And we have, you know, the biggest cells tend
to have several thousand tablet servers running,

00:33:37.580 --> 00:33:43.210
managing, you know, many petabytes of data
and shuffling data around to look balance

00:33:43.210 --> 00:33:45.960
load and so on, dealing with machine failures.

00:33:45.960 --> 00:33:49.970
So again, a given machine is responsible for
serving many tablets.

00:33:49.970 --> 00:33:55.500
So when a machine fails, the master notices
this and your recovery is very fast because

00:33:55.500 --> 00:34:00.140
we end up taking the 100 tablets that were
served by that machine and having each of

00:34:00.140 --> 00:34:02.730
a hundred other machines each pick up responsibility
for one of those tablets.

00:34:02.730 --> 00:34:07.790
And they each recover in parallel the state
so that, you know, within a second or two,

00:34:07.790 --> 00:34:16.049
typically, you can recover all the serving
state of a particular machine if that machine

00:34:16.049 --> 00:34:17.049
does.

00:34:17.049 --> 00:34:18.500
Let's see.

00:34:18.500 --> 00:34:22.359
So, the three systems I've talked about and
a bunch of other ones that I haven't talked

00:34:22.359 --> 00:34:29.730
about, mostly, their--a particular instance
of GFS or Big Table tends to run within a

00:34:29.730 --> 00:34:31.530
single cluster of machines.

00:34:31.530 --> 00:34:36.350
They don't tend to run across a really wide
area so we don't run a single Big Table cell

00:34:36.350 --> 00:34:39.919
with half the machines in Atlanta and half
in Oregon.

00:34:39.919 --> 00:34:42.779
We would run multiple Big Table cells.

00:34:42.779 --> 00:34:47.809
We do have replication systems set up so you
can kind of keep Big Table cells in rough

00:34:47.809 --> 00:34:50.079
consistencies.

00:34:50.079 --> 00:34:55.619
So writes done in Atlanta in one Big Table
cell can be propagated to a different Big

00:34:55.619 --> 00:34:56.619
Table cell in Oregon.

00:34:56.619 --> 00:34:58.970
You can kind of configure that.

00:34:58.970 --> 00:35:05.489
But one of the--one of the--one of the things
that we really noticed now is that we really

00:35:05.489 --> 00:35:12.279
want sort of our next generation infrastructure
to be a system that runs across all of our

00:35:12.279 --> 00:35:15.859
machines or a large fraction of our machine
rather than having separate instances.

00:35:15.859 --> 00:35:19.560
So, one of thing--one of the advantages that
would give is we would have a single global

00:35:19.560 --> 00:35:21.049
name space for all of our data.

00:35:21.049 --> 00:35:27.640
Right now, when you put files in GFS and--in
one cell and then you copy those files to

00:35:27.640 --> 00:35:33.450
another cell, they have different names, like
it's GFS/Oregon and GFS/Atlanta.

00:35:33.450 --> 00:35:37.869
And we've lost the association of those files
are the same or are meant to be, you know,

00:35:37.869 --> 00:35:39.470
copies of each other.

00:35:39.470 --> 00:35:43.990
So, what we really want is to have a single
name space where you can add replicas of particular

00:35:43.990 --> 00:35:47.119
pieces of data but keep the fact that that
data is the same.

00:35:47.119 --> 00:35:52.030
We'd also, in some, cases like stronger consistency
across different data centers.

00:35:52.030 --> 00:35:56.680
Right now, the Big Table replication system
gives you eventual consistency so if a data

00:35:56.680 --> 00:36:01.849
center is partitioned and then that partition
is healed, all the writes will eventually

00:36:01.849 --> 00:36:07.140
propagate, but in the meantime, you have kind
of weekly consistent semantics.

00:36:07.140 --> 00:36:11.980
We'd also like to be able to say things like
"This data is important.

00:36:11.980 --> 00:36:17.930
I want it on two disks in Europe, two in the
U.S. and one in Asia and I don't really care

00:36:17.930 --> 00:36:20.739
which particular data centers it lives in."

00:36:20.739 --> 00:36:24.489
One of the big problems we have is because
we run so many individual GFS cells or Big

00:36:24.489 --> 00:36:28.369
Table cells and humans have to make a decision
now where they're going to store their data

00:36:28.369 --> 00:36:33.849
and when they want to change which GFS cell
they store their data, that's kind of a less

00:36:33.849 --> 00:36:36.190
automated operation than you might like.

00:36:36.190 --> 00:36:41.180
So, the design goal of our next generation
systems is that we want to be much more automated

00:36:41.180 --> 00:36:43.910
in how systems run across different data centers.

00:36:43.910 --> 00:36:46.720
We're pretty automated at the--within cluster
level.

00:36:46.720 --> 00:36:50.400
So, machine failures and all that kind of
thing are taken care of pretty well by our

00:36:50.400 --> 00:36:56.079
existing infrastructure but across clusters
and data centers, we're not as good as we

00:36:56.079 --> 00:36:57.079
might hope.

00:36:57.079 --> 00:37:00.809
So, we want to be able to automatically move
jobs, move data automatically, shrink and

00:37:00.809 --> 00:37:06.890
grow capacity of jobs as, you know, request
loads change and so on.

00:37:06.890 --> 00:37:08.059
Okay?

00:37:08.059 --> 00:37:13.349
I just have one slide on this that I figure
everyone is kind of curious about what happens

00:37:13.349 --> 00:37:16.450
when a query comes in to Google.

00:37:16.450 --> 00:37:23.789
So, one of the things that sort of may not
be obvious to many people is that there are

00:37:23.789 --> 00:37:26.499
a huge number or subsystems involved in processing
that query.

00:37:26.499 --> 00:37:30.980
So, when a query comes in, it's routed by
our front end networking hardware to a particular

00:37:30.980 --> 00:37:36.069
web server that's going to kind of coordinate
the whole process for getting the results

00:37:36.069 --> 00:37:38.940
to show for that query.

00:37:38.940 --> 00:37:42.440
And initially, the system was relatively simple.

00:37:42.440 --> 00:37:51.589
We had kind of index servers that have a inverted
index of the documents in our index, so they

00:37:51.589 --> 00:37:56.989
map from--for a subset of the documents, they
each have a local copy of an inverted index

00:37:56.989 --> 00:38:02.039
which is, for each word, what are the documents
that occurs in and maybe some other per document

00:38:02.039 --> 00:38:08.140
data like how important is this page or, you
know, various other kinds of things.

00:38:08.140 --> 00:38:12.430
And so, when a query comes in, you send a
request to one copy of each of the different

00:38:12.430 --> 00:38:19.220
partitions of the index, thus those four lanes
going to I--zero through In things.

00:38:19.220 --> 00:38:23.609
You have many replicas of each of that piece
of index data so that you can handle the query

00:38:23.609 --> 00:38:28.680
volume and some of--this is showing one data
set and you might have many other sort of

00:38:28.680 --> 00:38:33.099
replicas of this entire set-up in different
data centers.

00:38:33.099 --> 00:38:38.599
And each of these local index shards computes
one of the best results for this query for

00:38:38.599 --> 00:38:40.670
it--for its subset of the documents.

00:38:40.670 --> 00:38:45.329
So given the query, you say New York restaurants,
it will compute what are the best results

00:38:45.329 --> 00:38:52.309
for its, you know, few million documents,
send back the results of the top five or ten

00:38:52.309 --> 00:38:58.700
and then you'd essentially merge those results
by score across all the deferent pieces of

00:38:58.700 --> 00:39:03.210
the index and you figure out one of the results
you actually want to show is the top 10 results

00:39:03.210 --> 00:39:05.200
on the result page.

00:39:05.200 --> 00:39:08.779
Once you figure that out, you essentially
are just getting identifiers of the documents

00:39:08.779 --> 00:39:14.609
back from the index system and you need do
then send the request down to what we called

00:39:14.609 --> 00:39:19.589
dock servers which are responsible for taking
that identifier in the query and computing

00:39:19.589 --> 00:39:26.079
the, you know, the little summary of the page,
the title and the snippet for the page.

00:39:26.079 --> 00:39:31.839
One thing to notice is that the snippets we
compute are query sensitive, so if I do a

00:39:31.839 --> 00:39:36.099
query, New York restaurants, that matches
a page, that may be a different snippet even

00:39:36.099 --> 00:39:40.599
for the same document for some other query
that matches the page.

00:39:40.599 --> 00:39:45.309
In the meantime, there's a bunch of other
systems that are involved in processing the

00:39:45.309 --> 00:39:46.309
query.

00:39:46.309 --> 00:39:49.640
We added a spelling checking system a long
time ago, an ad system.

00:39:49.640 --> 00:39:54.249
And the ad system itself is composed of lots
and lots different components.

00:39:54.249 --> 00:40:00.099
Over time, we've been adding more and more
different kinds of corpuses that can searched

00:40:00.099 --> 00:40:08.210
like news, different books that we've scanned,
blogs, various kinds of things like that,

00:40:08.210 --> 00:40:14.580
and we now sort of in parallel with the request
to the main web index do requests to all those

00:40:14.580 --> 00:40:20.450
other corpora and try to show those on our
results page if we think that a book result

00:40:20.450 --> 00:40:22.589
is appropriate for this particular query.

00:40:22.589 --> 00:40:25.660
That's actually an interesting problem is
when do you figure--how can you figure out

00:40:25.660 --> 00:40:32.999
for this query what results you want to show
and from what corpora make the most sense.

00:40:32.999 --> 00:40:34.029
Okay.

00:40:34.029 --> 00:40:35.470
Switch gears yet again.

00:40:35.470 --> 00:40:39.769
Let me talk a little bit about our machine
translation system because there is some interesting

00:40:39.769 --> 00:40:42.029
systems problems behind that.

00:40:42.029 --> 00:40:46.039
So, the goal with machine translation is basically
you want to take text in one language and

00:40:46.039 --> 00:40:52.220
translate it to text in another and you want
to do that in a way that gives you as understandable

00:40:52.220 --> 00:40:55.210
as possible a translation.

00:40:55.210 --> 00:41:03.380
So, the approach that a group at Google is
using is called statistical machine translation.

00:41:03.380 --> 00:41:05.839
So, machine translation is not really a new
concept.

00:41:05.839 --> 00:41:11.119
It started many, many years ago, I think,
in the '70s with a rule-based system where

00:41:11.119 --> 00:41:15.390
you say, for this word in English, I'm going
to translate it to this word in French or

00:41:15.390 --> 00:41:17.130
maybe this phrase.

00:41:17.130 --> 00:41:21.920
And over time, you build up more and more
rules that allow you to do better and better

00:41:21.920 --> 00:41:23.049
translations.

00:41:23.049 --> 00:41:25.109
The Statistical Approach is completely different.

00:41:25.109 --> 00:41:26.910
Basically, you start with a bunch of training
data.

00:41:26.910 --> 00:41:28.999
And there are two main kinds of training data.

00:41:28.999 --> 00:41:34.849
One is called parallel aligned corpora where
I have a bunch of text in one language and

00:41:34.849 --> 00:41:40.180
a bunch of translated versions of those sentences
in the other languages.

00:41:40.180 --> 00:41:45.010
And I build this statistical model that says,
okay, I see this English sentence and then

00:41:45.010 --> 00:41:48.529
I see the corresponding French sentence.

00:41:48.529 --> 00:41:55.979
I'm going to compute statistics over all the
words and phrases that I see in those sentences

00:41:55.979 --> 00:42:00.150
and I'll come up with a model that's says,
well, if hello is in the input sentence then

00:42:00.150 --> 00:42:03.339
with probability .87, the word Bonjour is
in the corresponding French sentence.

00:42:03.339 --> 00:42:12.319
And so you essentially get a model of word
by word and phrase by phrase replacements.

00:42:12.319 --> 00:42:17.109
The other thing that's really helped a lot
in recent years is the ability to have what's

00:42:17.109 --> 00:42:22.420
called a target language model because the
problem with it is you can't get many--you

00:42:22.420 --> 00:42:24.559
can't get much in the way of parallel aligned
corpora.

00:42:24.559 --> 00:42:29.490
There just aren't that many corpuses that
have really huge amounts of sentence by sentence

00:42:29.490 --> 00:42:32.210
translations for all language pairs.

00:42:32.210 --> 00:42:37.869
But one of the things that can really help
is if you have--you use the parallel model

00:42:37.869 --> 00:42:42.410
to generate a bunch of candidate translations
and then you can use the target language model

00:42:42.410 --> 00:42:45.980
to pick the most natural sounding ones of
the candidates.

00:42:45.980 --> 00:42:48.470
So, let me give you an example.

00:42:48.470 --> 00:42:55.580
Let's say I have two candidate sentences that
I--that I--are translations candidates and

00:42:55.580 --> 00:42:57.910
I want to pick the one that sounds the most
natural.

00:42:57.910 --> 00:43:02.240
I'm going to look up each five-word sequence
in a target language model and if I find that

00:43:02.240 --> 00:43:07.470
this five-word has sequences occurred in all
of our English documents three times and this

00:43:07.470 --> 00:43:12.000
other segments has a five-word sequence that's
never occurred, it's more likely to be that

00:43:12.000 --> 00:43:14.269
I should go with the one that's occurred three
times.

00:43:14.269 --> 00:43:20.759
So, the more data you can train the system
on, the better it's going to be.

00:43:20.759 --> 00:43:25.289
So essentially, you end up wanting to build
these target language models on really, really

00:43:25.289 --> 00:43:26.729
large amounts of text.

00:43:26.729 --> 00:43:30.940
Basically all the documents that we can get
our hands on.

00:43:30.940 --> 00:43:36.050
And so you end up with a relatively large
model even after you've just summarized it

00:43:36.050 --> 00:43:40.349
by counting how often every five-word sequence
occurs.

00:43:40.349 --> 00:43:44.589
And it turns out that the translation process
need to actually look up data in this model

00:43:44.589 --> 00:43:49.859
quite a lot, unfortunately, like a hundred
thousand to a million look-ups per sentence

00:43:49.859 --> 00:43:52.180
that you want to translate.

00:43:52.180 --> 00:43:56.019
And the model is one and half terabytes in
size.

00:43:56.019 --> 00:44:01.660
So you--we--you know, in typical fashion we
just spread it across a bunch of machines

00:44:01.660 --> 00:44:02.660
and do look-ups.

00:44:02.660 --> 00:44:05.990
So it seems to work pretty well.

00:44:05.990 --> 00:44:11.759
And one of the things that target language
model really helps is the translation quality,

00:44:11.759 --> 00:44:18.069
so this is a competition that's run annually
or every few years, I guess, by the National

00:44:18.069 --> 00:44:23.309
Institute of Standards and it has a bunch
of competitors submitting their machine translation

00:44:23.309 --> 00:44:24.869
systems.

00:44:24.869 --> 00:44:32.839
And this Y-axis is the BLEU score of how good
the translations were on some test data set

00:44:32.839 --> 00:44:34.479
and Google did quite well in these competitions.

00:44:34.479 --> 00:44:38.981
We can think of the BLEU score as sort of
the fraction of overlap of the translation

00:44:38.981 --> 00:44:44.930
system with a collection of human translations
of the same documents.

00:44:44.930 --> 00:44:48.349
And it actually doesn't really go to a hundred
because even if you had another human translate

00:44:48.349 --> 00:44:53.089
it, it wouldn't give you a hundred percent
because they tend to translate things differently

00:44:53.089 --> 00:44:54.089
as well.

00:44:54.089 --> 00:44:59.630
So, let me show you a non-Google translation
system that's available on the web.

00:44:59.630 --> 00:45:00.630
I won't say which one.

00:45:00.630 --> 00:45:05.500
And you can kind of try to read those middle
paragraphs.

00:45:05.500 --> 00:45:14.730
It's--it doesn't flow real well I would say.

00:45:14.730 --> 00:45:22.730
And here's the same text translated through
Google's Arabic to English translation system.

00:45:22.730 --> 00:45:28.470
And what you see is the impact of having a
really large language model makes the sentences

00:45:28.470 --> 00:45:29.720
flow a lot more naturally.

00:45:29.720 --> 00:45:34.410
So, if you read those middle two sentences
there, you know, they're not perfect grammatically--perfectly

00:45:34.410 --> 00:45:42.279
grammatically formed, but they're a lot better
than the other system.

00:45:42.279 --> 00:45:47.289
And as you might expect, the more training
data that you use for your target language

00:45:47.289 --> 00:45:50.010
model, the better your BLEU score is going
to be.

00:45:50.010 --> 00:45:56.839
And so, roughly a redoubling of your training
size for your language model gives you a little

00:45:56.839 --> 00:46:01.900
bit higher BLEU score which is kind of cool.

00:46:01.900 --> 00:46:06.579
So, more data is generally good which is in
part why we've been focusing a lot on building

00:46:06.579 --> 00:46:12.140
infrastructure to process large data sets
and, you know, do things like computing your

00:46:12.140 --> 00:46:17.329
language model fairly quickly or try out different
things in how we apply the language model

00:46:17.329 --> 00:46:22.780
to translation or any of a hundred other things
because the quicker you can turn around these

00:46:22.780 --> 00:46:28.460
kinds of experiments, the quicker you'll be
able to sort of improve things.

00:46:28.460 --> 00:46:32.839
Here's just another example of, you know,
having the right tools makes it so people

00:46:32.839 --> 00:46:34.550
can kind of ask questions.

00:46:34.550 --> 00:46:40.690
So, someone at Google who worked on our book
scanning group said, Gee, it'd be kind of

00:46:40.690 --> 00:46:47.640
cool to know where different locations were
mentioned in books and how that's changed

00:46:47.640 --> 00:46:48.640
over time."

00:46:48.640 --> 00:46:54.289
So, they essentially, you know, took all the
scanned books we've scanned which is I think

00:46:54.289 --> 00:46:59.559
more than a million books today, figured out,
you know, applied a geographic place name

00:46:59.559 --> 00:47:05.450
recognizer to all the text, took the extracted
locations and plotted them by year the book

00:47:05.450 --> 00:47:06.710
was published.

00:47:06.710 --> 00:47:10.180
And plotted the density of how often those
things were mentioned.

00:47:10.180 --> 00:47:13.890
And it's kind of interesting data, you know,
it's heavily biased towards English books

00:47:13.890 --> 00:47:17.829
because we tend to scan more English books
than other kinds of books but you can sort

00:47:17.829 --> 00:47:23.589
of see the expansion from Europe to the rest
of the world in terms of the publishing industry

00:47:23.589 --> 00:47:24.589
at least.

00:47:24.589 --> 00:47:25.589
Kind of fun.

00:47:25.589 --> 00:47:31.579
You know, and being able to, you know, take
questions like that and in a couple of hours,

00:47:31.579 --> 00:47:34.710
have a rough at that data is kind of fun.

00:47:34.710 --> 00:47:45.180
So, let me switch gears a bit again and talk
about some of our software engineering approaches

00:47:45.180 --> 00:47:53.019
for managing our source code base and our
projects as Google has grown over time.

00:47:53.019 --> 00:47:58.599
So, one of the things that I think is interesting
for a company our size now is that we still

00:47:58.599 --> 00:48:00.779
have one large shared source space.

00:48:00.779 --> 00:48:05.150
So, we have thousands of developers working
on the same source code base.

00:48:05.150 --> 00:48:09.599
There's a lot of lower level libraries that
are used by almost everything.

00:48:09.599 --> 00:48:13.089
There's a bunch of higher level libraries
that are shared across multiple projects that

00:48:13.089 --> 00:48:16.390
are kind of doing similar kinds of things.

00:48:16.390 --> 00:48:19.059
And then there's a bunch of application specific
code.

00:48:19.059 --> 00:48:24.910
Now, one of the big advantages of having this
single repository is that, you know, improvements

00:48:24.910 --> 00:48:26.910
in this core library benefit everyone.

00:48:26.910 --> 00:48:32.259
You don't have this sort of splintering effect
when you break your code into multiple repositories

00:48:32.259 --> 00:48:35.739
and then break things into different groups
and those groups then kind of stop talking

00:48:35.739 --> 00:48:40.369
to each other and just work on their own source
code and polishing their own libraries.

00:48:40.369 --> 00:48:44.890
And you don't get the benefit of those libraries
sort of benefiting everyone, but they benefit

00:48:44.890 --> 00:48:49.119
just a small subset of your [INDISTINCT] group.

00:48:49.119 --> 00:48:54.440
So, that's one of the main benefits and it's
also easy to find, you know, example code

00:48:54.440 --> 00:49:00.690
of, you know, how is someone using this library
function or what's the best way to count how

00:49:00.690 --> 00:49:02.969
often different things occur.

00:49:02.969 --> 00:49:09.200
Can I find the Map Reduction that does close
to what I want and maybe modify it or at least

00:49:09.200 --> 00:49:13.049
look at the code and see how they're doing
it.

00:49:13.049 --> 00:49:15.559
So, that's a pretty huge benefit.

00:49:15.559 --> 00:49:19.990
There are, of course, some drawbacks which
is that you sometimes have these kind of tangled

00:49:19.990 --> 00:49:24.559
dependencies that instead of just looking
at some piece of code, someone has said, oh,

00:49:24.559 --> 00:49:28.180
I'll just, you know--oh, I'm in Blogger and
I'm going to link into the middle of the book

00:49:28.180 --> 00:49:32.779
scanning project and use their library for
doing this because it does exactly what I

00:49:32.779 --> 00:49:33.779
want.

00:49:33.779 --> 00:49:35.829
And, you know, that happens.

00:49:35.829 --> 00:49:39.230
One of the things that we've found pretty
essential is that it's really helpful to be

00:49:39.230 --> 00:49:42.469
able to quickly search all of our source code.

00:49:42.469 --> 00:49:46.819
So, one of the things I did a few years ago
was built a little internal tool that's not

00:49:46.819 --> 00:49:48.220
particularly fancy, you know.

00:49:48.220 --> 00:49:51.989
Using a lot of the machines, you just spread
the requests over--spread the data over lots

00:49:51.989 --> 00:49:57.380
of machines and you just can run--basically
grab in parallel across all of our source

00:49:57.380 --> 00:49:58.380
code.

00:49:58.380 --> 00:50:03.099
So within, you know, quarter to a second,
you can search all of our code and find, you

00:50:03.099 --> 00:50:04.210
know, examples, uses.

00:50:04.210 --> 00:50:07.739
It's very helpful if you're maintaining low
level libraries so that you can find, you

00:50:07.739 --> 00:50:10.009
know, everyone who uses this routine.

00:50:10.009 --> 00:50:13.640
What if I change the semantics or what if
I want to add an argument?

00:50:13.640 --> 00:50:17.640
What would--you know, what would that do to
all the different call sites of this routine?

00:50:17.640 --> 00:50:20.040
So that's pretty helpful.

00:50:20.040 --> 00:50:26.339
We have pretty disciplines afternoon practice,
so we tend to code--every piece of code that's

00:50:26.339 --> 00:50:28.299
checked in is code reviewed by at least one
person.

00:50:28.299 --> 00:50:31.200
We do a lot of design reviews before writing
a lot of code.

00:50:31.200 --> 00:50:33.200
Lots and lots of testing.

00:50:33.200 --> 00:50:36.559
The testing is done for individual modules.

00:50:36.559 --> 00:50:38.190
Larger tests for all the systems.

00:50:38.190 --> 00:50:43.979
We have a continuous testing framework that
is always running all of our tests.

00:50:43.979 --> 00:50:49.380
Most of our development work is done in C++
Java and Python and the way it kind of breaks

00:50:49.380 --> 00:50:55.380
down is anything touched by, say, a web query
is done in C++ and a bunch of other things

00:50:55.380 --> 00:50:56.380
as well.

00:50:56.380 --> 00:51:00.880
Java tends to be used in lower volume applications
like the advertising front end that advertisers

00:51:00.880 --> 00:51:02.940
log into to get reports and so on.

00:51:02.940 --> 00:51:07.230
That's not quite as performance critical as
the code that runs on every web search and

00:51:07.230 --> 00:51:09.210
so that tends to be done in Java.

00:51:09.210 --> 00:51:16.229
Parts of Gmail are done that way and Python
tends to be used for configuration and so

00:51:16.229 --> 00:51:17.619
on.

00:51:17.619 --> 00:51:22.559
The other interesting change over the last
few years has been we've moved from being

00:51:22.559 --> 00:51:28.190
a, you know, a Mountain View-only engineering
organization to one that has many, many engineering

00:51:28.190 --> 00:51:32.579
sites all around the world in the last few
years.

00:51:32.579 --> 00:51:35.140
And the motivation for that for that is obvious.

00:51:35.140 --> 00:51:39.170
You somehow can't convince everyone in the
world to move to Mountain View.

00:51:39.170 --> 00:51:44.400
And so you want to hire really good candidates
regardless of where they actually are living.

00:51:44.400 --> 00:51:49.540
And so we've opened up engineering sites close
to where we think we can hire really good

00:51:49.540 --> 00:51:50.540
people.

00:51:50.540 --> 00:51:53.890
There's obviously a bunch of issues with having
of lots of small and medium-sized offices

00:51:53.890 --> 00:51:59.019
is that you need more coordination of who's
doing what.

00:51:59.019 --> 00:52:00.019
Communication is hard.

00:52:00.019 --> 00:52:02.589
You can't just kind of bump into people in
the hallway.

00:52:02.589 --> 00:52:07.460
And establishing trust between remote teams
working on the same source code base is sometimes

00:52:07.460 --> 00:52:10.869
tricky when you just haven't met someone face
to face.

00:52:10.869 --> 00:52:13.380
But, you know, there--we've developed a bunch
of techniques.

00:52:13.380 --> 00:52:16.720
First, we put all of our documentation and
source code online.

00:52:16.720 --> 00:52:21.069
Well, I mean source code is obviously online.

00:52:21.069 --> 00:52:24.170
We tend to use video conferencing and email
a fair amount.

00:52:24.170 --> 00:52:29.349
Careful choices of how, who's going to work
on what tends to make it so that you can try

00:52:29.349 --> 00:52:36.060
to break things at the sort of minimal communication
cut boundary so that this group can define

00:52:36.060 --> 00:52:40.160
an interface to the spelling system and can
go off and work on it and these people can

00:52:40.160 --> 00:52:43.059
use the spelling system through that fairly
narrow API.

00:52:43.059 --> 00:52:48.450
Ad the spelling system folks can roll out
new releases whenever they want without a

00:52:48.450 --> 00:52:50.069
lot of coordination with everyone else.

00:52:50.069 --> 00:52:51.630
That helps a lot.

00:52:51.630 --> 00:52:54.460
The Big Table project is split across three
sites.

00:52:54.460 --> 00:52:56.430
I tend to work pretty closely with some of
the New York.

00:52:56.430 --> 00:53:00.609
We have an--actually an office of people in
New York and we actually just have a permanent

00:53:00.609 --> 00:53:04.999
little portable video conferencing unit permanently
hooked up to their office and we just keep

00:53:04.999 --> 00:53:06.290
each other muted all the time.

00:53:06.290 --> 00:53:11.540
We can kind of un-mute and say, hey, hi, I'm
thinking of changing this code.

00:53:11.540 --> 00:53:15.219
It's actually better than having an office
of six people is to have two offices of three

00:53:15.219 --> 00:53:23.369
people in different places because they're
very quiet, at least when they're muted.

00:53:23.369 --> 00:53:26.359
And I think, you know, one of the things that
I really like about Google is that it's a

00:53:26.359 --> 00:53:28.640
really fun environment to write software.

00:53:28.640 --> 00:53:32.900
You know, there's a really interesting range
of problems and people who have different

00:53:32.900 --> 00:53:35.140
ranges of expertise.

00:53:35.140 --> 00:53:37.279
You can learn a lot about other areas.

00:53:37.279 --> 00:53:40.779
For example, the machine translation system,
I didn't know anything about machine translation

00:53:40.779 --> 00:53:44.930
but I knew they were having some issues with
how can they do look up some of this large

00:53:44.930 --> 00:53:46.549
language model.

00:53:46.549 --> 00:53:51.130
And so I went off and worked with them for
a little while in building this large language

00:53:51.130 --> 00:53:55.450
model serving system and, you know, I learned
a little about machine translation, they learned

00:53:55.450 --> 00:53:59.660
a little about how to distribute computations
across lots of machines.

00:53:59.660 --> 00:54:06.749
And the end result is something that probably
neither one of us could have produced alone.

00:54:06.749 --> 00:54:08.809
So, it's fun.

00:54:08.809 --> 00:54:15.039
I obviously covered a lot of ground; not much
in depth.

00:54:15.039 --> 00:54:20.109
So there's a bunch of papers and other papers
available that have a lot of details about

00:54:20.109 --> 00:54:22.200
some of the systems I talked about.

00:54:22.200 --> 00:54:28.190
And I'll take any questions.

00:54:28.190 --> 00:54:29.390
Yeah.

00:54:29.390 --> 00:54:32.989
&gt;&gt; [INDISTINCT]
&gt;&gt; Oh, yeah.

00:54:32.989 --> 00:54:35.749
Is there--I guess there's a microphone for
questions there.

00:54:35.749 --> 00:54:38.949
Now, you're third in line.

00:54:38.949 --> 00:54:42.160
&gt;&gt; Let me just ask you.

00:54:42.160 --> 00:54:45.349
You mentioned about the masters, the masters
and the slaves.

00:54:45.349 --> 00:54:49.159
Are you using any special hardware for the
masters compared to the slaves?

00:54:49.159 --> 00:54:50.159
&gt;&gt; DEAN: No.

00:54:50.159 --> 00:54:51.159
&gt;&gt; It's the same?

00:54:51.159 --> 00:54:54.920
&gt;&gt; DEAN: Yeah, it's basically, you know, our
scheduling system will pick, you know, a hundred

00:54:54.920 --> 00:54:59.249
and one machines, one of which will be started
up as the master and a hundred of which might

00:54:59.249 --> 00:55:00.449
be workers.

00:55:00.449 --> 00:55:01.839
&gt;&gt; Okay, the same hardware?

00:55:01.839 --> 00:55:02.839
&gt;&gt; DEAN: Same hardware.

00:55:02.839 --> 00:55:03.839
&gt;&gt; Thank you.

00:55:03.839 --> 00:55:04.839
&gt;&gt; DEAN: It makes things simpler.

00:55:04.839 --> 00:55:07.691
If you end up with a small number of machines
that are very specialized for masters, it's

00:55:07.691 --> 00:55:10.099
kind of an operations nightmare.

00:55:10.099 --> 00:55:17.190
&gt;&gt; Google is pretty good about scaling across
the--a bunch of distributed machines, how

00:55:17.190 --> 00:55:21.640
about scaling kind of deep down into the new
multi-cores that are going to come out?

00:55:21.640 --> 00:55:24.989
Are you going to do anything special or just
follow the commodity?

00:55:24.989 --> 00:55:32.209
&gt;&gt; DEAN: Well, we really, really like multi-core
machines because we've already had to parallelize

00:55:32.209 --> 00:55:38.549
our computations across lots and lots of machines
and essentially, to us, multi-core machines

00:55:38.549 --> 00:55:43.180
just look like lots of little machines with
really good interconnects.

00:55:43.180 --> 00:55:49.040
And so from that perspective, I think it's
relatively easy for us to use multi-core machines

00:55:49.040 --> 00:55:53.430
because we're already sort of used to parallelizing
things, breaking things and running multiple

00:55:53.430 --> 00:55:54.809
threads anyway.

00:55:54.809 --> 00:55:57.230
So, yeah, multi-core machines are great.

00:55:57.230 --> 00:56:02.489
We're very happy because single-thread performance
doesn't matter to us at all, not completely

00:56:02.489 --> 00:56:09.229
at all, but to a large extent, we have lots
of parallelizable problems and we're totally

00:56:09.229 --> 00:56:11.650
happy breaking them up.

00:56:11.650 --> 00:56:15.190
&gt;&gt; I have a question about overall architecture.

00:56:15.190 --> 00:56:20.569
There seems to be a trend nowadays, you've
talked of vendors like Sun, they tell you

00:56:20.569 --> 00:56:27.000
go get a Blade system and the blades don't
have disks on them and then they talk to some

00:56:27.000 --> 00:56:29.059
other kind of disk system over here.

00:56:29.059 --> 00:56:34.009
Do you think that's a mistake in your opinion
to go kind of separated protection?

00:56:34.009 --> 00:56:38.839
&gt;&gt; DEAN: Well, I think--you know, I guess
you're talking about storage area networks

00:56:38.839 --> 00:56:43.519
or, you know, fiber channel based disks where
you can access them for multiple machines.

00:56:43.519 --> 00:56:49.239
&gt;&gt; So you're--all the Google stuff seems to
say, okay, do the computation and data in

00:56:49.239 --> 00:56:50.559
the same place.

00:56:50.559 --> 00:56:55.859
&gt;&gt; DEAN: I mean, I think those systems have
their place but they--for example, if you're

00:56:55.859 --> 00:56:59.490
running a commercial database and you need
a really high performance disk system, you

00:56:59.490 --> 00:57:05.069
might buy one of those because that's a really
easy way for you to get a high performance

00:57:05.069 --> 00:57:07.930
disk system that can run underneath the commercial
database.

00:57:07.930 --> 00:57:13.759
But in our case, we--you know, we're happy
using cheap disks attached to lots and lots

00:57:13.759 --> 00:57:17.309
of machines because you get a lot more spindles
for the same price, you get a lot more disk

00:57:17.309 --> 00:57:21.249
storage capacity for the same price, and you
got a lot more computation near your disk

00:57:21.249 --> 00:57:22.869
for the same price.

00:57:22.869 --> 00:57:30.940
So, if you're willing to build things yourself
or use, you know, open source versions of

00:57:30.940 --> 00:57:36.769
things like, you know, the [INDISTINCT] project
has an open source MapReduce and an open source

00:57:36.769 --> 00:57:42.230
of version of GFS, those kind of things, I
think, can make it possible to use the techniques

00:57:42.230 --> 00:57:49.150
that we use on lots of commodity machines
and not buying into a large, you know, storage

00:57:49.150 --> 00:57:51.390
area network, for example.

00:57:51.390 --> 00:57:56.440
So--but obviously, you have to evaluate what
makes sense for the kinds of problems you're

00:57:56.440 --> 00:57:58.300
trying to solve.

00:57:58.300 --> 00:58:02.640
&gt;&gt; Speaking of that, some of the problems
here described including distributing it a

00:58:02.640 --> 00:58:06.780
large number of machines seem to be pretty
unique to Google and their--especially their

00:58:06.780 --> 00:58:08.619
solutions seem to be pretty unique to Google.

00:58:08.619 --> 00:58:11.670
So you can't buy a commercial database like
Oracle then.

00:58:11.670 --> 00:58:15.849
You have to you're your own files [INDISTINCT]
does that create a challenge in terms of being

00:58:15.849 --> 00:58:18.240
able to train new set of recruits?

00:58:18.240 --> 00:58:23.880
How long does it take for a typical programmer
to join and be productive and come up with

00:58:23.880 --> 00:58:24.880
a new application?

00:58:24.880 --> 00:58:25.880
&gt;&gt; DEAN: Right.

00:58:25.880 --> 00:58:30.599
So, I think one of the things that we've done
a pretty good job at is because we've been

00:58:30.599 --> 00:58:37.559
hiring quite quickly and growing pretty fast,
we've had to create a lot of online documentation

00:58:37.559 --> 00:58:43.039
and sort of initial training material to teach
people about the systems we've built and I

00:58:43.039 --> 00:58:44.380
think we've done a pretty good job of that.

00:58:44.380 --> 00:58:49.019
So, when someone is hired, they go in and
can pick and choose from a collection of,

00:58:49.019 --> 00:58:55.710
like, 10 or 12 kind of half day courses and
about, you know, how to use MapReduce or how

00:58:55.710 --> 00:58:59.359
to use GFS or how to use our source control
system, that kind of thing.

00:58:59.359 --> 00:59:03.380
And so I think, you know, we've gotten pretty
good at bringing people up to speed relatively

00:59:03.380 --> 00:59:04.710
quickly.

00:59:04.710 --> 00:59:07.430
That said, there is a lot of stuff to learn
when people join.

00:59:07.430 --> 00:59:12.759
And I've heard new people who are fairly experienced
when they join, say, wow, that's--there's

00:59:12.759 --> 00:59:15.980
a lot--it's like drinking from a fire hose
kind of thing.

00:59:15.980 --> 00:59:23.059
But, you know, I think the desire to just
jump in and learn stuff is the main thing

00:59:23.059 --> 00:59:29.959
that will keep people above water.

00:59:29.959 --> 00:59:31.180
Okay.

00:59:31.180 --> 00:59:32.459
I gather we're out of time.

00:59:32.459 --> 00:59:44.640
I'm happy to hang around in the hall and answer
questions if people have them.

