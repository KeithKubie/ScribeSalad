WEBVTT
Kind: captions
Language: en

00:00:00.650 --> 00:00:05.670
Lots of work to be done by the runtime system, to manage and map_reduce

00:00:05.670 --> 00:00:10.160
computation. The master data structures include the

00:00:10.160 --> 00:00:13.170
location of files created by the completed mappers.

00:00:13.170 --> 00:00:16.760
Remember that each mapper is working on

00:00:16.760 --> 00:00:19.820
some node of the computational cluster. Producing

00:00:19.820 --> 00:00:26.610
its output as files on its local disk. So the master has to have knowledge

00:00:26.610 --> 00:00:34.460
of where those files reside. Created by those mappers, and the namespace of

00:00:34.460 --> 00:00:40.350
the files that have been created by the mappers. And it also has to keep a score

00:00:40.350 --> 00:00:43.490
board, of the mappers and reducers that

00:00:43.490 --> 00:00:47.080
are currently assigned to work on different splits.

00:00:47.080 --> 00:00:48.960
Because the number of machines that may be

00:00:48.960 --> 00:00:52.080
available, may be less than the total number

00:00:52.080 --> 00:00:54.810
of machines that will be needed. If I wanted to

00:00:54.810 --> 00:00:58.620
do all of the input m splits in parallel and

00:00:58.620 --> 00:01:02.540
all of the R output splits in parallel. And therefore,

00:01:02.540 --> 00:01:05.690
the scoreboard is saying, at any point in time, who are

00:01:05.690 --> 00:01:08.100
all the workers carrying all the mapper functions? Who are

00:01:08.100 --> 00:01:10.700
all the workers carrying all the reducer functions? When are they

00:01:10.700 --> 00:01:14.720
done? And when they are done. How should I reassign

00:01:14.720 --> 00:01:17.280
that worker to a new task. So these are the kind

00:01:17.280 --> 00:01:21.330
of things that the master data structures facilitate

00:01:21.330 --> 00:01:24.360
the master to do. Then the big thing is

00:01:24.360 --> 00:01:27.630
fault tolerance. For a variety of reasons, the

00:01:27.630 --> 00:01:30.710
master may find, that an instance, of a map

00:01:30.710 --> 00:01:33.640
function that it started, is not responding in

00:01:33.640 --> 00:01:36.890
a timely manner. Maybe that node is down for

00:01:36.890 --> 00:01:38.820
some reason, maybe the link is down for

00:01:38.820 --> 00:01:42.280
some reason, or maybe that processor is taking a

00:01:42.280 --> 00:01:46.070
little more time than what the master

00:01:46.070 --> 00:01:49.080
expects. It can happen very easily, because in

00:01:49.080 --> 00:01:51.870
a. Data center environment where you have

00:01:51.870 --> 00:01:54.520
thousands of machines, there could be some machines

00:01:54.520 --> 00:01:56.610
that are slower than other machines, there

00:01:56.610 --> 00:02:01.070
could be generational differences between the machines that

00:02:01.070 --> 00:02:03.360
populate the data center, even though they

00:02:03.360 --> 00:02:07.140
may be homogenous in terms of the capabilities.

00:02:07.140 --> 00:02:11.530
Programming alignments and even the machine architecture. They

00:02:11.530 --> 00:02:14.990
may have speeds that are different because they

00:02:14.990 --> 00:02:17.830
correspond to different generations of processes. In any

00:02:17.830 --> 00:02:20.970
event, what might happen is that a master may

00:02:20.970 --> 00:02:24.070
notice that there is no timely response from

00:02:24.070 --> 00:02:27.190
a particular instance. Let's say of a mapper. In

00:02:27.190 --> 00:02:28.960
that case, it might say well I'm going

00:02:28.960 --> 00:02:32.110
to go ahead assume that that mapper is dead.

00:02:32.110 --> 00:02:34.670
I'm going to restart that mapping function on

00:02:34.670 --> 00:02:37.320
a different node of the cluster. Now, when

00:02:37.320 --> 00:02:41.460
that happens it is possible that the original

00:02:41.460 --> 00:02:44.390
mapper is actually dead, or it could be

00:02:44.390 --> 00:02:47.130
that it was just slow. In that case,

00:02:47.130 --> 00:02:49.730
you could get completion message from more than

00:02:49.730 --> 00:02:53.150
one mapper for the same split. So when

00:02:53.150 --> 00:02:57.780
you get multiple completion messages from redundant stragglers,

00:02:57.780 --> 00:03:00.300
the master has to be able to filter

00:03:00.300 --> 00:03:03.820
the mordancy. This is something that is redundant work.

00:03:03.820 --> 00:03:05.720
I don't have to care about that. So that's

00:03:05.720 --> 00:03:08.660
part of fault tolerance that the master has to

00:03:08.660 --> 00:03:12.950
worry about. Locality management is another important thing. In

00:03:12.950 --> 00:03:16.690
the memory hierarchy, making sure that the working set

00:03:16.690 --> 00:03:20.570
of computations fit in the closest level of the

00:03:20.570 --> 00:03:23.290
memory hierarchy of a process is very important, and

00:03:23.290 --> 00:03:25.500
this is another thing that has to

00:03:25.500 --> 00:03:29.080
be managed very carefully so that the computation

00:03:29.080 --> 00:03:32.480
can make good forward progress. In completing the

00:03:32.480 --> 00:03:36.050
map produce function. There is an inherent assumption

00:03:36.050 --> 00:03:38.270
in the fault tolerance model of the

00:03:38.270 --> 00:03:42.620
map produce framework. And that is item potency

00:03:42.620 --> 00:03:48.460
of the mapper. That is, even though the same input data split is being worked on

00:03:48.460 --> 00:03:52.050
by multiple mappers, It doesn't affect the

00:03:52.050 --> 00:03:53.960
semantics of the computation as a whole,

00:03:53.960 --> 00:03:55.640
and that is the reason why the

00:03:55.640 --> 00:03:59.660
master can simply ignore the redundant stragglers'

00:03:59.660 --> 00:04:05.510
message if in fact it was a slow computation engine that was working on a

00:04:05.510 --> 00:04:13.480
particular map function. And if it finishes later than when it was supposed to,

00:04:13.480 --> 00:04:18.680
and the master has already started another redundant

00:04:18.680 --> 00:04:23.470
computation to take care of that particular split, ignoring that

00:04:23.470 --> 00:04:28.820
redundant straggler message. Is okay because of the item potency assumption

00:04:28.820 --> 00:04:32.960
about the mapping operation. In a similar manner, the master may assign a new

00:04:32.960 --> 00:04:35.770
node to carry out reduced function corresponding

00:04:35.770 --> 00:04:38.510
to a particular split if there's no timely

00:04:38.510 --> 00:04:41.700
response from one of the reduced workers. And

00:04:41.700 --> 00:04:44.910
here again, the output file generation that is

00:04:44.910 --> 00:04:49.750
associated with a particular. A reducer split depends

00:04:49.750 --> 00:04:52.910
on the atomicity of the rename system call

00:04:52.910 --> 00:04:57.990
that is used to finally say that oh, this is the old profile generated, and the

00:04:57.990 --> 00:05:00.050
master says okay, I'm going to commit this

00:05:00.050 --> 00:05:03.690
as the real output file of the computation.

00:05:03.690 --> 00:05:06.570
Each reducer is writing a local file, and

00:05:06.570 --> 00:05:09.680
finally when the master gets the notification from

00:05:09.680 --> 00:05:12.460
the reducer, that it has completed its work,

00:05:12.460 --> 00:05:16.780
at that point, master renames that local file

00:05:16.780 --> 00:05:19.050
as the final output file, and this is

00:05:19.050 --> 00:05:22.320
where can get rid of redundant stranglers who

00:05:22.320 --> 00:05:24.050
come along later and say I produce the

00:05:24.050 --> 00:05:28.740
same output file for the same split. Master will

00:05:28.740 --> 00:05:33.030
say, I already got it, I don't need this, and it can ignore the completion

00:05:33.030 --> 00:05:36.660
message from that redundant straggler. The other

00:05:36.660 --> 00:05:39.230
thing that the programming environment has to worry

00:05:39.230 --> 00:05:42.280
about is locality management, and this is

00:05:42.280 --> 00:05:48.250
accomplished using the Google file system that provides

00:05:48.250 --> 00:05:50.600
a way by which, efficiently, data can

00:05:50.600 --> 00:05:54.090
be. Migrated from the mappers to the reducers.

00:05:54.090 --> 00:05:57.670
Task granularity is another important issue. As I

00:05:57.670 --> 00:06:00.920
mentioned, the number of nodes available in the

00:06:00.920 --> 00:06:05.940
computation cluster may be less than the sum M plus R, where M is the number

00:06:05.940 --> 00:06:08.140
of input splits, and R is the number

00:06:08.140 --> 00:06:11.970
of. Reducer splits. And it is the responsibility

00:06:11.970 --> 00:06:15.660
of the programming framework to come up with

00:06:15.660 --> 00:06:19.180
the right task granularity so that there can

00:06:19.180 --> 00:06:21.260
be good load balance of the computational

00:06:21.260 --> 00:06:24.740
resources. And the programming framework also offers

00:06:24.740 --> 00:06:27.720
several refinements to the basic model. The

00:06:27.720 --> 00:06:31.200
way the partitioning of the input data space

00:06:31.200 --> 00:06:37.260
is done, is by using a hash function that is built into the programming

00:06:37.260 --> 00:06:40.445
environment. But the user can override that

00:06:40.445 --> 00:06:44.346
partitioning function with his or her own partitioning

00:06:44.346 --> 00:06:47.583
function if they think that that'll result in

00:06:47.583 --> 00:06:50.496
a better way of organizing the data. And

00:06:50.496 --> 00:06:53.016
in the map reduced framework, there is no

00:06:53.016 --> 00:06:56.976
interaction between the mappers, but if there's some ordering

00:06:56.976 --> 00:06:59.856
guarantees needed in terms of ordering the keys

00:06:59.856 --> 00:07:02.376
and so on, that is something that the user

00:07:02.376 --> 00:07:05.120
may have to take care of. The other

00:07:05.120 --> 00:07:10.120
thing that the user may also do is combining.

00:07:10.120 --> 00:07:13.040
A partial merging to increase the granularity

00:07:13.040 --> 00:07:16.520
of the tasks that execute a mapping function.

00:07:16.520 --> 00:07:19.460
Remember that when we looked at the simple

00:07:19.460 --> 00:07:23.078
example of looking for specific names and input

00:07:23.078 --> 00:07:29.960
corpus of data, I mentioned that. The mapper can be as simple as emitting a one

00:07:29.960 --> 00:07:32.630
for a value every time it encounters the

00:07:32.630 --> 00:07:35.310
name Kishore for instance in an input file.

00:07:35.310 --> 00:07:38.230
Or, it could take an input file and count

00:07:38.230 --> 00:07:41.030
all the occurrences of Kishore in that input file

00:07:41.030 --> 00:07:44.110
and finally emit the total value as the output

00:07:44.110 --> 00:07:46.060
of the mapper. That kind of combining function is

00:07:46.060 --> 00:07:49.956
something that. A user may choose to incorporate in

00:07:49.956 --> 00:07:53.388
writing his mapping and reduce functions. It is very

00:07:53.388 --> 00:07:56.412
important to recognize that in order for the fault

00:07:56.412 --> 00:08:00.588
tolerance model of the programming environment to work correctly,

00:08:00.588 --> 00:08:03.950
map and reduce functions have to be item potent.

00:08:03.950 --> 00:08:07.600
That's a fundamental assumption. That the fault tolerant monitoring

00:08:07.600 --> 00:08:10.780
of the programming framework is based on. And there

00:08:10.780 --> 00:08:14.590
are also other bells and whistles in the programming framework

00:08:14.590 --> 00:08:17.350
for getting status information and logs and so on

00:08:17.350 --> 00:08:20.270
and so forth, but the basic idea I want to

00:08:20.270 --> 00:08:23.250
leave you with is the fact that the programming

00:08:23.250 --> 00:08:26.080
framework is a very simple one. It has two operations,

00:08:26.080 --> 00:08:32.280
map and reduce and using those two operations. You can specify a

00:08:32.280 --> 00:08:37.850
complex application that you want to call up, to run on a data

00:08:37.850 --> 00:08:41.780
center, using the computation resources that's available in the data center.

