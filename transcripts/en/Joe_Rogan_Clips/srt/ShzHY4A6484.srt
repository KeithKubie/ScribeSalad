1
00:00:00,180 --> 00:00:02,760
Well,
the Joe Rogan experience,

2
00:00:03,130 --> 00:00:04,600
you had a injury Yang here,

3
00:00:04,660 --> 00:00:09,250
like a million people asked me to talk
about Ubi [inaudible] so I, um, I,

4
00:00:09,251 --> 00:00:11,030
you still support her
at Uva. I think, yeah,

5
00:00:11,340 --> 00:00:16,340
we're probably going to have to do
something if I don't only argument against

6
00:00:16,891 --> 00:00:19,650
ubi in my eyes is human nature.

7
00:00:20,880 --> 00:00:25,880
The idea that we could possibly take
all these people that have no idea where

8
00:00:26,781 --> 00:00:31,320
the next meal's coming from and eliminate
that and always have a place to stay.

9
00:00:31,650 --> 00:00:33,860
And then from there on,
you're on your own. Yeah.

10
00:00:33,990 --> 00:00:36,180
But that's what universal basic
income essentially covers,

11
00:00:36,210 --> 00:00:39,840
covers food enough for food, right.
You're not going to starve to death.

12
00:00:40,080 --> 00:00:43,320
You're not going to be rich. It's not
like you could just live high on the hog,

13
00:00:44,130 --> 00:00:49,130
but you gotta wonder what the fuck the
world looks like when we lose millions

14
00:00:51,630 --> 00:00:53,880
and millions and millions of jobs
almost instantly due to automation.

15
00:00:54,750 --> 00:00:57,780
Yeah. It's a, it's a really
interesting question,

16
00:00:57,781 --> 00:00:59,330
especially with Andrew Ng's positions.

17
00:00:59,331 --> 00:01:03,900
So there's a lot of economics questions
then ubi, I think the spirit of it,

18
00:01:04,260 --> 00:01:06,410
just like, I agree with you,
we have to do something.

19
00:01:06,530 --> 00:01:10,730
Yeah, the economic seem
kind of questionable. Right?
There's $1,000 a month.

20
00:01:10,760 --> 00:01:11,331
Is that what it is?

21
00:01:11,331 --> 00:01:16,331
I thought for him it's a thousand
dollars thousand dollars a month for 300

22
00:01:16,820 --> 00:01:20,390
million people. So it's difficult
to, not to everybody. No,

23
00:01:20,391 --> 00:01:22,730
because the way I heard him explain
it, everybody, if you're 18,

24
00:01:22,760 --> 00:01:25,700
if you're already getting some sort of
welfare, you wouldn't get that thousand.

25
00:01:25,701 --> 00:01:27,680
You would get like the
difference of the thousand.

26
00:01:27,681 --> 00:01:28,970
So if you're already
taking money in some way,

27
00:01:28,971 --> 00:01:31,310
you just get like an extra 200 bucks,
something like that.

28
00:01:31,311 --> 00:01:35,300
So the thousand gets factored in.
Yeah. Yeah. So if you are wealthy,

29
00:01:35,301 --> 00:01:37,910
you get it too though. And you could
opt out, right? That was the idea.

30
00:01:39,180 --> 00:01:42,960
Yeah. So it's like, uh, like
everything else, it's super messy.

31
00:01:42,961 --> 00:01:47,190
So what is the right, what is the
right amount and how do we pay for it?

32
00:01:47,580 --> 00:01:51,310
And ultimately the problem is,
uh,

33
00:01:51,930 --> 00:01:52,980
helping people,

34
00:01:53,130 --> 00:01:58,130
giving them financial grounding to find
meaningful employment or just meaning in

35
00:01:58,711 --> 00:02:02,280
their life. And the, the main thing
of a job isn't just the money.

36
00:02:02,790 --> 00:02:07,790
It's finding meaning and purpose and
the re derive your identity from work.

37
00:02:08,131 --> 00:02:12,810
I mean that's, maybe that's one
of the downsides of us, uh, human,

38
00:02:12,870 --> 00:02:15,960
human that the biology is,
we kind of crave that meaning.

39
00:02:16,800 --> 00:02:18,790
And the question I,

40
00:02:18,900 --> 00:02:22,230
he has a lot of other ideas
around besides just ubi,

41
00:02:22,590 --> 00:02:25,530
but ubi by itself does not
simply provide that meaning.

42
00:02:26,040 --> 00:02:29,430
And that's a really difficult
question of what do we do next?

43
00:02:29,460 --> 00:02:31,560
What kind of retraining,
what kind of,

44
00:02:31,710 --> 00:02:35,850
how do we help people educate
themselves over their life? Right.

45
00:02:36,170 --> 00:02:40,970
And that's the real question. Yeah. This,
the end and the, and the other balances.

46
00:02:41,710 --> 00:02:46,260
Any underlying all of this.
So one of the things I disagree with it,

47
00:02:46,310 --> 00:02:50,910
uh, Andrew Yang on is
the, the fearmongering,

48
00:02:51,390 --> 00:02:53,610
which I think in this culture we have,

49
00:02:53,910 --> 00:02:56,910
you have to do as a presidential
candidate, that might be part of the game.

50
00:02:57,900 --> 00:03:02,470
But the, the mongering of saying that we
should really be afraid of automation,

51
00:03:03,020 --> 00:03:04,960
that automation is going
to take a lot of jobs.

52
00:03:05,470 --> 00:03:08,590
And from my understanding of the
technology, from everything I see,

53
00:03:08,800 --> 00:03:13,750
that is not going to be as
drastic or as fast as as he says.

54
00:03:14,350 --> 00:03:19,240
And, but then how much do you think
he's exaggerating by in your estimation?

55
00:03:19,680 --> 00:03:22,850
He, well, he doesn't, I mean, not
even exaggerating. What, what, how,

56
00:03:22,910 --> 00:03:27,640
how much do you differ on his prognosis?
I think,

57
00:03:28,000 --> 00:03:32,180
I think he doesn't really
provide significant, like
a specific prognostics is,

58
00:03:32,181 --> 00:03:36,370
nobody knows it's a, there's a lot of
uncertainty. More about the spirit of the,

59
00:03:36,380 --> 00:03:39,940
the language used.
I think AI will,

60
00:03:40,720 --> 00:03:41,710
technology,

61
00:03:41,770 --> 00:03:46,300
AI and automation will do a lot of good.

62
00:03:47,440 --> 00:03:48,490
The question is,

63
00:03:48,600 --> 00:03:53,600
it's a much deeper question about our
society of that balances capitalism versus

64
00:03:54,911 --> 00:03:59,860
socialism and nobody, I don't
think, if you're honest, it,

65
00:04:00,120 --> 00:04:03,220
capitalism is not bad.
Socialism is not bad.

66
00:04:03,520 --> 00:04:06,160
You have to grab ideas from each.
You have to,

67
00:04:06,660 --> 00:04:11,630
there you have to both
reward the crazy broke, uh,

68
00:04:12,820 --> 00:04:16,570
entrepreneur who dreams of creating
the next billion dollar startup that

69
00:04:16,750 --> 00:04:19,360
improves the world in
some fundamental way.

70
00:04:19,540 --> 00:04:22,990
The Ilan Musk has been broken
many times creating that startup.

71
00:04:23,020 --> 00:04:27,720
And you also have to empower the people
who just lost their job because there

72
00:04:27,721 --> 00:04:29,630
were data entry,
uh,

73
00:04:29,660 --> 00:04:34,540
their data entry job of some basic data
manipulation data management that was

74
00:04:34,541 --> 00:04:38,590
just replaced by a piece of software.
So that's,

75
00:04:38,620 --> 00:04:42,880
that's a social net that's needed. And
the question is how do we balance that?

76
00:04:42,880 --> 00:04:46,990
That doesn't have to do, that's
not new, that's not new to Ai.

77
00:04:47,200 --> 00:04:49,900
And when the,
the word automation is used,

78
00:04:50,230 --> 00:04:55,230
it's really not correctly attributing
where the biggest changes will happen.

79
00:04:56,861 --> 00:05:01,000
It's not AI, it's simply technology
of all kinds of software.

80
00:05:01,240 --> 00:05:05,830
It's pretty Dee Dee
digitalization of information.

81
00:05:06,220 --> 00:05:10,300
So a data entry becoming much more,
uh,

82
00:05:10,330 --> 00:05:14,920
much more automated, some basic
repetitive tasks. Uh, I think,

83
00:05:16,750 --> 00:05:19,600
I think the questions there aren't about,

84
00:05:19,630 --> 00:05:23,050
so the enemy isn't there.
First of all, there's no enemy,

85
00:05:23,051 --> 00:05:28,051
but it certainly isn't AI or automation
because I think AI and automation will

86
00:05:28,570 --> 00:05:31,350
help make a help,

87
00:05:31,360 --> 00:05:36,120
make a better world and a salad.
Your spokesperson for AI and automation.

88
00:05:36,280 --> 00:05:39,910
I am. I am. I am Anna for Ubi. I think I,

89
00:05:39,911 --> 00:05:44,911
I think we have to give fun
people financial freedom to learn,

90
00:05:46,210 --> 00:05:50,440
like lifelong learning and flexibility
to find meaningful employment.

91
00:05:50,680 --> 00:05:54,760
But like AI isn't the enemy.
I see what you're saying. Um,

92
00:05:54,790 --> 00:05:57,950
but what do you think ever could be done

93
00:05:57,980 --> 00:06:02,090
to give people meaning this, this
meaning thing? I agree with you.

94
00:06:02,330 --> 00:06:05,000
Giving people just money enough to
survive doesn't make them happy.

95
00:06:05,001 --> 00:06:08,750
And if you look at any dystopian movie
about the future Mad Max instead,

96
00:06:08,751 --> 00:06:09,710
it's like,
what is it?

97
00:06:09,711 --> 00:06:14,420
Society's Gone Fay wire and people are
like ragamuffins running through the

98
00:06:14,421 --> 00:06:17,660
streets and everyone's dirty and that
shooting each other and shit. Right?

99
00:06:17,661 --> 00:06:19,280
And that's what we're
really worried about. Well,

100
00:06:19,281 --> 00:06:24,230
we're really worried about some crazy
future where the rich people live in these

101
00:06:24,231 --> 00:06:29,231
like protected sky rises with helicopter
circling over him and down in the

102
00:06:29,961 --> 00:06:33,140
bottom it's desert chaos, right?
That's what we're worried about.

103
00:06:33,350 --> 00:06:35,470
So suddenly you'd be as a part of that.
Uh,

104
00:06:35,550 --> 00:06:39,710
so providing some backing some what any
kind of welfare program is a part of

105
00:06:39,711 --> 00:06:40,190
that,

106
00:06:40,190 --> 00:06:44,300
but also much more seriously looking at
our broken education system throughout.

107
00:06:44,330 --> 00:06:46,310
Yes. I mean, it's just like,

108
00:06:46,311 --> 00:06:51,110
not blaming AI or technology,
which are all inevitable developments,

109
00:06:51,111 --> 00:06:54,960
which I think will make a better
world. But saying we need to, uh,

110
00:06:55,280 --> 00:06:59,720
do lifelong learning education,
make it a lifestyle,

111
00:06:59,870 --> 00:07:01,940
invest in it.
Not Stupid,

112
00:07:01,941 --> 00:07:05,570
a rote learning memorization that we do.

113
00:07:05,571 --> 00:07:10,120
It's the sort of the way mathematics
and engineering and chemistry, biology,

114
00:07:10,130 --> 00:07:13,550
the sciences, and even art is
approached in high school and so on.

115
00:07:13,730 --> 00:07:16,970
But looking at education
as a lifelong thing,

116
00:07:17,150 --> 00:07:21,270
finding passion and like that
should be the big focus, the,

117
00:07:21,271 --> 00:07:22,400
the big investment.

118
00:07:23,000 --> 00:07:27,410
It's investing in the
knowledge and development of
knowledge of young people and

119
00:07:27,740 --> 00:07:32,120
everybody. So it's not learned
to code. It's just learn.

120
00:07:32,350 --> 00:07:33,620
Mm.
I couldn't agree more.

121
00:07:33,680 --> 00:07:37,010
And I also think you're always gonna have
a problem with people just not doing a

122
00:07:37,011 --> 00:07:40,790
really good job of raising children and,
you know,

123
00:07:40,820 --> 00:07:45,410
screwing them up and, you know,
making kids, there's a lot,

124
00:07:45,430 --> 00:07:48,230
a lot of people out there that
have terrible traumatic childhoods.

125
00:07:48,650 --> 00:07:52,880
There's just to fix that with universal
basic income, just to say, oh,

126
00:07:52,890 --> 00:07:55,590
we're gonna give you $1,000 a month
to hope are going to be happy. I was,

127
00:07:55,880 --> 00:07:57,510
that's not going to fix that.
You know,

128
00:07:57,540 --> 00:08:01,700
we have to figure out how to fix
the whole human race. You know?

129
00:08:01,701 --> 00:08:06,701
And I think there's a very little effort
that's put into thinking about how to

130
00:08:08,391 --> 00:08:13,391
prevent so much shitty parenting and how
to prevent so many kids growing up in

131
00:08:13,671 --> 00:08:17,690
bad neighborhoods and poverty
and crime and violence.

132
00:08:17,691 --> 00:08:21,680
And that's where a giant
chunk of all of our,

133
00:08:22,370 --> 00:08:27,020
the momentum of this chaos that a lot
of people carry with them into adulthood

134
00:08:27,021 --> 00:08:30,260
comes from, it comes from things beyond
their control when they're young.

135
00:08:30,650 --> 00:08:34,280
And that is the struggle at the core of
our society at the core of our country.

136
00:08:34,281 --> 00:08:37,170
That's bigger than raising humans.
Yeah.

137
00:08:37,190 --> 00:08:40,280
Raising and educating
humans making and you know,

138
00:08:40,580 --> 00:08:45,170
making a better world where people
get along with each other better,

139
00:08:45,171 --> 00:08:48,230
where it's pleasing for all of us.
Like we were talking about earlier,

140
00:08:48,231 --> 00:08:52,790
the thing that most of us agree on,
at least to a certain extent.

141
00:08:52,810 --> 00:08:57,720
So we enjoy people. We might not enjoy all
of them, but there's the ones we enjoy,

142
00:08:57,721 --> 00:09:01,170
we enjoy, and you really don't enjoy
being alone unless you're one of them.

143
00:09:01,171 --> 00:09:04,320
Ted Kaczynski type characters.
All those people that are like,

144
00:09:04,321 --> 00:09:07,350
I'm alone or like fuck you.
You are. Fuck you. You are.

145
00:09:07,410 --> 00:09:10,680
And you might like to spend some time
alone. You don't want to be in solitary,

146
00:09:10,681 --> 00:09:15,390
man. He don't want to be alone in the
forest with no one like Tom Hanks and cast

147
00:09:15,391 --> 00:09:19,740
away. You'll go fucking crazy. It's
not good for you. It's just not. Yeah,

148
00:09:19,741 --> 00:09:22,710
people get annoying. Fuck yeah,
I'm annoyed with me right now.

149
00:09:23,000 --> 00:09:27,180
Then listen to me for three hours. I'm
annoyed with me. People get annoying,

150
00:09:27,360 --> 00:09:29,430
but we like each other.
We really do.

151
00:09:29,520 --> 00:09:34,520
And the more we can figure out how to
make it a better place for these people

152
00:09:35,311 --> 00:09:39,180
that got a shitty roll. The dice that
grew up in poverty, that grew up in crime,

153
00:09:39,181 --> 00:09:42,960
they grew up with abusive parents. The
more we can figure out how to help them.

154
00:09:43,670 --> 00:09:46,440
And I don't know what that answer
is, you know, but I suspect

155
00:09:47,940 --> 00:09:50,760
if we put enough resources to it,
we could probably put a dent in it.

156
00:09:50,790 --> 00:09:53,640
At least if we really start thinking
about at least it would put the

157
00:09:53,641 --> 00:09:54,660
conversation out there.

158
00:09:54,661 --> 00:09:58,830
Like he can't pretend that this is a
just capitalism in this country when so

159
00:09:58,831 --> 00:10:03,660
many people were born like way
far behind the game, like way,

160
00:10:03,690 --> 00:10:04,500
way fucked.

161
00:10:04,500 --> 00:10:08,850
I mean if you're growing up right now
and you're in West Virginia in a fucking

162
00:10:08,851 --> 00:10:13,851
coal coal town and everyone's on pills
and it's just chaos and crime and face

163
00:10:16,141 --> 00:10:19,080
tattoos and fucking get
your teeth knocked out,

164
00:10:19,590 --> 00:10:22,770
what are you going to do?
I don't want to hear any of that.

165
00:10:22,770 --> 00:10:25,470
Pull yourself up by your bootstraps.
Bullshit man.

166
00:10:25,800 --> 00:10:29,190
Cause if you're growing up in an
environment like that, you, you,

167
00:10:29,220 --> 00:10:33,090
you're so far behind and
everyone around you is fucked up.

168
00:10:33,420 --> 00:10:36,720
And there's a lot of folks out there
listening to this that can relate to that.

169
00:10:37,560 --> 00:10:39,060
If we don't do something about that.

170
00:10:39,330 --> 00:10:44,190
If we don't do something about the crime
and the poverty and the chaos that so

171
00:10:44,191 --> 00:10:48,000
many people have to go through
every day just to survive until we,

172
00:10:48,660 --> 00:10:53,310
we shouldn't be looking at anything
else where all this traveling to other

173
00:10:53,311 --> 00:10:57,300
countries to fuck things up
and metal here and metal there.

174
00:10:57,450 --> 00:11:00,420
We should be fixing this first.

175
00:11:00,930 --> 00:11:04,890
We're like a person who yells at someone
for having a shitty lawn when our

176
00:11:04,891 --> 00:11:09,470
houses in array, full chaos
plants growing everywhere. It's,

177
00:11:09,530 --> 00:11:12,150
it's goofy. We're goofy. We, we, we,

178
00:11:12,330 --> 00:11:17,190
we almost like are waking up in the
middle of something that's already been in

179
00:11:17,191 --> 00:11:20,090
motion for hundreds of years and we
were like, well, what do we do? We,

180
00:11:20,160 --> 00:11:22,620
is this the right direction?
We'd go, we, okay,

181
00:11:22,740 --> 00:11:26,700
we're flying in this spaceship,
this spaceship earth,

182
00:11:26,850 --> 00:11:31,530
and in the middle of our lives we're
just realizing that we are now the adults

183
00:11:31,860 --> 00:11:35,820
and that all the adults that are running
everything on this planet are not that

184
00:11:35,821 --> 00:11:38,670
much different than you and I,
not that much.

185
00:11:38,700 --> 00:11:41,970
I mean like Elon Musk is way smarter
than me, but he's still human.

186
00:11:42,150 --> 00:11:45,900
You know what I mean? So he's probably
fucked up too. So everybody's fucked up.

187
00:11:45,930 --> 00:11:50,030
The whole world is filled with these
fucked up apes that are piloting this

188
00:11:50,031 --> 00:11:54,490
spaceship and you're waking up in the
middle of thousands of years of history

189
00:11:55,090 --> 00:11:57,160
and no one knows if we've
been doing it right along.

190
00:11:57,161 --> 00:11:58,630
We just know that got us to this point.

191
00:11:58,780 --> 00:12:02,350
So do we continue the same stupid fucking
patterns or do we just take a step

192
00:12:02,351 --> 00:12:07,120
back and go, hey, how should we really do
this? How should we do this? Because we,

193
00:12:07,121 --> 00:12:09,730
yeah, what do you got? Like
50 years left, 60 years left.

194
00:12:09,970 --> 00:12:11,440
We just kind of like hang on

195
00:12:11,450 --> 00:12:13,400
to our rubles into the end.

196
00:12:13,550 --> 00:12:16,710
We're going to clutch our
bag of gold in our bucket

197
00:12:16,800 --> 00:12:18,180
diamonds.
Is that what we're going to do?

198
00:12:18,181 --> 00:12:21,540
We're going to live in our mansions
and fly around in our planes.

199
00:12:21,700 --> 00:12:22,533
And I think,
uh,

200
00:12:23,470 --> 00:12:27,700
through the decades now
we've been developing a sense
of empathy that allows us

201
00:12:27,701 --> 00:12:31,620
to understand that Elon
Musk, Joe Rogan, and uh,

202
00:12:31,780 --> 00:12:35,800
somebody in Texas, somebody
in Russia, somebody in India,

203
00:12:36,100 --> 00:12:40,030
all suffer the same kinds of things
all get lonely. I'll get desperate,

204
00:12:41,080 --> 00:12:43,330
all need each other and
all need each other.

205
00:12:43,331 --> 00:12:48,270
And I think technology has a role
to help. They're not hurt. Uh,

206
00:12:48,340 --> 00:12:53,340
but we need to first to first
really acknowledge that we're all,

207
00:12:54,130 --> 00:12:59,130
we're all in this together and we need
to solve the basic problems of humankind

208
00:12:59,770 --> 00:13:03,570
as opposed to investing in sort of
keeping immigrants out or above law.

209
00:13:03,680 --> 00:13:08,530
These kinds of divisive kind of ideas as
opposed to just investing in education,

210
00:13:08,740 --> 00:13:13,540
investing in infrastructure, investing
in the people, uh, Ubi as part of that.

211
00:13:13,541 --> 00:13:16,460
That could be other totally
different solutions. Okay.

212
00:13:21,330 --> 00:13:24,080
[inaudible].

