WEBVTT
Kind: captions
Language: en

00:00:00.996 --> 00:00:38.447
[SIDE CONVERSATION]

00:00:38.447 --> 00:00:39.030
SPEAKER 1: OK.

00:00:39.030 --> 00:00:41.120
So if people are
in the room, we're

00:00:41.120 --> 00:00:45.336
going to start with
the next talk by Alex.

00:00:45.336 --> 00:00:51.784
[SIDE CONVERSATION]

00:00:51.784 --> 00:00:54.810
SPEAKER 1: So I'm happy to
introduce Alex, Alex Chen.

00:00:54.810 --> 00:00:56.640
He's working at
the Creative Lab.

00:00:56.640 --> 00:01:00.135
He has a background in
interactive design, music.

00:01:00.135 --> 00:01:02.010
And he's going to talk
about some of the work

00:01:02.010 --> 00:01:05.874
he's been doing around
teachable machines.

00:01:05.874 --> 00:01:09.190
ALEXANDER CHEN: Hi, everybody.

00:01:09.190 --> 00:01:10.090
Excited to be here.

00:01:10.090 --> 00:01:12.940
So I work at Google.

00:01:12.940 --> 00:01:16.830
I'm mostly based in New
York and in Cambridge,

00:01:16.830 --> 00:01:18.700
the other Cambridge.

00:01:18.700 --> 00:01:22.780
And I work on a team
called Creative Lab.

00:01:22.780 --> 00:01:25.270
You know, my background is in
a little bit of everything,

00:01:25.270 --> 00:01:26.920
a little bit of design,
a little bit of coding,

00:01:26.920 --> 00:01:27.640
and a little bit of music.

00:01:27.640 --> 00:01:29.980
But I'm definitely not a
machine learning expert.

00:01:29.980 --> 00:01:32.955
So in some ways,
part of the projects

00:01:32.955 --> 00:01:36.490
that I'm going to show you
are sort of my first dipping

00:01:36.490 --> 00:01:38.110
my toe into this
world of machine

00:01:38.110 --> 00:01:41.870
learning in collaboration
with a lot of other people.

00:01:41.870 --> 00:01:45.950
So I have a slide with
my now six-year-old.

00:01:45.950 --> 00:01:48.960
Because, as you'll see,
the way that he thinks,

00:01:48.960 --> 00:01:50.950
obviously, kind of fills my day.

00:01:50.950 --> 00:01:54.100
And it actually kind of seeps
into a lot of the projects

00:01:54.100 --> 00:01:56.470
that I'll be sharing today.

00:01:56.470 --> 00:01:58.480
And the project that
I'll be showing, just

00:01:58.480 --> 00:02:00.880
to skip ahead a little bit,
is called Teachable Machine.

00:02:00.880 --> 00:02:03.040
And I'll give some
background on it first.

00:02:03.040 --> 00:02:06.940
But in a nutshell, we
released this last year.

00:02:06.940 --> 00:02:10.240
And it sort of lets you train
a machine learning model right

00:02:10.240 --> 00:02:12.396
in your browser
using your webcam.

00:02:12.396 --> 00:02:14.020
And I'll get more
into what this means.

00:02:14.020 --> 00:02:16.478
But I wanted to kind of zoom
out before I got into this one

00:02:16.478 --> 00:02:18.160
and talk about some
of the motivations

00:02:18.160 --> 00:02:21.219
behind some of these
recent projects

00:02:21.219 --> 00:02:23.260
and also give credit and
thanks to all the people

00:02:23.260 --> 00:02:25.060
that kind of helped with this.

00:02:25.060 --> 00:02:27.760
It's built with something
called deeplearn.js

00:02:27.760 --> 00:02:29.510
made by Nikhil and
Daniel on Martin

00:02:29.510 --> 00:02:31.870
and Fernanda's
team in Cambridge.

00:02:31.870 --> 00:02:33.490
And I'll talk more
about this tool

00:02:33.490 --> 00:02:36.730
that they made, and also, a
collaborative effort with a lot

00:02:36.730 --> 00:02:39.570
of teammates in New York.

00:02:39.570 --> 00:02:41.500
Also, a lot of projects
that preceded this.

00:02:41.500 --> 00:02:44.680
A lot of you know Rebecca's
work in Wekinator.

00:02:44.680 --> 00:02:48.610
This was very much
inspired a lot of what

00:02:48.610 --> 00:02:50.680
we did with Teachable Machine.

00:02:50.680 --> 00:02:53.740
And a really high
level motivation

00:02:53.740 --> 00:02:54.940
behind a lot of this--

00:02:54.940 --> 00:02:58.030
I love this quote
from Seymour Papert--

00:02:58.030 --> 00:03:00.940
is, you know, he says,
"mathophobia blocks many people

00:03:00.940 --> 00:03:02.530
from learning anything
they recognize

00:03:02.530 --> 00:03:04.630
as math, although they
may have no trouble

00:03:04.630 --> 00:03:06.909
with mathematical knowledge."

00:03:06.909 --> 00:03:09.200
And this is something that
I'm really passionate about.

00:03:09.200 --> 00:03:11.726
And so playing in this
area of machine learning

00:03:11.726 --> 00:03:13.600
is something that I kind
of care a lot about.

00:03:13.600 --> 00:03:15.308
And I think about that
and kind of get it

00:03:15.308 --> 00:03:18.100
every time I watch my
now six-year-old playing

00:03:18.100 --> 00:03:18.970
with origami.

00:03:18.970 --> 00:03:21.030
You know, he doesn't even--

00:03:21.030 --> 00:03:23.710
he's still learning his
numbers in kindergarten.

00:03:23.710 --> 00:03:26.477
But he's, like, incredibly
advanced at origami.

00:03:26.477 --> 00:03:28.060
He could follow
instructions like this

00:03:28.060 --> 00:03:30.700
to do these cool things
faster than I can.

00:03:30.700 --> 00:03:34.150
And he's kind of playing with it
and figuring out all this stuff

00:03:34.150 --> 00:03:35.890
even before he can
kind of articulate

00:03:35.890 --> 00:03:38.930
all these mathematical
geometry concepts.

00:03:38.930 --> 00:03:42.310
So in a lot of ways, another
metaphor that I like to use

00:03:42.310 --> 00:03:45.104
is, you know, I'm trying
to teach him piano.

00:03:45.104 --> 00:03:47.020
I have a background in
playing a lot of music.

00:03:47.020 --> 00:03:48.340
But I often find
that he makes music

00:03:48.340 --> 00:03:49.750
more like this-- you
know, taking apart

00:03:49.750 --> 00:03:51.837
all the instruments that
I have sitting around.

00:03:51.837 --> 00:03:53.920
And in some ways, this
picture kind of shows a lot

00:03:53.920 --> 00:03:55.450
of the motivation behind
the projects that I'm

00:03:55.450 --> 00:03:57.533
going to share as how can
we make machine learning

00:03:57.533 --> 00:04:02.530
and math feel a little bit
more like this for everyone?

00:04:02.530 --> 00:04:04.780
A lot of the projects that
I'll share live on a site

00:04:04.780 --> 00:04:06.670
called AI Experiments.

00:04:06.670 --> 00:04:09.520
And these projects
that we've created

00:04:09.520 --> 00:04:11.750
are more of thought starters.

00:04:11.750 --> 00:04:14.680
You'll see they're kind of
these little toys that hopefully

00:04:14.680 --> 00:04:16.450
inspire other developers
and other people

00:04:16.450 --> 00:04:18.190
to kind of run with them.

00:04:18.190 --> 00:04:19.959
So we kind of put
them out in the world

00:04:19.959 --> 00:04:23.140
as these little
thoughts starter toys.

00:04:23.140 --> 00:04:24.710
And I'll show a couple of these.

00:04:24.710 --> 00:04:27.040
The first one I want to show
is this little experiment

00:04:27.040 --> 00:04:31.330
that we collaborated with a
friend of ours, Carl McDonald.

00:04:31.330 --> 00:04:34.420
And part of the motivation
behind this was, you know,

00:04:34.420 --> 00:04:37.030
I'd heard about, you know,
these visualizations techniques

00:04:37.030 --> 00:04:39.820
that all my friends here at
Google were telling me about--

00:04:39.820 --> 00:04:42.340
t-SNE, running it on data.

00:04:42.340 --> 00:04:45.700
But I thought one of the
tests Kyle showed really

00:04:45.700 --> 00:04:48.020
made it feel more
accessible through sound.

00:04:48.020 --> 00:04:50.920
So what he had done was
he had run, you know,

00:04:50.920 --> 00:04:53.677
a t-SNE algorithm on I think
up about 14,000 sounds.

00:04:53.677 --> 00:04:55.010
That's what you're seeing there.

00:04:55.010 --> 00:04:55.220
[SLAM]

00:04:55.220 --> 00:04:55.666
[BAM]

00:04:55.666 --> 00:04:56.165
[COUGH]

00:04:56.165 --> 00:04:59.680
I'll turn that
down a little bit.

00:04:59.680 --> 00:05:00.180
And--

00:05:00.180 --> 00:05:00.675
[SLAM]

00:05:00.675 --> 00:05:01.170
[TUMBLE]

00:05:01.170 --> 00:05:03.630
You know, I don't really have
a background in data science.

00:05:03.630 --> 00:05:05.338
And this was the first
time that I really

00:05:05.338 --> 00:05:08.104
felt like I wanted to like
dig into this data set.

00:05:08.104 --> 00:05:09.645
You know, like, what
is this island--

00:05:09.645 --> 00:05:09.900
[SLAM]

00:05:09.900 --> 00:05:10.566
[SHRILL WHISTLE]

00:05:10.566 --> 00:05:12.400
--of little sounds over here.

00:05:12.400 --> 00:05:16.284
That's the kind of
shrill whistle island.

00:05:16.284 --> 00:05:18.450
And, you know, what is this
purple island over here?

00:05:18.450 --> 00:05:18.949
[PLINKS]

00:05:18.949 --> 00:05:22.194
That's the kind of
plinky, plinky sound.

00:05:22.194 --> 00:05:22.694
[PLINKS]

00:05:22.694 --> 00:05:23.550
[TUMBLE]

00:05:23.550 --> 00:05:27.430
And I think by adding this layer
of sound and music onto data,

00:05:27.430 --> 00:05:30.092
it suddenly made me want
to become a data scientist.

00:05:30.092 --> 00:05:32.050
Like, you know, if somebody
had asked me, like,

00:05:32.050 --> 00:05:34.080
oh, you want to comb
some data for the day?

00:05:34.080 --> 00:05:36.115
And, you know, you
could really get lost.

00:05:36.115 --> 00:05:37.840
You know, what's
this teal island?

00:05:37.840 --> 00:05:39.340
[METAL CLANGING]

00:05:39.340 --> 00:05:42.930
Kind of strange staticky world.

00:05:42.930 --> 00:05:45.837
And, of course, as the
toy aspect, you know,

00:05:45.837 --> 00:05:47.670
we decided to throw a
little drum sequencer.

00:05:47.670 --> 00:05:48.750
[SQUEAKY HORN]

00:05:48.750 --> 00:05:51.190
So we sort of made
the drum sequencer

00:05:51.190 --> 00:05:53.593
like the world's biggest
drum machine out of this.

00:05:53.593 --> 00:05:59.160
[SQUEAKY HORN]

00:05:59.160 --> 00:06:01.680
Another project we did
kind of almost as a Save

00:06:01.680 --> 00:06:05.680
As of that project, you
know, is around bird sounds.

00:06:05.680 --> 00:06:08.340
So we talked to
Kyle about dropping

00:06:08.340 --> 00:06:09.744
in little snippets
of bird sounds

00:06:09.744 --> 00:06:10.785
into a similar algorithm.

00:06:10.785 --> 00:06:11.647
[HAWK CRIES]

00:06:11.647 --> 00:06:13.230
And what I really
like that he arrived

00:06:13.230 --> 00:06:15.020
at here was he
showed me this, you

00:06:15.020 --> 00:06:16.770
know, graphic that
he had exported

00:06:16.770 --> 00:06:18.395
of all these tiny
little spectrograms--

00:06:18.395 --> 00:06:19.134
[COUGH]

00:06:19.134 --> 00:06:20.050
--the visualizations--

00:06:20.050 --> 00:06:20.550
[BIRD CALL]

00:06:20.550 --> 00:06:21.480
--of these sounds.

00:06:21.480 --> 00:06:24.660
And I'd never really seen
spectrograms used in this way.

00:06:24.660 --> 00:06:25.596
[CHIRPS]

00:06:25.596 --> 00:06:27.720
And what I found interesting
was even if you're not

00:06:27.720 --> 00:06:30.424
interested in sound, I feel
like it makes it more accessible

00:06:30.424 --> 00:06:32.340
by seeing these tiny
little visual thumbnails.

00:06:32.340 --> 00:06:34.410
Because you can kind of be
like, ooh, what's those?

00:06:34.410 --> 00:06:34.909
You know--

00:06:34.909 --> 00:06:36.270
[CHIRPS]

00:06:36.270 --> 00:06:39.300
--sort of your visual
half of your brain

00:06:39.300 --> 00:06:41.100
is linking in the similarity.

00:06:41.100 --> 00:06:42.780
And even when you
zoom out, it's amazing

00:06:42.780 --> 00:06:46.095
how you can kind of scan and
find these little islands--

00:06:46.095 --> 00:06:47.340
[SQUEALS]

00:06:47.340 --> 00:06:50.140
--through the spectrogram.

00:06:50.140 --> 00:06:52.250
You can also search.

00:06:52.250 --> 00:06:55.000
So if I type owl, you can--

00:06:55.000 --> 00:06:55.620
[HOOTS]

00:06:55.620 --> 00:06:57.400
--kind of find the
little islands of sound

00:06:57.400 --> 00:06:58.630
that have come out of this.

00:06:58.630 --> 00:06:59.442
[HOOTS]

00:06:59.442 --> 00:07:01.830
So there's the kind of
ooohing area of the owl.

00:07:01.830 --> 00:07:03.130
[CHIRPS]

00:07:03.130 --> 00:07:04.045
Or that's the--

00:07:04.045 --> 00:07:06.190
I'm not sure.

00:07:06.190 --> 00:07:08.950
Not a bird expert.

00:07:08.950 --> 00:07:11.350
And this is a
collaboration with--

00:07:11.350 --> 00:07:15.460
Cornell Lab of Ornithology let
us use their data for this.

00:07:15.460 --> 00:07:16.930
And, you know, we
open source all

00:07:16.930 --> 00:07:19.179
of these things hoping that
somebody will kind of grab

00:07:19.179 --> 00:07:22.480
this stuff and kind of run with
it and figure out what else

00:07:22.480 --> 00:07:24.610
to do with it.

00:07:24.610 --> 00:07:26.260
Probably one of the
silliest projects

00:07:26.260 --> 00:07:29.200
I've ever been involved
in which I don't have time

00:07:29.200 --> 00:07:32.425
to demonstrate is
called Giorgio Cam.

00:07:32.425 --> 00:07:34.201
Oops.

00:07:34.201 --> 00:07:34.700
Oh.

00:07:34.700 --> 00:07:35.250
[SPEAKING ON TUTORIAL IN
 BACKGROUND]

00:07:35.250 --> 00:07:35.940
Oh, that's OK.

00:07:35.940 --> 00:07:38.314
I think it's my--

00:07:38.314 --> 00:07:39.480
we'll see the project later.

00:07:39.480 --> 00:07:41.610
It's my tutorial voice
on the website, which

00:07:41.610 --> 00:07:44.420
I heard somebody's opened up.

00:07:44.420 --> 00:07:47.550
But this project, you know,
you can try it at home.

00:07:47.550 --> 00:07:49.700
It's at g.co/giorgiocam.

00:07:49.700 --> 00:07:51.450
And it's really the
world's simplest demo.

00:07:51.450 --> 00:07:53.640
It's just image recognition.

00:07:53.640 --> 00:07:55.386
But when you take a
picture of something,

00:07:55.386 --> 00:07:56.760
there's a Mad Lib
format where it

00:07:56.760 --> 00:07:58.509
tries to take the
labels of what it's seen

00:07:58.509 --> 00:08:00.150
and tries to make
a song out of it.

00:08:00.150 --> 00:08:02.430
Very silly thing, but
the thing that we really

00:08:02.430 --> 00:08:05.181
liked about this was
that it kind of changed

00:08:05.181 --> 00:08:07.680
the feeling of image recognition
from being something that's

00:08:07.680 --> 00:08:10.022
being done to us to something
that's like in your hand

00:08:10.022 --> 00:08:11.730
that you can kind of
walk around the room

00:08:11.730 --> 00:08:13.080
and take pictures
of things and kind

00:08:13.080 --> 00:08:14.871
of almost start playing
with the algorithm.

00:08:14.871 --> 00:08:17.430
Like in this case, you know,
you point it at a globe.

00:08:17.430 --> 00:08:19.170
Is it going to get globe,
or is it going to get world?

00:08:19.170 --> 00:08:21.190
You know, you point it at
different string instruments.

00:08:21.190 --> 00:08:23.231
And, you know, it says,
pluck string instruments.

00:08:23.231 --> 00:08:27.090
It's kind of a fun, surprise way
of combing and probing at what

00:08:27.090 --> 00:08:29.430
image recognition can do.

00:08:29.430 --> 00:08:32.220
And, of course, the most
popular project among these

00:08:32.220 --> 00:08:36.179
was something that one of my
teammates Jonas came up with.

00:08:36.179 --> 00:08:39.659
And he collaborated with a team
that's actually half of them

00:08:39.659 --> 00:08:44.400
are based here in Zurich
as we saw earlier.

00:08:44.400 --> 00:08:46.350
And it was called Quick, Draw!.

00:08:46.350 --> 00:08:49.050
I know a lot of you
have probably played it.

00:08:49.050 --> 00:08:51.234
So you basically
draw, and the computer

00:08:51.234 --> 00:08:52.650
kind of plays a
game of Pictionary

00:08:52.650 --> 00:08:57.450
with you trying to run
stroke recognition on what

00:08:57.450 --> 00:08:58.780
you're doing.

00:08:58.780 --> 00:09:01.140
But the really interesting
part of this whole project

00:09:01.140 --> 00:09:04.680
was after you play
the game, we kind of

00:09:04.680 --> 00:09:06.666
wanted to find these
educational moments where

00:09:06.666 --> 00:09:08.040
you saw the drawings
that you got

00:09:08.040 --> 00:09:09.090
and the ones that
it didn't get right.

00:09:09.090 --> 00:09:10.740
So this was a game I
just played earlier.

00:09:10.740 --> 00:09:12.480
For some reason, it didn't
recognize this top one

00:09:12.480 --> 00:09:13.270
as a power outlet.

00:09:13.270 --> 00:09:15.690
I'm still not sure why.

00:09:15.690 --> 00:09:19.140
And we just tried to use these
moments just to kind of, like,

00:09:19.140 --> 00:09:20.500
tickle the brain a little bit.

00:09:20.500 --> 00:09:22.650
You know, said it thought
your drawing looked more

00:09:22.650 --> 00:09:26.760
like these, like a
calculator, oven, calendar.

00:09:26.760 --> 00:09:28.800
And it shows examples
of other power outlets

00:09:28.800 --> 00:09:31.440
that other people have drawn.

00:09:31.440 --> 00:09:34.470
I think this one has probably
modeled after a European outlet

00:09:34.470 --> 00:09:38.070
with the kind of diagonal up
ones, which kind of tickled

00:09:38.070 --> 00:09:39.720
our brains.

00:09:39.720 --> 00:09:41.932
After this project, Jonas
and a bunch of others

00:09:41.932 --> 00:09:43.890
said, oh, we have to open
source this data set.

00:09:43.890 --> 00:09:46.170
Because so many people
are talking about this

00:09:46.170 --> 00:09:48.270
and wanting to comb it.

00:09:48.270 --> 00:09:50.711
So Jonas set up
this effort to open

00:09:50.711 --> 00:09:52.710
source this data set in
collaboration with a lot

00:09:52.710 --> 00:09:54.810
of other people here at Google.

00:09:54.810 --> 00:09:56.670
And I'll just-- whoops.

00:09:56.670 --> 00:10:00.825
I'll just click this link,
because it's incredibly fun.

00:10:00.825 --> 00:10:02.480
I feel like a lot
of these projects

00:10:02.480 --> 00:10:05.840
have just made me realize
how much data can be

00:10:05.840 --> 00:10:08.960
a really fun thing to explore.

00:10:08.960 --> 00:10:11.255
So even here-- I'll close my--

00:10:11.255 --> 00:10:13.280
the windows.

00:10:13.280 --> 00:10:15.220
We made sure that when
we put out this data

00:10:15.220 --> 00:10:17.124
that it wasn't just a
link on GitHub that you

00:10:17.124 --> 00:10:18.540
had to be a developer
to get into,

00:10:18.540 --> 00:10:20.600
but that anybody who
comes to this website

00:10:20.600 --> 00:10:22.770
can literally scroll this page.

00:10:22.770 --> 00:10:25.100
Jonas built this web page
so that if you see this,

00:10:25.100 --> 00:10:29.514
tiny little scroll bar, I think,
has 147,000 outlets on it--

00:10:29.514 --> 00:10:31.430
so that everybody, even
if you're not a coder,

00:10:31.430 --> 00:10:33.138
could essentially
become a data scientist

00:10:33.138 --> 00:10:36.230
by combing through this thing
and finding different outlets

00:10:36.230 --> 00:10:37.721
people have drawn.

00:10:37.721 --> 00:10:39.470
It's amazing how well
you can comb things.

00:10:39.470 --> 00:10:42.590
Like, this person decided
to just write power outlet.

00:10:42.590 --> 00:10:46.800
You can find all kinds
of interesting things.

00:10:46.800 --> 00:10:51.470
And all kinds of fun
stuff came out of this.

00:10:51.470 --> 00:10:54.237
You know, we talked a little
bit about the Facets tool

00:10:54.237 --> 00:10:56.570
that was made by someone here,
James, as well as the Big

00:10:56.570 --> 00:10:57.660
Picture team.

00:10:57.660 --> 00:11:00.330
They used Quick, Draw! as
sort of the introductory data

00:11:00.330 --> 00:11:02.000
set for this new
tool because there's

00:11:02.000 --> 00:11:04.700
something about seeing these
drawings that really kind of,

00:11:04.700 --> 00:11:06.920
even if you're not
a data scientist,

00:11:06.920 --> 00:11:08.330
makes you kind of
curious to play

00:11:08.330 --> 00:11:12.340
with this tool, which was a
great collaboration with them.

00:11:12.340 --> 00:11:14.570
And these kind of overlaid
images were really fun.

00:11:14.570 --> 00:11:18.531
For some reason, in Italy I
guess three scoops is kind of--

00:11:18.531 --> 00:11:20.530
when you close your eyes
and picture ice cream--

00:11:20.530 --> 00:11:22.580
three scoops.

00:11:22.580 --> 00:11:24.120
And even people
outside of Google--

00:11:24.120 --> 00:11:26.222
so this was an amazing article.

00:11:26.222 --> 00:11:28.430
I think it was probably a
collaboration behind people

00:11:28.430 --> 00:11:31.610
who are interested in
culture and that the way

00:11:31.610 --> 00:11:34.290
that different cultures,
obviously, shape our instincts.

00:11:34.290 --> 00:11:37.130
And they just had the most
wonderful interactive thing

00:11:37.130 --> 00:11:39.170
on the front of just
to say, draw a circle.

00:11:39.170 --> 00:11:41.000
And which way did you
choose to draw it,

00:11:41.000 --> 00:11:42.530
clockwise or counter-clockwise?

00:11:42.530 --> 00:11:44.960
And then you're suddenly
drawn into this entire article

00:11:44.960 --> 00:11:49.220
about culture and drawing.

00:11:49.220 --> 00:11:51.890
And that kind of leads us up
to our most recent experiment

00:11:51.890 --> 00:11:54.230
that I just wanted
to demonstrate.

00:11:54.230 --> 00:11:56.450
And I'll do it live
as I walk through it.

00:12:00.270 --> 00:12:00.770
OK.

00:12:00.770 --> 00:12:03.609
So there is a tutorial
which actually has my voice

00:12:03.609 --> 00:12:04.900
kind of talking you through it.

00:12:04.900 --> 00:12:06.405
So I'll just do it sort of live.

00:12:06.405 --> 00:12:08.530
ALEXANDER CHEN IN TUTORIAL:
This is an experiment--

00:12:08.530 --> 00:12:09.696
ALEXANDER CHEN: Oops, sorry.

00:12:09.696 --> 00:12:11.410
I should have said
Skip Tutorial.

00:12:11.410 --> 00:12:12.880
OK.

00:12:12.880 --> 00:12:15.310
So you kind of land
on this web page.

00:12:15.310 --> 00:12:16.990
And the webcam is here.

00:12:16.990 --> 00:12:21.550
And what you can do
is train a class,

00:12:21.550 --> 00:12:25.300
train your first, you know,
image classifier here.

00:12:25.300 --> 00:12:30.070
And if you hold the green
button down, and I move around,

00:12:30.070 --> 00:12:33.800
it's now kind of grabbing
frames from my webcam.

00:12:33.800 --> 00:12:36.340
So it's grabbed, you
know, 120 frames.

00:12:36.340 --> 00:12:39.400
I'll explain the cat
GIF in just a second.

00:12:39.400 --> 00:12:42.650
Well, basically, on the
right side is outputs.

00:12:42.650 --> 00:12:44.650
So it's going to trigger
different outputs based

00:12:44.650 --> 00:12:47.140
on what class is triggering.

00:12:47.140 --> 00:12:49.570
For, you know, people who
are interested in the kind

00:12:49.570 --> 00:12:51.480
of details of this
design, you'll

00:12:51.480 --> 00:12:53.764
notice right now it says
100% confidence green.

00:12:53.764 --> 00:12:55.180
That was kind of
a design decision

00:12:55.180 --> 00:12:57.820
we debated a little bit.

00:12:57.820 --> 00:13:00.490
And it almost feels like
an error in this design.

00:13:00.490 --> 00:13:02.532
But we kind of thought it
was important to reveal

00:13:02.532 --> 00:13:04.781
what the machine learning
algorithm does when you only

00:13:04.781 --> 00:13:05.500
train one class.

00:13:05.500 --> 00:13:08.589
It's incredibly kind of naive.

00:13:08.589 --> 00:13:10.880
It's like, well, out of all
the things you've shown me,

00:13:10.880 --> 00:13:13.360
I choose green because
you haven't even

00:13:13.360 --> 00:13:14.740
moved onto the next one yet.

00:13:14.740 --> 00:13:16.656
So, obviously, you need
to train a second one.

00:13:16.656 --> 00:13:17.770
So we'll try this.

00:13:17.770 --> 00:13:21.342
If I-- it's going to be tricky.

00:13:21.342 --> 00:13:22.550
Here, I'll step out of frame.

00:13:27.450 --> 00:13:30.490
So now, if I'm in frame,
you know, it chooses green.

00:13:30.490 --> 00:13:33.660
And if I'm out of frame,
it chooses purple.

00:13:33.660 --> 00:13:37.020
And for the last one, I'll
just raise this mic in the air.

00:13:42.930 --> 00:13:44.680
So now, it's triggering
between all three.

00:13:44.680 --> 00:13:46.440
So I think if I raise
my mic in the air--

00:13:49.154 --> 00:13:51.320
and, of course, we hooked
it up to different sounds.

00:13:51.320 --> 00:13:51.610
So--

00:13:51.610 --> 00:13:52.270
[CHIRPS]

00:13:52.270 --> 00:13:55.690
--right now, it's playing
a bird sound, birds.mp3,

00:13:55.690 --> 00:13:56.875
when it's the green class.

00:13:56.875 --> 00:13:58.988
[CHIRPS]

00:13:58.988 --> 00:14:01.280
[ELECTRIC GUITAR]

00:14:01.280 --> 00:14:03.080
It plays an electric
guitar solo right now

00:14:03.080 --> 00:14:05.018
if I step out of frame.

00:14:05.018 --> 00:14:06.440
[CHIRPS]

00:14:06.440 --> 00:14:08.336
[SAD HORN]

00:14:08.336 --> 00:14:09.760
[CHIRPS]

00:14:09.760 --> 00:14:10.560
[SAD HORN]

00:14:10.560 --> 00:14:11.370
[CHIRPS]

00:14:11.370 --> 00:14:14.130
[SAD HORN]

00:14:14.130 --> 00:14:14.902
[CHIRPS]

00:14:14.902 --> 00:14:16.610
And, of course, we
have speech over here.

00:14:16.610 --> 00:14:16.880
COMPUTER: Hello.

00:14:16.880 --> 00:14:19.460
ALEXANDER CHEN: So we could,
you know, say anything we want.

00:14:19.460 --> 00:14:25.260
You know, so I could have
it say, where'd you go.

00:14:25.260 --> 00:14:27.960
See if it says that right.

00:14:27.960 --> 00:14:29.670
COMPUTER: Where'd you go?

00:14:29.670 --> 00:14:30.280
Hello.

00:14:30.280 --> 00:14:30.900
Yeah.

00:14:30.900 --> 00:14:32.160
Where'd you go?

00:14:32.160 --> 00:14:33.340
Hello.

00:14:33.340 --> 00:14:34.890
Where'd you go?

00:14:34.890 --> 00:14:35.729
Hello.

00:14:35.729 --> 00:14:38.020
ALEXANDER CHEN: And you know,
even these little things,

00:14:38.020 --> 00:14:42.760
you know, on that where'd you
go one, like, I'd kind of been--

00:14:42.760 --> 00:14:45.349
my day is mostly, you know,
begging the real machine

00:14:45.349 --> 00:14:47.140
learning researchers
at Google to just keep

00:14:47.140 --> 00:14:48.848
re-explaining this
machine learning stuff

00:14:48.848 --> 00:14:49.935
to me over and over.

00:14:49.935 --> 00:14:52.060
And it wasn't until I played
with that speech thing

00:14:52.060 --> 00:14:53.435
that I kind of
finally understood

00:14:53.435 --> 00:14:55.977
this idea of labeling
is essentially--

00:14:55.977 --> 00:14:57.310
you know, it's a human activity.

00:14:57.310 --> 00:15:00.550
And I've essentially labeled
that state "where'd you go"

00:15:00.550 --> 00:15:01.050
arbitrarily.

00:15:01.050 --> 00:15:02.758
But, you know, the
machine has no concept

00:15:02.758 --> 00:15:04.440
of person in frame,
person not in frame.

00:15:04.440 --> 00:15:06.114
It has no, you
know, understanding

00:15:06.114 --> 00:15:07.030
of that sort of thing.

00:15:07.030 --> 00:15:09.196
So, you know, just playing
with these little moments

00:15:09.196 --> 00:15:11.290
sort of gives you a
little bit of insight

00:15:11.290 --> 00:15:13.907
into how classification works.

00:15:13.907 --> 00:15:15.490
So I wanted to show
some of the things

00:15:15.490 --> 00:15:17.730
that people have
actually done with this.

00:15:17.730 --> 00:15:21.640
And I'll show some examples
of things that can be done.

00:15:21.640 --> 00:15:24.710
So this is one of the demos
that I tried early on is just--

00:15:24.710 --> 00:15:25.680
[ELECTRIC GUITAR]

00:15:25.680 --> 00:15:27.430
--playing air guitar
solos with your hand.

00:15:27.430 --> 00:15:28.447
[DRUMS]

00:15:28.447 --> 00:15:30.405
So you can train it on
different hand gestures.

00:15:30.405 --> 00:15:31.670
[ELECTRIC GUITAR]

00:15:31.670 --> 00:15:32.600
[DRUMS]

00:15:32.600 --> 00:15:33.530
[ELECTRIC GUITAR]

00:15:33.530 --> 00:15:35.860
[DRUMS]

00:15:35.860 --> 00:15:37.190
This is kind of a fun test.

00:15:37.190 --> 00:15:39.080
You know, I like doing
origami with my kid.

00:15:39.080 --> 00:15:40.846
So I wanted to see
if I could kind of--

00:15:40.846 --> 00:15:42.355
[CHIRPS]

00:15:42.355 --> 00:15:43.730
--do a little
origami classifier.

00:15:43.730 --> 00:15:44.591
[ORGAN]

00:15:44.591 --> 00:15:46.555
[CHIRPS]

00:15:46.555 --> 00:15:48.519
[ORGAN]

00:15:48.519 --> 00:15:50.000
[CHIRPS]

00:15:50.000 --> 00:15:51.415
But it was fun doing this test.

00:15:51.415 --> 00:15:52.540
Because, you know, I made--

00:15:52.540 --> 00:15:53.040
[ORGAN]

00:15:53.040 --> 00:15:53.996
Oh, yeah.

00:15:53.996 --> 00:15:54.496
[CHIRPS]

00:15:54.496 --> 00:15:54.996
[ORGAN]

00:15:54.996 --> 00:15:57.430
[CHIRPS]

00:15:57.430 --> 00:15:59.875
[ORGAN]

00:15:59.875 --> 00:16:01.331
[CHIRPS]

00:16:01.331 --> 00:16:01.831
[ORGAN]

00:16:01.831 --> 00:16:02.331
[CHIRPS]

00:16:02.331 --> 00:16:03.287
[ORGAN]

00:16:03.287 --> 00:16:03.787
[CHIRPS]

00:16:03.787 --> 00:16:04.280
[ORGAN]

00:16:04.280 --> 00:16:04.780
[CHIRPS]

00:16:04.780 --> 00:16:05.790
It's incredibly fast.

00:16:05.790 --> 00:16:06.770
[ORGAN]

00:16:06.770 --> 00:16:08.785
So part of the cool
technical aspect of this,

00:16:08.785 --> 00:16:10.160
as I mentioned
before, is it uses

00:16:10.160 --> 00:16:12.384
a library called deeplearn.js.

00:16:12.384 --> 00:16:14.300
And it's all happening
locally in the browser.

00:16:14.300 --> 00:16:16.490
So your images aren't
actually ever sent

00:16:16.490 --> 00:16:19.637
to servers, which is why,
you know, you can kind of put

00:16:19.637 --> 00:16:21.470
the ghost in and out
of frame, and it really

00:16:21.470 --> 00:16:23.450
switches pretty quickly.

00:16:23.450 --> 00:16:25.880
And I kind of felt like even
as I played with this stuff,

00:16:25.880 --> 00:16:27.290
I accidentally
found myself doing

00:16:27.290 --> 00:16:29.060
little, like,
science-y experiments,

00:16:29.060 --> 00:16:31.250
like choosing to make both
of those origami things

00:16:31.250 --> 00:16:34.070
white with a couple
of black dots on them.

00:16:34.070 --> 00:16:36.350
Like, could it really
tell the difference?

00:16:36.350 --> 00:16:38.585
And try to do the same
experiment with, you know,

00:16:38.585 --> 00:16:42.380
different colored
pieces of paper.

00:16:42.380 --> 00:16:43.400
A couple other things--

00:16:43.400 --> 00:16:45.920
this was one I trained on,
some drawings that my kid did.

00:16:45.920 --> 00:16:46.890
COMPUTER: Jellyfish.

00:16:46.890 --> 00:16:48.070
Submarine.

00:16:48.070 --> 00:16:49.650
Jellyfish.

00:16:49.650 --> 00:16:50.840
Submarine.

00:16:50.840 --> 00:16:52.740
Millennium Falcon.

00:16:52.740 --> 00:16:54.680
Jellyfish.

00:16:54.680 --> 00:16:56.190
Millennium Falcon.

00:16:56.190 --> 00:16:57.770
Jellyfish.

00:16:57.770 --> 00:16:58.842
Millennium Falcon.

00:16:58.842 --> 00:17:00.800
ALEXANDER CHEN: This was
a fun experiment, too.

00:17:00.800 --> 00:17:02.150
Because he'd
actually drawn two--

00:17:02.150 --> 00:17:03.290
I asked him to
draw a few things.

00:17:03.290 --> 00:17:04.581
And I asked him what they were.

00:17:04.581 --> 00:17:07.099
And then I put the labels
into the speech thing.

00:17:07.099 --> 00:17:08.500
He actually drew two jellyfish.

00:17:08.500 --> 00:17:09.500
And I trained it on one.

00:17:09.500 --> 00:17:11.000
And it sort of
understood the other.

00:17:11.000 --> 00:17:14.430
So it sort of decided that
among the three things

00:17:14.430 --> 00:17:16.609
it was able to figure
those three out.

00:17:16.609 --> 00:17:19.579
This is something somebody
did with their house plants.

00:17:19.579 --> 00:17:21.170
This is a project
somebody did online

00:17:21.170 --> 00:17:23.240
where they, I think
with their kid,

00:17:23.240 --> 00:17:25.220
had a toy train set going by.

00:17:25.220 --> 00:17:27.619
And they sort of
uploaded their own sounds

00:17:27.619 --> 00:17:28.520
into the source code.

00:17:28.520 --> 00:17:33.020
So that as the train went by, it
would trigger different sounds.

00:17:33.020 --> 00:17:35.350
But the most exciting
thing is that, you

00:17:35.350 --> 00:17:37.970
know, we put these things
out as thought starters,

00:17:37.970 --> 00:17:40.610
so that anybody can
kind of riff on them.

00:17:40.610 --> 00:17:44.030
So Dan Shiffman with the team,
who's here today, kind of

00:17:44.030 --> 00:17:47.040
recreated this in
their new framework,

00:17:47.040 --> 00:17:50.450
so that anybody could kind
of build this from scratch.

00:17:50.450 --> 00:17:53.030
And Jonas also kind of put
out a boilerplate example,

00:17:53.030 --> 00:17:55.400
almost stripped down of
all the UI and stuff,

00:17:55.400 --> 00:17:58.310
so that anybody could
kind of riff on top of it.

00:17:58.310 --> 00:18:01.680
And we did find a lot of
people playing with this stuff.

00:18:01.680 --> 00:18:03.960
So here's a ninth grade
class where, you know,

00:18:03.960 --> 00:18:06.320
the kids were kind of playing
with some sort of image

00:18:06.320 --> 00:18:08.000
classification.

00:18:08.000 --> 00:18:10.250
This one was really cool.

00:18:10.250 --> 00:18:11.750
So I found this on Twitter.

00:18:11.750 --> 00:18:13.637
These fifth grade
girls were actually

00:18:13.637 --> 00:18:15.720
creating what they called
a smart security system.

00:18:15.720 --> 00:18:18.770
And from what I can deduce,
they had different friends

00:18:18.770 --> 00:18:23.030
stand in front of it and
classify on each of them.

00:18:23.030 --> 00:18:26.120
So as the teacher says, they
gathered the data, trained it,

00:18:26.120 --> 00:18:27.970
and voila.

00:18:27.970 --> 00:18:29.075
And it's kind of amazing.

00:18:29.075 --> 00:18:30.825
I wish I could have
been a fly on the wall

00:18:30.825 --> 00:18:34.510
to, like, hear the questions
that, hopefully, this provoked.

00:18:34.510 --> 00:18:36.400
You know, like I
would have loved

00:18:36.400 --> 00:18:37.630
to see their debug process.

00:18:37.630 --> 00:18:39.820
Like, hey, how come it's
not working on my friend?

00:18:39.820 --> 00:18:42.130
Maybe we should have that
person jump in on this

00:18:42.130 --> 00:18:44.330
now and add a new class.

00:18:44.330 --> 00:18:46.360
And I think it really
kind of tickles

00:18:46.360 --> 00:18:49.107
the brain in certain
ways of, you know,

00:18:49.107 --> 00:18:51.190
as you play with this
thing, you start to realize,

00:18:51.190 --> 00:18:53.020
like, is it picking
up on the faces.

00:18:53.020 --> 00:18:54.580
What if you turn the laptop?

00:18:54.580 --> 00:18:56.260
Is it more picking
up on the background

00:18:56.260 --> 00:18:57.218
that you trained it on?

00:18:57.218 --> 00:19:00.100
You can quickly
uncover these sort

00:19:00.100 --> 00:19:04.990
of problems and opportunities
with this stuff.

00:19:04.990 --> 00:19:07.990
This was something I just
found this morning actually.

00:19:07.990 --> 00:19:11.260
So I found this teacher
who is, I think,

00:19:11.260 --> 00:19:15.850
building sort of a city
simulation using Lego robots.

00:19:15.850 --> 00:19:18.340
So they've sort of built
a whole city with sensors.

00:19:18.340 --> 00:19:19.900
And as a side activity--

00:19:19.900 --> 00:19:21.490
it's not actually
linked up to that--

00:19:21.490 --> 00:19:23.465
they decided to
cut out road signs.

00:19:23.465 --> 00:19:25.090
So you'll see they've
made a stop sign.

00:19:25.090 --> 00:19:26.465
And they're actually
going to use

00:19:26.465 --> 00:19:29.310
Teachable Machine to train it to
recognize different stop signs.

00:19:29.310 --> 00:19:31.893
I think they're still trying to
figure out how to hook the two

00:19:31.893 --> 00:19:33.220
things up together.

00:19:33.220 --> 00:19:35.650
But it's kind of amazing
that they're essentially

00:19:35.650 --> 00:19:38.510
exploring these, like,
really big questions.

00:19:38.510 --> 00:19:41.500
Apparently, they
actually had an assembly

00:19:41.500 --> 00:19:45.400
about the ethics of smart
cities and all kinds of things,

00:19:45.400 --> 00:19:47.590
which is an sort of amazing
teacher figuring out

00:19:47.590 --> 00:19:48.856
to do this.

00:19:48.856 --> 00:19:50.980
And, of course, the thing
that we saw earlier today

00:19:50.980 --> 00:19:53.335
is maybe one of my favorite
things that I've seen.

00:19:53.335 --> 00:19:57.934
A really inspiring
coder in Portland is--

00:19:57.934 --> 00:19:59.350
his friend just
suffered a stroke.

00:19:59.350 --> 00:20:01.120
So he actually took
the boilerplate code

00:20:01.120 --> 00:20:03.940
for Teachable Machine and is
training it to move a mouse

00:20:03.940 --> 00:20:05.980
cursor with a gesture.

00:20:05.980 --> 00:20:09.190
So he's working
on that right now.

00:20:09.190 --> 00:20:12.060
So I just thought I would close
with this quote from a teacher

00:20:12.060 --> 00:20:14.821
that I really admire that says,
"Mathematics is fundamentally

00:20:14.821 --> 00:20:16.320
about patterns and
structures rather

00:20:16.320 --> 00:20:19.685
than little manipulations
of numbers."

00:20:19.685 --> 00:20:21.060
And so I think
back to this as we

00:20:21.060 --> 00:20:24.390
try to build, like, tools
for interpretability

00:20:24.390 --> 00:20:26.070
and kind of being
able to investigate

00:20:26.070 --> 00:20:27.510
and kind of poke and
prod at these things.

00:20:27.510 --> 00:20:29.100
It's like, can we
get to this feeling

00:20:29.100 --> 00:20:32.760
that you are essentially
playing with it without being

00:20:32.760 --> 00:20:34.960
intimidated by it?

00:20:34.960 --> 00:20:35.800
And that's it.

00:20:35.800 --> 00:20:43.150
[APPLAUSE]

00:20:43.150 --> 00:20:46.580
SPEAKER 2: [INAUDIBLE].

00:20:46.580 --> 00:20:47.815
Question?

00:20:47.815 --> 00:20:51.116
He decided I was too
biased to that side, so.

00:20:51.116 --> 00:20:54.104
[LAUGHTER]

00:20:54.104 --> 00:20:55.100
No?

00:20:55.100 --> 00:20:58.088
[LAUGHTER]

00:20:58.088 --> 00:20:59.084
AUDIENCE: I'm sorry.

00:21:02.907 --> 00:21:03.740
AUDIENCE: Thank you.

00:21:03.740 --> 00:21:04.590
It's a switcheroo.

00:21:04.590 --> 00:21:07.477
I moved from the center, because
I knew you would ignore people

00:21:07.477 --> 00:21:08.060
in the center.

00:21:08.060 --> 00:21:09.070
ALEXANDER CHEN: Oh.

00:21:09.070 --> 00:21:10.000
AUDIENCE: Amazing.

00:21:10.000 --> 00:21:11.450
I've taken all these notes.

00:21:11.450 --> 00:21:14.840
And I'm really curious
about this great gift

00:21:14.840 --> 00:21:17.270
of all of the draw
with Google IP

00:21:17.270 --> 00:21:19.700
that you've put into
the open domain.

00:21:19.700 --> 00:21:21.590
Could you just talk
us through the process

00:21:21.590 --> 00:21:24.410
by which you persuaded a
publicly listed company

00:21:24.410 --> 00:21:26.930
to do that, and what
you learned from it,

00:21:26.930 --> 00:21:29.240
and kind of what internal
attitudes were and so on?

00:21:29.240 --> 00:21:31.692
Because it's a great
achievement to do that.

00:21:31.692 --> 00:21:33.650
ALEXANDER CHEN: Yeah, it
was a big team effort.

00:21:33.650 --> 00:21:37.260
I think I, obviously,
did very little of it.

00:21:37.260 --> 00:21:42.150
It's sort of this magic
of knowing how to--

00:21:42.150 --> 00:21:45.420
it was, you know, producers
talking to lawyers talking

00:21:45.420 --> 00:21:47.590
to designers talking to--

00:21:47.590 --> 00:21:51.600
but I think it's sort of a
story of, like, little steps

00:21:51.600 --> 00:21:52.530
one at a time.

00:21:52.530 --> 00:21:55.944
We essentially put out-- you
know, Jonas hacked together

00:21:55.944 --> 00:21:56.610
the Quick, Draw!

00:21:56.610 --> 00:21:58.711
Toy in, you know, under
a week, just that kind

00:21:58.711 --> 00:22:00.210
of delightful moment
of, wouldn't it

00:22:00.210 --> 00:22:01.293
be fun to play Pictionary?

00:22:01.293 --> 00:22:03.100
And we kind of took
these steps up there.

00:22:03.100 --> 00:22:06.030
So I think part of it
was the public interest

00:22:06.030 --> 00:22:08.940
in the game I think persuaded
other people at Google as,

00:22:08.940 --> 00:22:10.770
like, oh, yeah,
let's definitely--

00:22:10.770 --> 00:22:13.019
if people are asking for it
and other machine learning

00:22:13.019 --> 00:22:15.820
researchers are asking for it
and want to explore this data,

00:22:15.820 --> 00:22:17.400
then let's do it.

00:22:17.400 --> 00:22:21.100
So-- no magic
formula for doing it.

00:22:21.100 --> 00:22:24.030
But it really was sort of like
a little one step at a time.

00:22:24.030 --> 00:22:28.690
We had no grand plan all
the way from the beginning.

00:22:28.690 --> 00:22:31.540
AUDIENCE: Can you talk a little
bit about what I can only

00:22:31.540 --> 00:22:35.890
really describe in British terms
as, time to penis, which is--

00:22:35.890 --> 00:22:40.610
when in England being given the
opportunity to draw something,

00:22:40.610 --> 00:22:43.420
that's one of the first
things that gets drawn?

00:22:43.420 --> 00:22:48.140
And so can you talk
about managing data sets

00:22:48.140 --> 00:22:50.770
which may or may not be full
of the things that you want?

00:22:50.770 --> 00:22:55.060
And how much of that
plays or has an impact

00:22:55.060 --> 00:22:57.548
on the success of a project?

00:22:57.548 --> 00:22:58.940
ALEXANDER CHEN: Yeah.

00:22:58.940 --> 00:23:00.607
So an interesting
story about, you

00:23:00.607 --> 00:23:04.030
know, as we were working on
getting that data set out,

00:23:04.030 --> 00:23:06.370
kind of makes me [INAUDIBLE]
a lot of the talks today.

00:23:06.370 --> 00:23:07.900
But I feel like a
lot of design now

00:23:07.900 --> 00:23:10.240
is in the writing and
the framing of the thing,

00:23:10.240 --> 00:23:11.070
so a lot of--

00:23:11.070 --> 00:23:13.300
you know, the website
itself of this big scrolling

00:23:13.300 --> 00:23:17.590
page of drawings was designed,
you know, quite quickly.

00:23:17.590 --> 00:23:20.140
But then, you know,
the sentences above it

00:23:20.140 --> 00:23:21.900
were part of the design process.

00:23:21.900 --> 00:23:23.650
And, you know, it'd
suddenly be all of us,

00:23:23.650 --> 00:23:25.650
whether you're a designer,
a producer, a writer,

00:23:25.650 --> 00:23:28.660
being like what is the right
way to talk about this data set?

00:23:28.660 --> 00:23:32.200
It makes me think about the
sort of co-creation IKEA effect

00:23:32.200 --> 00:23:33.786
that Cliff talked about earlier.

00:23:33.786 --> 00:23:35.410
The sentence that we
arrived at the top

00:23:35.410 --> 00:23:38.669
was something along the lines
of if you see something,

00:23:38.669 --> 00:23:40.960
flag it as inappropriate
because it will make it better

00:23:40.960 --> 00:23:43.720
for everyone, which is true.

00:23:43.720 --> 00:23:45.340
But also, in this
sort of like, hey,

00:23:45.340 --> 00:23:47.410
we all want this data
set to be out here,

00:23:47.410 --> 00:23:50.207
so we kind of need your help
to not ruin this for everyone.

00:23:50.207 --> 00:23:52.540
You know, like we sort of
openly acknowledge that you're

00:23:52.540 --> 00:23:53.789
going to find some stuff here.

00:23:53.789 --> 00:23:57.370
And we kind of, I think, landed
on that true, honest thought

00:23:57.370 --> 00:23:59.230
that, hey, we need
everybody's help

00:23:59.230 --> 00:24:02.980
and being very honest
that we didn't have the--

00:24:02.980 --> 00:24:08.030
we also didn't want to
overly curate ahead of time.

00:24:08.030 --> 00:24:09.550
Because for researchers, they--

00:24:09.550 --> 00:24:12.801
some researchers might actually
need a really authentic data

00:24:12.801 --> 00:24:13.300
set.

00:24:13.300 --> 00:24:15.970
So it's sort of that
language of co-creation

00:24:15.970 --> 00:24:20.020
which helped us sort of land
that feeling of about having

00:24:20.020 --> 00:24:21.490
this data set out there.

00:24:26.890 --> 00:24:28.670
AUDIENCE: Hi, Alex.

00:24:28.670 --> 00:24:31.820
Could you explain
a little bit what

00:24:31.820 --> 00:24:34.850
other metadata is associated
with these data sets?

00:24:34.850 --> 00:24:36.890
Because you talked
a little bit about,

00:24:36.890 --> 00:24:40.650
you know, you can use it to
study culture, for example.

00:24:40.650 --> 00:24:47.130
So what other data is
there associated with that?

00:24:47.130 --> 00:24:50.114
ALEXANDER CHEN: I'd have to
ping Jonas after this call

00:24:50.114 --> 00:24:51.280
and get you a detailed list.

00:24:51.280 --> 00:24:52.680
But, yeah, as you could
see, they at least

00:24:52.680 --> 00:24:55.230
have, like, country associated
because that [? quotes ?]

00:24:55.230 --> 00:24:58.170
article from
different countries.

00:24:58.170 --> 00:25:01.200
Beyond that, I don't know
off the top of my head,

00:25:01.200 --> 00:25:02.960
but hopefully, enough
to be interesting.

00:25:02.960 --> 00:25:06.007
But I can ask him and
find out an exact list.

00:25:06.007 --> 00:25:06.507
Yeah.

00:25:16.950 --> 00:25:19.050
AUDIENCE: Right now,
people are generally

00:25:19.050 --> 00:25:22.570
happy to provide their training
data for these kind of things,

00:25:22.570 --> 00:25:25.020
because they're fun, and
interesting, and novel.

00:25:25.020 --> 00:25:27.990
That won't long be the case.

00:25:27.990 --> 00:25:31.167
How do you foresee the
future of experiments

00:25:31.167 --> 00:25:33.750
like that when people recognize
that this is essentially labor

00:25:33.750 --> 00:25:35.524
you're asking them to provide?

00:25:35.524 --> 00:25:37.940
ALEXANDER CHEN: Yeah, I mean,
that's an interesting also--

00:25:37.940 --> 00:25:39.980
Quick, Draw!, it was
essentially, you know,

00:25:39.980 --> 00:25:41.810
all going into one
collective data set.

00:25:41.810 --> 00:25:45.990
Whereas, Teachable Machine, it's
all happening on your laptop.

00:25:45.990 --> 00:25:50.167
So, essentially, the sort
of thing that you've trained

00:25:50.167 --> 00:25:51.500
is all sitting on your computer.

00:25:51.500 --> 00:25:53.360
So it is an interesting
kind of fork

00:25:53.360 --> 00:25:57.350
in the road where I could
imagine a world where

00:25:57.350 --> 00:26:00.489
if, you know, that
kind of classifier I'd

00:26:00.489 --> 00:26:02.030
made for my kid's
drawing, would that

00:26:02.030 --> 00:26:06.320
be something that I could kind
of grab and share and post?

00:26:06.320 --> 00:26:09.452
And so I think that idea of
you owning a little model

00:26:09.452 --> 00:26:10.910
that you've made
was very much what

00:26:10.910 --> 00:26:12.440
we wanted to go for
with Teachable Machine.

00:26:12.440 --> 00:26:14.060
But we haven't quite
gotten to that.

00:26:14.060 --> 00:26:16.855
That's almost the second
phase of that hopefully.

00:26:20.002 --> 00:26:20.960
AUDIENCE: Thanks, Alex.

00:26:20.960 --> 00:26:25.900
[APPLAUSE]

00:26:25.900 --> 00:26:27.940
SPEAKER 1: : The next
speaker is Patrick Hebron.

00:26:27.940 --> 00:26:29.910
He is an [? associate-- ?]

