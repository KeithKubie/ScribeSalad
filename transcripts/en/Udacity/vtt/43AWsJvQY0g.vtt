WEBVTT
Kind: captions
Language: en

00:00:00.940 --> 00:00:02.960
What we've covered so far in this class is

00:00:02.960 --> 00:00:07.250
a very simplistic and a very partial coverage of

00:00:07.250 --> 00:00:10.790
linear regression with gradient decent. If you were ever

00:00:10.790 --> 00:00:12.980
to try and use linear regression to solve a real

00:00:12.980 --> 00:00:16.190
world problem, there are a lot of additional considerations

00:00:16.190 --> 00:00:18.170
that you would want to think about in order

00:00:18.170 --> 00:00:22.205
to seriously implement linear regression with or without gradient

00:00:22.205 --> 00:00:26.210
descent. First of all, gradient descent is only one implementation

00:00:26.210 --> 00:00:28.440
of linear regression. There are a bunch of other

00:00:28.440 --> 00:00:31.280
ones, and in some sense, they may be better.

00:00:31.280 --> 00:00:34.060
Ordinary Least Squares for example, is always guaranteed to

00:00:34.060 --> 00:00:38.030
find the optimal solution when performing linear regression, whereas gradient

00:00:38.030 --> 00:00:42.050
descent is not. Additionally, we haven't really talked about

00:00:42.050 --> 00:00:46.490
parameter estimation and putting confidence intervals on those parameters. In

00:00:46.490 --> 00:00:48.880
the models that we've built, we've given exact values

00:00:48.880 --> 00:00:51.400
for all of our thetas. But as you can imagine,

00:00:51.400 --> 00:00:54.220
there's some confidence that we have in those values.

00:00:54.220 --> 00:00:57.840
You can imagine doing more thorough statistical analysis in saying

00:00:57.840 --> 00:01:01.070
what are the confidence intervals on these parameters? And

00:01:01.070 --> 00:01:04.069
we could answer questions like what is the likelihood that

00:01:04.069 --> 00:01:06.230
I would get this value for this parameter if

00:01:06.230 --> 00:01:08.969
this parameter actually had no effect on our output variable?

00:01:10.500 --> 00:01:13.430
We also might worry about issues like over or under

00:01:13.430 --> 00:01:16.920
fitting. This isn't so much a problem with linear regression,

00:01:16.920 --> 00:01:19.905
but with more complicated models, you might expect our model to

00:01:19.905 --> 00:01:23.350
over-fit. Say we had some data like these red points that

00:01:23.350 --> 00:01:26.240
was approximately linear. If we had a model with many different

00:01:26.240 --> 00:01:29.250
parameters, we might be able to fit the data points exactly

00:01:29.250 --> 00:01:33.740
whereas the actual underlying model is this black line. One way

00:01:33.740 --> 00:01:36.110
to deal with this is usually to split our data into

00:01:36.110 --> 00:01:38.940
a training set and a test set. Then we can train

00:01:38.940 --> 00:01:42.040
a model on the training data, and then test it on

00:01:42.040 --> 00:01:44.200
the test data, just as the name would imply. This

00:01:44.200 --> 00:01:48.610
is called cross validation. We might also see under-fitting, where we

00:01:48.610 --> 00:01:51.520
have data that's clearly nonlinear, but we only try and fit

00:01:51.520 --> 00:01:54.190
a linear model to it. This is also a problem that

00:01:54.190 --> 00:01:57.810
you might encounter. Another issue worth mentioning again is that our

00:01:57.810 --> 00:02:00.980
cost function may have numerous local minima. That means that when

00:02:00.980 --> 00:02:04.050
using gradient descent, our algorithm can become trapped in a local

00:02:04.050 --> 00:02:06.830
minimum that isn't actually the global minimum of the cost function.

00:02:08.100 --> 00:02:10.110
This isn't a problem of other implementations of linear

00:02:10.110 --> 00:02:13.230
regression, but when using gradient descent, this means that maybe

00:02:13.230 --> 00:02:17.690
we perform gradient descent numerous times, randomizing our initial values

00:02:17.690 --> 00:02:21.390
of theta with different random values every time. You may

00:02:21.390 --> 00:02:23.270
also already know this, but when using a random

00:02:23.270 --> 00:02:26.950
number generator to generate random values, it's important to seed

00:02:26.950 --> 00:02:29.790
your random values and to store that seed. That way,

00:02:29.790 --> 00:02:33.510
you have a result that's replicable and other researchers can

00:02:33.510 --> 00:02:36.460
look at your findings and also produce them on their own machines

00:02:36.460 --> 00:02:39.640
when they run the same script. This has been a whirlwind tour

00:02:39.640 --> 00:02:42.350
of a bunch of additional considerations that we didn't really touch in

00:02:42.350 --> 00:02:44.160
this lesson. These are all really

00:02:44.160 --> 00:02:47.270
important in machine learning, statistics, using

00:02:47.270 --> 00:02:50.160
linear regression in the real world. I really wanted to focus here

00:02:50.160 --> 00:02:53.550
on just implementing gradient descent and the basics of how it works.

00:02:53.550 --> 00:02:55.800
But if you really want to use any of these things to

00:02:55.800 --> 00:02:59.080
solve a real life problem, you'd want to think about local minima,

00:02:59.080 --> 00:03:02.770
you know other implementations of linear aggression, the confidence intervals

00:03:02.770 --> 00:03:06.370
on your parameters and if you're using a statistical modeling package

00:03:06.370 --> 00:03:09.650
in R or Python or Stata, they'll usually give you

00:03:09.650 --> 00:03:11.230
these confidence intervals. And use

00:03:11.230 --> 00:03:13.170
a reliable implementation of linear aggression.

