WEBVTT
Kind: captions
Language: en

00:00:00.436 --> 00:00:01.905
Okay. Let's talk about a related problem.

00:00:01.905 --> 00:00:07.981
What happens when you have lots of threads both reading and writing the same memory locations?

00:00:07.981 --> 00:00:10.881
If lots of threads are all trying to read and write the same memory locations,

00:00:10.881 --> 00:00:12.284
then you're going to start to get conflicts.

00:00:12.284 --> 00:00:13.884
Let's look at an example.

00:00:13.884 --> 00:00:18.089
Suppose you had 10,000 threads that were all trying to increment 10 array elements.

00:00:18.089 --> 00:00:19.325
What's going to happen?

00:00:19.325 --> 00:00:20.726
Let's look at some code.

00:00:20.726 --> 00:00:23.928
This goes a little more complicated than we've seen in these examples before,

00:00:23.928 --> 00:00:25.131
so let me walk through it.

00:00:25.131 --> 00:00:27.869
What I'm going to do is I'm going to try writing with many threads

00:00:27.869 --> 00:00:30.700
to a small number of array elements.

00:00:30.700 --> 00:00:36.806
In this case, I'm going to write with 1,000,000 threads into 10 array elements,

00:00:36.806 --> 00:00:39.579
and I'm going to do so in blocks of a thousand.

00:00:39.579 --> 00:00:41.880
That's the #defines.

00:00:41.880 --> 00:00:44.608
It got a little helper function that just prints out an array.

00:00:44.608 --> 00:00:45.975
We'll use that for debugging.

00:00:45.975 --> 00:00:48.611
And then here's the kernel we're going to use.

00:00:48.611 --> 00:00:51.051
It's a kernel because it's labeled global.

00:00:51.051 --> 00:00:58.016
It is called increment naive, and it takes a pointer to global memory an array of integers.

00:00:58.016 --> 00:01:03.766
Each thread simply figures out what thread it is by looking at the block index,

00:01:03.766 --> 00:01:10.803
which block it is times how many threads there are in a block plus which thread it is inside that block.

00:01:10.803 --> 00:01:16.943
Now, what we're going to do is we're going to have every thread increment

00:01:16.943 --> 00:01:20.711
one of the elements in the array.

00:01:20.711 --> 00:01:27.933
To do that, we're just going to mod the thread index by the array size.

00:01:27.933 --> 00:01:34.023
Every thread is going, every consecutive threads are going to increment consecutive elements in the array,

00:01:34.023 --> 00:01:36.125
wrapping around at the array size.

00:01:36.125 --> 00:01:41.096
The fact that we have a million threads writing into only ten elements means

00:01:41.096 --> 00:01:46.736
that after each thread has added one to its corresponding element in the array,

00:01:46.736 --> 00:01:51.774
we're going to end up with 10 elements each containing the number 100,000.

00:01:51.774 --> 00:01:54.477
And then the code itself is simple.

00:01:54.477 --> 00:01:56.314
We have a timer class.

00:01:56.314 --> 00:01:59.849
Again, I've sort of hidden that away so you don't have to deal with it right now.

00:01:59.849 --> 00:02:03.253
We're going to declare some host memory.

00:02:03.253 --> 00:02:07.259
We're going to declare some GPU memory.

00:02:07.259 --> 00:02:10.425
And we're going to zero out that memory.

00:02:10.425 --> 00:02:12.796
You haven't seen cudaMemset before, but it's exactly what you'd think.

00:02:12.796 --> 00:02:19.101
We're going to set all of the bytes of this device array to zero.

00:02:19.101 --> 00:02:21.411
Now, we're going to launch the kernel.

00:02:21.411 --> 00:02:23.005
And I've put a timer around this,

00:02:23.005 --> 00:02:26.492
because one of the things I want to show you is that atomic operations can slow things down.

00:02:26.492 --> 00:02:29.743
So here's the kernel that we called increment_naive.

00:02:29.743 --> 00:02:34.091
We're going to launch it with a number of blocks equal

00:02:34.091 --> 00:02:36.591
to the total number of threads divided by the block width

00:02:36.591 --> 00:02:39.941
and the number of threads per block equal to the block width.

00:02:39.941 --> 00:02:44.940
Remember, these numbers initially are 1,000,000 and 1,000, okay?

00:02:44.940 --> 00:02:47.559
We're going to end up launching a thousand blocks,

00:02:47.559 --> 00:02:50.210
and we're going to pass in the device array,

00:02:50.210 --> 00:02:53.342
and then each thread is going to do its incrementing.

00:02:53.342 --> 00:02:59.605
And when it's all done we're going to stop the timer and copy back the array using cudaMemcpy.

00:02:59.605 --> 00:03:02.509
Now, we'll take that array that we just incremented

00:03:02.509 --> 00:03:07.592
and copy it back to the host and then I hid away a little print array helper function.

00:03:07.592 --> 00:03:09.114
It just prints out the contents of the array.

00:03:09.114 --> 00:03:12.426
Then I'm going to print out the amount of time taken milliseconds

00:03:12.426 --> 00:03:15.758
by this kernel that I measured with a timer.

00:03:15.758 --> 00:03:19.758
Okay. That's the whole Cuda program. Let's compile and run it.

