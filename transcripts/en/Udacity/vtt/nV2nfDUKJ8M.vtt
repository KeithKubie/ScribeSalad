WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:02.830
SMDP, by the way, theres an entire
literature about semi-Markov

00:00:02.830 --> 00:00:06.510
decision processes well beyond
the sort of scope of this lesson.

00:00:06.510 --> 00:00:08.950
But from our point of view what's
really neat about this, and

00:00:08.950 --> 00:00:11.230
this comes up again and
again when you use SMDPs,

00:00:11.230 --> 00:00:15.716
is that once we define an SMDP and
the appropriate Bellman operator for

00:00:15.716 --> 00:00:18.490
it, we can basically just
pretend that it's an MDP.

00:00:18.490 --> 00:00:20.164
Because at bottom it is an MDP and

00:00:20.164 --> 00:00:23.026
these expectations allow to
ground out in that in MDP.

00:00:23.026 --> 00:00:24.923
And so we can just pretend we're
always dealing with an MDP.

00:00:24.923 --> 00:00:27.590
And we only have to deal with
the fact that it's a SMDP.

00:00:27.590 --> 00:00:29.850
It just works,
I take an option is just an action.

00:00:29.850 --> 00:00:34.413
It just takes time and
isn't atomic but whatever.

00:00:34.413 --> 00:00:36.190
I might as well treat them
as if they were atomic.

00:00:36.190 --> 00:00:37.799
I just let it run and it comes back.

00:00:37.799 --> 00:00:40.628
And when it comes back it will
have accumulated for me all of

00:00:40.628 --> 00:00:44.260
the reward that I would accumulate if
I were living in the underlying MDP.

00:00:44.260 --> 00:00:48.796
And it will discount the future in
exactly the right way after I've

00:00:48.796 --> 00:00:49.853
taken k steps.

00:00:49.853 --> 00:00:50.805
&gt;&gt; Whoo-hoo!

00:00:50.805 --> 00:00:53.610
&gt;&gt; [LAUGH] Yes, exactly.

00:00:53.610 --> 00:00:57.435
So this is a lot of sort of
equation writing and deltas and

00:00:57.435 --> 00:01:01.880
delta raised to powers and expectations
that allows us to say, you know what?

00:01:01.880 --> 00:01:03.890
We're still dealing
with MDPs with options.

00:01:03.890 --> 00:01:05.410
Everything we knew before works.

00:01:05.410 --> 00:01:09.480
&gt;&gt; Okay, that's a relief, I guess, that
we didn't have to throw all that out and

00:01:09.480 --> 00:01:10.194
start afresh.

00:01:10.194 --> 00:01:13.270
&gt;&gt; Yeah, and
again I mean it's actually important.

00:01:13.270 --> 00:01:16.608
For me this seemed like a subtle
point for a long time but

00:01:16.608 --> 00:01:18.650
it's actually not that subtle.

00:01:18.650 --> 00:01:21.308
It's just an SMDP is just an MDP and

00:01:21.308 --> 00:01:26.808
where we have to keep track of the fact
that our actions are not atomic and

00:01:26.808 --> 00:01:31.250
accumulate reward and
accumulate all the transitions.

00:01:31.250 --> 00:01:34.232
But really, that's all captured
very nicely by this expectation.

00:01:34.232 --> 00:01:37.435
It just means we have to hide things
like the discount factors inside of

00:01:37.435 --> 00:01:38.480
our functions.

00:01:38.480 --> 00:01:40.190
Whereas before with the MDP we could

00:01:40.190 --> 00:01:42.110
stick them outside of
the transition functions.

00:01:42.110 --> 00:01:45.955
&gt;&gt; Okay, well then I'm not quite
getting that name, I think.

00:01:45.955 --> 00:01:50.410
Semi-Markov decision process sounds
like it's kind of Markov but

00:01:50.410 --> 00:01:52.280
also kind of not Markov.

00:01:52.280 --> 00:01:54.530
&gt;&gt; Yeah, that's right, it's semi-Markov.

00:01:54.530 --> 00:01:56.097
&gt;&gt; But this seems Markov, right?

00:01:56.097 --> 00:02:00.826
I mean, underneath it's just a whole
bunch of standard MDP-type transitions.

00:02:00.826 --> 00:02:02.025
&gt;&gt; Yeah, that's exactly right.

00:02:02.025 --> 00:02:04.973
But what's allowing it, but
at the level of the options right,

00:02:04.973 --> 00:02:08.039
it's not Markov because you have
to know how much time has passed.

00:02:10.740 --> 00:02:14.840
&gt;&gt; I see, I see, so where, right,
the state that we end up in

00:02:14.840 --> 00:02:18.850
could actually depend on how much time
we've spent, how much time has elapsed.

00:02:18.850 --> 00:02:21.280
&gt;&gt; In fact it will, in general, right?

00:02:21.280 --> 00:02:23.940
Because if I do it in one step I'm
going to end up in some state.

00:02:23.940 --> 00:02:26.690
I can't reach the states that
are more than one step away.

00:02:26.690 --> 00:02:29.120
If I take ten steps then
I can reach more states.

00:02:29.120 --> 00:02:31.196
So the state I end up
in does depend upon k.

00:02:31.196 --> 00:02:34.150
It depends upon how long
the option was executed.

00:02:34.150 --> 00:02:34.780
And in general,

00:02:34.780 --> 00:02:39.160
since beta is a probability over states,
it could happen at any time.

00:02:39.160 --> 00:02:42.290
&gt;&gt; I see, so it's got elements of
Markov and elements of not Markov.

00:02:42.290 --> 00:02:46.340
So elements of Markov include
things like well, we can

00:02:46.340 --> 00:02:49.860
think about the probability distribution
over where we're going to end up.

00:02:49.860 --> 00:02:51.470
It really is Markov.

00:02:51.470 --> 00:02:54.050
But we have to be much
more careful when we

00:02:54.050 --> 00:02:58.140
actually sum up rewards because
the time that has lapsed is variable.

00:02:58.140 --> 00:03:03.010
And so we have to actually take into
consideration how many steps have passed

00:03:03.010 --> 00:03:04.240
to do that.

00:03:04.240 --> 00:03:06.770
&gt;&gt; Right, and in fact is
that's a very important point.

00:03:06.770 --> 00:03:08.670
And I'll sort of point
out two things here.

00:03:08.670 --> 00:03:12.570
One is when we have a process
that is not Markovian,

00:03:12.570 --> 00:03:16.170
often we can turn into, in fact we
can turn basically any non-Markovian

00:03:16.170 --> 00:03:19.880
process into a Markovian process by just
keeping track of enough history, right?

00:03:20.890 --> 00:03:22.780
And history is, often time is enough.

00:03:22.780 --> 00:03:25.130
Sometimes you actually want to
know what states you visited but

00:03:25.130 --> 00:03:27.020
often time is enough.

00:03:27.020 --> 00:03:30.190
And so we can turn a non-Markovian
thing into a Markovian thing.

00:03:30.190 --> 00:03:32.350
And in some ways that's what
we're doing here, right?

00:03:32.350 --> 00:03:37.050
We are keeping track of the places
that we saw in between.

00:03:37.050 --> 00:03:38.430
But at the level of the SMDP,

00:03:38.430 --> 00:03:42.200
at the level of the options,
I can ignore the MDP and

00:03:42.200 --> 00:03:46.600
pretend that the SMDP is an MDP and
that these are really atomic actions.

00:03:46.600 --> 00:03:52.160
But in reality they aren't, which is
why R and F actually depend upon k.

00:03:52.160 --> 00:03:54.600
It actually matters how
many steps you took.

00:03:54.600 --> 00:03:58.090
And that affects the expectation.

00:03:58.090 --> 00:04:02.170
And in fact, if you look at R I am
actually accumulating all the reward.

00:04:02.170 --> 00:04:06.100
So I'm taking something that's
non-Markovian because it depends upon

00:04:06.100 --> 00:04:08.080
the actual rewards that I see,

00:04:08.080 --> 00:04:10.389
which depends upon the actual
states that I visit.

00:04:11.570 --> 00:04:14.690
And I'm gathering them
with me as I go along.

00:04:14.690 --> 00:04:18.450
So that when I pop out a level it
goes back to looking Markovian again.

00:04:18.450 --> 00:04:19.300
&gt;&gt; Cool.

00:04:19.300 --> 00:04:25.400
&gt;&gt; Right, so an SMDP, it's an MDP and
I don't have to think about it anymore.

00:04:25.400 --> 00:04:27.840
&gt;&gt; I feel like I'm getting
the kind of math of this.

00:04:27.840 --> 00:04:31.641
But it's still, it's still
a bit of an abstraction to me.

00:04:31.641 --> 00:04:36.790
Can we get into some kind of an example
to to practice with these ideas?

00:04:36.790 --> 00:04:39.374
&gt;&gt; So yes, first I want to point
out that I appreciate the pun,

00:04:39.374 --> 00:04:40.887
using the word abstraction here.

00:04:40.887 --> 00:04:42.187
&gt;&gt; Thanks.
&gt;&gt; Very good, I like that.

00:04:42.187 --> 00:04:44.830
But I think the answer is both yes and
no.

00:04:44.830 --> 00:04:48.020
I'm going to go through an example

00:04:48.020 --> 00:04:50.290
that I think might help you
think a little bit about this.

00:04:50.290 --> 00:04:53.220
But I also think that actually working
out all these expectations would take us

00:04:53.220 --> 00:04:53.970
a long time.

00:04:53.970 --> 00:04:56.337
And it's a really good and
useful exercise,

00:04:56.337 --> 00:04:58.836
which is why it should
probably be an exercise.

00:04:58.836 --> 00:05:00.330
We'll let the students do it.

00:05:00.330 --> 00:05:03.550
&gt;&gt; I mean, I don't know, it seems like a
cool exercise in dynamic programming and

00:05:03.550 --> 00:05:04.590
algorithm design.

00:05:04.590 --> 00:05:08.050
&gt;&gt; But Michael, you think everything is
a cool exercise in dynamic programming.

00:05:08.050 --> 00:05:09.250
&gt;&gt; Well that may be, but

00:05:09.250 --> 00:05:12.480
that's because everything is a cool
exercise in dynamic programming.

00:05:12.480 --> 00:05:13.590
&gt;&gt; Yes, that's an excellent point, and

00:05:13.590 --> 00:05:14.810
why we're going to move
on to the next slide.

