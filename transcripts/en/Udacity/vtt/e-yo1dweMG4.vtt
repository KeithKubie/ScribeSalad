WEBVTT
Kind: captions
Language: en

00:00:00.290 --> 00:00:01.100
Here. Let me put it to you this way

00:00:01.100 --> 00:00:05.420
Michael. What is MIMIC giving you for all of that

00:00:05.420 --> 00:00:08.550
work that it's doing in building dependency trees and, and

00:00:08.550 --> 00:00:11.072
you know, running Prim's algorithm and finding maximum spanning trees.

00:00:11.072 --> 00:00:13.150
&gt;&gt; I'm going to say structure because that's

00:00:13.150 --> 00:00:14.640
the word that you've been using repeatedly.

00:00:14.640 --> 00:00:16.149
&gt;&gt; You get structure. And, and another way of

00:00:16.149 --> 00:00:18.930
thinking about structure in this case is you're getting information.

00:00:18.930 --> 00:00:21.670
You get a lot more information because you get structure,

00:00:23.030 --> 00:00:25.970
you get a lot more information for iteration as well.

00:00:25.970 --> 00:00:27.220
So, that's the price you're paying,

00:00:27.220 --> 00:00:30.050
you're getting more information every single time

00:00:30.050 --> 00:00:34.640
you do an iteration, at the cost of building this maximum spanning trees or

00:00:34.640 --> 00:00:35.995
whatever it is you're doing in,

00:00:35.995 --> 00:00:39.090
in estimating your probability distribution. So, why

00:00:39.090 --> 00:00:44.020
would it be worth it to do that? What's the other source of expense?

00:00:44.020 --> 00:00:45.660
&gt;&gt; Well, so, all right. So there's

00:00:45.660 --> 00:00:47.550
all this computation within an iteration, but

00:00:47.550 --> 00:00:51.910
what it's, what it's doing, what it's trying to do is trying to find inputs

00:00:51.910 --> 00:00:55.070
that have high scores and so you do have to compute the

00:00:55.070 --> 00:00:58.430
scores for all those inputs. So the, the fitness calculation is very important.

00:00:58.430 --> 00:01:02.360
&gt;&gt; Right, so MIMIC tends to work very well when the cost of evaluating

00:01:02.360 --> 00:01:05.129
your fitness function is high. So, it's

00:01:05.129 --> 00:01:06.710
really important that I only have to take

00:01:06.710 --> 00:01:11.770
100 iterations if every single time I look at a fitness function and try

00:01:11.770 --> 00:01:16.700
to figure, compute it for some particular x, I pay some huge cost in time.

00:01:16.700 --> 00:01:17.410
&gt;&gt; Well,

00:01:17.410 --> 00:01:20.410
have you told us how many function evaluations there are in an

00:01:20.410 --> 00:01:23.820
iteration? because it seems like, it could be a lot in MIMIC.

00:01:23.820 --> 00:01:25.710
&gt;&gt; Well, you get one, basically for every sample you generate.

00:01:25.710 --> 00:01:28.600
&gt;&gt; Yeah and, and so, but so. I guess,

00:01:28.600 --> 00:01:32.105
you can compare iterations but you can also compare samples.

00:01:32.105 --> 00:01:34.070
&gt;&gt; Right, so they would depend upon how many

00:01:34.070 --> 00:01:35.990
samples you feel like you need to generate, and of

00:01:35.990 --> 00:01:38.920
course you can be very clever because. Remember, theta

00:01:38.920 --> 00:01:41.793
will generate a bunch of samples for theta plus 1.

00:01:41.793 --> 00:01:43.000
&gt;&gt; Mm-hm.

00:01:43.000 --> 00:01:45.150
&gt;&gt; So, if you keep track of the ones that

00:01:45.150 --> 00:01:47.140
you've values you've seen before, you don't have to recompute

00:01:47.140 --> 00:01:50.740
them. So it's actually pretty hard to know exactly what

00:01:50.740 --> 00:01:53.880
that's going to be. But let's imagine that at every iteration, you

00:01:53.880 --> 00:01:57.560
generate 100 samples. So, at most, you're going to have to

00:01:57.560 --> 00:02:01.640
evaluate f of x, 100 times. So, MIMIC is still

00:02:01.640 --> 00:02:05.100
a win over something else, if the number of iterations

00:02:05.100 --> 00:02:07.740
that it takes is 100 or more than 100 times fewer.

00:02:07.740 --> 00:02:08.473
&gt;&gt; Wow.

00:02:08.473 --> 00:02:09.692
&gt;&gt; Right?

00:02:09.692 --> 00:02:10.680
&gt;&gt; Yeah.

00:02:10.680 --> 00:02:13.990
&gt;&gt; So, can you think of functions, fitness functions,

00:02:13.990 --> 00:02:17.620
real fitness functions that might actually be expensive to compute?

00:02:17.620 --> 00:02:20.200
&gt;&gt; Well, yeah, sure. I mean a lot of this stuff that is,

00:02:20.200 --> 00:02:23.550
is important is like that. So, if you're trying to design a rocket

00:02:23.550 --> 00:02:28.670
ship or something like that, that. Fitness evaluation is, is doing

00:02:28.670 --> 00:02:33.788
a detailed simulation of how it performs. And that could be a very costly thing.

00:02:33.788 --> 00:02:36.620
&gt;&gt; Right, actually it's, that's a good example because MIMIC has

00:02:36.620 --> 00:02:39.250
been used for things like antenna design. It's been used for

00:02:39.250 --> 00:02:42.690
things like designing exactly where you would put a rocket in

00:02:42.690 --> 00:02:45.821
order to minimize fuel cost sending it to the moon. These

00:02:45.821 --> 00:02:49.490
sorts of things where the, the cost really isn't evaluating some

00:02:49.490 --> 00:02:52.760
particular configuration where you have to run a simulation or you

00:02:52.760 --> 00:02:56.890
have to compute a huge number of values of equations and

00:02:56.890 --> 00:02:59.220
so on and so forth, you know, to figure out the answer.

00:02:59.220 --> 00:03:00.550
Another case where it comes up a lot is

00:03:00.550 --> 00:03:05.340
where Your evaluation function, your fitness function, involves human beings.

00:03:05.340 --> 00:03:06.480
&gt;&gt; Oh, interesting, yeah.

00:03:06.480 --> 00:03:08.310
&gt;&gt; Where you generate something, and you ask

00:03:08.310 --> 00:03:10.000
a human, how does this look, or does

00:03:10.000 --> 00:03:12.650
this do what you want it to do because humans, as it turns out, are really slow.

00:03:12.650 --> 00:03:15.700
&gt;&gt; [LAUGH] Yeah, they're not measured in megaflops.

00:03:15.700 --> 00:03:17.370
&gt;&gt; That's right. So, you end up

00:03:17.370 --> 00:03:20.100
with cases where fitness functions are expensive, something

00:03:20.100 --> 00:03:21.960
like this becomes a big win. And that's

00:03:21.960 --> 00:03:24.470
a general point to make, though, that when.

00:03:24.470 --> 00:03:26.840
You are looking at all of the different algorithms, at different

00:03:26.840 --> 00:03:29.120
models or at everything that we have been doing, both for unsupervised

00:03:29.120 --> 00:03:32.682
learning and earlier for supervisor learning. A lot of times, your trade-off

00:03:32.682 --> 00:03:35.720
is just in terms of whether you are going to over-fit or not.

00:03:35.720 --> 00:03:39.060
It's whether it's worth the price you need to pay in

00:03:39.060 --> 00:03:42.580
terms of, either space or time complexity to go with one model

00:03:42.580 --> 00:03:45.150
versus the other. We've been talking about it in terms of sample

00:03:45.150 --> 00:03:47.610
complexity, but there's still time complexity

00:03:47.610 --> 00:03:49.120
and space complexity to worry about.

00:03:49.120 --> 00:03:49.650
&gt;&gt; Cool. I

00:03:49.650 --> 00:03:49.900
get it.

00:03:49.900 --> 00:03:52.876
&gt;&gt; Okay, cool. So normally what we would do next is we would

00:03:52.876 --> 00:03:56.168
say what, whatever we learn, but I actually kind of think we just did that.

00:03:56.168 --> 00:03:58.580
&gt;&gt; [LAUGH] indeed.

00:03:58.580 --> 00:04:01.650
&gt;&gt; So, in fact, let me do something about that.

