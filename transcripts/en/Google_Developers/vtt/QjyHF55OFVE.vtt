WEBVTT
Kind: captions
Language: en

00:00:02.434 --> 00:00:03.850
SUZANNE XIE: Our
world has changed

00:00:03.850 --> 00:00:07.640
so much since I was a kid that
things that I want to do now

00:00:07.640 --> 00:00:09.334
weren't even possible
for me to imagine.

00:00:09.334 --> 00:00:10.834
JEANNIE YANG: I
don't even know if I

00:00:10.834 --> 00:00:12.800
knew it was called computer
science and programming.

00:00:12.800 --> 00:00:14.710
KELLY ELLIS: It all sort
of happened by accident.

00:00:14.710 --> 00:00:15.430
BRYNN EVANS: I
was always excited

00:00:15.430 --> 00:00:17.139
about technology in
some way, and people.

00:00:17.139 --> 00:00:18.638
LIMOR FRIED: I
thought that was just

00:00:18.638 --> 00:00:20.740
really neat to be able to
control the technology,

00:00:20.740 --> 00:00:22.390
instead of just
having it control you.

00:00:22.390 --> 00:00:23.620
ASHLEY GAVIN: I think
a lot of people wake up

00:00:23.620 --> 00:00:25.180
and they have an amazing idea.

00:00:25.180 --> 00:00:27.936
When you know how to code, you
can go and actually make that

00:00:27.936 --> 00:00:28.435
a reality.

00:00:28.435 --> 00:00:30.550
SHANNON SPANHAKE: I am the
deputy innovation officer.

00:00:30.550 --> 00:00:31.780
BRYNN EVANS: I'm a design lead.

00:00:31.780 --> 00:00:32.600
JAY WONG: A UI designer.

00:00:32.600 --> 00:00:33.420
CASSANDRA DIXON:
A port engineer.

00:00:33.420 --> 00:00:34.240
KELLY ELLIS: Software engineer.

00:00:34.240 --> 00:00:35.360
JOANNA SMITH: Developer
programs engineer.

00:00:35.360 --> 00:00:37.210
JEANNIE YANG: Chief
product and design officer.

00:00:37.210 --> 00:00:37.680
MEGAN SMITH: I'm a VP.

00:00:37.680 --> 00:00:39.370
NATALIE GORDON: I'm
the CEO and founder.

00:00:39.370 --> 00:00:39.910
HEIDI ZAK SPECTOR: Co-founder.

00:00:39.910 --> 00:00:41.250
BANDANA MALIK: I am a coder.

00:00:41.250 --> 00:00:44.100
MARY LOU JEPSEN: By learning
about engineering, computer

00:00:44.100 --> 00:00:45.640
science, and other
technical skills,

00:00:45.640 --> 00:00:48.000
you can solve the
really big problems.

00:00:48.000 --> 00:00:50.670
MEGAN SMITH: We're trying to
do things that could radically

00:00:50.670 --> 00:00:51.490
improve the world.

00:00:51.490 --> 00:00:53.170
JAY WONG: Things like
transportation and health.

00:00:53.170 --> 00:00:55.010
BANDANA MALIK: Making our
cities better places to live,

00:00:55.010 --> 00:00:56.420
and making people
more connected.

00:00:56.420 --> 00:00:59.020
JENNY YOUNG: A place for city
kids to build and create.

00:00:59.020 --> 00:01:02.390
MARY LOU JEPSEN: I created a
moon TV system to project video

00:01:02.390 --> 00:01:02.894
on the moon.

00:01:02.894 --> 00:01:04.810
HEIDI ZAK SPECTOR:
Computer vision technology.

00:01:04.810 --> 00:01:05.970
LIMOR FRIED: Open-source
hardware company.

00:01:05.970 --> 00:01:06.890
SHANNON SPANHAKE:
Measure air pollution

00:01:06.890 --> 00:01:08.410
with your mobile phone.

00:01:08.410 --> 00:01:11.070
BRYNN EVANS: I love helping
people do the things

00:01:11.070 --> 00:01:13.427
that they love to
do more easily.

00:01:13.427 --> 00:01:15.010
AMINATOU SOW: I
really, really, really

00:01:15.010 --> 00:01:17.997
enjoy the fact that I get
energy from so many people.

00:01:17.997 --> 00:01:20.080
ASHLEY GAVIN: I just hope
that I can see the world

00:01:20.080 --> 00:01:21.240
change in my lifetime.

00:01:21.240 --> 00:01:22.540
That inspires me.

00:01:22.540 --> 00:01:25.200
CASSANDRA DIXON: So build it
around something that you like.

00:01:25.200 --> 00:01:26.600
KELLY ELLIS: Find something
that interests you.

00:01:26.600 --> 00:01:27.810
MEGAN SMITH: What are
you passionate about?

00:01:27.810 --> 00:01:29.643
NATALIE GORDON: Know
what's the thing that's

00:01:29.643 --> 00:01:30.750
going to keep you going.

00:01:30.750 --> 00:01:32.930
MARY LOU JEPSEN: And
you can really, really

00:01:32.930 --> 00:01:36.848
have an impact on the world.

00:01:36.848 --> 00:01:39.824
[PIANO MUSIC PLAYING]

00:01:44.264 --> 00:01:45.430
PAVNI DIWANJI: Hi, everyone.

00:01:45.430 --> 00:01:45.950
Welcome.

00:01:45.950 --> 00:01:47.540
I am Pavni Diwanji.

00:01:47.540 --> 00:01:49.550
I'm a VP of
engineering at Google.

00:01:49.550 --> 00:01:53.800
And I was so stoked
when Natalie asked

00:01:53.800 --> 00:01:56.230
me to introduce this session.

00:01:56.230 --> 00:01:58.870
I have been, from a very
early age, very fascinated

00:01:58.870 --> 00:02:00.610
with robots.

00:02:00.610 --> 00:02:03.040
A lot of my childhood
conversations

00:02:03.040 --> 00:02:09.190
were dominated by R2D2 and
C3PO, at home and at school.

00:02:09.190 --> 00:02:11.450
There were two factions at
school in the geek crowd,

00:02:11.450 --> 00:02:11.950
you know?

00:02:11.950 --> 00:02:13.870
There was the R2D2 faction.

00:02:13.870 --> 00:02:16.170
And then there was
the C3PO faction.

00:02:16.170 --> 00:02:19.150
And we spent endless hours
debating virtues and vices

00:02:19.150 --> 00:02:22.340
of those beloved robots, so.

00:02:22.340 --> 00:02:25.930
And I have to say, nothing
really has changed.

00:02:25.930 --> 00:02:31.970
A few months ago, I was sitting
at my Sunday breakfast family

00:02:31.970 --> 00:02:35.960
place and my daughter--
we were eating pancakes.

00:02:35.960 --> 00:02:38.980
And my daughter blurted
out very suddenly

00:02:38.980 --> 00:02:43.520
that she wished C3
was more like Jarvis.

00:02:43.520 --> 00:02:49.270
Now, I kind of know that she's
a Iron Man, Tony Stark fanatic.

00:02:49.270 --> 00:02:52.480
But it still kind of
took me by surprise.

00:02:52.480 --> 00:02:55.280
Normally, our conversations
are not that interesting.

00:02:55.280 --> 00:02:59.200
It's normally about school,
and Minecraft, and Pokemon.

00:02:59.200 --> 00:03:01.790
And robots show up
in the conversation.

00:03:01.790 --> 00:03:02.949
I'm like, OK, OK.

00:03:02.949 --> 00:03:03.740
Take a deep breath.

00:03:03.740 --> 00:03:05.640
And I'm like, "Tell me more."

00:03:05.640 --> 00:03:08.400
And so-- and then, but
before she could answer,

00:03:08.400 --> 00:03:11.090
my younger daughter,
Maya, who's eight,

00:03:11.090 --> 00:03:13.570
was not going to be left
out of the conversation.

00:03:13.570 --> 00:03:15.340
And she jumped in.

00:03:15.340 --> 00:03:17.320
And she says,
"Well, it's obvious

00:03:17.320 --> 00:03:19.105
why she likes Jarvis more."

00:03:19.105 --> 00:03:20.210
I'm like, why?

00:03:20.210 --> 00:03:22.770
"Well, C3 just
doesn't get kids."

00:03:22.770 --> 00:03:25.000
I'm like, "Oh, is
that the reason?"

00:03:25.000 --> 00:03:27.620
So I'm like mentally getting
prepared to explain to them why

00:03:27.620 --> 00:03:30.300
it's such a hard
problem to recognize

00:03:30.300 --> 00:03:32.430
different types of
voices and stuff.

00:03:32.430 --> 00:03:34.880
But before I get a
chance to actually get

00:03:34.880 --> 00:03:38.890
into the conversation, there's
a sibling rivalry going on.

00:03:38.890 --> 00:03:41.614
And Ishana, who is a
teenager, gets back

00:03:41.614 --> 00:03:42.530
into the conversation.

00:03:42.530 --> 00:03:44.660
Like, "That's not it at all.

00:03:44.660 --> 00:03:47.250
I really like Jarvis
because it has

00:03:47.250 --> 00:03:50.840
a real personality
and an attitude."

00:03:50.840 --> 00:03:52.810
So I'm like, OK.

00:03:52.810 --> 00:03:54.950
Now we are talking about
robots with a personality

00:03:54.950 --> 00:03:55.640
and an attitude.

00:03:55.640 --> 00:03:56.360
OK.

00:03:56.360 --> 00:03:59.060
So anyway, where this
conversation ended was

00:03:59.060 --> 00:04:01.730
my eight-year-old
is like telling me,

00:04:01.730 --> 00:04:03.050
like, "I want to build a robot.

00:04:03.050 --> 00:04:06.180
And Mommy, I think I can build
a better robot than Jarvis."

00:04:06.180 --> 00:04:09.990
And, like parents do,
I'm like letting it pass.

00:04:09.990 --> 00:04:13.220
And next day, she's
like relentless, right,

00:04:13.220 --> 00:04:14.115
a few days following.

00:04:14.115 --> 00:04:15.990
So I'm, like, scratching
my head, what to do.

00:04:15.990 --> 00:04:18.589
And so finally, I ended up
getting her a LEGO Mindstorm

00:04:18.589 --> 00:04:19.089
Kit.

00:04:19.089 --> 00:04:20.790
And here she is!

00:04:20.790 --> 00:04:24.020
She built a little rover
robot a few months ago.

00:04:24.020 --> 00:04:28.373
And the reason I
tell this story is--

00:04:28.373 --> 00:04:31.211
[APPLAUSE]

00:04:31.211 --> 00:04:33.540
It's a silly family story.

00:04:33.540 --> 00:04:36.370
But I like to tell it,
because I find it fascinating

00:04:36.370 --> 00:04:39.090
that robots never cease to
be an interesting topic,

00:04:39.090 --> 00:04:41.450
no matter how old you get.

00:04:41.450 --> 00:04:45.730
And I remember, at
a very early age,

00:04:45.730 --> 00:04:50.290
going through thought exercises
on how to build a C3PO.

00:04:50.290 --> 00:04:55.510
And yeah, I was in that faction.

00:04:55.510 --> 00:04:57.180
But-- and it was kind of hard.

00:04:57.180 --> 00:04:59.500
You have to imagine a
time where I was in India.

00:04:59.500 --> 00:05:02.030
There's not that many
resources available.

00:05:02.030 --> 00:05:03.710
TV had just arrived.

00:05:03.710 --> 00:05:06.900
No internet, no cloud,
no Google, right?

00:05:06.900 --> 00:05:10.920
It's kind of hard to imagine
bringing C3PO to life.

00:05:10.920 --> 00:05:14.000
But now, the landscape
has changed, right?

00:05:14.000 --> 00:05:15.870
We have fast networks.

00:05:15.870 --> 00:05:17.100
We have maps.

00:05:17.100 --> 00:05:20.360
We know-- we understand
the terrain really well.

00:05:20.360 --> 00:05:21.530
We have the cloud.

00:05:21.530 --> 00:05:23.190
We have Google, right?

00:05:23.190 --> 00:05:26.740
Robots don't have to do it all
alone, if they have the cloud

00:05:26.740 --> 00:05:28.350
helpline, as I like to call it.

00:05:28.350 --> 00:05:31.680
Oh, I don't know how to
recognize this object.

00:05:31.680 --> 00:05:34.190
Let's dial the helpline, right?

00:05:34.190 --> 00:05:37.220
I don't know how to-- I don't
quite understand the language.

00:05:37.220 --> 00:05:40.910
They can also ask the cloud.

00:05:40.910 --> 00:05:48.300
So indeed, it's like a really
different, exciting time.

00:05:48.300 --> 00:05:51.140
And a lot has changed.

00:05:51.140 --> 00:05:55.050
And I feel like robots
are amongst us already.

00:05:55.050 --> 00:05:58.060
It's hard to imagine
a future without them,

00:05:58.060 --> 00:06:00.680
whether it's like cuddly
toys we're talking about,

00:06:00.680 --> 00:06:03.680
or robots helping
us in emergencies,

00:06:03.680 --> 00:06:09.570
or robots helping us with
the car assembly line,

00:06:09.570 --> 00:06:13.860
or driving cars for us,
or navigating on Mars.

00:06:13.860 --> 00:06:15.330
You know, they are here to stay.

00:06:15.330 --> 00:06:19.800
And they're here next to us,
helping us for the future.

00:06:19.800 --> 00:06:23.720
So today, I am
super-excited, because you're

00:06:23.720 --> 00:06:29.600
going to hear from three awesome
Googlers who have built robots

00:06:29.600 --> 00:06:32.830
to assist a person in
need, assist a community,

00:06:32.830 --> 00:06:35.310
and assist humanity at large.

00:06:35.310 --> 00:06:38.620
And I would like
to invite Yoky, who

00:06:38.620 --> 00:06:41.480
is our vice president
of technology at Nest,

00:06:41.480 --> 00:06:44.710
to kick off this very
exciting session.

00:06:44.710 --> 00:06:45.593
Welcome.

00:06:45.593 --> 00:06:50.430
[APPLAUSE]

00:06:50.430 --> 00:06:52.730
YOKY MATSUOKA: Hello, everybody.

00:06:52.730 --> 00:06:54.920
I'm so happy to be here today.

00:06:54.920 --> 00:06:58.470
At Nest, for those of you who
know Nest, as well as those

00:06:58.470 --> 00:07:00.780
of you who are getting
introduced today,

00:07:00.780 --> 00:07:04.650
we build-- or, we actually
reinvent-- unloved products

00:07:04.650 --> 00:07:07.960
in your house, such
as thermostats,

00:07:07.960 --> 00:07:09.180
and smoke detector.

00:07:09.180 --> 00:07:12.600
And we actually today
launched a developers program.

00:07:12.600 --> 00:07:13.180
Yay!

00:07:13.180 --> 00:07:15.930
[APPLAUSE]

00:07:15.930 --> 00:07:16.770
Plug.

00:07:16.770 --> 00:07:17.280
Unplug.

00:07:17.280 --> 00:07:21.830
So-- and then, what we--
and one of the things,

00:07:21.830 --> 00:07:24.780
actually, we do is that we
build beautiful product that

00:07:24.780 --> 00:07:25.750
used to be unloved.

00:07:25.750 --> 00:07:26.950
Now, people start to love.

00:07:26.950 --> 00:07:29.370
And not only beautiful
to look at, but also

00:07:29.370 --> 00:07:31.000
beautiful to interact with.

00:07:31.000 --> 00:07:33.140
And the kind of
things that we do

00:07:33.140 --> 00:07:35.510
is to really
understand how humans

00:07:35.510 --> 00:07:38.380
learn to interact
with certain devices,

00:07:38.380 --> 00:07:41.260
and then have machine
learning to interact together.

00:07:41.260 --> 00:07:45.070
And this synergy is where we can
make the experience beautiful.

00:07:45.070 --> 00:07:46.820
So that's what we're
going to talk about--

00:07:46.820 --> 00:07:48.760
except that this
is where I'm going

00:07:48.760 --> 00:07:50.090
to stop talking about Nest.

00:07:50.090 --> 00:07:51.520
And I'm going to
actually tell you

00:07:51.520 --> 00:07:54.500
a little bit about
robotics, and a little bit

00:07:54.500 --> 00:07:57.650
of a history of
how I got to Nest.

00:07:57.650 --> 00:08:00.460
And, really, I'm going
to talk about enabling

00:08:00.460 --> 00:08:01.750
human experience.

00:08:01.750 --> 00:08:04.800
And again, this machine
learning and human experience

00:08:04.800 --> 00:08:08.010
combination-- really,
what I like to do

00:08:08.010 --> 00:08:09.980
is to enable who you want to be.

00:08:09.980 --> 00:08:12.385
So just take a
thermostat as an example.

00:08:12.385 --> 00:08:14.260
How many people actually
like to save energy?

00:08:16.892 --> 00:08:17.600
Almost everybody.

00:08:17.600 --> 00:08:18.610
OK, that's great.

00:08:18.610 --> 00:08:21.420
So how many people who
are staying at the hotel

00:08:21.420 --> 00:08:24.170
and remember to turn off your
air conditioner or heating

00:08:24.170 --> 00:08:26.470
right before you
left the room today?

00:08:26.470 --> 00:08:27.800
Not enough.

00:08:27.800 --> 00:08:29.660
And that's pretty common.

00:08:29.660 --> 00:08:30.910
But don't worry.

00:08:30.910 --> 00:08:33.090
What we can do
with the technology

00:08:33.090 --> 00:08:36.350
through understanding who you
are-- you want to save energy.

00:08:36.350 --> 00:08:38.750
But you sometimes forget,
because you're human.

00:08:38.750 --> 00:08:41.110
Well, we can [INAUDIBLE]
for that together.

00:08:41.110 --> 00:08:42.820
See, this is kind
of the synergisting

00:08:42.820 --> 00:08:44.210
that I'm talking about.

00:08:44.210 --> 00:08:46.260
So, you want to be
an energy saver?

00:08:46.260 --> 00:08:47.440
Great.

00:08:47.440 --> 00:08:49.100
We'll enable it.

00:08:49.100 --> 00:08:51.790
Well, I actually wanted
to be a tennis player.

00:08:51.790 --> 00:08:55.170
So a technology that could
enable me to be a tennis player

00:08:55.170 --> 00:08:56.370
would be great.

00:08:56.370 --> 00:08:58.190
But, as I grew up
playing tennis,

00:08:58.190 --> 00:09:01.440
and I was competing at
an international level,

00:09:01.440 --> 00:09:04.900
I started to get injured
all the time in college.

00:09:04.900 --> 00:09:07.240
And I have come to
realize that maybe I'm

00:09:07.240 --> 00:09:10.210
not going to be the number-one
player in the world.

00:09:10.210 --> 00:09:13.400
So, at the point, I
had to reinvent myself,

00:09:13.400 --> 00:09:16.195
and said, well, what
am I going to do next?

00:09:16.195 --> 00:09:18.260
Well, I know tennis.

00:09:18.260 --> 00:09:21.690
How about if I built a little
tennis buddy for myself?

00:09:21.690 --> 00:09:24.190
It's made of robots,
has legs and arms,

00:09:24.190 --> 00:09:26.270
and then it has
buttons and a tummy.

00:09:26.270 --> 00:09:28.410
And then if you push
a certain button,

00:09:28.410 --> 00:09:31.600
it might make that robot to
be really, really amazing

00:09:31.600 --> 00:09:34.650
and a great person-- I mean,
great robot to play with.

00:09:34.650 --> 00:09:36.211
Or another button
that you can press,

00:09:36.211 --> 00:09:38.210
which would make it a
little bit weak on the day

00:09:38.210 --> 00:09:39.751
that I'm feeling a
little down, and I

00:09:39.751 --> 00:09:41.230
want to beat something up.

00:09:41.230 --> 00:09:43.439
So that's the kind of
things that I was imagining.

00:09:43.439 --> 00:09:45.480
Wouldn't that be great if
I could build something

00:09:45.480 --> 00:09:46.376
like this?

00:09:46.376 --> 00:09:47.750
So, throughout
college, I started

00:09:47.750 --> 00:09:49.410
to do much more of engineering.

00:09:49.410 --> 00:09:51.850
And I got involved, and I
even went to grad school

00:09:51.850 --> 00:09:54.090
to build a humanoid robot
that looks like this.

00:09:54.090 --> 00:09:55.240
This is called Cog.

00:09:55.240 --> 00:09:58.540
And I joined a team to
build arms and hands,

00:09:58.540 --> 00:10:02.250
and to really try to approach
the human-level cognition

00:10:02.250 --> 00:10:07.330
to intelligence, to movement
that we can mimic like humans.

00:10:07.330 --> 00:10:09.450
Well, but a couple
of years later,

00:10:09.450 --> 00:10:11.680
I realized that it's
really hard to do that

00:10:11.680 --> 00:10:14.070
with-- at that
time-- the technology

00:10:14.070 --> 00:10:16.680
of artificial intelligence
and machine learning.

00:10:16.680 --> 00:10:19.030
And I felt the reason
is because we just

00:10:19.030 --> 00:10:22.320
don't understand the human brain
and how it works well enough.

00:10:22.320 --> 00:10:23.830
So, I jumped across.

00:10:23.830 --> 00:10:26.800
I studied neuroscience
so that, one day,

00:10:26.800 --> 00:10:29.980
I was still hoping that I can
build a tennis player that

00:10:29.980 --> 00:10:32.220
could play tennis with me.

00:10:32.220 --> 00:10:35.120
But throughout this
whole sort of journey

00:10:35.120 --> 00:10:38.255
of migrating to neuroscience,
what I've come to learn

00:10:38.255 --> 00:10:39.630
is that there are
a lot of people

00:10:39.630 --> 00:10:43.137
who have neurological
disorders who could get help

00:10:43.137 --> 00:10:44.720
by all the knowledge
that I've already

00:10:44.720 --> 00:10:48.330
had in machine learning,
AI, and robotics.

00:10:48.330 --> 00:10:52.790
So that's when I started to
think, maybe, just maybe.

00:10:52.790 --> 00:10:55.560
What I like to do is to
really help other people.

00:10:55.560 --> 00:10:58.340
Somehow sort of reach
out to those people,

00:10:58.340 --> 00:11:00.580
using some things
that I already know.

00:11:00.580 --> 00:11:03.060
And trying to see
if I can help them

00:11:03.060 --> 00:11:04.720
become who they want to be.

00:11:04.720 --> 00:11:06.600
So that's pretty
much what I did.

00:11:06.600 --> 00:11:08.790
I was a professor
for over a decade,

00:11:08.790 --> 00:11:11.500
really trying to invent this
field called neurobiotics,

00:11:11.500 --> 00:11:13.880
or you might know, as it a
brain-computer interface,

00:11:13.880 --> 00:11:15.010
brain-machine interface.

00:11:15.010 --> 00:11:18.330
It's really about taking
some of the brain signals,

00:11:18.330 --> 00:11:21.280
and then extracting it, and
technologically speaking.

00:11:21.280 --> 00:11:22.550
And then enhance it.

00:11:22.550 --> 00:11:25.520
And then put it back on either
a limb that's paralyzed,

00:11:25.520 --> 00:11:29.690
or a prosthetic limb for those
people who lost their limbs.

00:11:29.690 --> 00:11:34.240
So this is a tool, as well
as a future prosthetic limb

00:11:34.240 --> 00:11:37.570
that I built, spending a long
time trying to understand how

00:11:37.570 --> 00:11:42.460
humans make such an amazing
motion using their hand.

00:11:42.460 --> 00:11:44.880
This is so simple for us.

00:11:44.880 --> 00:11:47.850
Yet, for a robot to
just even do this,

00:11:47.850 --> 00:11:51.170
this is almost completely
impossible, even today.

00:11:51.170 --> 00:11:52.350
So why is that?

00:11:52.350 --> 00:11:54.100
Is this a mechanical secret?

00:11:54.100 --> 00:11:57.020
Or is this something that
our brain is sending?

00:11:57.020 --> 00:12:00.010
So, really dove into
the scientific deep end

00:12:00.010 --> 00:12:02.940
of trying to understand how
we can enable those people who

00:12:02.940 --> 00:12:05.560
lost their movement.

00:12:05.560 --> 00:12:07.020
And through that
research, actually

00:12:07.020 --> 00:12:09.750
something-- lots of
nuggets have been found.

00:12:09.750 --> 00:12:11.130
And some of the
nuggets have gone

00:12:11.130 --> 00:12:13.430
into actual prosthetic devices
that thousands of people

00:12:13.430 --> 00:12:13.930
wear now.

00:12:13.930 --> 00:12:17.850
This is called Touch
Bionics i-Limb.

00:12:17.850 --> 00:12:19.760
So those are really
exciting little nuggets.

00:12:19.760 --> 00:12:22.720
But, primarily, I was
really thinking about people

00:12:22.720 --> 00:12:26.190
about 30 years down the line.

00:12:26.190 --> 00:12:29.870
But it didn't just have to
be about wearable devices.

00:12:29.870 --> 00:12:33.380
It could be something that
just sits on the ground.

00:12:33.380 --> 00:12:36.460
This is a robot that
sits on the ground.

00:12:36.460 --> 00:12:39.460
And then somebody-- this
is a student from Harvard,

00:12:39.460 --> 00:12:42.870
but pretending to be a stroke--
somebody who had a stroke.

00:12:42.870 --> 00:12:46.310
And sitting and then interacting
with the robotic device safely.

00:12:46.310 --> 00:12:49.150
As he's interacting
with it, now the robot

00:12:49.150 --> 00:12:51.474
is gathering information
about the person.

00:12:51.474 --> 00:12:53.050
The person wants to recover.

00:12:53.050 --> 00:12:55.100
The person who had
an injury, and then

00:12:55.100 --> 00:12:57.690
maybe have half
the body paralyzed,

00:12:57.690 --> 00:13:00.230
can actually try to
start to exercise.

00:13:00.230 --> 00:13:02.370
And then the robot
side can understand,

00:13:02.370 --> 00:13:03.274
is this person tired?

00:13:03.274 --> 00:13:04.690
Is this person
learning right now?

00:13:04.690 --> 00:13:07.330
Is this person able to get
pushed a little further?

00:13:07.330 --> 00:13:10.790
And can have this synergistic
activity between human learning

00:13:10.790 --> 00:13:12.280
and then the
machine learning, so

00:13:12.280 --> 00:13:14.880
that we can get just the
right amount of exercise

00:13:14.880 --> 00:13:17.880
and rehabilitation
to make them better.

00:13:17.880 --> 00:13:19.830
And we even tried
to miniaturize it.

00:13:19.830 --> 00:13:23.270
This is somebody who's
playing a virtual goggle video

00:13:23.270 --> 00:13:27.350
game as she is getting her
movements exercised in a way

00:13:27.350 --> 00:13:30.410
that she was not
possible before.

00:13:30.410 --> 00:13:32.760
So all of this was great.

00:13:32.760 --> 00:13:35.620
And I was a professor,
and I loved teaching.

00:13:35.620 --> 00:13:39.270
But I was getting a lot of
emails on the side from people,

00:13:39.270 --> 00:13:43.640
people today who were injured
and really needed some help.

00:13:43.640 --> 00:13:45.700
And they read about my
research, and said, well,

00:13:45.700 --> 00:13:47.580
is there anything
that you can do?

00:13:47.580 --> 00:13:50.252
And eventually, I sort of-- it
tickled me enough to the point

00:13:50.252 --> 00:13:51.710
that I thought,
you know what, it's

00:13:51.710 --> 00:13:53.550
great to push the
boundary of science.

00:13:53.550 --> 00:13:55.140
But at the same
time, I would love

00:13:55.140 --> 00:13:57.610
to be able to do something
for people today.

00:13:57.610 --> 00:13:59.640
So I built a
foundation on the side.

00:13:59.640 --> 00:14:00.890
It's called YokyWorks.

00:14:00.890 --> 00:14:04.322
It's really about making
something, technology--

00:14:04.322 --> 00:14:06.280
especially the intersection
of machine learning

00:14:06.280 --> 00:14:08.710
and human learning--
to enable you

00:14:08.710 --> 00:14:10.200
to become who you want to be.

00:14:10.200 --> 00:14:12.619
So we did things
anywhere from, you know,

00:14:12.619 --> 00:14:14.660
from a kid who wanted to
become a faster swimmer,

00:14:14.660 --> 00:14:17.530
but had an amputation.

00:14:17.530 --> 00:14:20.430
Is there something
that we can build?

00:14:20.430 --> 00:14:23.480
And then another project that
I'm going to highlight today,

00:14:23.480 --> 00:14:27.150
just as an example, is
called Project Maria.

00:14:27.150 --> 00:14:32.600
And Maria was a seven-year-old
girl who came to YokyWorks

00:14:32.600 --> 00:14:37.040
and said-- actually, Maria's
parents came to us-- and said,

00:14:37.040 --> 00:14:39.130
is there anything
that Maria can do?

00:14:39.130 --> 00:14:40.390
She has cerebral palsy.

00:14:40.390 --> 00:14:43.750
She basically had a stroke when
she was about seven months old.

00:14:43.750 --> 00:14:46.650
She lost almost all the
movements from her body.

00:14:46.650 --> 00:14:48.050
She was wheelchair bound.

00:14:48.050 --> 00:14:49.820
And there's absolutely
nothing on her body

00:14:49.820 --> 00:14:53.530
that really could
move with her wish.

00:14:53.530 --> 00:14:55.490
And we got together.

00:14:55.490 --> 00:14:57.950
And then we watched
her, how she moved,

00:14:57.950 --> 00:15:01.000
and if there's anything
we can bring out from her.

00:15:01.000 --> 00:15:04.850
As we watched, what she had
was this on the wheelchair.

00:15:04.850 --> 00:15:06.920
This is called a
communication board.

00:15:06.920 --> 00:15:10.890
And all she could
do was to move just

00:15:10.890 --> 00:15:14.820
in the general direction
of maybe four corners.

00:15:14.820 --> 00:15:17.460
But she had no motion
that could allow

00:15:17.460 --> 00:15:19.870
her to stop at any
of the squares.

00:15:19.870 --> 00:15:22.820
So all she could
do was to say, yes.

00:15:22.820 --> 00:15:23.960
I'm in pain.

00:15:23.960 --> 00:15:25.430
I have to go to the bathroom.

00:15:25.430 --> 00:15:27.060
That's about it.

00:15:27.060 --> 00:15:28.854
But her parents said--
and I felt the same,

00:15:28.854 --> 00:15:30.270
as we were watching
her-- that she

00:15:30.270 --> 00:15:33.840
has something inside, a very,
very smart person inside.

00:15:33.840 --> 00:15:36.260
And we wanted to enable this.

00:15:36.260 --> 00:15:37.980
So, as the parents
and I were talking,

00:15:37.980 --> 00:15:39.880
and the therapist was
there, Maria got bored.

00:15:39.880 --> 00:15:41.380
I mean, any
seven-year-old would get

00:15:41.380 --> 00:15:43.046
bored of parents
talking about something

00:15:43.046 --> 00:15:44.080
that's pretty serious.

00:15:44.080 --> 00:15:47.610
So she asked, in
sort of her way,

00:15:47.610 --> 00:15:49.610
and her parents noticed
that she should probably

00:15:49.610 --> 00:15:51.440
look at the iPhone pictures.

00:15:51.440 --> 00:15:54.022
So her dad gave her the phone.

00:15:54.022 --> 00:15:55.480
And as I watched
in the background,

00:15:55.480 --> 00:15:57.900
she was swiping like this.

00:15:57.900 --> 00:16:00.030
I was like, wait a second.

00:16:00.030 --> 00:16:03.590
So if she was motivated enough
to want to look at something,

00:16:03.590 --> 00:16:05.860
she can actually make
those swiping motions,

00:16:05.860 --> 00:16:09.250
even though she is unable to
stop at a specific location.

00:16:09.250 --> 00:16:10.640
So we said, aha!

00:16:10.640 --> 00:16:12.820
Why don't we take
advantage of the fact

00:16:12.820 --> 00:16:15.800
that she can make those
motions, and maybe

00:16:15.800 --> 00:16:19.070
allow us to-- so we should
build a technology that

00:16:19.070 --> 00:16:20.980
can adapt to her
motion over time.

00:16:20.980 --> 00:16:22.800
And it maybe just,
understand how much

00:16:22.800 --> 00:16:24.040
she can be pushed to learn.

00:16:24.040 --> 00:16:26.130
And then we'll build a
technology around it.

00:16:26.130 --> 00:16:29.350
So we went to-- a
nonprofit has no money.

00:16:29.350 --> 00:16:30.830
So we went to Toys 'R' Us.

00:16:30.830 --> 00:16:35.414
And we had found an
electronic drum machine.

00:16:35.414 --> 00:16:37.080
And we said, oh, this
looks really good.

00:16:37.080 --> 00:16:40.150
They have, like, areas
that she can go to.

00:16:40.150 --> 00:16:42.860
But wait, this is a
pushing motion still.

00:16:42.860 --> 00:16:45.000
She can't make
pushing precisely.

00:16:45.000 --> 00:16:48.210
So we also bought rubber
balls, cut it in half,

00:16:48.210 --> 00:16:52.950
and then we put them-- instead
of those little, the surface

00:16:52.950 --> 00:16:57.090
piece, now, she's moved
to swipe, like the iPhone,

00:16:57.090 --> 00:16:58.430
into different directions.

00:16:58.430 --> 00:17:02.300
She was able to knock
down on different corners.

00:17:02.300 --> 00:17:05.619
And now, this is
actually beautiful Maria.

00:17:05.619 --> 00:17:08.160
Just, she's learning how to use
this device-- as you can see,

00:17:08.160 --> 00:17:11.734
the drum set with the
rubber ball attached to it.

00:17:11.734 --> 00:17:13.109
Of course, we were
shooting them.

00:17:13.109 --> 00:17:15.109
And she was a little shy
in front of the camera.

00:17:15.109 --> 00:17:18.630
But you will see her sort
of general swiping motion,

00:17:18.630 --> 00:17:20.500
the kind of things
that she can do.

00:17:20.500 --> 00:17:22.800
So she's able to
actually be told

00:17:22.800 --> 00:17:24.544
which ones she should
go for, and then

00:17:24.544 --> 00:17:28.980
trying to target toward those
motions, the directions.

00:17:28.980 --> 00:17:32.660
So what I loved about
this idea was not only

00:17:32.660 --> 00:17:35.810
that we were replacing those
communications in a much more

00:17:35.810 --> 00:17:37.920
precise way-- she just
had to knock one of those

00:17:37.920 --> 00:17:41.040
over with the motions
she already has.

00:17:41.040 --> 00:17:43.570
But now, we've started
to say, you know what?

00:17:43.570 --> 00:17:46.330
If she can get good
at those four corners,

00:17:46.330 --> 00:17:49.410
she can actually start to
build a vocabulary around it.

00:17:49.410 --> 00:17:54.420
What if left top and
bottom right was letter a?

00:17:54.420 --> 00:17:58.220
What if some other combination
was letter b, and so forth.

00:17:58.220 --> 00:18:00.240
And if we learned
it together, she

00:18:00.240 --> 00:18:03.640
could actually learn to spell.

00:18:03.640 --> 00:18:05.650
She could learn to form essays.

00:18:05.650 --> 00:18:07.620
She could take SATs.

00:18:07.620 --> 00:18:08.830
She could go to college.

00:18:08.830 --> 00:18:12.770
And she could get her own job,
and she could live on her own.

00:18:12.770 --> 00:18:17.950
And that just was so
exciting to be able to do.

00:18:17.950 --> 00:18:20.250
So this is the
kind of way that we

00:18:20.250 --> 00:18:22.180
wanted to mix the
machine learning

00:18:22.180 --> 00:18:25.130
from the side of really
understanding her capabilities

00:18:25.130 --> 00:18:29.200
and trying to adapt, even
build devices to accommodate

00:18:29.200 --> 00:18:32.170
and then to make her give
the human experience.

00:18:35.949 --> 00:18:40.920
[APPLAUSE]

00:18:40.920 --> 00:18:41.850
Thanks.

00:18:41.850 --> 00:18:44.249
One of the things that
YokyWorks has taken, of course,

00:18:44.249 --> 00:18:45.790
is something that's
not just for her,

00:18:45.790 --> 00:18:48.480
but we wanted it to be
for a lot more people.

00:18:48.480 --> 00:18:50.240
Devices for disabled
people, of course,

00:18:50.240 --> 00:18:53.220
is not something that
you can mass-produce.

00:18:53.220 --> 00:18:54.970
It's sort of a good
thing and a bad thing.

00:18:54.970 --> 00:18:57.450
So, you know, we were hoping
that through this work,

00:18:57.450 --> 00:18:59.790
we could even find
hundreds, maybe just

00:18:59.790 --> 00:19:02.040
thousands of people who would
use the same technology.

00:19:02.040 --> 00:19:04.380
And then we could work
with them to get, you know,

00:19:04.380 --> 00:19:06.380
this enablement of these lives.

00:19:06.380 --> 00:19:10.080
So, just to close, I
wanted to sort of really

00:19:10.080 --> 00:19:12.610
highlight this idea of machine
learning and human learning

00:19:12.610 --> 00:19:13.720
intersection.

00:19:13.720 --> 00:19:16.300
And I'm really excited to
keep working this intersection

00:19:16.300 --> 00:19:19.240
in both consumer products,
as well as for individuals.

00:19:19.240 --> 00:19:20.000
Thank you.

00:19:20.000 --> 00:19:25.880
[APPLAUSE]

00:19:25.880 --> 00:19:29.650
So now, I'm happy to
introduce Gabriella Levine.

00:19:29.650 --> 00:19:32.370
She's on the rapid
eval team at Google X.

00:19:32.370 --> 00:19:34.610
And she travels
all over the world.

00:19:34.610 --> 00:19:36.302
And we'll hear all about that.

00:19:36.302 --> 00:19:42.700
[APPLAUSE]

00:19:42.700 --> 00:19:45.070
GABRIELLA LEVINE: Thank you.

00:19:45.070 --> 00:19:46.900
I'm Gabriella Levine.

00:19:46.900 --> 00:19:49.730
As introduced, I work
on the rapid eval team,

00:19:49.730 --> 00:19:54.410
part of Google X. What the rapid
eval team is tasked with doing

00:19:54.410 --> 00:19:57.680
is coming up with new
moon-shot projects.

00:19:57.680 --> 00:20:00.780
So this means crazy,
out-of-the-box technological

00:20:00.780 --> 00:20:04.510
solutions to projects that
potentially could solve some

00:20:04.510 --> 00:20:08.910
of the world's largest problems
that affect billions of people.

00:20:08.910 --> 00:20:13.060
What we value is to work
hands-on to build things,

00:20:13.060 --> 00:20:17.940
to dive right in, and to work
with tools that we can feel.

00:20:17.940 --> 00:20:20.050
But we embrace failure.

00:20:20.050 --> 00:20:23.097
And we like to fail fast.

00:20:23.097 --> 00:20:24.680
Today, I want to
tell you a little bit

00:20:24.680 --> 00:20:26.730
about the type of
work that I do,

00:20:26.730 --> 00:20:29.210
and what got me here
on the rapid eval team.

00:20:31.950 --> 00:20:34.320
I've always been really
passionate about working

00:20:34.320 --> 00:20:37.530
outside, in nature,
and dealing with things

00:20:37.530 --> 00:20:39.600
that were harsh conditions.

00:20:39.600 --> 00:20:41.500
So this led me to
spend a few seasons

00:20:41.500 --> 00:20:44.930
as a wildland firefighter,
based in Oregon.

00:20:44.930 --> 00:20:46.850
I was on a hot-shot
crew, which meant

00:20:46.850 --> 00:20:49.040
that we would travel
all over the US

00:20:49.040 --> 00:20:51.880
to America's largest fires.

00:20:51.880 --> 00:20:55.110
So we would be in the middle
of the forest, no access

00:20:55.110 --> 00:20:59.120
to roads, food,
far from anything.

00:20:59.120 --> 00:21:01.610
It was super-mentally
challenging and physically

00:21:01.610 --> 00:21:02.150
demanding.

00:21:02.150 --> 00:21:06.110
But it was one of the most
exciting jobs I ever had.

00:21:06.110 --> 00:21:09.290
But I was around
lots of big machines

00:21:09.290 --> 00:21:11.740
that tried to fight the fires.

00:21:11.740 --> 00:21:13.980
And often, it was efficient.

00:21:13.980 --> 00:21:18.060
But sometimes, it was
really not the best way

00:21:18.060 --> 00:21:20.720
to use these big machines
that cost a lot of money,

00:21:20.720 --> 00:21:22.760
took a lot of manpower.

00:21:22.760 --> 00:21:26.620
So sometimes, it was even just
to show nearby towns that there

00:21:26.620 --> 00:21:28.660
was a lot of airspace
happening, even

00:21:28.660 --> 00:21:31.470
when a fire was too
big to get near.

00:21:31.470 --> 00:21:33.320
It was often really
the ground crews

00:21:33.320 --> 00:21:35.040
that made a lot of difference.

00:21:35.040 --> 00:21:38.030
So it really got me thinking
about automated machines,

00:21:38.030 --> 00:21:39.960
and robots, and
especially those that

00:21:39.960 --> 00:21:42.790
dealt with the environment.

00:21:42.790 --> 00:21:45.560
From there, I really
just started prototyping,

00:21:45.560 --> 00:21:47.760
and wanted to build
my own robots that

00:21:47.760 --> 00:21:50.910
could go into the environment
and potentially solve

00:21:50.910 --> 00:21:53.740
some natural disaster issues.

00:21:53.740 --> 00:21:56.920
So I ended up building a
lot of bio-inspired robots,

00:21:56.920 --> 00:21:59.580
and a lot of snake
robot prototypes, which

00:21:59.580 --> 00:22:00.840
are called Sneel.

00:22:00.840 --> 00:22:02.770
That's a story for
a different day.

00:22:02.770 --> 00:22:06.460
But this led me to keep
building and learn as I went,

00:22:06.460 --> 00:22:08.900
and eventually
co-create a startup

00:22:08.900 --> 00:22:11.670
called Protei, dealing
with a robot that

00:22:11.670 --> 00:22:16.840
can go into the environment
and mitigate natural disasters.

00:22:16.840 --> 00:22:20.880
This startup led me to
another radical experiment.

00:22:20.880 --> 00:22:24.060
It was called
Unreasonable at Sea.

00:22:24.060 --> 00:22:27.400
I was one of ten tech
startups last year

00:22:27.400 --> 00:22:33.470
on a 4 and 1/2 month journey
around the world by ship.

00:22:33.470 --> 00:22:35.450
So this was my home.

00:22:35.450 --> 00:22:40.490
I was one of about 30 social
entrepreneurs on the ship.

00:22:40.490 --> 00:22:44.120
But it was a partnership
with Semester at Sea.

00:22:44.120 --> 00:22:47.730
So it was all of us part
of the accelerator program,

00:22:47.730 --> 00:22:52.910
with business mentors and
about 600 university students.

00:22:52.910 --> 00:22:53.820
It was weird.

00:22:53.820 --> 00:22:55.155
But it was really amazing.

00:22:58.250 --> 00:23:02.520
So we traveled together from
San Diego down to Mexico,

00:23:02.520 --> 00:23:06.440
across the Pacific Ocean,
around Asia, India, and then

00:23:06.440 --> 00:23:09.580
all around Africa,
up to Barcelona.

00:23:09.580 --> 00:23:13.240
Together, we were working with
each other-- the businesses

00:23:13.240 --> 00:23:15.870
and the students working
with the mentors,

00:23:15.870 --> 00:23:19.180
working with people at local
communities in the ports,

00:23:19.180 --> 00:23:21.530
just innovating
together, brainstorming.

00:23:21.530 --> 00:23:23.350
And we were
deploying our robots.

00:23:26.350 --> 00:23:29.760
So the startup that I
helped start and grow

00:23:29.760 --> 00:23:31.750
is called Protei.

00:23:31.750 --> 00:23:36.820
Protei is an open-source,
shape-shifting sailing robot,

00:23:36.820 --> 00:23:39.660
meant to explore and
preserve the ocean.

00:23:39.660 --> 00:23:43.740
So eventually, it's going to be
unmanned, fully sustainable, as

00:23:43.740 --> 00:23:45.460
well as autonomous.

00:23:45.460 --> 00:23:48.150
But it moves like a
snake through the water.

00:23:48.150 --> 00:23:49.640
And the purpose
is so that it can

00:23:49.640 --> 00:23:53.660
carry long and heavy material
behind it, like sensors,

00:23:53.660 --> 00:23:55.970
or payload for
cleaning up the ocean,

00:23:55.970 --> 00:24:00.860
like this oil-absorbent,
polypropylene tail.

00:24:00.860 --> 00:24:03.550
But the biomorphic
body actually moves

00:24:03.550 --> 00:24:06.930
like a snake, and curves
in the wind, presumably

00:24:06.930 --> 00:24:09.250
increasing efficiency
during the tack.

00:24:13.330 --> 00:24:16.180
So the idea is that,
with this flexible body,

00:24:16.180 --> 00:24:19.330
it can increase efficiency,
have lower drag,

00:24:19.330 --> 00:24:21.820
and decrease the
reduction of energy

00:24:21.820 --> 00:24:24.540
as it moves through the water.

00:24:24.540 --> 00:24:27.200
But Protei is a fully
open-source hardware project,

00:24:27.200 --> 00:24:29.490
which means that we
document the designs,

00:24:29.490 --> 00:24:31.300
really to enable
local communities

00:24:31.300 --> 00:24:34.970
to take a hands-on approach to
whatever environmental disaster

00:24:34.970 --> 00:24:38.750
they're trying to look at with
an autonomous surface vessel.

00:24:38.750 --> 00:24:41.280
So you could take out
the central electronics.

00:24:41.280 --> 00:24:44.230
And right now, it's
just a remote control

00:24:44.230 --> 00:24:46.210
talking to an
arduino on board that

00:24:46.210 --> 00:24:47.910
changes the shape
of the boat, as well

00:24:47.910 --> 00:24:49.400
as the trim of the sail.

00:24:49.400 --> 00:24:50.990
But you could put
a Linux computer

00:24:50.990 --> 00:24:52.890
on, or even an Android.

00:24:52.890 --> 00:24:54.840
You could put any sensor
you want on the boat,

00:24:54.840 --> 00:24:58.940
to measure any type of data
that you want on the water,

00:24:58.940 --> 00:25:01.110
or exchange the
radio for something

00:25:01.110 --> 00:25:05.140
like an iridium satellite,
in order to-- an antenna,

00:25:05.140 --> 00:25:08.910
to presumably have this vision
of multiple boats swarming

00:25:08.910 --> 00:25:10.600
around the world, anywhere.

00:25:10.600 --> 00:25:13.160
And then you could be at
home, controlling the boat

00:25:13.160 --> 00:25:14.680
from the comfort
of your own home,

00:25:14.680 --> 00:25:16.215
much like on a video
game interface.

00:25:19.770 --> 00:25:22.870
Protei's being developed to
solve some of the biggest

00:25:22.870 --> 00:25:27.200
natural disasters-- for
example, the oil spill.

00:25:27.200 --> 00:25:30.800
It was inspired by the
BP oil spill in 2010.

00:25:30.800 --> 00:25:34.180
And something like this might
enable all the fishermen

00:25:34.180 --> 00:25:38.140
who ended up helping and getting
skin diseases and lung diseases

00:25:38.140 --> 00:25:39.450
to be taken out of the picture.

00:25:39.450 --> 00:25:43.110
So robots could
end up doing that.

00:25:43.110 --> 00:25:45.760
It's also being developed
to map plastic trash

00:25:45.760 --> 00:25:49.020
in the middle of the ocean,
by having an optical camera

00:25:49.020 --> 00:25:51.540
on the front of the boat
that water streams through

00:25:51.540 --> 00:25:54.630
so it can measure the particles
and the microplastic size

00:25:54.630 --> 00:25:58.060
on the surface of the water.

00:25:58.060 --> 00:26:02.160
And it's being developed to
map radioactivity on the coast,

00:26:02.160 --> 00:26:04.870
as well as in the
water-- for example,

00:26:04.870 --> 00:26:08.290
near Fukushima, after
the Fukushima Daiichi

00:26:08.290 --> 00:26:10.630
nuclear meltdown.

00:26:10.630 --> 00:26:13.460
So wherever we went on
Unreasonable at Sea,

00:26:13.460 --> 00:26:15.020
we worked with
local communities,

00:26:15.020 --> 00:26:19.150
seeing what their needs might
be to have an autonomous boat.

00:26:19.150 --> 00:26:20.770
And we deployed the boat.

00:26:20.770 --> 00:26:22.650
And we really
tried to understand

00:26:22.650 --> 00:26:24.960
the needs and the
people, as well

00:26:24.960 --> 00:26:26.950
as how the boat would
work in different types

00:26:26.950 --> 00:26:29.760
of environments.

00:26:29.760 --> 00:26:32.590
When we got to Ghana, we
learned that oil had just

00:26:32.590 --> 00:26:35.250
been discovered eight years ago.

00:26:35.250 --> 00:26:38.130
Now there were oil rigs
all along the coast,

00:26:38.130 --> 00:26:40.190
as well as the
fishermen would go

00:26:40.190 --> 00:26:42.580
and fish right along the oil.

00:26:42.580 --> 00:26:45.130
We wanted to see,
how can Protei be

00:26:45.130 --> 00:26:47.110
used to understand
the relationship

00:26:47.110 --> 00:26:50.610
between the fishermen,
the fish, and the oil?

00:26:50.610 --> 00:26:52.620
Now, at night, the
fish would gather

00:26:52.620 --> 00:26:55.550
around the oil rigs, which
had bright lights, which

00:26:55.550 --> 00:26:57.930
was something that
never used to happen.

00:26:57.930 --> 00:27:00.260
Instead of just going and
interviewing and talking

00:27:00.260 --> 00:27:02.620
to some people, we
met some friends.

00:27:02.620 --> 00:27:06.350
We headed directly to the beach,
and got on a fishing boat.

00:27:06.350 --> 00:27:09.580
We rode out with them, and
ended up fishing with them.

00:27:09.580 --> 00:27:11.725
I was really sick this
day, and actually,

00:27:11.725 --> 00:27:13.850
throwing up over the side
of the boat as I paddled.

00:27:13.850 --> 00:27:15.016
But it really didn't matter.

00:27:15.016 --> 00:27:15.760
It was amazing.

00:27:15.760 --> 00:27:20.060
And I just kept paddling
as hard as I could.

00:27:20.060 --> 00:27:22.800
When we got to
Hawaii, we learned

00:27:22.800 --> 00:27:26.650
that every island has its
own rainbow beach, modeled

00:27:26.650 --> 00:27:30.879
with plastic trash that
comes in from the ocean.

00:27:30.879 --> 00:27:32.420
We worked with some
of the scientists

00:27:32.420 --> 00:27:35.640
there to see how Protei
might be used to map plastic

00:27:35.640 --> 00:27:38.780
trash in the middle
of the Pacific Ocean.

00:27:38.780 --> 00:27:40.970
Now, we're continuing
efforts with these scientists

00:27:40.970 --> 00:27:43.795
to see what Protei might be
able to do all over the world.

00:27:47.000 --> 00:27:50.360
Finally, Protei is
engaging with a project,

00:27:50.360 --> 00:27:55.390
working with the disaster that
happened in 2011, the Fukushima

00:27:55.390 --> 00:27:59.150
Daiichi nuclear meltdown
after the earthquake.

00:27:59.150 --> 00:28:02.370
This is one of the biggest
disasters of the century.

00:28:02.370 --> 00:28:06.390
And it's going to affect
people for centuries to come.

00:28:06.390 --> 00:28:09.330
We wanted to see if
Protei could help map

00:28:09.330 --> 00:28:12.830
the radioactivity off
the coast of Fukushima,

00:28:12.830 --> 00:28:15.200
as well as in the
water-- something

00:28:15.200 --> 00:28:19.430
that I never had seen
data about before.

00:28:19.430 --> 00:28:23.260
There's an organization called
Safecast, based in Tokyo.

00:28:23.260 --> 00:28:27.600
And they handed out open-source
Geiger counters in 2011.

00:28:27.600 --> 00:28:31.030
This allowed people to collect
data all over the country.

00:28:31.030 --> 00:28:34.510
And they crowdsource-mapped
the entire country,

00:28:34.510 --> 00:28:37.290
based on radioactivity levels.

00:28:37.290 --> 00:28:41.420
We wanted to see, could Protei
do this, but for the water?

00:28:41.420 --> 00:28:45.360
And all we needed was a boat to
put one of the Geiger counters

00:28:45.360 --> 00:28:46.890
on.

00:28:46.890 --> 00:28:50.290
So this gave us nine days to
build a boat between Hawaii

00:28:50.290 --> 00:28:54.920
and Japan, which was very
early on in the journey.

00:28:54.920 --> 00:28:57.310
So imagine building on a boat.

00:28:57.310 --> 00:28:58.510
It's not very easy.

00:28:58.510 --> 00:29:01.960
We were on the second-level
deck, way below the water line.

00:29:01.960 --> 00:29:05.460
And it was like trying to
solder on Space Mountain roller

00:29:05.460 --> 00:29:10.260
coaster, as it pendulums
back and forth.

00:29:10.260 --> 00:29:13.380
Plus, we ended up hitting
one of the worst storms

00:29:13.380 --> 00:29:16.720
that the seasoned British
captain said he's ever seen.

00:29:16.720 --> 00:29:18.730
At one point, he got
on the loudspeaker.

00:29:18.730 --> 00:29:21.960
And he said, "Everybody,
get to your rooms!

00:29:21.960 --> 00:29:26.270
Things are looking grim."

00:29:26.270 --> 00:29:28.370
So we didn't care.

00:29:28.370 --> 00:29:31.910
As I was, like, soldering
the PCBs for Geiger counters,

00:29:31.910 --> 00:29:34.520
the student interns
were epoxying

00:29:34.520 --> 00:29:36.060
the outside of the boats.

00:29:36.060 --> 00:29:39.580
And we kept working, even as
wood was flying everywhere

00:29:39.580 --> 00:29:42.550
as the boat shook, and
electronics, and epoxy.

00:29:42.550 --> 00:29:47.060
But we had to finish the boat
by the time we got to Japan.

00:29:47.060 --> 00:29:50.740
So after countless
nights of not sleeping,

00:29:50.740 --> 00:29:55.060
we finally were finishing as we
pulled into port that morning.

00:29:55.060 --> 00:29:57.660
And this was a huge relief.

00:29:57.660 --> 00:30:00.800
This was one of the most
beautiful sunrises I had ever

00:30:00.800 --> 00:30:05.330
seen, as the seas
finally calmed.

00:30:05.330 --> 00:30:08.790
So we head directly to
Safecast's headquarters.

00:30:08.790 --> 00:30:10.700
We modified the
Geiger counters so

00:30:10.700 --> 00:30:12.490
that we can put
them in the water,

00:30:12.490 --> 00:30:14.690
as well as get data
from under the water.

00:30:14.690 --> 00:30:18.150
And we head off in a car with
some of the Safecast team,

00:30:18.150 --> 00:30:20.705
drove through the night
to Fukushima from Tokyo.

00:30:23.630 --> 00:30:25.990
In the morning, we
finally arrived.

00:30:25.990 --> 00:30:30.100
And it was like a
post-apocalyptic scene.

00:30:30.100 --> 00:30:32.100
It was totally destroyed.

00:30:32.100 --> 00:30:34.630
And the landscape was desolate.

00:30:34.630 --> 00:30:36.720
It was a ghost town.

00:30:36.720 --> 00:30:38.680
But we drove as
close as we could

00:30:38.680 --> 00:30:42.630
to the last possible barrier
that had been opened up.

00:30:42.630 --> 00:30:45.770
We were five miles
from the nuclear plant.

00:30:45.770 --> 00:30:48.180
We put the boat in the
water with the sensors.

00:30:48.180 --> 00:30:50.480
And we didn't know
if we'd get any data.

00:30:50.480 --> 00:30:51.710
But it was really exciting.

00:30:51.710 --> 00:30:54.470
As we lowered the sensors
down to the mineral

00:30:54.470 --> 00:30:56.370
layer of the ocean
floor, we were actually

00:30:56.370 --> 00:31:00.250
able to see trace
amounts of radioactivity.

00:31:00.250 --> 00:31:01.670
So this was really exciting.

00:31:01.670 --> 00:31:04.970
And Protei's continued
collaboration with Safecast,

00:31:04.970 --> 00:31:07.840
and aims to go back
in October to continue

00:31:07.840 --> 00:31:10.264
to map radioactivity
there, and all

00:31:10.264 --> 00:31:11.430
over the world in the ocean.

00:31:14.350 --> 00:31:16.320
So sometimes, when
I've been faced

00:31:16.320 --> 00:31:21.960
with crazy problems, real-world
problems, natural disasters,

00:31:21.960 --> 00:31:24.600
I've been forced to
think of crazy solutions

00:31:24.600 --> 00:31:27.480
and try them out, right
there on the spot.

00:31:27.480 --> 00:31:30.020
Sometimes, they've
been total failures.

00:31:30.020 --> 00:31:33.150
But regardless, I've
learned a lot from it.

00:31:33.150 --> 00:31:35.870
So just by trying,
just by diving in,

00:31:35.870 --> 00:31:38.900
and not being afraid
to fail, that sometimes

00:31:38.900 --> 00:31:44.330
lead to crazy, wild ideas
that-- like weird snake boats--

00:31:44.330 --> 00:31:47.540
that might potentially solve
some of the world's biggest

00:31:47.540 --> 00:31:49.760
challenges.

00:31:49.760 --> 00:31:50.775
So thank you.

00:31:50.775 --> 00:31:58.700
[APPLAUSE AND CHEERING]

00:31:58.700 --> 00:32:01.280
Thank you.

00:32:01.280 --> 00:32:03.190
So now, I'm going to
introduce somebody

00:32:03.190 --> 00:32:07.460
else who works on Google X and
some crazy robots that deal

00:32:07.460 --> 00:32:12.473
with harsh environmental
terrain, this time on Mars.

00:32:12.473 --> 00:32:13.731
[APPLAUSE]

00:32:13.731 --> 00:32:14.397
JAIME WAYDO: Hi.

00:32:18.740 --> 00:32:19.770
My name is Jaime Waydo.

00:32:19.770 --> 00:32:25.290
I am a systems engineer
at Google X, where

00:32:25.290 --> 00:32:28.380
we work on self-driving cars.

00:32:28.380 --> 00:32:30.840
But I want to talk to
you today about not

00:32:30.840 --> 00:32:34.605
an Earth self-driving car, but
a self-driving car on Mars.

00:32:37.860 --> 00:32:41.010
Let's start with
me in eighth grade.

00:32:41.010 --> 00:32:44.640
And I can't believe I'm
showing you this picture.

00:32:44.640 --> 00:32:47.510
This is over-permed hair
that was very fashionable

00:32:47.510 --> 00:32:49.590
at the time, I promise you.

00:32:49.590 --> 00:32:52.060
I have decided in my
life, at this point,

00:32:52.060 --> 00:32:58.760
that I'm going to be a
professional musician.

00:32:58.760 --> 00:33:00.460
And then I go to
a science class.

00:33:00.460 --> 00:33:03.390
And my science teacher
is talking to us

00:33:03.390 --> 00:33:06.110
about Mars and space.

00:33:06.110 --> 00:33:09.310
And he's telling us
about the NASA missions,

00:33:09.310 --> 00:33:12.540
like Viking, that happened
in the '70s, where

00:33:12.540 --> 00:33:15.960
two Viking landers actually
landed on the surface of Mars,

00:33:15.960 --> 00:33:19.130
and collected data,
and maybe, maybe not,

00:33:19.130 --> 00:33:20.700
found evidence of life.

00:33:20.700 --> 00:33:22.470
And I'm like, gosh,
that's amazing.

00:33:22.470 --> 00:33:24.750
I'm doing that.

00:33:24.750 --> 00:33:27.540
Now, at the time, NASA
is not going to Mars.

00:33:27.540 --> 00:33:29.610
There are no Mars
missions on the books.

00:33:29.610 --> 00:33:32.240
We haven't been to
Mars since the '70s.

00:33:32.240 --> 00:33:34.160
I'm not that old.

00:33:34.160 --> 00:33:39.310
And so, we now need
a way to get there.

00:33:39.310 --> 00:33:40.410
But that's my dream.

00:33:40.410 --> 00:33:41.610
That's what I'm doing.

00:33:41.610 --> 00:33:43.090
By the time I get
to high school,

00:33:43.090 --> 00:33:46.480
I realize that to go to Mars
and to go to NASA, what I really

00:33:46.480 --> 00:33:50.130
need to do is work at NASA's Jet
Propulsion Laboratory, or JPL,

00:33:50.130 --> 00:33:51.540
as we call it.

00:33:51.540 --> 00:33:54.120
And I need to probably
be a mechanical engineer.

00:33:54.120 --> 00:33:56.160
So that's what I decide to do.

00:33:56.160 --> 00:33:58.390
I go to college in
mechanical engineering.

00:33:58.390 --> 00:34:01.990
And I end up waiting tables
at Perkins Family Restaurant

00:34:01.990 --> 00:34:06.250
and Bakery, which is
like a Denny's, maybe?

00:34:06.250 --> 00:34:08.050
And I am waiting tables.

00:34:08.050 --> 00:34:09.130
And a guy comes in.

00:34:09.130 --> 00:34:12.920
And it turns out he's
just retired from JPL.

00:34:12.920 --> 00:34:15.560
And he has just moved to
Bozeman, Montana, where

00:34:15.560 --> 00:34:17.429
I'm working and going to school.

00:34:17.429 --> 00:34:21.179
And he says, come over to my
house, and we'll talk about it.

00:34:21.179 --> 00:34:22.900
So of course, I go
to a stranger's house

00:34:22.900 --> 00:34:24.483
in the middle of
nowhere, because that

00:34:24.483 --> 00:34:25.550
seems like a good idea.

00:34:25.550 --> 00:34:29.760
And I get an interview
a week later.

00:34:29.760 --> 00:34:33.730
I have an internship
a week after that,

00:34:33.730 --> 00:34:34.820
which is pretty amazing.

00:34:34.820 --> 00:34:37.710
And so this is the early
years at JPL for me.

00:34:37.710 --> 00:34:39.730
The early years
were really focused

00:34:39.730 --> 00:34:42.600
on testing and proving
out the landing systems

00:34:42.600 --> 00:34:45.060
to get two twin
rovers, Curiosity--

00:34:45.060 --> 00:34:47.170
or, sorry, Spirit
and Opportunity,

00:34:47.170 --> 00:34:48.750
safely to the surface of Mars.

00:34:48.750 --> 00:34:52.219
And I ran a huge test program.

00:34:52.219 --> 00:34:54.050
And these are the guys
that worked for me.

00:34:54.050 --> 00:34:57.760
You can see me-- at the time,
I had red hair-- in the back.

00:34:57.760 --> 00:34:59.530
And then, all of
the guys helping me

00:34:59.530 --> 00:35:02.060
pull off all of those tests.

00:35:02.060 --> 00:35:04.960
And by the early
2000s, I had been

00:35:04.960 --> 00:35:09.170
assigned to work
on my first child.

00:35:09.170 --> 00:35:09.875
And this is her.

00:35:09.875 --> 00:35:12.210
Her name is Curiosity.

00:35:12.210 --> 00:35:15.170
She was slated to
go to Mars in 2007,

00:35:15.170 --> 00:35:17.690
turns out we launched in 2009.

00:35:17.690 --> 00:35:20.694
But I'm in charge of
all things mobility.

00:35:20.694 --> 00:35:21.860
And so, what does that mean?

00:35:21.860 --> 00:35:23.670
That means-- yeah!

00:35:23.670 --> 00:35:25.790
[APPLAUSE]

00:35:25.790 --> 00:35:29.020
Differential, the suspension,
the wheels-- everything

00:35:29.020 --> 00:35:30.830
on your car that
makes your car go.

00:35:30.830 --> 00:35:33.540
The same things are on a
Rover that make a Rover go.

00:35:33.540 --> 00:35:36.530
They're also the same things
on a self-driving car.

00:35:36.530 --> 00:35:38.470
And so it all kind of
ties together, right?

00:35:38.470 --> 00:35:40.160
I'm working on this.

00:35:40.160 --> 00:35:43.800
One of the things we
worked on is the wheel.

00:35:43.800 --> 00:35:46.490
And that was my
team, me and a bunch

00:35:46.490 --> 00:35:48.260
of guys who worked for me.

00:35:48.260 --> 00:35:51.030
And so, the wheel
is really special.

00:35:51.030 --> 00:35:55.600
When you go to Mars, it gets
down to 135 below centigrade.

00:35:55.600 --> 00:35:57.830
And it gets up to 70 centigrade.

00:35:57.830 --> 00:36:00.420
And so you can't go with
traditional materials.

00:36:00.420 --> 00:36:03.950
So the wheel's actually
made out of an aluminum rim.

00:36:03.950 --> 00:36:07.750
And that wall thickness of that
skin of that tire is 30,000ths

00:36:07.750 --> 00:36:08.750
thick.

00:36:08.750 --> 00:36:10.180
To put that in
perspective, that's

00:36:10.180 --> 00:36:12.290
the thickness of
three sheets of paper.

00:36:12.290 --> 00:36:17.220
And that's what makes
that wheel round and hold.

00:36:17.220 --> 00:36:19.810
People continually ask me,
why did you make so thin?

00:36:19.810 --> 00:36:22.430
Especially right now, with the
story I'm about to tell you.

00:36:22.430 --> 00:36:26.250
And the reason is, there are
six wheels on the vehicle.

00:36:26.250 --> 00:36:29.880
And so, every ounce of
material you put on there

00:36:29.880 --> 00:36:33.430
is multiplied by six.

00:36:33.430 --> 00:36:36.180
And so we're constantly--
one of our key constraints

00:36:36.180 --> 00:36:38.010
is that we have to
make it lightweight.

00:36:38.010 --> 00:36:40.680
Another one is that we have to
be able to climb large rocks.

00:36:40.680 --> 00:36:44.300
This is a mobile platform that
wants to climb up a mountain.

00:36:44.300 --> 00:36:45.920
And another thing
is that we have

00:36:45.920 --> 00:36:49.210
to really cushion the
way the vehicle drives.

00:36:49.210 --> 00:36:51.620
So you can see the
spoked wheels in here.

00:36:51.620 --> 00:36:55.770
It's six titanium flexures, kind
of like a spoked bicycle wheel,

00:36:55.770 --> 00:36:57.560
but now in three dimensions.

00:36:57.560 --> 00:36:59.950
And that's the
design we propose.

00:36:59.950 --> 00:37:01.280
And we start testing it.

00:37:01.280 --> 00:37:04.330
Now, you may know,
Curiosity is pretty special.

00:37:04.330 --> 00:37:06.900
It's a brand-new
landing system for NASA.

00:37:06.900 --> 00:37:08.690
So we actually
have a jetpack that

00:37:08.690 --> 00:37:10.850
flies her close to
the surface of Mars.

00:37:10.850 --> 00:37:13.850
And then she drops down
from the jetpack, and lands

00:37:13.850 --> 00:37:16.460
on her wheels--
which sounds so easy.

00:37:16.460 --> 00:37:18.750
And in all of the videos
that they show about it,

00:37:18.750 --> 00:37:19.940
it's super benign.

00:37:19.940 --> 00:37:22.170
But imagine, you don't get
to pick your landing spot.

00:37:22.170 --> 00:37:24.650
So there can be slopes,
and there can be rocks.

00:37:24.650 --> 00:37:26.320
So we actually test that.

00:37:26.320 --> 00:37:28.560
And we started out with some
of our early prototypes.

00:37:28.560 --> 00:37:29.830
And this is high-speed video.

00:37:29.830 --> 00:37:32.620
I promise we're not
moving that slow.

00:37:32.620 --> 00:37:34.070
And so that wheel
is coming down,

00:37:34.070 --> 00:37:36.070
and we're testing
to see what happens

00:37:36.070 --> 00:37:37.790
when you land on major rocks.

00:37:37.790 --> 00:37:41.080
We want to make sure we're
designing a wheel that works.

00:37:41.080 --> 00:37:42.990
And so the wheel's
going to come down.

00:37:42.990 --> 00:37:44.680
And it's going to hit this rock.

00:37:44.680 --> 00:37:46.330
And you'll watch
the rock puncture

00:37:46.330 --> 00:37:49.700
through the skin of the
tire, and totally deform it.

00:37:49.700 --> 00:37:52.320
But then, it almost
all bounces back.

00:37:52.320 --> 00:37:55.790
And all you've got are these
little cracks in the wheel.

00:37:55.790 --> 00:37:57.000
And then we tested that.

00:37:57.000 --> 00:37:59.320
And we drove it over a
whole bunch of terrain

00:37:59.320 --> 00:38:00.980
to make sure that
those cracks don't

00:38:00.980 --> 00:38:03.440
do what we call
propagate, or spread out.

00:38:03.440 --> 00:38:05.770
And we made sure
it was good to go.

00:38:05.770 --> 00:38:08.120
And so we tested all of that.

00:38:08.120 --> 00:38:12.180
We also took her out into what
we call the Mars Yard at JPL,

00:38:12.180 --> 00:38:14.880
where we simulate
the Martian terrain.

00:38:14.880 --> 00:38:17.090
And if you know a thing
or two about mobility

00:38:17.090 --> 00:38:19.120
systems for Mars, you
know that you really

00:38:19.120 --> 00:38:21.990
want to design a
mobility system that

00:38:21.990 --> 00:38:24.590
can climb one wheel diameter.

00:38:24.590 --> 00:38:27.090
But for me and my team,
that wasn't sufficient.

00:38:27.090 --> 00:38:28.370
So we got extra credit.

00:38:28.370 --> 00:38:30.740
And we can climb
two wheel diameters.

00:38:30.740 --> 00:38:33.600
We don't ever let her do
that on Mars, I promise.

00:38:33.600 --> 00:38:35.530
But she can do it.

00:38:35.530 --> 00:38:37.290
This is her early
prototype, just

00:38:37.290 --> 00:38:41.290
so you get a little
bit of a story here.

00:38:41.290 --> 00:38:43.410
Her name is Scarecrow.

00:38:43.410 --> 00:38:45.240
And we call her
Scarecrow because she

00:38:45.240 --> 00:38:48.530
doesn't have a brain.

00:38:48.530 --> 00:38:51.800
It's just a little button
box we drive her with.

00:38:51.800 --> 00:38:54.460
We also tested-- so
those were tall rocks.

00:38:54.460 --> 00:38:56.480
Now we're testing wide rocks.

00:38:56.480 --> 00:38:59.130
And one of the things we found
when we drove over wide rocks

00:38:59.130 --> 00:39:02.720
is this phenomena where
the vehicle actually gets

00:39:02.720 --> 00:39:04.750
one wheel on one
side of the rock, one

00:39:04.750 --> 00:39:06.390
wheel on the other
side of the rock,

00:39:06.390 --> 00:39:09.620
and then starts lifting those
rocks up off of the ground.

00:39:09.620 --> 00:39:11.860
And those really do
damage to the wheels.

00:39:11.860 --> 00:39:13.390
And so we thought, well, gosh.

00:39:13.390 --> 00:39:14.390
What are we going to do?

00:39:14.390 --> 00:39:15.640
We're getting close to launch.

00:39:15.640 --> 00:39:17.400
This is a bad idea.

00:39:17.400 --> 00:39:19.690
We need to fix this.

00:39:19.690 --> 00:39:20.970
And so I said, I know.

00:39:20.970 --> 00:39:24.730
We'll just tell the drivers,
don't drive over wide rocks.

00:39:24.730 --> 00:39:26.330
So that's what we did.

00:39:26.330 --> 00:39:28.400
And they don't drive
over wide rocks.

00:39:28.400 --> 00:39:31.830
They're doing a great job.

00:39:31.830 --> 00:39:34.320
We also test her
inside the clean room.

00:39:34.320 --> 00:39:36.990
And so this is me
and my friend Peter.

00:39:36.990 --> 00:39:38.900
And we're making
sure that, when we

00:39:38.900 --> 00:39:41.950
tell the wheels to go forward,
the rover goes forward.

00:39:41.950 --> 00:39:44.660
And when we tell her to go
backwards, she goes backwards.

00:39:44.660 --> 00:39:47.370
And when we tell her to steer,
she steers the right way.

00:39:47.370 --> 00:39:50.230
So a very basic test to
really build the confidence

00:39:50.230 --> 00:39:52.080
that we built the right vehicle.

00:39:52.080 --> 00:39:55.480
And then the extreme tests we
do on Scarecrow, to make sure

00:39:55.480 --> 00:39:58.500
that we've done
everything right.

00:39:58.500 --> 00:39:59.470
Then we launch her.

00:39:59.470 --> 00:40:01.510
This is out of Cape Canaveral.

00:40:01.510 --> 00:40:02.810
And we say goodbye.

00:40:02.810 --> 00:40:05.760
And, gosh, this is really
like postpartum depression

00:40:05.760 --> 00:40:07.300
for a rover momma.

00:40:07.300 --> 00:40:09.220
And so our baby's gone.

00:40:09.220 --> 00:40:10.650
Yeah.

00:40:10.650 --> 00:40:12.120
Which is an exciting day.

00:40:12.120 --> 00:40:13.910
It's like you've sent
her off to college.

00:40:13.910 --> 00:40:17.630
But she's never coming back.

00:40:17.630 --> 00:40:19.180
She gets to Mars.

00:40:19.180 --> 00:40:21.660
She takes a selfie, says hi.

00:40:21.660 --> 00:40:22.574
I'm here.

00:40:22.574 --> 00:40:23.240
She really does.

00:40:23.240 --> 00:40:25.360
That's her taking that.

00:40:25.360 --> 00:40:27.560
We don't fake that, I promise.

00:40:27.560 --> 00:40:29.660
And so everything
looks good, right?

00:40:29.660 --> 00:40:31.210
And I go on my merry way.

00:40:31.210 --> 00:40:35.270
I go off to Google X. And all
of a sudden, the phone rings.

00:40:35.270 --> 00:40:38.520
And they say, Jaime, have you
seen what we're driving on?

00:40:38.520 --> 00:40:39.720
And I look.

00:40:39.720 --> 00:40:42.769
And this is the picture I have.

00:40:42.769 --> 00:40:45.060
And the rocks are like nothing
we've ever seen on Mars.

00:40:45.060 --> 00:40:47.940
We've landed on Mars
at NASA quite a bit.

00:40:47.940 --> 00:40:51.620
And we're, in fact, fairly
good at landing on Mars.

00:40:51.620 --> 00:40:53.650
We have never seen rocks
that look like this.

00:40:53.650 --> 00:40:56.230
They're basically
knife-edged pyramids.

00:40:56.230 --> 00:40:58.880
And we're driving
over a lot of them.

00:40:58.880 --> 00:41:02.620
And I'm like, well,
how bad is it?

00:41:02.620 --> 00:41:05.930
This is the next
picture they sent me.

00:41:05.930 --> 00:41:08.980
And these are the
wheels-- sol 564.

00:41:08.980 --> 00:41:11.730
So a Martian day
is called a sol.

00:41:11.730 --> 00:41:14.900
It's 45 minutes longer
than an Earth day.

00:41:14.900 --> 00:41:19.240
And so we're about a year, a
little more, into the mission.

00:41:19.240 --> 00:41:22.430
We want to go for two years.

00:41:22.430 --> 00:41:23.900
And we've got holes all over.

00:41:23.900 --> 00:41:26.802
And they're pretty
significant, right.

00:41:26.802 --> 00:41:28.260
That's not really
what we designed.

00:41:28.260 --> 00:41:29.900
We did design holes
in the wheels.

00:41:29.900 --> 00:41:32.260
Does anybody know
what the holes say?

00:41:32.260 --> 00:41:33.230
Yeah?

00:41:33.230 --> 00:41:35.630
JPL in Morse code, that's right.

00:41:35.630 --> 00:41:39.510
Leaving our fingerprints all
over the surface of Mars.

00:41:39.510 --> 00:41:42.952
See, engineers have fun, right?

00:41:42.952 --> 00:41:44.530
So, yeah.

00:41:44.530 --> 00:41:47.860
We basically have the equivalent
of a Martian flat tire.

00:41:47.860 --> 00:41:51.910
And I realize that I'm in the
situation where there's not

00:41:51.910 --> 00:41:53.070
going to be a right answer.

00:41:53.070 --> 00:41:54.894
We are completely
over-constrained.

00:41:54.894 --> 00:41:56.810
And so I say, well, the
first thing I would do

00:41:56.810 --> 00:41:58.040
is I would drive backwards.

00:41:58.040 --> 00:41:59.680
And they're like, what?

00:41:59.680 --> 00:42:03.250
And I said, well, what we
did is we designed Curiosity

00:42:03.250 --> 00:42:06.560
so that whatever trouble
she gets into going forward,

00:42:06.560 --> 00:42:09.320
she's actually better
at driving in reverse.

00:42:09.320 --> 00:42:10.980
So turn her around.

00:42:10.980 --> 00:42:11.840
She's a robot.

00:42:11.840 --> 00:42:13.110
She doesn't care.

00:42:13.110 --> 00:42:14.570
Turn the camera around.

00:42:14.570 --> 00:42:15.790
And you're good to go.

00:42:15.790 --> 00:42:16.860
Just drive backwards.

00:42:16.860 --> 00:42:19.540
You're at least doing
damage to different wheels.

00:42:19.540 --> 00:42:21.256
And so they do that.

00:42:21.256 --> 00:42:22.880
They don't really
like it, because they

00:42:22.880 --> 00:42:25.100
don't have as good a
view of the cameras.

00:42:25.100 --> 00:42:26.950
But she's doing great.

00:42:26.950 --> 00:42:30.910
Then I tell them, can we
not drive on those rocks?

00:42:30.910 --> 00:42:34.910
So we start driving
on sandier soils.

00:42:34.910 --> 00:42:37.930
But sanding soil
isn't fun, either.

00:42:37.930 --> 00:42:40.560
Curiosity is like a
little kid in a sandbox.

00:42:40.560 --> 00:42:41.430
She gets in.

00:42:41.430 --> 00:42:43.000
And she can start
throwing up sand

00:42:43.000 --> 00:42:44.810
all over herself that
gets on the science

00:42:44.810 --> 00:42:46.480
instruments and the cameras.

00:42:46.480 --> 00:42:49.100
And she can actually
bury herself.

00:42:49.100 --> 00:42:52.880
The reason we actually put
JPL in the wheels is-- well,

00:42:52.880 --> 00:42:55.190
we wanted to put our
fingerprints on Mars.

00:42:55.190 --> 00:42:58.870
But it's also because we
know how big the wheel is.

00:42:58.870 --> 00:43:02.640
And we know how far apart
those asymmetric tread patterns

00:43:02.640 --> 00:43:04.090
should be in the surface.

00:43:04.090 --> 00:43:06.880
And so we can measure the
distance in our cameras.

00:43:06.880 --> 00:43:08.500
We know how far
apart they should be.

00:43:08.500 --> 00:43:10.550
We know how far apart they are.

00:43:10.550 --> 00:43:12.760
And that difference is
how much she's slipping,

00:43:12.760 --> 00:43:15.420
so that we make sure she's
not burying herself in,

00:43:15.420 --> 00:43:18.220
like Spirit did.

00:43:18.220 --> 00:43:18.990
So we do that.

00:43:18.990 --> 00:43:20.940
But it's slow going.

00:43:20.940 --> 00:43:24.020
And you can see here,
it's a lot of slipping,

00:43:24.020 --> 00:43:26.870
and a lot of dust being
thrown up everywhere.

00:43:26.870 --> 00:43:30.040
So again, the scientists
aren't super-happy

00:43:30.040 --> 00:43:31.040
that we have to do this.

00:43:31.040 --> 00:43:33.590
But it's better than
tearing up those wheels.

00:43:33.590 --> 00:43:37.440
Another thing we do is we
take a lot more pictures.

00:43:37.440 --> 00:43:39.570
She's not taking
selfies up here anymore.

00:43:39.570 --> 00:43:41.860
She's taking selfies
of her shoes,

00:43:41.860 --> 00:43:44.070
and making sure that we're
tracking the damage that's

00:43:44.070 --> 00:43:46.230
happening to those wheels.

00:43:46.230 --> 00:43:47.460
We have two twins now.

00:43:47.460 --> 00:43:48.460
You see Scarecrow.

00:43:48.460 --> 00:43:51.990
There's another twin, I'm not
really sure what her name is.

00:43:51.990 --> 00:43:54.820
It was VSTB, for
Vehicle System Test Bed.

00:43:54.820 --> 00:43:57.980
We're not always fun,
when we're engineers.

00:43:57.980 --> 00:43:59.900
But this is the Mars Yard again.

00:43:59.900 --> 00:44:03.830
And what we do now is we've got
orbiters that fly over Mars.

00:44:03.830 --> 00:44:05.660
They take
high-resolution pictures

00:44:05.660 --> 00:44:07.120
of the surface of Mars.

00:44:07.120 --> 00:44:09.480
And then, from
that, we mock up--

00:44:09.480 --> 00:44:14.034
with interns carrying rocks
all around-- what that terrain

00:44:14.034 --> 00:44:14.950
is going to look like.

00:44:14.950 --> 00:44:16.110
We test it on Earth.

00:44:16.110 --> 00:44:17.480
And we make sure it's OK.

00:44:17.480 --> 00:44:19.540
And then we can go on Mars.

00:44:19.540 --> 00:44:21.610
You can see that's a
huge amount of work,

00:44:21.610 --> 00:44:24.580
a ton of logistics, and
very time consuming.

00:44:24.580 --> 00:44:26.690
But we do it to
make sure that she's

00:44:26.690 --> 00:44:28.220
doing what she's supposed to.

00:44:28.220 --> 00:44:32.740
Another thing we do, we
just don't drive as much.

00:44:32.740 --> 00:44:33.610
So we sit here.

00:44:33.610 --> 00:44:36.100
This is a place called the
Kimberly, because apparently it

00:44:36.100 --> 00:44:39.130
looks like the
Kimberly in Australia.

00:44:39.130 --> 00:44:41.780
And she's sitting there
right now, actually,

00:44:41.780 --> 00:44:44.830
drilling and doing a lot
of really great science.

00:44:44.830 --> 00:44:46.590
But it's a compromise.

00:44:46.590 --> 00:44:47.980
Everything is a compromise.

00:44:47.980 --> 00:44:51.810
The scientists want to be
at the top of that mountain.

00:44:51.810 --> 00:44:54.540
And we're just probably
not going to get there.

00:44:54.540 --> 00:44:56.320
And that's really
unfortunate, when

00:44:56.320 --> 00:44:58.820
you're a mobility engineer,
and you've designed this thing

00:44:58.820 --> 00:45:02.450
to give the scientists all
of their dreams come true.

00:45:02.450 --> 00:45:04.200
But what I've
realized through this

00:45:04.200 --> 00:45:09.500
is that the thing that I'm most
passionate about in engineering

00:45:09.500 --> 00:45:12.780
is being on those
problems-- the problems that

00:45:12.780 --> 00:45:14.460
are completely over constrained.

00:45:14.460 --> 00:45:16.500
The ones where there's
not a right answer,

00:45:16.500 --> 00:45:18.920
because everything
is just wrong.

00:45:18.920 --> 00:45:21.470
And you get to be in
the center of that.

00:45:21.470 --> 00:45:25.230
And you get to figure out how
to solve those compromises.

00:45:25.230 --> 00:45:27.630
Which constraints are wrong?

00:45:27.630 --> 00:45:28.550
Push back.

00:45:28.550 --> 00:45:31.820
Which ones are really important,
and you have to listen to?

00:45:31.820 --> 00:45:35.930
And how do you tune that answer
to really optimize across

00:45:35.930 --> 00:45:37.360
no good answer?

00:45:37.360 --> 00:45:40.390
And that's the part that
I just totally love,

00:45:40.390 --> 00:45:42.430
because you get to be creative.

00:45:42.430 --> 00:45:47.210
And you get to really put your
fingerprints on the design.

00:45:47.210 --> 00:45:50.390
And so this is us, leaving our
fingerprints on the surface.

00:45:50.390 --> 00:45:52.790
And I encourage you, when
you're in those moments,

00:45:52.790 --> 00:45:54.910
to realize that
that answer is going

00:45:54.910 --> 00:45:57.890
to be uniquely yours, because
it's you, and no one else,

00:45:57.890 --> 00:45:59.540
solving it.

00:45:59.540 --> 00:46:00.447
Thank you.

00:46:00.447 --> 00:46:06.790
[APPLAUSE AND CHEERING]

00:46:06.790 --> 00:46:10.870
I get to-- I have to
tell you a little story.

00:46:10.870 --> 00:46:12.865
When I was getting
ready to go to Google X,

00:46:12.865 --> 00:46:15.900
I Googled Google X to try
to figure out what it was,

00:46:15.900 --> 00:46:17.280
because it was
pretty top secret.

00:46:17.280 --> 00:46:21.020
And I came across a video with
our next speaker, Megan Smith.

00:46:21.020 --> 00:46:22.570
And I was blown away.

00:46:22.570 --> 00:46:25.040
And I told my husband, I'm
like, I cannot wait to meet this

00:46:25.040 --> 00:46:25.540
woman.

00:46:25.540 --> 00:46:27.400
And every day I come home
from work at Google X,

00:46:27.400 --> 00:46:29.250
and he's like, have you
met Megan Smith yet?

00:46:29.250 --> 00:46:30.410
I'm like, no.

00:46:30.410 --> 00:46:33.490
And he calls her my work crush.

00:46:33.490 --> 00:46:36.720
I haven't told her
that until right now.

00:46:36.720 --> 00:46:39.020
And so I am very excited.

00:46:39.020 --> 00:46:41.040
It is on my bucket
list of things to do.

00:46:41.040 --> 00:46:43.850
I get to introduce
the Megan Smith.

00:46:43.850 --> 00:46:49.730
[APPLAUSE AND CHEERING]

00:46:49.730 --> 00:46:51.450
MEGAN SMITH: Hello.

00:46:51.450 --> 00:46:53.730
That was incredible.

00:46:53.730 --> 00:46:58.454
It's clear to me that hardware
is definitely the new black.

00:46:58.454 --> 00:47:01.450
You know, we wanted to
focus on robotics, because

00:47:01.450 --> 00:47:02.970
for this Women
Techmaker session,

00:47:02.970 --> 00:47:06.750
the second one that we've had
in our new tradition at I/O.

00:47:06.750 --> 00:47:08.910
Because there's just
extraordinary things happening

00:47:08.910 --> 00:47:09.760
in robotics right now.

00:47:09.760 --> 00:47:11.218
It's really entering
the mainstream

00:47:11.218 --> 00:47:14.140
in so many ways, and so
many parts of our lives.

00:47:14.140 --> 00:47:16.050
And there's really
extraordinary women

00:47:16.050 --> 00:47:18.300
who are part of those teams.

00:47:18.300 --> 00:47:19.890
And so we wanted to
bring it, like we

00:47:19.890 --> 00:47:21.370
do with Women Techmakers.

00:47:21.370 --> 00:47:25.430
We want to talk about and
show you extraordinary women.

00:47:25.430 --> 00:47:27.130
Also, the other
thing about robotics

00:47:27.130 --> 00:47:31.150
is that, for whatever reason,
because of media bias,

00:47:31.150 --> 00:47:32.880
are the things, when
we think of robotics

00:47:32.880 --> 00:47:35.610
and who is making them, we
do think about boys and men.

00:47:35.610 --> 00:47:38.830
And so, that kind of brings
me to Women Techmakers.

00:47:38.830 --> 00:47:40.810
And I just wanted
to say a little bit

00:47:40.810 --> 00:47:46.880
about Women Techmakers and this
idea of debugging inclusion.

00:47:46.880 --> 00:47:50.160
So this is a photo from 2012.

00:47:50.160 --> 00:47:52.110
And what we thought
of at that time--

00:47:52.110 --> 00:47:54.490
Google I/O was only
in the single digits

00:47:54.490 --> 00:47:55.970
of percentages of women.

00:47:55.970 --> 00:47:59.720
I think there were about 300
women in Google I/O of 2012.

00:47:59.720 --> 00:48:01.590
And so we decided that
the night before, we

00:48:01.590 --> 00:48:03.007
would have a
gathering, so people

00:48:03.007 --> 00:48:04.590
would have some
friends in the hallway

00:48:04.590 --> 00:48:06.720
and not feel like
such a minority.

00:48:06.720 --> 00:48:09.320
And so this is an
image from then.

00:48:09.320 --> 00:48:12.720
And I just wanted to show
you an image from, whatever,

00:48:12.720 --> 00:48:16.290
two nights ago, or last night--
the one of the many dinners.

00:48:16.290 --> 00:48:19.370
We have more than 1,000
women here at I/O.

00:48:19.370 --> 00:48:23.650
And we had 800 women out
to dinner in a gathering.

00:48:23.650 --> 00:48:24.930
Very exciting.

00:48:24.930 --> 00:48:28.275
The other thing that happened
in 2012 was, you know,

00:48:28.275 --> 00:48:32.710
one of things that's true about
diversity at work, none of us

00:48:32.710 --> 00:48:36.370
created any of the biases
that are happening to us.

00:48:36.370 --> 00:48:38.820
We inherited the
world that we live in.

00:48:38.820 --> 00:48:42.960
And so it's not that we
should feel guilty about them.

00:48:42.960 --> 00:48:45.810
What we should do is, as
we wake up and see them,

00:48:45.810 --> 00:48:48.462
you know, many of them
are very unconscious.

00:48:48.462 --> 00:48:50.920
As we wake up and see them, we
should do things about them.

00:48:50.920 --> 00:48:53.350
And so one of things
we thought in 2012,

00:48:53.350 --> 00:48:56.460
since we really noticed
just how invisible

00:48:56.460 --> 00:48:58.750
the women, the
technical women, are.

00:48:58.750 --> 00:49:01.180
And so, one of the
things we've started

00:49:01.180 --> 00:49:02.695
to do with Women
Techmakers is we

00:49:02.695 --> 00:49:04.200
began to make a video series.

00:49:04.200 --> 00:49:07.120
And so we had
videos from Mexico,

00:49:07.120 --> 00:49:09.235
from London, from
Silicon Valley,

00:49:09.235 --> 00:49:11.610
from New York, all around the
world, of just interviewing

00:49:11.610 --> 00:49:14.560
technical women and get
them in front of you.

00:49:14.560 --> 00:49:16.960
We also started to do things
around Google I/O again.

00:49:16.960 --> 00:49:19.546
So for 2013, we really
worked hard to make

00:49:19.546 --> 00:49:21.670
sure we put extraordinary
women onto the main stage

00:49:21.670 --> 00:49:22.340
at the key note.

00:49:22.340 --> 00:49:23.830
This is Johanna.

00:49:23.830 --> 00:49:26.000
And then we had our first
Women Techmakers session,

00:49:26.000 --> 00:49:28.710
which was called "Seven
Techmakers and a Microphone."

00:49:28.710 --> 00:49:31.960
And one of my favorite talks
at that was Kathy Kleiman.

00:49:31.960 --> 00:49:33.420
I don't know if
people were there.

00:49:33.420 --> 00:49:36.240
But she talked about a
70-year-old lost history

00:49:36.240 --> 00:49:39.650
story of the first
programmers in America.

00:49:39.650 --> 00:49:42.020
Six women, who
during World War II,

00:49:42.020 --> 00:49:43.410
were Rosie the Mathematician.

00:49:43.410 --> 00:49:47.290
And they were handed the
wiring diagrams of the ENIAC.

00:49:47.290 --> 00:49:50.100
And they had been calculating
ballistic trajectories, which

00:49:50.100 --> 00:49:54.874
was a differential equation
that took a good, you know,

00:49:54.874 --> 00:49:55.540
week long to do.

00:49:55.540 --> 00:49:56.490
And they said,
could you do this,

00:49:56.490 --> 00:49:57.702
because it'd be a great demo.

00:49:57.702 --> 00:49:59.410
And they went in and
wrote the first sort

00:49:59.410 --> 00:50:00.860
routines and made those happen.

00:50:00.860 --> 00:50:03.555
That movie has now been
made that Kathy introduced

00:50:03.555 --> 00:50:04.680
that was going to be there.

00:50:04.680 --> 00:50:07.080
And it's going to-- it's already
on the film festival tour.

00:50:07.080 --> 00:50:09.010
And we're going to start
to do some film festivals.

00:50:09.010 --> 00:50:10.890
We're doing one at
Hopper, with the Maker

00:50:10.890 --> 00:50:13.600
series, which is great.

00:50:13.600 --> 00:50:15.980
The next thing that
happened was Natalie joined.

00:50:15.980 --> 00:50:18.000
And she started to really
drive us to do things.

00:50:18.000 --> 00:50:22.824
And this is an idea that we had
for International Women's Day.

00:50:22.824 --> 00:50:25.240
During that month of March,
everybody's celebrating women.

00:50:25.240 --> 00:50:27.698
And we thought we could engage
the Google Developer Groups.

00:50:27.698 --> 00:50:30.650
And many of the leaders are
here in the room, so thank you.

00:50:30.650 --> 00:50:32.470
So this is the team in Nigeria.

00:50:32.470 --> 00:50:34.940
And, in fact, this is
the map that shows you.

00:50:34.940 --> 00:50:39.280
Over 11,000 technical
women met around the world,

00:50:39.280 --> 00:50:40.660
during the month of March.

00:50:40.660 --> 00:50:43.020
Met each other, began
to build community,

00:50:43.020 --> 00:50:46.500
began to know about resources,
and began to drive visibility.

00:50:46.500 --> 00:50:49.770
And so we had, I think,
52 different countries,

00:50:49.770 --> 00:50:51.550
and over 125 events.

00:50:51.550 --> 00:50:55.100
So thank you to the GDPs and
folks who made this happen.

00:50:55.100 --> 00:50:57.060
It's so important.

00:50:57.060 --> 00:51:00.860
So this is what we saw.

00:51:00.860 --> 00:51:03.230
And now we're also starting
to expand our resources.

00:51:03.230 --> 00:51:05.780
And one of things that
happened this year was many

00:51:05.780 --> 00:51:07.960
of these partners on
screen-- Girl Develop It,

00:51:07.960 --> 00:51:10.880
Women Who Code, and others,
Hackbright Academy--

00:51:10.880 --> 00:51:13.249
helped us find a lot of
the women that are here,

00:51:13.249 --> 00:51:15.040
the women who we should
have been inviting.

00:51:15.040 --> 00:51:17.056
And so we worked
with many of them.

00:51:17.056 --> 00:51:18.430
And then I also
wanted to let you

00:51:18.430 --> 00:51:20.256
know that we've partnered
with Code School.

00:51:20.256 --> 00:51:21.630
So the Women
Techmakers are going

00:51:21.630 --> 00:51:25.260
to be offering three
months of free access

00:51:25.260 --> 00:51:28.360
for women who want to
come in and hone skills,

00:51:28.360 --> 00:51:30.960
intermediate and beginner level.

00:51:30.960 --> 00:51:36.970
So I guess my ask to all of
you is, as Women Techmakers,

00:51:36.970 --> 00:51:39.070
now we've got a platform.

00:51:39.070 --> 00:51:40.360
So let's keep pushing.

00:51:40.360 --> 00:51:41.870
Let's all work on
this visibility.

00:51:41.870 --> 00:51:43.750
Let's all work on
building community.

00:51:43.750 --> 00:51:47.660
I think that what's cool and
amazing about the tech industry

00:51:47.660 --> 00:51:50.100
is we're really
mission-driven people.

00:51:50.100 --> 00:51:53.490
And we really believe that
we can make the changes.

00:51:53.490 --> 00:51:55.730
We work on all these
extraordinary products.

00:51:55.730 --> 00:51:58.130
And so, even though, like
all the other industries,

00:51:58.130 --> 00:52:01.510
we have these issues with
bias, and with trouble,

00:52:01.510 --> 00:52:03.240
and with things that
have gone before,

00:52:03.240 --> 00:52:05.990
we are one of the
most innovative groups

00:52:05.990 --> 00:52:07.180
on the planet.

00:52:07.180 --> 00:52:09.740
And so I think we actually
have the possibility

00:52:09.740 --> 00:52:12.380
to move the fastest
to change all of this,

00:52:12.380 --> 00:52:14.790
and to really include
the people that we need

00:52:14.790 --> 00:52:16.540
in the innovation in this world.

00:52:16.540 --> 00:52:19.190
So thank you for being
here, and for hearing

00:52:19.190 --> 00:52:21.250
these incredible women,
and for all the work

00:52:21.250 --> 00:52:22.160
that everybody does.

00:52:22.160 --> 00:52:24.110
It's definitely
only going to work

00:52:24.110 --> 00:52:26.200
if we have parallel
processing everywhere,

00:52:26.200 --> 00:52:27.415
parallel innovation.

00:52:27.415 --> 00:52:29.040
One of the things I
love about the fact

00:52:29.040 --> 00:52:32.380
that our L team-- Larry's team,
and the executives at Google--

00:52:32.380 --> 00:52:36.190
set a goal for us to be--
could we be the best company

00:52:36.190 --> 00:52:37.940
for women and
underrepresented minorities?

00:52:37.940 --> 00:52:39.390
Could we figure
out how to do that?

00:52:39.390 --> 00:52:39.890
Yeah.

00:52:39.890 --> 00:52:40.410
[APPLAUSE]

00:52:40.410 --> 00:52:40.910
And--

00:52:40.910 --> 00:52:45.210
[APPLAUSE]

00:52:45.210 --> 00:52:47.870
--and you see us doing
this innovative work,

00:52:47.870 --> 00:52:49.530
really piloting
things, trying things.

00:52:49.530 --> 00:52:50.230
Some of them will work.

00:52:50.230 --> 00:52:50.730
Some won't.

00:52:50.730 --> 00:52:52.450
We released our
numbers to try to get

00:52:52.450 --> 00:52:54.850
our mind around the challenges.

00:52:54.850 --> 00:52:57.770
But really, what it really did
by setting that goal, to me,

00:52:57.770 --> 00:52:59.650
was it opened up
the innovation space

00:52:59.650 --> 00:53:02.380
to all of us that wonder,
what does that mean?

00:53:02.380 --> 00:53:03.560
How would I measure that?

00:53:03.560 --> 00:53:05.250
How can I actually
make an impact?

00:53:05.250 --> 00:53:06.882
And so that's what
we're all up to.

00:53:06.882 --> 00:53:08.340
And we hope you'll
join us, and set

00:53:08.340 --> 00:53:10.010
that goal for our
whole industry.

00:53:10.010 --> 00:53:11.237
So thank you for being here.

00:53:11.237 --> 00:53:13.070
And I think we're going
to go to Q&amp;A, right?

00:53:13.070 --> 00:53:15.408
OK, so come on up.

00:53:15.408 --> 00:53:20.350
[APPLAUSE AND CHEERING]

00:53:20.350 --> 00:53:23.110
So all of our speakers
are going to come up.

00:53:23.110 --> 00:53:25.140
And also Natalie
Villalobos, who runs Women

00:53:25.140 --> 00:53:28.380
Techmakers and been driving
this, and [INAUDIBLE],

00:53:28.380 --> 00:53:30.240
together with Stephanie
Liu, who's here,

00:53:30.240 --> 00:53:32.070
and some of the other folks.

00:53:32.070 --> 00:53:35.540
So I think we have, like,
five minutes for some couple

00:53:35.540 --> 00:53:36.040
questions.

00:53:36.040 --> 00:53:37.915
NATALIE VILLALOBOS:
Yeah, about five minutes.

00:53:37.915 --> 00:53:41.040
I actually have a
question to Jaimie.

00:53:41.040 --> 00:53:44.092
So why do you call it a her?

00:53:44.092 --> 00:53:45.050
JAIME WAYDO: Thank you.

00:53:45.050 --> 00:53:46.330
I forgot to tell you that.

00:53:46.330 --> 00:53:51.030
So everybody at JPL calls our
rovers by the female pronoun

00:53:51.030 --> 00:53:52.440
she and her.

00:53:52.440 --> 00:53:55.300
Some will tell you that it's
after the Navy tradition, where

00:53:55.300 --> 00:53:57.010
they name their
ships after females,

00:53:57.010 --> 00:53:59.050
because they're
difficult to control.

00:53:59.050 --> 00:53:59.800
MEGAN SMITH: Whew!

00:53:59.800 --> 00:54:00.944
[LAUGHTER]

00:54:00.944 --> 00:54:02.360
JAIME WAYDO: I
think it's actually

00:54:02.360 --> 00:54:04.890
because they're amazing,
beautiful creations.

00:54:04.890 --> 00:54:06.370
And we should celebrate that.

00:54:06.370 --> 00:54:07.880
So that's why I her her.

00:54:07.880 --> 00:54:08.880
NATALIE VILLALOBOS: Yay!

00:54:08.880 --> 00:54:10.840
[APPLAUSE]

00:54:10.840 --> 00:54:12.890
But yeah, so we have some
mics up at the front.

00:54:12.890 --> 00:54:15.190
We already have one
nice gentleman here.

00:54:15.190 --> 00:54:16.160
What's your question?

00:54:16.160 --> 00:54:16.950
AUDIENCE: Hi.

00:54:16.950 --> 00:54:20.000
It's not as much a question,
but-- I'm from Toronto, Canada.

00:54:20.000 --> 00:54:22.745
And my wife actually
runs two organizations

00:54:22.745 --> 00:54:24.870
called Ladies Learning Code
and Kids Learning Code.

00:54:24.870 --> 00:54:27.605
I don't know if you've had a
chance to take a look at them.

00:54:27.605 --> 00:54:29.980
But if you haven't, I strongly
recommend you take a look.

00:54:29.980 --> 00:54:31.521
It's exactly what
you guys are doing.

00:54:31.521 --> 00:54:34.290
And I'm sure they'd love to
participate in whenever you do.

00:54:34.290 --> 00:54:34.940
MEGAN SMITH: Where's your wife?

00:54:34.940 --> 00:54:37.220
AUDIENCE: She is back in
Toronto, unfortunately.

00:54:37.220 --> 00:54:39.160
She's running some
workshops, and preparing

00:54:39.160 --> 00:54:41.212
for the entire summer
of camp, and what not.

00:54:41.212 --> 00:54:41.630
MEGAN SMITH: Nice.

00:54:41.630 --> 00:54:42.650
So Ladies-- say it again?

00:54:42.650 --> 00:54:43.510
AUDIENCE: Ladies Learning Code--

00:54:43.510 --> 00:54:44.565
MEGAN SMITH: Ladies
Learning Code.

00:54:44.565 --> 00:54:44.900
AUDIENCE: --and
Kids Learning Code.

00:54:44.900 --> 00:54:46.240
MEGAN SMITH: And
Kids Learning Code.

00:54:46.240 --> 00:54:47.420
AUDIENCE: Girls Learning
Code as well, sorry.

00:54:47.420 --> 00:54:48.360
MEGAN SMITH: OK, and
Girls Learning Code.

00:54:48.360 --> 00:54:51.080
So we will add those into--
we have the Women Techmakers

00:54:51.080 --> 00:54:53.280
site, which Natalie's got
all kinds of resources.

00:54:53.280 --> 00:54:54.910
Every time you want to
tell us about stuff,

00:54:54.910 --> 00:54:55.785
we'll add them there.

00:54:55.785 --> 00:54:58.930
And also, last week, we launched
the Made With Code high-school

00:54:58.930 --> 00:55:01.910
girl outreach program,
which is the things you love

00:55:01.910 --> 00:55:02.790
are made with code.

00:55:02.790 --> 00:55:04.540
For whatever reason,
they don't know that.

00:55:04.540 --> 00:55:05.310
And we need to tell them.

00:55:05.310 --> 00:55:07.130
So we'll add the
girls-related stuff there.

00:55:07.130 --> 00:55:08.450
AUDIENCE: We saw that, and
thought it was really amazing.

00:55:08.450 --> 00:55:10.408
She's really excited to
use some of that stuff.

00:55:10.408 --> 00:55:11.330
And thanks, yeah.

00:55:11.330 --> 00:55:13.380
NATALIE VILLALOBOS: Yeah,
thanks for being here.

00:55:13.380 --> 00:55:13.880
Yes?

00:55:16.640 --> 00:55:18.810
AUDIENCE: This question
is for Gabriella.

00:55:18.810 --> 00:55:22.330
When she was taking the
measurements for the Fukushima

00:55:22.330 --> 00:55:26.850
disaster along the coast
and measuring its impact

00:55:26.850 --> 00:55:31.910
in the coastal Pacific,
what was the outcome,

00:55:31.910 --> 00:55:34.700
or what is the
conclusion to that?

00:55:34.700 --> 00:55:39.130
Is that something that we could,
at this moment, be fearful

00:55:39.130 --> 00:55:41.530
of all the fish?

00:55:41.530 --> 00:55:49.190
Does it jeopardize
our food chain?

00:55:49.190 --> 00:55:51.170
Seafood chain?

00:55:51.170 --> 00:55:54.930
GABRIELLA LEVINE: So that
was my question as well.

00:55:54.930 --> 00:55:58.587
What's going on under water?

00:55:58.587 --> 00:56:00.170
Because people are
eating the seafood.

00:56:00.170 --> 00:56:00.961
People are fishing.

00:56:00.961 --> 00:56:02.470
And it's being distributed.

00:56:02.470 --> 00:56:04.930
We just got a very
little amount of data.

00:56:04.930 --> 00:56:07.560
And it was with a
very rough sensor.

00:56:07.560 --> 00:56:10.780
So we're trying to
continue that effort.

00:56:10.780 --> 00:56:13.770
And Protei's trying to
take to get more partners

00:56:13.770 --> 00:56:15.910
to get more data faster.

00:56:15.910 --> 00:56:20.250
At the moment, yeah, there's
not all that much data about it.

00:56:20.250 --> 00:56:22.260
But there are other
resources to look

00:56:22.260 --> 00:56:24.280
at that I could also
talk to you about after.

00:56:24.280 --> 00:56:27.237
MEGAN SMITH: We ran a
session, "Solve for X,"

00:56:27.237 --> 00:56:29.320
solveforx.com session,
which is one of the passion

00:56:29.320 --> 00:56:31.440
projects at Google X.
And there's a guy, Lou,

00:56:31.440 --> 00:56:33.820
who's here, who's doing
really interesting

00:56:33.820 --> 00:56:37.030
crowd-source measure
monitoring across China.

00:56:37.030 --> 00:56:39.270
The Ali Baba team actually
went home from vacation

00:56:39.270 --> 00:56:42.057
doing crowd source--
it's called Danger Map.

00:56:42.057 --> 00:56:42.890
Look for that video.

00:56:42.890 --> 00:56:46.250
But this idea that Gabriella
and others are onto,

00:56:46.250 --> 00:56:48.810
and your point which actually
drew from your insight

00:56:48.810 --> 00:56:51.690
from firefighting, that
sometimes the big, meta stuff

00:56:51.690 --> 00:56:55.490
isn't as good as small pieces
on the ground and lots of datas.

00:56:55.490 --> 00:56:59.540
And so using robots in this
form is what she's advocating,

00:56:59.540 --> 00:57:00.420
I think.

00:57:00.420 --> 00:57:02.260
And we need to get
out, as developers,

00:57:02.260 --> 00:57:05.380
and innovate more of that,
and do more measurement,

00:57:05.380 --> 00:57:07.790
so that we can exactly
have the answers.

00:57:07.790 --> 00:57:10.020
Not only for this
Chinese project in China,

00:57:10.020 --> 00:57:13.190
or projects around the world,
and to engage youth into them,

00:57:13.190 --> 00:57:15.300
but also specifically
for oceans.

00:57:15.300 --> 00:57:17.270
AUDIENCE: Well, living
in the Bay Area,

00:57:17.270 --> 00:57:19.130
you know, this is a
real major concern.

00:57:19.130 --> 00:57:20.940
Because already,
there are some areas

00:57:20.940 --> 00:57:25.001
that, you know, they have
signs posted, do not fish here.

00:57:25.001 --> 00:57:26.000
GABRIELLA LEVINE: Right.

00:57:26.000 --> 00:57:28.700
AUDIENCE: And that's kind
of in the back of my mind.

00:57:28.700 --> 00:57:29.290
Thank you.

00:57:29.290 --> 00:57:29.620
MEGAN SMITH: Thank you.

00:57:29.620 --> 00:57:30.745
NATALIE VILLALOBOS: Thanks.

00:57:33.120 --> 00:57:34.080
AUDIENCE: Hi.

00:57:34.080 --> 00:57:35.610
My name is Daphne Larose.

00:57:35.610 --> 00:57:38.610
And hearing all of your stories
was-- as a fellow engineer,

00:57:38.610 --> 00:57:41.430
it's, like, really
inspiring and wonderful.

00:57:41.430 --> 00:57:45.390
And as someone who has
made it my own mission

00:57:45.390 --> 00:57:48.230
to bring visibility not
just to women in tech,

00:57:48.230 --> 00:57:50.170
but women of color
in tech, I think

00:57:50.170 --> 00:57:53.470
it's just awesome to hear
about what you guys have done.

00:57:53.470 --> 00:57:56.050
One of the questions I've
often had for myself,

00:57:56.050 --> 00:57:59.260
and that I hear a lot
from other engineers,

00:57:59.260 --> 00:58:02.364
is how do you find your niche?

00:58:02.364 --> 00:58:03.780
So each of you
kind of have, like,

00:58:03.780 --> 00:58:06.675
an area that you just
seem to fall into.

00:58:06.675 --> 00:58:08.390
But, like, how did
you discover that?

00:58:08.390 --> 00:58:10.670
And where did you really
discover your passion?

00:58:15.380 --> 00:58:17.950
JAIME WAYDO: I'll tell you
how I discovered my passion.

00:58:17.950 --> 00:58:19.870
I had it at JPL.

00:58:19.870 --> 00:58:21.036
I was sending stuff to Mars.

00:58:21.036 --> 00:58:22.370
And I was super-stoked about it.

00:58:22.370 --> 00:58:24.785
And when you're doing things
that you're passionate about,

00:58:24.785 --> 00:58:25.490
you rock at it.

00:58:25.490 --> 00:58:27.240
And that's pretty fun.

00:58:27.240 --> 00:58:27.910
Then I left.

00:58:27.910 --> 00:58:30.210
And I went into medical
for a while, and hated it.

00:58:30.210 --> 00:58:31.140
Lost my passion.

00:58:31.140 --> 00:58:32.742
Couldn't figure out what it was.

00:58:32.742 --> 00:58:34.200
And it was actually
through Natalie

00:58:34.200 --> 00:58:36.890
calling me to do this
talk that I figured out

00:58:36.890 --> 00:58:37.840
what I love again.

00:58:37.840 --> 00:58:41.085
And it's solving those
gnarly, crazy problems.

00:58:41.085 --> 00:58:43.280
So thank you to Women
Tech, because you

00:58:43.280 --> 00:58:45.246
made me find my
passion again, so.

00:58:45.246 --> 00:58:46.495
MEGAN SMITH: Other questions--

00:58:46.495 --> 00:58:48.600
[APPLAUSE]

00:58:48.600 --> 00:58:49.720
--other passion stories?

00:58:49.720 --> 00:58:51.190
Passions?

00:58:51.190 --> 00:58:53.820
GABRIELLA LEVINE: I think
mine's a little similar.

00:58:53.820 --> 00:58:56.490
It was-- or, some
similar aspects.

00:58:56.490 --> 00:59:00.020
I found a community that I
really love to work with,

00:59:00.020 --> 00:59:02.320
and the tools that I
really loved working with.

00:59:02.320 --> 00:59:03.730
From there, it came easy.

00:59:03.730 --> 00:59:07.070
But I did have a very diverse
background, and a wide range

00:59:07.070 --> 00:59:09.620
of things that I've done,
both very technical as well

00:59:09.620 --> 00:59:10.710
as creative.

00:59:10.710 --> 00:59:12.410
So it was trying
a lot of things.

00:59:12.410 --> 00:59:14.210
Really, never saying no.

00:59:14.210 --> 00:59:16.882
And then, getting excited
about the tools and the people

00:59:16.882 --> 00:59:17.590
that I work with.

00:59:17.590 --> 00:59:19.215
And I was lucky to
find that, because I

00:59:19.215 --> 00:59:22.050
got very focused, and--
but also very unfocused,

00:59:22.050 --> 00:59:23.280
answering big questions.

00:59:23.280 --> 00:59:26.812
And so, a combination
of all of that.

00:59:26.812 --> 00:59:29.540
MEGAN SMITH: Awesome.

00:59:29.540 --> 00:59:32.260
PAVNI DIWANJI: Yeah,
I think I would

00:59:32.260 --> 00:59:35.290
say my passion has been
always around having impact.

00:59:35.290 --> 00:59:39.480
And that's what made me go
into technology, because I felt

00:59:39.480 --> 00:59:44.230
it was a creative
venue to be, and also

00:59:44.230 --> 00:59:45.590
have a huge impact on the world.

00:59:45.590 --> 00:59:48.790
So that's been kind of my
[INAUDIBLE] to go where I go.

00:59:50.805 --> 00:59:52.430
YOKY MATSUOKA: I'll
make sure it short.

00:59:52.430 --> 00:59:55.540
But, you know, for me,
it's don't think too hard.

00:59:55.540 --> 00:59:56.440
It's a big one.

00:59:56.440 --> 00:59:59.160
AUDIENCE: As engineers, that's
all we do is think too hard.

00:59:59.160 --> 01:00:00.480
YOKY MATSUOKA: Yeah, I
think it's really key.

01:00:00.480 --> 01:00:01.563
Just don't think too hard.

01:00:01.563 --> 01:00:03.950
Just get into something
that you love.

01:00:03.950 --> 01:00:06.900
And then that might lead
you to the next passion.

01:00:06.900 --> 01:00:08.560
And don't worry about
your life story.

01:00:08.560 --> 01:00:11.050
It's totally OK to have
multiple chapters, you know.

01:00:11.050 --> 01:00:12.800
I think that's
really what I think.

01:00:12.800 --> 01:00:15.057
MEGAN SMITH: So
you have to answer.

01:00:15.057 --> 01:00:16.640
NATALIE VILLALOBOS:
I also-- I'm going

01:00:16.640 --> 01:00:19.127
to-- I don't know if Pavni
said her name when she got up.

01:00:19.127 --> 01:00:20.210
But this is Pavni Diwajni.

01:00:20.210 --> 01:00:21.715
She's a VP of
engineering at Google.

01:00:21.715 --> 01:00:23.750
I just want to make
sure everyone knew that.

01:00:23.750 --> 01:00:24.210
MEGAN SMITH: She's amazing.

01:00:24.210 --> 01:00:24.640
NATALIE VILLALOBOS:
She's amazing.

01:00:24.640 --> 01:00:25.890
MEGAN SMITH: She does so
many different things.

01:00:25.890 --> 01:00:28.130
NATALIE VILLALOBOS: And then in
terms of me finding my passion,

01:00:28.130 --> 01:00:30.150
I think that-- so I
actually know Pavni,

01:00:30.150 --> 01:00:33.020
because we built
Google+ together.

01:00:33.020 --> 01:00:34.197
Obviously, I'm not a VP.

01:00:34.197 --> 01:00:36.530
But I really looked up to
her, when I was first starting

01:00:36.530 --> 01:00:38.490
on Google+ as a
community manager.

01:00:38.490 --> 01:00:40.160
And I think for me,
I started noticing

01:00:40.160 --> 01:00:42.780
that I loved nourishing
large groups of people.

01:00:42.780 --> 01:00:45.160
I loved hearing stories,
and helping people,

01:00:45.160 --> 01:00:47.490
and assisting them
with particular asks.

01:00:47.490 --> 01:00:51.340
But it was really about kind
of solving for a larger group.

01:00:51.340 --> 01:00:54.530
And so I think for me, I started
to see an opportunity of,

01:00:54.530 --> 01:00:57.434
how can I actually help
foster social movement?

01:00:57.434 --> 01:00:59.600
And with everything that's
going on in our industry,

01:00:59.600 --> 01:01:01.558
I think I just needed to
add more fuel to that.

01:01:01.558 --> 01:01:04.120
And I wanted to dedicate
my life to that.

01:01:04.120 --> 01:01:05.680
MEGAN SMITH: I got
to take acoustics

01:01:05.680 --> 01:01:07.992
from Professor Bose--
like Bose speakers.

01:01:07.992 --> 01:01:10.200
And he used to tell us, you
guys are high performers.

01:01:10.200 --> 01:01:11.430
You have to find your passion.

01:01:11.430 --> 01:01:13.471
If you find your passion,
you can be unstoppable.

01:01:13.471 --> 01:01:15.700
He said it's the number
one thing you need to do.

01:01:15.700 --> 01:01:18.320
And so, I think it's a
really important question

01:01:18.320 --> 01:01:19.900
for people to ask themselves.

01:01:19.900 --> 01:01:23.742
And I think also for women, to
like also be present-- like,

01:01:23.742 --> 01:01:25.700
get out and speak and
talk about your passions.

01:01:25.700 --> 01:01:28.990
Because you have amazing
things to talk about, too.

01:01:28.990 --> 01:01:31.030
For myself, I was
lucky because I

01:01:31.030 --> 01:01:32.980
got to do science fair as a kid.

01:01:32.980 --> 01:01:35.550
And I like-- I work on a
million different things.

01:01:35.550 --> 01:01:36.720
I'm very ADD.

01:01:36.720 --> 01:01:39.775
So I-- but they tend
to have a theme.

01:01:39.775 --> 01:01:41.400
One is, I really like
to work on things

01:01:41.400 --> 01:01:43.810
that help people
with their lives.

01:01:43.810 --> 01:01:46.120
And I like to work
on things that

01:01:46.120 --> 01:01:47.680
reduce our impact on the planet.

01:01:47.680 --> 01:01:49.180
And almost everything
kind of themes

01:01:49.180 --> 01:01:51.330
around that inclusion,
those kinds of themes.

01:01:51.330 --> 01:01:54.594
And I think, I also tend
to not be the founder.

01:01:54.594 --> 01:01:55.510
I've done that before.

01:01:55.510 --> 01:01:58.310
But I tend to have a
really good eye for people

01:01:58.310 --> 01:02:01.720
who have great ideas, especially
in the early, earliest stages,

01:02:01.720 --> 01:02:03.190
and figure out how to help them.

01:02:03.190 --> 01:02:05.020
Reid Hoffman calls it
the smart generalists

01:02:05.020 --> 01:02:06.312
who work with the founders.

01:02:06.312 --> 01:02:08.520
So I tend to come up underneath
people and help them.

01:02:08.520 --> 01:02:09.978
And I think that
in Silicon Valley,

01:02:09.978 --> 01:02:13.220
we always focus on the founder,
when really-- Redina Dugan, who

01:02:13.220 --> 01:02:15.990
was, I think, in this
room earlier today,

01:02:15.990 --> 01:02:19.260
who ran DARPA, she said, there's
a new thing the basketball

01:02:19.260 --> 01:02:21.530
teams are doing,
studying hustle.

01:02:21.530 --> 01:02:24.880
So who's on the court with
Michael Jordan when he's really

01:02:24.880 --> 01:02:25.920
performing?

01:02:25.920 --> 01:02:28.330
And so there's people around
who can do that stuff.

01:02:28.330 --> 01:02:30.199
And I like to do
that kind of work.

01:02:30.199 --> 01:02:31.990
And I think that those
are very valid jobs.

01:02:31.990 --> 01:02:33.400
And they really
move stuff forward.

01:02:33.400 --> 01:02:35.066
Because people have
extraordinary ideas.

01:02:35.066 --> 01:02:37.560
If you can amplify them,
major things can happen.

01:02:37.560 --> 01:02:39.560
NATALIE VILLALOBOS: And
to add to that, and then

01:02:39.560 --> 01:02:41.060
actually, we're going
to have end Q&amp;A.

01:02:41.060 --> 01:02:43.080
But we'll be over here,
and available to answer

01:02:43.080 --> 01:02:44.560
more questions.

01:02:44.560 --> 01:02:47.180
So Megan actually taught me
a really important thing,

01:02:47.180 --> 01:02:48.670
which was the term yes and.

01:02:48.670 --> 01:02:51.690
I never took improv theater
classes when I was younger.

01:02:51.690 --> 01:02:53.490
And so, surround
yourself with people

01:02:53.490 --> 01:02:56.570
that say "yes and" to
you, more than "yes but."

01:02:56.570 --> 01:02:58.720
Thank you so much
for joining us.

01:02:58.720 --> 01:03:02.970
[APPLAUSE]

