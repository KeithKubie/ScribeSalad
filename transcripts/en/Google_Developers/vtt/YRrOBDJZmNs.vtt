WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.994
[MUSIC PLAYING]

00:00:06.072 --> 00:00:08.280
LAURENCE MORONEY: Welcome
to "Coffee with a Googler."

00:00:08.280 --> 00:00:10.720
I'm here today to talk with
Christine Robson, who's

00:00:10.720 --> 00:00:12.407
a research PM for
machine learning.

00:00:12.407 --> 00:00:14.990
We're going to talk about what's
new and exciting in the field

00:00:14.990 --> 00:00:17.580
of machine learning, including
Project Magenta, which

00:00:17.580 --> 00:00:20.217
is all about machine learning
for generating music.

00:00:20.217 --> 00:00:22.050
We'll also take a look
at some of the things

00:00:22.050 --> 00:00:23.851
that are happening
in Youtube as well as

00:00:23.851 --> 00:00:25.100
advances in image recognition.

00:00:28.560 --> 00:00:30.720
So that video,
that was an artist

00:00:30.720 --> 00:00:33.467
who did a cover of a track that
was generated by a computer.

00:00:33.467 --> 00:00:34.050
Is that right?

00:00:34.050 --> 00:00:34.850
CHRISTINE ROBSON: Yes, exactly.

00:00:34.850 --> 00:00:36.800
So the piano that you
were hearing, that's us.

00:00:36.800 --> 00:00:38.290
That's the machine
learned music.

00:00:38.290 --> 00:00:40.620
That's the generative
model, the TensorFlow model,

00:00:40.620 --> 00:00:42.340
coming up with a piano track.

00:00:42.340 --> 00:00:45.110
And then we put this out there
as part of our open sourcing

00:00:45.110 --> 00:00:46.800
efforts to put all
this generative music

00:00:46.800 --> 00:00:48.390
models that we're doing online.

00:00:48.390 --> 00:00:50.325
And these artists, they
listened to the music.

00:00:50.325 --> 00:00:51.450
They were interested in it.

00:00:51.450 --> 00:00:52.620
They were inspired by it.

00:00:52.620 --> 00:00:55.962
And they decided to sort of
jam along with our music track.

00:00:55.962 --> 00:00:58.854
[MUSIC PLAYING]

00:01:06.225 --> 00:01:08.600
LAURENCE MORONEY: Were you
expecting somebody to do that?

00:01:08.600 --> 00:01:10.270
CHRISTINE ROBSON: No, we
weren't expecting that at all.

00:01:10.270 --> 00:01:11.495
It was really very exciting.

00:01:11.495 --> 00:01:12.870
We got all excited
in the office.

00:01:12.870 --> 00:01:14.990
We're all passing the video
around, taking a look.

00:01:14.990 --> 00:01:16.950
It was really pretty exciting.

00:01:16.950 --> 00:01:20.170
But this is what we
hope will happen, right,

00:01:20.170 --> 00:01:22.620
that we can use machine
learning for these really

00:01:22.620 --> 00:01:26.400
creative purposes and really
get a partnership going where

00:01:26.400 --> 00:01:29.250
artists and creators want to
work with machine learning

00:01:29.250 --> 00:01:31.960
tools to be more
creative and to have fun.

00:01:31.960 --> 00:01:32.960
LAURENCE MORONEY: Right.

00:01:32.960 --> 00:01:34.834
I remember you said
something once-- it's not

00:01:34.834 --> 00:01:35.820
about replacing music.

00:01:35.820 --> 00:01:37.907
It's almost like
inspiring musicians.

00:01:37.907 --> 00:01:39.240
CHRISTINE ROBSON: Yeah, exactly.

00:01:39.240 --> 00:01:42.156
And something-- you know, Doug,
who's the lead on this project,

00:01:42.156 --> 00:01:43.530
he talks about
this all the time,

00:01:43.530 --> 00:01:46.770
that it's not that we're trying
to make the best music out

00:01:46.770 --> 00:01:47.270
there.

00:01:47.270 --> 00:01:49.376
I mean, I'm not even a musician.

00:01:49.376 --> 00:01:51.550
It's that we're trying to
make tools that the best

00:01:51.550 --> 00:01:55.630
musicians can use, tools that
spur this kind of creativity.

00:01:55.630 --> 00:01:57.780
And the Magenta Project,
that's the project that

00:01:57.780 --> 00:02:01.270
was open source that we
generated this music from.

00:02:01.270 --> 00:02:03.510
The Magenta Project's really
about making those tools

00:02:03.510 --> 00:02:05.250
for creators.

00:02:05.250 --> 00:02:08.070
Starting with music, but we
want to have tools for creators

00:02:08.070 --> 00:02:11.090
to do art generation,
to do videos,

00:02:11.090 --> 00:02:12.990
to really tackle all
of these questions

00:02:12.990 --> 00:02:15.680
of how can you be
creative and work

00:02:15.680 --> 00:02:18.436
with a machine-learned model
to make yourself more creative?

00:02:18.436 --> 00:02:20.560
LAURENCE MORONEY: And now,
for this piece of music,

00:02:20.560 --> 00:02:23.820
it was-- I recognized the first
bars was like "Twinkle Twinkle

00:02:23.820 --> 00:02:24.440
Little Star."

00:02:24.440 --> 00:02:24.730
CHRISTINE ROBSON: Yeah.

00:02:24.730 --> 00:02:26.130
LAURENCE MORONEY: And
it took over from there?

00:02:26.130 --> 00:02:27.213
CHRISTINE ROBSON: Exactly.

00:02:27.213 --> 00:02:29.760
So we give it this quick
bar, like [SINGING].

00:02:29.760 --> 00:02:34.290
And then the model is
trained over a huge swath

00:02:34.290 --> 00:02:37.050
of music-- everything
from Bach to the Beatles.

00:02:37.050 --> 00:02:40.830
And it's just trying to predict
what the next note should be.

00:02:40.830 --> 00:02:42.760
But it uses a type
of machine learning

00:02:42.760 --> 00:02:44.260
called a recurrent
neural network.

00:02:44.260 --> 00:02:47.060
So it looks backwards in
time over the music that's

00:02:47.060 --> 00:02:49.750
been played already and
tries to predict sequentially

00:02:49.750 --> 00:02:52.030
what the next note
should be based on of all

00:02:52.030 --> 00:02:53.417
of the music it's seen before.

00:02:53.417 --> 00:02:55.750
So you give it just a couple
of notes to get it started,

00:02:55.750 --> 00:02:57.014
and then it just takes off.

00:02:57.014 --> 00:02:57.930
LAURENCE MORONEY: Wow.

00:02:57.930 --> 00:02:58.440
CHRISTINE ROBSON: Yeah.

00:02:58.440 --> 00:02:59.270
LAURENCE MORONEY:
That's pretty cool.

00:02:59.270 --> 00:03:00.440
CHRISTINE ROBSON:
It is really cool.

00:03:00.440 --> 00:03:01.800
LAURENCE MORONEY: And this
was built with Magenta?

00:03:01.800 --> 00:03:02.560
CHRISTINE ROBSON: Yes.

00:03:02.560 --> 00:03:03.180
LAURENCE MORONEY:
And I know you're

00:03:03.180 --> 00:03:04.610
the product lead for Magenta.

00:03:04.610 --> 00:03:06.130
Could you-- like
if somebody wanted

00:03:06.130 --> 00:03:08.120
to get started with
Magenta and maybe

00:03:08.120 --> 00:03:11.440
start inspiring other people
in some other kind of art form,

00:03:11.440 --> 00:03:13.910
or maybe with music, how would
they go about doing this?

00:03:13.910 --> 00:03:16.490
CHRISTINE ROBSON: So everything
from Magenta is online.

00:03:16.490 --> 00:03:16.810
We have a GitHub.

00:03:16.810 --> 00:03:18.330
It's part of the
TensorFlow project.

00:03:18.330 --> 00:03:23.560
So if you go to the
tensorflow/magenta on GitHub,

00:03:23.560 --> 00:03:25.630
you can go and you can
download all of the tools.

00:03:25.630 --> 00:03:27.540
We encourage everyone to
download, to contribute,

00:03:27.540 --> 00:03:29.000
to play with what
we've got there

00:03:29.000 --> 00:03:31.091
and see if they can do
something creative too.

00:03:31.091 --> 00:03:32.840
I mean, that's why
we've put it out there.

00:03:32.840 --> 00:03:33.140
LAURENCE MORONEY: Nice.

00:03:33.140 --> 00:03:34.450
And it's not just music, right?

00:03:34.450 --> 00:03:35.770
There are other art forms too?

00:03:35.770 --> 00:03:36.200
CHRISTINE ROBSON:
Well right now,

00:03:36.200 --> 00:03:37.908
we've been focusing
the project on music.

00:03:37.908 --> 00:03:40.170
But the tools are very general.

00:03:40.170 --> 00:03:43.330
The tools are really about
generative machine learning,

00:03:43.330 --> 00:03:47.310
this idea of generating music,
generating art, the tools

00:03:47.310 --> 00:03:49.774
to create rather than
just making predictions

00:03:49.774 --> 00:03:50.690
with machine learning.

00:03:50.690 --> 00:03:51.740
LAURENCE MORONEY: OK, cool.

00:03:51.740 --> 00:03:54.160
And machine learning, of course,
it's not just incubation.

00:03:54.160 --> 00:03:56.060
It's in use in the
real world already,

00:03:56.060 --> 00:03:59.370
like things like YouTube are
using it for recommendations.

00:03:59.370 --> 00:04:00.640
CHRISTINE ROBSON: When you think
about a machine-learned model

00:04:00.640 --> 00:04:02.340
and you think about all
of the possible data they

00:04:02.340 --> 00:04:03.940
can go into it and
all of the places

00:04:03.940 --> 00:04:05.648
where the signals are
going to come from,

00:04:05.648 --> 00:04:07.870
if you can capture more
signals than you might

00:04:07.870 --> 00:04:09.911
be able to think of as a
human, you can do better

00:04:09.911 --> 00:04:10.970
than a rule-based system.

00:04:10.970 --> 00:04:14.400
And there's also this notion
in-- it's a very hard concept

00:04:14.400 --> 00:04:15.570
to explain.

00:04:15.570 --> 00:04:19.160
There's this notion of
embeddings in computer science

00:04:19.160 --> 00:04:23.640
and machine learning
research, where

00:04:23.640 --> 00:04:26.520
you put a way to capture all
this information together.

00:04:26.520 --> 00:04:28.990
You capture relationships
which aren't easy to describe

00:04:28.990 --> 00:04:30.344
by simple, human labels.

00:04:30.344 --> 00:04:32.510
Like maybe the relationship
between these two things

00:04:32.510 --> 00:04:33.630
is really easy to explain.

00:04:33.630 --> 00:04:35.213
You can imagine
writing a rule for it,

00:04:35.213 --> 00:04:37.134
like I really like cat videos.

00:04:37.134 --> 00:04:38.550
So maybe you'd
say, this cat video

00:04:38.550 --> 00:04:40.008
is similar to this
other cat video.

00:04:40.008 --> 00:04:41.040
They both contain cats.

00:04:41.040 --> 00:04:44.109
They're both from Japan,
something like this.

00:04:44.109 --> 00:04:45.650
But then there might
be other factors

00:04:45.650 --> 00:04:47.850
that are really hard
for a human to suss out.

00:04:47.850 --> 00:04:49.650
Like maybe what I
really like about this

00:04:49.650 --> 00:04:52.270
is sort of an abstract
feel about the video,

00:04:52.270 --> 00:04:54.240
the way in which I
like watching the cat

00:04:54.240 --> 00:04:55.492
jump in and out of the box.

00:04:55.492 --> 00:04:57.200
And that kind of
abstract feel translates

00:04:57.200 --> 00:05:00.040
to this totally other also
kind of cute video involving

00:05:00.040 --> 00:05:01.570
pandas on slides.

00:05:01.570 --> 00:05:03.100
And these two
things are related,

00:05:03.100 --> 00:05:05.900
but it's hard to put your
tongue on exactly why.

00:05:05.900 --> 00:05:07.650
But they're obviously
related, and they're

00:05:07.650 --> 00:05:10.009
related for everybody who
looks at these YouTube videos.

00:05:10.009 --> 00:05:12.050
And that's something that
a machine-learned model

00:05:12.050 --> 00:05:13.760
can capture really
easily, especially

00:05:13.760 --> 00:05:15.630
with these embeddings
that we use,

00:05:15.630 --> 00:05:18.187
and that it's really hard for
a human to write a rule for.

00:05:18.187 --> 00:05:19.520
LAURENCE MORONEY: Right, got it.

00:05:19.520 --> 00:05:21.550
So it's like if you like
x, you might like y.

00:05:21.550 --> 00:05:23.060
And you may not
realize it, but when

00:05:23.060 --> 00:05:25.310
you have all these variables
coming out of an LM model

00:05:25.310 --> 00:05:27.400
that it can actually
show you that,

00:05:27.400 --> 00:05:29.530
and you can actually
learn that about yourself.

00:05:29.530 --> 00:05:29.990
CHRISTINE ROBSON: Yes, exactly.

00:05:29.990 --> 00:05:31.750
LAURENCE MORONEY: It's like
an emergent type thing.

00:05:31.750 --> 00:05:33.833
CHRISTINE ROBSON: It does
feel emergent sometimes,

00:05:33.833 --> 00:05:36.510
these notions of ideas that you
wouldn't have thought really

00:05:36.510 --> 00:05:39.022
mattered for your
preferences-- like things

00:05:39.022 --> 00:05:41.480
that I might not have realized
I wanted to see in a YouTube

00:05:41.480 --> 00:05:43.730
video, like how I
want to spend my time,

00:05:43.730 --> 00:05:46.300
that I really like
this particular niche

00:05:46.300 --> 00:05:47.252
kind of cat video.

00:05:47.252 --> 00:05:49.210
LAURENCE MORONEY: I've
discovered lots of music

00:05:49.210 --> 00:05:50.530
that I like in that
way, where it's

00:05:50.530 --> 00:05:51.920
like I'm watching
one piece of music,

00:05:51.920 --> 00:05:53.450
and then it suddenly
goes onto something else

00:05:53.450 --> 00:05:55.100
on the playlist of
somebody I've never

00:05:55.100 --> 00:05:58.035
even heard of in a style that
I probably don't even like.

00:05:58.035 --> 00:06:00.662
But hey, that's not
bad, that type of thing.

00:06:00.662 --> 00:06:01.620
CHRISTINE ROBSON: Yeah.

00:06:01.620 --> 00:06:02.620
And we actually do that.

00:06:02.620 --> 00:06:06.030
We do machine learning for
like music list curation.

00:06:06.030 --> 00:06:09.320
So actually, the lead
on the Magenta Project,

00:06:09.320 --> 00:06:13.470
Doug, his role at Google
before we started this project

00:06:13.470 --> 00:06:15.540
was in the music
recommendation system.

00:06:15.540 --> 00:06:16.220
He's a musician.

00:06:16.220 --> 00:06:17.790
He's got a real passion,
both for machine learning

00:06:17.790 --> 00:06:18.770
and for music.

00:06:18.770 --> 00:06:20.380
And so we spent a
lot of time working

00:06:20.380 --> 00:06:22.810
on how to predict what music
you're going to really enjoy

00:06:22.810 --> 00:06:25.180
listening to based on what
music you already like,

00:06:25.180 --> 00:06:28.720
and looking not just at a very
simple thing, like which people

00:06:28.720 --> 00:06:31.130
like the same kind of music,
but actually analyzing

00:06:31.130 --> 00:06:33.800
the structure of the music
and understanding how

00:06:33.800 --> 00:06:36.410
different musical pieces are
related to each other in sort

00:06:36.410 --> 00:06:38.730
of subtle ways and
fundamental ways,

00:06:38.730 --> 00:06:43.090
and being able to really do
very curated playlists for you

00:06:43.090 --> 00:06:45.322
to find things that you're
going to be interested in.

00:06:45.322 --> 00:06:47.280
LAURENCE MORONEY: So I
understand that YouTube,

00:06:47.280 --> 00:06:48.960
it's done so well with
this kind of stuff.

00:06:48.960 --> 00:06:50.043
It's even won some awards.

00:06:50.043 --> 00:06:50.840
Is that right?

00:06:50.840 --> 00:06:52.790
CHRISTINE ROBSON:
Yeah, so you don't

00:06:52.790 --> 00:06:54.190
think when you're a computer
scientist that you're

00:06:54.190 --> 00:06:56.440
going to be working in a
field that gets an Emmy.

00:06:56.440 --> 00:06:58.920
But actually, YouTube won an
Emmy for personalized video

00:06:58.920 --> 00:06:59.660
recommendations.

00:06:59.660 --> 00:07:00.210
LAURENCE MORONEY: Oh, wow.

00:07:00.210 --> 00:07:01.550
CHRISTINE ROBSON: It
was a technical Emmy.

00:07:01.550 --> 00:07:02.680
LAURENCE MORONEY: Hey,
an Emmy's an Emmy.

00:07:02.680 --> 00:07:03.070
CHRISTINE ROBSON: I know.

00:07:03.070 --> 00:07:03.584
It's true.

00:07:03.584 --> 00:07:05.000
And this was awarded
because we do

00:07:05.000 --> 00:07:07.740
such a good job of personalizing
video recommendations.

00:07:07.740 --> 00:07:09.770
When I see that
perfect cat video,

00:07:09.770 --> 00:07:12.340
that's because we're
doing such a good job

00:07:12.340 --> 00:07:14.500
prioritizing what
you want to watch

00:07:14.500 --> 00:07:17.290
and prioritizing how to find
the perfect video for you

00:07:17.290 --> 00:07:18.116
in this moment.

00:07:18.116 --> 00:07:19.490
LAURENCE MORONEY:
So going beyond

00:07:19.490 --> 00:07:21.170
like recommendation engines
and that kind of thing,

00:07:21.170 --> 00:07:22.870
one of the things I
keep reading about

00:07:22.870 --> 00:07:25.260
is how LM is now used
for image recognition

00:07:25.260 --> 00:07:27.160
and for attributing images.

00:07:27.160 --> 00:07:30.010
And I know you've been
involved in that a little bit.

00:07:30.010 --> 00:07:31.260
Could you tell us about it?

00:07:31.260 --> 00:07:32.860
CHRISTINE ROBSON:
Yeah, so if you

00:07:32.860 --> 00:07:34.480
haven't used the
Google Photos app,

00:07:34.480 --> 00:07:36.021
you should use the
Google Photos app.

00:07:36.021 --> 00:07:38.164
It's my new favorite thing.

00:07:38.164 --> 00:07:39.580
I love that I can
pick up my phone

00:07:39.580 --> 00:07:42.350
and ask it to find
me photos of my son,

00:07:42.350 --> 00:07:46.020
ask it to find me photos of
my son playing with my cat,

00:07:46.020 --> 00:07:48.020
and that it has this
great model where

00:07:48.020 --> 00:07:50.400
it knows-- it can
do face detection

00:07:50.400 --> 00:07:51.560
and find my son for me.

00:07:51.560 --> 00:07:54.580
It can do detection on my
cat and find my cat for me.

00:07:54.580 --> 00:07:57.040
And it does this
because we've fed

00:07:57.040 --> 00:07:59.880
this model all of the images
we've seen from the web of all

00:07:59.880 --> 00:08:01.372
of the different kinds of cats.

00:08:01.372 --> 00:08:03.330
And it has this model
built up where it sort of

00:08:03.330 --> 00:08:04.560
knows what a cat looks like.

00:08:04.560 --> 00:08:06.614
And so it can label
it very effectively.

00:08:06.614 --> 00:08:09.030
LAURENCE MORONEY: So one of
things about image recognition

00:08:09.030 --> 00:08:11.710
is I remember you had-- I
read something from you even

00:08:11.710 --> 00:08:13.490
talking about a cat.

00:08:13.490 --> 00:08:14.990
And it could recognize
a cat's face,

00:08:14.990 --> 00:08:16.330
even though the cat
didn't have ears.

00:08:16.330 --> 00:08:18.960
And the ears are one of the
most distinctive parts of a cat,

00:08:18.960 --> 00:08:19.459
right?

00:08:19.459 --> 00:08:21.100
CHRISTINE ROBSON:
Yeah, that's my cat.

00:08:21.100 --> 00:08:23.000
So a few years ago,
my cat, who lives

00:08:23.000 --> 00:08:25.590
in California of course
and suntans all the time,

00:08:25.590 --> 00:08:27.770
she got some skin
cancer in her ears.

00:08:27.770 --> 00:08:29.420
She's fine now.

00:08:29.420 --> 00:08:33.084
But the vet had to remove
her ears to clear the cancer.

00:08:33.084 --> 00:08:34.750
And so she looks a
little bit funny now.

00:08:34.750 --> 00:08:36.708
She's still really cute,
but she's got no ears.

00:08:36.708 --> 00:08:40.159
And she looks less like a
cat than you might imagine.

00:08:40.159 --> 00:08:42.280
But our image recognition
software is fine.

00:08:42.280 --> 00:08:43.880
It can still recognize
her as a cat.

00:08:43.880 --> 00:08:47.580
And so if I ask my phone for
photos of my son and my cat,

00:08:47.580 --> 00:08:50.990
it still recognizes that
she's a cat and can find her.

00:08:50.990 --> 00:08:53.730
And it's this
flexibility that you

00:08:53.730 --> 00:08:57.677
get from training a model
with a lot of different data.

00:08:57.677 --> 00:08:59.260
There's not probably
a lot of examples

00:08:59.260 --> 00:09:02.840
of cats with no ears on YouTube,
but we can sort of extrapolate

00:09:02.840 --> 00:09:05.400
when we use these really
powerful, deep, neural network

00:09:05.400 --> 00:09:08.430
models to sort of--
we know basically

00:09:08.430 --> 00:09:09.680
what the cat should look like.

00:09:09.680 --> 00:09:12.020
And so we can extrapolate
past things like the fact

00:09:12.020 --> 00:09:13.724
that she doesn't
have ears anymore.

00:09:13.724 --> 00:09:14.654
LAURENCE MORONEY: Aww.

00:09:14.654 --> 00:09:16.070
I'm glad she's
doing well, though.

00:09:16.070 --> 00:09:16.370
CHRISTINE ROBSON: Yes.

00:09:16.370 --> 00:09:16.900
LAURENCE MORONEY: So
a couple of months

00:09:16.900 --> 00:09:18.484
ago, we had Joshua
Gordon on the show.

00:09:18.484 --> 00:09:20.524
And there was one thing
that he said to me that I

00:09:20.524 --> 00:09:21.860
thought was really fascinating.

00:09:21.860 --> 00:09:24.150
And he was talking about
three, four, five years

00:09:24.150 --> 00:09:26.560
from now, the skills
of machine learning

00:09:26.560 --> 00:09:28.350
and the techniques
of machine learning

00:09:28.350 --> 00:09:30.120
will be as important
to developers

00:09:30.120 --> 00:09:33.790
as like JavaScript
or SQL are today.

00:09:33.790 --> 00:09:34.850
And I think he was right.

00:09:34.850 --> 00:09:38.837
And I would love to inspire
people to go out and do more

00:09:38.837 --> 00:09:40.920
into this, like learn more
about machine learning,

00:09:40.920 --> 00:09:43.330
do things such as
creating music or art,

00:09:43.330 --> 00:09:45.930
or who knows what other kind
of things they'll come up with.

00:09:45.930 --> 00:09:47.990
And I know you've got
a wealth of resources.

00:09:47.990 --> 00:09:49.640
And what kind of
things would you

00:09:49.640 --> 00:09:51.250
ask developers to
take a look at?

00:09:51.250 --> 00:09:51.710
CHRISTINE ROBSON:
Well, of course

00:09:51.710 --> 00:09:54.126
I'm very passionate about this
because machine learning is

00:09:54.126 --> 00:09:54.830
my passion.

00:09:54.830 --> 00:09:56.830
But I do think
that Josh is right.

00:09:56.830 --> 00:09:58.120
We've talked about this a lot.

00:09:58.120 --> 00:10:00.882
But thinking about
machine learning

00:10:00.882 --> 00:10:03.340
is so different than thinking
about other traditional kinds

00:10:03.340 --> 00:10:04.910
of software engineering.

00:10:04.910 --> 00:10:07.860
I often make the analogy
that the difference

00:10:07.860 --> 00:10:10.030
between functional programming
and object-oriented

00:10:10.030 --> 00:10:12.250
programming, that
paradigm shift is

00:10:12.250 --> 00:10:14.197
much smaller than
the shift of thinking

00:10:14.197 --> 00:10:15.780
about machine learning
versus thinking

00:10:15.780 --> 00:10:17.630
about traditional
programming methods.

00:10:17.630 --> 00:10:19.970
So the shift is very
big and very important,

00:10:19.970 --> 00:10:22.409
and that's why we want
everybody to be together

00:10:22.409 --> 00:10:23.450
on it from the beginning.

00:10:23.450 --> 00:10:25.230
That's why we're putting so
much effort into open sourcing

00:10:25.230 --> 00:10:25.929
our tools.

00:10:25.929 --> 00:10:27.970
We're really open sourcing
the best of everything

00:10:27.970 --> 00:10:30.740
we've got to try and get
everybody together around this.

00:10:30.740 --> 00:10:32.140
LAURENCE MORONEY: So
developers in the outside world

00:10:32.140 --> 00:10:33.931
kind of already get
the best Google, right?

00:10:33.931 --> 00:10:35.080
CHRISTINE ROBSON: Yes.

00:10:35.080 --> 00:10:36.050
LAURENCE MORONEY: --which
is just mind-blowing

00:10:36.050 --> 00:10:37.170
when you think about it.

00:10:37.170 --> 00:10:37.490
CHRISTINE ROBSON: Yeah.

00:10:37.490 --> 00:10:39.406
And I mean, this is why
we've put so much work

00:10:39.406 --> 00:10:41.550
into open sourcing
TensorFlow and why we really

00:10:41.550 --> 00:10:43.240
want to put this all out there.

00:10:43.240 --> 00:10:45.680
It's so important to think
about what you can do

00:10:45.680 --> 00:10:47.150
with machine learning models.

00:10:47.150 --> 00:10:48.650
Because I really
think this is going

00:10:48.650 --> 00:10:51.140
to be transformative for the
kinds of things we can do.

00:10:51.140 --> 00:10:53.390
I mean everything
from YouTube, where

00:10:53.390 --> 00:10:55.670
you're predicting the
next video or other angles

00:10:55.670 --> 00:10:58.086
you can think of that affect
your sort of day-to-day life,

00:10:58.086 --> 00:11:02.500
to these more esoteric, creative
things, like music generation,

00:11:02.500 --> 00:11:04.500
things where you really
want to have the best

00:11:04.500 --> 00:11:06.083
tools at your disposal,
and you really

00:11:06.083 --> 00:11:08.850
want to be able to be
as creative as possible.

00:11:08.850 --> 00:11:11.780
And machine learning is going
to enable all of these things.

00:11:11.780 --> 00:11:13.489
So we want everybody
to be really fluent.

00:11:13.489 --> 00:11:14.446
LAURENCE MORONEY: Yeah.

00:11:14.446 --> 00:11:16.570
I just thoroughly
recommend it for anybody.

00:11:16.570 --> 00:11:19.160
Any developers out there, just
go take a look at the sites

00:11:19.160 --> 00:11:20.370
that we've linked below.

00:11:20.370 --> 00:11:21.953
There's some fabulous
stuff out there,

00:11:21.953 --> 00:11:23.920
and you're getting
ready for the future.

00:11:23.920 --> 00:11:25.260
So thank you so much, Christine.

00:11:25.260 --> 00:11:26.340
This has been awesome.

00:11:26.340 --> 00:11:26.710
CHRISTINE ROBSON: Thank you.

00:11:26.710 --> 00:11:28.418
It was a real pleasure
chatting with you.

00:11:28.418 --> 00:11:30.100
LAURENCE MORONEY:
Had a lot of fun.

