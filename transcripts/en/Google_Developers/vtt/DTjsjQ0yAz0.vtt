WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:00.875
SPEAKER 1: All right.

00:00:00.875 --> 00:00:02.700
So I've got a couple
questions over here.

00:00:02.700 --> 00:00:04.771
If you have a question,
just line up over there.

00:00:04.771 --> 00:00:06.270
I see a couple
people already there.

00:00:06.270 --> 00:00:07.190
So go for it.

00:00:07.190 --> 00:00:08.490
Take it away.

00:00:08.490 --> 00:00:09.095
AUDIENCE: Hi.

00:00:09.095 --> 00:00:10.720
This question's
probably for Guellermo,

00:00:10.720 --> 00:00:14.060
but anybody else maybe if
they have some opinions too.

00:00:14.060 --> 00:00:16.490
So I have a pretty
large application

00:00:16.490 --> 00:00:19.270
which has users and communities
and things like that.

00:00:19.270 --> 00:00:22.080
Once in awhile, a user will go
onto the page where it says,

00:00:22.080 --> 00:00:25.890
like, list all the users in this
community which in the novice

00:00:25.890 --> 00:00:30.670
sense emits a whole lot of get
XHR's like one for each user.

00:00:30.670 --> 00:00:33.302
I've turned that into a batch
request to get all of them

00:00:33.302 --> 00:00:34.010
at the same time.

00:00:34.010 --> 00:00:37.170
But it still is going over
the regular HTTP connection.

00:00:37.170 --> 00:00:40.761
But I also have Socket.IO
running for real time updates.

00:00:40.761 --> 00:00:42.510
And I was thinking
about it earlier today.

00:00:42.510 --> 00:00:46.080
I was wondering if it
made sense in any way

00:00:46.080 --> 00:00:49.390
if the socket.IO connection was
actually websocket connection

00:00:49.390 --> 00:00:51.270
to route some of
these XHR request

00:00:51.270 --> 00:00:56.520
that would normally go through
the regular HitUp connection.

00:00:56.520 --> 00:00:59.114
Is that a sort of thing
that would be stupid?

00:00:59.114 --> 00:01:00.530
GUILLERMO RAUCH:
Yeah, definitely.

00:01:00.530 --> 00:01:05.459
I've seen a lot of people-- I
mean, if you're using socket,

00:01:05.459 --> 00:01:06.650
A, you're doing it right.

00:01:06.650 --> 00:01:08.240
So definitely.

00:01:08.240 --> 00:01:12.030
No, I've see a lot of people
do this HTTP over websocket.

00:01:12.030 --> 00:01:14.985
I think when it comes out
to specific optimization

00:01:14.985 --> 00:01:16.610
like the one you're
mentioning, there's

00:01:16.610 --> 00:01:19.020
a lot of things to consider.

00:01:19.020 --> 00:01:22.050
One of them is, if you're
making tons of XHR requests

00:01:22.050 --> 00:01:26.440
in parallel, SPDY solves
this problem really well.

00:01:26.440 --> 00:01:28.960
Because it allows you
to push resources.

00:01:28.960 --> 00:01:30.970
It allows you to multiplex.

00:01:30.970 --> 00:01:32.520
It also does compression.

00:01:32.520 --> 00:01:36.820
So by turning on SPDY, you might
get an immediate win there.

00:01:36.820 --> 00:01:40.260
The other thing to
consider is of course

00:01:40.260 --> 00:01:44.510
on the other extreme, which
is ditching HTTP requests

00:01:44.510 --> 00:01:49.470
and just requesting them through
some sort of event or RPC call

00:01:49.470 --> 00:01:50.680
sorts.

00:01:50.680 --> 00:01:54.650
So I think the thing that's most
important there kind of goes

00:01:54.650 --> 00:01:57.720
back to what I was managing
earlier is on that page,

00:01:57.720 --> 00:02:00.130
you sort of know what
requests are going to come.

00:02:00.130 --> 00:02:01.720
And it's optimizing around that.

00:02:01.720 --> 00:02:05.000
Maybe not so much optimizing
around the client requesting,

00:02:05.000 --> 00:02:09.350
but the server sending without
a client requesting itself.

00:02:09.350 --> 00:02:10.800
Does that make sense?

00:02:10.800 --> 00:02:11.550
OK.

00:02:11.550 --> 00:02:13.440
Anyone want to add something?

00:02:13.440 --> 00:02:15.980
Maybe say definitely?

00:02:15.980 --> 00:02:17.640
DAN SHAW: Definitely.

00:02:17.640 --> 00:02:20.480
Yeah, and I Guillermo
touched on the point

00:02:20.480 --> 00:02:23.750
that I was going to add
there that the routing

00:02:23.750 --> 00:02:27.440
for the returned
responses could--

00:02:27.440 --> 00:02:28.990
you could find a
lot of efficiency

00:02:28.990 --> 00:02:35.990
by having those be routed back
to your app with socket.IO.

00:02:35.990 --> 00:02:40.101
So you would have
response events and sort

00:02:40.101 --> 00:02:42.350
of dynamic events all coming
through the same channel.

00:02:45.827 --> 00:02:46.660
AUDIENCE: All right.

00:02:46.660 --> 00:02:50.400
Probably for Erik and
Dan, excluding some

00:02:50.400 --> 00:02:53.330
of the optimizations
that Trevor talked about,

00:02:53.330 --> 00:02:57.370
I would like to know the ratio
of front end programmers who

00:02:57.370 --> 00:03:03.280
become fully functional full
stack programmers versus

00:03:03.280 --> 00:03:10.270
the more server side Java C++
.NET whatever sort of experts

00:03:10.270 --> 00:03:13.180
who then become functional
front end people.

00:03:13.180 --> 00:03:16.120
So it's the ratio
of one to the other.

00:03:16.120 --> 00:03:18.850
DAN SHAW: Very complex question.

00:03:18.850 --> 00:03:22.668
All right, so there's
two questions there.

00:03:22.668 --> 00:03:24.060
The-- huh?

00:03:24.060 --> 00:03:25.330
0%?

00:03:25.330 --> 00:03:26.020
OK.

00:03:26.020 --> 00:03:26.520
0%.

00:03:26.520 --> 00:03:27.134
Present

00:03:27.134 --> 00:03:28.050
AUDIENCE: [INAUDIBLE].

00:03:32.145 --> 00:03:35.860
DAN SHAW: No one needs
to be full stack.

00:03:35.860 --> 00:03:38.560
I wanted to highlight
that on this panel here,

00:03:38.560 --> 00:03:41.600
there are at least two
front end developers

00:03:41.600 --> 00:03:43.490
who've become back developers.

00:03:43.490 --> 00:03:44.990
And that would be
myself and Trevor.

00:03:44.990 --> 00:03:45.781
TREVOR NORRIS: 50%.

00:03:45.781 --> 00:03:47.830
DAN SHAW: 50%, right?

00:03:47.830 --> 00:03:51.130
And then were you
back end first?

00:03:51.130 --> 00:03:52.915
GUILLERMO RAUCH:
I don't remember.

00:03:52.915 --> 00:03:53.790
DAN SHAW: Full stack.

00:03:53.790 --> 00:03:57.720
GUILLERMO RAUCH: I was-- I
was front end and back end.

00:03:57.720 --> 00:04:02.130
But Javascript got me
interested in, I think,

00:04:02.130 --> 00:04:03.760
optimizing the
entire network stack.

00:04:03.760 --> 00:04:05.939
And I think that's a cool
opportunity with Node.

00:04:05.939 --> 00:04:07.730
You might start-- or
Javascript in general.

00:04:07.730 --> 00:04:11.060
You might start with something
very, very front end based

00:04:11.060 --> 00:04:13.910
and then do whatever
you want like control

00:04:13.910 --> 00:04:20.740
a drone or [INAUDIBLE]
or write an entire OSTCP

00:04:20.740 --> 00:04:22.260
stack in JavaScript.

00:04:22.260 --> 00:04:23.945
So 0%.

00:04:27.540 --> 00:04:31.210
AUDIENCE: Specifically for Erik,
within Paypal, you probably

00:04:31.210 --> 00:04:33.530
have some data points
here-- front end people who

00:04:33.530 --> 00:04:37.540
became serving their
own data versus people

00:04:37.540 --> 00:04:42.670
who were responsible for serving
data becoming front end people.

00:04:42.670 --> 00:04:45.080
ERIK TOTH: Yeah, it's
an interesting question.

00:04:45.080 --> 00:04:49.540
I think first of all,
full stack changes

00:04:49.540 --> 00:04:50.810
depending on where you go.

00:04:50.810 --> 00:04:54.250
So when I think full stack,
I think front end developer.

00:04:54.250 --> 00:04:58.700
I think web application
the-- what did you call it?

00:04:58.700 --> 00:05:01.300
The back end of the front end?

00:05:01.300 --> 00:05:03.690
The back front end engineer?

00:05:03.690 --> 00:05:06.430
Front end back end engineer,
the mid tier engineer, and then

00:05:06.430 --> 00:05:09.550
somebody that knows
database level stuff.

00:05:09.550 --> 00:05:12.160
Like, that's full stack for me.

00:05:12.160 --> 00:05:14.317
So if you break
it down that way,

00:05:14.317 --> 00:05:16.400
I don't know that we have
any full-- we may have--

00:05:16.400 --> 00:05:18.870
I don't think we have any
full stack engineers at Paypal

00:05:18.870 --> 00:05:21.280
because we have a service
oriented architecture.

00:05:21.280 --> 00:05:23.710
We intentionally
segment people to have

00:05:23.710 --> 00:05:26.320
those disciplines on purpose.

00:05:26.320 --> 00:05:29.290
That said, if you're really
talking about mid-tier

00:05:29.290 --> 00:05:30.910
potentially, mid-tier,
or application

00:05:30.910 --> 00:05:35.210
level engineers that have--
almost said graduated.

00:05:35.210 --> 00:05:37.480
That's not the right
term-- but that embrace

00:05:37.480 --> 00:05:41.750
front end engineering and
then front end engineers

00:05:41.750 --> 00:05:45.420
that have embraced
application development.

00:05:45.420 --> 00:05:48.600
I think it's-- I mean, it's kind
of an expectation that has been

00:05:48.600 --> 00:05:49.250
set.

00:05:49.250 --> 00:05:51.434
And most everyone has
made that transition.

00:05:51.434 --> 00:05:53.600
One of the things that we
did during that transition

00:05:53.600 --> 00:05:55.724
is we acknowledged that
maybe everyone doesn't

00:05:55.724 --> 00:05:56.390
want to do this.

00:05:56.390 --> 00:05:57.360
And that's OK.

00:05:57.360 --> 00:06:00.180
And we moved that criteria
down so if you just

00:06:00.180 --> 00:06:02.380
want to work on air
quotes back end,

00:06:02.380 --> 00:06:05.580
then there's plenty of
mid-tier services, applications

00:06:05.580 --> 00:06:07.040
that you can work on.

00:06:07.040 --> 00:06:09.970
But if you want to work on
the front tier, that's great.

00:06:09.970 --> 00:06:12.460
Really, all that is is
aggregating services,

00:06:12.460 --> 00:06:16.570
aggregating data, and basically
transforming it into a format

00:06:16.570 --> 00:06:20.490
that you can use-- plucking
off the values that

00:06:20.490 --> 00:06:22.840
are important to
you and representing

00:06:22.840 --> 00:06:25.570
that for front end consumption.

00:06:25.570 --> 00:06:28.680
So I think we've had
really good success in it.

00:06:28.680 --> 00:06:29.930
I don't know.

00:06:29.930 --> 00:06:32.590
There's the whole, we
can reuse JavaScript

00:06:32.590 --> 00:06:34.610
on the front end
of the back end.

00:06:34.610 --> 00:06:40.050
I don't see that I've seen any
actual-- I don't see that I've

00:06:40.050 --> 00:06:43.290
seen that in practice that's
actually added a ton of value.

00:06:43.290 --> 00:06:47.670
But the fact that all
of our front engineers

00:06:47.670 --> 00:06:50.000
understand async
programming with JavaScript

00:06:50.000 --> 00:06:52.990
automatically put them a step
ahead when getting into Node

00:06:52.990 --> 00:06:55.820
and understanding how
Node API works, APIs work,

00:06:55.820 --> 00:06:58.940
how talking to services work,
and just the mental model

00:06:58.940 --> 00:07:02.340
around working at the back
end of the front end tier.

00:07:02.340 --> 00:07:05.240
I think we've seen really
good success with it just

00:07:05.240 --> 00:07:08.390
from that perspective
specifically.

00:07:08.390 --> 00:07:11.900
The corollary of seeing
Java engineers that

00:07:11.900 --> 00:07:13.400
haven't necessarily
worked-- they

00:07:13.400 --> 00:07:14.525
may have worked in futures.

00:07:14.525 --> 00:07:17.050
They may have worked in
multi-threaded applications.

00:07:17.050 --> 00:07:22.950
But to make that jump to async
development and not just,

00:07:22.950 --> 00:07:25.410
like, higher order
functions or lambda like,

00:07:25.410 --> 00:07:28.950
oh, I can pass functions around
and it looks async to actually

00:07:28.950 --> 00:07:34.040
grokking and understanding
async development has been-- I'm

00:07:34.040 --> 00:07:35.540
not going to say
it was a challenge.

00:07:35.540 --> 00:07:38.045
But it wasn't is natural
from my experience.

00:07:41.110 --> 00:07:42.585
That fair enough?

00:07:42.585 --> 00:07:43.575
AUDIENCE: Right on.

00:07:46.380 --> 00:07:50.280
DAN SHAW: Riffing on Erik's
talk, the great thing

00:07:50.280 --> 00:07:56.750
about the way the stacks
evolved and Node and front end

00:07:56.750 --> 00:07:59.690
JavaScript, It's really
given us opportunities

00:07:59.690 --> 00:08:03.530
to be creative and sort of
solve-- we're engineers solving

00:08:03.530 --> 00:08:07.030
the problems that we want to--
scratching the itches that we

00:08:07.030 --> 00:08:08.560
have, solving those problems.

00:08:08.560 --> 00:08:14.740
So we now can go and accomplish
those in new and unique ways.

00:08:14.740 --> 00:08:18.760
So encouraging or
forcing engineers

00:08:18.760 --> 00:08:22.190
to move one way or
the other I think

00:08:22.190 --> 00:08:23.970
is the wrong way of
thinking about it.

00:08:23.970 --> 00:08:28.190
Giving opportunities to grow
and expand your experience,

00:08:28.190 --> 00:08:30.150
your knowledge
and understanding,

00:08:30.150 --> 00:08:32.679
is what Node has provided.

00:08:32.679 --> 00:08:34.280
And I think that's
really compelling.

00:08:39.133 --> 00:08:39.799
AUDIENCE: Hello.

00:08:39.799 --> 00:08:43.409
My question is more
about the hardware.

00:08:43.409 --> 00:08:46.210
You mentioned in
your presentation

00:08:46.210 --> 00:08:51.140
that you're able to reduce the
number of servers like half

00:08:51.140 --> 00:08:56.710
while you had all your
application written in Java.

00:08:56.710 --> 00:08:58.550
And you moved to
Node.js and were

00:08:58.550 --> 00:09:00.990
able to reduce the
half for the server.

00:09:00.990 --> 00:09:04.540
So my question is, what
is the optimal setup

00:09:04.540 --> 00:09:06.610
of a server that you'll need?

00:09:06.610 --> 00:09:10.930
Suppose you have like
one high end server,

00:09:10.930 --> 00:09:17.520
like 128 gigabyte of RAM,
210 gigabit ethernet,

00:09:17.520 --> 00:09:21.410
and like 40 CPUs.

00:09:21.410 --> 00:09:25.860
So would you chop into
the virtual machines?

00:09:25.860 --> 00:09:29.720
What is the optimal way so you
get the best performance, maybe

00:09:29.720 --> 00:09:33.250
that sweet spot?

00:09:33.250 --> 00:09:35.340
ERIK TOTH: I think the
question is predicated

00:09:35.340 --> 00:09:37.700
on a comment I made, but I
think everybody can probably

00:09:37.700 --> 00:09:39.230
chime in to this.

00:09:39.230 --> 00:09:42.430
My very short answer is that
it has absolutely nothing

00:09:42.430 --> 00:09:44.320
to do with the
hardware profile and it

00:09:44.320 --> 00:09:46.060
has all to do with
how you design

00:09:46.060 --> 00:09:48.420
the system and specifically
that application.

00:09:48.420 --> 00:09:52.130
And the thing that I referenced,
the particular application,

00:09:52.130 --> 00:09:56.030
the profile of that application,
was way more amenable to a Node

00:09:56.030 --> 00:09:58.249
stack then it happened
to be for the JVM.

00:09:58.249 --> 00:10:00.290
It wasn't necessarily
about the hardware profile.

00:10:00.290 --> 00:10:01.300
It wasn't anything else.

00:10:01.300 --> 00:10:04.140
It was that specific
app had-- the work

00:10:04.140 --> 00:10:06.810
that it did just was
in Node's wheelhouse.

00:10:06.810 --> 00:10:11.170
And the framework that
it ran on on the JVM

00:10:11.170 --> 00:10:15.420
just was a square peg in a
round hole type of situation.

00:10:15.420 --> 00:10:18.190
AUDIENCE: My question
mostly-- question

00:10:18.190 --> 00:10:21.970
is that you know the
Node.jus uses one thread.

00:10:21.970 --> 00:10:25.740
So it's better to dedicate
one CPU per virtual machine.

00:10:25.740 --> 00:10:28.520
So then comes the
question, how much

00:10:28.520 --> 00:10:33.270
RAM is it better to dedicate
to application server

00:10:33.270 --> 00:10:37.280
and how much-- depending
maybe on the network or

00:10:37.280 --> 00:10:41.000
TREVOR NORRIS: All right,
so to address that,

00:10:41.000 --> 00:10:43.610
Node a single thread.

00:10:43.610 --> 00:10:51.160
So the general rule of thumb
is, one Node process per core.

00:10:51.160 --> 00:10:53.740
Given that you're
only running Node,

00:10:53.740 --> 00:11:01.040
right, the V8 heap maxes
out at 1.5 gigs by default.

00:11:01.040 --> 00:11:09.410
And so n cores times 2 is way
more RAM than you'll need.

00:11:09.410 --> 00:11:13.700
So if you're running 40 cores
with more than 80 gigs of RAM,

00:11:13.700 --> 00:11:20.300
you're going to be, like--
I mean, I hardly ever see

00:11:20.300 --> 00:11:25.340
hard hit applications using more
than 800 megs of RAM, right?

00:11:25.340 --> 00:11:30.506
So 40 cores and 60 gigs
of RAM should run you fine

00:11:30.506 --> 00:11:32.005
if you're running
40 Node processes.

00:11:35.110 --> 00:11:37.710
ERIK TOTH: Just a small,
like, editorial side

00:11:37.710 --> 00:11:40.720
note-- if your application
runs on two boxes that

00:11:40.720 --> 00:11:43.880
have 80 cores or
40 cores, you're

00:11:43.880 --> 00:11:46.090
kind of setting yourself
up for failure in the sense

00:11:46.090 --> 00:11:49.602
that if one of those boxes
goes down, you're kind of SOL.

00:11:49.602 --> 00:11:51.560
So I think at least for
what we've experienced,

00:11:51.560 --> 00:11:54.470
the optimum is actually
running more smaller boxes.

00:11:54.470 --> 00:11:56.230
You're more resilient
to any outages

00:11:56.230 --> 00:11:57.560
or any issues it may run.

00:11:57.560 --> 00:12:01.470
So I think the, like, how do you
optimize gigantic hardware may

00:12:01.470 --> 00:12:03.690
be answering the wrong
question in a way.

00:12:09.180 --> 00:12:12.860
DAN SHAW: So I would
encourage you to not

00:12:12.860 --> 00:12:19.870
think of your VMs as
just a single Node app.

00:12:19.870 --> 00:12:23.140
You're setting yourself
up for a monolith.

00:12:23.140 --> 00:12:26.930
So in each one of
your VMs, you may

00:12:26.930 --> 00:12:32.490
have multiple apps,
multiple processes doing

00:12:32.490 --> 00:12:34.260
and orchestrating
amongst themselves.

00:12:34.260 --> 00:12:38.530
So unfortunately, it's
one of those sort of not

00:12:38.530 --> 00:12:40.910
very satisfying, it depends.

00:12:40.910 --> 00:12:45.190
But I would encourage
you not to think

00:12:45.190 --> 00:12:48.450
of a single core,
single process,

00:12:48.450 --> 00:12:51.650
let's slice it,
those 40 cores up

00:12:51.650 --> 00:12:58.220
into a single process
in a single VM.

00:12:58.220 --> 00:13:00.850
Two can work in some
of those situations.

00:13:00.850 --> 00:13:08.890
But really looking at how each
individual deployment works

00:13:08.890 --> 00:13:13.400
effectively will color
how that gets carved up.

00:13:13.400 --> 00:13:16.890
And it'll probably
vary from application

00:13:16.890 --> 00:13:22.170
to application what the most
optimal profile of your VMs

00:13:22.170 --> 00:13:23.129
is going to look like.

00:13:23.129 --> 00:13:24.795
AUDIENCE: Actually I
was most interested

00:13:24.795 --> 00:13:30.960
in the ratio between the
network and the Rom memory.

00:13:30.960 --> 00:13:32.730
So if you have,
like, good network,

00:13:32.730 --> 00:13:37.250
if you want to keep a big
load on this one Node,

00:13:37.250 --> 00:13:38.190
then how much--

00:13:38.190 --> 00:13:38.900
TREVOR NORRIS: There is none.

00:13:38.900 --> 00:13:39.990
AUDIENCE: Rom-- sorry?

00:13:39.990 --> 00:13:41.590
TREVOR NORRIS: There's no ratio.

00:13:41.590 --> 00:13:42.210
AUDIENCE: There is no ratio.

00:13:42.210 --> 00:13:43.250
TREVOR NORRIS: It
completely depends

00:13:43.250 --> 00:13:44.330
on the work you're doing.

00:13:44.330 --> 00:13:45.950
If you're just
passing data through,

00:13:45.950 --> 00:13:48.790
I mean, I'm not crapping you--
if you're just passing data

00:13:48.790 --> 00:13:53.160
through on a Node app and you're
handling 10,000 connections

00:13:53.160 --> 00:13:58.091
on the box, I see no processes
run under 100 megs of RAM,

00:13:58.091 --> 00:13:58.590
right?

00:14:01.160 --> 00:14:03.370
But if you're doing,
like, template rendering,

00:14:03.370 --> 00:14:06.170
and you're doing the same
number of connections,

00:14:06.170 --> 00:14:10.250
you could be using 800
to a gig of RAM, right?

00:14:10.250 --> 00:14:14.040
There's no possible ratio
that is statistically

00:14:14.040 --> 00:14:18.542
significant in terms of
how much bandwidth you have

00:14:18.542 --> 00:14:19.750
and how much memory you need.

00:14:19.750 --> 00:14:22.480
AUDIENCE: So best approach is
benchmarking before going live?

00:14:22.480 --> 00:14:23.271
TREVOR NORRIS: Yes.

00:14:23.271 --> 00:14:25.630
Yeah, you need to know
the limitations and use

00:14:25.630 --> 00:14:27.994
case of your app to be
able to determine that.

00:14:27.994 --> 00:14:28.827
AUDIENCE: Thank you.

00:14:33.904 --> 00:14:35.570
AUDIENCE: This question
is for Guillermo

00:14:35.570 --> 00:14:39.100
and any other
member of the panel.

00:14:39.100 --> 00:14:42.740
In your presentation,
the issue of real time

00:14:42.740 --> 00:14:45.940
came up and really real time.

00:14:45.940 --> 00:14:48.830
How you considered
setting real time

00:14:48.830 --> 00:14:55.210
to, say, some common
denominator like 20 seconds

00:14:55.210 --> 00:14:57.530
so that no matter where
you are in the world,

00:14:57.530 --> 00:15:00.010
real time is always 20
seconds in the past?

00:15:02.739 --> 00:15:04.780
GUILLERMO RAUCH: I was
going to include something

00:15:04.780 --> 00:15:09.320
in this presentation, which
is the research by Nielsen

00:15:09.320 --> 00:15:14.520
on how we react or
how long it takes

00:15:14.520 --> 00:15:16.810
us to react to data changes.

00:15:16.810 --> 00:15:19.770
And I think that's a
meaningful addition to how

00:15:19.770 --> 00:15:23.490
we assess speed and real
timeness of applications.

00:15:23.490 --> 00:15:25.020
It roughly goes like this.

00:15:25.020 --> 00:15:29.090
If it's under 0.1
seconds, the user

00:15:29.090 --> 00:15:31.220
has a feeling that he's
interacting directly

00:15:31.220 --> 00:15:32.530
on the data.

00:15:32.530 --> 00:15:36.330
This is the case when you
use Excel on a local machine.

00:15:36.330 --> 00:15:38.430
Like, everything you
do is instantaneously.

00:15:38.430 --> 00:15:41.920
You think you're
altering data directly.

00:15:41.920 --> 00:15:47.430
And you sort of forget about
the persistence aspect of it.

00:15:47.430 --> 00:15:52.070
And he says, under one
second to most human beings,

00:15:52.070 --> 00:15:54.150
what happened was instantaneous.

00:15:54.150 --> 00:15:57.390
That's why I advocate for not
showing spinners or loading

00:15:57.390 --> 00:16:01.220
in the [INAUDIBLE] unless a
significant amount of time

00:16:01.220 --> 00:16:02.530
has passed.

00:16:02.530 --> 00:16:06.230
Because to most people, under
one second is instantaneous.

00:16:06.230 --> 00:16:10.020
I've trained myself to
recognize that, for example,

00:16:10.020 --> 00:16:14.430
my ps1 variable on batch
was taking too long.

00:16:14.430 --> 00:16:16.320
And it was like 50
milliseconds off.

00:16:16.320 --> 00:16:19.954
But most human beings that are
not staring at their terminals

00:16:19.954 --> 00:16:22.245
all think under one second,
everything's instantaneous.

00:16:25.890 --> 00:16:28.420
And then he says,
over 10 seconds,

00:16:28.420 --> 00:16:32.480
that's a limit at which you lose
your attention span and you,

00:16:32.480 --> 00:16:34.220
like, tab to something else.

00:16:34.220 --> 00:16:39.200
You open Reddit or
twitch.tv and you just

00:16:39.200 --> 00:16:40.770
do whatever-- you
do something else.

00:16:40.770 --> 00:16:40.970
Or you--

00:16:40.970 --> 00:16:42.100
DAN SHAW: [INAUDIBLE]
tablet's slowing down.

00:16:42.100 --> 00:16:43.266
GUILLERMO RAUCH: Like, yeah.

00:16:43.266 --> 00:16:45.550
When you guys were working
in the previous question,

00:16:45.550 --> 00:16:46.830
I was somewhere else.

00:16:46.830 --> 00:16:49.280
I'm just kidding.

00:16:49.280 --> 00:16:52.950
So I think there's a
theoretical basis to how

00:16:52.950 --> 00:16:55.950
we can assess, like, how real
time an application is, which

00:16:55.950 --> 00:16:58.880
has to do with, OK,
so the server learns

00:16:58.880 --> 00:17:00.690
of a change on the data.

00:17:00.690 --> 00:17:02.810
How fast can we get it to users?

00:17:02.810 --> 00:17:05.790
And that's why I was saying that
as long as your application is

00:17:05.790 --> 00:17:09.170
self-updating and
fast, And it could

00:17:09.170 --> 00:17:12.550
be fast just because
you're closer to server

00:17:12.550 --> 00:17:13.910
and you're [INAUDIBLE] right?

00:17:13.910 --> 00:17:17.210
Or even if the page was just
refreshing automatically,

00:17:17.210 --> 00:17:19.349
if the right conditions
are set, your application

00:17:19.349 --> 00:17:21.640
does feel real time.

00:17:21.640 --> 00:17:24.720
So yeah, it would
go from that angle

00:17:24.720 --> 00:17:27.619
to assess how real
time an application is.

00:17:27.619 --> 00:17:31.870
But I found that the best way
to, like, define it has been,

00:17:31.870 --> 00:17:33.210
it has to be fast.

00:17:33.210 --> 00:17:36.260
And the user has to learn
of the changes to the data

00:17:36.260 --> 00:17:38.610
without doing anything.

00:17:38.610 --> 00:17:41.860
AUDIENCE: That's all fine for
the biomechanical implications

00:17:41.860 --> 00:17:45.040
of a circuit, such as your
finger taps on something.

00:17:45.040 --> 00:17:47.850
You feel it in your
brain and so on.

00:17:47.850 --> 00:17:50.350
But if we're just talking
about setting the present

00:17:50.350 --> 00:17:54.220
to 20 seconds in
the past, then you

00:17:54.220 --> 00:17:55.840
could theoretically
bring everything

00:17:55.840 --> 00:17:59.900
that happened on the
server down to the client

00:17:59.900 --> 00:18:01.370
20 seconds in the past.

00:18:01.370 --> 00:18:03.320
And then that
person would always

00:18:03.320 --> 00:18:05.430
see the present as
it already happened.

00:18:08.110 --> 00:18:10.810
GUILLERMO RAUCH: I
don't follow that.

00:18:10.810 --> 00:18:14.010
AUDIENCE: So I--
we're all so tied up

00:18:14.010 --> 00:18:18.600
into trying to get something
transferred from a server

00:18:18.600 --> 00:18:19.184
to the client.

00:18:19.184 --> 00:18:21.516
GUILLERMO RAUCH: Oh, you're
saying if the client doesn't

00:18:21.516 --> 00:18:23.360
know the data has
changed, to them,

00:18:23.360 --> 00:18:25.520
it doesn't matter
how fast they get it?

00:18:25.520 --> 00:18:26.090
Is that--

00:18:26.090 --> 00:18:27.381
AUDIENCE: They already have it.

00:18:27.381 --> 00:18:30.090
It's already happened 20
seconds before they got it.

00:18:30.090 --> 00:18:33.160
Before they even saw it,
they already received it.

00:18:33.160 --> 00:18:34.160
GUILLERMO RAUCH: Maybe--

00:18:34.160 --> 00:18:36.510
AUDIENCE: [INAUDIBLE] delay.

00:18:36.510 --> 00:18:38.995
So, like, your stock
ticker, when you got it,

00:18:38.995 --> 00:18:40.290
would be 20 second behind.

00:18:40.290 --> 00:18:42.176
GUILLERMO RAUCH:
Like data sampling?

00:18:42.176 --> 00:18:42.926
TREVOR NORRIS: OK.

00:18:42.926 --> 00:18:44.532
So, yeah.

00:18:44.532 --> 00:18:45.990
I've actually seen
this and I think

00:18:45.990 --> 00:18:49.350
I know what you're saying
where it was actually updating

00:18:49.350 --> 00:18:52.260
by the second what
was happening.

00:18:52.260 --> 00:18:54.630
But when I looked the
code, what was happening

00:18:54.630 --> 00:18:58.950
is that it was sending me
chunk five second updates

00:18:58.950 --> 00:19:01.830
and then updating one second.

00:19:01.830 --> 00:19:04.950
But technically, the data
was five seconds in the past.

00:19:04.950 --> 00:19:07.550
But to me, it looked
instantaneous.

00:19:07.550 --> 00:19:08.810
Is that what you're saying?

00:19:08.810 --> 00:19:09.400
AUDIENCE: Yes.

00:19:09.400 --> 00:19:11.800
TREVOR NORRIS: OK.

00:19:11.800 --> 00:19:13.925
All right.

00:19:13.925 --> 00:19:15.050
GUILLERMO RAUCH: All right.

00:19:15.050 --> 00:19:17.320
So I think-- I think
that's important.

00:19:17.320 --> 00:19:18.820
Because a lot of
what we talk about

00:19:18.820 --> 00:19:20.890
is optimizing the
experience, right?

00:19:20.890 --> 00:19:24.770
But I think the most important
optimization we can make

00:19:24.770 --> 00:19:26.810
is have the power
to decide, right?

00:19:26.810 --> 00:19:29.540
And that's what serverpush
allows us to do.

00:19:29.540 --> 00:19:31.280
So even if we can
get away tricks

00:19:31.280 --> 00:19:33.330
like that, if we
don't have serverpush,

00:19:33.330 --> 00:19:37.270
we can't do the most optimal
way of sending data out.

00:19:37.270 --> 00:19:40.310
Because we want to send it right
away after the server knows.

00:19:40.310 --> 00:19:41.170
Maybe we don't.

00:19:41.170 --> 00:19:43.650
But if we have the
right architecture,

00:19:43.650 --> 00:19:47.800
we can make the choice, whereas
if you're stuck on some older

00:19:47.800 --> 00:19:50.230
architectures, you can't.

00:19:50.230 --> 00:19:53.861
But other than that, I
think that it's answered.

00:19:53.861 --> 00:19:55.244
You want to add something?

00:19:58.010 --> 00:20:04.290
AUDIENCE: So a couple years ago,
I went to a session on Node.js.

00:20:04.290 --> 00:20:13.380
And they were talking about SSL
termination at the Node tier

00:20:13.380 --> 00:20:19.050
and that one server failing
could take down or make

00:20:19.050 --> 00:20:23.730
the entire cluster
compute bound.

00:20:23.730 --> 00:20:26.604
And I wonder if
that's been addressed.

00:20:26.604 --> 00:20:27.145
DAN SHAW: No.

00:20:33.310 --> 00:20:35.010
So this is-- what
you're describing

00:20:35.010 --> 00:20:42.940
is actually a very-- I know
the exact scenario that you're

00:20:42.940 --> 00:20:43.560
talking about.

00:20:43.560 --> 00:20:46.790
Because I lived in it in Voxer.

00:20:46.790 --> 00:20:52.550
So that issue is
in part very, very

00:20:52.550 --> 00:20:59.310
specific to Voxer's
architecture.

00:20:59.310 --> 00:21:04.980
So in order to maintain the
rich, real time experience,

00:21:04.980 --> 00:21:06.640
there's a lot of
session affinity.

00:21:06.640 --> 00:21:11.280
So the fact there's
that session affinity,

00:21:11.280 --> 00:21:17.020
all of the SSL termination and
all that, the SSL handshakes,

00:21:17.020 --> 00:21:19.700
is very CPU intensive.

00:21:19.700 --> 00:21:24.620
And so if you do tens
of thousands all at

00:21:24.620 --> 00:21:29.050
the same time on a single
process and everybody

00:21:29.050 --> 00:21:32.090
is coming back to
the exact same Node,

00:21:32.090 --> 00:21:39.980
then you're going to
be in a world of hurt.

00:21:39.980 --> 00:21:44.350
So that's a challenge
very specific

00:21:44.350 --> 00:21:48.620
to that particular architecture.

00:21:48.620 --> 00:21:57.450
But the SSL termination story
in Node is still not great.

00:21:57.450 --> 00:22:02.330
We've made improvements
in what will be 12.

00:22:02.330 --> 00:22:04.650
But it's incremental.

00:22:04.650 --> 00:22:10.360
But typically in large
scale applications,

00:22:10.360 --> 00:22:15.240
the SSL termination is
done off the process.

00:22:15.240 --> 00:22:23.950
And if you look at the
proxies all way down story,

00:22:23.950 --> 00:22:30.060
there's a reasonable argument
to be made if you don't need SSL

00:22:30.060 --> 00:22:37.610
throughout your stack to do SSL
termination off the process.

00:22:37.610 --> 00:22:42.930
Do you want to talk about some
of the SSL termination stuff

00:22:42.930 --> 00:22:44.032
that you--

00:22:44.032 --> 00:22:44.990
ERIK TOTH: [INAUDIBLE].

00:22:44.990 --> 00:22:46.380
DAN SHAW: OK.

00:22:46.380 --> 00:22:48.890
OK.

00:22:48.890 --> 00:22:53.140
Let me let Erik talk about that.

00:22:53.140 --> 00:22:56.500
ERIK TOTH: So, a prerequisite
for everything at Paypal,

00:22:56.500 --> 00:22:59.370
obviously, is security.

00:22:59.370 --> 00:23:02.510
So we do use SSL
all the way down.

00:23:02.510 --> 00:23:05.060
We were proactive in
making the decision

00:23:05.060 --> 00:23:09.960
that we don't do any SSL in
Node, partially because it

00:23:09.960 --> 00:23:13.060
would be a performance
bottleneck for us.

00:23:13.060 --> 00:23:17.230
But as much as it's
like a warm, we

00:23:17.230 --> 00:23:20.120
love the Node community type of
this is how it operates thing,

00:23:20.120 --> 00:23:21.400
it actually is true.

00:23:21.400 --> 00:23:24.010
It's not a core competency
of our apps in the same way

00:23:24.010 --> 00:23:25.224
that running cluster is not.

00:23:25.224 --> 00:23:27.140
Like, understanding how
our deployments happen

00:23:27.140 --> 00:23:29.510
is not a core
competency of our apps.

00:23:29.510 --> 00:23:32.200
So, like, forcing
our apps to do SSL,

00:23:32.200 --> 00:23:33.940
forcing our apps
to do GZIP, that's

00:23:33.940 --> 00:23:35.430
not an application level thing.

00:23:35.430 --> 00:23:39.470
That's a system and
infrastructure level saying.

00:23:39.470 --> 00:23:41.159
So it's just work for us.

00:23:41.159 --> 00:23:43.200
And I think Dan was alluding
to some other things

00:23:43.200 --> 00:23:45.660
that we've had to do to
improve SSL performance

00:23:45.660 --> 00:23:47.670
since we have an n
tier architecture

00:23:47.670 --> 00:23:50.720
and doing SSL all the way down
is really expensive regardless

00:23:50.720 --> 00:23:53.190
of your implementation.

00:23:53.190 --> 00:23:55.940
But we've definitely
decided that those are not,

00:23:55.940 --> 00:23:58.270
like, core features
that our apps need

00:23:58.270 --> 00:23:59.330
to be concerned about.

00:23:59.330 --> 00:24:01.163
So we've done everything
we can to pull them

00:24:01.163 --> 00:24:04.080
into actual layers-- so
terminated engine x and then

00:24:04.080 --> 00:24:06.430
reverse proxy into
Node for example.

00:24:06.430 --> 00:24:09.090
Let things that do
are good at GZIP

00:24:09.090 --> 00:24:13.844
do GZIP and not force that
through the Node pipeline.

00:24:13.844 --> 00:24:15.260
GUILLERMO RAUCH:
Yeah, I was going

00:24:15.260 --> 00:24:18.120
to add something
specifically because we do it

00:24:18.120 --> 00:24:22.440
in the same way at
cloud up in Automatic.

00:24:22.440 --> 00:24:24.750
Even from a modularity
perspective,

00:24:24.750 --> 00:24:27.670
it makes more sense to always
consider your applications

00:24:27.670 --> 00:24:30.030
just vanilla HTTP server.

00:24:30.030 --> 00:24:34.040
And even if you decided
to handle SSL in Node,

00:24:34.040 --> 00:24:37.560
it would make more sense to
stack it with other modules

00:24:37.560 --> 00:24:40.540
or processes and
wrap those servers.

00:24:40.540 --> 00:24:42.490
So in our case, we
do with Engine X.

00:24:42.490 --> 00:24:47.360
But if Node becomes
better at it or we just

00:24:47.360 --> 00:24:49.030
want to do it in Node, we could.

00:24:49.030 --> 00:24:51.360
But your application
should just export

00:24:51.360 --> 00:24:55.170
lowest common denominator,
which would be just listening

00:24:55.170 --> 00:24:59.140
on some port and
exposing just HTTP.

