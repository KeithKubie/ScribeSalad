WEBVTT
Kind: captions
Language: en

00:00:00.140 --> 00:00:01.640
Notice I've changed
nothing about states.

00:00:01.640 --> 00:00:04.440
I've said nothing about function
approximation over states.

00:00:04.440 --> 00:00:08.150
I'm talking about abstraction over
this set of actions that you have.

00:00:08.150 --> 00:00:10.840
&gt;&gt; Well, you did implicitly talk
a little bit about states when you said

00:00:10.840 --> 00:00:16.340
that all the states in say the northeast
room end up being sort of equivalent.

00:00:16.340 --> 00:00:20.370
&gt;&gt; Right, equivalent with respect to
the action that you want to take, yes.

00:00:20.370 --> 00:00:21.100
&gt;&gt; Yeah.

00:00:21.100 --> 00:00:24.680
&gt;&gt; Right, which is its own kind of
approximation, its own kind of a track.

00:00:24.680 --> 00:00:26.270
All of these are different
kinds of abstraction.

00:00:27.300 --> 00:00:27.810
&gt;&gt; Okay.

00:00:27.810 --> 00:00:28.970
&gt;&gt; And will allow us to do scale.

00:00:28.970 --> 00:00:32.700
So let me just add one more thing to
this before we move on to the next line.

00:00:32.700 --> 00:00:35.110
And what I want to point out is
that this isn't just a way for

00:00:35.110 --> 00:00:38.130
you to think about this problem or
to sort of in a compact and

00:00:38.130 --> 00:00:41.430
efficient way explain how you would
accomplish a particular goal.

00:00:41.430 --> 00:00:43.930
It actually has other
computational benefits.

00:00:43.930 --> 00:00:46.870
So if we think in this sort
of southeast room here

00:00:46.870 --> 00:00:49.980
about how value propagation works,
right.

00:00:49.980 --> 00:00:51.560
You took a bunch of actions in here.

00:00:51.560 --> 00:00:53.130
You got to this good space.

00:00:53.130 --> 00:00:55.430
Then you start backing up values, right.

00:00:55.430 --> 00:00:57.970
So I know this is worth a lot,

00:00:57.970 --> 00:01:01.290
because one is the biggest number in
the universe without loss of generality.

00:01:01.290 --> 00:01:02.630
Assume one is the largest number.

00:01:02.630 --> 00:01:05.280
And then I'm going to learn
that being in this state and

00:01:05.280 --> 00:01:08.460
taking this particular
action is also good.

00:01:08.460 --> 00:01:10.280
I'll just draw a little dot here.

00:01:10.280 --> 00:01:13.960
The big size thing,
that's really good, and a smaller dot,

00:01:13.960 --> 00:01:15.810
a slightly smaller dog.

00:01:15.810 --> 00:01:19.520
And so the value of the states
around here are nice and

00:01:19.520 --> 00:01:24.920
big because I can easily get
from here to the goal state.

00:01:24.920 --> 00:01:29.360
And as I move farther
away the value gets to be

00:01:30.600 --> 00:01:35.540
a little bit less and then a little
bit less and a little bit less,

00:01:35.540 --> 00:01:40.920
and it sort of propagates
out into the other room.

00:01:40.920 --> 00:01:43.880
And the way that you propagate
this information is actually kind

00:01:43.880 --> 00:01:44.660
of slow, right.

00:01:44.660 --> 00:01:47.360
You have to get to the state and
sort of back it up a little bit.

00:01:47.360 --> 00:01:51.370
Your horizon is such that the value
seems relatively small and

00:01:51.370 --> 00:01:52.280
you kind of keep going.

00:01:52.280 --> 00:01:55.390
And you have to visit this often,
very often actually,

00:01:55.390 --> 00:01:58.300
to back up the information to
tell you sort of what the value

00:01:58.300 --> 00:02:01.110
of the states are, what the Q
values of state-action pairs are.

00:02:01.110 --> 00:02:06.580
And that's fine,
that's how we learn in MDPs.

00:02:06.580 --> 00:02:11.410
But, because I have this abstract
action, I have this go to doorway,

00:02:11.410 --> 00:02:16.470
I can actually from here, from this
particular state at the doorway,

00:02:16.470 --> 00:02:19.870
back up information all the way to here.

00:02:19.870 --> 00:02:24.510
Wherever it is I executed that
particular action in sort of one step.

00:02:24.510 --> 00:02:29.000
So I learned a lot about the value
of being in this state immediately

00:02:29.000 --> 00:02:30.480
from having taken that and got here.

00:02:30.480 --> 00:02:35.820
So soon as information from my little
plus one box here gets me to here,

00:02:35.820 --> 00:02:38.570
I can back that information
all the way up to where

00:02:38.570 --> 00:02:41.190
initially executed the go to doorway x.

00:02:41.190 --> 00:02:43.360
And similarly,
now that I've got information here,

00:02:43.360 --> 00:02:46.200
I can back it all the way up to
the value of being in this state.

00:02:46.200 --> 00:02:49.380
And so I can back up information
in value much more quickly

00:02:49.380 --> 00:02:53.180
without having to back it up sort
of one little state at a time.

00:02:53.180 --> 00:02:55.660
So there's a lot of advantages
here sort of computational and

00:02:55.660 --> 00:02:58.830
otherwise that will hopefully
allow us to learn faster.

00:02:58.830 --> 00:03:01.700
&gt;&gt; Cool, and I think that's
helping me understand why it's

00:03:01.700 --> 00:03:02.750
temporal abstraction, right.

00:03:02.750 --> 00:03:07.210
So state obstruction is treating a whole
bunch of states sort of equivalently.

00:03:07.210 --> 00:03:10.823
This is actually treating a whole bunch
of time steps equivalently, right,

00:03:10.823 --> 00:03:12.580
that can just kind of align over them.

00:03:12.580 --> 00:03:13.690
&gt;&gt; All right, and

00:03:13.690 --> 00:03:17.290
now you'll notice that it will have
the side effect of allowing us to treat

00:03:17.290 --> 00:03:23.050
a bunch of states similarly, because
we know that one way of thinking about

00:03:23.050 --> 00:03:27.500
similarity between states is how their
features are similar to one another.

00:03:27.500 --> 00:03:29.100
That's what you talked about.

00:03:29.100 --> 00:03:33.250
Another way to think about similarity
among states is to think about whether

00:03:33.250 --> 00:03:35.350
you should take the same
action when you're in them.

00:03:35.350 --> 00:03:40.000
And so every single state in this
room is the same in the sense that

00:03:40.000 --> 00:03:42.540
the optimal action is the same,
go to this doorway.

