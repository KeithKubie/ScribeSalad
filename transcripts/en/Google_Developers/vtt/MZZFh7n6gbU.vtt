WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.480
[MUSIC PLAYING]

00:00:03.454 --> 00:00:04.870
ANITHA VIJAYKUMAR:
TensorFlow Lite

00:00:04.870 --> 00:00:06.540
is a lightweight
machine learning

00:00:06.540 --> 00:00:09.630
library for mobile
and embedded devices.

00:00:09.630 --> 00:00:11.580
DAN GALPIN: Hi, I'm Dan
Galpin, and I'm here

00:00:11.580 --> 00:00:13.170
with Anitha Vijaykumar.

00:00:13.170 --> 00:00:15.780
She's a technical program
manager at TensorFlow Lite.

00:00:15.780 --> 00:00:18.030
Your background is
largely in infrastructure.

00:00:18.030 --> 00:00:19.920
How did you make the
jump from infrastructure

00:00:19.920 --> 00:00:21.421
to machine learning
with TensorFlow?

00:00:21.421 --> 00:00:23.586
ANITHA VIJAYKUMAR: So I've
been doing infrastructure

00:00:23.586 --> 00:00:25.170
for a very long
time, and one day I

00:00:25.170 --> 00:00:29.130
heard a talk from Jeff Dean
on how computers can see cats.

00:00:29.130 --> 00:00:31.400
And so this was something
really profound,

00:00:31.400 --> 00:00:34.380
that we enabled machines to
actually have vision and see

00:00:34.380 --> 00:00:35.100
something.

00:00:35.100 --> 00:00:37.149
And so I tried a few
code labs in TensorFlow,

00:00:37.149 --> 00:00:38.940
did a few programming
and machine learning,

00:00:38.940 --> 00:00:40.870
and became interested
and moved in.

00:00:40.870 --> 00:00:43.230
DAN GALPIN: So we already
have TensorFlow Mobile.

00:00:43.230 --> 00:00:46.047
What was the motivation for
developing TensorFlow Lite?

00:00:46.047 --> 00:00:47.880
ANITHA VIJAYKUMAR: This
is a great question.

00:00:47.880 --> 00:00:50.880
TensorFlow Light should
be viewed as an evolution

00:00:50.880 --> 00:00:52.670
of TensorFlow Mobile.

00:00:52.670 --> 00:00:56.100
TensorFlow Lite supports a
subset of operations today,

00:00:56.100 --> 00:00:58.800
and going forward, the
entire mobile support

00:00:58.800 --> 00:01:00.164
should be on TensorFlow Lite.

00:01:00.164 --> 00:01:02.580
DAN GALPIN: How did we decide
which operations to support?

00:01:02.580 --> 00:01:04.709
ANITHA VIJAYKUMAR: So we talked
to a lot of partner teams,

00:01:04.709 --> 00:01:06.792
the mobile vision team,
and then within Google, we

00:01:06.792 --> 00:01:09.090
talked to our Android
partners, and we came up

00:01:09.090 --> 00:01:11.346
with a list of models
that we wanted to support.

00:01:11.346 --> 00:01:12.720
And then we said,
we want to have

00:01:12.720 --> 00:01:15.900
a subset of operants
in the vision space,

00:01:15.900 --> 00:01:18.450
a few models on the text,
and a demo, light model

00:01:18.450 --> 00:01:19.170
on the speech.

00:01:19.170 --> 00:01:23.250
So TensorFlow Lite supported
flavors of these three models,

00:01:23.250 --> 00:01:25.674
and going forward,
we will extend that.

00:01:25.674 --> 00:01:27.090
DAN GALPIN: So how
did we approach

00:01:27.090 --> 00:01:30.510
the problem of improving
load times by comparison

00:01:30.510 --> 00:01:31.477
to full TensorFlow?

00:01:31.477 --> 00:01:33.060
ANITHA VIJAYKUMAR:
So we use something

00:01:33.060 --> 00:01:36.260
called flat buffers, which is
again an open source Google

00:01:36.260 --> 00:01:37.200
initiative.

00:01:37.200 --> 00:01:39.440
They allow for really
fast load times,

00:01:39.440 --> 00:01:42.300
and they don't
need unmarshalling,

00:01:42.300 --> 00:01:45.120
and so using these
techniques helps

00:01:45.120 --> 00:01:47.760
us to ensure that we can
have very low latency

00:01:47.760 --> 00:01:50.100
and load the models faster.

00:01:50.100 --> 00:01:51.750
DAN GALPIN: So
TensorFlow Lite uses

00:01:51.750 --> 00:01:53.890
a model that has been
trained with TensorFlow.

00:01:53.890 --> 00:01:56.340
Now, how does a
TensorFlow model actually

00:01:56.340 --> 00:01:57.909
use in TensorFlow Lite, then?

00:01:57.909 --> 00:02:00.450
ANITHA VIJAYKUMAR: So you start
off with a regular TensorFlow

00:02:00.450 --> 00:02:02.160
model, generate a
model just like you

00:02:02.160 --> 00:02:03.450
would do in normal TensorFlow.

00:02:03.450 --> 00:02:05.360
So have your
GraphDef checkpoints.

00:02:05.360 --> 00:02:07.860
And you go through an additional
step of freezing the graph.

00:02:07.860 --> 00:02:09.985
This is also standard
TensorFlow operation, nothing

00:02:09.985 --> 00:02:11.280
specific to Lite at this point.

00:02:11.280 --> 00:02:13.280
But once you have
a frozen graph,

00:02:13.280 --> 00:02:16.440
once you have a frozen
model, then you convert it,

00:02:16.440 --> 00:02:18.630
you feed it into a
TensorFlow Lite converter,

00:02:18.630 --> 00:02:21.150
and once you've converted to
a TensorFlow Lite converter,

00:02:21.150 --> 00:02:23.160
you get what is
called a .lite file.

00:02:23.160 --> 00:02:26.370
With this .lite file, it can
be fed into an interpreter,

00:02:26.370 --> 00:02:29.400
and using the interpreter, you
can run your model inference

00:02:29.400 --> 00:02:32.400
on the device with
TensorFlow Lite.

00:02:32.400 --> 00:02:36.180
DAN GALPIN: What techniques
can developers use

00:02:36.180 --> 00:02:38.092
to make their models smaller?

00:02:38.092 --> 00:02:40.050
ANITHA VIJAYKUMAR: We
use quantization, mainly.

00:02:40.050 --> 00:02:42.930
So quantization
models are-- usually,

00:02:42.930 --> 00:02:45.120
neural networks are
resistant to noise,

00:02:45.120 --> 00:02:47.160
and so we can get
better computation

00:02:47.160 --> 00:02:50.070
and use smaller memory if
we use 8-bit quantization.

00:02:50.070 --> 00:02:53.136
At the price of accuracy,
paying a price for accuracy.

00:02:53.136 --> 00:02:54.510
So what you could
do is you could

00:02:54.510 --> 00:02:56.970
train the model with
fake quantization nodes,

00:02:56.970 --> 00:03:00.540
like we did with MobileNet,
and get better results when

00:03:00.540 --> 00:03:02.834
you actually run your
inference on the device.

00:03:02.834 --> 00:03:04.500
When we did quantization
and we measured

00:03:04.500 --> 00:03:08.070
the results, what we found
was that MobileNet is about 4x

00:03:08.070 --> 00:03:12.180
smaller than its flawed and
even speed it's about 50%

00:03:12.180 --> 00:03:14.500
faster than the flowed
version of the model.

00:03:14.500 --> 00:03:18.150
So this was really great results
that we saw with quantization.

00:03:18.150 --> 00:03:21.660
DAN GALPIN: And so in terms
of developing TensorFlow Lite,

00:03:21.660 --> 00:03:24.007
what surprised you
about the process?

00:03:24.007 --> 00:03:26.215
ANITHA VIJAYKUMAR: So when
we started the development

00:03:26.215 --> 00:03:28.770
of TensorFlow Lite, we
had an audacious goal

00:03:28.770 --> 00:03:32.400
of hitting a very small
memory size of being

00:03:32.400 --> 00:03:34.860
at least 300k or lower.

00:03:34.860 --> 00:03:36.510
And to date, just
the base interpreter

00:03:36.510 --> 00:03:38.880
is about 70 kilobytes,
and with operators loaded

00:03:38.880 --> 00:03:40.630
is under 300 kilobytes.

00:03:40.630 --> 00:03:42.457
So we are at least
15 times smaller

00:03:42.457 --> 00:03:44.040
than traditional
TensorFlow, and so we

00:03:44.040 --> 00:03:45.209
are really excited about it.

00:03:45.209 --> 00:03:47.250
DAN GALPIN: We also
announced the Neural Networks

00:03:47.250 --> 00:03:48.330
API in Android.

00:03:48.330 --> 00:03:50.270
Now, how does
TensorFlow Lite support

00:03:50.270 --> 00:03:51.434
this kind of acceleration?

00:03:51.434 --> 00:03:52.850
ANITHA VIJAYKUMAR:
So we announced

00:03:52.850 --> 00:03:54.474
it a couple of days
back, beta versions

00:03:54.474 --> 00:03:56.520
of Android neural network API.

00:03:56.520 --> 00:04:00.600
And TensorFlow Lite has
hooks for Neural Network API.

00:04:00.600 --> 00:04:02.130
Using the Neural
Network API, you

00:04:02.130 --> 00:04:05.360
can leverage custom hardware
accelerators on your device.

00:04:05.360 --> 00:04:07.329
So if you have a DSP,
the application developer

00:04:07.329 --> 00:04:09.870
doesn't have to worry about it
and still write his TensorFlow

00:04:09.870 --> 00:04:15.000
Lite program, and if there's
an API and a specific hardware,

00:04:15.000 --> 00:04:16.709
your instructions
can be accelerated

00:04:16.709 --> 00:04:18.005
using that hardware.

00:04:18.005 --> 00:04:19.380
DAN GALPIN: So if
developers want

00:04:19.380 --> 00:04:21.922
to experiment with TensorFlow
Lite, how can they get started?

00:04:21.922 --> 00:04:23.879
ANITHA VIJAYKUMAR: We
would love for developers

00:04:23.879 --> 00:04:25.740
to start experimenting
with TensorFlow Lite.

00:04:25.740 --> 00:04:28.140
The tensorflow.org
has a mobile web page,

00:04:28.140 --> 00:04:29.910
and you can see
it's all revamped.

00:04:29.910 --> 00:04:33.300
And so there's a lot of data
on TensorFlow Lite over there.

00:04:33.300 --> 00:04:35.640
You can also download the
source code from GitHub.

00:04:35.640 --> 00:04:37.530
It's under
contrib/lite currently.

00:04:37.530 --> 00:04:39.360
And there's tons
of documentation.

00:04:39.360 --> 00:04:41.250
There's a model that
you can download.

00:04:41.250 --> 00:04:43.860
There's a demo application
that you can download

00:04:43.860 --> 00:04:45.240
and get started immediately.

00:04:45.240 --> 00:04:46.140
DAN GALPIN: Great.

00:04:46.140 --> 00:04:47.850
Well, thank you so much,
Anitha, for talking to me.

00:04:47.850 --> 00:04:50.308
Once again, this is Dan Galpin,
reporting for the Developer

00:04:50.308 --> 00:04:52.830
Show here from GDD India.

00:04:52.830 --> 00:04:54.810
Thanks for watching
the Developer Show.

00:04:54.810 --> 00:04:57.660
Catch up on the TL;DR and
additional interviews from GDD

00:04:57.660 --> 00:04:59.010
India right over here.

00:04:59.010 --> 00:05:01.760
[MUSIC PLAYING]

