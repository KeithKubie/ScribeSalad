WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:01.730
Alright Michael, so there's lots of definitions

00:00:01.730 --> 00:00:03.230
of game theory that we could use.

00:00:03.230 --> 00:00:04.790
One that I like in particular is

00:00:04.790 --> 00:00:08.109
that game theory is the mathematics of conflict.

00:00:08.109 --> 00:00:09.435
&gt;&gt; Hm, [CROSSTALK] that's interesting.

00:00:09.435 --> 00:00:11.727
&gt;&gt; I think it's kind of interesting. Or generally it's the

00:00:11.727 --> 00:00:16.218
mathematics of conflicts of interest when trying to make optimal choices.

00:00:16.218 --> 00:00:17.082
&gt;&gt; because I feel like a lot

00:00:17.082 --> 00:00:19.610
of people have their own conflicts with mathematics.

00:00:19.610 --> 00:00:21.310
&gt;&gt; I think everyone but mathematicians have

00:00:21.310 --> 00:00:22.990
their conflicts with mathematics. I think that's fair.

00:00:22.990 --> 00:00:23.080
&gt;&gt; I see.

00:00:23.080 --> 00:00:25.460
&gt;&gt; But do you see if you, can you see

00:00:25.460 --> 00:00:28.760
how worrying about the mathematical conflict might be a sort of

00:00:28.760 --> 00:00:31.870
natural next thing to think about after you've learned a lot

00:00:31.870 --> 00:00:35.020
about reinforcement learning? I guess then well the next bullet kind

00:00:35.020 --> 00:00:38.220
of, kind of suggests a trend. So, so we've been talking about

00:00:38.220 --> 00:00:41.530
decision making and it's almost always in the context of a

00:00:41.530 --> 00:00:44.030
single agent that lives in a world and it's trying to maximize

00:00:44.030 --> 00:00:47.750
reward. But that's kind of a lonely way to think about

00:00:47.750 --> 00:00:50.800
things, so what if there's other agents in the world with you?

00:00:50.800 --> 00:00:53.200
&gt;&gt; Right and of course evidence suggests that there are in

00:00:53.200 --> 00:00:56.370
fact other agents in the world with you. And what we've been

00:00:56.370 --> 00:00:59.140
doing with reinforcement learning which, you know, has worked out very well

00:00:59.140 --> 00:01:01.780
for us, is we've been mostly pretending that those other agents are

00:01:01.780 --> 00:01:04.360
just a part of the environment. Somehow all the stuff that

00:01:04.360 --> 00:01:08.160
the other agents do is hidden inside of the transition model. But

00:01:08.160 --> 00:01:11.970
truthfully it probably makes sense if you want to make optimal decisions

00:01:11.970 --> 00:01:15.990
to try to take into account explicitly the desires and the goals

00:01:15.990 --> 00:01:18.710
of all the other agents in the world with you. Does that seem fair?

00:01:18.710 --> 00:01:19.640
&gt;&gt; Yeah.

00:01:19.640 --> 00:01:22.140
&gt;&gt; Right. So that's what game theory helps us to do

00:01:22.140 --> 00:01:23.980
and then at the very end I think we'll, we'll be

00:01:23.980 --> 00:01:27.910
able to tie what we're going to learn Directly back into the

00:01:27.910 --> 00:01:31.600
reinforcement learning that we've done and even into the Bellman equation.

00:01:31.600 --> 00:01:32.600
&gt;&gt; Oh, okay, nice.

00:01:32.600 --> 00:01:34.960
&gt;&gt; Yeah, so that is going to work out pretty well but,

00:01:34.960 --> 00:01:36.620
but we have to get there first and there's a lot

00:01:36.620 --> 00:01:38.290
of stuff that we have to do to get there. But

00:01:38.290 --> 00:01:41.020
right now what I want you to think about is this notion

00:01:41.020 --> 00:01:45.000
that, we're going to move from reinforcement learning world of single

00:01:45.000 --> 00:01:48.430
agents to a game theory world of multiple agents and tie

00:01:48.430 --> 00:01:51.133
it all back back together. It's a sort of general

00:01:51.133 --> 00:01:54.543
note that I think that, that's worthwhile is that, game theory

00:01:54.543 --> 00:01:57.031
sort of comes out of economics. And then in fact,

00:01:57.031 --> 00:01:59.931
if you think about multiple agents there being millions and millions

00:01:59.931 --> 00:02:03.722
of multiple agents, in some sense that's economics. Right? Economics is

00:02:03.722 --> 00:02:06.059
kind of the math, and the science, and the art of

00:02:06.059 --> 00:02:09.251
thinking about what happens when there are, lots, and lots, and

00:02:09.251 --> 00:02:13.127
lots, and lots of people with their own goals possibility conflicting, trying

00:02:13.127 --> 00:02:16.460
to work together to accomplish something, right. And so what game

00:02:16.460 --> 00:02:19.180
theory does, is it gives us mathematical tools to think about that.

00:02:19.180 --> 00:02:22.340
&gt;&gt; I feel like I feel like other fields

00:02:22.340 --> 00:02:25.170
would care about some of these things too, like sociology.

00:02:25.170 --> 00:02:25.844
&gt;&gt; Right.

00:02:25.844 --> 00:02:27.812
&gt;&gt; And what about, I could kind

00:02:27.812 --> 00:02:31.560
of imagine biology caring about these things, too.

00:02:31.560 --> 00:02:33.390
&gt;&gt; Even biology, I like the idea of

00:02:33.390 --> 00:02:37.080
biology. Biology. Why would biology care about this?

00:02:37.080 --> 00:02:38.460
&gt;&gt; Well, I guess the way you described it

00:02:38.460 --> 00:02:42.010
in terms of lots of individual agents that are interacting.

00:02:42.010 --> 00:02:45.180
Like, you know, creatures that live and reproduce. I

00:02:45.180 --> 00:02:47.350
feel like they, they have some of those same issues.

00:02:47.350 --> 00:02:52.490
&gt;&gt; Sure. So certainly biology at at the level of entities, at the level of

00:02:52.490 --> 00:02:56.450
mammals or level of insects, you might be able to think about it that way.

00:02:56.450 --> 00:02:58.560
But perhaps even at the level of genes and at the level of

00:02:58.560 --> 00:03:01.380
cells. Little virii and, and bacteria. You

00:03:01.380 --> 00:03:03.440
could possibly think about it that way.

00:03:03.440 --> 00:03:05.470
&gt;&gt; because they're in conflict too, I guess.

00:03:05.470 --> 00:03:07.920
&gt;&gt; Yeah. Now there's probably this notion of

00:03:07.920 --> 00:03:10.170
intention. It's not entirely clear what that means here

00:03:10.170 --> 00:03:11.900
and I think implicit in the notion of

00:03:11.900 --> 00:03:13.800
what we're doing here is this notion of intention

00:03:13.800 --> 00:03:16.490
and explicit goals as opposed to ones that

00:03:16.490 --> 00:03:18.300
are kind of built into your genes, but I think

00:03:18.300 --> 00:03:19.590
that's a perfectly reasonable way of thinking about

00:03:19.590 --> 00:03:21.590
it. I think the, the lesson from this discussion

00:03:21.590 --> 00:03:25.220
though is that. What game theory sort of captures for us or

00:03:25.220 --> 00:03:28.590
what would like for it to capture for us, is ways of thinking

00:03:28.590 --> 00:03:32.560
about what happens when you're not the only thing with intention in

00:03:32.560 --> 00:03:36.070
the world and how do you incorporate other goals from other people who

00:03:36.070 --> 00:03:38.450
might not have your best interest at heart or might have your

00:03:38.450 --> 00:03:40.830
best interest at heart. How do you make that work? And so if

00:03:40.830 --> 00:03:43.510
you think about that problem then I think it makes sense that it's

00:03:43.510 --> 00:03:47.630
been increasingly a part of AI over the years, and in some ways

00:03:47.630 --> 00:03:49.430
machine learning has started to think of it

00:03:49.430 --> 00:03:51.800
as being a mainstream part of what we do.

00:03:51.800 --> 00:03:52.860
&gt;&gt; Cool.

00:03:52.860 --> 00:03:56.480
&gt;&gt; Hence, why it's worth talking about today. Okay. Sound good.

00:03:56.480 --> 00:03:57.580
&gt;&gt; Gotcha.

00:03:57.580 --> 00:03:59.050
&gt;&gt; Okay. Let's, let's try to make this

00:03:59.050 --> 00:04:01.960
concrete with a very simple sort of example.

