WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:08.892
[MUSIC PLAYING]

00:00:08.892 --> 00:00:11.210
MALE SPEAKER: Shanghai
GDG is a very

00:00:11.210 --> 00:00:13.260
interesting developer community.

00:00:13.260 --> 00:00:13.750
FEMALE SPEAKER: I'm
glad somebody

00:00:13.750 --> 00:00:15.140
has asked this question.

00:00:15.140 --> 00:00:16.480
MALE SPEAKER: This is where
the magic happens.

00:00:16.480 --> 00:00:19.190
FEMALE SPEAKER: This is
primarily a question and

00:00:19.190 --> 00:00:20.010
answer show, so if any
of you out there

00:00:20.010 --> 00:00:21.260
would like to ask questions.

00:00:25.770 --> 00:00:27.050
RYAN BOYD: Hi, everyone.

00:00:27.050 --> 00:00:28.640
Welcome to the Ryan
and Michael Show.

00:00:28.640 --> 00:00:29.710
I'm Ryan Boyd.

00:00:29.710 --> 00:00:31.060
MICHAEL MANOOCHEHRI: And I'm
Michael Manoochehri.

00:00:31.060 --> 00:00:33.590
RYAN BOYD: And we're here today
to talk about preparing

00:00:33.590 --> 00:00:36.730
your data for BigQuery.

00:00:36.730 --> 00:00:39.200
So if you have any questions
throughout this little

00:00:39.200 --> 00:00:41.490
discussion on preparing your
data and loading your data

00:00:41.490 --> 00:00:46.930
into BigQuery, you can feel free
to ask them in the Google

00:00:46.930 --> 00:00:51.070
Moderator that is available on
developers.google/com/live.

00:00:51.070 --> 00:00:53.150
Hopefully, you're watching
it there.

00:00:53.150 --> 00:00:55.460
And you can ask your questions,
and we'll save some

00:00:55.460 --> 00:00:57.490
time for the end
to answer them.

00:00:57.490 --> 00:01:00.760
Of course, if you have any other
questions after today or

00:01:00.760 --> 00:01:04.819
you're not watching it live, you
can ask those questions on

00:01:04.819 --> 00:01:06.670
Stack Overflow, and we'll
answer them there.

00:01:06.670 --> 00:01:10.500
Or reach out to Michael or
myself on our Google+ handles

00:01:10.500 --> 00:01:12.170
that you see on this page.

00:01:12.170 --> 00:01:14.640
So looking forward to
hearing from you.

00:01:14.640 --> 00:01:17.860
Now let's dive in.

00:01:17.860 --> 00:01:20.410
So the first thing we want to
talk about is the data format.

00:01:20.410 --> 00:01:22.855
What does your data need to look
like when you load your

00:01:22.855 --> 00:01:25.260
data into BigQuery?

00:01:25.260 --> 00:01:26.680
It's pretty simple.

00:01:26.680 --> 00:01:29.290
It's a CSV-formatted file.

00:01:29.290 --> 00:01:32.130
So here we can see three
lines of a CSV file.

00:01:32.130 --> 00:01:37.120
This is actually data
representing the--

00:01:37.120 --> 00:01:37.350
sorry.

00:01:37.350 --> 00:01:39.010
Michael, do you remember
what this data was?

00:01:39.010 --> 00:01:39.330
[LAUGHTER]

00:01:39.330 --> 00:01:39.600
RYAN BOYD: Sorry.

00:01:39.600 --> 00:01:40.110
It's been a little bit.

00:01:40.110 --> 00:01:40.500
MICHAEL MANOOCHEHRI: Yeah.

00:01:40.500 --> 00:01:41.160
That's right.

00:01:41.160 --> 00:01:42.990
It looks like this might be
election data or maybe

00:01:42.990 --> 00:01:43.840
natality data.

00:01:43.840 --> 00:01:44.990
RYAN BOYD: Yeah, exactly.

00:01:44.990 --> 00:01:49.480
So three rows of data
in a CSV file.

00:01:49.480 --> 00:01:52.210
And we can see some of
our basic types here.

00:01:52.210 --> 00:01:53.200
We have a strings.

00:01:53.200 --> 00:01:54.310
We have ints.

00:01:54.310 --> 00:01:55.280
We have floats.

00:01:55.280 --> 00:01:56.730
And we have Boolean values.

00:01:56.730 --> 00:01:58.060
Those are the main
types that you're

00:01:58.060 --> 00:02:00.930
used to using in BigQuery.

00:02:00.930 --> 00:02:04.910
And you just format a CSV file,
new line delimited.

00:02:04.910 --> 00:02:06.820
You can quote your strings
and things like

00:02:06.820 --> 00:02:08.830
that, but basic types.

00:02:08.830 --> 00:02:12.180
And our goal through the rest
of the discussion here today

00:02:12.180 --> 00:02:14.790
is how to prepare these
CSV files and

00:02:14.790 --> 00:02:19.060
get these into BigQuery.

00:02:19.060 --> 00:02:21.700
Now, we have developers
using a variety of

00:02:21.700 --> 00:02:23.200
different data sources.

00:02:23.200 --> 00:02:26.860
How you prepare your data and
load it into BigQuery often

00:02:26.860 --> 00:02:30.510
really depends on your data
source that you're using.

00:02:30.510 --> 00:02:33.660
So let's talk about the
types of data sources

00:02:33.660 --> 00:02:35.960
that BigQuery users--

00:02:35.960 --> 00:02:37.910
our developers or customers--

00:02:37.910 --> 00:02:42.830
are using to load
into BigQuery.

00:02:42.830 --> 00:02:45.780
It's all sort of transactional
data.

00:02:45.780 --> 00:02:48.510
So the data sources commonly
used would make sense in a

00:02:48.510 --> 00:02:51.270
BigQuery-style database are
transactional data, things

00:02:51.270 --> 00:02:56.290
where the append-only style of
BigQuery works out well.

00:02:56.290 --> 00:02:59.210
So web logs is one of
the most popular.

00:02:59.210 --> 00:03:03.480
The standard Apache web blog,
various fields representing

00:03:03.480 --> 00:03:06.050
information about the user
that's accessing the site,

00:03:06.050 --> 00:03:07.580
what page that they're
accessing, the

00:03:07.580 --> 00:03:09.380
host, things like that.

00:03:09.380 --> 00:03:13.300
That type of data is fantastic
for BigQuery.

00:03:13.300 --> 00:03:16.730
And those web logs can be things
that are on your own

00:03:16.730 --> 00:03:19.880
local web server in your
local data center.

00:03:19.880 --> 00:03:22.950
Or it can also be cloud-based
web logs.

00:03:22.950 --> 00:03:24.920
So App Engine logs or logs from

00:03:24.920 --> 00:03:27.760
other cloud-based platforms.

00:03:27.760 --> 00:03:29.960
So that's one of the
most common.

00:03:29.960 --> 00:03:33.580
But then you might actually have
some more specific types

00:03:33.580 --> 00:03:38.330
of log files for mobile or
web application events.

00:03:38.330 --> 00:03:41.580
So, for instance, on the Google
Play Store, there are a

00:03:41.580 --> 00:03:43.170
number of applications there.

00:03:43.170 --> 00:03:47.170
One of the top applications on
the Google Play Store actually

00:03:47.170 --> 00:03:51.590
uses BigQuery to run
a lot of analysis.

00:03:51.590 --> 00:03:54.040
And I'm not sure
what particular

00:03:54.040 --> 00:03:55.450
data they're running.

00:03:55.450 --> 00:03:59.970
But web application events and
mobile application events is a

00:03:59.970 --> 00:04:04.070
really great type of data to
load into BigQuery and analyze

00:04:04.070 --> 00:04:05.960
with BigQuery.

00:04:05.960 --> 00:04:09.900
And you could often get lots of
traffic on the initial day

00:04:09.900 --> 00:04:12.790
you launch on the Play Store.

00:04:12.790 --> 00:04:15.990
And you can potentially get a
lot of customers immediately.

00:04:15.990 --> 00:04:18.950
And you want to be able to track
how they're using your

00:04:18.950 --> 00:04:22.100
product and going to use that
information to improve your

00:04:22.100 --> 00:04:25.110
application that you're
offering.

00:04:25.110 --> 00:04:28.890
Syslog-style data for
machine performance.

00:04:28.890 --> 00:04:32.730
The range of different use
cases that developers use

00:04:32.730 --> 00:04:35.750
BigQuery for is very diverse.

00:04:35.750 --> 00:04:39.580
So we have people using it for
ad monetization, but also

00:04:39.580 --> 00:04:41.470
people using it to monitor
their servers.

00:04:41.470 --> 00:04:44.170
And that's what this one is
about-- syslog-style data

00:04:44.170 --> 00:04:46.370
monitoring machine
performance.

00:04:46.370 --> 00:04:48.780
We actually have a case study
that should be out soon that

00:04:48.780 --> 00:04:51.740
really talks about how one of
our developers uses it to

00:04:51.740 --> 00:04:54.370
monitor the performance of their
applications and the

00:04:54.370 --> 00:04:56.850
machines that are hosting
those applications.

00:04:56.850 --> 00:04:59.080
Then, of course, were Google
advertising click and

00:04:59.080 --> 00:05:02.880
impression data, Sales
transaction records, all very

00:05:02.880 --> 00:05:05.760
important types of transactional
data that is

00:05:05.760 --> 00:05:09.480
great to load into BigQuery
for analysis.

00:05:09.480 --> 00:05:12.650
And the last one I mention here
is App Engine data store.

00:05:12.650 --> 00:05:16.380
So oftentimes, you may have
sensors in machines all around

00:05:16.380 --> 00:05:19.480
the world, and you want to load
that data into BigQuery.

00:05:19.480 --> 00:05:23.390
It's best to use the cloud to
really collect that data, and

00:05:23.390 --> 00:05:24.890
App Engine's a great
way to do that.

00:05:24.890 --> 00:05:28.370
So you can collect the data in
App Engine, put it into the

00:05:28.370 --> 00:05:31.420
data store, and then take the
data out of the data store,

00:05:31.420 --> 00:05:33.490
and load it into BigQuery
in batches.

00:05:33.490 --> 00:05:36.660
And we'll talk about how we've
used some of our Google

00:05:36.660 --> 00:05:40.770
technologies for doing
that here shortly.

00:05:40.770 --> 00:05:45.010
So now I want to talk about the
process of loading data

00:05:45.010 --> 00:05:46.850
into BigQuery.

00:05:46.850 --> 00:05:49.190
The process of loading data
is pretty simple.

00:05:49.190 --> 00:05:52.540
Your data is loaded by preparing
your data, uploading

00:05:52.540 --> 00:05:57.130
it to Google Cloud Storage, and
then from Cloud Storage,

00:05:57.130 --> 00:06:00.610
you're basically making a API
call to BigQuery and telling

00:06:00.610 --> 00:06:02.750
it to ingest the data
from Cloud Storage.

00:06:02.750 --> 00:06:05.000
It's a fairly simple process.

00:06:05.000 --> 00:06:07.940
I will say that you can
technically skip the middle

00:06:07.940 --> 00:06:10.820
part of this process here
and upload your

00:06:10.820 --> 00:06:12.980
data directly to BigQuery.

00:06:12.980 --> 00:06:16.200
We generally don't recommend
it because Cloud Storage is

00:06:16.200 --> 00:06:17.380
really optimized.

00:06:17.380 --> 00:06:20.230
All the APIs and tools around
Cloud Storage are really

00:06:20.230 --> 00:06:23.180
optimized for handling
large files.

00:06:23.180 --> 00:06:25.330
And so that's often the
great way to do it.

00:06:25.330 --> 00:06:28.570
You get your large files into
Cloud Storage, and then you

00:06:28.570 --> 00:06:31.660
let BigQuery retrieve them from
Cloud Storage as part of

00:06:31.660 --> 00:06:33.500
its ingestion process.

00:06:33.500 --> 00:06:37.470
And that's the least error-prone
way of doing this.

00:06:37.470 --> 00:06:40.450
And so we've had developers
try to upload their files

00:06:40.450 --> 00:06:43.190
directly to BigQuery, and
sometimes they just run into

00:06:43.190 --> 00:06:44.780
some issues.

00:06:44.780 --> 00:06:47.230
And those issues are usually
because they're uploading very

00:06:47.230 --> 00:06:50.180
large files, and they're trying
to make a plain HTTP

00:06:50.180 --> 00:06:54.410
request to upload those large
files and not taking advantage

00:06:54.410 --> 00:06:57.640
of our resumable upload
feature that we have.

00:06:57.640 --> 00:07:00.580
And the resumable upload
feature, if you're using the

00:07:00.580 --> 00:07:02.690
resumable upload feature, I
think you should be able to

00:07:02.690 --> 00:07:06.090
about the same reliability, even
if you skip Cloud Storage

00:07:06.090 --> 00:07:09.990
in this process, uploading
directly to BigQuery.

00:07:09.990 --> 00:07:10.520
Try it out.

00:07:10.520 --> 00:07:12.490
Let us know what
you're seeing.

00:07:12.490 --> 00:07:16.450
And we'd be happy to know if
there's great success in using

00:07:16.450 --> 00:07:20.500
that resumable upload to upload
directly to BigQuery.

00:07:20.500 --> 00:07:22.880
Our libraries support this.

00:07:22.880 --> 00:07:26.960
And a lot of the libraries
support the ability to do

00:07:26.960 --> 00:07:28.560
resumable uploads.

00:07:28.560 --> 00:07:31.110
We don't yet have it super
well-documented on the

00:07:31.110 --> 00:07:31.730
BigQuery side.

00:07:31.730 --> 00:07:32.700
We're working on that.

00:07:32.700 --> 00:07:35.180
But you can see the generic
library documentation on how

00:07:35.180 --> 00:07:38.400
to do resumable uploads.

00:07:38.400 --> 00:07:40.565
Now, the next thing we wanted to
talk about-- and I'm going

00:07:40.565 --> 00:07:42.810
to turn over to Michael for
this-- is chunking your data,

00:07:42.810 --> 00:07:47.360
breaking up your data into the
parts required for ingesting

00:07:47.360 --> 00:07:48.370
it into BigQuery.

00:07:48.370 --> 00:07:48.570
MICHAEL MANOOCHEHRI: Yeah.

00:07:48.570 --> 00:07:49.240
So let's dive into it.

00:07:49.240 --> 00:07:51.950
So as you know, BigQuery is used
for analyzing or asking

00:07:51.950 --> 00:07:54.420
questions about massive
data sets.

00:07:54.420 --> 00:07:56.900
But to get your data into
BigQuery, it's good to chunk

00:07:56.900 --> 00:07:57.900
it into parts.

00:07:57.900 --> 00:08:01.530
So BigQuery supports ingestion
or loading of both compressed,

00:08:01.530 --> 00:08:04.230
with the gzip, or uncompressed
files.

00:08:04.230 --> 00:08:07.430
Right now, the maximum size of
each file is 4 gigabytes.

00:08:07.430 --> 00:08:09.780
But you can ingest large
batches of files.

00:08:09.780 --> 00:08:13.990
You can ingest up to 500 files
in one load job as long as the

00:08:13.990 --> 00:08:16.920
total job is under
100 gigabytes.

00:08:16.920 --> 00:08:19.680
So anecdotally, I've run some
tests using some of the data

00:08:19.680 --> 00:08:20.790
that we have internally.

00:08:20.790 --> 00:08:23.610
And I've been able to ingest
upwards of 300 gigabytes of

00:08:23.610 --> 00:08:27.430
uncompressed data in one load
job by first gzipping that

00:08:27.430 --> 00:08:30.840
data and then ingesting it
into a single batch.

00:08:30.840 --> 00:08:32.590
So you can actually put
in a lot of data

00:08:32.590 --> 00:08:33.779
in one batch ingestion.

00:08:33.779 --> 00:08:35.100
This is another advantage
to using the

00:08:35.100 --> 00:08:37.159
Cloud Storage for ingesting.

00:08:37.159 --> 00:08:38.539
You can ingest giant
batches that are

00:08:38.539 --> 00:08:40.220
stored in Cloud Storage.

00:08:40.220 --> 00:08:41.380
So it's really great.

00:08:41.380 --> 00:08:42.780
Let us deal with the
infrastructure.

00:08:42.780 --> 00:08:45.230
I think that's usually
a good lesson.

00:08:45.230 --> 00:08:47.140
And so let's talk about speeding
up the data loading.

00:08:47.140 --> 00:08:49.270
So I just told you that you
could ingest an enormous

00:08:49.270 --> 00:08:51.290
amount of data in
one load job.

00:08:51.290 --> 00:08:52.850
There's other ways that you
can even improve this

00:08:52.850 --> 00:08:53.860
ingestion rate.

00:08:53.860 --> 00:08:56.730
We actually let you ingest
two concurrent

00:08:56.730 --> 00:08:57.960
ingestion jobs at a time.

00:08:57.960 --> 00:09:00.720
And so you can have two-- these
huge batches-- two of

00:09:00.720 --> 00:09:02.856
them in flight at
a single time.

00:09:02.856 --> 00:09:04.790
And so you can do that,
two concurrent.

00:09:04.790 --> 00:09:07.340
And we can also allow you
to do 1,000 per day.

00:09:07.340 --> 00:09:08.580
So you can do the math.

00:09:08.580 --> 00:09:11.530
If you're ingesting hundreds of
gigabytes of data, you can

00:09:11.530 --> 00:09:14.250
ingest 1,000 of them a day.

00:09:14.250 --> 00:09:17.230
And so you can get a massive
amount of data ingested into

00:09:17.230 --> 00:09:19.276
BigQuery in one go.

00:09:19.276 --> 00:09:22.980
RYAN BOYD: Now, some people try
with that 1,000 per day to

00:09:22.980 --> 00:09:24.220
stream data in.

00:09:24.220 --> 00:09:25.112
MICHAEL MANOOCHEHRI: Right.

00:09:25.112 --> 00:09:29.000
RYAN BOYD: And generally,
BigQuery is not designed right

00:09:29.000 --> 00:09:30.960
now for streaming-style
ingestion.

00:09:30.960 --> 00:09:33.910
You can still upload and do many
ingestion jobs per hour

00:09:33.910 --> 00:09:37.030
and have sort of near
real-time data.

00:09:37.030 --> 00:09:38.360
But you're not going
to get down to the

00:09:38.360 --> 00:09:39.660
second data quite yet.

00:09:39.660 --> 00:09:41.450
If it's something you're
interested in-- yeah,

00:09:41.450 --> 00:09:42.020
definitely let us know.

00:09:42.020 --> 00:09:43.250
MICHAEL MANOOCHEHRI: Yeah, it's
still a batch process.

00:09:43.250 --> 00:09:43.860
We'd love to hear it.

00:09:43.860 --> 00:09:45.230
Actually, Ryan touched
on this earlier.

00:09:45.230 --> 00:09:47.280
Some people are using the data
store to collect data.

00:09:47.280 --> 00:09:50.050
So that's a very highly
available, performant way to

00:09:50.050 --> 00:09:52.870
collect data in real time and
then using a batch job from

00:09:52.870 --> 00:09:54.290
there to put the data
into BigQuery.

00:09:54.290 --> 00:09:56.120
So there's other ways
you can handle that.

00:09:56.120 --> 00:09:59.520
RYAN BOYD: And one thing I will
say on the job style,

00:09:59.520 --> 00:10:01.680
we've been asked this question
a number of times on Stack

00:10:01.680 --> 00:10:06.180
Overflow and other things, is
whether when the job fails--

00:10:06.180 --> 00:10:09.210
in some cases, you have
a misformatted line or

00:10:09.210 --> 00:10:10.440
something like that.

00:10:10.440 --> 00:10:15.350
If the job fails, does it
ever partially complete?

00:10:15.350 --> 00:10:16.570
And the answer is no.

00:10:16.570 --> 00:10:20.740
When a job fails with BigQuery,
that job is

00:10:20.740 --> 00:10:22.280
completely failed.

00:10:22.280 --> 00:10:23.780
We haven't updated
your data at all.

00:10:23.780 --> 00:10:26.380
So you just retry that job
without ever having to worry

00:10:26.380 --> 00:10:27.970
about potential error
conditions or

00:10:27.970 --> 00:10:30.130
anything along the way.

00:10:30.130 --> 00:10:33.410
And one other thing, actually,
I should say that we kind of

00:10:33.410 --> 00:10:35.970
forgot as we were preparing for
this but I think is really

00:10:35.970 --> 00:10:39.460
important is, at times, your
data's not perfect.

00:10:39.460 --> 00:10:42.570
You're loading data in, and it's
not absolutely perfect.

00:10:42.570 --> 00:10:44.900
There's some formatting issues
or what have you.

00:10:44.900 --> 00:10:48.630
You can actually specify kind
of your error tolerance when

00:10:48.630 --> 00:10:49.970
you load data into BigQuery.

00:10:49.970 --> 00:10:50.520
MICHAEL MANOOCHEHRI: Right.

00:10:50.520 --> 00:10:55.020
RYAN BOYD: Either using the
BigQ client or using the

00:10:55.020 --> 00:10:58.220
client libraries when you create
your job JSON object.

00:10:58.220 --> 00:11:02.110
You can basically say I can have
up to, say, 50 lines of

00:11:02.110 --> 00:11:03.080
data that are bad.

00:11:03.080 --> 00:11:03.450
MICHAEL MANOOCHEHRI: Yeah.

00:11:03.450 --> 00:11:05.340
Misformatted or have
bad encoding or

00:11:05.340 --> 00:11:05.710
something like that.

00:11:05.710 --> 00:11:06.680
RYAN BOYD: Yeah, exactly.

00:11:06.680 --> 00:11:09.050
So often, you'll want to kind
of do that as your first

00:11:09.050 --> 00:11:10.200
experimenting.

00:11:10.200 --> 00:11:12.410
You'll want to set a little bit
higher threshold as your

00:11:12.410 --> 00:11:13.220
first experiment.

00:11:13.220 --> 00:11:16.390
And as you get more and more
data into BigQuery and you get

00:11:16.390 --> 00:11:18.730
confidence in your data format
and your collection

00:11:18.730 --> 00:11:21.840
techniques, then you
can actually change

00:11:21.840 --> 00:11:22.700
that and back off.

00:11:22.700 --> 00:11:23.630
MICHAEL MANOOCHEHRI: When you're
dealing with huge file

00:11:23.630 --> 00:11:25.990
sizes, when you're dealing
terabyte file sizes or

00:11:25.990 --> 00:11:28.720
gigabyte files sizes, the
chances of having a bad line

00:11:28.720 --> 00:11:31.070
because of something that
your output writer

00:11:31.070 --> 00:11:32.470
dealt with is high.

00:11:32.470 --> 00:11:34.150
So this is a great
feature for that.

00:11:34.150 --> 00:11:34.740
RYAN BOYD: Yeah.

00:11:34.740 --> 00:11:35.390
Exactly.

00:11:35.390 --> 00:11:37.295
So fantastic.

00:11:37.295 --> 00:11:38.070
MICHAEL MANOOCHEHRI: Cool.

00:11:38.070 --> 00:11:41.370
So let's talk about designing
a BigQuery schema.

00:11:41.370 --> 00:11:43.260
So if you're from the relational
database world,

00:11:43.260 --> 00:11:46.960
you're probably used to what we
call normalization of data,

00:11:46.960 --> 00:11:49.770
which is to keep everything in
one place and just have one

00:11:49.770 --> 00:11:50.440
copy of it.

00:11:50.440 --> 00:11:53.870
So an example we have here, this
is a relational database

00:11:53.870 --> 00:11:56.960
of two tables, one with parents
and one with a person.

00:11:56.960 --> 00:11:58.800
So a person generally
has two parents.

00:11:58.800 --> 00:12:03.110
And in this type schema,
the parents will have

00:12:03.110 --> 00:12:04.490
a key like an ID.

00:12:04.490 --> 00:12:06.630
And basically all the data
will be in one place.

00:12:06.630 --> 00:12:08.530
So your birth record will
be in one place.

00:12:08.530 --> 00:12:11.430
And your parents will be a
single record in a parents

00:12:11.430 --> 00:12:13.060
table, your father and mother.

00:12:13.060 --> 00:12:14.740
So this is a normalized table.

00:12:14.740 --> 00:12:17.940
So BigQuery likes to have
denormalized data,

00:12:17.940 --> 00:12:18.980
non-relational data.

00:12:18.980 --> 00:12:22.970
So the idea will be to have
a record with every bit of

00:12:22.970 --> 00:12:24.310
information flattened.

00:12:24.310 --> 00:12:26.770
So all the data that you saw
on the other table will be

00:12:26.770 --> 00:12:28.600
flattened into a
single record.

00:12:28.600 --> 00:12:32.640
So, in practice, this means
there can be redundancy and

00:12:32.640 --> 00:12:35.050
can be null values, what
we call sparsity.

00:12:35.050 --> 00:12:36.870
And this is the kind
of table that

00:12:36.870 --> 00:12:38.750
BigQuery actually accepts.

00:12:38.750 --> 00:12:41.590
So when you have relational
data, what you want to do is

00:12:41.590 --> 00:12:45.160
flatten it out into
a single record

00:12:45.160 --> 00:12:46.905
denormalized table for BigQuery.

00:12:46.905 --> 00:12:50.240
RYAN BOYD: Actually, I want to
say in terms of the advantages

00:12:50.240 --> 00:12:54.990
or the reasons you're taught in
school to always normalize

00:12:54.990 --> 00:12:57.760
your data into the multiple
tables that Michael just

00:12:57.760 --> 00:13:00.710
talked about here, one of the
real advantages of that is

00:13:00.710 --> 00:13:03.230
just maintaining consistency
of your data.

00:13:03.230 --> 00:13:07.230
So if you update information
about a parent in this case,

00:13:07.230 --> 00:13:10.760
that all their children see
the same parent metadata.

00:13:10.760 --> 00:13:14.410
The parent change their age
or something like that.

00:13:14.410 --> 00:13:18.750
All of the children records
reflect that, and you never

00:13:18.750 --> 00:13:20.880
have inconsistent
sources there.

00:13:20.880 --> 00:13:25.030
But with a BigQuery-style
database, where it's sort of

00:13:25.030 --> 00:13:28.990
an append-only transactional
data and you're really trying

00:13:28.990 --> 00:13:32.910
to do an analysis, the
consistency isn't a problem

00:13:32.910 --> 00:13:35.180
there because you've had the
consistency through the life

00:13:35.180 --> 00:13:37.290
cycle of the data as the
data's being created.

00:13:37.290 --> 00:13:42.220
But then as you're analyzing it,
the flat structure is much

00:13:42.220 --> 00:13:43.290
better for performance.

00:13:43.290 --> 00:13:48.510
And you don't lose anything by
not having that consistency.

00:13:48.510 --> 00:13:49.095
MICHAEL MANOOCHEHRI: Right.

00:13:49.095 --> 00:13:49.780
That's great.

00:13:49.780 --> 00:13:54.090
So building off that, another
technique for BigQuery is to

00:13:54.090 --> 00:13:55.390
actually shard your data.

00:13:55.390 --> 00:13:57.180
So by taking, say--

00:13:57.180 --> 00:14:00.020
in this example, we're taking
birth records from different

00:14:00.020 --> 00:14:01.960
years and placing them into
different tables.

00:14:01.960 --> 00:14:03.610
There's a lot of advantages
to doing this.

00:14:03.610 --> 00:14:07.170
First, for example, BigQuery
actually is priced by the

00:14:07.170 --> 00:14:08.190
amount of data you query.

00:14:08.190 --> 00:14:10.660
So by sharding your data, you
can actually make your queries

00:14:10.660 --> 00:14:12.770
as efficiently priced
as possible.

00:14:12.770 --> 00:14:14.750
This is also a good way to just
make your queries more

00:14:14.750 --> 00:14:15.520
manageable.

00:14:15.520 --> 00:14:18.970
So you can say I want to deal
with data from 2011 and have

00:14:18.970 --> 00:14:21.020
the data sharded there.

00:14:21.020 --> 00:14:23.100
So this is a great way logically
to organize your

00:14:23.100 --> 00:14:25.920
data into tables, into these
kind of denormalized flattened

00:14:25.920 --> 00:14:27.560
tables that we talked about.

00:14:27.560 --> 00:14:29.790
RYAN BOYD: And, of course,
if you're trying to query

00:14:29.790 --> 00:14:32.080
multiple years of data, you
can just do simple union

00:14:32.080 --> 00:14:32.620
queries to do that.

00:14:32.620 --> 00:14:33.500
MICHAEL MANOOCHEHRI: Exactly.

00:14:33.500 --> 00:14:35.520
So we support that
as well, yeah.

00:14:35.520 --> 00:14:36.260
So great.

00:14:36.260 --> 00:14:38.030
So we talked to you a little
bit about designing these

00:14:38.030 --> 00:14:39.820
tables and how to ingest data.

00:14:39.820 --> 00:14:41.740
But let's talk about tools
for data preparation.

00:14:41.740 --> 00:14:43.185
What are the actual tools
you can use to get

00:14:43.185 --> 00:14:44.880
the data into BigQuery?

00:14:44.880 --> 00:14:46.440
RYAN BOYD: Yeah, so there
are a variety of

00:14:46.440 --> 00:14:47.650
different tools here.

00:14:47.650 --> 00:14:49.790
And it goes all over
the board.

00:14:49.790 --> 00:14:52.492
A lot of is based off of what
your experience is.

00:14:55.070 --> 00:14:57.950
what the source of data is,
where is the data being

00:14:57.950 --> 00:15:00.090
collected, and things like
that, what tools

00:15:00.090 --> 00:15:01.360
you're going to choose.

00:15:01.360 --> 00:15:03.400
But we'll give you an idea of
some of the tools that we've

00:15:03.400 --> 00:15:05.740
used and some of the tools that
our customers have used

00:15:05.740 --> 00:15:07.390
with BigQuery.

00:15:07.390 --> 00:15:10.780
So there's the App Engine
MapReduce and Pipelines API.

00:15:10.780 --> 00:15:14.020
We're gonna talk about this
a little bit more later.

00:15:14.020 --> 00:15:16.800
But App Engine MapReduce is
an implementation of the

00:15:16.800 --> 00:15:19.330
MapReduce algorithms
in App Engine.

00:15:19.330 --> 00:15:21.250
It's pure App Engine
implementation.

00:15:21.250 --> 00:15:22.800
And we've used it.

00:15:22.800 --> 00:15:26.300
And we've heard others using it
to great success in running

00:15:26.300 --> 00:15:29.570
sort of large jobs to
transform data.

00:15:29.570 --> 00:15:31.680
And then, of course, the
Pipelines API, which is

00:15:31.680 --> 00:15:35.860
actually what MapReduce is
built in part on top of.

00:15:35.860 --> 00:15:38.080
And the Pipelines API basically
allows you to create

00:15:38.080 --> 00:15:42.910
a dependency tree of jobs,
executions, so kind of a

00:15:42.910 --> 00:15:44.950
workflow of the various
processes that

00:15:44.950 --> 00:15:46.140
are going into play.

00:15:46.140 --> 00:15:48.020
And again, running
on App Engine.

00:15:48.020 --> 00:15:52.950
And it's very helpful for doing
things where you we want

00:15:52.950 --> 00:15:56.080
to process a bunch of files and
then load it into BigQuery

00:15:56.080 --> 00:15:58.330
if the files processed
correctly.

00:15:58.330 --> 00:16:01.880
And we'll see an example
here shortly.

00:16:01.880 --> 00:16:04.510
There are also, of course,
commercial ETL tools.

00:16:04.510 --> 00:16:08.030
There's an entire industry out
there preparing tools to allow

00:16:08.030 --> 00:16:10.160
you to prepare your data.

00:16:10.160 --> 00:16:12.020
And these tools are--

00:16:12.020 --> 00:16:13.490
there's a wide variety
of them.

00:16:13.490 --> 00:16:15.340
The licensing and the
way that they're

00:16:15.340 --> 00:16:16.300
structured is different.

00:16:16.300 --> 00:16:18.550
Some are actually even
open source models.

00:16:18.550 --> 00:16:21.220
But I'm going to talk about
Pervasive a little bit more.

00:16:21.220 --> 00:16:23.590
And then we also have
Informatica and Talend, who

00:16:23.590 --> 00:16:26.860
have all integrated with
BigQuery to allow it to be

00:16:26.860 --> 00:16:29.390
really easy for you to load your
data into Cloud Storage

00:16:29.390 --> 00:16:32.050
and ingest it into BigQuery.

00:16:32.050 --> 00:16:33.540
And then, of course,
our favorite--

00:16:33.540 --> 00:16:35.230
the UNIX command-line.

00:16:35.230 --> 00:16:35.950
MICHAEL MANOOCHEHRI: We've
rediscovered the UNIX

00:16:35.950 --> 00:16:36.240
command-line.

00:16:36.240 --> 00:16:38.540
RYAN BOYD: Yes, rediscovered
the UNIX command-line.

00:16:38.540 --> 00:16:41.900
Oftentimes, there are simple
UNIX command-line tools that

00:16:41.900 --> 00:16:44.240
you can use to prepare
your data.

00:16:44.240 --> 00:16:47.530
This is especially great if it's
a one-time load job that

00:16:47.530 --> 00:16:49.330
you're doing and not something
that you're regularly

00:16:49.330 --> 00:16:50.370
scheduling.

00:16:50.370 --> 00:16:52.340
These tools are pretty
fantastic.

00:16:52.340 --> 00:16:55.900
And we'll give you some hints
as to how we've used them.

00:16:55.900 --> 00:17:00.540
And then, of course, if you have
XML, use a bit of code.

00:17:00.540 --> 00:17:02.070
We don't have too much on
this in the rest of the

00:17:02.070 --> 00:17:05.810
presentation, but I will say
Wikipedia provides their

00:17:05.810 --> 00:17:08.510
revision history.

00:17:08.510 --> 00:17:10.740
So basically, all the revisions
that have happened

00:17:10.740 --> 00:17:13.630
to every single page in
Wikipedia, about 450 million

00:17:13.630 --> 00:17:17.210
rows, you can download
that as one XML file.

00:17:17.210 --> 00:17:18.980
And if I'm recalling correctly,
I think it was like

00:17:18.980 --> 00:17:21.280
130 gigabytes compressed
or something.

00:17:21.280 --> 00:17:22.055
MICHAEL MANOOCHEHRI:
Pretty massive.

00:17:22.055 --> 00:17:23.359
RYAN BOYD: Massive XML file.

00:17:23.359 --> 00:17:27.170
You're going to want to use a
streaming-based parser or a

00:17:27.170 --> 00:17:30.140
SAX-based parser for that XML.

00:17:30.140 --> 00:17:33.780
The lxml library in Python I've
found to be very helpful

00:17:33.780 --> 00:17:34.470
for doing that.

00:17:34.470 --> 00:17:38.120
So check that out.

00:17:38.120 --> 00:17:40.780
So the commercial side,
here's an example.

00:17:40.780 --> 00:17:42.470
It's a screenshot
from Pervasive's

00:17:42.470 --> 00:17:43.780
RushAnalyzer product.

00:17:43.780 --> 00:17:46.290
And we've worked with them
to integrate BigQuery.

00:17:46.290 --> 00:17:49.970
And you can see in a very visual
fashion and create this

00:17:49.970 --> 00:17:53.080
in a very visual fashion a
workflow for loading your data

00:17:53.080 --> 00:17:54.650
into BigQuery.

00:17:54.650 --> 00:17:58.200
So in this case, they're
actually taking data from a

00:17:58.200 --> 00:18:04.310
on-premise HFS file system, and
joining that data together

00:18:04.310 --> 00:18:07.870
with data from a MySQL database
that's either

00:18:07.870 --> 00:18:11.380
in-house or in the cloud, and
bringing that all together and

00:18:11.380 --> 00:18:16.080
joining it even with some,
I believe, FAT files, and

00:18:16.080 --> 00:18:18.700
selecting only particular fields
after they do that

00:18:18.700 --> 00:18:20.860
join, selecting only the fields
that they require, and

00:18:20.860 --> 00:18:25.430
then writing that out to
BigQuery and using Cloud

00:18:25.430 --> 00:18:27.200
Storage as kind of the
intermediate step.

00:18:27.200 --> 00:18:27.550
But--

00:18:27.550 --> 00:18:28.400
MICHAEL MANOOCHEHRI: I think
these guys are doing that

00:18:28.400 --> 00:18:30.250
denormalization and flattening
you described.

00:18:30.250 --> 00:18:31.090
Or we described earlier.

00:18:31.090 --> 00:18:31.800
RYAN BOYD: Yeah, exactly.

00:18:31.800 --> 00:18:34.700
That's actually all what
this joining is, is the

00:18:34.700 --> 00:18:40.200
denormalization process of
pre-joining the data before

00:18:40.200 --> 00:18:41.460
loading it into the BigQuery.

00:18:41.460 --> 00:18:45.600
So it's a great tool for doing
that type of thing.

00:18:45.600 --> 00:18:49.370
And you can do it in this
visual style workflow.

00:18:49.370 --> 00:18:52.400
The other commercial ETL tools,
the Informaticas and

00:18:52.400 --> 00:18:56.900
Talends provide similar style
of creating your workflows.

00:18:56.900 --> 00:19:00.850
And I know with Pervasive, when
you build that workflow,

00:19:00.850 --> 00:19:02.730
you can execute it locally.

00:19:02.730 --> 00:19:05.440
But you can also distribute the
job to actually run it in,

00:19:05.440 --> 00:19:08.800
say, one of your local
on-premise data centers or

00:19:08.800 --> 00:19:12.190
even run it in the cloud and
have this job really scale out

00:19:12.190 --> 00:19:15.050
to a lot of machines running
this load job and schedule it

00:19:15.050 --> 00:19:15.960
and that sort of thing.

00:19:15.960 --> 00:19:20.500
It provides all that power and
flexibility but with this nice

00:19:20.500 --> 00:19:22.140
GUI front end to it.

00:19:22.140 --> 00:19:23.730
So that's pretty powerful.

00:19:23.730 --> 00:19:27.430
But sometimes you don't need
that level of power.

00:19:27.430 --> 00:19:29.660
And that's where the UNIX
command-line tools come in.

00:19:29.660 --> 00:19:30.911
[LAUGHTER]

00:19:30.911 --> 00:19:33.760
RYAN BOYD: Here's a row data,
which I do know where this row

00:19:33.760 --> 00:19:35.220
of data came from.

00:19:35.220 --> 00:19:37.930
This is the campaign
finance data.

00:19:37.930 --> 00:19:38.780
MICHAEL MANOOCHEHRI: Ah, yes.

00:19:38.780 --> 00:19:40.060
RYAN BOYD: So this is--

00:19:40.060 --> 00:19:41.540
the government actually
publishes--

00:19:41.540 --> 00:19:43.610
the US Government publishes
donations to

00:19:43.610 --> 00:19:44.980
various political campaigns.

00:19:44.980 --> 00:19:47.880
So you can see here this
is a donation that I

00:19:47.880 --> 00:19:50.530
made in 2008 for $50.

00:19:50.530 --> 00:19:52.720
It's in that record.

00:19:52.720 --> 00:19:55.500
And it's all public data that
the government's publishing.

00:19:55.500 --> 00:19:57.930
So one thing that you'll
notice here that I've

00:19:57.930 --> 00:20:02.090
highlighted on this data is
the year and the date.

00:20:02.090 --> 00:20:07.440
So 2008-10-19 is when I
made this donation.

00:20:07.440 --> 00:20:10.970
Exactly two weeks after my
birthday, apparently, I made

00:20:10.970 --> 00:20:12.480
this donation for $50.

00:20:12.480 --> 00:20:16.330
But the thing is, BigQuery
doesn't handle date formats

00:20:16.330 --> 00:20:17.580
very well like this.

00:20:17.580 --> 00:20:20.410
I mentioned some of the
basic date formats--

00:20:20.410 --> 00:20:21.000
sorry--

00:20:21.000 --> 00:20:24.740
the basic field formats
that BigQuery

00:20:24.740 --> 00:20:25.810
provides in its schema.

00:20:25.810 --> 00:20:29.230
And it's things like ints
and strings and floats.

00:20:29.230 --> 00:20:29.790
MICHAEL MANOOCHEHRI: Booleans.

00:20:29.790 --> 00:20:30.695
RYAN BOYD: And Booleans.

00:20:30.695 --> 00:20:34.700
Yeah, exactly, so these
various formats.

00:20:34.700 --> 00:20:37.950
But dates currently don't have
a special format in BigQuery.

00:20:37.950 --> 00:20:39.810
We rely on integers.

00:20:39.810 --> 00:20:42.620
And this is actually quite
familiar with those UNIX geeks

00:20:42.620 --> 00:20:47.130
out there because time since
epoch, or January 1, 1970, is

00:20:47.130 --> 00:20:51.430
a very common way to keep track
of time in UNIX tools.

00:20:51.430 --> 00:20:53.430
And so we use that in
BigQuery as well.

00:20:53.430 --> 00:20:55.190
And you can sort your
data based off of

00:20:55.190 --> 00:20:58.150
that time since epoch.

00:20:58.150 --> 00:20:59.850
And it's really easy once
you get your data in.

00:20:59.850 --> 00:21:01.520
But some people have a little
bit of a challenge doing the

00:21:01.520 --> 00:21:04.430
conversion to get the data in.

00:21:04.430 --> 00:21:05.990
MICHAEL MANOOCHEHRI: This
is a very common

00:21:05.990 --> 00:21:07.270
data conversion issue.

00:21:07.270 --> 00:21:09.580
RYAN BOYD: Yeah, and something
we'll probably resolve in the

00:21:09.580 --> 00:21:10.155
core product.

00:21:10.155 --> 00:21:10.710
MICHAEL MANOOCHEHRI: Yes.

00:21:10.710 --> 00:21:10.980
[LAUGHTER]

00:21:10.980 --> 00:21:12.490
RYAN BOYD: And there are
actually functions in BigQuery

00:21:12.490 --> 00:21:14.990
when you're querying the data
to convert that time since

00:21:14.990 --> 00:21:17.940
epoch back out to a more human
representative of a form.

00:21:17.940 --> 00:21:19.326
MICHAEL MANOOCHEHRI:
Yeah, exactly.

00:21:19.326 --> 00:21:23.530
RYAN BOYD: But anyway, so
here's our basic data.

00:21:23.530 --> 00:21:26.840
You have the time
in 2008-10-19.

00:21:26.840 --> 00:21:32.130
And we want to convert it into
a time that looks like this,

00:21:32.130 --> 00:21:34.550
which is that time
since epoch.

00:21:34.550 --> 00:21:38.920
So that's the number of seconds
since January 1, 1970.

00:21:38.920 --> 00:21:42.300
And so to do that conversion is
really, really simple on a

00:21:42.300 --> 00:21:43.400
command-line tool.

00:21:43.400 --> 00:21:45.640
And I never would have known
this without the web.

00:21:45.640 --> 00:21:48.960
Google is your friend when
trying to do these things.

00:21:48.960 --> 00:21:50.800
I'm not that big of
an expert in awk.

00:21:50.800 --> 00:21:55.160
But here is an awk command.

00:21:55.160 --> 00:21:57.690
Now, you'll notice that I'm on
my machine, logged onto my

00:21:57.690 --> 00:22:00.420
machine here, campaign
finance directory.

00:22:00.420 --> 00:22:02.910
And what I'm doing first is
doing kind of a streaming

00:22:02.910 --> 00:22:09.500
unzip from gunzip and piping
that output to GNU's awk.

00:22:09.500 --> 00:22:12.930
And then basically what it's
doing here is replacing the

00:22:12.930 --> 00:22:18.000
dashes in the date with spaces
and then passing that time as

00:22:18.000 --> 00:22:22.700
a string time into the mktime
function to make a time object

00:22:22.700 --> 00:22:23.210
out of that.

00:22:23.210 --> 00:22:27.030
And that becomes our int that's
the time since epoch.

00:22:27.030 --> 00:22:33.030
And we're substituting column 10
of our original source file

00:22:33.030 --> 00:22:36.360
with that time and printing it
all out, and then again,

00:22:36.360 --> 00:22:40.400
piping it out to gzip to
compress the data again.

00:22:40.400 --> 00:22:43.300
So there's a really
large source file.

00:22:43.300 --> 00:22:47.490
We can actually only decompress
a couple lines at a

00:22:47.490 --> 00:22:50.560
time, whatever's in the buffer
here, so very powerful tool

00:22:50.560 --> 00:22:53.600
with just UNIX command-line.

00:22:53.600 --> 00:22:55.890
The other UNIX command-line tool
that we should talk about

00:22:55.890 --> 00:22:58.920
that's pretty powerful is
something to deal with that

00:22:58.920 --> 00:23:02.960
4-gigabyte chunk that BigQuery
allows for currently.

00:23:02.960 --> 00:23:08.290
So your file size on each file
can be up to 4 gigabytes.

00:23:08.290 --> 00:23:10.960
And you sometimes want to split
your source file into

00:23:10.960 --> 00:23:12.770
those 4-gigabyte chunks.

00:23:12.770 --> 00:23:14.850
And you can do that.

00:23:14.850 --> 00:23:17.380
Let's say you have a source file
that's 12 gigabytes, and

00:23:17.380 --> 00:23:19.820
you need this less than 4
gigabytes or less than or

00:23:19.820 --> 00:23:22.730
equal to chunk, you
can use split.

00:23:22.730 --> 00:23:24.375
And this is how you do it.

00:23:24.375 --> 00:23:25.570
It's really simple.

00:23:25.570 --> 00:23:29.700
You're saying split -C. And -C
is making sure that it doesn't

00:23:29.700 --> 00:23:32.230
split on anything other than a
new line so you don't get a

00:23:32.230 --> 00:23:33.990
partial line of data.

00:23:33.990 --> 00:23:37.410
But then it's writing out 4
gigs of data per chunk.

00:23:37.410 --> 00:23:39.460
And you have your source
file here.

00:23:39.460 --> 00:23:43.230
And it will just create a series
of other files, which

00:23:43.230 --> 00:23:47.880
will be numbered based off of
their position in that series

00:23:47.880 --> 00:23:49.610
up to-- in this case, it
would be three separate

00:23:49.610 --> 00:23:50.650
files with four gigs.

00:23:50.650 --> 00:23:51.700
MICHAEL MANOOCHEHRI: And I use
this command all the time.

00:23:51.700 --> 00:23:53.220
What's great about it is
it's easy to script.

00:23:53.220 --> 00:23:54.330
RYAN BOYD: Yeah, exactly.

00:23:54.330 --> 00:23:55.520
MICHAEL MANOOCHEHRI: So you
script it, and you put it into

00:23:55.520 --> 00:23:58.688
your ingestion job, and
it's really great.

00:23:58.688 --> 00:23:59.680
RYAN BOYD: All right.

00:23:59.680 --> 00:24:02.480
Now I'm going to hand this back
over to Michael to chat a

00:24:02.480 --> 00:24:05.600
little about a project that
we've been working on recently

00:24:05.600 --> 00:24:10.100
to load 6 terabytes of data or 2
terabytes compressed of data

00:24:10.100 --> 00:24:11.010
into BigQuery.

00:24:11.010 --> 00:24:11.280
Michael.

00:24:11.280 --> 00:24:11.560
MICHAEL MANOOCHEHRI: Yeah.

00:24:11.560 --> 00:24:12.780
So like as Ryan said, we've been
working on this really

00:24:12.780 --> 00:24:13.570
fun project.

00:24:13.570 --> 00:24:15.080
This is kind of putting
everything we just talked

00:24:15.080 --> 00:24:16.230
about in practice.

00:24:16.230 --> 00:24:19.540
It's a study in loading 6
terabytes of data into

00:24:19.540 --> 00:24:21.530
BigQuery using App Engine.

00:24:21.530 --> 00:24:23.250
So this has been really great.

00:24:23.250 --> 00:24:25.430
We're are looking at Wikipedia
page views.

00:24:25.430 --> 00:24:28.020
So Wikipedia publishes
hourly the page

00:24:28.020 --> 00:24:29.710
views of all the pages.

00:24:29.710 --> 00:24:32.190
And this is a huge-- since 2008,
I think, is the data

00:24:32.190 --> 00:24:32.750
that we're looking.

00:24:32.750 --> 00:24:33.210
RYAN BOYD: Yeah.

00:24:33.210 --> 00:24:35.040
It starts at the end of 2007,
beginning of 2008.

00:24:35.040 --> 00:24:35.440
MICHAEL MANOOCHEHRI: Exactly.

00:24:35.440 --> 00:24:36.780
RYAN BOYD: I think we
skipped the last

00:24:36.780 --> 00:24:38.020
couple of days of 2007.

00:24:38.020 --> 00:24:38.370
[LAUGHTER]

00:24:38.370 --> 00:24:39.170
MICHAEL MANOOCHEHRI:
OK, you're right.

00:24:39.170 --> 00:24:40.290
So we don't have all of it.

00:24:40.290 --> 00:24:42.860
But the data we have is
6 terabytes of data.

00:24:42.860 --> 00:24:45.480
And this is split up into
thousands of files of, I

00:24:45.480 --> 00:24:47.000
think, hundreds of
megabytes each.

00:24:47.000 --> 00:24:48.000
I think they're-- compressed,
they are

00:24:48.000 --> 00:24:49.440
something like 100 megs.

00:24:49.440 --> 00:24:51.190
RYAN BOYD: Anywhere
between 30--

00:24:51.190 --> 00:24:53.490
you can actually see from the
size of the files how much

00:24:53.490 --> 00:24:55.480
Wikipedia has grown,
even since 2008.

00:24:55.480 --> 00:24:56.050
MICHAEL MANOOCHEHRI: Yes, yes.

00:24:56.050 --> 00:24:57.960
RYAN BOYD: Because the file
sizes increase, I think, from

00:24:57.960 --> 00:25:00.720
30 megabytes compressed
up to about 100.

00:25:00.720 --> 00:25:01.500
MICHAEL MANOOCHEHRI: Yes.

00:25:01.500 --> 00:25:03.590
RYAN BOYD: I think there were
like 60,000 files or something

00:25:03.590 --> 00:25:04.090
crazy like that.

00:25:04.090 --> 00:25:05.330
MICHAEL MANOOCHEHRI: It's an
enormous amount of data.

00:25:05.330 --> 00:25:05.910
RYAN BOYD: Right.

00:25:05.910 --> 00:25:07.040
MICHAEL MANOOCHEHRI: So what
we're doing is using App

00:25:07.040 --> 00:25:10.880
Engine for its ability to
distribute tasks across a lot

00:25:10.880 --> 00:25:12.600
of instances automatically.

00:25:12.600 --> 00:25:14.620
So what we're actually doing--
here's an example of what

00:25:14.620 --> 00:25:15.280
we're actually doing.

00:25:15.280 --> 00:25:17.850
The original schema of these
Wikipedia page jumps looks

00:25:17.850 --> 00:25:19.440
something like this
on the left side.

00:25:19.440 --> 00:25:22.560
So there's a language, the title
of the Wikipedia page,

00:25:22.560 --> 00:25:25.130
the amount of page views it's
gotten in the particular hour

00:25:25.130 --> 00:25:29.570
that that file pertains to, and
the amount of bytes in the

00:25:29.570 --> 00:25:31.360
page, which is also great,
because you can see how that

00:25:31.360 --> 00:25:32.690
changes over time as well.

00:25:32.690 --> 00:25:34.610
So this is a really interesting
data set.

00:25:34.610 --> 00:25:37.780
What we want to do is take this
data, this raw data, and

00:25:37.780 --> 00:25:40.150
put it into a BigQuery's table
that looks like the

00:25:40.150 --> 00:25:41.630
schema on the right.

00:25:41.630 --> 00:25:44.190
So what we're doing is we're
doing some of that sort of

00:25:44.190 --> 00:25:47.200
date/time transformation that
Ryan talked about earlier.

00:25:47.200 --> 00:25:49.240
And we're breaking down your
month and day to make our

00:25:49.240 --> 00:25:50.650
queries simpler.

00:25:50.650 --> 00:25:53.800
RYAN BOYD: And on the date/time
transformation, we

00:25:53.800 --> 00:25:54.940
don't actually show it here.

00:25:54.940 --> 00:25:58.570
But the files are actually named
based off of the year,

00:25:58.570 --> 00:26:02.160
month, day, and hour
of the request.

00:26:02.160 --> 00:26:07.080
So there's one file for every
hour since the end of 2007.

00:26:07.080 --> 00:26:09.840
And so we're taking the info
from the file name and

00:26:09.840 --> 00:26:12.920
actually combining that and
joining that with the rest of

00:26:12.920 --> 00:26:13.620
the record that's in the file.

00:26:13.620 --> 00:26:14.370
MICHAEL MANOOCHEHRI:
Yeah, exactly.

00:26:14.370 --> 00:26:17.580
And we're actually parsing that
out and the first five

00:26:17.580 --> 00:26:19.760
fields are based on
that information.

00:26:19.760 --> 00:26:22.170
And also the Wikipedia project
is actually something we pull

00:26:22.170 --> 00:26:23.400
from the title.

00:26:23.400 --> 00:26:25.480
That's could be Wikipedia
itself, and I think there's

00:26:25.480 --> 00:26:27.780
other Wikipedia projects,
not the main

00:26:27.780 --> 00:26:30.480
Wikipedia, but other things.

00:26:30.480 --> 00:26:32.490
So we don't put the language
in there as well.

00:26:32.490 --> 00:26:33.800
We parse the title.

00:26:33.800 --> 00:26:35.840
And then we take the page
views and the bytes

00:26:35.840 --> 00:26:38.145
transferred just as integers and
put this into the table.

00:26:38.145 --> 00:26:41.180
RYAN BOYD: And actually on the
date/time stuff, originally we

00:26:41.180 --> 00:26:45.430
actually put the date/time
literally as just a string.

00:26:45.430 --> 00:26:49.290
And it was actually quite easy
to work with as a string if

00:26:49.290 --> 00:26:51.640
you knew some things about
regular expressions.

00:26:51.640 --> 00:26:54.510
You could do the sorts and the
searches and all with the

00:26:54.510 --> 00:26:58.260
strings because you were using
the regular expression to

00:26:58.260 --> 00:27:01.670
convert it to integers
for that process.

00:27:01.670 --> 00:27:04.750
But eventually we figured out
not everyone every time they

00:27:04.750 --> 00:27:05.800
run a query--

00:27:05.800 --> 00:27:07.890
even though regular expressions
are super fast,

00:27:07.890 --> 00:27:10.780
even over hundreds of millions
of rows of data with BigQuery,

00:27:10.780 --> 00:27:13.970
or billions of rows of data with
BigQuery, not everyone

00:27:13.970 --> 00:27:16.090
wants to do that regular
expression every time they're

00:27:16.090 --> 00:27:16.920
running the query.

00:27:16.920 --> 00:27:20.280
So that's why we did some kind
of pre-analysis here and

00:27:20.280 --> 00:27:22.040
separated out--

00:27:22.040 --> 00:27:25.440
put the date/time since epoch
and then the year, month, day,

00:27:25.440 --> 00:27:28.125
and hour just to make it easier
to query, even though

00:27:28.125 --> 00:27:29.080
it would have worked
otherwise.

00:27:29.080 --> 00:27:29.540
MICHAEL MANOOCHEHRI:
It's true.

00:27:29.540 --> 00:27:29.900
It's super convenient.

00:27:29.900 --> 00:27:31.310
Imagine running a query
about noon.

00:27:31.310 --> 00:27:33.800
So give me all the pages that
were looked at at noon.

00:27:33.800 --> 00:27:36.640
And that's an easy query to do
with this kind of format.

00:27:36.640 --> 00:27:38.630
One thing I forgot to mention
is we're also sharding this

00:27:38.630 --> 00:27:41.670
data, as we mentioned earlier,
into months.

00:27:41.670 --> 00:27:44.340
So every bit of data
is sharded into

00:27:44.340 --> 00:27:45.910
a table of a month.

00:27:45.910 --> 00:27:47.480
So we can look at the
month itself.

00:27:47.480 --> 00:27:49.630
And if we want to look at more
months, we could run a union

00:27:49.630 --> 00:27:50.830
query with BigQuery.

00:27:50.830 --> 00:27:52.950
We could even look at the entire
data set month after

00:27:52.950 --> 00:27:55.860
month by just specifying the
table name in our queries.

00:27:55.860 --> 00:27:57.560
So it makes it really
convenient.

00:27:57.560 --> 00:28:00.450
I think this is a really good
strategy for this type of data

00:28:00.450 --> 00:28:01.850
and this amount of data.

00:28:01.850 --> 00:28:03.540
But how did we get
the data in?

00:28:03.540 --> 00:28:05.210
So that's the really
interesting part.

00:28:05.210 --> 00:28:08.450
We decided to use App Engine and
the Pipelines API, which

00:28:08.450 --> 00:28:09.620
we mentioned earlier.

00:28:09.620 --> 00:28:11.600
So what's great about Pipelines,
as Ryan mentioned,

00:28:11.600 --> 00:28:15.420
is it allows you to build a
workflow and distribute the

00:28:15.420 --> 00:28:19.780
work of ingesting all of these
files into multiple instances

00:28:19.780 --> 00:28:21.500
on App Engine pretty
much automatically.

00:28:21.500 --> 00:28:24.780
It lets you just worry about
the code itself rather than

00:28:24.780 --> 00:28:28.215
actually distributing this
across a bunch of task queues.

00:28:28.215 --> 00:28:29.700
This has been really
convenient.

00:28:29.700 --> 00:28:30.390
RYAN BOYD: Yeah.

00:28:30.390 --> 00:28:33.860
So on the distributed nature
of it, it's been fantastic.

00:28:33.860 --> 00:28:35.310
So we have--

00:28:35.310 --> 00:28:38.050
we're basically breaking this
down by month and only

00:28:38.050 --> 00:28:39.510
processing a month
at the time.

00:28:39.510 --> 00:28:42.690
And depending on the number of
days in a given month, 720 to

00:28:42.690 --> 00:28:47.390
740 files for every month, and
we're distributing that

00:28:47.390 --> 00:28:50.700
across, I believe in our latest
configuration, about

00:28:50.700 --> 00:28:54.710
100 App Engine instances to
have all of those 100 App

00:28:54.710 --> 00:28:57.380
Engine instances process this
data simultaneously.

00:28:57.380 --> 00:29:01.480
And it just speeds this up to
what would have been a process

00:29:01.480 --> 00:29:03.010
that I probably would
take like half

00:29:03.010 --> 00:29:04.180
a year on my desktop.

00:29:04.180 --> 00:29:04.250
[LAUGHTER]

00:29:04.250 --> 00:29:04.950
MICHAEL MANOOCHEHRI: Right.

00:29:04.950 --> 00:29:06.390
RYAN BOYD: We can have
App Engine do it in

00:29:06.390 --> 00:29:07.500
a much faster way.

00:29:07.500 --> 00:29:08.560
MICHAEL MANOOCHEHRI: The
Pipelines API is great too

00:29:08.560 --> 00:29:10.890
because it provides a sort of
control panel, or status page

00:29:10.890 --> 00:29:12.380
actually, of what's going on.

00:29:12.380 --> 00:29:15.155
So you can actually follow the
workflow as it's happening and

00:29:15.155 --> 00:29:18.270
kind of check on the status of
individual, let's say, leaves

00:29:18.270 --> 00:29:20.750
on the workflow tree and see
what each one is doing.

00:29:20.750 --> 00:29:21.850
So it's been really
convenient.

00:29:21.850 --> 00:29:24.820
RYAN BOYD: Yeah, you can see
here that the children listed

00:29:24.820 --> 00:29:26.940
on the bottom right here, that's
the children of this

00:29:26.940 --> 00:29:28.250
process month.

00:29:28.250 --> 00:29:30.250
And we can click
on any of them.

00:29:30.250 --> 00:29:31.270
We actually did.

00:29:31.270 --> 00:29:32.370
You'll see some of these.

00:29:32.370 --> 00:29:33.690
I don't know if you can
see it in the video.

00:29:33.690 --> 00:29:34.590
It might be a little
too small.

00:29:34.590 --> 00:29:36.510
But some of these are calling
Pipelines called

00:29:36.510 --> 00:29:37.910
Process File Pipeline.

00:29:37.910 --> 00:29:40.200
And that's actually doing the
full transformation and

00:29:40.200 --> 00:29:41.560
processing the file.

00:29:41.560 --> 00:29:42.890
But some of the other
pipelines are

00:29:42.890 --> 00:29:44.280
called Return File Name.

00:29:44.280 --> 00:29:47.680
That's in the case that we
already processed the file and

00:29:47.680 --> 00:29:50.200
are just trying to make sure
that it gets included in the

00:29:50.200 --> 00:29:51.530
upcoming ingestion job.

00:29:51.530 --> 00:29:54.920
So we kept track of the things
that we were doing and stored

00:29:54.920 --> 00:29:57.210
that in the App Engine
data store so that

00:29:57.210 --> 00:29:58.850
when we actually do--

00:29:58.850 --> 00:30:01.440
if we do have an error, if one
of the files is bad, or

00:30:01.440 --> 00:30:03.400
something like that,
we can rerun

00:30:03.400 --> 00:30:04.400
this without any problems.

00:30:04.400 --> 00:30:05.820
MICHAEL MANOOCHEHRI: Yeah,
and it's been great.

00:30:05.820 --> 00:30:07.650
The other thing is sometimes
when you're dealing with this

00:30:07.650 --> 00:30:10.700
many files, you may have a
corrupted file in the source.

00:30:10.700 --> 00:30:12.180
There's all kinds of things
that can happen.

00:30:12.180 --> 00:30:16.560
So adding this kind of check on
have you processed the file

00:30:16.560 --> 00:30:17.830
is a really good
best practice.

00:30:17.830 --> 00:30:20.130
RYAN BOYD: yeah, so these are
basically just broke up by

00:30:20.130 --> 00:30:23.370
month, processed all the files
in that given month, and then

00:30:23.370 --> 00:30:26.100
went off and called a big
query ingestion job.

00:30:26.100 --> 00:30:27.270
And our queues--

00:30:27.270 --> 00:30:29.230
this is built on top
of the task queues.

00:30:29.230 --> 00:30:31.360
Our queues for calling BigQuery
ingestion, basically

00:30:31.360 --> 00:30:34.480
we're configuring them such that
we're only running the

00:30:34.480 --> 00:30:39.250
two ingestion job simultaneously
on BigQuery to

00:30:39.250 --> 00:30:41.550
go by that quota limit.

00:30:41.550 --> 00:30:44.250
And we can--

00:30:44.250 --> 00:30:44.470
sorry.

00:30:44.470 --> 00:30:45.820
We're running those two
simultaneously.

00:30:45.820 --> 00:30:48.770
But in the case sometimes that
we mess up, we're actually

00:30:48.770 --> 00:30:51.370
detecting failures and rerunning
the task in case

00:30:51.370 --> 00:30:51.980
there are any failures.

00:30:51.980 --> 00:30:52.940
MICHAEL MANOOCHEHRI: And
Ryan, remind me.

00:30:52.940 --> 00:30:55.405
I think each month has something
like 720 files.

00:30:55.405 --> 00:30:55.810
RYAN BOYD: Yeah, that's right.

00:30:55.810 --> 00:30:57.330
MICHAEL MANOOCHEHRI: So to
adjust an entire month we

00:30:57.330 --> 00:31:00.460
actually split that into two
batches of like 350 or

00:31:00.460 --> 00:31:04.900
something like that of files
in multiple batches.

00:31:04.900 --> 00:31:06.360
So it's a great way to
get most of that.

00:31:06.360 --> 00:31:09.460
We can almost ingest an entire
month's in one batch, which is

00:31:09.460 --> 00:31:10.070
pretty remarkable.

00:31:10.070 --> 00:31:13.095
It's quite a lot of data.

00:31:13.095 --> 00:31:14.040
RYAN BOYD: All right.

00:31:14.040 --> 00:31:17.100
So I think that's really about
it for today's episode.

00:31:17.100 --> 00:31:21.730
You've seen Michael and Ryan's
tips for how to work with

00:31:21.730 --> 00:31:24.200
BigQuery and getting your
data into BigQuery.

00:31:24.200 --> 00:31:27.600
And hopefully this helps
you a little bit as you

00:31:27.600 --> 00:31:30.520
load your own data.

00:31:30.520 --> 00:31:33.460
Let's give a quick overview of
what we discussed today, and

00:31:33.460 --> 00:31:37.200
then we'll get into your
questions that you have on the

00:31:37.200 --> 00:31:38.230
Google Moderator.

00:31:38.230 --> 00:31:41.910
And so we've talked about
denormalization being your

00:31:41.910 --> 00:31:45.470
friend, talked about
sharding your data.

00:31:45.470 --> 00:31:48.300
If you need to transform your
data, you can use a variety of

00:31:48.300 --> 00:31:51.040
different tools-- commercial
tools, App Engine, or your

00:31:51.040 --> 00:31:51.985
favorite UNIX commands.

00:31:51.985 --> 00:31:54.122
[LAUGHTER]

00:31:54.122 --> 00:31:56.590
RYAN BOYD: And now it's time
for your questions.

00:31:56.590 --> 00:31:59.270
Let's see what questions exist
over on our Google Moderator.

00:31:59.270 --> 00:32:02.840
And it actually looks like we
have one person live on the

00:32:02.840 --> 00:32:04.860
Hangout as well.

00:32:04.860 --> 00:32:09.800
So if the person that's live on
the Hangout can introduce

00:32:09.800 --> 00:32:13.210
themselves and talk a little bit
about what they're doing

00:32:13.210 --> 00:32:15.220
with BigQuery or what they'd
like to do with BigQuery.

00:32:15.220 --> 00:32:17.260
And then feel free to ask any
questions that you have.

00:32:21.670 --> 00:32:21.980
Hello?

00:32:21.980 --> 00:32:23.420
I believe you're muted
right now.

00:32:23.420 --> 00:32:25.326
[LAUGHTER]

00:32:25.326 --> 00:32:26.510
RYAN BOYD: There we go.

00:32:26.510 --> 00:32:29.106
I think we're hearing you now.

00:32:29.106 --> 00:32:30.530
MICHAEL MANOOCHEHRI:
All right.

00:32:30.530 --> 00:32:31.140
How's it going?

00:32:31.140 --> 00:32:33.901
Good to see you.

00:32:33.901 --> 00:32:34.720
MALE SPEAKER: Great.

00:32:34.720 --> 00:32:37.660
Yeah, so I'm the person
in the Hangout then.

00:32:37.660 --> 00:32:41.190
And actually, I'm not
doing currently

00:32:41.190 --> 00:32:42.640
anything with big data.

00:32:42.640 --> 00:32:44.100
I'm fascinated by it.

00:32:44.100 --> 00:32:51.820
I'm working with qualitative
data for my PhD thesis, which

00:32:51.820 --> 00:32:54.270
is badly structured.

00:32:54.270 --> 00:32:58.440
Basically, you're doing text
analysis and things like that.

00:32:58.440 --> 00:33:06.220
And I find it inspiring what
can be done with this

00:33:06.220 --> 00:33:09.030
processing and analysis
of date.

00:33:09.030 --> 00:33:13.400
Now, the amount of data which
I have I could possibly

00:33:13.400 --> 00:33:16.046
process on any recent
mobile phone.

00:33:16.046 --> 00:33:19.670
But there are--

00:33:19.670 --> 00:33:24.190
in my other life, I'm working
as a project manager in data

00:33:24.190 --> 00:33:25.770
center project management.

00:33:25.770 --> 00:33:33.060
And I find it intriguing to
apply some principles which

00:33:33.060 --> 00:33:36.590
are used to analyze qualitative
data with all the

00:33:36.590 --> 00:33:42.710
data that is available in all
those files loading and

00:33:42.710 --> 00:33:45.700
whatever information in
larger organizations.

00:33:45.700 --> 00:33:51.720
So it's a form of data mining,
but which escapes the typical

00:33:51.720 --> 00:33:52.955
data mining situation.

00:33:52.955 --> 00:33:53.360
MICHAEL MANOOCHEHRI: Yeah.

00:33:53.360 --> 00:33:55.970
BigQuery's really great for this
kind of taking data out

00:33:55.970 --> 00:33:58.440
of silos, joining them, and
then running aggregates.

00:33:58.440 --> 00:34:00.160
We actually have some customers
doing this.

00:34:00.160 --> 00:34:02.570
This is a great use case because
it handles such a huge

00:34:02.570 --> 00:34:03.370
amount of data.

00:34:03.370 --> 00:34:05.720
It's sometimes the only tool
that can handle this in an

00:34:05.720 --> 00:34:07.320
ad-hoc way to let you
run these queries

00:34:07.320 --> 00:34:08.329
in an ad-hoc way.

00:34:08.329 --> 00:34:08.889
RYAN BOYD: Yeah.

00:34:08.889 --> 00:34:11.860
We actually have a colleague
of ours, Katherine.

00:34:11.860 --> 00:34:15.090
Michael, Katherine, and I are
all speaking at upcoming

00:34:15.090 --> 00:34:18.219
Strata conferences between
London and New York.

00:34:18.219 --> 00:34:23.000
And Katherine's working
on preparing her talk.

00:34:23.000 --> 00:34:24.830
And she's actually, I believe,
keynoting at

00:34:24.830 --> 00:34:26.800
the Strata in London.

00:34:26.800 --> 00:34:29.670
And she's really looking at,
from a data journalism

00:34:29.670 --> 00:34:32.159
perspective, how to source
a bunch of data.

00:34:32.159 --> 00:34:34.310
A lot of the data,
you actually--

00:34:34.310 --> 00:34:37.010
you're looking at a subset of
data and trying to come up

00:34:37.010 --> 00:34:39.730
with a bigger meaning out
of that subset of data.

00:34:39.730 --> 00:34:40.639
And you really--

00:34:40.639 --> 00:34:44.010
a lot of the work is really
trying to find all the data

00:34:44.010 --> 00:34:47.270
that would possibly explain that
small subset that you're

00:34:47.270 --> 00:34:48.150
looking at.

00:34:48.150 --> 00:34:51.179
And it's a lot of hard times.

00:34:51.179 --> 00:34:54.250
But hopefully, some of the tips
that we gave you here

00:34:54.250 --> 00:34:56.389
today will at least, once you
can get your hands on that

00:34:56.389 --> 00:35:00.140
data, help you prepare it and
kind of denormalize it and get

00:35:00.140 --> 00:35:02.540
it into BigQuery if you guys
decide to do that.

00:35:02.540 --> 00:35:04.680
MICHAEL MANOOCHEHRI: This use
case you brought up of text

00:35:04.680 --> 00:35:05.660
analysis is really
interesting.

00:35:05.660 --> 00:35:08.240
We actually do some of that
on our team as well.

00:35:08.240 --> 00:35:11.470
One thing to know about BigQuery
is the maximum record

00:35:11.470 --> 00:35:14.450
size is 64K for any
single record.

00:35:14.450 --> 00:35:17.540
However, it's great for things
like n-gram analysis or word

00:35:17.540 --> 00:35:18.640
count analysis.

00:35:18.640 --> 00:35:21.360
So this is the kind of thing
where maybe a MapReduce job to

00:35:21.360 --> 00:35:24.230
find, say, word counts per
document and then put all of

00:35:24.230 --> 00:35:25.210
that data into BigQuery.

00:35:25.210 --> 00:35:27.470
That's a really good use of
BigQuery, especially when you

00:35:27.470 --> 00:35:28.355
have tons of data.

00:35:28.355 --> 00:35:28.930
RYAN BOYD: Yeah.

00:35:28.930 --> 00:35:31.950
And we actually have some
example data sets doing that

00:35:31.950 --> 00:35:36.080
already, like n-gram data set
and then a Wikipedia data set.

00:35:36.080 --> 00:35:36.690
MICHAEL MANOOCHEHRI: Yeah.

00:35:36.690 --> 00:35:37.292
RYAN BOYD: I think the
n-gram is from

00:35:37.292 --> 00:35:38.150
Google Books, I believe.

00:35:38.150 --> 00:35:38.420
Sorry.

00:35:38.420 --> 00:35:39.620
Not Wikipedia.

00:35:39.620 --> 00:35:40.590
n-gram and--

00:35:40.590 --> 00:35:41.100
MICHAEL MANOOCHEHRI:
Shakespeare.

00:35:41.100 --> 00:35:41.790
RYAN BOYD: And Shakespeare.

00:35:41.790 --> 00:35:42.300
MICHAEL MANOOCHEHRI: Right.

00:35:42.300 --> 00:35:43.300
RYAN BOYD: Yes.

00:35:43.300 --> 00:35:44.930
So we have some of the example
data sets doing that.

00:35:44.930 --> 00:35:47.650
We're actually working with the
Wikipedia stuff soon to

00:35:47.650 --> 00:35:50.830
basically take all of the text
in Wikipedia and make that

00:35:50.830 --> 00:35:54.350
into n-grams just to see what
words are most commonly

00:35:54.350 --> 00:35:56.890
appearing, when words first
appear, and things like that.

00:35:56.890 --> 00:35:58.880
Some fun analysis
on text data.

00:35:58.880 --> 00:35:59.860
MICHAEL MANOOCHEHRI: Yeah.

00:35:59.860 --> 00:36:01.540
One last thing is the other
thing that's really great

00:36:01.540 --> 00:36:03.130
about BigQuery is that
it's an API.

00:36:03.130 --> 00:36:05.280
And it's really easy to
integrate into, say, web

00:36:05.280 --> 00:36:05.750
applications.

00:36:05.750 --> 00:36:07.680
Any kind of application,
actually.

00:36:07.680 --> 00:36:10.480
You ask questions, send
queries to it via API.

00:36:10.480 --> 00:36:11.670
We also have tools.

00:36:11.670 --> 00:36:13.660
We have a browser tool and
a command-line tool.

00:36:13.660 --> 00:36:16.460
But you can actually write code
with this very simply.

00:36:16.460 --> 00:36:19.270
So it's all kind of built in
together with the API.

00:36:19.270 --> 00:36:21.450
So it makes application
processing super easy.

00:36:21.450 --> 00:36:24.680
You can build JavaScript apps
in just a few lines of code.

00:36:24.680 --> 00:36:25.640
You can use App Engine.

00:36:25.640 --> 00:36:28.190
And you can build a very
powerful application in very

00:36:28.190 --> 00:36:29.020
little code.

00:36:29.020 --> 00:36:31.950
So it's great for some of the
things you're talking about.

00:36:31.950 --> 00:36:35.120
MALE SPEAKER: Where is the best
place to get started?

00:36:35.120 --> 00:36:40.045
So I'm more a casual programmer,
if at all.

00:36:40.045 --> 00:36:44.560
So I can do some stuff in Python
or Java or processing

00:36:44.560 --> 00:36:49.580
to do little things,
visualizations, but it's--

00:36:49.580 --> 00:36:51.140
this is really old
school, yes?

00:36:51.140 --> 00:36:51.680
What I'm doing.

00:36:51.680 --> 00:36:57.575
So if I want to go into the big
data examples or just fool

00:36:57.575 --> 00:37:02.220
around with it, where
do I start?

00:37:02.220 --> 00:37:05.660
Is there any place where you can
get a step-by-step example

00:37:05.660 --> 00:37:07.730
to get you started?

00:37:07.730 --> 00:37:08.270
RYAN BOYD: Yeah.

00:37:08.270 --> 00:37:10.640
In BigQuery, I think I would
start out in BigQuery, I

00:37:10.640 --> 00:37:12.690
believe we have a step-by-step
getting started guide to

00:37:12.690 --> 00:37:14.815
working on some of the
public data samples.

00:37:14.815 --> 00:37:15.610
MICHAEL MANOOCHEHRI: Yeah.

00:37:15.610 --> 00:37:18.900
RYAN BOYD: So the public data
samples in BigQuery, you can

00:37:18.900 --> 00:37:24.280
actually run queries against
up to 100 gigabytes a day

00:37:24.280 --> 00:37:26.210
worth of queries for free
without putting down your

00:37:26.210 --> 00:37:27.650
credit card.

00:37:27.650 --> 00:37:30.370
So you can start by looking at
some of that existing data and

00:37:30.370 --> 00:37:32.600
not deal with the process of
loading your own data.

00:37:32.600 --> 00:37:35.140
But you can see what types of
queries that you can perform.

00:37:35.140 --> 00:37:37.200
I've had a lot of fun
looking at the

00:37:37.200 --> 00:37:39.300
natality data, for instance.

00:37:39.300 --> 00:37:43.690
That's US birth statistics
since 1969, I believe.

00:37:43.690 --> 00:37:47.050
And basically there's a line for
every single birth in the

00:37:47.050 --> 00:37:49.650
US that's occurred in
that time period.

00:37:49.650 --> 00:37:55.100
And you have some interesting
things like in Ohio in 2003 if

00:37:55.100 --> 00:37:58.930
you look at the babies born
there to mothers who smoked

00:37:58.930 --> 00:38:02.720
cigarettes versus didn't smoke
cigarettes, the babies born to

00:38:02.720 --> 00:38:06.270
mothers who smoked cigarettes
were half a pound lighter than

00:38:06.270 --> 00:38:10.500
the babies who had mothers that
didn't smoke cigarettes.

00:38:10.500 --> 00:38:12.780
And you find interesting
bits of data.

00:38:12.780 --> 00:38:14.820
And that's actually what
BigQuery is really great for,

00:38:14.820 --> 00:38:18.230
is this ad-hoc analysis in an
aggregate fashion on large

00:38:18.230 --> 00:38:19.360
chunks of data.

00:38:19.360 --> 00:38:22.920
So I would try that out, just
play around with that data.

00:38:22.920 --> 00:38:26.410
In terms of visualization, you
can actually see some of the

00:38:26.410 --> 00:38:29.130
demos from ClickView and BEAM,
who have built visualizations

00:38:29.130 --> 00:38:31.370
already on top of
that data set.

00:38:31.370 --> 00:38:34.350
And then use something like
Google Apps Script.

00:38:34.350 --> 00:38:39.070
In about 70 lines of code, you
can visualize data that comes

00:38:39.070 --> 00:38:42.420
from BigQuery queries in Google
Apps Script inside a

00:38:42.420 --> 00:38:43.390
spreadsheet.

00:38:43.390 --> 00:38:47.210
And then do graphs and pivot
tables and that sort of thing.

00:38:47.210 --> 00:38:49.360
And if you're a friend of Excel
instead, we have the big

00:38:49.360 --> 00:38:52.410
great connector for Excel that
we just launched a week or two

00:38:52.410 --> 00:38:55.620
ago that provides similar
functionality and a really

00:38:55.620 --> 00:38:56.910
easy way to get that.

00:38:56.910 --> 00:39:01.730
So you don't really need coding
experience right now.

00:39:01.730 --> 00:39:06.090
We aim ourselves as an API with
BigQuery for developers.

00:39:06.090 --> 00:39:08.770
But to get started in trying
it out, you don't actually

00:39:08.770 --> 00:39:11.790
need heavyweight coding
experience.

00:39:11.790 --> 00:39:13.310
So on the BigQuery side,
I would check some

00:39:13.310 --> 00:39:14.540
of that stuff out.

00:39:14.540 --> 00:39:17.210
App Engine MapReduce, I believe,
has a Getting Started

00:39:17.210 --> 00:39:19.680
like word count sort of
MapReduce that you can run.

00:39:19.680 --> 00:39:21.830
That's going to be a little bit
heavier coding a little

00:39:21.830 --> 00:39:23.810
bit heavier algorithms
understanding

00:39:23.810 --> 00:39:25.980
what's going on there.

00:39:25.980 --> 00:39:28.550
But hopefully, between those
two data sources.

00:39:28.550 --> 00:39:32.890
So in BigQuery just go to
developers.google.com/bigquery.

00:39:32.890 --> 00:39:34.360
And there's a Getting
Started guide there.

00:39:34.360 --> 00:39:34.685
And you can try it out.

00:39:34.685 --> 00:39:36.440
MICHAEL MANOOCHEHRI: Yeah, the
one you mentioned, the browser

00:39:36.440 --> 00:39:38.340
tool, so we have the BigQuery
browser tool, that's all

00:39:38.340 --> 00:39:39.120
through the web browser.

00:39:39.120 --> 00:39:40.430
And you can run some of
these queries for

00:39:40.430 --> 00:39:42.080
free by signing up.

00:39:42.080 --> 00:39:43.870
So, yeah, just go to the Getting
Started guide on

00:39:43.870 --> 00:39:46.186
developers.google.com/bigquery.

00:39:46.186 --> 00:39:47.020
RYAN BOYD: Yep.

00:39:47.020 --> 00:39:47.500
MICHAEL MANOOCHEHRI: That's the
best place to get started.

00:39:47.500 --> 00:39:48.490
MALE SPEAKER: Great,
great, great.

00:39:48.490 --> 00:39:50.800
RYAN BOYD: Well, thank
you for stopping by.

00:39:50.800 --> 00:39:52.560
It doesn't look like we actually
have any other

00:39:52.560 --> 00:39:54.450
questions in the Moderator
currently.

00:39:54.450 --> 00:39:58.080
So I think the Ryan and
Michael Show is about

00:39:58.080 --> 00:39:59.670
ready to sign off.

00:39:59.670 --> 00:40:01.610
But thank you all for joining.

00:40:01.610 --> 00:40:02.700
It's been a pleasure.

00:40:02.700 --> 00:40:04.300
And hopefully, we'll
continue this.

00:40:04.300 --> 00:40:07.650
And let us know your feedback
on Google+.

00:40:07.650 --> 00:40:11.180
Let us know your technical
questions on Stack Overflow.

00:40:11.180 --> 00:40:13.010
And we're excited to
keep on seeing how

00:40:13.010 --> 00:40:14.520
you're using BigQuery.

00:40:14.520 --> 00:40:14.950
Thanks.

00:40:14.950 --> 00:40:15.636
MICHAEL MANOOCHEHRI:
All right.

00:40:15.636 --> 00:40:16.010
Bye, everyone.

00:40:16.010 --> 00:40:16.580
RYAN BOYD: Goodbye.

00:40:16.580 --> 00:40:17.180
MALE SPEAKER: Bye.

00:40:17.180 --> 00:40:47.467
[MUSIC PLAYING]

