WEBVTT
Kind: captions
Language: en

00:00:00.770 --> 00:00:01.270
RAHUL: Hi.

00:00:01.270 --> 00:00:02.270
Thank you for attending.

00:00:02.270 --> 00:00:03.190
My name is Rahul.

00:00:03.190 --> 00:00:05.460
I do iOS and
Android at Pristine.

00:00:05.460 --> 00:00:07.510
And Aaron Alaniz
is with me as well,

00:00:07.510 --> 00:00:11.417
and he does Android
and Glass at Pristine.

00:00:11.417 --> 00:00:12.250
How do I click Next?

00:00:21.222 --> 00:00:22.230
AARON: Sorry about that.

00:00:22.230 --> 00:00:22.730
RAHUL: OK.

00:00:22.730 --> 00:00:25.520
So EyeSight is our
HIPAA-compliant communications

00:00:25.520 --> 00:00:26.050
solution.

00:00:26.050 --> 00:00:28.210
It's powered by WebRTC.

00:00:28.210 --> 00:00:30.340
And we try to keep the
UI and user experience

00:00:30.340 --> 00:00:31.701
with the physicians in mind.

00:00:31.701 --> 00:00:32.659
So what does that mean?

00:00:32.659 --> 00:00:36.050
That means minimal call
clicks and a simple call flow.

00:00:36.050 --> 00:00:38.290
And so why did we choose WebRTC?

00:00:38.290 --> 00:00:41.850
There's cross-platform native
support between platforms,

00:00:41.850 --> 00:00:44.895
including Android and
wearables, and iOS and browsers.

00:00:47.480 --> 00:00:49.570
Now I'm going to
dive into why and how

00:00:49.570 --> 00:00:51.820
building a WebRTC client
for Android or iOS

00:00:51.820 --> 00:00:54.140
is different than the browser.

00:00:54.140 --> 00:00:57.660
So here's a familiar diagram
you guys have all probably seen.

00:00:57.660 --> 00:01:00.070
The difference here is
that we replace the browser

00:01:00.070 --> 00:01:03.690
with libjingle PeerConnection,
the JAR and the SO.

00:01:03.690 --> 00:01:08.036
Now this is pretty much where
the difference comes in.

00:01:08.036 --> 00:01:10.160
I'm going to go into a
little bit more detail here.

00:01:10.160 --> 00:01:12.540
So what exactly are
we building when

00:01:12.540 --> 00:01:14.770
we go native for
Android and iOS?

00:01:14.770 --> 00:01:17.090
We need to compile
PeerConnection for the target

00:01:17.090 --> 00:01:18.760
platform, either iOS or Android.

00:01:18.760 --> 00:01:21.530
There's two parts, a mobile
layer and a native layer.

00:01:21.530 --> 00:01:27.030
Now, the native layer consists
of stuff like libvpx, libopus,

00:01:27.030 --> 00:01:30.100
voice and audio engine,
and bandwidth estimators.

00:01:30.100 --> 00:01:31.790
And the mobile side,
we have setting up

00:01:31.790 --> 00:01:34.320
the camera for the
platform, connecting

00:01:34.320 --> 00:01:35.970
the interface and protocols.

00:01:35.970 --> 00:01:38.080
And one thing I wanted
to mention as well

00:01:38.080 --> 00:01:42.010
is AppRTCDemo was very, very
important for our development.

00:01:42.010 --> 00:01:44.780
We needed to get AppRTCDemo
working correctly.

00:01:44.780 --> 00:01:46.560
And so we need to
understand AppRTCDemo

00:01:46.560 --> 00:01:50.780
before we can make our
own WebRTC solution.

00:01:50.780 --> 00:01:53.140
So as I was mentioning
earlier, that we

00:01:53.140 --> 00:01:56.510
had to make those
libjingle files,

00:01:56.510 --> 00:01:59.130
these platform-specific
APIs are not

00:01:59.130 --> 00:02:00.710
available for Android and iOS.

00:02:00.710 --> 00:02:03.080
For example, there's no
Maven or CocoaPods repo

00:02:03.080 --> 00:02:05.870
where a normal Android
or iOS user would just

00:02:05.870 --> 00:02:08.505
expect to be there and then
just pull it down immediately.

00:02:08.505 --> 00:02:11.130
This means we need to build the
libraries rather than expecting

00:02:11.130 --> 00:02:12.970
the platform to support it.

00:02:12.970 --> 00:02:16.150
And every revision change is
a pretty expensive update.

00:02:16.150 --> 00:02:17.390
We have to retest everything.

00:02:17.390 --> 00:02:19.260
The call might not
work after 90 minutes.

00:02:19.260 --> 00:02:20.860
The audio might drop out.

00:02:20.860 --> 00:02:22.820
There just could be
several problems.

00:02:22.820 --> 00:02:25.480
So we basically have to
go through a long testing

00:02:25.480 --> 00:02:26.730
process for each revision.

00:02:26.730 --> 00:02:31.050
So we're very careful
when we update revisions.

00:02:31.050 --> 00:02:35.760
So here's the WebRTC build
process that we follow through.

00:02:35.760 --> 00:02:39.150
So if you're targeting an
Android device or an iOS

00:02:39.150 --> 00:02:43.030
device, for Android you'll need
to install the Ubuntu build

00:02:43.030 --> 00:02:44.405
packages, and for
iOS you'll need

00:02:44.405 --> 00:02:46.300
to install the
Apple Xcode tools.

00:02:46.300 --> 00:02:48.919
That's a prerequisite
for every machine.

00:02:48.919 --> 00:02:50.960
And then after that, we'll
need to clone and pull

00:02:50.960 --> 00:02:53.930
depot tools, which is
our git and SVN client.

00:02:53.930 --> 00:02:56.144
WebRTC has so many
small libraries

00:02:56.144 --> 00:02:58.310
that it's pulling from that
we'll need a git and SVN

00:02:58.310 --> 00:03:02.300
client together to work to pull
all these products in sync.

00:03:02.300 --> 00:03:08.110
Once we get the depot tools set
up, we'll need to sync WebRTC

00:03:08.110 --> 00:03:11.220
by setting the target
platform to either iOS and Mac

00:03:11.220 --> 00:03:13.210
or Android and Unix.

00:03:13.210 --> 00:03:14.960
And then at that point,
we have an option.

00:03:14.960 --> 00:03:16.850
We can use a custom
gclient file to add

00:03:16.850 --> 00:03:19.960
in changes like camera or H.264.

00:03:19.960 --> 00:03:23.590
And then we prepare WebRTC
by running gclient runhooks.

00:03:23.590 --> 00:03:26.820
At that point, we have all
the make files there in place,

00:03:26.820 --> 00:03:29.420
and we can run ninja
AppRTCDemo, which

00:03:29.420 --> 00:03:33.670
will build all the native
code for AppRTCDemo.

00:03:33.670 --> 00:03:35.130
This will ensure
that we don't try

00:03:35.130 --> 00:03:37.840
to compile code that's not built
for the right target, which

00:03:37.840 --> 00:03:40.119
will almost guarantee a failure.

00:03:40.119 --> 00:03:41.660
And then you can
publish this archive

00:03:41.660 --> 00:03:44.650
into either an Archiva
repo or CocoaPods

00:03:44.650 --> 00:03:48.360
for an Android or iOS
developer to pull down.

00:03:48.360 --> 00:03:51.080
Or you can just use our build
scripts that we open source.

00:03:51.080 --> 00:03:52.290
They're really simple to use.

00:03:52.290 --> 00:03:53.790
All you have to do
is just pull down

00:03:53.790 --> 00:03:56.810
our GitHub repo and just
source the build script file.

00:03:56.810 --> 00:04:02.340
And you can do build_apprtc for
Android and dance for iPhone.

00:04:02.340 --> 00:04:03.950
For the iOS version
as well, I also

00:04:03.950 --> 00:04:07.580
add an Xcode project that
runs AppRTCDemo with an Xcode.

00:04:07.580 --> 00:04:11.370
So you guys don't have to worry
about all the complicated stuff

00:04:11.370 --> 00:04:13.260
underneath the hood.

00:04:13.260 --> 00:04:14.950
So I'm going to hand
it off to Aaron,

00:04:14.950 --> 00:04:18.930
who's going to explain a little
bit more of the EyeSight Core.

00:04:18.930 --> 00:04:20.450
AARON: Thanks, Rahul.

00:04:20.450 --> 00:04:22.660
So we spent a lot of time
familiarizing ourselves

00:04:22.660 --> 00:04:25.389
with the WebRTC repository,
the build process,

00:04:25.389 --> 00:04:27.930
and then the artifacts that are
sort of the byproduct of that

00:04:27.930 --> 00:04:29.016
build process.

00:04:29.016 --> 00:04:30.890
And we didn't want to
walk away empty-handed.

00:04:30.890 --> 00:04:34.630
So we set out to build what
we call EyeSight Core, which

00:04:34.630 --> 00:04:38.330
is a native WebRTC wrapper
library that allows us to build

00:04:38.330 --> 00:04:41.416
our client applications
on top of this.

00:04:41.416 --> 00:04:43.290
My talk's going to be
mainly Android-centric,

00:04:43.290 --> 00:04:45.080
but a lot of these
principles we used

00:04:45.080 --> 00:04:49.160
building out the iOS
application as well.

00:04:49.160 --> 00:04:51.750
So we set out to build the
core with a few goals in mind.

00:04:51.750 --> 00:04:54.960
And the main goal being, how do
we solve this general problem?

00:04:54.960 --> 00:04:58.464
How do we leverage the native
WebRTC libraries to build out

00:04:58.464 --> 00:04:59.880
a video streaming
solution that'll

00:04:59.880 --> 00:05:03.027
work cross-platform
natively for all devices?

00:05:03.027 --> 00:05:04.610
And as Rahul mentioned,
we spent a lot

00:05:04.610 --> 00:05:10.190
of time working with the
AppRTCDemo, which in our case

00:05:10.190 --> 00:05:15.250
is an actual Android AppRTCDemo
and the iOS AppRTCDemo,

00:05:15.250 --> 00:05:19.400
unlike the web version that's
available to a lot of you guys.

00:05:19.400 --> 00:05:21.910
And we poured over this
code, and we broke it down

00:05:21.910 --> 00:05:24.700
into three major components.

00:05:24.700 --> 00:05:26.940
So we have to have a
signaling module that

00:05:26.940 --> 00:05:31.060
handles what we call the
WebRTC dance to get a call up

00:05:31.060 --> 00:05:31.970
and running.

00:05:31.970 --> 00:05:34.450
And then we needed
some sort of wrappers

00:05:34.450 --> 00:05:36.420
to interact with the
native libraries.

00:05:36.420 --> 00:05:37.961
And then in addition
to that, we have

00:05:37.961 --> 00:05:43.320
to build out a client-specific
UX for each platform.

00:05:43.320 --> 00:05:46.602
So here's a bird's-eye
view of the core in action.

00:05:46.602 --> 00:05:48.060
So it's composed
of two components.

00:05:48.060 --> 00:05:49.560
So you have the
signaling component,

00:05:49.560 --> 00:05:51.620
which is interacting
with our signaling server

00:05:51.620 --> 00:05:54.540
and relaying stuff like the
ICE candidates, the offers,

00:05:54.540 --> 00:05:58.119
the answers, and sending
them across the line.

00:05:58.119 --> 00:05:59.910
And then we have our
PeerConnection wrapper

00:05:59.910 --> 00:06:03.515
that mediates the communication
with our application

00:06:03.515 --> 00:06:05.670
and with the native APIs.

00:06:05.670 --> 00:06:08.010
And collectively,
these two work together

00:06:08.010 --> 00:06:11.625
to push out very
client-consumable events

00:06:11.625 --> 00:06:13.630
for each application.

00:06:13.630 --> 00:06:15.070
So the iOS app
doesn't really care

00:06:15.070 --> 00:06:16.445
about receiving
an ICE candidate.

00:06:16.445 --> 00:06:19.080
It just really wants to know
about is there a call incoming?

00:06:19.080 --> 00:06:20.120
Is my call connected?

00:06:20.120 --> 00:06:20.920
Did they hang up?

00:06:20.920 --> 00:06:22.120
Did the call drop?

00:06:22.120 --> 00:06:25.800
So we wanted to abstract
all of the WebRTC dance

00:06:25.800 --> 00:06:28.520
into very client-consumable
events that

00:06:28.520 --> 00:06:32.450
can be built off of.

00:06:32.450 --> 00:06:34.986
So dive a little bit further
into what's inside the core.

00:06:34.986 --> 00:06:36.360
And as I mentioned,
the libjingle

00:06:36.360 --> 00:06:38.700
wrapper that we have
mediates that communication

00:06:38.700 --> 00:06:41.040
between our application
and the native libraries.

00:06:41.040 --> 00:06:43.030
In addition to that,
since we're trying

00:06:43.030 --> 00:06:46.050
to be very cross-platform
with our application,

00:06:46.050 --> 00:06:49.040
we have to configure
these STP packets

00:06:49.040 --> 00:06:51.220
for a different
context of a call.

00:06:51.220 --> 00:06:54.470
So you might have an iOS
device calling a Glass device,

00:06:54.470 --> 00:06:56.820
or an Android device
calling a Glass device,

00:06:56.820 --> 00:06:58.860
or a web browser
calling a Glass device.

00:06:58.860 --> 00:07:00.910
We handle all the
STP mangling inside

00:07:00.910 --> 00:07:02.357
of this portion of the core.

00:07:02.357 --> 00:07:04.690
In addition to that, probably
the most important feature

00:07:04.690 --> 00:07:07.330
of our wrapper is that it
encapsulates the interaction

00:07:07.330 --> 00:07:09.100
with the native
libraries in a way that's

00:07:09.100 --> 00:07:11.360
friendly with the
Android life cycle

00:07:11.360 --> 00:07:14.284
so that users can
opt into our app,

00:07:14.284 --> 00:07:16.450
and they feel like they're
inside of an Android app.

00:07:16.450 --> 00:07:18.283
They feel like they're
inside of an iOS app.

00:07:18.283 --> 00:07:21.791
It's not just some video
streaming application.

00:07:21.791 --> 00:07:23.790
And the signaling wrapper
is much, much simpler.

00:07:23.790 --> 00:07:25.104
We also implement Socket.IO.

00:07:25.104 --> 00:07:26.770
I know a lot of people
have been talking

00:07:26.770 --> 00:07:28.320
about how they use WebSockets.

00:07:28.320 --> 00:07:30.530
So for Android, we
use Android Async.

00:07:30.530 --> 00:07:33.710
It's a very good, very fast
Socket.IO implementation

00:07:33.710 --> 00:07:34.670
for Android.

00:07:34.670 --> 00:07:36.630
And on iOS, we use AZSocketIO.

00:07:36.630 --> 00:07:37.709
And this is much simpler.

00:07:37.709 --> 00:07:39.000
It's just a message dispatcher.

00:07:39.000 --> 00:07:40.600
It doesn't compose messages.

00:07:40.600 --> 00:07:44.010
It just gets a message from
our PeerConnection wrapper

00:07:44.010 --> 00:07:47.640
and sends it across the line.

00:07:47.640 --> 00:07:50.994
And I want to just show
a brief example of what

00:07:50.994 --> 00:07:52.660
I look at on a
day-to-day basis compared

00:07:52.660 --> 00:07:57.140
to what the standard WebRTC API
looks like in the JavaScript.

00:07:57.140 --> 00:08:00.300
And as you can see, it's
not really much different.

00:08:00.300 --> 00:08:02.387
Clearly, don't think that
you can build something

00:08:02.387 --> 00:08:03.220
off this right here.

00:08:03.220 --> 00:08:05.020
It's just a short example.

00:08:05.020 --> 00:08:09.290
But I just wanted to show the
similarities between the two.

00:08:09.290 --> 00:08:13.270
OK, so there are a handful of
WebRTC solutions out there.

00:08:13.270 --> 00:08:15.904
And I really wanted to
speak to our work on Glass

00:08:15.904 --> 00:08:18.070
because I feel like that's
what really sets us apart

00:08:18.070 --> 00:08:21.400
as a technology company.

00:08:21.400 --> 00:08:23.850
So before that, though,
we have to talk about some

00:08:23.850 --> 00:08:25.790
of the constraints that
come with developing

00:08:25.790 --> 00:08:28.370
on a platform like Google
Glass, or any other wearable

00:08:28.370 --> 00:08:29.990
device for that matter.

00:08:29.990 --> 00:08:33.100
And some of the obvious things
are these hardware bottlenecks

00:08:33.100 --> 00:08:35.517
that you don't really have to
think about whenever-- well,

00:08:35.517 --> 00:08:37.058
you have to think
about a little bit,

00:08:37.058 --> 00:08:38.990
but not definitely
as much for something

00:08:38.990 --> 00:08:40.520
like a wearable device.

00:08:40.520 --> 00:08:42.940
So these glasses,
you know, physicians

00:08:42.940 --> 00:08:45.060
are walking around
their Wi-Fi networks.

00:08:45.060 --> 00:08:48.660
And they're walking in and out
of good and bad Wi-Fi spots.

00:08:48.660 --> 00:08:50.820
I know the guy
from that last team

00:08:50.820 --> 00:08:52.970
made a really good comment
about most of the time

00:08:52.970 --> 00:08:55.060
it's just a problem
with the Wi-Fi.

00:08:55.060 --> 00:08:56.980
But they don't really care.

00:08:56.980 --> 00:09:00.220
So we have to build around
that, especially with Glass,

00:09:00.220 --> 00:09:03.270
because the response
time to Wi-Fi events

00:09:03.270 --> 00:09:04.910
is definitely a little
bit more delayed

00:09:04.910 --> 00:09:07.910
than a traditional
Android or iOS device.

00:09:07.910 --> 00:09:12.030
In addition to that, the CPU
is a real main concern of ours.

00:09:12.030 --> 00:09:14.530
So we use VP8 to obviously
encode and decode

00:09:14.530 --> 00:09:17.720
frames inside our core library.

00:09:17.720 --> 00:09:20.410
And we see great
performance between an iOS

00:09:20.410 --> 00:09:23.040
and a regular Android
device because the CPU power

00:09:23.040 --> 00:09:25.330
is there to do the VP8
encoding and decoding.

00:09:25.330 --> 00:09:27.090
But we definitely see
a bottleneck effect

00:09:27.090 --> 00:09:30.220
whenever Glass is forced to
do this sort of VP8 encoding

00:09:30.220 --> 00:09:31.470
in software.

00:09:31.470 --> 00:09:34.770
We are trying to work
with Glass specifically

00:09:34.770 --> 00:09:38.230
to try and get
some H.264 support.

00:09:38.230 --> 00:09:43.989
It's hard to guarantee any
sort of great performance

00:09:43.989 --> 00:09:44.530
improvements.

00:09:44.530 --> 00:09:46.530
But we have been working
with that as well.

00:09:46.530 --> 00:09:48.420
In addition to all of
that, battery is also

00:09:48.420 --> 00:09:50.700
a huge concern of ours
because like I said,

00:09:50.700 --> 00:09:52.960
these devices are in
constant movement.

00:09:52.960 --> 00:09:54.550
They're walking
from room to room.

00:09:54.550 --> 00:09:56.650
In some cases, they're
even in an ambulance.

00:09:56.650 --> 00:09:59.300
So we have to build
our app with the notion

00:09:59.300 --> 00:10:03.830
that these applications
need to last all day.

00:10:03.830 --> 00:10:06.080
And in addition to all
that-- UX limitations

00:10:06.080 --> 00:10:07.470
for a wearable device.

00:10:07.470 --> 00:10:10.180
It's completely different.

00:10:10.180 --> 00:10:13.990
We have to really simplify the
application for a wearable user

00:10:13.990 --> 00:10:15.990
so that they don't really
feel confused whenever

00:10:15.990 --> 00:10:18.819
a call's coming in
or they're in a call.

00:10:18.819 --> 00:10:21.110
We really want them to feel
like everything makes sense

00:10:21.110 --> 00:10:22.109
while they're in a call.

00:10:24.706 --> 00:10:26.830
So with that, I'm going to
talk about some features

00:10:26.830 --> 00:10:30.280
that we built out
specifically for Glass.

00:10:30.280 --> 00:10:33.270
Yes, I have thank
our business dev guys

00:10:33.270 --> 00:10:34.510
because this is really good.

00:10:37.070 --> 00:10:40.210
So I'm sure we all,
in some capacity,

00:10:40.210 --> 00:10:42.570
are familiar with the
AppRTCDemo on the web.

00:10:42.570 --> 00:10:46.500
And the standard demo is
showing your local video

00:10:46.500 --> 00:10:49.070
in the top right-hand corner,
and then the remote video

00:10:49.070 --> 00:10:51.715
gets most of the
screen real estate.

00:10:51.715 --> 00:10:53.590
We didn't really have
a compelling use case

00:10:53.590 --> 00:10:58.139
for showing the person calling
a Glass device's video.

00:10:58.139 --> 00:10:59.680
In addition to that,
we didn't really

00:10:59.680 --> 00:11:02.912
want to waste any
decoding time trying

00:11:02.912 --> 00:11:05.370
to decode these frames coming
across the line from a client

00:11:05.370 --> 00:11:06.190
user.

00:11:06.190 --> 00:11:08.640
So what we decided
to do was just only

00:11:08.640 --> 00:11:09.819
send video from Glass.

00:11:09.819 --> 00:11:11.360
And if you're calling
a Glass device,

00:11:11.360 --> 00:11:13.600
you don't have to worry
about sending your video.

00:11:13.600 --> 00:11:17.490
And Glass is not even going
to receive it if you try it.

00:11:17.490 --> 00:11:21.480
So this allows us to preserve
battery life on Glass

00:11:21.480 --> 00:11:24.956
and build a UX that make sense.

00:11:24.956 --> 00:11:26.330
And so, on the
left there you can

00:11:26.330 --> 00:11:29.800
see the early
implementation of our app.

00:11:29.800 --> 00:11:32.140
And it has a black screen
for the Glass user.

00:11:32.140 --> 00:11:34.390
And then of course, across
the line on the other side,

00:11:34.390 --> 00:11:36.940
you see the game of Operation.

00:11:36.940 --> 00:11:39.730
Very quickly we found
that Glass users

00:11:39.730 --> 00:11:41.780
want to see what
they're sending.

00:11:41.780 --> 00:11:43.710
A good example of this
is when a surgeon's

00:11:43.710 --> 00:11:48.012
operating in the
room, their vision

00:11:48.012 --> 00:11:51.080
is not necessarily looking
at what is probably

00:11:51.080 --> 00:11:52.860
the most important
part of the call.

00:11:52.860 --> 00:11:54.430
They might be looking
straight ahead,

00:11:54.430 --> 00:11:57.360
or they might be
looking down like this.

00:11:57.360 --> 00:12:00.300
So we added a viewfinder
so that they can line up

00:12:00.300 --> 00:12:02.687
their line of vision with
what's going across the line

00:12:02.687 --> 00:12:03.270
during a call.

00:12:05.850 --> 00:12:08.792
And to add on to that,
since we don't really

00:12:08.792 --> 00:12:11.250
have a use case for showing
local video when you're calling

00:12:11.250 --> 00:12:14.380
a wearable device, whenever you
call a wearable device on any

00:12:14.380 --> 00:12:18.450
of our applications,
you get the full screen

00:12:18.450 --> 00:12:20.790
of that wearable device.

00:12:20.790 --> 00:12:24.110
So on the left there,
we have a nice picture

00:12:24.110 --> 00:12:26.230
of our capital in Texas.

00:12:26.230 --> 00:12:30.060
So you can get a feel for
the entire screen real estate

00:12:30.060 --> 00:12:32.470
from a wearable device.

00:12:32.470 --> 00:12:36.250
And just to juxtapose that, we
have an iOS to Android call.

00:12:36.250 --> 00:12:37.959
And since we are
sending video both ways,

00:12:37.959 --> 00:12:39.791
you definitely want to
show your local video

00:12:39.791 --> 00:12:41.170
so that they can
see what they're

00:12:41.170 --> 00:12:43.540
sending across the line.

00:12:43.540 --> 00:12:46.830
OK, so with that, I want
to do a quick demo here.

00:12:46.830 --> 00:12:50.030
Hopefully it goes OK.

00:12:50.030 --> 00:12:53.260
So Rahul's got his glasses on,
and I've got my glasses on.

00:12:53.260 --> 00:12:55.090
He's going to call
from his iOS device,

00:12:55.090 --> 00:12:59.890
and I'm going to call
from my Glass device.

00:12:59.890 --> 00:13:03.260
RAHUL: OK, Glass,
start EyeSight.

00:13:03.260 --> 00:13:06.710
AARON: And as you can see, you
can share voices with Glass.

00:13:06.710 --> 00:13:07.946
RAHUL: It's good for testing.

00:13:07.946 --> 00:13:11.282
It's really good for testing.

00:13:11.282 --> 00:13:11.990
AARON: All right.

00:13:11.990 --> 00:13:13.290
So I'm calling.

00:13:13.290 --> 00:13:16.260
And you should see a
little prompt come up.

00:13:16.260 --> 00:13:17.090
And I apologize.

00:13:17.090 --> 00:13:19.382
The lag is pretty
apparent whenever

00:13:19.382 --> 00:13:21.090
you're doing Android
Screen Monitor here.

00:13:21.090 --> 00:13:22.680
So I'm answering the call.

00:13:22.680 --> 00:13:23.390
And there you go.

00:13:23.390 --> 00:13:28.930
You see-- oh, I think
I lost the call here.

00:13:28.930 --> 00:13:31.020
Anyway, on the
left there, you see

00:13:31.020 --> 00:13:32.780
Rahul showing off
the viewfinder.

00:13:32.780 --> 00:13:35.080
And if he actually
swipes back on the D-pad,

00:13:35.080 --> 00:13:36.620
it'll actually disappear.

00:13:36.620 --> 00:13:39.614
And we can use this to save
battery life in situations

00:13:39.614 --> 00:13:41.530
where you don't really
need to line up a call.

00:13:41.530 --> 00:13:43.404
When you slide forward,
it brings it back up.

00:13:46.590 --> 00:13:49.900
I'm wondering what
happened to mine.

00:13:49.900 --> 00:13:50.650
Oops.

00:13:50.650 --> 00:13:51.240
I'm sorry.

00:13:51.240 --> 00:13:54.590
I just took a
picture of everyone.

00:13:54.590 --> 00:13:55.575
Wrong application.

00:13:59.040 --> 00:14:01.241
AUDIENCE: We all
signed disclaimers.

00:14:01.241 --> 00:14:01.740
AARON: Yeah.

00:14:05.548 --> 00:14:08.430
Oh, interesting.

00:14:08.430 --> 00:14:12.240
Anyways, so that's
really all I have.

00:14:12.240 --> 00:14:13.610
I'm going to wrap it up here.

00:14:13.610 --> 00:14:15.710
So if you're interested in
what we're doing at Pristine,

00:14:15.710 --> 00:14:17.293
you can check out
our engineering blog

00:14:17.293 --> 00:14:18.600
at tech.pristine.io.

00:14:18.600 --> 00:14:21.345
We like to blog about
once or twice a month.

00:14:21.345 --> 00:14:24.535
And at this time, I'll open
the floor for any questions.

