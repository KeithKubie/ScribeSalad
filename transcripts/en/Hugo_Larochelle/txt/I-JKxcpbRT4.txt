Speaker 1:          00:00          Okay.

Speaker 2:          00:01          In this video we'll introduce the concept of pooling in sub sampling from convolutional neural networks. So we're ready now to look at this third idea that characterizes a convolutional neural networks, which is the idea of pooling and sub sampling hidden units in a convolutional neural net.

Speaker 1:          00:19          Yeah,

Speaker 2:          00:19          so we talked before, thanks to the local connectivity of the hidden units and with parameters sharing that we could compute, uh, the activation of a hidden there, a convolutional neural network by, uh, operating, doing multiple convolutions, one for each input channel between the input channel and, uh, some feature map summing over all the input channels to aggregate all the pre activations from all the input channels. And then applying like we usually do in feed for known network, some nonlinearity on the, uh, p activations. And then that would give us the activation of the hidden units in some given feature map. So this formula might characterize for one j the value of the feature map. And then we would have in the whole hidden layer, usually multiple feature maps. So we'd repeat this operation, but with different convolutional kernels. So with different hidden weights, matrix w and so this operation of multiple convolutions with nonlinearity with defined the activation value of a convolutional, a hidden layer.

Speaker 1:          01:31          Yeah.

Speaker 2:          01:31          On top of this, we'll usually add another operation, which we call pooling and sub sampling. So, uh, our first define what it does and then I'll describe a intuitively why it's a, it's a good thing to have. The idea of fooling is dia essentially corresponds to taking a set of a hidden units and aggregating their activation somehow to obtain just a single number. So one thing we could do, and this is referred to as Max pooling, is to take all the hidden units in some given feature map in a dollar in the same local region. Uh, so remember feature map. We can think of it as an image or we can, they're there locally, uh, uh, placed in the, uh, in the feature maps. So we can take just a passionate for, if I want to think about as a receptive field. And then, uh, in that region taking all of the activations of the hidden unit with Max pooling would be just taking, uh, so, uh, only taking the maximum value of all the hidden units, which is illustrated here.

Speaker 2:          02:34          So with some index p and Q, which indexes some local neighborhood in, uh, the, uh, in some region in the feature map, we just take the maximum value in that local neighborhood and that would then become the new value at a given position in the subsequent, uh, layer of our conclusion or neural network. And this pooling, we will normally do it over non overlapping neighborhoods. So if we have a big feature map we could say divided into three like this. And then in each neighborhood who would actually take individually for each neighborhood, the maximum, and that would give us, say maybe the, it was like nine by five, a nine by nine for this, say this was a feature map that was done by nine by nine. So there were actually three activations within each neighborhood. Then after the pooling operation, because we did a makeover, none overlapping neighborhoods who would actually get a matrix. That's only three by three. Okay. And then each value here would be the maximum activation within the curse funding local neighborhood. So that's why we say it's pooling. The pooling corresponds to taking the Max and then sub sampling because we have a, a smaller matrix with fewer rows and columns after we've done this operation.

Speaker 2:          04:03          So just know this also, again, I'm using the rotation from Jared. The way that the finding different operations is that the input of the operation is always x and then the output is always y. So x here would actually be from my previous slide, the, uh, computation of this, uh, of these convolutions plus nonlinearity. So this would be the x that we see here. And then we'd get an additional subsequent layer here, which would be the pooling, some sampling layer, which Chris Carr with curse bond in this case to doing Max pooling and other, uh, variants of, or an alternative of the Max pooling is to do average pooling. So if the size of the neighborhood over which I'm doing pooling is m, then I just take the average of all the activations in that neighborhood. So I would take the sum and divided by m squared. So that's uh, called average pooing instead of Max pooling?

Speaker 2:          05:06          No. Um, there are, uh, certain advantages to pooling some sampling operation. The first one, which is perhaps the most obvious, is that it reduces the number of hidden units in the next hidden layer. And a, this is because of the sub sampling operation essentially, and a f on that hidden layer. We were to then perform some more, uh, convolutions with nonlinearity. Then this means that computing, this subsequent convolutional layer will be more efficient because there will be a, the size of the, uh, uh, the feature maps that are fed as input to these new convolutions, uh, will be smaller so there will be more efficient to compute. So that's a first advantage. Another advantage is that it introduces some invariants to local translation. So that's actually mostly true for the Max operation. So, uh, essentially if the maximum value in some neighborhood, uh, doesn't change after we've apply on the image is small translation.

Speaker 2:          06:10          In other words, if the maximum value in the neighborhoods still remains in the same, uh, in the same neighborhood after the trends, the translation, it means that after pooling the value of the, uh, the pool, the pool value will not change because the Max is still the same. And so intuitively it means that if we have local translations like this where the Max remains in the same our local neighborhoods everywhere, that it means we get exactly the same pool than subsample a hidden layer in the compulsion of neural network. So this way we were able to build some sort of local translation in variants, which is useful for convolutional neural networks because, uh, for object recognition say, because the position of the object does not necessarily define the actual identity of the object.

Speaker 2:          07:02          So here's an example of two images over which we, uh, perform a convolution followed by Max pooling. Uh, so this is actually a next step after the conclusion we saw in the previous video where we did a couple solution with a filter with a zero zero, 0.5 0.5, and then we applied to nonlinearity. Well, turns out if I do this convolution plus nonlinearity and then I do a, uh, a Max pooling operation, I get exactly the same pool, the value of a pool, and some sample value of the hidden layer for both these images and a, and so, you know, in this case it's hard to see how both would be somewhat equivalent. They are cool in the sense that both have nothing happening here. The only thing that moved really is this, that went up. Uh, and so this was a, this, and then we see that the hidden, they are sort of reflected the fact that nothing changed really up to very small local translation. Uh, operations.