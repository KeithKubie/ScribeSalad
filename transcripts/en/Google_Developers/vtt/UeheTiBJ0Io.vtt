WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.669
[MUSIC PLAYING]

00:00:03.669 --> 00:00:04.960
FRANCOIS CHOLLET: Hi, everyone.

00:00:04.960 --> 00:00:06.501
I'm Francois, and
I'll be telling you

00:00:06.501 --> 00:00:09.750
about the integration of the
Keras API into TensorFlow.

00:00:09.750 --> 00:00:12.171
So first of all, what is Keras?

00:00:12.171 --> 00:00:13.920
If you know about
TensorFlow, you probably

00:00:13.920 --> 00:00:15.999
also know about Keras,
but just in case,

00:00:15.999 --> 00:00:17.040
for those who don't know.

00:00:17.040 --> 00:00:21.360
Keras is an API that makes
building deep-learning models

00:00:21.360 --> 00:00:23.290
easier and faster.

00:00:23.290 --> 00:00:26.850
So it's a deep-learning toolbox,
and it's all about ease of use,

00:00:26.850 --> 00:00:30.390
reducing complexity,
reducing cognitive load.

00:00:30.390 --> 00:00:33.040
And if you make deep learning
easier to use, what happens

00:00:33.040 --> 00:00:36.580
is that you're also making
it accessible to more people.

00:00:36.580 --> 00:00:38.700
So the key idea with Keras
is to put deep learning

00:00:38.700 --> 00:00:40.530
into the hands of everyone.

00:00:40.530 --> 00:00:42.990
We believe that, in the
future, deep learning

00:00:42.990 --> 00:00:45.720
will be part of every
developer's toolbox.

00:00:45.720 --> 00:00:48.690
It will not just be a tool
for experts and researchers.

00:00:48.690 --> 00:00:50.100
It will be into
everyone's hands.

00:00:50.100 --> 00:00:53.170
And this will happen with
platforms like TensorFlow

00:00:53.170 --> 00:00:55.750
and APIs like Keras.

00:00:55.750 --> 00:00:59.980
So Keras is better
understood not as a library

00:00:59.980 --> 00:01:01.770
or as a code base.

00:01:01.770 --> 00:01:03.420
It's more of an API spec.

00:01:03.420 --> 00:01:05.670
And it's an API spec that
has several different

00:01:05.670 --> 00:01:06.900
implementations.

00:01:06.900 --> 00:01:08.670
There's the Theano
implementations,

00:01:08.670 --> 00:01:12.180
which was the original
Keras [AUDIO OUT] released

00:01:12.180 --> 00:01:13.200
two years ago.

00:01:13.200 --> 00:01:15.314
There's a TensorFlow
implementation, of course.

00:01:15.314 --> 00:01:17.730
And then there will be more
implementations in the future.

00:01:17.730 --> 00:01:20.880
For instance, Microsoft is
working on a [INAUDIBLE]

00:01:20.880 --> 00:01:22.440
implementation of Keras.

00:01:22.440 --> 00:01:26.860
And so until now, the
TensorFlow implementation

00:01:26.860 --> 00:01:30.390
separates [INAUDIBLE]
with lots of uses.

00:01:30.390 --> 00:01:33.270
And what we are doing now is
that we are bringing the Keras

00:01:33.270 --> 00:01:35.670
API directly into
the Keras project,

00:01:35.670 --> 00:01:38.169
into the TensorFlow project.

00:01:38.169 --> 00:01:39.960
We believe that's a
big step towards making

00:01:39.960 --> 00:01:42.900
TensorFlow and deep learning
accessible to everyone.

00:01:42.900 --> 00:01:44.820
So what's concretely
happening is

00:01:44.820 --> 00:01:47.910
that we are bringing into
TensorFlow this Keras

00:01:47.910 --> 00:01:50.100
compatibility module,
tf.keras, which

00:01:50.100 --> 00:01:52.680
is an implementation
of the Keras spec

00:01:52.680 --> 00:01:55.620
that's built from the
ground up for TensorFlow.

00:01:55.620 --> 00:01:57.510
And so as part of
this move, we're

00:01:57.510 --> 00:02:00.510
introducing into core
TensorFlow new data structures

00:02:00.510 --> 00:02:03.230
like layers, which
Martin just introduced.

00:02:03.230 --> 00:02:07.340
And also models,
which are containers

00:02:07.340 --> 00:02:08.660
for graphs of layers.

00:02:08.660 --> 00:02:11.220
So these used to be
Keras data structures,

00:02:11.220 --> 00:02:14.070
and now they will be shared
between core TensorFlow

00:02:14.070 --> 00:02:16.620
and this tf.keras module.

00:02:16.620 --> 00:02:20.340
So, because this
Keras implementation

00:02:20.340 --> 00:02:22.374
is built from the
ground up on top

00:02:22.374 --> 00:02:23.790
of the shell of
the structures, it

00:02:23.790 --> 00:02:27.390
is fully compatible with all of
the TensorFlow functionality.

00:02:27.390 --> 00:02:31.350
That means it's also compatible
with [AUDIO OUT] features that

00:02:31.350 --> 00:02:35.950
has the experiment API
that Martin introduced.

00:02:35.950 --> 00:02:39.180
So if you're a Keras user,
here's what this means for you.

00:02:39.180 --> 00:02:42.060
First, it gives you access
to more flexibility.

00:02:42.060 --> 00:02:45.150
You become able to very
easily mix and match

00:02:45.150 --> 00:02:48.000
pure TensorFlow functionality
with Keras functionality.

00:02:48.000 --> 00:02:51.210
So you can build more
powerful, more flexible models.

00:02:51.210 --> 00:02:54.480
And what's more, you gain
access to really advanced,

00:02:54.480 --> 00:02:57.720
really powerful features
through the [INAUDIBLE]

00:02:57.720 --> 00:03:01.080
TensorFlow training API
that Martin mentioned.

00:03:01.080 --> 00:03:04.260
You gain access to very quick
and easy distributed training

00:03:04.260 --> 00:03:05.310
for Keras models.

00:03:05.310 --> 00:03:07.980
You gain access to training
on Cloud ML, distributed

00:03:07.980 --> 00:03:10.740
hyperparameter training, which
is really, really powerful,

00:03:10.740 --> 00:03:13.020
and you can very easily
deploy your Keras modules

00:03:13.020 --> 00:03:15.740
to production with
TensorFlow serving.

00:03:15.740 --> 00:03:18.310
If you're a
TensorFlow user, what

00:03:18.310 --> 00:03:20.550
does this change mean to you?

00:03:20.550 --> 00:03:25.440
Basically, it gives you access
to the full scope of the Keras

00:03:25.440 --> 00:03:27.930
API to make your life
easier without leaving

00:03:27.930 --> 00:03:30.000
your existing
TensorFlow workflow.

00:03:30.000 --> 00:03:32.075
So you can just start
using the Keras API

00:03:32.075 --> 00:03:33.709
at no loss of flexibility.

00:03:33.709 --> 00:03:35.250
You don't have to
adopt all of Keras,

00:03:35.250 --> 00:03:38.490
you can just adopt the layers
you need, for instance.

00:03:38.490 --> 00:03:43.320
And also, it gives you
access to any existing piece

00:03:43.320 --> 00:03:44.840
of open source Keras code.

00:03:44.840 --> 00:03:47.790
You can just go on GitHub,
copy some piece of Keras code,

00:03:47.790 --> 00:03:51.390
drop it into your codebase,
and, by just changing imports,

00:03:51.390 --> 00:03:52.346
it will just work.

00:03:52.346 --> 00:03:54.720
So this gives you access to
a lot of existing open source

00:03:54.720 --> 00:03:57.820
code that you can start
leveraging into your TensorFlow

00:03:57.820 --> 00:03:59.290
workflow.

00:03:59.290 --> 00:04:01.270
So to make things a
bit more concrete,

00:04:01.270 --> 00:04:03.460
I will walk you
through an example

00:04:03.460 --> 00:04:06.420
of what your workflow will
look like when using Keras

00:04:06.420 --> 00:04:07.725
with TensorFlow.

00:04:07.725 --> 00:04:12.040
And we'll be looking at a
video question answering model,

00:04:12.040 --> 00:04:15.574
and we'll be using
Keras to define a model

00:04:15.574 --> 00:04:16.990
to solve this
problem, and we will

00:04:16.990 --> 00:04:19.570
retrain it using the
high-level TensorFlow training

00:04:19.570 --> 00:04:22.980
API in a distributed setting.

00:04:22.980 --> 00:04:25.710
So this is a problem
that we'll be looking at.

00:04:25.710 --> 00:04:29.790
We have some videos, which
are sampled about four frames

00:04:29.790 --> 00:04:32.310
per second, roughly
10 seconds per video,

00:04:32.310 --> 00:04:35.290
so we have about 40
frames per video.

00:04:35.290 --> 00:04:38.110
And we are asking
questions about the video,

00:04:38.110 --> 00:04:41.250
about what's going on in the
video, and about the contents.

00:04:41.250 --> 00:04:44.940
Here we have this video
of a woman packing objects

00:04:44.940 --> 00:04:45.630
into a car.

00:04:45.630 --> 00:04:47.200
You can ask, what
is the woman doing?

00:04:47.200 --> 00:04:48.450
What's the color of her shirt?

00:04:48.450 --> 00:04:51.310
And the answer should
be a single word.

00:04:51.310 --> 00:04:53.400
So we'll build a
deep-learning model

00:04:53.400 --> 00:04:57.130
that it will take as inputs the
video as a sequence of frames,

00:04:57.130 --> 00:05:00.900
so about 40 frames in
order, and the question,

00:05:00.900 --> 00:05:04.410
and the model should output
one word, the answer.

00:05:04.410 --> 00:05:07.710
And so if you ask this question,
what is the woman doing?

00:05:07.710 --> 00:05:08.614
She's packing.

00:05:08.614 --> 00:05:10.530
It's actually interesting
because, if you only

00:05:10.530 --> 00:05:14.400
take one frame and try
to train a [INAUDIBLE],

00:05:14.400 --> 00:05:16.290
for instance, to
answer the question,

00:05:16.290 --> 00:05:18.720
the problem is that
just from one frame,

00:05:18.720 --> 00:05:21.450
she could also be
unpacking as well, right?

00:05:21.450 --> 00:05:23.430
And the reason you know
she's actually packing

00:05:23.430 --> 00:05:24.870
is because of the
order of the frames.

00:05:24.870 --> 00:05:27.260
If you look at the second frame,
the rug is outside the car.

00:05:27.260 --> 00:05:28.884
In the next frame,
it's inside the car.

00:05:28.884 --> 00:05:30.730
And so we expect all
the planning model

00:05:30.730 --> 00:05:33.750
to be able to leverage this
order to correctly answer

00:05:33.750 --> 00:05:36.190
this type of question.

00:05:36.190 --> 00:05:39.190
So needless to say, this is a
tremendously difficult problem.

00:05:39.190 --> 00:05:41.290
And just a few years
ago, three or four years

00:05:41.290 --> 00:05:44.050
ago before TensorFlow,
before Keras,

00:05:44.050 --> 00:05:46.690
this would only have been
accessible to a handful

00:05:46.690 --> 00:05:50.140
of large companies or in very
well-funded research labs,

00:05:50.140 --> 00:05:53.890
especially if you wanted
to make your model

00:05:53.890 --> 00:05:56.350
train on a cluster of GPUs.

00:05:56.350 --> 00:05:58.960
And now with TensorFlow
as a platform

00:05:58.960 --> 00:06:01.480
and with Keras as
an API, we're making

00:06:01.480 --> 00:06:03.910
these three difficult
problems accessible to anyone

00:06:03.910 --> 00:06:06.090
with just basic [INAUDIBLE]
scripting capabilities.

00:06:06.090 --> 00:06:08.131
So we are putting the full
power of deep learning

00:06:08.131 --> 00:06:10.270
into the hands of everyone.

00:06:10.270 --> 00:06:12.970
So here's a model that you
would use to solve this problem.

00:06:12.970 --> 00:06:15.670
At the high level,
it has two branches.

00:06:15.670 --> 00:06:19.570
There's one branch that's meant
to encode the video inputs

00:06:19.570 --> 00:06:22.510
into a single vector
and one branch meant

00:06:22.510 --> 00:06:24.850
to encode the question,
so a sequence of words,

00:06:24.850 --> 00:06:26.620
into a single vector.

00:06:26.620 --> 00:06:29.230
So we have one vector,
including information

00:06:29.230 --> 00:06:31.210
about the entire
video, one vector,

00:06:31.210 --> 00:06:33.270
including information
about the entire question.

00:06:33.270 --> 00:06:35.410
And then we concatenate
these two vectors.

00:06:35.410 --> 00:06:37.990
And this gives us
one single vector

00:06:37.990 --> 00:06:40.330
that encodes information
about the entire problem,

00:06:40.330 --> 00:06:43.261
about the combination of
the video and the question.

00:06:43.261 --> 00:06:45.760
And that's kind of the beauty
and the magic of deep learning

00:06:45.760 --> 00:06:48.830
is that you take perceptual
inputs, like a video,

00:06:48.830 --> 00:06:50.710
for instance, you
take semantic meaning,

00:06:50.710 --> 00:06:57.010
and you're embedding this
perceptual input [AUDIO OUT]

00:06:57.010 --> 00:07:00.350
And you're embedding this
meaning into geometric space.

00:07:00.350 --> 00:07:02.500
You're turning
meaning into vectors

00:07:02.500 --> 00:07:05.310
and then you can learn
interesting transformations

00:07:05.310 --> 00:07:07.040
of these geometric spaces.

00:07:07.040 --> 00:07:09.130
So that's really the
beauty of deep learning.

00:07:09.130 --> 00:07:11.500
And so in our case, we'll
be taking this vector

00:07:11.500 --> 00:07:14.539
and coding the entire problem.

00:07:14.539 --> 00:07:17.080
And we will be training a fully
connected [INAUDIBLE] on top.

00:07:17.080 --> 00:07:19.690
And it will end
up with a softmax

00:07:19.690 --> 00:07:22.720
over a predefined
vocabulary, and so the word

00:07:22.720 --> 00:07:25.330
with the highest probability
in this vocabulary

00:07:25.330 --> 00:07:27.839
will be the answer
to the question.

00:07:27.839 --> 00:07:30.130
So now if you want to look
at a higher level of detail,

00:07:30.130 --> 00:07:32.650
how do we turn a video
into a single vector?

00:07:32.650 --> 00:07:35.010
How do we turn a
question into a vector?

00:07:35.010 --> 00:07:38.140
So for the video, we're
starting out with the video

00:07:38.140 --> 00:07:39.590
as a sequence of frames.

00:07:39.590 --> 00:07:41.760
So each frame is just an image.

00:07:41.760 --> 00:07:46.080
So it's RGB, it has a height,
a width, a depth, color depths.

00:07:46.080 --> 00:07:48.010
The other sequence
of frame, you use

00:07:48.010 --> 00:07:51.370
a [INAUDIBLE] to transform each
frame into a single vector.

00:07:51.370 --> 00:07:54.040
So essentially you're
using the conditional base

00:07:54.040 --> 00:07:56.085
of a pre-trained
network, and you

00:07:56.085 --> 00:07:58.060
do some pulling to
extract one vector that

00:07:58.060 --> 00:08:00.990
encodes the visual
contents of the frame.

00:08:00.990 --> 00:08:03.490
And so what you get out
of this is the video

00:08:03.490 --> 00:08:06.040
anchored as a
sequence of vectors,

00:08:06.040 --> 00:08:09.730
where each vector encodes the
visual contents of one frame.

00:08:09.730 --> 00:08:12.880
And then you take the sequence
and you run it through an LSTM.

00:08:12.880 --> 00:08:14.510
So you probably know
what an LSTM is,

00:08:14.510 --> 00:08:17.350
but basically it's a
[INAUDIBLE] type of network

00:08:17.350 --> 00:08:22.960
that can process sequences
and takes into account order.

00:08:22.960 --> 00:08:26.230
And this LSTM will output
a single vector encoding

00:08:26.230 --> 00:08:29.830
information about
the entire video.

00:08:29.830 --> 00:08:31.510
To turn a question
into a vector,

00:08:31.510 --> 00:08:34.130
we are following a
very similar process.

00:08:34.130 --> 00:08:36.520
We represent a
question as a sequence

00:08:36.520 --> 00:08:39.530
of integers, each integer
standing for a word.

00:08:39.530 --> 00:08:41.870
And then we map each
word to a word vector

00:08:41.870 --> 00:08:45.040
via a process called embedding.

00:08:45.040 --> 00:08:49.330
So we get out of this a
sequence of word expressed

00:08:49.330 --> 00:08:50.574
as a sequence of vectors.

00:08:50.574 --> 00:08:51.990
We run it through
a different LSTM

00:08:51.990 --> 00:08:55.120
layer, which will encode
the entire question

00:08:55.120 --> 00:08:57.290
as a single vector.

00:08:57.290 --> 00:08:59.060
So in Keras, here's
the architecture

00:08:59.060 --> 00:09:03.110
that we would use to
implement this model,

00:09:03.110 --> 00:09:05.690
and it's literally just
a very simple translation

00:09:05.690 --> 00:09:07.200
of the previous graph.

00:09:07.200 --> 00:09:09.950
So for the video
encoder, basically, we

00:09:09.950 --> 00:09:13.390
are starting from the
video as a 5D tensor.

00:09:13.390 --> 00:09:15.980
The first axis will
be the batch axis.

00:09:15.980 --> 00:09:17.540
Then you have the time axis.

00:09:17.540 --> 00:09:21.495
Then you have a 3D tensor,
which encodes a frame.

00:09:21.495 --> 00:09:25.590
And so we will apply an
Inception v3 network,

00:09:25.590 --> 00:09:30.920
pre-trained on ImageNet to
every frame of the 5D tensor

00:09:30.920 --> 00:09:34.310
to extract one vector, one
feature vector, per frame.

00:09:34.310 --> 00:09:36.530
And so out of this,
we get a sequence

00:09:36.530 --> 00:09:39.800
of visual vectors, which will
run through an LSTM layer.

00:09:39.800 --> 00:09:42.200
And for the question
embedding part,

00:09:42.200 --> 00:09:46.190
we are simply using an embedding
layer to map our question.

00:09:46.190 --> 00:09:49.370
So a sequence of integers
into a sequence of vectors,

00:09:49.370 --> 00:09:51.140
and we run that
through an LSTM layer.

00:09:51.140 --> 00:09:54.440
Finally, we use a
concat operation

00:09:54.440 --> 00:09:56.270
to bring together
these two vectors,

00:09:56.270 --> 00:09:58.160
and we stack a few dense layers.

00:09:58.160 --> 00:10:02.780
And we end up with a softmax
over a predefined vocabulary.

00:10:02.780 --> 00:10:07.300
And you are training that with
a target answer word encoded

00:10:07.300 --> 00:10:09.740
as a one-hot vector.

00:10:09.740 --> 00:10:12.290
So what does
implementation look like?

00:10:12.290 --> 00:10:14.330
It's very simple.

00:10:14.330 --> 00:10:16.310
What you're seeing here
is the video encoder

00:10:16.310 --> 00:10:17.600
in just five lines.

00:10:17.600 --> 00:10:19.280
During the first
line, you're just

00:10:19.280 --> 00:10:23.990
specifying the shape
of your video input.

00:10:23.990 --> 00:10:25.520
So it's a 5D tensor.

00:10:25.520 --> 00:10:27.290
With a shape argument,
you do not actually

00:10:27.290 --> 00:10:29.300
specify the batch size.

00:10:29.300 --> 00:10:32.420
So the first axis, set
to none, is a time axis.

00:10:32.420 --> 00:10:34.670
So it's set to none because
we want it to be viable.

00:10:34.670 --> 00:10:38.030
We want the small one
to be able to process

00:10:38.030 --> 00:10:40.820
videos of any number of frames.

00:10:40.820 --> 00:10:42.740
And then you have the
shape of a single frame.

00:10:42.740 --> 00:10:47.090
So a single frame is
150 by 150 RGB image,

00:10:47.090 --> 00:10:49.850
so it's a tensor
that's 150, 150 by 3.

00:10:49.850 --> 00:10:52.160
In the next line,
what we are doing

00:10:52.160 --> 00:10:55.540
in a single line
is instantiating

00:10:55.540 --> 00:10:59.000
an Inception v3 network
that will automatically

00:10:59.000 --> 00:11:02.600
load pre-trained weights,
so train on ImageNet.

00:11:02.600 --> 00:11:05.930
And we'll configure this network
to be doing visual feature

00:11:05.930 --> 00:11:06.930
extraction.

00:11:06.930 --> 00:11:08.930
So essentially we'll
not be including

00:11:08.930 --> 00:11:11.090
the classifier part
of Inception v3,

00:11:11.090 --> 00:11:12.500
only in the convolutional base.

00:11:12.500 --> 00:11:15.860
And then we'll be applying
some average pulling

00:11:15.860 --> 00:11:17.900
on top of the bottleneck layer.

00:11:17.900 --> 00:11:23.060
You just extract one single
vector, bare image input.

00:11:23.060 --> 00:11:25.970
Then this next line, we
are setting [INAUDIBLE]

00:11:25.970 --> 00:11:27.500
to be non-trainable.

00:11:27.500 --> 00:11:29.570
So what this means is
that, during training,

00:11:29.570 --> 00:11:31.680
we will not be
updating the weights.

00:11:31.680 --> 00:11:32.180
And why?

00:11:32.180 --> 00:11:34.700
Simply because it's
a pre-trained model.

00:11:34.700 --> 00:11:37.610
And if you were to update
its weights while training

00:11:37.610 --> 00:11:39.410
on this new problem
of question answering,

00:11:39.410 --> 00:11:42.531
we will likely be wrecking
the representations

00:11:42.531 --> 00:11:44.530
that this model has already
learned on ImageNet.

00:11:44.530 --> 00:11:45.738
So you don't want to do that.

00:11:45.738 --> 00:11:47.372
You just freeze it.

00:11:47.372 --> 00:11:53.160
And so the next step is to
use a time-distributed layer

00:11:53.160 --> 00:11:55.200
to essentially take
this [INAUDIBLE]

00:11:55.200 --> 00:11:58.500
and apply it to every
step of the time

00:11:58.500 --> 00:12:01.680
axis of the video input.

00:12:01.680 --> 00:12:04.920
So this time-distributed layer
just takes this [INAUDIBLE]

00:12:04.920 --> 00:12:06.390
and apply it to every frame.

00:12:06.390 --> 00:12:10.010
And what comes out of
this layer is simply

00:12:10.010 --> 00:12:14.420
3D tensor, which is a
sequence of visual frames,

00:12:14.420 --> 00:12:16.120
a sequence of visual vectors.

00:12:16.120 --> 00:12:18.030
So each vector is
standing for one frame.

00:12:18.030 --> 00:12:21.580
And finally, we run
this sequence tensor

00:12:21.580 --> 00:12:26.660
through an LSTM layer, and
this gives us one single vector

00:12:26.660 --> 00:12:29.040
encoding a [INAUDIBLE] video.

00:12:29.040 --> 00:12:33.510
And as you can notice, when we
are instantiating this Keras

00:12:33.510 --> 00:12:36.100
LSTM layer, we only
specify one parameter,

00:12:36.100 --> 00:12:39.406
which is the number of
units in the LSTM layer.

00:12:39.406 --> 00:12:42.270
And if you think about it,
that's quite remarkable,

00:12:42.270 --> 00:12:45.810
because there are so many
advanced configuration

00:12:45.810 --> 00:12:48.909
parameters in this LSTM layer.

00:12:48.909 --> 00:12:50.700
There are a lot of
these practices you have

00:12:50.700 --> 00:12:52.260
to follow when using LSTMs.

00:12:52.260 --> 00:12:53.700
For instance, you
have to remember

00:12:53.700 --> 00:12:57.330
that your record on weights
should be initialized using

00:12:57.330 --> 00:12:58.680
[INAUDIBLE] initialization.

00:12:58.680 --> 00:13:01.976
And you have to remember that
the bias of the [INAUDIBLE]

00:13:01.976 --> 00:13:03.100
should be initialized to 1.

00:13:03.100 --> 00:13:05.580
These type of best
practices that you really

00:13:05.580 --> 00:13:08.190
need to follow if you want
training to go smoothly.

00:13:08.190 --> 00:13:11.010
And one key
principle of Keras is

00:13:11.010 --> 00:13:13.170
that best practices
are included,

00:13:13.170 --> 00:13:17.280
so every Keras layer has
well-optimized default

00:13:17.280 --> 00:13:20.230
configuration that takes
into account all these best

00:13:20.230 --> 00:13:20.730
practices.

00:13:20.730 --> 00:13:24.900
So you can't just rely on Keras
defaults to be a good defaults

00:13:24.900 --> 00:13:27.810
and to just get your model
to train out of the box

00:13:27.810 --> 00:13:31.680
without requiring you
to fine-tune every

00:13:31.680 --> 00:13:33.240
of these hyperparameters.

00:13:33.240 --> 00:13:35.730
So best practice is
included, and you should just

00:13:35.730 --> 00:13:39.390
care about only specifying
a few key parameters,

00:13:39.390 --> 00:13:41.820
like the number of units, and
everything else should just

00:13:41.820 --> 00:13:44.590
work right out of the box.

00:13:44.590 --> 00:13:47.700
So for the question
encoder, it's even simpler.

00:13:47.700 --> 00:13:49.080
We just have three lines.

00:13:49.080 --> 00:13:53.640
First, we specify the input
tensor for the question.

00:13:53.640 --> 00:13:55.710
So every question would
just be a sequence

00:13:55.710 --> 00:13:58.620
of at most 100 integers.

00:13:58.620 --> 00:14:03.780
So we will be limited to a
question at 100 words long.

00:14:03.780 --> 00:14:07.250
And then we embed every
integer into word vector.

00:14:07.250 --> 00:14:08.780
We have this embedding layer.

00:14:11.610 --> 00:14:15.390
Note that we use masking on
this embedding layer, which

00:14:15.390 --> 00:14:17.970
means that, for
instance, if we do

00:14:17.970 --> 00:14:20.760
[INAUDIBLE] on our
questions, which

00:14:20.760 --> 00:14:24.600
means that, if the questions
are shorter than 100 words,

00:14:24.600 --> 00:14:29.450
we will just pad the rest
with zeros to get to 100.

00:14:29.450 --> 00:14:31.770
This creates a
mask, which will be

00:14:31.770 --> 00:14:36.290
propagated to the [INAUDIBLE]
layer right after that.

00:14:36.290 --> 00:14:40.730
And so we finally encode
the sequence of four vectors

00:14:40.730 --> 00:14:44.380
into a single vector
via another LSTM layer.

00:14:44.380 --> 00:14:49.490
So note that, when we're using
these pre-trained inception

00:14:49.490 --> 00:14:52.690
network, it's
absolutely fundamental

00:14:52.690 --> 00:14:55.520
to be loading these
pre-trained weights

00:14:55.520 --> 00:14:58.790
because you're dealing with
a fairly small data set.

00:14:58.790 --> 00:15:02.090
And this set alone would
not have enough data

00:15:02.090 --> 00:15:05.090
to allow you to learn to extract
interesting visual features.

00:15:05.090 --> 00:15:09.005
So you really need to make
this network actually train.

00:15:09.005 --> 00:15:13.940
You really need to be leveraging
these pre-trained weights.

00:15:13.940 --> 00:15:18.040
And so finally this is how you
end up with the answer word.

00:15:18.040 --> 00:15:21.890
You are taking the video
vector and the question vector.

00:15:21.890 --> 00:15:24.890
You are concatenating them
with just a concat operation.

00:15:24.890 --> 00:15:28.910
And finally, you are applying
a couple of dense layers,

00:15:28.910 --> 00:15:32.210
and you're ending
up with 1,000 units.

00:15:32.210 --> 00:15:34.610
So we'll have a vocabulary
of possible answers

00:15:34.610 --> 00:15:39.080
that is just 1,000
different words.

00:15:39.080 --> 00:15:42.260
And here's a step where
you're specifying the training

00:15:42.260 --> 00:15:45.830
configuration of your network.

00:15:45.830 --> 00:15:49.010
So you're just
instantiating a model, which

00:15:49.010 --> 00:15:52.179
is a container for
a graph of layers,

00:15:52.179 --> 00:15:53.720
and you're instantiating
them by just

00:15:53.720 --> 00:15:56.360
specifying what are the
inputs of the model, what

00:15:56.360 --> 00:15:57.890
are the outputs.

00:15:57.890 --> 00:16:01.250
You can add any number of inputs
and any number of outputs.

00:16:01.250 --> 00:16:03.830
And you are telling
the model that it

00:16:03.830 --> 00:16:07.610
should use the ad
optimizer during training

00:16:07.610 --> 00:16:10.400
and this last [INAUDIBLE].

00:16:10.400 --> 00:16:13.890
You can notice that, when
specifying our classification

00:16:13.890 --> 00:16:16.550
layer with 1,000 units, we did
not specify any activation.

00:16:16.550 --> 00:16:18.530
So it's actually a
purely linear layer.

00:16:18.530 --> 00:16:22.310
The softmax activation will
be included with the loss.

00:16:27.800 --> 00:16:33.090
And so to sum up this entire
code, it's just about 15 lines.

00:16:33.090 --> 00:16:34.440
So it's very, very short.

00:16:34.440 --> 00:16:39.690
So we are essentially turning
this very complex architecture,

00:16:39.690 --> 00:16:41.720
including loading
pre-trained weights,

00:16:41.720 --> 00:16:43.230
into just a handful of lines.

00:16:43.230 --> 00:16:46.670
So really reducing
complexity and by allowing

00:16:46.670 --> 00:16:50.067
you to rely on different
configurations for every layer,

00:16:50.067 --> 00:16:51.650
we're already reducing
cognitive load.

00:16:51.650 --> 00:16:54.110
You don't have to
care about most

00:16:54.110 --> 00:16:57.500
of the parameters of your
LSTM layer, for instance.

00:17:00.220 --> 00:17:02.410
And the best part of
it, the most magic part,

00:17:02.410 --> 00:17:05.440
is that, because this
implementation of Keras

00:17:05.440 --> 00:17:08.020
is built from the ground
up for TensorFlow,

00:17:08.020 --> 00:17:10.690
it's fully compatible with
things like estimators

00:17:10.690 --> 00:17:11.780
and experiments.

00:17:11.780 --> 00:17:15.640
So in just one line, you
can instantiate a TensorFlow

00:17:15.640 --> 00:17:16.300
experiment.

00:17:16.300 --> 00:17:18.490
And this gives you access
to distributed training

00:17:18.490 --> 00:17:22.401
to train on Cloud ML and so on,
and also access to TensorFlow

00:17:22.401 --> 00:17:22.900
serving.

00:17:22.900 --> 00:17:27.220
So in just a few lines, you can
start running your experiment

00:17:27.220 --> 00:17:29.530
as you did with your
question answering model,

00:17:29.530 --> 00:17:34.810
reading your viewer data and
question data and answer data

00:17:34.810 --> 00:17:37.876
from [INAUDIBLE] data frame.

00:17:37.876 --> 00:17:39.500
You can start running
it on [INAUDIBLE]

00:17:39.500 --> 00:17:40.360
in just a few lines.

00:17:43.620 --> 00:17:47.040
So what you should take away
from this talk is very simple.

00:17:47.040 --> 00:17:50.070
We are taking the Keras API.

00:17:50.070 --> 00:17:51.690
We are bringing it
into TensorFlow.

00:17:51.690 --> 00:17:54.300
It's a new implementation of
Keras built from the ground up

00:17:54.300 --> 00:17:55.890
for TensorFlow.

00:17:55.890 --> 00:17:59.050
And this gives you, as a
Keras user, more flexibility.

00:17:59.050 --> 00:18:01.230
You can start
mixing and matching

00:18:01.230 --> 00:18:03.639
pure TensorFlow functionality
with your Keras model.

00:18:03.639 --> 00:18:06.180
As a TensorFlow user, this gives
you access to the full Keras

00:18:06.180 --> 00:18:08.700
API to make your life easier.

00:18:08.700 --> 00:18:11.880
And so you can expect these
Keras compatibility modules

00:18:11.880 --> 00:18:17.160
to exist as
tf.contrib.keras by TF 1.1.

00:18:17.160 --> 00:18:21.900
And finally, tf.keras
in TensorFlow 1.2.

00:18:21.900 --> 00:18:23.550
And so we believe
this is a big step

00:18:23.550 --> 00:18:25.440
towards making TensorFlow
and deep learning

00:18:25.440 --> 00:18:27.390
accessible to everyone.

00:18:27.390 --> 00:18:29.910
So that's it for me.

00:18:29.910 --> 00:18:33.521
I believe, next, we
have some snacks.

00:18:33.521 --> 00:18:34.520
Thank you for listening.

00:18:34.520 --> 00:18:37.270
[MUSIC PLAYING]

