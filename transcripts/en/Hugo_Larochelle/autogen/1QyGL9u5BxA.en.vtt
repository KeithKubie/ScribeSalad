WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.910
 in this capsule we will introduce the 

00:00:02.010 --> 00:00:07.529
 reinforcement learning concept 

00:00:04.910 --> 00:00:08.670
 so the goals in this part of 

00:00:07.529 --> 00:00:10.650
 short it may be necessary 

00:00:08.670 --> 00:00:12.000
 firstly the first algorithms 

00:00:10.650 --> 00:00:14.460
 learning necessarily takes one goes 

00:00:12.000 --> 00:00:16.379
 to say passive we will end up on the most 

00:00:14.460 --> 00:00:18.990
 delay still see three methods 

00:00:16.379 --> 00:00:20.070
 different that are part of the 

00:00:18.990 --> 00:00:22.470
 category of algorithms 

00:00:20.070 --> 00:00:23.640
 learning parents inevitably passive 

00:00:22.470 --> 00:00:25.769
 then we will go to algorithms 

00:00:23.640 --> 00:00:28.050
 learning parents necessarily active 

00:00:25.769 --> 00:00:30.330
 and we will see different methods 

00:00:28.050 --> 00:00:32.640
 we're going to have four different ones 

00:00:30.330 --> 00:00:34.380
 see here finally we will talk about the 

00:00:32.640 --> 00:00:35.969
 dilemma of exploration versus 

00:00:34.380 --> 00:00:37.230
 exploitation which is a very dilemma 

00:00:35.969 --> 00:00:38.850
 important in learning not 

00:00:37.230 --> 00:00:41.550
 reinforcement and finally we're going 

00:00:38.850 --> 00:00:43.440
 discuss the problem of generalizing well 

00:00:41.550 --> 00:00:50.340
 in the context of a certified 

00:00:43.440 --> 00:00:51.930
 learning by reinforcement then 

00:00:50.340 --> 00:00:53.399
 what kind of problem are we going to succeed 

00:00:51.930 --> 00:00:56.280
 solve with parents learning 

00:00:53.399 --> 00:00:57.750
 necessarily one of the first problem is 

00:00:56.280 --> 00:01:00.420
 that of learning an intelligence 

00:00:57.750 --> 00:01:03.870
 artificial for a game so we have a 

00:01:00.420 --> 00:01:06.600
 example here of a backgammon game and one 

00:01:03.870 --> 00:01:08.189
 will see later that actually with 

00:01:06.600 --> 00:01:09.600
 learning by inevitably we have 

00:01:08.189 --> 00:01:11.580
 managed to develop intelligences 

00:01:09.600 --> 00:01:14.610
 artificial very good for the game of 

00:01:11.580 --> 00:01:17.490
 bagad men we've already seen algorithms 

00:01:14.610 --> 00:01:19.080
 to do learning in 

00:01:17.490 --> 00:01:20.970
 context of two opponents game we have 

00:01:19.080 --> 00:01:23.159
 saw the mini max algorithm then 

00:01:20.970 --> 00:01:24.360
 alphabetic and a little more efficient and 

00:01:23.159 --> 00:01:26.280
 learning parents inevitably we will 

00:01:24.360 --> 00:01:30.360
 succeed in getting algorithms yet 

00:01:26.280 --> 00:01:32.579
 more effective at learning to play at 

00:01:30.360 --> 00:01:36.420
 a game like that being able to be a 

00:01:32.579 --> 00:01:38.009
 good player a game is in too much 

00:01:36.420 --> 00:01:39.240
 this approach there parents learning 

00:01:38.009 --> 00:01:40.369
 necessarily your data from players who 

00:01:39.240 --> 00:01:43.189
 can make decisions 

00:01:40.369 --> 00:01:45.689
 typically faster but more 

00:01:43.189 --> 00:01:47.460
 it's going to be a player who is going to be in 

00:01:45.689 --> 00:01:51.930
 able to learn from 

00:01:47.460 --> 00:01:55.950
 party simulations in a game without 

00:01:51.930 --> 00:01:59.060
 have to define everything the graph of the 

00:01:55.950 --> 00:02:00.979
 the result of the different actions in 

00:01:59.060 --> 00:02:07.610
 in a game like we have to do 

00:02:00.979 --> 00:02:08.840
 for mini max or alfa beta hello this 

00:02:07.610 --> 00:02:10.280
 that we will succeed in doing is learning 

00:02:08.840 --> 00:02:11.720
 to have an artificial intelligence 

00:02:10.280 --> 00:02:13.640
 in a real environment 

00:02:11.720 --> 00:02:16.220
 we have an example here of a controller 

00:02:13.640 --> 00:02:17.750
 of helicopters which so this is a 

00:02:16.220 --> 00:02:19.760
 artificial intelligence that controls 

00:02:17.750 --> 00:02:22.340
 an island remotely controlled sensors then who has 

00:02:19.760 --> 00:02:25.010
 learned to actually control the sensor 

00:02:22.340 --> 00:02:27.230
 to do an inverted flight like this and 

00:02:25.010 --> 00:02:28.519
 so we're really talking about one 

00:02:27.230 --> 00:02:30.739
 brings a little closer to robotics 

00:02:28.519 --> 00:02:32.600
 we really have an agent who is 

00:02:30.739 --> 00:02:33.920
 deploys in a real world to see 

00:02:32.600 --> 00:02:36.560
 that with learning necessarily 

00:02:33.920 --> 00:02:38.030
 these kinds of problems that we will succeed 

00:02:36.560 --> 00:02:40.010
 to solve 

00:02:38.030 --> 00:02:41.600
 in fact there are several works on the 

00:02:40.010 --> 00:02:44.269
 people's development 'artificial 

00:02:41.600 --> 00:02:46.160
 in this context where we have a helicopter 

00:02:44.269 --> 00:02:48.019
 it's learning to do different things 

00:02:46.160 --> 00:02:49.790
 maneuvering the acrobatic with a cape 

00:02:48.019 --> 00:02:50.630
 remote controlled town what kind of things we 

00:02:49.790 --> 00:02:55.940
 will be able to do also with 

00:02:50.630 --> 00:02:57.319
 learning not reinforcement ok 

00:02:55.940 --> 00:02:58.940
 why we need learning not 

00:02:57.319 --> 00:03:01.580
 strengthening almost everything we've seen 

00:02:58.940 --> 00:03:04.190
 learning super factories which who 

00:03:01.580 --> 00:03:06.320
 allows a calendar to take a certain 

00:03:04.190 --> 00:03:08.590
 behavior and we saw that 

00:03:06.320 --> 00:03:10.430
 learning supervise actually for 

00:03:08.590 --> 00:03:12.470
 execute a learning grid 

00:03:10.430 --> 00:03:14.060
 lost we need to give labeled 

00:03:12.470 --> 00:03:16.850
 so data that comes from a 

00:03:14.060 --> 00:03:18.230
 expert who tells another model what 

00:03:16.850 --> 00:03:20.200
 must be his behavior in 

00:03:18.230 --> 00:03:23.359
 different situations 

00:03:20.200 --> 00:03:25.220
 so if we wanted to have in the junta 

00:03:23.359 --> 00:03:27.019
 and people who play chess well 

00:03:25.220 --> 00:03:30.139
 what should be done has begun 

00:03:27.019 --> 00:03:32.450
 data where we would have an entry that 

00:03:30.139 --> 00:03:34.519
 would be the state of the game then a disciple to 

00:03:32.450 --> 00:03:36.530
 predict who would be the movement where 

00:03:34.519 --> 00:03:38.930
 the action to do it in my game 

00:03:36.530 --> 00:03:40.280
 chess in which room moved on 

00:03:38.930 --> 00:03:42.170
 should collect either ideally from a 

00:03:40.280 --> 00:03:44.000
 expert to have an agent who would 

00:03:42.170 --> 00:03:45.410
 comparable to an expert but lies 

00:03:44.000 --> 00:03:46.810
 that started this data there is a 

00:03:45.410 --> 00:03:49.910
 work that is very tedious 

00:03:46.810 --> 00:03:51.680
 several possibilities for the state of the 

00:03:49.910 --> 00:03:53.389
 game so it makes a lot a lot of 

00:03:51.680 --> 00:03:55.940
 data to collect to get a 

00:03:53.389 --> 00:03:58.400
 player who behaves well and who plays 

00:03:55.940 --> 00:04:00.620
 good at chess and so that's 

00:03:58.400 --> 00:04:03.830
 a lot of time so 

00:04:00.620 --> 00:04:06.260
 many of the high costs in terms 

00:04:03.830 --> 00:04:08.629
 to ask an expert who should 

00:04:06.260 --> 00:04:11.629
 save a lot of time 

00:04:08.629 --> 00:04:14.239
 to give us his provides these 

00:04:11.629 --> 00:04:16.609
 data to the tax authorities preferred this is 

00:04:14.239 --> 00:04:19.430
 have an agent who will teach him to 

00:04:16.609 --> 00:04:21.410
 from the result two parts that 

00:04:19.430 --> 00:04:22.130
 the agent will play himself a bit like 

00:04:21.410 --> 00:04:24.919
 we with 

00:04:22.130 --> 00:04:26.479
 we learn well played a game by doing 

00:04:24.919 --> 00:04:29.930
 several parties of supporters 

00:04:26.479 --> 00:04:32.600
 basically practice a game so 

00:04:29.930 --> 00:04:37.070
 it's true an agent who if he wins a 

00:04:32.600 --> 00:04:39.199
 part will realize so his full his 

00:04:37.070 --> 00:04:41.720
 politics so his behavior in 

00:04:39.199 --> 00:04:44.180
 the environment for this game there is good 

00:04:41.720 --> 00:04:46.250
 but that six toddlers who wants to achieve 

00:04:44.180 --> 00:04:47.419
 that some weaknesses in his way 

00:04:46.250 --> 00:04:50.960
 to play then who will try to 

00:04:47.419 --> 00:04:53.300
 adapt according to the outcome of the 

00:04:50.960 --> 00:04:54.800
 part and that's basically what 

00:04:53.300 --> 00:04:56.000
 learning does not necessarily 

00:04:54.800 --> 00:05:00.770
 will try to do that's how it is 

00:04:56.000 --> 00:05:02.449
 is basically worth working so 

00:05:00.770 --> 00:05:04.580
 learning parents inevitably goes 

00:05:02.449 --> 00:05:07.280
 try to learn from 

00:05:04.580 --> 00:05:08.750
 reward we see sometimes called 

00:05:07.280 --> 00:05:10.849
 also a reinforcement so six 

00:05:08.750 --> 00:05:12.680
 rewards that says that if they play 

00:05:10.849 --> 00:05:14.630
 such way I speak the part if 

00:05:12.680 --> 00:05:18.200
 Juicy rotations I lose the game 

00:05:14.630 --> 00:05:20.240
 or six jousting such way I'm I'm going 

00:05:18.200 --> 00:05:22.820
 orient myself to a position the part 

00:05:20.240 --> 00:05:25.039
 which is good if dead is so we are going 

00:05:22.820 --> 00:05:26.990
 see one way to specify these 

00:05:25.039 --> 00:05:28.340
 reinforce the water on do you 

00:05:26.990 --> 00:05:30.580
 cheeks well where do you play badly at 

00:05:28.340 --> 00:05:33.530
 inside an algorithm 

00:05:30.580 --> 00:05:35.300
 reinforcement learning 

00:05:33.530 --> 00:05:37.669
 learning a scent will be done 

00:05:35.300 --> 00:05:39.919
 a bit like an animal that perceives 

00:05:37.669 --> 00:05:41.780
 negative rewards like pain 

00:05:39.919 --> 00:05:43.099
 or hunger as part of a game this 

00:05:41.780 --> 00:05:46.669
 would be right there maybe the frustration 

00:05:43.099 --> 00:05:49.160
 pearls then positive so the pleasure or 

00:05:46.669 --> 00:05:51.110
 if we talk about the animal that moves 

00:05:49.160 --> 00:05:53.060
 in vain then who is sure so will 

00:05:51.110 --> 00:05:56.580
 eat among others popper have 

00:05:53.060 --> 00:05:58.919
 energy for to survive 

00:05:56.580 --> 00:06:01.020
 and in this case as low we expect 

00:05:58.919 --> 00:06:02.909
 what an animal want to maximize 

00:06:01.020 --> 00:06:05.009
 rewards there is somehow 

00:06:02.909 --> 00:06:06.659
 it's this kind of process there that 

00:06:05.009 --> 00:06:10.229
 learning parents necessarily is 

00:06:06.659 --> 00:06:13.099
 to simulate and then imitate to obtain 

00:06:10.229 --> 00:06:13.099
 intelligent agents 

00:06:14.240 --> 00:06:20.310
 we've already seen algorithms based on 

00:06:18.719 --> 00:06:21.659
 decision processes marking well 

00:06:20.310 --> 00:06:23.520
 then the editions editions mare that comes 

00:06:21.659 --> 00:06:24.659
 you come here know your concept 

00:06:23.520 --> 00:06:26.789
 important in learning by 

00:06:24.659 --> 00:06:28.740
 reinforcement so they will try to 

00:06:26.789 --> 00:06:31.229
 find an optimal plan to maximize 

00:06:28.740 --> 00:06:33.449
 precisely a sum of reward so 

00:06:31.229 --> 00:06:34.319
 the sum of the rewards we hoped we saw 

00:06:33.449 --> 00:06:38.069
 to the 8 regions 

00:06:34.319 --> 00:06:39.780
 we saw by the region but all these 

00:06:38.069 --> 00:06:43.800
 algorithms requires knowledge 

00:06:39.780 --> 00:06:44.460
 total of the transition model in my 

00:06:43.800 --> 00:06:47.340
 environment 

00:06:44.460 --> 00:06:48.990
 so this probability that if I'm a 

00:06:47.340 --> 00:06:52.169
 state and that's what I do that I do 

00:06:48.990 --> 00:06:54.270
 the action that I arrive at a new state 

00:06:52.169 --> 00:06:55.830
 expresses so we need to specify 

00:06:54.270 --> 00:06:58.379
 all these distributions there for 

00:06:55.830 --> 00:07:00.719
 execute the region cure where valid 

00:06:58.379 --> 00:07:02.699
 two very expensive policemen and we have 

00:07:00.719 --> 00:07:08.159
 also need the function of 

00:07:02.699 --> 00:07:09.419
 reinforcement therefore by cons in a 

00:07:08.159 --> 00:07:10.979
 real environment 

00:07:09.419 --> 00:07:13.110
 more often than not we can not 

00:07:10.979 --> 00:07:15.629
 make the site a good model of 

00:07:13.110 --> 00:07:18.889
 transition like that a priori we do not have 

00:07:15.629 --> 00:07:22.139
 good definition of pds prims has and 

00:07:18.889 --> 00:07:24.479
 so its robotic robot vacuum that gets 

00:07:22.139 --> 00:07:27.000
 moves into a room but goes to 

00:07:24.479 --> 00:07:28.349
 the real environment then that for a 

00:07:27.000 --> 00:07:30.060
 review on tactics or a piece 

00:07:28.349 --> 00:07:32.699
 whoever does not yet know the 

00:07:30.060 --> 00:07:34.860
 piece so he has to learn made it the 

00:07:32.699 --> 00:07:35.819
 configuration of helped the piece seen from 

00:07:34.860 --> 00:07:39.599
 rare objects tracks 

00:07:35.819 --> 00:07:42.060
 at the beginning we did not make this model of 

00:07:39.599 --> 00:07:44.849
 transition the same thing for an agent 

00:07:42.060 --> 00:07:47.580
 who controls a sensor so that's 

00:07:44.849 --> 00:07:49.199
 very difficult to define these boxes 

00:07:47.580 --> 00:07:51.000
 that happens if one sensor is in a 

00:07:49.199 --> 00:07:54.330
 certain spatial position and that 

00:07:51.000 --> 00:07:56.969
 I'm applying some control over good 

00:07:54.330 --> 00:07:58.349
 my controller that will control and the 

00:07:56.969 --> 00:08:00.659
 remote control sensor so what is going 

00:07:58.349 --> 00:08:02.310
 arrive at the new position 

00:08:00.659 --> 00:08:04.639
 the new configuration of the capa is 

00:08:02.310 --> 00:08:07.289
 very difficult to hold alone a priori 

00:08:04.639 --> 00:08:09.900
 and then even for games that are 

00:08:07.289 --> 00:08:13.889
 totally cimic are on computer 

00:08:09.900 --> 00:08:16.169
 like super mario get to a 

00:08:13.889 --> 00:08:17.460
 definition of this model is an exercise 

00:08:16.169 --> 00:08:19.949
 which is very complex 

00:08:17.460 --> 00:08:21.000
 after lyon we would actually not like 

00:08:19.949 --> 00:08:22.370
 having to do that can have a 

00:08:21.000 --> 00:08:24.690
 algorithm that is capable 

00:08:22.370 --> 00:08:27.060
 finally to discover what is this 

00:08:24.690 --> 00:08:28.229
 transition model there basically 

00:08:27.060 --> 00:08:30.479
 that's what learning by the 

00:08:28.229 --> 00:08:32.669
 form only does it will aim to 

00:08:30.479 --> 00:08:35.279
 find an optimal plan like with ben 

00:08:32.669 --> 00:08:38.430
 ali to the scheme by the situation but without 

00:08:35.279 --> 00:08:40.140
 we have to know has specified the 

00:08:38.430 --> 00:08:42.029
 transition model for my 

00:08:40.140 --> 00:08:45.000
 environment in which money gets 

00:08:42.029 --> 00:08:46.430
 so definitely definitely mean 

00:08:45.000 --> 00:08:48.029
 that's somehow 

00:08:46.430 --> 00:08:49.709
 learning will necessarily be a 

00:08:48.029 --> 00:08:51.000
 somehow the purest form 

00:08:49.709 --> 00:08:51.630
 learning in intelligence 

00:08:51.000 --> 00:08:53.850
 artificial 

00:08:51.630 --> 00:08:56.339
 actually we have a little like 

00:08:53.850 --> 00:08:58.709
 any living being an act that 

00:08:56.339 --> 00:09:02.190
 moves in the environment that 

00:08:58.709 --> 00:09:03.600
 accumulates reinforcement signal 

00:09:02.190 --> 00:09:07.650
 positive or negative 

00:09:03.600 --> 00:09:09.480
 and all the money has to do is 

00:09:07.650 --> 00:09:12.089
 maximize these reinforcements while they 

00:09:09.480 --> 00:09:15.420
 should be related to the fact that survival 

00:09:12.089 --> 00:09:17.070
 not so somehow 

00:09:15.420 --> 00:09:18.770
 actually it may be one of 

00:09:17.070 --> 00:09:21.120
 most fearful forms this is also one of 

00:09:18.770 --> 00:09:23.130
 forms of learning that is the most 

00:09:21.120 --> 00:09:24.540
 difficult so is still a lot 

00:09:23.130 --> 00:09:26.700
 a lot of research work in 

00:09:24.540 --> 00:09:28.440
 learning by reinforcement is 

00:09:26.700 --> 00:09:30.570
 so still a lot of way to go 

00:09:28.440 --> 00:09:31.920
 to have an agent who makes it in 

00:09:30.570 --> 00:09:34.620
 any environment capable 

00:09:31.920 --> 00:09:36.690
 to learn to behave well 

00:09:34.620 --> 00:09:38.970
 but we will see some algorithms 

00:09:36.690 --> 00:09:41.630
 in the next capsules on how 

00:09:38.970 --> 00:09:43.860
 to make some progress towards a 

00:09:41.630 --> 00:09:48.570
 learning not learning 

00:09:43.860 --> 00:09:52.070
 general reinforcement in one who is learning 

00:09:48.570 --> 00:09:52.070
 well a given environment 

00:09:54.339 --> 00:10:03.370
 and if we go back to the concepts of 

00:09:59.040 --> 00:10:05.350
 of agen so in fact here in an agent 

00:10:03.370 --> 00:10:08.170
 utility based one tries to maximize a 

00:10:05.350 --> 00:10:09.850
 utility that is derived from the reinforcements 

00:10:08.170 --> 00:10:11.499
 we're going to see the utility 

00:10:09.850 --> 00:10:13.959
 corresponds to the strengthening con 

00:10:11.499 --> 00:10:16.660
 the cumulative agent but in that if we get 

00:10:13.959 --> 00:10:18.759
 finds in the context of an agent where he 

00:10:16.660 --> 00:10:20.319
 does not know a priori comment on what 

00:10:18.759 --> 00:10:23.319
 the world evolves and then comment on what 

00:10:20.319 --> 00:10:25.839
 these actions will have an impact on his 

00:10:23.319 --> 00:10:27.519
 next and to ok so we see some 

00:10:25.839 --> 00:10:29.980
 algorithms that will try to learn 

00:10:27.519 --> 00:10:31.509
 this part of the agent this part 

00:10:29.980 --> 00:10:34.649
 there that our agent use for 

00:10:31.509 --> 00:10:34.649
 learn to behave 

