1
00:00:00,760 --> 00:00:05,510
And this video will describe the trick
that's a very useful and very general for

2
00:00:05,540 --> 00:00:07,610
improving any object recognition,

3
00:00:07,820 --> 00:00:10,430
trainable system like a
convolutional neural network,

4
00:00:10,890 --> 00:00:13,340
which we will refer to
as data set expansion.

5
00:00:15,490 --> 00:00:16,210
Okay.

6
00:00:16,210 --> 00:00:19,210
So we've seen that in a
commercial neural network. Uh,

7
00:00:19,211 --> 00:00:23,320
we get some invariances done
are essentially built in,

8
00:00:23,920 --> 00:00:27,860
uh, it's invariant to small
translations thanks to uh,

9
00:00:27,910 --> 00:00:30,910
the convolution and the Max
pooling with sub sampling.

10
00:00:32,050 --> 00:00:35,590
It's also a invariant to
small immunation changes.

11
00:00:35,591 --> 00:00:39,310
If we introduce some contrast
normalization. However,

12
00:00:39,311 --> 00:00:44,311
it's not built in us to be invariant with
respect to other things like rotations

13
00:00:44,801 --> 00:00:48,970
or changes in scale. Uh,
and you'd imagine for say,

14
00:00:48,971 --> 00:00:52,780
an object recognition application
because the object doesn't silly change.

15
00:00:52,781 --> 00:00:57,430
If we zoom in or zoom out in the image
or if we rotate it a little bit, uh,

16
00:00:57,431 --> 00:01:02,340
we would like to build in or to encourage
that type of invariance in our, uh,

17
00:01:02,630 --> 00:01:06,550
in our conventional neural
network does actually. Uh,

18
00:01:06,700 --> 00:01:08,950
but it turns out that
these, uh, variations,

19
00:01:08,951 --> 00:01:12,790
I actually very easy to
generate artificially. Uh,

20
00:01:12,820 --> 00:01:17,620
it's very easy to just take an image and
rotate it somewhat or change the scale

21
00:01:17,621 --> 00:01:21,010
at which, uh, we're looking
at this a given image.

22
00:01:21,760 --> 00:01:26,760
So very easy trick for improving the
performance of a computer vision system is

23
00:01:27,341 --> 00:01:31,190
to actually generate this artificial
data and put it into our training set.

24
00:01:31,191 --> 00:01:36,120
So expand the training set
using these transformation, uh,

25
00:01:36,130 --> 00:01:39,310
to guide us into how we could
generate additional examples.

26
00:01:40,150 --> 00:01:43,270
And so by doing this, by generating
this additional training data,

27
00:01:43,280 --> 00:01:44,320
the neural network,
well,

28
00:01:44,321 --> 00:01:48,370
instead learn to be in Varian to
these transformations because they're

29
00:01:48,371 --> 00:01:50,170
explicitly there and the training set,

30
00:01:50,200 --> 00:01:55,200
they will get explicitly the
signal that it should perform odd,

31
00:01:55,451 --> 00:01:58,310
put the same object,
uh,

32
00:01:58,360 --> 00:02:02,230
for any variation in the rotation
or scaling that will generate.

33
00:02:05,340 --> 00:02:08,990
Sorry, is there a cartoon illustration
of how we could generate these additional

34
00:02:08,991 --> 00:02:09,824
example?

35
00:02:09,980 --> 00:02:14,520
So I'm now in each case
will essentially take a,

36
00:02:14,521 --> 00:02:19,521
a smaller portion of the image and we'll
apply a transformation to it to get a,

37
00:02:20,440 --> 00:02:25,010
an undoing, the transformation to get
an image in the same reference frame.

38
00:02:25,400 --> 00:02:27,500
So, uh, for the original image,

39
00:02:27,501 --> 00:02:31,970
we just take some crop at the center
and that would become a new training

40
00:02:31,971 --> 00:02:35,420
example. Then if we want to
be invited to translation,

41
00:02:35,421 --> 00:02:39,740
we could take the crop of the original
image and instead translate somewhat.

42
00:02:40,280 --> 00:02:44,780
Uh, and then we would take that subset of
the image and this would become our new

43
00:02:44,781 --> 00:02:49,640
image. Uh, we could, if we want to be
invariant with respect to rotations,

44
00:02:49,641 --> 00:02:50,720
we could take the crop,

45
00:02:50,750 --> 00:02:55,750
the original crop a window and just
rotate it randomly and then we would,

46
00:02:56,660 --> 00:02:57,440
uh,

47
00:02:57,440 --> 00:03:01,780
consider only the pixels in that window
and we would rotate and do the rotation

48
00:03:01,781 --> 00:03:06,460
to get again as a rectangular
image, like a, the original image.

49
00:03:07,570 --> 00:03:10,300
Uh, and similarly for scaling, uh,

50
00:03:10,330 --> 00:03:15,330
we could just take the cropping window
and it stayed instead change its size.

51
00:03:15,371 --> 00:03:20,371
So if we made it smaller than we could
crop fewer pixels and then we could undo

52
00:03:20,531 --> 00:03:25,531
the scrubbing by scaling back to the
same size as the original image here,

53
00:03:25,750 --> 00:03:27,820
in which case we'd get
easy zooming operations.

54
00:03:27,821 --> 00:03:30,640
So we'd be changing the
scale of the actual image.

55
00:03:31,510 --> 00:03:34,570
So for performing these operations,
I won't go into the detail.

56
00:03:34,571 --> 00:03:38,860
You need to do some sort of interpolation
of pixels when you're doing these real

57
00:03:38,861 --> 00:03:41,350
valued rotations or scaling a,

58
00:03:41,351 --> 00:03:44,140
but often if you have
some software that does,

59
00:03:45,000 --> 00:03:47,380
that allows you to manipulate images,
uh,

60
00:03:47,381 --> 00:03:52,210
you can do a many of these
operations there. They often, uh, uh,

61
00:03:52,340 --> 00:03:53,173
support it.

62
00:03:55,260 --> 00:03:56,010
Yeah.

63
00:03:56,010 --> 00:04:00,580
And other type of additional
examples we can generate, uh, to,

64
00:04:00,780 --> 00:04:04,600
uh, improve the digitalization
performance of uh, uh,

65
00:04:04,630 --> 00:04:09,630
computer vision system is to add so-called
elastic the formations of the image.

66
00:04:09,830 --> 00:04:12,430
This is particularly useful
for character recognition.

67
00:04:13,730 --> 00:04:18,730
So the idea is that we want to take
some original image and sort of locally

68
00:04:19,701 --> 00:04:23,720
distorted a little bit such that it
still looks like the same kind of object,

69
00:04:24,080 --> 00:04:27,950
but it's somewhat a little bit different.
It has a slightly different appearance,

70
00:04:27,951 --> 00:04:32,840
but that's still, um, qualifies as
an appearance for that same object.

71
00:04:33,740 --> 00:04:38,740
So one way of doing this is to first
generate a random distortion field.

72
00:04:38,761 --> 00:04:42,650
So the distortion field,
which we see here would be for each Pixel,

73
00:04:42,800 --> 00:04:47,800
a vector that tells us how the value of
that pixel should be displaced into the,

74
00:04:48,981 --> 00:04:53,440
into dimensions. And so if
we generate the random, uh,

75
00:04:53,750 --> 00:04:56,150
distortion field like this a while,

76
00:04:56,151 --> 00:04:58,610
you get a bunch of arrows
pointing in random directions.

77
00:04:58,610 --> 00:05:03,470
So they're generated here, each, uh,
using some Gaussian where each simple.

78
00:05:03,471 --> 00:05:08,090
So each factor is, is
generated independently. So
we are generating each point,

79
00:05:08,480 --> 00:05:13,480
which is a factor as the two dimensional
from a two dimensional Goshen and I,

80
00:05:13,511 --> 00:05:17,180
if we applied that distortion field
to the image here, for instance,

81
00:05:17,181 --> 00:05:19,760
we'd get something that's
actually a bit noisy. It's,

82
00:05:19,970 --> 00:05:23,360
it's not a very convincing six,
uh,

83
00:05:23,410 --> 00:05:26,000
and it just looks like
we've just added some noise.

84
00:05:27,990 --> 00:05:32,300
However, if we take that random
field, we actually smooth it, uh,

85
00:05:32,310 --> 00:05:34,320
using some,
uh,

86
00:05:34,380 --> 00:05:39,030
smoothing kernel and doing a
correlation or convolution such as say,

87
00:05:39,031 --> 00:05:43,910
a Gaussian kernel for instance. Uh,
then we can just smooth this, uh,

88
00:05:43,980 --> 00:05:48,120
original random distortion field
to get a smoother transformation.

89
00:05:48,810 --> 00:05:52,980
And if we apply this distortion
field that's been smooth,

90
00:05:53,430 --> 00:05:55,880
then we actually get something like here,

91
00:05:55,890 --> 00:05:59,560
the six has been into
this sort of deformation,

92
00:05:59,570 --> 00:06:04,010
which still qualifies as a sixth to
pretty convincing a image of a six.

93
00:06:04,790 --> 00:06:07,040
And so by generating many
of these distortions,

94
00:06:07,041 --> 00:06:11,360
so we get another one where we
generated the random a distortion and,

95
00:06:11,361 --> 00:06:14,570
and smooth the distortion.
We get a new six here,

96
00:06:14,571 --> 00:06:17,660
which is actually quite
different from the previous one,

97
00:06:18,350 --> 00:06:22,130
and it's still a very convincing
six. So a, we see that, uh,

98
00:06:22,131 --> 00:06:24,510
this is another kind of operation,
uh,

99
00:06:24,680 --> 00:06:28,160
which allows us to generate for
free additional training data.

100
00:06:28,640 --> 00:06:33,620
So we would just add this as are in our
training set and label it as six. Again,

101
00:06:33,621 --> 00:06:37,130
you need to be careful with, uh, uh,

102
00:06:37,160 --> 00:06:40,700
in order to do some interpretation
of the pixels because this is, again,

103
00:06:40,701 --> 00:06:43,580
a transformation which is continuous.
So it's not,

104
00:06:43,820 --> 00:06:48,470
each vector does not displace the pixel
in the cell exactly on the original

105
00:06:48,530 --> 00:06:53,300
grid. And so you have to do some,
uh, uh, a spatial interpolation.

106
00:06:53,660 --> 00:06:57,670
Um, but again, this is the kind of thing
that's often supported and, and, uh, uh,

107
00:06:58,040 --> 00:07:02,600
softwares that allow you to manipulate
the images to get more details about all

108
00:07:02,601 --> 00:07:05,760
of this, I encourage you to, uh, uh,

109
00:07:05,860 --> 00:07:10,220
look at this smart out paper that is
referenced on the website for discourse.

110
00:07:11,090 --> 00:07:13,600
And, uh, just, uh, remember that, uh,

111
00:07:13,640 --> 00:07:18,350
these elastic deformations or
any of the other kind of, uh, uh,

112
00:07:18,420 --> 00:07:19,490
transformations we can,
uh,

113
00:07:19,940 --> 00:07:23,810
use to expand the Dataset is not
actually just useful for neural networks,

114
00:07:23,811 --> 00:07:28,160
but really any system that is trainable,
right?

115
00:07:28,161 --> 00:07:29,990
So that's it for a dataset expansion.

