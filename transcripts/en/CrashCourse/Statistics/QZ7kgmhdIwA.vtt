WEBVTT
Kind: captions
Language: en

00:00:03.100 --> 00:00:07.560
Hi, I’m Adriene Hill, and Welcome back to
Crash Course Statistics. Sometimes random

00:00:07.570 --> 00:00:12.540
variation can make it tricky to tell when
there are true differences or if it’s just random.

00:00:12.540 --> 00:00:15.320
Like whether a sample difference of $20 a

00:00:15.320 --> 00:00:20.080
month represents a real difference between
the average rates of two car insurance companies.

00:00:20.080 --> 00:00:25.150
Or whether a 1 point increase in your AP Stats grade for every hour you study represents

00:00:25.150 --> 00:00:27.350
a real relationship between the two.

00:00:27.350 --> 00:00:31.640
These situations seem pretty different, but
when we get down to it, they share a similar

00:00:31.640 --> 00:00:37.089
pattern. There’s actually one idea, which--with a few tweaks--can help us answer ALL of our

00:00:37.089 --> 00:00:40.039
“is it random...or is it real” questions.

00:00:40.039 --> 00:00:45.510
That’s what test statistics do. Test statistics allow us to quantify how close things are

00:00:45.510 --> 00:00:50.370
to our expectations or theories. Something
that’s not always easy for us to do based

00:00:50.370 --> 00:00:55.519
on our gut feelings. And test statistics allow us to add a little more mathematical rigor

00:00:55.520 --> 00:00:59.260
to the process, so that we can make decisions about these questions.

00:00:59.260 --> 00:01:08.740
INTRO

00:01:08.750 --> 00:01:13.560
In previous episodes, z-scores helped us understand the idea that differences are relative.

00:01:13.560 --> 00:01:18.020
A difference of 1 second is meaningful
when you are looking at the differences in

00:01:18.020 --> 00:01:24.460
the average time it takes two groups of elite Olympic athletes to complete a 100 meter freestyle swim.

00:01:24.469 --> 00:01:28.280
It’s less meaningful when you’re
looking at the differences in the average

00:01:28.280 --> 00:01:30.880
time it takes two groups of recreational swimmers.

00:01:30.880 --> 00:01:36.049
The amount of variance in a group is really
important in judging a difference. Elite Olympic

00:01:36.049 --> 00:01:42.159
athletes vary only a little. Their 100 meter
times are relatively close together, and a

00:01:42.159 --> 00:01:47.240
10th of a second can mean the difference between a gold and a bronze medal. Whereas non professionals

00:01:47.240 --> 00:01:52.439
have more variation; the fastest swimmers
could finish a whole minute before the slower

00:01:52.439 --> 00:01:53.439
swimmers.

00:01:53.439 --> 00:01:58.549
A difference of 1 second isn't a big deal
between two groups of recreational swimmers

00:01:58.549 --> 00:02:03.359
because the difference is small compared to the natural variation we’d expect to see.

00:02:03.359 --> 00:02:08.539
Two groups of casual swimmers may differ by 10 or more seconds, even if their true underlying

00:02:08.539 --> 00:02:10.989
times were the same, just because of random variation.

00:02:10.989 --> 00:02:15.240
That’s why test statistics look at the difference between data and what we’d expect to see

00:02:15.240 --> 00:02:20.620
if the null hypothesis is true. But they also
include some very important context: a measure

00:02:20.620 --> 00:02:26.180
of “average” variation we’d expect to
see, like how much novice or pro swimmers

00:02:26.180 --> 00:02:32.480
differ. Test statistics help us quantify whether data fits our null hypothesis well.

00:02:32.480 --> 00:02:38.240
A z-score is a test statistic. Let’s look
at a simple example. Say your IQ is 130. You’re

00:02:38.240 --> 00:02:41.120
so smart. And the population mean is 100.

00:02:41.120 --> 00:02:46.181
On average we expect someone to be about 15 points from the mean. So the difference we

00:02:46.181 --> 00:02:52.540
observed, 30, is twice the amount that we’d
expect to see on average. Your z score would be 2.

00:02:52.540 --> 00:02:58.340
And you can z-score any normal distribution--like a population distribution. But also a sampling

00:02:58.340 --> 00:03:04.140
distribution which is the distribution of
all possible group means for a certain sample size.

00:03:04.140 --> 00:03:08.040
You might remember we first learned
about sampling distribution in episode 19.

00:03:08.049 --> 00:03:12.549
We often have questions about groups of people. Finding out that you’re two standard deviations

00:03:12.549 --> 00:03:17.730
above the mean for IQ is pretty ego boosting, but it won’t really help further science.

00:03:17.730 --> 00:03:21.299
We could look at whether children with more than 100 books in their home have a higher

00:03:21.299 --> 00:03:27.000
than average IQs. Let’s say we take a random sample of 25 children with over 100 books.

00:03:27.000 --> 00:03:31.200
Then we measure their IQs. The average IQ
is 110.

00:03:31.200 --> 00:03:35.860
We can calculate a z-score for our particular group mean. The steps are exactly the same,

00:03:35.860 --> 00:03:41.700
we’re just now looking at the sampling distribution of sample means rather than the population distribution.

00:03:41.700 --> 00:03:46.280
Instead of taking an individual score and
subtracting the population mean, we take a

00:03:46.280 --> 00:03:52.070
group mean and subtract the mean of our sampling distribution under the null hypothesis. Then

00:03:52.070 --> 00:03:56.940
we divide by the standard error, which is
the standard deviation of the sampling distribution.

00:03:56.940 --> 00:04:02.020
So, the z-score--also called the z-statistic--tells us how many standard errors away from the

00:04:02.020 --> 00:04:05.540
sampling distribution mean our group mean is.

00:04:05.540 --> 00:04:10.980
Z-statistics around 1 or -1 tell us that the
sample mean is the typical distance we’d

00:04:10.980 --> 00:04:15.680
expect a typical sample mean to be from the mean of the null hypothesis.

00:04:15.680 --> 00:04:21.540
Z-statistics that are a lot bigger in magnitude than 1 or -1 mean that this sample mean is

00:04:21.540 --> 00:04:22.690
more extreme.

00:04:22.690 --> 00:04:25.860
Which matches the general form of a test statistic:

00:04:25.860 --> 00:04:30.640
The p-value will tell us how rare or extreme
our data is so that we can figure out whether

00:04:30.640 --> 00:04:34.320
we think there’s an effect. Like whether
children with more than 100 books in their

00:04:34.320 --> 00:04:39.800
home have a higher than average IQ. Historically we’ve done this with tables, but most statistical

00:04:39.800 --> 00:04:42.120
programs, even Excel, can calculate this.

00:04:42.120 --> 00:04:47.720
We can use z-tests to do hypothesis tests
about means, differences between means, proportions,

00:04:47.720 --> 00:04:50.040
or even differences between proportions.

00:04:50.040 --> 00:04:54.090
A researcher may want to know whether people in a certain region who got this year’s

00:04:54.090 --> 00:04:59.080
flu vaccine were less likely to get the flu.
They randomly sample 1000 people and found

00:04:59.080 --> 00:05:03.180
that 600 people got the flu vaccine, and 400 didn’t.

00:05:03.180 --> 00:05:08.900
Out of the 600 people who got the vaccine,
20% still got the flu. Out of the 400 people

00:05:08.901 --> 00:05:13.100
who did not get the vaccine, 26% got the flu.

00:05:13.100 --> 00:05:16.540
It seems like you’re more likely to get
the flu if you didn’t get a flu shot, but

00:05:16.540 --> 00:05:21.450
we’re not sure if this difference is pretty
small compared to random variation, or pretty large.

00:05:21.450 --> 00:05:24.580
To calculate our z-statistic for this question,

00:05:24.580 --> 00:05:27.350
we first have to remember our general form:

00:05:27.350 --> 00:05:32.840
There’s a 6% difference between the proportion of the vaccinated and unvaccinated groups,

00:05:32.840 --> 00:05:36.760
and we want to know how “different” 6%
is from 0%.

00:05:36.760 --> 00:05:41.480
A difference of 0% would mean there’s no
difference between flu rates between the two groups.

00:05:41.480 --> 00:05:46.680
So our observed difference is 6 minus 0 percent, or 6%.

00:05:46.680 --> 00:05:51.120
For this question, the “average variation”
of what percent of people get the flu is the

00:05:51.120 --> 00:05:56.800
standard error from our sampling distribution. We calculate it using the average proportion

00:05:56.800 --> 00:05:59.800
of people who got the flu, and didn’t get
the flu:

00:05:59.800 --> 00:06:03.880
If our observed difference of 6% is large
compared to the standard error--which is the

00:06:03.880 --> 00:06:09.520
amount of variation we expect by chance--we consider the difference to be “statistically

00:06:09.520 --> 00:06:13.430
significant”. We’ve found evidence suggesting the null might not be accurate.

00:06:13.430 --> 00:06:18.560
There’s two main ways of telling whether
this z-statistic, which is about 2.2295 in

00:06:18.560 --> 00:06:21.940
our case, represents a statistically significant result.

00:06:21.940 --> 00:06:26.490
The first way is to calculate a “critical”
value. A critical value is a value of our

00:06:26.490 --> 00:06:31.590
test statistic that marks the limits of our
“extreme” values. A test statistic that

00:06:31.590 --> 00:06:36.630
is more extreme than these critical values
(that is it’s towards the tails) causes

00:06:36.630 --> 00:06:37.880
us to reject the null .

00:06:37.880 --> 00:06:43.160
We calculate our critical value by finding
out which test-statistic value corresponds

00:06:43.160 --> 00:06:52.270
to the top 0.5, 1, or 5% most extreme values. For a z-test with alpha = 0.05, the critical

00:06:52.270 --> 00:06:55.940
values are 1.96 and -1.96.

00:06:55.940 --> 00:07:00.900
If your z-statistic is more extreme than the
critical value, you call it “statistically

00:07:00.900 --> 00:07:05.030
significant”. So, we found evidence...in
this case...that the flu shot is working.

00:07:05.030 --> 00:07:09.530
But sometimes, a z-test won’t apply. And
when that happens, we can use the t-distribution

00:07:09.530 --> 00:07:13.340
and corresponding t-statistic to conduct a
hypothesis test.

00:07:13.340 --> 00:07:18.770
The t-test is just like our z-test. It uses
the same general formula for its t-statistic.

00:07:18.770 --> 00:07:23.310
But we use a t-test if we don't know the true population standard deviation.

00:07:23.310 --> 00:07:28.810
As you can see, it looks like our z-statistic,
except that we’re using our sample standard

00:07:28.810 --> 00:07:32.430
deviation instead of the population standard deviation in the denominator.

00:07:32.430 --> 00:07:37.210
The t-distribution looks like the z-distribution, but with thicker tails. The tails are thicker

00:07:37.210 --> 00:07:41.340
because we're estimating the true population standard deviation.

00:07:41.340 --> 00:07:45.400
Estimation adds a little more uncertainty
...which means thicker tails, since extreme

00:07:45.400 --> 00:07:49.620
values are a little more common.
But as we get more and more data, the t-distribution

00:07:49.620 --> 00:07:55.940
converges to the z-distribution, so with really large samples, the z and t-tests should give

00:07:55.940 --> 00:07:57.320
us similar p-values.

00:07:57.320 --> 00:08:01.940
If we’re ever in a situation where we had
the population standard deviation, a z-test

00:08:01.940 --> 00:08:06.330
is the way to go. But a t-test is useful when
we don’t have that information.

00:08:06.330 --> 00:08:11.110
For example, we can use a t-test to ask whether the average wait time at a car repair shop

00:08:11.110 --> 00:08:16.300
across the street is different from the time
you’ll wait at a larger shop 10 minutes away.

00:08:16.300 --> 00:08:21.940
We collect data from 50 customers who need to take their cars in for major repairs. 25

00:08:21.940 --> 00:08:25.700
are randomly assigned to go to the smaller
repair shop, and the other 25 are sent to

00:08:25.710 --> 00:08:26.730
the larger shop.

00:08:26.730 --> 00:08:30.250
After measuring the amount of time it took
for repairs to be completed, we find that

00:08:30.250 --> 00:08:35.279
people who went to the smaller shop had an average wait time of 14 days. People who went

00:08:35.279 --> 00:08:41.149
to the larger shop had an average wait time
of 13.25 days, which means there was a difference

00:08:41.149 --> 00:08:44.110
of 0.75 days in wait time.

00:08:44.110 --> 00:08:48.410
But we don’t know whether it’s likely
that this 0.75 day difference is just due

00:08:48.410 --> 00:08:54.399
to random variation between customers....at least not until we conduct a t-test on the

00:08:54.399 --> 00:08:56.329
difference between the means of the two groups.

00:08:56.329 --> 00:09:01.660
Before we do our test, we need to decide on an alpha level. We set our alpha at 0.01,

00:09:01.660 --> 00:09:07.160
because we want to be a bit more cautious
about rejecting the null hypothesis than we

00:09:07.160 --> 00:09:10.100
would be if we used the standard of 0.05.

00:09:10.100 --> 00:09:15.490
Now we can calculate the t-statistic for our
two-sample t-test. If the null hypothesis

00:09:15.490 --> 00:09:20.319
was true, then there would be no real difference between the mean wait times of the two groups.

00:09:20.319 --> 00:09:24.069
And the alternative hypothesis is that the
two means are not equal.

00:09:24.069 --> 00:09:28.029
The two sample t-statistic again follows the general form:

00:09:28.029 --> 00:09:33.079
We observed a 0.75 day difference in wait
times between groups. We’d expect to see

00:09:33.080 --> 00:09:39.460
a difference of 0 if the null were true. Our
measure of average variation is the standard error.

00:09:39.460 --> 00:09:44.560
The standard error is the typical distance
that a sample mean will be from the population mean.

00:09:44.560 --> 00:09:50.040
This time, we’re looking at the sampling
distribution of differences between means--all

00:09:50.040 --> 00:09:54.960
the possible differences between two groups-- which is why the standard error formula may

00:09:54.960 --> 00:09:56.019
look a little different.

00:09:56.019 --> 00:09:59.770
Putting it all together we get a t-statistic
of about 2.65.

00:09:59.770 --> 00:10:03.410
If we plug that into our computer, we can
see that this test statistic has a p-value

00:10:03.410 --> 00:10:10.480
of about .0108. Since we set our alpha at
0.01, a p-value needs to be smaller than 0.01

00:10:10.480 --> 00:10:15.100
to reject the null hypothesis. Ours isn’t.
Barely, but it isn’t.

00:10:15.100 --> 00:10:18.810
So it might have seemed like the larger repair shop was definitely going to be faster but

00:10:18.810 --> 00:10:22.889
it’s actually not so clear. And this doesn’t
mean that there isn’t a difference, we just

00:10:22.889 --> 00:10:25.459
couldn’t find any evidence that there was
one.

00:10:25.459 --> 00:10:30.110
So if you’re trying to decide which shop
to take you car to, maybe consider something

00:10:30.110 --> 00:10:36.060
other than speed. And we could do similar
test experiments for cost or reliability or friendliness.

00:10:36.060 --> 00:10:40.180
You might notice that throughout the examples in this episode, we used two methods of deciding

00:10:40.189 --> 00:10:44.709
whether something was significant: critical
values and p-values.

00:10:44.709 --> 00:10:49.569
These two methods are equivalent. Large test statistics and small p-values both refer to

00:10:49.569 --> 00:10:53.050
samples that are extreme.
A test statistic that’s bigger than our

00:10:53.050 --> 00:10:58.670
critical value would allow us to reject the
null hypothesis. And any test-statistic that’s

00:10:58.670 --> 00:11:04.069
larger than the critical value will have a
p-value less than 0.05. So, the two methods

00:11:04.069 --> 00:11:06.000
will lead us to the same conclusion.

00:11:06.000 --> 00:11:15.360
If you have trouble remembering it, this rhyme may help: “Reject H-Oh if the p is too low”

00:11:15.360 --> 00:11:20.360
These two methods are equivalent. But we often use p-values instead of critical values. This

00:11:20.360 --> 00:11:25.450
is because each test-statistic, like the z
or t statistics, have different critical values,

00:11:25.450 --> 00:11:32.209
but a p-value of less than 0.05 means that
your sample is in the top 5% of extreme samples

00:11:32.209 --> 00:11:37.629
no matter if you use a z or t test-statistic
- or some of the other test-statistic we haven’t

00:11:37.629 --> 00:11:39.749
discussed like F or chi-square.

00:11:39.749 --> 00:11:44.569
Test statistics form the basis of how we can test if things are actually different or what

00:11:44.569 --> 00:11:48.970
we seeing is just normal variation. They help us know how likely it is that our results

00:11:48.970 --> 00:11:52.199
are normal, or if something interesting is
going on.

00:11:52.199 --> 00:11:56.970
Like whether drinking that water upside down is actually stopping your hiccups faster

00:11:56.970 --> 00:12:04.319
than doing nothing. Then you can test drinking pickle juice to stop hiccups. Or really slowly

00:12:04.320 --> 00:12:10.060
eating a spoonful of creamy peanut butter.
Let the testing commence! Thanks for watching.

00:12:10.160 --> 00:12:11.460
I’ll see  you next time.

