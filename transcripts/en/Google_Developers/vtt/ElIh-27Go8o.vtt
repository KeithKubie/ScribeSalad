WEBVTT
Kind: captions
Language: en

00:00:04.690 --> 00:00:05.690
&gt;&gt;Dave Sparks: All right. Good afternoon.
Welcome. I am really happy to have everyone

00:00:05.690 --> 00:00:07.330
here.
My name is Dave Sparks, and I am the technical

00:00:07.330 --> 00:00:11.880
lead on the Android media framework.
And I am going to give you a talk this afternoon

00:00:11.880 --> 00:00:17.910
about advanced audio techniques and Android,
and also give you some highlights about the

00:00:17.910 --> 00:00:24.100
Froyo release that we just announced today.
And then talk a little bit about roadmap.

00:00:24.100 --> 00:00:30.670
So if you are going to follow along in Wave,
here is the URL. And I'll leave this slide

00:00:30.670 --> 00:00:35.940
up for a little bit so you have a chance to
type it in if you want to follow along there.

00:00:35.940 --> 00:00:40.890
So a few questions. How many people here are
active Android developers?

00:00:40.890 --> 00:00:48.839
So you have written an application. Okay.
How many people actually have an application

00:00:48.839 --> 00:00:55.359
up in Market?
Wow, that's a pretty good showing. Not bad.

00:00:55.359 --> 00:00:58.469
And anybody have more than one?
All right.

00:00:58.469 --> 00:01:04.360
A few hard-core developers.
Everybody get a chance to see the Android

00:01:04.360 --> 00:01:08.100
keynote this morning?
Not many people?

00:01:08.100 --> 00:01:14.400
Okay. Well, one of the exciting things we
announced this morning was the Google TV stuff.

00:01:14.400 --> 00:01:22.110
It's pretty exciting, I think.
So did everybody hear about Google TV? Yeah.

00:01:22.110 --> 00:01:24.430
Anybody interested in developing for Google
TV as well?

00:01:24.430 --> 00:01:29.460
&gt;&gt;&gt; Yes.
&gt;&gt;Dave Sparks: All right. Okay.

00:01:29.460 --> 00:01:33.210
So I am just trying to get a feeling for where
people are in terms of development and excitement

00:01:33.210 --> 00:01:36.461
about Android.
So we will move on. Hopefully you have had

00:01:36.461 --> 00:01:41.630
a chance to get that URL.
So the agenda today, I am going to talk about

00:01:41.630 --> 00:01:47.000
native audio signal processing in Android.
There's been a lot of interest in doing something

00:01:47.000 --> 00:01:54.170
more than manipulating bytes in Java code,
so I will give you some sample code snippets

00:01:54.170 --> 00:01:59.710
that you can use to get started.
We'll talk a little bit about resource and

00:01:59.710 --> 00:02:04.871
power management. I revisit this every time
I do talks or anytime I talk to developers

00:02:04.871 --> 00:02:12.300
about battery power usage and resource management
is really critical on these battery powered

00:02:12.300 --> 00:02:15.700
devices, obviously. So I will talk a little
bit about that.

00:02:15.700 --> 00:02:21.080
As I mentioned earlier I am going to talk
about some of the new stuff we have thrown

00:02:21.080 --> 00:02:22.790
into Froyo. We think some of it is pretty
exciting.

00:02:22.790 --> 00:02:28.421
And finally, we will talk a little bit about
the roadmap for future releases and some of

00:02:28.421 --> 00:02:33.870
the stuff we are working on right now. And
then hopefully we will have at least a few

00:02:33.870 --> 00:02:36.531
minutes at the end for Q&amp;A.
We are going to try to catch up a little bit.

00:02:36.531 --> 00:02:40.420
I guess the keynote ran late this morning
and we are kind of behind on the schedule.

00:02:40.420 --> 00:02:46.709
So I will try to go fast and not lose too
many people.

00:02:46.709 --> 00:02:56.430
So we'll start with talking about native audio
signal processing on Android.

00:02:56.430 --> 00:03:01.430
So everything we do in Android right now,
basically you are doing through some sort

00:03:01.430 --> 00:03:07.440
of Java interface. So I will start with an
overview of the audiotrack interface in Android

00:03:07.440 --> 00:03:13.290
so you have a sense of how you connect into
things. So we will look at Java code first.

00:03:13.290 --> 00:03:19.870
So the audiotrack interface is basically a
raw PCM interface. You give it raw 16 bit

00:03:19.870 --> 00:03:25.380
or eight bit audio and it gets mixed in through
an audio subsystem we call audio flinger.

00:03:25.380 --> 00:03:32.170
It does whatever sample rate conversion it
needs to get to the sample output of the device.

00:03:32.170 --> 00:03:37.250
And then you can do mixing, left/right balance,
things like that in the API.

00:03:37.250 --> 00:03:44.959
So it basically gives you a way to send your
application's audio directly to an output.

00:03:44.959 --> 00:03:49.530
So you can generate that, you can take a Wave
file and output it. Whatever you want to do

00:03:49.530 --> 00:03:54.350
with the audio.
We have people doing things like soundboards

00:03:54.350 --> 00:03:57.590
and synthesizers and things like that using
this interface.

00:03:57.590 --> 00:04:05.250
It allows you to do both streaming and static
buffers. So a streaming buffer meaning you

00:04:05.250 --> 00:04:08.810
are generating the audio on the fly, maybe
you are streaming it over a network connection

00:04:08.810 --> 00:04:12.520
or you are reading out of a file, but you
have a small buffer. You fill the buffer,

00:04:12.520 --> 00:04:17.019
and there's a read pointer and a write pointer,
and the write pointer needs to stay ahead

00:04:17.019 --> 00:04:20.660
of the read pointer, and you just loop back.
So that's the streaming mode.

00:04:20.660 --> 00:04:25.130
There's also a static mode where you can fill
it up with typically a small sound that you

00:04:25.130 --> 00:04:30.130
are going to play a lot. An example might
be a prompt, a beep something or a gunshot

00:04:30.130 --> 00:04:35.160
in a game.
It's actually very low overhead because that

00:04:35.160 --> 00:04:42.400
data gets buffered up in the audio flinger
service, and you don't actually have data

00:04:42.400 --> 00:04:49.530
flowing across from the application side to
the server side.

00:04:49.530 --> 00:04:53.241
You can set call backs to refill the buffer.
So in the case where you are doing streaming,

00:04:53.241 --> 00:04:58.280
you can set a call back say halfway through
the buffer. That prompts you to go generate

00:04:58.280 --> 00:05:03.770
new audio for the part of the buffer that
just emptied. So it uses a poll model that

00:05:03.770 --> 00:05:07.930
way.
You can also retrieve the play position, so

00:05:07.930 --> 00:05:12.741
if you are trying to do some long stream,
if you want to do synchronization in a game

00:05:12.741 --> 00:05:17.210
or something like that, you can find out where
the play pointer is at any particular time,

00:05:17.210 --> 00:05:22.490
and it takes into account the latency in the
audio path.

00:05:22.490 --> 00:05:28.441
Now, obviously, this is really useful for
games, or if you are doing streaming audio.

00:05:28.441 --> 00:05:33.840
Any kind of thing where you are generating
audio on the fly.

00:05:33.840 --> 00:05:39.960
So here is some sample code using the Java
interface and then we will dive a little deeper

00:05:39.960 --> 00:05:47.120
into this and see how we do it in native code.
So if you look up here, we have this class

00:05:47.120 --> 00:05:52.680
called audiotrack sample. And it implements
runnable so it's going to spinoff a thread

00:05:52.680 --> 00:05:56.650
that's generating audio.
And we have declared a few variables here.

00:05:56.650 --> 00:06:03.870
End buffer size is the size of the buffer
in samples. In this case it's 8,000, which

00:06:03.870 --> 00:06:08.710
translates to roughly about 180 milliseconds
of audio, but you choose whatever you want.

00:06:08.710 --> 00:06:13.570
There are some constraints on that. We need
to buffer enough audio that we are not running

00:06:13.570 --> 00:06:18.190
out of data and having audio breakup, so you
may need to experiment with that depending

00:06:18.190 --> 00:06:25.110
on your application.
Mtrack is the audio track object itself, and

00:06:25.110 --> 00:06:29.949
then mbuffer is the audio sample buffer. And
then msample is a state variable we will talk

00:06:29.949 --> 00:06:34.100
about a later which we use for generating
audio. In the next slide you'll see where

00:06:34.100 --> 00:06:38.350
that comes into play.
We start off by initializing the buffer and

00:06:38.350 --> 00:06:43.840
we create an audio track. In this case it's
a music stream, so in Android we have this

00:06:43.840 --> 00:06:49.280
concept of different stream types, and each
of those stream types has their own individual

00:06:49.280 --> 00:06:55.580
volume control. So if you are doing a game
or something that's typically a foreground

00:06:55.580 --> 00:07:00.539
application, you probably want to use the
music stream. Ring tones and notifications

00:07:00.539 --> 00:07:04.950
are controlled by a different stream.
So we have -- basically you can set different

00:07:04.950 --> 00:07:11.419
audio levels for different types of streams.
And in this case it's a mono output so we

00:07:11.419 --> 00:07:16.780
are just generating one channel. The audio
mixer will automatically reflect that to both

00:07:16.780 --> 00:07:23.350
left and right channels if it's a stereo output.
And then the buffer size is specified in bytes.

00:07:23.350 --> 00:07:27.501
So that's where we are multiplying the buffer
size by two when you look at that. M buffer

00:07:27.501 --> 00:07:33.699
size times two is because audiotrack wants
the buffer size in bytes.

00:07:33.699 --> 00:07:38.740
And we want to operate in stream mode because
we are going to be generating this audio on

00:07:38.740 --> 00:07:43.150
the fly.
And then the run function here, next part

00:07:43.150 --> 00:07:48.421
of the slide here, we start playing the track,
basically arms the track to start playing.

00:07:48.421 --> 00:07:53.160
And as soon as you have buffered enough audio,
it will start output to go the audio output

00:07:53.160 --> 00:07:59.880
device. And then we have a loop to generate
the audio and write it to the audio track.

00:07:59.880 --> 00:08:04.350
So those are the next two lines there.
Obviously if were you doing a real application,

00:08:04.350 --> 00:08:07.130
would you need something a little bit more
than that because you probably would want

00:08:07.130 --> 00:08:11.980
to stop at some point. I haven't implemented
that code. It wouldn't all fit on the slide

00:08:11.980 --> 00:08:18.010
but I think you get the idea.
So moving on, this is the code that actually

00:08:18.010 --> 00:08:22.229
is going to generate a tone. And I used something
really simple here. We are just generating

00:08:22.229 --> 00:08:28.570
a sawtooth Wave, that's just basically a linear
ramp. It sounds like a fat synthesizer sound.

00:08:28.570 --> 00:08:37.169
We are not -- At this rate, by adding 600
we are going to wrap around, and we generate

00:08:37.169 --> 00:08:45.230
roughly 400 hertz sawtooth. So it's filling
the buffer with some really simple audio.

00:08:45.230 --> 00:08:48.910
So that's basically how the Java interface
works.

00:08:48.910 --> 00:08:55.250
Now, if you want to use native code, first
thing is you need to download the Android

00:08:55.250 --> 00:08:59.571
NDK. So you have the Android SDK. You are
going to download the NDK as well.

00:08:59.571 --> 00:09:06.492
You are going to build a Java application.
You start with a Java app. And you are going

00:09:06.492 --> 00:09:13.260
to add the native code to it as a library.
So once you have your Java app built, like

00:09:13.260 --> 00:09:18.790
let's say this one, you create a native library,
and that really just involves writing some

00:09:18.790 --> 00:09:29.300
source code. You can take a sample MIG file
from the NDK and use that to build your library.

00:09:29.300 --> 00:09:39.110
And the library will have basically implementations
of Java methods that are declared in the Java

00:09:39.110 --> 00:09:43.140
object.
And so that's basically -- we add a native

00:09:43.140 --> 00:09:46.861
method to a Java class and we can call the
native code from Java.

00:09:46.861 --> 00:09:52.760
And then in the Java class itself, at some
point you need to load the native library

00:09:52.760 --> 00:09:58.890
to allow the VM to link the methods that you
have declared in the Java class to the actual

00:09:58.890 --> 00:10:03.560
native code.
And then you just call your native methods

00:10:03.560 --> 00:10:11.920
from Java just like you would anything else.
So we'll take a look at some actual native

00:10:11.920 --> 00:10:14.880
code here.
So this is -- going back to our Java sample,

00:10:14.880 --> 00:10:20.630
this is the Java code again. This is actually
the same as the first slide. Nothing has changed

00:10:20.630 --> 00:10:26.080
here. We are doing all the same things.
All the magic happens on the next page, which

00:10:26.080 --> 00:10:31.990
is where we were doing the generate tone stuff
before.

00:10:31.990 --> 00:10:38.980
So now if you look at it, we have this static
declaration here, system load library, and

00:10:38.980 --> 00:10:47.120
that actually loads a .SO file called generate
underscore tone.so. So that's where the native

00:10:47.120 --> 00:10:51.611
code is getting written. So when you are doing
your native code, in particular this example

00:10:51.611 --> 00:10:59.110
I probably would have a generate tone.c or
C++ file, and in that I would have my native

00:10:59.110 --> 00:11:05.019
code, I would build it, link it, and I would
end up with an SO, and this is how it gets

00:11:05.019 --> 00:11:09.670
loaded. So as soon as this class gets loaded
in your application, the VM is going to go

00:11:09.670 --> 00:11:15.580
look for that library, load it, and then it
will do the links from the Java code to the

00:11:15.580 --> 00:11:22.500
native code for you.
And that all happens automatically.

00:11:22.500 --> 00:11:28.130
And then in this next section, we have the
public native int generate tone.

00:11:28.130 --> 00:11:33.149
So basically this is the same call signature
we had for the Java object. We just added

00:11:33.149 --> 00:11:41.950
a native thing to it and that tells the Java
byte code generator to look for an external

00:11:41.950 --> 00:11:48.910
symbol, which is this native code.
And then here we're moving on to the actual

00:11:48.910 --> 00:11:59.089
native code. So this is in C++. And you are
going to have a function here, generate tone,

00:11:59.089 --> 00:12:04.399
and it needs to have the package signature
and class signature that you declared in your

00:12:04.399 --> 00:12:09.070
Java class.
So going back to the Java class, it was, you

00:12:09.070 --> 00:12:15.230
know, com.example.JNI sample is the class,
and generate tone is the method. So this is

00:12:15.230 --> 00:12:18.180
how the linker finds the method and links
it up.

00:12:18.180 --> 00:12:26.550
This works if you have a single call signature.
So if I only have one generate tone, it will

00:12:26.550 --> 00:12:31.019
be able to find it that way.
If you have multiple -- and if you have got

00:12:31.019 --> 00:12:35.381
an overloaded method, then you are going to
need to do something more sophisticated than

00:12:35.381 --> 00:12:37.880
that which gives it the actual call signatures
with at parameters.

00:12:37.880 --> 00:12:43.040
SO but for a simple thing like this, this
is the easiest way to do it. You just have

00:12:43.040 --> 00:12:47.060
one method and overloading it is really simple
to get this to work.

00:12:47.060 --> 00:12:52.740
So just some simple stuff.
You typically, when you are writing native

00:12:52.740 --> 00:12:55.810
code, you want to do some sanity checking
on the parameters you are getting from Java.

00:12:55.810 --> 00:13:02.630
So -- because it's easy to end up with a crash
in native code as opposed to in Java where

00:13:02.630 --> 00:13:06.340
you get a stack trace.
If it happens in native code, you are not

00:13:06.340 --> 00:13:09.930
going to get the nice stack trace. And if
you remember this morning, if you were at

00:13:09.930 --> 00:13:15.110
the keynote we have this new reporting code
in Market that allows you to get stack traces

00:13:15.110 --> 00:13:19.470
if you have a Java crash, where you are not
going to get those nice traces if it happens

00:13:19.470 --> 00:13:23.940
in native code. So make sure your native code
is clean.

00:13:23.940 --> 00:13:27.709
So here we are just checking to make sure
that we are getting a reasonable volume for

00:13:27.709 --> 00:13:37.550
size. And the next line we call get primitive
array critical, and we are passing in the

00:13:37.550 --> 00:13:47.360
buffer pointer, basically the Java buffer
object. And we get back a C short star pointer.

00:13:47.360 --> 00:13:53.860
So that's the actual pointer to the buffer
data that Java would see.

00:13:53.860 --> 00:13:58.740
So it basically gives us the hooks we need
to go talk -- you know, access that buffer

00:13:58.740 --> 00:14:02.889
from C code.
One critical thing here, basically this is

00:14:02.889 --> 00:14:08.829
pinning that array in memory, so it can't
be GC'd.

00:14:08.829 --> 00:14:14.139
During a GC, it could potentially move around,
so this is basically telling the VM don't

00:14:14.139 --> 00:14:18.000
move that code -- that object. I am doing
something with it.

00:14:18.000 --> 00:14:21.910
And of course at the end of that, you need
to make sure you release it.

00:14:21.910 --> 00:14:28.149
So now that we have a valid pointer to the
buffer and we're going to check that, obviously,

00:14:28.149 --> 00:14:33.029
make sure that we actually get a valid pointer
back, because it's possible that object might

00:14:33.029 --> 00:14:40.720
have been -- might not be a valid Java object.
So doing another check there to make sure

00:14:40.720 --> 00:14:46.829
Pdata is actually not null.
Then we basically have implemented that code

00:14:46.829 --> 00:14:53.170
to fill the buffer with audio in C here.
So it's pretty much the same algorithm. We

00:14:53.170 --> 00:14:59.720
start with a sample of zero, fill it up, wrap
the -- have the value advance by 600 each

00:14:59.720 --> 00:15:02.660
time, and that gives us, again, roughly a
400 hertz tone.

00:15:02.660 --> 00:15:10.380
And at the end there, the release primitive
critical array, basically that -- array critical,

00:15:10.380 --> 00:15:16.019
unpins that memory and allows a GC to happen
if necessary later on.

00:15:16.019 --> 00:15:21.510
So it's a really simple way to take advantage
of native C processing.

00:15:21.510 --> 00:15:27.490
So this is a way, if you wanted do something
sophisticated like maybe a synthesize arrangement

00:15:27.490 --> 00:15:33.089
or your own mixer, audio mixer, or do some
signal processing in C, could you hook into

00:15:33.089 --> 00:15:43.209
the existing audio track class to do that.
So there's one little difference in this code

00:15:43.209 --> 00:15:48.519
from what we had in the Java code.
In the Java code, that sawtooth generator,

00:15:48.519 --> 00:15:54.060
we were actually saving the state of the sawtooth
generator at the end of each generate tone

00:15:54.060 --> 00:15:59.500
in a variable called Msample.
In the C code I just showed you, I wasn't

00:15:59.500 --> 00:16:03.089
doing that. We'd actually end up with discontinuity
in the signal.

00:16:03.089 --> 00:16:08.940
So the reason I didn't include that is it
wouldn't -- if I put that on one slide, you

00:16:08.940 --> 00:16:12.550
wouldn't be able to read it, so I had broken
it up into a couple of pieces.

00:16:12.550 --> 00:16:17.720
Now we are going to see how from C code you
can access other Java objects.

00:16:17.720 --> 00:16:25.470
So going back to the same code, I have kind
of pulled out some stuff -- the code-to-pin,

00:16:25.470 --> 00:16:28.839
the data array and stuff like that -- and
I am focusing right now on that one little

00:16:28.839 --> 00:16:32.190
section of code that actually generates the
tone.

00:16:32.190 --> 00:16:38.470
So at the beginning of this function, if you
look at the top you see a static there which

00:16:38.470 --> 00:16:46.449
is Msample field. This is an index into the
 -- it basically gives us a way to get at

00:16:46.449 --> 00:16:51.959
the Java objects from the Java -- Java members
from the Java object itself.

00:16:51.959 --> 00:16:57.079
So at the beginning of this function, we check
to see if that's zero. So if it was static

00:16:57.079 --> 00:17:01.490
and initialized to zero, the first time you
call this function it's going to be zero.

00:17:01.490 --> 00:17:06.480
And then what we do is we look up the class
through the Java environment.

00:17:06.480 --> 00:17:13.250
So in the call of the signature of this function,
we get the Java VM environment, and we also

00:17:13.250 --> 00:17:17.989
get the object.
So we're going to use the Java VM to call

00:17:17.989 --> 00:17:23.539
find class, and we know the name of that package,
so we are going to go look for that class,

00:17:23.539 --> 00:17:31.529
and we get back a field ID for the class itself.
And assuming we actually find that class,

00:17:31.529 --> 00:17:35.899
and it's hard to believe that might fail but,
you know, here is the sanity check just in

00:17:35.899 --> 00:17:45.039
case. We then do a lookup with get field ID
for the member called Msample.

00:17:45.039 --> 00:17:52.200
So in Java, we call -- this is an int member
of the Java class.

00:17:52.200 --> 00:17:57.350
So we're looking it up and the S tells it
that it's a short type. So we are looking

00:17:57.350 --> 00:18:06.019
for a variable Msample, type short, that's
in the class com.example.JNI sample.JNI sample.

00:18:06.019 --> 00:18:11.000
And it should return back a value that's not
zero. And that's the handle you need to get

00:18:11.000 --> 00:18:17.590
at the object within the member of that class
of that particular object.

00:18:17.590 --> 00:18:23.229
So I have commented out the code there that
actually does the pinning the data buffer.

00:18:23.229 --> 00:18:27.739
And instead we're going to focus on the actual
tone generator.

00:18:27.739 --> 00:18:35.179
So the first thing we do is we call get short
field, and we are passing in THIZ which is

00:18:35.179 --> 00:18:41.769
the pointer to the Java object we got and
the call to this native function. And we are

00:18:41.769 --> 00:18:47.580
passing M sample field which is the index
in the class to that member within the Java

00:18:47.580 --> 00:18:50.509
object.
And we get back the value of that object.

00:18:50.509 --> 00:18:56.730
So now we are able to retrieve state from
the Java side, and then in the function, we

00:18:56.730 --> 00:19:01.950
use that to generate the tone.
So now we can start with the last sample and

00:19:01.950 --> 00:19:07.330
we continue generating from where we were.
This little loop is the same as it was before.

00:19:07.330 --> 00:19:12.049
Then at the end, we are going to save that
value back into the Java object itself.

00:19:12.049 --> 00:19:18.820
So set short field, we pass the dot Java object
pointer, M sample field which is the field

00:19:18.820 --> 00:19:23.970
we want to set and the actual value that we
want to set it to.

00:19:23.970 --> 00:19:27.279
And then what that will do is now reflect
that value up into Java.

00:19:27.279 --> 00:19:33.549
So this allows you to store state information
in native code in the Java object itself,

00:19:33.549 --> 00:19:38.279
instead of having to keep a separate set of
 -- you know, a structure or something like

00:19:38.279 --> 00:19:42.730
that, to store that code.
If you are just doing a few variables, this

00:19:42.730 --> 00:19:46.999
is probably the way you want to do it.
The other option you can do is you can store

00:19:46.999 --> 00:19:54.210
a pointer. There are some -- Since Java doesn't
have a native pointer type, there's some pitfalls

00:19:54.210 --> 00:19:59.970
to that. At some point, if we change to larger
64-bit pointers, for example, if we ever had

00:19:59.970 --> 00:20:04.820
a device that had that, you could run into
problems. But we do a lot of this stuff in

00:20:04.820 --> 00:20:11.409
the platform code where we will store a pointer
to a bunch of data, because there is a fairly

00:20:11.409 --> 00:20:17.080
good cost to looking up the value of the Java
object through the VM.

00:20:17.080 --> 00:20:21.429
So if you have a lot of state information,
you might want to consider saving that as

00:20:21.429 --> 00:20:27.389
a C structure or something. So you allocate
it when the object is created, and you de-allocate

00:20:27.389 --> 00:20:31.770
it when it's released.
It's a little trickier to get that right because

00:20:31.770 --> 00:20:37.830
you have to do all the correct work to make
sure you are not leaking memory.

00:20:37.830 --> 00:20:43.560
So that's kind of the quick take-through of
the native signal processing.

00:20:43.560 --> 00:20:53.330
I am going to just really quickly run through
the power and resource stuff.

00:20:53.330 --> 00:20:58.879
If you have seen me before, I say this every
time. Make sure you call release on media

00:20:58.879 --> 00:21:04.809
objects. Almost every object we in the media
framework has a release call. The reason for

00:21:04.809 --> 00:21:11.190
the release call is if you don't call release,
you have to wait for a GC to actually call

00:21:11.190 --> 00:21:16.820
the finalizer, and that's when the resources
are de-allocated.

00:21:16.820 --> 00:21:21.129
Typically, for example, let's say you have
got a media player. There's very likely a

00:21:21.129 --> 00:21:26.179
hardware CODEC that's been instantiated in
the media server to decode video or decode

00:21:26.179 --> 00:21:34.679
audio. And as long as you are holding that
media player object in your Java application,

00:21:34.679 --> 00:21:40.050
there are resources in the media server that
are being consumed. That includes, you know,

00:21:40.050 --> 00:21:42.779
potentially battery power, DSP, et cetera,
et cetera.

00:21:42.779 --> 00:21:46.549
So you want to make sure you clean those up
quickly.

00:21:46.549 --> 00:21:53.419
And if you wait for a GC, it may be five,
ten seconds, who knows, before the GC happens.

00:21:53.419 --> 00:21:55.889
Just depends on how much memory pressure there
is.

00:21:55.889 --> 00:22:01.190
So the release forces it to immediately do
a finalize on any of the native resources

00:22:01.190 --> 00:22:06.299
that have been allocated. So it's really critical
for battery power and also for interoperability.

00:22:06.299 --> 00:22:11.139
You don't want to be holding onto an object
that potentially another application might

00:22:11.139 --> 00:22:17.830
want to use or even another activity in your
own application.

00:22:17.830 --> 00:22:23.879
And as I say, this powers down unneeded hardware.
In many cases, for example, if we have a hardware

00:22:23.879 --> 00:22:31.490
CODEC, we are powering up a DSP that's consuming
maybe 40 or 50 milliamps more than the device

00:22:31.490 --> 00:22:36.389
consumes without the DSP turned on and that
can have a significant impact upon battery

00:22:36.389 --> 00:22:42.559
life over the long run.
And like I say, it's nice to cooperate with

00:22:42.559 --> 00:22:49.419
other applications. If you are getting switched
out, another application is coming up and

00:22:49.419 --> 00:22:54.679
it needs that same resource, it may not be
able to get it. So making sure you call release

00:22:54.679 --> 00:23:03.529
during the on-pause, make sure that other
applications can get access to it.

00:23:03.529 --> 00:23:06.769
Another critical thing is making sure that
you are actually calling from on pause and

00:23:06.769 --> 00:23:11.289
not from on destroy.
There are circumstances where you may not

00:23:11.289 --> 00:23:17.929
immediately get a call to your on destroy
in your activity, and in that case, a good

00:23:17.929 --> 00:23:21.620
example is from the home screen -- if you
are -- sorry. If you have an application running

00:23:21.620 --> 00:23:27.080
and somebody hits the home button, you typically
don't get -- immediately get an on destroy.

00:23:27.080 --> 00:23:35.570
And so you need to take that into consideration.
You really should do it on on-pause.

00:23:35.570 --> 00:23:42.619
So kind of moving quickly on to the next segment,
we're going to talk about what's new in Froyo.

00:23:42.619 --> 00:23:49.289
We have a new API we call audio focus, basically,
audio focus and transport control API.

00:23:49.289 --> 00:23:55.200
I will get into this a little bit later with
some sample code.

00:23:55.200 --> 00:23:59.019
We've made some improvements to soundpool,
some things that people have requested.

00:23:59.019 --> 00:24:09.710
We've also added a new -- some improvements
and a way to route audio specifically in an

00:24:09.710 --> 00:24:13.169
application, this is to accommodate things
like voice mail.

00:24:13.169 --> 00:24:17.809
And I'll go over that a little bit later.
And then we have also made some improvements

00:24:17.809 --> 00:24:24.710
to the camera.
So there you go.

00:24:24.710 --> 00:24:32.849
So Audio Focus and transport APIs.
If you've done any development on Android

00:24:32.849 --> 00:24:37.510
or have done anything, particularly with audio,
there's been a lot of problems with conflicts

00:24:37.510 --> 00:24:40.749
between applications, particularly if you
have an application that's playing in the

00:24:40.749 --> 00:24:48.719
background and, you know, streaming audio
or music playing or method casts or whatever.

00:24:48.719 --> 00:24:54.669
There were some things we did early on that
were probably not the best decisions, and

00:24:54.669 --> 00:24:59.980
so we went back and looked at that and said,
how can we improve the situation with cooperating

00:24:59.980 --> 00:25:06.349
between applications that want to play audio.
This particularly became a little distressing

00:25:06.349 --> 00:25:10.229
when we launched the navigation, turn-by-turn
navigation.

00:25:10.229 --> 00:25:16.269
Because here's an application that wants to
periodically inject voice prompts so you can,

00:25:16.269 --> 00:25:19.700
you know, hear -- you know, find out where
you're going.

00:25:19.700 --> 00:25:22.940
You're in the car.
But if you're listening to a podcast, having

00:25:22.940 --> 00:25:28.309
the voice prompts over the top of a podcast
is really, really distracting.

00:25:28.309 --> 00:25:31.239
So we wanted to make sure we could handle
that use case.

00:25:31.239 --> 00:25:35.399
There are a number of other use cases.
And I have a quick little demo I'll do in

00:25:35.399 --> 00:25:41.679
a minute that shows how that works.
So one of the things in designing this new

00:25:41.679 --> 00:25:46.919
API is, we wanted to make sure that we left
developers in control of their own applications.

00:25:46.919 --> 00:25:48.840
Now, there are a couple of ways we could have
gone about this.

00:25:48.840 --> 00:25:53.989
One is we could enforce it at the system level
and dictate, you know, only one app is ever

00:25:53.989 --> 00:25:57.969
going to make audio, be able to generate audio,
and that's it.

00:25:57.969 --> 00:26:03.869
And that's not what Android is about.
So we actually made a choice to say, how can

00:26:03.869 --> 00:26:08.739
we give applications the information they
need to make good choices about audio and

00:26:08.739 --> 00:26:12.159
do innovation?
So that's basically one of the philosophies

00:26:12.159 --> 00:26:16.849
while we're designing this.
So, basically, the way it works is, apps and

00:26:16.849 --> 00:26:21.620
services can register to receive events about
what's happening with Audio Focus.

00:26:21.620 --> 00:26:29.539
They can request Audio Focus.
So the concept of Audio Focus is, my application

00:26:29.539 --> 00:26:33.480
needs to be in the foreground of audio.
It's going to be producing something that

00:26:33.480 --> 00:26:37.929
the user needs to pay attention to.
An example of that might be the ring tone.

00:26:37.929 --> 00:26:42.139
It might be a music player app.
It might be YouTube.

00:26:42.139 --> 00:26:45.350
It might be streaming audio.
It might be an e-mail notification.

00:26:45.350 --> 00:26:49.320
But for some reason, you want to draw the
user's attention, and that should be the focal

00:26:49.320 --> 00:26:53.149
point as far as audio is concerned.
It doesn't necessarily get associated with

00:26:53.149 --> 00:26:58.519
the app that's in the foreground.
If I am browsing the Web, it's perfectly fine

00:26:58.519 --> 00:27:04.070
for audio to be streaming in the background.
But if there's a Web app that wants to make

00:27:04.070 --> 00:27:12.159
a sound, then we need a way to enable that
to take control.

00:27:12.159 --> 00:27:16.889
Another kind of unique thing is, if -- the
Audio Focus request can be either a permanent

00:27:16.889 --> 00:27:21.029
request, in other words, I want to take over,
or it can be a transient request.

00:27:21.029 --> 00:27:25.339
A transient request, a good example of a transient
request is like an e-mail notification.

00:27:25.339 --> 00:27:29.679
We're just going to briefly interrupt the
audio and then we're going to come back.

00:27:29.679 --> 00:27:36.239
So the app will know -- the background app,
the app that's losing focus will know, hey,

00:27:36.239 --> 00:27:40.649
you're temporarily losing it and you're going
to get it back later.

00:27:40.649 --> 00:27:43.860
And we've also included something called a
ducking hint.

00:27:43.860 --> 00:27:48.679
If you're familiar with -- goes back to the
old radio days when the deejays would talk

00:27:48.679 --> 00:27:54.169
over the audio, ducking is reducing the volume
so that you can talk over the top of it.

00:27:54.169 --> 00:28:01.389
And so for options that are requesting focus,
they can give a hint to other applications,

00:28:01.389 --> 00:28:07.330
hey, it's okay if you go ahead and continue
playing, just reduce your volume.

00:28:07.330 --> 00:28:12.749
And the idea there, if you think about the
navigation thing, if music is playing, it's

00:28:12.749 --> 00:28:16.830
perfectly fine for that to continue playing.
Just make it a little bit softer so I can

00:28:16.830 --> 00:28:22.789
hear the navigation prompt.
And then when the application is done, it

00:28:22.789 --> 00:28:27.489
should abandon focus.
It should say, I'm done doing my thing.

00:28:27.489 --> 00:28:35.929
You know, send it back to -- send the focuses
back to the previous app.

00:28:35.929 --> 00:28:37.759
And then transport controls work in the same
way.

00:28:37.759 --> 00:28:42.509
So transport controls, if you've got the wired
headset or you've got a Bluetooth headset

00:28:42.509 --> 00:28:49.130
that has next track, previous track type controls,
pause, resume, those controls, we had the

00:28:49.130 --> 00:28:51.339
same sort of issues with apps competing over
that.

00:28:51.339 --> 00:28:57.379
So the transport controls now follow the sort
of same paradigm of an application can request

00:28:57.379 --> 00:29:03.190
them and take over control of it so I don't
have two apps that are competing.

00:29:03.190 --> 00:29:08.679
We had situations where somebody would hit
the "play" button, and two applications would

00:29:08.679 --> 00:29:14.299
try to start playing at the same time.
So this resolves that issue.

00:29:14.299 --> 00:29:19.659
And then what we've done is, we have helper
classes that will be available that use -- using

00:29:19.659 --> 00:29:24.950
Reflection, will enable you to put this in
your application and still be backwards-compatible.

00:29:24.950 --> 00:29:29.820
So you can write an app that's compatible
with Donut and take advantage of these new

00:29:29.820 --> 00:29:36.580
APIs that are in Froyo.
So I'm going to show you a little bit of code

00:29:36.580 --> 00:29:40.809
here.
Not too much.

00:29:40.809 --> 00:29:45.249
It's pretty simple.
So in your -- this particular sample is really

00:29:45.249 --> 00:29:50.839
aimed at a service.
So this would be a music application or a

00:29:50.839 --> 00:29:55.979
podcast or something that's playing in background.
If you're doing an app that's playing in foreground,

00:29:55.979 --> 00:29:58.999
you probably use -- put your hooks in different
places.

00:29:58.999 --> 00:30:05.769
You put it in the on resume and on pause instead.
But in this case, we're going to do it in

00:30:05.769 --> 00:30:11.219
on create and on destroy, because it's a service.
So in on create, we get a reference to the

00:30:11.219 --> 00:30:15.049
audio manager.
We're also going to tell it in this particular

00:30:15.049 --> 00:30:19.259
case that the streams we're outputting are
stream music type.

00:30:19.259 --> 00:30:25.559
This basically tells the system how the volume
control should behave when that app is in

00:30:25.559 --> 00:30:31.799
the foreground.
So the audio -- the volume controls will get

00:30:31.799 --> 00:30:36.999
routed to the music stream instead of a ring
tone or something else.

00:30:36.999 --> 00:30:41.589
The next thing is, so I've -- I kind of have
an abstract concept.

00:30:41.589 --> 00:30:51.799
So when the application starts playing, you
set this flag playing as true, and you start

00:30:51.799 --> 00:30:54.190
your thing.
And then at the very top of it, you can see

00:30:54.190 --> 00:30:58.299
"requesting audio focus."
So that tells the system, this application

00:30:58.299 --> 00:31:03.589
is now, you know, moving to the foreground
in audio.

00:31:03.589 --> 00:31:12.409
And then when the stream stops playing, you
are going to set the playing flag to false.

00:31:12.409 --> 00:31:19.000
And if we're not going to restart afterwards,
we're going to change the -- basically, abandon

00:31:19.000 --> 00:31:23.600
Audio Focus, so we're going to lose focus.
So this is a case where the user has deliberately

00:31:23.600 --> 00:31:27.200
said, "I'm done with this application.
I'm going to stop playing."

00:31:27.200 --> 00:31:30.209
We'll see how that restart flag works in the
next page of code.

00:31:30.209 --> 00:31:35.490
And then ondestroy, we always abandons focus.
So this takes your application out of the

00:31:35.490 --> 00:31:42.950
Audio Focus stack.
So in the next slide, this is kind of where

00:31:42.950 --> 00:31:52.090
the root of things happen with the system.
So there's this new interface called OnAudioFocusChangeListener.

00:31:52.090 --> 00:31:54.999
So we're creating a new on Audio Focus listener
 -- OnAudioFocusChangeListener.

00:31:54.999 --> 00:32:03.440
That's a long thing to say.
And it has a public method on Audio Focus

00:32:03.440 --> 00:32:08.129
change.
So you'll get a call anytime Audio Focus changes

00:32:08.129 --> 00:32:12.379
to your application, it's either going away
or it's coming back to you.

00:32:12.379 --> 00:32:18.229
So you can see here we're going to switch
on what event took place.

00:32:18.229 --> 00:32:22.039
And in the top -- the first part here, it
says Audio Focus loss.

00:32:22.039 --> 00:32:26.429
So we're losing Audio Focus.
So at that point, we're going to -- we're

00:32:26.429 --> 00:32:29.929
losing Audio Focus, and we're losing it permanently.
In other words, some other application has

00:32:29.929 --> 00:32:36.519
said, "I'm taking over audio now."
And so in this case, we set our restart flag

00:32:36.519 --> 00:32:39.129
to false.
Restart flag tells us whether we're going

00:32:39.129 --> 00:32:42.669
to restart later on.
Since we're losing it permanently, there's

00:32:42.669 --> 00:32:46.809
no point in restarting.
The only way that's going to happen is if

00:32:46.809 --> 00:32:52.249
the user actually goes back and says, "I want
to start this application again."

00:32:52.249 --> 00:32:56.350
In the case where it's a transient loss, we're
going to treat both transient and transient

00:32:56.350 --> 00:33:00.099
can duck the same.
And I'll show you how you can do the ten duck

00:33:00.099 --> 00:33:04.009
a little bit later.
So we set our restart flag to true.

00:33:04.009 --> 00:33:08.520
That means we've lost focus temporarily, we're
going to get it back.

00:33:08.520 --> 00:33:12.169
When we get it back, we want to start playing
again.

00:33:12.169 --> 00:33:18.089
And then we just pause the audio stream.
And then in this last case here, when we get

00:33:18.089 --> 00:33:24.009
the focus back, if we're not playing and the
restart flag is set, we're going to go ahead

00:33:24.009 --> 00:33:28.839
and resume playing and set the restart to
false.

00:33:28.839 --> 00:33:34.499
So now we've got Audio Focus back.
And this is how you resume back when whatever

00:33:34.499 --> 00:33:40.019
else was going on has finished, you get focus
back and you start playing again.

00:33:40.019 --> 00:33:44.909
So an example of this would be let's say an
e-mail notification comes in and it wants

00:33:44.909 --> 00:33:49.080
to play the little whatever you've chosen
for your notification tone.

00:33:49.080 --> 00:33:56.889
You're first going to get Audio Focus transient.
And that will tell you that you've lost focus

00:33:56.889 --> 00:33:59.320
temporarily.
Then you'll get Audio Focus gain.

00:33:59.320 --> 00:34:03.369
And that's when you restart your app.
So Audio Focus transient, stop playing.

00:34:03.369 --> 00:34:09.520
Audio Focus gain, start playing.
So it's pretty easy to implement this.

00:34:09.520 --> 00:34:14.510
The nice thing about this is that once you've
implemented this, the old tricks you might

00:34:14.510 --> 00:34:21.040
have used earlier to, like, look at the call
state, am I -- like, the music app used to

00:34:21.040 --> 00:34:25.710
do this, where it would look and say, is -- if
the phone is ringing, then I'm going to resume

00:34:25.710 --> 00:34:28.510
afterwards.
You don't have to worry about that anymore.

00:34:28.510 --> 00:34:33.379
This API will handle all of that.
So it's just one simple set of code you have

00:34:33.379 --> 00:34:38.950
to worry about.
And then this next bit of code shows how you

00:34:38.950 --> 00:34:46.500
might handle ducking.
So in a case where -- for a podcast type of

00:34:46.500 --> 00:34:51.599
application, you probably don't want to duck.
It doesn't make sense to have, you know, the

00:34:51.599 --> 00:34:55.829
audio talking, you know, from a podcast go
over some other prompt.

00:34:55.829 --> 00:35:00.680
But if it's a music application, streaming
music or some music player, it's probably

00:35:00.680 --> 00:35:03.010
okay to have that continue playing in the
background.

00:35:03.010 --> 00:35:10.660
So in this case, where all the code for transient
loss is the same as before but for the "can

00:35:10.660 --> 00:35:13.359
duck" case, we have a slightly different set
of code here.

00:35:13.359 --> 00:35:20.970
So in this case, I'm setting a flag in ducking
that says I'm ducking my volume temporarily

00:35:20.970 --> 00:35:26.570
to true, I save the old volume, and I set
my volume to the new volume.

00:35:26.570 --> 00:35:33.760
And the new volume in this case is whatever
that is, an eighth, it's, like, down 18 dB.

00:35:33.760 --> 00:35:39.300
And so now I've reduced the volume of my output
track while whatever it is is playing, continues

00:35:39.300 --> 00:35:43.099
to play.
And when I get focus back, if I was ducking,

00:35:43.099 --> 00:35:45.640
the first part of this clause is the same
as before.

00:35:45.640 --> 00:35:50.549
The second part of the clause is, if I ducked,
then I'm just going to restore the old volume.

00:35:50.549 --> 00:35:54.390
So this is where you can, in your application,
do something slightly different, maybe a better

00:35:54.390 --> 00:36:00.579
experience for the user.
Your choice, though.

00:36:00.579 --> 00:36:05.089
Then we have soundpool improvements.
Sorry. That was a kind of quick transition

00:36:05.089 --> 00:36:10.970
there.
We'll have sample code for this, by the way,

00:36:10.970 --> 00:36:17.130
in -- when we release the SDK or on -- probably
a blog entry coming up soon.

00:36:17.130 --> 00:36:22.779
And it will include the Reflection code, probably
some sample code on how a service might work.

00:36:22.779 --> 00:36:27.450
So going back to soundpool, a couple of things
we got asked about in soundpool.

00:36:27.450 --> 00:36:33.329
One of the problems people run into is, the
loading of sounds is asynchronous.

00:36:33.329 --> 00:36:36.529
And they wanted a way to know when a sound
was loaded.

00:36:36.529 --> 00:36:41.220
So we added new callbacks for once a sound
is loaded, you get a call back that tells

00:36:41.220 --> 00:36:43.770
you what sound got loaded.
So now you have a way of knowing when the

00:36:43.770 --> 00:36:49.630
sounds are all loaded.
Another big request, because soundpool keeps

00:36:49.630 --> 00:36:53.650
track of all the state of different voices,
you don't have a lot of visibility and what's

00:36:53.650 --> 00:36:56.211
going on.
If you're pausing the application, you don't

00:36:56.211 --> 00:37:00.510
know which streams to pause.
We added an API for automatically pausing

00:37:00.510 --> 00:37:06.269
all the streams that are currently playing,
and then the next bit is auto resuming all

00:37:06.269 --> 00:37:09.970
the streams that are playing.
So soundpool is a little more friendly in

00:37:09.970 --> 00:37:12.859
that respect when you're -- you don't have
to keep track of all the streams that are

00:37:12.859 --> 00:37:20.190
playing.
Talked about audio routing improvements.

00:37:20.190 --> 00:37:25.109
We have a -- the stream voice call is now
routed to ear piece by default.

00:37:25.109 --> 00:37:30.580
Previously, I think it went to speaker.
So if you have an application where you want

00:37:30.580 --> 00:37:34.910
to output to the ear piece, a good example
is a voice mail application.

00:37:34.910 --> 00:37:37.441
Users have an expectation of privacy in that
case.

00:37:37.441 --> 00:37:41.950
You don't want your voice mail necessarily
blasting, you know, if your girlfriend is

00:37:41.950 --> 00:37:49.261
calling with some maybe not safe for work
message, you don't want that to go out on

00:37:49.261 --> 00:37:51.559
the speaker.
In this case, you can have it routed to the

00:37:51.559 --> 00:37:56.270
ear piece by default.
And then you can call set speaker on to route

00:37:56.270 --> 00:38:00.630
it to the speaker if they want to.
So a voice mail app, you probably have a speaker

00:38:00.630 --> 00:38:05.990
button to route it to the speaker.
Another big request we've had is routing audio

00:38:05.990 --> 00:38:11.769
to the Bluetooth SCO device.
So SCO is the eight-kilohertz synchronous

00:38:11.769 --> 00:38:18.400
link that is used for in-call audio.
It's a low-quality audio stream, but it's

00:38:18.400 --> 00:38:20.549
ubiquitous.
Every Bluetooth headset supports it.

00:38:20.549 --> 00:38:27.900
A2DP is the high-quality stereo streams.
There are a lot of devices that don't support

00:38:27.900 --> 00:38:31.680
that, so there was a request to be able to
support Bluetooth SCO output.

00:38:31.680 --> 00:38:36.160
So now you can do that.
You call start Bluetooth SCO, open your voice

00:38:36.160 --> 00:38:41.650
stream, stream voice call, and that will get
it routed out to the SCO output.

00:38:41.650 --> 00:38:46.450
Again, useful for voice mail applications
or other applications where you're just going

00:38:46.450 --> 00:38:54.309
to send out low-quality audio.
Again, privacy issues.

00:38:54.309 --> 00:39:01.030
Camera improvements.
We have a new preview API if you want to do

00:39:01.030 --> 00:39:06.359
signal processing on preview frames.
It allows you to -- whoops.

00:39:06.359 --> 00:39:09.400
Go back.
It allows you to avoid GCs.

00:39:09.400 --> 00:39:14.119
The old way, we would allocate a buffer, copy
the data into it, and then you would end up

00:39:14.119 --> 00:39:19.440
with a GC every time you stopped using those.
In this one, you can preallocate your buffers.

00:39:19.440 --> 00:39:24.000
You send them down, you in-queue them to the
native code, and it fills the buffer and returns

00:39:24.000 --> 00:39:28.069
it to you, and there's no GC.
So we actually found that it doubled the frame

00:39:28.069 --> 00:39:39.130
rate for Goggles' application, for example.
And, let's see, there's also an API for compressing

00:39:39.130 --> 00:39:44.740
YUV frames from the preview to JPEG.
So if you're trying to do, like, a webcam

00:39:44.740 --> 00:39:49.920
type application, you can take the preview
frames, encode them as JPEG, and then ship

00:39:49.920 --> 00:39:54.411
them off over Wi-Fi.
We're actually getting, on Nexus One, I think

00:39:54.411 --> 00:40:02.180
about ten frames per second over a Wi-Fi connection.
So it's pretty decent.

00:40:02.180 --> 00:40:07.089
There's a new no-thumbnail mode for the JPEG
encoder.

00:40:07.089 --> 00:40:11.420
Previously, we automatically generated a thumbnail.
If you're doing really small JPEGs and it

00:40:11.420 --> 00:40:15.930
doesn't make any sense, so there's a way to
tell it you don't want the thumbnail generated

00:40:15.930 --> 00:40:21.920
and reduce the size of the JPEG.
Camera also reports field of view and focal

00:40:21.920 --> 00:40:29.430
length parameters in XF.
So now you can get that data back. for AR

00:40:29.430 --> 00:40:34.750
applications and things like that, it's -- can
be pretty useful, you know, determining what

00:40:34.750 --> 00:40:41.039
the field is you're looking at and using it
for signal processing.

00:40:41.039 --> 00:40:47.039
And then we added exposure control settings
so you can, you know, go plus one, plus two,

00:40:47.039 --> 00:40:52.549
minus one, minus two on your exposures.
And then, finally, we added portrait mode

00:40:52.549 --> 00:40:56.039
support.
We kind of had a hack-in for some devices

00:40:56.039 --> 00:40:58.630
previously that I know a few developers out
there figured out.

00:40:58.630 --> 00:41:03.369
It was never intended to be supported.
We now have an official way of supporting

00:41:03.369 --> 00:41:08.569
it.
I'm going to skip this, 'cause we're trying

00:41:08.569 --> 00:41:12.940
to catch up.
And I'm going to talk about road map really

00:41:12.940 --> 00:41:19.970
briefly, and then we'll go on to Q&amp;A.
So in the next release, we're planning to

00:41:19.970 --> 00:41:28.029
introduce an OpenSL ES API.
It's -- OpenSL ES is a huge surface.

00:41:28.029 --> 00:41:29.910
We're going to start small and build up from
there.

00:41:29.910 --> 00:41:36.220
But the intention is to expose a native audio
API and OpenSL ES will be the abstraction

00:41:36.220 --> 00:41:41.020
layer for that.
That means basically all the audio track stuff

00:41:41.020 --> 00:41:44.570
in native code will get converted to an SL
ES layer.

00:41:44.570 --> 00:41:48.920
We actually have bits of that code up and
running now.

00:41:48.920 --> 00:41:54.589
We're going to add OpenAL support, OpenAL
as a shim over the OpenSL ES.

00:41:54.589 --> 00:41:59.380
So, basically, for those of you who are writing
applications that use AL on other platforms,

00:41:59.380 --> 00:42:05.020
it should make it a little easier to port
your games, or other applications, for that

00:42:05.020 --> 00:42:08.380
matter.
We're adding -- as part of the OpenSL ES,

00:42:08.380 --> 00:42:13.670
we're going to add the effects processing
stuff, so that allows you to put in equalizers

00:42:13.670 --> 00:42:19.100
or things like that.
We're also going to start exposing some of

00:42:19.100 --> 00:42:23.210
the low-level media APIs.
Specifically, what we're going to do is have

00:42:23.210 --> 00:42:32.710
a Java API that allows you to build the player
graph dynamically in your application.

00:42:32.710 --> 00:42:38.829
In particular, if you have a streaming format
that we don't support, maybe you've invented

00:42:38.829 --> 00:42:44.410
the best RTSP protocol in the world and you
want to do some extension or something like

00:42:44.410 --> 00:42:48.890
that, one of the problems has been that you
couldn't get access to the codecs, and, you

00:42:48.890 --> 00:42:52.920
know, it was really hard to do that.
So this will actually allow you to build the

00:42:52.920 --> 00:42:58.720
graph in your code.
And then, again, using something similar to

00:42:58.720 --> 00:43:02.740
the techniques we talked about with audio
track, if you want to do native processing

00:43:02.740 --> 00:43:06.220
where you do something to the frames or whatever,
you'll be able to get access to it through

00:43:06.220 --> 00:43:14.559
native code.
Yesterday, we announced VP8 open source to

00:43:14.559 --> 00:43:18.430
the world.
We're saying now that we're going to -- I

00:43:18.430 --> 00:43:25.020
think I found another mike down there -- we're
going to support it in the next Android release.

00:43:25.020 --> 00:43:30.779
Unfortunately, the bit stream specifics were
a little too late for the Froyo release.

00:43:30.779 --> 00:43:38.420
But we have it up and running in our next
release, and so it includes VP8 support, Vorbis

00:43:38.420 --> 00:43:48.010
support, and WebM, which is our Metroska-specific
support for VP8 and Vorbis.

00:43:48.010 --> 00:43:58.049
We're going to add a FLAC decoder.
We are going to add AAC-LC encoder, software

00:43:58.049 --> 00:44:02.890
encoder, for the devices that don't have hardware
support already.

00:44:02.890 --> 00:44:11.309
And we're going to add an AMR wide-band encoder.
Those will both be open sourced codecs, so

00:44:11.309 --> 00:44:15.750
all the code would be available.
Obviously, there are still licensing issues.

00:44:15.750 --> 00:44:19.130
There are patents that you have to license
if you have a device.

00:44:19.130 --> 00:44:25.480
But the code will be open.
And now we're going to move on to Q&amp;A.

00:44:25.480 --> 00:44:31.849
I'll bring up the slide again if you want
to follow in Wave.

00:44:31.849 --> 00:44:36.040
So anybody want to come up to the microphone?
&gt;&gt;&gt; Hello.

00:44:36.040 --> 00:44:41.910
Now, one question I had is when to release
the media player.

00:44:41.910 --> 00:44:45.589
Because in Android, there's no concept of
exiting an application.

00:44:45.589 --> 00:44:51.600
So if a user pauses playback, how do you know
that they won't -- is that the time to release

00:44:51.600 --> 00:44:53.579
the media?
How do you know that they won't necessarily

00:44:53.579 --> 00:44:59.610
want to start playing a second later?
&gt;&gt;David Sparks: So I didn't quite understand

00:44:59.610 --> 00:45:00.769
the question.
Could you try again.

00:45:00.769 --> 00:45:05.690
&gt;&gt;&gt; I don't quite understand when I should
be calling -- when I should be releasing -- when

00:45:05.690 --> 00:45:12.250
I should call release on media player.
Because if you -- because you don't necessarily

00:45:12.250 --> 00:45:15.680
know when the user is kind of done with your
application.

00:45:15.680 --> 00:45:19.130
For example, if they just pause it --
&gt;&gt;David Sparks: Oh, oh, okay.

00:45:19.130 --> 00:45:22.410
So, yeah, right.
So if they're just pausing it, I think you

00:45:22.410 --> 00:45:28.769
probably need to make some intelligent choice,
like maybe you send yourself a delayed message

00:45:28.769 --> 00:45:32.740
at five seconds.
And if they haven't restarted in five seconds,

00:45:32.740 --> 00:45:37.710
then you tear it down.
A good example, so let's say you wanted to

00:45:37.710 --> 00:45:43.069
resume playing where you were.
You get the current play position, and you

00:45:43.069 --> 00:45:48.759
save that off, you know, recreate the media
player, and seek to that position and start

00:45:48.759 --> 00:45:51.990
playing again.
But I wouldn't go too much longer than that,

00:45:51.990 --> 00:45:56.250
because, you know, you leave that thing up,
the DSP, even if it's not doing anything,

00:45:56.250 --> 00:45:59.360
is still consuming energy.
&gt;&gt;&gt; Thank you.

00:45:59.360 --> 00:46:05.299
&gt;&gt;David Sparks: Uh-huh.
&gt;&gt;&gt; Does your Audio Focus API have any support

00:46:05.299 --> 00:46:10.700
for conflicts between recording and playback
or two different recording devices trying

00:46:10.700 --> 00:46:14.190
to record at the same time?
&gt;&gt;David Sparks: Two different recording -- So

00:46:14.190 --> 00:46:18.521
the question -- the question was, is there
a conflict between recording and playback?

00:46:18.521 --> 00:46:20.530
&gt;&gt;&gt; Yes.
That's part of it.

00:46:20.530 --> 00:46:22.990
Recording and playback, or two different recording
apps.

00:46:22.990 --> 00:46:26.380
&gt;&gt;David Sparks: Okay.
So we don't have a concept of two different

00:46:26.380 --> 00:46:30.339
recording inputs yet.
I think there's -- if I'm not mistaken, there's

00:46:30.339 --> 00:46:34.680
a source in the audio track -- or audio record
interface.

00:46:34.680 --> 00:46:40.010
So you could potentially have more than one.
There's no reason I could think of where -- that

00:46:40.010 --> 00:46:42.360
we couldn't support multiple inputs at the
same time.

00:46:42.360 --> 00:46:49.960
You know, conceptually, underneath, it would
just mean record -- a new -- there's a record

00:46:49.960 --> 00:46:54.220
thread in the media server that would need
to be duplicated for multiple inputs.

00:46:54.220 --> 00:46:58.910
But there's no reason from a platform perspective
we couldn't do that.

00:46:58.910 --> 00:47:03.200
And as far as recording and playback at the
same time, that should work.

00:47:03.200 --> 00:47:07.460
The issue you have with recording, if you're
playing something on the device, you probably

00:47:07.460 --> 00:47:11.600
have some leakage from the speaker back to
the microphone, for example, and we don't

00:47:11.600 --> 00:47:18.390
have any way to use the acoustic echo cancellation
algorithms that are typically built into the

00:47:18.390 --> 00:47:19.970
baseband.
So you have to do that yourself.

00:47:19.970 --> 00:47:24.070
&gt;&gt;&gt; Yeah, I was more interested in preventing
it than allowing it.

00:47:24.070 --> 00:47:27.500
&gt;&gt;David Sparks: Yeah, there's no way to prevent
it.

00:47:27.500 --> 00:47:31.510
&gt;&gt;&gt; Okay.
We have a media streaming application that

00:47:31.510 --> 00:47:39.390
we're working on that has content behind authenticated,
you know, protection SSO.

00:47:39.390 --> 00:47:44.819
And we had a little bit of difficulty because
the media player allows you to supply a progressive

00:47:44.819 --> 00:47:50.490
playback URL, but there's no way to manipulate
the cookie collection or anything with that.

00:47:50.490 --> 00:47:53.810
&gt;&gt;David Sparks: Right.
&gt;&gt;&gt; Are there any changes in that area?

00:47:53.810 --> 00:47:56.849
&gt;&gt;David Sparks: There's -- I know that there
is a feature request in.

00:47:56.849 --> 00:48:01.680
I'm not sure -- hopefully, we'll get to that
in the next release, but it's definitely not

00:48:01.680 --> 00:48:03.019
in Froyo.
&gt;&gt;&gt; Okay.

00:48:03.019 --> 00:48:04.940
Thanks.
&gt;&gt;David Sparks: That's a good one.

00:48:04.940 --> 00:48:08.029
And that's definitely something we want to
deal with.

00:48:08.029 --> 00:48:13.259
By the way, just some people have dealt with
that by having a proxy server on the device

00:48:13.259 --> 00:48:18.190
that does that.
&gt;&gt;&gt; Dave, do you have any plans for bookmarking?

00:48:18.190 --> 00:48:22.759
Right now, applications don't know when a
particular part of the audio stream has actually

00:48:22.759 --> 00:48:26.160
reached the codec.
So if you're giving directions, for example,

00:48:26.160 --> 00:48:31.839
you don't know where you are in that stream.
&gt;&gt;David Sparks: So basically you want to know

00:48:31.839 --> 00:48:34.950
what the latency is through the playback path?
&gt;&gt;&gt; Correct.

00:48:34.950 --> 00:48:38.839
And if it gets paused, that you can actually
find out that it was paused and that it hasn't

00:48:38.839 --> 00:48:42.839
reached it yet.
&gt;&gt;David Sparks: Right.

00:48:42.839 --> 00:48:46.900
So there's -- there's no provisions for that
yet.

00:48:46.900 --> 00:48:51.960
I think we get that through the OpenSL ES
interface, if I'm not mistaken.

00:48:51.960 --> 00:48:56.369
That drives the latency all the way back.
So you should be able to get that information

00:48:56.369 --> 00:48:59.749
when we get to there.
Audio track doesn't do that now.

00:48:59.749 --> 00:49:04.950
And there are some challenges with that.
If you look at Bluetooth, for example, we

00:49:04.950 --> 00:49:08.900
have no idea how much audio is actually getting
buffered in the Bluetooth device.

00:49:08.900 --> 00:49:15.269
It can be, you know, hundreds of milliseconds.
So it's somewhat, you know, nebulous what

00:49:15.269 --> 00:49:18.009
that actually means.
The ondevice we should be able to get a better

00:49:18.009 --> 00:49:22.119
enough.
&gt;&gt;&gt; I have two questions.

00:49:22.119 --> 00:49:26.490
Do we support track-level volume control?
I know that in Android, we have different

00:49:26.490 --> 00:49:31.590
streams, and each stream has volume.
Can I have two applications playing the same

00:49:31.590 --> 00:49:35.359
music streams, but can I have two different
volumes for two different tracks?

00:49:35.359 --> 00:49:37.720
Is that supported today?
&gt;&gt;David Sparks: Well, yeah.

00:49:37.720 --> 00:49:44.059
So each -- each audio track has a volume control.
And the media player is built on top of audio

00:49:44.059 --> 00:49:45.980
track.
So it has a separate volume control.

00:49:45.980 --> 00:49:51.259
And there is a -- if I'm not mistaken, there's
a set volume interface on the media player.

00:49:51.259 --> 00:49:57.660
So you can actually set the individual volume
for each media player, as well as each audio

00:49:57.660 --> 00:49:58.660
track.
&gt;&gt;&gt; Okay.

00:49:58.660 --> 00:50:04.359
The next question is, can I route -- I know
routing is -- something is taken care of by

00:50:04.359 --> 00:50:08.059
the framework.
Is there any plan of giving control to the

00:50:08.059 --> 00:50:13.410
application to say that, hey, track one broken,
fix it; track two open to speaker? Is there

00:50:13.410 --> 00:50:17.591
some kind of call to --
&gt;&gt;David Sparks: So, yeah, the question is,

00:50:17.591 --> 00:50:24.089
can you change routing for specific tracks.
A lot of the devices that we deal with don't

00:50:24.089 --> 00:50:29.359
actually allow for that.
We're seeing new devices that have sophisticated

00:50:29.359 --> 00:50:35.109
mixers down typically in the baseband or DSP
or something like that that have these fancy

00:50:35.109 --> 00:50:42.690
routing matrices.
And you -- you know, we should be able to

00:50:42.690 --> 00:50:47.609
expose that at some point.
Right now, most of the devices, like, if you

00:50:47.609 --> 00:50:52.440
look at the G1 and the myTouch, there's only
one output.

00:50:52.440 --> 00:50:56.980
We can route it to SCO, we can route it to
A2DP, we can route it to the speaker, but

00:50:56.980 --> 00:51:02.869
we can't do a separate route.
There is a concept in Froyo of what we're

00:51:02.869 --> 00:51:08.499
calling a direct output.
And we're using that for some things like

00:51:08.499 --> 00:51:15.420
voice dialer now works over SCO.
So we can take that one output and route it.

00:51:15.420 --> 00:51:19.490
But there is no mixer for that.
And it may not necessarily be supported in

00:51:19.490 --> 00:51:20.490
all devices.
&gt;&gt;&gt; Okay.

00:51:20.490 --> 00:51:26.170
&gt;&gt;David Sparks: So it's -- there's a lot of
issues we have to work with on the chipset

00:51:26.170 --> 00:51:28.619
and hardware manufacturers.
&gt;&gt;&gt; Last question.

00:51:28.619 --> 00:51:31.609
Any support for FM radio?
Because we saw some devices being added --

00:51:31.609 --> 00:51:33.971
&gt;&gt;David Sparks: FM radio?
&gt;&gt;&gt; Yeah.

00:51:33.971 --> 00:51:37.760
&gt;&gt;David Sparks: We're still working out the
details on that.

00:51:37.760 --> 00:51:42.529
We've had a couple of proposals, none that
I was particularly excited about.

00:51:42.529 --> 00:51:46.249
But I -- you know, I think we'll probably
come up with something.

00:51:46.249 --> 00:51:51.490
-- one of the things -- a lot of stuff we
do is device-driven, product-driven in the

00:51:51.490 --> 00:51:54.839
sense that we're working on something, we
kind of think about what we want on that,

00:51:54.839 --> 00:52:00.890
until we actually have one that has an FM
radio we're exposing as a feature -- it's

00:52:00.890 --> 00:52:04.431
not highly prioritized.
But hopefully in maybe the next year.

00:52:04.431 --> 00:52:10.999
&gt;&gt;&gt; Yes, messing around with the ring tone
picker, I noticed that there was a parameter

00:52:10.999 --> 00:52:15.480
there for DRM available.
I wonder is this open to developers to use,

00:52:15.480 --> 00:52:18.509
and how would we use this?
&gt;&gt;David Sparks: The ring tone picker, what

00:52:18.509 --> 00:52:21.991
was the --
&gt;&gt;&gt; I noticed that there was DRM support.

00:52:21.991 --> 00:52:26.789
I was wondering how we would access this.
&gt;&gt;David Sparks: Oh, yeah.

00:52:26.789 --> 00:52:33.749
My advice is, stay away from the DRM stuff.
We -- we had this forward lock stuff that

00:52:33.749 --> 00:52:40.630
we did for our first device.
It's not really well-integrated into the system.

00:52:40.630 --> 00:52:47.539
So I didn't get into all the road map stuff.
But we are working on a DRM integration for

00:52:47.539 --> 00:52:52.300
the next release.
And it should be easier to do what you want

00:52:52.300 --> 00:52:53.970
to do, I think, at that point.
&gt;&gt;&gt; Okay.

00:52:53.970 --> 00:52:56.960
&gt;&gt;David Sparks: But the stuff that's there
now is not well supported.

00:52:56.960 --> 00:52:58.060
&gt;&gt;&gt; All right.
Thanks.

00:52:58.060 --> 00:53:00.339
&gt;&gt;David Sparks: Unfortunately.
Sorry about that.

00:53:00.339 --> 00:53:06.640
&gt;&gt;&gt; There have been discussions on Google
Groups that you will be bringing in Stagefright

00:53:06.640 --> 00:53:12.589
and replacing that with packet radio OpenCORE.
So can you tell us, is that true?

00:53:12.589 --> 00:53:19.200
Or -- and what does the highlights of Stagefright?
&gt;&gt;David Sparks: The question is Stagefright

00:53:19.200 --> 00:53:24.200
versus OpenCORE, probably versus G streamer
versus who knows, whatever media framework

00:53:24.200 --> 00:53:30.400
might be out there.
We -- so we started Stagefright as, really,

00:53:30.400 --> 00:53:36.130
the vehicle for supporting Flash.
It was intended to be a lightweight framework

00:53:36.130 --> 00:53:43.259
that allowed us to do, essentially, what I
was talking about earlier, expose the stack

00:53:43.259 --> 00:53:47.579
so that you can send elementary streams down
to the codecs and the framework would handle

00:53:47.579 --> 00:53:54.980
all of -- AV sync and all that stuff.
So as we got further into that, we said, oh,

00:53:54.980 --> 00:53:58.710
if we just -- we'll add a stream extractor,
we'll at this and that.

00:53:58.710 --> 00:54:06.349
Next thing you know, we had a media framework.
We kind of accidentally fell into it.

00:54:06.349 --> 00:54:12.109
We've been talking to PacketVideo, and they're
going to be cooperating with us on Stagefright.

00:54:12.109 --> 00:54:16.369
So in the long run, we're going to move away
from OpenCORE.

00:54:16.369 --> 00:54:21.480
We hope to do that in the next release.
For Froyo, it's kind of a mixed bag.

00:54:21.480 --> 00:54:29.029
We have Stagefright for local file playback,
and for HTTP progressive.

00:54:29.029 --> 00:54:33.430
OpenCORE is still providing the authoring
side and RTSP stack.

00:54:33.430 --> 00:54:40.690
So the next piece is to move those over and
expose some of these APIs.

00:54:40.690 --> 00:54:46.500
For our hardware integrator, there's a little
bit of extra support, because you basically

00:54:46.500 --> 00:54:49.559
have to take your OpenMAX codecs and put them
into both frameworks.

00:54:49.559 --> 00:54:53.319
But we think that's not too painful.
And that will be one release.

00:54:53.319 --> 00:55:00.280
And then next release, it will all be in Stagefright.
That said, if somebody decided they wanted

00:55:00.280 --> 00:55:04.160
to go with OpenCORE or they want to go with
GStream or they want to go with whatever,

00:55:04.160 --> 00:55:11.829
we're supportive of that.
The requirement is that you pass the CTS tests.

00:55:11.829 --> 00:55:15.720
And CTS will include whatever new APIs we
invent on top of Stagefright.

00:55:15.720 --> 00:55:21.109
And I think that's going to kind of force
people to head to Stagefright, although I

00:55:21.109 --> 00:55:24.930
wouldn't preclude somebody taking GStream
or making it work.

00:55:24.930 --> 00:55:28.440
Did that answer your question?
&gt;&gt;&gt; So could you say the difference again

00:55:28.440 --> 00:55:32.500
between the use cases when OpenCORE is going
to be used in Froyo and when Stagefright is

00:55:32.500 --> 00:55:35.789
going to be called to Froyo?
&gt;&gt;David Sparks: So OpenCORE will be used for

00:55:35.789 --> 00:55:42.460
RTSP and authoring.
So the media recorder is Stagefright.

00:55:42.460 --> 00:55:46.960
If you have an RTSP URL, that's -- sorry,
that's OpenCORE.

00:55:46.960 --> 00:55:52.209
RTSP, also OpenCORE.
Everything else is Stagefright.

00:55:52.209 --> 00:55:57.380
&gt;&gt;&gt; So somebody who's integrating a new codec
will have to have a version integrated with

00:55:57.380 --> 00:56:01.309
OpenCORE as well as Stagefright?
&gt;&gt;David Sparks: Yeah, if you're using OpenMAX,

00:56:01.309 --> 00:56:07.529
it's really just a matter of dropping it into
the -- the tables for the two frameworks.

00:56:07.529 --> 00:56:11.369
&gt;&gt;&gt; And do you have any documentation available
as to -- about Stagefright?

00:56:11.369 --> 00:56:14.650
&gt;&gt;David Sparks: It's forthcoming.
When we put the source code out, we'll have

00:56:14.650 --> 00:56:17.470
that information as well.
I can take one more question.

00:56:17.470 --> 00:56:24.140
&gt;&gt;&gt; I was wondering if you could comment on
how -- how Android right now or in the future

00:56:24.140 --> 00:56:28.630
is going to handle with multiple audio streams
or channels, like, surround sound, what with

00:56:28.630 --> 00:56:32.200
Google TV and devices with many HTML and things
like that.

00:56:32.200 --> 00:56:38.200
&gt;&gt;David Sparks: So open OpenSL ES has a lot
of the answers to that.

00:56:38.200 --> 00:56:43.630
There's already provisions to that.
We're still thinking about how the surround

00:56:43.630 --> 00:56:48.099
stuff works.
Google TV I think is farther ahead.

00:56:48.099 --> 00:56:53.410
Even though it's -- they are both based on
Android, the actual trees are separate at

00:56:53.410 --> 00:56:55.289
this point.
We're going to merge them later.

00:56:55.289 --> 00:56:59.780
So they're probably better equipped to answer
that question right now.

00:56:59.780 --> 00:57:04.589
But, you know, we should -- we'll have -- I
would imagine by the end of next year we'll

00:57:04.589 --> 00:57:08.839
probably have some kind of solution for that.
&gt;&gt;&gt; All right.

00:57:08.839 --> 00:57:09.839
Thanks.
&gt;&gt;David Sparks: All right.

00:57:09.839 --> 00:57:09.842
Thanks, everybody.
[ Applause ]

