WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.097
[MUSIC PLAYING]

00:00:06.097 --> 00:00:08.680
MEGAN GREEN: I'm very pleased
to have Seth Stephens-Davidowitz

00:00:08.680 --> 00:00:09.420
here.

00:00:09.420 --> 00:00:12.420
He has used data from the
internet, particularly Google

00:00:12.420 --> 00:00:15.390
searches, to get new insights
into the human psyche.

00:00:15.390 --> 00:00:18.960
Today we'll be discussing
his book and research

00:00:18.960 --> 00:00:20.550
from "Everybody Lies."

00:00:20.550 --> 00:00:24.450
Seth has used Google
searches to measure racism,

00:00:24.450 --> 00:00:28.230
self-induced abortion,
depression, child abuse,

00:00:28.230 --> 00:00:31.890
hateful mobs, humor,
sexual preference, anxiety,

00:00:31.890 --> 00:00:35.300
and sexual insecurity,
among many other topics.

00:00:35.300 --> 00:00:38.500
Some a little less depressing
than some of those, right?

00:00:38.500 --> 00:00:41.430
Seth worked for one
and a half years here

00:00:41.430 --> 00:00:44.520
as a data scientist at Google,
so that's really exciting,

00:00:44.520 --> 00:00:47.460
and is currently a contributing
op-ed writer for "The New York

00:00:47.460 --> 00:00:48.570
Times."

00:00:48.570 --> 00:00:51.570
He's designing and teaching a
course at the Wharton School

00:00:51.570 --> 00:00:54.060
where he will be a
visiting lecturer.

00:00:54.060 --> 00:00:56.910
Seth received his BA in
philosophy from Stanford,

00:00:56.910 --> 00:00:59.950
and a PhD from
Harvard in economics.

00:00:59.950 --> 00:01:02.220
So please, let's give
Seth a warm welcome.

00:01:02.220 --> 00:01:06.040
[APPLAUSE]

00:01:06.040 --> 00:01:07.540
SETH STEPHENS-DAVIDOWITZ:
All right.

00:01:07.540 --> 00:01:10.580
Thanks everybody for attending.

00:01:10.580 --> 00:01:12.280
Thank you, Megan,
for the introduction.

00:01:12.280 --> 00:01:13.940
And it's great to be back here.

00:01:13.940 --> 00:01:16.990
I did work for one and a half
years in Mountain View Google.

00:01:16.990 --> 00:01:19.390
But I did come by
here a while ago,

00:01:19.390 --> 00:01:21.840
and I forgot how
spectacular it is here.

00:01:21.840 --> 00:01:24.080
So it's a nice reminder.

00:01:24.080 --> 00:01:26.650
But it's nice to be here.

00:01:26.650 --> 00:01:29.260
I'm talking about my
book "Everybody Lies,"

00:01:29.260 --> 00:01:31.750
which is how we can use
data from the internet

00:01:31.750 --> 00:01:34.400
to understand who we really are.

00:01:34.400 --> 00:01:36.100
So for the last 80
years, if you wanted

00:01:36.100 --> 00:01:40.900
to know what people want, why
people did the things they do,

00:01:40.900 --> 00:01:42.850
what people will
do in the future,

00:01:42.850 --> 00:01:44.590
you had one main approach--

00:01:44.590 --> 00:01:45.490
you ask them, right?

00:01:45.490 --> 00:01:47.020
You conducted a survey.

00:01:47.020 --> 00:01:50.170
Gallup or Pew or
Quinnipiac would go around

00:01:50.170 --> 00:01:52.480
and say, what do you want?

00:01:52.480 --> 00:01:54.040
What are you going to do?

00:01:54.040 --> 00:01:55.570
And the main
problem with this is

00:01:55.570 --> 00:01:59.290
that people have been shown to
lie to surveys, particularly

00:01:59.290 --> 00:02:01.000
on sensitive topics.

00:02:01.000 --> 00:02:02.620
They try to make
themselves look good.

00:02:02.620 --> 00:02:04.690
They tell surveys
what they think

00:02:04.690 --> 00:02:07.870
the surveyor wants to hear,
not necessarily the truth.

00:02:07.870 --> 00:02:10.270
So a classic example of
this is, if you ask people

00:02:10.270 --> 00:02:13.990
before an election, are you
going to vote in the election?

00:02:13.990 --> 00:02:16.270
A huge percentage, the
overwhelming majority

00:02:16.270 --> 00:02:18.080
of Americans say, sure.

00:02:18.080 --> 00:02:21.760
Of course I'm going to exercise
my civic duty and vote.

00:02:21.760 --> 00:02:24.921
And then when the election comes
around, about 55% of Americans

00:02:24.921 --> 00:02:25.420
vote.

00:02:25.420 --> 00:02:28.060
So people don't want to say
that they're not voting.

00:02:28.060 --> 00:02:31.480
One of my favorite examples is
the General Social Survey asks

00:02:31.480 --> 00:02:34.660
Americans how frequently
they have sex,

00:02:34.660 --> 00:02:36.910
and how frequently
they use a condom.

00:02:36.910 --> 00:02:39.530
So according to
women, they have sex

00:02:39.530 --> 00:02:43.540
on average about once a
week, and use a condom 20%

00:02:43.540 --> 00:02:44.960
of the time.

00:02:44.960 --> 00:02:48.540
So they say that they're using
1.1 billion condoms every year.

00:02:48.540 --> 00:02:50.960
And then they also must say
whether it's gay or straight.

00:02:50.960 --> 00:02:52.960
So they're using 1.1
billion condoms every year

00:02:52.960 --> 00:02:55.739
in heterosexual
sexual encounters.

00:02:55.739 --> 00:02:57.530
And then they asked
men the same questions.

00:02:57.530 --> 00:02:58.988
And according to
men, they're using

00:02:58.988 --> 00:03:01.090
1.6 billion condoms
every year in

00:03:01.090 --> 00:03:03.376
heterosexual sexual encounters.

00:03:03.376 --> 00:03:05.500
I hope everyone realizes
those, by definition, have

00:03:05.500 --> 00:03:06.416
to be the same, right?

00:03:06.416 --> 00:03:09.090
So we already know
that someone's lying,

00:03:09.090 --> 00:03:10.660
someone's not telling
the truth here

00:03:10.660 --> 00:03:12.850
about how much sex
they're having.

00:03:12.850 --> 00:03:14.620
And I got data from Nielsen.

00:03:14.620 --> 00:03:16.825
They track every condom
sold in the United States.

00:03:16.825 --> 00:03:18.700
Only 600 million condoms
are sold every year.

00:03:18.700 --> 00:03:20.710
[LAUGHTER]

00:03:20.710 --> 00:03:22.930
So basically now
everybody's lying about sex,

00:03:22.930 --> 00:03:25.355
it's just men are lying
even more than women.

00:03:25.355 --> 00:03:26.980
And this doesn't
mean-- this could just

00:03:26.980 --> 00:03:28.660
be they're lying
about using a condom,

00:03:28.660 --> 00:03:30.550
not necessarily how
much sex they have.

00:03:30.550 --> 00:03:32.710
But if you see how
much unprotected sex

00:03:32.710 --> 00:03:34.360
women of fertility
age say they're

00:03:34.360 --> 00:03:36.850
having, if they really
were having this much sex,

00:03:36.850 --> 00:03:39.070
there would basically be
more pregnancies every year

00:03:39.070 --> 00:03:39.944
in the United States.

00:03:39.944 --> 00:03:43.510
So I think in our sex-obsessed
culture, there is now pressure,

00:03:43.510 --> 00:03:45.980
both on men and women, to
say they're having more sex

00:03:45.980 --> 00:03:47.230
than they actually are having.

00:03:49.920 --> 00:03:51.620
All right, digital truth serum--

00:03:51.620 --> 00:03:52.160
Google.

00:03:52.160 --> 00:03:54.118
The thesis of my book
basically, and the thesis

00:03:54.118 --> 00:03:55.940
of my research for
the last five years

00:03:55.940 --> 00:03:59.510
is that people are much
more honest on Google

00:03:59.510 --> 00:04:01.970
than they are to basically
any other source, I think.

00:04:01.970 --> 00:04:05.840
People tend to feel comfortable
typing things into Google

00:04:05.840 --> 00:04:08.670
that they might not
tell anybody else.

00:04:08.670 --> 00:04:11.760
And this data of course is
all anonymous and aggregate,

00:04:11.760 --> 00:04:15.140
so nobody knows the searches
that any particular person

00:04:15.140 --> 00:04:15.890
makes.

00:04:15.890 --> 00:04:19.010
But by aggregating it all
and putting it all together,

00:04:19.010 --> 00:04:24.920
we can see different patterns in
human behavior and human wants.

00:04:24.920 --> 00:04:27.006
So like, example--
people are honest--

00:04:27.006 --> 00:04:29.130
tell things Google they
might not tell anyone else.

00:04:29.130 --> 00:04:30.890
There are more
searches on average--

00:04:30.890 --> 00:04:33.810
this is using Google Trends,
for porn than for weather.

00:04:33.810 --> 00:04:37.490
Though if you ask people if
they watch porn, only about 20%

00:04:37.490 --> 00:04:40.250
of men and 4% of women
say they watch porn.

00:04:40.250 --> 00:04:41.460
So that's hard to reconcile.

00:04:41.460 --> 00:04:43.550
But people are clearly
typing things into Google

00:04:43.550 --> 00:04:44.966
that they might
not be comfortable

00:04:44.966 --> 00:04:48.110
telling to a survey.

00:04:48.110 --> 00:04:51.267
So we can learn,
really, lots of--

00:04:51.267 --> 00:04:52.850
so why are people
so honest on Google?

00:04:52.850 --> 00:04:55.460
Well, one thing they're
alone, they're online.

00:04:55.460 --> 00:04:57.620
That tends to make
people more honest.

00:04:57.620 --> 00:05:00.890
But they also have an incentive
to tell the truth to Google.

00:05:00.890 --> 00:05:04.580
So there's no reason
for any person

00:05:04.580 --> 00:05:08.510
to tell a survey that they're
voting-- about their voting

00:05:08.510 --> 00:05:11.410
behavior, whether actually
voting or not voting.

00:05:11.410 --> 00:05:13.070
There's no reason to tell--

00:05:13.070 --> 00:05:14.610
to be honest about that.

00:05:14.610 --> 00:05:17.420
But if you're someone who
doesn't always vote, or is kind

00:05:17.420 --> 00:05:19.490
of a marginal voter,
you may not know

00:05:19.490 --> 00:05:21.865
where the polling places are,
so you have to tell Google.

00:05:21.865 --> 00:05:24.364
You have to say "where to vote,"
or "how to vote," or search

00:05:24.364 --> 00:05:25.340
something like that.

00:05:25.340 --> 00:05:28.904
And it is clear in the data
that this predicts turnout

00:05:28.904 --> 00:05:30.320
in different parts
of the country,

00:05:30.320 --> 00:05:32.300
that when people are
making a lot of searches

00:05:32.300 --> 00:05:34.100
for where to vote
and how to vote,

00:05:34.100 --> 00:05:36.747
people are much more
likely to turn out to vote.

00:05:36.747 --> 00:05:38.330
And if you're not
having a lot of sex,

00:05:38.330 --> 00:05:40.660
you don't have a reason
to tell that to a survey.

00:05:40.660 --> 00:05:42.250
There's no reason
for you to do that.

00:05:42.250 --> 00:05:45.159
But you might have an incentive
to search for this on Google.

00:05:45.159 --> 00:05:47.450
And the number one complaint
about a marriage on Google

00:05:47.450 --> 00:05:51.050
by far is that it's a
sexless marriage, much more

00:05:51.050 --> 00:05:54.170
common than loveless
or unhappy marriage.

00:05:54.170 --> 00:05:56.270
And we also start
seeing in this data

00:05:56.270 --> 00:05:58.400
some things that are
maybe counter-intuitive or

00:05:58.400 --> 00:05:59.900
surprising.

00:05:59.900 --> 00:06:01.730
The number one
complaints about partner

00:06:01.730 --> 00:06:05.460
on Google for husband, wife,
boyfriend or girlfriend

00:06:05.460 --> 00:06:08.866
is that the partner
won't have sex with me.

00:06:08.866 --> 00:06:10.490
Easily beats the
second place complaint

00:06:10.490 --> 00:06:11.990
that the partner
won't text me back.

00:06:11.990 --> 00:06:13.790
[LAUGHTER]

00:06:13.790 --> 00:06:18.534
But there are actually twice
as many complaints on Google

00:06:18.534 --> 00:06:21.200
that my boyfriend won't have sex
with me than that my girlfriend

00:06:21.200 --> 00:06:23.810
won't have sex with me,
which goes completely

00:06:23.810 --> 00:06:28.875
against conventional wisdom
about who's avoiding sex.

00:06:28.875 --> 00:06:31.500
So I think there are definitely
surprising things in this data.

00:06:34.340 --> 00:06:36.520
We can also use this
search data to answer

00:06:36.520 --> 00:06:39.850
big, big questions that have
kind of puzzled researchers

00:06:39.850 --> 00:06:41.110
for a while.

00:06:41.110 --> 00:06:46.180
One of them that I've done a
lot of research on is on racism.

00:06:46.180 --> 00:06:47.590
So this is kind
of a classic area

00:06:47.590 --> 00:06:49.000
where it may be
difficult to find

00:06:49.000 --> 00:06:51.260
the truth by using surveys.

00:06:51.260 --> 00:06:55.000
And for example, after
the 2008 election, one

00:06:55.000 --> 00:06:56.790
of the big questions
was, would voters--

00:06:56.790 --> 00:07:00.130
did voters care that Barack
Obama, the first major party

00:07:00.130 --> 00:07:02.890
general election candidate,
was African-American?

00:07:02.890 --> 00:07:05.080
Did they care that he was black?

00:07:05.080 --> 00:07:08.320
And if you ask in surveys,
the overwhelming majority

00:07:08.320 --> 00:07:11.530
of Americans, 98%,
99% of Americans

00:07:11.530 --> 00:07:13.960
said, no, I didn't care at
all that Obama was black.

00:07:13.960 --> 00:07:16.630
It was not a factor
in my voting decision.

00:07:16.630 --> 00:07:19.930
But of course, this
may be misleading

00:07:19.930 --> 00:07:22.840
because people may lie, and
not want to admit that they

00:07:22.840 --> 00:07:25.840
cared that Obama was black.

00:07:25.840 --> 00:07:26.740
So what I did--

00:07:26.740 --> 00:07:28.114
this is kind of
the first study I

00:07:28.114 --> 00:07:31.750
did with this research is,
I studied racist searches

00:07:31.750 --> 00:07:33.800
that people make on Google.

00:07:33.800 --> 00:07:36.920
And this is
obviously disturbing.

00:07:36.920 --> 00:07:40.879
This is a search for a
very, very nasty word about

00:07:40.879 --> 00:07:43.420
African-Americans that you can
kind of guess probably what it

00:07:43.420 --> 00:07:44.676
is, or look--

00:07:44.676 --> 00:07:46.300
read my book if you
want to learn more.

00:07:46.300 --> 00:07:49.870
But this is
basically just people

00:07:49.870 --> 00:07:53.710
searching for disparaging jokes
mocking African-Americans.

00:07:53.710 --> 00:07:55.482
So really, really
nasty searches.

00:07:55.482 --> 00:07:57.940
And the first thing that struck
me out about these searches

00:07:57.940 --> 00:07:59.680
was how frequent they were.

00:07:59.680 --> 00:08:02.350
In the time period
I was looking,

00:08:02.350 --> 00:08:05.140
this search was about as
common as searches for Lakers,

00:08:05.140 --> 00:08:07.720
and Economist and
migraine and Daily Show.

00:08:07.720 --> 00:08:11.630
So not, by any stretch of the
imagination, a fringe search.

00:08:11.630 --> 00:08:15.520
And the other thing that
was striking about this

00:08:15.520 --> 00:08:18.100
when I first saw this
data is that the map

00:08:18.100 --> 00:08:21.520
looks very different than the
map that I would have guessed.

00:08:21.520 --> 00:08:23.980
So if you were to ask me
before I did this research,

00:08:23.980 --> 00:08:26.860
where is racism highest
in the United States?

00:08:26.860 --> 00:08:28.390
I would have said south, right?

00:08:28.390 --> 00:08:28.960
Deep south.

00:08:28.960 --> 00:08:31.280
Like, that's-- when you think
of the country's history,

00:08:31.280 --> 00:08:32.780
you think Mississippi
and Louisiana,

00:08:32.780 --> 00:08:34.659
Alabama and South Carolina.

00:08:34.659 --> 00:08:38.559
And those areas definitely
are among the highest,

00:08:38.559 --> 00:08:42.039
but also among the
highest are West Virginia

00:08:42.039 --> 00:08:44.890
and western Pennsylvania,
eastern Ohio and upstate New

00:08:44.890 --> 00:08:48.340
York, and parts of industrial
Michigan and rural Illinois.

00:08:48.340 --> 00:08:52.120
The real divide this map
reveals in racism these days

00:08:52.120 --> 00:08:54.930
in the United States is
not south versus north,

00:08:54.930 --> 00:08:56.560
it's east versus west.

00:08:56.560 --> 00:08:57.880
So it's much higher.

00:08:57.880 --> 00:09:00.580
Racism is much higher east
of the Mississippi River

00:09:00.580 --> 00:09:03.370
than west of the
Mississippi River.

00:09:03.370 --> 00:09:06.250
So how can you use
this map to detect

00:09:06.250 --> 00:09:08.620
how much racism cost Obama?

00:09:08.620 --> 00:09:12.520
Basically, I compared Obama to
previous democratic candidates

00:09:12.520 --> 00:09:14.709
such as John Kerry, the
white candidate, who

00:09:14.709 --> 00:09:16.750
was similarly liberal in
the previous candidate--

00:09:16.750 --> 00:09:18.340
in the previous election.

00:09:18.340 --> 00:09:20.874
And you see a very,
very clear relationship

00:09:20.874 --> 00:09:22.290
that, in parts of
the country that

00:09:22.290 --> 00:09:23.926
are making the most
racist searches

00:09:23.926 --> 00:09:25.550
in western Pennsylvania
or eastern Ohio

00:09:25.550 --> 00:09:28.270
and western Michigan, you
see a clear relationship

00:09:28.270 --> 00:09:31.060
that Obama just does worse than
previous democratic candidates

00:09:31.060 --> 00:09:31.780
did.

00:09:31.780 --> 00:09:35.140
And you try to explain it by
any other variable you have,

00:09:35.140 --> 00:09:38.410
and nothing else can really
explain this relationship

00:09:38.410 --> 00:09:40.007
except racism.

00:09:40.007 --> 00:09:41.590
So I think it was
really, really clear

00:09:41.590 --> 00:09:43.720
in this data that despite
what people were saying,

00:09:43.720 --> 00:09:45.750
a significant fraction
of Americans--

00:09:45.750 --> 00:09:48.240
I say about four percentage
points overall he lost.

00:09:48.240 --> 00:09:51.640
And about 10% of white
Americans would not

00:09:51.640 --> 00:09:55.120
support a democratic candidate
just because he was black.

00:09:55.120 --> 00:09:58.390
I think that's what I
picked up in this data.

00:09:58.390 --> 00:10:00.140
And then I kind of--

00:10:00.140 --> 00:10:03.130
this kind of languished in
academic journals for a while.

00:10:03.130 --> 00:10:05.540
People weren't really
paying much attention to it.

00:10:05.540 --> 00:10:07.450
But then in this
recent election,

00:10:07.450 --> 00:10:10.840
Donald Trump started
saying some nasty things

00:10:10.840 --> 00:10:13.330
about black people, right,
and was still getting

00:10:13.330 --> 00:10:14.050
a lot of support.

00:10:14.050 --> 00:10:15.633
And this was kind
of puzzling to a lot

00:10:15.633 --> 00:10:17.470
of people who thought
that you're not really

00:10:17.470 --> 00:10:21.190
allowed to say those things in
the United States these days.

00:10:21.190 --> 00:10:25.650
So not me, but actually Nate
Cohn at "The "New York Times,"

00:10:25.650 --> 00:10:28.199
a stats guy there, he
asked for this data.

00:10:28.199 --> 00:10:29.990
He said, can I see your
racist search data?

00:10:29.990 --> 00:10:32.020
I have data on
how Trump is doing

00:10:32.020 --> 00:10:34.430
in the primary in all
different parts of the country,

00:10:34.430 --> 00:10:36.310
and I want to see
if it correlates

00:10:36.310 --> 00:10:38.170
with your racist search data.

00:10:38.170 --> 00:10:41.590
And he found that it was
the single highest variable

00:10:41.590 --> 00:10:42.440
that he could find.

00:10:45.700 --> 00:10:47.610
It was higher than
age and education

00:10:47.610 --> 00:10:51.310
and economic conditions
and trade and policy

00:10:51.310 --> 00:10:52.840
positions and gun ownership.

00:10:52.840 --> 00:10:55.300
Basically nothing could
explain support for Trump

00:10:55.300 --> 00:10:56.920
in the primary to
the same degree

00:10:56.920 --> 00:10:59.230
as this racist search did.

00:10:59.230 --> 00:11:03.220
So I think what happened is,
the same hidden racism that

00:11:03.220 --> 00:11:07.120
was hurting Obama but not being
picked up in the data also

00:11:07.120 --> 00:11:10.030
helped carry Trump to victory.

00:11:13.530 --> 00:11:16.420
So I think, yeah, this is
the digital truth serum.

00:11:16.420 --> 00:11:19.904
And Megan is right, my book is
[LAUGHS] kind of depressing,

00:11:19.904 --> 00:11:20.820
I think, a little bit.

00:11:20.820 --> 00:11:22.620
[LAUGHTER]

00:11:22.620 --> 00:11:25.770
Because yeah, like, if you
ask people what they're like,

00:11:25.770 --> 00:11:30.720
they're going to give you a
more positive view of themselves

00:11:30.720 --> 00:11:33.180
than they necessarily
really are.

00:11:33.180 --> 00:11:34.920
So yeah, I talk
about racism, and I

00:11:34.920 --> 00:11:37.726
talk about child abuse, and
do-it-yourself abortion.

00:11:37.726 --> 00:11:39.600
I think America has a
do-it-yourself abortion

00:11:39.600 --> 00:11:42.450
crisis that isn't being picked
up in the traditional data

00:11:42.450 --> 00:11:43.420
sources.

00:11:43.420 --> 00:11:47.760
So really dark, horrifying,
terrifying, disturbing

00:11:47.760 --> 00:11:48.870
material.

00:11:48.870 --> 00:11:51.780
But I put jokes in it,
so you won't really

00:11:51.780 --> 00:11:57.600
notice just how miserable
all the findings are.

00:11:57.600 --> 00:12:01.080
So I think there actually
is a lot of value

00:12:01.080 --> 00:12:05.980
to knowing some of this
stuff, to knowing the truth,

00:12:05.980 --> 00:12:08.910
even if it is sometimes
depressing, and sometimes

00:12:08.910 --> 00:12:10.557
disturbing.

00:12:10.557 --> 00:12:12.390
And I'll give you a
couple examples of that.

00:12:12.390 --> 00:12:16.140
So one of the studies
I did is, I just

00:12:16.140 --> 00:12:20.220
compared the Google
searches that people

00:12:20.220 --> 00:12:24.600
make about sons and daughters.

00:12:24.600 --> 00:12:27.390
And I would have thought or
hoped in the United States

00:12:27.390 --> 00:12:30.180
today that parents treated
their sons and daughters

00:12:30.180 --> 00:12:32.710
the same way.

00:12:32.710 --> 00:12:35.760
But if you look at everybody's
search data together,

00:12:35.760 --> 00:12:37.950
you see very, very
different patterns.

00:12:37.950 --> 00:12:42.240
Where, when American parents
start a search "is my son,"

00:12:42.240 --> 00:12:44.280
they're are about twice
as likely to complete it

00:12:44.280 --> 00:12:47.590
with "gifted" or "a genius"
than if they start a search

00:12:47.590 --> 00:12:48.770
"is my daughter."

00:12:48.770 --> 00:12:50.720
And they start a search
"is my daughter,"

00:12:50.720 --> 00:12:52.650
they're much more
likely to complete it

00:12:52.650 --> 00:12:54.465
with "is my daughter
overweight," or even,

00:12:54.465 --> 00:12:56.400
"is my daughter ugly?"

00:12:56.400 --> 00:13:00.560
So despite what I think
parents might think,

00:13:00.560 --> 00:13:04.050
there is clearly-- when you
put together everybody's data,

00:13:04.050 --> 00:13:06.630
parents on average
are much more excited

00:13:06.630 --> 00:13:09.120
about the intellectual
potential of their sons,

00:13:09.120 --> 00:13:12.540
and much more concerned
about the physical appearance

00:13:12.540 --> 00:13:13.830
of their daughters.

00:13:13.830 --> 00:13:15.750
And I think that's one finding--
so you talk about the racism

00:13:15.750 --> 00:13:17.458
thing, it's not clear
that the people who

00:13:17.458 --> 00:13:19.990
are making these
racist searches,

00:13:19.990 --> 00:13:21.990
if you just tell them,
they're going to be like,

00:13:21.990 --> 00:13:23.573
oh, OK, I didn't
realize I was racist.

00:13:23.573 --> 00:13:25.320
Sorry about that,
I'm going to stop

00:13:25.320 --> 00:13:29.534
making these terrible searches,
joke searches, and stuff.

00:13:29.534 --> 00:13:31.200
But I think with this
parenting finding,

00:13:31.200 --> 00:13:33.000
I think a lot of
parents don't even

00:13:33.000 --> 00:13:34.920
realize that they're doing that.

00:13:34.920 --> 00:13:36.557
It's maybe a
subconscious prejudice

00:13:36.557 --> 00:13:37.640
that they're not aware of.

00:13:37.640 --> 00:13:40.260
And that's one where
maybe just the information

00:13:40.260 --> 00:13:43.050
itself can actually
help to change behavior.

00:13:43.050 --> 00:13:45.031
Where if we tell
parents, oh, you know,

00:13:45.031 --> 00:13:47.280
you might not think so, but
look, when we put together

00:13:47.280 --> 00:13:51.330
everybody's data even
throughout the United States

00:13:51.330 --> 00:13:52.920
there are these prejudices.

00:13:52.920 --> 00:13:53.970
Think twice.

00:13:53.970 --> 00:13:56.220
Are you paying enough
attention to the report card

00:13:56.220 --> 00:13:58.290
that your daughter
is bringing home?

00:13:58.290 --> 00:14:01.622
Are you paying enough attention
to her intellectual interests?

00:14:01.622 --> 00:14:03.330
And I think a lot of
parents have told me

00:14:03.330 --> 00:14:06.624
that that has made them
think twice about some

00:14:06.624 --> 00:14:08.790
of the questions they ask,
and some of the ways they

00:14:08.790 --> 00:14:10.081
treat their sons and daughters.

00:14:10.081 --> 00:14:12.134
So there is a lot of
value in knowing things,

00:14:12.134 --> 00:14:13.800
knowing the truth,
not what people think

00:14:13.800 --> 00:14:15.330
or what people say.

00:14:15.330 --> 00:14:18.250
And another example
that I'm going to give

00:14:18.250 --> 00:14:21.570
is about Islamophobia.

00:14:21.570 --> 00:14:24.510
So if you can go back to
the San Bernardino attacks

00:14:24.510 --> 00:14:26.760
in December 2015, if
people remember that,

00:14:26.760 --> 00:14:31.560
it was two people with a Muslim
sounding name shot up basically

00:14:31.560 --> 00:14:33.300
one of the guys' coworkers.

00:14:33.300 --> 00:14:35.680
And it was kind of a big--

00:14:35.680 --> 00:14:38.710
many, many people died, and
it was a big news story.

00:14:38.710 --> 00:14:43.530
And right after this, there was
an explosion of Islamophobia.

00:14:43.530 --> 00:14:47.790
And you saw that really, really
clearly in the Google searches,

00:14:47.790 --> 00:14:49.920
where the number one
search with the word

00:14:49.920 --> 00:14:54.570
Muslims in it immediately after
the attack was "kill Muslims."

00:14:54.570 --> 00:14:56.400
And these are people
just kind of--

00:14:56.400 --> 00:14:57.692
they're maniacs to some degree.

00:14:57.692 --> 00:15:00.191
These are kind of just not the
most sane members of society.

00:15:00.191 --> 00:15:02.240
It's not even clear what
exactly they're saying,

00:15:02.240 --> 00:15:03.300
but they're saying--

00:15:03.300 --> 00:15:05.220
but they're very
angry, and kind of just

00:15:05.220 --> 00:15:06.960
want to do something bad.

00:15:06.960 --> 00:15:11.490
And they also make searches like
"I hate Muslims," or "Muslims

00:15:11.490 --> 00:15:13.410
must die," or
really, really nasty,

00:15:13.410 --> 00:15:15.900
nasty, horrible searches.

00:15:15.900 --> 00:15:17.730
And these searches
we've shown can

00:15:17.730 --> 00:15:21.480
predict hate crimes in the
United States against Muslims.

00:15:21.480 --> 00:15:23.310
So they're not-- even
though they're weird,

00:15:23.310 --> 00:15:27.610
they definitely contain
meaningful information.

00:15:27.610 --> 00:15:30.510
So a few days after the
San Bernardino attack,

00:15:30.510 --> 00:15:34.770
Barack Obama gave a
talk to the nation.

00:15:34.770 --> 00:15:37.050
And the theme of
this talk was both

00:15:37.050 --> 00:15:39.760
that we had to protect
ourselves against terrorism,

00:15:39.760 --> 00:15:42.090
but also we had to
fight this Islamophobia.

00:15:42.090 --> 00:15:44.850
We couldn't really
allow ourselves to give

00:15:44.850 --> 00:15:47.310
in to this hatred that some--

00:15:47.310 --> 00:15:50.310
a small but dangerous
percentage of people

00:15:50.310 --> 00:15:54.420
were letting
themselves give into.

00:15:54.420 --> 00:15:56.630
And this speech was
nationally televised,

00:15:56.630 --> 00:15:58.440
it got a lot of attention.

00:15:58.440 --> 00:16:01.380
And it was one of the
more beautiful speeches

00:16:01.380 --> 00:16:03.570
I'd heard Obama give.

00:16:03.570 --> 00:16:05.580
It was kind of classic
Obama, but even better

00:16:05.580 --> 00:16:07.500
than classic Obama
where he talked

00:16:07.500 --> 00:16:10.440
about how it's
our responsibility

00:16:10.440 --> 00:16:13.840
to not give into fear,
and to appeal to freedom,

00:16:13.840 --> 00:16:17.440
and how it's our
responsibility to not reject

00:16:17.440 --> 00:16:20.650
someone just because of
the religion they practice.

00:16:20.650 --> 00:16:25.544
And it got great reviews from
all the serious sources, right?

00:16:25.544 --> 00:16:27.460
The New York Times said
it was a great speech,

00:16:27.460 --> 00:16:29.320
and the LA Times said
it was a great speech,

00:16:29.320 --> 00:16:33.160
and the Boston Globe said
it was a great speech.

00:16:33.160 --> 00:16:35.500
So it was-- kind of all
the conventional wisdom

00:16:35.500 --> 00:16:38.140
was that Obama had
given this great speech

00:16:38.140 --> 00:16:40.750
about our
responsibility to treat

00:16:40.750 --> 00:16:44.380
our neighbors kindly, our
Muslim-American neighbors

00:16:44.380 --> 00:16:46.060
kindly.

00:16:46.060 --> 00:16:49.420
So Google breaks down minute
by minute their search data.

00:16:49.420 --> 00:16:52.930
And I wanted to see, did
this beautiful speech

00:16:52.930 --> 00:16:53.740
serve its purpose?

00:16:53.740 --> 00:16:58.240
Did it calm down
these Islamophobes?

00:16:58.240 --> 00:17:00.800
And I looked at the data.

00:17:00.800 --> 00:17:04.150
And I found that not only
did these crazy searches--

00:17:04.150 --> 00:17:06.819
"kill Muslims," "hate Muslims,"
"I hate Muslims," "die,

00:17:06.819 --> 00:17:10.510
Muslims," they didn't drop.

00:17:10.510 --> 00:17:14.410
They didn't even stay the
same, they went way up.

00:17:14.410 --> 00:17:15.220
They exploded.

00:17:15.220 --> 00:17:18.130
Basically every time
Obama was saying

00:17:18.130 --> 00:17:20.290
all these responsibility--

00:17:20.290 --> 00:17:22.720
the importance of
responsibility,

00:17:22.720 --> 00:17:26.140
this beautiful sermon just
seem to backfire completely

00:17:26.140 --> 00:17:28.119
on all the bio--

00:17:28.119 --> 00:17:31.000
on its main purpose.

00:17:31.000 --> 00:17:32.590
But there was one
line that Obama

00:17:32.590 --> 00:17:36.800
gave at the speech that seemed
to have a different response.

00:17:36.800 --> 00:17:39.570
So he said that we had to
remember that Muslim-Americans

00:17:39.570 --> 00:17:42.170
are our friends and neighbors.

00:17:42.170 --> 00:17:45.610
They're our athletes, and
they're our sports heroes,

00:17:45.610 --> 00:17:49.090
and they're the men and women
who will die for this country.

00:17:49.090 --> 00:17:53.410
And right after he said that
line, for the first time

00:17:53.410 --> 00:17:56.260
in the last five
years, the top word

00:17:56.260 --> 00:17:59.950
searched with Muslims was
not "Muslim terrorists,"

00:17:59.950 --> 00:18:03.550
or "Muslim refugees," it
was "Muslim athletes,"

00:18:03.550 --> 00:18:06.100
followed by "Muslim soldiers."

00:18:06.100 --> 00:18:08.230
And these stayed up for
about a week afterwards.

00:18:08.230 --> 00:18:09.730
And you saw throughout
the internet,

00:18:09.730 --> 00:18:12.400
people were talking about,
Shaquille O'Neal's a Muslim?

00:18:12.400 --> 00:18:15.700
I didn't know Shaquille
O'Neal was a Muslim.

00:18:15.700 --> 00:18:19.090
And I think that does--
you can kind of compare

00:18:19.090 --> 00:18:21.730
most of the speech versus
what that line was.

00:18:21.730 --> 00:18:23.240
So the first part--

00:18:23.240 --> 00:18:26.770
most of the speech was
basically a lecture

00:18:26.770 --> 00:18:30.160
telling people everything they'd
heard a million times before.

00:18:30.160 --> 00:18:31.120
Nothing new, right?

00:18:31.120 --> 00:18:36.220
Lecturing them to be better
people than they were.

00:18:36.220 --> 00:18:38.380
And that seemed to
totally backfire.

00:18:38.380 --> 00:18:41.740
But the line about the
athletes and sports heroes

00:18:41.740 --> 00:18:44.560
was provoking their curiosity,
giving them new information,

00:18:44.560 --> 00:18:48.070
changing what they might
think of as a Muslim-American.

00:18:48.070 --> 00:18:51.050
And that seemed to
be more successful.

00:18:51.050 --> 00:18:54.460
So we wrote this up in a
column in the New York Times.

00:18:54.460 --> 00:18:57.055
And I don't think
it's totally crazy

00:18:57.055 --> 00:18:58.930
that, when you write a
New York Times column,

00:18:58.930 --> 00:19:01.960
powerful people see that.

00:19:01.960 --> 00:19:06.040
Because a couple weeks later,
Obama gave an other speech.

00:19:06.040 --> 00:19:08.860
This time it was in
a Baltimore mosque.

00:19:08.860 --> 00:19:10.960
And again, it was
on national TV.

00:19:10.960 --> 00:19:13.660
And again, it got
a lot of attention.

00:19:13.660 --> 00:19:18.040
And the content of the
speech was totally different.

00:19:18.040 --> 00:19:21.100
Basically, he stopped with all
the sermon, all the lectures,

00:19:21.100 --> 00:19:23.650
all the talk of responsibility.

00:19:23.650 --> 00:19:26.260
And he just doubled down
or even quadrupled down

00:19:26.260 --> 00:19:27.760
on the curiosity.

00:19:27.760 --> 00:19:30.070
So he said that
Muslim-Americans are not just

00:19:30.070 --> 00:19:33.770
our sports heroes
and our soldiers,

00:19:33.770 --> 00:19:35.710
they're our farmers
and our merchants.

00:19:35.710 --> 00:19:38.140
And he talked about how
Thomas Jefferson had

00:19:38.140 --> 00:19:42.220
a copy of the Quran, and
how Muslim-Americans built

00:19:42.220 --> 00:19:44.360
the skyscrapers of Chicago.

00:19:44.360 --> 00:19:47.740
So it was all these new
images of Muslim-Americans

00:19:47.740 --> 00:19:50.590
that we didn't previously have.

00:19:50.590 --> 00:19:54.490
And I looked at the search
data again after this speech,

00:19:54.490 --> 00:19:57.850
in the hours following
this speech, and this time

00:19:57.850 --> 00:20:01.750
the searches for "kill Muslims"
and "I hate Muslims" dropped.

00:20:01.750 --> 00:20:03.790
So I think-- that's
only two speeches,

00:20:03.790 --> 00:20:05.980
and I don't want
to say that we've

00:20:05.980 --> 00:20:08.680
solved the problem of hatred.

00:20:08.680 --> 00:20:11.860
But I do think that this
is a radically new tool.

00:20:11.860 --> 00:20:14.330
And I think people don't
realize just how revolutionary

00:20:14.330 --> 00:20:16.780
this data from the internet
is, that we can actually

00:20:16.780 --> 00:20:21.550
peer into an angry mob, and
turn that into a science, right?

00:20:21.550 --> 00:20:23.560
In a survey of all
Americans, you're

00:20:23.560 --> 00:20:29.720
not going to get,
necessarily, these people.

00:20:29.720 --> 00:20:31.766
And even if you do,
they may not be honest.

00:20:31.766 --> 00:20:33.640
And they're not going
to agree to participate

00:20:33.640 --> 00:20:36.430
in a lab experiment at
Princeton or Harvard.

00:20:36.430 --> 00:20:38.380
But because Google
searches contains

00:20:38.380 --> 00:20:41.290
everybody's information,
they're going to be on there.

00:20:41.290 --> 00:20:47.260
And we can actually see how they
respond to big national events,

00:20:47.260 --> 00:20:49.960
and maybe learn a
lot of the things

00:20:49.960 --> 00:20:51.760
that we thought
worked don't work.

00:20:51.760 --> 00:20:54.200
But here are things
that actually work.

00:20:54.200 --> 00:20:57.280
So I think that kind of
shows that this window

00:20:57.280 --> 00:21:01.150
into some parts of the psyche
that are disturbing but are

00:21:01.150 --> 00:21:04.570
usually missed can really
serve a useful purpose,

00:21:04.570 --> 00:21:07.270
where knowing the truth
is maybe the first step

00:21:07.270 --> 00:21:10.090
towards improving
the world, I think.

00:21:10.090 --> 00:21:12.900
So that's all I have to say.

00:21:12.900 --> 00:21:15.800
Now I think we're going to
take questions from Megan

00:21:15.800 --> 00:21:17.600
and other people about the book.

00:21:17.600 --> 00:21:18.710
MEGAN GREEN: All right.

00:21:18.710 --> 00:21:19.340
Hi, Seth.

00:21:19.340 --> 00:21:20.420
That was great.

00:21:20.420 --> 00:21:21.740
The book's awesome.

00:21:21.740 --> 00:21:23.390
It's really cool if
you work at Google

00:21:23.390 --> 00:21:25.640
because you can see sort
of what's going on here

00:21:25.640 --> 00:21:29.960
in sort of a different variation
of what we do here every day.

00:21:29.960 --> 00:21:32.690
So I highly recommend it.

00:21:32.690 --> 00:21:35.300
And that was just a few of
the topics that you cover.

00:21:35.300 --> 00:21:37.080
You cover so many
topics in the book.

00:21:37.080 --> 00:21:39.590
But I thought I'd
start asking you

00:21:39.590 --> 00:21:41.780
a little bit about your
experience working here,

00:21:41.780 --> 00:21:45.020
and how that sort of led
you to become a writer.

00:21:45.020 --> 00:21:48.080
And if there's anything
you miss about being here.

00:21:48.080 --> 00:21:49.538
SETH STEPHENS-DAVIDOWITZ:
Yeah, no.

00:21:49.538 --> 00:21:52.040
I didn't miss it until I
just had, like, the food--

00:21:52.040 --> 00:21:52.577
[LAUGHTER]

00:21:52.577 --> 00:21:53.660
--right before I got here.

00:21:53.660 --> 00:21:55.370
I'm like, oh, man.

00:21:55.370 --> 00:21:57.630
What was I thinking?

00:21:57.630 --> 00:21:58.760
And the views of all--

00:21:58.760 --> 00:22:00.830
they have a new floor here
in the New York office where

00:22:00.830 --> 00:22:02.705
you can see the river,
and it's just like, oh

00:22:02.705 --> 00:22:06.470
my-- and all the furniture
is so comfortable.

00:22:06.470 --> 00:22:07.310
So yeah.

00:22:07.310 --> 00:22:09.060
Yeah, and the people
are all really smart.

00:22:09.060 --> 00:22:13.784
So I definitely-- you
don't appreciate what--

00:22:13.784 --> 00:22:14.450
what's the song?

00:22:14.450 --> 00:22:15.620
Joni Mitchell song,
you don't know

00:22:15.620 --> 00:22:16.680
what you got till it's gone?

00:22:16.680 --> 00:22:17.330
MEGAN GREEN: Yeah.

00:22:17.330 --> 00:22:18.240
SETH STEPHENS-DAVIDOWITZ:
So that kind

00:22:18.240 --> 00:22:19.615
of happened a
little with Google.

00:22:19.615 --> 00:22:22.350
Although I am enjoying the
writing thing as well, but--

00:22:22.350 --> 00:22:23.210
MEGAN GREEN: And
what was it like when

00:22:23.210 --> 00:22:24.080
you were working here?

00:22:24.080 --> 00:22:25.010
And what were you working on?

00:22:25.010 --> 00:22:26.900
Can you give a little
bit of color around--

00:22:26.900 --> 00:22:27.102
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:22:27.102 --> 00:22:29.210
So I was working
under Hal Varian,

00:22:29.210 --> 00:22:30.500
the chief economist at Google.

00:22:30.500 --> 00:22:31.999
He's the guy who
initially hired me.

00:22:31.999 --> 00:22:37.005
And then I was on his team, and
also in quantitative marketing

00:22:37.005 --> 00:22:37.880
out in Mountain View.

00:22:37.880 --> 00:22:42.620
So kind of a lot of
in-house data consulting,

00:22:42.620 --> 00:22:47.000
but also some advertising
effectiveness studies.

00:22:47.000 --> 00:22:48.470
And then I did,
also, some research

00:22:48.470 --> 00:22:51.059
because I think Google was kind
of getting interested in all

00:22:51.059 --> 00:22:52.100
this information we have.

00:22:52.100 --> 00:22:53.850
That's kind of one of
the original reasons

00:22:53.850 --> 00:22:57.180
that Hal hired me, that there
is this powerful data, and kind

00:22:57.180 --> 00:23:00.015
of how should we be
using this information?

00:23:00.015 --> 00:23:02.390
MEGAN GREEN: And that was sort
of part of the inspiration

00:23:02.390 --> 00:23:03.770
for this book, yes?

00:23:03.770 --> 00:23:05.720
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:23:05.720 --> 00:23:08.087
I think-- I mean,
it's just, the more

00:23:08.087 --> 00:23:09.920
I studied this data,
the more I'm like, wow,

00:23:09.920 --> 00:23:11.627
this stuff is really important.

00:23:11.627 --> 00:23:13.460
And there is a lot of
important information.

00:23:13.460 --> 00:23:15.390
So I kind of wanted to
get that message out.

00:23:15.390 --> 00:23:16.140
MEGAN GREEN: Cool.

00:23:16.140 --> 00:23:18.020
So in talking about
the information,

00:23:18.020 --> 00:23:20.330
sometimes you get an
answer and you go, wow,

00:23:20.330 --> 00:23:22.550
that totally makes sense.

00:23:22.550 --> 00:23:24.050
And sometimes you
get an answer that

00:23:24.050 --> 00:23:28.220
is sort of contrary to what
you would think the data

00:23:28.220 --> 00:23:29.220
would predict.

00:23:29.220 --> 00:23:32.390
And so there's a lot of
psychology involved in this.

00:23:32.390 --> 00:23:35.000
Can you talk a little
bit about how often

00:23:35.000 --> 00:23:37.580
that sort of happens,
that the answer is

00:23:37.580 --> 00:23:38.540
what you would expect?

00:23:38.540 --> 00:23:41.150
And how do you kind of
come to your conclusions

00:23:41.150 --> 00:23:44.270
when you get an answer that's
totally different than what

00:23:44.270 --> 00:23:46.159
you thought it would be?

00:23:46.159 --> 00:23:47.450
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:23:47.450 --> 00:23:53.450
I think frequently things are
just different than I expected.

00:23:53.450 --> 00:23:55.940
So I've done a lot of
research on anxiety.

00:23:55.940 --> 00:23:58.960
And I thought anxiety was
highest in New York City.

00:23:58.960 --> 00:24:01.200
Because I'm from
New Jersey, right?

00:24:01.200 --> 00:24:03.260
I'm from New Jersey, right
outside New York City.

00:24:03.260 --> 00:24:05.400
I'm Jewish, it was
always like, oh, you're

00:24:05.400 --> 00:24:06.789
a neurotic Woody Allen type.

00:24:06.789 --> 00:24:09.080
I always thought I was a
really anxious, neurotic type,

00:24:09.080 --> 00:24:12.020
and that that was a normal--

00:24:12.020 --> 00:24:14.940
and that we were all way more
anxious than everybody else.

00:24:14.940 --> 00:24:16.786
But then you see
in the search data

00:24:16.786 --> 00:24:18.410
that it's not true
at all, that anxiety

00:24:18.410 --> 00:24:21.680
is highest in Kentucky
and upstate Maine,

00:24:21.680 --> 00:24:23.900
and rural areas way
more than urban areas,

00:24:23.900 --> 00:24:25.940
and places with lower
levels of education

00:24:25.940 --> 00:24:28.320
more than higher
levels of education.

00:24:28.320 --> 00:24:31.440
So yeah, I think it just, over
and over again, the data--

00:24:31.440 --> 00:24:33.440
I think we're just basically
blind to the world.

00:24:33.440 --> 00:24:35.189
I think a lot of times
we think whatever's

00:24:35.189 --> 00:24:38.150
going on in our own head is
much more general than it is.

00:24:38.150 --> 00:24:41.820
Or we jump to conclusions
very, very fast.

00:24:41.820 --> 00:24:45.560
So that's why I think the
data is usually different

00:24:45.560 --> 00:24:46.400
than I expect.

00:24:46.400 --> 00:24:47.524
I mean, sometimes it's not.

00:24:47.524 --> 00:24:50.870
Like, if you search where
do people search for Lakers,

00:24:50.870 --> 00:24:52.130
it's Los Angeles.

00:24:52.130 --> 00:24:53.420
And you probably didn't need--

00:24:53.420 --> 00:24:55.400
OK, that makes sense.

00:24:55.400 --> 00:24:57.650
I'm not going to,
yeah, shock the world.

00:24:57.650 --> 00:25:00.110
Like, no, the Lakers are
more popular in Minneapolis

00:25:00.110 --> 00:25:01.220
than Los Angeles.

00:25:01.220 --> 00:25:02.192
That's not true, but--

00:25:02.192 --> 00:25:04.400
MEGAN GREEN: So has that
helped control your anxiety?

00:25:04.400 --> 00:25:06.400
Because you're like, wow,
those people in Maine,

00:25:06.400 --> 00:25:08.300
they've got it way
worse than I have here.

00:25:08.300 --> 00:25:08.660
SETH STEPHENS-DAVIDOWITZ:
Yeah, no.

00:25:08.660 --> 00:25:11.629
I think it-- it's just changed
how I thought about things.

00:25:11.629 --> 00:25:12.920
I'm like, oh, wow, that's not--

00:25:15.990 --> 00:25:18.310
it's even very particular
types of anxiety.

00:25:18.310 --> 00:25:19.910
Like anxiety about
death, people make

00:25:19.910 --> 00:25:21.370
searches "anxiety about death."

00:25:21.370 --> 00:25:23.450
And I'm like, oh,
that's the Woody Allen

00:25:23.450 --> 00:25:25.400
neurotic, intellectual thing.

00:25:25.400 --> 00:25:26.237
But it's not true.

00:25:26.237 --> 00:25:28.070
There are more of these
searches in Kentucky

00:25:28.070 --> 00:25:30.110
and Alabama and Louisiana.

00:25:30.110 --> 00:25:31.720
So it's just kind
of interesting.

00:25:31.720 --> 00:25:34.310
And I definitely do kind
of go through the world

00:25:34.310 --> 00:25:36.207
differently than I did before.

00:25:36.207 --> 00:25:37.790
MEGAN GREEN: That's
super interesting.

00:25:37.790 --> 00:25:39.260
So you ask a lot of questions?

00:25:39.260 --> 00:25:41.750
I mean, we just talked
about how many Americans

00:25:41.750 --> 00:25:44.300
are really racist, who
cheats on their taxes,

00:25:44.300 --> 00:25:45.500
does advertising work?

00:25:45.500 --> 00:25:47.240
But I'm sure there
are a lot of things

00:25:47.240 --> 00:25:48.770
that got left out in the book.

00:25:48.770 --> 00:25:50.570
So can you talk a
little bit about how

00:25:50.570 --> 00:25:52.361
you go about
choosing your topics,

00:25:52.361 --> 00:25:54.110
and if there was
anything interesting that

00:25:54.110 --> 00:25:55.430
maybe didn't make the cut?

00:25:55.430 --> 00:25:59.090
Because there's so many
interesting factoids.

00:25:59.090 --> 00:26:00.950
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:26:00.950 --> 00:26:04.569
I don't really-- there's not
a science to choosing a topic.

00:26:04.569 --> 00:26:06.110
You just kind of go
around the world,

00:26:06.110 --> 00:26:08.150
and talk to a lot of people,
and read a lot of things.

00:26:08.150 --> 00:26:09.691
And then one thing
sparks a question,

00:26:09.691 --> 00:26:11.450
or you play around
with data, and it goes

00:26:11.450 --> 00:26:13.560
in a totally other direction.

00:26:13.560 --> 00:26:17.540
So I don't-- so I think
that didn't make it.

00:26:17.540 --> 00:26:19.550
I have all this stuff
on anxiety that's

00:26:19.550 --> 00:26:22.121
just not ready for
prime time that got cut.

00:26:22.121 --> 00:26:22.870
I could have had--

00:26:22.870 --> 00:26:24.590
I wanted to have three
chapters on anxiety

00:26:24.590 --> 00:26:26.340
because I just find it
really, really interesting.

00:26:26.340 --> 00:26:27.380
MEGAN GREEN: So that's
a whole other book

00:26:27.380 --> 00:26:28.640
that we can look forward to?

00:26:28.640 --> 00:26:32.750
SETH STEPHENS-DAVIDOWITZ:
Yeah, that's my next book.

00:26:32.750 --> 00:26:36.429
But yeah, so I've done
this research on--

00:26:36.429 --> 00:26:38.720
if you break down the minute
by minute when people make

00:26:38.720 --> 00:26:41.230
searches for panic attacks.

00:26:41.230 --> 00:26:42.230
And it's not surprising.

00:26:42.230 --> 00:26:43.600
When do people make
searches for panic attack?

00:26:43.600 --> 00:26:45.140
Like 2:00 AM, 3:00 AM, right?

00:26:45.140 --> 00:26:48.610
They're in a cold sweat in
the middle of the night.

00:26:48.610 --> 00:26:51.860
But what this basically means
is that now because this data,

00:26:51.860 --> 00:26:55.809
we know on every given night,
we have a pretty good estimate

00:26:55.809 --> 00:26:58.100
of how many people are having
panic attacks in New York

00:26:58.100 --> 00:27:02.030
City and Boston and Los
Angeles and Indianapolis.

00:27:02.030 --> 00:27:04.809
And we can basically say,
OK, why are, on some nights,

00:27:04.809 --> 00:27:06.350
a lot of people
having panic attacks?

00:27:06.350 --> 00:27:07.808
Or is there something
that happened

00:27:07.808 --> 00:27:10.250
three days before, or two days
before, or the day before?

00:27:10.250 --> 00:27:14.270
So there's really just so
much information here that--

00:27:14.270 --> 00:27:15.720
I think it's revolutionary.

00:27:15.720 --> 00:27:18.651
But some people criticize me for
being too grandiose about it,

00:27:18.651 --> 00:27:19.150
but--

00:27:19.150 --> 00:27:20.733
MEGAN GREEN: No, I
think that's great.

00:27:20.733 --> 00:27:22.580
You got to sell it, right?

00:27:22.580 --> 00:27:25.310
Yeah, I think that data
is totally fascinating,

00:27:25.310 --> 00:27:29.120
I agree with you, and that
it applies to so many things.

00:27:29.120 --> 00:27:31.240
So back to the
election a little bit,

00:27:31.240 --> 00:27:33.320
you talked a little
bit about that.

00:27:33.320 --> 00:27:35.120
And you kind of
outsmarted Nate Silver.

00:27:35.120 --> 00:27:38.580
Would you say that that might
be correct in some ways?

00:27:38.580 --> 00:27:40.663
SETH STEPHENS-DAVIDOWITZ:
No, I wouldn't say that.

00:27:40.663 --> 00:27:41.234
[LAUGHTER]

00:27:41.234 --> 00:27:42.150
MEGAN GREEN: Are you--

00:27:42.150 --> 00:27:43.010
SETH STEPHENS-DAVIDOWITZ:
Especially since I'm trying

00:27:43.010 --> 00:27:45.260
to get him to agree to let
me write a column for his--

00:27:45.260 --> 00:27:46.040
[LAUGHTER]

00:27:46.040 --> 00:27:47.120
--for his site.

00:27:47.120 --> 00:27:49.610
MEGAN GREEN: Are you
looking at data now?

00:27:49.610 --> 00:27:52.160
And what are you seeing in
terms of trends in politics

00:27:52.160 --> 00:27:53.630
now that Trump is in office?

00:27:53.630 --> 00:27:56.390
What's the follow-up
to what we heard

00:27:56.390 --> 00:27:58.765
you say about this sort of
wave of populism and so forth?

00:27:58.765 --> 00:28:00.848
SETH STEPHENS-DAVIDOWITZ:
Yeah, so the thing about

00:28:00.848 --> 00:28:01.600
the election--

00:28:01.600 --> 00:28:03.900
so there's a question just
starting with politics--

00:28:03.900 --> 00:28:07.010
can you predict elections
with Google searches?

00:28:07.010 --> 00:28:10.100
Because if you looked at
the surveys this year,

00:28:10.100 --> 00:28:12.290
they didn't really
work so well, right?

00:28:12.290 --> 00:28:15.250
The surveys told us that Hillary
Clinton was going to win,

00:28:15.250 --> 00:28:16.230
then Donald Trump won.

00:28:16.230 --> 00:28:19.040
So is there a way to use all
this information on the Google

00:28:19.040 --> 00:28:22.220
where people are so honest
to predict elections?

00:28:22.220 --> 00:28:26.000
And it's not so simple.

00:28:26.000 --> 00:28:30.020
The top way most people have
tried to do it initially

00:28:30.020 --> 00:28:34.721
was, you just see, are people
searching for a candidate more?

00:28:34.721 --> 00:28:36.470
So if people are
searching for Trump more,

00:28:36.470 --> 00:28:37.700
they're going to go Trump.

00:28:37.700 --> 00:28:39.140
And if people are
searching for Clinton more,

00:28:39.140 --> 00:28:40.580
they're going to go Clinton.

00:28:40.580 --> 00:28:42.819
And you can probably
think yourself

00:28:42.819 --> 00:28:44.360
why that wouldn't
really work, right?

00:28:44.360 --> 00:28:46.825
Because you're not saying
whether you like the candidate

00:28:46.825 --> 00:28:47.908
or you hate the candidate.

00:28:47.908 --> 00:28:50.030
You could search for
Trump because you like him

00:28:50.030 --> 00:28:51.380
or because you hate him.

00:28:51.380 --> 00:28:55.070
So one of the indicators I
found with Stuart Gabriel, who's

00:28:55.070 --> 00:28:58.220
a professor at UCLA, we found
there is an indicator that

00:28:58.220 --> 00:29:01.010
has surprising predictive
power, and it's

00:29:01.010 --> 00:29:03.620
the order in which
people search candidates,

00:29:03.620 --> 00:29:04.790
which is pretty interesting.

00:29:04.790 --> 00:29:07.850
It's basically if people--

00:29:07.850 --> 00:29:10.700
25% of the searches people
make with Clinton also

00:29:10.700 --> 00:29:12.050
include the word "Trump."

00:29:12.050 --> 00:29:13.970
So people search for
"Clinton Trump polls,"

00:29:13.970 --> 00:29:17.410
or "Clinton Trump debate,"
or "Clinton Trump election."

00:29:17.410 --> 00:29:21.425
But if people search
Clinton before Trump,

00:29:21.425 --> 00:29:23.050
they're much more
likely to go Clinton.

00:29:23.050 --> 00:29:24.890
And if they search
Trump before Clinton,

00:29:24.890 --> 00:29:26.240
they're much more
likely to go Trump.

00:29:26.240 --> 00:29:27.530
So it's like something
subconscious.

00:29:27.530 --> 00:29:28.988
If you're a Trump
supporter, you're

00:29:28.988 --> 00:29:31.363
much more likely to think of
as a Trump/Clinton election.

00:29:31.363 --> 00:29:32.904
If you're a Clinton
supporter, you're

00:29:32.904 --> 00:29:34.490
much more likely to
think the reverse.

00:29:34.490 --> 00:29:37.760
And then you could see
that in general, Trump

00:29:37.760 --> 00:29:39.260
came before Clinton more.

00:29:39.260 --> 00:29:44.810
And then this was more true in
key midwest battleground states

00:29:44.810 --> 00:29:45.954
where he got victorious.

00:29:45.954 --> 00:29:47.870
But I think it's going
to take many, many more

00:29:47.870 --> 00:29:50.570
years of analyzing this
data before we know exactly

00:29:50.570 --> 00:29:51.806
how to weight it and stuff.

00:29:51.806 --> 00:29:54.180
But there definitely is a lot
of information in this data

00:29:54.180 --> 00:29:56.106
that would be missed
by other sources.

00:29:56.106 --> 00:29:58.730
MEGAN GREEN: That will give you
something to do for a long time

00:29:58.730 --> 00:29:59.280
to come.

00:29:59.280 --> 00:29:59.480
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:29:59.480 --> 00:30:01.980
There's no shortage of
things to do with this data.

00:30:01.980 --> 00:30:02.730
MEGAN GREEN: Cool.

00:30:02.730 --> 00:30:04.540
I'm going to invite
everyone here

00:30:04.540 --> 00:30:07.190
to start lining up at the
mic if you have questions.

00:30:07.190 --> 00:30:10.240
So please come on up.

00:30:10.240 --> 00:30:13.350
And in the meantime, I'll
ask you one more question.

00:30:13.350 --> 00:30:15.760
So some of the data
gets personal, right?

00:30:15.760 --> 00:30:18.155
You talk about being
a Mets fan, or--

00:30:18.155 --> 00:30:20.530
SETH STEPHENS-DAVIDOWITZ:
That's not embarrassing, is it?

00:30:20.530 --> 00:30:21.790
MEGAN GREEN: No.

00:30:21.790 --> 00:30:23.570
Just a little.

00:30:23.570 --> 00:30:27.650
Or you talk about
what women should do

00:30:27.650 --> 00:30:30.560
on a date to get a second date.

00:30:30.560 --> 00:30:34.110
So how much of this came from
sort of your own personal life?

00:30:34.110 --> 00:30:40.190
And is that a big way that
you sort of select topics?

00:30:40.190 --> 00:30:42.850
And does that make you
feel more justified

00:30:42.850 --> 00:30:45.980
in your everyday experiences?

00:30:45.980 --> 00:30:47.230
SETH STEPHENS-DAVIDOWITZ: No.

00:30:47.230 --> 00:30:50.030
A few of the topics were
my personal interest.

00:30:50.030 --> 00:30:52.592
A lot of the sports stuff
was my personal curiosities.

00:30:52.592 --> 00:30:54.800
MEGAN GREEN: You wanted to
be a baseball player, yes?

00:30:54.800 --> 00:30:55.250
SETH STEPHENS-DAVIDOWITZ:
Basketball player.

00:30:55.250 --> 00:30:56.090
MEGAN GREEN: Basketball player.

00:30:56.090 --> 00:30:56.390
SETH STEPHENS-DAVIDOWITZ:
But I would

00:30:56.390 --> 00:30:57.640
settled for a baseball player.

00:30:57.640 --> 00:30:59.380
[LAUGHTER]

00:30:59.380 --> 00:31:02.410
Yeah, but I think-- but
like the racism stuff,

00:31:02.410 --> 00:31:04.000
I don't think I'm
particularly racist.

00:31:04.000 --> 00:31:06.760
Or I did stuff on gay-- there
are a lot of closeted gay men.

00:31:06.760 --> 00:31:09.654
I'm not gay, but I thought that
was really interesting, too.

00:31:09.654 --> 00:31:10.320
So I don't know.

00:31:10.320 --> 00:31:14.809
It's not all-- it's probably
20% personal-based, but--

00:31:14.809 --> 00:31:15.850
MEGAN GREEN: Fair enough.

00:31:15.850 --> 00:31:19.566
OK, I'm going to turn it
over to you guys at the mic.

00:31:19.566 --> 00:31:21.470
And I'm going to
turn on the mic.

00:31:21.470 --> 00:31:24.326
Is it not on?

00:31:24.326 --> 00:31:25.714
It should be on.

00:31:25.714 --> 00:31:26.380
AUDIENCE: Hello?

00:31:26.380 --> 00:31:27.800
OK.

00:31:27.800 --> 00:31:29.550
So I was struck by
something you said,

00:31:29.550 --> 00:31:32.780
which was that people are
partially more honest online

00:31:32.780 --> 00:31:34.614
and with Google searches
because they have

00:31:34.614 --> 00:31:36.530
an incentive-- they're
trying to get something

00:31:36.530 --> 00:31:38.180
from their search.

00:31:38.180 --> 00:31:40.610
But then some of the
searches that you

00:31:40.610 --> 00:31:43.940
cited with the racism research
and some of the others,

00:31:43.940 --> 00:31:46.910
like "I hate
Muslims," and things

00:31:46.910 --> 00:31:49.550
that didn't seem to be a
question, which you would think

00:31:49.550 --> 00:31:51.090
is the incentive with a search.

00:31:51.090 --> 00:31:53.480
To what do you attribute
that type of honesty?

00:31:53.480 --> 00:31:55.880
Because it sounds
like those people

00:31:55.880 --> 00:31:58.550
are not, at the very least,
seeking additional information

00:31:58.550 --> 00:32:02.600
on a topic, which is our
usual definition of a search.

00:32:02.600 --> 00:32:05.430
SETH STEPHENS-DAVIDOWITZ:
Yeah, that's a great question.

00:32:05.430 --> 00:32:07.830
So yes, so there are two
reasons that Google is honest.

00:32:07.830 --> 00:32:09.220
The one is the incentive thing.

00:32:09.220 --> 00:32:11.095
So if you're talking
about a sexless marriage

00:32:11.095 --> 00:32:14.760
or racist jokes or
information on voting,

00:32:14.760 --> 00:32:17.644
where there's sensitive topics
but you need the information.

00:32:17.644 --> 00:32:19.310
And there's this other
class of searches

00:32:19.310 --> 00:32:21.052
that totally shocked me.

00:32:21.052 --> 00:32:23.260
It was one of the most
surprising things in the data.

00:32:23.260 --> 00:32:25.430
But it happens in big numbers.

00:32:25.430 --> 00:32:27.200
People just confess
things to Google.

00:32:27.200 --> 00:32:28.320
[LAUGHTER]

00:32:28.320 --> 00:32:32.240
So they say, "I hate my
boss," or like, "I'm sad,"

00:32:32.240 --> 00:32:33.140
or, "I'm drunk."

00:32:33.140 --> 00:32:35.296
And it's just like,
OK, like why are you--

00:32:35.296 --> 00:32:37.200
[LAUGHTER]

00:32:37.200 --> 00:32:39.500
--why are you telling Google?

00:32:39.500 --> 00:32:45.050
And I think a lot
of it is, it's kind

00:32:45.050 --> 00:32:47.390
of similar to the
confessional, right?

00:32:47.390 --> 00:32:49.700
The Catholic confe--
like, just something--

00:32:49.700 --> 00:32:53.031
I think people treat it
as-- like a confessional,

00:32:53.031 --> 00:32:54.530
there's no purpose
to saying things.

00:32:54.530 --> 00:32:56.613
But there's something about
saying things that you

00:32:56.613 --> 00:32:57.860
wouldn't tell anybody else.

00:32:57.860 --> 00:33:00.980
And people seem to
use it in that way.

00:33:00.980 --> 00:33:02.990
And yeah, it's
really surprising.

00:33:02.990 --> 00:33:06.940
My favorite example is that--

00:33:06.940 --> 00:33:11.345
I talk about how men are
insecure about their bodies.

00:33:11.345 --> 00:33:13.990
So we usually think that women--

00:33:13.990 --> 00:33:16.580
that body insecurity is
predominately a female thing.

00:33:16.580 --> 00:33:19.170
And it is majority female, but
if you look around the web,

00:33:19.170 --> 00:33:19.670
it's close.

00:33:19.670 --> 00:33:22.880
It's 55/45 women versus men.

00:33:22.880 --> 00:33:25.692
And men are really insecure
about their bodies as well.

00:33:25.692 --> 00:33:27.650
And a lot of this
insecurity, not surprisingly,

00:33:27.650 --> 00:33:31.040
focuses on one particular
body part, and the size of it

00:33:31.040 --> 00:33:32.780
in particular.

00:33:32.780 --> 00:33:36.440
Men ask more questions
about their Whatever than--

00:33:36.440 --> 00:33:37.590
[LAUGHTER]

00:33:37.590 --> 00:33:39.270
--than any other body part.

00:33:39.270 --> 00:33:40.790
But then one of
their top questions

00:33:40.790 --> 00:33:43.805
they ask about this body part
is, "how big is my penis?"

00:33:46.492 --> 00:33:47.950
Which is the
strangest search ever.

00:33:47.950 --> 00:33:48.760
AUDIENCE: I don't understand.

00:33:48.760 --> 00:33:49.335
[LAUGHTER]

00:33:49.335 --> 00:33:50.960
SETH STEPHENS-DAVIDOWITZ:
Yeah, so it's

00:33:50.960 --> 00:33:51.830
the strangest search ever.

00:33:51.830 --> 00:33:53.750
So people make, like, the
weirdest searches on Google.

00:33:53.750 --> 00:33:54.785
It's very bizarre.

00:33:54.785 --> 00:33:57.100
AUDIENCE: Follow-up
question, if I may--

00:33:57.100 --> 00:34:00.800
have you done any sort of--
or is there data correlation

00:34:00.800 --> 00:34:03.860
analysis between,
for example, people

00:34:03.860 --> 00:34:08.340
confessing on various forums and
Reddit and sort of places where

00:34:08.340 --> 00:34:12.889
you would confess, I think, with
an expectation of other people

00:34:12.889 --> 00:34:17.960
saying, oh, me,
too, and searches.

00:34:17.960 --> 00:34:20.475
Do they spike at
the same time maybe,

00:34:20.475 --> 00:34:22.684
or are they localized in
the same way geographically?

00:34:22.684 --> 00:34:24.516
SETH STEPHENS-DAVIDOWITZ:
I haven't seen it.

00:34:24.516 --> 00:34:25.441
It's a good question.

00:34:25.441 --> 00:34:27.440
Some of the Google searches,
I think that is it,

00:34:27.440 --> 00:34:28.909
that you kind of--

00:34:28.909 --> 00:34:32.510
if you type "I'm sad," you might
get message boards where people

00:34:32.510 --> 00:34:33.973
are saying, oh, I'm sad, too.

00:34:33.973 --> 00:34:35.389
So it might be
that you're looking

00:34:35.389 --> 00:34:38.449
for people who are feeling
the same way as you are.

00:34:38.449 --> 00:34:42.580
And just by saying it in that
way, you get that information.

00:34:42.580 --> 00:34:44.567
AUDIENCE: Thank you.

00:34:44.567 --> 00:34:46.150
AUDIENCE: Hi, thanks
for coming today.

00:34:46.150 --> 00:34:48.533
So when I do a search, often
I'll have one thing in mind.

00:34:48.533 --> 00:34:49.949
But as I'm typing
the search, I'll

00:34:49.949 --> 00:34:51.559
see the autocomplete suggestion.

00:34:51.559 --> 00:34:53.850
And even though it's not at
all what I meant to search,

00:34:53.850 --> 00:34:57.300
I might, out of curiosity,
complete that, and just

00:34:57.300 --> 00:34:59.460
have that search anyway.

00:34:59.460 --> 00:35:01.104
I'm wondering how
you account for that.

00:35:01.104 --> 00:35:03.270
Or how do you know that's
trivial if a lot of people

00:35:03.270 --> 00:35:04.200
do that.

00:35:04.200 --> 00:35:05.790
Because you don't want to
count those, like double count,

00:35:05.790 --> 00:35:05.990
right?

00:35:05.990 --> 00:35:06.920
SETH STEPHENS-DAVIDOWITZ:
Well, it's

00:35:06.920 --> 00:35:08.545
not clear you don't
want to count them.

00:35:08.545 --> 00:35:10.230
I think it makes--

00:35:10.230 --> 00:35:13.260
I don't know what percent of
searches-- somebody at Google

00:35:13.260 --> 00:35:16.110
probably knows what percent
of searches use autocomplete.

00:35:16.110 --> 00:35:17.550
I don't know that number.

00:35:17.550 --> 00:35:19.690
I don't think it's
publicly available.

00:35:19.690 --> 00:35:23.580
But I think it makes
small differences.

00:35:23.580 --> 00:35:27.507
In could magnify initial
differences, right?

00:35:27.507 --> 00:35:29.340
If people initially
have to search something

00:35:29.340 --> 00:35:30.900
to get there, but
then the winner

00:35:30.900 --> 00:35:34.080
will get potentially
more and more popular.

00:35:34.080 --> 00:35:37.486
So I don't think it totally
changes the level of things,

00:35:37.486 --> 00:35:38.610
like the ranking of things.

00:35:38.610 --> 00:35:40.950
But it can make a bigger
difference between the top

00:35:40.950 --> 00:35:41.910
and the other ones.

00:35:41.910 --> 00:35:42.451
AUDIENCE: OK.

00:35:42.451 --> 00:35:44.220
So overall, you're
saying that when

00:35:44.220 --> 00:35:45.900
you do look at the
top search results,

00:35:45.900 --> 00:35:48.600
you do keep in mind to
scale down a little bit just

00:35:48.600 --> 00:35:49.680
to account for that?

00:35:49.680 --> 00:35:50.160
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:35:50.160 --> 00:35:51.420
And I think the
regional ones are

00:35:51.420 --> 00:35:53.211
pretty-- the regional
differences are still

00:35:53.211 --> 00:35:57.540
pretty meaningful since,
from my understanding,

00:35:57.540 --> 00:36:00.235
they don't, on average, get
very different autocompletes.

00:36:00.235 --> 00:36:01.110
AUDIENCE: Yeah, cool.

00:36:01.110 --> 00:36:01.609
Thank you.

00:36:04.700 --> 00:36:06.380
AUDIENCE: On your
early chart that you

00:36:06.380 --> 00:36:09.200
had the comparison with
racism and correlation

00:36:09.200 --> 00:36:12.860
to Donald Trump, my
question would be,

00:36:12.860 --> 00:36:15.050
what was that
compared to Hillary?

00:36:15.050 --> 00:36:16.980
Because was that
a flip, or was it

00:36:16.980 --> 00:36:19.936
very similar in the two charts?

00:36:19.936 --> 00:36:22.310
SETH STEPHENS-DAVIDOWITZ: So
that was the primary voting.

00:36:22.310 --> 00:36:24.410
So it wasn't-- so you're saying,
what happened the general

00:36:24.410 --> 00:36:24.920
election?

00:36:24.920 --> 00:36:25.250
AUDIENCE: Yes.

00:36:25.250 --> 00:36:25.670
SETH STEPHENS-DAVIDOWITZ:
Yeah, I

00:36:25.670 --> 00:36:27.961
think the general election
is a little more complicated

00:36:27.961 --> 00:36:33.950
because it's like, Democrats and
Republicans differ in general.

00:36:33.950 --> 00:36:38.340
So it's not-- most of the
reason that one area goes

00:36:38.340 --> 00:36:40.340
Democrat and one area
goes Republican it's just,

00:36:40.340 --> 00:36:43.610
it's more Democrat and
Republican area in general.

00:36:43.610 --> 00:36:45.969
So you really want to compare
it to previous elections.

00:36:45.969 --> 00:36:47.510
And that's also a
little complicated,

00:36:47.510 --> 00:36:50.630
because then Obama had this
racism problem, and then Trump.

00:36:50.630 --> 00:36:52.970
I think it's
basically that Trump--

00:36:55.755 --> 00:36:57.620
a Republican
candidate who didn't

00:36:57.620 --> 00:37:04.700
appeal to racism in the
way that I think Trump did

00:37:04.700 --> 00:37:09.980
would have lost a lot of votes
in places relative to Obama

00:37:09.980 --> 00:37:12.200
in parts of the country
with lot of racist searches.

00:37:12.200 --> 00:37:14.140
But Trump didn't
lose those votes.

00:37:14.140 --> 00:37:15.770
So places like West Virginia
and western Pennsylvania

00:37:15.770 --> 00:37:17.270
and eastern Ohio,
if it was a norm--

00:37:17.270 --> 00:37:20.270
if it was a different Republican
candidate, those areas,

00:37:20.270 --> 00:37:22.850
they didn't like Obama.

00:37:22.850 --> 00:37:26.300
But they would've maybe
come back to a Democrat.

00:37:26.300 --> 00:37:30.320
But because Trump appeals
to those same feelings that

00:37:30.320 --> 00:37:33.430
made them mad about Obama,
they went Trump's way again.

00:37:33.430 --> 00:37:36.407
AUDIENCE: OK, because of the
variation between the primary

00:37:36.407 --> 00:37:37.990
and the general
election then probably

00:37:37.990 --> 00:37:43.990
has a great deal of effect that
wouldn't have been in that--

00:37:43.990 --> 00:37:46.066
the correlation between
those two charts

00:37:46.066 --> 00:37:47.690
might have been quite
different if they

00:37:47.690 --> 00:37:49.340
were done at the same time.

00:37:49.340 --> 00:37:49.600
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:37:49.600 --> 00:37:51.183
Well like I said,
the general election

00:37:51.183 --> 00:37:54.300
is a little more complicated
because most of it is--

00:37:54.300 --> 00:37:57.320
the map of any particular
general election

00:37:57.320 --> 00:38:00.089
is very similar year to year.

00:38:00.089 --> 00:38:01.880
So really, you just
want to see the changes

00:38:01.880 --> 00:38:04.630
in behavior, the changes in
votes in a general election.

00:38:04.630 --> 00:38:05.180
AUDIENCE: OK, very good.

00:38:05.180 --> 00:38:05.480
Thank you.

00:38:05.480 --> 00:38:07.105
SETH STEPHENS-DAVIDOWITZ:
Yeah, thanks.

00:38:08.728 --> 00:38:10.326
AUDIENCE: Hey, how's it going?

00:38:10.326 --> 00:38:10.950
Quick question.

00:38:10.950 --> 00:38:13.590
You mentioned earlier how
people lie on surveys,

00:38:13.590 --> 00:38:15.750
and we have the product
Google Consumer Surveys.

00:38:15.750 --> 00:38:18.510
So I was wondering if there's
any type of techniques

00:38:18.510 --> 00:38:21.300
or things you've seen from
data collection via surveys

00:38:21.300 --> 00:38:24.384
like OK Cupid, answer publicly
versus answer privately,

00:38:24.384 --> 00:38:25.800
have you seen
anything that either

00:38:25.800 --> 00:38:28.710
yielded more consistent
or reliable results

00:38:28.710 --> 00:38:30.054
from a survey-based format?

00:38:30.054 --> 00:38:31.470
SETH STEPHENS-DAVIDOWITZ:
So there

00:38:31.470 --> 00:38:35.430
are-- so online surveys tend to
be better than phone surveys,

00:38:35.430 --> 00:38:38.400
for example, because I
think talking to someone

00:38:38.400 --> 00:38:42.420
makes people that
much more dishonest.

00:38:42.420 --> 00:38:48.990
I think there are all
these games that scholars

00:38:48.990 --> 00:38:52.255
have invented to try to trick
people into telling the truth.

00:38:52.255 --> 00:38:53.630
And you can kind
of look them up.

00:38:53.630 --> 00:39:02.340
It's random digit examples,
or list experiments.

00:39:02.340 --> 00:39:04.590
They basically ask
people 10 questions.

00:39:04.590 --> 00:39:06.660
One of them is
embarrassing and the other

00:39:06.660 --> 00:39:09.805
aren't, and ask how many
are true, yes or no.

00:39:09.805 --> 00:39:11.430
And then can kind of
back out, and then

00:39:11.430 --> 00:39:13.350
ask another group that,
except the embarrassing one

00:39:13.350 --> 00:39:14.850
kind of can back
out the difference,

00:39:14.850 --> 00:39:16.330
if that makes sense.

00:39:16.330 --> 00:39:20.190
But my understanding is that
these don't really work,

00:39:20.190 --> 00:39:23.400
although a lot of papers have
been written about it that they

00:39:23.400 --> 00:39:24.150
don't really work.

00:39:24.150 --> 00:39:25.608
I think Google
Consumer Surveys has

00:39:25.608 --> 00:39:28.950
another pretty huge problem,
which is that people just

00:39:28.950 --> 00:39:30.660
answer randomly.

00:39:30.660 --> 00:39:34.650
Because at least if you're
answering a phone survey,

00:39:34.650 --> 00:39:38.850
you've gone through the time
to not hang up right away,

00:39:38.850 --> 00:39:40.336
you might give a--

00:39:40.336 --> 00:39:41.460
take it somewhat seriously.

00:39:41.460 --> 00:39:43.230
But I think Google
Consumer Surveys,

00:39:43.230 --> 00:39:47.507
a big percentage of the people
don't give a serious answer.

00:39:47.507 --> 00:39:49.590
AUDIENCE: Or for the app--
the screener questions,

00:39:49.590 --> 00:39:51.173
they always answer
whatever they think

00:39:51.173 --> 00:39:52.892
that we want to screen in for.

00:39:52.892 --> 00:39:53.392
Yeah.

00:39:53.392 --> 00:39:55.975
And then also, by the way, the
[INAUDIBLE] amplification study

00:39:55.975 --> 00:39:57.767
you worked on, the
clients really liked it.

00:39:57.767 --> 00:39:59.433
SETH STEPHENS-DAVIDOWITZ:
Oh, thank you.

00:39:59.433 --> 00:40:01.566
Yeah, that was another
thing that I worked on.

00:40:01.566 --> 00:40:02.220
No, OK.

00:40:02.220 --> 00:40:03.809
[LAUGHS]

00:40:03.809 --> 00:40:04.350
AUDIENCE: Hi.

00:40:04.350 --> 00:40:06.550
Are you concerned at all
about the Hawthorne Effect?

00:40:06.550 --> 00:40:09.600
By talking about what we can
learn from the Google search

00:40:09.600 --> 00:40:11.250
results, maybe
people won't be as

00:40:11.250 --> 00:40:14.639
willing to put their dirty
laundry out in Google?

00:40:14.639 --> 00:40:15.930
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:40:15.930 --> 00:40:17.760
I don't know if it's
what we can learn.

00:40:17.760 --> 00:40:19.846
There definitely are
changes in behavior.

00:40:19.846 --> 00:40:22.470
So this is all based on the idea
that people will tell anything

00:40:22.470 --> 00:40:23.880
to Google.

00:40:23.880 --> 00:40:27.570
But someone did a paper where
they compared Google searches

00:40:27.570 --> 00:40:31.530
before and after Snowden's leak.

00:40:31.530 --> 00:40:35.010
And they found that there
was a big drop in searches

00:40:35.010 --> 00:40:39.030
that were either sensitive--
so like on sensitive topics

00:40:39.030 --> 00:40:39.989
or embarrassing topics.

00:40:39.989 --> 00:40:41.738
Actually, my favorite
thing from the paper

00:40:41.738 --> 00:40:43.170
is that they had
a list of emba--

00:40:43.170 --> 00:40:46.850
they had to figure out
what's embarrassing search.

00:40:46.850 --> 00:40:49.380
So they asked people
at Mechanical Turk

00:40:49.380 --> 00:40:51.480
to rank the embarrassment
of searches.

00:40:51.480 --> 00:40:53.690
And one of them that got
classified as embarrassing

00:40:53.690 --> 00:40:54.570
was Nickelback.

00:40:54.570 --> 00:40:57.881
[LAUGHTER]

00:40:57.881 --> 00:40:59.880
And then they found that
those types of searches

00:40:59.880 --> 00:41:01.500
did drop, including Nickelback--

00:41:01.500 --> 00:41:04.080
[LAUGHTER]

00:41:04.080 --> 00:41:06.590
--after Snowden's revelation.

00:41:06.590 --> 00:41:08.080
So yeah, I don't think--

00:41:08.080 --> 00:41:12.840
yeah, it may be that this is
a brief period of time where

00:41:12.840 --> 00:41:16.530
we can really see
into the human psyche,

00:41:16.530 --> 00:41:18.750
and then it'll all
die down or something.

00:41:18.750 --> 00:41:21.420
I hope not, but I think--

00:41:21.420 --> 00:41:22.980
I always emphasize that--

00:41:22.980 --> 00:41:26.070
everyone's like, has
your search behavior

00:41:26.070 --> 00:41:28.140
changed since you've
done this research?

00:41:28.140 --> 00:41:33.660
And it hasn't at all because I'm
like, nobody knows my search.

00:41:33.660 --> 00:41:35.680
I don't see why it
should affect me

00:41:35.680 --> 00:41:39.227
if they know that
someone in Brooklyn

00:41:39.227 --> 00:41:40.560
is making a search or something.

00:41:40.560 --> 00:41:42.859
It doesn't seem like--

00:41:42.859 --> 00:41:44.400
AUDIENCE: Although
if I was a racist,

00:41:44.400 --> 00:41:46.770
I might not want to give my
state a bad name by doing

00:41:46.770 --> 00:41:48.320
lots of racist searches.

00:41:48.320 --> 00:41:50.570
I don't know, if I felt bad
about being racist, maybe.

00:41:50.570 --> 00:41:50.900
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:41:50.900 --> 00:41:51.940
So it could be like--

00:41:51.940 --> 00:41:54.790
yeah, that's kind of a subtle--

00:41:54.790 --> 00:41:59.730
I mean, that definitely would
be a problem in polls as well.

00:41:59.730 --> 00:42:01.857
But I don't know if that's--

00:42:01.857 --> 00:42:03.023
AUDIENCE: All right, thanks.

00:42:06.970 --> 00:42:08.080
AUDIENCE: Hi.

00:42:08.080 --> 00:42:11.010
Have you thought about comparing
the Google search results

00:42:11.010 --> 00:42:18.460
with their social
network results or posts?

00:42:18.460 --> 00:42:21.220
Are the social
network results tend

00:42:21.220 --> 00:42:25.210
to be more, or as honest,
as Google search terms,

00:42:25.210 --> 00:42:27.560
or even more honest
because people

00:42:27.560 --> 00:42:29.710
use that as a way to
express themselves

00:42:29.710 --> 00:42:37.890
and not necessarily exposing
their true identity?

00:42:37.890 --> 00:42:42.350
Or people still-- or Google
search results tend to be more

00:42:42.350 --> 00:42:43.350
honest than the social--

00:42:43.350 --> 00:42:45.433
SETH STEPHENS-DAVIDOWITZ:
No, I think no question,

00:42:45.433 --> 00:42:47.920
Google search results are
more honest than social media.

00:42:47.920 --> 00:42:50.860
So I think that
social media data,

00:42:50.860 --> 00:42:53.666
you can't really trust because--

00:42:53.666 --> 00:42:55.540
it's even, in some sense,
worse than surveys.

00:42:55.540 --> 00:42:57.490
Because you have an
incentive to make

00:42:57.490 --> 00:43:01.550
yourself look good, to
impress your friends.

00:43:01.550 --> 00:43:03.500
So if you compare, for example--

00:43:03.500 --> 00:43:05.980
one example I talk
about is the popularity

00:43:05.980 --> 00:43:09.250
of the "National Enquirer"
versus the "Atlantic Monthly."

00:43:09.250 --> 00:43:11.650
That the "National
Enquirer" actually sells--

00:43:11.650 --> 00:43:13.690
it's kind of a lowbrow,
trashy magazine.

00:43:13.690 --> 00:43:17.170
That actually sells more copies
than the "Atlantic Monthly"

00:43:17.170 --> 00:43:18.040
every year.

00:43:18.040 --> 00:43:20.320
But on social media,
the "Atlantic Monthly"

00:43:20.320 --> 00:43:22.029
is 45 times more
popular because everyone

00:43:22.029 --> 00:43:24.236
wants their friends they
think they're really, really

00:43:24.236 --> 00:43:25.492
intellectual, right?

00:43:25.492 --> 00:43:26.950
And then it actually
is interesting

00:43:26.950 --> 00:43:32.470
if you compare the social
media posts and Google search

00:43:32.470 --> 00:43:35.140
posts, Google searches.

00:43:35.140 --> 00:43:38.890
So if you look at the top ways
people describe their husband

00:43:38.890 --> 00:43:41.140
on social media, the
top five descriptors,

00:43:41.140 --> 00:43:45.340
"my husband is," it's "my
husband is amazing," "so cute,"

00:43:45.340 --> 00:43:49.040
"awesome," "the best," and "my
best friend," which is probably

00:43:49.040 --> 00:43:52.580
a misleading view of marriage,
to some degree at least.

00:43:52.580 --> 00:43:55.080
And then if you do
Google, the top five

00:43:55.080 --> 00:43:56.870
completes of "my
husband is" on Google--

00:43:56.870 --> 00:43:58.620
which also kind of a
weird search to make,

00:43:58.620 --> 00:44:01.480
but people do make "my
husband is," one of them

00:44:01.480 --> 00:44:02.770
is also awesome.

00:44:02.770 --> 00:44:06.910
So that checks out, but the
other ones are "gay," "a jerk,"

00:44:06.910 --> 00:44:08.820
"mean," and "annoying."

00:44:08.820 --> 00:44:11.230
So it's really-- yeah,
it's kind of interesting.

00:44:11.230 --> 00:44:12.310
I think you get--

00:44:12.310 --> 00:44:15.977
I'm not sure if one of them
is right or wrong on marriage,

00:44:15.977 --> 00:44:18.310
but it's definitely very
different because on one you're

00:44:18.310 --> 00:44:20.931
trying to impress your
friends, and on one you're not.

00:44:20.931 --> 00:44:22.180
AUDIENCE: Thank you very much.

00:44:22.180 --> 00:44:24.854
Very interesting.

00:44:24.854 --> 00:44:26.020
AUDIENCE: Yeah, I'm curious.

00:44:26.020 --> 00:44:28.180
Looking at that map, is
there a way to tell--

00:44:28.180 --> 00:44:32.720
or do you tell the difference
between two regions where

00:44:32.720 --> 00:44:33.310
there are--

00:44:33.310 --> 00:44:35.950
like one where there's a
lot of people doing those,

00:44:35.950 --> 00:44:39.080
for example, racist searches,
or another area where

00:44:39.080 --> 00:44:42.060
it's like fewer people doing
a lot of searches per person.

00:44:42.060 --> 00:44:44.920
Do you account for multiple
searches per person?

00:44:44.920 --> 00:44:47.295
SETH STEPHENS-DAVIDOWITZ:
Yeah, no, it's a good question.

00:44:47.295 --> 00:44:49.490
No, that data is just
not made available.

00:44:49.490 --> 00:44:51.220
I don't know if anyone
at Google has it,

00:44:51.220 --> 00:44:53.800
but it's not made available.

00:44:53.800 --> 00:44:56.067
Except what Google does
in Google Trends is,

00:44:56.067 --> 00:44:58.150
they take out-- if someone
makes a lot of searches

00:44:58.150 --> 00:45:01.120
in a short period of time,
it just counts as one search.

00:45:01.120 --> 00:45:04.624
So it's not like someone
just in a half hour

00:45:04.624 --> 00:45:06.290
searched this thing
over and over again,

00:45:06.290 --> 00:45:07.581
and that's driving the results.

00:45:07.581 --> 00:45:10.237
I think it is an interesting
comparison, I think.

00:45:10.237 --> 00:45:12.070
In some ways, it's an
advantage to this data

00:45:12.070 --> 00:45:13.340
relative to surveys.

00:45:13.340 --> 00:45:16.030
Because I think
surveys, not always,

00:45:16.030 --> 00:45:18.940
but we usually think of
as yes or no, as binary.

00:45:18.940 --> 00:45:21.490
Either you are racist
or you aren't racist.

00:45:21.490 --> 00:45:24.220
But there clearly are
degrees of it, right?

00:45:24.220 --> 00:45:26.784
You're more or less likely.

00:45:26.784 --> 00:45:28.200
So sometimes it's
an advantage, it

00:45:28.200 --> 00:45:30.075
includes people who've
done it a lot of times

00:45:30.075 --> 00:45:32.590
because they're probably
even more racist.

00:45:32.590 --> 00:45:34.248
So yeah.

00:45:34.248 --> 00:45:36.120
AUDIENCE: Thanks.

00:45:36.120 --> 00:45:37.290
MEGAN GREEN: Cool, OK.

00:45:37.290 --> 00:45:38.280
I just have a few more.

00:45:38.280 --> 00:45:41.250
Thank you, everybody.

00:45:41.250 --> 00:45:43.200
Those were great questions.

00:45:43.200 --> 00:45:45.330
So I know you
spent a lot of time

00:45:45.330 --> 00:45:48.120
talking about naming your
book, and went through a bunch

00:45:48.120 --> 00:45:49.320
of sort of different names.

00:45:49.320 --> 00:45:51.111
Can you talk a little
bit about the process

00:45:51.111 --> 00:45:54.614
of naming a book, and how
you came to Everybody Lies?

00:45:54.614 --> 00:45:56.280
SETH STEPHENS-DAVIDOWITZ:
Well, so yeah.

00:45:56.280 --> 00:45:59.300
So that's why I
initially brought up

00:45:59.300 --> 00:46:02.370
that ridiculous question that
men asked, because that's

00:46:02.370 --> 00:46:04.380
what I wanted to call my book.

00:46:04.380 --> 00:46:06.614
[LAUGHTER]

00:46:06.614 --> 00:46:08.280
I wanted to call it
How Big Is My Penis?

00:46:08.280 --> 00:46:08.670
What Google Searches--

00:46:08.670 --> 00:46:10.380
MEGAN GREEN: That's where
I was going with that.

00:46:10.380 --> 00:46:11.820
SETH STEPHENS-DAVIDOWITZ: --What
Google Searches Reveal About

00:46:11.820 --> 00:46:12.930
Human Nature.

00:46:12.930 --> 00:46:14.970
But then my publisher's
like, people

00:46:14.970 --> 00:46:17.552
will be embarrassed to
buy that in an airport.

00:46:17.552 --> 00:46:21.520
So I couldn't title it that.

00:46:21.520 --> 00:46:25.521
But I think-- although
I still kind of think

00:46:25.521 --> 00:46:26.520
that was a better title.

00:46:26.520 --> 00:46:27.650
[LAUGHTER]

00:46:27.650 --> 00:46:28.710
Although I do like this.

00:46:28.710 --> 00:46:29.918
This title I didn't think of.

00:46:29.918 --> 00:46:31.990
That was-- They came
up with this title.

00:46:31.990 --> 00:46:33.689
And I think it's good.

00:46:33.689 --> 00:46:34.230
I don't know.

00:46:34.230 --> 00:46:34.770
MEGAN GREEN: I like it.

00:46:34.770 --> 00:46:35.770
SETH STEPHENS-DAVIDOWITZ:
Yeah, I like it.

00:46:35.770 --> 00:46:37.229
I think it gets a
lot of the point.

00:46:37.229 --> 00:46:39.145
MEGAN GREEN: What about
some of the other data

00:46:39.145 --> 00:46:40.260
sources you look at?

00:46:40.260 --> 00:46:43.320
I mean, obviously you look at
Facebook, you look at Pornhub,

00:46:43.320 --> 00:46:47.730
you look at a variety
of different sources.

00:46:47.730 --> 00:46:49.830
How did those rank
compared to Google?

00:46:49.830 --> 00:46:53.010
And are you planning on
looking more at other sources,

00:46:53.010 --> 00:46:56.825
or is Google for you sort
of the ultimate place to be?

00:46:56.825 --> 00:46:58.950
SETH STEPHENS-DAVIDOWITZ:
I think Google's just way

00:46:58.950 --> 00:47:01.857
better than all the other
ones because the honesty.

00:47:01.857 --> 00:47:02.940
And then it just kind of--

00:47:02.940 --> 00:47:04.347
it's so universal.

00:47:04.347 --> 00:47:05.930
A lot of these data
sources, Twitter--

00:47:05.930 --> 00:47:06.974
like, who uses Twitter?

00:47:06.974 --> 00:47:08.140
It's a very selected sample.

00:47:08.140 --> 00:47:10.500
But pretty much
everybody uses Google.

00:47:10.500 --> 00:47:13.850
And then just any topic,
there is information there--

00:47:13.850 --> 00:47:20.330
like music or race or
sexuality or anything.

00:47:20.330 --> 00:47:22.010
There's probably at
least some insights.

00:47:22.010 --> 00:47:24.010
MEGAN GREEN: Whereas
Pornhub is pretty specific.

00:47:24.010 --> 00:47:25.170
SETH STEPHENS-DAVIDOWITZ:
Yeah, yeah, exactly.

00:47:25.170 --> 00:47:27.400
And like, they're
kind of more one-offs.

00:47:27.400 --> 00:47:31.350
You can find something
interesting in the data set,

00:47:31.350 --> 00:47:33.942
but it's not as
comprehensive with Googling

00:47:33.942 --> 00:47:36.150
and find something interesting
on any topic, I think.

00:47:36.150 --> 00:47:37.500
So it's more of a--

00:47:37.500 --> 00:47:42.690
I think it's orders of magnitude
better than the other data

00:47:42.690 --> 00:47:43.410
sources.

00:47:43.410 --> 00:47:44.190
MEGAN GREEN: We
like to hear that.

00:47:44.190 --> 00:47:45.400
That keeps us in business.

00:47:45.400 --> 00:47:51.480
So you mentioned before that
you kind of have a grandiose--

00:47:51.480 --> 00:47:57.150
I actually think it's a really
sort of enlightening sense

00:47:57.150 --> 00:48:00.450
of where data science is going
for both philosophy, medicine,

00:48:00.450 --> 00:48:02.970
sort of how it can be
used to really help

00:48:02.970 --> 00:48:04.720
people in the future.

00:48:04.720 --> 00:48:07.015
And so I was sort of wondering
if you had any advice.

00:48:07.015 --> 00:48:08.640
I know we have a lot
of data scientists

00:48:08.640 --> 00:48:11.580
out there, both at
Google and in the world.

00:48:11.580 --> 00:48:14.490
What advice would you give
them in terms of data science,

00:48:14.490 --> 00:48:17.230
what they should be doing,
and where it's going?

00:48:17.230 --> 00:48:19.035
SETH STEPHENS-DAVIDOWITZ:
I think it's--

00:48:19.035 --> 00:48:23.250
I just think it's a really,
really exciting area just

00:48:23.250 --> 00:48:26.700
because of all this new
data that's out there.

00:48:26.700 --> 00:48:30.610
I think the insights that are
coming are going to be huge.

00:48:30.610 --> 00:48:33.372
I would say that some--

00:48:33.372 --> 00:48:35.580
obviously if people work at
Google, part of their job

00:48:35.580 --> 00:48:38.490
is probably going to be a
lot of-- a decent percentage

00:48:38.490 --> 00:48:41.580
of Google employees have to get
people to click on ads, which

00:48:41.580 --> 00:48:46.080
is not necessarily the most
interesting, in my opinion,

00:48:46.080 --> 00:48:48.540
use of data, but is important
for Google's business.

00:48:48.540 --> 00:48:52.250
But I definitely
think the health stuff

00:48:52.250 --> 00:48:55.820
is really valuable.

00:48:55.820 --> 00:48:57.120
There is a study--

00:48:57.120 --> 00:48:58.170
how much time do we--

00:48:58.170 --> 00:48:59.820
MEGAN GREEN: You're
fine, go ahead.

00:48:59.820 --> 00:49:01.236
SETH STEPHENS-DAVIDOWITZ:
So there

00:49:01.236 --> 00:49:03.990
was this study that was done
by Microsoft researchers

00:49:03.990 --> 00:49:06.760
in collaboration with a
professor at Columbia.

00:49:06.760 --> 00:49:09.980
And they studied
pancreatic cancer.

00:49:09.980 --> 00:49:11.039
I talk about in the book.

00:49:11.039 --> 00:49:12.330
They studied pancreatic cancer.

00:49:12.330 --> 00:49:15.060
And they basically
could figure out that--

00:49:15.060 --> 00:49:19.050
they studied anonymized,
de-identified individuals,

00:49:19.050 --> 00:49:20.730
their search behavior over time.

00:49:20.730 --> 00:49:22.770
And they said that--
and they could

00:49:22.770 --> 00:49:24.312
guess based on
someone's searches

00:49:24.312 --> 00:49:26.520
that they maybe got a
diagnosis of pancreatic cancer.

00:49:26.520 --> 00:49:27.960
Because people typed
something like, "just

00:49:27.960 --> 00:49:29.960
diagnosed with pancreatic
cancer, what do I do?"

00:49:29.960 --> 00:49:32.190
Or very, very clear
searches that they

00:49:32.190 --> 00:49:33.590
had pancreatic cancer.

00:49:33.590 --> 00:49:37.470
Then what they did is,
they compared these people

00:49:37.470 --> 00:49:39.720
to another group of
users who were similar

00:49:39.720 --> 00:49:41.940
and never had such a diagnosis.

00:49:41.940 --> 00:49:45.300
Then they looked at the searches
in the months leading up

00:49:45.300 --> 00:49:46.890
to that diagnosis,
so the symptoms

00:49:46.890 --> 00:49:48.660
that people were searching.

00:49:48.660 --> 00:49:52.710
And they said, what symptoms
do people search that tells us

00:49:52.710 --> 00:49:54.510
in two or three
months they're going

00:49:54.510 --> 00:49:56.700
to have a diagnosis
of pancreatic cancer?

00:49:56.700 --> 00:49:57.627
And the key to--

00:49:57.627 --> 00:49:59.460
the reason this study's
potentially powerful

00:49:59.460 --> 00:50:02.710
is, if the earlier you get a
pancreatic cancer diagnosis,

00:50:02.710 --> 00:50:04.980
the higher your
chances of survival.

00:50:08.040 --> 00:50:11.910
And using this data, they
found really subtle patterns

00:50:11.910 --> 00:50:14.880
to the point that, if you
search for indigestion

00:50:14.880 --> 00:50:17.280
before abdominal
pain, that's a risk

00:50:17.280 --> 00:50:19.470
factor for pancreatic cancer.

00:50:19.470 --> 00:50:21.990
Whereas if you search
just indigestion alone,

00:50:21.990 --> 00:50:24.210
that's not a risk factor
for pancreatic cancer.

00:50:24.210 --> 00:50:26.043
So they had so much
data, they could pick up

00:50:26.043 --> 00:50:27.270
these really subtle patterns.

00:50:27.270 --> 00:50:28.080
I'm kind of scared.

00:50:28.080 --> 00:50:30.090
Whenever I hear this
story, it's like--

00:50:30.090 --> 00:50:32.250
right after I read
that paper, I thought

00:50:32.250 --> 00:50:34.580
I had indigestion followed
by abdominal pain.

00:50:34.580 --> 00:50:35.780
MEGAN GREEN: That's going
to be in the anxiety book.

00:50:35.780 --> 00:50:36.710
SETH STEPHENS-DAVIDOWITZ: Yeah.

00:50:36.710 --> 00:50:39.247
So I don't want everyone to go
home and be like, oh crap, I

00:50:39.247 --> 00:50:40.230
got pancreatic cancer.

00:50:40.230 --> 00:50:46.350
But I think that's a really
impressive way to do medicine,

00:50:46.350 --> 00:50:47.740
relative to what we usually do.

00:50:47.740 --> 00:50:48.964
That's a pattern that--

00:50:48.964 --> 00:50:50.130
I talked to the researchers.

00:50:50.130 --> 00:50:52.980
Doctors don't know that
series of symptoms.

00:50:52.980 --> 00:50:56.910
If you think the way that
doctors now diagnose diseases,

00:50:56.910 --> 00:50:59.340
it's not as sophisticated
as a time series

00:50:59.340 --> 00:51:02.370
of people's symptoms over
time, and a huge sample

00:51:02.370 --> 00:51:04.710
to pick up patterns like that.

00:51:04.710 --> 00:51:06.510
So I'm kind of--

00:51:06.510 --> 00:51:08.992
a lot of people after
that said, so what

00:51:08.992 --> 00:51:10.200
are the ethical implications?

00:51:10.200 --> 00:51:13.740
Should a search company, if
they know this information,

00:51:13.740 --> 00:51:15.270
should they tell you?

00:51:15.270 --> 00:51:17.430
Right below your
button I Feel Lucky,

00:51:17.430 --> 00:51:20.070
You Have Pancreatic Cancer,
or You Might Have Pancreatic

00:51:20.070 --> 00:51:22.547
Cancer, like, that's kind
of a depressing thing

00:51:22.547 --> 00:51:23.630
to see on a search engine.

00:51:23.630 --> 00:51:27.080
But I think-- so I
think that they should,

00:51:27.080 --> 00:51:29.100
but you should be
able to opt into it.

00:51:29.100 --> 00:51:32.130
Say, if I'm the type
that wants information,

00:51:32.130 --> 00:51:35.550
you should be able to say, hey,
if you can mind these patterns,

00:51:35.550 --> 00:51:39.540
and potentially, if I have some
series of symptoms that tell me

00:51:39.540 --> 00:51:41.850
that I'm at a risk of
a disease, and if I

00:51:41.850 --> 00:51:44.700
am told that, I'll increase my
odds, I want to be told that.

00:51:44.700 --> 00:51:46.090
But I go even further.

00:51:46.090 --> 00:51:48.840
I think that these businesses
now have an obligation

00:51:48.840 --> 00:51:51.865
to be researching this
to potentially find

00:51:51.865 --> 00:51:52.740
patterns of symptoms.

00:51:52.740 --> 00:51:54.600
Because I'm kind of
pissed that there

00:51:54.600 --> 00:51:57.300
may be diseases that, just
because not enough data

00:51:57.300 --> 00:52:02.430
scientists at these companies
are looking into it,

00:52:02.430 --> 00:52:05.230
they may be picking up
patterns in my symptoms

00:52:05.230 --> 00:52:06.730
that could potentially
save my life.

00:52:06.730 --> 00:52:07.710
So I think that--

00:52:07.710 --> 00:52:09.940
I go even more extreme
and think that there

00:52:09.940 --> 00:52:12.840
is an ethical
obligation of companies

00:52:12.840 --> 00:52:17.930
who have this data to be really
figuring out the health stuff,

00:52:17.930 --> 00:52:18.600
there.

00:52:18.600 --> 00:52:20.340
MEGAN GREEN: So
big implications.

00:52:20.340 --> 00:52:24.125
And with that, let's give
Seth a round of applause

00:52:24.125 --> 00:52:24.750
for being here.

00:52:24.750 --> 00:52:26.250
[APPLAUSE]

00:52:26.250 --> 00:52:28.100
Thank you.

