WEBVTT

1
00:00:00.480 --> 00:00:04.110
In this video will introduce a new type of neuron network for unsupervised

2
00:00:04.111 --> 00:00:06.060
learning known as the Ottonian quarter.

3
00:00:08.000 --> 00:00:08.550
<v 1>Okay.</v>

4
00:00:08.550 --> 00:00:13.140
<v 0>So in the previous videos we've talked about the restrictive Bolsa machine.
Uh,</v>

5
00:00:13.141 --> 00:00:18.141
we said that it's a neural network for unsupervised learning that is learning in

6
00:00:18.271 --> 00:00:22.730
the case where inputs only take the form of an input.
Vectors would have the,

7
00:00:22.770 --> 00:00:26.910
we don't have an associated target or we don't use it throughout learning.

8
00:00:26.911 --> 00:00:31.080
So we were only trying to learn on the basis of only the input vector x.

9
00:00:31.770 --> 00:00:36.270
Uh,
so this can be useful either to extract the meaningful features,
uh,

10
00:00:36.271 --> 00:00:40.140
or to leverage,
uh,
somehow the,
uh,

11
00:00:40.170 --> 00:00:44.250
some large amount of unlabeled data that we might have in order perhaps to

12
00:00:44.251 --> 00:00:49.080
improve our performance on some particular task for which we have a little

13
00:00:49.081 --> 00:00:52.290
amount of labeled data.
And,
uh,

14
00:00:52.320 --> 00:00:56.910
we'll also talk when we talk about deep learning,
uh,
how we can use,
uh,

15
00:00:56.930 --> 00:01:00.630
unsupervised learning as a data dependent,
regular riser for training.

16
00:01:01.410 --> 00:01:03.670
And now in the,
uh,

17
00:01:03.840 --> 00:01:08.610
following videos with will consider is another type of unsupervised neural

18
00:01:08.611 --> 00:01:11.010
network known as the auto in quarter.

19
00:01:14.350 --> 00:01:17.950
So the auto encoder is actually a very simple type of neural network.

20
00:01:17.951 --> 00:01:22.930
It's feed forward,
much like the neural network we've seen for a classification.

21
00:01:23.950 --> 00:01:28.870
And the main difference is that it's going to be trained not to produce a,

22
00:01:28.990 --> 00:01:31.610
uh,
class,
uh,
at the outlet.

23
00:01:31.611 --> 00:01:34.960
So not to define the distribution over a class at the output,

24
00:01:34.990 --> 00:01:39.990
but instead to reproduce the input that was fed to the neural network.

25
00:01:41.020 --> 00:01:44.330
So,
uh,
essentially it's going to be a,

26
00:01:44.370 --> 00:01:49.170
so we look at a photo in color with a single hidden layer.
Uh,

27
00:01:49.210 --> 00:01:50.470
it's output layer,

28
00:01:50.500 --> 00:01:55.500
which I'm now calling x hat is going to be of the same size as the input layer.

29
00:01:56.920 --> 00:02:01.360
And when we're going to train it were somehow going to compare the values at the

30
00:02:01.361 --> 00:02:05.950
output produced by the neural network deal doing color with the values at the

31
00:02:05.951 --> 00:02:10.780
input.
And we're going to encourage the neural net to reproduce as perfectly as

32
00:02:10.781 --> 00:02:15.370
possible at the output layer,
the value that was fed in the input.

33
00:02:16.780 --> 00:02:21.400
Um,
so,
uh,
and otherwise this is just a regular neural network,

34
00:02:21.730 --> 00:02:24.340
uh,
in the terminology of tone colors.

35
00:02:24.550 --> 00:02:29.550
We'll refer to the part of the model that computes the hidden layer that compute

36
00:02:30.080 --> 00:02:32.520
compute h of x as the end quarter.

37
00:02:32.521 --> 00:02:37.521
So it's going to encode our input into a latent representation and a one form it

38
00:02:38.020 --> 00:02:40.680
could take,
which is very popular and people use Iti.

39
00:02:40.810 --> 00:02:44.350
Usually use is just a sigmoid of a linear transformation,

40
00:02:44.380 --> 00:02:49.030
much like in a regular neural network.
And then we're going to have a decoder,

41
00:02:49.031 --> 00:02:54.031
which is going to take the a latent representation h of x,

42
00:02:55.150 --> 00:02:59.410
the output of the quarter and it's going again,

43
00:02:59.411 --> 00:03:02.800
linear transform it and pass it through nonlinearity.

44
00:03:02.801 --> 00:03:06.130
So if we have inputs that are binary between zero and one,

45
00:03:06.160 --> 00:03:10.390
then we use a sigmoid not only in the already to squash everything at the output

46
00:03:10.391 --> 00:03:11.200
between zero and one.

47
00:03:11.200 --> 00:03:16.200
So it has the same range as the values we can have at the input if we had other

48
00:03:16.841 --> 00:03:21.190
types of inputs.
Uh,
so if we have really validated and would use something else,

49
00:03:21.550 --> 00:03:23.370
but so this output,
we,

50
00:03:23.590 --> 00:03:28.420
we're now going to a node x hat is going to be the decoded,

51
00:03:28.840 --> 00:03:32.500
uh,
input based on the representation,

52
00:03:32.560 --> 00:03:37.490
the latent representation extracted by the autumn quarter.
Uh,
and other,

53
00:03:37.540 --> 00:03:37.960
uh,

54
00:03:37.960 --> 00:03:42.960
frequent practice is to actually tie the weights between w and what I'm calling

55
00:03:43.121 --> 00:03:44.080
w star,

56
00:03:44.081 --> 00:03:48.010
which is the weights between the hidden layer and the reconstruction output

57
00:03:48.011 --> 00:03:52.030
layer.
So,
uh,
by ty I mean that,
uh,

58
00:03:52.060 --> 00:03:57.060
w star would be set as the transpose of the,

59
00:03:57.970 --> 00:04:01.900
uh,
connections.
I go into the hidden layer.
So in other words,

60
00:04:01.901 --> 00:04:05.440
the connections going into their Hinden layers are going to be the same as the

61
00:04:05.441 --> 00:04:07.990
connections that go out of the hidden layer.

62
00:04:09.190 --> 00:04:13.780
And a one motivation for doing this is that this starts looking a bit more like

63
00:04:13.781 --> 00:04:15.100
a restricted poster machine.

64
00:04:15.430 --> 00:04:18.130
So like in a restricted Boltzmann machine where we're going from the hidden

65
00:04:18.131 --> 00:04:23.131
layer to a space which corresponds to the same space as the input space,

66
00:04:26.490 --> 00:04:29.530
then we're usually multiplying by the transpose.

67
00:04:30.190 --> 00:04:35.080
And so in this sense,
each hidden unit here extracts a particular feature.

68
00:04:35.260 --> 00:04:38.440
And then when we're trying to reconstruct with just putting back that feature

69
00:04:38.441 --> 00:04:43.170
into the input.
Okay,
so often we'll use that,
but that's not necessary.

70
00:04:43.520 --> 00:04:47.380
A,
if you read,
if you see in the paper that tide waits were used,

71
00:04:47.410 --> 00:04:51.320
that's what it means.
That the transpose connections were the same as December.

72
00:04:51.321 --> 00:04:55.780
So when we'll do grain in the sand,
for instance,
the gradient on double you will,

73
00:04:56.280 --> 00:04:59.980
there will be a great incoming from the hidden layer and the great and that goes

74
00:04:59.981 --> 00:05:04.450
directly,
uh,
uh,
through w transpose that we'll have,

75
00:05:04.750 --> 00:05:08.740
of course I have an impact on what w should be.
All right?

76
00:05:08.980 --> 00:05:11.770
So that's the main definition of in the top quarter.

77
00:05:11.771 --> 00:05:16.600
It's pretty simple fit for neural network that is trying to reproduce it's,
uh,

78
00:05:16.660 --> 00:05:17.950
input at the output.

79
00:05:18.340 --> 00:05:22.290
And one motivation for training a neural net like this is that essentially we're

80
00:05:22.300 --> 00:05:25.510
training it such that the imp,

81
00:05:25.600 --> 00:05:29.950
the hidden representation maintains all the information about the input.

82
00:05:30.790 --> 00:05:31.331
And for instance,

83
00:05:31.331 --> 00:05:34.900
if we use a head in there that's much smaller than the input layer,

84
00:05:35.050 --> 00:05:39.340
that this means that the automaker is sort of going to compress the information.

85
00:05:39.610 --> 00:05:40.300
So,
uh,

86
00:05:40.300 --> 00:05:45.300
ignore part of the input that is not useful for reconstruct,

87
00:05:46.090 --> 00:05:50.710
reconstructing it and just focus on what makes the input special and,
uh,

88
00:05:50.711 --> 00:05:53.980
what,
um,
is important to,
uh,

89
00:05:54.010 --> 00:05:58.160
extract from it if we were to be able to reconstruct afterwards.
Okay.

90
00:05:58.161 --> 00:06:02.630
So you could think that a tone color like this could be used for,
uh,

91
00:06:02.660 --> 00:06:07.130
Francis compressing the data.
Um,
but we'll talk about more,

92
00:06:07.220 --> 00:06:11.360
uh,
how we can use it to extract some meaningful features,
uh,

93
00:06:11.420 --> 00:06:12.740
for some given problem.

