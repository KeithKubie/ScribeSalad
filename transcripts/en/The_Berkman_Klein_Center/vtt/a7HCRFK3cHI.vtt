WEBVTT
Kind: captions
Language: en

00:00:05.800 --> 00:00:07.240
I think one of the things I want to say

00:00:07.240 --> 00:00:09.580
from the start is it's not like AI is

00:00:09.580 --> 00:00:12.580
going to appear. It's actually out there,

00:00:12.580 --> 00:00:16.029
in some instances in ways that we never

00:00:16.029 --> 00:00:18.369
even notice. So for example checking

00:00:18.369 --> 00:00:22.270
credit card usage, predicting

00:00:22.270 --> 00:00:24.730
patients who are likely to come back

00:00:24.730 --> 00:00:29.950
into the emergency room and therefore keeping them from going home and

00:00:29.950 --> 00:00:32.349
then having to come back. There are some

00:00:32.349 --> 00:00:34.239
very clever uses of artificial

00:00:34.239 --> 00:00:36.190
intelligence in education. But

00:00:36.190 --> 00:00:38.320
increasingly in ways in which we do

00:00:38.320 --> 00:00:40.239
notice it. For example the various

00:00:40.239 --> 00:00:42.670
personal assistants on our phones.

00:00:42.670 --> 00:00:44.829
So it's out there making a difference

00:00:44.829 --> 00:00:47.949
in most cases in situations where it's

00:00:47.949 --> 00:00:50.230
not replacing people but really

00:00:50.230 --> 00:00:52.929
working with people. So I stress that

00:00:52.929 --> 00:00:55.449
distinction between replacing people and

00:00:55.449 --> 00:00:57.699
complementing people because so much of

00:00:57.699 --> 00:00:59.859
the science-fiction that's out there and

00:00:59.859 --> 00:01:02.679
so much that's in the press presumes

00:01:02.679 --> 00:01:04.449
that the goal would be to replace people.

00:01:04.449 --> 00:01:08.200
But there's a perfectly wonderful way to

00:01:08.200 --> 00:01:10.420
replace human intelligence. It takes

00:01:10.420 --> 00:01:13.180
a man and a woman certain acts and

00:01:13.180 --> 00:01:16.480
you're done and human intelligence is

00:01:16.480 --> 00:01:19.480
limited in certain ways so why make that

00:01:19.480 --> 00:01:22.600
the aim. I mean it has fascinated people for

00:01:22.600 --> 00:01:26.110
centuries. Probably tied back to religion

00:01:26.110 --> 00:01:28.810
and people being

00:01:28.810 --> 00:01:30.340
concerned that people would try to

00:01:30.340 --> 00:01:34.210
imitate God. This is the story

00:01:34.210 --> 00:01:36.040
of The Golem it's the story of

00:01:36.040 --> 00:01:38.170
Frankenstein it's the story of ex

00:01:38.170 --> 00:01:41.590
machina. But that's not the best way to

00:01:41.590 --> 00:01:42.670
think about developing

00:01:42.670 --> 00:01:46.090
artificial intelligence methods nor

00:01:46.090 --> 00:01:47.440
embodying them in computer systems.

00:01:47.440 --> 00:01:50.770
Rather it would be better to complement

00:01:50.770 --> 00:01:55.300
people as many computer systems do

00:01:55.300 --> 00:01:57.490
now. So that's the reason I make that

00:01:57.490 --> 00:01:59.920
distinction and urge it is that

00:01:59.920 --> 00:02:03.880
regardless of which two aims you pick the

00:02:03.880 --> 00:02:06.040
systems are going to- unless we

00:02:06.040 --> 00:02:07.780
just send them to Mars by themselves -

00:02:07.780 --> 00:02:09.129
they're going to exist in a world that's

00:02:09.129 --> 00:02:11.920
populated with human beings. You can see

00:02:11.920 --> 00:02:14.200
this playing out actually in something

00:02:14.200 --> 00:02:16.060
that's been in the press a lot recently

00:02:16.060 --> 00:02:18.720
which is autonomous and semi-autonomous

00:02:18.720 --> 00:02:20.790
vehicles. So for example autonomous

00:02:20.790 --> 00:02:23.100
vehicles the idea is they just drive.

00:02:23.100 --> 00:02:24.900
No person's involved in the driving at all.

00:02:25.380 --> 00:02:27.540
Semi autonomous vehicles do some driving

00:02:27.540 --> 00:02:31.470
but then shift off with people. In both

00:02:31.470 --> 00:02:41.430
cases they're interacting with people. So until we build roads on which the only vehicles are

00:02:41.430 --> 00:02:44.610
fully autonomous the vehicles are going

00:02:44.610 --> 00:02:46.680
to have to interact with people. And even

00:02:46.680 --> 00:02:49.740
if the only vehicles are fully

00:02:49.740 --> 00:02:51.780
autonomous we have to get rid of all of

00:02:51.780 --> 00:02:53.550
the pedestrians and all of the bicycles

00:02:53.550 --> 00:02:55.709
and everything. That's the issue with

00:02:55.709 --> 00:02:57.330
fully autonomous they will still have to

00:02:57.330 --> 00:02:58.470
interact with people.

00:02:58.470 --> 00:03:00.600
Semi autonomous vehicles have to take

00:03:00.600 --> 00:03:01.920
into account people's cognitive

00:03:01.920 --> 00:03:04.530
capacities in order to handle the

00:03:04.530 --> 00:03:06.959
so-called handoff between people and

00:03:06.959 --> 00:03:09.300
computer systems appropriately. So

00:03:09.300 --> 00:03:13.530
there's no, except in a few instances,

00:03:13.530 --> 00:03:15.120
there's no taking people out of the

00:03:15.120 --> 00:03:17.400
picture. I think it's a much more

00:03:17.400 --> 00:03:21.090
valuable in societally useful

00:03:21.090 --> 00:03:22.739
perspective to think from the very

00:03:22.739 --> 00:03:25.260
beginning of designing in ways to

00:03:25.260 --> 00:03:26.880
interact appropriately with people

00:03:26.880 --> 00:03:28.950
rather than building something separate

00:03:28.950 --> 00:03:32.010
from people and then presuming people

00:03:32.010 --> 00:03:33.180
will adjust to it.

00:03:33.180 --> 00:03:35.370
What's crucial at this point is to bring

00:03:35.370 --> 00:03:37.440
together expertise from these different

00:03:37.440 --> 00:03:40.320
fields and that that expertise has to be

00:03:40.320 --> 00:03:42.180
brought in before the systems are

00:03:42.180 --> 00:03:45.780
designed and released to the world. And

00:03:45.780 --> 00:03:50.130
now is the time to think about this. To bring together people who

00:03:50.130 --> 00:03:51.720
are experts in artificial intelligence

00:03:51.720 --> 00:03:58.920
with people who understand ethics deeply with psychologists who understand

00:03:58.920 --> 00:04:01.920
human cognition, with social scientists

00:04:01.920 --> 00:04:04.680
who understand social organizations, so

00:04:04.680 --> 00:04:09.660
that we can as the rubric now is "make AI

00:04:09.660 --> 00:04:14.370
for social good." And that rubric

00:04:14.370 --> 00:04:17.370
actually covers also building systems

00:04:17.370 --> 00:04:19.560
that help low-resource communities,

00:04:19.560 --> 00:04:21.720
building systems that protect the

00:04:21.720 --> 00:04:27.380
environment, building systems that contribute to education and

00:04:27.380 --> 00:04:29.420
healthcare. I think both that we need to

00:04:29.420 --> 00:04:33.560
train and teach people about ethics.

00:04:33.560 --> 00:04:35.420
And here I want to say I'm not talking

00:04:35.420 --> 00:04:37.850
about professional ethics. I'm talking

00:04:37.850 --> 00:04:41.390
about really understanding the

00:04:41.390 --> 00:04:43.640
trade-offs between consequentialist ideas

00:04:43.640 --> 00:04:46.430
and deontological ideas, grappling

00:04:46.430 --> 00:04:48.410
with virtue ethics, thinking about

00:04:48.410 --> 00:04:51.080
justice, thinking about who you're serving, really

00:04:51.080 --> 00:04:54.080
a deep sense of ethics and about these

00:04:54.080 --> 00:04:57.560
systems, and then make it part of the

00:04:57.560 --> 00:05:01.190
process of design of the systems. It's a

00:05:01.190 --> 00:05:03.470
years-long process of having people from

00:05:03.470 --> 00:05:05.690
these different fields come together,

00:05:05.690 --> 00:05:10.730
explain their work, explain their

00:05:10.730 --> 00:05:12.500
perspectives to each other in ways that

00:05:12.500 --> 00:05:15.380
are accessible, treat those different

00:05:15.380 --> 00:05:17.900
perspectives with respect, and develop a

00:05:17.900 --> 00:05:20.210
common vocabulary and a

00:05:21.290 --> 00:05:22.940
way of approaching things

00:05:22.940 --> 00:05:29.600
together. That can't be

00:05:29.600 --> 00:05:44.240
short-circuited it's really a years-long process.

