WEBVTT
Kind: captions
Language: en

00:00:00.370 --> 00:00:03.358
Now we focused on the NVIDIA CUDA architecture for this course,

00:00:03.358 --> 00:00:06.610
but there are also some Cross Platform solutions that compile not only to CUDA

00:00:06.610 --> 00:00:10.947
but to GPUs and multicore CPUs from AMD, Intel, or ARM.

00:00:10.947 --> 00:00:13.165
Some of the tools and libraries we've already mentioned,

00:00:13.165 --> 00:00:16.658
like Halite and Copperhead, specifically target CPU back ends.

00:00:16.658 --> 00:00:23.816
Three other cross platform solutions worth mentioning are openCL, openGL Compute, and openACC.

00:00:23.816 --> 00:00:28.266
You'll find that these first 2 are really similar to CUDA in the overall shape.

00:00:28.266 --> 00:00:30.965
They share concepts such as thread blocks and shared memory,

00:00:30.965 --> 00:00:33.135
even though the names and syntax vary.

00:00:33.135 --> 00:00:36.607
For example, openCL refers to work groups rather than thread blocks.

00:00:36.607 --> 00:00:38.721
But the basic ideas are isomorphic to CUDA,

00:00:38.721 --> 00:00:40.718
and that's one reason why we focused on CUDA

00:00:40.718 --> 00:00:43.594
is that what you learn there is really applicable in many places.

00:00:43.594 --> 00:00:45.860
OpenGL compute, as the name implies,

00:00:45.860 --> 00:00:49.686
is tightly integrative with the extremely popular openGL graphics library,

00:00:49.686 --> 00:00:52.692
and this makes it a good choice for developers that are doing graphics

00:00:52.692 --> 00:00:55.889
or image processing work who need to support multiple GPU vendors.

00:00:55.889 --> 00:00:58.668
Now the third option, openACC, is a little different.

00:00:58.668 --> 00:01:00.964
This is a directives based approach.

00:01:00.964 --> 00:01:04.379
Directives are annotations that the programmer puts into his or her serial code

00:01:04.379 --> 00:01:09.846
that help the compiler figure out how to automatically parallelize some of the loops.

00:01:09.846 --> 00:01:12.605
For example, with the appropriate directives,

00:01:12.605 --> 00:01:17.201
an openACC compiler can transform a nested for loop into a thread launch on CUDA

00:01:17.201 --> 00:01:19.650
or a multi-threaded SIMD routine on a multicore CPU,

00:01:19.650 --> 00:01:23.729
so often adding just a few lines to an existing code base can get dramatic speed ups.

00:01:23.729 --> 00:01:28.166
And this is what makes openACC a great choice for programmers that have a large legacy code

00:01:28.166 --> 00:01:30.182
that they want to parallelize.

00:01:30.182 --> 00:01:33.931
OpenACC makes it easy and incremental to add parallelism to an existing code,

00:01:33.931 --> 00:01:37.507
and it'll be very familiar to programmers that are used to using openMP.

00:01:37.507 --> 00:01:39.710
The ACC stands for accelerators,

00:01:39.710 --> 00:01:42.572
and Open ACC is essentially updating of OpenMP

00:01:42.572 --> 00:01:46.009
to reflect the evolution of accelerators like the GPU.

00:01:46.009 --> 00:01:55.254
Open ACC compilers exist for C, C++, and Fortran.

