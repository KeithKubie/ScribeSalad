WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.210
why does your neural network need a

00:00:02.210 --> 00:00:02.220
why does your neural network need a
 

00:00:02.220 --> 00:00:04.730
why does your neural network need a
nonlinear activation function turns out

00:00:04.730 --> 00:00:04.740
nonlinear activation function turns out
 

00:00:04.740 --> 00:00:06.079
nonlinear activation function turns out
that for your neural network to compute

00:00:06.079 --> 00:00:06.089
that for your neural network to compute
 

00:00:06.089 --> 00:00:08.150
that for your neural network to compute
interesting functions you do need to

00:00:08.150 --> 00:00:08.160
interesting functions you do need to
 

00:00:08.160 --> 00:00:09.980
interesting functions you do need to
take a nonlinear activation function

00:00:09.980 --> 00:00:09.990
take a nonlinear activation function
 

00:00:09.990 --> 00:00:12.950
take a nonlinear activation function
less you want so just the for prop

00:00:12.950 --> 00:00:12.960
less you want so just the for prop
 

00:00:12.960 --> 00:00:15.980
less you want so just the for prop
equations for the neural network why

00:00:15.980 --> 00:00:15.990
equations for the neural network why
 

00:00:15.990 --> 00:00:18.019
equations for the neural network why
don't we just get rid of this get rid of

00:00:18.019 --> 00:00:18.029
don't we just get rid of this get rid of
 

00:00:18.029 --> 00:00:22.150
don't we just get rid of this get rid of
the function G and set a1 equals Z 1 or

00:00:22.150 --> 00:00:22.160
the function G and set a1 equals Z 1 or
 

00:00:22.160 --> 00:00:25.910
the function G and set a1 equals Z 1 or
alternatively you could say that G of Z

00:00:25.910 --> 00:00:25.920
alternatively you could say that G of Z
 

00:00:25.920 --> 00:00:28.279
alternatively you could say that G of Z
is equal to Z right sometimes this is

00:00:28.279 --> 00:00:28.289
is equal to Z right sometimes this is
 

00:00:28.289 --> 00:00:31.460
is equal to Z right sometimes this is
called the linear activation function

00:00:31.460 --> 00:00:31.470
called the linear activation function
 

00:00:31.470 --> 00:00:33.290
called the linear activation function
maybe a better name for it would be the

00:00:33.290 --> 00:00:33.300
maybe a better name for it would be the
 

00:00:33.300 --> 00:00:35.420
maybe a better name for it would be the
identity activation function because it

00:00:35.420 --> 00:00:35.430
identity activation function because it
 

00:00:35.430 --> 00:00:38.270
identity activation function because it
was just outputs whatever was input for

00:00:38.270 --> 00:00:38.280
was just outputs whatever was input for
 

00:00:38.280 --> 00:00:39.260
was just outputs whatever was input for
the purpose of this

00:00:39.260 --> 00:00:39.270
the purpose of this
 

00:00:39.270 --> 00:00:44.000
the purpose of this
what if a2 was just equal to z2 it turns

00:00:44.000 --> 00:00:44.010
what if a2 was just equal to z2 it turns
 

00:00:44.010 --> 00:00:46.400
what if a2 was just equal to z2 it turns
out if you do this then this model is

00:00:46.400 --> 00:00:46.410
out if you do this then this model is
 

00:00:46.410 --> 00:00:50.720
out if you do this then this model is
just computing Y or Y hat as a linear

00:00:50.720 --> 00:00:50.730
just computing Y or Y hat as a linear
 

00:00:50.730 --> 00:00:54.380
just computing Y or Y hat as a linear
function of your input features x2 take

00:00:54.380 --> 00:00:54.390
function of your input features x2 take
 

00:00:54.390 --> 00:00:57.160
function of your input features x2 take
the first two equations if you have that

00:00:57.160 --> 00:00:57.170
the first two equations if you have that
 

00:00:57.170 --> 00:01:05.960
the first two equations if you have that
a1 is equal to z1 is equal to w1 X plus

00:01:05.960 --> 00:01:05.970
a1 is equal to z1 is equal to w1 X plus
 

00:01:05.970 --> 00:01:12.710
a1 is equal to z1 is equal to w1 X plus
B and if then a2 is equal to z2 is equal

00:01:12.710 --> 00:01:12.720
B and if then a2 is equal to z2 is equal
 

00:01:12.720 --> 00:01:22.090
B and if then a2 is equal to z2 is equal
to W 2 a1 plus B then if you take the

00:01:22.090 --> 00:01:22.100
to W 2 a1 plus B then if you take the
 

00:01:22.100 --> 00:01:24.910
to W 2 a1 plus B then if you take the
definition of a 1 and plug it in there

00:01:24.910 --> 00:01:24.920
definition of a 1 and plug it in there
 

00:01:24.920 --> 00:01:30.930
definition of a 1 and plug it in there
you find that a 2 is equal to W 2 times

00:01:30.930 --> 00:01:30.940
you find that a 2 is equal to W 2 times
 

00:01:30.940 --> 00:01:38.740
you find that a 2 is equal to W 2 times
W 1 X plus B 1 a bit right so this is on

00:01:38.740 --> 00:01:38.750
W 1 X plus B 1 a bit right so this is on
 

00:01:38.750 --> 00:01:46.690
W 1 X plus B 1 a bit right so this is on
a 1 plus B 2 and so this simplifies to W

00:01:46.690 --> 00:01:46.700
a 1 plus B 2 and so this simplifies to W
 

00:01:46.700 --> 00:01:59.490
a 1 plus B 2 and so this simplifies to W
2 W 1 X plus W 2 B 1 plus B 2 so this is

00:01:59.490 --> 00:01:59.500
2 W 1 X plus W 2 B 1 plus B 2 so this is
 

00:01:59.500 --> 00:02:06.700
2 W 1 X plus W 2 B 1 plus B 2 so this is
just let's call this w prime B prime so

00:02:06.700 --> 00:02:06.710
just let's call this w prime B prime so
 

00:02:06.710 --> 00:02:10.150
just let's call this w prime B prime so
this is just equal to W prime X plus B

00:02:10.150 --> 00:02:10.160
this is just equal to W prime X plus B
 

00:02:10.160 --> 00:02:10.690
this is just equal to W prime X plus B
Prime

00:02:10.690 --> 00:02:10.700
Prime
 

00:02:10.700 --> 00:02:12.729
Prime
if you were to use linear activation

00:02:12.729 --> 00:02:12.739
if you were to use linear activation
 

00:02:12.739 --> 00:02:15.670
if you were to use linear activation
functions or we go to call them identity

00:02:15.670 --> 00:02:15.680
functions or we go to call them identity
 

00:02:15.680 --> 00:02:18.670
functions or we go to call them identity
activation functions then the new

00:02:18.670 --> 00:02:18.680
activation functions then the new
 

00:02:18.680 --> 00:02:20.949
activation functions then the new
network is just outputting a linear

00:02:20.949 --> 00:02:20.959
network is just outputting a linear
 

00:02:20.959 --> 00:02:24.370
network is just outputting a linear
function of the input and we'll talk

00:02:24.370 --> 00:02:24.380
function of the input and we'll talk
 

00:02:24.380 --> 00:02:26.560
function of the input and we'll talk
about deep networks later neural

00:02:26.560 --> 00:02:26.570
about deep networks later neural
 

00:02:26.570 --> 00:02:28.569
about deep networks later neural
networks with many many layers many many

00:02:28.569 --> 00:02:28.579
networks with many many layers many many
 

00:02:28.579 --> 00:02:30.550
networks with many many layers many many
hidden layers and it turns out that if

00:02:30.550 --> 00:02:30.560
hidden layers and it turns out that if
 

00:02:30.560 --> 00:02:33.580
hidden layers and it turns out that if
you use a linear activation function or

00:02:33.580 --> 00:02:33.590
you use a linear activation function or
 

00:02:33.590 --> 00:02:35.050
you use a linear activation function or
alternatively if you don't have an

00:02:35.050 --> 00:02:35.060
alternatively if you don't have an
 

00:02:35.060 --> 00:02:37.000
alternatively if you don't have an
activation function then no matter how

00:02:37.000 --> 00:02:37.010
activation function then no matter how
 

00:02:37.010 --> 00:02:38.800
activation function then no matter how
many layers your neural network has

00:02:38.800 --> 00:02:38.810
many layers your neural network has
 

00:02:38.810 --> 00:02:41.590
many layers your neural network has
always doing is just computing a linear

00:02:41.590 --> 00:02:41.600
always doing is just computing a linear
 

00:02:41.600 --> 00:02:43.599
always doing is just computing a linear
activation function so you might as well

00:02:43.599 --> 00:02:43.609
activation function so you might as well
 

00:02:43.609 --> 00:02:46.960
activation function so you might as well
not have any hidden layers some of the

00:02:46.960 --> 00:02:46.970
not have any hidden layers some of the
 

00:02:46.970 --> 00:02:49.780
not have any hidden layers some of the
cases that briefly mentioned it turns

00:02:49.780 --> 00:02:49.790
cases that briefly mentioned it turns
 

00:02:49.790 --> 00:02:52.000
cases that briefly mentioned it turns
out that if you have a linear activation

00:02:52.000 --> 00:02:52.010
out that if you have a linear activation
 

00:02:52.010 --> 00:02:54.490
out that if you have a linear activation
function here and a sigmoid function

00:02:54.490 --> 00:02:54.500
function here and a sigmoid function
 

00:02:54.500 --> 00:02:57.009
function here and a sigmoid function
here then this model is no more

00:02:57.009 --> 00:02:57.019
here then this model is no more
 

00:02:57.019 --> 00:02:58.990
here then this model is no more
expressive than standard logistic

00:02:58.990 --> 00:02:59.000
expressive than standard logistic
 

00:02:59.000 --> 00:03:02.680
expressive than standard logistic
regression without any hidden layer so I

00:03:02.680 --> 00:03:02.690
regression without any hidden layer so I
 

00:03:02.690 --> 00:03:04.420
regression without any hidden layer so I
won't bother to prove that but you could

00:03:04.420 --> 00:03:04.430
won't bother to prove that but you could
 

00:03:04.430 --> 00:03:06.550
won't bother to prove that but you could
try to do so if you want but to take

00:03:06.550 --> 00:03:06.560
try to do so if you want but to take
 

00:03:06.560 --> 00:03:09.670
try to do so if you want but to take
home is that a linear hidden layer is

00:03:09.670 --> 00:03:09.680
home is that a linear hidden layer is
 

00:03:09.680 --> 00:03:12.699
home is that a linear hidden layer is
more or less useless because on the

00:03:12.699 --> 00:03:12.709
more or less useless because on the
 

00:03:12.709 --> 00:03:15.039
more or less useless because on the
composition of two linear functions is a

00:03:15.039 --> 00:03:15.049
composition of two linear functions is a
 

00:03:15.049 --> 00:03:17.860
composition of two linear functions is a
sailfin linear function so unless you

00:03:17.860 --> 00:03:17.870
sailfin linear function so unless you
 

00:03:17.870 --> 00:03:20.199
sailfin linear function so unless you
throw a non-linearity in there then

00:03:20.199 --> 00:03:20.209
throw a non-linearity in there then
 

00:03:20.209 --> 00:03:21.970
throw a non-linearity in there then
you're not computing more interesting

00:03:21.970 --> 00:03:21.980
you're not computing more interesting
 

00:03:21.980 --> 00:03:24.039
you're not computing more interesting
functions even as you go deeper in the

00:03:24.039 --> 00:03:24.049
functions even as you go deeper in the
 

00:03:24.049 --> 00:03:27.220
functions even as you go deeper in the
network there is just one place where

00:03:27.220 --> 00:03:27.230
network there is just one place where
 

00:03:27.230 --> 00:03:29.050
network there is just one place where
you might use a linear activation

00:03:29.050 --> 00:03:29.060
you might use a linear activation
 

00:03:29.060 --> 00:03:32.160
you might use a linear activation
function G of Z equals Z

00:03:32.160 --> 00:03:32.170
function G of Z equals Z
 

00:03:32.170 --> 00:03:34.980
function G of Z equals Z
and that's if you are doing machine

00:03:34.980 --> 00:03:34.990
and that's if you are doing machine
 

00:03:34.990 --> 00:03:37.440
and that's if you are doing machine
learning on a regression problem so if y

00:03:37.440 --> 00:03:37.450
learning on a regression problem so if y
 

00:03:37.450 --> 00:03:40.589
learning on a regression problem so if y
is a real number so for example if

00:03:40.589 --> 00:03:40.599
is a real number so for example if
 

00:03:40.599 --> 00:03:42.059
is a real number so for example if
you're trying to predict housing prices

00:03:42.059 --> 00:03:42.069
you're trying to predict housing prices
 

00:03:42.069 --> 00:03:45.990
you're trying to predict housing prices
so why is a there's not 0 1 but is a

00:03:45.990 --> 00:03:46.000
so why is a there's not 0 1 but is a
 

00:03:46.000 --> 00:03:49.290
so why is a there's not 0 1 but is a
real number you know anywhere from zero

00:03:49.290 --> 00:03:49.300
real number you know anywhere from zero
 

00:03:49.300 --> 00:03:51.300
real number you know anywhere from zero
dollars is a price of homes up to

00:03:51.300 --> 00:03:51.310
dollars is a price of homes up to
 

00:03:51.310 --> 00:03:54.540
dollars is a price of homes up to
however expensive right how's the scale

00:03:54.540 --> 00:03:54.550
however expensive right how's the scale
 

00:03:54.550 --> 00:03:56.990
however expensive right how's the scale
I guess maybe houses can be you know

00:03:56.990 --> 00:03:57.000
I guess maybe houses can be you know
 

00:03:57.000 --> 00:03:59.360
I guess maybe houses can be you know
potentially millions of dollars so

00:03:59.360 --> 00:03:59.370
potentially millions of dollars so
 

00:03:59.370 --> 00:04:04.050
potentially millions of dollars so
however however much houses cost in your

00:04:04.050 --> 00:04:04.060
however however much houses cost in your
 

00:04:04.060 --> 00:04:08.490
however however much houses cost in your
data set but if Y takes on these real

00:04:08.490 --> 00:04:08.500
data set but if Y takes on these real
 

00:04:08.500 --> 00:04:12.300
data set but if Y takes on these real
values then it might be ok to have a

00:04:12.300 --> 00:04:12.310
values then it might be ok to have a
 

00:04:12.310 --> 00:04:14.400
values then it might be ok to have a
linear activation function here so that

00:04:14.400 --> 00:04:14.410
linear activation function here so that
 

00:04:14.410 --> 00:04:19.590
linear activation function here so that
your output Y hat is also a real number

00:04:19.590 --> 00:04:19.600
your output Y hat is also a real number
 

00:04:19.600 --> 00:04:21.599
your output Y hat is also a real number
going anywhere from minus infinity to

00:04:21.599 --> 00:04:21.609
going anywhere from minus infinity to
 

00:04:21.609 --> 00:04:25.909
going anywhere from minus infinity to
plus infinity but then the hidden units

00:04:25.909 --> 00:04:25.919
plus infinity but then the hidden units
 

00:04:25.919 --> 00:04:27.990
plus infinity but then the hidden units
should not use them your activation

00:04:27.990 --> 00:04:28.000
should not use them your activation
 

00:04:28.000 --> 00:04:31.100
should not use them your activation
functions they could use value or 10 H

00:04:31.100 --> 00:04:31.110
functions they could use value or 10 H
 

00:04:31.110 --> 00:04:34.469
functions they could use value or 10 H
or Li Q value or maybe something else so

00:04:34.469 --> 00:04:34.479
or Li Q value or maybe something else so
 

00:04:34.479 --> 00:04:36.030
or Li Q value or maybe something else so
the one place you might use a linear

00:04:36.030 --> 00:04:36.040
the one place you might use a linear
 

00:04:36.040 --> 00:04:38.490
the one place you might use a linear
activation function is usually in the

00:04:38.490 --> 00:04:38.500
activation function is usually in the
 

00:04:38.500 --> 00:04:42.600
activation function is usually in the
output layer but other than that using a

00:04:42.600 --> 00:04:42.610
output layer but other than that using a
 

00:04:42.610 --> 00:04:46.260
output layer but other than that using a
linear activation function in a hidden

00:04:46.260 --> 00:04:46.270
linear activation function in a hidden
 

00:04:46.270 --> 00:04:48.690
linear activation function in a hidden
layer except for some very special

00:04:48.690 --> 00:04:48.700
layer except for some very special
 

00:04:48.700 --> 00:04:51.360
layer except for some very special
circumstances relating to compression

00:04:51.360 --> 00:04:51.370
circumstances relating to compression
 

00:04:51.370 --> 00:04:53.580
circumstances relating to compression
that won't want to talk about using the

00:04:53.580 --> 00:04:53.590
that won't want to talk about using the
 

00:04:53.590 --> 00:04:55.200
that won't want to talk about using the
linear activation function is extremely

00:04:55.200 --> 00:04:55.210
linear activation function is extremely
 

00:04:55.210 --> 00:04:57.300
linear activation function is extremely
rare oh and of course they're actually

00:04:57.300 --> 00:04:57.310
rare oh and of course they're actually
 

00:04:57.310 --> 00:04:59.100
rare oh and of course they're actually
predicting housing prices as you saw in

00:04:59.100 --> 00:04:59.110
predicting housing prices as you saw in
 

00:04:59.110 --> 00:05:01.230
predicting housing prices as you saw in
the week 1 video because housing prices

00:05:01.230 --> 00:05:01.240
the week 1 video because housing prices
 

00:05:01.240 --> 00:05:03.450
the week 1 video because housing prices
are all non-negative perhaps even then

00:05:03.450 --> 00:05:03.460
are all non-negative perhaps even then
 

00:05:03.460 --> 00:05:05.790
are all non-negative perhaps even then
you can use a rare loop activation

00:05:05.790 --> 00:05:05.800
you can use a rare loop activation
 

00:05:05.800 --> 00:05:08.430
you can use a rare loop activation
function so that your outputs Y hat are

00:05:08.430 --> 00:05:08.440
function so that your outputs Y hat are
 

00:05:08.440 --> 00:05:12.029
function so that your outputs Y hat are
all greater than or equal to 0 so I hope

00:05:12.029 --> 00:05:12.039
all greater than or equal to 0 so I hope
 

00:05:12.039 --> 00:05:14.010
all greater than or equal to 0 so I hope
that gives you a sense of why having a

00:05:14.010 --> 00:05:14.020
that gives you a sense of why having a
 

00:05:14.020 --> 00:05:16.170
that gives you a sense of why having a
nonlinear activation function is a

00:05:16.170 --> 00:05:16.180
nonlinear activation function is a
 

00:05:16.180 --> 00:05:19.590
nonlinear activation function is a
critical part of neural networks next

00:05:19.590 --> 00:05:19.600
critical part of neural networks next
 

00:05:19.600 --> 00:05:21.029
critical part of neural networks next
we're going to start to talk about

00:05:21.029 --> 00:05:21.039
we're going to start to talk about
 

00:05:21.039 --> 00:05:24.330
we're going to start to talk about
gradient descent and to do that to set

00:05:24.330 --> 00:05:24.340
gradient descent and to do that to set
 

00:05:24.340 --> 00:05:26.219
gradient descent and to do that to set
up discussion for gradient descent in

00:05:26.219 --> 00:05:26.229
up discussion for gradient descent in
 

00:05:26.229 --> 00:05:28.320
up discussion for gradient descent in
the next video I want to show you how to

00:05:28.320 --> 00:05:28.330
the next video I want to show you how to
 

00:05:28.330 --> 00:05:30.960
the next video I want to show you how to
estimate how to compute the slope of the

00:05:30.960 --> 00:05:30.970
estimate how to compute the slope of the
 

00:05:30.970 --> 00:05:33.150
estimate how to compute the slope of the
derivative of individual activation

00:05:33.150 --> 00:05:33.160
derivative of individual activation
 

00:05:33.160 --> 00:05:35.070
derivative of individual activation
functions so let's go on to the next

00:05:35.070 --> 00:05:35.080
functions so let's go on to the next
 

00:05:35.080 --> 00:05:37.440
functions so let's go on to the next
video

