WEBVTT
Kind: captions
Language: en

00:00:00.539 --> 00:00:06.350
In a nutshell, we take the most useful research
from cognitive science about how the human

00:00:06.350 --> 00:00:11.219
brain reasons and makes decisions and the
errors that the human brain tends to make

00:00:11.219 --> 00:00:15.599
when reasoning or making decisions and we
turn that research into workshops that people

00:00:15.599 --> 00:00:20.400
can use to apply it to their own lives and
improve their own decision making about their

00:00:20.400 --> 00:00:25.390
health, their finances, their relationships
and the decisions that they make for society

00:00:25.390 --> 00:00:29.410
and the world in general about how to vote
and how to treat other people and what they

00:00:29.410 --> 00:00:31.119
can do to improve the world.&nbsp;

00:00:31.119 --> 00:00:35.400
One example of rationality in action, just
to give you a sense of what it looks like

00:00:35.400 --> 00:00:43.040
and how it's relevant, back in 1985 Intel
had a large foot in the memory chip manufacturing

00:00:43.040 --> 00:00:49.740
business and they’d been losing money on
memory chips for years, so the two cofounders

00:00:49.740 --> 00:00:55.579
Andy Grove and Gordon Moore met to figure
out what to do, and at one point Andy asked,

00:00:55.579 --> 00:01:01.030
"What do you think a new CEO would do if the
board kicked us out and brought in a new CEO?"&nbsp;

00:01:01.030 --> 00:01:04.640
And without hesitating Gordon replied, 'Oh,
he would get out of the memory business."&nbsp;

00:01:04.640 --> 00:01:08.950
And Andy said, "Well, so is there any reason
we shouldn't do that, if we just walk out

00:01:08.950 --> 00:01:12.450
of the door and come back in and switch out
of the memory business?"&nbsp;&nbsp;

00:01:12.450 --> 00:01:16.260
And in fact that’s exactly what they decided
to do, and it was a huge success.&nbsp; And this

00:01:16.260 --> 00:01:22.960
is just one example of a cognitive bias that
appears in lots of contexts in lots of skills

00:01:22.960 --> 00:01:28.630
called the&nbsp;commitment effect, where we stick
with a business plan or a career or a relationship

00:01:28.630 --> 00:01:32.260
long after it has become quite clear that
it's not doing anything for us or that it's

00:01:32.260 --> 00:01:36.350
actively destructive or self-destructive because
we have an irrational commitment to whatever

00:01:36.350 --> 00:01:40.120
we have been doing for a while because we
don't like to idea of our past investment

00:01:40.120 --> 00:01:42.820
having gone to waste or because it’s become
part of our identity.

00:01:42.820 --> 00:01:48.060
And the technique that Andy and Gordon used
to snap themselves out of the commitment effect

00:01:48.060 --> 00:01:53.310
is also a really generally useful technique
called&nbsp;looking at a problem as if you were

00:01:53.310 --> 00:01:57.159
an outsider, an outside party.&nbsp; Over the
past few decades cognitive scientists have

00:01:57.159 --> 00:02:02.380
learned a lot about this and many other biases
that human brains are subject to when we try

00:02:02.380 --> 00:02:06.380
to make decisions, but fortunately, cognitive
science has also learned a lot about things

00:02:06.380 --> 00:02:09.879
that we can do to improve.&nbsp; So at the Center
for Applied Rationality we're taking that

00:02:09.879 --> 00:02:14.040
research, teaching people about the biases,
where they occur, when we're vulnerable to

00:02:14.040 --> 00:02:19.060
biases and then teaching them simple and easy
mental habits like&nbsp;looking at a problem as

00:02:19.060 --> 00:02:22.379
if you're an outsider&nbsp;to overcome those biases.&nbsp;&nbsp;

00:02:22.379 --> 00:02:27.739
Rationality also is a significant public good,
and that's one of the main motivations behind

00:02:27.739 --> 00:02:32.389
the founding of CFAR.&nbsp; Society would look
very different if rationality, rational thinking

00:02:32.389 --> 00:02:36.159
and decision making were widespread.&nbsp; Just
to name a few of many, many things that I

00:02:36.159 --> 00:02:42.239
could name, we as a society would demand evidence
from politicians for the claims that they

00:02:42.239 --> 00:02:47.950
made, we would notice when politicians were
misdirecting us by playing on our emotions,

00:02:47.950 --> 00:02:53.360
we would be less vulnerable to prejudice and
to stereotypes because we would be weary of

00:02:53.360 --> 00:02:59.529
the confirmation bias, which is a very universal
bias in which you look for examples that fit

00:02:59.529 --> 00:03:03.140
a stereotype, but you don't look for examples
that don't fit the stereotypes, so you end

00:03:03.140 --> 00:03:05.779
up confirming and reinforcing a stereotype
in your mind.&nbsp;

00:03:05.779 --> 00:03:10.930
We would spend our money much more effectively
as a society to stave off important risks.&nbsp;

00:03:10.930 --> 00:03:18.870
The fact that things like terrorism and crimes
like abduction are so vividly portrayed on

00:03:18.870 --> 00:03:23.559
the news makes them much more salient and
makes us overweight those risks the same way

00:03:23.559 --> 00:03:27.579
we overweight any kind of evidence that is
particularly vivid or salient, even if it

00:03:27.579 --> 00:03:30.480
isn’t actually the best bang for our buck
in terms of risk reduction.

