WEBVTT
Kind: captions
Language: en

00:00:02.550 --> 00:00:05.750
hi everyone my name is Alvin and welcome

00:00:05.750 --> 00:00:05.760
hi everyone my name is Alvin and welcome
 

00:00:05.760 --> 00:00:08.180
hi everyone my name is Alvin and welcome
to our second lecture on deep sequence

00:00:08.180 --> 00:00:08.190
to our second lecture on deep sequence
 

00:00:08.190 --> 00:00:11.000
to our second lecture on deep sequence
modeling so in the first I was lecture

00:00:11.000 --> 00:00:11.010
modeling so in the first I was lecture
 

00:00:11.010 --> 00:00:13.310
modeling so in the first I was lecture
Alexander talked about the essentials of

00:00:13.310 --> 00:00:13.320
Alexander talked about the essentials of
 

00:00:13.320 --> 00:00:15.560
Alexander talked about the essentials of
neural networks and feed-forward models

00:00:15.560 --> 00:00:15.570
neural networks and feed-forward models
 

00:00:15.570 --> 00:00:17.570
neural networks and feed-forward models
and now we're going to turn our

00:00:17.570 --> 00:00:17.580
and now we're going to turn our
 

00:00:17.580 --> 00:00:21.110
and now we're going to turn our
attention to applying neural networks to

00:00:21.110 --> 00:00:21.120
attention to applying neural networks to
 

00:00:21.120 --> 00:00:23.450
attention to applying neural networks to
problems which involve sequential

00:00:23.450 --> 00:00:23.460
problems which involve sequential
 

00:00:23.460 --> 00:00:25.850
problems which involve sequential
processing of data and why these sorts

00:00:25.850 --> 00:00:25.860
processing of data and why these sorts
 

00:00:25.860 --> 00:00:28.130
processing of data and why these sorts
of tasks require a different type of

00:00:28.130 --> 00:00:28.140
of tasks require a different type of
 

00:00:28.140 --> 00:00:30.140
of tasks require a different type of
network architecture from what we've

00:00:30.140 --> 00:00:30.150
network architecture from what we've
 

00:00:30.150 --> 00:00:35.030
network architecture from what we've
seen so far so before we dive in I like

00:00:35.030 --> 00:00:35.040
seen so far so before we dive in I like
 

00:00:35.040 --> 00:00:36.710
seen so far so before we dive in I like
to start off with a really simple

00:00:36.710 --> 00:00:36.720
to start off with a really simple
 

00:00:36.720 --> 00:00:39.319
to start off with a really simple
example suppose we have this picture of

00:00:39.319 --> 00:00:39.329
example suppose we have this picture of
 

00:00:39.329 --> 00:00:41.960
example suppose we have this picture of
a ball and we want to predict where it

00:00:41.960 --> 00:00:41.970
a ball and we want to predict where it
 

00:00:41.970 --> 00:00:45.080
a ball and we want to predict where it
will travel to next without any prior

00:00:45.080 --> 00:00:45.090
will travel to next without any prior
 

00:00:45.090 --> 00:00:47.270
will travel to next without any prior
information about the ball's history any

00:00:47.270 --> 00:00:47.280
information about the ball's history any
 

00:00:47.280 --> 00:00:49.639
information about the ball's history any
guess on its next position will be

00:00:49.639 --> 00:00:49.649
guess on its next position will be
 

00:00:49.649 --> 00:00:53.569
guess on its next position will be
exactly that just a guess however if in

00:00:53.569 --> 00:00:53.579
exactly that just a guess however if in
 

00:00:53.579 --> 00:00:55.670
exactly that just a guess however if in
addition to the current location of the

00:00:55.670 --> 00:00:55.680
addition to the current location of the
 

00:00:55.680 --> 00:00:58.760
addition to the current location of the
ball I also gave you a history of its

00:00:58.760 --> 00:00:58.770
ball I also gave you a history of its
 

00:00:58.770 --> 00:01:01.160
ball I also gave you a history of its
previous locations now the problem

00:01:01.160 --> 00:01:01.170
previous locations now the problem
 

00:01:01.170 --> 00:01:03.770
previous locations now the problem
becomes much easier and I think we can

00:01:03.770 --> 00:01:03.780
becomes much easier and I think we can
 

00:01:03.780 --> 00:01:06.649
becomes much easier and I think we can
all agree that we have a pretty clear

00:01:06.649 --> 00:01:06.659
all agree that we have a pretty clear
 

00:01:06.659 --> 00:01:09.669
all agree that we have a pretty clear
sense of where the ball is going to next

00:01:09.669 --> 00:01:09.679
sense of where the ball is going to next
 

00:01:09.679 --> 00:01:11.929
sense of where the ball is going to next
so this is a really really simple

00:01:11.929 --> 00:01:11.939
so this is a really really simple
 

00:01:11.939 --> 00:01:14.510
so this is a really really simple
sequence modeling problem given this

00:01:14.510 --> 00:01:14.520
sequence modeling problem given this
 

00:01:14.520 --> 00:01:17.209
sequence modeling problem given this
this image this you know a thought

00:01:17.209 --> 00:01:17.219
this image this you know a thought
 

00:01:17.219 --> 00:01:18.919
this image this you know a thought
experiment of a balls travel through

00:01:18.919 --> 00:01:18.929
experiment of a balls travel through
 

00:01:18.929 --> 00:01:20.840
experiment of a balls travel through
space can we predict where it's going to

00:01:20.840 --> 00:01:20.850
space can we predict where it's going to
 

00:01:20.850 --> 00:01:24.830
space can we predict where it's going to
go next but in reality the truth is that

00:01:24.830 --> 00:01:24.840
go next but in reality the truth is that
 

00:01:24.840 --> 00:01:27.649
go next but in reality the truth is that
sequential data is all around us for

00:01:27.649 --> 00:01:27.659
sequential data is all around us for
 

00:01:27.659 --> 00:01:30.199
sequential data is all around us for
example audio can be split up into a

00:01:30.199 --> 00:01:30.209
example audio can be split up into a
 

00:01:30.209 --> 00:01:33.949
example audio can be split up into a
sequence of sound waves while text can

00:01:33.949 --> 00:01:33.959
sequence of sound waves while text can
 

00:01:33.959 --> 00:01:35.660
sequence of sound waves while text can
be split up into a sequence of either

00:01:35.660 --> 00:01:35.670
be split up into a sequence of either
 

00:01:35.670 --> 00:01:39.949
be split up into a sequence of either
characters or words and beyond these two

00:01:39.949 --> 00:01:39.959
characters or words and beyond these two
 

00:01:39.959 --> 00:01:42.529
characters or words and beyond these two
Ubiquiti examples there are many more

00:01:42.529 --> 00:01:42.539
Ubiquiti examples there are many more
 

00:01:42.539 --> 00:01:45.289
Ubiquiti examples there are many more
cases in which sequential processing may

00:01:45.289 --> 00:01:45.299
cases in which sequential processing may
 

00:01:45.299 --> 00:01:48.080
cases in which sequential processing may
be useful from analysis of medical

00:01:48.080 --> 00:01:48.090
be useful from analysis of medical
 

00:01:48.090 --> 00:01:51.559
be useful from analysis of medical
signals like EKGs to predicting stock

00:01:51.559 --> 00:01:51.569
signals like EKGs to predicting stock
 

00:01:51.569 --> 00:01:54.879
signals like EKGs to predicting stock
trends to processing genomic data and

00:01:54.879 --> 00:01:54.889
trends to processing genomic data and
 

00:01:54.889 --> 00:01:57.230
trends to processing genomic data and
now that we've gone in the sense of what

00:01:57.230 --> 00:01:57.240
now that we've gone in the sense of what
 

00:01:57.240 --> 00:02:00.709
now that we've gone in the sense of what
sequential data looks like I want to

00:02:00.709 --> 00:02:00.719
sequential data looks like I want to
 

00:02:00.719 --> 00:02:03.679
sequential data looks like I want to
turn our attention to another simple

00:02:03.679 --> 00:02:03.689
turn our attention to another simple
 

00:02:03.689 --> 00:02:07.399
turn our attention to another simple
problem to to motivate motivate the

00:02:07.399 --> 00:02:07.409
problem to to motivate motivate the
 

00:02:07.409 --> 00:02:08.900
problem to to motivate motivate the
types of networks that we're going to

00:02:08.900 --> 00:02:08.910
types of networks that we're going to
 

00:02:08.910 --> 00:02:11.990
types of networks that we're going to
use for this task and in this case

00:02:11.990 --> 00:02:12.000
use for this task and in this case
 

00:02:12.000 --> 00:02:14.510
use for this task and in this case
suppose we have a language model where

00:02:14.510 --> 00:02:14.520
suppose we have a language model where
 

00:02:14.520 --> 00:02:15.930
suppose we have a language model where
we're trying to Train

00:02:15.930 --> 00:02:15.940
we're trying to Train
 

00:02:15.940 --> 00:02:18.300
we're trying to Train
neural network to predict the next word

00:02:18.300 --> 00:02:18.310
neural network to predict the next word
 

00:02:18.310 --> 00:02:22.380
neural network to predict the next word
in a phrase or a sentence and suppose we

00:02:22.380 --> 00:02:22.390
in a phrase or a sentence and suppose we
 

00:02:22.390 --> 00:02:24.660
in a phrase or a sentence and suppose we
have this sentence this morning I took

00:02:24.660 --> 00:02:24.670
have this sentence this morning I took
 

00:02:24.670 --> 00:02:28.140
have this sentence this morning I took
my cat for a walk yes you heard and read

00:02:28.140 --> 00:02:28.150
my cat for a walk yes you heard and read
 

00:02:28.150 --> 00:02:30.270
my cat for a walk yes you heard and read
that right this morning I took my cat

00:02:30.270 --> 00:02:30.280
that right this morning I took my cat
 

00:02:30.280 --> 00:02:33.330
that right this morning I took my cat
for a walk and let's say we're given

00:02:33.330 --> 00:02:33.340
for a walk and let's say we're given
 

00:02:33.340 --> 00:02:35.970
for a walk and let's say we're given
these words this morning I took my cat

00:02:35.970 --> 00:02:35.980
these words this morning I took my cat
 

00:02:35.980 --> 00:02:38.970
these words this morning I took my cat
for a and we want to predict the next

00:02:38.970 --> 00:02:38.980
for a and we want to predict the next
 

00:02:38.980 --> 00:02:41.790
for a and we want to predict the next
word in the sequence and since this is a

00:02:41.790 --> 00:02:41.800
word in the sequence and since this is a
 

00:02:41.800 --> 00:02:43.590
word in the sequence and since this is a
class on deep learning we're going to

00:02:43.590 --> 00:02:43.600
class on deep learning we're going to
 

00:02:43.600 --> 00:02:45.930
class on deep learning we're going to
try to build a deep neural network like

00:02:45.930 --> 00:02:45.940
try to build a deep neural network like
 

00:02:45.940 --> 00:02:47.700
try to build a deep neural network like
a feed-forward Network from our first

00:02:47.700 --> 00:02:47.710
a feed-forward Network from our first
 

00:02:47.710 --> 00:02:51.540
a feed-forward Network from our first
lecture to do this and one problem that

00:02:51.540 --> 00:02:51.550
lecture to do this and one problem that
 

00:02:51.550 --> 00:02:53.490
lecture to do this and one problem that
we're immediately going to run into is

00:02:53.490 --> 00:02:53.500
we're immediately going to run into is
 

00:02:53.500 --> 00:02:55.890
we're immediately going to run into is
that our feed-forward network can only

00:02:55.890 --> 00:02:55.900
that our feed-forward network can only
 

00:02:55.900 --> 00:02:58.740
that our feed-forward network can only
take a fixed length vector as its input

00:02:58.740 --> 00:02:58.750
take a fixed length vector as its input
 

00:02:58.750 --> 00:03:01.890
take a fixed length vector as its input
and we have to specify this size of this

00:03:01.890 --> 00:03:01.900
and we have to specify this size of this
 

00:03:01.900 --> 00:03:04.320
and we have to specify this size of this
input right at the start and you can

00:03:04.320 --> 00:03:04.330
input right at the start and you can
 

00:03:04.330 --> 00:03:05.700
input right at the start and you can
imagine that this is going to be a

00:03:05.700 --> 00:03:05.710
imagine that this is going to be a
 

00:03:05.710 --> 00:03:07.770
imagine that this is going to be a
problem for our task in general because

00:03:07.770 --> 00:03:07.780
problem for our task in general because
 

00:03:07.780 --> 00:03:10.260
problem for our task in general because
sometimes we'll have a sentence we'll

00:03:10.260 --> 00:03:10.270
sometimes we'll have a sentence we'll
 

00:03:10.270 --> 00:03:12.360
sometimes we'll have a sentence we'll
have seen five words sometimes seven

00:03:12.360 --> 00:03:12.370
have seen five words sometimes seven
 

00:03:12.370 --> 00:03:14.910
have seen five words sometimes seven
words sometimes ten words and we want to

00:03:14.910 --> 00:03:14.920
words sometimes ten words and we want to
 

00:03:14.920 --> 00:03:17.360
words sometimes ten words and we want to
be able to predict what comes next so

00:03:17.360 --> 00:03:17.370
be able to predict what comes next so
 

00:03:17.370 --> 00:03:19.710
be able to predict what comes next so
fundamentally we need a way to handle

00:03:19.710 --> 00:03:19.720
fundamentally we need a way to handle
 

00:03:19.720 --> 00:03:23.250
fundamentally we need a way to handle
variable length input and one way we can

00:03:23.250 --> 00:03:23.260
variable length input and one way we can
 

00:03:23.260 --> 00:03:25.920
variable length input and one way we can
do this is to use this idea of a fixed

00:03:25.920 --> 00:03:25.930
do this is to use this idea of a fixed
 

00:03:25.930 --> 00:03:28.830
do this is to use this idea of a fixed
window to force our input vector to be a

00:03:28.830 --> 00:03:28.840
window to force our input vector to be a
 

00:03:28.840 --> 00:03:32.430
window to force our input vector to be a
certain length in this case - and this

00:03:32.430 --> 00:03:32.440
certain length in this case - and this
 

00:03:32.440 --> 00:03:34.050
certain length in this case - and this
means that no matter where we're trying

00:03:34.050 --> 00:03:34.060
means that no matter where we're trying
 

00:03:34.060 --> 00:03:36.360
means that no matter where we're trying
to make our prediction we just take the

00:03:36.360 --> 00:03:36.370
to make our prediction we just take the
 

00:03:36.370 --> 00:03:38.640
to make our prediction we just take the
previous two words and try to predict

00:03:38.640 --> 00:03:38.650
previous two words and try to predict
 

00:03:38.650 --> 00:03:42.000
previous two words and try to predict
the next word and we can represent these

00:03:42.000 --> 00:03:42.010
the next word and we can represent these
 

00:03:42.010 --> 00:03:44.460
the next word and we can represent these
two words as a fixed length vector where

00:03:44.460 --> 00:03:44.470
two words as a fixed length vector where
 

00:03:44.470 --> 00:03:46.979
two words as a fixed length vector where
we take a larger vector allocate some

00:03:46.979 --> 00:03:46.989
we take a larger vector allocate some
 

00:03:46.989 --> 00:03:49.590
we take a larger vector allocate some
space for the first word some space for

00:03:49.590 --> 00:03:49.600
space for the first word some space for
 

00:03:49.600 --> 00:03:52.020
space for the first word some space for
the second word and encode the identity

00:03:52.020 --> 00:03:52.030
the second word and encode the identity
 

00:03:52.030 --> 00:03:55.350
the second word and encode the identity
of each word in that vector but this is

00:03:55.350 --> 00:03:55.360
of each word in that vector but this is
 

00:03:55.360 --> 00:03:57.600
of each word in that vector but this is
problematic because because of the fact

00:03:57.600 --> 00:03:57.610
problematic because because of the fact
 

00:03:57.610 --> 00:03:59.580
problematic because because of the fact
that we're using this fixed window we're

00:03:59.580 --> 00:03:59.590
that we're using this fixed window we're
 

00:03:59.590 --> 00:04:02.400
that we're using this fixed window we're
giving ourselves a limited history which

00:04:02.400 --> 00:04:02.410
giving ourselves a limited history which
 

00:04:02.410 --> 00:04:04.830
giving ourselves a limited history which
means that we can't effectively model

00:04:04.830 --> 00:04:04.840
means that we can't effectively model
 

00:04:04.840 --> 00:04:07.290
means that we can't effectively model
long term dependencies in our input data

00:04:07.290 --> 00:04:07.300
long term dependencies in our input data
 

00:04:07.300 --> 00:04:09.449
long term dependencies in our input data
which is important in sentences like

00:04:09.449 --> 00:04:09.459
which is important in sentences like
 

00:04:09.459 --> 00:04:11.580
which is important in sentences like
this one where we clearly need

00:04:11.580 --> 00:04:11.590
this one where we clearly need
 

00:04:11.590 --> 00:04:13.530
this one where we clearly need
information from much earlier in the

00:04:13.530 --> 00:04:13.540
information from much earlier in the
 

00:04:13.540 --> 00:04:15.870
information from much earlier in the
sentence to accurately predict the next

00:04:15.870 --> 00:04:15.880
sentence to accurately predict the next
 

00:04:15.880 --> 00:04:19.500
sentence to accurately predict the next
word if we were only looking at the past

00:04:19.500 --> 00:04:19.510
word if we were only looking at the past
 

00:04:19.510 --> 00:04:21.750
word if we were only looking at the past
two words or the past three words or the

00:04:21.750 --> 00:04:21.760
two words or the past three words or the
 

00:04:21.760 --> 00:04:23.969
two words or the past three words or the
past five words even we wouldn't be able

00:04:23.969 --> 00:04:23.979
past five words even we wouldn't be able
 

00:04:23.979 --> 00:04:27.000
past five words even we wouldn't be able
to make this prediction being the the

00:04:27.000 --> 00:04:27.010
to make this prediction being the the
 

00:04:27.010 --> 00:04:28.270
to make this prediction being the the
word French

00:04:28.270 --> 00:04:28.280
word French
 

00:04:28.280 --> 00:04:30.980
word French
so we need a way to integrate

00:04:30.980 --> 00:04:30.990
so we need a way to integrate
 

00:04:30.990 --> 00:04:33.290
so we need a way to integrate
information from across the sentence but

00:04:33.290 --> 00:04:33.300
information from across the sentence but
 

00:04:33.300 --> 00:04:35.510
information from across the sentence but
also still represent the input as a

00:04:35.510 --> 00:04:35.520
also still represent the input as a
 

00:04:35.520 --> 00:04:38.899
also still represent the input as a
fixed length vector and another way we

00:04:38.899 --> 00:04:38.909
fixed length vector and another way we
 

00:04:38.909 --> 00:04:41.390
fixed length vector and another way we
could do this is by actually using the

00:04:41.390 --> 00:04:41.400
could do this is by actually using the
 

00:04:41.400 --> 00:04:44.089
could do this is by actually using the
entire sequence but representing it as a

00:04:44.089 --> 00:04:44.099
entire sequence but representing it as a
 

00:04:44.099 --> 00:04:46.999
entire sequence but representing it as a
set of counts and this representation is

00:04:46.999 --> 00:04:47.009
set of counts and this representation is
 

00:04:47.009 --> 00:04:49.339
set of counts and this representation is
what's called a bag of words where we

00:04:49.339 --> 00:04:49.349
what's called a bag of words where we
 

00:04:49.349 --> 00:04:52.100
what's called a bag of words where we
have some vector and each slot in this

00:04:52.100 --> 00:04:52.110
have some vector and each slot in this
 

00:04:52.110 --> 00:04:55.070
have some vector and each slot in this
vector represents a word and the value

00:04:55.070 --> 00:04:55.080
vector represents a word and the value
 

00:04:55.080 --> 00:04:56.960
vector represents a word and the value
that's in that slot represents the

00:04:56.960 --> 00:04:56.970
that's in that slot represents the
 

00:04:56.970 --> 00:04:59.059
that's in that slot represents the
number of times that that word appears

00:04:59.059 --> 00:04:59.069
number of times that that word appears
 

00:04:59.069 --> 00:05:03.260
number of times that that word appears
in this sentence and so we have a fixed

00:05:03.260 --> 00:05:03.270
in this sentence and so we have a fixed
 

00:05:03.270 --> 00:05:05.510
in this sentence and so we have a fixed
length vector over some vocabulary of

00:05:05.510 --> 00:05:05.520
length vector over some vocabulary of
 

00:05:05.520 --> 00:05:07.730
length vector over some vocabulary of
words regardless of the length of the

00:05:07.730 --> 00:05:07.740
words regardless of the length of the
 

00:05:07.740 --> 00:05:09.770
words regardless of the length of the
input sentence but the counts are just

00:05:09.770 --> 00:05:09.780
input sentence but the counts are just
 

00:05:09.780 --> 00:05:12.020
input sentence but the counts are just
going to be different and we can feed

00:05:12.020 --> 00:05:12.030
going to be different and we can feed
 

00:05:12.030 --> 00:05:14.450
going to be different and we can feed
this into our feed-forward neural

00:05:14.450 --> 00:05:14.460
this into our feed-forward neural
 

00:05:14.460 --> 00:05:16.760
this into our feed-forward neural
network to generate a prediction about

00:05:16.760 --> 00:05:16.770
network to generate a prediction about
 

00:05:16.770 --> 00:05:20.120
network to generate a prediction about
the next word and you may have already

00:05:20.120 --> 00:05:20.130
the next word and you may have already
 

00:05:20.130 --> 00:05:22.040
the next word and you may have already
realized that there's a big problem with

00:05:22.040 --> 00:05:22.050
realized that there's a big problem with
 

00:05:22.050 --> 00:05:24.680
realized that there's a big problem with
this approach in using counts we've just

00:05:24.680 --> 00:05:24.690
this approach in using counts we've just
 

00:05:24.690 --> 00:05:26.689
this approach in using counts we've just
completely abolished all sequence

00:05:26.689 --> 00:05:26.699
completely abolished all sequence
 

00:05:26.699 --> 00:05:28.999
completely abolished all sequence
information and so for example these two

00:05:28.999 --> 00:05:29.009
information and so for example these two
 

00:05:29.009 --> 00:05:31.399
information and so for example these two
sentences with completely opposite

00:05:31.399 --> 00:05:31.409
sentences with completely opposite
 

00:05:31.409 --> 00:05:33.800
sentences with completely opposite
semantic meanings would have the exact

00:05:33.800 --> 00:05:33.810
semantic meanings would have the exact
 

00:05:33.810 --> 00:05:37.189
semantic meanings would have the exact
same bag of words representation same

00:05:37.189 --> 00:05:37.199
same bag of words representation same
 

00:05:37.199 --> 00:05:40.010
same bag of words representation same
words same counts so obviously this

00:05:40.010 --> 00:05:40.020
words same counts so obviously this
 

00:05:40.020 --> 00:05:41.379
words same counts so obviously this
isn't going to work

00:05:41.379 --> 00:05:41.389
isn't going to work
 

00:05:41.389 --> 00:05:44.450
isn't going to work
another idea could be to simply extend

00:05:44.450 --> 00:05:44.460
another idea could be to simply extend
 

00:05:44.460 --> 00:05:46.519
another idea could be to simply extend
our first idea of a fixed window

00:05:46.519 --> 00:05:46.529
our first idea of a fixed window
 

00:05:46.529 --> 00:05:49.309
our first idea of a fixed window
thinking by thinking that by looking at

00:05:49.309 --> 00:05:49.319
thinking by thinking that by looking at
 

00:05:49.319 --> 00:05:51.469
thinking by thinking that by looking at
more words we can get most of the

00:05:51.469 --> 00:05:51.479
more words we can get most of the
 

00:05:51.479 --> 00:05:54.320
more words we can get most of the
context we need and so we can represent

00:05:54.320 --> 00:05:54.330
context we need and so we can represent
 

00:05:54.330 --> 00:05:57.320
context we need and so we can represent
our sentence in this way right just a

00:05:57.320 --> 00:05:57.330
our sentence in this way right just a
 

00:05:57.330 --> 00:06:00.950
our sentence in this way right just a
longer fixed window feed it into our

00:06:00.950 --> 00:06:00.960
longer fixed window feed it into our
 

00:06:00.960 --> 00:06:03.080
longer fixed window feed it into our
feed-forward model and make a prediction

00:06:03.080 --> 00:06:03.090
feed-forward model and make a prediction
 

00:06:03.090 --> 00:06:07.040
feed-forward model and make a prediction
and if we were to feed this vector into

00:06:07.040 --> 00:06:07.050
and if we were to feed this vector into
 

00:06:07.050 --> 00:06:09.290
and if we were to feed this vector into
a feed-forward neural network each of

00:06:09.290 --> 00:06:09.300
a feed-forward neural network each of
 

00:06:09.300 --> 00:06:12.230
a feed-forward neural network each of
these inputs each 0 or 1 in the vector

00:06:12.230 --> 00:06:12.240
these inputs each 0 or 1 in the vector
 

00:06:12.240 --> 00:06:14.330
these inputs each 0 or 1 in the vector
would have a separate weight connecting

00:06:14.330 --> 00:06:14.340
would have a separate weight connecting
 

00:06:14.340 --> 00:06:17.749
would have a separate weight connecting
it to the network and so if we

00:06:17.749 --> 00:06:17.759
it to the network and so if we
 

00:06:17.759 --> 00:06:19.670
it to the network and so if we
repeatedly were to see the words this

00:06:19.670 --> 00:06:19.680
repeatedly were to see the words this
 

00:06:19.680 --> 00:06:21.559
repeatedly were to see the words this
morning at the beginning of the sentence

00:06:21.559 --> 00:06:21.569
morning at the beginning of the sentence
 

00:06:21.569 --> 00:06:23.689
morning at the beginning of the sentence
the neural network would be able to

00:06:23.689 --> 00:06:23.699
the neural network would be able to
 

00:06:23.699 --> 00:06:25.879
the neural network would be able to
learn that this morning represents a

00:06:25.879 --> 00:06:25.889
learn that this morning represents a
 

00:06:25.889 --> 00:06:28.580
learn that this morning represents a
time or a setting but if in another

00:06:28.580 --> 00:06:28.590
time or a setting but if in another
 

00:06:28.590 --> 00:06:31.159
time or a setting but if in another
sentence this morning were to now appear

00:06:31.159 --> 00:06:31.169
sentence this morning were to now appear
 

00:06:31.169 --> 00:06:34.399
sentence this morning were to now appear
at the end of that sentence the network

00:06:34.399 --> 00:06:34.409
at the end of that sentence the network
 

00:06:34.409 --> 00:06:36.379
at the end of that sentence the network
is going to have difficulty recognizing

00:06:36.379 --> 00:06:36.389
is going to have difficulty recognizing
 

00:06:36.389 --> 00:06:39.290
is going to have difficulty recognizing
that this morning actually means this

00:06:39.290 --> 00:06:39.300
that this morning actually means this
 

00:06:39.300 --> 00:06:41.230
that this morning actually means this
morning because the

00:06:41.230 --> 00:06:41.240
morning because the
 

00:06:41.240 --> 00:06:44.020
morning because the
that see the end of the vector have

00:06:44.020 --> 00:06:44.030
that see the end of the vector have
 

00:06:44.030 --> 00:06:46.510
that see the end of the vector have
never seen that phrase before and the

00:06:46.510 --> 00:06:46.520
never seen that phrase before and the
 

00:06:46.520 --> 00:06:48.460
never seen that phrase before and the
parameters from the beginning haven't

00:06:48.460 --> 00:06:48.470
parameters from the beginning haven't
 

00:06:48.470 --> 00:06:52.480
parameters from the beginning haven't
been shared across the sequence and so

00:06:52.480 --> 00:06:52.490
been shared across the sequence and so
 

00:06:52.490 --> 00:06:54.580
been shared across the sequence and so
at a higher level what this means is

00:06:54.580 --> 00:06:54.590
at a higher level what this means is
 

00:06:54.590 --> 00:06:56.890
at a higher level what this means is
that what we learn about the sequence at

00:06:56.890 --> 00:06:56.900
that what we learn about the sequence at
 

00:06:56.900 --> 00:06:58.689
that what we learn about the sequence at
one point is not going to transfer

00:06:58.689 --> 00:06:58.699
one point is not going to transfer
 

00:06:58.699 --> 00:07:00.879
one point is not going to transfer
anywhere to anywhere else in the

00:07:00.879 --> 00:07:00.889
anywhere to anywhere else in the
 

00:07:00.889 --> 00:07:02.890
anywhere to anywhere else in the
sequence if we use this representation

00:07:02.890 --> 00:07:02.900
sequence if we use this representation
 

00:07:02.900 --> 00:07:06.010
sequence if we use this representation
and so hopefully by by walking through

00:07:06.010 --> 00:07:06.020
and so hopefully by by walking through
 

00:07:06.020 --> 00:07:08.469
and so hopefully by by walking through
this I've motivated that why a

00:07:08.469 --> 00:07:08.479
this I've motivated that why a
 

00:07:08.479 --> 00:07:10.540
this I've motivated that why a
traditional feed-forward neural network

00:07:10.540 --> 00:07:10.550
traditional feed-forward neural network
 

00:07:10.550 --> 00:07:12.580
traditional feed-forward neural network
is not really well suited to handle

00:07:12.580 --> 00:07:12.590
is not really well suited to handle
 

00:07:12.590 --> 00:07:16.390
is not really well suited to handle
sequential data and this simple example

00:07:16.390 --> 00:07:16.400
sequential data and this simple example
 

00:07:16.400 --> 00:07:19.210
sequential data and this simple example
further motivates a concrete set of

00:07:19.210 --> 00:07:19.220
further motivates a concrete set of
 

00:07:19.220 --> 00:07:21.790
further motivates a concrete set of
design criteria that we need to keep in

00:07:21.790 --> 00:07:21.800
design criteria that we need to keep in
 

00:07:21.800 --> 00:07:24.370
design criteria that we need to keep in
mind when thinking about building a

00:07:24.370 --> 00:07:24.380
mind when thinking about building a
 

00:07:24.380 --> 00:07:26.770
mind when thinking about building a
neural network for sequence modeling

00:07:26.770 --> 00:07:26.780
neural network for sequence modeling
 

00:07:26.780 --> 00:07:29.800
neural network for sequence modeling
problems specifically our network needs

00:07:29.800 --> 00:07:29.810
problems specifically our network needs
 

00:07:29.810 --> 00:07:32.439
problems specifically our network needs
to be able to handle variable length

00:07:32.439 --> 00:07:32.449
to be able to handle variable length
 

00:07:32.449 --> 00:07:35.920
to be able to handle variable length
sequences be able to track long term

00:07:35.920 --> 00:07:35.930
sequences be able to track long term
 

00:07:35.930 --> 00:07:39.460
sequences be able to track long term
dependencies in the data maintain

00:07:39.460 --> 00:07:39.470
dependencies in the data maintain
 

00:07:39.470 --> 00:07:41.969
dependencies in the data maintain
information about the sequence order and

00:07:41.969 --> 00:07:41.979
information about the sequence order and
 

00:07:41.979 --> 00:07:44.920
information about the sequence order and
share the parameters it learns across

00:07:44.920 --> 00:07:44.930
share the parameters it learns across
 

00:07:44.930 --> 00:07:47.529
share the parameters it learns across
the entirety of the sequence and today

00:07:47.529 --> 00:07:47.539
the entirety of the sequence and today
 

00:07:47.539 --> 00:07:49.029
the entirety of the sequence and today
we're going to talk about using

00:07:49.029 --> 00:07:49.039
we're going to talk about using
 

00:07:49.039 --> 00:07:51.580
we're going to talk about using
recurrent neural networks or RN ends as

00:07:51.580 --> 00:07:51.590
recurrent neural networks or RN ends as
 

00:07:51.590 --> 00:07:54.520
recurrent neural networks or RN ends as
a general framework for sequential

00:07:54.520 --> 00:07:54.530
a general framework for sequential
 

00:07:54.530 --> 00:07:56.290
a general framework for sequential
processing and sequence modeling

00:07:56.290 --> 00:07:56.300
processing and sequence modeling
 

00:07:56.300 --> 00:07:59.770
processing and sequence modeling
problems so let's go through the general

00:07:59.770 --> 00:07:59.780
problems so let's go through the general
 

00:07:59.780 --> 00:08:02.499
problems so let's go through the general
principle behind RN ends and explore how

00:08:02.499 --> 00:08:02.509
principle behind RN ends and explore how
 

00:08:02.509 --> 00:08:04.390
principle behind RN ends and explore how
they're a fundamentally different

00:08:04.390 --> 00:08:04.400
they're a fundamentally different
 

00:08:04.400 --> 00:08:06.850
they're a fundamentally different
architecture from what we saw in the

00:08:06.850 --> 00:08:06.860
architecture from what we saw in the
 

00:08:06.860 --> 00:08:11.200
architecture from what we saw in the
first lecture so this is a abstraction

00:08:11.200 --> 00:08:11.210
first lecture so this is a abstraction
 

00:08:11.210 --> 00:08:12.909
first lecture so this is a abstraction
of our standard feed-forward neural

00:08:12.909 --> 00:08:12.919
of our standard feed-forward neural
 

00:08:12.919 --> 00:08:15.670
of our standard feed-forward neural
network and in this architecture data

00:08:15.670 --> 00:08:15.680
network and in this architecture data
 

00:08:15.680 --> 00:08:17.770
network and in this architecture data
propagates in one direction from input

00:08:17.770 --> 00:08:17.780
propagates in one direction from input
 

00:08:17.780 --> 00:08:21.100
propagates in one direction from input
to output and we already motivated why a

00:08:21.100 --> 00:08:21.110
to output and we already motivated why a
 

00:08:21.110 --> 00:08:22.990
to output and we already motivated why a
network like this can't really handle

00:08:22.990 --> 00:08:23.000
network like this can't really handle
 

00:08:23.000 --> 00:08:27.129
network like this can't really handle
sequential data RNs in contrasts are

00:08:27.129 --> 00:08:27.139
sequential data RNs in contrasts are
 

00:08:27.139 --> 00:08:29.260
sequential data RNs in contrasts are
really well-suited for handling cases

00:08:29.260 --> 00:08:29.270
really well-suited for handling cases
 

00:08:29.270 --> 00:08:31.120
really well-suited for handling cases
where we have a sequence of inputs

00:08:31.120 --> 00:08:31.130
where we have a sequence of inputs
 

00:08:31.130 --> 00:08:33.519
where we have a sequence of inputs
rather than a single input and they're

00:08:33.519 --> 00:08:33.529
rather than a single input and they're
 

00:08:33.529 --> 00:08:35.199
rather than a single input and they're
great for problems like this one in

00:08:35.199 --> 00:08:35.209
great for problems like this one in
 

00:08:35.209 --> 00:08:37.810
great for problems like this one in
which a sequence of data is propagated

00:08:37.810 --> 00:08:37.820
which a sequence of data is propagated
 

00:08:37.820 --> 00:08:39.909
which a sequence of data is propagated
through the model to give a single

00:08:39.909 --> 00:08:39.919
through the model to give a single
 

00:08:39.919 --> 00:08:42.159
through the model to give a single
output for example you can imagine

00:08:42.159 --> 00:08:42.169
output for example you can imagine
 

00:08:42.169 --> 00:08:44.800
output for example you can imagine
training a model that takes as input a

00:08:44.800 --> 00:08:44.810
training a model that takes as input a
 

00:08:44.810 --> 00:08:47.079
training a model that takes as input a
sequence of words and outputs a

00:08:47.079 --> 00:08:47.089
sequence of words and outputs a
 

00:08:47.089 --> 00:08:49.090
sequence of words and outputs a
sentiment that's associated with that

00:08:49.090 --> 00:08:49.100
sentiment that's associated with that
 

00:08:49.100 --> 00:08:52.210
sentiment that's associated with that
phrase or that sentence alternatively

00:08:52.210 --> 00:08:52.220
phrase or that sentence alternatively
 

00:08:52.220 --> 00:08:54.700
phrase or that sentence alternatively
instead of returning a single output

00:08:54.700 --> 00:08:54.710
instead of returning a single output
 

00:08:54.710 --> 00:08:57.700
instead of returning a single output
could also train a model where we take

00:08:57.700 --> 00:08:57.710
could also train a model where we take
 

00:08:57.710 --> 00:09:00.010
could also train a model where we take
in a sequence of inputs propagate them

00:09:00.010 --> 00:09:00.020
in a sequence of inputs propagate them
 

00:09:00.020 --> 00:09:02.050
in a sequence of inputs propagate them
through our recurrent neural network

00:09:02.050 --> 00:09:02.060
through our recurrent neural network
 

00:09:02.060 --> 00:09:04.930
through our recurrent neural network
model and then return an output at each

00:09:04.930 --> 00:09:04.940
model and then return an output at each
 

00:09:04.940 --> 00:09:07.870
model and then return an output at each
time step in the sequence and an example

00:09:07.870 --> 00:09:07.880
time step in the sequence and an example
 

00:09:07.880 --> 00:09:09.850
time step in the sequence and an example
of this would be in text or music

00:09:09.850 --> 00:09:09.860
of this would be in text or music
 

00:09:09.860 --> 00:09:11.980
of this would be in text or music
generation and you'll get a chance to

00:09:11.980 --> 00:09:11.990
generation and you'll get a chance to
 

00:09:11.990 --> 00:09:14.440
generation and you'll get a chance to
explore this type of model later on in

00:09:14.440 --> 00:09:14.450
explore this type of model later on in
 

00:09:14.450 --> 00:09:18.400
explore this type of model later on in
the lab and beyond these two these two

00:09:18.400 --> 00:09:18.410
the lab and beyond these two these two
 

00:09:18.410 --> 00:09:21.250
the lab and beyond these two these two
examples there are a whole host of other

00:09:21.250 --> 00:09:21.260
examples there are a whole host of other
 

00:09:21.260 --> 00:09:22.900
examples there are a whole host of other
recurrent neural network arctor

00:09:22.900 --> 00:09:22.910
recurrent neural network arctor
 

00:09:22.910 --> 00:09:25.450
recurrent neural network arctor
architectures for sequential processing

00:09:25.450 --> 00:09:25.460
architectures for sequential processing
 

00:09:25.460 --> 00:09:27.430
architectures for sequential processing
and they've been applied to a range of

00:09:27.430 --> 00:09:27.440
and they've been applied to a range of
 

00:09:27.440 --> 00:09:30.640
and they've been applied to a range of
problems so what fundamentally is a

00:09:30.640 --> 00:09:30.650
problems so what fundamentally is a
 

00:09:30.650 --> 00:09:33.130
problems so what fundamentally is a
recurrent neural network as I mentioned

00:09:33.130 --> 00:09:33.140
recurrent neural network as I mentioned
 

00:09:33.140 --> 00:09:35.710
recurrent neural network as I mentioned
before to reiterate our standard vanilla

00:09:35.710 --> 00:09:35.720
before to reiterate our standard vanilla
 

00:09:35.720 --> 00:09:38.860
before to reiterate our standard vanilla
feed-forward neural network we're going

00:09:38.860 --> 00:09:38.870
feed-forward neural network we're going
 

00:09:38.870 --> 00:09:40.780
feed-forward neural network we're going
from input to output in one direction

00:09:40.780 --> 00:09:40.790
from input to output in one direction
 

00:09:40.790 --> 00:09:43.780
from input to output in one direction
and this fundamentally can't maintain

00:09:43.780 --> 00:09:43.790
and this fundamentally can't maintain
 

00:09:43.790 --> 00:09:48.190
and this fundamentally can't maintain
information about sequential data our

00:09:48.190 --> 00:09:48.200
information about sequential data our
 

00:09:48.200 --> 00:09:50.230
information about sequential data our
Nan's on the other hand are networks

00:09:50.230 --> 00:09:50.240
Nan's on the other hand are networks
 

00:09:50.240 --> 00:09:54.640
Nan's on the other hand are networks
where they have these loops these loops

00:09:54.640 --> 00:09:54.650
where they have these loops these loops
 

00:09:54.650 --> 00:09:56.770
where they have these loops these loops
in them which allow for information to

00:09:56.770 --> 00:09:56.780
in them which allow for information to
 

00:09:56.780 --> 00:10:00.070
in them which allow for information to
persist so in this diagram our RNN takes

00:10:00.070 --> 00:10:00.080
persist so in this diagram our RNN takes
 

00:10:00.080 --> 00:10:04.330
persist so in this diagram our RNN takes
as input this vector X of T outputs a

00:10:04.330 --> 00:10:04.340
as input this vector X of T outputs a
 

00:10:04.340 --> 00:10:07.540
as input this vector X of T outputs a
value like a prediction Y hat of T but

00:10:07.540 --> 00:10:07.550
value like a prediction Y hat of T but
 

00:10:07.550 --> 00:10:10.780
value like a prediction Y hat of T but
also makes this computation to update an

00:10:10.780 --> 00:10:10.790
also makes this computation to update an
 

00:10:10.790 --> 00:10:13.360
also makes this computation to update an
internal state which we call H of T and

00:10:13.360 --> 00:10:13.370
internal state which we call H of T and
 

00:10:13.370 --> 00:10:16.090
internal state which we call H of T and
then passes this information about its

00:10:16.090 --> 00:10:16.100
then passes this information about its
 

00:10:16.100 --> 00:10:18.610
then passes this information about its
state from this step of the network to

00:10:18.610 --> 00:10:18.620
state from this step of the network to
 

00:10:18.620 --> 00:10:22.300
state from this step of the network to
the next and we call these networks with

00:10:22.300 --> 00:10:22.310
the next and we call these networks with
 

00:10:22.310 --> 00:10:24.280
the next and we call these networks with
loops in them recurrent because

00:10:24.280 --> 00:10:24.290
loops in them recurrent because
 

00:10:24.290 --> 00:10:26.530
loops in them recurrent because
information is being passed internally

00:10:26.530 --> 00:10:26.540
information is being passed internally
 

00:10:26.540 --> 00:10:29.350
information is being passed internally
from one time step to the next so what's

00:10:29.350 --> 00:10:29.360
from one time step to the next so what's
 

00:10:29.360 --> 00:10:31.420
from one time step to the next so what's
going on under the hood how is

00:10:31.420 --> 00:10:31.430
going on under the hood how is
 

00:10:31.430 --> 00:10:34.750
going on under the hood how is
information being passed our nuns use a

00:10:34.750 --> 00:10:34.760
information being passed our nuns use a
 

00:10:34.760 --> 00:10:37.240
information being passed our nuns use a
simple recurrence relation in order to

00:10:37.240 --> 00:10:37.250
simple recurrence relation in order to
 

00:10:37.250 --> 00:10:40.090
simple recurrence relation in order to
process sequential data specifically

00:10:40.090 --> 00:10:40.100
process sequential data specifically
 

00:10:40.100 --> 00:10:43.180
process sequential data specifically
they maintain this internal state H of T

00:10:43.180 --> 00:10:43.190
they maintain this internal state H of T
 

00:10:43.190 --> 00:10:45.850
they maintain this internal state H of T
and at each time step we apply a

00:10:45.850 --> 00:10:45.860
and at each time step we apply a
 

00:10:45.860 --> 00:10:48.580
and at each time step we apply a
function parametrized by a set of

00:10:48.580 --> 00:10:48.590
function parametrized by a set of
 

00:10:48.590 --> 00:10:52.420
function parametrized by a set of
weights W to update this state based on

00:10:52.420 --> 00:10:52.430
weights W to update this state based on
 

00:10:52.430 --> 00:10:55.030
weights W to update this state based on
both the previous state H of T minus 1

00:10:55.030 --> 00:10:55.040
both the previous state H of T minus 1
 

00:10:55.040 --> 00:11:00.370
both the previous state H of T minus 1
and the current input X of T and the

00:11:00.370 --> 00:11:00.380
and the current input X of T and the
 

00:11:00.380 --> 00:11:02.590
and the current input X of T and the
important thing to know here is that the

00:11:02.590 --> 00:11:02.600
important thing to know here is that the
 

00:11:02.600 --> 00:11:04.930
important thing to know here is that the
same function and the same set of

00:11:04.930 --> 00:11:04.940
same function and the same set of
 

00:11:04.940 --> 00:11:07.480
same function and the same set of
parameters are used at every time step

00:11:07.480 --> 00:11:07.490
parameters are used at every time step
 

00:11:07.490 --> 00:11:09.519
parameters are used at every time step
and this addresses that important design

00:11:09.519 --> 00:11:09.529
and this addresses that important design
 

00:11:09.529 --> 00:11:12.010
and this addresses that important design
criteria from earlier of why it's useful

00:11:12.010 --> 00:11:12.020
criteria from earlier of why it's useful
 

00:11:12.020 --> 00:11:14.410
criteria from earlier of why it's useful
to share parameters in the context of

00:11:14.410 --> 00:11:14.420
to share parameters in the context of
 

00:11:14.420 --> 00:11:17.710
to share parameters in the context of
sequence modeling to be more specific

00:11:17.710 --> 00:11:17.720
sequence modeling to be more specific
 

00:11:17.720 --> 00:11:20.560
sequence modeling to be more specific
the RNN computation includes both a

00:11:20.560 --> 00:11:20.570
the RNN computation includes both a
 

00:11:20.570 --> 00:11:23.500
the RNN computation includes both a
state update as well as the output so

00:11:23.500 --> 00:11:23.510
state update as well as the output so
 

00:11:23.510 --> 00:11:26.019
state update as well as the output so
given our input vector we apply some

00:11:26.019 --> 00:11:26.029
given our input vector we apply some
 

00:11:26.029 --> 00:11:28.449
given our input vector we apply some
function to update the hidden state and

00:11:28.449 --> 00:11:28.459
function to update the hidden state and
 

00:11:28.459 --> 00:11:31.150
function to update the hidden state and
as we saw in the first lecture this

00:11:31.150 --> 00:11:31.160
as we saw in the first lecture this
 

00:11:31.160 --> 00:11:33.370
as we saw in the first lecture this
function is a standard neural net

00:11:33.370 --> 00:11:33.380
function is a standard neural net
 

00:11:33.380 --> 00:11:35.370
function is a standard neural net
operation that consists of

00:11:35.370 --> 00:11:35.380
operation that consists of
 

00:11:35.380 --> 00:11:37.920
operation that consists of
multiplication by a weight matrix and

00:11:37.920 --> 00:11:37.930
multiplication by a weight matrix and
 

00:11:37.930 --> 00:11:40.840
multiplication by a weight matrix and
applying a non linearity but in this

00:11:40.840 --> 00:11:40.850
applying a non linearity but in this
 

00:11:40.850 --> 00:11:43.329
applying a non linearity but in this
case since we both have the input vector

00:11:43.329 --> 00:11:43.339
case since we both have the input vector
 

00:11:43.339 --> 00:11:46.900
case since we both have the input vector
X of T as well as the previous state H

00:11:46.900 --> 00:11:46.910
X of T as well as the previous state H
 

00:11:46.910 --> 00:11:49.120
X of T as well as the previous state H
of T minus 1 as inputs to our function

00:11:49.120 --> 00:11:49.130
of T minus 1 as inputs to our function
 

00:11:49.130 --> 00:11:52.150
of T minus 1 as inputs to our function
we have two weight matrices and we can

00:11:52.150 --> 00:11:52.160
we have two weight matrices and we can
 

00:11:52.160 --> 00:11:54.280
we have two weight matrices and we can
then apply our non-linearity to the sum

00:11:54.280 --> 00:11:54.290
then apply our non-linearity to the sum
 

00:11:54.290 --> 00:11:56.250
then apply our non-linearity to the sum
of these two terms

00:11:56.250 --> 00:11:56.260
of these two terms
 

00:11:56.260 --> 00:11:59.680
of these two terms
finally we generate an output at a given

00:11:59.680 --> 00:11:59.690
finally we generate an output at a given
 

00:11:59.690 --> 00:12:02.500
finally we generate an output at a given
time step which is a transformed version

00:12:02.500 --> 00:12:02.510
time step which is a transformed version
 

00:12:02.510 --> 00:12:05.680
time step which is a transformed version
of our internal state the foes from a

00:12:05.680 --> 00:12:05.690
of our internal state the foes from a
 

00:12:05.690 --> 00:12:07.840
of our internal state the foes from a
multiplication by a separate weight

00:12:07.840 --> 00:12:07.850
multiplication by a separate weight
 

00:12:07.850 --> 00:12:11.980
multiplication by a separate weight
matrix so so far we've seen our n ends

00:12:11.980 --> 00:12:11.990
matrix so so far we've seen our n ends
 

00:12:11.990 --> 00:12:14.440
matrix so so far we've seen our n ends
as depicted as having these loops that

00:12:14.440 --> 00:12:14.450
as depicted as having these loops that
 

00:12:14.450 --> 00:12:17.560
as depicted as having these loops that
feedback back in on themselves another

00:12:17.560 --> 00:12:17.570
feedback back in on themselves another
 

00:12:17.570 --> 00:12:20.019
feedback back in on themselves another
way of thinking about the RNN can be in

00:12:20.019 --> 00:12:20.029
way of thinking about the RNN can be in
 

00:12:20.029 --> 00:12:23.100
way of thinking about the RNN can be in
terms of unrolling this loop across time

00:12:23.100 --> 00:12:23.110
terms of unrolling this loop across time
 

00:12:23.110 --> 00:12:27.190
terms of unrolling this loop across time
and if we do this we can think of the

00:12:27.190 --> 00:12:27.200
and if we do this we can think of the
 

00:12:27.200 --> 00:12:29.920
and if we do this we can think of the
RNN as multiple copies of the same

00:12:29.920 --> 00:12:29.930
RNN as multiple copies of the same
 

00:12:29.930 --> 00:12:32.740
RNN as multiple copies of the same
network where each copy is passing a

00:12:32.740 --> 00:12:32.750
network where each copy is passing a
 

00:12:32.750 --> 00:12:36.150
network where each copy is passing a
message onto its descendant and

00:12:36.150 --> 00:12:36.160
message onto its descendant and
 

00:12:36.160 --> 00:12:39.610
message onto its descendant and
continuing this this scheme throughout

00:12:39.610 --> 00:12:39.620
continuing this this scheme throughout
 

00:12:39.620 --> 00:12:42.579
continuing this this scheme throughout
time you can easily see that our n ends

00:12:42.579 --> 00:12:42.589
time you can easily see that our n ends
 

00:12:42.589 --> 00:12:44.710
time you can easily see that our n ends
have this chain like structure which

00:12:44.710 --> 00:12:44.720
have this chain like structure which
 

00:12:44.720 --> 00:12:46.810
have this chain like structure which
rely really highlights how and why

00:12:46.810 --> 00:12:46.820
rely really highlights how and why
 

00:12:46.820 --> 00:12:49.329
rely really highlights how and why
they're so well suited for processing

00:12:49.329 --> 00:12:49.339
they're so well suited for processing
 

00:12:49.339 --> 00:12:51.940
they're so well suited for processing
sequential data so in this

00:12:51.940 --> 00:12:51.950
sequential data so in this
 

00:12:51.950 --> 00:12:55.810
sequential data so in this
representation we can we can make our

00:12:55.810 --> 00:12:55.820
representation we can we can make our
 

00:12:55.820 --> 00:12:58.389
representation we can we can make our
weight matrices explicit beginning with

00:12:58.389 --> 00:12:58.399
weight matrices explicit beginning with
 

00:12:58.399 --> 00:13:01.449
weight matrices explicit beginning with
the weights that transform the inputs to

00:13:01.449 --> 00:13:01.459
the weights that transform the inputs to
 

00:13:01.459 --> 00:13:05.829
the weights that transform the inputs to
the hidden state transform the previous

00:13:05.829 --> 00:13:05.839
the hidden state transform the previous
 

00:13:05.839 --> 00:13:08.550
the hidden state transform the previous
hidden se to the next hidden safe and

00:13:08.550 --> 00:13:08.560
hidden se to the next hidden safe and
 

00:13:08.560 --> 00:13:11.230
hidden se to the next hidden safe and
finally transform the hidden sate to the

00:13:11.230 --> 00:13:11.240
finally transform the hidden sate to the
 

00:13:11.240 --> 00:13:14.350
finally transform the hidden sate to the
output and it's important once again to

00:13:14.350 --> 00:13:14.360
output and it's important once again to
 

00:13:14.360 --> 00:13:16.360
output and it's important once again to
note that we are using the same weight

00:13:16.360 --> 00:13:16.370
note that we are using the same weight
 

00:13:16.370 --> 00:13:19.780
note that we are using the same weight
matrices at every time step and from

00:13:19.780 --> 00:13:19.790
matrices at every time step and from
 

00:13:19.790 --> 00:13:21.319
matrices at every time step and from
these outputs we can

00:13:21.319 --> 00:13:21.329
these outputs we can
 

00:13:21.329 --> 00:13:23.720
these outputs we can
to loss at each time step and this

00:13:23.720 --> 00:13:23.730
to loss at each time step and this
 

00:13:23.730 --> 00:13:26.030
to loss at each time step and this
completes our what is called our forward

00:13:26.030 --> 00:13:26.040
completes our what is called our forward
 

00:13:26.040 --> 00:13:29.479
completes our what is called our forward
pass through the network and finally to

00:13:29.479 --> 00:13:29.489
pass through the network and finally to
 

00:13:29.489 --> 00:13:32.509
pass through the network and finally to
define the total loss we simply sum the

00:13:32.509 --> 00:13:32.519
define the total loss we simply sum the
 

00:13:32.519 --> 00:13:34.340
define the total loss we simply sum the
losses from all the individual time

00:13:34.340 --> 00:13:34.350
losses from all the individual time
 

00:13:34.350 --> 00:13:37.429
losses from all the individual time
steps and since our total loss consists

00:13:37.429 --> 00:13:37.439
steps and since our total loss consists
 

00:13:37.439 --> 00:13:39.919
steps and since our total loss consists
of these individual contributions over

00:13:39.919 --> 00:13:39.929
of these individual contributions over
 

00:13:39.929 --> 00:13:42.679
of these individual contributions over
time this means that training the

00:13:42.679 --> 00:13:42.689
time this means that training the
 

00:13:42.689 --> 00:13:45.410
time this means that training the
network will also have to involve some

00:13:45.410 --> 00:13:45.420
network will also have to involve some
 

00:13:45.420 --> 00:13:52.999
network will also have to involve some
time component okay so in terms of in

00:13:52.999 --> 00:13:53.009
time component okay so in terms of in
 

00:13:53.009 --> 00:13:55.160
time component okay so in terms of in
terms of actually training our Nets

00:13:55.160 --> 00:13:55.170
terms of actually training our Nets
 

00:13:55.170 --> 00:13:58.100
terms of actually training our Nets
how can we do this and the algorithm

00:13:58.100 --> 00:13:58.110
how can we do this and the algorithm
 

00:13:58.110 --> 00:14:00.559
how can we do this and the algorithm
that's used is an extension of the back

00:14:00.559 --> 00:14:00.569
that's used is an extension of the back
 

00:14:00.569 --> 00:14:03.410
that's used is an extension of the back
propagation idea that alexander

00:14:03.410 --> 00:14:03.420
propagation idea that alexander
 

00:14:03.420 --> 00:14:05.359
propagation idea that alexander
introduced in the first lecture and it's

00:14:05.359 --> 00:14:05.369
introduced in the first lecture and it's
 

00:14:05.369 --> 00:14:08.179
introduced in the first lecture and it's
called back propagation through time so

00:14:08.179 --> 00:14:08.189
called back propagation through time so
 

00:14:08.189 --> 00:14:10.759
called back propagation through time so
to remind you let's let's think back to

00:14:10.759 --> 00:14:10.769
to remind you let's let's think back to
 

00:14:10.769 --> 00:14:14.600
to remind you let's let's think back to
how we trained feed-forward models given

00:14:14.600 --> 00:14:14.610
how we trained feed-forward models given
 

00:14:14.610 --> 00:14:17.179
how we trained feed-forward models given
our given our inputs we first make a

00:14:17.179 --> 00:14:17.189
our given our inputs we first make a
 

00:14:17.189 --> 00:14:19.340
our given our inputs we first make a
forward pass through the network going

00:14:19.340 --> 00:14:19.350
forward pass through the network going
 

00:14:19.350 --> 00:14:21.949
forward pass through the network going
from input to output and then back

00:14:21.949 --> 00:14:21.959
from input to output and then back
 

00:14:21.959 --> 00:14:23.749
from input to output and then back
propagate gradients back through the

00:14:23.749 --> 00:14:23.759
propagate gradients back through the
 

00:14:23.759 --> 00:14:26.960
propagate gradients back through the
network taking the derivative of the

00:14:26.960 --> 00:14:26.970
network taking the derivative of the
 

00:14:26.970 --> 00:14:29.419
network taking the derivative of the
loss with respect to each parameter in

00:14:29.419 --> 00:14:29.429
loss with respect to each parameter in
 

00:14:29.429 --> 00:14:31.489
loss with respect to each parameter in
the network and then tweaking our

00:14:31.489 --> 00:14:31.499
the network and then tweaking our
 

00:14:31.499 --> 00:14:35.829
the network and then tweaking our
parameters in order to minimize the lost

00:14:35.829 --> 00:14:35.839
parameters in order to minimize the lost
 

00:14:35.839 --> 00:14:39.049
parameters in order to minimize the lost
for our n ends our forward pass through

00:14:39.049 --> 00:14:39.059
for our n ends our forward pass through
 

00:14:39.059 --> 00:14:41.119
for our n ends our forward pass through
the network consists of going forward

00:14:41.119 --> 00:14:41.129
the network consists of going forward
 

00:14:41.129 --> 00:14:44.389
the network consists of going forward
across time updating the cell state

00:14:44.389 --> 00:14:44.399
across time updating the cell state
 

00:14:44.399 --> 00:14:46.609
across time updating the cell state
based on the input and the previous

00:14:46.609 --> 00:14:46.619
based on the input and the previous
 

00:14:46.619 --> 00:14:47.169
based on the input and the previous
state

00:14:47.169 --> 00:14:47.179
state
 

00:14:47.179 --> 00:14:49.840
state
generating an output at each time step

00:14:49.840 --> 00:14:49.850
generating an output at each time step
 

00:14:49.850 --> 00:14:52.309
generating an output at each time step
computing a loss I each time set and

00:14:52.309 --> 00:14:52.319
computing a loss I each time set and
 

00:14:52.319 --> 00:14:54.530
computing a loss I each time set and
then finally summing these individual

00:14:54.530 --> 00:14:54.540
then finally summing these individual
 

00:14:54.540 --> 00:14:58.340
then finally summing these individual
losses to get the total loss and what

00:14:58.340 --> 00:14:58.350
losses to get the total loss and what
 

00:14:58.350 --> 00:14:59.840
losses to get the total loss and what
this means is that instead of back

00:14:59.840 --> 00:14:59.850
this means is that instead of back
 

00:14:59.850 --> 00:15:01.759
this means is that instead of back
propagating errors through a single

00:15:01.759 --> 00:15:01.769
propagating errors through a single
 

00:15:01.769 --> 00:15:04.220
propagating errors through a single
feed-forward network at a single time

00:15:04.220 --> 00:15:04.230
feed-forward network at a single time
 

00:15:04.230 --> 00:15:08.119
feed-forward network at a single time
step in our n ends errors are back

00:15:08.119 --> 00:15:08.129
step in our n ends errors are back
 

00:15:08.129 --> 00:15:11.210
step in our n ends errors are back
propagated at each individual time step

00:15:11.210 --> 00:15:11.220
propagated at each individual time step
 

00:15:11.220 --> 00:15:14.840
propagated at each individual time step
and then across time steps all the way

00:15:14.840 --> 00:15:14.850
and then across time steps all the way
 

00:15:14.850 --> 00:15:17.989
and then across time steps all the way
from where we are currently to the very

00:15:17.989 --> 00:15:17.999
from where we are currently to the very
 

00:15:17.999 --> 00:15:21.139
from where we are currently to the very
beginning of the sequence and this is

00:15:21.139 --> 00:15:21.149
beginning of the sequence and this is
 

00:15:21.149 --> 00:15:22.460
beginning of the sequence and this is
the reason why it's called back

00:15:22.460 --> 00:15:22.470
the reason why it's called back
 

00:15:22.470 --> 00:15:25.220
the reason why it's called back
propagation through time because as you

00:15:25.220 --> 00:15:25.230
propagation through time because as you
 

00:15:25.230 --> 00:15:27.319
propagation through time because as you
can see all the errors are flowing back

00:15:27.319 --> 00:15:27.329
can see all the errors are flowing back
 

00:15:27.329 --> 00:15:29.389
can see all the errors are flowing back
in time to the beginning of our data

00:15:29.389 --> 00:15:29.399
in time to the beginning of our data
 

00:15:29.399 --> 00:15:32.960
in time to the beginning of our data
sequence and so if we take a closer look

00:15:32.960 --> 00:15:32.970
sequence and so if we take a closer look
 

00:15:32.970 --> 00:15:34.790
sequence and so if we take a closer look
at how gradients flow

00:15:34.790 --> 00:15:34.800
at how gradients flow
 

00:15:34.800 --> 00:15:37.400
at how gradients flow
across this chain of repeating modules

00:15:37.400 --> 00:15:37.410
across this chain of repeating modules
 

00:15:37.410 --> 00:15:40.549
across this chain of repeating modules
you can see that in between these these

00:15:40.549 --> 00:15:40.559
you can see that in between these these
 

00:15:40.559 --> 00:15:42.559
you can see that in between these these
time steps in doing this back

00:15:42.559 --> 00:15:42.569
time steps in doing this back
 

00:15:42.569 --> 00:15:45.799
time steps in doing this back
propagation we have this factor whh

00:15:45.799 --> 00:15:45.809
propagation we have this factor whh
 

00:15:45.809 --> 00:15:48.739
propagation we have this factor whh
which is a matrix and this means that in

00:15:48.739 --> 00:15:48.749
which is a matrix and this means that in
 

00:15:48.749 --> 00:15:52.639
which is a matrix and this means that in
each step we have to perform a matrix

00:15:52.639 --> 00:15:52.649
each step we have to perform a matrix
 

00:15:52.649 --> 00:15:54.919
each step we have to perform a matrix
multiplication that involves this way

00:15:54.919 --> 00:15:54.929
multiplication that involves this way
 

00:15:54.929 --> 00:16:00.139
multiplication that involves this way
matrix W and furthermore each cell say

00:16:00.139 --> 00:16:00.149
matrix W and furthermore each cell say
 

00:16:00.149 --> 00:16:02.329
matrix W and furthermore each cell say
update results from a nonlinear

00:16:02.329 --> 00:16:02.339
update results from a nonlinear
 

00:16:02.339 --> 00:16:05.090
update results from a nonlinear
activation and what this means is that

00:16:05.090 --> 00:16:05.100
activation and what this means is that
 

00:16:05.100 --> 00:16:08.720
activation and what this means is that
in computing the gradient in an RNN the

00:16:08.720 --> 00:16:08.730
in computing the gradient in an RNN the
 

00:16:08.730 --> 00:16:10.699
in computing the gradient in an RNN the
derivative of the loss with respect to

00:16:10.699 --> 00:16:10.709
derivative of the loss with respect to
 

00:16:10.709 --> 00:16:13.879
derivative of the loss with respect to
our initial state H naught we have to

00:16:13.879 --> 00:16:13.889
our initial state H naught we have to
 

00:16:13.889 --> 00:16:16.069
our initial state H naught we have to
make many matrix multiplications that

00:16:16.069 --> 00:16:16.079
make many matrix multiplications that
 

00:16:16.079 --> 00:16:18.979
make many matrix multiplications that
involve the weight matrix as well as

00:16:18.979 --> 00:16:18.989
involve the weight matrix as well as
 

00:16:18.989 --> 00:16:20.989
involve the weight matrix as well as
repeated use of the derivative of the

00:16:20.989 --> 00:16:20.999
repeated use of the derivative of the
 

00:16:20.999 --> 00:16:23.569
repeated use of the derivative of the
activation function why might this be

00:16:23.569 --> 00:16:23.579
activation function why might this be
 

00:16:23.579 --> 00:16:26.660
activation function why might this be
problematic well if we consider these

00:16:26.660 --> 00:16:26.670
problematic well if we consider these
 

00:16:26.670 --> 00:16:29.179
problematic well if we consider these
multiplication operations if many of

00:16:29.179 --> 00:16:29.189
multiplication operations if many of
 

00:16:29.189 --> 00:16:31.460
multiplication operations if many of
these values are greater than one what

00:16:31.460 --> 00:16:31.470
these values are greater than one what
 

00:16:31.470 --> 00:16:33.259
these values are greater than one what
we can encounter is what's called this

00:16:33.259 --> 00:16:33.269
we can encounter is what's called this
 

00:16:33.269 --> 00:16:35.960
we can encounter is what's called this
exploding gradient problem where our

00:16:35.960 --> 00:16:35.970
exploding gradient problem where our
 

00:16:35.970 --> 00:16:38.150
exploding gradient problem where our
gradients become extremely large and we

00:16:38.150 --> 00:16:38.160
gradients become extremely large and we
 

00:16:38.160 --> 00:16:41.600
gradients become extremely large and we
can't do any optimization and to combat

00:16:41.600 --> 00:16:41.610
can't do any optimization and to combat
 

00:16:41.610 --> 00:16:43.309
can't do any optimization and to combat
this one thing that's done is in

00:16:43.309 --> 00:16:43.319
this one thing that's done is in
 

00:16:43.319 --> 00:16:45.350
this one thing that's done is in
practice is called gradient clipping

00:16:45.350 --> 00:16:45.360
practice is called gradient clipping
 

00:16:45.360 --> 00:16:47.749
practice is called gradient clipping
which basically means you scale back

00:16:47.749 --> 00:16:47.759
which basically means you scale back
 

00:16:47.759 --> 00:16:49.669
which basically means you scale back
your gradients when they become too

00:16:49.669 --> 00:16:49.679
your gradients when they become too
 

00:16:49.679 --> 00:16:51.650
your gradients when they become too
large and this is a really good

00:16:51.650 --> 00:16:51.660
large and this is a really good
 

00:16:51.660 --> 00:16:53.989
large and this is a really good
practical option especially when you

00:16:53.989 --> 00:16:53.999
practical option especially when you
 

00:16:53.999 --> 00:16:55.249
practical option especially when you
have a network that's not too

00:16:55.249 --> 00:16:55.259
have a network that's not too
 

00:16:55.259 --> 00:16:55.939
have a network that's not too
complicated

00:16:55.939 --> 00:16:55.949
complicated
 

00:16:55.949 --> 00:16:58.840
complicated
with and doesn't have many parameters on

00:16:58.840 --> 00:16:58.850
with and doesn't have many parameters on
 

00:16:58.850 --> 00:17:01.460
with and doesn't have many parameters on
the flip side we can also have the

00:17:01.460 --> 00:17:01.470
the flip side we can also have the
 

00:17:01.470 --> 00:17:02.359
the flip side we can also have the
opposite problem

00:17:02.359 --> 00:17:02.369
opposite problem
 

00:17:02.369 --> 00:17:05.049
opposite problem
where if our matrix values are too small

00:17:05.049 --> 00:17:05.059
where if our matrix values are too small
 

00:17:05.059 --> 00:17:07.610
where if our matrix values are too small
we can encounter what's called the

00:17:07.610 --> 00:17:07.620
we can encounter what's called the
 

00:17:07.620 --> 00:17:09.679
we can encounter what's called the
vanishing gradient problem and it's

00:17:09.679 --> 00:17:09.689
vanishing gradient problem and it's
 

00:17:09.689 --> 00:17:11.960
vanishing gradient problem and it's
really the motivating factor behind the

00:17:11.960 --> 00:17:11.970
really the motivating factor behind the
 

00:17:11.970 --> 00:17:14.980
really the motivating factor behind the
most widely used RNN architectures and

00:17:14.980 --> 00:17:14.990
most widely used RNN architectures and
 

00:17:14.990 --> 00:17:17.750
most widely used RNN architectures and
today we're gonna address three ways in

00:17:17.750 --> 00:17:17.760
today we're gonna address three ways in
 

00:17:17.760 --> 00:17:20.210
today we're gonna address three ways in
which we can alleviate the vanishing

00:17:20.210 --> 00:17:20.220
which we can alleviate the vanishing
 

00:17:20.220 --> 00:17:22.610
which we can alleviate the vanishing
gradient problem by changing the

00:17:22.610 --> 00:17:22.620
gradient problem by changing the
 

00:17:22.620 --> 00:17:25.220
gradient problem by changing the
activation function that's used being

00:17:25.220 --> 00:17:25.230
activation function that's used being
 

00:17:25.230 --> 00:17:26.899
activation function that's used being
clever about how we initialize the

00:17:26.899 --> 00:17:26.909
clever about how we initialize the
 

00:17:26.909 --> 00:17:29.480
clever about how we initialize the
weights in our network and finally how

00:17:29.480 --> 00:17:29.490
weights in our network and finally how
 

00:17:29.490 --> 00:17:32.450
weights in our network and finally how
we can fundamentally change the RNN

00:17:32.450 --> 00:17:32.460
we can fundamentally change the RNN
 

00:17:32.460 --> 00:17:35.240
we can fundamentally change the RNN
architecture to combat this and so

00:17:35.240 --> 00:17:35.250
architecture to combat this and so
 

00:17:35.250 --> 00:17:37.760
architecture to combat this and so
before we go into that let's take a step

00:17:37.760 --> 00:17:37.770
before we go into that let's take a step
 

00:17:37.770 --> 00:17:42.110
before we go into that let's take a step
back and try to establish some more

00:17:42.110 --> 00:17:42.120
back and try to establish some more
 

00:17:42.120 --> 00:17:44.930
back and try to establish some more
intuition as to why vanishing gradients

00:17:44.930 --> 00:17:44.940
intuition as to why vanishing gradients
 

00:17:44.940 --> 00:17:47.990
intuition as to why vanishing gradients
are such a big problem

00:17:47.990 --> 00:17:48.000
are such a big problem
 

00:17:48.000 --> 00:17:50.390
are such a big problem
so imagine you have a number right and

00:17:50.390 --> 00:17:50.400
so imagine you have a number right and
 

00:17:50.400 --> 00:17:52.370
so imagine you have a number right and
you keep multiplying that number by

00:17:52.370 --> 00:17:52.380
you keep multiplying that number by
 

00:17:52.380 --> 00:17:55.399
you keep multiplying that number by
something in between zero and one that

00:17:55.399 --> 00:17:55.409
something in between zero and one that
 

00:17:55.409 --> 00:17:57.500
something in between zero and one that
number is going to keep shrinking and

00:17:57.500 --> 00:17:57.510
number is going to keep shrinking and
 

00:17:57.510 --> 00:17:59.270
number is going to keep shrinking and
shrinking and eventually it's going to

00:17:59.270 --> 00:17:59.280
shrinking and eventually it's going to
 

00:17:59.280 --> 00:18:02.480
shrinking and eventually it's going to
vanish when this happens two gradients

00:18:02.480 --> 00:18:02.490
vanish when this happens two gradients
 

00:18:02.490 --> 00:18:04.730
vanish when this happens two gradients
this means it's going to be harder and

00:18:04.730 --> 00:18:04.740
this means it's going to be harder and
 

00:18:04.740 --> 00:18:07.549
this means it's going to be harder and
harder to propagate errors further back

00:18:07.549 --> 00:18:07.559
harder to propagate errors further back
 

00:18:07.559 --> 00:18:09.890
harder to propagate errors further back
into the past because the gradients are

00:18:09.890 --> 00:18:09.900
into the past because the gradients are
 

00:18:09.900 --> 00:18:12.310
into the past because the gradients are
going to become smaller and smaller and

00:18:12.310 --> 00:18:12.320
going to become smaller and smaller and
 

00:18:12.320 --> 00:18:15.320
going to become smaller and smaller and
this means that during training will end

00:18:15.320 --> 00:18:15.330
this means that during training will end
 

00:18:15.330 --> 00:18:18.470
this means that during training will end
up biasing our network to capture short

00:18:18.470 --> 00:18:18.480
up biasing our network to capture short
 

00:18:18.480 --> 00:18:21.409
up biasing our network to capture short
term dependencies which may not always

00:18:21.409 --> 00:18:21.419
term dependencies which may not always
 

00:18:21.419 --> 00:18:23.990
term dependencies which may not always
be a problem sometimes we only need to

00:18:23.990 --> 00:18:24.000
be a problem sometimes we only need to
 

00:18:24.000 --> 00:18:26.659
be a problem sometimes we only need to
consider very recent information to

00:18:26.659 --> 00:18:26.669
consider very recent information to
 

00:18:26.669 --> 00:18:30.740
consider very recent information to
perform our tasks of interest so to make

00:18:30.740 --> 00:18:30.750
perform our tasks of interest so to make
 

00:18:30.750 --> 00:18:32.630
perform our tasks of interest so to make
this concrete right let's go back to our

00:18:32.630 --> 00:18:32.640
this concrete right let's go back to our
 

00:18:32.640 --> 00:18:33.860
this concrete right let's go back to our
example from the beginning of the

00:18:33.860 --> 00:18:33.870
example from the beginning of the
 

00:18:33.870 --> 00:18:36.529
example from the beginning of the
lecture a language model we're trying to

00:18:36.529 --> 00:18:36.539
lecture a language model we're trying to
 

00:18:36.539 --> 00:18:40.220
lecture a language model we're trying to
predict the next word in a phrase so in

00:18:40.220 --> 00:18:40.230
predict the next word in a phrase so in
 

00:18:40.230 --> 00:18:42.140
predict the next word in a phrase so in
this case if we're trying to predict the

00:18:42.140 --> 00:18:42.150
this case if we're trying to predict the
 

00:18:42.150 --> 00:18:44.120
this case if we're trying to predict the
last word in the phrase the clouds are

00:18:44.120 --> 00:18:44.130
last word in the phrase the clouds are
 

00:18:44.130 --> 00:18:46.520
last word in the phrase the clouds are
in the blank it's pretty obvious what

00:18:46.520 --> 00:18:46.530
in the blank it's pretty obvious what
 

00:18:46.530 --> 00:18:49.460
in the blank it's pretty obvious what
the next word is going to be and there's

00:18:49.460 --> 00:18:49.470
the next word is going to be and there's
 

00:18:49.470 --> 00:18:51.260
the next word is going to be and there's
not much of a gap between the relevant

00:18:51.260 --> 00:18:51.270
not much of a gap between the relevant
 

00:18:51.270 --> 00:18:53.600
not much of a gap between the relevant
information like the word cloud and the

00:18:53.600 --> 00:18:53.610
information like the word cloud and the
 

00:18:53.610 --> 00:18:55.460
information like the word cloud and the
place where the prediction is needed and

00:18:55.460 --> 00:18:55.470
place where the prediction is needed and
 

00:18:55.470 --> 00:18:58.430
place where the prediction is needed and
so an a standard RNN can use the past

00:18:58.430 --> 00:18:58.440
so an a standard RNN can use the past
 

00:18:58.440 --> 00:19:02.630
so an a standard RNN can use the past
information to make the prediction but

00:19:02.630 --> 00:19:02.640
information to make the prediction but
 

00:19:02.640 --> 00:19:04.520
information to make the prediction but
there can be other cases where more

00:19:04.520 --> 00:19:04.530
there can be other cases where more
 

00:19:04.530 --> 00:19:06.649
there can be other cases where more
context is necessary like in this

00:19:06.649 --> 00:19:06.659
context is necessary like in this
 

00:19:06.659 --> 00:19:10.070
context is necessary like in this
example more recent information suggests

00:19:10.070 --> 00:19:10.080
example more recent information suggests
 

00:19:10.080 --> 00:19:11.899
example more recent information suggests
that the next word is most likely the

00:19:11.899 --> 00:19:11.909
that the next word is most likely the
 

00:19:11.909 --> 00:19:15.169
that the next word is most likely the
name of a language but to identify which

00:19:15.169 --> 00:19:15.179
name of a language but to identify which
 

00:19:15.179 --> 00:19:16.669
name of a language but to identify which
language we need information from

00:19:16.669 --> 00:19:16.679
language we need information from
 

00:19:16.679 --> 00:19:19.210
language we need information from
further back the context of France and

00:19:19.210 --> 00:19:19.220
further back the context of France and
 

00:19:19.220 --> 00:19:22.010
further back the context of France and
in many cases the gap between what's

00:19:22.010 --> 00:19:22.020
in many cases the gap between what's
 

00:19:22.020 --> 00:19:24.140
in many cases the gap between what's
relevant and the point where that

00:19:24.140 --> 00:19:24.150
relevant and the point where that
 

00:19:24.150 --> 00:19:25.970
relevant and the point where that
information is needed can become really

00:19:25.970 --> 00:19:25.980
information is needed can become really
 

00:19:25.980 --> 00:19:28.730
information is needed can become really
really large and as that grid gap grows

00:19:28.730 --> 00:19:28.740
really large and as that grid gap grows
 

00:19:28.740 --> 00:19:30.200
really large and as that grid gap grows
standard rnns

00:19:30.200 --> 00:19:30.210
standard rnns
 

00:19:30.210 --> 00:19:32.600
standard rnns
become increasingly unable to connect

00:19:32.600 --> 00:19:32.610
become increasingly unable to connect
 

00:19:32.610 --> 00:19:34.490
become increasingly unable to connect
the information and that's all because

00:19:34.490 --> 00:19:34.500
the information and that's all because
 

00:19:34.500 --> 00:19:37.610
the information and that's all because
of the vanishing gradient problem so how

00:19:37.610 --> 00:19:37.620
of the vanishing gradient problem so how
 

00:19:37.620 --> 00:19:40.549
of the vanishing gradient problem so how
can we alleviate this the first trick is

00:19:40.549 --> 00:19:40.559
can we alleviate this the first trick is
 

00:19:40.559 --> 00:19:42.890
can we alleviate this the first trick is
pretty simple we can change the

00:19:42.890 --> 00:19:42.900
pretty simple we can change the
 

00:19:42.900 --> 00:19:45.640
pretty simple we can change the
activation function the network uses and

00:19:45.640 --> 00:19:45.650
activation function the network uses and
 

00:19:45.650 --> 00:19:48.770
activation function the network uses and
specifically both the 10h and sigmoid

00:19:48.770 --> 00:19:48.780
specifically both the 10h and sigmoid
 

00:19:48.780 --> 00:19:51.200
specifically both the 10h and sigmoid
activation functions have derivatives

00:19:51.200 --> 00:19:51.210
activation functions have derivatives
 

00:19:51.210 --> 00:19:53.659
activation functions have derivatives
less than one pretty much everywhere

00:19:53.659 --> 00:19:53.669
less than one pretty much everywhere
 

00:19:53.669 --> 00:19:57.169
less than one pretty much everywhere
right in contrast if we use a rel ooh

00:19:57.169 --> 00:19:57.179
right in contrast if we use a rel ooh
 

00:19:57.179 --> 00:20:00.140
right in contrast if we use a rel ooh
activation function the derivative is

00:20:00.140 --> 00:20:00.150
activation function the derivative is
 

00:20:00.150 --> 00:20:02.540
activation function the derivative is
one four four whenever X is greater than

00:20:02.540 --> 00:20:02.550
one four four whenever X is greater than
 

00:20:02.550 --> 00:20:05.480
one four four whenever X is greater than
zero and so this helps prevent the value

00:20:05.480 --> 00:20:05.490
zero and so this helps prevent the value
 

00:20:05.490 --> 00:20:06.950
zero and so this helps prevent the value
of the derivative from shrinking our

00:20:06.950 --> 00:20:06.960
of the derivative from shrinking our
 

00:20:06.960 --> 00:20:09.680
of the derivative from shrinking our
gradients but it's only true for when

00:20:09.680 --> 00:20:09.690
gradients but it's only true for when
 

00:20:09.690 --> 00:20:14.420
gradients but it's only true for when
for when X is greater than zero another

00:20:14.420 --> 00:20:14.430
for when X is greater than zero another
 

00:20:14.430 --> 00:20:16.370
for when X is greater than zero another
trick is to be smart in terms of how we

00:20:16.370 --> 00:20:16.380
trick is to be smart in terms of how we
 

00:20:16.380 --> 00:20:18.500
trick is to be smart in terms of how we
in the initialize parameters in our

00:20:18.500 --> 00:20:18.510
in the initialize parameters in our
 

00:20:18.510 --> 00:20:22.070
in the initialize parameters in our
network by initialing our weights to the

00:20:22.070 --> 00:20:22.080
network by initialing our weights to the
 

00:20:22.080 --> 00:20:24.770
network by initialing our weights to the
identity matrix we can help prevent them

00:20:24.770 --> 00:20:24.780
identity matrix we can help prevent them
 

00:20:24.780 --> 00:20:27.110
identity matrix we can help prevent them
from shrinking to zero too rapidly

00:20:27.110 --> 00:20:27.120
from shrinking to zero too rapidly
 

00:20:27.120 --> 00:20:31.220
from shrinking to zero too rapidly
during back back propagation the final

00:20:31.220 --> 00:20:31.230
during back back propagation the final
 

00:20:31.230 --> 00:20:34.610
during back back propagation the final
and most robust solution is to use a

00:20:34.610 --> 00:20:34.620
and most robust solution is to use a
 

00:20:34.620 --> 00:20:38.000
and most robust solution is to use a
more complex type of recurrent unit that

00:20:38.000 --> 00:20:38.010
more complex type of recurrent unit that
 

00:20:38.010 --> 00:20:40.340
more complex type of recurrent unit that
can more effectively track long term

00:20:40.340 --> 00:20:40.350
can more effectively track long term
 

00:20:40.350 --> 00:20:43.340
can more effectively track long term
dependencies by controlling what

00:20:43.340 --> 00:20:43.350
dependencies by controlling what
 

00:20:43.350 --> 00:20:45.830
dependencies by controlling what
information is passed through and what's

00:20:45.830 --> 00:20:45.840
information is passed through and what's
 

00:20:45.840 --> 00:20:47.560
information is passed through and what's
used to update the cell state

00:20:47.560 --> 00:20:47.570
used to update the cell state
 

00:20:47.570 --> 00:20:49.610
used to update the cell state
specifically we'll use what we call

00:20:49.610 --> 00:20:49.620
specifically we'll use what we call
 

00:20:49.620 --> 00:20:51.830
specifically we'll use what we call
gated cells and there are many types of

00:20:51.830 --> 00:20:51.840
gated cells and there are many types of
 

00:20:51.840 --> 00:20:54.470
gated cells and there are many types of
these gated cells that exist and today

00:20:54.470 --> 00:20:54.480
these gated cells that exist and today
 

00:20:54.480 --> 00:20:56.630
these gated cells that exist and today
we'll focus on one type of gated cell

00:20:56.630 --> 00:20:56.640
we'll focus on one type of gated cell
 

00:20:56.640 --> 00:21:00.020
we'll focus on one type of gated cell
called a long short-term memory network

00:21:00.020 --> 00:21:00.030
called a long short-term memory network
 

00:21:00.030 --> 00:21:02.960
called a long short-term memory network
or LS TMS for short which are really

00:21:02.960 --> 00:21:02.970
or LS TMS for short which are really
 

00:21:02.970 --> 00:21:05.480
or LS TMS for short which are really
good at learning long-term dependencies

00:21:05.480 --> 00:21:05.490
good at learning long-term dependencies
 

00:21:05.490 --> 00:21:08.320
good at learning long-term dependencies
and overcoming this vanishing gradient

00:21:08.320 --> 00:21:08.330
and overcoming this vanishing gradient
 

00:21:08.330 --> 00:21:11.750
and overcoming this vanishing gradient
problem and Ellis hams are basically the

00:21:11.750 --> 00:21:11.760
problem and Ellis hams are basically the
 

00:21:11.760 --> 00:21:14.690
problem and Ellis hams are basically the
gold standard when it comes to building

00:21:14.690 --> 00:21:14.700
gold standard when it comes to building
 

00:21:14.700 --> 00:21:16.880
gold standard when it comes to building
RN ends in practice and they're very

00:21:16.880 --> 00:21:16.890
RN ends in practice and they're very
 

00:21:16.890 --> 00:21:19.190
RN ends in practice and they're very
very widely used by the deep learning

00:21:19.190 --> 00:21:19.200
very widely used by the deep learning
 

00:21:19.200 --> 00:21:23.150
very widely used by the deep learning
community so to understand what makes LS

00:21:23.150 --> 00:21:23.160
community so to understand what makes LS
 

00:21:23.160 --> 00:21:26.150
community so to understand what makes LS
am special right let's think back to the

00:21:26.150 --> 00:21:26.160
am special right let's think back to the
 

00:21:26.160 --> 00:21:28.160
am special right let's think back to the
general structure of an RN n all

00:21:28.160 --> 00:21:28.170
general structure of an RN n all
 

00:21:28.170 --> 00:21:31.010
general structure of an RN n all
recurrent neural networks have this form

00:21:31.010 --> 00:21:31.020
recurrent neural networks have this form
 

00:21:31.020 --> 00:21:32.930
recurrent neural networks have this form
of a series of repeating modules right

00:21:32.930 --> 00:21:32.940
of a series of repeating modules right
 

00:21:32.940 --> 00:21:35.360
of a series of repeating modules right
the RN n being unrolled across time and

00:21:35.360 --> 00:21:35.370
the RN n being unrolled across time and
 

00:21:35.370 --> 00:21:38.600
the RN n being unrolled across time and
in a standard RN n the repeating module

00:21:38.600 --> 00:21:38.610
in a standard RN n the repeating module
 

00:21:38.610 --> 00:21:41.900
in a standard RN n the repeating module
contains one computation node in this

00:21:41.900 --> 00:21:41.910
contains one computation node in this
 

00:21:41.910 --> 00:21:47.540
contains one computation node in this
case it's a 10 8 10 H layer LS PMS also

00:21:47.540 --> 00:21:47.550
case it's a 10 8 10 H layer LS PMS also
 

00:21:47.550 --> 00:21:49.730
case it's a 10 8 10 H layer LS PMS also
have this chain like structure but the

00:21:49.730 --> 00:21:49.740
have this chain like structure but the
 

00:21:49.740 --> 00:21:51.290
have this chain like structure but the
repeating module is slightly more

00:21:51.290 --> 00:21:51.300
repeating module is slightly more
 

00:21:51.300 --> 00:21:54.050
repeating module is slightly more
complex and don't get too frightened

00:21:54.050 --> 00:21:54.060
complex and don't get too frightened
 

00:21:54.060 --> 00:21:56.300
complex and don't get too frightened
hopefully by you know what these flow

00:21:56.300 --> 00:21:56.310
hopefully by you know what these flow
 

00:21:56.310 --> 00:21:58.730
hopefully by you know what these flow
diagrams mean we'll walk through except

00:21:58.730 --> 00:21:58.740
diagrams mean we'll walk through except
 

00:21:58.740 --> 00:22:02.360
diagrams mean we'll walk through except
bicep but the key idea here is that the

00:22:02.360 --> 00:22:02.370
bicep but the key idea here is that the
 

00:22:02.370 --> 00:22:05.000
bicep but the key idea here is that the
repeating unit in an LS TM contains

00:22:05.000 --> 00:22:05.010
repeating unit in an LS TM contains
 

00:22:05.010 --> 00:22:07.130
repeating unit in an LS TM contains
these different interacting layers that

00:22:07.130 --> 00:22:07.140
these different interacting layers that
 

00:22:07.140 --> 00:22:10.030
these different interacting layers that
control the flow of information

00:22:10.030 --> 00:22:10.040
control the flow of information
 

00:22:10.040 --> 00:22:13.620
control the flow of information
the first key idea behind Alice hands is

00:22:13.620 --> 00:22:13.630
the first key idea behind Alice hands is
 

00:22:13.630 --> 00:22:16.440
the first key idea behind Alice hands is
they maintain an internal cell state

00:22:16.440 --> 00:22:16.450
they maintain an internal cell state
 

00:22:16.450 --> 00:22:19.740
they maintain an internal cell state
which will denote C of T in addition to

00:22:19.740 --> 00:22:19.750
which will denote C of T in addition to
 

00:22:19.750 --> 00:22:23.549
which will denote C of T in addition to
the standard are n n say H of T and this

00:22:23.549 --> 00:22:23.559
the standard are n n say H of T and this
 

00:22:23.559 --> 00:22:26.970
the standard are n n say H of T and this
cell state runs through throughout the

00:22:26.970 --> 00:22:26.980
cell state runs through throughout the
 

00:22:26.980 --> 00:22:29.340
cell state runs through throughout the
chain of repeating modules and as you

00:22:29.340 --> 00:22:29.350
chain of repeating modules and as you
 

00:22:29.350 --> 00:22:31.020
chain of repeating modules and as you
can see there are only a couple of

00:22:31.020 --> 00:22:31.030
can see there are only a couple of
 

00:22:31.030 --> 00:22:33.690
can see there are only a couple of
simple linear interactions this is a

00:22:33.690 --> 00:22:33.700
simple linear interactions this is a
 

00:22:33.700 --> 00:22:36.390
simple linear interactions this is a
point wise multiplication and this is

00:22:36.390 --> 00:22:36.400
point wise multiplication and this is
 

00:22:36.400 --> 00:22:40.440
point wise multiplication and this is
addition that update the value of C of T

00:22:40.440 --> 00:22:40.450
addition that update the value of C of T
 

00:22:40.450 --> 00:22:42.930
addition that update the value of C of T
and this means that it's really easy for

00:22:42.930 --> 00:22:42.940
and this means that it's really easy for
 

00:22:42.940 --> 00:22:45.600
and this means that it's really easy for
information to flow along relatively

00:22:45.600 --> 00:22:45.610
information to flow along relatively
 

00:22:45.610 --> 00:22:48.810
information to flow along relatively
unchanged the second key idea that L

00:22:48.810 --> 00:22:48.820
unchanged the second key idea that L
 

00:22:48.820 --> 00:22:52.590
unchanged the second key idea that L
stems use is that they use these

00:22:52.590 --> 00:22:52.600
stems use is that they use these
 

00:22:52.600 --> 00:22:55.279
stems use is that they use these
structures called gates to add or remove

00:22:55.279 --> 00:22:55.289
structures called gates to add or remove
 

00:22:55.289 --> 00:22:58.440
structures called gates to add or remove
information to the cell state and gates

00:22:58.440 --> 00:22:58.450
information to the cell state and gates
 

00:22:58.450 --> 00:23:01.350
information to the cell state and gates
consists of a sigmoid neural net layer

00:23:01.350 --> 00:23:01.360
consists of a sigmoid neural net layer
 

00:23:01.360 --> 00:23:04.220
consists of a sigmoid neural net layer
followed by a point wise multiplication

00:23:04.220 --> 00:23:04.230
followed by a point wise multiplication
 

00:23:04.230 --> 00:23:06.899
followed by a point wise multiplication
so let's take a moment to think about

00:23:06.899 --> 00:23:06.909
so let's take a moment to think about
 

00:23:06.909 --> 00:23:09.899
so let's take a moment to think about
what these gates are doing this sigmoid

00:23:09.899 --> 00:23:09.909
what these gates are doing this sigmoid
 

00:23:09.909 --> 00:23:11.820
what these gates are doing this sigmoid
function is special because it's forcing

00:23:11.820 --> 00:23:11.830
function is special because it's forcing
 

00:23:11.830 --> 00:23:14.789
function is special because it's forcing
the input to the gate to be between 0

00:23:14.789 --> 00:23:14.799
the input to the gate to be between 0
 

00:23:14.799 --> 00:23:17.700
the input to the gate to be between 0
and 1 and intuitively you can think of

00:23:17.700 --> 00:23:17.710
and 1 and intuitively you can think of
 

00:23:17.710 --> 00:23:20.370
and 1 and intuitively you can think of
this as capturing how much of the input

00:23:20.370 --> 00:23:20.380
this as capturing how much of the input
 

00:23:20.380 --> 00:23:22.470
this as capturing how much of the input
should be passed through the gate if

00:23:22.470 --> 00:23:22.480
should be passed through the gate if
 

00:23:22.480 --> 00:23:25.409
should be passed through the gate if
it's 0 don't pass any of that

00:23:25.409 --> 00:23:25.419
it's 0 don't pass any of that
 

00:23:25.419 --> 00:23:27.570
it's 0 don't pass any of that
information through the gate if it's 1

00:23:27.570 --> 00:23:27.580
information through the gate if it's 1
 

00:23:27.580 --> 00:23:29.580
information through the gate if it's 1
pass all the information through the

00:23:29.580 --> 00:23:29.590
pass all the information through the
 

00:23:29.590 --> 00:23:33.560
pass all the information through the
gate and so this regulates the flow of

00:23:33.560 --> 00:23:33.570
gate and so this regulates the flow of
 

00:23:33.570 --> 00:23:37.770
gate and so this regulates the flow of
information through the LS TM so now

00:23:37.770 --> 00:23:37.780
information through the LS TM so now
 

00:23:37.780 --> 00:23:39.299
information through the LS TM so now
you're probably wondering ok these lines

00:23:39.299 --> 00:23:39.309
you're probably wondering ok these lines
 

00:23:39.309 --> 00:23:41.669
you're probably wondering ok these lines
look really complicated what's how do

00:23:41.669 --> 00:23:41.679
look really complicated what's how do
 

00:23:41.679 --> 00:23:44.310
look really complicated what's how do
these LS Siam's actually work thinking

00:23:44.310 --> 00:23:44.320
these LS Siam's actually work thinking
 

00:23:44.320 --> 00:23:47.700
these LS Siam's actually work thinking
of the lsdm operations at a high level

00:23:47.700 --> 00:23:47.710
of the lsdm operations at a high level
 

00:23:47.710 --> 00:23:52.110
of the lsdm operations at a high level
it boils down to three key steps the

00:23:52.110 --> 00:23:52.120
it boils down to three key steps the
 

00:23:52.120 --> 00:23:54.450
it boils down to three key steps the
first step in the LS TM is to decide

00:23:54.450 --> 00:23:54.460
first step in the LS TM is to decide
 

00:23:54.460 --> 00:23:56.940
first step in the LS TM is to decide
what information is going to be thrown

00:23:56.940 --> 00:23:56.950
what information is going to be thrown
 

00:23:56.950 --> 00:24:00.090
what information is going to be thrown
away from the prior cell state forget

00:24:00.090 --> 00:24:00.100
away from the prior cell state forget
 

00:24:00.100 --> 00:24:03.390
away from the prior cell state forget
irrelevant history right the next step

00:24:03.390 --> 00:24:03.400
irrelevant history right the next step
 

00:24:03.400 --> 00:24:05.909
irrelevant history right the next step
is to take both the prior information as

00:24:05.909 --> 00:24:05.919
is to take both the prior information as
 

00:24:05.919 --> 00:24:09.060
is to take both the prior information as
well as the current input process this

00:24:09.060 --> 00:24:09.070
well as the current input process this
 

00:24:09.070 --> 00:24:10.950
well as the current input process this
information in some way and then

00:24:10.950 --> 00:24:10.960
information in some way and then
 

00:24:10.960 --> 00:24:13.909
information in some way and then
selectively update the cell state and

00:24:13.909 --> 00:24:13.919
selectively update the cell state and
 

00:24:13.919 --> 00:24:16.830
selectively update the cell state and
our final step is to return an output

00:24:16.830 --> 00:24:16.840
our final step is to return an output
 

00:24:16.840 --> 00:24:20.010
our final step is to return an output
and for this Alice hams are going to use

00:24:20.010 --> 00:24:20.020
and for this Alice hams are going to use
 

00:24:20.020 --> 00:24:23.370
and for this Alice hams are going to use
an output gate to return a transformed

00:24:23.370 --> 00:24:23.380
an output gate to return a transformed
 

00:24:23.380 --> 00:24:27.000
an output gate to return a transformed
version of the cell state so now that we

00:24:27.000 --> 00:24:27.010
version of the cell state so now that we
 

00:24:27.010 --> 00:24:27.270
version of the cell state so now that we
have

00:24:27.270 --> 00:24:27.280
have
 

00:24:27.280 --> 00:24:29.250
have
a sense of these three key lsdm

00:24:29.250 --> 00:24:29.260
a sense of these three key lsdm
 

00:24:29.260 --> 00:24:33.450
a sense of these three key lsdm
operations forget update output let's

00:24:33.450 --> 00:24:33.460
operations forget update output let's
 

00:24:33.460 --> 00:24:36.150
operations forget update output let's
walk through each step by step to get a

00:24:36.150 --> 00:24:36.160
walk through each step by step to get a
 

00:24:36.160 --> 00:24:37.980
walk through each step by step to get a
concrete understanding of how these

00:24:37.980 --> 00:24:37.990
concrete understanding of how these
 

00:24:37.990 --> 00:24:40.590
concrete understanding of how these
computations work and even though we're

00:24:40.590 --> 00:24:40.600
computations work and even though we're
 

00:24:40.600 --> 00:24:42.240
computations work and even though we're
gonna walk through this I really want

00:24:42.240 --> 00:24:42.250
gonna walk through this I really want
 

00:24:42.250 --> 00:24:44.670
gonna walk through this I really want
you to keep in mind the high-level

00:24:44.670 --> 00:24:44.680
you to keep in mind the high-level
 

00:24:44.680 --> 00:24:46.680
you to keep in mind the high-level
concepts of each operation because

00:24:46.680 --> 00:24:46.690
concepts of each operation because
 

00:24:46.690 --> 00:24:48.650
concepts of each operation because
that's what's important in terms of

00:24:48.650 --> 00:24:48.660
that's what's important in terms of
 

00:24:48.660 --> 00:24:51.540
that's what's important in terms of
establishing the intuition behind how

00:24:51.540 --> 00:24:51.550
establishing the intuition behind how
 

00:24:51.550 --> 00:24:55.950
establishing the intuition behind how
Alice teams work and we'll go again go

00:24:55.950 --> 00:24:55.960
Alice teams work and we'll go again go
 

00:24:55.960 --> 00:24:58.260
Alice teams work and we'll go again go
back to our language model example that

00:24:58.260 --> 00:24:58.270
back to our language model example that
 

00:24:58.270 --> 00:25:00.350
back to our language model example that
we've been using throughout this lecture

00:25:00.350 --> 00:25:00.360
we've been using throughout this lecture
 

00:25:00.360 --> 00:25:02.610
we've been using throughout this lecture
where we're trying to predict the next

00:25:02.610 --> 00:25:02.620
where we're trying to predict the next
 

00:25:02.620 --> 00:25:06.390
where we're trying to predict the next
word in a sequence so our first task is

00:25:06.390 --> 00:25:06.400
word in a sequence so our first task is
 

00:25:06.400 --> 00:25:09.300
word in a sequence so our first task is
to identify what past information is

00:25:09.300 --> 00:25:09.310
to identify what past information is
 

00:25:09.310 --> 00:25:11.910
to identify what past information is
relevant and what's irrelevant and we

00:25:11.910 --> 00:25:11.920
relevant and what's irrelevant and we
 

00:25:11.920 --> 00:25:14.190
relevant and what's irrelevant and we
achieve this using a sigmoid layer

00:25:14.190 --> 00:25:14.200
achieve this using a sigmoid layer
 

00:25:14.200 --> 00:25:17.870
achieve this using a sigmoid layer
called the forget gate f of T right and

00:25:17.870 --> 00:25:17.880
called the forget gate f of T right and
 

00:25:17.880 --> 00:25:21.900
called the forget gate f of T right and
F of T is parametrized by a sets a set

00:25:21.900 --> 00:25:21.910
F of T is parametrized by a sets a set
 

00:25:21.910 --> 00:25:24.060
F of T is parametrized by a sets a set
of weights and biases just like any

00:25:24.060 --> 00:25:24.070
of weights and biases just like any
 

00:25:24.070 --> 00:25:27.240
of weights and biases just like any
neural network layer and this layer

00:25:27.240 --> 00:25:27.250
neural network layer and this layer
 

00:25:27.250 --> 00:25:29.400
neural network layer and this layer
looks at the previous previous

00:25:29.400 --> 00:25:29.410
looks at the previous previous
 

00:25:29.410 --> 00:25:32.820
looks at the previous previous
information H of T minus one as well as

00:25:32.820 --> 00:25:32.830
information H of T minus one as well as
 

00:25:32.830 --> 00:25:35.640
information H of T minus one as well as
the input X of T and then outputs a

00:25:35.640 --> 00:25:35.650
the input X of T and then outputs a
 

00:25:35.650 --> 00:25:37.890
the input X of T and then outputs a
number between zero and one between

00:25:37.890 --> 00:25:37.900
number between zero and one between
 

00:25:37.900 --> 00:25:39.870
number between zero and one between
completely forgetting that information

00:25:39.870 --> 00:25:39.880
completely forgetting that information
 

00:25:39.880 --> 00:25:42.060
completely forgetting that information
and completely keeping that information

00:25:42.060 --> 00:25:42.070
and completely keeping that information
 

00:25:42.070 --> 00:25:44.790
and completely keeping that information
and then passes it along this decision

00:25:44.790 --> 00:25:44.800
and then passes it along this decision
 

00:25:44.800 --> 00:25:48.120
and then passes it along this decision
so in our language model example the

00:25:48.120 --> 00:25:48.130
so in our language model example the
 

00:25:48.130 --> 00:25:49.950
so in our language model example the
cell state might have included some

00:25:49.950 --> 00:25:49.960
cell state might have included some
 

00:25:49.960 --> 00:25:52.800
cell state might have included some
information about the gender pronoun of

00:25:52.800 --> 00:25:52.810
information about the gender pronoun of
 

00:25:52.810 --> 00:25:55.470
information about the gender pronoun of
a subject in a sentence for example so

00:25:55.470 --> 00:25:55.480
a subject in a sentence for example so
 

00:25:55.480 --> 00:25:59.490
a subject in a sentence for example so
you can imagine updating the LST M to

00:25:59.490 --> 00:25:59.500
you can imagine updating the LST M to
 

00:25:59.500 --> 00:26:02.010
you can imagine updating the LST M to
forget the gender pronoun of a sentences

00:26:02.010 --> 00:26:02.020
forget the gender pronoun of a sentences
 

00:26:02.020 --> 00:26:04.980
forget the gender pronoun of a sentences
past subject once it encounters a new

00:26:04.980 --> 00:26:04.990
past subject once it encounters a new
 

00:26:04.990 --> 00:26:09.300
past subject once it encounters a new
subject in that set in that sentence our

00:26:09.300 --> 00:26:09.310
subject in that set in that sentence our
 

00:26:09.310 --> 00:26:11.760
subject in that set in that sentence our
second step is to decide what new

00:26:11.760 --> 00:26:11.770
second step is to decide what new
 

00:26:11.770 --> 00:26:13.830
second step is to decide what new
information is going to be stored in our

00:26:13.830 --> 00:26:13.840
information is going to be stored in our
 

00:26:13.840 --> 00:26:15.630
information is going to be stored in our
updated cell state and to actually

00:26:15.630 --> 00:26:15.640
updated cell state and to actually
 

00:26:15.640 --> 00:26:18.990
updated cell state and to actually
execute that update so there are two

00:26:18.990 --> 00:26:19.000
execute that update so there are two
 

00:26:19.000 --> 00:26:21.930
execute that update so there are two
steps to this the first is a sigmoid

00:26:21.930 --> 00:26:21.940
steps to this the first is a sigmoid
 

00:26:21.940 --> 00:26:24.900
steps to this the first is a sigmoid
layer which you can think of as gating

00:26:24.900 --> 00:26:24.910
layer which you can think of as gating
 

00:26:24.910 --> 00:26:27.780
layer which you can think of as gating
the input which identifies what values

00:26:27.780 --> 00:26:27.790
the input which identifies what values
 

00:26:27.790 --> 00:26:30.960
the input which identifies what values
we should update secondly we have a tan

00:26:30.960 --> 00:26:30.970
we should update secondly we have a tan
 

00:26:30.970 --> 00:26:33.330
we should update secondly we have a tan
H layer that generates a new vector of

00:26:33.330 --> 00:26:33.340
H layer that generates a new vector of
 

00:26:33.340 --> 00:26:35.820
H layer that generates a new vector of
candidate values that could be added to

00:26:35.820 --> 00:26:35.830
candidate values that could be added to
 

00:26:35.830 --> 00:26:39.330
candidate values that could be added to
the state and in our language model we

00:26:39.330 --> 00:26:39.340
the state and in our language model we
 

00:26:39.340 --> 00:26:40.230
the state and in our language model we
may

00:26:40.230 --> 00:26:40.240
may
 

00:26:40.240 --> 00:26:42.810
may
to add the gender of a new subject in

00:26:42.810 --> 00:26:42.820
to add the gender of a new subject in
 

00:26:42.820 --> 00:26:45.270
to add the gender of a new subject in
order to replace the gender of the old

00:26:45.270 --> 00:26:45.280
order to replace the gender of the old
 

00:26:45.280 --> 00:26:49.620
order to replace the gender of the old
subject now we can actually update our

00:26:49.620 --> 00:26:49.630
subject now we can actually update our
 

00:26:49.630 --> 00:26:53.250
subject now we can actually update our
old cell states EFT minus one into the

00:26:53.250 --> 00:26:53.260
old cell states EFT minus one into the
 

00:26:53.260 --> 00:26:55.950
old cell states EFT minus one into the
new cell states EFT our previous two

00:26:55.950 --> 00:26:55.960
new cell states EFT our previous two
 

00:26:55.960 --> 00:26:59.280
new cell states EFT our previous two
steps decided what we should do now it's

00:26:59.280 --> 00:26:59.290
steps decided what we should do now it's
 

00:26:59.290 --> 00:27:02.790
steps decided what we should do now it's
about actually executing that so to

00:27:02.790 --> 00:27:02.800
about actually executing that so to
 

00:27:02.800 --> 00:27:05.550
about actually executing that so to
perform this update we first multiply

00:27:05.550 --> 00:27:05.560
perform this update we first multiply
 

00:27:05.560 --> 00:27:09.030
perform this update we first multiply
our old cell state C of t minus one by

00:27:09.030 --> 00:27:09.040
our old cell state C of t minus one by
 

00:27:09.040 --> 00:27:12.630
our old cell state C of t minus one by
our forget state or forget gate F of T

00:27:12.630 --> 00:27:12.640
our forget state or forget gate F of T
 

00:27:12.640 --> 00:27:15.480
our forget state or forget gate F of T
this forgets what we decided to forget

00:27:15.480 --> 00:27:15.490
this forgets what we decided to forget
 

00:27:15.490 --> 00:27:20.310
this forgets what we decided to forget
right we can then add our our set of new

00:27:20.310 --> 00:27:20.320
right we can then add our our set of new
 

00:27:20.320 --> 00:27:22.770
right we can then add our our set of new
candidate values scaled by the input

00:27:22.770 --> 00:27:22.780
candidate values scaled by the input
 

00:27:22.780 --> 00:27:26.340
candidate values scaled by the input
gate to selectively update each state

00:27:26.340 --> 00:27:26.350
gate to selectively update each state
 

00:27:26.350 --> 00:27:28.950
gate to selectively update each state
value and so in our language model

00:27:28.950 --> 00:27:28.960
value and so in our language model
 

00:27:28.960 --> 00:27:30.960
value and so in our language model
example this means that we're dropping

00:27:30.960 --> 00:27:30.970
example this means that we're dropping
 

00:27:30.970 --> 00:27:32.910
example this means that we're dropping
information about the old subjects

00:27:32.910 --> 00:27:32.920
information about the old subjects
 

00:27:32.920 --> 00:27:34.950
information about the old subjects
gender and then adding the new

00:27:34.950 --> 00:27:34.960
gender and then adding the new
 

00:27:34.960 --> 00:27:38.160
gender and then adding the new
information finally we just need to

00:27:38.160 --> 00:27:38.170
information finally we just need to
 

00:27:38.170 --> 00:27:39.780
information finally we just need to
decide what we're going to output and

00:27:39.780 --> 00:27:39.790
decide what we're going to output and
 

00:27:39.790 --> 00:27:42.720
decide what we're going to output and
actually output it and what we are going

00:27:42.720 --> 00:27:42.730
actually output it and what we are going
 

00:27:42.730 --> 00:27:45.660
actually output it and what we are going
to output H of T is going to be a

00:27:45.660 --> 00:27:45.670
to output H of T is going to be a
 

00:27:45.670 --> 00:27:48.330
to output H of T is going to be a
filtered version of our internal state

00:27:48.330 --> 00:27:48.340
filtered version of our internal state
 

00:27:48.340 --> 00:27:49.890
filtered version of our internal state
that we've been maintaining and updating

00:27:49.890 --> 00:27:49.900
that we've been maintaining and updating
 

00:27:49.900 --> 00:27:52.950
that we've been maintaining and updating
all along so again we're going to use a

00:27:52.950 --> 00:27:52.960
all along so again we're going to use a
 

00:27:52.960 --> 00:27:55.110
all along so again we're going to use a
sigmoid layer to gate where we're going

00:27:55.110 --> 00:27:55.120
sigmoid layer to gate where we're going
 

00:27:55.120 --> 00:27:57.540
sigmoid layer to gate where we're going
to output and we put our recently

00:27:57.540 --> 00:27:57.550
to output and we put our recently
 

00:27:57.550 --> 00:28:02.450
to output and we put our recently
updated cell state C of T through

00:28:02.450 --> 00:28:02.460
updated cell state C of T through
 

00:28:02.460 --> 00:28:06.300
updated cell state C of T through
through a tan H layer and then multiply

00:28:06.300 --> 00:28:06.310
through a tan H layer and then multiply
 

00:28:06.310 --> 00:28:08.690
through a tan H layer and then multiply
this by the output of the sigmoid gate

00:28:08.690 --> 00:28:08.700
this by the output of the sigmoid gate
 

00:28:08.700 --> 00:28:11.490
this by the output of the sigmoid gate
essentially this amounts to transforming

00:28:11.490 --> 00:28:11.500
essentially this amounts to transforming
 

00:28:11.500 --> 00:28:14.610
essentially this amounts to transforming
the updated South State using that tan H

00:28:14.610 --> 00:28:14.620
the updated South State using that tan H
 

00:28:14.620 --> 00:28:18.570
the updated South State using that tan H
and then dating it and in our language

00:28:18.570 --> 00:28:18.580
and then dating it and in our language
 

00:28:18.580 --> 00:28:21.000
and then dating it and in our language
model for example you may want to output

00:28:21.000 --> 00:28:21.010
model for example you may want to output
 

00:28:21.010 --> 00:28:23.220
model for example you may want to output
information that relates to a verb for

00:28:23.220 --> 00:28:23.230
information that relates to a verb for
 

00:28:23.230 --> 00:28:26.520
information that relates to a verb for
example if we've just seen the subject a

00:28:26.520 --> 00:28:26.530
example if we've just seen the subject a
 

00:28:26.530 --> 00:28:30.540
example if we've just seen the subject a
new subject in the sentence so this

00:28:30.540 --> 00:28:30.550
new subject in the sentence so this
 

00:28:30.550 --> 00:28:32.790
new subject in the sentence so this
gives us a sense of the internal

00:28:32.790 --> 00:28:32.800
gives us a sense of the internal
 

00:28:32.800 --> 00:28:34.650
gives us a sense of the internal
workings of the LS TM but if there's one

00:28:34.650 --> 00:28:34.660
workings of the LS TM but if there's one
 

00:28:34.660 --> 00:28:37.650
workings of the LS TM but if there's one
thing I that you take away right it's

00:28:37.650 --> 00:28:37.660
thing I that you take away right it's
 

00:28:37.660 --> 00:28:39.120
thing I that you take away right it's
sort of those three high-level

00:28:39.120 --> 00:28:39.130
sort of those three high-level
 

00:28:39.130 --> 00:28:41.820
sort of those three high-level
operations of the LS TM forget old

00:28:41.820 --> 00:28:41.830
operations of the LS TM forget old
 

00:28:41.830 --> 00:28:44.160
operations of the LS TM forget old
information update the cell state and

00:28:44.160 --> 00:28:44.170
information update the cell state and
 

00:28:44.170 --> 00:28:47.730
information update the cell state and
output a filtered version right but to

00:28:47.730 --> 00:28:47.740
output a filtered version right but to
 

00:28:47.740 --> 00:28:50.160
output a filtered version right but to
really appreciate how the LS TM helps

00:28:50.160 --> 00:28:50.170
really appreciate how the LS TM helps
 

00:28:50.170 --> 00:28:52.020
really appreciate how the LS TM helps
overcome the vanishing gradient problem

00:28:52.020 --> 00:28:52.030
overcome the vanishing gradient problem
 

00:28:52.030 --> 00:28:53.870
overcome the vanishing gradient problem
let's consider the really

00:28:53.870 --> 00:28:53.880
let's consider the really
 

00:28:53.880 --> 00:28:56.960
let's consider the really
in ship between C of T and C of T minus

00:28:56.960 --> 00:28:56.970
in ship between C of T and C of T minus
 

00:28:56.970 --> 00:29:00.320
in ship between C of T and C of T minus
one right when we back propagate from C

00:29:00.320 --> 00:29:00.330
one right when we back propagate from C
 

00:29:00.330 --> 00:29:03.530
one right when we back propagate from C
of T our current cell state right to C

00:29:03.530 --> 00:29:03.540
of T our current cell state right to C
 

00:29:03.540 --> 00:29:07.280
of T our current cell state right to C
of t minus one what you'll notice is

00:29:07.280 --> 00:29:07.290
of t minus one what you'll notice is
 

00:29:07.290 --> 00:29:09.590
of t minus one what you'll notice is
that we only have to perform elementwise

00:29:09.590 --> 00:29:09.600
that we only have to perform elementwise
 

00:29:09.600 --> 00:29:12.860
that we only have to perform elementwise
multiplication and an addition and doing

00:29:12.860 --> 00:29:12.870
multiplication and an addition and doing
 

00:29:12.870 --> 00:29:15.500
multiplication and an addition and doing
this back propagation there's no matrix

00:29:15.500 --> 00:29:15.510
this back propagation there's no matrix
 

00:29:15.510 --> 00:29:17.330
this back propagation there's no matrix
multiplication that's involved and

00:29:17.330 --> 00:29:17.340
multiplication that's involved and
 

00:29:17.340 --> 00:29:19.400
multiplication that's involved and
that's entirely because we're

00:29:19.400 --> 00:29:19.410
that's entirely because we're
 

00:29:19.410 --> 00:29:21.830
that's entirely because we're
maintaining this separate cell state C

00:29:21.830 --> 00:29:21.840
maintaining this separate cell state C
 

00:29:21.840 --> 00:29:25.910
maintaining this separate cell state C
of T apart from H of T and that C of T

00:29:25.910 --> 00:29:25.920
of T apart from H of T and that C of T
 

00:29:25.920 --> 00:29:27.920
of T apart from H of T and that C of T
is only involved in really simple

00:29:27.920 --> 00:29:27.930
is only involved in really simple
 

00:29:27.930 --> 00:29:31.310
is only involved in really simple
computations and so when you link up

00:29:31.310 --> 00:29:31.320
computations and so when you link up
 

00:29:31.320 --> 00:29:33.650
computations and so when you link up
these repeating LS p.m. units in a chain

00:29:33.650 --> 00:29:33.660
these repeating LS p.m. units in a chain
 

00:29:33.660 --> 00:29:35.810
these repeating LS p.m. units in a chain
what you'll see is that you get this

00:29:35.810 --> 00:29:35.820
what you'll see is that you get this
 

00:29:35.820 --> 00:29:38.720
what you'll see is that you get this
completely uninterrupted gradient flow

00:29:38.720 --> 00:29:38.730
completely uninterrupted gradient flow
 

00:29:38.730 --> 00:29:41.510
completely uninterrupted gradient flow
unlike in a standard RNN where you have

00:29:41.510 --> 00:29:41.520
unlike in a standard RNN where you have
 

00:29:41.520 --> 00:29:44.150
unlike in a standard RNN where you have
to do repeated matrix multiplications

00:29:44.150 --> 00:29:44.160
to do repeated matrix multiplications
 

00:29:44.160 --> 00:29:46.850
to do repeated matrix multiplications
and this is really great for training

00:29:46.850 --> 00:29:46.860
and this is really great for training
 

00:29:46.860 --> 00:29:50.210
and this is really great for training
purposes and for overcoming the

00:29:50.210 --> 00:29:50.220
purposes and for overcoming the
 

00:29:50.220 --> 00:29:53.540
purposes and for overcoming the
vanishing gradient problem so to recap

00:29:53.540 --> 00:29:53.550
vanishing gradient problem so to recap
 

00:29:53.550 --> 00:29:56.900
vanishing gradient problem so to recap
the key ideas behind LS CMS we maintain

00:29:56.900 --> 00:29:56.910
the key ideas behind LS CMS we maintain
 

00:29:56.910 --> 00:29:58.550
the key ideas behind LS CMS we maintain
a separate cell state from what's

00:29:58.550 --> 00:29:58.560
a separate cell state from what's
 

00:29:58.560 --> 00:30:02.000
a separate cell state from what's
outputted we use gates to control the

00:30:02.000 --> 00:30:02.010
outputted we use gates to control the
 

00:30:02.010 --> 00:30:04.460
outputted we use gates to control the
flow of information first forgetting

00:30:04.460 --> 00:30:04.470
flow of information first forgetting
 

00:30:04.470 --> 00:30:07.310
flow of information first forgetting
what's irrelevant selectively updating

00:30:07.310 --> 00:30:07.320
what's irrelevant selectively updating
 

00:30:07.320 --> 00:30:09.290
what's irrelevant selectively updating
the cell state based on both the past

00:30:09.290 --> 00:30:09.300
the cell state based on both the past
 

00:30:09.300 --> 00:30:11.900
the cell state based on both the past
history and the current input and then

00:30:11.900 --> 00:30:11.910
history and the current input and then
 

00:30:11.910 --> 00:30:14.810
history and the current input and then
outputting some filtered version of what

00:30:14.810 --> 00:30:14.820
outputting some filtered version of what
 

00:30:14.820 --> 00:30:20.150
outputting some filtered version of what
we just computed and this this maintain

00:30:20.150 --> 00:30:20.160
we just computed and this this maintain
 

00:30:20.160 --> 00:30:22.130
we just computed and this this maintain
this maintenance of this separate cell

00:30:22.130 --> 00:30:22.140
this maintenance of this separate cell
 

00:30:22.140 --> 00:30:25.280
this maintenance of this separate cell
state allows for simple back propagation

00:30:25.280 --> 00:30:25.290
state allows for simple back propagation
 

00:30:25.290 --> 00:30:29.810
state allows for simple back propagation
with uninterrupted gradient flow so now

00:30:29.810 --> 00:30:29.820
with uninterrupted gradient flow so now
 

00:30:29.820 --> 00:30:31.670
with uninterrupted gradient flow so now
that we've gone through the fundamental

00:30:31.670 --> 00:30:31.680
that we've gone through the fundamental
 

00:30:31.680 --> 00:30:33.890
that we've gone through the fundamental
workings of our n ends back propagation

00:30:33.890 --> 00:30:33.900
workings of our n ends back propagation
 

00:30:33.900 --> 00:30:36.290
workings of our n ends back propagation
through time the vanishing and exploding

00:30:36.290 --> 00:30:36.300
through time the vanishing and exploding
 

00:30:36.300 --> 00:30:38.270
through time the vanishing and exploding
gradient problems and the STM

00:30:38.270 --> 00:30:38.280
gradient problems and the STM
 

00:30:38.280 --> 00:30:40.490
gradient problems and the STM
architecture I'd like to close by

00:30:40.490 --> 00:30:40.500
architecture I'd like to close by
 

00:30:40.500 --> 00:30:42.890
architecture I'd like to close by
considering three really concrete

00:30:42.890 --> 00:30:42.900
considering three really concrete
 

00:30:42.900 --> 00:30:46.030
considering three really concrete
examples of how to use our nuns

00:30:46.030 --> 00:30:46.040
examples of how to use our nuns
 

00:30:46.040 --> 00:30:48.650
examples of how to use our nuns
let's first imagine we're trying to

00:30:48.650 --> 00:30:48.660
let's first imagine we're trying to
 

00:30:48.660 --> 00:30:52.070
let's first imagine we're trying to
learn a RN n to predict the next musical

00:30:52.070 --> 00:30:52.080
learn a RN n to predict the next musical
 

00:30:52.080 --> 00:30:54.740
learn a RN n to predict the next musical
note and to use this model to generate

00:30:54.740 --> 00:30:54.750
note and to use this model to generate
 

00:30:54.750 --> 00:30:57.920
note and to use this model to generate
brand new musical sequences so you can

00:30:57.920 --> 00:30:57.930
brand new musical sequences so you can
 

00:30:57.930 --> 00:31:01.370
brand new musical sequences so you can
imagine inputting a sequence of notes

00:31:01.370 --> 00:31:01.380
imagine inputting a sequence of notes
 

00:31:01.380 --> 00:31:02.830
imagine inputting a sequence of notes
right

00:31:02.830 --> 00:31:02.840
right
 

00:31:02.840 --> 00:31:05.440
right
producing an output at each time step

00:31:05.440 --> 00:31:05.450
producing an output at each time step
 

00:31:05.450 --> 00:31:07.750
producing an output at each time step
where our output at each time step is

00:31:07.750 --> 00:31:07.760
where our output at each time step is
 

00:31:07.760 --> 00:31:10.210
where our output at each time step is
what we think is the next note in the

00:31:10.210 --> 00:31:10.220
what we think is the next note in the
 

00:31:10.220 --> 00:31:14.320
what we think is the next note in the
sequence right and if you train this

00:31:14.320 --> 00:31:14.330
sequence right and if you train this
 

00:31:14.330 --> 00:31:16.900
sequence right and if you train this
model like this you can actually use it

00:31:16.900 --> 00:31:16.910
model like this you can actually use it
 

00:31:16.910 --> 00:31:19.030
model like this you can actually use it
to generate brand new music that's never

00:31:19.030 --> 00:31:19.040
to generate brand new music that's never
 

00:31:19.040 --> 00:31:23.950
to generate brand new music that's never
been heard before and so for example

00:31:23.950 --> 00:31:23.960
been heard before and so for example
 

00:31:23.960 --> 00:31:36.160
been heard before and so for example
[Music]

00:31:36.160 --> 00:31:36.170
 

00:31:36.170 --> 00:31:38.630
right you get the idea this sounds like

00:31:38.630 --> 00:31:38.640
right you get the idea this sounds like
 

00:31:38.640 --> 00:31:40.730
right you get the idea this sounds like
classical music right but in reality

00:31:40.730 --> 00:31:40.740
classical music right but in reality
 

00:31:40.740 --> 00:31:43.430
classical music right but in reality
this was music that was generated by a

00:31:43.430 --> 00:31:43.440
this was music that was generated by a
 

00:31:43.440 --> 00:31:46.550
this was music that was generated by a
recurrent neural network that trained on

00:31:46.550 --> 00:31:46.560
recurrent neural network that trained on
 

00:31:46.560 --> 00:31:50.420
recurrent neural network that trained on
piano pieces from Chopin and after the

00:31:50.420 --> 00:31:50.430
piano pieces from Chopin and after the
 

00:31:50.430 --> 00:31:52.460
piano pieces from Chopin and after the
training process was asked okay now

00:31:52.460 --> 00:31:52.470
training process was asked okay now
 

00:31:52.470 --> 00:31:55.940
training process was asked okay now
generate some some new music based on

00:31:55.940 --> 00:31:55.950
generate some some new music based on
 

00:31:55.950 --> 00:31:57.920
generate some some new music based on
what it is you've learned and you can

00:31:57.920 --> 00:31:57.930
what it is you've learned and you can
 

00:31:57.930 --> 00:31:59.570
what it is you've learned and you can
see right this sounds like extremely

00:31:59.570 --> 00:31:59.580
see right this sounds like extremely
 

00:31:59.580 --> 00:32:02.090
see right this sounds like extremely
realistic you may not have been able to

00:32:02.090 --> 00:32:02.100
realistic you may not have been able to
 

00:32:02.100 --> 00:32:04.490
realistic you may not have been able to
tell that this was music generated by a

00:32:04.490 --> 00:32:04.500
tell that this was music generated by a
 

00:32:04.500 --> 00:32:06.620
tell that this was music generated by a
machine unless maybe you're you're an

00:32:06.620 --> 00:32:06.630
machine unless maybe you're you're an
 

00:32:06.630 --> 00:32:09.770
machine unless maybe you're you're an
expert piano aficionado and you'll

00:32:09.770 --> 00:32:09.780
expert piano aficionado and you'll
 

00:32:09.780 --> 00:32:11.960
expert piano aficionado and you'll
actually get some practice with building

00:32:11.960 --> 00:32:11.970
actually get some practice with building
 

00:32:11.970 --> 00:32:13.940
actually get some practice with building
a model to do exactly this in today's

00:32:13.940 --> 00:32:13.950
a model to do exactly this in today's
 

00:32:13.950 --> 00:32:16.820
a model to do exactly this in today's
lab where you'll be training an RNN to

00:32:16.820 --> 00:32:16.830
lab where you'll be training an RNN to
 

00:32:16.830 --> 00:32:19.730
lab where you'll be training an RNN to
generate brand new Irish folk music that

00:32:19.730 --> 00:32:19.740
generate brand new Irish folk music that
 

00:32:19.740 --> 00:32:21.880
generate brand new Irish folk music that
has never been heard before

00:32:21.880 --> 00:32:21.890
has never been heard before
 

00:32:21.890 --> 00:32:26.420
has never been heard before
as another cool example where we're

00:32:26.420 --> 00:32:26.430
as another cool example where we're
 

00:32:26.430 --> 00:32:28.640
as another cool example where we're
going from an input sequence to just a

00:32:28.640 --> 00:32:28.650
going from an input sequence to just a
 

00:32:28.650 --> 00:32:31.610
going from an input sequence to just a
single output we can train an RNN to

00:32:31.610 --> 00:32:31.620
single output we can train an RNN to
 

00:32:31.620 --> 00:32:35.200
single output we can train an RNN to
take as input words in a sentence and

00:32:35.200 --> 00:32:35.210
take as input words in a sentence and
 

00:32:35.210 --> 00:32:38.060
take as input words in a sentence and
actually output the sentiment or the

00:32:38.060 --> 00:32:38.070
actually output the sentiment or the
 

00:32:38.070 --> 00:32:40.370
actually output the sentiment or the
feeling of that particular sentence

00:32:40.370 --> 00:32:40.380
feeling of that particular sentence
 

00:32:40.380 --> 00:32:43.910
feeling of that particular sentence
either positive or negative so for

00:32:43.910 --> 00:32:43.920
either positive or negative so for
 

00:32:43.920 --> 00:32:45.350
either positive or negative so for
example if we if we were to train a

00:32:45.350 --> 00:32:45.360
example if we if we were to train a
 

00:32:45.360 --> 00:32:48.470
example if we if we were to train a
model like this on a set of tweets we

00:32:48.470 --> 00:32:48.480
model like this on a set of tweets we
 

00:32:48.480 --> 00:32:50.560
model like this on a set of tweets we
could train our RNN to predict that this

00:32:50.560 --> 00:32:50.570
could train our RNN to predict that this
 

00:32:50.570 --> 00:32:53.030
could train our RNN to predict that this
wonderful first tweet about our class

00:32:53.030 --> 00:32:53.040
wonderful first tweet about our class
 

00:32:53.040 --> 00:32:55.490
wonderful first tweet about our class
success 191 has a really positive

00:32:55.490 --> 00:32:55.500
success 191 has a really positive
 

00:32:55.500 --> 00:32:57.820
success 191 has a really positive
sentiment which hopefully you agree with

00:32:57.820 --> 00:32:57.830
sentiment which hopefully you agree with
 

00:32:57.830 --> 00:33:00.380
sentiment which hopefully you agree with
but that this other tweet about the

00:33:00.380 --> 00:33:00.390
but that this other tweet about the
 

00:33:00.390 --> 00:33:04.490
but that this other tweet about the
weather is actually negative the final

00:33:04.490 --> 00:33:04.500
weather is actually negative the final
 

00:33:04.500 --> 00:33:06.920
weather is actually negative the final
example I'll briefly talk about is one

00:33:06.920 --> 00:33:06.930
example I'll briefly talk about is one
 

00:33:06.930 --> 00:33:09.010
example I'll briefly talk about is one
of the most powerful and widely used

00:33:09.010 --> 00:33:09.020
of the most powerful and widely used
 

00:33:09.020 --> 00:33:12.080
of the most powerful and widely used
applications of Arlen's in industry and

00:33:12.080 --> 00:33:12.090
applications of Arlen's in industry and
 

00:33:12.090 --> 00:33:15.200
applications of Arlen's in industry and
it's the backbone of Google's Translate

00:33:15.200 --> 00:33:15.210
it's the backbone of Google's Translate
 

00:33:15.210 --> 00:33:17.810
it's the backbone of Google's Translate
algorithm and that's machine translation

00:33:17.810 --> 00:33:17.820
algorithm and that's machine translation
 

00:33:17.820 --> 00:33:20.570
algorithm and that's machine translation
where you input a sentence in one

00:33:20.570 --> 00:33:20.580
where you input a sentence in one
 

00:33:20.580 --> 00:33:25.880
where you input a sentence in one
language and train an RNN to output a

00:33:25.880 --> 00:33:25.890
language and train an RNN to output a
 

00:33:25.890 --> 00:33:28.670
language and train an RNN to output a
sentence in a new language and this is

00:33:28.670 --> 00:33:28.680
sentence in a new language and this is
 

00:33:28.680 --> 00:33:34.240
sentence in a new language and this is
done by having an encoder that encodes

00:33:34.240 --> 00:33:34.250
done by having an encoder that encodes
 

00:33:34.250 --> 00:33:37.070
done by having an encoder that encodes
encodes their original sentence into a

00:33:37.070 --> 00:33:37.080
encodes their original sentence into a
 

00:33:37.080 --> 00:33:41.480
encodes their original sentence into a
state vector and a decoder which decodes

00:33:41.480 --> 00:33:41.490
state vector and a decoder which decodes
 

00:33:41.490 --> 00:33:44.830
state vector and a decoder which decodes
that state vector into a new language

00:33:44.830 --> 00:33:44.840
that state vector into a new language
 

00:33:44.840 --> 00:33:48.120
that state vector into a new language
but there's a big problem in in the

00:33:48.120 --> 00:33:48.130
but there's a big problem in in the
 

00:33:48.130 --> 00:33:50.760
but there's a big problem in in the
approach as depicted here and that's the

00:33:50.760 --> 00:33:50.770
approach as depicted here and that's the
 

00:33:50.770 --> 00:33:53.190
approach as depicted here and that's the
fact that this entire original sentence

00:33:53.190 --> 00:33:53.200
fact that this entire original sentence
 

00:33:53.200 --> 00:33:56.850
fact that this entire original sentence
needs to be encoded in a single vector

00:33:56.850 --> 00:33:56.860
needs to be encoded in a single vector
 

00:33:56.860 --> 00:33:59.070
needs to be encoded in a single vector
that's passed from the encoder to the

00:33:59.070 --> 00:33:59.080
that's passed from the encoder to the
 

00:33:59.080 --> 00:34:01.950
that's passed from the encoder to the
decoder and this is a huge bottleneck

00:34:01.950 --> 00:34:01.960
decoder and this is a huge bottleneck
 

00:34:01.960 --> 00:34:04.200
decoder and this is a huge bottleneck
when you're considering large bodies of

00:34:04.200 --> 00:34:04.210
when you're considering large bodies of
 

00:34:04.210 --> 00:34:06.320
when you're considering large bodies of
text that you're trying to translate and

00:34:06.320 --> 00:34:06.330
text that you're trying to translate and
 

00:34:06.330 --> 00:34:09.210
text that you're trying to translate and
actually you know researchers devised a

00:34:09.210 --> 00:34:09.220
actually you know researchers devised a
 

00:34:09.220 --> 00:34:11.600
actually you know researchers devised a
clever way to get around this problem

00:34:11.600 --> 00:34:11.610
clever way to get around this problem
 

00:34:11.610 --> 00:34:15.720
clever way to get around this problem
which is this idea of attention and the

00:34:15.720 --> 00:34:15.730
which is this idea of attention and the
 

00:34:15.730 --> 00:34:18.180
which is this idea of attention and the
basic idea here is that instead of the

00:34:18.180 --> 00:34:18.190
basic idea here is that instead of the
 

00:34:18.190 --> 00:34:21.149
basic idea here is that instead of the
decoder only having access to the final

00:34:21.149 --> 00:34:21.159
decoder only having access to the final
 

00:34:21.159 --> 00:34:25.139
decoder only having access to the final
encoded state and now has access to each

00:34:25.139 --> 00:34:25.149
encoded state and now has access to each
 

00:34:25.149 --> 00:34:27.030
encoded state and now has access to each
of these states after each of the steps

00:34:27.030 --> 00:34:27.040
of these states after each of the steps
 

00:34:27.040 --> 00:34:30.330
of these states after each of the steps
in the original sentence and the actual

00:34:30.330 --> 00:34:30.340
in the original sentence and the actual
 

00:34:30.340 --> 00:34:33.090
in the original sentence and the actual
weighting of these vectors from encoder

00:34:33.090 --> 00:34:33.100
weighting of these vectors from encoder
 

00:34:33.100 --> 00:34:36.419
weighting of these vectors from encoder
to decoder is learned by the network

00:34:36.419 --> 00:34:36.429
to decoder is learned by the network
 

00:34:36.429 --> 00:34:39.869
to decoder is learned by the network
during training and this this technique

00:34:39.869 --> 00:34:39.879
during training and this this technique
 

00:34:39.879 --> 00:34:42.180
during training and this this technique
is called attention because when the

00:34:42.180 --> 00:34:42.190
is called attention because when the
 

00:34:42.190 --> 00:34:44.700
is called attention because when the
network learns this waiting its placing

00:34:44.700 --> 00:34:44.710
network learns this waiting its placing
 

00:34:44.710 --> 00:34:46.740
network learns this waiting its placing
its attention on different parts of the

00:34:46.740 --> 00:34:46.750
its attention on different parts of the
 

00:34:46.750 --> 00:34:49.169
its attention on different parts of the
input sequence and in this sense you can

00:34:49.169 --> 00:34:49.179
input sequence and in this sense you can
 

00:34:49.179 --> 00:34:51.300
input sequence and in this sense you can
think of it as actually capturing a sort

00:34:51.300 --> 00:34:51.310
think of it as actually capturing a sort
 

00:34:51.310 --> 00:34:56.040
think of it as actually capturing a sort
of memory of the original sentence so

00:34:56.040 --> 00:34:56.050
of memory of the original sentence so
 

00:34:56.050 --> 00:34:58.200
of memory of the original sentence so
hopefully you've gotten a sense of how

00:34:58.200 --> 00:34:58.210
hopefully you've gotten a sense of how
 

00:34:58.210 --> 00:35:00.180
hopefully you've gotten a sense of how
our ends work and why they're so

00:35:00.180 --> 00:35:00.190
our ends work and why they're so
 

00:35:00.190 --> 00:35:03.630
our ends work and why they're so
powerful for sequential processing we've

00:35:03.630 --> 00:35:03.640
powerful for sequential processing we've
 

00:35:03.640 --> 00:35:05.820
powerful for sequential processing we've
discussed why they're so well-suited for

00:35:05.820 --> 00:35:05.830
discussed why they're so well-suited for
 

00:35:05.830 --> 00:35:09.210
discussed why they're so well-suited for
sequential modeling tasks seen how to

00:35:09.210 --> 00:35:09.220
sequential modeling tasks seen how to
 

00:35:09.220 --> 00:35:11.280
sequential modeling tasks seen how to
define their operation using this

00:35:11.280 --> 00:35:11.290
define their operation using this
 

00:35:11.290 --> 00:35:13.620
define their operation using this
recurrence relation how to train them

00:35:13.620 --> 00:35:13.630
recurrence relation how to train them
 

00:35:13.630 --> 00:35:16.430
recurrence relation how to train them
using back propagation through time and

00:35:16.430 --> 00:35:16.440
using back propagation through time and
 

00:35:16.440 --> 00:35:19.200
using back propagation through time and
also looked at how gated cells can let

00:35:19.200 --> 00:35:19.210
also looked at how gated cells can let
 

00:35:19.210 --> 00:35:21.230
also looked at how gated cells can let
us model long-term dependencies and

00:35:21.230 --> 00:35:21.240
us model long-term dependencies and
 

00:35:21.240 --> 00:35:23.700
us model long-term dependencies and
finally we discussed three concrete

00:35:23.700 --> 00:35:23.710
finally we discussed three concrete
 

00:35:23.710 --> 00:35:28.620
finally we discussed three concrete
applications of RN ends and so this

00:35:28.620 --> 00:35:28.630
applications of RN ends and so this
 

00:35:28.630 --> 00:35:30.840
applications of RN ends and so this
concludes right the lecture portion of

00:35:30.840 --> 00:35:30.850
concludes right the lecture portion of
 

00:35:30.850 --> 00:35:33.450
concludes right the lecture portion of
our first day of six s-191

00:35:33.450 --> 00:35:33.460
our first day of six s-191
 

00:35:33.460 --> 00:35:35.630
our first day of six s-191
and we're really excited now to

00:35:35.630 --> 00:35:35.640
and we're really excited now to
 

00:35:35.640 --> 00:35:39.480
and we're really excited now to
transition to the lab portion which as I

00:35:39.480 --> 00:35:39.490
transition to the lab portion which as I
 

00:35:39.490 --> 00:35:43.470
transition to the lab portion which as I
mentioned is going to mostly focus on

00:35:43.470 --> 00:35:43.480
mentioned is going to mostly focus on
 

00:35:43.480 --> 00:35:46.560
mentioned is going to mostly focus on
training and RNN to generate brand new

00:35:46.560 --> 00:35:46.570
training and RNN to generate brand new
 

00:35:46.570 --> 00:35:49.350
training and RNN to generate brand new
music and so the the lab is going to be

00:35:49.350 --> 00:35:49.360
music and so the the lab is going to be
 

00:35:49.360 --> 00:35:51.860
music and so the the lab is going to be
broken down into two parts

00:35:51.860 --> 00:35:51.870
broken down into two parts
 

00:35:51.870 --> 00:35:56.130
broken down into two parts
but sorry but before I go into the

00:35:56.130 --> 00:35:56.140
but sorry but before I go into the
 

00:35:56.140 --> 00:35:58.170
but sorry but before I go into the
specifics of getting started with the

00:35:58.170 --> 00:35:58.180
specifics of getting started with the
 

00:35:58.180 --> 00:35:59.170
specifics of getting started with the
labs

00:35:59.170 --> 00:35:59.180
labs
 

00:35:59.180 --> 00:36:02.410
labs
I'd like to take a few minutes pause for

00:36:02.410 --> 00:36:02.420
I'd like to take a few minutes pause for
 

00:36:02.420 --> 00:36:06.670
I'd like to take a few minutes pause for
those of you who you know plan to stick

00:36:06.670 --> 00:36:06.680
those of you who you know plan to stick
 

00:36:06.680 --> 00:36:08.530
those of you who you know plan to stick
around for the labs we're happy to have

00:36:08.530 --> 00:36:08.540
around for the labs we're happy to have
 

00:36:08.540 --> 00:36:08.950
around for the labs we're happy to have
you

00:36:08.950 --> 00:36:08.960
you
 

00:36:08.960 --> 00:36:11.380
you
we're also happy to address any

00:36:11.380 --> 00:36:11.390
we're also happy to address any
 

00:36:11.390 --> 00:36:13.870
we're also happy to address any
questions that you may have about either

00:36:13.870 --> 00:36:13.880
questions that you may have about either
 

00:36:13.880 --> 00:36:16.210
questions that you may have about either
lecture one or lecture two here at the

00:36:16.210 --> 00:36:16.220
lecture one or lecture two here at the
 

00:36:16.220 --> 00:36:19.060
lecture one or lecture two here at the
front and we'll just take a 5-minute

00:36:19.060 --> 00:36:19.070
front and we'll just take a 5-minute
 

00:36:19.070 --> 00:36:21.940
front and we'll just take a 5-minute
break or so to orient ourselves and get

00:36:21.940 --> 00:36:21.950
break or so to orient ourselves and get
 

00:36:21.950 --> 00:36:26.230
break or so to orient ourselves and get
set up for the lab thank you

00:36:26.230 --> 00:36:26.240
set up for the lab thank you
 

00:36:26.240 --> 00:36:32.119
set up for the lab thank you
[Applause]

