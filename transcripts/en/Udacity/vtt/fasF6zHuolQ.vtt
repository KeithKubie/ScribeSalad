WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.100
In addition to constant-factor approximation algorithms, which we will look at

00:00:04.100 --> 00:00:07.810
in more detail soon, we are also going to be talking about advanced algorithms

00:00:07.810 --> 00:00:13.120
that are known as polynomial-time approximation schemes. Scary term.

00:00:13.120 --> 00:00:18.140
What the idea of a polynomial-time approximation scheme is, it's a sort of more

00:00:18.140 --> 00:00:19.790
advanced constant-factor approximation,

00:00:19.790 --> 00:00:22.850
so you can think of it as a sort of scale.

00:00:22.850 --> 00:00:26.980
The idea behind a polynomial-time approximation scheme is that the running time

00:00:26.980 --> 00:00:29.810
actually depends on the quality that you want to get.

00:00:29.810 --> 00:00:33.160
With a constant-factor approximation you always get the same guarantee,

00:00:33.160 --> 00:00:37.040
say a factor to approximation, and in a polynomial-time approximation scheme,

00:00:37.040 --> 00:00:40.210
you can more or less decide what the quality is that you want,

00:00:40.210 --> 00:00:44.330
but the better a guarantee you're demanding, with respect to quality,

00:00:44.330 --> 00:00:46.950
the more running time you'll have to put into that algorithm.

00:00:46.950 --> 00:00:48.020
That's the trade-off here.

00:00:48.020 --> 00:00:51.020
But first of all let's look at constant-factor approximation.

00:00:51.020 --> 00:00:55.920
We are now going to meet Alice again. Alice, as you know, is working on vertex cover.

00:00:55.920 --> 00:01:01.000
So far she has been the most lucky in this course, because she seems to actually

00:01:01.000 --> 00:01:04.129
be working on a very well-behaved NP-complete problem.

00:01:04.129 --> 00:01:07.550
We also learned that there's good search trees, there's good preprocessing,

00:01:07.550 --> 00:01:10.170
and vertex cover was even fixed parameter tractable.

00:01:10.170 --> 00:01:14.340
Alice can be quite optomistic about approximating vertex cover,

00:01:14.340 --> 00:01:16.980
and actually she has come up with 2 different ideas

00:01:16.980 --> 00:01:19.370
of how you could approximate vertex covers.

00:01:19.370 --> 00:01:21.830
Here are the 2 possible ideas that she has come up with.

00:01:21.830 --> 00:01:27.330
The first algorithm takes this input graph and then looks if some edges are still not covered

00:01:27.330 --> 00:01:32.300
in that graph, and if that is the case it takes the uncovered edge and puts both of the endpoints

00:01:32.300 --> 00:01:34.300
of that edge into the vertex cover.

00:01:34.300 --> 00:01:39.360
Then the algorithm looks again if there is still some uncovered edges, if there are any

00:01:39.360 --> 00:01:43.310
of those, then it takes again one and puts both endpoints into the vertex cover

00:01:43.310 --> 00:01:45.310
and so on until all edges are covered.

00:01:45.310 --> 00:01:48.020
And she calls that algorithm "Take 2," because it always takes 2 endpoints

00:01:48.020 --> 00:01:51.710
into the vertex cover each time you go through that loop until you're done.

00:01:51.710 --> 00:01:56.750
Here is her second idea, and she calls that idea "Greedy," because what that algorithm does--

00:01:56.750 --> 00:02:00.440
It's still the same in the first line here. It looks if some edges are uncovered.

00:02:00.440 --> 00:02:06.870
Then what it does is it goes through all of the vertices to determine which vertex could cover

00:02:06.870 --> 00:02:09.870
the most edges that are still uncovered.

00:02:09.870 --> 00:02:13.370
So which vertex is next to the most edges that have not yet an endpoint in the vertex cover,

00:02:13.370 --> 00:02:16.190
and then it puts that vertex in the vertex cover.

00:02:16.190 --> 00:02:20.850
It kind of tries to maximize the coverage that it can achieve in each round

00:02:20.850 --> 00:02:23.240
that this loop here is run.

00:02:23.240 --> 00:02:27.420
So looking at this strategy, so these 2 algorithms, what I would like you to think about

00:02:27.420 --> 00:02:32.420
for a moment here is, which of these 2 algorithms should we expect to perform better

00:02:32.420 --> 00:02:34.440
in terms of approximation quality?

00:02:34.440 --> 99:59:59.000
The one that always takes 2 endpoints, or the one that tries to maximize coverage?

