WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.856
[MUSIC PLAYING]

00:00:03.615 --> 00:00:05.490
ELIE BURZSTEIN: If you
are new to our session

00:00:05.490 --> 00:00:08.160
on cutting edge TensorFlow,
my name is Elie.

00:00:08.160 --> 00:00:10.830
And today with Josh,
Mike, and Sophien,

00:00:10.830 --> 00:00:13.440
we have four exciting
projects for you.

00:00:13.440 --> 00:00:15.420
First, I will talk
about Keras Tuner.

00:00:15.420 --> 00:00:18.260
Then, Josh will talk about
probability programming.

00:00:18.260 --> 00:00:20.460
Next, Mike will talk
about TF-Ranking.

00:00:20.460 --> 00:00:23.550
And finally, Sofien will show
you how to use TensorFlow.

00:00:23.550 --> 00:00:25.470
Graphics

00:00:25.470 --> 00:00:28.290
Let me start by telling you
about Keras Tuner, which

00:00:28.290 --> 00:00:31.080
is a framework we initially
developed internally

00:00:31.080 --> 00:00:34.500
to bring new [INAUDIBLE]
model to faster to production.

00:00:34.500 --> 00:00:37.150
Before getting started,
let me ask you a question.

00:00:37.150 --> 00:00:40.170
How many of you have ever spent
time optimizing your model

00:00:40.170 --> 00:00:42.800
performances.

00:00:42.800 --> 00:00:45.390
Right, I see quite
a few hands raised.

00:00:45.390 --> 00:00:47.640
This is not surprising
because getting the best model

00:00:47.640 --> 00:00:50.820
performances is not easy.

00:00:50.820 --> 00:00:52.720
Getting the best
model performances

00:00:52.720 --> 00:00:56.310
is hard because there are many
parameters such as the learning

00:00:56.310 --> 00:00:59.550
rate, the batch size,
and the number of layers

00:00:59.550 --> 00:01:02.730
which influence the
model performances.

00:01:02.730 --> 00:01:05.590
Moreover, those parameters
are interdependent,

00:01:05.590 --> 00:01:08.070
which make finding the
optimal combinations by hand

00:01:08.070 --> 00:01:09.760
very challenging.

00:01:09.760 --> 00:01:13.110
This is why we rely hypertuning
to automatically find

00:01:13.110 --> 00:01:15.430
the best combinations.

00:01:15.430 --> 00:01:19.110
However so far,
hypertuning have been

00:01:19.110 --> 00:01:21.540
known to be not easy to use.

00:01:21.540 --> 00:01:25.020
So if hypertuning
is essential to get

00:01:25.020 --> 00:01:27.970
the optimal performances,
this begs the question,

00:01:27.970 --> 00:01:31.380
can we make it easy
as one, two, three?

00:01:31.380 --> 00:01:32.972
And the answer is yes.

00:01:32.972 --> 00:01:34.680
This is why today I
am happy to introduce

00:01:34.680 --> 00:01:38.850
to Keras Tuner which is a tuning
framework made for humans.

00:01:41.500 --> 00:01:43.510
Keras Tuner is a
tuning framework

00:01:43.510 --> 00:01:45.910
designed to make the
life of AI practitioner

00:01:45.910 --> 00:01:47.750
as easy as possible.

00:01:47.750 --> 00:01:51.520
It also helps hypertuner
algorithm creators and model

00:01:51.520 --> 00:01:58.030
designers by providing them with
a clean and easy to use API.

00:01:58.030 --> 00:02:01.610
For AI practitioner,
which is most of us,

00:02:01.610 --> 00:02:04.750
Keras Tuner makes
moving from a base model

00:02:04.750 --> 00:02:07.850
to a hypertuned
model quick and easy.

00:02:07.850 --> 00:02:10.900
Let me show you how this is
done by converting a basic MNIST

00:02:10.900 --> 00:02:13.200
model to a hypertuned one.

00:02:13.200 --> 00:02:15.240
We will only have to
change a few lines of code.

00:02:18.550 --> 00:02:21.440
Here's our basic MNIST
model that is a TensorFlow

00:02:21.440 --> 00:02:23.347
Keras controlled API.

00:02:23.347 --> 00:02:25.680
This is something I'm sure
all of you have seen already.

00:02:28.370 --> 00:02:31.410
As you can set on the side,
all parameter are fixed.

00:02:31.410 --> 00:02:36.570
For example, our learning
rate is set to 0.001.

00:02:36.570 --> 00:02:39.990
So let's transition it
to a hypertunable model

00:02:39.990 --> 00:02:41.430
in three steps.

00:02:41.430 --> 00:02:44.240
First, we wrap up our
model in a function.

00:02:44.240 --> 00:02:47.640
Second, we define
hyper-parameter ranges

00:02:47.640 --> 00:02:49.690
for the parameter that we
would like to optimize.

00:02:49.690 --> 00:02:52.710
Here for example, we're going
to optimize the learning

00:02:52.710 --> 00:02:55.140
rate which is one of the
most important parameters

00:02:55.140 --> 00:02:57.300
to hypertune.

00:02:57.300 --> 00:03:01.680
Finally as the last step, we
replace our fixed parameters

00:03:01.680 --> 00:03:03.880
with our hyper-parameter range.

00:03:03.880 --> 00:03:04.600
And we're done.

00:03:04.600 --> 00:03:06.370
Our model is ready
to be hypertuned.

00:03:06.370 --> 00:03:07.650
This is as easy.

00:03:15.090 --> 00:03:18.060
Besides offering an
intuitive API Keras Tuner,

00:03:18.060 --> 00:03:21.850
we'll also provide you with
state of the art hypertuning

00:03:21.850 --> 00:03:24.910
algorithm, tuneable
architectures which

00:03:24.910 --> 00:03:27.630
are ready to go, and an
automatic experimental

00:03:27.630 --> 00:03:31.840
recording which make it easy
for you to analyze, share,

00:03:31.840 --> 00:03:34.620
and reproduce your results.

00:03:34.620 --> 00:03:38.650
Originally, I started developing
Keras Tuner to quickly try it

00:03:38.650 --> 00:03:41.180
on new models for
[INAUDIBLE] purposes

00:03:41.180 --> 00:03:43.450
including the one we
developed to protect Gmail

00:03:43.450 --> 00:03:46.460
against malware and phishing.

00:03:46.460 --> 00:03:48.430
Band detection is one
of the core building

00:03:48.430 --> 00:03:51.960
block we need to protect your
inbox against phishing email

00:03:51.960 --> 00:03:54.550
that impersonates
the brand you love.

00:03:54.550 --> 00:03:57.160
This is why today, our
[INAUDIBLE] example

00:03:57.160 --> 00:04:00.190
would be to show you how
to build a simple and yet

00:04:00.190 --> 00:04:04.510
accurate brand logo classifier
as logo identification is

00:04:04.510 --> 00:04:09.370
one of the critical components
to detect brand spoofing email.

00:04:09.370 --> 00:04:11.640
The first thing we need to
do is to load our dataset.

00:04:14.480 --> 00:04:18.310
In that example, we
have about the 150 icons

00:04:18.310 --> 00:04:22.089
from various brands, including
the ones displayed on the side.

00:04:22.089 --> 00:04:23.740
We also need to
set a few variables

00:04:23.740 --> 00:04:25.850
such as batch size and
the number of icons

00:04:25.850 --> 00:04:28.950
we're going to use [INAUDIBLE].

00:04:28.950 --> 00:04:33.440
Next, our data set is
very small so we'll

00:04:33.440 --> 00:04:37.460
rely on data augmentation to
create enough training data.

00:04:37.460 --> 00:04:40.040
This slide shows you a few
examples of the augmented icons

00:04:40.040 --> 00:04:42.140
we're going to feed to
the classifier as training

00:04:42.140 --> 00:04:45.700
input, the output
being the brand names.

00:04:45.700 --> 00:04:49.030
We are going to save the real
icon as our validation dataset

00:04:49.030 --> 00:04:53.620
to make sure that our
classifier degenerates well.

00:04:53.620 --> 00:04:55.680
To establish a
baseline, let's first

00:04:55.680 --> 00:04:59.950
train a ResNet101v2 which is
one of the most common and well

00:04:59.950 --> 00:05:03.720
known model architectures.

00:05:03.720 --> 00:05:05.880
As you can see on the
[INAUDIBLE] graph,

00:05:05.880 --> 00:05:08.220
our model did converge but
the actuality on real icon

00:05:08.220 --> 00:05:09.120
is not great.

00:05:09.120 --> 00:05:10.760
We barely reached 79% accuracy.

00:05:10.760 --> 00:05:14.850
And it's also quite big
with 44 million parameters.

00:05:14.850 --> 00:05:16.390
Well, that's OK.

00:05:16.390 --> 00:05:18.960
We can use Keras Tuner to
find a better model which

00:05:18.960 --> 00:05:21.810
is smaller and more accurate.

00:05:21.810 --> 00:05:24.000
So to do that, the first
thing we need to do

00:05:24.000 --> 00:05:28.570
is to create, as the MNIST
model a model function and input

00:05:28.570 --> 00:05:30.060
TunableResNet.

00:05:30.060 --> 00:05:32.790
TunableResNet is a
tunable version of ResNet

00:05:32.790 --> 00:05:35.010
that we will provide
with Keras Tuner

00:05:35.010 --> 00:05:37.830
as one of the architectures
which are ready to tune.

00:05:37.830 --> 00:05:40.170
Next, you add a few
layers on top of it

00:05:40.170 --> 00:05:42.780
and combine the model.

00:05:42.780 --> 00:05:44.700
Then well, we
initialize the tuner

00:05:44.700 --> 00:05:48.600
and we give it $500 to spend on
tuning to find the best model.

00:05:48.600 --> 00:05:53.550
And we ask it to maximize
evaluation accuracy.

00:05:53.550 --> 00:05:57.540
Finally, we have to
launch the tuning

00:05:57.540 --> 00:05:59.590
and wait for the results.

00:05:59.590 --> 00:06:02.070
So did it work?

00:06:02.070 --> 00:06:03.750
Well, yes it did.

00:06:03.750 --> 00:06:06.090
Actually, Keras Tuner
found a way better model.

00:06:06.090 --> 00:06:08.670
Our new model have
now 100% accuracy

00:06:08.670 --> 00:06:11.830
and only takes 24
million parameters.

00:06:11.830 --> 00:06:13.440
So we get a faster
and more accurate

00:06:13.440 --> 00:06:15.287
model thanks to hypertuning.

00:06:18.560 --> 00:06:20.900
Keras Tuner works with
many of the tools you love,

00:06:20.900 --> 00:06:23.180
including TensorBoard,
Colab, and BigQuery,

00:06:23.180 --> 00:06:26.440
and many more to come.

00:06:26.440 --> 00:06:28.940
One more thing.

00:06:28.940 --> 00:06:32.070
Hypertuning takes a long
time so to make everything

00:06:32.070 --> 00:06:33.970
more convenient, we
will be releasing

00:06:33.970 --> 00:06:37.750
alongside with Keras Tuner an
optional cloud service that

00:06:37.750 --> 00:06:39.840
will allow you to monitor
your tuning on the go

00:06:39.840 --> 00:06:43.350
whether it's from your
phone or from your laptop.

00:06:43.350 --> 00:06:46.890
Here is a screenshot of an early
version of the mobile dashboard

00:06:46.890 --> 00:06:49.230
to give you a sense
of what to expect.

00:06:49.230 --> 00:06:51.700
So the design is not
final, but the UI

00:06:51.700 --> 00:06:55.120
will show you how long before
you're tuning complete,

00:06:55.120 --> 00:06:58.120
as well as offer you a visual
summary of the model trained

00:06:58.120 --> 00:07:02.230
so far so you can know
how your tuning is going.

00:07:02.230 --> 00:07:03.653
Thank you for attending today.

00:07:03.653 --> 00:07:05.320
We are really excited
about Keras Tuner.

00:07:05.320 --> 00:07:08.380
And you can sign up today
for the early access program

00:07:08.380 --> 00:07:14.684
by heading to
g.co/research/kerastunereap.

00:07:14.684 --> 00:07:17.299
[APPLAUSE]

00:07:21.090 --> 00:07:24.130
Josh is now going to talk
about [INAUDIBLE] programming

00:07:24.130 --> 00:07:25.500
and TensorFlow.

00:07:30.123 --> 00:07:30.790
JOSH DILLON: OK.

00:07:30.790 --> 00:07:31.417
Hi, I'm Josh.

00:07:31.417 --> 00:07:33.250
And today, we'll be
talking about everyone's

00:07:33.250 --> 00:07:35.660
favorite topic-- probability.

00:07:35.660 --> 00:07:38.090
So let's just start
with a simple example.

00:07:38.090 --> 00:07:40.960
So suppose we're trying to
predict these blue dots--

00:07:40.960 --> 00:07:43.660
that is, the Y-coordinate
from the X-coordinate.

00:07:43.660 --> 00:07:46.690
And Keras makes this
pretty easy to do.

00:07:46.690 --> 00:07:48.880
As you can see here, we
have a dense neural network

00:07:48.880 --> 00:07:51.430
with one hidden layer
outputting one float.

00:07:51.430 --> 00:07:54.910
And that float is the
predicted Y-coordinate.

00:07:54.910 --> 00:07:57.130
And we've chosen mean squared
error as a loss, which

00:07:57.130 --> 00:07:59.770
is a good default choice.

00:07:59.770 --> 00:08:04.240
But the question is, how do we
make our loss function better?

00:08:04.240 --> 00:08:06.340
What does a better loss
function look like?

00:08:06.340 --> 00:08:08.200
And how would we even know?

00:08:08.200 --> 00:08:11.960
How would we evaluate
the fit of this model?

00:08:11.960 --> 00:08:17.080
And so we would like to be able
to specify the negative log

00:08:17.080 --> 00:08:21.720
likelihood as a
generic loss, but then

00:08:21.720 --> 00:08:24.520
encode the distributional
assumptions in the model.

00:08:24.520 --> 00:08:26.830
And furthermore, if
we're doing that,

00:08:26.830 --> 00:08:28.360
then wouldn't it
be nice to get back

00:08:28.360 --> 00:08:29.920
an object for which
we can query--

00:08:29.920 --> 00:08:33.970
ask for the mean, the variance,
the entropy, et cetera.

00:08:33.970 --> 00:08:36.130
And the answer is
we can do this.

00:08:36.130 --> 00:08:39.669
Using a TensorFlow probability
distribution layer,

00:08:39.669 --> 00:08:43.419
we can say that we want the
neural network to output

00:08:43.419 --> 00:08:45.040
a distribution, basically.

00:08:45.040 --> 00:08:46.840
And the way this
works is as you can

00:08:46.840 --> 00:08:50.410
see, the second to the last
layer here outputs one float.

00:08:50.410 --> 00:08:53.210
That float is interpreted
as the location or mean

00:08:53.210 --> 00:08:55.010
of a normal distribution.

00:08:55.010 --> 00:08:57.680
And that's how we can
implement linear regression.

00:08:57.680 --> 00:09:02.973
And as you see here, the fit
is this sort of red line.

00:09:02.973 --> 00:09:04.390
But what's cool
is, we're actually

00:09:04.390 --> 00:09:05.690
outputting a distribution.

00:09:05.690 --> 00:09:06.190
Right?

00:09:06.190 --> 00:09:08.260
So you can take
this output and just

00:09:08.260 --> 00:09:12.020
ask for the entropy or the
variance of the prediction.

00:09:12.020 --> 00:09:12.928
So that's nice.

00:09:12.928 --> 00:09:14.470
But now that we've
done this, there's

00:09:14.470 --> 00:09:16.660
sort of something that
looks a little fishy here.

00:09:16.660 --> 00:09:19.373
We're learning the mean, but
not the standard deviation.

00:09:19.373 --> 00:09:21.040
It seems like maybe
a missed opportunity

00:09:21.040 --> 00:09:22.470
to improve our model.

00:09:22.470 --> 00:09:24.580
And that missed opportunity
is now self-evident

00:09:24.580 --> 00:09:27.400
that we're learning a
distribution directly-- sort

00:09:27.400 --> 00:09:29.530
of an idea that was hidden
in what was otherwise

00:09:29.530 --> 00:09:33.440
the mean square error.

00:09:33.440 --> 00:09:35.740
So to learn standard
deviation, it's just

00:09:35.740 --> 00:09:37.840
another one or two line change.

00:09:37.840 --> 00:09:41.350
Instead of outputting one
float, we now output two.

00:09:41.350 --> 00:09:44.360
One is interpreted as the
mean as before, the location.

00:09:44.360 --> 00:09:46.720
The other when passed
through a soft plus function,

00:09:46.720 --> 00:09:48.882
is now interpreted as
the standard deviation.

00:09:48.882 --> 00:09:50.590
And what you see is
we're now able to get

00:09:50.590 --> 00:09:52.402
this sort of green
line and the red lime--

00:09:52.402 --> 00:09:53.860
green being the
standard deviation,

00:09:53.860 --> 00:09:55.510
red being the mean
fit from before.

00:09:59.700 --> 00:10:01.950
As you can see, sort
of the green line

00:10:01.950 --> 00:10:04.290
diverges as X increases.

00:10:04.290 --> 00:10:06.870
And so that suggests
that our data--

00:10:06.870 --> 00:10:09.420
the variability of
Y, actually-- changes

00:10:09.420 --> 00:10:11.220
as a function of
X. Statisticians

00:10:11.220 --> 00:10:13.260
call this heteroskedasticity.

00:10:13.260 --> 00:10:16.350
But I'd just like to think of
it as learning known unknowns.

00:10:16.350 --> 00:10:18.810
There was variability
present in our data.

00:10:18.810 --> 00:10:21.090
And because we took a
more probabilistic view

00:10:21.090 --> 00:10:26.940
of our model, it was pretty easy
to see how we should fit that.

00:10:26.940 --> 00:10:29.200
So that seems pretty cool.

00:10:29.200 --> 00:10:32.580
But I guess the question
is, now that we're

00:10:32.580 --> 00:10:34.740
thinking about sort
of known unknowns

00:10:34.740 --> 00:10:39.090
or aleatoric uncertainty,
what about unknown unknowns?

00:10:39.090 --> 00:10:42.972
Do we even have enough data
to accurately make the claim

00:10:42.972 --> 00:10:44.430
that this is the
standard deviation

00:10:44.430 --> 00:10:47.350
and mean of this
regression problem?

00:10:47.350 --> 00:10:50.950
And if we don't, how
might we get there?

00:10:50.950 --> 00:10:52.080
And the answer is--

00:10:52.080 --> 00:10:54.690
or an answer-- is
to be Bayesian.

00:10:54.690 --> 00:10:57.480
Rather than to just fit
the weights, if instead we

00:10:57.480 --> 00:11:00.990
think about weights is being
drawn from a distribution

00:11:00.990 --> 00:11:03.240
and try to find what might
be the best posterior

00:11:03.240 --> 00:11:05.910
distribution, then
we can actually

00:11:05.910 --> 00:11:10.720
capture some degree
of unknown unknowns.

00:11:10.720 --> 00:11:12.900
That is, keeping track
of how much evidence

00:11:12.900 --> 00:11:16.167
we have or don't have to make
the prediction we want to make.

00:11:16.167 --> 00:11:18.000
So in practice, this
boils down to something

00:11:18.000 --> 00:11:19.950
that looks a lot like
learning an ensemble.

00:11:19.950 --> 00:11:22.770
As you can see here,
there are numerous random

00:11:22.770 --> 00:11:25.060
draws each
corresponding to a line.

00:11:25.060 --> 00:11:26.820
But what's cool is
computationally,

00:11:26.820 --> 00:11:30.022
we only pay a small additional
overhead for fitting

00:11:30.022 --> 00:11:31.980
what is otherwise an
infinite number of models.

00:11:34.500 --> 00:11:36.190
So that seems pretty cool.

00:11:36.190 --> 00:11:39.550
But we seem to have lost
the aleatoric uncertainty--

00:11:39.550 --> 00:11:41.800
the known unknowns.

00:11:41.800 --> 00:11:44.040
And so can we get it back?

00:11:44.040 --> 00:11:44.970
Yes.

00:11:44.970 --> 00:11:47.100
Since all of this
is modular, you

00:11:47.100 --> 00:11:50.820
can simply specify whatever
assumptions you want to make.

00:11:50.820 --> 00:11:53.490
We're back to fitting
the standard deviation

00:11:53.490 --> 00:11:57.390
by outputting two floats
from that penultimate layer.

00:11:57.390 --> 00:12:00.450
And yet, each of those
dense variational layers

00:12:00.450 --> 00:12:03.150
are sort of representing
an infinite number

00:12:03.150 --> 00:12:04.710
of possible weights.

00:12:04.710 --> 00:12:07.860
So now, we're starting to get
a fairly sophisticated model.

00:12:07.860 --> 00:12:11.190
And to get here, all
we had to do is just

00:12:11.190 --> 00:12:15.180
keep swapping out one sort of
cross layer for a probability

00:12:15.180 --> 00:12:18.870
layer, output a
distribution, change weights

00:12:18.870 --> 00:12:20.580
to be distributions.

00:12:20.580 --> 00:12:23.280
So that's pretty
powerful and yet,

00:12:23.280 --> 00:12:26.430
a sequence of simple changes.

00:12:26.430 --> 00:12:31.770
Of course now we can
ask, what if we are not

00:12:31.770 --> 00:12:35.130
even sure that a line is the
right thing to be fitting here?

00:12:35.130 --> 00:12:37.320
What if we actually want
to think about the loss

00:12:37.320 --> 00:12:39.960
function itself as
being a random variable?

00:12:39.960 --> 00:12:44.670
In this framework where we're
able to just encode ideas

00:12:44.670 --> 00:12:46.800
as random variables,
everything's on the table,

00:12:46.800 --> 00:12:47.910
right?

00:12:47.910 --> 00:12:49.890
So what would that look like?

00:12:49.890 --> 00:12:53.290
And how would we do it?

00:12:53.290 --> 00:12:56.520
The answer in this case is to
use the variational Gaussian

00:12:56.520 --> 00:12:58.230
process layer.

00:12:58.230 --> 00:13:02.710
And from that, we conclude the
data wasn't even linear at all.

00:13:02.710 --> 00:13:06.480
In fact, it had this dampened
sinusoidal structure.

00:13:06.480 --> 00:13:09.690
So no wonder we're having
a hard time fitting it.

00:13:09.690 --> 00:13:11.280
We were using-- in
some sense-- just

00:13:11.280 --> 00:13:13.780
fundamentally the wrong model.

00:13:13.780 --> 00:13:17.220
And the way we got here
is by just questioning

00:13:17.220 --> 00:13:19.920
every assumption in
our model, but not

00:13:19.920 --> 00:13:23.040
having to think about
sort of the relationship

00:13:23.040 --> 00:13:24.750
between different
losses which otherwise

00:13:24.750 --> 00:13:28.200
might seem arbitrary--
rather, a sequence of modeling

00:13:28.200 --> 00:13:30.990
assumptions.

00:13:30.990 --> 00:13:34.170
So how was this all so easy?

00:13:34.170 --> 00:13:37.380
With TensorFlow probability.

00:13:37.380 --> 00:13:39.060
TensorFlow probability
is a toolbox

00:13:39.060 --> 00:13:44.070
for probabilistic modeling built
on or in or using TensorFlow.

00:13:44.070 --> 00:13:45.600
Statisticians and
data scientists

00:13:45.600 --> 00:13:48.340
will be able to write
and launch the same model

00:13:48.340 --> 00:13:50.070
and ML researchers
and practitioners

00:13:50.070 --> 00:13:54.300
can make predictions
with uncertainty.

00:13:54.300 --> 00:13:57.330
You saw just a small part of the
overall TensorFlow probability

00:13:57.330 --> 00:13:58.830
tool box.

00:13:58.830 --> 00:14:02.430
More broadly, it offers
tools for building models

00:14:02.430 --> 00:14:04.710
and for doing inference
within those models.

00:14:04.710 --> 00:14:07.260
On the model building
side, the lowest level

00:14:07.260 --> 00:14:09.930
of most basic abstraction
is distributions.

00:14:09.930 --> 00:14:11.460
You saw the normal distribution.

00:14:11.460 --> 00:14:12.810
It's exactly what you think--

00:14:12.810 --> 00:14:15.000
gamma, exponential, et cetera.

00:14:15.000 --> 00:14:17.610
These are sort of the
building blocks of your model.

00:14:17.610 --> 00:14:20.430
And they're all built to take
advantage of vector processing

00:14:20.430 --> 00:14:23.220
hardware, and in a way
that sort of automatically

00:14:23.220 --> 00:14:25.230
takes advantage of it.

00:14:25.230 --> 00:14:26.730
Next, we have bijectors.

00:14:26.730 --> 00:14:29.610
This is a module for
transforming distributions

00:14:29.610 --> 00:14:32.070
to bestill other distributions.

00:14:32.070 --> 00:14:35.370
Defeomorphisms is the $10
word to describe these.

00:14:35.370 --> 00:14:38.640
And they can range from a simple
sort of exponential logarithm

00:14:38.640 --> 00:14:40.980
transform to more
complicated transforms

00:14:40.980 --> 00:14:44.670
that combined neural nets
with the defeomorphism-- so

00:14:44.670 --> 00:14:48.180
for example, [INAUDIBLE]
regressive flows, real MVP.

00:14:48.180 --> 00:14:52.320
Fairly exotic neural densities
can be built using bijectors.

00:14:52.320 --> 00:14:54.780
You saw layers-- a
few examples of those.

00:14:54.780 --> 00:14:57.030
We also have a number of
losses for making Monte Carlo

00:14:57.030 --> 00:14:58.650
approximations.

00:14:58.650 --> 00:15:00.600
And joint distribution
is an abstraction

00:15:00.600 --> 00:15:04.728
for combining multiple
random variables as one.

00:15:04.728 --> 00:15:07.020
On the inference side of the
fence we have Markov chain

00:15:07.020 --> 00:15:09.353
Monte Carlo-- no probabilistic
modeling toolbox would be

00:15:09.353 --> 00:15:10.472
complete without it--

00:15:10.472 --> 00:15:12.180
within which have
Hamiltonian Monte Carlo

00:15:12.180 --> 00:15:14.013
and a number of other
transition kernels

00:15:14.013 --> 00:15:15.930
which generally take
advantage of TensorFlow's

00:15:15.930 --> 00:15:18.110
automatic differentiation
capability.

00:15:18.110 --> 00:15:20.200
We also tools for
variation inference,

00:15:20.200 --> 00:15:23.720
which turns inference into
an optimization problem.

00:15:23.720 --> 00:15:25.470
And finally, we have
additional optimizers

00:15:25.470 --> 00:15:27.360
that are useful for
probabilistic models--

00:15:27.360 --> 00:15:29.100
for example, quasi
second order methods

00:15:29.100 --> 00:15:33.030
like BFGS as well
as methods that

00:15:33.030 --> 00:15:35.070
don't use the gradient
for cases where that's

00:15:35.070 --> 00:15:38.530
computationally prohibitive.

00:15:38.530 --> 00:15:40.050
So TensorFlow
probability is widely

00:15:40.050 --> 00:15:44.190
used within alphabet, including
Google Brain and DeepMind.

00:15:44.190 --> 00:15:45.720
It also is used externally.

00:15:45.720 --> 00:15:48.780
One of the earliest
adopters is Baker Hughes GE.

00:15:48.780 --> 00:15:53.340
And they use TFP to
basically treat models

00:15:53.340 --> 00:15:56.260
as random variables for
purpose of detecting anomalies.

00:15:56.260 --> 00:15:58.860
So one problem that they're
particularly interested in

00:15:58.860 --> 00:16:01.260
is detecting when jet
engines will fail.

00:16:01.260 --> 00:16:03.240
And luckily, their
dataset doesn't

00:16:03.240 --> 00:16:04.920
have failing jet engines.

00:16:04.920 --> 00:16:06.930
That would be a terrible thing.

00:16:06.930 --> 00:16:11.250
And so we have to be Bayesian
and sort of infer the evidence

00:16:11.250 --> 00:16:13.140
that we don't have.

00:16:13.140 --> 00:16:16.020
So using TensorFlow
probability and TensorFlow,

00:16:16.020 --> 00:16:18.270
they're able to process an
enormous amount of data--

00:16:18.270 --> 00:16:19.980
six terabytes.

00:16:19.980 --> 00:16:24.480
They are able to explore
over 250,000 different model

00:16:24.480 --> 00:16:27.270
architectures and
to great profit,

00:16:27.270 --> 00:16:29.970
seeing a 50% reduction
in false positives

00:16:29.970 --> 00:16:33.610
and a 200% reduction
in false negatives.

00:16:33.610 --> 00:16:36.240
And this sort TensorFlow graph
represents their pipeline--

00:16:36.240 --> 00:16:38.670
the orange boxes you see
here-- heavily use TensorFlow

00:16:38.670 --> 00:16:44.143
probability to, as I said, treat
the model as a random variable.

00:16:44.143 --> 00:16:45.810
So the question I
want to leave you with

00:16:45.810 --> 00:16:47.970
is, who will be the
next success story?

00:16:47.970 --> 00:16:50.880
TensorFlow probability is an
open source Python library

00:16:50.880 --> 00:16:52.830
built using
TensorFlow, which makes

00:16:52.830 --> 00:16:55.560
it easy to combine deep learning
and probabilistic models

00:16:55.560 --> 00:16:56.970
on modern hardware.

00:16:56.970 --> 00:16:59.250
You can pip install
it right now.

00:16:59.250 --> 00:17:01.890
Learn more at
TensorFlow.org/probability

00:17:01.890 --> 00:17:03.010
or shoot us an email.

00:17:03.010 --> 00:17:05.010
If you're interested also
in learning more about

00:17:05.010 --> 00:17:07.369
Bayesian techniques
or just TensorFlow,

00:17:07.369 --> 00:17:10.680
TensorFlow probability, Google
Bayesian Methods for hackers--

00:17:10.680 --> 00:17:12.180
the online version
of this book, we

00:17:12.180 --> 00:17:14.520
rewrote to use
TensorFlow probability.

00:17:14.520 --> 00:17:17.589
I think it's a great way to get
started if you're interested.

00:17:17.589 --> 00:17:19.140
On our GitHub
repository, you'll also

00:17:19.140 --> 00:17:21.630
find numerous
examples, including

00:17:21.630 --> 00:17:23.710
the one you saw today.

00:17:23.710 --> 00:17:24.460
Thank you.

00:17:24.460 --> 00:17:27.132
And with that, Mike will
talk to you TF ranking.

00:17:27.132 --> 00:17:30.084
[APPLAUSE]

00:17:35.063 --> 00:17:36.480
MICHAEL BENDERSKY:
Thank you Josh.

00:17:40.875 --> 00:17:41.500
Hello everyone.

00:17:41.500 --> 00:17:42.380
My name is Michael.

00:17:42.380 --> 00:17:45.140
And today, I'll be talking about
TF ranking, a scalable learning

00:17:45.140 --> 00:17:48.230
to rank library for TensorFlow.

00:17:48.230 --> 00:17:50.030
So first of, I'll
start by defining

00:17:50.030 --> 00:17:51.890
what is learning
to rank, which is

00:17:51.890 --> 00:17:55.310
the problem we're trying to
solve with TensorFlow ranking.

00:17:55.310 --> 00:17:57.500
Imagine you have
a list of items.

00:17:57.500 --> 00:18:01.460
And here, the green shades
indicate the relevance levels

00:18:01.460 --> 00:18:03.200
of these items.

00:18:03.200 --> 00:18:07.520
The goal of learning to rank is
to learn a scoring function, F,

00:18:07.520 --> 00:18:10.310
such as to take a
list of these items

00:18:10.310 --> 00:18:13.220
and produces an optimal
ordering of these items

00:18:13.220 --> 00:18:15.150
in their order of the relevance.

00:18:15.150 --> 00:18:17.990
So the greenest item
would be at the top.

00:18:17.990 --> 00:18:20.970
This seems like a
very abstract problem.

00:18:20.970 --> 00:18:25.080
However, it has a lot of
practical applications.

00:18:25.080 --> 00:18:29.950
In search, we rank documents
in response to user queries.

00:18:29.950 --> 00:18:35.070
In recommendation systems, we
rank items for a given user.

00:18:35.070 --> 00:18:39.990
In dialogue systems, we rank
responses for a user request.

00:18:39.990 --> 00:18:42.690
And similar in questioning
answering systems,

00:18:42.690 --> 00:18:47.440
we rank answers in
response to user questions.

00:18:47.440 --> 00:18:50.130
One very common
application of ranking

00:18:50.130 --> 00:18:52.990
that requires massive
amount of data

00:18:52.990 --> 00:18:55.540
is a click position
optimization.

00:18:55.540 --> 00:18:58.780
In this setting, the function,
F, takes in as an input

00:18:58.780 --> 00:19:01.120
a rank list where
we have some clicks

00:19:01.120 --> 00:19:03.820
on the items in the list.

00:19:03.820 --> 00:19:05.890
The perfect ranking
in this case assumes

00:19:05.890 --> 00:19:07.550
that the click
documents should be

00:19:07.550 --> 00:19:08.800
placed at the top of the list.

00:19:12.570 --> 00:19:17.680
Later in this talk, I will show
an example of this application.

00:19:17.680 --> 00:19:19.720
OK, so let's talk about
TensorFlow ranking or TF

00:19:19.720 --> 00:19:21.750
ranking for short.

00:19:21.750 --> 00:19:27.200
It was first announced on
Google AI blog on December 2018.

00:19:27.200 --> 00:19:29.750
And it's a first
open source library

00:19:29.750 --> 00:19:32.570
that does learning to rank
at scale with deep learning

00:19:32.570 --> 00:19:35.550
approaches.

00:19:35.550 --> 00:19:38.730
It's actively maintained and
developed by the TF franking

00:19:38.730 --> 00:19:41.550
team here at Google.

00:19:41.550 --> 00:19:44.677
And it is fully compatible with
entire TensorFlow ecosystem,

00:19:44.677 --> 00:19:47.010
including tools like TensorBoard
and TensorFlow Serving.

00:19:49.612 --> 00:19:51.070
One interesting
problem which we're

00:19:51.070 --> 00:19:54.040
trying to solve
with TF ranking is

00:19:54.040 --> 00:19:57.460
that unlike classification
or regression metrics,

00:19:57.460 --> 00:20:00.432
ranking metrics are
usually non-convexed.

00:20:00.432 --> 00:20:02.140
In fact, most [INAUDIBLE]
ranking metrics

00:20:02.140 --> 00:20:04.780
are either
discontinuous or flat.

00:20:04.780 --> 00:20:07.300
I'll give an example.

00:20:07.300 --> 00:20:10.180
In this case, we
see a step function.

00:20:10.180 --> 00:20:11.950
And the step here
indicates what happens

00:20:11.950 --> 00:20:16.100
when we change the score
of the items in the list

00:20:16.100 --> 00:20:18.910
and then there is a
rank swap that occurs.

00:20:18.910 --> 00:20:22.510
When the score changes
and the swap occurs,

00:20:22.510 --> 00:20:25.120
we are basically becoming
discontinuous in the function

00:20:25.120 --> 00:20:26.277
space.

00:20:26.277 --> 00:20:28.360
The rest of the function
is flat because we do not

00:20:28.360 --> 00:20:31.630
change the ordering of the
items when we change the scores.

00:20:31.630 --> 00:20:34.900
These types of function are
very difficult or impossible

00:20:34.900 --> 00:20:38.860
to directly optimize
[INAUDIBLE] gradient descent.

00:20:38.860 --> 00:20:41.290
Due to that, researchers
in learning to rank

00:20:41.290 --> 00:20:44.800
have been developing different
types of loss functions.

00:20:44.800 --> 00:20:48.070
One common loss function is
the point [INAUDIBLE] loss,

00:20:48.070 --> 00:20:51.660
where we basically take
as an input each item

00:20:51.660 --> 00:20:55.360
and assign to them a probability
of them being relevant.

00:20:55.360 --> 00:20:57.880
This is very similar to a
classification or regression

00:20:57.880 --> 00:21:01.240
case, but completely
ignores the relationship

00:21:01.240 --> 00:21:04.040
between the different
items in the list.

00:21:04.040 --> 00:21:07.780
So pairwise ranking
losses were proposed.

00:21:07.780 --> 00:21:10.240
These use pair comparisons
to order the list.

00:21:10.240 --> 00:21:12.680
So instead of learning a
probability for each item,

00:21:12.680 --> 00:21:14.200
we learn probabilities
of one item

00:21:14.200 --> 00:21:16.720
being preferable
to another item.

00:21:16.720 --> 00:21:20.080
Again, this does not
capture the entire list--

00:21:20.080 --> 00:21:22.330
just pairs.

00:21:22.330 --> 00:21:24.400
So list-wise ranking
losses were proposed

00:21:24.400 --> 00:21:28.630
instead in the which function,
F, takes one item at a time

00:21:28.630 --> 00:21:32.260
but tries to optimize the
ordering of the entire list

00:21:32.260 --> 00:21:36.070
producing pi star, which
is the optimal permutation

00:21:36.070 --> 00:21:38.930
on the items.

00:21:38.930 --> 00:21:42.680
One new development we
propose in TensorFlow ranking

00:21:42.680 --> 00:21:44.810
is this idea of
multi-item scoring.

00:21:44.810 --> 00:21:47.750
So unlike in the previous slide
where the function, F, takes

00:21:47.750 --> 00:21:51.080
one item at a time, in
multi-item scoring scenario,

00:21:51.080 --> 00:21:53.410
the function, F, takes
all the item in at times

00:21:53.410 --> 00:21:57.800
and produces the optimal
ordering, pi star.

00:21:57.800 --> 00:22:01.130
This is really important for
complex interdependent inputs

00:22:01.130 --> 00:22:03.230
and allows to use the
context of other items

00:22:03.230 --> 00:22:05.018
to make better
ranking decisions.

00:22:08.300 --> 00:22:11.120
One important thing
to note is that we

00:22:11.120 --> 00:22:13.800
support many, many metrics
in TensorFlow ranking.

00:22:13.800 --> 00:22:17.540
So we support standard metrics
like (N)DCG, ARP, Precision@K,

00:22:17.540 --> 00:22:18.890
and others.

00:22:18.890 --> 00:22:20.990
But it's also very easy
to add your own metrics

00:22:20.990 --> 00:22:23.110
to TensorFlow ranking.

00:22:23.110 --> 00:22:25.730
And once you have the
metric you want to optimize

00:22:25.730 --> 00:22:28.305
and you use TensorBoard,
you can easily visualize it

00:22:28.305 --> 00:22:29.430
while you train your model.

00:22:29.430 --> 00:22:32.570
So you can see how your (N)DCG
or other metric progresses

00:22:32.570 --> 00:22:34.160
as you model trains
across the apex.

00:22:36.750 --> 00:22:41.010
So let me jump
into describing how

00:22:41.010 --> 00:22:43.430
you can develop your own state
of the art ready to deploy

00:22:43.430 --> 00:22:45.930
learning to rank model in four
simple steps using TensorFlow

00:22:45.930 --> 00:22:48.420
ranking.

00:22:48.420 --> 00:22:52.410
First, you specify
a scoring function.

00:22:52.410 --> 00:22:57.420
Then you specify the metrics
that you want to optimize for.

00:22:57.420 --> 00:23:00.880
Then you specify
your loss function.

00:23:00.880 --> 00:23:03.120
And finally, you build
your ranking estimate

00:23:03.120 --> 00:23:05.970
using all of these
three previous steps.

00:23:05.970 --> 00:23:07.320
Here how it looks in code.

00:23:10.050 --> 00:23:12.160
First, we define the
scoring function.

00:23:12.160 --> 00:23:17.380
And here, we use three hidden
layer scoring function.

00:23:17.380 --> 00:23:20.390
Then we specify the
evaluation metrics.

00:23:20.390 --> 00:23:22.790
In this case, we
use (N)DCG metrics

00:23:22.790 --> 00:23:24.290
and we use (N)DCG at top ranks.

00:23:27.170 --> 00:23:29.390
Finally, we need to
specify the lowest function

00:23:29.390 --> 00:23:32.240
and the ranking estimator.

00:23:32.240 --> 00:23:38.010
Here, we propose using a ranking
head with a soft max loss.

00:23:38.010 --> 00:23:39.920
However, note the
soft max loss can

00:23:39.920 --> 00:23:42.470
be easily replaced by any other
loss supported by TF ranking.

00:23:42.470 --> 00:23:44.480
So it's very easy to
switch between losses

00:23:44.480 --> 00:23:48.570
by simply replacing this
parameter to something else.

00:23:48.570 --> 00:23:51.633
And finally, you build
the ranking estimator.

00:23:51.633 --> 00:23:53.550
Interesting thing about
the ranking estimator,

00:23:53.550 --> 00:23:58.330
it takes into as a
parameter this group size.

00:23:58.330 --> 00:23:59.760
So this group
size, if you set it

00:23:59.760 --> 00:24:01.720
to something that
is greater than one,

00:24:01.720 --> 00:24:03.930
you're essentially enabling
the multi-item scoring

00:24:03.930 --> 00:24:05.760
I was referring to before.

00:24:05.760 --> 00:24:07.410
If you set up to
one, you fall back

00:24:07.410 --> 00:24:09.160
to using standard learn
to rank approaches

00:24:09.160 --> 00:24:10.790
with single item scoring.

00:24:13.820 --> 00:24:19.800
So let's say in less
than 50 lines of code,

00:24:19.800 --> 00:24:22.040
you're already build your
own learning to rank model.

00:24:22.040 --> 00:24:24.820
And now, you're ready to
train it on your train data.

00:24:28.220 --> 00:24:30.430
OK, I'm just going
to finish this

00:24:30.430 --> 00:24:33.012
by giving an example of how
TF ranking works in practice.

00:24:33.012 --> 00:24:35.470
And I'm going to go back to
the click position optimization

00:24:35.470 --> 00:24:37.120
problem I posed before.

00:24:37.120 --> 00:24:40.840
To remind you, in this case,
the perfect ranking when

00:24:40.840 --> 00:24:44.110
we take in as an
input a click data,

00:24:44.110 --> 00:24:45.608
we produce a rank
list [INAUDIBLE]

00:24:45.608 --> 00:24:47.650
the clicked items will be
at the top of the list.

00:24:51.830 --> 00:24:53.330
We're using an
internal dataset here

00:24:53.330 --> 00:24:57.080
of around one billion
query document pairs.

00:24:57.080 --> 00:24:59.690
And for each query
document pair,

00:24:59.690 --> 00:25:01.880
we extract the following--

00:25:01.880 --> 00:25:03.680
some numerical features
that we associate

00:25:03.680 --> 00:25:07.100
with this query document
pair, the query and document

00:25:07.100 --> 00:25:13.100
text, the position at which
document was displayed.

00:25:13.100 --> 00:25:15.470
And as labels, we use click
or no-click information

00:25:15.470 --> 00:25:17.870
about this particular
document for this query.

00:25:20.543 --> 00:25:22.710
Here, we compared the
performance of your TF ranking

00:25:22.710 --> 00:25:25.440
to lambdaMART which is a
state of the art learning

00:25:25.440 --> 00:25:27.230
to rank approach.

00:25:27.230 --> 00:25:29.860
It's interesting to know that
when you compare TF ranking

00:25:29.860 --> 00:25:31.860
using numerical
features only, it

00:25:31.860 --> 00:25:35.050
is comparable to
lambdaMART on this dataset.

00:25:35.050 --> 00:25:36.990
However, the more
interesting thing

00:25:36.990 --> 00:25:40.670
is that when we add
text as features--

00:25:40.670 --> 00:25:42.860
we achieve big improvements,
especially when

00:25:42.860 --> 00:25:46.640
using TF ranking in
ensemble with lambdaMART.

00:25:46.640 --> 00:25:49.310
On this dataset, we
achieve over 3% gain

00:25:49.310 --> 00:25:51.770
when we're adding sparce
textual features into the model

00:25:51.770 --> 00:25:54.175
and ensembling lambdaMART
and TF ranking, which

00:25:54.175 --> 00:25:55.550
is a very significant
improvement

00:25:55.550 --> 00:25:57.190
for a dataset this size.

00:26:00.043 --> 00:26:01.460
All right, so I
hope I got you all

00:26:01.460 --> 00:26:03.430
excited about using TF ranking.

00:26:03.430 --> 00:26:04.430
What should you do next?

00:26:07.037 --> 00:26:08.870
So you can go and read
our paper and archive

00:26:08.870 --> 00:26:11.090
about TF ranking and
all the work that

00:26:11.090 --> 00:26:13.640
went into developing it.

00:26:13.640 --> 00:26:15.560
Then, check out our
GitHub repository.

00:26:15.560 --> 00:26:18.260
It has all information
about TF ranking.

00:26:18.260 --> 00:26:20.870
And install TF ranking from
there by using pip install.

00:26:23.425 --> 00:26:24.800
You can run through
a simple call

00:26:24.800 --> 00:26:28.417
that we have on our
GitHub repository.

00:26:28.417 --> 00:26:29.000
And that's it.

00:26:29.000 --> 00:26:30.950
You're ready to start
building your next learn

00:26:30.950 --> 00:26:34.430
to rank application.

00:26:34.430 --> 00:26:36.390
I would like to extend
huge thanks to everyone

00:26:36.390 --> 00:26:39.260
on the TF ranking team who
made this project possible.

00:26:39.260 --> 00:26:42.901
And next up is Sofien to talk
about TensorFlow graphics.

00:26:42.901 --> 00:26:47.626
[APPLAUSE]

00:26:47.626 --> 00:26:48.793
SOFIEN BOUAZIZ: Thanks Mike.

00:26:53.212 --> 00:26:55.670
Thanks.

00:26:55.670 --> 00:26:56.520
Hi everyone.

00:26:56.520 --> 00:26:58.040
My name is Sofien.

00:26:58.040 --> 00:27:01.010
And today, I'm proud to
announce the first release

00:27:01.010 --> 00:27:03.860
of a new library called
TensorFlow graphics.

00:27:03.860 --> 00:27:05.720
But before getting
any further, let's

00:27:05.720 --> 00:27:10.310
start from the very beginning
and define computer graphics.

00:27:10.310 --> 00:27:13.950
Computer graphics is a
subfield of computer science

00:27:13.950 --> 00:27:16.820
which studies methods for
digitally synthesizing

00:27:16.820 --> 00:27:19.070
and manipulating visual content.

00:27:19.070 --> 00:27:21.290
Most of you have been
exposed to computer graphics

00:27:21.290 --> 00:27:24.260
through movies and
video games where

00:27:24.260 --> 00:27:26.270
amazingly beautiful
synthetic scenes are

00:27:26.270 --> 00:27:29.150
rendered photo-realistically.

00:27:29.150 --> 00:27:30.950
And this is thanks
to many advances

00:27:30.950 --> 00:27:34.130
in the computer graphics field.

00:27:34.130 --> 00:27:36.140
To give you some
perspective, this

00:27:36.140 --> 00:27:39.490
is what first computer
graphics in 1958

00:27:39.490 --> 00:27:43.190
with the first interactive
game called Tennis for Two.

00:27:43.190 --> 00:27:46.520
As can see, we have
come a long way.

00:27:46.520 --> 00:27:48.730
To generate
beautiful renderings,

00:27:48.730 --> 00:27:51.200
a computer graphics system
needs [INAUDIBLE] input

00:27:51.200 --> 00:27:52.910
in description.

00:27:52.910 --> 00:27:54.860
These often include
transformations

00:27:54.860 --> 00:27:58.030
which explain how the
objects are placed in space,

00:27:58.030 --> 00:28:00.620
camera models which describe
from which point of view

00:28:00.620 --> 00:28:04.390
the scene needs to be rendered,
light and matter models--

00:28:04.390 --> 00:28:08.910
defining object appearances--
and finally [INAUDIBLE]

00:28:08.910 --> 00:28:11.630
geometry.

00:28:11.630 --> 00:28:16.490
These parameters are then
interpreted by renderer

00:28:16.490 --> 00:28:19.528
to generate a beautiful image.

00:28:19.528 --> 00:28:22.550
Now in comparison to
computer graphics,

00:28:22.550 --> 00:28:24.650
computer vision is
concerned with the theory

00:28:24.650 --> 00:28:28.520
beyond artificial system
that extract information

00:28:28.520 --> 00:28:30.660
from images.

00:28:30.660 --> 00:28:32.930
So we can see computer
vision and computer graphics

00:28:32.930 --> 00:28:35.200
as a duality.

00:28:35.200 --> 00:28:38.030
A computer vision system
would start from an image

00:28:38.030 --> 00:28:42.500
and try to automatically
extract a scene description,

00:28:42.500 --> 00:28:44.960
estimating the three dimensional
position and orientation

00:28:44.960 --> 00:28:49.270
of objects, understanding
the material properties,

00:28:49.270 --> 00:28:52.550
or just recognizing these
objects based on their 3D

00:28:52.550 --> 00:28:55.390
geometry.

00:28:55.390 --> 00:28:58.180
Answering these type of
questions about the three

00:28:58.180 --> 00:29:00.730
dimensional world is
fundamental for many machine

00:29:00.730 --> 00:29:02.710
learning applications.

00:29:02.710 --> 00:29:06.160
A good example are autonomous
vehicles and robots

00:29:06.160 --> 00:29:09.530
that need to reason about
three dimensional objects

00:29:09.530 --> 00:29:12.410
and their relationship in space.

00:29:12.410 --> 00:29:15.190
However, to train a
machine learning system

00:29:15.190 --> 00:29:19.960
solving this complex 3D vision
tasks, a large quantity of data

00:29:19.960 --> 00:29:21.250
is needed.

00:29:21.250 --> 00:29:24.490
Labeling data being a
complex and costly process,

00:29:24.490 --> 00:29:26.980
it is important to a mechanism
to design machine learning

00:29:26.980 --> 00:29:30.790
systems that can reason about
the three dimensional world

00:29:30.790 --> 00:29:34.150
while being trained
without much supervision.

00:29:34.150 --> 00:29:37.120
So combining computer
vision and computer graphics

00:29:37.120 --> 00:29:39.760
provides a unique
opportunity to leverage

00:29:39.760 --> 00:29:43.250
a vast amount of readily
available unlabeled data.

00:29:43.250 --> 00:29:45.850
This can be done by a
technique called analysis

00:29:45.850 --> 00:29:48.805
by synthesis where the
vision system extracts

00:29:48.805 --> 00:29:52.170
the scene parameters
and the graphic system

00:29:52.170 --> 00:29:55.600
renders back an
image based on them.

00:29:55.600 --> 00:29:58.840
If the rendering
matches original image,

00:29:58.840 --> 00:30:01.270
the vision system
has done a great job

00:30:01.270 --> 00:30:03.910
at extracting the
correct scene parameters.

00:30:03.910 --> 00:30:07.450
In this setup, computer
vision and computer graphics

00:30:07.450 --> 00:30:11.110
go hand in hand,
forming a single machine

00:30:11.110 --> 00:30:15.360
learning system similar
to a neutron coder.

00:30:15.360 --> 00:30:18.580
TensorFlow graphics
is being developed

00:30:18.580 --> 00:30:20.320
to solve this type of problems.

00:30:20.320 --> 00:30:23.050
And we are aiming at providing
a set of differential graphics

00:30:23.050 --> 00:30:26.500
layer that can be used in your
[INAUDIBLE] machine learning

00:30:26.500 --> 00:30:28.070
models.

00:30:28.070 --> 00:30:31.810
So for the sake of time, we'll
focus on four useful components

00:30:31.810 --> 00:30:34.060
that can be included into
a deep learning models

00:30:34.060 --> 00:30:38.130
to solve these interesting
three division tasks.

00:30:38.130 --> 00:30:40.130
So during the next slide,
you will see a QR code

00:30:40.130 --> 00:30:41.370
like this one.

00:30:41.370 --> 00:30:43.617
If you are interested,
use your smartphone.

00:30:43.617 --> 00:30:45.200
Point your smartphone
toward the slide

00:30:45.200 --> 00:30:47.658
and you will be directed to
the [INAUDIBLE] free resources.

00:30:47.658 --> 00:30:48.810
So get ready.

00:30:48.810 --> 00:30:53.420
These QR codes are going to
come back in later slides.

00:30:53.420 --> 00:30:55.340
OK so now now,
let's jump right in

00:30:55.340 --> 00:30:59.060
and see how 3D transformation
can be expressed

00:30:59.060 --> 00:31:01.790
using TensorFlow graphics.

00:31:01.790 --> 00:31:04.790
One basic building block
for manipulating 3D shapes

00:31:04.790 --> 00:31:06.963
are 3D rotations.

00:31:06.963 --> 00:31:08.630
In TensorFlow graphics,
we are providing

00:31:08.630 --> 00:31:10.520
a set of classical
representation,

00:31:10.520 --> 00:31:14.340
also implemented function,
to convert between them.

00:31:14.340 --> 00:31:17.060
One easy way to
represent rotation

00:31:17.060 --> 00:31:21.250
is by using an axis and an
angle defining our major object

00:31:21.250 --> 00:31:24.110
to rotate around this axis.

00:31:24.110 --> 00:31:26.480
This can be easily expressed
in TensorFlow graphics

00:31:26.480 --> 00:31:29.450
using our transformation
module where in this code,

00:31:29.450 --> 00:31:32.700
we first loads the
vertices of the cube.

00:31:32.700 --> 00:31:36.590
We then define the axis
of rotation and the angle.

00:31:36.590 --> 00:31:38.960
And finally, we apply
the transformation

00:31:38.960 --> 00:31:41.510
to the cube using
rotate function

00:31:41.510 --> 00:31:44.110
of the axis single module.

00:31:44.110 --> 00:31:45.530
Now that we have
seen how easy it

00:31:45.530 --> 00:31:48.570
is to express rotation
in TensorFlow graphics,

00:31:48.570 --> 00:31:52.220
let's move on to camera models.

00:31:52.220 --> 00:31:54.740
Camera models play
a fundamental role

00:31:54.740 --> 00:31:56.765
in computer vision as the
great [? influencer ?]

00:31:56.765 --> 00:32:00.740
of appearances of three
dimensional objects that

00:32:00.740 --> 00:32:02.667
are projected onto
the camera plane.

00:32:02.667 --> 00:32:04.250
As you can see in
this slide, the cube

00:32:04.250 --> 00:32:05.708
appears to be
scanning up and down.

00:32:05.708 --> 00:32:09.960
But it is actually the camera
focal lens that is changing.

00:32:09.960 --> 00:32:11.660
Currently in
TensorFlow graphics,

00:32:11.660 --> 00:32:13.820
we propose two type of cameras--

00:32:13.820 --> 00:32:17.580
an autographic and a
perspective camera model.

00:32:17.580 --> 00:32:21.510
So let's see how the
perspective camera model works.

00:32:21.510 --> 00:32:24.380
One common operation
used with a camera model

00:32:24.380 --> 00:32:28.430
is to project a set of 3D
points onto a 2D camera plane.

00:32:28.430 --> 00:32:31.760
And this can also be expressed
easily with TensorFlow graphics

00:32:31.760 --> 00:32:34.190
where in this code
similarly we first

00:32:34.190 --> 00:32:36.540
load the vertices of the cube.

00:32:36.540 --> 00:32:39.380
We then define the intrinsic
parameters of the camera

00:32:39.380 --> 00:32:44.480
and this allows us to map
the 3D vertices of the cube

00:32:44.480 --> 00:32:47.360
to 2D using the project function
of the perspective camera

00:32:47.360 --> 00:32:50.290
module.

00:32:50.290 --> 00:32:54.000
So far, we
[INAUDIBLE] 3D objects

00:32:54.000 --> 00:32:55.980
and optimize them to 2D.

00:32:55.980 --> 00:32:58.628
But what about the
appearances of these objects?

00:32:58.628 --> 00:33:01.170
In TensorFlow graphics, we're
providing a few simple material

00:33:01.170 --> 00:33:04.920
models are going to render
3D objects using TensorFlow.

00:33:04.920 --> 00:33:07.650
To be slightly more concrete,
these material models

00:33:07.650 --> 00:33:11.530
define how the light reflects
off the surface of an object.

00:33:11.530 --> 00:33:14.040
So given an incoming light
action, how much of the light

00:33:14.040 --> 00:33:16.800
will bounce off the surface
toward a particular outgoing

00:33:16.800 --> 00:33:18.940
direction?

00:33:18.940 --> 00:33:22.590
So the input parameter needed
for the material model are

00:33:22.590 --> 00:33:25.040
the surface [INAUDIBLE]
of the 3D object,

00:33:25.040 --> 00:33:29.470
the incoming light direction,
the outgoing direction--

00:33:29.470 --> 00:33:32.040
for example, pointing
toward the camera--

00:33:32.040 --> 00:33:36.540
and the material parameter-- in
this case, color and shininess.

00:33:36.540 --> 00:33:38.940
Given all these inputs,
TensorFlow graphics

00:33:38.940 --> 00:33:40.680
can evaluate how much
light is reflected

00:33:40.680 --> 00:33:44.220
off the surface and the
outgoing direction which allows

00:33:44.220 --> 00:33:46.730
us to shape the camera pixel.

00:33:46.730 --> 00:33:48.930
And in case this
is all new for you,

00:33:48.930 --> 00:33:50.880
we provide a collab
where you would

00:33:50.880 --> 00:33:54.850
be able to see this
concept in more details.

00:33:54.850 --> 00:33:58.020
OK so now that we have seen
some of [INAUDIBLE] graphics

00:33:58.020 --> 00:34:00.140
functionality that TensorFlow
graphics introduces,

00:34:00.140 --> 00:34:01.890
let's talk about
geometry-- and especially

00:34:01.890 --> 00:34:05.790
how TensorFlow graphics can
help in expressing convolution,

00:34:05.790 --> 00:34:08.540
on measures, and point clouds.

00:34:08.540 --> 00:34:10.980
Image convolution is
a basic building block

00:34:10.980 --> 00:34:11.820
of deep learning.

00:34:11.820 --> 00:34:14.340
They have been extensively
used in many learning tasks

00:34:14.340 --> 00:34:17.400
such as image classification.

00:34:17.400 --> 00:34:19.650
The nice parts of
dealing with images

00:34:19.650 --> 00:34:22.949
is that they are represented
by a uniform grid, which

00:34:22.949 --> 00:34:26.940
makes convolution
[INAUDIBLE] easy to implement

00:34:26.940 --> 00:34:30.420
and consequently, convolution
[INAUDIBLE] networks.

00:34:30.420 --> 00:34:34.659
In TensorFlow image convolution
are readily available.

00:34:34.659 --> 00:34:38.100
However, things become
a bit more complicated

00:34:38.100 --> 00:34:40.739
when dealing with three
dimensional objects which

00:34:40.739 --> 00:34:42.929
are often defined
as measures in point

00:34:42.929 --> 00:34:48.199
clouds and are not
represented as a uniform grid.

00:34:48.199 --> 00:34:51.610
This makes convolution
hard to implement,

00:34:51.610 --> 00:34:55.830
and also now networks
based on them.

00:34:55.830 --> 00:34:59.100
In recent years sensor giving
three dimensional point clouds

00:34:59.100 --> 00:35:00.840
are becoming part
of everyday life

00:35:00.840 --> 00:35:04.830
from smartphone def sensors to
self-driving car [? radars. ?]

00:35:04.830 --> 00:35:06.900
It is therefore
important to open

00:35:06.900 --> 00:35:10.440
source such functionalities
for developers and researchers

00:35:10.440 --> 00:35:12.960
to be able to
efficiently use 3D data

00:35:12.960 --> 00:35:15.670
and extract
information from them.

00:35:15.670 --> 00:35:17.610
In TensorFlow graphics,
we propose a set

00:35:17.610 --> 00:35:19.680
of graph convolution
operators which

00:35:19.680 --> 00:35:22.680
can almost be used as a
drop in with placement

00:35:22.680 --> 00:35:23.880
after convolution.

00:35:23.880 --> 00:35:26.880
So in this code, we
first must load the mesh

00:35:26.880 --> 00:35:31.900
in the form of vertices
and connectivity.

00:35:31.900 --> 00:35:34.110
We then apply one of the
graph convolution operator

00:35:34.110 --> 00:35:35.927
that TensorFlow
graphics provide.

00:35:35.927 --> 00:35:38.010
It is also important to
note that we are providing

00:35:38.010 --> 00:35:40.650
the equivalent Keras layers.

00:35:40.650 --> 00:35:42.900
And finally, the
convolution layer

00:35:42.900 --> 00:35:48.590
can be followed by a classic
[INAUDIBLE],, all the layers.

00:35:48.590 --> 00:35:50.940
To demonstrate how
this can be used

00:35:50.940 --> 00:35:52.930
inside a more complex
neural networks,

00:35:52.930 --> 00:35:55.320
we also provide a
collab doing three

00:35:55.320 --> 00:35:57.970
dimensional semantic human part
segmentation as an example.

00:36:01.240 --> 00:36:03.760
So during the few
slides, we are seeing

00:36:03.760 --> 00:36:06.910
a small set of the TensorFlow
graphics functionalities.

00:36:06.910 --> 00:36:09.950
And the good news is that we
are providing many more of them.

00:36:09.950 --> 00:36:12.420
And we will also add more
of these functionalities

00:36:12.420 --> 00:36:15.670
in the near future.

00:36:15.670 --> 00:36:18.810
But there's also one more thing.

00:36:18.810 --> 00:36:22.130
We are glad to announce
a new TensorFlow

00:36:22.130 --> 00:36:26.000
plug-in allowing measures and
point cloud visualization.

00:36:26.000 --> 00:36:27.860
And I believe that
this plug-in will

00:36:27.860 --> 00:36:31.970
be amazingly useful for
developers and researchers that

00:36:31.970 --> 00:36:36.150
want to use TensorFlow to
analyze three dimensional data.

00:36:36.150 --> 00:36:38.420
So get started today.

00:36:38.420 --> 00:36:39.590
We provide the pre-package.

00:36:39.590 --> 00:36:42.230
And installing the library
is as easy as doing pip

00:36:42.230 --> 00:36:44.910
install tensorflow-graphics.

00:36:44.910 --> 00:36:47.320
In case you are really excited
about TensorFlow graphics,

00:36:47.320 --> 00:36:48.820
we have a GitHub
page from which you

00:36:48.820 --> 00:36:50.880
can pull our latest features.

00:36:50.880 --> 00:36:54.150
We are also providing a
comprehensive API documentation

00:36:54.150 --> 00:36:56.940
and multiple collabs from
which you can learn about some

00:36:56.940 --> 00:37:00.710
of the functionalities
we are providing.

00:37:00.710 --> 00:37:02.660
Before ending the
talk, I would like

00:37:02.660 --> 00:37:04.850
to thank people that
have really contributed

00:37:04.850 --> 00:37:08.180
to make this project happen.

00:37:08.180 --> 00:37:10.070
Thank you very much, everyone.

00:37:10.070 --> 00:37:11.800
I hope you enjoyed
this presentation.

00:37:11.800 --> 00:37:14.850
[MUSIC PLAYING]

