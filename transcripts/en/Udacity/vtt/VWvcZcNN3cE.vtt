WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.922
So today we're going to cover the following broad topics.

00:00:02.922 --> 00:00:06.530
We'll start by returning to the topic of optimizing GPU programs.

00:00:06.530 --> 00:00:09.822
Now in unit 5, we gave some pretty specific, detailed advice.

00:00:09.822 --> 00:00:13.318
And unit 6 explored some examples of how to think parallel.

00:00:13.318 --> 00:00:15.464
Here we're going to back off and talk more generally

00:00:15.464 --> 00:00:18.853
about strategies that parallel programmers use to optimize their programs.

00:00:18.853 --> 00:00:22.588
Now the best kind of programming, as we mentioned briefly in unit 5,

00:00:22.588 --> 00:00:25.930
is the kind of programming that you don't do because somebody else has already programmed

00:00:25.930 --> 00:00:28.594
it for you and packaged into a library you can use.

00:00:28.594 --> 00:00:31.944
So we'll talk about some important and useful CUDA libraries that are out there.

00:00:31.944 --> 00:00:37.568
Some of these libraries are less about packaging up code to solve a particular problem and more about

00:00:37.568 --> 00:00:41.929
providing what I call programming power tools to help code up your own solutions.

00:00:41.929 --> 00:00:46.180
So examples familiar to C++ programmers would be the standard template library,

00:00:46.180 --> 00:00:48.365
or STL, or the boost library.

00:00:48.365 --> 00:00:51.737
We'll discuss a few such power tools for CUDA C++ programmers.

00:00:51.737 --> 00:00:55.615
Now to write this class we focused on the CUDA C++ language,

00:00:55.615 --> 00:00:58.735
but there are many other platforms for parallel programming on GPUs.

00:00:58.735 --> 00:01:04.411
We'll talk about CUDA platforms that support other languages from Fortran to Python to Matlab.

00:01:04.411 --> 00:01:08.972
And we'll also discuss cross platform accelerator solutions like Open CL,

00:01:08.972 --> 00:01:11.285
Open ACC and Open GL compute.

00:01:11.285 --> 00:01:14.169
Now GPU computing is a young field and part of what

00:01:14.169 --> 00:01:17.710
makes it exciting is that the hardware and software are improving each year,

00:01:17.710 --> 00:01:21.463
not just getting incrementally faster but also adding fundamentally new capabilities.

00:01:21.463 --> 00:01:24.565
So we're going to finish the unit and the course with a fantastic

00:01:24.565 --> 00:01:28.785
guest lecture, inviting Dr. Stephen Jones from NVIDIA to come and teach us about

00:01:28.785 --> 00:01:31.955
the latest advance in CUDA called Dynamic Parllelism.

