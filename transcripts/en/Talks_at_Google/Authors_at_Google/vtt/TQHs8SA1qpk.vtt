WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.381
[MUSIC PLAYING]

00:00:07.728 --> 00:00:10.257
SPEAKER: I'm very excited
today to introduce to you

00:00:10.257 --> 00:00:12.590
Cathy O'Neil, who will be
speaking to us about her book,

00:00:12.590 --> 00:00:16.690
"Weapons of Math Destruction."

00:00:16.690 --> 00:00:19.090
Cathy is uniquely
welcome to talk

00:00:19.090 --> 00:00:21.570
about this subject of
mathematical models

00:00:21.570 --> 00:00:23.030
and how they can go bad.

00:00:23.030 --> 00:00:26.200
She has a PhD In math
from Harvard University.

00:00:26.200 --> 00:00:30.450
She has held positions at
Barnard College and MIT.

00:00:30.450 --> 00:00:33.370
She's worked at the
hedge fund DE Shaw,

00:00:33.370 --> 00:00:37.800
and also as a data scientist
at various startups.

00:00:37.800 --> 00:00:41.860
She also is active in the
Occupy Wall Street Movement,

00:00:41.860 --> 00:00:45.090
especially in their
alternative baking group.

00:00:45.090 --> 00:00:47.200
She blogs at mathbabe.org.

00:00:47.200 --> 00:00:49.170
It's a great blog, you
should check it out.

00:00:49.170 --> 00:00:54.080
As well as participates in
the Slate Money podcast.

00:00:54.080 --> 00:00:56.560
And she is the author or
co-author of two other books,

00:00:56.560 --> 00:01:02.270
"Doing Data Science" and
"On Being a Data Skeptic."

00:01:02.270 --> 00:01:05.046
Please join me in
introducing Cathy O'Neil.

00:01:05.046 --> 00:01:08.518
[APPLAUSE]

00:01:09.562 --> 00:01:10.520
CATHY O'NEIL: Hi, guys.

00:01:10.520 --> 00:01:11.730
Thank you so much for coming.

00:01:14.610 --> 00:01:16.970
So I'm a math nerd.

00:01:16.970 --> 00:01:17.786
I love math.

00:01:20.770 --> 00:01:25.430
I was attracted to math in high
school, because it was true.

00:01:25.430 --> 00:01:26.320
It was beautiful.

00:01:26.320 --> 00:01:28.360
Like, you could
disagree with somebody

00:01:28.360 --> 00:01:30.910
about politics and
everything, about what

00:01:30.910 --> 00:01:33.250
Manifest Destiny really meant.

00:01:33.250 --> 00:01:35.880
But you'd have to agree on
math, because you guys would

00:01:35.880 --> 00:01:37.670
be so careful with
your assumptions

00:01:37.670 --> 00:01:40.000
and then your logical arguments.

00:01:40.000 --> 00:01:43.520
I thought of it as something--
wouldn't it be beautiful

00:01:43.520 --> 00:01:45.650
if we could sort of
port this honesty

00:01:45.650 --> 00:01:51.920
and truth and clarity to other
fields of the real world?

00:01:51.920 --> 00:01:57.690
So after I became assistant
professor at Barnard College,

00:01:57.690 --> 00:01:59.840
I realized that it wasn't
the right pace for me,

00:01:59.840 --> 00:02:04.040
that I wanted to have more
to do with the real world.

00:02:04.040 --> 00:02:07.250
And it was 2007, so I did
what everybody else does,

00:02:07.250 --> 00:02:10.780
and I went to a hedge fund.

00:02:10.780 --> 00:02:15.510
And quickly realized that--
well, the world blew up almost

00:02:15.510 --> 00:02:19.020
the moment I stepped
into the hedge fund.

00:02:19.020 --> 00:02:23.780
I was working with the
experts like Larry Summers.

00:02:23.780 --> 00:02:26.300
I worked with him on
a couple projects.

00:02:26.300 --> 00:02:30.090
I was very disillusioned by what
the experts seemed to actually

00:02:30.090 --> 00:02:31.770
know about what was going on.

00:02:31.770 --> 00:02:34.270
But I was most disillusioned--
and actually ashamed,

00:02:34.270 --> 00:02:37.980
to be honest-- about one of
the most fundamental reasons

00:02:37.980 --> 00:02:39.780
that the financial
crisis happened.

00:02:39.780 --> 00:02:41.870
Which was the
triple-A ratings on

00:02:41.870 --> 00:02:43.680
the mortgage-backed
securities, which

00:02:43.680 --> 00:02:45.880
I consider a mathematical lie.

00:02:45.880 --> 00:02:48.377
It was the opposite
of how I had fallen

00:02:48.377 --> 00:02:49.960
in love with
mathematics, as something

00:02:49.960 --> 00:02:51.670
that would bring forth clarity.

00:02:51.670 --> 00:02:58.970
It really was hiding corrupt
practices behind mathematics.

00:02:58.970 --> 00:03:02.090
Like trust the math, be
impressed by the fact

00:03:02.090 --> 00:03:06.550
that we have a bunch of PhDs
in math, who are supposedly

00:03:06.550 --> 00:03:08.260
carefully going
through this data

00:03:08.260 --> 00:03:11.480
to double check, triple check
that these mortgage backed

00:03:11.480 --> 00:03:14.400
securities are very unrisky.

00:03:14.400 --> 00:03:15.940
Trust us.

00:03:15.940 --> 00:03:17.860
There's math here.

00:03:17.860 --> 00:03:23.109
And invest in these
very opaque instruments.

00:03:23.109 --> 00:03:25.400
And I'm not saying that's
the only thing that happened.

00:03:25.400 --> 00:03:30.780
Obviously, the mortgage brokers
made a bunch of bad mortgages.

00:03:30.780 --> 00:03:34.460
But the reason that they were
able to build this machine

00:03:34.460 --> 00:03:37.720
and scale it to the
massive international scale

00:03:37.720 --> 00:03:39.960
that it became was
largely because

00:03:39.960 --> 00:03:41.944
of the triple-A ratings,
because of the trust.

00:03:41.944 --> 00:03:43.360
And that was trust
in mathematics.

00:03:43.360 --> 00:03:46.180
And it was abused.

00:03:46.180 --> 00:03:51.850
I left finance after a
small stint in risk--

00:03:51.850 --> 00:03:53.420
which I could go
into if you guys are

00:03:53.420 --> 00:03:56.270
interested in the Q&amp;A--
where I was equally

00:03:56.270 --> 00:04:03.220
disillusioned by the part I was
playing in particular in risk.

00:04:03.220 --> 00:04:07.310
And I left finance altogether,
started my blog MathBabe.

00:04:07.310 --> 00:04:10.870
The idea was to sort of
expose corrupt practices

00:04:10.870 --> 00:04:13.900
and mathematical
abuse, as I saw it,

00:04:13.900 --> 00:04:15.321
which would happen in finance.

00:04:15.321 --> 00:04:16.820
In the meantime, I
needed a day job.

00:04:16.820 --> 00:04:18.380
So I became a data scientist.

00:04:18.380 --> 00:04:20.170
Very easily, l I should say.

00:04:20.170 --> 00:04:22.640
I just renamed myself
a data scientist

00:04:22.640 --> 00:04:26.520
and got a job the next day.

00:04:26.520 --> 00:04:28.560
Well, because I was qualified.

00:04:28.560 --> 00:04:30.770
Because what do you do when
you're a data scientist?

00:04:30.770 --> 00:04:33.180
You predict people instead
of predicting markets.

00:04:33.180 --> 00:04:36.930
I'd been predicting markets,
futures and credit default

00:04:36.930 --> 00:04:38.090
swaps and stuff.

00:04:38.090 --> 00:04:40.380
So it wasn't that much of a
stretch to predict people.

00:04:40.380 --> 00:04:42.359
In fact, it was
very, very simple.

00:04:42.359 --> 00:04:44.150
And I wanted to think
that what I was doing

00:04:44.150 --> 00:04:51.030
was less destructive than how
I'd been feeling in finance.

00:04:51.030 --> 00:04:54.350
But what I realized pretty
quickly within a year or so

00:04:54.350 --> 00:04:57.710
is that I was just very
much part of something

00:04:57.710 --> 00:05:01.300
that I still
considered not benign.

00:05:01.300 --> 00:05:03.190
Like actually negative.

00:05:03.190 --> 00:05:09.520
And what I realized was that
the difference between the havoc

00:05:09.520 --> 00:05:13.610
that was created by the
financial crisis, where

00:05:13.610 --> 00:05:16.230
everybody had noticed,
and the financial systems

00:05:16.230 --> 00:05:19.010
of the entire
universe-- entire world

00:05:19.010 --> 00:05:22.450
were sort of at risk
and everyone was worried

00:05:22.450 --> 00:05:32.706
was that the havoc
that was being wreaked

00:05:32.706 --> 00:05:34.640
at the data science
world was happening

00:05:34.640 --> 00:05:36.780
at that individual level.

00:05:36.780 --> 00:05:39.840
Individuals were
being lost or they

00:05:39.840 --> 00:05:43.810
were being deemed losers
or winners by this stuff.

00:05:43.810 --> 00:05:46.810
And moreover, what was most
disillusioning and kind of

00:05:46.810 --> 00:05:51.030
frightening for me-- because I
was still kind of idealistic--

00:05:51.030 --> 00:05:55.970
was that these were all
being sort of marketed

00:05:55.970 --> 00:06:01.360
as objective and fair, these
algorithms, when in fact they

00:06:01.360 --> 00:06:03.990
were not that.

00:06:03.990 --> 00:06:07.690
Algorithms are nothing more
than opinions embedded in code.

00:06:07.690 --> 00:06:09.149
So let me give you an example.

00:06:09.149 --> 00:06:10.690
And you guys are
technical, so you'll

00:06:10.690 --> 00:06:12.650
understand this quite well.

00:06:12.650 --> 00:06:15.064
This is my son.

00:06:15.064 --> 00:06:16.480
I like to give the
example of what

00:06:16.480 --> 00:06:18.550
is an algorithm by
using the example

00:06:18.550 --> 00:06:25.560
of my own internal algorithm for
cooking dinner for my family.

00:06:25.560 --> 00:06:29.041
So an algorithm essentially
has two big choices,

00:06:29.041 --> 00:06:30.790
lots of little choices,
which I'll ignore.

00:06:30.790 --> 00:06:33.373
But the big choices are the data
that you train your algorithm

00:06:33.373 --> 00:06:35.970
on, and the
definition of success,

00:06:35.970 --> 00:06:39.157
or the objective function,
which also includes

00:06:39.157 --> 00:06:40.240
your penalty for mistakes.

00:06:40.240 --> 00:06:44.157
But ignoring that for now.

00:06:44.157 --> 00:06:46.740
So what's the data going into
my algorithm for cooking a meal?

00:06:46.740 --> 00:06:52.060
Well, the food I have in my
kitchen, the time I have,

00:06:52.060 --> 00:06:54.200
the ambition I have.

00:06:54.200 --> 00:06:56.210
By the way, I should say,
I curate that already.

00:06:56.210 --> 00:06:58.230
I curate the food.

00:06:58.230 --> 00:06:59.870
I do not consider
ramen noodles food.

00:06:59.870 --> 00:07:01.360
My teenagers do.

00:07:01.360 --> 00:07:03.290
Have teenage sons as well.

00:07:03.290 --> 00:07:07.830
So I'm already imposing
my definition of food.

00:07:07.830 --> 00:07:11.510
And then the definition of
success for me for dinner

00:07:11.510 --> 00:07:14.366
is whether my kids
ate vegetables.

00:07:14.366 --> 00:07:16.740
Now, my 7-year-old, if he were
in charge-- this guy right

00:07:16.740 --> 00:07:18.970
here, Wolfy-- he would
define success for a meal

00:07:18.970 --> 00:07:21.750
is if you got a lot of Nutella.

00:07:21.750 --> 00:07:25.300
And the reason this matters is
because over time, we optimize

00:07:25.300 --> 00:07:26.819
our algorithms for success.

00:07:26.819 --> 00:07:28.110
That's why we define it, right?

00:07:28.110 --> 00:07:31.115
We want to get closer and
closer to that success.

00:07:31.115 --> 00:07:34.754
So over time, that means I make
more and more meals that have

00:07:34.754 --> 00:07:35.920
been successful in the past.

00:07:35.920 --> 00:07:38.600
I train my algorithm to success.

00:07:38.600 --> 00:07:40.410
That is a very
different succession

00:07:40.410 --> 00:07:43.260
of meals than my son's
algorithm would be,

00:07:43.260 --> 00:07:44.740
if we optimized on Nutella.

00:07:44.740 --> 00:07:50.190
So that's me imposing my
agenda onto the algorithm.

00:07:50.190 --> 00:07:51.800
We always do that.

00:07:51.800 --> 00:07:54.360
There is no such thing
as objective algorithm,

00:07:54.360 --> 00:07:57.530
because at the very
least, the person building

00:07:57.530 --> 00:07:59.920
the algorithm defines success.

00:07:59.920 --> 00:08:02.130
And that's usually
success for them.

00:08:02.130 --> 00:08:05.370
But there's often other
stakeholders in play,

00:08:05.370 --> 00:08:06.980
and the question
is, is that success

00:08:06.980 --> 00:08:08.360
for the stakeholders as well?

00:08:08.360 --> 00:08:09.170
It's not always.

00:08:11.850 --> 00:08:14.720
Generally speaking,
the patterns I

00:08:14.720 --> 00:08:17.670
found looking through
more and more data science

00:08:17.670 --> 00:08:21.450
was that we were more and more
marketing these things as fair

00:08:21.450 --> 00:08:24.350
and objective and following
the numbers and mathematical,

00:08:24.350 --> 00:08:27.330
but there were
people behind it who

00:08:27.330 --> 00:08:29.250
were building the
definitions of success--

00:08:29.250 --> 00:08:32.200
these objective functions-- and
they were essentially invisible

00:08:32.200 --> 00:08:34.370
behind that mathematical shield.

00:08:34.370 --> 00:08:38.080
A lot of these algorithms
were benign or fine or great.

00:08:38.080 --> 00:08:39.770
But some of them were
really not great,

00:08:39.770 --> 00:08:41.900
and I focus on those in my book.

00:08:41.900 --> 00:08:43.770
I focus on those,
because I do not

00:08:43.770 --> 00:08:46.700
like vague discussions
about what could go wrong

00:08:46.700 --> 00:08:47.820
with a bunch of big data.

00:08:47.820 --> 00:08:51.170
I want to know exactly what
is going wrong right now.

00:08:51.170 --> 00:08:55.830
It's a kind of triage on
the world of algorithms.

00:08:55.830 --> 00:08:59.270
So I focus specifically on
the worst possible algorithms.

00:08:59.270 --> 00:09:01.697
If you're wondering why
I'm so negative in my book,

00:09:01.697 --> 00:09:02.780
I'm not actually negative.

00:09:02.780 --> 00:09:03.530
I love data.

00:09:03.530 --> 00:09:06.950
I'm a data scientist still,
and I believe in data.

00:09:06.950 --> 00:09:10.560
But I do want to make it
very clear to the public--

00:09:10.560 --> 00:09:12.280
and this book is
written for the public--

00:09:12.280 --> 00:09:15.960
that some of the times, this
stuff is really fucked up.

00:09:15.960 --> 00:09:17.900
And I define that with
three characteristics.

00:09:17.900 --> 00:09:20.399
A weapon of math destruction
is one of these really terrible

00:09:20.399 --> 00:09:21.240
algorithms.

00:09:21.240 --> 00:09:21.920
They have three characteristics.

00:09:21.920 --> 00:09:23.211
One is that they're widespread.

00:09:23.211 --> 00:09:23.820
Important.

00:09:23.820 --> 00:09:26.180
OK, so nobody cares
about the algorithm

00:09:26.180 --> 00:09:28.080
I make food for my kids with.

00:09:28.080 --> 00:09:30.416
That is not important.

00:09:30.416 --> 00:09:31.790
What makes something
important is

00:09:31.790 --> 00:09:33.510
it happens to a lot
of people, and it

00:09:33.510 --> 00:09:35.580
makes big decisions for them.

00:09:35.580 --> 00:09:38.090
Of course, algorithms
don't make big decisions.

00:09:38.090 --> 00:09:40.430
To be clear, it's
humans that have set up

00:09:40.430 --> 00:09:43.860
processes by which a
score of an algorithm

00:09:43.860 --> 00:09:45.550
ends up being a decision.

00:09:45.550 --> 00:09:48.590
But I'm going to-- let's
just put that aside for now.

00:09:48.590 --> 00:09:51.650
If people's life's options--
important life options,

00:09:51.650 --> 00:09:54.800
like whether they get a job,
or whether they go to school,

00:09:54.800 --> 00:09:58.260
whether they get a loan, how
much they pay for insurance,

00:09:58.260 --> 00:09:59.460
how long they go to jail.

00:09:59.460 --> 00:10:01.550
If those kinds of things
are determined in part

00:10:01.550 --> 00:10:04.940
by algorithms, essentially
mostly scoring systems,

00:10:04.940 --> 00:10:05.990
then they're important.

00:10:05.990 --> 00:10:07.460
They're widespread
and important.

00:10:07.460 --> 00:10:09.960
The second characteristic of
things I'm really worried about

00:10:09.960 --> 00:10:12.540
is the mysteriousness,
the secrecy.

00:10:12.540 --> 00:10:14.480
These scoring
algorithms in general

00:10:14.480 --> 00:10:16.810
aren't things that
the formulas are not

00:10:16.810 --> 00:10:21.810
available for people who
are being scored by them.

00:10:21.810 --> 00:10:28.160
Often, they are not even aware
that they are being scored.

00:10:28.160 --> 00:10:31.360
And because they are
important, that is not OK.

00:10:31.360 --> 00:10:35.300
Because they are
tantamount to laws,

00:10:35.300 --> 00:10:37.500
it is not OK for
them to be secret.

00:10:37.500 --> 00:10:39.930
We have a constitutional
right to know what

00:10:39.930 --> 00:10:41.870
the laws of our country are.

00:10:41.870 --> 00:10:43.670
And finally, I care
about algorithms not

00:10:43.670 --> 00:10:46.480
that are improving the world--
I mean, those are great--

00:10:46.480 --> 00:10:48.190
but that are destructive.

00:10:48.190 --> 00:10:50.664
So I'm really focused on
things that are destructive.

00:10:50.664 --> 00:10:51.830
And I mean that in two ways.

00:10:51.830 --> 00:10:53.621
Like the first way I
mean that is that they

00:10:53.621 --> 00:10:55.270
ruin people's lives unfairly.

00:10:55.270 --> 00:10:58.010
So those people
whose important life

00:10:58.010 --> 00:11:01.640
decisions are being
informed by these scores,

00:11:01.640 --> 00:11:05.990
they're getting unfairly
booted from opportunities.

00:11:05.990 --> 00:11:08.670
Opportunities are unfairly
being taken from them.

00:11:08.670 --> 00:11:11.130
Moreover, they're destructive
in a larger sense.

00:11:11.130 --> 00:11:14.070
They actually set out to solve
a problem in a larger sense

00:11:14.070 --> 00:11:16.910
usually-- often with
good intentions.

00:11:16.910 --> 00:11:19.012
But not only do they fail
to solve that problem,

00:11:19.012 --> 00:11:20.720
but they actually make
the problem worse.

00:11:20.720 --> 00:11:23.254
They create usually a
destructive feedback loop.

00:11:23.254 --> 00:11:25.170
And I'm going to give
you a bunch of examples,

00:11:25.170 --> 00:11:27.350
and then we'll have Q&amp;A.

00:11:27.350 --> 00:11:30.020
So the first example comes
from teacher assessment.

00:11:30.020 --> 00:11:33.664
We've had a basically
two-decades long war

00:11:33.664 --> 00:11:34.830
on teachers in this country.

00:11:34.830 --> 00:11:36.740
The idea is we have a
problem with education,

00:11:36.740 --> 00:11:38.740
let's find the bad teachers
and get rid of them.

00:11:38.740 --> 00:11:41.080
And I will put aside
for now the question

00:11:41.080 --> 00:11:43.822
of whether we could
actually solve our problems.

00:11:43.822 --> 00:11:45.530
And the big problem
being the achievement

00:11:45.530 --> 00:11:47.550
gap between rich and poor kids.

00:11:47.550 --> 00:11:49.770
Could we solve that by
getting rid of bad teachers?

00:11:49.770 --> 00:11:50.960
That's a different question.

00:11:50.960 --> 00:11:53.501
The point is that we have been
looking for these bad teachers

00:11:53.501 --> 00:11:56.070
so we can get rid of
them for two decades.

00:11:56.070 --> 00:12:00.850
The first generation of teacher
assessment tools was really,

00:12:00.850 --> 00:12:02.210
really stupid and unfair.

00:12:02.210 --> 00:12:05.540
And here's how it was.

00:12:05.540 --> 00:12:07.970
Look for teachers a
majority of whose students

00:12:07.970 --> 00:12:12.110
did not pass some
proficiency standard

00:12:12.110 --> 00:12:14.470
and their standardized tests.

00:12:14.470 --> 00:12:16.960
Now, the thing you need
to know to understand this

00:12:16.960 --> 00:12:20.060
is that there's a strong
correlation between scores

00:12:20.060 --> 00:12:23.230
on standardized
test and poverty.

00:12:23.230 --> 00:12:25.660
So poor kids just don't do as
well on standardized tests.

00:12:25.660 --> 00:12:27.680
That's not specific
to the United States.

00:12:27.680 --> 00:12:28.920
That's true over time.

00:12:28.920 --> 00:12:30.970
That's true internationally.

00:12:30.970 --> 00:12:35.040
But what it means is that if you
target a teacher, if you label

00:12:35.040 --> 00:12:36.820
them as bad because
a lot of their kids

00:12:36.820 --> 00:12:39.780
do not pass a certain
threshold of proficiency,

00:12:39.780 --> 00:12:43.450
you're basically targeting
teachers of poor kids.

00:12:43.450 --> 00:12:45.520
So that was clearly unfair.

00:12:45.520 --> 00:12:46.610
So that was discarded.

00:12:46.610 --> 00:12:50.270
The second generation was
meant to be more fair,

00:12:50.270 --> 00:12:53.010
and it had good intentions,
but it didn't work.

00:12:53.010 --> 00:12:54.740
And let me tell you what it was.

00:12:54.740 --> 00:12:57.760
It was called the
value-added teacher model.

00:12:57.760 --> 00:13:00.040
And the idea here is
that you can't really

00:13:00.040 --> 00:13:03.231
blame a teacher if a kid gets
like a 60 in fourth grade,

00:13:03.231 --> 00:13:04.730
and they're the
fifth grade teacher.

00:13:04.730 --> 00:13:06.290
You can't blame them
if they don't get 100

00:13:06.290 --> 00:13:07.664
at the end of
fifth grade, right?

00:13:07.664 --> 00:13:09.390
They're starting from 60.

00:13:09.390 --> 00:13:13.400
So it's just better if they
get more than expected, right?

00:13:13.400 --> 00:13:15.490
So the idea was, there
was a primary model, which

00:13:15.490 --> 00:13:19.612
is the expected score for
each student in a class.

00:13:19.612 --> 00:13:21.070
And then there was
the actual score

00:13:21.070 --> 00:13:23.150
that that kid got at
the end of the year.

00:13:23.150 --> 00:13:28.850
And the picture was
basically held accountable

00:13:28.850 --> 00:13:31.010
for the difference
between those two scores.

00:13:31.010 --> 00:13:34.840
So if their expected score
was 62, but they got to 65,

00:13:34.840 --> 00:13:38.160
the teacher was sort of given
credit for those three points.

00:13:38.160 --> 00:13:41.459
If they were supposed to
get a 62, but they got a 55,

00:13:41.459 --> 00:13:44.000
they were dinged for those seven
points that they didn't get.

00:13:44.000 --> 00:13:45.170
Does that makes sense?

00:13:45.170 --> 00:13:48.210
Now the problem with this-- it
actually makes sense, right?

00:13:48.210 --> 00:13:52.170
And if you have like a
Google-level test of this,

00:13:52.170 --> 00:13:54.190
where instead of having
25 kids in the class,

00:13:54.190 --> 00:13:57.430
you had 20,000
kids in this class,

00:13:57.430 --> 00:14:00.760
then the statistical signal
would be pretty clean.

00:14:00.760 --> 00:14:03.330
The problem is we
do not have that.

00:14:03.330 --> 00:14:06.470
So what we have instead
is a pretty bad model,

00:14:06.470 --> 00:14:08.900
which is estimating the
expected score for a kid

00:14:08.900 --> 00:14:10.090
at the end of the year.

00:14:10.090 --> 00:14:12.646
It's actually really
hard to take a kid

00:14:12.646 --> 00:14:14.270
at the end of fourth
grade and estimate

00:14:14.270 --> 00:14:15.700
what they're going to get
at the end of fifth grade.

00:14:15.700 --> 00:14:16.741
It's not a precise thing.

00:14:16.741 --> 00:14:21.240
There's a lot of uncertainty,
especially for kids in poverty.

00:14:21.240 --> 00:14:23.336
Then the second
source of uncertainty

00:14:23.336 --> 00:14:25.710
is like what they actually
got at the end of fifth grade.

00:14:25.710 --> 00:14:27.084
Like, they could
have gotten more

00:14:27.084 --> 00:14:28.930
if they'd taken the
test in the morning

00:14:28.930 --> 00:14:31.555
instead of the afternoon, or in
an air-conditioned room instead

00:14:31.555 --> 00:14:33.970
of a hot room, or after
they ate a meal because they

00:14:33.970 --> 00:14:34.894
were hungry.

00:14:34.894 --> 00:14:37.060
So there's a lot of certainty
on both those numbers.

00:14:37.060 --> 00:14:38.825
And then you're
taking the difference.

00:14:38.825 --> 00:14:40.200
If you think about
statistically,

00:14:40.200 --> 00:14:41.170
that's the noise term.

00:14:41.170 --> 00:14:44.844
It's called the error term
of that expected score model.

00:14:44.844 --> 00:14:47.010
So you're holding teachers
accountable for the error

00:14:47.010 --> 00:14:48.850
terms of their students.

00:14:48.850 --> 00:14:51.630
Turns out, it's not
a very good model.

00:14:51.630 --> 00:14:54.410
It's noisy-- very, very noisy.

00:14:54.410 --> 00:14:57.370
Now, I don't know this
because I have the algorithm.

00:14:57.370 --> 00:14:59.920
Because I tried to get it.

00:14:59.920 --> 00:15:01.480
I did a Freedom
of Information Act

00:15:01.480 --> 00:15:05.719
request for the source
code, but it was denied me.

00:15:05.719 --> 00:15:07.510
I ended up talking to
someone in Wisconsin.

00:15:07.510 --> 00:15:08.980
This is for the New
York City version.

00:15:08.980 --> 00:15:10.280
There's versions all
over the country.

00:15:10.280 --> 00:15:12.190
But I focus on the
New York City version.

00:15:14.948 --> 00:15:16.800
And I eventually
talked to the people

00:15:16.800 --> 00:15:18.842
there in Madison, Wisconsin
who built that model,

00:15:18.842 --> 00:15:21.258
and they explained to me that
I would never get the source

00:15:21.258 --> 00:15:23.922
code for that, because in
fact, nobody in New York City

00:15:23.922 --> 00:15:25.130
got the source code for that.

00:15:25.130 --> 00:15:28.670
Because by the contract,
it was stipulated

00:15:28.670 --> 00:15:31.490
that it was proprietary.

00:15:31.490 --> 00:15:33.340
So no one in the
Department of Education

00:15:33.340 --> 00:15:36.280
understood how these teachers
were being evaluated.

00:15:36.280 --> 00:15:39.770
And yet these scores were being
used to deny people tenure.

00:15:42.540 --> 00:15:44.399
So I should mention
the reason why

00:15:44.399 --> 00:15:46.190
I thought I might be
able to get the source

00:15:46.190 --> 00:15:48.023
code through a Freedom
of Information Act is

00:15:48.023 --> 00:15:49.750
because "The New York
Post" actually did

00:15:49.750 --> 00:15:52.205
get the scores with
all the teacher's names

00:15:52.205 --> 00:15:54.080
through a Freedom of
Information Act request.

00:15:54.080 --> 00:15:57.780
And they published them all as
an act of shaming the teachers

00:15:57.780 --> 00:16:00.175
with bad scores.

00:16:00.175 --> 00:16:02.300
And I thought, OK, maybe
I can get the source code.

00:16:02.300 --> 00:16:03.880
If you can get the
scores, I can get the way

00:16:03.880 --> 00:16:04.830
the scores were made, right?

00:16:04.830 --> 00:16:05.550
No, I couldn't.

00:16:05.550 --> 00:16:07.249
But here's the
thing that happened.

00:16:07.249 --> 00:16:09.790
This really smart high school
math teacher at Stuyvesant High

00:16:09.790 --> 00:16:13.180
School, Gary Rubinstein,
he took those same numbers

00:16:13.180 --> 00:16:14.680
that "The New York
Post" had gotten,

00:16:14.680 --> 00:16:19.080
and he found teachers that had
two scores for the same year.

00:16:19.080 --> 00:16:21.330
You can get two scores if
you teach seventh grade math

00:16:21.330 --> 00:16:22.994
and eighth grade math.

00:16:22.994 --> 00:16:24.660
And he figured, if
they're both supposed

00:16:24.660 --> 00:16:27.720
to be like the final say on
whether you're a good teacher,

00:16:27.720 --> 00:16:29.600
they should be
consistent, right?

00:16:29.600 --> 00:16:32.480
So if you get a 75 for
seventh grade math,

00:16:32.480 --> 00:16:35.450
you should get a 78 maybe
for eighth grade math, right?

00:16:35.450 --> 00:16:37.220
Or a 72.

00:16:37.220 --> 00:16:42.182
So he plotted them
on a scatter plot.

00:16:42.182 --> 00:16:44.930
[LAUGHTER]

00:16:44.930 --> 00:16:47.660
It looks almost like uniform
distribution, which is to say,

00:16:47.660 --> 00:16:49.970
almost random.

00:16:49.970 --> 00:16:51.700
It's actually 24% correlation.

00:16:51.700 --> 00:16:53.221
There is some
signal in it, which

00:16:53.221 --> 00:16:54.720
is why I said the
thing about if you

00:16:54.720 --> 00:16:57.090
had 10,000 kids
in your class, you

00:16:57.090 --> 00:17:01.860
might actually know whether
your teacher brings up scores.

00:17:01.860 --> 00:17:04.710
But it's simply not
good enough to hold

00:17:04.710 --> 00:17:08.060
someone accountable for
that at an individual level.

00:17:08.060 --> 00:17:10.140
But we trusted it
anyway-- when I say we,

00:17:10.140 --> 00:17:12.010
I mean the DOE--
essentially because they

00:17:12.010 --> 00:17:14.170
didn't understand it.

00:17:14.170 --> 00:17:18.930
In spite of that, Sarah
Wysocki, who's pictured here,

00:17:18.930 --> 00:17:21.349
was fired after she
got a bad teacher

00:17:21.349 --> 00:17:24.150
assessment, 50% of which was
her teacher value out of model

00:17:24.150 --> 00:17:28.460
score in Washington DC in 2011.

00:17:28.460 --> 00:17:31.292
I should say it's 50% of her
score, but it's more than 50%

00:17:31.292 --> 00:17:32.500
of the variance of the score.

00:17:32.500 --> 00:17:36.840
The big complaint of the way
that teachers were assessed

00:17:36.840 --> 00:17:40.130
was that everybody got the same
grade from their principal.

00:17:40.130 --> 00:17:41.630
The principals would
categorize them

00:17:41.630 --> 00:17:44.040
all as acceptable teachers.

00:17:44.040 --> 00:17:46.330
So they're like, we need
some spread in these scores

00:17:46.330 --> 00:17:50.500
so we can like rank the teachers
and we find the worst 2%.

00:17:50.500 --> 00:17:55.200
So they got spread, but the
question was, is it meaningful?

00:17:55.200 --> 00:17:58.000
I claim it's not meaningful,
from the scatterplot we just

00:17:58.000 --> 00:17:58.650
saw.

00:17:58.650 --> 00:18:02.280
But it was particularly irksome
for Sarah because guess what?

00:18:02.280 --> 00:18:03.710
She's teaching a fifth grade.

00:18:03.710 --> 00:18:06.340
A bunch of her fourth graders
came in with very high scores

00:18:06.340 --> 00:18:10.430
at the end of fourth grade
who couldn't read or write.

00:18:10.430 --> 00:18:13.930
And it turns out that in
the Washington, DC district,

00:18:13.930 --> 00:18:18.260
Michelle Rhee had instituted
both carrots and sticks.

00:18:18.260 --> 00:18:20.020
You get bonus if you
get a high value out

00:18:20.020 --> 00:18:22.560
of model score or high
teacher assessment.

00:18:22.560 --> 00:18:25.517
You get fired if
you get a bad one.

00:18:25.517 --> 00:18:28.100
Moreover, she found out that the
school where some of her kids

00:18:28.100 --> 00:18:33.190
came from had exceptionally
high eraser rates for their end

00:18:33.190 --> 00:18:34.400
of year standardized tests.

00:18:34.400 --> 00:18:37.070
So she has reason to believe
that the previous teachers

00:18:37.070 --> 00:18:39.114
actually cheated on the test.

00:18:39.114 --> 00:18:40.780
If you think about
what that would mean,

00:18:40.780 --> 00:18:42.890
those kids get elevated
scores, and they're

00:18:42.890 --> 00:18:46.100
expected to keep them elevated
at the end of her class.

00:18:46.100 --> 00:18:48.150
But she just got them.

00:18:48.150 --> 00:18:50.090
She taught them
well, but they didn't

00:18:50.090 --> 00:18:51.750
get ridiculously
high scores, so she

00:18:51.750 --> 00:18:54.600
got punished for the
previous teacher's cheating.

00:18:57.270 --> 00:19:00.680
I should mention that she got a
new job a couple days after she

00:19:00.680 --> 00:19:04.830
got fired in an affluent
suburb of Washington, DC,

00:19:04.830 --> 00:19:09.100
because most of these-- this is
where the value-added model is

00:19:09.100 --> 00:19:09.990
being used.

00:19:09.990 --> 00:19:13.180
I think it's actually more
than that at this point, mostly

00:19:13.180 --> 00:19:15.260
in urban school districts.

00:19:15.260 --> 00:19:19.420
So that brings me
to the failure.

00:19:19.420 --> 00:19:22.160
Remember I said there's
failure on-- well,

00:19:22.160 --> 00:19:23.910
let me just go back
over the three things.

00:19:23.910 --> 00:19:27.940
It's widespread,
secret, and it's

00:19:27.940 --> 00:19:29.920
destructive on an
individual level--

00:19:29.920 --> 00:19:34.170
so we saw Sarah losing her
job-- and at a systemic level.

00:19:34.170 --> 00:19:36.580
I claim that the
value-added model does not

00:19:36.580 --> 00:19:38.037
get rid of bad teachers.

00:19:38.037 --> 00:19:40.620
Remember the whole point was get
rid of the bad teachers, that

00:19:40.620 --> 00:19:42.337
will solve the problem
with education.

00:19:42.337 --> 00:19:43.920
We're not getting
rid of bad teachers.

00:19:43.920 --> 00:19:45.880
We're getting rid
of good teachers.

00:19:45.880 --> 00:19:48.670
We got rid of Sarah,
which was just dumb.

00:19:48.670 --> 00:19:50.730
But there are
countless teachers who

00:19:50.730 --> 00:19:53.575
have quit, who
have retired early,

00:19:53.575 --> 00:19:54.950
who have not gone
into teaching--

00:19:54.950 --> 00:19:56.820
we have a national
teacher shortage--

00:19:56.820 --> 00:19:58.450
or have fled to
the suburbs, where

00:19:58.450 --> 00:20:01.580
they don't use this
arbitrary and punitive regime

00:20:01.580 --> 00:20:03.400
for teachers.

00:20:03.400 --> 00:20:07.840
My next example comes from the
world of personality tests.

00:20:07.840 --> 00:20:09.600
This is Kyle Beam.

00:20:09.600 --> 00:20:12.100
He was a college student
in the Atlanta area.

00:20:12.100 --> 00:20:16.630
He wanted to get a job after
school at a grocery store--

00:20:16.630 --> 00:20:17.710
Kroger's.

00:20:17.710 --> 00:20:21.490
And he took a personality test.

00:20:21.490 --> 00:20:22.061
He failed it.

00:20:22.061 --> 00:20:23.810
Most people never find
out they failed it,

00:20:23.810 --> 00:20:25.601
but he found out,
because his friend worked

00:20:25.601 --> 00:20:27.930
at Kroger's and told him.

00:20:27.930 --> 00:20:29.600
So he was unusual in
finding out that he

00:20:29.600 --> 00:20:31.670
got red lighted from
his algorithm, which

00:20:31.670 --> 00:20:36.040
was built by Chronos, a
small big data company,

00:20:36.040 --> 00:20:38.419
actually around
this neighborhood.

00:20:38.419 --> 00:20:40.210
The other thing that
was unusual about Kyle

00:20:40.210 --> 00:20:42.270
is that his father is a lawyer.

00:20:42.270 --> 00:20:45.060
Most people that are applying
for minimum wage jobs,

00:20:45.060 --> 00:20:47.790
which this was, do not have
fathers that are lawyers.

00:20:47.790 --> 00:20:51.339
So his father said what were
the questions like on this test?

00:20:51.339 --> 00:20:53.380
And Kyle said, they were
a lot like the questions

00:20:53.380 --> 00:20:54.963
I got at the hospital
when I was being

00:20:54.963 --> 00:20:57.530
treated for bipolar disorder.

00:20:57.530 --> 00:20:58.990
The Five Factor
Model it's called.

00:20:58.990 --> 00:21:00.630
And his father said,
well that's illegal.

00:21:00.630 --> 00:21:02.296
Under the Americans
with Disability Act,

00:21:02.296 --> 00:21:05.900
it's illegal to have a
health exam-- including

00:21:05.900 --> 00:21:10.499
a mental health exam-- as
part of a job application.

00:21:10.499 --> 00:21:12.540
And he said, well, can
you apply to other places?

00:21:12.540 --> 00:21:16.280
Kyle ended up applying to six
other places-- Lowe's, Yum

00:21:16.280 --> 00:21:19.280
Food, which owns Taco
Bell, other big companies.

00:21:19.280 --> 00:21:21.690
They all had exactly the
same personality test,

00:21:21.690 --> 00:21:23.650
and he failed all of them.

00:21:23.650 --> 00:21:27.504
So his father is suing those
seven companies as a class

00:21:27.504 --> 00:21:28.920
action lawsuit on
behalf of anyone

00:21:28.920 --> 00:21:34.480
who's ever taken that test on
the grounds that it is illegal,

00:21:34.480 --> 00:21:38.290
and it violates Americans
with Disability Act.

00:21:38.290 --> 00:21:41.810
So that's widespread.

00:21:41.810 --> 00:21:44.040
It's secret.

00:21:44.040 --> 00:21:48.570
I should mention personally
tests are not new.

00:21:48.570 --> 00:21:52.860
RadioShack's old sort of
easily gamed personality test

00:21:52.860 --> 00:21:54.880
looked a lot like this.
"Agree or disagree--

00:21:54.880 --> 00:21:57.420
I'm always happy."

00:21:57.420 --> 00:22:00.860
I think your employer wants
you to agree with that.

00:22:00.860 --> 00:22:04.442
These are the more
recent ones, much harder.

00:22:04.442 --> 00:22:05.670
"What do you agree with more?

00:22:05.670 --> 00:22:08.250
I sometimes get confused by
my thoughts and feelings,

00:22:08.250 --> 00:22:10.458
or I don't really like it
when I have to do something

00:22:10.458 --> 00:22:13.650
I haven't done before."

00:22:13.650 --> 00:22:16.410
Imagine doing 50 of those.

00:22:16.410 --> 00:22:18.188
So it's absolutely inscrutable.

00:22:22.689 --> 00:22:24.730
So it's secret, and I
would say it's destructive.

00:22:24.730 --> 00:22:26.040
It's destructive
for Kyle, because he

00:22:26.040 --> 00:22:28.050
didn't get a job at any
of those big companies

00:22:28.050 --> 00:22:31.477
he applied to in the
entire area of Atlanta.

00:22:31.477 --> 00:22:33.310
But it's also destructive
in a larger sense,

00:22:33.310 --> 00:22:34.810
if what we're worried
about is true,

00:22:34.810 --> 00:22:37.890
that they're actually sort
of systematically denying

00:22:37.890 --> 00:22:42.880
employment to an entire
subpopulation-- people

00:22:42.880 --> 00:22:46.110
with mental health problems.

00:22:46.110 --> 00:22:48.540
It's not relegated
to minimum wage

00:22:48.540 --> 00:22:51.540
work, this idea of
having algorithms

00:22:51.540 --> 00:22:54.790
that sort through resumes.

00:22:54.790 --> 00:22:57.695
Do you guys know who this?

00:22:57.695 --> 00:22:59.680
This is Roger Ailes.

00:22:59.680 --> 00:23:01.790
He recently got kicked
out of Fox News.

00:23:01.790 --> 00:23:03.660
He led Fox News for a long time.

00:23:03.660 --> 00:23:06.580
He got kicked out for like
basically sexually harassing

00:23:06.580 --> 00:23:08.820
women and keeping them
from getting promoted.

00:23:08.820 --> 00:23:10.474
So I was doing a
thought experiment.

00:23:10.474 --> 00:23:12.890
I don't know if Fox News uses
an algorithm to hire people,

00:23:12.890 --> 00:23:14.820
but I'm just imagining
that they do.

00:23:14.820 --> 00:23:16.620
Play along with me.

00:23:16.620 --> 00:23:18.240
What does that
algorithm take in?

00:23:18.240 --> 00:23:22.470
Well, it takes in data, probably
historical data from Fox News.

00:23:22.470 --> 00:23:25.320
So all the history of
people who've applied to be

00:23:25.320 --> 00:23:27.870
say anchors at Fox News.

00:23:27.870 --> 00:23:29.840
And then it takes a
definition of success.

00:23:29.840 --> 00:23:34.740
What does a successful
employee at Fox News look like?

00:23:34.740 --> 00:23:38.300
Let's stipulate a successful
employee at Fox News

00:23:38.300 --> 00:23:41.790
stays for three years and
gets promoted at least once.

00:23:41.790 --> 00:23:46.780
OK, that's typical for how
these algorithms are designed,

00:23:46.780 --> 00:23:48.890
and how they're trained.

00:23:48.890 --> 00:23:50.700
So if you think about
what that means,

00:23:50.700 --> 00:23:53.650
the algorithm, if you've trained
on a lot of historical data,

00:23:53.650 --> 00:23:55.770
what it will do
is given a new set

00:23:55.770 --> 00:23:59.000
of applicants for an
anchor job, it will say,

00:23:59.000 --> 00:24:01.490
who among these new
applicants looks like someone

00:24:01.490 --> 00:24:04.980
who was successful in the past?

00:24:04.980 --> 00:24:09.230
And I would not be surprised
if it filtered out women,

00:24:09.230 --> 00:24:13.290
because not only did
women not necessarily

00:24:13.290 --> 00:24:16.920
get that job as often, but
when they did get the job,

00:24:16.920 --> 00:24:19.420
they were systematically
pushed out

00:24:19.420 --> 00:24:21.800
by Roger Ailes and the
entire culture there.

00:24:21.800 --> 00:24:22.990
Does that make sense?

00:24:22.990 --> 00:24:27.720
So the point here-- and this
is a very important point--

00:24:27.720 --> 00:24:34.210
is that algorithms
are not inherently

00:24:34.210 --> 00:24:37.300
objective or fair all
they are doing there

00:24:37.300 --> 00:24:40.610
what they're really good at
is picking out past patterns

00:24:40.610 --> 00:24:42.310
and repeating them.

00:24:42.310 --> 00:24:46.720
Which is to say, if we had a
perfect way of hiring people,

00:24:46.720 --> 00:24:48.050
we would want to codify that.

00:24:48.050 --> 00:24:50.070
We would want to
automate that, because it

00:24:50.070 --> 00:24:52.190
would save us money and time.

00:24:52.190 --> 00:24:55.910
But until we have a perfect way
of doing this, all we're doing

00:24:55.910 --> 00:24:58.895
is we are literally
codifying past practices

00:24:58.895 --> 00:24:59.770
and propagating them.

00:25:04.590 --> 00:25:08.290
My last example comes
from criminal justice.

00:25:08.290 --> 00:25:09.790
There's actually
two different kinds

00:25:09.790 --> 00:25:13.240
of algorithms that worry me
a lot in criminal justice.

00:25:13.240 --> 00:25:16.530
The first is
predictive policing.

00:25:16.530 --> 00:25:18.770
You guys know all about
the Black Lives Matter

00:25:18.770 --> 00:25:22.846
movement, which is objecting
to black people getting shot.

00:25:22.846 --> 00:25:24.470
But it's not just of
course being shot.

00:25:24.470 --> 00:25:27.280
It's just the amount of
over-policing and uneven

00:25:27.280 --> 00:25:30.210
policing that's happening
to poor black communities.

00:25:30.210 --> 00:25:32.020
Some evidence.

00:25:32.020 --> 00:25:34.550
There's whites and
blacks smoking pot

00:25:34.550 --> 00:25:36.930
at similar rates,
whites smoking pot

00:25:36.930 --> 00:25:40.060
a little bit more
if they're young.

00:25:40.060 --> 00:25:45.020
But blacks get arrested a lot
more-- a lot more for that.

00:25:45.020 --> 00:25:48.200
In fact, depending
on the jurisdiction,

00:25:48.200 --> 00:25:51.310
blacks can get arrested
up to 10 times more

00:25:51.310 --> 00:25:55.745
often than whites
for smoking pot.

00:25:55.745 --> 00:25:57.870
So the two things I want
you to take away from this

00:25:57.870 --> 00:26:02.210
are it's biased against
blacks, and actually it

00:26:02.210 --> 00:26:06.100
depends very much on local
conditions of how the police

00:26:06.100 --> 00:26:08.560
force is expected to act.

00:26:08.560 --> 00:26:10.190
So when you hear
about arrest records,

00:26:10.190 --> 00:26:14.380
I want you to think just as
much about police practices

00:26:14.380 --> 00:26:20.300
as you do about crime, at least
when it's nonviolent crime.

00:26:20.300 --> 00:26:23.440
And I do make a distinction
between violent crime

00:26:23.440 --> 00:26:24.630
and nonviolent crime.

00:26:24.630 --> 00:26:28.330
The problem is that nonviolent
crime is much more prevalent,

00:26:28.330 --> 00:26:31.730
and it is much more predictable.

00:26:31.730 --> 00:26:32.540
Think about it.

00:26:32.540 --> 00:26:36.320
We have people being arrested in
this country for mental health

00:26:36.320 --> 00:26:39.310
problems-- when they're
poor, not when they're rich.

00:26:39.310 --> 00:26:41.720
We have people being arrested
for addiction problems

00:26:41.720 --> 00:26:43.420
when they're poor.

00:26:43.420 --> 00:26:45.590
And we have people being
arrested much more often

00:26:45.590 --> 00:26:49.680
for low level, nonviolent
crime like drug use.

00:26:49.680 --> 00:26:51.760
And for that matter, for
like crimes of poverty.

00:26:51.760 --> 00:26:55.040
Just like literally not having
a place to go to the bathroom.

00:26:55.040 --> 00:26:58.420
And that goes into
their arrest records.

00:26:58.420 --> 00:26:59.920
And the reason I'm
mentioning this

00:26:59.920 --> 00:27:02.420
is because the way predictive
policing works is very simple,

00:27:02.420 --> 00:27:04.050
and it's actually very stupid.

00:27:04.050 --> 00:27:08.120
It's geolocated arrest records.

00:27:08.120 --> 00:27:11.110
And the algorithm
says go put police

00:27:11.110 --> 00:27:14.960
where we saw a crime in the
past, which is really where

00:27:14.960 --> 00:27:16.330
we saw arrests in the past.

00:27:16.330 --> 00:27:20.380
Where we saw police arresting
people for things in the past.

00:27:20.380 --> 00:27:23.940
This is just as much a
way of predicting police

00:27:23.940 --> 00:27:26.070
as it is of predicting crime.

00:27:26.070 --> 00:27:27.800
I'm just reiterating that.

00:27:27.800 --> 00:27:32.210
It might be different, by the
way, if we really focused only

00:27:32.210 --> 00:27:33.180
on violent crime.

00:27:33.180 --> 00:27:33.865
Imagine that.

00:27:33.865 --> 00:27:35.490
Imagine a predictive
policing algorithm

00:27:35.490 --> 00:27:37.614
where they only focus on
violent crime like murder.

00:27:40.760 --> 00:27:44.040
The problem is that murder
is really hard to predict.

00:27:44.040 --> 00:27:45.730
Like even if you
did predict murder,

00:27:45.730 --> 00:27:49.180
would you what, stand
outside a house waiting

00:27:49.180 --> 00:27:50.460
for someone to get murdered?

00:27:50.460 --> 00:27:51.918
The problem is that
the things that

00:27:51.918 --> 00:27:56.990
are actually easy to predict
are things like poverty.

00:27:56.990 --> 00:28:00.320
So my claim is that predictive
policing is more or less

00:28:00.320 --> 00:28:02.120
creating a feedback
loop, where you're

00:28:02.120 --> 00:28:06.200
having a pseudo-scientific
basis for sending police back

00:28:06.200 --> 00:28:09.260
to neighborhoods that
are already over policed.

00:28:09.260 --> 00:28:11.990
Said another way, another
thought experiment,

00:28:11.990 --> 00:28:14.172
imagine that if after
the financial crisis,

00:28:14.172 --> 00:28:16.130
all the cops had been
told to go to Wall Street

00:28:16.130 --> 00:28:18.850
and arrest the
bankers and find out

00:28:18.850 --> 00:28:20.540
if they had cocaine
in their pockets,

00:28:20.540 --> 00:28:23.120
because they all
do-- not all of them.

00:28:23.120 --> 00:28:28.550
Then the police records-- the
police data, which is just

00:28:28.550 --> 00:28:31.314
a reflection of what police
do-- the data would tell them

00:28:31.314 --> 00:28:32.980
in these predictive
policing algorithms,

00:28:32.980 --> 00:28:35.440
go back to Wall Street, because
that's where the crime is.

00:28:35.440 --> 00:28:36.564
But that's not what we did.

00:28:41.460 --> 00:28:45.140
So the next example also in
the criminal justice system

00:28:45.140 --> 00:28:47.520
is recidivism risk.

00:28:47.520 --> 00:28:49.140
This is a score.

00:28:49.140 --> 00:28:51.840
Recidivism, by the way,
is coming back to jail.

00:28:51.840 --> 00:28:54.770
So recidivism risk is the risk
somebody comes back to jail.

00:28:54.770 --> 00:28:57.190
Recidivism risk scores are
given to judges when they

00:28:57.190 --> 00:29:01.110
sentence criminal defendants.

00:29:01.110 --> 00:29:03.570
Now, there's two kinds of
data that goes into that.

00:29:03.570 --> 00:29:04.650
One is arrest records.

00:29:04.650 --> 00:29:06.150
We just talked about
arrest records.

00:29:06.150 --> 00:29:07.620
They're very biased.

00:29:07.620 --> 00:29:10.320
The second is a questionnaire.

00:29:10.320 --> 00:29:12.160
The most commonly
used recidivism

00:29:12.160 --> 00:29:14.137
risk algorithm is
called the LSIR.

00:29:14.137 --> 00:29:16.470
And I'm going to show you a
couple of the questions that

00:29:16.470 --> 00:29:19.180
are asked of the
defendant in the LSIR.

00:29:21.950 --> 00:29:25.120
Number 29. "Do you live in
a high crime neighborhood?"

00:29:25.120 --> 00:29:28.160
It's a proxy for class
and race, because that

00:29:28.160 --> 00:29:30.896
is pointing to people who
are already poor and black.

00:29:35.970 --> 00:29:38.470
And by the way, if you say
yes or no to these things,

00:29:38.470 --> 00:29:40.040
it goes exactly as you imagine.

00:29:40.040 --> 00:29:43.830
If you say yes to "I have
a mental health problem,"

00:29:43.830 --> 00:29:46.360
you're a higher risk.

00:29:46.360 --> 00:29:49.590
And I forgot to mention,
if you're a higher risk,

00:29:49.590 --> 00:29:51.730
a judge will sentence
you to longer in jail.

00:29:54.570 --> 00:29:57.240
I mean, I mention that
because it's not obvious

00:29:57.240 --> 00:29:59.170
that that's what
you would do, right?

00:29:59.170 --> 00:30:01.100
Like, if you're a higher
risk of recidivism,

00:30:01.100 --> 00:30:03.669
you get put in jail longer.

00:30:03.669 --> 00:30:05.210
It's a little bit
minority reportish,

00:30:05.210 --> 00:30:07.210
because what you're doing
is you're preemptively

00:30:07.210 --> 00:30:09.840
punishing someone for something
they haven't done yet.

00:30:09.840 --> 00:30:13.580
But that is the practice
that judges now have.

00:30:13.580 --> 00:30:16.556
Here's another set of questions.

00:30:16.556 --> 00:30:19.300
"Have you been
suspended from school?"

00:30:19.300 --> 00:30:20.540
Number 17.

00:30:20.540 --> 00:30:26.670
I'll show you a plot of just how
much that is a proxy for race.

00:30:26.670 --> 00:30:28.600
Black girls and
boys are much more

00:30:28.600 --> 00:30:31.760
likely to be
suspended from school.

00:30:31.760 --> 00:30:33.980
But I think the thing
that bothers me the most--

00:30:33.980 --> 00:30:36.250
and I think should bother
absolutely everyone-- is

00:30:36.250 --> 00:30:38.096
number 26.

00:30:38.096 --> 00:30:42.620
"Was somebody in your
family in prison?"

00:30:42.620 --> 00:30:44.890
This is something
that in an open court

00:30:44.890 --> 00:30:48.370
would be thrown out by a
judge as unconstitutional.

00:30:48.370 --> 00:30:51.070
If a lawyer said, "Your honor,
please sentence this person

00:30:51.070 --> 00:30:53.740
to longer, because their
father was also a criminal."

00:30:53.740 --> 00:30:56.060
That is not how we do it.

00:30:56.060 --> 00:30:58.770
But because it's being
embedded in a risk score, which

00:30:58.770 --> 00:31:01.170
is being claimed
to be scientific,

00:31:01.170 --> 00:31:03.860
this somehow has
the authenticity

00:31:03.860 --> 00:31:06.910
of mathematics and
science, and again,

00:31:06.910 --> 00:31:08.995
is being used to send
people to jail for longer.

00:31:12.170 --> 00:31:15.470
So I talk about failures.

00:31:15.470 --> 00:31:17.740
I should also mention,
this is important.

00:31:17.740 --> 00:31:20.470
It's being used in more
than half the states.

00:31:20.470 --> 00:31:21.500
It's secret.

00:31:21.500 --> 00:31:23.660
People do not
understand what they're

00:31:23.660 --> 00:31:25.030
getting into with these scores.

00:31:25.030 --> 00:31:28.830
And judges are actually very
secretive about exactly how

00:31:28.830 --> 00:31:31.710
much weight they
put on these scores.

00:31:31.710 --> 00:31:34.780
But my claim is that they are
the most destructive thing

00:31:34.780 --> 00:31:36.070
you can imagine.

00:31:36.070 --> 00:31:37.460
They create their own reality.

00:31:37.460 --> 00:31:40.029
If you're a high risk, you're
sentenced to jail for longer.

00:31:40.029 --> 00:31:40.570
I guess what?

00:31:40.570 --> 00:31:43.120
If you're sentenced to prison--
if you're in prison longer,

00:31:43.120 --> 00:31:46.030
you don't tend to benefit
from that experience.

00:31:46.030 --> 00:31:50.560
You end up out of jail-- 97% of
people eventually leave prison,

00:31:50.560 --> 00:31:51.800
I should mention.

00:31:51.800 --> 00:31:53.310
So this happens.

00:31:53.310 --> 00:31:55.050
But you end up
with no resources,

00:31:55.050 --> 00:31:58.417
no connections to your
community, very little wealth.

00:31:58.417 --> 00:32:00.000
You have a felony
to your name, often,

00:32:00.000 --> 00:32:02.360
so you it's hard to get a job.

00:32:02.360 --> 00:32:04.410
And then you end
up back in prison,

00:32:04.410 --> 00:32:08.011
partly because you've
got this high risk score.

00:32:08.011 --> 00:32:09.760
So you were deemed
high risk, and then you

00:32:09.760 --> 00:32:12.520
end up back in prison.

00:32:12.520 --> 00:32:13.670
So I'm almost done.

00:32:13.670 --> 00:32:14.760
Those are my examples.

00:32:14.760 --> 00:32:19.590
But I do want to mention
again that I don't hate data.

00:32:19.590 --> 00:32:22.410
I just really think
we have not yet

00:32:22.410 --> 00:32:27.240
started understanding what it
means to build safe algorithms.

00:32:27.240 --> 00:32:29.344
It's like we are
building cars and just

00:32:29.344 --> 00:32:31.260
putting them on the road
without understanding

00:32:31.260 --> 00:32:34.680
that cars can kill people.

00:32:34.680 --> 00:32:36.640
So I want data
scientists to take

00:32:36.640 --> 00:32:39.330
their ethical
responsibility seriously,

00:32:39.330 --> 00:32:41.920
which means building some
kind of ethical framework,

00:32:41.920 --> 00:32:46.700
like a Hippocratic oath
for data scientists.

00:32:46.700 --> 00:32:48.550
I also think that
we need to learn

00:32:48.550 --> 00:32:50.980
how to scrutinize these
algorithms, to monitor them,

00:32:50.980 --> 00:32:53.970
to audit them for
safety, for fairness,

00:32:53.970 --> 00:32:57.040
and for discrimination, and
for meaning-- making sure

00:32:57.040 --> 00:33:00.400
that we're actually
building meaningful things.

00:33:00.400 --> 00:33:04.230
The teacher value-added
model was not meaningful.

00:33:04.230 --> 00:33:07.000
And I also think
that in situations

00:33:07.000 --> 00:33:10.604
where it's very, very important
to a given person's life

00:33:10.604 --> 00:33:12.770
how they're being scored,
they should have the right

00:33:12.770 --> 00:33:14.810
to scrutinize that score.

00:33:14.810 --> 00:33:16.000
Like the teachers.

00:33:16.000 --> 00:33:18.780
If they are being given
a score that will maybe

00:33:18.780 --> 00:33:21.330
make them fired, they should
be able to understand exactly

00:33:21.330 --> 00:33:22.580
how that scoring system works.

00:33:22.580 --> 00:33:24.380
Because to be
clear, the teachers

00:33:24.380 --> 00:33:26.600
were not-- even when they
appealed their score,

00:33:26.600 --> 00:33:29.796
they were not told how they
were being actually evaluated.

00:33:29.796 --> 00:33:30.670
And that's not right.

00:33:33.570 --> 00:33:35.650
Thank you guys.

00:33:35.650 --> 00:33:37.930
[APPLAUSE]

00:33:40.230 --> 00:33:42.245
If you have any
questions, I'm here.

00:33:42.245 --> 00:33:44.620
AUDIENCE: What's the best way
to account for and mitigate

00:33:44.620 --> 00:33:47.600
limitations in a model?

00:33:47.600 --> 00:33:50.690
So suppose you have to come
up with a mathematical model

00:33:50.690 --> 00:33:54.330
to effect somebody's
life, how do you

00:33:54.330 --> 00:33:56.910
figure out where the
boundaries of application are?

00:33:56.910 --> 00:33:59.690
And how do you I guess
iteratively readdress

00:33:59.690 --> 00:34:03.295
that to see if it's doing
what you expect it to?

00:34:03.295 --> 00:34:05.420
CATHY O'NEIL: I mean, it's
really a vague question,

00:34:05.420 --> 00:34:06.670
so it's hard for me to answer.

00:34:10.110 --> 00:34:12.659
I'll tell you how frustrating
it is as a data scientist.

00:34:12.659 --> 00:34:13.800
I'm a data scientist.

00:34:13.800 --> 00:34:17.370
Like, I worked in the
city hall of New York.

00:34:17.370 --> 00:34:20.840
And I was asked to
use the data they

00:34:20.840 --> 00:34:23.140
had to figure out how
long a family was going

00:34:23.140 --> 00:34:24.264
to be in homeless services.

00:34:27.150 --> 00:34:29.469
And I had race, I had
the number of children,

00:34:29.469 --> 00:34:34.000
I had how whether the parents
had been in social services.

00:34:34.000 --> 00:34:35.840
I had all this data.

00:34:35.840 --> 00:34:38.290
But they weren't
telling me how they

00:34:38.290 --> 00:34:40.739
were going to use this
algorithm-- this scoring

00:34:40.739 --> 00:34:42.990
system, once I had it.

00:34:42.990 --> 00:34:44.489
So in particular,
one thing I didn't

00:34:44.489 --> 00:34:48.659
know whether I should use
was the attribute race,

00:34:48.659 --> 00:34:50.550
or all the other
things that are proxies

00:34:50.550 --> 00:34:51.050
to race.

00:34:51.050 --> 00:34:55.510
Or should I decorrelate those
other things from the race?

00:34:55.510 --> 00:34:57.160
How is this going to be used?

00:34:57.160 --> 00:34:58.975
Number one possibility.

00:34:58.975 --> 00:35:00.350
If you are high
risk, then you're

00:35:00.350 --> 00:35:02.420
going to be put
into worse housing.

00:35:02.420 --> 00:35:05.030
If you're expected to
be in long term housing,

00:35:05.030 --> 00:35:07.910
you're going to put
into worse housing.

00:35:07.910 --> 00:35:10.200
If I if I knew that was how
it was going to be used,

00:35:10.200 --> 00:35:13.420
then I would know that
people at higher risk--

00:35:13.420 --> 00:35:16.730
there'd be a disproportionate
impact racially, right?

00:35:16.730 --> 00:35:19.380
But if instead it was being
used to sort of figure

00:35:19.380 --> 00:35:25.490
out interventions
so that-- maybe they

00:35:25.490 --> 00:35:27.810
wanted to understand
why black families were

00:35:27.810 --> 00:35:30.610
in homeless services for
longer, and to intervene

00:35:30.610 --> 00:35:33.404
to try to make that
discrepancy smaller,

00:35:33.404 --> 00:35:35.070
then it would be by
all means make sense

00:35:35.070 --> 00:35:36.670
to have race as the attribute.

00:35:36.670 --> 00:35:37.590
Does that make sense?

00:35:37.590 --> 00:35:40.630
So I'm basically not
answering your question.

00:35:40.630 --> 00:35:44.630
But I am making it clear that
you cannot answer that question

00:35:44.630 --> 00:35:49.110
until you really know the
use case for this algorithm.

00:35:49.110 --> 00:35:51.350
And I should also
add that a given

00:35:51.350 --> 00:35:54.910
algorithm could have a
positive or negative effect

00:35:54.910 --> 00:35:56.210
on the world.

00:35:56.210 --> 00:35:59.250
It's really tricky.

00:35:59.250 --> 00:36:01.230
Let me give you an example.

00:36:01.230 --> 00:36:02.024
Health.

00:36:02.024 --> 00:36:03.440
Everyone's talking
about how great

00:36:03.440 --> 00:36:06.620
it is that we can predict
each other's health.

00:36:06.620 --> 00:36:09.240
Well that's great if your
doctor has that algorithm

00:36:09.240 --> 00:36:11.390
and can keep you well.

00:36:11.390 --> 00:36:14.460
But it's not great if
an insurance company

00:36:14.460 --> 00:36:16.380
has that algorithm and
can charge you more

00:36:16.380 --> 00:36:17.580
if you're about to get sick.

00:36:17.580 --> 00:36:21.880
Or if Walmart-- and I'm not
saying this is happening--

00:36:21.880 --> 00:36:25.280
but if Walmart put a health
risk on top of everybody who's

00:36:25.280 --> 00:36:27.067
applying to their
job, and says we

00:36:27.067 --> 00:36:29.150
don't want to spend people's
money on our people's

00:36:29.150 --> 00:36:31.400
insurance, so we're
going to not hire people

00:36:31.400 --> 00:36:32.450
with high health risks.

00:36:32.450 --> 00:36:35.970
I mean, just saying, same
algorithm, different use cases.

00:36:35.970 --> 00:36:37.290
Could be used for good or bad.

00:36:37.290 --> 00:36:42.390
So the answer is,
there is no rule here.

00:36:42.390 --> 00:36:45.250
Algorithms are decision
making processes

00:36:45.250 --> 00:36:47.970
that are as complicated
as anything in the world.

00:36:47.970 --> 00:36:50.440
So we can't pretend
that there are

00:36:50.440 --> 00:36:53.347
like formulas for how to use
them and how to make them safe.

00:36:53.347 --> 00:36:54.596
They're very, very contextual.

00:36:57.320 --> 00:37:00.520
AUDIENCE: Do you have any sense
of-- this is probably a naive

00:37:00.520 --> 00:37:06.100
question-- whether the
scale of the problem

00:37:06.100 --> 00:37:10.440
is bigger or smaller in a
country with stronger libel

00:37:10.440 --> 00:37:14.722
and slander laws, like
for example, England?

00:37:14.722 --> 00:37:15.680
CATHY O'NEIL: Say more.

00:37:15.680 --> 00:37:18.392
Why would you imagine that?

00:37:18.392 --> 00:37:21.370
AUDIENCE: Oh, why would I
imagine it would be different?

00:37:21.370 --> 00:37:22.650
CATHY O'NEIL: Yeah.

00:37:22.650 --> 00:37:24.733
AUDIENCE: Well, I guess
the people who are victims

00:37:24.733 --> 00:37:26.740
probably don't have
the resources to pursue

00:37:26.740 --> 00:37:28.940
slander cases, anyways.

00:37:28.940 --> 00:37:31.190
CATHY O'NEIL: I think that's
one very important point.

00:37:31.190 --> 00:37:32.190
Thank you for making it.

00:37:32.190 --> 00:37:37.446
Which is that one of the
commonalities of almost all

00:37:37.446 --> 00:37:39.070
these algorithms is
that the people who

00:37:39.070 --> 00:37:41.990
are losing by these
scoring systems

00:37:41.990 --> 00:37:44.690
are often the most vulnerable
people in our society.

00:37:44.690 --> 00:37:49.180
So generally speaking,
they don't have

00:37:49.180 --> 00:37:51.580
lawyers to protect them, right?

00:37:51.580 --> 00:37:54.820
But I would also
say the following.

00:37:54.820 --> 00:37:57.280
Anonymization, which is a tool
that people often bring up

00:37:57.280 --> 00:37:59.440
as like a way of solving
some of these problems,

00:37:59.440 --> 00:38:02.120
is not really a solution to me.

00:38:02.120 --> 00:38:06.210
Let's think about the medical
model I just mentioned.

00:38:06.210 --> 00:38:10.835
Like let's say I'm Walmart--
and again, Walmart's not--

00:38:10.835 --> 00:38:15.651
I'm a large employer, and
I get long term health data

00:38:15.651 --> 00:38:18.150
off of all my employees, because
I force them to use Fitbits

00:38:18.150 --> 00:38:19.460
or whatever I do.

00:38:19.460 --> 00:38:22.840
I get long term health outcomes.

00:38:22.840 --> 00:38:24.022
And I don't charge them.

00:38:24.022 --> 00:38:25.230
They're already my employees.

00:38:25.230 --> 00:38:26.740
I treat them well.

00:38:26.740 --> 00:38:28.710
But the point is, I can
build the algorithm.

00:38:28.710 --> 00:38:31.650
I can build the neural
network, what have you.

00:38:31.650 --> 00:38:35.980
I can train it so that when
someone applies to my job,

00:38:35.980 --> 00:38:38.220
I only have to ask
them six questions,

00:38:38.220 --> 00:38:41.080
and I've already categorized--
I've segmented them

00:38:41.080 --> 00:38:44.850
into a risk group score.

00:38:44.850 --> 00:38:46.550
And it is completely anonymous.

00:38:46.550 --> 00:38:49.510
In other words, you can build
an algorithm anonymously,

00:38:49.510 --> 00:38:54.470
and still apply it to someone
in a very precise way,

00:38:54.470 --> 00:38:56.460
and it still can be problematic.

00:38:56.460 --> 00:39:00.191
So for me, it's not
about anonymity.

00:39:03.787 --> 00:39:05.370
AUDIENCE: So in a
lot of the reporting

00:39:05.370 --> 00:39:07.340
that I have read about
this sort of thing--

00:39:07.340 --> 00:39:11.210
and I'm thinking specifically
for example the [INAUDIBLE]

00:39:11.210 --> 00:39:15.270
thing on the criminal
justice, the sentencing thing.

00:39:15.270 --> 00:39:18.140
I haven't seen a lot of talk
about how these algorithms

00:39:18.140 --> 00:39:21.580
perform relative to
this example just

00:39:21.580 --> 00:39:27.010
a judge without this score, or
what the preexisting case was.

00:39:27.010 --> 00:39:29.280
CATHY O'NEIL: Yeah, great point.

00:39:29.280 --> 00:39:31.710
And I'm desperate
for that, right?

00:39:31.710 --> 00:39:33.570
Because the point is
that the justice system

00:39:33.570 --> 00:39:37.420
is very racist already.

00:39:37.420 --> 00:39:40.030
The intention of
this recidivism stuff

00:39:40.030 --> 00:39:44.090
was to make judges more
objective and less racist.

00:39:44.090 --> 00:39:45.727
And it might be doing that.

00:39:45.727 --> 00:39:48.060
With all the flaws that I
just mentioned, which I really

00:39:48.060 --> 00:39:50.540
believe in, it
might actually still

00:39:50.540 --> 00:39:53.110
be better than what
we have already.

00:39:53.110 --> 00:39:55.740
We do not have data for that,
and the Department of Justice

00:39:55.740 --> 00:39:57.460
is not coughing up that data.

00:39:57.460 --> 00:39:58.760
I've tried.

00:39:58.760 --> 00:40:03.260
I've tried to get data to
do an audit of the DOJ.

00:40:03.260 --> 00:40:04.440
It should be possible.

00:40:04.440 --> 00:40:07.610
The data exists, because some
jurisdictions have this stuff

00:40:07.610 --> 00:40:09.360
in use, and some of them don't.

00:40:09.360 --> 00:40:11.790
You can just compare
them before and after

00:40:11.790 --> 00:40:14.970
or whatever, and compare
it to each other.

00:40:14.970 --> 00:40:18.930
I should also add that
like here's the good news.

00:40:18.930 --> 00:40:25.160
If we made those recidivism risk
algorithms actually not racist,

00:40:25.160 --> 00:40:28.705
then that would definitely be
better than the current judges.

00:40:28.705 --> 00:40:30.080
It's like we threw
them out there

00:40:30.080 --> 00:40:31.320
and we're just like
oh, they must be good,

00:40:31.320 --> 00:40:32.890
because they're algorithms.

00:40:32.890 --> 00:40:34.840
They're by definition good.

00:40:34.840 --> 00:40:37.311
No, they're not
necessarily good.

00:40:37.311 --> 00:40:38.310
But again, you're right.

00:40:38.310 --> 00:40:40.330
They might be better
than the existing system.

00:40:40.330 --> 00:40:42.704
AUDIENCE: So in the other
examples that you talked about,

00:40:42.704 --> 00:40:47.400
do you have a similar sense of
whether that data is out there?

00:40:47.400 --> 00:40:49.790
Like the teachers, you know.

00:40:49.790 --> 00:40:51.290
Maybe the teachers
is a bad example.

00:40:51.290 --> 00:40:53.460
CATHY O'NEIL: The teachers
are just terrible.

00:40:53.460 --> 00:40:56.229
AUDIENCE: You're not even trying
to solve the right problem.

00:40:56.229 --> 00:40:57.770
CATHY O'NEIL: And
by the way, I still

00:40:57.770 --> 00:41:00.220
don't think
recidivism risk being

00:41:00.220 --> 00:41:04.000
high shouldn't necessarily
mean you go to jail longer.

00:41:04.000 --> 00:41:06.642
Maybe we should be like, why
do we sentence people the way

00:41:06.642 --> 00:41:08.600
we sentence people they
way we sentence people?

00:41:08.600 --> 00:41:13.030
We should maybe make a redo
of that entire conversation.

00:41:13.030 --> 00:41:15.420
If we were thinking
data-driven-ly,

00:41:15.420 --> 00:41:19.030
like if we're
Google-thinking people,

00:41:19.030 --> 00:41:21.340
we should ask really
basic questions.

00:41:21.340 --> 00:41:25.480
Like to what extent does
GED training in a prison

00:41:25.480 --> 00:41:27.260
help people when they leave?

00:41:27.260 --> 00:41:30.970
To what extent does solitary
confinement help or hurt

00:41:30.970 --> 00:41:32.820
people, or sexual assault?

00:41:32.820 --> 00:41:35.480
Like, we should actually
know what these things are

00:41:35.480 --> 00:41:39.280
doing to our final
outcomes, which hopefully

00:41:39.280 --> 00:41:41.290
is a combination
of public safety

00:41:41.290 --> 00:41:43.800
and the well-being of
the actual prisoners.

00:41:43.800 --> 00:41:45.771
We don't have any of that.

00:41:45.771 --> 00:41:46.270
Yeah?

00:41:50.320 --> 00:41:53.800
AUDIENCE: I entirely agree
with your wonderful notion

00:41:53.800 --> 00:41:57.290
that some of the
data is disparate.

00:41:57.290 --> 00:42:00.260
I mean, it hurts
vulnerable communities.

00:42:00.260 --> 00:42:03.770
But despite that, the thing
you most drew attention to

00:42:03.770 --> 00:42:06.750
was the fact that
people whose relatives

00:42:06.750 --> 00:42:10.090
are criminals are judged
more likely to commit crimes.

00:42:10.090 --> 00:42:11.776
Well, people whose
relatives are actors

00:42:11.776 --> 00:42:13.150
are more likely
to become actors.

00:42:13.150 --> 00:42:14.270
People whose
relatives are plumbers

00:42:14.270 --> 00:42:15.728
are more likely to
become plumbers.

00:42:15.728 --> 00:42:18.340
And even though it's
a horrible thing,

00:42:18.340 --> 00:42:22.130
I strongly suspect that
someone whose parents grew up

00:42:22.130 --> 00:42:24.880
by stealing cars knew a heck of
a lot more about stealing cars

00:42:24.880 --> 00:42:26.690
than someone's who don't.

00:42:26.690 --> 00:42:27.835
What do you suggest?

00:42:27.835 --> 00:42:29.210
CATHY O'NEIL: I
suggest we follow

00:42:29.210 --> 00:42:33.290
the Constitution, which
privileges justice

00:42:33.290 --> 00:42:36.700
over anything like that.

00:42:36.700 --> 00:42:40.010
AUDIENCE: So send the
police to Wall Street,

00:42:40.010 --> 00:42:43.050
because it's equally
likely to have muggings.

00:42:43.050 --> 00:42:47.170
CATHY O'NEIL: As a culture, we
decide what is against the law.

00:42:47.170 --> 00:42:49.430
But the Constitution
declares that we

00:42:49.430 --> 00:42:53.454
have to care about fairness
to the criminal above all.

00:42:53.454 --> 00:42:55.370
And that this is not
fairness to the criminal.

00:42:55.370 --> 00:42:57.715
By saying you're implicated
because your father

00:42:57.715 --> 00:42:58.340
was implicated.

00:42:58.340 --> 00:43:00.670
And I also want to
add that I live next

00:43:00.670 --> 00:43:01.884
to Columbia University.

00:43:01.884 --> 00:43:03.300
And every couple
of years, there's

00:43:03.300 --> 00:43:07.920
a huge drug bust in
the fraternities.

00:43:07.920 --> 00:43:10.290
Like a huge drug bust.

00:43:10.290 --> 00:43:13.830
And then I followed the
NYPD, and they're constantly

00:43:13.830 --> 00:43:16.740
boasting about their data-driven
criminal justice stuff,

00:43:16.740 --> 00:43:20.550
and how they peg
people as gang members

00:43:20.550 --> 00:43:24.280
if they're associated on
social media with gang members.

00:43:24.280 --> 00:43:26.620
But those gang members
are always in Harlem,

00:43:26.620 --> 00:43:27.970
and they're black.

00:43:27.970 --> 00:43:28.960
You see what I mean?

00:43:28.960 --> 00:43:31.130
They're not Columbia students.

00:43:31.130 --> 00:43:33.998
So it's also an
inconsistently-defined

00:43:33.998 --> 00:43:34.498
association.

00:43:37.260 --> 00:43:39.140
So there's two different
kinds of problems.

00:43:39.140 --> 00:43:40.280
AUDIENCE: OK, I see that.

00:43:40.280 --> 00:43:41.080
Thank you.

00:43:41.080 --> 00:43:41.871
CATHY O'NEIL: Yeah.

00:43:44.820 --> 00:43:47.990
AUDIENCE: So I
think it's very easy

00:43:47.990 --> 00:43:51.500
to see that if you're not
careful about how you define

00:43:51.500 --> 00:43:54.530
what kind of success
you're optimizing for,

00:43:54.530 --> 00:43:59.470
you wind up with an algorithm
that very strongly reinforces

00:43:59.470 --> 00:44:00.330
the status quo.

00:44:00.330 --> 00:44:04.820
And if we're looking at things
like are hiring practices fair,

00:44:04.820 --> 00:44:06.470
then if there is
existing unfairness,

00:44:06.470 --> 00:44:08.120
then we're going
to perpetuate it.

00:44:08.120 --> 00:44:11.220
So that makes total sense to me.

00:44:11.220 --> 00:44:16.540
What I struggle with, is if
I'm asked to do something

00:44:16.540 --> 00:44:22.250
like that, and I try
to not incorporate

00:44:22.250 --> 00:44:28.130
the existing structural biases
into the algorithm that's

00:44:28.130 --> 00:44:31.980
going to be my new recommended
hiring practices thing,

00:44:31.980 --> 00:44:35.910
how do I deal with the challenge
that I'm actually optimizing

00:44:35.910 --> 00:44:39.070
away from success conditions?

00:44:42.070 --> 00:44:44.790
CATHY O'NEIL: OK, so there's
two strategies I suggest.

00:44:44.790 --> 00:44:48.110
The first one is
basically a parable.

00:44:48.110 --> 00:44:53.470
So there's this thing called a
blind audition for orchestras.

00:44:53.470 --> 00:44:55.430
Where the orchestras
acknowledged

00:44:55.430 --> 00:44:58.110
that they were being nepotistic
in their practices of hiring.

00:44:58.110 --> 00:45:00.500
So they wanted to get
rid of their nepotism.

00:45:00.500 --> 00:45:03.030
So they decided to put a
sheet between their judges

00:45:03.030 --> 00:45:05.590
and their auditioner
so they wouldn't

00:45:05.590 --> 00:45:07.720
know if the person who
was behind the sheet

00:45:07.720 --> 00:45:10.090
was their friend.

00:45:10.090 --> 00:45:11.560
At first, they saw the shoes.

00:45:11.560 --> 00:45:12.520
And they were like,
wait a second.

00:45:12.520 --> 00:45:13.650
We can see if it's
a man or woman.

00:45:13.650 --> 00:45:15.070
So they brought the
sheet down to the ground.

00:45:15.070 --> 00:45:17.590
And they also installed rugs
in the hallway walking up

00:45:17.590 --> 00:45:20.877
to the spot so they couldn't
hear if it's high heels or not.

00:45:20.877 --> 00:45:22.460
Not only did they
get rid of nepotism,

00:45:22.460 --> 00:45:24.660
but they increased the
number of women in orchestras

00:45:24.660 --> 00:45:26.020
by a factor of five.

00:45:28.850 --> 00:45:31.940
So that's the story.

00:45:31.940 --> 00:45:34.810
And the way I would characterize
what they successfully

00:45:34.810 --> 00:45:37.290
did there is two things.

00:45:37.290 --> 00:45:40.320
First, they decided
a priori what

00:45:40.320 --> 00:45:42.920
was it that was a
requirement for this job.

00:45:42.920 --> 00:45:44.790
How do you assess
somebody for this job?

00:45:44.790 --> 00:45:47.290
And the answer with sound.

00:45:47.290 --> 00:45:49.800
The second thing they did, which
was absolutely as important

00:45:49.800 --> 00:45:54.590
as the first thing, was
to ignore everything else.

00:45:54.590 --> 00:45:58.000
The promise of big data that
we get confused by, because it

00:45:58.000 --> 00:45:59.840
sounds really
convincing-- but it's

00:45:59.840 --> 00:46:03.330
wrong-- the promise of big
data is the more the data

00:46:03.330 --> 00:46:04.660
is, the better.

00:46:04.660 --> 00:46:07.120
Just throw all this
data at the wall.

00:46:07.120 --> 00:46:11.370
Correlations are just
as good as causation.

00:46:11.370 --> 00:46:12.540
Not true.

00:46:12.540 --> 00:46:15.380
Because if we did the
orchestra example again,

00:46:15.380 --> 00:46:18.850
but we didn't have the sheet,
and we thought to ourselves,

00:46:18.850 --> 00:46:20.935
we're looking for good
sound, but we also

00:46:20.935 --> 00:46:22.560
knew that it was our
friend, or we also

00:46:22.560 --> 00:46:25.230
knew that as a woman,
that creeps into us.

00:46:25.230 --> 00:46:27.240
That creeps into our brain.

00:46:27.240 --> 00:46:29.650
That's excess information
that we should be ignoring,

00:46:29.650 --> 00:46:31.450
but we're not ignoring.

00:46:31.450 --> 00:46:34.340
So the first strategy I would
suggest for hiring people

00:46:34.340 --> 00:46:35.420
is to think a priori.

00:46:35.420 --> 00:46:41.560
To actually build a model
that by construction is fair.

00:46:41.560 --> 00:46:43.960
So build the module, can
say, these are the things

00:46:43.960 --> 00:46:46.510
we actually want for this job.

00:46:46.510 --> 00:46:49.124
The second possibility
is use a machine

00:46:49.124 --> 00:46:51.290
learning algorithm that
takes all these correlations

00:46:51.290 --> 00:46:52.230
and blah, blah.

00:46:52.230 --> 00:46:54.670
But then audit it for fairness.

00:46:54.670 --> 00:46:58.350
Then say, wait, does this
basically filter out women?

00:46:58.350 --> 00:46:59.370
Check.

00:46:59.370 --> 00:47:01.112
Check to see if women
are filtered out.

00:47:01.112 --> 00:47:02.820
Check to see whether
they're filtered out

00:47:02.820 --> 00:47:04.700
in a reasonable way.

00:47:04.700 --> 00:47:05.495
Sometimes they are.

00:47:08.150 --> 00:47:10.860
Like I heard this
story on Twitter

00:47:10.860 --> 00:47:14.790
just the other day, where way
more Asian people were filtered

00:47:14.790 --> 00:47:17.296
out of a certain
job, and people were

00:47:17.296 --> 00:47:19.670
complaining because so many
of the applicants were Asian.

00:47:19.670 --> 00:47:21.087
But then people
were saying, well,

00:47:21.087 --> 00:47:22.461
there were more
Asians that we're

00:47:22.461 --> 00:47:25.020
applying that weren't actually
qualified for this job.

00:47:25.020 --> 00:47:26.840
It's complicated.

00:47:26.840 --> 00:47:28.410
And expect it to be complicated.

00:47:28.410 --> 00:47:29.980
That's my third suggestion.

00:47:29.980 --> 00:47:33.580
Is that the other promise of
big data that I'm pushing back

00:47:33.580 --> 00:47:36.270
against and I hope you
understand and agree with

00:47:36.270 --> 00:47:39.670
is that big data is
not a silver bullet.

00:47:39.670 --> 00:47:41.710
It's just a tool.

00:47:41.710 --> 00:47:44.450
It is not automatically
going to solve our problems.

00:47:44.450 --> 00:47:46.170
It's just a tool
that we might be

00:47:46.170 --> 00:47:48.130
able to use to solve
some of our problems.

00:47:48.130 --> 00:47:51.872
But we have to be careful
about it, and we have to check.

00:47:51.872 --> 00:47:54.440
AUDIENCE: Do you have any
other success stories,

00:47:54.440 --> 00:47:58.880
where you would go to a judge
or a jurisdiction or a school

00:47:58.880 --> 00:48:01.420
system, and they would say,
oh my god, you're right.

00:48:01.420 --> 00:48:02.530
We have to fix this.

00:48:02.530 --> 00:48:04.277
And they do?

00:48:04.277 --> 00:48:05.360
CATHY O'NEIL: Sorry, what?

00:48:05.360 --> 00:48:06.970
AUDIENCE: Do you have any
other success stories,

00:48:06.970 --> 00:48:09.040
like the orchestra, where
you might go to someone

00:48:09.040 --> 00:48:11.290
and say, you know, there's
this problem, and they say,

00:48:11.290 --> 00:48:14.110
oh my gosh, as opposed to,
no you can't see our data,

00:48:14.110 --> 00:48:16.364
no that's proprietary,
go away, don't bother us?

00:48:16.364 --> 00:48:18.530
CATHY O'NEIL: You'd be
surprised how few people want

00:48:18.530 --> 00:48:20.042
me to see their source code.

00:48:23.480 --> 00:48:25.644
Although I should
say that someone

00:48:25.644 --> 00:48:27.310
asked me whether I
thought it would just

00:48:27.310 --> 00:48:30.530
be impossible to ever get
a good teacher assessment

00:48:30.530 --> 00:48:32.290
tool using data.

00:48:32.290 --> 00:48:33.665
And I said, I'll
never say never.

00:48:33.665 --> 00:48:35.373
Because like 10 years
ago, I didn't think

00:48:35.373 --> 00:48:36.550
we'd have self-driving cars.

00:48:36.550 --> 00:48:38.480
But I think now that
we're going to save lives

00:48:38.480 --> 00:48:41.290
from drunk driving
with self-driving cars.

00:48:41.290 --> 00:48:43.860
But what is it going to take to
get a good teacher assessment

00:48:43.860 --> 00:48:47.460
tool is-- we're going to have
to have a lot of evidence

00:48:47.460 --> 00:48:48.740
that this is actually working.

00:48:48.740 --> 00:48:50.170
Right now we have
no ground truth

00:48:50.170 --> 00:48:52.530
for teacher value-added model.

00:48:52.530 --> 00:48:54.742
Literally there is no
comparison to some other kind

00:48:54.742 --> 00:48:56.950
of qualitative assessment
to see whether a teacher is

00:48:56.950 --> 00:48:58.980
good-- a good teacher
is getting a good score

00:48:58.980 --> 00:49:00.000
and a bad teacher is
getting a bad score.

00:49:00.000 --> 00:49:02.430
It's just whatever the number
is is your score, which

00:49:02.430 --> 00:49:03.980
is ridiculous.

00:49:03.980 --> 00:49:06.000
So we actually do have
qualitative assessments

00:49:06.000 --> 00:49:06.789
for teachers.

00:49:06.789 --> 00:49:08.580
They're political,
because different people

00:49:08.580 --> 00:49:10.860
disagree about what
makes a good teacher.

00:49:10.860 --> 00:49:13.670
But let's say we had agreement.

00:49:13.670 --> 00:49:16.160
We're defining this kind
of qualitative assessment

00:49:16.160 --> 00:49:20.330
to be important, to
be the ground truth.

00:49:20.330 --> 00:49:23.462
And then we could try to find--
maybe we put a bunch of sensors

00:49:23.462 --> 00:49:24.170
in the classroom.

00:49:24.170 --> 00:49:28.000
We try to replicate that using
data, literally replicate it.

00:49:28.000 --> 00:49:31.890
And if we got an algorithm
that after 4,000 teachers

00:49:31.890 --> 00:49:33.652
got almost exactly
the same score using

00:49:33.652 --> 00:49:35.110
the qualitative
assessment as using

00:49:35.110 --> 00:49:36.770
this data-driven
assessment, then we

00:49:36.770 --> 00:49:39.560
would build trust that
this data-driven assessment

00:49:39.560 --> 00:49:40.600
is doing a good job.

00:49:40.600 --> 00:49:43.710
It's very similar to saying,
we have this many miles

00:49:43.710 --> 00:49:45.730
of a self-driving car
without an accident.

00:49:45.730 --> 00:49:49.210
So we start trusting that
car-- that algorithm.

00:49:49.210 --> 00:49:50.830
That make sense?

00:49:50.830 --> 00:49:52.640
Right now, we have
nothing like that

00:49:52.640 --> 00:49:54.780
in the field of teacher value.

00:49:54.780 --> 00:49:56.880
They just want a silver bullet.

00:49:56.880 --> 00:49:58.160
That's all they want.

00:49:58.160 --> 00:50:00.020
They don't want any
prying eyes, either.

00:50:00.020 --> 00:50:01.770
AUDIENCE: Something
that's kind of come up

00:50:01.770 --> 00:50:04.810
a bunch is sort of
as an undercurrent

00:50:04.810 --> 00:50:09.690
like the idea of open sourcing
how you're doing things.

00:50:09.690 --> 00:50:12.840
And I can see that potentially
being really helpful,

00:50:12.840 --> 00:50:15.340
for example, now you
can actually criticize,

00:50:15.340 --> 00:50:16.990
because you can see the code.

00:50:16.990 --> 00:50:20.210
But I can also see it being
somewhat ineffective if you're

00:50:20.210 --> 00:50:23.120
still doing-- if you're
still asking questions that

00:50:23.120 --> 00:50:25.750
are totally beside the point.

00:50:25.750 --> 00:50:27.830
So I guess I'm curious
about your thoughts

00:50:27.830 --> 00:50:30.810
as to the effectiveness of
things like open sourcing,

00:50:30.810 --> 00:50:34.037
and also the limitations.

00:50:34.037 --> 00:50:35.620
CATHY O'NEIL: That's
a great question.

00:50:35.620 --> 00:50:38.010
And you guys know
that it's hard.

00:50:38.010 --> 00:50:42.970
Because it's relatively
easy to make something

00:50:42.970 --> 00:50:45.195
arbitrarily difficult
to understand.

00:50:45.195 --> 00:50:47.570
If you know in advance that
it's going to be open source,

00:50:47.570 --> 00:50:50.280
you can just make it
impossible to understand.

00:50:50.280 --> 00:50:52.890
And the other example
I gave is like, nobody

00:50:52.890 --> 00:50:54.916
would want the code
for Google search,

00:50:54.916 --> 00:50:56.540
because literally it
probably would not

00:50:56.540 --> 00:50:58.706
work on any computer system
except Google's computer

00:50:58.706 --> 00:50:59.820
system, right?

00:50:59.820 --> 00:51:03.180
So there's a limit to
what open sourcing can do.

00:51:03.180 --> 00:51:06.590
But that's not to say I
think we should give up.

00:51:06.590 --> 00:51:10.200
I think what we should ask for
is an auditing, like a fairness

00:51:10.200 --> 00:51:10.935
auditing.

00:51:10.935 --> 00:51:12.310
And I think we
should think about

00:51:12.310 --> 00:51:13.870
along the lines of
the way sociologists

00:51:13.870 --> 00:51:14.953
audit things for fairness.

00:51:14.953 --> 00:51:18.280
So like sociologists will see
whether a hiring practice is

00:51:18.280 --> 00:51:20.150
racist by sending a
bunch of applications

00:51:20.150 --> 00:51:22.020
with black names
and white names,

00:51:22.020 --> 00:51:24.550
and like similar kind
of qualifications,

00:51:24.550 --> 00:51:26.880
and see whether white
people get more callbacks.

00:51:26.880 --> 00:51:28.820
So we can do that kind
of thing to algorithms.

00:51:28.820 --> 00:51:31.260
And Latanya Sweeney did
that for Google search.

00:51:31.260 --> 00:51:33.050
You guys know that example?

00:51:33.050 --> 00:51:34.650
Latanya Sweeney
googled her own name,

00:51:34.650 --> 00:51:38.100
and she found the ad next
to her search results

00:51:38.100 --> 00:51:40.310
was, "Are you looking
for the criminal arrest

00:51:40.310 --> 00:51:42.160
records for Latanya Sweeney?"

00:51:42.160 --> 00:51:44.374
And she Google a white
name, and it didn't happen.

00:51:44.374 --> 00:51:45.790
Then she did a
comprehensive test,

00:51:45.790 --> 00:51:47.579
because she's a
computer scientist,

00:51:47.579 --> 00:51:49.870
and found that black names
were way more likely to have

00:51:49.870 --> 00:51:53.630
arrest records next to them--
this ad for arrest records.

00:51:53.630 --> 00:51:56.230
Now that's not in my opinion
a weapon of math destruction,

00:51:56.230 --> 00:51:59.280
because it's not as directly
impactful in people's lives.

00:51:59.280 --> 00:52:02.912
But it's certainly not great.

00:52:02.912 --> 00:52:03.860
Go ahead.

00:52:03.860 --> 00:52:04.670
AUDIENCE: Thanks.

00:52:04.670 --> 00:52:06.420
I was just going to
agree with you before,

00:52:06.420 --> 00:52:09.020
when you were talking
about there is no easy way

00:52:09.020 --> 00:52:11.240
to do the teacher evaluation.

00:52:11.240 --> 00:52:13.780
I think it goes back to your
point of success, right?

00:52:13.780 --> 00:52:17.065
Because then it gets into
like student evaluation,

00:52:17.065 --> 00:52:20.950
and are the assessments that
they do on students fair?

00:52:20.950 --> 00:52:24.870
Or is that just a proxy for
success in their long term

00:52:24.870 --> 00:52:25.640
career?

00:52:25.640 --> 00:52:27.190
So I think the
teacher evaluation,

00:52:27.190 --> 00:52:30.350
there's probably
no right answer.

00:52:30.350 --> 00:52:30.930
Do you agree?

00:52:30.930 --> 00:52:33.240
CATHY O'NEIL: Yeah, it's
a very tricky problem.

00:52:33.240 --> 00:52:36.965
So I think the trickier the
question is, the less likely

00:52:36.965 --> 00:52:40.090
it's going to be solved by
some simple scoring system.

00:52:44.077 --> 00:52:46.160
AUDIENCE: A lot of the
machine learning algorithms

00:52:46.160 --> 00:52:47.951
that are very popular
these days are things

00:52:47.951 --> 00:52:51.780
like neural networks, which are
not designed-- they're opaque,

00:52:51.780 --> 00:52:53.780
without even having
had attention.

00:52:53.780 --> 00:52:56.070
Like, if you click on the
little why button about why

00:52:56.070 --> 00:52:57.800
it made a decision,
it'll just say

00:52:57.800 --> 00:52:59.510
I did these 3,000
matrix multiplies,

00:52:59.510 --> 00:53:02.610
and there's no simpler
answer than that.

00:53:02.610 --> 00:53:05.150
But they do give much better
predictions than other things

00:53:05.150 --> 00:53:07.500
like decision trees, which
are much more interpretable.

00:53:07.500 --> 00:53:08.916
Do you have any
thoughts about how

00:53:08.916 --> 00:53:12.609
in the context of these
problems to-- should

00:53:12.609 --> 00:53:14.650
we give deliberately worse
predictions because we

00:53:14.650 --> 00:53:15.899
need them to be interpretable?

00:53:15.899 --> 00:53:17.246
CATHY O'NEIL: Yes, absolutely.

00:53:17.246 --> 00:53:18.620
That's probably
the hardest thing

00:53:18.620 --> 00:53:20.536
that you guys are going
to hear from me today.

00:53:20.536 --> 00:53:23.410
But I definitely think we
need to sacrifice accuracy

00:53:23.410 --> 00:53:25.670
for fairness.

00:53:25.670 --> 00:53:27.330
Absolutely.

00:53:27.330 --> 00:53:30.100
And one of the reasons is
because we as technologists,

00:53:30.100 --> 00:53:33.820
we are not the ones
that are at risk.

00:53:33.820 --> 00:53:37.290
So our concept of
what looks fair to us,

00:53:37.290 --> 00:53:38.280
not good enough, right?

00:53:38.280 --> 00:53:40.050
We might have some
better understanding

00:53:40.050 --> 00:53:42.040
of how things work,
but we're like, oh,

00:53:42.040 --> 00:53:44.900
but it makes it so
much more accurate.

00:53:44.900 --> 00:53:46.300
And I actually
talked to somebody

00:53:46.300 --> 00:53:48.770
who does recidivism risk
algorithms for a living.

00:53:48.770 --> 00:53:52.230
He does it for a state.

00:53:52.230 --> 00:53:54.446
And I said, well, do
you ever use race?

00:53:54.446 --> 00:53:56.320
And he was like, oh,
no, that would be wrong.

00:53:56.320 --> 00:53:57.370
And I said, well do
you ever use zip code?

00:53:57.370 --> 00:53:59.310
He goes like, well,
sometimes, because it

00:53:59.310 --> 00:54:01.670
makes it so much more accurate.

00:54:01.670 --> 00:54:03.959
And I'm like, OK, but
that's a proxy for race.

00:54:03.959 --> 00:54:05.250
So you're basically using race.

00:54:07.980 --> 00:54:11.300
So another way of saying that
is if it's interpretable,

00:54:11.300 --> 00:54:14.500
then it's much more easy for
other people to say that's

00:54:14.500 --> 00:54:15.340
not fair.

00:54:15.340 --> 00:54:17.370
It's a transparency measure.

00:54:17.370 --> 00:54:19.350
There is precedent
for this, by the way.

00:54:19.350 --> 00:54:22.650
In credit card law, if
you're a credit card company

00:54:22.650 --> 00:54:25.040
and you deny someone
a credit card,

00:54:25.040 --> 00:54:26.660
you have to be able
to explain why.

00:54:26.660 --> 00:54:30.150
Which restricts people
to using decision trees,

00:54:30.150 --> 00:54:33.010
for the most part.

00:54:33.010 --> 00:54:34.410
They don't like it.

00:54:34.410 --> 00:54:35.530
But that's OK.

00:54:35.530 --> 00:54:39.230
There's actually lots
of things about lending

00:54:39.230 --> 00:54:42.420
that are a trade off for
fairness and accuracy.

00:54:42.420 --> 00:54:46.250
So FICO scores by law--
anti-discrimination laws

00:54:46.250 --> 00:54:50.960
called Fair Credit Reporting
Act and the Equal Credit

00:54:50.960 --> 00:54:53.470
Opportunity Act make
it illegal for you

00:54:53.470 --> 00:54:58.670
to base FICO scores on
race or gender or zip code.

00:54:58.670 --> 00:55:02.560
Is that because it's more
accurate when you restrict?

00:55:02.560 --> 00:55:08.060
No, it's actually keeping
these credit scores

00:55:08.060 --> 00:55:11.240
from being as accurate
as they might be.

00:55:11.240 --> 00:55:13.860
But it was deemed
more fair that way.

00:55:13.860 --> 00:55:18.280
As a society, we care
about the public, right?

00:55:18.280 --> 00:55:21.970
So the idea is-- this
basically happened

00:55:21.970 --> 00:55:25.700
in the '70s when especially
divorced women were

00:55:25.700 --> 00:55:27.550
being denied loans.

00:55:27.550 --> 00:55:31.200
And the idea there was, if
you never let women get loans,

00:55:31.200 --> 00:55:33.700
then they're never going to be
able to build up their credit

00:55:33.700 --> 00:55:35.860
scores so they'll
be credit worthy.

00:55:35.860 --> 00:55:38.340
So it's a feedback loop, right?

00:55:38.340 --> 00:55:40.500
And so these
anti-discrimination laws

00:55:40.500 --> 00:55:44.530
were specifically designed
to prevent the feedback loop.

00:55:44.530 --> 00:55:45.690
And we need that.

00:55:45.690 --> 00:55:48.770
We need that kind of
thing, because algorithms--

00:55:48.770 --> 00:55:50.770
like that algorithm
I just talked about.

00:55:50.770 --> 00:55:55.659
That benefits the banks or
the people that make loans

00:55:55.659 --> 00:55:56.950
to the detriment of the public.

00:55:56.950 --> 00:55:59.714
So we have to
balance the benefits

00:55:59.714 --> 00:56:01.130
for the public
versus the benefits

00:56:01.130 --> 00:56:02.213
for the private companies.

00:56:05.100 --> 00:56:06.434
I have time for one question.

00:56:10.700 --> 00:56:13.910
AUDIENCE: So I was going to say
I'm in the opposite boat, where

00:56:13.910 --> 00:56:17.400
I work in a search feature
that looks at lots of queries.

00:56:17.400 --> 00:56:22.510
And we have a model for English
where the masculine pronouns

00:56:22.510 --> 00:56:25.530
are the default, and some
common names are the default.

00:56:25.530 --> 00:56:27.700
So the model knows
a lot about that.

00:56:27.700 --> 00:56:32.110
And we're kind of wondering
if queries that women like,

00:56:32.110 --> 00:56:35.574
we do terrible on, or worse,
or there's some class.

00:56:35.574 --> 00:56:37.490
Except we don't know
anything about the people

00:56:37.490 --> 00:56:38.323
who ask the queries.

00:56:38.323 --> 00:56:40.430
We've tried really
hard not to know.

00:56:40.430 --> 00:56:42.100
I don't know gender.

00:56:42.100 --> 00:56:44.500
I don't know income.

00:56:44.500 --> 00:56:48.080
I could probably get
zip code in bulk.

00:56:48.080 --> 00:56:50.777
So I'm sitting here.

00:56:50.777 --> 00:56:53.360
I'm trying to think how I would
write a machine learning model

00:56:53.360 --> 00:56:55.650
that would do the
reverse question

00:56:55.650 --> 00:56:59.530
and figure out what's the most
unfair thing my current system

00:56:59.530 --> 00:57:00.030
is up to?

00:57:00.030 --> 00:57:02.488
CATHY O'NEIL: That's a good
question to be asking yourself.

00:57:02.488 --> 00:57:04.070
But I would also
add-- and thank you

00:57:04.070 --> 00:57:05.400
for asking that to yourself.

00:57:05.400 --> 00:57:06.820
I think that's what
we all need to ask.

00:57:06.820 --> 00:57:08.220
That's one of the
first questions

00:57:08.220 --> 00:57:10.600
we should be asking ourselves.

00:57:10.600 --> 00:57:14.600
You made the point yourself,
but I'll just reiterate it.

00:57:14.600 --> 00:57:17.620
I'm certainly not the first
person to make this point.

00:57:17.620 --> 00:57:20.430
Sometimes you actually do
need to know these attributes

00:57:20.430 --> 00:57:24.550
like race and gender in order
to measure your fairness.

00:57:24.550 --> 00:57:28.620
So I understand that the
desire to be race-neutral

00:57:28.620 --> 00:57:31.370
or gender-neutral, but that
doesn't mean you should not

00:57:31.370 --> 00:57:33.500
collect that data.

00:57:33.500 --> 00:57:36.170
Because then you're basically
saying, I'm not going to look.

00:57:36.170 --> 00:57:38.680
You know, it must
be fair, because I

00:57:38.680 --> 00:57:39.680
don't collect that data.

00:57:39.680 --> 00:57:42.530
That's not proof that it's fair.

00:57:42.530 --> 00:57:45.800
Which isn't to say that it's
easy to collect that data.

00:57:45.800 --> 00:57:48.470
AUDIENCE: That's
not why we don't.

00:57:48.470 --> 00:57:49.300
CATHY O'NEIL: OK.

00:57:49.300 --> 00:57:51.310
I'm saying maybe collect it.

00:57:51.310 --> 00:57:56.440
At least for some
experiments as an audit.

00:57:56.440 --> 00:57:57.790
Thank you, guys.

00:57:57.790 --> 00:58:01.440
[APPLAUSE]

