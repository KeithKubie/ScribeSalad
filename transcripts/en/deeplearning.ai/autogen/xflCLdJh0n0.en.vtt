WEBVTT
Kind: captions
Language: en

00:00:00.329 --> 00:00:03.050 align:start position:0%
 
if<00:00:01.290><c> the</c><00:00:01.500><c> basic</c><00:00:01.920><c> technical</c><00:00:02.340><c> idea</c><00:00:02.760><c> is</c><00:00:02.879><c> behind</c>

00:00:03.050 --> 00:00:03.060 align:start position:0%
if the basic technical idea is behind
 

00:00:03.060 --> 00:00:05.690 align:start position:0%
if the basic technical idea is behind
deep<00:00:03.419><c> learning</c><00:00:03.629><c> behind</c><00:00:04.380><c> your</c><00:00:04.529><c> networks</c><00:00:05.009><c> have</c>

00:00:05.690 --> 00:00:05.700 align:start position:0%
deep learning behind your networks have
 

00:00:05.700 --> 00:00:07.460 align:start position:0%
deep learning behind your networks have
been<00:00:05.850><c> around</c><00:00:06.120><c> for</c><00:00:06.330><c> decades</c><00:00:06.359><c> why</c><00:00:07.200><c> are</c><00:00:07.350><c> they</c>

00:00:07.460 --> 00:00:07.470 align:start position:0%
been around for decades why are they
 

00:00:07.470 --> 00:00:09.860 align:start position:0%
been around for decades why are they
only<00:00:07.529><c> just</c><00:00:07.740><c> now</c><00:00:08.010><c> taking</c><00:00:08.400><c> off</c><00:00:08.550><c> in</c><00:00:08.820><c> this</c><00:00:09.510><c> video</c>

00:00:09.860 --> 00:00:09.870 align:start position:0%
only just now taking off in this video
 

00:00:09.870 --> 00:00:12.080 align:start position:0%
only just now taking off in this video
let's<00:00:10.260><c> go</c><00:00:10.440><c> over</c><00:00:10.860><c> some</c><00:00:10.889><c> of</c><00:00:11.219><c> the</c><00:00:11.309><c> main</c><00:00:11.490><c> drivers</c>

00:00:12.080 --> 00:00:12.090 align:start position:0%
let's go over some of the main drivers
 

00:00:12.090 --> 00:00:14.120 align:start position:0%
let's go over some of the main drivers
behind<00:00:12.120><c> the</c><00:00:12.570><c> rise</c><00:00:12.750><c> of</c><00:00:12.780><c> deep</c><00:00:13.080><c> learning</c><00:00:13.290><c> because</c>

00:00:14.120 --> 00:00:14.130 align:start position:0%
behind the rise of deep learning because
 

00:00:14.130 --> 00:00:16.160 align:start position:0%
behind the rise of deep learning because
I<00:00:14.160><c> think</c><00:00:14.309><c> this</c><00:00:14.549><c> will</c><00:00:14.700><c> help</c><00:00:14.759><c> you</c><00:00:15.150><c> that</c><00:00:15.719><c> the</c><00:00:15.870><c> spot</c>

00:00:16.160 --> 00:00:16.170 align:start position:0%
I think this will help you that the spot
 

00:00:16.170 --> 00:00:18.080 align:start position:0%
I think this will help you that the spot
the<00:00:16.470><c> best</c><00:00:16.680><c> opportunities</c><00:00:17.369><c> within</c><00:00:17.640><c> your</c><00:00:17.880><c> own</c>

00:00:18.080 --> 00:00:18.090 align:start position:0%
the best opportunities within your own
 

00:00:18.090 --> 00:00:20.840 align:start position:0%
the best opportunities within your own
organization<00:00:18.960><c> to</c><00:00:19.199><c> apply</c><00:00:19.710><c> these</c><00:00:19.770><c> to</c><00:00:20.160><c> over</c><00:00:20.730><c> the</c>

00:00:20.840 --> 00:00:20.850 align:start position:0%
organization to apply these to over the
 

00:00:20.850 --> 00:00:22.429 align:start position:0%
organization to apply these to over the
last<00:00:20.970><c> few</c><00:00:21.270><c> years</c><00:00:21.510><c> a</c><00:00:21.600><c> lot</c><00:00:21.720><c> of</c><00:00:21.930><c> people</c><00:00:22.140><c> have</c>

00:00:22.429 --> 00:00:22.439 align:start position:0%
last few years a lot of people have
 

00:00:22.439 --> 00:00:24.230 align:start position:0%
last few years a lot of people have
asked<00:00:22.650><c> me</c><00:00:22.800><c> Andrew</c><00:00:23.400><c> why</c><00:00:23.670><c> is</c><00:00:23.730><c> deep</c><00:00:24.060><c> learning</c>

00:00:24.230 --> 00:00:24.240 align:start position:0%
asked me Andrew why is deep learning
 

00:00:24.240 --> 00:00:26.810 align:start position:0%
asked me Andrew why is deep learning
certainly<00:00:24.810><c> working</c><00:00:25.170><c> so</c><00:00:25.350><c> well</c><00:00:25.590><c> and</c><00:00:25.890><c> when</c><00:00:26.789><c> a</c>

00:00:26.810 --> 00:00:26.820 align:start position:0%
certainly working so well and when a
 

00:00:26.820 --> 00:00:28.939 align:start position:0%
certainly working so well and when a
marsan<00:00:27.210><c> question</c><00:00:27.630><c> this</c><00:00:27.960><c> is</c><00:00:28.019><c> usually</c><00:00:28.560><c> the</c>

00:00:28.939 --> 00:00:28.949 align:start position:0%
marsan question this is usually the
 

00:00:28.949 --> 00:00:31.099 align:start position:0%
marsan question this is usually the
picture<00:00:29.250><c> I</c><00:00:29.310><c> draw</c><00:00:29.580><c> for</c><00:00:29.820><c> them</c><00:00:29.970><c> let's</c><00:00:30.900><c> say</c><00:00:30.990><c> we</c>

00:00:31.099 --> 00:00:31.109 align:start position:0%
picture I draw for them let's say we
 

00:00:31.109 --> 00:00:33.200 align:start position:0%
picture I draw for them let's say we
plot<00:00:31.349><c> a</c><00:00:31.380><c> figure</c><00:00:31.800><c> where</c><00:00:32.130><c> on</c><00:00:32.369><c> the</c><00:00:32.579><c> horizontal</c>

00:00:33.200 --> 00:00:33.210 align:start position:0%
plot a figure where on the horizontal
 

00:00:33.210 --> 00:00:36.170 align:start position:0%
plot a figure where on the horizontal
axis<00:00:33.390><c> we</c><00:00:33.870><c> plot</c><00:00:33.899><c> the</c><00:00:34.469><c> amount</c><00:00:34.680><c> of</c><00:00:34.890><c> data</c><00:00:35.160><c> we</c><00:00:35.940><c> have</c>

00:00:36.170 --> 00:00:36.180 align:start position:0%
axis we plot the amount of data we have
 

00:00:36.180 --> 00:00:39.260 align:start position:0%
axis we plot the amount of data we have
for<00:00:36.390><c> a</c><00:00:36.450><c> task</c><00:00:36.750><c> and</c><00:00:37.370><c> let's</c><00:00:38.370><c> say</c><00:00:38.579><c> on</c><00:00:38.760><c> the</c><00:00:38.910><c> vertical</c>

00:00:39.260 --> 00:00:39.270 align:start position:0%
for a task and let's say on the vertical
 

00:00:39.270 --> 00:00:42.560 align:start position:0%
for a task and let's say on the vertical
axis<00:00:39.840><c> we</c><00:00:39.870><c> plot</c><00:00:40.110><c> the</c><00:00:40.590><c> performance</c><00:00:41.280><c> on</c><00:00:41.570><c> above</c>

00:00:42.560 --> 00:00:42.570 align:start position:0%
axis we plot the performance on above
 

00:00:42.570 --> 00:00:44.420 align:start position:0%
axis we plot the performance on above
learning<00:00:43.020><c> algorithms</c><00:00:43.530><c> such</c><00:00:43.770><c> as</c><00:00:43.800><c> the</c><00:00:44.010><c> accuracy</c>

00:00:44.420 --> 00:00:44.430 align:start position:0%
learning algorithms such as the accuracy
 

00:00:44.430 --> 00:00:48.170 align:start position:0%
learning algorithms such as the accuracy
of<00:00:44.670><c> our</c><00:00:45.030><c> spam</c><00:00:45.840><c> classifier</c><00:00:46.410><c> or</c><00:00:46.770><c> our</c><00:00:47.160><c> ad</c><00:00:47.700><c> click</c>

00:00:48.170 --> 00:00:48.180 align:start position:0%
of our spam classifier or our ad click
 

00:00:48.180 --> 00:00:51.950 align:start position:0%
of our spam classifier or our ad click
predictor<00:00:48.719><c> or</c><00:00:49.050><c> the</c><00:00:49.530><c> accuracy</c><00:00:49.980><c> of</c><00:00:50.489><c> our</c><00:00:50.960><c> neural</c>

00:00:51.950 --> 00:00:51.960 align:start position:0%
predictor or the accuracy of our neural
 

00:00:51.960 --> 00:00:53.959 align:start position:0%
predictor or the accuracy of our neural
net<00:00:52.230><c> for</c><00:00:52.559><c> figuring</c><00:00:53.219><c> out</c><00:00:53.370><c> the</c><00:00:53.520><c> position</c><00:00:53.879><c> of</c>

00:00:53.959 --> 00:00:53.969 align:start position:0%
net for figuring out the position of
 

00:00:53.969 --> 00:00:56.389 align:start position:0%
net for figuring out the position of
other<00:00:54.149><c> calls</c><00:00:54.420><c> for</c><00:00:54.629><c> our</c><00:00:54.690><c> self-driving</c><00:00:55.170><c> car</c><00:00:55.440><c> it</c>

00:00:56.389 --> 00:00:56.399 align:start position:0%
other calls for our self-driving car it
 

00:00:56.399 --> 00:00:58.430 align:start position:0%
other calls for our self-driving car it
turns<00:00:56.730><c> out</c><00:00:56.910><c> if</c><00:00:57.120><c> you</c><00:00:57.239><c> plot</c><00:00:57.480><c> the</c><00:00:57.690><c> performance</c><00:00:58.320><c> of</c>

00:00:58.430 --> 00:00:58.440 align:start position:0%
turns out if you plot the performance of
 

00:00:58.440 --> 00:01:00.260 align:start position:0%
turns out if you plot the performance of
a<00:00:58.530><c> traditional</c><00:00:59.100><c> learning</c><00:00:59.430><c> algorithm</c><00:00:59.940><c> like</c>

00:01:00.260 --> 00:01:00.270 align:start position:0%
a traditional learning algorithm like
 

00:01:00.270 --> 00:01:02.450 align:start position:0%
a traditional learning algorithm like
support<00:01:01.050><c> vector</c><00:01:01.289><c> machine</c><00:01:01.620><c> or</c><00:01:01.890><c> logistic</c>

00:01:02.450 --> 00:01:02.460 align:start position:0%
support vector machine or logistic
 

00:01:02.460 --> 00:01:04.700 align:start position:0%
support vector machine or logistic
regression<00:01:02.969><c> as</c><00:01:03.120><c> a</c><00:01:03.199><c> function</c><00:01:04.199><c> of</c><00:01:04.350><c> the</c><00:01:04.500><c> amount</c>

00:01:04.700 --> 00:01:04.710 align:start position:0%
regression as a function of the amount
 

00:01:04.710 --> 00:01:07.609 align:start position:0%
regression as a function of the amount
of<00:01:04.830><c> data</c><00:01:05.070><c> you</c><00:01:05.430><c> have</c><00:01:05.869><c> you</c><00:01:06.869><c> might</c><00:01:07.080><c> get</c><00:01:07.260><c> a</c><00:01:07.290><c> curve</c>

00:01:07.609 --> 00:01:07.619 align:start position:0%
of data you have you might get a curve
 

00:01:07.619 --> 00:01:09.710 align:start position:0%
of data you have you might get a curve
that<00:01:07.650><c> looks</c><00:01:07.920><c> like</c><00:01:08.159><c> this</c><00:01:08.659><c> where</c><00:01:09.659><c> the</c>

00:01:09.710 --> 00:01:09.720 align:start position:0%
that looks like this where the
 

00:01:09.720 --> 00:01:11.660 align:start position:0%
that looks like this where the
performance<00:01:10.320><c> improves</c><00:01:10.799><c> for</c><00:01:10.950><c> a</c><00:01:11.010><c> while</c><00:01:11.250><c> as</c><00:01:11.490><c> you</c>

00:01:11.660 --> 00:01:11.670 align:start position:0%
performance improves for a while as you
 

00:01:11.670 --> 00:01:14.270 align:start position:0%
performance improves for a while as you
add<00:01:11.850><c> more</c><00:01:11.880><c> data</c><00:01:12.350><c> but</c><00:01:13.350><c> after</c><00:01:13.680><c> a</c><00:01:13.710><c> while</c><00:01:13.950><c> the</c>

00:01:14.270 --> 00:01:14.280 align:start position:0%
add more data but after a while the
 

00:01:14.280 --> 00:01:16.190 align:start position:0%
add more data but after a while the
performance<00:01:14.909><c> you</c><00:01:15.689><c> know</c><00:01:15.810><c> pretty</c><00:01:16.110><c> much</c>

00:01:16.190 --> 00:01:16.200 align:start position:0%
performance you know pretty much
 

00:01:16.200 --> 00:01:18.620 align:start position:0%
performance you know pretty much
plateaus<00:01:16.979><c> right</c><00:01:17.580><c> suppose</c><00:01:18.210><c> your</c><00:01:18.390><c> horizontal</c>

00:01:18.620 --> 00:01:18.630 align:start position:0%
plateaus right suppose your horizontal
 

00:01:18.630 --> 00:01:21.170 align:start position:0%
plateaus right suppose your horizontal
lines<00:01:19.170><c> enjoy</c><00:01:19.560><c> that</c><00:01:19.710><c> very</c><00:01:19.799><c> well</c><00:01:20.040><c> you</c><00:01:20.729><c> know</c><00:01:20.820><c> was</c>

00:01:21.170 --> 00:01:21.180 align:start position:0%
lines enjoy that very well you know was
 

00:01:21.180 --> 00:01:25.310 align:start position:0%
lines enjoy that very well you know was
it<00:01:21.540><c> they</c><00:01:22.200><c> didn't</c><00:01:22.560><c> know</c><00:01:22.740><c> what</c><00:01:23.520><c> to</c><00:01:23.640><c> do</c><00:01:23.850><c> with</c><00:01:24.320><c> huge</c>

00:01:25.310 --> 00:01:25.320 align:start position:0%
it they didn't know what to do with huge
 

00:01:25.320 --> 00:01:28.130 align:start position:0%
it they didn't know what to do with huge
amounts<00:01:25.680><c> of</c><00:01:25.770><c> data</c><00:01:25.950><c> and</c><00:01:26.400><c> what</c><00:01:27.299><c> happened</c><00:01:27.780><c> in</c><00:01:28.020><c> our</c>

00:01:28.130 --> 00:01:28.140 align:start position:0%
amounts of data and what happened in our
 

00:01:28.140 --> 00:01:30.679 align:start position:0%
amounts of data and what happened in our
society<00:01:28.710><c> over</c><00:01:29.189><c> the</c><00:01:29.220><c> last</c><00:01:29.430><c> 10</c><00:01:29.880><c> years</c><00:01:29.909><c> maybe</c><00:01:30.360><c> is</c>

00:01:30.679 --> 00:01:30.689 align:start position:0%
society over the last 10 years maybe is
 

00:01:30.689 --> 00:01:32.840 align:start position:0%
society over the last 10 years maybe is
that<00:01:30.960><c> for</c><00:01:31.170><c> a</c><00:01:31.200><c> lot</c><00:01:31.350><c> of</c><00:01:31.380><c> problems</c><00:01:31.890><c> we</c><00:01:32.400><c> went</c><00:01:32.670><c> from</c>

00:01:32.840 --> 00:01:32.850 align:start position:0%
that for a lot of problems we went from
 

00:01:32.850 --> 00:01:34.810 align:start position:0%
that for a lot of problems we went from
having<00:01:33.119><c> a</c><00:01:33.360><c> relatively</c><00:01:33.750><c> small</c><00:01:34.140><c> amount</c><00:01:34.229><c> of</c><00:01:34.560><c> data</c>

00:01:34.810 --> 00:01:34.820 align:start position:0%
having a relatively small amount of data
 

00:01:34.820 --> 00:01:38.600 align:start position:0%
having a relatively small amount of data
to<00:01:35.820><c> having</c><00:01:36.270><c> you</c><00:01:36.930><c> know</c><00:01:37.049><c> often</c><00:01:37.500><c> a</c><00:01:37.770><c> fairly</c><00:01:38.369><c> large</c>

00:01:38.600 --> 00:01:38.610 align:start position:0%
to having you know often a fairly large
 

00:01:38.610 --> 00:01:40.969 align:start position:0%
to having you know often a fairly large
amount<00:01:38.729><c> of</c><00:01:39.150><c> data</c><00:01:39.360><c> and</c><00:01:39.780><c> all</c><00:01:40.500><c> of</c><00:01:40.650><c> this</c><00:01:40.740><c> was</c>

00:01:40.969 --> 00:01:40.979 align:start position:0%
amount of data and all of this was
 

00:01:40.979 --> 00:01:43.969 align:start position:0%
amount of data and all of this was
thanks<00:01:41.220><c> to</c><00:01:41.430><c> the</c><00:01:41.689><c> digitization</c><00:01:42.689><c> of</c><00:01:43.079><c> a</c><00:01:43.409><c> society</c>

00:01:43.969 --> 00:01:43.979 align:start position:0%
thanks to the digitization of a society
 

00:01:43.979 --> 00:01:46.969 align:start position:0%
thanks to the digitization of a society
where<00:01:44.369><c> so</c><00:01:44.670><c> much</c><00:01:44.880><c> human</c><00:01:45.180><c> activity</c><00:01:45.750><c> is</c><00:01:46.530><c> now</c><00:01:46.710><c> in</c>

00:01:46.969 --> 00:01:46.979 align:start position:0%
where so much human activity is now in
 

00:01:46.979 --> 00:01:48.710 align:start position:0%
where so much human activity is now in
the<00:01:47.159><c> digital</c><00:01:47.399><c> realm</c><00:01:47.700><c> we</c><00:01:48.060><c> spend</c><00:01:48.299><c> so</c><00:01:48.479><c> much</c><00:01:48.659><c> time</c>

00:01:48.710 --> 00:01:48.720 align:start position:0%
the digital realm we spend so much time
 

00:01:48.720 --> 00:01:51.170 align:start position:0%
the digital realm we spend so much time
on<00:01:49.170><c> the</c><00:01:49.259><c> computers</c><00:01:49.770><c> on</c><00:01:50.070><c> websites</c><00:01:50.579><c> on</c><00:01:50.850><c> mobile</c>

00:01:51.170 --> 00:01:51.180 align:start position:0%
on the computers on websites on mobile
 

00:01:51.180 --> 00:01:54.310 align:start position:0%
on the computers on websites on mobile
apps<00:01:51.390><c> and</c><00:01:51.770><c> activities</c><00:01:52.770><c> on</c><00:01:53.009><c> digital</c><00:01:53.430><c> devices</c>

00:01:54.310 --> 00:01:54.320 align:start position:0%
apps and activities on digital devices
 

00:01:54.320 --> 00:01:57.950 align:start position:0%
apps and activities on digital devices
creates<00:01:55.320><c> data</c><00:01:55.619><c> and</c><00:01:56.180><c> thanks</c><00:01:57.180><c> to</c><00:01:57.360><c> the</c><00:01:57.479><c> rise</c><00:01:57.630><c> of</c>

00:01:57.950 --> 00:01:57.960 align:start position:0%
creates data and thanks to the rise of
 

00:01:57.960 --> 00:02:00.350 align:start position:0%
creates data and thanks to the rise of
inexpensive<00:01:58.530><c> cameras</c><00:01:59.520><c> built</c><00:01:59.820><c> into</c><00:01:59.939><c> our</c><00:02:00.149><c> cell</c>

00:02:00.350 --> 00:02:00.360 align:start position:0%
inexpensive cameras built into our cell
 

00:02:00.360 --> 00:02:02.359 align:start position:0%
inexpensive cameras built into our cell
phones<00:02:00.649><c> accelerometers</c><00:02:01.649><c> all</c><00:02:01.950><c> sorts</c><00:02:02.340><c> of</c>

00:02:02.359 --> 00:02:02.369 align:start position:0%
phones accelerometers all sorts of
 

00:02:02.369 --> 00:02:05.899 align:start position:0%
phones accelerometers all sorts of
sensors<00:02:02.969><c> in</c><00:02:03.960><c> the</c><00:02:04.259><c> Internet</c><00:02:04.770><c> of</c><00:02:04.799><c> Things</c><00:02:04.950><c> we</c>

00:02:05.899 --> 00:02:05.909 align:start position:0%
sensors in the Internet of Things we
 

00:02:05.909 --> 00:02:07.880 align:start position:0%
sensors in the Internet of Things we
also<00:02:06.060><c> just</c><00:02:06.570><c> have</c><00:02:06.810><c> been</c><00:02:06.960><c> collecting</c><00:02:07.439><c> one</c><00:02:07.649><c> more</c>

00:02:07.880 --> 00:02:07.890 align:start position:0%
also just have been collecting one more
 

00:02:07.890 --> 00:02:11.119 align:start position:0%
also just have been collecting one more
and<00:02:08.009><c> more</c><00:02:08.129><c> data</c><00:02:08.989><c> so</c><00:02:09.989><c> over</c><00:02:10.440><c> the</c><00:02:10.530><c> last</c><00:02:10.619><c> 20</c><00:02:11.099><c> years</c>

00:02:11.119 --> 00:02:11.129 align:start position:0%
and more data so over the last 20 years
 

00:02:11.129 --> 00:02:12.860 align:start position:0%
and more data so over the last 20 years
for<00:02:11.580><c> a</c><00:02:11.610><c> lot</c><00:02:11.730><c> of</c><00:02:11.790><c> applications</c><00:02:12.030><c> we</c><00:02:12.660><c> just</c>

00:02:12.860 --> 00:02:12.870 align:start position:0%
for a lot of applications we just
 

00:02:12.870 --> 00:02:13.550 align:start position:0%
for a lot of applications we just
accumulate

00:02:13.550 --> 00:02:13.560 align:start position:0%
accumulate
 

00:02:13.560 --> 00:02:16.309 align:start position:0%
accumulate
a<00:02:13.590><c> lot</c><00:02:13.890><c> more</c><00:02:14.040><c> data</c><00:02:14.360><c> more</c><00:02:15.360><c> than</c><00:02:15.420><c> traditional</c>

00:02:16.309 --> 00:02:16.319 align:start position:0%
a lot more data more than traditional
 

00:02:16.319 --> 00:02:17.540 align:start position:0%
a lot more data more than traditional
learning<00:02:16.440><c> algorithms</c><00:02:16.980><c> were</c><00:02:17.099><c> able</c><00:02:17.520><c> to</c>

00:02:17.540 --> 00:02:17.550 align:start position:0%
learning algorithms were able to
 

00:02:17.550 --> 00:02:20.510 align:start position:0%
learning algorithms were able to
effectively<00:02:18.120><c> take</c><00:02:18.750><c> advantage</c><00:02:19.200><c> of</c><00:02:19.380><c> and</c><00:02:19.590><c> what</c>

00:02:20.510 --> 00:02:20.520 align:start position:0%
effectively take advantage of and what
 

00:02:20.520 --> 00:02:22.550 align:start position:0%
effectively take advantage of and what
new<00:02:20.730><c> network</c><00:02:21.270><c> lead</c><00:02:21.480><c> turns</c><00:02:21.750><c> out</c><00:02:21.959><c> that</c><00:02:22.230><c> if</c><00:02:22.440><c> you</c>

00:02:22.550 --> 00:02:22.560 align:start position:0%
new network lead turns out that if you
 

00:02:22.560 --> 00:02:26.300 align:start position:0%
new network lead turns out that if you
train<00:02:22.920><c> a</c><00:02:23.250><c> small</c><00:02:23.670><c> neural</c><00:02:24.569><c> net</c><00:02:25.160><c> then</c><00:02:26.160><c> this</c>

00:02:26.300 --> 00:02:26.310 align:start position:0%
train a small neural net then this
 

00:02:26.310 --> 00:02:28.460 align:start position:0%
train a small neural net then this
performance<00:02:26.880><c> maybe</c><00:02:27.569><c> looks</c><00:02:27.840><c> like</c><00:02:27.959><c> that</c>

00:02:28.460 --> 00:02:28.470 align:start position:0%
performance maybe looks like that
 

00:02:28.470 --> 00:02:31.339 align:start position:0%
performance maybe looks like that
if<00:02:29.160><c> you</c><00:02:29.340><c> train</c><00:02:29.550><c> a</c><00:02:29.610><c> somewhat</c><00:02:30.180><c> larger</c><00:02:30.660><c> Internet</c>

00:02:31.339 --> 00:02:31.349 align:start position:0%
if you train a somewhat larger Internet
 

00:02:31.349 --> 00:02:34.580 align:start position:0%
if you train a somewhat larger Internet
that's<00:02:32.069><c> called</c><00:02:32.340><c> as</c><00:02:32.489><c> a</c><00:02:32.610><c> medium-sized</c><00:02:33.590><c> internet</c>

00:02:34.580 --> 00:02:34.590 align:start position:0%
that's called as a medium-sized internet
 

00:02:34.590 --> 00:02:36.320 align:start position:0%
that's called as a medium-sized internet
to<00:02:34.920><c> fall</c><00:02:35.130><c> in</c><00:02:35.280><c> something</c><00:02:35.670><c> a</c><00:02:35.760><c> little</c><00:02:35.790><c> bit</c><00:02:36.180><c> better</c>

00:02:36.320 --> 00:02:36.330 align:start position:0%
to fall in something a little bit better
 

00:02:36.330 --> 00:02:39.890 align:start position:0%
to fall in something a little bit better
and<00:02:36.980><c> if</c><00:02:37.980><c> you</c><00:02:38.040><c> train</c><00:02:38.340><c> a</c><00:02:38.370><c> very</c><00:02:38.849><c> large</c><00:02:39.209><c> neural</c><00:02:39.720><c> net</c>

00:02:39.890 --> 00:02:39.900 align:start position:0%
and if you train a very large neural net
 

00:02:39.900 --> 00:02:42.170 align:start position:0%
and if you train a very large neural net
then<00:02:40.530><c> it's</c><00:02:40.680><c> the</c><00:02:40.800><c> form</c><00:02:41.069><c> and</c><00:02:41.220><c> often</c><00:02:41.670><c> just</c><00:02:41.940><c> keeps</c>

00:02:42.170 --> 00:02:42.180 align:start position:0%
then it's the form and often just keeps
 

00:02:42.180 --> 00:02:44.570 align:start position:0%
then it's the form and often just keeps
getting<00:02:42.540><c> better</c><00:02:42.690><c> and</c><00:02:42.900><c> better</c><00:02:43.099><c> so</c><00:02:44.099><c> couple</c>

00:02:44.570 --> 00:02:44.580 align:start position:0%
getting better and better so couple
 

00:02:44.580 --> 00:02:46.880 align:start position:0%
getting better and better so couple
observations<00:02:44.730><c> one</c><00:02:45.569><c> is</c><00:02:45.780><c> if</c><00:02:46.049><c> you</c><00:02:46.170><c> want</c><00:02:46.410><c> to</c><00:02:46.560><c> hit</c>

00:02:46.880 --> 00:02:46.890 align:start position:0%
observations one is if you want to hit
 

00:02:46.890 --> 00:02:49.400 align:start position:0%
observations one is if you want to hit
this<00:02:47.130><c> very</c><00:02:47.459><c> high</c><00:02:47.730><c> level</c><00:02:48.030><c> of</c><00:02:48.090><c> performance</c><00:02:48.410><c> then</c>

00:02:49.400 --> 00:02:49.410 align:start position:0%
this very high level of performance then
 

00:02:49.410 --> 00:02:52.610 align:start position:0%
this very high level of performance then
you<00:02:49.709><c> need</c><00:02:49.890><c> two</c><00:02:50.100><c> things</c><00:02:50.310><c> first</c><00:02:51.260><c> often</c><00:02:52.260><c> you</c><00:02:52.470><c> need</c>

00:02:52.610 --> 00:02:52.620 align:start position:0%
you need two things first often you need
 

00:02:52.620 --> 00:02:54.410 align:start position:0%
you need two things first often you need
to<00:02:52.680><c> be</c><00:02:53.040><c> able</c><00:02:53.100><c> to</c><00:02:53.190><c> train</c><00:02:53.519><c> a</c><00:02:53.549><c> big</c><00:02:53.910><c> enough</c><00:02:54.150><c> neural</c>

00:02:54.410 --> 00:02:54.420 align:start position:0%
to be able to train a big enough neural
 

00:02:54.420 --> 00:02:57.350 align:start position:0%
to be able to train a big enough neural
network<00:02:55.010><c> in</c><00:02:56.010><c> order</c><00:02:56.160><c> to</c><00:02:56.459><c> take</c><00:02:56.670><c> advantage</c><00:02:56.790><c> of</c>

00:02:57.350 --> 00:02:57.360 align:start position:0%
network in order to take advantage of
 

00:02:57.360 --> 00:02:59.660 align:start position:0%
network in order to take advantage of
the<00:02:57.420><c> huge</c><00:02:57.660><c> amount</c><00:02:57.930><c> of</c><00:02:58.049><c> data</c><00:02:58.110><c> and</c><00:02:58.590><c> second</c><00:02:59.519><c> you</c>

00:02:59.660 --> 00:02:59.670 align:start position:0%
the huge amount of data and second you
 

00:02:59.670 --> 00:03:02.000 align:start position:0%
the huge amount of data and second you
need<00:02:59.849><c> to</c><00:02:59.940><c> be</c><00:03:00.030><c> out</c><00:03:00.180><c> here</c><00:03:00.540><c> on</c><00:03:00.720><c> the</c><00:03:00.989><c> x</c><00:03:01.140><c> axes</c><00:03:01.560><c> you</c><00:03:01.799><c> do</c>

00:03:02.000 --> 00:03:02.010 align:start position:0%
need to be out here on the x axes you do
 

00:03:02.010 --> 00:03:05.420 align:start position:0%
need to be out here on the x axes you do
need<00:03:02.250><c> a</c><00:03:02.280><c> lot</c><00:03:02.519><c> of</c><00:03:02.730><c> data</c><00:03:02.940><c> so</c><00:03:03.620><c> we</c><00:03:04.620><c> often</c><00:03:04.799><c> say</c><00:03:05.160><c> that</c>

00:03:05.420 --> 00:03:05.430 align:start position:0%
need a lot of data so we often say that
 

00:03:05.430 --> 00:03:07.789 align:start position:0%
need a lot of data so we often say that
scale<00:03:05.940><c> has</c><00:03:06.720><c> been</c><00:03:06.750><c> driving</c><00:03:07.380><c> deep</c><00:03:07.590><c> learning</c>

00:03:07.789 --> 00:03:07.799 align:start position:0%
scale has been driving deep learning
 

00:03:07.799 --> 00:03:10.850 align:start position:0%
scale has been driving deep learning
progress<00:03:08.220><c> and</c><00:03:08.760><c> by</c><00:03:08.910><c> scale</c><00:03:09.299><c> I</c><00:03:09.599><c> mean</c><00:03:09.840><c> both</c><00:03:10.080><c> the</c>

00:03:10.850 --> 00:03:10.860 align:start position:0%
progress and by scale I mean both the
 

00:03:10.860 --> 00:03:12.890 align:start position:0%
progress and by scale I mean both the
size<00:03:11.160><c> of</c><00:03:11.340><c> the</c><00:03:11.459><c> neural</c><00:03:11.700><c> network</c><00:03:11.730><c> we</c><00:03:12.450><c> need</c><00:03:12.630><c> just</c>

00:03:12.890 --> 00:03:12.900 align:start position:0%
size of the neural network we need just
 

00:03:12.900 --> 00:03:15.140 align:start position:0%
size of the neural network we need just
a<00:03:12.989><c> new</c><00:03:13.140><c> network</c><00:03:13.410><c> a</c><00:03:13.739><c> lot</c><00:03:14.010><c> of</c><00:03:14.069><c> hidden</c><00:03:14.489><c> units</c><00:03:14.670><c> a</c>

00:03:15.140 --> 00:03:15.150 align:start position:0%
a new network a lot of hidden units a
 

00:03:15.150 --> 00:03:17.059 align:start position:0%
a new network a lot of hidden units a
lot<00:03:15.330><c> of</c><00:03:15.480><c> parameters</c><00:03:15.720><c> a</c><00:03:16.319><c> lot</c><00:03:16.470><c> of</c><00:03:16.620><c> connections</c>

00:03:17.059 --> 00:03:17.069 align:start position:0%
lot of parameters a lot of connections
 

00:03:17.069 --> 00:03:21.470 align:start position:0%
lot of parameters a lot of connections
as<00:03:18.030><c> well</c><00:03:18.090><c> as</c><00:03:18.510><c> scale</c><00:03:19.170><c> of</c><00:03:19.500><c> the</c><00:03:19.829><c> data</c><00:03:20.489><c> in</c><00:03:21.180><c> fact</c>

00:03:21.470 --> 00:03:21.480 align:start position:0%
as well as scale of the data in fact
 

00:03:21.480 --> 00:03:23.900 align:start position:0%
as well as scale of the data in fact
today<00:03:21.750><c> one</c><00:03:22.350><c> of</c><00:03:22.380><c> the</c><00:03:22.560><c> most</c><00:03:22.739><c> reliable</c><00:03:23.160><c> ways</c><00:03:23.670><c> to</c>

00:03:23.900 --> 00:03:23.910 align:start position:0%
today one of the most reliable ways to
 

00:03:23.910 --> 00:03:25.430 align:start position:0%
today one of the most reliable ways to
get<00:03:24.090><c> better</c><00:03:24.269><c> performance</c><00:03:24.420><c> in</c><00:03:25.079><c> the</c><00:03:25.170><c> neural</c>

00:03:25.430 --> 00:03:25.440 align:start position:0%
get better performance in the neural
 

00:03:25.440 --> 00:03:27.380 align:start position:0%
get better performance in the neural
network<00:03:25.829><c> is</c><00:03:26.010><c> often</c><00:03:26.400><c> to</c><00:03:26.579><c> either</c><00:03:26.850><c> train</c><00:03:27.329><c> a</c>

00:03:27.380 --> 00:03:27.390 align:start position:0%
network is often to either train a
 

00:03:27.390 --> 00:03:29.930 align:start position:0%
network is often to either train a
bigger<00:03:27.600><c> network</c><00:03:28.079><c> or</c><00:03:28.320><c> throw</c><00:03:29.040><c> more</c><00:03:29.250><c> data</c><00:03:29.459><c> at</c><00:03:29.760><c> it</c>

00:03:29.930 --> 00:03:29.940 align:start position:0%
bigger network or throw more data at it
 

00:03:29.940 --> 00:03:31.819 align:start position:0%
bigger network or throw more data at it
and<00:03:30.150><c> that</c><00:03:30.750><c> only</c><00:03:30.959><c> works</c><00:03:31.260><c> up</c><00:03:31.440><c> to</c><00:03:31.620><c> a</c><00:03:31.650><c> point</c>

00:03:31.819 --> 00:03:31.829 align:start position:0%
and that only works up to a point
 

00:03:31.829 --> 00:03:33.349 align:start position:0%
and that only works up to a point
because<00:03:32.010><c> eventually</c><00:03:32.609><c> you</c><00:03:32.730><c> run</c><00:03:32.910><c> out</c><00:03:33.060><c> of</c><00:03:33.090><c> data</c>

00:03:33.349 --> 00:03:33.359 align:start position:0%
because eventually you run out of data
 

00:03:33.359 --> 00:03:35.630 align:start position:0%
because eventually you run out of data
or<00:03:33.690><c> eventually</c><00:03:33.930><c> then</c><00:03:34.290><c> your</c><00:03:34.440><c> network</c><00:03:34.859><c> is</c><00:03:35.100><c> so</c>

00:03:35.630 --> 00:03:35.640 align:start position:0%
or eventually then your network is so
 

00:03:35.640 --> 00:03:37.759 align:start position:0%
or eventually then your network is so
big<00:03:35.880><c> that</c><00:03:36.090><c> it</c><00:03:36.209><c> takes</c><00:03:36.359><c> too</c><00:03:36.630><c> long</c><00:03:36.660><c> to</c><00:03:36.810><c> train</c><00:03:37.170><c> but</c>

00:03:37.759 --> 00:03:37.769 align:start position:0%
big that it takes too long to train but
 

00:03:37.769 --> 00:03:40.190 align:start position:0%
big that it takes too long to train but
just<00:03:38.340><c> improving</c><00:03:38.760><c> scale</c><00:03:39.269><c> has</c><00:03:39.600><c> actually</c><00:03:39.930><c> taken</c>

00:03:40.190 --> 00:03:40.200 align:start position:0%
just improving scale has actually taken
 

00:03:40.200 --> 00:03:42.680 align:start position:0%
just improving scale has actually taken
us<00:03:40.350><c> a</c><00:03:40.470><c> long</c><00:03:40.500><c> way</c><00:03:40.739><c> in</c><00:03:41.549><c> the</c><00:03:41.670><c> world</c><00:03:41.880><c> of</c><00:03:42.150><c> learning</c>

00:03:42.680 --> 00:03:42.690 align:start position:0%
us a long way in the world of learning
 

00:03:42.690 --> 00:03:45.800 align:start position:0%
us a long way in the world of learning
in<00:03:43.590><c> order</c><00:03:43.920><c> to</c><00:03:44.100><c> make</c><00:03:44.310><c> this</c><00:03:44.489><c> diagram</c><00:03:45.329><c> a</c><00:03:45.450><c> bit</c><00:03:45.600><c> more</c>

00:03:45.800 --> 00:03:45.810 align:start position:0%
in order to make this diagram a bit more
 

00:03:45.810 --> 00:03:48.050 align:start position:0%
in order to make this diagram a bit more
technically<00:03:46.350><c> precise</c><00:03:46.769><c> and</c><00:03:47.040><c> just</c><00:03:47.250><c> add</c><00:03:47.639><c> a</c><00:03:47.700><c> few</c>

00:03:48.050 --> 00:03:48.060 align:start position:0%
technically precise and just add a few
 

00:03:48.060 --> 00:03:49.910 align:start position:0%
technically precise and just add a few
more<00:03:48.090><c> things</c><00:03:48.510><c> I</c><00:03:48.810><c> wrote</c><00:03:49.139><c> the</c><00:03:49.319><c> amount</c><00:03:49.530><c> of</c><00:03:49.709><c> data</c>

00:03:49.910 --> 00:03:49.920 align:start position:0%
more things I wrote the amount of data
 

00:03:49.920 --> 00:03:53.030 align:start position:0%
more things I wrote the amount of data
on<00:03:50.370><c> the</c><00:03:50.459><c> x-axis</c><00:03:51.170><c> technically</c><00:03:52.170><c> this</c><00:03:52.380><c> is</c><00:03:52.590><c> amount</c>

00:03:53.030 --> 00:03:53.040 align:start position:0%
on the x-axis technically this is amount
 

00:03:53.040 --> 00:03:57.890 align:start position:0%
on the x-axis technically this is amount
of<00:03:53.420><c> labeled</c><00:03:54.420><c> data</c><00:03:55.760><c> where</c><00:03:56.760><c> by</c><00:03:57.120><c> label</c><00:03:57.510><c> data</c>

00:03:57.890 --> 00:03:57.900 align:start position:0%
of labeled data where by label data
 

00:03:57.900 --> 00:04:00.170 align:start position:0%
of labeled data where by label data
I<00:03:58.109><c> mean</c><00:03:58.350><c> training</c><00:03:59.040><c> examples</c><00:03:59.489><c> we</c><00:03:59.700><c> have</c><00:03:59.910><c> both</c>

00:04:00.170 --> 00:04:00.180 align:start position:0%
I mean training examples we have both
 

00:04:00.180 --> 00:04:03.620 align:start position:0%
I mean training examples we have both
the<00:04:00.359><c> input</c><00:04:00.690><c> X</c><00:04:00.870><c> and</c><00:04:00.989><c> the</c><00:04:01.380><c> label</c><00:04:01.739><c> Y</c><00:04:01.920><c> I</c><00:04:02.269><c> went</c><00:04:03.269><c> to</c>

00:04:03.620 --> 00:04:03.630 align:start position:0%
the input X and the label Y I went to
 

00:04:03.630 --> 00:04:05.900 align:start position:0%
the input X and the label Y I went to
introduce<00:04:04.200><c> a</c><00:04:04.530><c> little</c><00:04:04.889><c> bit</c><00:04:05.040><c> of</c><00:04:05.190><c> notation</c><00:04:05.519><c> that</c>

00:04:05.900 --> 00:04:05.910 align:start position:0%
introduce a little bit of notation that
 

00:04:05.910 --> 00:04:07.699 align:start position:0%
introduce a little bit of notation that
we'll<00:04:06.090><c> use</c><00:04:06.120><c> later</c><00:04:06.299><c> in</c><00:04:06.750><c> this</c><00:04:07.109><c> course</c><00:04:07.380><c> we're</c>

00:04:07.699 --> 00:04:07.709 align:start position:0%
we'll use later in this course we're
 

00:04:07.709 --> 00:04:10.759 align:start position:0%
we'll use later in this course we're
going<00:04:07.739><c> to</c><00:04:07.980><c> use</c><00:04:08.180><c> lowercase</c><00:04:09.180><c> alphabet</c><00:04:09.989><c> to</c>

00:04:10.759 --> 00:04:10.769 align:start position:0%
going to use lowercase alphabet to
 

00:04:10.769 --> 00:04:12.530 align:start position:0%
going to use lowercase alphabet to
denote<00:04:11.069><c> the</c><00:04:11.280><c> size</c><00:04:11.519><c> of</c><00:04:11.670><c> my</c><00:04:11.760><c> training</c><00:04:11.819><c> sets</c><00:04:12.329><c> or</c>

00:04:12.530 --> 00:04:12.540 align:start position:0%
denote the size of my training sets or
 

00:04:12.540 --> 00:04:13.729 align:start position:0%
denote the size of my training sets or
the<00:04:12.630><c> number</c><00:04:12.870><c> of</c><00:04:12.930><c> training</c><00:04:13.139><c> examples</c>

00:04:13.729 --> 00:04:13.739 align:start position:0%
the number of training examples
 

00:04:13.739 --> 00:04:15.680 align:start position:0%
the number of training examples
this<00:04:14.220><c> lowercase</c><00:04:14.700><c> M</c><00:04:14.970><c> so</c><00:04:15.299><c> that's</c><00:04:15.510><c> the</c>

00:04:15.680 --> 00:04:15.690 align:start position:0%
this lowercase M so that's the
 

00:04:15.690 --> 00:04:18.979 align:start position:0%
this lowercase M so that's the
horizontal<00:04:16.169><c> axis</c><00:04:17.000><c> couple</c><00:04:18.000><c> other</c><00:04:18.209><c> details</c><00:04:18.750><c> to</c>

00:04:18.979 --> 00:04:18.989 align:start position:0%
horizontal axis couple other details to
 

00:04:18.989 --> 00:04:20.300 align:start position:0%
horizontal axis couple other details to
this<00:04:19.109><c> Tigger</c>

00:04:20.300 --> 00:04:20.310 align:start position:0%
this Tigger
 

00:04:20.310 --> 00:04:23.330 align:start position:0%
this Tigger
in<00:04:20.370><c> this</c><00:04:20.639><c> regime</c><00:04:21.090><c> of</c><00:04:21.660><c> smaller</c><00:04:22.590><c> training</c><00:04:22.919><c> sets</c>

00:04:23.330 --> 00:04:23.340 align:start position:0%
in this regime of smaller training sets
 

00:04:23.340 --> 00:04:26.960 align:start position:0%
in this regime of smaller training sets
the<00:04:24.180><c> relative</c><00:04:24.870><c> ordering</c><00:04:25.440><c> of</c><00:04:25.800><c> the</c><00:04:26.280><c> algorithms</c>

00:04:26.960 --> 00:04:26.970 align:start position:0%
the relative ordering of the algorithms
 

00:04:26.970 --> 00:04:29.690 align:start position:0%
the relative ordering of the algorithms
is<00:04:27.180><c> actually</c><00:04:27.510><c> not</c><00:04:27.720><c> very</c><00:04:27.750><c> well</c><00:04:28.200><c> defined</c><00:04:28.410><c> so</c><00:04:29.400><c> if</c>

00:04:29.690 --> 00:04:29.700 align:start position:0%
is actually not very well defined so if
 

00:04:29.700 --> 00:04:31.580 align:start position:0%
is actually not very well defined so if
you<00:04:29.850><c> don't</c><00:04:30.060><c> have</c><00:04:30.270><c> a</c><00:04:30.300><c> lot</c><00:04:30.630><c> of</c><00:04:30.660><c> training</c><00:04:31.050><c> data</c><00:04:31.410><c> is</c>

00:04:31.580 --> 00:04:31.590 align:start position:0%
you don't have a lot of training data is
 

00:04:31.590 --> 00:04:34.490 align:start position:0%
you don't have a lot of training data is
often<00:04:32.070><c> up</c><00:04:32.490><c> to</c><00:04:32.550><c> your</c><00:04:33.389><c> skill</c><00:04:33.810><c> at</c><00:04:34.110><c> hand</c>

00:04:34.490 --> 00:04:34.500 align:start position:0%
often up to your skill at hand
 

00:04:34.500 --> 00:04:36.500 align:start position:0%
often up to your skill at hand
engineering<00:04:34.830><c> features</c><00:04:35.520><c> that</c><00:04:35.790><c> determines</c><00:04:36.389><c> the</c>

00:04:36.500 --> 00:04:36.510 align:start position:0%
engineering features that determines the
 

00:04:36.510 --> 00:04:39.080 align:start position:0%
engineering features that determines the
foreman<00:04:36.930><c> so</c><00:04:37.200><c> it's</c><00:04:37.380><c> quite</c><00:04:37.500><c> possible</c><00:04:37.770><c> that</c><00:04:38.190><c> if</c>

00:04:39.080 --> 00:04:39.090 align:start position:0%
foreman so it's quite possible that if
 

00:04:39.090 --> 00:04:41.900 align:start position:0%
foreman so it's quite possible that if
someone<00:04:39.690><c> training</c><00:04:40.380><c> an</c><00:04:40.590><c> SVM</c><00:04:41.310><c> is</c><00:04:41.430><c> more</c>

00:04:41.900 --> 00:04:41.910 align:start position:0%
someone training an SVM is more
 

00:04:41.910 --> 00:04:44.060 align:start position:0%
someone training an SVM is more
motivated<00:04:42.600><c> to</c><00:04:42.630><c> hand</c><00:04:43.020><c> engineer</c><00:04:43.410><c> features</c><00:04:43.800><c> and</c>

00:04:44.060 --> 00:04:44.070 align:start position:0%
motivated to hand engineer features and
 

00:04:44.070 --> 00:04:46.310 align:start position:0%
motivated to hand engineer features and
someone<00:04:44.460><c> training</c><00:04:45.210><c> even</c><00:04:45.600><c> large</c><00:04:45.990><c> their</c><00:04:46.200><c> own</c>

00:04:46.310 --> 00:04:46.320 align:start position:0%
someone training even large their own
 

00:04:46.320 --> 00:04:48.290 align:start position:0%
someone training even large their own
that<00:04:46.470><c> may</c><00:04:46.770><c> be</c><00:04:46.830><c> in</c><00:04:47.220><c> this</c><00:04:47.580><c> small</c><00:04:47.880><c> training</c><00:04:48.180><c> set</c>

00:04:48.290 --> 00:04:48.300 align:start position:0%
that may be in this small training set
 

00:04:48.300 --> 00:04:50.720 align:start position:0%
that may be in this small training set
regime<00:04:48.810><c> the</c><00:04:49.440><c> SEM</c><00:04:49.889><c> could</c><00:04:50.160><c> do</c><00:04:50.310><c> better</c>

00:04:50.720 --> 00:04:50.730 align:start position:0%
regime the SEM could do better
 

00:04:50.730 --> 00:04:53.120 align:start position:0%
regime the SEM could do better
so<00:04:51.000><c> you</c><00:04:51.480><c> know</c><00:04:51.570><c> in</c><00:04:51.780><c> this</c><00:04:51.990><c> region</c><00:04:52.770><c> to</c><00:04:52.919><c> the</c><00:04:53.100><c> left</c>

00:04:53.120 --> 00:04:53.130 align:start position:0%
so you know in this region to the left
 

00:04:53.130 --> 00:04:55.010 align:start position:0%
so you know in this region to the left
of<00:04:53.430><c> the</c><00:04:53.550><c> figure</c><00:04:53.850><c> the</c><00:04:54.210><c> relative</c><00:04:54.630><c> ordering</c>

00:04:55.010 --> 00:04:55.020 align:start position:0%
of the figure the relative ordering
 

00:04:55.020 --> 00:04:57.080 align:start position:0%
of the figure the relative ordering
between<00:04:55.440><c> gene</c><00:04:55.590><c> algorithms</c><00:04:56.040><c> is</c><00:04:56.160><c> not</c><00:04:56.639><c> that</c><00:04:56.880><c> well</c>

00:04:57.080 --> 00:04:57.090 align:start position:0%
between gene algorithms is not that well
 

00:04:57.090 --> 00:04:59.540 align:start position:0%
between gene algorithms is not that well
defined<00:04:57.120><c> and</c><00:04:57.889><c> performance</c><00:04:58.889><c> depends</c><00:04:59.370><c> much</c>

00:04:59.540 --> 00:04:59.550 align:start position:0%
defined and performance depends much
 

00:04:59.550 --> 00:05:01.909 align:start position:0%
defined and performance depends much
more<00:04:59.610><c> on</c><00:05:00.030><c> your</c><00:05:00.210><c> skill</c><00:05:00.570><c> at</c><00:05:00.840><c> engine</c><00:05:01.380><c> features</c>

00:05:01.909 --> 00:05:01.919 align:start position:0%
more on your skill at engine features
 

00:05:01.919 --> 00:05:03.379 align:start position:0%
more on your skill at engine features
and<00:05:02.250><c> other</c><00:05:02.370><c> mobile</c><00:05:02.639><c> details</c><00:05:03.150><c> of</c><00:05:03.300><c> the</c>

00:05:03.379 --> 00:05:03.389 align:start position:0%
and other mobile details of the
 

00:05:03.389 --> 00:05:05.960 align:start position:0%
and other mobile details of the
algorithms<00:05:03.900><c> and</c><00:05:04.229><c> there's</c><00:05:04.860><c> only</c><00:05:05.040><c> in</c><00:05:05.430><c> this</c><00:05:05.669><c> some</c>

00:05:05.960 --> 00:05:05.970 align:start position:0%
algorithms and there's only in this some
 

00:05:05.970 --> 00:05:08.840 align:start position:0%
algorithms and there's only in this some
big<00:05:06.660><c> data</c><00:05:06.900><c> regime</c><00:05:07.440><c> very</c><00:05:07.830><c> large</c><00:05:08.130><c> training</c><00:05:08.490><c> sets</c>

00:05:08.840 --> 00:05:08.850 align:start position:0%
big data regime very large training sets
 

00:05:08.850 --> 00:05:11.990 align:start position:0%
big data regime very large training sets
very<00:05:09.120><c> large</c><00:05:09.450><c> M</c><00:05:09.690><c> regime</c><00:05:10.410><c> in</c><00:05:10.560><c> the</c><00:05:10.620><c> right</c><00:05:10.830><c> that</c><00:05:11.820><c> we</c>

00:05:11.990 --> 00:05:12.000 align:start position:0%
very large M regime in the right that we
 

00:05:12.000 --> 00:05:14.659 align:start position:0%
very large M regime in the right that we
more<00:05:12.210><c> consistently</c><00:05:12.960><c> see</c><00:05:13.320><c> largely</c><00:05:14.190><c> Ronettes</c>

00:05:14.659 --> 00:05:14.669 align:start position:0%
more consistently see largely Ronettes
 

00:05:14.669 --> 00:05:17.629 align:start position:0%
more consistently see largely Ronettes
dominating<00:05:15.660><c> the</c><00:05:16.080><c> other</c><00:05:16.110><c> approaches</c><00:05:16.860><c> and</c><00:05:17.040><c> so</c>

00:05:17.629 --> 00:05:17.639 align:start position:0%
dominating the other approaches and so
 

00:05:17.639 --> 00:05:19.550 align:start position:0%
dominating the other approaches and so
if<00:05:17.970><c> any</c><00:05:18.210><c> of</c><00:05:18.330><c> your</c><00:05:18.450><c> friends</c><00:05:18.780><c> ask</c><00:05:18.900><c> you</c><00:05:19.169><c> why</c><00:05:19.410><c> are</c>

00:05:19.550 --> 00:05:19.560 align:start position:0%
if any of your friends ask you why are
 

00:05:19.560 --> 00:05:21.590 align:start position:0%
if any of your friends ask you why are
known<00:05:19.740><c> as</c><00:05:19.979><c> you</c><00:05:20.520><c> know</c><00:05:20.580><c> taking</c><00:05:20.940><c> off</c><00:05:21.120><c> I</c><00:05:21.360><c> would</c>

00:05:21.590 --> 00:05:21.600 align:start position:0%
known as you know taking off I would
 

00:05:21.600 --> 00:05:23.690 align:start position:0%
known as you know taking off I would
encourage<00:05:21.900><c> you</c><00:05:22.200><c> to</c><00:05:22.440><c> draw</c><00:05:22.770><c> this</c><00:05:22.919><c> picture</c><00:05:22.979><c> for</c>

00:05:23.690 --> 00:05:23.700 align:start position:0%
encourage you to draw this picture for
 

00:05:23.700 --> 00:05:26.719 align:start position:0%
encourage you to draw this picture for
them<00:05:23.940><c> as</c><00:05:24.120><c> well</c><00:05:24.330><c> so</c><00:05:24.590><c> I</c><00:05:25.590><c> will</c><00:05:25.860><c> say</c><00:05:26.070><c> that</c><00:05:26.280><c> in</c><00:05:26.520><c> the</c>

00:05:26.719 --> 00:05:26.729 align:start position:0%
them as well so I will say that in the
 

00:05:26.729 --> 00:05:28.880 align:start position:0%
them as well so I will say that in the
early<00:05:27.000><c> days</c><00:05:27.450><c> in</c><00:05:27.720><c> their</c><00:05:27.870><c> modern</c><00:05:28.169><c> rise</c><00:05:28.500><c> of</c><00:05:28.710><c> deep</c>

00:05:28.880 --> 00:05:28.890 align:start position:0%
early days in their modern rise of deep
 

00:05:28.890 --> 00:05:29.300 align:start position:0%
early days in their modern rise of deep
learning

00:05:29.300 --> 00:05:29.310 align:start position:0%
learning
 

00:05:29.310 --> 00:05:32.060 align:start position:0%
learning
it<00:05:29.460><c> was</c><00:05:29.669><c> scaled</c><00:05:30.539><c> data</c><00:05:30.870><c> and</c><00:05:31.320><c> scale</c><00:05:31.950><c> of</c>

00:05:32.060 --> 00:05:32.070 align:start position:0%
it was scaled data and scale of
 

00:05:32.070 --> 00:05:34.909 align:start position:0%
it was scaled data and scale of
computation<00:05:32.870><c> just</c><00:05:33.870><c> our</c><00:05:33.990><c> ability</c><00:05:34.080><c> to</c><00:05:34.620><c> Train</c>

00:05:34.909 --> 00:05:34.919 align:start position:0%
computation just our ability to Train
 

00:05:34.919 --> 00:05:36.320 align:start position:0%
computation just our ability to Train
very<00:05:35.070><c> large</c><00:05:35.490><c> dinner</c><00:05:35.700><c> networks</c>

00:05:36.320 --> 00:05:36.330 align:start position:0%
very large dinner networks
 

00:05:36.330 --> 00:05:39.469 align:start position:0%
very large dinner networks
either<00:05:36.539><c> on</c><00:05:36.750><c> a</c><00:05:36.810><c> CPU</c><00:05:37.229><c> or</c><00:05:37.320><c> GPU</c><00:05:37.669><c> that</c><00:05:38.669><c> enabled</c><00:05:39.270><c> us</c>

00:05:39.469 --> 00:05:39.479 align:start position:0%
either on a CPU or GPU that enabled us
 

00:05:39.479 --> 00:05:41.840 align:start position:0%
either on a CPU or GPU that enabled us
to<00:05:39.510><c> make</c><00:05:40.080><c> a</c><00:05:40.110><c> lot</c><00:05:40.350><c> of</c><00:05:40.380><c> progress</c><00:05:40.850><c> but</c>

00:05:41.840 --> 00:05:41.850 align:start position:0%
to make a lot of progress but
 

00:05:41.850 --> 00:05:43.580 align:start position:0%
to make a lot of progress but
increasingly<00:05:42.510><c> especially</c><00:05:42.990><c> in</c><00:05:43.320><c> the</c><00:05:43.380><c> last</c>

00:05:43.580 --> 00:05:43.590 align:start position:0%
increasingly especially in the last
 

00:05:43.590 --> 00:05:45.790 align:start position:0%
increasingly especially in the last
several<00:05:43.860><c> years</c><00:05:43.979><c> we've</c><00:05:44.370><c> seen</c><00:05:44.700><c> tremendous</c>

00:05:45.790 --> 00:05:45.800 align:start position:0%
several years we've seen tremendous
 

00:05:45.800 --> 00:05:48.350 align:start position:0%
several years we've seen tremendous
algorithmic<00:05:46.800><c> innovation</c><00:05:47.520><c> as</c><00:05:47.669><c> well</c><00:05:47.700><c> so</c><00:05:48.090><c> I</c><00:05:48.150><c> also</c>

00:05:48.350 --> 00:05:48.360 align:start position:0%
algorithmic innovation as well so I also
 

00:05:48.360 --> 00:05:50.529 align:start position:0%
algorithmic innovation as well so I also
don't<00:05:48.840><c> want</c><00:05:49.050><c> to</c><00:05:49.139><c> understate</c><00:05:49.680><c> that</c>

00:05:50.529 --> 00:05:50.539 align:start position:0%
don't want to understate that
 

00:05:50.539 --> 00:05:53.690 align:start position:0%
don't want to understate that
interestingly<00:05:51.680><c> many</c><00:05:52.680><c> of</c><00:05:52.860><c> the</c><00:05:52.979><c> algorithmic</c>

00:05:53.690 --> 00:05:53.700 align:start position:0%
interestingly many of the algorithmic
 

00:05:53.700 --> 00:05:56.930 align:start position:0%
interestingly many of the algorithmic
innovations<00:05:54.600><c> have</c><00:05:55.229><c> been</c><00:05:55.260><c> about</c><00:05:56.130><c> trying</c><00:05:56.880><c> to</c>

00:05:56.930 --> 00:05:56.940 align:start position:0%
innovations have been about trying to
 

00:05:56.940 --> 00:06:01.129 align:start position:0%
innovations have been about trying to
make<00:05:57.180><c> neural</c><00:05:57.630><c> networks</c><00:05:57.990><c> run</c><00:05:58.410><c> much</c><00:05:58.620><c> faster</c><00:06:00.139><c> so</c>

00:06:01.129 --> 00:06:01.139 align:start position:0%
make neural networks run much faster so
 

00:06:01.139 --> 00:06:03.500 align:start position:0%
make neural networks run much faster so
as<00:06:01.530><c> a</c><00:06:01.830><c> concrete</c><00:06:02.190><c> example</c><00:06:02.340><c> one</c><00:06:03.030><c> of</c><00:06:03.060><c> the</c><00:06:03.270><c> huge</c>

00:06:03.500 --> 00:06:03.510 align:start position:0%
as a concrete example one of the huge
 

00:06:03.510 --> 00:06:05.300 align:start position:0%
as a concrete example one of the huge
breakthroughs<00:06:04.020><c> in</c><00:06:04.260><c> your</c><00:06:04.410><c> networks</c><00:06:04.919><c> has</c><00:06:05.100><c> been</c>

00:06:05.300 --> 00:06:05.310 align:start position:0%
breakthroughs in your networks has been
 

00:06:05.310 --> 00:06:08.719 align:start position:0%
breakthroughs in your networks has been
switching<00:06:05.940><c> from</c><00:06:06.360><c> a</c><00:06:06.450><c> sigmoid</c><00:06:06.960><c> function</c><00:06:07.729><c> which</c>

00:06:08.719 --> 00:06:08.729 align:start position:0%
switching from a sigmoid function which
 

00:06:08.729 --> 00:06:12.320 align:start position:0%
switching from a sigmoid function which
looks<00:06:08.760><c> like</c><00:06:09.210><c> this</c><00:06:09.450><c> to</c><00:06:10.289><c> a</c><00:06:10.700><c> railer</c><00:06:11.700><c> function</c>

00:06:12.320 --> 00:06:12.330 align:start position:0%
looks like this to a railer function
 

00:06:12.330 --> 00:06:14.750 align:start position:0%
looks like this to a railer function
which<00:06:12.840><c> we</c><00:06:12.960><c> talked</c><00:06:13.200><c> about</c><00:06:13.289><c> briefly</c><00:06:13.560><c> in</c><00:06:14.130><c> an</c>

00:06:14.750 --> 00:06:14.760 align:start position:0%
which we talked about briefly in an
 

00:06:14.760 --> 00:06:18.469 align:start position:0%
which we talked about briefly in an
early<00:06:15.090><c> video</c><00:06:15.650><c> that</c><00:06:16.650><c> looks</c><00:06:16.860><c> like</c><00:06:16.979><c> this</c><00:06:17.270><c> if</c><00:06:18.270><c> you</c>

00:06:18.469 --> 00:06:18.479 align:start position:0%
early video that looks like this if you
 

00:06:18.479 --> 00:06:20.180 align:start position:0%
early video that looks like this if you
don't<00:06:18.720><c> understand</c><00:06:19.169><c> the</c><00:06:19.260><c> details</c><00:06:19.680><c> of</c><00:06:19.950><c> one</c>

00:06:20.180 --> 00:06:20.190 align:start position:0%
don't understand the details of one
 

00:06:20.190 --> 00:06:22.250 align:start position:0%
don't understand the details of one
about<00:06:20.370><c> the</c><00:06:20.550><c> state</c><00:06:20.729><c> don't</c><00:06:21.030><c> worry</c><00:06:21.180><c> about</c><00:06:21.270><c> it</c><00:06:21.660><c> but</c>

00:06:22.250 --> 00:06:22.260 align:start position:0%
about the state don't worry about it but
 

00:06:22.260 --> 00:06:24.379 align:start position:0%
about the state don't worry about it but
it<00:06:22.440><c> turns</c><00:06:22.560><c> out</c><00:06:22.860><c> that</c><00:06:23.160><c> one</c><00:06:23.550><c> of</c><00:06:23.669><c> the</c><00:06:23.760><c> problems</c><00:06:24.240><c> of</c>

00:06:24.379 --> 00:06:24.389 align:start position:0%
it turns out that one of the problems of
 

00:06:24.389 --> 00:06:26.000 align:start position:0%
it turns out that one of the problems of
using<00:06:24.570><c> sigmoid</c><00:06:25.169><c> functions</c><00:06:25.620><c> and</c><00:06:25.800><c> machine</c>

00:06:26.000 --> 00:06:26.010 align:start position:0%
using sigmoid functions and machine
 

00:06:26.010 --> 00:06:27.860 align:start position:0%
using sigmoid functions and machine
learning<00:06:26.039><c> is</c><00:06:26.550><c> that</c><00:06:26.700><c> there</c><00:06:27.210><c> these</c><00:06:27.360><c> regions</c>

00:06:27.860 --> 00:06:27.870 align:start position:0%
learning is that there these regions
 

00:06:27.870 --> 00:06:29.510 align:start position:0%
learning is that there these regions
here<00:06:28.200><c> where</c><00:06:28.800><c> the</c><00:06:28.890><c> slope</c><00:06:29.159><c> of</c><00:06:29.310><c> the</c><00:06:29.400><c> function</c>

00:06:29.510 --> 00:06:29.520 align:start position:0%
here where the slope of the function
 

00:06:29.520 --> 00:06:30.270 align:start position:0%
here where the slope of the function
would

00:06:30.270 --> 00:06:30.280 align:start position:0%
would
 

00:06:30.280 --> 00:06:32.910 align:start position:0%
would
gradient<00:06:30.790><c> is</c><00:06:30.940><c> nearly</c><00:06:31.300><c> zero</c><00:06:31.720><c> and</c><00:06:32.050><c> so</c><00:06:32.290><c> learning</c>

00:06:32.910 --> 00:06:32.920 align:start position:0%
gradient is nearly zero and so learning
 

00:06:32.920 --> 00:06:35.340 align:start position:0%
gradient is nearly zero and so learning
becomes<00:06:33.310><c> really</c><00:06:33.639><c> slow</c><00:06:34.090><c> because</c><00:06:34.960><c> when</c><00:06:35.290><c> you</c>

00:06:35.340 --> 00:06:35.350 align:start position:0%
becomes really slow because when you
 

00:06:35.350 --> 00:06:37.050 align:start position:0%
becomes really slow because when you
implement<00:06:35.620><c> gradient</c><00:06:35.920><c> descent</c><00:06:36.430><c> and</c><00:06:36.610><c> gradient</c>

00:06:37.050 --> 00:06:37.060 align:start position:0%
implement gradient descent and gradient
 

00:06:37.060 --> 00:06:39.629 align:start position:0%
implement gradient descent and gradient
is<00:06:37.180><c> zero</c><00:06:37.510><c> the</c><00:06:38.110><c> parameters</c><00:06:38.740><c> just</c><00:06:38.770><c> change</c><00:06:39.340><c> very</c>

00:06:39.629 --> 00:06:39.639 align:start position:0%
is zero the parameters just change very
 

00:06:39.639 --> 00:06:41.460 align:start position:0%
is zero the parameters just change very
slowly<00:06:40.030><c> and</c><00:06:40.330><c> so</c><00:06:40.480><c> learning</c><00:06:40.630><c> is</c><00:06:40.840><c> very</c><00:06:41.110><c> slow</c>

00:06:41.460 --> 00:06:41.470 align:start position:0%
slowly and so learning is very slow
 

00:06:41.470 --> 00:06:44.730 align:start position:0%
slowly and so learning is very slow
whereas<00:06:42.190><c> by</c><00:06:42.400><c> changing</c><00:06:43.030><c> the</c><00:06:43.500><c> what's</c><00:06:44.500><c> called</c>

00:06:44.730 --> 00:06:44.740 align:start position:0%
whereas by changing the what's called
 

00:06:44.740 --> 00:06:46.440 align:start position:0%
whereas by changing the what's called
the<00:06:44.889><c> activation</c><00:06:45.370><c> function</c><00:06:45.610><c> the</c><00:06:46.210><c> neural</c>

00:06:46.440 --> 00:06:46.450 align:start position:0%
the activation function the neural
 

00:06:46.450 --> 00:06:48.590 align:start position:0%
the activation function the neural
network<00:06:46.870><c> to</c><00:06:47.020><c> use</c><00:06:47.169><c> this</c><00:06:47.410><c> function</c><00:06:47.650><c> called</c><00:06:48.310><c> the</c>

00:06:48.590 --> 00:06:48.600 align:start position:0%
network to use this function called the
 

00:06:48.600 --> 00:06:52.050 align:start position:0%
network to use this function called the
value<00:06:50.160><c> function</c><00:06:51.160><c> of</c><00:06:51.340><c> the</c><00:06:51.400><c> rectified</c><00:06:51.880><c> linear</c>

00:06:52.050 --> 00:06:52.060 align:start position:0%
value function of the rectified linear
 

00:06:52.060 --> 00:06:54.960 align:start position:0%
value function of the rectified linear
unit<00:06:52.570><c> our</c><00:06:52.870><c> elu</c><00:06:53.560><c> the</c><00:06:53.950><c> gradient</c><00:06:54.460><c> is</c><00:06:54.580><c> equal</c><00:06:54.700><c> to</c>

00:06:54.960 --> 00:06:54.970 align:start position:0%
unit our elu the gradient is equal to
 

00:06:54.970 --> 00:06:57.060 align:start position:0%
unit our elu the gradient is equal to
one<00:06:55.210><c> for</c><00:06:55.570><c> all</c><00:06:55.600><c> positive</c><00:06:56.080><c> values</c><00:06:56.740><c> of</c><00:06:56.889><c> input</c>

00:06:57.060 --> 00:06:57.070 align:start position:0%
one for all positive values of input
 

00:06:57.070 --> 00:07:00.210 align:start position:0%
one for all positive values of input
right<00:06:57.550><c> and</c><00:06:57.790><c> so</c><00:06:58.480><c> the</c><00:06:59.050><c> gradient</c><00:06:59.350><c> is</c><00:06:59.770><c> much</c><00:06:59.980><c> less</c>

00:07:00.210 --> 00:07:00.220 align:start position:0%
right and so the gradient is much less
 

00:07:00.220 --> 00:07:03.090 align:start position:0%
right and so the gradient is much less
likely<00:07:00.250><c> to</c><00:07:00.960><c> gradually</c><00:07:01.960><c> shrink</c><00:07:02.320><c> to</c><00:07:02.500><c> zero</c><00:07:02.800><c> and</c>

00:07:03.090 --> 00:07:03.100 align:start position:0%
likely to gradually shrink to zero and
 

00:07:03.100 --> 00:07:04.740 align:start position:0%
likely to gradually shrink to zero and
the<00:07:03.370><c> gradient</c><00:07:03.610><c> here</c><00:07:03.940><c> the</c><00:07:04.210><c> slope</c><00:07:04.419><c> of</c><00:07:04.570><c> this</c><00:07:04.690><c> line</c>

00:07:04.740 --> 00:07:04.750 align:start position:0%
the gradient here the slope of this line
 

00:07:04.750 --> 00:07:07.290 align:start position:0%
the gradient here the slope of this line
is<00:07:05.110><c> zero</c><00:07:05.470><c> on</c><00:07:05.620><c> the</c><00:07:05.770><c> left</c><00:07:05.980><c> but</c><00:07:06.310><c> it</c><00:07:06.790><c> turns</c><00:07:07.180><c> out</c>

00:07:07.290 --> 00:07:07.300 align:start position:0%
is zero on the left but it turns out
 

00:07:07.300 --> 00:07:09.510 align:start position:0%
is zero on the left but it turns out
that<00:07:07.360><c> just</c><00:07:07.960><c> by</c><00:07:08.080><c> switching</c><00:07:08.380><c> to</c><00:07:08.650><c> the</c><00:07:09.040><c> sigmoid</c>

00:07:09.510 --> 00:07:09.520 align:start position:0%
that just by switching to the sigmoid
 

00:07:09.520 --> 00:07:12.570 align:start position:0%
that just by switching to the sigmoid
function<00:07:10.000><c> to</c><00:07:10.900><c> the</c><00:07:11.050><c> rayleigh</c><00:07:11.410><c> function</c><00:07:11.650><c> has</c>

00:07:12.570 --> 00:07:12.580 align:start position:0%
function to the rayleigh function has
 

00:07:12.580 --> 00:07:14.400 align:start position:0%
function to the rayleigh function has
made<00:07:12.880><c> an</c><00:07:13.030><c> algorithm</c><00:07:13.510><c> called</c><00:07:13.870><c> gradient</c>

00:07:14.400 --> 00:07:14.410 align:start position:0%
made an algorithm called gradient
 

00:07:14.410 --> 00:07:16.950 align:start position:0%
made an algorithm called gradient
descent<00:07:14.800><c> work</c><00:07:15.250><c> much</c><00:07:15.550><c> faster</c><00:07:15.580><c> and</c><00:07:16.270><c> so</c><00:07:16.630><c> this</c><00:07:16.870><c> is</c>

00:07:16.950 --> 00:07:16.960 align:start position:0%
descent work much faster and so this is
 

00:07:16.960 --> 00:07:19.159 align:start position:0%
descent work much faster and so this is
an<00:07:17.020><c> example</c><00:07:17.320><c> of</c><00:07:17.500><c> maybe</c><00:07:18.040><c> relatively</c><00:07:18.640><c> simple</c>

00:07:19.159 --> 00:07:19.169 align:start position:0%
an example of maybe relatively simple
 

00:07:19.169 --> 00:07:22.020 align:start position:0%
an example of maybe relatively simple
algorithm<00:07:20.169><c> in</c><00:07:20.320><c> Bayesian</c><00:07:20.770><c> but</c><00:07:21.250><c> ultimately</c><00:07:21.669><c> the</c>

00:07:22.020 --> 00:07:22.030 align:start position:0%
algorithm in Bayesian but ultimately the
 

00:07:22.030 --> 00:07:23.850 align:start position:0%
algorithm in Bayesian but ultimately the
impact<00:07:22.450><c> of</c><00:07:22.630><c> this</c><00:07:22.720><c> algorithmic</c><00:07:23.290><c> innovation</c>

00:07:23.850 --> 00:07:23.860 align:start position:0%
impact of this algorithmic innovation
 

00:07:23.860 --> 00:07:27.510 align:start position:0%
impact of this algorithmic innovation
was<00:07:24.100><c> it</c><00:07:24.310><c> really</c><00:07:24.610><c> hope</c><00:07:25.050><c> computation</c><00:07:26.310><c> so</c><00:07:27.310><c> the</c>

00:07:27.510 --> 00:07:27.520 align:start position:0%
was it really hope computation so the
 

00:07:27.520 --> 00:07:29.070 align:start position:0%
was it really hope computation so the
regimen<00:07:27.880><c> quite</c><00:07:28.090><c> a</c><00:07:28.120><c> lot</c><00:07:28.330><c> of</c><00:07:28.479><c> examples</c><00:07:28.960><c> like</c>

00:07:29.070 --> 00:07:29.080 align:start position:0%
regimen quite a lot of examples like
 

00:07:29.080 --> 00:07:31.230 align:start position:0%
regimen quite a lot of examples like
this<00:07:29.380><c> of</c><00:07:29.740><c> where</c><00:07:29.979><c> we</c><00:07:30.430><c> change</c><00:07:30.729><c> the</c><00:07:30.910><c> algorithm</c>

00:07:31.230 --> 00:07:31.240 align:start position:0%
this of where we change the algorithm
 

00:07:31.240 --> 00:07:33.330 align:start position:0%
this of where we change the algorithm
because<00:07:31.810><c> it</c><00:07:31.990><c> allows</c><00:07:32.260><c> that</c><00:07:32.440><c> code</c><00:07:32.740><c> to</c><00:07:32.889><c> run</c><00:07:33.100><c> much</c>

00:07:33.330 --> 00:07:33.340 align:start position:0%
because it allows that code to run much
 

00:07:33.340 --> 00:07:35.130 align:start position:0%
because it allows that code to run much
faster<00:07:33.370><c> and</c><00:07:33.970><c> this</c><00:07:34.150><c> allows</c><00:07:34.450><c> us</c><00:07:34.479><c> to</c><00:07:34.720><c> train</c>

00:07:35.130 --> 00:07:35.140 align:start position:0%
faster and this allows us to train
 

00:07:35.140 --> 00:07:37.469 align:start position:0%
faster and this allows us to train
bigger<00:07:35.350><c> neural</c><00:07:35.680><c> networks</c><00:07:36.040><c> or</c><00:07:36.400><c> to</c><00:07:36.820><c> do</c><00:07:37.000><c> so</c><00:07:37.240><c> the</c>

00:07:37.469 --> 00:07:37.479 align:start position:0%
bigger neural networks or to do so the
 

00:07:37.479 --> 00:07:39.510 align:start position:0%
bigger neural networks or to do so the
reason<00:07:37.780><c> or</c><00:07:37.840><c> multi-client</c><00:07:38.350><c> even</c><00:07:39.040><c> when</c><00:07:39.220><c> we</c><00:07:39.340><c> have</c>

00:07:39.510 --> 00:07:39.520 align:start position:0%
reason or multi-client even when we have
 

00:07:39.520 --> 00:07:42.240 align:start position:0%
reason or multi-client even when we have
a<00:07:39.550><c> large</c><00:07:40.240><c> network</c><00:07:40.540><c> roam</c><00:07:41.020><c> all</c><00:07:41.169><c> the</c><00:07:41.320><c> data</c><00:07:41.530><c> the</c>

00:07:42.240 --> 00:07:42.250 align:start position:0%
a large network roam all the data the
 

00:07:42.250 --> 00:07:45.800 align:start position:0%
a large network roam all the data the
other<00:07:42.280><c> reason</c><00:07:43.090><c> that</c><00:07:43.500><c> fast</c><00:07:44.500><c> computation</c><00:07:45.220><c> is</c>

00:07:45.800 --> 00:07:45.810 align:start position:0%
other reason that fast computation is
 

00:07:45.810 --> 00:07:48.600 align:start position:0%
other reason that fast computation is
important<00:07:46.810><c> is</c><00:07:47.110><c> that</c><00:07:47.140><c> it</c><00:07:47.979><c> turns</c><00:07:48.160><c> out</c><00:07:48.430><c> the</c>

00:07:48.600 --> 00:07:48.610 align:start position:0%
important is that it turns out the
 

00:07:48.610 --> 00:07:51.060 align:start position:0%
important is that it turns out the
process<00:07:49.120><c> of</c><00:07:49.270><c> training</c><00:07:49.539><c> your</c><00:07:49.840><c> network</c><00:07:50.320><c> this</c><00:07:50.620><c> is</c>

00:07:51.060 --> 00:07:51.070 align:start position:0%
process of training your network this is
 

00:07:51.070 --> 00:07:53.700 align:start position:0%
process of training your network this is
very<00:07:51.490><c> intuitive</c><00:07:52.050><c> often</c><00:07:53.050><c> you</c><00:07:53.140><c> have</c><00:07:53.260><c> an</c><00:07:53.350><c> idea</c>

00:07:53.700 --> 00:07:53.710 align:start position:0%
very intuitive often you have an idea
 

00:07:53.710 --> 00:07:56.340 align:start position:0%
very intuitive often you have an idea
for<00:07:53.950><c> a</c><00:07:54.010><c> neural</c><00:07:54.280><c> network</c><00:07:54.580><c> architecture</c><00:07:54.700><c> and</c><00:07:55.510><c> so</c>

00:07:56.340 --> 00:07:56.350 align:start position:0%
for a neural network architecture and so
 

00:07:56.350 --> 00:07:58.010 align:start position:0%
for a neural network architecture and so
you<00:07:56.410><c> implement</c><00:07:56.979><c> your</c><00:07:57.100><c> idea</c><00:07:57.370><c> and</c><00:07:57.490><c> code</c>

00:07:58.010 --> 00:07:58.020 align:start position:0%
you implement your idea and code
 

00:07:58.020 --> 00:08:01.050 align:start position:0%
you implement your idea and code
implementing<00:07:59.020><c> your</c><00:07:59.650><c> idea</c><00:08:00.039><c> then</c><00:08:00.340><c> lets</c><00:08:00.550><c> you</c><00:08:00.700><c> run</c>

00:08:01.050 --> 00:08:01.060 align:start position:0%
implementing your idea then lets you run
 

00:08:01.060 --> 00:08:02.820 align:start position:0%
implementing your idea then lets you run
an<00:08:01.240><c> experiment</c><00:08:01.600><c> which</c><00:08:02.050><c> tells</c><00:08:02.410><c> you</c><00:08:02.560><c> how</c><00:08:02.590><c> well</c>

00:08:02.820 --> 00:08:02.830 align:start position:0%
an experiment which tells you how well
 

00:08:02.830 --> 00:08:05.040 align:start position:0%
an experiment which tells you how well
your<00:08:03.190><c> neural</c><00:08:03.400><c> network</c><00:08:03.700><c> does</c><00:08:03.970><c> and</c><00:08:04.240><c> then</c><00:08:04.870><c> by</c>

00:08:05.040 --> 00:08:05.050 align:start position:0%
your neural network does and then by
 

00:08:05.050 --> 00:08:07.500 align:start position:0%
your neural network does and then by
looking<00:08:05.410><c> at</c><00:08:05.560><c> it</c><00:08:05.740><c> you</c><00:08:05.770><c> go</c><00:08:06.130><c> back</c><00:08:06.370><c> to</c><00:08:06.940><c> change</c><00:08:07.240><c> the</c>

00:08:07.500 --> 00:08:07.510 align:start position:0%
looking at it you go back to change the
 

00:08:07.510 --> 00:08:10.020 align:start position:0%
looking at it you go back to change the
details<00:08:07.750><c> of</c><00:08:08.169><c> your</c><00:08:08.289><c> new</c><00:08:08.380><c> network</c><00:08:08.650><c> and</c><00:08:09.070><c> then</c><00:08:09.760><c> you</c>

00:08:10.020 --> 00:08:10.030 align:start position:0%
details of your new network and then you
 

00:08:10.030 --> 00:08:12.920 align:start position:0%
details of your new network and then you
go<00:08:10.450><c> around</c><00:08:10.570><c> this</c><00:08:10.930><c> circle</c><00:08:11.620><c> over</c><00:08:12.100><c> and</c><00:08:12.220><c> over</c><00:08:12.490><c> and</c>

00:08:12.920 --> 00:08:12.930 align:start position:0%
go around this circle over and over and
 

00:08:12.930 --> 00:08:15.870 align:start position:0%
go around this circle over and over and
when<00:08:13.930><c> your</c><00:08:14.229><c> new</c><00:08:14.770><c> network</c><00:08:15.010><c> takes</c><00:08:15.520><c> a</c><00:08:15.640><c> long</c><00:08:15.850><c> time</c>

00:08:15.870 --> 00:08:15.880 align:start position:0%
when your new network takes a long time
 

00:08:15.880 --> 00:08:18.540 align:start position:0%
when your new network takes a long time
to<00:08:16.210><c> Train</c><00:08:16.630><c> it</c><00:08:17.110><c> just</c><00:08:17.320><c> takes</c><00:08:17.530><c> a</c><00:08:17.650><c> long</c><00:08:17.860><c> time</c><00:08:17.890><c> to</c><00:08:18.400><c> go</c>

00:08:18.540 --> 00:08:18.550 align:start position:0%
to Train it just takes a long time to go
 

00:08:18.550 --> 00:08:21.390 align:start position:0%
to Train it just takes a long time to go
around<00:08:18.669><c> this</c><00:08:19.030><c> cycle</c><00:08:19.930><c> and</c><00:08:20.169><c> there's</c><00:08:21.070><c> a</c><00:08:21.130><c> huge</c>

00:08:21.390 --> 00:08:21.400 align:start position:0%
around this cycle and there's a huge
 

00:08:21.400 --> 00:08:24.029 align:start position:0%
around this cycle and there's a huge
difference<00:08:21.880><c> in</c><00:08:22.180><c> your</c><00:08:22.240><c> productivity</c><00:08:23.039><c> building</c>

00:08:24.029 --> 00:08:24.039 align:start position:0%
difference in your productivity building
 

00:08:24.039 --> 00:08:26.730 align:start position:0%
difference in your productivity building
effective<00:08:24.310><c> neural</c><00:08:24.820><c> networks</c><00:08:25.270><c> when</c><00:08:26.080><c> you</c><00:08:26.470><c> can</c>

00:08:26.730 --> 00:08:26.740 align:start position:0%
effective neural networks when you can
 

00:08:26.740 --> 00:08:29.550 align:start position:0%
effective neural networks when you can
have<00:08:27.550><c> an</c><00:08:27.669><c> idea</c><00:08:27.820><c> and</c><00:08:28.240><c> try</c><00:08:28.539><c> it</c><00:08:28.600><c> and</c><00:08:28.900><c> see</c><00:08:28.990><c> the</c><00:08:29.289><c> work</c>

00:08:29.550 --> 00:08:29.560 align:start position:0%
have an idea and try it and see the work
 

00:08:29.560 --> 00:08:34.159 align:start position:0%
have an idea and try it and see the work
in<00:08:30.160><c> ten</c><00:08:31.120><c> minutes</c><00:08:31.330><c> or</c><00:08:32.159><c> maybe</c><00:08:33.159><c> ammos</c><00:08:33.580><c> a</c><00:08:33.610><c> day</c>

00:08:34.159 --> 00:08:34.169 align:start position:0%
in ten minutes or maybe ammos a day
 

00:08:34.169 --> 00:08:36.360 align:start position:0%
in ten minutes or maybe ammos a day
versus<00:08:35.169><c> if</c><00:08:35.380><c> you've</c><00:08:35.500><c> to</c><00:08:35.650><c> train</c><00:08:35.919><c> your</c><00:08:36.070><c> neural</c>

00:08:36.360 --> 00:08:36.370 align:start position:0%
versus if you've to train your neural
 

00:08:36.370 --> 00:08:39.480 align:start position:0%
versus if you've to train your neural
network<00:08:36.400><c> for</c><00:08:37.089><c> a</c><00:08:37.690><c> month</c><00:08:38.020><c> which</c><00:08:38.890><c> sometimes</c><00:08:39.280><c> does</c>

00:08:39.480 --> 00:08:39.490 align:start position:0%
network for a month which sometimes does
 

00:08:39.490 --> 00:08:40.580 align:start position:0%
network for a month which sometimes does
happened

00:08:40.580 --> 00:08:40.590 align:start position:0%
happened
 

00:08:40.590 --> 00:08:42.560 align:start position:0%
happened
because<00:08:41.039><c> you</c><00:08:41.190><c> get</c><00:08:41.370><c> a</c><00:08:41.430><c> result</c><00:08:41.610><c> back</c><00:08:42.029><c> you</c><00:08:42.450><c> know</c>

00:08:42.560 --> 00:08:42.570 align:start position:0%
because you get a result back you know
 

00:08:42.570 --> 00:08:44.660 align:start position:0%
because you get a result back you know
in<00:08:42.779><c> ten</c><00:08:43.020><c> minutes</c><00:08:43.289><c> or</c><00:08:43.529><c> maybe</c><00:08:43.650><c> in</c><00:08:43.860><c> a</c><00:08:43.950><c> day</c><00:08:44.159><c> you</c>

00:08:44.660 --> 00:08:44.670 align:start position:0%
in ten minutes or maybe in a day you
 

00:08:44.670 --> 00:08:47.240 align:start position:0%
in ten minutes or maybe in a day you
should<00:08:44.730><c> just</c><00:08:45.000><c> try</c><00:08:45.330><c> a</c><00:08:45.360><c> lot</c><00:08:45.630><c> more</c><00:08:45.810><c> ideas</c><00:08:46.200><c> and</c><00:08:46.560><c> be</c>

00:08:47.240 --> 00:08:47.250 align:start position:0%
should just try a lot more ideas and be
 

00:08:47.250 --> 00:08:49.160 align:start position:0%
should just try a lot more ideas and be
much<00:08:47.430><c> more</c><00:08:47.490><c> likely</c><00:08:47.880><c> to</c><00:08:48.090><c> discover</c><00:08:48.779><c> in</c><00:08:49.020><c> your</c>

00:08:49.160 --> 00:08:49.170 align:start position:0%
much more likely to discover in your
 

00:08:49.170 --> 00:08:50.600 align:start position:0%
much more likely to discover in your
network<00:08:49.529><c> and</c><00:08:49.620><c> it</c><00:08:49.770><c> works</c><00:08:49.950><c> well</c><00:08:50.250><c> for</c><00:08:50.310><c> your</c>

00:08:50.600 --> 00:08:50.610 align:start position:0%
network and it works well for your
 

00:08:50.610 --> 00:08:53.710 align:start position:0%
network and it works well for your
application<00:08:51.240><c> and</c><00:08:51.710><c> so</c><00:08:52.710><c> faster</c><00:08:53.310><c> computation</c>

00:08:53.710 --> 00:08:53.720 align:start position:0%
application and so faster computation
 

00:08:53.720 --> 00:08:57.890 align:start position:0%
application and so faster computation
has<00:08:54.720><c> really</c><00:08:55.200><c> helped</c><00:08:55.620><c> in</c><00:08:56.250><c> terms</c><00:08:56.700><c> of</c><00:08:56.970><c> speeding</c>

00:08:57.890 --> 00:08:57.900 align:start position:0%
has really helped in terms of speeding
 

00:08:57.900 --> 00:08:59.720 align:start position:0%
has really helped in terms of speeding
up<00:08:58.080><c> the</c><00:08:58.410><c> rate</c><00:08:58.680><c> at</c><00:08:58.920><c> which</c><00:08:58.950><c> you</c><00:08:59.220><c> can</c><00:08:59.370><c> get</c><00:08:59.580><c> an</c>

00:08:59.720 --> 00:08:59.730 align:start position:0%
up the rate at which you can get an
 

00:08:59.730 --> 00:09:02.600 align:start position:0%
up the rate at which you can get an
experimental<00:09:00.450><c> result</c><00:09:00.990><c> back</c><00:09:01.200><c> and</c><00:09:01.650><c> this</c><00:09:02.220><c> has</c>

00:09:02.600 --> 00:09:02.610 align:start position:0%
experimental result back and this has
 

00:09:02.610 --> 00:09:05.390 align:start position:0%
experimental result back and this has
really<00:09:03.210><c> helped</c><00:09:03.779><c> both</c><00:09:04.200><c> practitioners</c><00:09:05.100><c> of</c>

00:09:05.390 --> 00:09:05.400 align:start position:0%
really helped both practitioners of
 

00:09:05.400 --> 00:09:07.540 align:start position:0%
really helped both practitioners of
neuro<00:09:05.640><c> networks</c><00:09:05.670><c> as</c><00:09:06.390><c> well</c><00:09:06.450><c> as</c><00:09:06.779><c> researchers</c>

00:09:07.540 --> 00:09:07.550 align:start position:0%
neuro networks as well as researchers
 

00:09:07.550 --> 00:09:10.640 align:start position:0%
neuro networks as well as researchers
working<00:09:08.550><c> and</c><00:09:08.880><c> deep</c><00:09:09.029><c> learning</c><00:09:09.410><c> iterate</c><00:09:10.410><c> much</c>

00:09:10.640 --> 00:09:10.650 align:start position:0%
working and deep learning iterate much
 

00:09:10.650 --> 00:09:13.310 align:start position:0%
working and deep learning iterate much
faster<00:09:11.250><c> and</c><00:09:11.460><c> improve</c><00:09:12.060><c> your</c><00:09:12.660><c> ideas</c><00:09:13.020><c> much</c>

00:09:13.310 --> 00:09:13.320 align:start position:0%
faster and improve your ideas much
 

00:09:13.320 --> 00:09:16.579 align:start position:0%
faster and improve your ideas much
faster<00:09:13.800><c> and</c><00:09:14.070><c> so</c><00:09:14.330><c> all</c><00:09:15.330><c> this</c><00:09:15.570><c> has</c><00:09:15.870><c> also</c><00:09:16.110><c> been</c><00:09:16.320><c> a</c>

00:09:16.579 --> 00:09:16.589 align:start position:0%
faster and so all this has also been a
 

00:09:16.589 --> 00:09:18.560 align:start position:0%
faster and so all this has also been a
huge<00:09:16.680><c> boon</c><00:09:17.160><c> to</c><00:09:17.430><c> the</c><00:09:17.610><c> entire</c><00:09:17.970><c> deep</c><00:09:18.360><c> learning</c>

00:09:18.560 --> 00:09:18.570 align:start position:0%
huge boon to the entire deep learning
 

00:09:18.570 --> 00:09:21.019 align:start position:0%
huge boon to the entire deep learning
research<00:09:19.170><c> community</c><00:09:19.830><c> which</c><00:09:20.670><c> has</c><00:09:20.850><c> been</c>

00:09:21.019 --> 00:09:21.029 align:start position:0%
research community which has been
 

00:09:21.029 --> 00:09:23.360 align:start position:0%
research community which has been
incredible<00:09:21.690><c> with</c><00:09:21.870><c> just</c><00:09:22.140><c> you</c><00:09:22.650><c> know</c><00:09:22.740><c> inventing</c>

00:09:23.360 --> 00:09:23.370 align:start position:0%
incredible with just you know inventing
 

00:09:23.370 --> 00:09:25.610 align:start position:0%
incredible with just you know inventing
new<00:09:23.400><c> algorithms</c><00:09:24.150><c> and</c><00:09:24.450><c> making</c><00:09:24.839><c> nonstop</c>

00:09:25.610 --> 00:09:25.620 align:start position:0%
new algorithms and making nonstop
 

00:09:25.620 --> 00:09:28.910 align:start position:0%
new algorithms and making nonstop
progress<00:09:26.100><c> on</c><00:09:26.279><c> that</c><00:09:26.310><c> front</c><00:09:27.020><c> so</c><00:09:28.020><c> these</c><00:09:28.500><c> are</c><00:09:28.740><c> some</c>

00:09:28.910 --> 00:09:28.920 align:start position:0%
progress on that front so these are some
 

00:09:28.920 --> 00:09:30.980 align:start position:0%
progress on that front so these are some
of<00:09:28.950><c> the</c><00:09:29.040><c> forces</c><00:09:29.310><c> powering</c><00:09:30.060><c> the</c><00:09:30.210><c> rise</c><00:09:30.390><c> of</c><00:09:30.420><c> deep</c>

00:09:30.980 --> 00:09:30.990 align:start position:0%
of the forces powering the rise of deep
 

00:09:30.990 --> 00:09:33.560 align:start position:0%
of the forces powering the rise of deep
learning<00:09:31.200><c> but</c><00:09:31.980><c> the</c><00:09:32.070><c> good</c><00:09:32.520><c> news</c><00:09:32.550><c> is</c><00:09:33.029><c> that</c><00:09:33.270><c> these</c>

00:09:33.560 --> 00:09:33.570 align:start position:0%
learning but the good news is that these
 

00:09:33.570 --> 00:09:35.990 align:start position:0%
learning but the good news is that these
forces<00:09:33.900><c> are</c><00:09:34.350><c> still</c><00:09:34.740><c> working</c><00:09:34.980><c> powerfully</c><00:09:35.490><c> to</c>

00:09:35.990 --> 00:09:36.000 align:start position:0%
forces are still working powerfully to
 

00:09:36.000 --> 00:09:38.480 align:start position:0%
forces are still working powerfully to
make<00:09:36.180><c> deep</c><00:09:36.360><c> learning</c><00:09:36.540><c> even</c><00:09:37.170><c> better</c><00:09:37.410><c> Tech</c><00:09:38.279><c> Data</c>

00:09:38.480 --> 00:09:38.490 align:start position:0%
make deep learning even better Tech Data
 

00:09:38.490 --> 00:09:41.120 align:start position:0%
make deep learning even better Tech Data
society<00:09:39.450><c> is</c><00:09:39.630><c> still</c><00:09:39.960><c> throwing</c><00:09:40.380><c> up</c><00:09:40.589><c> one</c><00:09:40.890><c> more</c>

00:09:41.120 --> 00:09:41.130 align:start position:0%
society is still throwing up one more
 

00:09:41.130 --> 00:09:43.790 align:start position:0%
society is still throwing up one more
digital<00:09:41.580><c> data</c><00:09:41.790><c> or</c><00:09:42.089><c> take</c><00:09:42.810><c> computation</c><00:09:43.470><c> with</c>

00:09:43.790 --> 00:09:43.800 align:start position:0%
digital data or take computation with
 

00:09:43.800 --> 00:09:45.650 align:start position:0%
digital data or take computation with
the<00:09:43.920><c> rise</c><00:09:44.100><c> of</c><00:09:44.339><c> specialized</c><00:09:44.910><c> hardware</c><00:09:45.420><c> like</c>

00:09:45.650 --> 00:09:45.660 align:start position:0%
the rise of specialized hardware like
 

00:09:45.660 --> 00:09:48.290 align:start position:0%
the rise of specialized hardware like
GPUs<00:09:46.200><c> and</c><00:09:46.589><c> faster</c><00:09:46.980><c> networking</c><00:09:47.520><c> many</c><00:09:47.820><c> types</c><00:09:48.180><c> of</c>

00:09:48.290 --> 00:09:48.300 align:start position:0%
GPUs and faster networking many types of
 

00:09:48.300 --> 00:09:50.930 align:start position:0%
GPUs and faster networking many types of
hardware<00:09:49.020><c> I'm</c><00:09:49.920><c> actually</c><00:09:50.070><c> quite</c><00:09:50.310><c> confident</c>

00:09:50.930 --> 00:09:50.940 align:start position:0%
hardware I'm actually quite confident
 

00:09:50.940 --> 00:09:53.240 align:start position:0%
hardware I'm actually quite confident
that<00:09:51.089><c> our</c><00:09:51.270><c> ability</c><00:09:51.720><c> to</c><00:09:52.020><c> do</c><00:09:52.440><c> very</c><00:09:52.740><c> large</c><00:09:52.980><c> neural</c>

00:09:53.240 --> 00:09:53.250 align:start position:0%
that our ability to do very large neural
 

00:09:53.250 --> 00:09:55.130 align:start position:0%
that our ability to do very large neural
networks<00:09:53.700><c> or</c><00:09:53.940><c> should</c><00:09:54.180><c> a</c><00:09:54.210><c> computation</c><00:09:54.930><c> point</c>

00:09:55.130 --> 00:09:55.140 align:start position:0%
networks or should a computation point
 

00:09:55.140 --> 00:09:57.310 align:start position:0%
networks or should a computation point
of<00:09:55.260><c> view</c><00:09:55.410><c> will</c><00:09:55.830><c> keep</c><00:09:56.040><c> on</c><00:09:56.220><c> getting</c><00:09:56.400><c> better</c><00:09:56.670><c> and</c>

00:09:57.310 --> 00:09:57.320 align:start position:0%
of view will keep on getting better and
 

00:09:57.320 --> 00:10:00.350 align:start position:0%
of view will keep on getting better and
take<00:09:58.320><c> algorithms</c><00:09:59.000><c> relative</c><00:10:00.000><c> learning</c>

00:10:00.350 --> 00:10:00.360 align:start position:0%
take algorithms relative learning
 

00:10:00.360 --> 00:10:02.870 align:start position:0%
take algorithms relative learning
research<00:10:00.690><c> communities</c><00:10:01.200><c> though</c><00:10:01.880><c> continuously</c>

00:10:02.870 --> 00:10:02.880 align:start position:0%
research communities though continuously
 

00:10:02.880 --> 00:10:05.060 align:start position:0%
research communities though continuously
phenomenal<00:10:03.720><c> at</c><00:10:03.930><c> innovating</c><00:10:04.740><c> on</c><00:10:04.950><c> the</c>

00:10:05.060 --> 00:10:05.070 align:start position:0%
phenomenal at innovating on the
 

00:10:05.070 --> 00:10:07.670 align:start position:0%
phenomenal at innovating on the
algorithms<00:10:05.610><c> front</c><00:10:05.970><c> so</c><00:10:06.900><c> because</c><00:10:07.140><c> of</c><00:10:07.260><c> this</c><00:10:07.410><c> I</c>

00:10:07.670 --> 00:10:07.680 align:start position:0%
algorithms front so because of this I
 

00:10:07.680 --> 00:10:09.829 align:start position:0%
algorithms front so because of this I
think<00:10:07.710><c> that</c><00:10:07.980><c> we</c><00:10:08.460><c> can</c><00:10:08.610><c> be</c><00:10:08.700><c> optimistic</c><00:10:09.089><c> answer</c>

00:10:09.829 --> 00:10:09.839 align:start position:0%
think that we can be optimistic answer
 

00:10:09.839 --> 00:10:11.360 align:start position:0%
think that we can be optimistic answer
the<00:10:09.990><c> optimistic</c><00:10:10.380><c> the</c><00:10:10.740><c> deep</c><00:10:10.920><c> learning</c><00:10:11.100><c> will</c>

00:10:11.360 --> 00:10:11.370 align:start position:0%
the optimistic the deep learning will
 

00:10:11.370 --> 00:10:13.640 align:start position:0%
the optimistic the deep learning will
keep<00:10:11.610><c> on</c><00:10:11.790><c> getting</c><00:10:11.970><c> better</c><00:10:12.360><c> for</c><00:10:12.990><c> many</c><00:10:13.200><c> years</c><00:10:13.470><c> to</c>

00:10:13.640 --> 00:10:13.650 align:start position:0%
keep on getting better for many years to
 

00:10:13.650 --> 00:10:14.110 align:start position:0%
keep on getting better for many years to
come

00:10:14.110 --> 00:10:14.120 align:start position:0%
come
 

00:10:14.120 --> 00:10:17.090 align:start position:0%
come
so<00:10:15.120><c> that</c><00:10:15.450><c> let's</c><00:10:15.930><c> go</c><00:10:16.110><c> on</c><00:10:16.200><c> to</c><00:10:16.230><c> the</c><00:10:16.380><c> last</c><00:10:16.589><c> video</c><00:10:16.830><c> of</c>

00:10:17.090 --> 00:10:17.100 align:start position:0%
so that let's go on to the last video of
 

00:10:17.100 --> 00:10:18.530 align:start position:0%
so that let's go on to the last video of
the<00:10:17.220><c> section</c><00:10:17.640><c> where</c><00:10:17.970><c> we'll</c><00:10:18.150><c> talk</c><00:10:18.360><c> a</c><00:10:18.390><c> little</c>

00:10:18.530 --> 00:10:18.540 align:start position:0%
the section where we'll talk a little
 

00:10:18.540 --> 00:10:20.270 align:start position:0%
the section where we'll talk a little
bit<00:10:18.720><c> more</c><00:10:18.900><c> about</c><00:10:19.140><c> what</c><00:10:19.440><c> you</c><00:10:19.589><c> learn</c><00:10:19.860><c> from</c><00:10:20.100><c> this</c>

00:10:20.270 --> 00:10:20.280 align:start position:0%
bit more about what you learn from this
 

00:10:20.280 --> 00:10:22.610 align:start position:0%
bit more about what you learn from this
course

