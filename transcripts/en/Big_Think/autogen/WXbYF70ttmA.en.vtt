WEBVTT
Kind: captions
Language: en

00:00:03.330 --> 00:00:08.770
some people are gravely worried about

00:00:08.770 --> 00:00:08.780
some people are gravely worried about
 

00:00:08.780 --> 00:00:12.470
some people are gravely worried about
the uncertainty and the negative

00:00:12.470 --> 00:00:12.480
the uncertainty and the negative
 

00:00:12.480 --> 00:00:17.630
the uncertainty and the negative
potential associated with transhumans

00:00:17.630 --> 00:00:17.640
potential associated with transhumans
 

00:00:17.640 --> 00:00:22.220
potential associated with transhumans
superhuman AGI and indeed we are

00:00:22.220 --> 00:00:22.230
superhuman AGI and indeed we are
 

00:00:22.230 --> 00:00:26.479
superhuman AGI and indeed we are
stepping into a great unknown realm it's

00:00:26.479 --> 00:00:26.489
stepping into a great unknown realm it's
 

00:00:26.489 --> 00:00:28.729
stepping into a great unknown realm it's
almost like a rock the hot type of thing

00:00:28.729 --> 00:00:28.739
almost like a rock the hot type of thing
 

00:00:28.739 --> 00:00:31.130
almost like a rock the hot type of thing
really having we fundamentally don't

00:00:31.130 --> 00:00:31.140
really having we fundamentally don't
 

00:00:31.140 --> 00:00:33.560
really having we fundamentally don't
know what a superhuman AI is gonna do

00:00:33.560 --> 00:00:33.570
know what a superhuman AI is gonna do
 

00:00:33.570 --> 00:00:36.020
know what a superhuman AI is gonna do
and that's the truth of it right and

00:00:36.020 --> 00:00:36.030
and that's the truth of it right and
 

00:00:36.030 --> 00:00:39.740
and that's the truth of it right and
then if you tend to be an optimist you

00:00:39.740 --> 00:00:39.750
then if you tend to be an optimist you
 

00:00:39.750 --> 00:00:42.799
then if you tend to be an optimist you
will focus on the good possibilities if

00:00:42.799 --> 00:00:42.809
will focus on the good possibilities if
 

00:00:42.809 --> 00:00:45.229
will focus on the good possibilities if
you tend to be a worried person who's

00:00:45.229 --> 00:00:45.239
you tend to be a worried person who's
 

00:00:45.239 --> 00:00:48.169
you tend to be a worried person who's
pessimistic you'll focus on on the bad

00:00:48.169 --> 00:00:48.179
pessimistic you'll focus on on the bad
 

00:00:48.179 --> 00:00:49.729
pessimistic you'll focus on on the bad
possibilities if you tend to be a

00:00:49.729 --> 00:00:49.739
possibilities if you tend to be a
 

00:00:49.739 --> 00:00:53.450
possibilities if you tend to be a
Hollywood movie maker you focus on scary

00:00:53.450 --> 00:00:53.460
Hollywood movie maker you focus on scary
 

00:00:53.460 --> 00:00:56.239
Hollywood movie maker you focus on scary
possibilities maybe with a happy ending

00:00:56.239 --> 00:00:56.249
possibilities maybe with a happy ending
 

00:00:56.249 --> 00:00:58.220
possibilities maybe with a happy ending
because because that's what sells movies

00:00:58.220 --> 00:00:58.230
because because that's what sells movies
 

00:00:58.230 --> 00:00:59.869
because because that's what sells movies
we don't we don't know what's gonna

00:00:59.869 --> 00:00:59.879
we don't we don't know what's gonna
 

00:00:59.879 --> 00:01:02.389
we don't we don't know what's gonna
happen I do think however this is the

00:01:02.389 --> 00:01:02.399
happen I do think however this is the
 

00:01:02.399 --> 00:01:06.410
happen I do think however this is the
situation humanity has been in for a

00:01:06.410 --> 00:01:06.420
situation humanity has been in for a
 

00:01:06.420 --> 00:01:09.290
situation humanity has been in for a
very long time when the cavemen stepped

00:01:09.290 --> 00:01:09.300
very long time when the cavemen stepped
 

00:01:09.300 --> 00:01:11.840
very long time when the cavemen stepped
out of their caves and began agriculture

00:01:11.840 --> 00:01:11.850
out of their caves and began agriculture
 

00:01:11.850 --> 00:01:14.480
out of their caves and began agriculture
we really had no idea that was gonna

00:01:14.480 --> 00:01:14.490
we really had no idea that was gonna
 

00:01:14.490 --> 00:01:17.750
we really had no idea that was gonna
lead to cities in space flight and so

00:01:17.750 --> 00:01:17.760
lead to cities in space flight and so
 

00:01:17.760 --> 00:01:21.460
lead to cities in space flight and so
forth and when the first early humans

00:01:21.460 --> 00:01:21.470
forth and when the first early humans
 

00:01:21.470 --> 00:01:24.440
forth and when the first early humans
created language to carry out simple

00:01:24.440 --> 00:01:24.450
created language to carry out simple
 

00:01:24.450 --> 00:01:26.960
created language to carry out simple
communication about the moose they had

00:01:26.960 --> 00:01:26.970
communication about the moose they had
 

00:01:26.970 --> 00:01:29.990
communication about the moose they had
just killed over there they did not

00:01:29.990 --> 00:01:30.000
just killed over there they did not
 

00:01:30.000 --> 00:01:32.840
just killed over there they did not
envision Facebook differential calculus

00:01:32.840 --> 00:01:32.850
envision Facebook differential calculus
 

00:01:32.850 --> 00:01:37.970
envision Facebook differential calculus
and MC Hammer I don't know all the rest

00:01:37.970 --> 00:01:37.980
and MC Hammer I don't know all the rest
 

00:01:37.980 --> 00:01:41.030
and MC Hammer I don't know all the rest
right I mean there's so much that is has

00:01:41.030 --> 00:01:41.040
right I mean there's so much that is has
 

00:01:41.040 --> 00:01:44.030
right I mean there's so much that is has
come about out of early inventions which

00:01:44.030 --> 00:01:44.040
come about out of early inventions which
 

00:01:44.040 --> 00:01:46.070
come about out of early inventions which
humans couldn't have ever foreseen and I

00:01:46.070 --> 00:01:46.080
humans couldn't have ever foreseen and I
 

00:01:46.080 --> 00:01:48.440
humans couldn't have ever foreseen and I
think we're just in the same situation I

00:01:48.440 --> 00:01:48.450
think we're just in the same situation I
 

00:01:48.450 --> 00:01:50.420
think we're just in the same situation I
mean the invention of language or

00:01:50.420 --> 00:01:50.430
mean the invention of language or
 

00:01:50.430 --> 00:01:52.790
mean the invention of language or
civilization could have led to

00:01:52.790 --> 00:01:52.800
civilization could have led to
 

00:01:52.800 --> 00:01:55.640
civilization could have led to
everyone's death right hence and the way

00:01:55.640 --> 00:01:55.650
everyone's death right hence and the way
 

00:01:55.650 --> 00:01:58.240
everyone's death right hence and the way
it's still could and the creation of

00:01:58.240 --> 00:01:58.250
it's still could and the creation of
 

00:01:58.250 --> 00:02:02.690
it's still could and the creation of
superhuman AI it could kill everyone and

00:02:02.690 --> 00:02:02.700
superhuman AI it could kill everyone and
 

00:02:02.700 --> 00:02:04.520
superhuman AI it could kill everyone and
I don't I don't want it to over almost

00:02:04.520 --> 00:02:04.530
I don't I don't want it to over almost
 

00:02:04.530 --> 00:02:07.940
I don't I don't want it to over almost
almost none of us do of course the way

00:02:07.940 --> 00:02:07.950
almost none of us do of course the way
 

00:02:07.950 --> 00:02:11.750
almost none of us do of course the way
we got to this point as a species and a

00:02:11.750 --> 00:02:11.760
we got to this point as a species and a
 

00:02:11.760 --> 00:02:15.110
we got to this point as a species and a
culture has been to keep doing amazing

00:02:15.110 --> 00:02:15.120
culture has been to keep doing amazing
 

00:02:15.120 --> 00:02:16.250
culture has been to keep doing amazing
new things that

00:02:16.250 --> 00:02:16.260
new things that
 

00:02:16.260 --> 00:02:19.670
new things that
didn't fully understand and that's what

00:02:19.670 --> 00:02:19.680
didn't fully understand and that's what
 

00:02:19.680 --> 00:02:23.240
didn't fully understand and that's what
we're gonna keep on doing Nick Bostrom's

00:02:23.240 --> 00:02:23.250
we're gonna keep on doing Nick Bostrom's
 

00:02:23.250 --> 00:02:28.130
we're gonna keep on doing Nick Bostrom's
book was influential but I felt that in

00:02:28.130 --> 00:02:28.140
book was influential but I felt that in
 

00:02:28.140 --> 00:02:31.640
book was influential but I felt that in
some ways it was a bit deceptive the way

00:02:31.640 --> 00:02:31.650
some ways it was a bit deceptive the way
 

00:02:31.650 --> 00:02:35.390
some ways it was a bit deceptive the way
he phrased things if he read his precise

00:02:35.390 --> 00:02:35.400
he phrased things if he read his precise
 

00:02:35.400 --> 00:02:39.130
he phrased things if he read his precise
philosophical arguments which are very

00:02:39.130 --> 00:02:39.140
philosophical arguments which are very
 

00:02:39.140 --> 00:02:44.089
philosophical arguments which are very
logically drawn what both Sturm says in

00:02:44.089 --> 00:02:44.099
logically drawn what both Sturm says in
 

00:02:44.099 --> 00:02:49.309
logically drawn what both Sturm says in
his book super intelligence is that we

00:02:49.309 --> 00:02:49.319
his book super intelligence is that we
 

00:02:49.319 --> 00:02:51.530
his book super intelligence is that we
cannot rule out the possibility that the

00:02:51.530 --> 00:02:51.540
cannot rule out the possibility that the
 

00:02:51.540 --> 00:02:53.930
cannot rule out the possibility that the
super intelligence will do some very bad

00:02:53.930 --> 00:02:53.940
super intelligence will do some very bad
 

00:02:53.940 --> 00:02:57.220
super intelligence will do some very bad
things and that's true on the other hand

00:02:57.220 --> 00:02:57.230
things and that's true on the other hand
 

00:02:57.230 --> 00:03:00.349
things and that's true on the other hand
some of the associated rhetoric makes it

00:03:00.349 --> 00:03:00.359
some of the associated rhetoric makes it
 

00:03:00.359 --> 00:03:03.229
some of the associated rhetoric makes it
sound like it's very likely his super

00:03:03.229 --> 00:03:03.239
sound like it's very likely his super
 

00:03:03.239 --> 00:03:04.849
sound like it's very likely his super
intelligence will do these bad things

00:03:04.849 --> 00:03:04.859
intelligence will do these bad things
 

00:03:04.859 --> 00:03:07.670
intelligence will do these bad things
and if you follow his philosophical

00:03:07.670 --> 00:03:07.680
and if you follow his philosophical
 

00:03:07.680 --> 00:03:10.309
and if you follow his philosophical
arguments closely he doesn't show that

00:03:10.309 --> 00:03:10.319
arguments closely he doesn't show that
 

00:03:10.319 --> 00:03:13.220
arguments closely he doesn't show that
what he just shows is that that you

00:03:13.220 --> 00:03:13.230
what he just shows is that that you
 

00:03:13.230 --> 00:03:15.050
what he just shows is that that you
can't rule it out and we don't know

00:03:15.050 --> 00:03:15.060
can't rule it out and we don't know
 

00:03:15.060 --> 00:03:17.629
can't rule it out and we don't know
what's going on I don't think Nick

00:03:17.629 --> 00:03:17.639
what's going on I don't think Nick
 

00:03:17.639 --> 00:03:20.000
what's going on I don't think Nick
Bostrom or anyone else is gonna stop the

00:03:20.000 --> 00:03:20.010
Bostrom or anyone else is gonna stop the
 

00:03:20.010 --> 00:03:23.680
Bostrom or anyone else is gonna stop the
human race from developing advanced AI

00:03:23.680 --> 00:03:23.690
human race from developing advanced AI
 

00:03:23.690 --> 00:03:26.780
human race from developing advanced AI
because it's a source of tremendous

00:03:26.780 --> 00:03:26.790
because it's a source of tremendous
 

00:03:26.790 --> 00:03:29.390
because it's a source of tremendous
intellectual curiosity but also of

00:03:29.390 --> 00:03:29.400
intellectual curiosity but also of
 

00:03:29.400 --> 00:03:31.539
intellectual curiosity but also of
tremendous economic advantage so if

00:03:31.539 --> 00:03:31.549
tremendous economic advantage so if
 

00:03:31.549 --> 00:03:35.170
tremendous economic advantage so if
let's say President Trump decided to ban

00:03:35.170 --> 00:03:35.180
let's say President Trump decided to ban
 

00:03:35.180 --> 00:03:38.240
let's say President Trump decided to ban
artificial intelligence research I don't

00:03:38.240 --> 00:03:38.250
artificial intelligence research I don't
 

00:03:38.250 --> 00:03:41.110
artificial intelligence research I don't
think he's going to but suppose he did

00:03:41.110 --> 00:03:41.120
think he's going to but suppose he did
 

00:03:41.120 --> 00:03:43.699
think he's going to but suppose he did
China will keep doing artificial

00:03:43.699 --> 00:03:43.709
China will keep doing artificial
 

00:03:43.709 --> 00:03:45.949
China will keep doing artificial
intelligence research if US and China

00:03:45.949 --> 00:03:45.959
intelligence research if US and China
 

00:03:45.959 --> 00:03:48.409
intelligence research if US and China
Bennet you know Africa will do it

00:03:48.409 --> 00:03:48.419
Bennet you know Africa will do it
 

00:03:48.419 --> 00:03:50.059
Bennet you know Africa will do it
everywhere around the world has AI

00:03:50.059 --> 00:03:50.069
everywhere around the world has AI
 

00:03:50.069 --> 00:03:54.589
everywhere around the world has AI
textbooks and computers and everyone now

00:03:54.589 --> 00:03:54.599
textbooks and computers and everyone now
 

00:03:54.599 --> 00:03:56.900
textbooks and computers and everyone now
knows you can make people's lives better

00:03:56.900 --> 00:03:56.910
knows you can make people's lives better
 

00:03:56.910 --> 00:03:59.930
knows you can make people's lives better
and make money from developing more

00:03:59.930 --> 00:03:59.940
and make money from developing more
 

00:03:59.940 --> 00:04:03.129
and make money from developing more
advanced AI so there's no possibility in

00:04:03.129 --> 00:04:03.139
advanced AI so there's no possibility in
 

00:04:03.139 --> 00:04:07.670
advanced AI so there's no possibility in
practice to halt in our development what

00:04:07.670 --> 00:04:07.680
practice to halt in our development what
 

00:04:07.680 --> 00:04:11.180
practice to halt in our development what
we can do is try to direct it in the

00:04:11.180 --> 00:04:11.190
we can do is try to direct it in the
 

00:04:11.190 --> 00:04:15.680
we can do is try to direct it in the
most beneficial direction according to

00:04:15.680 --> 00:04:15.690
most beneficial direction according to
 

00:04:15.690 --> 00:04:18.080
most beneficial direction according to
our our best judgment and that that's

00:04:18.080 --> 00:04:18.090
our our best judgment and that that's
 

00:04:18.090 --> 00:04:22.310
our our best judgment and that that's
part of what leads me to pursue AGI via

00:04:22.310 --> 00:04:22.320
part of what leads me to pursue AGI via
 

00:04:22.320 --> 00:04:26.080
part of what leads me to pursue AGI via
an open-source project such as OpenCog I

00:04:26.080 --> 00:04:26.090
an open-source project such as OpenCog I
 

00:04:26.090 --> 00:04:30.119
an open-source project such as OpenCog I
respect very much what Google Baidu

00:04:30.119 --> 00:04:30.129
respect very much what Google Baidu
 

00:04:30.129 --> 00:04:33.959
respect very much what Google Baidu
Facebook Microsoft and these other big

00:04:33.959 --> 00:04:33.969
Facebook Microsoft and these other big
 

00:04:33.969 --> 00:04:35.999
Facebook Microsoft and these other big
companies are doing in AI there's many

00:04:35.999 --> 00:04:36.009
companies are doing in AI there's many
 

00:04:36.009 --> 00:04:38.339
companies are doing in AI there's many
good people they're doing good research

00:04:38.339 --> 00:04:38.349
good people they're doing good research
 

00:04:38.349 --> 00:04:41.719
good people they're doing good research
and with good hearted motivations but I

00:04:41.719 --> 00:04:41.729
and with good hearted motivations but I
 

00:04:41.729 --> 00:04:45.809
and with good hearted motivations but I
guess I'm enough of an old leftist

00:04:45.809 --> 00:04:45.819
guess I'm enough of an old leftist
 

00:04:45.819 --> 00:04:50.249
guess I'm enough of an old leftist
raised by socialists and I sort of I'm

00:04:50.249 --> 00:04:50.259
raised by socialists and I sort of I'm
 

00:04:50.259 --> 00:04:52.290
raised by socialists and I sort of I'm
skeptical that the company whose main

00:04:52.290 --> 00:04:52.300
skeptical that the company whose main
 

00:04:52.300 --> 00:04:54.839
skeptical that the company whose main
motive is to maximize shareholder value

00:04:54.839 --> 00:04:54.849
motive is to maximize shareholder value
 

00:04:54.849 --> 00:04:58.049
motive is to maximize shareholder value
is really going to do the best thing for

00:04:58.049 --> 00:04:58.059
is really going to do the best thing for
 

00:04:58.059 --> 00:05:01.229
is really going to do the best thing for
the human race if they create a human

00:05:01.229 --> 00:05:01.239
the human race if they create a human
 

00:05:01.239 --> 00:05:04.529
the human race if they create a human
level and I I mean they might on the

00:05:04.529 --> 00:05:04.539
level and I I mean they might on the
 

00:05:04.539 --> 00:05:05.959
level and I I mean they might on the
other hand there's a lot of other

00:05:05.959 --> 00:05:05.969
other hand there's a lot of other
 

00:05:05.969 --> 00:05:08.489
other hand there's a lot of other
motivations there the public company in

00:05:08.489 --> 00:05:08.499
motivations there the public company in
 

00:05:08.499 --> 00:05:11.489
motivations there the public company in
the end has a fiduciary responsibility

00:05:11.489 --> 00:05:11.499
the end has a fiduciary responsibility
 

00:05:11.499 --> 00:05:14.519
the end has a fiduciary responsibility
to their their shareholders all in all I

00:05:14.519 --> 00:05:14.529
to their their shareholders all in all I
 

00:05:14.529 --> 00:05:18.059
to their their shareholders all in all I
think the odds are better if AI is

00:05:18.059 --> 00:05:18.069
think the odds are better if AI is
 

00:05:18.069 --> 00:05:21.480
think the odds are better if AI is
developed in a way that is owned by the

00:05:21.480 --> 00:05:21.490
developed in a way that is owned by the
 

00:05:21.490 --> 00:05:24.329
developed in a way that is owned by the
whole human race and can be developed by

00:05:24.329 --> 00:05:24.339
whole human race and can be developed by
 

00:05:24.339 --> 00:05:28.129
whole human race and can be developed by
all of humanity for for its own good and

00:05:28.129 --> 00:05:28.139
all of humanity for for its own good and
 

00:05:28.139 --> 00:05:31.049
all of humanity for for its own good and
open source software is sort of the

00:05:31.049 --> 00:05:31.059
open source software is sort of the
 

00:05:31.059 --> 00:05:33.779
open source software is sort of the
closest approximation that we have to

00:05:33.779 --> 00:05:33.789
closest approximation that we have to
 

00:05:33.789 --> 00:05:36.269
closest approximation that we have to
that now so our our aspiration is to

00:05:36.269 --> 00:05:36.279
that now so our our aspiration is to
 

00:05:36.279 --> 00:05:39.719
that now so our our aspiration is to
grow OpenCog into sort of the the Linux

00:05:39.719 --> 00:05:39.729
grow OpenCog into sort of the the Linux
 

00:05:39.729 --> 00:05:42.569
grow OpenCog into sort of the the Linux
of AGI and have people all around the

00:05:42.569 --> 00:05:42.579
of AGI and have people all around the
 

00:05:42.579 --> 00:05:45.449
of AGI and have people all around the
world developing it to serve their own

00:05:45.449 --> 00:05:45.459
world developing it to serve their own
 

00:05:45.459 --> 00:05:49.529
world developing it to serve their own
local needs and putting their own values

00:05:49.529 --> 00:05:49.539
local needs and putting their own values
 

00:05:49.539 --> 00:05:51.839
local needs and putting their own values
and understanding into it as it becomes

00:05:51.839 --> 00:05:51.849
and understanding into it as it becomes
 

00:05:51.849 --> 00:05:54.779
and understanding into it as it becomes
more and more intelligent certainly this

00:05:54.779 --> 00:05:54.789
more and more intelligent certainly this
 

00:05:54.789 --> 00:05:58.379
more and more intelligent certainly this
doesn't give us any guarantee we can

00:05:58.379 --> 00:05:58.389
doesn't give us any guarantee we can
 

00:05:58.389 --> 00:06:00.689
doesn't give us any guarantee we can
observe things like you know Linux has

00:06:00.689 --> 00:06:00.699
observe things like you know Linux has
 

00:06:00.699 --> 00:06:04.649
observe things like you know Linux has
fewer bugs then Windows or OSX and it's

00:06:04.649 --> 00:06:04.659
fewer bugs then Windows or OSX and it's
 

00:06:04.659 --> 00:06:07.139
fewer bugs then Windows or OSX and it's
open source so more more eyeballs on

00:06:07.139 --> 00:06:07.149
open source so more more eyeballs on
 

00:06:07.149 --> 00:06:08.939
open source so more more eyeballs on
something sometimes can make it more

00:06:08.939 --> 00:06:08.949
something sometimes can make it more
 

00:06:08.949 --> 00:06:11.909
something sometimes can make it more
reliable but there's no solid guarantee

00:06:11.909 --> 00:06:11.919
reliable but there's no solid guarantee
 

00:06:11.919 --> 00:06:16.290
reliable but there's no solid guarantee
that making an AGI open source will make

00:06:16.290 --> 00:06:16.300
that making an AGI open source will make
 

00:06:16.300 --> 00:06:20.040
that making an AGI open source will make
the singularity come out well but my gut

00:06:20.040 --> 00:06:20.050
the singularity come out well but my gut
 

00:06:20.050 --> 00:06:22.139
the singularity come out well but my gut
feel is that there's enough hard

00:06:22.139 --> 00:06:22.149
feel is that there's enough hard
 

00:06:22.149 --> 00:06:25.709
feel is that there's enough hard
problems with creating a superhuman AI

00:06:25.709 --> 00:06:25.719
problems with creating a superhuman AI
 

00:06:25.719 --> 00:06:29.129
problems with creating a superhuman AI
and having it respect human values and

00:06:29.129 --> 00:06:29.139
and having it respect human values and
 

00:06:29.139 --> 00:06:31.259
and having it respect human values and
have a relationship of empathy with

00:06:31.259 --> 00:06:31.269
have a relationship of empathy with
 

00:06:31.269 --> 00:06:33.299
have a relationship of empathy with
people as it grows there's enough

00:06:33.299 --> 00:06:33.309
people as it grows there's enough
 

00:06:33.309 --> 00:06:36.359
people as it grows there's enough
problems there without the young AGI

00:06:36.359 --> 00:06:36.369
problems there without the young AGI
 

00:06:36.369 --> 00:06:38.249
problems there without the young AGI
getting wrapped up in competition of

00:06:38.249 --> 00:06:38.259
getting wrapped up in competition of
 

00:06:38.259 --> 00:06:41.219
getting wrapped up in competition of
country versus country and and company

00:06:41.219 --> 00:06:41.229
country versus country and and company
 

00:06:41.229 --> 00:06:43.679
country versus country and and company
versus company and internal politics

00:06:43.679 --> 00:06:43.689
versus company and internal politics
 

00:06:43.689 --> 00:06:46.499
versus company and internal politics
within companies or militaries I feel

00:06:46.499 --> 00:06:46.509
within companies or militaries I feel
 

00:06:46.509 --> 00:06:49.889
within companies or militaries I feel
like we don't want to add these problems

00:06:49.889 --> 00:06:49.899
like we don't want to add these problems
 

00:06:49.899 --> 00:06:54.299
like we don't want to add these problems
of sort of human / primate social status

00:06:54.299 --> 00:06:54.309
of sort of human / primate social status
 

00:06:54.309 --> 00:06:56.819
of sort of human / primate social status
competition dynamics we don't want to

00:06:56.819 --> 00:06:56.829
competition dynamics we don't want to
 

00:06:56.829 --> 00:06:58.799
competition dynamics we don't want to
add those problems into the challenges

00:06:58.799 --> 00:06:58.809
add those problems into the challenges
 

00:06:58.809 --> 00:07:01.529
add those problems into the challenges
that that are faced in in Ag our

00:07:01.529 --> 00:07:01.539
that that are faced in in Ag our
 

00:07:01.539 --> 00:07:06.710
that that are faced in in Ag our
development

00:07:06.710 --> 00:07:06.720
 

00:07:06.720 --> 00:07:10.109
[Music]

