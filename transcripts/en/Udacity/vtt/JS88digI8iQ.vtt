WEBVTT
Kind: captions
Language: en

00:00:00.460 --> 00:00:04.510
So now let's consider how different lock implementations

00:00:04.510 --> 00:00:07.660
interact with the coherence protocol and what performance

00:00:07.660 --> 00:00:13.808
do they achieve. Let's look at three cores, 0, 1, and 2. Let's first try to do

00:00:13.808 --> 00:00:17.260
this with atomic exchange instructions. Core 0 let's

00:00:17.260 --> 00:00:19.580
say, tries to grab the lock first. And

00:00:19.580 --> 00:00:22.500
it succeeds. It does an atomic exchange, gets

00:00:22.500 --> 00:00:25.590
that the lock was free. The lock var is

00:00:25.590 --> 00:00:31.190
now going to be in the modified state in the cache of core 0. And it's going to

00:00:31.190 --> 00:00:34.570
not be present in core 1 and core

00:00:34.570 --> 00:00:39.370
2's caches. Now eventually core 0, which grabbed the

00:00:39.370 --> 00:00:45.310
lock, will unlock by writing 0 to the lock var. But the question is what happens

00:00:45.310 --> 00:00:50.882
meanwhile on core 1 and core 2? Both core 1 and core 2 might want to also enter

00:00:50.882 --> 00:00:57.062
the same critical section. Let's say core 1 tries first. He does an exchange of

00:00:57.062 --> 00:01:03.166
R1 and lock var, see that lock var is one. So, it didn't grab the lock, but

00:01:03.166 --> 00:01:08.626
because there was a write to lock var, there is a transfer of lock var into this

00:01:08.626 --> 00:01:16.360
cache, where it's now modified. And now lock var is invalid in core 0's cache.

00:01:16.360 --> 00:01:18.880
Let's say that core 0 now does it's own

00:01:18.880 --> 00:01:22.540
exchange. Does the same thing. Tries to grab the lock,

00:01:22.540 --> 00:01:25.804
sees that it's one, will try again. But because it

00:01:25.804 --> 00:01:29.084
did the write to lockvar, now it grabs the block

00:01:29.084 --> 00:01:31.954
in the modified state and core 1 gets it in

00:01:31.954 --> 00:01:36.100
invalid state and core 2 stays invalid. And now, as

00:01:36.100 --> 00:01:38.390
long as the lock is busy, these two will be

00:01:38.390 --> 00:01:41.880
spinning in that loop. So what's going to happen is

00:01:41.880 --> 00:01:44.810
this block will move many many times between

00:01:44.810 --> 00:01:48.380
their caches, each time causing communication on the shared

00:01:48.380 --> 00:01:51.500
bus. And each time spending a lot of power.

00:01:51.500 --> 00:01:54.820
None of this activity here will actually succeed in

00:01:54.820 --> 00:01:58.470
grabbing the lock until the lockvar becomes 0.

00:01:58.470 --> 00:02:02.470
So at some point, let's say that core 1

00:02:02.470 --> 00:02:04.830
was the last one to grab the lock at

00:02:04.830 --> 00:02:07.370
the point where lock lockvar is written to 0.

00:02:07.370 --> 00:02:10.828
This write will cause yet another movement of

00:02:10.828 --> 00:02:14.741
the block, it becomes modified in core 0's cache

00:02:14.741 --> 00:02:17.994
now. And now, whoever tries to grab the lock

00:02:17.994 --> 00:02:21.122
first will succeed by obtaining the block, writing to

00:02:21.122 --> 00:02:24.008
it, but this time grabbing the lock. So,

00:02:24.008 --> 00:02:26.302
if core one now succeeds in grabbing the lock

00:02:26.302 --> 00:02:29.526
after it became available, core two continues this activity

00:02:29.526 --> 00:02:32.600
further. So, there is a lot of this going

00:02:32.600 --> 00:02:34.610
on as long as anybody's waiting on a

00:02:34.610 --> 00:02:38.161
lock. This is very power hungry because these

00:02:38.161 --> 00:02:41.163
movements of data between caches are going to

00:02:41.163 --> 00:02:44.160
cost us a lot of energy per transfer.

00:02:44.160 --> 00:02:48.920
And also, this activity here keeps the interconnect

00:02:48.920 --> 00:02:51.120
between the course busy, for example, the shared

00:02:51.120 --> 00:02:54.060
bus. So the one core that is actually

00:02:54.060 --> 00:02:57.900
doing useful work here while this is going on.

00:02:57.900 --> 00:02:59.694
If it has a cache miss will now be

00:02:59.694 --> 00:03:02.523
slower in handling this cache miss because it has

00:03:02.523 --> 00:03:04.662
to wait for the busy bus that these two

00:03:04.662 --> 00:03:07.760
are keeping busy. So not only is it power

00:03:07.760 --> 00:03:10.385
hungry to do this, it also by busy waiting

00:03:10.385 --> 00:03:13.160
in such an active way we're slowing down the

00:03:13.160 --> 00:03:15.785
useful work that is the only thing that can

00:03:15.785 --> 00:03:19.550
really result in us actually getting the lock eventually.

