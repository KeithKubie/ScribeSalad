WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:02.690
So we can take all of that
in just turn it into an MDP.

00:00:02.690 --> 00:00:06.010
I think it's pretty straightforward
to think how you'd turn this kind of

00:00:06.010 --> 00:00:08.790
story management system into
a market decision process, right.

00:00:08.790 --> 00:00:09.675
So what are states?

00:00:09.675 --> 00:00:10.970
Well, we said that last time.

00:00:10.970 --> 00:00:14.820
States are just partial trajectories or
partial plot sequences.

00:00:14.820 --> 00:00:17.500
Actions, well, they're the story
actions that the system can take.

00:00:17.500 --> 00:00:21.080
Your model is still the same model
that we're using before except it's

00:00:21.080 --> 00:00:22.995
a player model that means you know.

00:00:22.995 --> 00:00:24.850
We've seen a sequence of states.

00:00:24.850 --> 00:00:27.190
Or we've seen these trajectories.

00:00:27.190 --> 00:00:30.270
Some action is being taken, and
I want to know what plot state

00:00:30.270 --> 00:00:33.660
we're likely to be in, given that
the player is doing something.

00:00:33.660 --> 00:00:35.320
So it still looks just
like a transition model.

00:00:35.320 --> 00:00:37.790
And then the rewards mean
exactly what they meant before,

00:00:37.790 --> 00:00:39.360
except here they're
actually standing in for

00:00:39.360 --> 00:00:43.640
some notion of what some writer of
a story thinks of as a good story.

00:00:43.640 --> 00:00:47.770
&gt;&gt; And you can somehow formalize that so
that you can plan?

00:00:47.770 --> 00:00:49.400
&gt;&gt; Yeah, well, you could, but

00:00:49.400 --> 00:00:52.330
that gets us to a whole other discussion
about where rewards come from,

00:00:52.330 --> 00:00:54.710
which is probably a discussion
that we should have someday.

00:00:54.710 --> 00:00:56.530
So maybe we'll do that
towards the end of the class.

00:00:56.530 --> 00:00:59.380
But just imagine the right now you
have the reward function the same way

00:00:59.380 --> 00:01:01.550
that you would with any other MDP and

00:01:01.550 --> 00:01:03.470
you're really worry about where
it comes from it's just there.

00:01:03.470 --> 00:01:07.240
&gt;&gt; Okay and
given that these trajectories, or sorry,

00:01:07.240 --> 00:01:09.490
the states are now these
full trajectories.

00:01:09.490 --> 00:01:11.980
It seems like that makes some things
harder because now there could be

00:01:11.980 --> 00:01:15.900
a lot of them but on the other hand you
can never go back to a state you've

00:01:15.900 --> 00:01:18.310
been in before so it's acyclic.

00:01:18.310 --> 00:01:20.460
So you can actually solve it
more efficiently because you

00:01:20.460 --> 00:01:23.920
don't have to run, I mean basically
one value iteration and you're done.

00:01:23.920 --> 00:01:26.620
&gt;&gt; Well, that's actually kind
of neat that you said that.

00:01:26.620 --> 00:01:30.490
Because I was about to say that
there are two problems with this.

00:01:30.490 --> 00:01:33.900
The first problem is, well,
now that you don't have states, but

00:01:33.900 --> 00:01:37.450
you have sequences of states,
that's a lot of new states.

00:01:37.450 --> 00:01:38.650
How big do you think that is?

00:01:38.650 --> 00:01:40.160
&gt;&gt; I'm going to say very.

00:01:40.160 --> 00:01:41.470
&gt;&gt; It is very, very big.

00:01:41.470 --> 00:01:44.340
In fact, we can be even
more quantitative in that.

00:01:44.340 --> 00:01:46.680
It's actually, hyper exponential.

00:01:46.680 --> 00:01:48.090
It's like 2 to the 2 to the f.

00:01:48.090 --> 00:01:50.800
&gt;&gt; Where,
in terms of the length of the sequence?

00:01:50.800 --> 00:01:53.540
&gt;&gt; Yes, of all possible sequences
you can have the number of states.

00:01:53.540 --> 00:01:54.890
&gt;&gt; Even.
Okay, yes, yes.

00:01:54.890 --> 00:01:55.570
Okay, all right.

00:01:55.570 --> 00:01:58.200
&gt;&gt; Because the number of states
is sort of in factorial and

00:01:58.200 --> 00:02:00.040
all the different kind of that
we can have but we don't.

00:02:00.040 --> 00:02:02.890
All the states don't have
to actually be there and so

00:02:02.890 --> 00:02:06.020
at the end of the day it ends up looking
something like 2 to the 2, you know.

00:02:06.020 --> 00:02:07.240
And be really really really big.

00:02:07.240 --> 00:02:11.580
So the space that we're looking over of
all possible trajectories where things

00:02:11.580 --> 00:02:14.730
might be there things might not be there
gets really really big very quickly.

00:02:14.730 --> 00:02:16.060
So that's one point that you said.

00:02:16.060 --> 00:02:18.660
But on the other hand it turns out that
there's structure there that we're going

00:02:18.660 --> 00:02:19.970
to be able to take advantage of.

00:02:19.970 --> 00:02:22.140
And, in fact, that's what we do and

00:02:22.140 --> 00:02:26.350
why we're bothering to introduce this
formalism that I mentioned earlier.

00:02:26.350 --> 00:02:27.040
So let's do that.

00:02:27.040 --> 00:02:29.880
Actually before we do that,
let me mention one other

00:02:29.880 --> 00:02:33.260
problem with thinking about things
as Markov Decision Processes.

00:02:33.260 --> 00:02:36.920
And it's going to be a problem with the
strength of a Markov Decision Process.

00:02:36.920 --> 00:02:43.220
So what is the goal of reinforcement
was the goal of solving an MDP?

00:02:43.220 --> 00:02:44.130
&gt;&gt; Maximize reward.

00:02:44.130 --> 00:02:45.470
&gt;&gt; You maximize reward.

00:02:45.470 --> 00:02:49.800
So, in fact, it turns out that if you
take this sort of notion of storytelling

00:02:49.800 --> 00:02:53.840
and how I can get someone to do a choose
your own adventure and I've got rewards.

00:02:53.840 --> 00:02:57.360
I've got some evaluation
of what best stories are.

00:02:57.360 --> 00:02:58.400
What's going to happen?

00:02:58.400 --> 00:03:00.090
What is the system going
to learn how to do?

00:03:00.090 --> 00:03:01.520
&gt;&gt; Make the author happy?

00:03:01.520 --> 00:03:02.800
&gt;&gt; Make the author happy.

00:03:02.800 --> 00:03:05.440
That's exactly right,
but not necessarily.

00:03:05.440 --> 00:03:06.850
Make the player happy.

00:03:06.850 --> 00:03:09.380
&gt;&gt; So what ends up happening
with Markov Decision Processes,

00:03:09.380 --> 00:03:12.910
using sort of modeling as Markov
Decision Processes in the obvious way,

00:03:12.910 --> 00:03:18.060
is that you force the player
to experience the best story.

00:03:18.060 --> 00:03:19.810
No matter what the player does.

00:03:19.810 --> 00:03:23.380
No matter whatever choices
you're trying to make.

00:03:23.380 --> 00:03:25.400
Nope, this system is
going to make certain

00:03:25.400 --> 00:03:26.730
that you're going to enjoy yourself.

00:03:26.730 --> 00:03:27.860
Dammit, and

00:03:27.860 --> 00:03:30.640
enjoying yourself means whatever
the author thinks is enjoying yourself.

00:03:30.640 --> 00:03:31.510
&gt;&gt; So like grad school.

00:03:31.510 --> 00:03:32.790
&gt;&gt; It's just like grad school.

00:03:32.790 --> 00:03:34.760
&gt;&gt; No, no, no, that's not right.

00:03:34.760 --> 00:03:39.140
Why is that a problem
because the the author.

00:03:39.140 --> 00:03:41.180
I mean in a book that is
certainly what happens.

00:03:41.180 --> 00:03:43.860
The author gets to choose
what you can experience.

00:03:43.860 --> 00:03:45.040
&gt;&gt; And that's a good argument, but

00:03:45.040 --> 00:03:48.050
if we're bothered with this whole
kind of interactive entertainment,

00:03:48.050 --> 00:03:52.510
choose your own adventure story, then if
you don't really have any choice in what

00:03:52.510 --> 00:03:54.590
you do, then it's not really
a choose your own adventure story.

00:03:54.590 --> 00:03:55.870
&gt;&gt; Got it.
&gt;&gt; The basic idea here,

00:03:55.870 --> 00:03:58.910
is because there is a reward, and
because all the algorithms we've ever

00:03:58.910 --> 00:04:01.570
thought about with
Markov Decision Processes are about

00:04:01.570 --> 00:04:04.500
maximizing long term award
that it's finding an answer.

00:04:04.500 --> 00:04:06.650
It's going to find an answer
that's going to force you

00:04:06.650 --> 00:04:07.720
down a particular path.

00:04:07.720 --> 00:04:12.250
But it turns out we can relax that
need to find the best answer and

00:04:12.250 --> 00:04:14.950
actually end up solving hyperextend
is a problem at the same time.

00:04:14.950 --> 00:04:15.450
So, let's do that.

