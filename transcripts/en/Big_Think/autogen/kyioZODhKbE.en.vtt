WEBVTT
Kind: captions
Language: en

00:00:05.660 --> 00:00:11.690
So most of us think that information is the
best way to convince people of our truth,

00:00:11.690 --> 00:00:16.360
and in fact it doesn’t work that well.We
see that all the time.

00:00:16.360 --> 00:00:21.410
We see it with climate change, where there’s
tons of data suggesting that climate change

00:00:21.410 --> 00:00:27.029
is man-made but about 50 percent of the population
doesn’t believe it, or with people arguing

00:00:27.029 --> 00:00:31.680
about things like how many people were in
the presidential inauguration.

00:00:31.680 --> 00:00:36.010
So we have facts but people decide which facts
they want to listen to, which facts they want

00:00:36.010 --> 00:00:40.890
to take and change their opinions, and which
they want to disregard.

00:00:40.890 --> 00:00:49.129
And one of the reasons for this is when something
doesn’t conform to what I already believe,

00:00:49.129 --> 00:00:55.969
what people tend to do is either disregard
it or rationalize it away; because information

00:00:55.969 --> 00:01:00.039
doesn’t take into account what makes us
human, which is our emotions, our desires,

00:01:00.039 --> 00:01:02.360
our motives and our prior beliefs.

00:01:02.360 --> 00:01:06.540
So for example, in one study my colleagues
and I tried it to see whether we could use

00:01:06.540 --> 00:01:10.060
science to change people’s opinions about
climate change.

00:01:10.060 --> 00:01:14.610
The first thing we did was ask people, “Do
you believe in man-made climate change?

00:01:14.610 --> 00:01:16.570
Do you support the Paris Agreement?”

00:01:16.570 --> 00:01:21.830
And based on their answers we divided them
into the strong believers and the weak believers.

00:01:21.830 --> 00:01:23.540
And then we gave them information.

00:01:23.540 --> 00:01:29.610
For some people we said that scientists have
reevaluated the data and now conclude that

00:01:29.610 --> 00:01:33.649
things are actually much worse than they thought
before, that the temperature would rise by

00:01:33.649 --> 00:01:36.330
about seven degrees to ten degrees.

00:01:36.330 --> 00:01:40.700
For some people we said the scientists have
reevaluated the data and they now believe

00:01:40.700 --> 00:01:45.530
that actually this situation is not as bad
as they thought, it’s much better, and the

00:01:45.530 --> 00:01:48.330
rise in temperature would be quite small.

00:01:48.330 --> 00:01:52.880
And what we found is that people who did not
believe in climate change, when they heard

00:01:52.880 --> 00:01:56.670
that the scientists are saying, “Actually
it’s not that bad,” they changed their

00:01:56.670 --> 00:02:00.820
beliefs even more in that direction, so they
became more extremist in that direction, but

00:02:00.820 --> 00:02:04.400
when they heard that the scientists think
it’s much worse they didn’t nudge.

00:02:04.400 --> 00:02:08.899
And the people who already believe that climate
change is man-made, when they heard that scientists

00:02:08.899 --> 00:02:12.800
are saying things are much worse than they
said before, they moved more in that direction,

00:02:12.800 --> 00:02:16.750
so they became more polarized, but when they
heard scientists are saying it’s not that

00:02:16.750 --> 00:02:18.360
bad they didn’t nudge much.

00:02:18.360 --> 00:02:23.850
So we gave people information and as a result
it caused polarization, it didn’t cause

00:02:23.850 --> 00:02:26.690
people to come together.

00:02:26.690 --> 00:02:31.620
So the question is, what’s happening inside
our brain that causes this?

00:02:31.620 --> 00:02:38.320
And in one study my colleagues and I scanned
brain activity of two people who were interacting,

00:02:38.320 --> 00:02:43.540
and what we found was when those two people
agreed on a question that we gave them, the

00:02:43.540 --> 00:02:49.760
brain was really encoding what the other person
was saying, the details that they gave; but

00:02:49.760 --> 00:02:56.320
when the two people disagreed it looked metaphorically
as if the brain was switching off and not

00:02:56.320 --> 00:03:01.690
encoding what the other person was saying.And
as a result when the two agreed they became

00:03:01.690 --> 00:03:06.210
even more confident, but when they disagreed
there wasn’t as much of a change in their

00:03:06.210 --> 00:03:08.400
confidence in their own view.

00:03:08.400 --> 00:03:13.970
What has been shown by Kahan and colleagues
from Yale University is that the more intelligent

00:03:13.970 --> 00:03:19.420
you are the more likely you are to change
data at will.

00:03:19.420 --> 00:03:26.040
So what they did is they first gave participants
in their experiment analytical and math questions

00:03:26.040 --> 00:03:32.709
to solve, and then they gave them data about
gun control: is gun control actually reducing

00:03:32.709 --> 00:03:34.010
violence?

00:03:34.010 --> 00:03:39.690
And they found that more “intelligent”
people actually were more likely to twist

00:03:39.690 --> 00:03:44.130
data at will to make it conform to what they
already believed.So it seems that people are

00:03:44.130 --> 00:03:50.959
using their intelligence not necessarily to
find the truth, but to take in the information

00:03:50.959 --> 00:03:56.550
and change it to conform to what they already
believe.

00:03:56.550 --> 00:04:02.209
So that suggests that just giving people information
without considering first where they’re

00:04:02.209 --> 00:04:11.030
coming from may backfire at us, but we don’t
always need to go against someone’s conviction

00:04:11.030 --> 00:04:13.709
in order to change their behavior, and let
me give you an example.

00:04:13.709 --> 00:04:20.299
So this is a study that was conducted at UCLA
where what they wanted to do is convince parents

00:04:20.299 --> 00:04:21.630
to vaccinate their kids.

00:04:21.630 --> 00:04:26.100
And some of the parents didn’t want to vaccinate
their kids because they were afraid of the

00:04:26.100 --> 00:04:32.670
link with autism.So they had two approaches,
first they said, “Well the link with autism

00:04:32.670 --> 00:04:36.310
is actually not real, here’s all the data
suggesting there isn’t a link between vaccines

00:04:36.310 --> 00:04:37.530
and autism.”

00:04:37.530 --> 00:04:39.139
And it didn’t really work that well.

00:04:39.139 --> 00:04:41.460
But instead they used another approach.

00:04:41.460 --> 00:04:46.200
So instead of going that way they used another
approach, which was: let’s not talk about

00:04:46.200 --> 00:04:50.040
autism, we don’t necessarily need to talk
about autism to convince you to vaccinate

00:04:50.040 --> 00:04:51.040
your kids.

00:04:51.040 --> 00:04:56.540
Instead they said, “Well look, these vaccines
protect kids from deadly diseases, from the

00:04:56.540 --> 00:05:00.590
measles,” and they showed them pictures
of what the measles are.

00:05:00.590 --> 00:05:05.040
Because in this argument about vaccines people
actually forgot what the vaccines are for,

00:05:05.040 --> 00:05:07.470
what are they protecting us from.

00:05:07.470 --> 00:05:11.919
And they highlighted that and didn’t necessarily
go on to discuss autism.

00:05:11.919 --> 00:05:14.970
That had a much better outcome.

00:05:14.970 --> 00:05:20.260
The parents were much more likely to say,
“Yes we are going to vaccinate our kids.”

00:05:20.260 --> 00:05:24.720
So the lesson here is that we need to find
the common motive.

00:05:24.720 --> 00:05:30.410
The common motive in this case was the health
of the children, not necessarily going back

00:05:30.410 --> 00:05:33.370
to the thing that they were arguing about,
that they disagreed about.

