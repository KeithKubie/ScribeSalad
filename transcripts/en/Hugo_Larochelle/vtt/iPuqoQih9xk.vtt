WEBVTT

1
00:00:00.490 --> 00:00:01.031
In this video,

2
00:00:01.031 --> 00:00:05.050
we'll discuss some extensions of the standard restricted Boltzmann machine.

3
00:00:07.660 --> 00:00:08.110
<v 1>Okay.</v>

4
00:00:08.110 --> 00:00:09.340
<v 0>So,
uh,</v>

5
00:00:09.550 --> 00:00:13.930
we've seen the basic restricted Boltzmann machine and a dark do things that

6
00:00:13.931 --> 00:00:17.590
makes the restricted Boltzmann machine,
uh,
basic.

7
00:00:17.591 --> 00:00:20.290
The first is that we have,
um,

8
00:00:20.320 --> 00:00:23.680
we have only considered binary visible units.

9
00:00:23.920 --> 00:00:26.260
And so we'll talk a little bit more about,
uh,

10
00:00:26.261 --> 00:00:29.080
whether we can extend it to other types of observation.

11
00:00:29.290 --> 00:00:33.310
And the other part of it is the fact that it's restricted.

12
00:00:33.311 --> 00:00:38.311
That is we are not allowing for connections between either,

13
00:00:38.440 --> 00:00:42.820
uh,
the elements in the visible layers are the elements in the hidden layers.

14
00:00:43.600 --> 00:00:46.260
So we also talk about,
uh,
uh,

15
00:00:46.261 --> 00:00:49.270
what happens when the Boltzmann machine is not restricted.

16
00:00:51.960 --> 00:00:55.530
So first the generalization to other observations.
Um,

17
00:00:55.590 --> 00:01:00.010
one particular observation type that we'd like to be able to support is a

18
00:01:00.011 --> 00:01:04.740
unbounded wheels.
So values between minus infinity to plus update it.

19
00:01:05.430 --> 00:01:10.350
And it turns out that there's an extension known as the Galician Bernoulli that

20
00:01:10.351 --> 00:01:14.510
allows to a model will valid observations.
Uh,

21
00:01:14.580 --> 00:01:18.480
and the extension is actually a fairly straight forward to obtain.
Uh,

22
00:01:18.720 --> 00:01:23.640
all it requires is that we added quadratic term into the energy function,
uh,

23
00:01:23.670 --> 00:01:28.020
which,
uh,
and the quadratic term needs to be positive.
So,

24
00:01:28.050 --> 00:01:32.550
uh,
so here we have the quarterly Terman question.
Uh,

25
00:01:32.610 --> 00:01:34.350
and uh,
if we do this,

26
00:01:34.530 --> 00:01:39.530
then the only thing that this is changing actually the definition or the,

27
00:01:39.780 --> 00:01:40.890
uh,
um,

28
00:01:41.230 --> 00:01:45.370
a corresponding conditional distribution p of x given h,
uh,

29
00:01:45.390 --> 00:01:48.720
that's because we've added the term that only depends on it on x.

30
00:01:48.721 --> 00:01:49.710
So it's a constant,

31
00:01:49.950 --> 00:01:54.000
if we were to compute p of a h given x and uh,

32
00:01:54.020 --> 00:01:57.930
actually what this is going to yield is a Gaussian distribution,

33
00:01:58.410 --> 00:01:59.160
uh,

34
00:01:59.160 --> 00:02:03.510
which is such that it's conditional mean is going to be just a linear

35
00:02:03.511 --> 00:02:05.610
transformation of the hidden layer.

36
00:02:06.090 --> 00:02:10.560
And the covariance matrix is actually going to be an identity covariance matrix.

37
00:02:10.570 --> 00:02:13.170
We have a variance of one over each arm in the vex,

38
00:02:13.620 --> 00:02:18.620
a conditional veterans of one given the admin layer and a no correlation between

39
00:02:18.960 --> 00:02:22.950
the elements in the Matrix.
Um,
you can,
uh,

40
00:02:22.980 --> 00:02:26.280
there's a variant which is a bit more involved,
which,
uh,

41
00:02:26.310 --> 00:02:31.310
where we can actually play with the a and model the conditional covariance

42
00:02:31.891 --> 00:02:36.750
matrix.
Uh,
I encourage you to look at the literature for more details on this,

43
00:02:37.020 --> 00:02:41.400
but a very simple thing to do to,
uh,

44
00:02:41.790 --> 00:02:42.960
in the sense,
uh,

45
00:02:43.000 --> 00:02:47.460
avoid having to do this is to just normalize the training set,
uh,

46
00:02:47.540 --> 00:02:52.350
by,
uh,
first subtracting the mean.
So it's just a will allow us to,
uh,

47
00:02:52.450 --> 00:02:56.890
uh,
make it easier to not have to learn the mean of the,
uh,
the,

48
00:02:56.900 --> 00:02:59.320
the average value of the inputs.
Uh,

49
00:02:59.380 --> 00:03:04.380
and then to sort of standardize it in terms of it's a covariance a structure.

50
00:03:04.720 --> 00:03:09.010
Then we could just divide each of the input by its training set,

51
00:03:09.011 --> 00:03:13.630
standard deviation.
So if we do this,
then,
uh,
it's uh,

52
00:03:13.810 --> 00:03:18.450
uh,
it tends to be easier to learn this and Bernoulli rpm and intense.

53
00:03:18.490 --> 00:03:23.490
Also to be a bit less important to learn the conditional a covariance or the

54
00:03:23.951 --> 00:03:28.570
conditional variants of each element in the input vector.
Still,

55
00:03:28.600 --> 00:03:33.600
it's a model that that is still a bit harder to train than a regular RBM in

56
00:03:35.861 --> 00:03:39.190
particular in practice,
unless we use a,
uh,

57
00:03:39.460 --> 00:03:43.840
usually smaller learning rate than what we use for the standard RBM.
Uh,

58
00:03:43.870 --> 00:03:47.770
we tend to find sometimes that training can diverged so you have to be a bit

59
00:03:47.771 --> 00:03:52.720
more careful in the selection of the learning rate.
But,
uh,
still,

60
00:03:52.780 --> 00:03:55.010
uh,
it's still the fact that,
you know,

61
00:03:55.030 --> 00:03:59.770
this is a valid model for a real valued,
uh,

62
00:03:59.771 --> 00:04:02.130
observations and,
uh,

63
00:04:02.170 --> 00:04:05.860
it's been used by some with success to learn good features from real value

64
00:04:05.861 --> 00:04:06.694
factors.

65
00:04:09.750 --> 00:04:10.330
<v 2>Uh,</v>

66
00:04:10.330 --> 00:04:14.620
<v 0>it will learn different features.
So if we train it on the amness Dataset,</v>

67
00:04:14.680 --> 00:04:19.680
this is an example of what you can get a we do get are sort of pen stroke

68
00:04:20.471 --> 00:04:23.170
detectors,
like hidden units.
Uh,

69
00:04:23.200 --> 00:04:26.980
other features here are harder to interpret.
Uh,

70
00:04:26.981 --> 00:04:31.180
you can get more meaningful features by introducing some sparsity in the hidden

71
00:04:31.181 --> 00:04:34.150
layer.
Uh,
you can consult the,
uh,

72
00:04:34.240 --> 00:04:37.460
course website for literature on that.
Uh,

73
00:04:37.510 --> 00:04:41.430
but I'm just showing this really do illustrate the fact that,
uh,

74
00:04:41.470 --> 00:04:45.010
you will get different features.
Uh,
if you use,
say,

75
00:04:45.011 --> 00:04:48.970
a gash in our Bernoulli RBN than if you use a regular RBM.

76
00:04:51.470 --> 00:04:51.940
<v 2>Okay.</v>

77
00:04:51.940 --> 00:04:55.600
<v 0>And then there exist extensions for other types of observations.
For instance,</v>

78
00:04:55.630 --> 00:04:57.130
binomial observations.

79
00:04:57.131 --> 00:05:01.700
So an observation between a zero and an infant,

80
00:05:01.701 --> 00:05:04.240
you're a c n a.

81
00:05:04.241 --> 00:05:06.910
So he can look at this paper for more about that.

82
00:05:07.450 --> 00:05:12.450
This can be useful in this case if you want to actually model explicitly the

83
00:05:12.641 --> 00:05:15.960
integer value intensity of pixels,
uh,

84
00:05:16.060 --> 00:05:20.020
instead of treating it as either a binary value or real value value,

85
00:05:20.770 --> 00:05:24.790
uh,
for modeling multinomial observation.
So for instance,

86
00:05:24.791 --> 00:05:28.570
words in text,
uh,
I give you two references here.

87
00:05:28.630 --> 00:05:32.620
Darson and challenges related to modeling words,
uh,

88
00:05:32.650 --> 00:05:34.510
which are related to,
uh,

89
00:05:34.660 --> 00:05:38.830
the fact that words are intrinsically very high dimensional objects that can

90
00:05:38.831 --> 00:05:43.510
come from very large vocabulary.
And so these two papers are example,
uh,

91
00:05:43.790 --> 00:05:47.920
examples of papers that tried to deal with this difficulty.
And A,

92
00:05:47.921 --> 00:05:52.780
I encourage you to look at the course website to,
uh,
get more,
uh,
uh,

93
00:05:52.960 --> 00:05:55.600
more papers on other types of Barrons for modeling,

94
00:05:55.601 --> 00:05:58.440
different types of observations,
uh,

95
00:05:58.540 --> 00:06:01.610
and in particular for modeling veal valued data.

96
00:06:01.640 --> 00:06:06.170
There are other more sophisticated extensions of the RBM,

97
00:06:06.440 --> 00:06:11.180
which,
uh,
for instance,
for,
uh,
uh,
modeling pixels,
uh,

98
00:06:11.210 --> 00:06:11.930
can,
uh,

99
00:06:11.930 --> 00:06:16.930
actually perform better than the Gash in our Bernoulli RBM in its basic form as

100
00:06:17.211 --> 00:06:22.100
I've described it.
And finally,

101
00:06:22.220 --> 00:06:26.870
what about if we make a restrictive Bolsa machine non restricted?
Uh,

102
00:06:26.900 --> 00:06:29.620
in fact,
uh,
originally the,
uh,

103
00:06:29.660 --> 00:06:33.290
Boltzmann machine was proposed and it's only later that the restricted Ferran

104
00:06:33.291 --> 00:06:35.030
was proposed.
Uh,

105
00:06:35.120 --> 00:06:38.990
how I think the fact that if we didn't have connections between the hidden layer

106
00:06:39.020 --> 00:06:43.790
and the visible layer,
then a certain things became a much more efficiently,

107
00:06:44.210 --> 00:06:48.320
uh,
certain computations were much more efficient to,
for instance,

108
00:06:48.350 --> 00:06:52.310
we get some interesting conditional independence between layers.

109
00:06:52.610 --> 00:06:57.590
But in the original Boltzmann machine you would have connections between,
uh,

110
00:06:57.620 --> 00:07:02.090
the input elements.
So that would allow us to model direct,
uh,

111
00:07:02.120 --> 00:07:04.550
dependencies between elements of the input vector.

112
00:07:04.551 --> 00:07:07.910
And we do this by adding this term in the energy function.

113
00:07:08.210 --> 00:07:12.260
So we have a matrix which a wood model,
our preference,

114
00:07:12.590 --> 00:07:15.830
uh,
between the values of,
uh,

115
00:07:15.890 --> 00:07:20.750
pairs of input elements in the input vector.
And then,

116
00:07:21.170 --> 00:07:22.970
uh,
we could have again,

117
00:07:23.090 --> 00:07:28.090
such connections by adding a term like this in the energy function,

118
00:07:29.180 --> 00:07:34.180
which will allow us to model directly the dependencies between a hidden units

119
00:07:34.520 --> 00:07:36.950
and have certain times of interaction patterns,

120
00:07:37.220 --> 00:07:42.170
inhibition and excitation patterns between the,
uh,
hidden units.
And,

121
00:07:42.510 --> 00:07:46.760
uh,
sometimes we can actually use just one and not the other,
uh,

122
00:07:46.790 --> 00:07:49.940
in which case we'll talk about a semi restricted Boltzmann machine.

123
00:07:50.180 --> 00:07:52.490
So this would be a Boltzmann machine,
for instance.

124
00:07:52.760 --> 00:07:57.760
We could decide to remove those only have interactions between the hidden units.

125
00:07:58.520 --> 00:08:00.950
In this case,
uh,
the,

126
00:08:00.980 --> 00:08:05.300
the visible layer would be conditionally,
uh,
each element in the visible layer.

127
00:08:05.301 --> 00:08:08.540
We conditioned independent given the hidden unit,

128
00:08:08.570 --> 00:08:13.460
but he would not be true the other way around.
Uh,
and,
uh,

129
00:08:13.540 --> 00:08:18.440
uh,
there's been some papers that have shown that for modeling natural image

130
00:08:18.441 --> 00:08:20.990
patches,
uh,
removing this,

131
00:08:21.080 --> 00:08:25.760
but only maintaining these connections can be beneficial.
So again,

132
00:08:25.761 --> 00:08:30.080
there's a lot of literature on,
uh,
tried to,
uh,

133
00:08:30.110 --> 00:08:35.060
actually do try to incorporate these types of interactions between either hidden

134
00:08:35.061 --> 00:08:39.140
units are visible units,
coach you to look at the literature in the,

135
00:08:39.690 --> 00:08:43.670
uh,
you know,
by googling different papers on Boltzmann machine.

136
00:08:44.030 --> 00:08:48.710
The main thing to remember is that if we do introduce these interactions,

137
00:08:48.800 --> 00:08:50.330
perhaps who have a more powerful model,

138
00:08:50.331 --> 00:08:54.260
but it's much harder to train because if we want to do contrast the divergence,

139
00:08:54.261 --> 00:08:56.430
for instance,
sampling from this model,

140
00:08:56.431 --> 00:09:01.431
performing Gib sampling now requires that we update each element in our

141
00:09:03.271 --> 00:09:06.480
undirected graphical model individually given the others.

142
00:09:06.510 --> 00:09:10.530
Because now we don't have the nice conditional independence property that we

143
00:09:10.560 --> 00:09:14.580
have in the restricted Boltzmann machine.
So for that reason,
this model,

144
00:09:14.610 --> 00:09:18.120
the footballs,
the machine is not as popular as the restricted Boltzmann machine.

