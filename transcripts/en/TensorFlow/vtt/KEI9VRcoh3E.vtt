WEBVTT
Kind: captions
Language: en

00:00:00.126 --> 00:00:02.689
♪ (intro music) ♪

00:00:04.509 --> 00:00:09.280
Shall we take a look at the first one
that came in, was from @alpharthur:

00:00:09.710 --> 00:00:15.314
"Can I ask about any prebuilt binary
for the RTX 2080 GPU on Ubuntu 16?"

00:00:15.314 --> 00:00:16.759
That is very specific.

00:00:16.759 --> 00:00:18.202
- That is very specific.
- (laughter)

00:00:18.202 --> 00:00:21.388
So, I like specific questions,
even if I can't answer them.

00:00:21.388 --> 00:00:22.523
(laughter)

00:00:22.523 --> 00:00:24.197
So, in this case,

00:00:24.197 --> 00:00:27.581
the prebuilt binaries for TensorFlow

00:00:27.581 --> 00:00:30.780
tend to be associated
with a specific driver from Nvidia.

00:00:30.780 --> 00:00:32.927
So, the version of CUDA that we support

00:00:32.927 --> 00:00:35.412
or the version of cuDNN that we support.

00:00:35.412 --> 00:00:37.005
So, my recommendation would be

00:00:37.005 --> 00:00:40.347
if you're taking a look
at any of the prebuilt binaries,

00:00:40.347 --> 00:00:43.122
take a look at what driver
or what version of the driver

00:00:43.122 --> 00:00:45.487
you have supported on that specific card.

00:00:45.487 --> 00:00:48.423
I'm not an expert on Nvidia cards,
although I love them,

00:00:48.423 --> 00:00:51.273
so I don't really know
what's supported by that card, Arthur.

00:00:51.273 --> 00:00:53.718
But if you go over here,
like on my laptop,

00:00:53.718 --> 00:00:59.030
I've called up some of what Nvidia say
as their TensorFlow system requirements

00:00:59.030 --> 00:01:02.333
and the specific versions
of the drivers that they support.

00:01:02.333 --> 00:01:03.791
And the one gotcha--

00:01:03.791 --> 00:01:05.558
and we had this
in the last segment as well--

00:01:05.558 --> 00:01:07.551
that I find one working with GPUs

00:01:07.551 --> 00:01:10.942
is that it's easy for you
to go to the driver vendor

00:01:10.942 --> 00:01:12.910
and download the latest version.

00:01:12.910 --> 00:01:15.273
But that may not be the one
that TensorFlow is built for

00:01:15.273 --> 00:01:16.417
or the one that it supports.

00:01:16.417 --> 00:01:18.575
So, just make sure
that they actually match each other.

00:01:18.575 --> 00:01:21.331
And you should be good to go,
even with that particular card.

00:01:21.331 --> 00:01:25.394
Yes. And if you have
warm feelings and excitement

00:01:25.394 --> 00:01:27.794
about builds in general for TensorFlow,

00:01:27.794 --> 00:01:29.744
we have a great special interest group

00:01:29.744 --> 00:01:32.296
specifically focused on that
called SIG-Build.

00:01:32.296 --> 00:01:33.496
(Laurence) SIG-Build.

00:01:33.496 --> 00:01:37.241
So, I strongly suggest going
to the community section of our GitHub

00:01:37.611 --> 00:01:40.621
and checking out the SIG-Build listserv

00:01:40.621 --> 00:01:45.654
and joining it and joining
our weekly stand-ups.

00:01:45.654 --> 00:01:48.136
Right. So, thanks, Arthur,
for that question.

00:01:48.136 --> 00:01:51.314
And the next question
is a really funny one, I think.

00:01:51.314 --> 00:01:53.118
How many times
have you been asked this today?

00:01:53.118 --> 00:01:55.462
Oh, my God, at least 12.

00:01:55.462 --> 00:01:57.180
- At least!
- (laughter)

00:01:57.180 --> 00:01:58.737
And then the other flavor of it is,

00:01:58.737 --> 00:02:01.683
well, is this is a particular symbol
that I use all the time?

00:02:01.683 --> 00:02:04.423
Is this going to also be supported
in TensorFlow 2.0?

00:02:04.423 --> 00:02:06.223
And if not, what is changed?

00:02:06.223 --> 00:02:07.503
(giggling)

00:02:08.033 --> 00:02:11.017
People have invested so much time
building stuff in TensorFlow 1.x,

00:02:11.017 --> 00:02:12.461
they don't want it to be deprecated,

00:02:12.461 --> 00:02:14.967
- they don't want it to go away...
- (Paige) Understandable.

00:02:14.967 --> 00:02:16.317
So, how do we answer it?

00:02:16.317 --> 00:02:18.559
"Do my TensorFlow scripts
work with TensorFlow 2.0"?

00:02:19.749 --> 00:02:23.838
The sad fact is that probably not.

00:02:23.838 --> 00:02:28.048
They would not work
with TensorFlow 2.0 out of the box.

00:02:28.048 --> 00:02:32.597
But we have created
an upgrade utility for you to use.

00:02:32.597 --> 00:02:35.405
It's automatically downloaded
with TensorFlow 2.0

00:02:35.405 --> 00:02:36.752
whenever you download it.

00:02:36.752 --> 00:02:39.591
For more information on it
and what in particular it's doing,

00:02:39.591 --> 00:02:44.093
you can check out this medium blog post
that I and my colleague Anna created,

00:02:44.093 --> 00:02:47.113
as well as this
Upgrade to TensorFlow 2.0 video.

00:02:47.813 --> 00:02:49.674
It goes through, and with GIFs,

00:02:49.674 --> 00:02:53.160
which is the best communication
medium possible.

00:02:53.730 --> 00:02:57.317
It shows you how you can use
the upgrade script on an end file.

00:02:58.097 --> 00:03:01.876
So, any sort of arbitrary Python file
or even Jupyter Notebooks--

00:03:01.876 --> 00:03:04.632
one of our machine-learning GDEs
created an extension

00:03:04.632 --> 00:03:07.015
that will allow you to do that as well.

00:03:07.425 --> 00:03:13.501
And it'll give you an <i>export.txt</i> file
that shows you all of the symbol renames,

00:03:13.501 --> 00:03:14.806
the added keywords,

00:03:14.806 --> 00:03:18.308
and then also some manual changes
if you have to make manual changes.

00:03:18.308 --> 00:03:20.524
- (Lawrence) Cool.
- (Paige) Usually, you do not.

00:03:20.794 --> 00:03:25.132
So, to see this in action,
we can go and take a look

00:03:25.542 --> 00:03:32.472
at this particular text generation example
that we have running a Shakespeare--

00:03:32.472 --> 00:03:35.722
well, it takes all of the corpus
of Shakespeare text,

00:03:35.722 --> 00:03:39.809
trains against the Shakespeare text
and generates something

00:03:39.809 --> 00:03:42.311
that the bard could have
potentially written

00:03:43.641 --> 00:03:49.430
should he have had access
to deep learning resources.

00:03:49.430 --> 00:03:51.372
(Laurence) "I know you all and will uphold

00:03:51.372 --> 00:03:54.206
the wildest unyoked humour
of your [idleness.]"

00:03:54.206 --> 00:03:56.226
(Paige) I did not know
you knew Shakespeare.

00:03:56.226 --> 00:03:57.239
(Lawrence chuckles)

00:03:57.239 --> 00:03:59.341
I actually played Henry IV in high school.

00:03:59.341 --> 00:04:00.750
That's amazing.

00:04:00.750 --> 00:04:02.099
That's why I love this notebook.

00:04:02.099 --> 00:04:05.258
I was Beatrice
in <i>Much Ado About Nothing</i>.

00:04:05.258 --> 00:04:06.835
(Laurence) Oh, cool.

00:04:06.835 --> 00:04:09.949
While we're on <i>Much Ado About Nothing,</i>
maybe we should go back to the notebook.

00:04:09.949 --> 00:04:13.094
Yes, so here's what it looks like
in collab form,

00:04:13.094 --> 00:04:15.917
text generation using an RRN
with eager execution.

00:04:15.917 --> 00:04:18.339
You could export the Python file,

00:04:18.339 --> 00:04:20.198
and then to upgrade it--

00:04:20.568 --> 00:04:22.655
(Laurence) You've got to reconnect
the runtime first.

00:04:22.655 --> 00:04:23.706
(Paige) This is true.

00:04:23.976 --> 00:04:27.189
So... starting it.

00:04:27.189 --> 00:04:29.323
It looks like the requirements--

00:04:29.873 --> 00:04:32.601
we can check to see
that we're using TensorFlow Alpha.

00:04:32.601 --> 00:04:35.100
And then, like I mentioned before,

00:04:35.100 --> 00:04:38.157
all you would have to do
is preface this with the <i>!tf_upgrade_v2</i>,

00:04:38.557 --> 00:04:41.943
the name of the Python file
is <i>text_generation</i>.

00:04:41.943 --> 00:04:43.817
I want to create an upgrade.

00:04:43.817 --> 00:04:45.262
<i>Shift+Enter</i>.

00:04:45.262 --> 00:04:47.812
It does its upgrading magic,

00:04:47.812 --> 00:04:49.381
and very quickly,

00:04:49.381 --> 00:04:51.751
tells me all of the things
that would need to be changed

00:04:51.751 --> 00:04:54.445
to make it 2.0 compatible

00:04:54.445 --> 00:04:57.850
and creates that file for me
off to the side.

00:04:57.850 --> 00:05:01.459
So, now, if I wanted to run this model,

00:05:01.819 --> 00:05:07.888
it should be able to train as it would.

00:05:07.888 --> 00:05:12.031
So, let's just check
to make sure that would be the case.

00:05:12.031 --> 00:05:14.215
(Laurence) I think a lot of the errors
that you're seeing here--

00:05:14.215 --> 00:05:15.720
it's more just renamed APIs

00:05:15.720 --> 00:05:17.843
rather than breaking changes
within the API--

00:05:17.843 --> 00:05:18.853
(Paige) This is true.

00:05:18.853 --> 00:05:24.166
So, you can see that you have some renames
and some additional keywords.

00:05:24.166 --> 00:05:25.405
Sounds good. And I saw you have

00:05:25.405 --> 00:05:27.080
some handy-dandy GIFs in there?

00:05:27.080 --> 00:05:28.212
Yes! Absolutely.

00:05:28.212 --> 00:05:30.602
Are there any GIFs
for those who don't say "JIF"?

00:05:30.602 --> 00:05:31.606
(laughter)

00:05:31.606 --> 00:05:33.145
Sorry, I had to work that joke in.

00:05:34.595 --> 00:05:35.623
Well, I'm PB,

00:05:35.623 --> 00:05:38.251
so peanut butter automatically works.

00:05:38.251 --> 00:05:39.522
Exactly. Sounds good.

00:05:39.522 --> 00:05:43.574
So, when it comes to upgrade,
there are a few little gotchas in summary,

00:05:43.574 --> 00:05:45.281
but hopefully this blog post
and your video

00:05:45.281 --> 00:05:46.763
and all of the stuff that we're doing

00:05:46.763 --> 00:05:48.631
will help you get around those gotchas.

00:05:48.631 --> 00:05:50.501
And even more amazingly,

00:05:50.501 --> 00:05:52.746
the community
that you were mentioning before--

00:05:52.746 --> 00:05:56.313
we've had such an interest
in testing TensorFlow 2.0

00:05:56.313 --> 00:05:58.565
and trying it out against historic models

00:05:59.295 --> 00:06:01.902
that we've formed
a weekly testing stand-up,

00:06:02.292 --> 00:06:05.305
and also we have a migration support hour

00:06:05.305 --> 00:06:08.928
that's being implemented
with the internal support hour.

00:06:08.928 --> 00:06:11.922
So, if you have
an external group to Google

00:06:12.272 --> 00:06:14.810
that's interested
in upgrading your models,

00:06:14.810 --> 00:06:18.619
please join the testing group,
and we can get you situated.

00:06:18.619 --> 00:06:21.475
And a lot of stuff that we've seen,
like in Keras models, for example--

00:06:21.475 --> 00:06:24.102
Karmel had that great slide
where she was training Fashion MNIST.

00:06:24.102 --> 00:06:26.574
- The code is the exact same.
- It's exactly the same.

00:06:26.574 --> 00:06:28.858
So, while there might be
stuff changing under the hood,

00:06:28.858 --> 00:06:31.044
a lot of the surface-level code

00:06:31.044 --> 00:06:34.105
that you're going to be writing
in Keras, at least, isn't changing.

00:06:34.105 --> 00:06:35.368
If you've used Keras,

00:06:35.368 --> 00:06:37.533
you're probably
not going to have any problems.

00:06:38.073 --> 00:06:39.314
So... good stuff.

00:06:39.314 --> 00:06:41.253
So, shall we move on to the next question?

00:06:41.253 --> 00:06:43.475
- Yes!
- I know we could talk about 2.0 all day.

00:06:44.125 --> 00:06:46.539
Okay, we just mentioned Keras,
and it appears.

00:06:47.109 --> 00:06:49.075
So, I guess I could ask you this question.

00:06:49.815 --> 00:06:51.094
Hopefully, you know the answer.

00:06:51.094 --> 00:06:55.379
"What is the purpose of keeping Estimators
and Keras as separate APIs?

00:06:55.379 --> 00:06:58.039
Is there going to be
something native to Keras models

00:06:58.039 --> 00:07:01.067
that allows for distributed training
à la <i>train_and_evaluate</i>?"

00:07:01.067 --> 00:07:03.932
Okay, so the purpose
of keeping them, I think,

00:07:03.932 --> 00:07:05.480
there are many purposes, right?

00:07:05.480 --> 00:07:08.508
So, I think for me, the main purpose
that I would like to think of

00:07:08.508 --> 00:07:11.425
is one that is because a lot
of people are using them.

00:07:11.425 --> 00:07:13.933
And including internal Google teams

00:07:13.933 --> 00:07:16.495
that would tar and feather us
if we removed them.

00:07:16.495 --> 00:07:17.899
(laughter)

00:07:18.749 --> 00:07:21.579
So, when it comes to Estimators,

00:07:21.579 --> 00:07:23.684
Estimators are really great
for large-scale training.

00:07:23.684 --> 00:07:24.681
Yes!

00:07:24.681 --> 00:07:27.241
A lot of the time, if you're doing
a lot of large-scale training,

00:07:27.241 --> 00:07:29.817
keep going with Estimators
because they're great!

00:07:29.817 --> 00:07:33.307
Because when I started with TensorFlow,
I started with Estimators,

00:07:33.307 --> 00:07:36.162
because I couldn't figure out
what a node was in a neural network,

00:07:36.162 --> 00:07:38.573
and there were all
these concepts that I had to learn,

00:07:38.573 --> 00:07:41.379
while I had this simple Estimator
that I could use

00:07:41.379 --> 00:07:43.286
to do a DNN or something like that.

00:07:43.286 --> 00:07:46.419
So, they're there for a reason,
and they're staying for the reason.

00:07:46.419 --> 00:07:48.058
Keras is one of the things

00:07:48.058 --> 00:07:51.048
that, from the point of view
of making life easier for developers,

00:07:51.048 --> 00:07:53.763
that we've really been doubling down
on TensorFlow 2.0.

00:07:53.763 --> 00:07:58.134
And things like we just spoke about,
the code is the same between 1 and 2,

00:07:58.134 --> 00:08:00.930
and it's the layers API,
I think, makes it super simple

00:08:00.930 --> 00:08:02.761
for you to design in a neural network,

00:08:02.761 --> 00:08:05.057
and then the fact that you can go
low level beyond that--

00:08:05.057 --> 00:08:06.662
like define your own layers.

00:08:06.662 --> 00:08:09.501
It really allows you to drive stick
instead of driving automatic.

00:08:09.501 --> 00:08:14.339
Absolutely. One of the beauties
of Keras and 2.0 is that you have Keras

00:08:14.339 --> 00:08:16.558
the way that you're probably
familiar with using it,

00:08:16.558 --> 00:08:19.495
and then, if you need to do
additional customizations,

00:08:19.495 --> 00:08:21.817
there's a subclassing component.

00:08:21.817 --> 00:08:26.538
And then, if you need to go even lower,
then we have something called TF Module,

00:08:26.538 --> 00:08:31.297
and we even expose some of the basic,
most core ops of TensorFlow as well.

00:08:31.297 --> 00:08:33.522
So, at any sort of level

00:08:33.522 --> 00:08:35.778
you want to interact
with the API, you can.

00:08:35.778 --> 00:08:37.579
I think there was another part
of the question

00:08:37.579 --> 00:08:39.564
was around distributed training.

00:08:39.564 --> 00:08:41.453
Sorry, it scrolled off,
so I can't see it now.

00:08:41.453 --> 00:08:44.562
But there's something
called distributed strategy

00:08:45.352 --> 00:08:46.933
with Keras and TensorFlow 2,

00:08:46.933 --> 00:08:48.301
and the whole idea behind that

00:08:48.301 --> 00:08:50.801
is to allow you to be able
to distribute your training,

00:08:50.801 --> 00:08:53.194
maybe across multiple GPUs
on the same machine,

00:08:53.194 --> 00:08:55.241
maybe across multiple GPUs
on different machines,

00:08:55.241 --> 00:08:57.459
maybe across TPUs
spread all over the place,

00:08:57.459 --> 00:08:58.461
that kind of thing.

00:08:58.461 --> 00:09:00.727
So, distribution strategy
is really all about that--

00:09:00.727 --> 00:09:02.764
to help you with that.

00:09:02.764 --> 00:09:05.394
So, Estimators and Keras,
we love them both,

00:09:05.394 --> 00:09:06.617
they're both still there.

00:09:06.617 --> 00:09:09.945
Hopefully, this is something
that will help you with that question.

00:09:09.945 --> 00:09:12.086
I think we've got time for just one more.

00:09:12.086 --> 00:09:14.830
- Absolutely.
- Oh, this is a Paige question!

00:09:14.830 --> 00:09:16.479
This is totally a me question.

00:09:16.479 --> 00:09:17.802
I am the Python person.

00:09:17.802 --> 00:09:21.119
So, "Ask TensorFlow,
when will TensorFlow be supported

00:09:21.119 --> 00:09:26.675
in Python 3.7 and hence
be accessed in Anaconda 3?"

00:09:26.675 --> 00:09:30.610
So, I can certainly answer the Python 3.7,

00:09:30.610 --> 00:09:34.008
and also, I would love to speak
a little bit more about support

00:09:34.008 --> 00:09:35.664
for Python going forward.

00:09:36.074 --> 00:09:38.655
So, to answer the 3.7 question,

00:09:38.655 --> 00:09:42.780
I'm going to bounce over
to our TensorFlow 2.0 project tracker.

00:09:43.110 --> 00:09:45.008
These are all of the standing issues

00:09:45.008 --> 00:09:47.897
that we have when doing development 
for TensorFlow 2.0.

00:09:47.897 --> 00:09:49.935
- It's transparent--
- (Lawrence) I see your avatar.

00:09:49.935 --> 00:09:52.170
(Paige) Yes, I have filed many issues.

00:09:52.680 --> 00:09:55.372
And all of them
are transparent to the public.

00:09:55.372 --> 00:09:59.714
So, if you ever want to have context
on where we stand currently,

00:09:59.714 --> 00:10:01.559
and what we have yet to do,

00:10:01.559 --> 00:10:05.149
this project tracker is a great way
to understand that.

00:10:05.149 --> 00:10:07.380
But let's take a look at 3.7.

00:10:07.380 --> 00:10:08.702
And there we go.

00:10:08.702 --> 00:10:13.965
So, in process of releasing binaries
for Python 3.5 and 3.7.

00:10:13.965 --> 00:10:17.451
That's issued 25420--

00:10:17.451 --> 00:10:20.088
and it's going
a little bit off the screen--

00:10:20.088 --> 00:10:21.719
429.

00:10:21.719 --> 00:10:25.647
But you can take a look at that issue
and see that it's currently in progress.

00:10:25.647 --> 00:10:27.717
There's not really an ETA,

00:10:27.717 --> 00:10:29.875
but it's something
that we want to have complete

00:10:29.875 --> 00:10:32.275
by the time that the alpha
or [C] is released.

00:10:32.275 --> 00:10:34.984
So, that is wonderful to see.

00:10:34.984 --> 00:10:39.957
There's also a website
called Python 3 Statement.

00:10:39.957 --> 00:10:42.952
I think it's <i>python3statement.com</i>

00:10:42.952 --> 00:10:44.771
Maybe it's <i>.org</i>

00:10:45.931 --> 00:10:47.493
There we go, cool!

00:10:47.493 --> 00:10:51.838
So, TensorFlow has made the commitment
that as of January 1, 2020,

00:10:51.838 --> 00:10:54.152
we no longer support Python 2.

00:10:54.762 --> 00:10:59.946
And we have done that with a plethora
of our Python community.

00:10:59.946 --> 00:11:04.442
So, TensorFlow, pandas, scikit-learn, etc.

00:11:04.442 --> 00:11:08.399
We are firmly committed to Python 3
and Python 3 support.

00:11:08.399 --> 00:11:13.184
So, you will be getting
your Python 3 support,

00:11:13.184 --> 00:11:15.628
and we're firmly committed to having that.

00:11:15.628 --> 00:11:18.401
The nice thing about the issue tracker
is it's not going to be a big--

00:11:18.401 --> 00:11:19.635
"Hey, we have it!"--

00:11:19.635 --> 00:11:21.761
coming at some random point
in the future.

00:11:21.761 --> 00:11:23.590
It'll be a case of totally transparent,

00:11:23.590 --> 00:11:25.323
and you can keep an eye
on what we're doing.

00:11:25.323 --> 00:11:28.355
And you can see people commenting
and our engineers commenting back.

00:11:28.355 --> 00:11:30.772
Like, "Yeah, man,
I totally ran the thing last night,

00:11:30.772 --> 00:11:32.751
and it's almost there, one more test."

00:11:32.751 --> 00:11:34.671
(chuckles) Sounds good.

00:11:34.671 --> 00:11:36.481
Okay, I think that's all we have time for.

00:11:36.481 --> 00:11:39.004
So, whatever you do, don't forget
to hit that subscribe button.

00:11:39.004 --> 00:11:41.419
Alright, thank you so much,
and thanks for being engaged.

00:11:41.419 --> 00:11:42.646
Thank you.

00:11:42.646 --> 00:11:44.772
♪ (music) ♪

