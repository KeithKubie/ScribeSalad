WEBVTT
Kind: captions
Language: en

00:00:09.250 --> 00:00:11.560
Our discussion about the
future of the workforce

00:00:11.560 --> 00:00:14.670
would not be complete without
an exploration of what impact

00:00:14.670 --> 00:00:17.060
the current innovations
in technology will have,

00:00:17.060 --> 00:00:19.780
specifically,
artificial intelligence.

00:00:19.780 --> 00:00:21.840
Over the past three
years, there have

00:00:21.840 --> 00:00:24.150
been significant
innovations in big data,

00:00:24.150 --> 00:00:27.270
database architecture, and
artificial intelligence,

00:00:27.270 --> 00:00:30.730
that are enabling new
business models and products.

00:00:30.730 --> 00:00:33.070
In simplest terms,
the innovations

00:00:33.070 --> 00:00:35.900
in artificial intelligence
are equipping algorithms

00:00:35.900 --> 00:00:39.670
to make smarter decisions
about tasks and problems

00:00:39.670 --> 00:00:43.520
that have, so far, been thought
only to be reserved for humans.

00:00:43.520 --> 00:00:47.230
The accompanying innovations,
and cheapening of hardware,

00:00:47.230 --> 00:00:50.310
that enables artificial
intelligence algorithms to read

00:00:50.310 --> 00:00:52.910
and process incredibly
large data sets,

00:00:52.910 --> 00:00:55.550
has motivated more
entrepreneurs,

00:00:55.550 --> 00:00:58.830
and technologists, to innovate
with artificial intelligence.

00:00:58.830 --> 00:01:01.570
Many think that the
implications for the workplace

00:01:01.570 --> 00:01:03.010
will hurt workers.

00:01:03.010 --> 00:01:05.630
Similar to how the Industrial
Revolution displaced

00:01:05.630 --> 00:01:07.780
many workers through
the creation of tech

00:01:07.780 --> 00:01:11.180
that was cheaper and overall
more efficient than people.

00:01:11.180 --> 00:01:14.250
People believe that
AI will replace, not

00:01:14.250 --> 00:01:17.700
just blue collar workers, but
also white collar workers.

00:01:17.700 --> 00:01:21.190
However, the fact is that
the impact on workers

00:01:21.190 --> 00:01:24.470
is under our control, the
control of entrepreneurs,

00:01:24.470 --> 00:01:26.990
technologists, and
everyday citizens,

00:01:26.990 --> 00:01:28.880
that are part of the workforce.

00:01:28.880 --> 00:01:31.440
Business owners and
workers can harness

00:01:31.440 --> 00:01:34.670
AI to augment human
intelligence in the workplace

00:01:34.670 --> 00:01:39.630
to creatively improve decision
making instead of replacing it.

00:01:39.630 --> 00:01:44.290
Here's Rob High, the CTO of IBM
Watson, one of the primary R&amp;D

00:01:44.290 --> 00:01:48.930
leaders in the AI space, making
this exact point at the 2016

00:01:48.930 --> 00:01:52.220
MIT Technology Conference.

00:01:52.220 --> 00:01:54.210
If we become experts
in everything,

00:01:54.210 --> 00:01:55.540
we're experts and nothing.

00:01:55.540 --> 00:01:57.170
And so the same
phenomenon occurs

00:01:57.170 --> 00:01:58.860
within these cognitive
systems as well.

00:01:58.860 --> 00:02:01.230
They can actually begin to
get a little bit confused

00:02:01.230 --> 00:02:03.340
in how to ration--

00:02:03.340 --> 00:02:04.740
this would be a good question.

00:02:04.740 --> 00:02:06.840
So a couple points to
be made about that.

00:02:06.840 --> 00:02:10.150
That said, because each
of these are distinct,

00:02:10.150 --> 00:02:12.210
we can actually
set the system up

00:02:12.210 --> 00:02:15.397
based on whatever is appropriate
for the application we're

00:02:15.397 --> 00:02:16.230
trying to solve for.

00:02:16.230 --> 00:02:19.470
For the case of oncology
treatment advice,

00:02:19.470 --> 00:02:21.390
the goal is actually
about trying

00:02:21.390 --> 00:02:24.510
to identify the best treatment
based on outcomes, based

00:02:24.510 --> 00:02:27.691
on standard of care practices
and clinical expertise,

00:02:27.691 --> 00:02:29.940
based on similarity of this
patient to other patients.

00:02:29.940 --> 00:02:31.900
And all those can
contribute to help

00:02:31.900 --> 00:02:35.560
Watson come back with an ordered
list of potential treatments.

00:02:35.560 --> 00:02:37.560
But again, we're
not trying to make

00:02:37.560 --> 00:02:38.770
the decision for the human.

00:02:38.770 --> 00:02:40.616
We're not trying to
think for the human.

00:02:40.616 --> 00:02:42.990
What we're trying to do is do
the research for the human,

00:02:42.990 --> 00:02:45.410
so the human can then go
make the decision better.

00:02:45.410 --> 00:02:46.880
In this case the doctor.

00:02:46.880 --> 00:02:48.930
So we're not going take--

00:02:48.930 --> 00:02:51.180
Watson doesn't make the
decision about what treatment

00:02:51.180 --> 00:02:52.850
to give for the doctor.

00:02:52.850 --> 00:02:55.840
It presents the doctor a set
of relevant treatments that

00:02:55.840 --> 00:02:59.250
has been rationalized based
on all this other information,

00:02:59.250 --> 00:03:01.910
at a speed that a doctor
could not do on their own,

00:03:01.910 --> 00:03:03.870
including finding all
the literature that's

00:03:03.870 --> 00:03:05.661
relevant to support
why they should believe

00:03:05.661 --> 00:03:11.720
in that treatment or not.

00:03:11.720 --> 00:03:14.550
So what specific
type of disruptions

00:03:14.550 --> 00:03:18.060
can we expect AI to cause
for employers and employees?

00:03:18.060 --> 00:03:21.470
Let's take a look at what David
Autor, a professor of economics

00:03:21.470 --> 00:03:23.900
at MIT, said in the
Future of Work panel

00:03:23.900 --> 00:03:26.120
at the Nobel Week
Dialogue of 2015.

00:03:26.120 --> 00:03:30.310
In terms of employment,
as was said earlier,

00:03:30.310 --> 00:03:32.030
people have been
worried for centuries

00:03:32.030 --> 00:03:33.610
about displacement of labor.

00:03:33.610 --> 00:03:35.380
And labor has been
vastly displaced.

00:03:35.380 --> 00:03:38.610
In the start of the 20th
century, 40% of US employment

00:03:38.610 --> 00:03:40.030
was on farms.

00:03:40.030 --> 00:03:41.280
Now it's under 2%.

00:03:41.280 --> 00:03:43.740
We had no idea what was coming.

00:03:43.740 --> 00:03:44.710
But it came.

00:03:44.710 --> 00:03:46.620
We find uses for ourselves.

00:03:46.620 --> 00:03:49.870
And that's partly because
the technology augments us,

00:03:49.870 --> 00:03:51.480
it doesn't just replace us.

00:03:51.480 --> 00:03:54.380
It makes the rest of
what we do more valuable.

00:03:54.380 --> 00:03:57.390
So, you know, I can make
more as an Uber driver

00:03:57.390 --> 00:03:59.340
than as a rickshaw
driver, not just

00:03:59.340 --> 00:04:02.970
because I'm in a rich world,
but because I have a tool that

00:04:02.970 --> 00:04:05.096
makes me able to transport
people much faster

00:04:05.096 --> 00:04:07.220
and further, and more
safely, and more comfortably,

00:04:07.220 --> 00:04:08.470
in a given amount of time.

00:04:08.470 --> 00:04:10.990
In many, many ways,
the tools we make

00:04:10.990 --> 00:04:12.322
make our time more valuable.

00:04:12.322 --> 00:04:14.280
Because there's always
some piece that we still

00:04:14.280 --> 00:04:17.760
have to supply, and because
that becomes a scarce factor

00:04:17.760 --> 00:04:19.640
that raises our labor value.

00:04:19.640 --> 00:04:21.089
So there's a challenge--

00:04:21.089 --> 00:04:23.410
So now let me say, but
let's say I'm wrong,

00:04:23.410 --> 00:04:25.897
and that we're going to
be all replaced by robots.

00:04:25.897 --> 00:04:27.480
That they can do all
the work we want.

00:04:27.480 --> 00:04:28.960
Is that a problem?

00:04:28.960 --> 00:04:31.330
Well, if it's a problem,
it's a very unusual

00:04:31.330 --> 00:04:32.250
historical problem.

00:04:32.250 --> 00:04:34.870
Most problems, economic
problems, societal problems,

00:04:34.870 --> 00:04:36.710
are problems of
scarcity, of not having

00:04:36.710 --> 00:04:39.196
enough of something,
enough food, enough power,

00:04:39.196 --> 00:04:40.445
enough safety, enough shelter.

00:04:45.020 --> 00:04:47.660
In addition to changing
the kinds of roles

00:04:47.660 --> 00:04:49.900
that workers can take
on in the workplace,

00:04:49.900 --> 00:04:53.270
AI-based innovation can also
impact social inequality,

00:04:53.270 --> 00:04:55.410
for better or worse,
to be determined

00:04:55.410 --> 00:04:57.120
by how we choose to behave.

00:04:57.120 --> 00:05:00.030
For example, women
and people of color

00:05:00.030 --> 00:05:03.490
are very under-represented among
technology entrepreneurs that

00:05:03.490 --> 00:05:05.310
receive funding from
venture capitalists

00:05:05.310 --> 00:05:06.960
for their enterprises.

00:05:06.960 --> 00:05:10.420
If venture capital investment
goes towards a more diverse set

00:05:10.420 --> 00:05:13.310
of entrepreneurs, then we
will see social inequality

00:05:13.310 --> 00:05:14.750
improve positively.

00:05:14.750 --> 00:05:18.060
Here's Amanda Kahlow,
CEO of 6Sense,

00:05:18.060 --> 00:05:20.520
a predictive intelligence
platform for marketing

00:05:20.520 --> 00:05:23.720
and sales, expanding on the
current under-representation,

00:05:23.720 --> 00:05:25.970
based on her experiences.

00:05:25.970 --> 00:05:28.300
The short answer is
yes, I feel supported.

00:05:28.300 --> 00:05:32.310
But I do feel-- so I go
to a lot of CEO events

00:05:32.310 --> 00:05:33.977
and sit on a panel like this.

00:05:33.977 --> 00:05:36.060
The last, I was in Hawaii
just a little while ago.

00:05:36.060 --> 00:05:38.460
250 CEOs, I was the only woman.

00:05:38.460 --> 00:05:41.750
And in enterprise tech, I'm
less than 1% in this world.

00:05:41.750 --> 00:05:42.800
And it's crazy.

00:05:42.800 --> 00:05:44.327
And it doesn't feel right.

00:05:44.327 --> 00:05:45.910
And I think it's
because we're missing

00:05:45.910 --> 00:05:48.243
that confidence, and the
passion to go forward, and want

00:05:48.243 --> 00:05:49.560
to dream big.

00:05:49.560 --> 00:05:50.896
Do I feel supported by the men?

00:05:50.896 --> 00:05:52.020
I think yes, I'm supported.

00:05:52.020 --> 00:05:54.260
I think pretty much,
I feel very supported.

00:05:54.260 --> 00:05:57.170
Especially my male co-founders
are ridiculously supportive.

00:05:57.170 --> 00:05:58.670
And actually, I
think that they have

00:05:58.670 --> 00:06:00.970
more faith and confidence
in me than I do myself,

00:06:00.970 --> 00:06:02.010
which is amazing.

00:06:02.010 --> 00:06:05.570
I think I have had
to, maybe, guide

00:06:05.570 --> 00:06:07.630
and teach some of my
VCs and other people

00:06:07.630 --> 00:06:09.340
in the world what's important.

00:06:09.340 --> 00:06:11.195
And it's not always just about--

00:06:11.195 --> 00:06:13.070
I think we were talking
earlier about the end

00:06:13.070 --> 00:06:15.070
goal of the business, and
growing the business--

00:06:15.070 --> 00:06:16.720
but creating a
sustainable company

00:06:16.720 --> 00:06:18.550
is also about the
culture and the people.

00:06:18.550 --> 00:06:22.330
And I think that what we're
missing in today's workforce

00:06:22.330 --> 00:06:24.380
is putting an emphasis on that.

00:06:24.380 --> 00:06:27.280
In addition to all of the great
technologies that we can do.

00:06:27.280 --> 00:06:30.090
And that's the thing that
the women bring to the table.

00:06:30.090 --> 00:06:33.060
I'm super proud of our lack
of attrition rate at 6Sense.

00:06:33.060 --> 00:06:34.560
We don't have people
leave, and it's

00:06:34.560 --> 00:06:36.900
because they feel
supported and heard,

00:06:36.900 --> 00:06:39.540
and they can also be creative
and do amazing things

00:06:39.540 --> 00:06:41.640
in the work that
we're providing.

00:06:41.640 --> 00:06:45.435
But I think it's not intentional
when men aren't supporting.

00:06:45.435 --> 00:06:46.810
I was actually
sitting on a plane

00:06:46.810 --> 00:06:49.245
the other day talking
to a gentleman.

00:06:49.245 --> 00:06:50.370
And he asked me what I did.

00:06:50.370 --> 00:06:52.640
And I never come out and say,
I'm the CEO of this company.

00:06:52.640 --> 00:06:54.110
I say I work for a
software company.

00:06:54.110 --> 00:06:56.380
And I started talking about
the stuff that we're doing.

00:06:56.380 --> 00:06:58.190
And then two hours into the
conversation, he's like, well,

00:06:58.190 --> 00:06:58.900
what do you do there?

00:06:58.900 --> 00:06:59.910
And he's like, are
you in marketing?

00:06:59.910 --> 00:07:01.370
And I was like, no, I'm the CEO.

00:07:01.370 --> 00:07:04.794
And he got up from his seat, and
he had this visceral reaction.

00:07:04.794 --> 00:07:05.710
And he didn't mean to.

00:07:05.710 --> 00:07:06.310
And then he sat down.

00:07:06.310 --> 00:07:08.726
He's like, I'm so sorry, I
didn't mean to react like that.

00:07:08.726 --> 00:07:10.300
And [? he ?] didn't intend to--

00:07:10.300 --> 00:07:11.010
[LAUGHTER]

00:07:11.010 --> 00:07:15.990
It was just a lack of filter
which we all have sometimes.

00:07:15.990 --> 00:07:18.990
When he was realizing who I
was in the company-- and I

00:07:18.990 --> 00:07:21.080
wear braids and pigtails,
and I don't always

00:07:21.080 --> 00:07:23.440
dress the part of
something else.

00:07:23.440 --> 00:07:25.700
I always struggle with being
my authentic woman self.

00:07:25.700 --> 00:07:29.760
I want to maintain that while
dominating in the male world

00:07:29.760 --> 00:07:30.260
that I'm in.

00:07:30.260 --> 00:07:32.750
So I think there are
levels of education

00:07:32.750 --> 00:07:33.750
that we can give others.

00:07:33.750 --> 00:07:35.070
Thank you.

00:07:35.070 --> 00:07:37.620
Income and social
inequality between genders

00:07:37.620 --> 00:07:39.380
will not improve
unless we choose

00:07:39.380 --> 00:07:41.620
to make fairer
decisions about who

00:07:41.620 --> 00:07:43.420
gets opportunities
to create and run

00:07:43.420 --> 00:07:45.620
companies in the tech industry.

00:07:45.620 --> 00:07:48.860
If we do not actively
monitor our biases,

00:07:48.860 --> 00:07:50.880
we will not be able
to take advantage

00:07:50.880 --> 00:07:53.500
of this period of innovation
and wealth creation

00:07:53.500 --> 00:07:55.460
towards a more equitable future.

00:07:55.460 --> 00:07:57.880
There is also an
opportunity gap among people

00:07:57.880 --> 00:08:01.330
for who gets to work within
companies in the tech industry.

00:08:01.330 --> 00:08:05.820
Leila Janah, founder and CEO
of Sama Group, an organization

00:08:05.820 --> 00:08:09.350
that lifts people out of poverty
by enabling them to do work

00:08:09.350 --> 00:08:12.690
in the digital economy,
discusses this opportunity gap,

00:08:12.690 --> 00:08:14.680
and how we can mitigate it.

00:08:14.680 --> 00:08:17.400
Now interestingly,
traditionally,

00:08:17.400 --> 00:08:19.580
in the informal sector,
if you're doing,

00:08:19.580 --> 00:08:21.950
say construction
work, and you go

00:08:21.950 --> 00:08:23.590
and you find your
job by standing

00:08:23.590 --> 00:08:26.060
on the side of the road and
waiting for someone to hire you

00:08:26.060 --> 00:08:29.070
as a labor contractor,
there is no such thing

00:08:29.070 --> 00:08:31.120
as getting rewarded
for good work.

00:08:31.120 --> 00:08:33.760
You might put in a
wonderful 10 hour shift,

00:08:33.760 --> 00:08:36.809
but you will get no five
star rating on a website.

00:08:36.809 --> 00:08:38.542
You go back to
zero the next day,

00:08:38.542 --> 00:08:41.000
standing on the side of the
road waiting for your next job.

00:08:41.000 --> 00:08:44.010
So I think that
technology actually

00:08:44.010 --> 00:08:47.950
brings a lot more transparency
in areas like wages,

00:08:47.950 --> 00:08:51.060
and reviews, and feedback, that
are very good for low income

00:08:51.060 --> 00:08:51.910
workers.

00:08:51.910 --> 00:08:53.610
And I'll say one
last thing on this,

00:08:53.610 --> 00:08:56.200
which is that, David and I
were talking about this just

00:08:56.200 --> 00:08:58.720
before the panel,
we so often assume

00:08:58.720 --> 00:09:03.290
that technology has some kind
of morality built into it.

00:09:03.290 --> 00:09:05.420
Technology, in
fact, is agnostic.

00:09:05.420 --> 00:09:08.070
It's as agnostic as
roads, and waterways.

00:09:08.070 --> 00:09:10.340
It's all about what
we do with technology.

00:09:10.340 --> 00:09:14.030
We can choose to inject morality
into the technology we build.

00:09:14.030 --> 00:09:18.020
We can choose to build systems
that enfranchise the poor.

00:09:18.020 --> 00:09:19.570
Or we can choose not to.

00:09:19.570 --> 00:09:22.610
But we're not victims of
technological progress.

00:09:22.610 --> 00:09:26.170
It's not happening in a
way that we can't control.

00:09:26.170 --> 00:09:28.510
At least until the singularity
happens, at which point

00:09:28.510 --> 00:09:29.820
all bets are off.

00:09:29.820 --> 00:09:30.700
[LAUGHTER]

00:09:30.700 --> 00:09:33.680
The first is, job training
needs to completely change.

00:09:33.680 --> 00:09:37.880
In the US, which is the best
example I know, it's abysmal.

00:09:37.880 --> 00:09:41.350
We are still training people
for jobs that are disappearing

00:09:41.350 --> 00:09:42.670
at an alarming rate.

00:09:42.670 --> 00:09:45.289
We are not training them in
how to be entrepreneurial

00:09:45.289 --> 00:09:46.330
and marketing themselves.

00:09:46.330 --> 00:09:48.770
The skill that you need
for 21st century jobs

00:09:48.770 --> 00:09:50.220
is marketing yourself.

00:09:50.220 --> 00:09:52.620
If Kim Kardashian can
teach us anything,

00:09:52.620 --> 00:09:54.600
it's probably only that.

00:09:54.600 --> 00:09:57.070
But interestingly, all
of these new platforms

00:09:57.070 --> 00:09:59.250
require you to be able
to set up a profile,

00:09:59.250 --> 00:10:01.690
to choose the right kind of a
photograph that will attract

00:10:01.690 --> 00:10:03.687
customers, to send
a follow up email,

00:10:03.687 --> 00:10:05.020
to have customer service skills.

00:10:05.020 --> 00:10:08.100
And you're absolutely right,
that many low income people--

00:10:08.100 --> 00:10:11.130
And just to give you an idea of
how profound this is, in the US

00:10:11.130 --> 00:10:13.100
we have one of our
centers which is

00:10:13.100 --> 00:10:15.590
less than a mile from
the Facebook headquarters

00:10:15.590 --> 00:10:17.790
in Silicon Valley,
in East Palo Alto.

00:10:17.790 --> 00:10:19.430
Which is a very poor community.

00:10:19.430 --> 00:10:24.080
And fully 25% of our incoming
class for one of our trainings

00:10:24.080 --> 00:10:28.250
last year had people who had
zero internet access at home,

00:10:28.250 --> 00:10:30.420
and no smartphone.

00:10:30.420 --> 00:10:32.860
A mile from the Facebook campus.

00:10:32.860 --> 00:10:35.200
So we can talk all we want
about how technology is going

00:10:35.200 --> 00:10:37.100
to make everyone's lives
better, but the reality

00:10:37.100 --> 00:10:38.710
is that many people
are disconnected.

00:10:38.710 --> 00:10:41.600
And if you didn't grow up having
the internet at home, how can

00:10:41.600 --> 00:10:44.740
you possibly understand
how to market yourself?

00:10:44.740 --> 00:10:47.310
It's not easy to
be an autodidact,

00:10:47.310 --> 00:10:49.840
and to learn how to teach
yourself things on YouTube,

00:10:49.840 --> 00:10:51.506
if you didn't grow
up with the internet.

00:10:55.470 --> 00:10:57.610
Advancements in technology
taking place right

00:10:57.610 --> 00:10:59.390
now are incredibly exciting.

00:10:59.390 --> 00:11:02.160
And we all stand to gain
from them as consumers.

00:11:02.160 --> 00:11:05.680
However, as with other
periods of societal change,

00:11:05.680 --> 00:11:08.350
we, citizens and
the government, have

00:11:08.350 --> 00:11:11.490
to ensure that the opportunities
to launch, run, and work,

00:11:11.490 --> 00:11:14.610
in these new enterprises is
made available to all members

00:11:14.610 --> 00:11:16.420
of society.

