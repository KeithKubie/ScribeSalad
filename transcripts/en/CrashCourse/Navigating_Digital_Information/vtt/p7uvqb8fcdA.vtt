WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:03.939
Hi I’m John Green, and this is Crash Course:
Navigating Digital Information.

00:00:03.939 --> 00:00:07.700
So, images are incredibly powerful to human
brains.

00:00:07.700 --> 00:00:12.280
Like, I read and loved the first four Harry
Potter books before seeing a Harry Potter

00:00:12.280 --> 00:00:13.280
movie.

00:00:13.280 --> 00:00:17.760
And I really liked the movie, but after watching
it, I could never see my Harry Potter or Hermione

00:00:17.760 --> 00:00:21.189
ever again--I saw only Daniel Radcliffe and
Emma Watson.

00:00:21.189 --> 00:00:24.460
And also I learned that Hermione is pronounced
Hermione.

00:00:24.460 --> 00:00:25.920
And not Her-mee-own.

00:00:25.920 --> 00:00:29.949
They say a picture is worth a thousand words
-- and by “they” I mean the advertiser

00:00:29.949 --> 00:00:33.130
who supposedly coined that idiom in the 1940s.

00:00:33.130 --> 00:00:38.790
Photographs in particular feel real and objective
to us, because they seem to capture a moment

00:00:38.790 --> 00:00:39.790
of reality.

00:00:39.790 --> 00:00:44.670
More than 150 years ago, Matthew Brady’s
iconic Civil War photographs were often staged,

00:00:44.670 --> 00:00:50.300
for instance, his assistants would move corpses
and change their postures to maximize the

00:00:50.300 --> 00:00:51.300
images’ visual power.

00:00:51.300 --> 00:00:55.020
But while images have never been as reliable
as they seem, this is especially true in the

00:00:55.020 --> 00:00:56.330
era of photoshop.

00:00:56.330 --> 00:00:58.770
In fact, consider the image you’re looking
at right now.

00:00:58.770 --> 00:01:00.820
That flower is not actually here.

00:01:00.820 --> 00:01:05.680
If you spend as much time online as I do,
you spend a lot of it looking at images.

00:01:05.680 --> 00:01:10.940
Sometimes those images are unedited, although
even then choices are made--how to frame the

00:01:10.940 --> 00:01:14.110
image, what to photograph, when and how to
share it.

00:01:14.110 --> 00:01:19.330
Other times, the images are obviously altered
with bunny ear filters or meme text.

00:01:19.330 --> 00:01:22.940
Sometimes images are altered in ways meant
to fool us.

00:01:22.940 --> 00:01:25.350
So how can we decipher what’s real and what’s
not?

00:01:25.350 --> 00:01:26.430
Well It’s easy!

00:01:26.430 --> 00:01:28.170
You can tell by looking at the pixels.

00:01:28.170 --> 00:01:31.470
Meredith says that meme is so old that nobody
is going to get the joke.

00:01:31.470 --> 00:01:31.960
OK.

00:01:31.960 --> 00:01:32.940
Roll the intro.

00:01:32.940 --> 00:01:41.920
[intro]

00:01:41.930 --> 00:01:45.940
So far during this series we’ve talked about
how important it is to find out who’s behind

00:01:45.950 --> 00:01:51.520
information we learn online, why they’re
posting it, and whether the evidence is reliable.

00:01:51.520 --> 00:01:56.260
And thanks to their power, images are a very
common form of online evidence.

00:01:56.260 --> 00:02:02.210
But just like data or text, image-based evidence
can be relevant and reliable or irrelevant

00:02:02.210 --> 00:02:03.380
and unreliable.

00:02:03.380 --> 00:02:08.259
In order to make sense of our online surroundings
it is critical to think carefully about whether

00:02:08.259 --> 00:02:13.720
image-based evidence is trustworthy because
we’re used to thinking that “seeing is

00:02:13.720 --> 00:02:14.720
believing.”

00:02:14.720 --> 00:02:18.790
I means, special effects-laden movies are
popular in part because they are so visually

00:02:18.790 --> 00:02:24.390
thrilling--even though we know they aren’t
real, they look real, or at least adjacent

00:02:24.390 --> 00:02:25.390
to real.

00:02:25.390 --> 00:02:30.170
That is why, for instance, I found all five
transformers films completely watchable despite

00:02:30.170 --> 00:02:36.780
their lack of … you know, plot, character
and comprehensible worldbuilding and etc.

00:02:36.780 --> 00:02:38.210
They also have that Shia LeBouef in them.

00:02:38.210 --> 00:02:39.860
He’s a fascinating character.

00:02:39.860 --> 00:02:40.800
Don’t do it Stan.

00:02:40.800 --> 00:02:42.640
DON’T. Oh.

00:02:42.760 --> 00:02:43.760
Hello Shia.

00:02:43.770 --> 00:02:48.861
So, in movies, filmmakers depend partly on
our ability to get lost in images--when we

00:02:48.861 --> 00:02:52.860
watch a conversation between two people in
a film, for instance, we rarely consider that

00:02:52.860 --> 00:02:57.950
forty-five minutes elapsed between this shot
and this one, because the camera and lights

00:02:57.950 --> 00:02:58.970
had to be moved.

00:02:58.970 --> 00:03:03.550
The willingness of the human brain to assume
that images are real is consistently manipulated

00:03:03.550 --> 00:03:06.099
by filmmakers, but also by other people.

00:03:06.099 --> 00:03:10.700
Consider, for instance, this manipulated picture
of mass shooting survivor and activist Emma

00:03:10.700 --> 00:03:11.700
Gonzalez.

00:03:11.700 --> 00:03:16.080
It’s doctored to make her look like she
was tearing up the U.S. Constitution instead

00:03:16.080 --> 00:03:19.130
of the real picture she took with a gun-range
target.

00:03:19.130 --> 00:03:23.180
Or this one of President Trump supporters
whose shirts were digitally altered to read

00:03:23.180 --> 00:03:27.720
“Make America White Again” instead of
their actual “Make America Great Again”

00:03:27.720 --> 00:03:28.220
shirts.

00:03:28.220 --> 00:03:30.680
But images don’t have to be altered to fools
us, though.

00:03:30.680 --> 00:03:35.550
Sometimes bad actors use real, untouched photos
but falsify their context.

00:03:35.550 --> 00:03:38.010
And that can have really serious consequences.

00:03:38.010 --> 00:03:43.430
For instance, this image of an election in
Mexico in 2017 circulated online as a meme

00:03:43.430 --> 00:03:48.800
claiming undocumented immigrants were voting
in the nonexistent town of Battsville, Arizona.

00:03:48.800 --> 00:03:52.920
Or this image of children sleeping in what
looks like a cage at a detention facility

00:03:52.920 --> 00:03:55.560
for undocumented children in 2014.

00:03:55.560 --> 00:04:01.750
It was circulated widely in 2018 as controversy
grew over policies for separating undocumented

00:04:01.750 --> 00:04:04.370
migrant children and parents at the U.S. border.

00:04:04.370 --> 00:04:08.319
Although the conditions were similar for many
of the children being held in 2018, when the

00:04:08.319 --> 00:04:13.740
photo went viral it was unaccompanied by its
original context: the date.

00:04:13.740 --> 00:04:17.849
And then once this mistake was revealed, it
was used by many to dismiss the entire controversy

00:04:17.849 --> 00:04:19.099
as “fake news.”

00:04:19.099 --> 00:04:23.659
A study by the Stanford History Education
Group has shown just how easy it can be for

00:04:23.659 --> 00:04:27.309
people to let images and their context go
unchallenged.

00:04:27.309 --> 00:04:31.409
So, as you know from previous episodes, the
Stanford History Education Group is affiliated

00:04:31.409 --> 00:04:32.509
with this series.

00:04:32.509 --> 00:04:35.479
They developed MediaWise, which is what this
series is based on.

00:04:35.479 --> 00:04:38.979
Anyway, during the Stanford History Education
Group study, they showed 170 high school students

00:04:38.979 --> 00:04:42.490
a photo from Imgur of these weird looking
flowers.

00:04:42.490 --> 00:04:46.740
The photo’s caption claimed that the flowers
had “nuclear birth defects.”

00:04:46.740 --> 00:04:51.699
Fukushima was in the photo title, implying
they were from the Fukushima nuclear disaster

00:04:51.699 --> 00:04:52.789
in Japan.

00:04:52.789 --> 00:04:58.110
Despite no evidence that the photo actually
showed these effects, or that radiation caused

00:04:58.110 --> 00:05:02.669
the mutations, over 80 percent of the students
did not question the source of the photo.

00:05:02.669 --> 00:05:06.819
There wasn’t even any evidence to show the
photo was taken in Japan!

00:05:06.819 --> 00:05:13.240
In reality, these daisies are most likely
the victims of a genetic mutation called “fascination”

00:05:13.240 --> 00:05:16.669
that isn’t related to nuclear radiation
in any way.

00:05:16.669 --> 00:05:19.849
Bottom line: nature is really wild all by
herself.

00:05:19.849 --> 00:05:23.840
I mean, do I need to bring back the picture
of the star-nosed mole?

00:05:23.840 --> 00:05:25.219
I do.

00:05:25.219 --> 00:05:29.530
Because it’s so easy to turn images into
manipulation machines, when you encounter

00:05:29.530 --> 00:05:35.210
a suspicious image online, it’s crucial
to investigate who is behind it and whether

00:05:35.210 --> 00:05:36.699
they are a reliable source.

00:05:36.699 --> 00:05:40.919
We also must look for context, to be sure
an image supports the claim being made.

00:05:40.920 --> 00:05:44.760
Does the story, blog, or social media post
where you encountered the image provide a

00:05:44.760 --> 00:05:45.260
link?

00:05:45.400 --> 00:05:45.900
Great!

00:05:46.040 --> 00:05:46.540
Click it.

00:05:46.600 --> 00:05:50.260
If you can get a reliable explanation of that
photo and where it came from.

00:05:50.260 --> 00:05:52.960
That can help you know if the image is reliable.

00:05:52.960 --> 00:05:54.369
Is a caption provided?

00:05:54.369 --> 00:05:58.350
Use your lateral reading skills to determine
whether the context surrounding the image

00:05:58.350 --> 00:05:59.350
is accurate.

00:05:59.350 --> 00:06:03.849
But if the source sharing the photo doesn’t
provide any context, or they provide a caption,

00:06:03.849 --> 00:06:09.120
but no other reason to find that information
credible, then maybe you can’t trust it.

00:06:09.120 --> 00:06:14.389
But, there are online tools you can use to
hunt down an image’s origin story.

00:06:14.389 --> 00:06:15.849
Let’s go to the Thought Bubble.

00:06:15.849 --> 00:06:20.080
OK, so it’s raining hard in your hometown
and you just got one of those startling flash

00:06:20.080 --> 00:06:21.770
flood warnings on your phone.

00:06:21.770 --> 00:06:26.479
So you hop online to find the latest weather
report and a friend has reposted this in your

00:06:26.479 --> 00:06:27.599
news feed.

00:06:27.599 --> 00:06:28.689
Just saw this on the highway.

00:06:28.689 --> 00:06:30.039
Be careful out there, friends.

00:06:30.039 --> 00:06:34.610
Oh my god, there’s a shark swimming around
the floodwaters in your town.

00:06:34.610 --> 00:06:37.599
That’s certainly terrifying -- if it’s
true.

00:06:37.599 --> 00:06:40.619
Before sharing it with anyone else you want
to be sure that it is.

00:06:40.619 --> 00:06:45.370
Your friend hasn’t provided any other context
or tagged the photo’s location or anything.

00:06:45.370 --> 00:06:47.970
She hasn’t said whether she took it or someone
else did,

00:06:47.970 --> 00:06:50.039
and isn’t responding to your texts.

00:06:50.039 --> 00:06:52.629
So it’s time to do a Google reverse image
search.

00:06:52.629 --> 00:06:56.150
Quick reminder: Google is one of our sponsors
for this series, but we also think they have

00:06:56.150 --> 00:06:58.509
the strongest reverse image search engine.

00:06:58.509 --> 00:07:02.009
If you’re looking for an alternative, TinEye
is another popular one.

00:07:02.009 --> 00:07:05.089
Right, so, if you’re using their Chrome
browser, you can right click on an image and

00:07:05.089 --> 00:07:07.050
select “Search Google for image.”

00:07:07.050 --> 00:07:10.770
If you’re using a different browser, you
can right click on an image and copy its URL.

00:07:10.770 --> 00:07:14.789
Then you paste the URL into the search window
at images.google.com.

00:07:14.789 --> 00:07:19.679
Whoa there -- the search results for this
shark photo are full of fact-checking sites

00:07:19.679 --> 00:07:22.369
saying that this photo is a viral hoax.

00:07:22.369 --> 00:07:26.460
It seems this photoshopped image makes the
rounds every time there is a hurricane or

00:07:26.460 --> 00:07:27.460
huge flood.

00:07:27.460 --> 00:07:31.919
The shark has been “spotted” in Puerto
Rico during Hurricane Irene, Florida during

00:07:31.919 --> 00:07:36.949
Hurricane Irma, in Texas during Hurricane
Harvey, New Jersey during Hurricane Sandy,

00:07:36.949 --> 00:07:40.189
and in North Carolina during floods in 2015.

00:07:40.189 --> 00:07:41.189
What a shark!

00:07:41.189 --> 00:07:45.389
The original photo of this shark was captured
in its natural habitat, off the coast of South

00:07:45.389 --> 00:07:46.389
Africa.

00:07:46.389 --> 00:07:48.930
But after someone photoshopped it into a highway
setting,

00:07:48.930 --> 00:07:53.399
plenty of social media posts have cited the
image as “evidence” over the years.

00:07:53.399 --> 00:07:54.669
Thanks, Thought Bubble.

00:07:54.669 --> 00:07:59.240
You can use reverse image searches to check
in on all kinds of photos.

00:07:59.240 --> 00:08:03.339
Using what you know about finding reliable
sources, you can then track down whether an

00:08:03.339 --> 00:08:08.039
image has originated with a trustworthy source
or whether it’s only been distributed on

00:08:08.039 --> 00:08:09.289
unreliable sites.

00:08:09.289 --> 00:08:13.340
And you can turn to fact checking organizations
like Snopes and Politifact which are really

00:08:13.340 --> 00:08:14.830
great at hunting down these hoaxes.

00:08:14.830 --> 00:08:19.929
And then there’s videos, which can be just
as powerful as images when it comes to providing

00:08:19.929 --> 00:08:20.929
evidence.

00:08:20.929 --> 00:08:23.460
Unfortunately, they can also be used to mislead.

00:08:23.460 --> 00:08:28.099
For instance, a carefully edited clip can
misrepresent how an event actually happened

00:08:28.099 --> 00:08:30.229
or what someone actually said.

00:08:30.229 --> 00:08:34.830
At least according to every villain on every
reality TV show ever, that’s the entire

00:08:34.830 --> 00:08:36.910
genre of reality TV.

00:08:36.910 --> 00:08:41.010
It was just the /editing/ that made it /look/
like you were awkwardly breaking up with your

00:08:41.010 --> 00:08:43.440
fiancée on national television, Arie.

00:08:43.440 --> 00:08:48.640
But also, unedited videos can be posted alongside
inaccurate information that claims footage

00:08:48.640 --> 00:08:52.950
depicts one event when it really shows something
completely different.

00:08:52.950 --> 00:08:57.410
Like this clip of me saying “I have messed
it up a lot in the past, hence, part of my

00:08:57.410 --> 00:08:58.750
aforementioned nervousness.”

00:08:58.750 --> 00:09:02.960
Now as it happens, that was about communicating
news to fans about my books being adapted

00:09:02.960 --> 00:09:03.960
into movies.

00:09:03.960 --> 00:09:08.720
But it could be applied and adapted to other
things, for instance, if someone said I was

00:09:08.720 --> 00:09:11.270
talking about writing my books.

00:09:11.270 --> 00:09:14.880
Or my taste in Polo shirts, which is excellent
by the way.

00:09:14.880 --> 00:09:19.430
You’d only understand what I was talking
about if you saw the whole clip, but in another

00:09:19.430 --> 00:09:23.110
context it could be almost anything you want
it to be.

00:09:23.110 --> 00:09:25.810
There is no text without context.

00:09:25.810 --> 00:09:28.210
And videos can also be dramatically altered,
too.

00:09:28.210 --> 00:09:32.440
We don’t always think of videos as easy
to change -- maybe by skilled filmmakers,

00:09:32.440 --> 00:09:36.240
but not in the same way that we can easily
use filters to alter our Instagrams.

00:09:36.240 --> 00:09:39.610
But, if you’ve ever seen an episode of Bad
Lip Reading, you’ll know that it’s getting

00:09:39.610 --> 00:09:45.360
easier and easier to considerably alter a
video, or even fabricate one from scratch.

00:09:45.360 --> 00:09:48.060
And uploading and posting videos has never
been easier.

00:09:48.060 --> 00:09:50.350
Almost anyone with an internet connection
can do it.

00:09:50.350 --> 00:09:55.710
That’s why it’s important to know where
a video came from, and who created it, and

00:09:55.710 --> 00:09:59.340
whether it’s been altered before you believe
what you see.

00:09:59.340 --> 00:10:04.710
But the type of manipulated video that freaks
me out personally the most is the deepfake.

00:10:04.710 --> 00:10:09.010
Deep fake uses deep learning and artificial
intelligence to create video images that can

00:10:09.010 --> 00:10:12.540
be combined and superimposed onto existing
videos.

00:10:12.540 --> 00:10:17.250
So, for example, Nicholas Cage’s face can
be grafted onto other actors’ faces to create

00:10:17.250 --> 00:10:19.390
some really funny movie mashups.

00:10:19.390 --> 00:10:25.530
Or, an impersonator can have their voice and
facial movements convincingly woven into the

00:10:25.530 --> 00:10:26.720
video of a president.

00:10:26.720 --> 00:10:31.240
BuzzFeed, for instance, once made a video
of President Obama saying things like “Killmonger

00:10:31.240 --> 00:10:33.860
was right” to illustrate how deepfakes work.

00:10:33.860 --> 00:10:35.080
And this is happening more and more.

00:10:35.080 --> 00:10:39.940
The Belgian socialist party once created a
video of President Trump saying “climate

00:10:39.940 --> 00:10:40.940
change is fake.”

00:10:40.940 --> 00:10:44.690
They said they weren’t trying to dupe anyone,
but lots of commenters on the party’s Facebook

00:10:44.690 --> 00:10:46.310
page did not know it wasn’t real.

00:10:46.310 --> 00:10:50.240
Now you can certainly gain clues about a video’s
validity by checking the source.

00:10:50.240 --> 00:10:51.940
Is it an anonymous YouTube channel?

00:10:51.940 --> 00:10:53.560
A stranger on Facebook?

00:10:53.560 --> 00:10:55.250
Or a news source you trust?

00:10:55.250 --> 00:11:00.200
But to determine for sure whether videos like
these are real or fake, we need to read laterally.

00:11:00.200 --> 00:11:02.680
Or watch laterally, I suppose.

00:11:02.680 --> 00:11:06.850
Either way, open up a new tab and try to find
where the video originally came from.

00:11:06.850 --> 00:11:10.480
You might be able to do this by using a keyword
search based on the content of the video to

00:11:10.480 --> 00:11:11.630
see where it surfaces.

00:11:11.630 --> 00:11:15.790
Like, in the case of the videos I just mentioned
we could’ve searched Obama and Killmonger

00:11:15.790 --> 00:11:18.190
or Trump, Belgium, and climate change.

00:11:18.190 --> 00:11:21.870
And if the video you’re searching depicts
an important event of some kind, you might

00:11:21.870 --> 00:11:23.740
find it posted on several news sites.

00:11:23.740 --> 00:11:27.350
Or if it’s a known hoax, it may show up
on fact-checking sites.

00:11:27.350 --> 00:11:32.450
And if the only place you find the video is
on dubious sites or random social media posts,

00:11:32.450 --> 00:11:33.990
it’s probably bogus.

00:11:33.990 --> 00:11:39.530
But look, as technology advances and changing
photos and videos gets easier and easier,

00:11:39.530 --> 00:11:44.720
there will be more and more deep fakes, and
it will be much harder to tell them apart

00:11:44.720 --> 00:11:46.040
from reality.

00:11:46.040 --> 00:11:50.590
That freaks me out, and it’s a reminder
of how critical it is, especially for young

00:11:50.590 --> 00:11:55.820
people, to learn how to evaluate the quality
of information they encounter online.

00:11:55.820 --> 00:11:59.690
Because without using our lateral reading
skills, and looking for additional context

00:11:59.690 --> 00:12:04.380
for images we encounter, we risk being duped
by bad actors spreading misinformation.

00:12:04.380 --> 00:12:08.900
And as I’ve talked about before, when the
quality and reliability of our information

00:12:08.900 --> 00:12:13.790
decreases, the quality and reliability of
our decisions also decreases.

00:12:13.790 --> 00:12:16.840
So that’s why we’re going to continue
learning how to interrogate different types

00:12:16.840 --> 00:12:19.060
of evidence next time. I'll see you then.

