WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:01.000
Hi again Michael.

00:00:01.000 --> 00:00:02.590
&gt;&gt; Hey, how's it going?

00:00:02.590 --> 00:00:05.510
&gt;&gt; It's going pretty well. We are ready to do our

00:00:05.510 --> 00:00:08.410
last lesson of this mini course

00:00:08.410 --> 00:00:10.480
on unsupervised learning and randomized optimization.

00:00:10.480 --> 00:00:11.920
&gt;&gt; Cool, what's it about?

00:00:11.920 --> 00:00:16.470
&gt;&gt; It is about feature transformation. As opposed to feature selection.

00:00:16.470 --> 00:00:21.200
&gt;&gt; Cool. So they can change from vehicles into robots, I assume.

00:00:21.200 --> 00:00:23.400
&gt;&gt; yeah, typically. Usually it's from robots

00:00:23.400 --> 00:00:25.500
into cars, but that's a technical detail.

00:00:25.500 --> 00:00:26.360
&gt;&gt; Okay.

00:00:26.360 --> 00:00:29.820
&gt;&gt; Okay. So I'm going to define feature transformation for you, or at

00:00:29.820 --> 00:00:31.810
least I'm going to do my best to. It turns out that it's

00:00:31.810 --> 00:00:35.850
a slightly more slippery definition than the one that we used for

00:00:35.850 --> 00:00:38.075
feature selection but it has a lot of things in common. Okay?

00:00:38.075 --> 00:00:39.083
&gt;&gt; Yep.

00:00:39.083 --> 00:00:40.160
&gt;&gt; And you tell me if this makes sense.

00:00:40.160 --> 00:00:44.240
Okay, so feature transformation as opposed to feature selection which

00:00:44.240 --> 00:00:46.730
we talked about last time is the problem of

00:00:46.730 --> 00:00:50.680
doing some kind of pre-processing on a set of features.

00:00:52.010 --> 00:00:54.380
In order to create a new set of

00:00:54.380 --> 00:00:57.840
features. Now typically, we expect that new set of

00:00:57.840 --> 00:01:00.250
the future to be smaller or in some way

00:01:00.250 --> 00:01:04.209
more compact. Okay? But while we create this new

00:01:04.209 --> 00:01:07.270
set of features, we want to explicitly retain as

00:01:07.270 --> 00:01:10.360
much information as possible, and when I say retain

00:01:10.360 --> 00:01:14.150
as much information as possible, I probably mean, I

00:01:14.150 --> 00:01:17.050
think we'll, we'll see as we continue this conversation.

00:01:17.050 --> 00:01:20.260
Information that is relevant and hopefully useful.

00:01:20.260 --> 00:01:23.050
Now, the first thing you might ask is,

00:01:23.050 --> 00:01:25.990
what's the difference between this and feature selection.

00:01:25.990 --> 00:01:27.520
Is that a question you might ask, Michael?

00:01:27.520 --> 00:01:29.640
&gt;&gt; I was thinking about that, though I think you kind

00:01:29.640 --> 00:01:32.150
of told me at the beginning of the feature selection lecture.

00:01:32.150 --> 00:01:33.440
&gt;&gt; well, I told you that I was going to

00:01:33.440 --> 00:01:34.720
to tell you. I'm not sure I actually told you.

00:01:34.720 --> 00:01:35.930
&gt;&gt; Hm.

00:01:35.930 --> 00:01:38.260
&gt;&gt; So one thing you might say Michael, is that if I

00:01:38.260 --> 00:01:42.450
looked at this definition, this seems to actually be consistent with feature

00:01:42.450 --> 00:01:44.590
selection as well. I'd take a set

00:01:44.590 --> 00:01:46.850
of features, a feature selection. Then I'd create

00:01:46.850 --> 00:01:51.730
a new feature set which happens to be smaller. And my goal was to retain

00:01:51.730 --> 00:01:53.210
as much information as possible and we

00:01:53.210 --> 00:01:55.620
talked about the difference between relevant features and

00:01:55.620 --> 00:01:57.840
useful features, but really this sort of

00:01:57.840 --> 00:02:00.810
describes feature selection as well. You see that?

00:02:00.810 --> 00:02:01.430
&gt;&gt; Yeah, definitely.

00:02:01.430 --> 00:02:04.420
&gt;&gt; Now the difference is, in fact probably the right way to

00:02:04.420 --> 00:02:07.740
think about is this that feature selection is in fact a subset

00:02:07.740 --> 00:02:12.000
of feature transformation. Where the pre-processing you're doing is

00:02:12.000 --> 00:02:15.320
literally taking a subset of the features. Here, feature

00:02:15.320 --> 00:02:17.790
transformation can be more powerful, and can be an

00:02:17.790 --> 00:02:20.890
arbitrary pre-processing, not just something that goes from a set

00:02:20.890 --> 00:02:23.780
to to a subset. But what we're going to do

00:02:23.780 --> 00:02:26.730
is we're going to restrict our notion of future transformation to

00:02:26.730 --> 00:02:29.580
what's called linear future transformation. So, let me define

00:02:29.580 --> 00:02:32.840
that for you explicitly. So as before with the feature

00:02:32.840 --> 00:02:34.480
subset, we said that we were taking a

00:02:34.480 --> 00:02:38.800
bunch of features or a bunch of instances that

00:02:38.800 --> 00:02:42.000
were in some individual feature space and transforming

00:02:42.000 --> 00:02:45.670
it into another feature space of size M. Yup.

00:02:45.670 --> 00:02:48.890
&gt;&gt; And in the, the feature selection problem, m was meant

00:02:48.890 --> 00:02:52.160
to be less than N, hopefully much less than N. And that's

00:02:52.160 --> 00:02:55.450
typically the case of feature transformation, though as we'll discuss towards

00:02:55.450 --> 00:02:57.920
the very end, it doesn't have to be. But when I say

00:02:57.920 --> 00:03:01.580
usually we expect M to be less than N, usually means almost always.

00:03:01.580 --> 00:03:03.960
&gt;&gt; [LAUGH] Okay. And that's because of

00:03:03.960 --> 00:03:06.160
the cause of Dimensionality problem. But the difference

00:03:06.160 --> 00:03:08.100
between what we were doing with future selection

00:03:08.100 --> 00:03:10.020
and future transformation, because this is exactly what

00:03:10.020 --> 00:03:14.390
I wrote before, is that this transformation operator

00:03:14.390 --> 00:03:17.560
is usually something like this, in linear transformation

00:03:17.560 --> 00:03:23.870
operator. So the goal here is to find some matrix P, such that I can project.

00:03:23.870 --> 00:03:26.260
My examples my points in my instance

00:03:26.260 --> 00:03:29.670
space, into this new subspace, typically smaller

00:03:29.670 --> 00:03:34.000
than the original subspace. And I'll get some set of new features which are

00:03:34.000 --> 00:03:39.030
combinations in a particular linear combinations of the old features. Does that

00:03:39.030 --> 00:03:45.020
make sense? I think so. So then that p matrix would want to be N by M.

00:03:45.020 --> 00:03:46.210
&gt;&gt; Yes.

00:03:46.210 --> 00:03:49.770
&gt;&gt; So the transpose would be M by N and

00:03:49.770 --> 00:03:53.130
that multiplies by the N dimensional feature space in X

00:03:53.130 --> 00:03:55.480
and projects it out to an M dimensional feature space.

00:03:55.480 --> 00:03:55.660
&gt;&gt; Right.

00:03:55.660 --> 00:03:59.580
&gt;&gt; Okay. Yeah. Pretty good. So if we wanted to write that

00:03:59.580 --> 00:04:02.380
out. We could say that feature selection was about taking features like

00:04:02.380 --> 00:04:10.110
X1, X2, X3 and X4 and finding a subset like X1 and X2.

00:04:10.110 --> 00:04:15.090
And that would be feature selection. But feature transformation would be taking

00:04:15.090 --> 00:04:22.089
something like taking X1. X2, X3, and X4, and translating

00:04:22.089 --> 00:04:27.670
into something like 2X1 plus X2. Which creates

00:04:27.670 --> 00:04:30.380
a single new feature. Which is a linear combination

00:04:30.380 --> 00:04:32.370
of the subset of the. Does that make sense?

00:04:32.370 --> 00:04:34.900
&gt;&gt; Yeah, but it could actually make other feature

00:04:34.900 --> 00:04:37.610
combinations as well, right. It's just projected down to one.

00:04:37.610 --> 00:04:40.400
&gt;&gt; Right, it could be projected down to two, or it could be projected

00:04:40.400 --> 00:04:42.950
down to three Or could even project it out to a different for.

00:04:42.950 --> 00:04:43.530
&gt;&gt; Okay.

00:04:43.530 --> 00:04:46.720
&gt;&gt; And in principal you could imagine that you could even project

00:04:46.720 --> 00:04:48.370
up into other dimensions which is

00:04:48.370 --> 00:04:50.640
something that we've done before. Conceptually, anyway.

00:04:50.640 --> 00:04:53.110
&gt;&gt; When we talked about kernels?

00:04:53.110 --> 00:04:57.680
&gt;&gt; Yeah, although those were typically non-linear transformations implicit

00:04:57.680 --> 00:05:00.270
in the notion was doing a non-linear feature transformation

00:05:00.270 --> 00:05:01.570
but we even did it before we even learned

00:05:01.570 --> 00:05:06.160
about kernels. Very second thing I think we did. Perceptrons.

00:05:06.160 --> 00:05:08.760
How do perceptrons go into a higher dimensional space.

00:05:08.760 --> 00:05:10.420
&gt;&gt; Well, when we talked about XOR.

00:05:11.910 --> 00:05:13.470
&gt;&gt; Oh, right.

00:05:13.470 --> 00:05:15.700
&gt;&gt; What we effectively did was we showed that we could

00:05:15.700 --> 00:05:21.130
project the original. Two dimensional space into what looks like a three

00:05:21.130 --> 00:05:24.120
dimensional space where the third dimension was a combination of the first

00:05:24.120 --> 00:05:27.700
two. And then you could actually do it with a linear separator.

00:05:27.700 --> 00:05:29.110
&gt;&gt; But that wasn't a linear

00:05:29.110 --> 00:05:31.560
transformation, that c was a nonlinear transformation.

00:05:31.560 --> 00:05:33.360
&gt;&gt; Right. Because we were talking about Boolean verbs.

00:05:33.360 --> 00:05:33.970
&gt;&gt; Yeah.

00:05:33.970 --> 00:05:35.930
&gt;&gt; But, in the end of the day it

00:05:35.930 --> 00:05:38.250
was still a kind of feature transformation and in this

00:05:38.250 --> 00:05:40.560
case a feature transformation where we went to a higher

00:05:40.560 --> 00:05:43.420
number of features. And today, what we're going to be talking

00:05:43.420 --> 00:05:48.140
about is linear transformations as opposed to non-linear transformations.

00:05:48.140 --> 00:05:51.090
And we're going to be focusing specifically on the case

00:05:51.090 --> 00:05:53.520
where we're trying to reduce the number of dimensions. So

00:05:53.520 --> 00:05:57.010
the implicit assumption here, right, the kind of domain knowledge

00:05:57.010 --> 00:06:00.770
that we're bringing to here or the belief that we're bringing here is that. We

00:06:00.770 --> 00:06:04.990
don't need all N features. We need a much smaller set of features that'll still

00:06:04.990 --> 00:06:07.910
capture all the original information, and therefore,

00:06:07.910 --> 00:06:10.220
help us to overcome the curse of dimensionality.

00:06:10.220 --> 00:06:13.030
It's just that that smaller subset might require

00:06:13.030 --> 00:06:16.060
bringing in information across the various features. Okay?

00:06:16.060 --> 00:06:16.820
&gt;&gt; Yeah.

00:06:16.820 --> 00:06:17.600
&gt;&gt; Alright.

