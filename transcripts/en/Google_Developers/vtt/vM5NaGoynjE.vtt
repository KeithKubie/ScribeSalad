WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.019
[MUSIC PLAYING]

00:00:04.019 --> 00:00:06.330
DOUG ECK: My name is
Doug and I'm a team

00:00:06.330 --> 00:00:07.862
member on Google Brain team.

00:00:07.862 --> 00:00:10.320
And I'm going to talk to you
about a project called Magenta

00:00:10.320 --> 00:00:12.357
that's focusing on really
hard challenge, which

00:00:12.357 --> 00:00:16.230
is generating music and
art using deep learning.

00:00:16.230 --> 00:00:17.970
So why are we here?

00:00:17.970 --> 00:00:20.970
We're here to ask the question,
can we use deep learning

00:00:20.970 --> 00:00:23.994
and reinforcement learning
to generate compelling media.

00:00:23.994 --> 00:00:25.410
And treating that
generally, we're

00:00:25.410 --> 00:00:28.860
talking about music, and
images, and video, and text,

00:00:28.860 --> 00:00:30.630
for example, of joke telling.

00:00:30.630 --> 00:00:32.371
And this is a hard problem.

00:00:32.371 --> 00:00:33.870
I'd missed the
keynote this morning,

00:00:33.870 --> 00:00:35.250
but someone in
the audience maybe

00:00:35.250 --> 00:00:37.497
still is working with
robots and painting.

00:00:37.497 --> 00:00:38.580
Is that person still here?

00:00:38.580 --> 00:00:38.870
All right.

00:00:38.870 --> 00:00:40.203
So you know this is hard, right?

00:00:40.203 --> 00:00:42.325
This is not an easy
thing just to generate--

00:00:42.325 --> 00:00:43.950
Furthermore, we want
to generate things

00:00:43.950 --> 00:00:45.690
that are interesting
and are surprising,

00:00:45.690 --> 00:00:47.050
and we want to measure success.

00:00:47.050 --> 00:00:49.260
So we want to understand
what people are really

00:00:49.260 --> 00:00:50.796
doing with this media.

00:00:50.796 --> 00:00:52.170
And I think it's
an exciting time

00:00:52.170 --> 00:00:54.003
to be able to try a
project like this, where

00:00:54.003 --> 00:00:56.040
we have some of the
tools available to us

00:00:56.040 --> 00:00:57.250
that weren't there before.

00:00:57.250 --> 00:01:01.310
And this is what we're about.

00:01:01.310 --> 00:01:03.250
So one of the things
that we're trying to do

00:01:03.250 --> 00:01:06.700
is do this out in the open
and build a community.

00:01:06.700 --> 00:01:09.010
So a community of
creative coders,

00:01:09.010 --> 00:01:11.680
and a community of artists
and musicians to work with us.

00:01:11.680 --> 00:01:13.388
So everything that
we're doing in Magenta

00:01:13.388 --> 00:01:18.280
is being put into open-source in
GitHub in the Tensorflow repo,

00:01:18.280 --> 00:01:19.879
rather Magenta
repo in TensorFlow.

00:01:19.879 --> 00:01:21.670
And we're trying really
hard to have tools,

00:01:21.670 --> 00:01:24.340
for example, Jupiter notebooks,
that allow people to really

00:01:24.340 --> 00:01:27.040
quickly do some things, so
that we can draw them in

00:01:27.040 --> 00:01:29.450
and all work on this together.

00:01:29.450 --> 00:01:34.290
So this is an example of style
transfer that's out there.

00:01:34.290 --> 00:01:36.950
But I think more
to the point and--

00:01:36.950 --> 00:01:40.940
I think the challenge that I
want people to leave with--

00:01:40.940 --> 00:01:42.889
if you're going to think
about this problem,

00:01:42.889 --> 00:01:45.430
and I think it's a problem we're
thinking about a challenge--

00:01:45.430 --> 00:01:47.490
is understanding
just how important

00:01:47.490 --> 00:01:50.730
it is for these models to
be embedded in the world

00:01:50.730 --> 00:01:52.200
and to get critical feedback.

00:01:52.200 --> 00:01:53.866
As a thought experiment,
you can imagine

00:01:53.866 --> 00:01:55.440
we train some
model that can just

00:01:55.440 --> 00:02:00.240
generate tens of thousands or
hundreds of thousands of songs

00:02:00.240 --> 00:02:02.666
that sound just like "the
Beatles", or whatever.

00:02:02.666 --> 00:02:04.290
It's not very
interesting to just think

00:02:04.290 --> 00:02:05.730
of pushing that button, right?

00:02:05.730 --> 00:02:07.688
You just keep generating
more and more material

00:02:07.688 --> 00:02:08.910
and it becomes overwhelming.

00:02:08.910 --> 00:02:10.410
I think more
interesting is thinking

00:02:10.410 --> 00:02:12.570
about making a
feedback loop where

00:02:12.570 --> 00:02:14.430
Magenta is being
used by musicians,

00:02:14.430 --> 00:02:16.862
being used by artists
in some interactive way.

00:02:16.862 --> 00:02:18.570
And also the stuff
that's being generated

00:02:18.570 --> 00:02:19.778
is being evaluated by people.

00:02:23.420 --> 00:02:25.670
Another way to look at
this is with respect

00:02:25.670 --> 00:02:29.510
to what we can do as
engineers, and what we can

00:02:29.510 --> 00:02:32.450
do as artists and musicians.

00:02:32.450 --> 00:02:36.420
Art and technology
have always co-evolved.

00:02:36.420 --> 00:02:38.600
The film camera was
initially thought

00:02:38.600 --> 00:02:44.840
of as A, this device that
is not artistic and is there

00:02:44.840 --> 00:02:47.420
to just capture reality,
and B, something

00:02:47.420 --> 00:02:49.040
that is not very
interesting, compared

00:02:49.040 --> 00:02:50.164
to what painters are doing.

00:02:50.164 --> 00:02:53.142
And people came along and turned
it into an artistic instrument.

00:02:53.142 --> 00:02:55.100
And you would note that
when people turn things

00:02:55.100 --> 00:02:57.475
into artistic instruments,
they tend to try to break them

00:02:57.475 --> 00:02:59.810
or to use them in ways in
which they're not intended.

00:02:59.810 --> 00:03:01.351
So on the slide
you're looking at now

00:03:01.351 --> 00:03:02.840
on the left you'll see Les Paul.

00:03:02.840 --> 00:03:05.030
He's one of the inventors
of the electric guitar.

00:03:05.030 --> 00:03:06.530
Rickenbacker is another.

00:03:06.530 --> 00:03:09.080
And what they were trying to
do with the electric guitar

00:03:09.080 --> 00:03:10.970
was make it louder,
so they could

00:03:10.970 --> 00:03:14.630
compete with other instruments
in a concert setting.

00:03:14.630 --> 00:03:17.660
And they weren't trying
to make it distort, right?

00:03:17.660 --> 00:03:19.010
And they weren't trying to--

00:03:19.010 --> 00:03:20.990
distortion was a failure case.

00:03:20.990 --> 00:03:23.900
And you have people like Jimi
Hendrix, above, or St. Vincent,

00:03:23.900 --> 00:03:26.870
on the bottom, that come along
and they take this technology,

00:03:26.870 --> 00:03:29.600
and they do things with
it that are unexpected.

00:03:29.600 --> 00:03:32.042
And I think that's a really
important part of the recipe.

00:03:32.042 --> 00:03:34.250
What makes, I think, Magenta
interesting with respect

00:03:34.250 --> 00:03:37.010
to this, if you
will, is to say, OK,

00:03:37.010 --> 00:03:40.430
what if we can make something
like an electric guitar,

00:03:40.430 --> 00:03:44.670
or like a drum machine,
or like a camera,

00:03:44.670 --> 00:03:48.470
but that it itself has some
machine learning intelligence.

00:03:48.470 --> 00:03:50.030
So it's pushing
back a little bit.

00:03:50.030 --> 00:03:51.320
The idea's that you might
actually have something

00:03:51.320 --> 00:03:53.630
that you, as an artist or a
musician, can push against,

00:03:53.630 --> 00:03:55.670
that listens to you,
that responds to you.

00:03:55.670 --> 00:03:58.520
And the question then is, can
we, the people on Magenta,

00:03:58.520 --> 00:04:00.710
probably more like Les
Paul or a Rickenbacker

00:04:00.710 --> 00:04:02.690
than like Jimi Hendrix--
so I do play guitar

00:04:02.690 --> 00:04:04.700
and I am left-handed,
so there's a little bit

00:04:04.700 --> 00:04:06.270
of Jimi Hendrix in me.

00:04:06.270 --> 00:04:08.349
The basic idea is,
can we, as engineers,

00:04:08.349 --> 00:04:09.890
build some technology
and then create

00:04:09.890 --> 00:04:15.800
that loop of collaboration
with them, with artists, right?

00:04:15.800 --> 00:04:19.130
So let's dive into a
little bit of research now.

00:04:19.130 --> 00:04:22.460
Let's talk about a nice
problem, image inpainting.

00:04:22.460 --> 00:04:25.640
How many people in the room are
familiar with this basic idea?

00:04:25.640 --> 00:04:27.560
OK, quite a few.

00:04:27.560 --> 00:04:30.560
It's interesting that you
chop a chunk out of an image

00:04:30.560 --> 00:04:32.690
and then you try
to fill it back in.

00:04:32.690 --> 00:04:36.170
And it's a kind of generative
problem, except, in one way,

00:04:36.170 --> 00:04:38.930
it's easier than generating
an image from whole cloth,

00:04:38.930 --> 00:04:41.210
because you don't have the
surrounding context yet.

00:04:41.210 --> 00:04:43.626
It's also harder, because you
have to obey the surrounding

00:04:43.626 --> 00:04:44.600
context.

00:04:44.600 --> 00:04:46.190
This image is
actually a research

00:04:46.190 --> 00:04:49.304
from [INAUDIBLE] not
from the [INAUDIBLE] team

00:04:49.304 --> 00:04:49.970
of Google Brain.

00:04:49.970 --> 00:04:51.560
But I thought it
was a great image.

00:04:51.560 --> 00:04:53.560
You see that the human
artist on the upper right

00:04:53.560 --> 00:04:57.237
has filled in the missing
spot quite nicely.

00:04:57.237 --> 00:04:58.820
Probably those of
you that have looked

00:04:58.820 --> 00:05:02.450
at the recent work in GANs,
generative adversarial

00:05:02.450 --> 00:05:03.950
networks, are aware
of this finding

00:05:03.950 --> 00:05:07.310
that a normal L2
loss will give rise

00:05:07.310 --> 00:05:09.680
to something kind of
blurry and boring and safe.

00:05:09.680 --> 00:05:12.620
And that you need something
like an adversarial network

00:05:12.620 --> 00:05:14.900
or reinforcement
learning to force models

00:05:14.900 --> 00:05:16.760
to generate away
from the mean and do

00:05:16.760 --> 00:05:18.920
something more interesting.

00:05:18.920 --> 00:05:22.370
So we had an idea,
led by Anna Huang,

00:05:22.370 --> 00:05:25.400
who was a Google Brain intern
working with me in the Magenta

00:05:25.400 --> 00:05:29.990
team, to say, can we compose
music using something similar.

00:05:29.990 --> 00:05:32.330
And this wasn't a one to
one reuse of inpainting.

00:05:32.330 --> 00:05:34.996
The idea was that you would take
multi-voice music, in this case

00:05:34.996 --> 00:05:38.350
Bach, and you would
then remove voices.

00:05:38.350 --> 00:05:40.640
A voice is a sort
of one melodic line.

00:05:40.640 --> 00:05:42.429
And there are four
voices in this music.

00:05:42.429 --> 00:05:44.220
We use Bach because
the data was available,

00:05:44.220 --> 00:05:47.540
and because for some
reason AI music generation

00:05:47.540 --> 00:05:48.980
people obsess on Bach.

00:05:48.980 --> 00:05:50.240
So we did the same thing.

00:05:50.240 --> 00:05:54.736
And you can either remove
part of the score as a chunk,

00:05:54.736 --> 00:05:57.110
or, more interestingly, you
can remove a voice at a time.

00:05:57.110 --> 00:05:59.420
So you remove one voice,
and the other three voices

00:05:59.420 --> 00:06:01.110
are providing context.

00:06:01.110 --> 00:06:04.546
And then, using Gibbs sampling,
you sample in the missing bits.

00:06:04.546 --> 00:06:06.920
And then you remove another
voice from the original data,

00:06:06.920 --> 00:06:09.022
and you sample in the
missing bits and continue.

00:06:09.022 --> 00:06:10.730
And so you can do this
really interesting

00:06:10.730 --> 00:06:12.188
conditional
generation, conditioned

00:06:12.188 --> 00:06:13.550
by the original data.

00:06:13.550 --> 00:06:18.620
Or you can simply start
with something empty

00:06:18.620 --> 00:06:20.940
and start to build
voices one at a time.

00:06:20.940 --> 00:06:23.070
And it turns out the
sampling helps a lot.

00:06:23.070 --> 00:06:25.951
So the first thing I'd like
to do is just play this.

00:06:25.951 --> 00:06:27.950
Don't pay attention to
the quality of the sound.

00:06:27.950 --> 00:06:28.730
It's just MIDI.

00:06:28.730 --> 00:06:30.259
Pay attention to
the melody and see

00:06:30.259 --> 00:06:32.800
if you think there's anything
there in what's been generated.

00:06:32.800 --> 00:06:34.910
So this would be the
time to play the video.

00:06:34.910 --> 00:07:08.841
[MUSIC PLAYING]

00:07:08.841 --> 00:07:09.340
All right.

00:07:09.340 --> 00:07:12.610
So it's not perfect, but
it's quite interesting.

00:07:12.610 --> 00:07:15.610
And what's interesting about
it-- the graph that I'm showing

00:07:15.610 --> 00:07:18.250
you is from our paper.

00:07:18.250 --> 00:07:23.890
We compared the three different
sampling routines to real Bach.

00:07:23.890 --> 00:07:26.800
And we found that when listeners
were asked to rate the music,

00:07:26.800 --> 00:07:30.700
they actually preferred the
Gibbs sampling to Bach himself,

00:07:30.700 --> 00:07:33.730
thus proving that we're
better than Bach, right?

00:07:33.730 --> 00:07:34.660
Game over.

00:07:34.660 --> 00:07:36.670
No, what it shows, I
think, is that these models

00:07:36.670 --> 00:07:37.920
capture something interesting.

00:07:37.920 --> 00:07:40.070
I think the way we put it
in listening to them is,

00:07:40.070 --> 00:07:41.650
of course, it's not
as good as Bach.

00:07:41.650 --> 00:07:44.740
But it captures almost some
cartoonish aspects of Bach.

00:07:44.740 --> 00:07:46.562
It's almost like
more Bach than Bach,

00:07:46.562 --> 00:07:48.520
because it's simpler than
Bach, but it captures

00:07:48.520 --> 00:07:51.077
some salient Bach pieces.

00:07:51.077 --> 00:07:52.660
I'm also convinced,
if we had gone out

00:07:52.660 --> 00:07:55.765
to musical experts, not
standard side-by-sides,

00:07:55.765 --> 00:07:57.640
that we would have found
something different.

00:07:57.640 --> 00:07:58.870
That said, we were
happy to see that we

00:07:58.870 --> 00:08:01.120
had the kind of structure
that people just sit and go,

00:08:01.120 --> 00:08:02.470
god, this is horrible, right?

00:08:02.470 --> 00:08:05.260
So yeah, more Bach than
Bach, with very big quotes

00:08:05.260 --> 00:08:08.870
around either "more" or
"Bach", you take your pick.

00:08:08.870 --> 00:08:09.370
OK.

00:08:09.370 --> 00:08:11.680
So now, with what
little time is left,

00:08:11.680 --> 00:08:14.845
let's move on to some
image-based work.

00:08:14.845 --> 00:08:16.970
Also some work done in
Brain with an intern Vincent

00:08:16.970 --> 00:08:18.466
Dumoulin.

00:08:18.466 --> 00:08:19.840
Some style transfer,
where we can

00:08:19.840 --> 00:08:21.400
do style transfer very quickly.

00:08:21.400 --> 00:08:23.800
We participated in
Magenta in this project

00:08:23.800 --> 00:08:26.250
and actually put
out on our GitHub.

00:08:26.250 --> 00:08:30.520
Some, I think, really nice
code for doing style transfer.

00:08:30.520 --> 00:08:33.279
This was a tweet yesterday
from Josh, here at Google.

00:08:33.279 --> 00:08:33.820
"Recommended.

00:08:33.820 --> 00:08:35.361
Magenta style transfer
code works out

00:08:35.361 --> 00:08:36.996
of the box with
their Docker image."

00:08:36.996 --> 00:08:38.620
So I put that in as
a plug to maybe get

00:08:38.620 --> 00:08:39.985
you people to try things out.

00:08:39.985 --> 00:08:41.860
I also think they're
really beautiful images.

00:08:41.860 --> 00:08:44.770
It's a really nice use
of multi-style transform,

00:08:44.770 --> 00:08:47.450
or pastiche.

00:08:47.450 --> 00:08:48.880
Here's the obligatory video.

00:08:48.880 --> 00:08:50.630
I believe we'll be at
a point where we can

00:08:50.630 --> 00:08:53.247
do this in real time on device.

00:08:53.247 --> 00:08:55.580
I also would point out that
because there's an isolating

00:08:55.580 --> 00:08:58.520
space, you can move back
and forth between styles

00:08:58.520 --> 00:09:02.480
and mix them and match them.

00:09:02.480 --> 00:09:04.820
This is a Magenta
team members dog.

00:09:04.820 --> 00:09:07.230
Shout out to Peekaboo.

00:09:07.230 --> 00:09:09.180
However I think the
main reason we've all

00:09:09.180 --> 00:09:10.812
seen a lot of style transfer--

00:09:10.812 --> 00:09:13.020
I want to point out that,
at least from the viewpoint

00:09:13.020 --> 00:09:15.690
of Magenta, the style transfer
work that's being done

00:09:15.690 --> 00:09:17.816
is extremely preliminary.

00:09:17.816 --> 00:09:19.440
I mean, first, it's
really interesting.

00:09:19.440 --> 00:09:21.315
I think it's really cool
that we can do this,

00:09:21.315 --> 00:09:24.150
that we can pull
style from one network

00:09:24.150 --> 00:09:27.360
and then via convolution
apply it to another.

00:09:27.360 --> 00:09:29.710
But to give you an example
where the challenges lay--

00:09:29.710 --> 00:09:31.860
we just have this kind of
standard style transfer

00:09:31.860 --> 00:09:32.850
up above.

00:09:32.850 --> 00:09:35.950
And down below our four
portraits by Picasso.

00:09:35.950 --> 00:09:38.760
And what I would argue
from looking at portraits

00:09:38.760 --> 00:09:42.152
by an artist like this is
that when artists are playing

00:09:42.152 --> 00:09:44.610
with geometry, when they're
playing with sort of the deeper

00:09:44.610 --> 00:09:46.800
geometry of the human face
or something else that

00:09:46.800 --> 00:09:49.020
has to do with deeper
structure, it's clear

00:09:49.020 --> 00:09:51.390
that these convolutional
patch-based,

00:09:51.390 --> 00:09:54.420
pixel-based models are lost.

00:09:54.420 --> 00:09:56.990
They simply can't do
these kinds of transfers.

00:09:56.990 --> 00:10:00.180
And it turns out that
structure, whether it's

00:10:00.180 --> 00:10:01.740
structure in language
to understand

00:10:01.740 --> 00:10:04.710
meaning across paragraphs,
or structure in music,

00:10:04.710 --> 00:10:08.280
understanding phrases of music
that span longer time scales,

00:10:08.280 --> 00:10:10.230
or structure in art--
it all boils down

00:10:10.230 --> 00:10:12.060
to that, to this
understanding of meaning

00:10:12.060 --> 00:10:13.470
at deeper and deeper layers.

00:10:13.470 --> 00:10:15.330
And I think that's
where the Grail is

00:10:15.330 --> 00:10:18.910
for us in terms of research.

00:10:18.910 --> 00:10:21.530
So why TensorFlow?

00:10:21.530 --> 00:10:23.581
Magenta is a TensorFlow.

00:10:23.581 --> 00:10:24.080
GitHub.

00:10:24.080 --> 00:10:27.530
We are at
github.com/tensorflow/magenta.

00:10:27.530 --> 00:10:30.230
And we're dedicated
to being, if you

00:10:30.230 --> 00:10:31.940
will, the glue
between TensorFlow,

00:10:31.940 --> 00:10:34.850
and the artistic community
and the music community.

00:10:34.850 --> 00:10:36.660
It has some real
advantages for us.

00:10:36.660 --> 00:10:38.402
The first one is,
because it's Python,

00:10:38.402 --> 00:10:39.860
we're able to work
with everything.

00:10:39.860 --> 00:10:41.318
A lot of working
with music and art

00:10:41.318 --> 00:10:43.130
has to do with just
piping data around,

00:10:43.130 --> 00:10:44.810
being able to do
the simple things.

00:10:44.810 --> 00:10:46.476
It's hard to see in
the blue over there,

00:10:46.476 --> 00:10:49.020
but I just grabbed some
of our HTTP archive.

00:10:49.020 --> 00:10:51.950
Dependencies-- pretty_midi
and mido, our two big MIDI I/O

00:10:51.950 --> 00:10:55.280
packages that we can use
immediately in Magenta.

00:10:55.280 --> 00:10:58.520
Thus in TensorFlow we've
really fast and flexible image,

00:10:58.520 --> 00:11:01.307
audio and video I/O,
which is crucial.

00:11:01.307 --> 00:11:03.140
A lot of our machine
learning for even music

00:11:03.140 --> 00:11:04.100
becomes I/O bound.

00:11:04.100 --> 00:11:07.070
If we're learning
from say 16 kHz audio,

00:11:07.070 --> 00:11:08.960
it's just a lot of
data to move around.

00:11:08.960 --> 00:11:10.460
We have TensorBoard.

00:11:10.460 --> 00:11:12.072
If you're not using
TensorBoard, you

00:11:12.072 --> 00:11:13.280
want to be using TensorBoard.

00:11:13.280 --> 00:11:16.340
Board Here's a view of
just some spectrograms

00:11:16.340 --> 00:11:18.480
from some audio generation
stuff we're working on.

00:11:18.480 --> 00:11:21.350
It's so nice to be able to,
while models are training,

00:11:21.350 --> 00:11:24.200
sample from them, look at
for us at their spectra

00:11:24.200 --> 00:11:25.640
and listen to them,
play with them

00:11:25.640 --> 00:11:27.530
without having to stop
the model training

00:11:27.530 --> 00:11:29.070
while the models are working.

00:11:29.070 --> 00:11:30.650
And find that there is a
developer community there,

00:11:30.650 --> 00:11:31.280
all of you.

00:11:31.280 --> 00:11:34.010
So the ability to
build out a community

00:11:34.010 --> 00:11:37.310
of coders, and of artists
and musicians that

00:11:37.310 --> 00:11:39.340
can work with us.

00:11:39.340 --> 00:11:40.570
What's next?

00:11:40.570 --> 00:11:43.390
Well, first, you can follow our
blog at magenta.tensorflow.org.

00:11:43.390 --> 00:11:46.510
We use the blog to communicate
the research that we're doing

00:11:46.510 --> 00:11:51.760
and to also highlight some
code that we've written.

00:11:51.760 --> 00:11:53.950
And also, please,
if you're a coder--

00:11:53.950 --> 00:11:55.630
and I guess everybody
in this room is,

00:11:55.630 --> 00:11:58.629
this is a developer conference--
give it a shot at trying out

00:11:58.629 --> 00:11:59.670
some things with Magenta.

00:11:59.670 --> 00:12:03.070
We're just sitting there at
github.com/tensorflow/magenta.

00:12:03.070 --> 00:12:04.660
And we have Docker install.

00:12:04.660 --> 00:12:06.370
We also have a Pip install.

00:12:06.370 --> 00:12:08.140
It's all pretty painless to use.

00:12:08.140 --> 00:12:10.870
So in closing, I want to
leave you with this quote.

00:12:10.870 --> 00:12:12.449
I won't have time
to read it all but

00:12:12.449 --> 00:12:13.990
there's a beautiful
thing at the end.

00:12:13.990 --> 00:12:16.390
"The excitement of grainy
film, of bleached-out black

00:12:16.390 --> 00:12:19.015
and white, is the excitement of
witnessing events too momentous

00:12:19.015 --> 00:12:21.370
for the medium assigned
to record them. "

00:12:21.370 --> 00:12:24.070
I think the exciting part
of Magenta is this idea

00:12:24.070 --> 00:12:26.890
of actually taking machine
learning and its generative

00:12:26.890 --> 00:12:29.080
capacities, and actually
connecting it to people

00:12:29.080 --> 00:12:32.260
and allowing people to
experience and work with this.

00:12:32.260 --> 00:12:37.559
It's-- the opportunity to create
something like a drum machine,

00:12:37.559 --> 00:12:39.850
but that actually has some
intelligence to it, I think,

00:12:39.850 --> 00:12:41.890
is compelling.

00:12:41.890 --> 00:12:43.360
I want to thank
you for your time.

00:12:43.360 --> 00:12:45.220
I just want to mention,
we will have a demo

00:12:45.220 --> 00:12:48.400
during drinks that is
an ability to play along

00:12:48.400 --> 00:12:50.606
on a piano keyboard with
a Magenta piano model.

00:12:50.606 --> 00:12:51.730
So that can be kind of fun.

00:12:51.730 --> 00:12:52.240
Stop by.

00:12:52.240 --> 00:12:55.127
I'll have a beer in my hand and
we can look at that together.

00:12:55.127 --> 00:12:57.460
And then I'm going to introduce
our next speaker, Lilly,

00:12:57.460 --> 00:12:59.590
who's going to talk
about TensorFlow

00:12:59.590 --> 00:13:03.546
and analyzing retinal images to
diagnose causes of blindness.

00:13:03.546 --> 00:13:05.170
Thank you very much
for your attention.

00:13:05.170 --> 00:13:05.770
Cheers.

00:13:05.770 --> 00:13:07.920
[APPLAUSE]

