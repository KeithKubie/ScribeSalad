WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.850
[MUSIC PLAYING]

00:00:06.175 --> 00:00:08.140
SARA ROBINSON: Hello, everyone.

00:00:08.140 --> 00:00:11.010
Welcome to Machine
Learning APIs By Example.

00:00:11.010 --> 00:00:12.940
Did everyone enjoy the
concert last night?

00:00:12.940 --> 00:00:13.820
[CHEERING]

00:00:13.820 --> 00:00:14.750
Awesome.

00:00:14.750 --> 00:00:16.700
LCD Soundsystem is a
tough act to follow,

00:00:16.700 --> 00:00:17.932
but I'm going to try my best.

00:00:17.932 --> 00:00:19.390
I'm going to show
you how to access

00:00:19.390 --> 00:00:23.030
pre-trained machine learning
models with a single API call.

00:00:23.030 --> 00:00:24.320
My name is Sara Robinson.

00:00:24.320 --> 00:00:26.920
I'm a Developer Advocate on
the Google Cloud Platform team.

00:00:26.920 --> 00:00:30.710
You can find me on
Twitter @SRobTweets.

00:00:30.710 --> 00:00:32.450
So to get started at
a high level, what

00:00:32.450 --> 00:00:33.590
is machine learning?

00:00:33.590 --> 00:00:35.270
Machine learning is
teaching computers

00:00:35.270 --> 00:00:38.570
to recognize patterns in the
same way that our brains do.

00:00:38.570 --> 00:00:41.670
So it's very easy for a child to
differentiate between a picture

00:00:41.670 --> 00:00:43.180
of a cat and a dog.

00:00:43.180 --> 00:00:45.650
But it's much, much more
difficult to teach a computer

00:00:45.650 --> 00:00:46.890
to do the same thing.

00:00:46.890 --> 00:00:49.530
So the idea is, we
train these models

00:00:49.530 --> 00:00:51.770
and they improve over time
with more and more data

00:00:51.770 --> 00:00:54.390
and experience.

00:00:54.390 --> 00:00:56.140
So what we have
here is a diagram

00:00:56.140 --> 00:00:57.930
of a typical deep
neural network.

00:00:57.930 --> 00:01:00.980
And we can think of the input
to this network as pixels,

00:01:00.980 --> 00:01:02.959
and the output is going
to be a prediction.

00:01:02.959 --> 00:01:04.500
And the layers
throughout the network

00:01:04.500 --> 00:01:06.464
are looking for certain
features in the image.

00:01:06.464 --> 00:01:08.880
They're looking for maybe the
shape of the ears, the eyes,

00:01:08.880 --> 00:01:09.850
or the nose.

00:01:09.850 --> 00:01:12.430
And the output is a
prediction or a classification

00:01:12.430 --> 00:01:14.045
as either a cat or a dog.

00:01:14.045 --> 00:01:16.760
But let's take a step
back for a moment.

00:01:16.760 --> 00:01:18.860
And let's do this
without a neural network.

00:01:18.860 --> 00:01:22.540
Let's do some human-powered
image detection.

00:01:22.540 --> 00:01:25.000
So looking at these two pictures
of an apple and an orange,

00:01:25.000 --> 00:01:26.250
if we were to
write an algorithm,

00:01:26.250 --> 00:01:27.850
what are some
features we might look

00:01:27.850 --> 00:01:29.810
for in our code to be
able to differentiate

00:01:29.810 --> 00:01:30.757
between these two?

00:01:30.757 --> 00:01:31.590
You can shout it out

00:01:31.590 --> 00:01:32.355
AUDIENCE: Color.

00:01:32.355 --> 00:01:33.230
SARA ROBINSON: Color.

00:01:33.230 --> 00:01:34.242
Color is a great one.

00:01:34.242 --> 00:01:35.700
So if we were to
look for color, we

00:01:35.700 --> 00:01:37.950
could look at the majority
of the pixels in the image.

00:01:37.950 --> 00:01:40.464
If the majority of the pixels
are red, it's an apple.

00:01:40.464 --> 00:01:41.880
If they're orange,
it's an orange.

00:01:41.880 --> 00:01:45.792
So that would work great
on these two examples.

00:01:45.792 --> 00:01:47.375
Then what if we have
grayscale images?

00:01:47.375 --> 00:01:49.010
Then we have to
start all over again

00:01:49.010 --> 00:01:50.760
and look for possibly
a different feature.

00:01:50.760 --> 00:01:52.050
What might we look
for this time?

00:01:52.050 --> 00:01:52.862
AUDIENCE: Texture.

00:01:52.862 --> 00:01:54.660
SARA ROBINSON: Texture,
I heard texture.

00:01:54.660 --> 00:01:56.630
So we could look at the texture,
that would differentiate

00:01:56.630 --> 00:01:57.379
between these two.

00:01:57.379 --> 00:02:00.290
But then what would
happen if we got crazy

00:02:00.290 --> 00:02:02.830
and added a third fruit,
if we added a mango?

00:02:02.830 --> 00:02:03.550
You get the idea.

00:02:03.550 --> 00:02:05.631
We'd have to start
all over again.

00:02:05.631 --> 00:02:07.380
Now all these pictures
are pretty similar.

00:02:07.380 --> 00:02:09.289
These fruits are
all very circular.

00:02:09.289 --> 00:02:12.110
So if we instead had
two images of things

00:02:12.110 --> 00:02:13.610
that were completely
different, this

00:02:13.610 --> 00:02:16.060
should be a pretty
easy task, right?

00:02:16.060 --> 00:02:19.882
What if we had a picture
of a dog and a mop,

00:02:19.882 --> 00:02:21.840
and we needed to
differentiate between the two.

00:02:21.840 --> 00:02:23.980
These two things have
nothing in common, right?

00:02:23.980 --> 00:02:25.455
The mop is not
living or breathing,

00:02:25.455 --> 00:02:29.437
it doesn't have eyes,
a nose, or a mouth.

00:02:29.437 --> 00:02:30.770
It's actually kind of difficult.

00:02:30.770 --> 00:02:32.930
[LAUGHTER]

00:02:32.930 --> 00:02:35.924
So here we have four pictures
of sheepdogs and mops.

00:02:35.924 --> 00:02:37.757
And it's kind of hard
even for the human eye

00:02:37.757 --> 00:02:39.985
to tell the difference
between the two.

00:02:39.985 --> 00:02:41.360
So the idea here
is we don't want

00:02:41.360 --> 00:02:43.480
to write specific
rules in our code

00:02:43.480 --> 00:02:46.230
to look for certain
things in our pictures.

00:02:46.230 --> 00:02:50.402
We want to write code that will
find these patterns for us.

00:02:50.402 --> 00:02:52.860
Because we might not have just
photos of animals and fruit.

00:02:52.860 --> 00:02:54.700
We might have photos
of everything.

00:02:54.700 --> 00:02:57.000
And in addition to
photos, we might

00:02:57.000 --> 00:02:58.940
have other types of
unstructured data

00:02:58.940 --> 00:03:01.580
like video, audio, and text.

00:03:01.580 --> 00:03:04.140
So what I'm here to talk about
today is a suite of tools

00:03:04.140 --> 00:03:05.580
that Google Cloud
Platform offers

00:03:05.580 --> 00:03:07.760
to help you make sense
of this unstructured data

00:03:07.760 --> 00:03:09.300
through machine learning.

00:03:09.300 --> 00:03:11.350
On the left-hand
side, we have tools

00:03:11.350 --> 00:03:14.400
to help you build and train your
own machine learning models.

00:03:14.400 --> 00:03:16.124
TensorFlow is an open
source framework,

00:03:16.124 --> 00:03:17.790
lets you build and
train your own models

00:03:17.790 --> 00:03:19.330
with your own custom data.

00:03:19.330 --> 00:03:21.320
And Cloud Machine
Learning Engine

00:03:21.320 --> 00:03:24.080
lets you run and manage
TensorFlow models on Google's

00:03:24.080 --> 00:03:25.800
managed infrastructure.

00:03:25.800 --> 00:03:28.250
What I'm going to focus on
today is the right-hand side.

00:03:28.250 --> 00:03:30.890
And is what I like to call
friendly machine learning.

00:03:30.890 --> 00:03:33.590
These are APIs they give you
access to pre-trained machine

00:03:33.590 --> 00:03:35.070
learning models
so that you don't

00:03:35.070 --> 00:03:37.445
have to worry about building
and training your own model.

00:03:37.445 --> 00:03:40.560
All you need to know how to
do is make a REST API request.

00:03:40.560 --> 00:03:42.700
And as developers, you
can focus on building

00:03:42.700 --> 00:03:45.290
awesome functionality
into your applications.

00:03:45.290 --> 00:03:48.600
So I'm going to give you a tour
of each of these five APIs.

00:03:48.600 --> 00:03:51.175
And I'm going to get
started with the Vision API.

00:03:51.175 --> 00:03:52.450
So let's dive right in.

00:03:52.450 --> 00:03:56.060
The Vision API lets you
perform complex image detection

00:03:56.060 --> 00:03:57.420
with a simple REST request.

00:03:57.420 --> 00:03:59.970
And before I dive into
the specifics of the API,

00:03:59.970 --> 00:04:01.470
for each section,
I want to give you

00:04:01.470 --> 00:04:03.220
an example of a
customer that's using it

00:04:03.220 --> 00:04:04.636
so that you can
get a sense of how

00:04:04.636 --> 00:04:07.510
it's being used in production.

00:04:07.510 --> 00:04:10.310
So for the Vision API, my
example is realtor.com,

00:04:10.310 --> 00:04:12.930
and they are a real
estate listing service.

00:04:12.930 --> 00:04:15.210
And they're using the Vision
API in their mobile app.

00:04:15.210 --> 00:04:16.870
So they have an iOS
and Android app.

00:04:16.870 --> 00:04:19.110
And when people are out and
about looking at houses,

00:04:19.110 --> 00:04:21.360
they can take a
picture on the app.

00:04:21.360 --> 00:04:24.040
And they're using the Vision
API's OCR, their Optical

00:04:24.040 --> 00:04:26.267
Character Recognition,
to extract the text

00:04:26.267 --> 00:04:28.100
from the image of the
listing, and then they

00:04:28.100 --> 00:04:31.030
can pull up the relevant
listing inside the application.

00:04:33.180 --> 00:04:35.680
Let's take a look at some of
the core features of the Vision

00:04:35.680 --> 00:04:36.560
API.

00:04:36.560 --> 00:04:38.643
The first one we have here
all the way on the left

00:04:38.643 --> 00:04:39.550
is label detection.

00:04:39.550 --> 00:04:42.860
And label detection can tell
us, what is this a picture of?

00:04:42.860 --> 00:04:46.590
For this picture, it might
tell us cheetah, animal, field,

00:04:46.590 --> 00:04:48.040
et cetera.

00:04:48.040 --> 00:04:49.830
Then we have face detection.

00:04:49.830 --> 00:04:52.840
Face detection can tell us,
are there faces in the image?

00:04:52.840 --> 00:04:55.710
If so, where are they,
how many faces are there,

00:04:55.710 --> 00:04:58.380
and what emotions can
we find in the image?

00:04:58.380 --> 00:05:01.600
Are they happy, sad,
angry, or surprised?

00:05:01.600 --> 00:05:04.270
OCR is the use case that I
mentioned on the previous slide

00:05:04.270 --> 00:05:05.446
with realtor.com.

00:05:05.446 --> 00:05:08.220
So this feature can
identify text in an image.

00:05:08.220 --> 00:05:10.510
It can tell you what
language that text is in

00:05:10.510 --> 00:05:14.030
and where the text is
located inside the image.

00:05:14.030 --> 00:05:16.960
Explicit content detection
is pretty self-explanatory.

00:05:16.960 --> 00:05:19.371
This will tell you, is the
image appropriate or not?

00:05:19.371 --> 00:05:21.370
This is actually really
useful if you got a site

00:05:21.370 --> 00:05:24.130
with a lot of user-generated
content, people uploading

00:05:24.130 --> 00:05:25.130
tons and tons of images.

00:05:25.130 --> 00:05:26.796
You probably don't
want to have somebody

00:05:26.796 --> 00:05:29.190
sitting there manually
classifying each image as

00:05:29.190 --> 00:05:30.000
appropriate or not.

00:05:30.000 --> 00:05:32.550
Instead, you can just use
an API call to do this,

00:05:32.550 --> 00:05:34.059
send the image to
the Vision API,

00:05:34.059 --> 00:05:35.850
it'll tell you of the
image is appropriate.

00:05:35.850 --> 00:05:38.100
And then you only have
to manually review

00:05:38.100 --> 00:05:40.480
a small subset of your images.

00:05:40.480 --> 00:05:42.710
Then we have landmark
detection, which can tell us,

00:05:42.710 --> 00:05:44.502
is there a common
landmark in this photo?

00:05:44.502 --> 00:05:46.460
And what are the
latitude-longitude coordinates

00:05:46.460 --> 00:05:48.000
of that landmark?

00:05:48.000 --> 00:05:49.940
And finally, logo
detection to tell us,

00:05:49.940 --> 00:05:52.991
is there a company logo
present in the image?

00:05:52.991 --> 00:05:55.490
So I want to show you what the
JSON response from the Vision

00:05:55.490 --> 00:05:59.240
API looks like for just
two of these features.

00:05:59.240 --> 00:06:00.800
The first one is face detection.

00:06:00.800 --> 00:06:03.200
This is a selfie I took
with two teammates on a trip

00:06:03.200 --> 00:06:05.300
to Petra in Jordan last year.

00:06:05.300 --> 00:06:08.780
And it returns an object for
each face found in the image.

00:06:08.780 --> 00:06:11.100
So this is just the
response for my face.

00:06:11.100 --> 00:06:13.400
So we can see that
from my face, it

00:06:13.400 --> 00:06:16.510
returns headwear likelihood,
very unlikely, which is true.

00:06:16.510 --> 00:06:18.000
I'm not wearing a
hat in the image,

00:06:18.000 --> 00:06:19.810
although my two teammates are.

00:06:19.810 --> 00:06:22.110
And then joy likelihood
is very likely.

00:06:22.110 --> 00:06:23.650
I'm smiling in the image.

00:06:23.650 --> 00:06:26.870
It also gives me a lot more data
on where different features are

00:06:26.870 --> 00:06:28.340
in my face.

00:06:28.340 --> 00:06:29.890
So I can use that
if I want to draw

00:06:29.890 --> 00:06:34.010
a box around the face in the
image or something like that.

00:06:34.010 --> 00:06:36.940
The next JSON response I want to
show you is landmark detection.

00:06:36.940 --> 00:06:39.010
So everyone knows
what this is, right?

00:06:39.010 --> 00:06:41.180
This is the Eiffel
Tower in Paris.

00:06:41.180 --> 00:06:42.350
It's actually not.

00:06:42.350 --> 00:06:44.360
I was fooled too, don't worry.

00:06:44.360 --> 00:06:47.920
It is via Paris Hotel
and Casino in Las Vegas.

00:06:47.920 --> 00:06:53.030
So I wanted to see, could the
Vision API spot the difference?

00:06:53.030 --> 00:06:54.330
And the answer was yes.

00:06:54.330 --> 00:06:57.780
With 80% confidence, it's able
to identify this as the Paris

00:06:57.780 --> 00:06:59.870
Hotel and Casino in Las Vegas.

00:06:59.870 --> 00:07:01.300
And we get the
latitude-longitude

00:07:01.300 --> 00:07:02.740
coordinates of that landmark.

00:07:05.690 --> 00:07:07.190
So in addition to
those six features

00:07:07.190 --> 00:07:08.920
that I just touched on,
a couple of weeks ago,

00:07:08.920 --> 00:07:11.336
we added some new features to
the Vision API, which I want

00:07:11.336 --> 00:07:12.910
to highlight on this slide.

00:07:12.910 --> 00:07:14.867
The first of these
is crop hints.

00:07:14.867 --> 00:07:16.700
This will give you
suggested crop dimensions

00:07:16.700 --> 00:07:18.170
for your photos.

00:07:18.170 --> 00:07:20.200
The second, which I'm
most excited about,

00:07:20.200 --> 00:07:22.969
is web annotations, which
lets you search the internet

00:07:22.969 --> 00:07:24.260
for more details on your image.

00:07:24.260 --> 00:07:26.218
And I'm going to dive
into that in a little bit

00:07:26.218 --> 00:07:28.260
more detail on the next slide.

00:07:28.260 --> 00:07:30.890
And then finally, we have
document text annotations,

00:07:30.890 --> 00:07:32.590
which improves the OCR model.

00:07:32.590 --> 00:07:35.020
So if you have images
with lots of text,

00:07:35.020 --> 00:07:38.460
like a picture of a receipt or
a business card, or a resume,

00:07:38.460 --> 00:07:40.940
the model will do a much
better job extracting all

00:07:40.940 --> 00:07:43.180
of that text from the image.

00:07:43.180 --> 00:07:47.057
So looking a little more
closely into web annotations,

00:07:47.057 --> 00:07:48.265
I'm a big "Harry Potter" fan.

00:07:48.265 --> 00:07:51.280
So I'm going to use this image
to show you web annotations.

00:07:51.280 --> 00:07:52.740
This is a picture of a car.

00:07:52.740 --> 00:07:54.120
It's not just any car.

00:07:54.120 --> 00:07:55.870
It's a car from the
"Harry Potter" movies,

00:07:55.870 --> 00:07:58.570
and it is currently on
display in a museum.

00:07:58.570 --> 00:08:01.180
So I wanted to send this to
the web annotations endpoint

00:08:01.180 --> 00:08:05.810
and see what entities it was
able to find in the image.

00:08:05.810 --> 00:08:08.550
The first one I found is
Ford Anglia, which is indeed

00:08:08.550 --> 00:08:11.980
the correct model of this car.

00:08:11.980 --> 00:08:14.320
And then it also returned
Art Science Museum,

00:08:14.320 --> 00:08:16.920
which is a museum in Singapore
where the car is currently

00:08:16.920 --> 00:08:19.800
on display.

00:08:19.800 --> 00:08:22.770
And then finally, it tells us
that this picture is indeed

00:08:22.770 --> 00:08:23.700
from "Harry Potter."

00:08:23.700 --> 00:08:27.191
We get this all just by
sending the image to the API.

00:08:27.191 --> 00:08:29.440
We also get some other things
from the web annotations

00:08:29.440 --> 00:08:33.179
endpoint, one of which
is full matching images.

00:08:33.179 --> 00:08:36.270
So this will tell us all of
URLs across the web, where

00:08:36.270 --> 00:08:38.376
this exact same image exists.

00:08:38.376 --> 00:08:40.750
So this is useful if you want
to do any sort of copyright

00:08:40.750 --> 00:08:41.720
detection.

00:08:41.720 --> 00:08:44.178
If you've got users uploading
an image and you want to say,

00:08:44.178 --> 00:08:46.890
OK, has this image already
been published on another site,

00:08:46.890 --> 00:08:50.310
you can use full matching
images to get all those URLs.

00:08:50.310 --> 00:08:53.080
Partial matching images
is kind of like a search

00:08:53.080 --> 00:08:54.080
by image feature.

00:08:54.080 --> 00:08:58.640
It'll show you all the visually
similar images on the internet.

00:08:58.640 --> 00:09:00.590
And then pages with
matching images

00:09:00.590 --> 00:09:04.740
will give you the URL where
all these images were found.

00:09:07.122 --> 00:09:09.080
So I encourage you all
to try it in the browser

00:09:09.080 --> 00:09:10.000
with your own images.

00:09:10.000 --> 00:09:12.380
If you go to
cloud.google.com/vision,

00:09:12.380 --> 00:09:14.524
I'll have the link on
the last slide again.

00:09:14.524 --> 00:09:15.940
Before you start
writing any code,

00:09:15.940 --> 00:09:18.280
you can upload your images
directly in the browser

00:09:18.280 --> 00:09:19.980
and see all of
the JSON responses

00:09:19.980 --> 00:09:21.775
that I just described
for your image.

00:09:24.720 --> 00:09:27.150
Now, I wanted to show you
a demo of the Vision API.

00:09:27.150 --> 00:09:28.566
And because we're
at Google I/O, I

00:09:28.566 --> 00:09:30.852
wanted to incorporate
Firebase into my demo.

00:09:30.852 --> 00:09:32.560
So what I've got here
is a web app that's

00:09:32.560 --> 00:09:34.610
hosted on Firebase Hosting.

00:09:34.610 --> 00:09:36.560
And it lets users upload images.

00:09:36.560 --> 00:09:39.730
The images are stored in
Cloud Storage for Firebase.

00:09:39.730 --> 00:09:42.485
And then I'm using the Firebase
SDK for Cloud Functions.

00:09:42.485 --> 00:09:45.420
So I've got a function listening
on this storage bucket.

00:09:45.420 --> 00:09:47.550
Whenever a new image
is added to the bucket,

00:09:47.550 --> 00:09:49.775
it's going to send that
image to the Vision API.

00:09:49.775 --> 00:09:52.510
This is an entirely
serverless application.

00:09:52.510 --> 00:09:54.500
And the Vision API
returns JSON, which

00:09:54.500 --> 00:09:56.870
makes it really easy for us
to just write that response

00:09:56.870 --> 00:09:58.420
to a Firebase database.

00:09:58.420 --> 00:10:00.630
And then we can display
the result in a web UI.

00:10:00.630 --> 00:10:02.160
So if we could
switch to the demo.

00:10:05.001 --> 00:10:05.500
Awesome.

00:10:05.500 --> 00:10:07.360
So this is the web
app I've got here.

00:10:07.360 --> 00:10:08.710
I've got my latest photo.

00:10:08.710 --> 00:10:10.260
In this case, it's a coffee cup.

00:10:10.260 --> 00:10:12.230
And then I've got
some entities that

00:10:12.230 --> 00:10:14.730
are returned from the Vision
API, the total number of photos

00:10:14.730 --> 00:10:17.160
I've seen, and then the
total number of emotions

00:10:17.160 --> 00:10:19.970
detected if there was
a face in the image.

00:10:19.970 --> 00:10:22.070
So I'm going to go ahead
and add a photo here.

00:10:22.070 --> 00:10:24.480
I'm going to add this photo
of the Golden Gate Bridge.

00:10:24.480 --> 00:10:26.240
And then while
it's uploading, I'm

00:10:26.240 --> 00:10:28.610
going to show you my
Firebase database here.

00:10:28.610 --> 00:10:31.200
So we can see it was just
uploaded to Cloud Storage.

00:10:31.200 --> 00:10:33.575
And my function is being called
right now, there it goes.

00:10:33.575 --> 00:10:36.320
My function just wrote the
Vision API response back

00:10:36.320 --> 00:10:37.587
to my Firebase database.

00:10:37.587 --> 00:10:39.170
And if we look in
here, we can examine

00:10:39.170 --> 00:10:42.100
all the different things that
come back from the Vision API,

00:10:42.100 --> 00:10:44.030
including pages of
matching images.

00:10:44.030 --> 00:10:46.750
We can see the Wikipedia
page where this was found,

00:10:46.750 --> 00:10:48.727
which is actually
where I got the image.

00:10:48.727 --> 00:10:50.560
And then we can see
some of the web entities

00:10:50.560 --> 00:10:52.036
that were returned.

00:10:52.036 --> 00:10:54.020
And if we go back
to our app, we can

00:10:54.020 --> 00:10:58.370
see that the new picture is
reflected in our application,

00:10:58.370 --> 00:11:00.610
that the numbers
of entities were

00:11:00.610 --> 00:11:03.620
reflected to represent
the entities that

00:11:03.620 --> 00:11:06.089
were found in the image.

00:11:06.089 --> 00:11:07.630
So if we could go
back to the slides.

00:11:10.652 --> 00:11:12.860
So I just wanted to incorporate
Firebase in this demo

00:11:12.860 --> 00:11:16.330
to show you how you can combine
the cloud machine learning APIs

00:11:16.330 --> 00:11:19.100
with Firebase to create awesome
web or mobile applications.

00:11:19.100 --> 00:11:21.980
I definitely encourage you to
check out these Firebase APIs

00:11:21.980 --> 00:11:24.720
that I used.

00:11:24.720 --> 00:11:27.330
And going back to our
sheepdog versus mop example,

00:11:27.330 --> 00:11:30.330
in case you were wondering,
how did the Vision API perform

00:11:30.330 --> 00:11:31.350
on these images?

00:11:31.350 --> 00:11:33.360
So if we look at the
response for that dog photo

00:11:33.360 --> 00:11:34.860
all the way on the
right, we can see

00:11:34.860 --> 00:11:37.380
that the Vision API
was 99% confident

00:11:37.380 --> 00:11:39.070
that this was a photo of a dog.

00:11:39.070 --> 00:11:41.730
And it's even 77%
confident that it's

00:11:41.730 --> 00:11:46.340
a Commodore, which is the
exact breed of dog that is.

00:11:46.340 --> 00:11:47.750
Skipping ahead to
our mop photos,

00:11:47.750 --> 00:11:50.774
it was 70% confident that
this was a broom or a tool.

00:11:50.774 --> 00:11:52.440
And I'm going to skip
ahead and show you

00:11:52.440 --> 00:11:54.620
the response for all of them.

00:11:54.620 --> 00:11:56.080
So overall, this
pre-trained model

00:11:56.080 --> 00:11:58.090
did pretty well on
these images that

00:11:58.090 --> 00:12:00.650
are even pretty hard for
the human eye to discern.

00:12:00.650 --> 00:12:02.689
So on the dog photos, it
got three out of four.

00:12:02.689 --> 00:12:04.855
The third one there, it
didn't return the label dog,

00:12:04.855 --> 00:12:06.050
it said fur.

00:12:06.050 --> 00:12:08.780
So wasn't sure if that
one was a hit or a miss.

00:12:08.780 --> 00:12:11.210
And then on the third
row on the bottom,

00:12:11.210 --> 00:12:13.850
it didn't return the label mop
or broom for the third one,

00:12:13.850 --> 00:12:14.960
it returned textile.

00:12:14.960 --> 00:12:17.120
But overall, this
model was pre-trained.

00:12:17.120 --> 00:12:19.570
It wasn't trained specifically
to identify the difference

00:12:19.570 --> 00:12:21.650
between dog and mop photos.

00:12:21.650 --> 00:12:23.190
But it did a very
good job doing so.

00:12:26.000 --> 00:12:27.500
And for each API,
I want to show you

00:12:27.500 --> 00:12:29.180
how easy it is to call the API.

00:12:29.180 --> 00:12:31.240
I'm going to show you
these examples in Node.

00:12:31.240 --> 00:12:33.364
But we have Google
Cloud client libraries.

00:12:33.364 --> 00:12:35.530
You can choose the library
in your favorite language

00:12:35.530 --> 00:12:36.360
to call the API.

00:12:36.360 --> 00:12:38.870
I'm going to use Node because
that's my language of choice.

00:12:38.870 --> 00:12:41.920
So here I'm requiring the
Google Cloud Node Module.

00:12:41.920 --> 00:12:44.600
And then on my Vision object
I'm just calling a Detect.

00:12:44.600 --> 00:12:46.076
I'm passing it in my image file.

00:12:46.076 --> 00:12:47.575
And then it returns
the annotations.

00:12:47.575 --> 00:12:51.020
I tell it what types of feature
detection I want to run.

00:12:51.020 --> 00:12:54.535
I can also pass it, the URL, to
a file stored in Google Cloud

00:12:54.535 --> 00:12:55.035
Storage.

00:12:58.530 --> 00:12:59.630
So we talked about images.

00:12:59.630 --> 00:13:01.546
The next thing I want
to talk about is speech.

00:13:01.546 --> 00:13:03.580
So how many of you
have used the OK Google

00:13:03.580 --> 00:13:06.490
functionality on your phone?

00:13:06.490 --> 00:13:07.900
Looks like most of you.

00:13:07.900 --> 00:13:11.040
So what the Speech API does is
it lets developers integrate

00:13:11.040 --> 00:13:13.660
that functionality into
their own applications,

00:13:13.660 --> 00:13:16.150
lets you do speech to text
transcription in over 80

00:13:16.150 --> 00:13:19.300
languages.

00:13:19.300 --> 00:13:21.510
An app that's using it
is Azar, and they're

00:13:21.510 --> 00:13:24.400
a chat application that has
connected over 15 billion

00:13:24.400 --> 00:13:25.390
matches.

00:13:25.390 --> 00:13:27.920
And they're using the
Speech API anytime

00:13:27.920 --> 00:13:31.640
somebody sends an audio snippet
to someone else in a chat.

00:13:31.640 --> 00:13:33.430
And there's a lot of
potential use cases

00:13:33.430 --> 00:13:35.490
where you may want to
combine different cloud

00:13:35.490 --> 00:13:36.520
machine learning APIs.

00:13:36.520 --> 00:13:39.430
In this case, they're using
the Speech API in combination

00:13:39.430 --> 00:13:40.800
with the translation API.

00:13:40.800 --> 00:13:43.790
So whenever the two people
don't speak the same language,

00:13:43.790 --> 00:13:46.314
they'll transcribe the
audio with the Speech API.

00:13:46.314 --> 00:13:48.730
And then they'll translate it
into the other person's host

00:13:48.730 --> 00:13:52.478
language with the
translation API.

00:13:52.478 --> 00:13:54.610
So the best way to see
how the Speech API works

00:13:54.610 --> 00:13:55.420
is through a demo.

00:13:55.420 --> 00:13:58.770
And first I want to explain what
we're going to do in the demo.

00:13:58.770 --> 00:14:00.500
So we're going to
make a recording using

00:14:00.500 --> 00:14:04.037
SoX, which is a command
line utility for audio.

00:14:04.037 --> 00:14:06.620
And then we're going to create
our API request in a JSON file,

00:14:06.620 --> 00:14:09.150
and send the JSON request
to the Speech API.

00:14:09.150 --> 00:14:12.180
And then we will receive
the JSON response.

00:14:12.180 --> 00:14:16.308
So if we could switch
over to the demo.

00:14:16.308 --> 00:14:18.300
Looks like we got that here.

00:14:18.300 --> 00:14:20.774
So I'm going to
call my Bash script.

00:14:20.774 --> 00:14:22.190
And it's going to
ask me to record

00:14:22.190 --> 00:14:24.060
a five-second audio file.

00:14:24.060 --> 00:14:27.350
So I'm going to record
something right now.

00:14:27.350 --> 00:14:32.747
I built a demo of the Speech
API using SoX at Google I/O.

00:14:32.747 --> 00:14:34.580
So I'm going to send
that to the Speech API,

00:14:34.580 --> 00:14:37.400
and we can see here what
my JSON request looks like.

00:14:37.400 --> 00:14:39.762
We tell it our encoding type.

00:14:39.762 --> 00:14:41.220
We can see that it
did pretty well.

00:14:41.220 --> 00:14:42.680
It didn't get the
word built, maybe

00:14:42.680 --> 00:14:44.420
it's the sound system in here.

00:14:44.420 --> 00:14:47.800
I don't the demo of the Speech
API using socks at Google I/O.

00:14:47.800 --> 00:14:49.970
But it did get
Google I/O correct.

00:14:49.970 --> 00:14:53.470
But notice that it incorrectly
transcribed the word SoX, which

00:14:53.470 --> 00:14:55.850
makes sense because SoX
is a proper noun, that's

00:14:55.850 --> 00:14:59.330
the command line tool I used
to transcribe the audio.

00:14:59.330 --> 00:15:00.910
And that's where
the speech context

00:15:00.910 --> 00:15:02.880
parameter comes into play.

00:15:02.880 --> 00:15:04.620
So if you have any
sort of proper nouns

00:15:04.620 --> 00:15:06.540
that are specific
to your application

00:15:06.540 --> 00:15:08.615
that the API might not
otherwise recognize,

00:15:08.615 --> 00:15:09.990
like I wouldn't
normally expected

00:15:09.990 --> 00:15:12.310
to recognize the
proper noun SoX,

00:15:12.310 --> 00:15:15.500
you can pass it these
phrases as hints.

00:15:15.500 --> 00:15:18.330
So if I hop on over
here to Sublime,

00:15:18.330 --> 00:15:23.270
I'm actually going to add SoX
to that phrases suggestion.

00:15:23.270 --> 00:15:25.080
And I'm going to run
the same thing again.

00:15:25.080 --> 00:15:30.040
I'm going to make
the same recording.

00:15:30.040 --> 00:15:35.570
I built a demo of the Speech
API using SoX at Google I/O.

00:15:35.570 --> 00:15:38.700
And we can see it's now looking
for that word in my text,

00:15:38.700 --> 00:15:42.040
and I'm going to send
it to the Speech API.

00:15:42.040 --> 00:15:44.030
It did pretty well.

00:15:44.030 --> 00:15:46.150
Again, might be
the sound in here.

00:15:46.150 --> 00:15:47.910
But it did get the
proper noun, thank you.

00:15:47.910 --> 00:15:51.410
[APPLAUSE]

00:15:51.410 --> 00:15:53.630
You never know with live demos.

00:15:53.630 --> 00:15:57.520
So again, just to reiterate,
I added this speech context

00:15:57.520 --> 00:16:00.560
parameter so that it was
able to identify correctly

00:16:00.560 --> 00:16:04.140
this proper noun that I
wanted to say in my text.

00:16:04.140 --> 00:16:05.730
So if we could go
back to the slides.

00:16:09.144 --> 00:16:12.540
If we go back to the slides.

00:16:12.540 --> 00:16:13.260
Awesome.

00:16:13.260 --> 00:16:16.010
So in this demo, I used
a complete audio file.

00:16:16.010 --> 00:16:17.860
So I used it in batch mode.

00:16:17.860 --> 00:16:19.710
But it also works
in streaming mode.

00:16:19.710 --> 00:16:22.490
So if you want to send it a
continuous stream of audio,

00:16:22.490 --> 00:16:24.190
you can send it
stream of audio and it

00:16:24.190 --> 00:16:28.120
will return transcriptions
as that audio is coming in.

00:16:28.120 --> 00:16:29.630
So that's the Speech API.

00:16:29.630 --> 00:16:31.830
Once you're done
transcribing your text,

00:16:31.830 --> 00:16:35.752
you may want to do more
analysis on your text.

00:16:35.752 --> 00:16:37.210
One more thing I
want to talk about

00:16:37.210 --> 00:16:39.510
is how you can call the
Speech API from Node.

00:16:39.510 --> 00:16:42.610
And so here I've required
the Google Cloud module.

00:16:42.610 --> 00:16:45.090
And I just need to tell it the
encoding type, in this case

00:16:45.090 --> 00:16:48.300
linear 16, and then the
sample rate in hertz.

00:16:48.300 --> 00:16:50.480
And then I call
speech.recognize,

00:16:50.480 --> 00:16:52.050
passing it my local file.

00:16:52.050 --> 00:16:56.732
Again I can also pass it a file
stored in Google Cloud Storage.

00:16:56.732 --> 00:16:58.190
So as I mentioned
before, you might

00:16:58.190 --> 00:17:00.155
want to do something
more with your text

00:17:00.155 --> 00:17:01.280
once you've transcribed it.

00:17:01.280 --> 00:17:02.488
You might want to analyze it.

00:17:02.488 --> 00:17:04.995
And that's where the Natural
Language API comes into play.

00:17:04.995 --> 00:17:06.369
The Natural Language
API lets you

00:17:06.369 --> 00:17:09.384
extracts entities, sentiment,
and syntax from your text.

00:17:12.349 --> 00:17:15.069
A customer that's using the API
is a company called Wootric,

00:17:15.069 --> 00:17:17.406
and they are a customer
feedback platform.

00:17:17.406 --> 00:17:19.280
And what they enable
all of their users to do

00:17:19.280 --> 00:17:21.530
is gather feedback
from their app's users

00:17:21.530 --> 00:17:24.407
as they're going throughout
their application.

00:17:24.407 --> 00:17:26.240
So you may have seen
something like that box

00:17:26.240 --> 00:17:28.710
in the top right when you're
on a website or an app.

00:17:28.710 --> 00:17:31.200
It asks you, how did you
feel about this experience

00:17:31.200 --> 00:17:33.500
on a scale of 0 to 10?

00:17:33.500 --> 00:17:36.130
So it's pretty easy for them
to parse those numbered scores.

00:17:36.130 --> 00:17:38.330
But Wootric's job is to
help their customers make

00:17:38.330 --> 00:17:40.660
sense of all this
open-ended feedback.

00:17:40.660 --> 00:17:42.390
And it's that
open-ended text feedback

00:17:42.390 --> 00:17:45.130
that's much more difficult
for them to make sense of.

00:17:45.130 --> 00:17:47.775
And that's where the Natural
Language API comes into play.

00:17:47.775 --> 00:17:49.680
And they're actually
using all the methods

00:17:49.680 --> 00:17:51.580
of that Natural Language API.

00:17:51.580 --> 00:17:54.250
So they're using sentiment
analysis to say OK,

00:17:54.250 --> 00:17:57.510
did the numbered score that
the person gave line up

00:17:57.510 --> 00:18:01.080
with the open-ended
feedback that they sent?

00:18:01.080 --> 00:18:04.260
And then they're also using
entity and syntax annotation

00:18:04.260 --> 00:18:07.620
to pull out the key subjects
and terms from the feedback,

00:18:07.620 --> 00:18:09.140
and then if
necessary, route those

00:18:09.140 --> 00:18:11.860
to the right person
in near real time

00:18:11.860 --> 00:18:13.261
to respond to the feedback.

00:18:13.261 --> 00:18:14.760
So let's say there
is a customer who

00:18:14.760 --> 00:18:17.370
wrote in and was really
angry about usability

00:18:17.370 --> 00:18:18.662
in this application.

00:18:18.662 --> 00:18:20.370
They could route that
to the right person

00:18:20.370 --> 00:18:22.500
to make sure they got
a response right away,

00:18:22.500 --> 00:18:24.360
rather than having
to manually review

00:18:24.360 --> 00:18:26.820
each piece of open-ended text.

00:18:26.820 --> 00:18:30.060
So I want to dive into each
method of the Natural Language

00:18:30.060 --> 00:18:32.990
API, starting with
entity analysis.

00:18:32.990 --> 00:18:35.050
So I've got this
sentence and I sent it

00:18:35.050 --> 00:18:37.765
to the entity extraction
endpoint of the Natural

00:18:37.765 --> 00:18:39.620
Language API.

00:18:39.620 --> 00:18:43.130
And it returned all of these
as entities in my text.

00:18:43.130 --> 00:18:45.490
And I wanted to show you
the JSON response for some

00:18:45.490 --> 00:18:48.049
of these entities, so we can
see that for each entity,

00:18:48.049 --> 00:18:49.340
we get the name of the entity--

00:18:49.340 --> 00:18:50.610
in this case, Google.

00:18:50.610 --> 00:18:53.120
The type of the
entity, organization.

00:18:53.120 --> 00:18:54.970
And then we get
back some metadata.

00:18:54.970 --> 00:18:56.790
So you'll notice this MID.

00:18:56.790 --> 00:18:59.860
This is an ID that maps to
Google's Knowledge Graph.

00:18:59.860 --> 00:19:02.310
And if you want to get more
information about the entity,

00:19:02.310 --> 00:19:06.005
you can call Google's Knowledge
Graph API, passing it this ID.

00:19:06.005 --> 00:19:10.940
And we also get the Wikipedia
URL for this particular entity.

00:19:10.940 --> 00:19:15.465
We can see it correctly
identifies I/O as an event.

00:19:15.465 --> 00:19:18.090
And then we can see the response
for some of the other entities

00:19:18.090 --> 00:19:18.560
here.

00:19:18.560 --> 00:19:20.518
And what you'll notice
is that my name is here,

00:19:20.518 --> 00:19:23.120
is still returned as an
entity even though I don't

00:19:23.120 --> 00:19:25.660
have a Wikipedia page yet.

00:19:25.660 --> 00:19:27.760
So if you have
entities in your text

00:19:27.760 --> 00:19:29.820
that might not have
associated metadata,

00:19:29.820 --> 00:19:33.011
the API will still be able
to extract those as key terms

00:19:33.011 --> 00:19:34.010
and give you a response.

00:19:34.010 --> 00:19:37.790
It'll just have an
empty metadata object.

00:19:37.790 --> 00:19:41.436
So that is entity analysis.

00:19:41.436 --> 00:19:43.060
The next thing that
the API lets you do

00:19:43.060 --> 00:19:45.210
is analyze the
sentiment of your text.

00:19:45.210 --> 00:19:47.140
So if we have this
restaurant review,

00:19:47.140 --> 00:19:49.230
the food at that
restaurant had no taste,

00:19:49.230 --> 00:19:50.692
I will not be going back.

00:19:50.692 --> 00:19:52.150
If I worked at this
restaurant, I'd

00:19:52.150 --> 00:19:53.660
probably want to
flag this review

00:19:53.660 --> 00:19:55.493
and potentially follow
up with this customer

00:19:55.493 --> 00:19:57.080
to see why they didn't like it.

00:19:57.080 --> 00:19:59.610
But it's likely that I would
have lots and lots of reviews,

00:19:59.610 --> 00:20:02.637
and I probably wouldn't want
to read each one manually.

00:20:02.637 --> 00:20:04.970
I might want to flag the most
positive and most negative

00:20:04.970 --> 00:20:08.100
ones, and then
respond just to those.

00:20:08.100 --> 00:20:10.570
So we get two numbers back
from the Natural Language API

00:20:10.570 --> 00:20:11.980
to help us do this.

00:20:11.980 --> 00:20:15.010
The first thing we get back
is score, which will tell us

00:20:15.010 --> 00:20:18.010
on a scale from negative 1 to
1, how positive or negative

00:20:18.010 --> 00:20:19.260
is this text?

00:20:19.260 --> 00:20:21.150
In this example, we
get negative 0.8,

00:20:21.150 --> 00:20:23.300
which is almost fully negative.

00:20:23.300 --> 00:20:25.660
And then we get
magnitude, which tells us

00:20:25.660 --> 00:20:27.930
regardless of being
positive or negative,

00:20:27.930 --> 00:20:30.500
how strong is the
sentiment in this text?

00:20:30.500 --> 00:20:32.750
And this is a range
from 0 to infinity,

00:20:32.750 --> 00:20:35.420
and it's normalized based
on the length of the text.

00:20:35.420 --> 00:20:37.800
So we get a pretty
small number here, 0.8,

00:20:37.800 --> 00:20:39.920
because this is just
a small piece of text.

00:20:39.920 --> 00:20:42.862
So that's sentiment analysis.

00:20:42.862 --> 00:20:45.320
And the final thing we can do
with the Natural Language API

00:20:45.320 --> 00:20:46.960
is analyze syntax.

00:20:46.960 --> 00:20:49.960
And this is going to give us
a lot more linguistic details

00:20:49.960 --> 00:20:51.460
about the piece of text.

00:20:51.460 --> 00:20:54.290
So here we have the sentence,
the Natural Language API

00:20:54.290 --> 00:20:56.620
helps us understand text.

00:20:56.620 --> 00:20:59.180
And what I'm going to show you
here is a visualization of all

00:20:59.180 --> 00:21:01.320
the JSON we get back.

00:21:01.320 --> 00:21:03.460
So the first thing we
get back is what we

00:21:03.460 --> 00:21:05.580
call a dependency parse tree.

00:21:05.580 --> 00:21:07.790
And this tells us which
words in a sentence

00:21:07.790 --> 00:21:13.000
depend on each other, what
word depends on this word.

00:21:13.000 --> 00:21:15.300
Then we get the parse
label, which will tell us,

00:21:15.300 --> 00:21:18.130
what is the role of each
word in the sentence?

00:21:18.130 --> 00:21:20.570
So helps is the root
verb in this case.

00:21:20.570 --> 00:21:22.665
API and us are nominal subjects.

00:21:25.200 --> 00:21:27.450
And then we get the part of
speech which will tell us,

00:21:27.450 --> 00:21:30.174
is this a noun, a
verb, or an adjective?

00:21:30.174 --> 00:21:31.090
Then we get the lemma.

00:21:31.090 --> 00:21:34.869
The lemma is the canonical
form of the word.

00:21:34.869 --> 00:21:36.660
So in this case, we
only get one for helps.

00:21:36.660 --> 00:21:39.850
The canonical form
of helps is help.

00:21:39.850 --> 00:21:41.930
This is useful if
you're counting maybe

00:21:41.930 --> 00:21:44.810
how many times a specific
word or term is being

00:21:44.810 --> 00:21:46.197
used to describe your product.

00:21:46.197 --> 00:21:47.780
You probably don't
want to count helps

00:21:47.780 --> 00:21:49.100
and help as two
different things.

00:21:49.100 --> 00:21:51.016
You probably want to use
the lemma form to get

00:21:51.016 --> 00:21:54.422
the canonical form of the word.

00:21:54.422 --> 00:21:56.880
And then finally, we get some
additional morphology details

00:21:56.880 --> 00:21:57.880
on the text.

00:21:57.880 --> 00:21:59.700
So is it first, second,
or third person?

00:21:59.700 --> 00:22:01.270
Is it plural or singular?

00:22:01.270 --> 00:22:03.090
And these are
going to vary based

00:22:03.090 --> 00:22:05.760
on the language of the text that
you send the Natural Language

00:22:05.760 --> 00:22:06.260
API.

00:22:09.300 --> 00:22:12.110
So I want to show you a demo
of the Natural Language API.

00:22:12.110 --> 00:22:15.300
And how this demo works is I
am using the Twitter Streaming

00:22:15.300 --> 00:22:18.620
API to stream tweets
related to Google I/O using

00:22:18.620 --> 00:22:20.910
a couple of the search
terms that you see here--

00:22:20.910 --> 00:22:22.950
Google I/O, Firebase
Cloud, TensorFlow

00:22:22.950 --> 00:22:24.740
and machine learning.

00:22:24.740 --> 00:22:26.780
I'm sending the
text of these tweets

00:22:26.780 --> 00:22:28.670
to the Natural Language API.

00:22:28.670 --> 00:22:32.590
And I'm using the sentiment
and syntax annotation methods.

00:22:32.590 --> 00:22:35.432
I'm writing that JSON response
to the Firebase database.

00:22:35.432 --> 00:22:36.890
And I'm using the
Firebase database

00:22:36.890 --> 00:22:39.310
to only store the most
recent tweets so that I can

00:22:39.310 --> 00:22:41.320
display a dashboard of them.

00:22:41.320 --> 00:22:43.200
And then I'm writing
all of my tweets

00:22:43.200 --> 00:22:46.020
to BigQuery, which is Google
Cloud's big data as a service

00:22:46.020 --> 00:22:49.150
tool, lets you easily query
lots and lots of data.

00:22:49.150 --> 00:22:51.890
So I've got two separate
processes running for that.

00:22:51.890 --> 00:22:55.750
So if we could
switch to the demo.

00:22:55.750 --> 00:22:58.120
I'm going to go ahead and
start up my Node server.

00:22:58.120 --> 00:23:00.530
And I'm going to go over here.

00:23:00.530 --> 00:23:04.680
And what we see here is we have
the latest tweets displaying.

00:23:04.680 --> 00:23:06.874
We should see some
start streaming soon.

00:23:06.874 --> 00:23:08.790
I'm not displaying the
full text of the tweets

00:23:08.790 --> 00:23:10.820
because this is a
public event, so what

00:23:10.820 --> 00:23:14.120
could go wrong with displaying
tweets on a giant screen?

00:23:14.120 --> 00:23:17.000
And we've got the nouns, verbs,
and adjectives in each tweet.

00:23:17.000 --> 00:23:19.560
There we go, we
got some new ones.

00:23:19.560 --> 00:23:22.680
And it's displaying the
sentiment of the latest tweets.

00:23:22.680 --> 00:23:26.300
So it goes from negative 1 to
1, how positive or negative

00:23:26.300 --> 00:23:27.460
is this tweet?

00:23:27.460 --> 00:23:29.240
And then we can see--

00:23:29.240 --> 00:23:30.650
make this a little bit bigger.

00:23:30.650 --> 00:23:33.200
We can see the
sentiment by hashtag.

00:23:33.200 --> 00:23:35.730
And we can also see the
most common adjectives

00:23:35.730 --> 00:23:38.910
that are being used
in the latest tweets.

00:23:38.910 --> 00:23:39.940
So that's pretty cool.

00:23:39.940 --> 00:23:43.290
But the other thing I did was
I am sending all of my tweets

00:23:43.290 --> 00:23:46.090
to BigQuery for some
historical analysis.

00:23:46.090 --> 00:23:48.300
So if I open up the
BigQuery web UI here,

00:23:48.300 --> 00:23:51.750
we use SQL to write
queries in BigQuery.

00:23:51.750 --> 00:23:53.352
We can see all of
the tweets that I've

00:23:53.352 --> 00:23:55.060
collected since I
started streaming this,

00:23:55.060 --> 00:23:56.410
which is a couple of days ago.

00:23:56.410 --> 00:24:00.140
So I've collected, in
total, over 54,000 tweets.

00:24:00.140 --> 00:24:02.360
And this is what my data
looks like in BigQuery.

00:24:02.360 --> 00:24:04.540
I can visualize it and
write queries directly

00:24:04.540 --> 00:24:05.585
in the browser.

00:24:05.585 --> 00:24:08.440
So this is what each row
in my table looks like.

00:24:08.440 --> 00:24:10.530
I've got the ID of
the tweet, the text

00:24:10.530 --> 00:24:14.970
of the tweet, the timestamp
of when it was created,

00:24:14.970 --> 00:24:17.640
the number of
followers the user has,

00:24:17.640 --> 00:24:20.540
the hashtags, which is
returned from the Twitter API,

00:24:20.540 --> 00:24:23.550
and then I have this
giant JSON string

00:24:23.550 --> 00:24:26.210
of all the syntax annotation
data that was returned

00:24:26.210 --> 00:24:28.200
from the Natural Language API.

00:24:28.200 --> 00:24:33.030
You might be wondering, how am
I going to parse that using SQL?

00:24:33.030 --> 00:24:36.026
Well, BigQuery has a feature
called user-defined functions,

00:24:36.026 --> 00:24:38.400
which lets you write custom
JavaScript functions that can

00:24:38.400 --> 00:24:40.337
be run on rows in your table.

00:24:40.337 --> 00:24:41.670
So I've written a function here.

00:24:41.670 --> 00:24:43.750
I want to count all
the adjectives that

00:24:43.750 --> 00:24:46.360
have been used in all these
tweets that I've streamed.

00:24:46.360 --> 00:24:49.250
So this is going to be run over
that syntax annotation JSON

00:24:49.250 --> 00:24:49.750
string.

00:24:49.750 --> 00:24:51.640
It's going to parse
that JSON string

00:24:51.640 --> 00:24:55.367
and count all the adjectives
in all of my tweets so far.

00:24:55.367 --> 00:24:56.950
So I'm going to go
ahead and run this.

00:24:56.950 --> 00:24:59.230
Keep in mind it's running
this custom function on all

00:24:59.230 --> 00:25:03.620
54,000-plus rows in my table.

00:25:03.620 --> 00:25:06.240
And we can see it finished
in just under eight seconds.

00:25:06.240 --> 00:25:09.920
And if we look down here, we can
see the most common adjectives

00:25:09.920 --> 00:25:13.000
that are being used to
describe Google I/O and machine

00:25:13.000 --> 00:25:15.560
learning.

00:25:15.560 --> 00:25:17.990
And one other thing I wanted
to show you-- thank you.

00:25:17.990 --> 00:25:20.816
[APPLAUSE]

00:25:23.231 --> 00:25:24.730
One other thing I
wanted to show you

00:25:24.730 --> 00:25:27.800
is, this is the Natural
Language API Products Page.

00:25:27.800 --> 00:25:29.060
And you can all go here--

00:25:29.060 --> 00:25:30.410
I'll share the link at the end--

00:25:30.410 --> 00:25:32.984
and try out the Natural
Language API on your own text.

00:25:32.984 --> 00:25:34.400
And I wanted to
show you a feature

00:25:34.400 --> 00:25:36.850
that we added to the
API just two weeks ago.

00:25:36.850 --> 00:25:38.694
And this is
entity-based sentiment.

00:25:38.694 --> 00:25:40.360
So within a sentence,
this will give you

00:25:40.360 --> 00:25:43.330
the sentiment of specific
entities in your text.

00:25:43.330 --> 00:25:47.490
So if I have a sentence like,
I really liked the sushi,

00:25:47.490 --> 00:25:52.380
but the service was terrible,
I'm going to send this to API.

00:25:52.380 --> 00:25:57.320
And we can see that
sushi got a score of 0.8.

00:25:57.320 --> 00:25:59.507
Service got a score
of negative 0.9.

00:25:59.507 --> 00:26:01.090
And that's really
useful, because if I

00:26:01.090 --> 00:26:03.170
had sent this
sentence to the API

00:26:03.170 --> 00:26:05.620
and got a sentiment
analysis response

00:26:05.620 --> 00:26:07.220
for the whole
sentence, it probably

00:26:07.220 --> 00:26:08.428
wouldn't be too useful to me.

00:26:08.428 --> 00:26:10.270
What I really wanted
to know is what

00:26:10.270 --> 00:26:14.369
this person liked about specific
entities in my sentence.

00:26:14.369 --> 00:26:15.660
So we can go back to the sides.

00:26:17.797 --> 00:26:19.630
So with this demo, I
just wanted to show you

00:26:19.630 --> 00:26:21.400
how you can combine
Firebase, again,

00:26:21.400 --> 00:26:24.040
with Google Cloud Platform,
using Firebase to create

00:26:24.040 --> 00:26:27.380
a real time dashboard
of my latest tweet data,

00:26:27.380 --> 00:26:30.380
and then BigQuery so that I
can do a historical analysis

00:26:30.380 --> 00:26:33.090
once I'm done gathering the data
on all the data I have received

00:26:33.090 --> 00:26:34.561
so far.

00:26:34.561 --> 00:26:37.060
Another thing that you might
want to do with your text-- oh,

00:26:37.060 --> 00:26:38.780
just forgot, the
code snippet here.

00:26:38.780 --> 00:26:40.571
So if you want to call
the Natural Language

00:26:40.571 --> 00:26:42.300
API from Node.js,
here's a snippet

00:26:42.300 --> 00:26:44.090
of code of how
you would do that.

00:26:44.090 --> 00:26:47.110
In this example, I am passing
it a file from Google Cloud

00:26:47.110 --> 00:26:48.090
Storage.

00:26:48.090 --> 00:26:50.250
You can also pass it raw text.

00:26:50.250 --> 00:26:53.270
And just call language.annotate,
and it'll give you

00:26:53.270 --> 00:26:54.800
your annotation response back.

00:26:57.071 --> 00:26:59.320
So the next thing you might
want to do with your text,

00:26:59.320 --> 00:27:01.330
once you're done
analyzing it, is

00:27:01.330 --> 00:27:04.140
you might want to translate it
into many different languages.

00:27:04.140 --> 00:27:06.922
You likely have users for your
application all over the world.

00:27:06.922 --> 00:27:09.130
You want to make sure your
app is accessible to them,

00:27:09.130 --> 00:27:10.780
wherever they are.

00:27:10.780 --> 00:27:13.430
With the Translation API,
you can translate your text

00:27:13.430 --> 00:27:16.544
in over 100 languages.

00:27:16.544 --> 00:27:18.210
Before I get into
that, I wanted to talk

00:27:18.210 --> 00:27:19.720
a bit about Google Translate.

00:27:19.720 --> 00:27:22.280
How many of you have used
Google Translate before?

00:27:22.280 --> 00:27:24.360
Looks like almost
all hands went up.

00:27:24.360 --> 00:27:26.720
I rely on it a
bunch when I travel.

00:27:26.720 --> 00:27:28.650
Last year I was on
a trip to Japan,

00:27:28.650 --> 00:27:31.520
and I was at a restaurant where
almost nobody spoke English.

00:27:31.520 --> 00:27:33.150
And I really wanted
to order octopus.

00:27:33.150 --> 00:27:35.000
So I got out my handy
dandy Google Translate

00:27:35.000 --> 00:27:37.110
app, typed in the
word for octopus,

00:27:37.110 --> 00:27:38.785
and found out that it was taco.

00:27:38.785 --> 00:27:41.690
So I was a little confused,
but I decided to roll with it,

00:27:41.690 --> 00:27:45.650
order my taco, and I was
able to get this delicious

00:27:45.650 --> 00:27:48.150
octopus as a result.
So Google Translate

00:27:48.150 --> 00:27:50.950
is very handy for more things
than just translating the word

00:27:50.950 --> 00:27:52.160
octopus.

00:27:52.160 --> 00:27:53.860
And with the
Translation API, you

00:27:53.860 --> 00:27:56.020
can integrate the functionality
of Google Translate

00:27:56.020 --> 00:27:59.360
into your own applications.

00:27:59.360 --> 00:28:02.250
And one company that's
doing that is Airbnb.

00:28:02.250 --> 00:28:04.000
What you might not
know about Airbnb

00:28:04.000 --> 00:28:06.660
is that 60% of
their bookings are

00:28:06.660 --> 00:28:08.290
between people that
use the application

00:28:08.290 --> 00:28:10.046
in different languages.

00:28:10.046 --> 00:28:11.420
They're using the
Translation API

00:28:11.420 --> 00:28:13.380
not only to translate
the listings,

00:28:13.380 --> 00:28:16.130
but also to translate the
reviews and conversations.

00:28:16.130 --> 00:28:18.880
And they've found that
using the Translation API

00:28:18.880 --> 00:28:21.430
has significantly improved a
guest's likelihood to book.

00:28:23.637 --> 00:28:26.220
Here's a snippet of code of how
you would call the Translation

00:28:26.220 --> 00:28:28.170
API for Node.js.

00:28:28.170 --> 00:28:29.920
And notice here that
I'm only passing

00:28:29.920 --> 00:28:32.460
the string I want to translate
along with the language

00:28:32.460 --> 00:28:34.049
that I want to translate it to.

00:28:34.049 --> 00:28:35.840
I actually don't need
to tell it the source

00:28:35.840 --> 00:28:36.991
language of the text.

00:28:36.991 --> 00:28:38.990
So another thing you can
do with the Translation

00:28:38.990 --> 00:28:41.860
API is if you just want to
detect the language of the text

00:28:41.860 --> 00:28:45.800
that a user's typing in, you
can use API for that as well.

00:28:45.800 --> 00:28:47.880
So I pass it my text
and the language

00:28:47.880 --> 00:28:50.560
I want to translate it to, and
I get the translation back.

00:28:53.290 --> 00:28:54.940
Recently, we made
some improvements

00:28:54.940 --> 00:28:58.080
to the underlying model
behind the Translation API.

00:28:58.080 --> 00:29:01.174
And this new version is called
neural machine translation.

00:29:01.174 --> 00:29:03.090
You'll just notice a
difference in the quality

00:29:03.090 --> 00:29:05.110
of the translations
as you send your text

00:29:05.110 --> 00:29:07.020
through the Translation API.

00:29:07.020 --> 00:29:09.390
So the first model of
translation that we use,

00:29:09.390 --> 00:29:11.580
called first
generation translation,

00:29:11.580 --> 00:29:13.850
the way that works is
it translated each word

00:29:13.850 --> 00:29:15.390
in the sentence word for word.

00:29:15.390 --> 00:29:17.960
So this might be similar to how
you would translate something

00:29:17.960 --> 00:29:20.001
if you were traveling,
had a dictionary with you,

00:29:20.001 --> 00:29:21.710
and you wanted to
look up a sentence.

00:29:21.710 --> 00:29:24.390
You might translate each
word, word for word.

00:29:24.390 --> 00:29:26.884
And the result would
probably be pretty accurate,

00:29:26.884 --> 00:29:28.925
but you might miss some
of the linguistic nuances

00:29:28.925 --> 00:29:30.760
of that text.

00:29:30.760 --> 00:29:32.510
And what neural machine
translation does

00:29:32.510 --> 00:29:35.460
is it takes each word and
looks at it in the context

00:29:35.460 --> 00:29:37.560
of other words in a sentence.

00:29:37.560 --> 00:29:40.047
And it's able to produce a
much more accurate translation.

00:29:40.047 --> 00:29:42.130
There's a great "New York
Times" article about it.

00:29:42.130 --> 00:29:43.360
I've got a bit.ly
link right there

00:29:43.360 --> 00:29:45.943
if you're interested in learning
more about how neural machine

00:29:45.943 --> 00:29:48.540
translation works
under the hood.

00:29:48.540 --> 00:29:50.899
And just to show you what
the improvements look

00:29:50.899 --> 00:29:52.940
like, I know there's a
lot of text on this slide,

00:29:52.940 --> 00:29:54.085
I'll explain it.

00:29:54.085 --> 00:29:55.960
As I mentioned, I'm a
big "Harry Potter" fan.

00:29:55.960 --> 00:29:57.751
And so what we have
all the way on the left

00:29:57.751 --> 00:30:00.430
is a paragraph from the
original Spanish version

00:30:00.430 --> 00:30:01.670
of "Harry Potter."

00:30:01.670 --> 00:30:05.710
And obviously, "Harry Potter"
was not translated via an API.

00:30:05.710 --> 00:30:07.632
It's been translated
in over 70 languages.

00:30:07.632 --> 00:30:09.090
And they actually
had somebody that

00:30:09.090 --> 00:30:12.810
spoke each language in charge of
writing the translated version.

00:30:12.810 --> 00:30:15.095
So this is from the
original translated version.

00:30:15.095 --> 00:30:16.470
And then in the
middle column, we

00:30:16.470 --> 00:30:19.500
see what first generation
translation produced

00:30:19.500 --> 00:30:21.177
when I translate it to English.

00:30:21.177 --> 00:30:23.760
On the right-hand side, we see
what neural machine translation

00:30:23.760 --> 00:30:24.570
produced.

00:30:24.570 --> 00:30:26.370
And I just bolded some
of the differences.

00:30:26.370 --> 00:30:30.010
So we can see that the verb made
was changed to manufactured,

00:30:30.010 --> 00:30:33.510
which is much more specific to
the context of the sentence.

00:30:33.510 --> 00:30:35.710
And we can see if we
skip to the bottom, fence

00:30:35.710 --> 00:30:38.470
of the gardens changed
to garden fence.

00:30:38.470 --> 00:30:41.637
So lots of improvements with
neural machine translation.

00:30:44.440 --> 00:30:46.310
The final API I want
to talk about today

00:30:46.310 --> 00:30:48.240
is the Video Intelligence API.

00:30:48.240 --> 00:30:50.880
And this is the newest addition
to our suite of Cloud Machine

00:30:50.880 --> 00:30:52.250
Learning APIs.

00:30:52.250 --> 00:30:54.030
What the API lets
you do is understand

00:30:54.030 --> 00:30:59.320
your video's entities at
shot, frame, and video level.

00:30:59.320 --> 00:31:01.290
A company that's
using this is Cantemo.

00:31:01.290 --> 00:31:03.630
They are a media asset
management company.

00:31:03.630 --> 00:31:06.930
And all of their customers have
lots and lots of video content.

00:31:06.930 --> 00:31:09.780
So they upload their content
to Cantemo's platform.

00:31:09.780 --> 00:31:12.500
Cantemo helps them
transcode it, edit it.

00:31:12.500 --> 00:31:14.550
And what they're doing
with the Video API

00:31:14.550 --> 00:31:16.170
is they're enabling
their customers

00:31:16.170 --> 00:31:18.170
to make their video
content searchable so they

00:31:18.170 --> 00:31:21.384
can search within their videos.

00:31:21.384 --> 00:31:23.550
The best way to see the
Video API is through a demo.

00:31:23.550 --> 00:31:27.470
So if we could switch
back to the demo.

00:31:27.470 --> 00:31:29.700
What I've got here is
a Superbowl commercial

00:31:29.700 --> 00:31:32.000
for Google Home from
this past Superbowl.

00:31:32.000 --> 00:31:36.480
And I'm going to play just
the first few seconds of this.

00:31:36.480 --> 00:31:39.600
So what we can see here is a
lot of scene changes happening

00:31:39.600 --> 00:31:41.335
in this video.

00:31:41.335 --> 00:31:42.710
I'm going to stop
it here, but we

00:31:42.710 --> 00:31:45.780
can see that it started with
a mountain pass, switched

00:31:45.780 --> 00:31:49.610
to a house, then a street scene,
then we saw a dog and a garage.

00:31:49.610 --> 00:31:51.840
So if we were to manually
transcribe this video,

00:31:51.840 --> 00:31:54.204
we'd have to watch what was
happening in every scene

00:31:54.204 --> 00:31:55.870
and write down those
tags and store them

00:31:55.870 --> 00:31:57.275
in a database somewhere.

00:31:57.275 --> 00:31:58.900
What we get with the
Video Intelligence

00:31:58.900 --> 00:32:01.870
API is we get some
data at a high level,

00:32:01.870 --> 00:32:03.440
what is this video about?

00:32:03.440 --> 00:32:05.290
And then at a more
granular level, what

00:32:05.290 --> 00:32:07.155
is happening in every scene?

00:32:07.155 --> 00:32:09.280
So if we look down here,
we can see a visualization

00:32:09.280 --> 00:32:11.780
of the JSON response we get
back from the Video Intelligence

00:32:11.780 --> 00:32:12.720
API.

00:32:12.720 --> 00:32:14.457
So it tells us that
there's a dog here.

00:32:14.457 --> 00:32:16.790
And not only does it know
that there's a dog this video,

00:32:16.790 --> 00:32:20.200
it tells us exactly which
scene the dog appears.

00:32:20.200 --> 00:32:23.460
If we skip to the end, we
can see a birthday cake.

00:32:23.460 --> 00:32:25.220
And then if we
scroll down, we can

00:32:25.220 --> 00:32:27.220
see a couple of other
labels that were returned.

00:32:27.220 --> 00:32:29.280
So not only did it
know it was a dog,

00:32:29.280 --> 00:32:31.830
it knows exactly what
type of dog it is.

00:32:31.830 --> 00:32:33.920
And if we scroll
down a little more,

00:32:33.920 --> 00:32:35.670
we notice that it
identifies that mountain

00:32:35.670 --> 00:32:38.660
pass from the opening scene.

00:32:38.660 --> 00:32:40.730
So this is what the API
can do with one video.

00:32:40.730 --> 00:32:42.230
But if you're
analyzing videos, it's

00:32:42.230 --> 00:32:44.721
likely that you have lots
and lots of video content.

00:32:44.721 --> 00:32:46.220
So you could be a
media company that

00:32:46.220 --> 00:32:47.950
has hundreds of
petabytes of videos

00:32:47.950 --> 00:32:49.292
sitting in storage buckets.

00:32:49.292 --> 00:32:51.000
And one common thing
you might want to do

00:32:51.000 --> 00:32:52.485
is create a highlight reel.

00:32:52.485 --> 00:32:54.510
You might want to look
for a specific entity

00:32:54.510 --> 00:32:57.620
and pull that out across
your entire video library.

00:32:57.620 --> 00:32:59.070
And with the JSON
data we get back

00:32:59.070 --> 00:33:00.640
from the Video
Intelligence API, it

00:33:00.640 --> 00:33:02.970
makes this really easy to do.

00:33:02.970 --> 00:33:06.240
So if I go here, I've got quite
a few videos in my library.

00:33:06.240 --> 00:33:08.430
And let's say I'm a
sports media company.

00:33:08.430 --> 00:33:10.960
So I have lots and
lots of sports footage.

00:33:10.960 --> 00:33:15.020
I only want to find the
scenes relevant to baseball.

00:33:15.020 --> 00:33:17.390
Oops, go back here.

00:33:17.390 --> 00:33:19.804
I'm going to search
for baseball.

00:33:19.804 --> 00:33:21.220
And we can see
that, boom, the API

00:33:21.220 --> 00:33:24.040
tells us immediately which
videos have baseball in them.

00:33:24.040 --> 00:33:27.030
And not only does it tell us
which videos have baseball,

00:33:27.030 --> 00:33:30.802
it tells us exactly where in
those videos baseball appears.

00:33:30.802 --> 00:33:32.260
So we can see in
this video there's

00:33:32.260 --> 00:33:33.931
lots of baseball scenes.

00:33:33.931 --> 00:33:35.430
But this one is my
favorite example,

00:33:35.430 --> 00:33:38.810
because there's only one, tiny,
one-second clip of baseball

00:33:38.810 --> 00:33:39.446
in this video.

00:33:39.446 --> 00:33:40.820
This is the year-end
search video

00:33:40.820 --> 00:33:42.778
that Google publishes at
the end of every year,

00:33:42.778 --> 00:33:45.567
highlighting top searches
from throughout the year.

00:33:45.567 --> 00:33:47.400
So if we were to look
through this manually,

00:33:47.400 --> 00:33:50.860
we'd have to watch the entire
video just for this one scene.

00:33:50.860 --> 00:33:54.340
With the Video API, we can
skip directly to that scene.

00:33:54.340 --> 00:33:57.630
And that's from last year's
World Series when the Cubs won.

00:33:57.630 --> 00:34:00.130
And we might have even missed
that scene if we were manually

00:34:00.130 --> 00:34:01.950
reviewing the video.

00:34:01.950 --> 00:34:03.880
So I'm going to do
one more search.

00:34:03.880 --> 00:34:07.380
Since we're at Google I/O, I'm
going to search for Android.

00:34:07.380 --> 00:34:11.100
I'm going to find all the
Android phones in my videos.

00:34:11.100 --> 00:34:12.699
And we can see that
I'm jumping here

00:34:12.699 --> 00:34:16.040
and I can see exactly
all the places

00:34:16.040 --> 00:34:20.050
where Android phones appear in
my video, which is pretty cool.

00:34:20.050 --> 00:34:22.420
So with the Video Intelligence
API, what used to take

00:34:22.420 --> 00:34:24.239
hours now takes minutes.

00:34:24.239 --> 00:34:28.210
And it's easy to search a
large library of video content.

00:34:28.210 --> 00:34:30.469
If we could go
back to the slides?

00:34:30.469 --> 00:34:30.969
Thank you.

00:34:30.969 --> 00:34:33.294
[APPLAUSE]

00:34:36.249 --> 00:34:37.790
I wanted to share
a little bit of how

00:34:37.790 --> 00:34:39.639
that demo works under the hood.

00:34:39.639 --> 00:34:42.000
All of the video content
is stored in a Google Cloud

00:34:42.000 --> 00:34:43.489
Storage bucket.

00:34:43.489 --> 00:34:45.830
And I've got a cloud function
listening on that bucket.

00:34:45.830 --> 00:34:47.451
So whenever a new
video is added,

00:34:47.451 --> 00:34:49.909
that cloud function is going
to send the video to the Video

00:34:49.909 --> 00:34:51.739
Intelligence API.

00:34:51.739 --> 00:34:53.429
And one of the
request parameters

00:34:53.429 --> 00:34:57.090
that I get with the Video API
is it lets me pass it a file

00:34:57.090 --> 00:34:59.220
URL of a Google Cloud
Storage file where I

00:34:59.220 --> 00:35:01.530
want to write my JSON response.

00:35:01.530 --> 00:35:03.547
So when it's done
annotating the video,

00:35:03.547 --> 00:35:05.880
I write that response to a
separate Google Cloud Storage

00:35:05.880 --> 00:35:07.110
bucket.

00:35:07.110 --> 00:35:09.280
And the front end
of this application

00:35:09.280 --> 00:35:11.607
is a Node.js app running
on Google App Engine.

00:35:11.607 --> 00:35:13.440
So the front end of the
app doesn't actually

00:35:13.440 --> 00:35:15.280
call the Video API directly.

00:35:15.280 --> 00:35:18.260
It simply references the videos
from one Cloud Storage bucket

00:35:18.260 --> 00:35:21.190
and then gets the associated
metadata from another Cloud

00:35:21.190 --> 00:35:21.900
Storage bucket.

00:35:25.069 --> 00:35:27.610
Here's what the JSON response
looks like for label detection.

00:35:27.610 --> 00:35:29.110
So we can see in
this video, there's

00:35:29.110 --> 00:35:30.520
a scene with a bird's eye view.

00:35:30.520 --> 00:35:33.400
It tells us what the
label is, bird's eye view.

00:35:33.400 --> 00:35:36.330
We get the start and end time
of this label in microseconds.

00:35:36.330 --> 00:35:38.650
And if this label appeared
in more than one scene,

00:35:38.650 --> 00:35:40.875
there'd be multiple
segment objects there.

00:35:40.875 --> 00:35:42.380
And we also get a
confident score.

00:35:42.380 --> 00:35:47.470
It's 96% confident that it's
identified this correctly.

00:35:47.470 --> 00:35:50.127
Here's another one from the
same video of a portrait label.

00:35:50.127 --> 00:35:52.210
We also get the start and
end time for this label.

00:35:55.299 --> 00:35:56.840
And here's some code
of how you would

00:35:56.840 --> 00:35:59.200
call the Video API for Node.js.

00:35:59.200 --> 00:36:01.620
The Video API's currently
in private beta.

00:36:04.692 --> 00:36:05.900
So that's all I've got today.

00:36:05.900 --> 00:36:07.370
That concludes my presentation.

00:36:07.370 --> 00:36:10.150
I really encourage you all
to start using the ML APIs,

00:36:10.150 --> 00:36:12.572
and you can try them all
directly in the browser.

00:36:12.572 --> 00:36:14.030
So if you go to
the browser, I only

00:36:14.030 --> 00:36:16.540
showed you the Natural Language
API demo in the browser.

00:36:16.540 --> 00:36:18.570
But all of the APIs
have a browser demo

00:36:18.570 --> 00:36:20.670
where you can add your
own data before you start

00:36:20.670 --> 00:36:24.430
writing any code and see all
of the API responses visualized

00:36:24.430 --> 00:36:26.100
directly right there.

00:36:26.100 --> 00:36:28.670
I've got the code for some of
the demos you saw in this talk

00:36:28.670 --> 00:36:30.470
in my personal GitHub.

00:36:30.470 --> 00:36:32.607
ML Talk Demos has the code
for most of the demos.

00:36:32.607 --> 00:36:34.190
I haven't yet added
the Firebase ones,

00:36:34.190 --> 00:36:36.080
but I hope to do that soon.

00:36:36.080 --> 00:36:38.190
And then the Video
API demo app code

00:36:38.190 --> 00:36:40.420
is in a separate repo
on my personal GitHub

00:36:40.420 --> 00:36:42.550
under Video Intelligence Demo.

00:36:42.550 --> 00:36:45.780
So I encourage you all to
try them out and get started.

00:36:45.780 --> 00:36:46.860
And thank you for coming.

00:36:46.860 --> 00:36:48.660
[APPLAUSE]

00:36:50.760 --> 00:36:56.210
[MUSIC PLAYING]

