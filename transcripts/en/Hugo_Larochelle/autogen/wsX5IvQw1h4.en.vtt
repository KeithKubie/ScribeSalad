WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.060
 in this capsule we will see our 

00:00:01.650 --> 00:00:08.340
 first learning algorithms by 

00:00:03.060 --> 00:00:11.099
 reinforcement assets then in the code 

00:00:08.340 --> 00:00:13.049
 of passive learning what we 

00:00:11.099 --> 00:00:15.480
 had to value the value for a 

00:00:13.049 --> 00:00:19.470
 plan that was near determined prices 

00:00:15.480 --> 00:00:21.150
 which was given to us in itself 

00:00:19.470 --> 00:00:24.019
 learning by reinforcement not 

00:00:21.150 --> 00:00:26.130
 this time particularly useful for 

00:00:24.019 --> 00:00:28.619
 since in fact we do not know what is it 

00:00:26.130 --> 00:00:31.050
 the optimal plan to follow 

00:00:28.619 --> 00:00:34.110
 he does it if we want to develop a good 

00:00:31.050 --> 00:00:35.550
 player in a game or a good controller 

00:00:34.110 --> 00:00:37.290
 for a helicopter if what we are looking for 

00:00:35.550 --> 00:00:40.010
 we are looking for the best strategy 

00:00:37.290 --> 00:00:43.010
 best policy to follow for good 

00:00:40.010 --> 00:00:45.899
 operate in a given environment 

00:00:43.010 --> 00:00:49.050
 so learning parents inevitably 

00:00:45.899 --> 00:00:52.289
 active in this case we do the agent 

00:00:49.050 --> 00:00:54.180
 precisely must find this optimal plan 

00:00:52.289 --> 00:00:55.800
 so we give him more a plan then to 

00:00:54.180 --> 00:00:57.180
 from simulations he has to determine 

00:00:55.800 --> 00:00:58.859
 his value 

00:00:57.180 --> 00:01:00.719
 the dance what if the agent has two 

00:00:58.859 --> 00:01:04.260
 objectives is to find the small plan 

00:01:00.719 --> 00:01:06.360
 male and its value so that's what is 

00:01:04.260 --> 00:01:10.320
 here we have to find the 

00:01:06.360 --> 00:01:13.530
 plan so badly and it works value so 

00:01:10.320 --> 00:01:14.750
 in this case v2 is what we put in 

00:01:13.530 --> 00:01:17.700
 day as and when 

00:01:14.750 --> 00:01:19.439
 learning is an estimate of 

00:01:17.700 --> 00:01:23.369
 the value function for the plan 

00:01:19.439 --> 00:01:25.979
 optima so in the case of the sentry box 

00:01:23.369 --> 00:01:29.009
 adaptive dynamic programming 

00:01:25.979 --> 00:01:33.000
 have done here we have two changes but 

00:01:29.009 --> 00:01:35.460
 three changes to actually surveyed 

00:01:33.000 --> 00:01:36.869
 very simple changes the first 

00:01:35.460 --> 00:01:39.600
 is that now when we go 

00:01:36.869 --> 00:01:40.950
 estimate the peace transition model of 

00:01:39.600 --> 00:01:43.710
 the same mind knowing 1 6 1 

00:01:40.950 --> 00:01:48.509
 it's going to be in fact for a state s 

00:01:43.710 --> 00:01:51.180
 gave for more than one action so 

00:01:48.509 --> 00:01:55.680
 in the passive coc because the action at 

00:01:51.180 --> 00:01:57.180
 it's going to be necessarily kid's okay 

00:01:55.680 --> 00:01:58.500
 to be necessarily the kim action is 

00:01:57.180 --> 00:02:01.130
 dictated by my policy 

00:01:58.500 --> 00:02:02.659
 actually in this way if I was estimating 

00:02:01.130 --> 00:02:04.670
 may 

00:02:02.659 --> 00:02:07.310
 my model other actions simply to 

00:02:04.670 --> 00:02:08.539
 simply go for a single action 

00:02:07.310 --> 00:02:10.700
 that is, the action of politics 

00:02:08.539 --> 00:02:13.400
 in this case also the agent will be able 

00:02:10.700 --> 00:02:14.900
 change action then try for a 

00:02:13.400 --> 00:02:16.190
 self is given several actions 

00:02:14.900 --> 00:02:17.930
 different so the other in that so 

00:02:16.190 --> 00:02:19.430
 we will have to estimate our model of 

00:02:17.930 --> 00:02:21.260
 transition for several actions 

00:02:19.430 --> 00:02:23.209
 possible and we'll do it again 

00:02:21.260 --> 00:02:25.129
 times from relative frequencies 

00:02:23.209 --> 00:02:27.319
 as we do before, that is to say in 

00:02:25.129 --> 00:02:29.629
 gold but in ten years the number of times that 

00:02:27.319 --> 00:02:30.080
 I was going for an action to leave and 

00:02:29.629 --> 00:02:31.970
 at s 

00:02:30.080 --> 00:02:33.799
 so once I went to 

00:02:31.970 --> 00:02:36.769
 the state expresses its being the same technique 

00:02:33.799 --> 00:02:38.450
 of estimating the other difference it's 

00:02:36.769 --> 00:02:41.709
 that we will apply worth a 

00:02:38.450 --> 00:02:44.959
 regeneration by value on our 

00:02:41.709 --> 00:02:47.989
 marco position comes we are going to have 

00:02:44.959 --> 00:02:48.680
 so estimated our model estimate 

00:02:47.989 --> 00:02:51.110
 of transition 

00:02:48.680 --> 00:02:52.640
 so in fact what we will do is 

00:02:51.110 --> 00:02:56.349
 solve the male equations for 

00:02:52.640 --> 00:02:59.060
 find out what is the optimal policy 

00:02:56.349 --> 00:03:01.010
 associated with my decision process 

00:02:59.060 --> 00:03:05.180
 brand returns as estimated by my 

00:03:01.010 --> 00:03:07.130
 model pay day springs has and at and 

00:03:05.180 --> 00:03:08.750
 finally the last difference is 

00:03:07.130 --> 00:03:11.030
 that now in hostage we will be able 

00:03:08.750 --> 00:03:12.620
 choose an action and the policy to 

00:03:11.030 --> 00:03:18.019
 any time that follows is the 

00:03:12.620 --> 00:03:20.329
 policy that is associated with a sum of 

00:03:18.019 --> 00:03:22.579
 future rewards expect the highest 

00:03:20.329 --> 00:03:26.660
 whose maximizing ends here 

00:03:22.579 --> 00:03:33.940
 according to our estimate of the model of 

00:03:26.660 --> 00:03:36.380
 traction and parents so if we get 

00:03:33.940 --> 00:03:38.450
 based on the worst codes we've seen 

00:03:36.380 --> 00:03:41.690
 previously for the passive coop for 

00:03:38.450 --> 00:03:45.139
 learning by pda to get 

00:03:41.690 --> 00:03:46.549
 an algorithm that is active a 

00:03:45.139 --> 00:03:49.819
 changes we make is that 

00:03:46.549 --> 00:03:51.919
 now the action to which is returned 

00:03:49.819 --> 00:03:52.760
 will not be politics I'm 

00:03:51.919 --> 00:03:54.859
 supposed to follow 

00:03:52.760 --> 00:03:56.389
 kim is dictated since their research 

00:03:54.859 --> 00:03:58.790
 in fact the optimal policy I can 

00:03:56.389 --> 00:04:01.250
 change the politics it's going to be rather 

00:03:58.790 --> 00:04:05.780
 the action that maximizes the sum of 

00:04:01.250 --> 00:04:07.760
 future rewards breathe second 

00:04:05.780 --> 00:04:10.099
 modification is actually when I 

00:04:07.760 --> 00:04:13.220
 do the liberation 

00:04:10.099 --> 00:04:16.250
 when I'm going to actually estimate 

00:04:13.220 --> 00:04:18.560
 my value board what am I going 

00:04:16.250 --> 00:04:23.180
 execute her daisy as vale 

00:04:18.560 --> 00:04:25.130
 authorization that will try to solve 

00:04:23.180 --> 00:04:27.020
 the penman equations so to find the 

00:04:25.130 --> 00:04:31.010
 values ​​of both is what satisfies 

00:04:27.020 --> 00:04:33.850
 this equality here here at the dock so in 

00:04:31.010 --> 00:04:36.920
 what if i find my maxi cie in 

00:04:33.850 --> 00:04:38.330
 my system of equations and I have 

00:04:36.920 --> 00:04:42.260
 use by the origins for the 

00:04:38.330 --> 00:04:43.660
 solve and in fact there is in the grid 

00:04:42.260 --> 00:04:46.610
 for young people since wind of change 

00:04:43.660 --> 00:04:48.050
 who the third change that was 

00:04:46.610 --> 00:04:49.460
 we will now estimate models 

00:04:48.050 --> 00:04:51.350
 pressure for different actions 

00:04:49.460 --> 00:04:53.150
 offered the pilot who had to see 

00:04:51.350 --> 00:04:56.530
 work as well ie that 

00:04:53.150 --> 00:04:59.090
 so I make a distinction too 

00:04:56.530 --> 00:05:02.710
 I actually consider the action that have 

00:04:59.090 --> 00:05:05.150
 been taken at the previous stage for 

00:05:02.710 --> 00:05:06.830
 when I calculate the number of fake that 

00:05:05.150 --> 00:05:09.020
 I reached because I created my 

00:05:06.830 --> 00:05:11.419
 heel the project reaches the you so have 

00:05:09.020 --> 00:05:12.919
 I will keep in memory whose fake 

00:05:11.419 --> 00:05:14.570
 that I reach the state is this for a 

00:05:12.919 --> 00:05:15.980
 action to the data and what happens 

00:05:14.570 --> 00:05:17.390
 that's when I'm going to run their 

00:05:15.980 --> 00:05:23.300
 gatehouse for data cleaning 

00:05:17.390 --> 00:05:25.550
 the action offers varied from one go 

00:05:23.300 --> 00:05:27.740
 come back on our same example play 

00:05:25.550 --> 00:05:30.830
 with three states here so we remember 

00:05:27.740 --> 00:05:33.800
 40-0 the possible actions c ar 2 and in 

00:05:30.830 --> 00:05:36.310
 this meaning and a2 a3 and s2 remains one and at 

00:05:33.800 --> 00:05:38.540
 terminal where I have no action possible 

00:05:36.310 --> 00:05:41.050
 when I had instit upstream and 

00:05:38.540 --> 00:05:41.050
 It's over 

00:05:41.680 --> 00:05:48.440
 imagine that until now I have 

00:05:44.120 --> 00:05:50.840
 in my essay went from 0 to 1 0 as 1 

00:05:48.440 --> 00:05:56.080
 to 1 0 and that at any time I did 

00:05:50.840 --> 00:06:01.280
 action at 1 to 1 and at three in the man 

00:05:56.080 --> 00:06:02.540
 so at this moment it so now 

00:06:01.280 --> 00:06:06.500
 I made these transitions there it wants 

00:06:02.540 --> 00:06:09.560
 to say that my model my estimate of 

00:06:06.500 --> 00:06:12.830
 my transition model to f zero if I 

00:06:09.560 --> 00:06:14.419
 do the action to one is that with a 

00:06:12.830 --> 00:06:16.220
 luck on two i will stay in 0 and 

00:06:14.419 --> 00:06:18.919
 a chance on two i'm going to go to be 

00:06:16.220 --> 00:06:21.169
 why because when I was 1-0 

00:06:18.919 --> 00:06:23.120
 then I performed the action at one eye and 

00:06:21.169 --> 00:06:25.969
 reviewed once I stayed at 0 

00:06:23.120 --> 00:06:27.289
 then send 0 when I executed 

00:06:25.969 --> 00:06:29.479
 it will be an intervention once I 

00:06:27.289 --> 00:06:31.909
 have gone instead to and will be in 

00:06:29.479 --> 00:06:32.629
 my system of equations the equations 

00:06:31.909 --> 00:06:34.789
 field 

00:06:32.629 --> 00:06:36.229
 I will have only with a practice 0.5 

00:06:34.789 --> 00:06:39.110
 here I do the action at 1 

00:06:36.229 --> 00:06:39.469
 I will stay a 10-0 by 2.5 I will 

00:06:39.110 --> 00:06:41.629
 stay 

00:06:39.469 --> 00:06:46.459
 I will spend later to 

00:06:41.629 --> 00:06:47.569
 finish here this is where it's the part of 

00:06:46.459 --> 00:06:49.849
 the demand equation associated with 

00:06:47.569 --> 00:06:51.860
 the action will now have because I have 

00:06:49.849 --> 00:06:55.249
 not done the action to 2 again made it 

00:06:51.860 --> 00:06:57.079
 I do not know anything about glass she 

00:06:55.249 --> 00:07:00.559
 is on other floors could do a 

00:06:57.079 --> 00:07:02.299
 tradition so the term here finally vote 

00:07:00.559 --> 00:07:03.949
 equal to zero because I claim 

00:07:02.299 --> 00:07:06.709
 part of the zero-rate loan to pass 

00:07:03.949 --> 00:07:09.739
 to any other state 

00:07:06.709 --> 00:07:10.879
 same thing for s1 a care up 

00:07:09.739 --> 00:07:14.059
 now I only did the action 

00:07:10.879 --> 00:07:17.389
 at 3 so I only have a non-zero term 

00:07:14.059 --> 00:07:21.379
 for the part associated with the action at 3 

00:07:17.389 --> 00:07:23.629
 then for the action at 2 g here 1 0 and then 

00:07:21.379 --> 00:07:25.459
 in population to 3 because I did 

00:07:23.629 --> 00:07:27.559
 a good transition could also bring 

00:07:25.459 --> 00:07:32.360
 at s 0 in fact the term if it's 

00:07:27.559 --> 00:07:36.769
 just once the value in s is 

00:07:32.360 --> 00:07:38.209
 here I saw 2 vs 2 equal to 1 in that 

00:07:36.769 --> 00:07:40.989
 if I guess I know some pretty 

00:07:38.209 --> 00:07:44.179
 reinforcement for each of the eight years 

00:07:40.989 --> 00:07:47.300
 so now let's imagine that at this moment 

00:07:44.179 --> 00:07:49.759
 this is the time for agent as0 de 

00:07:47.300 --> 00:07:53.029
 decide what action to take on this 

00:07:49.759 --> 00:07:54.649
 amazing in this case also what they 

00:07:53.029 --> 00:07:58.789
 have to do is compare what is it 

00:07:54.649 --> 00:08:02.029
 the sum of future rewards 

00:07:58.789 --> 00:08:03.679
 so was hoping for action at 2 and 

00:08:02.029 --> 00:08:08.059
 the action thus has both actions 

00:08:03.679 --> 00:08:09.589
 possible if I find myself at s 01 to 2 

00:08:08.059 --> 00:08:10.849
 because I have never done the action well 

00:08:09.589 --> 00:08:12.529
 the sum of the rewards always needs 

00:08:10.849 --> 00:08:14.479
 hope as currently estimated 

00:08:12.529 --> 00:08:16.779
 on 10 0 because my model pull 

00:08:14.479 --> 00:08:20.319
 is initialized to 0 for all 

00:08:16.779 --> 00:08:20.319
 possible transitions 

00:08:20.510 --> 00:08:26.750
 so here and for the action to 1 bat 

00:08:24.440 --> 00:08:29.810
 currently at mons an estimate this is 

00:08:26.750 --> 00:08:33.320
 of 0.5 a probability to go from 0 to 

00:08:29.810 --> 00:08:36.770
 1,060 points 5 as pass quality and 

00:08:33.320 --> 00:08:38.960
 2 0 have a what gives me an estimate 

00:08:36.770 --> 00:08:42.080
 future rewards was hoping 2 - 0 

00:08:38.960 --> 00:08:43.880
 point because it's smaller than 

00:08:42.080 --> 00:08:46.580
 the sum of future rewards expect 

00:08:43.880 --> 00:08:49.310
 associated with the action at 2 I'm going rather 

00:08:46.580 --> 00:08:52.670
 this time choose the action at 2 to 

00:08:49.310 --> 00:08:55.250
 the state hesitates so we see here that our 

00:08:52.670 --> 00:08:57.230
 agent even change of action he goes 

00:08:55.250 --> 00:09:02.000
 explore a new decision 

00:08:57.230 --> 00:09:05.030
 do you have 0 for like actions 

00:09:02.000 --> 00:09:06.760
 possible as a change 

00:09:05.030 --> 00:09:09.290
 possible that would bring me closer 

00:09:06.760 --> 00:09:10.610
 an optimal policy 

00:09:09.290 --> 00:09:13.370
 so if that's how we move from 

00:09:10.610 --> 00:09:16.150
 learning with green passive pda 

00:09:13.370 --> 00:09:16.150
 active 

