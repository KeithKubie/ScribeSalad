WEBVTT
Kind: captions
Language: en

00:00:00.330 --> 00:00:03.380
Okay Michael. So here's a quick quiz. So we just tried

00:00:03.380 --> 00:00:06.990
to argue that boosting has this annoying habbit of not always

00:00:06.990 --> 00:00:10.920
over fitting, but of course something can always over fit. Because

00:00:10.920 --> 00:00:13.400
otherwise we just do boosting and we're done, then neither of us

00:00:13.400 --> 00:00:16.580
would have jobs. And we don't want that to happen. So

00:00:16.580 --> 00:00:18.190
here's a little quiz to see if we can figure out the

00:00:18.190 --> 00:00:21.480
circumstances under which boosting might over fit, or tends to over

00:00:21.480 --> 00:00:25.180
fit. So here are five possiblities. Let me read them to you.

00:00:25.180 --> 00:00:27.410
Tell me if they actually make sense. So

00:00:27.410 --> 00:00:30.860
here's possiblity number one, boosting will tend to over

00:00:30.860 --> 00:00:34.270
fit if The weak learner that it's boosting

00:00:34.270 --> 00:00:38.520
over, always chooses the weakest output that is, it,

00:00:38.520 --> 00:00:41.930
among all the hypothesis that it finds that

00:00:41.930 --> 00:00:45.190
do better than chance over the training with whatever

00:00:45.190 --> 00:00:47.540
given distribution. It always pick the one that is

00:00:47.540 --> 00:00:50.350
still nonetheless closest to chance, while still being better.

00:00:50.350 --> 00:00:51.720
&gt;&gt; Well, why would it do that?

00:00:52.810 --> 00:00:53.700
&gt;&gt; Just to be difficult.

00:00:53.700 --> 00:00:56.150
&gt;&gt; Alright, and so you want to know, whether that makes

00:00:56.150 --> 00:01:00.552
it over, would [INAUDIBLE] make it over fit? [UNKNOWN] boosting overfit.

00:01:00.552 --> 00:01:01.680
&gt;&gt; Okay. Alright.

00:01:01.680 --> 00:01:05.280
&gt;&gt; The second one is weak learner actually ends up using...or

00:01:05.280 --> 00:01:08.930
the weak leaner itself that boosting is using is in fact

00:01:08.930 --> 00:01:12.730
a neural network learner. And just for a little specificity, let's

00:01:12.730 --> 00:01:15.570
say this is a neural network that has many many layers and

00:01:15.570 --> 00:01:18.550
many many nodes. So, you know, it's a big

00:01:18.550 --> 00:01:22.270
powerful neural network, alright? the other option is... Boosting has

00:01:22.270 --> 00:01:24.650
a lot of data. So you're trying to learn, your

00:01:24.650 --> 00:01:26.810
training data is actually very, very, very large. You have

00:01:26.810 --> 00:01:30.490
lots and lots of examples. The fourth case, is that,

00:01:30.490 --> 00:01:35.350
the true underlying hypothesis,the true underlying concept, is in fact

00:01:35.350 --> 00:01:37.560
non linear. So you can't just draw a line. And

00:01:37.560 --> 00:01:41.020
then the fifth case is that we let boosting train

00:01:41.020 --> 00:01:44.130
much too long. Whatever that means. Let's just say we let it

00:01:44.130 --> 00:01:48.770
train a lot. Not just a thousand iterations but a hundred billion iterations.

00:01:48.770 --> 00:01:49.480
&gt;&gt; Okay.

00:01:49.480 --> 00:01:53.590
&gt;&gt; Okay. Billions and billions of itterations. Okay. You got it?

00:01:53.590 --> 00:01:54.620
&gt;&gt; Yeah.

00:01:54.620 --> 00:01:55.630
&gt;&gt; Alright. Go.

