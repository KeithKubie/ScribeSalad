WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.320
[MUSIC PLAYING]

00:00:03.320 --> 00:00:06.230
MALE SPEAKER 1: Shanghai GDG
is a very interesting

00:00:06.230 --> 00:00:07.190
developer community.

00:00:07.190 --> 00:00:08.029
FEMALE SPEAKER 1: I'm
glad somebody

00:00:08.029 --> 00:00:09.200
has asked this question.

00:00:09.200 --> 00:00:10.533
MALE SPEAKER 2: This is where
the magic happens.

00:00:10.533 --> 00:00:11.520
FEMALE SPEAKER 2: This is
primarily a question and

00:00:11.520 --> 00:00:12.260
answer show.

00:00:12.260 --> 00:00:14.745
So if any of you out there would
like to ask questions.

00:00:19.227 --> 00:00:20.310
JOHN MCCUTCHAN: Hey everyone.

00:00:20.310 --> 00:00:21.390
It's great to be here.

00:00:21.390 --> 00:00:22.840
I'm John McCutchan, a developer

00:00:22.840 --> 00:00:24.670
programs engineer at Google.

00:00:24.670 --> 00:00:27.840
And my passion is to make
games on the web better.

00:00:27.840 --> 00:00:30.600
And I'm going to introduce a
really neat new way to do

00:00:30.600 --> 00:00:33.650
that, which is called Native
Client Acceleration Modules.

00:00:33.650 --> 00:00:36.230
This is a new concept
that doesn't exist.

00:00:36.230 --> 00:00:38.540
But I think it's going to
really make some new

00:00:38.540 --> 00:00:39.840
experiences possible
on the web.

00:00:42.370 --> 00:00:44.620
So what if you wanted
write most of your

00:00:44.620 --> 00:00:45.720
application as a web app?

00:00:45.720 --> 00:00:49.000
Meaning you're writing it in
HTML5, you're using JavaScript

00:00:49.000 --> 00:00:51.400
or Dart, but there's still
a few sections in your

00:00:51.400 --> 00:00:53.750
application where you just can't
extract the performance

00:00:53.750 --> 00:00:56.160
out of JavaScript or Dart?

00:00:56.160 --> 00:01:00.040
Well, you can get native
performance where you need it

00:01:00.040 --> 00:01:02.110
with Native Client Acceleration
Modules, which

00:01:02.110 --> 00:01:04.800
allow you to extend your
JavaScript or Dart application

00:01:04.800 --> 00:01:06.050
with C++ code.

00:01:08.410 --> 00:01:10.570
So right after that, I want to
show you an example Native

00:01:10.570 --> 00:01:11.950
Client Acceleration Module.

00:01:11.950 --> 00:01:15.750
We have Bullet Physics running
inside an acceleration module,

00:01:15.750 --> 00:01:20.470
simulating about 130 rigid
body objects at 60 Hertz.

00:01:20.470 --> 00:01:24.340
This is not possible in
the browser today.

00:01:24.340 --> 00:01:26.990
But what's interesting here is,
at the front end, all the

00:01:26.990 --> 00:01:29.540
graphics that you're seeing
are being drawn by a Dart

00:01:29.540 --> 00:01:32.330
application by making
WebGL calls.

00:01:32.330 --> 00:01:35.300
The physics simulation is
happening in a separate module

00:01:35.300 --> 00:01:40.000
implemented in C that transmits
the updated physics

00:01:40.000 --> 00:01:42.590
objects positions to the
Dart application,

00:01:42.590 --> 00:01:43.650
which then draws them.

00:01:43.650 --> 00:01:45.030
So let me run it here.

00:01:45.030 --> 00:01:46.970
You're going to see a whole
bunch of blocks fall on top of

00:01:46.970 --> 00:01:48.730
each other, and then some
balls land on them.

00:01:58.580 --> 00:01:59.960
Isn't this really
exciting stuff?

00:01:59.960 --> 00:02:02.600
I mean, you can't do this
in a browser today.

00:02:02.600 --> 00:02:04.915
But you can with Native Client
Acceleration Modules.

00:02:08.330 --> 00:02:10.479
So some of you might be asking,
what is Native Client?

00:02:10.479 --> 00:02:12.510
I've never heard
of this before.

00:02:12.510 --> 00:02:15.580
Well, Native Client allows you
to take your C++ code and

00:02:15.580 --> 00:02:20.370
compile it with a version of
GCC which outputs a Native

00:02:20.370 --> 00:02:22.720
Client executable and
a manifest file.

00:02:22.720 --> 00:02:25.800
This manifest file can be
included in your website with

00:02:25.800 --> 00:02:28.820
a single line of HTML code,
which then allows you to

00:02:28.820 --> 00:02:31.840
launch an entire C++ application
and have it run in

00:02:31.840 --> 00:02:33.090
the browser.

00:02:36.130 --> 00:02:40.910
So specifically, Native Client
is a secure C and C++ runtime

00:02:40.910 --> 00:02:42.600
for the web.

00:02:42.600 --> 00:02:45.580
There's no chance that the C
code could take over someone's

00:02:45.580 --> 00:02:47.440
computer or damage it.

00:02:47.440 --> 00:02:49.930
It's secured in a sandbox.

00:02:49.930 --> 00:02:53.880
But your existing C and C++
code works, along with

00:02:53.880 --> 00:02:57.620
languages like C#
through mono.

00:02:57.620 --> 00:02:59.720
And these applications interact
with the browser to

00:02:59.720 --> 00:03:05.180
take input and draw by
using the Pepper API.

00:03:05.180 --> 00:03:08.320
But what makes Native Client
somewhat limited from the

00:03:08.320 --> 00:03:10.820
perspective of most web
developers is that it requires

00:03:10.820 --> 00:03:13.000
the entire application to be
written in C. So if you want

00:03:13.000 --> 00:03:16.200
to get involved with Native
Client, you have to be a C

00:03:16.200 --> 00:03:18.755
programmer, which many web
developers aren't.

00:03:22.540 --> 00:03:25.980
So can we provide native
performance where it counts,

00:03:25.980 --> 00:03:29.640
but make it accessible to
all web app developers?

00:03:29.640 --> 00:03:32.460
And yes, Native Client
Acceleration Modules is

00:03:32.460 --> 00:03:33.220
exactly that.

00:03:33.220 --> 00:03:36.870
It allows you to write most of
your application as an HTML5

00:03:36.870 --> 00:03:40.910
application, and communicate
with these very fast C-backed

00:03:40.910 --> 00:03:45.600
runtimes that do some
computation for you.

00:03:45.600 --> 00:03:48.340
So drilling down a little bit,
we see that the HTML5

00:03:48.340 --> 00:03:51.870
application talks to some kind
of JavaScript or Dart library,

00:03:51.870 --> 00:03:54.450
which manages the communication
to the back-end

00:03:54.450 --> 00:03:55.700
acceleration module.

00:04:00.490 --> 00:04:04.460
So if you're a C++ library
author and you want to expose

00:04:04.460 --> 00:04:07.170
one of your libraries as an
extension to the browser, how

00:04:07.170 --> 00:04:08.120
do you go about doing it?

00:04:08.120 --> 00:04:09.660
It's very straightforward.

00:04:09.660 --> 00:04:12.890
First, you have to port your
application to Native Client,

00:04:12.890 --> 00:04:16.589
which is very straightforward,
because almost all of your C++

00:04:16.589 --> 00:04:20.110
code is just going
to compile over.

00:04:20.110 --> 00:04:23.180
The only areas where they'll
be any extra work is if you

00:04:23.180 --> 00:04:27.130
need to interact with any
Pepper API calls.

00:04:27.130 --> 00:04:29.120
But that's a fairly
straightforward port, just

00:04:29.120 --> 00:04:32.410
like porting into any other
operating system.

00:04:32.410 --> 00:04:37.710
You then expose your C++ API as
an interface of messages.

00:04:37.710 --> 00:04:39.440
And what this really means is
that you're going to be doing

00:04:39.440 --> 00:04:42.470
a lot of JSON message parsing
and construction.

00:04:42.470 --> 00:04:44.950
You could use any communication
mechanism you

00:04:44.950 --> 00:04:47.960
want, but JSON is very
convenient because the browser

00:04:47.960 --> 00:04:49.210
can consume it trivially.

00:04:51.610 --> 00:04:54.770
And because JavaScript
applications execute

00:04:54.770 --> 00:04:58.290
asynchronously, you need to add
a work queue to your C++

00:04:58.290 --> 00:05:01.380
module where the JavaScript
application can queue some

00:05:01.380 --> 00:05:02.530
work for you to do.

00:05:02.530 --> 00:05:05.190
You do the work, and then you
send the reply back later on.

00:05:07.970 --> 00:05:10.430
Once that's complete, you write
the other side of the

00:05:10.430 --> 00:05:13.320
coin, which is a JavaScript or
Dart library, which does the

00:05:13.320 --> 00:05:16.000
communication to the Native
Client Acceleration Module.

00:05:18.590 --> 00:05:21.440
To give you an idea, it took
me one day to implement the

00:05:21.440 --> 00:05:24.130
acceleration module that we saw

00:05:24.130 --> 00:05:26.200
earlier, of Bullet Physics.

00:05:26.200 --> 00:05:30.340
Just a single day to parse the
messages and construct new

00:05:30.340 --> 00:05:32.370
messages to send back
to the browser.

00:05:32.370 --> 00:05:33.620
And it was really
straightforward.

00:05:36.570 --> 00:05:38.710
So if your web developer and
you want to consume one of

00:05:38.710 --> 00:05:42.080
these extensions from a Native
Client Acceleration Module,

00:05:42.080 --> 00:05:43.170
how do you go about doing it?

00:05:43.170 --> 00:05:45.560
Well, it's really easy and
exactly like you would take

00:05:45.560 --> 00:05:49.050
any other JavaScript module
that exists today.

00:05:49.050 --> 00:05:52.570
You include the script tag that
references the API that

00:05:52.570 --> 00:05:54.150
you're going to be calling.

00:05:54.150 --> 00:05:57.390
And the one extra step is that
you include the embed tag for

00:05:57.390 --> 00:05:59.920
the Native Client manifest.

00:05:59.920 --> 00:06:02.620
Once you've done that, you just
make JavaScript calls as

00:06:02.620 --> 00:06:05.610
normal, and they just get
routed in to the C

00:06:05.610 --> 00:06:06.630
code in the back end.

00:06:06.630 --> 00:06:09.030
But that's invisible to you
as a web app developer.

00:06:09.030 --> 00:06:10.950
So it's similar to using
any other JavaScript

00:06:10.950 --> 00:06:13.390
library like jQuery.

00:06:13.390 --> 00:06:15.880
It's really straightforward.

00:06:15.880 --> 00:06:18.970
So what this is going to give us
is, 90% of your application

00:06:18.970 --> 00:06:21.080
is going to be a web app, but
you're going to get that

00:06:21.080 --> 00:06:23.630
native performance where you
need it, including Bullet

00:06:23.630 --> 00:06:27.360
Physics simulating hundreds of
rigid bodies at 60 Hertz,

00:06:27.360 --> 00:06:30.150
blazing fast zip decompression,
video encoding

00:06:30.150 --> 00:06:33.220
and processing, image
processing, encryption, all of

00:06:33.220 --> 00:06:36.240
these very computationally
expensive things that you can

00:06:36.240 --> 00:06:38.760
implement in JavaScript, but
you're going to see a big

00:06:38.760 --> 00:06:43.490
performance difference between
the native C code.

00:06:43.490 --> 00:06:45.800
So now I want to shift gears
and talk about the back-end

00:06:45.800 --> 00:06:48.880
implementation of this system.

00:06:48.880 --> 00:06:51.590
So you remember this diagram
from earlier, where I'm

00:06:51.590 --> 00:06:54.780
showing the HTML5 application
using the JavaScript module

00:06:54.780 --> 00:06:58.210
that they've included which is
shunting the messages back and

00:06:58.210 --> 00:07:01.740
forth between the Native Client
acceleration module.

00:07:01.740 --> 00:07:05.070
What I want to ask is, what are
these messages and how are

00:07:05.070 --> 00:07:07.650
they transmitted?

00:07:07.650 --> 00:07:08.980
So let's get to it.

00:07:08.980 --> 00:07:12.400
All of the messages start with
a message header, which is a

00:07:12.400 --> 00:07:14.140
serialized JSON string.

00:07:14.140 --> 00:07:17.570
It includes a command name,
a request ID, and then an

00:07:17.570 --> 00:07:19.150
arbitrary set of arguments,
depending

00:07:19.150 --> 00:07:22.500
on the command itself.

00:07:22.500 --> 00:07:26.050
But sometimes these messages are
going to need binary data,

00:07:26.050 --> 00:07:29.320
and JavaScript has a mechanism
of accessing binary data

00:07:29.320 --> 00:07:31.720
through something called
the Typed Array.

00:07:31.720 --> 00:07:36.110
And so after the JSON message
header is sent, subsequently

00:07:36.110 --> 00:07:41.060
the n number, where n could be
0, of Typed Arrays are sent

00:07:41.060 --> 00:07:42.570
afterwards.

00:07:42.570 --> 00:07:45.410
So depending on the format of
the message, there might be

00:07:45.410 --> 00:07:47.770
zero Typed Arrays following
the header, or

00:07:47.770 --> 00:07:49.110
there could be five.

00:07:49.110 --> 00:07:52.240
It's completely up to the
implementer to decide the

00:07:52.240 --> 00:07:54.810
optimal way of structuring
these messages.

00:07:54.810 --> 00:07:57.560
But they all start with a
header, and then optional set

00:07:57.560 --> 00:07:58.810
of Typed Arrays following
that.

00:08:01.250 --> 00:08:02.750
So let's give an example.

00:08:02.750 --> 00:08:05.740
If I'm a JavaScript app
developer, and I just want to

00:08:05.740 --> 00:08:08.210
call this fibonacciAccelerator,
and get

00:08:08.210 --> 00:08:10.930
the 10th Fibonacci number, I
make this single call in

00:08:10.930 --> 00:08:12.150
JavaScript.

00:08:12.150 --> 00:08:16.250
I tell it the nth Fibonacci
number I want, and the

00:08:16.250 --> 00:08:18.200
callback that I want
to have fire when

00:08:18.200 --> 00:08:19.810
the result is available.

00:08:19.810 --> 00:08:23.720
But underneath the hood, what
happens is a command is

00:08:23.720 --> 00:08:25.100
structured--

00:08:25.100 --> 00:08:27.670
and you can see that the command
is fib, the argument

00:08:27.670 --> 00:08:31.430
is 10, and the request
ID is 27.

00:08:31.430 --> 00:08:34.740
The request ID is used to
connect the reply that will be

00:08:34.740 --> 00:08:36.360
coming later on from
the Native Client

00:08:36.360 --> 00:08:37.610
Acceleration Module.

00:08:37.610 --> 00:08:40.530
This message gets packaged
up and sent over to the

00:08:40.530 --> 00:08:43.730
acceleration module, where the
acceleration module does its

00:08:43.730 --> 00:08:45.600
computation and figures
out that the 10th

00:08:45.600 --> 00:08:47.570
Fibonacci number is 55.

00:08:47.570 --> 00:08:51.220
It then packages up this message
and sends it back to

00:08:51.220 --> 00:08:55.640
the JavaScript side, with the
request ID being 27, just like

00:08:55.640 --> 00:08:57.890
the original message, so that
you can connect them on the

00:08:57.890 --> 00:09:01.020
other side and the correct
call back is fired.

00:09:01.020 --> 00:09:03.500
It's a very contrived example,
but this gives you the flavor

00:09:03.500 --> 00:09:04.750
of how this operates.

00:09:07.500 --> 00:09:09.510
So how are these
messages sent?

00:09:09.510 --> 00:09:13.210
Well, they're sent with
calls to postMessage.

00:09:13.210 --> 00:09:16.700
And postMessage takes these
messages and it puts them into

00:09:16.700 --> 00:09:19.830
a queue that the web application
consumes, or the

00:09:19.830 --> 00:09:21.805
Native Client Acceleration
Module consumes.

00:09:28.520 --> 00:09:30.170
So I want to talk about
message ordering.

00:09:30.170 --> 00:09:33.280
Let's say I have a JSON message
header, and then it

00:09:33.280 --> 00:09:35.820
requires three Typed Arrays
following it.

00:09:35.820 --> 00:09:38.830
But some other JSON message
header gets in the way.

00:09:38.830 --> 00:09:39.870
It gets in the middle.

00:09:39.870 --> 00:09:41.360
Question is, is this possible?

00:09:41.360 --> 00:09:45.350
And if it is, how can we work
around this problem?

00:09:45.350 --> 00:09:47.860
Well, turns out that it is
possible in one specific

00:09:47.860 --> 00:09:50.680
configuration, the configuration
of where there's

00:09:50.680 --> 00:09:52.920
many Native Client Acceleration
Models embedded

00:09:52.920 --> 00:09:55.810
on a page, and they're all
communicating back to the web

00:09:55.810 --> 00:09:57.890
application at the same time.

00:09:57.890 --> 00:10:00.150
Because there's only one queue
for the entire web

00:10:00.150 --> 00:10:03.170
application, what could happen
is that the messages will end

00:10:03.170 --> 00:10:05.970
up interleaved between
each other.

00:10:05.970 --> 00:10:09.480
And so you'll get some of the
green messages, and then

00:10:09.480 --> 00:10:11.710
you'll get the yellow and red
message, and then you'll get

00:10:11.710 --> 00:10:13.500
the tail of the green message.

00:10:13.500 --> 00:10:17.610
This would be very confusing for
the web application, and

00:10:17.610 --> 00:10:21.580
we need to do something
about this.

00:10:21.580 --> 00:10:24.920
So what you do is you place a
router between the message

00:10:24.920 --> 00:10:27.130
queue and the Native
Client Acceleration

00:10:27.130 --> 00:10:29.150
Module JavaScript libraries.

00:10:29.150 --> 00:10:34.260
And this router uses the event
source element ID as a way to

00:10:34.260 --> 00:10:39.510
route the messages from this
single queue to the specific

00:10:39.510 --> 00:10:40.990
JavaScript libraries.

00:10:40.990 --> 00:10:44.570
And this solves this problem.

00:10:44.570 --> 00:10:45.850
So now I want to talk about the

00:10:45.850 --> 00:10:47.360
performance of these messages.

00:10:47.360 --> 00:10:50.360
Because if the performance isn't
great, then this is just

00:10:50.360 --> 00:10:51.240
not going to work.

00:10:51.240 --> 00:10:54.180
But it turns out that the
performance is actually pretty

00:10:54.180 --> 00:10:59.040
fast, and will be getting faster
as we move forward.

00:10:59.040 --> 00:11:01.680
So what we're looking at here
is a graph of the number of

00:11:01.680 --> 00:11:03.860
round trips in five seconds.

00:11:03.860 --> 00:11:06.710
A message is being sent from
JavaScript to the Native

00:11:06.710 --> 00:11:09.740
Client Acceleration Module,
and then back.

00:11:09.740 --> 00:11:12.600
How many times can this happen
in five seconds?

00:11:12.600 --> 00:11:16.750
The x-axis is showing you the
message sizes, starting at 16

00:11:16.750 --> 00:11:20.620
bytes and ending at 16 megabytes
per message.

00:11:20.620 --> 00:11:23.510
And the y-axis is telling you
how many round trips.

00:11:23.510 --> 00:11:27.170
So until your message size
hits about 4K, you don't

00:11:27.170 --> 00:11:28.300
really see much of a drop off.

00:11:28.300 --> 00:11:33.250
It's between 25,000 and 30,000
messages in five seconds.

00:11:33.250 --> 00:11:36.970
But after 4K, it starts
to drop off steeply,

00:11:36.970 --> 00:11:38.300
but it's only at--

00:11:38.300 --> 00:11:43.060
your message size is above 256
kilobytes, that you start to

00:11:43.060 --> 00:11:48.510
see the latency go beyond one
millisecond for a round trip.

00:11:52.760 --> 00:11:55.390
Another way of looking at the
same set of data is, what's

00:11:55.390 --> 00:11:55.950
the throughput?

00:11:55.950 --> 00:11:58.850
How much can you push
through this pipe?

00:11:58.850 --> 00:12:01.930
And it varies depending
on the message size.

00:12:01.930 --> 00:12:04.820
So if you send larger messages,
you get more

00:12:04.820 --> 00:12:06.490
throughput.

00:12:06.490 --> 00:12:10.970
And it peaks at about 420
megabytes per second.

00:12:10.970 --> 00:12:14.670
But once you hit about
640k per message--

00:12:14.670 --> 00:12:18.040
sorry, 64k per message,
you get pretty

00:12:18.040 --> 00:12:20.380
close to this maximum.

00:12:20.380 --> 00:12:21.760
So that's the sweet spot.

00:12:21.760 --> 00:12:24.800
If you could package up all your
data for computation and

00:12:24.800 --> 00:12:29.020
the results in less than 64k,
you're going to be great.

00:12:33.260 --> 00:12:38.040
So I'm going to just pause here
and note that I ran these

00:12:38.040 --> 00:12:41.460
benchmarks on a machine running
Chrome 21, Windows 7

00:12:41.460 --> 00:12:45.550
with 64-bit, a Core i7 with
12 gigabytes of RAM.

00:12:45.550 --> 00:12:48.240
Benchmarking is difficult, and
your operating system and

00:12:48.240 --> 00:12:52.630
hardware configuration will give
you different results.

00:12:52.630 --> 00:12:58.310
So don't take this as the
absolute truth, but it was

00:12:58.310 --> 00:12:59.560
true for my machine.

00:13:03.670 --> 00:13:06.360
So the Native Client
Acceleration Module message

00:13:06.360 --> 00:13:09.660
system allows for many Native
Client modules on a single

00:13:09.660 --> 00:13:11.090
page, which is awesome.

00:13:11.090 --> 00:13:13.940
So you, the developer, can mix
and match based on your

00:13:13.940 --> 00:13:16.500
application needs.

00:13:16.500 --> 00:13:21.170
It offers low latency and high
throughput, which is key.

00:13:21.170 --> 00:13:23.130
To make this a little bit more
concrete, I know we looked at

00:13:23.130 --> 00:13:26.350
graphs and we saw that
we can transmit 420

00:13:26.350 --> 00:13:27.730
megabytes per second.

00:13:27.730 --> 00:13:29.540
What does that actually mean?

00:13:29.540 --> 00:13:33.550
Well, let's say you wanted to
stream a 1024 by 1024 video at

00:13:33.550 --> 00:13:36.960
60 Hertz and just transmit the
entire frame at a time.

00:13:36.960 --> 00:13:40.480
So no compression involved, just
60 times a second you're

00:13:40.480 --> 00:13:43.460
sending the 1024 by
1024 image across.

00:13:43.460 --> 00:13:47.160
Well, that would use 60% of your
bandwidth, at about 240

00:13:47.160 --> 00:13:48.520
megabytes per second.

00:13:48.520 --> 00:13:51.320
You still lots of room for
other communications

00:13:51.320 --> 00:13:53.770
that need to happen.

00:13:53.770 --> 00:13:56.360
You'll have an eight millisecond
latency per frame,

00:13:56.360 --> 00:13:57.110
which is fine.

00:13:57.110 --> 00:14:00.340
Because since we're displaying
at 60 Hertz, eight

00:14:00.340 --> 00:14:04.490
milliseconds is actually only
half of the time that you have

00:14:04.490 --> 00:14:05.060
to display.

00:14:05.060 --> 00:14:10.620
So you won't even be an
entire frame back.

00:14:10.620 --> 00:14:13.100
Another example, let's say you
wanted to physically simulate

00:14:13.100 --> 00:14:14.780
1,000 rigid bodies.

00:14:14.780 --> 00:14:16.860
This is something that
modern CPUs can't

00:14:16.860 --> 00:14:18.810
even do at 60 Hertz.

00:14:18.810 --> 00:14:22.270
But if they could, you would
be using just 1.7% of your

00:14:22.270 --> 00:14:25.970
bandwidth available, with a two
millisecond latency, which

00:14:25.970 --> 00:14:27.220
is awesome.

00:14:29.140 --> 00:14:30.560
So let's look at some
case studies a

00:14:30.560 --> 00:14:31.810
little bit more in depth.

00:14:35.060 --> 00:14:37.650
So I showed the video earlier
of the Bullet Physics

00:14:37.650 --> 00:14:40.860
acceleration module operating,
communicating its results back

00:14:40.860 --> 00:14:41.690
to a Dart application.

00:14:41.690 --> 00:14:44.930
But how does this really
work under the hood?

00:14:44.930 --> 00:14:48.270
The Dart application sends an
initial message which is, load

00:14:48.270 --> 00:14:48.840
this scene.

00:14:48.840 --> 00:14:51.580
And the scene includes a
description of all the rigid

00:14:51.580 --> 00:14:54.040
bodies in the scene, where you
want them to be, how heavy

00:14:54.040 --> 00:14:56.730
they are, how much friction they
have, all these type of

00:14:56.730 --> 00:14:59.220
physical properties
that you get.

00:14:59.220 --> 00:15:01.650
This gets sent to the Native
Client Acceleration Module,

00:15:01.650 --> 00:15:04.650
which remembers the state
of the scene.

00:15:04.650 --> 00:15:07.330
Later on, when your application
is ready, it sends

00:15:07.330 --> 00:15:09.980
a step simulation message to
the acceleration module.

00:15:09.980 --> 00:15:13.390
It says, OK, I want you to
simulate the physics world and

00:15:13.390 --> 00:15:18.100
have it update so that it's 16
milliseconds in the future.

00:15:18.100 --> 00:15:21.540
It replies back with the set
of transformation matrices

00:15:21.540 --> 00:15:24.780
after the simulation
has been updated.

00:15:24.780 --> 00:15:28.150
Those transformation matrices
can be fed back into the Dart

00:15:28.150 --> 00:15:33.000
application and used to draw the
world as it is, based on

00:15:33.000 --> 00:15:35.170
the state of the physics
simulation.

00:15:35.170 --> 00:15:36.330
Then this loops.

00:15:36.330 --> 00:15:38.640
After we do that, we send
another message back to the

00:15:38.640 --> 00:15:41.100
Native Client Acceleration
Modules saying update the

00:15:41.100 --> 00:15:41.720
simulation.

00:15:41.720 --> 00:15:43.260
We get the new transforms
and we draw it.

00:15:43.260 --> 00:15:44.510
And we just keep looping.

00:15:47.420 --> 00:15:50.070
So let's look at the performance
of a Native Client

00:15:50.070 --> 00:15:53.180
Acceleration Module built
around Bullet, to

00:15:53.180 --> 00:15:55.540
machine-translated JavaScript.

00:15:55.540 --> 00:15:57.090
So for those of you who
aren't familiar with

00:15:57.090 --> 00:15:59.590
machine-translated JavaScript,
there are tools out there that

00:15:59.590 --> 00:16:05.300
will take a C or C++ Program and
compile it to JavaScript,

00:16:05.300 --> 00:16:07.220
which is pretty awesome.

00:16:07.220 --> 00:16:10.440
But how does the performance
compare when you're executing

00:16:10.440 --> 00:16:13.670
that JavaScript that was
generated by the tool, versus

00:16:13.670 --> 00:16:16.820
a Native Client Acceleration
Module?

00:16:16.820 --> 00:16:19.440
Well, what the graph we're
looking at here, the x-axis is

00:16:19.440 --> 00:16:22.230
frames, the y-axis is
microseconds to update the

00:16:22.230 --> 00:16:25.360
simulation state.

00:16:25.360 --> 00:16:29.620
So red is the JavaScript
execution time, blue is the

00:16:29.620 --> 00:16:31.060
Native Client Acceleration
Module.

00:16:31.060 --> 00:16:33.390
You probably notice you don't
see a lot of blue, and that's

00:16:33.390 --> 00:16:37.340
because the performance
difference is so large that

00:16:37.340 --> 00:16:39.630
blue is just essentially at 0.

00:16:39.630 --> 00:16:42.560
I'll zoom in a moment and show
you the true difference

00:16:42.560 --> 00:16:43.100
between them.

00:16:43.100 --> 00:16:45.240
But what's interesting about
this graph is you see these

00:16:45.240 --> 00:16:49.540
two huge spikes where the
JavaScript is just like, even

00:16:49.540 --> 00:16:53.030
for its own baseline
performance, is many times

00:16:53.030 --> 00:16:55.530
slower for these moments.

00:16:55.530 --> 00:16:57.950
And so the question is,
what's causing this?

00:16:57.950 --> 00:17:00.350
And it's difficult to nail down
exactly what's causing

00:17:00.350 --> 00:17:02.210
it, but there's two
likely causes.

00:17:02.210 --> 00:17:05.300
The first is, JavaScript is a
garbage collected language.

00:17:05.300 --> 00:17:08.730
So after a certain amount of
time, enough garbage--

00:17:08.730 --> 00:17:12.569
which is objects that are
no longer referenced--

00:17:12.569 --> 00:17:13.240
accumulates.

00:17:13.240 --> 00:17:16.440
And what happens is the
JavaScript runtime stops the

00:17:16.440 --> 00:17:18.609
execution of the program, it
goes in and collects all this

00:17:18.609 --> 00:17:22.660
garbage, frees it, and makes it
available for future use.

00:17:22.660 --> 00:17:25.349
But what that means is that if
that garbage collection kicks

00:17:25.349 --> 00:17:30.530
in in the middle of your physics
simulation, you're

00:17:30.530 --> 00:17:32.490
going to see a huge spike
in the time it takes

00:17:32.490 --> 00:17:35.300
to update the world.

00:17:35.300 --> 00:17:39.470
Another cause is that, because
JavaScript is a JIT engine,

00:17:39.470 --> 00:17:42.560
what it does is it initially
outputs unoptimized code.

00:17:42.560 --> 00:17:44.660
And then it collects statistics,
like every time

00:17:44.660 --> 00:17:48.300
this function gets called, the
first argument is a string.

00:17:48.300 --> 00:17:50.650
And then it outputs optimized
code based around that

00:17:50.650 --> 00:17:51.790
assumption.

00:17:51.790 --> 00:17:54.590
But what's inside that optimized
code is checks that

00:17:54.590 --> 00:17:55.590
that assumption is true.

00:17:55.590 --> 00:17:59.230
The moment that assumption is
not true, it pulls a parachute

00:17:59.230 --> 00:18:02.500
out, and it goes back to
the non-optimized code.

00:18:02.500 --> 00:18:04.630
So what we're seeing with these
two spikes could either

00:18:04.630 --> 00:18:08.800
be really extreme moments of
JavaScript garbage collection,

00:18:08.800 --> 00:18:12.370
or some assumption is being
broken, and everything is

00:18:12.370 --> 00:18:14.460
getting de-optimized and
then re-optimized.

00:18:14.460 --> 00:18:16.300
Eventually it stabilizes.

00:18:16.300 --> 00:18:20.250
And when we zoom in, we can
actually see the execution

00:18:20.250 --> 00:18:22.600
time for the Native Client
Acceleration Module.

00:18:22.600 --> 00:18:25.640
In this specific example, it's
about 100 microseconds per

00:18:25.640 --> 00:18:28.770
frame update, and you could
see that it's very steady.

00:18:28.770 --> 00:18:33.480
Whereas the JavaScript update
is 1,000 microseconds, or an

00:18:33.480 --> 00:18:36.900
order of magnitude slower,
with these weird spikes.

00:18:36.900 --> 00:18:39.140
I mean, this is even--

00:18:39.140 --> 00:18:42.480
this was noise on the image that
we were just looking at a

00:18:42.480 --> 00:18:42.900
moment ago.

00:18:42.900 --> 00:18:44.770
But you can still see that
every once in a while,

00:18:44.770 --> 00:18:47.580
JavaScript performance goes from
1,000 microseconds per

00:18:47.580 --> 00:18:50.380
frame update to 2,000
or 4,000.

00:18:50.380 --> 00:18:52.430
It's very bumpy.

00:18:52.430 --> 00:18:54.260
You never see this kind of
variation with the Native

00:18:54.260 --> 00:18:55.510
Client Acceleration Module.

00:18:58.650 --> 00:18:59.810
So what about download size?

00:18:59.810 --> 00:19:03.010
These are web applications and
this is really important.

00:19:03.010 --> 00:19:06.260
If the JavaScript performance
is not that great, but it

00:19:06.260 --> 00:19:09.670
takes only a fraction of the
time to download than the

00:19:09.670 --> 00:19:11.850
Native Client Acceleration
Module, maybe that trade-off

00:19:11.850 --> 00:19:13.960
is worth making.

00:19:13.960 --> 00:19:17.890
So looking at the uncompressed
sizes, the acceleration module

00:19:17.890 --> 00:19:20.950
is a 3 megabyte download, and
the machine-generated

00:19:20.950 --> 00:19:23.290
JavaScript is 2 megabytes.

00:19:23.290 --> 00:19:29.290
So we're looking at about 66% of
the acceleration module to

00:19:29.290 --> 00:19:31.160
download the JavaScript.

00:19:31.160 --> 00:19:34.770
Compressed, it gets a
little bit better.

00:19:34.770 --> 00:19:38.120
It gets close to about two times
the download size for

00:19:38.120 --> 00:19:44.360
the acceleration module to
one JavaScript module.

00:19:44.360 --> 00:19:47.100
So again, this really comes down
to what's important here.

00:19:47.100 --> 00:19:49.590
If you have very light load
and you can get away with

00:19:49.590 --> 00:19:52.070
using the JavaScript
code, good.

00:19:52.070 --> 00:19:53.110
That's awesome.

00:19:53.110 --> 00:19:54.980
Because it's going download
a little bit faster.

00:19:54.980 --> 00:19:56.570
But if you have something
really heavy, where this

00:19:56.570 --> 00:19:58.900
really matters, where it counts,
when you need that

00:19:58.900 --> 00:20:02.231
native performance, the overhead
is not that big.

00:20:05.920 --> 00:20:07.320
So let's look at another
case study.

00:20:07.320 --> 00:20:11.850
Let's say you want to be able to
zip and unzip things inside

00:20:11.850 --> 00:20:13.340
your web application.

00:20:13.340 --> 00:20:16.950
There are implementations of
this in JavaScript, but again,

00:20:16.950 --> 00:20:19.185
as we'll see, the performance
is just--

00:20:19.185 --> 00:20:20.840
it can't be matched.

00:20:20.840 --> 00:20:23.470
So the Native Client
Acceleration Modules for the

00:20:23.470 --> 00:20:26.090
compressor and at the
decompressor are stateless.

00:20:26.090 --> 00:20:26.750
They're kind of dumb.

00:20:26.750 --> 00:20:27.980
They just accept a message.

00:20:27.980 --> 00:20:30.280
They take the compressed
data and then they

00:20:30.280 --> 00:20:33.700
decompress it and--

00:20:33.700 --> 00:20:35.570
sorry, they take the compressed
data, and they

00:20:35.570 --> 00:20:38.460
compress it, and they send that
back to you as a message.

00:20:38.460 --> 00:20:39.830
The decompressor works
similarly.

00:20:39.830 --> 00:20:42.710
You give it the compressed blob,
and it sends back an

00:20:42.710 --> 00:20:45.030
uncompressed blob.

00:20:45.030 --> 00:20:46.410
It's very simple.

00:20:46.410 --> 00:20:48.430
These acceleration modules
have no state.

00:20:48.430 --> 00:20:50.090
They don't really know
what's going on.

00:20:50.090 --> 00:20:52.270
They just do one thing, and
they do it really well.

00:20:52.270 --> 00:20:54.790
But what's great about the
acceleration model framework

00:20:54.790 --> 00:20:57.830
is that you can include both of
these on your page and send

00:20:57.830 --> 00:20:59.610
them a message whenever
you need to.

00:21:02.150 --> 00:21:03.990
So let's look at the
performance again.

00:21:03.990 --> 00:21:06.880
Again you can see that the
JavaScript implementation

00:21:06.880 --> 00:21:09.600
executes about an order of
magnitude slower than the

00:21:09.600 --> 00:21:13.400
Native Client Acceleration
Module does.

00:21:13.400 --> 00:21:16.150
And again, you see that very
rocky performance.

00:21:16.150 --> 00:21:18.910
It's not as consistently--

00:21:18.910 --> 00:21:21.470
the performance is just like,
sometimes it's fast, sometimes

00:21:21.470 --> 00:21:22.970
it's slow with JavaScript.

00:21:22.970 --> 00:21:25.100
Whereas with the Native Client
Acceleration Module, it's much

00:21:25.100 --> 00:21:27.110
more stable.

00:21:27.110 --> 00:21:30.530
And looking at decompression
times, again you can see that

00:21:30.530 --> 00:21:33.930
the Native Client implementation
is far faster

00:21:33.930 --> 00:21:35.745
than the JavaScript execution.

00:21:39.030 --> 00:21:42.460
So what am I saying about
machine-translated JavaScript?

00:21:42.460 --> 00:21:45.000
Well first of all, it works,
and it works really well.

00:21:45.000 --> 00:21:48.650
Depending on your needs,
it might be a great

00:21:48.650 --> 00:21:49.600
solution for you.

00:21:49.600 --> 00:21:52.590
It's pretty amazing that you
could take a C++ program and

00:21:52.590 --> 00:21:55.210
get a working JavaScript
program out of it.

00:21:55.210 --> 00:21:57.650
It's kind of mind blowing.

00:21:57.650 --> 00:21:59.300
But what I'm really showing
with this performance

00:21:59.300 --> 00:22:02.220
comparison is the difference
in execution time for a

00:22:02.220 --> 00:22:05.360
JavaScript JIT engine versus
native code running in the

00:22:05.360 --> 00:22:06.610
Native Client sandbox.

00:22:11.450 --> 00:22:14.640
So that being said, Native
Client Acceleration Modules

00:22:14.640 --> 00:22:17.310
are an order of magnitude faster
than the JavaScript

00:22:17.310 --> 00:22:20.800
equivalents, and they offer
consistent performance.

00:22:20.800 --> 00:22:23.400
And for interactive applications
like a physics

00:22:23.400 --> 00:22:25.810
simulation, system performance
is key.

00:22:25.810 --> 00:22:28.350
If there's a frame that's four
times slower than the previous

00:22:28.350 --> 00:22:31.490
frame, it really gets
weird and the

00:22:31.490 --> 00:22:33.290
experience is just not there.

00:22:36.610 --> 00:22:40.870
So wrapping up, I first want to
talk about some concerns.

00:22:40.870 --> 00:22:43.500
So one, currently Native
Client is only

00:22:43.500 --> 00:22:45.050
available in Chrome.

00:22:45.050 --> 00:22:47.460
But as I've shown,
machine-translated JavaScript

00:22:47.460 --> 00:22:49.820
can offer the same
functionality, but just at a

00:22:49.820 --> 00:22:51.490
performance penalty.

00:22:51.490 --> 00:22:57.310
So for web browsers that don't
have Native Client yet, you

00:22:57.310 --> 00:22:59.460
can fall back to the
machine-translated JavaScript

00:22:59.460 --> 00:23:00.710
implementation.

00:23:02.560 --> 00:23:04.220
Another concern, it's early.

00:23:04.220 --> 00:23:04.940
There's no shipping

00:23:04.940 --> 00:23:06.340
acceleration modules out there.

00:23:06.340 --> 00:23:09.350
There's just some prototypes
and a bunch of sample code.

00:23:09.350 --> 00:23:11.730
But this is the perfect
time to help out.

00:23:11.730 --> 00:23:13.125
You can help make
this a reality.

00:23:16.730 --> 00:23:18.990
So the third concern is that
there are certain classes of

00:23:18.990 --> 00:23:23.410
applications, maybe a
spreadsheet, where if you need

00:23:23.410 --> 00:23:29.670
to keep all of the cells in the
web app side and all of

00:23:29.670 --> 00:23:32.930
the cells in the Native Client
side, or constantly be

00:23:32.930 --> 00:23:35.450
communicating back and forth
with each other, the

00:23:35.450 --> 00:23:38.290
performance gains here are not
going to be as drastic as

00:23:38.290 --> 00:23:39.520
we're seeing.

00:23:39.520 --> 00:23:43.440
But the Native Client
Acceleration Module framework

00:23:43.440 --> 00:23:45.040
is really designed
for these, like--

00:23:45.040 --> 00:23:48.900
90% of your application is a web
app, 10% of it needs to be

00:23:48.900 --> 00:23:51.660
really fast, and you want it
to run in a Native Client

00:23:51.660 --> 00:23:52.910
Acceleration Module.

00:23:57.730 --> 00:23:59.600
So Native Client Acceleration
Modules are going to allow us

00:23:59.600 --> 00:24:02.480
to bring new types of
experiences to the web, things

00:24:02.480 --> 00:24:06.540
that you can do today, but you
can't do it fast enough to

00:24:06.540 --> 00:24:09.350
make it a great experience.

00:24:09.350 --> 00:24:12.210
The usage for you as a web
developer is really

00:24:12.210 --> 00:24:12.710
straightforward.

00:24:12.710 --> 00:24:15.800
It's just like including any
other JavaScript library.

00:24:15.800 --> 00:24:17.930
You don't have to worry about
the C implementation or

00:24:17.930 --> 00:24:18.530
anything like that.

00:24:18.530 --> 00:24:20.180
It's taken care for you.

00:24:20.180 --> 00:24:22.680
You just make some function
calls in JavaScript and it

00:24:22.680 --> 00:24:25.450
works really fast.

00:24:25.450 --> 00:24:29.440
So 90% web app, native
performance where you need it.

00:24:29.440 --> 00:24:31.020
That's what you're going to
get with Native Client

00:24:31.020 --> 00:24:32.270
Acceleration Modules.

00:24:37.040 --> 00:24:38.850
So, to do--

00:24:38.850 --> 00:24:40.930
you've got to go and get
the Native Client SDK.

00:24:40.930 --> 00:24:45.730
You can get that from going
to www.gonacl.com.

00:24:45.730 --> 00:24:48.370
Clone an example Native Client
Acceleration Module.

00:24:48.370 --> 00:24:50.060
You can get that from
the URL there.

00:24:50.060 --> 00:24:52.190
It's coming out of my github.

00:24:52.190 --> 00:24:55.160
You could take that and use
that as the base to start

00:24:55.160 --> 00:24:56.410
writing your own.

00:24:58.770 --> 00:24:59.790
And join the discussion.

00:24:59.790 --> 00:25:02.390
Participate in the mailing list
and start talking with

00:25:02.390 --> 00:25:04.100
other people who are
implementing Native Client

00:25:04.100 --> 00:25:06.290
Acceleration Modules, and
come up with better

00:25:06.290 --> 00:25:09.180
ways of doing this.

00:25:09.180 --> 00:25:10.000
Thank you.

00:25:10.000 --> 00:25:12.260
I'm John McCutchan.

00:25:12.260 --> 00:25:15.040
You can read my blog at
johnmccutchan.com or find me

00:25:15.040 --> 00:25:16.800
on Google+.

00:25:16.800 --> 00:25:19.380
Check out all the code
I read on github or

00:25:19.380 --> 00:25:20.540
check me out on Twitter.

00:25:20.540 --> 00:25:21.790
Thank you.

