WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.191
♪ (music) ♪

00:00:05.741 --> 00:00:06.781
I'm Vijay.

00:00:06.781 --> 00:00:10.383
Today I'll be talking to you
or hopefully convincing you

00:00:10.389 --> 00:00:13.857
that when we try to apply
machine learning to solving problems,

00:00:13.857 --> 00:00:16.789
that we should really be thinking
about designing search spaces

00:00:16.789 --> 00:00:18.818
over solutions to those problems.

00:00:18.912 --> 00:00:21.569
And then we can use
automated machine-learning techniques

00:00:21.569 --> 00:00:25.300
in order to evaluate our ideas
much more efficiently.

00:00:25.300 --> 00:00:27.956
I think a big reason
why a lot of us are here today

00:00:27.967 --> 00:00:30.168
is due to the incredible impact
that machine-learning

00:00:30.168 --> 00:00:32.776
can have on practical problems.

00:00:32.975 --> 00:00:37.001
Two often cited reasons for this
is that we have increasing amounts

00:00:37.001 --> 00:00:40.627
of compute capability
and access to data to train on.

00:00:41.239 --> 00:00:44.926
But I think one other aspect
is all of you-- right?

00:00:44.926 --> 00:00:47.764
There's so many more people involved
in machine-learning today

00:00:47.764 --> 00:00:50.320
that are contributing
and publishing ideas.

00:00:51.470 --> 00:00:54.862
So ThisCraft tries to put this
into perspective by measuring

00:00:54.862 --> 00:00:58.120
how many machine learning papers
are published on <i>Arxiv</i>

00:00:58.120 --> 00:01:00.320
every year since 2009,

00:01:00.320 --> 00:01:03.691
and plotting that against
a Moore's Law exponential growth curve.

00:01:03.691 --> 00:01:06.803
As you can see here,
we've been keeping up with Moore's Law

00:01:06.955 --> 00:01:09.210
2x every two years very, very well.

00:01:09.210 --> 00:01:11.708
And this is demonstrating just
how many new ideas

00:01:11.708 --> 00:01:13.772
are being developed in the field.

00:01:13.772 --> 00:01:16.000
This is a great thing, right?

00:01:16.000 --> 00:01:21.811
So, one concrete way of looking at this
is that in the field of computer vision,

00:01:21.811 --> 00:01:27.452
we've seen Top 1 imaging accuracy
start from something like in the 50% range

00:01:27.452 --> 00:01:29.857
from the <i>AlexNet</i> architecture,

00:01:29.857 --> 00:01:33.807
which, by the way, revolutionized
the field of image classification.

00:01:33.807 --> 00:01:39.457
And every year we've been just
getting better and better up until 2017.

00:01:40.677 --> 00:01:42.177
Now, these improvements haven't come

00:01:42.177 --> 00:01:44.695
just because we've been training
bigger models, right?

00:01:44.695 --> 00:01:47.332
These improvements have also come
from the fact

00:01:47.332 --> 00:01:50.339
that we have lots of great ideas, right?

00:01:50.339 --> 00:01:54.360
Things like batch normalization,
residual or skip connections,

00:01:54.490 --> 00:01:56.761
and various regularization techniques.

00:01:56.761 --> 00:01:59.451
Now, each of these points,
like Jeff mentioned earlier,

00:01:59.451 --> 00:02:02.120
is the result of years of research effort.

00:02:02.120 --> 00:02:04.423
And we build on each other's ideas.

00:02:04.423 --> 00:02:08.117
But one of the challenging things
is how do we keep up with so much,

00:02:08.256 --> 00:02:11.073
so many ideas that are being produced?

00:02:11.073 --> 00:02:12.910
And I also want you
to zoom-in a little bit

00:02:12.910 --> 00:02:15.579
in terms of the complexity
of some of these models.

00:02:15.579 --> 00:02:18.765
So we are going to zoom-in a little bit
on <i>InceptionV4</i> here

00:02:18.765 --> 00:02:21.990
and look at the kind of idea
that's embedded inside there.

00:02:21.990 --> 00:02:28.912
These are two convolutional cell modules
within the <i>Inception-V4</i> architecture.

00:02:28.912 --> 00:02:32.951
Every single one of these arrows
and operations and even the numbers

00:02:33.026 --> 00:02:34.573
was designed by a human.

00:02:34.573 --> 00:02:36.690
Somebody wrote some code

00:02:36.690 --> 00:02:41.233
in order to specify
all of these little details.

00:02:41.233 --> 00:02:42.773
Now, there are high-level reasons

00:02:42.773 --> 00:02:45.719
why this kind of architecture
might make sense,

00:02:45.719 --> 00:02:49.569
but our theory doesn't really explain
with so much certainty

00:02:49.569 --> 00:02:52.693
how every detail seems to matter.

00:02:52.693 --> 00:02:55.338
And, as a field, I think
we're definitely working on trying

00:02:55.338 --> 00:02:57.229
to improve the theory behind this.

00:02:57.229 --> 00:03:00.164
But for many of us, we're just happy
to use this kind of complexity

00:03:00.164 --> 00:03:04.316
out of the box, if we can,
because it really helps to solve problems.

00:03:06.066 --> 00:03:09.798
Now this isn't too surprising-- we know
that because machine-learning

00:03:09.798 --> 00:03:13.590
has had such an impact on real products
that we are going to be

00:03:13.590 --> 00:03:15.945
willing to use
anything we possibly can.

00:03:15.945 --> 00:03:19.089
And even if we don't understand
all the little, minor details,

00:03:19.089 --> 00:03:24.491
as long as it solves our problems well
and hopefully, are understandable.

00:03:24.491 --> 00:03:26.131
So, given all these ideas,

00:03:26.131 --> 00:03:30.177
how can we harness this explosion
of ideas much more efficiently?

00:03:30.957 --> 00:03:35.010
So let's step back and kind of ask
a few questions that we might've heard

00:03:35.010 --> 00:03:37.104
when we were just trying 
to train machine-learning models.

00:03:37.104 --> 00:03:39.880
These are simple questions,
but hard questions.

00:03:39.880 --> 00:03:42.960
Like, "What learning rate should I apply
for my optimization?"

00:03:42.960 --> 00:03:45.308
Or, if I am training
a <i>Deep Neural Network</i> model,

00:03:45.308 --> 00:03:47.483
"What dropout rate should I apply?"

00:03:48.393 --> 00:03:50.721
How do we answer this question today?

00:03:50.721 --> 00:03:53.871
I think we combine a few
different types of benefits.

00:03:53.871 --> 00:03:55.410
One of them is

00:03:55.410 --> 00:03:58.971
leveraging researcher intuition
and engineering intuition.

00:03:59.371 --> 00:04:03.318
And what this means is that we start
with code or we ask our colleagues,

00:04:03.318 --> 00:04:07.729
"Hey, what are good settings
for these fields?"

00:04:07.729 --> 00:04:10.504
And if it were the case that there was 
one setting that works for everybody,

00:04:10.504 --> 00:04:12.898
we wouldn't even be looking
at these parameters.

00:04:12.898 --> 00:04:14.214
But it does matter.

00:04:14.214 --> 00:04:16.638
So then we move on
to some trial and error process.

00:04:16.837 --> 00:04:18.503
We try a certain setting
and then we see

00:04:18.503 --> 00:04:22.438
how well it works on our problem
and then we continue to iterate.

00:04:22.691 --> 00:04:26.385
And I think the other aspect, 
which is becoming more common, hopefully,

00:04:26.385 --> 00:04:30.796
is increasing access to computing data
by which we can evaluate these ideas.

00:04:31.737 --> 00:04:36.211
So this combination is really ripe
for automation, right?

00:04:36.323 --> 00:04:38.543
And not surprisingly, this exists today.

00:04:38.543 --> 00:04:42.224
It's called <i>hyperparameter optimization</i>
and in this kind of set up what we have

00:04:42.224 --> 00:04:44.622
is a tuner that's giving out settings

00:04:44.622 --> 00:04:46.885
for these hyperparameter settings.

00:04:46.885 --> 00:04:51.087
We have a trainer
that trains our model our dataset,

00:04:51.087 --> 00:04:52.944
and then tries to give some kind of signal

00:04:52.944 --> 00:04:54.846
about how good those settings were.

00:04:54.846 --> 00:04:58.116
So it might give a validation accuracy
of some value,

00:04:58.116 --> 00:05:02.292
and the tuner can then learn
from this feedback to find better points

00:05:02.292 --> 00:05:04.712
from the search space.

00:05:04.712 --> 00:05:07.873
This is an existing big field
and there are existing systems

00:05:07.873 --> 00:05:13.150
like those shown at the very bottom
that can help you to do this.

00:05:13.150 --> 00:05:15.178
But now, let's kind of ask a few more

00:05:15.178 --> 00:05:17.397
complicated questions
or detailed questions

00:05:17.397 --> 00:05:20.644
that I think people do
often ask as well.

00:05:20.644 --> 00:05:22.588
"Why do you use batch norm before relu?"

00:05:22.588 --> 00:05:25.251
"I switched the order
and it seems to work better."

00:05:25.251 --> 00:05:28.543
Or, if you're trying to train
a completely new model,

00:05:28.543 --> 00:05:32.310
"Should I use one type of sub-architecture
or another type of sub-architecture?"

00:05:33.867 --> 00:05:34.964
Now, if you think about it,

00:05:34.964 --> 00:05:39.195
these questions aren't really that
different from hyperparameter settings.

00:05:39.955 --> 00:05:43.699
So, if we think
of hyperparameter optimization

00:05:43.869 --> 00:05:46.936
as searching over
a specific domain of ideas,

00:05:46.936 --> 00:05:49.838
then it seems possible that maybe
we can actually treat the decisions

00:05:49.838 --> 00:05:51.801
made in this type of model

00:05:51.801 --> 00:05:55.927
as another form of searching
over a domain of ideas.

00:05:56.687 --> 00:06:00.209
And we can therefore think 
about de-emphasizing any specific decision

00:06:00.209 --> 00:06:02.480
that we make in our architectures,

00:06:02.480 --> 00:06:05.998
and, instead, think about the search space
of ideas that we might have.

00:06:05.998 --> 00:06:09.658
So let's take a concrete example
of a search space design

00:06:09.658 --> 00:06:12.461
that my colleague Derek did,
where he tried to design

00:06:12.461 --> 00:06:15.278
a search space for a convolutional cell.

00:06:15.278 --> 00:06:19.073
I'll walk you through
how you might design such a search space.

00:06:21.373 --> 00:06:24.359
So the first question is--
you have to get your inputs.

00:06:24.359 --> 00:06:27.320
So we might say, you have access
to your previous input,

00:06:27.468 --> 00:06:29.585
and if you want to have support
for things like skip connections,

00:06:29.585 --> 00:06:32.511
you might have
the previous-previous input.

00:06:32.511 --> 00:06:35.909
So the first job in the search space
is to define which inputs

00:06:35.909 --> 00:06:37.799
I am going to select.

00:06:37.799 --> 00:06:41.558
And then, once you have
those inputs selected,

00:06:41.558 --> 00:06:44.087
you want to then to figure out
what operation should I apply

00:06:44.087 --> 00:06:47.880
to each of those inputs
before summing them together.

00:06:47.880 --> 00:06:50.639
So I might select something
like 3x3 convolution,

00:06:50.639 --> 00:06:55.360
or 3x3 max pooling and then combine
those operations together.

00:06:55.360 --> 00:06:59.020
Now, we can then recursively 
turn that crank

00:06:59.020 --> 00:07:01.006
and apply it several more times,

00:07:01.006 --> 00:07:04.262
where we use, maybe, different operations
for different inputs.

00:07:04.262 --> 00:07:05.427
And you can even use

00:07:05.427 --> 00:07:09.945
the intermediate outputs 
of previous decisions in our search.

00:07:09.945 --> 00:07:13.307
And then, finally, you take
all of your outputs, that are unused,

00:07:13.307 --> 00:07:15.077
and you can catenate them together.

00:07:15.169 --> 00:07:17.526
And that is your convolutional cell.

00:07:17.927 --> 00:07:20.862
And then finally, if you want to build
your whole model just like ResNet,

00:07:20.862 --> 00:07:23.171
you might stack that same model
several times.

00:07:23.171 --> 00:07:25.575
Now, I want to point out
that this is just one example.

00:07:25.575 --> 00:07:27.584
Like one point
from the search space of ideas.

00:07:27.584 --> 00:07:29.486
There are a billion possible ways

00:07:29.486 --> 00:07:32.835
that you can construct a cell like this
in the search space.

00:07:32.835 --> 00:07:35.477
by changing the set of operations
you have on your list,

00:07:35.477 --> 00:07:40.042
the kind of ways that the connections
can be made, and so forth.

00:07:41.482 --> 00:07:44.227
So, now, we've defined our search space,

00:07:44.227 --> 00:07:47.220
we kind of go back to that type
of parameter-tuning type system

00:07:47.220 --> 00:07:49.755
where we have
a program generator on the left

00:07:49.755 --> 00:07:54.179
that generates samples
from this search space.

00:07:54.179 --> 00:07:57.152
We then train and evaluate
on the task at hand,

00:07:57.152 --> 00:07:59.365
and often time it's going
to be a proxy task.

00:07:59.365 --> 00:08:01.517
And then we iterate to quickly find

00:08:01.709 --> 00:08:04.849
what are the best programs
from our search space.

00:08:04.849 --> 00:08:07.535
And the system on the left-- 
this program generator--

00:08:07.537 --> 00:08:09.771
can optionally learn from feedback.

00:08:09.771 --> 00:08:12.559
So it might use something like
reinforcement learning,

00:08:12.559 --> 00:08:14.127
evolutionary algorithms,

00:08:14.127 --> 00:08:17.232
or even random search
can work well in certain situations.

00:08:19.362 --> 00:08:21.132
So we did this type of approach.

00:08:21.132 --> 00:08:24.851
We took this convolutional cell,
we trained it on a proxy task,

00:08:24.858 --> 00:08:27.599
such as <i>Cifar10</i>
for a small number of epochs

00:08:27.599 --> 00:08:30.671
to make quick progress
on the evaluation of an idea.

00:08:30.854 --> 00:08:34.521
And then we took the best candidate-cells
that we found from that search.

00:08:34.673 --> 00:08:38.348
We enlarged in terms of number of filters
and number of times we've stacked it,

00:08:38.348 --> 00:08:41.138
and we applied that
to the <i>ImageNet</i> dataset.

00:08:41.138 --> 00:08:44.252
And these are two cells
found from that search.

00:08:44.252 --> 00:08:46.816
If you then look at the results
of applying those,

00:08:46.816 --> 00:08:48.576
you'll see that we were actually able

00:08:48.576 --> 00:08:52.127
to do better than the existing state
of the art models,

00:08:52.127 --> 00:08:54.418
in terms of Top-1 accuracy.

00:08:56.007 --> 00:08:59.594
Now, this effort was an example
where we took a model

00:08:59.594 --> 00:09:02.209
where decisions were pretty complex

00:09:02.372 --> 00:09:06.191
and we, honestly, found
another complex model that was better.

00:09:06.366 --> 00:09:10.856
But next, I'll show you an example
where we can use this general technique

00:09:10.856 --> 00:09:13.897
to find even more interpretable outputs.

00:09:15.647 --> 00:09:18.662
So, let's look
at optimization update rules.

00:09:19.498 --> 00:09:23.141
Most of you are, probably, familiar
with stochastic gradient descent,

00:09:23.141 --> 00:09:26.498
and the update rule is shown on the left
where you take the gradient

00:09:26.498 --> 00:09:30.373
and you multiply it by the learning rate
and that's your way to delta.

00:09:30.540 --> 00:09:33.530
And then we have
more advanced optimization update rules,

00:09:33.530 --> 00:09:38.532
like <i>Adam,</i> and these can be expressed
fairly concisely just by being given

00:09:38.532 --> 00:09:41.571
things like the moving average
of the gradient, and so forth.

00:09:42.629 --> 00:09:47.619
But we really only have like a handful
of this type of optimization update rules

00:09:47.619 --> 00:09:51.595
that we're typically applying
for deep learning, for example.

00:09:51.733 --> 00:09:55.083
What if we, instead, treat
these update equation rules

00:09:55.083 --> 00:09:57.939
as parts of a larger search space?

00:09:57.939 --> 00:10:00.432
And so, you can take these expressions
and turn them

00:10:00.432 --> 00:10:04.669
into a kind of a data flow craft
that describes the update rule.

00:10:04.669 --> 00:10:07.474
By using a small set
of primitive operations

00:10:07.474 --> 00:10:09.324
and ways of combining them,

00:10:09.452 --> 00:10:12.604
we can express these two update rules
using this simple tree,

00:10:12.604 --> 00:10:15.007
but also a lot of other ideas.

00:10:15.007 --> 00:10:18.606
And so you can then turn this crank
on this new search space,

00:10:18.822 --> 00:10:22.132
and try to find
a better optimization update rule.

00:10:23.620 --> 00:10:26.459
So, my colleagues Irwan and Barret
ran this experiment,

00:10:26.459 --> 00:10:28.873
where they took
a fixed convolutional model

00:10:28.873 --> 00:10:31.595
and tried to search
over the optimization rules.

00:10:31.595 --> 00:10:35.055
And they found update equations
that did better than some of the others

00:10:35.055 --> 00:10:37.531
I'd shown you on this particular task.

00:10:38.025 --> 00:10:40.488
And one nice feature of this search space

00:10:40.488 --> 00:10:42.831
is that the results
are much more interpretable.

00:10:42.831 --> 00:10:45.312
If you look
аt the fourth update rule here,

00:10:45.312 --> 00:10:48.837
it's taking the gradient
and multiplying it by some expression

00:10:48.837 --> 00:10:51.040
that essentially says
that when the gradient

00:10:51.164 --> 00:10:54.201
and the moving average of the gradient
agree in their direction,

00:10:54.334 --> 00:10:56.889
then we should take a bigger step
in that direction.

00:10:56.889 --> 00:10:58.382
And when they disagree,

00:10:58.382 --> 00:11:00.386
that we should take a smaller step.

00:11:00.386 --> 00:11:02.954
This is actually a form of momentum.

00:11:03.374 --> 00:11:05.659
And so one thing we can get from this is--

00:11:05.659 --> 00:11:07.546
maybe we should be designing search spaces

00:11:07.546 --> 00:11:10.235
that have more notions of momentum encoded

00:11:10.235 --> 00:11:12.157
within the search space of ideas.

00:11:12.157 --> 00:11:15.446
We may be able to find
even better results.

00:11:19.996 --> 00:11:23.269
So far I've focused on techniques
and search space ideas,

00:11:23.438 --> 00:11:25.677
where we care about accuracy.

00:11:25.677 --> 00:11:28.068
But what's great about searching
over many ideas

00:11:28.068 --> 00:11:31.730
is that we might have the potential
to search over more than just accuracy.

00:11:32.124 --> 00:11:36.172
So, for example, a lot of us care
about inference speed, right?

00:11:36.172 --> 00:11:40.304
We want to take a model and deploy it
on real hardware, real mobile platform.

00:11:40.745 --> 00:11:42.424
And very often we spend a lot of time

00:11:42.424 --> 00:11:44.907
trying to figure out how to take one idea

00:11:44.907 --> 00:11:46.879
and make it fast enough.

00:11:46.879 --> 00:11:50.627
But what if you could, as part
of the search space of ideas,

00:11:50.797 --> 00:11:54.349
find ones that balance both speed
and accuracy?

00:11:55.121 --> 00:11:58.271
So we tried to do this experiment
where we included the run time

00:11:58.271 --> 00:12:02.547
on a real mobile device as part
of this interloop of the evaluation.

00:12:02.547 --> 00:12:05.268
So we tried to focus optimize on both--

00:12:05.268 --> 00:12:07.890
accuracy, as well as inference speed.

00:12:07.890 --> 00:12:10.490
And as this process goes on over time,

00:12:10.490 --> 00:12:13.421
the program generator is able
to find faster models

00:12:13.421 --> 00:12:17.821
while also figuring out how to make
those models even more accurate.

00:12:21.150 --> 00:12:22.849
One interesting side effect of this

00:12:22.849 --> 00:12:25.301
is that when you run searches over ideas,

00:12:25.467 --> 00:12:28.025
the output is actually not just one model,

00:12:28.025 --> 00:12:31.161
it's a culture of models
that implicitly encodes this tradeoff.

00:12:31.161 --> 00:12:34.902
So, this kind of shows you
that we have points along the space

00:12:34.902 --> 00:12:37.569
that provide a tradeoff
between inference speed

00:12:37.569 --> 00:12:38.875
on the multi-platform,

00:12:38.875 --> 00:12:42.010
and accuracy on the dataset
that we're trying to solve.

00:12:42.236 --> 00:12:44.795
And so rather than
manually engineering the one point

00:12:44.795 --> 00:12:46.419
that I really want get working,

00:12:46.419 --> 00:12:49.191
I can get a result that maybe
can be deployed

00:12:49.191 --> 00:12:51.387
on various types of platforms.

00:12:53.878 --> 00:12:56.209
So I'll emphasize this
in maybe a slightly different way.

00:12:56.209 --> 00:12:59.426
Which is that we could define
a search space of ideas

00:12:59.426 --> 00:13:03.265
in TensorFlow and through this automatic
machine-learning process,

00:13:03.382 --> 00:13:05.583
we can get models
that have a guaranteed runtime

00:13:05.583 --> 00:13:08.897
performance target
on a TargetPlatform device.

00:13:08.897 --> 00:13:11.485
One of the nice things
about having an integrated ecosystem

00:13:11.485 --> 00:13:14.847
like TensorFlow is that you can just
use the libraries that convert

00:13:14.847 --> 00:13:16.977
from program to program,

00:13:17.132 --> 00:13:18.912
so that you can get
this end to end pipeline

00:13:18.912 --> 00:13:19.994
working well together.

00:13:19.994 --> 00:13:23.330
There is no manual work required
to specifically tune a model.

00:13:25.806 --> 00:13:30.562
So, let me conclude by returning 
to this process of evaluating ideas

00:13:30.772 --> 00:13:33.639
in this world, where we're trying
to explore different ideas.

00:13:34.689 --> 00:13:36.587
The first is that we design search spaces

00:13:36.587 --> 00:13:39.872
to try to test out a large set
of possible ideas.

00:13:40.522 --> 00:13:43.299
Now, note that when we designed
the search space,

00:13:43.299 --> 00:13:45.520
that required human intuition.

00:13:45.520 --> 00:13:47.908
There is still a need for human ingenuity

00:13:47.908 --> 00:13:49.438
as part of this process.

00:13:49.438 --> 00:13:52.845
So, designing the search space properly,
takes a lot of effort

00:13:52.845 --> 00:13:56.852
but at least it allows you to evaluate
many more ideas much more quickly.

00:13:58.050 --> 00:14:00.238
And when it comes to trial-and-error,

00:14:00.238 --> 00:14:01.777
we had to start thinking about

00:14:01.777 --> 00:14:03.750
how software should be changed,

00:14:03.750 --> 00:14:06.986
so that we can permit
this type of search process.

00:14:06.986 --> 00:14:09.242
So, for example, I think many of us

00:14:09.242 --> 00:14:12.061
have probably written scripts where
you take things like learning rate

00:14:12.061 --> 00:14:14.669
and dropout rate as command-line flags.

00:14:14.848 --> 00:14:18.381
What if you wanted to test out
much more deeper ideas in your programs?

00:14:18.381 --> 00:14:20.808
How do you design it such
that it's much more tunable

00:14:20.932 --> 00:14:22.786
at all levels of your program?

00:14:22.786 --> 00:14:26.234
I think this is a big question
for us to tackle.

00:14:26.960 --> 00:14:30.937
And lastly, we think these ideas
will become increasingly relevant

00:14:30.937 --> 00:14:34.969
as many of you get access to more and more
computation capabilities,

00:14:34.969 --> 00:14:36.721
such as things like TPU pods

00:14:36.721 --> 00:14:39.032
Imagine a world
where all you have to do

00:14:39.032 --> 00:14:41.605
is take your idea,
submit it to an idea bank,

00:14:41.605 --> 00:14:45.983
and you have some like pod of TPUs
crunching overnight, trying to figure out

00:14:45.983 --> 00:14:49.108
which solutions or ideas are the best.

00:14:49.108 --> 00:14:50.527
And then waking up in the morning

00:14:50.527 --> 00:14:52.692
and it telling you,
"these were the good ideas,

00:14:52.692 --> 00:14:55.290
these were the bad ideas," and so forth.

00:14:55.290 --> 00:14:57.278
I think part of the reason
this excites me

00:14:57.278 --> 00:14:58.648
is that automatic machine-learning

00:14:58.648 --> 00:15:02.126
can keep these machines
much more busy than we can.

00:15:02.126 --> 00:15:05.597
We have to sleep, but machines
can keep on turning 24/7.

00:15:06.487 --> 00:15:08.874
So, with that, thanks for listening!

00:15:09.185 --> 00:15:11.237
(applause)

00:15:12.573 --> 00:15:16.168
♪ (music) ♪

