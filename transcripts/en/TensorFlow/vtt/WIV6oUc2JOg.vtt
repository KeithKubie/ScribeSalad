WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.597
♪ (music) ♪

00:00:06.322 --> 00:00:07.554
Alright.

00:00:07.554 --> 00:00:11.155
So yes, I'm Patrick.
I'm a Solutions Strategist from Coca-Cola.

00:00:11.155 --> 00:00:14.029
Today I'm going to share with you
how we're using TensorFlow

00:00:14.029 --> 00:00:16.335
to support some of our largest,
most popular

00:00:16.335 --> 00:00:18.580
digital marketing programs
in North America.

00:00:18.580 --> 00:00:20.889
So, we're going to take
the TensorFlow Dev Summit

00:00:20.889 --> 00:00:23.568
off to a marketing tangent for a minute,

00:00:23.568 --> 00:00:25.064
before we come back.

00:00:25.064 --> 00:00:26.839
Alright, so as a background:

00:00:27.599 --> 00:00:30.811
What is proof of purchase
and what is its relationship to marketing?

00:00:31.401 --> 00:00:33.736
As an example, back in the day,

00:00:33.736 --> 00:00:37.223
folks could clip the barcodes
off their cereal boxes

00:00:37.223 --> 00:00:39.974
and then mail these barcodes
back into the cereal company

00:00:39.974 --> 00:00:41.946
to receive a reward--

00:00:41.946 --> 00:00:45.188
some kind of coupon or prize
back through the mail.

00:00:45.188 --> 00:00:47.511
And this is basic loyalty marketing.

00:00:47.511 --> 00:00:50.057
Brands--in this case, the cereal company--

00:00:50.057 --> 00:00:51.760
rewarding consumers who purchase,

00:00:51.760 --> 00:00:52.942
and at the same time

00:00:52.942 --> 00:00:56.240
opening up a line of communication
between the brand and the consumer.

00:00:57.021 --> 00:01:01.355
Over the last 15-odd years
of marketing digitization,

00:01:01.355 --> 00:01:04.852
this concept has evolved
into digital engagement marketing--

00:01:04.852 --> 00:01:07.028
rewarding consumers
in the moment, in real time,

00:01:07.028 --> 00:01:08.747
through web and mobile channels.

00:01:08.747 --> 00:01:12.546
But often, proof of purchase is still
an important component of that experience.

00:01:13.556 --> 00:01:17.367
We have a very active digital engagement
marketing program at Coca-Cola.

00:01:17.367 --> 00:01:18.568
Through proof of purchase

00:01:18.568 --> 00:01:21.503
our consumers can earn
a magazine subscription,

00:01:21.503 --> 00:01:23.353
or the chance to win a cruise,

00:01:23.353 --> 00:01:25.325
or a vintage vending machine.

00:01:26.005 --> 00:01:28.548
And this is what proof of purchase
looks like at Coke.

00:01:28.548 --> 00:01:31.564
Underneath our bottle caps
and inside those cardboard fridge packs

00:01:31.564 --> 00:01:33.345
that you can buy at the grocery store

00:01:33.345 --> 00:01:36.949
we've printed these
14-character product pincodes.

00:01:36.949 --> 00:01:39.070
These are unique to every product

00:01:39.070 --> 00:01:41.875
and these are what our consumers
enter into our promotions.

00:01:42.445 --> 00:01:44.864
You can enter these in by hand,

00:01:44.864 --> 00:01:46.989
but on your mobile device,
you can scan them.

00:01:47.649 --> 00:01:52.548
This had been the holy grail
of marketing IT at Coke for a long time.

00:01:52.548 --> 00:01:55.434
We looked at both commercial

00:01:55.434 --> 00:01:59.068
and open source optical
character recognition software, OCR,

00:01:59.068 --> 00:02:01.258
but it could never
read these codes very well.

00:02:02.068 --> 00:02:05.288
The problem has to do
with the [failure] of the code.

00:02:05.288 --> 00:02:09.544
So, these are 4 x 7 dot matrix printed.

00:02:10.194 --> 00:02:12.164
The printer head
is about an inch off the surface

00:02:12.164 --> 00:02:13.265
of the cap and fridge pack,

00:02:13.265 --> 00:02:16.583
and these things are flying underneath
that printer head at a very rapid rate.

00:02:16.583 --> 00:02:19.534
So this creates
a lot of visual artifacts--

00:02:19.534 --> 00:02:21.877
things like character skew
and pixel drift--

00:02:21.877 --> 00:02:25.277
things that normal OCR
can't handle very well.

00:02:25.277 --> 00:02:28.049
We knew that if we wanted to unlock
this experience for our consumers,

00:02:28.049 --> 00:02:30.504
we were going to have
to build something from scratch.

00:02:30.844 --> 00:02:32.398
When I look at these codes,

00:02:32.398 --> 00:02:34.830
a couple of characteristics
jump out at me.

00:02:35.250 --> 00:02:38.565
We're using a small alphabet--
let's say, ten characters--

00:02:38.565 --> 00:02:42.606
and there's a decent amount of variability
in the presentation of these characters.

00:02:42.986 --> 00:02:44.749
This reminds me of MNIST--

00:02:44.749 --> 00:02:48.784
the online database
of 60,000 handwritten digit images.

00:02:49.654 --> 00:02:52.542
And Convolutional Neural Networks,
or ConvNets,

00:02:52.542 --> 00:02:55.227
are particular good
at extracting the text from these images.

00:02:55.227 --> 00:02:57.347
I'll probably tell you all
something you already know,

00:02:57.347 --> 00:02:58.347
but here we go.

00:02:58.347 --> 00:03:01.148
ConvNets work by taking an image
and initially breaking it down

00:03:01.148 --> 00:03:02.854
into many smaller pieces,

00:03:02.854 --> 00:03:05.918
and then detecting varied
granular features within these pieces--

00:03:05.918 --> 00:03:08.802
things like edges and textures and colors.

00:03:08.802 --> 00:03:12.517
And these varied granular
feature activations are pulled up, often,

00:03:12.877 --> 00:03:15.391
into a more general feature layer,

00:03:15.391 --> 00:03:18.976
and that's filtered, and those feature
activations are pulled up, and so on,

00:03:18.976 --> 00:03:20.990
until the output of the neural net

00:03:20.990 --> 00:03:23.007
is run through a softmax function,

00:03:23.007 --> 00:03:25.168
which creates a probability distribution

00:03:25.168 --> 00:03:29.440
of a likelihood that a set of objects
exist within the image.

00:03:29.440 --> 00:03:32.153
But ConvNets have a really nice property

00:03:32.153 --> 00:03:34.083
in that they handle the translation

00:03:34.083 --> 00:03:36.012
and variant nature
of the images very well.

00:03:36.012 --> 00:03:37.929
That means, from our perspective,

00:03:37.929 --> 00:03:42.679
they can handle the tilt and twist
of a bottle cap held in someone's hand.

00:03:42.679 --> 00:03:43.900
It's perfect.

00:03:44.860 --> 00:03:47.937
So, this is what we're going to use,
we're going to move forward.

00:03:47.937 --> 00:03:51.596
So, now we need to build our platform,
and that begins with training--

00:03:51.596 --> 00:03:54.327
the beating heart
of any applied AI solution.

00:03:54.579 --> 00:03:56.750
And we knew that we needed
high-quality images

00:03:56.750 --> 00:03:59.498
with accurate labels
of the codes within those images,

00:03:59.498 --> 00:04:01.462
and we likely needed a lot of them.

00:04:01.462 --> 00:04:03.866
We started by generating
a synthetic data set

00:04:03.866 --> 00:04:05.400
of randomly generated strings

00:04:05.400 --> 00:04:07.960
that were superimposed
over blank bottle cap images,

00:04:07.960 --> 00:04:11.236
which were then, in turn,
superimposed over random backgrounds.

00:04:11.236 --> 00:04:14.554
This gave us a base
for transfer learning in the future,

00:04:14.554 --> 00:04:16.868
once we created our real-world data set.

00:04:16.868 --> 00:04:19.512
And we did that by doing a production run
of caps and fridge packs

00:04:19.512 --> 00:04:20.664
out of printing facilities,

00:04:20.664 --> 00:04:23.844
and then distributing those
to multiple third-party suppliers,

00:04:23.844 --> 00:04:25.915
along with some custom tools
that we created

00:04:25.915 --> 00:04:30.526
to allow them to scan a cap
and then label it with a pincode.

00:04:30.526 --> 00:04:32.804
But a really important component
to this process

00:04:32.804 --> 00:04:34.819
was an existing pincode validation service

00:04:34.819 --> 00:04:38.690
that we've had in production
for a long time to support our programs.

00:04:39.450 --> 00:04:42.223
So, any time a trainer labeled an image,

00:04:42.223 --> 00:04:44.300
we'd send that label
through our validation service,

00:04:44.300 --> 00:04:45.753
and if it was a valid pin code,

00:04:45.753 --> 00:04:47.952
we knew we had an accurate label.

00:04:48.962 --> 00:04:52.935
So, this gets our model trained,
and now we need to release it to the wild.

00:04:52.935 --> 00:04:55.575
We had some pretty aggressive
performance requirements.

00:04:55.575 --> 00:04:58.570
We wanted one second
average processing time,

00:04:58.570 --> 00:05:01.592
we wanted 95% accuracy at launch,

00:05:01.592 --> 00:05:05.120
but we also wanted
to host the model remotely for the Web,

00:05:05.120 --> 00:05:08.994
and embed it natively on mobile devices
to support mobile apps.

00:05:08.994 --> 00:05:12.379
So, this means that our model
has to be small--

00:05:12.379 --> 00:05:14.441
small enough to support
over-the-air updates

00:05:14.441 --> 00:05:17.344
as the model improves over time.

00:05:17.344 --> 00:05:19.416
And to help us
improve that model over time

00:05:19.416 --> 00:05:23.056
we created an active learning UI,
User Interface,

00:05:23.056 --> 00:05:25.727
that allows our consumers
to train the model

00:05:25.727 --> 00:05:27.272
once it's in production.

00:05:27.652 --> 00:05:29.707
And that's what this looks like.

00:05:29.707 --> 00:05:32.080
So, if I, as a consumer, scan a cap,

00:05:32.080 --> 00:05:34.337
and the model
cannot infer a valid pincode,

00:05:34.337 --> 00:05:36.923
it sends down to the UI
a per character confidence

00:05:36.923 --> 00:05:38.798
of every character at every position.

00:05:38.798 --> 00:05:41.050
And this can be used to render a screen

00:05:41.050 --> 00:05:43.385
much like what you see here.

00:05:43.385 --> 00:05:44.644
So I, as a user,

00:05:44.644 --> 00:05:49.460
am only directed to address those
particularly low confidence characters.

00:05:49.460 --> 00:05:52.667
I see a couple of red characters there--
I tap them, it brings up a keyboard,

00:05:52.667 --> 00:05:55.577
I correct them,
then I'm entered into my promotion.

00:05:56.587 --> 00:05:58.619
It's a good user experience for me.

00:05:59.029 --> 00:06:04.220
I scan a code and I'm only a few taps away
from being entered into a promotion,

00:06:04.220 --> 00:06:08.638
but on the back end, we now have
extremely valuable data for training,

00:06:08.638 --> 00:06:12.306
because we have the image that created
the invalid difference to begin with,

00:06:12.306 --> 00:06:15.178
as well as the user corrected label

00:06:15.178 --> 00:06:18.322
that they needed to correct
to get into the promotion.

00:06:18.322 --> 00:06:20.096
So, we can throw this into the hopper

00:06:20.096 --> 00:06:22.527
for future rounds of training
to improve the model.

00:06:23.587 --> 00:06:25.809
When you put it all together
this is what it looks like.

00:06:25.809 --> 00:06:27.641
The user takes a picture of a cap,

00:06:27.641 --> 00:06:30.337
the region of interest is found,
the image is normalized.

00:06:30.337 --> 00:06:33.256
It's then sent into our ConvNet model,

00:06:33.256 --> 00:06:36.590
the output of which
is a character probability matrix.

00:06:36.590 --> 00:06:40.623
This is the per character confidence
of every character at every position.

00:06:41.303 --> 00:06:44.168
That is then further analyzed
to create a top-ten prediction.

00:06:44.698 --> 00:06:47.769
Each one of those predictions is vetted
to our Bingo validation service.

00:06:48.219 --> 00:06:51.582
The first one that is valid--
which is often the first one on the list--

00:06:51.582 --> 00:06:53.453
is entered into the promotion,

00:06:53.453 --> 00:06:55.013
and if none of them are valid,

00:06:55.013 --> 00:06:58.050
our user sees
the active learning experience.

00:06:59.020 --> 00:07:03.315
So, our model development effort
went through three [vague] iterations.

00:07:03.315 --> 00:07:06.644
Initially, in an effort
to keep the model size small upfront,

00:07:06.644 --> 00:07:10.399
our data science team used binarization
to normalize the image.

00:07:10.399 --> 00:07:12.383
But this was too lossy.

00:07:12.383 --> 00:07:15.378
It didn't produce enough data
to create an accurate model.

00:07:15.378 --> 00:07:17.629
So, they switched
to best channel conversion,

00:07:17.629 --> 00:07:19.388
which got the accuracy up,

00:07:19.388 --> 00:07:22.810
but then the model size grew too large
to support over-the-air updates.

00:07:23.640 --> 00:07:26.985
So, at this point,
our team starts over. (chuckles)

00:07:26.985 --> 00:07:30.667
They just completely re-architect
the ConvNet using SqueezeNet,

00:07:30.667 --> 00:07:32.740
which is designed to reduce model size

00:07:32.740 --> 00:07:36.944
by reducing the number
of learnable parameters within the model.

00:07:36.944 --> 00:07:39.659
But, after making this move,
we had a problem.

00:07:41.269 --> 00:07:44.325
We started to experience
internal covariate shift,

00:07:44.325 --> 00:07:47.370
which is the result of reducing
the number of learnable parameters.

00:07:47.370 --> 00:07:52.309
And that means that very small changes
to upstream parameter values

00:07:52.309 --> 00:07:56.485
cascaded to huge gyrations
in downstream parameter values.

00:07:56.485 --> 00:07:59.899
So, this slowed
our training process considerably,

00:07:59.899 --> 00:08:03.406
because we had to grind
through this covariate shift

00:08:03.406 --> 00:08:04.965
in order to get the model to converge,

00:08:04.965 --> 00:08:06.665
if it would converge at all.

00:08:06.665 --> 00:08:08.478
So, to solve this problem,

00:08:08.478 --> 00:08:11.187
our team introduced batch normalization,

00:08:11.187 --> 00:08:13.905
which sped up training,
it got the model to converge,

00:08:13.905 --> 00:08:15.786
and now we're exactly where we want to be.

00:08:15.786 --> 00:08:17.228
We have a 5MB model,

00:08:17.228 --> 00:08:19.691
it's a 25-fold decrease
from where we started,

00:08:19.691 --> 00:08:22.786
with accuracy greater than 95%.

00:08:23.496 --> 00:08:25.723
And the results are impressive.

00:08:25.723 --> 00:08:28.627
These are some screen grabs
from a test site that I built,

00:08:28.627 --> 00:08:30.088
and you can see across the top row

00:08:30.088 --> 00:08:33.297
how the model handles
different types of occlusion.

00:08:33.297 --> 00:08:36.218
It also handles translation--
tilting the cap,

00:08:36.218 --> 00:08:38.337
rotation-- twisting the cap,

00:08:38.337 --> 00:08:40.891
and camera focus issues.

00:08:42.241 --> 00:08:44.148
So, you can try this out for yourself.

00:08:44.898 --> 00:08:48.778
I'm going to pitch the newly-launched
Coca-Cola USA app.

00:08:48.778 --> 00:08:51.728
It hit Android and iPhone app stores
a couple of days ago.

00:08:51.728 --> 00:08:54.994
It does many things,
but you can use it to scan a code.

00:08:54.994 --> 00:08:58.227
You can also go online
with your mobile browser

00:08:58.227 --> 00:09:00.188
to <i>coke.com/rewards,</i>

00:09:00.188 --> 00:09:04.010
and take a picture of a code
to be entered into a promotion.

00:09:05.050 --> 00:09:08.388
Alright, so some quick shout-outs--
I can't not mention these folks.

00:09:08.388 --> 00:09:12.207
Quantiphi is the data science team
that built our image processing pipeline

00:09:12.207 --> 00:09:14.661
and the pincode recognition model.

00:09:14.661 --> 00:09:18.361
Digital Platforms &amp; Innovation at Coke,
led by Ellen Duncan.

00:09:18.361 --> 00:09:21.098
She spearheaded this
from the marketing side.

00:09:21.098 --> 00:09:23.815
And then, my people in IT.

00:09:23.815 --> 00:09:26.892
My colleague, Andy Donaldson,
shepherded this into production.

00:09:27.872 --> 00:09:29.416
So, thank you.

00:09:29.416 --> 00:09:31.095
It's been a privilege to speak with you.

00:09:31.805 --> 00:09:34.194
I covered a lot of ground
in ten short minutes.

00:09:34.194 --> 00:09:36.328
There's a lot of stuff
I didn't talk about.

00:09:36.328 --> 00:09:38.354
So, if you have any questions
or any follow-up,

00:09:38.354 --> 00:09:41.547
please feel free to reach out
to me on Twitter @patrickbrandt.

00:09:41.547 --> 00:09:43.264
You can also hit me up on LinkedIn.

00:09:43.264 --> 00:09:49.493
That shortcode URL will get you
to my profile page that's <i>wpb.is/linkedin.</i>

00:09:49.493 --> 00:09:54.330
You can also read an article
I published last year on this solution.

00:09:54.330 --> 00:09:56.057
It's a Google developers' blog

00:09:56.057 --> 00:09:59.968
and you can get there
at <i>wpb.is/tensorflow.</i>

00:10:00.338 --> 00:10:02.857
Alright. So, thank you.

00:10:02.857 --> 00:10:04.112
Next up is Alex.

00:10:04.112 --> 00:10:08.534
And Alex is going to talk to us
about Applied ML in Robotics.

00:10:09.254 --> 00:10:11.104
(applause)

00:10:11.104 --> 00:10:13.353
♪ (music) ♪

