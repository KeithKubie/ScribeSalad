WEBVTT

1
00:00:00.990 --> 00:00:02.490
The Joe Rogan experience.

2
00:00:02.990 --> 00:00:04.070
<v 1>That's what I was kind of getting out.</v>

3
00:00:04.071 --> 00:00:06.320
Are you concerned at all with artificial life?

4
00:00:06.440 --> 00:00:09.650
Are you concerned about the propagation of,
of artificial intelligence?

5
00:00:10.480 --> 00:00:13.900
<v 0>Well,
there are different kinds of artificial life.
So,
um,</v>

6
00:00:14.260 --> 00:00:18.880
one is artificial intelligence and I know people like Elon Musk and late Stephen

7
00:00:18.881 --> 00:00:23.230
hawking or are afraid.
Terrified.
Yeah.
And I think that we need,

8
00:00:23.260 --> 00:00:24.820
whether it's right or not,

9
00:00:24.880 --> 00:00:27.970
I think it's great for us to focus on those risks because if we just say,

10
00:00:27.971 --> 00:00:30.700
oh that's,
that's crazy and we don't focus on it,

11
00:00:30.910 --> 00:00:35.910
it increases the likelihood of these bad things happening.
So Kudos to,

12
00:00:35.911 --> 00:00:38.050
to Elon Musk.
But I also think that we are,

13
00:00:38.140 --> 00:00:43.140
we're a long way away from that threat and we are and we will be enormous

14
00:00:43.781 --> 00:00:47.950
beneficiaries of these technologies.
And that's why my,

15
00:00:47.951 --> 00:00:49.390
I don't want to sound like a broken record,

16
00:00:49.391 --> 00:00:51.490
but that's why I keep saying it's all about values.

17
00:00:51.491 --> 00:00:54.760
If I think we should take those threats very seriously and then say his are so

18
00:00:54.761 --> 00:00:58.150
abstract and we don't agree on them,
it's true.
But like,
like Elon Musk,

19
00:00:58.151 --> 00:01:00.940
I mean they've set up this,
this institute where to say,
well,

20
00:01:00.941 --> 00:01:04.180
what are the dangers,
right?
And then what are the things that we can do now,

21
00:01:04.181 --> 00:01:06.760
what are standards that we can integrate,
for example,

22
00:01:06.761 --> 00:01:08.530
into our computer programming.

23
00:01:08.531 --> 00:01:12.340
And so I mentioned my World Health Organization a committee,
the question is,

24
00:01:12.341 --> 00:01:13.174
well,
what are the,

25
00:01:13.510 --> 00:01:18.190
what are the standards that we can integrate into scientific culture that's not

26
00:01:18.191 --> 00:01:19.001
going to cure everything,

27
00:01:19.001 --> 00:01:21.610
but it may increase the likelihood rather better rather than worse.

28
00:01:21.870 --> 00:01:26.070
<v 1>But isn't there an inherent danger in other companies or other countries rather</v>

29
00:01:26.071 --> 00:01:28.230
not complying with any standards that we said?

30
00:01:28.231 --> 00:01:31.640
Because there would be anti competitive yes.
Like that would,
that would,

31
00:01:31.660 --> 00:01:35.940
it would somehow or another diminish competition or diminish their competitive

32
00:01:35.941 --> 00:01:36.580
edge.

33
00:01:36.580 --> 00:01:39.010
<v 0>It's true.
And that's why,
and that's the balance that we're,</v>

34
00:01:39.040 --> 00:01:42.700
we're going to need to need to hold.
It's,
and it's really hard,

35
00:01:43.060 --> 00:01:48.060
but we have a window of opportunity now to try to get ahead of that.

36
00:01:48.191 --> 00:01:50.590
And like I said,
we have chemical weapons,
biological weapons,

37
00:01:50.591 --> 00:01:55.360
nuclear weapons where we've had international standards that have roughly held,

38
00:01:55.390 --> 00:01:59.080
I mean there was a time when slavery was the norm and there was a movement to

39
00:01:59.081 --> 00:02:02.410
say this is,
this is wrong.
And it was largely successful.

40
00:02:02.411 --> 00:02:07.180
So we have history of being more successful rather than,

41
00:02:07.240 --> 00:02:10.450
uh,
than less.
And I think that's the goal.
But you're,
you're right.

42
00:02:10.451 --> 00:02:14.250
I mean this is a race between the technology and the best value.

43
00:02:14.380 --> 00:02:14.630
<v 1>My,</v>

44
00:02:14.630 --> 00:02:18.560
I'm real concerned about artificial intelligence is that this paradigm shifting

45
00:02:18.561 --> 00:02:22.310
moment will happen before we recognize it's happening and then it'll be

46
00:02:22.320 --> 00:02:25.080
<v 0>too late.
Yes,
that's exactly right.
And that's like I saying,
that's,</v>

47
00:02:25.230 --> 00:02:26.160
that's why I've written the book.

48
00:02:26.430 --> 00:02:30.300
That's why I'm out on the road so much talking to people.
Why,
why?

49
00:02:30.301 --> 00:02:33.390
It's such an honor for me to be in pleasure for me to be here with you talking

50
00:02:33.391 --> 00:02:33.631
about it.

51
00:02:33.631 --> 00:02:38.631
Cause we have to reach out to people and people can't be afraid of entering this

52
00:02:39.751 --> 00:02:42.780
conversation because it feels too technical or it feels like it's somebody

53
00:02:42.781 --> 00:02:43.441
else's business.

54
00:02:43.441 --> 00:02:47.400
This is all of our business because this is all of our lives and it's all of our

55
00:02:47.401 --> 00:02:48.000
futures.

56
00:02:48.000 --> 00:02:50.280
<v 1>So if in the future,
you think 20 years,</v>

57
00:02:50.281 --> 00:02:53.580
the thing that's gonna really change the most is predictive genetics and to be

58
00:02:53.581 --> 00:02:57.750
able to be able to predict accurately a person's health.

59
00:02:57.930 --> 00:03:00.130
What do you think in life,
health and law.
Yeah.
What

60
00:03:00.130 --> 00:03:04.360
<v 0>do you think is going to be the biggest detriment for all this stuff and the</v>

61
00:03:04.361 --> 00:03:08.080
thing that we have to avoid the most?
Yeah,
so one is,
as I mentioned,

62
00:03:08.081 --> 00:03:09.310
this determinism,

63
00:03:09.400 --> 00:03:14.400
just because if we just tend to take our sense of wonder about what it means to

64
00:03:14.831 --> 00:03:19.600
be a human away,
like that's really going to harm us.

65
00:03:19.601 --> 00:03:23.340
We talked about equity and access to these technologies and,

66
00:03:23.660 --> 00:03:28.660
and the technologies don't even need to be real in order to have a negative

67
00:03:29.410 --> 00:03:31.660
impact.
So in India,

68
00:03:31.870 --> 00:03:35.530
there are no significant genetic differences between people in different castes,

69
00:03:35.531 --> 00:03:39.160
but the caste system has been maintained for thousands of years because people

70
00:03:39.161 --> 00:03:43.330
just have accepted these,
uh,
these differences.
So this,

71
00:03:43.331 --> 00:03:46.780
it's a whole new way of,
of understanding what is a human.

72
00:03:47.230 --> 00:03:50.110
And it's really going to be complicated and we aren't ready for it.

73
00:03:50.111 --> 00:03:53.980
We weren't ready for it.
Culturally,
we aren't ready for it educationally.

74
00:03:53.981 --> 00:03:57.870
Certainly our political leaders aren't paying much of any attention to all this.

75
00:03:57.880 --> 00:04:00.820
So you have a huge job.
Oof.
Oof.

76
00:04:00.850 --> 00:04:05.410
So when you sit down and you give this speech to Congress,

77
00:04:05.680 --> 00:04:06.513
yeah.
What,

78
00:04:06.520 --> 00:04:11.290
what are you anticipating from them in terms of like what do you think that

79
00:04:11.470 --> 00:04:14.980
there's anything that they can do now?
Absolutely certain steps.
Yes.

80
00:04:15.010 --> 00:04:16.210
So a few things.

81
00:04:16.211 --> 00:04:20.530
One is we need to have a national education campaign.
I mean,

82
00:04:20.531 --> 00:04:21.550
this is so important.

83
00:04:21.551 --> 00:04:25.960
I would say it's on the future of genetics revolution and of Ai because I think

84
00:04:25.961 --> 00:04:28.720
we,
it's just,
it's,
it's crazy.
Um,

85
00:04:28.780 --> 00:04:33.430
that we aren't focusing on these.
Like I,
I learned French in,
in,

86
00:04:33.431 --> 00:04:36.550
um,
in grade school and high school and I'm happy to speak French,

87
00:04:37.000 --> 00:04:41.980
but I would rather have people say this is really important stuff.
So that's,
uh,

88
00:04:41.981 --> 00:04:42.814
that's number one.

89
00:04:43.180 --> 00:04:48.180
Number two is we need to make sure that we have a functioning regulatory system

90
00:04:48.920 --> 00:04:52.660
in,
in this country,
in every country.
And I do a lot of of comparative work.

91
00:04:52.720 --> 00:04:55.390
And like the United Kingdom,
they're really well organized.

92
00:04:55.391 --> 00:04:59.620
They have a national healthcare system which allows them at a national level to

93
00:04:59.650 --> 00:05:04.650
kind of think about longterm care and the tradeoffs in this country.

94
00:05:05.800 --> 00:05:08.560
Average person changes health plans every 18 months.

95
00:05:08.980 --> 00:05:11.320
And I was talking with somebody the other night and they were,

96
00:05:11.350 --> 00:05:15.940
they were working on a predictive health company and they said their first idea

97
00:05:15.941 --> 00:05:19.120
was they were going to sell this information,
um,
to,
uh,

98
00:05:19.121 --> 00:05:22.330
health insurers because like wouldn't this be great if you could?
Hell,

99
00:05:22.331 --> 00:05:24.160
if you're a health insurer and you could,

100
00:05:24.161 --> 00:05:26.800
you had somebody who was your client and you could say,
hey,

101
00:05:26.801 --> 00:05:27.611
here's some information.

102
00:05:27.611 --> 00:05:30.790
You can live healthier and you're not going to have this disease 20 years from

103
00:05:30.791 --> 00:05:32.440
now.
And when he found out is the health insurers,

104
00:05:32.710 --> 00:05:34.690
they could have cared less because people were just,

105
00:05:34.720 --> 00:05:37.180
they were only going to be part of it for a year and a half.

106
00:05:37.181 --> 00:05:42.181
So we really need to think differently about how do we invest in people over the

107
00:05:42.251 --> 00:05:45.460
course of their lives.
And certainly education is one,

108
00:05:45.461 --> 00:05:48.040
but thinking longterm about health and wellbeing is another.

