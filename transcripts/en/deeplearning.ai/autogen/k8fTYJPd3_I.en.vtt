WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.149
 
there's an algorithm called momentum or

00:00:02.149 --> 00:00:02.159
there's an algorithm called momentum or
 

00:00:02.159 --> 00:00:04.789
there's an algorithm called momentum or
gradient descent with momentum that

00:00:04.789 --> 00:00:04.799
gradient descent with momentum that
 

00:00:04.799 --> 00:00:06.950
gradient descent with momentum that
almost always works faster than the

00:00:06.950 --> 00:00:06.960
almost always works faster than the
 

00:00:06.960 --> 00:00:09.259
almost always works faster than the
standard gradient descent algorithm in

00:00:09.259 --> 00:00:09.269
standard gradient descent algorithm in
 

00:00:09.269 --> 00:00:11.570
standard gradient descent algorithm in
one sentence the basic idea is to

00:00:11.570 --> 00:00:11.580
one sentence the basic idea is to
 

00:00:11.580 --> 00:00:13.310
one sentence the basic idea is to
compute an exponentially weighted

00:00:13.310 --> 00:00:13.320
compute an exponentially weighted
 

00:00:13.320 --> 00:00:15.320
compute an exponentially weighted
average of your gradients and then they

00:00:15.320 --> 00:00:15.330
average of your gradients and then they
 

00:00:15.330 --> 00:00:17.599
average of your gradients and then they
use that gradient update your ways

00:00:17.599 --> 00:00:17.609
use that gradient update your ways
 

00:00:17.609 --> 00:00:20.210
use that gradient update your ways
instead in this video let's unpack that

00:00:20.210 --> 00:00:20.220
instead in this video let's unpack that
 

00:00:20.220 --> 00:00:22.340
instead in this video let's unpack that
one sentence description and see how you

00:00:22.340 --> 00:00:22.350
one sentence description and see how you
 

00:00:22.350 --> 00:00:24.620
one sentence description and see how you
can actually implement this as the most

00:00:24.620 --> 00:00:24.630
can actually implement this as the most
 

00:00:24.630 --> 00:00:26.480
can actually implement this as the most
of an example let's say that you're

00:00:26.480 --> 00:00:26.490
of an example let's say that you're
 

00:00:26.490 --> 00:00:28.910
of an example let's say that you're
trying to optimize a cost function which

00:00:28.910 --> 00:00:28.920
trying to optimize a cost function which
 

00:00:28.920 --> 00:00:31.310
trying to optimize a cost function which
has contours like this so the red dots

00:00:31.310 --> 00:00:31.320
has contours like this so the red dots
 

00:00:31.320 --> 00:00:34.370
has contours like this so the red dots
denote the position of the minimum maybe

00:00:34.370 --> 00:00:34.380
denote the position of the minimum maybe
 

00:00:34.380 --> 00:00:37.970
denote the position of the minimum maybe
you start a gradient descent here and if

00:00:37.970 --> 00:00:37.980
you start a gradient descent here and if
 

00:00:37.980 --> 00:00:39.650
you start a gradient descent here and if
you take one iteration of gradient

00:00:39.650 --> 00:00:39.660
you take one iteration of gradient
 

00:00:39.660 --> 00:00:41.720
you take one iteration of gradient
descent either Batchelor mini-batch

00:00:41.720 --> 00:00:41.730
descent either Batchelor mini-batch
 

00:00:41.730 --> 00:00:43.850
descent either Batchelor mini-batch
mendes and maybe end up heading there

00:00:43.850 --> 00:00:43.860
mendes and maybe end up heading there
 

00:00:43.860 --> 00:00:46.430
mendes and maybe end up heading there
but now you're on the other side of this

00:00:46.430 --> 00:00:46.440
but now you're on the other side of this
 

00:00:46.440 --> 00:00:48.560
but now you're on the other side of this
ellipse you kind of if you take another

00:00:48.560 --> 00:00:48.570
ellipse you kind of if you take another
 

00:00:48.570 --> 00:00:50.630
ellipse you kind of if you take another
step of green descent maybe end up doing

00:00:50.630 --> 00:00:50.640
step of green descent maybe end up doing
 

00:00:50.640 --> 00:00:53.810
step of green descent maybe end up doing
that and then another step another step

00:00:53.810 --> 00:00:53.820
that and then another step another step
 

00:00:53.820 --> 00:00:56.569
that and then another step another step
and so on and you see that gradient

00:00:56.569 --> 00:00:56.579
and so on and you see that gradient
 

00:00:56.579 --> 00:00:59.119
and so on and you see that gradient
descent will you know sort of take a lot

00:00:59.119 --> 00:00:59.129
descent will you know sort of take a lot
 

00:00:59.129 --> 00:01:04.270
descent will you know sort of take a lot
of steps right just slowly oscillate

00:01:04.270 --> 00:01:04.280
of steps right just slowly oscillate
 

00:01:04.280 --> 00:01:07.460
of steps right just slowly oscillate
towards the minimum and these

00:01:07.460 --> 00:01:07.470
towards the minimum and these
 

00:01:07.470 --> 00:01:09.800
towards the minimum and these
up-and-down oscillations slows down

00:01:09.800 --> 00:01:09.810
up-and-down oscillations slows down
 

00:01:09.810 --> 00:01:11.870
up-and-down oscillations slows down
gradient descent and prevents you from

00:01:11.870 --> 00:01:11.880
gradient descent and prevents you from
 

00:01:11.880 --> 00:01:14.570
gradient descent and prevents you from
using a much larger learning rate in

00:01:14.570 --> 00:01:14.580
using a much larger learning rate in
 

00:01:14.580 --> 00:01:16.550
using a much larger learning rate in
particular if you were to use a much

00:01:16.550 --> 00:01:16.560
particular if you were to use a much
 

00:01:16.560 --> 00:01:18.249
particular if you were to use a much
larger learning rate you might end up

00:01:18.249 --> 00:01:18.259
larger learning rate you might end up
 

00:01:18.259 --> 00:01:20.539
larger learning rate you might end up
overshooting and then that diverging

00:01:20.539 --> 00:01:20.549
overshooting and then that diverging
 

00:01:20.549 --> 00:01:22.910
overshooting and then that diverging
like so and so the need to prevent the

00:01:22.910 --> 00:01:22.920
like so and so the need to prevent the
 

00:01:22.920 --> 00:01:25.399
like so and so the need to prevent the
oscillations are getting too big forces

00:01:25.399 --> 00:01:25.409
oscillations are getting too big forces
 

00:01:25.409 --> 00:01:27.340
oscillations are getting too big forces
you to use the learning rate as not

00:01:27.340 --> 00:01:27.350
you to use the learning rate as not
 

00:01:27.350 --> 00:01:30.319
you to use the learning rate as not
itself too much another way of viewing

00:01:30.319 --> 00:01:30.329
itself too much another way of viewing
 

00:01:30.329 --> 00:01:32.390
itself too much another way of viewing
this problem is that on the vertical

00:01:32.390 --> 00:01:32.400
this problem is that on the vertical
 

00:01:32.400 --> 00:01:35.359
this problem is that on the vertical
axis you once you're learning to be a

00:01:35.359 --> 00:01:35.369
axis you once you're learning to be a
 

00:01:35.369 --> 00:01:37.580
axis you once you're learning to be a
bit slower because you don't want those

00:01:37.580 --> 00:01:37.590
bit slower because you don't want those
 

00:01:37.590 --> 00:01:40.999
bit slower because you don't want those
oscillations but on the horizontal axis

00:01:40.999 --> 00:01:41.009
oscillations but on the horizontal axis
 

00:01:41.009 --> 00:01:45.800
oscillations but on the horizontal axis
you want to faster learning right

00:01:45.800 --> 00:01:45.810
you want to faster learning right
 

00:01:45.810 --> 00:01:47.660
you want to faster learning right
because you wanted to aggressively move

00:01:47.660 --> 00:01:47.670
because you wanted to aggressively move
 

00:01:47.670 --> 00:01:50.149
because you wanted to aggressively move
from left to right or that minimum - or

00:01:50.149 --> 00:01:50.159
from left to right or that minimum - or
 

00:01:50.159 --> 00:01:52.819
from left to right or that minimum - or
that very thought so here's what you can

00:01:52.819 --> 00:01:52.829
that very thought so here's what you can
 

00:01:52.829 --> 00:01:54.830
that very thought so here's what you can
do if you implement gradient descent

00:01:54.830 --> 00:01:54.840
do if you implement gradient descent
 

00:01:54.840 --> 00:01:57.370
do if you implement gradient descent
with momentum

00:01:57.370 --> 00:01:57.380
with momentum
 

00:01:57.380 --> 00:02:01.389
with momentum
on each iteration or more specifically

00:02:01.389 --> 00:02:01.399
on each iteration or more specifically
 

00:02:01.399 --> 00:02:08.210
on each iteration or more specifically
drink elevation tea you would compute

00:02:08.210 --> 00:02:08.220
drink elevation tea you would compute
 

00:02:08.220 --> 00:02:12.290
drink elevation tea you would compute
the usual derivatives DWD be I omit the

00:02:12.290 --> 00:02:12.300
the usual derivatives DWD be I omit the
 

00:02:12.300 --> 00:02:15.140
the usual derivatives DWD be I omit the
superscript square bracket else but you

00:02:15.140 --> 00:02:15.150
superscript square bracket else but you
 

00:02:15.150 --> 00:02:18.290
superscript square bracket else but you
compute DWD beyond the current

00:02:18.290 --> 00:02:18.300
compute DWD beyond the current
 

00:02:18.300 --> 00:02:21.020
compute DWD beyond the current
mini-batch and we're using bash Korean

00:02:21.020 --> 00:02:21.030
mini-batch and we're using bash Korean
 

00:02:21.030 --> 00:02:22.370
mini-batch and we're using bash Korean
descent then you know the current

00:02:22.370 --> 00:02:22.380
descent then you know the current
 

00:02:22.380 --> 00:02:23.690
descent then you know the current
mini-batch would be just your whole

00:02:23.690 --> 00:02:23.700
mini-batch would be just your whole
 

00:02:23.700 --> 00:02:25.640
mini-batch would be just your whole
batch and this works as well of a batch

00:02:25.640 --> 00:02:25.650
batch and this works as well of a batch
 

00:02:25.650 --> 00:02:27.380
batch and this works as well of a batch
gradient descent so if you're currently

00:02:27.380 --> 00:02:27.390
gradient descent so if you're currently
 

00:02:27.390 --> 00:02:29.570
gradient descent so if you're currently
me batches your entire training set this

00:02:29.570 --> 00:02:29.580
me batches your entire training set this
 

00:02:29.580 --> 00:02:32.089
me batches your entire training set this
works fine as well and then what you do

00:02:32.089 --> 00:02:32.099
works fine as well and then what you do
 

00:02:32.099 --> 00:02:40.539
works fine as well and then what you do
is you compute V D W to be beta v DW

00:02:40.539 --> 00:02:40.549
is you compute V D W to be beta v DW
 

00:02:40.549 --> 00:02:46.580
is you compute V D W to be beta v DW
plus 1 minus beta DW so this is similar

00:02:46.580 --> 00:02:46.590
plus 1 minus beta DW so this is similar
 

00:02:46.590 --> 00:02:49.970
plus 1 minus beta DW so this is similar
to when we're previously computing V

00:02:49.970 --> 00:02:49.980
to when we're previously computing V
 

00:02:49.980 --> 00:02:54.140
to when we're previously computing V
theta equals beta V theta plus 1 minus

00:02:54.140 --> 00:02:54.150
theta equals beta V theta plus 1 minus
 

00:02:54.150 --> 00:02:57.890
theta equals beta V theta plus 1 minus
beta theta T right so it's computing a

00:02:57.890 --> 00:02:57.900
beta theta T right so it's computing a
 

00:02:57.900 --> 00:03:00.979
beta theta T right so it's computing a
moving average of the derivatives for W

00:03:00.979 --> 00:03:00.989
moving average of the derivatives for W
 

00:03:00.989 --> 00:03:03.410
moving average of the derivatives for W
you're getting and then you similarly

00:03:03.410 --> 00:03:03.420
you're getting and then you similarly
 

00:03:03.420 --> 00:03:09.890
you're getting and then you similarly
compute V DP equals that plus 1 minus

00:03:09.890 --> 00:03:09.900
compute V DP equals that plus 1 minus
 

00:03:09.900 --> 00:03:14.930
compute V DP equals that plus 1 minus
beta times DB and then you would update

00:03:14.930 --> 00:03:14.940
beta times DB and then you would update
 

00:03:14.940 --> 00:03:19.819
beta times DB and then you would update
your weights using W J's updated as W

00:03:19.819 --> 00:03:19.829
your weights using W J's updated as W
 

00:03:19.829 --> 00:03:22.069
your weights using W J's updated as W
minus or learning rate times instead of

00:03:22.069 --> 00:03:22.079
minus or learning rate times instead of
 

00:03:22.079 --> 00:03:24.650
minus or learning rate times instead of
updating it with DW with the derivative

00:03:24.650 --> 00:03:24.660
updating it with DW with the derivative
 

00:03:24.660 --> 00:03:27.650
updating it with DW with the derivative
you would updated with v DW and

00:03:27.650 --> 00:03:27.660
you would updated with v DW and
 

00:03:27.660 --> 00:03:31.789
you would updated with v DW and
similarly B of J's updated as B minus

00:03:31.789 --> 00:03:31.799
similarly B of J's updated as B minus
 

00:03:31.799 --> 00:03:36.740
similarly B of J's updated as B minus
alpha times V DB so what this does is

00:03:36.740 --> 00:03:36.750
alpha times V DB so what this does is
 

00:03:36.750 --> 00:03:39.559
alpha times V DB so what this does is
smooth out the steps of gradient descent

00:03:39.559 --> 00:03:39.569
smooth out the steps of gradient descent
 

00:03:39.569 --> 00:03:43.129
smooth out the steps of gradient descent
for example let's say the last few

00:03:43.129 --> 00:03:43.139
for example let's say the last few
 

00:03:43.139 --> 00:03:45.430
for example let's say the last few
derivatives you computer were this this

00:03:45.430 --> 00:03:45.440
derivatives you computer were this this
 

00:03:45.440 --> 00:03:49.250
derivatives you computer were this this
this this this if you average out these

00:03:49.250 --> 00:03:49.260
this this this if you average out these
 

00:03:49.260 --> 00:03:51.349
this this this if you average out these
gradients you find that the oscillations

00:03:51.349 --> 00:03:51.359
gradients you find that the oscillations
 

00:03:51.359 --> 00:03:53.449
gradients you find that the oscillations
in the vertical direction will tend to

00:03:53.449 --> 00:03:53.459
in the vertical direction will tend to
 

00:03:53.459 --> 00:03:54.949
in the vertical direction will tend to
average out to something close to the

00:03:54.949 --> 00:03:54.959
average out to something close to the
 

00:03:54.959 --> 00:03:57.920
average out to something close to the
zero so in the vertical direction where

00:03:57.920 --> 00:03:57.930
zero so in the vertical direction where
 

00:03:57.930 --> 00:04:00.069
zero so in the vertical direction where
you want to slow things down this will

00:04:00.069 --> 00:04:00.079
you want to slow things down this will
 

00:04:00.079 --> 00:04:02.900
you want to slow things down this will
average out positive negative numbers so

00:04:02.900 --> 00:04:02.910
average out positive negative numbers so
 

00:04:02.910 --> 00:04:04.819
average out positive negative numbers so
the average should be close to zero

00:04:04.819 --> 00:04:04.829
the average should be close to zero
 

00:04:04.829 --> 00:04:07.580
the average should be close to zero
whereas on the horizontal direction all

00:04:07.580 --> 00:04:07.590
whereas on the horizontal direction all
 

00:04:07.590 --> 00:04:09.499
whereas on the horizontal direction all
the derivatives are pointing to the

00:04:09.499 --> 00:04:09.509
the derivatives are pointing to the
 

00:04:09.509 --> 00:04:10.400
the derivatives are pointing to the
right and horizont

00:04:10.400 --> 00:04:10.410
right and horizont
 

00:04:10.410 --> 00:04:12.320
right and horizont
direction so the average in a horizontal

00:04:12.320 --> 00:04:12.330
direction so the average in a horizontal
 

00:04:12.330 --> 00:04:14.360
direction so the average in a horizontal
direction will still be prepaid so

00:04:14.360 --> 00:04:14.370
direction will still be prepaid so
 

00:04:14.370 --> 00:04:16.340
direction will still be prepaid so
that's why with this algorithm with a

00:04:16.340 --> 00:04:16.350
that's why with this algorithm with a
 

00:04:16.350 --> 00:04:18.830
that's why with this algorithm with a
few innovations you find that the

00:04:18.830 --> 00:04:18.840
few innovations you find that the
 

00:04:18.840 --> 00:04:21.259
few innovations you find that the
de-rating dissembled momentum ends up

00:04:21.259 --> 00:04:21.269
de-rating dissembled momentum ends up
 

00:04:21.269 --> 00:04:24.010
de-rating dissembled momentum ends up
eventually just taking steps that are

00:04:24.010 --> 00:04:24.020
eventually just taking steps that are
 

00:04:24.020 --> 00:04:26.960
eventually just taking steps that are
much smaller oscillations in a vertical

00:04:26.960 --> 00:04:26.970
much smaller oscillations in a vertical
 

00:04:26.970 --> 00:04:27.620
much smaller oscillations in a vertical
direction

00:04:27.620 --> 00:04:27.630
direction
 

00:04:27.630 --> 00:04:30.320
direction
but are more directed to what the

00:04:30.320 --> 00:04:30.330
but are more directed to what the
 

00:04:30.330 --> 00:04:32.090
but are more directed to what the
horizontal to just moving quickly in the

00:04:32.090 --> 00:04:32.100
horizontal to just moving quickly in the
 

00:04:32.100 --> 00:04:34.340
horizontal to just moving quickly in the
horizontal direction and so this allows

00:04:34.340 --> 00:04:34.350
horizontal direction and so this allows
 

00:04:34.350 --> 00:04:36.290
horizontal direction and so this allows
your algorithm to you know take a more

00:04:36.290 --> 00:04:36.300
your algorithm to you know take a more
 

00:04:36.300 --> 00:04:39.200
your algorithm to you know take a more
straightforward path or less to damp out

00:04:39.200 --> 00:04:39.210
straightforward path or less to damp out
 

00:04:39.210 --> 00:04:42.020
straightforward path or less to damp out
the oscillations in its path to the

00:04:42.020 --> 00:04:42.030
the oscillations in its path to the
 

00:04:42.030 --> 00:04:46.100
the oscillations in its path to the
minimum one intuition for this momentum

00:04:46.100 --> 00:04:46.110
minimum one intuition for this momentum
 

00:04:46.110 --> 00:04:47.450
minimum one intuition for this momentum
which works for some people and not for

00:04:47.450 --> 00:04:47.460
which works for some people and not for
 

00:04:47.460 --> 00:04:50.600
which works for some people and not for
everyone is that if you kind of minimize

00:04:50.600 --> 00:04:50.610
everyone is that if you kind of minimize
 

00:04:50.610 --> 00:04:52.760
everyone is that if you kind of minimize
you know a bowl shape function right

00:04:52.760 --> 00:04:52.770
you know a bowl shape function right
 

00:04:52.770 --> 00:04:55.390
you know a bowl shape function right
this is really the contours of a bowl

00:04:55.390 --> 00:04:55.400
this is really the contours of a bowl
 

00:04:55.400 --> 00:04:57.200
this is really the contours of a bowl
because I'm not very good at drawing

00:04:57.200 --> 00:04:57.210
because I'm not very good at drawing
 

00:04:57.210 --> 00:04:58.970
because I'm not very good at drawing
they trying to minimize this type of

00:04:58.970 --> 00:04:58.980
they trying to minimize this type of
 

00:04:58.980 --> 00:05:01.940
they trying to minimize this type of
shape function then these derivative

00:05:01.940 --> 00:05:01.950
shape function then these derivative
 

00:05:01.950 --> 00:05:05.660
shape function then these derivative
terms you can think of as providing

00:05:05.660 --> 00:05:05.670
terms you can think of as providing
 

00:05:05.670 --> 00:05:09.290
terms you can think of as providing
acceleration to a ball that you're

00:05:09.290 --> 00:05:09.300
acceleration to a ball that you're
 

00:05:09.300 --> 00:05:13.970
acceleration to a ball that you're
rolling downhill and these momentum

00:05:13.970 --> 00:05:13.980
rolling downhill and these momentum
 

00:05:13.980 --> 00:05:17.870
rolling downhill and these momentum
terms you can think of as representing

00:05:17.870 --> 00:05:17.880
terms you can think of as representing
 

00:05:17.880 --> 00:05:22.340
terms you can think of as representing
the velocity and so imagine that you're

00:05:22.340 --> 00:05:22.350
the velocity and so imagine that you're
 

00:05:22.350 --> 00:05:25.580
the velocity and so imagine that you're
a bowl and you take a ball and the

00:05:25.580 --> 00:05:25.590
a bowl and you take a ball and the
 

00:05:25.590 --> 00:05:28.190
a bowl and you take a ball and the
derivative in pause acceleration to this

00:05:28.190 --> 00:05:28.200
derivative in pause acceleration to this
 

00:05:28.200 --> 00:05:30.200
derivative in pause acceleration to this
little ball the little ball is rolling

00:05:30.200 --> 00:05:30.210
little ball the little ball is rolling
 

00:05:30.210 --> 00:05:32.900
little ball the little ball is rolling
down this hill right and so it rolls

00:05:32.900 --> 00:05:32.910
down this hill right and so it rolls
 

00:05:32.910 --> 00:05:35.480
down this hill right and so it rolls
faster and faster because of a

00:05:35.480 --> 00:05:35.490
faster and faster because of a
 

00:05:35.490 --> 00:05:39.170
faster and faster because of a
celebration and beta because this number

00:05:39.170 --> 00:05:39.180
celebration and beta because this number
 

00:05:39.180 --> 00:05:40.970
celebration and beta because this number
a little bit less than one this plays a

00:05:40.970 --> 00:05:40.980
a little bit less than one this plays a
 

00:05:40.980 --> 00:05:43.700
a little bit less than one this plays a
row of friction and it prevents your

00:05:43.700 --> 00:05:43.710
row of friction and it prevents your
 

00:05:43.710 --> 00:05:46.220
row of friction and it prevents your
ball from you know speeding up without

00:05:46.220 --> 00:05:46.230
ball from you know speeding up without
 

00:05:46.230 --> 00:05:49.880
ball from you know speeding up without
limit but so rather than on gradient

00:05:49.880 --> 00:05:49.890
limit but so rather than on gradient
 

00:05:49.890 --> 00:05:51.650
limit but so rather than on gradient
descent just taking every single step

00:05:51.650 --> 00:05:51.660
descent just taking every single step
 

00:05:51.660 --> 00:05:54.110
descent just taking every single step
independently of all previous steps now

00:05:54.110 --> 00:05:54.120
independently of all previous steps now
 

00:05:54.120 --> 00:05:56.300
independently of all previous steps now
your little ball can roll downhill and

00:05:56.300 --> 00:05:56.310
your little ball can roll downhill and
 

00:05:56.310 --> 00:05:59.030
your little ball can roll downhill and
gain momentum is going to sell rate down

00:05:59.030 --> 00:05:59.040
gain momentum is going to sell rate down
 

00:05:59.040 --> 00:06:01.400
gain momentum is going to sell rate down
this bowl and therefore gain momentum I

00:06:01.400 --> 00:06:01.410
this bowl and therefore gain momentum I
 

00:06:01.410 --> 00:06:03.680
this bowl and therefore gain momentum I
find that this ball rolling down the

00:06:03.680 --> 00:06:03.690
find that this ball rolling down the
 

00:06:03.690 --> 00:06:05.720
find that this ball rolling down the
bowl analogy it seems to work for some

00:06:05.720 --> 00:06:05.730
bowl analogy it seems to work for some
 

00:06:05.730 --> 00:06:07.820
bowl analogy it seems to work for some
people who enjoy physics intuitions but

00:06:07.820 --> 00:06:07.830
people who enjoy physics intuitions but
 

00:06:07.830 --> 00:06:09.800
people who enjoy physics intuitions but
it doesn't work for everyone so if this

00:06:09.800 --> 00:06:09.810
it doesn't work for everyone so if this
 

00:06:09.810 --> 00:06:12.650
it doesn't work for everyone so if this
analogy of a ball rolling down a bowl

00:06:12.650 --> 00:06:12.660
analogy of a ball rolling down a bowl
 

00:06:12.660 --> 00:06:14.120
analogy of a ball rolling down a bowl
doesn't work for you don't worry about

00:06:14.120 --> 00:06:14.130
doesn't work for you don't worry about
 

00:06:14.130 --> 00:06:16.730
doesn't work for you don't worry about
it finally let's look at some details on

00:06:16.730 --> 00:06:16.740
it finally let's look at some details on
 

00:06:16.740 --> 00:06:18.409
it finally let's look at some details on
how you implement this here's the

00:06:18.409 --> 00:06:18.419
how you implement this here's the
 

00:06:18.419 --> 00:06:21.399
how you implement this here's the
algorithm and so you now have to

00:06:21.399 --> 00:06:21.409
algorithm and so you now have to
 

00:06:21.409 --> 00:06:24.579
algorithm and so you now have to
Hyper parameters the learning rate alpha

00:06:24.579 --> 00:06:24.589
Hyper parameters the learning rate alpha
 

00:06:24.589 --> 00:06:27.219
Hyper parameters the learning rate alpha
as well as this parameter beta which

00:06:27.219 --> 00:06:27.229
as well as this parameter beta which
 

00:06:27.229 --> 00:06:29.409
as well as this parameter beta which
controls your exponentially weighted

00:06:29.409 --> 00:06:29.419
controls your exponentially weighted
 

00:06:29.419 --> 00:06:31.809
controls your exponentially weighted
average the most common value for beta

00:06:31.809 --> 00:06:31.819
average the most common value for beta
 

00:06:31.819 --> 00:06:34.359
average the most common value for beta
is 0.9 we're averaging over the last 10

00:06:34.359 --> 00:06:34.369
is 0.9 we're averaging over the last 10
 

00:06:34.369 --> 00:06:36.100
is 0.9 we're averaging over the last 10
days temperature so this is like

00:06:36.100 --> 00:06:36.110
days temperature so this is like
 

00:06:36.110 --> 00:06:38.309
days temperature so this is like
averaging or the last 10 iterations

00:06:38.309 --> 00:06:38.319
averaging or the last 10 iterations
 

00:06:38.319 --> 00:06:41.079
averaging or the last 10 iterations
gradients and in practice beta equals

00:06:41.079 --> 00:06:41.089
gradients and in practice beta equals
 

00:06:41.089 --> 00:06:44.320
gradients and in practice beta equals
0.9 works very well feel free to try

00:06:44.320 --> 00:06:44.330
0.9 works very well feel free to try
 

00:06:44.330 --> 00:06:45.820
0.9 works very well feel free to try
different values and do some hyper

00:06:45.820 --> 00:06:45.830
different values and do some hyper
 

00:06:45.830 --> 00:06:48.579
different values and do some hyper
parameter search but 0.9 appears to be a

00:06:48.579 --> 00:06:48.589
parameter search but 0.9 appears to be a
 

00:06:48.589 --> 00:06:50.589
parameter search but 0.9 appears to be a
pretty robust value well in the how

00:06:50.589 --> 00:06:50.599
pretty robust value well in the how
 

00:06:50.599 --> 00:06:52.269
pretty robust value well in the how
about bias correction right so do you

00:06:52.269 --> 00:06:52.279
about bias correction right so do you
 

00:06:52.279 --> 00:06:55.779
about bias correction right so do you
want to take vvw and BTB and divide it

00:06:55.779 --> 00:06:55.789
want to take vvw and BTB and divide it
 

00:06:55.789 --> 00:06:58.569
want to take vvw and BTB and divide it
by 1 minus beta to the T in practice

00:06:58.569 --> 00:06:58.579
by 1 minus beta to the T in practice
 

00:06:58.579 --> 00:06:59.949
by 1 minus beta to the T in practice
people don't usually do this because

00:06:59.949 --> 00:06:59.959
people don't usually do this because
 

00:06:59.959 --> 00:07:03.429
people don't usually do this because
after just 10 iterations your moving

00:07:03.429 --> 00:07:03.439
after just 10 iterations your moving
 

00:07:03.439 --> 00:07:04.989
after just 10 iterations your moving
average will have warmed up and there's

00:07:04.989 --> 00:07:04.999
average will have warmed up and there's
 

00:07:04.999 --> 00:07:07.119
average will have warmed up and there's
no longer a bias estimate so in practice

00:07:07.119 --> 00:07:07.129
no longer a bias estimate so in practice
 

00:07:07.129 --> 00:07:10.329
no longer a bias estimate so in practice
I don't really see people bothering with

00:07:10.329 --> 00:07:10.339
I don't really see people bothering with
 

00:07:10.339 --> 00:07:12.579
I don't really see people bothering with
bias correction when implementing

00:07:12.579 --> 00:07:12.589
bias correction when implementing
 

00:07:12.589 --> 00:07:14.979
bias correction when implementing
gradient descent or momentum and of

00:07:14.979 --> 00:07:14.989
gradient descent or momentum and of
 

00:07:14.989 --> 00:07:16.779
gradient descent or momentum and of
course this process is initialize of a

00:07:16.779 --> 00:07:16.789
course this process is initialize of a
 

00:07:16.789 --> 00:07:20.469
course this process is initialize of a
bTW equals 0 note that this is a matrix

00:07:20.469 --> 00:07:20.479
bTW equals 0 note that this is a matrix
 

00:07:20.479 --> 00:07:23.619
bTW equals 0 note that this is a matrix
of zeros or the same dimension as DW

00:07:23.619 --> 00:07:23.629
of zeros or the same dimension as DW
 

00:07:23.629 --> 00:07:27.279
of zeros or the same dimension as DW
which is the same dimension as W and B

00:07:27.279 --> 00:07:27.289
which is the same dimension as W and B
 

00:07:27.289 --> 00:07:30.399
which is the same dimension as W and B
DB is also initialized to a vector of 0

00:07:30.399 --> 00:07:30.409
DB is also initialized to a vector of 0
 

00:07:30.409 --> 00:07:32.769
DB is also initialized to a vector of 0
so the same dimension as DB which in

00:07:32.769 --> 00:07:32.779
so the same dimension as DB which in
 

00:07:32.779 --> 00:07:34.829
so the same dimension as DB which in
terms of the same dimensions as B

00:07:34.829 --> 00:07:34.839
terms of the same dimensions as B
 

00:07:34.839 --> 00:07:37.329
terms of the same dimensions as B
finally as you mentioned that if you

00:07:37.329 --> 00:07:37.339
finally as you mentioned that if you
 

00:07:37.339 --> 00:07:40.269
finally as you mentioned that if you
read the literature on gradient descent

00:07:40.269 --> 00:07:40.279
read the literature on gradient descent
 

00:07:40.279 --> 00:07:44.290
read the literature on gradient descent
with momentum often you see it with this

00:07:44.290 --> 00:07:44.300
with momentum often you see it with this
 

00:07:44.300 --> 00:07:47.009
with momentum often you see it with this
term omitted which is 1 minus beta term

00:07:47.009 --> 00:07:47.019
term omitted which is 1 minus beta term
 

00:07:47.019 --> 00:07:51.399
term omitted which is 1 minus beta term
omitted so you end up with VD w equals

00:07:51.399 --> 00:07:51.409
omitted so you end up with VD w equals
 

00:07:51.409 --> 00:07:57.999
omitted so you end up with VD w equals
beta v DW plus DW and the net effect of

00:07:57.999 --> 00:07:58.009
beta v DW plus DW and the net effect of
 

00:07:58.009 --> 00:08:00.699
beta v DW plus DW and the net effect of
using this version in purple is that v

00:08:00.699 --> 00:08:00.709
using this version in purple is that v
 

00:08:00.709 --> 00:08:03.759
using this version in purple is that v
DW ends up being scaled by a factor of 1

00:08:03.759 --> 00:08:03.769
DW ends up being scaled by a factor of 1
 

00:08:03.769 --> 00:08:06.549
DW ends up being scaled by a factor of 1
minus beta a really 1 over 1 minus beta

00:08:06.549 --> 00:08:06.559
minus beta a really 1 over 1 minus beta
 

00:08:06.559 --> 00:08:08.439
minus beta a really 1 over 1 minus beta
and so when you're performing these

00:08:08.439 --> 00:08:08.449
and so when you're performing these
 

00:08:08.449 --> 00:08:11.079
and so when you're performing these
gradient descent update alpha just needs

00:08:11.079 --> 00:08:11.089
gradient descent update alpha just needs
 

00:08:11.089 --> 00:08:14.319
gradient descent update alpha just needs
to change by corresponding value of a 1

00:08:14.319 --> 00:08:14.329
to change by corresponding value of a 1
 

00:08:14.329 --> 00:08:17.049
to change by corresponding value of a 1
over 1 minus beta in practice both of

00:08:17.049 --> 00:08:17.059
over 1 minus beta in practice both of
 

00:08:17.059 --> 00:08:18.790
over 1 minus beta in practice both of
these will work just fine it just

00:08:18.790 --> 00:08:18.800
these will work just fine it just
 

00:08:18.800 --> 00:08:21.609
these will work just fine it just
effects um what's the best value of the

00:08:21.609 --> 00:08:21.619
effects um what's the best value of the
 

00:08:21.619 --> 00:08:24.669
effects um what's the best value of the
learning rate alpha but I find that this

00:08:24.669 --> 00:08:24.679
learning rate alpha but I find that this
 

00:08:24.679 --> 00:08:27.159
learning rate alpha but I find that this
particular formulation is a little less

00:08:27.159 --> 00:08:27.169
particular formulation is a little less
 

00:08:27.169 --> 00:08:30.549
particular formulation is a little less
intuitive because one impact of this is

00:08:30.549 --> 00:08:30.559
intuitive because one impact of this is
 

00:08:30.559 --> 00:08:32.589
intuitive because one impact of this is
the end up tuning the hyper parameter

00:08:32.589 --> 00:08:32.599
the end up tuning the hyper parameter
 

00:08:32.599 --> 00:08:33.870
the end up tuning the hyper parameter
beta then the

00:08:33.870 --> 00:08:33.880
beta then the
 

00:08:33.880 --> 00:08:37.320
beta then the
Effects of scaling of bTW and VDB as

00:08:37.320 --> 00:08:37.330
Effects of scaling of bTW and VDB as
 

00:08:37.330 --> 00:08:39.270
Effects of scaling of bTW and VDB as
well and so you end up meeting to retune

00:08:39.270 --> 00:08:39.280
well and so you end up meeting to retune
 

00:08:39.280 --> 00:08:42.750
well and so you end up meeting to retune
the learning rate alpha as well maybe so

00:08:42.750 --> 00:08:42.760
the learning rate alpha as well maybe so
 

00:08:42.760 --> 00:08:44.850
the learning rate alpha as well maybe so
I personally prefer the formulations

00:08:44.850 --> 00:08:44.860
I personally prefer the formulations
 

00:08:44.860 --> 00:08:46.650
I personally prefer the formulations
that I've written here on the left

00:08:46.650 --> 00:08:46.660
that I've written here on the left
 

00:08:46.660 --> 00:08:48.660
that I've written here on the left
rather than leaving out the 1 minus beta

00:08:48.660 --> 00:08:48.670
rather than leaving out the 1 minus beta
 

00:08:48.670 --> 00:08:51.540
rather than leaving out the 1 minus beta
term that I tend to use the formula on

00:08:51.540 --> 00:08:51.550
term that I tend to use the formula on
 

00:08:51.550 --> 00:08:53.820
term that I tend to use the formula on
the left the printer formula with the 1

00:08:53.820 --> 00:08:53.830
the left the printer formula with the 1
 

00:08:53.830 --> 00:08:55.980
the left the printer formula with the 1
minus beta term but for both versions

00:08:55.980 --> 00:08:55.990
minus beta term but for both versions
 

00:08:55.990 --> 00:08:58.530
minus beta term but for both versions
having beta equals 0.9 there's a common

00:08:58.530 --> 00:08:58.540
having beta equals 0.9 there's a common
 

00:08:58.540 --> 00:08:59.820
having beta equals 0.9 there's a common
choice of hyper parameter

00:08:59.820 --> 00:08:59.830
choice of hyper parameter
 

00:08:59.830 --> 00:09:01.830
choice of hyper parameter
it's just that alpha the learning rate

00:09:01.830 --> 00:09:01.840
it's just that alpha the learning rate
 

00:09:01.840 --> 00:09:03.390
it's just that alpha the learning rate
will need to be tuned differently for

00:09:03.390 --> 00:09:03.400
will need to be tuned differently for
 

00:09:03.400 --> 00:09:05.220
will need to be tuned differently for
these two different versions so that's

00:09:05.220 --> 00:09:05.230
these two different versions so that's
 

00:09:05.230 --> 00:09:07.230
these two different versions so that's
it for gradient descent Wolfe momentum

00:09:07.230 --> 00:09:07.240
it for gradient descent Wolfe momentum
 

00:09:07.240 --> 00:09:09.750
it for gradient descent Wolfe momentum
this will almost always work better than

00:09:09.750 --> 00:09:09.760
this will almost always work better than
 

00:09:09.760 --> 00:09:11.670
this will almost always work better than
the straightforward gradient descent

00:09:11.670 --> 00:09:11.680
the straightforward gradient descent
 

00:09:11.680 --> 00:09:14.130
the straightforward gradient descent
algorithm without momentum but there's

00:09:14.130 --> 00:09:14.140
algorithm without momentum but there's
 

00:09:14.140 --> 00:09:15.690
algorithm without momentum but there's
no other things we could do to speed up

00:09:15.690 --> 00:09:15.700
no other things we could do to speed up
 

00:09:15.700 --> 00:09:17.400
no other things we could do to speed up
your learning algorithm let's continue

00:09:17.400 --> 00:09:17.410
your learning algorithm let's continue
 

00:09:17.410 --> 00:09:19.200
your learning algorithm let's continue
talking about these in the next couple

00:09:19.200 --> 00:09:19.210
talking about these in the next couple
 

00:09:19.210 --> 00:09:21.450
talking about these in the next couple
videos

