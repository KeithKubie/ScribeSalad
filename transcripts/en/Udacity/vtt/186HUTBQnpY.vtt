WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:05.140
&gt;&gt; If I say the cat purrs or
this cat hunts mice, it's

00:00:05.140 --> 00:00:09.850
perfectly reasonable to also say the
kitty purrs or this kitty hunts mice.

00:00:11.030 --> 00:00:15.130
The context gives you a strong
idea that those words are similar.

00:00:15.130 --> 00:00:18.850
You have to be catlike to purr and
hunt mice.

00:00:18.850 --> 00:00:21.690
So, let's learn to
predict a word's context.

00:00:21.690 --> 00:00:25.680
The hope is that a model that's good at
predicting a word's context will have to

00:00:25.680 --> 00:00:30.830
treat cat and kitty similarly, and
will tend to bring them closer together.

00:00:30.830 --> 00:00:34.190
The beauty of this approach is that you
don't have to worry about what the words

00:00:34.190 --> 00:00:39.360
actually mean, giving further meaning
directly by the company they keep.

00:00:39.360 --> 00:00:44.350
There are many way to use this idea that
similar words occur in similar contexts.

00:00:44.350 --> 00:00:48.050
In our case, we're going to use
it to map words to small vectors

00:00:48.050 --> 00:00:51.350
called embeddings which are going
to be close to each other

00:00:51.350 --> 00:00:54.700
when words have similar meanings,
and far apart when they don't.

00:00:55.820 --> 00:00:58.570
Embedding solves of
the sparsity problem.

00:00:58.570 --> 00:01:02.680
Once you have embedded your word into
this small vector, now you have a word

00:01:02.680 --> 00:01:07.510
representation where all the catlike
things like cats, kitties, kittens,

00:01:07.510 --> 00:01:12.280
pets, lions, are all represented
by vectors that are very similar.

00:01:12.280 --> 00:01:15.020
Your model no longer has
to learn new things for

00:01:15.020 --> 00:01:17.650
every way there is to talk about a cat.

00:01:17.650 --> 00:01:21.260
It can generalize from this
particular pattern of catlike things.

