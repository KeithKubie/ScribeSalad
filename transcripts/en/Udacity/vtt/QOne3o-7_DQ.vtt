WEBVTT
Kind: captions
Language: en

00:00:00.780 --> 00:00:03.330
So why don't we walk through our solution here. First we

00:00:03.330 --> 00:00:06.120
initialize this variable m, which we say is equal to length

00:00:06.120 --> 00:00:08.340
of values. So we're basically saying here hey, this is how

00:00:08.340 --> 00:00:11.940
many data points we have. We also initialize this variable called

00:00:11.940 --> 00:00:15.170
cost history, which is going to track how our cost function

00:00:15.170 --> 00:00:18.990
evolves over our iterations. We say for i in range num

00:00:18.990 --> 00:00:22.020
iterations, which basically says hey we're going to update theta in

00:00:22.020 --> 00:00:26.040
the costs. And number of times equal to the number of iterations.

00:00:26.040 --> 00:00:28.740
Let's calculate the predicted values. We'll do that by taking

00:00:28.740 --> 00:00:32.430
the dot product of the features and theta. Then we're

00:00:32.430 --> 00:00:35.360
going to update our values of theta. This looks pretty

00:00:35.360 --> 00:00:37.580
complicated, but you'll note that it's the same as the

00:00:37.580 --> 00:00:40.050
equation that we just discussed, which we'll pull up again

00:00:40.050 --> 00:00:44.700
here. So, we subtract our observed values from our predicted

00:00:44.700 --> 00:00:47.430
values, take the dot product of this vector with the

00:00:47.430 --> 00:00:51.140
features, and then essentially multiply by this alpha over m

00:00:51.140 --> 00:00:54.460
feature and subtract that from theta. This is effectively gradient

00:00:54.460 --> 00:00:57.240
descent in action, so we're taking a very small step

00:00:57.240 --> 00:01:00.490
in the direction of the steepest gradient. Then we're going

00:01:00.490 --> 00:01:03.930
to use another function called compute_cost which we see up

00:01:03.930 --> 00:01:07.310
here. So you compute the cost function given our features,

00:01:07.310 --> 00:01:10.900
our values and our theta. Then we'll append the newest

00:01:10.900 --> 00:01:14.450
cost to the cost history list. And once we've done

00:01:14.450 --> 00:01:16.760
this a number of times equal to the number of iterations,

00:01:16.760 --> 00:01:19.330
we'll not only return our final set of thetas,

00:01:19.330 --> 00:01:22.570
but also the entire cost history. You'll see that if

00:01:22.570 --> 00:01:25.320
we run this function, we get this array, which

00:01:25.320 --> 00:01:29.460
are theta values. About 45 minus 9 and 13, and

00:01:29.460 --> 00:01:32.320
these values kind of indicate how significant each factor

00:01:32.320 --> 00:01:35.870
is in determining our predicted value. So this 45 corresponds

00:01:35.870 --> 00:01:38.680
to a constant term, the minus 9 corresponds to

00:01:38.680 --> 00:01:41.800
height and the 13 and change corresponds to the weight.

00:01:41.800 --> 00:01:44.450
And then we see here how the cost function has evolved

00:01:44.450 --> 00:01:48.740
over time. Ideally, if we had a perfect model, this cost function

00:01:48.740 --> 00:01:51.640
would approach zero. You'll see that our cost function doesn't, you

00:01:51.640 --> 00:01:54.640
know, get close to zero. It, it does decrease a good amount

00:01:54.640 --> 00:01:57.480
as we train this model, but it doesn't do super well.

00:01:57.480 --> 00:02:01.100
And so this might lead you logically to a natural question. Which

00:02:01.100 --> 00:02:03.840
is, hey you know not all models are created equal. Maybe

00:02:03.840 --> 00:02:07.230
our model is actually bad. Is there some way we can evaluate

00:02:07.230 --> 00:02:09.660
our models? Why don't we discuss this a little bit more?

