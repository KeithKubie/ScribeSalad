WEBVTT
Kind: captions
Language: en

00:00:00.690 --> 00:00:03.800
Recall that, if we have a random variable x, and

00:00:03.800 --> 00:00:07.320
we want to understand its relationship with another random variable, v.

00:00:07.320 --> 00:00:11.520
The way we do that is we examined the expected loss and

00:00:11.520 --> 00:00:13.930
entropy due to that variable.

00:00:13.930 --> 00:00:17.310
Specifically, to understand the relationship between x and

00:00:17.310 --> 00:00:22.520
y, we examine the expected loss and entropy due to y.

00:00:22.520 --> 00:00:25.840
Similarly, for z, we do exactly the same thing.

00:00:25.840 --> 00:00:29.680
We examine the expected loss and entropy due to z.

00:00:29.680 --> 00:00:32.600
After we know these two quantities the feature that we

00:00:32.600 --> 00:00:37.070
want is the one that results in the biggest drop from h of x.

00:00:37.070 --> 00:00:40.950
So as you can see in order to do this we'll have to calculate three quantities.

00:00:42.050 --> 00:00:47.740
h of x, the conditional entropy, the entropy of x.

00:00:47.740 --> 00:00:52.820
The conditional entropy of x given y and the conditional entropy of x given z.

00:00:53.980 --> 00:00:56.160
I've written them here, over off to the side, so

00:00:56.160 --> 00:00:59.690
that we can keep track of them as we do our calculations.

00:00:59.690 --> 00:01:01.750
Let's go ahead and find them.

00:01:01.750 --> 00:01:03.500
So h of x is just this expression

00:01:04.500 --> 00:01:10.160
minus one times the probability of x times log of probability of x.

00:01:10.160 --> 00:01:14.700
Minus the probability of x2 times the log of probability of x2.

00:01:15.820 --> 00:01:19.720
Now, remember the base of the logarithm doesn't really matter.

00:01:19.720 --> 00:01:22.670
Traditionally, either base two or base e is used.

00:01:22.670 --> 00:01:27.770
Let's assume for these calculations we're using base e.

00:01:27.770 --> 00:01:29.310
So, we can just pull the values for

00:01:29.310 --> 00:01:33.240
the probability of x1 and x2 straight from the joint probability table.

00:01:35.020 --> 00:01:40.802
And if we do that, we find that we get minus one times 0.472

00:01:40.802 --> 00:01:48.914
times log of 0.472 minus 0.528 times log of 0.528 which is approximately 0.691.

00:01:48.914 --> 00:01:52.460
So let's just keep track of that by writing it up here.

00:01:52.460 --> 00:01:54.210
So one of three down.

00:01:54.210 --> 00:01:57.830
Now we just have to do h of x given y and h of x given z.

00:01:59.320 --> 00:02:02.400
Recall that the conditional entropy of h of

00:02:02.400 --> 00:02:05.290
x given y is just a conditional expectation.

00:02:05.290 --> 00:02:11.520
It is the average of how much we expect h of x to change due to y1 and

00:02:11.520 --> 00:02:12.130
due to y2.

00:02:12.130 --> 00:02:16.900
So, the first term is just this expression below.

00:02:16.900 --> 00:02:20.380
So first, so first, let's tackle h of x given y1.

00:02:20.380 --> 00:02:25.790
H of x given y1 is just minus 1 times the probability of x1 given y1,

00:02:25.790 --> 00:02:30.060
times the log of probability x1 given y1,

00:02:30.060 --> 00:02:36.550
minus probability of x2 given y one times the log of probability of x2 given y1.

00:02:36.550 --> 00:02:39.860
And again we can just pull these numbers straight out of the joint probability

00:02:39.860 --> 00:02:41.360
table above and

00:02:41.360 --> 00:02:47.505
if we do that we find we get minus one times .0318 times log .0318.

00:02:47.505 --> 00:02:54.322
Minus 0.779 times log 0.779 and that's approximately 0.3043.

00:02:54.322 --> 00:02:55.927
Let's just make a note of that here.

00:02:55.927 --> 00:03:02.230
The calculation for h of x given y2 proceeds in exactly the same way.

00:03:02.230 --> 00:03:05.754
And if you evaluate that, you get approximately 0.6888.

00:03:06.950 --> 00:03:08.640
And let's just make a note of that here.

00:03:09.840 --> 00:03:16.404
So if we put it all together, h of x given y is .32075

00:03:16.404 --> 00:03:24.934
times .3043 plus .67925 times .6888, which is approximately .566.

00:03:24.934 --> 00:03:27.550
Let's make a note of our second term above.

00:03:28.690 --> 00:03:29.990
Now the calculation for

00:03:29.990 --> 00:03:35.390
the conditional entropy of x given z proceeds in exactly the same way.

00:03:35.390 --> 00:03:37.210
I'm not going to step through the calculation for

00:03:37.210 --> 00:03:40.580
this one as well, I'll leave that to you.

00:03:40.580 --> 00:03:44.400
But you'll find that if you do, you'll get approximately 0.5787.

00:03:44.400 --> 00:03:48.370
So now we have all three of our terms.

00:03:48.370 --> 00:03:53.639
Before knowing anything about y or z, h of x is .691.

00:03:53.639 --> 00:03:57.970
It drops to .566 when we condition on y and

00:03:57.970 --> 00:04:01.490
it drops to .5787 when we condition on z.

00:04:02.620 --> 00:04:06.500
So recall that the feature that we want is the one that results in

00:04:06.500 --> 00:04:09.040
the biggest change in entropy.

00:04:09.040 --> 00:04:13.890
Which is the arg max of h of x minus h of x given a variable.

00:04:13.890 --> 00:04:16.519
Where in this case the variables range over y and z.

00:04:17.890 --> 00:04:21.500
So you can see that in this case the answer is y.

00:04:21.500 --> 00:04:23.890
Although not by that much.

00:04:23.890 --> 00:04:26.890
This difference for y is 0.125.

00:04:26.890 --> 00:04:32.680
Whereas for z it's just slightly less at 0.1123.

00:04:32.680 --> 00:04:37.060
So although y is the winner here, z doesn't do that much worse.

00:04:37.060 --> 00:04:42.770
And so in practice, if I were going to build a model in order to understand x,

00:04:42.770 --> 00:04:46.030
I might consider including both y and z.

00:04:46.030 --> 00:04:48.220
But if I had to choose one, y is the winner.

