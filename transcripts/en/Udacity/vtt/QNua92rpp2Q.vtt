WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:03.050
Okay, so will you work this out? You will need

00:00:03.050 --> 00:00:05.980
to know the frequency of A, B, C, and D. But

00:00:05.980 --> 00:00:09.070
we already know that. You will also need to know

00:00:09.070 --> 00:00:12.190
how many bits each of those symbols require. Now we also

00:00:12.190 --> 00:00:15.840
know that. So, we will calculate the expected number of

00:00:15.840 --> 00:00:18.800
bits to transmit each symbol and then add them up. So

00:00:18.800 --> 00:00:21.820
for any symbol, the expected number of bits is given

00:00:21.820 --> 00:00:26.040
by the probability of seeing that symbol, and the size required

00:00:26.040 --> 00:00:31.940
to transmit that symbol. And we add them up for all the symbols in the language.

00:00:31.940 --> 00:00:39.300
This is going to give us 1.75 bits on an average. Now, since we had to ask

00:00:39.300 --> 00:00:42.170
less questions in this language, than the previous

00:00:42.170 --> 00:00:45.854
language, this language has less information. This is

00:00:45.854 --> 00:00:48.430
also called as variable length encoding. This should

00:00:48.430 --> 00:00:51.140
give you some idea into figuring out why some

00:00:51.140 --> 00:00:54.610
symbols in Morse code are smaller than others. In

00:00:54.610 --> 00:00:57.360
the English alphabet, the letters e and t occur

00:00:57.360 --> 00:01:00.340
most frequently. That's why, in the Morse code, e

00:01:00.340 --> 00:01:02.890
is generated by a dot and t is generated by

00:01:02.890 --> 00:01:06.220
a dash. Since e and t occur more frequently,

00:01:06.220 --> 00:01:09.658
they have the smallest message size. This measure, which calculates

00:01:09.658 --> 00:01:11.980
the number of bits per symbol, is also called

00:01:11.980 --> 00:01:16.720
as entropy. And it is mathematically given as this formula.

00:01:16.720 --> 00:01:19.740
To make it more legible, we need to find out how

00:01:19.740 --> 00:01:23.740
to denote the size of s more properly. The size of s

00:01:23.740 --> 00:01:27.250
is also given by the log of 1 upon the probability

00:01:27.250 --> 00:01:30.770
of that symbol. So the formula of entropy is given as this.

