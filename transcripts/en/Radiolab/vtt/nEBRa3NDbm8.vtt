WEBVTT

1
00:00:00.600 --> 00:00:05.600
You are listening to radio lab radio from W.

2
00:00:05.610 --> 00:00:09.800
N y. S. A. N. N. P. R.

3
00:00:11.490 --> 00:00:14.970
All right. Hey, I'm Jad Abumrad. I'm Robert Krulwich. This is radio lab,

4
00:00:14.971 --> 00:00:18.450
the podcast. So here's the story. We've been falling for a while. Yes,

5
00:00:19.240 --> 00:00:23.180
it comes from a friend of mine, Andrew Zali who is a great thinker and writer.

6
00:00:23.181 --> 00:00:25.550
He wrote a book called resilience, why things bounce back.

7
00:00:25.880 --> 00:00:28.520
And he's a guy who thinks a lot about technology.

8
00:00:28.580 --> 00:00:31.160
I have been interested in a long time, uh,

9
00:00:31.161 --> 00:00:34.520
for a long time in the relationship between, uh,

10
00:00:34.550 --> 00:00:39.320
technology and emotion. Uh, and uh,

11
00:00:39.380 --> 00:00:40.850
because, um,

12
00:00:41.720 --> 00:00:45.200
well I've thrown more than one cell phone to the ground. Uh,

13
00:00:46.130 --> 00:00:49.160
Andrew and I were having breakfast one day and he pitched me on this idea of

14
00:00:49.161 --> 00:00:53.510
doing a story about Facebook. Remember iron, not a huge,

15
00:00:53.570 --> 00:00:55.820
no believer in doing stories about Facebook,

16
00:00:56.060 --> 00:01:00.350
but this story was wickedly interesting and profound in its way.

17
00:01:00.680 --> 00:01:03.160
So he and I had been following it for a couple of years, um,

18
00:01:03.320 --> 00:01:05.750
up and down through this roller coaster of events.

19
00:01:05.751 --> 00:01:09.650
It really begins in 2011 well, let me back up for a minute.

20
00:01:09.710 --> 00:01:14.510
One of the challenges talking about Facebook is just the scale of the thing.

21
00:01:15.810 --> 00:01:16.740
<v 1>So you know, there's,</v>

22
00:01:16.770 --> 00:01:21.770
there's 1.3 billion people on Earth as of March,

23
00:01:22.830 --> 00:01:26.460
2014. Those are active monthly users.

24
00:01:26.461 --> 00:01:29.550
There's a billion people who access the site through, uh,

25
00:01:29.610 --> 00:01:31.050
through mobile devices. [inaudible]

26
00:01:32.410 --> 00:01:33.670
<v 0>just to put that in perspective,</v>

27
00:01:33.671 --> 00:01:37.620
there's more Facebook users than there are Catholics. That can't be true. Yeah,

28
00:01:37.920 --> 00:01:41.950
yeah, yeah. It turns out it is true, but they're neck and neck

29
00:01:42.980 --> 00:01:43.690
<v 2>[inaudible].</v>

30
00:01:43.690 --> 00:01:43.700
<v 0>Anyhow,</v>

31
00:01:43.700 --> 00:01:47.180
the overall point is that when you have one out of every seven people on the

32
00:01:47.181 --> 00:01:52.040
planet in the same space trying to connect across time and geography,

33
00:01:52.100 --> 00:01:54.470
you are bound to create problems.

34
00:01:54.471 --> 00:01:58.400
Sometimes Facebook making headlines again tonight, big issue this time prior.

35
00:02:00.020 --> 00:02:02.990
Before we go there, we should introduce you to the guy in our story.

36
00:02:02.991 --> 00:02:04.160
Who is the problem solver?

37
00:02:04.730 --> 00:02:08.870
My name is [inaudible] and I am the director of engineering at Facebook.

38
00:02:09.600 --> 00:02:11.760
Story Begins Christmas 2011.

39
00:02:13.190 --> 00:02:15.140
<v 3>People are doing what they do every holiday season.</v>

40
00:02:15.230 --> 00:02:17.360
It's just they're getting back together with their families and they're going to

41
00:02:17.361 --> 00:02:21.780
family parties and they're taking lots and lots of pictures. Um,

42
00:02:22.040 --> 00:02:25.250
and they're all uploading them to Facebook. And at the time,

43
00:02:25.430 --> 00:02:29.930
the number of photos that were getting uploaded, what's going pretty crazy.

44
00:02:30.920 --> 00:02:34.790
In fact, in just those few days between Christmas and new years,

45
00:02:34.820 --> 00:02:39.160
there are more images uploaded to Facebook than the word,

46
00:02:39.161 --> 00:02:40.540
the entirety of flicker. Wait,

47
00:02:41.490 --> 00:02:44.250
<v 0>you're saying more images were uploaded in a week to Facebook then?</v>

48
00:02:44.270 --> 00:02:47.930
All a flicker all time. Yeah. Oh.

49
00:02:48.510 --> 00:02:50.550
Which created a situation,

50
00:02:50.580 --> 00:02:53.490
the number of photos was going up and along with the number of photos going up,

51
00:02:53.790 --> 00:02:55.470
the number of reports was going up.

52
00:02:57.840 --> 00:03:01.420
What he means by reports is this, back in 2011,

53
00:03:01.480 --> 00:03:04.150
if you saw something on Facebook that really upset you,

54
00:03:04.300 --> 00:03:07.630
you could click a button to report it.

55
00:03:07.840 --> 00:03:09.370
You could tell Facebook to take it down,

56
00:03:09.371 --> 00:03:14.371
which from their perspective is a really important mechanism because if you're

57
00:03:14.651 --> 00:03:17.950
Facebook, you don't want certain kinds of content on your site.

58
00:03:18.010 --> 00:03:21.310
<v 3>You don't want nudity, you don't want like drug use, um, hate speech,</v>

59
00:03:21.340 --> 00:03:24.970
things like that. So a day or so after Christmas, thereabouts,

60
00:03:25.210 --> 00:03:29.200
Facebook engineers come back to work and they find waiting for them.

61
00:03:29.560 --> 00:03:33.490
Literally millions of photo reports. Yes,

62
00:03:33.580 --> 00:03:36.250
the number of people that would be necessary to review everything that was

63
00:03:36.251 --> 00:03:39.760
coming in. Um, it kind of boggled my mind.

64
00:03:40.180 --> 00:03:41.350
How many people would you have needed?

65
00:03:41.430 --> 00:03:44.200
I think at the time we were looking at it, which was two years ago, and again,

66
00:03:44.201 --> 00:03:48.070
all of this has grown much since then. We're looking at like thousands,

67
00:03:48.070 --> 00:03:48.100
thousands,

68
00:03:48.100 --> 00:03:53.100
<v 0>like some giant facility in Nevada filled with nothing but humans looking at</v>

69
00:03:54.641 --> 00:03:59.380
Christmas porn. We were actually joking about this, but we found out later, um,

70
00:03:59.560 --> 00:04:03.130
there actually are thousands of people across the world who do this for Internet

71
00:04:03.131 --> 00:04:06.400
companies all day long, which clearly warrants its own show.

72
00:04:06.401 --> 00:04:09.820
But for our purposes, just know that when our photo is reported,

73
00:04:09.850 --> 00:04:11.380
a human being has to look at it

74
00:04:11.390 --> 00:04:15.590
<v 3>exactly right because there needs to be a judgment on the image and humans are</v>

75
00:04:15.591 --> 00:04:18.380
the best at that. So Arturo decided,

76
00:04:18.830 --> 00:04:22.460
before we do anything, let's just figure out what we're dealing with.

77
00:04:22.520 --> 00:04:26.570
And so we sat down with a team of people and we started going through the photos

78
00:04:26.571 --> 00:04:27.680
that people were reporting.

79
00:04:27.780 --> 00:04:32.780
<v 0>And what they found was that about 97% of these million or so photo reports were</v>

80
00:04:33.421 --> 00:04:34.590
drastically

81
00:04:35.120 --> 00:04:36.090
<v 3>ms categorized.</v>

82
00:04:38.890 --> 00:04:42.510
They were seeing moms holding little babies reported for harassment,

83
00:04:42.810 --> 00:04:46.260
pictures of families and matching Christmas sweaters reported for nudity.

84
00:04:46.800 --> 00:04:51.000
Pictures of puppies reported for hate speech. Puppies reported as hate speech.

85
00:04:51.060 --> 00:04:54.990
Yes. And we're like, what's going on? Right. Hm. So they decide,

86
00:04:54.991 --> 00:04:59.080
let's investigate. Okay, so step one for Facebook, just

87
00:04:59.460 --> 00:05:02.070
<v 0>ask a few of these people, why don't you like this photo?</v>

88
00:05:02.220 --> 00:05:05.340
Why did you report this responses come back.

89
00:05:05.341 --> 00:05:08.520
And the first thing they realize is that almost always the person complaining

90
00:05:08.521 --> 00:05:13.110
about the image was in the image they were complaining about and they just hate

91
00:05:13.111 --> 00:05:15.300
the picture. Like maybe they were doing a goofy dance,

92
00:05:15.301 --> 00:05:18.060
someone snapped a photo and that's why did you post that? Take it down.

93
00:05:18.270 --> 00:05:21.210
Maybe they were at a party, they got a little too drunk.

94
00:05:21.211 --> 00:05:26.070
They hooked up with their ex. Somebody took a picture and that person says,

95
00:05:26.071 --> 00:05:28.440
Oh, you know, that's a one time thing and it's never happened. And again,

96
00:05:28.500 --> 00:05:29.220
take it down.

97
00:05:29.220 --> 00:05:32.070
Arturo said they were definitely a lot of reports from people who used to be

98
00:05:32.071 --> 00:05:33.690
couples and then they broke up

99
00:05:35.130 --> 00:05:38.400
<v 3>and then they're asking to take the photos down and the puppy. What would,</v>

100
00:05:38.430 --> 00:05:41.550
what would be the reason for that? Oh, because it was maybe a shared puppy,

101
00:05:41.650 --> 00:05:45.220
<v 0>you know, maybe it's your ex wife's puppy, you see it makes you sad</v>

102
00:05:46.030 --> 00:05:48.570
<v 3>get down. So once we've begun investigating,</v>

103
00:05:48.571 --> 00:05:51.690
you find that there's all of this relationship,

104
00:05:51.691 --> 00:05:54.900
things that happen that are like really complicated.

105
00:05:55.170 --> 00:06:00.170
You're talking about stuff that's the kind of natural detritus of human dramas.

106
00:06:01.070 --> 00:06:03.770
<v 0>And the only reason that the person reporting it flagged it,</v>

107
00:06:03.860 --> 00:06:07.250
it's like hate speech is because that was one of the only options.

108
00:06:07.460 --> 00:06:11.210
They were just picking because they needed to get to the next screen to submit

109
00:06:11.211 --> 00:06:12.044
the report.

110
00:06:12.410 --> 00:06:16.190
<v 3>So we edit a step, our turn,</v>

111
00:06:16.310 --> 00:06:19.160
his team set it up so that when people were choosing that option,

112
00:06:19.300 --> 00:06:21.170
I want this photo to be removed from Facebook.

113
00:06:21.171 --> 00:06:24.770
Some of them would see a little box on the screen that said,

114
00:06:25.190 --> 00:06:30.190
how does the photo make you feel in the box gave several choices.

115
00:06:30.561 --> 00:06:35.270
The options were embarrass, embarrassing, upsetting,

116
00:06:35.271 --> 00:06:36.170
bad photo,

117
00:06:36.230 --> 00:06:39.860
and then we always put in an other where you could write in whatever you wanted

118
00:06:40.370 --> 00:06:42.470
about the image and it worked incredibly well.

119
00:06:42.471 --> 00:06:46.610
I mean like 50% of people would select an emotion like for instance,

120
00:06:46.640 --> 00:06:51.640
embarrassing and then 34% of people would select other and we read those.

121
00:06:51.741 --> 00:06:55.190
We sit down and we were reading the other and what was the most frequent thing

122
00:06:55.191 --> 00:06:58.700
that people were typing into other? It was, it's embarrassing.

123
00:06:59.720 --> 00:07:03.470
It's embarrassing, but you had embarrassing on the list. I know. That's weird.

124
00:07:03.590 --> 00:07:04.423
I know.

125
00:07:05.900 --> 00:07:09.690
<v 0>Yes, it's on our Touro is like, okay,</v>

126
00:07:10.140 --> 00:07:14.070
maybe we should just put, it's in front of the choices as in,

127
00:07:14.100 --> 00:07:16.860
please describe this piece of content. It's embarrassing.

128
00:07:16.900 --> 00:07:19.720
<v 3>It's about four of me. It makes me sad, etc.</v>

129
00:07:19.840 --> 00:07:22.720
And when they wrote out the choices that way with that extra word,

130
00:07:22.750 --> 00:07:27.750
we went from 50% of people selecting an emotion to 78% people selecting an

131
00:07:27.821 --> 00:07:28.600
emotion.

132
00:07:28.600 --> 00:07:29.590
<v 0>In other words,</v>

133
00:07:29.620 --> 00:07:34.620
the word it's all by itself boosted the response by 28% from 50 to 78 and in

134
00:07:38.531 --> 00:07:41.560
Facebook land that means thousands and thousands of people.

135
00:07:42.410 --> 00:07:44.470
I'm trying to think of what could, what could that be?

136
00:07:44.530 --> 00:07:48.550
It's to people like full sentences or here's thinking. Um,

137
00:07:48.640 --> 00:07:53.200
it's always good to mirror the way people talk. Right? Arturo's idea though,

138
00:07:53.680 --> 00:07:55.390
which I find kind of interesting is that, um,

139
00:07:55.960 --> 00:07:58.540
when you just say embarrassing and there's no subject,

140
00:07:58.930 --> 00:08:02.380
it's silently implied that you are embarrassing.

141
00:08:02.800 --> 00:08:04.810
But if you say it's embarrassing,

142
00:08:04.811 --> 00:08:08.710
well then that shifts the sort of emotional energy to this photograph thing.

143
00:08:09.100 --> 00:08:12.700
And so then it's less hot and it's easier to deal with. Oh, how interesting.

144
00:08:12.850 --> 00:08:16.870
That thing is embarrassing. I'm fine. It's embarrassing.

145
00:08:16.900 --> 00:08:20.470
It is responsible, not me. Good for our Taro. That's like, it's interesting,

146
00:08:20.750 --> 00:08:21.880
right? It's very subtle,

147
00:08:22.000 --> 00:08:25.450
but it still doesn't solve their basic problem because even if Facebook now

148
00:08:25.451 --> 00:08:29.170
knows why the person flagged the photo that it was embarrassing and not actually

149
00:08:29.171 --> 00:08:31.330
hate speech, they still can't take it down.

150
00:08:31.360 --> 00:08:33.400
I mean there's nothing in the policy,

151
00:08:33.401 --> 00:08:37.300
the terms of service that says you can't put up embarrassing photos.

152
00:08:37.330 --> 00:08:38.710
And in fact if they took it down,

153
00:08:38.711 --> 00:08:41.320
they'd be violating the rights of the person who posted it.

154
00:08:41.510 --> 00:08:43.250
<v 3>Like there's nothing we can do. I'm sorry.</v>

155
00:08:43.390 --> 00:08:47.080
<v 0>Oh, so they'd actually fence themselves in a little bit. Yeah. For me,</v>

156
00:08:47.081 --> 00:08:49.420
I'd always put it in another, I would just be like,

157
00:08:49.580 --> 00:08:51.100
just go deal with it yourself.

158
00:08:51.760 --> 00:08:56.640
But I would say talk to the person know. Honestly that's the solution.

159
00:08:56.790 --> 00:08:58.080
I, he wouldn't put it that way,

160
00:08:58.081 --> 00:09:01.770
but I think what he needed to have happen was for the person who posted the

161
00:09:01.771 --> 00:09:03.570
picture and the person who was pissed about it,

162
00:09:04.110 --> 00:09:06.390
they'll talk to each other to work it out themselves.

163
00:09:07.530 --> 00:09:11.430
So Arturo and his team made a tweak where if you said this photo was

164
00:09:11.431 --> 00:09:16.140
embarrassing or whatever, a new screen would pop up and it would ask,

165
00:09:16.141 --> 00:09:19.200
do you want your friend to take the photo down? And if you said yes,

166
00:09:19.201 --> 00:09:21.690
I would like my stupid friend to take the photo down.

167
00:09:21.720 --> 00:09:23.670
We put up an empty message box,

168
00:09:23.730 --> 00:09:26.790
just an empty box that said,

169
00:09:26.850 --> 00:09:31.050
we think it's a good idea for you to tell the person who upset you that they

170
00:09:31.051 --> 00:09:36.051
upset you and only 20% of people would type something in and send that message.

171
00:09:36.360 --> 00:09:40.020
They just didn't do it. They just said, I, I'd rather you deal with this.

172
00:09:40.380 --> 00:09:43.290
So our tour in his team were like, okay, let's take it one step further.

173
00:09:43.320 --> 00:09:45.150
When that message box popped up,

174
00:09:45.180 --> 00:09:49.500
we gave people a default message that we that'd be crafted to start the

175
00:09:49.501 --> 00:09:52.140
conversation. Just get the conversation going and it's kind of funny.

176
00:09:52.141 --> 00:09:55.740
The first question of the message that we did was like, hey,

177
00:09:55.830 --> 00:09:59.430
I didn't like this photo. Take it down. Hey, I don't like that photo.

178
00:09:59.431 --> 00:10:01.050
That's a little aggressive. It is.

179
00:10:01.380 --> 00:10:05.460
But when they started presenting people with a message box with that sentence

180
00:10:05.461 --> 00:10:06.590
pre-written in your,

181
00:10:06.591 --> 00:10:11.591
almost immediately we went from 20% of people sending a message to 50% of people

182
00:10:12.331 --> 00:10:15.030
sending a message. Really as it's been surprising to all of us,

183
00:10:15.031 --> 00:10:17.180
like we weren't expecting to see that big of a shift. Okay,

184
00:10:17.181 --> 00:10:19.290
so this means that people just don't want to, right.

185
00:10:19.710 --> 00:10:23.220
They'll sign up for pretty much anything? No, not necessarily. No.

186
00:10:23.250 --> 00:10:28.250
Maybe it's just that it's so easy to shirk the responsibility of confronting

187
00:10:28.861 --> 00:10:33.060
another person that you need every little stupid nudge you can get to see, okay,

188
00:10:33.130 --> 00:10:36.180
that's how I see it. Okay, so they put out this pre-written message,

189
00:10:36.181 --> 00:10:40.650
it seems to really have an effect, so they're like, okay, if that works so well,

190
00:10:41.160 --> 00:10:44.670
why don't we try some different wording instead of, hey,

191
00:10:44.850 --> 00:10:48.930
I didn't like this photo. Take it down. Why don't we try? Hey Robert,

192
00:10:49.200 --> 00:10:51.090
I didn't like this photo. Take it down.

193
00:10:51.210 --> 00:10:55.590
Just putting in your name works about 7% better than leaving it out.

194
00:10:56.070 --> 00:10:56.790
Meaning what?

195
00:10:56.790 --> 00:11:00.270
It means that you're 7% more likely either to get the person to do what you

196
00:11:00.590 --> 00:11:01.110
asked them to do.

197
00:11:01.110 --> 00:11:04.320
Take down the photo or to start a conversation about how to resolve your

198
00:11:04.321 --> 00:11:08.670
feelings about, oh, we're now measuring the effectiveness of the message.

199
00:11:08.880 --> 00:11:12.840
If I'm objecting, will the other party pull it off the computer, pull it off,

200
00:11:12.841 --> 00:11:16.170
or just talk to you about it? They also tried variations like, Hey Robert,

201
00:11:16.171 --> 00:11:19.320
would you please take it down throwing in the word please,

202
00:11:19.321 --> 00:11:22.200
or would you mind taking it down?

203
00:11:22.201 --> 00:11:25.350
And it turns out that would you please performance 4% better than what your

204
00:11:25.351 --> 00:11:29.370
mind, you're not totally sure why, but there they tried dozens of phrases like,

205
00:11:29.371 --> 00:11:31.800
would you please mind? Would you mind, I'm sorry to bring this up,

206
00:11:31.801 --> 00:11:33.810
but would you please take it down? I'm sorry to bring this up,

207
00:11:33.811 --> 00:11:35.910
but would you mind taking it down? And at a certain point,

208
00:11:38.820 --> 00:11:41.910
Andrew and I got recruited to see part true.

209
00:11:42.990 --> 00:11:46.020
We just want to see this whole process. They're going through up close.

210
00:11:46.800 --> 00:11:50.190
So we took a trip out to Facebook headquarters, Menlo Park, California,

211
00:11:50.191 --> 00:11:54.430
and this was about a year ago. No, I have done, just before the hubbub,

212
00:11:54.670 --> 00:11:58.070
we met up with Arturo who sort of walked us through the campus. [inaudible]

213
00:11:58.120 --> 00:12:00.260
like the Hammock is kind of a little like hang out.

214
00:12:00.460 --> 00:12:04.840
It's one of these sort of like a socialist utopic silicon valley campuses where

215
00:12:05.290 --> 00:12:08.560
people are like in hammocks and there there's volleyball happening.

216
00:12:08.920 --> 00:12:11.640
We actually have our baby foxes here really?

217
00:12:12.300 --> 00:12:14.140
And they had foxes running around at one point.

218
00:12:14.710 --> 00:12:18.880
So we were there on a Friday because every Friday afternoon Arturo assembles

219
00:12:18.881 --> 00:12:20.110
this really big group

220
00:12:22.030 --> 00:12:27.010
welcome to the Union to review all the data and you've got about 15 people

221
00:12:27.011 --> 00:12:29.440
crammed into a conference room like technical folks,

222
00:12:29.760 --> 00:12:33.130
which Dev on software engineer. Interesting thing at Facebook, Dan feral.

223
00:12:33.131 --> 00:12:34.240
I'm a data scientist.

224
00:12:34.720 --> 00:12:39.010
I'm also an engineer and a lot of these guys called themselves trust engineers

225
00:12:40.640 --> 00:12:43.900
and every Friday the trust engineers are joined by a bunch of outside

226
00:12:43.901 --> 00:12:47.320
scientists. Dacher Keltner, professor psychology, UC Berkeley,

227
00:12:47.350 --> 00:12:51.820
Matt Killingsworth. I studied the causes and nature of human happiness. Liliana,

228
00:12:51.821 --> 00:12:54.580
Simon Thomas and my background is neuroscience.

229
00:12:55.390 --> 00:12:58.540
This is the meeting where the team was reviewing all the data about these

230
00:12:58.541 --> 00:13:02.260
phrases and so everybody was looking at a giant graph projected on the wall,

231
00:13:02.440 --> 00:13:06.760
kind of supporting your, your slightly u shaped curve there in that,

232
00:13:07.080 --> 00:13:10.750
especially in the deletion numbers, the hey, I don't like this photo,

233
00:13:10.751 --> 00:13:13.630
take it down and the, hey I don't like this photo,

234
00:13:13.631 --> 00:13:17.170
would you please take it down? Our are kind of the winters here.

235
00:13:17.930 --> 00:13:21.730
[inaudible] it's kind of interesting that you see the person that's receiving a

236
00:13:21.731 --> 00:13:26.731
more direct message is higher 11% versus 4% one of the things they noticed is

237
00:13:27.220 --> 00:13:30.310
that anytime they used the word sorry and a phrase like, Hey Robert,

238
00:13:30.370 --> 00:13:32.620
sorry to bring this up, but would you please take it down?

239
00:13:32.860 --> 00:13:35.680
Turns out the m sorry doesn't actually help make the numbers go down.

240
00:13:35.950 --> 00:13:38.560
Really the seven and nine are the low.

241
00:13:38.620 --> 00:13:40.740
Some of the low points and those are the ones that say sorry. Okay.

242
00:13:40.930 --> 00:13:42.760
So like don't just don't apologize,

243
00:13:42.761 --> 00:13:46.400
just don't apologize cause it's like it shifts the responsibility back to you I

244
00:13:46.620 --> 00:13:49.860
guess. No it doesn't. It's just, it's just, it's like it's a,

245
00:13:49.861 --> 00:13:54.280
it's a linguistic psychology subtle thing. I'm making that up. I am kind of,

246
00:13:54.460 --> 00:13:54.520
yeah,

247
00:13:54.520 --> 00:13:57.850
but one of the things that really struck me at this meeting on different subject

248
00:13:57.851 --> 00:14:00.400
is that the scientists in the room as they were looking at the graph,

249
00:14:00.690 --> 00:14:01.840
taken in the numbers sufficient.

250
00:14:01.870 --> 00:14:05.100
A lot of them had this look on their face of like, oh, brain's holy.

251
00:14:06.090 --> 00:14:06.310
<v 4>Um,</v>

252
00:14:06.310 --> 00:14:11.290
I'm just stunned and humbled at the numbers that we generally get in in these

253
00:14:11.291 --> 00:14:14.080
studies. It's Emilio, Simon Thomas from Berkeley.

254
00:14:14.230 --> 00:14:17.620
My background is in neuroscience and I'm used to studies where we look at 20

255
00:14:17.621 --> 00:14:21.760
people and that's sufficient to say something general about how brains work.

256
00:14:21.830 --> 00:14:26.090
<v 0>Like in general at Facebook, like people would scoff at sample sizes that small.</v>

257
00:14:27.140 --> 00:14:29.570
That's rob Boyle, who's a project manager at Facebook.

258
00:14:29.630 --> 00:14:32.990
The main and shoes that we're used to working with are in the hundreds of

259
00:14:32.990 --> 00:14:33.250
thousands to millions.

260
00:14:33.250 --> 00:14:35.840
This is kind of an interesting moment because there's been a lot of criticism

261
00:14:35.841 --> 00:14:39.770
recently, especially in social science, about the sample sizes,

262
00:14:40.190 --> 00:14:42.260
how they're too small and how there's,

263
00:14:42.290 --> 00:14:45.410
they're too often filled with white undergraduate college kids.

264
00:14:45.411 --> 00:14:46.910
And how can you generalize from that?

265
00:14:47.270 --> 00:14:50.810
So you could tell that some of the scientists in the room, like for example,

266
00:14:50.811 --> 00:14:54.680
Dacher, he's a psychologist at UC Berkeley and we're like, oh my God,

267
00:14:54.710 --> 00:14:56.090
look at what we can do now.

268
00:14:56.120 --> 00:14:59.150
We can get all these different people of different class backgrounds,

269
00:14:59.151 --> 00:15:02.300
different countries to him, this kind of work with Facebook.

270
00:15:03.480 --> 00:15:05.850
This could be the future of social science right here.

271
00:15:06.060 --> 00:15:10.860
There has never been a human community like this in human history.

272
00:15:12.850 --> 00:15:14.360
It's somewhere in the middle of all the, uh,

273
00:15:14.380 --> 00:15:17.620
excitement about the data and the speed at which they can now test things.

274
00:15:17.800 --> 00:15:21.340
The bottleneck is no longer how fast we can test, how things work.

275
00:15:21.341 --> 00:15:24.970
It's coming up with the right things to test. Andrew threw out a question.

276
00:15:26.410 --> 00:15:29.830
What is the statistical likelihood that I have been a Guinea pig in one of your

277
00:15:29.831 --> 00:15:34.150
experiments? Uh, is I believe 100%, but

278
00:15:36.490 --> 00:15:40.570
[inaudible] any given person that Stan feral data scientist and when we look at

279
00:15:40.571 --> 00:15:43.870
the data, any given person is probably currently involved in what,

280
00:15:43.871 --> 00:15:48.871
10 different experiments and they'd been exposed to to 10 different experimental

281
00:15:49.421 --> 00:15:51.370
things. Yup.

282
00:15:54.160 --> 00:15:55.320
<v 5>That kind of [inaudible]</v>

283
00:15:55.460 --> 00:15:58.030
<v 0>blew me back a little bit. I was like, uh,</v>

284
00:15:58.290 --> 00:16:01.340
may I have been a research subject and I had no idea

285
00:16:04.060 --> 00:16:08.650
coming up. Yeah. Everybody gets the idea and the lab rats revolt.

286
00:16:09.010 --> 00:16:09.843
Stay with us.

287
00:16:12.500 --> 00:16:15.320
<v 6>Hi there. This is feather [inaudible] calling from London, England.</v>

288
00:16:15.920 --> 00:16:19.400
Radiolab is supported in part by the National Science Foundation and by the

289
00:16:19.430 --> 00:16:22.910
offered p Sloan Foundation enhancing public understanding of science and

290
00:16:22.911 --> 00:16:24.290
technology in the modern world.

291
00:16:24.770 --> 00:16:29.000
More information about sloane@wwwdotsloan.org oh,

292
00:16:29.001 --> 00:16:30.350
that's a bit of a tongue twister.

293
00:16:34.410 --> 00:16:37.260
<v 7>Hey guys, this is Alison calling from Luxembourg.</v>

294
00:16:37.350 --> 00:16:40.320
Radiolab is supported by rocket mortgage by quicken loans.

295
00:16:40.500 --> 00:16:43.890
Rocket mortgage brings the mortgage process into the 21st century.

296
00:16:43.891 --> 00:16:45.930
With this completely online process,

297
00:16:46.110 --> 00:16:49.260
safely share your bank statements and pay stubs with the touch of a button.

298
00:16:49.710 --> 00:16:53.250
Ditch the paperwork and use your phone or tablet to get approved for purchase or

299
00:16:53.251 --> 00:16:54.720
refinance in minutes.

300
00:16:54.780 --> 00:16:58.110
Get accustomed mortgage solution whenever and wherever you want.

301
00:16:58.290 --> 00:16:59.460
Check out rocket mortgage

302
00:16:59.461 --> 00:17:04.461
today@quickenloansdotcomslashradiolabequalhousinglenderlicensedinallfiftystatesandmlsconsumeraccess.org

303
00:17:08.071 --> 00:17:10.800
number 30 30 hi,

304
00:17:11.130 --> 00:17:15.090
<v 8>I'm Robert Krulwich. Radiolab is supported by Google and Squarespace.</v>

305
00:17:15.360 --> 00:17:18.390
Make your business official with Google and square space.

306
00:17:18.391 --> 00:17:22.230
When you create a custom domain and a beautiful business website with

307
00:17:22.231 --> 00:17:25.470
Squarespace, you'll receive a free year of business,

308
00:17:25.471 --> 00:17:27.690
email and professional tools from Google.

309
00:17:27.870 --> 00:17:30.300
It's the simplest way to look professional online.

310
00:17:30.420 --> 00:17:34.140
Visit squarespace.com/google to start your free trial.

311
00:17:34.230 --> 00:17:38.580
Use the offer code document for 10% off on your first purchase.

312
00:17:38.640 --> 00:17:42.450
Google and Squarespace. Make it professional. Make it beautiful.

313
00:17:45.330 --> 00:17:48.780
<v 0>This is Radiolab and uh, we'll pick up the story with Andrews.</v>

314
00:17:49.060 --> 00:17:51.480
Ali and I sitting in a meeting at Facebook headquarters.

315
00:17:51.481 --> 00:17:52.920
This was about a year and a half ago.

316
00:17:52.921 --> 00:17:56.370
We had just learned that at any given moment,

317
00:17:56.371 --> 00:18:01.371
any given Facebook user is part of 10 experiments at once without really their

318
00:18:01.861 --> 00:18:05.220
knowledge and sitting there in that meeting, you know, this was a while ago,

319
00:18:05.310 --> 00:18:08.910
we both were like, did we just hear that correctly?

320
00:18:10.320 --> 00:18:14.340
<v 9>That kind of blew me back a little bit and I was like, I,</v>

321
00:18:14.590 --> 00:18:19.590
I've been a research subject and I had no idea and I had that moment of

322
00:18:21.601 --> 00:18:26.160
discovery on a Friday and literally the next day,

323
00:18:26.190 --> 00:18:27.023
Saturday,

324
00:18:27.260 --> 00:18:31.010
<v 10>this is scary. The world had that experience.</v>

325
00:18:31.430 --> 00:18:36.430
Facebook using you and me as lab rats for a Facebook experiment on emotion.

326
00:18:37.760 --> 00:18:41.360
Barely a day after we'd gotten off the plane from Facebook headquarters,

327
00:18:41.720 --> 00:18:46.240
the kerfuffle occurred, Facebook exposed for using us as lab rats,

328
00:18:46.280 --> 00:18:50.600
as lab rat lab rats probably say Facebook messing with your emotion.

329
00:18:50.960 --> 00:18:53.540
You may remember the story cause for a hot second. It was everywhere.

330
00:18:53.541 --> 00:18:56.150
Facebook altered the amount of you said that it was all over.

331
00:18:56.151 --> 00:19:00.050
Facebook story was an academic paper had come out that showed that with some

332
00:19:00.051 --> 00:19:00.650
scientists,

333
00:19:00.650 --> 00:19:05.630
the company had intentionally manipulated user newsfeeds to study a person's

334
00:19:05.631 --> 00:19:07.160
emotional response. Seriously.

335
00:19:07.161 --> 00:19:10.160
They wanted to see how emotion is spread on social media.

336
00:19:10.190 --> 00:19:13.850
They basically tinkered with the news feeds about 700,000 people,

337
00:19:13.880 --> 00:19:18.470
700,000 users to test how they react if they saw more positive versus negative

338
00:19:18.471 --> 00:19:19.820
posts and vice versa,

339
00:19:19.940 --> 00:19:23.360
and they found an effect that when people saw more positive stuff in their news

340
00:19:23.780 --> 00:19:28.550
feeds, they would post more positive things themselves and vice versa was a tiny

341
00:19:28.551 --> 00:19:32.030
effect, tiny effect, but the results weren't really the story.

342
00:19:32.210 --> 00:19:36.470
The real story was that Facebook was messing with us. Yeah,

343
00:19:36.471 --> 00:19:39.960
you pause and scares me when you think that they were just doing an experiment

344
00:19:39.990 --> 00:19:43.320
to manipulate how people were feeling and how they then reacted on Facebook if

345
00:19:43.321 --> 00:19:45.170
people went apoplectic.

346
00:19:45.330 --> 00:19:47.700
[inaudible] has this big brother element to it that I think people are going to

347
00:19:47.701 --> 00:19:52.701
be very uncomfortable with and some people went so far as to argue,

348
00:19:53.340 --> 00:19:57.300
I wonder if Facebook killed anyone with their emotional manipulation.

349
00:19:57.710 --> 00:20:02.710
Stunned as if a person had a psychological psychiatric disorder manipulating

350
00:20:03.931 --> 00:20:06.870
their social world could cause them real harm.

351
00:20:06.871 --> 00:20:09.810
Make sure you read those terms and conditions. My friends asked what

352
00:20:12.100 --> 00:20:16.090
<v 0>what you hear is a sense of betrayal that I really wasn't aware that this space</v>

353
00:20:16.091 --> 00:20:20.020
of mine was being treated in these ways and that I was part of your

354
00:20:20.230 --> 00:20:22.930
psychological experimentation. That's Kate Crawford.

355
00:20:22.990 --> 00:20:26.680
I'm a principal researcher at Microsoft research visiting professor at MIT,

356
00:20:26.681 --> 00:20:29.110
strong critic of Facebook. Throughout the Kerfuffle,

357
00:20:29.260 --> 00:20:30.880
there is a power imbalance at work.

358
00:20:30.940 --> 00:20:33.550
I think when we look at the way that that experiment was done,

359
00:20:33.580 --> 00:20:38.580
it's an example of highly centralized power and highly opaque power at work.

360
00:20:39.130 --> 00:20:42.220
And I don't want to see us in a situation where we just have to blindly trust

361
00:20:42.221 --> 00:20:44.170
that platforms are looking out for us here.

362
00:20:44.171 --> 00:20:49.171
I'm thinking of one of an earlier Facebook study back in 2010 where they did a

363
00:20:49.631 --> 00:20:52.690
study looking at whether they could increase voter turnout.

364
00:20:53.080 --> 00:20:56.890
They had this quite simple design. They, they came up with, you know,

365
00:20:56.980 --> 00:20:57.890
a little uh,

366
00:20:58.240 --> 00:21:01.330
box that would pop up and show you where your nearest voting booth was.

367
00:21:01.330 --> 00:21:04.120
And then they said, oh, well in addition to that, when you voted,

368
00:21:04.150 --> 00:21:06.400
here's a button you can press that says I voted.

369
00:21:06.580 --> 00:21:09.670
And then you'll also see the pictures of six of your friends who'd also voted

370
00:21:09.671 --> 00:21:13.030
that day. What this change. The number of people who went out to vote the day,

371
00:21:13.600 --> 00:21:14.890
and Facebook found that it did,

372
00:21:14.891 --> 00:21:19.500
that if you saw a bunch of pictures of your friends who had voted and you felt

373
00:21:19.501 --> 00:21:21.010
those pictures on election day,

374
00:21:21.220 --> 00:21:25.150
you were then 2% more likely to click the I voted button yourself.

375
00:21:25.210 --> 00:21:27.700
Presumably because you two had gone out and voted.

376
00:21:28.160 --> 00:21:32.300
Now 2% might not sound like a lot, but it was not insignificant. Again,

377
00:21:32.301 --> 00:21:35.870
I think by the order of 340,000 votes,

378
00:21:36.140 --> 00:21:38.970
whether the votes that they estimate, they actually, uh,

379
00:21:39.080 --> 00:21:42.110
shifted by getting people to go on that. These are people who wouldn't have, it,

380
00:21:42.160 --> 00:21:45.650
wouldn't have voted and who they have said in their own paper and published

381
00:21:45.651 --> 00:21:49.850
paper that they increased the number of votes that day by 340,000 simply by

382
00:21:49.851 --> 00:21:53.720
saying that your neighbors did it too. Yeah. By your friends. No.

383
00:21:53.721 --> 00:21:57.470
My first reaction to this, I must admit, was okay. I mean,

384
00:21:57.650 --> 00:22:00.440
we're at historic lows when it comes to voter turnout.

385
00:22:00.470 --> 00:22:02.270
This sounds like a good thing. Yes,

386
00:22:02.300 --> 00:22:05.480
but what happens if someone's running a platform that a lot of people are on and

387
00:22:05.481 --> 00:22:09.200
they say, Hey, you know, I'm, I'm really interested in this candidate.

388
00:22:09.201 --> 00:22:12.050
This candidate is going to look out not just for my interest,

389
00:22:12.051 --> 00:22:14.150
but the interest of the technology sector and I think they're,

390
00:22:14.180 --> 00:22:18.050
you know that they're a great candidate. Why don't we just show that,

391
00:22:18.051 --> 00:22:23.051
get out to vote message and that that little system design that we have to the

392
00:22:23.211 --> 00:22:26.630
people who clearly because we already have their political preferences,

393
00:22:26.840 --> 00:22:30.020
the ones who kind of agree with us and the people who disagree with that

394
00:22:30.021 --> 00:22:32.300
candidate, they won't get those little nudges.

395
00:22:32.570 --> 00:22:37.070
Now that is a profound democratic palette that you have.

396
00:22:37.890 --> 00:22:40.440
Kate's basic position is that when it comes to social engineering,

397
00:22:40.441 --> 00:22:43.710
which is what this is companies and the people that use them,

398
00:22:43.940 --> 00:22:47.010
you need to be really, really careful. In fact, when,

399
00:22:47.070 --> 00:22:51.480
when Andrew mentioned to her that Arturo had this group and the group had a

400
00:22:51.481 --> 00:22:55.500
name, he actually runs a group called the trust engineering group.

401
00:22:55.620 --> 00:23:00.330
His job is to engineer trust. When Andrew told her that Facebook users,

402
00:23:00.331 --> 00:23:04.020
that's easier. You're smacking your forehead. I think we call that a face palm.

403
00:23:05.070 --> 00:23:06.840
She face palm really hard.

404
00:23:06.990 --> 00:23:10.560
These ideas that we could somehow engineer compassion,

405
00:23:10.830 --> 00:23:13.800
I think to some degree have a kind of Hubris in them.

406
00:23:15.210 --> 00:23:16.043
<v 11>[inaudible]</v>

407
00:23:16.200 --> 00:23:20.580
<v 0>who are we to decide whether we can make somebody more compassionate or not.</v>

408
00:23:23.210 --> 00:23:24.043
<v 11>[inaudible]</v>

409
00:23:26.400 --> 00:23:28.690
<v 0>how do you want to set this up? Let's see. How do we do this?</v>

410
00:23:29.110 --> 00:23:32.920
A couple months after our first interview, we spoke to our Touro Bay Har. Again,

411
00:23:33.730 --> 00:23:38.230
at this point the kerfuffle was dying down. We asked them about all the uproar.

412
00:23:38.320 --> 00:23:41.290
I know this is not your work, this is the emotional contagion stuff.

413
00:23:42.130 --> 00:23:47.130
But literally like hours after we got from that meeting that thing erupted.

414
00:23:47.930 --> 00:23:51.080
Do you understand the backlash? No, I mean I think that,

415
00:23:54.290 --> 00:23:55.250
I mean we, we,

416
00:23:56.860 --> 00:24:00.280
<v 3>we really care about the people who use Facebook.</v>

417
00:24:03.420 --> 00:24:06.130
I don't think that there's such a thing as a as uh,

418
00:24:07.790 --> 00:24:12.420
I mean if anything I've learned in this work is that you really have to respect

419
00:24:12.690 --> 00:24:16.320
people's response and emotions no matter what they are.

420
00:24:16.840 --> 00:24:18.460
<v 0>He says the whole thing. Definitely</v>

421
00:24:19.770 --> 00:24:21.720
<v 3>made them take stock. Um,</v>

422
00:24:21.990 --> 00:24:25.380
there was a moment of concern of what it would mean to the work and there there

423
00:24:25.381 --> 00:24:29.520
was like this is this gonna is this gonna mean that um,

424
00:24:29.670 --> 00:24:31.690
that we can't do this? Hmm.

425
00:24:32.280 --> 00:24:36.210
Part of me like being on this committee here is I actually wanna reclaim back

426
00:24:36.211 --> 00:24:37.800
the word emotion, um,

427
00:24:37.890 --> 00:24:41.640
and reclaimed back the ability to do um,

428
00:24:41.670 --> 00:24:44.910
very thoughtful and careful experiments and when it come back the word

429
00:24:44.911 --> 00:24:45.800
experiment. Okay.

430
00:24:45.940 --> 00:24:48.990
<v 0>Where do you want to reclaim it from? From what? Um,</v>

431
00:24:49.740 --> 00:24:54.090
well suddenly like the word emotion and the word experiment only things became

432
00:24:54.091 --> 00:24:55.410
really charged. Well, yeah,

433
00:24:55.411 --> 00:24:58.350
because people thought that Facebook was manipulating emotion and they were

434
00:24:58.351 --> 00:25:00.860
like, yes. [inaudible] case in our case,

435
00:25:00.940 --> 00:25:03.640
<v 3>right then in, in, in, in the work that we're talking about right now,</v>

436
00:25:04.000 --> 00:25:08.380
all of the work that we do begins with a person asking us for help.

437
00:25:08.710 --> 00:25:13.450
<v 0>This was Arturo's most emphatic point. He said it over and over that, you know,</v>

438
00:25:13.600 --> 00:25:17.470
Facebook isn't just doing this for fun. People are asking for help.

439
00:25:17.500 --> 00:25:18.333
They need help,

440
00:25:20.540 --> 00:25:23.690
which points to one of the biggest challenges of living online, which is that,

441
00:25:23.691 --> 00:25:27.260
you know, offline, you know, when we try and engineer trust offline,

442
00:25:27.500 --> 00:25:29.120
or at least just read one another,

443
00:25:29.121 --> 00:25:33.210
we do it in these super subtle ways using eye contact,

444
00:25:33.950 --> 00:25:38.300
facial expressions, and posture and tone of voice, all this nonverbal stuff.

445
00:25:38.301 --> 00:25:42.490
And of course when we go online, we don't have access to any of that.

446
00:25:42.550 --> 00:25:46.140
In the absence of that feedback, how do we communicate what,

447
00:25:46.150 --> 00:25:48.040
what does communication turn into

448
00:25:49.310 --> 00:25:53.150
<v 3>me too? This isn't just because of the other stupid kid's best riff on this,</v>

449
00:25:53.151 --> 00:25:57.590
I got to say is a Louis C K on Conan for kids. It's just this thing.

450
00:25:57.591 --> 00:26:01.760
It's bad. And he did this great bit about technology and kids, you know,

451
00:26:01.761 --> 00:26:06.590
kids are mean and it's cause they're trying it out. They look at a kid,

452
00:26:06.591 --> 00:26:09.950
they go, you're fat. And then they see the kid's face scrunched up and they go,

453
00:26:09.980 --> 00:26:12.560
Ooh, that doesn't feel good to make a person do that.

454
00:26:13.010 --> 00:26:16.520
But they got to start with doing the mean thing. But when they write your fat,

455
00:26:16.700 --> 00:26:19.450
then they just go, hmm. That was fun. I like that.

456
00:26:21.510 --> 00:26:23.040
Anyhow, back to Arturo.

457
00:26:23.190 --> 00:26:28.190
I mean I think about like what it means to be in the presence of a friend or a

458
00:26:30.391 --> 00:26:31.224
loved one.

459
00:26:32.470 --> 00:26:33.310
<v 0>Um, and,</v>

460
00:26:34.690 --> 00:26:39.690
and how will you build experiences that facilitate that when you cannot be

461
00:26:39.881 --> 00:26:43.320
physically together? Arturo says that's really all up to,

462
00:26:43.321 --> 00:26:48.090
he's just trying to nudge people a tiny bit so that their online cells are a

463
00:26:48.091 --> 00:26:50.880
little bit closer to how they are offline.

464
00:26:50.881 --> 00:26:55.110
And I got to say if he can do that by engineering, a couple of phrases like,

465
00:26:55.111 --> 00:26:58.770
Hey Robert, would you mind? Et Cetera, et cetera. Well then I'm all for it.

466
00:26:59.000 --> 00:27:04.000
<v 8>Why not take the position that to create a company that stands between two</v>

467
00:27:04.401 --> 00:27:05.420
people who

468
00:27:06.980 --> 00:27:11.980
who are interacting and then giving them boxes and statuses and,

469
00:27:12.240 --> 00:27:14.470
and, and little, um,

470
00:27:15.100 --> 00:27:19.910
and advertising and stuff like this is not doing a service,

471
00:27:19.911 --> 00:27:21.320
this is just, this is,

472
00:27:21.770 --> 00:27:26.450
this is a way to wedge yourself into the ordinary business of social intercourse

473
00:27:26.660 --> 00:27:27.830
and make money on it.

474
00:27:28.180 --> 00:27:31.400
And you're acting like this group of people now is going to try to,

475
00:27:31.760 --> 00:27:36.470
to create the, the moral equivalent of an actual conversation. First of all,

476
00:27:36.471 --> 00:27:40.640
it's probably not engineerable and of all, I don't believe that for a moment.

477
00:27:41.180 --> 00:27:44.660
All I'm thinking is they're going to just go and figure out other ways in which

478
00:27:44.661 --> 00:27:46.160
to make a revenue enhancer.

479
00:27:46.420 --> 00:27:49.330
<v 0>No, I don't think it's one or the other. I think they're in it for the money.</v>

480
00:27:49.390 --> 00:27:54.100
In fact, if they can figure this out and make the Internet universe,

481
00:27:55.060 --> 00:27:57.790
uh, more inducive to trust and less annoying,

482
00:27:58.030 --> 00:28:01.930
it could mean trillions of dollars. So yeah, it's the money. But still,

483
00:28:01.931 --> 00:28:05.200
that doesn't negate the fact that we have to build these systems, right,

484
00:28:05.260 --> 00:28:07.060
that we have to make the Internet a little bit better.

485
00:28:07.400 --> 00:28:12.080
<v 8>Does, that's fine. This idea, however, that, um,</v>

486
00:28:12.350 --> 00:28:17.270
you're going to have to coach people into the subtleties of the relationship.

487
00:28:18.200 --> 00:28:22.220
Tell him you're sorry. Tone. Just, you know, here's the formula for this.

488
00:28:22.221 --> 00:28:26.720
Here's you. He doesn't want, he did something. You, you need to repair that.

489
00:28:26.721 --> 00:28:30.930
Here are the seven ways you might repair that to do all that. It's,

490
00:28:31.000 --> 00:28:36.000
it's as if the hallmark card company instead of living only on mother's Day,

491
00:28:36.560 --> 00:28:41.560
Father's day and birthdays just spread its evil wings out into the whole rest of

492
00:28:43.251 --> 00:28:44.084
your life.

493
00:28:44.240 --> 00:28:47.810
<v 0>I don't know. I think that's a wonderful thing. I think, you know,</v>

494
00:28:47.811 --> 00:28:49.760
I have a slightly different opinion of it. I mean,

495
00:28:49.890 --> 00:28:51.950
you've got to keep in mind how this thing came about. I mean,

496
00:28:51.951 --> 00:28:53.690
they tried to get people to talk to each other.

497
00:28:53.691 --> 00:28:57.560
They gave them the blank text box, but nobody used it. Right. So they're like,

498
00:28:57.561 --> 00:29:01.220
okay, let's come up with some stock phrases that yes, are generic,

499
00:29:01.400 --> 00:29:05.660
but think about the next step after you send the message saying, you know,

500
00:29:05.661 --> 00:29:07.760
Jad p I don't like the photo. Police take it down.

501
00:29:07.820 --> 00:29:11.510
Presumably then you and I get into a conversation. Maybe I explained myself.

502
00:29:11.511 --> 00:29:13.340
I say, Oh my God, I'm so sorry.

503
00:29:13.341 --> 00:29:14.990
I didn't realize that you didn't like that photo.

504
00:29:14.991 --> 00:29:17.450
I just thought that that was an amazing night.

505
00:29:17.480 --> 00:29:18.740
I just thought that was a great night.

506
00:29:18.800 --> 00:29:23.370
I didn't realize you thought you look so sorry. I'll take it down. It's cool.

507
00:29:23.620 --> 00:29:26.990
See now presumably we're having that conversation as a next step.

508
00:29:27.820 --> 00:29:31.690
<v 8>I presume that how many of the birthday cards that you've sent to first cousins</v>

509
00:29:31.990 --> 00:29:33.490
have resulted in a conversation,

510
00:29:34.180 --> 00:29:36.960
maybe [inaudible] these things are actually not,

511
00:29:37.230 --> 00:29:40.480
they're really the opposite of what you're saying. Are Conversation substitutes.

512
00:29:40.870 --> 00:29:42.010
<v 0>Maybe, maybe</v>

513
00:29:42.160 --> 00:29:45.970
<v 3>they're conversation starters. Maybe that's the deep experiment.</v>

514
00:29:46.450 --> 00:29:48.910
Are they conversation starters or were substitutes we'll I hope they're

515
00:29:48.911 --> 00:29:52.390
conversation starters. Yeah, cause maybe that would be a beginning.

516
00:29:52.480 --> 00:29:57.480
It kind of in my mind goes back to like the beginning of the automobile age

517
00:30:00.520 --> 00:30:01.353
[inaudible] Andrew puts it,

518
00:30:01.390 --> 00:30:05.800
there was a time when automobiles were new and you know,

519
00:30:05.801 --> 00:30:09.490
they didn't have turn signals. The tools they did have,

520
00:30:09.491 --> 00:30:14.080
like the horn didn't necessarily indicate all the things that we use.

521
00:30:14.081 --> 00:30:17.350
It indicated it wasn't clear what the Horn was actually there to do.

522
00:30:17.351 --> 00:30:20.530
Is it there to say hello or is it there to say get out of the way.

523
00:30:21.940 --> 00:30:25.540
And over time we created norms.

524
00:30:26.140 --> 00:30:27.940
We created roads with lanes.

525
00:30:28.000 --> 00:30:33.000
We created turn signals that are primarily there for other people so that we can

526
00:30:33.851 --> 00:30:37.450
coexist in this great flow without crashing into each other.

527
00:30:38.080 --> 00:30:40.720
And we still have road rage and we still have road rage.

528
00:30:40.750 --> 00:30:43.000
We still have places where those tools are incomplete.

