WEBVTT
Kind: captions
Language: en

00:00:00.950 --> 00:00:06.650
[MUSIC PLAYING]

00:00:06.650 --> 00:00:07.610
[VIDEO PLAYBACK]

00:00:07.610 --> 00:00:09.480
- I'll tell people,
I work at Google.

00:00:09.480 --> 00:00:10.360
And they go, what
do you work on?

00:00:10.360 --> 00:00:11.120
I design Search.

00:00:11.120 --> 00:00:12.670
And they kind of
pause for a second

00:00:12.670 --> 00:00:15.058
and they're like, what
is there to design?

00:00:15.058 --> 00:00:20.426
[MUSIC PLAYING]

00:01:11.182 --> 00:01:12.179
- Oh, I know.

00:01:12.179 --> 00:01:12.679
It's cat.

00:01:26.651 --> 00:01:27.648
[BEEP]

00:01:27.648 --> 00:01:28.148
- Hi.

00:01:28.148 --> 00:01:29.645
How can I help?

00:01:29.645 --> 00:01:43.118
[MUSIC PLAYING]

00:02:39.006 --> 00:02:40.640
[END PLAYBACK]

00:02:40.640 --> 00:02:43.185
BRENDA FOGG: Hello, everybody.

00:02:43.185 --> 00:02:44.590
I'm Brenda Fogg.

00:02:44.590 --> 00:02:46.480
I work in the
Creative Lab, which

00:02:46.480 --> 00:02:50.920
is sort of a multi-disciplinary
creative group within Google

00:02:50.920 --> 00:02:53.230
that often collaborates
with other teams

00:02:53.230 --> 00:02:59.980
across the company on product
or technology experiments.

00:02:59.980 --> 00:03:03.580
And sometimes, finding new
ways to talk about some

00:03:03.580 --> 00:03:06.190
of the innovative work that's
going on inside of Google.

00:03:06.190 --> 00:03:08.620
And at some point
over the years,

00:03:08.620 --> 00:03:11.080
I've worked with
each of our panelists

00:03:11.080 --> 00:03:14.350
on some of those projects that
you probably saw on the video

00:03:14.350 --> 00:03:15.910
just now.

00:03:15.910 --> 00:03:18.130
So we're going to talk a
little bit about some things

00:03:18.130 --> 00:03:20.050
that you may have seen.

00:03:20.050 --> 00:03:21.910
A little bit about
design at Google

00:03:21.910 --> 00:03:28.480
and what that means as sort
of a fundamental framework

00:03:28.480 --> 00:03:31.960
and connective tissue between
the things that Google makes

00:03:31.960 --> 00:03:36.340
and making them as useful and
accessible to as many people as

00:03:36.340 --> 00:03:38.090
possible.

00:03:38.090 --> 00:03:41.740
So let's start with
some introductions.

00:03:41.740 --> 00:03:44.800
We have Doug Eck
right here to my left,

00:03:44.800 --> 00:03:46.390
who leads a project
called Magenta,

00:03:46.390 --> 00:03:48.580
which is all about
exploring machine learning

00:03:48.580 --> 00:03:50.180
and creativity.

00:03:50.180 --> 00:03:52.930
We have Isabelle Olsson, who's
responsible for the design

00:03:52.930 --> 00:03:56.290
of the Google Home and
wearables and hardware.

00:03:56.290 --> 00:03:59.860
And over there is Ryan
Germick, Principal Designer,

00:03:59.860 --> 00:04:03.430
who also is known
as the Doodle guy.

00:04:03.430 --> 00:04:06.310
And has been quite involved
in the Google Assistant.

00:04:06.310 --> 00:04:08.860
And generally, just
in the business

00:04:08.860 --> 00:04:11.800
of delighting users everywhere.

00:04:11.800 --> 00:04:13.690
So let's start by
letting everybody

00:04:13.690 --> 00:04:16.962
talk a little bit about
what you do at Google.

00:04:16.962 --> 00:04:18.670
And we'll start with
Doug, because you're

00:04:18.670 --> 00:04:19.461
sitting next to me.

00:04:19.461 --> 00:04:22.450
You head up this
project called Magenta.

00:04:22.450 --> 00:04:25.240
For anybody who doesn't
know exactly what that is,

00:04:25.240 --> 00:04:27.880
maybe you can talk a
little bit about that.

00:04:27.880 --> 00:04:33.340
And maybe touch on
what inspired the group

00:04:33.340 --> 00:04:35.670
and the focus of the team.

00:04:35.670 --> 00:04:37.060
DOUGLAS ECK: OK, sure.

00:04:37.060 --> 00:04:40.060
So yeah, I lead a
project called Magenta.

00:04:40.060 --> 00:04:43.120
The M-A-G in Magenta stands
for Music and Art Generation.

00:04:43.120 --> 00:04:46.330
And we started by trying
to understand the capacity

00:04:46.330 --> 00:04:49.090
to use AI, specifically
machine learning,

00:04:49.090 --> 00:04:50.920
to generate music and art.

00:04:50.920 --> 00:04:54.910
And it took us about a
month to realize that that's

00:04:54.910 --> 00:04:56.549
asking the wrong question.

00:04:56.549 --> 00:04:58.090
Because if all you're
doing is trying

00:04:58.090 --> 00:04:59.380
to generate music
and art, then you

00:04:59.380 --> 00:05:01.630
just keep pushing this button
and the machine learning

00:05:01.630 --> 00:05:03.220
keeps making music
and art for you.

00:05:03.220 --> 00:05:05.497
And it gets boring fast.

00:05:05.497 --> 00:05:07.330
So we pivoted very
quickly to talking about,

00:05:07.330 --> 00:05:10.990
how can we use machine learning
to enable artists and enable

00:05:10.990 --> 00:05:12.644
musicians to make something new?

00:05:12.644 --> 00:05:13.810
To make something different?

00:05:13.810 --> 00:05:17.500
So it's sitting on the same
idea of technology and art

00:05:17.500 --> 00:05:20.740
interacting with one
another in a virtuous way,

00:05:20.740 --> 00:05:22.870
starting with cave
drawings and moving forward

00:05:22.870 --> 00:05:26.560
through the film camera and
other bits of technology.

00:05:26.560 --> 00:05:29.470
So yeah, I'm about
AI and art and music.

00:05:29.470 --> 00:05:31.810
BRENDA FOGG: So
you're not necessarily

00:05:31.810 --> 00:05:36.490
trying to replace creativity
or duplicate creativity,

00:05:36.490 --> 00:05:39.940
but more providing the tools
to enable people to do that?

00:05:39.940 --> 00:05:40.690
DOUGLAS ECK: Yeah.

00:05:40.690 --> 00:05:42.820
And I think it's not
just because that's

00:05:42.820 --> 00:05:43.960
what we choose to focus on.

00:05:43.960 --> 00:05:46.600
I think creativity is
fundamentally human.

00:05:46.600 --> 00:05:48.380
And it's about communication.

00:05:48.380 --> 00:05:50.260
So if we take the
communication loop out,

00:05:50.260 --> 00:05:55.360
we can imagine in some
analytical way, a computer

00:05:55.360 --> 00:05:56.710
generating new things.

00:05:56.710 --> 00:05:58.960
But what makes creativity
work is how we respond to it

00:05:58.960 --> 00:06:01.310
and how we then feedback
in to that process.

00:06:01.310 --> 00:06:06.927
So I think it's very much a
societal communicative act.

00:06:06.927 --> 00:06:09.010
BRENDA FOGG: And that idea
of creating new things,

00:06:09.010 --> 00:06:11.830
like maybe some
things that weren't

00:06:11.830 --> 00:06:13.480
necessarily possible before.

00:06:13.480 --> 00:06:15.690
That weren't humanly
possible to create.

00:06:15.690 --> 00:06:19.430
So there's an-- I don't know
if you want to talk about this,

00:06:19.430 --> 00:06:23.650
but this example that was shown
in the lead-up into the keynote

00:06:23.650 --> 00:06:25.000
yesterday.

00:06:25.000 --> 00:06:29.140
The NSynth Project, which
is one of those things

00:06:29.140 --> 00:06:34.720
that can sort of augment
what human creativity can do.

00:06:34.720 --> 00:06:35.980
You want to touch on that?

00:06:35.980 --> 00:06:38.110
DOUGLAS ECK: Yeah, sure.

00:06:38.110 --> 00:06:40.610
For NSynth and the
following hardware device,

00:06:40.610 --> 00:06:42.550
NSynth Super, you
may have seen was

00:06:42.550 --> 00:06:45.347
played on stage before the
keynote and discussed there.

00:06:45.347 --> 00:06:46.930
I think the main
idea there is, can we

00:06:46.930 --> 00:06:50.380
use machine learning to allow
us to generate new sounds?

00:06:50.380 --> 00:06:53.480
And sounds that are
musically meaningful to us?

00:06:53.480 --> 00:06:55.810
And one thing to point
out is that we already

00:06:55.810 --> 00:06:56.860
have ways to do that.

00:06:56.860 --> 00:06:59.014
There's a bunch
of great software.

00:06:59.014 --> 00:07:01.180
I have a piano at my house,
so I can-- and a guitar.

00:07:01.180 --> 00:07:02.680
There are lots of
ways to make sounds.

00:07:02.680 --> 00:07:04.210
What we hope we can
get is some kind

00:07:04.210 --> 00:07:06.870
of expressive edge with AI.

00:07:06.870 --> 00:07:09.160
Something that we can
do with these models.

00:07:09.160 --> 00:07:12.160
A kind of intuitiveness
or a kind of new kind

00:07:12.160 --> 00:07:15.250
of mobility artistically
by having a new tool.

00:07:15.250 --> 00:07:16.960
And the one thing I
would say, I don't

00:07:16.960 --> 00:07:18.850
want to take up too much
time because there's

00:07:18.850 --> 00:07:20.600
a lot of other great
people here on stage.

00:07:20.600 --> 00:07:23.050
But I really like to think
about the film camera.

00:07:23.050 --> 00:07:25.180
The film camera
was initially not

00:07:25.180 --> 00:07:26.740
treated as an artistic device.

00:07:26.740 --> 00:07:29.290
It was treated as something
to capture reality.

00:07:29.290 --> 00:07:31.960
And it was transformed
into an artistic device

00:07:31.960 --> 00:07:33.544
by artists, by photographers.

00:07:33.544 --> 00:07:34.960
And our hope on
Magenta is that we

00:07:34.960 --> 00:07:36.910
find the right artists
and the right musicians

00:07:36.910 --> 00:07:38.590
to take what we're
doing and turn it

00:07:38.590 --> 00:07:40.255
into something creative.

00:07:40.255 --> 00:07:41.630
BRENDA FOGG: So
turning something

00:07:41.630 --> 00:07:43.610
into creative, Isabelle.

00:07:43.610 --> 00:07:47.934
When you're designing a
product, the Home for example,

00:07:47.934 --> 00:07:49.850
you're trying to create
something that appeals

00:07:49.850 --> 00:07:54.620
to everyone through its design.

00:07:54.620 --> 00:07:56.150
But everybody's
different, right?

00:07:56.150 --> 00:08:01.051
So these are physical products
that share a physical space

00:08:01.051 --> 00:08:02.300
with the people that use them.

00:08:02.300 --> 00:08:05.390
And like sometimes,
you have to cohabitate.

00:08:05.390 --> 00:08:08.314
So talk a little bit about
how you approach that problem.

00:08:08.314 --> 00:08:09.230
ISABELLE OLSSON: Yeah.

00:08:09.230 --> 00:08:11.810
I mean, I think I have
the utmost respect

00:08:11.810 --> 00:08:13.490
for people's homes.

00:08:13.490 --> 00:08:15.710
And like you said,
they're all different.

00:08:15.710 --> 00:08:17.930
And I think next to
your body, your home

00:08:17.930 --> 00:08:19.910
is your most intimate space.

00:08:19.910 --> 00:08:22.220
And it's the place for
you share with your loved

00:08:22.220 --> 00:08:23.880
ones and your family.

00:08:23.880 --> 00:08:27.410
So to enter that space
with our products,

00:08:27.410 --> 00:08:32.090
we have to be super-thoughtful
about what we do there.

00:08:32.090 --> 00:08:35.809
And I think for us, the
most important thing

00:08:35.809 --> 00:08:41.360
is to be inspired by the context
in which our products live in.

00:08:41.360 --> 00:08:44.850
So when we were designing
Google Home Mini for example,

00:08:44.850 --> 00:08:48.380
the goal was to design an
Assistant for every room.

00:08:48.380 --> 00:08:50.750
And that means
your bedside table.

00:08:50.750 --> 00:08:53.450
And your bedside table,
that's where you put devices

00:08:53.450 --> 00:08:55.130
that help you see better.

00:08:55.130 --> 00:08:58.520
Or like, a book that
helps you dream.

00:08:58.520 --> 00:09:02.840
So that space is
just so special.

00:09:02.840 --> 00:09:07.370
We wanted to create
something that was beautiful,

00:09:07.370 --> 00:09:10.640
that fit into the home,
and didn't take up

00:09:10.640 --> 00:09:13.760
too much attention, and
kind of faded a little bit

00:09:13.760 --> 00:09:15.560
into the background.

00:09:15.560 --> 00:09:18.680
BRENDA FOGG: And you're
also responsible for CMF

00:09:18.680 --> 00:09:22.550
at Google, which is Color
Material Finish, right?

00:09:22.550 --> 00:09:23.780
ISABELLE OLSSON: Yes.

00:09:23.780 --> 00:09:28.610
BRENDA FOGG: And I've heard
this story about testing,

00:09:28.610 --> 00:09:33.950
like 150 different versions of
color palettes for the Mini.

00:09:33.950 --> 00:09:35.344
Is that right?

00:09:35.344 --> 00:09:36.260
ISABELLE OLSSON: Yeah.

00:09:36.260 --> 00:09:39.980
I mean, I think for us,
developing the color palette

00:09:39.980 --> 00:09:42.380
for the Google
family of products

00:09:42.380 --> 00:09:44.900
and individual
products is a very--

00:09:44.900 --> 00:09:47.720
it's a combination of art
and science, I would say.

00:09:47.720 --> 00:09:51.080
And we start usually
two to three years

00:09:51.080 --> 00:09:52.740
before the products come out.

00:09:52.740 --> 00:09:55.040
So we have to do a
lot of anticipation

00:09:55.040 --> 00:09:58.460
of where society is going,
where trends are going.

00:09:58.460 --> 00:10:00.440
And take all of
those kind of inputs

00:10:00.440 --> 00:10:03.920
into account to make sure that
when we release a product,

00:10:03.920 --> 00:10:06.470
it makes sense to people.

00:10:06.470 --> 00:10:10.700
In addition to that, of course
when you design for the home,

00:10:10.700 --> 00:10:13.580
you have to think about the fact
that there is going to be light

00:10:13.580 --> 00:10:14.610
hitting the product.

00:10:14.610 --> 00:10:17.700
How does it stand
the test of time?

00:10:17.700 --> 00:10:19.730
We want to make sure the
products look beautiful

00:10:19.730 --> 00:10:22.680
for a long time.

00:10:22.680 --> 00:10:26.120
So we have to go through a lot
of iteration to get it right.

00:10:26.120 --> 00:10:29.300
And then also,
especially as we're

00:10:29.300 --> 00:10:33.110
developing fabrics for example,
depending on where you put it,

00:10:33.110 --> 00:10:35.544
it takes different--

00:10:35.544 --> 00:10:37.710
it looks different in
different lighting conditions.

00:10:37.710 --> 00:10:42.530
So when we designed
Mini, we went through,

00:10:42.530 --> 00:10:48.545
I think 150 iterations
of just the gray color.

00:10:48.545 --> 00:10:49.730
But it was a lot of fun.

00:10:49.730 --> 00:10:53.360
And it was about finding
that right balance with,

00:10:53.360 --> 00:10:54.140
what is too light?

00:10:54.140 --> 00:10:55.950
What's too dark?

00:10:55.950 --> 00:10:58.910
And the other day, I got
this lovely email by someone

00:10:58.910 --> 00:11:03.350
on the team who had picked out
his couch to match Google Home

00:11:03.350 --> 00:11:04.820
Max.

00:11:04.820 --> 00:11:06.650
So I took that as
a giant compliment

00:11:06.650 --> 00:11:09.330
because we were trying to
do it the other way around.

00:11:09.330 --> 00:11:11.270
But that was a beautiful story.

00:11:13.940 --> 00:11:16.730
BRENDA FOGG: What is the
intersection of the intuition

00:11:16.730 --> 00:11:19.010
that you use as a
designer when you approach

00:11:19.010 --> 00:11:23.390
these kinds of problems with
the sort of iterative testing?

00:11:23.390 --> 00:11:27.440
And sort of the scientific
materials examination?

00:11:27.440 --> 00:11:29.810
ISABELLE OLSSON: Yeah,
it's a hodgepodge.

00:11:29.810 --> 00:11:31.670
The process is not linear.

00:11:31.670 --> 00:11:34.070
It's pretty messy, usually.

00:11:34.070 --> 00:11:35.390
But we have fun with it.

00:11:35.390 --> 00:11:39.860
I think the key is gather
as much input as possible,

00:11:39.860 --> 00:11:41.270
and then digest it.

00:11:41.270 --> 00:11:44.840
And then, come up with
prototypes and ways

00:11:44.840 --> 00:11:48.500
of relating to how this will
fit into people's homes.

00:11:48.500 --> 00:11:52.880
So even right next to my
desk, I have a big bookshelf

00:11:52.880 --> 00:11:54.770
that we place random
objects from all

00:11:54.770 --> 00:11:56.670
over the world for inspiration.

00:11:56.670 --> 00:11:59.780
But also, to kind of put
our stuff there quickly

00:11:59.780 --> 00:12:01.370
to see, how does it feel?

00:12:01.370 --> 00:12:03.800
And how does it feel over time?

00:12:03.800 --> 00:12:05.750
Because it's not only
about creating something

00:12:05.750 --> 00:12:07.850
that you are first
attracted to, but it

00:12:07.850 --> 00:12:13.820
has to be things that you can
live with for a long time.

00:12:13.820 --> 00:12:18.470
BRENDA FOGG: So Ryan, you
lead the Google Doodles team.

00:12:18.470 --> 00:12:21.590
And this team is unique
in a lot of ways.

00:12:21.590 --> 00:12:27.890
Namely, one of them is that you
regularly and willfully break

00:12:27.890 --> 00:12:29.430
the brand rules.

00:12:29.430 --> 00:12:30.680
RYAN GERMICK: Gleefully, yeah.

00:12:30.680 --> 00:12:33.200
BRENDA FOGG:
Gleefully for Google.

00:12:33.200 --> 00:12:36.650
In fact, on a daily
basis, many, many times.

00:12:36.650 --> 00:12:42.490
And that's unusual because
it's the core of the brand.

00:12:42.490 --> 00:12:44.985
And that's something that seems
to keep working and working

00:12:44.985 --> 00:12:46.110
and working over the years.

00:12:46.110 --> 00:12:48.880
So talk a little
bit about why you

00:12:48.880 --> 00:12:50.950
think it's important to
have the ability to kind

00:12:50.950 --> 00:12:53.489
of just mess with it.

00:12:53.489 --> 00:12:54.280
RYAN GERMICK: Sure.

00:12:54.280 --> 00:12:57.460
I mean, Google's mission
is to organize the world's

00:12:57.460 --> 00:12:58.990
information and
make it universally

00:12:58.990 --> 00:13:00.830
accessible and useful.

00:13:00.830 --> 00:13:02.080
And I believe in that mission.

00:13:02.080 --> 00:13:04.970
I think it's a very powerful,
good thing to do for the world.

00:13:04.970 --> 00:13:06.850
And we hone in on
the idea of making

00:13:06.850 --> 00:13:10.420
things accessible by creating
an emotional connection

00:13:10.420 --> 00:13:11.470
with users.

00:13:11.470 --> 00:13:14.500
And sometimes, like
mucking up the standards.

00:13:14.500 --> 00:13:16.600
That's like collateral
damage for people

00:13:16.600 --> 00:13:19.000
getting a positive charge
and learning something new

00:13:19.000 --> 00:13:22.330
or having fun with something,
then we think it's worthwhile.

00:13:22.330 --> 00:13:23.920
And yeah, I think
on a human level,

00:13:23.920 --> 00:13:29.680
there are things that are more
important than consistency.

00:13:29.680 --> 00:13:33.910
It's, for us, more about
using our creativity and craft

00:13:33.910 --> 00:13:36.800
to make people feel welcome
in the space of technology.

00:13:36.800 --> 00:13:38.050
BRENDA FOGG: Yeah.

00:13:38.050 --> 00:13:40.610
So making people feel welcome
in the space of technology.

00:13:40.610 --> 00:13:44.710
You also lead the team who
created the personality

00:13:44.710 --> 00:13:46.120
for the Google Assistant.

00:13:46.120 --> 00:13:46.953
RYAN GERMICK: Right.

00:13:46.953 --> 00:13:50.590
BRENDA FOGG: And how do
you create a personality?

00:13:50.590 --> 00:13:54.280
I mean, there's sort of the
transactional things that

00:13:54.280 --> 00:13:56.770
have to happen between a
user when they're interacting

00:13:56.770 --> 00:13:58.632
with a digital assistant.

00:13:58.632 --> 00:14:00.340
And they have an
expectation that they're

00:14:00.340 --> 00:14:02.298
going to be delivered
the information that they

00:14:02.298 --> 00:14:03.370
asked for.

00:14:03.370 --> 00:14:05.620
And you felt like it needed
to go a little bit farther

00:14:05.620 --> 00:14:08.230
than that sort of
transactional relationship.

00:14:08.230 --> 00:14:10.360
But people have-- a
little bit the way

00:14:10.360 --> 00:14:13.360
we were talking with
Isabella, everyone

00:14:13.360 --> 00:14:17.350
has different things that
they like to interact with.

00:14:17.350 --> 00:14:19.800
And some people like small
talk and some people don't.

00:14:19.800 --> 00:14:22.810
And some people think things are
funny that other people think

00:14:22.810 --> 00:14:26.080
are totally not funny at all.

00:14:26.080 --> 00:14:27.280
Talk a little about that.

00:14:27.280 --> 00:14:28.071
RYAN GERMICK: Yeah.

00:14:28.071 --> 00:14:30.460
I mean, I think that
as Isabella mentioned,

00:14:30.460 --> 00:14:32.290
that technology, like
the assistant that

00:14:32.290 --> 00:14:34.200
comes in a smart speaker,
or a smart display,

00:14:34.200 --> 00:14:37.042
or in your phone,
is really personal.

00:14:37.042 --> 00:14:37.750
That's one thing.

00:14:37.750 --> 00:14:40.166
And so we recognize that we
have a different set of design

00:14:40.166 --> 00:14:43.900
challenges than if it was more
objective, like a Google Search

00:14:43.900 --> 00:14:45.910
engine [INAUDIBLE].

00:14:45.910 --> 00:14:49.510
And then also, when you
invite this technology

00:14:49.510 --> 00:14:50.770
into your life--

00:14:50.770 --> 00:14:53.455
we're using this conversational
interface as a metaphor.

00:14:53.455 --> 00:14:55.330
You can talk to it and
it can respond to you.

00:14:55.330 --> 00:14:57.490
And as soon as you
hear the human voice,

00:14:57.490 --> 00:14:59.140
it not only opens
up an opportunity

00:14:59.140 --> 00:15:01.360
to have a character,
but what we've seen,

00:15:01.360 --> 00:15:04.030
it's almost like an obligation
to design for the character.

00:15:04.030 --> 00:15:05.488
Because if you
don't design for it,

00:15:05.488 --> 00:15:08.890
then people will just assume you
don't have much of a character.

00:15:08.890 --> 00:15:10.810
But there's still some
implicit character.

00:15:10.810 --> 00:15:12.893
So we took the learnings
that we had from Doodles,

00:15:12.893 --> 00:15:17.380
and being an implicit
character for Google

00:15:17.380 --> 00:15:18.790
where we celebrate
certain things

00:15:18.790 --> 00:15:21.492
and we get creative
and nerdy and excited.

00:15:21.492 --> 00:15:23.950
And we tried to transfer that
over to the Google Assistant,

00:15:23.950 --> 00:15:25.366
where it could be
like a character

00:15:25.366 --> 00:15:28.580
that you'd want to spend time
with because it has things

00:15:28.580 --> 00:15:29.890
that it gets excited about.

00:15:29.890 --> 00:15:33.160
Or it has a perspective where
it really wants to help you.

00:15:33.160 --> 00:15:36.350
And not just be something
that you want to use,

00:15:36.350 --> 00:15:39.250
but something that you
want to spend time with.

00:15:39.250 --> 00:15:41.310
So yeah, surprising
number of the principles

00:15:41.310 --> 00:15:42.220
are things that
we did for Doodles

00:15:42.220 --> 00:15:44.190
were applicable for
the Google Assistant.

00:15:44.190 --> 00:15:45.370
And it's a huge project.

00:15:45.370 --> 00:15:47.740
And there's a lot of
pieces to the puzzle,

00:15:47.740 --> 00:15:50.560
but we think it's an important
part of the user experience

00:15:50.560 --> 00:15:53.100
to have a sense of
who the character is.

00:15:53.100 --> 00:15:54.670
BRENDA FOGG: Yeah.

00:15:54.670 --> 00:15:57.400
So each of you have
talked a little bit

00:15:57.400 --> 00:16:02.410
about how technology interacts
with humans and vice-versa.

00:16:02.410 --> 00:16:05.290
And how those two things
have to kind of co-exist.

00:16:05.290 --> 00:16:07.930
So good design and
thoughtful design

00:16:07.930 --> 00:16:13.930
is a means to make
technology, in this case,

00:16:13.930 --> 00:16:17.210
more approachable and
useful and usable.

00:16:17.210 --> 00:16:17.770
And friendly.

00:16:17.770 --> 00:16:21.430
And to make people
comfortable with that.

00:16:21.430 --> 00:16:25.150
And you all approach your
work and problem-solving

00:16:25.150 --> 00:16:28.390
in this way from a very
human perspective, right?

00:16:28.390 --> 00:16:31.400
A very like-- you
inject empathy.

00:16:31.400 --> 00:16:33.900
We're going to get real and
talk about humanity and empathy,

00:16:33.900 --> 00:16:35.560
right?

00:16:35.560 --> 00:16:38.030
Injecting this empathy
into your process.

00:16:38.030 --> 00:16:41.510
So let's talk about that.

00:16:41.510 --> 00:16:45.030
Doug, do you think--

00:16:45.030 --> 00:16:47.920
can the work you do with
machine learning allow a machine

00:16:47.920 --> 00:16:52.090
to express art in a human way?

00:16:55.600 --> 00:16:56.639
Let's start there.

00:16:56.639 --> 00:16:57.680
DOUGLAS ECK: Start there.

00:16:57.680 --> 00:17:01.170
Yes, with some constraints
on how this all works.

00:17:01.170 --> 00:17:04.920
I think what we
realized early was

00:17:04.920 --> 00:17:09.460
that we need at least
two players in this game,

00:17:09.460 --> 00:17:11.970
so to speak.

00:17:11.970 --> 00:17:14.099
Part of the work is
building new technology.

00:17:14.099 --> 00:17:17.220
So in some sense, we're
taking on the role

00:17:17.220 --> 00:17:19.680
that a luthier might take
on in building a guitar.

00:17:19.680 --> 00:17:22.109
Or that someone doing
a music tech program

00:17:22.109 --> 00:17:25.540
might take on in building a new
kind of electronic instrument.

00:17:25.540 --> 00:17:28.500
And I think there's a
thought process that

00:17:28.500 --> 00:17:30.510
goes with building
something like that

00:17:30.510 --> 00:17:32.430
that is very creative.

00:17:32.430 --> 00:17:36.030
But I think you're also in
some very real way constrained

00:17:36.030 --> 00:17:38.460
by the act of
building the thing.

00:17:38.460 --> 00:17:40.200
To understand it
in a certain way.

00:17:40.200 --> 00:17:40.860
It's your baby.

00:17:40.860 --> 00:17:41.410
You built it.

00:17:41.410 --> 00:17:43.950
And so you know-- you
wrote the operating manual,

00:17:43.950 --> 00:17:46.350
so you know what this
thing is supposed to do.

00:17:46.350 --> 00:17:51.030
And in most cases, what we see
is that for something to become

00:17:51.030 --> 00:17:54.960
a truly expressive,
artistic device,

00:17:54.960 --> 00:17:58.780
it has to in some very real
way be broken by someone else.

00:17:58.780 --> 00:18:00.780
And I think it's almost
impossible for us

00:18:00.780 --> 00:18:03.780
as the builders of
that device to also

00:18:03.780 --> 00:18:05.280
be the ones that break it.

00:18:05.280 --> 00:18:09.990
And so our dream in Magenta
is to connect with artists

00:18:09.990 --> 00:18:12.110
and musicians and people.

00:18:12.110 --> 00:18:13.920
People that don't
know how to code.

00:18:13.920 --> 00:18:15.810
People that don't
necessarily even care

00:18:15.810 --> 00:18:20.560
much about computation and draw
them into this conversation.

00:18:20.560 --> 00:18:25.830
And so what we found is that
we started by releasing machine

00:18:25.830 --> 00:18:30.450
learning models in open source
on GitHub as part of TensorFlow

00:18:30.450 --> 00:18:35.621
with instructions like, please
run this 15-line-long Python

00:18:35.621 --> 00:18:36.120
command.

00:18:36.120 --> 00:18:37.140
It's going to be great.

00:18:37.140 --> 00:18:38.924
Just run this command
and hit Enter.

00:18:38.924 --> 00:18:40.340
And then, just
wait because you're

00:18:40.340 --> 00:18:43.080
going to get 100 MIDI files
in a temp directory somewhere

00:18:43.080 --> 00:18:44.692
on your machine, right?

00:18:44.692 --> 00:18:47.290
Everybody's like, that's
not how I make music.

00:18:47.290 --> 00:18:50.760
So what we've seen is
that part of our work,

00:18:50.760 --> 00:18:55.860
even on the technologist side,
even as luthiers, so to speak--

00:18:55.860 --> 00:18:57.000
guitar makers--

00:18:57.000 --> 00:18:59.100
part of our job is
to do good design.

00:18:59.100 --> 00:19:02.250
And to build interfaces
that people can use.

00:19:02.250 --> 00:19:03.750
And then, hopefully
the interfaces

00:19:03.750 --> 00:19:05.730
are flexible enough
and expressive enough

00:19:05.730 --> 00:19:07.560
that in some very
meaningful way,

00:19:07.560 --> 00:19:09.970
people can also do
some fun breakage.

00:19:09.970 --> 00:19:13.350
And getting there requires
a lot of moving parts.

00:19:13.350 --> 00:19:16.560
A large component of
which is very good design.

00:19:16.560 --> 00:19:18.750
BRENDA FOGG: I like that
notion of breaking things.

00:19:18.750 --> 00:19:21.240
You told a story once about--

00:19:21.240 --> 00:19:24.270
or you made an analogy once
about the electric guitar,

00:19:24.270 --> 00:19:27.150
I think, and how that's
a little bit similar.

00:19:27.150 --> 00:19:29.400
Like the dissonance
that people create

00:19:29.400 --> 00:19:32.690
with electric guitars is not--

00:19:32.690 --> 00:19:33.440
DOUGLAS ECK: Yeah.

00:19:33.440 --> 00:19:33.981
That's right.

00:19:33.981 --> 00:19:36.150
So first, I tell
the same stories.

00:19:36.150 --> 00:19:37.170
I'm like the grandpa--

00:19:37.170 --> 00:19:39.670
BRENDA FOGG: Well, I don't know
if you tell it to everybody.

00:19:39.670 --> 00:19:40.670
DOUGLAS ECK: Please,
tell us that story again.

00:19:40.670 --> 00:19:41.550
No, but it's true.

00:19:41.550 --> 00:19:44.520
The electric guitar was invented
to be a loud acoustic guitar.

00:19:44.520 --> 00:19:46.566
To overcome noise on stage.

00:19:46.566 --> 00:19:48.690
And the worst-- they were
trying really hard to not

00:19:48.690 --> 00:19:51.030
have these amplifiers distort.

00:19:51.030 --> 00:19:53.520
So imagine a world where
amplifiers don't distort

00:19:53.520 --> 00:19:56.040
and electric guitars sound
like acoustic guitars.

00:19:56.040 --> 00:19:57.870
You actually haven't
moved very far.

00:19:57.870 --> 00:19:59.340
And the breakage
there was actually

00:19:59.340 --> 00:20:01.080
having fun with the distortion.

00:20:01.080 --> 00:20:03.720
And actually, going
for sounds that aren't

00:20:03.720 --> 00:20:04.860
like an acoustic guitar.

00:20:07.306 --> 00:20:08.930
BRENDA FOGG: Let's
go back to Isabelle.

00:20:11.810 --> 00:20:14.740
One of the things I think that's
so interesting about your work,

00:20:14.740 --> 00:20:18.500
and that people
have to co-habitate

00:20:18.500 --> 00:20:21.410
with these physical
things, is that it's just

00:20:21.410 --> 00:20:24.200
as important, or
maybe more important,

00:20:24.200 --> 00:20:26.540
how people feel
about these things

00:20:26.540 --> 00:20:30.950
rather than just what
their utility is.

00:20:30.950 --> 00:20:33.262
What kind of considerations
do you make for--

00:20:33.262 --> 00:20:34.970
we're starting to
sound like hippies now.

00:20:34.970 --> 00:20:39.050
But like, what people's feelings
and their empathies and the way

00:20:39.050 --> 00:20:41.990
they co-exist in the
space with these things?

00:20:41.990 --> 00:20:44.090
ISABELLE OLSSON: I
think a good tool

00:20:44.090 --> 00:20:48.020
that I use a lot is that I
put stuff in front of people

00:20:48.020 --> 00:20:50.270
and ask them, what do
you think it looks like?

00:20:50.270 --> 00:20:51.380
It's a fun game.

00:20:51.380 --> 00:20:54.130
You don't always get back
what you want to hear.

00:20:54.130 --> 00:20:55.910
But it's a really
good way of testing

00:20:55.910 --> 00:20:58.670
if the object you've
created, does it

00:20:58.670 --> 00:21:02.780
have positive connotations
or negative connotations?

00:21:02.780 --> 00:21:06.200
The first time I showed
a prototype of Mini,

00:21:06.200 --> 00:21:09.830
I showed it to a French person
who said, it's like a macaron.

00:21:09.830 --> 00:21:13.310
And I thought that was
amazing because, first of all,

00:21:13.310 --> 00:21:15.080
I love macarons.

00:21:15.080 --> 00:21:18.150
And then, I think having
something connotate

00:21:18.150 --> 00:21:22.620
something sweet and
delicious is just excellent.

00:21:22.620 --> 00:21:26.000
And again, we surround
ourselves with food.

00:21:26.000 --> 00:21:27.710
That was just--

00:21:27.710 --> 00:21:30.950
I knew we were onto
something there.

00:21:30.950 --> 00:21:33.980
And food is something
universally appealing,

00:21:33.980 --> 00:21:35.120
generally.

00:21:35.120 --> 00:21:41.300
So that's one
exercise out of many.

00:21:41.300 --> 00:21:44.720
I think the key is just
to really make the thing

00:21:44.720 --> 00:21:46.910
real really quickly.

00:21:46.910 --> 00:21:49.520
To translate the big
idea into something

00:21:49.520 --> 00:21:53.830
tangible, and then ourselves
living with it for a while,

00:21:53.830 --> 00:21:56.000
too.

00:21:56.000 --> 00:21:58.520
And then also,
think about not only

00:21:58.520 --> 00:22:00.620
the food analogies,
but also making sure

00:22:00.620 --> 00:22:03.620
that the objects we
design are understandable.

00:22:03.620 --> 00:22:05.190
You understand what it is.

00:22:05.190 --> 00:22:07.625
So again with Mini,
we wanted it to look

00:22:07.625 --> 00:22:10.250
a little bit like a speaker and
a little bit like a microphone,

00:22:10.250 --> 00:22:12.200
but not too much of either.

00:22:12.200 --> 00:22:15.120
But be very honest
to that function.

00:22:15.120 --> 00:22:17.090
And then, connotate that
this goes in the home.

00:22:17.090 --> 00:22:21.650
And therefore, the fabrics
and the things that we use to

00:22:21.650 --> 00:22:23.055
surround ourselves with.

00:22:23.055 --> 00:22:23.805
BRENDA FOGG: Yeah.

00:22:23.805 --> 00:22:25.520
It has to have that
human touch to it

00:22:25.520 --> 00:22:27.420
as part of the design process.

00:22:27.420 --> 00:22:29.000
ISABELLE OLSSON:
And the beauty of it

00:22:29.000 --> 00:22:33.410
is when you find these
solutions, a lot of the times,

00:22:33.410 --> 00:22:37.220
they enhance the function
or help with the function.

00:22:37.220 --> 00:22:39.770
Like fabric is this
excellent material

00:22:39.770 --> 00:22:42.990
that is most of the
time audio transparent.

00:22:42.990 --> 00:22:44.900
You can have lights through it.

00:22:44.900 --> 00:22:49.610
You can kind of create this
calmness in the object itself

00:22:49.610 --> 00:22:52.400
by getting all the
functionality through it.

00:22:52.400 --> 00:22:54.830
And I'm really
passionate about trying

00:22:54.830 --> 00:22:59.060
to design pieces of technology,
which hopefully people think

00:22:59.060 --> 00:23:02.310
about they're just stuff
and not as technology,

00:23:02.310 --> 00:23:04.100
but that can live
out in the open.

00:23:04.100 --> 00:23:06.560
There's just way too
many pieces of furniture

00:23:06.560 --> 00:23:09.600
that are purely designed
to hide technology.

00:23:09.600 --> 00:23:14.780
So my goal in life is if we
could get rid of those things.

00:23:14.780 --> 00:23:19.910
BRENDA FOGG: And Ryan,
that sort of human touch

00:23:19.910 --> 00:23:23.790
is pretty evident in most
everything that you do.

00:23:23.790 --> 00:23:27.380
So if we can talk about
the Google Assistant again.

00:23:27.380 --> 00:23:30.050
It was designed to
operate and to be

00:23:30.050 --> 00:23:32.150
used through the
power of conversation,

00:23:32.150 --> 00:23:37.800
which is a fundamental
human interface, I guess.

00:23:37.800 --> 00:23:42.350
And through the course of your
work on creating a personality,

00:23:42.350 --> 00:23:46.640
talk a little bit about how
you sort of steered through

00:23:46.640 --> 00:23:49.135
the landmines of
what kinds of-- aside

00:23:49.135 --> 00:23:51.260
from the transactional
things, what kinds of things

00:23:51.260 --> 00:23:54.669
are people going to want to
talk about with their Assistant?

00:23:54.669 --> 00:23:55.460
RYAN GERMICK: Yeah.

00:23:55.460 --> 00:23:59.220
I mean, I think this
may be a bit cliche,

00:23:59.220 --> 00:24:00.360
but it's so early days.

00:24:00.360 --> 00:24:02.690
So I think we're still steering.

00:24:02.690 --> 00:24:05.960
But for us, a guiding
principle for our success

00:24:05.960 --> 00:24:09.860
is, is a feeling thing.

00:24:09.860 --> 00:24:12.450
Does this feel like a character
you want to spend time with,

00:24:12.450 --> 00:24:14.360
like I mentioned earlier.

00:24:14.360 --> 00:24:18.544
As far as like finding
things that people--

00:24:18.544 --> 00:24:19.710
we wanted to steer clear of.

00:24:19.710 --> 00:24:22.160
I mean, it was really
interesting to look

00:24:22.160 --> 00:24:25.010
at the different queries that
people ask Google Search,

00:24:25.010 --> 00:24:26.980
and then what people
ask Google Assistant.

00:24:26.980 --> 00:24:28.480
And at Google, as
you might imagine,

00:24:28.480 --> 00:24:30.965
there's a lot of people
that have a background

00:24:30.965 --> 00:24:33.740
in like information retrieval.

00:24:33.740 --> 00:24:36.120
And like, data ranking,
things like search ranking,

00:24:36.120 --> 00:24:36.980
things like that.

00:24:36.980 --> 00:24:38.730
And it kind of turned
things on their head

00:24:38.730 --> 00:24:41.180
when now people are
asking questions like,

00:24:41.180 --> 00:24:43.130
what's your favorite
flavor of ice cream?

00:24:43.130 --> 00:24:46.040
Or like, did you fart?

00:24:46.040 --> 00:24:48.320
And those are like pretty--

00:24:48.320 --> 00:24:50.330
more common than you
think when people first

00:24:50.330 --> 00:24:53.090
get a piece of technology
that's been lovingly crafted.

00:24:53.090 --> 00:24:55.130
They all of a sudden have a very
different relationship to it.

00:24:55.130 --> 00:24:56.690
A very sizable
number of the queries

00:24:56.690 --> 00:24:57.890
that we get on the
Google Assistant

00:24:57.890 --> 00:24:59.150
are like, first-date queries.

00:24:59.150 --> 00:25:02.900
Like, do you have any
brothers or sisters?

00:25:02.900 --> 00:25:03.650
It's really sweet.

00:25:03.650 --> 00:25:05.816
DOUGLAS ECK: Ryan, you're
the only person that can--

00:25:05.816 --> 00:25:08.976
what is her favorite
flavor of ice cream?

00:25:08.976 --> 00:25:10.350
I'm sure everybody
wants to know.

00:25:10.350 --> 00:25:11.610
RYAN GERMICK: This is a very
illuminating question, Doug.

00:25:11.610 --> 00:25:13.570
Thank you for asking.

00:25:13.570 --> 00:25:16.400
So basically, we
have a principle.

00:25:16.400 --> 00:25:18.150
And this speaks to
Brenda's question, too.

00:25:18.150 --> 00:25:20.670
We basically set up
principles where we--

00:25:20.670 --> 00:25:22.500
for example, we
have one principle

00:25:22.500 --> 00:25:24.046
that we want to
talk like a human.

00:25:24.046 --> 00:25:26.670
We want to take advantage of the
human voice and the interface.

00:25:26.670 --> 00:25:27.930
But we don't want to
pretend to be one.

00:25:27.930 --> 00:25:29.822
So if you were to
ask a question like,

00:25:29.822 --> 00:25:31.530
what's your favorite
flavor of ice cream,

00:25:31.530 --> 00:25:35.460
we would do what we would
call an artful dodge.

00:25:35.460 --> 00:25:39.000
And we look to our
training in improv theater

00:25:39.000 --> 00:25:41.430
where we don't want to
deny the user of like,

00:25:41.430 --> 00:25:42.840
I do not eat ice cream.

00:25:42.840 --> 00:25:44.030
I do not have a mouth.

00:25:44.030 --> 00:25:45.510
That's like a really
bummer answer.

00:25:45.510 --> 00:25:47.610
If you're exploring
a new technology,

00:25:47.610 --> 00:25:50.292
that's a shut down
to the conversation.

00:25:50.292 --> 00:25:51.750
But at the same
time, we don't want

00:25:51.750 --> 00:25:55.740
to lie and say like, well,
salted caramel, obviously.

00:25:55.740 --> 00:25:59.910
This is like a position that
is disingenuous because it

00:25:59.910 --> 00:26:01.630
does not eat ice cream.

00:26:01.630 --> 00:26:04.110
So we would say
something like, you

00:26:04.110 --> 00:26:05.580
can't go wrong with Neapolitan.

00:26:05.580 --> 00:26:07.620
There's something
in it for everyone.

00:26:07.620 --> 00:26:10.560
And we would take that
question, understand

00:26:10.560 --> 00:26:13.020
that the subtext is like, I'm
getting to know what you are

00:26:13.020 --> 00:26:15.030
and what your capabilities are.

00:26:15.030 --> 00:26:17.212
And we would-- yes, and.

00:26:17.212 --> 00:26:19.170
And we would kind of
continue to play the game.

00:26:19.170 --> 00:26:20.711
And use it actually
as an opportunity

00:26:20.711 --> 00:26:23.270
to make a value statement
that we're inclusive.

00:26:23.270 --> 00:26:25.022
And we want everybody to know--

00:26:25.022 --> 00:26:27.480
we want to reflect that ice
cream that is good for everyone

00:26:27.480 --> 00:26:29.490
is good.

00:26:29.490 --> 00:26:34.440
BRENDA FOGG: How much dialog
goes on within your team

00:26:34.440 --> 00:26:35.730
when you're trying to--

00:26:35.730 --> 00:26:37.380
when you're talking about, OK.

00:26:37.380 --> 00:26:40.680
What if someone asks the
Google Assistant, do you fart?

00:26:40.680 --> 00:26:43.410
RYAN GERMICK: Yeah.

00:26:43.410 --> 00:26:46.050
As soon as that question I
knew was going to be answered.

00:26:46.050 --> 00:26:48.091
And it wasn't just going
to default to an answer,

00:26:48.091 --> 00:26:49.260
I knew that we already won.

00:26:49.260 --> 00:26:51.600
The humanist amongst
us already won.

00:26:51.600 --> 00:26:54.660
Because there was a school of
thought that you would say,

00:26:54.660 --> 00:26:55.500
I don't fart.

00:26:55.500 --> 00:26:56.600
I don't have a body.

00:26:56.600 --> 00:26:58.680
And that was like, end of story.

00:26:58.680 --> 00:26:59.980
And that just seems--

00:26:59.980 --> 00:27:02.580
that's true, but
kind of not in line

00:27:02.580 --> 00:27:06.570
with keeping the game going.

00:27:06.570 --> 00:27:08.380
So we would have a
lot of back and forth.

00:27:08.380 --> 00:27:11.070
And we would then
like, take that answer.

00:27:11.070 --> 00:27:14.250
And we'd say, well, at least you
could say, I don't have a butt.

00:27:14.250 --> 00:27:16.500
Because at least then you'd
be a little more specific.

00:27:16.500 --> 00:27:17.541
BRENDA FOGG: Start there.

00:27:17.541 --> 00:27:19.356
No butt.

00:27:19.356 --> 00:27:20.730
RYAN GERMICK: But
in our case, we

00:27:20.730 --> 00:27:23.295
ended up with something a little
more playful and little more

00:27:23.295 --> 00:27:25.920
addressing the subtext, which is
of the school of like, whoever

00:27:25.920 --> 00:27:27.097
smelt it, dealt it.

00:27:27.097 --> 00:27:28.930
Which we said, you can
blame me if you want.

00:27:28.930 --> 00:27:31.470
I don't mind.

00:27:31.470 --> 00:27:33.630
If the user's asking
about that, let's

00:27:33.630 --> 00:27:36.360
just take it one step further
and put them on the spot.

00:27:36.360 --> 00:27:37.860
BRENDA FOGG: Are
you all going to go

00:27:37.860 --> 00:27:39.130
ask your Google Assistant now?

00:27:39.130 --> 00:27:41.160
RYAN GERMICK: I think there's
like 25 different answers

00:27:41.160 --> 00:27:43.340
because that is definitely
a key use case for--

00:27:43.340 --> 00:27:44.381
BRENDA FOGG: Keep asking.

00:27:44.381 --> 00:27:45.650
Just keep asking.

00:27:45.650 --> 00:27:46.150
OK.

00:27:46.150 --> 00:27:49.950
So let's talk a little bit
about how all of this humanity

00:27:49.950 --> 00:27:53.230
plays out in the context
of a brand, like Google.

00:27:53.230 --> 00:27:56.610
So Isabelle, the Home
Mini, you mentioned,

00:27:56.610 --> 00:27:59.160
needed to be both a
speaker and a microphone

00:27:59.160 --> 00:28:02.310
as well as an assistant and
behave like an assistant.

00:28:05.290 --> 00:28:09.660
So if you're starting from
those kind of very engineering

00:28:09.660 --> 00:28:12.210
kind of product
requirements, how

00:28:12.210 --> 00:28:18.720
do you go from there into the
idea of personality of a brand?

00:28:18.720 --> 00:28:21.660
In Ryan's case, his work talks.

00:28:21.660 --> 00:28:24.100
The personality comes
through that way.

00:28:24.100 --> 00:28:26.370
In your work, it comes
through sort of the materials

00:28:26.370 --> 00:28:27.900
and the things.

00:28:27.900 --> 00:28:31.800
How do you consider the
personality of the Google brand

00:28:31.800 --> 00:28:33.300
in the work that you do?

00:28:33.300 --> 00:28:34.216
ISABELLE OLSSON: Yeah.

00:28:34.216 --> 00:28:36.450
I mean, I think it's
a huge responsibility.

00:28:36.450 --> 00:28:39.960
And we're only a few
years into making hardware

00:28:39.960 --> 00:28:42.570
that people actually
put down money for.

00:28:42.570 --> 00:28:47.070
And you know, the brand
is just really incredible.

00:28:47.070 --> 00:28:50.910
So we're trying to figure
out, what's core to Google?

00:28:50.910 --> 00:28:54.690
And how do we translate
that into physical form?

00:28:54.690 --> 00:28:58.200
And sometimes, it's not
about a direct translation.

00:28:58.200 --> 00:29:01.110
Because most people
don't want to pay money

00:29:01.110 --> 00:29:03.540
for something quirky, maybe.

00:29:03.540 --> 00:29:06.510
So taking that kind of
principle and that idea,

00:29:06.510 --> 00:29:09.150
and then thinking about
what it means for hardware.

00:29:09.150 --> 00:29:11.520
So in this case
for example, to me,

00:29:11.520 --> 00:29:14.220
Google stands for a
sense of optimism.

00:29:14.220 --> 00:29:16.720
And kind of this optimistic
outlook on the future.

00:29:16.720 --> 00:29:20.580
So if I can do things that
remind people of that or that

00:29:20.580 --> 00:29:25.170
makes people smile, I think
that that naturally then feels

00:29:25.170 --> 00:29:26.380
like a Google product.

00:29:26.380 --> 00:29:28.600
So just one simple
example of that

00:29:28.600 --> 00:29:30.850
is you turn Mini
upside down, there

00:29:30.850 --> 00:29:32.490
is a pop of color on the back.

00:29:32.490 --> 00:29:35.760
And only you as a person who
bought the product know that.

00:29:35.760 --> 00:29:38.310
But you know it kind of has
that Google on the inside.

00:29:38.310 --> 00:29:40.620
BRENDA FOGG: Yeah.

00:29:40.620 --> 00:29:41.760
Let's go back to Ryan then.

00:29:41.760 --> 00:29:44.910
Because over the years,
over seven or eight years,

00:29:44.910 --> 00:29:45.750
or whatever.

00:29:45.750 --> 00:29:47.282
However many years you've been--

00:29:47.282 --> 00:29:48.240
DOUGLAS ECK: 12 almost.

00:29:48.240 --> 00:29:49.835
BRENDA FOGG: 12 years.

00:29:49.835 --> 00:29:51.210
You've had a lot
of opportunities

00:29:51.210 --> 00:29:55.050
to craft those sort
of moments of delight

00:29:55.050 --> 00:29:57.390
and those sort of
little user experiences

00:29:57.390 --> 00:29:59.130
that are like
turning over the Mini

00:29:59.130 --> 00:30:00.570
and finding a little surprise.

00:30:00.570 --> 00:30:06.810
So everything from-- you're
responsible for the Pegman,

00:30:06.810 --> 00:30:10.660
which is the character that
you drop into Google Maps

00:30:10.660 --> 00:30:12.789
when you go into Street View.

00:30:12.789 --> 00:30:14.830
And we talked about the
personality of the Google

00:30:14.830 --> 00:30:16.090
Assistant a little bit.

00:30:16.090 --> 00:30:19.840
And then of course, the Doodles
taking over the Home page.

00:30:19.840 --> 00:30:22.354
So over the 12 years
that you've been

00:30:22.354 --> 00:30:23.770
kind of working
in that territory,

00:30:23.770 --> 00:30:26.380
and as the Google brand
has grown and evolved,

00:30:26.380 --> 00:30:30.430
have you found-- how has that
growth of the brand impacted

00:30:30.430 --> 00:30:32.740
the work that you do?

00:30:32.740 --> 00:30:36.850
RYAN GERMICK: I think the
core of what I try to do,

00:30:36.850 --> 00:30:38.710
I almost discovered
it by accident.

00:30:38.710 --> 00:30:40.870
Like the Street
View Pegman, maybe

00:30:40.870 --> 00:30:42.680
is a story for another day.

00:30:42.680 --> 00:30:45.730
But I was just
glad that I worked

00:30:45.730 --> 00:30:48.040
in a place that had free
strawberries when I got here.

00:30:48.040 --> 00:30:49.965
That was very exciting to me.

00:30:49.965 --> 00:30:52.090
And then, that they paid
me to draw and be creative

00:30:52.090 --> 00:30:53.955
was just beyond
my wildest dream.

00:30:53.955 --> 00:30:55.480
So I'm just like,
happy to be here.

00:30:55.480 --> 00:30:58.330
Still, I'm happy to be here.

00:30:58.330 --> 00:31:01.720
But what kind of worked for
me because it was always

00:31:01.720 --> 00:31:05.140
sort of my MO was, how can I
use my position of privilege

00:31:05.140 --> 00:31:10.600
to bring other people up and to
give them a sense of belonging?

00:31:10.600 --> 00:31:13.310
And that has stayed consistent.

00:31:13.310 --> 00:31:17.680
So whether it's trying to make
sure we have inclusive Doodles

00:31:17.680 --> 00:31:21.310
or creating an opportunity for
a little, like mannequin that

00:31:21.310 --> 00:31:25.870
can be dressed up for holidays
or whatever, for Street View.

00:31:25.870 --> 00:31:28.869
There's been a through line
where maybe in the beginning,

00:31:28.869 --> 00:31:30.160
Google was more of an underdog.

00:31:30.160 --> 00:31:32.050
And now, Google is like
a very important part

00:31:32.050 --> 00:31:33.409
of people's lives.

00:31:33.409 --> 00:31:35.950
I don't think you could really
say it's a small organization,

00:31:35.950 --> 00:31:37.020
by any stretch.

00:31:37.020 --> 00:31:38.770
But there are still
human touch points

00:31:38.770 --> 00:31:41.780
that matter to make people
feel like they belong,

00:31:41.780 --> 00:31:44.899
which is what Google's
trying to do for everyone.

00:31:44.899 --> 00:31:47.440
BRENDA FOGG: I want to make sure
we leave time for questions,

00:31:47.440 --> 00:31:48.460
if anybody has them.

00:31:48.460 --> 00:31:50.830
So if you have
questions, you could

00:31:50.830 --> 00:31:54.280
start coming to the
microphones while we kind of go

00:31:54.280 --> 00:31:56.150
a little bit into the future.

00:31:56.150 --> 00:31:57.950
Let's talk about the future.

00:31:57.950 --> 00:32:00.280
So if we're sitting here a
year from now or a few years

00:32:00.280 --> 00:32:07.090
from now, Doug, what do you
expect that machine learning

00:32:07.090 --> 00:32:09.010
might do for art in the future?

00:32:09.010 --> 00:32:12.670
Whether it's your aspirations
for the next 12 or 18 months,

00:32:12.670 --> 00:32:15.950
or maybe 5 years from now.

00:32:15.950 --> 00:32:18.880
DOUGLAS ECK: So I think
the really interesting way

00:32:18.880 --> 00:32:23.710
to think about this is consider
generative models as a family

00:32:23.710 --> 00:32:25.420
of machine learning models.

00:32:25.420 --> 00:32:28.690
Models that generate new
instances of the data

00:32:28.690 --> 00:32:31.450
upon which they're trained.

00:32:31.450 --> 00:32:35.620
Where I see us going is actually
very heavily integrating

00:32:35.620 --> 00:32:39.640
the design process of products
with generative models.

00:32:39.640 --> 00:32:42.070
We're seeing machine
learning generating part

00:32:42.070 --> 00:32:43.289
of what we're trying to do.

00:32:43.289 --> 00:32:45.580
And I think that's going to
touch our lives in the arts

00:32:45.580 --> 00:32:48.250
and music and communication
in a number of ways.

00:32:48.250 --> 00:32:49.840
And to those of you
who are-- anybody

00:32:49.840 --> 00:32:51.020
in the room a developer?

00:32:51.020 --> 00:32:53.200
It's an easy question
because we're at a developer

00:32:53.200 --> 00:32:54.095
conference, right?

00:32:54.095 --> 00:32:55.720
So we're going to
have a responsibility

00:32:55.720 --> 00:32:57.820
as machine learning
experts to understand

00:32:57.820 --> 00:32:59.387
a little bit about design.

00:32:59.387 --> 00:33:00.970
A responsibility as
back-end engineers

00:33:00.970 --> 00:33:03.130
to understand a little bit about
machine learning and design.

00:33:03.130 --> 00:33:05.046
I think we're going to
see much more of a need

00:33:05.046 --> 00:33:06.760
for end-to-end integration.

00:33:06.760 --> 00:33:10.360
For me, the future started
happening already in a sense.

00:33:10.360 --> 00:33:12.640
I have teenage kids, and
I watched just how they

00:33:12.640 --> 00:33:14.290
use Snapchat to communicate.

00:33:14.290 --> 00:33:17.200
And how they've built their
own kind of grammar around it.

00:33:17.200 --> 00:33:19.000
And it's a very,
very simple product.

00:33:19.000 --> 00:33:23.200
Now, imagine 10 years of
advances in assistive writing.

00:33:23.200 --> 00:33:25.660
So you know, you're using
Google Docs and you're writing.

00:33:25.660 --> 00:33:27.970
And you have some machine
learning algorithm

00:33:27.970 --> 00:33:30.580
helping you communicate, right?

00:33:30.580 --> 00:33:32.890
We're going to get very
good at this very fast.

00:33:32.890 --> 00:33:35.254
And I expect that when
my kids were younger,

00:33:35.254 --> 00:33:37.420
the teachers were all worried
if they used Wikipedia

00:33:37.420 --> 00:33:39.100
too much to write their papers.

00:33:39.100 --> 00:33:41.620
And now it's going to be
like, wait, how much of this

00:33:41.620 --> 00:33:42.940
did you actually write?

00:33:42.940 --> 00:33:44.620
What part of it did you write?

00:33:44.620 --> 00:33:47.440
And what part of it did
your Assistant write?

00:33:47.440 --> 00:33:50.440
And I think we can be
dystopian about that.

00:33:50.440 --> 00:33:53.290
There are potentially some
very difficult issues here.

00:33:53.290 --> 00:33:55.420
But it's also
wonderful, I think.

00:33:55.420 --> 00:33:57.430
As long as we use
this to communicate

00:33:57.430 --> 00:33:59.270
more effectively and
in different ways,

00:33:59.270 --> 00:34:01.270
and we make it into
something creative,

00:34:01.270 --> 00:34:03.190
I think it's very
exciting to think

00:34:03.190 --> 00:34:05.140
about how machine
learning can become

00:34:05.140 --> 00:34:08.830
really more deeply integrated
in the process of communicating.

00:34:08.830 --> 00:34:11.035
And again, that's what I
see the arts as being about

00:34:11.035 --> 00:34:12.159
and music being about.

00:34:12.159 --> 00:34:13.210
It's about communicating.

00:34:13.210 --> 00:34:15.280
It's about sharing our
thoughts, our feelings,

00:34:15.280 --> 00:34:17.020
our beliefs with each other.

00:34:17.020 --> 00:34:20.120
And I'm seeing in my career
that happening more deeply

00:34:20.120 --> 00:34:21.370
with machine learning as well.

00:34:21.370 --> 00:34:22.592
So that's my future vision.

00:34:22.592 --> 00:34:23.550
BRENDA FOGG: I love it.

00:34:23.550 --> 00:34:25.040
I love your vision.

00:34:25.040 --> 00:34:26.290
Isabelle, what about hardware?

00:34:26.290 --> 00:34:27.730
What do you what do you
want to see in hardware

00:34:27.730 --> 00:34:29.065
in the next year or two?

00:34:29.065 --> 00:34:31.139
ISABELLE OLSSON:
Well, number 1, I

00:34:31.139 --> 00:34:33.770
hope people find out about it.

00:34:33.770 --> 00:34:36.340
So we just did a small
exhibition in Milan

00:34:36.340 --> 00:34:38.830
a couple of weeks ago.

00:34:38.830 --> 00:34:41.500
And part of the exhibition
was the portfolio

00:34:41.500 --> 00:34:43.939
that we launched last year.

00:34:43.939 --> 00:34:45.730
A lot of people will
come up to me and say,

00:34:45.730 --> 00:34:47.531
these concepts are great.

00:34:47.531 --> 00:34:48.989
And I'm like,
they're not concepts.

00:34:48.989 --> 00:34:50.340
They're actual products.

00:34:50.340 --> 00:34:53.880
So I think it was a
little bit of that.

00:34:53.880 --> 00:34:57.730
And then, I hope we
just continue to design

00:34:57.730 --> 00:35:01.072
for everyone in everyday life.

00:35:01.072 --> 00:35:03.030
BRENDA FOGG: And Ryan,
what would you like to--

00:35:03.030 --> 00:35:03.940
what would you say?

00:35:03.940 --> 00:35:06.190
What would you like
people to take away today?

00:35:06.190 --> 00:35:08.020
RYAN GERMICK: I think just
remember that technology is

00:35:08.020 --> 00:35:09.311
for people, first and foremost.

00:35:09.311 --> 00:35:11.465
So just always
keep that question

00:35:11.465 --> 00:35:12.840
in the back of
your mind of like,

00:35:12.840 --> 00:35:17.562
how is what I'm
doing helping people?

00:35:17.562 --> 00:35:18.490
BRENDA FOGG: OK.

00:35:18.490 --> 00:35:20.080
Do we have questions?

00:35:20.080 --> 00:35:26.120
[MUSIC PLAYING]

