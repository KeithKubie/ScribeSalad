WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.332
[MUSIC PLAYING]

00:00:07.977 --> 00:00:10.560
THORSTEN KURTH: Hello, and thank
you, everybody, for attending

00:00:10.560 --> 00:00:12.080
the afternoon sessions.

00:00:12.080 --> 00:00:13.200
My name is Thorsten Kurth.

00:00:13.200 --> 00:00:15.200
And I'm an application
performance specialist

00:00:15.200 --> 00:00:16.250
at NERSC.

00:00:16.250 --> 00:00:17.970
And my day-to-day
work is helping

00:00:17.970 --> 00:00:21.090
scientists optimize their codes
for contemporary supercomputer

00:00:21.090 --> 00:00:22.230
systems.

00:00:22.230 --> 00:00:24.993
Today I'm going to talk
about a project I care about

00:00:24.993 --> 00:00:26.910
because it combines three
different things I'm

00:00:26.910 --> 00:00:28.020
excited about.

00:00:28.020 --> 00:00:30.330
This is big computers,
so exascale.

00:00:30.330 --> 00:00:31.320
It's deep learning.

00:00:31.320 --> 00:00:32.759
And it's climate
change because it

00:00:32.759 --> 00:00:37.420
will affect everybody, every
one of us sooner or later.

00:00:37.420 --> 00:00:39.290
So this is a team effort.

00:00:39.290 --> 00:00:41.530
And I want to thank, at
this point, everybody

00:00:41.530 --> 00:00:45.310
in this collaborative effort
between NERSC, Nvidia, UC

00:00:45.310 --> 00:00:47.720
Berkeley, and Oak Ridge
for making this a success.

00:00:47.720 --> 00:00:50.850
So thank you at this point.

00:00:50.850 --> 00:00:54.100
So I want to talk about our
extreme weather phenomena.

00:00:54.100 --> 00:00:55.340
So why are they important?

00:00:55.340 --> 00:00:56.757
They're important
because they can

00:00:56.757 --> 00:00:59.350
incur a lot of damage
and loss of life

00:00:59.350 --> 00:01:00.430
and these kind of things.

00:01:00.430 --> 00:01:04.940
For example, 2017, the
damage to the US economy

00:01:04.940 --> 00:01:08.840
was about 200 billion for
the combined extreme weather

00:01:08.840 --> 00:01:09.340
events.

00:01:09.340 --> 00:01:12.130
So these can be hurricanes,
or tropical cyclones,

00:01:12.130 --> 00:01:13.630
and, for example,
atmospheric rivers

00:01:13.630 --> 00:01:19.600
because they can cause heavy
flooding and major disruption.

00:01:19.600 --> 00:01:22.100
So we won't understand
these events better.

00:01:22.100 --> 00:01:25.290
But what a typical climate data
analysis is-- so for example,

00:01:25.290 --> 00:01:27.390
you have these
simulations, which

00:01:27.390 --> 00:01:29.460
look into the future
up to 100 years.

00:01:29.460 --> 00:01:32.420
You run different
models and get these.

00:01:32.420 --> 00:01:36.660
So on your left, you see the
output of the simulations.

00:01:36.660 --> 00:01:39.620
And they basically contain
14 million observables

00:01:39.620 --> 00:01:40.810
for a three-hour interval.

00:01:40.810 --> 00:01:43.800
And then you have like
100 years worth of that.

00:01:43.800 --> 00:01:46.710
And what people usually do when
you look at the IPCC report,

00:01:46.710 --> 00:01:49.142
for example, or in
popular magazines,

00:01:49.142 --> 00:01:50.850
they boil it down to
a couple of numbers.

00:01:50.850 --> 00:01:53.490
For example, temperature
rise, sea level rise,

00:01:53.490 --> 00:01:54.760
these kind of things.

00:01:54.760 --> 00:01:57.750
However, if the temperature
increases by one degrees

00:01:57.750 --> 00:01:59.178
or two, that matters.

00:01:59.178 --> 00:02:00.720
But it might not
matter to you if you

00:02:00.720 --> 00:02:02.340
live in the middle
of the Sahara, right?

00:02:02.340 --> 00:02:03.740
It might matter to
you, though, if you

00:02:03.740 --> 00:02:06.450
are in different regions of the
globe-- and also the sea level

00:02:06.450 --> 00:02:07.510
rise.

00:02:07.510 --> 00:02:09.300
So the thing is
now, what you want

00:02:09.300 --> 00:02:13.640
to do is you want to have a
geospatial analysis of climate

00:02:13.640 --> 00:02:14.140
change.

00:02:14.140 --> 00:02:16.080
So how does climate
change impact your life

00:02:16.080 --> 00:02:18.400
where you live?

00:02:18.400 --> 00:02:20.610
So we want to
answer things like,

00:02:20.610 --> 00:02:22.680
will there be more
hurricanes, for example?

00:02:22.680 --> 00:02:24.280
And if yes, will
they be more intense?

00:02:24.280 --> 00:02:25.530
Will they make more landfalls?

00:02:25.530 --> 00:02:27.420
If they stay over the
sea, it's usually not

00:02:27.420 --> 00:02:29.790
as bad as when they
hit the coastline.

00:02:29.790 --> 00:02:32.230
And for atmospheric
rivers, for example,

00:02:32.230 --> 00:02:35.130
50% of all rain in California
is due to atmospheric rivers.

00:02:35.130 --> 00:02:38.400
So it's an important
question to ask

00:02:38.400 --> 00:02:41.662
if we will get more water,
like more rain, due to this.

00:02:41.662 --> 00:02:43.620
And you, for example,
think about forest fires,

00:02:43.620 --> 00:02:45.810
like the campfire last year
we had in the Bay Area.

00:02:45.810 --> 00:02:48.595
We had a hard time
breathing for two weeks.

00:02:48.595 --> 00:02:50.970
It's really a question if you
get more or fewer of these.

00:02:50.970 --> 00:02:53.345
And this is really dependent
on these atmospheric rivers,

00:02:53.345 --> 00:02:54.490
for example.

00:02:54.490 --> 00:02:59.730
So insurance industry-- for
example, water planners--

00:02:59.730 --> 00:03:01.350
a lot of different
people need to know

00:03:01.350 --> 00:03:04.570
what they need to get up for.

00:03:04.570 --> 00:03:05.870
So how can we do this?

00:03:05.870 --> 00:03:09.430
So we have this high
fidelity climate simulations.

00:03:09.430 --> 00:03:11.380
And what we, for
example, can start with--

00:03:11.380 --> 00:03:12.972
picking out the these events.

00:03:12.972 --> 00:03:14.930
For example, hurricanes
and atmospheric rivers.

00:03:14.930 --> 00:03:16.480
Let's start with these.

00:03:16.480 --> 00:03:18.910
And image segmentation
techniques

00:03:18.910 --> 00:03:20.920
can offer pixel-level
resolution.

00:03:20.920 --> 00:03:23.020
So they can do a
per-pixel classification

00:03:23.020 --> 00:03:25.900
to pick these events out
and then correlate them

00:03:25.900 --> 00:03:29.633
geospatially with the
underlying region, for example.

00:03:29.633 --> 00:03:32.050
And deep learning, as you know,
is very successful in here

00:03:32.050 --> 00:03:35.170
because, for example, the whole
autonomous driving industry

00:03:35.170 --> 00:03:37.040
is doing that day in, day out.

00:03:37.040 --> 00:03:42.260
And there's a lot of research
going on in this direction.

00:03:42.260 --> 00:03:45.510
So the data set we have
is of 20 terabytes.

00:03:45.510 --> 00:03:47.900
So we have like 400
terabyte in storage.

00:03:47.900 --> 00:03:50.710
But for this work, we
use 20 terabytes of it.

00:03:50.710 --> 00:03:53.690
And what I call an image
here is more like a tensor.

00:03:53.690 --> 00:03:57.940
It's a three-dimensional tensor
of this 1152 times 768 times

00:03:57.940 --> 00:03:59.170
16.

00:03:59.170 --> 00:04:01.750
And the channels are not RGB.

00:04:01.750 --> 00:04:04.510
They present observables
like wind speed,

00:04:04.510 --> 00:04:09.410
temperature, pressure,
for different altitudes,

00:04:09.410 --> 00:04:10.970
and these kind of things.

00:04:10.970 --> 00:04:14.080
So they're general observables.

00:04:14.080 --> 00:04:15.070
We have free classes.

00:04:15.070 --> 00:04:17.940
So background, which is not
nothing interesting going on.

00:04:17.940 --> 00:04:20.170
Then the tropical
cyclones, or hurricanes,

00:04:20.170 --> 00:04:21.820
and the atmospheric rivers.

00:04:21.820 --> 00:04:24.220
Fortunately, these events
are still rare in the future.

00:04:24.220 --> 00:04:27.315
So 95% of the pixels
are background,

00:04:27.315 --> 00:04:28.190
which is good for us.

00:04:28.190 --> 00:04:30.100
But it's harder to
train a model on that

00:04:30.100 --> 00:04:32.530
because of this high imbalance.

00:04:32.530 --> 00:04:35.260
And another thing which makes
it different from the classical,

00:04:35.260 --> 00:04:36.700
let's say, streets
in segmentation

00:04:36.700 --> 00:04:38.290
is that all the
objects here are--

00:04:38.290 --> 00:04:40.790
so first, there's a lot of stuff
going on in the background.

00:04:40.790 --> 00:04:42.850
It's not static or slow moving.

00:04:42.850 --> 00:04:45.790
And also the objects
themselves, they change rapidly

00:04:45.790 --> 00:04:48.160
in size and shape, right?

00:04:48.160 --> 00:04:51.370
So even when you look at this
image, this satellite image

00:04:51.370 --> 00:04:55.170
from the hurricane, even as an
expert, you don't know actually

00:04:55.170 --> 00:04:58.120
where you want to say, like
where this hurricane starts

00:04:58.120 --> 00:04:58.780
or ends, right?

00:04:58.780 --> 00:05:01.800
So the labels are pretty fuzzy.

00:05:01.800 --> 00:05:03.970
So talking about that,
how did we get those?

00:05:03.970 --> 00:05:06.430
Of course, the best would be
using human annotated labels.

00:05:06.430 --> 00:05:08.800
But for that data, we didn't
have that at the time.

00:05:08.800 --> 00:05:10.600
We are currently
working on that, though.

00:05:10.600 --> 00:05:13.580
So for this effort, we use
some algorithmic labeling,

00:05:13.580 --> 00:05:15.430
which is an old school
approach in the sense

00:05:15.430 --> 00:05:19.390
that it's basically based
on future engineering

00:05:19.390 --> 00:05:22.990
together with some thresholding
to get the binary masks.

00:05:22.990 --> 00:05:25.333
One can say, OK, why don't
you do the predictions

00:05:25.333 --> 00:05:26.500
with these algorithms, then?

00:05:26.500 --> 00:05:28.875
Because you have a lot of
shortcomings in this algorithm.

00:05:28.875 --> 00:05:30.880
So they are regional dependent.

00:05:30.880 --> 00:05:34.020
Even for different thresholds
get vastly different labels.

00:05:34.020 --> 00:05:35.683
So however, they're
still good enough

00:05:35.683 --> 00:05:36.850
to fit in a network with it.

00:05:36.850 --> 00:05:38.308
And it can pick up
better features,

00:05:38.308 --> 00:05:41.245
as I will show you later.

00:05:41.245 --> 00:05:42.870
So for image segmentation
architecture,

00:05:42.870 --> 00:05:45.450
we picked DeepLab
version 3+ variant.

00:05:45.450 --> 00:05:47.910
So it was developed by Google.

00:05:47.910 --> 00:05:49.300
And basically, it
has an-- as all

00:05:49.300 --> 00:05:51.300
these segmentation network
has an encoder, which

00:05:51.300 --> 00:05:53.070
extracts the features.

00:05:53.070 --> 00:05:55.320
And the decoder part, which
then makes the predictions

00:05:55.320 --> 00:05:56.790
and the skip
connections in order

00:05:56.790 --> 00:05:58.500
to feed the features
at different levels

00:05:58.500 --> 00:06:00.690
from the encoder
stage into the decoder

00:06:00.690 --> 00:06:03.180
to improve the
prediction quality.

00:06:03.180 --> 00:06:07.161
So the original DeepLab had
a [INAUDIBLE] interpolation

00:06:07.161 --> 00:06:08.070
as a decoder.

00:06:08.070 --> 00:06:10.890
And we replaced this with a
fully deconvolution decoder.

00:06:10.890 --> 00:06:13.810
I think the original choice
was made for training reasons

00:06:13.810 --> 00:06:16.200
because it's easier to train
the [INAUDIBLE] interpolater

00:06:16.200 --> 00:06:18.130
because it doesn't
have a lot of weights.

00:06:18.130 --> 00:06:21.720
So our model has 44.7
million parameters.

00:06:21.720 --> 00:06:23.550
And the training cost
for a single step

00:06:23.550 --> 00:06:24.700
on a single sample--

00:06:24.700 --> 00:06:27.630
so forward, backward--
is 14.4 teraflop,

00:06:27.630 --> 00:06:32.210
which is 14.4 times 10 to the
12 floating point operations.

00:06:32.210 --> 00:06:35.700
And on a modern GPU,
like this Nvidia V100,

00:06:35.700 --> 00:06:38.100
you can only fit two
batches in half precision

00:06:38.100 --> 00:06:42.720
or one in single
precision on the GPU.

00:06:42.720 --> 00:06:46.710
So what you need to do is you
need to train it in parallel.

00:06:46.710 --> 00:06:50.860
And we took a purely data
parallel approach here.

00:06:50.860 --> 00:06:52.500
So we used Horovod for this.

00:06:52.500 --> 00:06:54.910
So Horovod is
basically a framework

00:06:54.910 --> 00:06:58.050
which hooks into the TensorFlow
graph in synchronous fashion.

00:06:58.050 --> 00:07:01.080
And reduces tensors
across all the workers

00:07:01.080 --> 00:07:03.660
as they are ready to be reduced.

00:07:03.660 --> 00:07:05.520
It does this using MPI.

00:07:05.520 --> 00:07:07.410
So it provides MPI
callback function.

00:07:07.410 --> 00:07:10.380
MPI is Message
Passing Interface.

00:07:10.380 --> 00:07:13.620
It's a very common framework
for exchanging messages

00:07:13.620 --> 00:07:15.810
between different-- in
a distributed memory

00:07:15.810 --> 00:07:18.030
system such as HPC systems.

00:07:18.030 --> 00:07:21.420
The good thing is that since
a lot of people in HPC use it,

00:07:21.420 --> 00:07:22.980
it's very highly
optimized usually

00:07:22.980 --> 00:07:25.458
for these supercomputers.

00:07:25.458 --> 00:07:27.500
You're still, of course,
responsible for sharding

00:07:27.500 --> 00:07:28.958
your data set,
distribute the data,

00:07:28.958 --> 00:07:31.670
and all these kind of things.

00:07:31.670 --> 00:07:33.800
So we ran on the Summit
supercomputer system.

00:07:33.800 --> 00:07:36.670
So this is the number one
supercomputer in the world.

00:07:36.670 --> 00:07:40.123
So there's this top 500 list,
which is updated twice a year.

00:07:40.123 --> 00:07:42.415
So this is the system at Oak
Ridge National Laboratory.

00:07:42.415 --> 00:07:45.110
It consists of 4,600 nodes.

00:07:45.110 --> 00:07:49.810
It has two Power CPUs in
them and six Nvidia V100

00:07:49.810 --> 00:07:51.670
GPUs with Tensor Cores.

00:07:51.670 --> 00:07:55.658
They are connected using his
high speed NVLink interconnect,

00:07:55.658 --> 00:07:56.450
which is very nice.

00:07:56.450 --> 00:07:58.945
So we can do all
reductions within the node

00:07:58.945 --> 00:08:00.310
very efficiently.

00:08:00.310 --> 00:08:02.740
And it also features
800 gigabyte

00:08:02.740 --> 00:08:05.380
of nonvolatile memory per note,
which is quite cool because you

00:08:05.380 --> 00:08:07.150
can stage part of your
data set into that

00:08:07.150 --> 00:08:09.490
and read it almost
with a DRAM speed.

00:08:09.490 --> 00:08:12.550
So it's almost as fast as
reading it from main memory,

00:08:12.550 --> 00:08:14.560
but it's much bigger.

00:08:14.560 --> 00:08:16.573
So the network is pretty
fast and low latency.

00:08:16.573 --> 00:08:17.990
And what I want
to point out here,

00:08:17.990 --> 00:08:22.720
though, is that we talk a lot
about exascale computing, so

00:08:22.720 --> 00:08:25.360
capability of 10 to the 18
floating point operations

00:08:25.360 --> 00:08:28.410
per second in double precision.

00:08:28.410 --> 00:08:30.300
So this is the next
generation of systems

00:08:30.300 --> 00:08:33.799
want to deploy or
develop and deploy.

00:08:33.799 --> 00:08:35.048
But really look at it.

00:08:35.048 --> 00:08:36.590
If you can stick
with half precision,

00:08:36.590 --> 00:08:39.100
so if you can basically
have an application which

00:08:39.100 --> 00:08:42.700
can utilize half
precision almost for most

00:08:42.700 --> 00:08:45.490
of the computations, you have
an exascale system available

00:08:45.490 --> 00:08:46.220
right now.

00:08:46.220 --> 00:08:47.230
So it's there.

00:08:47.230 --> 00:08:47.980
It's in Oak Ridge.

00:08:47.980 --> 00:08:50.550
You can just go and use it.

00:08:50.550 --> 00:08:52.800
So there are some performance
optimizations necessary,

00:08:52.800 --> 00:08:53.760
of course.

00:08:53.760 --> 00:08:56.542
So when you think
about deep learning,

00:08:56.542 --> 00:08:58.500
you have to optimize the
whole pipeline, right?

00:08:58.500 --> 00:08:59.967
Starting from like the data--

00:08:59.967 --> 00:09:01.050
where do you read it from?

00:09:01.050 --> 00:09:02.200
Where to stage it in?

00:09:02.200 --> 00:09:03.865
Then how do you feed it
efficiently to accelerator,

00:09:03.865 --> 00:09:04.365
right?

00:09:04.365 --> 00:09:05.895
The accelerator is
so fast that you

00:09:05.895 --> 00:09:07.770
need to feed them
efficiently that they don't

00:09:07.770 --> 00:09:09.420
stall waiting for that data.

00:09:09.420 --> 00:09:10.920
For the computational
part, you want

00:09:10.920 --> 00:09:13.825
to minimize the data
organization, for example.

00:09:13.825 --> 00:09:16.200
And the reductions also need
to be very efficient, right?

00:09:16.200 --> 00:09:19.950
Because you want to reduce
the gradients at a very, very

00:09:19.950 --> 00:09:22.020
high frequency.

00:09:22.020 --> 00:09:24.420
One thing we also use
was some overlapping

00:09:24.420 --> 00:09:26.460
or grading pipelining
or asynchronous

00:09:26.460 --> 00:09:30.810
approach you call it where you
do not reduce the gradients--

00:09:30.810 --> 00:09:32.520
they do not compute
the fresh gradients

00:09:32.520 --> 00:09:34.770
and produce them and
then integrate them.

00:09:34.770 --> 00:09:37.440
But instead, you
come on the GPU.

00:09:37.440 --> 00:09:39.780
You compute fresh gradients.

00:09:39.780 --> 00:09:42.330
And then on the CPU, you
read all to the gradients

00:09:42.330 --> 00:09:43.860
from the last step
from a buffer.

00:09:43.860 --> 00:09:46.410
Reduce those asynchronously
to the competition

00:09:46.410 --> 00:09:47.583
of the new gradients.

00:09:47.583 --> 00:09:49.000
And integrate them
into the model.

00:09:49.000 --> 00:09:54.360
So by that you can overlap
these two steps very nicely.

00:09:54.360 --> 00:09:56.360
So this is a plot for
the performance we got.

00:09:56.360 --> 00:09:58.652
So you see, the throughput
metric of images per second,

00:09:58.652 --> 00:10:02.820
or call it samples per second,
versus the number of GPUs,

00:10:02.820 --> 00:10:05.180
if you divide it by 6, you
get the number of nodes.

00:10:05.180 --> 00:10:08.180
And the other
y-axis is basically

00:10:08.180 --> 00:10:10.400
a translation of this
image throughput metric

00:10:10.400 --> 00:10:13.460
into a more HPC
metric of petaflops

00:10:13.460 --> 00:10:16.830
per second-- so 10 to the
15 operations per second.

00:10:16.830 --> 00:10:18.750
So what you see is the FP32.

00:10:18.750 --> 00:10:20.930
So the single precision
points are blues.

00:10:20.930 --> 00:10:22.430
So I don't want to
talk about these.

00:10:22.430 --> 00:10:24.890
What you can see that the
FP16, so the half precision

00:10:24.890 --> 00:10:26.442
performance much,
much better, right?

00:10:26.442 --> 00:10:27.900
So the Tensor Cores
can, in theory,

00:10:27.900 --> 00:10:30.860
deliver 125 teraflops per card.

00:10:30.860 --> 00:10:33.830
And that is what you see is
vast performance difference.

00:10:33.830 --> 00:10:36.973
The dashed line
represents the ideal case.

00:10:36.973 --> 00:10:38.390
in the ideal case,
where you don't

00:10:38.390 --> 00:10:40.190
have any lost due
to communication,

00:10:40.190 --> 00:10:42.920
you would be basically
on this line.

00:10:42.920 --> 00:10:46.815
So we are a bit below with
the solid red line but not

00:10:46.815 --> 00:10:47.315
far things.

00:10:47.315 --> 00:10:49.600
I think it's
70-something percent, 79%

00:10:49.600 --> 00:10:51.410
scanning efficiency.

00:10:51.410 --> 00:10:53.390
And also what you see
that the lacked version--

00:10:53.390 --> 00:10:55.210
so where you can
basically overlap

00:10:55.210 --> 00:10:58.015
the computation of the
communication very nicely--

00:10:58.015 --> 00:10:59.390
it's very crucial
to do this here

00:10:59.390 --> 00:11:02.510
because the GPUs are so
fast that they really

00:11:02.510 --> 00:11:05.690
need to wait for it
or reduce otherwise.

00:11:05.690 --> 00:11:07.760
So and after we saw
this, we thought, OK, we

00:11:07.760 --> 00:11:09.680
can go to a couple more nodes.

00:11:09.680 --> 00:11:12.380
But we might not still
hit the exaflop mark,

00:11:12.380 --> 00:11:14.430
which is this 1,000
petaflops per second.

00:11:14.430 --> 00:11:16.610
So we restructured the
decoder a little bit,

00:11:16.610 --> 00:11:22.450
and not like from
the predictive power.

00:11:22.450 --> 00:11:25.440
But we removed some additional
data transpositions.

00:11:25.440 --> 00:11:27.600
And we ran it on a
couple of more nodes

00:11:27.600 --> 00:11:30.603
and actually got there.

00:11:30.603 --> 00:11:32.520
So the performance number
we got at that scale

00:11:32.520 --> 00:11:35.850
was 1.13 exaflops in FP16.

00:11:35.850 --> 00:11:39.210
So half precision on 27,360 GPU.

00:11:39.210 --> 00:11:42.560
And that is so far the biggest
deep learning calculation

00:11:42.560 --> 00:11:44.717
I'm aware of.

00:11:44.717 --> 00:11:45.925
So this is the training loss.

00:11:45.925 --> 00:11:48.100
This is on a
slightly lower scale.

00:11:48.100 --> 00:11:51.340
We don't have this full
history for the big scale.

00:11:51.340 --> 00:11:52.605
However, what you can see--

00:11:52.605 --> 00:11:54.730
the case I to make here is
that the select version,

00:11:54.730 --> 00:11:57.962
although it's
partially asynchronous,

00:11:57.962 --> 00:11:59.920
but it's like predictable
asynchronous in a way

00:11:59.920 --> 00:12:02.990
that the network at the
beginning is a bit unstable.

00:12:02.990 --> 00:12:05.700
So basically the training
[INAUDIBLE] grows.

00:12:05.700 --> 00:12:07.060
So it oscillates heavily.

00:12:07.060 --> 00:12:09.110
But then when you
just wait long enough,

00:12:09.110 --> 00:12:11.470
it will outperform
the unlagged version.

00:12:11.470 --> 00:12:13.408
So that, of course,
is not true for

00:12:13.408 --> 00:12:15.200
every arbitrary like
deep learning network.

00:12:15.200 --> 00:12:17.570
But for us, it's
definitely true.

00:12:17.570 --> 00:12:19.480
And I think it's
definitely worth

00:12:19.480 --> 00:12:22.780
a try if you have a
problem like that.

00:12:22.780 --> 00:12:25.780
So talking about the results,
I have a video for this.

00:12:25.780 --> 00:12:28.663
So on the left-hand side,
you see the predicted weather

00:12:28.663 --> 00:12:29.580
patterns by the model.

00:12:29.580 --> 00:12:31.990
In the right-hand side,
you see the ground truth.

00:12:31.990 --> 00:12:33.570
So I have three things to say.

00:12:33.570 --> 00:12:36.960
So first, there's some
qualitative agreement and also

00:12:36.960 --> 00:12:40.110
quantitative agreement,
which is satisfactory.

00:12:40.110 --> 00:12:41.880
What you also see is
that there are more

00:12:41.880 --> 00:12:43.980
predicted events than
actually in the labels.

00:12:43.980 --> 00:12:50.630
And that is mainly because
the aggressive thresholding,

00:12:50.630 --> 00:12:52.757
sometimes forgets
to label stuff.

00:12:52.757 --> 00:12:54.590
So when you maybe show
some of these samples

00:12:54.590 --> 00:12:57.410
where we overpredict
atmospheric rivers, for example,

00:12:57.410 --> 00:12:58.520
to experts, they say, yes.

00:12:58.520 --> 00:13:01.340
Actually, the model picked up an
atmospheric river which was not

00:13:01.340 --> 00:13:03.240
present in the ground truth.

00:13:03.240 --> 00:13:05.720
And then you can also see
that the ground truth,

00:13:05.720 --> 00:13:07.840
you see the video is flickering.

00:13:07.840 --> 00:13:09.260
And this is because--

00:13:09.260 --> 00:13:12.130
there's like a frame before and
after where it, for example,

00:13:12.130 --> 00:13:13.880
picked up an atmospheric
river but a frame

00:13:13.880 --> 00:13:15.290
in between where it did not.

00:13:15.290 --> 00:13:16.640
But of course, it
should be continuous.

00:13:16.640 --> 00:13:17.765
It should not be like this.

00:13:17.765 --> 00:13:19.460
So the model actually
predicts something

00:13:19.460 --> 00:13:21.585
which is much more continuous
and much more smooth.

00:13:21.585 --> 00:13:23.590
Even if it did not--

00:13:23.590 --> 00:13:26.580
the temporal dependence
into account.

00:13:26.580 --> 00:13:30.110
So that is quite interesting.

00:13:30.110 --> 00:13:31.780
So my conclusions are--

00:13:31.780 --> 00:13:34.120
so TensorFlow is one of
the first applications

00:13:34.120 --> 00:13:36.970
which reached exascale
performance, although only

00:13:36.970 --> 00:13:37.720
in FP16.

00:13:37.720 --> 00:13:39.100
But still it's remarkable.

00:13:39.100 --> 00:13:41.530
And I think this is a
community achievement.

00:13:41.530 --> 00:13:45.230
And HPC systems are suitable
for these workloads.

00:13:45.230 --> 00:13:47.372
Of course, there are
some insufficiencies--

00:13:47.372 --> 00:13:48.580
for example, the file system.

00:13:48.580 --> 00:13:51.580
So we needed this large,
[INAUDIBLE] storage in order

00:13:51.580 --> 00:13:53.380
to feed the data efficiently.

00:13:53.380 --> 00:13:55.520
If you try to read from a
distributed file system,

00:13:55.520 --> 00:13:58.660
it's very bad because HPC
file systems are optimized

00:13:58.660 --> 00:14:03.100
for writing large chunks of data
but not doing random reads, OK?

00:14:03.100 --> 00:14:05.582
So if you want to design HPC
system in the future, which

00:14:05.582 --> 00:14:07.040
is very suitable
for deep learning,

00:14:07.040 --> 00:14:08.250
you need to take
this into account.

00:14:08.250 --> 00:14:09.400
So this is also
a very important.

00:14:09.400 --> 00:14:11.192
And also, we want to
talk to storage people

00:14:11.192 --> 00:14:15.010
to help us to develop better
distributed storage which

00:14:15.010 --> 00:14:18.050
can cope with these
workflows better.

00:14:18.050 --> 00:14:22.490
This work was awarded
the ACM Gordon Bell

00:14:22.490 --> 00:14:24.550
prize at the last
supercomputing conference.

00:14:24.550 --> 00:14:28.730
This price usually awarded for
an interesting and challenging

00:14:28.730 --> 00:14:33.020
science problem for which you
need massive amounts of compute

00:14:33.020 --> 00:14:33.620
to solve it.

00:14:33.620 --> 00:14:35.000
And then you can show
that you actually

00:14:35.000 --> 00:14:36.350
use this massive
amount of compute

00:14:36.350 --> 00:14:37.350
efficiently to solve it.

00:14:39.720 --> 00:14:41.905
So this is the paper link.

00:14:41.905 --> 00:14:43.530
Thank you very much
for your attention.

00:14:43.530 --> 00:14:46.880
[MUSIC PLAYING]

