WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:02.607
♪ (music) ♪

00:00:05.768 --> 00:00:06.737
Hi.

00:00:10.029 --> 00:00:12.140
I'd like to get started again.

00:00:12.140 --> 00:00:14.852
Hi, my name is Brennan,
and I'm going to be talking to you today

00:00:14.852 --> 00:00:17.281
about training performance.

00:00:17.281 --> 00:00:19.596
Now this talk is centered

00:00:19.596 --> 00:00:23.222
around a user's guide
to how to improve performance.

00:00:23.222 --> 00:00:27.506
So performance is very complicated,
there's a lot of internals to <i>TensorFlow</i>

00:00:27.506 --> 00:00:31.144
as to things we're doing
to optimize your training time.

00:00:31.144 --> 00:00:33.544
What I'm going to talk to you today
is about how you can make

00:00:33.544 --> 00:00:36.158
the most of the <i>TensorFlow</i>
you know and love

00:00:36.158 --> 00:00:37.948
to converge faster.

00:00:37.948 --> 00:00:40.149
Now before I go any further,
I do want to take a moment

00:00:40.149 --> 00:00:42.945
to acknowledge the great work
that is done not just by other engineers

00:00:42.945 --> 00:00:46.637
on the <i>TensorFlow</i> team,
and not just by other teams at <i>Google</i>,

00:00:46.637 --> 00:00:49.527
but actually also by our partner teams,
for example the performance team

00:00:49.527 --> 00:00:50.738
at <i>Nvidia</i>.

00:00:50.738 --> 00:00:53.297
They've done a lot of work
to help make <i>TensorFlow</i> work fast

00:00:53.297 --> 00:00:55.627
and I want to make sure
we acknowledge that.

00:00:56.814 --> 00:00:59.747
With that, let's dig in 
to a little bit of motivation.

00:00:59.747 --> 00:01:01.182
Why do we need performance?

00:01:01.182 --> 00:01:04.239
Why do we need to improve performance?
Isn't it just fine today?

00:01:04.964 --> 00:01:07.236
Some folks at <i>Baidu</i>
put together some research,

00:01:07.236 --> 00:01:09.595
and they showed that
if you want to improve

00:01:09.595 --> 00:01:11.793
the quality of your models,

00:01:11.793 --> 00:01:13.942
just train on larger datasets.

00:01:13.942 --> 00:01:15.856
These beautiful straight lines are showing

00:01:15.856 --> 00:01:18.488
that for multiple different models

00:01:18.488 --> 00:01:20.751
as you just give more
and more training data,

00:01:20.751 --> 00:01:22.813
you just get linearly more accurate.

00:01:23.751 --> 00:01:25.638
Now I'm actually being 
slightly facetious here.

00:01:25.638 --> 00:01:28.613
If you look closely, 
the axes on this graph

00:01:28.613 --> 00:01:29.937
are actually logarithmic.

00:01:29.937 --> 00:01:31.268
They're not linear.

00:01:31.268 --> 00:01:35.002
And so, really, we don't need
linearly increasing amounts of data,

00:01:35.002 --> 00:01:38.590
we need exponentially more data
to keep improving our models.

00:01:38.590 --> 00:01:42.436
And they found that this trend holds
not just for different model classes--

00:01:42.436 --> 00:01:44.057
in this case, it was...

00:01:45.787 --> 00:01:48.836
sequence to sequence type work,
but they found this applied to images.

00:01:48.836 --> 00:01:51.630
They found this applied to translation,
to multiple different areas

00:01:51.630 --> 00:01:53.301
across multiple different model types.

00:01:53.301 --> 00:01:56.161
So we're going to need to train
on exponentially more data

00:01:56.161 --> 00:01:58.153
to improve our model quality.

00:01:58.868 --> 00:02:03.069
Unfortunately, we're running into
quite the obstinate adversary:

00:02:03.069 --> 00:02:04.846
physics.

00:02:04.846 --> 00:02:07.508
Here's a graph of 
microprocessor trend data

00:02:07.508 --> 00:02:08.949
over 40 years.

00:02:08.949 --> 00:02:13.017
And we can see that clock frequency
has entirely hit a wall.

00:02:13.017 --> 00:02:15.807
Single-threaded performance 
is not getting that much faster

00:02:15.807 --> 00:02:17.783
compared to how it used to.

00:02:17.783 --> 00:02:20.506
We're going to have to work a lot harder

00:02:20.506 --> 00:02:23.625
to meet the challenges 
of today and tomorrow

00:02:23.625 --> 00:02:24.558
with performance.

00:02:24.558 --> 00:02:27.295
<i>Silicon</i> itself is not 
going to get us there

00:02:27.295 --> 00:02:29.479
without a little bit of cleverness.

00:02:31.219 --> 00:02:34.694
The result of these two forces
coming together has resulted

00:02:34.694 --> 00:02:38.603
in a Cambrian explosion
of accelerator hardware.

00:02:38.603 --> 00:02:40.837
Not just CPUs, but we have GPUs today,

00:02:40.837 --> 00:02:44.207
and TPUs, and there's other more
exciting things coming.

00:02:44.207 --> 00:02:47.940
There's startups like <i>Nirvana</i>
that is now part of <i>Intel</i>.

00:02:47.940 --> 00:02:50.106
And there's the IPU from <i>Graphcore</i>.

00:02:50.106 --> 00:02:53.285
They're taking a bunch 
of different points in the design space

00:02:53.285 --> 00:02:56.859
and are trying different hardware,
methodologies and layouts

00:02:56.859 --> 00:02:58.978
to try and get the best 
machine learning performance.

00:02:58.978 --> 00:03:03.288
This is going to be a very exciting area
coming forward

00:03:03.969 --> 00:03:06.043
as we think about performance
in the future.

00:03:07.133 --> 00:03:09.396
Now before I dig in 
to the meat of my talk,

00:03:09.396 --> 00:03:11.166
I do want to give--

00:03:11.166 --> 00:03:12.661
I do want to acknowledge

00:03:12.661 --> 00:03:16.418
this is actually the Jurassic period 
and not the Precambrian era,

00:03:16.418 --> 00:03:18.608
so if you have 
a good picture of trilobites

00:03:18.608 --> 00:03:22.017
that is Creative Commons-ly licensed,
please do send it my way.

00:03:24.567 --> 00:03:25.699
In a nutshell,

00:03:25.699 --> 00:03:27.098
the machine learning training loop,

00:03:27.098 --> 00:03:30.729
across a wide variety 
of different models and paradigms,

00:03:30.729 --> 00:03:32.588
looks roughly as follows.

00:03:32.588 --> 00:03:35.223
You have your training data,
and you need to load that in--

00:03:35.223 --> 00:03:38.171
this is sort of phase one--
you read it from either disk

00:03:38.171 --> 00:03:40.619
or generate it from 
a reinforcement learning environment,

00:03:40.619 --> 00:03:42.726
you decompress it, you parse it.

00:03:42.726 --> 00:03:44.507
If you're doing, for example,
an image model,

00:03:44.507 --> 00:03:47.297
you perform image augmentations,
random flips, color distortions

00:03:47.297 --> 00:03:50.449
and whatnot, and you batch it up
to feed it into your model.

00:03:50.449 --> 00:03:53.489
In phase two, you compute 
the forward pass of your model,

00:03:53.489 --> 00:03:57.147
you compute the loss,
the backwards pass, your gradients.

00:03:57.147 --> 00:04:00.083
And finally, after you have the gradients,
you update the weights,

00:04:00.083 --> 00:04:03.531
the things you are trying to learn,
and you repeat the cycle all over again.

00:04:04.433 --> 00:04:06.486
Now again, there's 
a wide variety of accelerators

00:04:06.486 --> 00:04:10.459
but to a first approximation,
we see the following patterns emerge.

00:04:10.459 --> 00:04:14.979
Your training data and your pre-processing,
your phase one, happens mostly on CPUs.

00:04:14.979 --> 00:04:18.350
The accelerator then takes over 
phase two and phase three.

00:04:19.770 --> 00:04:21.966
And with that, let's dig in.

00:04:23.316 --> 00:04:27.500
In my experience, as people migrate
to modern accelerators,

00:04:27.500 --> 00:04:31.169
new TPUs, new generation of GPUs, etc.,

00:04:31.169 --> 00:04:34.659
phase one is actually where 
the most performance problems are,

00:04:34.659 --> 00:04:37.575
things <i>everyone</i> hits--
there's always a problem with phase one.

00:04:37.575 --> 00:04:41.116
so we're going to spend a bit of time
digging into to input pipelines.

00:04:41.639 --> 00:04:45.069
Now you heard earlier 
from Derek about <i>tf.data.</i>

00:04:45.069 --> 00:04:48.811
This is definitely the far and away 
recommended API

00:04:48.811 --> 00:04:51.017
and way to load data into <i>TensorFlow</i>.

00:04:51.017 --> 00:04:53.373
And here's if you're doing 
a simple image model,

00:04:53.373 --> 00:04:55.122
for example <i>ResNet-50</i>,

00:04:55.122 --> 00:04:58.223
your input pipeline
will probably start off looking like this.

00:04:58.223 --> 00:05:02.890
You have your data, your images
batched together into TFRecord files.

00:05:02.890 --> 00:05:05.325
You load them in 
with the TFRecord dataset.

00:05:05.325 --> 00:05:06.949
You can shuffle and repeat.

00:05:06.949 --> 00:05:11.192
Your parser function that you map
across every input image,

00:05:11.192 --> 00:05:13.958
that will do things like 
parse <i>tf.Example</i>s, JPEG decode,

00:05:13.958 --> 00:05:15.991
your color transformations, augmentations.

00:05:15.991 --> 00:05:18.509
You batch it up 
and you return your dataset.

00:05:19.203 --> 00:05:21.929
Now if you run this 
on a fancy pants Cloud TPU,

00:05:21.929 --> 00:05:23.851
a modern accelerator,

00:05:23.851 --> 00:05:26.968
you're actually only going to get
about a 150 images a second.

00:05:27.833 --> 00:05:30.654
This is nowhere near 
what you should expect.

00:05:31.071 --> 00:05:33.902
Now before you think, "Wow, 
Cloud TPUs must be total garbage!

00:05:33.902 --> 00:05:35.942
I'm going to go back 
to what I was doing before!"

00:05:35.942 --> 00:05:37.412
It behooves you

00:05:37.412 --> 00:05:39.404
to try and optimize performance.

00:05:40.541 --> 00:05:42.536
Now when you're optimizing performance,

00:05:42.536 --> 00:05:46.083
it's incredibly important
to follow a methodology.

00:05:46.083 --> 00:05:47.891
You have to measure your performance,

00:05:47.891 --> 00:05:49.388
find your bottleneck,

00:05:49.388 --> 00:05:50.870
optimize your bottleneck,

00:05:50.870 --> 00:05:52.461
and repeat.

00:05:52.461 --> 00:05:54.959
So what does that look like
with a Cloud TPU?

00:05:54.959 --> 00:05:58.693
Again, there's tools in <i>TensorFlow</i>
for GPUs and TPUs.

00:05:58.693 --> 00:06:01.102
But for Cloud TPUs we have this tool

00:06:01.102 --> 00:06:04.491
called <i>capture_tpu_ profile</i>,
and you can run this--

00:06:06.202 --> 00:06:07.212
Excuse me.

00:06:07.212 --> 00:06:09.481
You can run this, pointing it to your TPU,

00:06:09.481 --> 00:06:12.134
in this case the TPU's name is "saeta".

00:06:12.134 --> 00:06:17.072
And you will also have it capture
the profile into a log directory.

00:06:17.072 --> 00:06:19.733
This is the same log directory 
you use with <i>TensorBoard</i>.

00:06:19.733 --> 00:06:21.911
And so you can load it up
into <i>TensorBoard</i>.

00:06:21.911 --> 00:06:23.786
And with that I'd like 
to switch to my laptop

00:06:23.786 --> 00:06:27.248
where you can actually see 
what the profiling tools look like.

00:06:27.978 --> 00:06:32.105
So here is a trace from actually
the very same input pipeline,

00:06:32.105 --> 00:06:34.754
or very similar to the input pipeline
I just showed.

00:06:34.754 --> 00:06:37.012
And you can see here

00:06:37.012 --> 00:06:38.783
that your step-time graph,

00:06:38.783 --> 00:06:41.553
you have this tiny bit 
of orange at the bottom.

00:06:41.553 --> 00:06:44.622
And this is the compute time
on the Cloud TPU.

00:06:44.622 --> 00:06:47.329
And everything up here that's blue,

00:06:47.329 --> 00:06:48.728
that's actually waiting for data.

00:06:48.728 --> 00:06:50.532
That's input pipeline processing.

00:06:50.532 --> 00:06:53.468
So our TPU is actually sitting idle,
and this is telling you

00:06:53.468 --> 00:06:55.624
92% of the time.

00:06:56.262 --> 00:06:58.948
Totally, totally not what 
we want to be doing.

00:06:58.948 --> 00:07:00.753
So let's dig in.

00:07:00.753 --> 00:07:02.528
Now what I recommend using--

00:07:02.528 --> 00:07:04.814
We have a bunch of tools
and they're constantly improving

00:07:04.814 --> 00:07:07.461
and getting better, but for now
to really understand what's going on

00:07:07.461 --> 00:07:10.612
underneath the hood,
we're going to use the Trace Viewer.

00:07:10.612 --> 00:07:11.781
So here I've loaded it up.

00:07:11.781 --> 00:07:14.566
One thing I should note is that
the Trace Viewer--

00:07:14.566 --> 00:07:18.223
it's very much designed for power users
so it may be a little unapproachable.

00:07:18.803 --> 00:07:21.339
Let me walk you through
a little bit about

00:07:21.339 --> 00:07:23.486
where to look in the Trace Viewer.

00:07:23.486 --> 00:07:25.724
So in the top you actually have your TPU.

00:07:25.724 --> 00:07:29.533
Now one Cloud TPU has eight compute cores.

00:07:29.533 --> 00:07:32.593
And these operate independently,
although typically they're operating

00:07:32.593 --> 00:07:35.674
on the same sorts of data,
just in parallel.

00:07:35.674 --> 00:07:38.622
So these are at the top,
you can see your step number.

00:07:38.622 --> 00:07:41.290
You can see the <i>TensorFlow</i> ops
that it's executing.

00:07:41.290 --> 00:07:43.063
And finally, the <i>XLA</i> ops.

00:07:43.063 --> 00:07:46.205
So TPUs are programmed via <i>XLA</i>,
you can see what's going on

00:07:46.205 --> 00:07:48.028
underneath the hood.

00:07:48.028 --> 00:07:50.344
Below that you see 
your CPU compute threads.

00:07:50.344 --> 00:07:51.625
Now this <i>tf_Compute</i>,

00:07:51.625 --> 00:07:54.470
these are your general
<i>TensorFlow</i> thread pool threads.

00:07:54.470 --> 00:07:56.535
You got your iterator thread.

00:07:56.535 --> 00:07:59.637
Then finally, there's a set of threads
for infeed and outfeed.

00:07:59.637 --> 00:08:04.166
These are the threads that are managing
DMAs to and from the TPU device.

00:08:05.082 --> 00:08:07.730
Now it's a little hard to see at the top,

00:08:08.403 --> 00:08:09.806
so we're going to need to zoom in.

00:08:09.806 --> 00:08:11.963
We're going to need to look
in a little bit more depth.

00:08:11.963 --> 00:08:14.286
In order to navigate 
around within the trace,

00:08:14.286 --> 00:08:17.573
the keyboard shortcuts,
I find, are the most useful.

00:08:17.573 --> 00:08:20.822
Now, keyboard shortcuts
are from the left hand

00:08:20.822 --> 00:08:23.595
so A and D move left and right,

00:08:23.595 --> 00:08:25.168
W and S move in and out.

00:08:25.168 --> 00:08:28.113
So this is a little bit
like the arrow on a keypad

00:08:28.113 --> 00:08:31.006
just with your left hand on the home row.

00:08:31.006 --> 00:08:32.847
Now there's a couple 
other keyboard shortcuts.

00:08:32.847 --> 00:08:34.484
For example, if you click on something,

00:08:34.484 --> 00:08:36.727
you can see the details 
about what this is.

00:08:36.727 --> 00:08:42.285
And if you press F, you'll focus in
on just that element of the timeline,

00:08:42.285 --> 00:08:44.623
so you can zoom in
and navigate around really easily.

00:08:44.623 --> 00:08:46.795
And if you want to see what
a little bit more about it,

00:08:46.795 --> 00:08:49.494
you can press M,
and this marks it on the UI.

00:08:49.494 --> 00:08:50.904
So you can see that our step time,

00:08:50.904 --> 00:08:54.916
our training step 80, took 5.4 seconds.

00:08:54.916 --> 00:08:56.944
And if we go to the infeed queue,

00:08:56.944 --> 00:08:59.906
that was actually also 5.4 seconds.

00:08:59.906 --> 00:09:02.224
So let's dig in to what's going on
on the CPU,

00:09:02.224 --> 00:09:04.758
since the TPU is just sitting idle
waiting for data.

00:09:05.295 --> 00:09:08.565
Now there's a lot of things going on,
so we're going to zoom in a lot farther.

00:09:08.565 --> 00:09:11.436
So let's zoom in, 
not just at the second range,

00:09:11.436 --> 00:09:14.254
but let's zoom in down
to the millisecond range.

00:09:14.254 --> 00:09:17.337
So here we can see 
each of these vertical bars,

00:09:17.337 --> 00:09:18.797
that's 5 milliseconds.

00:09:18.797 --> 00:09:19.868
Okay?

00:09:19.868 --> 00:09:23.182
And if we zoom in this far,
we can see that our iterator

00:09:23.182 --> 00:09:25.227
is running continuously.

00:09:25.227 --> 00:09:29.333
And the map function is what's taking
the longest amount of time.

00:09:29.945 --> 00:09:30.657
Okay?

00:09:30.657 --> 00:09:32.386
The map function--
there's a bunch of other

00:09:32.386 --> 00:09:35.574
little ops that are happening here

00:09:35.574 --> 00:09:38.164
that are your batching,
or your [inaudible] or whatnot.

00:09:38.164 --> 00:09:39.979
But the map function 
is the bulk of the time

00:09:39.979 --> 00:09:42.285
so that's where we need
to focus our optimization efforts.

00:09:42.285 --> 00:09:46.145
And the map function runs
the elements of the map

00:09:46.145 --> 00:09:49.830
on your normal standard
<i>TensorFlow</i> thread pool.

00:09:49.830 --> 00:09:52.345
And if you look actually closely,
we can zoom in a bit further,

00:09:52.345 --> 00:09:54.820
there's actually no two ops 
running at the same time.

00:09:54.820 --> 00:09:56.797
Even though we're using
multiple different threads

00:09:56.797 --> 00:09:59.586
in the thread pool, we're actually
processing this single-threaded.

00:09:59.586 --> 00:10:01.674
And that leads us 
to our first optimization.

00:10:01.674 --> 00:10:03.384
So I'm going to switch back to the slides.

00:10:06.924 --> 00:10:09.939
This is what you need to do to use
multiple threads for your input pipeline

00:10:09.939 --> 00:10:11.165
for your map function.

00:10:11.165 --> 00:10:15.407
You just set <i>num_parallel_calls</i> to 64
and you'll be using up to 64 threads

00:10:15.407 --> 00:10:17.846
on your compute node
and because Cloud TPUs

00:10:17.846 --> 00:10:20.157
are hooked up to 
a very powerful host machine,

00:10:20.157 --> 00:10:24.040
you can certainly use all 
of these threads running concurrently.

00:10:24.040 --> 00:10:28.293
And if you do this and rerun
your tool, rerun your model,

00:10:28.293 --> 00:10:31.596
you have a 4x improvement--
over 600 images a second.

00:10:31.596 --> 00:10:33.128
So that's pretty great.

00:10:33.756 --> 00:10:36.021
But we're not done.

00:10:36.021 --> 00:10:38.294
An important part 
of the performance methodology

00:10:38.294 --> 00:10:40.030
is step three - repeat.

00:10:40.030 --> 00:10:42.097
You <i>have</i> to repeat again.

00:10:42.097 --> 00:10:44.263
So we take a new trace...

00:10:44.263 --> 00:10:46.417
I'm not going to do it live on the laptop

00:10:46.417 --> 00:10:49.954
because I want to go through--
we have a lot of stuff to cover.

00:10:49.954 --> 00:10:52.001
We now see right here.

00:10:52.001 --> 00:10:53.780
We have a lot more 
compute threads going on

00:10:53.780 --> 00:10:56.336
but we're still very much input-bound.

00:10:56.336 --> 00:10:58.712
And if we zoom in a lot,
you can actually see

00:10:58.712 --> 00:11:01.748
that the bottom element here,
this <i>.tf record</i>,

00:11:01.748 --> 00:11:05.316
we're waiting for data to load
from our file system.

00:11:05.316 --> 00:11:07.496
We then process things
in parallel really quickly,

00:11:07.496 --> 00:11:09.757
and then we take a while
to transfer them to the device

00:11:09.757 --> 00:11:11.265
over PCIE.

00:11:11.265 --> 00:11:13.930
And so this presents 
a pipelining opportunity.

00:11:14.779 --> 00:11:17.467
Now to give you a bit of intuition
for what I mean,

00:11:17.467 --> 00:11:21.977
input pipelines, you should always
mentally associate with ETL.

00:11:21.977 --> 00:11:25.341
Extract is the first phase
where you load the data from storage

00:11:25.341 --> 00:11:27.869
Transform phase is where 
you prepare it for training.

00:11:27.869 --> 00:11:29.997
And finally, you load it 
into the accelerator.

00:11:29.997 --> 00:11:32.194
And this is reflected
not just in the API

00:11:32.194 --> 00:11:36.206
but this is a very useful mental model
when you think about performance.

00:11:36.206 --> 00:11:38.373
Now to give you a bit 
of intuition for that,

00:11:39.317 --> 00:11:43.469
each of the different phases of ETL
use different hardware components

00:11:43.469 --> 00:11:45.331
in your server system.

00:11:45.331 --> 00:11:49.210
Your extract phase is exercising your disk
in your storage system

00:11:49.210 --> 00:11:53.258
or your network link if you're loading
from a remote storage system.

00:11:53.258 --> 00:11:56.266
Transform typically happens on the CPU

00:11:56.266 --> 00:11:58.021
and is very CPU-hungry.

00:11:58.021 --> 00:12:01.545
And your load phase is exercising
the DMA, the connections

00:12:01.545 --> 00:12:03.130
to your accelerator.

00:12:03.130 --> 00:12:06.508
This is true whether you're using
a GPU or a TPU,

00:12:06.508 --> 00:12:09.270
or potentially any other accelerators
that you might be using.

00:12:10.520 --> 00:12:13.956
And so what's going on is,
if you map this out over time,

00:12:13.956 --> 00:12:15.664
you're extracting,

00:12:15.664 --> 00:12:18.406
and while you're extracting,
you're doing nothing with the CPU.

00:12:18.406 --> 00:12:20.471
And during the transform phase,
you're doing nothing

00:12:20.471 --> 00:12:23.880
with the connection between
the CPU memory in your accelerator.

00:12:23.880 --> 00:12:26.060
And while you're training,
the entire CPU and the whole

00:12:26.060 --> 00:12:27.902
rest of the machine is just sitting idle.

00:12:27.902 --> 00:12:29.947
This is incredibly wasteful.

00:12:29.947 --> 00:12:33.442
Because they're all using 
different components in your system,

00:12:33.442 --> 00:12:35.364
you can actually overlap all of this

00:12:35.364 --> 00:12:37.951
in a technique called 
<i>software pipelining</i>.

00:12:37.951 --> 00:12:42.014
So with software pipelining,
you actually are extracting for step five

00:12:42.014 --> 00:12:44.991
while you're transforming for step four,
you're loading data for step three,

00:12:44.991 --> 00:12:46.547
and you're training for step two.

00:12:46.547 --> 00:12:48.620
And this results in a very efficient use

00:12:48.620 --> 00:12:52.609
of your compute resources;
you get to train a whole lot faster.

00:12:52.609 --> 00:12:56.595
Now you'll notice that
in a well-pipelined model,

00:12:56.595 --> 00:12:58.456
your accelerator will be 100% utilized

00:12:58.456 --> 00:13:00.831
but it's possible that your CPU,
or maybe even your disk,

00:13:00.831 --> 00:13:02.102
will be a little bit idle.

00:13:02.102 --> 00:13:03.093
And that's okay.

00:13:03.093 --> 00:13:05.727
Your accelerator is typically
your most precious resource

00:13:05.727 --> 00:13:07.742
and so that's what you want 
to be the bottleneck.

00:13:07.742 --> 00:13:09.315
It's okay if some
of the other resources

00:13:09.315 --> 00:13:11.796
are slightly faster 
than you need them to be.

00:13:13.002 --> 00:13:16.283
So how do you enable 
software pipelining with datasets?

00:13:16.283 --> 00:13:17.834
It's actually very easy.

00:13:18.530 --> 00:13:21.771
You set <i>num_parallel_reads</i> as equal to 32,
and underneath the hood,

00:13:21.771 --> 00:13:25.206
the <i>tf.data</i> is automatically
using parallel interleave

00:13:25.206 --> 00:13:27.968
which is a key dataset transformation

00:13:27.968 --> 00:13:30.320
that enables software pipelining.

00:13:30.320 --> 00:13:32.852
The other important change
to make to your input pipeline

00:13:32.852 --> 00:13:35.283
is if you set <i>prefetch</i> as equal to 2
right at the end,

00:13:35.283 --> 00:13:38.660
and this ensures that everything above
is pipelined with everything below,

00:13:38.660 --> 00:13:42.181
in particular your data transformations
and your extraction is pipelined

00:13:42.181 --> 00:13:44.277
from your loading into your accelerator.

00:13:45.737 --> 00:13:47.019
Now one thing I want to mention,

00:13:47.019 --> 00:13:49.532
when you set <i>num_parallel_reads</i>
as equal to 32,

00:13:49.532 --> 00:13:52.922
you're also using, well, parallel reads.

00:13:52.922 --> 00:13:54.183
And this actually--

00:13:54.183 --> 00:13:57.293
the reason why we've sort of
conflated these two in the API

00:13:57.293 --> 00:13:59.143
is because we believe
that distributed storage

00:13:59.143 --> 00:14:01.344
is going to be critical going forward

00:14:01.344 --> 00:14:02.603
for machine learning workloads.

00:14:02.603 --> 00:14:03.745
So why is that?

00:14:05.465 --> 00:14:07.634
As we have the research,
we see that datasets

00:14:07.634 --> 00:14:09.786
are going to become larger
and larger over time.

00:14:09.786 --> 00:14:12.933
And so you're really going to need--
They just won't fit on a single machine.

00:14:12.933 --> 00:14:16.003
You're going to need 
to distribute them across a cluster.

00:14:16.003 --> 00:14:18.773
Additionally, when you have data

00:14:18.773 --> 00:14:21.012
disaggregated from your accelerator nodes,

00:14:21.012 --> 00:14:23.679
it means you can more efficiently
share your accelerator nodes,

00:14:23.679 --> 00:14:27.213
so that if you're training on them today,

00:14:27.213 --> 00:14:30.664
and tomorrow, or maybe in three minutes,
someone else wants to train on,

00:14:30.664 --> 00:14:32.313
you're not copying datasets around.

00:14:32.313 --> 00:14:34.024
It's just easier to use.

00:14:34.024 --> 00:14:35.554
And finally, it makes it a lot nicer

00:14:35.554 --> 00:14:38.213
when you're doing
large-scale hyperparameter searches--

00:14:38.213 --> 00:14:41.013
if you have one high-performance cluster

00:14:41.013 --> 00:14:42.927
and a fungible pool
of accelerator resources.

00:14:42.927 --> 00:14:45.662
So we believe that distributed storage
is going to be very important,

00:14:45.662 --> 00:14:48.718
and we've worked very hard
to make that fast with <i>tf.data.</i>

00:14:49.886 --> 00:14:52.426
So what happens if you do this?

00:14:52.426 --> 00:14:54.547
Well, as it turns out, with a Cloud TPU,

00:14:54.547 --> 00:14:59.406
you'll get over 1,700 images a second
with these optimizations.

00:14:59.406 --> 00:15:02.969
So we're now about 12 times faster
than our initial input pipeline

00:15:02.969 --> 00:15:06.095
with actually less
than 60 characters worth of typing.

00:15:06.095 --> 00:15:07.622
So that's pretty good.

00:15:08.116 --> 00:15:09.734
But we can do better.

00:15:12.024 --> 00:15:13.711
If you capture the trace,

00:15:13.711 --> 00:15:17.213
you'll actually see that
our transform step is slightly longer

00:15:17.213 --> 00:15:19.836
than our accelerator training model time.

00:15:19.836 --> 00:15:21.892
The TPU is just too fast.

00:15:21.892 --> 00:15:25.139
And we need to break out
some advanced optimization techniques

00:15:25.139 --> 00:15:27.263
that are available today.

00:15:27.263 --> 00:15:30.287
One of the most powerful ones
is to use these fused dataset operators,

00:15:30.287 --> 00:15:32.152
map-and-batch, shuffle-and-repeat,

00:15:32.152 --> 00:15:34.365
that fuse together these operations

00:15:34.365 --> 00:15:36.857
to improve your performance on your CPU.

00:15:37.348 --> 00:15:39.559
<i>tf.data</i> works very, very hard

00:15:39.559 --> 00:15:44.044
to ensure that the elements produced
out of your dataset by your iterator

00:15:44.044 --> 00:15:45.797
are in a deterministic order.

00:15:45.797 --> 00:15:48.599
But if you give <i>tf.data </i>
permission to reorder,

00:15:48.599 --> 00:15:51.237
we can enable a number 
of additional performance optimizations.

00:15:51.237 --> 00:15:53.584
So we can use sloppy interleave
underneath the hood

00:15:53.584 --> 00:15:57.285
which can work around a lot of variability
in certain storage systems.

00:15:57.285 --> 00:15:59.270
There's other small tweaks 
that you can do

00:15:59.270 --> 00:16:00.830
but if you apply all of them together--

00:16:00.830 --> 00:16:02.977
In, for example,
this optimized input pipeline,

00:16:02.977 --> 00:16:05.352
we get well over 2,000 images a second,

00:16:05.352 --> 00:16:07.865
and we are now accelerator-bound.

00:16:07.865 --> 00:16:10.099
To see what that looks like,
here's <i>TensorBoard</i>

00:16:10.099 --> 00:16:12.325
and you can see that
everything is a 100% orange.

00:16:12.325 --> 00:16:15.450
We've entirely optimized away
our input pipeline.

00:16:15.450 --> 00:16:18.224
We're entirely accelerator-bound,
and the accelerator is just churning

00:16:18.224 --> 00:16:19.933
100% of the time.

00:16:19.933 --> 00:16:21.595
This is really great.

00:16:21.595 --> 00:16:24.599
This means that we now can
actually start looking into

00:16:24.599 --> 00:16:26.796
optimizations that we can do on the TPU

00:16:26.796 --> 00:16:28.915
to make the TPU faster,

00:16:28.915 --> 00:16:30.658
because we can see that our CPU is idle.

00:16:30.658 --> 00:16:34.940
And so we can see that there's
some overhead that's reshape and copy

00:16:34.940 --> 00:16:39.204
that we might think about optimizing away
with some device specific optimizations,

00:16:39.204 --> 00:16:40.915
which brings me to phase two.

00:16:44.295 --> 00:16:47.438
Now as I mentioned before,
we're in this sort of Cambrian explosion

00:16:47.438 --> 00:16:49.547
and while we're still in the early days,

00:16:49.547 --> 00:16:51.998
we're finding that a lot 
of these different accelerators

00:16:51.998 --> 00:16:53.703
behave very differently.

00:16:53.703 --> 00:16:55.268
Some chips are smaller,

00:16:55.268 --> 00:16:56.838
some chips are bigger,

00:16:56.838 --> 00:16:59.568
some chips use HBM2.

00:16:59.568 --> 00:17:01.896
So, for example, TPUs and GPUs,

00:17:01.896 --> 00:17:03.906
whereas some chips
do away with that entirely

00:17:03.906 --> 00:17:05.515
such as <i>Graphcore</i>'s IPU,

00:17:05.515 --> 00:17:08.647
and go entirely for SRAM
and optimize for communication.

00:17:08.647 --> 00:17:11.907
And so it's a little hard to provide
out-of-the-box performance recommendations

00:17:11.907 --> 00:17:15.067
that are going to apply to all 
of these different hardware platforms.

00:17:15.067 --> 00:17:18.553
That said, there are a few common things
that if we peer into the future,

00:17:18.553 --> 00:17:20.378
if we gaze into our crystal balls,

00:17:20.378 --> 00:17:22.782
we think this is
what we're going to see more of.

00:17:24.033 --> 00:17:26.045
One thing that I expect
we're going to see a lot of

00:17:26.045 --> 00:17:28.257
is interesting numerical formats.

00:17:28.257 --> 00:17:29.997
Now this is fp32.

00:17:29.997 --> 00:17:33.192
It's a 4-byte 32-bit 
floating point format

00:17:33.192 --> 00:17:34.935
that we know and love, IEEE.

00:17:34.935 --> 00:17:37.909
Most models today 
have been trained in fp32.

00:17:38.927 --> 00:17:43.051
There is some really great work
put in by the folks at <i>Nvidia</i> and <i>Baidu</i>.

00:17:43.051 --> 00:17:45.697
And they showed that,
with a little bit of clever tricks,

00:17:45.697 --> 00:17:47.477
you can train an fp16.

00:17:47.477 --> 00:17:51.298
Your waits stay in fp32,
but your activations are in fp16

00:17:51.298 --> 00:17:54.098
and this turns out to be 
a big win on two dimensions.

00:17:54.098 --> 00:17:56.279
Number one, you can run a larger model

00:17:56.279 --> 00:18:00.246
because more layers fits in memory.

00:18:00.246 --> 00:18:03.017
But additionally,
your model tends to run faster

00:18:03.017 --> 00:18:06.991
because accelerators today,
GPUs and actually even TPUs,

00:18:06.991 --> 00:18:08.696
are not compute-bound,

00:18:08.696 --> 00:18:10.487
they are actually memory bandwidth-bound.

00:18:10.487 --> 00:18:14.536
The high-bandwidth memory that we use
on the accelerators is still too slow.

00:18:16.257 --> 00:18:20.861
And so fp16 can unlock a lot
of great performance on devices.

00:18:21.700 --> 00:18:24.159
But I want to mention
one other floating point format

00:18:24.159 --> 00:18:26.709
that's available, and that's bfloat16.

00:18:26.709 --> 00:18:30.263
TPUs are designed--
have hardware support for bfloat16,

00:18:30.263 --> 00:18:34.451
and this actually is different than fp16--
even though it uses just 16 bits,

00:18:35.382 --> 00:18:38.878
the range is the same as fp32
and so you don't worry as much

00:18:38.878 --> 00:18:41.169
about vanishing gradients 
or exploding gradients

00:18:41.169 --> 00:18:44.781
and NaNs that you might
encounter if you use fp16.

00:18:44.781 --> 00:18:47.018
There are a number 
of additional floating point formats

00:18:47.018 --> 00:18:49.018
or numerical formats 
that just flex point.

00:18:49.018 --> 00:18:51.958
Some folks from <i>Intel</i> presented at NIPS
in a poster session,

00:18:51.958 --> 00:18:53.874
and I encourage you to check that out.

00:18:54.556 --> 00:18:57.627
But across all of this, we in <i>TensorFlow</i>,

00:18:57.627 --> 00:18:59.613
we're going to work hard
to make it easier to use

00:18:59.613 --> 00:19:01.872
all these different 
numerical floating point formats,

00:19:01.872 --> 00:19:05.202
so stay tuned for more APIs
and development in this space.

00:19:06.451 --> 00:19:08.715
One other hardware trend
that I think we're going to see

00:19:08.715 --> 00:19:10.469
is we're going to see 
hardware optimization

00:19:10.469 --> 00:19:14.077
for matrix multiplication,
especially in this mixed precision mode.

00:19:14.077 --> 00:19:17.789
So <i>Nvidia Volta</i> GPUs,
they have these tensor cores

00:19:17.789 --> 00:19:21.910
which are hardware-optimized
4x4 matrix multiplies.

00:19:21.910 --> 00:19:25.674
TPUs are built around 
a 128x128 matrix unit,

00:19:25.674 --> 00:19:27.023
and this is a systolic array.

00:19:27.023 --> 00:19:30.160
This special hardware
transistor configuration

00:19:30.160 --> 00:19:33.079
that makes it really efficient 
to compute multiplication

00:19:33.079 --> 00:19:35.244
and actually, as it turns out,
convolutions.

00:19:35.893 --> 00:19:39.310
Here's this little graphic illustrating
what a systolic array sort of does,

00:19:39.310 --> 00:19:43.351
and it sort of flows through in this--

00:19:43.351 --> 00:19:46.957
it's named after the heart
which is pumping blood in these cycles

00:19:46.957 --> 00:19:48.542
and it pumps through the data

00:19:48.542 --> 00:19:51.546
and you get really fast
matrix multiplication.

00:19:52.854 --> 00:19:57.090
What this means is because we're seeing
hardware-supported matrix multiplication

00:19:57.090 --> 00:19:59.117
at different sizes and scales,

00:19:59.117 --> 00:20:00.841
the way you layout your data,

00:20:00.841 --> 00:20:02.635
and the way your model is implemented

00:20:02.635 --> 00:20:06.059
can make a huge difference
on performance on these accelerators.

00:20:06.059 --> 00:20:10.757
Here I'm calling out two different models

00:20:10.757 --> 00:20:12.317
running on GPUs

00:20:12.317 --> 00:20:16.329
where if you use channels last on a GPU,

00:20:16.329 --> 00:20:18.674
you end up losing a fair bit
of performance

00:20:18.674 --> 00:20:21.370
compared to a channel's
first implementation.

00:20:22.341 --> 00:20:25.559
If you compare different 
LSTM cell implementations,

00:20:25.559 --> 00:20:27.570
the folks at <i>Nvidia</i>
worked really hard to make

00:20:27.570 --> 00:20:30.242
really fast kernels for LSTMs,

00:20:30.242 --> 00:20:32.692
and they make it available
as part of the cuDNN package.

00:20:32.692 --> 00:20:36.660
So if you're using a GPU
and you want to get a better performance,

00:20:36.660 --> 00:20:40.390
make sure you use 
the optimized libraries for your platform.

00:20:40.914 --> 00:20:41.981
Now this is true.

00:20:41.981 --> 00:20:43.906
You need to use the latest
version of <i>TensorFlow</i>.

00:20:43.906 --> 00:20:46.015
We're constantly working on
performance improvements.

00:20:46.015 --> 00:20:50.881
Latest version of cuDNN and <i>Intel</i> MKL
which we talked about earlier today.

00:20:50.881 --> 00:20:53.715
If you still want better performance,

00:20:53.715 --> 00:20:56.705
investigate your 
16-bit numerical representations.

00:20:56.705 --> 00:20:59.555
We a lot of potential
performance advantages there.

00:20:59.555 --> 00:21:02.826
And if you're doing inference, 
we've talked previously about <i>TensorRT</i>

00:21:02.826 --> 00:21:06.075
which is available for <i>Nvidia</i> platforms
that can help quantize

00:21:06.075 --> 00:21:08.281
and make inference really fast.

00:21:09.921 --> 00:21:12.632
As a really advanced technique,
if you see it that, for example,

00:21:12.632 --> 00:21:16.725
a particular computation is a bottleneck,
you can try and substitute it

00:21:16.725 --> 00:21:18.913
with something 
that's computationally faster.

00:21:18.913 --> 00:21:20.854
That said, you have to be very careful,

00:21:20.854 --> 00:21:23.340
because you may change
the quality of your model

00:21:23.340 --> 00:21:25.077
as part of doing this.

00:21:27.436 --> 00:21:29.821
With that, I'd like 
to move on to phase three.

00:21:31.064 --> 00:21:32.795
Now, typically when 
you use an accelerator,

00:21:32.795 --> 00:21:35.064
you're actually using more
than one accelerator.

00:21:35.064 --> 00:21:36.611
Even if you're using a single device,

00:21:36.611 --> 00:21:40.923
it may have different components
within it that operate in parallel.

00:21:40.923 --> 00:21:44.234
Here is for example a picture
of the <i>Nvidia DGX-1</i>

00:21:44.234 --> 00:21:47.804
and shows the connectivity
between the eight GPUs.

00:21:47.804 --> 00:21:50.845
You have two cliques of four GPUs each

00:21:50.845 --> 00:21:52.965
with connectivity between them.

00:21:52.965 --> 00:21:56.320
And as it turns out if you don't take
advantage of this topology,

00:21:56.320 --> 00:21:58.854
if you do, for example,
a naive gradient aggregation

00:21:58.854 --> 00:22:02.641
within the single server,
for example by going to the CPU

00:22:02.641 --> 00:22:05.047
or going via the PCIe switches,

00:22:05.047 --> 00:22:08.377
you will have a significant 
performance disadvantage

00:22:08.377 --> 00:22:14.624
compared to a clever utilization
of NVLink via NCCL 2

00:22:15.745 --> 00:22:18.445
Now we have an optimized
implementation available

00:22:18.445 --> 00:22:22.535
as part of the TFC and in benchmarks
but it's a little tricky to use

00:22:22.535 --> 00:22:26.830
and so we're going to be working on
making this easy for everyone to use

00:22:26.830 --> 00:22:28.195
in distribution strategies.

00:22:28.195 --> 00:22:30.692
And you'll hear a little bit more
about distribution strategies

00:22:30.692 --> 00:22:32.415
in just a few minutes.

00:22:33.616 --> 00:22:37.144
TPUs, you also need to carefully
aggregate your gradients

00:22:37.144 --> 00:22:39.337
and we have
the CrossShardOptimizer.

00:22:39.337 --> 00:22:41.937
So what you'll do is
you take your existing optimizer,

00:22:41.937 --> 00:22:45.414
a Momentum Stochastic
Gradient Descent (SGD),

00:22:45.414 --> 00:22:47.958
and you just wrap it with 
a TPU CrossShardOptimizer.

00:22:47.958 --> 00:22:52.383
This will take care of aggregating
across all the different compute shards

00:22:52.383 --> 00:22:54.582
within a single device,

00:22:54.582 --> 00:22:59.520
but the exact same code works
all the way up to a whole-cloud TPU pod

00:22:59.520 --> 00:23:01.741
across 64 different devices.

00:23:04.941 --> 00:23:07.773
Now I want to take one moment
to actually talk a little bit

00:23:07.773 --> 00:23:09.918
about measuring performance.

00:23:09.918 --> 00:23:13.642
I guess the saying goes,
"There's lies, damn lies and statistics."

00:23:13.642 --> 00:23:17.167
Well I'm going to add a fourth one;
"performance benchmarks".

00:23:17.167 --> 00:23:20.333
The internet is replete
with shoddy benchmarks

00:23:20.333 --> 00:23:23.762
and misinformation
and this irks me to no end.

00:23:24.962 --> 00:23:28.757
We've seen benchmarks
that use synthetic data,

00:23:28.757 --> 00:23:30.853
or they're measuring only certain subsets,

00:23:30.853 --> 00:23:34.697
and so you have incomplete comparisons.

00:23:34.697 --> 00:23:37.286
One benchmark is
comparing the full device,

00:23:37.286 --> 00:23:39.990
one is comparing only
one part of the device.

00:23:40.763 --> 00:23:42.357
We've seen bugs in the machine learning

00:23:42.357 --> 00:23:44.826
where they've optimized a way
or done performance tricks

00:23:44.826 --> 00:23:47.056
that make it run faster

00:23:47.056 --> 00:23:50.710
but actually make it not converge
to the same accuracy.

00:23:50.710 --> 00:23:53.746
You've lost quality of your model.

00:23:53.746 --> 00:23:56.826
Additionally, as we look forward, 
this is actually, to be fair,

00:23:56.826 --> 00:23:58.791
a very nuanced space.

00:23:58.791 --> 00:24:02.841
And hardware is becoming harder and harder
to give an apples-to-apples comparison

00:24:02.841 --> 00:24:04.617
because we have
different numerical formats

00:24:04.617 --> 00:24:08.175
and different algorithms fit better 
on different classes of hardware.

00:24:08.175 --> 00:24:11.802
Some chips, for example the IPU,
have very small amounts of memory.

00:24:11.802 --> 00:24:15.833
And so if you have a very, 
very big model that just can't fit,

00:24:15.833 --> 00:24:18.366
that's a very unfair comparison.

00:24:18.366 --> 00:24:20.513
As a result, I strongly encourage you

00:24:20.513 --> 00:24:23.883
if you're trying to choose
and evaluate different hardware platforms,

00:24:23.883 --> 00:24:27.681
take your workloads 
and measure them end to end

00:24:27.681 --> 00:24:30.386
to the accuracy that you want
in your application.

00:24:31.616 --> 00:24:34.139
That said, that's a fair amount of work,

00:24:34.139 --> 00:24:36.501
so if you can't run your own workloads,

00:24:36.501 --> 00:24:40.667
look to quality end-to-end benchmarks
that measure time to accuracy.

00:24:40.667 --> 00:24:44.807
And I think the best example
of this today is Stanford's <i>DAWNBench</i>.

00:24:44.807 --> 00:24:48.554
Now while there's nuance in the parameters
of how the benchmarking are set,

00:24:48.554 --> 00:24:50.353
for example the data set size--

00:24:50.353 --> 00:24:52.547
are you training on a smaller
data set size that fits in RAM

00:24:52.547 --> 00:24:55.478
or large dataset size that you've got
to load in over the network?

00:24:55.478 --> 00:24:58.760
And there's nuance on how
you set the accuracy threshold.

00:24:58.760 --> 00:25:01.296
It's a lot harder--
Despite these, it's a lot harder

00:25:01.296 --> 00:25:04.409
for a system to perform well
on an end-to-end benchmark

00:25:04.409 --> 00:25:06.278
but then not be useful for real work.

00:25:06.278 --> 00:25:08.496
You're less likely to be
misled when you look

00:25:08.496 --> 00:25:10.519
at these end-to-end benchmarks.

00:25:11.446 --> 00:25:15.226
That said, while we're pushing very hard
for these end-to-end benchmarks,

00:25:15.226 --> 00:25:18.645
there's actually a lot of utility
in what we call micro-benchmarks

00:25:18.645 --> 00:25:21.886
to try and understand how fast
are these different components.

00:25:21.886 --> 00:25:25.093
When I was preparing these slides
and optimizing <i>ResNet-50</i>

00:25:25.093 --> 00:25:27.972
for the Cloud TPU case,

00:25:27.972 --> 00:25:31.860
how would I know that 150 images
a second was way too slow?

00:25:31.860 --> 00:25:34.710
We can look to micro-benchmarks
to give ourselves calibration.

00:25:34.710 --> 00:25:38.434
So as Derek mentioned before,
input pipelines on a <i>DGX-1</i>

00:25:38.434 --> 00:25:44.020
can have over 1,300 images a second,
and that's using VGG pre-processing.

00:25:44.020 --> 00:25:46.970
This pre-processing is a little bit
more computationally cheaper

00:25:46.970 --> 00:25:49.538
than the <i>ResNet</i>
or <i>Inception</i> pre-processing

00:25:49.538 --> 00:25:51.739
but it shows that we
can go really, really fast.

00:25:52.770 --> 00:25:56.732
<i>ResNet-50</i> on a <i>DGX-1</i> with real data

00:25:56.732 --> 00:25:59.816
is about 5.8 thousand images a second.

00:25:59.816 --> 00:26:02.687
This is using a
<i>Mixed-Precision IEEE float 16.</i>

00:26:02.687 --> 00:26:06.813
This is based on TFCNN benchmarks
available with <i>TensorFlow Nightly.</i>

00:26:06.813 --> 00:26:09.855
So this is the performance you can
expect coming forward in the future.

00:26:09.855 --> 00:26:12.711
Now if you just want to test
the performance of the GPUs themselves

00:26:12.711 --> 00:26:16.098
in isolation, there's about 
6.1 synthetic images a second.

00:26:16.098 --> 00:26:19.811
So you basically are excluding
the cost of your input pipeline.

00:26:21.951 --> 00:26:25.401
For a <i>Cloud TPU</i> we have
a few other micro-benchmarks.

00:26:25.401 --> 00:26:28.048
For <i>TensorFlow 1.7</i>
that's available today,

00:26:28.048 --> 00:26:33.107
you can expect to achieve 
2.65 thousand images a second

00:26:33.107 --> 00:26:34.127
on a Cloud TPU.

00:26:34.127 --> 00:26:36.807
And that's using 
<i>Mixed-Precision B float 16</i>

00:26:36.807 --> 00:26:38.797
and <i>FP32.</i>

00:26:38.797 --> 00:26:41.816
You're streaming your data in
from <i>GCS</i> with a batch size of 1024

00:26:41.816 --> 00:26:46.452
and you can get to 76%,
over 76% accuracy,

00:26:46.452 --> 00:26:47.945
in about 13 hours.

00:26:48.992 --> 00:26:51.800
If you lop off the input pipeline
and just test the device performance,

00:26:51.800 --> 00:26:55.123
you're actually over 3,200
images a second which is very cool

00:26:55.123 --> 00:26:58.703
with <i>TensorFlow 1.7</i>
and with <i>TensorFlow Nightly</i>,

00:26:58.703 --> 00:27:00.810
so this will be coming in <i>TensorFlow 1.8</i>.

00:27:00.810 --> 00:27:03.774
We've done a huge amount of work
to optimize the input pipeline performance

00:27:03.774 --> 00:27:07.846
and you go from 2,600 images a second

00:27:07.846 --> 00:27:11.103
to over 3,000 images a second
on a single <i>Cloud TPU</i>

00:27:11.103 --> 00:27:13.283
and this is coming in <i>TensorFlow 1.8</i>.

00:27:13.283 --> 00:27:14.537
So very exciting.

00:27:14.537 --> 00:27:17.040
A lot of work happening 
underneath the hood.

00:27:17.967 --> 00:27:20.392
As we stare into the future even further

00:27:20.392 --> 00:27:24.121
what is coming in
<i>TensorFlow</i> with performance?

00:27:24.971 --> 00:27:28.066
This is actually our
optimized input pipeline

00:27:28.066 --> 00:27:29.377
or something very close to it.

00:27:29.377 --> 00:27:32.029
You'll notice that there's sort of
a lot of magic numbers in there

00:27:32.029 --> 00:27:34.615
that we just hand-tuned and picked out.

00:27:34.615 --> 00:27:36.184
How do we choose them?

00:27:36.184 --> 00:27:38.164
We spent a lot of time 
playing around with it

00:27:38.164 --> 00:27:39.623
and picking them out.

00:27:39.623 --> 00:27:40.899
Do you need to do that?

00:27:40.899 --> 00:27:43.014
There's absolutely no reason
why you need to do that.

00:27:43.014 --> 00:27:45.024
We can actually auto-tune
a lot of these values

00:27:45.024 --> 00:27:47.764
and we're going to be working on
adding in smarts into <i>TensorFlow</i>

00:27:47.764 --> 00:27:49.328
to just tune your pipelines for you.

00:27:49.328 --> 00:27:52.036
And this is true not just
of these magic numbers

00:27:52.694 --> 00:27:55.094
but actually of data set transformations,

00:27:55.094 --> 00:27:58.715
these fused data set
transformation operations.

00:27:58.715 --> 00:28:01.155
We'll be working
on automatically switching

00:28:01.155 --> 00:28:03.191
a naive straightforward implementation

00:28:03.191 --> 00:28:05.508
to use the fused operations
underneath the hood.

00:28:05.508 --> 00:28:08.304
And if you give us permission,
we'll be able to adjust things

00:28:08.304 --> 00:28:11.121
where we're not preserving
necessarily the order

00:28:11.121 --> 00:28:13.643
but we're able to do
the right things for you.

00:28:14.048 --> 00:28:17.185
Now one thing I'd like to mention
is that automatically tuning

00:28:17.185 --> 00:28:20.055
the prefetch buffer size,
so that last line,

00:28:20.055 --> 00:28:22.715
that's available 
and coming in <i>TensorFlow 1.8</i>.

00:28:25.195 --> 00:28:27.574
We're not just working on
optimizing the input pipelines,

00:28:27.574 --> 00:28:29.940
we're working on optimizing
on-device performance.

00:28:29.940 --> 00:28:31.968
There's two frameworks
within <i>TensorFlow</i>--

00:28:31.968 --> 00:28:34.696
there's <i>XLA</i> and there's <i>Grappler</i>.

00:28:34.696 --> 00:28:37.154
They are automatically doing
graph transformations,

00:28:37.154 --> 00:28:41.130
picking optimal image layouts,
for example, or re-writing your model

00:28:41.130 --> 00:28:43.344
to work well on different
hardware platforms.

00:28:43.344 --> 00:28:46.306
And so there's a lot of exciting work
going on here that we'll be excited

00:28:46.306 --> 00:28:48.194
to share with you over time.

00:28:48.823 --> 00:28:50.006
There's a lot more reading,

00:28:50.006 --> 00:28:52.313
and there's a huge amount
of literature on this,

00:28:52.313 --> 00:28:54.727
so I encourage you to check out
some of these things.

00:28:54.727 --> 00:28:56.477
If you would like to learn
a little bit more

00:28:56.477 --> 00:28:59.478
about reduced precision training,
here's some useful references.

00:28:59.478 --> 00:29:02.780
And actually the traces that I had
screenshots of in this talk,

00:29:02.780 --> 00:29:05.301
I'll tweet out this link shortly,
but they'll be available

00:29:05.301 --> 00:29:07.421
so you can download and load
them up into <i>TensorBoard</i>

00:29:07.421 --> 00:29:08.854
and play around with them yourself.

00:29:08.854 --> 00:29:11.719
With that, thank you very much
for listening to me today.

00:29:11.983 --> 00:29:14.013
(applause)

00:29:14.143 --> 00:29:16.727
♪ (music) ♪

