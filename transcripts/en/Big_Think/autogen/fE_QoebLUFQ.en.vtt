WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:04.880
 
[Music]

00:00:04.880 --> 00:00:04.890
[Music]
 

00:00:04.890 --> 00:00:08.600
[Music]
we live in a world now where there is an

00:00:08.600 --> 00:00:08.610
we live in a world now where there is an
 

00:00:08.610 --> 00:00:11.650
we live in a world now where there is an
economic model that strongly

00:00:11.650 --> 00:00:11.660
economic model that strongly
 

00:00:11.660 --> 00:00:14.869
economic model that strongly
incentivizes online platforms like

00:00:14.869 --> 00:00:14.879
incentivizes online platforms like
 

00:00:14.879 --> 00:00:19.190
incentivizes online platforms like
Facebook Google Twitter to capture as

00:00:19.190 --> 00:00:19.200
Facebook Google Twitter to capture as
 

00:00:19.200 --> 00:00:22.190
Facebook Google Twitter to capture as
much of our attention as possible the

00:00:22.190 --> 00:00:22.200
much of our attention as possible the
 

00:00:22.200 --> 00:00:25.010
much of our attention as possible the
way to do that is is to promote content

00:00:25.010 --> 00:00:25.020
way to do that is is to promote content
 

00:00:25.020 --> 00:00:28.130
way to do that is is to promote content
that is the most engaging and what is

00:00:28.130 --> 00:00:28.140
that is the most engaging and what is
 

00:00:28.140 --> 00:00:30.920
that is the most engaging and what is
the most engaging moral content there

00:00:30.920 --> 00:00:30.930
the most engaging moral content there
 

00:00:30.930 --> 00:00:34.060
the most engaging moral content there
was a recent study that came out of NYU

00:00:34.060 --> 00:00:34.070
was a recent study that came out of NYU
 

00:00:34.070 --> 00:00:39.230
was a recent study that came out of NYU
recently that characterized the language

00:00:39.230 --> 00:00:39.240
recently that characterized the language
 

00:00:39.240 --> 00:00:42.170
recently that characterized the language
in tweets and this study which was led

00:00:42.170 --> 00:00:42.180
in tweets and this study which was led
 

00:00:42.180 --> 00:00:44.990
in tweets and this study which was led
by William Brady and Jay Ben Babel and

00:00:44.990 --> 00:00:45.000
by William Brady and Jay Ben Babel and
 

00:00:45.000 --> 00:00:48.979
by William Brady and Jay Ben Babel and
colleagues found that each moral

00:00:48.979 --> 00:00:48.989
colleagues found that each moral
 

00:00:48.989 --> 00:00:52.400
colleagues found that each moral
emotional word in a tweet increase the

00:00:52.400 --> 00:00:52.410
emotional word in a tweet increase the
 

00:00:52.410 --> 00:00:56.770
emotional word in a tweet increase the
likelihood of a retweet by 20% so

00:00:56.770 --> 00:00:56.780
likelihood of a retweet by 20% so
 

00:00:56.780 --> 00:01:01.299
likelihood of a retweet by 20% so
content that has moral and emotional

00:01:01.299 --> 00:01:01.309
content that has moral and emotional
 

00:01:01.309 --> 00:01:03.950
content that has moral and emotional
qualities to it of which moral outrage

00:01:03.950 --> 00:01:03.960
qualities to it of which moral outrage
 

00:01:03.960 --> 00:01:08.860
qualities to it of which moral outrage
is the poster child is the most engaging

00:01:08.860 --> 00:01:08.870
is the poster child is the most engaging
 

00:01:08.870 --> 00:01:12.230
is the poster child is the most engaging
content and so that means that the

00:01:12.230 --> 00:01:12.240
content and so that means that the
 

00:01:12.240 --> 00:01:15.530
content and so that means that the
algorithms that select for what is shown

00:01:15.530 --> 00:01:15.540
algorithms that select for what is shown
 

00:01:15.540 --> 00:01:17.860
algorithms that select for what is shown
to all of us in our news feeds are

00:01:17.860 --> 00:01:17.870
to all of us in our news feeds are
 

00:01:17.870 --> 00:01:20.210
to all of us in our news feeds are
selecting for the content that's going

00:01:20.210 --> 00:01:20.220
selecting for the content that's going
 

00:01:20.220 --> 00:01:21.770
selecting for the content that's going
to be the most engaging because that

00:01:21.770 --> 00:01:21.780
to be the most engaging because that
 

00:01:21.780 --> 00:01:23.480
to be the most engaging because that
draws the most attention because that

00:01:23.480 --> 00:01:23.490
draws the most attention because that
 

00:01:23.490 --> 00:01:26.480
draws the most attention because that
creates the most revenue through ad

00:01:26.480 --> 00:01:26.490
creates the most revenue through ad
 

00:01:26.490 --> 00:01:29.390
creates the most revenue through ad
sales for these companies and so this

00:01:29.390 --> 00:01:29.400
sales for these companies and so this
 

00:01:29.400 --> 00:01:32.450
sales for these companies and so this
creates an information ecosystem where

00:01:32.450 --> 00:01:32.460
creates an information ecosystem where
 

00:01:32.460 --> 00:01:33.859
creates an information ecosystem where
there's a kind of natural selection

00:01:33.859 --> 00:01:33.869
there's a kind of natural selection
 

00:01:33.869 --> 00:01:36.859
there's a kind of natural selection
process going on and the most outrageous

00:01:36.859 --> 00:01:36.869
process going on and the most outrageous
 

00:01:36.869 --> 00:01:40.399
process going on and the most outrageous
content is going to rise to the top so

00:01:40.399 --> 00:01:40.409
content is going to rise to the top so
 

00:01:40.409 --> 00:01:43.820
content is going to rise to the top so
this suggests that the kinds of stories

00:01:43.820 --> 00:01:43.830
this suggests that the kinds of stories
 

00:01:43.830 --> 00:01:46.450
this suggests that the kinds of stories
that we read in our news feeds on line

00:01:46.450 --> 00:01:46.460
that we read in our news feeds on line
 

00:01:46.460 --> 00:01:49.940
that we read in our news feeds on line
might be artificially inflated in terms

00:01:49.940 --> 00:01:49.950
might be artificially inflated in terms
 

00:01:49.950 --> 00:01:52.520
might be artificially inflated in terms
of how much outrage they provoke and

00:01:52.520 --> 00:01:52.530
of how much outrage they provoke and
 

00:01:52.530 --> 00:01:54.679
of how much outrage they provoke and
I've actually found some data that

00:01:54.679 --> 00:01:54.689
I've actually found some data that
 

00:01:54.689 --> 00:01:57.260
I've actually found some data that
speaks to this so there was a study a

00:01:57.260 --> 00:01:57.270
speaks to this so there was a study a
 

00:01:57.270 --> 00:02:00.140
speaks to this so there was a study a
few years ago by wil Hoffman Linda's

00:02:00.140 --> 00:02:00.150
few years ago by wil Hoffman Linda's
 

00:02:00.150 --> 00:02:01.520
few years ago by wil Hoffman Linda's
Kitka and colleagues at the University

00:02:01.520 --> 00:02:01.530
Kitka and colleagues at the University
 

00:02:01.530 --> 00:02:04.280
Kitka and colleagues at the University
of Chicago where they tracked people's

00:02:04.280 --> 00:02:04.290
of Chicago where they tracked people's
 

00:02:04.290 --> 00:02:07.839
of Chicago where they tracked people's
daily experiences with moral and immoral

00:02:07.839 --> 00:02:07.849
daily experiences with moral and immoral
 

00:02:07.849 --> 00:02:11.210
daily experiences with moral and immoral
events in their everyday lives and they

00:02:11.210 --> 00:02:11.220
events in their everyday lives and they
 

00:02:11.220 --> 00:02:12.830
events in their everyday lives and they
pinged people's smartphones a few times

00:02:12.830 --> 00:02:12.840
pinged people's smartphones a few times
 

00:02:12.840 --> 00:02:15.740
pinged people's smartphones a few times
a day and had them rate whether in the

00:02:15.740 --> 00:02:15.750
a day and had them rate whether in the
 

00:02:15.750 --> 00:02:17.480
a day and had them rate whether in the
past hour they had had

00:02:17.480 --> 00:02:17.490
past hour they had had
 

00:02:17.490 --> 00:02:20.630
past hour they had had
the moral or immoral experiences and

00:02:20.630 --> 00:02:20.640
the moral or immoral experiences and
 

00:02:20.640 --> 00:02:22.490
the moral or immoral experiences and
they had people rate how emotional they

00:02:22.490 --> 00:02:22.500
they had people rate how emotional they
 

00:02:22.500 --> 00:02:25.430
they had people rate how emotional they
felt how outraged they felt how happy

00:02:25.430 --> 00:02:25.440
felt how outraged they felt how happy
 

00:02:25.440 --> 00:02:29.330
felt how outraged they felt how happy
and so on this data became publicly

00:02:29.330 --> 00:02:29.340
and so on this data became publicly
 

00:02:29.340 --> 00:02:31.760
and so on this data became publicly
available and so I was able to reanalyze

00:02:31.760 --> 00:02:31.770
available and so I was able to reanalyze
 

00:02:31.770 --> 00:02:33.590
available and so I was able to reanalyze
the data because these researchers had

00:02:33.590 --> 00:02:33.600
the data because these researchers had
 

00:02:33.600 --> 00:02:35.270
the data because these researchers had
asked them where did you learn about

00:02:35.270 --> 00:02:35.280
asked them where did you learn about
 

00:02:35.280 --> 00:02:39.200
asked them where did you learn about
these immoral events online in person on

00:02:39.200 --> 00:02:39.210
these immoral events online in person on
 

00:02:39.210 --> 00:02:43.730
these immoral events online in person on
TV radio newspaper etc and so I was able

00:02:43.730 --> 00:02:43.740
TV radio newspaper etc and so I was able
 

00:02:43.740 --> 00:02:46.220
TV radio newspaper etc and so I was able
to analyze this data and show that

00:02:46.220 --> 00:02:46.230
to analyze this data and show that
 

00:02:46.230 --> 00:02:48.230
to analyze this data and show that
immoral events that people learn about

00:02:48.230 --> 00:02:48.240
immoral events that people learn about
 

00:02:48.240 --> 00:02:52.060
immoral events that people learn about
online trigger more outraged than

00:02:52.060 --> 00:02:52.070
online trigger more outraged than
 

00:02:52.070 --> 00:02:54.290
online trigger more outraged than
immoral events that they learn about in

00:02:54.290 --> 00:02:54.300
immoral events that they learn about in
 

00:02:54.300 --> 00:02:56.360
immoral events that they learn about in
person or through traditional forms of

00:02:56.360 --> 00:02:56.370
person or through traditional forms of
 

00:02:56.370 --> 00:02:59.630
person or through traditional forms of
media like TV newspaper and radio so

00:02:59.630 --> 00:02:59.640
media like TV newspaper and radio so
 

00:02:59.640 --> 00:03:02.000
media like TV newspaper and radio so
this supports the idea that the

00:03:02.000 --> 00:03:02.010
this supports the idea that the
 

00:03:02.010 --> 00:03:05.420
this supports the idea that the
algorithms that drive the presentation

00:03:05.420 --> 00:03:05.430
algorithms that drive the presentation
 

00:03:05.430 --> 00:03:08.270
algorithms that drive the presentation
of news content online are selecting

00:03:08.270 --> 00:03:08.280
of news content online are selecting
 

00:03:08.280 --> 00:03:11.330
of news content online are selecting
that content that provokes perhaps

00:03:11.330 --> 00:03:11.340
that content that provokes perhaps
 

00:03:11.340 --> 00:03:14.000
that content that provokes perhaps
higher levels of outrage then we even

00:03:14.000 --> 00:03:14.010
higher levels of outrage then we even
 

00:03:14.010 --> 00:03:16.100
higher levels of outrage then we even
see on the news and of course what we

00:03:16.100 --> 00:03:16.110
see on the news and of course what we
 

00:03:16.110 --> 00:03:19.640
see on the news and of course what we
see normally in our daily lives it's an

00:03:19.640 --> 00:03:19.650
see normally in our daily lives it's an
 

00:03:19.650 --> 00:03:21.710
see normally in our daily lives it's an
open question what are the long-term

00:03:21.710 --> 00:03:21.720
open question what are the long-term
 

00:03:21.720 --> 00:03:25.160
open question what are the long-term
consequences of this constant exposure

00:03:25.160 --> 00:03:25.170
consequences of this constant exposure
 

00:03:25.170 --> 00:03:29.360
consequences of this constant exposure
to outrage triggering material one

00:03:29.360 --> 00:03:29.370
to outrage triggering material one
 

00:03:29.370 --> 00:03:32.030
to outrage triggering material one
possibility that has been floated in the

00:03:32.030 --> 00:03:32.040
possibility that has been floated in the
 

00:03:32.040 --> 00:03:35.150
possibility that has been floated in the
news recently is outrage fatigue and I

00:03:35.150 --> 00:03:35.160
news recently is outrage fatigue and I
 

00:03:35.160 --> 00:03:38.060
news recently is outrage fatigue and I
think many of us can relate to the idea

00:03:38.060 --> 00:03:38.070
think many of us can relate to the idea
 

00:03:38.070 --> 00:03:40.730
think many of us can relate to the idea
that if you're constantly feeling

00:03:40.730 --> 00:03:40.740
that if you're constantly feeling
 

00:03:40.740 --> 00:03:44.690
that if you're constantly feeling
outraged and it's exhausting and there

00:03:44.690 --> 00:03:44.700
outraged and it's exhausting and there
 

00:03:44.700 --> 00:03:46.970
outraged and it's exhausting and there
may be a limit to how much outrage were

00:03:46.970 --> 00:03:46.980
may be a limit to how much outrage were
 

00:03:46.980 --> 00:03:52.570
may be a limit to how much outrage were
able to experience day to day that is

00:03:52.570 --> 00:03:52.580
able to experience day to day that is
 

00:03:52.580 --> 00:03:55.940
able to experience day to day that is
potentially harmful in terms of the long

00:03:55.940 --> 00:03:55.950
potentially harmful in terms of the long
 

00:03:55.950 --> 00:03:59.690
potentially harmful in terms of the long
term social consequences because if we

00:03:59.690 --> 00:03:59.700
term social consequences because if we
 

00:03:59.700 --> 00:04:03.050
term social consequences because if we
are feeling outraged about relatively

00:04:03.050 --> 00:04:03.060
are feeling outraged about relatively
 

00:04:03.060 --> 00:04:05.480
are feeling outraged about relatively
minor things and that's depleting some

00:04:05.480 --> 00:04:05.490
minor things and that's depleting some
 

00:04:05.490 --> 00:04:08.690
minor things and that's depleting some
kind of reserve that may mean that we're

00:04:08.690 --> 00:04:08.700
kind of reserve that may mean that we're
 

00:04:08.700 --> 00:04:11.300
kind of reserve that may mean that we're
not able to feel outraged for things

00:04:11.300 --> 00:04:11.310
not able to feel outraged for things
 

00:04:11.310 --> 00:04:13.970
not able to feel outraged for things
that really matter on the other hand

00:04:13.970 --> 00:04:13.980
that really matter on the other hand
 

00:04:13.980 --> 00:04:17.320
that really matter on the other hand
there is also research in aggression

00:04:17.320 --> 00:04:17.330
there is also research in aggression
 

00:04:17.330 --> 00:04:20.180
there is also research in aggression
showing that if you give people the

00:04:20.180 --> 00:04:20.190
showing that if you give people the
 

00:04:20.190 --> 00:04:23.360
showing that if you give people the
opportunity to vent their aggressive

00:04:23.360 --> 00:04:23.370
opportunity to vent their aggressive
 

00:04:23.370 --> 00:04:24.800
opportunity to vent their aggressive
feelings about something that's made

00:04:24.800 --> 00:04:24.810
feelings about something that's made
 

00:04:24.810 --> 00:04:27.680
feelings about something that's made
them mad that actually can increase the

00:04:27.680 --> 00:04:27.690
them mad that actually can increase the
 

00:04:27.690 --> 00:04:31.010
them mad that actually can increase the
likelihood of future aggression so

00:04:31.010 --> 00:04:31.020
likelihood of future aggression so
 

00:04:31.020 --> 00:04:35.650
likelihood of future aggression so
in the literature on anger and outrage

00:04:35.650 --> 00:04:35.660
in the literature on anger and outrage
 

00:04:35.660 --> 00:04:39.020
in the literature on anger and outrage
there are two possibilities one being

00:04:39.020 --> 00:04:39.030
there are two possibilities one being
 

00:04:39.030 --> 00:04:41.480
there are two possibilities one being
this long term depletion outreach

00:04:41.480 --> 00:04:41.490
this long term depletion outreach
 

00:04:41.490 --> 00:04:45.460
this long term depletion outreach
fatigue the other being a kind of

00:04:45.460 --> 00:04:45.470
fatigue the other being a kind of
 

00:04:45.470 --> 00:04:48.740
fatigue the other being a kind of
sensitization and we need to do more

00:04:48.740 --> 00:04:48.750
sensitization and we need to do more
 

00:04:48.750 --> 00:04:51.320
sensitization and we need to do more
research to to figure out which of those

00:04:51.320 --> 00:04:51.330
research to to figure out which of those
 

00:04:51.330 --> 00:04:53.990
research to to figure out which of those
might be operating in the context of

00:04:53.990 --> 00:04:54.000
might be operating in the context of
 

00:04:54.000 --> 00:04:57.830
might be operating in the context of
online outrage expression it may be

00:04:57.830 --> 00:04:57.840
online outrage expression it may be
 

00:04:57.840 --> 00:05:00.110
online outrage expression it may be
different for different people social

00:05:00.110 --> 00:05:00.120
different for different people social
 

00:05:00.120 --> 00:05:02.540
different for different people social
media is very unlikely to go away

00:05:02.540 --> 00:05:02.550
media is very unlikely to go away
 

00:05:02.550 --> 00:05:06.170
media is very unlikely to go away
because it taps into the things that we

00:05:06.170 --> 00:05:06.180
because it taps into the things that we
 

00:05:06.180 --> 00:05:09.140
because it taps into the things that we
find most rewarding connection with

00:05:09.140 --> 00:05:09.150
find most rewarding connection with
 

00:05:09.150 --> 00:05:12.170
find most rewarding connection with
others expressing our moral values

00:05:12.170 --> 00:05:12.180
others expressing our moral values
 

00:05:12.180 --> 00:05:14.480
others expressing our moral values
sharing those moral values with others

00:05:14.480 --> 00:05:14.490
sharing those moral values with others
 

00:05:14.490 --> 00:05:17.420
sharing those moral values with others
building our reputation and of course

00:05:17.420 --> 00:05:17.430
building our reputation and of course
 

00:05:17.430 --> 00:05:19.850
building our reputation and of course
what makes social media so compelling

00:05:19.850 --> 00:05:19.860
what makes social media so compelling
 

00:05:19.860 --> 00:05:25.460
what makes social media so compelling
and so so addictive even is the fact

00:05:25.460 --> 00:05:25.470
and so so addictive even is the fact
 

00:05:25.470 --> 00:05:29.990
and so so addictive even is the fact
that these platforms are really tapping

00:05:29.990 --> 00:05:30.000
that these platforms are really tapping
 

00:05:30.000 --> 00:05:33.430
that these platforms are really tapping
into very ancient neural circuitry that

00:05:33.430 --> 00:05:33.440
into very ancient neural circuitry that
 

00:05:33.440 --> 00:05:38.120
into very ancient neural circuitry that
that we know are involved in reward

00:05:38.120 --> 00:05:38.130
that we know are involved in reward
 

00:05:38.130 --> 00:05:42.020
that we know are involved in reward
processing in habit formation one

00:05:42.020 --> 00:05:42.030
processing in habit formation one
 

00:05:42.030 --> 00:05:44.510
processing in habit formation one
intriguing possibility because the way

00:05:44.510 --> 00:05:44.520
intriguing possibility because the way
 

00:05:44.520 --> 00:05:45.980
intriguing possibility because the way
these apps are designed are so

00:05:45.980 --> 00:05:45.990
these apps are designed are so
 

00:05:45.990 --> 00:05:49.910
these apps are designed are so
streamlined you have stimuli icons that

00:05:49.910 --> 00:05:49.920
streamlined you have stimuli icons that
 

00:05:49.920 --> 00:05:53.420
streamlined you have stimuli icons that
are that are so recognizable and

00:05:53.420 --> 00:05:53.430
are that are so recognizable and
 

00:05:53.430 --> 00:05:55.910
are that are so recognizable and
familiar to all of us who use these apps

00:05:55.910 --> 00:05:55.920
familiar to all of us who use these apps
 

00:05:55.920 --> 00:06:00.830
familiar to all of us who use these apps
and very effortless responses to like to

00:06:00.830 --> 00:06:00.840
and very effortless responses to like to
 

00:06:00.840 --> 00:06:03.740
and very effortless responses to like to
share to retweet and then we get

00:06:03.740 --> 00:06:03.750
share to retweet and then we get
 

00:06:03.750 --> 00:06:06.260
share to retweet and then we get
feedback and that feedback in the form

00:06:06.260 --> 00:06:06.270
feedback and that feedback in the form
 

00:06:06.270 --> 00:06:09.440
feedback and that feedback in the form
of likes and shares is delivered at

00:06:09.440 --> 00:06:09.450
of likes and shares is delivered at
 

00:06:09.450 --> 00:06:12.740
of likes and shares is delivered at
unpredictable times and unpredictable

00:06:12.740 --> 00:06:12.750
unpredictable times and unpredictable
 

00:06:12.750 --> 00:06:15.440
unpredictable times and unpredictable
rewards we know from decades of research

00:06:15.440 --> 00:06:15.450
rewards we know from decades of research
 

00:06:15.450 --> 00:06:18.260
rewards we know from decades of research
in neuroscience are the fastest way to

00:06:18.260 --> 00:06:18.270
in neuroscience are the fastest way to
 

00:06:18.270 --> 00:06:21.320
in neuroscience are the fastest way to
establish a habit now habit is a

00:06:21.320 --> 00:06:21.330
establish a habit now habit is a
 

00:06:21.330 --> 00:06:23.600
establish a habit now habit is a
behavior that is expressed without

00:06:23.600 --> 00:06:23.610
behavior that is expressed without
 

00:06:23.610 --> 00:06:25.970
behavior that is expressed without
regard to its long-term consequences

00:06:25.970 --> 00:06:25.980
regard to its long-term consequences
 

00:06:25.980 --> 00:06:28.210
regard to its long-term consequences
just as someone who's habitually

00:06:28.210 --> 00:06:28.220
just as someone who's habitually
 

00:06:28.220 --> 00:06:30.260
just as someone who's habitually
reaching for the bag of potato chips

00:06:30.260 --> 00:06:30.270
reaching for the bag of potato chips
 

00:06:30.270 --> 00:06:32.270
reaching for the bag of potato chips
when they're not hungry they're eating

00:06:32.270 --> 00:06:32.280
when they're not hungry they're eating
 

00:06:32.280 --> 00:06:34.130
when they're not hungry they're eating
those potato chips not to achieve some

00:06:34.130 --> 00:06:34.140
those potato chips not to achieve some
 

00:06:34.140 --> 00:06:36.320
those potato chips not to achieve some
goal to satisfy their hunger but just

00:06:36.320 --> 00:06:36.330
goal to satisfy their hunger but just
 

00:06:36.330 --> 00:06:39.130
goal to satisfy their hunger but just
mindlessly we might be mindlessly

00:06:39.130 --> 00:06:39.140
mindlessly we might be mindlessly
 

00:06:39.140 --> 00:06:41.740
mindlessly we might be mindlessly
expressing moral emotions like outrage

00:06:41.740 --> 00:06:41.750
expressing moral emotions like outrage
 

00:06:41.750 --> 00:06:44.170
expressing moral emotions like outrage
without actually necessarily

00:06:44.170 --> 00:06:44.180
without actually necessarily
 

00:06:44.180 --> 00:06:47.590
without actually necessarily
Aaron Singh them strongly or desiring to

00:06:47.590 --> 00:06:47.600
Aaron Singh them strongly or desiring to
 

00:06:47.600 --> 00:06:52.300
Aaron Singh them strongly or desiring to
express those so broadly the way that we

00:06:52.300 --> 00:06:52.310
express those so broadly the way that we
 

00:06:52.310 --> 00:06:55.120
express those so broadly the way that we
just do on social media and so I think

00:06:55.120 --> 00:06:55.130
just do on social media and so I think
 

00:06:55.130 --> 00:06:58.120
just do on social media and so I think
it's really worth considering and having

00:06:58.120 --> 00:06:58.130
it's really worth considering and having
 

00:06:58.130 --> 00:07:00.670
it's really worth considering and having
a conversation about whether we want

00:07:00.670 --> 00:07:00.680
a conversation about whether we want
 

00:07:00.680 --> 00:07:03.820
a conversation about whether we want
some of our strongest moral emotions

00:07:03.820 --> 00:07:03.830
some of our strongest moral emotions
 

00:07:03.830 --> 00:07:08.350
some of our strongest moral emotions
which are so core to who we are do we

00:07:08.350 --> 00:07:08.360
which are so core to who we are do we
 

00:07:08.360 --> 00:07:10.240
which are so core to who we are do we
want those under the control of

00:07:10.240 --> 00:07:10.250
want those under the control of
 

00:07:10.250 --> 00:07:12.340
want those under the control of
algorithms whose main purpose is to

00:07:12.340 --> 00:07:12.350
algorithms whose main purpose is to
 

00:07:12.350 --> 00:07:15.460
algorithms whose main purpose is to
generate advertising revenue for big

00:07:15.460 --> 00:07:15.470
generate advertising revenue for big
 

00:07:15.470 --> 00:07:18.410
generate advertising revenue for big
tech companies

00:07:18.410 --> 00:07:18.420
 
 

00:07:18.420 --> 00:07:21.670
 
[Music]

