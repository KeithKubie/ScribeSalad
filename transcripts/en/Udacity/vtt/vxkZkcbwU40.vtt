WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:04.689
An efficient cache aware matrix multiply
performs a sequence of of submatrix or

00:00:04.689 --> 00:00:06.580
block by block multiplies.

00:00:06.580 --> 00:00:10.700
You choose the block size so that,
nominally, three blocks fit in cache.

00:00:10.700 --> 00:00:14.370
The corresponding algorithm looks like
this, and notice that the block size is

00:00:14.370 --> 00:00:17.140
denoted by little b appears
directly in the algorithm.

00:00:17.140 --> 00:00:21.410
Now you can show that the number of
cache misses is asymptotically n cubed

00:00:21.410 --> 00:00:23.550
divided by L times b.

00:00:23.550 --> 00:00:25.820
And since b is proportional
to square root of Z,

00:00:25.820 --> 00:00:28.400
that's just n cubed over L root z.

00:00:28.400 --> 00:00:31.480
As it happens, for
any non-Strassen matrix multiply,

00:00:31.480 --> 00:00:34.410
no algorithm can do
asymptotically better than this.

00:00:34.410 --> 00:00:37.400
Now, since b is a function
of the cache size, and

00:00:37.400 --> 00:00:41.020
it appears in the algorithm,
we say this algorithm is cache-aware.

00:00:41.020 --> 00:00:42.320
Here's a question for you.

00:00:42.320 --> 00:00:46.350
Is there a different algorithm
that dos not refer to L or Z, yet

00:00:46.350 --> 00:00:48.850
somehow still attains the lower bound?

00:00:48.850 --> 00:00:50.390
The answer is yes.

00:00:50.390 --> 00:00:52.850
It's based on the the idea of divide and
conquer.

00:00:52.850 --> 00:00:55.800
Let's assume for simplicity,
that the matrices are n by n,

00:00:55.800 --> 00:00:58.010
where n is an integer power of two.

00:00:58.010 --> 00:01:02.280
The algorithm first partitions the
matrices into two by two submatrices.

00:01:02.280 --> 00:01:05.400
Then performs eight recursive
matrix multiplies to update

00:01:05.400 --> 00:01:07.250
the two by two C block.

00:01:07.250 --> 00:01:09.910
To be more precise,
here's the pseudocode algorithm.

00:01:09.910 --> 00:01:13.750
The base case occurs when A,
B and C are scalars.

00:01:13.750 --> 00:01:16.340
The algorithm just
performs a scalar update.

00:01:16.340 --> 00:01:19.710
Otherwise, the input matrices are
logically partitioned into quadrants as

00:01:19.710 --> 00:01:20.910
shown in the figure.

00:01:20.910 --> 00:01:24.150
The algorithm performs eight
recursive matrix multiplies to

00:01:24.150 --> 00:01:26.460
update the four quadrants of C.

00:01:26.460 --> 00:01:30.348
Now as a pseudo code reveals,
this algorithm is cache oblivious.

00:01:30.348 --> 00:01:34.570
Unlike the block algorithm, it makes no
reference to the size of the cache, or

00:01:34.570 --> 00:01:35.990
of the line size.

00:01:35.990 --> 00:01:39.430
Now we need to analyze this algorithm
Let's forget about cache misses for

00:01:39.430 --> 00:01:42.130
the moment and suppose you just wanted
to count the number of adds and

00:01:42.130 --> 00:01:42.794
multiplies.

00:01:42.794 --> 00:01:43.990
How would you do it?

00:01:43.990 --> 00:01:45.910
Well the algorithm's recursive so

00:01:45.910 --> 00:01:48.880
you'd write down a recurrence
relation maybe like this one.

00:01:48.880 --> 00:01:51.430
The first case counts
the eight recursive calls

00:01:51.430 --> 00:01:53.310
on half the problem size.

00:01:53.310 --> 00:01:57.700
The base case counts the multiplies and
adds or flops and namely there are two.

00:01:57.700 --> 00:02:01.300
If you solve this recurrence exactly
you would get 2 n cubed flops which is

00:02:01.300 --> 00:02:02.880
exactly what you would expect.

00:02:02.880 --> 00:02:04.570
Now, what about cache misses?

00:02:04.570 --> 00:02:06.510
Imagine the recursion tree.

00:02:06.510 --> 00:02:08.320
Each node is a function call.

00:02:08.320 --> 00:02:10.479
There's eight way
branching at each node,

00:02:10.479 --> 00:02:12.970
and the leaves correspond
to the base case.

00:02:12.970 --> 00:02:17.510
And of course the problem size shrinks
progressively from n down to one.

00:02:17.510 --> 00:02:22.210
At some point in this recursion, all
the operands of a subtree fit in cache.

00:02:22.210 --> 00:02:26.800
Let's say that level is little l,
and call that size n sub l.

00:02:26.800 --> 00:02:30.620
At that point, it must be that
three blocks fit in cache.

00:02:30.620 --> 00:02:32.090
If the matrix is stored in row or

00:02:32.090 --> 00:02:36.300
column major layout, then you'd
also need to assume tall caches.

00:02:36.300 --> 00:02:39.890
Then you can conclude that n
sub l will be less than or

00:02:39.890 --> 00:02:43.180
equal to some constant fraction of z.

00:02:43.180 --> 00:02:46.940
I'll denote that fraction by
the fudge constant f for the moment.

00:02:46.940 --> 00:02:50.650
So a recurrence for
the cache misses will have two cases.

00:02:50.650 --> 00:02:53.300
The first is when
the operands fit in cache.

00:02:53.300 --> 00:02:55.960
The number of cache misses
is just proportional to

00:02:55.960 --> 00:02:58.410
n sub L squared divided by L.

00:02:58.410 --> 00:02:59.850
That's the base case.

00:02:59.850 --> 00:03:02.020
Otherwise, there's the recursive case.

00:03:02.020 --> 00:03:05.540
You pay for misses at each of
the eight branches plus, let's assume,

00:03:05.540 --> 00:03:08.540
a constant number within
the function call itself.

00:03:08.540 --> 00:03:09.750
If you solve this recurrence,

00:03:09.750 --> 00:03:13.920
you should get the following,
n cubed over L root z.

00:03:13.920 --> 00:03:18.400
This matches the cache aware algorithm,
as well as the lower bound, yet

00:03:18.400 --> 00:03:22.390
remember that the algorithm
itself never refers to z or L.

00:03:22.390 --> 00:03:23.470
That's oblivious, baby!

