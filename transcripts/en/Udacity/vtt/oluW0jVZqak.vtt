WEBVTT
Kind: captions
Language: en

00:00:00.360 --> 00:00:02.440
So, yeah, so next time we're
going to dive into algorithms and

00:00:02.440 --> 00:00:05.210
talk about temporal difference learning,
so why don't we just sort of summarize

00:00:05.210 --> 00:00:08.630
the philosophical stuff that
we talked about just now.

00:00:08.630 --> 00:00:10.020
&gt;&gt; Okay.
&gt;&gt; So what have we learned?

00:00:10.020 --> 00:00:11.880
&gt;&gt; So,
I think we learned a bunch of stuff, but

00:00:11.880 --> 00:00:14.540
we really learned three
classes of things.

00:00:15.640 --> 00:00:18.671
We learned sort of what
reinforcement learning is.

00:00:18.671 --> 00:00:20.560
&gt;&gt; All right,
that's one class of things.

00:00:20.560 --> 00:00:25.070
&gt;&gt; Yeah, and as you wrote up there, we
realize RL is about agents interacting

00:00:25.070 --> 00:00:29.190
with environments and getting rewards
and figuring out what the world is.

00:00:29.190 --> 00:00:33.300
And by the way, as a part of that,
we learned just how frustrating it is

00:00:33.300 --> 00:00:36.550
to actually do reinforcement learning
rather than just solving an MDP.

00:00:36.550 --> 00:00:37.830
&gt;&gt; I agree with that.

00:00:37.830 --> 00:00:40.400
&gt;&gt; Okay, the second thing we learned or

00:00:40.400 --> 00:00:45.430
we talked about was a description of
what a policy is and the different

00:00:45.430 --> 00:00:50.270
ways you might go around solving these
RL problems and evaluating them.

00:00:50.270 --> 00:00:52.300
What does it mean to have a good policy?

00:00:53.340 --> 00:00:55.880
And then the third class
of things that we learned

00:00:55.880 --> 00:00:58.350
was about evaluating learners.

00:00:58.350 --> 00:01:01.730
And we realized that there's lots of
different ways to evaluate learners just

00:01:01.730 --> 00:01:05.810
like there's lots of different ways
to evaluate policies and plans.

00:01:05.810 --> 00:01:09.830
You can talk about truncating them or
doing things that are infinite.

00:01:09.830 --> 00:01:12.390
We can talk about whether
a learning is fast,

00:01:12.390 --> 00:01:15.290
whether a learner is efficient
in a different sense like with

00:01:15.290 --> 00:01:18.260
sample complexity whether it
actually returns good values.

00:01:18.260 --> 00:01:22.860
If you think about it just in terms of
it's output or how it got to its output.

00:01:22.860 --> 00:01:23.390
&gt;&gt; Right.
And so

00:01:23.390 --> 00:01:26.600
we're going to make some choices so that
we have concrete algorithms that we can

00:01:26.600 --> 00:01:28.630
present and
analyses that we can present.

00:01:28.630 --> 00:01:31.810
But it's worth keeping in mind that some
of these things are kind of arbitrary.

00:01:31.810 --> 00:01:33.320
I mean, they're justified.

00:01:33.320 --> 00:01:35.970
But there's other things that
would also be justified.

00:01:35.970 --> 00:01:38.180
&gt;&gt; That seems fair in
machine learning All right,

00:01:38.180 --> 00:01:39.570
well that all make sense to me Michael.

00:01:39.570 --> 00:01:43.640
So, I guess we'll continue
this conversation next week.

00:01:43.640 --> 00:01:44.620
&gt;&gt; Sounds good.

00:01:44.620 --> 00:01:45.870
&gt;&gt; All right, have a good day.

