Speaker 1:          00:00          Dear fellow scholars, this is two minute papers with cartilage all Nifa here. When I opened my inbox today, I was greeted by a huge deluge of messages about wavenet. Well first it's great to see that so many people are excited about these inventions and second, may all your wishes come true as quickly as this one. So here we go. This piece of work is about generating audio wave forms for text to speech and more text to speech basically means that we have a voice reading whatever we have written down. The difference in this work is however, that it can synthesize these samples in someone's voice, provided that we have training samples of this person speaking. The avocado is a pear shaped fruit with leathery skin, smooth edible flesh and a large stone.

Speaker 2:          00:50          The avocado is a pear shaped fruit with leathery skin, smooth edible flesh and a large stone. The avocado is a pear shaped fruit with leathery skin, smooth edible flesh and a large stone. The avocado is a pear shaped fruit with leathery skin, smooth edible flesh and a large stone.

Speaker 1:          01:08          It also generates way forms sample by sample, which is particularly perilous because we typically need to produce these at the rate of 16 to 24,000 samples per second and as we listen to the TV, radio and talk to each other several hours a day, the human ear and brain is particularly suited to processing this kind of signal. If the result is off by only the slightest amount, we immediately recognize it. It is not using a recurrent neural network which is typically suited to learn sequences of things and it's widely used for some synthesis. It is using a convolutional neural network, which is quite surprising because it is not meant to process sequences of data that change in time. However, this variant contains an extension that is able to do that. They call this extension dilated convolutions and the open up the possibility of making large skips in the input data so we have a better global view of it.

Speaker 1:          02:09          If we were working in computer vision, it would be like increasing the receptive field of the eye so we can see the entire landscape and not only a tree on a photograph. It is also a bit like the temporary coherence problem we have talked about earlier. Taking all this into consideration results in more consistent outputs over larger time scales so the technique knows what it had done several seconds ago. Also training a convolutional neural network is a walk in the park compared to our recurrent neural network. Really cool and the results speed all existing widely used techniques by a large margin. One of these is the concrete, the native technique which builds sentences from a huge amount of small speech fragments. These have seen a ton of improvements during the years but the outputs are still robotic and it is noticeable that we are not listening to human but a computer. The deep mind guys also report that quote. Notice that non speech sounds such as breathing and mouth movements are also sometimes generated by wavenet. This reflects the greater flexibility of a raw audio model. End Quote,

Speaker 2:          03:22          the Blue Lagoon is an 1980 American romance and adventure film directed by Randal Kleiser. The Blue Lagoon is a 1980 American romance and adventure film directed by Randal Kleiser aspects of the sublime in English poetry and painting 1770 to 1850 aspects of the sublime in English, poetry and painting 1770 to 1850

Speaker 1:          03:48          at the same time. I like to note that in the next few episodes it may be that my voice is a bit different. Don't worry about that. It may also happen that I am on a vacation, but new episodes and voice samples pop up on the channel. Please don't worry about that either. Everything is working as intended. They also experimented with music generation and the results are just stunning.

Speaker 3:          04:14          Yeah. [inaudible] [inaudible]

Speaker 1:          04:49          I don't know what to say. These difficult problems. These impenetrable walls crumble one after another as deep mind takes on them. Insanity. Their blog post and the paper are both really well written. Make sure to check them out. They are both linked in the video description box. I wager that artistic style transfer for sound and instruments is not only coming, but it will be here soon. I imagined that will play a guitar and it will sound like a harp and we'll be able to sing something in. Lady Gaga has voice and the intonation. I've also seen someone pitching the idea of creating audio books automatically with such a technique. Wow. I travel a lot and then almost always on the go, so I personally would love to have such audio books. I have linked to the mentioned machine learning reddit thread in the description box. As always, there's lots of great discussion and ideas there.

Speaker 1:          05:43          It was also reported that the algorithm currently takes 90 minutes to synthesize one second of sound wave forms. You know the trail, one followup paper down the line. It will take only a few minutes, a few more papers down the line. It will be real time. Just think about all these advancements, what a time we are living in and I am extremely excited to present them all to you. Fellow scholars into minute papers. Make sure to leave your thoughts and ideas in the comment section. We'll love reading them. Thanks for watching and for your generous support and I'll see you next time.

Speaker 3:          06:18          [inaudible].