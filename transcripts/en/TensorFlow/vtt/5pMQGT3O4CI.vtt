WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.390
[MUSIC PLAYING]

00:00:03.687 --> 00:00:05.270
JEN GENNAI: I'm an
operations manager,

00:00:05.270 --> 00:00:09.290
so my role is to ensure that
we're making our considerations

00:00:09.290 --> 00:00:13.070
around ethically AI
deliberate, actionable,

00:00:13.070 --> 00:00:18.880
and scalable across the
whole organization in Google.

00:00:18.880 --> 00:00:21.120
So one of the first
things to think about

00:00:21.120 --> 00:00:23.580
if you're a business
leader or a developer

00:00:23.580 --> 00:00:27.280
is ensuring that people
understand what you stand for.

00:00:27.280 --> 00:00:29.640
What does ethics mean to you?

00:00:29.640 --> 00:00:33.270
For us, that meant setting
values-driven principles

00:00:33.270 --> 00:00:35.510
as a company.

00:00:35.510 --> 00:00:37.810
These value-driven
principles, for us,

00:00:37.810 --> 00:00:39.620
are known as our AI principles.

00:00:39.620 --> 00:00:42.640
And last year, we
announced them in June.

00:00:42.640 --> 00:00:45.880
So these are seven guidelines
around AI development

00:00:45.880 --> 00:00:49.300
and deployment, which
assigned to us how

00:00:49.300 --> 00:00:51.730
we want to develop AI.

00:00:51.730 --> 00:00:55.660
We want to ensure that we're not
creating or reinforcing bias.

00:00:55.660 --> 00:00:58.120
We want to make sure that
we're building technology

00:00:58.120 --> 00:00:59.780
that's accountable to people.

00:00:59.780 --> 00:01:01.870
And we have five others
here that you can read.

00:01:01.870 --> 00:01:04.060
It's available on our website.

00:01:04.060 --> 00:01:06.490
But at the same time
that we announce

00:01:06.490 --> 00:01:10.070
these aspirational
principles for the company,

00:01:10.070 --> 00:01:14.170
we also identified
four areas that we

00:01:14.170 --> 00:01:16.070
have considered our red lines.

00:01:16.070 --> 00:01:19.000
So these are technologies
that we will not pursue.

00:01:19.000 --> 00:01:21.280
These cover things like
weapons technology.

00:01:21.280 --> 00:01:23.710
We will not build
or deploy weapons.

00:01:23.710 --> 00:01:26.380
We will also not build
or deploy technologies

00:01:26.380 --> 00:01:29.980
that we feel violate
international human rights.

00:01:29.980 --> 00:01:32.260
So if you're a business
leader or a developer,

00:01:32.260 --> 00:01:34.600
we'd also encourage
you to understand what

00:01:34.600 --> 00:01:36.470
are your aspirational goals.

00:01:36.470 --> 00:01:39.610
But at the same time,
what are your guardrails?

00:01:39.610 --> 00:01:41.410
What point are you're
not going to cross?

00:01:41.410 --> 00:01:44.620
It's the most important thing
to do, is to know what is

00:01:44.620 --> 00:01:49.410
your definition of
ethical AI development.

00:01:49.410 --> 00:01:51.660
After you've set
your AI principles,

00:01:51.660 --> 00:01:54.000
the next thing is, how
do you make them real?

00:01:54.000 --> 00:01:58.240
How do you make sure that you're
aligning with those principles?

00:01:58.240 --> 00:02:00.220
So here, there are
three main things

00:02:00.220 --> 00:02:02.150
I'd suggest keeping in mind.

00:02:02.150 --> 00:02:05.980
The first one is you need an
accountable and authoritative

00:02:05.980 --> 00:02:07.010
body.

00:02:07.010 --> 00:02:10.060
So for us in Google, this means
that we have senior executives

00:02:10.060 --> 00:02:13.000
across the whole company
who have the authority

00:02:13.000 --> 00:02:16.365
to approve or decline a launch.

00:02:16.365 --> 00:02:17.740
So they have to
wrestle with some

00:02:17.740 --> 00:02:20.410
of these very complex
ethical questions

00:02:20.410 --> 00:02:23.020
to ensure that we
are launching things

00:02:23.020 --> 00:02:26.530
that we do believe will lead
to fair and ethical outcomes.

00:02:26.530 --> 00:02:29.800
So they provide the authority
and the accountability

00:02:29.800 --> 00:02:32.860
to make some really
tough decisions.

00:02:32.860 --> 00:02:36.390
Secondly, you have to make sure
that the decision-makers have

00:02:36.390 --> 00:02:38.100
the right information.

00:02:38.100 --> 00:02:42.310
This involves talking to diverse
people within the company,

00:02:42.310 --> 00:02:45.090
but also listening to
your external users,

00:02:45.090 --> 00:02:47.640
external stakeholders,
and feeding that

00:02:47.640 --> 00:02:49.950
into your
decision-making criteria.

00:02:49.950 --> 00:02:52.200
Jamila will talk
more about engaging

00:02:52.200 --> 00:02:55.000
with external
communites in a moment.

00:02:55.000 --> 00:02:57.390
And then the third key
part of building governance

00:02:57.390 --> 00:02:59.970
and accountability
is having operations.

00:02:59.970 --> 00:03:01.420
Who's going to do the work?

00:03:01.420 --> 00:03:03.390
What are the structures
and frameworks

00:03:03.390 --> 00:03:05.940
that are repeatable,
that are transparent,

00:03:05.940 --> 00:03:07.740
and that are understood
by the people who

00:03:07.740 --> 00:03:09.550
are making these decisions?

00:03:09.550 --> 00:03:12.960
So for that, in Google, we've
established a central team

00:03:12.960 --> 00:03:15.450
that's not based in our
engineering and product teams

00:03:15.450 --> 00:03:18.360
to ensure that there's a
level of objectivity here.

00:03:18.360 --> 00:03:20.880
So the same people who
are building the products

00:03:20.880 --> 00:03:22.770
are not the only
people who are looking

00:03:22.770 --> 00:03:27.670
to make sure that those
products are fair and ethical.

00:03:27.670 --> 00:03:29.730
So now you have your
principles that you're

00:03:29.730 --> 00:03:32.100
trying to ensure that
people understand

00:03:32.100 --> 00:03:33.972
what does ethics mean for you.

00:03:33.972 --> 00:03:36.180
We're talking about establishing
governance structure

00:03:36.180 --> 00:03:38.430
to make sure that you're
achieving those goals,

00:03:38.430 --> 00:03:42.000
and the next thing to do is to
ensure that you're encouraging

00:03:42.000 --> 00:03:45.000
everyone within your company or
the people that you work with

00:03:45.000 --> 00:03:48.250
and for are aligned
on those goals.

00:03:48.250 --> 00:03:52.080
So making sure, one, that you've
set overall goals in alignment

00:03:52.080 --> 00:03:53.640
with ethical AI--

00:03:53.640 --> 00:03:56.520
so how are you going to
achieve ethical development

00:03:56.520 --> 00:03:58.740
and deployment of technology?

00:03:58.740 --> 00:04:01.080
Next, you want to make sure
that you're training people

00:04:01.080 --> 00:04:04.360
to think about these
issues from the start.

00:04:04.360 --> 00:04:06.720
You don't want to catch
some ethical consideration

00:04:06.720 --> 00:04:08.645
late in the product
development lifecycle.

00:04:08.645 --> 00:04:10.020
You want to make
sure that you're

00:04:10.020 --> 00:04:12.390
starting that as early
as possible-- so getting

00:04:12.390 --> 00:04:15.570
people trained to think
about these types of issues.

00:04:15.570 --> 00:04:16.953
Then we have rewards.

00:04:16.953 --> 00:04:18.870
You have to make sure
if you're holding people

00:04:18.870 --> 00:04:21.579
accountable to ethical
development and deployment,

00:04:21.579 --> 00:04:24.360
you may have to accept
that that might slow down

00:04:24.360 --> 00:04:27.540
some development in order to
get to the right outcomes--

00:04:27.540 --> 00:04:30.000
making sure people feel
rewarded for thinking

00:04:30.000 --> 00:04:32.710
about ethical development
and deployment.

00:04:32.710 --> 00:04:35.910
And then, finally, making
sure that you're hiring people

00:04:35.910 --> 00:04:37.920
and developing people
who are helping you

00:04:37.920 --> 00:04:40.990
achieve those goals.

00:04:40.990 --> 00:04:43.240
Next, you've established
your frameworks,

00:04:43.240 --> 00:04:45.520
you've hired the right
people, you're rewarding them.

00:04:45.520 --> 00:04:47.600
How do you know you're
achieving your goals?

00:04:47.600 --> 00:04:50.820
So we think about this as
validating and testing.

00:04:50.820 --> 00:04:52.930
So an example here
is replicating

00:04:52.930 --> 00:04:54.700
a user's experience.

00:04:54.700 --> 00:04:55.872
Who are your users?

00:04:55.872 --> 00:04:57.580
How do you make sure
that you're thinking

00:04:57.580 --> 00:05:00.380
about a representative
sample of your users?

00:05:00.380 --> 00:05:04.150
So you think about trying to
test different experiences,

00:05:04.150 --> 00:05:06.755
mostly from your core subgroups.

00:05:06.755 --> 00:05:08.380
But you also want to
be thinking about,

00:05:08.380 --> 00:05:09.850
who are your marginalized users?

00:05:09.850 --> 00:05:12.675
Who might be underrepresented
in your workforce?

00:05:12.675 --> 00:05:15.175
And therefore, you might have
to pay additional attention to

00:05:15.175 --> 00:05:17.200
to get it right.

00:05:17.200 --> 00:05:20.290
We also think about, what
are the failure modes?

00:05:20.290 --> 00:05:23.860
And what we mean by that is
if people have been negatively

00:05:23.860 --> 00:05:26.050
affected by a
product in the past,

00:05:26.050 --> 00:05:28.360
we want to make sure they
won't be negatively affected

00:05:28.360 --> 00:05:29.360
in the future.

00:05:29.360 --> 00:05:31.120
So how do we learn
from that and make sure

00:05:31.120 --> 00:05:34.670
that we're testing deliberately
for that in the future?

00:05:34.670 --> 00:05:37.030
And then the final bit
of testing and validation

00:05:37.030 --> 00:05:39.250
is introducing some
of those failures

00:05:39.250 --> 00:05:42.410
into the product to make sure
that you're stress testing,

00:05:42.410 --> 00:05:44.650
and, again, have
some objectivity

00:05:44.650 --> 00:05:47.560
to stress test a product
to make sure it's achieving

00:05:47.560 --> 00:05:50.690
your fair and ethical goals.

00:05:50.690 --> 00:05:53.120
And then we think about
it's not just you.

00:05:53.120 --> 00:05:54.120
You're not alone.

00:05:54.120 --> 00:05:56.930
How do we ensure that we're
all sharing information

00:05:56.930 --> 00:06:00.170
to make us more fair and
ethical and to make sure

00:06:00.170 --> 00:06:03.450
that the products we deliver
are fair and ethical?

00:06:03.450 --> 00:06:07.230
So we encourage the sharing of
best practices and guidelines.

00:06:07.230 --> 00:06:10.190
We do that ourselves
in Google by providing

00:06:10.190 --> 00:06:13.700
our research and best practices
on the Google AI site.

00:06:13.700 --> 00:06:15.560
So these best practices
cover everything

00:06:15.560 --> 00:06:18.110
from ML fairness
tools and research

00:06:18.110 --> 00:06:20.550
that Margaret Mitchell will
talk about in a moment,

00:06:20.550 --> 00:06:22.670
but also best practices
and guidelines

00:06:22.670 --> 00:06:24.770
that any developer or
any business leader

00:06:24.770 --> 00:06:26.580
could follow themselves.

00:06:26.580 --> 00:06:29.300
So we try to both provide
that ourselves, as well

00:06:29.300 --> 00:06:31.970
as encouraging other people
to share their research

00:06:31.970 --> 00:06:33.870
and learnings also.

00:06:33.870 --> 00:06:36.770
So with that, as we talk
about sharing with external,

00:06:36.770 --> 00:06:38.840
it's also about
bringing voices in.

00:06:38.840 --> 00:06:40.770
So I'll pass over
to Jamila Smith-Loud

00:06:40.770 --> 00:06:43.790
to talk about understanding
human impacts.

00:06:43.790 --> 00:06:45.230
JAMILA SMITH-LOUD: Thank you.

00:06:45.230 --> 00:06:48.110
[APPLAUSE]

00:06:50.990 --> 00:06:51.638
Hi, everyone.

00:06:51.638 --> 00:06:53.180
I'm going to talk
to you a little bit

00:06:53.180 --> 00:06:58.130
today about understanding,
conceptualizing, and assessing

00:06:58.130 --> 00:07:02.030
human consequences and impacts
on real people and communities

00:07:02.030 --> 00:07:05.030
through the use of tools
like social equity impact

00:07:05.030 --> 00:07:05.960
assessments.

00:07:05.960 --> 00:07:07.790
Social and equity
impact assessments

00:07:07.790 --> 00:07:10.550
come primarily from the
social science discipline

00:07:10.550 --> 00:07:12.920
and give us a
research-based method

00:07:12.920 --> 00:07:16.490
to assess these questions in
a way that is broad enough

00:07:16.490 --> 00:07:18.410
to be able to apply
across products,

00:07:18.410 --> 00:07:21.260
but also specific enough
for us to think about what

00:07:21.260 --> 00:07:27.398
are tangible product changes and
interventions that we can make.

00:07:27.398 --> 00:07:29.190
So I'll start off with
one of the questions

00:07:29.190 --> 00:07:31.950
that we often start when
thinking about these questions.

00:07:31.950 --> 00:07:33.450
I always like to
say that when we're

00:07:33.450 --> 00:07:35.940
thinking about ethics, when
we're thinking about fairness,

00:07:35.940 --> 00:07:37.740
and even thinking about
questions of bias,

00:07:37.740 --> 00:07:39.660
these are really
social problems.

00:07:39.660 --> 00:07:43.410
And one major entry point into
understanding social problems

00:07:43.410 --> 00:07:46.830
is really thinking about what's
the geographic context in which

00:07:46.830 --> 00:07:49.950
users live, and how does
that impact their engagement

00:07:49.950 --> 00:07:50.920
with the product?

00:07:50.920 --> 00:07:52.680
So really asking,
what experiences

00:07:52.680 --> 00:07:55.770
do people have that are based
solely on where they live

00:07:55.770 --> 00:07:58.552
and that may differ greatly
for other peoples who

00:07:58.552 --> 00:08:00.510
live in different
neighborhoods that are either

00:08:00.510 --> 00:08:04.050
more resourced, more
connected to internet-- all

00:08:04.050 --> 00:08:07.170
of these different aspects that
make regional differences so

00:08:07.170 --> 00:08:09.090
important?

00:08:09.090 --> 00:08:12.920
Secondly, we like to ask what
happens to people when they're

00:08:12.920 --> 00:08:15.920
engaging with our
products in their families

00:08:15.920 --> 00:08:17.480
and in their communities.

00:08:17.480 --> 00:08:20.450
We like to think about, what
are economic changes that

00:08:20.450 --> 00:08:24.210
may come as a part of engagement
with this new technology?

00:08:24.210 --> 00:08:27.650
What are social and cultural
changes that really do impact

00:08:27.650 --> 00:08:31.430
how people view the technology
and view their participation

00:08:31.430 --> 00:08:33.640
in the process?

00:08:33.640 --> 00:08:36.850
And so I'll start a little bit
of talking about our approach.

00:08:36.850 --> 00:08:38.460
The good thing
about utilizing kind

00:08:38.460 --> 00:08:42.780
of existing frameworks of social
and equity impact assessments

00:08:42.780 --> 00:08:44.700
which come from--

00:08:44.700 --> 00:08:47.220
if you think about when
we do new land development

00:08:47.220 --> 00:08:49.630
projects or even
environmental assessments,

00:08:49.630 --> 00:08:52.890
there's already the standard
of considering social impacts

00:08:52.890 --> 00:08:54.220
as a part of that process.

00:08:54.220 --> 00:08:57.263
And so we really do think of
employing new technologies

00:08:57.263 --> 00:08:57.930
in the same way.

00:08:57.930 --> 00:09:01.170
We should be asking similar
questions about how communities

00:09:01.170 --> 00:09:03.390
are impacted, what
are their perceptions,

00:09:03.390 --> 00:09:05.620
and how are they framing
these engagements?

00:09:05.620 --> 00:09:07.800
And so one of the things
that we think about

00:09:07.800 --> 00:09:10.320
are kind of what is a
principled approach to asking

00:09:10.320 --> 00:09:11.450
these questions?

00:09:11.450 --> 00:09:13.020
And the first one
really is around

00:09:13.020 --> 00:09:15.323
engaging in the hard questions.

00:09:15.323 --> 00:09:16.740
When we're talking
about fairness,

00:09:16.740 --> 00:09:18.450
when we're talking
about ethics, we're

00:09:18.450 --> 00:09:20.250
not talking about
them separately

00:09:20.250 --> 00:09:24.000
from issues of racism,
social class, homophobia,

00:09:24.000 --> 00:09:26.340
and all forms of
cultural prejudice.

00:09:26.340 --> 00:09:29.010
We're talking about what
are the issues as they

00:09:29.010 --> 00:09:30.850
overlay in those systems.?

00:09:30.850 --> 00:09:32.550
And so it really
requires us to be

00:09:32.550 --> 00:09:35.370
OK with those hard questions,
and engaging with them,

00:09:35.370 --> 00:09:38.010
and realizing that our
technologies and our products

00:09:38.010 --> 00:09:40.710
don't exist separately
from that world.

00:09:40.710 --> 00:09:44.000
The next approach is really
towards thinking anticipatory.

00:09:44.000 --> 00:09:45.750
I think the different
thing about thinking

00:09:45.750 --> 00:09:47.880
about social and equity
impact assessments

00:09:47.880 --> 00:09:50.400
from other social
science research methods

00:09:50.400 --> 00:09:53.730
is that the relationships
between causal impacts

00:09:53.730 --> 00:09:56.350
and correlations are going
to be a little bit different,

00:09:56.350 --> 00:09:58.470
and we really are
trying to anticipate

00:09:58.470 --> 00:10:00.310
harms and consequences.

00:10:00.310 --> 00:10:03.840
And so it requires you to be OK
with the fuzzy conversations,

00:10:03.840 --> 00:10:06.300
but also realize that
there's enough research,

00:10:06.300 --> 00:10:07.840
there's enough
data that gives us

00:10:07.840 --> 00:10:11.680
the understanding of how history
and contexts impact outcomes.

00:10:11.680 --> 00:10:14.220
And so being anticipatory
in your process

00:10:14.220 --> 00:10:17.027
is really, really an
important part of it.

00:10:17.027 --> 00:10:19.610
And lastly, in terms of thinking
about the principled approach

00:10:19.610 --> 00:10:22.680
is really centering the
voices and experiences

00:10:22.680 --> 00:10:24.990
of those communities who
often bear the burden

00:10:24.990 --> 00:10:26.580
of the negative impacts.

00:10:26.580 --> 00:10:28.770
And that requires
understanding how

00:10:28.770 --> 00:10:31.560
those communities would even
conceptualize these problems.

00:10:31.560 --> 00:10:34.733
I think sometimes we come
from a technical standpoint,

00:10:34.733 --> 00:10:36.150
and we think about
the communities

00:10:36.150 --> 00:10:37.358
as separate from the problem.

00:10:37.358 --> 00:10:40.870
But if we're ready to center
those voices and engaged

00:10:40.870 --> 00:10:42.540
throughout the whole
process, I think

00:10:42.540 --> 00:10:46.470
it results in better outcomes.

00:10:46.470 --> 00:10:48.570
So to go a little bit
deeper into engaging

00:10:48.570 --> 00:10:51.150
in the hard questions, what
we're really trying to do

00:10:51.150 --> 00:10:53.790
is be able to assess how
a product will impact

00:10:53.790 --> 00:10:55.710
communities,
particularly communities

00:10:55.710 --> 00:10:58.270
who have been historically and
traditionally marginalized.

00:10:58.270 --> 00:11:00.450
So it requires us
to really think

00:11:00.450 --> 00:11:02.160
about history and context.

00:11:02.160 --> 00:11:04.500
How is that shaping this
issue, and what could we

00:11:04.500 --> 00:11:07.030
learn from that assessment?

00:11:07.030 --> 00:11:09.450
It also requires an
intersectional approach.

00:11:09.450 --> 00:11:11.640
If we're thinking
about gender equity,

00:11:11.640 --> 00:11:13.560
if we're thinking
about racial equity,

00:11:13.560 --> 00:11:15.540
these are not issues
that live separately.

00:11:15.540 --> 00:11:18.270
They really do
intersect, and being OK

00:11:18.270 --> 00:11:20.730
with understanding of that
intersectional approach

00:11:20.730 --> 00:11:23.100
allows for a much
fuller assessment.

00:11:23.100 --> 00:11:26.070
And then, lastly, in thinking
about new technologies

00:11:26.070 --> 00:11:27.900
and thinking about
new products, how does

00:11:27.900 --> 00:11:32.310
power influence outcomes and the
feasibility of interventions?

00:11:32.310 --> 00:11:35.760
I think that the question
of power and social impact

00:11:35.760 --> 00:11:37.500
go hand-in-hand,
and it requires us

00:11:37.500 --> 00:11:40.650
to be OK with [? answering. ?]
Answering might not

00:11:40.650 --> 00:11:42.120
get the best
answer, but at least

00:11:42.120 --> 00:11:45.510
asking those hard questions.

00:11:45.510 --> 00:11:50.340
So our anticipatory process is
part of a full process, right?

00:11:50.340 --> 00:11:53.610
So it's not just us thinking
about the social and equity

00:11:53.610 --> 00:11:56.040
impacts, but it really
is thinking about them

00:11:56.040 --> 00:11:57.840
within the context
of the product--

00:11:57.840 --> 00:12:01.800
so really having domain-specific
application of these questions,

00:12:01.800 --> 00:12:04.230
and then having some
assessment of the likelihood

00:12:04.230 --> 00:12:06.390
of the severity of the risk.

00:12:06.390 --> 00:12:09.690
And then, lastly, thinking
about what are meaningful

00:12:09.690 --> 00:12:13.060
mitigations for whatever impacts
that we have to developed.

00:12:13.060 --> 00:12:14.340
And so it's a full process.

00:12:14.340 --> 00:12:16.957
It requires work on
our team in terms

00:12:16.957 --> 00:12:18.415
of understanding
in the assessment,

00:12:18.415 --> 00:12:21.630
but it also requires partnership
with our product teams

00:12:21.630 --> 00:12:24.090
to really do that
domain-specific analysis.

00:12:27.220 --> 00:12:28.480
Centering the assessment.

00:12:28.480 --> 00:12:30.368
I talked a little bit
about this before,

00:12:30.368 --> 00:12:32.410
but when we're centering
this assessment, really,

00:12:32.410 --> 00:12:35.900
what we're trying to ask
is, who's impacted most?

00:12:35.900 --> 00:12:40.090
So if we're thinking
about a problem that

00:12:40.090 --> 00:12:42.040
may have some
economic impact, it

00:12:42.040 --> 00:12:45.040
would require us to
disaggregate the data based

00:12:45.040 --> 00:12:48.250
on income to see what
communities, what populations,

00:12:48.250 --> 00:12:51.850
are most impacted-- so being OK
with thinking about it in very

00:12:51.850 --> 00:12:56.200
specific population
data and understanding

00:12:56.200 --> 00:12:58.270
who is impacted the most.

00:12:58.270 --> 00:13:00.220
Another important
part is validation.

00:13:00.220 --> 00:13:02.350
And I think Jen mentioned
that a lot, but really

00:13:02.350 --> 00:13:04.990
thinking about community-based
research engagements,

00:13:04.990 --> 00:13:07.270
whether that's a
participatory approach,

00:13:07.270 --> 00:13:08.620
whether that's focus groups.

00:13:08.620 --> 00:13:11.530
But really, how do we
validate our assessments

00:13:11.530 --> 00:13:16.750
by engaging communities
directly and really centering

00:13:16.750 --> 00:13:19.450
their framing of the problem
as part of our project?

00:13:19.450 --> 00:13:21.610
And then going through
iteration and realizing

00:13:21.610 --> 00:13:23.985
that it's not going to be
perfect the first time, that it

00:13:23.985 --> 00:13:28.090
requires some pull and
tugging from both sides

00:13:28.090 --> 00:13:29.830
to really get the
conversation right.

00:13:32.780 --> 00:13:37.070
So what types of social
problems are we thinking of?

00:13:37.070 --> 00:13:39.660
We're thinking about
income inequality, housing

00:13:39.660 --> 00:13:42.390
and displacement,
health disparities,

00:13:42.390 --> 00:13:44.300
the digital divide,
and food access.

00:13:44.300 --> 00:13:47.105
We're thinking about these and
all different types of ways,

00:13:47.105 --> 00:13:48.480
but I thought it
might be helpful

00:13:48.480 --> 00:13:50.195
if we thought about
a specific example.

00:13:53.580 --> 00:13:55.190
So let's look at
the example of one

00:13:55.190 --> 00:13:56.900
of the types of social
problems that we

00:13:56.900 --> 00:14:00.800
want to understand in relation
to our products and users.

00:14:00.800 --> 00:14:04.130
The topic of inequity
related to food access, which

00:14:04.130 --> 00:14:05.970
this map shows you--

00:14:05.970 --> 00:14:09.050
and it's definitely a
US context that we're

00:14:09.050 --> 00:14:10.700
thinking about this
question for now,

00:14:10.700 --> 00:14:13.458
but also always thinking
about it from a global way.

00:14:13.458 --> 00:14:15.500
But I thought that this
map was a good way for us

00:14:15.500 --> 00:14:17.040
to look at it.

00:14:17.040 --> 00:14:19.250
As you can see, the areas
that are shaded darker

00:14:19.250 --> 00:14:23.380
are the areas where those users
might have a significantly

00:14:23.380 --> 00:14:25.880
different experience when we're
thinking about products that

00:14:25.880 --> 00:14:28.850
give personalization and
recommendations maybe

00:14:28.850 --> 00:14:30.740
for something like restaurants.

00:14:30.740 --> 00:14:32.360
So we're thinking
about questions

00:14:32.360 --> 00:14:35.608
about how those users are
either included or excluded

00:14:35.608 --> 00:14:37.400
from the product
experience, and then we're

00:14:37.400 --> 00:14:41.150
thinking about going even
further and thinking about how

00:14:41.150 --> 00:14:44.360
small businesses and
low resource businesses

00:14:44.360 --> 00:14:47.100
also impact that
type of product.

00:14:47.100 --> 00:14:49.490
So it requires us to
realize that there's

00:14:49.490 --> 00:14:52.400
a wealth of data that
allows us to even go here as

00:14:52.400 --> 00:14:55.700
deep as the census tract level
and understand that there are

00:14:55.700 --> 00:14:58.100
certain communities who
have a significantly

00:14:58.100 --> 00:15:00.630
different experience
than other communities.

00:15:00.630 --> 00:15:02.870
And so, like I said,
this map is looking

00:15:02.870 --> 00:15:05.600
at communities at a
census tract level

00:15:05.600 --> 00:15:07.700
where there's no car
and no supermarket

00:15:07.700 --> 00:15:10.670
store within a mile.

00:15:10.670 --> 00:15:12.780
And if we want it
to look even deeper,

00:15:12.780 --> 00:15:15.140
we can overlay this
information with income.

00:15:15.140 --> 00:15:18.420
So thinking about food
access and income disparity,

00:15:18.420 --> 00:15:21.110
which are often
connected, gives us

00:15:21.110 --> 00:15:24.200
a better understanding of
how different groups may

00:15:24.200 --> 00:15:26.930
engage with a product.

00:15:26.930 --> 00:15:30.490
And so when thinking about a
hard social problem like this,

00:15:30.490 --> 00:15:32.440
it really requires
us to think, what's

00:15:32.440 --> 00:15:35.080
the logical process
for us to get

00:15:35.080 --> 00:15:38.950
towards a big social problem
and have very specific outcomes

00:15:38.950 --> 00:15:42.010
and effects that are meaningful
and are making a change?

00:15:42.010 --> 00:15:44.350
And it requires us
to really acknowledge

00:15:44.350 --> 00:15:46.750
that there's contexts
that overlays

00:15:46.750 --> 00:15:49.630
all parts of this process,
from the inputs that we have,

00:15:49.630 --> 00:15:52.090
from the activities that we
do-- which may, in my case,

00:15:52.090 --> 00:15:54.370
be very much
research-based activities--

00:15:54.370 --> 00:15:57.080
and then thinking about
what are meaningful outputs.

00:15:57.080 --> 00:15:58.990
And so to go in a
little bit deeper

00:15:58.990 --> 00:16:02.230
in kind of this logic model
way of thinking about it,

00:16:02.230 --> 00:16:05.590
we have a purpose now, in
thinking about the food access

00:16:05.590 --> 00:16:08.890
example, to reduce negative
unintended consequences

00:16:08.890 --> 00:16:12.340
in areas where access to
quality food is an issue.

00:16:12.340 --> 00:16:14.360
We're also very
aware of the context.

00:16:14.360 --> 00:16:17.200
So we're thinking about
the context of food access,

00:16:17.200 --> 00:16:20.673
but we're also thinking about
questions of gentrification.

00:16:20.673 --> 00:16:22.090
We're thinking
about displacement.

00:16:22.090 --> 00:16:24.260
We're thinking about
community distrust.

00:16:24.260 --> 00:16:26.410
So we realize that
this question has

00:16:26.410 --> 00:16:30.370
many other issues that
inform the context, not just

00:16:30.370 --> 00:16:32.370
access to food.

00:16:32.370 --> 00:16:35.290
But as part of the process,
we're identifying resources.

00:16:35.290 --> 00:16:38.770
We're thinking, where are there
multidisciplinary research

00:16:38.770 --> 00:16:40.570
teams that can help
us think through?

00:16:40.570 --> 00:16:42.580
What are our external
stakeholders that

00:16:42.580 --> 00:16:45.160
can help us frame the problem?

00:16:45.160 --> 00:16:47.380
And then, what are the
cross-functional relationships

00:16:47.380 --> 00:16:49.120
that we need to
build to really be

00:16:49.120 --> 00:16:50.740
able to solve this
kind of problem,

00:16:50.740 --> 00:16:52.780
while acknowledging what
our constraints are?

00:16:52.780 --> 00:16:55.510
Oftentimes, time is
a huge constraint,

00:16:55.510 --> 00:16:57.560
and then gaps just in
knowledge and comfort

00:16:57.560 --> 00:16:59.560
in being able to talk
about these hard problems.

00:17:02.430 --> 00:17:03.990
Some of the
activities and inputs

00:17:03.990 --> 00:17:05.460
that we are thinking
about can help

00:17:05.460 --> 00:17:06.900
us get to some
answers are really

00:17:06.900 --> 00:17:09.630
thinking about case studies,
thinking about surveys,

00:17:09.630 --> 00:17:12.240
thinking about user research
where we're asking user

00:17:12.240 --> 00:17:14.130
perception about this issue.

00:17:14.130 --> 00:17:16.589
How does engagement
based on your geography

00:17:16.589 --> 00:17:19.290
differ in being able
to do that analysis?

00:17:19.290 --> 00:17:21.359
And then creating
tangible outputs,

00:17:21.359 --> 00:17:23.609
some that are product
interventions and really

00:17:23.609 --> 00:17:27.160
focused on how we can make
changes to the product,

00:17:27.160 --> 00:17:30.270
but also really community-based
mitigations in thinking about

00:17:30.270 --> 00:17:31.860
are there ways in
which we're engaging

00:17:31.860 --> 00:17:34.500
with the community, ways
in which we're pulling data

00:17:34.500 --> 00:17:40.410
that we can really use to create
a fuller set of solutions.

00:17:40.410 --> 00:17:44.160
And really, it's always towards
aspiring for positive effects

00:17:44.160 --> 00:17:45.460
in principle and practice.

00:17:45.460 --> 00:17:47.220
So this is one of
those areas where

00:17:47.220 --> 00:17:50.010
you can feel like you have
a very principled approach,

00:17:50.010 --> 00:17:52.990
but it really is about being
able to put them into practice.

00:17:52.990 --> 00:17:55.320
And so some of the things
that I'll leave you

00:17:55.320 --> 00:17:58.470
with today in thinking
about understanding

00:17:58.470 --> 00:18:02.340
these human impacts are really
being able to apply them

00:18:02.340 --> 00:18:04.650
and thinking about applying
them in specific technical

00:18:04.650 --> 00:18:07.020
applications,
building trust through

00:18:07.020 --> 00:18:09.720
equitable collaboration--
so really thinking about,

00:18:09.720 --> 00:18:12.070
when you're engaging with
external stakeholders,

00:18:12.070 --> 00:18:13.980
how do you make
it feel equitable

00:18:13.980 --> 00:18:15.930
and that we're both
sharing knowledge

00:18:15.930 --> 00:18:18.420
and experiences in ways
that are meaningful--

00:18:18.420 --> 00:18:21.242
and then validating the
knowledge generation.

00:18:21.242 --> 00:18:23.200
When we're engaging with
different communities,

00:18:23.200 --> 00:18:27.720
we really have to be OK that
information, data, and the way

00:18:27.720 --> 00:18:31.170
that we frame this can come
from multiple different sources,

00:18:31.170 --> 00:18:32.860
and it's really important.

00:18:32.860 --> 00:18:35.770
And then really thinking about,
within your organization,

00:18:35.770 --> 00:18:38.190
within your team,
what are change agents

00:18:38.190 --> 00:18:40.320
and what are change
instruments that really

00:18:40.320 --> 00:18:42.510
make it a meaningful process?

00:18:42.510 --> 00:18:43.080
Thank you.

00:18:43.080 --> 00:18:45.600
Now Margaret will talk more
about the machine learning

00:18:45.600 --> 00:18:46.545
pipeline.

00:18:46.545 --> 00:18:47.483
[APPLAUSE]

00:18:47.483 --> 00:18:48.525
MARGARET MITCHELL: Great.

00:18:52.490 --> 00:18:55.160
Thanks, Jamila.

00:18:55.160 --> 00:18:57.710
So I'll be talking a bit about
fairness and transparency

00:18:57.710 --> 00:18:59.960
and some frameworks and
approaches for developing

00:18:59.960 --> 00:19:01.990
ethical AI.

00:19:01.990 --> 00:19:05.500
So in a typical machine
learning development pipeline,

00:19:05.500 --> 00:19:08.740
the starting point for
developers is often the data.

00:19:08.740 --> 00:19:12.670
Training data is first
collected and annotated.

00:19:12.670 --> 00:19:16.240
From there, a model
can be trained.

00:19:16.240 --> 00:19:18.430
The model can then be
used to output content

00:19:18.430 --> 00:19:23.100
such as predictions or rankings,
and then downstream users

00:19:23.100 --> 00:19:25.080
will see the output.

00:19:25.080 --> 00:19:26.790
And we often see
this approach as

00:19:26.790 --> 00:19:29.160
if it's a relatively
clean pipeline that

00:19:29.160 --> 00:19:33.680
provides objective information
that we can act on.

00:19:33.680 --> 00:19:36.470
However, from the
beginning of this pipeline,

00:19:36.470 --> 00:19:41.720
human bias has already shaped
the data that's collected.

00:19:41.720 --> 00:19:44.840
Human bias then further
shapes what we collect

00:19:44.840 --> 00:19:48.090
and how we annotate it.

00:19:48.090 --> 00:19:51.420
Here are some of the human
biases that commonly contribute

00:19:51.420 --> 00:19:54.780
to problematic biases and
data, and in the interpretation

00:19:54.780 --> 00:19:56.280
of model outputs.

00:19:56.280 --> 00:19:59.100
Things like reporting bias--
where we tend to remark

00:19:59.100 --> 00:20:01.560
on things that are
noticeable to us,

00:20:01.560 --> 00:20:03.510
as opposed to things
that are typical--

00:20:03.510 --> 00:20:05.940
things like out-group
homogeneity bias--

00:20:05.940 --> 00:20:09.090
where we tend to see people
outside of our social group

00:20:09.090 --> 00:20:11.760
as somehow being
less nuanced or less

00:20:11.760 --> 00:20:15.600
complex than people within
the group that we work with--

00:20:15.600 --> 00:20:17.490
and things like
automation bias--

00:20:17.490 --> 00:20:20.250
where we tend to favor
the outputs of systems

00:20:20.250 --> 00:20:24.390
that are automated over the
outputs of what humans actually

00:20:24.390 --> 00:20:29.540
say even when there's
contradictory information.

00:20:29.540 --> 00:20:32.390
So rather than this
straightforward, clean,

00:20:32.390 --> 00:20:35.090
end-to-end pipeline,
we have human bias

00:20:35.090 --> 00:20:39.055
coming in at the
start of the cycle,

00:20:39.055 --> 00:20:40.930
and then being propagated
throughout the rest

00:20:40.930 --> 00:20:41.590
of the system.

00:20:44.560 --> 00:20:46.570
And this creates a
feedback loop where,

00:20:46.570 --> 00:20:49.660
as users see the output of
biased systems and start

00:20:49.660 --> 00:20:52.900
to click or start to
interact with those outputs,

00:20:52.900 --> 00:20:55.300
this then feeds data
that is further trained

00:20:55.300 --> 00:20:57.580
on-- that's already been
biased in this way--

00:20:57.580 --> 00:20:59.410
creating problematic
feedback loops

00:20:59.410 --> 00:21:01.165
where biases can
get worse and worse.

00:21:03.690 --> 00:21:06.200
We call this a sort of
bias network effect,

00:21:06.200 --> 00:21:07.710
or bias "laundering."

00:21:07.710 --> 00:21:10.175
And a lot of our work
seeks to disrupt this cycle

00:21:10.175 --> 00:21:12.425
so that we can bring the
best kind of output possible.

00:21:16.900 --> 00:21:20.790
So some of the
questions we consider

00:21:20.790 --> 00:21:22.450
is, who is at the table?

00:21:22.450 --> 00:21:25.590
What are the priorities
in what we're working on?

00:21:25.590 --> 00:21:28.200
Should we be thinking
about different aspects

00:21:28.200 --> 00:21:32.130
of the problem and different
perspectives as we develop?

00:21:32.130 --> 00:21:34.970
How is the data that we're
working with collected?

00:21:34.970 --> 00:21:37.370
What kind of things
does it represent?

00:21:37.370 --> 00:21:39.620
Are there problematic
correlations in the data?

00:21:39.620 --> 00:21:42.710
Or are some kinds of subgroups
underrepresented in a way

00:21:42.710 --> 00:21:44.750
that will lead to
disproportionate errors

00:21:44.750 --> 00:21:47.000
downstream?

00:21:47.000 --> 00:21:49.220
What are some foreseeable risks?

00:21:49.220 --> 00:21:50.990
So actually thinking
with foresight

00:21:50.990 --> 00:21:54.500
and anticipating possible
negative consequences

00:21:54.500 --> 00:21:57.140
of everything that we work on
in order to better understand

00:21:57.140 --> 00:21:59.830
how we should prioritize.

00:21:59.830 --> 00:22:02.740
What constraints and
supplements should be in place?

00:22:02.740 --> 00:22:04.840
Beyond a basic machine
learning system,

00:22:04.840 --> 00:22:07.450
what can we do to ensure
that we can account

00:22:07.450 --> 00:22:09.910
for the kinds of risks
that we've anticipated

00:22:09.910 --> 00:22:12.430
and can foresee?

00:22:12.430 --> 00:22:14.560
And then what can we share
with you, the public,

00:22:14.560 --> 00:22:15.950
about this process?

00:22:15.950 --> 00:22:18.010
We aim to be transparent
as we can about this

00:22:18.010 --> 00:22:22.000
in order to bring about
information about how we're

00:22:22.000 --> 00:22:25.450
focusing on this and make
it clear that this is part

00:22:25.450 --> 00:22:28.460
of our development lifecycle.

00:22:28.460 --> 00:22:31.280
I'm going to briefly talk about
some technical approaches.

00:22:31.280 --> 00:22:32.610
This is in the research world.

00:22:32.610 --> 00:22:34.640
You can look at papers on
this, if you're interested,

00:22:34.640 --> 00:22:35.348
for more details.

00:22:37.830 --> 00:22:39.723
So there are two sorts of ML--

00:22:39.723 --> 00:22:41.390
Machine Learning--
techniques that we've

00:22:41.390 --> 00:22:43.530
found to be relatively useful.

00:22:43.530 --> 00:22:45.980
One is bias mitigation,
and the other one we've

00:22:45.980 --> 00:22:48.050
been broadly calling inclusion.

00:22:48.050 --> 00:22:52.490
So bias mitigation focuses
on removing a signal

00:22:52.490 --> 00:22:54.450
for problematic variables.

00:22:54.450 --> 00:22:56.720
So for example,
say you're working

00:22:56.720 --> 00:22:59.300
on a system that is supposed
to predict whether or not

00:22:59.300 --> 00:23:01.690
someone should be promoted.

00:23:01.690 --> 00:23:03.940
You want to make sure
that that system is not

00:23:03.940 --> 00:23:07.570
keying on something like gender,
which we know is correlated

00:23:07.570 --> 00:23:09.190
with promotion decisions.

00:23:09.190 --> 00:23:12.310
In particular, women are
less likely to be promoted

00:23:12.310 --> 00:23:16.910
or are promoted less quickly
than men in a lot of places,

00:23:16.910 --> 00:23:19.520
including in tech.

00:23:19.520 --> 00:23:22.330
We can do this using an
adversarial multi-task learning

00:23:22.330 --> 00:23:24.730
framework where, while
we predict something

00:23:24.730 --> 00:23:27.730
like getting promoted,
we also try and predict

00:23:27.730 --> 00:23:30.640
the subgroup that we'd like
to make sure isn't affecting

00:23:30.640 --> 00:23:32.770
the decision and
discourage the model

00:23:32.770 --> 00:23:36.430
from being able to see that,
removing the representation

00:23:36.430 --> 00:23:41.650
by basically reversing the
gradient and backpropagating.

00:23:41.650 --> 00:23:43.360
When we work on
inclusion, we're working

00:23:43.360 --> 00:23:46.300
on adding signal for
something-- trying to make sure

00:23:46.300 --> 00:23:49.270
that there are subgroups
that are accounted for,

00:23:49.270 --> 00:23:51.995
even if they're not
well-represented in the data.

00:23:51.995 --> 00:23:54.370
And one of the approaches that
works really well for this

00:23:54.370 --> 00:23:55.690
is transfer learning.

00:23:55.690 --> 00:23:58.210
So we might take a
pre-trained network

00:23:58.210 --> 00:23:59.770
with some understanding
of gender,

00:23:59.770 --> 00:24:02.650
for example, or some
understanding of skin tone,

00:24:02.650 --> 00:24:04.420
and use that in
order to influence

00:24:04.420 --> 00:24:06.910
the decisions of
another network that

00:24:06.910 --> 00:24:10.060
is able to key on these
representations in order

00:24:10.060 --> 00:24:15.220
to better understand nuances in
the world that it's looking at.

00:24:15.220 --> 00:24:17.020
This is a little bit
of an example of one

00:24:17.020 --> 00:24:20.920
of the projects I was working on
where we were able to increase

00:24:20.920 --> 00:24:23.710
how well we could detect whether
or not someone was smiling

00:24:23.710 --> 00:24:27.610
based on working with some
consented gender-identified

00:24:27.610 --> 00:24:30.520
individuals and having
representations of what

00:24:30.520 --> 00:24:33.375
these gender presentations
looked like, using that

00:24:33.375 --> 00:24:35.500
within the model that then
predicted whether or not

00:24:35.500 --> 00:24:36.400
someone was smiling.

00:24:39.040 --> 00:24:40.840
Some of the
transparency approaches

00:24:40.840 --> 00:24:43.900
that we've been working on
help to further explain to you

00:24:43.900 --> 00:24:47.110
and also help keep us
accountable for doing

00:24:47.110 --> 00:24:48.770
good work here.

00:24:48.770 --> 00:24:51.110
So one of them is model cards.

00:24:51.110 --> 00:24:53.780
In model cards, we're
focusing on reporting

00:24:53.780 --> 00:24:56.870
what model performance
is, disaggregating

00:24:56.870 --> 00:25:00.380
across various subgroups, and
making it clear that we've

00:25:00.380 --> 00:25:03.200
taken ethical
considerations into account,

00:25:03.200 --> 00:25:05.150
making it clear
what the intended

00:25:05.150 --> 00:25:09.440
applications of the
model or the API is,

00:25:09.440 --> 00:25:11.690
and sharing, generally,
different kinds

00:25:11.690 --> 00:25:15.560
of considerations that
developers should keep in mind

00:25:15.560 --> 00:25:18.730
as they work with the models.

00:25:18.730 --> 00:25:20.540
Another one is data cards.

00:25:20.540 --> 00:25:25.120
And this provides
evaluation data about,

00:25:25.120 --> 00:25:28.120
when we report numbers,
what is this based on?

00:25:28.120 --> 00:25:32.650
Who is represented when we
decide a model can be used--

00:25:32.650 --> 00:25:34.560
that it's safe for use?

00:25:34.560 --> 00:25:37.440
These kinds of things are useful
for learners-- so people who

00:25:37.440 --> 00:25:39.870
generally want to
better understand

00:25:39.870 --> 00:25:42.510
how models are working and
what are the sort of things

00:25:42.510 --> 00:25:47.580
that are affecting model
performance for third party

00:25:47.580 --> 00:25:48.330
users.

00:25:48.330 --> 00:25:52.230
So non-ML professionals
who just want

00:25:52.230 --> 00:25:54.410
to have a better
understanding of their data

00:25:54.410 --> 00:25:58.080
sets that they're working with
or what the representation

00:25:58.080 --> 00:26:01.260
is in different data sets that
machine learning models are

00:26:01.260 --> 00:26:04.920
based on or evaluated
on, as well as machine

00:26:04.920 --> 00:26:05.970
learning researchers.

00:26:05.970 --> 00:26:09.660
So people like me, who want to
compare model performance, they

00:26:09.660 --> 00:26:13.020
want to understand what
needs to be improved,

00:26:13.020 --> 00:26:16.650
what is already
doing well, and help

00:26:16.650 --> 00:26:19.410
be able to sort of
benchmark and make progress

00:26:19.410 --> 00:26:22.980
in a way that's sensitive
to the nuanced differences

00:26:22.980 --> 00:26:24.780
in different kinds
of populations.

00:26:27.640 --> 00:26:29.680
Our commitment to
you, working on

00:26:29.680 --> 00:26:32.710
fair and ethical artificial
intelligence and machine

00:26:32.710 --> 00:26:36.490
learning, is to continue
to measure, to improve,

00:26:36.490 --> 00:26:39.180
and to share real-world
impact related to ethical AI

00:26:39.180 --> 00:26:41.280
development.

00:26:41.280 --> 00:26:42.780
Thanks.

00:26:42.780 --> 00:26:46.130
[APPLAUSE]

