WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.500
The following content is
provided under a Creative

00:00:02.500 --> 00:00:03.910
Commons license.

00:00:03.910 --> 00:00:06.950
Your support will help MIT
OpenCourseWare continue to

00:00:06.950 --> 00:00:10.600
offer high quality educational
resources for free.

00:00:10.600 --> 00:00:13.500
To make a donation or view
additional materials from

00:00:13.500 --> 00:00:18.480
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:18.480 --> 00:00:19.730
ocw.mit.edu.

00:00:30.430 --> 00:00:35.490
PROFESSOR: So John is going to
present project three, beta.

00:00:35.490 --> 00:00:35.900
JOHN: All right.

00:00:35.900 --> 00:00:38.410
So here's the performance
grades.

00:00:38.410 --> 00:00:41.180
In general, the submission went
a lot better than last

00:00:41.180 --> 00:00:44.950
time in that things were on
time and nobody failed to

00:00:44.950 --> 00:00:50.020
build, or forgot to add files
to their project, or so on.

00:00:50.020 --> 00:00:54.280
We did change the scoring
mechanism a little bit.

00:00:54.280 --> 00:00:56.430
In the [? mdriver ?]

00:00:56.430 --> 00:00:59.550
that we gave you, if your
validator failed you on any of

00:00:59.550 --> 00:01:02.460
your traces, your
score is a zero.

00:01:02.460 --> 00:01:04.400
In this one, we decided
to be nicer.

00:01:04.400 --> 00:01:08.080
We replaced your validator with
our correct validator.

00:01:08.080 --> 00:01:11.100
And for traces that you failed,
you get a zero for the

00:01:11.100 --> 00:01:12.950
points that those traces
contribute.

00:01:12.950 --> 00:01:16.000
But you did get an overall
partial score, even if you

00:01:16.000 --> 00:01:17.920
failed a couple traces.

00:01:17.920 --> 00:01:23.330
So on that note, the reference
implementation does get a 56

00:01:23.330 --> 00:01:24.370
on this score.

00:01:24.370 --> 00:01:27.240
And there were people who had
slower than reference

00:01:27.240 --> 00:01:30.750
implementations that
landed below 56.

00:01:30.750 --> 00:01:32.970
So that might be something to
think about for your final

00:01:32.970 --> 00:01:34.610
submission.

00:01:34.610 --> 00:01:36.540
The high score was a 96.

00:01:36.540 --> 00:01:39.430
And there were actually quite
a few groups in the 90s.

00:01:39.430 --> 00:01:43.330
So overall, people did
really well on this.

00:01:43.330 --> 00:01:49.160
With that said, your validators
didn't really--

00:01:49.160 --> 00:01:50.920
I guess they were OK.

00:01:50.920 --> 00:01:56.800
But there's some people whose
validators failed projects

00:01:56.800 --> 00:01:59.470
that were correct, and other
people whose validators failed

00:01:59.470 --> 00:02:01.620
to detect certain situations.

00:02:01.620 --> 00:02:04.650
So that's also something to
work on for the final.

00:02:04.650 --> 00:02:06.730
We won't be releasing the
stock validators.

00:02:06.730 --> 00:02:09.434
So it'll be up to you guys to
find out what's wrong with

00:02:09.434 --> 00:02:11.750
your validators and fix them.

00:02:11.750 --> 00:02:15.090
And along the same lines of
correctness, once again, for

00:02:15.090 --> 00:02:16.850
the final submission,
we'll be running--

00:02:16.850 --> 00:02:19.270
actually even for the beta,
I believe, we're going to

00:02:19.270 --> 00:02:22.610
Valgrind your projects and
look for memory errors.

00:02:22.610 --> 00:02:25.760
So do that to your own projects
and investigate any

00:02:25.760 --> 00:02:27.010
messages you get.

00:02:32.267 --> 00:02:34.710
AUDIENCE: [INAUDIBLE]

00:02:34.710 --> 00:02:35.310
JOHN: OK.

00:02:35.310 --> 00:02:40.890
So the highlighted column
number 31 refers to the

00:02:40.890 --> 00:02:43.550
reference implementation
of the validator.

00:02:43.550 --> 00:02:45.000
So that's the authority.

00:02:45.000 --> 00:02:48.110
If that's green, then your
implementation is correct.

00:02:48.110 --> 00:02:51.660
And so hopefully, a correct
validator would

00:02:51.660 --> 00:02:53.015
agree with column 31.

00:02:56.210 --> 00:02:57.460
AUDIENCE: [INAUDIBLE]

00:02:59.580 --> 00:03:00.560
JOHN: Yes.

00:03:00.560 --> 00:03:01.862
AUDIENCE: [INAUDIBLE]

00:03:01.862 --> 00:03:02.730
question.

00:03:02.730 --> 00:03:05.120
How can it be that most of--

00:03:05.120 --> 00:03:08.725
so an implementation is
vertical, so tests are

00:03:08.725 --> 00:03:09.120
[UNINTELLIGIBLE]?

00:03:09.120 --> 00:03:10.000
JOHN: No.

00:03:10.000 --> 00:03:11.700
The implementations are
horizontal, and

00:03:11.700 --> 00:03:15.074
the tests are vertical.

00:03:15.074 --> 00:03:17.484
AUDIENCE: So we want our column
to look like column 31?

00:03:17.484 --> 00:03:18.230
Or we want--

00:03:18.230 --> 00:03:20.820
JOHN: You want-- your validators
correctness score

00:03:20.820 --> 00:03:27.590
will be determined by whether
or not your column number

00:03:27.590 --> 00:03:29.950
corresponds with column 31.

00:03:29.950 --> 00:03:33.130
And then, your implementations
correctness will purely be

00:03:33.130 --> 00:03:37.098
determined by whether 31 marks
your row red or green.

00:03:39.910 --> 00:03:41.160
Does that make sense?

00:03:43.955 --> 00:03:45.452
AUDIENCE: [INAUDIBLE PHRASE]

00:03:45.452 --> 00:03:47.947
columns that are all green,
our validators are not

00:03:47.947 --> 00:03:48.446
[UNINTELLIGIBLE]?

00:03:48.446 --> 00:03:48.950
Is that what you're saying?

00:03:48.950 --> 00:03:49.630
PROFESSOR: That's right.

00:03:49.630 --> 00:03:51.500
JOHN: That's correct.

00:03:51.500 --> 00:03:53.500
PROFESSOR: Whereas the rows that
are green, that's what we

00:03:53.500 --> 00:03:56.230
like to see.

00:03:56.230 --> 00:03:57.520
We like green rows.

00:03:57.520 --> 00:04:00.950
And then, we like columns
that match column 31.

00:04:00.950 --> 00:04:02.200
AUDIENCE: [INAUDIBLE PHRASE].

00:04:04.359 --> 00:04:06.307
The first row should
be all red.

00:04:06.307 --> 00:04:10.210
And right now, [INAUDIBLE].

00:04:10.210 --> 00:04:10.370
JOHN: Right.

00:04:10.370 --> 00:04:11.170
PROFESSOR: That's correct.

00:04:11.170 --> 00:04:14.280
JOHN: Whatever error this
person had, very few

00:04:14.280 --> 00:04:16.209
validators seems to
have caught them.

00:04:16.209 --> 00:04:18.930
Which is very surprising,
because what we did for your

00:04:18.930 --> 00:04:23.220
validator.c is that we removed
the line of code that it

00:04:23.220 --> 00:04:25.750
contained, and we added the
comment that explained in

00:04:25.750 --> 00:04:29.070
English exactly what that
line of code did.

00:04:29.070 --> 00:04:32.620
So it was kind of interesting to
see that not everybody came

00:04:32.620 --> 00:04:35.200
up with the validator that's
identical to reference one.

00:04:39.110 --> 00:04:39.415
PROFESSOR: OK--

00:04:39.415 --> 00:04:39.720
JOHN:JOHN: Yeah.

00:04:39.720 --> 00:04:42.630
So please run Valgrind on your
code before the final

00:04:42.630 --> 00:04:43.930
submission.

00:04:43.930 --> 00:04:47.670
And we'll be posting your
personalized results to your

00:04:47.670 --> 00:04:50.220
repose sometime probably by
the end of the day, either

00:04:50.220 --> 00:04:53.480
today or tomorrow.

00:04:53.480 --> 00:04:53.970
PROFESSOR: Great.

00:04:53.970 --> 00:04:55.164
All right, you can take
this [UNINTELLIGIBLE].

00:04:55.164 --> 00:04:56.976
Or you can [UNINTELLIGIBLE].

00:04:56.976 --> 00:04:58.060
Here you go.

00:04:58.060 --> 00:05:00.520
You guys can have it here, in
case you need to chip in.

00:05:05.550 --> 00:05:06.930
OK.

00:05:06.930 --> 00:05:11.960
So today, we're going to talk
about programming in parallel.

00:05:11.960 --> 00:05:13.380
Parallel programming
and so forth.

00:05:13.380 --> 00:05:16.480
So this is I'm sure what you've
all been waiting for.

00:05:16.480 --> 00:05:18.060
Oops.

00:05:18.060 --> 00:05:20.512
Oh, we have no power here.

00:05:20.512 --> 00:05:21.898
There we go.

00:05:30.710 --> 00:05:33.240
There we go.

00:05:33.240 --> 00:05:34.490
Now I've got power.

00:05:37.170 --> 00:05:39.160
OK.

00:05:39.160 --> 00:05:42.070
Let's see here.

00:05:42.070 --> 00:05:42.420
How's that?

00:05:42.420 --> 00:05:42.890
Good.

00:05:42.890 --> 00:05:43.800
OK.

00:05:43.800 --> 00:05:46.650
So we talk about multicore
programming.

00:05:46.650 --> 00:05:52.990
And let me start with a
little bit of history.

00:05:57.690 --> 00:06:03.890
So since the mid
to late 1960s--

00:06:03.890 --> 00:06:06.420
so how many years is that?

00:06:06.420 --> 00:06:07.270
50 years.

00:06:07.270 --> 00:06:08.520
Wow.

00:06:11.230 --> 00:06:15.650
Semiconductor density
has been increasing

00:06:15.650 --> 00:06:17.812
at the rate of about--

00:06:17.812 --> 00:06:22.510
it's been doubling about
every 18 to 24 months.

00:06:22.510 --> 00:06:22.790
OK.

00:06:22.790 --> 00:06:34.100
So every year, every one to two
years, every year and a

00:06:34.100 --> 00:06:37.280
half to two years, we
get a doubling of

00:06:37.280 --> 00:06:40.410
density on the chips.

00:06:40.410 --> 00:06:44.220
And that's a trend that
still is continuing.

00:06:44.220 --> 00:06:44.480
OK.

00:06:44.480 --> 00:06:47.620
So that's called Moore's law,
the doubling of density of

00:06:47.620 --> 00:06:48.940
integrated circuits.

00:06:48.940 --> 00:06:53.890
And so, this is basically a
curve showing how transistor

00:06:53.890 --> 00:06:56.110
count is rising.

00:06:56.110 --> 00:06:56.550
OK.

00:06:56.550 --> 00:07:01.060
So all these green things are
Intel CPUs and what the

00:07:01.060 --> 00:07:03.460
transistor count is on them.

00:07:03.460 --> 00:07:04.350
Yeah, question?

00:07:04.350 --> 00:07:06.210
AUDIENCE: [INAUDIBLE PHRASE]

00:07:06.210 --> 00:07:10.120
the lines in [INAUDIBLE]?

00:07:10.120 --> 00:07:12.150
PROFESSOR: So there have
been some technology

00:07:12.150 --> 00:07:14.550
changes along the way.

00:07:14.550 --> 00:07:19.530
So in particular, the
[UNINTELLIGIBLE] transition is

00:07:19.530 --> 00:07:21.800
back down here I think.

00:07:21.800 --> 00:07:24.812
I don't remember which
one that is.

00:07:24.812 --> 00:07:26.590
Well, this is actually
a different one.

00:07:26.590 --> 00:07:29.050
What we're looking at right now
is the transistors, which

00:07:29.050 --> 00:07:31.590
have been very smooth.

00:07:31.590 --> 00:07:32.180
OK.

00:07:32.180 --> 00:07:34.090
So I'll explain this
curve in a minute.

00:07:34.090 --> 00:07:36.330
So there's two things
plotted on here.

00:07:36.330 --> 00:07:42.000
One is the Intel CPU density,
and the other is what the

00:07:42.000 --> 00:07:45.090
clock speed of those
processes is.

00:07:45.090 --> 00:07:48.450
And so these are the clock
speed numbers.

00:07:48.450 --> 00:07:54.890
And so, the integrated circuit
technology has been--

00:07:54.890 --> 00:07:56.430
the density has been doubling.

00:07:56.430 --> 00:08:00.400
And it's really an unbelievable
sort of social

00:08:00.400 --> 00:08:04.460
and economic process, that
this has basically

00:08:04.460 --> 00:08:06.790
been called a law.

00:08:06.790 --> 00:08:11.470
Because what happens is if a--

00:08:11.470 --> 00:08:14.020
there's so many people that
contribute to making

00:08:14.020 --> 00:08:15.880
integrated circuits be dense.

00:08:15.880 --> 00:08:18.970
There's so many pieces of
technology that go into that.

00:08:18.970 --> 00:08:21.360
And what happens is if you
decide that you're going to

00:08:21.360 --> 00:08:24.980
try to jump and try to make
something that goes faster

00:08:24.980 --> 00:08:28.490
than Moore's law, what happens
is it's more expensive

00:08:28.490 --> 00:08:29.440
for you to do it.

00:08:29.440 --> 00:08:33.860
And none of the other
participants in that economy

00:08:33.860 --> 00:08:34.500
can keep up.

00:08:34.500 --> 00:08:36.100
And you're just going to
be more expensive.

00:08:36.100 --> 00:08:41.159
So people will op for the
cheapest thing that gets the

00:08:41.159 --> 00:08:45.730
factor of two every
18 to 24 months.

00:08:45.730 --> 00:08:51.020
Whereas if you're behind, then
nobody uses your stuff.

00:08:51.020 --> 00:08:55.790
So everybody's got this sort
of self-fulfilling prophecy

00:08:55.790 --> 00:08:59.610
that the rate at which the
density is increasing has just

00:08:59.610 --> 00:09:03.190
been extremely stable
for over 50 years.

00:09:03.190 --> 00:09:04.180
It's remarkable.

00:09:04.180 --> 00:09:05.544
Yeah, question?

00:09:05.544 --> 00:09:06.794
AUDIENCE: [INAUDIBLE PHRASE]

00:09:09.234 --> 00:09:11.118
every six months.

00:09:11.118 --> 00:09:13.473
And somehow, [INAUDIBLE]

00:09:13.473 --> 00:09:15.360
you would have self-replicated?

00:09:15.360 --> 00:09:16.500
PROFESSOR: No, I'm
not saying that.

00:09:16.500 --> 00:09:22.230
What I'm saying is that there
is some amount of everybody

00:09:22.230 --> 00:09:24.140
expecting that this
is the point that

00:09:24.140 --> 00:09:25.990
everybody's going to be at.

00:09:25.990 --> 00:09:30.720
And so if you try to go more
aggressively than that, you

00:09:30.720 --> 00:09:33.690
can get burned because you'll
be more expensive.

00:09:33.690 --> 00:09:35.860
If you don't go that fast,
you're going to get burned

00:09:35.860 --> 00:09:38.390
because nobody's going to adopt
your particular piece of

00:09:38.390 --> 00:09:39.960
the technology.

00:09:39.960 --> 00:09:43.500
And so, what happens is
everybody sort of settles for

00:09:43.500 --> 00:09:46.270
this regular repeating.

00:09:46.270 --> 00:09:50.540
It's a remarkable social and
economic phenomenon.

00:09:50.540 --> 00:09:53.320
It's got very little to do at
some level of technology.

00:09:53.320 --> 00:09:56.530
It's just that we know that
we can improve things.

00:09:56.530 --> 00:09:59.060
But what's amazing is this
growth has gone through many

00:09:59.060 --> 00:10:01.360
transitions.

00:10:01.360 --> 00:10:03.120
At one point, they said we
aren't going to be able to

00:10:03.120 --> 00:10:08.680
build integrated circuits any
more densely because all of

00:10:08.680 --> 00:10:11.190
the masks that were made--

00:10:11.190 --> 00:10:13.970
it's basically, you make
computers with a photographic

00:10:13.970 --> 00:10:20.110
process of exposing and
using masks that

00:10:20.110 --> 00:10:22.000
you shine light through.

00:10:22.000 --> 00:10:23.610
It's the way they
used to do it.

00:10:23.610 --> 00:10:26.360
And what happened was the wave
lengths of light were such

00:10:26.360 --> 00:10:28.020
that you were just simply not
going to be able to get the

00:10:28.020 --> 00:10:29.070
resolutions.

00:10:29.070 --> 00:10:29.680
So what did they do?

00:10:29.680 --> 00:10:32.160
They switched to eBeams.

00:10:32.160 --> 00:10:32.430
OK.

00:10:32.430 --> 00:10:38.180
Electrons rather than photons
to expose the silicon wafers

00:10:38.180 --> 00:10:39.100
and so forth.

00:10:39.100 --> 00:10:42.120
And so, they've gone through a
whole bunch of transitions and

00:10:42.120 --> 00:10:42.910
different technologies.

00:10:42.910 --> 00:10:47.020
And yet, throughout all of that,
it's been just a very

00:10:47.020 --> 00:10:51.630
steady progress at about the
rate of 18 to 24 months per

00:10:51.630 --> 00:10:53.240
doubling of density.

00:10:53.240 --> 00:10:56.530
And that is still going on,
and is projected to go on

00:10:56.530 --> 00:10:59.980
maybe for 10 years more.

00:10:59.980 --> 00:11:02.955
It's going to run out, I
hope in my lifetime.

00:11:06.000 --> 00:11:10.680
And certainly within
your lifetimes.

00:11:10.680 --> 00:11:12.710
So that has been going.

00:11:12.710 --> 00:11:15.080
Then, there's second phenomenon
that has been going

00:11:15.080 --> 00:11:22.200
on since about mid-1980s.

00:11:22.200 --> 00:11:26.450
And that is that the clock
speed has actually been

00:11:26.450 --> 00:11:30.730
growing on a similar curve,
where basically, we've been

00:11:30.730 --> 00:11:38.540
getting 30% faster
processors, clock

00:11:38.540 --> 00:11:42.210
speed, since the mid-1980s.

00:11:42.210 --> 00:11:47.140
But something happened there,
which was in around 2003, it

00:11:47.140 --> 00:11:48.390
flattened out.

00:11:51.370 --> 00:11:57.320
And the reason is, as a
practical matter, clock speed

00:11:57.320 --> 00:11:59.950
for air cooled systems
is bounded at

00:11:59.950 --> 00:12:01.730
somewhere around 5 gigahertz.

00:12:01.730 --> 00:12:07.450
If you want to liquid cool
it or nitrogen cool it or

00:12:07.450 --> 00:12:09.500
something, you could
make it go faster.

00:12:09.500 --> 00:12:15.240
But basically, the problem is
that things get too hot.

00:12:15.240 --> 00:12:18.000
And they cannot convey
the heat out.

00:12:18.000 --> 00:12:20.680
So for a while, if you have
greater density, the

00:12:20.680 --> 00:12:22.010
transistors get smaller.

00:12:22.010 --> 00:12:23.460
They switch faster.

00:12:23.460 --> 00:12:26.030
And you can make the clock
speed go faster.

00:12:26.030 --> 00:12:28.710
But at some point, they
hit the wall.

00:12:28.710 --> 00:12:30.790
And so there the vendors were.

00:12:30.790 --> 00:12:35.150
People like Intel,
AMD, Motorola.

00:12:35.150 --> 00:12:37.350
A variety of the semiconductor
manufacturers.

00:12:37.350 --> 00:12:41.860
And what's happened is they
can still make integrated

00:12:41.860 --> 00:12:44.340
circuits more and more dense.

00:12:44.340 --> 00:12:47.820
But they can't clock
them any faster.

00:12:47.820 --> 00:12:49.460
OK.

00:12:49.460 --> 00:12:53.230
So here's what's going
on in the circuits.

00:12:53.230 --> 00:12:58.150
So here's essentially how much
power was being dissipated by

00:12:58.150 --> 00:13:01.960
a variety of Intel processors
along the way, and what they

00:13:01.960 --> 00:13:02.280
[INAUDIBLE]

00:13:02.280 --> 00:13:03.910
2000.

00:13:03.910 --> 00:13:07.020
They started getting hot and
hotter, until if they just

00:13:07.020 --> 00:13:15.610
continued this trend, they were
going to be trying to

00:13:15.610 --> 00:13:17.720
have junction temperatures that
are as hot as the surface

00:13:17.720 --> 00:13:19.500
of the sun.

00:13:19.500 --> 00:13:22.140
Well, they clearly
couldn't do that.

00:13:22.140 --> 00:13:22.900
OK.

00:13:22.900 --> 00:13:25.590
So you might say, well, let's
put it off a few years.

00:13:25.590 --> 00:13:28.200
Yeah, but how many years are
you going to put this off?

00:13:28.200 --> 00:13:30.920
And so, what happened
was they got stuck.

00:13:30.920 --> 00:13:37.620
They simply could not make chips
get clocked in faster.

00:13:37.620 --> 00:13:39.370
So what did they decide to do?

00:13:39.370 --> 00:13:43.110
They got all the silicon area,
but they can't make the

00:13:43.110 --> 00:13:45.810
processors faster with it.

00:13:45.810 --> 00:13:51.720
So their solution was to scale
performance to put many

00:13:51.720 --> 00:13:55.130
processing cores on the
microprocessor chip.

00:13:55.130 --> 00:13:57.895
So this is an example
of a Core i7.

00:13:57.895 --> 00:13:58.780
It's a four core.

00:13:58.780 --> 00:14:02.180
One, two, three, four
cores processor.

00:14:02.180 --> 00:14:04.020
We actually have six
core machines now.

00:14:04.020 --> 00:14:08.290
But I didn't update
the figure.

00:14:08.290 --> 00:14:11.700
And what's going to happen now
is Moore's law is going to

00:14:11.700 --> 00:14:14.080
continue for a few more years.

00:14:14.080 --> 00:14:17.180
And so it looks like each new
generation of Moore's law is

00:14:17.180 --> 00:14:21.860
going to potentially double the
number of cores per chip.

00:14:21.860 --> 00:14:25.090
So you folks are using
12 core machines.

00:14:25.090 --> 00:14:29.090
Two six core chips.

00:14:29.090 --> 00:14:32.210
Well, that's going to basically
keep increasing.

00:14:32.210 --> 00:14:35.220
And so, we're going to get more
and more cores per chip.

00:14:35.220 --> 00:14:35.470
OK.

00:14:35.470 --> 00:14:36.770
That's all well and good.

00:14:36.770 --> 00:14:42.110
But it turns out that there's
a major issue.

00:14:42.110 --> 00:14:44.480
And that's software.

00:14:44.480 --> 00:14:46.620
Everybody has written
their software.

00:14:46.620 --> 00:14:50.050
And there's billions and
billions and billions of

00:14:50.050 --> 00:14:54.620
dollars invested in existing
legacy software that's written

00:14:54.620 --> 00:14:57.040
for how many cores?

00:14:57.040 --> 00:14:58.910
One.

00:14:58.910 --> 00:15:04.220
And moving it to multicore is a

00:15:04.220 --> 00:15:06.160
nightmare for these companies.

00:15:06.160 --> 00:15:06.760
OK.

00:15:06.760 --> 00:15:09.850
And it's potentially a nightmare
for these vendors.

00:15:09.850 --> 00:15:13.100
Because if people say, gee, you
can't make the processors

00:15:13.100 --> 00:15:16.770
go any faster, why should
I buy a new processor?

00:15:16.770 --> 00:15:20.100
My old processor is as
good as my new one.

00:15:20.100 --> 00:15:20.530
OK.

00:15:20.530 --> 00:15:25.100
And so, anyway, so that's
sometimes been called the

00:15:25.100 --> 00:15:26.990
multicore challenge.

00:15:26.990 --> 00:15:29.890
The multicore menace.

00:15:29.890 --> 00:15:33.860
The multicore revolution.

00:15:33.860 --> 00:15:34.430
Whatever.

00:15:34.430 --> 00:15:35.680
But that's what it's
all about.

00:15:35.680 --> 00:15:39.260
It's all about the issue of the
frequency scaling of the

00:15:39.260 --> 00:15:43.390
clocks, verses, Moore's law.

00:15:43.390 --> 00:15:46.300
Which talks about what
the density is.

00:15:46.300 --> 00:15:47.310
OK.

00:15:47.310 --> 00:15:49.870
So their solution is to do--

00:15:49.870 --> 00:15:52.130
and so what we're going to talk
about for a bunch of the

00:15:52.130 --> 00:15:55.900
rest of the term is going to
be, how do you actually

00:15:55.900 --> 00:15:58.300
program multicore processors?

00:15:58.300 --> 00:16:00.640
We're going to look at some
fairly new software technology

00:16:00.640 --> 00:16:03.110
for doing that.

00:16:03.110 --> 00:16:08.170
So here's an abstract multicore
architecture.

00:16:08.170 --> 00:16:10.310
It's not precise.

00:16:10.310 --> 00:16:12.160
This is only showing
one level of cache.

00:16:12.160 --> 00:16:14.850
So we have processors connected
to a cache.

00:16:14.850 --> 00:16:18.360
In fact, of course, you
know that there are

00:16:18.360 --> 00:16:21.070
multiple levels of cache.

00:16:21.070 --> 00:16:24.490
Yeah, this is the international
symbol for cache

00:16:24.490 --> 00:16:25.740
if you live in the US.

00:16:29.310 --> 00:16:32.130
So the processors have
their cache.

00:16:32.130 --> 00:16:34.610
Of course, you know that what
actually happens is you have

00:16:34.610 --> 00:16:35.900
multiple levels of cache.

00:16:35.900 --> 00:16:38.390
And it's shared cache
at some levels.

00:16:38.390 --> 00:16:38.660
OK.

00:16:38.660 --> 00:16:40.120
So it's more complex
than this.

00:16:40.120 --> 00:16:43.210
But this is sort of an abstract
way of understanding

00:16:43.210 --> 00:16:44.550
a bunch of the issues.

00:16:44.550 --> 00:16:46.780
And then, of course, they only
get more complicated as we

00:16:46.780 --> 00:16:54.200
look at reality, as with all
these hardware related things.

00:16:54.200 --> 00:16:55.690
And so, this is a chip
multiprocessor.

00:16:55.690 --> 00:16:58.150
Now there are other ways
of using the silicon.

00:16:58.150 --> 00:17:00.830
So another way of using the
silicon is building things

00:17:00.830 --> 00:17:04.230
like graphics processors and
using silicon for a very

00:17:04.230 --> 00:17:07.130
special purpose thing.

00:17:07.130 --> 00:17:09.890
So that instead of saying,
let's build multiple

00:17:09.890 --> 00:17:13.720
processors, you can say, let's
dedicate some fraction of the

00:17:13.720 --> 00:17:14.829
silicon real estate.

00:17:14.829 --> 00:17:18.280
Instead of to general purpose
computing, let's dedicate it

00:17:18.280 --> 00:17:22.770
to some specific purpose, like
graphics, or some kind of

00:17:22.770 --> 00:17:26.290
stream processing,
or what have you.

00:17:26.290 --> 00:17:29.800
Sensor processing.

00:17:29.800 --> 00:17:31.360
A variety of other things
you can do.

00:17:31.360 --> 00:17:34.100
But one main trend is doing
chip multiprocessors.

00:17:37.930 --> 00:17:39.680
So we're going to talk
a little bit about

00:17:39.680 --> 00:17:40.820
shared memory hardware.

00:17:40.820 --> 00:17:44.680
Just enough to get you folks off
the ground to understand

00:17:44.680 --> 00:17:47.400
what's going on underneath
the system.

00:17:47.400 --> 00:17:49.710
And then, we're going to talk
about four concurrency

00:17:49.710 --> 00:17:53.230
platforms, which are not
the only platforms

00:17:53.230 --> 00:17:54.230
one can program in.

00:17:54.230 --> 00:17:59.640
But they're ones that you
should be familiar with.

00:17:59.640 --> 00:18:02.740
The last one, Cilk++, is the
one we're going to do our

00:18:02.740 --> 00:18:05.520
programming assignments in.

00:18:05.520 --> 00:18:10.030
And then, race conditions, we're
going to talk about,

00:18:10.030 --> 00:18:13.320
because that's the biggest thing
that comes up when you

00:18:13.320 --> 00:18:16.900
do parallel programming compared
to ordinary serial

00:18:16.900 --> 00:18:17.240
programming.

00:18:17.240 --> 00:18:19.850
It's the most pernicious
type of bugs.

00:18:19.850 --> 00:18:24.670
And you need to understand race
conditions and need a way

00:18:24.670 --> 00:18:25.340
of handling it.

00:18:25.340 --> 00:18:27.240
So here's basically--

00:18:27.240 --> 00:18:28.785
so we'll start with shared
memory hardware.

00:18:33.830 --> 00:18:36.920
So the main thing that shared
memory hardware provides is a

00:18:36.920 --> 00:18:39.940
thing called cache coherence.

00:18:39.940 --> 00:18:40.880
OK.

00:18:40.880 --> 00:18:45.360
And the basic idea is that you
want every processor to be

00:18:45.360 --> 00:18:47.330
able to fetch stuff out
of local caches

00:18:47.330 --> 00:18:50.300
because that's fast.

00:18:50.300 --> 00:18:55.020
But at the same time, you want
them to have a common view of

00:18:55.020 --> 00:18:58.760
what is stored in a
given location.

00:18:58.760 --> 00:19:00.690
So let's run through this
example and see what the

00:19:00.690 --> 00:19:01.580
problem is.

00:19:01.580 --> 00:19:03.270
And then, I'll show
you how they solve

00:19:03.270 --> 00:19:05.325
it in sketchy detail.

00:19:09.070 --> 00:19:10.050
So here's a processor.

00:19:10.050 --> 00:19:11.880
Says he wants to load
the value of x.

00:19:11.880 --> 00:19:14.100
And in main memory here,
x has got the value of

00:19:14.100 --> 00:19:16.210
3, up here in DRAM.

00:19:16.210 --> 00:19:16.890
OK.

00:19:16.890 --> 00:19:20.840
So x moves through
to the processor,

00:19:20.840 --> 00:19:22.160
where it gets consumed.

00:19:22.160 --> 00:19:25.850
And it leaves behind the
fact that x equals 3

00:19:25.850 --> 00:19:28.690
in its local cache.

00:19:28.690 --> 00:19:32.040
Well, now along comes the
second processor.

00:19:32.040 --> 00:19:33.860
It says, I want x too.

00:19:33.860 --> 00:19:37.500
And perhaps the same
thing happens.

00:19:37.500 --> 00:19:38.020
Very good.

00:19:38.020 --> 00:19:39.160
So far, no problem.

00:19:39.160 --> 00:19:43.250
So two caches may have
the same value of x.

00:19:43.250 --> 00:19:45.590
They may both want to
use x, and it's both

00:19:45.590 --> 00:19:47.090
in their local caches.

00:19:47.090 --> 00:19:49.960
Now comes along the
third processor.

00:19:49.960 --> 00:19:51.620
Says load x as well.

00:19:51.620 --> 00:19:53.940
Well, it turns out that
it's actually--

00:19:53.940 --> 00:19:55.470
what I showed you on
the second case is

00:19:55.470 --> 00:19:56.940
not the common case.

00:19:56.940 --> 00:20:00.080
If these two processors, these
two processing cores, are on

00:20:00.080 --> 00:20:04.750
the same chip, it's generally
cheaper for this guy to fetch

00:20:04.750 --> 00:20:08.780
it out of one of these guys
caches than it is to fetch it

00:20:08.780 --> 00:20:10.500
out of DRAM.

00:20:10.500 --> 00:20:12.000
DRAM is slow.

00:20:12.000 --> 00:20:14.530
Getting it locally
is much cheaper.

00:20:14.530 --> 00:20:20.030
So basically, in this case, he
gets it from this processor.

00:20:20.030 --> 00:20:21.710
The first processor.

00:20:21.710 --> 00:20:22.650
All is well and good.

00:20:22.650 --> 00:20:24.900
They're all sharing
merrily around.

00:20:24.900 --> 00:20:26.110
OK.

00:20:26.110 --> 00:20:29.710
And then this fella decides
if he wants

00:20:29.710 --> 00:20:30.700
to load it, no problem.

00:20:30.700 --> 00:20:31.400
He can just load it.

00:20:31.400 --> 00:20:32.320
He loads it locally.

00:20:32.320 --> 00:20:33.550
No problem.

00:20:33.550 --> 00:20:34.580
OK.

00:20:34.580 --> 00:20:36.410
This guy decides, oh,
he's going to store

00:20:36.410 --> 00:20:37.500
some value to x.

00:20:37.500 --> 00:20:40.420
In this case, he's going
to store the value 5.

00:20:40.420 --> 00:20:43.480
So he sets x equal to 5.

00:20:43.480 --> 00:20:44.520
OK.

00:20:44.520 --> 00:20:45.810
fine.

00:20:45.810 --> 00:20:47.310
OK, now what?

00:20:47.310 --> 00:20:51.130
Now this guy says,
let me load x.

00:20:51.130 --> 00:20:54.130
He gets the value x equals 3.

00:20:54.130 --> 00:20:55.920
Uh-oh.

00:20:55.920 --> 00:20:58.810
If your parallel program
expected that this guy had

00:20:58.810 --> 00:21:02.510
gone first and it set x value
x equal to 5, these guys are

00:21:02.510 --> 00:21:04.250
now incorrect.

00:21:07.000 --> 00:21:11.670
And so, the idea of cache
coherence is not letting this

00:21:11.670 --> 00:21:18.020
happen, making it so that
whenever a value is changed by

00:21:18.020 --> 00:21:21.750
a processor, the other
processors see that change and

00:21:21.750 --> 00:21:25.330
yet, they're still able most
of the time to execute

00:21:25.330 --> 00:21:29.200
effectively out of their
own local caches.

00:21:29.200 --> 00:21:29.530
OK.

00:21:29.530 --> 00:21:31.660
So that's the problem.

00:21:31.660 --> 00:21:34.650
So do people understand
basically what the cache

00:21:34.650 --> 00:21:37.470
coherence problem is?

00:21:37.470 --> 00:21:38.140
Yes, question?

00:21:38.140 --> 00:21:41.880
AUDIENCE: If the last processor
was to store x and

00:21:41.880 --> 00:21:47.408
set x equals 5, as soon as that
happens, wouldn't that

00:21:47.408 --> 00:21:49.050
write DRAM x equals 5?

00:21:49.050 --> 00:21:49.370
PROFESSOR: Good.

00:21:49.370 --> 00:21:52.365
So there's actually two
types of strategies

00:21:52.365 --> 00:21:54.910
that are used in caches.

00:21:54.910 --> 00:21:58.040
One is called write through.

00:21:58.040 --> 00:22:00.180
And one is called write back.

00:22:00.180 --> 00:22:02.150
What you're describing
is write through.

00:22:02.150 --> 00:22:05.240
What right through caches do
is if you write a value, it

00:22:05.240 --> 00:22:08.440
pushes it all the
way out to DRAM.

00:22:08.440 --> 00:22:11.170
These days, nobody uses
write through.

00:22:11.170 --> 00:22:12.640
You're always going to DRAM.

00:22:12.640 --> 00:22:18.610
You're always exercising the
slow DRAM versus being able to

00:22:18.610 --> 00:22:20.770
just write it locally.

00:22:20.770 --> 00:22:23.960
But you do have to do something
about these guys

00:22:23.960 --> 00:22:27.000
that are going to have
the shared values.

00:22:27.000 --> 00:22:29.290
So here's the mechanism
that they use.

00:22:29.290 --> 00:22:32.580
So what most people do these
days is write back caches.

00:22:32.580 --> 00:22:35.890
Which basically means you only
write it back when you really

00:22:35.890 --> 00:22:40.330
need to evict or
what have you.

00:22:40.330 --> 00:22:43.340
You don't always write it
all the way through.

00:22:43.340 --> 00:22:44.990
And so here's how these
schemes work.

00:22:44.990 --> 00:22:45.390
So, right.

00:22:45.390 --> 00:22:51.470
So that's a bogus value for
that kind to be getting.

00:22:51.470 --> 00:22:52.410
So let's take a look.

00:22:52.410 --> 00:22:54.680
So what they use is what's
called-- the simplest is

00:22:54.680 --> 00:22:59.060
called an MSI protocol.

00:22:59.060 --> 00:23:02.620
There are somewhat more
complicated ones called MESI

00:23:02.620 --> 00:23:06.480
protocols, and ones
that are MOESI.

00:23:06.480 --> 00:23:08.930
"Mo-esi" and "messy".

00:23:08.930 --> 00:23:11.610
Anyway, the MESI one is probably
the one you'll hear

00:23:11.610 --> 00:23:12.730
most often.

00:23:12.730 --> 00:23:14.920
It's just a little bit more
complicated than this one.

00:23:14.920 --> 00:23:22.510
But it saves you one extra
access when we do a write.

00:23:22.510 --> 00:23:23.930
I'll explain it in
just a minute.

00:23:23.930 --> 00:23:28.610
But let's first understand the
simplest of these mechanisms.

00:23:28.610 --> 00:23:31.820
So what you do is in each cache,
you're going to label

00:23:31.820 --> 00:23:34.840
each cache line with a state.

00:23:34.840 --> 00:23:38.350
And basically, it's because
of these states that you

00:23:38.350 --> 00:23:41.420
associate with a cache line that
cache lines end up having

00:23:41.420 --> 00:23:42.860
to be long.

00:23:42.860 --> 00:23:43.140
OK?

00:23:43.140 --> 00:23:47.100
Because if you think about,
you'd like cache lines to be

00:23:47.100 --> 00:23:51.830
at some level very short, in
that then you have more

00:23:51.830 --> 00:23:55.480
opportunity to have just the
stuff in cache that you want,

00:23:55.480 --> 00:23:57.740
from a temporal locality
point of view.

00:23:57.740 --> 00:24:01.160
It's one thing if you want to
bring in extra lines, extra

00:24:01.160 --> 00:24:03.120
data, for spatial locality.

00:24:03.120 --> 00:24:05.710
But to insist that it all be
there whether you access it or

00:24:05.710 --> 00:24:09.870
not, that's not clear how
helpful that it is.

00:24:09.870 --> 00:24:12.470
However, what instead is we have
things like, on the Intel

00:24:12.470 --> 00:24:16.590
architecture, 64 bytes
of cache line.

00:24:16.590 --> 00:24:19.090
And the reason is because
they're keeping extra data

00:24:19.090 --> 00:24:21.360
with each cache line.

00:24:21.360 --> 00:24:25.570
And they want the data to be
the larger fraction of what

00:24:25.570 --> 00:24:27.160
they're keeping compared
to the control

00:24:27.160 --> 00:24:28.900
information about the data.

00:24:28.900 --> 00:24:31.120
So in this case, they're
keeping three values.

00:24:31.120 --> 00:24:33.520
Three bits.

00:24:33.520 --> 00:24:36.370
The M bit says this cache
block has been modified.

00:24:36.370 --> 00:24:38.140
Somebody's written to it.

00:24:38.140 --> 00:24:43.130
And what they do is they, in
this protocol, they guarantee

00:24:43.130 --> 00:24:46.210
in the protocol that if somebody
has it in the M

00:24:46.210 --> 00:24:50.490
state, no other caches contain
this block in either the M

00:24:50.490 --> 00:24:53.980
state or S state.

00:24:53.980 --> 00:24:54.920
So what are those states?

00:24:54.920 --> 00:24:58.540
So the S state is when
other caches may be

00:24:58.540 --> 00:25:00.500
sharing this block.

00:25:00.500 --> 00:25:04.280
And the I state is that this
cache block is invalid.

00:25:04.280 --> 00:25:05.720
It's the same as if
it's not there.

00:25:05.720 --> 00:25:08.250
It's empty entry.

00:25:08.250 --> 00:25:10.460
So it just marks this entry.

00:25:10.460 --> 00:25:12.680
There's no data there.

00:25:12.680 --> 00:25:16.860
The cache line that's there is
not really there, is basically

00:25:16.860 --> 00:25:17.820
what it says.

00:25:17.820 --> 00:25:24.100
So here, you see for example
that this fella has x equals

00:25:24.100 --> 00:25:26.890
13 in the modified state.

00:25:26.890 --> 00:25:29.770
And so, if you look across here,
oh, nobody else has that

00:25:29.770 --> 00:25:32.770
in either the M or
the S state.

00:25:32.770 --> 00:25:37.160
They only have it in the
I state or not at all.

00:25:37.160 --> 00:25:39.780
If you have it in the shared
state, as these guys have,

00:25:39.780 --> 00:25:41.950
well, they all have it in the
shared state and notice the

00:25:41.950 --> 00:25:45.340
values are all the same.

00:25:45.340 --> 00:25:48.130
And then, if it's in the invalid
state, here this guy

00:25:48.130 --> 00:25:51.130
once again has it in the
modified state, which means

00:25:51.130 --> 00:25:54.230
these guys don't have it in
either the S or M state.

00:25:54.230 --> 00:25:55.610
So that's the invariant.

00:25:55.610 --> 00:25:58.950
So what's the basic idea
behind the cache?

00:25:58.950 --> 00:26:00.650
The MSI protocol?

00:26:00.650 --> 00:26:05.360
The idea is that before you can
write on a location, you

00:26:05.360 --> 00:26:09.445
must first invalidate all
the other copies.

00:26:12.260 --> 00:26:14.760
So whenever you try to write
on something that's shared

00:26:14.760 --> 00:26:17.160
across a bunch of things or
that somebody else has

00:26:17.160 --> 00:26:20.940
modified, what happens is over
the network goes out a

00:26:20.940 --> 00:26:25.000
protocol to invalidate
all the other copies.

00:26:25.000 --> 00:26:27.540
So if they're just being shared,
that's no problem.

00:26:27.540 --> 00:26:28.970
Because all you do
is just have them

00:26:28.970 --> 00:26:31.300
drop it from the cache.

00:26:31.300 --> 00:26:35.000
If it's modified, then it may
have to be written back or the

00:26:35.000 --> 00:26:38.820
value brought back to you, so
that you're in a position of

00:26:38.820 --> 00:26:39.200
changing it.

00:26:39.200 --> 00:26:41.610
If somebody has it modified,
then you don't have it.

00:26:41.610 --> 00:26:45.420
So therefore, you need to
bring it in and make the

00:26:45.420 --> 00:26:46.250
change to it.

00:26:46.250 --> 00:26:47.117
Question?

00:26:47.117 --> 00:26:49.470
AUDIENCE: [INAUDIBLE]
three states?

00:26:49.470 --> 00:26:50.160
PROFESSOR: Three states.

00:26:50.160 --> 00:26:51.210
Not three bits.

00:26:51.210 --> 00:26:51.600
Two bits.

00:26:51.600 --> 00:26:52.900
Right.

00:26:52.900 --> 00:26:55.150
OK.

00:26:55.150 --> 00:26:57.470
So the idea is you first
invalidate the other copies.

00:26:57.470 --> 00:27:03.250
Therefore, when a processor core
is changing the value of

00:27:03.250 --> 00:27:05.545
some variable, it has
the only copy.

00:27:08.320 --> 00:27:10.940
And by making sure that it only
has the only copy, you

00:27:10.940 --> 00:27:13.660
make sure that you never have
copies out there that are

00:27:13.660 --> 00:27:20.020
anything except copies of
what everybody else has.

00:27:20.020 --> 00:27:22.170
That they're all the same.

00:27:22.170 --> 00:27:23.340
OK.

00:27:23.340 --> 00:27:26.320
Does everybody follow that?

00:27:26.320 --> 00:27:28.440
So there's hardware under
there doing that.

00:27:28.440 --> 00:27:30.250
It's actually pretty
clever hardware.

00:27:30.250 --> 00:27:36.550
In fact, the verification of
cache protocols is a huge

00:27:36.550 --> 00:27:41.780
problem for which there's a lot
of technology built to try

00:27:41.780 --> 00:27:45.290
to verify to make sure these
cache protocols work the way

00:27:45.290 --> 00:27:46.960
they're supposed to work.

00:27:46.960 --> 00:27:48.860
Because what happens in practice
is there are all

00:27:48.860 --> 00:27:50.070
these intermediate states.

00:27:50.070 --> 00:27:52.980
What happens if this guy starts
doing this while this

00:27:52.980 --> 00:27:57.870
guy is doing that, and these
protocols start getting mixed,

00:27:57.870 --> 00:27:59.130
and so forth?

00:27:59.130 --> 00:28:00.770
And you've got to make
sure that works out.

00:28:00.770 --> 00:28:03.410
And that's what's going
on in the hardware.

00:28:03.410 --> 00:28:07.200
The MESI protocol does a
simple optimization.

00:28:07.200 --> 00:28:11.610
It says, look, before I store
something, I probably

00:28:11.610 --> 00:28:13.230
want to read it.

00:28:13.230 --> 00:28:15.120
It's likely I'm going
to read it.

00:28:15.120 --> 00:28:16.590
So I can read it in two ways.

00:28:16.590 --> 00:28:21.100
I can read it in a way that
says that it is--

00:28:21.100 --> 00:28:23.330
where it's just going
to be shared.

00:28:23.330 --> 00:28:26.390
But if I expect that I'm going
to write it, let me when I

00:28:26.390 --> 00:28:31.530
read it instead of getting a
shared copy, let me get an

00:28:31.530 --> 00:28:32.770
exclusive copy.

00:28:32.770 --> 00:28:34.920
And that's where the
E comes from.

00:28:34.920 --> 00:28:36.350
Let me get an exclusive copy.

00:28:36.350 --> 00:28:39.420
In other words, go through the
invalidation protocols on the

00:28:39.420 --> 00:28:43.030
read, so that with the
expectation that when you

00:28:43.030 --> 00:28:47.320
write, you don't have to then
wait for the invalidation to

00:28:47.320 --> 00:28:48.240
occur at that point.

00:28:48.240 --> 00:28:53.980
So it's a way of reducing the
latency of the protocol by

00:28:53.980 --> 00:28:56.270
getting it exclusively by
the read that you do

00:28:56.270 --> 00:28:58.860
before you do the write.

00:28:58.860 --> 00:29:01.410
So rather than doing a
read, which would go

00:29:01.410 --> 00:29:04.210
out and get the value--

00:29:04.210 --> 00:29:05.250
but everybody [? has them ?]

00:29:05.250 --> 00:29:07.940
shared-- then doing the write,
and then doing a whole

00:29:07.940 --> 00:29:12.020
invalidation protocol, if I
basically get it in exclusive

00:29:12.020 --> 00:29:15.480
mode on the read, then I go out,
I get the value, and I

00:29:15.480 --> 00:29:17.570
invalidate everybody else.

00:29:17.570 --> 00:29:20.340
Now I've just saved myself
half the work

00:29:20.340 --> 00:29:22.630
and half the latency.

00:29:22.630 --> 00:29:24.770
Or basically saved myself
some latency.

00:29:24.770 --> 00:29:26.100
Not half the latency.

00:29:26.100 --> 00:29:27.900
OK?

00:29:27.900 --> 00:29:32.300
So basically, what you should
know is there is invalidation

00:29:32.300 --> 00:29:35.030
stuff going on behind when you
start using shared memory,

00:29:35.030 --> 00:29:39.200
behind the scenes which
can slow down your

00:29:39.200 --> 00:29:42.800
processor from executing.

00:29:42.800 --> 00:29:45.520
Because it can't do the things
that it needs to do until it

00:29:45.520 --> 00:29:49.810
goes through the protocol.

00:29:49.810 --> 00:29:52.060
Any questions about that?

00:29:52.060 --> 00:29:58.920
That's basically the level
we're going to cover the

00:29:58.920 --> 00:30:01.390
hardware at.

00:30:01.390 --> 00:30:04.510
And so, you'll discover that in
doing some your problems,

00:30:04.510 --> 00:30:06.880
that if you're not careful,
you're going to create what

00:30:06.880 --> 00:30:10.220
are called invalidation storms,
where you have a whole

00:30:10.220 --> 00:30:12.880
bunch of things that are red,
and they're distributed across

00:30:12.880 --> 00:30:13.560
the processor.

00:30:13.560 --> 00:30:16.220
And then you go in, and
you set one value.

00:30:16.220 --> 00:30:18.460
And suddenly, vrrrrrruuuum.

00:30:18.460 --> 00:30:21.420
Gee, how come that wasn't
a fast store?

00:30:21.420 --> 00:30:23.550
The answer is it's going through
and invalidating all

00:30:23.550 --> 00:30:24.800
those other copies.

00:30:27.630 --> 00:30:29.330
Good.

00:30:29.330 --> 00:30:31.700
So let's turn to the
real hard problem.

00:30:31.700 --> 00:30:35.290
So it turns out that building
these things is not

00:30:35.290 --> 00:30:36.930
particularly well understood.

00:30:36.930 --> 00:30:38.640
But it's understood
a lot better than

00:30:38.640 --> 00:30:41.310
programming these beasts.

00:30:41.310 --> 00:30:42.400
OK.

00:30:42.400 --> 00:30:47.370
And so, we're going to focus on
some of the strategies for

00:30:47.370 --> 00:30:48.620
programming.

00:30:51.120 --> 00:30:55.760
So it turns out that trying to
program their processor cores

00:30:55.760 --> 00:30:58.530
directly is painful.

00:30:58.530 --> 00:31:04.110
And you're liable to make a lot
of errors, as we'll see.

00:31:04.110 --> 00:31:08.170
Because we're going to talk
about races soon.

00:31:08.170 --> 00:31:11.910
And so the idea of a current
currency platform is to do

00:31:11.910 --> 00:31:16.880
some level of abstraction of the
processor cores to handle

00:31:16.880 --> 00:31:20.540
synchronization communication
protocols, and often to do

00:31:20.540 --> 00:31:24.870
things like load balancing, so
that the work that you're

00:31:24.870 --> 00:31:28.750
doing can be moved across from
processor to processor.

00:31:28.750 --> 00:31:31.390
And so, here are some examples
of concurrency platforms.

00:31:31.390 --> 00:31:34.500
Pthreads and WinAPI threads,
we're going to talk more in

00:31:34.500 --> 00:31:35.500
detail about.

00:31:35.500 --> 00:31:38.880
Pthreads is basically for
Unix type systems,

00:31:38.880 --> 00:31:40.470
like Linux and such.

00:31:40.470 --> 00:31:43.930
WinAPI threads is for Windows.

00:31:43.930 --> 00:31:48.380
There's threading building
blocks, TBB, OpenMP, which is

00:31:48.380 --> 00:31:50.120
a standard, and Cilk++.

00:31:50.120 --> 00:31:54.060
Those are all examples of
concurrency platforms that

00:31:54.060 --> 00:31:59.280
make it easier to program
these parallel machines.

00:31:59.280 --> 00:32:01.520
So I'm going to do, as an
example, I'm going to use the

00:32:01.520 --> 00:32:06.320
Fibonacci numbers, which you
have seen before I'm sure,

00:32:06.320 --> 00:32:11.260
because we've actually even
used it in this class.

00:32:11.260 --> 00:32:16.040
This is Leonardo da Pisa, who
was also known as Fibonacci.

00:32:16.040 --> 00:32:17.820
And he introduced--

00:32:17.820 --> 00:32:20.680
he was the most brilliant
mathematician of his day.

00:32:20.680 --> 00:32:24.230
He came basically out of the
blue, doing all kinds of

00:32:24.230 --> 00:32:28.150
beautiful mathematics very
early in the Renaissance.

00:32:28.150 --> 00:32:31.085
You'll recognize 1202 is
very early Renaissance.

00:32:35.610 --> 00:32:38.440
But it turns out, for those of
you of Indian descent, the

00:32:38.440 --> 00:32:39.900
Indian mathematicians
had already

00:32:39.900 --> 00:32:41.150
discovered all this stuff.

00:32:43.700 --> 00:32:46.480
But it didn't make it into
Western culture except for

00:32:46.480 --> 00:32:50.040
Leonardo da Pisa.

00:32:50.040 --> 00:32:57.740
So here's a program as you might
write it in C. So Fib

00:32:57.740 --> 00:33:00.980
int n says, well, if n is
less than 2, return n.

00:33:00.980 --> 00:33:04.100
So if it's 0 or 1, we return,
Fib of 0 is 0.

00:33:04.100 --> 00:33:05.580
Fib of 1 is 1.

00:33:05.580 --> 00:33:09.815
And otherwise, we compute Fib of
n minus 1, compute Fib of n

00:33:09.815 --> 00:33:12.220
minus 2, and return the sum.

00:33:12.220 --> 00:33:13.950
Simple recursive program.

00:33:13.950 --> 00:33:15.080
Here's the main routine.

00:33:15.080 --> 00:33:18.940
We get the argument from the
command line, compute the

00:33:18.940 --> 00:33:22.170
result, and then print
out Fibonacci

00:33:22.170 --> 00:33:24.210
of whatever is whatever.

00:33:24.210 --> 00:33:26.290
Pretty simple piece of code.

00:33:26.290 --> 00:33:28.240
So what we're going to do is
take a look at what happens in

00:33:28.240 --> 00:33:32.900
each of these four concurrency
platforms to see how it is

00:33:32.900 --> 00:33:37.510
that they make this easy to
run this in parallel.

00:33:37.510 --> 00:33:40.770
Now just a disclaimer here.

00:33:40.770 --> 00:33:42.720
This is a really bad way--

00:33:42.720 --> 00:33:43.850
I hope you all recognize--

00:33:43.850 --> 00:33:46.365
of computing Fibonacci
numbers.

00:33:46.365 --> 00:33:49.960
So this is exponential
time algorithm.

00:33:49.960 --> 00:33:52.990
And you all know the linear
time algorithm, which is

00:33:52.990 --> 00:33:55.440
basically computed up
from the bottom.

00:33:55.440 --> 00:33:58.210
And some of you probably know
there's a logarithmic time

00:33:58.210 --> 00:34:00.910
algorithm based on squaring
matrices.

00:34:00.910 --> 00:34:02.160
Two by two matrices.

00:34:05.030 --> 00:34:12.670
So in any case, we're all
about performance here.

00:34:12.670 --> 00:34:15.820
But obviously, this is a really
poor choice to do

00:34:15.820 --> 00:34:16.330
performance on.

00:34:16.330 --> 00:34:19.570
But it is a good didactic
example, because it's so the

00:34:19.570 --> 00:34:24.409
structure and the issues that
you get into in doing this

00:34:24.409 --> 00:34:28.639
with a very simple program that
I can fit on a slide.

00:34:28.639 --> 00:34:28.909
OK.

00:34:28.909 --> 00:34:33.219
So when you execute Fibonacci,
when you call Fib of 4, it

00:34:33.219 --> 00:34:36.469
calls Fib of 3 and Fib of 2.

00:34:36.469 --> 00:34:39.570
And Fib of 3 calls Fib
of 2 and Fib of 1.

00:34:39.570 --> 00:34:42.489
And Fib of 1 just returns Fib
of 2, calls [UNINTELLIGIBLE]

00:34:42.489 --> 00:34:44.060
1, 0, et cetera.

00:34:44.060 --> 00:34:49.659
And so basically, you get an
execution trace that basically

00:34:49.659 --> 00:34:53.270
corresponds to walk
of this tree.

00:34:53.270 --> 00:34:57.720
So if you were doing this in C,
you'd basically call this,

00:34:57.720 --> 00:34:58.770
call this, call this.

00:34:58.770 --> 00:34:59.670
Get a value return.

00:34:59.670 --> 00:35:00.550
Call this.

00:35:00.550 --> 00:35:02.930
Add the two values together.

00:35:02.930 --> 00:35:04.540
Return here.

00:35:04.540 --> 00:35:05.170
Call this.

00:35:05.170 --> 00:35:06.330
Add the two values together.

00:35:06.330 --> 00:35:07.780
Call the return there.

00:35:07.780 --> 00:35:08.260
And so forth.

00:35:08.260 --> 00:35:12.190
You walk that using a stack, a
call stack, in the execution.

00:35:15.240 --> 00:35:19.550
The key idea for parallelization
is, well, gee.

00:35:19.550 --> 00:35:23.390
Fib of n minus 1 and fib of n
minus 2 are really, in this

00:35:23.390 --> 00:35:26.420
calculation, completely
independently calculated.

00:35:26.420 --> 00:35:27.940
So let's just do them
at the same time.

00:35:31.040 --> 00:35:35.590
And they can be executed at
the same time without

00:35:35.590 --> 00:35:37.350
interference, because
all they're doing is

00:35:37.350 --> 00:35:38.290
basing it on n.

00:35:38.290 --> 00:35:41.220
They're not using any shared
memory or anything even for

00:35:41.220 --> 00:35:43.800
this particular program.

00:35:43.800 --> 00:35:45.450
So let's take a look,
to begin with, how

00:35:45.450 --> 00:35:48.320
Pthreads might do this.

00:35:48.320 --> 00:35:56.090
So Pthreads is a standard that
ANSI and the IEEE have

00:35:56.090 --> 00:35:58.550
established for--

00:35:58.550 --> 00:36:00.960
and I actually believe this is
a little bit out of date.

00:36:00.960 --> 00:36:03.700
I believe there's now
a 2010 version.

00:36:03.700 --> 00:36:05.870
I'm not sure.

00:36:05.870 --> 00:36:07.990
But I recall that they were
working on a new version.

00:36:07.990 --> 00:36:10.330
But anyway, this is a recent
enough standard.

00:36:10.330 --> 00:36:13.520
It's a standard that has been
revised over the years, the

00:36:13.520 --> 00:36:15.980
so-called POSIX standard.

00:36:15.980 --> 00:36:21.020
So you'll hear, Pthreads is
basically POSIX threads.

00:36:21.020 --> 00:36:23.530
It's basically what you might
characterize as a do it

00:36:23.530 --> 00:36:25.920
yourself concurrency platform.

00:36:25.920 --> 00:36:30.370
It's kind of like assembly
language for parallelism.

00:36:30.370 --> 00:36:34.000
It allows you to do the things
you need to do, but you're

00:36:34.000 --> 00:36:38.230
sort of doing it all by hand,
one step at a time.

00:36:38.230 --> 00:36:42.190
It's built as a library of
functions with special non-C

00:36:42.190 --> 00:36:43.440
or C++ semantics.

00:36:50.760 --> 00:36:53.650
And we'll look at what some
of those semantics are.

00:36:53.650 --> 00:36:57.670
Each thread implements an
abstraction of a processor,

00:36:57.670 --> 00:37:01.640
which are multiplexed onto the
machine resources by the

00:37:01.640 --> 00:37:05.700
Pthread runtime implementation.

00:37:05.700 --> 00:37:08.800
Threads communicate through
shared memory.

00:37:08.800 --> 00:37:13.090
And library functions mask
the protocols involved in

00:37:13.090 --> 00:37:15.680
interthread coordination.

00:37:15.680 --> 00:37:20.290
So you can start up threads, et
cetera, and their library

00:37:20.290 --> 00:37:21.230
function for doing that.

00:37:21.230 --> 00:37:23.310
So let's just see
how that works.

00:37:23.310 --> 00:37:25.860
So here are, basically, the two

00:37:25.860 --> 00:37:29.800
important Pthread functions.

00:37:29.800 --> 00:37:31.560
There are actually a whole bunch
of them, because they

00:37:31.560 --> 00:37:34.730
also provide a bunch of
other facilities.

00:37:34.730 --> 00:37:38.200
One is pthread_create, which
creates Pthread.

00:37:38.200 --> 00:37:39.450
And one is pthread_join.

00:37:41.620 --> 00:37:49.990
So pthread_create basically
is return an identifier.

00:37:49.990 --> 00:37:53.160
So when you say create a
Pthread, the Pthread system

00:37:53.160 --> 00:37:55.860
says, here's a handle by which
you can name this thread in

00:37:55.860 --> 00:37:57.420
the future.

00:37:57.420 --> 00:37:57.660
OK.

00:37:57.660 --> 00:38:00.490
So it's a very common thing
that the implementer says,

00:38:00.490 --> 00:38:01.730
here's the name that you get.

00:38:01.730 --> 00:38:02.850
It's called a handle.

00:38:02.850 --> 00:38:05.640
So it returns a handle.

00:38:05.640 --> 00:38:12.020
It then has an object to set
various thread attributes.

00:38:12.020 --> 00:38:14.250
And for most of what we're
going to need, we're just

00:38:14.250 --> 00:38:15.860
going to need NULL
for default.

00:38:15.860 --> 00:38:18.390
We don't need any special
things like changing the

00:38:18.390 --> 00:38:21.720
priority or what have you.

00:38:21.720 --> 00:38:28.390
Then what you pass is a void*
pointer to a function, which

00:38:28.390 --> 00:38:32.360
is going to be the routine
executed after creation.

00:38:32.360 --> 00:38:35.310
So you can name the function
that you want to have it

00:38:35.310 --> 00:38:36.560
operate on.

00:38:39.220 --> 00:38:42.290
And then you have a single
pointer to an argument that

00:38:42.290 --> 00:38:43.700
you're going to pass
to the function.

00:38:46.710 --> 00:38:49.860
So when you call something with
Pthreads to create them,

00:38:49.860 --> 00:38:53.070
you can't say, and here's
my list of arguments.

00:38:53.070 --> 00:38:55.610
If you have more than one
argument, you have to pack it

00:38:55.610 --> 00:38:58.670
together into a struct
and pass the

00:38:58.670 --> 00:39:00.380
pointer to the struct.

00:39:00.380 --> 00:39:02.830
And this function has to be
smart enough to understand how

00:39:02.830 --> 00:39:04.470
to unpack it.

00:39:04.470 --> 00:39:07.150
We'll see an example
in a minute.

00:39:07.150 --> 00:39:09.620
And then, it returns
an error status.

00:39:09.620 --> 00:39:11.810
So the most common thing people
do is they don't bother

00:39:11.810 --> 00:39:14.140
to check the error status.

00:39:14.140 --> 00:39:14.610
OK.

00:39:14.610 --> 00:39:16.950
And yet sometimes, you try to
create a Pthread, there's a

00:39:16.950 --> 00:39:18.810
reason it can't create one.

00:39:18.810 --> 00:39:21.300
And now you keep going thinking
you have one, and

00:39:21.300 --> 00:39:24.640
then your program crashes
and you wonder why.

00:39:24.640 --> 00:39:26.990
So when you create things,
you should check.

00:39:26.990 --> 00:39:31.540
I'm not sure in my code here
whether I checked everywhere.

00:39:31.540 --> 00:39:33.700
But you should check.

00:39:33.700 --> 00:39:36.720
Do as I say, not as I do.

00:39:36.720 --> 00:39:37.670
OK.

00:39:37.670 --> 00:39:40.280
So the other key function
is join.

00:39:40.280 --> 00:39:43.310
And basically, what you do is
you say, you name the thread

00:39:43.310 --> 00:39:44.900
that you want to wait for.

00:39:44.900 --> 00:39:46.680
This is the name that
would be returned

00:39:46.680 --> 00:39:49.700
by the create function.

00:39:49.700 --> 00:39:57.860
And you also give a place where
it can store the status

00:39:57.860 --> 00:40:01.000
of the thread when
it terminated.

00:40:01.000 --> 00:40:03.290
It's allowed to say, I
terminated normally.

00:40:03.290 --> 00:40:05.950
I terminated with a given error
condition or whatever.

00:40:05.950 --> 00:40:07.430
But if you don't care
what it is, you

00:40:07.430 --> 00:40:08.990
just put in NULL there.

00:40:08.990 --> 00:40:10.610
And then it returns
to the error

00:40:10.610 --> 00:40:13.230
status of the join function.

00:40:13.230 --> 00:40:15.560
So those are the two functions
that you program with.

00:40:15.560 --> 00:40:16.408
Question?

00:40:16.408 --> 00:40:17.658
AUDIENCE: [INAUDIBLE PHRASE]?

00:40:21.090 --> 00:40:22.170
PROFESSOR: It's different.

00:40:22.170 --> 00:40:22.690
It's different.

00:40:22.690 --> 00:40:26.350
So it's basically, if the error
status, if it returns

00:40:26.350 --> 00:40:29.170
NULL, it just means everything
went OK.

00:40:33.710 --> 00:40:37.800
The handle is you pass a name,
and basically this is *thread.

00:40:37.800 --> 00:40:41.790
It stuffs the name into
whatever you give it.

00:40:41.790 --> 00:40:44.070
OK so you're not saying,
here's the name.

00:40:44.070 --> 00:40:47.430
This is returned as an
output parameter.

00:40:47.430 --> 00:40:52.560
So you're giving it an address
of some place to put the name.

00:40:52.560 --> 00:40:52.690
OK.

00:40:52.690 --> 00:40:54.270
Let's see an example.

00:40:54.270 --> 00:40:59.840
So here's Fibonacci
with Pthreads.

00:40:59.840 --> 00:41:02.280
So let's just go through that.

00:41:02.280 --> 00:41:06.330
So the first part
is pretty good.

00:41:06.330 --> 00:41:11.750
This is your original code
that does Fibonacci.

00:41:11.750 --> 00:41:15.930
And now what we do is
we have a structure

00:41:15.930 --> 00:41:17.750
for the thread arguments.

00:41:17.750 --> 00:41:20.110
And so we're going to have an
input argument and an output

00:41:20.110 --> 00:41:21.500
argument in this example.

00:41:21.500 --> 00:41:23.980
Because Fib takes an input
argument in and

00:41:23.980 --> 00:41:27.280
returns Fib of n.

00:41:27.280 --> 00:41:29.180
So we're going to call those
input and output.

00:41:29.180 --> 00:41:31.570
And we'll call them
thread_args.

00:41:31.570 --> 00:41:37.660
And now, here is my void*
function, thread_func, which

00:41:37.660 --> 00:41:39.790
takes a pointer.

00:41:39.790 --> 00:41:43.980
And what it does is
when it executes--

00:41:43.980 --> 00:41:46.070
so what you're going to be able
to do is, as we'll see in

00:41:46.070 --> 00:41:47.300
a minute--.

00:41:47.300 --> 00:41:48.910
Let me just go through this.

00:41:48.910 --> 00:41:50.610
This is going to be the function
called when the

00:41:50.610 --> 00:41:52.140
thread is created.

00:41:52.140 --> 00:41:53.211
So when the thread is created,
you're just going

00:41:53.211 --> 00:41:54.990
to call this function.

00:41:54.990 --> 00:42:00.150
And what it's going to get is
the argument that was passed,

00:42:00.150 --> 00:42:03.660
which is this *star thing.

00:42:03.660 --> 00:42:06.050
And what it does in this case
is it's basically going to

00:42:06.050 --> 00:42:12.140
cast the pointer to a thread_arg
struct and

00:42:12.140 --> 00:42:16.570
dereference the input, and stick
that into I. Then going

00:42:16.570 --> 00:42:19.710
to compute Fib of I. And then
it's going to take, once

00:42:19.710 --> 00:42:24.100
again, deference the pointer as
if it's a thread_arg, and

00:42:24.100 --> 00:42:29.190
store into the output field
the result of the Fib.

00:42:29.190 --> 00:42:30.590
And then it returns NULL.

00:42:34.060 --> 00:42:36.170
So that's basically the function
that's going to be

00:42:36.170 --> 00:42:37.910
called when the thread
is created.

00:42:37.910 --> 00:42:43.560
So in your main routine now,
what happens is we initialize

00:42:43.560 --> 00:42:44.400
a bunch of things.

00:42:44.400 --> 00:42:48.350
And now, if argc is less
than 2, we'll return 1.

00:42:48.350 --> 00:42:50.860
That's fine.

00:42:50.860 --> 00:42:54.850
Then we're going to get the
reading that we fail.

00:42:54.850 --> 00:42:56.280
That's actually the reading
of the input.

00:42:56.280 --> 00:43:00.220
So then, what we do here is we
get n from the command line.

00:43:00.220 --> 00:43:03.430
And then if n is less than
30, we're just going to

00:43:03.430 --> 00:43:05.710
compute Fib of n.

00:43:05.710 --> 00:43:10.680
This is what I evaluated on my
laptop was a good number.

00:43:10.680 --> 00:43:13.870
So the idea is there's no point
in creating the extra

00:43:13.870 --> 00:43:17.740
thread to do the work if it's
going to be more expensive

00:43:17.740 --> 00:43:19.710
than me just doing
the work myself.

00:43:19.710 --> 00:43:23.150
So I looked at the overhead of
thread creation and discovered

00:43:23.150 --> 00:43:27.420
that if it was smaller than 30,
it's going to be slower to

00:43:27.420 --> 00:43:30.780
create another thread
to help me out.

00:43:30.780 --> 00:43:33.780
It's sort of like you folks
when you're doing pair

00:43:33.780 --> 00:43:35.850
programming, which you're
supposed to be doing, versus

00:43:35.850 --> 00:43:36.990
handing it off.

00:43:36.990 --> 00:43:38.790
Sometimes, there are some things
that are too small to

00:43:38.790 --> 00:43:40.920
ask somebody else to do.

00:43:40.920 --> 00:43:43.897
You might as well just do it,
by time you explain what it

00:43:43.897 --> 00:43:45.310
is, and so forth.

00:43:45.310 --> 00:43:47.160
Same thing here.

00:43:47.160 --> 00:43:49.710
What's the point in starting
up a thread to do something

00:43:49.710 --> 00:43:53.630
else, because the startup cost
is rather substantial.

00:43:53.630 --> 00:43:56.180
So if it's less than 30, well,
we'll just be done.

00:43:56.180 --> 00:44:01.120
Otherwise, what we do
is we marshall the

00:44:01.120 --> 00:44:02.300
argument to the thread.

00:44:02.300 --> 00:44:06.370
We basically set args.input
to n minus 1.

00:44:06.370 --> 00:44:08.860
Because args is going to be
what I'm going to pass in.

00:44:08.860 --> 00:44:11.700
So I say the input number
is n minus 1.

00:44:11.700 --> 00:44:17.520
And now what I do is I create
the thread by saying, give me

00:44:17.520 --> 00:44:22.840
the name of the thread
that I'm creating.

00:44:22.840 --> 00:44:28.520
This was the field that I said
you could put to be NULL,

00:44:28.520 --> 00:44:30.710
which basically lets
you set some policy

00:44:30.710 --> 00:44:32.370
parameters and so forth.

00:44:32.370 --> 00:44:34.470
I say, execute the
thread_func.

00:44:34.470 --> 00:44:36.000
This guy here.

00:44:36.000 --> 00:44:38.650
And here's the argument list
that I want to provide it,

00:44:38.650 --> 00:44:42.000
which is this args thing.

00:44:42.000 --> 00:44:44.950
Once you do the thread_create,
and this is where you depart

00:44:44.950 --> 00:44:48.420
from normal C or
C++ semantics.

00:44:48.420 --> 00:44:51.000
And in fact, we're going to be
doing more moving in the

00:44:51.000 --> 00:44:52.260
direction of C++.

00:44:52.260 --> 00:44:57.240
We'll have some tutorials
on that.

00:44:57.240 --> 00:45:00.200
What happens is we
check the status.

00:45:00.200 --> 00:45:03.200
OK, I actually did check the
status to see whether or not

00:45:03.200 --> 00:45:05.550
it created it properly.

00:45:05.550 --> 00:45:09.240
But basically now, what's
happening is after I execute

00:45:09.240 --> 00:45:13.520
this, it goes off and all the
magic in Pthreads starts

00:45:13.520 --> 00:45:16.370
another thread doing
that computation.

00:45:16.370 --> 00:45:19.910
And control returns to the
statement after the

00:45:19.910 --> 00:45:21.850
pthread_create.

00:45:21.850 --> 00:45:25.230
So when the pthread_create
returns, that doesn't mean

00:45:25.230 --> 00:45:28.150
it's done computing the thing
you told it to do.

00:45:28.150 --> 00:45:30.420
Then, what would be the point?

00:45:30.420 --> 00:45:35.230
It returns after it's set up
to operate in parallel the

00:45:35.230 --> 00:45:36.650
other thread.

00:45:36.650 --> 00:45:38.110
People follow that?

00:45:38.110 --> 00:45:41.480
So now at this point, there
are two threads operating.

00:45:41.480 --> 00:45:43.060
There's the thread we've
called thread.

00:45:43.060 --> 00:45:45.260
And there's whatever the name
of the thread is that we

00:45:45.260 --> 00:45:46.510
started on.

00:45:48.740 --> 00:45:52.510
So then we, in our own processor
here, we compute Fib

00:45:52.510 --> 00:45:54.610
of N minus 2.

00:45:54.610 --> 00:45:58.960
And now, what we do is we go
on to join this thread with

00:45:58.960 --> 00:46:04.220
the thread that we
had created.

00:46:08.130 --> 00:46:10.740
So let's see here.

00:46:10.740 --> 00:46:13.810
And the thing that the join does
is if the other thread

00:46:13.810 --> 00:46:17.620
isn't done, it sits there and
waits until it is done.

00:46:17.620 --> 00:46:19.050
And it does that
synchronization

00:46:19.050 --> 00:46:20.420
automatically for you.

00:46:20.420 --> 00:46:21.530
And this is the kind of thing a

00:46:21.530 --> 00:46:23.130
concurrency platform provides.

00:46:23.130 --> 00:46:28.250
It provides the coordination
under the covers for you to be

00:46:28.250 --> 00:46:31.960
able to synchronize with it
without you having to

00:46:31.960 --> 00:46:34.930
synchronize on your own.

00:46:34.930 --> 00:46:41.400
And then, once it does return,
it adds the results together

00:46:41.400 --> 00:46:46.710
by taking the result which came
from the Fib of n minus 2

00:46:46.710 --> 00:46:50.750
and adds to it the value that
this thread has returned in

00:46:50.750 --> 00:46:52.000
the args.output.

00:46:54.660 --> 00:46:57.420
And then it prints the result.

00:46:57.420 --> 00:46:59.910
So any question about that?

00:46:59.910 --> 00:47:02.860
Wouldn't this be fun to write
a really big system in?

00:47:02.860 --> 00:47:04.230
People do.

00:47:04.230 --> 00:47:05.480
People do.

00:47:05.480 --> 00:47:07.928
Yeah, question?

00:47:07.928 --> 00:47:09.178
AUDIENCE: [INAUDIBLE PHRASE]

00:47:13.540 --> 00:47:14.795
PROFESSOR: That's a
tuning parameter.

00:47:14.795 --> 00:47:15.595
That's a voodoo parameter.

00:47:15.595 --> 00:47:15.930
AUDIENCE: Right.

00:47:15.930 --> 00:47:19.374
But in this particular case, it
makes no difference at all.

00:47:19.374 --> 00:47:23.310
It would've made a difference
if it was an actual person

00:47:23.310 --> 00:47:23.802
[INAUDIBLE]?

00:47:23.802 --> 00:47:25.290
PROFESSOR: No, it does
make a difference.

00:47:25.290 --> 00:47:27.080
For how fast it computes this?

00:47:27.080 --> 00:47:28.170
Absolutely does.

00:47:28.170 --> 00:47:29.530
AUDIENCE: That's
not recursive?

00:47:29.530 --> 00:47:30.050
PROFESSOR: No, that's right.

00:47:30.050 --> 00:47:30.920
This is not recursive.

00:47:30.920 --> 00:47:33.238
I'm just doing two things
and then quitting.

00:47:33.238 --> 00:47:36.202
AUDIENCE: [INAUDIBLE] if it's
less than 30, then it's going

00:47:36.202 --> 00:47:39.390
to be [INAUDIBLE], right?

00:47:39.390 --> 00:47:41.660
PROFESSOR: If it's less than
30, it's fast enough that I

00:47:41.660 --> 00:47:43.175
might as well just return.

00:47:43.175 --> 00:47:46.025
AUDIENCE: Then why
[INAUDIBLE PHRASE]

00:47:46.025 --> 00:47:46.975
to do it.

00:47:46.975 --> 00:47:49.350
It would return [INAUDIBLE]
too.

00:47:49.350 --> 00:47:49.560
PROFESSOR: No.

00:47:49.560 --> 00:47:51.470
But it would be slower.

00:47:51.470 --> 00:47:52.645
It would be wasteful
of resources.

00:47:52.645 --> 00:47:53.434
Maybe somebody--

00:47:53.434 --> 00:47:56.338
AUDIENCE: Well, because you're
using such a bad

00:47:56.338 --> 00:47:56.822
algorithm, I guess?

00:47:56.822 --> 00:47:57.306
PROFESSOR: Yeah.

00:47:57.306 --> 00:47:57.790
AUDIENCE: Oh, I see.

00:47:57.790 --> 00:47:58.280
Oh, OK.

00:47:58.280 --> 00:47:59.430
PROFESSOR: OK.

00:47:59.430 --> 00:48:02.410
So in any case, that's Pthread's
programming.

00:48:02.410 --> 00:48:03.450
There are a bunch of issues.

00:48:03.450 --> 00:48:08.090
One is that the overhead of
creating a thread is more than

00:48:08.090 --> 00:48:10.220
10,000 cycles.

00:48:10.220 --> 00:48:13.130
So it leaves you to only be able
to do very coarse grain

00:48:13.130 --> 00:48:13.760
concurrency.

00:48:13.760 --> 00:48:15.590
There are some tricks
around that.

00:48:15.590 --> 00:48:17.870
One is to use what's called
thread pools.

00:48:17.870 --> 00:48:21.600
What I do is I start up, and I
create a bunch of threads.

00:48:21.600 --> 00:48:22.560
And I have their names.

00:48:22.560 --> 00:48:23.810
I put them in a link list.

00:48:23.810 --> 00:48:26.270
And whenever I need to create
one, rather than actually

00:48:26.270 --> 00:48:29.090
creating one, I take one out of
the list, much as I would

00:48:29.090 --> 00:48:30.820
do memory allocation.

00:48:30.820 --> 00:48:32.310
Which you folks are
familiar with.

00:48:35.580 --> 00:48:36.050
OK.

00:48:36.050 --> 00:48:38.490
Ha, ha, ha, ha, ha.

00:48:38.490 --> 00:48:45.340
[MANIACAL LAUGHTER]

00:48:45.340 --> 00:48:48.960
So basically, you can have
a free list of threads.

00:48:48.960 --> 00:48:53.550
And when you need a thread,
you grab the thread.

00:48:53.550 --> 00:48:57.750
The second thing
is scalability.

00:48:57.750 --> 00:49:01.580
So this code gets about a 1.5
speed up for two cores.

00:49:01.580 --> 00:49:05.450
If I want to use three cores
or four cores, what

00:49:05.450 --> 00:49:07.470
do I have to do?

00:49:07.470 --> 00:49:09.210
Rewrite the whole program.

00:49:09.210 --> 00:49:12.490
This program only works
for two cores.

00:49:12.490 --> 00:49:14.780
It will also work
for one core.

00:49:14.780 --> 00:49:17.600
but basically, it doesn't
really exploit

00:49:17.600 --> 00:49:20.550
three or four cores.

00:49:20.550 --> 00:49:22.300
It's really bad for
modulatary.

00:49:22.300 --> 00:49:25.670
The Fibonacci logic is no longer
neatly encapsulated in

00:49:25.670 --> 00:49:28.510
the Fib function.

00:49:28.510 --> 00:49:31.320
So where do we see if we
go back to this code?

00:49:31.320 --> 00:49:32.910
Here's the Fib function.

00:49:32.910 --> 00:49:35.410
Oh, but now, I've
kind of got--

00:49:35.410 --> 00:49:37.510
well, this is sort of just
marshaling and calling.

00:49:37.510 --> 00:49:41.570
But over here, oh my goodness,
I've got some arguments here.

00:49:41.570 --> 00:49:43.640
If n is less than 30,
I give a result.

00:49:43.640 --> 00:49:45.960
Otherwise, I'm adding
together--

00:49:45.960 --> 00:49:46.310
but wait a minute.

00:49:46.310 --> 00:49:48.730
I already specified
Fib up here.

00:49:48.730 --> 00:49:51.510
So I'm specifying my serial
implementation, and I'm

00:49:51.510 --> 00:49:55.000
specifying a parallel
way of doing it.

00:49:55.000 --> 00:49:56.410
And so that's not modular.

00:49:56.410 --> 00:49:59.640
If I decided I wanted to change
the Fib, I've got to

00:49:59.640 --> 00:50:01.860
change things in two places.

00:50:01.860 --> 00:50:07.840
If Fib were something I did.

00:50:07.840 --> 00:50:08.870
Code simplicity.

00:50:08.870 --> 00:50:10.280
The programmers for this are

00:50:10.280 --> 00:50:12.630
actually marshalling arguments.

00:50:12.630 --> 00:50:15.070
This is what I call
shades of 1958.

00:50:15.070 --> 00:50:18.495
What happened in 1958 that's
relevant to computer science?

00:50:21.060 --> 00:50:25.320
What was the big innovation
in 1958?

00:50:25.320 --> 00:50:27.310
Programming language.

00:50:27.310 --> 00:50:29.380
Fortran.

00:50:29.380 --> 00:50:30.820
So, Fortran.

00:50:30.820 --> 00:50:35.320
Before Fortran, people wrote
in assembly language.

00:50:35.320 --> 00:50:39.710
If you wanted to put three
arguments to a function, you

00:50:39.710 --> 00:50:43.790
did a push, push, push, or
passed them in parameters.

00:50:43.790 --> 00:50:46.720
Actually, their machines were
so much more primitive than

00:50:46.720 --> 00:50:48.680
that it was even more
complicated than you could

00:50:48.680 --> 00:50:54.100
imagine, given how complicated
it is today what the

00:50:54.100 --> 00:50:56.170
compilers are doing.

00:50:56.170 --> 00:50:57.870
But you had marshal the
arguments yourself.

00:50:57.870 --> 00:51:00.150
What Fortran did was say,
no, you can actually

00:51:00.150 --> 00:51:03.780
write f of a, b, c.

00:51:03.780 --> 00:51:06.320
Close paren.

00:51:06.320 --> 00:51:10.820
And that it will cause a, b,
and c all to be marshalled

00:51:10.820 --> 00:51:13.160
automatically for you.

00:51:13.160 --> 00:51:15.450
Well, Pthreads doesn't have that
automatic marshalling.

00:51:15.450 --> 00:51:18.300
You got to marshall by hand if
you're going to use pthreads.

00:51:21.790 --> 00:51:24.770
And of course, as you can
imagine, that was error prone.

00:51:24.770 --> 00:51:27.800
Because there is
no type safety.

00:51:27.800 --> 00:51:31.730
Are you calling things with the
right types and so forth?

00:51:31.730 --> 00:51:33.940
And so forth.

00:51:33.940 --> 00:51:39.460
And also, one of the things here
is that we've created two

00:51:39.460 --> 00:51:41.010
jobs that aren't
the same size.

00:51:41.010 --> 00:51:46.230
So there's no way that they
have of load balancing.

00:51:46.230 --> 00:51:50.070
So this is why pthreads is sort
of the assembly language

00:51:50.070 --> 00:51:53.320
level, so that you can do
anything you want in pthreads.

00:51:53.320 --> 00:51:55.790
But you have to program
at this kind of very

00:51:55.790 --> 00:51:59.500
protocol-laden level.

00:51:59.500 --> 00:52:00.600
Next thing I want
to talk about is

00:52:00.600 --> 00:52:01.850
threading building blocks.

00:52:04.700 --> 00:52:09.250
This is a technology
developed by Intel.

00:52:09.250 --> 00:52:12.930
It's implemented as a C++
library that runs on top of

00:52:12.930 --> 00:52:16.480
the native Pthreads, typically,
or WinAPI threads.

00:52:16.480 --> 00:52:21.590
So it's basically a layer on
top of the Pthread layer.

00:52:21.590 --> 00:52:23.580
In this case, the program
specifies

00:52:23.580 --> 00:52:26.400
tasks rather than threads.

00:52:26.400 --> 00:52:30.690
And tasks are automatically
load balanced across the

00:52:30.690 --> 00:52:33.540
threads using a strategy called
work-stealing, which

00:52:33.540 --> 00:52:36.640
we'll talk about a little
bit more later.

00:52:36.640 --> 00:52:38.610
And the focus for this
is on performance.

00:52:38.610 --> 00:52:43.130
They want to write programs that
actually perform well.

00:52:43.130 --> 00:52:45.700
So here's Fibonacci in TBB.

00:52:45.700 --> 00:52:48.190
So as you'll see, it's better.

00:52:48.190 --> 00:52:51.805
But maybe not ideal for what
you might like to express.

00:52:56.220 --> 00:53:02.070
So what we do is we declare the
computer, the computation,

00:53:02.070 --> 00:53:05.290
it's going to organized as a
bunch of explicit tasks.

00:53:05.290 --> 00:53:10.030
So you say that it's
going to be a task.

00:53:10.030 --> 00:53:19.280
And FibTask is going to have an
input parameter, n, and an

00:53:19.280 --> 00:53:22.720
output parameters, sum.

00:53:22.720 --> 00:53:28.990
And what we're going to do is
when the task is started, it

00:53:28.990 --> 00:53:36.890
automatically executes the
execute method of this tasking

00:53:36.890 --> 00:53:38.500
object here.

00:53:38.500 --> 00:53:40.350
And the execute method now
starts to do something that

00:53:40.350 --> 00:53:41.850
looks very much like
Fibonacci.

00:53:41.850 --> 00:53:46.880
It says if n is less than
2, sum is equal to n.

00:53:46.880 --> 00:53:48.200
That's we had before.

00:53:48.200 --> 00:53:49.550
And otherwise.

00:53:49.550 --> 00:53:53.570
And now what we're going to do
is recursively create two

00:53:53.570 --> 00:53:57.630
child tasks, which we basically
do with this

00:53:57.630 --> 00:54:07.490
function, allocate_task, giving
it the fib task a name,

00:54:07.490 --> 00:54:13.040
where this is basically a method
for allocating out of a

00:54:13.040 --> 00:54:16.150
particular type of the
pool, which is an

00:54:16.150 --> 00:54:18.760
allocate child pool.

00:54:18.760 --> 00:54:23.080
And then similarly for b, we
recursively do for n minus 2.

00:54:23.080 --> 00:54:25.240
And then what it does is
it sets the number of

00:54:25.240 --> 00:54:27.630
tasks to wait for.

00:54:27.630 --> 00:54:30.250
In this case, it's basically
two children plus 1 for

00:54:30.250 --> 00:54:32.140
bookkeeping.

00:54:32.140 --> 00:54:35.580
So this ends up always being one
more than the things that

00:54:35.580 --> 00:54:39.240
you created as subtasks.

00:54:39.240 --> 00:54:44.160
And then what we do is we
say, OK, let's spawn.

00:54:44.160 --> 00:54:46.050
So this will only
set up the task.

00:54:46.050 --> 00:54:48.070
It doesn't actually
say, do it.

00:54:48.070 --> 00:54:52.390
So the spawn command says
actually do this computation

00:54:52.390 --> 00:54:53.870
here that I set up.

00:54:53.870 --> 00:54:57.000
So it actually does b.

00:54:57.000 --> 00:54:58.440
Start task b.

00:54:58.440 --> 00:55:02.760
And then itself, it executes a
and waits for all of the other

00:55:02.760 --> 00:55:05.640
tasks, namely both a
and b, to finish.

00:55:05.640 --> 00:55:08.760
And once it's finished, it adds
the results together to

00:55:08.760 --> 00:55:10.160
produce the final output.

00:55:13.300 --> 00:55:17.600
So this, notice, has the big
advantage over the previous

00:55:17.600 --> 00:55:22.260
implementation that this
is actually recursive.

00:55:22.260 --> 00:55:26.010
So in doing Fib, you're not
just getting two tasks.

00:55:26.010 --> 00:55:29.010
You're recursively getting each
of those two more, and

00:55:29.010 --> 00:55:30.830
two more, and two more, down
to the leaves of the

00:55:30.830 --> 00:55:32.630
computation.

00:55:32.630 --> 00:55:36.660
And then what TBB does is it
load balances those across the

00:55:36.660 --> 00:55:42.450
number of available processors
by creating these tasks.

00:55:42.450 --> 00:55:45.270
And then, it automatically does
all the load balancing of

00:55:45.270 --> 00:55:47.610
the tasks and so forth.

00:55:47.610 --> 00:55:50.180
Questions about that?

00:55:50.180 --> 00:55:51.130
Any questions?

00:55:51.130 --> 00:55:55.720
I don't expect you to be able
to program a TBB, unless I

00:55:55.720 --> 00:55:57.480
gave you a book and said,
program a TBB.

00:55:57.480 --> 00:55:58.730
But I'm not going to do that.

00:56:00.900 --> 00:56:03.320
This is mainly to give you a
flavor of what's in there.

00:56:03.320 --> 00:56:05.500
What the alternatives are.

00:56:05.500 --> 00:56:08.670
So TBB provides many
C++ templates that

00:56:08.670 --> 00:56:10.150
simplify common patterns.

00:56:10.150 --> 00:56:13.020
So rather than having to write
that kind of thing for

00:56:13.020 --> 00:56:16.010
everything, for example, if
you have loop parallelism.

00:56:16.010 --> 00:56:19.380
If you have n things that you
want to have that operate

00:56:19.380 --> 00:56:23.520
parallel, you can do a parallel
four and not actually

00:56:23.520 --> 00:56:24.500
see the tasks.

00:56:24.500 --> 00:56:27.960
It covers them over and
creates the tasks

00:56:27.960 --> 00:56:32.940
automatically, so that you can
just say, for I gets 1 to n,

00:56:32.940 --> 00:56:36.220
do this to all I, and do them at
the same time essentially.

00:56:36.220 --> 00:56:39.920
And it then balances
those and so forth.

00:56:39.920 --> 00:56:42.930
It also has to things like
parallel reduce.

00:56:42.930 --> 00:56:46.880
Sometimes what you want to do
across an array is not just do

00:56:46.880 --> 00:56:48.530
something for every element
of the array.

00:56:48.530 --> 00:56:51.770
You may want to add up all the
elements into a single value.

00:56:51.770 --> 00:56:54.870
And so it basically has what's
called a reduction function.

00:56:54.870 --> 00:56:56.980
It does parallel reduce
to aggregate.

00:56:56.980 --> 00:56:59.120
And it's got various other
things, like pipelining and

00:56:59.120 --> 00:57:02.250
filtering for doing what's
called software pipelining,

00:57:02.250 --> 00:57:08.810
where you have one subsystem
that basically is going to

00:57:08.810 --> 00:57:11.230
process the data and pass
it to the next.

00:57:11.230 --> 00:57:13.270
So you're going to process it
and pass it to the next.

00:57:13.270 --> 00:57:18.810
And it allows you to set up a
software pipeline of things.

00:57:18.810 --> 00:57:22.150
It also collides with some
container classes, such as

00:57:22.150 --> 00:57:25.180
hash tables, concurrent hash
tables, that allow you to have

00:57:25.180 --> 00:57:33.670
multiple tasks beating
on a hash table.

00:57:33.670 --> 00:57:35.680
Inserting and deleting from
the hash table at the same

00:57:35.680 --> 00:57:39.790
time and a variety of mutual
exclusion library functions,

00:57:39.790 --> 00:57:42.630
including locks and
atomic updates.

00:57:42.630 --> 00:57:48.230
So it has a bunch of other
facilities that make it much

00:57:48.230 --> 00:57:50.950
easier to use than just using
the raw task interface.

00:57:54.360 --> 00:57:55.610
OpenMP.

00:57:57.220 --> 00:58:00.100
So OpenMP is a specification
produced by an industry

00:58:00.100 --> 00:58:04.290
consortium of which the
principal players--

00:58:04.290 --> 00:58:09.780
the original principal player
was Silicon Graphics, which

00:58:09.780 --> 00:58:13.160
essentially has become
less important in the

00:58:13.160 --> 00:58:14.080
industry, let's say.

00:58:14.080 --> 00:58:15.820
Put it that way.

00:58:15.820 --> 00:58:19.270
And for the most part, recently,
it's been players

00:58:19.270 --> 00:58:24.290
from Intel and Sun, which is now
no longer Sun, except that

00:58:24.290 --> 00:58:33.160
it is Sun part of Oracle, and
of IBM, and variety of other

00:58:33.160 --> 00:58:37.200
industry players.

00:58:37.200 --> 00:58:39.430
There's several compilers
available.

00:58:39.430 --> 00:58:43.860
Both open source and
proprietary, including gcc,

00:58:43.860 --> 00:58:46.190
has OpenMP built-in.

00:58:46.190 --> 00:58:51.370
And also, Visual Studio
has OpenMP built-in.

00:58:51.370 --> 00:58:55.460
These are a set of linguistic
extensions to C and C++ or

00:58:55.460 --> 00:58:59.710
Fortran in the form of compiler
practice pragmas.

00:58:59.710 --> 00:59:03.460
So who knows what a pragma is?

00:59:03.460 --> 00:59:05.150
OK.

00:59:05.150 --> 00:59:05.350
Good.

00:59:05.350 --> 00:59:06.490
Can you tell us what
a pragma is?

00:59:06.490 --> 00:59:07.740
AUDIENCE: [INAUDIBLE PHRASE]

00:59:12.140 --> 00:59:15.390
PROFESSOR: Yeah, it's kind
of like a compiler hint.

00:59:15.390 --> 00:59:18.420
It's a way of saying to the
compiler, here's something I

00:59:18.420 --> 00:59:21.970
want to tell you about the
code that I'm writing.

00:59:21.970 --> 00:59:25.150
And it basically is a hint.

00:59:25.150 --> 00:59:27.920
So technically, it's not
supposed to have any semantic

00:59:27.920 --> 00:59:31.490
impact, but rather suggest how
something might be implemented

00:59:31.490 --> 00:59:33.810
by the compiler.

00:59:33.810 --> 00:59:36.160
However, in OpenMP's case, they

00:59:36.160 --> 00:59:39.110
actually have a compiler--

00:59:39.110 --> 00:59:42.570
it does change the semantics
in certain cases.

00:59:42.570 --> 00:59:44.990
It runs on top of native threads
and it supports,

00:59:44.990 --> 00:59:46.700
especially, loop parallelism.

00:59:46.700 --> 00:59:49.050
And then, in the latest version,
it supports a kind of

00:59:49.050 --> 00:59:54.560
task parallelism like
we saw with TBB.

00:59:54.560 --> 00:59:56.270
So, in fact, their
task parallelism

00:59:56.270 --> 00:59:58.750
is fairly to specify.

00:59:58.750 --> 01:00:00.420
So here's the Fib code.

01:00:00.420 --> 01:00:03.710
So now, this is not
looking too bad.

01:00:03.710 --> 01:00:06.960
We basically inserted
a few lines here.

01:00:06.960 --> 01:00:08.440
And otherwise, we actually
have the

01:00:08.440 --> 01:00:13.530
original Fibonacci code.

01:00:13.530 --> 01:00:18.520
So the sharp pragma says, here's
a compiler directive.

01:00:18.520 --> 01:00:21.450
And it says, the OMP
says it is an

01:00:21.450 --> 01:00:24.210
OpenMP compiler directive.

01:00:24.210 --> 01:00:26.850
The task says, oh, the following
things should be

01:00:26.850 --> 01:00:30.000
interpreted as an independent
task.

01:00:30.000 --> 01:00:33.760
And now, the sharing of memory
in OpenMP is managed

01:00:33.760 --> 01:00:35.760
explicitly, because they're
trying to allow for

01:00:35.760 --> 01:00:39.360
programming both of distributed
memory clusters,

01:00:39.360 --> 01:00:43.000
as well as shared
memory machines.

01:00:43.000 --> 01:00:48.020
And so, you have to explicitly
name the shared variables that

01:00:48.020 --> 01:00:50.000
you're using.

01:00:50.000 --> 01:00:52.740
And here, we're basically
saying, wait for the two

01:00:52.740 --> 01:00:56.180
things that we spawned
off here to complete.

01:00:56.180 --> 01:01:00.430
So pretty simple code.

01:01:00.430 --> 01:01:05.250
It provides many pragma
directives to express common

01:01:05.250 --> 01:01:08.990
patterns, such as a parallel
for parallelization.

01:01:08.990 --> 01:01:10.230
It also has reduction.

01:01:10.230 --> 01:01:14.490
It also has directives for
scheduling and data sharing.

01:01:14.490 --> 01:01:16.360
And it has a whole bunch
of synchronization

01:01:16.360 --> 01:01:18.010
constructs and so forth.

01:01:18.010 --> 01:01:21.650
So it's another interesting
one to do.

01:01:21.650 --> 01:01:24.370
The main downside, I would say,
of OpenMP is that the

01:01:24.370 --> 01:01:27.990
performance is not really
very composable.

01:01:27.990 --> 01:01:30.660
So if you have a program you've
written with OpenMP

01:01:30.660 --> 01:01:33.090
over here, another one here,
and you want to put them

01:01:33.090 --> 01:01:37.380
together, they fight
with each other.

01:01:37.380 --> 01:01:40.310
You have to have your
concept of what are

01:01:40.310 --> 01:01:42.350
going to be the programs.

01:01:42.350 --> 01:01:45.250
The task parallelism helps
a bit with that.

01:01:45.250 --> 01:01:49.410
But the basic OpenMP is very
much of the model, I know how

01:01:49.410 --> 01:01:50.800
many cores I'm running on.

01:01:50.800 --> 01:01:52.350
I can set that.

01:01:52.350 --> 01:01:55.430
And then I can have it
automatically parse up the

01:01:55.430 --> 01:01:56.820
work for those many.

01:01:56.820 --> 01:02:00.310
But once you've done that, some
other job, some other

01:02:00.310 --> 01:02:03.170
part of the system that wants to
do the same thing, then you

01:02:03.170 --> 01:02:07.490
get oversubscription and perhaps
some [UNINTELLIGIBLE].

01:02:07.490 --> 01:02:10.970
Nevertheless, a very
interesting system.

01:02:10.970 --> 01:02:14.960
And very accessible, because
it's in most of the standard

01:02:14.960 --> 01:02:16.210
compilers these days.

01:02:19.090 --> 01:02:23.130
What we're going to
look at is Cilk++.

01:02:23.130 --> 01:02:28.740
So this is actually a small set
of linguistics extensions

01:02:28.740 --> 01:02:31.320
to C++ to support fork-join
parallelism.

01:02:31.320 --> 01:02:33.890
And it was developed by Cilk
Arts, which is an MIT

01:02:33.890 --> 01:02:38.050
spin-off, which was acquired
by Intel last year.

01:02:38.050 --> 01:02:40.790
So this is now an Intel
technology.

01:02:40.790 --> 01:02:43.770
And the reason I know about it
is because I was the founder

01:02:43.770 --> 01:02:44.680
of Cilk Arts.

01:02:44.680 --> 01:02:48.300
It was based on 15 years of
research at MIT out of my

01:02:48.300 --> 01:02:50.940
research group.

01:02:50.940 --> 01:02:55.850
And we won a bunch of awards,
actually, for this work.

01:02:55.850 --> 01:02:59.490
In fact, the work-stealing
scheduler that's in it is

01:02:59.490 --> 01:03:00.500
provably efficient.

01:03:00.500 --> 01:03:02.440
In other words, it's not just
a heuristic scheduler.

01:03:02.440 --> 01:03:05.080
It's actually got a mathematical
proof that it's

01:03:05.080 --> 01:03:06.480
an effective scheduler.

01:03:06.480 --> 01:03:10.200
And in fact, was the inspiration
for things like

01:03:10.200 --> 01:03:14.090
the work-stealing in TBB and the
new task mechanisms and so

01:03:14.090 --> 01:03:19.640
forth in OpenMP, as well as a
bunch of other people who've

01:03:19.640 --> 01:03:21.360
done work-stealing.

01:03:21.360 --> 01:03:24.520
It in addition provides a
hyperobject library for

01:03:24.520 --> 01:03:27.140
parallelizing code with global
variables, which we'll talk

01:03:27.140 --> 01:03:28.120
about later.

01:03:28.120 --> 01:03:32.720
And it includes two tools that
you'll come to know and love.

01:03:32.720 --> 01:03:35.570
One is the Cilkscreen race
detector, and the other is the

01:03:35.570 --> 01:03:39.460
Cilkview scalability analyzer.

01:03:39.460 --> 01:03:41.890
Now, what we're going to be
using in this class is going

01:03:41.890 --> 01:03:49.580
to be the Cilk++ technology
that was developed at Cilk

01:03:49.580 --> 01:03:51.400
Arts and then massaged
a little bit

01:03:51.400 --> 01:03:52.630
when it got to Intel.

01:03:52.630 --> 01:03:55.990
There is a brand new Intel
technology with Cilk built

01:03:55.990 --> 01:03:58.030
into their compiler.

01:03:58.030 --> 01:04:02.000
And it is due to come out
in like, two weeks.

01:04:05.480 --> 01:04:08.830
So our timing for this was it
would've been nice to have you

01:04:08.830 --> 01:04:13.510
folks on the new Intel
Cilk+ technology.

01:04:13.510 --> 01:04:16.950
But we're going to go with
this one for now.

01:04:16.950 --> 01:04:19.690
It's not going to make too big
a difference to you folks.

01:04:19.690 --> 01:04:22.190
But you should just be aware
that coming down the pike,

01:04:22.190 --> 01:04:27.430
there's actually some much
more cleanly integrated

01:04:27.430 --> 01:04:33.120
technology that you can use
that's in the Intel compiler.

01:04:33.120 --> 01:04:36.670
So here's how we do nested
parallelism in Cilk++.

01:04:36.670 --> 01:04:38.420
So basically, this
is Fibonacci.

01:04:38.420 --> 01:04:42.580
And now, what I have here is,
if you notice, I've got two

01:04:42.580 --> 01:04:46.430
keywords, cilk_spawn
and cilk_sync.

01:04:46.430 --> 01:04:50.160
And this is how you write
parallel Fibonacci in Cilk.

01:04:50.160 --> 01:04:51.825
This is it.

01:04:51.825 --> 01:04:56.200
I've inserted two key words,
and my program is parallel.

01:04:56.200 --> 01:05:00.350
The cilk_spawn keyword says that
the named child function

01:05:00.350 --> 01:05:03.260
can execute in parallel with
the parent caller.

01:05:03.260 --> 01:05:06.070
So when you say x equals
cilk_spawn or Fib of n minus

01:05:06.070 --> 01:05:08.660
1, it does the same thing
that you normally think.

01:05:08.660 --> 01:05:09.910
It calls the child.

01:05:12.810 --> 01:05:16.340
But after it calls the child,
rather than waiting for it to

01:05:16.340 --> 01:05:21.360
return, it goes on to
the next statement.

01:05:21.360 --> 01:05:24.650
So then, the statement y equals
Fib of n minus 2 is

01:05:24.650 --> 01:05:26.960
going on at the same time
as the calculation of

01:05:26.960 --> 01:05:28.210
Fib of n minus 1.

01:05:30.730 --> 01:05:34.560
And then, the cilk_sync says,
don't go past this point until

01:05:34.560 --> 01:05:36.390
all the children you've spawned
off have returned.

01:05:39.530 --> 01:05:44.580
And since this is a recursive
program, it generates gobs of

01:05:44.580 --> 01:05:47.130
parallelism, if it's
a big thing.

01:05:47.130 --> 01:05:50.720
So one of the key things about
Cilk++, is unlike Pthreads--

01:05:50.720 --> 01:05:54.490
Pthreads, when you say,
pthread_create, it actually

01:05:54.490 --> 01:05:57.080
goes and creates a
piece of work.

01:05:57.080 --> 01:06:02.630
In Cilk++, these keywords
only grant permission.

01:06:02.630 --> 01:06:06.050
They say you may execute these
things in parallel.

01:06:06.050 --> 01:06:08.630
It doesn't insist that they
be executed in parallel.

01:06:08.630 --> 01:06:11.880
The program may decide, no, in
fact, I'm going to just call

01:06:11.880 --> 01:06:15.190
this, and then return, and
then execute this.

01:06:18.320 --> 01:06:25.550
So it only grants permission,
and the Cilk++ runtime system

01:06:25.550 --> 01:06:28.690
figures out how to load balance
it and schedule it.

01:06:31.260 --> 01:06:36.590
Cilk++ also supports
loop parallelism.

01:06:36.590 --> 01:06:39.360
So here's an example of an
in-place matrix transpose.

01:06:39.360 --> 01:06:42.830
So I want to take this matrix
and flip it on its major axis.

01:06:45.330 --> 01:06:47.480
And we can do it
with for loops.

01:06:47.480 --> 01:06:49.040
As you know, for loops
are not the best way

01:06:49.040 --> 01:06:50.270
to do matrix transpose.

01:06:50.270 --> 01:06:53.070
Right?

01:06:53.070 --> 01:06:56.090
It's better to do divide
and conquer.

01:06:56.090 --> 01:07:00.640
But here's how you
could do it.

01:07:00.640 --> 01:07:04.240
And here, I made the indices
run from 0, not 1, because

01:07:04.240 --> 01:07:05.610
that's the way you do
it in programming.

01:07:05.610 --> 01:07:08.150
But if I did it up here, then
these things get to be n minus

01:07:08.150 --> 01:07:10.810
1, n minus 1, and then it gets
too crowded on the slide.

01:07:10.810 --> 01:07:15.030
And I said, OK, I'll just put
a comment there rather than

01:07:15.030 --> 01:07:17.450
try to sort it out.

01:07:17.450 --> 01:07:21.150
So here's what I'm saying, is
this outer loop is parallel.

01:07:21.150 --> 01:07:24.340
It's going from 1
to n minus 1.

01:07:24.340 --> 01:07:26.620
And saying, do all those
things in parallel.

01:07:26.620 --> 01:07:29.290
And each one is going through
a different number of

01:07:29.290 --> 01:07:30.390
iterations of j.

01:07:30.390 --> 01:07:33.240
So you can see you actually need
some load balancing here,

01:07:33.240 --> 01:07:36.760
because some of these are going
through just one step,

01:07:36.760 --> 01:07:39.520
and some are going through
n minus 1 steps.

01:07:39.520 --> 01:07:43.440
It's basically the amount of
work in every iteration of the

01:07:43.440 --> 01:07:47.310
outer loop here is different.

01:07:47.310 --> 01:07:47.810
I'm sorry?

01:07:47.810 --> 01:07:50.130
AUDIENCE: [INAUDIBLE PHRASE].

01:07:50.130 --> 01:07:53.610
PROFESSOR: No. i equals 1 is
where you want to start.

01:07:53.610 --> 01:07:54.995
Because you don't have
to move the diagonal.

01:07:58.170 --> 01:08:01.750
You only have to go across
the top here.

01:08:01.750 --> 01:08:07.170
And for each of those, copy it
into the appropriate column.

01:08:07.170 --> 01:08:08.790
Flip it into the appropriate
column.

01:08:08.790 --> 01:08:11.030
Flip the two things.

01:08:11.030 --> 01:08:12.720
Actually, transpose is one
of these functions.

01:08:12.720 --> 01:08:15.220
I remember writing my first
transpose functions.

01:08:15.220 --> 01:08:16.989
And when I was done, I somehow
had the identity.

01:08:19.569 --> 01:08:24.520
Because I basically made the
loops go from 1 to n and 1 to

01:08:24.520 --> 01:08:27.260
n and swapped them.

01:08:27.260 --> 01:08:27.819
So I swapped them.

01:08:27.819 --> 01:08:29.720
So I said, oh, that
was a lot of work

01:08:29.720 --> 01:08:32.979
to compute the identity.

01:08:32.979 --> 01:08:34.630
No, you've got to make sure
you only go through a

01:08:34.630 --> 01:08:39.210
triangular iteration space in
order to make sure you swap--

01:08:39.210 --> 01:08:40.460
and then swap.

01:08:43.670 --> 01:08:45.450
This is an in-place swap.

01:08:45.450 --> 01:08:46.970
So that's cilk_for.

01:08:46.970 --> 01:08:48.490
That's basically it.

01:08:48.490 --> 01:08:50.470
There are some more facilities
we'll talk about.

01:08:50.470 --> 01:08:52.630
But that's basically
it for parallel

01:08:52.630 --> 01:08:54.210
programming in Cilk++.

01:08:54.210 --> 01:08:58.000
The other part is, how do you
do it so you get fast code?

01:08:58.000 --> 01:08:59.920
Which we'll talk about.

01:08:59.920 --> 01:09:04.670
Now, Cilk has serial
semantics.

01:09:04.670 --> 01:09:09.060
And what that means is unlike
some of the other ones, it's

01:09:09.060 --> 01:09:13.220
kind of what OpenMP was
aspiring to do.

01:09:13.220 --> 01:09:17.240
The idea is that if I, for
example here, delete these two

01:09:17.240 --> 01:09:22.510
keywords, I get a C++ code.

01:09:22.510 --> 01:09:25.600
And that code is always a legal
way to execute this

01:09:25.600 --> 01:09:27.560
parallel code.

01:09:27.560 --> 01:09:29.840
So the parallel code may have
more behaviors of its

01:09:29.840 --> 01:09:31.630
nondeterministic code.

01:09:31.630 --> 01:09:35.609
But always, it's legal to
treat it as if it's just

01:09:35.609 --> 01:09:36.859
straight C++.

01:09:38.939 --> 01:09:41.270
And the reason for that is
that, really, we're only

01:09:41.270 --> 01:09:44.439
granting permission for
parallel execution.

01:09:44.439 --> 01:09:47.149
So even though I put in these
keywords, I still can execute

01:09:47.149 --> 01:09:50.779
it serially if I wish.

01:09:50.779 --> 01:09:52.420
They don't command parallel
execution.

01:09:52.420 --> 01:09:55.430
To obtain this serialization,
you can do it by hand by just

01:09:55.430 --> 01:09:58.675
defining a cilk_for to be for,
and the cilk_spawn and

01:09:58.675 --> 01:10:01.420
cilk_sync to be empty.

01:10:01.420 --> 01:10:04.950
Or there's a switch to the
Cilk++ composite that does

01:10:04.950 --> 01:10:06.340
that for you automatically.

01:10:06.340 --> 01:10:10.750
And it's probably the preferred
way of doing it.

01:10:10.750 --> 01:10:15.440
But the idea is conceptually,
you can sprinkle in these

01:10:15.440 --> 01:10:18.140
keywords, and if you don't
want it anymore, fine.

01:10:18.140 --> 01:10:21.470
If you want to compile it with
the straight c compilers, it's

01:10:21.470 --> 01:10:23.600
better to use the Cilk++
compiler to do it.

01:10:23.600 --> 01:10:27.970
But if you wanted to ship it
off to somebody else, you

01:10:27.970 --> 01:10:30.440
could just do these sharp
defines, and they could

01:10:30.440 --> 01:10:32.120
compile it with their compilers,
and it would be the

01:10:32.120 --> 01:10:36.870
same as a serial C++ code.

01:10:36.870 --> 01:10:41.180
So the Cilk++ concurrency
platform allows the program to

01:10:41.180 --> 01:10:45.290
express potential parallelism
in application.

01:10:45.290 --> 01:10:47.110
So it says, where is
the parallelism?

01:10:47.110 --> 01:10:49.240
It doesn't say how
to schedule it.

01:10:49.240 --> 01:10:50.910
It says, where is it?

01:10:50.910 --> 01:10:56.800
And then, it gets mapped onto,
at runtime, dynamically mapped

01:10:56.800 --> 01:10:58.240
onto the processor cores.

01:11:01.680 --> 01:11:05.510
And the way that it does the
mapping is mathematically

01:11:05.510 --> 01:11:09.130
provably a good way
of doing it.

01:11:09.130 --> 01:11:12.530
And if you take one of my
graduate courses, I can teach

01:11:12.530 --> 01:11:15.840
you how that works.

01:11:15.840 --> 01:11:19.120
We'll do a little bit of study
of simple scheduling.

01:11:19.120 --> 01:11:23.380
But the actual schedule it
uses is more involved.

01:11:23.380 --> 01:11:25.680
But we'll cover it
a little bit.

01:11:25.680 --> 01:11:28.280
Here's the components
of the Cilk++

01:11:28.280 --> 01:11:31.330
platform on a single slide.

01:11:31.330 --> 01:11:32.380
So let me just say
what they are.

01:11:32.380 --> 01:11:34.600
The first one is the keywords.

01:11:34.600 --> 01:11:36.630
So you get to put
things in there.

01:11:36.630 --> 01:11:42.900
And if you elide or create the
serialization, then you get

01:11:42.900 --> 01:11:47.700
the C++ code or C code, for
which then you can run your

01:11:47.700 --> 01:11:50.780
regression test and demonstrate
you have some good

01:11:50.780 --> 01:11:53.170
single-threaded program.

01:11:53.170 --> 01:11:56.270
Alternatively, you can send it
through the Cilk++ compiler,

01:11:56.270 --> 01:11:58.120
which is based on a conventional
compiler.

01:11:58.120 --> 01:12:00.520
In our case, it will be GCC.

01:12:00.520 --> 01:12:02.920
You can link that with the
hyperobject library, which

01:12:02.920 --> 01:12:05.780
we'll talk about when we start
talking about synchronization.

01:12:05.780 --> 01:12:07.515
It produces a binary.

01:12:07.515 --> 01:12:11.090
If you run that binary on the
runtime system, you can also

01:12:11.090 --> 01:12:12.300
run it to the regression test.

01:12:12.300 --> 01:12:14.980
And in particular, if you run
it on the runtime system,

01:12:14.980 --> 01:12:20.780
running on one core, it should
behave identically to having

01:12:20.780 --> 01:12:23.920
run it through this path with
just the serial code.

01:12:26.670 --> 01:12:29.080
And of course, you get
exceptional performance.

01:12:29.080 --> 01:12:31.080
These, I think, were originally
marketing slides.

01:12:34.290 --> 01:12:38.900
However, there's also the fact
that you may get what are

01:12:38.900 --> 01:12:42.910
called races in your code, which
are bugs that will come

01:12:42.910 --> 01:12:45.870
up that won't occur in your
serial code, but will occur in

01:12:45.870 --> 01:12:48.680
your parallel code.

01:12:48.680 --> 01:12:51.330
Cilk has a race detector to
detect those, for which you

01:12:51.330 --> 01:12:54.450
can run parallel regression
tests to produce your reliable

01:12:54.450 --> 01:12:55.960
multi-threaded code.

01:12:55.960 --> 01:12:58.270
And then, the final piece of
it is there's this thing

01:12:58.270 --> 01:13:02.250
called Cilkview, which allows
you to analyze the scalability

01:13:02.250 --> 01:13:03.410
of your software.

01:13:03.410 --> 01:13:07.370
So you can run, in fact, on a
single core or on a small

01:13:07.370 --> 01:13:08.550
number of cores.

01:13:08.550 --> 01:13:10.680
And then, you can predict how
it's going to behave on a

01:13:10.680 --> 01:13:14.320
large number of cores.

01:13:14.320 --> 01:13:18.450
So let's just, to conclude
here, talk about races.

01:13:18.450 --> 01:13:21.590
Because they're the nasty,
nasty, nasty thing we get into

01:13:21.590 --> 01:13:22.820
parallel programming.

01:13:22.820 --> 01:13:25.620
And then next time, we'll
get deeper into the Cilk

01:13:25.620 --> 01:13:26.870
technology itself.

01:13:29.400 --> 01:13:32.930
So the most basic kind of race
there is what's called a

01:13:32.930 --> 01:13:35.330
determinacy race.

01:13:35.330 --> 01:13:38.710
Because if you have one of these
things, your program

01:13:38.710 --> 01:13:41.350
becomes nondeterministic.

01:13:41.350 --> 01:13:44.400
It doesn't do the same
thing every time.

01:13:44.400 --> 01:13:47.570
A determinacy race occurs when
two logically parallel

01:13:47.570 --> 01:13:51.990
instructions access the same
memory location, and at least

01:13:51.990 --> 01:13:55.500
one of the instructions performs
a write, performs a

01:13:55.500 --> 01:13:58.190
store, to that location.

01:13:58.190 --> 01:14:01.030
So here's an example.

01:14:01.030 --> 01:14:06.050
I have a cilk_for here, both
branches of which are

01:14:06.050 --> 01:14:08.540
incrementing x.

01:14:08.540 --> 01:14:09.560
This is basically going.

01:14:09.560 --> 01:14:10.740
The index is going.

01:14:10.740 --> 01:14:13.010
i equals 0 and i equals 1.

01:14:13.010 --> 01:14:15.640
And then, it's asserting
that x equals 2.

01:14:15.640 --> 01:14:19.200
If I run this serially,
the assertion passes.

01:14:22.200 --> 01:14:27.230
But when I run it in parallel,
it may not produce a 2.

01:14:27.230 --> 01:14:28.730
It can produce a 1.

01:14:28.730 --> 01:14:31.000
And let's see why that is.

01:14:31.000 --> 01:14:34.350
So the way to understand this
code is to think about its

01:14:34.350 --> 01:14:37.520
execution in terms of a
dependency [? dag ?].

01:14:37.520 --> 01:14:41.650
So here I have my initialization
of x.

01:14:41.650 --> 01:14:45.330
Then once that's done, the
cilk_for loop allows me to do

01:14:45.330 --> 01:14:52.030
two things at a time, b and c,
which are both incrementing x.

01:14:52.030 --> 01:14:55.440
And then, I assert that
x equals 2 when

01:14:55.440 --> 01:14:58.340
they're both done.

01:14:58.340 --> 01:15:01.970
Because that's the semantics
of the cilk_for.

01:15:01.970 --> 01:15:04.400
So let's see where
the race occurs.

01:15:04.400 --> 01:15:06.580
So remember that it occurs
when I have two logically

01:15:06.580 --> 01:15:08.550
parallel instructions
that access

01:15:08.550 --> 01:15:10.390
the same memory location.

01:15:10.390 --> 01:15:12.350
Here, it's going to
be the location x.

01:15:12.350 --> 01:15:18.750
And at least one of them
performs a write execution.

01:15:18.750 --> 01:15:22.630
So if we actually looked closer,
I want to expand this

01:15:22.630 --> 01:15:23.910
into this larger thing.

01:15:23.910 --> 01:15:28.040
Because as you know, X++ is not
done on a memory location.

01:15:28.040 --> 01:15:30.120
It's not done as a single
instruction.

01:15:30.120 --> 01:15:33.480
It's done as a load,
x into a register.

01:15:33.480 --> 01:15:38.470
Increment the register, and then
store the value back in.

01:15:38.470 --> 01:15:41.140
And meanwhile, there's another
register on another processor,

01:15:41.140 --> 01:15:45.030
presumably, that's doing
the same thing.

01:15:45.030 --> 01:15:46.590
So this is the one I
want to look at.

01:15:46.590 --> 01:15:50.390
This is just a zooming in, if
you will, on this dependency

01:15:50.390 --> 01:15:53.070
graph to look a little bit finer
grain at what's actually

01:15:53.070 --> 01:15:56.570
happening one step at a time.

01:15:56.570 --> 01:15:59.980
So the determinacy race,
recall, occurs--

01:15:59.980 --> 01:16:01.210
this is by something,
I'm going to say

01:16:01.210 --> 01:16:04.420
again, you should memorize.

01:16:04.420 --> 01:16:05.850
So you should know
what this is.

01:16:05.850 --> 01:16:09.750
You should be able to say what
a determinacy race is.

01:16:09.750 --> 01:16:12.430
It's when you have two
instructions that are both

01:16:12.430 --> 01:16:14.540
accessing the same location,
and one of

01:16:14.540 --> 01:16:15.230
them performs write.

01:16:15.230 --> 01:16:16.160
And here, I have that.

01:16:16.160 --> 01:16:17.780
This guy is in parallel.

01:16:17.780 --> 01:16:20.360
He's being stored to here.

01:16:20.360 --> 01:16:22.160
This is also a race.

01:16:22.160 --> 01:16:26.080
He's been reading it, and
this guy is writing it.

01:16:26.080 --> 01:16:29.470
So let's see what can happen
and what can go wrong here.

01:16:29.470 --> 01:16:31.650
So here's my value,
x, in memory.

01:16:31.650 --> 01:16:34.820
And here's my two registers on,
presumably, two different

01:16:34.820 --> 01:16:36.020
processors.

01:16:36.020 --> 01:16:38.690
So one thing is that
you can typically--

01:16:38.690 --> 01:16:42.580
and this is not quite the case
with real hardware-- but an

01:16:42.580 --> 01:16:45.620
abstraction of the hardware
is that you can treat the

01:16:45.620 --> 01:16:49.320
parallel execution from a
logical point of view as if

01:16:49.320 --> 01:16:51.690
you're interleaving instructions
from the

01:16:51.690 --> 01:16:53.190
different processors.

01:16:53.190 --> 01:16:53.680
OK.

01:16:53.680 --> 01:16:57.850
We're going to talk in three or
four lectures about where

01:16:57.850 --> 01:17:00.000
that isn't the right
abstraction.

01:17:00.000 --> 01:17:03.480
But it is close to the
right abstraction.

01:17:03.480 --> 01:17:06.690
So here, basically, we execute
statement one, which causes x

01:17:06.690 --> 01:17:09.430
to become 0.

01:17:09.430 --> 01:17:11.300
Now let's execute
statement two.

01:17:11.300 --> 01:17:16.730
That causes r1 to become 0.

01:17:16.730 --> 01:17:18.130
Then, I can increment that.

01:17:18.130 --> 01:17:19.310
It becomes a 1.

01:17:19.310 --> 01:17:21.130
All well and good.

01:17:21.130 --> 01:17:25.670
But now if the next logical
thing that happens is that r2

01:17:25.670 --> 01:17:31.390
is set to the value x,
then it becomes 0.

01:17:31.390 --> 01:17:33.360
Then we increment it.

01:17:33.360 --> 01:17:36.900
And now, he stores
back 1 into x.

01:17:36.900 --> 01:17:39.530
And now, this guy stores
1 back into x.

01:17:39.530 --> 01:17:41.770
And notice that now,
we [UNINTELLIGIBLE]

01:17:41.770 --> 01:17:43.320
go to the assertion.

01:17:43.320 --> 01:17:47.630
And we assert that it's
2, and it's not the 2.

01:17:47.630 --> 01:17:49.660
It's a 1.

01:17:49.660 --> 01:17:51.870
Because we lost one
of the updates.

01:17:51.870 --> 01:17:55.090
Now the reason race bugs are
really pernicious is, notice

01:17:55.090 --> 01:17:58.570
that if I had executed this
whole branch, and then this

01:17:58.570 --> 01:18:02.510
whole branch, I get
the right answer.

01:18:02.510 --> 01:18:06.100
Or if I executed this whole
branch, and then this whole

01:18:06.100 --> 01:18:08.600
branch, I get the
right answer.

01:18:08.600 --> 01:18:11.310
The only time I don't get the
right answer is when those two

01:18:11.310 --> 01:18:14.490
things happen to interleave
just so.

01:18:14.490 --> 01:18:18.370
And that's what happens with
race conditions generally, is

01:18:18.370 --> 01:18:22.050
that you can run your code a
million times and not see the

01:18:22.050 --> 01:18:27.670
bug, and then run it once, and
it crashes out in the field.

01:18:27.670 --> 01:18:31.400
Or what's happened is there
have been race bugs

01:18:31.400 --> 01:18:35.640
responsible for failure of
space shuttle to launch.

01:18:35.640 --> 01:18:42.042
You have the North American
blackout of 2001?

01:18:42.042 --> 01:18:43.505
2003?

01:18:43.505 --> 01:18:44.890
It wasn't that long ago.

01:18:44.890 --> 01:18:45.680
It was like, 10 years ago.

01:18:45.680 --> 01:18:48.970
We had big black out caused by
a race condition in the code

01:18:48.970 --> 01:18:51.840
run by the power companies.

01:18:51.840 --> 01:18:56.600
There been medical instruments
that have fried people, killed

01:18:56.600 --> 01:19:00.030
them and maimed them, because
of race conditions.

01:19:00.030 --> 01:19:02.890
These are really serious bugs.

01:19:02.890 --> 01:19:03.835
Question?

01:19:03.835 --> 01:19:08.290
AUDIENCE: [INAUDIBLE] when you
said, the only time that that

01:19:08.290 --> 01:19:12.260
code is actually execute
serially?

01:19:12.260 --> 01:19:15.380
PROFESSOR: It could execute in
parallel if it happened that

01:19:15.380 --> 01:19:17.330
these guys executed
before these guys.

01:19:17.330 --> 01:19:20.690
If you think of a larger
context, a whole bunch of

01:19:20.690 --> 01:19:23.350
these things, and I have two
routines where they're both

01:19:23.350 --> 01:19:26.210
incrementing x in the middle
of great big parallel

01:19:26.210 --> 01:19:29.710
programs, it could be that
they're executing perfectly

01:19:29.710 --> 01:19:31.280
well in parallel.

01:19:31.280 --> 01:19:34.910
But if those two small sections
of code happen to

01:19:34.910 --> 01:19:40.670
execute like this or like this,
then you're going to end

01:19:40.670 --> 01:19:42.360
up with it executing
correctly.

01:19:42.360 --> 01:19:46.170
But if they execute sort of at
the same time, it would not

01:19:46.170 --> 01:19:49.710
necessarily behave correctly.

01:19:49.710 --> 01:19:54.500
So there are two types of races
that people talk about,

01:19:54.500 --> 01:19:56.900
a read race and a write race.

01:19:56.900 --> 01:19:59.580
So suppose you have two
instructions that access a

01:19:59.580 --> 01:20:00.880
location, x.

01:20:00.880 --> 01:20:03.405
And suppose that a
is parallel to b.

01:20:03.405 --> 01:20:06.170
Both a and b are both reads,
you get no race.

01:20:06.170 --> 01:20:08.920
That's good.

01:20:08.920 --> 01:20:09.950
Because there's no way.

01:20:09.950 --> 01:20:13.130
But if one is a read and one is
a write, then one of them

01:20:13.130 --> 01:20:15.330
is going to see a different
value, depending upon whether

01:20:15.330 --> 01:20:16.930
it occurred before and
after the write.

01:20:16.930 --> 01:20:19.155
Or if they both are writing,
one can lose a value.

01:20:22.190 --> 01:20:25.490
So these are read races.

01:20:25.490 --> 01:20:27.210
And this is a write race.

01:20:27.210 --> 01:20:28.760
So we say that the two
sections of code are

01:20:28.760 --> 01:20:30.630
independent if they have
no determinacy

01:20:30.630 --> 01:20:32.480
races between them.

01:20:32.480 --> 01:20:35.490
So for example, this piece of
code is incrementing y, and

01:20:35.490 --> 01:20:37.050
this is incrementing x.

01:20:37.050 --> 01:20:39.050
And y is not equal to x.

01:20:39.050 --> 01:20:41.240
Those are independent
pieces of code.

01:20:41.240 --> 01:20:45.970
So to avoid races, you want to
make sure that the iterations

01:20:45.970 --> 01:20:48.720
of your cilk_for are
independent.

01:20:48.720 --> 01:20:51.970
So what's going on in one
iteration is different from

01:20:51.970 --> 01:20:52.990
what's going on in another.

01:20:52.990 --> 01:20:54.910
That you're not writing
something in one that you're

01:20:54.910 --> 01:20:58.740
using in the next,
for example.

01:20:58.740 --> 01:21:02.460
Between a cilk_spawn and the
corresponding cilk_sync, the

01:21:02.460 --> 01:21:05.230
code of the spawn child should
be independent of the code of

01:21:05.230 --> 01:21:06.360
the parent.

01:21:06.360 --> 01:21:06.630
OK?

01:21:06.630 --> 01:21:09.400
Including any code executed
by additional

01:21:09.400 --> 01:21:11.430
spawned or called children.

01:21:11.430 --> 01:21:13.800
So it's basically saying, when
you spawn something off, don't

01:21:13.800 --> 01:21:16.840
then go and do something that's
going to modify the

01:21:16.840 --> 01:21:18.140
same locations.

01:21:18.140 --> 01:21:19.985
You really want to modify
different locations.

01:21:22.530 --> 01:21:24.730
It's fine if they both read
the same locations.

01:21:24.730 --> 01:21:26.765
But it's not fine for one
of them to read and

01:21:26.765 --> 01:21:29.730
one of them to write.

01:21:29.730 --> 01:21:33.350
One thing here to understand
is that when you spawn a

01:21:33.350 --> 01:21:36.540
function, the arguments are
actually executed serially

01:21:36.540 --> 01:21:38.880
before the actual
spawn occurs.

01:21:38.880 --> 01:21:41.900
So you evaluate the arguments,
and you set it all up, then

01:21:41.900 --> 01:21:45.070
you spawn the function.

01:21:45.070 --> 01:21:46.950
So the actual spawn
occurs after the

01:21:46.950 --> 01:21:48.260
evaluation of arguments.

01:21:48.260 --> 01:21:49.760
So they're evaluated
in the parent.

01:21:52.350 --> 01:21:54.690
Machine word size matters.

01:21:54.690 --> 01:21:58.250
So this is generally
the case for races.

01:21:58.250 --> 01:22:00.640
By the way, races are
not just Cilk stuff.

01:22:00.640 --> 01:22:05.600
These races occur in all of
these concurrency platforms.

01:22:05.600 --> 01:22:07.430
I'm illustrating Cilk because
that's what we're going to be

01:22:07.430 --> 01:22:10.480
using in our labs
and so forth.

01:22:10.480 --> 01:22:12.430
So it turns out machine
word size matters.

01:22:12.430 --> 01:22:16.370
And you can have races in
packed data structures.

01:22:16.370 --> 01:22:22.180
So for example, on some
machines, if you declare a

01:22:22.180 --> 01:22:28.580
char a and char b in a struct,
then updating x and x, b in

01:22:28.580 --> 01:22:32.420
parallel may cause a race,
because they're both actually

01:22:32.420 --> 01:22:34.380
operating on a word basis.

01:22:34.380 --> 01:22:36.650
Now on the Intel architectures,

01:22:36.650 --> 01:22:37.550
that doesn't happen.

01:22:37.550 --> 01:22:42.090
Because Intel supports atomic
updates of single bytes.

01:22:42.090 --> 01:22:43.740
So you don't have to
worry about it.

01:22:43.740 --> 01:22:47.360
But if you were accessing bits
within a word, you could end

01:22:47.360 --> 01:22:48.240
up with the same thing.

01:22:48.240 --> 01:22:52.990
You access bit five and bit
three, you think you're acting

01:22:52.990 --> 01:22:55.670
independently, but in fact,
you're reading the whole word

01:22:55.670 --> 01:22:57.800
or the whole byte in
order to access it.

01:23:01.070 --> 01:23:04.450
The technology that you're going
to be using fortunately

01:23:04.450 --> 01:23:09.180
comes with a race detector,
which you will find invaluable

01:23:09.180 --> 01:23:11.590
for debugging your stuff.

01:23:11.590 --> 01:23:14.240
And so this is kind of like
a Valgrind for races.

01:23:18.920 --> 01:23:22.740
What's good about this race
detector is it provides a rock

01:23:22.740 --> 01:23:24.150
hard guarantee.

01:23:24.150 --> 01:23:28.520
If you have a deterministic
program that on a given input

01:23:28.520 --> 01:23:31.250
could possibly behave any
differently from your serial

01:23:31.250 --> 01:23:35.080
program, from the corresponding
serial program,

01:23:35.080 --> 01:23:38.970
if you got rid of the parallel
keywords, this tool,

01:23:38.970 --> 01:23:41.570
Cilkscreen, guarantees to
report and localize the

01:23:41.570 --> 01:23:43.370
offending race.

01:23:43.370 --> 01:23:45.470
It'll tell you, you got
a race between this

01:23:45.470 --> 01:23:47.100
location and that location.

01:23:47.100 --> 01:23:49.980
And it's up to you to find
it and fix it, but it

01:23:49.980 --> 01:23:51.065
can tell you that.

01:23:51.065 --> 01:23:53.850
It employs regression test
methodology, where the

01:23:53.850 --> 01:23:55.820
programmer provides
test inputs.

01:23:55.820 --> 01:24:00.720
So if you don't provide test
inputs to elicit the race, you

01:24:00.720 --> 01:24:02.650
still can have a bug.

01:24:02.650 --> 01:24:05.430
But if you have a test input
that in any way could behave

01:24:05.430 --> 01:24:08.090
differently than the serial
execution, bingo.

01:24:08.090 --> 01:24:09.340
It'll tell you.

01:24:11.850 --> 01:24:14.860
It identifies a bunch of things
involving the race,

01:24:14.860 --> 01:24:16.700
including a stack trace.

01:24:16.700 --> 01:24:19.580
It runs off the binary
executable using what's called

01:24:19.580 --> 01:24:21.530
dynamic instrumentation.

01:24:21.530 --> 01:24:24.170
So that's kind of like Valgrind,
except it actually

01:24:24.170 --> 01:24:26.700
does this as it's running.

01:24:26.700 --> 01:24:31.030
It uses a technology called PIN,
which you can read about.

01:24:31.030 --> 01:24:37.340
P-I-N, which is a nice platform
for doing code

01:24:37.340 --> 01:24:39.620
rewriting and analysis
on the fly.

01:24:39.620 --> 01:24:42.600
It runs about 20 times slower
than real time.

01:24:42.600 --> 01:24:47.600
So you basically use
it for debugging.

01:24:47.600 --> 01:24:56.660
So the first part of project
four is basically coming up to

01:24:56.660 --> 01:24:58.750
speed with this technology.

01:24:58.750 --> 01:24:59.790
And so, there's some
good things.

01:24:59.790 --> 01:25:01.180
And that's going to be
available tomorrow.

01:25:01.180 --> 01:25:01.900
Is that what we said?

01:25:01.900 --> 01:25:05.310
Yeah, that will be available
tomorrow.

01:25:05.310 --> 01:25:07.410
So this is actually--
this is tons of fun.

01:25:07.410 --> 01:25:10.120
Most people in most places
don't get to play with

01:25:10.120 --> 01:25:11.560
parallel technology like this.

