WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.945
[MUSIC PLAYING]

00:00:02.945 --> 00:00:04.420
ASHISH AGARWAL: Hi, I'm Ashish.

00:00:04.420 --> 00:00:07.530
I'll be talking
about ML Toolkit.

00:00:07.530 --> 00:00:09.950
So let me start by saying
that TensorFlow is this really

00:00:09.950 --> 00:00:11.824
amazing piece of technology.

00:00:11.824 --> 00:00:13.490
At Google, it's already
powering systems

00:00:13.490 --> 00:00:16.760
ranging from search ranking,
ads auction, YouTube

00:00:16.760 --> 00:00:19.890
recommendations, Translate,
Photos, and many, many more.

00:00:19.890 --> 00:00:22.250
We can train complicated
models with hundreds

00:00:22.250 --> 00:00:24.376
of billions of
parameters and solve them

00:00:24.376 --> 00:00:26.970
on platforms ranging from mobile
phones to cluster of TPUs.

00:00:31.180 --> 00:00:34.645
However, TensorFlow is this
really low-level framework.

00:00:34.645 --> 00:00:36.270
As Martin mentioned
in an earlier talk,

00:00:36.270 --> 00:00:38.770
we are working on high-level
parameters like layers,

00:00:38.770 --> 00:00:41.340
estimators, losses,
and metrics to make

00:00:41.340 --> 00:00:43.200
it easier for researchers
and developers

00:00:43.200 --> 00:00:45.406
to create custom
model architectures.

00:00:45.406 --> 00:00:47.280
However, what's still
missing from TensorFlow

00:00:47.280 --> 00:00:50.010
is algorithms that
work out of the box.

00:00:50.010 --> 00:00:51.601
What a lot of
developers really want

00:00:51.601 --> 00:00:53.850
is packaged solutions that
they can quickly and easily

00:00:53.850 --> 00:00:56.620
integrate into their workflows.

00:00:56.620 --> 00:00:58.900
So I'm happy to
tell you that now we

00:00:58.900 --> 00:01:01.480
have a toolkit of really popular
machine-learning algorithms

00:01:01.480 --> 00:01:05.600
that we have built right
on top of TensorFlow.

00:01:05.600 --> 00:01:09.890
So starting with linear and
logistic regression, k-means

00:01:09.890 --> 00:01:12.680
and Gaussian mixture models
for unsupervised clustering

00:01:12.680 --> 00:01:16.030
of data, WALS matrix
factorization,

00:01:16.030 --> 00:01:18.280
which is a popular collaborative
[INAUDIBLE] algorithm

00:01:18.280 --> 00:01:22.587
for the computer systems,
support vector machines, all

00:01:22.587 --> 00:01:24.170
the way to state of
the art algorithms

00:01:24.170 --> 00:01:26.540
like SDCA, Stochastic
Dual Coordinate Ascent,

00:01:26.540 --> 00:01:30.770
for context optimization, random
forest and decision trees,

00:01:30.770 --> 00:01:32.985
and various architectures
of deep networks.

00:01:32.985 --> 00:01:34.610
Now, all these are
available on GitHub,

00:01:34.610 --> 00:01:36.943
and we are constantly working
on improving and expanding

00:01:36.943 --> 00:01:38.445
the site.

00:01:38.445 --> 00:01:40.320
Next I'll highlight a
few of these algorithms

00:01:40.320 --> 00:01:42.870
to give you a better sense
of the existing and upcoming

00:01:42.870 --> 00:01:45.150
features for these algorithms.

00:01:45.150 --> 00:01:48.030
Starting with k-means.

00:01:48.030 --> 00:01:50.550
So here we implemented the
standard layouts iterative

00:01:50.550 --> 00:01:54.320
algorithm, along with random
and k-means++ initialization.

00:01:54.320 --> 00:01:58.317
We support both full batch
and mini batch training modes,

00:01:58.317 --> 00:02:00.650
and we also allow the user
to specify distance functions

00:02:00.650 --> 00:02:05.010
like cosine or Euclidean
square distances.

00:02:05.010 --> 00:02:06.950
Now GMMs, on the other
hand, model the data

00:02:06.950 --> 00:02:08.580
as a mixture of Gaussians.

00:02:08.580 --> 00:02:11.000
These are much more powerful
models, and also harder to

00:02:11.000 --> 00:02:12.320
train.

00:02:12.320 --> 00:02:14.780
For these, we use an
iterative EM algorithm,

00:02:14.780 --> 00:02:17.280
and we allow the user to choose
from a combination of means,

00:02:17.280 --> 00:02:21.300
covariances, mixture
weights to train on.

00:02:21.300 --> 00:02:24.200
Next, we have WALS, which is
matrix factorization using

00:02:24.200 --> 00:02:26.330
Weighted Alternating
Least Squares.

00:02:26.330 --> 00:02:29.640
I will be revisiting this
later in my talk, as well.

00:02:29.640 --> 00:02:31.950
So here, you are given
a very sparse matrix.

00:02:31.950 --> 00:02:34.110
For example, you
could be given ratings

00:02:34.110 --> 00:02:36.270
that users have
provided on videos.

00:02:36.270 --> 00:02:38.910
Note that the matrix is
sparse, so not all the videos

00:02:38.910 --> 00:02:41.621
are being rated by each user.

00:02:41.621 --> 00:02:43.120
And now you want
to answer questions

00:02:43.120 --> 00:02:47.020
like, which video should you
recommend next to the user,

00:02:47.020 --> 00:02:50.980
or to find user-user similarity
or video-video similarity.

00:02:50.980 --> 00:02:53.680
Now, this is typically done by
factorizing this huge matrix

00:02:53.680 --> 00:02:58.250
into a product of two
factors, two dense factors.

00:02:58.250 --> 00:03:00.350
Note also that, as the
algorithm indicates,

00:03:00.350 --> 00:03:03.310
this loss is weighted, which
allows you to, say, downgrade

00:03:03.310 --> 00:03:07.660
things that were unrated in
the original input, or to avoid

00:03:07.660 --> 00:03:12.480
spam, or popular entries from
drowning out the total loss.

00:03:12.480 --> 00:03:14.152
So this value is
highly nonconvex.

00:03:14.152 --> 00:03:16.610
However, it turns out, that if
you fix some of the factors,

00:03:16.610 --> 00:03:19.820
there's a close analytical
solution for the other factor.

00:03:19.820 --> 00:03:21.980
And so the training
works in an iterative way

00:03:21.980 --> 00:03:25.210
where you, first,
fix V, compute U,

00:03:25.210 --> 00:03:29.140
then fix U, compute
V, and go on.

00:03:29.140 --> 00:03:31.169
Next, we have SVMs.

00:03:31.169 --> 00:03:33.210
So these work by finding
a decision boundary that

00:03:33.210 --> 00:03:35.680
maximizes the margin.

00:03:35.680 --> 00:03:38.950
We are working with soft margin
methods using a hinge loss,

00:03:38.950 --> 00:03:41.310
and the current implementation
is of linear SVMs

00:03:41.310 --> 00:03:43.940
with L1 and L2 regularization.

00:03:43.940 --> 00:03:47.040
Note that SVMs become much
more powerful with a nonlinear

00:03:47.040 --> 00:03:50.360
kernel, which allows a much more
complicated decision boundary.

00:03:50.360 --> 00:03:53.300
So we are working on providing
nonlinear kernels using

00:03:53.300 --> 00:03:56.950
the kernel approximation trick.

00:03:56.950 --> 00:03:58.700
Next, we have SDCA.

00:03:58.700 --> 00:04:00.590
Now here, you have a
convex loss function

00:04:00.590 --> 00:04:02.820
with L1 and L2
regularization, and SDCA

00:04:02.820 --> 00:04:05.420
uses a neat trick of
transforming this last function

00:04:05.420 --> 00:04:06.622
into a dual form.

00:04:06.622 --> 00:04:08.330
And it turns out that,
for many problems,

00:04:08.330 --> 00:04:10.880
solving it in the dual form
is much more efficient.

00:04:10.880 --> 00:04:12.380
And it turns out
that this algorithm

00:04:12.380 --> 00:04:14.963
can power models from linear
and logistic regression all

00:04:14.963 --> 00:04:17.920
the way to SVMs.

00:04:17.920 --> 00:04:19.890
We also have random
forest and decision trees.

00:04:19.890 --> 00:04:22.160
Random forest are
[INAUDIBLE] decision trees,

00:04:22.160 --> 00:04:23.960
and decision trees
work by creating

00:04:23.960 --> 00:04:26.979
a hierarchical partitioning
of the feature space.

00:04:26.979 --> 00:04:28.520
Currently our training
method is what

00:04:28.520 --> 00:04:30.561
we call "extremely random
forest training," which

00:04:30.561 --> 00:04:32.590
allows better
parallelization and scaling,

00:04:32.590 --> 00:04:34.340
and we are also working
on having gradient

00:04:34.340 --> 00:04:37.150
boosted decision trees, which
are very, very popular also.

00:04:40.149 --> 00:04:41.940
So I talked about the
different algorithms.

00:04:41.940 --> 00:04:44.430
Now we want to make it
really, really easy for users

00:04:44.430 --> 00:04:45.540
to use them.

00:04:45.540 --> 00:04:48.330
All of these are exposed via
very high level scikit-learn

00:04:48.330 --> 00:04:51.780
inspired estimator APIs.

00:04:51.780 --> 00:04:56.550
Here's an example for how when
[INAUDIBLE] k-means clustering.

00:04:56.550 --> 00:04:59.990
So you start by creating a
k-means clustering object,

00:04:59.990 --> 00:05:02.120
and here you can pass
in a bunch of options,

00:05:02.120 --> 00:05:04.370
like the number of clusters,
how you want to train,

00:05:04.370 --> 00:05:07.910
how you want to
initialize, and so on.

00:05:07.910 --> 00:05:11.470
Next, you call the fit function
and pass it your input,

00:05:11.470 --> 00:05:12.640
and that's it.

00:05:12.640 --> 00:05:14.420
Not tons of logos
behind the scenes.

00:05:14.420 --> 00:05:17.354
It creates the graph for you.

00:05:17.354 --> 00:05:19.520
It'll run training iterations,
configure the runtime

00:05:19.520 --> 00:05:20.864
until the training is done.

00:05:20.864 --> 00:05:22.280
When you're finally
ready, you can

00:05:22.280 --> 00:05:25.160
start inspecting the model
parameters like the clusters,

00:05:25.160 --> 00:05:27.725
and start running inference,
here finding the assignment

00:05:27.725 --> 00:05:31.140
to clusters, and so on.

00:05:31.140 --> 00:05:32.746
So I talked about
high-level APIs,

00:05:32.746 --> 00:05:34.620
but you also want to
maintain the flexibility

00:05:34.620 --> 00:05:37.126
and extensibility that
TensorFlow promises.

00:05:37.126 --> 00:05:39.000
So these are not opaque
objects that are only

00:05:39.000 --> 00:05:40.455
accessible via this API.

00:05:40.455 --> 00:05:43.230
They, in fact, allow the
users to inspect the graphs,

00:05:43.230 --> 00:05:45.450
and also to be able to embed
these graphs into larger

00:05:45.450 --> 00:05:47.550
training models.

00:05:47.550 --> 00:05:50.130
So here's a training-- here's
a [? trial ?] example that

00:05:50.130 --> 00:05:54.210
embeds k-means as a layer
in a bigger, deeper network.

00:05:54.210 --> 00:05:56.780
You start with an
input, you run k-means,

00:05:56.780 --> 00:05:58.190
and get your graph for k-means.

00:05:58.190 --> 00:06:00.980
It also returns your training
op to drive the clustering,

00:06:00.980 --> 00:06:03.860
and the output is the input
transformed to a distance

00:06:03.860 --> 00:06:06.310
to cluster space.

00:06:06.310 --> 00:06:08.515
Next, you feed this to
your dense stack of layers,

00:06:08.515 --> 00:06:10.310
you create your
supervised loss, you

00:06:10.310 --> 00:06:12.701
run STD to get a
final DNN training op.

00:06:12.701 --> 00:06:14.200
Now you have these
two training ops,

00:06:14.200 --> 00:06:16.074
you can join them to
create a single training

00:06:16.074 --> 00:06:20.260
op that can drive the core
training of these two models.

00:06:20.260 --> 00:06:21.951
Here's a code
snippet for the same.

00:06:24.496 --> 00:06:26.370
So you start by creating
this k-means object,

00:06:26.370 --> 00:06:28.210
and you'll inspect
the training graph.

00:06:28.210 --> 00:06:31.620
It returns you a training
op and the output.

00:06:31.620 --> 00:06:33.770
Next, you feed your
output into dense layers,

00:06:33.770 --> 00:06:35.770
and you create your model
architecture as usual.

00:06:35.770 --> 00:06:38.395
Finally, you get the training op
to drive your supervised loss,

00:06:38.395 --> 00:06:40.080
or your dense stack.

00:06:40.080 --> 00:06:42.850
And finally, use TensorFlow's
group operation to group

00:06:42.850 --> 00:06:44.850
together these ops, so
that you have a single op

00:06:44.850 --> 00:06:46.378
to drive the co-train.

00:06:49.430 --> 00:06:50.805
So just to recap
the talk so far,

00:06:50.805 --> 00:06:52.721
I talked to you about
the different algorithms

00:06:52.721 --> 00:06:53.760
that are available now.

00:06:53.760 --> 00:06:56.970
I showed you examples for how
to use high-level APIs to access

00:06:56.970 --> 00:06:59.730
them, and also talked about
flexibility and extensibility

00:06:59.730 --> 00:07:01.434
of these algorithms.

00:07:01.434 --> 00:07:02.850
Next, I want to
highlight the fact

00:07:02.850 --> 00:07:04.350
that all these
algorithms are backed

00:07:04.350 --> 00:07:06.397
by distributed implementations.

00:07:06.397 --> 00:07:08.730
Just to give you a better
sense of the complexity hidden

00:07:08.730 --> 00:07:11.012
behind these few lines
of byte and code,

00:07:11.012 --> 00:07:12.720
I will dig deeper into
the implementation

00:07:12.720 --> 00:07:16.470
of one of the algorithms.

00:07:16.470 --> 00:07:18.690
So, in general, the
distribution architecture

00:07:18.690 --> 00:07:20.160
is going to do the same.

00:07:20.160 --> 00:07:22.540
You'll have a set of
parameter servers, which

00:07:22.540 --> 00:07:25.090
will store all the
parameters in a sharded way.

00:07:25.090 --> 00:07:26.910
You will have a bunch
of worker replicas

00:07:26.910 --> 00:07:28.800
that will be running
training steps,

00:07:28.800 --> 00:07:30.750
with many batches of inputs.

00:07:30.750 --> 00:07:34.640
In each step, the worker replica
would fetch some parameters.

00:07:34.640 --> 00:07:37.320
It will run compute on the
input to compute the new value

00:07:37.320 --> 00:07:38.386
of the parameters.

00:07:38.386 --> 00:07:40.260
And finally, it will
write back these updates

00:07:40.260 --> 00:07:43.080
to the parameter server.

00:07:43.080 --> 00:07:44.560
So let's look at WALS again.

00:07:44.560 --> 00:07:46.350
Just to recap,
with WALS, you want

00:07:46.350 --> 00:07:49.822
to factorize a really sparse
matrix into dense factors.

00:07:49.822 --> 00:07:51.530
And as I told you,
the training algorithm

00:07:51.530 --> 00:07:53.810
involves fixing
one of the factors,

00:07:53.810 --> 00:07:56.680
computing the other
one in your training.

00:07:56.680 --> 00:07:59.380
Now I want to be able to scale
to inputs that are terabytes

00:07:59.380 --> 00:08:00.954
in size, and you
want factors that

00:08:00.954 --> 00:08:03.370
have hundreds of millions of
rows and hundreds of columns.

00:08:06.440 --> 00:08:07.893
So how do you make this work?

00:08:12.690 --> 00:08:15.250
So we have to carefully think
about all the RAM, compute,

00:08:15.250 --> 00:08:16.590
and info bottlenecks.

00:08:16.590 --> 00:08:18.180
The first obvious
thing is, given

00:08:18.180 --> 00:08:20.638
the size of these factors, you
will have to distribute them

00:08:20.638 --> 00:08:22.150
across parameter server shards.

00:08:22.150 --> 00:08:26.019
So you take a U, split them
across all these shards.

00:08:26.019 --> 00:08:28.060
Next thing, given the
amount of compute involved,

00:08:28.060 --> 00:08:30.335
you want a bunch
of worker replicas.

00:08:30.335 --> 00:08:32.710
The sharding scheme that we
use for these worker replicas

00:08:32.710 --> 00:08:34.289
is row slicing.

00:08:34.289 --> 00:08:36.429
And what that means
is, each mini batch

00:08:36.429 --> 00:08:38.260
is a subset of
rows of the input,

00:08:38.260 --> 00:08:42.308
and we compute the
corresponding rows of U.

00:08:42.308 --> 00:08:44.670
Note, however, there's
still a problem.

00:08:44.670 --> 00:08:47.520
Each update to U depends
on the full V factor.

00:08:47.520 --> 00:08:50.097
And as I said, V could be
large, it could be terabytes.

00:08:50.097 --> 00:08:51.680
So what this entails
is that you might

00:08:51.680 --> 00:08:54.920
be copying terabytes of data
to each worker in each step.

00:08:54.920 --> 00:08:56.850
So how do we get around that?

00:08:56.850 --> 00:08:59.360
So at this point, we use
some [INAUDIBLE] magic.

00:08:59.360 --> 00:09:02.090
It turns out, you can
compute certain summaries,

00:09:02.090 --> 00:09:06.200
let's call them G. And
given G, your updates to U

00:09:06.200 --> 00:09:09.080
only depend on a few rows of
V. And these are specifically

00:09:09.080 --> 00:09:12.680
the rows that correspond to the
non-zero entries of the input.

00:09:12.680 --> 00:09:13.760
So this is great.

00:09:13.760 --> 00:09:16.010
What that means is that,
instead of copying terabytes,

00:09:16.010 --> 00:09:19.780
you can just copy
megabytes of data per step.

00:09:19.780 --> 00:09:22.310
At this point, we can run
TensorFlow's gather operation

00:09:22.310 --> 00:09:25.250
and get the required rows.

00:09:25.250 --> 00:09:28.640
We then compute using highly
optimized custom C++ kernels

00:09:28.640 --> 00:09:30.170
that we wrote.

00:09:30.170 --> 00:09:32.300
And finally, we use
TensorFlow's scatter operation

00:09:32.300 --> 00:09:34.758
to write back these updates to
the parameter server shards.

00:09:36.950 --> 00:09:38.900
So how well does this work?

00:09:38.900 --> 00:09:44.140
We believe our implementations
are really high performance.

00:09:44.140 --> 00:09:46.580
We did some benchmarking,
and on single machine,

00:09:46.580 --> 00:09:48.590
our implementations,
in general, look

00:09:48.590 --> 00:09:51.500
on par with scikit in terms
of model quality and speed,

00:09:51.500 --> 00:09:54.320
often even working faster
for moderate-sized problems,

00:09:54.320 --> 00:09:55.394
as well.

00:09:55.394 --> 00:09:56.810
But where TensorFlow
really shines

00:09:56.810 --> 00:09:59.660
is being able to run seamlessly
across hundreds of thousands

00:09:59.660 --> 00:10:01.227
of machines.

00:10:01.227 --> 00:10:03.060
In fact, in many cases,
we are able to train

00:10:03.060 --> 00:10:04.601
models that are much
larger than what

00:10:04.601 --> 00:10:07.872
we had seen in [INAUDIBLE].

00:10:07.872 --> 00:10:09.330
For example, with
random forest, we

00:10:09.330 --> 00:10:13.110
were able to train thousands of
trees with billions of nodes.

00:10:13.110 --> 00:10:16.380
And we have a paper at
NIPS that talks about that.

00:10:16.380 --> 00:10:20.530
With SDCA, we saw 10x to 50x
faster convergence compared

00:10:20.530 --> 00:10:23.170
to Google's highly optimized
internal [INAUDIBLE]

00:10:23.170 --> 00:10:26.800
implementations on logistic
regression for [INAUDIBLE]

00:10:26.800 --> 00:10:30.400
with billions of examples.

00:10:30.400 --> 00:10:32.200
With WALS, we are
able to factorize

00:10:32.200 --> 00:10:36.040
a huge matrix, 400 million
rows by 600 million columns,

00:10:36.040 --> 00:10:41.360
into factors with 500
columns in under 12 hours.

00:10:41.360 --> 00:10:43.160
Note that this is
50x larger than what

00:10:43.160 --> 00:10:45.650
we could do with an early
MapReduce-based implementation.

00:10:49.070 --> 00:10:51.670
So, in summary, we have really
high performance distributed

00:10:51.670 --> 00:10:54.730
and extensible implementations
of different ML algorithms

00:10:54.730 --> 00:10:57.100
that are working out of
the box in TensorFlow.

00:10:57.100 --> 00:10:59.350
These bring together
the ease of scikit

00:10:59.350 --> 00:11:01.540
with the power and
scalability of TensorFlow

00:11:01.540 --> 00:11:03.880
and moves TensorFlow one
step closer to being a more

00:11:03.880 --> 00:11:06.521
complete ML solution.

00:11:06.521 --> 00:11:08.020
I am hoping, at
this point, you guys

00:11:08.020 --> 00:11:09.480
are excited to get
started, so here

00:11:09.480 --> 00:11:12.924
are some links to core
and documentation,

00:11:12.924 --> 00:11:14.340
and here are a
bunch of estimators

00:11:14.340 --> 00:11:16.570
that are ready to use.

00:11:16.570 --> 00:11:18.320
With that, thank you.

00:11:18.320 --> 00:11:19.220
[APPLAUSE]

00:11:19.220 --> 00:11:21.670
[MUSIC PLAYING]

