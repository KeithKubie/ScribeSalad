WEBVTT
Kind: captions
Language: en

00:00:05.250 --> 00:00:07.310
JAY: Hi, I'm Jay from
the security team.

00:00:07.310 --> 00:00:11.860
We have a really
great talk today.

00:00:11.860 --> 00:00:14.620
Bruce is here to
talk about big data.

00:00:14.620 --> 00:00:16.590
"Data and Goliath"
is his new book.

00:00:16.590 --> 00:00:19.840
And I had a hard
time figuring out

00:00:19.840 --> 00:00:22.450
what to talk about,
how to introduce him.

00:00:22.450 --> 00:00:24.060
There's the stuff you read.

00:00:24.060 --> 00:00:30.490
And you're a Harvard
fellow, EFF, 13 books,

00:00:30.490 --> 00:00:34.450
well known in the
security community.

00:00:34.450 --> 00:00:37.480
But last night I stayed
up, and I read his book.

00:00:37.480 --> 00:00:41.569
I was up most of the night,
and then I got up at 5:30

00:00:41.569 --> 00:00:42.110
this morning.

00:00:42.110 --> 00:00:45.470
I was really tired, and with the
time change it was like 4:30.

00:00:45.470 --> 00:00:51.900
And I wanted to take some notes,
and I had an event this morning

00:00:51.900 --> 00:00:56.050
that made me appreciate
the book, what was in it.

00:00:56.050 --> 00:00:58.160
And I got into my car.

00:00:58.160 --> 00:01:00.610
I started to drive away,
and I realized that I

00:01:00.610 --> 00:01:02.800
left his book in my house.

00:01:02.800 --> 00:01:07.560
So I left my car running,
went inside, got the book,

00:01:07.560 --> 00:01:09.270
left my door open.

00:01:09.270 --> 00:01:12.410
My car rolled down the
driveway, took out my door,

00:01:12.410 --> 00:01:16.400
crashed into an oak tree,
pretty much totaled my car.

00:01:16.400 --> 00:01:18.762
This is at 5:30 this morning.

00:01:18.762 --> 00:01:21.440
AUDIENCE: Aw.

00:01:21.440 --> 00:01:26.010
JAY: So 5:45 I'm on
the phone with Geico

00:01:26.010 --> 00:01:28.320
filing an insurance claim.

00:01:28.320 --> 00:01:33.020
And it dawned on me, I am now
even more involved in Google.

00:01:33.020 --> 00:01:35.510
I'm involved with
Geico and big data.

00:01:35.510 --> 00:01:39.350
And in reading his
book, there was

00:01:39.350 --> 00:01:44.705
a company that looks at
car websites, right Bruce?

00:01:44.705 --> 00:01:46.080
Hey, you're looking
for new cars,

00:01:46.080 --> 00:01:48.455
and when you go in to buy a
new car, you read about this.

00:01:48.455 --> 00:01:50.420
They already know you're
looking for a new car.

00:01:50.420 --> 00:01:52.650
They can sort of take
advantage of that,

00:01:52.650 --> 00:01:56.740
and was it $300, $400 they
can take advantage of you

00:01:56.740 --> 00:01:58.130
in that way?

00:01:58.130 --> 00:01:59.710
And I was like, oh my god.

00:01:59.710 --> 00:02:02.360
I was looking at new cars
a couple of weeks ago.

00:02:02.360 --> 00:02:04.660
Is Geico going to
say, hey, I'm not

00:02:04.660 --> 00:02:07.100
going to deal with your
claim because you just pushed

00:02:07.100 --> 00:02:09.740
your car down the driveway
because you wanted a new car?

00:02:09.740 --> 00:02:12.410
Or there's companies that
look at where you drive.

00:02:12.410 --> 00:02:14.050
They're collecting
your license plates.

00:02:14.050 --> 00:02:16.680
There's all this big data
that they're generating.

00:02:16.680 --> 00:02:20.180
And I just realized that a lot
of stuff that was in his book

00:02:20.180 --> 00:02:22.910
was completely relevant not
only to what I do at work here

00:02:22.910 --> 00:02:27.180
at Google but in my daily life
of, hey, I just crashed my car.

00:02:27.180 --> 00:02:28.940
What's going to happen?

00:02:28.940 --> 00:02:33.010
So just keep that in
mind that I didn't

00:02:33.010 --> 00:02:35.226
realize that I would
have this big data event

00:02:35.226 --> 00:02:36.350
right before this happened.

00:02:36.350 --> 00:02:40.230
But Bruce, why don't you come up
here and talk about your book?

00:02:40.230 --> 00:02:41.730
It actually is a
really good read,

00:02:41.730 --> 00:02:43.860
and there's a lot to go over.

00:02:43.860 --> 00:02:47.388
So thanks for coming out.

00:02:47.388 --> 00:02:49.873
[APPLAUSE]

00:02:51.419 --> 00:02:52.710
BRUCE SCHNEIER: Hey, thank you.

00:02:52.710 --> 00:02:55.918
Maybe we should talk about the
value of the parking brake.

00:02:55.918 --> 00:02:57.412
[LAUGHTER]

00:03:00.440 --> 00:03:02.500
I want to talk about data.

00:03:02.500 --> 00:03:04.577
The book I wrote is
"Data and Goliath."

00:03:04.577 --> 00:03:06.160
I appreciate the
proper pronunciation.

00:03:06.160 --> 00:03:08.250
I didn't realize this, but there
are lots of people out there

00:03:08.250 --> 00:03:10.000
who will pronounce
it "Data and Goliath"

00:03:10.000 --> 00:03:11.330
and then not get the joke.

00:03:14.880 --> 00:03:18.400
"Data and Goliath," I'm
happy with the title.

00:03:18.400 --> 00:03:20.650
The title was a collaboration
between my editor and I.

00:03:20.650 --> 00:03:22.070
We were going around
with different titles,

00:03:22.070 --> 00:03:23.695
and we came up with
"Data and Goliath."

00:03:23.695 --> 00:03:26.457
I immediately loved it
because it's so evocative,

00:03:26.457 --> 00:03:28.540
but the problem with "Data
and Goliath" as a title

00:03:28.540 --> 00:03:30.123
is that Malcolm
Gladwell just came out

00:03:30.123 --> 00:03:32.600
with a book called
"David and Goliath."

00:03:32.600 --> 00:03:35.080
And that would be OK, except
that Malcolm Gladwell's

00:03:35.080 --> 00:03:37.582
previous book was
called "Outliers"

00:03:37.582 --> 00:03:39.790
and my previous book was
called "Liars and Outliers."

00:03:39.790 --> 00:03:42.430
It came out after his.

00:03:42.430 --> 00:03:45.320
And aping him twice
seemed like too much.

00:03:45.320 --> 00:03:49.630
So I wrote on my blog this
story of the title that's great

00:03:49.630 --> 00:03:52.930
but cannot be, and I got an
email out of the blue from

00:03:52.930 --> 00:03:56.240
Malcolm Gladwell, who
I don't know, saying,

00:03:56.240 --> 00:03:57.910
you should use the title.

00:03:57.910 --> 00:03:58.910
And I said, thanks.

00:03:58.910 --> 00:04:02.370
Will you blurb book?

00:04:02.370 --> 00:04:03.574
And he said, sure.

00:04:03.574 --> 00:04:04.990
And I said, you
know the publisher

00:04:04.990 --> 00:04:09.454
will put it on the front cover
in a font bigger than my name.

00:04:09.454 --> 00:04:10.620
So it is on the front cover.

00:04:10.620 --> 00:04:13.050
It's actually not in a
font bigger than my name,

00:04:13.050 --> 00:04:17.600
but I appreciate the permission
to use the title and the kind

00:04:17.600 --> 00:04:18.779
words.

00:04:18.779 --> 00:04:22.200
What I'm writing about is data.

00:04:22.200 --> 00:04:27.660
I'm really writing about data
in society and how it's used.

00:04:27.660 --> 00:04:32.050
You all know that all
computers produce data

00:04:32.050 --> 00:04:35.480
about things that are happening,
about what's going on,

00:04:35.480 --> 00:04:37.900
transaction records.

00:04:37.900 --> 00:04:40.220
So I'm writing about how
these transaction records are

00:04:40.220 --> 00:04:42.770
generated, how they
are increasingly

00:04:42.770 --> 00:04:47.360
stored and increasingly
used, saved, bought, and sold

00:04:47.360 --> 00:04:48.830
by different companies.

00:04:48.830 --> 00:04:51.060
I look at Google.

00:04:51.060 --> 00:04:53.870
I look at license plate capture.

00:04:53.870 --> 00:04:58.860
I look at different systems
that follow you around

00:04:58.860 --> 00:05:02.955
on the internet, cameras,
all of these technologies

00:05:02.955 --> 00:05:05.980
of collection, which I
think have been well talked

00:05:05.980 --> 00:05:07.190
about in the media.

00:05:07.190 --> 00:05:12.680
What's talked about less
are systems of analysis.

00:05:12.680 --> 00:05:15.330
The media likes to
focus on collecting

00:05:15.330 --> 00:05:18.900
this, collecting that,
spends less time on analysis.

00:05:18.900 --> 00:05:20.830
And I think one of the
common misconceptions

00:05:20.830 --> 00:05:22.730
I find talking to
people is they have

00:05:22.730 --> 00:05:26.640
very human views of analysis,
that people are looking

00:05:26.640 --> 00:05:29.100
at it or people-like entities.

00:05:29.100 --> 00:05:33.620
So you can hide in
a sea of a big data.

00:05:33.620 --> 00:05:35.600
If you've got thousands
or millions of records,

00:05:35.600 --> 00:05:37.140
they'll never find you.

00:05:37.140 --> 00:05:40.000
There isn't this
conception that computers

00:05:40.000 --> 00:05:43.850
are really good at incredibly
boring, time-consuming,

00:05:43.850 --> 00:05:45.600
repetitive tasks.

00:05:45.600 --> 00:05:48.310
And the intuition
of how this data

00:05:48.310 --> 00:05:51.380
can be used, how things
can be correlated

00:05:51.380 --> 00:05:54.090
doesn't apply anymore.

00:05:54.090 --> 00:05:57.910
And I'm reminded, this is only
related of the programs that

00:05:57.910 --> 00:06:01.450
would take shredded
paper and reassemble

00:06:01.450 --> 00:06:05.580
the documents basically by brute
force, laying them all flat,

00:06:05.580 --> 00:06:07.580
taking a photograph, and
moving the parts around

00:06:07.580 --> 00:06:11.520
until you got-- it's
basically a puzzle.

00:06:11.520 --> 00:06:15.560
But it's a puzzle that a
human being could never solve.

00:06:15.560 --> 00:06:19.270
So just like paper shredders are
designed for a human adversary,

00:06:19.270 --> 00:06:25.570
not really a computer
adversary, our notions of data

00:06:25.570 --> 00:06:29.810
and what happens to our data
is conceptualized mostly

00:06:29.810 --> 00:06:33.250
based on human adversaries.

00:06:33.250 --> 00:06:34.760
So I talk a lot about this.

00:06:34.760 --> 00:06:39.420
I talk about the data
being surveillance data.

00:06:39.420 --> 00:06:41.150
And I think this is a problem.

00:06:41.150 --> 00:06:43.250
Another problem in the
popular conception,

00:06:43.250 --> 00:06:45.060
the notion that
it's only metadata.

00:06:45.060 --> 00:06:49.430
We heard President Obama say
that a year and change ago.

00:06:49.430 --> 00:06:52.310
And metadata is fundamentally
surveillance data.

00:06:52.310 --> 00:06:55.490
To me the way to think of it is
just do a thought experiment.

00:06:55.490 --> 00:06:59.910
Imagine I hired a private
detective to spy on that guy,

00:06:59.910 --> 00:07:02.430
and the detective would
put a bug in your home,

00:07:02.430 --> 00:07:03.950
in your office, in your car.

00:07:03.950 --> 00:07:08.610
And I get a report of
the conversations he had.

00:07:08.610 --> 00:07:09.570
That's the data.

00:07:09.570 --> 00:07:11.236
That's the kind of
thing President Obama

00:07:11.236 --> 00:07:14.260
says is not being captured
for all Americans.

00:07:14.260 --> 00:07:16.290
Now I imagine I take
the same detective

00:07:16.290 --> 00:07:18.175
and said, put this guy
under surveillance.

00:07:18.175 --> 00:07:19.780
I would get a different report.

00:07:19.780 --> 00:07:25.250
I get a report of where he went,
who he spoke to, what he did.

00:07:25.250 --> 00:07:27.030
That's all the metadata.

00:07:27.030 --> 00:07:30.620
The metadata is the
surveillance data,

00:07:30.620 --> 00:07:34.160
and it's surprisingly intimate.

00:07:34.160 --> 00:07:36.600
Metadata speaks to
our relationships,

00:07:36.600 --> 00:07:39.500
our associations, what
we're interested in,

00:07:39.500 --> 00:07:41.160
what's important to us.

00:07:41.160 --> 00:07:44.570
It's really who we are.

00:07:44.570 --> 00:07:47.930
And it's much easier to
store, to search and analyze.

00:07:50.810 --> 00:07:54.540
Take an obvious
example of metadata

00:07:54.540 --> 00:07:57.370
is our location data
produced by our cellphones.

00:07:57.370 --> 00:07:59.990
We carry them
around all the time.

00:07:59.990 --> 00:08:00.830
Not out of malice.

00:08:00.830 --> 00:08:02.250
It's how the phones operate.

00:08:02.250 --> 00:08:05.100
They can't deliver phone calls
unless they know where you are.

00:08:05.100 --> 00:08:11.120
But a pretty accurate picture
of where you are reveals a lot.

00:08:11.120 --> 00:08:16.320
It's much more important for
an entity trying to control us,

00:08:16.320 --> 00:08:18.610
whoever that might
be, that we are

00:08:18.610 --> 00:08:21.100
all in the same room
together much less

00:08:21.100 --> 00:08:23.638
so than what I happen to
be saying from the podium.

00:08:26.450 --> 00:08:29.414
So we are in the golden
age of surveillance.

00:08:29.414 --> 00:08:31.330
And there's a couple of
characteristics of it.

00:08:31.330 --> 00:08:32.770
It's incidental.

00:08:32.770 --> 00:08:37.326
It's generally a side-effect
of the things we want to do.

00:08:37.326 --> 00:08:39.409
We don't pick up our phone
in the morning and say,

00:08:39.409 --> 00:08:42.799
I'm going to put my surveillance
device in my pocket today.

00:08:42.799 --> 00:08:44.410
But it has to be that.

00:08:44.410 --> 00:08:47.140
Otherwise it can't
operate as a cellphone.

00:08:47.140 --> 00:08:48.550
It's covert.

00:08:48.550 --> 00:08:50.260
We don't see it.

00:08:50.260 --> 00:08:52.440
If there were 50 people
standing over your shoulder

00:08:52.440 --> 00:08:54.510
as you surfed the web,
you would notice that.

00:08:54.510 --> 00:08:55.800
You'd say, hey, get away.

00:08:55.800 --> 00:08:58.740
I'm doing something.

00:08:58.740 --> 00:09:03.600
But if it's 50 cookies tracking
you, you don't notice it.

00:09:03.600 --> 00:09:04.890
It's hard to opt out of.

00:09:04.890 --> 00:09:06.320
I'm always asked,
what can people

00:09:06.320 --> 00:09:07.740
do to avoid surveillance?

00:09:07.740 --> 00:09:10.280
And the advice like
don't carry a cell phone

00:09:10.280 --> 00:09:14.960
and don't have an email
address is kind of dumb advice.

00:09:14.960 --> 00:09:16.030
Don't have a credit card.

00:09:16.030 --> 00:09:17.360
Don't have a Facebook account.

00:09:17.360 --> 00:09:20.210
These are the things you need
to be a fully functioning member

00:09:20.210 --> 00:09:21.730
of society.

00:09:21.730 --> 00:09:25.702
And those aren't things
we can easily do without.

00:09:25.702 --> 00:09:28.170
And it's also ubiquitous.

00:09:28.170 --> 00:09:30.240
It's happening everywhere.

00:09:30.240 --> 00:09:37.010
Simply because more and more
of our life involves computers.

00:09:37.010 --> 00:09:42.860
Our commerce, our socialization,
our research, our reading,

00:09:42.860 --> 00:09:47.410
intermediated by computers,
so the data is collected.

00:09:47.410 --> 00:09:50.210
And ubiquitous surveillance
is fundamentally different.

00:09:50.210 --> 00:09:51.455
It's not follow that car.

00:09:51.455 --> 00:09:53.500
It's follow every car.

00:09:53.500 --> 00:09:54.740
You can do more things.

00:09:54.740 --> 00:09:57.460
You can follow people
backwards in time.

00:09:57.460 --> 00:10:01.200
You can do what the
NSA calls hop searches.

00:10:01.200 --> 00:10:02.610
Who am I talking to?

00:10:02.610 --> 00:10:03.610
Who are they talking to?

00:10:03.610 --> 00:10:04.990
Who are they talking to?

00:10:04.990 --> 00:10:08.580
When you hear three hops, that's
what they're talking about.

00:10:08.580 --> 00:10:11.230
You can do about searches.

00:10:11.230 --> 00:10:13.810
Don't search this
person, but tell me

00:10:13.810 --> 00:10:15.870
who's spoken these words.

00:10:15.870 --> 00:10:19.170
Tell me who writes
about this topic.

00:10:19.170 --> 00:10:21.670
Find me somebody that meets
these particular surveillance

00:10:21.670 --> 00:10:24.330
characteristics.

00:10:24.330 --> 00:10:26.800
Maybe three time
location parameters,

00:10:26.800 --> 00:10:29.755
probably a unique person,
or flagging something

00:10:29.755 --> 00:10:30.630
based on interesting.

00:10:33.450 --> 00:10:38.747
This data is being collected and
used primarily by corporations.

00:10:38.747 --> 00:10:41.080
It's surveillance is the
business model of the internet.

00:10:41.080 --> 00:10:43.960
You guys know that.

00:10:43.960 --> 00:10:47.300
And we build systems
that spy on people

00:10:47.300 --> 00:10:48.680
in exchange for services.

00:10:48.680 --> 00:10:51.470
A lot of reasons why this is so.

00:10:51.470 --> 00:10:55.470
When the internet began,
there wasn't any way

00:10:55.470 --> 00:10:57.170
to charge for anything.

00:10:57.170 --> 00:10:59.965
Then it began to be used
for commercial reasons.

00:10:59.965 --> 00:11:02.340
And then people expect the
internet to be free just based

00:11:02.340 --> 00:11:05.490
on its pre-commerce history.

00:11:05.490 --> 00:11:07.890
So advertising was
the obvious way

00:11:07.890 --> 00:11:11.220
to make money on the internet,
and personalized advertising

00:11:11.220 --> 00:11:13.190
was the obvious way to
extend that and make

00:11:13.190 --> 00:11:15.690
that more profitable.

00:11:15.690 --> 00:11:17.519
So I think of this as
free and convenient

00:11:17.519 --> 00:11:18.810
as the drivers of surveillance.

00:11:23.140 --> 00:11:25.160
And corporations
know a lot about us,

00:11:25.160 --> 00:11:28.050
and that's sort of
an amazing amount.

00:11:28.050 --> 00:11:30.720
My cell phone knows where
I live, where I work,

00:11:30.720 --> 00:11:33.169
when I go to sleep,
when I wake up.

00:11:33.169 --> 00:11:34.210
We all carry cell phones.

00:11:34.210 --> 00:11:37.390
My cell phone knows
who I sleep with.

00:11:37.390 --> 00:11:39.420
I used to say that
Google knows more

00:11:39.420 --> 00:11:42.110
about me than my wife does.

00:11:42.110 --> 00:11:44.560
And that's true, but
it's not even enough.

00:11:44.560 --> 00:11:47.570
I think Google knows more
about me that than I do

00:11:47.570 --> 00:11:50.552
because Google remembers
things that I don't.

00:11:50.552 --> 00:11:55.370
And I think all of us, when
we're interested in something,

00:11:55.370 --> 00:11:56.250
we search for it.

00:11:59.010 --> 00:12:03.270
You guys know what kind of
porn every American likes,

00:12:03.270 --> 00:12:04.450
and that's sort of creepy.

00:12:07.590 --> 00:12:09.310
Now, do you remember
last year we

00:12:09.310 --> 00:12:12.745
saw that Uber post where
Uber was looking at rides.

00:12:12.745 --> 00:12:15.250
It was looking at
rides to a location

00:12:15.250 --> 00:12:19.070
at night and then rides from
a location the next morning.

00:12:19.070 --> 00:12:22.980
So it basically found people
using Uber to go have sex.

00:12:22.980 --> 00:12:24.530
And they published
stats about this,

00:12:24.530 --> 00:12:27.482
what cities, what neighborhoods
were the best for this.

00:12:27.482 --> 00:12:28.690
And they were all aggregates.

00:12:28.690 --> 00:12:31.320
They all hid the
individual people,

00:12:31.320 --> 00:12:34.360
but Uber knows those
individual people.

00:12:34.360 --> 00:12:40.060
Uber can produce the list
if they wanted to of people

00:12:40.060 --> 00:12:41.147
who use Uber to have sex.

00:12:41.147 --> 00:12:42.730
It's probably in the
license agreement

00:12:42.730 --> 00:12:43.730
that they're allowed to.

00:12:46.820 --> 00:12:49.780
And this is done
as a side-effect

00:12:49.780 --> 00:12:51.980
of using a very useful service.

00:12:55.270 --> 00:12:57.080
Government surveillance
largely piggybacks

00:12:57.080 --> 00:12:57.610
on these capabilities.

00:12:57.610 --> 00:12:59.901
We've learned that a little
from the Snowden documents.

00:12:59.901 --> 00:13:01.406
We know that from China.

00:13:01.406 --> 00:13:03.530
It's not like the NSA woke
up one morning and said,

00:13:03.530 --> 00:13:05.054
let's spy on everybody.

00:13:05.054 --> 00:13:06.720
They woke up one
morning and said, look,

00:13:06.720 --> 00:13:08.386
these companies are
spying on everybody.

00:13:08.386 --> 00:13:10.320
Let's get ourselves a copy.

00:13:10.320 --> 00:13:11.780
They do it a lot
of different ways.

00:13:11.780 --> 00:13:14.030
They do it through legal
compulsion, national security

00:13:14.030 --> 00:13:15.990
letters.

00:13:15.990 --> 00:13:22.170
They do it through subversion,
when they hacked your data

00:13:22.170 --> 00:13:25.080
center links outside the US.

00:13:25.080 --> 00:13:27.260
They do it all different ways.

00:13:27.260 --> 00:13:29.340
And this really allows
government to get away

00:13:29.340 --> 00:13:33.050
with a level of surveillance
we would never allow otherwise.

00:13:33.050 --> 00:13:34.910
We would never agree
that we would all

00:13:34.910 --> 00:13:38.340
put our tracking device in
our pockets every morning.

00:13:38.340 --> 00:13:40.930
But we carry a cell phone.

00:13:40.930 --> 00:13:45.620
Or if the FBI said, whenever
you make a new friend,

00:13:45.620 --> 00:13:47.362
you must alert the police.

00:13:47.362 --> 00:13:48.945
You laugh, but you
all alert Facebook.

00:13:51.690 --> 00:13:54.050
Or give the police a copy
of all our correspondence.

00:13:54.050 --> 00:13:55.510
No, we just put it in Gmail.

00:13:58.540 --> 00:14:02.080
I mean, I don't use Gmail,
but last time I checked,

00:14:02.080 --> 00:14:04.580
about a third of my
email is stored by Google

00:14:04.580 --> 00:14:06.730
because everyone else does.

00:14:09.780 --> 00:14:14.550
And government surveillance
is largely driven by fear.

00:14:14.550 --> 00:14:17.255
On the good side, you can
say it's fear of terrorists

00:14:17.255 --> 00:14:18.210
and fear of criminals.

00:14:18.210 --> 00:14:20.580
On the bad side, you can
say it's fear of dissidence

00:14:20.580 --> 00:14:26.117
and fear of new ideas and
fear of political organizing,

00:14:26.117 --> 00:14:28.200
depending on which country
we're talking about it.

00:14:32.080 --> 00:14:37.175
But this is how it happens,
and this is how it extends.

00:14:40.660 --> 00:14:44.460
So, I spend the first
part of the book on that,

00:14:44.460 --> 00:14:48.610
on all this complex
discussion of surveillance.

00:14:48.610 --> 00:14:50.260
Corporate and
government, what I think

00:14:50.260 --> 00:14:52.509
of as the public-private
surveillance partnership, how

00:14:52.509 --> 00:14:54.559
things are working together.

00:14:54.559 --> 00:14:56.100
I use a lot of the
Snowden documents.

00:14:56.100 --> 00:14:58.920
I use a lot of things that
happen in the commercial world.

00:14:58.920 --> 00:15:02.782
It's important to understand
that this isn't just the US.

00:15:02.782 --> 00:15:05.700
The Snowden documents have
given us an extraordinary window

00:15:05.700 --> 00:15:09.160
into US NSA
surveillance, but there's

00:15:09.160 --> 00:15:11.470
no reason to believe that
other countries don't

00:15:11.470 --> 00:15:13.660
do the same thing to the
extent of their ability,

00:15:13.660 --> 00:15:15.600
certainly China,
Russia, other countries

00:15:15.600 --> 00:15:18.270
with big military budgets.

00:15:18.270 --> 00:15:20.460
And a lot of what
we see the NSA doing

00:15:20.460 --> 00:15:24.820
are straightforward
extensions of hacker tools.

00:15:24.820 --> 00:15:27.500
To me, the most surprising thing
about the Snowden documents

00:15:27.500 --> 00:15:30.900
is the lack of surprising
things in the Snowden documents,

00:15:30.900 --> 00:15:33.980
that the NSA is
not made of magic.

00:15:33.980 --> 00:15:36.400
You'd think with their
budget there'd be some magic.

00:15:36.400 --> 00:15:39.970
But it seems to turn
out that it's just

00:15:39.970 --> 00:15:44.340
putting more formal
process and bigger budget

00:15:44.340 --> 00:15:46.970
and more people on
the [? tach ?] tools,

00:15:46.970 --> 00:15:48.880
and you get
straightforward extensions

00:15:48.880 --> 00:15:54.530
of commercial and hacker tools.

00:15:54.530 --> 00:15:56.140
And technology democratizes us.

00:15:56.140 --> 00:15:58.050
I make this point
again and again.

00:15:58.050 --> 00:16:01.230
Today's top secret NSA programs
are tomorrow's PhD theses

00:16:01.230 --> 00:16:03.440
and the next day's hacker tools.

00:16:03.440 --> 00:16:08.067
So when you see a really
impressive NSA trick,

00:16:08.067 --> 00:16:09.650
it's a preview of
what the hackers are

00:16:09.650 --> 00:16:11.980
going to do two years from now.

00:16:11.980 --> 00:16:16.130
The latest one was the ability--
we saw this from Kaspersky--

00:16:16.130 --> 00:16:20.090
the ability to hide malware in
the boot areas of hard drives,

00:16:20.090 --> 00:16:22.080
a truly impressive
technique that even

00:16:22.080 --> 00:16:25.320
if you take your computer,
erase all the memory,

00:16:25.320 --> 00:16:27.870
reinstall the operating
system, it's still infected.

00:16:31.000 --> 00:16:36.670
So this, I guess in 2008,
was a secret NSA program.

00:16:36.670 --> 00:16:39.310
After Kaspersky
talked about it, I

00:16:39.310 --> 00:16:41.570
started looking at
the academic research,

00:16:41.570 --> 00:16:43.579
and I found three papers
over the past few years

00:16:43.579 --> 00:16:44.995
talking about the
same techniques.

00:16:47.520 --> 00:16:49.500
So do we know that the
government of China

00:16:49.500 --> 00:16:52.294
doesn't do this?

00:16:52.294 --> 00:16:53.710
I wouldn't trust
that they didn't.

00:16:53.710 --> 00:16:56.750
I still believe the best way,
when you come back from a trip

00:16:56.750 --> 00:17:00.040
to China, to scrub your computer
is to throw it away and get

00:17:00.040 --> 00:17:00.540
a new one.

00:17:03.570 --> 00:17:08.680
So when we're looking
at these techniques,

00:17:08.680 --> 00:17:13.109
we need to keep in mind
that they're not NSA only.

00:17:13.109 --> 00:17:16.930
I spend the second
part of the book on,

00:17:16.930 --> 00:17:19.120
it's a section called
What's at Stake where

00:17:19.120 --> 00:17:23.349
I talk about why this
matters, why privacy matters,

00:17:23.349 --> 00:17:24.650
why data matters.

00:17:24.650 --> 00:17:28.820
And I head on attack
the two tropes

00:17:28.820 --> 00:17:31.370
you'll hear in common
discussion about this.

00:17:31.370 --> 00:17:34.312
And the first one is
security versus privacy.

00:17:34.312 --> 00:17:36.270
And the second is, if
you have nothing to hide,

00:17:36.270 --> 00:17:38.080
you have nothing to fear.

00:17:38.080 --> 00:17:40.750
Those are the two
main talking points

00:17:40.750 --> 00:17:45.850
you'll hear out in
the common world.

00:17:45.850 --> 00:17:48.670
Security versus privacy, I
think that's obviously not true.

00:17:48.670 --> 00:17:51.310
Whenever someone says security
versus privacy, look at them

00:17:51.310 --> 00:17:55.135
and say door lock,
burglar alarm, tall fence.

00:17:55.135 --> 00:17:58.810
There's a lot of security that
has nothing to do with privacy.

00:17:58.810 --> 00:18:01.020
And also you don't
actually feel secure

00:18:01.020 --> 00:18:03.130
when your privacy is violated.

00:18:03.130 --> 00:18:06.520
This notion that they don't
go hand in hand I think just

00:18:06.520 --> 00:18:11.400
doesn't make any sense, that
privacy is a part of security.

00:18:11.400 --> 00:18:14.310
To be sure, there are
aspects of security

00:18:14.310 --> 00:18:17.640
that require a privacy
violation, police investigating

00:18:17.640 --> 00:18:19.320
crimes.

00:18:19.320 --> 00:18:21.680
And we have lots
of systems in place

00:18:21.680 --> 00:18:25.050
to ensure that that
process happens fairly

00:18:25.050 --> 00:18:27.080
with minimal abuse.

00:18:27.080 --> 00:18:29.880
And I talk a lot about them.

00:18:29.880 --> 00:18:33.230
So the nothing to hide
nothing to fear argument, that

00:18:33.230 --> 00:18:37.920
mischaracterizes privacy
as something to hide.

00:18:37.920 --> 00:18:43.640
Privacy is much more about
autonomy and power and control.

00:18:43.640 --> 00:18:48.100
Privacy is my ability to
decide how I present myself

00:18:48.100 --> 00:18:51.150
to the world.

00:18:51.150 --> 00:18:55.690
And stripping someone of
that is very dehumanizing.

00:18:55.690 --> 00:19:03.280
And lots of scientific studies
on surveillance bear that out.

00:19:03.280 --> 00:19:10.200
And more importantly, privacy
allows society to move forward.

00:19:10.200 --> 00:19:11.910
Right now in the
United States, we're

00:19:11.910 --> 00:19:15.690
at the brink of two
amazing social changes.

00:19:15.690 --> 00:19:19.560
Gay marriage will be legal
in all 50 states soon,

00:19:19.560 --> 00:19:23.000
and marijuana will be legal
in all 50 states soon,

00:19:23.000 --> 00:19:27.300
two things which right now feel
inevitable but three years ago

00:19:27.300 --> 00:19:30.380
would have felt impossible.

00:19:30.380 --> 00:19:36.940
Now, the process to get from one
to the other requires privacy.

00:19:36.940 --> 00:19:40.010
In order for pot
to become legal,

00:19:40.010 --> 00:19:42.640
it has to be that sometime
in the past, someone

00:19:42.640 --> 00:19:44.190
tried pot and said, you know?

00:19:44.190 --> 00:19:45.450
That wasn't that bad.

00:19:48.240 --> 00:19:50.880
And then a couple of
generations occur,

00:19:50.880 --> 00:19:52.610
and more and more
people think, you know?

00:19:52.610 --> 00:19:56.100
That's not that bad.

00:19:56.100 --> 00:19:59.910
But if you could imagine a
world with perfect surveillance,

00:19:59.910 --> 00:20:05.880
and every gay relationship
is stopped and prosecuted,

00:20:05.880 --> 00:20:09.950
absolutely you'd never get to
a world where a lot of people

00:20:09.950 --> 00:20:11.860
are saying, why should I care?

00:20:11.860 --> 00:20:15.560
This is fine.

00:20:15.560 --> 00:20:17.720
It's a really
interesting process

00:20:17.720 --> 00:20:23.640
from something being illegal,
to illegal and tolerated,

00:20:23.640 --> 00:20:27.820
to illegal and really
tolerated, to legal.

00:20:27.820 --> 00:20:31.540
And that process only works
through imperfect enforcement.

00:20:34.240 --> 00:20:36.510
So I spent a lot of time
on the value of privacy.

00:20:36.510 --> 00:20:38.627
I talk about the business
reasons and the fact

00:20:38.627 --> 00:20:40.710
that we have a lot of
trouble in the United States

00:20:40.710 --> 00:20:43.915
now that US products and
services are not trusted.

00:20:46.470 --> 00:20:48.150
I remember sometime last year.

00:20:48.150 --> 00:20:50.300
It was after the
muscular revelations,

00:20:50.300 --> 00:20:54.074
and Google did a bunch of things
to secure their data centers.

00:20:54.074 --> 00:20:55.990
And it was Eric Schmidt
who said that now he's

00:20:55.990 --> 00:20:58.840
confident that the NSA
can't penetrate his systems.

00:20:58.840 --> 00:21:00.530
One, I don't think that's true.

00:21:00.530 --> 00:21:03.200
But even the best he
actually could say

00:21:03.200 --> 00:21:05.980
is, the NSA can't
penetrate my systems

00:21:05.980 --> 00:21:08.560
except for the ways
I don't know about

00:21:08.560 --> 00:21:12.100
and the ways I've
been legally compelled

00:21:12.100 --> 00:21:14.760
not to tell you about.

00:21:14.760 --> 00:21:19.410
We know in the United States
companies have gotten orders

00:21:19.410 --> 00:21:21.440
to deliberately
break their security

00:21:21.440 --> 00:21:23.800
and not tell anybody about it.

00:21:23.800 --> 00:21:27.010
And as long as we're living
in a country where those sorts

00:21:27.010 --> 00:21:31.900
of secret orders are possible,
we cannot get to a point where

00:21:31.900 --> 00:21:36.170
we can trust any company's
attestations about its

00:21:36.170 --> 00:21:36.770
security.

00:21:36.770 --> 00:21:38.250
This is very bad.

00:21:38.250 --> 00:21:40.760
This is very dangerous.

00:21:40.760 --> 00:21:42.275
I'm amazed we are at this point.

00:21:45.960 --> 00:21:47.140
So I talk about all that.

00:21:47.140 --> 00:21:51.950
The third part of the book
is entitled How to Fix It,

00:21:51.950 --> 00:21:54.360
and there I spend
time on solutions.

00:21:54.360 --> 00:21:56.280
This is very hard.

00:21:56.280 --> 00:21:59.620
In the first chapter I just
talk about the principles.

00:21:59.620 --> 00:22:01.920
And one of the principles
I want to mention here

00:22:01.920 --> 00:22:05.310
is the notion that we have just
one network and one answer.

00:22:05.310 --> 00:22:08.710
The NSA traditionally
has a dual mission.

00:22:08.710 --> 00:22:10.900
You look back at their
Cold War origins,

00:22:10.900 --> 00:22:13.120
they had two different
complementary missions.

00:22:13.120 --> 00:22:15.640
One was to protect
US communications.

00:22:15.640 --> 00:22:18.880
The other was to attack
Soviet communications.

00:22:18.880 --> 00:22:23.650
And those two missions
were able to coexist

00:22:23.650 --> 00:22:28.270
because they were separate.

00:22:28.270 --> 00:22:29.130
Think of radios.

00:22:29.130 --> 00:22:31.230
The US and Soviet Union
had different radios

00:22:31.230 --> 00:22:33.890
on different frequencies,
different hardware,

00:22:33.890 --> 00:22:35.200
different systems.

00:22:35.200 --> 00:22:38.920
And you could attack theirs
while defending ours.

00:22:41.500 --> 00:22:46.530
If you were eavesdropping on
an undersea cable out of Moscow

00:22:46.530 --> 00:22:50.720
to Vladivostok, you would
never get conversations

00:22:50.720 --> 00:22:54.170
from Peoria in it.

00:22:54.170 --> 00:22:59.270
The physical object allowed
you to separate the defensive

00:22:59.270 --> 00:23:01.200
and the offensive mission.

00:23:01.200 --> 00:23:04.880
That doesn't work
on the internet.

00:23:04.880 --> 00:23:10.360
Now everybody uses TCP/IP and
Cisco routers and iPhones,

00:23:10.360 --> 00:23:12.470
Chrome browsers.

00:23:12.470 --> 00:23:16.270
We're all using the same stuff.

00:23:16.270 --> 00:23:20.090
And it cannot be that you can
defend ours and attack theirs

00:23:20.090 --> 00:23:20.930
at the same time.

00:23:20.930 --> 00:23:23.950
You have to make choices.

00:23:23.950 --> 00:23:26.280
If you find a vulnerability,
you can either

00:23:26.280 --> 00:23:28.680
use it to defend ours.

00:23:28.680 --> 00:23:33.350
You can fix it, at the same
time making them more secure.

00:23:33.350 --> 00:23:37.240
Or we can use it to attack
them, at the same time leaving

00:23:37.240 --> 00:23:39.850
us less secure.

00:23:39.850 --> 00:23:43.566
And you have to make that
choice again and again.

00:23:43.566 --> 00:23:44.940
And I think from
everything we've

00:23:44.940 --> 00:23:49.530
learned that security is more
important than surveillance.

00:23:49.530 --> 00:23:51.040
Take StingRays as an example.

00:23:51.040 --> 00:23:54.200
A StingRay is basically
a fake cell phone tower.

00:23:54.200 --> 00:23:55.624
That's a product name.

00:23:55.624 --> 00:23:57.040
It's probably a
series of products

00:23:57.040 --> 00:24:00.940
from Harris Corporation
sold to the FBI that

00:24:00.940 --> 00:24:03.160
allow the FBI without
a warrant to figure out

00:24:03.160 --> 00:24:05.110
who's in a location
and get a bunch of data

00:24:05.110 --> 00:24:06.520
from their phones.

00:24:06.520 --> 00:24:09.160
FBI's been very secretive
about this to the extent

00:24:09.160 --> 00:24:11.420
that they will
instruct prosecutors

00:24:11.420 --> 00:24:12.779
to lie about it in court.

00:24:12.779 --> 00:24:14.570
Even though a lot of
information is public,

00:24:14.570 --> 00:24:17.060
they're still very,
very secretive.

00:24:17.060 --> 00:24:20.830
So last year, some website,
I forget which one,

00:24:20.830 --> 00:24:26.680
started looking around DC
and found these StingRays

00:24:26.680 --> 00:24:31.110
all over the city run
by who knows who for who

00:24:31.110 --> 00:24:33.770
knows what reason.

00:24:33.770 --> 00:24:34.750
So here's our choice.

00:24:34.750 --> 00:24:38.820
Either we can exploit this
technology, leaving all of us

00:24:38.820 --> 00:24:41.920
vulnerable to whatever other
country or organization wants

00:24:41.920 --> 00:24:43.290
to exploit the technology.

00:24:43.290 --> 00:24:48.970
Or we can fix it, add some
authentication to our air

00:24:48.970 --> 00:24:53.470
to ground, air to
cell phone traffic,

00:24:53.470 --> 00:24:59.620
depriving the FBI of a tool
but making us all secure.

00:24:59.620 --> 00:25:01.490
I talk about a bunch
of these principles.

00:25:01.490 --> 00:25:04.560
I talk about things I think
governments should do,

00:25:04.560 --> 00:25:06.210
things I think
corporations should do,

00:25:06.210 --> 00:25:08.870
which often are government
[INAUDIBLE] corporations.

00:25:11.560 --> 00:25:13.260
Things that I
think people should

00:25:13.260 --> 00:25:15.850
do as individuals,
technologies we can employ.

00:25:15.850 --> 00:25:17.050
This is very hard.

00:25:17.050 --> 00:25:22.080
Everyone wants to know what can
they do to avoid surveillance.

00:25:22.080 --> 00:25:25.469
And a lot of the
answer is not much,

00:25:25.469 --> 00:25:28.010
because so much of our data is
in the hands of third parties.

00:25:30.580 --> 00:25:32.950
If Anthem Health
gets hacked, there's

00:25:32.950 --> 00:25:35.640
nothing I can do about it.

00:25:35.640 --> 00:25:37.990
I can't even decide whether
or not they get my data.

00:25:37.990 --> 00:25:40.250
It probably is mandated
by my employer.

00:25:43.020 --> 00:25:47.430
And again, the opting out tools
are just not viable answers.

00:25:47.430 --> 00:25:53.640
So we can do things like use
encrypted email or SSL or OTR.

00:25:53.640 --> 00:25:56.170
But they tend to work
around the edges.

00:25:56.170 --> 00:25:58.460
They don't affect the
metadata, and they rarely

00:25:58.460 --> 00:26:01.116
affect third-party data.

00:26:01.116 --> 00:26:02.740
So we're living in
an interesting world

00:26:02.740 --> 00:26:05.565
where the solutions are
a combination of law

00:26:05.565 --> 00:26:06.510
and technology.

00:26:06.510 --> 00:26:08.230
Even worse, we're
living in a world

00:26:08.230 --> 00:26:11.540
where law can
undermine technology.

00:26:11.540 --> 00:26:12.270
Right?

00:26:12.270 --> 00:26:15.520
Google gets a secret court
order to break their security

00:26:15.520 --> 00:26:20.116
and fights it in court, and it's
two years out, and you lose.

00:26:20.116 --> 00:26:22.830
And we also live
in a world where

00:26:22.830 --> 00:26:26.590
two caffeine-fueled
undergrads at Stanford

00:26:26.590 --> 00:26:31.300
could write an app that
undermines the law, which

00:26:31.300 --> 00:26:33.930
means that both have
to work in order for us

00:26:33.930 --> 00:26:37.630
to get security again.

00:26:37.630 --> 00:26:40.570
And this is a thorny problem.

00:26:40.570 --> 00:26:43.840
What I want people to do, when
people ask what should I do,

00:26:43.840 --> 00:26:46.850
the thing I say is, we need to
start observing surveillance

00:26:46.850 --> 00:26:49.170
and talking about surveillance.

00:26:49.170 --> 00:26:52.000
Pew Research did an
international survey

00:26:52.000 --> 00:26:54.880
on the effects of the
Snowden documents.

00:26:54.880 --> 00:26:57.390
And one of the things
they asked is, have you

00:26:57.390 --> 00:27:01.640
taken any steps to protect
your privacy since the Snowden

00:27:01.640 --> 00:27:02.980
revelations.

00:27:02.980 --> 00:27:04.670
And they produced
numbers by country.

00:27:04.670 --> 00:27:08.320
I did the calculation by
population and percentage

00:27:08.320 --> 00:27:11.050
and came up with a figure
that 700 million people

00:27:11.050 --> 00:27:14.240
on the planet have
done something

00:27:14.240 --> 00:27:17.020
to protect their privacy in the
wake of the Snowden documents

00:27:17.020 --> 00:27:18.200
and the NSA's activities.

00:27:18.200 --> 00:27:22.230
Now, probably most of the stuff
people did wasn't effective.

00:27:22.230 --> 00:27:24.780
Probably some of the people
who said I did something

00:27:24.780 --> 00:27:27.060
didn't actually do something.

00:27:27.060 --> 00:27:29.430
But what that is
a measure of, it's

00:27:29.430 --> 00:27:34.500
a measure of people's changing
their perceptions of data

00:27:34.500 --> 00:27:37.210
and security based
on these documents.

00:27:37.210 --> 00:27:39.500
I can't think of
another issue that

00:27:39.500 --> 00:27:43.620
moved 700 million people on this
planet in the course of a year.

00:27:43.620 --> 00:27:46.255
That is truly amazing.

00:27:46.255 --> 00:27:48.630
What Snowden said is he wanted
to start the conversation.

00:27:48.630 --> 00:27:51.330
I think this proves he did it.

00:27:51.330 --> 00:27:56.460
And I think we have to continue
the conversation, so observing

00:27:56.460 --> 00:27:58.090
surveillance and discussing it.

00:27:58.090 --> 00:28:01.570
And it's not a matter of
all surveillance is bad.

00:28:01.570 --> 00:28:04.330
I think this is a complex issue.

00:28:04.330 --> 00:28:10.460
This is an issue of
designing systems

00:28:10.460 --> 00:28:13.780
to extract group
value from our data

00:28:13.780 --> 00:28:16.122
while protecting
people individually.

00:28:16.122 --> 00:28:18.365
And I actually think this
is a fundamental issue

00:28:18.365 --> 00:28:20.110
of the information age.

00:28:20.110 --> 00:28:23.250
Our data has enormous
value to us collectively,

00:28:23.250 --> 00:28:27.540
and our data has enormous
value to us each individually.

00:28:27.540 --> 00:28:31.140
How do we reconcile this?

00:28:31.140 --> 00:28:32.890
What law enforcement
will say is,

00:28:32.890 --> 00:28:36.460
we need to get your
data to prevent crime.

00:28:36.460 --> 00:28:41.460
NSA will say, we need your
data to prevent terrorism.

00:28:41.460 --> 00:28:44.420
Behavioral data is
valuable for advertising.

00:28:44.420 --> 00:28:46.495
Medical data, I think
there's huge value

00:28:46.495 --> 00:28:48.000
in taking all of
our medical data,

00:28:48.000 --> 00:28:51.180
putting it in one big database,
and letting researchers at it.

00:28:51.180 --> 00:28:54.080
Yet it's incredibly personal.

00:28:54.080 --> 00:28:56.390
Or something as easy
as movement data,

00:28:56.390 --> 00:28:59.320
I like it when
Google Maps tells me

00:28:59.320 --> 00:29:01.070
real-time traffic
information based

00:29:01.070 --> 00:29:02.270
on real-time surveillance.

00:29:02.270 --> 00:29:05.270
That is a valuable service.

00:29:05.270 --> 00:29:09.610
How do we extract these valuable
group benefits of our data

00:29:09.610 --> 00:29:13.940
while protecting us
each individually?

00:29:13.940 --> 00:29:18.070
And I think this is a very
core problem to big data

00:29:18.070 --> 00:29:20.950
and one we really
need to address.

00:29:20.950 --> 00:29:22.990
And I've said this
before, but I think

00:29:22.990 --> 00:29:25.430
data is the pollution problem
of the information age.

00:29:25.430 --> 00:29:28.730
I think it's a reasonably
robust analogy.

00:29:28.730 --> 00:29:30.670
All processes produce it.

00:29:30.670 --> 00:29:31.940
It stays around.

00:29:31.940 --> 00:29:38.170
We're discussing secondary uses,
recycling, storage, disposal.

00:29:38.170 --> 00:29:42.290
And I really think that in
the same way that we look back

00:29:42.290 --> 00:29:46.710
at the titans of industry
100 years ago, 150 years ago

00:29:46.710 --> 00:29:49.700
ignoring pollution as they built
the industrial age, that we're

00:29:49.700 --> 00:29:51.910
going to be judged
by our grandchildren

00:29:51.910 --> 00:29:54.050
and great grandchildren
on the decisions

00:29:54.050 --> 00:29:57.230
we make about data here
in the early decades

00:29:57.230 --> 00:29:59.390
of the information age.

00:29:59.390 --> 00:30:01.830
So that's the book I wrote,
and that's why I wrote it.

00:30:01.830 --> 00:30:05.064
I'm happy to take questions.

00:30:05.064 --> 00:30:13.482
[APPLAUSE]

00:30:13.482 --> 00:30:15.440
AUDIENCE: I'd like to
ask you a little bit more

00:30:15.440 --> 00:30:17.160
about Edward Snowden.

00:30:17.160 --> 00:30:19.545
Last year Governor Bill
Richardson was where you were,

00:30:19.545 --> 00:30:21.810
and he said, I don't think
Snowden is a patriot,

00:30:21.810 --> 00:30:25.544
I think Snowden, what
he did was wrong.

00:30:25.544 --> 00:30:26.960
Do you think Snowden
is a patriot?

00:30:26.960 --> 00:30:29.460
Do you think we need to keep
relying on people like Snowden

00:30:29.460 --> 00:30:30.710
to keep exposing these things?

00:30:30.710 --> 00:30:33.525
And if so, is that any way
to run a society, to really

00:30:33.525 --> 00:30:35.900
just fingers crossed somebody
puts themselves on the line

00:30:35.900 --> 00:30:37.540
and has to go and
escape to Russia

00:30:37.540 --> 00:30:39.670
and hope the CIA
doesn't get them?

00:30:39.670 --> 00:30:42.920
BRUCE SCHNEIER: Well, that
was a pretty extreme story.

00:30:42.920 --> 00:30:46.350
I think whistleblowers are
extremely valuable in society.

00:30:46.350 --> 00:30:50.620
And I think they act
somewhat as a random audit,

00:30:50.620 --> 00:30:54.130
and they do provide
a great service.

00:30:54.130 --> 00:30:56.900
And yes, it's not something
you want to rely on.

00:30:56.900 --> 00:30:59.280
But they are a safety valve.

00:30:59.280 --> 00:31:02.180
And Yochai Benkler has
written a paper on this.

00:31:02.180 --> 00:31:08.000
I think it's called "Leaky
Leviathan" talking about how

00:31:08.000 --> 00:31:11.320
good systems, robust
systems are leaky,

00:31:11.320 --> 00:31:14.240
and the leaks are valuable.

00:31:14.240 --> 00:31:18.410
I personally think what Snowden
did was moral and sound.

00:31:18.410 --> 00:31:21.670
But the discussion
of patriot or traitor

00:31:21.670 --> 00:31:23.380
is really a history discussion.

00:31:23.380 --> 00:31:27.110
I tend not to like it because it
focuses the story on the person

00:31:27.110 --> 00:31:28.770
rather than the documents.

00:31:28.770 --> 00:31:32.050
And I think the real story
is the documents and the NSA,

00:31:32.050 --> 00:31:34.030
and not the method
by which we learned

00:31:34.030 --> 00:31:36.278
about the documents and the NSA.

00:31:36.278 --> 00:31:37.274
AUDIENCE: Thank you.

00:31:40.760 --> 00:31:43.730
AUDIENCE: So in hindsight when
the NSA proposed the Clipper

00:31:43.730 --> 00:31:47.090
chip, it was obvious that they
wanted to do surveillance,

00:31:47.090 --> 00:31:49.490
and collectively we rejected it.

00:31:49.490 --> 00:31:52.870
And now we have the NSA
hacking all our systems.

00:31:52.870 --> 00:31:57.120
Do you think some sort of key
escrow or voluntary key escrow

00:31:57.120 --> 00:31:59.700
perhaps might be the way to go?

00:31:59.700 --> 00:32:00.700
BRUCE SCHNEIER: I don't.

00:32:00.700 --> 00:32:02.060
There are a whole
lot of reasons why

00:32:02.060 --> 00:32:04.559
that was a dumb idea in the
'90s and is an equally dumb idea

00:32:04.559 --> 00:32:05.590
today.

00:32:05.590 --> 00:32:08.240
The basic reason is I
can't make it secure.

00:32:08.240 --> 00:32:11.670
Basically I can't
build any back door

00:32:11.670 --> 00:32:14.620
into a system that
somehow regulates

00:32:14.620 --> 00:32:17.035
for the morality of
the person using it.

00:32:17.035 --> 00:32:22.130
That as soon as I build a method
for access into the system,

00:32:22.130 --> 00:32:24.650
I have to assume that the bad
guys will use it just as much

00:32:24.650 --> 00:32:26.820
as the bad guys,
or possibly more.

00:32:26.820 --> 00:32:30.510
And I have a much better secure
system if nobody has access.

00:32:30.510 --> 00:32:33.460
And that's why key escrow
didn't make sense then,

00:32:33.460 --> 00:32:36.190
and that's why it
doesn't make sense now.

00:32:36.190 --> 00:32:40.292
I tend to be OK
with NSA hacking.

00:32:40.292 --> 00:32:41.000
It's interesting.

00:32:41.000 --> 00:32:43.125
One of the things we learned
from the NSA documents

00:32:43.125 --> 00:32:46.250
is that cryptography
works, that cryptography

00:32:46.250 --> 00:32:50.650
properly implemented gives us
NSA trouble at least at scale.

00:32:50.650 --> 00:32:55.290
And I was saying this earlier,
the NSA is not made of magic.

00:32:55.290 --> 00:32:57.350
And they are subject
to the same laws

00:32:57.350 --> 00:33:01.750
of mathematics and physics and
economics as everybody else is.

00:33:01.750 --> 00:33:05.720
And what good cryptography
does is leverage the economics.

00:33:05.720 --> 00:33:08.390
I actually have no
doubt that if the NSA

00:33:08.390 --> 00:33:10.260
wants to be in your network.

00:33:10.260 --> 00:33:11.520
They are in your network.

00:33:11.520 --> 00:33:12.040
Period.

00:33:12.040 --> 00:33:13.070
Done.

00:33:13.070 --> 00:33:15.660
If they are not, it's
for one of two reasons.

00:33:15.660 --> 00:33:19.305
One, it is illegal under their
very aggressive interpretations

00:33:19.305 --> 00:33:20.470
of the law.

00:33:20.470 --> 00:33:23.950
And two, you are
not that important

00:33:23.950 --> 00:33:26.060
in the scheme of
budgetary allocation.

00:33:29.340 --> 00:33:33.180
Breaking crypto, being able
to read encrypted traffic

00:33:33.180 --> 00:33:37.830
en masse, is a much
more cost-effective way

00:33:37.830 --> 00:33:39.930
of getting everybody's data.

00:33:39.930 --> 00:33:43.640
If the NSA has to target
companies or individuals one

00:33:43.640 --> 00:33:48.560
by one, it's going to force
them to target on the bad guys.

00:33:48.560 --> 00:33:50.790
And maybe if we're lucky
the Belgian phone company

00:33:50.790 --> 00:33:53.432
won't make the cut.

00:33:53.432 --> 00:33:57.140
AUDIENCE: I had a follow-up
question about ways

00:33:57.140 --> 00:33:59.190
of securely aggregating data.

00:33:59.190 --> 00:34:01.850
I know the biggest recent
thing I've heard of

00:34:01.850 --> 00:34:03.960
is fully homomorphic
encryption, which

00:34:03.960 --> 00:34:07.480
if it ever becomes feasible,
could in theory aggregate data.

00:34:07.480 --> 00:34:09.860
Do you know of any other
technical solutions

00:34:09.860 --> 00:34:12.321
for doing the
hidden aggregation?

00:34:12.321 --> 00:34:13.820
BRUCE SCHNEIER:
There really aren't.

00:34:13.820 --> 00:34:16.739
Homomorphic encryption
still is theoretical.

00:34:16.739 --> 00:34:19.159
I'm not convinced it'll
ever be practical.

00:34:19.159 --> 00:34:21.155
I mean, I'd like to be
wrong, but it is going

00:34:21.155 --> 00:34:23.320
to require a lot more advances.

00:34:23.320 --> 00:34:27.294
Right now, I don't know
if homomorphic encryption

00:34:27.294 --> 00:34:29.460
would change it because of
so much hardware hacking.

00:34:29.460 --> 00:34:33.650
We have to trust the
platforms that have our data.

00:34:33.650 --> 00:34:36.150
We have no choice.

00:34:36.150 --> 00:34:40.480
They are trusted in the sense of
they can subvert our security,

00:34:40.480 --> 00:34:42.679
not that they are trustworthy.

00:34:42.679 --> 00:34:46.100
And because we are
seeing so much hacking

00:34:46.100 --> 00:34:50.179
underneath the
software layer, I don't

00:34:50.179 --> 00:34:54.190
know if homomorphic
software-- how much

00:34:54.190 --> 00:34:57.040
is that going to help when you
have all this hardware hacking?

00:34:57.040 --> 00:34:59.310
So we really need to
rethink our trust models.

00:34:59.310 --> 00:35:03.420
They seem to be failing in
this world of everybody hacking

00:35:03.420 --> 00:35:04.531
everything.

00:35:04.531 --> 00:35:05.239
AUDIENCE: Thanks.

00:35:08.890 --> 00:35:10.390
AUDIENCE: Thank you
for coming here.

00:35:10.390 --> 00:35:14.350
I know you said everybody
asks you, what can I do?

00:35:14.350 --> 00:35:21.110
But this is a particularly--
we are part of the surveillance

00:35:21.110 --> 00:35:22.310
system.

00:35:22.310 --> 00:35:25.420
And it was described
to me when I first

00:35:25.420 --> 00:35:28.230
got here that the company only
works because users trust us.

00:35:28.230 --> 00:35:29.914
If we lose user
trust, then that's

00:35:29.914 --> 00:35:31.455
an existential threat
to the company.

00:35:34.426 --> 00:35:35.800
And because of
these revelations,

00:35:35.800 --> 00:35:37.290
we're kind of seen
as the bad guy.

00:35:37.290 --> 00:35:39.270
Now, I know you
don't particularly

00:35:39.270 --> 00:35:44.720
have a stake in the
company, but people here

00:35:44.720 --> 00:35:46.940
tend to be pretty well
meaning about this.

00:35:46.940 --> 00:35:52.440
What can we do to lean towards
the good uses of surveillance

00:35:52.440 --> 00:35:54.060
as opposed to the evil ones?

00:35:54.060 --> 00:35:57.375
And how can we move the
system towards that?

00:35:57.375 --> 00:35:59.250
BRUCE SCHNEIER: So, I
have a few suggestions.

00:35:59.250 --> 00:36:01.885
The first one is
transparency, that the more

00:36:01.885 --> 00:36:03.570
that your systems
are transparent,

00:36:03.570 --> 00:36:06.100
the more they're trusted.

00:36:06.100 --> 00:36:12.250
And I'd like there to be some
way to make the search results,

00:36:12.250 --> 00:36:15.350
the system that produces search
results, more transparent.

00:36:15.350 --> 00:36:16.580
I know that's hard.

00:36:16.580 --> 00:36:20.500
I know there's proprietary
data all through that.

00:36:20.500 --> 00:36:22.560
But the more
transparency the better.

00:36:22.560 --> 00:36:26.230
And that's my first
suggestion in all systems.

00:36:26.230 --> 00:36:30.730
The second is we need to fight
this notion of secret law,

00:36:30.730 --> 00:36:33.650
that as long as you can
be legally compelled

00:36:33.650 --> 00:36:37.820
to lie to all of us about
how secure your stuff is,

00:36:37.820 --> 00:36:39.710
there can be no final trust.

00:36:39.710 --> 00:36:40.880
Now, this isn't your fault.

00:36:40.880 --> 00:36:42.960
This is something
you've been thrust into.

00:36:42.960 --> 00:36:46.190
But you could help
us solve this.

00:36:46.190 --> 00:36:51.320
So fighting these secret orders
in every way possible I think

00:36:51.320 --> 00:36:53.170
would be a huge thing.

00:36:53.170 --> 00:36:55.930
And I think doing that,
doing that in public

00:36:55.930 --> 00:36:57.970
is another way to
engender trust.

00:36:57.970 --> 00:37:00.090
Microsoft is getting
huge PR value

00:37:00.090 --> 00:37:02.800
out of fighting this court
order to turn over data that's

00:37:02.800 --> 00:37:06.460
in their servers in Ireland.

00:37:06.460 --> 00:37:08.510
On a purely self-serving
point of view,

00:37:08.510 --> 00:37:09.926
it's a great
decision of Microsoft

00:37:09.926 --> 00:37:12.920
to fight, win or lose.

00:37:12.920 --> 00:37:19.580
The third thing is to think
about encrypting Gmail.

00:37:19.580 --> 00:37:23.730
I wonder what the
marginal value is

00:37:23.730 --> 00:37:28.910
from being able to get people's
interest out of their Gmail.

00:37:28.910 --> 00:37:30.710
And my hope is and my
thought is that it's

00:37:30.710 --> 00:37:36.320
low enough that you can
offer more common encryption.

00:37:36.320 --> 00:37:38.400
I mean, yes, if someone's
using Google Now,

00:37:38.400 --> 00:37:41.270
we need to figure out
how to make that work.

00:37:41.270 --> 00:37:44.750
Or I guess you can't use Google
Now if your stuff is encrypted,

00:37:44.750 --> 00:37:48.480
but some way to give
more users easier access

00:37:48.480 --> 00:37:51.730
to email encryption is
something Google can do and make

00:37:51.730 --> 00:37:53.544
an enormous difference.

00:37:53.544 --> 00:37:54.960
So those are my
three suggestions.

00:37:54.960 --> 00:37:57.370
There's probably more
if I thought about it.

00:37:57.370 --> 00:37:58.787
Those are three
that come to mind.

00:37:58.787 --> 00:37:59.620
AUDIENCE: Thank you.

00:37:59.620 --> 00:38:02.130
And just as a note, there's
a public project called End

00:38:02.130 --> 00:38:04.639
to End for doing client-side
email encryption for Gmail.

00:38:04.639 --> 00:38:06.930
It's not widely rolled out,
but we are working on that.

00:38:06.930 --> 00:38:08.570
BRUCE SCHNEIER: Yeah, but I
want it almost to be default.

00:38:08.570 --> 00:38:10.410
I want it to be not
just rolled out.

00:38:10.410 --> 00:38:13.670
I want it to be a thing
that an average Gmail

00:38:13.670 --> 00:38:17.940
user without any
technical knowledge gets.

00:38:17.940 --> 00:38:20.500
The reason SSL works
to the extent it does

00:38:20.500 --> 00:38:21.870
is you're not thinking about it.

00:38:21.870 --> 00:38:24.720
It just works.

00:38:24.720 --> 00:38:28.490
And what you're doing
on trying to make more

00:38:28.490 --> 00:38:30.864
SSL everywhere is great.

00:38:30.864 --> 00:38:32.280
And those are the
things that make

00:38:32.280 --> 00:38:35.620
a difference, because Google
can move so many users just

00:38:35.620 --> 00:38:37.587
by changing a default.

00:38:37.587 --> 00:38:38.420
AUDIENCE: Thank you.

00:38:41.650 --> 00:38:43.870
AUDIENCE: I get the sense
that in this country

00:38:43.870 --> 00:38:46.779
people are more worried about
government surveillance,

00:38:46.779 --> 00:38:48.320
and in Europe people
are more worried

00:38:48.320 --> 00:38:51.680
about corporate surveillance.

00:38:51.680 --> 00:38:54.640
What do you think
is the more damaging

00:38:54.640 --> 00:38:56.202
and risky to our society?

00:38:56.202 --> 00:38:58.160
BRUCE SCHNEIER: So, I
think your generalization

00:38:58.160 --> 00:38:58.974
is largely correct.

00:38:58.974 --> 00:39:00.265
There are certainly exceptions.

00:39:00.265 --> 00:39:02.580
There are a lot of people
in both countries worried

00:39:02.580 --> 00:39:03.980
about the other thing.

00:39:03.980 --> 00:39:05.280
But by and large you're right.

00:39:05.280 --> 00:39:08.350
I think the biggest problem
is the two together,

00:39:08.350 --> 00:39:10.199
that separating
doesn't make sense,

00:39:10.199 --> 00:39:12.240
that it's the public-private
security partnership

00:39:12.240 --> 00:39:13.310
that I worry about.

00:39:13.310 --> 00:39:16.370
It's governments
using corporate data.

00:39:16.370 --> 00:39:19.040
It's corporations getting
government contracts

00:39:19.040 --> 00:39:20.840
and lobbying for
government surveillance.

00:39:20.840 --> 00:39:23.460
Now, these things
working together

00:39:23.460 --> 00:39:25.250
is really what's
causing the problem,

00:39:25.250 --> 00:39:27.800
and separating them
doesn't make sense anymore.

00:39:27.800 --> 00:39:29.540
Because it's all
about power using

00:39:29.540 --> 00:39:33.350
data against the less powerful.

00:39:33.350 --> 00:39:36.222
You're going to be my
last two questions.

00:39:36.222 --> 00:39:37.805
AUDIENCE: I'm wondering
what you think

00:39:37.805 --> 00:39:41.760
of David Brin's thesis in
"The Transparent Society"

00:39:41.760 --> 00:39:46.350
where he says that,
essentially, a world of privacy

00:39:46.350 --> 00:39:51.070
is not something that we
can achieve going forward,

00:39:51.070 --> 00:39:53.340
that loss of privacy
is inevitable.

00:39:53.340 --> 00:39:55.380
But the choice we
have is symmetric

00:39:55.380 --> 00:39:58.120
versus asymmetric
loss of privacy,

00:39:58.120 --> 00:40:02.800
and that the worst outcome is
the one we have right now where

00:40:02.800 --> 00:40:07.540
the powerful, the NSA, get to
do their surveillance in secret,

00:40:07.540 --> 00:40:12.000
and that the way forward is
in the direction that supports

00:40:12.000 --> 00:40:15.750
democratic
accountability that there

00:40:15.750 --> 00:40:18.420
are real limits on their
ability to do things in secret.

00:40:18.420 --> 00:40:21.580
BRUCE SCHNEIER: So, I think what
Brin misses in the analysis is

00:40:21.580 --> 00:40:25.100
how power factors into
it, that you're really

00:40:25.100 --> 00:40:27.660
talking about the powerful
state and corporations,

00:40:27.660 --> 00:40:29.410
the less powerful individuals.

00:40:29.410 --> 00:40:32.940
And that just allowing
surveillance in this direction

00:40:32.940 --> 00:40:34.920
doesn't even the score.

00:40:34.920 --> 00:40:38.600
When a policeman
asks for your ID,

00:40:38.600 --> 00:40:40.750
asking for the
policeman's ID also

00:40:40.750 --> 00:40:43.640
doesn't make that
an even exchange.

00:40:43.640 --> 00:40:46.980
So I'm all in favor
of transparency,

00:40:46.980 --> 00:40:50.650
and I like the idea of
sousveillance and surveillance

00:40:50.650 --> 00:40:51.930
from below.

00:40:51.930 --> 00:40:54.630
But I don't think
that changes things.

00:40:54.630 --> 00:40:58.340
I disagree with his thesis that
loss of privacy is inevitable.

00:40:58.340 --> 00:41:00.060
I think that that's not true.

00:41:00.060 --> 00:41:02.430
That's too fatalistic,
and we haven't lost.

00:41:02.430 --> 00:41:05.730
And there are ways to get
privacy, maybe not technically

00:41:05.730 --> 00:41:08.950
but certainly legally,
because that's

00:41:08.950 --> 00:41:10.550
the kind of society we are.

00:41:13.202 --> 00:41:14.910
AUDIENCE: Several
years ago you published

00:41:14.910 --> 00:41:17.820
on how you maintained
for the guests

00:41:17.820 --> 00:41:22.250
to your home an
open Wi-Fi network.

00:41:22.250 --> 00:41:26.040
In 2015, and particularly
in high-density areas

00:41:26.040 --> 00:41:28.070
like the Bay Area,
would you still

00:41:28.070 --> 00:41:30.342
recommend maintaining
an open Wi-Fi hotspot?

00:41:30.342 --> 00:41:32.050
BRUCE SCHNEIER: So,
I tell you what I do.

00:41:32.050 --> 00:41:34.210
I still have an
open Wi-Fi hotspot

00:41:34.210 --> 00:41:36.480
in my house in Minneapolis.

00:41:36.480 --> 00:41:38.474
I recently got an
apartment in Cambridge

00:41:38.474 --> 00:41:40.390
because I'm spending a
lot of time at Harvard,

00:41:40.390 --> 00:41:41.940
and there I have a password.

00:41:41.940 --> 00:41:44.030
So I guess your
notion of high density

00:41:44.030 --> 00:41:46.020
is what made a difference to me.

00:41:46.020 --> 00:41:48.380
I still think it's fine
to have open wireless.

00:41:48.380 --> 00:41:50.200
I think it's easy and polite.

00:41:50.200 --> 00:41:52.030
But in an apartment
building, I decided

00:41:52.030 --> 00:41:56.990
that putting a password on
it was the right thing to do.

00:41:56.990 --> 00:41:59.250
It's an easy to
guess password, so--

00:41:59.250 --> 00:42:00.150
[APPLAUSE]

00:42:01.950 --> 00:42:05.000
[MUSIC PLAYING]

