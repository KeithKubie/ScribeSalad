WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.916
[MUSIC PLAYING]

00:00:06.230 --> 00:00:08.340
SPEAKER: OK, thank you
everyone for coming.

00:00:08.340 --> 00:00:11.520
Today at Google, we're delighted
to have Mr. Scott Hartley.

00:00:11.520 --> 00:00:15.560
Mr. Hartley is a venture
capitalist and an author.

00:00:15.560 --> 00:00:19.580
He was a partner at
Morh Davidow Ventures,

00:00:19.580 --> 00:00:22.380
a very highly regarded
venture capital firm based out

00:00:22.380 --> 00:00:23.860
of Sand Hill Road.

00:00:23.860 --> 00:00:26.500
Then prior to that, he was a
presidential innovation fellow

00:00:26.500 --> 00:00:29.160
at the White House.

00:00:29.160 --> 00:00:31.330
He has also worked
at Google, Facebook,

00:00:31.330 --> 00:00:35.010
and he was a fellow at
the Harvard Berkman Center

00:00:35.010 --> 00:00:37.740
for Internet and Society.

00:00:37.740 --> 00:00:42.860
He took his BA from Stanford
and his MBA from Columbia.

00:00:42.860 --> 00:00:46.350
And he is a term member on the
Council of Foreign Relations.

00:00:46.350 --> 00:00:48.930
He is here today to discuss
his new book, "The Fuzzy

00:00:48.930 --> 00:00:53.740
and the Techie, Why Liberal Arts
Will Rule the Digital World."

00:00:53.740 --> 00:00:57.100
And joining him on stage is
Google's on own Albert Chen.

00:00:57.100 --> 00:00:59.640
Please join me in welcoming
Mr. Scott Hartley.

00:00:59.640 --> 00:01:01.476
[APPLAUSE]

00:01:03.827 --> 00:01:05.160
ALBERT CHEN: So welcomes, Scott.

00:01:05.160 --> 00:01:07.040
Or I should say, welcome back.

00:01:07.040 --> 00:01:11.070
So let's start with the
inspiration behind the book,

00:01:11.070 --> 00:01:13.474
and what a fuzzy is and
what a techie is as well.

00:01:13.474 --> 00:01:14.890
SCOTT HARTLEY:
Well, first of all,

00:01:14.890 --> 00:01:16.600
it's really a
pleasure and an honor

00:01:16.600 --> 00:01:19.590
to be back on Google
campus about 10 years

00:01:19.590 --> 00:01:21.170
or so after leaving.

00:01:21.170 --> 00:01:25.730
Albert and I go way back,
12-plus years, here at Google.

00:01:25.730 --> 00:01:29.940
So it's really fun to be back
and to join all of you guys.

00:01:29.940 --> 00:01:32.450
So the impetus for writing
the book, "The Fuzzy

00:01:32.450 --> 00:01:36.630
and the Techie" was really
based on my experience, both

00:01:36.630 --> 00:01:39.570
in my own personal journey,
being a political science

00:01:39.570 --> 00:01:42.130
major, being someone who
studied political theory,

00:01:42.130 --> 00:01:45.830
definitely identifying as a
fuzzy, yet coming out of school

00:01:45.830 --> 00:01:49.030
and joining Google and having
all my friends say well,

00:01:49.030 --> 00:01:50.350
what are you doing at Google?

00:01:50.350 --> 00:01:52.650
How are you working in tech?

00:01:52.650 --> 00:01:55.780
And I think this has become
something that people

00:01:55.780 --> 00:01:56.740
understand more today.

00:01:56.740 --> 00:01:59.940
But 10, 12 years ago, I felt
like all my friends coming out

00:01:59.940 --> 00:02:03.650
of university, if they didn't
study electronic engineering

00:02:03.650 --> 00:02:05.501
or computer science,
they didn't understand

00:02:05.501 --> 00:02:07.000
that there were
other people in tech

00:02:07.000 --> 00:02:09.889
and it wasn't just this
monolith of techies.

00:02:09.889 --> 00:02:14.800
And so based on my own personal
journey of being this--

00:02:14.800 --> 00:02:16.449
presumed to be this
black sheep when I

00:02:16.449 --> 00:02:17.740
didn't feel like a black sheep.

00:02:17.740 --> 00:02:19.970
I felt like there were so
many people in this world that

00:02:19.970 --> 00:02:22.136
were participating from all
different walks of life,

00:02:22.136 --> 00:02:25.000
all different sides of
academic background--

00:02:25.000 --> 00:02:30.200
I wanted to tell that story that
was a myth busting of the tech

00:02:30.200 --> 00:02:33.530
monolith that we presume
to be Silicon Valley.

00:02:33.530 --> 00:02:36.540
And the second reason
was because on Sand Hill,

00:02:36.540 --> 00:02:39.940
where I was I in BC, so
many of the companies that

00:02:39.940 --> 00:02:43.100
were pitching, so many of the
companies that were starting

00:02:43.100 --> 00:02:45.101
were founded by people
that were coming from all

00:02:45.101 --> 00:02:46.266
these different backgrounds.

00:02:46.266 --> 00:02:48.855
And I thought the narrative in
Silicon Valley, the narrative

00:02:48.855 --> 00:02:50.790
that we hear about
in the media, has

00:02:50.790 --> 00:02:54.130
been if you don't have a STEM
degree, you're worthless.

00:02:54.130 --> 00:02:56.410
If you didn't drop
out of middle school

00:02:56.410 --> 00:02:57.890
to study computer
science, there's

00:02:57.890 --> 00:02:59.420
no hope for you in the future.

00:02:59.420 --> 00:03:03.680
And the actual observation I
had, the empirical observation

00:03:03.680 --> 00:03:05.710
sitting on Sand Hill
and meeting with all

00:03:05.710 --> 00:03:07.251
these different
entrepreneurs and all

00:03:07.251 --> 00:03:09.860
these different companies
was so many of the good ideas

00:03:09.860 --> 00:03:11.900
were coming from
people that came out

00:03:11.900 --> 00:03:14.360
of different walks of life,
different backgrounds,

00:03:14.360 --> 00:03:17.530
different disciplines,
different passions.

00:03:17.530 --> 00:03:21.120
And the observation was
that the code in some ways

00:03:21.120 --> 00:03:22.900
had been commoditized,
and the ability

00:03:22.900 --> 00:03:27.680
to get things built, we were
getting farther and farther

00:03:27.680 --> 00:03:29.700
away, more and more
abstractions away

00:03:29.700 --> 00:03:33.790
from having to write
really esoteric syntax

00:03:33.790 --> 00:03:37.680
or really learning the details
and nuance of certain coding

00:03:37.680 --> 00:03:38.330
languages.

00:03:38.330 --> 00:03:41.220
We were getting these bigger
and bigger building blocks.

00:03:41.220 --> 00:03:43.739
And so the code in some
ways had been commoditized.

00:03:43.739 --> 00:03:45.280
And really, the
comparative advantage

00:03:45.280 --> 00:03:48.200
of these really good companies
was somebody with passion,

00:03:48.200 --> 00:03:49.860
somebody with the
domain expertise,

00:03:49.860 --> 00:03:53.320
somebody with some sort of
other disciplinary knowledge

00:03:53.320 --> 00:03:55.800
to which they were
playing the code.

00:03:55.800 --> 00:03:58.040
And so that was impetus
for writing the book

00:03:58.040 --> 00:04:00.401
was myth-bust this
narrative, that so many

00:04:00.401 --> 00:04:01.900
of the interesting
companies that we

00:04:01.900 --> 00:04:03.730
were meeting with
and funding and that

00:04:03.730 --> 00:04:07.570
are profiled in the book were
founded by people coming out

00:04:07.570 --> 00:04:09.610
of backgrounds in
philosophy, backgrounds

00:04:09.610 --> 00:04:12.410
in sociology, backgrounds
in political science,

00:04:12.410 --> 00:04:16.180
coming out of fashion or media
or finance or telecom, all

00:04:16.180 --> 00:04:17.680
these different backgrounds.

00:04:17.680 --> 00:04:20.720
And so the terms,
fuzzy and techie,

00:04:20.720 --> 00:04:24.240
is anyone in the room
familiar with these terms?

00:04:24.240 --> 00:04:24.910
No?

00:04:24.910 --> 00:04:29.490
So they date back to the
1960s, 1970s from Stanford,

00:04:29.490 --> 00:04:32.530
where there was this
lighthearted association where

00:04:32.530 --> 00:04:35.690
people would ask a question--
and they still do--

00:04:35.690 --> 00:04:37.980
are you more of a Fuzzy
are you more of a techie?

00:04:37.980 --> 00:04:39.590
Are you taking
more fuzzy classes

00:04:39.590 --> 00:04:41.560
this quarter, more
techie classes?

00:04:41.560 --> 00:04:46.190
And obviously, techie being
computer science, engineering

00:04:46.190 --> 00:04:49.250
disciplines, fuzzy being
more arts, humanities,

00:04:49.250 --> 00:04:50.880
and social sciences.

00:04:50.880 --> 00:04:52.980
But really, the book
is not about one

00:04:52.980 --> 00:04:55.240
versus the other,
fuzzy versus techie.

00:04:55.240 --> 00:04:58.580
It's about this
intertwining of the two,

00:04:58.580 --> 00:05:00.580
because I think you look
under the hood of any

00:05:00.580 --> 00:05:03.370
of these disciplines, any
of these methodologies,

00:05:03.370 --> 00:05:06.120
and if you're studying
social science,

00:05:06.120 --> 00:05:10.460
you're often forced to grapple
with statistical software

00:05:10.460 --> 00:05:12.779
or deal with independent
and dependent variables.

00:05:12.779 --> 00:05:14.570
If you're studying
international relations,

00:05:14.570 --> 00:05:15.950
you're dealing with game theory.

00:05:15.950 --> 00:05:19.020
And if you're studying
mechanical engineering,

00:05:19.020 --> 00:05:21.390
on the flip side, you're
learning design thinking.

00:05:21.390 --> 00:05:24.780
You're using methodologies
of sociology and anthropology

00:05:24.780 --> 00:05:26.750
to do user experience research.

00:05:26.750 --> 00:05:29.240
So I think it's
not that we can say

00:05:29.240 --> 00:05:31.950
one group is uniformly techie
and one group is uniformly

00:05:31.950 --> 00:05:32.650
fuzzy.

00:05:32.650 --> 00:05:35.350
It's much more about how
we blend these two sides

00:05:35.350 --> 00:05:37.950
and how so many of the
innovations that I think

00:05:37.950 --> 00:05:40.850
make Google successful and
make so many great startups

00:05:40.850 --> 00:05:42.880
in Silicon Valley
successful is actually

00:05:42.880 --> 00:05:47.120
this blending of the two, rather
than this narrative of tech

00:05:47.120 --> 00:05:51.020
is going to solve all,
big data, algorithms, AI,

00:05:51.020 --> 00:05:53.270
this uniformly
techie explanation

00:05:53.270 --> 00:05:55.802
is going to solve all.

00:05:55.802 --> 00:05:56.760
ALBERT CHEN: Thank you.

00:05:56.760 --> 00:05:57.830
Yeah, yeah, absolutely.

00:05:57.830 --> 00:05:59.800
So let's start with
one of his themes

00:05:59.800 --> 00:06:01.710
and closer to home for Google.

00:06:01.710 --> 00:06:03.920
In the book, you talk
a lot about big data

00:06:03.920 --> 00:06:07.264
and that need for human
input to inform things,

00:06:07.264 --> 00:06:08.680
including a company
called Kaggle,

00:06:08.680 --> 00:06:11.590
which is now a part of Google.

00:06:11.590 --> 00:06:13.570
Can you talk a little
bit more about that?

00:06:13.570 --> 00:06:14.903
SCOTT HARTLEY: Yeah, so Kaggle--

00:06:14.903 --> 00:06:17.610
I don't know if any of you
guys are familiar with Kaggle.

00:06:17.610 --> 00:06:19.810
It was founded by an
Australian economist.

00:06:19.810 --> 00:06:21.340
It's basically a
platform where you

00:06:21.340 --> 00:06:23.910
can put big data
to work and then

00:06:23.910 --> 00:06:25.980
crowdsource different
explanations,

00:06:25.980 --> 00:06:28.720
different observations
on the data.

00:06:28.720 --> 00:06:31.820
And so companies can post
public or proprietary data

00:06:31.820 --> 00:06:35.620
sets, behind an NDA, behind
a non-disclosure agreement,

00:06:35.620 --> 00:06:39.830
and really get this diverse set
of people looking at the data,

00:06:39.830 --> 00:06:42.260
trying to find whatever
answer you're seeking.

00:06:42.260 --> 00:06:44.900
And so it's a competition
platform, which

00:06:44.900 --> 00:06:46.500
was acquired, I
believe, in March,

00:06:46.500 --> 00:06:48.690
It's now part of Google Cloud.

00:06:48.690 --> 00:06:52.454
But Anthony Goldbloom, he
really had this observation

00:06:52.454 --> 00:06:54.120
while he was working
for "The Economist"

00:06:54.120 --> 00:06:58.220
actually that there is all this
really fascinating data that

00:06:58.220 --> 00:07:00.440
was locked up, siloed
within companies.

00:07:00.440 --> 00:07:02.180
And he said, why
can't we crowdsource?

00:07:02.180 --> 00:07:05.110
Why can't we bring the
wealth of global expertise

00:07:05.110 --> 00:07:06.904
against this data?

00:07:06.904 --> 00:07:08.570
And there are these
really fascinating--

00:07:08.570 --> 00:07:11.770
if you look at the
case studies of Kaggle,

00:07:11.770 --> 00:07:14.940
where for example, The
Royal Astronomical Society

00:07:14.940 --> 00:07:17.280
and NASA and the
European Space Agency

00:07:17.280 --> 00:07:20.130
were trying to quantify
dark matter in the universe.

00:07:20.130 --> 00:07:23.070
And they had been working
on this for a decade.

00:07:23.070 --> 00:07:26.030
And they released this dataset.

00:07:26.030 --> 00:07:28.970
And they found-- it was
a glaciologist, actually,

00:07:28.970 --> 00:07:32.820
from the UK, who was using
algorithms that were looking

00:07:32.820 --> 00:07:36.610
at measuring the edge of a
glacier from space imagery,

00:07:36.610 --> 00:07:38.400
that he was able
to quantify, he was

00:07:38.400 --> 00:07:40.990
able to use some of those
tactics, kind of orthogonally,

00:07:40.990 --> 00:07:43.570
from this completely
different domain

00:07:43.570 --> 00:07:48.390
in this explanation of trying
to quantify dark matter.

00:07:48.390 --> 00:07:50.390
And he got closer to
the approximations

00:07:50.390 --> 00:07:53.230
that NASA and these
cosmologists had

00:07:53.230 --> 00:07:55.427
been working on for a
decade in a matter of weeks.

00:07:55.427 --> 00:07:57.260
And I think it's a
really incredible example

00:07:57.260 --> 00:08:03.540
of putting other human expertise
in the mix with big data.

00:08:03.540 --> 00:08:06.360
Another example in
the book is Palantir,

00:08:06.360 --> 00:08:10.120
which is down the street in Palo
Alto and now around the world.

00:08:10.120 --> 00:08:11.820
And Palantir is a
big data company

00:08:11.820 --> 00:08:15.030
that we forget is run by
a guy named Alex Karp, who

00:08:15.030 --> 00:08:18.180
has a PhD in neoclassical
social theory, and was also

00:08:18.180 --> 00:08:19.950
co-founded by Peter
Thiel, who has

00:08:19.950 --> 00:08:21.870
a degree in philosophy and law.

00:08:21.870 --> 00:08:23.220
So it's a big data company.

00:08:23.220 --> 00:08:27.210
But one of the guys who's
a director at Palantir,

00:08:27.210 --> 00:08:30.250
he has a TED Talk, this
guy named Shyam Sankar.

00:08:30.250 --> 00:08:33.250
And he talks about how there's
no terrorist find button.

00:08:33.250 --> 00:08:34.799
So for anyone familiar
with Palantir,

00:08:34.799 --> 00:08:36.470
it's a big data science company.

00:08:36.470 --> 00:08:39.665
But there are these huge human
components behind the scenes.

00:08:42.669 --> 00:08:47.170
When you think or you
collect the data from bunch

00:08:47.170 --> 00:08:50.909
of three-letter agencies-- from
FBI and from different groups--

00:08:50.909 --> 00:08:52.450
and you think, well,
if we just going

00:08:52.450 --> 00:08:54.533
to plug all this data
together, suddenly, big data

00:08:54.533 --> 00:08:56.100
is going to have
all these answers,

00:08:56.100 --> 00:09:01.420
we forget that Voltaire said,
judge a man by his questions,

00:09:01.420 --> 00:09:02.720
not by his answers.

00:09:02.720 --> 00:09:06.100
And I think we still require
smart questioners sitting

00:09:06.100 --> 00:09:11.270
in front of this newly surfaced
data in interesting new ways.

00:09:11.270 --> 00:09:13.670
But in the case of
Palantir, it really

00:09:13.670 --> 00:09:16.570
requires the battlefield
commander, the intelligence

00:09:16.570 --> 00:09:20.855
officer, the people that are
transcribing data and codifying

00:09:20.855 --> 00:09:22.870
it in certain ways,
putting it into the system

00:09:22.870 --> 00:09:25.980
in certain ways,
picking the taxonomy

00:09:25.980 --> 00:09:27.840
of how they look at the data.

00:09:27.840 --> 00:09:30.790
So we forget that
behind these buzz words

00:09:30.790 --> 00:09:33.200
that from a VC
standpoint, we love

00:09:33.200 --> 00:09:35.220
to see them on slide
decks, but at some point,

00:09:35.220 --> 00:09:36.761
you have to take a
step back and say,

00:09:36.761 --> 00:09:39.660
well, is every company that
we see a big data company?

00:09:39.660 --> 00:09:42.190
Is every company we see
an artificial intelligence

00:09:42.190 --> 00:09:42.930
company?

00:09:42.930 --> 00:09:44.320
Not really.

00:09:44.320 --> 00:09:47.600
It's really about this blending
of human and data, human

00:09:47.600 --> 00:09:49.320
and machine.

00:09:49.320 --> 00:09:51.440
ALBERT CHEN: OK, OK, awesome.

00:09:51.440 --> 00:09:53.120
One of the other
things you touched on

00:09:53.120 --> 00:09:55.740
is automation and the
concept of algorithms.

00:09:55.740 --> 00:09:57.630
And those are some examples.

00:09:57.630 --> 00:10:00.390
The example you use in the book
is Stitch Fix, which is what--

00:10:00.390 --> 00:10:01.140
I think it's cool.

00:10:01.140 --> 00:10:02.055
How does that apply to fashion?

00:10:02.055 --> 00:10:03.650
And why is that a great example?

00:10:03.650 --> 00:10:05.490
SCOTT HARTLEY:
Yeah, well, I think

00:10:05.490 --> 00:10:08.180
you're much more fashionable
up here than I am.

00:10:08.180 --> 00:10:10.300
But Stitch Fix, I don't know.

00:10:10.300 --> 00:10:13.350
Is anyone familiar
with Stitch Fix?

00:10:13.350 --> 00:10:15.070
Yeah, a few hands
in the audience.

00:10:15.070 --> 00:10:17.340
They just rolled out the
men's clothing recently.

00:10:17.340 --> 00:10:20.500
But it's been in the
market for women's clothing

00:10:20.500 --> 00:10:21.480
for a few years.

00:10:21.480 --> 00:10:24.130
But Stitch Fix is a
fascinating example

00:10:24.130 --> 00:10:27.890
because, really, it's sort
of Netflix for fashion.

00:10:27.890 --> 00:10:30.980
So what they've done is they've
taken articles of clothing.

00:10:30.980 --> 00:10:32.660
They've employed
different people

00:10:32.660 --> 00:10:36.040
from fashion backgrounds,
from style backgrounds, where

00:10:36.040 --> 00:10:38.560
they are able to classify
the clothing according

00:10:38.560 --> 00:10:41.310
to 100 to 150 characteristics.

00:10:41.310 --> 00:10:45.650
So whether it's the width of the
lapel or the style of button,

00:10:45.650 --> 00:10:47.330
all these different
characteristics,

00:10:47.330 --> 00:10:50.820
the start identifying
what clothing,

00:10:50.820 --> 00:10:52.630
you know where it fits.

00:10:52.630 --> 00:10:55.110
And then through connecting
like Pinterest APIs,

00:10:55.110 --> 00:10:59.790
through queries and polling
of your style habits,

00:10:59.790 --> 00:11:02.800
they're able to start matching
what your preferences might

00:11:02.800 --> 00:11:06.280
be for a certain type of
fashion with the characteristics

00:11:06.280 --> 00:11:08.500
inherent in certain clothing
items that you pick.

00:11:08.500 --> 00:11:09.874
And over time,
they're developing

00:11:09.874 --> 00:11:13.510
this Netflix-like algorithm to
better service clothing to you

00:11:13.510 --> 00:11:16.230
in a really automated,
streamlined way.

00:11:16.230 --> 00:11:18.530
But what's fascinating
about Stitch Fix is--

00:11:18.530 --> 00:11:21.570
I mean, one, it was founded
by Katrina Lake, who

00:11:21.570 --> 00:11:27.581
is an economics major,
not deeply technical.

00:11:27.581 --> 00:11:28.580
She had a vision though.

00:11:28.580 --> 00:11:32.910
And she had this passion to
want to solve this problem.

00:11:32.910 --> 00:11:34.810
She was working in
retail consulting,

00:11:34.810 --> 00:11:37.240
didn't have any time
to shop for herself,

00:11:37.240 --> 00:11:41.220
didn't have the time
to do it on her own.

00:11:41.220 --> 00:11:43.780
And she said, why is it
that I can sit down and have

00:11:43.780 --> 00:11:46.070
all of my content served
to me via Netflix,

00:11:46.070 --> 00:11:48.870
but I can't have
certain e-commerce items

00:11:48.870 --> 00:11:50.690
served me in the
time and the fashion

00:11:50.690 --> 00:11:52.190
that I want to receive them?

00:11:52.190 --> 00:11:56.120
And so she realized that
this was a heavy logistics

00:11:56.120 --> 00:11:56.620
operation.

00:11:56.620 --> 00:11:58.161
It was going to
require a lot of SKUs

00:11:58.161 --> 00:12:01.610
and a lot of clothing
items moving around.

00:12:01.610 --> 00:12:05.067
So she said, well,
who's good at logistics?

00:12:05.067 --> 00:12:06.900
She said, well, Walmart's
good at logistics.

00:12:06.900 --> 00:12:09.760
Why don't I go try to
hire the CEO of Walmart?

00:12:09.760 --> 00:12:13.190
And so she went and found
the CEO of walmart.com.

00:12:13.190 --> 00:12:14.300
Got him to quit his job.

00:12:14.300 --> 00:12:16.760
Worked for her when she
was still in the dorm room.

00:12:16.760 --> 00:12:19.420
And that kind of grit,
that kind of passion

00:12:19.420 --> 00:12:22.740
to be able to convince
and tell the story

00:12:22.740 --> 00:12:27.100
is so much more important in
some ways than the ability

00:12:27.100 --> 00:12:28.309
to actually code the product.

00:12:28.309 --> 00:12:28.808
Right?

00:12:28.808 --> 00:12:30.730
It was that ability to
tell the story, ability

00:12:30.730 --> 00:12:33.630
to tell the narrative to
convince somebody to leave

00:12:33.630 --> 00:12:36.050
a job as CEO of walmart.com.

00:12:36.050 --> 00:12:39.260
And she not only did that,
but then she said, well,

00:12:39.260 --> 00:12:41.680
if this is Netflix,
we need somebody who's

00:12:41.680 --> 00:12:43.360
really good at data science.

00:12:43.360 --> 00:12:46.075
So why not go get the guy who
built Netflix's algorithms?

00:12:46.075 --> 00:12:47.800
His name is Eric Colson.

00:12:47.800 --> 00:12:49.780
So she went and
approached Eric and was

00:12:49.780 --> 00:12:53.450
able to convince Eric to leave
his job as this chief data

00:12:53.450 --> 00:12:57.490
scientist at Netflix, who had
built the whole team to build

00:12:57.490 --> 00:13:03.510
the algorithms that run how
Netflix provides its movies.

00:13:03.510 --> 00:13:05.490
And so through this
combination, she

00:13:05.490 --> 00:13:09.360
was able to build this
killer team around her.

00:13:09.360 --> 00:13:12.200
But the most interesting thing,
I think, into your question

00:13:12.200 --> 00:13:15.580
is Eric is somebody that,
even though he's a data

00:13:15.580 --> 00:13:18.560
scientist, even though he's
a huge believer in the tech

00:13:18.560 --> 00:13:21.910
behind the scenes, he's also
a huge believer in the human.

00:13:21.910 --> 00:13:23.570
And so he basically
talks about how

00:13:23.570 --> 00:13:26.110
he has his M
algorithm for machine,

00:13:26.110 --> 00:13:28.500
and he has his H
algorithm for his humans.

00:13:28.500 --> 00:13:32.300
And he's got about 70 or 80
data scientists on his team.

00:13:32.300 --> 00:13:34.760
But they've got about 4,000
stylists that do all the

00:13:34.760 --> 00:13:36.930
and delivery of the product.

00:13:36.930 --> 00:13:40.190
So I think when we take a
step back from this assumption

00:13:40.190 --> 00:13:43.500
in the 140-character
Twitter universe of reading

00:13:43.500 --> 00:13:45.940
headlines and seeing
AI and automation

00:13:45.940 --> 00:13:47.800
are taking everything
over, and we

00:13:47.800 --> 00:13:49.920
forget that behind
the 60 or 80 data

00:13:49.920 --> 00:13:54.040
scientists are 4,000
stylists really looking

00:13:54.040 --> 00:13:59.160
at the nuance of geography
and your own fashion sense.

00:13:59.160 --> 00:14:01.800
The machine algorithm is
serving them a subset of items

00:14:01.800 --> 00:14:04.210
that they think will be
pretty relevant to you,

00:14:04.210 --> 00:14:05.960
but then the last
mile delivery is all

00:14:05.960 --> 00:14:09.600
based on who you are,
what you say you like,

00:14:09.600 --> 00:14:13.290
your personality and
actual interactions.

00:14:13.290 --> 00:14:15.980
There's really this last
mile human delivery.

00:14:15.980 --> 00:14:19.230
And one of the most fascinating
things about that is it's

00:14:19.230 --> 00:14:22.100
not treating the
human as problematic,

00:14:22.100 --> 00:14:24.140
but it's using machine
learning to try

00:14:24.140 --> 00:14:27.230
to use the human as a
classification problem.

00:14:27.230 --> 00:14:30.750
So if I have my own biases in
how I serve clothing to you

00:14:30.750 --> 00:14:33.990
that you either accept
or don't accept,

00:14:33.990 --> 00:14:36.340
we can use the machine
learning to actually mitigate

00:14:36.340 --> 00:14:39.070
my own biases, and how
if I'm based in Brooklyn,

00:14:39.070 --> 00:14:42.290
or I'm based in Des
Moines, I might have

00:14:42.290 --> 00:14:44.130
a different sense of
what's fashion forward,

00:14:44.130 --> 00:14:47.641
or what's hipster, or what's
one of these classifications.

00:14:47.641 --> 00:14:49.140
So the machine
learning can actually

00:14:49.140 --> 00:14:51.967
help me perform better
as a human stylist.

00:14:51.967 --> 00:14:53.800
So I think it's a really
fascinating example

00:14:53.800 --> 00:14:56.940
of not primacy of--

00:14:56.940 --> 00:14:59.230
it's not the Luddite
no technology,

00:14:59.230 --> 00:15:02.260
and it's not the primacy
of techie-only solutions,

00:15:02.260 --> 00:15:05.810
but it's really about
blending these two sides.

00:15:05.810 --> 00:15:09.000
So Stitch Fix is
something that I think

00:15:09.000 --> 00:15:10.490
is a great example of that.

00:15:10.490 --> 00:15:12.080
ALBERT CHEN: OK, OK.

00:15:12.080 --> 00:15:13.590
I want to dive
into one of things

00:15:13.590 --> 00:15:15.920
you mentioned, so
about automation,

00:15:15.920 --> 00:15:17.770
about tasks moving
away from human.

00:15:17.770 --> 00:15:21.020
So depending on what you read,
anywhere between like 1/3 up

00:15:21.020 --> 00:15:24.290
to close to 50% of American
jobs, quote unquote,

00:15:24.290 --> 00:15:26.450
are at risk due to
automation and machines.

00:15:26.450 --> 00:15:27.659
What do you think about that?

00:15:27.659 --> 00:15:29.158
SCOTT HARTLEY: Yeah,
so I mean, back

00:15:29.158 --> 00:15:31.570
to the question about the
reason for writing the book.

00:15:31.570 --> 00:15:35.130
I really felt like the
narrative and the pendulum of--

00:15:35.130 --> 00:15:37.700
Martin Ford's "Rise of the
Robots" was a great book.

00:15:37.700 --> 00:15:39.600
I don't know if
anyone has read that.

00:15:39.600 --> 00:15:43.624
But this sort of message of
fear and the narrative of fear.

00:15:43.624 --> 00:15:45.040
And I thought the
pendulum has got

00:15:45.040 --> 00:15:47.510
to come back toward
hope because I

00:15:47.510 --> 00:15:53.470
don't agree with 50% of all
jobs are disappearing tomorrow.

00:15:53.470 --> 00:15:55.530
And that quote, it really
refers to this study.

00:15:55.530 --> 00:15:58.090
In 2014, Oxford came
out with a study

00:15:58.090 --> 00:16:03.460
where they said 47% of US jobs
are at high risk of machine

00:16:03.460 --> 00:16:04.450
automation.

00:16:04.450 --> 00:16:08.110
It didn't really define
on what timeline.

00:16:08.110 --> 00:16:10.690
Technological ability
is not the same thing

00:16:10.690 --> 00:16:13.310
as immediately substituting
for jobs tomorrow.

00:16:13.310 --> 00:16:15.970
Obviously, there's
an economic question.

00:16:15.970 --> 00:16:17.061
There's a labor question.

00:16:17.061 --> 00:16:18.810
There's the flexibility
of labor question.

00:16:18.810 --> 00:16:21.400
There's all these other
components to that.

00:16:21.400 --> 00:16:24.450
So more recently, in January,
McKinsey Global Institute

00:16:24.450 --> 00:16:27.920
came out with a study
that hit pause and said,

00:16:27.920 --> 00:16:29.850
let's take a step back.

00:16:29.850 --> 00:16:33.000
Let's look at this 47% number,
and let's think about this.

00:16:33.000 --> 00:16:35.530
And so they looked at
800 different occupations

00:16:35.530 --> 00:16:36.640
across the US.

00:16:36.640 --> 00:16:38.700
And they broke up
those occupations

00:16:38.700 --> 00:16:42.180
into the tasks that make up
those jobs, because like we all

00:16:42.180 --> 00:16:45.490
know, any job, you've got
100 different tasks that you

00:16:45.490 --> 00:16:46.947
do on a daily basis.

00:16:46.947 --> 00:16:48.780
And some of those things
are best practices.

00:16:48.780 --> 00:16:50.450
Some of the things
are really rote.

00:16:50.450 --> 00:16:51.482
They're repeatable.

00:16:51.482 --> 00:16:53.190
You've got maybe your
best practices list

00:16:53.190 --> 00:16:56.519
on your desk on the wall for how
to go through a process Those

00:16:56.519 --> 00:16:58.810
are things that, because
you've done them so many times

00:16:58.810 --> 00:17:01.010
before, they can be scripted.

00:17:01.010 --> 00:17:02.210
They can be programmmed.

00:17:02.210 --> 00:17:04.400
They can be machine
automatable pretty fast.

00:17:04.400 --> 00:17:04.900
Right?

00:17:04.900 --> 00:17:08.519
So I think it behooves all of
us to look within our own jobs

00:17:08.519 --> 00:17:10.560
and say, well, what are
the things that I'm doing

00:17:10.560 --> 00:17:12.394
over and over and over again?

00:17:12.394 --> 00:17:14.060
What are things that
are best practices?

00:17:14.060 --> 00:17:15.476
Those are things
we could probably

00:17:15.476 --> 00:17:17.460
make more efficient
through the use of machine

00:17:17.460 --> 00:17:20.190
learning through the use
of different technology.

00:17:20.190 --> 00:17:23.960
But then there's all these other
tasks that are highly variable.

00:17:23.960 --> 00:17:26.280
David Autor, who's
an economist at MIT,

00:17:26.280 --> 00:17:30.040
talks about routine
and non-routine tasks.

00:17:30.040 --> 00:17:33.110
And so routine tasks
can be both manual,

00:17:33.110 --> 00:17:34.960
or they can be cognitive.

00:17:34.960 --> 00:17:36.410
Routine tasks that
are manual are

00:17:36.410 --> 00:17:39.847
things that robots can maybe
do because they're physical.

00:17:39.847 --> 00:17:42.180
Routine cognitive tasks are
things that machine learning

00:17:42.180 --> 00:17:43.600
can proudly start doing.

00:17:43.600 --> 00:17:47.060
But non-routine things,
whether they're cognitive,

00:17:47.060 --> 00:17:49.180
like things you do at
work that are highly

00:17:49.180 --> 00:17:52.480
variable, interfacing
with other humans,

00:17:52.480 --> 00:17:55.330
collaborating with
teams, those are

00:17:55.330 --> 00:17:57.610
things that are not going
to go away anytime soon.

00:17:57.610 --> 00:18:00.000
And then on the
manual side of things,

00:18:00.000 --> 00:18:02.270
non-routine manual
would be, for example,

00:18:02.270 --> 00:18:04.290
like livestock farming.

00:18:04.290 --> 00:18:07.570
Things that have
a high variability

00:18:07.570 --> 00:18:11.010
are things that maybe machines--

00:18:11.010 --> 00:18:14.154
maybe robots can get some
way into hydroponic farming

00:18:14.154 --> 00:18:15.820
indoors, because it's
highly repeatable.

00:18:15.820 --> 00:18:20.340
But for things like livestock
farming or construction

00:18:20.340 --> 00:18:23.730
outdoors, these are things that
are still pretty non-routine,

00:18:23.730 --> 00:18:28.710
things that are still generally
going to be done by humans.

00:18:28.710 --> 00:18:30.440
And so I really like
the McKinsey study

00:18:30.440 --> 00:18:33.870
because it really takes this
granularity and looks at that.

00:18:33.870 --> 00:18:38.470
And overall, what they found
was that they thought 5% of jobs

00:18:38.470 --> 00:18:41.380
had 100% of tasks
that could eventually,

00:18:41.380 --> 00:18:44.880
over some period of time, be
substituted with machines.

00:18:44.880 --> 00:18:46.930
And that's still a
huge number of jobs

00:18:46.930 --> 00:18:50.380
that, I think, beg real social
and political questions and all

00:18:50.380 --> 00:18:51.540
that.

00:18:51.540 --> 00:18:55.530
But for 60% of jobs, they said
30% of the tasks in those jobs

00:18:55.530 --> 00:18:58.180
are things that could be
substituted over time.

00:18:58.180 --> 00:19:01.400
So much more than just this
all-or-nothing robots are

00:19:01.400 --> 00:19:04.150
taking over the world, AI
is taking over the world,

00:19:04.150 --> 00:19:07.670
I think it's going to look
much more like this gradual

00:19:07.670 --> 00:19:09.150
progression of--

00:19:09.150 --> 00:19:13.060
In our cars, we've had manual
to automatic transmissions.

00:19:13.060 --> 00:19:14.780
We've got park assists.

00:19:14.780 --> 00:19:16.610
We've got antilock
brakes, we've got

00:19:16.610 --> 00:19:21.130
all this sort of fly-by-wire
in our cars and our planes.

00:19:21.130 --> 00:19:24.210
I think thinking about this
more like desktop assists

00:19:24.210 --> 00:19:26.780
and driver assists in
the sense that we'll

00:19:26.780 --> 00:19:28.817
have things that
say, hey, you're

00:19:28.817 --> 00:19:29.900
gonna send this email out.

00:19:29.900 --> 00:19:31.775
You're gonna post this
thing to social media.

00:19:31.775 --> 00:19:33.790
You're going to do whatever.

00:19:33.790 --> 00:19:35.980
Based on the data
that we have, we

00:19:35.980 --> 00:19:37.480
would suggest you
wait 30 minutes.

00:19:37.480 --> 00:19:39.630
Would you like to
delay that post?

00:19:39.630 --> 00:19:44.120
These minor nudges, I
think, are much more

00:19:44.120 --> 00:19:46.990
where this is going to impact
us on a day-to-day basis

00:19:46.990 --> 00:19:48.080
in the near term.

00:19:48.080 --> 00:19:48.580
OK

00:19:48.580 --> 00:19:49.871
ALBERT CHEN: OK, I love that's.

00:19:49.871 --> 00:19:51.347
So I shouldn't be worried yet.

00:19:51.347 --> 00:19:53.032
I'm kidding.

00:19:53.032 --> 00:19:54.990
SCOTT HARTLEY: I think
the pendulum is swinging

00:19:54.990 --> 00:19:56.940
a little bit back
toward hope because I

00:19:56.940 --> 00:19:59.010
think these tools really have--

00:19:59.010 --> 00:20:01.680
flipping the letters
around, there's

00:20:01.680 --> 00:20:04.010
a lot of talk about
artificial intelligence.

00:20:04.010 --> 00:20:05.850
But I think when you
go back to the debates

00:20:05.850 --> 00:20:10.470
of the 1950s between Marvin
Minsky JCR Licklider, who

00:20:10.470 --> 00:20:11.970
I talk about the book a bit--

00:20:11.970 --> 00:20:14.360
there was this debate about
artificial intelligence

00:20:14.360 --> 00:20:17.330
and intelligence augmentation
and thinking about,

00:20:17.330 --> 00:20:19.510
if you flip the
letters around, how do

00:20:19.510 --> 00:20:20.830
we make humans more productive?

00:20:20.830 --> 00:20:22.200
How do we make humans better?

00:20:22.200 --> 00:20:24.790
I think the Stitch Fix example
is a really interesting case

00:20:24.790 --> 00:20:28.070
for that, because a stylist has
all these fallabilities, all

00:20:28.070 --> 00:20:31.270
these biases that are implicit.

00:20:31.270 --> 00:20:32.925
But can we use machine
learning, can we

00:20:32.925 --> 00:20:35.290
use these things to actually
help mitigate those biases

00:20:35.290 --> 00:20:36.880
and make that person
more productive?

00:20:36.880 --> 00:20:40.100
And that's the intelligence
augmentation, rather than

00:20:40.100 --> 00:20:40.969
just the pure AI.

00:20:40.969 --> 00:20:43.510
ALBERT CHEN: One of the themes
that I love that you touch on,

00:20:43.510 --> 00:20:46.194
the attention economy
and that concept.

00:20:46.194 --> 00:20:47.610
Can you explain
that a little bit?

00:20:47.610 --> 00:20:49.235
And then there's
another former Googler

00:20:49.235 --> 00:20:51.470
who's written a lot about
this, Tristan Harris,

00:20:51.470 --> 00:20:53.890
that ethics in design
and how we have

00:20:53.890 --> 00:20:56.630
to think about the menu of
choices in product design.

00:20:56.630 --> 00:21:00.430
SCOTT HARTLEY: Yes, so
Tristan Harris, he's

00:21:00.430 --> 00:21:01.790
a former Googler as well.

00:21:01.790 --> 00:21:02.600
He was here.

00:21:02.600 --> 00:21:04.520
He sold his company--
it's called Apture--

00:21:04.520 --> 00:21:07.240
to Google back in 2011 or so.

00:21:07.240 --> 00:21:10.120
He became a product manager,
and then quickly fashioned

00:21:10.120 --> 00:21:13.860
himself the in-house product
philosopher, or one of many,

00:21:13.860 --> 00:21:17.220
and really took
this observation,

00:21:17.220 --> 00:21:19.720
who was in the room as we're
making these product decisions.

00:21:19.720 --> 00:21:21.250
I think in the
past, we've thought

00:21:21.250 --> 00:21:25.850
a lot about who
wields massive scale

00:21:25.850 --> 00:21:28.020
or impact of their decisions.

00:21:28.020 --> 00:21:32.530
And we're quick to
point fingers at the MBA

00:21:32.530 --> 00:21:34.130
at the helm of an
investment bank,

00:21:34.130 --> 00:21:37.860
or at the helm of an
Enron, or a company that's

00:21:37.860 --> 00:21:39.280
done something wrong.

00:21:39.280 --> 00:21:43.490
On we forget that today,
in the digital economy,

00:21:43.490 --> 00:21:45.710
we have this same
moral obligation

00:21:45.710 --> 00:21:48.740
to think about how we're
multiplying out effects

00:21:48.740 --> 00:21:50.470
when we make product
decisions that are

00:21:50.470 --> 00:21:52.600
affecting billions of people.

00:21:52.600 --> 00:21:55.680
And so Tristan was
asking these questions,

00:21:55.680 --> 00:21:59.390
shining a light on what's the
diversity of our teams, who's

00:21:59.390 --> 00:22:01.110
in the room making
product decisions,

00:22:01.110 --> 00:22:03.880
how are we thinking about
not just engagement metrics,

00:22:03.880 --> 00:22:08.220
not just monetizing via
ad revenue, not just

00:22:08.220 --> 00:22:10.317
the key performance
indicators that our venture

00:22:10.317 --> 00:22:11.900
capitalists or our
public shareholders

00:22:11.900 --> 00:22:12.870
want to think about.

00:22:12.870 --> 00:22:16.400
But if we flip this around
we think about as consumers,

00:22:16.400 --> 00:22:20.940
as humans, 100
million-plus smartphones,

00:22:20.940 --> 00:22:23.860
60 to 80 interruptions
per day, you

00:22:23.860 --> 00:22:27.450
start looking at the actual
externalities of this,

00:22:27.450 --> 00:22:30.220
from a time perspective, from
an interruption with their kids

00:22:30.220 --> 00:22:32.850
perspective, from a
lifestyle perspective,

00:22:32.850 --> 00:22:35.760
and it begs a whole
new set of questions.

00:22:35.760 --> 00:22:40.670
And so Tristan basically
doesn't have these as answers,

00:22:40.670 --> 00:22:42.350
but he's posing these questions.

00:22:42.350 --> 00:22:45.310
And he left Google recently
to start a movement called

00:22:45.310 --> 00:22:48.570
the Time Well Spent movement,
which looks at this attention

00:22:48.570 --> 00:22:51.310
economy and how we think
about these choices

00:22:51.310 --> 00:22:54.040
that we're making, how
we develop the menus,

00:22:54.040 --> 00:22:56.030
he calls them,
within our products.

00:22:56.030 --> 00:22:59.670
Because we're all
moving around our world.

00:22:59.670 --> 00:23:03.740
If we want to get together to
grab a drink after this event,

00:23:03.740 --> 00:23:05.770
we'll, chances
are, turn to Yelp,

00:23:05.770 --> 00:23:07.300
or we'll turn to OpenTable.

00:23:07.300 --> 00:23:09.310
We'll turn to a
product where we're

00:23:09.310 --> 00:23:11.820
interfacing with the
technology layer that

00:23:11.820 --> 00:23:15.580
has implicit choices of how it
surfaces information to us, how

00:23:15.580 --> 00:23:19.730
it presumes what we want because
of our geography or our age

00:23:19.730 --> 00:23:22.740
or various things
that those product

00:23:22.740 --> 00:23:27.400
managers, those engineers,
have had determined for us.

00:23:27.400 --> 00:23:30.620
And so these sorts of questions
that Tristan poses, I think,

00:23:30.620 --> 00:23:33.020
are really important
as we build our team

00:23:33.020 --> 00:23:35.850
to think about these things.

00:23:35.850 --> 00:23:39.640
And he gets into some of
the sort of philosophy

00:23:39.640 --> 00:23:42.034
in thinking about
how Steve Jobs made

00:23:42.034 --> 00:23:43.950
this comment that the
personal computer should

00:23:43.950 --> 00:23:45.720
be a bicycle for the mind.

00:23:45.720 --> 00:23:48.270
And Tristan says, wait a minute.

00:23:48.270 --> 00:23:51.030
Unfortunately, the smartphone
in our pocket has in some ways

00:23:51.030 --> 00:23:52.900
become a slot machine.

00:23:52.900 --> 00:23:54.990
It's become the
one finger bandit

00:23:54.990 --> 00:23:57.270
that steals away our
time the same way

00:23:57.270 --> 00:24:01.270
that a one lever bandit
from a slot machine

00:24:01.270 --> 00:24:05.006
takes our money
one coin at a time.

00:24:05.006 --> 00:24:06.869
It's one perspective.

00:24:06.869 --> 00:24:08.160
But how do we think about this?

00:24:08.160 --> 00:24:12.050
Should there be LEED certified,
like for a green building?

00:24:12.050 --> 00:24:13.290
Should there be an FDA?

00:24:13.290 --> 00:24:16.230
Should there be some
sort of thinking

00:24:16.230 --> 00:24:21.260
about how we as a
society construct

00:24:21.260 --> 00:24:23.300
some sort of check
and balance on how

00:24:23.300 --> 00:24:28.410
we want our own personal time
to be manipulated in some ways.

00:24:28.410 --> 00:24:31.750
And really, he doesn't
have a yes or no answer

00:24:31.750 --> 00:24:33.850
for any of these
things, but says,

00:24:33.850 --> 00:24:38.340
well, we retain agency
as personal individuals.

00:24:38.340 --> 00:24:40.340
We still do that when
we walk into Starbucks.

00:24:40.340 --> 00:24:42.010
But we have calorie menus.

00:24:42.010 --> 00:24:44.834
We know how many calories
in our frappucino

00:24:44.834 --> 00:24:45.750
versus a black coffee.

00:24:45.750 --> 00:24:48.040
We can still choose
whatever we want to choose.

00:24:48.040 --> 00:24:50.050
But should there be
those sorts of things

00:24:50.050 --> 00:24:52.540
baked into our
technology that allow us,

00:24:52.540 --> 00:24:57.414
as consumers, as users, as
humans, as individuals, to be

00:24:57.414 --> 00:24:59.080
making the choices
that we actually want

00:24:59.080 --> 00:25:01.250
to make with our technology?

00:25:01.250 --> 00:25:03.160
ALBERT CHEN: OK, awesome.

00:25:03.160 --> 00:25:05.980
Let's take a step back towards
the overall theme of the book.

00:25:05.980 --> 00:25:08.320
So you mentioned--
and I think some of us

00:25:08.320 --> 00:25:11.470
feel in the room too, folks who,
for me, like political science

00:25:11.470 --> 00:25:15.700
and history background, looking
at the industry that I'm in,

00:25:15.700 --> 00:25:19.480
not having a STEM degree,
that maybe I don't know

00:25:19.480 --> 00:25:21.440
how long my career could go.

00:25:21.440 --> 00:25:24.020
So can you talk a bit more
about your perspective on this

00:25:24.020 --> 00:25:26.520
and why these skills
continue to matter?

00:25:26.520 --> 00:25:30.680
SCOTT HARTLEY: Yeah,
so back to the code--

00:25:30.680 --> 00:25:34.450
I think it's a multipart
question where really,

00:25:34.450 --> 00:25:36.750
in some ways, the
chunks of technology

00:25:36.750 --> 00:25:38.890
have become larger and
larger building blocks.

00:25:38.890 --> 00:25:40.765
And there was a great
article in "TechCrunch"

00:25:40.765 --> 00:25:43.610
a few years ago about the end
of the full stack developer

00:25:43.610 --> 00:25:45.520
and the rise of the
full stack integrator.

00:25:45.520 --> 00:25:47.810
And I love that
quote because I think

00:25:47.810 --> 00:25:51.330
it speaks so much to being
able to know where the building

00:25:51.330 --> 00:25:52.090
blocks are.

00:25:52.090 --> 00:25:55.210
TensorFlow is an
incredible tool.

00:25:55.210 --> 00:25:57.109
You no longer need to
know all the nuances

00:25:57.109 --> 00:25:58.900
to be able to get a
product off the ground,

00:25:58.900 --> 00:26:03.190
or to be able to create an MVP,
a minimum viable product, sir

00:26:03.190 --> 00:26:07.140
create applicable prototype.

00:26:07.140 --> 00:26:09.661
So I think more
and more it's about

00:26:09.661 --> 00:26:10.910
what are you passionate about?

00:26:10.910 --> 00:26:15.840
What the context to which
you would apply the code?

00:26:15.840 --> 00:26:17.150
The book is not anti-STEM.

00:26:17.150 --> 00:26:19.060
System It's not anti
any of these things.

00:26:19.060 --> 00:26:22.110
I think you've got to have
enough technical literacy,

00:26:22.110 --> 00:26:25.770
enough break the ice,
be technical enough

00:26:25.770 --> 00:26:27.870
to be dangerous, which
anyone at Google, I think,

00:26:27.870 --> 00:26:30.300
has already crossed that line.

00:26:30.300 --> 00:26:34.310
But I think it's about
taking these methodologies,

00:26:34.310 --> 00:26:38.760
taking these passions,
and really unlocking

00:26:38.760 --> 00:26:40.040
these new industries.

00:26:40.040 --> 00:26:41.720
Marc Andreesson
talks a lot about how

00:26:41.720 --> 00:26:43.060
software is eating the world.

00:26:43.060 --> 00:26:45.390
But on the flip side,
I think software

00:26:45.390 --> 00:26:46.940
is feeding the
world in the sense

00:26:46.940 --> 00:26:49.370
that now, if you're
coming from a background

00:26:49.370 --> 00:26:52.030
like political science,
like the two of us--

00:26:52.030 --> 00:26:55.090
there's a guy I feature in
the book named Zac Bookman.

00:26:55.090 --> 00:26:57.760
And he founded this amazing
company called OpenGov.

00:26:57.760 --> 00:27:01.550
And OpenGov was based on
his own personal passion

00:27:01.550 --> 00:27:03.880
for transparency in
government, which

00:27:03.880 --> 00:27:08.030
was rooted in studying political
science, going to law school.

00:27:08.030 --> 00:27:10.370
He worked at the Ninth
Circuit in San Francisco

00:27:10.370 --> 00:27:12.630
on issues around transparency.

00:27:12.630 --> 00:27:14.940
He went to Mexico looking
at transparency issues

00:27:14.940 --> 00:27:16.460
on a Fulbright.

00:27:16.460 --> 00:27:18.920
He went to Afghanistan
with General HR McMaster

00:27:18.920 --> 00:27:21.590
to work on transparency
issues in Afghanistan.

00:27:21.590 --> 00:27:24.090
And rather than the
proverbial garage,

00:27:24.090 --> 00:27:27.530
he was living in a container,
literally a shipping container,

00:27:27.530 --> 00:27:28.750
in Kabul.

00:27:28.750 --> 00:27:30.840
And he had this idea
of, wait a minute.

00:27:30.840 --> 00:27:33.720
We're holding the Afghans to
this incredibly high standard

00:27:33.720 --> 00:27:35.830
of transparency.

00:27:35.830 --> 00:27:37.610
Do even have this
at home in America?

00:27:37.610 --> 00:27:39.950
And so he came back,
started talking

00:27:39.950 --> 00:27:41.100
to City Hall in Palo Alto.

00:27:41.100 --> 00:27:42.660
He thought, this was the
heart of Silicon Valley.

00:27:42.660 --> 00:27:44.480
They must have their
ducks in a row.

00:27:44.480 --> 00:27:48.530
They must have all their
data visible and digestible.

00:27:48.530 --> 00:27:51.380
And on the contrary, it
was literally in boxes,

00:27:51.380 --> 00:27:52.930
printed out Excel files.

00:27:52.930 --> 00:27:55.440
And so they created,
effectively,

00:27:55.440 --> 00:27:58.750
the mint.com of
municipal finance data,

00:27:58.750 --> 00:28:02.200
where you can look and see all
of the public expenditures,

00:28:02.200 --> 00:28:06.240
public revenues, for
1,000-plus cities.

00:28:06.240 --> 00:28:07.859
And I think that
kind of company,

00:28:07.859 --> 00:28:10.150
sitting on Sand Hill Road,
sitting in a venture capital

00:28:10.150 --> 00:28:12.670
seat, is so interesting
because it's

00:28:12.670 --> 00:28:16.050
taking technology and applying
it meaningfully to this problem

00:28:16.050 --> 00:28:20.130
that Zac really understood
and really had a passion for.

00:28:20.130 --> 00:28:22.580
And he partnered
with Joe Lonsdale,

00:28:22.580 --> 00:28:25.190
actually, who co-founded
Palantir, co-founded a couple

00:28:25.190 --> 00:28:27.449
other companies, as a techie.

00:28:27.449 --> 00:28:29.740
But it was really about this
blending of the two sides.

00:28:29.740 --> 00:28:31.281
And really, the
comparative advantage

00:28:31.281 --> 00:28:34.070
of what OpenGov off the
ground was Zac's passion

00:28:34.070 --> 00:28:37.700
for transparency, not the
ability to create a platform

00:28:37.700 --> 00:28:39.480
to visualize data.

00:28:39.480 --> 00:28:42.500
And so I think there are
so many examples like that,

00:28:42.500 --> 00:28:46.280
that really show the way of--

00:28:46.280 --> 00:28:48.555
it's not just people in
management positions.

00:28:48.555 --> 00:28:49.930
There are plenty
of those people,

00:28:49.930 --> 00:28:53.580
like Susan Wojcicki
from YouTube, history

00:28:53.580 --> 00:28:55.300
and literature major.

00:28:55.300 --> 00:28:58.270
Or I mentioned Alex
Karp from Palantir,

00:28:58.270 --> 00:28:59.980
know neoclassical social theory.

00:28:59.980 --> 00:29:03.880
Or Steve Case from AOL
was a history major.

00:29:03.880 --> 00:29:06.620
Emily White, who used to
run Instagram and Snapchat

00:29:06.620 --> 00:29:09.520
was a studio art
and fine arts major.

00:29:09.520 --> 00:29:12.197
So there's so many
different examples.

00:29:12.197 --> 00:29:14.280
It's not just about cherry
picking those examples,

00:29:14.280 --> 00:29:18.480
but thinking about the
underlying methodologies of--

00:29:18.480 --> 00:29:21.760
Slack, Stewart
Butterfield, for example,

00:29:21.760 --> 00:29:26.200
both in undergrad and then grad
school, studied philosophy.

00:29:26.200 --> 00:29:29.260
And he really tributes product
design, product development

00:29:29.260 --> 00:29:33.030
to this unlocking of
this pursuit of truth,

00:29:33.030 --> 00:29:34.790
where you peel pack
the onion repeatedly

00:29:34.790 --> 00:29:37.250
and you go through this
process of inquiry.

00:29:37.250 --> 00:29:43.290
And I think that we understand
the A-to-A linearity

00:29:43.290 --> 00:29:45.900
of study engineering,
become an engineer.

00:29:45.900 --> 00:29:50.310
But we forget study
anthropology, study sociology,

00:29:50.310 --> 00:29:54.240
user experience, study
philosophy, product management,

00:29:54.240 --> 00:29:54.910
product design.

00:29:54.910 --> 00:29:56.670
There's so many
different lines that I

00:29:56.670 --> 00:29:58.870
think we can draw
from these skillsets

00:29:58.870 --> 00:30:02.740
if you take a step back
and don't fall victim

00:30:02.740 --> 00:30:04.880
to the narrative that these
degrees are worthless,

00:30:04.880 --> 00:30:06.213
that these skills are worthless.

00:30:06.213 --> 00:30:10.420
Because if anything, these
are the vital components

00:30:10.420 --> 00:30:12.850
to how we humanize
technology, how we actually

00:30:12.850 --> 00:30:16.430
take technology make it useful
for what it's meant for,

00:30:16.430 --> 00:30:17.980
which is to improve our lives.

00:30:17.980 --> 00:30:21.980
ALBERT CHEN: OK, so if we think
about the next generation--

00:30:21.980 --> 00:30:24.480
and you knew I was going to ask
you this question, because I

00:30:24.480 --> 00:30:26.690
love the concept of education.

00:30:26.690 --> 00:30:28.870
There's a number of tech
billionaires investing

00:30:28.870 --> 00:30:31.620
in schools, being somewhat
directive about curriculum

00:30:31.620 --> 00:30:33.349
content, things like that.

00:30:33.349 --> 00:30:34.890
And "The New York
Times" is currently

00:30:34.890 --> 00:30:37.280
publishing a series of
articles about this.

00:30:37.280 --> 00:30:39.816
What happens if we take it
too far in one direction?

00:30:39.816 --> 00:30:41.690
SCOTT HARTLEY: Yeah,
that's a great question,

00:30:41.690 --> 00:30:45.560
and it's something that Steve
Jobs talked a number of years

00:30:45.560 --> 00:30:48.860
back about how the next
big platform for innovation

00:30:48.860 --> 00:30:50.880
was going to be education.

00:30:50.880 --> 00:30:52.950
And I think you're
starting to see that.

00:30:52.950 --> 00:30:55.330
Rick Levin, former
president of Yale,

00:30:55.330 --> 00:30:58.460
left Yale to become
the CEO of Coursera.

00:30:58.460 --> 00:31:01.280
That's not the path that
most academics would have

00:31:01.280 --> 00:31:03.670
foreseen a few decades ago.

00:31:03.670 --> 00:31:06.190
So I think there is
this understanding

00:31:06.190 --> 00:31:08.980
that the scale and impact we
can have through technology

00:31:08.980 --> 00:31:10.880
is massive, but
thinking about, how

00:31:10.880 --> 00:31:13.290
do we engage with the
new tools, but not

00:31:13.290 --> 00:31:15.570
lose some of the old ways.

00:31:15.570 --> 00:31:18.860
And there are a bunch of
examples in "The New York

00:31:18.860 --> 00:31:21.620
Times" article you mentioned,
like Marc Benioff has been

00:31:21.620 --> 00:31:24.110
really engaged in this process.

00:31:24.110 --> 00:31:27.460
Zuckerberg, through the
Chan Zuckerberg Initiative,

00:31:27.460 --> 00:31:30.430
has invested in a group called
the Summit Public Schools,

00:31:30.430 --> 00:31:33.190
which is based mostly
here in the Bay Area.

00:31:33.190 --> 00:31:36.710
But what they've done is they've
engaged with technologies.

00:31:36.710 --> 00:31:40.560
They use the iPad, for
example, to deliver lectures.

00:31:40.560 --> 00:31:42.740
But they create this really
personalized experience

00:31:42.740 --> 00:31:45.230
where the teacher actually
has become the coach.

00:31:45.230 --> 00:31:48.150
So the teacher is
walking around the room,

00:31:48.150 --> 00:31:51.595
almost like if you had in
soccer practice, doing a drill,

00:31:51.595 --> 00:31:53.970
and the coach is walking
around, checking in on everybody

00:31:53.970 --> 00:31:56.090
as they're performing.

00:31:56.090 --> 00:31:59.390
But I think that way of
engaging with technology,

00:31:59.390 --> 00:32:02.190
not shunning it--

00:32:02.190 --> 00:32:05.130
I talk a little bit about
Waldorf Schools in the book

00:32:05.130 --> 00:32:08.140
and taking a completely
hands-off approach to tech.

00:32:08.140 --> 00:32:10.470
But in some ways, I
think that while that's

00:32:10.470 --> 00:32:12.920
popular in Silicon Valley,
we're privileged in the sense

00:32:12.920 --> 00:32:16.130
that we have tech all around
us, in our homes, other places.

00:32:16.130 --> 00:32:18.790
So it's one thing to say, don't
deal with tech in the school

00:32:18.790 --> 00:32:21.360
because we all work at Google,
we all work at Facebook,

00:32:21.360 --> 00:32:23.270
we all work at these
fancy tech companies.

00:32:23.270 --> 00:32:24.680
That's easy for us to say.

00:32:24.680 --> 00:32:27.190
But I think across
America, where

00:32:27.190 --> 00:32:30.480
people don't have that access
to technology in our schools,

00:32:30.480 --> 00:32:33.512
how do we think about
engaging with the tools,

00:32:33.512 --> 00:32:36.650
but doing it in a way that still
fosters collaboration, still

00:32:36.650 --> 00:32:38.880
fosters communication,
still builds

00:32:38.880 --> 00:32:42.360
a lot of these soft skills
that are so important?

00:32:42.360 --> 00:32:44.030
So there's a really
interesting example

00:32:44.030 --> 00:32:48.670
of a guy in India
who put computers out

00:32:48.670 --> 00:32:53.230
in public in New Delhi,
in the slums in basically

00:32:53.230 --> 00:32:55.840
this self-organizing
learning environment.

00:32:55.840 --> 00:32:59.550
He called it a sole, S-O-L-E,
and gave us a great TED Talk

00:32:59.550 --> 00:33:00.260
about this.

00:33:00.260 --> 00:33:05.030
And it actually inspired the
movie "Slumdog Millionaire"

00:33:05.030 --> 00:33:06.640
through this exposure.

00:33:06.640 --> 00:33:11.160
And in these soles, he basically
unleashes technology and lets

00:33:11.160 --> 00:33:13.540
kids do with it what they want.

00:33:13.540 --> 00:33:15.610
And the results are
really astounding.

00:33:15.610 --> 00:33:17.920
And so I think
what's interesting is

00:33:17.920 --> 00:33:22.390
if you provide the tools and
provide a messy question, which

00:33:22.390 --> 00:33:24.590
is something I talk
about in the book--

00:33:24.590 --> 00:33:27.820
these messy questions are
things you can't really Google.

00:33:27.820 --> 00:33:30.270
And they're fun because
we live in this world

00:33:30.270 --> 00:33:33.060
where people say, well, all
the answers are already Google.

00:33:33.060 --> 00:33:33.910
So what's the point?

00:33:33.910 --> 00:33:34.910
Just give a kid an iPad.

00:33:34.910 --> 00:33:36.810
They'll figure it out.

00:33:36.810 --> 00:33:39.785
Actually, this example
of messy questions

00:33:39.785 --> 00:33:42.610
is really fun because you ask
things that are unGoogleable,

00:33:42.610 --> 00:33:44.415
like why aren't our ears square?

00:33:47.290 --> 00:33:51.310
So it begs the question of,
well, how do acoustics work?

00:33:51.310 --> 00:33:53.536
How did the ears develop?

00:33:53.536 --> 00:33:56.690
It goes into all these
different areas of exploration.

00:33:56.690 --> 00:34:00.040
And so you show this to
a class of third graders

00:34:00.040 --> 00:34:01.690
and they immediately
start googling

00:34:01.690 --> 00:34:03.590
all these different things,
watching YouTube videos,

00:34:03.590 --> 00:34:05.006
learning about
acoustics, learning

00:34:05.006 --> 00:34:08.270
about different human biology.

00:34:08.270 --> 00:34:11.100
But then also, the second layer
is this collaboration that's

00:34:11.100 --> 00:34:14.530
happening between them and
even this tertiary layer

00:34:14.530 --> 00:34:16.920
of figuring out what
sources they trust,

00:34:16.920 --> 00:34:18.699
what sources they don't trust.

00:34:18.699 --> 00:34:21.480
In this Facebook
newsfeed world, how

00:34:21.480 --> 00:34:24.570
are we thinking about
red feed and blue feed

00:34:24.570 --> 00:34:27.580
and what sources we choose
to trust and all that?

00:34:27.580 --> 00:34:29.469
And I think these
are things that

00:34:29.469 --> 00:34:31.510
are really interesting
ways that you

00:34:31.510 --> 00:34:35.530
can engage with the technology,
but then still teach

00:34:35.530 --> 00:34:36.701
these underlying skills.

00:34:36.701 --> 00:34:38.159
ALBERT CHEN: It's
been about a year

00:34:38.159 --> 00:34:39.409
since you've written the book.

00:34:39.409 --> 00:34:41.560
And things move at quite
a pace in the Valley,

00:34:41.560 --> 00:34:42.860
and outside as well.

00:34:42.860 --> 00:34:44.710
What companies are
doing this well?

00:34:44.710 --> 00:34:46.989
And if you're not a
company that's doing well,

00:34:46.989 --> 00:34:49.230
how do you think about that?

00:34:49.230 --> 00:34:51.380
SCOTT HARTLEY: Well, I
think we've seen Uber

00:34:51.380 --> 00:34:53.969
in the news a lot recently.

00:34:53.969 --> 00:34:57.070
I'd say, from a cultural
standpoint, that's

00:34:57.070 --> 00:34:59.300
one that I think they
probably could figure out

00:34:59.300 --> 00:35:00.810
a way to do better.

00:35:00.810 --> 00:35:01.792
I think Google--

00:35:01.792 --> 00:35:04.650
I'm biased from having
spent so much time here

00:35:04.650 --> 00:35:05.770
and loving the company.

00:35:05.770 --> 00:35:10.280
But I think that Google, from
back to days as a product

00:35:10.280 --> 00:35:12.660
specialist here
sitting on core teams,

00:35:12.660 --> 00:35:15.240
where there really are a lot of
different people in the room.

00:35:15.240 --> 00:35:17.050
There are different
perspectives.

00:35:17.050 --> 00:35:21.510
There is a marketing
person, a lawyer.

00:35:21.510 --> 00:35:25.951
I was the representative to
online sales or direct sales.

00:35:25.951 --> 00:35:27.450
There are all these
different people

00:35:27.450 --> 00:35:29.120
bringing this diversity
of perspectives.

00:35:29.120 --> 00:35:31.980
And I think that
modular approach

00:35:31.980 --> 00:35:35.990
to product development, I
found to be really effective.

00:35:35.990 --> 00:35:39.150
I know that creating these
trusted environments where

00:35:39.150 --> 00:35:43.700
you can brainstorm openly, IDO
does a good job at that, where

00:35:43.700 --> 00:35:45.990
you don't judge on
the surface, you

00:35:45.990 --> 00:35:49.490
provide an open sharing
platform for brainstorming.

00:35:49.490 --> 00:35:52.300
I think that fosters the
trust and the ability

00:35:52.300 --> 00:35:58.619
to unlock using social
skills and all this.

00:35:58.619 --> 00:35:59.660
But it's a good question.

00:35:59.660 --> 00:36:01.034
And I think it's
something that--

00:36:01.034 --> 00:36:03.850
I think through blending
these two sides,

00:36:03.850 --> 00:36:06.150
bringing together
fuzzy and techie--

00:36:06.150 --> 00:36:09.160
again, it's not a monolith of
techie and a monolith of fuzzy,

00:36:09.160 --> 00:36:11.720
but the two sides.

00:36:11.720 --> 00:36:13.590
And then from
innovation standpoint,

00:36:13.590 --> 00:36:16.720
I think there are
so many ways that we

00:36:16.720 --> 00:36:20.330
can be unlocking the
problems that we have

00:36:20.330 --> 00:36:23.770
all across the country
and all across the world,

00:36:23.770 --> 00:36:26.620
and partnering with
technologists who really know

00:36:26.620 --> 00:36:28.770
the new tools, that are
in search of applications

00:36:28.770 --> 00:36:29.890
for the tools.

00:36:29.890 --> 00:36:31.990
And so the Defense
Department has

00:36:31.990 --> 00:36:35.834
created DIUx, which is down
the street at Moffett Field.

00:36:35.834 --> 00:36:37.250
They've done a
great job, I think,

00:36:37.250 --> 00:36:40.820
of taking some of the problems
that are really rooted in DC,

00:36:40.820 --> 00:36:45.830
rooted in different parts of the
State Department, USA military,

00:36:45.830 --> 00:36:48.620
and brought them out to
Silicon Valley and said,

00:36:48.620 --> 00:36:50.080
here's what we're
struggling with.

00:36:50.080 --> 00:36:52.010
Here the problems that we have.

00:36:52.010 --> 00:36:54.410
Are there ways we can
engage with technology

00:36:54.410 --> 00:36:56.620
to help fix these.

00:36:56.620 --> 00:36:58.830
And Steve Blank, who's an
entrepreneurship professor

00:36:58.830 --> 00:37:00.840
at Stanford and
Berkeley and at Columbia

00:37:00.840 --> 00:37:04.795
sometimes while,
he's created a class,

00:37:04.795 --> 00:37:06.670
it's called Hack for
Defense, and another one

00:37:06.670 --> 00:37:07.940
called Hack for Diplomacy.

00:37:07.940 --> 00:37:11.930
And it basically takes a
mixture of political scientists,

00:37:11.930 --> 00:37:14.560
people from different
fuzzy backgrounds,

00:37:14.560 --> 00:37:17.177
and engineering students
and pairs them on teams

00:37:17.177 --> 00:37:19.260
and puts them against one
of these problems that's

00:37:19.260 --> 00:37:21.807
sourced by a different agency.

00:37:21.807 --> 00:37:23.390
I think they're a
really great example

00:37:23.390 --> 00:37:29.470
of Fuzzy and techie, how we
can take this problem centric

00:37:29.470 --> 00:37:31.880
approach, rather
than just building

00:37:31.880 --> 00:37:34.940
technology in the abstract
and then looking for some way

00:37:34.940 --> 00:37:36.610
to apply it.

00:37:36.610 --> 00:37:39.110
AUDIENCE: I have a question
just in regards to the education

00:37:39.110 --> 00:37:39.860
piece again.

00:37:39.860 --> 00:37:42.220
I went to a college
that had a big core

00:37:42.220 --> 00:37:44.720
curriculum where I took a
lot of liberal arts things.

00:37:44.720 --> 00:37:47.070
I had to take an art
class and a music class.

00:37:47.070 --> 00:37:49.480
And some of the benefit
I feel like you're

00:37:49.480 --> 00:37:51.410
talking about with
the fuzzy comes

00:37:51.410 --> 00:37:53.665
from having this
innate subject matter

00:37:53.665 --> 00:37:55.540
expertise or this deep
knowledge of something

00:37:55.540 --> 00:37:56.990
that you're curious about.

00:37:56.990 --> 00:37:59.760
I was wondering, as you look
at education moving forward,

00:37:59.760 --> 00:38:03.047
is there still a place for this
kind of general core curriculum

00:38:03.047 --> 00:38:04.880
or base level of knowledge
you need to have?

00:38:04.880 --> 00:38:06.700
Or do you really
think it comes down

00:38:06.700 --> 00:38:09.116
to just letting somebody pursue
what they're interested in

00:38:09.116 --> 00:38:11.420
and just going as deep as
possible in that direction?

00:38:11.420 --> 00:38:12.253
SCOTT HARTLEY: Yeah.

00:38:12.253 --> 00:38:13.350
It's a great question.

00:38:13.350 --> 00:38:15.970
Actually, yesterday, I sat
down with the Vice Provost

00:38:15.970 --> 00:38:18.824
of Undergraduate
Education at Stanford.

00:38:18.824 --> 00:38:20.490
And we were talking
about this metaphor.

00:38:20.490 --> 00:38:22.781
He's talked to a bunch students
that treat their-- they

00:38:22.781 --> 00:38:24.850
call them ballistic
students, where

00:38:24.850 --> 00:38:28.640
they're good at getting
where they want to get.

00:38:28.640 --> 00:38:31.340
And they're on this trajectory
of I'm trying to get to this--

00:38:31.340 --> 00:38:32.780
you know, from
point A to point B.

00:38:32.780 --> 00:38:34.613
And I don't want to see
anything in between.

00:38:34.613 --> 00:38:37.391
I know how to execute along
that vector and I'm going.

00:38:37.391 --> 00:38:39.390
And he said, rather than
treating your education

00:38:39.390 --> 00:38:41.705
like a plane ticket,
where you get from here

00:38:41.705 --> 00:38:43.830
to Frankfurt, what do you
do with the plane ticket?

00:38:43.830 --> 00:38:45.120
You throw it in the trash can.

00:38:45.120 --> 00:38:46.730
Now you're in Frankfurt.

00:38:46.730 --> 00:38:51.020
How do you think about this in
terms of tugging on the mind

00:38:51.020 --> 00:38:53.520
and getting a breadth of
exposure to different ideas?

00:38:53.520 --> 00:38:57.480
And he had this notion of
education as the passport.

00:38:57.480 --> 00:38:59.890
And it's a passport that's
infinitely renewable,

00:38:59.890 --> 00:39:02.499
that you want to get stamps
from all over the world.

00:39:02.499 --> 00:39:04.040
And you really want
to collect stamps

00:39:04.040 --> 00:39:06.144
from things that
push your boundaries.

00:39:06.144 --> 00:39:08.810
So if you spend a lot of time in
Europe, you want to go to Asia.

00:39:08.810 --> 00:39:11.351
If you spend a lot of time in
Asia, you want to go to Africa.

00:39:11.351 --> 00:39:14.420
How do you create this
metaphor and how do you

00:39:14.420 --> 00:39:17.000
get students to think about
their education not as a plane

00:39:17.000 --> 00:39:20.110
ticket to point B, which is
a job as a product manager

00:39:20.110 --> 00:39:21.800
at a fancy tech company?

00:39:21.800 --> 00:39:26.120
But how do you think about
developing this holistic skill

00:39:26.120 --> 00:39:26.840
set?

00:39:26.840 --> 00:39:30.070
Being able to explore different
disciplines, different genres.

00:39:30.070 --> 00:39:32.470
And I think that's the only
way that you really unlock

00:39:32.470 --> 00:39:33.830
passion and true interest.

00:39:33.830 --> 00:39:37.990
And that's really
what makes technology

00:39:37.990 --> 00:39:40.550
useful and applicable
is somebody that's

00:39:40.550 --> 00:39:43.130
taking the new tools
and putting them

00:39:43.130 --> 00:39:46.360
in the context of
something that they love.

00:39:46.360 --> 00:39:50.216
I'm a firm believer in
the broad education.

00:39:50.216 --> 00:39:51.590
And I think if
you listen to some

00:39:51.590 --> 00:39:53.706
of the things Mark Cuban's
been saying recently,

00:39:53.706 --> 00:39:56.080
some of the things that Jonathan
Rosenberg here at Google

00:39:56.080 --> 00:39:59.370
has been saying recently about
the value of liberal arts

00:39:59.370 --> 00:40:00.794
in addition to STEM.

00:40:00.794 --> 00:40:02.710
You know, we've got to
become technical enough

00:40:02.710 --> 00:40:05.340
to be dangerous, to
use the new tools,

00:40:05.340 --> 00:40:08.890
to be conversing with data,
to be conversing with these

00:40:08.890 --> 00:40:13.870
new things, but not losing
sight of the breadth of exposure

00:40:13.870 --> 00:40:15.980
that makes it all
useful to begin with.

00:40:15.980 --> 00:40:20.630
And so the liberal arts--
the second deck of the book,

00:40:20.630 --> 00:40:22.820
why the liberal arts will
rule the digital world,

00:40:22.820 --> 00:40:26.470
it's really kind of about
exactly what you're getting at,

00:40:26.470 --> 00:40:30.340
is this breadth of exposure and
how we apply tech meaningfully

00:40:30.340 --> 00:40:33.740
to all these different
areas of our lives.

00:40:33.740 --> 00:40:36.170
And then, the second
kind of component to that

00:40:36.170 --> 00:40:38.310
gets back to the
question of automation,

00:40:38.310 --> 00:40:42.070
where if we have these
simple parts of our jobs,

00:40:42.070 --> 00:40:44.230
parts of our lives that
are automated away,

00:40:44.230 --> 00:40:46.910
which probably will be
over time, what's left

00:40:46.910 --> 00:40:48.230
are the complex tasks.

00:40:48.230 --> 00:40:52.299
What's left are the tasks where
we trade tasks more frequently.

00:40:52.299 --> 00:40:53.340
You're good at one thing.

00:40:53.340 --> 00:40:54.800
I'm good at something else.

00:40:54.800 --> 00:40:57.400
And there's the guy a Harvard
Graduate School of Education

00:40:57.400 --> 00:41:00.220
who talks about the
importance of soft skills.

00:41:00.220 --> 00:41:02.530
And he puts it in
the terms of trade,

00:41:02.530 --> 00:41:05.070
almost like an economic
argument, where

00:41:05.070 --> 00:41:06.570
when there's a lot
of trade, there's

00:41:06.570 --> 00:41:08.580
a lot of transaction
costs, or friction.

00:41:08.580 --> 00:41:10.130
And the thing that
reduces friction,

00:41:10.130 --> 00:41:11.630
that reduces
transaction costs, that

00:41:11.630 --> 00:41:14.524
makes teams more effective,
are these social skills.

00:41:14.524 --> 00:41:15.940
And so, how do you
train for that?

00:41:15.940 --> 00:41:17.660
And I think one way
to train for that

00:41:17.660 --> 00:41:21.850
is through real breadth of
exposure, Socratic methodology,

00:41:21.850 --> 00:41:24.254
debating in class,
debating at night.

00:41:24.254 --> 00:41:25.920
And I think if you're
just on this plane

00:41:25.920 --> 00:41:28.620
ticket, one-way plane ticket,
you're not getting that.

00:41:28.620 --> 00:41:34.100
So I love the passport,
sort of that metaphor.

00:41:34.100 --> 00:41:36.670
AUDIENCE: So you
talked about how

00:41:36.670 --> 00:41:39.200
people see the fuzzy and the
techie as separate things.

00:41:39.200 --> 00:41:41.270
But truly, to kind
of the way we're

00:41:41.270 --> 00:41:45.050
kind of moving towards kind of
seeing how they both interact.

00:41:45.050 --> 00:41:47.720
But I guess I'm just graduated
college, or I'm a year out.

00:41:47.720 --> 00:41:50.580
But it seems like in these big
companies like Google, or Uber,

00:41:50.580 --> 00:41:53.270
or Snapchat, to get
into product or to get

00:41:53.270 --> 00:41:55.620
into the techie
side of things, you

00:41:55.620 --> 00:41:58.770
do have to have a
techie prerequisite.

00:41:58.770 --> 00:42:02.615
So kind of how do you
give advice, I guess,

00:42:02.615 --> 00:42:05.600
to someone like me, who's
trying to see, to blend

00:42:05.600 --> 00:42:10.300
that fuzzy and techie, to
get to be in the product side

00:42:10.300 --> 00:42:12.597
or development of certain
products at a company?

00:42:12.597 --> 00:42:13.430
SCOTT HARTLEY: Yeah.

00:42:13.430 --> 00:42:14.600
It's a great question.

00:42:14.600 --> 00:42:16.510
I think it's true.

00:42:16.510 --> 00:42:18.570
At Google, to be on
the product team,

00:42:18.570 --> 00:42:20.320
there is a requirement
in some way, shape,

00:42:20.320 --> 00:42:25.360
or form of being able
to deal with technology

00:42:25.360 --> 00:42:27.319
in a rigorous way.

00:42:27.319 --> 00:42:28.610
But there are so many examples.

00:42:28.610 --> 00:42:30.460
Like Snapchat, for example.

00:42:30.460 --> 00:42:34.110
One of the core people behind
a lot of the product innovation

00:42:34.110 --> 00:42:35.134
is a sociologist.

00:42:35.134 --> 00:42:37.050
And I talk about him a
little bit in the book.

00:42:37.050 --> 00:42:38.850
His name is Nathan Jorgenson.

00:42:38.850 --> 00:42:40.150
And he's based in Brooklyn.

00:42:40.150 --> 00:42:43.480
He did a PhD in sociology,
and was basically writing

00:42:43.480 --> 00:42:45.980
about where he thought
Snap was going,

00:42:45.980 --> 00:42:47.580
why he thought it was relevant.

00:42:47.580 --> 00:42:51.560
And I think it's a
fascinating case from my seat.

00:42:51.560 --> 00:42:53.950
Why did Snapchat take off?

00:42:53.950 --> 00:42:55.660
Yet to see if it's
truly successful.

00:42:55.660 --> 00:42:57.690
I think it's proven to
be somewhat successful.

00:42:57.690 --> 00:42:59.740
At least, it went public,
which is more than we

00:42:59.740 --> 00:43:02.200
can say for Uber and others.

00:43:02.200 --> 00:43:06.929
But you know, I think given
Instagram, given Google+,

00:43:06.929 --> 00:43:09.470
given Facebook, given all these
different platforms that were

00:43:09.470 --> 00:43:12.202
out there already,
why did Snap--

00:43:12.202 --> 00:43:13.910
why was it able to
break into the market?

00:43:13.910 --> 00:43:16.370
And really, I think
the underlying reason

00:43:16.370 --> 00:43:20.840
why goes back to Nathan's
insights as a sociologist,

00:43:20.840 --> 00:43:23.250
observing sort of
these Gen Zers, who

00:43:23.250 --> 00:43:25.650
had grown up in a world
of digital abundance.

00:43:25.650 --> 00:43:27.850
In a world where every
photo they had ever taken

00:43:27.850 --> 00:43:31.040
was in their Dropbox
or their Google Drive.

00:43:31.040 --> 00:43:33.170
Saved infinitely forever.

00:43:33.170 --> 00:43:35.330
And he said, how do
you create scarcity

00:43:35.330 --> 00:43:36.920
in a world of digital abundance?

00:43:36.920 --> 00:43:39.050
You do it by making
things disappear.

00:43:39.050 --> 00:43:41.450
And it was this core
understanding, I think,

00:43:41.450 --> 00:43:44.710
of the sociological
insight that I think

00:43:44.710 --> 00:43:47.200
really gave them an advantage.

00:43:47.200 --> 00:43:49.250
And Evan Spiegel read
some of these posts

00:43:49.250 --> 00:43:52.390
and immediately hired Nathan,
brought him into Snap.

00:43:52.390 --> 00:43:54.290
And now, he's an
instrumental part

00:43:54.290 --> 00:43:56.740
of the product organization.

00:43:56.740 --> 00:44:00.340
Similarly, I would point to Snap
Spectacles versus Google Glass.

00:44:00.340 --> 00:44:02.930
I think that the insight
with Snap Spectacles

00:44:02.930 --> 00:44:06.750
is that there is a difference
between clear glasses

00:44:06.750 --> 00:44:09.820
and sunglasses, worn indoors
versus worn outdoors.

00:44:09.820 --> 00:44:12.280
I think it's a techie-only
approach versus sort

00:44:12.280 --> 00:44:16.080
of a fashion-forward approach
of looking at the sociological

00:44:16.080 --> 00:44:17.380
insights of how people--

00:44:17.380 --> 00:44:19.100
where people want
to wear glasses.

00:44:19.100 --> 00:44:21.230
What the inherent
assumptions of trust are.

00:44:21.230 --> 00:44:25.459
I think with sunglasses, there's
already this lack of trust

00:44:25.459 --> 00:44:27.000
because you can't
see someone's eyes.

00:44:27.000 --> 00:44:29.920
If they're recording, if they're
posting, if it's done outdoors,

00:44:29.920 --> 00:44:30.960
it's very different
than if you're

00:44:30.960 --> 00:44:32.090
sitting in someone's
living room and you

00:44:32.090 --> 00:44:34.440
think that you're just having
a private conversation.

00:44:34.440 --> 00:44:36.140
And suddenly, it's
being recorded.

00:44:36.140 --> 00:44:38.400
So those small
sociological insights,

00:44:38.400 --> 00:44:40.060
I think are really
at the core of what

00:44:40.060 --> 00:44:44.340
makes a lot of these
products work versus another.

00:44:44.340 --> 00:44:46.260
So I think if you
can form a thesis.

00:44:46.260 --> 00:44:49.200
If you can sort of
be vocal about having

00:44:49.200 --> 00:44:53.280
an opinion of where you see the
market going and sort of owning

00:44:53.280 --> 00:44:58.230
that opinion and providing
that perspective to a product

00:44:58.230 --> 00:45:01.980
manager, or to a product that
you think is interesting,

00:45:01.980 --> 00:45:04.860
I think that's something that
people will find valuable.

00:45:04.860 --> 00:45:07.090
And they'll probably bring
you in on a project basis

00:45:07.090 --> 00:45:08.930
or bring you in in a
consultative capacity.

00:45:08.930 --> 00:45:10.750
And then over time,
that can really

00:45:10.750 --> 00:45:13.010
snowball into a lot of
different, interesting

00:45:13.010 --> 00:45:15.570
opportunities.

00:45:15.570 --> 00:45:17.470
AUDIENCE: I was just
kind of wondering.

00:45:17.470 --> 00:45:21.340
I think when you think about
models and machine learning,

00:45:21.340 --> 00:45:23.860
I think probably the biggest
risk is, like you said,

00:45:23.860 --> 00:45:27.292
the human input part, not
putting enough thought

00:45:27.292 --> 00:45:28.750
into what those
inputs are and then

00:45:28.750 --> 00:45:31.860
your results can
kind of be skewed.

00:45:31.860 --> 00:45:33.630
While I think
there is definitely

00:45:33.630 --> 00:45:37.860
a place for people who are
like "fuzzy" if you will,

00:45:37.860 --> 00:45:40.850
like is there a
danger in not being

00:45:40.850 --> 00:45:43.370
able to look under the hood
and understand what's going on,

00:45:43.370 --> 00:45:46.420
what those inputs actually are
in those models when we think

00:45:46.420 --> 00:45:49.650
about 10, 15 years from now when
our tasks are being automated,

00:45:49.650 --> 00:45:52.040
even for the roles
we're doing today?

00:45:52.040 --> 00:45:53.230
SCOTT HARTLEY: Yeah.

00:45:53.230 --> 00:45:57.470
It's a real question
about black box.

00:45:57.470 --> 00:45:58.870
I think it's
something that we're

00:45:58.870 --> 00:46:02.300
going to be grappling
with for a long time.

00:46:02.300 --> 00:46:03.860
You read an econ paper.

00:46:03.860 --> 00:46:05.900
And you ask the author
of the econ paper,

00:46:05.900 --> 00:46:09.310
why did you assume
this, this, and this?

00:46:09.310 --> 00:46:11.130
Those are just words
and people can barely

00:46:11.130 --> 00:46:12.770
describe their
assumptions and why they

00:46:12.770 --> 00:46:14.274
chose one thing versus another.

00:46:14.274 --> 00:46:15.815
And now we're talking
about thousands

00:46:15.815 --> 00:46:18.530
of lines of code, things
baked into algorithms,

00:46:18.530 --> 00:46:22.500
things pushed out into ones and
zeros, built by composite teams

00:46:22.500 --> 00:46:23.900
across the world.

00:46:23.900 --> 00:46:27.850
And I think there are these
very real questions of bias,

00:46:27.850 --> 00:46:31.740
and who's in the room, and who's
helping make those decisions.

00:46:31.740 --> 00:46:35.120
But I do think we need pure
fuzzies in those scenarios

00:46:35.120 --> 00:46:35.620
as well.

00:46:35.620 --> 00:46:38.280
Because if you look
at predictive policing

00:46:38.280 --> 00:46:41.330
as one example, there's so
many things we could probably

00:46:41.330 --> 00:46:45.350
do with predictive
analytics around IoT sensors

00:46:45.350 --> 00:46:47.800
on police vehicles saying,
OK, where are they going?

00:46:47.800 --> 00:46:49.510
Where are they at
certain times of day?

00:46:49.510 --> 00:46:50.910
Where is reported crime?

00:46:50.910 --> 00:46:54.090
How do we think about deploying
police force in advance

00:46:54.090 --> 00:46:56.460
to mitigate crime?

00:46:56.460 --> 00:46:58.340
And that seems all
nice and harmless.

00:46:58.340 --> 00:47:00.410
But behind the scenes,
you say, well, OK.

00:47:00.410 --> 00:47:03.980
Actually, crime data
is reported crime data.

00:47:03.980 --> 00:47:06.080
Reported crime
data comes from who

00:47:06.080 --> 00:47:08.420
has trust in
community, who feels

00:47:08.420 --> 00:47:11.002
acceptable to report crime.

00:47:11.002 --> 00:47:12.960
If somebody has a dozen
unpaid parking tickets,

00:47:12.960 --> 00:47:16.840
they're probably not calling
the cop for petty theft.

00:47:16.840 --> 00:47:19.950
Certain crimes like hate
crimes and sexual assault

00:47:19.950 --> 00:47:24.150
are chronically under-reported.

00:47:24.150 --> 00:47:26.640
On the flip side,
the arbitrary nature

00:47:26.640 --> 00:47:28.596
of if you sit at
one intersection

00:47:28.596 --> 00:47:30.470
and you watch how many
people run red lights.

00:47:30.470 --> 00:47:32.110
Over time, the
reported crime data

00:47:32.110 --> 00:47:35.117
of running red lights in that
intersection will be very high.

00:47:35.117 --> 00:47:37.200
You might think that people
are disproportionately

00:47:37.200 --> 00:47:38.866
running red lights
at that intersection,

00:47:38.866 --> 00:47:43.190
but that just may be a fallacy
in how the data was collected.

00:47:43.190 --> 00:47:45.380
So I think these questions
behind the scenes

00:47:45.380 --> 00:47:50.980
of if you had a social worker in
the room talking about details

00:47:50.980 --> 00:47:53.670
of how crime statistics
are reported,

00:47:53.670 --> 00:47:56.360
that's a really important
component to have in the room

00:47:56.360 --> 00:47:59.530
if you're about to deploy
predictive policing for Oakland

00:47:59.530 --> 00:48:00.910
PD or something.

00:48:00.910 --> 00:48:03.794
And so I think you ask a
really important question

00:48:03.794 --> 00:48:05.210
that we need to
be thinking about.

00:48:05.210 --> 00:48:06.501
And I don't have a good answer.

00:48:06.501 --> 00:48:10.080
But really, kind of involving
a plurality of people, diverse

00:48:10.080 --> 00:48:11.420
backgrounds.

00:48:11.420 --> 00:48:14.960
Fei-Fei Li, who now runs
Google Cloud's Machine Learning

00:48:14.960 --> 00:48:18.360
and AI, was at Stanford before.

00:48:18.360 --> 00:48:21.340
She's created a
nonprofit called AI4ALL

00:48:21.340 --> 00:48:23.820
that's about bringing
women into AI,

00:48:23.820 --> 00:48:25.710
bringing different
ethnicities into AI,

00:48:25.710 --> 00:48:27.767
different academic
backgrounds into AI.

00:48:27.767 --> 00:48:29.350
And I think those
sorts of initiatives

00:48:29.350 --> 00:48:32.570
are really important to
get those perspectives

00:48:32.570 --> 00:48:34.280
from the ground
floor in how we're

00:48:34.280 --> 00:48:35.446
thinking about these things.

00:48:35.446 --> 00:48:37.590
Just because we pump
it into ones and zeros

00:48:37.590 --> 00:48:40.810
and call it an AI, call
it machine learning,

00:48:40.810 --> 00:48:43.480
call it whatever we want,
doesn't make it objective.

00:48:43.480 --> 00:48:46.770
It still has all these
implicit assumptions and biases

00:48:46.770 --> 00:48:49.289
that are made by humans.

00:48:49.289 --> 00:48:51.580
AUDIENCE: So I'm not sure if
you're familiar with Tyler

00:48:51.580 --> 00:48:52.660
Cowen, I think it is.

00:48:52.660 --> 00:48:56.670
But he runs the blog
"Marginal Revolution."

00:48:56.670 --> 00:48:58.370
But anyway, his
vision of the future

00:48:58.370 --> 00:49:02.300
is basically that there are
these technical superstars who

00:49:02.300 --> 00:49:05.180
become sort of like
the capitalist class.

00:49:05.180 --> 00:49:09.350
And they designed the sort of
Ubers for everyone else to use.

00:49:09.350 --> 00:49:11.240
But without that skill
set, it's sort of

00:49:11.240 --> 00:49:14.730
hard to become a value
producer in society.

00:49:14.730 --> 00:49:17.310
And sort of using your
example of the company,

00:49:17.310 --> 00:49:19.430
I think it's [? Stitchist ?]
or something,

00:49:19.430 --> 00:49:24.040
where the number of
fuzzies just far outweighed

00:49:24.040 --> 00:49:27.420
the number of like the
small number of elite data

00:49:27.420 --> 00:49:30.380
scientist/techies,
how do you sort

00:49:30.380 --> 00:49:33.720
of deal with the question
of these techies--

00:49:33.720 --> 00:49:37.820
or is there going to be
big inequality in terms

00:49:37.820 --> 00:49:40.380
of earning potential
for people, for fuzzies?

00:49:40.380 --> 00:49:44.270
And sort of also, how do
you go about as a company,

00:49:44.270 --> 00:49:47.640
like quantifying and valuing
the impact of fuzzies?

00:49:47.640 --> 00:49:50.547
Because it seems like something
that's much less tangible.

00:49:50.547 --> 00:49:51.380
SCOTT HARTLEY: Yeah.

00:49:51.380 --> 00:49:53.312
I mean, the questions
of inequality.

00:49:53.312 --> 00:49:55.520
And there's another great
book that I would recommend

00:49:55.520 --> 00:49:56.780
that I just picked up.

00:49:56.780 --> 00:49:59.460
It's translation
from Dutch, but it's

00:49:59.460 --> 00:50:02.740
called "Utopia for Realists."

00:50:02.740 --> 00:50:05.520
And it's by a Dutch
author that goes

00:50:05.520 --> 00:50:09.280
into the questions of basic
income and some of these ideas

00:50:09.280 --> 00:50:10.780
that's really interesting.

00:50:10.780 --> 00:50:12.510
I mean on the flip
side of that question,

00:50:12.510 --> 00:50:15.700
I think is the democratization
of the toolkits.

00:50:15.700 --> 00:50:17.230
And the TensorFlows
of the world.

00:50:17.230 --> 00:50:19.190
But The way that you
can kind of-- being

00:50:19.190 --> 00:50:21.850
a full-stack integrator,
more and more people

00:50:21.850 --> 00:50:26.070
with less and less quote,
"technical expertise"

00:50:26.070 --> 00:50:29.680
can actually put these tools
to use in incredible ways.

00:50:29.680 --> 00:50:31.900
And from the Google
blog actually,

00:50:31.900 --> 00:50:34.600
one example in the book
is about a Japanese farmer

00:50:34.600 --> 00:50:37.690
that uses like Arduino
and uses TensorFlow.

00:50:37.690 --> 00:50:41.580
And took 3,000 images
of different cucumbers

00:50:41.580 --> 00:50:44.640
that he has on his farm and
created a classification engine

00:50:44.640 --> 00:50:46.780
to basically auto-sort
all of his cucumbers.

00:50:46.780 --> 00:50:50.730
So he created a robotic
system to completely automate

00:50:50.730 --> 00:50:52.780
his farming routine.

00:50:52.780 --> 00:50:56.210
And so I think we'll equally see
that sort of thing happening.

00:50:56.210 --> 00:50:58.330
To the question of
inequality, I think

00:50:58.330 --> 00:51:01.340
that's a big-- it's a
big question that we all

00:51:01.340 --> 00:51:02.560
need to grapple with.

00:51:02.560 --> 00:51:05.270
But the reality is
that if you have

00:51:05.270 --> 00:51:07.490
a degree in one
thing versus another

00:51:07.490 --> 00:51:09.790
it's a slip of paper
from a university,

00:51:09.790 --> 00:51:12.460
it's not a carte blanche
relevance regardless

00:51:12.460 --> 00:51:13.210
of what it says.

00:51:13.210 --> 00:51:14.960
You know, whether it
says computer science

00:51:14.960 --> 00:51:16.422
or anthropology.

00:51:16.422 --> 00:51:17.880
What's a really
interesting example

00:51:17.880 --> 00:51:21.870
in the book is Zach Sims
when he founded Codecademy,

00:51:21.870 --> 00:51:24.090
he dropped out as a
political science major.

00:51:24.090 --> 00:51:25.520
He went to MIT
and Harvard to try

00:51:25.520 --> 00:51:28.260
to hire all the people to build
Codecademy and none of them

00:51:28.260 --> 00:51:30.950
had the requisite coding
skills that he actually needed.

00:51:30.950 --> 00:51:33.350
And so he said to all of
them, you need to go upskill.

00:51:33.350 --> 00:51:34.320
You need to go to
General Assembly.

00:51:34.320 --> 00:51:36.028
You need to go to some
of these platforms

00:51:36.028 --> 00:51:37.760
to learn the relevant skills.

00:51:37.760 --> 00:51:40.600
And I think it's case in point
for we all have to kind of keep

00:51:40.600 --> 00:51:42.097
our education in beta.

00:51:42.097 --> 00:51:44.180
We've all got to continually
sort of break the ice

00:51:44.180 --> 00:51:45.451
and continue to learn.

00:51:45.451 --> 00:51:47.825
Because realistically, if
you're coming out of university

00:51:47.825 --> 00:51:52.480
in 2017, who knows what the
job market in 2060 looks like?

00:51:52.480 --> 00:51:54.730
Anybody that
speculates has no idea.

00:51:54.730 --> 00:51:58.690
So I think the reality is we've
got to keep education in beta,

00:51:58.690 --> 00:52:01.200
continually reinvest
in ourselves.

00:52:01.200 --> 00:52:03.700
Try to be technical enough
to be dangerous and not

00:52:03.700 --> 00:52:06.739
forget some of these other
methodologies as well.

00:52:06.739 --> 00:52:08.530
SPEAKER 1: Thank you,
everyone, for coming.

00:52:08.530 --> 00:52:11.410
And let's, once again, give
a thank you to Scott Hartley.

00:52:11.410 --> 00:52:11.910
[APPLAUSE]

00:52:11.910 --> 00:52:13.368
ALBERT CHEN: Thanks
so much, Scott.

00:52:13.368 --> 00:52:14.760
SCOTT HARTLEY: Thanks, guys.

