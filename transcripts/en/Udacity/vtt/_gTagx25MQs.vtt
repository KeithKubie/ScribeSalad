WEBVTT
Kind: captions
Language: en

00:00:00.510 --> 00:00:03.220
So to do soft clustering, we're going to use a similar trick to what

00:00:03.220 --> 00:00:08.430
we've used in some of the other lectures, which is to lean on probability theory

00:00:08.430 --> 00:00:13.580
so that now points instead of coming from one cluster or another cluster can

00:00:13.580 --> 00:00:16.560
be probabilistically from one of multiple possible

00:00:16.560 --> 00:00:18.530
clusters. Does that seem like a good idea?

00:00:18.530 --> 00:00:21.270
&gt;&gt; It does. I feel a song coming on. Lean on

00:00:21.270 --> 00:00:24.730
probability when you're not strongly believing

00:00:24.730 --> 00:00:26.940
one thing. No, that doesn't work.

00:00:26.940 --> 00:00:28.240
&gt;&gt; Yeah, that almost worked.

00:00:28.240 --> 00:00:28.780
&gt;&gt; Almost.

00:00:28.780 --> 00:00:30.580
&gt;&gt; Yeah. So that's because it's soft

00:00:30.580 --> 00:00:32.630
clustering or soft assignments instead of hard ones.

00:00:32.630 --> 00:00:33.580
&gt;&gt; I like it.

00:00:33.580 --> 00:00:35.700
&gt;&gt; All right. So to do this, we're

00:00:35.700 --> 00:00:38.900
going to have to connect up a probabilistic generator

00:00:38.900 --> 00:00:40.580
of process with the data that we actually

00:00:40.580 --> 00:00:44.000
observed. So let's assume, and there's many ways

00:00:44.000 --> 00:00:47.130
to go down this road, but we're going to go down the road this way. Assume that

00:00:47.130 --> 00:00:49.300
the data was generated by, what happens is

00:00:49.300 --> 00:00:53.270
we're going to select one of K possible Gaussian distributions.

00:00:53.270 --> 00:00:54.440
So we're going to imagine that the data's

00:00:54.440 --> 00:00:57.950
going to be generated by draws from Gaussians, from normals.

00:00:57.950 --> 00:00:58.755
&gt;&gt; Mm-hm.

00:00:58.755 --> 00:01:01.620
&gt;&gt; Let's assume that we know the variants, sigma

00:01:01.620 --> 00:01:06.240
square, and that the K Gaussians are sampled from uniformly.

00:01:06.240 --> 00:01:09.160
&gt;&gt; Okay. And then what we're going to do is that given

00:01:09.160 --> 00:01:11.840
that Gaussian we're going to select an actual point, an actual data

00:01:11.840 --> 00:01:15.430
point in the space from that Gaussian. And then we repeat

00:01:15.430 --> 00:01:18.310
that n times. So if n is bigger than K then

00:01:18.310 --> 00:01:20.900
we're going to see some points that come from the same Gaussian,

00:01:20.900 --> 00:01:23.450
and if those Gaussians are well separated they're going to look like clusters.

00:01:23.450 --> 00:01:25.090
&gt;&gt; Assuming they have very different means.

00:01:25.090 --> 00:01:28.770
&gt;&gt; Alright. That's what I mean by, we'll separate it. Yeah exactly so.

00:01:28.770 --> 00:01:30.090
&gt;&gt; Oh, yeah, yeah, yeah, okay. Good.

00:01:30.090 --> 00:01:33.310
&gt;&gt; Alright, and in particular, what we'd like to do now is say, alright,

00:01:33.310 --> 00:01:35.290
well, now what we're really, happening, is,

00:01:35.290 --> 00:01:36.960
we're given the data, we're thinking kind

00:01:36.960 --> 00:01:39.600
Bayesianly, right, we're given the data and we want to try to figure out

00:01:39.600 --> 00:01:41.580
what the clusters would have been to

00:01:41.580 --> 00:01:43.380
have generated that data. So we're going to try

00:01:43.380 --> 00:01:45.960
to find a hypothesis, which is this case is just going to

00:01:45.960 --> 00:01:51.973
be a collection of k means, not to be confused with K-means.

00:01:51.973 --> 00:01:52.030
&gt;&gt; Mm.

00:01:52.030 --> 00:01:56.055
&gt;&gt; That maximizes the probability of the data. Right? So find me

00:01:56.055 --> 00:02:00.240
K-mu values, which are the means of those Gaussian distributions. So that

00:02:00.240 --> 00:02:04.600
the probability of the data given that hypothesis is as high as

00:02:04.600 --> 00:02:09.009
possible. And this is an ML hypothesis. And ML of course stands for

00:02:09.009 --> 00:02:09.449
Michael Lipman.

00:02:09.449 --> 00:02:11.640
&gt;&gt; I don't think that's right.

00:02:11.640 --> 00:02:12.140
&gt;&gt; Machine learning.

00:02:13.460 --> 00:02:15.510
&gt;&gt; That's closer, but not quite right.

00:02:15.510 --> 00:02:16.270
&gt;&gt; Maximum likelihood.

00:02:16.270 --> 00:02:17.320
&gt;&gt; That I think is correct.

00:02:17.320 --> 00:02:20.760
&gt;&gt; Alright. So that's now the problem setup. I didn't actually

00:02:20.760 --> 00:02:23.620
give you an algorithm for doing this, but presumably it's going

00:02:23.620 --> 00:02:26.800
to depend on various kinds of things and probability theory and

00:02:26.800 --> 00:02:29.430
optimization, but is it sort of clear what we're shooting for now?

00:02:29.430 --> 00:02:31.250
&gt;&gt; It is, it is. And I actually think the

00:02:31.250 --> 00:02:34.090
fact that we're looking for k means probably menas that

00:02:34.090 --> 00:02:37.380
we are going to end up tying it back to k means. Maybe so, but

00:02:37.380 --> 00:02:40.830
again, it's a softer kind of k means, it's a softer, gentler kind of k means.

00:02:40.830 --> 00:02:41.420
&gt;&gt; Mm.

00:02:41.420 --> 00:02:44.840
&gt;&gt; I think some people call it K Gaussians. Does that sound right?

00:02:44.840 --> 00:02:45.730
&gt;&gt; No.

00:02:45.730 --> 00:02:47.560
&gt;&gt; Alright then.

00:02:47.560 --> 00:02:49.220
&gt;&gt; I mean it sounds correct, but it doesn't sound right.

00:02:49.220 --> 00:02:53.339
&gt;&gt; [LAUGH] Alright then.

