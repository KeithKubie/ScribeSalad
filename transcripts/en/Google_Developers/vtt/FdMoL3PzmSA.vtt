WEBVTT
Kind: captions
Language: en

00:00:06.156 --> 00:00:08.280
COLT MCANLIS: In the mystical
world of compression,

00:00:08.280 --> 00:00:11.320
there's one algorithm that
constantly puzzles developers.

00:00:11.320 --> 00:00:13.470
Its ability to
compress data so well

00:00:13.470 --> 00:00:16.730
is shrouded in ancient secrets
of statistical probability.

00:00:16.730 --> 00:00:19.280
I'm talking, of course,
of arithmetic compression,

00:00:19.280 --> 00:00:21.600
the de facto statistical
compressor that

00:00:21.600 --> 00:00:23.900
has its hands in every part
of the modern compression

00:00:23.900 --> 00:00:25.180
industry.

00:00:25.180 --> 00:00:28.090
Everything you see online,
from images to videos, and even

00:00:28.090 --> 00:00:30.330
your text data, has
been sent through

00:00:30.330 --> 00:00:31.990
its enchanted algorithm.

00:00:31.990 --> 00:00:35.420
Writing an arithmetic compressor
yourself is no easy task.

00:00:35.420 --> 00:00:38.005
It requires you to stare
into the face of the unknown

00:00:38.005 --> 00:00:41.280
and accept what it
has to tell you.

00:00:41.280 --> 00:00:43.800
But fear not, young programmer,
because I'm here to help.

00:00:43.800 --> 00:00:46.060
My name is Colt McAnlis,
and this is Compressor Head.

00:00:50.270 --> 00:00:52.080
Now, if you remember,
variable length

00:00:52.080 --> 00:00:56.160
codes or, VLCs, take a given
symbol from our input stream

00:00:56.160 --> 00:00:58.430
and assigns a code
word to it that

00:00:58.430 --> 00:01:00.710
has a variable number of bits.

00:01:00.710 --> 00:01:03.890
That VLC is then pushed
to the output stream,

00:01:03.890 --> 00:01:06.530
producing a compressed
version of our data.

00:01:06.530 --> 00:01:08.440
The trick is that
variable length codes

00:01:08.440 --> 00:01:10.650
produce compression
that's close to entropy,

00:01:10.650 --> 00:01:13.772
as long as the probability
of your symbol distribution

00:01:13.772 --> 00:01:15.480
matches the probability
table that you've

00:01:15.480 --> 00:01:17.310
built your VLC from.

00:01:17.310 --> 00:01:20.560
But the trick is that VLCs are
generated with a specific set

00:01:20.560 --> 00:01:22.140
of probabilities in mind.

00:01:22.140 --> 00:01:25.720
For example, here--
aha, there we go.

00:01:25.720 --> 00:01:29.450
Let's take a look at Omega
codes next to Even Rodeh codes.

00:01:29.450 --> 00:01:31.840
You can see that, for
these two VLC types,

00:01:31.840 --> 00:01:34.760
they're not equal for
every given value of n.

00:01:34.760 --> 00:01:37.500
This is because of the
statistical probabilities

00:01:37.500 --> 00:01:39.360
that they've made
assumptions towards.

00:01:39.360 --> 00:01:41.900
See, Omega codes
expect that there's not

00:01:41.900 --> 00:01:44.414
going to be lots and lots
of most probable symbols.

00:01:44.414 --> 00:01:45.830
If we end up with
a lot of symbols

00:01:45.830 --> 00:01:48.490
that are close in probability
or have a shared probability,

00:01:48.490 --> 00:01:49.960
then we get into trouble.

00:01:49.960 --> 00:01:53.080
Omega codes are weighted
towards having a dominant symbol

00:01:53.080 --> 00:01:55.890
with the most probability,
while everything else trails off

00:01:55.890 --> 00:01:57.570
in very small amounts.

00:01:57.570 --> 00:01:59.610
Even Rodeh codes,
on the other hand,

00:01:59.610 --> 00:02:02.050
expect that the top four
symbols are actually

00:02:02.050 --> 00:02:04.140
pretty close in
probability to each other.

00:02:04.140 --> 00:02:06.790
But once you hit the eighth
most probable symbol,

00:02:06.790 --> 00:02:09.312
the code word size
increases dramatically.

00:02:09.312 --> 00:02:10.770
This expects there
to be a grouping

00:02:10.770 --> 00:02:13.980
of symbols that are pretty
close and a drop off after that.

00:02:13.980 --> 00:02:16.210
As such, you run into a problem.

00:02:16.210 --> 00:02:17.890
What if the probability
distribution

00:02:17.890 --> 00:02:21.360
of your data stream doesn't
match your chosen VLC set?

00:02:21.360 --> 00:02:23.461
Well, in that case,
you get bloat.

00:02:23.461 --> 00:02:25.210
You'll end up with
code words that are not

00:02:25.210 --> 00:02:27.350
minimal, and as such,
your compressed stream

00:02:27.350 --> 00:02:28.782
won't be equal to entropy.

00:02:28.782 --> 00:02:30.240
This is the trouble
that most folks

00:02:30.240 --> 00:02:32.080
have with practical
implementations

00:02:32.080 --> 00:02:33.600
of variable length codes.

00:02:33.600 --> 00:02:35.840
Which VLC do we use
for our data that

00:02:35.840 --> 00:02:37.090
gives us the best compression?

00:02:37.090 --> 00:02:39.870
I mean, it's not like we can
iterate through every VLC

00:02:39.870 --> 00:02:41.060
and find the best match.

00:02:41.060 --> 00:02:43.300
I mean, that would
just take way too long.

00:02:43.300 --> 00:02:45.030
Instead, we want
some way to encode

00:02:45.030 --> 00:02:47.370
a stream that optimizes
for a probability,

00:02:47.370 --> 00:02:50.540
without having to fit some
pre-existing statistical table.

00:02:50.540 --> 00:02:53.400
And this, my friend, is
where arithmetic compression

00:02:53.400 --> 00:02:54.740
comes in.

00:02:54.740 --> 00:02:58.720
Jorma Rissanen pr-- published--

00:02:58.720 --> 00:03:00.910
Most standard literature
that discusses

00:03:00.910 --> 00:03:03.559
arithmetic compression puts it
a little something like this.

00:03:03.559 --> 00:03:05.350
[CLEARS THROAT] (IN
ELEGANT SPEAKING VOICE)

00:03:05.350 --> 00:03:10.880
"In contrast to VLCs, which
assign a code word per symbol,

00:03:10.880 --> 00:03:13.780
arithmetic compression
assigns one code word

00:03:13.780 --> 00:03:16.820
to the entire data stream."

00:03:16.820 --> 00:03:18.400
Which is elegantly
stated, but really

00:03:18.400 --> 00:03:19.441
doesn't tell us anything.

00:03:19.441 --> 00:03:22.086
I mean, what kind of
code should we actually

00:03:22.086 --> 00:03:22.960
apply to that stream?

00:03:22.960 --> 00:03:26.000
If I'm given AZZ as an input,
I can really pick any number.

00:03:26.000 --> 00:03:29.440
I mean, if I make it
12626, that's cool, right?

00:03:29.440 --> 00:03:31.180
But that ends up
bloating the stream.

00:03:31.180 --> 00:03:34.710
Or I can make AZZ equals 1,
but how would I decompress that

00:03:34.710 --> 00:03:35.370
later?

00:03:35.370 --> 00:03:37.450
See, the magic of
arithmetic compression

00:03:37.450 --> 00:03:40.470
is a data transform that is
applied to the source data

00:03:40.470 --> 00:03:43.300
in order to create a
single output number which

00:03:43.300 --> 00:03:46.110
uses less bits to represent
than the source stream itself.

00:03:46.110 --> 00:03:48.470
But let's not get too
far ahead of ourselves.

00:03:48.470 --> 00:03:50.990
First, let's talk
about binary search.

00:03:50.990 --> 00:03:54.140
Am I-- am I not
toitally-- toitally?

00:03:54.140 --> 00:03:58.090
Let's say I search for the value
6 in the number range 0 to 9.

00:03:58.090 --> 00:04:01.230
A binary search works by
dividing the space in half

00:04:01.230 --> 00:04:03.890
repeatedly, until
we find our number.

00:04:03.890 --> 00:04:06.860
So effectively, we
start with low equals 0

00:04:06.860 --> 00:04:10.860
and high equals 9, which means
the number that we're checking

00:04:10.860 --> 00:04:12.450
is the 5.

00:04:12.450 --> 00:04:16.920
Now, if 6 is greater than 5,
which it is, we erase our low

00:04:16.920 --> 00:04:19.300
and move it around so that
low is now equal to 5,

00:04:19.300 --> 00:04:20.870
and we're checking 7.

00:04:20.870 --> 00:04:24.080
Now, we continue doing this
process of subdividing space

00:04:24.080 --> 00:04:28.680
in half each iteration until
we find the number that we're

00:04:28.680 --> 00:04:30.040
looking for.

00:04:30.040 --> 00:04:32.950
But now the question is, what
if, instead of subdividing

00:04:32.950 --> 00:04:35.060
our space in half
each iteration,

00:04:35.060 --> 00:04:38.690
we instead subdivide it in
thirds, fourths, or 16ths?

00:04:38.690 --> 00:04:42.057
For example, let's subdivide our
number space into four parts.

00:04:42.057 --> 00:04:44.140
And when we choose what
section we're looking for,

00:04:44.140 --> 00:04:47.230
we subdivide that into
four parts as well.

00:04:47.230 --> 00:04:50.260
This concept of subdividing
the space each time

00:04:50.260 --> 00:04:52.450
we refine our
search is the basis

00:04:52.450 --> 00:04:55.110
for how arithmetic
transforms work.

00:04:55.110 --> 00:04:58.800
Effectively, we
assign a subspace

00:04:58.800 --> 00:05:00.540
to each one of our symbols.

00:05:00.540 --> 00:05:03.720
When we encode the symbol
C from our input stream,

00:05:03.720 --> 00:05:07.280
we subdivide C space further,
assigning new symbols

00:05:07.280 --> 00:05:08.940
to the space in
the proper order.

00:05:08.940 --> 00:05:10.220
And this continues on.

00:05:10.220 --> 00:05:12.910
Every single input that we
grab subdivides the space

00:05:12.910 --> 00:05:15.840
into smaller and smaller
subsections each step,

00:05:15.840 --> 00:05:18.214
but with one simple
modification.

00:05:18.214 --> 00:05:19.880
Rather than subdividing
our search space

00:05:19.880 --> 00:05:22.530
into equal portions,
we subdivide

00:05:22.530 --> 00:05:26.070
it based on probabilities of
the symbols in our stream,

00:05:26.070 --> 00:05:27.900
kind of like our
code word tables.

00:05:27.900 --> 00:05:31.570
So if C is the most probable
symbol, it gets the most space.

00:05:31.570 --> 00:05:33.920
Now, in order for this to
work, we need our number range

00:05:33.920 --> 00:05:37.000
to be something that makes sense
to actually be subdividing,

00:05:37.000 --> 00:05:38.630
so I mean we can't use integers.

00:05:38.630 --> 00:05:42.310
Instead, we will
subdivide our number space

00:05:42.310 --> 00:05:47.010
and define it existing
on the range, 0 to 1.

00:05:47.010 --> 00:05:50.640
And then that allows us
to assign each symbol

00:05:50.640 --> 00:05:54.810
an area on the word space
that's equal to its probability.

00:05:54.810 --> 00:05:57.700
And this right here, my
friends, is the starting process

00:05:57.700 --> 00:05:59.660
of how arithmetic
compression works.

00:05:59.660 --> 00:06:01.230
But enough chit chat.

00:06:01.230 --> 00:06:03.020
Let's encode something.

00:06:03.020 --> 00:06:06.460
So let's take an example with
three symbols, R, G, and B,

00:06:06.460 --> 00:06:11.075
with the probabilities of 0.4,
0.5, and 0.1, respectively.

00:06:11.075 --> 00:06:13.700
Now, it's pretty easy, given the
probabilities of these values,

00:06:13.700 --> 00:06:17.320
to assign them ranges on
the interval of 0 to 1.

00:06:17.320 --> 00:06:20.180
Or perhaps there's a better way
to look at this, in number line

00:06:20.180 --> 00:06:22.652
form.

00:06:22.652 --> 00:06:27.530
Let's encode a
simple string, GGB.

00:06:27.530 --> 00:06:30.474
First, we pull the symbol
G from the input stream.

00:06:30.474 --> 00:06:32.640
And since the interval of
G, according to our table,

00:06:32.640 --> 00:06:37.390
is 0.4 to 0.9, we then
subdivide our space,

00:06:37.390 --> 00:06:39.970
giving us the values
between 0.4 and 0.9,

00:06:39.970 --> 00:06:43.842
respectively, which sets
our low and high values, 0.4

00:06:43.842 --> 00:06:47.100
to 0.59999.

00:06:47.100 --> 00:06:51.800
G actually ends up
being 0.6 to 0.84999999.

00:06:51.800 --> 00:06:57.450
And B ends up being
0.85 to 0.999999.

00:06:57.450 --> 00:07:00.120
It's now time to read the
second symbol from our stream.

00:07:00.120 --> 00:07:03.820
It's a G, once again.

00:07:03.820 --> 00:07:07.230
When we read our B value,
we subdivide that interval

00:07:07.230 --> 00:07:08.730
once again.

00:07:08.730 --> 00:07:13.670
After encoding B, our final
interval is now 0.825 and 0.85.

00:07:13.670 --> 00:07:16.955
This is the range that we want
to output as our encoded value.

00:07:16.955 --> 00:07:17.830
Now, congratulations.

00:07:17.830 --> 00:07:19.890
You've actually just
arithmetically encoded

00:07:19.890 --> 00:07:21.560
a stream.

00:07:21.560 --> 00:07:23.920
Now, decoding can be
viewed as the same process,

00:07:23.920 --> 00:07:25.340
but in reverse.

00:07:25.340 --> 00:07:30.490
Now, since our final value
range is 0.825 to 0.85,

00:07:30.490 --> 00:07:33.495
honestly, we can now put
any value between those two

00:07:33.495 --> 00:07:34.910
to our encoded stream.

00:07:34.910 --> 00:07:40.012
So let's just randomly pick a
value and say that it's 0.83.

00:07:40.012 --> 00:07:41.470
The decoding process
can be thought

00:07:41.470 --> 00:07:45.180
of as drawing a vertical line
through our subdivided ranges,

00:07:45.180 --> 00:07:47.010
updating the intervals
as we move down.

00:07:47.010 --> 00:07:49.710
So if we start with
0.83, the symbol

00:07:49.710 --> 00:07:53.060
for that range that it
falls in is the letter G,

00:07:53.060 --> 00:07:55.700
so we output that
as our first symbol.

00:07:55.700 --> 00:07:56.970
Then we subdivide that space.

00:07:56.970 --> 00:08:01.170
And 0.83 once again
falls in the G area,

00:08:01.170 --> 00:08:03.120
which is our second symbol.

00:08:03.120 --> 00:08:06.450
And finally, 0.83
falls in the B range,

00:08:06.450 --> 00:08:08.560
which is our final symbol.

00:08:08.560 --> 00:08:10.440
There you go,
encoding and decoding

00:08:10.440 --> 00:08:14.558
of arithmetic compression
all in one nice package.

00:08:14.558 --> 00:08:17.974
[MUSIC PLAYING]

00:08:17.974 --> 00:08:19.438
I'm good?

00:08:19.438 --> 00:08:20.660
Remember this guy?

00:08:20.660 --> 00:08:23.850
Besides inventing a plethora of
variable length code patterns,

00:08:23.850 --> 00:08:26.620
Peter Elias first
proposed the concept

00:08:26.620 --> 00:08:29.565
behind arithmetic compression
in the early 1960s.

00:08:29.565 --> 00:08:31.190
It wasn't until a
decade later, though,

00:08:31.190 --> 00:08:34.690
when Jorma Rissanen published
some of the first symbol

00:08:34.690 --> 00:08:39.159
research for its implementation,
alongside a massive patent.

00:08:39.159 --> 00:08:41.409
For the next couple of
decades, arithmetic compression

00:08:41.409 --> 00:08:44.620
pretty much fell off the map
due to an impossibly aggressive

00:08:44.620 --> 00:08:47.480
patent strategy enforced
by IBM at the time.

00:08:47.480 --> 00:08:49.360
The patent problem
was so aggressive

00:08:49.360 --> 00:08:51.120
and arithmetic
compression was so good

00:08:51.120 --> 00:08:54.410
that a separate coding
algorithm, called range coding,

00:08:54.410 --> 00:08:57.226
was invented in
1979, which basically

00:08:57.226 --> 00:08:59.100
did the same thing as
arithmetic compression,

00:08:59.100 --> 00:09:00.749
but was free from patents.

00:09:00.749 --> 00:09:02.290
Nothing like a good
government system

00:09:02.290 --> 00:09:03.450
to stifle creativity, huh?

00:09:06.431 --> 00:09:06.930
OK.

00:09:06.930 --> 00:09:09.099
You know, I really
don't think this is all

00:09:09.099 --> 00:09:10.015
really that necessary.

00:09:10.015 --> 00:09:12.892
I mean, it's just a
comment on a video.

00:09:12.892 --> 00:09:13.828
What have we got here?

00:09:13.828 --> 00:09:15.090
We've got another signature?

00:09:15.090 --> 00:09:15.590
Fine.

00:09:15.590 --> 00:09:17.360
I don't even know
what this says.

00:09:17.360 --> 00:09:18.070
Initial, too.

00:09:18.070 --> 00:09:19.770
You guys, of course,
need an initial.

00:09:19.770 --> 00:09:22.423
So really, I think that--

00:09:22.423 --> 00:09:25.250
EULA FRUMPKINS: And here.

00:09:25.250 --> 00:09:27.460
COLT MCANLIS: You guys
need blood samples and hair

00:09:27.460 --> 00:09:28.490
and everything else too?

00:09:28.490 --> 00:09:29.290
EULA FRUMPKINS: Just signatures.

00:09:29.290 --> 00:09:30.090
COLT MCANLIS: OK, fine.

00:09:30.090 --> 00:09:30.770
Listen, I'm very busy.

00:09:30.770 --> 00:09:32.300
Can I at least back to all this?

00:09:32.300 --> 00:09:33.110
EULA FRUMPKINS:
Let me just check.

00:09:33.110 --> 00:09:33.865
COLT MCANLIS: You're
going to check it all.

00:09:33.865 --> 00:09:34.160
OK, thanks.

00:09:34.160 --> 00:09:35.243
EULA FRUMPKINS: Thank you.

00:09:39.646 --> 00:09:41.020
COLT MCANLIS: In
the early 2000s,

00:09:41.020 --> 00:09:42.540
though, the patent expired.

00:09:42.540 --> 00:09:44.460
And arithmetic compression
took off again,

00:09:44.460 --> 00:09:46.050
allowing itself to
achieve the status

00:09:46.050 --> 00:09:48.780
of the gold standard for
the current generation

00:09:48.780 --> 00:09:50.190
of statistical encoders.

00:09:50.190 --> 00:09:52.610
In fact, the majority of
modern compression formats

00:09:52.610 --> 00:09:55.190
for archives, like
LZMA and BZIP,

00:09:55.190 --> 00:09:59.250
audio and video, like
JPEG, WEBP, WEBM, and H.264

00:09:59.250 --> 00:10:01.660
all work with an arithmetic
compression system

00:10:01.660 --> 00:10:03.690
as its statistical
compression step.

00:10:06.970 --> 00:10:09.220
So here's the tricky part
with arithmetic compression.

00:10:09.220 --> 00:10:10.630
Why the hell does it work?

00:10:10.630 --> 00:10:12.630
I mean, it's a simple
concept, but how does it

00:10:12.630 --> 00:10:13.890
get us close to entropy?

00:10:13.890 --> 00:10:16.930
Well, it has everything to do
with probability, distribution,

00:10:16.930 --> 00:10:18.700
and how we subdivide our space.

00:10:18.700 --> 00:10:21.140
So let's try to subdivide
our space evenly.

00:10:21.140 --> 00:10:24.350
If we've got 10 values and we
subdivide our space between 0.2

00:10:24.350 --> 00:10:28.050
and 0.3, we end up with 10
new ranges, starting at,

00:10:28.050 --> 00:10:32.230
0.2, 0.21, 0.22, 0.23,
0.24, 0.25, 0.26,

00:10:32.230 --> 00:10:35.535
0.27, 0.28, and 0.29.

00:10:35.535 --> 00:10:36.660
Look at what just happened.

00:10:36.660 --> 00:10:39.590
By subdividing our
space, our range values

00:10:39.590 --> 00:10:42.280
have been increased
in size by one digit.

00:10:42.280 --> 00:10:44.790
If we stopped right
now, all of our outputs

00:10:44.790 --> 00:10:46.820
would be two digits in length.

00:10:46.820 --> 00:10:49.380
So if we subdivided
any of them again,

00:10:49.380 --> 00:10:52.259
we would end up with
three digits per range.

00:10:52.259 --> 00:10:53.800
And if we divided
those regions, we'd

00:10:53.800 --> 00:10:57.740
end up with four digits per
range, and so on, and so on.

00:10:57.740 --> 00:11:00.242
Can we--- let's move
it really quick.

00:11:00.242 --> 00:11:01.879
Fantastic.

00:11:01.879 --> 00:11:02.670
Thank you, so much.

00:11:02.670 --> 00:11:03.720
All right.

00:11:03.720 --> 00:11:05.830
Now, let's subdivide
that space unevenly

00:11:05.830 --> 00:11:07.570
and, just for the
sake of argument,

00:11:07.570 --> 00:11:10.330
give one range
9/10 of the space.

00:11:10.330 --> 00:11:16.120
So we'd end up with the starting
spaces of 0.2, 0.291, 0.292,

00:11:16.120 --> 00:11:22.550
0.293, 0.294, 0.295, 0.296,
0.297, 0.298, and 0.299.

00:11:22.550 --> 00:11:23.300
Now, look at that.

00:11:23.300 --> 00:11:27.990
Every time we fall in this first
range, going from 0.2 to 0.291,

00:11:27.990 --> 00:11:31.220
we're outputting less
digits to the output stream.

00:11:31.220 --> 00:11:33.800
At best, we could simply
use the lower bound

00:11:33.800 --> 00:11:37.070
of 0.2, which is less digits
than any other starting

00:11:37.070 --> 00:11:37.980
value in our range.

00:11:37.980 --> 00:11:41.320
And at best, we don't end
up outputting any new digits

00:11:41.320 --> 00:11:43.290
to our output code at all.

00:11:43.290 --> 00:11:48.090
Even if we're 10,024 levels
deep into the subdivision tree,

00:11:48.090 --> 00:11:51.200
picking this first range
will output less digits

00:11:51.200 --> 00:11:52.760
to our output,
and at best cases,

00:11:52.760 --> 00:11:55.280
won't put out any
new digits at all.

00:11:55.280 --> 00:11:58.410
This skewing of unit space
based on the probability

00:11:58.410 --> 00:12:00.580
of the symbol is
the core idea that

00:12:00.580 --> 00:12:03.420
makes arithmetic compression
a statistical compressor.

00:12:03.420 --> 00:12:05.230
Effectively, the more
probable the symbol,

00:12:05.230 --> 00:12:08.370
the less digits it
outputs to our encoding.

00:12:08.370 --> 00:12:10.130
Props.

00:12:10.130 --> 00:12:12.260
When all the dust settles,
arithmetic compression

00:12:12.260 --> 00:12:14.500
can consistently
get close to entropy

00:12:14.500 --> 00:12:17.180
for arbitrary streams,
regardless of the data type

00:12:17.180 --> 00:12:19.540
or the probability of
its symbol distribution.

00:12:19.540 --> 00:12:22.950
It's single minded focus
makes it a Swiss army knife

00:12:22.950 --> 00:12:24.410
of statistical compression.

00:12:24.410 --> 00:12:26.740
But there's a large
battle between Huffman

00:12:26.740 --> 00:12:29.130
and arithmetic in the minds
of compression programmers

00:12:29.130 --> 00:12:29.650
out there.

00:12:29.650 --> 00:12:33.330
Some claim that the Huffman king
is dead, long live arithmetic

00:12:33.330 --> 00:12:35.480
compression, while the
true believers claim

00:12:35.480 --> 00:12:37.450
that the king is
alive and well, living

00:12:37.450 --> 00:12:40.140
under the alias, dynamic
Huffman encoding.

00:12:40.140 --> 00:12:41.741
But that's for a
different episode.

00:12:41.741 --> 00:12:42.740
My name is Colt McAnlis.

00:12:42.740 --> 00:12:43.760
Thanks, for watching.

00:12:43.760 --> 00:12:45.310
[MUSIC PLAYING]

