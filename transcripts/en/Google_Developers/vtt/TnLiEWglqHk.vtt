WEBVTT
Kind: captions
Language: en

00:00:04.059 --> 00:00:05.610
REUVEN LAX: Hi everyone.

00:00:05.610 --> 00:00:06.460
My name is Reuven.

00:00:06.460 --> 00:00:08.210
I'm an engineer at Google.

00:00:08.210 --> 00:00:11.850
This is my colleague Marwa
from Product Management.

00:00:11.850 --> 00:00:15.010
And we're here to talk to you
today about big data analysis

00:00:15.010 --> 00:00:17.720
and how to make it fast.

00:00:17.720 --> 00:00:21.730
So those of you who've
done this before,

00:00:21.730 --> 00:00:24.460
who've written big
data analysis before,

00:00:24.460 --> 00:00:26.520
know how painful it
can be sometimes.

00:00:26.520 --> 00:00:31.090
You spend a lot of time writing
your configuration files,

00:00:31.090 --> 00:00:33.710
managing how much
parallelism you need,

00:00:33.710 --> 00:00:36.640
managing pools of machines.

00:00:36.640 --> 00:00:38.450
You often spend
more time doing this

00:00:38.450 --> 00:00:41.460
then you actually spend
writing the code that

00:00:41.460 --> 00:00:44.640
does your analysis.

00:00:44.640 --> 00:00:46.450
We want you to be
able to move very,

00:00:46.450 --> 00:00:50.700
very quickly from concept,
from sort of your white board

00:00:50.700 --> 00:00:53.910
design of what you want
your analysis to be,

00:00:53.910 --> 00:00:58.780
to a production code that
runs and runs at scale.

00:00:58.780 --> 00:01:01.540
These pipelines also
need to be really fast.

00:01:01.540 --> 00:01:05.440
The world we live in is
very, very quickly changing.

00:01:05.440 --> 00:01:09.740
And you need to keep up with
real time trends in your data.

00:01:09.740 --> 00:01:12.750
Your pipeline needs to be
able to stream in current data

00:01:12.750 --> 00:01:16.900
and constantly update its
results as the data comes in.

00:01:16.900 --> 00:01:19.900
So to allow you to
solve these problems,

00:01:19.900 --> 00:01:24.270
I'm really excited to give a
peek at Google Cloud Dataflow.

00:01:27.840 --> 00:01:29.340
Those of you who
were at the keynote

00:01:29.340 --> 00:01:31.090
yesterday and
actually stayed there

00:01:31.090 --> 00:01:33.940
until the end of the keynote
already got a peak of Dataflow

00:01:33.940 --> 00:01:35.100
in action.

00:01:35.100 --> 00:01:37.580
So what is Dataflow?

00:01:37.580 --> 00:01:41.990
Dataflow is a system for
building big and fast data

00:01:41.990 --> 00:01:43.990
analysis pipelines.

00:01:43.990 --> 00:01:47.230
The way it works is you write
a sequence of logical data

00:01:47.230 --> 00:01:51.460
transformations that are easy
to write and very intuitive

00:01:51.460 --> 00:01:54.170
that specify the
analysis you want to do.

00:01:54.170 --> 00:01:56.360
You submit this to
the data flow service.

00:01:56.360 --> 00:01:58.750
And it runs for you.

00:01:58.750 --> 00:01:59.700
It's fully managed.

00:01:59.700 --> 00:02:03.680
So it runs for you on as many
machines as are necessary

00:02:03.680 --> 00:02:05.890
in order to run your pipeline.

00:02:05.890 --> 00:02:11.380
All the configuration, all the
parallelism, all of the tuning,

00:02:11.380 --> 00:02:13.057
it's all taken care of for you.

00:02:13.057 --> 00:02:15.140
And you don't need to spend
a lot of time thinking

00:02:15.140 --> 00:02:19.120
about that when you're
writing your program.

00:02:19.120 --> 00:02:22.690
So let's look at a quick
example that uses Dataflow.

00:02:26.710 --> 00:02:28.700
So a feature that
you're familiar

00:02:28.700 --> 00:02:33.650
with from many websites out
there is auto completing words.

00:02:33.650 --> 00:02:35.440
So for instance,
Google does this.

00:02:35.440 --> 00:02:37.730
As you start typing
a search, we auto

00:02:37.730 --> 00:02:40.170
complete and give
search suggestions.

00:02:40.170 --> 00:02:41.730
And also Twitter does this.

00:02:41.730 --> 00:02:43.630
If you start typing
in a hash tag,

00:02:43.630 --> 00:02:46.330
Twitter will automatically
start suggesting hashtags

00:02:46.330 --> 00:02:49.630
that you might actually
be trying to type.

00:02:49.630 --> 00:02:53.525
So what does it take to build
an application like this?

00:02:53.525 --> 00:02:56.570
And let's focus now on this
Twitter example of auto

00:02:56.570 --> 00:02:58.260
completing hash tags.

00:02:58.260 --> 00:03:03.420
Well, there's a front
end that displays this.

00:03:03.420 --> 00:03:05.940
And the front end is
all about displaying

00:03:05.940 --> 00:03:09.120
it well and
intuitively and making

00:03:09.120 --> 00:03:11.650
it easy for the user to use.

00:03:11.650 --> 00:03:13.720
But the front end
needs a back end

00:03:13.720 --> 00:03:16.320
to serve up the data, to
tell the front end, OK,

00:03:16.320 --> 00:03:18.200
the user has typed in "ar."

00:03:18.200 --> 00:03:20.500
What are the suggestions
that I should

00:03:20.500 --> 00:03:23.190
display right now for "ar"?

00:03:23.190 --> 00:03:26.850
And this is something that's
been traditionally painful

00:03:26.850 --> 00:03:30.600
building these data models
and keeping them up to date.

00:03:30.600 --> 00:03:35.110
And this is particularly what
Dataflow makes easy to do.

00:03:35.110 --> 00:03:40.030
So for this auto
completing of hashtag let's

00:03:40.030 --> 00:03:42.780
start off by thinking
about what we would need.

00:03:42.780 --> 00:03:45.090
So what we need to do,
this is a prefix model.

00:03:45.090 --> 00:03:49.480
What the users are typing
are prefixes of hashtags

00:03:49.480 --> 00:03:52.910
that they want until they've
typed the entire hashtag,

00:03:52.910 --> 00:03:55.460
you have increasing prefixes.

00:03:55.460 --> 00:03:58.260
So we want something like this.

00:03:58.260 --> 00:03:59.837
Actually, unfortunately,
the slides

00:03:59.837 --> 00:04:01.920
seem to have scrolled off
the screen a little bit.

00:04:01.920 --> 00:04:03.200
I'm not sure why.

00:04:03.200 --> 00:04:06.990
But what's missing here
is on the left side here,

00:04:06.990 --> 00:04:10.540
I'm putting "ar," "arg," "arge."

00:04:10.540 --> 00:04:15.450
And we're showing that
the suggestions for our

00:04:15.450 --> 00:04:16.320
these prefixes.

00:04:16.320 --> 00:04:19.079
And you see as the user types
in more and more characters--

00:04:19.079 --> 00:04:20.620
we've colored the
prefix that they've

00:04:20.620 --> 00:04:26.707
typed-- the suggestions
keep refining.

00:04:26.707 --> 00:04:28.290
So this is the model
we want to build.

00:04:28.290 --> 00:04:33.080
This is the data we want
for our application.

00:04:33.080 --> 00:04:34.970
Let's think for a
minute how we would

00:04:34.970 --> 00:04:38.970
go about logically
building this model.

00:04:38.970 --> 00:04:41.280
I'm going to start
off with a batch

00:04:41.280 --> 00:04:44.250
analysis over a
bunch of static data.

00:04:44.250 --> 00:04:48.480
For instance, all the tweets
that happened yesterday.

00:04:48.480 --> 00:04:50.570
Then later I'll show
how we will modify

00:04:50.570 --> 00:04:55.100
this to process streaming
data to keep up with changing

00:04:55.100 --> 00:04:57.620
hashtags as they start
and stop printing.

00:05:01.040 --> 00:05:03.680
So I know where
I'm starting from.

00:05:03.680 --> 00:05:04.995
I know where I'm going to.

00:05:04.995 --> 00:05:07.280
I'm starting from this
big batch of tweets

00:05:07.280 --> 00:05:08.850
that happened yesterday.

00:05:08.850 --> 00:05:11.450
And at the end, I want
to produce predictions

00:05:11.450 --> 00:05:16.010
that I can write out somewhere
where my app can read it.

00:05:16.010 --> 00:05:19.130
So we want to start off
by reading these tweets.

00:05:19.130 --> 00:05:21.570
So let's read.

00:05:21.570 --> 00:05:24.450
So we redid a whole
bunch of tweets.

00:05:24.450 --> 00:05:27.220
Argentina scores,
Armenia versus Argentina,

00:05:27.220 --> 00:05:31.550
which is not a real game by
the way-- my art project.

00:05:31.550 --> 00:05:35.400
And for reference, I
put down the models

00:05:35.400 --> 00:05:37.570
that I actually want
to write out here,

00:05:37.570 --> 00:05:40.950
so I know where I'm going to
as I'm building up my pipeline.

00:05:40.950 --> 00:05:42.540
So I want these prefix models.

00:05:42.540 --> 00:05:44.949
I want models that
map every prefix

00:05:44.949 --> 00:05:45.990
to a list of suggestions.

00:05:49.040 --> 00:05:53.810
So I'm trying to auto
complete hashtags.

00:05:53.810 --> 00:05:55.440
So I actually don't
care about anything

00:05:55.440 --> 00:05:57.080
in the tweet that's
not a hashtag.

00:05:57.080 --> 00:05:59.010
So let's extract the hashtags.

00:05:59.010 --> 00:06:04.640
And now I have Argentina,
Armenia, Argentina, and so on.

00:06:04.640 --> 00:06:09.830
Next step is, I know that I
need to rank these hashtags

00:06:09.830 --> 00:06:12.730
and decide which are
the most popular ones.

00:06:12.730 --> 00:06:16.220
So let's count them.

00:06:16.220 --> 00:06:21.010
So I'm counting here how often
every unique hashtag happened

00:06:21.010 --> 00:06:22.160
yesterday.

00:06:22.160 --> 00:06:25.130
And in this fake
data set up here,

00:06:25.130 --> 00:06:27.330
Argentina happened 5
million times yesterday,

00:06:27.330 --> 00:06:31.470
Armenia 2 million
times and so on.

00:06:31.470 --> 00:06:36.120
Now, looking back here at
my results I want to get,

00:06:36.120 --> 00:06:41.040
I'm reminded that I want
prefixes of all these hashtags.

00:06:41.040 --> 00:06:44.110
So let's expand
all the prefixes.

00:06:44.110 --> 00:06:45.970
Let's take every one
of these hashtags

00:06:45.970 --> 00:06:50.510
and explode it into all the
prefixes of the hashtags.

00:06:50.510 --> 00:06:54.140
So expand the prefixes.

00:06:54.140 --> 00:06:56.190
And now this is what we have.

00:06:56.190 --> 00:06:59.760
I've expanded them into
a set of key value pairs,

00:06:59.760 --> 00:07:05.220
where the keys are all the
prefixes of every word that

00:07:05.220 --> 00:07:06.680
was a hashtag.

00:07:06.680 --> 00:07:12.690
And the values are the original
hashtags that had this prefix.

00:07:12.690 --> 00:07:15.410
And I'm keeping around
this ranking information,

00:07:15.410 --> 00:07:16.780
these counts.

00:07:16.780 --> 00:07:19.740
Because we are almost here.

00:07:19.740 --> 00:07:23.290
The only problem is that
we have too many results.

00:07:23.290 --> 00:07:25.820
A is going to have
millions of results.

00:07:25.820 --> 00:07:29.840
And we only want a few results
to suggest to the user.

00:07:29.840 --> 00:07:32.130
So there's only
one step left to do

00:07:32.130 --> 00:07:34.360
to get from here to
my actual suggestions.

00:07:34.360 --> 00:07:39.760
And that's to just take the top
three results per key, using

00:07:39.760 --> 00:07:44.350
this ranking that I've
attached to it from the count.

00:07:44.350 --> 00:07:46.430
And now I have all
the data I need

00:07:46.430 --> 00:07:51.860
to do this prefix suggestion,
this hashtag suggestion.

00:07:51.860 --> 00:07:53.860
All that's left to do is
write it out somewhere

00:07:53.860 --> 00:08:00.160
and for my application to read
that data as the user types it.

00:08:00.160 --> 00:08:05.010
So we just went through
this in the design phase.

00:08:05.010 --> 00:08:09.800
We wrote a sequence of
logical data transformations

00:08:09.800 --> 00:08:14.263
that made a lot of sense and
got us from a batch of tweets

00:08:14.263 --> 00:08:18.060
to a set of predictions.

00:08:18.060 --> 00:08:23.780
What does it take to go
from this to actual code?

00:08:23.780 --> 00:08:26.670
And with Dataflow the
answer is not much.

00:08:29.390 --> 00:08:32.799
This is the actual code
I would write in order

00:08:32.799 --> 00:08:36.030
to run this pipeline.

00:08:36.030 --> 00:08:38.580
So notice that the
lines of code here

00:08:38.580 --> 00:08:43.070
are matching one to one
with these logical steps

00:08:43.070 --> 00:08:43.919
that I wrote here.

00:08:43.919 --> 00:08:47.680
Read, extract tags,
count and so on.

00:08:47.680 --> 00:08:51.360
So most of my time here
was just spent designing.

00:08:51.360 --> 00:08:53.250
I spent time on the
whiteboard or here

00:08:53.250 --> 00:08:57.080
on the slide designing what
the pipeline would look like.

00:08:57.080 --> 00:09:00.350
And once I was finished with
designing, at that point,

00:09:00.350 --> 00:09:03.350
writing the code was really
easy and took very little time.

00:09:06.420 --> 00:09:09.420
I want to quickly point
out these two lines here

00:09:09.420 --> 00:09:15.330
that say ParDo under my
extract and expand prefixes.

00:09:15.330 --> 00:09:18.820
This is how you insert
user defined code

00:09:18.820 --> 00:09:21.160
into a Dataflow pipeline.

00:09:21.160 --> 00:09:25.870
So a ParDo, we call it ParDo
because they are parallel.

00:09:25.870 --> 00:09:27.350
It's a little bit
of code you write

00:09:27.350 --> 00:09:30.980
that runs in parallel over
all the records coming out

00:09:30.980 --> 00:09:33.990
of the previous stage
and transforms them

00:09:33.990 --> 00:09:36.550
into records for the next stage.

00:09:36.550 --> 00:09:38.860
Those of you who have
used MapReduce before,

00:09:38.860 --> 00:09:42.240
just something that we invented
at Google about a little

00:09:42.240 --> 00:09:44.760
over 10 years ago,
I believe, might

00:09:44.760 --> 00:09:47.950
recognize this as very
similar to a mapper function.

00:09:47.950 --> 00:09:51.790
Although Dataflow is far more
general than just MapReduce.

00:09:51.790 --> 00:09:54.380
These ParDo functions you write
are usually very short though.

00:09:54.380 --> 00:09:57.570
So for this expand
prefixes, this

00:09:57.570 --> 00:09:59.350
is the ParDo for
expand prefixes.

00:09:59.350 --> 00:10:02.490
It's a short snippet of
code that takes in a word

00:10:02.490 --> 00:10:05.130
and just iterates over
the word and outputs

00:10:05.130 --> 00:10:08.710
every prefix of the word.

00:10:08.710 --> 00:10:12.620
So now you have your code here.

00:10:12.620 --> 00:10:17.580
You could run it locally on your
machine with some sample data,

00:10:17.580 --> 00:10:20.570
iterate on it,
debug it, keep going

00:10:20.570 --> 00:10:24.560
until it appears to do
exactly what you want.

00:10:24.560 --> 00:10:26.150
And then you hit
the point where you

00:10:26.150 --> 00:10:31.400
want to run this on your
actual big data at scale.

00:10:31.400 --> 00:10:35.030
And that's where
Dataflow comes in.

00:10:35.030 --> 00:10:39.640
So you take your code, and now
instead of running it locally

00:10:39.640 --> 00:10:42.810
on your machine, you submit it
to the Google Cloud Dataflow

00:10:42.810 --> 00:10:45.110
Service.

00:10:45.110 --> 00:10:48.240
Once it's submitted to the
Google Cloud Dataflow Service,

00:10:48.240 --> 00:10:51.570
it starts running for you
and processing all your data.

00:10:54.350 --> 00:10:57.650
The Dataflow service also
gives you a monitoring console,

00:10:57.650 --> 00:11:02.590
which allows you to monitor
this data while it's processing.

00:11:02.590 --> 00:11:05.600
It let's you see
progress of your data

00:11:05.600 --> 00:11:10.794
and helps you debug problems in
your pipeline as they happen.

00:11:10.794 --> 00:11:14.000
This is a simple and
intuitive monitoring console

00:11:14.000 --> 00:11:17.220
that matches one to one to
the lines of code you wrote.

00:11:17.220 --> 00:11:20.290
So we're not giving you
these big blocks saying,

00:11:20.290 --> 00:11:21.790
something is wrong
in your pipeline.

00:11:21.790 --> 00:11:24.110
We're giving you,
here's what's happening

00:11:24.110 --> 00:11:25.540
with your extract tags.

00:11:25.540 --> 00:11:27.290
Here's what's happening
with your account.

00:11:27.290 --> 00:11:33.900
Here's what's happening with
your expand prefixes and so on.

00:11:33.900 --> 00:11:36.400
So this is all very
simple in terms

00:11:36.400 --> 00:11:37.560
of the use of the pipeline.

00:11:37.560 --> 00:11:40.840
You don't have to do
much to use this service.

00:11:40.840 --> 00:11:42.970
But behind the
scenes, a little more

00:11:42.970 --> 00:11:44.630
has to happen to make this work.

00:11:48.230 --> 00:11:49.520
So here's your view.

00:11:49.520 --> 00:11:53.450
You write your code and
submit it to Cloud Dataflow

00:11:53.450 --> 00:11:57.110
and just watch it via
this monitoring console.

00:11:57.110 --> 00:11:59.910
However, this programming
model we gave you

00:11:59.910 --> 00:12:04.820
allowed you to write
very fine-grained steps.

00:12:04.820 --> 00:12:07.429
You're writing
your extract tags.

00:12:07.429 --> 00:12:08.470
And then you're counting.

00:12:08.470 --> 00:12:10.110
And then you're
expanding each prefix.

00:12:10.110 --> 00:12:14.030
If we actually ran
these as written

00:12:14.030 --> 00:12:17.280
separately on all of
Twitter's data from yesterday,

00:12:17.280 --> 00:12:20.090
it would take way
too long to finish.

00:12:20.090 --> 00:12:23.781
So in order for this to
work, we need an optimizer.

00:12:23.781 --> 00:12:29.310
The optimizer takes in the
code you've written, determines

00:12:29.310 --> 00:12:33.770
which lines of code can actually
run together as one group,

00:12:33.770 --> 00:12:35.720
rearranges things a
little bit into something

00:12:35.720 --> 00:12:41.810
that can run efficiently, and
produces this optimized program

00:12:41.810 --> 00:12:46.650
on the right here
to actually execute.

00:12:46.650 --> 00:12:50.940
Older systems actually
didn't provide an optimizer.

00:12:50.940 --> 00:12:53.310
So when you use those
systems, you usually

00:12:53.310 --> 00:12:55.350
had to hand optimize your code.

00:12:55.350 --> 00:12:59.300
So for example, MapReduce again,
if you're using MapReduce,

00:12:59.300 --> 00:13:02.800
you have to spend a lot of time
thinking which lines of code

00:13:02.800 --> 00:13:05.210
should run in the
first MapReduce

00:13:05.210 --> 00:13:08.180
and which lines of code should
run in the second MapReduce.

00:13:08.180 --> 00:13:10.220
We let you just write
things logically

00:13:10.220 --> 00:13:11.960
as they would work
in your application.

00:13:11.960 --> 00:13:15.640
And we do this
optimization for you.

00:13:15.640 --> 00:13:18.210
You need a scheduler.

00:13:18.210 --> 00:13:20.590
In reality, this is
running on a lot of data.

00:13:20.590 --> 00:13:22.900
This is running on big data.

00:13:22.900 --> 00:13:26.940
In order to process it, you
need many VMs, many machines

00:13:26.940 --> 00:13:29.830
to process this data.

00:13:29.830 --> 00:13:33.090
You need to schedule reading
stuff in from Google Cloud

00:13:33.090 --> 00:13:36.290
Storage, distributing
the Google Cloud Storage

00:13:36.290 --> 00:13:40.380
data on to all these worker
machines, onto all these VMS,

00:13:40.380 --> 00:13:44.400
getting data off the VMs,
and back into your output,

00:13:44.400 --> 00:13:48.340
which appears also on
Google Cloud Storage.

00:13:48.340 --> 00:13:50.656
You may need to expand.

00:13:50.656 --> 00:13:52.030
Halfway through
the pipeline, you

00:13:52.030 --> 00:13:54.185
may decide we actually
need more machines.

00:13:54.185 --> 00:13:55.060
We don't have enough.

00:13:55.060 --> 00:13:59.290
You may need to add some
machines to this pool.

00:13:59.290 --> 00:14:01.380
You need a scheduler
to run this.

00:14:01.380 --> 00:14:06.390
And you don't want to have
to configure it yourself.

00:14:06.390 --> 00:14:11.170
So with Google Cloud Dataflow,
we're fully managing this.

00:14:11.170 --> 00:14:15.730
So all of this lives
behind the cloud.

00:14:15.730 --> 00:14:19.780
You don't have to be
aware of all these things

00:14:19.780 --> 00:14:22.100
that need to happen when
you submit your pipeline

00:14:22.100 --> 00:14:24.280
to be executed.

00:14:24.280 --> 00:14:25.870
Now we believe in
transparency here.

00:14:25.870 --> 00:14:32.350
So these worker machines here,
our Google Compute Engine VMs,

00:14:32.350 --> 00:14:36.166
so you are free
to SSH into them.

00:14:36.166 --> 00:14:37.790
Take a look at your
actual code running

00:14:37.790 --> 00:14:39.710
and see what's happening.

00:14:39.710 --> 00:14:41.320
But this is never
something that you

00:14:41.320 --> 00:14:43.660
have to do in order
to use Dataflow.

00:14:47.260 --> 00:14:52.650
So we talked about
doing this in batch.

00:14:52.650 --> 00:14:55.880
But we're talking about
Twitter hashtags here.

00:14:55.880 --> 00:14:58.580
Twitter hashtags change
very, very quickly.

00:14:58.580 --> 00:15:01.190
A hashtag that is
trending now may no longer

00:15:01.190 --> 00:15:03.200
be trending in an hour.

00:15:03.200 --> 00:15:06.650
If we wait until the
next day to process

00:15:06.650 --> 00:15:09.900
yesterday's batch of
data to give suggestions,

00:15:09.900 --> 00:15:12.967
our application is
going to be displaying

00:15:12.967 --> 00:15:14.175
very out of date information.

00:15:14.175 --> 00:15:16.060
It will be displaying--

00:15:16.060 --> 00:15:18.330
We won't be displaying the
current trending hashtags.

00:15:18.330 --> 00:15:22.280
We'll be display hashtags
that were trending yesterday.

00:15:22.280 --> 00:15:27.800
We need to actually keep
up with the current data.

00:15:27.800 --> 00:15:29.450
So let's stream it.

00:15:29.450 --> 00:15:33.290
Let's take this batch
pipeline that we just wrote

00:15:33.290 --> 00:15:37.460
and change it to process
streaming data to process

00:15:37.460 --> 00:15:41.400
the hashtag information
as it's happening.

00:15:41.400 --> 00:15:43.320
So how do we jump this gap?

00:15:43.320 --> 00:15:45.820
This sounds like a
difficult problem.

00:15:45.820 --> 00:15:48.010
And in the past, it
actually often required

00:15:48.010 --> 00:15:50.690
rewriting your entire
pipeline from scratch

00:15:50.690 --> 00:15:53.600
using a completely
different framework.

00:15:53.600 --> 00:15:56.530
And with Dataflow,
we make this easy

00:15:56.530 --> 00:16:00.900
and don't require you
to rewrite the pipeline.

00:16:00.900 --> 00:16:01.990
So what needs to change?

00:16:01.990 --> 00:16:04.580
Let's think about
that for a second.

00:16:04.580 --> 00:16:06.930
Your reads and writes.

00:16:06.930 --> 00:16:11.170
We were before reading a static
set of files containing tweets

00:16:11.170 --> 00:16:13.390
from Google Cloud Storage.

00:16:13.390 --> 00:16:17.130
Now, instead, we want to
read it from a streaming

00:16:17.130 --> 00:16:21.100
source providing tweets.

00:16:21.100 --> 00:16:26.010
So for that we're using
Google Cloud Pub/Sub.

00:16:26.010 --> 00:16:30.390
Google Cloud Pub/Sub is
a new product from Google

00:16:30.390 --> 00:16:33.460
that just went into
limited preview that

00:16:33.460 --> 00:16:38.670
provides a reliable many to
many message delivery channel.

00:16:38.670 --> 00:16:41.060
And here it integrates
with Dataflow

00:16:41.060 --> 00:16:44.630
in order to allow you to
constantly stream data in

00:16:44.630 --> 00:16:47.950
and out of a
pipeline, essentially

00:16:47.950 --> 00:16:51.880
to create a data flow pipeline,
a data analysis pipeline, that

00:16:51.880 --> 00:16:55.037
never finishes-- an
infinite pipeline tracking

00:16:55.037 --> 00:16:55.870
trends in real time.

00:16:58.910 --> 00:17:01.290
The other thing
we need to do when

00:17:01.290 --> 00:17:06.030
we write a streaming pipeline,
a pipeline that never finishes,

00:17:06.030 --> 00:17:09.380
is we need to think
about the time element.

00:17:09.380 --> 00:17:12.880
We don't want the tweets that
happen last week to still

00:17:12.880 --> 00:17:17.550
be influencing the suggestions
that we're providing.

00:17:17.550 --> 00:17:20.660
So we want somehow to
age out the old data

00:17:20.660 --> 00:17:25.119
and prioritize the new
data that's coming in.

00:17:25.119 --> 00:17:31.940
So continuing my example of the
fake Argentina-Armenia game,

00:17:31.940 --> 00:17:36.890
here's a graph of suggestions
that may begin with "ar."

00:17:36.890 --> 00:17:39.810
As you see, for a
while a lot of people

00:17:39.810 --> 00:17:41.190
were searching for argyle.

00:17:41.190 --> 00:17:43.480
And over time that
doesn't change.

00:17:43.480 --> 00:17:46.140
But as this game
begins and ends,

00:17:46.140 --> 00:17:51.180
the data of people tweeting
about Argentina and Armenia

00:17:51.180 --> 00:17:53.270
and the game starts trending.

00:17:53.270 --> 00:17:56.840
And after the game ends,
excited and disappointed fans

00:17:56.840 --> 00:17:58.740
start tweeting.

00:17:58.740 --> 00:18:01.080
And then eventually
over time, as the game

00:18:01.080 --> 00:18:03.520
fades into the past and
people forget about it,

00:18:03.520 --> 00:18:06.300
we would expect this graph
to start converging back

00:18:06.300 --> 00:18:09.750
to something looking
like the beginning.

00:18:09.750 --> 00:18:12.140
So what do we want to do
to modify our pipeline

00:18:12.140 --> 00:18:15.820
to keep track of
what's going on now?

00:18:15.820 --> 00:18:19.230
So we're going to
use a sliding window.

00:18:19.230 --> 00:18:22.320
So if we take a
window of time that

00:18:22.320 --> 00:18:29.830
keeps advancing
as time advances,

00:18:29.830 --> 00:18:34.290
we want our pipeline to
track data within this window

00:18:34.290 --> 00:18:37.470
and immediately forget about
data before the window.

00:18:37.470 --> 00:18:43.300
So we're prioritizing the
stuff that's happening now.

00:18:43.300 --> 00:18:48.900
So what does it take to go
from this to actual code

00:18:48.900 --> 00:18:54.150
to change my pipeline to be
a proper streaming pipeline?

00:18:54.150 --> 00:18:58.280
Again, with Dataflow,
very, very little.

00:18:58.280 --> 00:19:02.440
So here is the pipeline we wrote
earlier, the batch pipeline

00:19:02.440 --> 00:19:04.590
for tweet analysis.

00:19:04.590 --> 00:19:08.750
And let's change this to
be a streaming pipeline.

00:19:08.750 --> 00:19:11.020
So first of all, our
core logic that's

00:19:11.020 --> 00:19:14.640
doing our actual analysis
doesn't have to change.

00:19:14.640 --> 00:19:17.900
Extracting hashtags,
counting, expanding prefixes,

00:19:17.900 --> 00:19:20.697
there's nothing streaming or
batch specific about that.

00:19:20.697 --> 00:19:21.780
So let's leave that alone.

00:19:24.465 --> 00:19:26.840
We no longer want to read data
from Google Cloud Storage,

00:19:26.840 --> 00:19:29.100
we want to instead
read data from Pub/Sub

00:19:29.100 --> 00:19:30.790
from our streaming source.

00:19:30.790 --> 00:19:33.020
So let's change that.

00:19:33.020 --> 00:19:37.080
Now we're reading and
running data from Pub/Sub.

00:19:37.080 --> 00:19:40.960
How do we add this sliding
window to age out old data?

00:19:40.960 --> 00:19:43.500
Well, it takes a whole
extra line of code

00:19:43.500 --> 00:19:45.880
to do that with Dataflow.

00:19:45.880 --> 00:19:46.920
and there we go.

00:19:46.920 --> 00:19:50.560
We add a Bucket.by
SlidingWindows of 60 minutes.

00:19:50.560 --> 00:19:54.540
And now our entire pipeline is
processing on the last window.

00:19:54.540 --> 00:19:59.570
And data older than
the window is aged out.

00:19:59.570 --> 00:20:04.050
Dataflow provides far
more sophisticated buckets

00:20:04.050 --> 00:20:07.960
than this as well, although
the SlidingWindow bucket is

00:20:07.960 --> 00:20:11.110
often sufficient for most needs.

00:20:14.240 --> 00:20:17.840
Changing the code here
was very, very simple.

00:20:17.840 --> 00:20:21.270
We did very little to go
from batch to streaming.

00:20:21.270 --> 00:20:26.010
But behind the scenes, again,
a lot more has to happen.

00:20:26.010 --> 00:20:30.335
So this is our behind
the scenes picture again.

00:20:30.335 --> 00:20:32.390
But now it's been
modified to show what's

00:20:32.390 --> 00:20:35.690
happening for a
streaming pipeline.

00:20:35.690 --> 00:20:37.520
Our optimizer has to change.

00:20:37.520 --> 00:20:41.940
The optimization algorithms for
optimizing a streaming pipeline

00:20:41.940 --> 00:20:44.190
are different than the
optimization algorithms

00:20:44.190 --> 00:20:47.210
for optimizing a batch pipeline.

00:20:47.210 --> 00:20:51.100
Scheduling is also slightly
different between streaming

00:20:51.100 --> 00:20:52.770
and batch.

00:20:52.770 --> 00:20:55.320
Instead of managing a
bunch of static files

00:20:55.320 --> 00:20:57.620
in Google Cloud Storage,
we now have that we now

00:20:57.620 --> 00:21:01.810
have to manage Pub/Sub
topics and subscriptions.

00:21:01.810 --> 00:21:04.700
Our worker VMs need a
little bit of storage

00:21:04.700 --> 00:21:06.950
in order to keep track
of this bucketing in data

00:21:06.950 --> 00:21:10.210
to see what data is seen so far.

00:21:10.210 --> 00:21:12.970
However, again, the
point of Dataflow

00:21:12.970 --> 00:21:15.880
is even though many things
have to change in the back end

00:21:15.880 --> 00:21:20.770
to go from batch to streaming,
you are insulated from this.

00:21:20.770 --> 00:21:22.590
You use a programming
model that's

00:21:22.590 --> 00:21:25.390
focused on the what you
want to do to the data

00:21:25.390 --> 00:21:27.860
and not as focused on
the how you want to do it

00:21:27.860 --> 00:21:31.760
to your data, which means
that changing this how,

00:21:31.760 --> 00:21:36.200
changing the batch analysis to
a stream analysis, is simple.

00:21:36.200 --> 00:21:39.930
And you don't have to write a
completely different program.

00:21:39.930 --> 00:21:46.130
So let's take a
quick look at what

00:21:46.130 --> 00:21:49.420
this actual running pipeline is.

00:21:49.420 --> 00:21:54.170
This is this monitoring UI
that I hinted at earlier.

00:21:54.170 --> 00:21:58.870
And this is this actual
pipeline running right now.

00:21:58.870 --> 00:22:01.750
So as I scroll
through it, you'll

00:22:01.750 --> 00:22:07.910
see that all of these
boxes correspond exactly

00:22:07.910 --> 00:22:08.840
to the code I write.

00:22:08.840 --> 00:22:14.220
I have a bucket.by an
extract tags, account.

00:22:14.220 --> 00:22:15.720
If you look for a
second, you'll see

00:22:15.720 --> 00:22:19.780
that extract tags, the
data coming out of count

00:22:19.780 --> 00:22:22.650
is lower data rate then
coming out of extract tags,

00:22:22.650 --> 00:22:27.240
because it's coalescing
all unique tags.

00:22:27.240 --> 00:22:30.130
But then after we go
out of expand prefixes,

00:22:30.130 --> 00:22:33.790
the data blows up from
1,200 to about 11,000.

00:22:33.790 --> 00:22:36.800
Because we're taking
every word and fanning it

00:22:36.800 --> 00:22:40.230
out to all of its prefixes.

00:22:40.230 --> 00:22:42.500
This is also a
place where I would

00:22:42.500 --> 00:22:44.850
look for problems
in my pipeline.

00:22:44.850 --> 00:22:50.070
So if there was a problem, if
the data out of expand prefixes

00:22:50.070 --> 00:22:52.720
dropped to zero, I
would know that there

00:22:52.720 --> 00:22:54.230
was a problem in
expand prefixes.

00:22:54.230 --> 00:22:57.840
And I should go and look
at that part of my code.

00:22:57.840 --> 00:23:00.840
I can also hover over
these edges here.

00:23:00.840 --> 00:23:04.780
And I can see that this edge
between into extract tags, here

00:23:04.780 --> 00:23:06.910
is the total records
we've seen so far.

00:23:06.910 --> 00:23:11.190
And it's currently lagging about
between four and seven seconds

00:23:11.190 --> 00:23:12.090
behind real time.

00:23:15.810 --> 00:23:21.430
So here we have a really
quick App Engine app

00:23:21.430 --> 00:23:25.560
I threw together that is
listening to the Pub/Sub output

00:23:25.560 --> 00:23:27.780
stream of this
data flow pipeline

00:23:27.780 --> 00:23:30.160
and using it give suggestions.

00:23:30.160 --> 00:23:35.030
So I had it prefilled with "b."

00:23:35.030 --> 00:23:38.980
Right now there's a game going
on between Germany and the US.

00:23:38.980 --> 00:23:43.560
So I'm going to type in
"germ" and see what we get.

00:23:43.560 --> 00:23:44.680
And here we go.

00:23:44.680 --> 00:23:49.879
Germany, Germany for World
Cup, Germany versus USA.

00:23:49.879 --> 00:23:51.420
If we kept watching
this long enough,

00:23:51.420 --> 00:23:53.960
we would see these
suggestions constantly refresh

00:23:53.960 --> 00:23:58.410
and change as the
Dataflow pipeline keeps

00:23:58.410 --> 00:24:02.190
giving new suggestions based
off the Twitter input data.

00:24:11.760 --> 00:24:14.480
So now I'm going to introduce
Marwa again, our product

00:24:14.480 --> 00:24:16.820
manager who's going to talk
to you a little bit more

00:24:16.820 --> 00:24:19.830
about the Dataflow product
and various use cases.

00:24:22.370 --> 00:24:23.980
MARWA MABROUK: So hello.

00:24:23.980 --> 00:24:25.830
I hope after all
the details we've

00:24:25.830 --> 00:24:28.470
covered, you would
agree that this

00:24:28.470 --> 00:24:33.900
is a new dawn for
analyzing real time data,

00:24:33.900 --> 00:24:37.080
especially if it's very
large in size like stream,

00:24:37.080 --> 00:24:38.530
like Twitter data.

00:24:38.530 --> 00:24:44.370
And per that, this seems like
something also that's reachable

00:24:44.370 --> 00:24:46.520
and that you could start using.

00:24:46.520 --> 00:24:49.110
So to go and
summarize the details

00:24:49.110 --> 00:24:52.580
that you've just seen, which
Reuven has just showed you,

00:24:52.580 --> 00:24:54.275
we have two very
important things

00:24:54.275 --> 00:24:56.740
that we need to remember.

00:24:56.740 --> 00:25:00.180
The first one is that
solving a problem

00:25:00.180 --> 00:25:02.490
uses a logical
sequence of thinking.

00:25:02.490 --> 00:25:05.990
And that writing code
should follow that

00:25:05.990 --> 00:25:08.410
without having to
compromise efficiency.

00:25:08.410 --> 00:25:10.530
You shouldn't have to twist
how you think in order

00:25:10.530 --> 00:25:12.960
to write code that is efficient.

00:25:12.960 --> 00:25:16.170
You can write the code the
way you think and depend

00:25:16.170 --> 00:25:20.170
on the system to do the
optimization on your behalf.

00:25:20.170 --> 00:25:22.690
Additionally, when
you are writing

00:25:22.690 --> 00:25:24.340
code for parallel
processing, you

00:25:24.340 --> 00:25:26.330
don't have to take into
account the parallel

00:25:26.330 --> 00:25:27.710
processing aspects themselves.

00:25:27.710 --> 00:25:29.160
We've seen the custom code.

00:25:29.160 --> 00:25:32.210
It writes through
a simple data type

00:25:32.210 --> 00:25:34.550
that you don't have to take
into account what is really

00:25:34.550 --> 00:25:38.840
happening when the parallel
processing is taking place.

00:25:38.840 --> 00:25:41.790
Also we've seen that
model allows you not

00:25:41.790 --> 00:25:43.910
to worry about
deploying a framework,

00:25:43.910 --> 00:25:46.450
setting it up, optimizing
it, fine tuning it.

00:25:46.450 --> 00:25:48.770
All of that is
inherently taken care of.

00:25:48.770 --> 00:25:52.050
However, it is not a black box.

00:25:52.050 --> 00:25:54.050
You can still have
full visibility

00:25:54.050 --> 00:25:56.910
into what is happening
within the system.

00:25:56.910 --> 00:25:58.470
So that was the first point.

00:25:58.470 --> 00:26:02.730
The second point that we
see is that your data logic,

00:26:02.730 --> 00:26:05.380
whether you're running
it in batch or streaming,

00:26:05.380 --> 00:26:06.570
is the same.

00:26:06.570 --> 00:26:08.390
And switching between
batch and streaming

00:26:08.390 --> 00:26:10.710
should be pretty seamless
by just switching

00:26:10.710 --> 00:26:13.830
the data input
and the data sync.

00:26:13.830 --> 00:26:16.190
And it happens very
easily and quickly.

00:26:16.190 --> 00:26:20.520
And you can do all of that
using Google Cloud Dataflow.

00:26:20.520 --> 00:26:25.820
So when you're starting to build
an application or a solution,

00:26:25.820 --> 00:26:28.000
typically developers
have a lot on their mind.

00:26:28.000 --> 00:26:29.900
They have to worry about
the infrastructure,

00:26:29.900 --> 00:26:32.710
where the application will
run, what kind of frameworks

00:26:32.710 --> 00:26:34.490
are included to
provide the features

00:26:34.490 --> 00:26:36.350
and meet the
requirements in addition

00:26:36.350 --> 00:26:38.880
to the UI and visibility
and how to engage

00:26:38.880 --> 00:26:41.510
the users-- a whole lot
of things to think about.

00:26:41.510 --> 00:26:44.280
So what we're seeing
today simply allows

00:26:44.280 --> 00:26:46.720
you not to worry about
infrastructure management

00:26:46.720 --> 00:26:48.390
anymore.

00:26:48.390 --> 00:26:51.400
You don't have to worry about
how parallel processing will

00:26:51.400 --> 00:26:53.880
happen and how to
tune that yourself.

00:26:53.880 --> 00:26:55.460
You don't have to
do that anymore.

00:26:55.460 --> 00:26:58.160
And additionally, you
don't have to go hunting

00:26:58.160 --> 00:27:00.930
through logs that are
distributed all over the place

00:27:00.930 --> 00:27:03.620
or understand what is
happening in the system

00:27:03.620 --> 00:27:07.500
by purchasing very
expensive logging systems.

00:27:07.500 --> 00:27:09.510
You can simply rely
on the monitoring

00:27:09.510 --> 00:27:12.180
that we provide that allows
you to see what is happening

00:27:12.180 --> 00:27:14.960
in addition to accessing your
log seamlessly from that one

00:27:14.960 --> 00:27:16.500
location.

00:27:16.500 --> 00:27:19.590
And simply just focus
on your application,

00:27:19.590 --> 00:27:22.310
the logic of your application.

00:27:22.310 --> 00:27:25.050
And now you can easily
analyze massive data

00:27:25.050 --> 00:27:28.340
and include that as part
of your application.

00:27:28.340 --> 00:27:30.700
You can include
big data analytics

00:27:30.700 --> 00:27:34.430
that rely on parallel
processing immediately.

00:27:34.430 --> 00:27:36.460
You don't have to
learn new skill sets.

00:27:36.460 --> 00:27:38.740
You don't have to
go and understand

00:27:38.740 --> 00:27:41.200
how complicated frameworks work.

00:27:41.200 --> 00:27:43.806
You can start today,
right away with the skill

00:27:43.806 --> 00:27:45.660
set that you already have.

00:27:45.660 --> 00:27:48.112
So let's discuss some
examples for that.

00:27:48.112 --> 00:27:48.950
Log analysis.

00:27:48.950 --> 00:27:52.900
I hope this is something
that is not new to everyone.

00:27:52.900 --> 00:27:55.470
Certainly we can't imagine
a world without our logs.

00:27:55.470 --> 00:27:59.250
So let's imagine that your
logs are all over the place.

00:27:59.250 --> 00:28:02.690
And you want to do something
like click analysis.

00:28:02.690 --> 00:28:05.340
You start by wanting to
read them from everywhere.

00:28:05.340 --> 00:28:09.650
So if we start applying
Dataflow pipeline,

00:28:09.650 --> 00:28:12.460
we would start by reading
from those different locations

00:28:12.460 --> 00:28:14.370
just by pointing to them.

00:28:14.370 --> 00:28:15.790
And then through
the pipeline, you

00:28:15.790 --> 00:28:17.590
would apply the kind
of analysis, what kind

00:28:17.590 --> 00:28:19.360
of information
you're looking for,

00:28:19.360 --> 00:28:21.390
how to understand
the user experience

00:28:21.390 --> 00:28:24.167
as they navigate
through the application.

00:28:24.167 --> 00:28:25.750
And usually people
would run this type

00:28:25.750 --> 00:28:27.990
of analysis every
night or every week

00:28:27.990 --> 00:28:31.380
to optimize the application
to understand how the user

00:28:31.380 --> 00:28:34.080
experience is working out.

00:28:34.080 --> 00:28:36.870
But now think that you
can easily switch this

00:28:36.870 --> 00:28:39.890
from taking the log
files to inputting

00:28:39.890 --> 00:28:42.220
this data as a stream.

00:28:42.220 --> 00:28:44.630
And your application
now has the opportunity

00:28:44.630 --> 00:28:48.050
to not just understand what
the user experience is like,

00:28:48.050 --> 00:28:52.120
but also to react to
that user experience.

00:28:52.120 --> 00:28:54.530
You're able to decide
on the fly based

00:28:54.530 --> 00:28:57.050
on how the user is navigating
the application, what

00:28:57.050 --> 00:29:00.490
is the right thing
to show them next.

00:29:00.490 --> 00:29:01.920
Right?

00:29:01.920 --> 00:29:04.260
Well, the pipeline
can also generate

00:29:04.260 --> 00:29:06.510
information that is
usable for reports.

00:29:06.510 --> 00:29:08.680
So at the end, maybe
that information

00:29:08.680 --> 00:29:12.680
is something you would like to
save in storage of some sort

00:29:12.680 --> 00:29:16.290
where you can generate reports
like be queried for example.

00:29:16.290 --> 00:29:19.270
Another example is data
generated by mobile phones.

00:29:19.270 --> 00:29:22.960
Today we all have Smartphones
and they generate tons of data.

00:29:22.960 --> 00:29:25.940
Now you can imagine
a pipeline that

00:29:25.940 --> 00:29:28.360
is reading some information
from your mobile phone.

00:29:28.360 --> 00:29:31.370
Something like the
GPS data for example.

00:29:31.370 --> 00:29:35.160
And then using that data, you
can generate traffic model.

00:29:35.160 --> 00:29:36.990
Using that traffic
model, your application

00:29:36.990 --> 00:29:40.430
can identify where are the
areas that people actually

00:29:40.430 --> 00:29:41.410
pass by most.

00:29:41.410 --> 00:29:46.160
And from there, where's the
best place to open a shop?

00:29:46.160 --> 00:29:48.240
Also you can switch
this to streaming

00:29:48.240 --> 00:29:51.430
and apply it to an application
that not just allows

00:29:51.430 --> 00:29:54.070
you to understand where
people are going through,

00:29:54.070 --> 00:29:56.620
when to avoid the
most busy roads,

00:29:56.620 --> 00:29:59.390
but also to think about
things like what's

00:29:59.390 --> 00:30:00.240
happening right now.

00:30:00.240 --> 00:30:02.660
So if there's a big event
where people are hanging out,

00:30:02.660 --> 00:30:04.620
where can I go and join them?

00:30:04.620 --> 00:30:08.365
And from there kind of
make it more interactive.

00:30:08.365 --> 00:30:10.720
Another good example
is fraud detection.

00:30:10.720 --> 00:30:14.520
Now fraud detection is a bit
of a more special example

00:30:14.520 --> 00:30:17.240
because it is dealing with
sensitive information.

00:30:17.240 --> 00:30:18.590
You're dealing with identity.

00:30:18.590 --> 00:30:22.960
You dealing with information
about financial services.

00:30:22.960 --> 00:30:27.680
And usually the things people
are very careful about.

00:30:27.680 --> 00:30:31.850
So in addition to that, fraud
detection is something that

00:30:31.850 --> 00:30:33.910
cannot happen in past tense.

00:30:33.910 --> 00:30:37.580
You would not be able to
be very patient if you

00:30:37.580 --> 00:30:39.360
know someone already
has got your bank

00:30:39.360 --> 00:30:43.180
account or your identity
and are using them.

00:30:43.180 --> 00:30:45.090
So fraud detection
is something that

00:30:45.090 --> 00:30:46.640
has to be very interactive.

00:30:46.640 --> 00:30:49.380
And usually the challenge
is the amount of data

00:30:49.380 --> 00:30:52.740
that you have to do in
you're real time analysis.

00:30:52.740 --> 00:30:54.570
But now this is all
possible because

00:30:54.570 --> 00:30:55.980
of technologies like that.

00:30:55.980 --> 00:31:02.240
We were talking about doing
true streaming analysis, not

00:31:02.240 --> 00:31:05.800
micro batch or being limited
by specific amounts of data.

00:31:05.800 --> 00:31:07.780
You can actually--
just as much data

00:31:07.780 --> 00:31:10.330
and go to very, very
large data as much

00:31:10.330 --> 00:31:14.190
as you like with this solution.

00:31:14.190 --> 00:31:17.590
So with that said, if
you haven't been thinking

00:31:17.590 --> 00:31:21.950
about big data before, maybe
it's time to think bigger.

00:31:21.950 --> 00:31:25.650
And it's time to think of the
Cloud and how it's limitless

00:31:25.650 --> 00:31:28.110
and makes things a lot easier.

00:31:28.110 --> 00:31:30.520
And because they
are much easier,

00:31:30.520 --> 00:31:33.950
naturally it's much
faster to develop.

00:31:33.950 --> 00:31:36.100
And also, you could start
dealing with data types

00:31:36.100 --> 00:31:38.177
you were not
naturally using before

00:31:38.177 --> 00:31:40.010
because of the complexity
of the technology.

00:31:40.010 --> 00:31:42.500
But now it makes
things a lot easier

00:31:42.500 --> 00:31:45.830
for you to be able to include
it in your application.

00:31:45.830 --> 00:31:47.600
And as you've learned
throughout I/O

00:31:47.600 --> 00:31:50.330
from the very start
of the keynote,

00:31:50.330 --> 00:31:53.120
we want to collaborate
with you on innovation.

00:31:53.120 --> 00:31:56.110
So this is really how
true progress happens.

00:31:56.110 --> 00:31:58.645
And we can't wait to see
the kind of applications

00:31:58.645 --> 00:32:00.900
that you will build
on top of this.

00:32:00.900 --> 00:32:03.580
We're really looking
forward to that.

00:32:03.580 --> 00:32:06.990
And speaking of that, I want
to point you to a few more

00:32:06.990 --> 00:32:09.490
locations where you can
get more information.

00:32:09.490 --> 00:32:12.430
We still have events
happening in the Sandbox area

00:32:12.430 --> 00:32:14.600
if you haven't
visited them already.

00:32:14.600 --> 00:32:17.460
Also, if you're interested
in getting access

00:32:17.460 --> 00:32:19.180
to these products
that we've mentioned,

00:32:19.180 --> 00:32:22.327
first Google Cloud Dataflow
is in private testing.

00:32:22.327 --> 00:32:23.785
You can register
for announcements.

00:32:23.785 --> 00:32:27.340
And I recognize that the
link is very hard to see.

00:32:27.340 --> 00:32:29.880
So I'll read it out loud.

00:32:29.880 --> 00:32:32.200
Cloud underscore
Dataflow underscore

00:32:32.200 --> 00:32:34.870
announce at Google Groups.

00:32:34.870 --> 00:32:37.560
And this is a link
to be able to go

00:32:37.560 --> 00:32:41.850
to if you would like to apply
for the private beta testing.

00:32:41.850 --> 00:32:44.070
Additionally Pub/Sub
that we've mentioned

00:32:44.070 --> 00:32:45.470
is already in limited preview.

00:32:45.470 --> 00:32:47.390
So you should be able
to get it off the Cloud

00:32:47.390 --> 00:32:50.600
at google.com/pubsub website.

00:32:50.600 --> 00:32:54.380
And if you would like to
get started with the Cloud,

00:32:54.380 --> 00:32:57.100
there's also special
offer you could

00:32:57.100 --> 00:32:59.520
start over there
using this promo code.

00:32:59.520 --> 00:33:02.620
And you would get that
much amount of credit

00:33:02.620 --> 00:33:05.890
to play around and get a feel
for these products that are not

00:33:05.890 --> 00:33:08.926
in limited preview on
private data testing.

00:33:08.926 --> 00:33:10.800
But play around with
the other Cloud services

00:33:10.800 --> 00:33:13.920
that you'd like to
get familiar with.

00:33:13.920 --> 00:33:16.594
But all of that said,
I'd like to thank you.

00:33:16.594 --> 00:33:18.010
We are really happy
that you could

00:33:18.010 --> 00:33:19.093
make it today and join us.

00:33:19.093 --> 00:33:22.790
And I hope you could see the
links better now in this light.

00:33:22.790 --> 00:33:24.270
I will leave them up there.

00:33:24.270 --> 00:33:26.150
And we're very happy
to take your questions.

00:33:26.150 --> 00:33:26.650
Thank you.

00:33:35.050 --> 00:33:35.630
AUDIENCE: Hi.

00:33:35.630 --> 00:33:36.580
Two questions.

00:33:36.580 --> 00:33:38.850
First, you guys talked a
lot about the optimizations

00:33:38.850 --> 00:33:40.920
you guys do behind the scenes.

00:33:40.920 --> 00:33:43.720
But then in the visual, where
you show the flow of data,

00:33:43.720 --> 00:33:47.850
it seemed to be in the
original set of code

00:33:47.850 --> 00:33:49.250
that the developer would put in.

00:33:49.250 --> 00:33:50.010
MARWA MABROUK: Mm-hmm.

00:33:50.010 --> 00:33:50.676
REUVEN LAX: Yes.

00:33:50.676 --> 00:33:55.120
And that's actually the
point of the visual console

00:33:55.120 --> 00:33:57.220
there that even though
what we're actually

00:33:57.220 --> 00:34:00.560
running behind the scenes is
not that code as it's written,

00:34:00.560 --> 00:34:04.120
it's the optimized code,
we untangle it for you

00:34:04.120 --> 00:34:07.993
and let you view it as
your code is written.

00:34:07.993 --> 00:34:08.659
AUDIENCE: Great.

00:34:08.659 --> 00:34:10.917
And second question,
what's the pricing of this?

00:34:10.917 --> 00:34:12.500
MARWA MABROUK: It's
a little bit early

00:34:12.500 --> 00:34:14.929
because we are in private
beta, to talk about pricing.

00:34:14.929 --> 00:34:17.120
Right now, users are
showing the private beta

00:34:17.120 --> 00:34:20.725
and naturally that doesn't
cost anything at the moment.

00:34:20.725 --> 00:34:21.433
AUDIENCE: Thanks.

00:34:25.510 --> 00:34:28.920
AUDIENCE: You mentioned Google
Cloud Storage and Pub/Sub

00:34:28.920 --> 00:34:31.179
as input readers and writers.

00:34:31.179 --> 00:34:34.040
I was curious what
the others are.

00:34:34.040 --> 00:34:36.810
REUVEN LAX: As Marwa
said, this is still early.

00:34:36.810 --> 00:34:40.320
So those are our main input
and output sources now.

00:34:40.320 --> 00:34:42.204
But we plan on supporting more.

00:34:42.204 --> 00:34:43.629
AUDIENCE: Can we write our own?

00:34:43.629 --> 00:34:44.420
MARWA MABROUK: Yes.

00:34:44.420 --> 00:34:45.224
AUDIENCE: OK.

00:34:45.224 --> 00:34:45.890
REUVEN LAX: Yes.

00:34:48.770 --> 00:34:51.270
AUDIENCE: Can you guys talk a
little bit more about Pub/Sub.

00:34:51.270 --> 00:34:53.100
Is this you're
thinking about this

00:34:53.100 --> 00:34:58.850
is sort of a programmable or
governed inner input, output

00:34:58.850 --> 00:35:01.300
layer that I can just say
who can access to this.

00:35:01.300 --> 00:35:01.960
You can't.

00:35:01.960 --> 00:35:04.335
How they can use the data,
whether they can read or write

00:35:04.335 --> 00:35:05.860
it or is it much
simpler than that?

00:35:05.860 --> 00:35:06.526
REUVEN LAX: Yes.

00:35:06.526 --> 00:35:10.270
So you can specify access
control on Pub/Sub.

00:35:10.270 --> 00:35:12.030
And it's a many to many.

00:35:12.030 --> 00:35:15.200
So you configure Pub/Sub topics.

00:35:15.200 --> 00:35:18.404
And then subscribers can
configure subscriptions

00:35:18.404 --> 00:35:19.695
that subscribe to those topics.

00:35:27.820 --> 00:35:30.612
AUDIENCE: I have a question
about your example here.

00:35:30.612 --> 00:35:37.454
What exactly were you omitting
on the output topic or topics?

00:35:37.454 --> 00:35:39.620
REUVEN LAX: What I was
omitting on that output topic

00:35:39.620 --> 00:35:44.980
from my example was exactly
what I showed in the diagram.

00:35:44.980 --> 00:35:51.820
A set of key prefix to list
of suggestion suggestions.

00:35:51.820 --> 00:35:55.790
I had it serialized in
JSON so my App Engine

00:35:55.790 --> 00:35:59.580
app just subscribed to that,
listened to all these JSONs

00:35:59.580 --> 00:36:02.289
and wrote them to
App Engine Datastore.

00:36:02.289 --> 00:36:04.080
And then when I typed
it in the App Engine,

00:36:04.080 --> 00:36:06.409
it just read the
suggestions back Datastore.

00:36:06.409 --> 00:36:06.950
AUDIENCE: OK.

00:36:06.950 --> 00:36:10.310
So you basically cached
all the output in Datastore

00:36:10.310 --> 00:36:12.860
to be able to fetch it
easily from your app?

00:36:12.860 --> 00:36:15.550
REUVEN LAX: Yes.

00:36:15.550 --> 00:36:16.320
AUDIENCE: Hey.

00:36:16.320 --> 00:36:20.355
So I was wondering what
kind of delivery guarantees

00:36:20.355 --> 00:36:25.505
do you have both on the Pub/Sub
and actually on the processing

00:36:25.505 --> 00:36:27.360
pipeline.

00:36:27.360 --> 00:36:29.540
REUVEN LAX: So internal to
the processing pipeline,

00:36:29.540 --> 00:36:32.010
we have exactly once guarantees.

00:36:32.010 --> 00:36:35.720
So all the data
internal to the pipeline

00:36:35.720 --> 00:36:38.160
goes through exactly once.

00:36:38.160 --> 00:36:42.050
If you use Pub/Sub directly,
the guarantee you get

00:36:42.050 --> 00:36:43.060
is at least once.

00:36:43.060 --> 00:36:44.640
The data what will be delivered.

00:36:44.640 --> 00:36:47.623
But it may try multiple
times to get it there.

00:36:47.623 --> 00:36:48.164
AUDIENCE: OK.

00:36:48.164 --> 00:36:50.267
Thanks.

00:36:50.267 --> 00:36:51.850
AUDIENCE: Do you
guys have any numbers

00:36:51.850 --> 00:36:54.250
that you can share
latency, SLAs or anything

00:36:54.250 --> 00:36:59.124
like that for Pub/Sub and
for the Cloud Dataflow?

00:36:59.124 --> 00:37:01.290
REUVEN LAX: I don't think
we have any public numbers

00:37:01.290 --> 00:37:02.675
yet that we can share on that.

00:37:02.675 --> 00:37:03.860
MARWA MABROUK: Yeah.

00:37:03.860 --> 00:37:05.940
REUVEN LAX: General,
I would expect latency

00:37:05.940 --> 00:37:07.340
on the order of seconds.

00:37:07.340 --> 00:37:09.600
But I don't think we have
any public SLA numbers yet.

00:37:09.600 --> 00:37:09.910
MARWA MABROUK: Yeah.

00:37:09.910 --> 00:37:10.655
Not yet.

00:37:10.655 --> 00:37:12.780
AUDIENCE: And then one
other question with Pub/Sub.

00:37:12.780 --> 00:37:14.830
Traditionally the scaling
problem with Pub/Sub

00:37:14.830 --> 00:37:18.510
is if you have one producer
that many, many consumers are

00:37:18.510 --> 00:37:20.980
happening, like the
Beyonce problem on Twitter.

00:37:20.980 --> 00:37:23.620
When she tweets, you have to
get it to millions of people.

00:37:23.620 --> 00:37:26.610
What are-- do you have any
scale numbers around there

00:37:26.610 --> 00:37:29.010
like how many messages
can be duplicated

00:37:29.010 --> 00:37:31.230
to multiple consumers if
you're going that way?

00:37:31.230 --> 00:37:33.200
REUVEN LAX: So we scaled
pretty well there.

00:37:33.200 --> 00:37:37.450
So you can have many millions of
subscribers to a single topic.

00:37:37.450 --> 00:37:38.080
No problem.

00:37:38.080 --> 00:37:38.705
AUDIENCE: Cool.

00:37:41.920 --> 00:37:43.220
AUDIENCE: I have a question.

00:37:43.220 --> 00:37:48.340
Can we combine a multiple
data source in Dataflow?

00:37:48.340 --> 00:37:53.290
Like a batch and a stream
combining batch DataSource

00:37:53.290 --> 00:37:58.417
and a stream DataSource
and making one result?

00:37:58.417 --> 00:38:00.000
REUVEN LAX: So your
question is can we

00:38:00.000 --> 00:38:02.040
combine batch and
streaming in one pipeline?

00:38:02.040 --> 00:38:02.930
AUDIENCE: Yeah.

00:38:02.930 --> 00:38:06.110
REUVEN LAX: And the
answer is, it actually

00:38:06.110 --> 00:38:09.735
depends on the specific
problem you're trying to solve.

00:38:09.735 --> 00:38:11.590
Because there are
different ways to do that.

00:38:11.590 --> 00:38:12.820
In general, yes.

00:38:12.820 --> 00:38:16.362
But it depends on
the specific problem.

00:38:16.362 --> 00:38:18.570
Because that could mean
different things in different

00:38:18.570 --> 00:38:19.437
contexts.

00:38:19.437 --> 00:38:20.270
AUDIENCE: Thank you.

00:38:23.260 --> 00:38:24.780
AUDIENCE: Do we
have documentation

00:38:24.780 --> 00:38:26.630
for say running
this on local right

00:38:26.630 --> 00:38:29.629
now just to try a prototype?

00:38:29.629 --> 00:38:31.670
MARWA MABROUK: So we don't
have any documentation

00:38:31.670 --> 00:38:33.110
that is public yet.

00:38:33.110 --> 00:38:35.260
Users who joined
the private beta

00:38:35.260 --> 00:38:37.520
are able to see
the documentation.

00:38:37.520 --> 00:38:40.730
We might not have
highlighted this in the talk.

00:38:40.730 --> 00:38:44.920
But the SDK, the Java code
that Reuven was showing

00:38:44.920 --> 00:38:45.930
runs in two modes.

00:38:45.930 --> 00:38:48.350
One of them is in
local mode where

00:38:48.350 --> 00:38:51.530
you're able to execute it on
your local machine pointing

00:38:51.530 --> 00:38:54.420
to data that's accessible
from your local machine.

00:38:54.420 --> 00:38:57.170
But that's just meant for
debugging and development.

00:38:57.170 --> 00:38:59.010
Another mode is just
talking to the service

00:38:59.010 --> 00:39:01.793
and submitting your workload
back into the Cloud.

00:39:01.793 --> 00:39:04.260
AUDIENCE: Thank you.

00:39:04.260 --> 00:39:06.380
AUDIENCE: Just in
terms of data types,

00:39:06.380 --> 00:39:10.350
how well does the data pipeline,
or rather the data flow,

00:39:10.350 --> 00:39:13.650
work with spatial
information in case

00:39:13.650 --> 00:39:17.780
we explore on using this
for GIS applications?

00:39:17.780 --> 00:39:21.910
REUVEN LAX: There is
so-- we are data type

00:39:21.910 --> 00:39:24.730
agnostic in the way
that you can process

00:39:24.730 --> 00:39:26.640
spatial information yourself.

00:39:26.640 --> 00:39:30.180
We don't currently provide
any specific libraries

00:39:30.180 --> 00:39:33.320
for managing spatial
information in Dataflow

00:39:33.320 --> 00:39:34.594
if that's what you are asking.

00:39:34.594 --> 00:39:35.396
AUDIENCE: OK.

00:39:35.396 --> 00:39:35.896
Great.

00:39:35.896 --> 00:39:36.396
Thank you.

00:39:36.396 --> 00:39:38.390
REUVEN LAX: It's
pretty early as well.

00:39:38.390 --> 00:39:39.098
REUVEN LAX: Yeah.

00:39:39.098 --> 00:39:40.337
This is all pretty early.

00:39:40.337 --> 00:39:41.211
AUDIENCE: OK.

00:39:41.211 --> 00:39:41.710
Thank you.

00:39:41.710 --> 00:39:45.970
AUDIENCE: I was curious if
you had done any large joins

00:39:45.970 --> 00:39:48.765
with the data in terms
of streaming and Pub/Sub

00:39:48.765 --> 00:39:50.729
and how you manage that.

00:39:50.729 --> 00:39:52.020
REUVEN LAX: I'm glad you asked.

00:39:52.020 --> 00:39:54.790
So we actually, we didn't
demo it in this pipeline,

00:39:54.790 --> 00:39:58.860
but we do support joins
both in batch and streaming.

00:39:58.860 --> 00:40:01.730
In streaming the joins
are often temporal joins.

00:40:01.730 --> 00:40:07.260
So you want to join based off
of some buckling information.

00:40:07.260 --> 00:40:10.150
In order to do a join
you essentially need to,

00:40:10.150 --> 00:40:12.360
these key value pairs,
and you join based off

00:40:12.360 --> 00:40:16.875
of identical keys on two
collections or two streams.

00:40:21.635 --> 00:40:22.740
Any more questions?

00:40:27.430 --> 00:40:28.180
MARWA MABROUK: OK.

00:40:28.180 --> 00:40:28.360
REUVEN LAX: OK.

00:40:28.360 --> 00:40:29.630
Thank you very much for coming.

00:40:29.630 --> 00:40:30.463
MARWA MABROUK: Yeah.

00:40:30.463 --> 00:40:31.480
Thank you.

