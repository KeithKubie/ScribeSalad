WEBVTT
Kind: captions
Language: en

00:00:00.275 --> 00:00:01.350
We're really interested in solving this

00:00:01.350 --> 00:00:03.300
temporal credit assignment problem as we will

00:00:03.300 --> 00:00:06.220
see as we talk later on in the course. This sort of issue

00:00:06.220 --> 00:00:09.190
comes up again and again and again. But, I didn't really want to talk

00:00:09.190 --> 00:00:12.040
about that in any great detail, I just wanted to make certain that,

00:00:12.040 --> 00:00:13.690
that people saw the difference between

00:00:13.690 --> 00:00:15.840
supervised learning and then sort of problems

00:00:15.840 --> 00:00:19.200
we're trying to solve now. That really has to do with sequences and

00:00:19.200 --> 00:00:21.170
time. What I want to just is focus a little bit more on

00:00:21.170 --> 00:00:25.280
rewards. And in particular, I want to look at the little grid world

00:00:25.280 --> 00:00:28.710
that we've been playing with so far, and think about how

00:00:28.710 --> 00:00:32.540
we would learn how to get from our start state. We

00:00:32.540 --> 00:00:35.880
always start it over here, over to one of the plus

00:00:35.880 --> 00:00:38.550
one or the minus 1, depending upon the kind of rewards

00:00:38.550 --> 00:00:40.970
that we see. So, let's push on this rewards notion for

00:00:40.970 --> 00:00:43.180
a minute, if we think about how MDP's are made up.

00:00:43.180 --> 00:00:44.920
And I'm just going to write down for you what the

00:00:44.920 --> 00:00:47.850
reward is, for the states in this particular grid world, okay?

00:00:47.850 --> 00:00:48.748
&gt;&gt; Yeah.

00:00:48.748 --> 00:00:51.043
&gt;&gt; So, let's say

00:00:51.043 --> 00:00:55.768
I'm going to set the rewards to be equal to,

00:00:55.768 --> 00:00:58.767
minus 0.04. So, the first thing I want to do, is

00:00:58.767 --> 00:01:00.015
I want to make certain you understand what I mean

00:01:00.015 --> 00:01:01.608
by this. So, what I mean when I write this

00:01:01.608 --> 00:01:04.540
down, for this particular example. Is that the reward

00:01:04.540 --> 00:01:08.009
that I'm going to receive in every single state is minus

00:01:08.009 --> 00:01:11.550
0.04, except of course for these two states which

00:01:11.550 --> 00:01:14.470
I've already labelled as plus 1 and minus 1. Okay?

00:01:14.470 --> 00:01:16.100
&gt;&gt; Alright, so,

00:01:16.100 --> 00:01:19.570
it seems like you should just never ever move then, right?

00:01:19.570 --> 00:01:23.180
&gt;&gt; no, I don't think that's true. Let's think

00:01:23.180 --> 00:01:24.680
about that for a little while. So, if I

00:01:24.680 --> 00:01:29.280
set this reward to be minus 0.04 and you're

00:01:29.280 --> 00:01:31.620
starting out here in this state, and you have

00:01:31.620 --> 00:01:33.620
to keep living, you always have to take an

00:01:33.620 --> 00:01:35.940
action, right? That's the way I've set this up.

00:01:35.940 --> 00:01:38.900
And you don't get to stop until you reach

00:01:38.900 --> 00:01:41.220
either plus 1 or negative 1, we'll call these

00:01:41.220 --> 00:01:43.360
terminating or absorbing states. Once you reach

00:01:43.360 --> 00:01:44.480
into one of these things that ends

00:01:44.480 --> 00:01:47.530
the game. What would having a small

00:01:47.530 --> 00:01:50.310
negative reward like, this encourage you to do?

00:01:50.310 --> 00:01:56.420
&gt;&gt; It's, it's making me think about kind of walking across hot beach, because

00:01:56.420 --> 00:01:59.290
like each time you take a step you're going to get a little bit of punishment

00:01:59.290 --> 00:02:01.780
and the only thing that ends it is if you can get into that nice

00:02:01.780 --> 00:02:06.937
cool ocean. And then you get a plus 1 for that and the minus 0.04s,

00:02:06.937 --> 00:02:08.848
minus 0.04s, stop.

00:02:08.848 --> 00:02:11.610
&gt;&gt; Right, so, that means, so wait how would

00:02:11.610 --> 00:02:12.880
you put that another way. Sounds to me like

00:02:12.880 --> 00:02:15.200
what you're saying is, by having a small negative

00:02:15.200 --> 00:02:17.700
reward everywhere, it encourages you to end the game.

00:02:17.700 --> 00:02:20.990
&gt;&gt; Yeah, that's [UNKNOWN]. Yeah, I agree with that.

00:02:20.990 --> 00:02:22.917
&gt;&gt; Okay, yeah and, and I think that's exactly what I like

00:02:22.917 --> 00:02:25.660
that analogy a lot. Your in a, well, it's not a very

00:02:25.660 --> 00:02:30.210
hot beach it's just a you know, a slightly unpleasantly warm beach

00:02:30.210 --> 00:02:32.340
and you really want to hurry up and get into the ocean.

00:02:32.340 --> 00:02:33.800
In fact, you want to hurry up and get into the ocean

00:02:33.800 --> 00:02:36.820
even though on your way to getting to the ocean. You might step

00:02:36.820 --> 00:02:40.430
on some glass, that the minus 1 is. So, let's go back

00:02:40.430 --> 00:02:43.220
to this notion of policy of mapping states to action and let's think

00:02:43.220 --> 00:02:46.120
about as being in this world where you have a reward of

00:02:46.120 --> 00:02:50.640
minus 0.04 every where except in the absorbing states. What do you think

00:02:50.640 --> 00:02:54.410
the best set of actions are to take in these different states?

00:02:54.410 --> 00:02:57.620
What do you think the best policy is? Actually before you tell me,

00:02:57.620 --> 00:02:59.740
let me write down what the actual policy is. I'm not

00:02:59.740 --> 00:03:01.890
going to tell you, how we get here but I'm about to

00:03:01.890 --> 00:03:03.680
write down the policy that's sort of the best one to

00:03:03.680 --> 00:03:06.350
take in a world where you have these sets of rewards. Okay.

00:03:06.350 --> 00:03:07.040
&gt;&gt; Sure.

00:03:07.040 --> 00:03:08.310
&gt;&gt; Okay, you see this Michael.

00:03:08.310 --> 00:03:10.310
&gt;&gt; Yeah, and it makes a lot of sense. Like if

00:03:10.310 --> 00:03:13.879
you're basically heading towards the, the rewarding state, the green state.

00:03:14.880 --> 00:03:18.520
&gt;&gt; Right, so if I start out here in the leftmost bottom state, basically,

00:03:18.520 --> 00:03:22.900
it says, go up and then go to the right which, coincidentally is the policy

00:03:22.900 --> 00:03:24.130
that we had found before.

00:03:24.130 --> 00:03:27.780
&gt;&gt; Cool. There's a couple spots that are a little bit strange, though.

00:03:27.780 --> 00:03:29.770
&gt;&gt; Like what?

00:03:29.770 --> 00:03:32.330
&gt;&gt; Well, I'm thinking about the one, not directly under the minus 1,

00:03:32.330 --> 00:03:34.930
that makes sense to me, but the one, it takes you now to a

00:03:34.930 --> 00:03:39.780
position. Where it seems like you want to go straight up to the goal.

00:03:39.780 --> 00:03:42.790
But yet, it's going the long way around. It just didn't see the goal.

00:03:43.890 --> 00:03:45.600
&gt;&gt; No that's not it at all because of the way we

00:03:45.600 --> 00:03:48.470
are going to learn this, you're going to. So this is a global policy and

00:03:48.470 --> 00:03:51.260
I'm just telling you it's the optimal policy given the

00:03:51.260 --> 00:03:53.450
rewards. So let's kind of work out why going to

00:03:53.450 --> 00:03:55.900
the left makes sense here. Well, so here's my argument.

00:03:55.900 --> 00:03:59.180
What this basically says is take the long way around. Yes?

00:03:59.180 --> 00:03:59.660
&gt;&gt; Yeah?

00:03:59.660 --> 00:04:03.860
&gt;&gt; But, by taking the long way around, what's happening? Well, on

00:04:03.860 --> 00:04:07.260
the downside I'm going to pick up a little bit of negative reward,

00:04:07.260 --> 00:04:10.500
for a while, you know, one, two, three, four, five, six or

00:04:10.500 --> 00:04:12.422
so which is something like negative

00:04:12.422 --> 00:04:15.630
0.2, negative 0.3, something like that, right?

00:04:15.630 --> 00:04:18.970
But, by doing this, I avoid every getting into

00:04:18.970 --> 00:04:22.180
a state where it might fall into the negative 1.

00:04:22.180 --> 00:04:25.133
&gt;&gt; Because it's, it's stochastic.

00:04:25.133 --> 00:04:27.299
&gt;&gt; Right, it's exactly, so stochastic means

00:04:27.299 --> 00:04:28.952
if I'm in this state here no matter

00:04:28.952 --> 00:04:31.289
what I do I have som, if I go up I have some chance of

00:04:31.289 --> 00:04:33.113
moving to the right and following into

00:04:33.113 --> 00:04:36.468
the negative 1. It's a relatively low chance,

00:04:36.468 --> 00:04:40.915
it's only 10%. But, it works out that moving from here to here, the probability

00:04:40.915 --> 00:04:47.210
of me falling into here. Is to high compared to the sort of near impossibility

00:04:47.210 --> 00:04:53.110
of me ending up falling into the minus 1, if I can follow this path.

00:04:53.110 --> 00:04:55.300
&gt;&gt; Interesting, okay, I guess that makes sense, it's cool.

00:04:56.370 --> 00:04:57.800
&gt;&gt; Which actually suggests sort of the point that

00:04:57.800 --> 00:05:00.530
I want to make here which is, that minor changes to

00:05:00.530 --> 00:05:02.990
your reward function actually matter. So, if we had

00:05:02.990 --> 00:05:06.001
a slightly different reward here, say. Not minus 0.04 but

00:05:06.001 --> 00:05:08.479
something else, you might find that some of these

00:05:08.479 --> 00:05:11.830
decisions would be different. I think you can see that?

00:05:11.830 --> 00:05:16.945
&gt;&gt; Hm. I mean at the level that I'm understanding it, it seems like if

00:05:16.945 --> 00:05:21.885
it's zero, that then it's in no particular hurry. If

00:05:21.885 --> 00:05:26.170
it's minus something big, then maybe it's in a bigger hurry. But,

00:05:26.170 --> 00:05:28.530
I don't, yeah I don't see exactly what the difference is going to be.

00:05:28.530 --> 00:05:31.050
&gt;&gt; Okay, so let's see if we can help you see the difference by

00:05:31.050 --> 00:05:32.680
making you take a quiz.

00:05:32.680 --> 00:05:33.350
&gt;&gt; Aha!

