Speaker 1:          00:00          Okay.

Speaker 2:          00:00          And this video will introduce the idea of parameters sharing and convolutional neural networks.

Speaker 1:          00:05          Yes.

Speaker 2:          00:08          So we've seen before the, uh, idea of local connectivity that convolutional neural networks a leverage. And now we're ready to talk about the, uh, an additional idea of parameter sharing across hidden units.

Speaker 1:          00:23          Yeah.

Speaker 2:          00:24          All right. So this second idea that we're adding on top of local connectivity is that, uh, we will be sharing the parameters. So the way that connections, the values of the weights of the connections between the hidden units and the units and the layer below, uh, between, uh, certain units. Specifically what we'll do is that we'll take all of our, uh, hidden units in some given hidden there and we will segment the hidden units into different so-called feature maps. The units in the feature map will actually share their parameters. That is the, uh, value of the way that connection between a hidden unit and it's receptive field. Uh, it's receptive to input from the previous layer, uh, will be exactly the same. It will be the same values have the same connections. And, uh, now the hidden units within a same feature map, they will distinguish themselves by being connected at a different position.

Speaker 2:          01:26          So a different receptive fields centered at a different possession in the previous layer. So, uh, specifically, normally, uh, okay. So if we have a hidden layer with three feature maps, one, two, and three, then say for the first feature map, all of these hidden units, we'll be using the same matrix of connections. Uh, so for instance, decision unit, uh, imagine there were two pixels here than it would be, I would require a, a matrix two by two, uh, specifying the connection between a dissident unit and each of the pixels within the receptive fields. And now say this hidden unit connected with that, uh, receptive field, which to simplify things would be just two by two would use the same matrix of weighted connections to compute it's pre activation. And similarly for that in unit, also within the feature map, we'll assume that we will actually have a receptive field at each possible position, uh, within the image.

Speaker 2:          02:32          Uh, there are extensions of convolutional neural networks that are consider having skipping certain, uh, receptive fields. But just to simplify the description, always assume where, uh, connecting, uh, a feature map to all possible receptive fields within the image. And so when this feature map within this feature and when connect, uh, computing the reactivation of all hidden units, what we'll do is we'll use the same set of parameters, compute it. And so that hidden layer with multiply this weight here by December, put less this weight here by this input, does this weight here times disempowered and plus this weight here at time, this input. And then this hidden unit here would do a with compute this input here, times this weight, this input here at times this weight, uh, with at times this way and this input here times this week. So really I want to emphasize that we're using the same matrix, the same set of parameters, uh, for this, uh, this hidden unit and this hidden here. And now I'll say in the second feature map, it would be using a different matrix of parameters. So it would have its own, uh, it's own two by two matrix of parameters. And then when it's computing, uh, say this hidden Munis computing, because activation based on that receptive field, instead of multiplying the inputs by this matrix, multiply its input by that matrix instead. And then same thing for the third feature map.

Speaker 2:          03:59          So I'll use the notation Wij for refereeing to these matrices of connection. So the Matrix that connects the iff input channel in the previous layer with DJF feature map in the hidden layer will be noted as w Ij. So WIP is actually a matrix here. And uh, so this, for instance, would be w, uh, say we have just one, a one channel in the input when Dotson color, imagine it was, uh, in gray. So this would be w one, one. This matrix here would be w one, two, and then there'd be another matrix here, presumably, uh, with, uh, which would correspond to one three.

Speaker 1:          04:45          So

Speaker 2:          04:46          I didn't, parameter sharing allows us to reduce the number of parameters even more because now the different hidden units connected at different positions actually have the same set of parameters that we all need to have for each feature map to store one matrix of connections. And also it has the nice property that effectively within one feature map will be extracting the same feature, but at every position in the input. So say feature map corresponded to a it filter. If we were to visualize it that uh, looked something like a positive weights here and then some negative weights in that region. Now it would mean that we would take a, we'll be extracting that feature, which is essentially detecting whether there is an edge at this particular position within the receptive field will be detecting that feature everywhere in the input because we'll be comparing that, uh, set of, uh, that filters centrally that a set of connections with that receptive field, but also the same filter with this other receptive feeling, every receptive fields, uh, in the image. And now maybe feature map too. So, okay. So in feature map one, we extracting the actual feature, but everywhere. Whereas feature map do might be extracting say, uh, that feature. So more like a, perhaps a, uh, uh, horizontal, uh, uh, action. And that's going like this. And in feature map three could be something different like a diagonal.

Speaker 2:          06:19          So, um, to describe this property, that feature map one is extracting the same feature. Uh, it turns out we can say that the feature map before to visualize it as an image would, uh, correspond to tens formation. That is equity variant. That is if we, uh, once we, uh, say we take this input image and we apply a transformation like a translation, well in the visualization that feature map, we'll actually see exactly the same transformation. That pattern of activations will be translated based on the same transformation as the one we applied in the original input. For that reason, we'll say that, uh, the, uh, feature map will be equivalent.

Speaker 1:          07:03          Yeah.

Speaker 2:          07:04          So putting the local connectivity and the private or sharing, uh, ideas together, uh, then it means that we computing these feature maps and these feature maps, actually the computation of these feature maps, uh, actually corresponds to a computing aid discreet convolution, which is a notion taken from signal processing. This asterisk symbol is the symbol for that operation. And so computing the feature maps, which are the pre activations for the hidden layer, will correspond to, uh, doing a discrete convolution with a channel from the previous layer using a kernel matrix, which we note that Ski Ij where I is the, uh, uh, index of the channel, the previous layer, and Jay's the index of the feature map again. And that kernel is computed from the, uh, hidden weights matrix. W Ij, and specifically I'll just correspond to a, uh, Wij, but where we flipped the rows and the columns in that Matrix.

Speaker 2:          08:08          So in the next videos, I'll talk more precisely about what we mean by discreet contribution. For those that are not familiar with what it is, but, uh, just ignore that. For now, it means that we're ready to look at, uh, what would happen if we were to compute the activation of hidden layer. So, um, uh, specifically I'm using here the notation from this paper by Jared at all from Nyu. And, um, so if we call x, I d I have input channel from the previous there, Kay. I, Jay is going to be this convolution kernel, which is a function of the weight matrix and a, and so we essentially consistently information about the connections between the IOC channel of input, uh, and the GF feature. They'll also introduce, uh, these, uh, GJS here, which are learned scaling factors. And uh, in their papers they note as Yj, uh, the value of the activation, that hidden layer.

Speaker 2:          09:09          Uh, and so what they considered in that paper for how they would compute the activation of fit and layer is that they would compute the pre activations using a compilation from each of the IMF input channel to Djf a feature map, and then they would some across the, uh, input channels. So for, we compute the P activations coming from each input channel that we sum all the p activations, then you apply some nonlinearity in their paper, they chose 10 h, but we could have used something else. And they also decided to introduce this, uh, a Gj learn factor, which, uh, just in general for commercial neural network is not something that's necessary, but that's something they decided to introduce. And a that will correspond to Yj j which is the activation a corresponding to the JFF feature maps. So we will have a j here will, uh, be from one to the number of feature map to want to have in our hidden layer.

Speaker 2:          10:08          And so each why Jay is one of these feature maths, but after the nonlinearity, uh, such as the Tan h or the sigmoid, a nonlinearity or any other nonlinearity we've seen in regular people or neural networks. So are, we could have decided to introduce a bias here. Uh, so we could have added a bias. A, a, B, a j would have a separate bias in a unique bias shared by all units across the feature map. That's another variation. So if you look at the literature, there will be variations, uh, with respect to this particular equation. But the general idea remains the same. We're computing feature maps and often these feature maps are, uh, we then apply a non linearity on it, like a sigmoid or a talent or a rectified linear activation function. So in the next video, I'll talk about this discrete convolution, which is going to be, this procedure will actually involve a invoked when we're computing the feature maps. And it's actually important because there are very efficient softwares that were written for performing this convolution very efficiently. We'll be able to leverage that, uh, when we computed, we're doing for propagation and the compulsion or a neural network. And also it's important because it makes it explicit. Now, what's the convolutional a word when we're talking about what we say that these networks are convolutional.