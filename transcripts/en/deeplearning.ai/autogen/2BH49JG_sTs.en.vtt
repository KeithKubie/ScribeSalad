WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:02.570
 
estimating the bias and variance of your

00:00:02.570 --> 00:00:02.580
estimating the bias and variance of your
 

00:00:02.580 --> 00:00:04.280
estimating the bias and variance of your
learning algorithm really helps you

00:00:04.280 --> 00:00:04.290
learning algorithm really helps you
 

00:00:04.290 --> 00:00:06.710
learning algorithm really helps you
prioritize what to work on Nick's but

00:00:06.710 --> 00:00:06.720
prioritize what to work on Nick's but
 

00:00:06.720 --> 00:00:08.600
prioritize what to work on Nick's but
the way you analyze bison variance

00:00:08.600 --> 00:00:08.610
the way you analyze bison variance
 

00:00:08.610 --> 00:00:11.299
the way you analyze bison variance
changes when your training set comes

00:00:11.299 --> 00:00:11.309
changes when your training set comes
 

00:00:11.309 --> 00:00:12.890
changes when your training set comes
from a different distribution than your

00:00:12.890 --> 00:00:12.900
from a different distribution than your
 

00:00:12.900 --> 00:00:16.580
from a different distribution than your
dev and test set let's see how let's

00:00:16.580 --> 00:00:16.590
dev and test set let's see how let's
 

00:00:16.590 --> 00:00:18.500
dev and test set let's see how let's
keep using our cat classification

00:00:18.500 --> 00:00:18.510
keep using our cat classification
 

00:00:18.510 --> 00:00:21.200
keep using our cat classification
example and let's say humans get near

00:00:21.200 --> 00:00:21.210
example and let's say humans get near
 

00:00:21.210 --> 00:00:23.689
example and let's say humans get near
perfect performance on this so Bayes

00:00:23.689 --> 00:00:23.699
perfect performance on this so Bayes
 

00:00:23.699 --> 00:00:25.880
perfect performance on this so Bayes
error based on so error we know is

00:00:25.880 --> 00:00:25.890
error based on so error we know is
 

00:00:25.890 --> 00:00:28.759
error based on so error we know is
nearly zero percent on this problem so

00:00:28.759 --> 00:00:28.769
nearly zero percent on this problem so
 

00:00:28.769 --> 00:00:31.310
nearly zero percent on this problem so
to carry out error analysis you usually

00:00:31.310 --> 00:00:31.320
to carry out error analysis you usually
 

00:00:31.320 --> 00:00:34.130
to carry out error analysis you usually
look at the training error and also look

00:00:34.130 --> 00:00:34.140
look at the training error and also look
 

00:00:34.140 --> 00:00:37.580
look at the training error and also look
at the error on the DEF set so let's say

00:00:37.580 --> 00:00:37.590
at the error on the DEF set so let's say
 

00:00:37.590 --> 00:00:40.100
at the error on the DEF set so let's say
in this example that your training error

00:00:40.100 --> 00:00:40.110
in this example that your training error
 

00:00:40.110 --> 00:00:44.299
in this example that your training error
is one percent and your def error is ten

00:00:44.299 --> 00:00:44.309
is one percent and your def error is ten
 

00:00:44.309 --> 00:00:45.110
is one percent and your def error is ten
percent

00:00:45.110 --> 00:00:45.120
percent
 

00:00:45.120 --> 00:00:48.200
percent
if your dev data came from the same

00:00:48.200 --> 00:00:48.210
if your dev data came from the same
 

00:00:48.210 --> 00:00:49.880
if your dev data came from the same
distribution as your training set you

00:00:49.880 --> 00:00:49.890
distribution as your training set you
 

00:00:49.890 --> 00:00:51.979
distribution as your training set you
will say that here you have a large

00:00:51.979 --> 00:00:51.989
will say that here you have a large
 

00:00:51.989 --> 00:00:54.020
will say that here you have a large
variance problem that your album is just

00:00:54.020 --> 00:00:54.030
variance problem that your album is just
 

00:00:54.030 --> 00:00:56.240
variance problem that your album is just
not generalizing well from the training

00:00:56.240 --> 00:00:56.250
not generalizing well from the training
 

00:00:56.250 --> 00:00:58.400
not generalizing well from the training
set which is doing well on to the DEF

00:00:58.400 --> 00:00:58.410
set which is doing well on to the DEF
 

00:00:58.410 --> 00:01:00.080
set which is doing well on to the DEF
set which is suddenly doing much worse

00:01:00.080 --> 00:01:00.090
set which is suddenly doing much worse
 

00:01:00.090 --> 00:01:02.869
set which is suddenly doing much worse
on but in the setting where your

00:01:02.869 --> 00:01:02.879
on but in the setting where your
 

00:01:02.879 --> 00:01:05.119
on but in the setting where your
training data and your dev data comes

00:01:05.119 --> 00:01:05.129
training data and your dev data comes
 

00:01:05.129 --> 00:01:07.070
training data and your dev data comes
from different distribution you can no

00:01:07.070 --> 00:01:07.080
from different distribution you can no
 

00:01:07.080 --> 00:01:09.320
from different distribution you can no
longer safely draw this conclusion in

00:01:09.320 --> 00:01:09.330
longer safely draw this conclusion in
 

00:01:09.330 --> 00:01:12.410
longer safely draw this conclusion in
particular maybe it's doing just fine on

00:01:12.410 --> 00:01:12.420
particular maybe it's doing just fine on
 

00:01:12.420 --> 00:01:14.780
particular maybe it's doing just fine on
the dev set it's just that the training

00:01:14.780 --> 00:01:14.790
the dev set it's just that the training
 

00:01:14.790 --> 00:01:17.060
the dev set it's just that the training
set was really easy because it was

00:01:17.060 --> 00:01:17.070
set was really easy because it was
 

00:01:17.070 --> 00:01:20.240
set was really easy because it was
high-res very clear images and maybe the

00:01:20.240 --> 00:01:20.250
high-res very clear images and maybe the
 

00:01:20.250 --> 00:01:23.780
high-res very clear images and maybe the
DEF set is just much harder so maybe

00:01:23.780 --> 00:01:23.790
DEF set is just much harder so maybe
 

00:01:23.790 --> 00:01:26.240
DEF set is just much harder so maybe
there isn't a variance problem and this

00:01:26.240 --> 00:01:26.250
there isn't a variance problem and this
 

00:01:26.250 --> 00:01:28.640
there isn't a variance problem and this
just affects that the dev site contains

00:01:28.640 --> 00:01:28.650
just affects that the dev site contains
 

00:01:28.650 --> 00:01:30.440
just affects that the dev site contains
images that are much more difficult to

00:01:30.440 --> 00:01:30.450
images that are much more difficult to
 

00:01:30.450 --> 00:01:34.520
images that are much more difficult to
classify accurately so the problem with

00:01:34.520 --> 00:01:34.530
classify accurately so the problem with
 

00:01:34.530 --> 00:01:36.740
classify accurately so the problem with
this analysis is that when you went from

00:01:36.740 --> 00:01:36.750
this analysis is that when you went from
 

00:01:36.750 --> 00:01:38.810
this analysis is that when you went from
the training error to the depth error

00:01:38.810 --> 00:01:38.820
the training error to the depth error
 

00:01:38.820 --> 00:01:42.710
the training error to the depth error
two things changed at a time one is that

00:01:42.710 --> 00:01:42.720
two things changed at a time one is that
 

00:01:42.720 --> 00:01:45.109
two things changed at a time one is that
the algorithm saw data in the training

00:01:45.109 --> 00:01:45.119
the algorithm saw data in the training
 

00:01:45.119 --> 00:01:47.899
the algorithm saw data in the training
set but not in the dev set to the

00:01:47.899 --> 00:01:47.909
set but not in the dev set to the
 

00:01:47.909 --> 00:01:50.210
set but not in the dev set to the
distribution of data in your dev set is

00:01:50.210 --> 00:01:50.220
distribution of data in your dev set is
 

00:01:50.220 --> 00:01:52.039
distribution of data in your dev set is
different and because you changed two

00:01:52.039 --> 00:01:52.049
different and because you changed two
 

00:01:52.049 --> 00:01:54.020
different and because you changed two
things at the same time it's difficult

00:01:54.020 --> 00:01:54.030
things at the same time it's difficult
 

00:01:54.030 --> 00:01:56.569
things at the same time it's difficult
to know of this nine percent increase in

00:01:56.569 --> 00:01:56.579
to know of this nine percent increase in
 

00:01:56.579 --> 00:01:59.060
to know of this nine percent increase in
error how much of it is because the

00:01:59.060 --> 00:01:59.070
error how much of it is because the
 

00:01:59.070 --> 00:02:01.069
error how much of it is because the
algorithm didn't see the data in the dev

00:02:01.069 --> 00:02:01.079
algorithm didn't see the data in the dev
 

00:02:01.079 --> 00:02:03.469
algorithm didn't see the data in the dev
set so that's sort of the variance part

00:02:03.469 --> 00:02:03.479
set so that's sort of the variance part
 

00:02:03.479 --> 00:02:05.330
set so that's sort of the variance part
of the problem and how much of it is

00:02:05.330 --> 00:02:05.340
of the problem and how much of it is
 

00:02:05.340 --> 00:02:07.219
of the problem and how much of it is
because the deficit theater is just

00:02:07.219 --> 00:02:07.229
because the deficit theater is just
 

00:02:07.229 --> 00:02:08.430
because the deficit theater is just
different

00:02:08.430 --> 00:02:08.440
different
 

00:02:08.440 --> 00:02:11.560
different
so in order to tease out these two

00:02:11.560 --> 00:02:11.570
so in order to tease out these two
 

00:02:11.570 --> 00:02:15.160
so in order to tease out these two
effects and if you didn't totally follow

00:02:15.160 --> 00:02:15.170
effects and if you didn't totally follow
 

00:02:15.170 --> 00:02:16.720
effects and if you didn't totally follow
what these two different effects are

00:02:16.720 --> 00:02:16.730
what these two different effects are
 

00:02:16.730 --> 00:02:18.520
what these two different effects are
don't worry we'll go over it again in a

00:02:18.520 --> 00:02:18.530
don't worry we'll go over it again in a
 

00:02:18.530 --> 00:02:20.680
don't worry we'll go over it again in a
second but in order to tease out these

00:02:20.680 --> 00:02:20.690
second but in order to tease out these
 

00:02:20.690 --> 00:02:22.570
second but in order to tease out these
two effects it will be useful to define

00:02:22.570 --> 00:02:22.580
two effects it will be useful to define
 

00:02:22.580 --> 00:02:25.210
two effects it will be useful to define
a new piece of data which we'll call the

00:02:25.210 --> 00:02:25.220
a new piece of data which we'll call the
 

00:02:25.220 --> 00:02:28.150
a new piece of data which we'll call the
training deaf set so this is a new

00:02:28.150 --> 00:02:28.160
training deaf set so this is a new
 

00:02:28.160 --> 00:02:30.460
training deaf set so this is a new
subset of data which you carve out that

00:02:30.460 --> 00:02:30.470
subset of data which you carve out that
 

00:02:30.470 --> 00:02:32.830
subset of data which you carve out that
should have the same distribution as

00:02:32.830 --> 00:02:32.840
should have the same distribution as
 

00:02:32.840 --> 00:02:34.870
should have the same distribution as
training sets but you don't explicitly

00:02:34.870 --> 00:02:34.880
training sets but you don't explicitly
 

00:02:34.880 --> 00:02:38.080
training sets but you don't explicitly
train a neural network on this so here's

00:02:38.080 --> 00:02:38.090
train a neural network on this so here's
 

00:02:38.090 --> 00:02:42.000
train a neural network on this so here's
what I mean previously we had set up

00:02:42.000 --> 00:02:42.010
what I mean previously we had set up
 

00:02:42.010 --> 00:02:47.070
what I mean previously we had set up
some training set and some dev set and

00:02:47.070 --> 00:02:47.080
some training set and some dev set and
 

00:02:47.080 --> 00:02:51.550
some training set and some dev set and
some test set as follows and the dev and

00:02:51.550 --> 00:02:51.560
some test set as follows and the dev and
 

00:02:51.560 --> 00:02:53.260
some test set as follows and the dev and
test sets have the same distribution but

00:02:53.260 --> 00:02:53.270
test sets have the same distribution but
 

00:02:53.270 --> 00:02:54.940
test sets have the same distribution but
the training set will have some

00:02:54.940 --> 00:02:54.950
the training set will have some
 

00:02:54.950 --> 00:02:57.070
the training set will have some
different distribution what we're going

00:02:57.070 --> 00:02:57.080
different distribution what we're going
 

00:02:57.080 --> 00:02:59.650
different distribution what we're going
to do is randomly shuffle the training

00:02:59.650 --> 00:02:59.660
to do is randomly shuffle the training
 

00:02:59.660 --> 00:03:02.170
to do is randomly shuffle the training
set and then carve out just a piece of

00:03:02.170 --> 00:03:02.180
set and then carve out just a piece of
 

00:03:02.180 --> 00:03:06.750
set and then carve out just a piece of
the training set to be the training -

00:03:06.750 --> 00:03:06.760
the training set to be the training -
 

00:03:06.760 --> 00:03:11.500
the training set to be the training -
dev set so just as the dev and test set

00:03:11.500 --> 00:03:11.510
dev set so just as the dev and test set
 

00:03:11.510 --> 00:03:13.870
dev set so just as the dev and test set
has the same distribution the training

00:03:13.870 --> 00:03:13.880
has the same distribution the training
 

00:03:13.880 --> 00:03:17.620
has the same distribution the training
set and the training dev set also have

00:03:17.620 --> 00:03:17.630
set and the training dev set also have
 

00:03:17.630 --> 00:03:21.970
set and the training dev set also have
the same distribution but the difference

00:03:21.970 --> 00:03:21.980
the same distribution but the difference
 

00:03:21.980 --> 00:03:24.310
the same distribution but the difference
is that now you train your new network

00:03:24.310 --> 00:03:24.320
is that now you train your new network
 

00:03:24.320 --> 00:03:27.970
is that now you train your new network
just on the training set proper you

00:03:27.970 --> 00:03:27.980
just on the training set proper you
 

00:03:27.980 --> 00:03:29.560
just on the training set proper you
won't let the neural network you won't

00:03:29.560 --> 00:03:29.570
won't let the neural network you won't
 

00:03:29.570 --> 00:03:31.840
won't let the neural network you won't
run back propagation on the training

00:03:31.840 --> 00:03:31.850
run back propagation on the training
 

00:03:31.850 --> 00:03:35.170
run back propagation on the training
deaf portion of this data to carry out

00:03:35.170 --> 00:03:35.180
deaf portion of this data to carry out
 

00:03:35.180 --> 00:03:37.270
deaf portion of this data to carry out
error analysis what you should do is now

00:03:37.270 --> 00:03:37.280
error analysis what you should do is now
 

00:03:37.280 --> 00:03:39.400
error analysis what you should do is now
look at the area of a classifier on the

00:03:39.400 --> 00:03:39.410
look at the area of a classifier on the
 

00:03:39.410 --> 00:03:41.710
look at the area of a classifier on the
training set on the training dev set as

00:03:41.710 --> 00:03:41.720
training set on the training dev set as
 

00:03:41.720 --> 00:03:45.160
training set on the training dev set as
well as on the deficit so let's say in

00:03:45.160 --> 00:03:45.170
well as on the deficit so let's say in
 

00:03:45.170 --> 00:03:49.110
well as on the deficit so let's say in
this example that your training error is

00:03:49.110 --> 00:03:49.120
this example that your training error is
 

00:03:49.120 --> 00:03:55.060
this example that your training error is
1% and let's say the error on the

00:03:55.060 --> 00:03:55.070
1% and let's say the error on the
 

00:03:55.070 --> 00:04:02.170
1% and let's say the error on the
training dev set is 9% and the error on

00:04:02.170 --> 00:04:02.180
training dev set is 9% and the error on
 

00:04:02.180 --> 00:04:06.420
training dev set is 9% and the error on
the dev set is 10%

00:04:06.420 --> 00:04:06.430
the dev set is 10%
 

00:04:06.430 --> 00:04:09.520
the dev set is 10%
same as before what you can conclude

00:04:09.520 --> 00:04:09.530
same as before what you can conclude
 

00:04:09.530 --> 00:04:12.600
same as before what you can conclude
from this is that when you went from

00:04:12.600 --> 00:04:12.610
from this is that when you went from
 

00:04:12.610 --> 00:04:15.640
from this is that when you went from
training data to training dev data the

00:04:15.640 --> 00:04:15.650
training data to training dev data the
 

00:04:15.650 --> 00:04:17.830
training data to training dev data the
error really went up a lot and the only

00:04:17.830 --> 00:04:17.840
error really went up a lot and the only
 

00:04:17.840 --> 00:04:21.039
error really went up a lot and the only
difference between the training data

00:04:21.039 --> 00:04:21.049
difference between the training data
 

00:04:21.049 --> 00:04:23.740
difference between the training data
and the training deaf data is that your

00:04:23.740 --> 00:04:23.750
and the training deaf data is that your
 

00:04:23.750 --> 00:04:26.499
and the training deaf data is that your
neural network got to saw the first part

00:04:26.499 --> 00:04:26.509
neural network got to saw the first part
 

00:04:26.509 --> 00:04:27.129
neural network got to saw the first part
of this

00:04:27.129 --> 00:04:27.139
of this
 

00:04:27.139 --> 00:04:30.640
of this
it was trained explicitly on this but it

00:04:30.640 --> 00:04:30.650
it was trained explicitly on this but it
 

00:04:30.650 --> 00:04:32.020
it was trained explicitly on this but it
wasn't trained explicitly on the

00:04:32.020 --> 00:04:32.030
wasn't trained explicitly on the
 

00:04:32.030 --> 00:04:35.649
wasn't trained explicitly on the
training Jeff data yeah so this tells

00:04:35.649 --> 00:04:35.659
training Jeff data yeah so this tells
 

00:04:35.659 --> 00:04:39.210
training Jeff data yeah so this tells
you that you have a variance problem

00:04:39.210 --> 00:04:39.220
you that you have a variance problem
 

00:04:39.220 --> 00:04:42.430
you that you have a variance problem
because the training Deborah was

00:04:42.430 --> 00:04:42.440
because the training Deborah was
 

00:04:42.440 --> 00:04:43.960
because the training Deborah was
measured on data that comes from the

00:04:43.960 --> 00:04:43.970
measured on data that comes from the
 

00:04:43.970 --> 00:04:45.670
measured on data that comes from the
same distribution as your training set

00:04:45.670 --> 00:04:45.680
same distribution as your training set
 

00:04:45.680 --> 00:04:48.100
same distribution as your training set
so you know that even though your neural

00:04:48.100 --> 00:04:48.110
so you know that even though your neural
 

00:04:48.110 --> 00:04:50.469
so you know that even though your neural
network does well in the training set is

00:04:50.469 --> 00:04:50.479
network does well in the training set is
 

00:04:50.479 --> 00:04:53.589
network does well in the training set is
just not generalizing well to data in

00:04:53.589 --> 00:04:53.599
just not generalizing well to data in
 

00:04:53.599 --> 00:04:55.899
just not generalizing well to data in
the training def set which comes from

00:04:55.899 --> 00:04:55.909
the training def set which comes from
 

00:04:55.909 --> 00:04:58.149
the training def set which comes from
the same distribution but it's just not

00:04:58.149 --> 00:04:58.159
the same distribution but it's just not
 

00:04:58.159 --> 00:05:00.189
the same distribution but it's just not
generalizing well to data from the same

00:05:00.189 --> 00:05:00.199
generalizing well to data from the same
 

00:05:00.199 --> 00:05:03.119
generalizing well to data from the same
distribution that it hasn't seen before

00:05:03.119 --> 00:05:03.129
distribution that it hasn't seen before
 

00:05:03.129 --> 00:05:06.159
distribution that it hasn't seen before
so in this example you have really a

00:05:06.159 --> 00:05:06.169
so in this example you have really a
 

00:05:06.169 --> 00:05:10.119
so in this example you have really a
variance problem let's look at a

00:05:10.119 --> 00:05:10.129
variance problem let's look at a
 

00:05:10.129 --> 00:05:12.129
variance problem let's look at a
different example let's say the training

00:05:12.129 --> 00:05:12.139
different example let's say the training
 

00:05:12.139 --> 00:05:14.860
different example let's say the training
error is 1% and the training dev error

00:05:14.860 --> 00:05:14.870
error is 1% and the training dev error
 

00:05:14.870 --> 00:05:18.610
error is 1% and the training dev error
is 1.5 percent but when you go to the

00:05:18.610 --> 00:05:18.620
is 1.5 percent but when you go to the
 

00:05:18.620 --> 00:05:22.270
is 1.5 percent but when you go to the
deaths that your error is 10% so now you

00:05:22.270 --> 00:05:22.280
deaths that your error is 10% so now you
 

00:05:22.280 --> 00:05:24.879
deaths that your error is 10% so now you
have actually a pretty low variance

00:05:24.879 --> 00:05:24.889
have actually a pretty low variance
 

00:05:24.889 --> 00:05:26.769
have actually a pretty low variance
problem because when you went from

00:05:26.769 --> 00:05:26.779
problem because when you went from
 

00:05:26.779 --> 00:05:28.870
problem because when you went from
training data that you've seen to the

00:05:28.870 --> 00:05:28.880
training data that you've seen to the
 

00:05:28.880 --> 00:05:30.939
training data that you've seen to the
training def data that you that the

00:05:30.939 --> 00:05:30.949
training def data that you that the
 

00:05:30.949 --> 00:05:32.529
training def data that you that the
neural network has not seen the error

00:05:32.529 --> 00:05:32.539
neural network has not seen the error
 

00:05:32.539 --> 00:05:34.870
neural network has not seen the error
increases only a little bit but then it

00:05:34.870 --> 00:05:34.880
increases only a little bit but then it
 

00:05:34.880 --> 00:05:37.300
increases only a little bit but then it
really jumps when you go to the DEF set

00:05:37.300 --> 00:05:37.310
really jumps when you go to the DEF set
 

00:05:37.310 --> 00:05:41.110
really jumps when you go to the DEF set
so this is a data mismatch problem right

00:05:41.110 --> 00:05:41.120
so this is a data mismatch problem right
 

00:05:41.120 --> 00:05:41.730
so this is a data mismatch problem right
data

00:05:41.730 --> 00:05:41.740
data
 

00:05:41.740 --> 00:05:50.170
data
mismatch so this is a data mismatch

00:05:50.170 --> 00:05:50.180
mismatch so this is a data mismatch
 

00:05:50.180 --> 00:05:53.499
mismatch so this is a data mismatch
problem because your learning algorithm

00:05:53.499 --> 00:05:53.509
problem because your learning algorithm
 

00:05:53.509 --> 00:05:55.719
problem because your learning algorithm
was not trained explicitly on data from

00:05:55.719 --> 00:05:55.729
was not trained explicitly on data from
 

00:05:55.729 --> 00:05:58.570
was not trained explicitly on data from
training death or death but these two

00:05:58.570 --> 00:05:58.580
training death or death but these two
 

00:05:58.580 --> 00:05:59.740
training death or death but these two
data sets come from different

00:05:59.740 --> 00:05:59.750
data sets come from different
 

00:05:59.750 --> 00:06:01.510
data sets come from different
distributions but whatever algorithm is

00:06:01.510 --> 00:06:01.520
distributions but whatever algorithm is
 

00:06:01.520 --> 00:06:03.279
distributions but whatever algorithm is
learning it works great on training

00:06:03.279 --> 00:06:03.289
learning it works great on training
 

00:06:03.289 --> 00:06:05.800
learning it works great on training
death but it doesn't work well on death

00:06:05.800 --> 00:06:05.810
death but it doesn't work well on death
 

00:06:05.810 --> 00:06:09.010
death but it doesn't work well on death
so somehow your algorithm has learned to

00:06:09.010 --> 00:06:09.020
so somehow your algorithm has learned to
 

00:06:09.020 --> 00:06:11.230
so somehow your algorithm has learned to
do well on a different distribution than

00:06:11.230 --> 00:06:11.240
do well on a different distribution than
 

00:06:11.240 --> 00:06:12.879
do well on a different distribution than
what you really care about some call it

00:06:12.879 --> 00:06:12.889
what you really care about some call it
 

00:06:12.889 --> 00:06:18.020
what you really care about some call it
a data mismatch problem listen

00:06:18.020 --> 00:06:18.030
a data mismatch problem listen
 

00:06:18.030 --> 00:06:20.720
a data mismatch problem listen
look at a few more examples on rectus on

00:06:20.720 --> 00:06:20.730
look at a few more examples on rectus on
 

00:06:20.730 --> 00:06:23.540
look at a few more examples on rectus on
the next row since I'm running out of

00:06:23.540 --> 00:06:23.550
the next row since I'm running out of
 

00:06:23.550 --> 00:06:25.720
the next row since I'm running out of
space on top so your training era

00:06:25.720 --> 00:06:25.730
space on top so your training era
 

00:06:25.730 --> 00:06:33.980
space on top so your training era
training death era and death era let's

00:06:33.980 --> 00:06:33.990
training death era and death era let's
 

00:06:33.990 --> 00:06:37.010
training death era and death era let's
say that trading era is 10% training

00:06:37.010 --> 00:06:37.020
say that trading era is 10% training
 

00:06:37.020 --> 00:06:41.620
say that trading era is 10% training
deaf era is 11% and deaf era is 12%

00:06:41.620 --> 00:06:41.630
deaf era is 11% and deaf era is 12%
 

00:06:41.630 --> 00:06:45.380
deaf era is 11% and deaf era is 12%
remember that human level proxy for

00:06:45.380 --> 00:06:45.390
remember that human level proxy for
 

00:06:45.390 --> 00:06:51.470
remember that human level proxy for
Bay's era is roughly 0% so if you have

00:06:51.470 --> 00:06:51.480
Bay's era is roughly 0% so if you have
 

00:06:51.480 --> 00:06:54.950
Bay's era is roughly 0% so if you have
this type of performance then you really

00:06:54.950 --> 00:06:54.960
this type of performance then you really
 

00:06:54.960 --> 00:06:58.330
this type of performance then you really
have a bias or an avoidable bias problem

00:06:58.330 --> 00:06:58.340
have a bias or an avoidable bias problem
 

00:06:58.340 --> 00:07:01.250
have a bias or an avoidable bias problem
because you're doing much worse than

00:07:01.250 --> 00:07:01.260
because you're doing much worse than
 

00:07:01.260 --> 00:07:04.610
because you're doing much worse than
human level so this is really a high

00:07:04.610 --> 00:07:04.620
human level so this is really a high
 

00:07:04.620 --> 00:07:08.780
human level so this is really a high
bias setting and one last example if

00:07:08.780 --> 00:07:08.790
bias setting and one last example if
 

00:07:08.790 --> 00:07:12.200
bias setting and one last example if
your training error is 10% your training

00:07:12.200 --> 00:07:12.210
your training error is 10% your training
 

00:07:12.210 --> 00:07:15.980
your training error is 10% your training
dev error is 11% and your Jeff error is

00:07:15.980 --> 00:07:15.990
dev error is 11% and your Jeff error is
 

00:07:15.990 --> 00:07:19.100
dev error is 11% and your Jeff error is
20% then it looks like this actually has

00:07:19.100 --> 00:07:19.110
20% then it looks like this actually has
 

00:07:19.110 --> 00:07:22.820
20% then it looks like this actually has
two issues one the avoidable bias is

00:07:22.820 --> 00:07:22.830
two issues one the avoidable bias is
 

00:07:22.830 --> 00:07:25.160
two issues one the avoidable bias is
quite high because you're not even doing

00:07:25.160 --> 00:07:25.170
quite high because you're not even doing
 

00:07:25.170 --> 00:07:27.200
quite high because you're not even doing
that well on the training side humans

00:07:27.200 --> 00:07:27.210
that well on the training side humans
 

00:07:27.210 --> 00:07:29.600
that well on the training side humans
get nearly zero percent error but you're

00:07:29.600 --> 00:07:29.610
get nearly zero percent error but you're
 

00:07:29.610 --> 00:07:30.470
get nearly zero percent error but you're
getting 10 percent error on your

00:07:30.470 --> 00:07:30.480
getting 10 percent error on your
 

00:07:30.480 --> 00:07:35.810
getting 10 percent error on your
training set the variance here seems

00:07:35.810 --> 00:07:35.820
training set the variance here seems
 

00:07:35.820 --> 00:07:41.900
training set the variance here seems
quite small but this data mismatch is

00:07:41.900 --> 00:07:41.910
quite small but this data mismatch is
 

00:07:41.910 --> 00:07:42.970
quite small but this data mismatch is
quite large

00:07:42.970 --> 00:07:42.980
quite large
 

00:07:42.980 --> 00:07:45.980
quite large
so for this example I will say you have

00:07:45.980 --> 00:07:45.990
so for this example I will say you have
 

00:07:45.990 --> 00:07:48.740
so for this example I will say you have
a large bias or a voidable bias problem

00:07:48.740 --> 00:07:48.750
a large bias or a voidable bias problem
 

00:07:48.750 --> 00:07:56.750
a large bias or a voidable bias problem
as well as a data mismatch problem so

00:07:56.750 --> 00:07:56.760
as well as a data mismatch problem so
 

00:07:56.760 --> 00:07:58.790
as well as a data mismatch problem so
let's take what we've done on this slide

00:07:58.790 --> 00:07:58.800
let's take what we've done on this slide
 

00:07:58.800 --> 00:08:02.840
let's take what we've done on this slide
and write out the general principles the

00:08:02.840 --> 00:08:02.850
and write out the general principles the
 

00:08:02.850 --> 00:08:07.400
and write out the general principles the
key quantities I will look at are human

00:08:07.400 --> 00:08:07.410
key quantities I will look at are human
 

00:08:07.410 --> 00:08:15.770
key quantities I will look at are human
level error your training set error your

00:08:15.770 --> 00:08:15.780
level error your training set error your
 

00:08:15.780 --> 00:08:18.360
level error your training set error your
training

00:08:18.360 --> 00:08:18.370
training
 

00:08:18.370 --> 00:08:22.920
training
deaf cetera so that same distribution as

00:08:22.920 --> 00:08:22.930
deaf cetera so that same distribution as
 

00:08:22.930 --> 00:08:24.330
deaf cetera so that same distribution as
the training set but you didn't train

00:08:24.330 --> 00:08:24.340
the training set but you didn't train
 

00:08:24.340 --> 00:08:26.850
the training set but you didn't train
explicitly on it you're deaf sets era

00:08:26.850 --> 00:08:26.860
explicitly on it you're deaf sets era
 

00:08:26.860 --> 00:08:30.600
explicitly on it you're deaf sets era
and depending on these the differences

00:08:30.600 --> 00:08:30.610
and depending on these the differences
 

00:08:30.610 --> 00:08:32.310
and depending on these the differences
between these errors you can get a sense

00:08:32.310 --> 00:08:32.320
between these errors you can get a sense
 

00:08:32.320 --> 00:08:34.440
between these errors you can get a sense
of how big lists the avoidable bias the

00:08:34.440 --> 00:08:34.450
of how big lists the avoidable bias the
 

00:08:34.450 --> 00:08:38.909
of how big lists the avoidable bias the
viren's the data mismatch problems so

00:08:38.909 --> 00:08:38.919
viren's the data mismatch problems so
 

00:08:38.919 --> 00:08:40.860
viren's the data mismatch problems so
let's say that human level error is four

00:08:40.860 --> 00:08:40.870
let's say that human level error is four
 

00:08:40.870 --> 00:08:43.320
let's say that human level error is four
percent your training error is seven

00:08:43.320 --> 00:08:43.330
percent your training error is seven
 

00:08:43.330 --> 00:08:45.660
percent your training error is seven
percent your training deaf error is ten

00:08:45.660 --> 00:08:45.670
percent your training deaf error is ten
 

00:08:45.670 --> 00:08:48.570
percent your training deaf error is ten
percent and your dev error is twelve

00:08:48.570 --> 00:08:48.580
percent and your dev error is twelve
 

00:08:48.580 --> 00:08:52.250
percent and your dev error is twelve
percent so this gives you a sense of the

00:08:52.250 --> 00:08:52.260
percent so this gives you a sense of the
 

00:08:52.260 --> 00:08:55.890
percent so this gives you a sense of the
avoidable bias because you know you like

00:08:55.890 --> 00:08:55.900
avoidable bias because you know you like
 

00:08:55.900 --> 00:08:57.480
avoidable bias because you know you like
your algorithm to do and these as well

00:08:57.480 --> 00:08:57.490
your algorithm to do and these as well
 

00:08:57.490 --> 00:08:58.740
your algorithm to do and these as well
will approach human level performance

00:08:58.740 --> 00:08:58.750
will approach human level performance
 

00:08:58.750 --> 00:09:02.370
will approach human level performance
maybe on the training set this is a

00:09:02.370 --> 00:09:02.380
maybe on the training set this is a
 

00:09:02.380 --> 00:09:05.240
maybe on the training set this is a
sense of the variance so how well do you

00:09:05.240 --> 00:09:05.250
sense of the variance so how well do you
 

00:09:05.250 --> 00:09:07.590
sense of the variance so how well do you
generalize from the training set to the

00:09:07.590 --> 00:09:07.600
generalize from the training set to the
 

00:09:07.600 --> 00:09:12.060
generalize from the training set to the
training depth sent this is a sense of

00:09:12.060 --> 00:09:12.070
training depth sent this is a sense of
 

00:09:12.070 --> 00:09:14.460
training depth sent this is a sense of
how much of a data mismatch problem you

00:09:14.460 --> 00:09:14.470
how much of a data mismatch problem you
 

00:09:14.470 --> 00:09:17.340
how much of a data mismatch problem you
have and technically you could also add

00:09:17.340 --> 00:09:17.350
have and technically you could also add
 

00:09:17.350 --> 00:09:19.200
have and technically you could also add
one more thing which is the test set

00:09:19.200 --> 00:09:19.210
one more thing which is the test set
 

00:09:19.210 --> 00:09:21.510
one more thing which is the test set
performance and we write test error you

00:09:21.510 --> 00:09:21.520
performance and we write test error you
 

00:09:21.520 --> 00:09:23.040
performance and we write test error you
shouldn't be doing development on your

00:09:23.040 --> 00:09:23.050
shouldn't be doing development on your
 

00:09:23.050 --> 00:09:24.570
shouldn't be doing development on your
test set because you don't want to over

00:09:24.570 --> 00:09:24.580
test set because you don't want to over
 

00:09:24.580 --> 00:09:26.190
test set because you don't want to over
fit your test set but if you also look

00:09:26.190 --> 00:09:26.200
fit your test set but if you also look
 

00:09:26.200 --> 00:09:30.000
fit your test set but if you also look
at this then this gap here tells you the

00:09:30.000 --> 00:09:30.010
at this then this gap here tells you the
 

00:09:30.010 --> 00:09:37.519
at this then this gap here tells you the
on degree of overfitting to the dev set

00:09:37.519 --> 00:09:37.529
on degree of overfitting to the dev set
 

00:09:37.529 --> 00:09:40.230
on degree of overfitting to the dev set
so if there's a huge gap between your

00:09:40.230 --> 00:09:40.240
so if there's a huge gap between your
 

00:09:40.240 --> 00:09:41.880
so if there's a huge gap between your
def set performance and your test set

00:09:41.880 --> 00:09:41.890
def set performance and your test set
 

00:09:41.890 --> 00:09:43.680
def set performance and your test set
performance it means you may be over

00:09:43.680 --> 00:09:43.690
performance it means you may be over
 

00:09:43.690 --> 00:09:47.250
performance it means you may be over
tuned to the DEF set room and so maybe

00:09:47.250 --> 00:09:47.260
tuned to the DEF set room and so maybe
 

00:09:47.260 --> 00:09:49.260
tuned to the DEF set room and so maybe
you need to find a bigger deficit all

00:09:49.260 --> 00:09:49.270
you need to find a bigger deficit all
 

00:09:49.270 --> 00:09:50.940
you need to find a bigger deficit all
right so remember that your deficits in

00:09:50.940 --> 00:09:50.950
right so remember that your deficits in
 

00:09:50.950 --> 00:09:52.170
right so remember that your deficits in
your test set come from the same

00:09:52.170 --> 00:09:52.180
your test set come from the same
 

00:09:52.180 --> 00:09:54.480
your test set come from the same
distribution so the only way for that to

00:09:54.480 --> 00:09:54.490
distribution so the only way for that to
 

00:09:54.490 --> 00:09:56.610
distribution so the only way for that to
be a huge gap here for it to much better

00:09:56.610 --> 00:09:56.620
be a huge gap here for it to much better
 

00:09:56.620 --> 00:09:58.890
be a huge gap here for it to much better
on the deficit in the test set is if you

00:09:58.890 --> 00:09:58.900
on the deficit in the test set is if you
 

00:09:58.900 --> 00:10:00.780
on the deficit in the test set is if you
somehow managed to over fit the deficit

00:10:00.780 --> 00:10:00.790
somehow managed to over fit the deficit
 

00:10:00.790 --> 00:10:03.090
somehow managed to over fit the deficit
and if that's the case what you might

00:10:03.090 --> 00:10:03.100
and if that's the case what you might
 

00:10:03.100 --> 00:10:04.590
and if that's the case what you might
because they're doing is going back and

00:10:04.590 --> 00:10:04.600
because they're doing is going back and
 

00:10:04.600 --> 00:10:07.380
because they're doing is going back and
just getting more deficit data now I've

00:10:07.380 --> 00:10:07.390
just getting more deficit data now I've
 

00:10:07.390 --> 00:10:09.720
just getting more deficit data now I've
written these numbers as if as you go

00:10:09.720 --> 00:10:09.730
written these numbers as if as you go
 

00:10:09.730 --> 00:10:12.000
written these numbers as if as you go
down the list of numbers always keep

00:10:12.000 --> 00:10:12.010
down the list of numbers always keep
 

00:10:12.010 --> 00:10:15.630
down the list of numbers always keep
going up here's one example of numbers

00:10:15.630 --> 00:10:15.640
going up here's one example of numbers
 

00:10:15.640 --> 00:10:17.490
going up here's one example of numbers
that doesn't always go home

00:10:17.490 --> 00:10:17.500
that doesn't always go home
 

00:10:17.500 --> 00:10:20.060
that doesn't always go home
maybe hew level performance is 4 percent

00:10:20.060 --> 00:10:20.070
maybe hew level performance is 4 percent
 

00:10:20.070 --> 00:10:23.430
maybe hew level performance is 4 percent
training or 7% training def error is 10

00:10:23.430 --> 00:10:23.440
training or 7% training def error is 10
 

00:10:23.440 --> 00:10:25.470
training or 7% training def error is 10
percent but let's say that we go to the

00:10:25.470 --> 00:10:25.480
percent but let's say that we go to the
 

00:10:25.480 --> 00:10:27.170
percent but let's say that we go to the
DEF set you find that you actually

00:10:27.170 --> 00:10:27.180
DEF set you find that you actually
 

00:10:27.180 --> 00:10:29.750
DEF set you find that you actually
surprisingly do much better on the depth

00:10:29.750 --> 00:10:29.760
surprisingly do much better on the depth
 

00:10:29.760 --> 00:10:38.329
surprisingly do much better on the depth
maybe this is 6% 6% as well so you know

00:10:38.329 --> 00:10:38.339
maybe this is 6% 6% as well so you know
 

00:10:38.339 --> 00:10:40.370
maybe this is 6% 6% as well so you know
I've seen effects like this working on

00:10:40.370 --> 00:10:40.380
I've seen effects like this working on
 

00:10:40.380 --> 00:10:42.620
I've seen effects like this working on
for example a speech recognition toss

00:10:42.620 --> 00:10:42.630
for example a speech recognition toss
 

00:10:42.630 --> 00:10:45.920
for example a speech recognition toss
where the training data turned out to be

00:10:45.920 --> 00:10:45.930
where the training data turned out to be
 

00:10:45.930 --> 00:10:48.110
where the training data turned out to be
much harder than your deaf set and test

00:10:48.110 --> 00:10:48.120
much harder than your deaf set and test
 

00:10:48.120 --> 00:10:51.620
much harder than your deaf set and test
set so these two were evaluated on your

00:10:51.620 --> 00:10:51.630
set so these two were evaluated on your
 

00:10:51.630 --> 00:10:54.199
set so these two were evaluated on your
training set distribution and these two

00:10:54.199 --> 00:10:54.209
training set distribution and these two
 

00:10:54.209 --> 00:10:56.420
training set distribution and these two
were evaluated on your deaf test set

00:10:56.420 --> 00:10:56.430
were evaluated on your deaf test set
 

00:10:56.430 --> 00:10:59.540
were evaluated on your deaf test set
distribution so sometimes if you're deaf

00:10:59.540 --> 00:10:59.550
distribution so sometimes if you're deaf
 

00:10:59.550 --> 00:11:02.000
distribution so sometimes if you're deaf
test set distribution is much easier for

00:11:02.000 --> 00:11:02.010
test set distribution is much easier for
 

00:11:02.010 --> 00:11:04.100
test set distribution is much easier for
whatever application working on then

00:11:04.100 --> 00:11:04.110
whatever application working on then
 

00:11:04.110 --> 00:11:06.439
whatever application working on then
these numbers can actually go down

00:11:06.439 --> 00:11:06.449
these numbers can actually go down
 

00:11:06.449 --> 00:11:08.420
these numbers can actually go down
so if you see funny things like this

00:11:08.420 --> 00:11:08.430
so if you see funny things like this
 

00:11:08.430 --> 00:11:10.189
so if you see funny things like this
that there's a even more general

00:11:10.189 --> 00:11:10.199
that there's a even more general
 

00:11:10.199 --> 00:11:12.139
that there's a even more general
formulation of this analysis that might

00:11:12.139 --> 00:11:12.149
formulation of this analysis that might
 

00:11:12.149 --> 00:11:14.269
formulation of this analysis that might
be helpful let me quickly explain that

00:11:14.269 --> 00:11:14.279
be helpful let me quickly explain that
 

00:11:14.279 --> 00:11:18.350
be helpful let me quickly explain that
on the next slide so let me motivate

00:11:18.350 --> 00:11:18.360
on the next slide so let me motivate
 

00:11:18.360 --> 00:11:23.480
on the next slide so let me motivate
this using the speech activated rearview

00:11:23.480 --> 00:11:23.490
this using the speech activated rearview
 

00:11:23.490 --> 00:11:27.740
this using the speech activated rearview
mirror example it turns out that the

00:11:27.740 --> 00:11:27.750
mirror example it turns out that the
 

00:11:27.750 --> 00:11:30.050
mirror example it turns out that the
numbers we've been writing down can be

00:11:30.050 --> 00:11:30.060
numbers we've been writing down can be
 

00:11:30.060 --> 00:11:33.470
numbers we've been writing down can be
placed into a table where on the

00:11:33.470 --> 00:11:33.480
placed into a table where on the
 

00:11:33.480 --> 00:11:35.300
placed into a table where on the
horizontal axis I'm gonna place

00:11:35.300 --> 00:11:35.310
horizontal axis I'm gonna place
 

00:11:35.310 --> 00:11:37.879
horizontal axis I'm gonna place
different data sets so for example you

00:11:37.879 --> 00:11:37.889
different data sets so for example you
 

00:11:37.889 --> 00:11:40.790
different data sets so for example you
might have data from your general speech

00:11:40.790 --> 00:11:40.800
might have data from your general speech
 

00:11:40.800 --> 00:11:44.269
might have data from your general speech
recognition tasks so you might have a

00:11:44.269 --> 00:11:44.279
recognition tasks so you might have a
 

00:11:44.279 --> 00:11:47.600
recognition tasks so you might have a
bunch of data then you've just collected

00:11:47.600 --> 00:11:47.610
bunch of data then you've just collected
 

00:11:47.610 --> 00:11:48.829
bunch of data then you've just collected
from a lot of speech recognition

00:11:48.829 --> 00:11:48.839
from a lot of speech recognition
 

00:11:48.839 --> 00:11:50.930
from a lot of speech recognition
problems you've worked on from small

00:11:50.930 --> 00:11:50.940
problems you've worked on from small
 

00:11:50.940 --> 00:11:52.879
problems you've worked on from small
speakers day to your purchase and so on

00:11:52.879 --> 00:11:52.889
speakers day to your purchase and so on
 

00:11:52.889 --> 00:11:55.309
speakers day to your purchase and so on
and then you also have the rearview

00:11:55.309 --> 00:11:55.319
and then you also have the rearview
 

00:11:55.319 --> 00:11:57.579
and then you also have the rearview
mirror

00:11:57.579 --> 00:11:57.589
mirror
 

00:11:57.589 --> 00:12:01.430
mirror
specific speech these are recorded

00:12:01.430 --> 00:12:01.440
specific speech these are recorded
 

00:12:01.440 --> 00:12:06.439
specific speech these are recorded
inside the car so on this x-axis on the

00:12:06.439 --> 00:12:06.449
inside the car so on this x-axis on the
 

00:12:06.449 --> 00:12:10.430
inside the car so on this x-axis on the
table going to vary the data set on this

00:12:10.430 --> 00:12:10.440
table going to vary the data set on this
 

00:12:10.440 --> 00:12:15.500
table going to vary the data set on this
other axis I'm gonna label different

00:12:15.500 --> 00:12:15.510
other axis I'm gonna label different
 

00:12:15.510 --> 00:12:17.509
other axis I'm gonna label different
ways or algorithms for examining the

00:12:17.509 --> 00:12:17.519
ways or algorithms for examining the
 

00:12:17.519 --> 00:12:20.720
ways or algorithms for examining the
data so first there's human level

00:12:20.720 --> 00:12:20.730
data so first there's human level
 

00:12:20.730 --> 00:12:22.519
data so first there's human level
performance which is how accurate are

00:12:22.519 --> 00:12:22.529
performance which is how accurate are
 

00:12:22.529 --> 00:12:26.110
performance which is how accurate are
humans on each of these data sets them

00:12:26.110 --> 00:12:26.120
humans on each of these data sets them
 

00:12:26.120 --> 00:12:33.400
humans on each of these data sets them
then there is the error on the examples

00:12:33.400 --> 00:12:33.410
then there is the error on the examples
 

00:12:33.410 --> 00:12:34.519
then there is the error on the examples
then

00:12:34.519 --> 00:12:34.529
then
 

00:12:34.529 --> 00:12:39.019
then
your network has trained on and then

00:12:39.019 --> 00:12:39.029
your network has trained on and then
 

00:12:39.029 --> 00:12:45.139
your network has trained on and then
finally this error on examples that your

00:12:45.139 --> 00:12:45.149
finally this error on examples that your
 

00:12:45.149 --> 00:12:50.360
finally this error on examples that your
neural network has not trained on so

00:12:50.360 --> 00:12:50.370
neural network has not trained on so
 

00:12:50.370 --> 00:12:52.879
neural network has not trained on so
turns out that what we're calling on the

00:12:52.879 --> 00:12:52.889
turns out that what we're calling on the
 

00:12:52.889 --> 00:12:55.639
turns out that what we're calling on the
human level on the previous slide is the

00:12:55.639 --> 00:12:55.649
human level on the previous slide is the
 

00:12:55.649 --> 00:12:59.030
human level on the previous slide is the
number that goes in this box which is

00:12:59.030 --> 00:12:59.040
number that goes in this box which is
 

00:12:59.040 --> 00:13:02.420
number that goes in this box which is
how well do humans do on this category

00:13:02.420 --> 00:13:02.430
how well do humans do on this category
 

00:13:02.430 --> 00:13:04.730
how well do humans do on this category
of data say data from all sorts of

00:13:04.730 --> 00:13:04.740
of data say data from all sorts of
 

00:13:04.740 --> 00:13:06.679
of data say data from all sorts of
speech recognition tasks you know the

00:13:06.679 --> 00:13:06.689
speech recognition tasks you know the
 

00:13:06.689 --> 00:13:09.019
speech recognition tasks you know the
5,000 other answers that you could power

00:13:09.019 --> 00:13:09.029
5,000 other answers that you could power
 

00:13:09.029 --> 00:13:11.449
5,000 other answers that you could power
into your training set and their example

00:13:11.449 --> 00:13:11.459
into your training set and their example
 

00:13:11.459 --> 00:13:12.739
into your training set and their example
the previous slide this is 4 percent

00:13:12.739 --> 00:13:12.749
the previous slide this is 4 percent
 

00:13:12.749 --> 00:13:18.710
the previous slide this is 4 percent
this number here was our maybe the

00:13:18.710 --> 00:13:18.720
this number here was our maybe the
 

00:13:18.720 --> 00:13:24.170
this number here was our maybe the
training error which in the example in

00:13:24.170 --> 00:13:24.180
training error which in the example in
 

00:13:24.180 --> 00:13:30.019
training error which in the example in
the previous slide was 7 percent right

00:13:30.019 --> 00:13:30.029
the previous slide was 7 percent right
 

00:13:30.029 --> 00:13:31.759
the previous slide was 7 percent right
if your learning algorithm has seen this

00:13:31.759 --> 00:13:31.769
if your learning algorithm has seen this
 

00:13:31.769 --> 00:13:33.829
if your learning algorithm has seen this
example performed gradient descent as an

00:13:33.829 --> 00:13:33.839
example performed gradient descent as an
 

00:13:33.839 --> 00:13:36.259
example performed gradient descent as an
example and this example came from your

00:13:36.259 --> 00:13:36.269
example and this example came from your
 

00:13:36.269 --> 00:13:37.939
example and this example came from your
Soviet training site distribution or

00:13:37.939 --> 00:13:37.949
Soviet training site distribution or
 

00:13:37.949 --> 00:13:39.619
Soviet training site distribution or
some general specialization distribution

00:13:39.619 --> 00:13:39.629
some general specialization distribution
 

00:13:39.629 --> 00:13:42.619
some general specialization distribution
how well does your algorithm do on the

00:13:42.619 --> 00:13:42.629
how well does your algorithm do on the
 

00:13:42.629 --> 00:13:47.749
how well does your algorithm do on the
example it has trained on then here is

00:13:47.749 --> 00:13:47.759
example it has trained on then here is
 

00:13:47.759 --> 00:13:54.350
example it has trained on then here is
the training def set error so you see a

00:13:54.350 --> 00:13:54.360
the training def set error so you see a
 

00:13:54.360 --> 00:13:57.199
the training def set error so you see a
bit higher which is for data from this

00:13:57.199 --> 00:13:57.209
bit higher which is for data from this
 

00:13:57.209 --> 00:13:58.699
bit higher which is for data from this
distribution from general speech

00:13:58.699 --> 00:13:58.709
distribution from general speech
 

00:13:58.709 --> 00:14:00.980
distribution from general speech
recognition if your algorithm did not

00:14:00.980 --> 00:14:00.990
recognition if your algorithm did not
 

00:14:00.990 --> 00:14:03.590
recognition if your algorithm did not
train explicitly on some examples from

00:14:03.590 --> 00:14:03.600
train explicitly on some examples from
 

00:14:03.600 --> 00:14:05.869
train explicitly on some examples from
this distribution how well does it do

00:14:05.869 --> 00:14:05.879
this distribution how well does it do
 

00:14:05.879 --> 00:14:07.939
this distribution how well does it do
and that's what we call the training def

00:14:07.939 --> 00:14:07.949
and that's what we call the training def
 

00:14:07.949 --> 00:14:11.869
and that's what we call the training def
error and then if you move over to the

00:14:11.869 --> 00:14:11.879
error and then if you move over to the
 

00:14:11.879 --> 00:14:12.410
error and then if you move over to the
right

00:14:12.410 --> 00:14:12.420
right
 

00:14:12.420 --> 00:14:17.480
right
this box here is the death set ever or

00:14:17.480 --> 00:14:17.490
this box here is the death set ever or
 

00:14:17.490 --> 00:14:22.970
this box here is the death set ever or
maybe also the test set error which was

00:14:22.970 --> 00:14:22.980
maybe also the test set error which was
 

00:14:22.980 --> 00:14:25.249
maybe also the test set error which was
6 percent in that example just now and

00:14:25.249 --> 00:14:25.259
6 percent in that example just now and
 

00:14:25.259 --> 00:14:26.569
6 percent in that example just now and
Devon test set error actually

00:14:26.569 --> 00:14:26.579
Devon test set error actually
 

00:14:26.579 --> 00:14:28.699
Devon test set error actually
technically two numbers but either one

00:14:28.699 --> 00:14:28.709
technically two numbers but either one
 

00:14:28.709 --> 00:14:33.110
technically two numbers but either one
could go to this box here boom and this

00:14:33.110 --> 00:14:33.120
could go to this box here boom and this
 

00:14:33.120 --> 00:14:34.999
could go to this box here boom and this
is if you have data from your rear view

00:14:34.999 --> 00:14:35.009
is if you have data from your rear view
 

00:14:35.009 --> 00:14:37.519
is if you have data from your rear view
mirror from actually recording a car

00:14:37.519 --> 00:14:37.529
mirror from actually recording a car
 

00:14:37.529 --> 00:14:38.809
mirror from actually recording a car
from the rear view mirror

00:14:38.809 --> 00:14:38.819
from the rear view mirror
 

00:14:38.819 --> 00:14:41.150
from the rear view mirror
application but your neural network did

00:14:41.150 --> 00:14:41.160
application but your neural network did
 

00:14:41.160 --> 00:14:43.009
application but your neural network did
not perform back propagation on this

00:14:43.009 --> 00:14:43.019
not perform back propagation on this
 

00:14:43.019 --> 00:14:46.050
not perform back propagation on this
example what is the error

00:14:46.050 --> 00:14:46.060
example what is the error
 

00:14:46.060 --> 00:14:48.880
example what is the error
so what we're doing in the analysis and

00:14:48.880 --> 00:14:48.890
so what we're doing in the analysis and
 

00:14:48.890 --> 00:14:50.610
so what we're doing in the analysis and
the previous slide was look at

00:14:50.610 --> 00:14:50.620
the previous slide was look at
 

00:14:50.620 --> 00:14:52.690
the previous slide was look at
differences between these two numbers

00:14:52.690 --> 00:14:52.700
differences between these two numbers
 

00:14:52.700 --> 00:14:55.660
differences between these two numbers
these two numbers and these two numbers

00:14:55.660 --> 00:14:55.670
these two numbers and these two numbers
 

00:14:55.670 --> 00:15:00.090
these two numbers and these two numbers
and this gap here is a measure of

00:15:00.090 --> 00:15:00.100
and this gap here is a measure of
 

00:15:00.100 --> 00:15:05.080
and this gap here is a measure of
avoidable bias this gap here is a

00:15:05.080 --> 00:15:05.090
avoidable bias this gap here is a
 

00:15:05.090 --> 00:15:08.770
avoidable bias this gap here is a
measure of variance and this gap here

00:15:08.770 --> 00:15:08.780
measure of variance and this gap here
 

00:15:08.780 --> 00:15:14.020
measure of variance and this gap here
was a measure of data mismatch but it

00:15:14.020 --> 00:15:14.030
was a measure of data mismatch but it
 

00:15:14.030 --> 00:15:15.940
was a measure of data mismatch but it
turns out that it could be useful to

00:15:15.940 --> 00:15:15.950
turns out that it could be useful to
 

00:15:15.950 --> 00:15:18.580
turns out that it could be useful to
also throw in the remaining the two

00:15:18.580 --> 00:15:18.590
also throw in the remaining the two
 

00:15:18.590 --> 00:15:22.780
also throw in the remaining the two
entries in this table and so if this

00:15:22.780 --> 00:15:22.790
entries in this table and so if this
 

00:15:22.790 --> 00:15:25.570
entries in this table and so if this
turns out to be also six percent and the

00:15:25.570 --> 00:15:25.580
turns out to be also six percent and the
 

00:15:25.580 --> 00:15:27.550
turns out to be also six percent and the
way you get this number is you are some

00:15:27.550 --> 00:15:27.560
way you get this number is you are some
 

00:15:27.560 --> 00:15:29.350
way you get this number is you are some
humans to label the rearview mirror

00:15:29.350 --> 00:15:29.360
humans to label the rearview mirror
 

00:15:29.360 --> 00:15:31.600
humans to label the rearview mirror
speech data and just measure how good

00:15:31.600 --> 00:15:31.610
speech data and just measure how good
 

00:15:31.610 --> 00:15:33.820
speech data and just measure how good
humans are at this toss and maybe this

00:15:33.820 --> 00:15:33.830
humans are at this toss and maybe this
 

00:15:33.830 --> 00:15:36.070
humans are at this toss and maybe this
turns out also to be six percent and the

00:15:36.070 --> 00:15:36.080
turns out also to be six percent and the
 

00:15:36.080 --> 00:15:37.780
turns out also to be six percent and the
way you do that is you take some

00:15:37.780 --> 00:15:37.790
way you do that is you take some
 

00:15:37.790 --> 00:15:40.090
way you do that is you take some
rearview mirror speech data put in the

00:15:40.090 --> 00:15:40.100
rearview mirror speech data put in the
 

00:15:40.100 --> 00:15:41.470
rearview mirror speech data put in the
training set so the neural network

00:15:41.470 --> 00:15:41.480
training set so the neural network
 

00:15:41.480 --> 00:15:42.730
training set so the neural network
learns on it as well and then you

00:15:42.730 --> 00:15:42.740
learns on it as well and then you
 

00:15:42.740 --> 00:15:44.980
learns on it as well and then you
measure the error on that subset of the

00:15:44.980 --> 00:15:44.990
measure the error on that subset of the
 

00:15:44.990 --> 00:15:47.530
measure the error on that subset of the
data but if this is what you get then

00:15:47.530 --> 00:15:47.540
data but if this is what you get then
 

00:15:47.540 --> 00:15:49.450
data but if this is what you get then
well it turns out that you're actually

00:15:49.450 --> 00:15:49.460
well it turns out that you're actually
 

00:15:49.460 --> 00:15:51.250
well it turns out that you're actually
already performing at the level of

00:15:51.250 --> 00:15:51.260
already performing at the level of
 

00:15:51.260 --> 00:15:53.890
already performing at the level of
humans on this rear-view mirror speech

00:15:53.890 --> 00:15:53.900
humans on this rear-view mirror speech
 

00:15:53.900 --> 00:15:55.390
humans on this rear-view mirror speech
data so maybe you're actually doing

00:15:55.390 --> 00:15:55.400
data so maybe you're actually doing
 

00:15:55.400 --> 00:15:57.660
data so maybe you're actually doing
quite well on that distribution of data

00:15:57.660 --> 00:15:57.670
quite well on that distribution of data
 

00:15:57.670 --> 00:16:00.580
quite well on that distribution of data
when you do this more sub skill analysis

00:16:00.580 --> 00:16:00.590
when you do this more sub skill analysis
 

00:16:00.590 --> 00:16:03.910
when you do this more sub skill analysis
it doesn't always give you a one clear

00:16:03.910 --> 00:16:03.920
it doesn't always give you a one clear
 

00:16:03.920 --> 00:16:05.440
it doesn't always give you a one clear
powerful wave but sometimes it just

00:16:05.440 --> 00:16:05.450
powerful wave but sometimes it just
 

00:16:05.450 --> 00:16:07.270
powerful wave but sometimes it just
gives you additional insights as well so

00:16:07.270 --> 00:16:07.280
gives you additional insights as well so
 

00:16:07.280 --> 00:16:09.520
gives you additional insights as well so
for example comparing these two numbers

00:16:09.520 --> 00:16:09.530
for example comparing these two numbers
 

00:16:09.530 --> 00:16:12.280
for example comparing these two numbers
in this case tells us that for humans

00:16:12.280 --> 00:16:12.290
in this case tells us that for humans
 

00:16:12.290 --> 00:16:14.170
in this case tells us that for humans
the rearview mirror speech danger is

00:16:14.170 --> 00:16:14.180
the rearview mirror speech danger is
 

00:16:14.180 --> 00:16:17.170
the rearview mirror speech danger is
actually harder than for generous speech

00:16:17.170 --> 00:16:17.180
actually harder than for generous speech
 

00:16:17.180 --> 00:16:19.510
actually harder than for generous speech
recognition because humans get 6% error

00:16:19.510 --> 00:16:19.520
recognition because humans get 6% error
 

00:16:19.520 --> 00:16:22.540
recognition because humans get 6% error
rather than 4% error but then looking at

00:16:22.540 --> 00:16:22.550
rather than 4% error but then looking at
 

00:16:22.550 --> 00:16:25.290
rather than 4% error but then looking at
these differences as well may help you

00:16:25.290 --> 00:16:25.300
these differences as well may help you
 

00:16:25.300 --> 00:16:28.270
these differences as well may help you
understand bias and variance in data

00:16:28.270 --> 00:16:28.280
understand bias and variance in data
 

00:16:28.280 --> 00:16:31.230
understand bias and variance in data
mismatch problems due to the degrees so

00:16:31.230 --> 00:16:31.240
mismatch problems due to the degrees so
 

00:16:31.240 --> 00:16:34.150
mismatch problems due to the degrees so
this more general formulation is

00:16:34.150 --> 00:16:34.160
this more general formulation is
 

00:16:34.160 --> 00:16:36.010
this more general formulation is
something I've used a few times I've not

00:16:36.010 --> 00:16:36.020
something I've used a few times I've not
 

00:16:36.020 --> 00:16:38.860
something I've used a few times I've not
used it but for a lot of problems you

00:16:38.860 --> 00:16:38.870
used it but for a lot of problems you
 

00:16:38.870 --> 00:16:41.740
used it but for a lot of problems you
find that examining this subset of

00:16:41.740 --> 00:16:41.750
find that examining this subset of
 

00:16:41.750 --> 00:16:44.620
find that examining this subset of
entries kind of looking at this

00:16:44.620 --> 00:16:44.630
entries kind of looking at this
 

00:16:44.630 --> 00:16:46.120
entries kind of looking at this
difference in this difference in just

00:16:46.120 --> 00:16:46.130
difference in this difference in just
 

00:16:46.130 --> 00:16:48.310
difference in this difference in just
DIF that that's enough to point you in a

00:16:48.310 --> 00:16:48.320
DIF that that's enough to point you in a
 

00:16:48.320 --> 00:16:51.730
DIF that that's enough to point you in a
pretty promising direction but sometimes

00:16:51.730 --> 00:16:51.740
pretty promising direction but sometimes
 

00:16:51.740 --> 00:16:53.680
pretty promising direction but sometimes
filling out this whole table can give

00:16:53.680 --> 00:16:53.690
filling out this whole table can give
 

00:16:53.690 --> 00:16:55.330
filling out this whole table can give
you additional in

00:16:55.330 --> 00:16:55.340
you additional in
 

00:16:55.340 --> 00:16:58.760
you additional in
finally with previously talks a lot

00:16:58.760 --> 00:16:58.770
finally with previously talks a lot
 

00:16:58.770 --> 00:17:02.600
finally with previously talks a lot
about ideas for addressing bias tough

00:17:02.600 --> 00:17:02.610
about ideas for addressing bias tough
 

00:17:02.610 --> 00:17:04.100
about ideas for addressing bias tough
all about techniques for accessing

00:17:04.100 --> 00:17:04.110
all about techniques for accessing
 

00:17:04.110 --> 00:17:07.130
all about techniques for accessing
variance but how do you address data

00:17:07.130 --> 00:17:07.140
variance but how do you address data
 

00:17:07.140 --> 00:17:10.670
variance but how do you address data
mismatch in particular training on data

00:17:10.670 --> 00:17:10.680
mismatch in particular training on data
 

00:17:10.680 --> 00:17:12.470
mismatch in particular training on data
that comes from different distributions

00:17:12.470 --> 00:17:12.480
that comes from different distributions
 

00:17:12.480 --> 00:17:14.569
that comes from different distributions
and your dev and test set can get you

00:17:14.569 --> 00:17:14.579
and your dev and test set can get you
 

00:17:14.579 --> 00:17:16.340
and your dev and test set can get you
more data and really help your learning

00:17:16.340 --> 00:17:16.350
more data and really help your learning
 

00:17:16.350 --> 00:17:18.890
more data and really help your learning
outruns performance but rather than just

00:17:18.890 --> 00:17:18.900
outruns performance but rather than just
 

00:17:18.900 --> 00:17:20.750
outruns performance but rather than just
bias and variance problems you now have

00:17:20.750 --> 00:17:20.760
bias and variance problems you now have
 

00:17:20.760 --> 00:17:22.970
bias and variance problems you now have
this new potential problem of data

00:17:22.970 --> 00:17:22.980
this new potential problem of data
 

00:17:22.980 --> 00:17:25.490
this new potential problem of data
mismatch what are some good ways that

00:17:25.490 --> 00:17:25.500
mismatch what are some good ways that
 

00:17:25.500 --> 00:17:27.860
mismatch what are some good ways that
you could use to address data mismatch

00:17:27.860 --> 00:17:27.870
you could use to address data mismatch
 

00:17:27.870 --> 00:17:29.600
you could use to address data mismatch
I'll be honest and say there actually

00:17:29.600 --> 00:17:29.610
I'll be honest and say there actually
 

00:17:29.610 --> 00:17:31.270
I'll be honest and say there actually
aren't too great or at least not very

00:17:31.270 --> 00:17:31.280
aren't too great or at least not very
 

00:17:31.280 --> 00:17:33.650
aren't too great or at least not very
systematic ways to address data mismatch

00:17:33.650 --> 00:17:33.660
systematic ways to address data mismatch
 

00:17:33.660 --> 00:17:35.270
systematic ways to address data mismatch
but there are some things you could try

00:17:35.270 --> 00:17:35.280
but there are some things you could try
 

00:17:35.280 --> 00:17:37.190
but there are some things you could try
that could help let's take a look at

00:17:37.190 --> 00:17:37.200
that could help let's take a look at
 

00:17:37.200 --> 00:17:39.380
that could help let's take a look at
them in the next video so what we're

00:17:39.380 --> 00:17:39.390
them in the next video so what we're
 

00:17:39.390 --> 00:17:42.020
them in the next video so what we're
seeing is that by using training data

00:17:42.020 --> 00:17:42.030
seeing is that by using training data
 

00:17:42.030 --> 00:17:43.370
seeing is that by using training data
that can come from a different

00:17:43.370 --> 00:17:43.380
that can come from a different
 

00:17:43.380 --> 00:17:45.740
that can come from a different
distribution as a Defen test set this

00:17:45.740 --> 00:17:45.750
distribution as a Defen test set this
 

00:17:45.750 --> 00:17:47.750
distribution as a Defen test set this
could give you a lot more data and

00:17:47.750 --> 00:17:47.760
could give you a lot more data and
 

00:17:47.760 --> 00:17:49.190
could give you a lot more data and
therefore help the performance of your

00:17:49.190 --> 00:17:49.200
therefore help the performance of your
 

00:17:49.200 --> 00:17:51.890
therefore help the performance of your
learning algorithm but instead of just

00:17:51.890 --> 00:17:51.900
learning algorithm but instead of just
 

00:17:51.900 --> 00:17:53.960
learning algorithm but instead of just
having bias and variance as two

00:17:53.960 --> 00:17:53.970
having bias and variance as two
 

00:17:53.970 --> 00:17:55.640
having bias and variance as two
potential problems you now have this

00:17:55.640 --> 00:17:55.650
potential problems you now have this
 

00:17:55.650 --> 00:17:58.610
potential problems you now have this
third potential problem data mismatch so

00:17:58.610 --> 00:17:58.620
third potential problem data mismatch so
 

00:17:58.620 --> 00:18:00.290
third potential problem data mismatch so
what do we perform error analysis and

00:18:00.290 --> 00:18:00.300
what do we perform error analysis and
 

00:18:00.300 --> 00:18:02.570
what do we perform error analysis and
conclude that data mismatch is a huge

00:18:02.570 --> 00:18:02.580
conclude that data mismatch is a huge
 

00:18:02.580 --> 00:18:04.460
conclude that data mismatch is a huge
source of error how can you go about

00:18:04.460 --> 00:18:04.470
source of error how can you go about
 

00:18:04.470 --> 00:18:06.290
source of error how can you go about
addressing that it turns out that

00:18:06.290 --> 00:18:06.300
addressing that it turns out that
 

00:18:06.300 --> 00:18:08.210
addressing that it turns out that
unfortunately there aren't super

00:18:08.210 --> 00:18:08.220
unfortunately there aren't super
 

00:18:08.220 --> 00:18:11.000
unfortunately there aren't super
systematic ways to address data mismatch

00:18:11.000 --> 00:18:11.010
systematic ways to address data mismatch
 

00:18:11.010 --> 00:18:12.830
systematic ways to address data mismatch
but there are a few things you can try

00:18:12.830 --> 00:18:12.840
but there are a few things you can try
 

00:18:12.840 --> 00:18:14.780
but there are a few things you can try
that could help let's take a look at

00:18:14.780 --> 00:18:14.790
that could help let's take a look at
 

00:18:14.790 --> 00:18:17.750
that could help let's take a look at
them in the next video

