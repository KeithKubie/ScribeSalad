WEBVTT
Kind: captions
Language: en

00:00:01.640 --> 00:00:03.480
- [Narrator] This
program is a presentation

00:00:03.480 --> 00:00:07.703
of UCTV for educational
and noncommercial use only.

00:00:10.372 --> 00:00:13.289
(electronic music)

00:00:36.140 --> 00:00:37.900
- Welcome to a Conversation with History,

00:00:37.900 --> 00:00:38.890
I"m Harry Kreisler

00:00:38.890 --> 00:00:41.180
of the Institute of International Studies.

00:00:41.180 --> 00:00:43.970
Our guest today is Richard H. Thaler,

00:00:43.970 --> 00:00:46.510
who is the Ralph and Dorothy Keller

00:00:46.510 --> 00:00:48.900
Distinguished Service Professor

00:00:48.900 --> 00:00:51.520
of Behavioral Science and Economics

00:00:51.520 --> 00:00:53.390
at the Graduate School of Business,

00:00:53.390 --> 00:00:55.550
the University of Chicago.

00:00:55.550 --> 00:00:59.180
His publications include, The Winter's,

00:00:59.180 --> 00:01:03.980
The Winner's Curse: Paradoxes
and Anomalies of Economic Life

00:01:03.980 --> 00:01:08.440
and Nudge: Improving
Decisions About Health,

00:01:08.440 --> 00:01:10.150
Wealth, and Happiness.

00:01:10.150 --> 00:01:15.150
He is the 2010 Hitchcock
Professor on the Berkeley Campus.

00:01:15.730 --> 00:01:17.660
Well, welcome, Professor Thaler.

00:01:17.660 --> 00:01:19.770
- Thank you, it's very nice
to be here in Berkeley.

00:01:19.770 --> 00:01:21.650
- Where were you born and raised?

00:01:21.650 --> 00:01:23.950
- I was born and raised in New Jersey

00:01:26.000 --> 00:01:29.850
and grew up in, grew up
in suburban New Jersey.

00:01:29.850 --> 00:01:33.150
My, my father was an actuary,

00:01:33.150 --> 00:01:37.580
and commuted on the train into
Newark, worked at Prudential.

00:01:37.580 --> 00:01:39.890
- And looking back, how do
you think your parents shaped

00:01:39.890 --> 00:01:42.127
your thinking about the world?

00:01:42.127 --> 00:01:45.100
- Oh, you know, it's always hard to tell.

00:01:45.100 --> 00:01:48.350
I think my father's
greatest influence on me

00:01:48.350 --> 00:01:52.080
probably was to convince me
that I didn't have it in me

00:01:52.080 --> 00:01:54.260
to become a business man.

00:01:54.260 --> 00:01:57.000
I saw what he did and I
knew I couldn't do that,

00:01:57.000 --> 00:01:59.800
and so, I've never held an honest job

00:01:59.800 --> 00:02:01.893
and hope to keep it that way.

00:02:03.000 --> 00:02:07.790
- And, but he peaked your
interest in economics?

00:02:07.790 --> 00:02:11.603
- Not really, I would say he
peaked my interest in math.

00:02:12.500 --> 00:02:15.410
Actuaries are really
applied mathematicians,

00:02:15.410 --> 00:02:18.970
and he was always giving
me little brain teasers

00:02:18.970 --> 00:02:22.460
and math problems, and
I'd say he cultivated

00:02:22.460 --> 00:02:24.370
my analytical thinking ability.

00:02:24.370 --> 00:02:26.480
- And where were you educated?

00:02:26.480 --> 00:02:29.260
- I was an undergraduate
at Case Western Reserve

00:02:29.260 --> 00:02:32.707
and did my PhD at the
University of Rochester.

00:02:32.707 --> 00:02:35.150
- And what was your dissertation on?

00:02:35.150 --> 00:02:39.000
- My dissertation was
on the value of a life,

00:02:39.000 --> 00:02:42.120
and it sounds like a unusual topic,

00:02:42.120 --> 00:02:47.120
but I had taught a course
in cost benefit analysis

00:02:47.360 --> 00:02:49.530
and this was kind of a problem

00:02:49.530 --> 00:02:52.300
that hadn't really been
dealt with satisfactorily.

00:02:52.300 --> 00:02:54.610
How should we decide how much to spend

00:02:54.610 --> 00:02:55.963
on making things safer?

00:02:57.040 --> 00:02:59.420
And so ...

00:02:59.420 --> 00:03:02.990
The theoretically correct approach is

00:03:02.990 --> 00:03:05.700
to see how much you
would be willing to pay

00:03:05.700 --> 00:03:08.990
to make your life a little bit safer,

00:03:08.990 --> 00:03:12.150
and, and then, because that's what we do,

00:03:12.150 --> 00:03:14.140
we don't save specific lives,

00:03:14.140 --> 00:03:16.590
we make everyone's lives
a little bit safer,

00:03:16.590 --> 00:03:19.920
and so, I estimated that by looking at

00:03:19.920 --> 00:03:23.880
how much you had to pay
people to take risky jobs,

00:03:23.880 --> 00:03:28.880
and it was a very
classical economics thesis.

00:03:29.410 --> 00:03:30.850
- And ...

00:03:30.850 --> 00:03:35.850
You, you are classified, I
guess, as a behavioral economist.

00:03:36.860 --> 00:03:38.440
- [Richard] Yes.
- And I wanna

00:03:38.440 --> 00:03:41.110
help our audience understand

00:03:41.110 --> 00:03:44.920
what that, that exactly means

00:03:44.920 --> 00:03:47.460
and I want to help students understand

00:03:47.460 --> 00:03:52.120
how you prepare for a career
in behavioral economics.

00:03:52.120 --> 00:03:54.920
So, so first, let me ask,
what are the skills involved

00:03:55.860 --> 00:03:58.090
in the kind of work you do?

00:03:58.090 --> 00:04:00.510
- Well, being a behavioral economist

00:04:00.510 --> 00:04:02.800
is really being an economist,

00:04:02.800 --> 00:04:06.960
and the training is really 90%

00:04:06.960 --> 00:04:09.833
the same as being any
other kind of economist.

00:04:11.498 --> 00:04:14.170
I think it's just taking a
somewhat different approach

00:04:14.170 --> 00:04:17.563
and borrowing some psychology, so ...

00:04:18.480 --> 00:04:21.210
A student who sets out to
become a behavioral economist

00:04:21.210 --> 00:04:24.060
goes to an economics graduate program,

00:04:24.060 --> 00:04:27.130
and then, take some psychology courses

00:04:27.130 --> 00:04:29.003
to add some breadth.
- Mhmm.

00:04:29.933 --> 00:04:33.870
And, and what accounts for

00:04:33.870 --> 00:04:38.390
your interest in human behavior

00:04:38.390 --> 00:04:40.510
and psychology ...

00:04:40.510 --> 00:04:43.480
As a way to inform economics?

00:04:43.480 --> 00:04:47.190
Was it the anomalies of economic theory

00:04:47.190 --> 00:04:48.650
that led you in that direction

00:04:48.650 --> 00:04:49.900
or was this just an interest

00:04:49.900 --> 00:04:52.390
you developed earlier in your career?

00:04:52.390 --> 00:04:55.630
- No, I just kept running
into counter examples

00:04:55.630 --> 00:04:56.463
to the theory.

00:04:56.463 --> 00:05:01.463
So, the first one I found while
I was working on my thesis.

00:05:01.850 --> 00:05:05.790
So, I decided, I had mentioned I was

00:05:05.790 --> 00:05:07.770
doing really an econometrics exercise,

00:05:07.770 --> 00:05:10.520
but I decided just for the fun of it

00:05:10.520 --> 00:05:12.860
to ask people some questions.

00:05:12.860 --> 00:05:16.053
So, I would ask people two questions.

00:05:17.507 --> 00:05:19.787
"Suppose, by conducting this interview,

00:05:19.787 --> 00:05:24.763
"you've been exposed to a one
in a thousand risk of death,

00:05:26.137 --> 00:05:28.187
"how much would you pay to eliminate it?"

00:05:29.700 --> 00:05:32.407
And then, I'd ask a second question,

00:05:32.407 --> 00:05:34.977
"How much would you be, have to be paid

00:05:34.977 --> 00:05:38.987
"to agree to accept an
extra one in a thousand

00:05:38.987 --> 00:05:40.940
"risk of death?"

00:05:40.940 --> 00:05:42.470
Now, economic theory says

00:05:42.470 --> 00:05:44.883
those answers have to
be virtually the same,

00:05:45.810 --> 00:05:48.817
but when I would ask those
questions, people would say,

00:05:48.817 --> 00:05:52.867
"Oh, I would pay $5,000
to eliminate the risk,

00:05:52.867 --> 00:05:56.457
"but you wouldn't get me to
take that risk for a million."

00:05:57.640 --> 00:06:01.190
And, I thought, "Well,
that's interesting,"

00:06:01.190 --> 00:06:04.780
and so, I started having a list

00:06:06.888 --> 00:06:10.810
on my blackboard of funny behavior

00:06:10.810 --> 00:06:14.740
and I was trying to make
sense of this behavior,

00:06:14.740 --> 00:06:19.440
and then, I ran into the
work of two psychologists,

00:06:19.440 --> 00:06:23.363
two Israeli psychologists,
Daniel Kahneman and Amos Tversky.

00:06:25.080 --> 00:06:27.680
Kahneman, of course, was a
faculty member at Berkeley here

00:06:27.680 --> 00:06:28.513
for many years.
- [Harry] And he was

00:06:28.513 --> 00:06:30.380
a guest on our program, actually.

00:06:30.380 --> 00:06:31.443
- And ...

00:06:32.430 --> 00:06:35.520
So, they were ...

00:06:35.520 --> 00:06:38.630
They had planned a trip to ...

00:06:38.630 --> 00:06:42.450
Stanford for a year, they were professors

00:06:42.450 --> 00:06:44.180
at the Hebrew University

00:06:44.180 --> 00:06:48.460
and they were gonna go to
Stanford for a year, 1977,

00:06:48.460 --> 00:06:53.050
and I made it my business to
be in Stanford that year too.

00:06:53.050 --> 00:06:57.660
We spent a year talking
and the rest is history.

00:06:57.660 --> 00:07:02.660
- Now, in your book on The Winner's Curse,

00:07:03.170 --> 00:07:08.170
you, you help us understand
what a theory is in economics.

00:07:08.620 --> 00:07:10.913
I wanna talk a little about that.

00:07:12.122 --> 00:07:15.280
What is a theory, and then,

00:07:15.280 --> 00:07:18.860
and what is the driving
theory of economics,

00:07:18.860 --> 00:07:22.110
if there is one, what is its assumptions

00:07:22.110 --> 00:07:25.980
about the way people
act in the marketplace?

00:07:25.980 --> 00:07:26.933
- Well ...

00:07:29.240 --> 00:07:34.240
The first and most
important hypothesis is ...

00:07:34.760 --> 00:07:37.453
That agents optimize.

00:07:38.440 --> 00:07:40.440
Individuals or firms.

00:07:40.440 --> 00:07:44.363
So, firms maximize the value of the firm,

00:07:45.200 --> 00:07:48.830
consumers maximize utility.

00:07:48.830 --> 00:07:53.830
They solve any problem as
well as an economist would,

00:07:55.760 --> 00:07:56.883
and ...

00:07:57.790 --> 00:08:01.880
In fact, the sort of
culture of economics is

00:08:02.800 --> 00:08:06.370
that suppose you right
down some theory of,

00:08:06.370 --> 00:08:08.370
well, the recent Nobel Prize winners

00:08:08.370 --> 00:08:11.450
won research for search behavior.

00:08:11.450 --> 00:08:13.150
So, suppose Peter Diamond

00:08:13.150 --> 00:08:14.790
writes down a model of search behavior,

00:08:14.790 --> 00:08:17.210
and then, I come along

00:08:17.210 --> 00:08:19.260
and I write down the
model of search behavior

00:08:19.260 --> 00:08:24.260
where the agents are more
clever in the way they search,

00:08:24.350 --> 00:08:27.333
then, my model is taken to
be better than Peter's model.

00:08:28.610 --> 00:08:32.513
Now, it's not that it's
a better description.

00:08:34.940 --> 00:08:37.613
We've evolved that ...

00:08:38.470 --> 00:08:43.310
The agents in the economy,
as assumed by economists,

00:08:43.310 --> 00:08:47.360
have been getting smarter and
smarter for the last 60 years,

00:08:47.360 --> 00:08:50.950
but human beings, are, you know,

00:08:50.950 --> 00:08:54.560
just as dumb as we always
were, we're just as human.

00:08:54.560 --> 00:08:58.410
So, we've had this growing gap,

00:08:58.410 --> 00:09:02.970
and I'm trying to fill in that gap by

00:09:02.970 --> 00:09:07.360
studying humans in economic situations,

00:09:07.360 --> 00:09:12.360
as to, as supposed to what,
in Nudge, Cass Sunstein and I,

00:09:12.460 --> 00:09:16.300
call them econs, which are
these mythical creatures

00:09:16.300 --> 00:09:19.030
that populate economics textbooks.

00:09:19.030 --> 00:09:21.713
- Now, in a theory,

00:09:22.680 --> 00:09:25.260
and we'll talk about
your insights in a minute

00:09:25.260 --> 00:09:28.810
in terms of the flaws of human nature

00:09:28.810 --> 00:09:30.430
that enter into the equation,

00:09:30.430 --> 00:09:35.060
but in a theory, the goal is to simplify

00:09:35.060 --> 00:09:37.140
- [Richard] Right.
- To push away

00:09:37.140 --> 00:09:40.490
all of the brush, and to have,

00:09:40.490 --> 00:09:43.720
I guess, elegant, is the word, a statement

00:09:43.720 --> 00:09:48.490
with few assumptions, and
then, the measure of success

00:09:48.490 --> 00:09:51.600
is whether you generally predict

00:09:51.600 --> 00:09:54.610
the way things actually
happen in the real world.

00:09:54.610 --> 00:09:58.900
- Well, but notice there's
a bit of a conflict there

00:09:58.900 --> 00:10:02.160
between elegance and predictive power.

00:10:02.160 --> 00:10:06.392
So, you know, Einstein said,
"Elegance is for tailors,"

00:10:06.392 --> 00:10:09.240
(Harry laughs)
and that ...

00:10:09.240 --> 00:10:11.510
We want a theory to be

00:10:12.460 --> 00:10:16.720
as simple as possible, but no simpler.

00:10:16.720 --> 00:10:18.230
And ...

00:10:18.230 --> 00:10:20.220
So ...

00:10:20.220 --> 00:10:24.573
Economic theory is elegant,
and simple, and raw.

00:10:25.420 --> 00:10:26.573
Now ...

00:10:28.050 --> 00:10:29.773
So, let me give you an example,

00:10:31.300 --> 00:10:36.300
one of the, one of the unstated
assumptions of economics

00:10:38.250 --> 00:10:41.680
is that people have perfect self-control

00:10:42.950 --> 00:10:45.763
because they choose just
what they should choose.

00:10:46.750 --> 00:10:50.470
So, an econ never has a hangover,

00:10:50.470 --> 00:10:52.170
never needs to go on a diet

00:10:52.170 --> 00:10:55.553
because he eats just the right amount.

00:10:58.660 --> 00:11:01.173
And, you know ...

00:11:01.173 --> 00:11:04.203
Does that, saves for
retirement, perfectly.

00:11:05.120 --> 00:11:10.120
Now, we look around and
20% of America is obese,

00:11:10.790 --> 00:11:12.980
and 40% is overweight.

00:11:12.980 --> 00:11:15.610
That doesn't look like a world

00:11:15.610 --> 00:11:18.470
in which people have perfect self-control.

00:11:18.470 --> 00:11:20.017
Now, an economist, of course, could say,

00:11:20.017 --> 00:11:22.187
"Well, they choose to be obese.

00:11:22.187 --> 00:11:24.870
"They choose to be overweight,"

00:11:24.870 --> 00:11:29.870
but there's a whole industry
helping people go on diets,

00:11:30.160 --> 00:11:32.490
which suggests that, no there,

00:11:32.490 --> 00:11:35.750
it's not that people
are choosing to be fat,

00:11:35.750 --> 00:11:37.780
there, there's a conflict there.

00:11:37.780 --> 00:11:40.620
One part of them would like to be thinner,

00:11:40.620 --> 00:11:44.030
another part of them reaches
for the bag of potato chips

00:11:44.030 --> 00:11:47.830
and so, if we're gonna have a
good model of that conflict,

00:11:47.830 --> 00:11:49.433
we have to face it head-on.

00:11:50.480 --> 00:11:52.053
- So, in ...

00:11:53.340 --> 00:11:57.780
In your work, as you got
into this, it was ...

00:11:59.650 --> 00:12:03.880
The situation that you would
run up against anomalies

00:12:05.010 --> 00:12:08.680
that the theory couldn't account for.

00:12:08.680 --> 00:12:11.300
So, your book is called
The Winner's Curse,

00:12:11.300 --> 00:12:12.740
what, what is The Winner's Curse?

00:12:12.740 --> 00:12:15.370
And what did you learn from that?

00:12:15.370 --> 00:12:17.230
- Well, The Winner's Curse is

00:12:18.570 --> 00:12:22.090
kind of a well-known
phenomena in economics,

00:12:22.090 --> 00:12:27.020
in which, the more bidders
there are for some object,

00:12:27.020 --> 00:12:29.370
an object that's worth
the same to everybody,

00:12:29.370 --> 00:12:30.853
like say an oil well.

00:12:32.510 --> 00:12:34.870
The more bidders there
are, the more likely it is

00:12:34.870 --> 00:12:37.243
that the winner pays too much,

00:12:38.550 --> 00:12:41.270
and so, this has been well-documented

00:12:41.270 --> 00:12:43.990
in things like studies
of bidding for oil wells

00:12:43.990 --> 00:12:45.683
or for baseball players.

00:12:47.580 --> 00:12:52.580
When the Yankees paid so much
money to get Alex Rodriguez,

00:12:53.010 --> 00:12:55.023
or, well, originally, the Rangers,

00:12:55.920 --> 00:12:57.160
he was a very good player,

00:12:57.160 --> 00:12:59.390
but they paid him an astronomical amount

00:12:59.390 --> 00:13:02.223
and he probably wasn't worth
the astronomical amount.

00:13:04.124 --> 00:13:05.380
So, the more bidders there are,

00:13:05.380 --> 00:13:08.560
the more careful you wanna be in bidding,

00:13:08.560 --> 00:13:11.523
but that's a very counterintuitive idea,

00:13:12.370 --> 00:13:16.910
and so, in many contexts, we see people

00:13:16.910 --> 00:13:21.400
bid too much, and then, the
winner's curse is the fact that

00:13:21.400 --> 00:13:23.440
when you win the auction,
you should be unhappy,

00:13:23.440 --> 00:13:25.020
rather than happy.

00:13:25.020 --> 00:13:30.020
- Now, the thrust of behavioral ...

00:13:30.170 --> 00:13:32.398
Economics ...

00:13:32.398 --> 00:13:35.290
It is empowered by

00:13:35.290 --> 00:13:39.950
observation, and I'm a
little confused as ...

00:13:39.950 --> 00:13:41.920
How does ...

00:13:41.920 --> 00:13:46.910
Theory, classical theory,
continue to survive

00:13:46.910 --> 00:13:51.530
when it really doesn't
emphasize observation

00:13:51.530 --> 00:13:53.900
in the way behavioral economists do,

00:13:53.900 --> 00:13:55.500
or am I missing something here?

00:13:55.500 --> 00:13:58.273
- Well, I think, you know, there ...

00:14:00.140 --> 00:14:02.070
Geocentric models of the universe

00:14:02.070 --> 00:14:07.070
survived for hundreds
of years with epicycles.

00:14:07.890 --> 00:14:11.940
So, you know, epicycles were the kludges

00:14:13.050 --> 00:14:17.230
that were invented by defenders
of the geocentric model

00:14:17.230 --> 00:14:20.400
to explain the anomalies, and they're,

00:14:20.400 --> 00:14:23.560
the, as they got better,

00:14:23.560 --> 00:14:26.470
you know, as they invented
telescopes and things,

00:14:26.470 --> 00:14:28.380
the anomalies became more and more clear,

00:14:28.380 --> 00:14:31.803
and the epicycles became
more and more convoluted.

00:14:33.528 --> 00:14:34.361
So ...

00:14:36.380 --> 00:14:39.867
The defenders of the
classical economic theory,

00:14:41.400 --> 00:14:44.720
say, "Well, it's the only theory we have,"

00:14:44.720 --> 00:14:49.720
and what's true is, it is
very elegant and parsimonious,

00:14:51.700 --> 00:14:55.090
and it often makes good predictions,

00:14:55.090 --> 00:14:58.220
so it is the case, of course,

00:14:58.220 --> 00:15:00.300
that people respond to incentives.

00:15:00.300 --> 00:15:02.233
If you raise the price, people buy less.

00:15:02.233 --> 00:15:04.163
That's a pretty good prediction.

00:15:05.850 --> 00:15:08.480
Now, do they do things perfectly?

00:15:08.480 --> 00:15:12.140
No, and we can argue about

00:15:12.140 --> 00:15:15.460
what are the situations
in which these departures

00:15:15.460 --> 00:15:20.260
are most important, and
my strategy has been

00:15:20.260 --> 00:15:22.010
To aim at those.

00:15:22.010 --> 00:15:23.760
So, I've spent a lot of time

00:15:23.760 --> 00:15:26.340
talking about self-control problems

00:15:26.340 --> 00:15:30.750
because I think those are glaring examples

00:15:30.750 --> 00:15:32.973
in which the standard model fails.

00:15:34.490 --> 00:15:36.423
I've also studied ...

00:15:37.810 --> 00:15:40.200
Situations and economic theory assumes

00:15:40.200 --> 00:15:41.733
that people are selfish.

00:15:43.140 --> 00:15:44.303
And ...

00:15:45.370 --> 00:15:47.190
So ...

00:15:47.190 --> 00:15:52.190
I, if there's some way of
taking advantage of you, I will,

00:15:52.920 --> 00:15:55.073
if I'm a good economic agent.

00:15:55.920 --> 00:15:58.460
Humans are a little nicer than that.

00:15:58.460 --> 00:16:00.940
If you ask somebody what time it is,

00:16:00.940 --> 00:16:02.530
most people they just look at their watch,

00:16:02.530 --> 00:16:04.840
and respond truthfully.

00:16:04.840 --> 00:16:05.673
They don't think,

00:16:05.673 --> 00:16:08.850
"Oh, maybe this is a
strategic opportunity,"

00:16:08.850 --> 00:16:12.437
and, you know, "if I tell
you it's five minutes later

00:16:12.437 --> 00:16:15.037
"then, I'll get a better
seat in the movie theater."

00:16:16.522 --> 00:16:17.355
So ...

00:16:18.710 --> 00:16:23.710
You know, I think we haven't,
we, behavioral economists,

00:16:24.320 --> 00:16:27.720
have not abandoned the standard model.

00:16:27.720 --> 00:16:31.980
What we've done is append things to it

00:16:33.610 --> 00:16:35.880
to make it more realistic,

00:16:35.880 --> 00:16:39.270
and the reason is there will not be one

00:16:39.270 --> 00:16:43.550
single parsimonious behavioral model

00:16:43.550 --> 00:16:46.660
anymore than there's a
single parsimonious model

00:16:46.660 --> 00:16:49.550
of psychology or political science,

00:16:49.550 --> 00:16:51.653
or international relations.

00:16:52.980 --> 00:16:55.940
Alas, there is no such model.

00:16:55.940 --> 00:16:59.910
- And, and so, the world,
when you get into the,

00:16:59.910 --> 00:17:03.970
to the world of behavior
and the way you do it,

00:17:03.970 --> 00:17:08.470
it's messier, and so,
therefore, that, your

00:17:08.470 --> 00:17:13.470
your goal is really not
to come up with a new

00:17:13.540 --> 00:17:18.040
theory, so much as really to ...

00:17:18.040 --> 00:17:20.553
Enhance, inform ...

00:17:22.205 --> 00:17:26.277
And show the errors of
economic theory, is that--

00:17:26.277 --> 00:17:28.670
- No, no, I mean, there are,

00:17:28.670 --> 00:17:29.803
I mean, I'm not ...

00:17:30.730 --> 00:17:34.620
I don't practice what economists

00:17:34.620 --> 00:17:37.053
generally would call economic theory,

00:17:38.250 --> 00:17:41.270
but many of my colleagues,

00:17:41.270 --> 00:17:44.750
several of my colleagues here at Berkeley,

00:17:44.750 --> 00:17:48.673
Matthew Rabin, Matthew
Bolton, Stefano DellaVigna ...

00:17:49.760 --> 00:17:53.410
Do engage precisely in

00:17:53.410 --> 00:17:56.740
standard looking economic theory,

00:17:56.740 --> 00:17:58.520
lots of mathematical models

00:17:59.500 --> 00:18:02.670
that are departures from
the standard theory.

00:18:02.670 --> 00:18:05.249
So, it is possible to write down

00:18:05.249 --> 00:18:08.910
mathematically rigorous economic theory

00:18:08.910 --> 00:18:11.570
that's behaviorally enlightened.

00:18:11.570 --> 00:18:16.570
What's true is that the model we have ...

00:18:16.670 --> 00:18:18.503
That explains ...

00:18:19.440 --> 00:18:22.830
Why we seem to lack self control

00:18:22.830 --> 00:18:26.010
might be a different one
than the model that we have

00:18:26.010 --> 00:18:29.500
that explains why if you're nice to me,

00:18:29.500 --> 00:18:31.453
I'm probably gonna be nice to you.

00:18:32.556 --> 00:18:35.390
- And I wanna be clear about something,

00:18:35.390 --> 00:18:39.960
if one were to look at your career

00:18:39.960 --> 00:18:43.040
and the insights you've
brought to the table,

00:18:43.040 --> 00:18:46.740
is what has propelled you

00:18:46.740 --> 00:18:49.590
the anomalies that you would up collecting

00:18:49.590 --> 00:18:53.940
so that you were, it
was through observation,

00:18:53.940 --> 00:18:58.940
or was there some philosophical
reason for your ...

00:19:01.620 --> 00:19:02.730
Looking in this direction?

00:19:02.730 --> 00:19:07.730
By that I mean, "Hey, people
are nicer than, than what ...

00:19:08.387 --> 00:19:13.380
"What economic, the way economic
theory posits they are?"

00:19:13.380 --> 00:19:16.410
- No, I don't think there was
some philosophical motivation,

00:19:16.410 --> 00:19:17.283
I think it was,

00:19:18.480 --> 00:19:22.040
I, it was, you know,
Milton Friedman used to,

00:19:22.040 --> 00:19:26.630
he had a book on, that he
called Positive Economics,

00:19:26.630 --> 00:19:29.130
and he used to say,

00:19:29.130 --> 00:19:31.160
in defending the rational model, actually,

00:19:31.160 --> 00:19:35.557
that, "We should judge a
theory by its predictive power.

00:19:35.557 --> 00:19:37.230
"Not by its assumptions."

00:19:37.230 --> 00:19:40.880
So, that's the way he
defended the seemingly

00:19:41.770 --> 00:19:44.130
artificiality of the assumptions,

00:19:44.130 --> 00:19:48.887
and I've, my approach as
been to say, "Okay, Milt.

00:19:48.887 --> 00:19:50.757
"Let's do that,

00:19:50.757 --> 00:19:55.400
"but look here, here, and
here, the model fails,"

00:19:55.400 --> 00:20:00.380
and so, I think what's
driven me over the years is

00:20:00.380 --> 00:20:02.680
it was first that list of anomalies,

00:20:02.680 --> 00:20:07.680
and then, second, the challenge
of convincing my colleagues

00:20:07.930 --> 00:20:10.940
that these were worth taking seriously.

00:20:10.940 --> 00:20:15.940
- You call yourself a
libertarian paternalist.

00:20:16.880 --> 00:20:19.410
What is that and what does it tell you

00:20:19.410 --> 00:20:23.940
about the way you look at public policy?

00:20:23.940 --> 00:20:27.103
- So, obviously, the phrase
sounds like an oxymoron,

00:20:28.160 --> 00:20:32.223
and it was kind of meant
to be a poke in the eye,

00:20:34.270 --> 00:20:39.270
but by libertarian, we mean
we like to devise policies

00:20:40.220 --> 00:20:42.830
that respect freedom of choice,

00:20:42.830 --> 00:20:45.093
and don't tell anybody
what they have to do.

00:20:45.980 --> 00:20:47.620
By ...

00:20:47.620 --> 00:20:51.190
Paternalism, we simply mean

00:20:51.190 --> 00:20:54.880
helping people achieve their own goals,

00:20:54.880 --> 00:20:57.570
as defined by themselves.

00:20:57.570 --> 00:21:01.660
So, suppose when we
walk out of the studio,

00:21:01.660 --> 00:21:03.657
somebody comes up to you and says,

00:21:03.657 --> 00:21:06.880
"Can you direct me
toward Telegraph Avenue?"

00:21:06.880 --> 00:21:08.210
If you're the sort of person

00:21:08.210 --> 00:21:11.090
that points them in the correct direction,

00:21:11.090 --> 00:21:12.473
then you're a paternalist.

00:21:13.460 --> 00:21:15.890
If you give them directions

00:21:15.890 --> 00:21:19.420
that may not be the actual,

00:21:19.420 --> 00:21:21.450
most direct route, but the route

00:21:21.450 --> 00:21:24.520
that they're least
likely to make a mistake,

00:21:24.520 --> 00:21:26.743
you also qualify as a paternalist,

00:21:27.870 --> 00:21:30.100
but this is a new kind of paternalism

00:21:30.100 --> 00:21:31.930
that doesn't require coercion.

00:21:31.930 --> 00:21:33.880
It's not a nanny state,

00:21:33.880 --> 00:21:38.700
it doesn't say, "You
shouldn't eat fatty food."

00:21:38.700 --> 00:21:42.257
It says, "If you'd like to lose weight,

00:21:42.257 --> 00:21:45.240
"here's an environment
that might help you."

00:21:45.240 --> 00:21:47.270
So, that's our goal

00:21:47.270 --> 00:21:49.683
is to create an ...

00:21:50.970 --> 00:21:54.430
A new sort of political framework,

00:21:54.430 --> 00:21:56.150
and our goal was to create one

00:21:56.150 --> 00:21:58.253
that was neither left nor right.

00:21:59.160 --> 00:22:01.150
So ...

00:22:01.150 --> 00:22:04.720
It has the libertarian flavor ...

00:22:04.720 --> 00:22:05.653
That ...

00:22:07.040 --> 00:22:08.243
Some on the right ...

00:22:10.407 --> 00:22:13.970
Seem to endorse, and it has the ...

00:22:13.970 --> 00:22:18.670
Caring about people that
many progressive endorse.

00:22:19.590 --> 00:22:22.513
It's not that different from
compassionate conservatism,

00:22:24.993 --> 00:22:28.390
a phrase that, from our distant past.

00:22:28.390 --> 00:22:32.350
- And what are there categories

00:22:32.350 --> 00:22:35.520
of, help us organize

00:22:35.520 --> 00:22:39.240
the way people fail ...

00:22:39.240 --> 00:22:41.500
In seeing their best interest,

00:22:41.500 --> 00:22:45.760
because what you and
your co-author go through

00:22:45.760 --> 00:22:49.010
is a list of, of kind of human failings

00:22:50.444 --> 00:22:53.477
that sort of are the foundation upon which

00:22:53.477 --> 00:22:54.971
you build your model.

00:22:54.971 --> 00:22:58.990
Are there characteristics
of all of these things?

00:22:58.990 --> 00:23:01.870
Are there certain kind
of general situations

00:23:01.870 --> 00:23:04.630
where these failings manifest themselves?

00:23:04.630 --> 00:23:07.470
- Well, we've talked about self-control.

00:23:07.470 --> 00:23:09.580
So ...

00:23:09.580 --> 00:23:13.100
That's one problem we humans face.

00:23:13.100 --> 00:23:18.100
So many of us eat too much,
drink too much, smoke too much,

00:23:18.820 --> 00:23:20.633
fail to save for retirement,

00:23:21.780 --> 00:23:26.780
and people admit to
that, if you ask people,

00:23:27.020 --> 00:23:31.380
if you, there have been surveys
conducted of participants

00:23:31.380 --> 00:23:33.057
in 401k plans, and you say,

00:23:33.057 --> 00:23:36.107
"Do you think you're saving too
much, just the right amount,

00:23:36.107 --> 00:23:37.210
"or too little?"

00:23:37.210 --> 00:23:39.560
Two-thirds will say too little.

00:23:39.560 --> 00:23:42.170
Now, an economist will make fun of that

00:23:42.170 --> 00:23:45.427
and say, Well, if they think
they're saving too little,

00:23:45.427 --> 00:23:48.760
"then, why aren't they saving more?"

00:23:48.760 --> 00:23:51.320
And the answer, if you ask people that,

00:23:51.320 --> 00:23:53.740
they'll say, "Well, I can't afford it."

00:23:53.740 --> 00:23:58.220
Okay, so how can we, that
seems to be an opportunity.

00:23:58.220 --> 00:24:02.980
If you have somebody who's
tried to quit smoking

00:24:02.980 --> 00:24:05.950
four times and has failed,

00:24:05.950 --> 00:24:08.230
well you have an opportunity
if you can come up

00:24:08.230 --> 00:24:10.870
with a better nicotine patch,

00:24:10.870 --> 00:24:15.870
then, you're able to help
people achieve their own goals.

00:24:16.960 --> 00:24:17.793
So ...

00:24:17.793 --> 00:24:21.970
One of, one of the success stories

00:24:21.970 --> 00:24:26.930
of, of behavioral economics
is a program a colleague,

00:24:26.930 --> 00:24:31.510
a former student of mine
Shlomo Benartzi and I devised

00:24:31.510 --> 00:24:33.290
to help people save more.

00:24:33.290 --> 00:24:35.513
We call it Save More Tomorrow,

00:24:36.710 --> 00:24:40.080
and it's based on the
premise that all of us

00:24:40.080 --> 00:24:44.330
have more self-control in
the future than we do now.

00:24:44.330 --> 00:24:49.230
So, many of us are
planning diets next month,

00:24:49.230 --> 00:24:54.060
and so, what we do is
give people an opportunity

00:24:54.060 --> 00:24:58.330
to commit themselves to saving more later

00:24:58.330 --> 00:24:59.573
when they can afford it.

00:25:01.540 --> 00:25:03.090
And ...

00:25:03.090 --> 00:25:06.020
We also link it to raises

00:25:06.020 --> 00:25:10.700
because we know people display
what we call loss aversion,

00:25:10.700 --> 00:25:14.780
they hate losing, they don't
like to see their pay go down.

00:25:14.780 --> 00:25:17.550
So, we allow people to sign up for a plan

00:25:17.550 --> 00:25:19.430
where every time they get a raise,

00:25:19.430 --> 00:25:22.570
they increase the amount
that they're contributing

00:25:22.570 --> 00:25:25.410
to the savings plan by a little bit

00:25:25.410 --> 00:25:27.840
in a way they don't even notice.

00:25:27.840 --> 00:25:29.810
The first company that adopted this,

00:25:29.810 --> 00:25:32.363
we tripled saving rates
in two and a half years.

00:25:33.320 --> 00:25:36.050
Nobody complained,
people signed up for it,

00:25:36.050 --> 00:25:37.083
they were happy.

00:25:37.960 --> 00:25:40.790
Now in, according to economic theory,

00:25:40.790 --> 00:25:43.020
our plan wouldn't have worked.

00:25:43.020 --> 00:25:44.600
No one would've signed up

00:25:46.020 --> 00:25:48.290
because they were already
saving just the right amount.

00:25:48.290 --> 00:25:49.640
What do they need this for?

00:25:50.720 --> 00:25:52.540
And ...

00:25:52.540 --> 00:25:54.970
So, that's an examp--

00:25:54.970 --> 00:25:59.680
A perfect example of a nudge,
and a successful nudge ...

00:25:59.680 --> 00:26:03.900
And, you know, for years,
people had struggled

00:26:03.900 --> 00:26:08.630
about the right tax incentives
to get people to save.

00:26:08.630 --> 00:26:10.030
The ...

00:26:10.030 --> 00:26:13.010
And this is gonna sound
totally banal and obvious

00:26:13.010 --> 00:26:14.240
when I say it,

00:26:14.240 --> 00:26:17.770
but it's a point that most
economists have missed,

00:26:17.770 --> 00:26:21.390
and it's the basic lesson
from social psychology,

00:26:21.390 --> 00:26:24.770
if you want to encourage
people to do something,

00:26:24.770 --> 00:26:26.573
make it easier for them to do it.

00:26:28.860 --> 00:26:29.833
Full stop.

00:26:30.830 --> 00:26:35.280
And if you do that, wonders will happen.

00:26:35.280 --> 00:26:38.070
- And it sounds like, in
this example you're giving

00:26:38.070 --> 00:26:41.110
that you're sort of realizing

00:26:41.110 --> 00:26:44.330
or you're accepting the reality

00:26:44.330 --> 00:26:46.010
that people are contradictory.

00:26:46.010 --> 00:26:49.270
They have the best
intentions for the future,

00:26:49.270 --> 00:26:51.680
but in the short term,
they may not act on them,

00:26:51.680 --> 00:26:56.130
and then, you're essentially,
finding the beauty

00:26:56.130 --> 00:26:59.010
in the default position, really,

00:26:59.010 --> 00:27:00.987
that you're, you're essentially saying,

00:27:00.987 --> 00:27:04.267
"Okay, let's do something in the future

00:27:04.267 --> 00:27:06.987
"that you do right now,
so it happens automatic.

00:27:06.987 --> 00:27:08.730
"It becomes the default position," yeah.

00:27:08.730 --> 00:27:13.020
- Exactly, and then, we
let inertia work its magic

00:27:13.890 --> 00:27:17.143
because people can withdraw
from the system anytime,

00:27:18.290 --> 00:27:22.177
but, you know, we lose
two or 3% a year, so ...

00:27:25.141 --> 00:27:27.053
Because they don't bother to,

00:27:28.420 --> 00:27:30.380
to pull out of the system.

00:27:30.380 --> 00:27:32.693
So, we always put a cap,

00:27:33.700 --> 00:27:36.100
you know, I sometimes joke
I could get people saving

00:27:36.100 --> 00:27:40.300
150% of their income, if
this worked long enough.

00:27:40.300 --> 00:27:43.757
They wonder, "Geez, well how
come I have to write a check

00:27:43.757 --> 00:27:45.357
"to Berkeley every month?"

00:27:46.510 --> 00:27:47.900
And, you know, I'm joking,

00:27:47.900 --> 00:27:52.370
but the point is that
inertia creates big problems,

00:27:52.370 --> 00:27:54.260
we get stuck in ruts,

00:27:54.260 --> 00:27:57.520
but then we can use inertia.

00:27:57.520 --> 00:27:59.500
So, one of the tools,

00:27:59.500 --> 00:28:02.080
one of our most powerful
tools in Nudge is,

00:28:02.080 --> 00:28:04.773
precisely, the power
of the default option,

00:28:06.100 --> 00:28:08.590
and default options are simply

00:28:08.590 --> 00:28:10.970
what happens if you do nothing.

00:28:10.970 --> 00:28:13.780
So, when you get a new computer,

00:28:13.780 --> 00:28:17.090
there are all sorts of
options that have been picked

00:28:17.090 --> 00:28:18.810
by the manufacturer,

00:28:18.810 --> 00:28:21.920
how long it takes for the
screensaver to come on

00:28:21.920 --> 00:28:22.883
if you do nothing.

00:28:24.360 --> 00:28:25.803
What is the screensaver?

00:28:27.140 --> 00:28:28.500
Well, if you're like me,

00:28:28.500 --> 00:28:31.760
you never changed any of those options.

00:28:31.760 --> 00:28:35.810
So, the computer manufacturer
has a lot of power

00:28:35.810 --> 00:28:37.103
And ...

00:28:37.950 --> 00:28:42.950
They can use that to
help you or to abuse you.

00:28:43.000 --> 00:28:45.420
It can be an option that

00:28:46.590 --> 00:28:51.210
you've automatically checked
to get emails once a week

00:28:51.210 --> 00:28:54.900
from the manufacturer trying
to sell you other things.

00:28:54.900 --> 00:28:58.700
You know, you would prefer
that box not be checked,

00:28:58.700 --> 00:29:02.340
but if you have to dig down
far enough to uncheck it,

00:29:02.340 --> 00:29:03.890
chances are, you won't.

00:29:03.890 --> 00:29:08.040
- Now, now, there's a
political problem here

00:29:08.040 --> 00:29:11.710
and it would seem to lead to resistance

00:29:11.710 --> 00:29:13.560
to what you're saying, that is,

00:29:13.560 --> 00:29:16.440
some actors in the market, for example,

00:29:16.440 --> 00:29:21.270
use human weakness and
human frailty to actually

00:29:22.470 --> 00:29:23.620
take advantage
- [Richard] Yes.

00:29:23.620 --> 00:29:24.690
- Of the consumer.

00:29:24.690 --> 00:29:26.537
Whereas, what you're saying is,

00:29:26.537 --> 00:29:29.617
"You know, let's look at
the greater good here,

00:29:29.617 --> 00:29:33.347
"and without interfering
with people's decision,

00:29:33.347 --> 00:29:34.743
"you know, let them see,

00:29:35.937 --> 00:29:39.660
"help them act on the basis
of the bigger picture,"

00:29:39.660 --> 00:29:43.370
but what about that, that
problem of the people

00:29:43.370 --> 00:29:47.310
one thinks here of the
whole mortgage industry

00:29:47.310 --> 00:29:48.400
- [Richard] Right.
- Where a lot--

00:29:48.400 --> 00:29:50.130
- [Richard] That's exactly
where I was gonna go.

00:29:50.130 --> 00:29:53.699
- Yeah.
- So, if you think about

00:29:53.699 --> 00:29:58.060
the crisis we're still living through,

00:29:58.060 --> 00:30:01.210
much of it was started with
people taking out mortgages

00:30:01.210 --> 00:30:02.950
that weren't good for them.

00:30:02.950 --> 00:30:07.040
Many of those were sold door-to-door

00:30:07.040 --> 00:30:11.493
in the old fashion,
high-pressure sales technique.

00:30:12.457 --> 00:30:17.350
"Psst, buddy, I can get you
a good deal on a mortgage,"

00:30:17.350 --> 00:30:18.950
and ...

00:30:18.950 --> 00:30:21.320
In quite deceptive ways.

00:30:21.320 --> 00:30:23.580
Now, you can say they're nudging.

00:30:23.580 --> 00:30:26.660
We didn't invent nudging,
and it's been around

00:30:26.660 --> 00:30:28.630
for thousands of years.

00:30:28.630 --> 00:30:29.813
So ...

00:30:30.770 --> 00:30:33.130
Well what can we do about that?

00:30:33.130 --> 00:30:36.603
In a sense, this is my co-author's job.

00:30:38.390 --> 00:30:39.730
He--
- Your co-author,

00:30:39.730 --> 00:30:40.800
tell our audience who your co--

00:30:40.800 --> 00:30:42.590
- My co-author is Cass Sunstein,

00:30:42.590 --> 00:30:46.280
a world-renown constitutional lawyer,

00:30:46.280 --> 00:30:49.130
whose current job is, in
the media, they call him

00:30:49.130 --> 00:30:50.920
The Regulation Czar.

00:30:50.920 --> 00:30:54.580
His official title is the
Director of the Office

00:30:54.580 --> 00:30:58.500
of Information and Regulatory
Affairs, part of the OMB.

00:30:58.500 --> 00:31:00.130
And ...

00:31:00.130 --> 00:31:05.130
He, his job is to make
sure that regulations,

00:31:05.440 --> 00:31:07.940
new regulations adopted by the government,

00:31:07.940 --> 00:31:11.183
do more good than harm, and ...

00:31:12.136 --> 00:31:12.969
So ...

00:31:13.880 --> 00:31:15.407
He might think about,

00:31:15.407 --> 00:31:18.630
"Okay, what are we going
to do about mortgages?"

00:31:18.630 --> 00:31:20.668
Now, we know there's just been a new

00:31:20.668 --> 00:31:23.900
consumer financial
protection agency created,

00:31:23.900 --> 00:31:26.980
and Elizabeth Warren and
another law professor

00:31:26.980 --> 00:31:29.030
has been given the job of figuring out

00:31:29.030 --> 00:31:32.233
what that agency will do, and ...

00:31:34.900 --> 00:31:38.923
Certainly, I would encourage
Cass and Elizabeth to ...

00:31:40.110 --> 00:31:43.357
Think about the mortgage market as,

00:31:43.357 --> 00:31:47.500
"How can we, how can we help

00:31:48.765 --> 00:31:53.720
"people who are choosing
mortgages make fewer blunders?"

00:31:53.720 --> 00:31:55.033
Without ...

00:31:56.305 --> 00:31:58.197
Going so far as to say,

00:31:58.197 --> 00:32:00.007
"You must take this kind of mortgage,

00:32:00.007 --> 00:32:01.850
"you must not take that,"

00:32:01.850 --> 00:32:06.800
and one of the things that
we advocate in the book,

00:32:06.800 --> 00:32:11.800
and that I've been continuing
to work on and advocate for,

00:32:11.930 --> 00:32:16.930
is a new kind of disclosure,
electronic disclosure,

00:32:18.040 --> 00:32:20.170
and let me explain what that means,

00:32:20.170 --> 00:32:23.730
when you get a mortgage, you get

00:32:23.730 --> 00:32:27.890
30 pages of forms that you have to sign,

00:32:27.890 --> 00:32:30.417
and you're signing something that says,

00:32:30.417 --> 00:32:35.417
"I've read this thing," and
anyone who's ever bought a house

00:32:35.960 --> 00:32:39.100
knows you go to a closing
and you spend an hour

00:32:39.100 --> 00:32:42.200
signing pieces of paper
that you don't read.

00:32:42.200 --> 00:32:46.550
Now, so that disclosure is useless,

00:32:46.550 --> 00:32:49.420
and there's always waves
that we're gonna have

00:32:49.420 --> 00:32:52.020
plain English disclosure,

00:32:52.020 --> 00:32:56.250
plain English disclosure for
really complicated products

00:32:56.250 --> 00:32:58.340
is a hopeless task.

00:32:58.340 --> 00:33:00.910
I don't need a plain English disclosure

00:33:00.910 --> 00:33:03.840
for how my iPhone works

00:33:03.840 --> 00:33:06.400
because it's obvious how it works.

00:33:06.400 --> 00:33:09.140
I press the button and it works.

00:33:09.140 --> 00:33:12.770
So, but I don't, no one can explain to me

00:33:12.770 --> 00:33:15.330
the complexities of that mortgage.

00:33:15.330 --> 00:33:17.380
So, here's what we advocate,

00:33:17.380 --> 00:33:18.980
when you apply for a mortgage,

00:33:18.980 --> 00:33:23.950
you would get an electronic
file, instead of 30 pages.

00:33:23.950 --> 00:33:26.780
Now, it's not that you
would look at that file,

00:33:26.780 --> 00:33:31.110
you would, with one click,
upload it to a website,

00:33:31.110 --> 00:33:34.920
a private website, that
would help explain it to you,

00:33:34.920 --> 00:33:37.970
and what we'd like is mortgage shopping

00:33:37.970 --> 00:33:40.920
to be as easy as airline shopping.

00:33:40.920 --> 00:33:43.710
So, we're all pretty good
now at finding the best fare

00:33:43.710 --> 00:33:46.340
between San Francisco and Chicago,

00:33:46.340 --> 00:33:49.410
'cause there's a whole array of websites

00:33:49.410 --> 00:33:51.550
that makes it easy for us to do that.

00:33:51.550 --> 00:33:53.120
There are even websites

00:33:53.120 --> 00:33:56.010
that search across the other websites.

00:33:56.010 --> 00:33:59.000
So, I think that market works pretty well,

00:33:59.000 --> 00:34:02.770
and it's because the airfares
are readily available.

00:34:02.770 --> 00:34:05.030
We wanna make everything
readily available.

00:34:05.030 --> 00:34:08.840
The Obama Administration has
been extremely aggressive

00:34:08.840 --> 00:34:10.820
in releasing data,

00:34:10.820 --> 00:34:14.510
and so has the Bay Area.

00:34:14.510 --> 00:34:18.160
One of the best things the
local government has done

00:34:18.160 --> 00:34:21.810
is BART, Bay Area Rapid Transit,

00:34:21.810 --> 00:34:26.160
has GPS locators in every train and bus.

00:34:26.160 --> 00:34:29.540
They just released that
data that they already had

00:34:29.540 --> 00:34:32.320
where every train and bus is,

00:34:32.320 --> 00:34:35.660
and then, third party
providers created apps.

00:34:35.660 --> 00:34:37.170
So, if you're standing at a bus stop

00:34:37.170 --> 00:34:39.210
wondering where the bus is,

00:34:39.210 --> 00:34:40.120
you can look in your app,

00:34:40.120 --> 00:34:42.547
and say, "It's actually
caught in a traffic jam.

00:34:42.547 --> 00:34:45.410
"It's four blocks down, it's
never gonna get to you."

00:34:45.410 --> 00:34:47.833
So, that cost the government nothing.

00:34:48.700 --> 00:34:50.530
They just released the data.

00:34:50.530 --> 00:34:54.090
The market provides the fancy app,

00:34:54.090 --> 00:34:59.090
and so, the administration
is busy releasing

00:34:59.660 --> 00:35:03.370
all kinds of data that was
hidden in file cabinets,

00:35:03.370 --> 00:35:06.450
and we'll let the market do its wonders.

00:35:06.450 --> 00:35:07.763
- Now, will ...

00:35:08.680 --> 00:35:13.030
What can be done about the resistance

00:35:14.360 --> 00:35:16.780
in, let's go back to
the case of mortgages,

00:35:16.780 --> 00:35:20.610
where, you know, there's
something of a fight

00:35:20.610 --> 00:35:24.670
to do the simple things that
might make things better?

00:35:24.670 --> 00:35:29.670
In your book, you propose,
with regard to credit cards,

00:35:29.820 --> 00:35:32.126
the notion of a recap, end of the year,

00:35:32.126 --> 00:35:35.110
a very simple state, this how
much interest you're paying,

00:35:35.110 --> 00:35:40.028
this is, this is what
your account looks like,

00:35:40.028 --> 00:35:43.297
which would allow you to
do shopping around to see,

00:35:43.297 --> 00:35:46.070
"Well, do I really want this credit card?"

00:35:46.070 --> 00:35:49.950
How do we win the battle
of making that happen?

00:35:49.950 --> 00:35:51.610
The insight is the right one,

00:35:51.610 --> 00:35:55.900
but is there a resistance
to having that happen?

00:35:55.900 --> 00:35:59.750
- Well, you know, I think
some players in the market

00:35:59.750 --> 00:36:01.870
will resist this.

00:36:01.870 --> 00:36:06.290
Regulators often get
captured by the industry.

00:36:06.290 --> 00:36:11.290
We've set up a new
regulator and I'm sure that

00:36:11.360 --> 00:36:14.740
the President and Ms. Warren

00:36:14.740 --> 00:36:16.740
have every intention not to be captured,

00:36:17.810 --> 00:36:20.193
but they will be under pressure.

00:36:21.300 --> 00:36:24.700
I think that this electronic
disclosure and recap

00:36:24.700 --> 00:36:27.663
is a middle course, so,

00:36:29.100 --> 00:36:32.740
not every provider may embrace it,

00:36:32.740 --> 00:36:37.740
but I think the honest ones
will, the big players will,

00:36:38.670 --> 00:36:42.070
and so, I have some hope that if

00:36:42.070 --> 00:36:47.070
the companies are to choose
between a regulator telling them

00:36:48.450 --> 00:36:51.130
they can't charge this sort of fee,

00:36:51.130 --> 00:36:53.680
but they can charge that sort of fee,

00:36:53.680 --> 00:36:57.720
in which, they play this
endless game of whack-a-mole,

00:36:57.720 --> 00:37:00.010
where the regulator abolishes some fee

00:37:00.010 --> 00:37:02.190
and they invent some new one,

00:37:02.190 --> 00:37:04.100
that if we just go to disclosure,

00:37:04.100 --> 00:37:09.100
then I think that the honest
firms will win, and I'm,

00:37:09.220 --> 00:37:10.183
so ...

00:37:11.310 --> 00:37:16.310
I'm, I make a pitch whenever
I speak to groups like that,

00:37:16.570 --> 00:37:21.380
that this is the sort of
regulation that you should want

00:37:22.530 --> 00:37:26.820
because it just creates good competition,

00:37:26.820 --> 00:37:30.030
and if you're in the business
of trying to fool people,

00:37:30.030 --> 00:37:33.083
then, by all means,
you should oppose this.

00:37:35.130 --> 00:37:39.180
- Let's look now at the
economic collapse of 2008,

00:37:40.260 --> 00:37:45.260
and are there, is there some
way of looking at that crisis

00:37:46.060 --> 00:37:50.990
that is especially informed
by behavioral economics?

00:37:50.990 --> 00:37:55.990
- Well, you know, one of
the behavioral problems

00:37:56.160 --> 00:37:57.860
we haven't talked about yet,

00:37:57.860 --> 00:38:01.443
and it's one of the most
important, is overconfidence.

00:38:02.290 --> 00:38:03.333
So ...

00:38:04.500 --> 00:38:07.400
The first week of, in my, I teach a course

00:38:07.400 --> 00:38:12.340
in decision-making in the
Chicago Booth School of Business,

00:38:12.340 --> 00:38:13.173
and ...

00:38:15.370 --> 00:38:17.880
Before the class even
starts, we have the students

00:38:17.880 --> 00:38:19.740
fill out a survey on the web,

00:38:19.740 --> 00:38:21.457
and one of the questions is,

00:38:21.457 --> 00:38:23.437
"Where in the gray distribution,

00:38:23.437 --> 00:38:25.907
"do you think you will come out?"

00:38:26.810 --> 00:38:31.720
And 90% of the class thinks
they'll be above the medium.

00:38:31.720 --> 00:38:35.810
Now, of course, that
can't happen, and slowly,

00:38:35.810 --> 00:38:38.850
during the course of the
quarter, they realize that

00:38:38.850 --> 00:38:43.330
that prediction was overconfident.

00:38:43.330 --> 00:38:48.330
Well, business people are
overconfident too, and ...

00:38:49.880 --> 00:38:53.290
You know, Tony Hayward ...

00:38:53.290 --> 00:38:58.290
The CEO of BP, said the risk
of that oil well exploding

00:38:58.900 --> 00:39:00.043
was one in a million.

00:39:01.640 --> 00:39:03.480
And ...

00:39:03.480 --> 00:39:06.450
I think the CEOs of Bear Stearns

00:39:06.450 --> 00:39:11.450
and Lehman Brothers never thought
this could happen to them,

00:39:12.830 --> 00:39:17.830
and so, I think there were
lots of human failings.

00:39:18.230 --> 00:39:20.310
Part of it came from complexity

00:39:21.170 --> 00:39:22.890
that they were dealing with

00:39:22.890 --> 00:39:27.650
ever-increasingly complex mechanisms,

00:39:27.650 --> 00:39:30.910
be it, a black box trading system

00:39:30.910 --> 00:39:34.310
or an oil well that's a mile down,

00:39:34.310 --> 00:39:39.310
and the CEOs didn't really understand

00:39:39.490 --> 00:39:44.200
what some of their bright
employees were doing,

00:39:44.200 --> 00:39:49.200
and here's where I think
my plea for more disclosure

00:39:49.350 --> 00:39:51.290
may have a hidden benefit.

00:39:51.290 --> 00:39:53.590
I think in some cases,

00:39:53.590 --> 00:39:57.253
firms will end up disclosing
things to themselves.

00:39:58.090 --> 00:40:02.453
So, going back to the government's ...

00:40:05.040 --> 00:40:09.550
Disclosure, one of the things
the Obama Administration did

00:40:09.550 --> 00:40:11.180
in their first few months,

00:40:11.180 --> 00:40:15.823
was release on CD-ROM, a record of ...

00:40:17.750 --> 00:40:22.170
The details of every
401K plan in the country.

00:40:22.170 --> 00:40:25.400
Now, there's a company
down in Southern California

00:40:26.330 --> 00:40:29.670
that rates 401K plans.

00:40:29.670 --> 00:40:32.380
They could, now that they got this data,

00:40:32.380 --> 00:40:34.670
they could rate every company.

00:40:34.670 --> 00:40:38.420
They've told me that some CEOs

00:40:38.420 --> 00:40:41.957
have called them up, and said,

00:40:41.957 --> 00:40:46.307
"How, we gotta 68 on your score,

00:40:46.307 --> 00:40:49.437
"we had no idea our plan was
so bad, we're gonna fix it."

00:40:50.570 --> 00:40:52.640
So, that's disclosure to yourself,

00:40:52.640 --> 00:40:54.190
it's like getting on the scale,

00:40:55.110 --> 00:40:58.270
and if you're forced to get on the scale,

00:40:58.270 --> 00:41:00.973
maybe you'll be nudged to go on a diet.

00:41:01.880 --> 00:41:06.860
- So, so it's interesting
because what you're saying

00:41:06.860 --> 00:41:11.860
is that with the insights
of behavioral economics,

00:41:12.000 --> 00:41:17.000
you're able to push the
system in positive directions

00:41:17.320 --> 00:41:18.873
just by ...

00:41:19.950 --> 00:41:21.920
Looking at ...

00:41:21.920 --> 00:41:24.390
The individual, the organization,

00:41:24.390 --> 00:41:27.127
the CEO, the consumer, and saying,

00:41:27.127 --> 00:41:30.467
"If you knew more, if you
had better information,

00:41:30.467 --> 00:41:33.517
"better feedback and systems that

00:41:33.517 --> 00:41:35.667
"sort of transcended human frailty,

00:41:35.667 --> 00:41:39.197
"that you would actually
not be undoing the market,

00:41:39.197 --> 00:41:40.620
"but, making it better."

00:41:40.620 --> 00:41:43.053
- Exactly, we're pro-market,

00:41:45.410 --> 00:41:47.150
but we'd like to create,

00:41:47.150 --> 00:41:51.410
market's work best for simple products.

00:41:51.410 --> 00:41:53.420
If you go to the farmer's market,

00:41:53.420 --> 00:41:58.420
and there are eight vendors
selling heirloom tomatoes,

00:41:58.490 --> 00:42:02.070
you know, you can do a pretty good job

00:42:02.070 --> 00:42:05.863
of picking the best tomatoes
at the price you wanna pay.

00:42:07.760 --> 00:42:10.023
You know, if you're picking a mortgage,

00:42:11.650 --> 00:42:14.830
most PhDs in Finance

00:42:14.830 --> 00:42:16.908
struggle with understanding

00:42:16.908 --> 00:42:19.240
all the terms of a mortgage.

00:42:19.240 --> 00:42:22.650
So, that market isn't gonna work as well

00:42:22.650 --> 00:42:23.893
as the farmer's market,

00:42:24.750 --> 00:42:28.855
and we wanna make
everything work as well as,

00:42:28.855 --> 00:42:32.650
as the example I gave before
of picking an airline ticket.

00:42:32.650 --> 00:42:35.380
That has become pretty easy,

00:42:35.380 --> 00:42:39.090
even though the pricing
strategies are very complicated.

00:42:39.090 --> 00:42:44.090
Right, I have now idea why
the, the flight at 10:00 a.m.

00:42:44.640 --> 00:42:48.692
costs half as much as
the flight at 4:00 p.m.,

00:42:48.692 --> 00:42:51.000
I don't know what the formula is,

00:42:51.000 --> 00:42:53.450
but I can see it there and I can decide

00:42:53.450 --> 00:42:56.180
whether I have the
flexibility to fly at 10,

00:42:56.180 --> 00:42:58.630
and I can save $400.

00:42:58.630 --> 00:43:03.250
- Now, what does this
tell us about the way

00:43:03.250 --> 00:43:08.250
we should act in the future,
with regard, to regulation?

00:43:09.300 --> 00:43:14.300
Is it just what you said,
namely, more information

00:43:14.910 --> 00:43:19.340
at our fingertips, or what else?

00:43:19.340 --> 00:43:23.470
I mean, because there
are people who want to

00:43:23.470 --> 00:43:28.330
exploit the system, you
know, that actually were

00:43:29.200 --> 00:43:32.140
sort of party to the making of the crisis.

00:43:32.140 --> 00:43:34.830
- Well, I think we wanna
put a spotlight on them.

00:43:34.830 --> 00:43:37.100
I think the danger here is,

00:43:37.100 --> 00:43:40.540
and people often accuse me of thinking

00:43:41.380 --> 00:43:45.560
that we want lots of
heavy-handed regulation,

00:43:45.560 --> 00:43:49.340
and they accuse me of not realizing

00:43:49.340 --> 00:43:51.973
that bureaucrats are humans too.

00:43:52.880 --> 00:43:57.600
So, let me say right here,
in front of the camera,

00:43:57.600 --> 00:44:00.390
that I understand bureaucrats,

00:44:00.390 --> 00:44:05.390
with the possible exception
of my co-author, are fallible,

00:44:06.230 --> 00:44:09.980
and, you know, I write a ...

00:44:09.980 --> 00:44:12.550
A monthly column in the New York Times,

00:44:12.550 --> 00:44:17.550
and I wrote one on World Cup refereeing.

00:44:20.100 --> 00:44:22.480
Precisely, to make this point.

00:44:22.480 --> 00:44:25.500
In the recent World Cup
there was an incident,

00:44:25.500 --> 00:44:28.920
in which, England was playing Germany,

00:44:28.920 --> 00:44:32.730
and a ball went one yard into the goal

00:44:32.730 --> 00:44:34.380
and everyone missed it,

00:44:34.380 --> 00:44:36.550
and there were thousands of replays

00:44:36.550 --> 00:44:40.810
where you see the ball landing
a yard over the goal line,

00:44:40.810 --> 00:44:45.810
and everyone missing it, and
the thrust of my essay was,

00:44:47.290 --> 00:44:49.420
if you were designing the rules,

00:44:49.420 --> 00:44:51.620
the first thing you
would take into account,

00:44:51.620 --> 00:44:54.010
that the referee is fallible.

00:44:54.010 --> 00:44:56.030
He can only be in one place at a time,

00:44:56.030 --> 00:44:57.970
he can only see in one direction.

00:44:57.970 --> 00:45:02.490
So, we need to create rules
that take into account

00:45:02.490 --> 00:45:03.850
that everyone's fallible.

00:45:03.850 --> 00:45:06.110
Larry Summers said something very smart

00:45:06.110 --> 00:45:08.890
about the new financial regulations,

00:45:08.890 --> 00:45:13.777
and he said, "The goal should
be to create regulations

00:45:13.777 --> 00:45:17.187
"that don't require anyone
to get any smarter,"

00:45:18.220 --> 00:45:19.920
and by that, he meant,

00:45:19.920 --> 00:45:22.293
both the consumers and the regulators.

00:45:23.230 --> 00:45:27.020
So, that's my goal, is to create a world

00:45:27.020 --> 00:45:31.420
where we don't need to be
geniuses to pick a mortgage,

00:45:31.420 --> 00:45:34.620
and we don't need to be geniuses

00:45:34.620 --> 00:45:38.853
to run the Consumer
Finance Protection Agency.

00:45:40.130 --> 00:45:44.900
- And what sort of politics
make that possible?

00:45:44.900 --> 00:45:48.410
Because you've been, your
book has been attacked,

00:45:48.410 --> 00:45:53.370
I assume, by some people who view you

00:45:53.370 --> 00:45:55.813
a person who only wants to nudge,

00:45:56.820 --> 00:45:59.340
as a socialist, basically.

00:45:59.340 --> 00:46:01.453
- Yeah, that's quite funny because ...

00:46:04.130 --> 00:46:07.210
Well, Glenn Beck, in particular,

00:46:07.210 --> 00:46:10.363
thinks that we have some evil agenda.

00:46:11.840 --> 00:46:13.890
He's referred to my co-author

00:46:13.890 --> 00:46:16.430
as the most dangerous man in America,

00:46:16.430 --> 00:46:18.640
which I can assure you he's not.

00:46:18.640 --> 00:46:22.253
There certainly must be someone
more dangerous than him,

00:46:24.370 --> 00:46:26.980
and I would nominate him, actually,

00:46:26.980 --> 00:46:29.330
as one of the least
dangerous men in America,

00:46:29.330 --> 00:46:33.770
he's, if he has a flaw, it's
moderate, he's too moderate,

00:46:33.770 --> 00:46:35.360
but ...

00:46:35.360 --> 00:46:37.290
But, you know, the irony is,

00:46:37.290 --> 00:46:40.490
that the new government in the U.K.

00:46:40.490 --> 00:46:43.260
that's led by the Conservative Party,

00:46:43.260 --> 00:46:46.703
has set up a Nudge Unit in Number 10.

00:46:48.310 --> 00:46:51.900
It's, informally, it's
called the Nudge Unit,

00:46:51.900 --> 00:46:56.120
the formal name for it is
the Behavioral Insight Team,

00:46:56.120 --> 00:46:58.838
and I'm an advisor to that team,

00:46:58.838 --> 00:47:03.030
and our mandate is to just think of ways

00:47:03.030 --> 00:47:05.870
that we can improve what government does

00:47:05.870 --> 00:47:09.110
using behavioral science techniques.

00:47:09.110 --> 00:47:13.110
So, I don't think that this
is a left versus right.

00:47:13.110 --> 00:47:15.660
I'm just back from a trip to Korea,

00:47:15.660 --> 00:47:18.010
where the President of
Korea, read the book

00:47:18.010 --> 00:47:20.390
and assigned it to his cabinet,

00:47:20.390 --> 00:47:22.810
and he is ...

00:47:22.810 --> 00:47:27.810
From an extremely conservative
party, a former businessman.

00:47:29.774 --> 00:47:30.607
So ...

00:47:31.570 --> 00:47:35.470
We set out to write a book that
was neither left nor right,

00:47:35.470 --> 00:47:39.690
and, to some extent, we've achieved,

00:47:39.690 --> 00:47:44.221
but there's, with a few exceptions.

00:47:44.221 --> 00:47:49.221
- If students were
watching this interview,

00:47:49.430 --> 00:47:52.470
they wanna go into
economics, public policy,

00:47:52.470 --> 00:47:56.160
how would you advise them to prepare

00:47:56.160 --> 00:47:58.193
for a career ...

00:48:00.500 --> 00:48:04.370
So that they're able to take advantage of

00:48:06.430 --> 00:48:10.400
psychology and the kind of
analysis that you are doing,

00:48:10.400 --> 00:48:13.180
while remaining in economics?

00:48:13.180 --> 00:48:17.020
- Well, you mean, aside
from reading our book

00:48:17.020 --> 00:48:18.880
cover to cover?
- [Harry] Yeah, yeah, right--

00:48:18.880 --> 00:48:20.160
- Several times.

00:48:20.160 --> 00:48:22.110
- [Harry] And making it
available to all their students.

00:48:22.110 --> 00:48:23.540
- Yes, assigning.

00:48:23.540 --> 00:48:25.010
I think that's one of the,

00:48:25.010 --> 00:48:30.010
I've been on a campaign to
replace the Bible in hotel rooms

00:48:31.070 --> 00:48:34.350
with copies of Nudge,
Gideon's Nudge, I think,

00:48:34.350 --> 00:48:37.780
but, you know, I think that

00:48:37.780 --> 00:48:41.070
many leading public policy schools

00:48:41.070 --> 00:48:43.660
are starting to incorporate
psychology courses

00:48:43.660 --> 00:48:45.280
in their curriculum,

00:48:45.280 --> 00:48:48.530
and I think that makes really great sense,

00:48:48.530 --> 00:48:51.930
and it's not all economics,

00:48:51.930 --> 00:48:55.630
it's very important that
policymakers understand economics,

00:48:55.630 --> 00:48:58.020
but what we're trying
to convince them of is

00:48:58.020 --> 00:48:59.680
that it's just as important

00:48:59.680 --> 00:49:02.980
that they have some
understanding of human nature,

00:49:02.980 --> 00:49:06.090
- And, and that ...

00:49:06.090 --> 00:49:10.490
I think you're saying that
that comes from observation,

00:49:10.490 --> 00:49:13.143
and not necessarily course work.

00:49:14.160 --> 00:49:18.230
- No, look, I think I've
learned a tremendous amount

00:49:18.230 --> 00:49:21.320
from reading the work of psychologists

00:49:23.710 --> 00:49:28.110
like the Danny Kahneman and Amos Tversky,

00:49:28.110 --> 00:49:33.110
and I teach a course that's
more psychology than economics,

00:49:34.340 --> 00:49:38.390
and I think there should
be a course like that

00:49:38.390 --> 00:49:40.610
in every public policy program.

00:49:40.610 --> 00:49:41.923
- Well, on that note,

00:49:43.120 --> 00:49:44.690
Professor Thaler, I wanna thank you

00:49:44.690 --> 00:49:45.810
for being on our program,

00:49:45.810 --> 00:49:47.610
and I'm gonna show your book,

00:49:47.610 --> 00:49:52.150
'cause everybody is gonna
wanna go out and buy it.

00:49:52.150 --> 00:49:53.900
- Right, Christmas is coming up.

00:49:53.900 --> 00:49:56.830
You know, this is a perfect
Christmas gift (laughs).

00:49:56.830 --> 00:49:58.750
- Thank you very much for being here,

00:49:58.750 --> 00:50:00.880
and thank you very much for joining us

00:50:00.880 --> 00:50:03.904
for this Conversation with History.

00:50:03.904 --> 00:50:06.904
(electronic music)

