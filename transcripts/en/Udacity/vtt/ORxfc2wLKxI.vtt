WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:03.560
Now that we talked about doing
reinforcement learning in a POMDP

00:00:03.560 --> 00:00:07.220
setting, we're actually now going to
talk about a much weirder combination.

00:00:07.220 --> 00:00:10.370
We're going to talk about how we can
think of reinforcement learning as

00:00:10.370 --> 00:00:12.498
actually being itself a POMDP.

00:00:12.498 --> 00:00:14.360
&gt;&gt; Okay.
&gt;&gt; This is an area that

00:00:14.360 --> 00:00:16.420
is often referred to as
Bayesian Reinforcement Learning.

00:00:16.420 --> 00:00:19.180
The idea is that we're going to keep

00:00:19.180 --> 00:00:22.210
a a Bayesian posterior over
possible MDPs that we're in.

00:00:22.210 --> 00:00:26.690
And we're going to use that to actually
optimally balance exploration and

00:00:26.690 --> 00:00:27.330
exploitations.

00:00:27.330 --> 00:00:29.690
We're actually going to
behave optimally and

00:00:29.690 --> 00:00:31.444
reinforcement learning becomes planning.

00:00:31.444 --> 00:00:34.000
&gt;&gt; Okay, so does it mean we're
going to use Bayes rule?

00:00:34.000 --> 00:00:37.550
&gt;&gt; Well I guess technically in that
whenever your compute a posterior,

00:00:37.550 --> 00:00:39.359
you're actually doing
something like Bayes rule.

00:00:39.359 --> 00:00:41.470
Yeah, but maybe this is not so clear.

00:00:41.470 --> 00:00:42.860
Let me try to draw a picture.

00:00:42.860 --> 00:00:45.210
So I'm hoping this example might
make things a little bit clearer.

00:00:45.210 --> 00:00:49.140
So let's consider a really simple
reinforcement learning problem.

00:00:49.140 --> 00:00:54.020
We've got two states, state 1 and
state 2, two actions a black one and

00:00:54.020 --> 00:00:56.180
a, I don't know, purplish kind of one.

00:00:56.180 --> 00:00:59.280
And if we were doing reinforcement
learning in this environment, and

00:00:59.280 --> 00:01:01.740
you were say the reinforcement learner,
what would you do?

00:01:01.740 --> 00:01:04.200
Let's say we're in state 1,
what would you do?

00:01:04.200 --> 00:01:06.221
Would it be better to do purple or
black?

00:01:06.221 --> 00:01:07.360
&gt;&gt; I don't know.

00:01:07.360 --> 00:01:08.568
&gt;&gt; Okay so then what would you do?

00:01:08.568 --> 00:01:09.550
&gt;&gt; I'd pick one of them.

00:01:09.550 --> 00:01:12.331
&gt;&gt; All right, so
let's say you pick black and

00:01:12.331 --> 00:01:14.732
then we would observe a transition.

00:01:14.732 --> 00:01:16.560
And ultimately we can sort
of build out a model or

00:01:16.560 --> 00:01:19.630
use queue learning or something
like that to work out the values.

00:01:19.630 --> 00:01:23.090
So what I'd like to do is actually
try to have you think about this

00:01:23.090 --> 00:01:24.370
like a POMDP.

00:01:24.370 --> 00:01:26.760
And I think a simpler
way to show you that

00:01:26.760 --> 00:01:31.120
would be instead of imagining
all possible MDPs this could be,

00:01:31.120 --> 00:01:33.790
let's just pretend it comes
from of a small finite set.

00:01:33.790 --> 00:01:35.996
Okay, are there any small
sets that aren't finite?

00:01:35.996 --> 00:01:37.110
&gt;&gt; No.
&gt;&gt; Good,

00:01:37.110 --> 00:01:38.270
then I think we're on the same page.

00:01:38.270 --> 00:01:39.390
&gt;&gt; All right, then I agree with you.

00:01:39.390 --> 00:01:40.760
So here's the set that I had in mind,

00:01:40.760 --> 00:01:43.820
we've got three possible worlds
that we're in A, B and C.

00:01:43.820 --> 00:01:45.590
So these are three possible MDPs, right?

00:01:45.590 --> 00:01:50.490
&gt;&gt; Okay and they all look exactly the
same to me except where you get reward.

00:01:50.490 --> 00:01:53.510
&gt;&gt; Yeah, so it just so happens that
the actions all do the same thing in all

00:01:53.510 --> 00:01:55.210
three of these and we know that.

00:01:55.210 --> 00:01:56.600
We just don't know where the reward is.

00:01:56.600 --> 00:01:59.640
Is the reward something you get for
purple when you go from state 2?

00:01:59.640 --> 00:02:02.290
Is it something that you get for
purple from state 1?

00:02:02.290 --> 00:02:06.310
Or is it something that you get from
taking the black action from state 1?

00:02:06.310 --> 00:02:06.930
And we don't know.

00:02:06.930 --> 00:02:08.892
Now if you had to solve this
reinforcement learning problem,

00:02:08.892 --> 00:02:10.791
it's kind of a reinforcement
learning a problem, right?

00:02:10.791 --> 00:02:13.700
You're in some state you get to
see what state you're in, 1 or 1.

00:02:13.700 --> 00:02:15.980
You can try actions black or purple.

00:02:15.980 --> 00:02:18.960
And what you're trying to do is actually
figure out how to maximize reward.

00:02:18.960 --> 00:02:21.590
But now that it's just
a finite set of possibilities,

00:02:21.590 --> 00:02:23.090
we can think about this like a POMDP.

00:02:23.090 --> 00:02:25.300
&gt;&gt; Sure and in fact, I think this
one's pretty straightforward.

00:02:25.300 --> 00:02:28.350
If I'm in state 2,
I know what I should do.

00:02:28.350 --> 00:02:28.850
&gt;&gt; And what is that?

00:02:28.850 --> 00:02:30.280
&gt;&gt; I should take the purple action.

00:02:31.380 --> 00:02:31.970
&gt;&gt; And why is that?

00:02:31.970 --> 00:02:33.510
Explain.
&gt;&gt; Because I immediately either

00:02:33.510 --> 00:02:38.270
get reward for it or I know that I
get reward for being in state 1.

00:02:38.270 --> 00:02:40.170
And then when I'm in state 1,

00:02:40.170 --> 00:02:42.450
well actually it's not clear
which one I should do.

00:02:42.450 --> 00:02:46.040
&gt;&gt; All right, so
let's well let's just do two things.

00:02:46.040 --> 00:02:50.055
Let's start off by just tracking belief
states in this because I think that's

00:02:50.055 --> 00:02:52.205
already kind of an interesting
problem in and of itself.

00:02:52.205 --> 00:02:55.473
What we're thinking about here is
that we're in POMDP with six states.

00:02:55.473 --> 00:02:58.155
Often state 1, but
we don't know which MDP were in,

00:02:58.155 --> 00:02:59.973
then the belief state is going to
look something like this.

00:02:59.973 --> 00:03:00.511
Agreed?

00:03:00.511 --> 00:03:03.275
&gt;&gt; Agreed, if this is where we're
starting, we know nothing else, and

00:03:03.275 --> 00:03:06.760
we haven't done anything yet,
then we can be in any of those states,

00:03:06.760 --> 00:03:08.320
presumably with equal probability.

00:03:08.320 --> 00:03:11.990
&gt;&gt; Good, right, so
now the question is, if you were to do

00:03:11.990 --> 00:03:15.970
the purple action from here,
what would your expected reward be?

00:03:15.970 --> 00:03:20.250
&gt;&gt; My expected the reward
would be one-third.

00:03:20.250 --> 00:03:22.180
&gt;&gt; Right, so
if we happen to have been in MDP B,

00:03:22.180 --> 00:03:25.910
which we don't know if we are and we
took the action purple, we'd get a one.

00:03:25.910 --> 00:03:28.030
If not, then we get zero.

00:03:28.030 --> 00:03:30.260
But what also would
happen if we get the one?

00:03:30.260 --> 00:03:31.720
What's our new belief state?

00:03:31.720 --> 00:03:32.630
&gt;&gt; If we get the one?

00:03:32.630 --> 00:03:35.220
&gt;&gt; Yeah.
&gt;&gt; Well then we know that we are in B1.

00:03:35.220 --> 00:03:36.830
&gt;&gt; Good, B1, right.

00:03:36.830 --> 00:03:38.060
So we're in this state, so

00:03:38.060 --> 00:03:40.640
the belief state becomes basically
fully observable at this point.

00:03:40.640 --> 00:03:42.190
We know which MDP, we're in.

00:03:42.190 --> 00:03:43.820
And we know what state
of that MDP we're in.

00:03:43.820 --> 00:03:46.650
And we can just execute
the optimal policy for that MDP,

00:03:46.650 --> 00:03:49.357
which in this case, as you might guess,
is just purple all the time.

00:03:49.357 --> 00:03:52.916
&gt;&gt; Purple all the time,
purple all the time.

00:03:52.916 --> 00:03:53.900
Yeah, I agree.
&gt;&gt; Excellent.

00:03:53.900 --> 00:03:56.660
So let's make sure you get this,
we'll do a little quizzy kind of thing.

00:03:56.660 --> 00:03:57.160
&gt;&gt; Okay.

