WEBVTT
Kind: captions
Language: en

00:00:00.149 --> 00:00:02.000
 
when training a neural network one of

00:00:02.000 --> 00:00:02.010
when training a neural network one of
 

00:00:02.010 --> 00:00:03.560
when training a neural network one of
the techniques to speed up your training

00:00:03.560 --> 00:00:03.570
the techniques to speed up your training
 

00:00:03.570 --> 00:00:06.110
the techniques to speed up your training
is if you normalize your inputs let's

00:00:06.110 --> 00:00:06.120
is if you normalize your inputs let's
 

00:00:06.120 --> 00:00:08.089
is if you normalize your inputs let's
see what that means let's see the

00:00:08.089 --> 00:00:08.099
see what that means let's see the
 

00:00:08.099 --> 00:00:10.459
see what that means let's see the
training sets with two input features so

00:00:10.459 --> 00:00:10.469
training sets with two input features so
 

00:00:10.469 --> 00:00:12.799
training sets with two input features so
the input features X are two-dimensional

00:00:12.799 --> 00:00:12.809
the input features X are two-dimensional
 

00:00:12.809 --> 00:00:14.870
the input features X are two-dimensional
and here's a scatterplot of your

00:00:14.870 --> 00:00:14.880
and here's a scatterplot of your
 

00:00:14.880 --> 00:00:18.040
and here's a scatterplot of your
training set normalizing your inputs

00:00:18.040 --> 00:00:18.050
training set normalizing your inputs
 

00:00:18.050 --> 00:00:22.300
training set normalizing your inputs
corresponds to two steps the first is to

00:00:22.300 --> 00:00:22.310
corresponds to two steps the first is to
 

00:00:22.310 --> 00:00:26.359
corresponds to two steps the first is to
subtract out or to zero out the mean so

00:00:26.359 --> 00:00:26.369
subtract out or to zero out the mean so
 

00:00:26.369 --> 00:00:32.350
subtract out or to zero out the mean so
you set mu equals 1 over m sum over I of

00:00:32.350 --> 00:00:32.360
you set mu equals 1 over m sum over I of
 

00:00:32.360 --> 00:00:36.560
you set mu equals 1 over m sum over I of
X I so this is a vector and then X gets

00:00:36.560 --> 00:00:36.570
X I so this is a vector and then X gets
 

00:00:36.570 --> 00:00:38.569
X I so this is a vector and then X gets
set as X minus mu for every training

00:00:38.569 --> 00:00:38.579
set as X minus mu for every training
 

00:00:38.579 --> 00:00:41.299
set as X minus mu for every training
example so this means you just move the

00:00:41.299 --> 00:00:41.309
example so this means you just move the
 

00:00:41.309 --> 00:00:43.970
example so this means you just move the
training set until it has zero mean and

00:00:43.970 --> 00:00:43.980
training set until it has zero mean and
 

00:00:43.980 --> 00:00:48.709
training set until it has zero mean and
then the second step is to normalize the

00:00:48.709 --> 00:00:48.719
then the second step is to normalize the
 

00:00:48.719 --> 00:00:50.600
then the second step is to normalize the
variances so notice here that the

00:00:50.600 --> 00:00:50.610
variances so notice here that the
 

00:00:50.610 --> 00:00:53.270
variances so notice here that the
feature x1 has a much larger variance

00:00:53.270 --> 00:00:53.280
feature x1 has a much larger variance
 

00:00:53.280 --> 00:00:55.970
feature x1 has a much larger variance
than the feature x2 here so what we do

00:00:55.970 --> 00:00:55.980
than the feature x2 here so what we do
 

00:00:55.980 --> 00:01:01.939
than the feature x2 here so what we do
is set Sigma equals 1 over m sum of X I

00:01:01.939 --> 00:01:01.949
is set Sigma equals 1 over m sum of X I
 

00:01:01.949 --> 00:01:05.630
is set Sigma equals 1 over m sum of X I
star saw two I guess this is element

00:01:05.630 --> 00:01:05.640
star saw two I guess this is element
 

00:01:05.640 --> 00:01:08.420
star saw two I guess this is element
wise squaring and so now Sigma squared

00:01:08.420 --> 00:01:08.430
wise squaring and so now Sigma squared
 

00:01:08.430 --> 00:01:11.660
wise squaring and so now Sigma squared
is a vector with the variances of each

00:01:11.660 --> 00:01:11.670
is a vector with the variances of each
 

00:01:11.670 --> 00:01:13.789
is a vector with the variances of each
of the features and notice we've already

00:01:13.789 --> 00:01:13.799
of the features and notice we've already
 

00:01:13.799 --> 00:01:16.789
of the features and notice we've already
subtracted out the means so X I squared

00:01:16.789 --> 00:01:16.799
subtracted out the means so X I squared
 

00:01:16.799 --> 00:01:18.920
subtracted out the means so X I squared
element Y squared is just the variances

00:01:18.920 --> 00:01:18.930
element Y squared is just the variances
 

00:01:18.930 --> 00:01:21.499
element Y squared is just the variances
and you take you an example and divide

00:01:21.499 --> 00:01:21.509
and you take you an example and divide
 

00:01:21.509 --> 00:01:24.080
and you take you an example and divide
it by you know this vector Sigma squared

00:01:24.080 --> 00:01:24.090
it by you know this vector Sigma squared
 

00:01:24.090 --> 00:01:27.580
it by you know this vector Sigma squared
and so in pictures you end up with this

00:01:27.580 --> 00:01:27.590
and so in pictures you end up with this
 

00:01:27.590 --> 00:01:31.640
and so in pictures you end up with this
where now the variance of x1 and x2 are

00:01:31.640 --> 00:01:31.650
where now the variance of x1 and x2 are
 

00:01:31.650 --> 00:01:37.010
where now the variance of x1 and x2 are
both equal to 1 oh into one tip if you

00:01:37.010 --> 00:01:37.020
both equal to 1 oh into one tip if you
 

00:01:37.020 --> 00:01:39.859
both equal to 1 oh into one tip if you
use this to scale your training data

00:01:39.859 --> 00:01:39.869
use this to scale your training data
 

00:01:39.869 --> 00:01:43.940
use this to scale your training data
then use the same mu and Sigma squared

00:01:43.940 --> 00:01:43.950
then use the same mu and Sigma squared
 

00:01:43.950 --> 00:01:47.840
then use the same mu and Sigma squared
to normalize your test set right in

00:01:47.840 --> 00:01:47.850
to normalize your test set right in
 

00:01:47.850 --> 00:01:50.300
to normalize your test set right in
particular you don't want to normalize

00:01:50.300 --> 00:01:50.310
particular you don't want to normalize
 

00:01:50.310 --> 00:01:51.590
particular you don't want to normalize
the training set and the test set

00:01:51.590 --> 00:01:51.600
the training set and the test set
 

00:01:51.600 --> 00:01:54.050
the training set and the test set
differently whatever this value is and

00:01:54.050 --> 00:01:54.060
differently whatever this value is and
 

00:01:54.060 --> 00:01:56.690
differently whatever this value is and
whatever this value is use them in you

00:01:56.690 --> 00:01:56.700
whatever this value is use them in you
 

00:01:56.700 --> 00:01:59.330
whatever this value is use them in you
know these two formulas so that you

00:01:59.330 --> 00:01:59.340
know these two formulas so that you
 

00:01:59.340 --> 00:02:01.310
know these two formulas so that you
scare your test set in exactly the same

00:02:01.310 --> 00:02:01.320
scare your test set in exactly the same
 

00:02:01.320 --> 00:02:03.230
scare your test set in exactly the same
way rather than estimating mu and Sigma

00:02:03.230 --> 00:02:03.240
way rather than estimating mu and Sigma
 

00:02:03.240 --> 00:02:05.060
way rather than estimating mu and Sigma
squared separately on your training set

00:02:05.060 --> 00:02:05.070
squared separately on your training set
 

00:02:05.070 --> 00:02:05.730
squared separately on your training set
and test

00:02:05.730 --> 00:02:05.740
and test
 

00:02:05.740 --> 00:02:08.340
and test
because you want your data both training

00:02:08.340 --> 00:02:08.350
because you want your data both training
 

00:02:08.350 --> 00:02:10.770
because you want your data both training
and test examples to go through the same

00:02:10.770 --> 00:02:10.780
and test examples to go through the same
 

00:02:10.780 --> 00:02:13.380
and test examples to go through the same
transformation defined by the same Mew

00:02:13.380 --> 00:02:13.390
transformation defined by the same Mew
 

00:02:13.390 --> 00:02:15.720
transformation defined by the same Mew
and Sigma squared calculated on your

00:02:15.720 --> 00:02:15.730
and Sigma squared calculated on your
 

00:02:15.730 --> 00:02:18.270
and Sigma squared calculated on your
training data so why do we do this why

00:02:18.270 --> 00:02:18.280
training data so why do we do this why
 

00:02:18.280 --> 00:02:20.160
training data so why do we do this why
do we want to normalize the input

00:02:20.160 --> 00:02:20.170
do we want to normalize the input
 

00:02:20.170 --> 00:02:22.260
do we want to normalize the input
features recall that the cost function

00:02:22.260 --> 00:02:22.270
features recall that the cost function
 

00:02:22.270 --> 00:02:25.830
features recall that the cost function
is defined as written on top right it

00:02:25.830 --> 00:02:25.840
is defined as written on top right it
 

00:02:25.840 --> 00:02:28.650
is defined as written on top right it
turns out that if you use unnormalized

00:02:28.650 --> 00:02:28.660
turns out that if you use unnormalized
 

00:02:28.660 --> 00:02:31.199
turns out that if you use unnormalized
input features is more likely that your

00:02:31.199 --> 00:02:31.209
input features is more likely that your
 

00:02:31.209 --> 00:02:33.000
input features is more likely that your
cost function will look like this at a

00:02:33.000 --> 00:02:33.010
cost function will look like this at a
 

00:02:33.010 --> 00:02:36.479
cost function will look like this at a
very squished out bow very elongated

00:02:36.479 --> 00:02:36.489
very squished out bow very elongated
 

00:02:36.489 --> 00:02:39.060
very squished out bow very elongated
cost function where you know the minimum

00:02:39.060 --> 00:02:39.070
cost function where you know the minimum
 

00:02:39.070 --> 00:02:40.740
cost function where you know the minimum
you're trying to find this maybe over

00:02:40.740 --> 00:02:40.750
you're trying to find this maybe over
 

00:02:40.750 --> 00:02:43.259
you're trying to find this maybe over
there but if you're beakers are on very

00:02:43.259 --> 00:02:43.269
there but if you're beakers are on very
 

00:02:43.269 --> 00:02:45.740
there but if you're beakers are on very
different scales say the feature x1

00:02:45.740 --> 00:02:45.750
different scales say the feature x1
 

00:02:45.750 --> 00:02:49.370
different scales say the feature x1
ranges from 1 to 1000 and the feature x2

00:02:49.370 --> 00:02:49.380
ranges from 1 to 1000 and the feature x2
 

00:02:49.380 --> 00:02:53.759
ranges from 1 to 1000 and the feature x2
ranges from 0 to 1 then it turns out

00:02:53.759 --> 00:02:53.769
ranges from 0 to 1 then it turns out
 

00:02:53.769 --> 00:02:56.460
ranges from 0 to 1 then it turns out
that ratio or the range of values for

00:02:56.460 --> 00:02:56.470
that ratio or the range of values for
 

00:02:56.470 --> 00:03:00.150
that ratio or the range of values for
the parameters w1 and w2 will end up

00:03:00.150 --> 00:03:00.160
the parameters w1 and w2 will end up
 

00:03:00.160 --> 00:03:02.340
the parameters w1 and w2 will end up
taking on very different values and so

00:03:02.340 --> 00:03:02.350
taking on very different values and so
 

00:03:02.350 --> 00:03:05.370
taking on very different values and so
maybe these axes should be w1 and w2

00:03:05.370 --> 00:03:05.380
maybe these axes should be w1 and w2
 

00:03:05.380 --> 00:03:07.710
maybe these axes should be w1 and w2
probe intuition I'll plot W and B be a

00:03:07.710 --> 00:03:07.720
probe intuition I'll plot W and B be a
 

00:03:07.720 --> 00:03:09.180
probe intuition I'll plot W and B be a
cost function can be a very elongated

00:03:09.180 --> 00:03:09.190
cost function can be a very elongated
 

00:03:09.190 --> 00:03:11.850
cost function can be a very elongated
bow like that so if you plot the

00:03:11.850 --> 00:03:11.860
bow like that so if you plot the
 

00:03:11.860 --> 00:03:14.940
bow like that so if you plot the
contours of this function you can have a

00:03:14.940 --> 00:03:14.950
contours of this function you can have a
 

00:03:14.950 --> 00:03:16.850
contours of this function you can have a
very elongated function like that

00:03:16.850 --> 00:03:16.860
very elongated function like that
 

00:03:16.860 --> 00:03:19.259
very elongated function like that
whereas if you normalize the features

00:03:19.259 --> 00:03:19.269
whereas if you normalize the features
 

00:03:19.269 --> 00:03:22.410
whereas if you normalize the features
then your cost function well on average

00:03:22.410 --> 00:03:22.420
then your cost function well on average
 

00:03:22.420 --> 00:03:25.199
then your cost function well on average
look more symmetric and if you are

00:03:25.199 --> 00:03:25.209
look more symmetric and if you are
 

00:03:25.209 --> 00:03:26.880
look more symmetric and if you are
running bathe in the scent on the cost

00:03:26.880 --> 00:03:26.890
running bathe in the scent on the cost
 

00:03:26.890 --> 00:03:28.860
running bathe in the scent on the cost
function like the one on the left then

00:03:28.860 --> 00:03:28.870
function like the one on the left then
 

00:03:28.870 --> 00:03:29.970
function like the one on the left then
you might have to use a very small

00:03:29.970 --> 00:03:29.980
you might have to use a very small
 

00:03:29.980 --> 00:03:31.770
you might have to use a very small
learning rate because over here

00:03:31.770 --> 00:03:31.780
learning rate because over here
 

00:03:31.780 --> 00:03:33.960
learning rate because over here
you know the grading descent might need

00:03:33.960 --> 00:03:33.970
you know the grading descent might need
 

00:03:33.970 --> 00:03:35.970
you know the grading descent might need
a lot of steps to oscillate back and

00:03:35.970 --> 00:03:35.980
a lot of steps to oscillate back and
 

00:03:35.980 --> 00:03:39.390
a lot of steps to oscillate back and
forth right before it finally finds its

00:03:39.390 --> 00:03:39.400
forth right before it finally finds its
 

00:03:39.400 --> 00:03:41.970
forth right before it finally finds its
way to the minimum whereas if you have a

00:03:41.970 --> 00:03:41.980
way to the minimum whereas if you have a
 

00:03:41.980 --> 00:03:44.880
way to the minimum whereas if you have a
more spherical contours than wherever

00:03:44.880 --> 00:03:44.890
more spherical contours than wherever
 

00:03:44.890 --> 00:03:47.520
more spherical contours than wherever
you start breathing descents can pretty

00:03:47.520 --> 00:03:47.530
you start breathing descents can pretty
 

00:03:47.530 --> 00:03:49.199
you start breathing descents can pretty
much go straight to the minimum you can

00:03:49.199 --> 00:03:49.209
much go straight to the minimum you can
 

00:03:49.209 --> 00:03:51.270
much go straight to the minimum you can
take much larger steps but gradient

00:03:51.270 --> 00:03:51.280
take much larger steps but gradient
 

00:03:51.280 --> 00:03:53.099
take much larger steps but gradient
descent need rather than needing to

00:03:53.099 --> 00:03:53.109
descent need rather than needing to
 

00:03:53.109 --> 00:03:55.500
descent need rather than needing to
oscillate around like the picture on the

00:03:55.500 --> 00:03:55.510
oscillate around like the picture on the
 

00:03:55.510 --> 00:03:57.960
oscillate around like the picture on the
left of course in practice W is a high

00:03:57.960 --> 00:03:57.970
left of course in practice W is a high
 

00:03:57.970 --> 00:04:00.780
left of course in practice W is a high
dimensional vector and so trying to plot

00:04:00.780 --> 00:04:00.790
dimensional vector and so trying to plot
 

00:04:00.790 --> 00:04:02.910
dimensional vector and so trying to plot
this in 2d doesn't convey all the

00:04:02.910 --> 00:04:02.920
this in 2d doesn't convey all the
 

00:04:02.920 --> 00:04:04.920
this in 2d doesn't convey all the
intuitions correctly but the rough

00:04:04.920 --> 00:04:04.930
intuitions correctly but the rough
 

00:04:04.930 --> 00:04:06.720
intuitions correctly but the rough
intuition that your cost function will

00:04:06.720 --> 00:04:06.730
intuition that your cost function will
 

00:04:06.730 --> 00:04:08.490
intuition that your cost function will
be you know more round and easier to

00:04:08.490 --> 00:04:08.500
be you know more round and easier to
 

00:04:08.500 --> 00:04:09.620
be you know more round and easier to
optimize

00:04:09.620 --> 00:04:09.630
optimize
 

00:04:09.630 --> 00:04:11.810
optimize
your features are all on similar skills

00:04:11.810 --> 00:04:11.820
your features are all on similar skills
 

00:04:11.820 --> 00:04:15.290
your features are all on similar skills
not all not from 1 to 1000 0 to 1 but

00:04:15.290 --> 00:04:15.300
not all not from 1 to 1000 0 to 1 but
 

00:04:15.300 --> 00:04:18.440
not all not from 1 to 1000 0 to 1 but
mostly from your minus 1 to 1 or with

00:04:18.440 --> 00:04:18.450
mostly from your minus 1 to 1 or with
 

00:04:18.450 --> 00:04:20.390
mostly from your minus 1 to 1 or with
about similar variants as each other

00:04:20.390 --> 00:04:20.400
about similar variants as each other
 

00:04:20.400 --> 00:04:22.850
about similar variants as each other
that just makes your cost function J

00:04:22.850 --> 00:04:22.860
that just makes your cost function J
 

00:04:22.860 --> 00:04:25.550
that just makes your cost function J
easier and faster to optimize in

00:04:25.550 --> 00:04:25.560
easier and faster to optimize in
 

00:04:25.560 --> 00:04:29.030
easier and faster to optimize in
practice if one feature say x1 ranges

00:04:29.030 --> 00:04:29.040
practice if one feature say x1 ranges
 

00:04:29.040 --> 00:04:32.000
practice if one feature say x1 ranges
from 0 to 1 and x2 ranges from minus 1

00:04:32.000 --> 00:04:32.010
from 0 to 1 and x2 ranges from minus 1
 

00:04:32.010 --> 00:04:35.480
from 0 to 1 and x2 ranges from minus 1
to 1 and x3 ranges from 1 to 2 you know

00:04:35.480 --> 00:04:35.490
to 1 and x3 ranges from 1 to 2 you know
 

00:04:35.490 --> 00:04:37.700
to 1 and x3 ranges from 1 to 2 you know
these are fairly similar ranges so this

00:04:37.700 --> 00:04:37.710
these are fairly similar ranges so this
 

00:04:37.710 --> 00:04:39.650
these are fairly similar ranges so this
will work just fine is when there are

00:04:39.650 --> 00:04:39.660
will work just fine is when there are
 

00:04:39.660 --> 00:04:41.510
will work just fine is when there are
dramatically different ranges like ones

00:04:41.510 --> 00:04:41.520
dramatically different ranges like ones
 

00:04:41.520 --> 00:04:43.100
dramatically different ranges like ones
from one to a thousand and another from

00:04:43.100 --> 00:04:43.110
from one to a thousand and another from
 

00:04:43.110 --> 00:04:44.960
from one to a thousand and another from
zero to one that that really hurts the

00:04:44.960 --> 00:04:44.970
zero to one that that really hurts the
 

00:04:44.970 --> 00:04:47.180
zero to one that that really hurts the
optimization algorithm but by just

00:04:47.180 --> 00:04:47.190
optimization algorithm but by just
 

00:04:47.190 --> 00:04:49.460
optimization algorithm but by just
setting all of them to zero mean and say

00:04:49.460 --> 00:04:49.470
setting all of them to zero mean and say
 

00:04:49.470 --> 00:04:51.740
setting all of them to zero mean and say
variance 1 like we did in the last slide

00:04:51.740 --> 00:04:51.750
variance 1 like we did in the last slide
 

00:04:51.750 --> 00:04:53.240
variance 1 like we did in the last slide
that just guarantees that all your

00:04:53.240 --> 00:04:53.250
that just guarantees that all your
 

00:04:53.250 --> 00:04:55.430
that just guarantees that all your
features are similar scale and will

00:04:55.430 --> 00:04:55.440
features are similar scale and will
 

00:04:55.440 --> 00:04:57.230
features are similar scale and will
usually help your learning Avrum run

00:04:57.230 --> 00:04:57.240
usually help your learning Avrum run
 

00:04:57.240 --> 00:05:00.020
usually help your learning Avrum run
faster so if your input features came

00:05:00.020 --> 00:05:00.030
faster so if your input features came
 

00:05:00.030 --> 00:05:01.940
faster so if your input features came
from very different scales maybe some

00:05:01.940 --> 00:05:01.950
from very different scales maybe some
 

00:05:01.950 --> 00:05:03.830
from very different scales maybe some
features are from 0 to 1 some from 1 to

00:05:03.830 --> 00:05:03.840
features are from 0 to 1 some from 1 to
 

00:05:03.840 --> 00:05:06.470
features are from 0 to 1 some from 1 to
1000 then it's important to normalize

00:05:06.470 --> 00:05:06.480
1000 then it's important to normalize
 

00:05:06.480 --> 00:05:08.990
1000 then it's important to normalize
your features if your features came in

00:05:08.990 --> 00:05:09.000
your features if your features came in
 

00:05:09.000 --> 00:05:10.610
your features if your features came in
on similar skills in this step is less

00:05:10.610 --> 00:05:10.620
on similar skills in this step is less
 

00:05:10.620 --> 00:05:12.800
on similar skills in this step is less
important although performing this type

00:05:12.800 --> 00:05:12.810
important although performing this type
 

00:05:12.810 --> 00:05:14.960
important although performing this type
of normalization pretty much never does

00:05:14.960 --> 00:05:14.970
of normalization pretty much never does
 

00:05:14.970 --> 00:05:17.420
of normalization pretty much never does
any harm so often you know do it anyway

00:05:17.420 --> 00:05:17.430
any harm so often you know do it anyway
 

00:05:17.430 --> 00:05:19.490
any harm so often you know do it anyway
if I'm not sure whether or not they were

00:05:19.490 --> 00:05:19.500
if I'm not sure whether or not they were
 

00:05:19.500 --> 00:05:21.650
if I'm not sure whether or not they were
help with speeding up training for your

00:05:21.650 --> 00:05:21.660
help with speeding up training for your
 

00:05:21.660 --> 00:05:24.560
help with speeding up training for your
algorithm so that's it for normalizing

00:05:24.560 --> 00:05:24.570
algorithm so that's it for normalizing
 

00:05:24.570 --> 00:05:26.720
algorithm so that's it for normalizing
your input features next let's keep

00:05:26.720 --> 00:05:26.730
your input features next let's keep
 

00:05:26.730 --> 00:05:28.430
your input features next let's keep
talking about ways to speed up the

00:05:28.430 --> 00:05:28.440
talking about ways to speed up the
 

00:05:28.440 --> 00:05:31.580
talking about ways to speed up the
training of your new network

