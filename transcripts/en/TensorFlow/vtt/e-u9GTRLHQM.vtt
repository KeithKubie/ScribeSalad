WEBVTT
Kind: captions
Language: en

00:00:00.540 --> 00:00:02.280
WAHID BHIMJI: OK, so I'm Wahid.

00:00:02.280 --> 00:00:03.750
I'm not actually part of Google.

00:00:03.750 --> 00:00:05.970
I'm at Lawrence
Berkeley National Lab.

00:00:05.970 --> 00:00:07.920
And I'm sort of
going to tell you

00:00:07.920 --> 00:00:11.460
how we're using
TensorFlow to perform

00:00:11.460 --> 00:00:13.890
deep learning for the
fundamental sciences

00:00:13.890 --> 00:00:16.920
and also using
high-performance computing.

00:00:16.920 --> 00:00:20.400
OK, so the
fundamental sciences--

00:00:20.400 --> 00:00:23.271
particle physics,
cosmology, and lots

00:00:23.271 --> 00:00:25.020
of other things I'll
explain in a minute--

00:00:25.020 --> 00:00:27.030
make heavy use of
high-performance computing

00:00:27.030 --> 00:00:28.560
at the center I
work at, which is

00:00:28.560 --> 00:00:31.500
part of the
Department of Energy,

00:00:31.500 --> 00:00:33.720
initially for simulation
and data analysis,

00:00:33.720 --> 00:00:35.370
or traditionally.

00:00:35.370 --> 00:00:38.220
But progress in deep learning
and tools like TensorFlow

00:00:38.220 --> 00:00:41.160
have really enabled the use
of higher-dimensional data

00:00:41.160 --> 00:00:44.190
and through deep learning
enable the possibility

00:00:44.190 --> 00:00:47.100
of new discoveries, faster
computation, and actually

00:00:47.100 --> 00:00:48.570
whole new approaches.

00:00:48.570 --> 00:00:50.160
And I'm going to
talk about that here,

00:00:50.160 --> 00:00:52.110
illustrating with a
few examples of stuff

00:00:52.110 --> 00:00:53.550
we're running at NERSC.

00:00:53.550 --> 00:00:55.540
So what is NERSC?

00:00:55.540 --> 00:00:57.930
It's the high-performance
computing center

00:00:57.930 --> 00:01:01.030
for the Department of
Energy Office for Science,

00:01:01.030 --> 00:01:03.750
which means we support the
whole breadth of science

00:01:03.750 --> 00:01:06.420
that the DoE does, which
actually includes things like

00:01:06.420 --> 00:01:09.810
not just cosmology or what
you might think of as energy

00:01:09.810 --> 00:01:12.120
research, like batteries
and so forth, but also

00:01:12.120 --> 00:01:15.670
materials and climate and
genomics and things like this.

00:01:15.670 --> 00:01:18.660
So we have a huge range
of users and a vast range

00:01:18.660 --> 00:01:22.440
of projects across a
whole variety of science.

00:01:22.440 --> 00:01:24.180
And we have big machines.

00:01:24.180 --> 00:01:27.670
Cori, our latest machine was
number five, and, in fact,

00:01:27.670 --> 00:01:30.044
the highest by P flops in
the US when it was installed

00:01:30.044 --> 00:01:30.960
a couple of years ago.

00:01:30.960 --> 00:01:33.240
But things dropped
down those numbers,

00:01:33.240 --> 00:01:37.140
and now it's number
10 in the top 500.

00:01:37.140 --> 00:01:42.600
OK, so we see the use of AI
now across the whole science

00:01:42.600 --> 00:01:43.100
domain.

00:01:43.100 --> 00:01:45.630
So you probably can't
see this slide very well,

00:01:45.630 --> 00:01:48.699
but this is a take on an
industry part of machine

00:01:48.699 --> 00:01:50.990
learning, sort of splitting
it into supervised learning

00:01:50.990 --> 00:01:53.531
and unsupervised learning, and
classification and regression,

00:01:53.531 --> 00:01:54.265
and so forth.

00:01:54.265 --> 00:01:55.640
And there's kind
of examples here

00:01:55.640 --> 00:01:58.160
that we see across the sciences.

00:01:58.160 --> 00:02:01.107
But I'll be mostly talking about
particle physics and cosmology

00:02:01.107 --> 00:02:02.690
because actually
that's my background.

00:02:02.690 --> 00:02:06.900
So I'm more comfortable
with those examples.

00:02:06.900 --> 00:02:08.810
So what we're trying
to do in these fields

00:02:08.810 --> 00:02:10.800
is really uncover the
secrets of the universe.

00:02:10.800 --> 00:02:12.680
So this is a sort of
evolution from the Big

00:02:12.680 --> 00:02:14.420
Bang to the present day.

00:02:14.420 --> 00:02:17.750
And there's planets
and galaxies and stuff.

00:02:17.750 --> 00:02:20.360
And they've all been
influenced by, for example,

00:02:20.360 --> 00:02:22.827
dark matter and dark
energy over this evolution.

00:02:22.827 --> 00:02:24.410
So obviously our
understanding of this

00:02:24.410 --> 00:02:26.551
has come a long way
in recent years.

00:02:26.551 --> 00:02:28.550
But there's still plenty
of mysteries and things

00:02:28.550 --> 00:02:30.758
that we don't know about,
like the very things I just

00:02:30.758 --> 00:02:32.090
mentioned, like dark matter.

00:02:32.090 --> 00:02:35.150
What is the nature
of dark matter?

00:02:35.150 --> 00:02:37.895
And what is the relationship
between particle physics that

00:02:37.895 --> 00:02:40.640
explains extremely well
the very small, and even

00:02:40.640 --> 00:02:42.800
everything around us,
but yet, breaks down

00:02:42.800 --> 00:02:44.870
at cosmological scales?

00:02:47.384 --> 00:02:49.300
So in order to answer
those kind of questions,

00:02:49.300 --> 00:02:51.040
we have huge
complex instruments,

00:02:51.040 --> 00:02:55.100
such as the planned LSST
telescope on the left,

00:02:55.100 --> 00:02:59.470
which is going to look at
the very big in resolutions

00:02:59.470 --> 00:03:01.840
that currently I'm presenting.

00:03:01.840 --> 00:03:04.630
And the ATLAS
Detector at the LHC

00:03:04.630 --> 00:03:07.090
on the right, which is at
the Large Hadron Collider,

00:03:07.090 --> 00:03:09.340
so at the Swiss,
French border, it's

00:03:09.340 --> 00:03:11.044
a detector the
size of a building.

00:03:11.044 --> 00:03:12.210
There's little people there.

00:03:12.210 --> 00:03:13.300
You can sort of see them.

00:03:13.300 --> 00:03:16.150
And it has hundreds of millions
of channels of electronics

00:03:16.150 --> 00:03:20.950
to record collisions that
occur in the middle every 25

00:03:20.950 --> 00:03:24.250
nanoseconds, so a
huge stream of data.

00:03:24.250 --> 00:03:27.040
So both of these experiments
have vast streams of data.

00:03:27.040 --> 00:03:29.410
Really the ATLAS experiment
has processed exabytes

00:03:29.410 --> 00:03:30.880
of data over its time.

00:03:30.880 --> 00:03:33.340
And this has to be filtered
through a process of data

00:03:33.340 --> 00:03:36.460
analysis and so forth.

00:03:36.460 --> 00:03:41.950
And so if you get like
high-resolution images or very

00:03:41.950 --> 00:03:45.290
detailed detector
outputs, the first stage

00:03:45.290 --> 00:03:46.930
is to kind of
simplify these, maybe

00:03:46.930 --> 00:03:50.470
build catalogs of
objects in the sky,

00:03:50.470 --> 00:03:56.710
such as stars and galaxies, or
in the case of particle physics

00:03:56.710 --> 00:04:00.550
to kind of combine these into
what particles might have been

00:04:00.550 --> 00:04:05.110
produced, and these lines
or tracks and deposits that

00:04:05.110 --> 00:04:08.810
have occurred in the detector.

00:04:08.810 --> 00:04:11.780
So this obviously also involves
a large amount of computing.

00:04:11.780 --> 00:04:13.210
So computing fits in here.

00:04:13.210 --> 00:04:15.280
But computing also
fits in because the way

00:04:15.280 --> 00:04:20.470
that these analyses are done is
to compare with simulated data,

00:04:20.470 --> 00:04:22.410
in this case cosmology
simulations that

00:04:22.410 --> 00:04:25.870
are big HPC simulations done
for different types of universes

00:04:25.870 --> 00:04:27.250
that might have
existed depending

00:04:27.250 --> 00:04:29.680
on different
cosmology parameters.

00:04:29.680 --> 00:04:31.840
And in the particle
physics case,

00:04:31.840 --> 00:04:34.300
you do extremely
detailed simulations

00:04:34.300 --> 00:04:36.130
because of the
precision you require

00:04:36.130 --> 00:04:37.990
in terms of how
the detector would

00:04:37.990 --> 00:04:40.750
have reacted to, for example,
a particle coming in here.

00:04:40.750 --> 00:04:42.670
So here you've got all
this kind of showering

00:04:42.670 --> 00:04:46.370
of what would have happened
inside the detector.

00:04:46.370 --> 00:04:48.340
And then from each
of these, you might

00:04:48.340 --> 00:04:51.860
produce summary statistics
and compare one to the other

00:04:51.860 --> 00:04:54.460
and what are secrets
of the universe,

00:04:54.460 --> 00:04:56.800
I guess, such as the
nature of dark matter,

00:04:56.800 --> 00:05:01.389
or new particles at the LHC.

00:05:01.389 --> 00:05:02.930
OK, so you might
have guessed there's

00:05:02.930 --> 00:05:05.040
many areas where deep
learning can help with this.

00:05:05.040 --> 00:05:07.700
So one is classification
to, for example,

00:05:07.700 --> 00:05:10.850
find those physics objects
that I showed identified

00:05:10.850 --> 00:05:14.330
in collisions at the LHC.

00:05:14.330 --> 00:05:16.370
Or, indeed, just
to directly find

00:05:16.370 --> 00:05:18.530
from the raw data what
was interesting and what

00:05:18.530 --> 00:05:21.462
was not interesting.

00:05:21.462 --> 00:05:23.920
Another way you might use it
is regression to, for example,

00:05:23.920 --> 00:05:27.520
find what kind of energies were
deposited inside the detector

00:05:27.520 --> 00:05:29.860
or what were the
physics parameters that

00:05:29.860 --> 00:05:34.760
were responsible for stuff
that you saw in the images

00:05:34.760 --> 00:05:37.490
at the telescopes.

00:05:37.490 --> 00:05:39.530
Another is sort of
clustering feature detection

00:05:39.530 --> 00:05:41.120
in a more unsupervised
way, where

00:05:41.120 --> 00:05:43.550
you might want to look
for anomalies in the data,

00:05:43.550 --> 00:05:45.710
either because these
are signs of new physics

00:05:45.710 --> 00:05:49.080
or because they're actually
problems with the instruments.

00:05:49.080 --> 00:05:51.150
And then another
last way, perhaps,

00:05:51.150 --> 00:05:54.090
is to generate data to replace
the full physics simulations

00:05:54.090 --> 00:05:56.310
that I just described,
which are, as I mentioned,

00:05:56.310 --> 00:05:58.990
extremely computationally
expensive.

00:05:58.990 --> 00:06:03.060
So I'll give a few examples
across these domains, OK?

00:06:03.060 --> 00:06:04.800
So the first is classification.

00:06:04.800 --> 00:06:06.990
So there you're trying
to answer the question,

00:06:06.990 --> 00:06:08.100
is this new physics?

00:06:08.100 --> 00:06:09.600
For example,
supersymmetry, which

00:06:09.600 --> 00:06:11.700
is also a dark matter candidate.

00:06:11.700 --> 00:06:13.200
So I could give you
the answer later

00:06:13.200 --> 00:06:17.140
whether that's actually
new physics or not.

00:06:17.140 --> 00:06:20.524
So here the idea is kind of be--

00:06:20.524 --> 00:06:22.440
there's several papers
now that are exploiting

00:06:22.440 --> 00:06:25.560
this kind of idea is
to take the detector,

00:06:25.560 --> 00:06:28.860
like the ATLAS detector,
and sort of unroll--

00:06:28.860 --> 00:06:33.280
so it's a cylindrical detector
with many cylindrical layers,

00:06:33.280 --> 00:06:35.610
and to take that and unroll
it into an image where

00:06:35.610 --> 00:06:37.810
you have phi along
here, which is

00:06:37.810 --> 00:06:41.220
the angle around this direction,
and eta this way, which

00:06:41.220 --> 00:06:42.970
is a physicist's
way of describing

00:06:42.970 --> 00:06:46.170
the forwardness of the deposit.

00:06:46.170 --> 00:06:47.980
And then just simply
put it in an image,

00:06:47.980 --> 00:06:50.190
and then you get
the ability to use

00:06:50.190 --> 00:06:53.200
all of the kind of developments
in image recognition,

00:06:53.200 --> 00:06:55.210
such as convolutional
neural networks.

00:06:55.210 --> 00:06:58.230
So here's what is now a
relatively simple sort

00:06:58.230 --> 00:07:00.660
of convolutional network,
several convolution and pooling

00:07:00.660 --> 00:07:03.390
layers, and then fully
connected layers.

00:07:03.390 --> 00:07:06.900
And we've exploited
this at scales,

00:07:06.900 --> 00:07:09.870
either taking 64 by
64 images to represent

00:07:09.870 --> 00:07:14.730
this, or at the 224
by 224, sort of closer

00:07:14.730 --> 00:07:17.520
to the resolution
of the detector.

00:07:17.520 --> 00:07:19.380
And then you can also
have multiple channels

00:07:19.380 --> 00:07:21.960
which correspond
to different layers

00:07:21.960 --> 00:07:25.320
of this cylindrical detector.

00:07:25.320 --> 00:07:27.120
And we saw that
it kind of works.

00:07:27.120 --> 00:07:30.260
So here you have a rock curve
of the two class probability,

00:07:30.260 --> 00:07:31.520
so false positive rate.

00:07:31.520 --> 00:07:33.470
So the first thing
to notice is that you

00:07:33.470 --> 00:07:36.710
need a very, very high
rejection of the physics

00:07:36.710 --> 00:07:39.394
that you already know about
because that vastly dominates.

00:07:39.394 --> 00:07:41.810
So actually, the data's even
been prefiltered before this.

00:07:41.810 --> 00:07:45.552
So you need a low
false positive rate.

00:07:45.552 --> 00:07:47.510
And generally, of course,
in these rock curves,

00:07:47.510 --> 00:07:49.850
higher this way is better.

00:07:49.850 --> 00:07:52.220
So this point represents
the physics selections

00:07:52.220 --> 00:07:55.280
that were traditionally
used in this analysis.

00:07:55.280 --> 00:07:58.730
These curves here, like
incorporating those higher

00:07:58.730 --> 00:08:00.720
level physics variables.

00:08:00.720 --> 00:08:04.790
But in shallow neural
net-- in shallow machine

00:08:04.790 --> 00:08:06.530
learning approaches.

00:08:06.530 --> 00:08:08.810
And then the blue
line shows that you

00:08:08.810 --> 00:08:11.450
can gain quite a lot,
and this from using

00:08:11.450 --> 00:08:14.420
convolution neural networks,
which gives you access.

00:08:14.420 --> 00:08:16.440
It's not just the
technique of deep learning,

00:08:16.440 --> 00:08:20.470
but also being able to use
all the data that's available.

00:08:20.470 --> 00:08:22.220
And then, similarly,
there's another boost

00:08:22.220 --> 00:08:24.380
from using three channels,
which corresponds

00:08:24.380 --> 00:08:26.630
to the other detectors.

00:08:26.630 --> 00:08:29.990
OK, so I'd just like to
caveat a few of these results.

00:08:29.990 --> 00:08:33.230
So even though there's
obvious gains to be had here,

00:08:33.230 --> 00:08:35.179
these analyses are
still not currently used

00:08:35.179 --> 00:08:37.429
in the Large Hadron
Collider analyses.

00:08:37.429 --> 00:08:38.990
And part of the
reason for that is

00:08:38.990 --> 00:08:43.025
because this is trained
on simulation, whereas--

00:08:43.025 --> 00:08:46.010
and it might pick up on very
small aspects of the simulation

00:08:46.010 --> 00:08:47.950
that differ from the real data.

00:08:47.950 --> 00:08:49.700
So there's several
work that's carrying on

00:08:49.700 --> 00:08:52.190
from this to look at
how to incorporate

00:08:52.190 --> 00:08:55.470
real data into what's done.

00:08:55.470 --> 00:08:58.130
But also, I think
methodological developments

00:08:58.130 --> 00:08:59.600
in how to interrogate
these models,

00:08:59.600 --> 00:09:01.700
and really discover
what they're learning

00:09:01.700 --> 00:09:04.910
would be kind of
useful for the field.

00:09:04.910 --> 00:09:07.054
OK, so taking a
regression problem now,

00:09:07.054 --> 00:09:08.470
the kind of question
you might ask

00:09:08.470 --> 00:09:10.820
is, what possible universe
would look like this?

00:09:10.820 --> 00:09:14.704
So this is an image from the
Sloan Digital Sky Survey.

00:09:14.704 --> 00:09:16.120
So here the Earth's
in the middle,

00:09:16.120 --> 00:09:20.630
and increasing redshift
takes you out this way.

00:09:20.630 --> 00:09:23.560
So this is like the
older universe over here.

00:09:23.560 --> 00:09:24.610
And you can kind of see.

00:09:24.610 --> 00:09:28.130
And the histogram is a
structure of galaxies' density.

00:09:28.130 --> 00:09:29.800
So there's like
more-- you can see

00:09:29.800 --> 00:09:32.950
that structure sort of appears
as the universe evolves.

00:09:32.950 --> 00:09:35.200
And that kind of
evolution of structure

00:09:35.200 --> 00:09:37.809
tells you something about
the cosmology parameters

00:09:37.809 --> 00:09:38.600
that were involved.

00:09:38.600 --> 00:09:40.930
So can you actually
regress those parameters

00:09:40.930 --> 00:09:44.080
from looking at these
kind of distributions?

00:09:44.080 --> 00:09:46.120
So the idea here was
to take a method that

00:09:46.120 --> 00:09:49.360
was developed by these people
in CMU, which is to use, again,

00:09:49.360 --> 00:09:50.740
a convolutional neural network.

00:09:50.740 --> 00:09:53.350
But here, running on a 3D--

00:09:53.350 --> 00:09:57.070
this is actually simulated
data, so 3D distribution

00:09:57.070 --> 00:10:00.000
of dark matter in the universe.

00:10:00.000 --> 00:10:02.350
And these can be large datasets.

00:10:02.350 --> 00:10:05.110
So part of the work we did here
was to kind of scale this up

00:10:05.110 --> 00:10:08.500
and run on extremely large data
and across the kind of machines

00:10:08.500 --> 00:10:10.790
we have at Cori on
8,000 CPU nodes,

00:10:10.790 --> 00:10:12.610
which is kind of some
of the largest scale

00:10:12.610 --> 00:10:16.660
that TensorFlow has been run
on in a data parallel fashion,

00:10:16.660 --> 00:10:19.330
and then being able to predict
cosmology parameters that

00:10:19.330 --> 00:10:23.200
went into this simulation in
a matter of minutes because

00:10:23.200 --> 00:10:26.590
of the scale of
computation used.

00:10:26.590 --> 00:10:29.380
So, again, it's a
fairly standard network,

00:10:29.380 --> 00:10:30.502
but this time in 3D.

00:10:30.502 --> 00:10:31.960
And actually, there
was quite a bit

00:10:31.960 --> 00:10:34.314
of work to get this to work
well on CPU, for example.

00:10:34.314 --> 00:10:36.730
Because that's-- sorry, I
should have mentioned, actually,

00:10:36.730 --> 00:10:39.700
that the machine that we have
is primarily composed of Intel

00:10:39.700 --> 00:10:41.800
Knights Landing CPU.

00:10:41.800 --> 00:10:46.240
So that's another area
that we help develop.

00:10:46.240 --> 00:10:47.980
OK, so again, it kind of works.

00:10:47.980 --> 00:10:51.786
And so here they like
to plot the true value

00:10:51.786 --> 00:10:53.410
of the parameter
that's being regressed

00:10:53.410 --> 00:10:54.650
against the predicted value.

00:10:54.650 --> 00:10:57.184
So you hope that it would
match up along the line.

00:10:57.184 --> 00:10:58.600
And, indeed, the
points which come

00:10:58.600 --> 00:11:02.650
from a run of this
regression do actually

00:11:02.650 --> 00:11:04.770
lie across this line
in all the cases.

00:11:04.770 --> 00:11:08.680
Now, the actual crosses come
from the larger-scale run.

00:11:08.680 --> 00:11:12.160
So the points come from a
2,000 node run of the network,

00:11:12.160 --> 00:11:15.310
and the crosses come
from an 8,000 node run.

00:11:15.310 --> 00:11:19.450
So the sort of caveat here is
that the 8,000 node run doesn't

00:11:19.450 --> 00:11:22.164
actually do as well as the
2,000 node run, which kind of

00:11:22.164 --> 00:11:24.580
indicates a point, which is
another thing we're working on

00:11:24.580 --> 00:11:26.950
about convergence of
these neural networks

00:11:26.950 --> 00:11:29.320
when running at a large
distributed scale.

00:11:29.320 --> 00:11:33.950
So I'll come back to
that a bit later as well.

00:11:33.950 --> 00:11:38.300
OK, so then the last example I
have here is around generation.

00:11:38.300 --> 00:11:40.289
So, as I mentioned,
simulations went

00:11:40.289 --> 00:11:41.830
into that previous
analysis, and they

00:11:41.830 --> 00:11:43.980
go into all of the
kind of analyses

00:11:43.980 --> 00:11:45.916
that are done at these--

00:11:45.916 --> 00:11:48.520
for these kind of purposes.

00:11:48.520 --> 00:11:50.680
But in order to get some
of these simulations,

00:11:50.680 --> 00:11:53.110
it actually takes two
weeks of computational time

00:11:53.110 --> 00:11:55.240
on a large supercomputer
of the scale of Cori,

00:11:55.240 --> 00:11:57.790
which is kind of a
huge amount of resource

00:11:57.790 --> 00:11:59.570
to be giving over to these.

00:11:59.570 --> 00:12:01.972
And for different
versions of the cosmology

00:12:01.972 --> 00:12:03.430
that you want to
generate, you need

00:12:03.430 --> 00:12:05.710
to do more of these simulations.

00:12:05.710 --> 00:12:09.130
And part of the output that
you might get from these

00:12:09.130 --> 00:12:13.990
is, this is a 2D mass
map, which corresponds

00:12:13.990 --> 00:12:16.705
to the actual
galaxy distribution

00:12:16.705 --> 00:12:18.080
that you might
observe from data.

00:12:18.080 --> 00:12:20.800
So this is what you would
want to compare with data.

00:12:20.800 --> 00:12:23.260
So the question is, is it
possible to augment your data

00:12:23.260 --> 00:12:25.480
set and generate
different cosmologies

00:12:25.480 --> 00:12:29.260
in a kind of fast simulation
that doesn't require running

00:12:29.260 --> 00:12:31.700
this full infrastructure?

00:12:31.700 --> 00:12:33.200
And the way that
we tried to do that

00:12:33.200 --> 00:12:35.630
was to use generative
adversarial networks that

00:12:35.630 --> 00:12:38.120
were mentioned briefly in
the last talk, where you have

00:12:38.120 --> 00:12:41.000
two networks that work
in tandem with each other

00:12:41.000 --> 00:12:44.370
and that are optimized together,
but against each other,

00:12:44.370 --> 00:12:46.760
and one is trying to tell
the difference between.

00:12:46.760 --> 00:12:50.180
So the advantage here is that
we have some real examples

00:12:50.180 --> 00:12:52.230
from the full simulation
to compare with.

00:12:52.230 --> 00:12:54.540
And the discriminator
can take those and say,

00:12:54.540 --> 00:13:01.050
does the generator do any good
job of producing fake maps?

00:13:01.050 --> 00:13:04.152
And we used a pretty
standard DCGAN architecture.

00:13:04.152 --> 00:13:06.110
And there was a few
modifications in this paper

00:13:06.110 --> 00:13:08.665
to make that work.

00:13:08.665 --> 00:13:10.790
But at the time last year
there weren't many people

00:13:10.790 --> 00:13:11.630
trying to apply this.

00:13:11.630 --> 00:13:14.060
One of the other applications
that was being done at the time

00:13:14.060 --> 00:13:15.518
is actually to the
particle physics

00:13:15.518 --> 00:13:19.940
problem from other
people at Berkeley Lab.

00:13:19.940 --> 00:13:21.600
And, again, it works.

00:13:21.600 --> 00:13:24.017
And so the top plot
is a validation set

00:13:24.017 --> 00:13:25.850
of images that weren't
used in the training.

00:13:25.850 --> 00:13:27.962
And the bottom is
the generated images.

00:13:27.962 --> 00:13:29.420
I mean, the first
time cosmologists

00:13:29.420 --> 00:13:31.694
saw this, they were kind
of pretty well surprised

00:13:31.694 --> 00:13:33.860
that they couldn't tell the
difference between them,

00:13:33.860 --> 00:13:35.630
because they weren't
really expecting

00:13:35.630 --> 00:13:37.820
us to be able to do so
well that they wouldn't

00:13:37.820 --> 00:13:38.830
be able to tell by eye.

00:13:38.830 --> 00:13:41.060
But you certainly
can't tell by eye.

00:13:41.060 --> 00:13:44.090
But one of the advantages of
working with this in science,

00:13:44.090 --> 00:13:45.830
as opposed to
celebrity faces, is

00:13:45.830 --> 00:13:49.460
that we do actually have
good metrics for determining

00:13:49.460 --> 00:13:51.410
whether we've done well enough.

00:13:51.410 --> 00:13:53.760
So the top right is
the power spectrum,

00:13:53.760 --> 00:13:56.090
which is something often
used by cosmologists,

00:13:56.090 --> 00:13:58.105
but it's just the
Fourier transform

00:13:58.105 --> 00:13:59.970
of a two-point correlation.

00:13:59.970 --> 00:14:03.320
So it sort of represents
the Gaussian fluctuations

00:14:03.320 --> 00:14:04.790
in the plot.

00:14:04.790 --> 00:14:08.210
And here you can see the
black is the validation,

00:14:08.210 --> 00:14:11.630
and the pink is the GAN.

00:14:11.630 --> 00:14:14.690
And so it not only agrees on the
mean, this kind of middle line,

00:14:14.690 --> 00:14:19.462
but it also captures the
distribution of this variable.

00:14:19.462 --> 00:14:21.170
And it's not just
two-point correlations.

00:14:21.170 --> 00:14:22.640
But the plot on the
right shows something

00:14:22.640 --> 00:14:24.230
called the Minkowski
functional, which

00:14:24.230 --> 00:14:27.380
is a form with a
three-point correlation.

00:14:27.380 --> 00:14:33.185
So even non-Gaussian structures
in these maps are reproduced.

00:14:33.185 --> 00:14:34.560
And the important
point, I guess,

00:14:34.560 --> 00:14:37.670
is that you could just sample
from these distributions

00:14:37.670 --> 00:14:39.670
and reproduce those well.

00:14:39.670 --> 00:14:43.089
But all this was trained on
was to reproduce the images.

00:14:43.089 --> 00:14:45.380
And it got these kind of
structures, the physics that's

00:14:45.380 --> 00:14:47.770
important, right?

00:14:47.770 --> 00:14:49.130
OK, so this is very promising.

00:14:49.130 --> 00:14:51.230
But obviously, the
holy grail of this

00:14:51.230 --> 00:14:54.050
is really to be able to do
this for different values

00:14:54.050 --> 00:14:57.020
of the initial cosmology, sort
of parameterized generation,

00:14:57.020 --> 00:15:01.130
and that's what we're
working towards now.

00:15:01.130 --> 00:15:03.190
OK, so I mentioned
throughout that we

00:15:03.190 --> 00:15:05.620
have these big computers.

00:15:05.620 --> 00:15:07.930
So another part of
what we're trying to do

00:15:07.930 --> 00:15:15.440
is use extreme computing
scales in a data parallel way

00:15:15.440 --> 00:15:17.207
to train these things
faster, so not just

00:15:17.207 --> 00:15:19.040
different hyperparameters,
which, of course,

00:15:19.040 --> 00:15:21.260
people also do, is to train
different hyperparameters

00:15:21.260 --> 00:15:23.310
on different nodes
in our computer,

00:15:23.310 --> 00:15:26.140
but just to train also
one model quicker.

00:15:26.140 --> 00:15:28.730
And we've done that for
all of these examples

00:15:28.730 --> 00:15:32.570
here so that the one
on the left is the LHC

00:15:32.570 --> 00:15:35.120
CNN, which I described first.

00:15:35.120 --> 00:15:37.400
So I should say also
that we have a large Cray

00:15:37.400 --> 00:15:40.580
machine that has an optimized
high-performance network.

00:15:40.580 --> 00:15:42.890
But that network has been
particularly optimized

00:15:42.890 --> 00:15:48.470
for HPC simulations and stuff
and MPI-based architectures.

00:15:48.470 --> 00:15:52.040
So really it's been a huge
gain for this kind of work

00:15:52.040 --> 00:15:55.280
that these plug-ins
now exist that allow

00:15:55.280 --> 00:15:58.460
you to do MPI-distributed
training, such as Horovod

00:15:58.460 --> 00:15:59.420
from Uber.

00:15:59.420 --> 00:16:03.230
And also Cray have developed a
machine-learning plug-in here.

00:16:03.230 --> 00:16:05.210
So we used that
for this problem.

00:16:05.210 --> 00:16:08.990
And you can basically see
that all the lines kind of

00:16:08.990 --> 00:16:13.510
follow the ideal scaling
up to thousands of nodes.

00:16:13.510 --> 00:16:17.620
And so you can really
process more data faster.

00:16:17.620 --> 00:16:19.370
So there's quite a bit
of engineering work

00:16:19.370 --> 00:16:20.850
that goes into this, of course.

00:16:20.850 --> 00:16:23.720
And this is shown
in the papers here.

00:16:23.720 --> 00:16:26.684
For the LHC CNN, there's
still a gap between the ideal.

00:16:26.684 --> 00:16:28.100
And that's partly
because it's not

00:16:28.100 --> 00:16:30.320
such a computationally
intensive model.

00:16:30.320 --> 00:16:33.470
So I/O and stuff
becomes more important.

00:16:33.470 --> 00:16:36.122
For the CosmoGAN on the
right, that's the GAN example.

00:16:36.122 --> 00:16:38.330
And there it is a bit more
computationally intensive.

00:16:38.330 --> 00:16:43.040
So it does follow the
scaling curve here.

00:16:43.040 --> 00:16:45.290
And CosmoFlow I just
put in the middle here.

00:16:45.290 --> 00:16:48.300
I mean, here we show
that I/O was important.

00:16:48.300 --> 00:16:51.950
But we were able to
exploit something we have,

00:16:51.950 --> 00:16:54.950
which is this burst buffer,
which is a layer of SSDs that

00:16:54.950 --> 00:16:57.120
sits on the high-speed network.

00:16:57.120 --> 00:17:01.490
And we were much
more able to scale up

00:17:01.490 --> 00:17:04.040
to the scale of the full
machine, 8,000 nodes using

00:17:04.040 --> 00:17:06.470
that than the shared
disc space file system.

00:17:09.220 --> 00:17:12.319
OK, so another area that I
just wanted to briefly mention

00:17:12.319 --> 00:17:14.984
at the end is that we have
these fancy supercomputers,

00:17:14.984 --> 00:17:17.150
but as I mentioned, this
takes a lot of engineering.

00:17:17.150 --> 00:17:19.381
And it was projects that
we work particularly with.

00:17:19.381 --> 00:17:20.839
Something that we
really want to do

00:17:20.839 --> 00:17:24.079
is allow people to use the
supercomputer scale of kind

00:17:24.079 --> 00:17:26.700
of deep learning via
Jupyter notebooks,

00:17:26.700 --> 00:17:28.910
which is really how
scientists prefer to interact

00:17:28.910 --> 00:17:30.540
with our machines these ways.

00:17:30.540 --> 00:17:34.490
So we do provide
JupyterHub at NERSC.

00:17:34.490 --> 00:17:36.080
But generally what
people are running

00:17:36.080 --> 00:17:41.390
sits on a dedicated machine
outside the main compute

00:17:41.390 --> 00:17:42.800
infrastructure.

00:17:42.800 --> 00:17:46.670
But what we did here was
to enable them to run stuff

00:17:46.670 --> 00:17:51.170
actually on the
supercomputing machine, either

00:17:51.170 --> 00:17:55.460
using ipyparallel or Dask, which
are tools used for distributed

00:17:55.460 --> 00:18:00.380
computing, but interfacing
with this via the notebook,

00:18:00.380 --> 00:18:02.270
and where a lot of
the heavy computation

00:18:02.270 --> 00:18:05.180
actually occurs with the MPI
backend in Horovod and stuff.

00:18:05.180 --> 00:18:07.730
So we were able to show that
you can scale to large scales

00:18:07.730 --> 00:18:11.030
here without adding any
extra overhead from being

00:18:11.030 --> 00:18:12.480
able to interact.

00:18:12.480 --> 00:18:14.960
And then, of course, you
can add nice Jupyter things,

00:18:14.960 --> 00:18:16.670
like widgets and
buttons, so that you

00:18:16.670 --> 00:18:19.250
can run different hyperparameter
trials and sort of click

00:18:19.250 --> 00:18:24.140
on them and display
them in the pane there.

00:18:24.140 --> 00:18:27.470
OK, so I just have
my conclusion now.

00:18:27.470 --> 00:18:29.502
Basically deep
learning, particularly

00:18:29.502 --> 00:18:31.460
in combination with
high-performance computing,

00:18:31.460 --> 00:18:33.710
but productive software
like TensorFlow,

00:18:33.710 --> 00:18:35.074
can really accelerate science.

00:18:35.074 --> 00:18:36.740
And we've seen that
in various examples.

00:18:36.740 --> 00:18:39.530
I only mentioned a few here.

00:18:39.530 --> 00:18:42.609
But it requires developments,
not only in methods.

00:18:42.609 --> 00:18:44.150
So we have other
projects where we're

00:18:44.150 --> 00:18:46.370
working with
machine-learning researchers

00:18:46.370 --> 00:18:47.764
to develop new methods.

00:18:47.764 --> 00:18:50.180
Also, there's different ways
of applying these, of course.

00:18:50.180 --> 00:18:51.710
And we're well-placed
to do that.

00:18:51.710 --> 00:18:54.680
And also well-placed to do
some of the computing work.

00:18:54.680 --> 00:18:57.050
But it can really benefit
from collaboration, I think,

00:18:57.050 --> 00:19:00.680
between scientists and the
industry, which is sort

00:19:00.680 --> 00:19:02.060
of better represented here.

00:19:02.060 --> 00:19:04.580
And we've had a good
relationship with Google there.

00:19:04.580 --> 00:19:07.680
But I think we can also
do that with others.

00:19:07.680 --> 00:19:10.337
So basically my last
slide is a call to help.

00:19:10.337 --> 00:19:11.795
If you have any
questions, but also

00:19:11.795 --> 00:19:13.640
if you have ideas
or collaborations

00:19:13.640 --> 00:19:15.470
or want to work with
us on these problems,

00:19:15.470 --> 00:19:18.028
that would be good
to hear about.

00:19:18.028 --> 00:19:19.525
Thanks.

00:19:19.525 --> 00:19:22.519
[APPLAUSE]

00:19:50.656 --> 00:19:54.140
AUDIENCE: [INAUDIBLE]

00:19:54.140 --> 00:19:55.330
WAHID BHIMJI: Yeah.

00:19:55.330 --> 00:19:59.190
Well, what was I going to
say about supersymmetry?

00:19:59.190 --> 00:20:02.328
AUDIENCE: [INAUDIBLE]

00:20:02.328 --> 00:20:03.800
[LAUGHS]

00:20:03.800 --> 00:20:06.810
WAHID BHIMJI: Well, yeah,
I don't think it's real.

00:20:06.810 --> 00:20:10.580
But no, I guess I was going
to say whether or not--

00:20:10.580 --> 00:20:13.050
so this example
isn't supersymmetry.

00:20:13.050 --> 00:20:15.730
I can tell you that.

00:20:15.730 --> 00:20:19.700
But yeah, part of the point
of this network, I guess,

00:20:19.700 --> 00:20:23.810
is it vastly improves our
sensitivity to supersymmetry.

00:20:23.810 --> 00:20:26.210
So hopefully it could
help answer the question

00:20:26.210 --> 00:20:27.560
if it's real or not.

00:20:27.560 --> 00:20:29.610
But yeah, certainly
there's no current evidence

00:20:29.610 --> 00:20:34.537
at the Large Hadron Collider,
so that's why we keep looking.

00:20:34.537 --> 00:20:36.620
And also I would say that
some of these approaches

00:20:36.620 --> 00:20:39.720
also might help you look in
different ways, for example,

00:20:39.720 --> 00:20:42.530
not being so
sensitive to the model

00:20:42.530 --> 00:20:44.510
that theorists
have come up with.

00:20:44.510 --> 00:20:48.380
So this approach really
trains on simulated samples

00:20:48.380 --> 00:20:50.990
of a particular model that
you're looking out for.

00:20:50.990 --> 00:20:53.420
But some of the ways that
we're trying to extend this

00:20:53.420 --> 00:20:56.554
is to be more of a sort
of anomaly detection,

00:20:56.554 --> 00:20:58.720
looking for things that you
might not have expected.

00:21:03.930 --> 00:21:15.680
AUDIENCE: [INAUDIBLE]

00:21:15.680 --> 00:21:20.180
WAHID BHIMJI: Yeah, so the thing
with these big collaborations

00:21:20.180 --> 00:21:22.580
that work at the LHC is they're
very sensitive about you

00:21:22.580 --> 00:21:26.160
working on the real data.

00:21:26.160 --> 00:21:28.970
So, for example, someone
who's coming from outside

00:21:28.970 --> 00:21:31.430
can't just apply this
model on the real data.

00:21:31.430 --> 00:21:34.280
You have to work within
the collaboration.

00:21:34.280 --> 00:21:36.707
So the reason why
they don't use things

00:21:36.707 --> 00:21:38.540
like this in the
collaboration at the moment

00:21:38.540 --> 00:21:43.040
is partly just because
of the kind of rigor

00:21:43.040 --> 00:21:45.620
that goes into
cross-validating and checking

00:21:45.620 --> 00:21:47.350
that all these
models are correct.

00:21:47.350 --> 00:21:49.940
And that just takes a bit
of time for it to percolate.

00:21:49.940 --> 00:21:51.800
There's a little
bit of a skepticism

00:21:51.800 --> 00:21:55.220
amongst certain people
about new ideas, I guess.

00:21:55.220 --> 00:21:57.797
And so I think the
fact that this has now

00:21:57.797 --> 00:21:59.630
been demonstrated in
several studies sort of

00:21:59.630 --> 00:22:03.350
adds a bit of
mitigation to that.

00:22:03.350 --> 00:22:04.970
But then the other
reason, I think,

00:22:04.970 --> 00:22:07.130
is that there's another
practical reason, which

00:22:07.130 --> 00:22:10.670
is that this was exploiting
kind of the full raw data.

00:22:10.670 --> 00:22:13.045
And apart from-- there are
sort of practical reasons

00:22:13.045 --> 00:22:14.420
why you wouldn't
want to do that,

00:22:14.420 --> 00:22:17.450
because these are large
hundreds of petabyte datasets.

00:22:17.450 --> 00:22:21.080
And so filtering, in terms
of these high-level physics

00:22:21.080 --> 00:22:23.540
variables, is not
only done because they

00:22:23.540 --> 00:22:25.310
think they know the
physics, but also

00:22:25.310 --> 00:22:28.020
because of practicality
of storing the data.

00:22:28.020 --> 00:22:30.150
So those are a bunch of reasons.

00:22:30.150 --> 00:22:33.350
But then I gave another sort
of technical reason, which

00:22:33.350 --> 00:22:36.010
is that you might be
more sensitive to this,

00:22:36.010 --> 00:22:38.150
to mismodeling in
the training data.

00:22:38.150 --> 00:22:41.010
And they really care about
systematic uncertainties

00:22:41.010 --> 00:22:42.000
and so forth.

00:22:42.000 --> 00:22:44.750
And those are
difficult to model when

00:22:44.750 --> 00:22:48.209
you don't know what it is you're
trying to model for, I guess,

00:22:48.209 --> 00:22:50.750
because you don't know what the
network necessarily picked up

00:22:50.750 --> 00:22:52.420
on.

00:22:52.420 --> 00:22:55.270
And so there's various
ways to mitigate

00:22:55.270 --> 00:22:56.890
some of these
technical challenges,

00:22:56.890 --> 00:23:03.220
including sort of mixing of
data with simulated data,

00:23:03.220 --> 00:23:07.150
and also using different
samples that don't necessarily

00:23:07.150 --> 00:23:11.190
have all the same modeling
effects, and things like that.

00:23:11.190 --> 00:23:12.760
So there's a part
that's cultural,

00:23:12.760 --> 00:23:14.520
but there's also a
part that's technical.

00:23:14.520 --> 00:23:18.100
And so I think we can try and
address the technical issues.

00:23:23.130 --> 00:23:40.957
AUDIENCE: [INAUDIBLE]

00:23:40.957 --> 00:23:42.540
WAHID BHIMJI: Not
directly this model.

00:23:42.540 --> 00:23:45.870
But yeah, there is certainly
a huge range of work.

00:23:45.870 --> 00:23:49.320
So we mostly work with the
Department of Energy science.

00:23:49.320 --> 00:23:52.710
And there's projects
across all of here.

00:23:52.710 --> 00:23:55.050
So, I mean, we can
only work in depth,

00:23:55.050 --> 00:23:56.465
I guess, with a few projects.

00:23:56.465 --> 00:23:57.840
So we have a sort
of few projects

00:23:57.840 --> 00:23:58.798
where we work in depth.

00:23:58.798 --> 00:24:01.200
But really part of what
our group does at NERSC

00:24:01.200 --> 00:24:03.570
is also to make these tools
like TensorFlow working

00:24:03.570 --> 00:24:06.210
at scale on the computer
available to the whole science

00:24:06.210 --> 00:24:07.740
community.

00:24:07.740 --> 00:24:10.380
And so we're having more
and more training events,

00:24:10.380 --> 00:24:13.210
and the community
are picking up--

00:24:13.210 --> 00:24:16.470
different communities
are picking this up more

00:24:16.470 --> 00:24:17.730
by themselves.

00:24:17.730 --> 00:24:19.340
And that really allows--

00:24:19.340 --> 00:24:21.120
so there certainly
will be groups

00:24:21.120 --> 00:24:24.459
working with other parts
of physics, I think.

00:24:24.459 --> 00:24:30.880
AUDIENCE: [INAUDIBLE]

00:24:30.880 --> 00:24:32.670
WAHID BHIMJI: Why is
deep learning, right?

00:24:32.670 --> 00:24:34.810
I mean, it's not really
the only approach.

00:24:34.810 --> 00:24:37.410
So like this actual
diagram doesn't just

00:24:37.410 --> 00:24:38.410
refer to deep learning.

00:24:38.410 --> 00:24:40.118
So some of these
projects around the edge

00:24:40.118 --> 00:24:42.850
here are not using
deep learning.

00:24:42.850 --> 00:24:45.410
My talk was just about a
few deep learning examples.

00:24:45.410 --> 00:24:48.960
So it's not necessarily
the be all and end all.

00:24:48.960 --> 00:24:51.630
I think one of the
advantages is that there

00:24:51.630 --> 00:24:54.720
is a huge amount of development
work in methods in things

00:24:54.720 --> 00:24:58.640
like convolution neural networks
and stuff that we can exploit.

00:24:58.640 --> 00:25:00.192
Relatively off
the shelf, they're

00:25:00.192 --> 00:25:02.400
already available in tools
like TensorFlow and stuff.

00:25:02.400 --> 00:25:07.290
So I mean, I think
that's one advantage.

00:25:07.290 --> 00:25:12.880
The other advantage is there is
quite a large amount of data.

00:25:12.880 --> 00:25:15.720
There are nonlinear
features and stuff

00:25:15.720 --> 00:25:18.690
that can be captured by
these more detailed models

00:25:18.690 --> 00:25:20.860
and so forth.

00:25:20.860 --> 00:25:22.766
So I don't know.

00:25:22.766 --> 00:25:24.390
It's not necessarily
the only approach.

00:25:24.390 --> 00:25:53.492
AUDIENCE: [INAUDIBLE]

00:25:53.492 --> 00:25:55.450
WAHID BHIMJI: Yeah, so,
I mean, there certainly

00:25:55.450 --> 00:25:59.410
are cases where the current
models are being well tuned,

00:25:59.410 --> 00:26:01.580
and they are right.

00:26:01.580 --> 00:26:05.545
But there's many cases where
you can get an advantage.

00:26:05.545 --> 00:26:08.170
I mean, you might think that the
physics sections here, they're

00:26:08.170 --> 00:26:09.420
well tuned over certain years.

00:26:09.420 --> 00:26:11.950
They know what the
variables are to look for.

00:26:11.950 --> 00:26:15.260
But there is a performance gain.

00:26:15.260 --> 00:26:17.330
And it can just be from
the fact that you're

00:26:17.330 --> 00:26:19.280
exploiting more
information in the event

00:26:19.280 --> 00:26:21.830
that you were otherwise throwing
away, because you thought

00:26:21.830 --> 00:26:24.840
you knew everything that was
important about those events,

00:26:24.840 --> 00:26:26.540
but actually there
was stuff that

00:26:26.540 --> 00:26:30.240
leaked outside, if you like, the
data that you were looking at.

00:26:30.240 --> 00:26:35.200
So there are gains to
be had I think, yeah.

