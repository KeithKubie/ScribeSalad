WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.327
[MUSIC PLAYING]

00:00:03.327 --> 00:00:05.160
MAGNUS HYTTSTEN: Hello,
I am Magnus Hyttsten

00:00:05.160 --> 00:00:06.201
for "The Developer Show."

00:00:06.201 --> 00:00:07.980
We're here at the
TensorFlow Dev Summit.

00:00:07.980 --> 00:00:09.570
Next to me, I have Zak Stone.

00:00:09.570 --> 00:00:10.364
Hi, Zak.

00:00:10.364 --> 00:00:11.280
ZAK STONE: Hi, Magnus.

00:00:11.280 --> 00:00:13.321
MAGNUS HYTTSTEN: So I hear
that you're the person

00:00:13.321 --> 00:00:14.580
to talk to about TPUs.

00:00:14.580 --> 00:00:15.280
Is that correct?

00:00:15.280 --> 00:00:17.070
ZAK STONE: Oh, there's
a huge team working

00:00:17.070 --> 00:00:20.071
on TPUs all across the company,
but I'm definitely involved.

00:00:20.071 --> 00:00:22.320
I'm real excited about
bringing a lot more performance

00:00:22.320 --> 00:00:23.494
to the TensorFlow community.

00:00:23.494 --> 00:00:24.410
MAGNUS HYTTSTEN: Cool.

00:00:24.410 --> 00:00:28.160
So can you describe what the
TPU is and what you use it for?

00:00:28.160 --> 00:00:29.310
ZAK STONE: Sure.

00:00:29.310 --> 00:00:34.800
Google has developed a
system of ASICs, boards,

00:00:34.800 --> 00:00:37.530
and entire supercomputers
to accelerate performance

00:00:37.530 --> 00:00:38.680
in machine learning.

00:00:38.680 --> 00:00:42.150
So we needed this for
internal purposes, initially.

00:00:42.150 --> 00:00:45.210
There was concern
several years ago that,

00:00:45.210 --> 00:00:48.090
if every Android phone
user spoke to their phone

00:00:48.090 --> 00:00:50.180
for a few minutes a
day, with algorithms

00:00:50.180 --> 00:00:51.604
of the time on big
fleets of CPUs,

00:00:51.604 --> 00:00:53.895
Google might need to double
the number of data centers.

00:00:53.895 --> 00:00:55.260
And so that didn't make sense.

00:00:55.260 --> 00:00:55.680
MAGNUS HYTTSTEN: Wow.

00:00:55.680 --> 00:00:56.580
ZAK STONE: Right?

00:00:56.580 --> 00:00:58.590
And so there's
this crash program

00:00:58.590 --> 00:01:01.392
to develop a chip that was
specialized to accelerate

00:01:01.392 --> 00:01:03.100
some of these machine
learning workloads.

00:01:03.100 --> 00:01:05.370
And so that was Google's
first TPU, which

00:01:05.370 --> 00:01:08.437
was announced at Google
I/O, I think, two years ago.

00:01:08.437 --> 00:01:10.020
So then last year,
we revealed that we

00:01:10.020 --> 00:01:12.600
have the second
generation system, the TPU

00:01:12.600 --> 00:01:17.730
v2, that's also available
now in Cloud as a Cloud TPU.

00:01:17.730 --> 00:01:19.400
It's just recently gone to beta.

00:01:19.400 --> 00:01:22.810
And so this supports
training and inference.

00:01:22.810 --> 00:01:25.150
It's 180 teraflops per device.

00:01:25.150 --> 00:01:27.150
And then these devices
can be connected together

00:01:27.150 --> 00:01:30.731
into these TPU pods that go
up to 11 and 1/2 petaflops,

00:01:30.731 --> 00:01:31.230
and so--

00:01:31.230 --> 00:01:32.690
MAGNUS HYTTSTEN: 11
and 1/2 petaflops?

00:01:32.690 --> 00:01:33.170
ZAK STONE: Exactly.

00:01:33.170 --> 00:01:33.410
MAGNUS HYTTSTEN: That's amazing.

00:01:33.410 --> 00:01:34.060
ZAK STONE: Let me put
that in context for you.

00:01:34.060 --> 00:01:36.480
So if you're training an
image recognition model--

00:01:36.480 --> 00:01:38.627
let's say ResNet
50, it's the sort

00:01:38.627 --> 00:01:39.960
of standard benchmark right now.

00:01:39.960 --> 00:01:42.160
It was state of the
art not too long ago.

00:01:42.160 --> 00:01:45.750
And if you want to train that
to 75% or 76% accuracy, which

00:01:45.750 --> 00:01:48.780
is what you'd expect from
publications on the subject,

00:01:48.780 --> 00:01:51.420
that might previously have
taken you days a few years

00:01:51.420 --> 00:01:53.220
ago, when that
paper was published.

00:01:53.220 --> 00:01:55.630
Now that's down to
about 12 and 1/2 hours

00:01:55.630 --> 00:01:57.540
on one of these Cloud TPUs.

00:01:57.540 --> 00:02:01.170
And on the full TPU pod, you
can do that in less than 12

00:02:01.170 --> 00:02:01.837
and 1/2 minutes.

00:02:01.837 --> 00:02:03.169
MAGNUS HYTTSTEN: That's amazing.

00:02:03.169 --> 00:02:03.900
ZAK STONE: Yeah.

00:02:03.900 --> 00:02:05.760
MAGNUS HYTTSTEN: So I bet
you're all wondering out there,

00:02:05.760 --> 00:02:07.940
how can I get my hands
on one of these things?

00:02:07.940 --> 00:02:08.940
ZAK STONE: That's right.

00:02:08.940 --> 00:02:10.860
Well, they're in beta
right now, so you have

00:02:10.860 --> 00:02:12.240
to request quota at the moment.

00:02:12.240 --> 00:02:13.950
But soon, we'll lift
that requirement,

00:02:13.950 --> 00:02:15.366
and you'll be able
to just fire up

00:02:15.366 --> 00:02:17.511
a Cloud TPU as
infrastructure, just like you

00:02:17.511 --> 00:02:18.510
would a virtual machine.

00:02:18.510 --> 00:02:20.676
MAGNUS HYTTSTEN: So it's
not like you can go and buy

00:02:20.676 --> 00:02:22.500
one and install on
your local hard--

00:02:22.500 --> 00:02:23.100
the computer or anything.

00:02:23.100 --> 00:02:24.100
ZAK STONE: That's right.

00:02:24.100 --> 00:02:27.870
You go to cloud.google.com/tpu,
and you'd fill out the quota

00:02:27.870 --> 00:02:28.371
form.

00:02:28.371 --> 00:02:30.328
Or if you're already in
touch with Google Cloud

00:02:30.328 --> 00:02:32.940
in some other way, just talk to
your contact at Google Cloud.

00:02:32.940 --> 00:02:33.780
MAGNUS HYTTSTEN: That's amazing.

00:02:33.780 --> 00:02:34.090
ZAK STONE: Yes.

00:02:34.090 --> 00:02:36.298
MAGNUS HYTTSTEN: 180 teraflops,
ladies and gentlemen.

00:02:36.298 --> 00:02:38.055
ZAK STONE: I know.

00:02:38.055 --> 00:02:39.720
Another possibility.

00:02:39.720 --> 00:02:42.180
We also have this TensorFlow
Research Cloud, which

00:02:42.180 --> 00:02:43.720
is a set of 1,000 of these.

00:02:43.720 --> 00:02:46.200
So together, that's
180 petaflops.

00:02:46.200 --> 00:02:47.206
MAGNUS HYTTSTEN: 180?

00:02:47.206 --> 00:02:47.706
Awesome.

00:02:47.706 --> 00:02:48.748
ZAK STONE: 180 petaflops.

00:02:48.748 --> 00:02:49.955
MAGNUS HYTTSTEN: Oh, my gosh.

00:02:49.955 --> 00:02:52.360
ZAK STONE: And those, we're
making available at no cost

00:02:52.360 --> 00:02:56.592
through an application process
to top ML researchers out

00:02:56.592 --> 00:02:57.300
in the community.

00:02:57.300 --> 00:02:58.883
Because what we're
really trying to do

00:02:58.883 --> 00:03:01.710
here is accelerate the rate of
progress in machine learning.

00:03:01.710 --> 00:03:04.080
And so we know
that performance is

00:03:04.080 --> 00:03:05.580
one of the main gating factors.

00:03:05.580 --> 00:03:07.110
A lot of the amazing
results you've

00:03:07.110 --> 00:03:09.750
seen across vision, and speech,
and language, and robotics,

00:03:09.750 --> 00:03:11.166
and other things,
have really been

00:03:11.166 --> 00:03:13.650
driven by this massive
increase in the amount

00:03:13.650 --> 00:03:16.690
of available computation
and reduction in cost.

00:03:16.690 --> 00:03:18.540
And so we're trying to
make ML acceleration

00:03:18.540 --> 00:03:20.340
universally
accessible and useful,

00:03:20.340 --> 00:03:23.790
affordable for everybody,
and available at scale

00:03:23.790 --> 00:03:24.730
in the Cloud.

00:03:24.730 --> 00:03:25.980
MAGNUS HYTTSTEN: That's great.

00:03:25.980 --> 00:03:30.501
So 180 teraflops, all the
way up to 180 petaflops.

00:03:30.501 --> 00:03:31.670
This is Magnus Hyttsten.

00:03:31.670 --> 00:03:33.490
I'm here with Zak Stone.

00:03:33.490 --> 00:03:35.460
We're at the
TensorFlow Dev Summit,

00:03:35.460 --> 00:03:37.090
and this is "The
Developer Show."

00:03:37.090 --> 00:03:39.540
[MUSIC PLAYING]

