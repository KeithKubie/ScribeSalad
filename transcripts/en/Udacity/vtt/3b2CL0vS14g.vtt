WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:02.680
Now, let's actually try
to quantify all of this.

00:00:02.680 --> 00:00:07.020
Imagine I have an input image
which has been defocused, and

00:00:07.020 --> 00:00:09.740
what we basically want
to do is now look at.

00:00:09.740 --> 00:00:12.610
Remember the, all those defocused
regions that we'd looked at,

00:00:12.610 --> 00:00:16.379
basically calibrate these blur
kernels at different depths.

00:00:16.379 --> 00:00:17.440
If can calibrate them,

00:00:17.440 --> 00:00:22.230
perhaps we can now model the response
on an image because of different types,

00:00:22.230 --> 00:00:24.340
you know, different blur
kernels that are coming in.

00:00:24.340 --> 00:00:25.270
Because again,

00:00:25.270 --> 00:00:28.770
these are response functions we
saw when we moved objects around.

00:00:28.770 --> 00:00:30.918
If you actually went through
an exercise of doing that,

00:00:30.918 --> 00:00:33.380
what we did of trying to move an object.

00:00:33.380 --> 00:00:36.740
We might be able to kind of build
a model that could be used to generate

00:00:36.740 --> 00:00:38.160
this information.

00:00:38.160 --> 00:00:40.420
So, let's look at this
a little bit more carefully.

00:00:40.420 --> 00:00:45.790
So imagine, I could now come up with
some sort of a local sub-window and

00:00:45.790 --> 00:00:49.070
also look at how
the calibrated blur kernel for

00:00:49.070 --> 00:00:51.750
that image would look
like at certain depth k.

00:00:51.750 --> 00:00:54.990
That would actually be fk,
so, I have a window, yk,

00:00:54.990 --> 00:01:00.140
I have a new calibrated
blur at different depths k.

00:01:00.140 --> 00:01:01.100
And we call that fk.

00:01:01.100 --> 00:01:05.717
And basically, now, we can basically
come up with a sharp output x on that

00:01:05.717 --> 00:01:10.424
sub window, by doing what we know how
to do which is convolutions, right?

00:01:10.424 --> 00:01:13.154
So, it basically means is for
a specific depth,

00:01:13.154 --> 00:01:16.158
I can now come up with
different types of images.

00:01:16.158 --> 00:01:18.740
So, k1, k2 and k3.

00:01:18.740 --> 00:01:20.990
So, I'm just showing you
a simple table here.

00:01:20.990 --> 00:01:23.390
This is my image,
I found different sub-windows.

00:01:23.390 --> 00:01:27.040
And I can now compute if
I know the blur kernel.

00:01:27.040 --> 00:01:29.770
And, these are again, if you noticed
the ones that we had looked at as from

00:01:29.770 --> 00:01:32.190
the example of images earlier.

00:01:32.190 --> 00:01:34.615
When we basically moved
the lens moved the object,

00:01:34.615 --> 00:01:36.220
the light source away from the lens.

00:01:36.220 --> 00:01:38.420
We've got different spread
functions out of it.

00:01:38.420 --> 00:01:42.770
We can actually use this to now,
of course, compute sharp images.

00:01:42.770 --> 00:01:47.340
And of course, no surprise to you how
we do this is a simple convolution

00:01:47.340 --> 00:01:49.250
of the calibrated blur kernel.

00:01:49.250 --> 00:01:53.690
With the sharp window here to be
able to create a local sub-window.

00:01:53.690 --> 00:01:54.290
Right?

00:01:54.290 --> 00:01:55.650
So this is how we would actually do it.

00:01:55.650 --> 00:01:57.500
This is a forward process,
if I had this,

00:01:57.500 --> 00:01:58.620
I would actually be able to compute it.

00:01:58.620 --> 00:02:01.550
Of course, my interest is in
actually computing this, right?

00:02:01.550 --> 00:02:04.160
I want to be able to actually
come up with a sharps sub-window.

00:02:04.160 --> 00:02:06.700
So the inverse of this process
is what we are interested in.

00:02:06.700 --> 00:02:09.008
But, we can actually do
calibration this way.

