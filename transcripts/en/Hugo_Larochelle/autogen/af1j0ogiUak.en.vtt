WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.750
 in this capture we will see the cure 

00:00:02.280 --> 00:00:06.109
 learning regression 

00:00:03.750 --> 00:00:10.860
 logistics 

00:00:06.109 --> 00:00:12.509
 so we saw that we could derive the 

00:00:10.860 --> 00:00:14.219
 the tax collector's box we violate the 

00:00:12.509 --> 00:00:16.830
 principle of minimizing a loss 

00:00:14.219 --> 00:00:18.810
 we also saw that if we 

00:00:16.830 --> 00:00:21.029
 did that we had to do some 

00:00:18.810 --> 00:00:22.800
 approximations ie if we 

00:00:21.029 --> 00:00:24.300
 wanted to see the mouth of the tax collector have 

00:00:22.800 --> 00:00:26.300
 as an algorithm that minimize a 

00:00:24.300 --> 00:00:29.279
 some loss 

00:00:26.300 --> 00:00:33.239
 unfortunately we can not see it 

00:00:29.279 --> 00:00:36.320
 exactly like a licensed person who does 

00:00:33.239 --> 00:00:39.000
 gradient descent because at the point 

00:00:36.320 --> 00:00:41.730
 hxx is equal to zero 

00:00:39.000 --> 00:00:43.649
 we have a problem or the loss is actually 

00:00:41.730 --> 00:00:45.660
 no rivals ie we did not 

00:00:43.649 --> 00:00:48.870
 not to drift to the one we can 

00:00:45.660 --> 00:00:54.329
 calculate by our elements in the 

00:00:48.870 --> 00:00:57.210
 rector w &amp; cie wxx equals zero is 

00:00:54.329 --> 00:00:59.280
 one of the consequences of this is that 

00:00:57.210 --> 00:01:01.620
 learning can be a little 

00:00:59.280 --> 00:01:04.979
 unstable and so when we see her here 

00:01:01.620 --> 00:01:08.610
 it's an algorithm that poses a problem 

00:01:04.979 --> 00:01:10.619
 and who has the advantage between 

00:01:08.610 --> 00:01:12.200
 other than as 

00:01:10.619 --> 00:01:15.780
 learning will unfold 

00:01:12.200 --> 00:01:17.490
 optimizing the average loss on 

00:01:15.780 --> 00:01:20.310
 the training set will vary from 

00:01:17.490 --> 00:01:23.250
 way more list of way less unstable 

00:01:20.310 --> 00:01:24.869
 and so that will give us an algorithm 

00:01:23.250 --> 00:01:28.920
 which is sometimes easier to analyze 

00:01:24.869 --> 00:01:31.880
 at the level of its convergence then the 

00:01:28.920 --> 00:01:31.880
 progression of learning 

00:01:33.290 --> 00:01:38.280
 so that's the war we're going to see that has 

00:01:36.390 --> 00:01:40.920
 this property is the regression 

00:01:38.280 --> 00:01:42.270
 logistics versus the tax collector have 

00:01:40.920 --> 00:01:44.310
 in fact there is not much to 

00:01:42.270 --> 00:01:45.680
 change what we are going to change is that 

00:01:44.310 --> 00:01:49.490
 rather than make a prediction 

00:01:45.680 --> 00:01:52.770
 direct from the class either 0 or 1 

00:01:49.490 --> 00:01:55.130
 when we are going to compute hdx actually our 

00:01:52.770 --> 00:02:00.420
 model he will rather predict 

00:01:55.130 --> 00:02:03.450
 yes predict a probability that our 

00:02:00.420 --> 00:02:05.670
 input x belong to the class on 

00:02:03.450 --> 00:02:08.429
 could have been moderate the probability 

00:02:05.670 --> 00:02:10.739
 that entrance to leave with their 0 that 

00:02:08.429 --> 00:02:11.890
 change nothing but the signares witness of 

00:02:10.739 --> 00:02:13.569
 obviously if 

00:02:11.890 --> 00:02:18.010
 model the probity of belonging to the 

00:02:13.569 --> 00:02:21.190
 class does so h2x for regression 

00:02:18.010 --> 00:02:23.650
 logistics it does not correspond to either 0.1 

00:02:21.190 --> 00:02:25.870
 but it will rather match the 

00:02:23.650 --> 00:02:30.850
 model belief that the input x 

00:02:25.870 --> 00:02:32.470
 belong to the class and we are going 

00:02:30.850 --> 00:02:34.150
 mobilize this first in the form 

00:02:32.470 --> 00:02:35.950
 of a probability actually it's fine 

00:02:34.150 --> 00:02:38.290
 give a probability according to model that 

00:02:35.950 --> 00:02:41.170
 x belong to the class so that there 

00:02:38.290 --> 00:02:43.510
 be galley and to do that we go 

00:02:41.170 --> 00:02:44.800
 use a predictions function 

00:02:43.510 --> 00:02:47.320
 so a model that is going to be very 

00:02:44.800 --> 00:02:49.720
 similar to the perceptor 

00:02:47.320 --> 00:02:51.220
 we will once again make the product 

00:02:49.720 --> 00:02:54.640
 that with the rent with you the parameter of the 

00:02:51.220 --> 00:02:56.790
 bright blue xx but rather than passing 

00:02:54.640 --> 00:02:59.560
 this result 7 

00:02:56.790 --> 00:03:00.970
 this calculation is through a cei function 

00:02:59.560 --> 00:03:02.860
 we will go through a function 

00:03:00.970 --> 00:03:05.380
 called the logistics function or 

00:03:02.860 --> 00:03:10.260
 logistics in English which is also 

00:03:05.380 --> 00:03:14.170
 sometimes called the function if Moses 

00:03:10.260 --> 00:03:15.310
 which is illustrated here so it's a 

00:03:14.170 --> 00:03:17.049
 function that looks a bit like the 

00:03:15.310 --> 00:03:20.470
 function is the function if we 

00:03:17.049 --> 00:03:23.500
 remember gather here and so that 

00:03:20.470 --> 00:03:26.260
 it's zero in this long time that wxx 

00:03:23.500 --> 00:03:29.560
 is smaller 0 and then suddenly 0 

00:03:26.260 --> 00:03:31.959
 it goes up to 1 to double times exqi more 

00:03:29.560 --> 00:03:33.430
 big so they have for the function 

00:03:31.959 --> 00:03:34.690
 logistics in fact we will rather have 

00:03:33.430 --> 00:03:36.190
 a function lice 

00:03:34.690 --> 00:03:39.519
 it's a bit like an approximation 

00:03:36.190 --> 00:03:41.709
 finally list of the threshold function the 

00:03:39.519 --> 00:03:43.570
 very hot function and this 

00:03:41.709 --> 00:03:46.390
 function there and the drifts abl everywhere 

00:03:43.570 --> 00:03:49.360
 we do not have the continuity problem at 

00:03:46.390 --> 00:03:51.070
 zero point and what will make us go 

00:03:49.360 --> 00:03:52.750
 to be able to apply the December war 

00:03:51.070 --> 00:03:55.989
 two guards without problem at the level of 

00:03:52.750 --> 00:03:58.120
 some of the payroll calculation and the 

00:03:55.989 --> 00:04:00.730
 logistic function so its form by 

00:03:58.120 --> 00:04:03.400
 my tricks and the following form so the 

00:04:00.730 --> 00:04:07.360
 logistic function applied to wxx and 

00:04:03.400 --> 00:04:10.989
 1/1 + the exponential minus 

00:04:07.360 --> 00:04:12.190
 wx is ok so general way the 

00:04:10.989 --> 00:04:14.680
 logistics function or function 

00:04:12.190 --> 00:04:19.269
 sigmoid it's the same thing on a 

00:04:14.680 --> 00:04:21.609
 entrance to it's just one on one more 

00:04:19.269 --> 00:04:24.479
 the exponential of less 

00:04:21.609 --> 00:04:24.479
 entry 

00:04:24.820 --> 00:04:29.860
 and so that's fine it gives us a 

00:04:27.900 --> 00:04:31.750
 optimization problem that is going to be more 

00:04:29.860 --> 00:04:33.670
 smooth we have no problem derby 

00:04:31.750 --> 00:04:35.200
 which is not defined now commented 

00:04:33.670 --> 00:04:37.300
 what we do with the ex classification 

00:04:35.200 --> 00:04:39.880
 model there to just choose the 

00:04:37.300 --> 00:04:43.900
 class that is most likely according to 

00:04:39.880 --> 00:04:45.640
 our model so if h2x and bigger 

00:04:43.900 --> 00:04:47.200
 treat at 0.5 

00:04:45.640 --> 00:04:48.520
 we will choose class 1 it is the 

00:04:47.200 --> 00:04:50.530
 class that is most likely according to 

00:04:48.520 --> 00:04:53.350
 our model then if not you choose the 

00:04:50.530 --> 00:04:57.880
 class here what the world is a 

00:04:53.350 --> 00:05:00.190
 more visual commentary illustration 

00:04:57.880 --> 00:05:01.180
 our algorithm some navigation 

00:05:00.190 --> 00:05:03.280
 logistics calculation 

00:05:01.180 --> 00:05:05.650
 h2x when you need the product 

00:05:03.280 --> 00:05:09.880
 than at their den x and w so that's the 

00:05:05.650 --> 00:05:12.550
 sum of 6 xw 1 x 2 x w21 located 

00:05:09.880 --> 00:05:15.790
 until ex'im time wm we do the sum 

00:05:12.550 --> 00:05:16.960
 of all that and we go through the name 

00:05:15.790 --> 00:05:19.960
 arctic line and showcasing 

00:05:16.960 --> 00:05:21.820
 ideal Ticino or logistics and that that 

00:05:19.960 --> 00:05:25.030
 gives us our exit from another 

00:05:21.820 --> 00:05:28.330
 model this illustration the and takes the 

00:05:25.030 --> 00:05:29.860
 form the practices just to see the 

00:05:28.330 --> 00:05:31.150
 kind of calculation that we are going to be 

00:05:29.860 --> 00:05:32.050
 practice later also when we go 

00:05:31.150 --> 00:05:34.470
 talk about neural networks 

00:05:32.050 --> 00:05:34.470
 artificial 

00:05:35.850 --> 00:05:42.100
 ok so we define it 

00:05:39.030 --> 00:05:44.200
 h2x now to make 

00:05:42.100 --> 00:05:46.120
 learning permissible to losses 

00:05:44.200 --> 00:05:48.070
 we have to define a loss so our 

00:05:46.120 --> 00:05:50.440
 lass function is in regression 

00:05:48.070 --> 00:05:52.840
 logistics lodges the loss or we go 

00:05:50.440 --> 00:05:55.570
 use it's going to be what's called the 

00:05:52.840 --> 00:05:56.260
 Entropy lacrosse that takes shape 

00:05:55.570 --> 00:05:59.830
 next 

00:05:56.260 --> 00:06:04.810
 it will be less yt times the log buy 

00:05:59.830 --> 00:06:07.240
 xt - 1 - yt times the log of a month hdx 

00:06:04.810 --> 00:06:09.820
 Say it's a form that looks like a 

00:06:07.240 --> 00:06:11.020
 little strange maybe but for good 

00:06:09.820 --> 00:06:14.530
 understand what it's like to watch 

00:06:11.020 --> 00:06:16.230
 the two cases that can happen so 

00:06:14.530 --> 00:06:19.300
 the two possible values ​​of not moving away 

00:06:16.230 --> 00:06:21.520
 which is price is equal to 1 6 and that's 

00:06:19.300 --> 00:06:25.600
 from class 1 or 0.6 was 

00:06:21.520 --> 00:06:26.290
 belongs to the class zero so 6 yt 

00:06:25.600 --> 00:06:30.420
 is equal to 1 

00:06:26.290 --> 00:06:34.120
 we will see that his disappearance 

00:06:30.420 --> 00:06:35.080
 yt is equal to a nice to have a minus 

00:06:34.120 --> 00:06:37.240
 1 2 0 

00:06:35.080 --> 00:06:39.520
 so that everything ends there 

00:06:37.240 --> 00:06:41.949
 ferret here so let's say that the loss 

00:06:39.520 --> 00:06:42.699
 in fact your correspondent - loeb 2 h 

00:06:41.949 --> 00:06:44.710
 two excited 

00:06:42.699 --> 00:06:46.780
 so we will try to minimize - the 

00:06:44.710 --> 00:06:51.039
 pledge of equity is the same as 

00:06:46.780 --> 00:06:52.539
 maximize log of h2x earth and since 

00:06:51.039 --> 00:06:54.580
 loeb it's a function that is 

00:06:52.539 --> 00:06:56.970
 now monotonous growth with the 

00:06:54.580 --> 00:06:59.590
 same thing in fact that maximize h2x 

00:06:56.970 --> 00:07:02.310
 until hd exist this is the part that 

00:06:59.590 --> 00:07:05.319
 xts is equal to 1 knowing x had 

00:07:02.310 --> 00:07:07.240
 minimized this flat pair in the https 

00:07:05.319 --> 00:07:10.240
 galls has a it's going to be the same thing as 

00:07:07.240 --> 00:07:13.360
 maximize the probability that the model 

00:07:10.240 --> 00:07:20.650
 model a thing or make yt equal 

00:07:13.360 --> 00:07:23.139
 to 1 for the x in question so of in 

00:07:20.650 --> 00:07:25.300
 to contract gala we will indeed have 

00:07:23.139 --> 00:07:28.030
 a workout that will maximize the 

00:07:25.300 --> 00:07:30.789
 probability that the model has thing at 

00:07:28.030 --> 00:07:33.639
 does yt be equal to 1 so exist at 

00:07:30.789 --> 00:07:35.979
 from class 1 for our example 

00:07:33.639 --> 00:07:37.900
 and it and maintenance did the same 

00:07:35.979 --> 00:07:40.630
 thing i'm going to kidnap the kid are doing 

00:07:37.900 --> 00:07:44.259
 the same thing for y is zero 

00:07:40.630 --> 00:07:48.490
 well it's m6 who will disappear is m6 

00:07:44.259 --> 00:07:50.080
 va is equal to 1 before 1 - 0 and so this 

00:07:48.490 --> 00:07:54.699
 what will happen is that we are going to 

00:07:50.080 --> 00:07:56.889
 minimize - loeb 2 1 - completed xtc la 

00:07:54.699 --> 00:08:01.090
 same thing as just maximizing the log a 

00:07:56.889 --> 00:08:02.110
 excited month so minitel negative - a 

00:08:01.090 --> 00:08:05.800
 function it's the same thing as 

00:08:02.110 --> 00:08:08.169
 maximize a function and then again 

00:08:05.800 --> 00:08:09.190
 once maximize the look of a - hd 

00:08:08.169 --> 00:08:12.310
 excited it's going to be the same thing as 

00:08:09.190 --> 00:08:14.919
 maximize one month hdx so you're the same 

00:08:12.310 --> 00:08:17.590
 thing for the body y tega 0 it's 

00:08:14.919 --> 00:08:21.849
 as on maximize 1 - h to exist for 

00:08:17.590 --> 00:08:24.490
 equity data and hdx tc policy 

00:08:21.849 --> 00:08:26.889
 direct is equal to 1 knowing etc according to the 

00:08:24.490 --> 00:08:31.539
 model so 1 - this is the policy to 

00:08:26.889 --> 00:08:34.779
 y is zero to exist given 

00:08:31.539 --> 00:08:37.270
 according to the model so in the case there is 

00:08:34.779 --> 00:08:39.940
 equal to zero for the examples that are 

00:08:37.270 --> 00:08:41.950
 labeled according to class zero we're going 

00:08:39.940 --> 00:08:44.680
 rather maximize the probability according to 

00:08:41.950 --> 00:08:47.910
 our model that yt be is equal to zero 

00:08:44.680 --> 00:08:50.649
 knowing then avoid maximizing his 

00:08:47.910 --> 00:08:53.559
 trust in belief that 

00:08:50.649 --> 00:08:56.860
 leave belong to the class irritated 

00:08:53.559 --> 00:08:58.990
 gaza so that's for the function 

00:08:56.860 --> 00:09:01.629
 loss to explain why it's 

00:08:58.990 --> 00:09:03.939
 a loss function that makes sense and 

00:09:01.629 --> 00:09:05.379
 remain ultimately only to derivatives the 

00:09:03.939 --> 00:09:07.360
 learning rule for descent 

00:09:05.379 --> 00:09:09.670
 of guards who simply take 

00:09:07.360 --> 00:09:12.459
 each of the parameters of the country then of there 

00:09:09.670 --> 00:09:14.199
 subtract a learning rate or a 

00:09:12.459 --> 00:09:17.350
 gradient tax that's called 

00:09:14.199 --> 00:09:19.990
 sometimes times the partial derbi of my 

00:09:17.350 --> 00:09:22.569
 loss with respect to my parameters and one 

00:09:19.990 --> 00:09:25.449
 strikes at the diversion here but if we 

00:09:22.569 --> 00:09:28.059
 does the partial derivatives of by 

00:09:25.449 --> 00:09:32.559
 report a parameter w here it lies 

00:09:28.059 --> 00:09:33.939
 that we get this form here and we 

00:09:32.559 --> 00:09:35.379
 notice that it's exactly like a 

00:09:33.939 --> 00:09:35.949
 times the same shape as for the 

00:09:35.379 --> 00:09:37.779
 collector 

00:09:35.949 --> 00:09:40.029
 but what has changed is necessary 

00:09:37.779 --> 00:09:42.309
 remember that hdx for the 

00:09:40.029 --> 00:09:46.480
 logistic regression this is not the 

00:09:42.309 --> 00:09:48.759
 threshold function on applied to wx 

00:09:46.480 --> 00:09:51.309
 external is not it for regression 

00:09:48.759 --> 00:09:56.860
 logistics but it's rather the function 

00:09:51.309 --> 00:09:59.709
 logistics or symrise applied to ew times 

00:09:56.860 --> 00:10:01.329
 exit so the rule learns exactly the 

00:09:59.709 --> 00:10:05.050
 same form but that's the definition of 

00:10:01.329 --> 00:10:07.240
 hd explains that they come charging so 

00:10:05.050 --> 00:10:09.939
 that's the learning rule 

00:10:07.240 --> 00:10:11.230
 for logistical appreciation and for 

00:10:09.939 --> 00:10:11.949
 lead a regression model 

00:10:11.230 --> 00:10:13.329
 logistics 

00:10:11.949 --> 00:10:14.920
 what we do is the descent of 

00:10:13.329 --> 00:10:16.449
 stochastic gaza that is to say 

00:10:14.920 --> 00:10:18.550
 interests especially the examples 

00:10:16.449 --> 00:10:21.009
 drive then to each example we 

00:10:18.550 --> 00:10:22.120
 would calculate this update where there we 

00:10:21.009 --> 00:10:26.860
 will do that until we reach 

00:10:22.120 --> 00:10:28.990
 some certain stop criteria by 

00:10:26.860 --> 00:10:31.120
 example to rsa criteria would be that 

00:10:28.990 --> 00:10:32.980
 the error does not change more than 

00:10:31.120 --> 00:10:36.160
 certain threshold where we could determine 

00:10:32.980 --> 00:10:38.199
 a number of iterations taken so that's it 

00:10:36.160 --> 00:10:40.540
 so we can see that we managed to 

00:10:38.199 --> 00:10:43.629
 hervé still quite easily 

00:10:40.540 --> 00:10:47.610
 simply defined according to loss 

00:10:43.629 --> 00:10:49.209
 and a h2x prediction model does 

00:10:47.610 --> 00:10:52.509
 then people just did 

00:10:49.209 --> 00:10:54.259
 applied the derivation procedure for 

00:10:52.509 --> 00:10:56.819
 get our 

00:10:54.259 --> 00:10:58.129
 learning rule and like that we have 

00:10:56.819 --> 00:10:59.730
 got a new algorithm 

00:10:58.129 --> 00:11:02.480
 of learning which is the negation 

00:10:59.730 --> 00:11:02.480
 logistics 

