Speaker 1:          00:00          In this video, we'll complete our description of the contrasted divergence algorithm as applied to every sector, both the machine and get an actual concrete parameter update rule.

Speaker 2:          00:14          Yeah.

Speaker 1:          00:15          And the previous video we've talked about how to estimate the negative log probability gradient. We needed to estimate this a double expectation over both x and h in the model. And then, uh, this justified because this is actually attracted book with them to Compute is justified in approximation and you contrasted divergence. The idea is to replace this expectation by a point estimate where we'll actually just simple a single value for x and a to obtain that value of x, which we'll call [inaudible]. We actually perform a few steps of gib sampling initialized at the current training observation. So now let's look at how we can in the context of a restricted Boltzmann machine compute this fall, uh, update, uh, assuming that we've performed a, a few steps of gib sampling at starting at the current trainee example. X. T

Speaker 2:          01:10          yeah.

Speaker 1:          01:12          Okay. Um, we need, uh, to perform a few more their vacations to get the full algorithm. So I remember that we have to perform an expectation over age of the partial derivative of the energy with respect to any parameter. So let's do first the partial derivative of the energy with respect to uh, any prevnar. So actually we'll just do the case for the connection between a Djf hidden unit and the kief input. So the partial do they have of the energy with respect to a connection is does the partial there, they've of this, that's just the energy function. And we notice that there's only one term which involves this particular connection between h j and x. K. So really to be a strictly correct, I should have a j prime and k prime. But just to simplify, I'm using the same indices. Um, um, but you should understand what's going on here.

Speaker 1:          02:04          So really this whole some here in particular, this sum here is the only one that involves connections. So the partial [inaudible] affected this or that is zero. And then the only term here that involves exactly the same connection between HD and ex gay is the one that involves HJ index gay. So the partial derivative of that here is really the partial [inaudible] of the term Wj k times h j Aix, Aix, k for the same value of j and k. And uh, this is a term that's linear in my parameter, so it's value, it's just whatever it was multiplying that weight, which is h j o n x game. And then I get them minus which was here. And so if I want to put that into matrix form so they get the grade and that the energy with respect to the full matrix w then you should be able to see that this is just mine as the outer product of the age vector with the, uh, transport. So with the, the x factors, or in other words, the Matrix product between the column vector minus the column vector, h times their row vector x.

Speaker 1:          03:14          Now, if we perform the expectation with respect to age condition on any value of the factor x, so, uh, we have this, we're trying to do this for a theater equals to our connection. Uh, so we've shown that this is equal to that here minus h j times x. K. Um, so this is, uh, an expectation over all neurons in the hidden there h but this involves on the one had in neuron a DJ at hidden neuron. So really this is just a, some uh, over, uh, that neuron being called two zero, one of the term that we have here, but waited by the probability that HJ is equal equal to it's given value. And I notice that in this, um, there's two cases, uh, HD equals to zero. HP, h j sorry, equals zero h j equals two one if Hja zero, then this is zero.

Speaker 1:          04:12          And otherwise this term is just that equals to one. And so that's the only non zero terms. So really this is just equivalent to minus X. Gay Times are probably that Aisha is equal to one given x, which is given right here. And again, if we want to write this into matrix form, um, then we know this again, that we could write this as the other product of minus a vector, which I'm going to call h of x, which is the vector that contains the probability of each hidden unit being equal to one given X. I put that into a vector and a notice that this is just the sigmoid of, uh, the bias vector plus the Matrix of weights w times x. So I take that vector Hvacs, uh, and I, uh, do the other product with the vector x. And so the, uh, entry in that matrix at ro, uh, Jay and columnK is going to be exactly this. So none of this also that conveniently, uh, h of x is actually just the value of hidden layer in a feed for neural network with sigmoidal activation function. Uh, this is somewhat of a coincidence, but, uh, it start seeing that, you know, this is, this is really a neural network. There are relationships between the restrictive Bolsa machine and, uh, uh, uh, neural networks.

Speaker 1:          05:41          And so if we put everything together, it means that, eh, for a given a training example and a negative sample that I've, uh, simple with case steps of gib sampling initialize that XD some calling that x stilled. Uh, then if I want to update w by Matrix of connections, then want to take w minus some learning rate of an estimator of the gradient of the negative log probability of the observed training sample. We've seen that this expression was the difference between a, a positive phase and a negative phase, the positive phase and expectation condition on the observation. And the other one's an expectation where I'm also taking the expectation over the x factor. Uh, then I've argued that I can compute this exactly. So what I'll do is that I'll replace this double expectation, uh, with a point estimate where I'm going to a sample, a value for x, which I'm calling instilled and I'm given x still.

Speaker 1:          06:44          I can actually do the remaining part of the expectation over H and. N. I've shown that, uh, in a four and the value of x, this expectation with correspond to my h of x vector or four XD cause I'm conditioning our XD here, uh, uh, multiply by the transpose of XD and then this which has the same form, but instead of conditioning on x IX stilled, that's going to be uh, age stilled times the transpose of external. So I combine that together and multiply this by uh, Alpha. And then I can just increment w by that. Okay. So notice we had a minus here and the minus there. So that's why there's no minus here anymore. We have a plus here but, and then we have a minus here.

Speaker 1:          07:32          So to sum up the sort of go for a contrasted divergence corresponds to, uh, going over to training examples for the current training example performed case steps of Gib sampling to obtain a negative sample x stilled and, and I update my parameters. So I have the update for the Matrix W I'll leave it as an exercise to show that we get a similar update for B and c. Uh, so it's going to be what is observed minus what I've sampled. So for the sea vector, which is the bias for x, I get a term that involves x. And for the B vector, which, uh, is a bias for the hidden units, then I get the vector of probabilities as, uh, given, uh, condition on either x or x tilt. And so, and then I keep doing this until some stopping criteria. So for instance, if I, uh, until my parameters don't change too much or for a fixed number of a parameter which I could cross validate somehow, uh, uh, using some given criteria of generalization.

Speaker 1:          08:39          So, uh, this is, uh, uh, so the acronym we use code for contrast advisor divergence is CD and sometimes you'll see CD dash gave, uh, where I explicitly, uh, illustrate what's the value of, of, of gay. Um, so, uh, so the number of gifts as assembling steps, the bigger key is the less biased our estimate of the gradient is going to be. So if k was very, very large and it means that my gifts chain would have essentially converged to the true model of distribution. And so I just get a Monte Carlo estimate of the expectation. So in other words, in expectation, this estimate would be with corresponds to the true expectation. Um, and now the, the, the, the less k is big, however, uh, the less it's going to be, uh, the more bias. There's going to be my estimate of the, uh, uh, negative log likelihood gradient.

Speaker 1:          09:33          However, it turns out that to extract some features or uh, to initialize neural networks, which we call pre training, which is something we'll discuss later in this course. Uh, k equals one works well in practice. And, um, so there's been some research for trying to understand why that is. The initial paper and contrasted divergence was trying to argue that, uh, you know, this, this actually makes sense. Um, and so we'll just live with the fact that, and we are actually quite happy with this, that k equals one. So performing just one sample of age and then immediately resembling IX stilled actually provides a, a is a good procedure for getting a meaningful value for x still in order to learn a, uh, learn a good value of, of hidden units, uh, at least in terms of extracting features. Um, however, if we were to look at how good the model is and, uh, uh, it turns out that using larger values of k is actually beneficial. That is if we look at, um, uh, whether the value of the, uh, average negative log probability of, uh, a test set, for instance, um, if k becomes bigger than typically, this will be much better. But in terms of just extracting features that is getting a good value of the vector h of x, uh, which we could then use as a feature representation for another learning algorithm. It turns out that key equals one works well in practice. All right, so that summarizes the contrasted divergence algorithms.