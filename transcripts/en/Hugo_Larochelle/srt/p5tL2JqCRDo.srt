1
00:00:00,850 --> 00:00:03,550
In this video, we'll
finally look at the, uh,

2
00:00:03,680 --> 00:00:08,140
expression for the dividends of the last
function with respect to the parameters

3
00:00:08,141 --> 00:00:13,000
in our neural network. So
we're finally ready to, uh,

4
00:00:13,030 --> 00:00:15,370
get the expression for,
uh,

5
00:00:15,390 --> 00:00:18,460
the great end of the last function with
respect to the parameters that we need

6
00:00:18,670 --> 00:00:23,590
to, uh, apply these to castic
gradient descent out with
them for training a neural

7
00:00:23,591 --> 00:00:24,424
network.

8
00:00:26,260 --> 00:00:26,910
Yeah.

9
00:00:26,910 --> 00:00:31,070
So let's first look at the partial it
is with respect to the weights. So, uh,

10
00:00:31,090 --> 00:00:32,610
so when we are looking at the partial,

11
00:00:32,611 --> 00:00:36,860
the they've for the weight I j uh,

12
00:00:36,900 --> 00:00:41,030
for the hidden layer, Kay. So
one of the parcel with this, uh,

13
00:00:41,220 --> 00:00:42,390
the loss function.

14
00:00:42,900 --> 00:00:47,900
Now I'm the only pre activation that
depends on this weight is going to be

15
00:00:48,151 --> 00:00:53,151
deeply activation at the hidden
layer k for the IMF hidden unit.

16
00:00:54,330 --> 00:00:59,330
So Wij or k Ij is going to
connect that hidden unit with the,

17
00:01:00,600 --> 00:01:01,260
uh,

18
00:01:01,260 --> 00:01:06,260
hidden unit below in the using its
activation for the GF hidden unit that the

19
00:01:07,291 --> 00:01:10,260
layer below.
So using the chain rule,

20
00:01:10,261 --> 00:01:13,830
we can write that partial derivative
as the product of the partial.

21
00:01:13,831 --> 00:01:17,920
They'll live up the last function with
respect to deeply activation at the kid

22
00:01:17,940 --> 00:01:19,830
hidden layer for hidden unit.

23
00:01:20,040 --> 00:01:23,970
I times the partial native of that active,

24
00:01:24,110 --> 00:01:27,540
that pre activation with
respect to our parameter.

25
00:01:28,180 --> 00:01:33,180
And now we know that the reactivation
is a linear function with respect to the

26
00:01:33,841 --> 00:01:34,151
weight.

27
00:01:34,151 --> 00:01:39,151
So it's a linear transformation of the
activation of the hidden layer below and

28
00:01:41,251 --> 00:01:42,930
now in this son.
So the gradient,

29
00:01:42,931 --> 00:01:47,931
which respect Wij for the
[inaudible] layer of this term,

30
00:01:48,091 --> 00:01:50,640
the biases zero,
this does not depend on the weights.

31
00:01:51,030 --> 00:01:55,110
And here we have a some that involves
a bunch of different weights,

32
00:01:55,350 --> 00:01:57,240
but the only one that matters that is a,

33
00:01:57,241 --> 00:02:02,241
that involves Wij is the one for the value
of j that corresponds to this j here.

34
00:02:05,130 --> 00:02:08,760
And so, uh, here, then
we'll have this parameter.

35
00:02:08,761 --> 00:02:13,390
So the parameter multiply by
the activation of the JFK,

36
00:02:13,391 --> 00:02:16,400
then unit at the layer below.
So if we take the narratives,

37
00:02:16,410 --> 00:02:20,280
a narrative of that wear
suits back to Wk Ij, uh,

38
00:02:20,310 --> 00:02:23,790
we just get a,
the activation value.

39
00:02:24,270 --> 00:02:29,270
So this is just equal to the activation
of the JFF hidden unit in the hidden

40
00:02:29,491 --> 00:02:32,490
layer. That's just below the
hidden layer came my last one.

41
00:02:33,660 --> 00:02:34,010
Yeah.

42
00:02:34,010 --> 00:02:38,570
And uh, now, uh, so we just take
that and we multiply it by the, uh,

43
00:02:38,630 --> 00:02:42,410
pre activation,
a partial derivative,

44
00:02:42,590 --> 00:02:46,580
which we've seen before. We've
seen how to uh, wishing a form.

45
00:02:46,670 --> 00:02:51,310
We've seen the formula for,
okay,

46
00:02:51,311 --> 00:02:52,900
now what if we want the gradient.

47
00:02:53,710 --> 00:02:57,010
So if on the grain and
with respect to a matrix,

48
00:02:57,011 --> 00:03:01,240
then that's going to be instead of being
a of partial data live is going to be a

49
00:03:01,241 --> 00:03:06,241
matrix of partial dividends where the
elements I Jay is going to be the partial

50
00:03:07,510 --> 00:03:11,860
derivative with respect to
Wij for day Keith in layer,

51
00:03:13,200 --> 00:03:16,640
if you go back in the expression,
we see that this parcel,

52
00:03:16,680 --> 00:03:21,680
the four elements at row I n column j
is a just a multiplication of the pre

53
00:03:24,701 --> 00:03:28,390
activation,
a partial David dative for neuron.

54
00:03:28,420 --> 00:03:33,070
I times the value,
the activation value of the neuron,

55
00:03:33,520 --> 00:03:36,640
uh,
for the JF neuron at the layer below.

56
00:03:38,490 --> 00:03:43,490
So what we can do is take the vector
of a pre activation gradients.

57
00:03:46,140 --> 00:03:49,110
So that's uh, that's what the
grain in vector is going to be.

58
00:03:49,230 --> 00:03:52,690
The vector of partial. That
is for all potential, uh,

59
00:03:52,740 --> 00:03:55,530
p activation in the layer cake.

60
00:03:56,280 --> 00:04:01,280
And then multiply that by the vector
of activations at the layer below.

61
00:04:02,860 --> 00:04:06,900
But we're going to transpose
it to get a row vector.

62
00:04:07,410 --> 00:04:10,020
Now if we take a column
vector times a row vector,

63
00:04:10,140 --> 00:04:15,140
we're going to get a matrix
and d element at position.

64
00:04:15,960 --> 00:04:16,793
I,

65
00:04:17,410 --> 00:04:22,410
a row I n Colin Jay is simply going to
be the IAFF elements of the column Vector

66
00:04:25,500 --> 00:04:28,410
Times Djf elements of the row vector.

67
00:04:29,730 --> 00:04:34,730
And so indeed we'll get a matrix where
the element Ij is going to be the

68
00:04:35,041 --> 00:04:40,041
multiplication of the ASM and of that
vector times DJF element of that vector,

69
00:04:40,260 --> 00:04:42,570
which turns out to be,
uh,

70
00:04:42,620 --> 00:04:47,620
the actual value of the partial data
with this Spec to connection I Jay.

71
00:04:48,240 --> 00:04:52,860
So we get this expression actually does
correspond to the gradients with respect

72
00:04:52,861 --> 00:04:56,070
to the full matrix w for
the hidden layer cake.

73
00:04:59,560 --> 00:05:03,760
And, uh, now we can look at the partial
that is with respect to the biases.

74
00:05:04,480 --> 00:05:07,450
Uh, so again, the uh,

75
00:05:07,470 --> 00:05:12,470
only pre activation that depends on the
IMF bias for the hidden layer cake is

76
00:05:14,380 --> 00:05:19,380
going to be deeply activation for hidden
layer gay looking at it's hidden unit.

77
00:05:20,190 --> 00:05:23,380
So we can take with the chain rule
that partial there they have time,

78
00:05:23,381 --> 00:05:27,680
the partial [inaudible] of that pre
activation for the eye, uh, hitting unit,

79
00:05:27,780 --> 00:05:32,400
hidden layer k with respect
to our parameter. And uh,

80
00:05:32,470 --> 00:05:36,220
looking at this formula, we see that
this term does not depend on the bias,

81
00:05:36,460 --> 00:05:40,540
but this term depends on it. And uh,
it's actually just that variable.

82
00:05:40,541 --> 00:05:45,040
So if we think the partial derivative,
which respected that is going to be one.

83
00:05:45,340 --> 00:05:47,890
So this term multiplied by that,

84
00:05:47,891 --> 00:05:50,620
just essentially cancel this
out because it's equal to one.

85
00:05:50,650 --> 00:05:55,650
So we only get the value of the partial
daily of the pre activation for the IMF

86
00:05:56,570 --> 00:06:00,350
hidden unit.
Any food put that into a vector.

87
00:06:00,351 --> 00:06:03,320
Well that's just the gradients
for the p activation.

88
00:06:04,040 --> 00:06:08,660
So actually the gradient
for the bias for the hidden,

89
00:06:08,661 --> 00:06:11,360
they are k,
it's just a gradient for the reactivation.

90
00:06:11,450 --> 00:06:14,150
So it's actually very simple.
So with that,

91
00:06:14,151 --> 00:06:19,151
we have an expression for the grain is
for all of our weights and all our biases

92
00:06:19,641 --> 00:06:23,090
that we need to use to apply
stochastic gradient descent,

93
00:06:23,091 --> 00:06:24,440
training of neural networks.

