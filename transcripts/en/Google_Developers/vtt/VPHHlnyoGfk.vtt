WEBVTT
Kind: captions
Language: en

00:00:06.422 --> 00:00:08.490
BRIAN QUINLAN: Hey, everyone.

00:00:08.490 --> 00:00:10.260
My name's Brian.

00:00:10.260 --> 00:00:11.850
Thanks, Gus.

00:00:11.850 --> 00:00:15.720
And I'm a software engineer on
the App Engine Python team.

00:00:18.260 --> 00:00:22.380
Four months ago, we released the
latest App Engine runtime,

00:00:22.380 --> 00:00:24.600
Python 2.7.

00:00:24.600 --> 00:00:30.440
And it's already in the last
four months, it now gets 47%

00:00:30.440 --> 00:00:35.620
more applications than the
Python 2.5 runtime, which we

00:00:35.620 --> 00:00:38.070
released four years ago.

00:00:38.070 --> 00:00:41.135
So this talk is basically about
how you get the most out

00:00:41.135 --> 00:00:44.450
of the new Python 2.7 runtime.

00:00:44.450 --> 00:00:48.310
So I'm kind of targeting this
talk for people who know

00:00:48.310 --> 00:00:50.995
something about Python and know

00:00:50.995 --> 00:00:52.810
something about App Engine.

00:00:52.810 --> 00:00:55.410
If you don't know anything about
either, this talk is

00:00:55.410 --> 00:00:58.410
going to completely
go over your head.

00:00:58.410 --> 00:01:00.380
And this is where the people
who are watching this on

00:01:00.380 --> 00:01:01.700
YouTube have the
huge advantage.

00:01:01.700 --> 00:01:03.900
They can just hit the Back
button and go find something

00:01:03.900 --> 00:01:05.019
else to do.

00:01:05.019 --> 00:01:06.630
The people here who are sitting
in the middle are

00:01:06.630 --> 00:01:09.290
going to have to use some sort
of ninja skills to sneak by

00:01:09.290 --> 00:01:10.930
people and find a new
session without

00:01:10.930 --> 00:01:12.180
getting in people's way.

00:01:15.280 --> 00:01:17.970
Even if this talk isn't valuable
for you, I actually

00:01:17.970 --> 00:01:21.780
have App Engine plushies to give
away at the end as some

00:01:21.780 --> 00:01:22.850
form of bribery.

00:01:22.850 --> 00:01:25.320
AUDIENCE: Yeah!

00:01:25.320 --> 00:01:28.865
BRIAN QUINLAN: You can't
pre-claim them.

00:01:28.865 --> 00:01:31.025
They'll be first come, first
serve at the end.

00:01:36.530 --> 00:01:40.530
There's actually more to the
Python 2.7 runtime than just

00:01:40.530 --> 00:01:43.110
Python 2.7 language features,
which is actually why it's the

00:01:43.110 --> 00:01:45.450
second point on the slide,
not the first.

00:01:45.450 --> 00:01:48.850
So I'm going to talk
about four things.

00:01:48.850 --> 00:01:53.280
One is our new library support
for the Python 2.7.

00:01:53.280 --> 00:01:56.315
Then I will talk a bit about
just language features in

00:01:56.315 --> 00:02:00.390
Python 2.7 that are useful for
web application development,

00:02:00.390 --> 00:02:04.710
some of the limitations that
we've removed from the Python

00:02:04.710 --> 00:02:09.380
2.7 App Engine runtime versus
2.5, and concurrent requests,

00:02:09.380 --> 00:02:11.210
which is hard to explain
in one sentence.

00:02:11.210 --> 00:02:13.480
But it's possibly the most
valuable feature that we're

00:02:13.480 --> 00:02:15.180
adding to the Python
2.7 runtime.

00:02:18.220 --> 00:02:23.590
So I'm going to start talking
about third-party libraries.

00:02:23.590 --> 00:02:25.560
And I'm just going to leave this
slide up for a couple of

00:02:25.560 --> 00:02:30.950
seconds, so people read that
I designed this part.

00:02:30.950 --> 00:02:32.010
And then people laughed.

00:02:32.010 --> 00:02:32.880
I have my acknowledgement.

00:02:32.880 --> 00:02:33.760
And we can move on.

00:02:33.760 --> 00:02:37.800
We actually have to start a
bit in the past to really

00:02:37.800 --> 00:02:40.140
understand why this
is important.

00:02:40.140 --> 00:02:41.620
This isn't actually
the Big Bang.

00:02:41.620 --> 00:02:47.970
This is a picture of the
simulation of the LHC, Large

00:02:47.970 --> 00:02:53.350
Hadron Collider, producing Higgs
bosons, because, if you

00:02:53.350 --> 00:02:55.800
remember, this is all what
we were doing in 2008.

00:02:55.800 --> 00:03:00.180
We were basically waiting for
the LHC to start and end the

00:03:00.180 --> 00:03:01.430
entire universe.

00:03:04.010 --> 00:03:05.940
Well, the App Engine
team actually

00:03:05.940 --> 00:03:06.730
beat them to the punch.

00:03:06.730 --> 00:03:13.450
And we released App Engine in
April of 2008 with Python 2.5

00:03:13.450 --> 00:03:17.810
as our launch runtime.

00:03:17.810 --> 00:03:20.200
So I don't know if you remember
what the environment

00:03:20.200 --> 00:03:25.160
was like in 2008, but it
was pretty primitive.

00:03:25.160 --> 00:03:30.350
There were Tyrannosaurus rexes
chasing Triceratops around.

00:03:30.350 --> 00:03:34.920
Well not really, but we had some
pretty barbaric software.

00:03:34.920 --> 00:03:37.850
Django hadn't hit 1.0 yet.

00:03:37.850 --> 00:03:40.700
It's like the premiere
Python web framework.

00:03:40.700 --> 00:03:43.970
WebOb, which probably represents
another half of

00:03:43.970 --> 00:03:46.770
Python web applications that
are built on top of it, it

00:03:46.770 --> 00:03:50.060
hadn't hit 1.0 yet.

00:03:50.060 --> 00:03:53.230
And App Engine evolved.

00:03:53.230 --> 00:03:56.450
We made our Datastore
much more reliable.

00:03:56.450 --> 00:03:57.810
We offered an SLA.

00:03:57.810 --> 00:04:00.460
We went out of preview.

00:04:00.460 --> 00:04:04.740
But we were kind of stuck using
these older libraries

00:04:04.740 --> 00:04:06.170
for backwards compatibility.

00:04:06.170 --> 00:04:10.850
We had no real way of upgrading
without breaking all

00:04:10.850 --> 00:04:12.140
of our existing applications.

00:04:12.140 --> 00:04:13.810
And backwards compatibility
is something

00:04:13.810 --> 00:04:15.730
we take really seriously.

00:04:15.730 --> 00:04:18.430
It also meant that we couldn't
add features to some of our

00:04:18.430 --> 00:04:20.200
own libraries for
the same reason.

00:04:20.200 --> 00:04:23.610
We couldn't make changes that
we wanted to make, just

00:04:23.610 --> 00:04:27.070
because they would have
broken people.

00:04:27.070 --> 00:04:29.450
So the solution for this, well,
for the dinosaurs, it

00:04:29.450 --> 00:04:30.850
was a big comet.

00:04:30.850 --> 00:04:35.100
And for App Engine, it
was the release of

00:04:35.100 --> 00:04:36.810
the Python 2.7 runtime.

00:04:36.810 --> 00:04:37.970
This was our launch cake.

00:04:37.970 --> 00:04:39.890
It was tasty.

00:04:39.890 --> 00:04:43.450
And let me just jump
to the punch here.

00:04:43.450 --> 00:04:49.280
So in Python 2.7 on App Engine,
anyone who's done any

00:04:49.280 --> 00:04:51.740
App Engine development should
recognize this configuration

00:04:51.740 --> 00:04:54.890
file, or at least the
top, non-red part.

00:04:54.890 --> 00:04:57.550
It's basically we're saying,
we have an application that

00:04:57.550 --> 00:05:01.580
I've creatively called
myapp and then just

00:05:01.580 --> 00:05:02.595
some data for it.

00:05:02.595 --> 00:05:07.930
It handles every URL pattern
with a script called "main."

00:05:07.930 --> 00:05:10.340
And the new part is, we have
an explicit libraries

00:05:10.340 --> 00:05:11.350
declaration at the end.

00:05:11.350 --> 00:05:15.580
So this means my application
is dependent on Django.

00:05:15.580 --> 00:05:18.680
And I really need version 1.2.

00:05:18.680 --> 00:05:21.650
I'm explicitly depending
on version 1.2.

00:05:21.650 --> 00:05:24.800
So version 1.2 of Django is not
the latest version that we

00:05:24.800 --> 00:05:27.560
support on App Engine.

00:05:27.560 --> 00:05:30.230
But the person who wrote this
application doesn't have to

00:05:30.230 --> 00:05:32.920
care, because they've explicitly
stated, this is my

00:05:32.920 --> 00:05:34.040
dependency.

00:05:34.040 --> 00:05:37.370
And they can upgrade to a newer
version later, whenever

00:05:37.370 --> 00:05:38.220
they feel like it.

00:05:38.220 --> 00:05:39.940
It's under their control.

00:05:39.940 --> 00:05:45.280
So this basically solves the
problem of us not being able

00:05:45.280 --> 00:05:48.820
to provide newer software
due to reverse

00:05:48.820 --> 00:05:50.220
compatibility problems.

00:05:50.220 --> 00:05:54.760
You just explicitly state what
third-party modules you're

00:05:54.760 --> 00:05:55.770
dependent on.

00:05:55.770 --> 00:06:02.400
So let me make this more
concrete by showing you a

00:06:02.400 --> 00:06:05.460
real, Hello World
web application.

00:06:05.460 --> 00:06:12.270
So this is pulled out of the
tutorial for Python 2.5.

00:06:12.270 --> 00:06:17.540
So you can see we start by
importing a very simple

00:06:17.540 --> 00:06:19.390
framework that we wrote,
called webapp.

00:06:19.390 --> 00:06:21.080
And this is actually the
framework I was saying that we

00:06:21.080 --> 00:06:24.390
couldn't upgrade, or we had
problems upgrading for

00:06:24.390 --> 00:06:26.460
backwards compatibility
reason.

00:06:26.460 --> 00:06:29.200
We define a very simple handler
that, in response to

00:06:29.200 --> 00:06:32.690
get requests, just outputs
"Hello World!".

00:06:32.690 --> 00:06:35.500
We make an application object.

00:06:35.500 --> 00:06:38.800
We define a main function that
runs the application object.

00:06:38.800 --> 00:06:41.750
And then we do the standard
Python, if name equals, equals

00:06:41.750 --> 00:06:43.060
main, run the main function.

00:06:45.670 --> 00:06:46.120
It's simple.

00:06:46.120 --> 00:06:48.650
And a lot of this
is boilerplate.

00:06:48.650 --> 00:06:49.630
So if you look at the Python--

00:06:49.630 --> 00:06:50.692
AUDIENCE: 2.7!

00:06:50.692 --> 00:06:51.790
BRIAN QUINLAN: Ah!

00:06:51.790 --> 00:06:52.210
Right.

00:06:52.210 --> 00:06:54.010
So I have a terrible memory.

00:06:54.010 --> 00:06:57.650
So I have audience plants
to remind me of things.

00:06:57.650 --> 00:07:01.770
So the reminder here was this
code will work absolutely fine

00:07:01.770 --> 00:07:05.110
in the Python 2.7 runtime.

00:07:05.110 --> 00:07:08.010
You can just take this Python
2.5 application, and it will

00:07:08.010 --> 00:07:10.170
run perfectly in 2.7.

00:07:10.170 --> 00:07:12.680
But this is, if you were writing
new code, here was how

00:07:12.680 --> 00:07:16.500
you would write it idiomatically
in Python 2.7.

00:07:16.500 --> 00:07:19.770
So you can see I've replaced
the import

00:07:19.770 --> 00:07:21.770
of webapp with webapp2.

00:07:21.770 --> 00:07:27.000
So webapp2 is an open source,
lightweight web framework that

00:07:27.000 --> 00:07:34.160
was strongly inspired by
Google's webapp Framework.

00:07:34.160 --> 00:07:39.300
But the fact that it's versioned
like the other

00:07:39.300 --> 00:07:42.950
third-party like I showed you
before-- so that means it

00:07:42.950 --> 00:07:45.710
doesn't have to be strictly
compatible with the releases.

00:07:45.710 --> 00:07:48.980
You can say I am dependent
on an exact version.

00:07:48.980 --> 00:07:51.840
So in fact, it's so compatible
that the only thing I had to

00:07:51.840 --> 00:07:54.640
change to make the example work
is I changed webapp to

00:07:54.640 --> 00:07:57.210
webapp2 everywhere.

00:07:57.210 --> 00:07:59.585
And you also might have noticed
that the example is a

00:07:59.585 --> 00:08:01.210
bit smaller.

00:08:01.210 --> 00:08:04.800
That's because all of the
boilerplate that came at the

00:08:04.800 --> 00:08:09.500
end is no longer necessary
in the App

00:08:09.500 --> 00:08:11.740
Engine Python 2.7 runtime.

00:08:11.740 --> 00:08:16.700
This is because Python 2.7 can
natively speak WSGI, also

00:08:16.700 --> 00:08:20.110
called "Wiz-gee." Well,
the Python 2.5 runtime

00:08:20.110 --> 00:08:21.680
could only use CGI.

00:08:21.680 --> 00:08:24.030
So If you don't know what any
of those mean, it doesn't

00:08:24.030 --> 00:08:24.970
really matter.

00:08:24.970 --> 00:08:27.570
Think of it this way, five lines
of boilerplate are gone.

00:08:27.570 --> 00:08:28.820
Yay.

00:08:31.870 --> 00:08:36.480
And here's what the
configuration would look for

00:08:36.480 --> 00:08:36.900
Python 2.5.

00:08:36.900 --> 00:08:39.820
I have defined my application,
version 1.

00:08:39.820 --> 00:08:42.929
I'm using Python.

00:08:42.929 --> 00:08:44.820
This first start is completely
standard.

00:08:44.820 --> 00:08:46.500
It will be in every application
in the end.

00:08:46.500 --> 00:08:50.750
I say every URL should
be handled by

00:08:50.750 --> 00:08:52.290
my Hello World script.

00:08:52.290 --> 00:08:54.640
And in 2.7, it looks
very similar.

00:08:54.640 --> 00:08:58.460
The runtime has changed from
Python to Python 2.7.

00:08:58.460 --> 00:09:02.290
I have to add this explicit
thread, safety declaration.

00:09:02.290 --> 00:09:05.300
Right now, I'm just setting it
to false, which is kind of the

00:09:05.300 --> 00:09:06.920
safe option.

00:09:06.920 --> 00:09:12.790
And I'll talk a lot about how
you can set that to true at

00:09:12.790 --> 00:09:14.940
the end of the talk.

00:09:14.940 --> 00:09:20.460
The handler, the script changed
from helloworld.py to

00:09:20.460 --> 00:09:21.990
helloworld.app.

00:09:21.990 --> 00:09:24.340
I'll talk about why you make
that change later as well.

00:09:24.340 --> 00:09:26.860
And finally, I say I'm dependent
on the third-party

00:09:26.860 --> 00:09:28.130
library webapp.2.

00:09:28.130 --> 00:09:31.060
And I want to use the latest
version, which is 2.5.1.

00:09:35.236 --> 00:09:43.360
OK, but no one is going to make
a billion dollars with

00:09:43.360 --> 00:09:45.310
creating a Hello World
application.

00:09:45.310 --> 00:09:50.810
So this is my Google+,
some photos.

00:09:50.810 --> 00:09:55.110
Here's me hiking with some
friends, holding a big rock.

00:09:55.110 --> 00:09:56.910
And yeah, it's interesting.

00:09:56.910 --> 00:10:02.910
And here, this is an App Engine
application I wrote.

00:10:02.910 --> 00:10:08.260
And I just pasted the URL for
that picture at the end.

00:10:08.260 --> 00:10:12.720
And here's a picture of me
hiking with friends as viewed

00:10:12.720 --> 00:10:17.900
by The Predator two seconds
before it kills us all.

00:10:17.900 --> 00:10:21.336
Has everyone seen the
movie Predator?

00:10:21.336 --> 00:10:22.900
Oh, not everyone has
their hand up.

00:10:22.900 --> 00:10:27.230
It had great one-liners like,
"Get to the chopper," and "If

00:10:27.230 --> 00:10:31.800
it bleeds, we can kill it."

00:10:31.800 --> 00:10:34.350
So anyway, I'm thinking
about calling my

00:10:34.350 --> 00:10:38.930
new start-up Predigram.

00:10:38.930 --> 00:10:41.520
I think we can probably flip it
for about $1 billion in a

00:10:41.520 --> 00:10:42.980
year or so.

00:10:42.980 --> 00:10:46.900
And so, if you have maybe
$50 million of VC

00:10:46.900 --> 00:10:50.410
money, give me a call.

00:10:50.410 --> 00:10:54.090
And unlike most developers, I
will show you the code for our

00:10:54.090 --> 00:10:57.370
start-up before you even
have to invest.

00:10:57.370 --> 00:11:00.810
But looking at the code does
commit you to investing, so

00:11:00.810 --> 00:11:04.230
leave the room if you're--
ah, whatever.

00:11:04.230 --> 00:11:07.600
So if you look at the first line
of this example-- this is

00:11:07.600 --> 00:11:09.900
a complete example, by the way,
the entire application,

00:11:09.900 --> 00:11:12.680
21 lines of code that
you can see I

00:11:12.680 --> 00:11:13.810
squashed the imports together.

00:11:13.810 --> 00:11:15.590
So the first line
is just imports

00:11:15.590 --> 00:11:18.740
of core Python libraries.

00:11:18.740 --> 00:11:20.650
And it's the next line that's
actually a bit more

00:11:20.650 --> 00:11:21.340
interesting.

00:11:21.340 --> 00:11:26.000
I'm importing NumPy, which is
a Python numeric library and

00:11:26.000 --> 00:11:28.070
PIL, which is an imaging
library.

00:11:28.070 --> 00:11:31.600
So NumPy was the third most
requested third-party library

00:11:31.600 --> 00:11:32.520
for App Engine.

00:11:32.520 --> 00:11:34.250
And PIL was the number one.

00:11:34.250 --> 00:11:39.140
So both of those are now
supported in Python 2.7.

00:11:39.140 --> 00:11:41.410
And I'm not going to go into too
much detail about how you

00:11:41.410 --> 00:11:44.700
would actually do this kind
of image transformation.

00:11:44.700 --> 00:11:48.350
The basic idea is I don't care
what the original colors in

00:11:48.350 --> 00:11:51.320
the image are, I'm going to
replace them by my own color

00:11:51.320 --> 00:11:54.580
palette, which I defined
in the beginning.

00:11:54.580 --> 00:11:58.600
I'm using NumPy in a very
trivial way when I write 4H

00:11:58.600 --> 00:12:00.210
and NumPy linspace.

00:12:00.210 --> 00:12:03.390
That's basically I want a linear
interpolation between 0

00:12:03.390 --> 00:12:09.610
and 1 and 256 steps, so I build
a 256-color palette.

00:12:09.610 --> 00:12:13.350
And then, if you look in the get
handler for the request, I

00:12:13.350 --> 00:12:17.030
take a URL, which is all the
stuff that comes after image

00:12:17.030 --> 00:12:18.630
and the URL scheme.

00:12:18.630 --> 00:12:22.630
I load it using the
Python URL lib.

00:12:22.630 --> 00:12:25.340
I open the image I've
downloaded, convert it into

00:12:25.340 --> 00:12:29.070
grayscale, replace the palette,
convert it into PNG

00:12:29.070 --> 00:12:29.970
and output it.

00:12:29.970 --> 00:12:38.240
And that 21 lines of code can
be yours for $50 million.

00:12:38.240 --> 00:12:39.310
And the configuration--

00:12:39.310 --> 00:12:43.500
this is what the configuration
looks like.

00:12:43.500 --> 00:12:49.200
It's basically the same as what
we saw before, the same

00:12:49.200 --> 00:12:52.220
initial five lines to define
the application.

00:12:52.220 --> 00:12:56.150
Now I'm handling image slash
in my image space.

00:12:56.150 --> 00:12:58.200
My script name changed a bit.

00:12:58.200 --> 00:13:01.380
And I'm dependent
on NumPy, PIL.

00:13:01.380 --> 00:13:02.630
And I'm still using webapp2.

00:13:07.250 --> 00:13:12.820
OK, so those are some neat
libraries I touched on.

00:13:12.820 --> 00:13:16.640
And webapp2 is useful,
but less neat.

00:13:16.640 --> 00:13:20.170
Here is the full set of
third-party libraries that we

00:13:20.170 --> 00:13:24.390
support explicitly in the
Python 2.7 runtime.

00:13:24.390 --> 00:13:27.010
I'm not going to talk about them
all, but I'll point out a

00:13:27.010 --> 00:13:27.740
few others.

00:13:27.740 --> 00:13:32.410
So Jinja2 is a templating
library.

00:13:32.410 --> 00:13:37.750
In the Python 2.7 runtime, we
had our own simple templating

00:13:37.750 --> 00:13:42.160
system called webapp.template,
which was a very simple facade

00:13:42.160 --> 00:13:45.340
over Django.

00:13:45.340 --> 00:13:49.810
And our use of Django
was kind of sketchy.

00:13:49.810 --> 00:13:51.030
It wasn't thread-safe.

00:13:51.030 --> 00:13:52.600
It had some problems.

00:13:52.600 --> 00:13:56.440
So we're actually recommending
people who like Django

00:13:56.440 --> 00:13:59.350
templates and Django
configuration to just use

00:13:59.350 --> 00:14:01.740
Django templates directly.

00:14:01.740 --> 00:14:04.290
And if they want a very
compatible solution without

00:14:04.290 --> 00:14:05.220
having to do Django

00:14:05.220 --> 00:14:09.480
configuration, just use Jinja2.

00:14:09.480 --> 00:14:12.070
lxml I should probably mention,
because it was the

00:14:12.070 --> 00:14:15.200
second most requested
third-party library for App

00:14:15.200 --> 00:14:17.500
Engine Python.

00:14:17.500 --> 00:14:20.240
I only didn't create a demo for
it, because it's very hard

00:14:20.240 --> 00:14:24.510
to make an interesting demo
that involves XML parsing.

00:14:24.510 --> 00:14:28.490
But please contact me if
you can think of a way.

00:14:28.490 --> 00:14:32.050
And then the rest is just--

00:14:32.050 --> 00:14:35.400
there's more, but I'm not going
to get into them all.

00:14:35.400 --> 00:14:37.270
So I will mention that
this is early days.

00:14:37.270 --> 00:14:39.630
We released this runtime
four months ago.

00:14:39.630 --> 00:14:44.250
So this is the set four
months after release.

00:14:44.250 --> 00:14:48.770
This list will grow over time,
or you can expect it

00:14:48.770 --> 00:14:50.670
to grow over time.

00:14:50.670 --> 00:14:54.680
And we are responsive to people
filing feature requests

00:14:54.680 --> 00:14:56.390
on our external issue tracker.

00:14:56.390 --> 00:15:00.320
So you can see we package the
first most-requested module,

00:15:00.320 --> 00:15:02.565
the second most-requested
module, the

00:15:02.565 --> 00:15:03.700
third requested module.

00:15:03.700 --> 00:15:07.970
You can kind of see there's
probably a pattern here.

00:15:07.970 --> 00:15:10.070
Yeah, I think that's it.

00:15:10.070 --> 00:15:10.830
Move to the--

00:15:10.830 --> 00:15:10.935
AUDIENCE: [INAUDIBLE].

00:15:10.935 --> 00:15:11.040
BRIAN QUINLAN: Sorry?

00:15:11.040 --> 00:15:11.460
AUDIENCE: [INAUDIBLE].

00:15:11.460 --> 00:15:12.090
BRIAN QUINLAN: Ah.

00:15:12.090 --> 00:15:12.920
Right.

00:15:12.920 --> 00:15:13.460
See?

00:15:13.460 --> 00:15:15.110
Audience plants,
very important.

00:15:15.110 --> 00:15:21.520
So I also mentioned that these
are the libraries that we

00:15:21.520 --> 00:15:24.050
provide explicit support
for you.

00:15:24.050 --> 00:15:27.400
Python says it's kind of a
batteries included language.

00:15:27.400 --> 00:15:30.250
We throw in some more batteries
that are included to

00:15:30.250 --> 00:15:32.980
make it easier to do
web development.

00:15:32.980 --> 00:15:35.290
But if there's another battery
you want, like your own web

00:15:35.290 --> 00:15:39.570
application framework like Flask
or Bottle or whatever,

00:15:39.570 --> 00:15:41.160
then it's no problem.

00:15:41.160 --> 00:15:42.550
You just download
that yourself.

00:15:42.550 --> 00:15:47.370
And it will get uploaded
with your application.

00:15:47.370 --> 00:15:50.880
Thank you, audience plant.

00:15:50.880 --> 00:15:51.530
OK, cool.

00:15:51.530 --> 00:15:54.420
This is my favorite section.

00:15:54.420 --> 00:15:58.360
There's things in Python 2.7
itself that are new that are

00:15:58.360 --> 00:16:00.260
useful for web development.

00:16:00.260 --> 00:16:03.010
And the great thing about this
is, basically, I can take

00:16:03.010 --> 00:16:05.240
credit for work that
other people did.

00:16:08.000 --> 00:16:10.560
So-- oh, more demo.

00:16:10.560 --> 00:16:14.230
So this is my value add.

00:16:14.230 --> 00:16:17.800
Having a billion dollar
company is cool, but

00:16:17.800 --> 00:16:19.070
I want a bit more.

00:16:19.070 --> 00:16:22.600
So I think this makes our
company worth $1.5 billion.

00:16:22.600 --> 00:16:26.670
So before, I had to paste a URL,
and it would generate the

00:16:26.670 --> 00:16:28.640
predator view of the image.

00:16:28.640 --> 00:16:36.000
Now I can just take Google+ IDs
and go to this different

00:16:36.000 --> 00:16:40.480
application, called Predator +,
and just paste the ID here.

00:16:40.480 --> 00:16:46.400
And it'll show you a Google+
profile using Predator Vision.

00:16:46.400 --> 00:16:50.590
So I think this will allow us to
make our own Google+ clone

00:16:50.590 --> 00:16:54.840
that provides the high-contrast
colors that the

00:16:54.840 --> 00:16:57.230
discriminating predator wants.

00:16:57.230 --> 00:17:00.890
And we can completely corner
the extraterrestrial market

00:17:00.890 --> 00:17:05.359
for Google+ or, at least,
homicidal extraterrestrials.

00:17:05.359 --> 00:17:08.660
Anyway, value add.

00:17:08.660 --> 00:17:12.270
Call me. $50 million
gets you in.

00:17:12.270 --> 00:17:18.069
So here's the app that adds
that little feature.

00:17:18.069 --> 00:17:19.960
Google+ has a JSON API.

00:17:19.960 --> 00:17:23.960
Or sorry, it has a high-level
Python API and also just a

00:17:23.960 --> 00:17:26.369
JSON RESTful API.

00:17:26.369 --> 00:17:29.450
I am using the JSON API because,
basically, my example

00:17:29.450 --> 00:17:31.730
would suck if I didn't,
because I'm trying to

00:17:31.730 --> 00:17:33.710
demonstrate JSON.

00:17:33.710 --> 00:17:39.413
So you access this profile URL,
filling in, you can see I

00:17:39.413 --> 00:17:40.540
have user ID there.

00:17:40.540 --> 00:17:42.990
You fill in the user ID with
the user's actual ID, that

00:17:42.990 --> 00:17:45.100
number you saw.

00:17:45.100 --> 00:17:46.640
And then let's look at
the actual code.

00:17:46.640 --> 00:17:52.000
So in my get request, instead
of taking a URL, I'm

00:17:52.000 --> 00:17:55.690
interpreting it as a user ID.

00:17:55.690 --> 00:18:00.060
If we read this code from the
inside out, I'm taking the

00:18:00.060 --> 00:18:06.310
profile URL and formatting it
using the passed-in user ID.

00:18:06.310 --> 00:18:09.040
So Format is a feature
that wasn't

00:18:09.040 --> 00:18:12.870
available in Python 2.5.

00:18:12.870 --> 00:18:17.750
The next thing I'm doing is I'm
using a URL Open to load

00:18:17.750 --> 00:18:19.530
the Google+ profile.

00:18:19.530 --> 00:18:23.770
So there's a Timeout Argument
that is now available in

00:18:23.770 --> 00:18:25.880
Python 2.7 for URL Open.

00:18:25.880 --> 00:18:27.650
And that seems like a really
small thing, but it's actually

00:18:27.650 --> 00:18:30.430
a huge thing for web application
development.

00:18:30.430 --> 00:18:35.100
If you are using a Twitter API
or Facebook or whatever,

00:18:35.100 --> 00:18:37.810
you're interacting with any
other service when you're

00:18:37.810 --> 00:18:41.550
building your web application,
it sucks if it's taking them

00:18:41.550 --> 00:18:45.510
10 seconds to handle requests,
if you are blocked for those

00:18:45.510 --> 00:18:49.040
10 seconds waiting for them
to come up with an answer.

00:18:49.040 --> 00:18:52.900
It's probably preferable to say,
look, I'll wait, in this

00:18:52.900 --> 00:18:54.060
case, 200 milliseconds.

00:18:54.060 --> 00:18:56.840
And if I don't get an answer
from them, then I'll just

00:18:56.840 --> 00:18:59.900
serve some sort of degraded
response.

00:18:59.900 --> 00:19:01.910
Well, in this particular case,
it's very degraded.

00:19:01.910 --> 00:19:03.720
I redirect them to
a static image.

00:19:03.720 --> 00:19:07.690
But you can probably do
something smarter than that.

00:19:07.690 --> 00:19:09.440
So I'm using this
Timeout thing.

00:19:09.440 --> 00:19:12.950
And finally, I'm using this new
C implemented JSON module

00:19:12.950 --> 00:19:16.130
that's available in
Python 2.7 that

00:19:16.130 --> 00:19:20.500
parses the Google+ profile.

00:19:20.500 --> 00:19:22.230
There is the code.

00:19:22.230 --> 00:19:26.970
By the way, this is a
programming demo hat trick,

00:19:26.970 --> 00:19:30.220
three new features in
three lines of code.

00:19:30.220 --> 00:19:32.460
And they're consecutive lines
of code, so it's actually a

00:19:32.460 --> 00:19:33.540
natural hat trick.

00:19:33.540 --> 00:19:36.240
No one cares about hockey
and-- anyway.

00:19:36.240 --> 00:19:36.660
[APPLAUSE]

00:19:36.660 --> 00:19:37.910
BRIAN QUINLAN: Thank you.

00:19:39.770 --> 00:19:47.770
So finally, down here, you can
see I build the URL just by

00:19:47.770 --> 00:19:51.310
extracting the image
from this.

00:19:51.310 --> 00:19:53.110
The profile is a Python
dictionary now.

00:19:53.110 --> 00:19:57.330
I look up the image
key, the URL key.

00:19:57.330 --> 00:19:59.170
I split off all these
query arguments that

00:19:59.170 --> 00:20:00.520
I don't care about.

00:20:00.520 --> 00:20:02.000
And I've got a URL.

00:20:02.000 --> 00:20:03.810
And then I just proceed,
like you saw in

00:20:03.810 --> 00:20:05.470
the previous example.

00:20:05.470 --> 00:20:07.710
There's good docs for the
Google+ API, if you're

00:20:07.710 --> 00:20:09.585
interested in how you would do
something like this yourself.

00:20:12.450 --> 00:20:16.540
OK, I'm kind of hurt,
actually.

00:20:16.540 --> 00:20:19.540
You all laughed at my VC thing,
but no one actually

00:20:19.540 --> 00:20:22.010
offered money.

00:20:22.010 --> 00:20:26.130
So I'm going to go into banking,
because banking is

00:20:26.130 --> 00:20:28.140
where the money's at.

00:20:28.140 --> 00:20:30.630
Sorry, that was bad.

00:20:30.630 --> 00:20:34.270
But I'm going to highlight
a few little things here.

00:20:34.270 --> 00:20:39.850
So I'm trying to model a banking
account system here.

00:20:42.700 --> 00:20:44.910
You can see that the first thing
that's kind of new, like

00:20:44.910 --> 00:20:47.840
in the last couple of months
new, is I'm using a new module

00:20:47.840 --> 00:20:49.210
called NDB.

00:20:49.210 --> 00:20:56.490
So NDB is the new,
next-generation Python

00:20:56.490 --> 00:20:58.780
Datastore framework.

00:20:58.780 --> 00:21:02.280
It has some great features that
are really hard to see,

00:21:02.280 --> 00:21:03.560
so I'll just tell
you about them.

00:21:03.560 --> 00:21:07.700
So one feature is it does
implicit caching.

00:21:07.700 --> 00:21:11.920
So when you're creating items
or looking them up, it'll

00:21:11.920 --> 00:21:14.400
cache them locally
in local memory.

00:21:16.930 --> 00:21:18.450
It has another level of caching

00:21:18.450 --> 00:21:20.520
where it will use Memcache.

00:21:20.520 --> 00:21:22.920
And then, finally, it will
use the Datastore

00:21:22.920 --> 00:21:24.340
as persistent storage.

00:21:24.340 --> 00:21:25.850
And that's done all
behind the scenes.

00:21:25.850 --> 00:21:28.820
You don't have to worry about
how that actually works.

00:21:28.820 --> 00:21:32.140
Another thing is it can be used
asynchronously, which I

00:21:32.140 --> 00:21:34.070
guess I could show,
but I won't.

00:21:34.070 --> 00:21:40.660
And I will show one feature,
because Guido van Rossum wrote

00:21:40.660 --> 00:21:42.270
this, and he told me I had to.

00:21:42.270 --> 00:21:44.990
And it's called Structured
Properties.

00:21:44.990 --> 00:21:47.670
So you can see I have
an Account class.

00:21:47.670 --> 00:21:51.310
And the second thing I have to
find, the most important

00:21:51.310 --> 00:21:54.720
thing, the balance, how
much cash you have.

00:21:54.720 --> 00:21:57.600
But above that, I've said
there's account_holder.

00:21:57.600 --> 00:21:59.750
And the account_holder
has a Structured

00:21:59.750 --> 00:22:02.110
Property, which is a contact.

00:22:02.110 --> 00:22:06.010
And in this bit of code, I don't
have to define exactly

00:22:06.010 --> 00:22:07.400
what a contact is.

00:22:07.400 --> 00:22:11.950
That can be another NDB model
defined elsewhere, in this

00:22:11.950 --> 00:22:14.060
case, another module
that defines

00:22:14.060 --> 00:22:15.770
exactly what's in a contact.

00:22:15.770 --> 00:22:21.920
And that can be reused across
different NDB entities.

00:22:21.920 --> 00:22:25.290
So this is a huge boon for
people who are doing

00:22:25.290 --> 00:22:26.710
complicated data modeling.

00:22:26.710 --> 00:22:28.900
It means that they don't
have to have--

00:22:28.900 --> 00:22:34.940
you don't need one entity that
defines 50,000 properties in

00:22:34.940 --> 00:22:37.070
it, like everything that
you would need to

00:22:37.070 --> 00:22:38.270
have a real bank account.

00:22:38.270 --> 00:22:41.580
It means you can abstract
away content

00:22:41.580 --> 00:22:44.600
into different modules.

00:22:44.600 --> 00:22:46.620
OK, but here's where the
important stuff happens.

00:22:46.620 --> 00:22:48.180
How do you transfer money?

00:22:48.180 --> 00:22:51.240
So ignore that decorator on
the next line, the NDB

00:22:51.240 --> 00:22:53.010
transactional thing.

00:22:53.010 --> 00:22:54.600
So here's how our
function works.

00:22:54.600 --> 00:22:57.440
So we're going to take
account_ID, where we're

00:22:57.440 --> 00:23:00.140
transferring the money from,
another account_ID that we're

00:23:00.140 --> 00:23:01.800
transferring to,
and an amount.

00:23:01.800 --> 00:23:04.220
That's probably fairly
straightforward.

00:23:04.220 --> 00:23:06.250
So we're going to actually,
given the ID, we're going to

00:23:06.250 --> 00:23:08.440
look up the two accounts,
the from_account and the

00:23:08.440 --> 00:23:08.990
to_account.

00:23:08.990 --> 00:23:10.840
And then we're going to just
to update the balances.

00:23:10.840 --> 00:23:16.200
So from_account gets the amount
deducted from it,

00:23:16.200 --> 00:23:19.790
to_account has the balance
added to it.

00:23:19.790 --> 00:23:22.600
If the from_account balance is
negative, then you get an

00:23:22.600 --> 00:23:24.600
insufficient cash exception.

00:23:24.600 --> 00:23:29.060
Otherwise, both of the
entities get saved.

00:23:29.060 --> 00:23:32.070
Now let's look back at
the decorator, NDB

00:23:32.070 --> 00:23:34.140
transactional xg=True.

00:23:34.140 --> 00:23:37.580
So what this means-- and this is
a fairly new feature in App

00:23:37.580 --> 00:23:42.030
Engine-- it means I'm defining
a transaction.

00:23:42.030 --> 00:23:44.860
So when you're calling this
function, this is done in a

00:23:44.860 --> 00:23:45.560
transaction.

00:23:45.560 --> 00:23:52.920
Either both accounts will be
updated with the semantics I

00:23:52.920 --> 00:23:55.320
defined here, or neither
will be.

00:23:55.320 --> 00:23:56.910
And this is guaranteed.

00:23:56.910 --> 00:24:02.280
So until recently, you could
only guarantee modifications

00:24:02.280 --> 00:24:05.620
to one entity group
in a transaction.

00:24:05.620 --> 00:24:07.380
Now you can do several.

00:24:07.380 --> 00:24:12.510
This is a huge win for writing
this kind of apps where you

00:24:12.510 --> 00:24:16.270
have to guarantee that
modifications are made to

00:24:16.270 --> 00:24:20.310
several entities at once.

00:24:20.310 --> 00:24:26.170
OK, I've written this code, but
who knows if it actually

00:24:26.170 --> 00:24:28.130
works or not.

00:24:28.130 --> 00:24:30.585
So one thing that a lot of
people don't take good

00:24:30.585 --> 00:24:33.760
advantage of in App Engine is we
have a really sweet testing

00:24:33.760 --> 00:24:35.010
framework called Testbed.

00:24:37.810 --> 00:24:41.760
And it basically allows you to
write unit tests, but using

00:24:41.760 --> 00:24:46.150
the simulated App Engine
framework, so you don't have

00:24:46.150 --> 00:24:49.270
to do complicated mocking.

00:24:49.270 --> 00:24:53.380
So let me skip over, basically,
all of the imports,

00:24:53.380 --> 00:24:54.790
the set up and tear down.

00:24:54.790 --> 00:24:58.440
This is boilerplate that you can
get just by looking at the

00:24:58.440 --> 00:25:01.400
Testbed documentation.

00:25:01.400 --> 00:25:05.790
And let's look at this test
insufficient cache.

00:25:05.790 --> 00:25:09.870
So basically, I create and store
two bank accounts, one

00:25:09.870 --> 00:25:13.530
for the VC, which has $1 million
in it, and one for me,

00:25:13.530 --> 00:25:15.510
which has $200 in it.

00:25:15.510 --> 00:25:17.930
And then this next line is
kind of interesting.

00:25:17.930 --> 00:25:20.810
This is a Python 2.7
testing feature.

00:25:20.810 --> 00:25:26.170
It means the following code,
like the code within this with

00:25:26.170 --> 00:25:29.380
block, I expect it to raise the

00:25:29.380 --> 00:25:31.720
insufficient cash exception.

00:25:31.720 --> 00:25:33.990
If it raises that exception,
it's working as intended.

00:25:33.990 --> 00:25:35.660
And if it doesn't raise
that exception, it's

00:25:35.660 --> 00:25:37.230
not working as intended.

00:25:37.230 --> 00:25:42.420
And then I transfer the
money from the VC

00:25:42.420 --> 00:25:44.530
account to Brian account.

00:25:44.530 --> 00:25:47.170
But I want $5 million,
not $1 million,

00:25:47.170 --> 00:25:48.070
which is all they have.

00:25:48.070 --> 00:25:50.210
So it all fails.

00:25:50.210 --> 00:25:55.350
So this assertRaises is
new in Python 2.7.

00:25:55.350 --> 00:25:59.680
But this is like a tiny tip of
what's new for testing in 2.7.

00:25:59.680 --> 00:26:05.140
Python 2.7 actually added over
20 new test methods.

00:26:05.140 --> 00:26:08.160
They rethought how the framework
fits together.

00:26:08.160 --> 00:26:10.440
There's a lot more places
where you can

00:26:10.440 --> 00:26:13.210
inject your test logic.

00:26:13.210 --> 00:26:16.950
So I would, if you're using
Python 2.7, look at the

00:26:16.950 --> 00:26:21.040
release notes for it and look at
all the new testing things.

00:26:21.040 --> 00:26:24.540
It's very easy to let testing
features slip by, unless

00:26:24.540 --> 00:26:27.820
you're really keeping up.

00:26:27.820 --> 00:26:31.980
There's a bunch of other stuff
that's added to Python 2.7.

00:26:31.980 --> 00:26:34.690
So I asked some Python
people what they

00:26:34.690 --> 00:26:36.270
thought was most important.

00:26:36.270 --> 00:26:38.100
I started by asking myself.

00:26:38.100 --> 00:26:41.270
And I thought this was the
most important thing, the

00:26:41.270 --> 00:26:45.190
datetime, timedelta class gained
a total seconds method.

00:26:45.190 --> 00:26:46.150
I added this myself.

00:26:46.150 --> 00:26:47.860
I think this will revolutionize
Python

00:26:47.860 --> 00:26:49.550
development.

00:26:49.550 --> 00:26:51.905
It replaces the one-liner that
you had to use before.

00:26:54.770 --> 00:26:58.070
I asked some other people, some
core Python developers.

00:26:58.070 --> 00:27:00.930
I got this really excited
email from Guido.

00:27:00.930 --> 00:27:05.780
It goes, Python 2.7 advantages,
class decorators!

00:27:05.780 --> 00:27:08.360
Very exciting for him.

00:27:08.360 --> 00:27:12.420
Alexander Motelli, author,
Python luminary,

00:27:12.420 --> 00:27:15.610
"Multi-context with"
was his thing.

00:27:15.610 --> 00:27:19.300
Brett Cannon, huge contributor,
"Dict and set

00:27:19.300 --> 00:27:22.930
comprehensions." So what we
really learned here is that

00:27:22.930 --> 00:27:25.920
core Python developers like
really esoteric features.

00:27:28.440 --> 00:27:31.970
But I have some other
real users.

00:27:31.970 --> 00:27:34.410
This conversation's going
on on my Google+.

00:27:34.410 --> 00:27:36.450
So here's what some
people think.

00:27:36.450 --> 00:27:39.440
"There was a big revamp
to itertools.

00:27:39.440 --> 00:27:45.790
A lot of powerful features
there." "The collections

00:27:45.790 --> 00:27:50.380
library has a lot of new classes
in it, like Counter

00:27:50.380 --> 00:27:54.850
and OrderDict." "You have set
literals now." And someone

00:27:54.850 --> 00:27:56.940
pointed out that the docs
are much better than

00:27:56.940 --> 00:27:58.190
they were for 2.5.

00:28:00.740 --> 00:28:05.770
OK, done taking credit time.

00:28:05.770 --> 00:28:08.930
So next, I'm going to talk about
some of the limitations

00:28:08.930 --> 00:28:11.870
we've removed from Python
2.7 versus Python 2.5.

00:28:15.350 --> 00:28:19.110
So most of them are fairly
esoteric, but this is kind of

00:28:19.110 --> 00:28:21.690
an interesting one.

00:28:21.690 --> 00:28:25.415
Who here knows the Python
timeit module?

00:28:25.415 --> 00:28:27.840
Oh, good.

00:28:27.840 --> 00:28:28.760
I actually think
that's probably

00:28:28.760 --> 00:28:30.670
the best Python module.

00:28:30.670 --> 00:28:32.870
And I looked up who wrote it,
because I was going to send

00:28:32.870 --> 00:28:33.710
them flowers or something.

00:28:33.710 --> 00:28:34.880
But it turned out
it was Guido.

00:28:34.880 --> 00:28:36.900
And I think he's got
enough credit for

00:28:36.900 --> 00:28:39.410
the language already.

00:28:39.410 --> 00:28:43.210
So it basically allows you to
run code many times and

00:28:43.210 --> 00:28:44.510
generate timings for it.

00:28:44.510 --> 00:28:50.370
So the code I'm testing
is code =.

00:28:50.370 --> 00:28:52.980
And then I have this
Pickle dumps line.

00:28:52.980 --> 00:28:55.160
So what that's doing is it's
taking a Python data

00:28:55.160 --> 00:28:59.440
structure, serializing it, and
writing it into a string I/O.

00:28:59.440 --> 00:29:03.140
And setup, the block above it,
defines the setup that's used

00:29:03.140 --> 00:29:05.060
before that code is run.

00:29:05.060 --> 00:29:08.710
Then I define the timer, which
takes the code in the setup.

00:29:08.710 --> 00:29:14.360
And I basically run
this 5,000 times.

00:29:14.360 --> 00:29:15.530
I do 10 trials.

00:29:15.530 --> 00:29:17.920
And I take the fastest one.

00:29:17.920 --> 00:29:19.060
And so I did this.

00:29:19.060 --> 00:29:20.630
And I ran it in Python 2.5.

00:29:20.630 --> 00:29:24.150
And I ran it in Python 2.7.

00:29:24.150 --> 00:29:28.350
And the Python 2.5 output was
3.33, meaning that it took

00:29:28.350 --> 00:29:33.550
3.33 seconds to execute
this benchmark.

00:29:33.550 --> 00:29:37.230
And then I ran it again in
Python 2.7, which took 0.45

00:29:37.230 --> 00:29:39.820
seconds to run this benchmark.

00:29:39.820 --> 00:29:41.290
So you can see--

00:29:41.290 --> 00:29:41.630
[APPLAUSE]

00:29:41.630 --> 00:29:43.640
BRIAN QUINLAN: Yay!

00:29:43.640 --> 00:29:46.160
Yeah, be excited.

00:29:46.160 --> 00:29:49.500
So you can see, in my previous
examples, like when I was

00:29:49.500 --> 00:29:54.140
doing PIL image manipulation, I
was using cStringIO, a very

00:29:54.140 --> 00:29:57.110
commonly used Python class.

00:29:59.630 --> 00:30:05.110
cPickle is a lot less commonly
used, at least explicitly.

00:30:05.110 --> 00:30:07.930
But we use it internally in
App Engine for things like

00:30:07.930 --> 00:30:11.830
Memcache, when you're storing
data in Memcache and also when

00:30:11.830 --> 00:30:14.410
you're doing things like if
you're using tasks to defer,

00:30:14.410 --> 00:30:15.490
things like that.

00:30:15.490 --> 00:30:18.540
So even if you don't use Pickle
directly, you can

00:30:18.540 --> 00:30:23.930
expect your App Engine apps that
indirectly use it, their

00:30:23.930 --> 00:30:26.710
performance to improve.

00:30:26.710 --> 00:30:28.875
Most of the rest of this stuff
is a bit esoteric, so I'll

00:30:28.875 --> 00:30:29.840
just skim through it.

00:30:29.840 --> 00:30:33.860
So we have some new modules,
cPickle and cStringIO.

00:30:33.860 --> 00:30:36.950
Before, in Python 2.5, they
were just aliased to their

00:30:36.950 --> 00:30:39.170
slower pure Python versions.

00:30:39.170 --> 00:30:40.560
We have imp.

00:30:40.560 --> 00:30:43.390
Please don't use that.

00:30:43.390 --> 00:30:46.100
It's kind of a dangerous module
to use, unless you

00:30:46.100 --> 00:30:47.120
really know what you're doing.

00:30:47.120 --> 00:30:51.390
Marshall, also fairly
esoteric.

00:30:51.390 --> 00:30:54.880
We support bytecode modification
on Python 2.7.

00:30:54.880 --> 00:30:59.170
So if you want to add go tos
to Python, you can do that.

00:30:59.170 --> 00:31:02.600
Actually, I can't think of a
non-esoteric use for bytecode

00:31:02.600 --> 00:31:04.050
modification either.

00:31:04.050 --> 00:31:07.250
And finally, we have direct
support for compiled Python

00:31:07.250 --> 00:31:09.640
files, so you don't have
to upload your

00:31:09.640 --> 00:31:11.190
source in Python 2.7.

00:31:11.190 --> 00:31:14.340
You can just upload
the compiled data.

00:31:14.340 --> 00:31:16.430
Probably, you don't want to
do that most of the time.

00:31:16.430 --> 00:31:19.790
But if you have a vendor who's
only providing you compiled

00:31:19.790 --> 00:31:24.660
code, then you can just use it
without needing the source.

00:31:24.660 --> 00:31:26.280
Or you can just re-negotiate
with your vendor,

00:31:26.280 --> 00:31:29.180
which is what I do.

00:31:29.180 --> 00:31:30.630
Finally, concurrent requests.

00:31:33.890 --> 00:31:34.810
I like my tagline.

00:31:34.810 --> 00:31:36.220
It's basically saving
you money while

00:31:36.220 --> 00:31:38.720
making your app faster.

00:31:38.720 --> 00:31:42.370
So we need a bit of
background here.

00:31:42.370 --> 00:31:45.940
Basically, what concurrent
requests do is they reduce the

00:31:45.940 --> 00:31:49.530
number of instances that
your application needs.

00:31:49.530 --> 00:31:53.980
So an instance is basically a
copy of the Python interpreter

00:31:53.980 --> 00:31:56.720
that's used to handle
your requests.

00:31:56.720 --> 00:32:01.460
So this is from the App
Engine Admin Console.

00:32:01.460 --> 00:32:04.420
I've clicked on the
Instances tab.

00:32:04.420 --> 00:32:08.820
And it's showing me here that
I have seven instances.

00:32:08.820 --> 00:32:12.230
And it's giving me some
data about them.

00:32:12.230 --> 00:32:16.390
So for many applications, the
biggest charge that you're

00:32:16.390 --> 00:32:18.980
going to get for App Engine is
going to be for the instances

00:32:18.980 --> 00:32:20.780
that you're using.

00:32:20.780 --> 00:32:24.550
So keeping your instance usage
low is potentially a big win.

00:32:27.200 --> 00:32:30.320
So this is just kind
of a pro tip.

00:32:30.320 --> 00:32:33.090
So if you're in your App Engine

00:32:33.090 --> 00:32:35.450
dashboard, there's a chart.

00:32:35.450 --> 00:32:37.050
And there's a little
pop-up there.

00:32:37.050 --> 00:32:44.620
And buried in there is an item
called Instances, which shows,

00:32:44.620 --> 00:32:51.320
broken down into 6, 12, 24, et
cetera, time chunks, how many

00:32:51.320 --> 00:32:53.990
instances you're using, how many
you're getting charged

00:32:53.990 --> 00:32:55.610
for and how many there
are total.

00:32:55.610 --> 00:32:59.205
So App Engine will sometimes
keep some instances around and

00:32:59.205 --> 00:33:03.010
not charge you for that, just
to handle extra load.

00:33:03.010 --> 00:33:04.280
But you shouldn't
count on that.

00:33:04.280 --> 00:33:09.450
That's just a thing we do, if
we have spare capacity.

00:33:09.450 --> 00:33:13.660
And so you can see how your
instance charges are

00:33:13.660 --> 00:33:14.910
changing over time.

00:33:17.820 --> 00:33:20.020
OK, so let me explain
the actual feature.

00:33:20.020 --> 00:33:26.420
So this is the way requests are
handled at a 50,000-foot

00:33:26.420 --> 00:33:29.090
view without concurrent
requests.

00:33:29.090 --> 00:33:31.900
So let's say you have an
application and you have one

00:33:31.900 --> 00:33:35.210
instance, so one copy of Python,
that's not currently

00:33:35.210 --> 00:33:36.720
handling a request.

00:33:36.720 --> 00:33:39.780
So a request will come in
for your application.

00:33:39.780 --> 00:33:43.560
And App Engine will go, oh,
I have a free instance.

00:33:43.560 --> 00:33:46.740
And it will forward the request
to the instance.

00:33:46.740 --> 00:33:49.380
That's probably what you
expected would happen.

00:33:49.380 --> 00:33:52.590
So if you get another request
that comes in and that one

00:33:52.590 --> 00:33:54.960
instance, which is the only
instance you have in this

00:33:54.960 --> 00:33:58.820
example, is already busy
handling the first request,

00:33:58.820 --> 00:34:03.020
then App Engine has to create
a second instance, wait til

00:34:03.020 --> 00:34:06.760
that's created, and then forward
that request there.

00:34:06.760 --> 00:34:11.480
So if you have two concurrent
requests, it basically means

00:34:11.480 --> 00:34:13.520
you need two instances
to handle them.

00:34:13.520 --> 00:34:18.139
So looking at the math, very
simply, if you have 10

00:34:18.139 --> 00:34:21.389
requests per second, like every
second you're getting 10

00:34:21.389 --> 00:34:25.530
requests, and each request takes
one second, then you'd

00:34:25.530 --> 00:34:29.530
need 10 instances to handle the
load, if I did the math

00:34:29.530 --> 00:34:31.230
right, which I think I did.

00:34:31.230 --> 00:34:33.659
So with concurrent requests,
it changes a bit.

00:34:33.659 --> 00:34:37.110
So your app receives
a request.

00:34:37.110 --> 00:34:41.620
And the request is forwarded
to the instance.

00:34:41.620 --> 00:34:44.070
And if you get another request,
the request is

00:34:44.070 --> 00:34:47.270
forwarded to the same instance,
which then creates a

00:34:47.270 --> 00:34:51.639
thread and handles the request
in that thread.

00:34:51.639 --> 00:34:55.150
So this means, in this diagram,
we're serving twice

00:34:55.150 --> 00:35:00.250
the traffic with a single--

00:35:00.250 --> 00:35:02.760
I'm sorry, we're serving the
same traffic, but with half

00:35:02.760 --> 00:35:05.300
the instances.

00:35:05.300 --> 00:35:11.320
So the obvious question here
is, will this make my

00:35:11.320 --> 00:35:12.480
application slower?

00:35:12.480 --> 00:35:17.210
Because I have the same CPU
resources for one instance,

00:35:17.210 --> 00:35:18.970
but I'm now sending
two requests for

00:35:18.970 --> 00:35:21.330
it instead of one.

00:35:21.330 --> 00:35:26.740
And the answer, of course, is
yes, this will make the

00:35:26.740 --> 00:35:27.950
request handling slower.

00:35:27.950 --> 00:35:29.990
I'm trying not to look at the
App Engine people who might be

00:35:29.990 --> 00:35:31.170
glaring at me now.

00:35:31.170 --> 00:35:35.830
But the slowdown is probably
much smaller than you expect.

00:35:35.830 --> 00:35:40.890
And for, basically, the rest of
the talk, I'll walk through

00:35:40.890 --> 00:35:44.130
an example where I load test
a real application--

00:35:44.130 --> 00:35:46.290
and by real, I mean my Predator
application--

00:35:46.290 --> 00:35:50.060
and see how it behaves.

00:35:50.060 --> 00:35:56.010
So here's a reminder of,
basically, what the Google+

00:35:56.010 --> 00:35:58.270
variation of the Predator
application does.

00:35:58.270 --> 00:36:01.170
I removed some of the exception
handling, because

00:36:01.170 --> 00:36:03.390
cool people don't do exception
handling, and it makes the

00:36:03.390 --> 00:36:04.440
code too big.

00:36:04.440 --> 00:36:06.800
And I've put comments
that break the code

00:36:06.800 --> 00:36:07.890
out into three stages.

00:36:07.890 --> 00:36:11.230
So first, we load the
Google+ profile.

00:36:11.230 --> 00:36:14.840
And basically, when we're
loading the Google+ profile,

00:36:14.840 --> 00:36:15.850
we're not using the CPU.

00:36:15.850 --> 00:36:19.150
We're just waiting for a
network to get back to

00:36:19.150 --> 00:36:21.270
us with some data.

00:36:21.270 --> 00:36:25.960
Then we load the image,
the profile image.

00:36:25.960 --> 00:36:28.470
And once again, we're just
waiting for IO there.

00:36:28.470 --> 00:36:34.670
And finally, we use a ton of
CPU to decode the JPEG,

00:36:34.670 --> 00:36:38.100
convert to grayscale, do a
palette substitution, and then

00:36:38.100 --> 00:36:42.480
re-compress as PNG.

00:36:42.480 --> 00:36:48.270
So if you look at this over
time, I did timing.

00:36:48.270 --> 00:36:52.670
And of course, these timings
lined up perfectly on 100

00:36:52.670 --> 00:36:54.310
millisecond boundaries.

00:36:54.310 --> 00:36:56.210
It always takes exactly 100

00:36:56.210 --> 00:36:58.530
milliseconds to load the profile.

00:36:58.530 --> 00:37:01.280
It takes 300 milliseconds
to load the image.

00:37:01.280 --> 00:37:04.660
And then it takes 100
milliseconds, exactly, every

00:37:04.660 --> 00:37:06.270
single time, to transform
the image.

00:37:06.270 --> 00:37:07.605
OK, I totally made
these numbers up.

00:37:07.605 --> 00:37:10.010
No, I didn't make them up,
but they're obviously

00:37:10.010 --> 00:37:12.550
approximations, based
on many runs.

00:37:12.550 --> 00:37:15.510
And I've rounded to the nearest
100 milliseconds.

00:37:15.510 --> 00:37:20.640
Now, if we look at this program
flow versus CPU usage,

00:37:20.640 --> 00:37:22.990
we can see we start by basically
using no CPU.

00:37:22.990 --> 00:37:25.190
We're just waiting
for a URL to be

00:37:25.190 --> 00:37:27.700
downloaded from the internet.

00:37:27.700 --> 00:37:30.620
And that continues the same
for when we're loading the

00:37:30.620 --> 00:37:31.330
profile image.

00:37:31.330 --> 00:37:32.880
We're just waiting for
the image to come

00:37:32.880 --> 00:37:34.290
back from the network.

00:37:34.290 --> 00:37:37.970
And finally, we have a big chunk
of CPU usage while we're

00:37:37.970 --> 00:37:39.930
transforming the image.

00:37:39.930 --> 00:37:44.140
So in this particular case, we
can see that we're spending,

00:37:44.140 --> 00:37:47.470
basically, one fifth of our
request time using the CPU.

00:37:47.470 --> 00:37:48.800
And the rest, it's idle.

00:37:48.800 --> 00:37:52.360
So with that concurrent request,
that CPU time is

00:37:52.360 --> 00:37:53.480
basically wasted.

00:37:53.480 --> 00:37:58.310
There's just nothing for the
CPU to do while it's doing

00:37:58.310 --> 00:37:59.560
this downloading.

00:38:01.480 --> 00:38:04.410
So that's the theory.

00:38:04.410 --> 00:38:06.190
Now let's load test it.

00:38:06.190 --> 00:38:09.540
So I don't want to actually do
URL fetching in my load test,

00:38:09.540 --> 00:38:11.660
because the performance
is too variable.

00:38:11.660 --> 00:38:13.700
Downloading stuff from the
internet takes a variable

00:38:13.700 --> 00:38:14.680
amount of time.

00:38:14.680 --> 00:38:17.620
So I've cleverly replaced the
downloading with sleep

00:38:17.620 --> 00:38:18.680
statements.

00:38:18.680 --> 00:38:22.790
So I sleep 100 milliseconds to
simulate loading the Google+

00:38:22.790 --> 00:38:26.050
profile and another 300
milliseconds to simulate

00:38:26.050 --> 00:38:27.330
loading the image.

00:38:27.330 --> 00:38:32.300
And I put a picture of a rock in
the application directory.

00:38:32.300 --> 00:38:35.140
And that's the one
I transform.

00:38:35.140 --> 00:38:36.840
So I upload this application.

00:38:36.840 --> 00:38:39.120
And then I use a tool
called jMeter.

00:38:39.120 --> 00:38:41.412
So has anyone used jMeter?

00:38:41.412 --> 00:38:42.660
Oh, wow, lots of people.

00:38:42.660 --> 00:38:45.840
So jMeter is basically
a load testing tool.

00:38:45.840 --> 00:38:50.820
And basically what it does is
you tell it, create a number

00:38:50.820 --> 00:38:54.240
of fake clients that hit
a particular URL.

00:38:54.240 --> 00:38:56.290
And it simulates hitting
that URL.

00:38:56.290 --> 00:39:00.020
And as soon as it's loading, hit
refresh, wait til it loads

00:39:00.020 --> 00:39:01.540
again and keep hitting
refresh.

00:39:01.540 --> 00:39:03.070
It's basically simulating
a person who

00:39:03.070 --> 00:39:05.390
keeps hitting refresh.

00:39:05.390 --> 00:39:08.980
And in this particular example,
I'm simulating 25

00:39:08.980 --> 00:39:12.880
people hitting refresh
continuously on my Predator

00:39:12.880 --> 00:39:14.010
application.

00:39:14.010 --> 00:39:18.990
So at the top, I've copied out
the data that I got here.

00:39:18.990 --> 00:39:28.020
So in this case, it's done 790
requests, so it's giving it a

00:39:28.020 --> 00:39:31.580
throughput of 1,500 requests
per minute.

00:39:31.580 --> 00:39:34.850
And the average and median
numbers you see on the right

00:39:34.850 --> 00:39:35.630
are latency.

00:39:35.630 --> 00:39:41.980
So the average request is taking
950 milliseconds to be

00:39:41.980 --> 00:39:45.090
responded to.

00:39:45.090 --> 00:39:47.810
So I actually did
this load test.

00:39:47.810 --> 00:39:54.420
And I compiled a table
with my results.

00:39:54.420 --> 00:39:58.050
So if you look at the top row,
this is just the default

00:39:58.050 --> 00:40:00.290
configuration for
App Engine use.

00:40:00.290 --> 00:40:02.820
It's what's called
F1 instances.

00:40:02.820 --> 00:40:06.410
There are 600 megahertz
virtual CPUs.

00:40:06.410 --> 00:40:09.320
And I generated the timing's
for that without concurrent

00:40:09.320 --> 00:40:10.170
requests enabled.

00:40:10.170 --> 00:40:15.090
So the latency that I measured
was, basically, one second, so

00:40:15.090 --> 00:40:17.020
1,015 milliseconds.

00:40:17.020 --> 00:40:20.250
It created 24 instances
to handle this load.

00:40:20.250 --> 00:40:22.330
And I just normalized
the dollar cost.

00:40:22.330 --> 00:40:26.910
To handle this load, let's
say it costs $1.

00:40:26.910 --> 00:40:29.880
Now in the next line what I've
done is I've done the same

00:40:29.880 --> 00:40:33.690
load test, but turning
concurrent requests on.

00:40:33.690 --> 00:40:37.150
So you can see that latency
actually increased by 30%.

00:40:37.150 --> 00:40:38.500
Kind of sucks.

00:40:38.500 --> 00:40:41.170
But the number of instances
required to handle this

00:40:41.170 --> 00:40:45.170
traffic went from 24 to 9,
which is a big reduction.

00:40:45.170 --> 00:40:48.010
And finally, the normalized
dollar cost is $0.38.

00:40:48.010 --> 00:40:54.210
So our latency increased
by 30%, but our cost

00:40:54.210 --> 00:40:56.170
was reduced by 60%.

00:40:56.170 --> 00:40:58.760
So this might be a trade-off
you're willing to make.

00:40:58.760 --> 00:41:02.940
You might be saying, well, to
pay 60% less money, I'm

00:41:02.940 --> 00:41:06.650
willing to make my users
wait 30% longer.

00:41:06.650 --> 00:41:09.180
But if you don't like that
trade-off, you can bump up the

00:41:09.180 --> 00:41:11.410
instance cost from
an F1 to an F2.

00:41:11.410 --> 00:41:16.290
So an F2 is basically twice
as fast as an F1.

00:41:16.290 --> 00:41:18.530
So we get slightly different
latency numbers.

00:41:18.530 --> 00:41:20.700
We're now, instead of
being 30% faster,

00:41:20.700 --> 00:41:22.080
we're a bit less than--

00:41:22.080 --> 00:41:24.970
sorry, it's 30% slower with
concurrent requests on.

00:41:24.970 --> 00:41:28.830
We're about 10% slower,
a bit less than 10%.

00:41:28.830 --> 00:41:30.980
We're using eight instances,
instead of nine.

00:41:30.980 --> 00:41:35.030
And our normalized dollar cost
is 60-something cents.

00:41:35.030 --> 00:41:37.560
So this is also a possible
trade-off that you'd make.

00:41:37.560 --> 00:41:40.040
Latency has only gone up a bit,
but you're still saving

00:41:40.040 --> 00:41:41.760
30% of the cache.

00:41:41.760 --> 00:41:45.530
And then I did a
final increase.

00:41:45.530 --> 00:41:49.470
I jumped from an F2 to an F4
and reran the benchmark.

00:41:49.470 --> 00:41:54.930
So in this particular case,
latency actually fell from one

00:41:54.930 --> 00:41:58.630
second to 0.9 seconds, about.

00:41:58.630 --> 00:42:00.280
Number of instances
went to six.

00:42:00.280 --> 00:42:05.060
And due to a coincidence of
math, the normalized dollar

00:42:05.060 --> 00:42:05.880
cost is the same.

00:42:05.880 --> 00:42:08.640
So you have 1/4 of the
instances, but each of them is

00:42:08.640 --> 00:42:10.760
four times more expensive.

00:42:10.760 --> 00:42:14.270
So you end up with the
same dollar cost.

00:42:14.270 --> 00:42:17.450
So you can see it, concurrent
requests, in this scenario,

00:42:17.450 --> 00:42:19.560
are a win on, basically,
every level.

00:42:19.560 --> 00:42:23.520
If you turn them on and switch
to an F4 instance, you would

00:42:23.520 --> 00:42:26.960
get reduced latency,
so a better user

00:42:26.960 --> 00:42:29.070
experience at the same cost.

00:42:29.070 --> 00:42:33.510
Or you could choose to give the
user a slightly degraded

00:42:33.510 --> 00:42:35.990
user experience from a latency
point of view and save

00:42:35.990 --> 00:42:37.240
yourself some money.

00:42:39.920 --> 00:42:43.560
And I will point out that this
workload is actually pretty

00:42:43.560 --> 00:42:45.380
bad from App Engine's
point of view.

00:42:45.380 --> 00:42:47.770
If you think about what these
requests are doing, they're

00:42:47.770 --> 00:42:50.210
basically idling the CPU.

00:42:50.210 --> 00:42:52.810
And then they're doing
a huge CPU spike.

00:42:52.810 --> 00:42:55.520
So that's a very tough workload
for a scheduler to

00:42:55.520 --> 00:42:57.600
deal with, because it's
like, oh, this

00:42:57.600 --> 00:42:58.810
instance, it's doing nothing.

00:42:58.810 --> 00:43:00.870
I can start packing
on requests.

00:43:00.870 --> 00:43:03.630
And then suddenly, there's a
huge CPU spike that actually

00:43:03.630 --> 00:43:06.270
lasts for quite a long time.

00:43:06.270 --> 00:43:09.900
So there are other workloads
that have a more even mix of

00:43:09.900 --> 00:43:13.290
CPU and IO.

00:43:13.290 --> 00:43:15.210
And by IO, I don't mean
just URL fetch.

00:43:15.210 --> 00:43:16.990
I mean Datastore, Memcache.

00:43:16.990 --> 00:43:26.190
Anything where the CPU is not
used will generate a better

00:43:26.190 --> 00:43:30.060
trade-off than the
one you see here.

00:43:30.060 --> 00:43:33.160
So I've clearly convinced you
that concurrent requests are

00:43:33.160 --> 00:43:35.220
awesome and that you
should use them.

00:43:35.220 --> 00:43:37.200
So how do you actually do it?

00:43:37.200 --> 00:43:39.100
So the first thing you have to
do is make sure that your code

00:43:39.100 --> 00:43:41.630
is actually thread safe.

00:43:41.630 --> 00:43:47.250
And thread safety is this huge
thing that I don't want to

00:43:47.250 --> 00:43:49.150
spend too much time
talking about.

00:43:49.150 --> 00:43:53.940
But basically, the easiest way
to get yourself into trouble

00:43:53.940 --> 00:43:58.350
and make your code not thread
safe is to, basically, change

00:43:58.350 --> 00:44:01.300
global variables without using
some sort of lock.

00:44:01.300 --> 00:44:06.040
So has anyone figured out
what the problem with

00:44:06.040 --> 00:44:09.270
this code is yet?

00:44:09.270 --> 00:44:14.380
So I've cleverly decided to add
a cache of the images I've

00:44:14.380 --> 00:44:15.880
already transformed before.

00:44:15.880 --> 00:44:19.250
So basically, what I'm going to
do is, when a URL comes in,

00:44:19.250 --> 00:44:24.920
I'm going to generate
the Predator image.

00:44:24.920 --> 00:44:26.375
And then I'm going to save
it in this cache.

00:44:29.200 --> 00:44:33.580
So if the URL is already in the
cache, which you can see

00:44:33.580 --> 00:44:37.850
in the second line in the get
handler, then I'm going to

00:44:37.850 --> 00:44:39.460
just output the cached
version.

00:44:39.460 --> 00:44:41.900
Otherwise, I'm going to
compute the image as

00:44:41.900 --> 00:44:43.610
normal in the end.

00:44:43.610 --> 00:44:45.210
I don't want the cache
to grow forever.

00:44:45.210 --> 00:44:47.400
So if there is greater than 10
items in it, I'm just going to

00:44:47.400 --> 00:44:49.970
randomly throw an item away.

00:44:49.970 --> 00:44:54.000
And then I'm going to store
the image I just

00:44:54.000 --> 00:44:55.120
generated in the cache.

00:44:55.120 --> 00:45:00.350
So the disaster here is I can
check to see if the image is

00:45:00.350 --> 00:45:05.320
in the cache while a concurrent
request runs along.

00:45:05.320 --> 00:45:08.040
And it removes that item
from the cache.

00:45:08.040 --> 00:45:09.947
And then, when I get to the next
line where actually I'm

00:45:09.947 --> 00:45:13.880
writing the cached item,
then I blow up, because

00:45:13.880 --> 00:45:15.610
the value's not there.

00:45:15.610 --> 00:45:21.030
So this isn't a comprehensive
examination of thread safety,

00:45:21.030 --> 00:45:23.640
but this is the kind of pattern
that will screw people

00:45:23.640 --> 00:45:28.490
up where they're mutating a
global variable without having

00:45:28.490 --> 00:45:29.900
some sort of lock.

00:45:29.900 --> 00:45:31.930
But I'm sure it'll be easy for
you to fix that, if you have

00:45:31.930 --> 00:45:33.570
problems like this.

00:45:33.570 --> 00:45:37.360
So just do that and then
go to step number two.

00:45:37.360 --> 00:45:40.010
So step number two is
you need to define a

00:45:40.010 --> 00:45:42.560
WSGI application object.

00:45:42.560 --> 00:45:47.920
Probably, existing App Engine
apps will, pretty much,

00:45:47.920 --> 00:45:50.280
already be doing that.

00:45:50.280 --> 00:45:53.750
In all my examples, I've been
defining a webapp2 WSGI

00:45:53.750 --> 00:45:57.510
application, but you can use any
WSGI application you want,

00:45:57.510 --> 00:45:59.360
Django, Bottle, Flask.

00:45:59.360 --> 00:46:04.800
WSGI is the dominant web
application standard for

00:46:04.800 --> 00:46:06.830
Python, so you're going to
have no problems using

00:46:06.830 --> 00:46:08.910
alternatives, if you want.

00:46:08.910 --> 00:46:12.270
Then you basically take your
app.yaml file and you take the

00:46:12.270 --> 00:46:17.690
name of your module, put it in
the script field, put a period

00:46:17.690 --> 00:46:21.390
and then the application
name and you're done.

00:46:21.390 --> 00:46:24.760
Surprisingly, this is not
the easiest step.

00:46:24.760 --> 00:46:26.980
This is the easiest step.

00:46:26.980 --> 00:46:30.390
You just take that thread safe
false I showed you before, set

00:46:30.390 --> 00:46:33.330
it to true, and you have
concurrent requests enabled.

00:46:33.330 --> 00:46:35.900
And you're all done.

00:46:35.900 --> 00:46:40.050
Speaking of all done, I'm all
done, except for just let me

00:46:40.050 --> 00:46:42.580
summarize what we've
talked about.

00:46:42.580 --> 00:46:48.530
So with the Python 2.7 runtime,
you get a much nicer

00:46:48.530 --> 00:46:51.130
third-party library
experience.

00:46:51.130 --> 00:46:53.965
We support a bunch of libraries
that people have

00:46:53.965 --> 00:46:54.980
been really asking for.

00:46:54.980 --> 00:46:58.610
And we support it in a way
that's basically future-proof,

00:46:58.610 --> 00:47:01.900
where we have an upgrade
path for them.

00:47:01.900 --> 00:47:04.050
There's some Python 2.7 features
that are really

00:47:04.050 --> 00:47:06.700
useful for web application
development.

00:47:06.700 --> 00:47:10.250
And there's some other features
that are really

00:47:10.250 --> 00:47:13.430
esoteric, but core Python
developers seem to like them.

00:47:13.430 --> 00:47:15.660
We have fewer limitations
in the runtime.

00:47:15.660 --> 00:47:18.090
And finally, we support
concurrent requests, which

00:47:18.090 --> 00:47:22.690
have the potential to save you
some serious money and/or make

00:47:22.690 --> 00:47:23.970
your apps faster
for your users.

00:47:27.030 --> 00:47:27.670
So thanks.

00:47:27.670 --> 00:47:29.450
Thanks for coming.

00:47:29.450 --> 00:47:35.950
[APPLAUSE]

00:47:35.950 --> 00:47:37.330
BRIAN QUINLAN: Thanks,
for Wikipedia, for

00:47:37.330 --> 00:47:38.690
the dinosaur pictures.

00:47:38.690 --> 00:47:41.130
Thanks to the Python community
for making Python.

00:47:41.130 --> 00:47:43.530
We couldn't really do App
Engine without it.

00:47:43.530 --> 00:47:45.240
And thanks for all you
App Engine people,

00:47:45.240 --> 00:47:46.570
for making cool apps.

00:47:46.570 --> 00:47:48.000
AUDIENCE: Questions?

00:47:48.000 --> 00:47:48.780
BRIAN QUINLAN: Absolutely.

00:47:48.780 --> 00:47:53.128
Could you go up to the mics,
just so this is recorded?

00:47:53.128 --> 00:47:55.980
AUDIENCE: So I have
a question.

00:47:55.980 --> 00:48:01.100
Do you plan or is there any
plan to support additional

00:48:01.100 --> 00:48:04.752
runtimes, namely PyPy?

00:48:04.752 --> 00:48:06.800
BRIAN QUINLAN: That's a great
question that I won't answer.

00:48:10.050 --> 00:48:12.860
We're thinking about new
runtimes all the time, but we

00:48:12.860 --> 00:48:17.590
can't really talk about whether
we're actually going

00:48:17.590 --> 00:48:18.680
to support them or not.

00:48:18.680 --> 00:48:20.210
Sorry about that.

00:48:20.210 --> 00:48:22.420
You can bug our product
management folks-- for

00:48:22.420 --> 00:48:26.020
example, there's one right
there-- and try and beat some

00:48:26.020 --> 00:48:27.601
sort of answer out of
him, if you want.

00:48:27.601 --> 00:48:29.310
AUDIENCE: A second
quick question.

00:48:29.310 --> 00:48:30.420
BRIAN QUINLAN: Yep.

00:48:30.420 --> 00:48:33.150
AUDIENCE: Can you use
duck tests with

00:48:33.150 --> 00:48:34.550
the unit test framework?

00:48:34.550 --> 00:48:37.930
BRIAN QUINLAN: I haven't tried
it, but I can't see why that

00:48:37.930 --> 00:48:38.330
wouldn't work.

00:48:38.330 --> 00:48:39.326
AUDIENCE: OK, thank you.

00:48:39.326 --> 00:48:40.650
BRIAN QUINLAN: No worries.

00:48:40.650 --> 00:48:42.430
Maybe alternate mics?

00:48:42.430 --> 00:48:45.490
AUDIENCE: So you talked about
concurrent requests.

00:48:45.490 --> 00:48:49.270
One of the common requirements
was to reduce the cost by

00:48:49.270 --> 00:48:50.355
having concurrent requests.

00:48:50.355 --> 00:48:50.740
BRIAN QUINLAN: Yes.

00:48:50.740 --> 00:48:52.770
AUDIENCE: The other most
important thing is the

00:48:52.770 --> 00:48:55.430
functionality that comes with
concurrent requests,

00:48:55.430 --> 00:48:57.860
especially with stateful
applications.

00:48:57.860 --> 00:49:01.520
So earlier, you never had to
go to the same server.

00:49:01.520 --> 00:49:02.820
It always goes to a
different server.

00:49:02.820 --> 00:49:03.050
BRIAN QUINLAN: Yeah.

00:49:03.050 --> 00:49:07.500
AUDIENCE: But if you have to
have a server with data

00:49:07.500 --> 00:49:11.720
already in memory and you have
to go back to the same server,

00:49:11.720 --> 00:49:13.290
that's not possible
with App Engine.

00:49:13.290 --> 00:49:15.440
Now, with concurrent requests,
that's possible.

00:49:15.440 --> 00:49:18.720
But then it's not possible
with App Engine.

00:49:18.720 --> 00:49:21.410
BRIAN QUINLAN: So it's not
possible in the model you're

00:49:21.410 --> 00:49:22.670
thinking about.

00:49:22.670 --> 00:49:27.190
So App Engine will send
concurrent requests to one

00:49:27.190 --> 00:49:29.160
instance, but it won't
do that forever.

00:49:29.160 --> 00:49:33.060
So say you have, for example, a
million concurrent requests.

00:49:33.060 --> 00:49:35.740
It's not going to send a million
requests at one Python

00:49:35.740 --> 00:49:36.440
[INAUDIBLE].

00:49:36.440 --> 00:49:38.610
It will start ramping
up new ones as the

00:49:38.610 --> 00:49:41.770
instances get saturated.

00:49:41.770 --> 00:49:45.680
So you won't have shared memory,
potentially, across

00:49:45.680 --> 00:49:47.300
those shared requests.

00:49:47.300 --> 00:49:49.870
Now we have another thing
called Servers.

00:49:49.870 --> 00:49:50.990
I don't know if you've
looked into that.

00:49:50.990 --> 00:49:51.660
AUDIENCE: Backends?

00:49:51.660 --> 00:49:52.360
BRIAN QUINLAN: Sorry?

00:49:52.360 --> 00:49:53.500
AUDIENCE: You mean backends?

00:49:53.500 --> 00:49:54.590
BRIAN QUINLAN: Sorry,
backends.

00:49:54.590 --> 00:49:56.590
Sorry, my mistake, backends.

00:49:56.590 --> 00:50:03.280
And they basically allow you to
have some fixed number of

00:50:03.280 --> 00:50:07.250
backends with, potentially, a
large amount of memory to deal

00:50:07.250 --> 00:50:08.240
with this kind of thing.

00:50:08.240 --> 00:50:09.430
AUDIENCE: OK.

00:50:09.430 --> 00:50:12.000
BRIAN QUINLAN: But actually,
maybe you could come by the

00:50:12.000 --> 00:50:12.960
booth afterwards.

00:50:12.960 --> 00:50:14.030
And we could talk about this.

00:50:14.030 --> 00:50:16.510
I'm curious what your
use case is.

00:50:16.510 --> 00:50:17.480
So could you wait a second?

00:50:17.480 --> 00:50:18.670
[INAUDIBLE] will switch sides.

00:50:18.670 --> 00:50:19.260
AUDIENCE: Hello.

00:50:19.260 --> 00:50:19.930
I have a question.

00:50:19.930 --> 00:50:22.310
BRIAN QUINLAN: Yeah.

00:50:22.310 --> 00:50:24.850
AUDIENCE: In your code you show
that you were using the

00:50:24.850 --> 00:50:28.810
webapp to HTTP request
handler.

00:50:28.810 --> 00:50:29.230
BRIAN QUINLAN: Yeah.

00:50:29.230 --> 00:50:31.600
AUDIENCE: In order to use
concurrent requests, do you

00:50:31.600 --> 00:50:33.020
need to use those handlers?

00:50:33.020 --> 00:50:33.860
BRIAN QUINLAN: Absolutely not.

00:50:33.860 --> 00:50:40.090
You can use any thread safe WSGI
application framework.

00:50:40.090 --> 00:50:45.382
So you can use webapp2, Django,
Bottle, PIL, whatever.

00:50:45.382 --> 00:50:47.490
AUDIENCE: And I have a side
question to that.

00:50:47.490 --> 00:50:48.120
BRIAN QUINLAN: Yeah.

00:50:48.120 --> 00:50:53.890
AUDIENCE: In the webapp2,
you define a get method.

00:50:53.890 --> 00:50:57.145
Do those methods map to
the request type?

00:50:57.145 --> 00:50:57.950
BRIAN QUINLAN: Oh, yeah.

00:50:57.950 --> 00:50:58.330
AUDIENCE: Like as opposed--

00:50:58.330 --> 00:50:59.700
BRIAN QUINLAN: Yeah,
sorry, they do.

00:50:59.700 --> 00:51:02.430
So if you want to handle get
requests, you define a method

00:51:02.430 --> 00:51:02.930
called get.

00:51:02.930 --> 00:51:05.530
If you want to handle post,
you define a method called

00:51:05.530 --> 00:51:07.900
post, push for push, et
cetera, et cetera.

00:51:07.900 --> 00:51:10.285
AUDIENCE: OK, so it's like a
magical wrap kind of thing?

00:51:10.285 --> 00:51:12.860
BRIAN QUINLAN: Yeah, yeah, but
if there's a framework that

00:51:12.860 --> 00:51:14.970
you're more comfortable
with, just use it.

00:51:14.970 --> 00:51:16.220
AUDIENCE: OK.

00:51:18.570 --> 00:51:20.010
BRIAN QUINLAN: Not that
webapp2 is bad

00:51:20.010 --> 00:51:21.841
or anything, but--

00:51:21.841 --> 00:51:26.110
AUDIENCE: So how does the
saturation of an instance, how

00:51:26.110 --> 00:51:28.080
is that determined?

00:51:28.080 --> 00:51:30.230
What I'm specifically wondering
is, if you have

00:51:30.230 --> 00:51:33.270
requests that are really
variable with their latency--

00:51:33.270 --> 00:51:38.070
so in your benchmark example,
they were all uniform latency

00:51:38.070 --> 00:51:39.290
per request--

00:51:39.290 --> 00:51:42.376
but if that's highly variable,
say you have some that are a

00:51:42.376 --> 00:51:44.160
couple of hundred milliseconds,
some that are

00:51:44.160 --> 00:51:48.780
over a second, how is that going
to play into threading?

00:51:48.780 --> 00:51:53.050
BRIAN QUINLAN: So it's hard to
give you an answer that's not

00:51:53.050 --> 00:51:56.620
going to get dated, because
we want to be able to make

00:51:56.620 --> 00:51:58.870
scheduled improvements without
committing ourselves to a

00:51:58.870 --> 00:52:01.100
particular implementation
strategy.

00:52:01.100 --> 00:52:06.400
But right now, we're basically
sampling the CPU usage, back

00:52:06.400 --> 00:52:08.770
by, I think, a couple hundred
milliseconds.

00:52:08.770 --> 00:52:13.260
And we're using that to judge
whether the instance can

00:52:13.260 --> 00:52:16.310
handle more requests.

00:52:16.310 --> 00:52:19.740
So this is why this is
pathological, because it

00:52:19.740 --> 00:52:23.620
doesn't really have a long-term
history of how

00:52:23.620 --> 00:52:25.290
requests have behaved
over time.

00:52:25.290 --> 00:52:26.720
AUDIENCE: Mm-hm, I see.

00:52:26.720 --> 00:52:28.170
BRIAN QUINLAN: Well, was that
sufficiently precise?

00:52:28.170 --> 00:52:28.940
AUDIENCE: Yes.

00:52:28.940 --> 00:52:29.520
Yeah, yeah, yeah.

00:52:29.520 --> 00:52:30.650
I think that makes sense.

00:52:30.650 --> 00:52:32.170
That's what I was
wondering about.

00:52:32.170 --> 00:52:33.420
BRIAN QUINLAN: Yeah.

00:52:36.010 --> 00:52:36.850
Thanks.

00:52:36.850 --> 00:52:40.030
AUDIENCE: You mentioned
the new NDB module.

00:52:40.030 --> 00:52:40.620
BRIAN QUINLAN: Yep.

00:52:40.620 --> 00:52:44.055
AUDIENCE: And you talked about
a new property type there.

00:52:44.055 --> 00:52:44.670
BRIAN QUINLAN: Yeah.

00:52:44.670 --> 00:52:46.600
AUDIENCE: What were the
differences between that and

00:52:46.600 --> 00:52:48.040
the reference property,
in short?

00:52:48.040 --> 00:52:50.830
BRIAN QUINLAN: So a reference
property creates

00:52:50.830 --> 00:52:52.830
two separate entities.

00:52:52.830 --> 00:52:54.590
So it's basically, you have two

00:52:54.590 --> 00:52:57.320
separate entities in Datastore.

00:52:57.320 --> 00:52:59.620
And a reference property
joins them up.

00:52:59.620 --> 00:53:02.760
What a structured property does
is, basically, it puts

00:53:02.760 --> 00:53:07.010
them into the same
logical entity.

00:53:07.010 --> 00:53:13.720
But it's an organizational
thing, not a

00:53:13.720 --> 00:53:16.600
Datastore level thing.

00:53:16.600 --> 00:53:20.200
So it helps you with data
modeling, but it doesn't

00:53:20.200 --> 00:53:23.390
actually change the entity
abstraction.

00:53:23.390 --> 00:53:25.920
I felt I explained that
really poorly.

00:53:25.920 --> 00:53:28.870
Feel free to ask a question that
gets me to clarify what I

00:53:28.870 --> 00:53:29.430
meant there.

00:53:29.430 --> 00:53:30.760
AUDIENCE: No, that's
clear enough.

00:53:30.760 --> 00:53:31.220
Thank you.

00:53:31.220 --> 00:53:32.612
BRIAN QUINLAN: OK, thank you.

00:53:32.612 --> 00:53:36.620
AUDIENCE: So structured
properties and indexing,

00:53:36.620 --> 00:53:40.862
what's the impact on the number
of Datastore writes

00:53:40.862 --> 00:53:42.840
per-- yeah.

00:53:42.840 --> 00:53:45.430
BRIAN QUINLAN: It essentially
doesn't change anything.

00:53:45.430 --> 00:53:50.380
So if you have an indexed
structured property--

00:53:50.380 --> 00:53:52.420
sorry, let me phrase
it in another way.

00:53:52.420 --> 00:53:58.690
Whether a property appears as
a structured property or in

00:53:58.690 --> 00:54:04.280
the top level of a model, it
has the same cost from an

00:54:04.280 --> 00:54:05.240
indexing point of view.

00:54:05.240 --> 00:54:06.210
You have another question.

00:54:06.210 --> 00:54:09.190
AUDIENCE: But is it going to
create an index for each

00:54:09.190 --> 00:54:12.752
property within this
structure?

00:54:12.752 --> 00:54:14.570
BRIAN QUINLAN: If the properties
within the

00:54:14.570 --> 00:54:17.110
structured property are defined
to be indexed, then it

00:54:17.110 --> 00:54:22.580
will create an index for
each of those fields.

00:54:22.580 --> 00:54:25.480
So basically, if you imagine a
structured property, like you

00:54:25.480 --> 00:54:31.210
have a model and you have a
group of code that's logically

00:54:31.210 --> 00:54:34.200
combinable, like an address,
and you want to have a line

00:54:34.200 --> 00:54:38.140
one, line two zip code,
whatever, if you extract that

00:54:38.140 --> 00:54:40.930
out and put it in a structured
property, it doesn't change

00:54:40.930 --> 00:54:44.790
the indexing behavior at all.

00:54:44.790 --> 00:54:45.617
I hope that helped.

00:54:45.617 --> 00:54:46.960
AUDIENCE: Yep.

00:54:46.960 --> 00:54:48.670
BRIAN QUINLAN: Sorry, sorry,
structured property,

00:54:48.670 --> 00:54:49.400
structured property.

00:54:49.400 --> 00:54:50.880
Did I say referenced property?

00:54:50.880 --> 00:54:54.070
AUDIENCE: [INAUDIBLE].

00:54:54.070 --> 00:54:56.570
BRIAN QUINLAN: Well, no, because
now you just have an--

00:54:56.570 --> 00:55:00.250
sorry, the question was, does
referenced property change the

00:55:00.250 --> 00:55:02.030
indexing characteristics?

00:55:02.030 --> 00:55:03.920
And the answer is, with
referenced property, you have

00:55:03.920 --> 00:55:08.190
another Datastore entity
that represents the

00:55:08.190 --> 00:55:09.240
other set of data.

00:55:09.240 --> 00:55:13.540
And that other entity will
still get indexed.

00:55:13.540 --> 00:55:18.523
So no, no, it won't change the
indexing characteristics.

00:55:21.335 --> 00:55:22.880
AUDIENCE: So it might be a while
since I looked at this,

00:55:22.880 --> 00:55:25.820
but is it possible now, with a
third-party library, to have

00:55:25.820 --> 00:55:28.470
one that connects to an external
service using a

00:55:28.470 --> 00:55:30.510
custom TCP protocol?

00:55:30.510 --> 00:55:31.310
BRIAN QUINLAN: No, it's not.

00:55:31.310 --> 00:55:33.160
AUDIENCE: And is it ever
going to be possible?

00:55:33.160 --> 00:55:35.270
BRIAN QUINLAN: Ever going to
be possible is such a hard

00:55:35.270 --> 00:55:36.280
question to answer.

00:55:36.280 --> 00:55:38.360
AUDIENCE: Well, is there any
plans to make that possible?

00:55:38.360 --> 00:55:40.785
BRIAN QUINLAN: Oh, it is on
the road map, so yes.

00:55:40.785 --> 00:55:41.885
AUDIENCE: Thank you.

00:55:41.885 --> 00:55:44.150
BRIAN QUINLAN: Sorry, I didn't
know if it was public or not.

00:55:44.150 --> 00:55:46.750
My mistake.

00:55:46.750 --> 00:55:47.280
Thanks for the question.

00:55:47.280 --> 00:55:49.300
It allowed us to highlight
a new feature

00:55:49.300 --> 00:55:50.640
or an upcoming feature.

00:55:50.640 --> 00:55:51.080
Yeah, sorry.

00:55:51.080 --> 00:55:54.630
AUDIENCE: I understand that we
have to pay for bandwidth when

00:55:54.630 --> 00:55:57.180
we're talking between App
Engine instances.

00:55:57.180 --> 00:56:00.350
Is that going to be free
at some point?

00:56:00.350 --> 00:56:03.040
BRIAN QUINLAN: I have
no information on

00:56:03.040 --> 00:56:04.330
pricing changes there.

00:56:04.330 --> 00:56:07.870
But I suspect not, because
a PM is nodding his head.

00:56:10.970 --> 00:56:13.240
But once again, he is
available for your

00:56:13.240 --> 00:56:16.970
haranguement or other
entertainment, if you want to

00:56:16.970 --> 00:56:18.860
have a go at him.

00:56:18.860 --> 00:56:19.680
Anything else?

00:56:19.680 --> 00:56:21.160
Or should I start giving
away stuff?

00:56:21.160 --> 00:56:22.784
AUDIENCE: Yeah, [INAUDIBLE].

00:56:22.784 --> 00:56:23.660
BRIAN QUINLAN: OK.

00:56:23.660 --> 00:56:25.750
AUDIENCE: I want you to start
giving away stuff, but are you

00:56:25.750 --> 00:56:28.519
still working on startup time
at all, to reduce that a

00:56:28.519 --> 00:56:30.500
little bit?

00:56:30.500 --> 00:56:33.100
BRIAN QUINLAN: I am, personally,
not working on

00:56:33.100 --> 00:56:33.890
startup time.

00:56:33.890 --> 00:56:37.360
I'm trying to think if anyone's
doing anything to

00:56:37.360 --> 00:56:40.206
reduce startup time right now.

00:56:40.206 --> 00:56:42.110
I don't think so, actually.

00:56:42.110 --> 00:56:45.400
But feel free to talk to me,
if you're hitting some case

00:56:45.400 --> 00:56:46.550
where startup time is slow.

00:56:46.550 --> 00:56:47.800
AUDIENCE: OK.

00:56:49.770 --> 00:56:50.720
BRIAN QUINLAN: Cool.

00:56:50.720 --> 00:56:51.696
[APPLAUSE]

00:56:51.696 --> 00:56:52.946
BRIAN QUINLAN: Thanks.

00:56:56.088 --> 00:56:58.040
AUDIENCE: Throw it.

00:56:58.040 --> 00:56:58.625
BRIAN QUINLAN: What?

00:56:58.625 --> 00:56:59.600
You want me to throw them?

00:56:59.600 --> 00:57:00.340
Oh, really?

00:57:00.340 --> 00:57:00.772
[LAUGHTER]

00:57:00.772 --> 00:57:01.420
AUDIENCE: No!

00:57:01.420 --> 00:57:01.636
No!

00:57:01.636 --> 00:57:03.360
BRIAN QUINLAN: No, no, no, no,
I like the throw it into the

00:57:03.360 --> 00:57:06.348
crowd strategy.

00:57:06.348 --> 00:57:08.190
Ah, it seems so unsafe.

00:57:08.190 --> 00:57:09.220
[LAUGHTER]

00:57:09.220 --> 00:57:10.705
AUDIENCE: I want one!

00:57:10.705 --> 00:57:14.170
AUDIENCE: [INAUDIBLE].

00:57:14.170 --> 00:57:17.635
[LAUGHTER]

00:57:17.635 --> 00:57:19.120
AUDIENCE: Give me
one for my boss.

00:57:19.120 --> 00:57:22.090
He would kill me, if I come
home without one.

00:57:22.090 --> 00:57:23.870
BRIAN QUINLAN: Well, if you
have some sort of death

00:57:23.870 --> 00:57:25.120
threat, then--

00:57:28.086 --> 00:57:30.626
could you go stand with that
group and take your chances?

00:57:35.884 --> 00:57:36.362
Oops.

00:57:36.362 --> 00:57:36.840
[LAUGHTER]

00:57:36.840 --> 00:57:38.573
BRIAN QUINLAN: Oh, sorry,
it wasn't for--

00:57:38.573 --> 00:57:39.026
OK.

00:57:39.026 --> 00:57:39.480
AUDIENCE: I think we're
out, Brian.

00:57:39.480 --> 00:57:40.130
BRIAN QUINLAN: We're out.

00:57:40.130 --> 00:57:40.430
Sorry, guys.

00:57:40.430 --> 00:57:41.680
AUDIENCE: [INAUDIBLE].

