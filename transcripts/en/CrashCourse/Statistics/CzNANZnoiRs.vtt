WEBVTT
Kind: captions
Language: en

00:00:03.100 --> 00:00:06.660
Hi, I’m Adriene Hill, and Welcome back to
Crash Course, Statistics.

00:00:06.660 --> 00:00:11.130
Today we’re going to step back from sampling
and regressions to talk about the impact of

00:00:11.130 --> 00:00:12.980
all that statistical gathering.

00:00:12.980 --> 00:00:16.830
We’ve seen that the interpretation of this
information can have real lasting effects

00:00:16.830 --> 00:00:21.100
on our society, but its collection can also
have lasting effects on the subjects.

00:00:21.100 --> 00:00:26.640
The process of gathering and applying statistics can affect real people’s lives which means

00:00:26.650 --> 00:00:30.340
there’s an responsibility to gather and
use this data ethically.

00:00:30.340 --> 00:00:32.140
Today we’ll discuss 5 stories.

00:00:32.140 --> 00:00:36.960
Four of them are real and all of them can
help us learn where collecting data can go

00:00:36.960 --> 00:00:40.600
wrong, and how we can help prevent these things
from happening again.

00:00:40.600 --> 00:00:49.680
INTRO

00:00:49.680 --> 00:00:55.380
Our first story begins in 1822 when a young
fur-trapper named Alexis St. Martin got shot

00:00:55.380 --> 00:00:59.100
in the stomach when another trapper’s gun
accidentally went off.

00:00:59.100 --> 00:01:04.380
The wound was serious but a local army Doctor,
William Beaumont was able to stabilize St.

00:01:04.380 --> 00:01:09.720
Martin through a series of presumably painful
anesthetic free surgeries.

00:01:09.720 --> 00:01:15.030
But Dr. Beaumont couldn’t close the wound,
which left a small hole--called a gastric

00:01:15.030 --> 00:01:17.630
fistula--that allowed access to the stomach.

00:01:17.630 --> 00:01:21.850
St. Martin was out of a job since it’s hard
to be an active fur trapper with a hole in

00:01:21.850 --> 00:01:22.850
your stomach.

00:01:22.850 --> 00:01:26.590
So, he signed a contract to become a servant
to Dr. Beaumont.

00:01:26.590 --> 00:01:30.909
In addition to traditional chores, St. Martin
participated in all sorts of experiments at

00:01:30.909 --> 00:01:32.020
the whim of the doctor.

00:01:32.020 --> 00:01:36.620
Beaumont used the gastric fistula to study
how the body digested food.

00:01:36.620 --> 00:01:41.530
He made huge strides in the field, including
exploring the influence of mental disturbance

00:01:41.530 --> 00:01:46.600
on the process of digestion and correcting
the long held belief that the stomach digested

00:01:46.600 --> 00:01:48.640
food by grinding it up.

00:01:48.640 --> 00:01:54.390
When, in 1838, the two finally parted ways,
Beaumont spent the last 15 years of his life

00:01:54.390 --> 00:01:56.789
pleading with St. Martin to come back.

00:01:56.789 --> 00:01:59.460
Unsurprisingly, St. Martin declined.

00:01:59.460 --> 00:02:05.420
Without this strange situation, the field
of gastroenterology may have progressed more slowly. 

00:02:05.420 --> 00:02:12.000
In fact St. Martin’s fistula was an inspiration
to Pavlov who used fistulas in dogs during

00:02:12.000 --> 00:02:15.160
his famous classical condition experiments.

00:02:15.160 --> 00:02:19.570
But all this progress came at a cost--to St.
Martin but also to those dogs.

00:02:19.570 --> 00:02:24.570
One of the most important ethical considerations
in research is whether humans who participate

00:02:24.570 --> 00:02:27.110
are able to feasibly say “no”.

00:02:27.110 --> 00:02:31.920
People with little power, resources, or money
can be coerced into participating in experiments

00:02:31.920 --> 00:02:33.700
that they’re uncomfortable with.

00:02:33.700 --> 00:02:38.720
Most research institutions have a committee,
called the Institutional Review Board-- IRB--which

00:02:38.720 --> 00:02:42.740
oversees all the research at that institution
to make sure that it’s ethical.

00:02:42.740 --> 00:02:46.480
“Voluntariness” is one of the most important
things that they check for.

00:02:46.480 --> 00:02:52.000
This prohibits people with undue power or
influence over us from asking that we participate

00:02:52.000 --> 00:02:53.180
in a research study.

00:02:53.180 --> 00:02:57.150
For example your boss or professor is limited
in how they can ask you to participate in

00:02:57.150 --> 00:03:01.430
a research study because you might feel that
you have no choice--you have to participate

00:03:01.430 --> 00:03:05.280
otherwise this they might fire you or give
you a failing grade.

00:03:05.280 --> 00:03:08.820
Ethical research needs to be voluntary--at
least in humans.

00:03:08.820 --> 00:03:14.000
Animal rights activists argue that since animals
cannot volunteer for a study, we shouldn’t

00:03:14.000 --> 00:03:14.520
use them.

00:03:14.520 --> 00:03:18.380
In addition to their voluntary participation,
subjects should also know what will happen

00:03:18.380 --> 00:03:19.880
to them during the study.

00:03:19.890 --> 00:03:26.980
This was not the case in 1932 when the Tuskegee
Institute began a 40 year long study on over

00:03:26.980 --> 00:03:28.200
600 black men.

00:03:28.200 --> 00:03:32.060
Under the guise of free medical care, the
men were secretly enrolled in a study to observe

00:03:32.060 --> 00:03:35.230
the long term progression of Syphilis.

00:03:35.230 --> 00:03:39.930
Over 300 of the men enrolled had the disease,
but Researchers failed to treat them with

00:03:39.930 --> 00:03:46.470
anything but fake or innocuous medicines like
aspirin even after it became clear that penicillin

00:03:46.470 --> 00:03:49.280
was a highly effective treatment for the disease.

00:03:49.280 --> 00:03:53.440
Late stage symptoms of Syphilis include serious
neurological and cardiovascular problems,

00:03:53.440 --> 00:03:56.260
yet The Institute allowed the study to go
on.

00:03:56.260 --> 00:03:59.360
Some wives and kids also contracted syphilis.

00:03:59.360 --> 00:04:04.680
In 1972 public outrage caused the study to
close down when news of the unethical conditions

00:04:04.680 --> 00:04:06.110
was leaked to the media.

00:04:06.110 --> 00:04:11.440
In 1951, at the same time the Tuskegee study
was running a poor tobacco farmer named Henrietta

00:04:11.440 --> 00:04:16.949
Lacks went to Johns Hopkins Hospital in Maryland
and had cells from a tumor collected without

00:04:16.949 --> 00:04:18.579
her knowledge or consent.

00:04:18.579 --> 00:04:23.710
These cells were used to grow a new cell line--called
the HeLa line--which scientists use to do

00:04:23.710 --> 00:04:25.150
in vitro experiments.

00:04:25.150 --> 00:04:30.070
The cells’ ability to thrive and multiply
outside her body made the cell line useful

00:04:30.070 --> 00:04:31.280
to researchers.

00:04:31.280 --> 00:04:35.250
It is still used for medical research today,
lending itself to cancer and AIDS research

00:04:35.250 --> 00:04:40.070
as well as immunology studies like the one
that led Jonas Salk to discover the Polio

00:04:40.070 --> 00:04:41.100
Vaccine.

00:04:41.100 --> 00:04:45.870
And in 1955 HeLa cells were the first human
cells to be successfully cloned.

00:04:45.870 --> 00:04:51.880
Overtime, the cell line and discoveries it
facilitated became extremely lucrative for researchers.

00:04:51.900 --> 00:04:55.860
But Lacks and her family...didn’t receive
any financial benefit.

00:04:55.860 --> 00:04:59.040
These studies emphasize the need for informed
consent.

00:04:59.040 --> 00:05:02.910
Subjects have the right to not only receive
all the facts relevant to their decision to

00:05:02.910 --> 00:05:05.970
participate, they have the right to understand
them.

00:05:05.970 --> 00:05:10.180
Many institutions require that information
must be presented clearly and in a way that’s

00:05:10.180 --> 00:05:12.820
appropriate for the subjects’ comprehension
level.

00:05:12.820 --> 00:05:18.140
Even children--whose parents are legally allowed
to consent for them, must get an age-appropriate

00:05:18.150 --> 00:05:20.840
explanation of what will happen in the study.

00:05:20.840 --> 00:05:25.650
This is incredibly important because it respects
the dignity and autonomy of the subject, allowing

00:05:25.650 --> 00:05:28.710
them to stop research procedures at any time.

00:05:28.710 --> 00:05:33.570
That incentivizes researchers to design studies
with more acceptable levels of risk.

00:05:33.570 --> 00:05:36.990
In all three of those stories the research
procedures did not have any benefit to the

00:05:36.990 --> 00:05:38.070
patients.

00:05:38.070 --> 00:05:43.220
In 1947, the Nuremberg code was created in
order to establish guidelines for the ethical

00:05:43.220 --> 00:05:44.800
treatment of human subjects.

00:05:44.800 --> 00:05:49.080
One of the main tenets is beneficence, which
not only requires that researchers minimize

00:05:49.080 --> 00:05:54.710
the potential for risk to subjects, but also
requires that the risk should be outweighed

00:05:54.710 --> 00:05:58.140
by potential benefits to the patient and the
scientific community.

00:05:58.140 --> 00:06:03.350
The Nuremberg code was created and implemented
after the second World War, during which horrifying

00:06:03.350 --> 00:06:07.410
experiments were conducted on prisoners in
Nazi concentration camps.

00:06:07.410 --> 00:06:13.070
The Nuremberg Code lays out ten principles
to which modern day studies still must adhere.

00:06:13.070 --> 00:06:18.010
These 10 principles stand as the basis for
much of current research ethics and include

00:06:18.010 --> 00:06:21.860
things like voluntariness, informed consent,
and beneficence.

00:06:21.860 --> 00:06:27.050
But as we settle into the age of technology,
the application of these ethical principles

00:06:27.050 --> 00:06:29.080
can become more cloudy.

00:06:29.080 --> 00:06:33.440
Our last story here isn’t real but it illustrates
the complexities of research ethics in the

00:06:33.440 --> 00:06:34.440
digital age.

00:06:34.440 --> 00:06:38.300
In the seventh season of the hit show Parks
and Recreation, a giant internet corporation

00:06:38.300 --> 00:06:44.260
comes to the small town of Pawnee, Indiana,
to offer free Wifi to the entire city.

00:06:44.260 --> 00:06:47.100
Everyone gladly accepts, they like the free
service.

00:06:47.100 --> 00:06:53.320
But when boxes of personalized gifts arrive
at every citizen’s doorstep, some become--understandably--concerned.

00:06:53.320 --> 00:06:58.300
Because, the gifts are perfect, fitting the
exact interests of the recipient.

00:06:58.300 --> 00:07:03.090
Someone who collects stuffed pigs dressed
as celebrities get “Hamuel L. Jackson”

00:07:03.090 --> 00:07:07.030
and someone obsessed with politics get the
newest Joe Biden poetry collection.

00:07:07.030 --> 00:07:11.260
These boxes are perfect for the people who
received them--eerily perfect.

00:07:11.260 --> 00:07:14.700
So how did the internet company know what
each person would want?

00:07:14.700 --> 00:07:18.910
It turns out that the free WiFi came with
a pretty high cost, privacy.

00:07:18.910 --> 00:07:23.800
In exchange for the free WiFi the internet
company, Gryzzl, was collecting all data that

00:07:23.800 --> 00:07:27.250
was transferred over their Network, this gets
called Data Mining.

00:07:27.250 --> 00:07:30.630
And it may seem far-fetched, but it’s happening
right now.

00:07:30.630 --> 00:07:31.639
Not the gift stuff.

00:07:31.639 --> 00:07:32.639
The data mining.

00:07:32.639 --> 00:07:35.250
Grocery stores track what we buy with our
rewards cards.

00:07:35.250 --> 00:07:37.280
Netflix keeps track of everything we watch.

00:07:37.280 --> 00:07:40.720
Amazon knows exactly what we buy - what we
look at.

00:07:40.720 --> 00:07:44.980
And those Terms of Service Agreements we click
on without reading them when we download an

00:07:44.980 --> 00:07:50.710
app or sign up for a social media account
they often include some kind of stipulation.

00:07:50.710 --> 00:07:55.949
When we use “free” internet services we’re
agreeing to pay, not with money, but usually

00:07:55.949 --> 00:07:56.949
our information.

00:07:56.949 --> 00:08:01.680
Facebook and Google offer there services for
free in part because they’re profiting off

00:08:01.680 --> 00:08:02.770
of our data.

00:08:02.770 --> 00:08:04.540
They might be using it for research.

00:08:04.540 --> 00:08:09.800
Or to customize our experience on their site
so that we buy or watch more stuff on Amazon

00:08:09.800 --> 00:08:11.070
and Youtube.

00:08:11.070 --> 00:08:15.870
They also use it to sell targeted ads, giving
advertisers the opportunity select exactly

00:08:15.870 --> 00:08:18.449
the type of people who will see their ads.

00:08:18.449 --> 00:08:22.180
And sometimes the way these ads are targeted
can be pretty unethical.

00:08:22.180 --> 00:08:27.699
For example, companies discriminating based
on age by specifying that job ads should be

00:08:27.699 --> 00:08:29.290
shown only to young people.

00:08:29.290 --> 00:08:32.510
Data is being used in ways that affect every
facet of your life.

00:08:32.510 --> 00:08:37.099
But, since we’re still in the beginning
stages of this huge influx of digital information,

00:08:37.099 --> 00:08:41.469
we get to see the progression of ethics in
this area unfold in front of us.

00:08:41.469 --> 00:08:46.899
The laws that will protect your data and privacy,
like the Nuremberg Code protects participants

00:08:46.899 --> 00:08:52.769
in scientific experiments, are still being
written, and many of the same concepts are coming up.

00:08:52.769 --> 00:08:57.259
For example, using the internet, Google, and
social media have become so entrenched in

00:08:57.259 --> 00:09:01.089
some societies that it’s almost impossible
to hold a job without them.

00:09:01.089 --> 00:09:05.709
If that’s the case, we need to ask whether
it’s ethical to require that users sign

00:09:05.709 --> 00:09:10.850
over their right to privacy in order to use
them, or like in most clinical studies, would

00:09:10.850 --> 00:09:12.199
that border on coercion?

00:09:12.199 --> 00:09:16.260
We also need to ask whether companies that
use or sell our information be held to the

00:09:16.260 --> 00:09:20.769
standard of “informed consent” which requires
agreements to be in language that’s simple

00:09:20.769 --> 00:09:26.160
enough for the user to understand what they’re
agreeing to--even without a law degree.

00:09:26.160 --> 00:09:30.790
Or, on the other hand whether they should
be exempt from this requirement if they only

00:09:30.790 --> 00:09:32.300
use the data internally.

00:09:32.300 --> 00:09:36.790
It possible to draw parallels between data
mining and the stories we talked about at

00:09:36.790 --> 00:09:41.059
the beginning of this episode - though admittedly
not quite as harrowing.

00:09:41.059 --> 00:09:44.930
Like Alexis St. Martin may have felt pressure
to stay with Dr. Beaumont because he couldn’t

00:09:44.930 --> 00:09:49.820
work as a fur-trapper anymore, it can be argued--
to a much lesser degree -- that we use sites

00:09:49.820 --> 00:09:54.100
like Google and Twitter because we feel that
there’s no other option as we try to remain

00:09:54.100 --> 00:09:56.449
informed in our hyperconnected world.

00:09:56.449 --> 00:10:01.240
And we might not be getting all the information
we need to consent in an understandable way,

00:10:01.240 --> 00:10:06.269
similarly to how Henrietta Lacks was not informed
why her cells were being taken or what they’d

00:10:06.269 --> 00:10:07.140
be used for.

00:10:07.140 --> 00:10:13.340
These situations are obviously not exactly
the same, and we, as a society, need to decide

00:10:13.340 --> 00:10:17.880
how to apply the principles of research ethics
in these new digital spaces.

00:10:17.880 --> 00:10:23.260
As we move forward and gain the ability to
do things like sequence an entire genome in

00:10:23.260 --> 00:10:28.309
days, rather than years, we open the door
for amazing advances in personalized medicine

00:10:28.309 --> 00:10:30.769
that could save millions of lives.

00:10:30.769 --> 00:10:34.060
But we also open the door for abuse of this
sensitive information.

00:10:34.060 --> 00:10:38.800
The conversation about how to handle these
types of situations is still going on.

00:10:38.800 --> 00:10:41.320
We’re are the ones who will decide what
is said.

00:10:41.321 --> 00:10:44.160
And we’re going to be the subject of those
decisions. 

00:10:44.160 --> 00:10:46.180
Thanks  for watching. I'll see you next time. 

