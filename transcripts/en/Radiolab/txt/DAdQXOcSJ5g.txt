Speaker 1:          00:02          Oh wait, you're listening.

Speaker 2:          00:08          You are listening to radio lab radio from W N y. S. T e. All right. Hello? Hello? Hello? Hello? Hello? Hello. I can hear you. They can't hear me? No, we can hear you. We can hear you. We can also hear it twice. Yeah. Hey, I'm Jen. I boom. Rod. I'm Robert Krulwich, Radiolab, and today, oops. Oops. You don't, you don't hear. You don't hear us. We have a story about how, hmm. The echoes of you can go out into the world and come back and bite you in. All of us. Really in the bud. Hmm. Uh, oh wait, wait. Maybe we're fine now. Is My echo gone? Yes. Okay. Okay. We're good. We're good. And it comes to us from our producers. Simon Adler. Yeah. Nick. Hello. Hello. Sorry. Okay, so this is Nick Bilton. My name is Nick Bilton. I'm a special correspondent for Vanity Fair and is beat, you could say is trying to predict the future of technology to look into the future.

Speaker 2:          01:11          I'm into this kind of crystal ball and try to predict what the next five, 10, 15 years would look like for the media industry. Do you have a good batting record? Like did you, did you call some big well yeah, you know, phones in our pockets that would be like super computers that social media would drive news, not newspapers and so on and things like that. So it's, it's, it's been pretty good. I reached out to you because I came across this article that you wrote an article that, uh, sent shivers down my spine and I'm not one to typically be given shivers by articles. So I guess how did you stumble into all of this and where did, where does this start for you? So, uh, I was sitting around with some friends in my living room and friend of mine mentioned, Oh, did you see this thing that Adobe put out recently?

Speaker 3:          02:00          We live in a time when more people than ever before believe that they can change the world.

Speaker 2:          02:05          Conversation led nick to a video, a video online of the Adobe Max 2016.

Speaker 3:          02:13          There are tons and tons of people in the audience. Yeah,

Speaker 2:          02:16          he's amazing. And, and up in front of them, it looks like the stage of a apple product launch, but sort of beach themed. Why Beach? I have absolutely no idea. That's a little bit to your mind. I got, Hey, there are two hosts that are sitting in these like lifeguard chairs, comedian Jordan Peele, Jordan Peele as in key and Peele, Jordan Peele. Yes. And then the other host is this woman Kim Chambers who is a marathon swimmer and uh, an Adobe employee. And then please welcome to the stage [inaudible] Jay.

Speaker 3:          02:49          Hello everyone. Young guy glasses. You guys have been making weird stuff online. Great photo ID chain

Speaker 2:          02:59          and he says Adobe is known for Photoshop. We're known for editing a photos and

Speaker 3:          03:05          doing magical things visually. Well, we all do the next thing today. Let's do something to humans. Speech pulls up the screen on a MAC computer. Wow. I have obtained today's piece of audio where, um, there's Meco key talking to peel about his feeling after getting nominated.

Speaker 2:          03:24          Keegan Michael Key had been nominated for an Emmy and a, he and Jordan Peele were talking about it.

Speaker 3:          03:30          Ah, there's a pretty interesting, uh, joke here. So let's, uh, let's just get yet, uh, I jumped on the bed and um, and uh, uh, I kissed my dogs and my wife in that order. Not a bad joke. So that's a, that's do something here. Okay. So suppose I'm Michael K one, 2000. There's a audio to his life.

Speaker 2:          03:51          In other words, what if Keegan Michael Key with feeling like that was a little bit rough on my wife? That was a little bit mean, uh, you know, maybe, uh, he wanted to go and rewrite history and say that he kissed his wife before the dog.

Speaker 3:          04:05          Okay. Actually want his wife to go before the dogs. So, okay. So what do we do

Speaker 2:          04:12          easily? So Xavier Clicks a button in the program, automatically generates a transcript of the audio and projects it up on the screen behind him. You know, just text of what Keegan Michael Key said. Okay, let me zooming a little bit and then copy paste. He just highlights the word wife and pastes it over in front of dogs.

Speaker 3:          04:30          Okay, let's listen to it. Clicks play. And uh, uh, I kissed my wife and my dogs. Woo.

Speaker 4:          04:37          [inaudible]

Speaker 5:          04:37          oh, so he was able to move the, edit the audio by moving the text around in a text box. Yes, exactly. Okay. Well that's kind of cool. Kind of impressive.

Speaker 3:          04:46          Wait, but then here's more, here's more. Uh, we can actually type something that's not here. So, wait, wait, what? Just hang on just again. I have heard that, um, actually that on that day, uh, Michael actually the hour Jordan, so sorry to recover the truth. Let's go with you. Goes back into that little word box though. Let's remove the award of my here, your secrets out to type the word Jordan.

Speaker 2:          05:18          So he typed it out. J, o. R. D. A. N. N. Just to be clear, Keegan Michael Key did not see Jordan anywhere in this clip.

Speaker 3:          05:24          And here we go. And, uh, I kissed Jordan and my dogs.

Speaker 5:          05:31          Wait, he just typed in a word that the guy never said and it made the guy say the word that he never said as if he actually said it

Speaker 3:          05:39          exactly. Well you will, which jumps out of this lifeguard. Yeah.

Speaker 2:          05:44          Chair started sort of stomping around the stage.

Speaker 3:          05:47          You were, you were demon. Oh yeah. I have a magic and to the lot magic, I cannot show you guys as we calculate types, small phrases. So that's

Speaker 2:          05:59          okay. So what were your mood? He deletes the words. My dogs. Any types? Three, three times. Oh Man. Tea Bag and a, I kissed Jordan three times.

Speaker 6:          06:15          [inaudible]

Speaker 2:          06:15          all right. Wait, you are saying that Keegan Michael Key never said, ever said Jordan. Never said three. Never said time's never, never said any of those words. And somehow just from the typing in of it, the guy is now saying them and we're hearing them in his voice. That's what just happened. Yup. That is exactly what the demo claims. It's essentially Photoshop for audio. Nick built. And again, you can take, uh, uh, as little as 20 minutes of someone's voice type the words. And it creates in that voice, um, that sentence with, with just 20 minutes of the guy talking. Yes. What, how, how in heaven do you do this? Okay. Well, and so we're, we're here to Adobe. Um, what, what exactly do you do here? Sure. I'm the product manager for audio. This is during Gleaves. Um, I flew out to Seattle and tracked him down to ask him exactly that question.

Speaker 2:          07:08          So essentially what it does is it does an analysis of the speech and it creates a models and it, it basically, uh, and he explained to me that this program, which they call it Voco by the way, um, what it does is it takes 20 minutes or actually 40 if you want the best results, uh, of you talking. And it figures out all of the phonetics of your speech. All of the sounds that you make finds each little block of sound and speech that is in the recordings, chops them all up. And then when you go and type things in, it will recombine those, um, into that new word. But what if it encounters a sound that I've never made? Well a, the theory is in 40 minutes of speech, which is the amount they recommend you feed in, you're going to probably say just about every sound in the English language.

Speaker 2:          07:50          So if really, so like phonetically I go, I run through the gamut and in 40 minutes, yes. Wow. Well, and like what would you, or what are you hoping people will, would use a product like Foco for a, so for the video production tools and for what audition is used for a lot as dialogue editing, the whole idea during said is to help people that work in movies and TV. A lot of our customers record great audio on set the actors and the dialogues and everything. Uh, and when they come back, if sometimes there's a mistake or they make a change, like the actor on set said shoe. Uh, but what he was pointing at was obviously a boot. And right now there's, they do what's called ADR. They'll bring the actor in, they'll rerecord some lines and they'll try and drop that into the video.

Speaker 2:          08:40          But there's a lot of, you're not using the same microphones, you're not in the same location. The actor might be sick that day. So up his voice sounds different and things, a lot of times you can really hear that stand out in productions if they don't get it just right or with Voco you just delete the word shoe type in boot and boom, there it is using the same source media and the same characteristics and have it just sound seamless and natural. And so it, it, it's going to be a sort of, the hope is that it will make the lives of professional post-production editors easier the world over. That's our hope right now. Yeah, but that's not exactly, well, it's, I mean it's, um, what Nick Bilton thought when, um, when he saw this video, it could be Donald Trump's voice or Vladimir Putin. Um, so I saw that and I thought, wow, if imagine if audio clips start getting shared around the Internet, um, as fake news about fake conversation between, you know, Vladimir Putin and Paul Manafort about trying to get Trump into the white house or something like that. Right now. I was like, Whoa, this is, this is scary stuff, but we're just getting started. In the words of John Raymond Arnold played by Samuel L. Jackson in the movie Jurassic Park in his own voice. Hold on to you, but things are about to get a lot crazy

Speaker 6:          10:05          [inaudible]

Speaker 2:          10:10          so forget voices for a second because now one, two, three, four, five, one, two, three, four, five. It's facetime. All right. We are at the Paul G. Allen Center at the University of Washington in Seattle, so I left Adobe and went across town to talk to the head of the grail lab hero. Yeah. Very nice to meet you. Dr Eric [inaudible]. I'm a professor in the computer science department at the University of Washington and Elsa work at Facebook. Can I just have you come a little closer? Okay. Just to back up for a second, when nick first saw the VOCO demonstration, he started to wonder, okay, like how could this be used down the road? And my original thesis was, oh, well, maybe what'll happen is that you will be able to create three d actors just like you didn't star wars. Then join it with the voco stuff to create a fake Hillary Clinton and you know, Donald Trump having a conversation or making out or whatever it is you want to do.

Speaker 2:          11:05          And that led him to investigate the type of work that era does. Should I've been using these terms like facial reenactment and facial manipulation? Are those the, are those the right words and then what the hell do these words mean? Yeah, so, um, I mean it's all, it's all a way of animating faces and it started from the movies, right? The concept is to drive these remotely controlled bodies called avatars. I think like the aptly named movie Avatar or a Sargent. Yes. Going a little further back. No sign of intelligent life anywhere, toy story, and to make the characters come alive. What you need is the expressions of the actors playing them. This is the movie space. It means that you will bring the person to a studio. Then you covered their face with these sticky sensory marker things and then they will spend hours, hours, hours capturing the person's little dynamics like smiled up in miles teeth, claws, mas, no teeth had surprised disturbance like that.

Speaker 2:          12:02          Angry, bloated. Yeah, frustrated. And from that they create a virtual character capable of emoting all those expressions. And to make that character believable, the animators sometimes have to model a bone structure and muscles. And as you can imagine, this can get very, very expensive. And so what people like eras started to wonder was like, can this be done on a budget? So she and others in the field started feeding videos of faces into computers and trained those computers to break down the face into a series of points. Our model started about a two 50 by two 50 that is 62,500 points on one human face. And the once we know that right, we can track the points. Okay. So once you can track how my face moves through a video clip by these 250 by 250 data points, what can you then do with that information?

Speaker 2:          12:54          Well, I can apply the points on the face on a different model of a different person. Now this is, this is where things get a quite strange, uh, because instead of being able to map all of your facial movements onto a computer generated a virtual character or person, uh, with era and others in this field of facial reenactment have figured out how to do is to map your facial movements onto a real person, a, a prerecorded real person. What, what does that even mean? What, how does that work? The best example of this is a, is this piece of software that nick showed us. Uh, this software that I found from these university students called face to face sweet present, a novel realtime facial reenactment method that works with any commodity Webcam. There's a video demo of this and when you open it up, uh, this very monotone voice comes in saying, since our method only uses RGB data for both the source and target actor, and you're like, what the heck is this?

Speaker 2:          13:53          And the screen pops up here, we demonstrate our method in a live setup. On the right, you've got this heavyset man, a goatee, spiked hair on the right, a source actor is captured with the standard web, is arching his eyebrows, he's pursing his lips, he's opening his mouth widely. The sorta like a, like if you're making funny faces for a two year old kind of thing. Yeah. And then this input drives the animation of the face and the video shown on the Monitor to the left. On the left you've got this Dell computer screen displaying a CNN clip of George Bush. This is a real clip of Bush back from 2013 and his face is, they're looking right at the camera, occupies most of that screen. Significant difference to previous methods. What you start to notice is when the man with the goatee smiles, George Bush in the CNN clip also smiles. And when the man raises his eyebrows, George Bush raises his eyebrows and you realize this man is controlling George Bush's face. Wait, so this is a guy in, in the present controlling a past George Bush. A real George Bush from an old video clip.

Speaker 7:          14:56          Yeah. Okay. I pulled up a video for you here. Okay, cool. And a little while back when we were just learning about this, we happen to have our friend Andrew Morantz who writes for the New Yorker in the studio.

Speaker 2:          15:07          So that is George Bushes face. [inaudible] what? Oh God. Oh God. That's terrifying. His, okay. So yeah, I cannot stop watching George Bush's face. Oh, they're doing with Putin now. Holy God. So I just have a guy just sort of going [inaudible] and then that's what Putin is doing. Yeah. Oh, now it's Trump. You know, I mean those videos online, how my mouth agape again, Nick Bilton, this is this, this is a form of puppetry where your face is the, is the puppeteer than the only thing is, is, is that George W. Bush as the puppet. So I sit in front of a camera, I smile and the business is taken care of. That's real time. This isn't like you have to render some software on your computer. It's literally you download a clip or you take a clip from the cable news and you turn on your Webcam and however long it takes you to do it and you're done. It's the same as to shooting a video on your phone. What is this for? What? So what, what are the applications,

Speaker 8:          16:13          Jason? Um, I want to be able to help cal develop telepresence. This is era again. So I love telepresence. Yeah. So for example, so my mom leaves in these rail, um, and I'm here and, um, what is it be cool if I could, um, have some, it's kind of crazy, but right. But if I could have some kind of Hologram of her sitting on my couch here and we can have a conversation

Speaker 7:          16:44          and going one step further. One of your colleagues, a guy by the name of Steve Sights, I'm a professor at the University of Washington and I also work part time at Google. He told me that they see this technology as like a building block that could one day be used to essentially virtually bring someone back from the dead. I just think this technology combined with the virtual reality and other innovations could help me, you know, just be there in the room with Albert Einstein or Carl Sagan. You know, that that's sort of the motivation. That's what they want to do. That's the motivation to ghosts. Well, for them. Yes. And uh, when I was talking to some folks who work in commercials, they're developing their own version of this and the idea is that they're going to make a million or $1 billion off of this because a, say you bring, I dunno, uh, Jennifer Anniston in to film some makeup commercial and in the makeup commercial in English.

Speaker 7:          17:39          She says, so come and buy this product. This is the best sort of whatever product around right now. You've got China, which is a booming market. You maybe want to market things to China and really like to be able to use Jennifer Anniston. Problem is Jennifer Annison doesn't speak Mandarin. So either you use the same audio clip and you have someone come in and speak a mandarin over her and the lips don't line up. Or You have to hire a Mandarin speaking, uh, actor to come in and do the part of Jennifer Anniston with this technology. All you have to do is record Jennifer Aniston. Once you can hire a mandarin speaker and the mandarin speakers voice will be coming out of Jennifer Aniston's mouth is if she had said it and in front of the camera, her lips would be moving as if she were a perfect mandarin speaker. Exactly. Exactly. Wow. I think that part of it is actually, that's, that's amazing. Yeah. Oh my God. I'm amazed and completely frightened by what you're telling me. And that's the whole point of what nick was writing about that, uh, that gave me shivers that someday if you join the video manipulation with the Voco voice manipulation, you, you're, you're the, the ultimate puppeteer. You can create

Speaker 9:          18:55          anyone talking about anything that you want in their own voice and having any kind of emotion around it and you'd have it right there for everyone to see in video. And all you need to do is take that and put it on Twitter or Facebook. And if it's shocking enough, I minutes later it's everywhere.

Speaker 6:          19:18          [inaudible]

Speaker 9:          19:22          like the timing of you guys making this thing and then this explosion of fake news like, Huh? How do you guys think about, about how it could be used for nefarious purposes?

Speaker 8:          19:35          Uh, yeah, it's a good question. Um, again, you're a camel mockers lizard man. I feel like when every technology is developed, then there, there is this danger of, uh, with our technology, you, um, you can create fake videos and so on, or I don't want to call it fake videos, but like to create video from audio. Right. But they are fake videos. Yeah. Yeah. But the way that I think about it is that like, scientists are doing their job in science, like inventing the technology and sign it off. And then we all need to like think about the next steps. Obviously. I mean, people shouldn't work on that. Um, and the answer is not clear. Maybe it's education. Maybe it's, um, every video should come up with some code now that this is, this is like authentic video or authentic attacks and you don't believe anything else. I mean, yeah, but like it maybe it was the timing more than anything, but I saw this video and it really felt like, oh my God, like America can't handle this right now. Like we're in a moment where we're, we're truth seems to be sort of a, an open disk. What is true is has become an open discussion and this seems to be adding fuel on the fire of sort of, uh, competing narratives in a way that I find

Speaker 2:          20:48          troubling. And I'm just curious that you don't,

Speaker 8:          20:53          um, a thing that I think that people, if people know the touch technology exists, then they will be more skeptical. My guess, I don't know. But if people know that fake news exists, if they know that fake texts exists, fake videos exist, fake photos exist, then everyone is more skeptical in what they read and see.

Speaker 2:          21:16          But like, Eh, a man in North Carolina, I think he was from North Carolina, believed from a fake print article that Hillary Clinton was running a sex ring out of a pizza parlor in DC, which is like insane. This man believed it and showed up with a gun. And if people are at a moment where they are willing to believe stories as ludicrous as that, like I don't expect them to wonder if this video is real or not.

Speaker 6:          21:43          [inaudible]

Speaker 8:          21:46          um, so whether you asking,

Speaker 2:          21:48          I'm asking, I'm asking, do you, are you afraid of the power of [inaudible]?

Speaker 8:          21:52          Yes. And if not, why? Just I'm just giving my, I don't know, just, uh, I'm answering your questions, but I'm a technologist. I'm a computer scientist, so I'm not really, because I know how to read and I know that because I know that this technology is reversible. I mean nobody. Well, there is not, not worried too much

Speaker 6:          22:30          [inaudible].

Speaker 2:          22:30          Have you seen these videos otherwise? I can text it, yeah. Okay. Yeah, and as we were feeling worried and more than that, surprised that the folks making these technologies weren't, we decided to do a third gut check. See if we were totally off base and get in touch with one of the guys who's on the front lines of this. Can you describe what was going through your head when you were watching bushes face? I can tell you exactly what I was thinking. I was thinking, how are we going to develop a forensic technique to detect this?

Speaker 2:          23:00          This is honey for reed. I am a professor of computer science at Dartmouth College. He started like a Sherlock Holmes of digital misdeeds, which means that it spends a lot of time sitting around looking at pictures and videos, trying to understand where has this come from? Has it been manipulated and should we trust it? He's been worked for all sorts of organizations. The AP, The Times Reuters who want to know if say a picture is fake or not, they often will ask me, you have to ugly when this just happened. Actually yesterday images came out of North Korea. Um, and every time images come out of these regimes where there's a history of photo manipulation, there are real concerns about this. So I was asked to determine if they'd been manipulated in some way and if so, how had they been manipulated and what, how the heck would you do that? Wow. Every time you manipulate data, you're going to leave something

Speaker 10:         23:46          behind. So let's say you do some funny business to a photo, you might create some noticeable distortion in the picture itself, but you also might distort the data. And we're in the business of basically finding those distortions in the data. For example, imagine he gets sent a photo, it's probably a Jpeg, jpeg, which now is 99% of the image formats that we see out there is what is called the lossy compression scheme. Just a fancy way to say that when a photo is taken and stored as a Jpeg, the camera, you know, just to save space throws a little bit of the data away. So for example, if I went out to the Dartmouth green right now took a picture of the grass, the camera isn't going to store all those millions of little variations of green hidden in the grass because that would be just a huge file.

Speaker 10:         24:32          It's going to save space by throwing some of those greens away. You just don't notice if it changes like a lot or a little bit less than that. It just grass as far as you can tell. Now here's how next trick. Every camera has a subtly different Palette of Greens that's going to keep and greens that's going to throw away. This varies tremendously from device to device. An iPhone compresses the image and much more so less Greens and a high end icon or a high on cannon, which would keep more of those variations of green. Now if you hold these two pictures side by side, you might not be able to tell the difference. But honey says, when you look at the underlying pixels, there are different recognizable patterns. If you take an image off of your iPhone, I should be able to go into that jpeg and look at the packaging and say, ah yes, this should have come out of an iPhone.

Speaker 10:         25:18          But if that image is uploaded to Facebook and then redownloaded or put into Photoshop and re saved, it will not look like jpeg consistent with an iPhone. So basically he can see at the level of the pixels or data whether the picture has been messed with in any way. Huh. And this is of course, just one of many different ways that honey can spot a fake. Yeah. Let me ask, if you could go up against the top 100 best counterfeiters, do you think you'd catch them 10% of the time? 50% of the time just to have kids? What's your sense? Um, I would say we could probably catch 75% of the fakes, but I would say that would take a long time to do this is not an easy task. And so, you know, the, the pace at which the media moves does not lend itself to careful forensic analysis of images.

Speaker 10:         26:07          Um, I'm always amazed that you get these emails, you're like, all right, you got 20 minutes and, um, you would need, you know, half a day, a day per image, still a very manual and a very human process. It's so are, is this video editing and this audio editing that's coming down the pipeline here? Yeah, I guess, should I be, should I be terrified? Um, yes, you should. Um, oh no. Did you really mean that? Yeah, I think it's, I think it's going to raise the fake news thing to a whole new level. I did see some artifacts by the way, and the videos, they are not perfect, but that's neither here nor there because the ability of technology to manipulate and alter reality is growing at a breakneck speed. Um, and the ability to disseminate that information is phenomenal. So I, I can't stop that by the way, because at the end of the day it's always going to be easier to create a fake but to detect a fake. Hmm.

Speaker 6:          27:06          [inaudible]

Speaker 10:         27:07          thank you very much. We're going to, Chad himself just handed me a cup of water, which shows none of you have gotten too big for your britches and that could be a serious problem. I would like to have seen Peter Jennings do that ever for this guy. My name is John Klein, Co founder and CEO of tap media. Before that, president of CNN, u s before that I was executive vice president of CBS News where I was executive in charge of 60 minutes, 48 hours and a bunch of other things and he's had to react to some serious evolutions in the media industry. Uh, he was manning the helm as social media exploded, as smartphones became ubiquitous. And consequently he had to deal with figuring out how and if to trust thousands of hours of video taken on these smart phones incented by viewers, what to broadcast and what not.

Speaker 10:         27:57          And so we wanted to know how someone in his position would think about these fake videos. So we sent him all of the different demos and videos we'd come across just to see what he thought. First thought was that this is the kind of thing that a James Bond villain would put to use or a the joker in Batman. Now we're an eighth grade girl who right. Wants to be most popular. Exactly. Yeah. You know, I mean, this is, there's so many ways to abuse this blows your mind. I mean, it, it goes to the very core of communication of any sort, whether it's television or radio or interpersonal. Um, it is what I'm seeing. True is what I'm hearing real, uh, in, in your, over the course of your career, you've seen multiple technological developments that have impacted the media in rather profound ways. Where is your terror level right now, or your fear level caused by this relative to all of the other sort of advancements that have occurred over, over your career? It's terrifying. And it, it hurdles us even faster toward that point where no one believes anything.

Speaker 2:          29:17          How do you have a democracy in a country where people can't trust anything that they see or read anymore? What, what we saw happen with the fake news during the election cycle was that, um, all the, the, it doesn't, it didn't even need to matter if anyone, you know, would rebuff it afterwards. This is nick built. And again, it would reach millions and millions of in me or seconds and, and you, and that was it. You've done it had done as j it's job. And I think that with this audio stuff and the video stuff that's gonna that's gonna come down, uh, on online in the next few years. Um, it's gonna do the same thing, but, but no one's going to know what's real and what's not.

Speaker 11:         29:59          I moved on her actually, you know, she was down in Palm Beach. I moved on her NFL.

Speaker 2:          30:03          And what's more, Nick says, if you think about the video that came out of Donald Trump from access Hollywood

Speaker 11:         30:09          automatically attracted to beautiful. I just started.

Speaker 2:          30:11          Um, the thing that was really interesting about that video

Speaker 11:         30:14          when you were started, they let you do it. You can do anything, whatever you want, grab by the,

Speaker 2:          30:18          you don't actually see Donald Trump until the very last second when he gets off the bus. How were you high? You only hear him make me a soapstone. I have a little hug with Donnelley and so if that technology existed today, I can guarantee you that Donald Trump would have responded by saying, oh, it's fake. It's fake news, fake audio. You can't see me. I didn't say that. And it would just be this video, his word against his

Speaker 6:          30:45          [inaudible].

Speaker 12:         30:46          Okay. Actually that's kind of like for me, that's sort of the real problem here. Like you create this, this possibility for like plausible deniability that's so broad. You know what I mean? It's like, you know, it's like the tobacco industry in the sixties and seventies I was just reading this great article by a, the writer Tim Harford about this in the sixties and seventies the tobacco industry led this very calculated effort to sort of push back against cancer science by, you know, just injecting a little bit of doubt here. A little bit of doubt there. Right. On the other hand, on the other hand, this and on the other hand that, and the idea was to create just enough wiggle room that nothing happens. They do that with climate change too. Exactly. And it's that little bit of doubt that creates paralysis. And is that what's going to happen that like there's going to be paralysis now writ large because now we're talking about the very things we see, the very things we hear

Speaker 6:          31:43          [inaudible]

Speaker 2:          31:43          but wait, but we don't you think that before we get completely carried away with the threat of this technology because, you know, maybe we should just find out literally where we are now. Yeah. We should give it a spin. Yeah. So at this moment, do you think making one of these eclipses is, is possible? I think it's entirely possible. Um, it just, I would be careful what it is.

Speaker 6:          32:11          [inaudible]

Speaker 12:         32:12          after the break, things get

Speaker 6:          32:15          fake. [inaudible]

Speaker 13:         32:30          how do you everyone, it's Angela calling from Dallas, Texas. Radiolab is supported in part by the outfit piece. Sloan Foundation, enhancing public understanding of science and technology in the modern world. More information aboutSloan@wwwdotsloan.org thanks. Radiolab.

Speaker 7:          32:56          Jad Robert Radiolab. So we're back. We're going to now fake something. We're going to build our own video from scratch. Fake words, fake faces, because we want to know like in use, how dangerous are these technologies really? Can they make a convincing fake? Are they as easy as advertised? So we will find out by giving the assignment as always to our long suffering. Simon Adler. So while I was in Seattle talking to during Gleaves, Dolby polite, not so subtly hinted that I would really like to, uh, to give vocal world. Let's say I had my hands on it somehow. What can I do with it? Well, right now, nothing, because we haven't shared it with them. At first, I just thought he didn't want me to be able to play around with it. But then I realized that, and I didn't even have a personal copy for myself, but yet. Oh, so it's not even on the premises here? No, it's still very much a contained to research.

Speaker 14:         33:52          Oh, but

Speaker 7:          33:55          hi. Hi there. Hey Matt. Oh yeah, yeah, I'm here. Great. Eventually I got in touch with this guy. So I'm Dr Matthew Elites. I'm the Chief Science Officer at Sarah Proc Limited, which is the vocal synthesis research company based in Edinburgh. Yeah. Okay. So I called you up because I was hoping that you could help me to make a video clip that has, I don't know, like George Bush or Barack Obama saying things that they have never said. Yep. That sounds great. That's it. He was just game. Yeah, no. See, the thing is, what his company does is not quite the same as Voco. What they do is like for a client, they'll create a voice that you can then just type in words or sentences and make that voice say whatever you want it to say. I feel sad. That's an interesting idea. They've, uh, created voices with a variety of accents. Great for you said last summer in a variety of languages. [inaudible]

Speaker 7:          34:50          and in their spare time when they're not making voices for clients, govern knowledge. What's N***a? I think they're building celebrity voices future and it just so happens they've got a Barack Obama and a George Bush bought. Yeah. How did did you create a George Bush robot? Well, a great thing about George Bush is that he was president of the United States for some time the morning, good morning, good morning, which means he had to give the weekly presidential address. We can go today. I received a great honor and the other great thing about the addresses, it's completely copyright free, so we were allowed to do anything we liked with that audio for the people of America, maybe things that they haven't envisaged that we're going to do with that. Real quick digression here just because it's absolutely fascinating, it looks like we're actually about to enter this really sticky gray area when it comes to ownership.

Speaker 7:          35:37          For example, in audiobook, if you record an audio book and you've signed over the rights to those audio files to the publisher, the publisher has the copyright. You, you don't own it. You do not own your own voice. Is that really true? Yeah. Anyway, back to Bush. So I took all those weekly addresses about six hours worth, which is a lot more tape than Volvo's 20 minutes. But what he did with it is pretty similar, right? You Fed them into this machine learning algorithm along with their transcripts. And then with the program we'll do a, it will take the text and it will analyze it in terms of the linguistics that will say this is the word social security, social, social is made up of the sound, sir. Oh, oh right. And so we'll cut those sounds out into lots of little tiny pieces. And it did that for all of the words in all of these addresses around 80,000 in total.

Speaker 7:          36:34          Put them all in this database with tons of info about what sound came before it, after, etc. And once that database is built, all that's left to do by typing some text. And then I push, go and try and find a set of little sounds which will join together really nicely. And then I pushed play and see how well they came out. So what we did was we found an old video of former presidents, George Bush and Barack Obama together, right? They're shaking hands, making generic statements. The exact clip isn't important, but we wondered could we turn that clip from a boring meet and greet to a scenario where Bush is telling Obama a joke. So we convinced a comedy writer Rachel Axler who works for the show veep to write as few jokes and then sent it off to Matt and this is what the computer spat out

Speaker 15:         37:26          and well it goes something like knock knock, who's there overall overall, I think it's something about the Oval Office. Probably. That was a a very good joke. Mr President, my wife Lori tells it better.

Speaker 7:          37:44          What the hell is that? Wait, what was that? That was terrible. That was like was it, I don't even get it. Hey, I don't understand that joke at all. And that's literally what the computer spit out. That is what the computer sped out. And truth be told, I don't think it's anywhere. It is not worthy of the negative response that you are getting. Terrible. Terrible. Let me show you another one.

Speaker 15:         38:07          So happy to be joining forces with this good man to put cortisone in your drinking water. What gets to help protect people's teeth so they don't get fillings? Isn't that fluoride? Oh shoot. I think I signed the wrong bill.

Speaker 7:          38:22          That's good. That's a legitimate, no, the robots are terrible. The joke is funny. Yeah, I like the joke, but the robots just massacred that joke, which is in itself kind of a joke. Well, I do think it, yeah, I'll let me get it. Well, I think the youtube are far more than you should be and you're far more critical than the average listener. However, Matt is so wrong about it. But anyway. Anyhow, Matt did tell me that conversations I getting people to s to talk back and forth to each other are still really difficult for a synthesizer to do.

Speaker 16:         38:50          But you know, conversational stuff is always difficult and in fact we're going to see, it's going to be a long time before we get really, really easy conversational synthesis. There's all sorts of barriers to that.

Speaker 7:          39:00          There's a human quality, uh, to a, to a conversation that, uh, that the synthesizers can't quite capture yet. But he also told us that, um, you know, if we add, once we add the video or if we add a video to this, uh, it will smooth out a lot of, a lot of the problems

Speaker 16:         39:14          when you have the faces as well speaking. People are not focusing in the audio and you can't hear that the areas in the same way.

Speaker 7:          39:21          So

Speaker 16:         39:26          hello?

Speaker 7:          39:26          Hey, is this Kyle? Oh yeah, great, great. I found these two Grad students. My name is Sinskey cyto. I am Tyler Shefsky from uh, the University of southern California USC. They also do a lot of facial reenactment research and agreed to help us. But, uh, making these visuals also turned out to be way harder than we thought. It turned out the clip we chose posed some serious challenges. Uh, there were too many side shots of Obama's face. Uh, the lighting was all wrong and, uh, eventually I got an email one late Sunday night saying, uh, it's not gonna work. Okay. So now I think I can draw a line here and I can point out that this, that we maybe got overexcited about this technology. It is not yet ready for true deceit. You have been fumbling and fumbling and fumbling here. I have with them fumble [inaudible] people. Okay. I find it interesting psychologically that Simon feels like it's a person who failure. I don't like to fail. You should. This is failure.

Speaker 12:         40:26          So, okay, just, just, uh, just on Simon's behalf, on the behalf of actually trying to answer the question, we felt like, okay, maybe, maybe we should try this one last time. Let's find a simpler Obama video and with the audio rather than like whole phrases. Let's just do a couple of word replacements here or there. By the way, the only reason we're using Obama is that he seems to be the cow. All of these technologies are built around it. In case we chose the video of Obama's last weekly address and we chose the audio from a talk he'd given in Chicago, uh, after he'd left off. What's, what's been going on in his speech. He sort of talks about what he's going to do next, how he's still gonna keep fighting for what he believes is right. Filled with idealism and absolutely certain that somehow I was going to change the world. But we thought, what if in an alternate reality he didn't want to keep fighting. What if he could at that moment see the divisions ahead? And he was just like, that's too much. I give up. No truth is we didn't think too hard about this because we didn't have much time. We just whipped it together, did a script based on words. Obama used with a few changes, send it off to the guys at USC.

Speaker 10:         41:32          Right. And I myself, uh, seeing this new script so that, uh, we can use that video of my face to pop a ties, a the former president. And, uh, when, when we got the final video back,

Speaker 12:         41:47          I have to say it was, I was expecting it to be horrible and where did it have a good laugh? But I, it went from like Laffy Keighley to, to, oh wait, this is creepy.

Speaker 10:         42:02          Well, yeah, no, I, I was suddenly, uh, I had been gang busters. We got to release this thing, uh, and not tell anybody and try to fake out the entire world. Uh, but when, when I saw it, there was, uh, a reluctancy. You mean you went, oh no, I went, Oh God, yeah, yeah. I thought, oh,

Speaker 12:         42:17          this, this, you know, my personal thought was like, it was convincing enough that I got genuinely spooked. But you know, just in fairness, we shouldn't sit around talking about something people can't see. Go to future of fake news.com and check it out for yourself. It's all one word, future of fake news.com and it'll pop right up. You can see, tell us what you think. You can see how Simon made the video. Check it out. Anyhow, the whole process got us all thinking like, Oh wow, what if we bunch of idiots can do this, uh, for no money very, very quickly. What will this mean to like a newsroom, for example, just to start there.

Speaker 10:         42:58          Uh, we're at the level now with, with this kind of thing where we need technologists to, to verify or knocked down. And again, news executive, John Klein, I don't think journalists, English majors are, are, are gonna be the ones to, to solve this. You know, you may have been editor of your school paper, but this is beyond your, your capability. But if you're good at collaborating with engineers and scientists, uh, you know, you'll, you'll have a good chance of working together to you to figure it out. So we need, we need technical expertise more than we ever have.

Speaker 12:         43:38          Okay. Can I ask you in your heart, this isn't the only, let me compare your heart to my heart for a second. In my heart. I want somebody to tell the researchers he has, sorry, you can't do that. Sorry. You know, I know. It's really cool. I know you, I know you probably a really proud of that algorithm, but, um, some men in black are gonna walk in right now and they're going to take your computers away. And and you, you just can't, sorry. Society is going to overrule you right now. Did you, is there a part of you that just dictatorial, he wants to just like squash this?

Speaker 10:         44:10          Sure, but when you still have the, what are they, the FSB and Moscow or the CIA utilizing this and developing it anyway, weaponizing it so to speak, probably. I think that the top down model could never contain that. John says ultimately happening is probably going to be bigger than any one organization or any one newsroom can solve. He said it'll probably end up coming down to the 14 and 15 year olds of tomorrow who will grow up using this technology, making fake videos, being the victims of fake videos and that maybe in the maze of them having to parse truth from fiction in such a personal way. Some kind of code will develop. I'm an optimist by nature. I do. I look at this and I say, well, somebody's going to figure it out. What worries me is the larger context within this state within which this takes place a way.

Speaker 10:         45:10          This is all occurring within a context of massive news. Illiteracy and the, the consumers seem to be just throwing their hands up and tiring of trying to even figure it out. And so just the, the, the, the work involved in getting to the bottom of the truth is unappealing to a growing percentage of the audience. And I'm not sure where Gen z the teenagers of today come out on this. Let's hope that they are more willing to do the work. Maybe out of self interest may be so that they're not dissed by, you know, the girl in social studies. But that's our best hope for overcoming it.

Speaker 6:          45:56          Okay.

Speaker 10:         45:57          Because everybody else seems to be sick of trying

Speaker 6:          46:08          [inaudible] uh,

Speaker 17:         46:16          reporter Simon Adler, this piece was produced by Simon and Andy McCune. Very special thanks to Kyle Olszewski and the entire team at USC Institute for Creative Technology for all their work, manipulating that video of President Obama. And thanks to Matthew Aylett for [inaudible]

Speaker 12:         46:46          the sizing. So, so many words for us. Rachel Axler for writing us the jokes that we tried to use. So whom power for building? It's an amazing website. Angus, Neil, Amy, pearl, everybody in the WWE NYC newsroom for advising us and giving us reaction shots to the face to face video and to David Carroll for putting us in touch with Nick Bilton in the first place. And to Nick Dalton for inspiring this whole story with his article, he's got a new one, a book actually American kingpin about the founder of a black market website called the Silk Road and to superstar in switching the coin computer scientist who works in eras lab, who helped us understand what the heck was going on. And finally, you can see the video that we created as well as a bunch of other kind of a crazy that we mentioned throughout this episode. It's at future of fake news.com all one word, future of fake news.com and uh, with that, my real cohosts and I will bet you a Jew. I'm jet Booman. I'm Robert Krulwich, who we really are. I'm glad we could finally be honest about that all these years.