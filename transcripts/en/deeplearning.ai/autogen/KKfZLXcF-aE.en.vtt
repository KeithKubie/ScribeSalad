WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:01.910
 
in a previous video you saw how to

00:00:01.910 --> 00:00:01.920
in a previous video you saw how to
 

00:00:01.920 --> 00:00:03.770
in a previous video you saw how to
compute derivatives and implement

00:00:03.770 --> 00:00:03.780
compute derivatives and implement
 

00:00:03.780 --> 00:00:05.869
compute derivatives and implement
gradient descent with respect to just

00:00:05.869 --> 00:00:05.879
gradient descent with respect to just
 

00:00:05.879 --> 00:00:07.550
gradient descent with respect to just
one training example for Lich's

00:00:07.550 --> 00:00:07.560
one training example for Lich's
 

00:00:07.560 --> 00:00:09.950
one training example for Lich's
regression now we want to do it for M

00:00:09.950 --> 00:00:09.960
regression now we want to do it for M
 

00:00:09.960 --> 00:00:12.470
regression now we want to do it for M
training examples to get started let's

00:00:12.470 --> 00:00:12.480
training examples to get started let's
 

00:00:12.480 --> 00:00:14.419
training examples to get started let's
remind ourselves of the definition of

00:00:14.419 --> 00:00:14.429
remind ourselves of the definition of
 

00:00:14.429 --> 00:00:17.480
remind ourselves of the definition of
the cost function J cost function WP

00:00:17.480 --> 00:00:17.490
the cost function J cost function WP
 

00:00:17.490 --> 00:00:19.550
the cost function J cost function WP
which you care about is this average

00:00:19.550 --> 00:00:19.560
which you care about is this average
 

00:00:19.560 --> 00:00:21.650
which you care about is this average
right 1 over m sum from I equals 1

00:00:21.650 --> 00:00:21.660
right 1 over m sum from I equals 1
 

00:00:21.660 --> 00:00:24.920
right 1 over m sum from I equals 1
through m you know the loss when your

00:00:24.920 --> 00:00:24.930
through m you know the loss when your
 

00:00:24.930 --> 00:00:28.040
through m you know the loss when your
algorithm output a I on the example why

00:00:28.040 --> 00:00:28.050
algorithm output a I on the example why
 

00:00:28.050 --> 00:00:33.260
algorithm output a I on the example why
we're you know AI is D prediction on the

00:00:33.260 --> 00:00:33.270
we're you know AI is D prediction on the
 

00:00:33.270 --> 00:00:35.750
we're you know AI is D prediction on the
I've trained example which is Sigma of

00:00:35.750 --> 00:00:35.760
I've trained example which is Sigma of
 

00:00:35.760 --> 00:00:39.770
I've trained example which is Sigma of
Zi which is equal to Sigma of W

00:00:39.770 --> 00:00:39.780
Zi which is equal to Sigma of W
 

00:00:39.780 --> 00:00:46.279
Zi which is equal to Sigma of W
transpose X plus B ok so what we show in

00:00:46.279 --> 00:00:46.289
transpose X plus B ok so what we show in
 

00:00:46.289 --> 00:00:48.529
transpose X plus B ok so what we show in
the previous line is for any single

00:00:48.529 --> 00:00:48.539
the previous line is for any single
 

00:00:48.539 --> 00:00:51.439
the previous line is for any single
training example how to compute you know

00:00:51.439 --> 00:00:51.449
training example how to compute you know
 

00:00:51.449 --> 00:00:55.639
training example how to compute you know
the derivatives when you have just one

00:00:55.639 --> 00:00:55.649
the derivatives when you have just one
 

00:00:55.649 --> 00:00:56.840
the derivatives when you have just one
training example

00:00:56.840 --> 00:00:56.850
training example
 

00:00:56.850 --> 00:01:02.810
training example
great so dw1 d w2 and d be with now the

00:01:02.810 --> 00:01:02.820
great so dw1 d w2 and d be with now the
 

00:01:02.820 --> 00:01:04.960
great so dw1 d w2 and d be with now the
superscript I to denote the

00:01:04.960 --> 00:01:04.970
superscript I to denote the
 

00:01:04.970 --> 00:01:07.250
superscript I to denote the
corresponding values you get if you were

00:01:07.250 --> 00:01:07.260
corresponding values you get if you were
 

00:01:07.260 --> 00:01:08.929
corresponding values you get if you were
doing what we did on the previous slide

00:01:08.929 --> 00:01:08.939
doing what we did on the previous slide
 

00:01:08.939 --> 00:01:12.070
doing what we did on the previous slide
but just using the one training example

00:01:12.070 --> 00:01:12.080
but just using the one training example
 

00:01:12.080 --> 00:01:16.219
but just using the one training example
X I Y I those use me missing on I there

00:01:16.219 --> 00:01:16.229
X I Y I those use me missing on I there
 

00:01:16.229 --> 00:01:19.340
X I Y I those use me missing on I there
as well so now you notice the overall

00:01:19.340 --> 00:01:19.350
as well so now you notice the overall
 

00:01:19.350 --> 00:01:21.649
as well so now you notice the overall
cost function the sum was really the

00:01:21.649 --> 00:01:21.659
cost function the sum was really the
 

00:01:21.659 --> 00:01:23.990
cost function the sum was really the
average gives a 1 over m term of the

00:01:23.990 --> 00:01:24.000
average gives a 1 over m term of the
 

00:01:24.000 --> 00:01:27.620
average gives a 1 over m term of the
individual losses so it turns out that

00:01:27.620 --> 00:01:27.630
individual losses so it turns out that
 

00:01:27.630 --> 00:01:31.789
individual losses so it turns out that
the derivative respect to say w1 of the

00:01:31.789 --> 00:01:31.799
the derivative respect to say w1 of the
 

00:01:31.799 --> 00:01:34.880
the derivative respect to say w1 of the
overall cost function is also going to

00:01:34.880 --> 00:01:34.890
overall cost function is also going to
 

00:01:34.890 --> 00:01:40.910
overall cost function is also going to
be the average of derivatives respect to

00:01:40.910 --> 00:01:40.920
be the average of derivatives respect to
 

00:01:40.920 --> 00:01:45.319
be the average of derivatives respect to
w1 of the individual loss terms but

00:01:45.319 --> 00:01:45.329
w1 of the individual loss terms but
 

00:01:45.329 --> 00:01:47.450
w1 of the individual loss terms but
previously we have already shown how to

00:01:47.450 --> 00:01:47.460
previously we have already shown how to
 

00:01:47.460 --> 00:01:52.999
previously we have already shown how to
compute this term as say d w1 I right

00:01:52.999 --> 00:01:53.009
compute this term as say d w1 I right
 

00:01:53.009 --> 00:01:55.340
compute this term as say d w1 I right
which we you know on the previous slide

00:01:55.340 --> 00:01:55.350
which we you know on the previous slide
 

00:01:55.350 --> 00:01:57.319
which we you know on the previous slide
show how the computers on a single

00:01:57.319 --> 00:01:57.329
show how the computers on a single
 

00:01:57.329 --> 00:01:59.480
show how the computers on a single
training example so what you need to do

00:01:59.480 --> 00:01:59.490
training example so what you need to do
 

00:01:59.490 --> 00:02:01.969
training example so what you need to do
is really compute these um derivatives

00:02:01.969 --> 00:02:01.979
is really compute these um derivatives
 

00:02:01.979 --> 00:02:04.069
is really compute these um derivatives
as we showed on the previous training

00:02:04.069 --> 00:02:04.079
as we showed on the previous training
 

00:02:04.079 --> 00:02:06.770
as we showed on the previous training
example and average them and this will

00:02:06.770 --> 00:02:06.780
example and average them and this will
 

00:02:06.780 --> 00:02:09.529
example and average them and this will
give you the overall gradient that you

00:02:09.529 --> 00:02:09.539
give you the overall gradient that you
 

00:02:09.539 --> 00:02:11.960
give you the overall gradient that you
can use to implement gradient descent

00:02:11.960 --> 00:02:11.970
can use to implement gradient descent
 

00:02:11.970 --> 00:02:14.660
can use to implement gradient descent
so I know there was a lot of details but

00:02:14.660 --> 00:02:14.670
so I know there was a lot of details but
 

00:02:14.670 --> 00:02:16.460
so I know there was a lot of details but
let's take all of this up and wrap this

00:02:16.460 --> 00:02:16.470
let's take all of this up and wrap this
 

00:02:16.470 --> 00:02:19.340
let's take all of this up and wrap this
up into a concrete algorithms and what

00:02:19.340 --> 00:02:19.350
up into a concrete algorithms and what
 

00:02:19.350 --> 00:02:21.050
up into a concrete algorithms and what
you should implement together to see

00:02:21.050 --> 00:02:21.060
you should implement together to see
 

00:02:21.060 --> 00:02:23.000
you should implement together to see
regression with gradient descent working

00:02:23.000 --> 00:02:23.010
regression with gradient descent working
 

00:02:23.010 --> 00:02:26.360
regression with gradient descent working
so here's what you can do let's

00:02:26.360 --> 00:02:26.370
so here's what you can do let's
 

00:02:26.370 --> 00:02:33.920
so here's what you can do let's
initialize J equals 0 DW 1 equals 0 DW 2

00:02:33.920 --> 00:02:33.930
initialize J equals 0 DW 1 equals 0 DW 2
 

00:02:33.930 --> 00:02:39.200
initialize J equals 0 DW 1 equals 0 DW 2
equals 0 DB equals 0 and what we're

00:02:39.200 --> 00:02:39.210
equals 0 DB equals 0 and what we're
 

00:02:39.210 --> 00:02:41.440
equals 0 DB equals 0 and what we're
going to do is use a for loop over the

00:02:41.440 --> 00:02:41.450
going to do is use a for loop over the
 

00:02:41.450 --> 00:02:44.900
going to do is use a for loop over the
training set and compute the derivatives

00:02:44.900 --> 00:02:44.910
training set and compute the derivatives
 

00:02:44.910 --> 00:02:46.070
training set and compute the derivatives
with respect to each training example

00:02:46.070 --> 00:02:46.080
with respect to each training example
 

00:02:46.080 --> 00:02:48.500
with respect to each training example
and then add them up all right so here's

00:02:48.500 --> 00:02:48.510
and then add them up all right so here's
 

00:02:48.510 --> 00:02:50.900
and then add them up all right so here's
we do for I equals 1 through m so M is

00:02:50.900 --> 00:02:50.910
we do for I equals 1 through m so M is
 

00:02:50.910 --> 00:02:52.400
we do for I equals 1 through m so M is
the number of training examples we

00:02:52.400 --> 00:02:52.410
the number of training examples we
 

00:02:52.410 --> 00:02:55.940
the number of training examples we
compute Z I equals W transpose X I plus

00:02:55.940 --> 00:02:55.950
compute Z I equals W transpose X I plus
 

00:02:55.950 --> 00:02:59.600
compute Z I equals W transpose X I plus
B the prediction AI is equal to Sigma of

00:02:59.600 --> 00:02:59.610
B the prediction AI is equal to Sigma of
 

00:02:59.610 --> 00:03:02.750
B the prediction AI is equal to Sigma of
zi and then you know let's let's add up

00:03:02.750 --> 00:03:02.760
zi and then you know let's let's add up
 

00:03:02.760 --> 00:03:09.140
zi and then you know let's let's add up
j j plus equals y i log a i plus 1 minus

00:03:09.140 --> 00:03:09.150
j j plus equals y i log a i plus 1 minus
 

00:03:09.150 --> 00:03:12.380
j j plus equals y i log a i plus 1 minus
y i log 1 minus AI and then this put a

00:03:12.380 --> 00:03:12.390
y i log 1 minus AI and then this put a
 

00:03:12.390 --> 00:03:14.030
y i log 1 minus AI and then this put a
negative sign in front of the whole

00:03:14.030 --> 00:03:14.040
negative sign in front of the whole
 

00:03:14.040 --> 00:03:16.010
negative sign in front of the whole
thing and then as we saw earlier we have

00:03:16.010 --> 00:03:16.020
thing and then as we saw earlier we have
 

00:03:16.020 --> 00:03:20.600
thing and then as we saw earlier we have
d zi or it is equal to AI minus y i and

00:03:20.600 --> 00:03:20.610
d zi or it is equal to AI minus y i and
 

00:03:20.610 --> 00:03:28.520
d zi or it is equal to AI minus y i and
DW gets plus equals x1 i d zi d w2 plus

00:03:28.520 --> 00:03:28.530
DW gets plus equals x1 i d zi d w2 plus
 

00:03:28.530 --> 00:03:33.199
DW gets plus equals x1 i d zi d w2 plus
equals x i2 d zi o and i'm doing this

00:03:33.199 --> 00:03:33.209
equals x i2 d zi o and i'm doing this
 

00:03:33.209 --> 00:03:35.300
equals x i2 d zi o and i'm doing this
calculation assuming that you have just

00:03:35.300 --> 00:03:35.310
calculation assuming that you have just
 

00:03:35.310 --> 00:03:37.430
calculation assuming that you have just
feet two features so that n is equal to

00:03:37.430 --> 00:03:37.440
feet two features so that n is equal to
 

00:03:37.440 --> 00:03:39.949
feet two features so that n is equal to
2 otherwise you do this for DW 1 DW

00:03:39.949 --> 00:03:39.959
2 otherwise you do this for DW 1 DW
 

00:03:39.959 --> 00:03:42.949
2 otherwise you do this for DW 1 DW
tunity number 3 and so on and then g p+

00:03:42.949 --> 00:03:42.959
tunity number 3 and so on and then g p+
 

00:03:42.959 --> 00:03:44.660
tunity number 3 and so on and then g p+
equals d zi

00:03:44.660 --> 00:03:44.670
equals d zi
 

00:03:44.670 --> 00:03:47.120
equals d zi
and i guess that's the end of the for

00:03:47.120 --> 00:03:47.130
and i guess that's the end of the for
 

00:03:47.130 --> 00:03:48.740
and i guess that's the end of the for
loop and then finally having done this

00:03:48.740 --> 00:03:48.750
loop and then finally having done this
 

00:03:48.750 --> 00:03:51.170
loop and then finally having done this
for all M training examples you will

00:03:51.170 --> 00:03:51.180
for all M training examples you will
 

00:03:51.180 --> 00:03:54.380
for all M training examples you will
still need to divide by M because we're

00:03:54.380 --> 00:03:54.390
still need to divide by M because we're
 

00:03:54.390 --> 00:03:58.660
still need to divide by M because we're
computing averages so d w1 if I equals m

00:03:58.660 --> 00:03:58.670
computing averages so d w1 if I equals m
 

00:03:58.670 --> 00:04:03.650
computing averages so d w1 if I equals m
DW to 2 Phi cos M DB devising cost M in

00:04:03.650 --> 00:04:03.660
DW to 2 Phi cos M DB devising cost M in
 

00:04:03.660 --> 00:04:06.560
DW to 2 Phi cos M DB devising cost M in
all the computer bridges and so with all

00:04:06.560 --> 00:04:06.570
all the computer bridges and so with all
 

00:04:06.570 --> 00:04:08.330
all the computer bridges and so with all
of these calculations you've just

00:04:08.330 --> 00:04:08.340
of these calculations you've just
 

00:04:08.340 --> 00:04:10.280
of these calculations you've just
computed the derivative of the cost

00:04:10.280 --> 00:04:10.290
computed the derivative of the cost
 

00:04:10.290 --> 00:04:12.199
computed the derivative of the cost
function J with respect to e 3

00:04:12.199 --> 00:04:12.209
function J with respect to e 3
 

00:04:12.209 --> 00:04:16.159
function J with respect to e 3
parameters w1 w2 and be just a couple

00:04:16.159 --> 00:04:16.169
parameters w1 w2 and be just a couple
 

00:04:16.169 --> 00:04:18.460
parameters w1 w2 and be just a couple
details of what we're doing we're using

00:04:18.460 --> 00:04:18.470
details of what we're doing we're using
 

00:04:18.470 --> 00:04:24.310
details of what we're doing we're using
DW 1 + DW 2 and DP 2 as accumulators

00:04:24.310 --> 00:04:24.320
DW 1 + DW 2 and DP 2 as accumulators
 

00:04:24.320 --> 00:04:26.680
DW 1 + DW 2 and DP 2 as accumulators
so that after this computation you know

00:04:26.680 --> 00:04:26.690
so that after this computation you know
 

00:04:26.690 --> 00:04:30.790
so that after this computation you know
DW 1 is equal to D derivative of your

00:04:30.790 --> 00:04:30.800
DW 1 is equal to D derivative of your
 

00:04:30.800 --> 00:04:33.160
DW 1 is equal to D derivative of your
overall cost function with respect to W

00:04:33.160 --> 00:04:33.170
overall cost function with respect to W
 

00:04:33.170 --> 00:04:36.340
overall cost function with respect to W
1 and similarly for DW 2 and DB so

00:04:36.340 --> 00:04:36.350
1 and similarly for DW 2 and DB so
 

00:04:36.350 --> 00:04:38.890
1 and similarly for DW 2 and DB so
notice that DW 1 + DW to do not have a

00:04:38.890 --> 00:04:38.900
notice that DW 1 + DW to do not have a
 

00:04:38.900 --> 00:04:40.840
notice that DW 1 + DW to do not have a
superscript I because we're using them

00:04:40.840 --> 00:04:40.850
superscript I because we're using them
 

00:04:40.850 --> 00:04:42.940
superscript I because we're using them
in this code as accumulators to sum over

00:04:42.940 --> 00:04:42.950
in this code as accumulators to sum over
 

00:04:42.950 --> 00:04:44.680
in this code as accumulators to sum over
the entire training set whereas in

00:04:44.680 --> 00:04:44.690
the entire training set whereas in
 

00:04:44.690 --> 00:04:48.490
the entire training set whereas in
contrast DZ I here this was d Z with

00:04:48.490 --> 00:04:48.500
contrast DZ I here this was d Z with
 

00:04:48.500 --> 00:04:50.680
contrast DZ I here this was d Z with
respect to just once single training

00:04:50.680 --> 00:04:50.690
respect to just once single training
 

00:04:50.690 --> 00:04:52.090
respect to just once single training
example so that's why that had a

00:04:52.090 --> 00:04:52.100
example so that's why that had a
 

00:04:52.100 --> 00:04:53.980
example so that's why that had a
superscript I to refer to the one

00:04:53.980 --> 00:04:53.990
superscript I to refer to the one
 

00:04:53.990 --> 00:04:56.350
superscript I to refer to the one
training example either that's computed

00:04:56.350 --> 00:04:56.360
training example either that's computed
 

00:04:56.360 --> 00:04:58.660
training example either that's computed
on and so having finished all these

00:04:58.660 --> 00:04:58.670
on and so having finished all these
 

00:04:58.670 --> 00:05:01.180
on and so having finished all these
calculations to implement one step of

00:05:01.180 --> 00:05:01.190
calculations to implement one step of
 

00:05:01.190 --> 00:05:03.970
calculations to implement one step of
gradient descent you implement W 1 gets

00:05:03.970 --> 00:05:03.980
gradient descent you implement W 1 gets
 

00:05:03.980 --> 00:05:06.400
gradient descent you implement W 1 gets
updated as W 1 minus a learning rate

00:05:06.400 --> 00:05:06.410
updated as W 1 minus a learning rate
 

00:05:06.410 --> 00:05:10.720
updated as W 1 minus a learning rate
times D W 1 W 2 kids up patients W 2 -

00:05:10.720 --> 00:05:10.730
times D W 1 W 2 kids up patients W 2 -
 

00:05:10.730 --> 00:05:13.750
times D W 1 W 2 kids up patients W 2 -
learning rate times DW 2 and B gets

00:05:13.750 --> 00:05:13.760
learning rate times DW 2 and B gets
 

00:05:13.760 --> 00:05:17.230
learning rate times DW 2 and B gets
updated as B - learning rate times G B

00:05:17.230 --> 00:05:17.240
updated as B - learning rate times G B
 

00:05:17.240 --> 00:05:21.010
updated as B - learning rate times G B
where DW 1 DW + DB where you know as

00:05:21.010 --> 00:05:21.020
where DW 1 DW + DB where you know as
 

00:05:21.020 --> 00:05:23.770
where DW 1 DW + DB where you know as
computed and finally J here would also

00:05:23.770 --> 00:05:23.780
computed and finally J here would also
 

00:05:23.780 --> 00:05:27.010
computed and finally J here would also
be a correct value for your cost

00:05:27.010 --> 00:05:27.020
be a correct value for your cost
 

00:05:27.020 --> 00:05:28.600
be a correct value for your cost
function so everything on this slide

00:05:28.600 --> 00:05:28.610
function so everything on this slide
 

00:05:28.610 --> 00:05:31.030
function so everything on this slide
implements just one single step of

00:05:31.030 --> 00:05:31.040
implements just one single step of
 

00:05:31.040 --> 00:05:33.100
implements just one single step of
gradient descent and so you have to

00:05:33.100 --> 00:05:33.110
gradient descent and so you have to
 

00:05:33.110 --> 00:05:35.740
gradient descent and so you have to
repeat everything on this slide multiple

00:05:35.740 --> 00:05:35.750
repeat everything on this slide multiple
 

00:05:35.750 --> 00:05:37.720
repeat everything on this slide multiple
times in order to take multiple steps of

00:05:37.720 --> 00:05:37.730
times in order to take multiple steps of
 

00:05:37.730 --> 00:05:40.510
times in order to take multiple steps of
gradient descent in case these details

00:05:40.510 --> 00:05:40.520
gradient descent in case these details
 

00:05:40.520 --> 00:05:42.970
gradient descent in case these details
seem too complicated again don't worry

00:05:42.970 --> 00:05:42.980
seem too complicated again don't worry
 

00:05:42.980 --> 00:05:45.160
seem too complicated again don't worry
too much about it for now hopefully all

00:05:45.160 --> 00:05:45.170
too much about it for now hopefully all
 

00:05:45.170 --> 00:05:47.470
too much about it for now hopefully all
this would be clearer when you go and

00:05:47.470 --> 00:05:47.480
this would be clearer when you go and
 

00:05:47.480 --> 00:05:49.570
this would be clearer when you go and
implement as in G programming assignment

00:05:49.570 --> 00:05:49.580
implement as in G programming assignment
 

00:05:49.580 --> 00:05:51.750
implement as in G programming assignment
but it turns out there are two

00:05:51.750 --> 00:05:51.760
but it turns out there are two
 

00:05:51.760 --> 00:05:55.420
but it turns out there are two
weaknesses with the calculation as well

00:05:55.420 --> 00:05:55.430
weaknesses with the calculation as well
 

00:05:55.430 --> 00:05:58.420
weaknesses with the calculation as well
as we've implemented it here which is

00:05:58.420 --> 00:05:58.430
as we've implemented it here which is
 

00:05:58.430 --> 00:06:00.670
as we've implemented it here which is
that to implement logistic regression

00:06:00.670 --> 00:06:00.680
that to implement logistic regression
 

00:06:00.680 --> 00:06:03.040
that to implement logistic regression
this way you need to write to for loops

00:06:03.040 --> 00:06:03.050
this way you need to write to for loops
 

00:06:03.050 --> 00:06:04.990
this way you need to write to for loops
the first for loop is this for loop over

00:06:04.990 --> 00:06:05.000
the first for loop is this for loop over
 

00:06:05.000 --> 00:06:07.210
the first for loop is this for loop over
the M training examples and the second

00:06:07.210 --> 00:06:07.220
the M training examples and the second
 

00:06:07.220 --> 00:06:09.370
the M training examples and the second
for loop is a for loop over all the

00:06:09.370 --> 00:06:09.380
for loop is a for loop over all the
 

00:06:09.380 --> 00:06:12.010
for loop is a for loop over all the
features over here right so in this

00:06:12.010 --> 00:06:12.020
features over here right so in this
 

00:06:12.020 --> 00:06:14.110
features over here right so in this
example we just had two features so n

00:06:14.110 --> 00:06:14.120
example we just had two features so n
 

00:06:14.120 --> 00:06:17.350
example we just had two features so n
it's equal to 2 and x equals 2 but if

00:06:17.350 --> 00:06:17.360
it's equal to 2 and x equals 2 but if
 

00:06:17.360 --> 00:06:18.580
it's equal to 2 and x equals 2 but if
you have more features you end up

00:06:18.580 --> 00:06:18.590
you have more features you end up
 

00:06:18.590 --> 00:06:21.640
you have more features you end up
writing yo DW 1 DW 2 and here similar

00:06:21.640 --> 00:06:21.650
writing yo DW 1 DW 2 and here similar
 

00:06:21.650 --> 00:06:24.430
writing yo DW 1 DW 2 and here similar
computations for DW 3 and so on down to

00:06:24.430 --> 00:06:24.440
computations for DW 3 and so on down to
 

00:06:24.440 --> 00:06:26.860
computations for DW 3 and so on down to
DW n so it seems like you need to have a

00:06:26.860 --> 00:06:26.870
DW n so it seems like you need to have a
 

00:06:26.870 --> 00:06:30.400
DW n so it seems like you need to have a
for loop over the features over our n

00:06:30.400 --> 00:06:30.410
for loop over the features over our n
 

00:06:30.410 --> 00:06:33.010
for loop over the features over our n
features when you're implementing deep

00:06:33.010 --> 00:06:33.020
features when you're implementing deep
 

00:06:33.020 --> 00:06:35.590
features when you're implementing deep
learning algorithms you find that having

00:06:35.590 --> 00:06:35.600
learning algorithms you find that having
 

00:06:35.600 --> 00:06:36.460
learning algorithms you find that having
explicit

00:06:36.460 --> 00:06:36.470
explicit
 

00:06:36.470 --> 00:06:38.440
explicit
for loops in your code makes your

00:06:38.440 --> 00:06:38.450
for loops in your code makes your
 

00:06:38.450 --> 00:06:41.860
for loops in your code makes your
algorithm run less efficiency and so in

00:06:41.860 --> 00:06:41.870
algorithm run less efficiency and so in
 

00:06:41.870 --> 00:06:44.140
algorithm run less efficiency and so in
the deep learning error would move to

00:06:44.140 --> 00:06:44.150
the deep learning error would move to
 

00:06:44.150 --> 00:06:46.660
the deep learning error would move to
bigger and bigger data sets and so being

00:06:46.660 --> 00:06:46.670
bigger and bigger data sets and so being
 

00:06:46.670 --> 00:06:48.670
bigger and bigger data sets and so being
able to implement your algorithms

00:06:48.670 --> 00:06:48.680
able to implement your algorithms
 

00:06:48.680 --> 00:06:50.830
able to implement your algorithms
without using explicit for loops is

00:06:50.830 --> 00:06:50.840
without using explicit for loops is
 

00:06:50.840 --> 00:06:52.840
without using explicit for loops is
really important and will help you to

00:06:52.840 --> 00:06:52.850
really important and will help you to
 

00:06:52.850 --> 00:06:55.150
really important and will help you to
scale to much bigger data sets so it

00:06:55.150 --> 00:06:55.160
scale to much bigger data sets so it
 

00:06:55.160 --> 00:06:56.740
scale to much bigger data sets so it
turns out that there are set of

00:06:56.740 --> 00:06:56.750
turns out that there are set of
 

00:06:56.750 --> 00:06:58.150
turns out that there are set of
techniques called vectorization

00:06:58.150 --> 00:06:58.160
techniques called vectorization
 

00:06:58.160 --> 00:07:01.180
techniques called vectorization
techniques that allow you to get rid of

00:07:01.180 --> 00:07:01.190
techniques that allow you to get rid of
 

00:07:01.190 --> 00:07:03.550
techniques that allow you to get rid of
these explicit full loops in your code I

00:07:03.550 --> 00:07:03.560
these explicit full loops in your code I
 

00:07:03.560 --> 00:07:06.190
these explicit full loops in your code I
think in the pre deep learning era

00:07:06.190 --> 00:07:06.200
think in the pre deep learning era
 

00:07:06.200 --> 00:07:08.220
think in the pre deep learning era
that's before the rise of deep learning

00:07:08.220 --> 00:07:08.230
that's before the rise of deep learning
 

00:07:08.230 --> 00:07:11.260
that's before the rise of deep learning
vectorization was a nice to have you

00:07:11.260 --> 00:07:11.270
vectorization was a nice to have you
 

00:07:11.270 --> 00:07:13.180
vectorization was a nice to have you
could sometimes do it to speed Agricole

00:07:13.180 --> 00:07:13.190
could sometimes do it to speed Agricole
 

00:07:13.190 --> 00:07:15.610
could sometimes do it to speed Agricole
and sometimes not but in the deep

00:07:15.610 --> 00:07:15.620
and sometimes not but in the deep
 

00:07:15.620 --> 00:07:17.770
and sometimes not but in the deep
learning era vectorization that is

00:07:17.770 --> 00:07:17.780
learning era vectorization that is
 

00:07:17.780 --> 00:07:20.050
learning era vectorization that is
getting rid of for loops like this and

00:07:20.050 --> 00:07:20.060
getting rid of for loops like this and
 

00:07:20.060 --> 00:07:22.720
getting rid of for loops like this and
like this has become really important

00:07:22.720 --> 00:07:22.730
like this has become really important
 

00:07:22.730 --> 00:07:25.060
like this has become really important
because we're more and more training on

00:07:25.060 --> 00:07:25.070
because we're more and more training on
 

00:07:25.070 --> 00:07:27.010
because we're more and more training on
very large datasets and so you really

00:07:27.010 --> 00:07:27.020
very large datasets and so you really
 

00:07:27.020 --> 00:07:29.260
very large datasets and so you really
need your code to be very efficient so

00:07:29.260 --> 00:07:29.270
need your code to be very efficient so
 

00:07:29.270 --> 00:07:31.230
need your code to be very efficient so
in the next few videos we'll talk about

00:07:31.230 --> 00:07:31.240
in the next few videos we'll talk about
 

00:07:31.240 --> 00:07:34.270
in the next few videos we'll talk about
vectorization and how to implement all

00:07:34.270 --> 00:07:34.280
vectorization and how to implement all
 

00:07:34.280 --> 00:07:37.330
vectorization and how to implement all
this without using even a single full

00:07:37.330 --> 00:07:37.340
this without using even a single full
 

00:07:37.340 --> 00:07:40.930
this without using even a single full
loop so of this I hope you have a sense

00:07:40.930 --> 00:07:40.940
loop so of this I hope you have a sense
 

00:07:40.940 --> 00:07:42.580
loop so of this I hope you have a sense
of how to implement the gistic

00:07:42.580 --> 00:07:42.590
of how to implement the gistic
 

00:07:42.590 --> 00:07:43.990
of how to implement the gistic
regression or gradient descent for

00:07:43.990 --> 00:07:44.000
regression or gradient descent for
 

00:07:44.000 --> 00:07:46.120
regression or gradient descent for
logistic regression things will be

00:07:46.120 --> 00:07:46.130
logistic regression things will be
 

00:07:46.130 --> 00:07:47.409
logistic regression things will be
clearer when you implement the program

00:07:47.409 --> 00:07:47.419
clearer when you implement the program
 

00:07:47.419 --> 00:07:50.080
clearer when you implement the program
exercise but before actually doing the

00:07:50.080 --> 00:07:50.090
exercise but before actually doing the
 

00:07:50.090 --> 00:07:51.850
exercise but before actually doing the
program exercise let's first talk about

00:07:51.850 --> 00:07:51.860
program exercise let's first talk about
 

00:07:51.860 --> 00:07:54.070
program exercise let's first talk about
vectorization so that you can implement

00:07:54.070 --> 00:07:54.080
vectorization so that you can implement
 

00:07:54.080 --> 00:07:56.440
vectorization so that you can implement
this whole thing implement in single

00:07:56.440 --> 00:07:56.450
this whole thing implement in single
 

00:07:56.450 --> 00:07:58.180
this whole thing implement in single
iteration of gradient descent without

00:07:58.180 --> 00:07:58.190
iteration of gradient descent without
 

00:07:58.190 --> 00:08:01.540
iteration of gradient descent without
using any full news

