WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:03.210
Does splitting your data into
three separate data sets

00:00:03.210 --> 00:00:04.990
sound overly complicated?

00:00:04.990 --> 00:00:07.660
Let's look at this in
action in the real world.

00:00:07.660 --> 00:00:10.970
Kaggle is a competition platform for
machine learning.

00:00:10.970 --> 00:00:13.336
People compete on classification tasks,
and

00:00:13.336 --> 00:00:15.648
whoever gets the highest
performance wins.

00:00:15.648 --> 00:00:20.147
Here, I'm showing the example of
a scientific competition on data that

00:00:20.147 --> 00:00:21.928
relates to the Higgs Boson.

00:00:21.928 --> 00:00:26.822
Kaggle also has three data sets,
the training data, the public validation

00:00:26.822 --> 00:00:32.080
data set, and a private data set that
is not revealed to the competitors.

00:00:32.080 --> 00:00:35.610
Here Kaggle shows you the performance
of the top competitors

00:00:35.610 --> 00:00:38.020
when measured on the private test sets.

00:00:38.020 --> 00:00:42.070
The green and red arrows show
how different the ranking was

00:00:42.070 --> 00:00:45.090
compared to the ranking
on the public set.

00:00:45.090 --> 00:00:46.970
Let's look at the rankings.

00:00:46.970 --> 00:00:51.190
The top competitors were doing well
on the public validation data and

00:00:51.190 --> 00:00:54.100
they remain at the top once
their private data was revealed.

00:00:55.250 --> 00:00:59.260
If you go further down the leaderboard,
however, it's a real bloodshed.

00:00:59.260 --> 00:01:01.920
Many competitors who thought
they were doing well,

00:01:01.920 --> 00:01:05.019
were not doing well at all
in the private data sets.

00:01:05.019 --> 00:01:05.990
As a result,

00:01:05.990 --> 00:01:10.970
their ranks went down dramatically once
the private data set was revealed.

00:01:10.970 --> 00:01:12.440
Why is that?

00:01:12.440 --> 00:01:15.945
Maybe they had a good model that did
well in validation just by chance.

00:01:15.945 --> 00:01:21.670
What's more likely however, is that
by validating themselves over and

00:01:21.670 --> 00:01:26.150
over dozens of times on the validation
set, they ended up over fitting

00:01:26.150 --> 00:01:29.005
to the validation set and
failing to generalize.

00:01:29.005 --> 00:01:33.760
The top competitors] however,
had good experimental design.

00:01:33.760 --> 00:01:36.850
They were not misled into thinking
that they were doing well.

00:01:36.850 --> 00:01:39.990
They probably took out some of
the training sets to validate their

00:01:39.990 --> 00:01:44.480
algorithm or used more sophisticated
methods like cross validation and

00:01:44.480 --> 00:01:47.870
didn't make many decisions based
on the public data set scores.

