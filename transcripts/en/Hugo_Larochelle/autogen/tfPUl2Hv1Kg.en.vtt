WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:08.309
 this capsule we will approach the concept 

00:00:02.970 --> 00:00:12.059
 of language modeling in the 

00:00:08.309 --> 00:00:15.000
 synovial business model that we have 

00:00:12.059 --> 00:00:18.000
 view that models the attached distribution 

00:00:15.000 --> 00:00:21.090
 between the category of a document and the 

00:00:18.000 --> 00:00:22.890
 document we can indeed see this model 

00:00:21.090 --> 00:00:24.930
 there as dividing the calculation of this 

00:00:22.890 --> 00:00:27.930
 probity modeling this 

00:00:24.930 --> 00:00:29.820
 probability there in two parts but 

00:00:27.930 --> 00:00:31.859
 firstly a category model 

00:00:29.820 --> 00:00:33.450
 which is all about a thought that involves 

00:00:31.859 --> 00:00:36.120
 week when the cac priority on the 

00:00:33.450 --> 00:00:38.399
 classes and a model that assigns 

00:00:36.120 --> 00:00:42.420
 probabilities on words by being 

00:00:38.399 --> 00:00:44.040
 given the class and this part there if 

00:00:42.420 --> 00:00:47.219
 what is called the language model 

00:00:44.040 --> 00:00:48.960
 so generally a model of 

00:00:47.219 --> 00:00:52.230
 language we sometimes say model of 

00:00:48.960 --> 00:00:53.789
 English is a distribution on the 

00:00:52.230 --> 00:00:56.640
 text ie on sequences of 

00:00:53.789 --> 00:00:58.710
 words so more precisely what that 

00:00:56.640 --> 00:01:02.670
 means that if I have a document 

00:00:58.710 --> 00:01:04.589
 w 1 up to wd but a language model 

00:01:02.670 --> 00:01:08.310
 will be able to assign a probability 

00:01:04.589 --> 00:01:13.229
 to observe this document there for parity 

00:01:08.310 --> 00:01:15.030
 from w 1 to w so in the model of 

00:01:13.229 --> 00:01:16.950
 business moutinot but actually the model 

00:01:15.030 --> 00:01:19.380
 of the language range is super simple 

00:01:16.950 --> 00:01:21.360
 then that was one of the goals actually 

00:01:19.380 --> 00:01:24.119
 so that leaves so simple 

00:01:21.360 --> 00:01:27.360
 suppose the words are made generate 

00:01:24.119 --> 00:01:29.310
 independently in the document being 

00:01:27.360 --> 00:01:31.650
 given the category this is it 

00:01:29.310 --> 00:01:33.329
 actually assumes that among other things their 

00:01:31.650 --> 00:01:35.100
 demo is not important so obviously 

00:01:33.329 --> 00:01:35.880
 that would not be a very good model of 

00:01:35.100 --> 00:01:39.150
 document 

00:01:35.880 --> 00:01:40.530
 when writing a document is the 

00:01:39.150 --> 00:01:43.110
 next word we write depends 

00:01:40.530 --> 00:01:44.490
 obviously words previous then this 

00:01:43.110 --> 00:01:48.630
 we want to write so clearly 

00:01:44.490 --> 00:01:51.299
 dependencies between limoux and 

00:01:48.630 --> 00:01:52.979
 for some applications other than 

00:01:51.299 --> 00:01:56.990
 classification of documents 

00:01:52.979 --> 00:01:58.770
 a model that assumes that words are 

00:01:56.990 --> 00:02:00.869
 their demos and important point is 

00:01:58.770 --> 00:02:02.759
 clearly in machine translation into 

00:02:00.869 --> 00:02:05.340
 voice recognition need them 

00:02:02.759 --> 00:02:06.899
 of language models that are a lot 

00:02:05.340 --> 00:02:11.030
 more powerful ones that will start to 

00:02:06.899 --> 00:02:11.030
 to treat the relation between the mobilities 

00:02:11.640 --> 00:02:17.610
 a language model that often used 

00:02:14.280 --> 00:02:20.220
 this is called the mg model 

00:02:17.610 --> 00:02:21.810
 this model, which is often used 

00:02:20.220 --> 00:02:24.860
 for in translation systems 

00:02:21.810 --> 00:02:27.600
 automatic or voice recognition 

00:02:24.860 --> 00:02:30.060
 so this is a model that tries 

00:02:27.600 --> 00:02:32.130
 to improve on the language model 

00:02:30.060 --> 00:02:33.360
 used in a model of 

00:02:32.130 --> 00:02:38.370
 classification of documents 

00:02:33.360 --> 00:02:42.900
 assuming that the mmo a sentence is 

00:02:38.370 --> 00:02:44.970
 actually generate or eventually start from 

00:02:42.900 --> 00:02:46.459
 a few words previous in the same 

00:02:44.970 --> 00:02:49.530
 sentence or in the same document 

00:02:46.459 --> 00:02:52.680
 precisely in a model ng we're going 

00:02:49.530 --> 00:02:55.980
 suppose the probability of words w1 

00:02:52.680 --> 00:02:58.950
 until wd in a document it's equal 

00:02:55.980 --> 00:03:02.880
 to the product of the probability of each 

00:02:58.950 --> 00:03:04.410
 words w hui given hates -1 

00:03:02.880 --> 00:03:07.170
 my previous 

00:03:04.410 --> 00:03:08.910
 so the words at the position i - n + 1 

00:03:07.170 --> 00:03:11.160
 up to 10 - 1 

00:03:08.910 --> 00:03:13.890
 for this fact in all they - weapons at 

00:03:11.160 --> 00:03:16.829
 previous is a first property 

00:03:13.890 --> 00:03:20.790
 models called model and g 

00:03:16.829 --> 00:03:23.070
 we call them model ng because so 

00:03:20.790 --> 00:03:25.530
 firstly i'll define you a gg 

00:03:23.070 --> 00:03:29.340
 it's actually a sub sequence of 

00:03:25.530 --> 00:03:31.079
 enemy that we extracted from a corpus we go 

00:03:29.340 --> 00:03:33.120
 see a concrete example of this early time 

00:03:31.079 --> 00:03:35.519
 and we call them ingram model because 

00:03:33.120 --> 00:03:39.989
 that in fact these models will be estimated 

00:03:35.519 --> 00:03:42.360
 from the 2nd g frequencies we're going 

00:03:39.989 --> 00:03:44.209
 see extract or calculated from a 

00:03:42.360 --> 00:03:47.220
 corpus so of a set of documents 

00:03:44.209 --> 00:03:49.140
 we will also notice that in fact a 

00:03:47.220 --> 00:03:53.940
 ng model this is the equivalent to a 

00:03:49.140 --> 00:03:58.350
 model or a n order markov chain 

00:03:53.940 --> 00:04:00.600
 - and so we in the capsules on the 

00:03:58.350 --> 00:04:03.299
 networks stolen kisses but we got 

00:04:00.600 --> 00:04:05.340
 concentrated on model of my order cop 1 

00:04:03.299 --> 00:04:08.070
 so that would correspond to a model a 

00:04:05.340 --> 00:04:10.890
 gramoune would be equal to two so a 

00:04:08.070 --> 00:04:13.859
 model where the next word idea not 

00:04:10.890 --> 00:04:15.780
 only from the word preceding the position y 

00:04:13.859 --> 00:04:17.190
 younger and in general the 

00:04:15.780 --> 00:04:18.900
 model 1 g we can not depend on 

00:04:17.190 --> 00:04:20.760
 only from previous mount but from 

00:04:18.900 --> 00:04:23.539
 two months previous too my previous 

00:04:20.760 --> 00:04:23.539
 and institutes 

00:04:24.759 --> 00:04:29.750
 so let's keep what we'll see 

00:04:28.009 --> 00:04:31.699
 we speak of a ng so magic no 

00:04:29.750 --> 00:04:35.030
 that we are a corpus that counted only one 

00:04:31.699 --> 00:04:36.560
 collector document have lived a bad 

00:04:35.030 --> 00:04:38.800
 or v learning fast so we have 

00:04:36.560 --> 00:04:41.810
 applied pretreatment where we have 

00:04:38.800 --> 00:04:44.240
 applied the vocabulary with the four 

00:04:41.810 --> 00:04:45.039
 words as we saw in the examples 

00:04:44.240 --> 00:04:47.750
 previous 

00:04:45.039 --> 00:04:51.319
 so in this document there in our 

00:04:47.750 --> 00:04:54.770
 corpus there are united g so united grand 

00:04:51.319 --> 00:04:56.569
 pot n equal to 1 of which five different 

00:04:54.770 --> 00:05:00.099
 so 5 a big difference 

00:04:56.569 --> 00:05:03.409
 so actually father 70.1 prove 

00:05:00.099 --> 00:05:05.389
 learning and to wc 17 a g and there 

00:05:03.409 --> 00:05:08.569
 actually has five different from 

00:05:05.389 --> 00:05:12.199
 scion collector a bad and 

00:05:08.569 --> 00:05:19.490
 learnings are the five united g 

00:05:12.199 --> 00:05:22.159
 different we also have 6 bis g big g 

00:05:19.490 --> 00:05:22.849
 for the case n equal to two and they are 

00:05:22.159 --> 00:05:24.770
 all different 

00:05:22.849 --> 00:05:29.569
 the first big g these people will be 

00:05:24.770 --> 00:05:32.900
 comma therefore these two Mauritians to this 

00:05:29.569 --> 00:05:38.630
 that is 9.1 2.1 

00:05:32.900 --> 00:05:40.729
 we have a bad 1 to ivg and this up 

00:05:38.630 --> 00:05:44.210
 the end of the sentence 

00:05:40.729 --> 00:05:46.909
 prove learning prove it does 

00:05:44.210 --> 00:05:49.520
 actually 6.6 g and they are all 

00:05:46.909 --> 00:05:55.310
 different and not one that is equal to 

00:05:49.520 --> 00:05:59.479
 another in my picard list we have 

00:05:55.310 --> 00:06:00.710
 then 5 trigram tri g for an equal to 

00:05:59.479 --> 00:06:01.159
 3 and again are all 

00:06:00.710 --> 00:06:03.080
 different 

00:06:01.159 --> 00:06:06.469
 so with the same reasoning we have 

00:06:03.080 --> 00:06:11.479
 pierced pts 1,1 which the first price sorting g 

00:06:06.469 --> 00:06:15.370
 here during comma a pot so will a 

00:06:11.479 --> 00:06:18.740
 sequence of t3 the second sequence of 

00:06:15.370 --> 00:06:20.240
 3.1 at the v and institutes but still a 

00:06:18.740 --> 00:06:22.810
 times they are all different and we 

00:06:20.240 --> 00:06:27.229
 could continue like this in extract 

00:06:22.810 --> 00:06:30.580
 the 4 g the 5 g and institutes and 

00:06:27.229 --> 00:06:33.469
 besides, out of curiosity we can 

00:06:30.580 --> 00:06:35.300
 extract statistics from different 

00:06:33.469 --> 00:06:37.430
 and g 

00:06:35.300 --> 00:06:40.610
 and observe historical trends 

00:06:37.430 --> 00:06:42.169
 google provides a tool for 

00:06:40.610 --> 00:06:44.360
 do this or the 

00:06:42.169 --> 00:06:47.330
 the corpus corresponds to the lines they 

00:06:44.360 --> 00:06:48.440
 have digitized by doing the 

00:06:47.330 --> 00:06:51.199
 character recognition for 

00:06:48.440 --> 00:06:52.099
 scan all the books of different 

00:06:51.199 --> 00:06:54.379
 moments in history and that's 

00:06:52.099 --> 00:06:58.669
 possible to see how often 

00:06:54.379 --> 00:07:02.930
 we observe different g as and 

00:06:58.669 --> 00:07:05.690
 measure for their different years the 

00:07:02.930 --> 00:07:09.280
 time so it can be denied to them 

00:07:05.690 --> 00:07:09.280
 consult on you encourages to do it 

00:07:10.449 --> 00:07:16.039
 ok rome and to comment what we 

00:07:12.969 --> 00:07:17.719
 does not lead where we learn a model and g 

00:07:16.039 --> 00:07:18.770
 commented on what we finally adapted 

00:07:17.719 --> 00:07:21.680
 to statistics 

00:07:18.770 --> 00:07:22.280
 present in our in a corpus of 

00:07:21.680 --> 00:07:26.900
 Documents 

00:07:22.280 --> 00:07:29.210
 we have several documents to do good 

00:07:26.900 --> 00:07:32.719
 like me we did finally way 

00:07:29.210 --> 00:07:33.770
 similar what we did in our 

00:07:32.719 --> 00:07:35.060
 document classification model 

00:07:33.770 --> 00:07:37.159
 that is, we will use 

00:07:35.060 --> 00:07:40.460
 relative frequencies so the probability 

00:07:37.159 --> 00:07:42.919
 in my ng model that who is going and adapted 

00:07:40.460 --> 00:07:46.819
 to my body more drive so his 

00:07:42.919 --> 00:07:49.729
 probability that w illi mo equals w 

00:07:46.819 --> 00:07:51.979
 given the help - previous ermo 

00:07:49.729 --> 00:07:56.599
 it may be the number of times that the 

00:07:51.979 --> 00:08:01.039
 word w follows the words wy has less between 

00:07:56.599 --> 00:08:03.289
 st up wi - in my body according 

00:08:01.039 --> 00:08:06.710
 twice that I observe the soft w and that 

00:08:03.289 --> 00:08:08.599
 this word is preceded by a lesser word 

00:08:06.710 --> 00:08:11.509
 previous on which I condition 

00:08:08.599 --> 00:08:13.610
 here and for this to be this option 

00:08:11.509 --> 00:08:14.990
 who normalizes who are 1 

00:08:13.610 --> 00:08:17.479
 but I will be divided by the number 

00:08:14.990 --> 00:08:20.479
 many times I observe it's less of a 

00:08:17.479 --> 00:08:23.449
 gram there so the country blue flies - 

00:08:20.479 --> 00:08:25.340
 between st up to wai -1 and the standard times 

00:08:23.449 --> 00:08:28.129
 that I observe them and that this big g the 

00:08:25.340 --> 00:08:31.370
 share in this 1 - 1 g there is followed by a 

00:08:28.129 --> 00:08:33.769
 other word any way a little more 

00:08:31.370 --> 00:08:38.649
 formal it's going to be the sum on all my 

00:08:33.769 --> 00:08:42.169
 gw frequency documents 

00:08:38.649 --> 00:08:44.630
 - next end up double life - 

00:08:42.169 --> 00:08:47.480
 hours followed by w 

00:08:44.630 --> 00:08:48.040
 its frequency of 20 grams of gold each 

00:08:47.480 --> 00:08:53.259
 docks 

00:08:48.040 --> 00:08:55.269
 1 pt / the frequency of my united g is 

00:08:53.259 --> 00:08:57.850
 less a follow-up of any moment 

00:08:55.269 --> 00:09:00.579
 I use the star symbol to say 

00:08:57.850 --> 00:09:02.740
 the number of times that I observe that 

00:09:00.579 --> 00:09:05.980
 I observe it's a less than a gram has it 

00:09:02.740 --> 00:09:07.180
 followed by any other word is 

00:09:05.980 --> 00:09:10.269
 so I'm summing up those frequencies 

00:09:07.180 --> 00:09:13.060
 there for all the documents bp one uses 

00:09:10.269 --> 00:09:19.470
 assure me that my distribution is going well 

00:09:13.060 --> 00:09:22.420
 well this summit so let's do an example 

00:09:19.470 --> 00:09:25.600
 so let's imagine we want to calculate the 

00:09:22.420 --> 00:09:28.000
 distribution of the following word given 

00:09:25.600 --> 00:09:31.120
 the two previous words so in this 

00:09:28.000 --> 00:09:38.500
 In this case, we would have three 

00:09:31.120 --> 00:09:41.259
 we would have a price and suppose that 

00:09:38.500 --> 00:09:44.290
 does I want the volt is conditional on the 

00:09:41.259 --> 00:09:48.040
 word that follows since I before 

00:09:44.290 --> 00:09:52.420
 model of and by putting same name as 

00:09:48.040 --> 00:09:58.420
 in my corpus the pattern frequency 

00:09:52.420 --> 00:10:01.980
 basic is 5 since the frequency of 

00:09:58.420 --> 00:10:06.880
 big markov type sort of 25 

00:10:01.980 --> 00:10:08.079
 language models be 10 and that 

00:10:06.880 --> 00:10:10.779
 finally the number of times that 

00:10:08.079 --> 00:10:13.930
 I observe model of anyone's 

00:10:10.779 --> 00:10:16.389
 what other word it was 200 years had 

00:10:13.930 --> 00:10:20.670
 finally the sum of all the elements 

00:10:16.389 --> 00:10:24.160
 that I have here but because if the way 

00:10:20.670 --> 00:10:27.339
 that I just wrote calculating estimate 

00:10:24.160 --> 00:10:29.949
 and there my model and learn it but 

00:10:27.339 --> 00:10:33.540
 given it's going to be to say that the 

00:10:29.949 --> 00:10:35.889
 Belgian policy follows model of 128 5 

00:10:33.540 --> 00:10:38.740
 so the number of times that bill sui 

00:10:35.889 --> 00:10:40.240
 model to divide by two hundred and 200 the 

00:10:38.740 --> 00:10:42.160
 number of times I see model of 

00:10:40.240 --> 00:10:43.720
 followed by any other word and I 

00:10:42.160 --> 00:10:46.839
 do the same thing to mark things 

00:10:43.720 --> 00:10:52.079
 to be 25 out of 200 then the language its 

00:10:46.839 --> 00:10:52.079
 parity is 1.10 out of 200 

00:10:53.730 --> 00:10:57.870
 what will be important to women by 

00:10:55.769 --> 00:11:00.980
 against this approach the naive sell the 

00:10:57.870 --> 00:11:03.959
 same problem that we saw with the model 

00:11:00.980 --> 00:11:06.899
 for documents to know that it's okay 

00:11:03.959 --> 00:11:10.050
 to be fairly easy on on new 

00:11:06.899 --> 00:11:12.570
 documents to get probabilities to 0 

00:11:10.050 --> 00:11:15.089
 it would be very important to smooth 

00:11:12.570 --> 00:11:16.680
 models in bold and actually a whole 

00:11:15.089 --> 00:11:19.199
 literature on different techniques 

00:11:16.680 --> 00:11:23.040
 of smoothing of models and g 

00:11:19.199 --> 00:11:24.180
 we are only going to see 2 so I 

00:11:23.040 --> 00:11:25.470
 recalls that the reason why 

00:11:24.180 --> 00:11:29.459
 it's important to do that 

00:11:25.470 --> 00:11:30.930
 does more ng and lons - it's going to be 

00:11:29.459 --> 00:11:33.149
 therefore more frequent the probability 

00:11:30.930 --> 00:11:35.310
 ultimately or the chance they 

00:11:33.149 --> 00:11:38.310
 do not appear in our corpus on 

00:11:35.310 --> 00:11:42.269
 the statistical calculation will be high so 

00:11:38.310 --> 00:11:44.160
 the more the arm of leningrad clio 

00:11:42.269 --> 00:11:46.199
 chances we will never have 

00:11:44.160 --> 00:11:50.730
 observed that we have never observed on 

00:11:46.199 --> 00:11:52.440
 a range there is so a way of high school 

00:11:50.730 --> 00:11:53.670
 it's to do the same thing as in the 

00:11:52.440 --> 00:11:57.060
 document model know how to make a 

00:11:53.670 --> 00:11:59.130
 smoothing delta where I'm going to add to 

00:11:57.060 --> 00:12:02.670
 numerator a constant of ltte 

00:11:59.130 --> 00:12:04.170
 positive and to get a model that 

00:12:02.670 --> 00:12:05.610
 sleep again I take the 

00:12:04.170 --> 00:12:07.160
 world and the top I will multiply by 

00:12:05.610 --> 00:12:11.519
 the number of words in my vocabulary 

00:12:07.160 --> 00:12:13.740
 +1 so to restate the body or w 

00:12:11.519 --> 00:12:16.230
 would not be in my vocabulary would 

00:12:13.740 --> 00:12:19.170
 bad so it's the same approach that 

00:12:16.230 --> 00:12:21.600
 previously I have the same two terms 

00:12:19.170 --> 00:12:23.579
 hamon numerator claudine denominator 

00:12:21.600 --> 00:12:25.410
 but I add a positive constant 

00:12:23.579 --> 00:12:27.569
 car than anything that's in my 

00:12:25.410 --> 00:12:30.000
 vocabulary and imported workers have 

00:12:27.569 --> 00:12:31.620
 null and Thursday here aims by that term there 

00:12:30.000 --> 00:12:35.940
 to make sure I do have a 

00:12:31.620 --> 00:12:37.319
 instruction that so summon in my 

00:12:35.940 --> 00:12:39.360
 previous example if I had a 

00:12:37.319 --> 00:12:42.139
 constant rate smoothing is equal to 

00:12:39.360 --> 00:12:46.050
 0.1 and that the size of my vocabulary 

00:12:42.139 --> 00:12:49.170
 it was 999 but in this case what 

00:12:46.050 --> 00:12:53.579
 jaurès and the numerator the frequencies 

00:12:49.170 --> 00:12:57.600
 sorting g 5 plus the constant of 

00:12:53.579 --> 00:13:01.350
 smoothing 0.1 divided by the number of 

00:12:57.600 --> 00:13:04.920
 Once I have the big g tracking model 

00:13:01.350 --> 00:13:07.110
 of any other word plus the 

00:13:04.920 --> 00:13:11.459
 constant smoothing times the term ap 

00:13:07.110 --> 00:13:14.670
 that's it 1 0 point 1 x 999 the 

00:13:11.459 --> 00:13:16.370
 fifth it so healthy and so that's me 

00:13:14.670 --> 00:13:19.050
 would give new probability 

00:13:16.370 --> 00:13:20.279
 better smooth in this case ci which me 

00:13:19.050 --> 00:13:23.190
 among other things, to deal with the case 

00:13:20.279 --> 00:13:26.070
 where I have a word that is great than ever 

00:13:23.190 --> 00:13:28.649
 appeared and that assured me that the words that 

00:13:26.070 --> 00:13:30.750
 a very basic frequency have not a 

00:13:28.649 --> 00:13:31.230
 influence too much on the calculation of 

00:13:30.750 --> 00:13:33.300
 probabilities 

00:13:31.230 --> 00:13:36.269
 because if we observe that models of 

00:13:33.300 --> 00:13:38.130
 language that was good quality of 10 m 2 

00:13:36.269 --> 00:13:39.720
 0 before because we had never seen 

00:13:38.130 --> 00:13:42.149
 language models in my caucus 

00:13:39.720 --> 00:13:47.640
 now a non-zero quality of 0.1 

00:13:42.149 --> 00:13:50.160
 over three years to a last technique 

00:13:47.640 --> 00:13:53.250
 smoothing that I mention is the 

00:13:50.160 --> 00:13:54.779
 smoothing by linear interpolation so 

00:13:53.250 --> 00:13:56.700
 the idea to get an idea to make a 

00:13:54.779 --> 00:13:58.980
 kind of weighted average between 

00:13:56.700 --> 00:14:00.540
 several models and g for values 

00:13:58.980 --> 00:14:01.380
 dell different so when a model 

00:14:00.540 --> 00:14:04.440
 unique g 

00:14:01.380 --> 00:14:06.750
 a model bi g a model tri g is 

00:14:04.440 --> 00:14:10.500
 located up to the n that I have 

00:14:06.750 --> 00:14:11.970
 so choose here so this will be 

00:14:10.500 --> 00:14:12.990
 mathematically it corresponds to what 

00:14:11.970 --> 00:14:16.380
 so 

00:14:12.990 --> 00:14:19.260
 my high school model by a vector of 

00:14:16.380 --> 00:14:21.510
 settings that the social peas 

00:14:19.260 --> 00:14:24.420
 different ngram that I'm going to call from 

00:14:21.510 --> 00:14:28.370
 the amdh to the probability according to this smoothing 

00:14:24.420 --> 00:14:32.190
 by linear interpolation to see w 

00:14:28.370 --> 00:14:34.019
 following a less than one gram lever 

00:14:32.190 --> 00:14:37.500
 way of placing up to w no less than one 

00:14:34.019 --> 00:14:40.589
 it's going maybe the man 2.1 times the 

00:14:37.500 --> 00:14:42.560
 probity associated with an immigrant so united 

00:14:40.589 --> 00:14:47.339
 This is a model that does not depend on 

00:14:42.560 --> 00:14:51.959
 previous words foiles m2 for longer 

00:14:47.339 --> 00:14:54.209
 the soul of twice the social significance big 

00:14:51.959 --> 00:14:57.120
 raemdonck it exceeds summit of the mount 

00:14:54.209 --> 00:14:59.699
 previous plus the soul of three times the 

00:14:57.120 --> 00:15:03.209
 pole t of a sort g that depends on both 

00:14:59.699 --> 00:15:05.820
 my previous and so on until 

00:15:03.209 --> 00:15:06.990
 the value of 1.3 alas what is 

00:15:05.820 --> 00:15:09.089
 important is that it's true in 

00:15:06.990 --> 00:15:10.570
 weighted average so that means that 

00:15:09.089 --> 00:15:14.110
 the 

00:15:10.570 --> 00:15:16.300
 hamdaoui hairs should peak at 1 and 

00:15:14.110 --> 00:15:18.810
 should not be positive so were 

00:15:16.300 --> 00:15:21.910
 the blades of i must also be 

00:15:18.810 --> 00:15:24.870
 fat or equal to zero and typically goes 

00:15:21.910 --> 00:15:27.760
 take baraki strictly legal that m 

00:15:24.870 --> 00:15:30.520
 among others if we only have the sorting range 

00:15:27.760 --> 00:15:33.550
 as in our example the trio models 

00:15:30.520 --> 00:15:35.580
 of language in frequency 0 if I did 

00:15:33.550 --> 00:15:39.130
 linear interpolation smoothing 

00:15:35.580 --> 00:15:41.710
 imagine that the big g mode then the 

00:15:39.130 --> 00:15:42.820
 language gram is present to him in 

00:15:41.710 --> 00:15:46.510
 my body puce 

00:15:42.820 --> 00:15:47.910
 but that means that by smoothing by 

00:15:46.510 --> 00:15:50.950
 Linear interpolation 

00:15:47.910 --> 00:15:55.150
 the probability are also smoothing there that 

00:15:50.950 --> 00:15:58.570
 I observe language given model of 

00:15:55.150 --> 00:16:00.940
 your bigger than zero because and that 

00:15:58.570 --> 00:16:05.290
 noting that double parents than amdh 

00:16:00.940 --> 00:16:07.470
 odors that is the weight associated with the 

00:16:05.290 --> 00:16:10.360
 big g language models so whoever 

00:16:07.470 --> 00:16:12.070
 will associate a priority to the angle 

00:16:10.360 --> 00:16:13.510
 only from the previous word of 

00:16:12.070 --> 00:16:19.000
 foccart long as the water slides 

00:16:13.510 --> 00:16:21.610
 sweet is bigger than 0 it's 

00:16:19.000 --> 00:16:23.110
 sure and certain that the probability here 

00:16:21.610 --> 00:16:24.580
 your greater than zero since the 

00:16:23.110 --> 00:16:27.130
 probity this forces to be greater 

00:16:24.580 --> 00:16:30.210
 that 0 or if he liked alain had been 

00:16:27.130 --> 00:16:32.380
 strictly bigger 0 the still so 

00:16:30.210 --> 00:16:34.180
 we would have a strictly more product 

00:16:32.380 --> 00:16:35.290
 great than zero because the probability 

00:16:34.180 --> 00:16:39.370
 to see languages 

00:16:35.290 --> 00:16:41.110
 this the environment of which Russia would be 

00:16:39.370 --> 00:16:42.760
 also larger of zero since so 

00:16:41.110 --> 00:16:46.000
 I have two languages ​​in my body more 

00:16:42.760 --> 00:16:47.050
 I obviously only have the word there so 

00:16:46.000 --> 00:16:49.510
 we see this is another way of 

00:16:47.050 --> 00:16:51.120
 make sure the frequency gaps 2 

00:16:49.510 --> 00:16:53.930
 0 

00:16:51.120 --> 00:16:56.940
 we do not necessarily have a probability 

00:16:53.930 --> 00:17:00.210
 equal to zero and among others the g as 

00:16:56.940 --> 00:17:02.790
 frequency 2 0 have brought to associate who 

00:17:00.210 --> 00:17:05.699
 is zero and this will allow us 

00:17:02.790 --> 00:17:06.990
 to have no instability in the 

00:17:05.699 --> 00:17:08.880
 quality of priorities better 

00:17:06.990 --> 00:17:11.160
 generalized to new documents that 

00:17:08.880 --> 00:17:13.850
 are not exactly like what we have 

00:17:11.160 --> 00:17:13.850
 in our corpus 

