WEBVTT
Kind: captions
Language: en

00:00:00.290 --> 00:00:01.400
Oh, hey Charles!

00:00:01.400 --> 00:00:02.180
&gt;&gt; Hi Michael!

00:00:02.180 --> 00:00:03.130
How are you doing today?

00:00:03.130 --> 00:00:04.019
&gt;&gt; I'm doing well, thank you.

00:00:04.019 --> 00:00:04.640
How are you?

00:00:04.640 --> 00:00:06.790
&gt;&gt; I'm doing just fine,
thank you so much for asking.

00:00:06.790 --> 00:00:08.460
But I do have a question.

00:00:08.460 --> 00:00:08.960
&gt;&gt; And what is that?

00:00:10.260 --> 00:00:11.210
&gt;&gt; Why are we here?

00:00:11.210 --> 00:00:12.270
&gt;&gt; Why are we here?

00:00:12.270 --> 00:00:14.439
&gt;&gt; Because we're teaching a class
on reinforcement learning.

00:00:14.439 --> 00:00:16.760
&gt;&gt; But we've already taught
a class on reinforcement learning.

00:00:16.760 --> 00:00:21.022
&gt;&gt; [SOUND] We taught a class on machine
learning, and there was a section of

00:00:21.022 --> 00:00:25.375
that class on reinforcement learning,
so it was kind of a mini course.

00:00:25.375 --> 00:00:29.100
&gt;&gt; Okay, but we've done this already,
so why would we be doing this again?

00:00:29.100 --> 00:00:31.800
&gt;&gt; The idea was that when we were
doing the mini course last time,

00:00:31.800 --> 00:00:34.980
there was all sorts of topics that
we didn't get to go into depth on.

00:00:34.980 --> 00:00:37.930
And it's a topic that's
really interesting to us, so

00:00:37.930 --> 00:00:40.700
we thought we could just kind of
expand it out and do a whole class.

00:00:40.700 --> 00:00:43.432
&gt;&gt; Okay, that seems reasonable,
but you haven't convinced me yet

00:00:43.432 --> 00:00:46.930
that this is really a whole class, and
that we're doing something different.

00:00:46.930 --> 00:00:47.950
So what are we doing that's different?

00:00:47.950 --> 00:00:51.298
&gt;&gt; So, well mostly we're just going to
be going more deeply into the theory and

00:00:51.298 --> 00:00:53.140
practice of reinforcement learning.

00:00:53.140 --> 00:00:54.880
&gt;&gt; Hm, okay well,
half of that sounds interesting.

00:00:54.880 --> 00:00:56.404
&gt;&gt; Oh, good.
So on the theory side,

00:00:56.404 --> 00:00:59.649
we're going to be studying things
like the convergence of algorithms.

00:00:59.649 --> 00:01:01.690
How long it takes them to converge.

00:01:01.690 --> 00:01:04.730
Whether you can put bounds on how many
mistakes it's going to make before it

00:01:04.730 --> 00:01:05.470
converges.

00:01:05.470 --> 00:01:07.860
Situations in which you
don't have convergence.

00:01:07.860 --> 00:01:12.107
These are all sorts of really, you
don't want to hear so much about this.

00:01:12.107 --> 00:01:12.629
&gt;&gt; Oh, were you talking?

00:01:12.629 --> 00:01:14.490
I'm sorry.
What about the practice side?

00:01:14.490 --> 00:01:16.620
&gt;&gt; All right, so on the practice side
we're going to be doing a lot more

00:01:16.620 --> 00:01:19.370
in terms of implementations of
reinforcement learning algorithms.

00:01:19.370 --> 00:01:21.160
And we're going to do, for each lesson,

00:01:21.160 --> 00:01:24.370
some kind of exercise where you use
a system that we built called Burlap.

00:01:24.370 --> 00:01:26.970
The sorts of topics we're going to
talk about include things like

00:01:26.970 --> 00:01:29.370
temporal difference learning,
like the TD lambda algorithm.

00:01:29.370 --> 00:01:30.239
&gt;&gt; Oh, I like TD lambda.

00:01:30.239 --> 00:01:31.817
I actually did a thesis on that once.

00:01:31.817 --> 00:01:33.872
&gt;&gt; Oh nice, okay good, so you should-
&gt;&gt; Yes, exactly once.

00:01:33.872 --> 00:01:36.279
&gt;&gt; Okay, [LAUGH] so you should be
interested in hearing about that or

00:01:36.279 --> 00:01:37.540
maybe even talking about that.

00:01:37.540 --> 00:01:41.110
We're also going to be talking about
how you can express things using

00:01:41.110 --> 00:01:42.230
reward functions.

00:01:42.230 --> 00:01:45.010
Which is a really important topic
in reinforcement learning because

00:01:45.010 --> 00:01:47.810
all of the way that we communicate
with the algorithms is in terms of

00:01:47.810 --> 00:01:49.750
their incentives in the form of rewards.

00:01:49.750 --> 00:01:51.080
&gt;&gt; I like that,
that makes a lot of sense.

00:01:51.080 --> 00:01:53.688
So that's sort of the practical
things that matter about making

00:01:53.688 --> 00:01:56.700
reinforcement learning actually
work in the, wow, let's do that.

00:01:56.700 --> 00:01:57.640
I like that.
What else?

00:01:57.640 --> 00:01:59.380
&gt;&gt; We should be talking
about generalization and

00:01:59.380 --> 00:02:00.780
scaling to some degree.

00:02:00.780 --> 00:02:02.450
&gt;&gt; Oh, I like that.
I want to talk about generalization and

00:02:02.450 --> 00:02:05.500
scaling, and
I want to tie it back in to abstraction.

00:02:05.500 --> 00:02:07.140
&gt;&gt; Oh, that's a really important topic,
great.

00:02:07.140 --> 00:02:07.920
&gt;&gt; Okay, cool.

00:02:07.920 --> 00:02:08.860
Can I do that?

00:02:08.860 --> 00:02:09.990
&gt;&gt; Sure, why don't you do that piece.

00:02:09.990 --> 00:02:13.120
And I'll talk about partially
observable Markov decision processes,

00:02:13.120 --> 00:02:15.880
which is decision-making when you
don't have complete knowledge about

00:02:15.880 --> 00:02:17.450
the current state of the environment.

00:02:17.450 --> 00:02:19.433
&gt;&gt; Okay, that's seems reasonable.

00:02:19.433 --> 00:02:20.037
I'll trade you.

00:02:20.037 --> 00:02:22.312
You can do POMDPs,
if you let me do options.

00:02:22.312 --> 00:02:23.856
&gt;&gt; Okay, all right.

00:02:23.856 --> 00:02:25.179
&gt;&gt; And, Monte Carlo methods.

00:02:25.179 --> 00:02:26.376
&gt;&gt; Okay, it's a deal.

00:02:26.376 --> 00:02:28.931
But we're also going to talk about
game theory which is awesome,

00:02:28.931 --> 00:02:30.752
because it has theory
right there in the name.

00:02:30.752 --> 00:02:33.269
&gt;&gt; Okay that's true, but it also has
game right there in the name, and

00:02:33.269 --> 00:02:34.470
that's what makes it cool.

00:02:34.470 --> 00:02:35.361
&gt;&gt; It should be fun for everyone.

00:02:35.361 --> 00:02:36.476
&gt;&gt; Yes, fun for the whole family.

00:02:36.476 --> 00:02:39.792
&gt;&gt; So, in the game theory section, we're
actually going to hearken back to things

00:02:39.792 --> 00:02:41.820
that we did in the previous mini course.

00:02:41.820 --> 00:02:44.490
But then we're going to update
it by giving more details about

00:02:44.490 --> 00:02:46.340
different solution concepts.

00:02:46.340 --> 00:02:49.654
&gt;&gt; Oh right, that's right, last time we
did this class we talked about all these

00:02:49.654 --> 00:02:52.386
things besides Nash equilibria
that were really interesting.

00:02:52.386 --> 00:02:53.663
And so we're going to have
a chance to talk about that now?

00:02:53.663 --> 00:02:54.348
&gt;&gt; Yep.

00:02:54.348 --> 00:02:57.120
&gt;&gt; Good, so
can we do correlated equilibrium?

00:02:57.120 --> 00:02:58.438
&gt;&gt; You can do correlated equilibrium.

00:02:58.438 --> 00:03:00.180
&gt;&gt; I'll do correlated equilibrium,
sounds good.

00:03:00.180 --> 00:03:01.740
&gt;&gt; All right, so are you ready to go?

00:03:01.740 --> 00:03:04.800
&gt;&gt; I think I'm ready to go, so let's go.

