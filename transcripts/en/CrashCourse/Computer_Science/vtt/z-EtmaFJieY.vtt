WEBVTT
Kind: captions
Language: en

00:00:02.960 --> 00:00:05.900
Hi, I’m Carrie Anne, and welcome to Crash
Course Computer Science!

00:00:05.900 --> 00:00:11.080
As we’ve touched on many times in this series,
computers are incredible at storing, organizing,

00:00:11.080 --> 00:00:13.509
fetching and processing huge volumes of data.

00:00:13.509 --> 00:00:17.640
That’s perfect for things like e-commerce
websites with millions of items for sale,

00:00:17.640 --> 00:00:20.760
and for storing billions of health records
for quick access by doctors.

00:00:20.760 --> 00:00:24.760
But what if we want to use computers not just
to fetch and display data, but to actually

00:00:24.760 --> 00:00:26.790
make decisions about data?

00:00:26.790 --> 00:00:31.270
This is the essence of machine learning – algorithms
that give computers the ability to learn from

00:00:31.270 --> 00:00:34.130
data, and then make predictions and decisions.

00:00:34.130 --> 00:00:38.740
Computer programs with this ability are extremely
useful in answering questions like Is an email

00:00:38.740 --> 00:00:39.240
spam?

00:00:39.240 --> 00:00:41.080
Does a person’s heart have arrhythmia?

00:00:41.090 --> 00:00:43.340
What video should youtube recommend after
this one?

00:00:43.350 --> 00:00:46.590
While useful, we probably wouldn’t describe
these programs as “intelligent” in the

00:00:46.590 --> 00:00:48.590
same way we think of human intelligence.

00:00:48.590 --> 00:00:52.920
So, even though the terms are often interchanged,
most computer scientists would say that machine

00:00:52.920 --> 00:00:57.371
learning is a set of techniques that sits
inside the even more ambitious goal of Artificial

00:00:57.380 --> 00:00:59.340
Intelligence, or AI for short.

00:00:59.340 --> 00:01:08.320
INTRO

00:01:08.320 --> 00:01:12.120
Machine Learning and AI algorithms tend to
be pretty sophisticated.

00:01:12.120 --> 00:01:16.100
So rather than wading into the mechanics of
how they work, we're going to focus on what

00:01:16.100 --> 00:01:17.940
the algorithms do conceptually.

00:01:17.940 --> 00:01:23.000
Let’s start with a simple example: deciding
if a moth is a Luna Moth or an Emperor Moth.

00:01:23.009 --> 00:01:27.039
This decision process is called classification,
and an algorithm that does it is called a

00:01:27.039 --> 00:01:28.039
classifier.

00:01:28.039 --> 00:01:31.229
Although there are techniques that can use
raw data for training – like photos and

00:01:31.229 --> 00:01:36.000
sounds – many algorithms reduce the complexity
of real world objects and phenomena into what

00:01:36.000 --> 00:01:37.380
are called features.

00:01:37.380 --> 00:01:41.260
Features are values that usefully characterize
the things we wish to classify.

00:01:41.260 --> 00:01:45.240
For our moth example, we’re going to use
two features: “wingspan” and “mass”.

00:01:45.240 --> 00:01:49.049
In order to train our machine learning classifier
to make good predictions, we’re going to

00:01:49.049 --> 00:01:50.439
need training data.

00:01:50.439 --> 00:01:54.909
To get that, we’d send an entomologist out
into a forest to collect data for both luna

00:01:54.909 --> 00:01:56.299
and emperor moths.

00:01:56.299 --> 00:02:00.649
These experts can recognize different moths,
so they not only record the feature values,

00:02:00.649 --> 00:02:03.479
but also label that data with the actual moth
species.

00:02:03.479 --> 00:02:05.250
This is called labeled data.

00:02:05.250 --> 00:02:09.560
Because we only have two features, it’s
easy to visualize this data in a scatterplot.

00:02:09.560 --> 00:02:14.340
Here, I’ve plotted data for 100 Emperor
Moths in red and 100 Luna Moths in blue.

00:02:14.340 --> 00:02:17.390
We can see that the species make two groupings,
but….

00:02:17.390 --> 00:02:21.530
there’s some overlap in the middle… so
it’s not entirely obvious how to best separate

00:02:21.530 --> 00:02:22.530
the two.

00:02:22.530 --> 00:02:26.120
That’s what machine learning algorithms
do – find optimal separations!

00:02:26.120 --> 00:02:30.769
I’m just going to eyeball it and say anything
less than 45 millimeters in wingspan is likely

00:02:30.769 --> 00:02:31.900
to be an Emperor Moth.

00:02:31.900 --> 00:02:37.300
We can add another division that says additionally
mass must be less than .75 in order for our

00:02:37.300 --> 00:02:38.480
guess to be Emperor Moth.

00:02:38.480 --> 00:02:42.020
These lines that chop up the decision space
are called decision boundaries.

00:02:42.020 --> 00:02:46.740
If we look closely at our data, we can see
that 86 emperor moths would correctly end

00:02:46.740 --> 00:02:52.349
up inside the emperor decision region, but
14 would end up incorrectly in luna moth territory.

00:02:52.349 --> 00:02:57.310
On the other hand, 82 luna moths would be
correct, with 18 falling onto the wrong side.

00:02:57.310 --> 00:03:02.079
A table, like this, showing where a classifier
gets things right and wrong is called a confusion

00:03:02.079 --> 00:03:05.120
matrix... which probably should have also
been the title of the last two movies in the

00:03:05.120 --> 00:03:06.340
Matrix Trilogy!

00:03:06.340 --> 00:03:10.219
Notice that there’s no way for us to draw
lines that give us 100% accuracy.

00:03:10.219 --> 00:03:15.019
If we lower our wingspan decision boundary,
we misclassify more Emperor moths as Lunas.

00:03:15.020 --> 00:03:17.530
If we raise it, we misclassify more Luna moths.

00:03:17.530 --> 00:03:22.000
The job of machine learning algorithms, at
a high level, is to maximize correct classifications

00:03:22.020 --> 00:03:23.790
while minimizing errors

00:03:23.790 --> 00:03:29.860
On our training data, we get 168 moths correct,
and 32 moths wrong, for an average classification

00:03:29.860 --> 00:03:31.540
accuracy of 84%.

00:03:31.540 --> 00:03:35.560
Now, using these decision boundaries, if we
go out into the forest and encounter an unknown

00:03:35.569 --> 00:03:39.170
moth, we can measure its features and plot
it onto our decision space.

00:03:39.170 --> 00:03:40.730
This is unlabeled data.

00:03:40.730 --> 00:03:44.390
Our decision boundaries offer a guess as to
what species the moth is.

00:03:44.390 --> 00:03:46.790
In this case, we’d predict it’s a Luna Moth.

00:03:46.790 --> 00:03:50.840
This simple approach, of dividing the decision
space up into boxes, can be represented by

00:03:50.840 --> 00:03:54.659
what’s called a decision tree, which would
look like this pictorially or could be written

00:03:54.659 --> 00:03:56.749
in code using If-Statements, like this.

00:03:56.749 --> 00:04:01.010
A machine learning algorithm that produces
decision trees needs to choose what features

00:04:01.010 --> 00:04:05.849
to divide on…and then for each of those
features, what values to use for the division.

00:04:05.849 --> 00:04:09.329
Decision Trees are just one basic example
of a machine learning technique.

00:04:09.329 --> 00:04:12.049
There are hundreds of algorithms in computer
science literature today.

00:04:12.049 --> 00:04:14.170
And more are being published all the time.

00:04:14.170 --> 00:04:19.040
A few algorithms even use many decision trees
working together to make a prediction.

00:04:19.040 --> 00:04:22.890
Computer scientists smugly call those Forests…
because they contain lots of trees.

00:04:22.890 --> 00:04:27.240
There are also non-tree-based approaches,
like Support Vector Machines, which essentially

00:04:27.240 --> 00:04:30.130
slice up the decision space using arbitrary
lines.

00:04:30.130 --> 00:04:33.660
And these don’t have to be straight lines;
they can be polynomials or some other fancy

00:04:33.660 --> 00:04:35.130
mathematical function.

00:04:35.130 --> 00:04:38.990
Like before, it’s the machine learning algorithm's
job to figure out the best lines to provide

00:04:38.990 --> 00:04:40.570
the most accurate decision boundaries.

00:04:40.570 --> 00:04:45.320
So far, my examples have only had two features,
which is easy enough for a human to figure

00:04:45.320 --> 00:04:46.000
out.

00:04:46.000 --> 00:04:51.120
If we add a third feature, let’s say, length
of antennae, then our 2D lines become 3D planes,

00:04:51.120 --> 00:04:53.680
creating decision boundaries in three dimensions.

00:04:53.680 --> 00:04:56.060
These planes don’t have to be straight either.

00:04:56.060 --> 00:05:00.000
Plus, a truly useful classifier would contend
with many different moth species.

00:05:00.000 --> 00:05:03.670
Now I think you’d agree this is getting
too complicated to figure out by hand…

00:05:03.670 --> 00:05:07.120
But even this is a very basic example – just three features

00:05:07.130 --> 00:05:08.640
and five moth species.

00:05:08.650 --> 00:05:10.810
We can still show it in this 3D scatter plot.

00:05:10.810 --> 00:05:15.140
Unfortunately, there’s no good way to visualize
four features at once, or twenty features,

00:05:15.140 --> 00:05:17.880
let alone hundreds or even thousands of features.

00:05:17.880 --> 00:05:21.030
But that’s what many real-world machine
learning problems face.

00:05:21.030 --> 00:05:25.410
Can YOU imagine trying to figure out the equation for a hyperplane rippling through a thousand-dimensional

00:05:25.410 --> 00:05:26.660
decision space?

00:05:26.660 --> 00:05:30.940
Probably not, but computers, with clever machine
learning algorithms can… and they do, all

00:05:30.940 --> 00:05:35.220
day long, on computers at places like Google,
Facebook, Microsoft and Amazon.

00:05:35.220 --> 00:05:38.780
Techniques like Decision Trees and Support
Vector Machines are strongly rooted in the

00:05:38.790 --> 00:05:43.040
field of statistics, which has dealt with
making confident decisions, using data, long

00:05:43.040 --> 00:05:44.950
before computers ever existed.

00:05:44.950 --> 00:05:49.110
There’s a very large class of widely used
statistical machine learning techniques, but

00:05:49.110 --> 00:05:52.950
there are also some approaches with no origins
in statistics.

00:05:52.950 --> 00:05:57.790
Most notable are artificial neural networks,
which were inspired by neurons in our brains!

00:05:57.790 --> 00:06:02.500
For a primer of biological neurons, check
out our three-part overview here, but basically

00:06:02.500 --> 00:06:07.330
neurons are cells that process and transmit
messages using electrical and chemical signals.

00:06:07.330 --> 00:06:11.630
They take one or more inputs from other cells,
process those signals, and then emit their

00:06:11.630 --> 00:06:12.760
own signal.

00:06:12.760 --> 00:06:17.640
These form into huge interconnected networks
that are able to process complex information.

00:06:17.640 --> 00:06:20.000
Just like your brain watching this youtube
video.

00:06:20.000 --> 00:06:21.940
Artificial Neurons are very similar.

00:06:21.940 --> 00:06:25.220
Each takes a series of inputs, combines them,
and emits a signal.

00:06:25.220 --> 00:06:29.740
Rather than being electrical or chemical signals,
artificial neurons take numbers in, and spit

00:06:29.750 --> 00:06:30.750
numbers out.

00:06:30.750 --> 00:06:34.910
They are organized into layers that are connected
by links, forming a network of neurons, hence

00:06:34.910 --> 00:06:35.500
the name.

00:06:35.500 --> 00:06:40.300
Let’s return to our moth example to see
how neural nets can be used for classification.

00:06:40.300 --> 00:06:44.960
Our first layer – the input layer – provides
data from a single moth needing classification.

00:06:44.960 --> 00:06:47.080
Again, we’ll use mass and wingspan.

00:06:47.080 --> 00:06:51.590
At the other end, we have an output layer,
with two neurons: one for Emperor Moth and

00:06:51.590 --> 00:06:52.900
another for Luna Moth.

00:06:52.900 --> 00:06:55.930
The most excited neuron will be our classification
decision.

00:06:55.930 --> 00:07:00.690
In between, we have a hidden layer, that transforms
our inputs into outputs, and does the hard

00:07:00.690 --> 00:07:01.690
work of classification.

00:07:01.690 --> 00:07:05.440
To see how this is done, let’s zoom into
one neuron in the hidden layer.

00:07:05.440 --> 00:07:09.560
The first thing a neuron does is multiply
each of its inputs by a specific weight, let’s

00:07:09.560 --> 00:07:12.990
say 2.8 for its first input, and .1 for it’s
second input.

00:07:12.990 --> 00:07:17.620
Then, it sums these weighted inputs together,
which is in this case, is a grand total of

00:07:17.620 --> 00:07:18.120
9.74.

00:07:18.200 --> 00:07:23.140
The neuron then applies a bias to this result
- in other words, it adds or subtracts a fixed

00:07:23.150 --> 00:07:27.020
value, for example, minus six, for a new value
of 3.74.

00:07:27.030 --> 00:07:31.130
These bias and inputs weights are initially
set to random values when a neural network

00:07:31.130 --> 00:07:32.130
is created.

00:07:32.130 --> 00:07:36.610
Then, an algorithm goes in, and starts tweaking
all those values to train the neural network,

00:07:36.610 --> 00:07:39.160
using labeled data for training and testing.

00:07:39.160 --> 00:07:43.710
This happens over many interactions, gradually
improving accuracy – a process very much

00:07:43.710 --> 00:07:44.920
like human learning.

00:07:44.920 --> 00:07:49.500
Finally, neurons have an activation function,
also called a transfer function, that gets

00:07:49.510 --> 00:07:53.930
applied to the output, performing a final
mathematical modification to the result.

00:07:53.930 --> 00:07:58.560
For example, limiting the value to a range
from negative one and positive one, or setting

00:07:58.560 --> 00:08:00.350
any negative values to 0.

00:08:00.350 --> 00:08:05.740
We’ll use a linear transfer function that
passes the value through unchanged, so 3.74

00:08:05.740 --> 00:08:07.510
stays as 3.74.

00:08:07.510 --> 00:08:13.640
So for our example neuron, given the inputs
.55 and 82, the output would be 3.74.

00:08:13.640 --> 00:08:17.950
This is just one neuron, but this process
of weighting, summing, biasing and applying

00:08:17.950 --> 00:08:22.710
an activation function is computed for all
neurons in a layer, and the values propagate

00:08:22.710 --> 00:08:24.950
forward in the network, one layer at a time.

00:08:24.950 --> 00:08:29.590
In this example, the output neuron with the
highest value is our decision: Luna Moth.

00:08:29.590 --> 00:08:33.520
Importantly, the hidden layer doesn’t have
to be just one layer… it can be many layers

00:08:33.520 --> 00:08:34.180
deep.

00:08:34.180 --> 00:08:36.580
This is where the term deep learning comes
from.

00:08:36.590 --> 00:08:40.820
Training these more complicated networks takes
a lot more computation and data.

00:08:40.820 --> 00:08:44.770
Despite the fact that neural networks were
invented over fifty years ago, deep neural

00:08:44.770 --> 00:08:49.820
nets have only been practical very recently,
thanks to powerful processors, but even more

00:08:49.820 --> 00:08:51.410
so, wicked fast GPUs.

00:08:51.410 --> 00:08:55.210
So, thank you gamers for being so demanding
about silky smooth framerates!

00:08:55.210 --> 00:08:59.460
A couple of years ago, Google and Facebook
demonstrated deep neural nets that could find

00:08:59.460 --> 00:09:03.520
faces in photos as well as humans – and
humans are really good at this!

00:09:03.520 --> 00:09:05.370
It was a huge milestone.

00:09:05.370 --> 00:09:10.400
Now deep neural nets are driving cars, translating
human speech, diagnosing medical conditions

00:09:10.400 --> 00:09:11.420
and much more.

00:09:11.420 --> 00:09:14.890
These algorithms are very sophisticated, but
it’s less clear if they should be described

00:09:14.890 --> 00:09:15.890
as “intelligent”.

00:09:15.890 --> 00:09:21.020
They can really only do one thing like classify
moths, find faces, or translate languages.

00:09:21.020 --> 00:09:23.810
This type of AI is called Weak AI or Narrow
AI.

00:09:23.810 --> 00:09:25.830
It’s only intelligent at specific tasks.

00:09:25.830 --> 00:09:30.360
But that doesn’t mean it’s not useful;
I mean medical devices that can make diagnoses,

00:09:30.360 --> 00:09:32.490
and cars that can drive themselves are amazing!

00:09:32.490 --> 00:09:36.390
But do we need those computers to compose
music and look up delicious recipes in their

00:09:36.390 --> 00:09:37.610
free time?

00:09:37.610 --> 00:09:38.610
Probably not.

00:09:38.610 --> 00:09:39.890
Although that would be kinda cool.

00:09:39.890 --> 00:09:44.070
Truly general-purpose AI, one as smart and
well-rounded as a human, is called Strong

00:09:44.070 --> 00:09:45.090
AI.

00:09:45.090 --> 00:09:49.250
No one has demonstrated anything close to
human-level artificial intelligence yet.

00:09:49.250 --> 00:09:53.150
Some argue it’s impossible, but many people
point to the explosion of digitized knowledge

00:09:53.150 --> 00:09:57.520
– like Wikipedia articles, web pages, and
Youtube videos – as the perfect kindling

00:09:57.520 --> 00:09:58.620
for Strong AI.

00:09:58.620 --> 00:10:02.750
Although you can only watch a maximum of 24
hours of youtube a day, a computer can watch

00:10:02.750 --> 00:10:04.250
millions of hours.

00:10:04.250 --> 00:10:10.130
For example, IBM’s Watson consults and synthesizes
information from 200 million pages of content,

00:10:10.130 --> 00:10:11.800
including the full text of Wikipedia.

00:10:11.800 --> 00:10:17.120
While not a Strong AI, Watson is pretty smart,
and it crushed its human competition in Jeopardy

00:10:17.120 --> 00:10:18.730
way back in 2011.

00:10:18.730 --> 00:10:22.970
Not only can AIs gobble up huge volumes of
information, but they can also learn over

00:10:22.970 --> 00:10:25.060
time, often much faster than humans.

00:10:25.060 --> 00:10:30.260
In 2016, Google debuted AlphaGo, a Narrow
AI that plays the fiendishly complicated board

00:10:30.260 --> 00:10:31.260
game Go.

00:10:31.260 --> 00:10:35.080
One of the ways it got so good and able to
beat the very best human players, was by playing

00:10:35.080 --> 00:10:37.400
clones of itself millions and millions of
times.

00:10:37.400 --> 00:10:41.620
It learned what worked and what didn’t,
and along the way, discovered successful strategies

00:10:41.620 --> 00:10:42.800
all by itself.

00:10:42.800 --> 00:10:45.940
This is called Reinforcement Learning, and
it’s a super powerful approach.

00:10:45.940 --> 00:10:48.710
In fact, it’s very similar to how humans
learn.

00:10:48.710 --> 00:10:52.330
People don’t just magically acquire the
ability to walk... it takes thousands of hours

00:10:52.330 --> 00:10:54.340
of trial and error to figure it out.

00:10:54.340 --> 00:10:58.700
Computers are now on the cusp of learning
by trial and error, and for many narrow problems,

00:10:58.700 --> 00:11:00.620
reinforcement learning is already widely used.

00:11:00.620 --> 00:11:04.210
What will be interesting to see, is if these
types of learning techniques can be applied

00:11:04.210 --> 00:11:09.061
more broadly, to create human-like, Strong
AIs that learn much like how kids learn, but

00:11:09.061 --> 00:11:10.760
at super accelerated rates.

00:11:10.760 --> 00:11:15.350
If that happens, there are some pretty big
changes in store for humanity – a topic

00:11:15.350 --> 00:11:16.850
we’ll revisit later.

00:11:16.850 --> 00:11:17.850
Thanks for watching.

00:11:17.850 --> 00:11:18.860
See you next week.

