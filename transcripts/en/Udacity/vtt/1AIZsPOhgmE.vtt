WEBVTT
Kind: captions
Language: en

00:00:00.250 --> 00:00:02.890
Okay, Michael, so that was a great conversation, what have we learned?

00:00:02.890 --> 00:00:05.460
&gt;&gt; Alright, well we talked about ensemble learning

00:00:05.460 --> 00:00:07.010
which was the idea of instead of just

00:00:07.010 --> 00:00:08.820
learning one thing, if it's good to learn

00:00:08.820 --> 00:00:11.840
once, it's even better to learn multiple times,

00:00:11.840 --> 00:00:12.510
&gt;&gt; In multiple ways.

00:00:12.510 --> 00:00:15.110
&gt;&gt; The simple version that we concentrated on first was

00:00:15.110 --> 00:00:17.840
this notion of bagging. Where what we did is instead

00:00:17.840 --> 00:00:20.125
of just learning on the whole data set, we would

00:00:20.125 --> 00:00:24.700
sub-sample bunch of examples from the training set, different ways,

00:00:24.700 --> 00:00:28.570
and train up different classifiers or different learners on each

00:00:28.570 --> 00:00:30.920
of those and then merge them together with the average.

00:00:30.920 --> 00:00:32.390
&gt;&gt; Okay, so if I can summarize

00:00:32.390 --> 00:00:36.702
that, we learned that ensembles are good. [LAUGH]

00:00:36.702 --> 00:00:40.670
&gt;&gt; We learned that even simple ensembles like bagging are good.

00:00:40.670 --> 00:00:45.090
&gt;&gt; We talked about the fact that by using this kind

00:00:45.090 --> 00:00:51.010
of ensemble approach, you can take simple learners or simple classifiers and

00:00:51.010 --> 00:00:53.160
merge them together and get more complicated classifiers.

00:00:53.160 --> 00:00:55.750
&gt;&gt; Mm, yeah, so we can take. We

00:00:55.750 --> 00:00:59.370
can. Combining simple gives you complex. Anything else?

00:00:59.370 --> 00:01:01.960
&gt;&gt; And we talked about the idea of boosting where you

00:01:01.960 --> 00:01:04.930
can Oh, maybe this is why it's called boosting. You can take

00:01:04.930 --> 00:01:08.570
something that has possibly very high error but always less than

00:01:08.570 --> 00:01:11.400
a half, and turn it into something that has very low error.

00:01:11.400 --> 00:01:14.080
&gt;&gt; So we learned that boosting is really good.

00:01:14.080 --> 00:01:16.360
And, we talked a little bit about why, that's

00:01:16.360 --> 00:01:17.700
good. By the way, there's a whole bunch of

00:01:17.700 --> 00:01:20.190
other details here too, right? Boosting also has the

00:01:20.190 --> 00:01:23.180
advan, as does bagging Not only has these little

00:01:23.180 --> 00:01:25.030
properties you've talked about before, but it tends to be

00:01:25.030 --> 00:01:28.980
very fast. It's agnostic to the learner. As you

00:01:28.980 --> 00:01:31.450
noticed, that in no time, did we say, try

00:01:31.450 --> 00:01:33.900
to take advantage of what the actual learner was

00:01:33.900 --> 00:01:36.550
doing. Just that it was, in fact, a weak learner.

00:01:36.550 --> 00:01:37.060
&gt;&gt; Hm.

00:01:37.060 --> 00:01:39.730
&gt;&gt; So I think that's important. It's agnostic.

00:01:39.730 --> 00:01:41.680
&gt;&gt; Meaning you can plug in any learner you want?

00:01:41.680 --> 00:01:44.900
&gt;&gt; Yeah. So long as it's a weak learner. So there's something

00:01:44.900 --> 00:01:47.720
we learned about. We learned about weak learners that we defined with that

00:01:47.720 --> 00:01:53.030
meant. And, we also talked about ,um, what error really, really means.

00:01:53.030 --> 00:01:56.950
With respect to some kind of underlying distribution. What do you think Michael?

00:01:56.950 --> 00:01:58.240
&gt;&gt; That seems like useful stuff.

00:01:58.240 --> 00:02:00.650
&gt;&gt; These are useful stuff to me. I'm going to throw one more

00:02:00.650 --> 00:02:02.920
thing at you, Michael, before I let you go. Okay, you ready?

00:02:02.920 --> 00:02:03.850
&gt;&gt; Yep.

00:02:03.850 --> 00:02:06.820
&gt;&gt; Here's a simple fact. About boosting that

00:02:06.820 --> 00:02:08.910
turns out in practice. You know our

00:02:08.910 --> 00:02:11.620
favorite little over-fitting example. Do you know

00:02:11.620 --> 00:02:14.420
how over-fitting works? You have a training

00:02:14.420 --> 00:02:16.470
line that tends to get better, and better,

00:02:16.470 --> 00:02:21.180
and better. Maybe even going down to zero error. But then you have test

00:02:21.180 --> 00:02:25.260
error Which gets better and better and at some point it starts to get worse.

00:02:25.260 --> 00:02:26.580
&gt;&gt; Mm.

00:02:26.580 --> 00:02:29.320
&gt;&gt; And at that point you have over fitting and I think,

00:02:29.320 --> 00:02:32.325
Michael, you asserted it at some point or maybe I asserted that

00:02:32.325 --> 00:02:34.980
,you always have to worry about over fitting. Over fitting is

00:02:34.980 --> 00:02:36.630
just the kind of fact of life. You got to come

00:02:36.630 --> 00:02:38.750
up with ways to deal with it or sort of over

00:02:38.750 --> 00:02:42.380
believing your data. Well, what if I told you that in practice

00:02:42.380 --> 00:02:45.620
When you run boosting, even as you run it over time

00:02:45.620 --> 00:02:49.130
so that your training error keeps getting better and better and

00:02:49.130 --> 00:02:52.960
better and better, it also turns out that your testing error

00:02:52.960 --> 00:02:57.280
keeps getting better and better and better and better and better and

00:02:57.280 --> 00:02:58.250
better and better.

00:02:58.250 --> 00:03:00.240
&gt;&gt; That seems too good to be true.

00:03:00.240 --> 00:03:03.490
&gt;&gt; It does seem too good to be true. It turns out it's

00:03:03.490 --> 00:03:07.020
not too good to be true. And I have an explanation for it.

00:03:07.020 --> 00:03:07.830
&gt;&gt; Tell me.

00:03:07.830 --> 00:03:10.200
&gt;&gt; Not until next time.

00:03:10.200 --> 00:03:11.650
&gt;&gt; alright, see you then.

00:03:11.650 --> 00:03:12.990
&gt;&gt; See you then. Bye.

