WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:04.740
 this capsule we will look at what 

00:00:02.340 --> 00:00:08.150
 different types of agents one can 

00:00:04.740 --> 00:00:08.150
 meet in artificial intelligence 

00:00:08.210 --> 00:00:13.620
 so as we live in the ps model 

00:00:11.160 --> 00:00:17.880
 we must also specify what are the 

00:00:13.620 --> 00:00:21.000
 shareholders and sensors it's between 

00:00:17.880 --> 00:00:23.250
 others from the captured observation 

00:00:21.000 --> 00:00:25.109
 by the agent that he will take 

00:00:23.250 --> 00:00:27.449
 decisions to finally make a 

00:00:25.109 --> 00:00:28.949
 action in his environment and 

00:00:27.449 --> 00:00:32.489
 now what we want to see is 

00:00:28.949 --> 00:00:36.809
 how are you that we can distinguish 

00:00:32.489 --> 00:00:41.059
 different ways to model or 

00:00:36.809 --> 00:00:43.760
 to implement the process of 

00:00:41.059 --> 00:00:50.700
 data observation followed by 

00:00:43.760 --> 00:00:55.170
 selection or action decisions we go into 

00:00:50.700 --> 00:00:57.329
 distinguishes four types of agents 

00:00:55.170 --> 00:00:59.190
 people there are actually pulled this 

00:00:57.329 --> 00:01:00.559
 categorization them pulled from the book of 

00:00:59.190 --> 00:01:03.750
 francois dhainaut league 

00:01:00.559 --> 00:01:07.500
 so I'm going to use their nomenclature 

00:01:03.750 --> 00:01:10.430
 we'll talk about the simple agent reflex 

00:01:07.500 --> 00:01:13.470
 then you rumba me is reflected in 

00:01:10.430 --> 00:01:18.500
 of hugo boss jean and jean love you pardon 

00:01:13.470 --> 00:01:18.500
 and finally youtube and ebay egypt 

00:01:20.509 --> 00:01:24.540
 the agent probably the simplest 

00:01:22.979 --> 00:01:26.640
 it is the symbol ief the ex agent 

00:01:24.540 --> 00:01:29.670
 this agent the water all that does is 

00:01:26.640 --> 00:01:30.390
 that he gets via his censors a 

00:01:29.670 --> 00:01:34.680
 observation 

00:01:30.390 --> 00:01:36.210
 it allows him to know what is the state 

00:01:34.680 --> 00:01:37.770
 of the environment or have some 

00:01:36.210 --> 00:01:40.200
 estimates of the state of the environment 

00:01:37.770 --> 00:01:40.979
 and from this estimate of 

00:01:40.200 --> 00:01:43.350
 the state of the environment 

00:01:40.979 --> 00:01:45.509
 well he will invoke knowledge 

00:01:43.350 --> 00:01:48.600
 money that tells him given 

00:01:45.509 --> 00:01:50.070
 certain conditions on the state of 

00:01:48.600 --> 00:01:51.659
 the environment that should be 

00:01:50.070 --> 00:01:53.040
 the action that should be taken 

00:01:51.659 --> 00:01:55.380
 concretely this knowledge there they 

00:01:53.040 --> 00:01:58.950
 take the form of a series of rules 

00:01:55.380 --> 00:02:00.090
 from which to identify for 

00:01:58.950 --> 00:02:03.180
 different conditions what is 

00:02:00.090 --> 00:02:06.079
 the action to be taken by this agent and that's 

00:02:03.180 --> 00:02:09.149
 finally this action that will be 

00:02:06.079 --> 00:02:11.450
 returned and then going to be instantiated 

00:02:09.149 --> 00:02:13.430
 by the shareholders 

00:02:11.450 --> 00:02:17.660
 so in fact in this case we have 

00:02:13.430 --> 00:02:19.519
 really a rather simple agent that acts 

00:02:17.660 --> 00:02:21.680
 only from the perception of 

00:02:19.519 --> 00:02:24.110
 current observation by ignoring 

00:02:21.680 --> 00:02:30.620
 history of observations 

00:02:24.110 --> 00:02:32.930
 previous ones we actually have here an example 

00:02:30.620 --> 00:02:36.130
 or a species of this caliber 

00:02:32.930 --> 00:02:37.959
 of agent where we would have a perception 

00:02:36.130 --> 00:02:40.670
 the interpretation of perception 

00:02:37.959 --> 00:02:41.269
 give an estimate of the state of 

00:02:40.670 --> 00:02:44.090
 the environment 

00:02:41.269 --> 00:02:47.720
 from that state and from 

00:02:44.090 --> 00:02:49.750
 rules that are part of his wish of 

00:02:47.720 --> 00:02:52.670
 prior knowledge of our money 

00:02:49.750 --> 00:02:55.430
 this one will determine what is the rule 

00:02:52.670 --> 00:02:57.019
 appropriate for this state there and then for 

00:02:55.430 --> 00:02:58.040
 this rule is one would determine 

00:02:57.019 --> 00:03:00.110
 is the action to follow 

00:02:58.040 --> 00:03:02.630
 looking for this rule years equal 

00:03:00.110 --> 00:03:05.780
 the dictated action and that action 

00:03:02.630 --> 00:03:07.280
 would be returned by our agent ok so 

00:03:05.780 --> 00:03:09.349
 it's an axis it's actually an agent who 

00:03:07.280 --> 00:03:09.650
 is very very simple and that looks 

00:03:09.349 --> 00:03:12.470
 a lot 

00:03:09.650 --> 00:03:14.690
 an action programming style 

00:03:12.470 --> 00:03:18.590
 we previously saw a capsule 

00:03:14.690 --> 00:03:24.049
 preceding where we have explained 

00:03:18.590 --> 00:03:26.989
 fathers of conditions and action to that 

00:03:24.049 --> 00:03:29.480
 his agent a little more complicated then a 

00:03:26.989 --> 00:03:33.019
 little more powerful is the urban mode 

00:03:29.480 --> 00:03:36.380
 reflex is young so this one from 

00:03:33.019 --> 00:03:39.200
 captured data will finally be able to 

00:03:36.380 --> 00:03:41.709
 update a persistent estimate 

00:03:39.200 --> 00:03:44.030
 of the state of the environment 

00:03:41.709 --> 00:03:45.560
 this estimate loves to be influenced 

00:03:44.030 --> 00:03:46.459
 by knowledge about commenting this 

00:03:45.560 --> 00:03:48.560
 as the world evolves 

00:03:46.459 --> 00:03:52.100
 then also commented what my 

00:03:48.560 --> 00:03:52.609
 actions can influence the state of 

00:03:52.100 --> 00:03:54.560
 the environment 

00:03:52.609 --> 00:03:57.560
 but again from this 

00:03:54.560 --> 00:03:59.780
 estimate the possibly a little more 

00:03:57.560 --> 00:04:02.030
 accurate of my environment 

00:03:59.780 --> 00:04:04.819
 but we will once again invoke 

00:04:02.030 --> 00:04:06.680
 rules of who says given certain 

00:04:04.819 --> 00:04:08.269
 conditions related to my environment 

00:04:06.680 --> 00:04:12.049
 that it should be the action that I 

00:04:08.269 --> 00:04:15.620
 should know and so in this case we have 

00:04:12.049 --> 00:04:17.419
 at least one agent who can accumulate 

00:04:15.620 --> 00:04:19.250
 information in time for better 

00:04:17.419 --> 00:04:21.590
 estimate the state of the environment 

00:04:19.250 --> 00:04:23.130
 it's particularly useful if in fact 

00:04:21.590 --> 00:04:24.990
 I have a 

00:04:23.130 --> 00:04:26.450
 at each is every time I get 

00:04:24.990 --> 00:04:28.320
 an observation only one 

00:04:26.450 --> 00:04:28.950
 partial observation of my 

00:04:28.320 --> 00:04:31.110
 environment 

00:04:28.950 --> 00:04:33.120
 so by accumulating this information there 

00:04:31.110 --> 00:04:36.090
 I can possibly specify that she 

00:04:33.120 --> 00:04:36.660
 is kelly finally the tod of my 

00:04:36.090 --> 00:04:39.780
 environment 

00:04:36.660 --> 00:04:42.660
 so it's going to be particularly useful 

00:04:39.780 --> 00:04:45.360
 in then more powerful in the context 

00:04:42.660 --> 00:04:49.590
 of an environment that is partially 

00:04:45.360 --> 00:04:52.830
 observable what will be much more 

00:04:49.590 --> 00:04:55.800
 interesting though is to add to 

00:04:52.830 --> 00:04:57.810
 our agent a final ability of 

00:04:55.800 --> 00:05:00.450
 make decisions by oneself where one 

00:04:57.810 --> 00:05:03.210
 would no longer have to specify a set of 

00:05:00.450 --> 00:05:06.300
 rules but rather specific a purpose or 

00:05:03.210 --> 00:05:08.490
 start reaching through the agent and in 

00:05:06.300 --> 00:05:10.800
 adding as hostage knowledge to 

00:05:08.490 --> 00:05:14.220
 knowledge about what will be 

00:05:10.800 --> 00:05:15.420
 the impact of an action I'm doing on 

00:05:14.220 --> 00:05:17.430
 the state of the environment 

00:05:15.420 --> 00:05:19.710
 but then it's going to be possible to define 

00:05:17.430 --> 00:05:21.240
 and invoke algorithms 

00:05:19.710 --> 00:05:24.630
 of artificial intelligence that are going 

00:05:21.240 --> 00:05:26.940
 finally be able to forgive who will be able 

00:05:24.630 --> 00:05:28.890
 make the connection between the two in order to 

00:05:26.940 --> 00:05:30.630
 determine from my knowledge 

00:05:28.890 --> 00:05:32.190
 on what is the impact that I can have 

00:05:30.630 --> 00:05:33.930
 on the environment and what is the purpose 

00:05:32.190 --> 00:05:35.940
 that I want to reach what should 

00:05:33.930 --> 00:05:38.430
 to be the next action I have 

00:05:35.940 --> 00:05:41.280
 so in this case we are talking about 

00:05:38.430 --> 00:05:43.500
 really an agent that does not need 

00:05:41.280 --> 00:05:45.990
 specify conditions 

00:05:43.500 --> 00:05:48.270
 actions but who will actually have the 

00:05:45.990 --> 00:05:48.840
 ability to make decisions by 

00:05:48.270 --> 00:05:51.990
 yourself 

00:05:48.840 --> 00:05:54.840
 for example by simulating what she is 

00:05:51.990 --> 00:05:57.240
 what will happen if I do the action 

00:05:54.840 --> 00:06:01.410
 in the current environment that I value 

00:05:57.240 --> 00:06:04.530
 so an example of that would be an agent 

00:06:01.410 --> 00:06:06.450
 who sounds sudoku simulating what is 

00:06:04.530 --> 00:06:09.000
 that could happen if I register as 

00:06:06.450 --> 00:06:10.890
 next digit a given figure that 

00:06:09.000 --> 00:06:12.330
 to give myself in my grid but to be 

00:06:10.890 --> 00:06:13.620
 able to simulate hockey is what 

00:06:12.330 --> 00:06:18.300
 I will eventually succeed in 

00:06:13.620 --> 00:06:20.370
 reach a solution to my sudoku and 

00:06:18.300 --> 00:06:22.020
 in this case also by combining my 

00:06:20.370 --> 00:06:23.460
 knowledge about what my 

00:06:22.020 --> 00:06:25.260
 actions have as impact on 

00:06:23.460 --> 00:06:26.700
 the environment and what is the purpose so 

00:06:25.260 --> 00:06:28.860
 in my example of this caucus would have 

00:06:26.700 --> 00:06:31.380
 solve the sudoku the agent is going to be able 

00:06:28.860 --> 00:06:33.220
 go internal reasoned simulate what 

00:06:31.380 --> 00:06:34.390
 could happen is so 

00:06:33.220 --> 00:06:36.010
 his indications the actions that are going 

00:06:34.390 --> 00:06:38.110
 to eventually reach the 

00:06:36.010 --> 00:06:42.490
 purpose then which ones will not allow 

00:06:38.110 --> 00:06:44.080
 to reach a world where is ultimately 

00:06:42.490 --> 00:06:46.900
 the fourth type of agent this is going to be a 

00:06:44.080 --> 00:06:48.760
 agent utility bad or rather than 

00:06:46.900 --> 00:06:50.650
 specify beginnings at athens 

00:06:48.760 --> 00:06:53.500
 we want to actually specify a function 

00:06:50.650 --> 00:06:55.660
 of utility that for the leg is going to be 

00:06:53.500 --> 00:06:57.220
 in a way a measure of to 

00:06:55.660 --> 00:07:00.150
 how good is he happy if 

00:06:57.220 --> 00:07:02.830
 we want to be in that state 

00:07:00.150 --> 00:07:04.750
 by this measure of telecom tulle is in 

00:07:02.830 --> 00:07:07.300
 able to incorporate preference notions 

00:07:04.750 --> 00:07:08.890
 between the actions among others are going 

00:07:07.300 --> 00:07:11.860
 ability to combine higher utility 

00:07:08.890 --> 00:07:14.110
 to 2 actions that would allow all 

00:07:11.860 --> 00:07:15.460
 two to reach seen but where there would be 

00:07:14.110 --> 00:07:17.650
 an action that might allow for 

00:07:15.460 --> 00:07:20.880
 hold it faster so for example 

00:07:17.650 --> 00:07:23.260
 in the case of a chess player 

00:07:20.880 --> 00:07:24.880
 well there may be two actions that 

00:07:23.260 --> 00:07:26.230
 would eventually win the 

00:07:24.880 --> 00:07:29.830
 part but we will perhaps want 

00:07:26.230 --> 00:07:31.510
 promote actions that will allow 

00:07:29.830 --> 00:07:34.030
 to win faster 

00:07:31.510 --> 00:07:37.870
 ok for maybe diminish people this 

00:07:34.030 --> 00:07:40.600
 that we are wrong and that we lose 

00:07:37.870 --> 00:07:42.400
 long term so that's really the 

00:07:40.600 --> 00:07:44.680
 only distinction between google and 

00:07:42.400 --> 00:07:45.850
 youtube and we are changing the 

00:07:44.680 --> 00:07:47.740
 goal specification by a 

00:07:45.850 --> 00:07:49.720
 specification of a utility that will us 

00:07:47.740 --> 00:07:53.040
 to distinguish quality 

00:07:49.720 --> 00:07:53.040
 finally different actions 

00:07:54.930 --> 00:08:01.720
 the four agents agrees to see in 

00:07:57.640 --> 00:08:03.700
 do are going to use ways 

00:08:01.720 --> 00:08:05.070
 different to make their decision 

00:08:03.700 --> 00:08:08.440
 that's what sets them apart 

00:08:05.070 --> 00:08:10.690
 he uses everything from knowledge to 

00:08:08.440 --> 00:08:12.250
 a priori that it is there then 

00:08:10.690 --> 00:08:13.479
 knowledge that is assumed to be acquired 

00:08:12.250 --> 00:08:16.090
 whether it's a set of rules 

00:08:13.479 --> 00:08:17.680
 applied conditions actions or 

00:08:16.090 --> 00:08:19.330
 knowledge about what is the impact of 

00:08:17.680 --> 00:08:21.220
 actions on the environment then 

00:08:19.330 --> 00:08:26.010
 comment on what the world is changing 

00:08:21.220 --> 00:08:28.120
 world is evolving and yet even a 

00:08:26.010 --> 00:08:29.620
 instill knowledge in an agent 

00:08:28.120 --> 00:08:32.260
 can ask a lot of work 

00:08:29.620 --> 00:08:33.760
 by cons what we could be tempted 

00:08:32.260 --> 00:08:35.770
 to do some agents that could 

00:08:33.760 --> 00:08:38.789
 actually learn this knowledge there 

00:08:35.770 --> 00:08:40.750
 and improved as one's life 

00:08:38.789 --> 00:08:43.630
 of his life people 

00:08:40.750 --> 00:08:45.970
 to improve his knowledge of the world 

00:08:43.630 --> 00:08:47.530
 is what we will see in the short is 

00:08:45.970 --> 00:08:48.580
 also algorithms that are going 

00:08:47.530 --> 00:08:50.440
 allow to do this type 

00:08:48.580 --> 00:08:51.910
 learning the algorithms 

00:08:50.440 --> 00:08:54.280
 learning that will allow a 

00:08:51.910 --> 00:08:56.950
 agent to improve and accumulate 

00:08:54.280 --> 00:08:59.760
 knowledge about his environment and 

00:08:56.950 --> 00:08:59.760
 on his actions 

