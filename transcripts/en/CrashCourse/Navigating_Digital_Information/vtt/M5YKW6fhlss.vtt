WEBVTT
Kind: captions
Language: en

00:00:00.089 --> 00:00:03.669
Hi, I’m John Green, and this is Crash Course: Navigating Digital Information.

00:00:03.669 --> 00:00:08.000
So we’re going to talk about your social
media feed today, but first: At the beginning

00:00:08.000 --> 00:00:12.230
of this series, I told you one of the two
jokes I know, and now that we’ve reached

00:00:12.230 --> 00:00:15.150
the last episode, I’d like to tell you the
other one.

00:00:15.150 --> 00:00:18.930
So a moth walks into a podiatrist’s office,
and the podiatrist says, “What seems to

00:00:18.930 --> 00:00:20.070
be the problem, moth?”

00:00:20.070 --> 00:00:21.949
And the moth answers, “Awww, doc.

00:00:21.949 --> 00:00:24.070
If only there were only one problem.

00:00:24.070 --> 00:00:27.310
I can’t hold down a job because I’m not
good at anything.

00:00:27.310 --> 00:00:29.650
My wife can hardly stand to look at me;

00:00:29.650 --> 00:00:31.279
we don’t even love each other anymore,

00:00:31.279 --> 00:00:35.380
worse than that, I can’t even remember if
we ever loved each other.

00:00:35.380 --> 00:00:37.930
When I look into the eyes of my children,

00:00:37.930 --> 00:00:43.120
All I see is the same emptiness and despair
that I feel in my own heart, doc.”

00:00:43.120 --> 00:00:45.489
And then the podiatrist says, “Whoa, moth.

00:00:45.489 --> 00:00:46.489
Okay.

00:00:46.489 --> 00:00:49.809
Those are very serious problems, but it seems
like you need to see a psychologist.

00:00:49.809 --> 00:00:51.730
I’m a podiatrist.

00:00:51.730 --> 00:00:53.890
What brought you here today?”

00:00:53.890 --> 00:00:56.039
And the moth says, “Oh.

00:00:56.039 --> 00:00:57.039
The light was on.”

00:00:57.039 --> 00:01:01.020
We humans like to think of ourselves as extremely
sophisticated animals.

00:01:01.020 --> 00:01:05.979
Like moths may fly toward the light, but humans
are endowed with free will.

00:01:05.979 --> 00:01:07.270
We make choices.

00:01:07.270 --> 00:01:11.209
Except a lot of the time, we just go where
the light is on.

00:01:11.209 --> 00:01:14.029
We do whatever feels like the natural thing.

00:01:14.029 --> 00:01:17.450
We get on facebook because other people are
on facebook.

00:01:17.450 --> 00:01:22.740
We scroll through posts because the architecture
of the site tells us to scroll.

00:01:22.740 --> 00:01:23.859
We become passive.

00:01:23.859 --> 00:01:27.960
In the past decade especially, social media
has fundamentally changed us.

00:01:27.960 --> 00:01:30.340
Like take your vocabulary, for example.

00:01:30.340 --> 00:01:35.270
Silicon Valley rivals Shakespeare in its prolific
additions to the English language.

00:01:35.270 --> 00:01:39.280
Friend, Google, and ‘gram are all verbs
now.

00:01:39.280 --> 00:01:41.350
Snap and handle have new definitions.

00:01:41.350 --> 00:01:44.200
Sliding into someone’s DMs is a thing.

00:01:44.200 --> 00:01:48.659
But it’s not just how we speak -- these
apps have not-so-subtly become embedded in

00:01:48.659 --> 00:01:51.759
our daily lives very quickly.

00:01:51.759 --> 00:01:54.229
Sometimes we don’t even realize how much
they impact us.

00:01:54.229 --> 00:01:59.119
They’ve changed our perceptions and expectations
of privacy and they’ve also helped to shape

00:01:59.119 --> 00:02:00.789
our offline experience.

00:02:00.789 --> 00:02:05.729
In 2016 for instance, Russian agents organized
political rallies all over the U.S. by creating

00:02:05.729 --> 00:02:12.510
fake Facebook pages for made-up grassroots
communities that then had real offline rallies.

00:02:12.510 --> 00:02:16.560
Just by posing as organizers against Donald
Trump or against Hillary Clinton, they actually

00:02:16.560 --> 00:02:22.480
got real people to show up in Florida, New
York, North Carolina, Washington, and Texas.

00:02:22.480 --> 00:02:25.400
And those rally-goers didn’t know that it
was a ruse.

00:02:25.400 --> 00:02:26.810
I find that scary.

00:02:26.810 --> 00:02:31.050
So today, for our big finale, we’re talking
about the great white whale of navigating

00:02:31.050 --> 00:02:33.780
online information: your social media feed.

00:02:33.780 --> 00:02:43.040
INTRO

00:02:43.040 --> 00:02:44.459
So quick note here at the start.

00:02:44.459 --> 00:02:47.370
I’m not currently using a bunch of social
media platforms.

00:02:47.370 --> 00:02:50.811
Which may mean that I’m no longer an expert
in them, but it’s only been six weeks and

00:02:50.811 --> 00:02:52.849
I don’t think anything has changed that
much.

00:02:52.849 --> 00:02:57.110
Also, it turns out that whether or not you
participate in Twitter is irrelevant to whether

00:02:57.110 --> 00:03:02.189
Twitter effects you life because what’s
shared online has offline consequences.

00:03:02.189 --> 00:03:06.709
Like online shouting matches about politics
can influence how we vote and also how we

00:03:06.709 --> 00:03:09.910
talk to our extended family at the Thanksgiving
dinner table.

00:03:09.910 --> 00:03:13.060
Unless you don’t live in the US or Canada
in which case I guess you don’t have Thanksgiving

00:03:13.060 --> 00:03:16.469
and presumably you never fight with your aunts
and uncles about politics.

00:03:16.469 --> 00:03:21.480
The way we interact in social media is shaping
all of our offline behaviors, from how we

00:03:21.480 --> 00:03:25.499
engage with IRL communities to how we consume
goods and services.

00:03:25.499 --> 00:03:29.590
That’s why there are so many people you
don’t know, and companies and organizations

00:03:29.590 --> 00:03:33.519
using social media to try to influence your
thoughts and actions.

00:03:33.519 --> 00:03:37.439
Sometimes those who want to influence you
use false identities like those with the Russian

00:03:37.440 --> 00:03:37.940
rallies.

00:03:37.940 --> 00:03:41.880
Sometimes, and more overtly, they buy your
attention with advertising.

00:03:41.890 --> 00:03:46.240
Some just create really engaging videos about
a kitten saved during a hurricane to steal

00:03:46.249 --> 00:03:47.249
your attention.

00:03:47.249 --> 00:03:51.180
Some of these actors have relatively benign
goals and act fairly, like a company sending

00:03:51.180 --> 00:03:55.109
ads into your feed for a Harry Potter mug
that it turns out you actually want because

00:03:55.109 --> 00:03:57.359
you are a Hufflepuff and you are proud!

00:03:57.359 --> 00:04:01.159
But others have terrible motives and spread
disinformation, like hoax news sites which

00:04:01.159 --> 00:04:02.780
are all run by Slytherins.

00:04:02.780 --> 00:04:04.790
Still others aren’t quite in either camp.

00:04:04.790 --> 00:04:08.439
They might unwittingly spread inaccurate information,
or misinformation.

00:04:08.439 --> 00:04:12.629
Like your aunt who always posts about Onion
articles like they’re actual news.

00:04:12.629 --> 00:04:17.620
Or me, on the several occasions when I have
failed to pause and laterally read before

00:04:17.620 --> 00:04:19.900
retweeting news that turned out to be false.

00:04:19.900 --> 00:04:25.620
The big problem with all of that is that 68%
of U.S. adults get news through some form

00:04:25.620 --> 00:04:30.419
of social media and nearly half of U.S. adults
get news through Facebook.

00:04:30.419 --> 00:04:34.330
And across the globe, people between 18 and
29 years old are more likely to get their

00:04:34.330 --> 00:04:36.610
news from social media than older adults.

00:04:36.610 --> 00:04:42.289
When we’re this reliant on a media ecosystem
full of pollution, we have to take responsibility

00:04:42.289 --> 00:04:47.560
for what we read, post and share and to do
that we should fully understand how social

00:04:47.560 --> 00:04:52.620
media networks really function including the
good stuff, and also the terrible stuff.

00:04:52.620 --> 00:04:53.630
First, the good side.

00:04:53.630 --> 00:04:57.610
For one thing, platforms like Facebook, Twitter
and Instagram allow us to share information

00:04:57.610 --> 00:05:00.840
and thoughts without the help of traditional
gatekeepers.

00:05:00.840 --> 00:05:06.340
Prior to social media it was really difficult
to have your voice heard in a large public

00:05:06.340 --> 00:05:06.840
forum.

00:05:06.840 --> 00:05:10.200
And because all the posts in our feeds look
more or less equal social media has allowed

00:05:10.200 --> 00:05:14.879
people to have voices in public discourse
who previously would have been silenced by

00:05:14.879 --> 00:05:15.879
power structures.

00:05:15.879 --> 00:05:16.879
That’s great!

00:05:16.879 --> 00:05:21.500
All tweets were created equal and everybody’s
faces look weird with that one square-jawed

00:05:21.500 --> 00:05:24.389
snapchat filter and we’re all in this together!

00:05:24.389 --> 00:05:28.280
Also, social media is great for making friends
and finding communities.

00:05:28.280 --> 00:05:33.169
We can organize ourselves into these little
affinity groups around special interests or

00:05:33.169 --> 00:05:37.389
organization, which makes communication much
easier than it was before.

00:05:37.389 --> 00:05:41.210
Like for example, what if a group of people
who want to get together and figure out how

00:05:41.210 --> 00:05:44.180
decrease overall the worldwide level of suck.

00:05:44.180 --> 00:05:48.380
Or, when I need to know what is eating my
tomatoes, I can go to a gardening facebook

00:05:48.380 --> 00:05:48.880
group.

00:05:48.880 --> 00:05:52.400
That example by the way is for old people
alienated by my previous mention of snapchat

00:05:52.400 --> 00:05:52.940
filters.

00:05:52.940 --> 00:05:58.419
That said there are plenty of problems with
social media from cyberbullying to catfishing

00:05:58.419 --> 00:06:03.789
to scams to massive disinformation campaigns
to people live tweeting shows you wanted to

00:06:03.789 --> 00:06:04.789
watch later.

00:06:04.789 --> 00:06:08.320
And if you’re going to live partly inside
these feeds I think it’s really important

00:06:08.320 --> 00:06:12.949
to understand both the kinds of information
that are likely to be shared with you and

00:06:12.949 --> 00:06:16.360
the kinds of information you’re incentivised
to share.

00:06:16.360 --> 00:06:18.100
Let’s start with targeted advertising.

00:06:18.100 --> 00:06:22.110
So you’re probably seeing an ad in this
corner.. possibly this one.

00:06:22.110 --> 00:06:25.770
I don’t have a great sense of direction
when I’m inside the feed.

00:06:25.770 --> 00:06:28.159
Or maybe you watched an ad before this video
played.

00:06:28.159 --> 00:06:32.430
Regardless, you may have noticed that something
you searched for recently has been advertised

00:06:32.430 --> 00:06:33.430
to you.

00:06:33.430 --> 00:06:37.580
Like for instance I’m trying to improve
my collection of vintage cameras for the background

00:06:37.580 --> 00:06:41.830
and suddenly all I see are advertisements
for vintage cameras.

00:06:41.830 --> 00:06:44.750
Social media companies make money by selling
advertisements.

00:06:44.750 --> 00:06:47.819
That’s why you get to use those platforms
for free.

00:06:47.819 --> 00:06:52.979
But these ads are very different from billboards
or ads in a local newspaper, because these

00:06:52.979 --> 00:06:58.570
ads were crafted just for you, or people like
you, based on what social media companies

00:06:58.570 --> 00:07:00.069
know about you.

00:07:00.069 --> 00:07:01.289
And they know a lot.

00:07:01.289 --> 00:07:05.300
They can learn your interests and habits based
on how you use their app, but they also track

00:07:05.300 --> 00:07:10.620
you elsewhere -- via other apps associated
with that company, or by using geolocation

00:07:10.620 --> 00:07:13.910
features to figure out where you physically
are.

00:07:13.910 --> 00:07:18.210
Social media companies take all that information
and present it to advertisers in one form

00:07:18.210 --> 00:07:23.199
or another so that those advertisers can target
their ads based on your interests and browsing

00:07:23.199 --> 00:07:27.300
history and location and age and gender and
much more.

00:07:27.300 --> 00:07:31.689
Can you protect your privacy and your feeds
from targeted advertising?

00:07:31.689 --> 00:07:32.689
Kind of.

00:07:32.689 --> 00:07:33.689
Sometimes.

00:07:33.689 --> 00:07:37.210
You can check your favorite apps and disable
data and location tracking where you can -- these

00:07:37.210 --> 00:07:41.970
features may fall under Ad Preferences or
Security or Privacy settings.

00:07:41.970 --> 00:07:46.570
Another potential downside to social media:
how algorithms organize our feeds.

00:07:46.570 --> 00:07:51.940
So algorithms are sets of rules or operations
a computer follows to complete a task.

00:07:51.940 --> 00:07:56.990
To put it very simply: social media sites
use what they know about your habits, they

00:07:56.990 --> 00:08:01.320
combine that with their knowledge of other
people and the things you’ve self-selected

00:08:01.320 --> 00:08:05.530
to follow, and funnel all that information
through an algorithm.

00:08:05.530 --> 00:08:09.720
And then the algorithm decides what to show
you in your newsfeed.

00:08:09.720 --> 00:08:14.539
Generally speaking, a newsfeed algorithm looks
for what you’re most likely to engage with,

00:08:14.539 --> 00:08:16.550
by liking or sharing it.

00:08:16.550 --> 00:08:21.930
Social media companies want you to stay engaged
with their app or site for as long as possible.

00:08:21.930 --> 00:08:27.050
So they show you stuff that you like so you
won’t leave so that they can sell more of

00:08:27.050 --> 00:08:28.050
your attention.

00:08:28.050 --> 00:08:33.080
And because the algorithms mostly show us
things we are likely to like and agree with

00:08:33.080 --> 00:08:38.710
we often find ourselves in so-called filter
bubbles, surrounded by voices we already know

00:08:38.710 --> 00:08:42.320
we agree with, and often unable to hear from
those we don’t.

00:08:42.320 --> 00:08:48.380
This also means that most newsfeed algorithms
are skewed toward engagement rather than truth.

00:08:48.380 --> 00:08:52.880
This is so often the case in fact that entire
businesses have been successfully run on posting

00:08:52.880 --> 00:08:55.970
engaging, but false, news stories.

00:08:55.970 --> 00:09:00.650
Many newsfeed algorithms favor outrageous
and emotional content, so companies looking

00:09:00.650 --> 00:09:04.760
to make money from clicks and advertisements
can use that to their advantage.

00:09:04.760 --> 00:09:10.200
Hundreds of websites were built on false viral
stories leading up to the 2016 U.S. election,

00:09:10.200 --> 00:09:14.840
and Buzzfeed later found out many were run
by teenagers in Macedonia.

00:09:14.840 --> 00:09:20.770
Valuing engagement over quality makes it harder
for users to distinguish between truth and

00:09:20.770 --> 00:09:21.440
fiction.

00:09:21.440 --> 00:09:26.660
Like humans tend to interpret information
in a way that matches our pre-existing beliefs.

00:09:26.660 --> 00:09:28.410
That’s called confirmation bias.

00:09:28.410 --> 00:09:34.290
But even if you did somehow manage to be completely
emotionally and ideologically neutral on a

00:09:34.290 --> 00:09:34.800
topic.

00:09:34.800 --> 00:09:40.500
Research has shown that if there’s information
you know is bogus, encountering it again and

00:09:40.500 --> 00:09:43.220
again means you might start to believe it.

00:09:43.220 --> 00:09:48.680
Warding off the negative effects of algorithmic
newsfeeds and filter bubbles is really hard.

00:09:48.680 --> 00:09:52.770
But I do think you can limit these effects
by A) following people and pages that have

00:09:52.770 --> 00:09:57.470
different viewpoints and perspectives than
you do, to add some variety to your feed.

00:09:57.470 --> 00:09:58.040
And B)

00:09:58.040 --> 00:10:02.180
looking for ways to turn off the “best”
or “top” posts features in your favorite

00:10:02.190 --> 00:10:06.840
social apps so that they display information
to you in a more neutral way.

00:10:06.840 --> 00:10:11.590
All of these negative features of social media
combine to create the feature that I personally

00:10:11.590 --> 00:10:15.830
worry about the most: extreme recommendation
engines.

00:10:15.830 --> 00:10:19.320
Social media algorithms show you more of what
you’ve already indicated you like.

00:10:19.320 --> 00:10:24.010
The way we use those apps tends to keep us
surrounded by information we’re primed to

00:10:24.010 --> 00:10:25.680
believe and agree with.

00:10:25.680 --> 00:10:29.930
And because engagement is the most important
thing, and we tend to engage with what most

00:10:29.930 --> 00:10:32.530
outrages, angers, and shocks us.

00:10:32.530 --> 00:10:38.540
The longer we hang out on some social media
apps and engage with outrageous content the

00:10:38.540 --> 00:10:43.220
more likely those apps are to push outrageous
content to us.

00:10:43.220 --> 00:10:47.360
Researchers have found that YouTube’s recommendation algorithms, for instance, consistently showed

00:10:47.360 --> 00:10:53.340
users more and more extreme, far-right channels
once they began watching political videos.

00:10:53.340 --> 00:10:55.340
They called it a radical rabbit hole.

00:10:55.340 --> 00:11:00.030
YouTube was lumping together outlets like
Fox News and the channels of Republican politicians

00:11:00.030 --> 00:11:04.180
with those of known far-right conspiracy theorists
and white nationalists.

00:11:04.180 --> 00:11:08.690
They also found that far-left channels have
smaller followings and were not nearly as

00:11:08.690 --> 00:11:10.840
visible via those same pathways.

00:11:10.840 --> 00:11:15.360
Now beginning in 2017, YouTube started to
update its algorithm to prioritize what they

00:11:15.360 --> 00:11:17.170
call “authoritativeness."

00:11:17.170 --> 00:11:19.790
In part to try to stop this from happening.

00:11:19.790 --> 00:11:23.920
But as previously noted, no algorithm is perfect
or objective.

00:11:23.920 --> 00:11:29.850
Ultimately, it’s on us as users not to fall
down these rabbit holes, not to go merely

00:11:29.850 --> 00:11:31.150
where the light is on.

00:11:31.150 --> 00:11:34.690
That’s why I think it’s so important to
follow accounts with differing viewpoints

00:11:34.690 --> 00:11:39.980
and to turn off data tracking if you can,
and in general to try to unwind the algorithmic

00:11:39.980 --> 00:11:42.240
web around your social media life.

00:11:42.240 --> 00:11:46.230
And while you’re in the feed it’s important
to remember to read laterally about sources

00:11:46.230 --> 00:11:47.360
you don’t recognize.

00:11:47.360 --> 00:11:50.220
And also take a break once in a while.

00:11:50.220 --> 00:11:52.240
Talk to actual people.

00:11:52.240 --> 00:11:53.680
Get some fresh air.

00:11:53.680 --> 00:11:55.730
I really think that’s valuable.

00:11:55.730 --> 00:11:59.530
But even though I personally had to leave
lots of the social Internet I do believe that

00:11:59.530 --> 00:12:04.550
social media can be an effective way to learn
about news and other information--if you’re

00:12:04.550 --> 00:12:05.910
able to protect yourself.

00:12:05.910 --> 00:12:07.740
Let’s try this in the Filter Bubble.

00:12:07.740 --> 00:12:12.260
Oh yeah, that looks about right.

00:12:12.260 --> 00:12:16.790
Yes, surrounded by everything I love and believe
in.

00:12:16.790 --> 00:12:18.740
Okay, that’s enough, let’s go to the Thought
Bubble.

00:12:18.740 --> 00:12:23.870
Okay, so your cousin DMed you a link headlined:
Singing Creek Park Sold, Will Be Home to Monster

00:12:23.870 --> 00:12:24.980
Truck Rally.

00:12:24.980 --> 00:12:25.660
Wow.

00:12:25.660 --> 00:12:28.300
That is your favorite park, so that is a huge
bummer.

00:12:28.300 --> 00:12:32.980
Your first instinct, of course, is to repost
it with an angry comment like “UGH we need

00:12:32.980 --> 00:12:35.330
nature WTH this is so unfair.”

00:12:35.330 --> 00:12:36.330
But wait, no.

00:12:36.330 --> 00:12:37.800
Take a deep breath and think.

00:12:37.800 --> 00:12:42.520
Your cousin is kind of a big deal -- he’s
Blue-check verified and everything.

00:12:42.520 --> 00:12:46.750
But blue checkmarks and verified profiles
do not denote truth.

00:12:46.750 --> 00:12:49.550
They just mean an account itself is who they
claim to be.

00:12:49.550 --> 00:12:50.550
So you click the link.

00:12:50.550 --> 00:12:55.300
It’s from a site called localnews.co, which
you’ve never heard of.

00:12:55.300 --> 00:12:57.250
And this is where your lateral reading kicks
in.

00:12:57.250 --> 00:12:59.810
Use a search engine to look up the name of
that site.

00:12:59.810 --> 00:13:04.410
Its Wikipedia entry reveals it’s a recently
founded independent news site for your area,

00:13:04.410 --> 00:13:08.220
but it’s a very short Wikipedia article
- not many reputable sources have written

00:13:08.220 --> 00:13:11.820
about the site to give us a better idea of
its perspective or authority.

00:13:11.820 --> 00:13:16.230
So you search for their claim instead: singing
creek park sale.

00:13:16.230 --> 00:13:18.610
The first result is that sketchy Local News
site.

00:13:18.610 --> 00:13:20.280
Let’s peruse the entire page.

00:13:20.280 --> 00:13:25.220
Ah, there you go -- the seventh result is
from a website you do know and trust, your

00:13:25.220 --> 00:13:29.730
local TV station and they say the park was
sold, but it’s actually going to be turned

00:13:29.730 --> 00:13:32.430
into a nonprofit wildflower preserve.

00:13:32.430 --> 00:13:34.850
Which you know what sounds pretty lovely.

00:13:34.850 --> 00:13:35.850
You could leave it at that.

00:13:35.850 --> 00:13:39.440
But as a good citizen of the internet, you
should correct this misinformation.

00:13:39.440 --> 00:13:41.670
Tell your cousin what’s up, they won’t
at all be defensive,

00:13:41.670 --> 00:13:45.850
ask them not to share it, and then post the
trustworthy article yourself.

00:13:45.850 --> 00:13:49.350
With the headline, “Condolences to monster
truck enthusiasts.”

00:13:49.350 --> 00:13:50.350
Mission accomplished.

00:13:50.350 --> 00:13:51.570
Thanks, Thought Bubble.

00:13:51.570 --> 00:13:56.180
So during this series we’ve talked a lot
about using lateral reading to check the source,

00:13:56.180 --> 00:14:00.910
look for authority and perspective, and then
check the claim and its evidence.

00:14:00.910 --> 00:14:04.840
With social media, a more flexible approach
is probably best.

00:14:04.840 --> 00:14:08.540
Like sometimes it makes sense to find out
who’s behind the account you’re seeing.

00:14:08.540 --> 00:14:11.630
Sometimes you should investigate the source
of what they’re sharing.

00:14:11.630 --> 00:14:15.380
Other times it’s best to evaluate the claim
being made.

00:14:15.380 --> 00:14:19.790
As you practice you’ll develop a better
idea of how to spend your time online.

00:14:19.790 --> 00:14:24.040
No matter where you begin, lateral reading
will help you get the information you’re

00:14:24.040 --> 00:14:25.040
looking for.

00:14:25.040 --> 00:14:28.680
When in doubt about anything you encounter
online you can challenge your source and your

00:14:28.680 --> 00:14:32.260
own assumptions and see what others people
have to say.

00:14:32.260 --> 00:14:37.390
And there’s one last thing I’d add: Be
suspicious of information that confirms your

00:14:37.390 --> 00:14:43.390
pre-existing worldview, especially stuff that
confirms that people you believe to be evil

00:14:43.390 --> 00:14:46.650
or stupid are evil or stupid.

00:14:46.650 --> 00:14:51.750
Read laterally not only when it comes to stuff
you don’t want to be true, but also when

00:14:51.750 --> 00:14:54.350
it comes to stuff you do want to be true.

00:14:54.350 --> 00:14:58.020
I know our current information environment
can be frustrating.

00:14:58.020 --> 00:14:59.990
Believe me, I am frustrated by it.

00:14:59.990 --> 00:15:04.700
It is really difficult to know where to look
for truth and accuracy, and I wish I could

00:15:04.700 --> 00:15:11.030
tell you there is one right way, one source
you can always rely upon, but the truth is,

00:15:11.030 --> 00:15:16.590
anyone who tells you that is selling you an
ideology or a product or both.

00:15:16.590 --> 00:15:21.740
But by making a habit of following up and
following through, we can be expert navigators

00:15:21.740 --> 00:15:27.160
of digital information, and maybe even go
to places where the lights are not on.

00:15:27.160 --> 00:15:30.231
Thanks so much for joining us for Crash Course:
Navigating Digital Information.

00:15:30.231 --> 00:15:34.720
And thanks to the Poynter Institute and the
Stanford History Education Group for making

00:15:34.720 --> 00:15:36.450
this series possible.

00:15:36.450 --> 00:15:38.240
MediaWise is supported by Google.

00:15:38.240 --> 00:15:42.141
If you’re interested in learning more about
MediaWise and fact-checking, a good place

00:15:42.141 --> 00:15:45.160
to start is @mediawise on Instagram.

00:15:45.160 --> 00:15:46.160
Thanks again for watching.

00:15:46.160 --> 00:15:47.850
Good luck out there in the wild west.

00:15:47.850 --> 00:15:50.560
And as they say in my hometown, “don’t
forget to be awesome."

