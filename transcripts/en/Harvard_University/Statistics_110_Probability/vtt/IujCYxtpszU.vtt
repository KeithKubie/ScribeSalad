WEBVTT
Kind: captions
Language: en

00:00:00.090 --> 00:00:02.860
So today is covariance day.

00:00:02.860 --> 00:00:05.900
Covariant, is a long
awaited moment that will

00:00:05.900 --> 00:00:08.350
let us finally deal with
the variance of a sum.

00:00:08.350 --> 00:00:12.110
For one thing, we said variance is
not linear, unlike expectation.

00:00:12.110 --> 00:00:16.010
That doesn't mean we don't need ways
to deal with the variance of a sum,

00:00:16.010 --> 00:00:20.590
it just means we need to think harder,
rather than falsely applying linearity.

00:00:20.590 --> 00:00:23.740
So on the one hand covariance is what we
need to deal with variance of the sum,

00:00:23.740 --> 00:00:26.640
on the other hand it's what we need when

00:00:26.640 --> 00:00:29.950
we wanna study two random
variables together instead of one.

00:00:29.950 --> 00:00:32.050
So it's like variance
except two of them and

00:00:32.050 --> 00:00:37.400
that's why it's called covariance,
so let's define it.

00:00:37.400 --> 00:00:39.370
Do some properties, do some examples.

00:00:40.490 --> 00:00:43.070
So at first start with the definition.

00:00:44.220 --> 00:00:48.360
It's analagous to how we define variance,
except now we have an X and a Y.

00:00:48.360 --> 00:00:50.260
Cuz we're looking at joint distributions.

00:00:50.260 --> 00:00:54.320
So we have X, we have Y,
we want their covariance.

00:00:54.320 --> 00:00:59.490
And we define it like this,
covariance of X and Y.

00:00:59.490 --> 00:01:02.808
X and Y are any two random
variables on the same space.

00:01:02.808 --> 00:01:08.467
Covariance X,Y equals expected

00:01:08.467 --> 00:01:13.315
value of X minus it's mean,

00:01:13.315 --> 00:01:17.366
times Y minus its mean.

00:01:23.414 --> 00:01:28.702
That's just the definitions, so you can't
really argue with it too much but let's

00:01:28.702 --> 00:01:34.230
stare at it intuitively for a bit and just
see where might this thing have come from?

00:01:34.230 --> 00:01:36.540
Why define it this way
instead of any other way?

00:01:38.300 --> 00:01:45.850
Well, first of all, it's a product,
something times something.

00:01:45.850 --> 00:01:48.930
So we've brought the X stuff and
the Y stuff together into one thing,

00:01:48.930 --> 00:01:51.410
cuz we're trying to see
how they vary together.

00:01:51.410 --> 00:01:53.640
And just, obviously,

00:01:53.640 --> 00:01:56.670
we all know that a positive number
times a positive number is positive,

00:01:56.670 --> 00:02:00.480
negative times negative is positive,
positive times negative is negative.

00:02:00.480 --> 00:02:04.627
So if it happens to be true that,

00:02:07.006 --> 00:02:11.790
This is X relative to its mean,
Y relative to its mean.

00:02:11.790 --> 00:02:16.540
So now imagine drawing a random sample,
suppose we had a lot of i.i.d

00:02:16.540 --> 00:02:21.530
pairs X, Y but, the pairs are i.i.d, but

00:02:21.530 --> 00:02:25.680
within each pair Xi, [INAUDIBLE] Yi,
they have some joint distribution.

00:02:25.680 --> 00:02:27.390
They may not be independent.

00:02:27.390 --> 00:02:30.750
By the way, we did show before
that if they're independent,

00:02:30.750 --> 00:02:33.490
then you can write this is just
E of this times E of this.

00:02:33.490 --> 00:02:38.340
So this is you know, we're interested in
what happens if they are not independent.

00:02:38.340 --> 00:02:41.480
Well if in that random sample we drew,

00:02:41.480 --> 00:02:46.360
if most of the time when X is above it's
mean, then also Y is above it's mean.

00:02:46.360 --> 00:02:49.210
Then you're getting
positive times positive.

00:02:49.210 --> 00:02:53.870
And if X is below it's mean,
tends to imply that Y is below it's mean,

00:02:53.870 --> 00:02:57.020
you get negative times
negative is positive.

00:02:58.090 --> 00:03:02.540
So if X being above it's mean

00:03:02.540 --> 00:03:06.310
tends to imply that Y is above it's mean,
and being below, being below.

00:03:06.310 --> 00:03:08.700
Then we would say that they're
positively correlated.

00:03:10.010 --> 00:03:10.810
And vice versus.

00:03:10.810 --> 00:03:13.410
It's be negatively coordinated
if X is above it's mean.

00:03:13.410 --> 00:03:15.610
It doesn't imply that Y
is below it's mean but

00:03:15.610 --> 00:03:18.050
it has more of a tendency
that Y is below it's mean.

00:03:18.050 --> 00:03:21.870
Then we would say that they're
negatively correlated.

00:03:21.870 --> 00:03:25.040
So this is just a measure of that.

00:03:25.040 --> 00:03:27.900
We'll actually define
correlation in a little while.

00:03:27.900 --> 00:03:32.410
But correlation is a very familiar term to
everyone cuz people talk about correlation

00:03:32.410 --> 00:03:33.000
all the time.

00:03:33.000 --> 00:03:35.500
But mathematically, what is correlation?

00:03:35.500 --> 00:03:37.110
It's defined in times of covariance.

00:03:37.110 --> 00:03:39.020
So we'll get to that soon.

00:03:39.020 --> 00:03:40.560
That's just the definition.

00:03:40.560 --> 00:03:46.610
But just like, you know how for variance,
we had two different ways to write it.

00:03:46.610 --> 00:03:49.110
We define variance as, notice

00:03:49.110 --> 00:03:53.830
the way we define variance was expect
the value of X to minus its mean squared.

00:03:53.830 --> 00:03:58.400
So, if we let X equal Y,
that is just the variance.

00:03:58.400 --> 00:04:03.620
So we've just proved the theorem already,
so I'll just call this properties.

00:04:03.620 --> 00:04:05.320
The first property to keep in mind

00:04:06.340 --> 00:04:10.570
is that covariance of X with
itself is the variance.

00:04:12.650 --> 00:04:15.900
Proof is just let X equal Y,
well that's the definition of variance.

00:04:17.260 --> 00:04:19.720
But that's a very useful
fact to keep in mind.

00:04:21.630 --> 00:04:23.902
And secondly, it's symmetric.

00:04:23.902 --> 00:04:30.476
Covariance X, Y equals covariance Y, X.

00:04:30.476 --> 00:04:35.285
And that's, again, something you can just
see immediately just swap the X and Y,

00:04:35.285 --> 00:04:40.540
but it's the same thing, so
it's immediately true that it's symmetric.

00:04:40.540 --> 00:04:41.890
That's also a useful fact.

00:04:45.290 --> 00:04:48.650
I don't even want to
group it into this list.

00:04:48.650 --> 00:04:53.190
Right here, what's the alternative
way to write covariance?

00:04:53.190 --> 00:04:54.670
This is completely analogous to how,

00:04:54.670 --> 00:05:00.360
where we defined variance as this thing,
this part squared without that part.

00:05:00.360 --> 00:05:03.876
But then we quickly showed that we could
also write it as E of X squared minus E of

00:05:03.876 --> 00:05:06.640
X squared,
you know parenthesize the other way.

00:05:06.640 --> 00:05:12.051
The analog of that formula
which is generalization,

00:05:12.051 --> 00:05:15.426
so this is E of XY minus E of X E of Y.

00:05:17.710 --> 00:05:19.940
So in general these two
things are not equal.

00:05:19.940 --> 00:05:24.410
We proved that they are equal if X and
Y are independent, but

00:05:24.410 --> 00:05:25.430
in general they're not equal.

00:05:27.420 --> 00:05:31.540
Notice that if we let X equal Y,
like in property one here,

00:05:31.540 --> 00:05:34.200
that's just E of X squared,
minus E of X squared the other way.

00:05:34.200 --> 00:05:36.540
So that is just a version of that formula.

00:05:37.790 --> 00:05:44.410
And the proof of this is just to
multiply this out and use linearity.

00:05:45.460 --> 00:05:49.680
We'll just quickly do that
over here just for practice.

00:05:49.680 --> 00:05:51.780
And we'll just have four
terms use linearity,

00:05:51.780 --> 00:05:53.550
so it should be very straightforward.

00:05:53.550 --> 00:05:56.500
We're doing this times this,
this times this, and so on.

00:05:56.500 --> 00:05:58.380
So we have E of X.

00:05:58.380 --> 00:06:01.070
I'm just gonna use linearity.

00:06:02.210 --> 00:06:04.330
The first term, X times Y, E of XY.

00:06:04.330 --> 00:06:10.370
And then minus, and then we do
this times this, but notice we're

00:06:10.370 --> 00:06:15.170
doing E of X times this, this thing is
a constant, you can take out the constant.

00:06:15.170 --> 00:06:22.950
So, that term would just be E of X, E of
Y, and then we have another cross term.

00:06:22.950 --> 00:06:24.940
This one times this one.

00:06:24.940 --> 00:06:27.680
E of X is just a constant, that comes out.

00:06:27.680 --> 00:06:32.605
So, that's minus another one that
looks the same, E of X E of Y, and

00:06:32.605 --> 00:06:35.710
then the last term is this times this.

00:06:35.710 --> 00:06:38.750
Again, that's just a constant,
E of a constant is a constant, so

00:06:38.750 --> 00:06:41.610
it's plus that thing again, and so

00:06:41.610 --> 00:06:47.260
that's all it is, minus 2 of them plus
1 of them, so it's the same thing.

00:06:47.260 --> 00:06:51.570
All right, so that's just an easy
application of linearity of expectation.

00:06:53.420 --> 00:06:59.140
So most of the time this way is
a little bit easier than this for

00:06:59.140 --> 00:07:00.950
computing covariance.

00:07:00.950 --> 00:07:02.410
But, like with variance,

00:07:02.410 --> 00:07:06.120
this one has a little bit more intuitive
appeal because it's just saying

00:07:06.120 --> 00:07:09.900
X relative to its mean Y relative to
its mean, but it's the same thing.

00:07:11.770 --> 00:07:14.420
So well we already have two properties,
well,

00:07:14.420 --> 00:07:17.200
let's get some more
properties of covariance.

00:07:17.200 --> 00:07:20.922
What if we have a covariance
of X with a constant?

00:07:25.773 --> 00:07:28.060
So I'm letting Y equal a constant C.

00:07:29.790 --> 00:07:32.540
So here Y is C.

00:07:32.540 --> 00:07:36.390
The expected value of constant C is C,
that's just 0.

00:07:36.390 --> 00:07:40.450
So it's immediately just 0 just from
the definition if C is a constant.

00:07:41.770 --> 00:07:46.060
Similarly by symmetry we could
have covariance of C with X.

00:07:46.060 --> 00:07:48.220
I just happened to write it on
this side but it's symmetric.

00:07:49.570 --> 00:07:51.220
So if C is a constant.

00:07:54.060 --> 00:07:58.000
Okay, now what if we multiplied by

00:07:58.000 --> 00:08:01.370
a constant instead of just
having a constant there?

00:08:01.370 --> 00:08:09.301
So if we have,
let's say the covariance of CX with Y.

00:08:09.301 --> 00:08:11.470
And let's just use this one.

00:08:11.470 --> 00:08:15.690
To compute this, all we have to
do is replace X by C times X.

00:08:15.690 --> 00:08:17.470
C comes out, C comes out.

00:08:17.470 --> 00:08:19.430
So C just comes out of the whole thing.

00:08:19.430 --> 00:08:20.820
So constants come out.

00:08:23.740 --> 00:08:26.470
Okay, so we just prove that just
by plugging in cX in for X,

00:08:26.470 --> 00:08:27.530
and then it's just immediate.

00:08:29.050 --> 00:08:31.341
Okay, again, c is any constant here.

00:08:31.341 --> 00:08:33.652
Similarly they could have constant here,
a constant here,

00:08:33.652 --> 00:08:35.020
and just take them both out.

00:08:35.020 --> 00:08:36.090
Very, very straightforward.

00:08:37.120 --> 00:08:41.008
All right, and now we want something
that looks kind of like linearity.

00:08:43.448 --> 00:08:47.957
What happens if we have
the covariance of x with y plus z?

00:08:51.945 --> 00:08:56.675
So if we take the covariance
of x with y plus z,

00:08:56.675 --> 00:09:03.590
then what that says to do is to
replace y by y plus z here, okay?

00:09:03.590 --> 00:09:07.650
And just as a quick
little scratch work for

00:09:07.650 --> 00:09:12.520
seeing what's going on,
I'm taking xy, replace y by y plus z.

00:09:12.520 --> 00:09:21.180
Well, of course, that's just xy plus xz,
and now we expect a value of that.

00:09:21.180 --> 00:09:24.560
So we use linearity, so
it's E of this plus E of that.

00:09:24.560 --> 00:09:31.700
Similarly, we replace this Y by Y+Z,
so again use linearity, E(Y) + E(Z).

00:09:31.700 --> 00:09:38.113
And so those terms you get are simply
the sum of the two covariances.

00:09:38.113 --> 00:09:43.397
So Cov(X,Y) + Cov(X,Z).

00:09:47.026 --> 00:09:50.490
Just write down the four terms you get and
you've just added the two covariances.

00:09:50.490 --> 00:09:53.670
So again, all of these things
are basically immediate.

00:09:53.670 --> 00:09:57.810
I'm not writing out long proofs for these
because all of these things are immediate

00:09:57.810 --> 00:10:00.160
from plugging into the definition.

00:10:00.160 --> 00:10:04.240
Either this definition or
this equivalent, plug into either one and

00:10:04.240 --> 00:10:07.380
use linearity of expectation and
all of these follow immediately.

00:10:09.270 --> 00:10:13.895
So these two together are especially
useful, and they're called,

00:10:13.895 --> 00:10:17.576
it's not linearity, but
it's called bilinearity.

00:10:20.459 --> 00:10:23.134
Bilinearity is just
a fancy term that means,

00:10:25.165 --> 00:10:28.651
If you imagine treating one
coordinate as just kind of fixed and

00:10:28.651 --> 00:10:33.610
you're working with the other coordinate,
it looks like linearity, right?

00:10:33.610 --> 00:10:38.860
So like here, notice the Y just stayed
as Y, and what happened to the cX?

00:10:38.860 --> 00:10:42.023
Well, I took out the constant
just like linearity.

00:10:42.023 --> 00:10:42.970
And what happened here?

00:10:45.450 --> 00:10:47.300
X just stayed x throughout.

00:10:47.300 --> 00:10:52.050
But if you just look at the y + z part,
we split it out into the y and a z.

00:10:52.050 --> 00:10:56.020
So it looks like linearity if you're
going one coordinate at a time.

00:10:56.020 --> 00:11:00.230
I just happened to write it this way, but
obviously, I could have done x + y, z and

00:11:00.230 --> 00:11:01.400
it would be analogous.

00:11:01.400 --> 00:11:03.190
I could have put the constant over there,
or

00:11:03.190 --> 00:11:06.960
a constant here, a constant there, okay.

00:11:06.960 --> 00:11:12.100
So those are really useful
properties that kind of

00:11:12.100 --> 00:11:17.210
if you use these properties, you can avoid
a lot of ugly calculations that is you can

00:11:17.210 --> 00:11:21.190
just like apply this rather than always
having to go back to the definition.

00:11:21.190 --> 00:11:23.560
Just like linearity is incredibly useful,

00:11:23.560 --> 00:11:28.210
bilinearity is incredible useful,
when working with covariances.

00:11:29.400 --> 00:11:34.627
So, and kind of an easy kind of
way to remember this is it kind

00:11:34.627 --> 00:11:39.542
of looks this distributive property,
here this is just

00:11:39.542 --> 00:11:44.902
the distributive property x times
y plus z is x, y plus x, z.

00:11:44.902 --> 00:11:47.502
It kind of looks like that
as if I'm doing code expect,

00:11:47.502 --> 00:11:50.500
it's not literally multiplication,
it's covariance, but

00:11:50.500 --> 00:11:53.520
I'm doing covariance of this and
this and this and this.

00:11:53.520 --> 00:11:58.564
Right, so if I wanted to extend that to
what happens if we have more of them.

00:12:02.307 --> 00:12:06.530
Let's say we had covariance of X plus Y.

00:12:06.530 --> 00:12:10.410
I mean this doesn't really need to be
listed separately but for practice,

00:12:10.410 --> 00:12:11.320
let's just do it.

00:12:13.300 --> 00:12:15.840
Just apply that property five repeatedly.

00:12:15.840 --> 00:12:18.700
And we're gonna get the covariance of
this and this, this and this, this and

00:12:18.700 --> 00:12:19.430
this, that.

00:12:19.430 --> 00:12:22.530
It's just like multiplying
two polynomials,

00:12:22.530 --> 00:12:24.250
or however you usually do that thing.

00:12:24.250 --> 00:12:32.573
So, So we can immediately just
write this down as four terms.

00:12:32.573 --> 00:12:39.260
Cov(X,Z) + Cov(X,W) + Cov(Y,Z) + Cov(Y,W).

00:12:39.260 --> 00:12:43.381
And that follows immediately just by
using that property 5 repeatedly.

00:12:46.177 --> 00:12:47.828
And more generally than that,

00:12:47.828 --> 00:12:52.970
let's just write what happens if we have
a covariance of one sum with another sum.

00:12:52.970 --> 00:12:55.060
I don't wanna write out nine terms,

00:12:55.060 --> 00:12:57.260
let's just write the general
thing once and for all.

00:12:57.260 --> 00:13:00.400
So we have a covariance
of one sum of terms.

00:13:00.400 --> 00:13:04.140
Let's say we have a sum over i of

00:13:06.740 --> 00:13:10.650
AiXi, where Ai is a constants.

00:13:10.650 --> 00:13:14.150
so this is linear combination
of random variables.

00:13:14.150 --> 00:13:16.110
And then, let's say i goes from 1 to m.

00:13:16.110 --> 00:13:22.310
And then we have another one,
let's say j=1 to n of bjYj.

00:13:22.310 --> 00:13:28.350
So, we want the covariance, so
it looks like this complicated thing.

00:13:28.350 --> 00:13:31.880
Okay, but as soon as you think about
what's the structure of the problem,

00:13:31.880 --> 00:13:35.380
it's just the covariance of
one sum with another sum.

00:13:35.380 --> 00:13:38.220
So if you apply that property
five over and over and

00:13:38.220 --> 00:13:40.760
over again,
we don't literally have to do that.

00:13:40.760 --> 00:13:44.490
But conceptually we're just using that
property over and over and over again, and

00:13:44.490 --> 00:13:46.340
just think about what you're gonna get.

00:13:46.340 --> 00:13:49.710
And also use property four
to take out the constants.

00:13:49.710 --> 00:13:52.420
Well, it just means you're
gonna get a sum over all ij

00:13:53.590 --> 00:13:56.150
of the covariance of individual terms,
right?

00:13:56.150 --> 00:13:56.830
Cuz it's just saying,

00:13:56.830 --> 00:14:00.780
you know, take one term here and
co-vary it with one term here.

00:14:00.780 --> 00:14:03.490
For all possible pairs.

00:14:03.490 --> 00:14:07.899
So the sum over all ij of

00:14:07.899 --> 00:14:12.550
aibj covariance Xi Yj.

00:14:12.550 --> 00:14:17.639
So that's just a very, I know this
looks complicated but it's no different

00:14:17.639 --> 00:14:22.663
from property five that just means we
used it a lot of times instead of once.

00:14:22.663 --> 00:14:26.967
So a lot of times it'd be easier to use
this kind of thing rather than going back

00:14:26.967 --> 00:14:31.870
to the definition and multiplying
everything out in terms of expectation.

00:14:31.870 --> 00:14:34.560
It's often easier to be able to
work directly with covariance.

00:14:36.350 --> 00:14:41.460
All right, so that shows us how,

00:14:41.460 --> 00:14:45.190
property one says how covariance
is related to variance, but

00:14:45.190 --> 00:14:49.600
it doesn't show us how it would be
useful in actually computing a variance,

00:14:50.790 --> 00:14:54.120
the variance of a sum that is.

00:14:54.120 --> 00:14:57.490
Okay, so one of the main reasons
we want covariance is so

00:14:57.490 --> 00:14:59.530
that we can deal with sums.

00:15:00.530 --> 00:15:03.530
So let's just work out
the variants of a sum.

00:15:05.840 --> 00:15:11.730
Let's say we have the variance of x1 + x2
to start with, but then we can generalize

00:15:11.730 --> 00:15:16.660
that to a sum of any number of terms
just by using this one repeatedly, okay?

00:15:16.660 --> 00:15:21.080
Well, we already know how to do this,
because by property 1,

00:15:21.080 --> 00:15:26.250
that's the covariance of (x1+x2),
with itself.

00:15:26.250 --> 00:15:30.649
But by property five, or
whichever property six,

00:15:30.649 --> 00:15:34.960
what's the covariance
of x1 + x2 with itself?

00:15:34.960 --> 00:15:36.380
Well, we just have those four terms.

00:15:36.380 --> 00:15:40.890
We have the covariance of x1 with
itself but that's just the variance.

00:15:40.890 --> 00:15:46.163
And we have covariance of x2 with itself,
that's just the variance of x2.

00:15:46.163 --> 00:15:51.152
And then we have two cross terms,
we have the covariance of x1 and x2.

00:15:51.152 --> 00:15:53.559
And we have the covariance of x2 and x1.

00:15:53.559 --> 00:15:56.010
But by the symmetry property,
those are the same thing.

00:15:56.010 --> 00:16:01.010
So it's simpler to just write it as
2 times the covariance of x1 and x2.

00:16:05.310 --> 00:16:09.496
In particular this says that
if the covariance is 0,

00:16:09.496 --> 00:16:14.120
then the variance of the sum
is the sum of the variances.

00:16:14.120 --> 00:16:16.770
And that's an if and only if statement.

00:16:16.770 --> 00:16:20.190
So one case were that's true
is if they're independent,

00:16:20.190 --> 00:16:24.390
we showed that before that if they're
independent then the covariance is 0.

00:16:24.390 --> 00:16:26.980
So if they're independent, this is gone.

00:16:26.980 --> 00:16:29.290
And we'll also see examples where
they're not independent, but

00:16:29.290 --> 00:16:32.370
this term is still zero and
so, so then it's true.

00:16:32.370 --> 00:16:35.770
Okay, but in general, you can't say
the variance of the sum of the sum

00:16:35.770 --> 00:16:38.358
of the variances,
because you have these covariance terms.

00:16:38.358 --> 00:16:39.729
Yeah, question?

00:16:39.729 --> 00:16:44.880
&gt;&gt; [INAUDIBLE]
&gt;&gt; That's if and only if the covariance

00:16:44.880 --> 00:16:51.030
is 0, that the variance of the sum
will be the sum of the variance.

00:16:51.030 --> 00:16:54.498
So let's write what would happen
if there's more than two of them,

00:16:54.498 --> 00:16:58.000
variance x1 + blah, blah, blah + xn.

00:16:59.560 --> 00:17:03.510
Just applying, so that's the covariance
of this sum with itself, so

00:17:03.510 --> 00:17:06.240
we can just apply this result.

00:17:07.270 --> 00:17:09.870
So again, it's gonna be the sum
of all the variances, and

00:17:09.870 --> 00:17:11.820
then we're gonna have
all these covariances.

00:17:14.090 --> 00:17:18.431
So add up all the variances, and
then add up all the covariances.

00:17:18.431 --> 00:17:23.307
And so, you're gonna have a covariance
of x1 and x2, x2 and x1, x1 and x3,

00:17:23.307 --> 00:17:25.229
x3 and x1, all those things.

00:17:25.229 --> 00:17:30.207
I think it's easiest if
we write it as 2 times

00:17:30.207 --> 00:17:35.185
the sum over i less than j covariance x i,
x j,

00:17:35.185 --> 00:17:38.870
it's easy to forget the 2 here.

00:17:38.870 --> 00:17:42.951
I could have also written it to
i not equal to j in this case I

00:17:42.951 --> 00:17:44.830
would have not put the 2.

00:17:44.830 --> 00:17:49.488
It's simply the question of
are you going to list cov(x1,x2)

00:17:49.488 --> 00:17:53.660
separately from cov(x2,
x1) or group them together.

00:17:53.660 --> 00:17:56.659
Seems a little simpler to
group them together, but

00:17:56.659 --> 00:17:58.900
then we need to remember to put the 2.

00:17:58.900 --> 00:18:03.660
Since I specified less than j,
then I have cov(x1, x2) listed here, but

00:18:03.660 --> 00:18:06.750
not cov(x2, x1), cuz I included that here.

00:18:07.860 --> 00:18:11.556
All right, so that's the general
way to get the variance of a sum,

00:18:11.556 --> 00:18:14.860
and we'll there are some examples
of that in a few minutes.

00:18:14.860 --> 00:18:19.162
First I just wanna make sure
that the connection with

00:18:19.162 --> 00:18:24.361
independence is clear and
we also need to define correlation.

00:18:24.361 --> 00:18:28.755
So theorem says that if x and

00:18:28.755 --> 00:18:36.766
y are independent,
Then they are uncorrelated.

00:18:39.383 --> 00:18:42.603
The definition of uncorrelated is
just that the covariance is 0.

00:18:42.603 --> 00:18:44.390
That's just definition.

00:18:46.110 --> 00:18:49.130
I.e, cov(x,y) = 0.

00:18:50.920 --> 00:18:59.640
And we actually proved this last time when
we just didn't have the terminology yet.

00:18:59.640 --> 00:19:04.060
At least we proved in the continuous case,
but the discreet case is analogous.

00:19:04.060 --> 00:19:08.310
So we proved this using
the 2 dimensional lotus

00:19:08.310 --> 00:19:12.320
thing that we did e of x times y,
all right?

00:19:12.320 --> 00:19:16.510
Equals e of x, e of y in the independent
case, so we showed that before.

00:19:18.330 --> 00:19:19.850
Converse is false.

00:19:23.630 --> 00:19:28.902
And that's a common mistake is
to show the covariance is 0, and

00:19:28.902 --> 00:19:34.650
then just leap to the conclusion
that they're independent.

00:19:34.650 --> 00:19:39.570
If the covariance is 0, and that's all we
know, they may or may not be independent.

00:19:41.260 --> 00:19:47.290
So just to give a simple counter example
showing why this doesn't imply this.

00:19:49.390 --> 00:19:54.340
Let's just consider an example
with normal random variables.

00:19:54.340 --> 00:20:01.160
So let's let z be standard normal,
and let n and we'll let x = z.

00:20:02.300 --> 00:20:07.430
Slightly redundant notation but
I'm just in the habit of using z for

00:20:07.430 --> 00:20:10.590
standard normals and y = z squared.

00:20:10.590 --> 00:20:16.140
So we're looking at a normal and
it's square, okay?

00:20:16.140 --> 00:20:22.095
So now let's compute the covariance for
this example.

00:20:22.095 --> 00:20:29.160
Cov(X, Y) = E(X, Y)- E(X)E(Y).

00:20:32.905 --> 00:20:38.841
In terms of Z,
that's E(Z cubed) -E(Z)E(Z squared),

00:20:38.841 --> 00:20:43.433
but both terms are just 0,
because we saw before

00:20:43.433 --> 00:20:48.060
that the odd moments of
a standard normal are 0.

00:20:48.060 --> 00:20:52.605
That's an odd moment and
that's an odd moment so it's just 0- 0.

00:20:52.605 --> 00:20:58.720
So they're uncorrelated, but
they're clearly not independent.

00:21:00.690 --> 00:21:06.070
In fact, they are very non-independent,
I should say, very dependent.

00:21:07.270 --> 00:21:09.060
Avoid too many double negatives.

00:21:10.240 --> 00:21:15.250
So they're very dependent, in fact, y is

00:21:15.250 --> 00:21:20.140
a function of x, so
that they're extremely dependent.

00:21:20.140 --> 00:21:23.620
If you know x, you know y,
complete information.

00:21:25.500 --> 00:21:27.450
So y is actually a function of x.

00:21:27.450 --> 00:21:32.600
Dependent just means there's
some information, right?

00:21:32.600 --> 00:21:33.970
It doesn't have to be
complete information.

00:21:33.970 --> 00:21:39.325
In this case, if we know x,
we have complete information about y,

00:21:39.325 --> 00:21:45.000
y is a function of x.

00:21:45.000 --> 00:21:49.030
And if we go the other way around,
if we know y, well, we don't know x, but

00:21:49.030 --> 00:21:51.830
we do know its magnitude, all right?

00:21:51.830 --> 00:21:54.520
If we know z squared,
then we can take the square root and

00:21:54.520 --> 00:21:57.679
we'll get the absolute value, so
we know it up to a plus or minus.

00:21:59.080 --> 00:22:03.370
So that also shows it's dependent going
the other, which we didn't need to do but

00:22:03.370 --> 00:22:04.750
it's just nice to think.

00:22:04.750 --> 00:22:08.290
If you know this, okay, we know this,
if we know this, then what do we know?

00:22:08.290 --> 00:22:09.800
Well, we know it up to a sign.

00:22:11.600 --> 00:22:17.607
So I would just say y also determines x,
at least it determines

00:22:17.607 --> 00:22:22.700
it up to a sign, so
it determines the magnitude of x.

00:22:26.592 --> 00:22:30.570
So, okay, so that's just an example
that shows the converse is false.

00:22:30.570 --> 00:22:36.360
But it's kind of a handy counter example
to keep in mind for a lot of things.

00:22:36.360 --> 00:22:39.670
So kind of intuitively
what's going wrong here,

00:22:39.670 --> 00:22:41.230
I mean, there's nothing wrong with this.

00:22:41.230 --> 00:22:46.050
But why the definition
doesn't capture this is part

00:22:46.050 --> 00:22:50.470
of the intuition of correlation is it's
kind of a measure of linear association.

00:22:50.470 --> 00:22:52.980
And those of you who
have taken Stat 100 or

00:22:52.980 --> 00:22:55.220
104 see a lot of things a lot like that.

00:22:55.220 --> 00:22:58.260
Where you actually have a data set,
and if it kind of looks like it's

00:22:58.260 --> 00:23:01.360
sloping upwards generally you
have this cloud of points.

00:23:01.360 --> 00:23:05.510
And as they kind of go upwards or
downwards, that kind of thing.

00:23:05.510 --> 00:23:08.040
It's measuring linear
trends in some sense.

00:23:09.190 --> 00:23:13.810
There's a theorem that we're not gonna
prove that says, if every function of x is

00:23:13.810 --> 00:23:17.830
uncorrelated every function of y,
then they're independent.

00:23:17.830 --> 00:23:21.310
But just having the linear things be

00:23:21.310 --> 00:23:24.290
uncorrelated is not enough
as this example shows.

00:23:26.420 --> 00:23:30.064
Okay, here they have this
quadratic relationship

00:23:30.064 --> 00:23:34.901
there is no linear relationship
that the kind of intuition on that.

00:23:34.901 --> 00:23:38.324
All right, so
let's also define correlation and

00:23:38.324 --> 00:23:43.133
then I will do some examples of how to
use this to compute the variance of

00:23:43.133 --> 00:23:46.819
the things that we did not
already know the variance.

00:23:48.230 --> 00:23:52.650
Okay, so once we have covariance, which
we do, correlation is easy to define.

00:23:52.650 --> 00:23:58.190
And I'll tell you some of the intuition
as well as what's the math.

00:23:59.200 --> 00:24:01.500
So here's the definition of correlation.

00:24:03.678 --> 00:24:07.095
You can think of it as just
a standardized version of covariance.

00:24:10.093 --> 00:24:16.653
So correlation, which you either write
as Cor, or usually I write it as Corr.

00:24:16.653 --> 00:24:20.821
Just because R's tend to look like V's
sometimes if you're writing too fast.

00:24:22.462 --> 00:24:28.190
Corr(X, Y), usually it's defined this way,

00:24:28.190 --> 00:24:34.830
as the covariance, and then we divide by
the product of the standard deviations.

00:24:37.688 --> 00:24:40.287
Remember, standard deviation's
just the square root of variance.

00:24:40.287 --> 00:24:42.192
So take the covariance,

00:24:42.192 --> 00:24:46.645
divide by the square root of
the product of the variances.

00:24:46.645 --> 00:24:49.020
But that's the usual definition.

00:24:49.020 --> 00:24:52.900
I actually would prefer to define it
a different way, and I'll show you why,

00:24:52.900 --> 00:24:54.020
that these are equivalent.

00:24:54.020 --> 00:25:00.791
I would prefer to define
it as the covariance of X,

00:25:00.791 --> 00:25:04.840
remember standardization?

00:25:04.840 --> 00:25:09.130
If we have any normal, we subtract the
mean, divide by the standard deviation,

00:25:09.130 --> 00:25:10.390
that gives us standard normal.

00:25:10.390 --> 00:25:11.630
So that's called standardization.

00:25:11.630 --> 00:25:14.537
Now here,
I'm not assuming anything is normal, but

00:25:14.537 --> 00:25:17.000
that the same standardization makes sense.

00:25:17.000 --> 00:25:22.062
That we take X, we subtract its mean,
we divide by its

00:25:22.062 --> 00:25:27.840
standard deviation, and
then we do the same thing with Y.

00:25:27.840 --> 00:25:34.090
So we've standardized both X and
Y, And we take their covariance.

00:25:34.090 --> 00:25:40.845
So correlation means standardize them
first, then take the covariance.

00:25:40.845 --> 00:25:44.128
The reason that this is a useful thing to

00:25:44.128 --> 00:25:48.708
do is that covariance kinda
has an annoying property,

00:25:48.708 --> 00:25:54.830
as far as interpretation in terms
of units and things like that.

00:25:54.830 --> 00:26:00.984
If you imagine X and Y are distances,
right, they're random variables but

00:26:00.984 --> 00:26:05.327
they're representing a distance quantity,
okay?

00:26:05.327 --> 00:26:09.292
And if you measured X and Y in nanometers,

00:26:09.292 --> 00:26:13.939
and then someone else
working on the same problem

00:26:13.939 --> 00:26:19.039
measures them in light years
instead of nanometers,

00:26:19.039 --> 00:26:24.510
you're gonna get extremely
different answers.

00:26:24.510 --> 00:26:29.246
So if I just tell you,
the covariance between my X and

00:26:29.246 --> 00:26:32.221
Y is 42, what does that tell you?

00:26:32.221 --> 00:26:37.119
You have to think really hard about
what are the units, what's going on,

00:26:37.119 --> 00:26:40.310
is 42 a big number or
a small number, right?

00:26:40.310 --> 00:26:43.007
I mean, it's the answer to life,
the universe and everything, but

00:26:43.007 --> 00:26:44.416
is it a big number or a small number?

00:26:44.416 --> 00:26:47.448
I don't know, because the units thing.

00:26:47.448 --> 00:26:49.525
This is a dimensionless quantity,

00:26:49.525 --> 00:26:52.660
dimensionless just
basically means unitless.

00:26:52.660 --> 00:26:56.900
So if X is measured in nanometers,
and you're subtracting off nanometers,

00:26:56.900 --> 00:26:58.270
that's still nanometers.

00:26:59.400 --> 00:27:03.570
Remember, that's why we define
standard deviation, also.

00:27:03.570 --> 00:27:05.370
Standard deviation has
a square root in it, so

00:27:05.370 --> 00:27:08.460
mathematically, it's pretty annoying
to deal with these square roots.

00:27:08.460 --> 00:27:10.470
Mathematically, it's nicer
to work with variance,

00:27:10.470 --> 00:27:14.660
but intuitively, the variance
would be in nanometers squared.

00:27:14.660 --> 00:27:17.690
Now we're back to nanometers,
divide nanometers by nanometers,

00:27:17.690 --> 00:27:20.000
we'll get a dimensionless quantity.

00:27:20.000 --> 00:27:23.020
So that's a major advantage of this.

00:27:23.020 --> 00:27:25.910
And I guess I should tell you briefly,
why is this thing the same as this?

00:27:25.910 --> 00:27:29.450
Well, you should kind of just
think about those properties,

00:27:29.450 --> 00:27:31.270
I'll just say this kind of quickly.

00:27:31.270 --> 00:27:36.110
First of all, subtracting the mean,
that's just adding a constant,

00:27:36.110 --> 00:27:39.430
that's not gonna affect
the covariance at all.

00:27:39.430 --> 00:27:43.937
So I could have left this out, but it's
just useful to think of standardizing.

00:27:43.937 --> 00:27:48.291
Cuz this standardization, what it does is
takes X, which could have any mean and

00:27:48.291 --> 00:27:51.390
any variance, and
makes it have mean 0 and variance 1.

00:27:51.390 --> 00:27:54.078
That's why it's called standardization.

00:27:54.078 --> 00:27:58.360
The part that's affecting what's
going on is the standard deviation.

00:27:58.360 --> 00:28:00.241
But from one of those
properties that we wrote,

00:28:00.241 --> 00:28:03.076
we can just pull out the standard
deviations, and we get exactly that.

00:28:03.076 --> 00:28:04.861
So they're exactly the same thing,

00:28:04.861 --> 00:28:07.980
I just think this one's a little
more intuitive to think about.

00:28:09.180 --> 00:28:11.926
Okay, so
one quick theorem about correlation.

00:28:15.505 --> 00:28:17.480
Correlation can never equal 42.

00:28:17.480 --> 00:28:22.212
More generally,
correlation is always between -1 and 1.

00:28:31.590 --> 00:28:35.314
So not only is it something more
interpretable in the sense that it doesn't

00:28:35.314 --> 00:28:37.336
depend on what system of units you used.

00:28:37.336 --> 00:28:42.096
It's also more interpretable in that,
if I say a correlation is 0.9?

00:28:42.096 --> 00:28:44.717
That's a pretty high correlation,

00:28:44.717 --> 00:28:50.280
cuz I know the largest it can be is 1,
okay, so that's very useful.

00:28:50.280 --> 00:28:53.845
And kind of an interesting fact about this
inequality is that it's essentially just

00:28:53.845 --> 00:28:54.755
Cauchy-Schwartz.

00:28:54.755 --> 00:28:59.586
For those of you who have seen
the Cauchy-Schwarz inequality in linear

00:28:59.586 --> 00:29:01.207
algebra or elsewhere.

00:29:01.207 --> 00:29:04.855
The Cauchy-Schwarz is one of the most
important inequalities in all of

00:29:04.855 --> 00:29:05.657
mathematics.

00:29:05.657 --> 00:29:09.995
And if you put this, if you rewrite this
statement in a linear algebra setting,

00:29:09.995 --> 00:29:13.080
you can show that it's
essentially Cauchy-Schwarz.

00:29:13.080 --> 00:29:15.960
If you haven't seen Cauchy-Schwartz yet,
we'll come back

00:29:15.960 --> 00:29:18.490
to it later in the semester, and you
don't need to worry about it right now.

00:29:18.490 --> 00:29:22.571
But for those of you who have,
I wanted to make the connection right now.

00:29:22.571 --> 00:29:24.390
So let's prove this fact.

00:29:26.690 --> 00:29:30.034
So one proof would just be to put it
into the Cauchy-Schwarz framework, and

00:29:30.034 --> 00:29:31.195
apply Cauchy-Schwarz.

00:29:31.195 --> 00:29:35.296
But that doesn't really show
what's going on, first of all.

00:29:35.296 --> 00:29:38.910
And secondly, that assumes you're
familiar with Cauchy-Schwarz.

00:29:38.910 --> 00:29:40.689
So let's just prove it directly.

00:29:43.249 --> 00:29:49.606
So, first of all, Math classes,

00:29:49.606 --> 00:29:54.254
you'll often see the acronym WLOG,
Without Loss of Generality.

00:29:54.254 --> 00:30:00.154
We're going to assume X and
Y are already standardized.

00:30:03.985 --> 00:30:08.180
If they're not already standardized, so
we're trying to prove this inequality.

00:30:08.180 --> 00:30:13.270
We may as well just assume from the start
that they've been standardized,

00:30:13.270 --> 00:30:16.110
standardized meaning that
they have mean 0, variance 1.

00:30:16.110 --> 00:30:18.010
Because if they weren't standardized,
well,

00:30:18.010 --> 00:30:23.140
I could just make up some new notation, x
tilde, y tilde for the standardized ones.

00:30:23.140 --> 00:30:26.520
But this says that the correlation
will be the same anyway, so

00:30:26.520 --> 00:30:28.840
we may as well assume that
they're already standardized.

00:30:28.840 --> 00:30:32.600
All right, so
now let's just compute the variance.

00:30:32.600 --> 00:30:41.149
This is actually good practice
with property seven there.

00:30:41.149 --> 00:30:44.848
Let's compute Var(X + Y).

00:30:44.848 --> 00:30:51.576
Well, that's Var(X) + Var(Y) + 2 Cov(X,
Y).

00:30:51.576 --> 00:30:56.778
And for some reasons,
statisticians often like to call

00:30:56.778 --> 00:31:01.544
the correlation rho, so
I'll follow that trend.

00:31:01.544 --> 00:31:09.119
Corr(X, Y), that's just notation,
let's just name that rho.

00:31:09.119 --> 00:31:13.557
All right, so that's the variance, but
I assumed they were standardized, so

00:31:13.557 --> 00:31:14.495
this is 1 + 1.

00:31:14.495 --> 00:31:18.704
And if they're standardized already,
then the covariance is the correlation,

00:31:18.704 --> 00:31:20.480
because they're standardized.

00:31:20.480 --> 00:31:27.350
So that's just 1 + 1 + 2 rho, so
that's really just 2 + 2 rho, right?

00:31:29.040 --> 00:31:34.030
On the other hand, we could look
at the variance of the difference.

00:31:34.030 --> 00:31:39.042
Again, that's good practice with
variances of sums and differences.

00:31:39.042 --> 00:31:43.221
A common mistake is to say,
that's Var(X)- Var(Y).

00:31:43.221 --> 00:31:46.290
Which we talked about that fact before
when we were talking about sums and

00:31:46.290 --> 00:31:49.550
differences of normals,
variances can't be negative.

00:31:49.550 --> 00:31:56.192
So think of this not as X- Y,
think of this as X + -Y,

00:31:56.192 --> 00:32:01.380
so it still adds, Var(X)- Var(Y).

00:32:01.380 --> 00:32:02.540
Now we subtract.

00:32:04.610 --> 00:32:07.430
Just check this, right,
is the covariance of x-y with itself?

00:32:07.430 --> 00:32:12.585
So we have a- on the covariance part,
but not on these variance terms.

00:32:14.954 --> 00:32:16.840
So that's just 2- 2 rho.

00:32:19.980 --> 00:32:22.948
Okay, well we're running
out of space on this board,

00:32:22.948 --> 00:32:27.218
and that's actually the end of the proof
because variance is non-negative.

00:32:28.854 --> 00:32:33.606
So these two inequalities say
that rho is between- 1 and 1.

00:32:36.705 --> 00:32:41.914
All right, so that shows a correlation
is always between- 1 and 1.

00:32:44.360 --> 00:32:49.070
And so in general it is easier to work
with covariances than correlations but

00:32:49.070 --> 00:32:51.940
correlations are more intuitive and

00:32:51.940 --> 00:32:55.575
standardized with everything
between- one and one.

00:32:55.575 --> 00:33:01.840
Okay, so I wanted to for
the rest of the time do some examples with

00:33:03.220 --> 00:33:07.750
with this thing, and
also with computing covariances for

00:33:07.750 --> 00:33:10.840
certain problems we
might be interested in.

00:33:10.840 --> 00:33:14.719
So let's talk about the multinomial cuz
we were talking about that last time.

00:33:14.719 --> 00:33:19.184
And now we actually have the tools
to deal with the covariances,

00:33:19.184 --> 00:33:22.030
within a multinomial, okay?

00:33:22.030 --> 00:33:24.010
So, this is just an example.

00:33:24.010 --> 00:33:27.190
But it's an important example,
cuz multinomials come up a lot.

00:33:27.190 --> 00:33:32.240
So we wanna compute covariances,
if we have a multinomial, okay?

00:33:32.240 --> 00:33:35.630
So covariances in a multinomial.

00:33:37.560 --> 00:33:39.780
That is,
this multinomial is this vector, right?

00:33:39.780 --> 00:33:41.720
It's about how many people
are in category one,

00:33:41.720 --> 00:33:43.621
how many people are in category two,
and so on.

00:33:43.621 --> 00:33:47.589
So you can take any two of those counts
of how many people are in category one,

00:33:47.589 --> 00:33:49.759
how many people are in category five, and

00:33:49.759 --> 00:33:52.378
compute the covariance
of those things,right?

00:33:52.378 --> 00:33:54.020
That's a very natural thing to look at.

00:33:56.690 --> 00:34:02.370
And I actually know four or
five ways to derive this,

00:34:02.370 --> 00:34:06.670
and I really like this example, so I will
probably come to this later with some of

00:34:06.670 --> 00:34:10.990
the other methods, but for
now let's just do one method, okay?

00:34:10.990 --> 00:34:15.590
So, we have this multinomial,
so we have this vector,

00:34:15.590 --> 00:34:19.650
using the notation from last time,
we have k different categories.

00:34:19.650 --> 00:34:23.950
And Xj is the number of people or
objects in the jth category.

00:34:25.470 --> 00:34:31.190
And this is multinomial, n,
that there are n objects or people.

00:34:31.190 --> 00:34:35.670
And the probabilities are given
back by some vector P.

00:34:35.670 --> 00:34:40.745
That's just gives the probability for
each category, okay?

00:34:40.745 --> 00:34:44.999
And we wanna find
the covariance of Xi with Xj,

00:34:50.027 --> 00:34:53.827
For all i and j, Right?

00:34:56.210 --> 00:35:00.229
So first of all let's
consider the case I = j.

00:35:02.437 --> 00:35:06.260
Then we just have a covariance
of Xi with itself.

00:35:06.260 --> 00:35:09.460
And we know that's just a variance of Xi.

00:35:09.460 --> 00:35:14.488
And last time we talked about
the fact that if we define

00:35:14.488 --> 00:35:20.297
success to be being in category i,
we just have a binomial, so

00:35:20.297 --> 00:35:26.473
for this, we just use the variance
of the binomial, npi 1- pi.

00:35:28.437 --> 00:35:35.396
So that's easy, the more interesting part
is what happens if i is not equal to j.

00:35:40.595 --> 00:35:41.984
Okay, now if we,

00:35:41.984 --> 00:35:47.557
I think it's easier to just think
concretely in terms of the first two.

00:35:47.557 --> 00:35:50.746
So let's just find covariance of X1 and
X2.

00:35:54.010 --> 00:35:58.330
If we know how to do this,
we could always just relabel things and

00:35:58.330 --> 00:36:00.900
get X5 and X12 or whatever we want.

00:36:00.900 --> 00:36:04.090
But it's just easier to think
concretely in terms of X1 and

00:36:04.090 --> 00:36:06.290
X2, rather than having so
much notation going around.

00:36:07.880 --> 00:36:11.730
Okay, so I'll find this one first.

00:36:11.730 --> 00:36:13.910
And there's a lot of ways to do this.

00:36:13.910 --> 00:36:16.280
Let's just think about
it intuitively first.

00:36:18.720 --> 00:36:21.385
Intuitively, you think this is positive,
negative or zero?

00:36:21.385 --> 00:36:26.264
&gt;&gt; [INAUDIBLE]
&gt;&gt; Negative, why?

00:36:26.264 --> 00:36:29.152
&gt;&gt; [INAUDIBLE]
&gt;&gt; Exactly, so

00:36:29.152 --> 00:36:33.170
if you somehow computed this
an you got a positive number,

00:36:33.170 --> 00:36:38.091
you shouldn't just be happy you're
done with the problem and move on.

00:36:38.091 --> 00:36:40.960
You should stop and think,
does a positive number make sense here?

00:36:40.960 --> 00:36:45.860
As you just said, if you knew that there
were more people in the first category,

00:36:45.860 --> 00:36:48.470
like there's tons of people
in the first category,

00:36:48.470 --> 00:36:51.500
there's fewer people left over who
could be in the second category.

00:36:51.500 --> 00:36:56.150
So It's like these categories are kind
of competing for membership, right?

00:36:56.150 --> 00:36:59.171
You have a fixed number of people,
not like the chicken and

00:36:59.171 --> 00:37:01.835
egg problem we had a Poisson
number of eggs, okay.

00:37:01.835 --> 00:37:05.371
So fixed number of eggs competing for
different categories,

00:37:05.371 --> 00:37:08.750
more in one then you'd expect
less in the other, right?

00:37:08.750 --> 00:37:11.017
So they should be negatively correlated,
all right?

00:37:11.017 --> 00:37:15.918
So now how do we do this,
well there's a bunch of ways as I said,

00:37:15.918 --> 00:37:18.877
but one way that I especially like is,

00:37:18.877 --> 00:37:23.595
to relate this back to stuff we
did last time, we talked about

00:37:23.595 --> 00:37:29.290
the lumping property of the multi
nominal try to relate it to this.

00:37:29.290 --> 00:37:32.820
Normally, you'd think of this as
a way to find the variance of a sum.

00:37:32.820 --> 00:37:37.310
But if we know this, this and this,
then obviously we know this also.

00:37:37.310 --> 00:37:39.994
So let's actually do it this way and

00:37:39.994 --> 00:37:44.744
I'll probably do some of the other
methods later, not today.

00:37:44.744 --> 00:37:50.101
So we have,
Let's take the variance of the sum,

00:37:55.685 --> 00:37:56.780
Let's call this thing C.

00:37:59.730 --> 00:38:03.160
Just to have some notation, so
we're trying to solve for C, okay.

00:38:03.160 --> 00:38:07.365
So at the variance of the sum
equals the sum of the variances.

00:38:07.365 --> 00:38:11.323
Now the variance of X1 is nP1 (1- P1) and

00:38:11.323 --> 00:38:15.992
the variance of X2 is nP2 (1- P2) and
then plus,

00:38:15.992 --> 00:38:20.763
twice the covariance but
I just named the covariance C,

00:38:20.763 --> 00:38:25.050
just to have a simple name for
it, so it's + 2C.

00:38:26.190 --> 00:38:28.680
So the only thing,
we wanna solve for this.

00:38:28.680 --> 00:38:31.770
The only thing left that
we haven't gotten is this.

00:38:31.770 --> 00:38:36.327
But then variance of X1 + X2 follows
immediately from what I was talking about

00:38:36.327 --> 00:38:38.513
last time with the lumping property.

00:38:38.513 --> 00:38:43.292
This just says merge the first two
categories together into one bigger

00:38:43.292 --> 00:38:44.590
category, okay?

00:38:44.590 --> 00:38:47.230
If we do that, It's still binomial, right?

00:38:47.230 --> 00:38:54.320
Now we're defining success to mean being
a member of category one or category two.

00:38:54.320 --> 00:38:59.430
Still binomial, so we can
immediately right down, thus well n.

00:38:59.430 --> 00:39:04.722
Now the probability of
success is (P1 + P2)(1- P1 +

00:39:04.722 --> 00:39:12.400
P2) So now we know everything
in this equation except C.

00:39:12.400 --> 00:39:15.955
To solve for C,
multiply things out, factor it,

00:39:15.955 --> 00:39:20.713
however you want just do the algebra,
easy algebra at this point.

00:39:20.713 --> 00:39:24.673
So I'm not gonna show
you like write that or

00:39:24.673 --> 00:39:30.833
multiply this times this and
just multiply it out, simplify it and

00:39:30.833 --> 00:39:36.665
what you'll get is the covariance
of X1 and X2 = -nP1 P2.

00:39:36.665 --> 00:39:44.600
And so in general, That was just for
X1 and X2, just for concreteness.

00:39:44.600 --> 00:39:53.372
The general result would be
the covariance of Xi Xj =- nPi Pj for

00:39:53.372 --> 00:40:00.360
i not equal j Notice it is a- number.

00:40:02.730 --> 00:40:07.341
Okay, so
That's the covariance in a multinomial.

00:40:07.341 --> 00:40:12.315
And, Let's do

00:40:12.315 --> 00:40:16.628
a few variants examples now.

00:40:16.628 --> 00:40:20.345
For example, variants of the binomial,
we did derive the variants of

00:40:20.345 --> 00:40:24.265
the binomial before using indicator
random variables, just directly.

00:40:24.265 --> 00:40:27.126
Because we didn't have these
tools available yet, okay?

00:40:27.126 --> 00:40:31.436
So let's redo the variance
of the binomial and

00:40:31.436 --> 00:40:34.871
then do one more example after that.

00:40:34.871 --> 00:40:39.056
So okay, so
the variance of the binomial NP is NPQ.

00:40:39.056 --> 00:40:43.165
And let's just derive that really quickly,

00:40:45.614 --> 00:40:49.468
So let X be binomial np, and

00:40:49.468 --> 00:40:54.836
we write it as we've done many times.

00:40:54.836 --> 00:40:59.764
X = x1 + blah, blah, blah plus Xn,

00:40:59.764 --> 00:41:04.232
where the Xis are IID Bernoulli P.

00:41:08.004 --> 00:41:12.873
Now each Xi, let's do a quick little

00:41:12.873 --> 00:41:17.749
indicator random variable review.

00:41:17.749 --> 00:41:20.263
We can think of these Xj's,
they're Bernoulli's, but

00:41:20.263 --> 00:41:22.222
they're also indicator random variables.

00:41:22.222 --> 00:41:28.097
It's the indicator of
success on the jth trial,

00:41:28.097 --> 00:41:32.691
solet's just state this in general.

00:41:32.691 --> 00:41:36.569
Let's let capital I and capital J,

00:41:36.569 --> 00:41:43.814
let IA be indicator random variable for
event A, just in general.

00:41:43.814 --> 00:41:47.881
A is any event, IA is it's indicator

00:41:47.881 --> 00:41:52.749
random variable, indicator rv of event A.

00:41:52.749 --> 00:42:00.226
Okay, so just a couple quick simple
facts about indicator random variables.

00:42:00.226 --> 00:42:05.145
What's IA squared?

00:42:05.145 --> 00:42:09.246
It's just IA, cuz you're squaring 0 or 1.

00:42:09.246 --> 00:42:14.140
Similarly IA cubed = IA and
you can generalise this to

00:42:14.140 --> 00:42:18.396
other powers if you want,
just it's 0 or 1.

00:42:18.396 --> 00:42:20.956
There's a very, very,
very simple fact, but

00:42:20.956 --> 00:42:24.740
I've seen it get overlooked many times,
so I'm emphasizing it now.

00:42:24.740 --> 00:42:30.991
Pick any positive power, nothing happens
because it's 0 or 1, very easy, okay?

00:42:30.991 --> 00:42:35.516
Now let's look at something else
IA times I times B where A and

00:42:35.516 --> 00:42:37.131
B are both of events.

00:42:38.789 --> 00:42:43.443
How would you write that as
one indicator random variable?

00:42:43.443 --> 00:42:50.580
Intersection, extremely useful simple
fact, but often gets overlooked.

00:42:50.580 --> 00:42:55.640
Product of these indicators is 0 or
1 times 0 or 1, that's gonna be 0 or 1.

00:42:55.640 --> 00:42:56.636
It's gonna be 1,

00:42:56.636 --> 00:43:00.695
if and only if both of those are 1,
that's the definition of intersection.

00:43:00.695 --> 00:43:04.057
So that's immediately true,
very useful fact.

00:43:04.057 --> 00:43:10.655
Okay, now coming back to this binomial,
if we want the variance of Xj,

00:43:14.241 --> 00:43:21.015
That's just E of xj
squared-E of xj squared, But

00:43:21.015 --> 00:43:27.102
xj squared is xj, so that's just E of xj,
and we know E of xj is p, for Bernoulli P.

00:43:27.102 --> 00:43:31.172
This one is p squared, so
that's just p1-p, okay?

00:43:31.172 --> 00:43:35.760
So it's extremely easy to get
the variance of a Bernoulli.

00:43:35.760 --> 00:43:41.576
If we define this, let's define this as q
then we're just saying p times q, okay?

00:43:41.576 --> 00:43:46.385
So Bernoulli P,
you get p times q, very easy.

00:43:46.385 --> 00:43:55.115
So now we want the variance of
the binomial, Well it's just npq, done.

00:43:55.115 --> 00:43:58.315
Because you're adding up, and
they're independent for the binomial.

00:43:58.315 --> 00:44:03.811
We have independent Bernoulli trials,
so just to write out a little bit more.

00:44:03.811 --> 00:44:10.498
Covariance of Xi, Xj = 0 for i not
equal j because they are independent.

00:44:10.498 --> 00:44:12.927
They are not only uncorrelated
they are independent, so

00:44:12.927 --> 00:44:14.504
we don't have any covariance term.

00:44:14.504 --> 00:44:20.520
So we just add up the variances and
n times this, npq, all right?

00:44:20.520 --> 00:44:23.944
So now you can do the variance
of a binomial in your head.

00:44:23.944 --> 00:44:25.889
You don't need to memorize this,

00:44:25.889 --> 00:44:29.793
it's just n times the variance of
one of these Bernoulli's, okay?

00:44:29.793 --> 00:44:36.758
So that's easy, Well let's talk
about a more complicated one though.

00:44:36.758 --> 00:44:42.134
Hypergeometric, So

00:44:42.134 --> 00:44:49.858
let X be hypergeometric,
With parameters w,

00:44:49.858 --> 00:44:54.536
b, n, which we interpret as saying,

00:44:54.536 --> 00:45:00.840
we have a jar that has w white balls,
b black balls.

00:45:00.840 --> 00:45:02.506
We take a sample of size n, and

00:45:02.506 --> 00:45:06.326
we want the distribution of the number
of white balls in the sample.

00:45:06.326 --> 00:45:13.589
Well again, we can decompose it in
terms of indicator random variables.

00:45:18.594 --> 00:45:23.998
So Xj = 1,
we can interpret this as drawing balls

00:45:23.998 --> 00:45:29.288
from that jar one at a time
without replacement.

00:45:29.288 --> 00:45:31.420
We'd get a binomial if we
did with replacement, but

00:45:31.420 --> 00:45:34.320
the hypergeometric would
be without replacement.

00:45:34.320 --> 00:45:37.524
Take the balls one at a time, and

00:45:37.524 --> 00:45:43.001
we just say one if the jth ball is white,
0 otherwise.

00:45:43.001 --> 00:45:47.802
The problem, the reason it's more
difficult than this is that these

00:45:47.802 --> 00:45:53.623
are dependant indicator random variables,
because it's without replacement.

00:45:57.032 --> 00:46:01.610
So if we write this thing out,
variance of x = so

00:46:01.610 --> 00:46:06.530
we're gonna write out all
these variance terms and

00:46:06.530 --> 00:46:09.633
all of these covariance terms.

00:46:09.633 --> 00:46:11.743
Sounds like it's gonna be a nightmare,
okay?

00:46:11.743 --> 00:46:15.710
But there are some symmetries
that we can take advantage of.

00:46:15.710 --> 00:46:19.821
First of all,
we have the sum of all the variances,

00:46:19.821 --> 00:46:24.612
we're gonna use some symmetries
here to make life easier.

00:46:24.612 --> 00:46:29.000
This goes back to our homework problem,
I'll talk a little bit more about.

00:46:29.000 --> 00:46:33.699
Variance of x = n times
the variance of x1,

00:46:33.699 --> 00:46:38.271
because let's say we're looking at x7.

00:46:38.271 --> 00:46:40.357
Let's say the seventh ball,

00:46:40.357 --> 00:46:44.202
like the homework problem
where you picked two balls.

00:46:44.202 --> 00:46:48.110
And a lot of students were struggling
somewhat with the fact that to consider

00:46:48.110 --> 00:46:51.661
the second ball, don't you have
to consider the first ball, okay?

00:46:51.661 --> 00:46:55.601
But when we're just looking at like x7,
that depends on the seventh ball,

00:46:55.601 --> 00:46:58.507
we're imagining before
we've done anything, okay?

00:46:58.507 --> 00:47:04.490
Now the seventh ball is equally
likely to be any of the balls, right?

00:47:04.490 --> 00:47:07.372
There's isn't like some balls
like to be chosen seventh and

00:47:07.372 --> 00:47:08.736
other ones don't, right?

00:47:08.736 --> 00:47:13.210
It's completely symmetrical, so
this is just n times the variance of x1.

00:47:13.210 --> 00:47:17.776
Similarly, for
all the covariance terms, 2 times and

00:47:17.776 --> 00:47:22.260
then there are n choose two
of these covariance terms.

00:47:22.260 --> 00:47:27.150
But we may as well just consider
the covariance of x1 and

00:47:27.150 --> 00:47:31.782
x2, Symmetry, so

00:47:31.782 --> 00:47:39.059
you should think through to make sure
you see why this symmetry hold here.

00:47:39.059 --> 00:47:43.212
So for the first ball,
I mean variance of x1,

00:47:43.212 --> 00:47:47.485
that would just get using a Bernoulli,
right?

00:47:47.485 --> 00:47:48.770
So that's easy to get,

00:47:48.770 --> 00:47:52.320
but let's think a little bit about
the covariance of x1 and x2.

00:47:54.840 --> 00:48:00.160
So this part we already know, this we
now know if we see the symmetry or not.

00:48:00.160 --> 00:48:02.968
But you should make sure you see
the symmetry in this problem,

00:48:02.968 --> 00:48:05.514
cuz if there's symmetry you
only take advantage of it.

00:48:05.514 --> 00:48:08.399
And if there isn't symmetry you
don't wanna falsely assume, and so

00:48:08.399 --> 00:48:10.400
you have to be very careful about that.

00:48:10.400 --> 00:48:13.880
Symmetry is powerful but
the danger is, all right?

00:48:13.880 --> 00:48:16.537
Let's quickly get covariance of x1 and x2.

00:48:16.537 --> 00:48:22.342
Well that's E(x1 x2)-E(x1) E(x2).

00:48:26.081 --> 00:48:30.030
E(x1 x2), let's do the second term first.

00:48:30.030 --> 00:48:32.977
That's easy,
that's just the fundamental bridge,

00:48:32.977 --> 00:48:37.408
the probability of the first ball is white
times probably the second one is white.

00:48:37.408 --> 00:48:42.319
But both of those are w over w + b, Okay?

00:48:42.319 --> 00:48:47.079
Now for this term, E(x1 x2), let's use
the fact here that the product of two

00:48:47.079 --> 00:48:51.810
indicator random variables is
the indicator of the intersection.

00:48:51.810 --> 00:48:54.772
So this event here,
it's expected I have an indicator,

00:48:54.772 --> 00:48:59.228
fundamental bridge that's the probability
that the first two balls are both white.

00:48:59.228 --> 00:49:04.886
While the first ball has probably w/w + b,
and then the second ball

00:49:04.886 --> 00:49:10.360
being white given that the first
ball is white is w-1/w + b-1

00:49:12.376 --> 00:49:15.271
So then we have the covariance, so
we know this thing, we know this thing.

00:49:15.271 --> 00:49:18.543
So at this point, we can just do some
algebra and simplify everything together.

00:49:18.543 --> 00:49:21.456
I'll clean this up next time,
and give the final answer, but

00:49:21.456 --> 00:49:24.050
at this point we know the answer,
it's just algebra.

00:49:24.050 --> 00:49:25.680
Okay, so see you on Friday.

