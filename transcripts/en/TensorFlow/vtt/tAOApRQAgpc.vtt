WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.577
[MUSIC PLAYING]

00:00:02.577 --> 00:00:04.160
SERGIO GUADARRAMA:
Today, we are going

00:00:04.160 --> 00:00:07.110
to talk about reinforcement
learning, how you can apply

00:00:07.110 --> 00:00:09.330
to many different problems.

00:00:09.330 --> 00:00:11.220
So hopefully, by
the end of the talk,

00:00:11.220 --> 00:00:13.740
you will know how to use
reinforcement learning

00:00:13.740 --> 00:00:15.940
for your problem, for
your applications, what

00:00:15.940 --> 00:00:18.180
other things we are
doing at Google with all

00:00:18.180 --> 00:00:20.320
these new technology.

00:00:20.320 --> 00:00:22.770
So let me go a little bit--

00:00:22.770 --> 00:00:25.260
do you remember when you try
to do something difficult

00:00:25.260 --> 00:00:27.940
that was hard that
you need to try a lot?

00:00:27.940 --> 00:00:31.990
For example, when you learned
how to walk, do you remember?

00:00:31.990 --> 00:00:32.759
I don't remember.

00:00:32.759 --> 00:00:35.280
But it's pretty hard
because nobody tells you

00:00:35.280 --> 00:00:36.710
exactly how to do it.

00:00:36.710 --> 00:00:38.160
You just keep trying.

00:00:38.160 --> 00:00:41.400
And eventually, you're able
to stand up, keep the balance,

00:00:41.400 --> 00:00:44.370
wobble around,
and start walking.

00:00:44.370 --> 00:00:48.690
So what if we want to teach this
cute little robot how to walk?

00:00:48.690 --> 00:00:50.610
Imagine-- how you will do that?

00:00:50.610 --> 00:00:54.000
How would you tell
this robot how to walk?

00:00:54.000 --> 00:00:55.680
So what we are
going to do today is

00:00:55.680 --> 00:00:58.990
learn how we can do that
with machine learning.

00:00:58.990 --> 00:01:01.680
And the reason for
that is because if we

00:01:01.680 --> 00:01:04.989
want to do this by
calling a set of rules,

00:01:04.989 --> 00:01:06.510
it will be really hard.

00:01:06.510 --> 00:01:09.960
What kind of rules would we
put in code that can actually

00:01:09.960 --> 00:01:11.190
make this robot the walk?

00:01:11.190 --> 00:01:13.020
We have to do
coordination, balance.

00:01:13.020 --> 00:01:15.090
It's really difficult.
And then they probably

00:01:15.090 --> 00:01:17.240
would just fall over.

00:01:17.240 --> 00:01:19.390
And we don't know what
to change in the code.

00:01:19.390 --> 00:01:21.182
Instead of that, we're
going to use machine

00:01:21.182 --> 00:01:24.250
learning to learn from it.

00:01:24.250 --> 00:01:26.570
So the agenda for today
is going to be this.

00:01:26.570 --> 00:01:28.690
We are going to cover
very quickly what

00:01:28.690 --> 00:01:31.950
is supervised learning,
reinforcement learning, what

00:01:31.950 --> 00:01:34.485
is TF-Agents, these things
we just talk about it.

00:01:34.485 --> 00:01:36.330
And we will go through
multiple examples.

00:01:36.330 --> 00:01:40.020
So you can see we can build up
different pieces to actually

00:01:40.020 --> 00:01:43.370
go and solve this problem,
teach this robot how to walk.

00:01:43.370 --> 00:01:45.900
And finally, we will have some
take home methods that you

00:01:45.900 --> 00:01:49.180
can take with you all today.

00:01:49.180 --> 00:01:53.560
So how many of you know
what is supervised learning?

00:01:53.560 --> 00:01:54.060
OK.

00:01:54.060 --> 00:01:56.227
That's pretty good.

00:01:56.227 --> 00:01:57.935
For those of you who
don't know, let's go

00:01:57.935 --> 00:01:59.920
to a very simple example.

00:01:59.920 --> 00:02:02.070
So we're going to have
some inputs, in this case,

00:02:02.070 --> 00:02:03.270
like an image.

00:02:03.270 --> 00:02:05.275
And we're going to
pass through our model,

00:02:05.275 --> 00:02:06.900
and we're going to
put in some outputs.

00:02:06.900 --> 00:02:09.539
In this case, there's going
to be a cat or the dog.

00:02:09.539 --> 00:02:12.458
And then, we're going to tell
you what is the right answer.

00:02:12.458 --> 00:02:13.500
So that's the key aspect.

00:02:13.500 --> 00:02:15.458
In supervising learning,
we tell you the label.

00:02:15.458 --> 00:02:16.780
What is the right answer?

00:02:16.780 --> 00:02:20.760
So you can modify your model
and learn from these mistakes.

00:02:20.760 --> 00:02:22.650
In this case, you
might use a neural net.

00:02:22.650 --> 00:02:24.840
We have a lot of ways
that you can learn.

00:02:24.840 --> 00:02:26.760
And you can modify
those connections

00:02:26.760 --> 00:02:32.120
to basically learn over time
what is the right answer.

00:02:32.120 --> 00:02:33.970
The thing that
supervised learning need

00:02:33.970 --> 00:02:35.380
is a lot of labels.

00:02:35.380 --> 00:02:38.360
So many of you probably
heard about IMAGENET.

00:02:38.360 --> 00:02:40.630
It's a data set
collected by Stanford.

00:02:40.630 --> 00:02:43.330
It took like over two
years and $1 million

00:02:43.330 --> 00:02:45.700
to gather all this data.

00:02:45.700 --> 00:02:49.060
And they could annotate
millions of images with labels.

00:02:49.060 --> 00:02:51.250
Say, in this image, there's
a container received.

00:02:51.250 --> 00:02:52.380
There's a motor scooter.

00:02:52.380 --> 00:02:53.500
There's a leopard.

00:02:53.500 --> 00:02:55.120
And then, you label
all these images

00:02:55.120 --> 00:02:57.220
so your model can learn from it.

00:02:57.220 --> 00:02:58.608
And that worked
really well where

00:02:58.608 --> 00:03:00.400
you can have all these
labels, and then you

00:03:00.400 --> 00:03:02.680
can train your model from it.

00:03:02.680 --> 00:03:05.630
The question is like, how
will you provide the labels

00:03:05.630 --> 00:03:06.830
for this robot?

00:03:06.830 --> 00:03:08.840
What is the right actions?

00:03:08.840 --> 00:03:09.440
I don't know.

00:03:09.440 --> 00:03:10.460
It's not that clear.

00:03:10.460 --> 00:03:13.790
What will be the right
answer for this case?

00:03:13.790 --> 00:03:15.890
So we are going to take a
different approach, what

00:03:15.890 --> 00:03:18.350
is like reinforcement learning.

00:03:18.350 --> 00:03:20.450
Instead of trying to
provide the right answer--

00:03:20.450 --> 00:03:23.510
like in a classical setting,
you will go to class,

00:03:23.510 --> 00:03:25.580
and they tell you what
is the right answers.

00:03:25.580 --> 00:03:29.180
You know, you study, this is
the answer for this problem.

00:03:29.180 --> 00:03:31.370
We already know what
is the right answer.

00:03:31.370 --> 00:03:32.870
In reinforcement
learning, we assume

00:03:32.870 --> 00:03:34.670
we don't know what
is the right answer.

00:03:34.670 --> 00:03:36.440
We need to figure
it out ourselves.

00:03:36.440 --> 00:03:37.450
It's more like a kid.

00:03:37.450 --> 00:03:40.640
It's playing around, putting
these labels together.

00:03:40.640 --> 00:03:43.550
And eventually, they're able to
stack it up together, and stand

00:03:43.550 --> 00:03:44.520
up.

00:03:44.520 --> 00:03:46.050
And that gives you
like some reward.

00:03:46.050 --> 00:03:49.560
It's like, oh, you feel proud of
it, and then you keep doing it.

00:03:49.560 --> 00:03:50.900
Which are the actions you took?

00:03:50.900 --> 00:03:53.220
Not so relevant.

00:03:53.220 --> 00:03:56.340
So let's formalize a little more
what reinforcement learning is

00:03:56.340 --> 00:03:57.875
and how you can
actually make these

00:03:57.875 --> 00:04:01.130
into more concrete examples.

00:04:01.130 --> 00:04:04.120
Let's take a simpler example,
like this little game

00:04:04.120 --> 00:04:05.920
that you're trying to play.

00:04:05.920 --> 00:04:08.700
You want to bounce the
ball around, move the pile

00:04:08.700 --> 00:04:10.900
at the bottom left or
right, and then you

00:04:10.900 --> 00:04:14.200
want to hit all these
bricks, and play this game,

00:04:14.200 --> 00:04:16.403
clear up, and win the game.

00:04:16.403 --> 00:04:18.320
So we're going to have
this notion of an agent

00:04:18.320 --> 00:04:20.399
or program that's going
to get some reservation.

00:04:20.399 --> 00:04:22.370
In this case, a friend is
going to look at the game.

00:04:22.370 --> 00:04:24.828
What is the ball, where are
the brakes, what is the puzzle,

00:04:24.828 --> 00:04:25.900
and take an action.

00:04:25.900 --> 00:04:28.690
I'm going to move to the left or
I'm going to move to the right.

00:04:28.690 --> 00:04:31.600
And depending where you
move, the ball will drop,

00:04:31.600 --> 00:04:34.530
or you actually start keeping
the ball bouncing back.

00:04:34.530 --> 00:04:36.790
And we're going to have
this notion of reward,

00:04:36.790 --> 00:04:38.560
what is like when
you do well, we

00:04:38.560 --> 00:04:41.920
want you to get positive reward,
so you reinforce that behavior.

00:04:41.920 --> 00:04:45.040
And when you do poorly, you
will get negative reward.

00:04:45.040 --> 00:04:48.650
So we can define simple
rules and simple things

00:04:48.650 --> 00:04:52.560
to basically call this
behavior as a reward function.

00:04:52.560 --> 00:04:55.540
Every time you hit a
brick, you get 10 points.

00:04:55.540 --> 00:04:58.030
Which actions do you need
to do to hit the brick?

00:04:58.030 --> 00:04:58.870
I don't tell you.

00:04:58.870 --> 00:05:00.490
That's what you need to learn.

00:05:00.490 --> 00:05:03.230
But if you do it, I'm going
to give the 10 points.

00:05:03.230 --> 00:05:04.750
And if you clear
all the bricks, I'm

00:05:04.750 --> 00:05:06.250
going to give you
actually a hundred

00:05:06.250 --> 00:05:08.260
points to encourage
you to actually play

00:05:08.260 --> 00:05:10.190
this game very well.

00:05:10.190 --> 00:05:11.710
And every time the
ball drops, you

00:05:11.710 --> 00:05:14.260
lose 50 points, which
means, probably not

00:05:14.260 --> 00:05:16.120
a good idea to do that.

00:05:16.120 --> 00:05:18.870
And if you let the ball drop
three times, game is over,

00:05:18.870 --> 00:05:21.530
you need to stop the game.

00:05:21.530 --> 00:05:23.870
So the good thing is about
the reinforcement learning,

00:05:23.870 --> 00:05:26.130
you can apply to many
different problems.

00:05:26.130 --> 00:05:28.750
And here are some examples
that over the last year people

00:05:28.750 --> 00:05:30.500
have been applying
reinforcement learning.

00:05:30.500 --> 00:05:33.230
And it goes from recommender
instance in YouTube, data

00:05:33.230 --> 00:05:35.660
set to cooling, real robots.

00:05:35.660 --> 00:05:38.000
You can apply to
math, chemistry,

00:05:38.000 --> 00:05:40.430
or a cute little robot
in the middle, and things

00:05:40.430 --> 00:05:41.900
as complex as they go.

00:05:41.900 --> 00:05:44.240
Like DeepMind applied
to AlphaGo and beat

00:05:44.240 --> 00:05:48.140
the best player in the world by
using reinforcement learning.

00:05:48.140 --> 00:05:52.700
Now, let me switch a little bit
to TF-Agents and what it is.

00:05:52.700 --> 00:05:55.432
So main idea of TF-Agents like
doing reinforcement learning

00:05:55.432 --> 00:05:56.140
is not very easy.

00:05:56.140 --> 00:05:59.360
It requires a lot of
tools and a lot of things

00:05:59.360 --> 00:06:01.940
that you need to
build on your own.

00:06:01.940 --> 00:06:04.520
So we built this library
that we use at Google,

00:06:04.520 --> 00:06:07.250
and we open source
so everybody can

00:06:07.250 --> 00:06:10.970
use it to make reinforcement
learning a lot easier to use.

00:06:10.970 --> 00:06:12.225
So we make it very robust.

00:06:12.225 --> 00:06:15.725
It's scalable, and it's
good for beginners.

00:06:15.725 --> 00:06:17.410
If you are new to
RL, we have a lot

00:06:17.410 --> 00:06:19.370
of notebooks,
example documentation

00:06:19.370 --> 00:06:21.512
that you can start working on.

00:06:21.512 --> 00:06:22.970
And also, for
complex problems, you

00:06:22.970 --> 00:06:24.920
can apply to real
complex problems

00:06:24.920 --> 00:06:28.100
and use it for realistic cases.

00:06:28.100 --> 00:06:30.200
For people who want to
create their own algorithm,

00:06:30.200 --> 00:06:32.840
we also make it easy
to add new algorithms.

00:06:32.840 --> 00:06:35.340
It's well tested and
easy to configure.

00:06:35.340 --> 00:06:39.230
And furthermore, we build
it on top of TensorFlow 2.0

00:06:39.230 --> 00:06:43.130
that you probably heard
over at Google I/O before.

00:06:43.130 --> 00:06:46.290
And we make it in such a way so
it's developing and debugging

00:06:46.290 --> 00:06:47.730
is a lot easier.

00:06:47.730 --> 00:06:51.050
You can use see TF-Eager mode
and Keras and TF functions

00:06:51.050 --> 00:06:53.240
to make things a
lot easier to build.

00:06:53.240 --> 00:06:55.880
Very modular, very extensible.

00:06:55.880 --> 00:06:59.763
Let me cover a little bit the
main pieces of the software,

00:06:59.763 --> 00:07:01.430
so then when we go
through the examples,

00:07:01.430 --> 00:07:03.230
you have a better sense.

00:07:03.230 --> 00:07:06.080
On the left side, we have
all the data collection.

00:07:06.080 --> 00:07:09.140
When we play this game, we
are going to collect data.

00:07:09.140 --> 00:07:10.390
We are going to play the game.

00:07:10.390 --> 00:07:12.583
We're collecting data
so we can learn from it.

00:07:12.583 --> 00:07:14.000
And on the right
side, we're going

00:07:14.000 --> 00:07:15.520
to have a training pipeline.

00:07:15.520 --> 00:07:17.490
When we have the
data, like a data set,

00:07:17.490 --> 00:07:21.230
or log in, or games we play,
we're going to transfer any

00:07:21.230 --> 00:07:23.280
proof or model-- in this
case, the neural net--

00:07:23.280 --> 00:07:28.110
I'm going to deploy, collect
more data, and repeat.

00:07:28.110 --> 00:07:30.600
So now, let me hand
it over to Eugene,

00:07:30.600 --> 00:07:33.070
who is going to go over
the CartPole example.

00:07:33.070 --> 00:07:34.670
EUGENE BREVDO: Thanks, Sergio.

00:07:34.670 --> 00:07:38.030
Yeah, so the first example
we're going to go over

00:07:38.030 --> 00:07:40.580
is a problem called Cartpole.

00:07:40.580 --> 00:07:42.890
This is one of the
classical control problems

00:07:42.890 --> 00:07:45.950
where imagine that you
have a pole in your hand,

00:07:45.950 --> 00:07:48.570
and it wants to fall
over because of gravity.

00:07:48.570 --> 00:07:50.900
And you kind of have to move
your hand left and right

00:07:50.900 --> 00:07:52.400
to keep it upright.

00:07:52.400 --> 00:07:55.760
And if it falls
over, then game over.

00:07:55.760 --> 00:08:00.210
If you move off the screen
by accident, then game over.

00:08:00.210 --> 00:08:04.340
So let's make that a
little bit more concrete.

00:08:04.340 --> 00:08:07.790
In this environment, the
observation is not the images

00:08:07.790 --> 00:08:08.730
that you see here.

00:08:08.730 --> 00:08:11.030
Instead, it's a four
vector containing

00:08:11.030 --> 00:08:15.420
angles and velocities of
the pole and the cart.

00:08:15.420 --> 00:08:17.960
The actions are
the values 0 and 1

00:08:17.960 --> 00:08:21.800
representing being able
to take a left or a right.

00:08:21.800 --> 00:08:28.550
And the reward is the value
1.0 every time step or frame

00:08:28.550 --> 00:08:31.460
that the pole is up and
hasn't fallen over more

00:08:31.460 --> 00:08:33.590
than 15 degrees from vertical.

00:08:33.590 --> 00:08:38.669
And once it has,
the episode ends.

00:08:38.669 --> 00:08:43.860
OK, so if you were to implement
this problem or environment

00:08:43.860 --> 00:08:48.840
yourself, you would subclass the
TF-Agents by environment class,

00:08:48.840 --> 00:08:50.980
and you would provide
two properties.

00:08:50.980 --> 00:08:53.550
One is called the
observation aspect,

00:08:53.550 --> 00:08:55.890
and that defines what
the observations are.

00:08:55.890 --> 00:08:58.680
And you would implement
the action spec property,

00:08:58.680 --> 00:09:01.710
and that describes what
actions the environment allows.

00:09:01.710 --> 00:09:03.460
And there are two major methods.

00:09:03.460 --> 00:09:05.880
One is reset, which
resets the environment

00:09:05.880 --> 00:09:08.730
and brings the pole back
to the center and vertical.

00:09:08.730 --> 00:09:12.470
And the set method, which
accepts the action and updates

00:09:12.470 --> 00:09:17.564
any internal state and emits
the observation and the reward

00:09:17.564 --> 00:09:20.170
for that time stamp.

00:09:20.170 --> 00:09:22.490
Now, for this
particular problem,

00:09:22.490 --> 00:09:23.870
you don't have to do that.

00:09:23.870 --> 00:09:28.210
We support OpenAi, which
is a very popular framework

00:09:28.210 --> 00:09:30.610
for environments in Python.

00:09:30.610 --> 00:09:32.560
And you can simply load
CartPole from that.

00:09:32.560 --> 00:09:33.970
That's the first line.

00:09:33.970 --> 00:09:35.840
And now you can perform
some introspection.

00:09:35.840 --> 00:09:38.750
You can interrogate the
environment, say what is

00:09:38.750 --> 00:09:40.420
Observation Spec.

00:09:40.420 --> 00:09:43.690
Here, you can see that it's
a forward vector of floating

00:09:43.690 --> 00:09:44.800
point values.

00:09:44.800 --> 00:09:51.130
Again, describing the angle
and velocities of the pole.

00:09:51.130 --> 00:09:53.980
And the Action Spec
is a scalar integer

00:09:53.980 --> 00:09:59.060
picking on values 0 and 1,
representing left and right.

00:09:59.060 --> 00:10:02.480
So if you had your own
policy that you had built,

00:10:02.480 --> 00:10:05.720
maybe a scripted
policy, you would

00:10:05.720 --> 00:10:07.700
be able to interact
with the environment

00:10:07.700 --> 00:10:12.200
by loading it, building
your policy object,

00:10:12.200 --> 00:10:15.140
resetting the environment
to get an initial state,

00:10:15.140 --> 00:10:17.880
and then iterating
over and over again,

00:10:17.880 --> 00:10:22.250
passing the observation or
the state to the policy,

00:10:22.250 --> 00:10:25.490
getting an action from that,
passing the action back

00:10:25.490 --> 00:10:30.110
to the environment, maybe
calculating your return, which

00:10:30.110 --> 00:10:33.700
is the sum of the
rewards or all steps.

00:10:33.700 --> 00:10:36.130
Now, the interesting
part comes when

00:10:36.130 --> 00:10:39.460
you want to make
a tenable policy

00:10:39.460 --> 00:10:42.700
and you wanted to learn from its
successes in the environment.

00:10:42.700 --> 00:10:45.840
To do that, we put a
neural network in the loop.

00:10:45.840 --> 00:10:50.490
So the neural network takes
in the observations, an image.

00:10:50.490 --> 00:10:52.990
In this case-- and the algorithm
that we're talking about is

00:10:52.990 --> 00:10:57.280
called policy gradients,
also known as reinforce--

00:10:57.280 --> 00:11:00.520
it's going to emit probabilities
over the actions that

00:11:00.520 --> 00:11:01.250
can be taken.

00:11:01.250 --> 00:11:02.708
So in this case,
it's going to emit

00:11:02.708 --> 00:11:05.260
a probability of taking a left
or a probability of taking

00:11:05.260 --> 00:11:07.660
a right, and that's
parameterized

00:11:07.660 --> 00:11:12.820
by the weight of the
neural network called data.

00:11:12.820 --> 00:11:17.020
And ultimately, the
goal of this algorithm

00:11:17.020 --> 00:11:20.470
is going to be modifying
the neural network over time

00:11:20.470 --> 00:11:24.070
to maximize what's called
the expected return.

00:11:24.070 --> 00:11:26.020
And as I mentioned,
the return is

00:11:26.020 --> 00:11:29.770
the sum of the rewards over
the duration of the episode.

00:11:29.770 --> 00:11:32.980
And you can calculate it--

00:11:32.980 --> 00:11:39.160
by just this expectation,
it's difficult to calculate

00:11:39.160 --> 00:11:40.210
analytically.

00:11:40.210 --> 00:11:43.540
So what we're going to do is
we're going to sample episodes

00:11:43.540 --> 00:11:46.210
by playing, we're going
to get trajectories,

00:11:46.210 --> 00:11:51.040
and we're going to store
those trajectories.

00:11:51.040 --> 00:11:55.570
These are observation action
pairs over the episode.

00:11:55.570 --> 00:11:57.400
We're going to add them up.

00:11:57.400 --> 00:12:01.380
And that's our Monte Carlo
estimate of the return.

00:12:01.380 --> 00:12:01.960
OK?

00:12:01.960 --> 00:12:04.650
And we're going to
make a couple of checks

00:12:04.650 --> 00:12:09.030
to convert that expectation
optimization problem into a sum

00:12:09.030 --> 00:12:11.430
that we can optimize
using gradient descent.

00:12:11.430 --> 00:12:13.200
I'm going to skip
over some of the math,

00:12:13.200 --> 00:12:15.630
but basically, what we use
is something called the log

00:12:15.630 --> 00:12:18.630
trick to convert
this gradient problem

00:12:18.630 --> 00:12:21.630
into the gradient over the
outputs of the neural network.

00:12:21.630 --> 00:12:23.340
That's that log pi
theta right there.

00:12:23.340 --> 00:12:25.240
That's the output
of the network.

00:12:25.240 --> 00:12:28.530
And we're going to multiply
that by the Monte Carlo

00:12:28.530 --> 00:12:30.310
estimate of the returns.

00:12:30.310 --> 00:12:33.240
And we're going to average
over the timestamps

00:12:33.240 --> 00:12:38.090
within the episode and over
many batches of episodes.

00:12:38.090 --> 00:12:39.250
Putting this into code--

00:12:39.250 --> 00:12:41.000
and by the way, we
implement this for you,

00:12:41.000 --> 00:12:43.910
but that's kind of
a pseudo code here.

00:12:43.910 --> 00:12:46.860
You get this experience
when you're training,

00:12:46.860 --> 00:12:50.570
you extract its rewards, and
you do a cumulative sum type

00:12:50.570 --> 00:12:53.180
operation to
calculate the returns.

00:12:53.180 --> 00:12:56.720
Then, you take the observations
over all the time steps,

00:12:56.720 --> 00:12:59.720
and you calculate the lotus,
the log probabilities coming out

00:12:59.720 --> 00:13:01.640
of the neural network.

00:13:01.640 --> 00:13:05.450
You pass those to a
distribution object--

00:13:05.450 --> 00:13:08.060
this is a TensorFlow probability
distribution object--

00:13:08.060 --> 00:13:11.420
to get the distributions
over the action.

00:13:11.420 --> 00:13:14.690
And then you can calculate
the full log probability

00:13:14.690 --> 00:13:18.160
of the actions that were
taken in your trajectories

00:13:18.160 --> 00:13:21.560
and your logs, and
calculate this approximation

00:13:21.560 --> 00:13:26.720
of the expectation
and take its gradient.

00:13:26.720 --> 00:13:29.750
OK, so as an end
user, you don't need

00:13:29.750 --> 00:13:31.810
to worry about that too much.

00:13:31.810 --> 00:13:35.480
What you want to do is
you load your environment,

00:13:35.480 --> 00:13:38.960
you wrap it in something
called a TF Py environment.

00:13:38.960 --> 00:13:43.700
And that's easy as the
interaction between the Python

00:13:43.700 --> 00:13:45.740
problem setting
and the environment

00:13:45.740 --> 00:13:48.350
and the neural network,
which is being executed

00:13:48.350 --> 00:13:51.220
by the TensorFlow runtime.

00:13:51.220 --> 00:13:55.930
Now, you can also create
your neural network.

00:13:55.930 --> 00:13:58.110
And here, you can
write your own.

00:13:58.110 --> 00:14:00.302
And basically, it's a
sequence of Keras layers.

00:14:00.302 --> 00:14:02.010
Those of you who are
familiar with Keras,

00:14:02.010 --> 00:14:05.110
that makes it very easy to
describe your own architecture

00:14:05.110 --> 00:14:07.350
for the network.

00:14:07.350 --> 00:14:10.270
We provide a number
of neural networks.

00:14:10.270 --> 00:14:13.350
This one accepts a
number of parameters that

00:14:13.350 --> 00:14:14.590
configure the architecture.

00:14:14.590 --> 00:14:18.380
So here, there are two
fully connected layers

00:14:18.380 --> 00:14:21.420
with sizes 32 and 64.

00:14:21.420 --> 00:14:23.730
You pass this
network and the specs

00:14:23.730 --> 00:14:27.540
associated with the
environment to the agent class.

00:14:27.540 --> 00:14:31.140
And now you're ready to
collect that and to train.

00:14:31.140 --> 00:14:34.110
So to collect data, you
need a place to store it.

00:14:34.110 --> 00:14:38.040
And Sergio we'll talk about
this more in the second example.

00:14:38.040 --> 00:14:39.900
But basically, we
use something called

00:14:39.900 --> 00:14:44.490
replay buffers that are going
to store these trajectories.

00:14:44.490 --> 00:14:47.610
And we provide a
number of utilities

00:14:47.610 --> 00:14:49.200
that will collect
the data for you,

00:14:49.200 --> 00:14:50.610
and they're called drivers.

00:14:50.610 --> 00:14:53.610
So this driver takes
the environment,

00:14:53.610 --> 00:14:57.540
takes the policy exposed
by the agent path

00:14:57.540 --> 00:14:59.340
and a number of callbacks.

00:14:59.340 --> 00:15:00.720
And what it's
going to do is it's

00:15:00.720 --> 00:15:03.323
going to iterate collecting
data, interacting

00:15:03.323 --> 00:15:05.490
with the environment, sending
it actions, collecting

00:15:05.490 --> 00:15:10.000
observations, sending those to
the policy, does that for you.

00:15:10.000 --> 00:15:12.320
And each time it does
that, for every time stop,

00:15:12.320 --> 00:15:14.700
it's stores that in
the replay buffer.

00:15:14.700 --> 00:15:21.240
So to train, you iterate
calling a driver run, which

00:15:21.240 --> 00:15:22.938
populates the replay buffer.

00:15:22.938 --> 00:15:25.230
Then you pull out all of the
trajectories in the replay

00:15:25.230 --> 00:15:27.510
buffer with gather
all, you pass those

00:15:27.510 --> 00:15:31.710
to agent.train, which updates
the underlying neural networks.

00:15:31.710 --> 00:15:34.980
And because policy gradients
is in something called an

00:15:34.980 --> 00:15:37.500
on policy algorithm,
all that hard earned

00:15:37.500 --> 00:15:39.000
collected data that
you've done, you

00:15:39.000 --> 00:15:42.630
have to throw it away
and collect more.

00:15:42.630 --> 00:15:43.440
OK?

00:15:43.440 --> 00:15:47.790
So that said, CartPole is
a fairly straightforward

00:15:47.790 --> 00:15:49.770
classical problem,
as I mentioned.

00:15:49.770 --> 00:15:53.970
And policy gradients is a
fairly standard, somewhat simple

00:15:53.970 --> 00:15:55.800
algorithm.

00:15:55.800 --> 00:15:59.460
And after about 400 iterations
of playing the game,

00:15:59.460 --> 00:16:02.070
you can see that
whereas you started

00:16:02.070 --> 00:16:06.000
with a random policy, that
can't keep the pole up at all.

00:16:06.000 --> 00:16:08.280
After 400 iterations
of playing the game,

00:16:08.280 --> 00:16:10.600
you basically have
a perfect policy.

00:16:10.600 --> 00:16:13.290
And if you were to look
at your TensorBoard

00:16:13.290 --> 00:16:15.840
while you're training,
you'd see a plot

00:16:15.840 --> 00:16:19.500
like this, which shows that
as the number of episodes

00:16:19.500 --> 00:16:22.330
that are being collected
increases the total return--

00:16:22.330 --> 00:16:25.560
which is the sum of the
rewards over the episode--

00:16:25.560 --> 00:16:27.690
goes up pretty consistently.

00:16:27.690 --> 00:16:32.130
And at around 400, 500 episodes,
we have a perfect algorithm

00:16:32.130 --> 00:16:36.130
that runs for 200 steps, at
which point the episode says,

00:16:36.130 --> 00:16:39.510
all right, you're good, you win.

00:16:39.510 --> 00:16:42.670
And then you're done.

00:16:42.670 --> 00:16:45.400
OK, so I'm going to hand
it back over to Sergio

00:16:45.400 --> 00:16:48.680
to talk about Atari
and deep Q-learning.

00:16:48.680 --> 00:16:50.830
SERGIO GUADARRAMA:
Thank you, again.

00:16:50.830 --> 00:16:52.990
So now we're going
back to this example

00:16:52.990 --> 00:16:55.783
that I talked at the beginning
about how to play this game.

00:16:55.783 --> 00:16:57.700
And now we're going to
go through more details

00:16:57.700 --> 00:17:00.910
how this actually works, and
how this deep Q-learning works

00:17:00.910 --> 00:17:03.950
to help us in this case.

00:17:03.950 --> 00:17:05.680
So let's go back to our setting.

00:17:05.680 --> 00:17:07.746
Now we have our
environment where

00:17:07.746 --> 00:17:08.829
we're going to be playing.

00:17:08.829 --> 00:17:10.371
We're going to get
some observations,

00:17:10.371 --> 00:17:11.589
in this case, frames.

00:17:11.589 --> 00:17:14.230
The agent role is to
produce different actions

00:17:14.230 --> 00:17:17.290
like go left with the paddle or
go right, and get some rewards

00:17:17.290 --> 00:17:19.810
in the process, and
then improve over time

00:17:19.810 --> 00:17:25.430
by basically incorporating
those rewards into the model.

00:17:25.430 --> 00:17:28.000
Let's take a little
step and say,

00:17:28.000 --> 00:17:30.640
what if while I'm
playing Breakout,

00:17:30.640 --> 00:17:32.380
I have seen how far
what I've been doing,

00:17:32.380 --> 00:17:34.360
the ball is going
somewhere, I'm moving

00:17:34.360 --> 00:17:38.110
in the centered direction, and
then, what should I do now?

00:17:38.110 --> 00:17:40.990
Should I go to the right
or should I go to the left?

00:17:40.990 --> 00:17:43.670
If I knew what is going to
happen, it will be very easy.

00:17:43.670 --> 00:17:47.090
If I knew oh, the balls
are going to go this way,

00:17:47.090 --> 00:17:49.330
the things are going to be
that, that will be easy.

00:17:49.330 --> 00:17:50.320
But that one, we
don't easily know.

00:17:50.320 --> 00:17:52.480
We don't know what's going
to happen in the future.

00:17:52.480 --> 00:17:53.938
Instead, what we're
going to do, we

00:17:53.938 --> 00:17:57.200
are going to try to estimate
if I move to the right,

00:17:57.200 --> 00:17:58.440
maybe the ball will drop.

00:17:58.440 --> 00:18:00.010
It's more likely
that the ball drop

00:18:00.010 --> 00:18:01.843
because I'm moving in
the opposite direction

00:18:01.843 --> 00:18:03.170
that the ball is going.

00:18:03.170 --> 00:18:05.770
And if I move to the
left, on the contrary,

00:18:05.770 --> 00:18:08.350
I'm going to hit the ball
I'm going to hit some bricks,

00:18:08.350 --> 00:18:10.792
I'm getting closer to
clear all the bricks.

00:18:10.792 --> 00:18:12.250
So the idea is like
I want to learn

00:18:12.250 --> 00:18:14.390
a model that can estimate that.

00:18:14.390 --> 00:18:17.590
If this action is going to make
me go better into the future

00:18:17.590 --> 00:18:19.390
or is going to make it go worse.

00:18:19.390 --> 00:18:22.240
And that's something that
we call expected return.

00:18:22.240 --> 00:18:24.710
So is this nothing that
Eugene was talking before,

00:18:24.710 --> 00:18:26.710
that before we were just
computing, just summing

00:18:26.710 --> 00:18:27.620
on the rewards.

00:18:27.620 --> 00:18:29.170
And here, we're
going to say, I want

00:18:29.170 --> 00:18:32.440
to estimate this action,
how much reward it's

00:18:32.440 --> 00:18:34.160
going to give me in the future.

00:18:34.160 --> 00:18:35.590
And then, I choose
the thing what

00:18:35.590 --> 00:18:39.230
it's according to my
estimate is the best action.

00:18:39.230 --> 00:18:43.000
So we can formulate
this with using math.

00:18:43.000 --> 00:18:45.740
It's basically like an
expectancy over the sum

00:18:45.740 --> 00:18:47.300
of the rewards into the future.

00:18:47.300 --> 00:18:50.640
And that's when I call this
Q function, or like critic.

00:18:50.640 --> 00:18:53.870
It's also a critic because
it's going to basically tell us

00:18:53.870 --> 00:18:57.890
given some state and
possible actions, which

00:18:57.890 --> 00:18:59.460
action is actually better?

00:18:59.460 --> 00:19:02.650
What I criticize in some way,
like if you take this action,

00:19:02.650 --> 00:19:04.710
my expectation of the
return is very high.

00:19:04.710 --> 00:19:07.427
And you take a different
action, my expectation is low.

00:19:07.427 --> 00:19:09.010
And then what we're
going to do, we're

00:19:09.010 --> 00:19:10.873
going to learn
these Q functions.

00:19:10.873 --> 00:19:11.790
Because we don't know.

00:19:11.790 --> 00:19:13.440
We don't know what's
going to happen.

00:19:13.440 --> 00:19:16.120
But by playing,
we can learn what

00:19:16.120 --> 00:19:18.760
is the expected return by
comparing our expectation

00:19:18.760 --> 00:19:21.350
with the actual returns.

00:19:21.350 --> 00:19:24.500
So we are going to use our Q
function-- and in this case,

00:19:24.500 --> 00:19:25.270
a neural net--

00:19:25.270 --> 00:19:27.153
to learn this model.

00:19:27.153 --> 00:19:28.570
And while we have
a learned model,

00:19:28.570 --> 00:19:31.270
then we can just take the best
action according to our model

00:19:31.270 --> 00:19:33.550
and play the game.

00:19:33.550 --> 00:19:37.500
So conceptually, this looks
similar to what we saw before.

00:19:37.500 --> 00:19:39.350
We're going to have
another neural net.

00:19:39.350 --> 00:19:41.040
And this case, the
output is going

00:19:41.040 --> 00:19:43.180
to be the Q values,
this expectation

00:19:43.180 --> 00:19:45.980
of our future returns.

00:19:45.980 --> 00:19:48.660
And the idea is we're going
to get an observation,

00:19:48.660 --> 00:19:50.050
in this case, the frames.

00:19:50.050 --> 00:19:52.420
We're going to maybe have
some history about it.

00:19:52.420 --> 00:19:54.640
And then we're going to
preview some Q value, which

00:19:54.640 --> 00:19:57.480
like our current expectation
if I move to the left,

00:19:57.480 --> 00:20:00.170
and my current expectation
if I move to the right.

00:20:00.170 --> 00:20:02.770
And then I'm going to I
compare my expectation,

00:20:02.770 --> 00:20:04.297
what will actually happen.

00:20:04.297 --> 00:20:06.130
And if basically my
expectation is too high,

00:20:06.130 --> 00:20:07.280
I'm going to lower down.

00:20:07.280 --> 00:20:09.970
And if my expectation is too
low, I'm going to increase it.

00:20:09.970 --> 00:20:12.460
So that way, we're
going to change

00:20:12.460 --> 00:20:15.010
the weight of this network to
basically improve over time

00:20:15.010 --> 00:20:15.960
by playing this game.

00:20:20.240 --> 00:20:23.660
We go back to how you
do this into code.

00:20:23.660 --> 00:20:26.630
Basically, we're going
to log this environment.

00:20:26.630 --> 00:20:28.940
In this case, from the
suite Atari where it's also

00:20:28.940 --> 00:20:31.190
available for an OpenAi.

00:20:31.190 --> 00:20:33.420
I'm going to say, OK,
load the Breakout game.

00:20:33.420 --> 00:20:34.940
And now, we are ready to play.

00:20:34.940 --> 00:20:36.540
We're going to have
some reservations,

00:20:36.540 --> 00:20:38.270
we'll define what
kind of reservations

00:20:38.270 --> 00:20:40.830
we have from this case
where it's frames of like 84

00:20:40.830 --> 00:20:45.200
by 84 pixels, and we also have
multiple answers we can take.

00:20:45.200 --> 00:20:47.180
In this game, we can
only go left and right,

00:20:47.180 --> 00:20:49.595
but there are other
games in this suite that

00:20:49.595 --> 00:20:50.720
can have different actions.

00:20:50.720 --> 00:20:52.790
Maybe jumping, firing,
and doing other things

00:20:52.790 --> 00:20:56.090
that different games have.

00:20:56.090 --> 00:21:00.510
So now, we want to do that
notion what we said before.

00:21:00.510 --> 00:21:02.120
We're going to define
this Q network.

00:21:02.120 --> 00:21:03.970
Remember, it's a neural
net that is going

00:21:03.970 --> 00:21:06.310
to represent these Q values.

00:21:06.310 --> 00:21:07.930
I'm going to have
some parameters that

00:21:07.930 --> 00:21:09.950
define how many layers,
how many things we want

00:21:09.950 --> 00:21:11.810
to have on all those things.

00:21:11.810 --> 00:21:13.820
And then, we're going to
have the Q agent that's

00:21:13.820 --> 00:21:16.220
going to take the network,
and an optimizer which

00:21:16.220 --> 00:21:19.950
is going to basically be able to
improve this network over time,

00:21:19.950 --> 00:21:21.742
given some experience.

00:21:21.742 --> 00:21:23.450
So this experience,
we're going to assume

00:21:23.450 --> 00:21:25.942
we have collected some data
and we have played the game.

00:21:25.942 --> 00:21:27.650
And maybe not very
well at the beginning,

00:21:27.650 --> 00:21:30.200
because we are doing random
actions, for example.

00:21:30.200 --> 00:21:31.830
So we're not playing very well.

00:21:31.830 --> 00:21:33.950
but we can get some
experience, and then we

00:21:33.950 --> 00:21:35.930
can improve over time basically.

00:21:35.930 --> 00:21:37.880
We try to improve our estimates.

00:21:37.880 --> 00:21:40.140
Every time we improve,
we play a little better.

00:21:40.140 --> 00:21:42.160
And then we collect more data.

00:21:42.160 --> 00:21:43.660
And then the idea
is that this agent

00:21:43.660 --> 00:21:45.320
is going to have a
train method that

00:21:45.320 --> 00:21:46.945
is going to go through
this experience,

00:21:46.945 --> 00:21:49.920
and is going to
improve over time.

00:21:49.920 --> 00:21:54.362
In general, for cases like games
or environments are too slow,

00:21:54.362 --> 00:21:56.570
we don't want to play one
game of the time, you know.

00:21:56.570 --> 00:21:59.400
These computers can play
multiple games in parallel.

00:21:59.400 --> 00:22:01.910
So we have this notion
that parallel environments,

00:22:01.910 --> 00:22:04.700
you can play multiple copies of
the same game at the same time.

00:22:04.700 --> 00:22:08.090
So we can make
learning a lot faster.

00:22:08.090 --> 00:22:11.000
And in this case, we are
playing four games in parallel,

00:22:11.000 --> 00:22:14.380
we're going to have for a policy
that we've got just defined.

00:22:14.380 --> 00:22:16.520
And in parallel, we can
just play four games

00:22:16.520 --> 00:22:17.240
at the same time.

00:22:17.240 --> 00:22:18.770
So the agent in
this case will try

00:22:18.770 --> 00:22:20.730
to play four games
at the same time.

00:22:20.730 --> 00:22:22.830
And that way, we'll get
a lot of more experience

00:22:22.830 --> 00:22:25.980
and can learn a lot faster.

00:22:25.980 --> 00:22:29.030
So as we mentioned before, where
we have collected all this data

00:22:29.030 --> 00:22:30.998
by playing this
game, in this case,

00:22:30.998 --> 00:22:32.540
we don't want to
throw away the data.

00:22:32.540 --> 00:22:33.960
We can use it to learn it.

00:22:33.960 --> 00:22:35.960
So we're going to have
this replay buffer, which

00:22:35.960 --> 00:22:37.970
is going to keep all the
data we're collecting,

00:22:37.970 --> 00:22:40.250
like different games will
go in different positions

00:22:40.250 --> 00:22:41.475
so we don't mix the games.

00:22:41.475 --> 00:22:43.850
But we're going to just throw
all the data in some replay

00:22:43.850 --> 00:22:45.110
buffer.

00:22:45.110 --> 00:22:49.080
And that into the
code, it's simple.

00:22:49.080 --> 00:22:50.180
We have this environment.

00:22:50.180 --> 00:22:53.130
We cleared the replay buffer
we have already defined.

00:22:53.130 --> 00:22:55.160
And then basically, using
the driver, and then

00:22:55.160 --> 00:22:57.410
more important than this,
add to the replay buffer.

00:22:57.410 --> 00:23:00.060
Every time you play, take
an action in this game,

00:23:00.060 --> 00:23:01.610
add it to the replay buffer.

00:23:01.610 --> 00:23:05.000
So later, the agent contains
all that experience.

00:23:05.000 --> 00:23:08.430
And because DQN is
our policy method--

00:23:08.430 --> 00:23:11.150
what is different than their
previous method was on policy--

00:23:11.150 --> 00:23:13.640
in this case, we can
actually use all the data.

00:23:13.640 --> 00:23:15.830
We can keep all the data
around and keep training

00:23:15.830 --> 00:23:17.210
on all data too.

00:23:17.210 --> 00:23:19.650
We don't need you to throw away.

00:23:19.650 --> 00:23:21.230
And that's very
important because we

00:23:21.230 --> 00:23:23.292
make it more efficient.

00:23:23.292 --> 00:23:24.750
What we're going
to do when we have

00:23:24.750 --> 00:23:26.480
called the data and
this replay buffer,

00:23:26.480 --> 00:23:27.612
we're going to do a sample.

00:23:27.612 --> 00:23:29.570
We're going to sample a
different set of games,

00:23:29.570 --> 00:23:30.920
different parts of the game.

00:23:30.920 --> 00:23:33.410
I'm going to say, OK, let's
try to replay the game

00:23:33.410 --> 00:23:35.720
and maybe take a different
outcome this time.

00:23:35.720 --> 00:23:38.850
What action will you take if
you were in the same situation?

00:23:38.850 --> 00:23:41.630
Maybe you move to the
left, and the ball drop.

00:23:41.630 --> 00:23:44.130
So maybe now you want
to move to the right.

00:23:44.130 --> 00:23:45.795
So that's the ways
the model is going

00:23:45.795 --> 00:23:48.170
to be learning, by basically
sample games that you played

00:23:48.170 --> 00:23:51.200
before, and now improve
your key function is going

00:23:51.200 --> 00:23:55.130
to change the way you behave.

00:23:55.130 --> 00:23:59.030
So now let's try to put
these things back together.

00:23:59.030 --> 00:24:01.280
Let's go slowly because
there's a lot of pieces.

00:24:01.280 --> 00:24:03.540
So we have our Q
network we're going

00:24:03.540 --> 00:24:07.128
to use to define the
DQN agent in this case.

00:24:07.128 --> 00:24:08.920
We're going to have
the replay buffer where

00:24:08.920 --> 00:24:10.510
we're going to put
all the data we're

00:24:10.510 --> 00:24:12.030
collecting what we played.

00:24:12.030 --> 00:24:13.990
We have this driver,
which basically

00:24:13.990 --> 00:24:15.913
drive the agent in the game.

00:24:15.913 --> 00:24:17.330
So it's going to
basically driving

00:24:17.330 --> 00:24:20.890
the agent making play and
add it to the replay buffer.

00:24:20.890 --> 00:24:23.170
And then, once we
have enough data,

00:24:23.170 --> 00:24:25.360
we can basically
iterate with that data.

00:24:25.360 --> 00:24:29.602
We can iterate, get batches of
experience, different samples.

00:24:29.602 --> 00:24:31.810
And that's what we're going
to do to train the agent.

00:24:31.810 --> 00:24:35.170
So we are going to
alternate, collect more data,

00:24:35.170 --> 00:24:36.070
and train the agent.

00:24:36.070 --> 00:24:38.020
So every time we collect,
we train the agent,

00:24:38.020 --> 00:24:39.700
the agent gets a
little better, and we

00:24:39.700 --> 00:24:42.460
want to collect more
data, and we alternate.

00:24:42.460 --> 00:24:45.350
At the end, what we want to
do is evaluate this agent.

00:24:45.350 --> 00:24:47.050
So we have a method
that says, OK, I

00:24:47.050 --> 00:24:48.730
want to compute some metrics.

00:24:48.730 --> 00:24:50.530
For example, how
long are you playing

00:24:50.530 --> 00:24:52.600
the game, how many
points are you getting,

00:24:52.600 --> 00:24:54.267
all those things that
we want to compare

00:24:54.267 --> 00:24:56.140
metrics and aggressively
have these methods.

00:24:56.140 --> 00:24:59.900
OK, how about take all these
metrics in this environment,

00:24:59.900 --> 00:25:02.020
take the agent
policy, and evaluate

00:25:02.020 --> 00:25:06.050
for multiple games, multiple
episodes, and multiple things.

00:25:06.050 --> 00:25:08.960
How this actually looks
like is something like that.

00:25:08.960 --> 00:25:10.720
For example, in
the Breakout game,

00:25:10.720 --> 00:25:12.230
the curves looks like that.

00:25:12.230 --> 00:25:15.240
At the beginning, we
don't score any points.

00:25:15.240 --> 00:25:17.090
We don't know how
to move the pole,

00:25:17.090 --> 00:25:19.130
the ball just keep
dropping, and we just

00:25:19.130 --> 00:25:20.900
lose the game over and over.

00:25:20.900 --> 00:25:23.532
Eventually, we figure out
that by moving the paddle

00:25:23.532 --> 00:25:25.490
in different directions,
the ball bounced back,

00:25:25.490 --> 00:25:27.670
and it started
hitting the bricks.

00:25:27.670 --> 00:25:32.900
And about 4 or 5 million
frames, they multilaterally

00:25:32.900 --> 00:25:35.310
learn how to actually
play this game.

00:25:35.310 --> 00:25:37.460
And you can see
around 4 or 5 million

00:25:37.460 --> 00:25:40.700
frames, basically, the
model gets very good scores

00:25:40.700 --> 00:25:41.720
around 100 points.

00:25:41.720 --> 00:25:44.000
It's breaking all these
things, all these points,

00:25:44.000 --> 00:25:46.190
and you know, clear
all the bricks.

00:25:46.190 --> 00:25:48.290
We also put graphs
of different games

00:25:48.290 --> 00:25:50.990
like Pong, which is basically
two different paddles trying

00:25:50.990 --> 00:25:52.700
to bounce the ball between them.

00:25:52.700 --> 00:25:56.150
Enduro, Qbert, there's
another like 50 or 60

00:25:56.150 --> 00:25:57.710
games in this suite.

00:25:57.710 --> 00:26:00.240
And you can basically just
change one line of code

00:26:00.240 --> 00:26:01.820
and play a different game.

00:26:01.820 --> 00:26:03.890
I'm not going to go
through those details,

00:26:03.890 --> 00:26:07.730
but just to make clear that it's
simple to play different games.

00:26:07.730 --> 00:26:10.210
Now, let me hand it
over back to Eugene,

00:26:10.210 --> 00:26:12.710
who's going to talk a little
more into the Minitaur.

00:26:12.710 --> 00:26:15.010
Thanks, again.

00:26:15.010 --> 00:26:19.430
EUGENE BREVDO: OK, so our
third and final example

00:26:19.430 --> 00:26:23.960
is the problem of
the Minitaur robot

00:26:23.960 --> 00:26:26.200
and kind of goes back to
one of the first slides

00:26:26.200 --> 00:26:29.240
that Sergio showed at the
beginning of the talk,

00:26:29.240 --> 00:26:30.500
learning for walk.

00:26:30.500 --> 00:26:32.740
So there is a real robot.

00:26:32.740 --> 00:26:34.160
It's called the Minitaur.

00:26:34.160 --> 00:26:36.890
And here, it's kind
of failing hard.

00:26:36.890 --> 00:26:39.630
We're going to see
if we can fix that.

00:26:39.630 --> 00:26:43.610
The algorithm we're going to
use is called Soft Actor Critic.

00:26:43.610 --> 00:26:44.110
OK.

00:26:44.110 --> 00:26:48.930
So again, on the bottom is
some images of the robot.

00:26:48.930 --> 00:26:51.580
And you can see it
looks a little fragile.

00:26:51.580 --> 00:26:56.220
We want to train it,
and we want to avoid

00:26:56.220 --> 00:26:59.910
breaking it in it beginning
when our policy can't really

00:26:59.910 --> 00:27:00.670
stay up.

00:27:00.670 --> 00:27:02.250
So what we're going
to do is we're

00:27:02.250 --> 00:27:06.870
going to model it in a physics
simulator called PyBullet,

00:27:06.870 --> 00:27:08.740
and that's what
you see at the top.

00:27:08.740 --> 00:27:10.980
And then, once we've
trained it, we're

00:27:10.980 --> 00:27:14.433
confident about the
policy on that version,

00:27:14.433 --> 00:27:16.350
we're going to transfer
it back into the robot

00:27:16.350 --> 00:27:18.220
and do some final fine tuning.

00:27:18.220 --> 00:27:22.890
And here, we're going to focus
on the training and simulation.

00:27:22.890 --> 00:27:25.470
So I won't go into the
mathematical details

00:27:25.470 --> 00:27:29.010
of the Soft Actor Critic, but
here's some fundamental aspects

00:27:29.010 --> 00:27:30.450
of that algorithm.

00:27:30.450 --> 00:27:33.270
One is that it can handle both
discrete and continuous action

00:27:33.270 --> 00:27:34.282
spaces.

00:27:34.282 --> 00:27:36.240
Here, we're going to be
controlling some motors

00:27:36.240 --> 00:27:39.390
and actuators, so it's a
fairly continuous action space.

00:27:39.390 --> 00:27:44.910
It's data-efficient, meaning
that all this hard earned data

00:27:44.910 --> 00:27:47.130
that you run in simulation
or you got from the robot,

00:27:47.130 --> 00:27:49.380
you don't have to throw it
away while you're training,

00:27:49.380 --> 00:27:52.140
you can keep it
around for retraining.

00:27:52.140 --> 00:27:53.760
Also, the training is stable.

00:27:53.760 --> 00:27:56.020
Compared to some
other algorithms,

00:27:56.020 --> 00:27:59.040
this one is less likely to
diverge during training.

00:27:59.040 --> 00:28:01.170
And finally, one of
the fundamental aspects

00:28:01.170 --> 00:28:03.420
is that it's Soft
Actor Critic, it

00:28:03.420 --> 00:28:07.230
combines an actor neural network
and a critic neural network

00:28:07.230 --> 00:28:09.510
to accelerate training
and to keep it stable.

00:28:12.960 --> 00:28:17.850
Again, so Minitaur,
you can basically

00:28:17.850 --> 00:28:20.370
do a pip install of
PyBullet, and you'll

00:28:20.370 --> 00:28:22.330
get Minitaur for free.

00:28:22.330 --> 00:28:25.690
You can load it using the
PyBullet fleet with TF-Agents.

00:28:25.690 --> 00:28:27.540
And if you were to look
at this environment,

00:28:27.540 --> 00:28:32.610
you'd see that there are
about 28 sensors on the robot

00:28:32.610 --> 00:28:34.677
that return floating
point values,

00:28:34.677 --> 00:28:36.510
different aspects of
the configuration where

00:28:36.510 --> 00:28:39.950
you are, forces, velocities,
things like that.

00:28:39.950 --> 00:28:43.550
And the action, there are
eight actuators on the robot.

00:28:43.550 --> 00:28:45.360
It can apply a force--

00:28:45.360 --> 00:28:48.540
positive or negative,
minus 1 to 1--

00:28:48.540 --> 00:28:52.090
for each of those
eight actuators.

00:28:52.090 --> 00:28:56.810
Now, here's kind of bringing
together the whole setup.

00:28:56.810 --> 00:29:00.177
You can load four of
these simulations,

00:29:00.177 --> 00:29:01.760
have them running
in parallel, and try

00:29:01.760 --> 00:29:03.843
to maximize the number of
course that you're using

00:29:03.843 --> 00:29:05.610
when you're collecting data.

00:29:05.610 --> 00:29:08.220
And to do that, we provide the
parallel Py environment, which

00:29:08.220 --> 00:29:11.720
Sergio spoke about, wrapped
in TF Py environment.

00:29:11.720 --> 00:29:14.420
And now, we get
down to the business

00:29:14.420 --> 00:29:17.990
of setting up the neural network
architecture for the problem.

00:29:17.990 --> 00:29:19.670
First, we create
the actor network.

00:29:19.670 --> 00:29:21.890
And so what the actor
network is going to do

00:29:21.890 --> 00:29:25.790
is it's going to take these
sensor observations, this 28

00:29:25.790 --> 00:29:31.610
vector, and it's going to limit
samples of actuator values.

00:29:31.610 --> 00:29:34.700
And those samples are
random draws from,

00:29:34.700 --> 00:29:38.060
in this case, Gaussian
or normal distribution.

00:29:38.060 --> 00:29:42.000
So as a result, this
action distribution network

00:29:42.000 --> 00:29:44.520
takes something called
a projection network.

00:29:44.520 --> 00:29:47.480
And we provide a number of
standard projection networks.

00:29:47.480 --> 00:29:52.970
This one emits samples from
a Gaussian distribution.

00:29:52.970 --> 00:29:56.660
And the neural network
that feeds into it

00:29:56.660 --> 00:30:00.080
is going to be setting
up the hyperparameters

00:30:00.080 --> 00:30:03.030
of that distribution.

00:30:03.030 --> 00:30:06.020
Now, the critic network,
which is in the top right,

00:30:06.020 --> 00:30:09.320
is going to take a combination
of the current sensor

00:30:09.320 --> 00:30:13.460
observations and
the action sample

00:30:13.460 --> 00:30:15.590
that the actor network
emitted, and it's

00:30:15.590 --> 00:30:19.680
going to estimate
the expected return.

00:30:19.680 --> 00:30:21.820
How much longer,
given this action,

00:30:21.820 --> 00:30:25.670
is my robot going to stay up?

00:30:25.670 --> 00:30:27.920
How well is it going to gallop?

00:30:27.920 --> 00:30:31.310
And that is going to be
trained from the trajectories

00:30:31.310 --> 00:30:33.230
from the rewards that
you're collecting.

00:30:33.230 --> 00:30:36.560
And that, in turn, is going
to help train the actor.

00:30:36.560 --> 00:30:39.200
So you pass these
networks and these specs

00:30:39.200 --> 00:30:42.680
to the Soft Actor Critic
agent, and you can

00:30:42.680 --> 00:30:45.027
look at its collection policy.

00:30:45.027 --> 00:30:46.610
And that's the thing
that you're going

00:30:46.610 --> 00:30:49.610
to pass on the driver
to start collecting data

00:30:49.610 --> 00:30:51.880
and interacting with
the environment.

00:30:51.880 --> 00:30:54.170
So I won't go into the
details of actually doing

00:30:54.170 --> 00:30:56.280
that because it's literally
identical to the deep

00:30:56.280 --> 00:30:58.760
Q-learning example before.

00:30:58.760 --> 00:31:02.660
You need the replay buffer,
and you use the driver

00:31:02.660 --> 00:31:07.030
and you go through
the same motion.

00:31:07.030 --> 00:31:09.490
What I'm going to
show is what you

00:31:09.490 --> 00:31:12.970
should expect to see
in the TensorBoard

00:31:12.970 --> 00:31:15.670
while you're training
the simulation.

00:31:15.670 --> 00:31:19.680
On the top, you see the
average episode length,

00:31:19.680 --> 00:31:23.800
the average return as a function
of the number of environment

00:31:23.800 --> 00:31:26.387
steps that you've taken--
the number of time steps.

00:31:26.387 --> 00:31:27.970
On the bottom, you
see the same thing.

00:31:27.970 --> 00:31:31.090
But on the x-axis, you
see the number of episodes

00:31:31.090 --> 00:31:33.010
that you've gone through.

00:31:33.010 --> 00:31:37.540
And what you can see is
that after about 13,000,

00:31:37.540 --> 00:31:41.710
14,000 simulated episodes,
we're starting to really learn

00:31:41.710 --> 00:31:43.540
how to walk and gallop.

00:31:43.540 --> 00:31:46.300
The episode lengths
get longer because it

00:31:46.300 --> 00:31:48.100
takes longer to fall down.

00:31:48.100 --> 00:31:50.200
And the average
return also goes up

00:31:50.200 --> 00:31:52.600
because it's also a function
of how long we stay up

00:31:52.600 --> 00:31:54.310
and how well we can gallop.

00:31:57.240 --> 00:32:02.070
So again, if this is
a pilot simulation,

00:32:02.070 --> 00:32:05.610
a rendering of the Minitaur,
at the very beginning,

00:32:05.610 --> 00:32:08.203
when the policy just
emits random values,

00:32:08.203 --> 00:32:09.870
the neural network
emits random values--

00:32:09.870 --> 00:32:12.250
it's randomly initialized--

00:32:12.250 --> 00:32:17.020
and it can barely stay up,
it basically falls over.

00:32:17.020 --> 00:32:18.610
About halfway through
training, it's

00:32:18.610 --> 00:32:22.930
starting to be able to get
up, maybe make a few steps,

00:32:22.930 --> 00:32:23.860
falls over.

00:32:23.860 --> 00:32:26.320
If you apply some external
forces, it'll just fall over.

00:32:29.300 --> 00:32:33.930
By about 16,000
iterations of this,

00:32:33.930 --> 00:32:36.730
it's a pretty robust policy.

00:32:36.730 --> 00:32:38.270
And it can stand, it can gallop.

00:32:38.270 --> 00:32:40.450
If there's an external
force pushing it over,

00:32:40.450 --> 00:32:43.770
it'll be able to get
back up and keep going.

00:32:43.770 --> 00:32:48.360
And once you have
that trained policy,

00:32:48.360 --> 00:32:51.460
you can transfer it,
export it as a safe model,

00:32:51.460 --> 00:32:55.140
put it on the actual robot,
and then start the fine tuning

00:32:55.140 --> 00:32:56.250
process.

00:32:56.250 --> 00:33:02.160
Once you fine tuned it, you
have a pretty neat robot.

00:33:02.160 --> 00:33:04.110
In my head, when I
look at this video,

00:33:04.110 --> 00:33:08.500
I think of the "Chariots
of Fire" theme song.

00:33:08.500 --> 00:33:11.400
I don't know if you've ever
seen it, but it's pretty cool.

00:33:11.400 --> 00:33:15.600
So now, I'm going to
return it back to Sergio

00:33:15.600 --> 00:33:18.530
to provide some final words.

00:33:18.530 --> 00:33:21.910
SERGIO GUADARRAMA:
Thank you, again.

00:33:21.910 --> 00:33:23.320
So pretty cool, no?

00:33:23.320 --> 00:33:24.880
You can get from
the beginning how

00:33:24.880 --> 00:33:28.120
to learn to walk and naturally
make these in simulation.

00:33:28.120 --> 00:33:30.030
But then, we can transfer
it to a real robot

00:33:30.030 --> 00:33:32.270
and make it work
into a real robot.

00:33:32.270 --> 00:33:34.060
So that's part of the
goal of TF-Agents.

00:33:34.060 --> 00:33:36.177
We want to make
our role very easy.

00:33:36.177 --> 00:33:38.260
You can download the code,
you can scan over there

00:33:38.260 --> 00:33:40.510
and go to the GitHub,
start playing with it.

00:33:40.510 --> 00:33:43.480
We have already a lot of
different environments,

00:33:43.480 --> 00:33:44.710
more than we talked today.

00:33:44.710 --> 00:33:46.250
There's many more.

00:33:46.250 --> 00:33:49.330
So we just covered three
examples, but you can go there,

00:33:49.330 --> 00:33:52.260
there's many other
environments available.

00:33:52.260 --> 00:33:54.040
We are hoping that
Unity ML-Agents

00:33:54.040 --> 00:33:57.150
come soon so you can also
interact with the Unity

00:33:57.150 --> 00:33:58.200
renders.

00:33:58.200 --> 00:33:59.950
Maybe there's some of
you who are actually

00:33:59.950 --> 00:34:02.158
interesting to contribute
into your own environments,

00:34:02.158 --> 00:34:03.510
your own problems.

00:34:03.510 --> 00:34:05.640
We are also very happy
to take proof requests

00:34:05.640 --> 00:34:08.350
and contributions to everything.

00:34:08.350 --> 00:34:11.060
For those of you who say, OK,
those games are really good.

00:34:11.060 --> 00:34:13.790
The games looks nice, but
I have my own problem.

00:34:13.790 --> 00:34:14.717
What do I do?

00:34:14.717 --> 00:34:16.300
So let's go back to
the beginning when

00:34:16.300 --> 00:34:19.179
we talk about you can
define your environment,

00:34:19.179 --> 00:34:20.491
you can define your own task.

00:34:20.491 --> 00:34:22.449
This is the main piece
that you need to follow.

00:34:22.449 --> 00:34:24.370
This is the API
you need to follow

00:34:24.370 --> 00:34:27.460
to bring your task or
your problem to TF-Agents.

00:34:27.460 --> 00:34:30.010
You define the specifications
of your observations,

00:34:30.010 --> 00:34:31.480
like what things can I see?

00:34:31.480 --> 00:34:34.780
Can I see images, can I see
numbers, what that means?

00:34:34.780 --> 00:34:37.030
What actions
available do I have?

00:34:37.030 --> 00:34:39.820
Do I have two options, three
options, 10 different options?

00:34:39.820 --> 00:34:41.980
What are the
possibilities I have?

00:34:41.980 --> 00:34:44.065
And then the reset
method because as we say,

00:34:44.065 --> 00:34:45.940
while we're learning,
we need to keep trying.

00:34:45.940 --> 00:34:48.328
So we need to reset
and start again.

00:34:48.328 --> 00:34:49.870
And then the stop
function where it's

00:34:49.870 --> 00:34:53.159
like, if I give you an
action, what will happen?

00:34:53.159 --> 00:34:55.880
How the environment, how
the task is going to evolve?

00:34:55.880 --> 00:34:58.090
What is this state
is going to change?

00:34:58.090 --> 00:35:00.400
And you need to
tell me the reward.

00:35:00.400 --> 00:35:01.073
Am I doing well?

00:35:01.073 --> 00:35:02.490
Am I going in the
right direction,

00:35:02.490 --> 00:35:04.032
or am I going in
the wrong direction?

00:35:04.032 --> 00:35:05.720
So I can learn from it.

00:35:05.720 --> 00:35:07.120
So this is the
main piece of code

00:35:07.120 --> 00:35:11.260
that you will need to implement
to solve your own problem.

00:35:11.260 --> 00:35:14.020
Additionally, we only talked
about three algorithms,

00:35:14.020 --> 00:35:16.330
but we have many more
in the code base.

00:35:16.330 --> 00:35:19.160
You can see here, there
are many more coming.

00:35:19.160 --> 00:35:21.370
So there's a lot of variety
of different algorithms

00:35:21.370 --> 00:35:22.900
have different
strands that you can

00:35:22.900 --> 00:35:24.530
apply to different problems.

00:35:24.530 --> 00:35:26.500
So you can just try
different combinations

00:35:26.500 --> 00:35:29.170
and see which one actually
works for your problem.

00:35:29.170 --> 00:35:32.560
And also, we are taking
contributions for other people

00:35:32.560 --> 00:35:34.540
who say, oh, I have
this algorithm,

00:35:34.540 --> 00:35:37.165
I want to implement this
one, I have this new idea,

00:35:37.165 --> 00:35:41.670
and maybe you can solve other
problems with your algorithm.

00:35:41.670 --> 00:35:46.250
And furthermore, we also apply
these not only to this game,

00:35:46.250 --> 00:35:49.570
but we apply at Google,
for example, in robotics.

00:35:49.570 --> 00:35:51.850
In this really complex
problem, that we

00:35:51.850 --> 00:35:55.200
have multiple robots trying
to learn how to grasp objects

00:35:55.200 --> 00:35:57.140
and moving to different places.

00:35:57.140 --> 00:35:59.500
So in this case, we have
all these robots just trying

00:35:59.500 --> 00:36:01.870
to grasp and fail
at the beginning.

00:36:01.870 --> 00:36:05.710
And eventually, they learned
like, oh, where is the object?

00:36:05.710 --> 00:36:07.160
How do I move the hand?

00:36:07.160 --> 00:36:09.215
How do I close the gripper
in the proper place?

00:36:09.215 --> 00:36:11.000
And now how do I grasp it?

00:36:11.000 --> 00:36:12.730
And this is a very
complex task you can

00:36:12.730 --> 00:36:15.610
solve with reinforced learning.

00:36:15.610 --> 00:36:18.730
Furthermore, you can also
solve many other problems

00:36:18.730 --> 00:36:21.830
for simple recommender systems
like YouTube recommendations,

00:36:21.830 --> 00:36:24.440
Google Play, Navigation, News.

00:36:24.440 --> 00:36:26.620
Those are many other problems
that you can basically

00:36:26.620 --> 00:36:31.600
say, I want to optimize for my
objective, my long term value.

00:36:31.600 --> 00:36:34.380
Not only the short term, but
like the long term value.

00:36:34.380 --> 00:36:36.167
And that is really
good for that when

00:36:36.167 --> 00:36:37.750
you want to optimize
for the long term

00:36:37.750 --> 00:36:39.042
value, not only the short term.

00:36:41.970 --> 00:36:45.080
Finally, we have
put a lot of effort

00:36:45.080 --> 00:36:46.700
to make this code
available and make

00:36:46.700 --> 00:36:48.580
it usable for a lot of people.

00:36:48.580 --> 00:36:52.220
But at Google, we also
defined these AI principles.

00:36:52.220 --> 00:36:56.120
So when we developed all this
code, we make it available,

00:36:56.120 --> 00:36:57.570
we follow these principles.

00:36:57.570 --> 00:37:01.670
We want to make it sure that it
is used for things that benefit

00:37:01.670 --> 00:37:06.170
the society that doesn't
reinforce unfair bias, that

00:37:06.170 --> 00:37:08.510
doesn't discriminate,
that this built

00:37:08.510 --> 00:37:15.160
for tests for safety,
privacy, in the beginning

00:37:15.160 --> 00:37:16.610
it's accountable.

00:37:16.610 --> 00:37:18.600
We keep very high standards.

00:37:18.600 --> 00:37:20.690
And we also want to
make sure that everybody

00:37:20.690 --> 00:37:23.780
who uses this code also
embraces those principles

00:37:23.780 --> 00:37:25.880
and trying to make it better.

00:37:25.880 --> 00:37:28.540
And there's many applications
we want to pursue.

00:37:28.540 --> 00:37:30.890
We don't want to be
this used for harming

00:37:30.890 --> 00:37:34.760
and all these damaged things
that we know will happen.

00:37:34.760 --> 00:37:38.370
Finally, I want to
thank the whole team.

00:37:38.370 --> 00:37:41.250
You know, it's not just Eugene
and me, we made this happen.

00:37:41.250 --> 00:37:42.770
There's other people behind.

00:37:42.770 --> 00:37:44.863
These are the
TF-Agents over here.

00:37:44.863 --> 00:37:46.280
There's a lot of
contributors that

00:37:46.280 --> 00:37:49.010
have contributed
to the code, and we

00:37:49.010 --> 00:37:50.600
are very proud of
all the work they

00:37:50.600 --> 00:37:52.670
have done to make
this happen, to make

00:37:52.670 --> 00:37:57.780
this possible to be open source
and available for everyone.

00:37:57.780 --> 00:38:01.180
So as we said before,
we want all of you

00:38:01.180 --> 00:38:02.930
to join us in GitHub.

00:38:02.930 --> 00:38:06.830
Go to the web page, download
it, start playing with it.

00:38:06.830 --> 00:38:09.770
A really good place is go to
the collapse on their notebooks

00:38:09.770 --> 00:38:12.470
and say, OK, I want to
try the REINFORCE example,

00:38:12.470 --> 00:38:15.560
I want to try the DQN or
the Soft Actor Critic.

00:38:15.560 --> 00:38:18.400
We have notebooks you
can play, Google Cloud

00:38:18.400 --> 00:38:20.990
will run for you
all these examples.

00:38:20.990 --> 00:38:24.770
And also, you have issues of
pole requests, we welcome.

00:38:24.770 --> 00:38:27.020
So we want you to be
part of their community,

00:38:27.020 --> 00:38:30.290
contribute to make
this a lot better.

00:38:30.290 --> 00:38:31.790
And furthermore,
we are also looking

00:38:31.790 --> 00:38:34.100
for new applications,
what all of you

00:38:34.100 --> 00:38:36.970
can do with these new tools.

00:38:36.970 --> 00:38:39.250
There's a lot of new
problems you can apply this,

00:38:39.250 --> 00:38:42.150
and we are looking
forward to it.

00:38:42.150 --> 00:38:46.340
So thank you very much,
and hope to see you around.

00:38:46.340 --> 00:38:48.440
[APPLAUSE]

00:38:49.040 --> 00:38:52.390
[MUSIC PLAYING]

