WEBVTT

1
00:00:00.690 --> 00:00:01.310
Yeah.

2
00:00:01.310 --> 00:00:04.970
<v 1>In this video will introduce the idea of learning word representations.</v>

3
00:00:06.700 --> 00:00:07.190
<v 0>Yeah.</v>

4
00:00:07.190 --> 00:00:10.910
<v 1>In the previous video we introduced the notion of one hot encoding or 100</v>

5
00:00:10.911 --> 00:00:11.810
representation,

6
00:00:11.811 --> 00:00:15.410
which allows us to get a vector oral representation out of a word.

7
00:00:15.710 --> 00:00:20.090
And the way we do that is that we just create a vector filled with zeros of

8
00:00:20.091 --> 00:00:23.150
size,
corresponding number of words in our vocabulary.

9
00:00:23.390 --> 00:00:27.380
And then we put a single one at the position associated with the idea of a word.

10
00:00:27.650 --> 00:00:29.000
And if we have a sequence of words,

11
00:00:29.001 --> 00:00:32.240
we can concatenate these one hot vectors and a,

12
00:00:32.300 --> 00:00:36.290
this way we can get a virtual representation that is more amenable to machine

13
00:00:36.291 --> 00:00:40.160
learning and and particular usage.
For instance,
in the neural network.

14
00:00:40.910 --> 00:00:43.910
Now unfortunately this vector is going to be very high dimensional and they are

15
00:00:43.911 --> 00:00:47.240
consequences,
uh,
associated with that,
uh,

16
00:00:47.270 --> 00:00:50.390
particular were more vulnerable to overfitting because we're much higher

17
00:00:50.391 --> 00:00:54.500
dimensions.
So we're going to have many more parameters to train.

18
00:00:54.501 --> 00:00:56.930
So we'll need more data for that.
Uh,

19
00:00:56.960 --> 00:01:01.010
there might also be some problems computationally with manipulating such factors

20
00:01:01.011 --> 00:01:04.610
for certain operations.
So now what we'll see in this video,

21
00:01:04.880 --> 00:01:08.900
we'll introduce this fundamental idea that had been introduced in the context of

22
00:01:08.901 --> 00:01:13.901
neural networks for tackling a as an alternative to using the a 100 encoding,

23
00:01:15.000 --> 00:01:17.480
uh,
and which works really well in practice.

24
00:01:19.870 --> 00:01:20.260
<v 0>Yeah.</v>

25
00:01:20.260 --> 00:01:25.260
<v 1>So the idea is that instead of using this fixed representation that makes no</v>

26
00:01:27.101 --> 00:01:28.990
assumption about the similarity of words,

27
00:01:29.350 --> 00:01:33.520
we instead going to try to learn or a representation of vector representation

28
00:01:33.521 --> 00:01:38.520
for words a directly,
which is going to,
uh,
hopefully,
uh,

29
00:01:38.890 --> 00:01:40.480
uh,
model the,

30
00:01:40.481 --> 00:01:44.530
the similarities in tactic in semantic between the words.

31
00:01:44.950 --> 00:01:47.890
So,
um,
what we'll do is that for each word,

32
00:01:47.891 --> 00:01:52.891
we'll learn some sort of mapping that takes the word id and then maps it into a

33
00:01:53.381 --> 00:01:57.490
representation CW,
which was going to be in this case,
a vector.

34
00:01:57.790 --> 00:02:00.520
So for instance,
uh,
in,
uh,

35
00:02:00.670 --> 00:02:05.200
this is just a made up example before the word de,
say it has the id one,

36
00:02:05.380 --> 00:02:10.270
then a CW might give us this vector.
And then for a similar word,

37
00:02:10.360 --> 00:02:15.340
uh,
uh,
which would have in this example,
the,
uh,
id,
uh,

38
00:02:15.370 --> 00:02:20.140
to then it might give us a very similar vector where we see that the values in

39
00:02:20.141 --> 00:02:22.690
the vectors,
I actually quite a similar,

40
00:02:22.691 --> 00:02:26.470
so the square distance you can distance between the two vectors would be,

41
00:02:26.500 --> 00:02:29.260
would be small.
And then again in this made up example,

42
00:02:29.261 --> 00:02:32.680
having be would have a similar vector representation,

43
00:02:32.710 --> 00:02:36.990
cat and dog and so on.
And so,
um,

44
00:02:37.180 --> 00:02:39.430
the idea here we're introducing is decide,
yeah,

45
00:02:39.431 --> 00:02:43.300
we will try to learn what these vectors are.

46
00:02:43.780 --> 00:02:48.430
And a,
ideally we want to learn a representation which maintained similarity that

47
00:02:48.431 --> 00:02:52.510
syntactic and semantic between the words and uh,

48
00:02:52.900 --> 00:02:57.700
uh,
as we'll see,
it's going to be possible to do that in,
uh,

49
00:02:57.880 --> 00:02:59.230
inside of the neural network.

50
00:03:02.500 --> 00:03:05.420
So here's an example of,
uh,

51
00:03:05.470 --> 00:03:09.840
some work representations that were learned with some given that with them by a

52
00:03:09.960 --> 00:03:12.690
blitzer Italian in 2004.
Uh,

53
00:03:12.790 --> 00:03:16.480
so I'm showing this figure here because it really illustrates well this what

54
00:03:16.481 --> 00:03:20.660
we're trying to accomplish here.
Uh,
so the word representation that they learn,

55
00:03:20.661 --> 00:03:23.130
if they were projected in two dimensions,
uh,

56
00:03:23.160 --> 00:03:28.030
they were able to learn representations such that,
uh,
the all the months,

57
00:03:28.320 --> 00:03:33.100
uh,
were,
uh,
associated with the vector into d,
which was,
uh,

58
00:03:33.190 --> 00:03:35.900
in the same region of the two d space.
Uh,

59
00:03:35.901 --> 00:03:39.460
here at learn all the different numbers,
uh,

60
00:03:39.490 --> 00:03:43.300
hear that million and billion all the days of the week.

61
00:03:43.301 --> 00:03:47.380
We're all around the same,
uh,
position again in the two d space.

62
00:03:47.381 --> 00:03:48.820
So that's what we're trying to achieve.

63
00:03:48.821 --> 00:03:53.200
And trying to find a way of actually learning this,
uh,

64
00:03:53.230 --> 00:03:56.110
vectorial space for,
uh,
the different words.

65
00:04:00.250 --> 00:04:01.120
So for now,

66
00:04:01.121 --> 00:04:05.170
just imagine we actually had these word representations then we could use them

67
00:04:05.260 --> 00:04:10.180
instead of the one hot encoding to create the vector representations of a

68
00:04:10.181 --> 00:04:12.550
sequence of words that we could feed to our neural network.

69
00:04:12.970 --> 00:04:15.940
So if you add a problem where we wanted to,
uh,

70
00:04:16.180 --> 00:04:18.790
make a prediction based on a window of 10 words,

71
00:04:18.970 --> 00:04:21.290
so wd one of two w 10,

72
00:04:21.470 --> 00:04:25.710
then similarly as with the a one having coatings,

73
00:04:25.750 --> 00:04:29.170
we could just concatenate the learn word representation.

74
00:04:29.171 --> 00:04:34.171
So we take CW one and then concatenated with CW to see what CW three up to CW

75
00:04:35.570 --> 00:04:39.910
10,
and then we'll use that as our input representation x and then we could feed

76
00:04:39.911 --> 00:04:42.880
that to the neural network.
Um,

77
00:04:43.270 --> 00:04:47.110
now the question is how do we actually learn this,
uh,
uh,

78
00:04:47.140 --> 00:04:50.920
this mapping these vectors for all the different words in our vocabulary?

79
00:04:51.970 --> 00:04:56.860
Well,
DLD,
uh,
is that we actually use gradient descent and the,

80
00:04:56.861 --> 00:04:58.180
we were going to do that does get,

81
00:04:58.210 --> 00:05:02.500
is that we are going to treat these words representations as parameters inside

82
00:05:02.501 --> 00:05:04.720
the neural network parameters that the neural network,

83
00:05:04.930 --> 00:05:08.710
when we are performing backpropagation we'll be able to,
uh,

84
00:05:08.740 --> 00:05:11.320
to train when we do stochastic gradient descent.

85
00:05:11.980 --> 00:05:15.310
So we won't only update the neural network parameters,

86
00:05:15.370 --> 00:05:18.750
we'll also update the vectors,
uh,

87
00:05:18.790 --> 00:05:20.740
in our vector space.

88
00:05:21.550 --> 00:05:24.880
So each representation CW,
uh,

89
00:05:24.910 --> 00:05:29.910
that's is in some given input x is going to be updated by taking its current

90
00:05:31.211 --> 00:05:34.840
value and then subtracting step size or learning,

91
00:05:34.841 --> 00:05:39.841
right time's degradent of whatever loss we're optimizing in our neural network

92
00:05:40.150 --> 00:05:44.890
with respect to that vector that is at the input of the neural network.

93
00:05:45.190 --> 00:05:50.080
And that will give us a new,
uh,
representation,
uh,
which we can use next time.

94
00:05:50.081 --> 00:05:53.140
We,
we use the word ws input to a neural net.

95
00:05:53.620 --> 00:05:57.040
And so instead of just doing backpropagation,
sorry,

96
00:05:57.050 --> 00:05:59.660
stochastic gradient descent on the parameters of their neural network.

97
00:05:59.661 --> 00:06:03.380
The idea here is that to learn the word representations,

98
00:06:03.620 --> 00:06:07.760
we'll also do gradient descent on the word representation themselves.

99
00:06:08.880 --> 00:06:09.070
<v 0>Yeah.</v>

100
00:06:09.070 --> 00:06:12.250
<v 1>So to some of the procedure for learning,
these were representations.</v>

101
00:06:12.400 --> 00:06:16.630
We would first initialize all the word vectors randomly in some way,

102
00:06:16.631 --> 00:06:20.440
much like the parameters of the neural network.
And then,
uh,

103
00:06:20.500 --> 00:06:25.240
every time we would feed a representation as part of the input of the neural

104
00:06:25.241 --> 00:06:30.160
net,
uh,
then during backpropagation would back propagate the gradients,
uh,

105
00:06:30.190 --> 00:06:31.780
towards the word representation.

106
00:06:31.810 --> 00:06:35.930
And then we would update the word representation also based on,
uh,

107
00:06:36.120 --> 00:06:39.070
the standards to a gradient descent,
a rule.

108
00:06:39.550 --> 00:06:43.030
And so as the neural network trains,
it will not only adapt,

109
00:06:43.031 --> 00:06:44.830
it's a connections,

110
00:06:44.831 --> 00:06:49.660
it's hidden units and I'll put it's up with units to perform the task better.

111
00:06:49.661 --> 00:06:53.350
It will also adapt the word representation themselves so that it's better able

112
00:06:53.351 --> 00:06:55.780
to learn the task that it has to solve.

113
00:06:59.650 --> 00:07:00.370
<v 0>Okay.</v>

114
00:07:00.370 --> 00:07:05.140
<v 1>So we'll often represent all of these word representations and store them into a</v>

115
00:07:05.141 --> 00:07:07.960
matrix,
which I'm calling see here in bold.

116
00:07:08.740 --> 00:07:13.540
And essentially the roles of that matrix will curse bond to the word

117
00:07:13.541 --> 00:07:17.520
representation CW.
Now,
uh,

118
00:07:17.620 --> 00:07:22.480
one thing we can notice is that obtaining CW,
so for our word,

119
00:07:22.490 --> 00:07:26.560
IDW obtaining the rug representation if it's stored in a matrix,

120
00:07:26.561 --> 00:07:31.561
c actually just corresponds to taking the 100 vector of that word w and

121
00:07:32.471 --> 00:07:36.490
multiplying it on the left of the Matrix by the Matrix c.

122
00:07:36.690 --> 00:07:37.600
So if you think about it,

123
00:07:37.601 --> 00:07:42.580
if you take a vector filled with Zeros and a single one in some position and you

124
00:07:42.581 --> 00:07:45.580
multiply that by a matrix on its left,

125
00:07:45.820 --> 00:07:50.590
then the result is going to be the a row of the Matrix,

126
00:07:50.950 --> 00:07:55.510
uh,
at the position where we put a one in the one hot factor.

127
00:07:56.560 --> 00:08:01.180
And so another way of viewing this,
uh,
operation of,

128
00:08:01.210 --> 00:08:03.420
of taking a,
uh,

129
00:08:03.650 --> 00:08:08.080
a word id and providing a word,
a representation or vector,

130
00:08:08.380 --> 00:08:12.670
is that we're actually taking the original 100 and coding representation of the

131
00:08:12.671 --> 00:08:17.671
word and we're projecting it on the columns of w.

132
00:08:19.030 --> 00:08:20.230
And in other words,

133
00:08:20.231 --> 00:08:24.490
we're actually doing some sort of dimentionality reduction,

134
00:08:24.670 --> 00:08:29.550
reducing the dimensionality of the original one hut or a 100,
uh,

135
00:08:29.650 --> 00:08:34.360
representation of the words into a lower dimensions,
uh,

136
00:08:34.450 --> 00:08:37.630
all lower dimensional space,
which,
uh,
uh,

137
00:08:37.660 --> 00:08:41.080
is going to be such that we're less likely to over fit for instance.

138
00:08:41.590 --> 00:08:44.740
And um,
that's something we haven't mentioned thus far,

139
00:08:44.741 --> 00:08:46.900
but actually the size of the word representations,

140
00:08:46.901 --> 00:08:49.480
it's something that we can choose a,
we,

141
00:08:49.600 --> 00:08:52.150
it's going to actually be treated as hyper parameter,

142
00:08:52.190 --> 00:08:54.520
much like they're hidden itself.
There are neural network

143
00:08:56.070 --> 00:08:58.770
and also if you view this,
uh,

144
00:08:58.800 --> 00:09:03.360
process of providing a word representation for were w as just this linear

145
00:09:03.361 --> 00:09:04.350
transformation,

146
00:09:04.560 --> 00:09:09.030
then it becomes more obvious why it is that we can actually propagate gradients

147
00:09:09.031 --> 00:09:11.650
and train this matrix.
C,
uh,

148
00:09:11.690 --> 00:09:16.110
it's just a linear transformation of some original vector representation of the,

149
00:09:16.270 --> 00:09:16.560
uh,

150
00:09:16.560 --> 00:09:21.120
words and much like the other Matrix multiplications we do in their neural net,

151
00:09:21.121 --> 00:09:25.350
we have no difficulty back,
properly back propagate ingredients,
uh,

152
00:09:25.410 --> 00:09:29.340
through these vector multiplications.
Now that being said,

153
00:09:29.341 --> 00:09:33.910
this is mainly conceptual.
That is when we implement,
uh,

154
00:09:33.930 --> 00:09:38.550
taking a word IDW and providing a vector,
a CW,

155
00:09:38.850 --> 00:09:39.390
uh,

156
00:09:39.390 --> 00:09:44.250
we really implement that as a lookup table and not as expensive multiplication

157
00:09:44.251 --> 00:09:47.610
of a vector that's mostly zeros with a matrix.
Uh,

158
00:09:47.640 --> 00:09:52.140
specifically one way of doing that is that CW would just return and the rate

159
00:09:52.141 --> 00:09:56.790
that points a to DWF row up the Matrix c.

160
00:09:58.020 --> 00:10:00.660
So that's the main idea for learning word representations.

161
00:10:00.750 --> 00:10:02.940
And in the following videos,

162
00:10:02.941 --> 00:10:06.170
we'll see different approaches that uses,
uh,

163
00:10:06.180 --> 00:10:10.620
that use this idea for a solving natural language processing tasks.

164
00:10:10.680 --> 00:10:15.420
And the first one that we'll see is language modeling a task.

165
00:10:15.780 --> 00:10:19.650
And it's that it's in that context that we're representations have been first

166
00:10:19.651 --> 00:10:23.070
introduced.
So in that context,
it will become,
I think,

167
00:10:23.400 --> 00:10:27.960
a much more concrete what me mean by a word representation.

