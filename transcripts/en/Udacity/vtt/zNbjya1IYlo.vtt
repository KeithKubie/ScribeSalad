WEBVTT
Kind: captions
Language: en

00:00:00.142 --> 00:00:01.490
&gt;&gt; All right, so that gets us to the end

00:00:01.490 --> 00:00:04.939
of talking about VC dimension. So Charles, what did we learn?

00:00:04.939 --> 00:00:06.610
&gt;&gt; Well, we learned about VC dimension.

00:00:06.610 --> 00:00:07.877
&gt;&gt; [LAUGH]

00:00:07.877 --> 00:00:10.310
&gt;&gt; I think that was probably the key thing.

00:00:10.310 --> 00:00:14.220
&gt;&gt; Indeed, which, which was capturing this notion of shattering.

00:00:14.220 --> 00:00:18.300
&gt;&gt; Right. We learned that VC dimension, the

00:00:18.300 --> 00:00:21.890
relationship between VC dimensions, VC dimension and parameters.

00:00:21.890 --> 00:00:25.180
&gt;&gt; Yeah. Very good, and in fact so like we'd even be able to have a

00:00:25.180 --> 00:00:27.160
guess to say, well what if you have a

00:00:27.160 --> 00:00:29.960
neural network and you're thinking of adding additional nodes to

00:00:29.960 --> 00:00:32.240
the hidden layer. What do we suppose that would

00:00:32.240 --> 00:00:34.320
do to the VC dimension of what you can represent?

00:00:34.320 --> 00:00:37.140
&gt;&gt; So, we didn't really talk about it, but it does occur to me when

00:00:37.140 --> 00:00:38.460
you put it that way that it's pretty

00:00:38.460 --> 00:00:41.370
subtle, right? Because it's the, it's not just

00:00:41.370 --> 00:00:43.030
that it's related to the number of parameters.

00:00:43.030 --> 00:00:45.720
It's related to the true number of parameters.

00:00:45.720 --> 00:00:47.640
Because you can always come up with inefficient

00:00:47.640 --> 00:00:50.700
ways to represent your parameters. So, for example,

00:00:50.700 --> 00:00:52.830
if you have a real number I could represent that as

00:00:52.830 --> 00:00:56.320
two parameters. Everything to the left of the decimal point and

00:00:56.320 --> 00:00:58.600
everything to the right of the decimal point, but that doesn't

00:00:58.600 --> 00:01:01.550
make it really two separate parameters. It's still just one parameter.

00:01:01.550 --> 00:01:03.380
&gt;&gt; All right, that's fair.

00:01:03.380 --> 00:01:09.650
&gt;&gt; We also saw how VC relates to the size of the hypothesis space for

00:01:09.650 --> 00:01:15.950
finite hypothesis spaces. And I guess we learned how sample complexity relates

00:01:15.950 --> 00:01:17.480
to VC dimension, and in fact, all

00:01:17.480 --> 00:01:20.000
these things are themselves related. And we learned

00:01:20.000 --> 00:01:23.260
a little bit, or some tricks, or at least went through some examples of how

00:01:23.260 --> 00:01:25.430
to actually compute the VC dimension. In

00:01:25.430 --> 00:01:26.850
particular, we learned that you need to give

00:01:26.850 --> 00:01:30.790
an example to find the lower bound, and you need to prove an upper bound,

00:01:30.790 --> 00:01:35.600
&gt;&gt; Good. And one other thing that I'd want to add to that, is that VC

00:01:35.600 --> 00:01:41.480
dimension captures this notion of PAC Learnability. Cool.

00:01:41.480 --> 00:01:43.180
I think that's enough for one session.

00:01:43.180 --> 00:01:45.080
&gt;&gt; All right. See you next time, Michael. Bye.

00:01:45.080 --> 00:01:46.110
&gt;&gt; All right. See you next time.

