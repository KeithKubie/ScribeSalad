WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:01.220
Now, given a dag and

00:00:01.220 --> 00:00:06.640
a weight estimated execution time, how
can I tell if the dag is good or bad?

00:00:06.640 --> 00:00:08.460
Let's derive an answer.

00:00:08.460 --> 00:00:11.780
First, we'll identify
a metric of goodness, and

00:00:11.780 --> 00:00:14.160
then we will optimize the metric.

00:00:14.160 --> 00:00:16.595
Let's use speedup as our metric.

00:00:16.595 --> 00:00:19.450
Speedup is defined as the best
sequential time divided by

00:00:19.450 --> 00:00:20.390
the parallel time.

00:00:21.490 --> 00:00:26.420
In terms of symbols, I'll use T* to
denote the best sequential time, and

00:00:26.420 --> 00:00:30.150
of course we'll use Tp(n) to denote
the parallel time, as before.

00:00:31.410 --> 00:00:35.380
Now, a quick comment about the numerator
and the denominator of speedup.

00:00:35.380 --> 00:00:39.430
The denominator will, in general, depend
on the work, the span, the problem size,

00:00:39.430 --> 00:00:41.760
and the number of processors.

00:00:41.760 --> 00:00:44.630
The numerator will depend
essentially on the work

00:00:44.630 --> 00:00:46.380
done by the best sequential algorithm.

00:00:47.400 --> 00:00:51.100
So for notational consistency, since I'm
using work in the case of the parallel

00:00:51.100 --> 00:00:55.860
algorithm, I'll use another work
symbol for the sequential algorithm.

00:00:55.860 --> 00:00:59.170
The best sequential time will
essentially always be equal to the best

00:00:59.170 --> 00:00:59.980
sequential work.

00:01:00.980 --> 00:01:04.959
Now, you might be wondering,
why am I saying best sequential time?

00:01:04.959 --> 00:01:07.080
What's special about best?

00:01:07.080 --> 00:01:11.600
The answer, is I'm a good professor,
and I want to make it hard to cheat!

00:01:11.600 --> 00:01:15.290
After all, you can always make
speedup artificially large by choosing

00:01:15.290 --> 00:01:17.730
a terrible baseline.

00:01:17.730 --> 00:01:20.380
If any of you are into marketing,
you know what I'm talking about.

00:01:21.440 --> 00:01:25.120
Now if I give you a PRAM with
P processors then ideally,

00:01:25.120 --> 00:01:28.530
you'd like the parallel
algorithm to be P times faster

00:01:28.530 --> 00:01:29.850
than the best sequential algorithm.

00:01:30.930 --> 00:01:33.760
This condition is
called ideal speedup or

00:01:33.760 --> 00:01:38.250
sometimes linear speedup,
linear scaling, or ideal scaling.

00:01:38.250 --> 00:01:41.970
But basically, they all say you
want the speedup to be linear in p.

00:01:43.530 --> 00:01:46.380
Here I'm using big theta notation
because you usually won't care about

00:01:46.380 --> 00:01:49.230
the constant factors,
at least not at this pencil and

00:01:49.230 --> 00:01:51.870
paper algorithm design stage.

00:01:51.870 --> 00:01:52.420
Now essentially,

00:01:52.420 --> 00:01:57.170
what this says is if p doubles,
then you want Sp to also double.

00:01:57.170 --> 00:02:00.587
Of course, that's an ideal,
we might not always achieve it.

00:02:00.587 --> 00:02:05.410
Let's write speedup in terms of best
sequential work and parallel time.

00:02:05.410 --> 00:02:08.930
Now, in the general case we can use
Brent's Theorem to get an upper bound on

00:02:08.930 --> 00:02:12.530
time and
therefore a lower bound on speedup.

00:02:12.530 --> 00:02:15.430
So let's go ahead and
plug in Brent's Theorem.

00:02:15.430 --> 00:02:20.650
For notational ease, I've dropped this
paren of n in the right hand side.

00:02:20.650 --> 00:02:23.730
Just remember that there's
a dependence on n there.

00:02:23.730 --> 00:02:26.490
Let's do a little bit of algebra on the
right hand side to get it into a form

00:02:26.490 --> 00:02:27.980
that we can analyze more easily.

00:02:29.140 --> 00:02:30.690
Okay, written in this form,

00:02:30.690 --> 00:02:36.180
you can now immediately see what has to
be true in order to get ideal scaling.

00:02:36.180 --> 00:02:39.850
First notice the numerator, which
has the number of processors in it.

00:02:39.850 --> 00:02:43.320
So relative to this goal,
I will pay a penalty,

00:02:43.320 --> 00:02:45.650
which is determined by the denominator.

00:02:45.650 --> 00:02:48.570
In other words,
if I want to get linear scaling,

00:02:48.570 --> 00:02:51.330
I need the denominator to be a constant.

00:02:51.330 --> 00:02:54.640
So what would it take for
this to be true of the denominator?

00:02:54.640 --> 00:02:57.010
Let's look at each term in turn.

00:02:57.010 --> 00:03:01.400
For this term to be constant, the work
of the parallel algorithm has to match

00:03:01.400 --> 00:03:04.140
the work of the best
sequential algorithm.

00:03:04.140 --> 00:03:07.850
This principle is something
we call work-optimality.

00:03:07.850 --> 00:03:11.970
It's a necessary condition
to ensure ideal scaling.

00:03:11.970 --> 00:03:14.960
Intuitively, it prevents
another form of cheating.

00:03:14.960 --> 00:03:18.990
It says that if you get a very parallel
algorithm by dramatically increasing

00:03:18.990 --> 00:03:21.540
the work relative to the best
sequential algorithm,

00:03:21.540 --> 00:03:24.150
then that's actually bad for speedup.

00:03:24.150 --> 00:03:26.760
Now let's look at the other
term in the denominator.

00:03:26.760 --> 00:03:28.290
For this term to be constant,

00:03:28.290 --> 00:03:33.650
it essentially says that p should
be proportional to W* over D.

00:03:33.650 --> 00:03:37.490
So this is similar to the idea of
the average available parallelism.

00:03:37.490 --> 00:03:41.530
And the main difference is that you
have a W* here instead of just W.

00:03:41.530 --> 00:03:44.340
Now there's another way to write this.

00:03:44.340 --> 00:03:48.020
This other way to write it is to
say that W* divided by P has to be

00:03:48.020 --> 00:03:49.070
big omega(D).

00:03:49.070 --> 00:03:52.125
Let's think about this for a second.

00:03:52.125 --> 00:03:56.420
W* over P is basically
the work per processor.

00:03:56.420 --> 00:04:00.580
This expression says that the work
per processor has to grow, and

00:04:00.580 --> 00:04:04.200
in particular it has to grow
proportional to the span.

00:04:04.200 --> 00:04:07.760
And the span, remember,
depends on the problem size, n.

00:04:07.760 --> 00:04:08.350
So in other words,

00:04:08.350 --> 00:04:12.780
this says that the work per processor
has to grow as some function of n.

00:04:12.780 --> 00:04:16.970
In the parallel computing literature,
this is called weak scalability.

00:04:16.970 --> 00:04:21.519
So basically what it says is as you
increase the concurrency of the machine,

00:04:21.519 --> 00:04:23.420
then if you want to get good scaling,

00:04:23.420 --> 00:04:25.895
you might need to increase
the problem size.

00:04:25.895 --> 00:04:27.515
Okay, that was a little messy.

00:04:27.515 --> 00:04:31.780
Let's try to recap the algorithm
design goals you just derived.

00:04:31.780 --> 00:04:35.980
So starting form speedup we set
linear scaling as our goal.

00:04:35.980 --> 00:04:38.210
Now, to achieve linear scaling,

00:04:38.210 --> 00:04:42.950
you derived two fundamental principles
of good parallel algorithm design.

00:04:42.950 --> 00:04:46.890
The first is work-optimality, which says
that the work of the parallel algorithm

00:04:46.890 --> 00:04:50.520
should match the work of
the best sequential algorithm.

00:04:50.520 --> 00:04:53.560
And the second principle
is weak-scalability.

00:04:53.560 --> 00:04:57.130
In one interpretation, it basically
says that the work per processor should

00:04:57.130 --> 00:05:01.500
grow as a function of n, and
that function is determined by the span

