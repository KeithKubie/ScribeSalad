WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:06.030
So Apache spark is another kind of framework for doing big data processing distributed across clusters like MapReduce

00:00:07.270 --> 00:00:15.210
The differences is kind of come in how those computations have done. So for example with spark

00:00:16.900 --> 00:00:19.230
You've got a lot more flexibility in the computations

00:00:19.230 --> 00:00:24.180
So with MapReduce you've got to do map and then reduce like there's no way of getting around it in spark

00:00:24.180 --> 00:00:29.909
They provide like a load of different operations that you can do on the data such as joins between different data structures

00:00:32.529 --> 00:00:36.598
Why would a you spark this purpose is to process large volumes of data

00:00:36.850 --> 00:00:40.200
So it's mainly data that's not going to fit on a single node

00:00:41.290 --> 00:00:45.929
There's also computations that over a large volume of data. You don't want to go through and sequentially data

00:00:45.930 --> 00:00:51.899
And if you've got parts of your computation that are independent of each other and so you can do it on the data items

00:00:52.030 --> 00:00:54.449
Individually, you can split that data across the cluster

00:00:55.390 --> 00:00:56.739
and then

00:00:56.739 --> 00:01:02.279
Do the computations on that single node exactly like with MapReduce you there's the data locality prints?

00:01:02.280 --> 00:01:05.189
but again do the computations on the nodes where the data is stored and

00:01:05.379 --> 00:01:07.768
Then you reduce those results down to what you want

00:01:07.770 --> 00:01:13.110
The main programming structure that you're going to be dealing with is called a resilient distributed datasets

00:01:13.180 --> 00:01:15.209
Which is usually shorter than two RDD

00:01:15.400 --> 00:01:20.759
Which is kind of a collection of objects that spread across a cluster

00:01:20.830 --> 00:01:26.729
But as a programmer when you're dealing with that, you're kind of just interacting with it as if it's on a single node

00:01:26.729 --> 00:01:30.149
So it kind of hidden from you this it's distributed in a spark cluster

00:01:30.150 --> 00:01:31.770
You'll have a driver node

00:01:31.770 --> 00:01:37.110
and then several worker nodes and the driver node is running the main program where kind of has all of the

00:01:37.240 --> 00:01:42.570
Transformations that you want to do to your data and then these kind of get sent out to the worker nodes who then operate that

00:01:42.570 --> 00:01:46.619
On their chunks of data that they have. In fact, the transformations can be similar to MapReduce

00:01:46.619 --> 00:01:54.089
So it still provides the same map function and reduce functions, but then you have additional stuff on top of that. So they like

00:01:54.759 --> 00:01:56.710
Give you a filter operation directly

00:01:56.710 --> 00:01:58.799
so you can do dental to implement that you can just

00:01:58.869 --> 00:02:03.599
Call the filter function on an RDD and say I only want to return objects to which this is true

00:02:04.420 --> 00:02:11.190
so here we've just got a very very simple spark example of just loading in a text file from the local file system and

00:02:11.890 --> 00:02:16.669
We're just going to go through and count the number of occurrences of each word

00:02:16.950 --> 00:02:22.759
This is exactly the same as the MapReduce example. We looked at last time but we're doing this in spark this time

00:02:22.890 --> 00:02:26.029
Okay, so at the start we set off a spark config

00:02:26.030 --> 00:02:31.280
So we just set the app name which allows us to seize the which of our jobs is currently running within the web UI

00:02:31.409 --> 00:02:37.339
We then set the spark master. So because we're running this locally on a single computer. That's just local

00:02:37.340 --> 00:02:43.609
We then set up a spark context which gives us access to like the spark functions for dealing with rdd's

00:02:43.639 --> 00:02:45.979
We first of all need to load our data into an RDD

00:02:45.980 --> 00:02:52.789
so we do this using need text file function and that puts the contents of that text file into an RDD the RDD you can

00:02:53.519 --> 00:02:57.709
Kind of just view it as like an array if you want to it's been like an array

00:02:58.319 --> 00:02:59.909
distributed across the cluster

00:02:59.909 --> 00:03:04.819
So here we've got our lines RDD each element in the RDD is a single line from the text file

00:03:04.819 --> 00:03:07.819
We then go through and split each line

00:03:08.400 --> 00:03:14.840
Using the flat map function so that map's a single function over every single item in the dataset. So every line

00:03:15.450 --> 00:03:22.369
We go through is split it up into words and then because we're using flat map that then takes that from an RDD of arrays

00:03:22.680 --> 00:03:25.669
to an RDD of strings again we

00:03:26.250 --> 00:03:29.509
Then go through and exactly the same as in the Map Reduce example

00:03:29.510 --> 00:03:30.479
We use the map function

00:03:30.479 --> 00:03:36.199
Sum up each word to a key value pair where the key is the word and then the value is the value 1 so indicating

00:03:36.199 --> 00:03:38.329
We've got one instance of that word at that point

00:03:38.549 --> 00:03:42.228
That then gives us a new RTD and for that one we go reduce by key

00:03:42.479 --> 00:03:48.199
instead of it's a Map Reduce that would just be reduced but here in SPARC if we just did reduce it would give us a

00:03:48.199 --> 00:03:52.939
single value for the entire RDD back at the driver reduced by key takes

00:03:53.639 --> 00:03:56.149
an ID D of key value pairs and

00:03:56.699 --> 00:04:01.399
For each key you give it a function to apply to those values for how you want it to be combined

00:04:01.400 --> 00:04:06.019
So for us we want to add up the number of those instances of that word that we have

00:04:06.090 --> 00:04:09.679
so we use just a simple + to

00:04:10.379 --> 00:04:17.569
Aggregate those values so that finally gives us our word count RDD which contains key value pairs of words and a number of instances

00:04:17.570 --> 00:04:22.909
Of those words. And so we then called the collect function which will bring that back to the

00:04:23.580 --> 00:04:29.740
Driver node, and then for each one of those lines we've in them out. So we right now the counts all those words

00:04:29.740 --> 00:04:34.059
So this at the moment that code is written for something that might be on your own computer

00:04:34.780 --> 00:04:39.760
How would it differ if it was on a cluster and I'm server farm or a massive data center or something like that?

00:04:39.910 --> 00:04:41.210
How would that vary?

00:04:41.210 --> 00:04:46.479
And so if you're running this on an actual cluster and not just on your local computer

00:04:47.870 --> 00:04:49.040
then

00:04:49.040 --> 00:04:53.110
Rather than setting master within your code and setting it to run locally

00:04:53.390 --> 00:05:00.159
What you do is you would have SPARC running on the cluster and you would use something called spark submit submit your spark jobs to

00:05:00.160 --> 00:05:01.580
Spark to then be run

00:05:01.580 --> 00:05:04.119
So it would it's just a different way of running them

00:05:04.130 --> 00:05:09.460
Basically rather than hard coding it within your program even though you're their money on the cluster

00:05:09.460 --> 00:05:11.420
The rest of the code would be the same

00:05:11.420 --> 00:05:18.670
so the work that I've done with spark has been kind of using it to analyze large volumes of telematics data coming off of

00:05:18.980 --> 00:05:22.869
Lorries as they're driving around and using the data from that to identify

00:05:23.420 --> 00:05:28.719
Locations where incidents are occurring such as if they're cornering harshly or breaking harshly

00:05:29.450 --> 00:05:32.890
Outside of research what sorts of things is spark we use. Yes

00:05:32.890 --> 00:05:36.039
So Sparky is used quite a lot in the real world

00:05:36.040 --> 00:05:36.440
like

00:05:36.440 --> 00:05:42.760
You would find a lot of companies will be using it to kind of do large-scale jobs and all of the data that they have

00:05:43.070 --> 00:05:50.379
and it can be used for like analysis or simply just processing that data and putting it into storage the good thing about the

00:05:50.630 --> 00:05:55.779
Distributed computing in clusters is that if you want to scale the program more you just add

00:05:56.600 --> 00:05:57.980
more

00:05:57.980 --> 00:06:03.610
Like nodes to the cluster. So the point is if you want to increase your processing power you don't have to

00:06:04.250 --> 00:06:07.600
Buy new hardware in terms of replacing your hardware

00:06:07.600 --> 00:06:08.540
You keep your old hardware

00:06:08.540 --> 00:06:13.629
You buy new nodes and just stick them on the end and you've instantly increased how much processing power you have

00:06:13.820 --> 00:06:19.390
So if you suddenly say get a load more data that you need to be posting. You think Oh miss current cluster size

00:06:19.610 --> 00:06:21.610
That's not great. We can

00:06:21.680 --> 00:06:24.940
Then expand that and then just add a few more nodes

00:06:24.940 --> 00:06:32.140
so the SPARC program would then just scale automatically going back to RDDs these are immutable data structures so that

00:06:32.930 --> 00:06:39.130
Immutable means they can't be changed. Right? Is that right? Yes. Yeah. So yeah, and they're immutable they cannot be changed once they're created

00:06:39.680 --> 00:06:44.200
You can pass them to other functions, but you can't change the contents of that single RDD

00:06:44.419 --> 00:06:47.259
so what the spark program ends up being is kind of a chain of

00:06:47.570 --> 00:06:52.779
Transformations with each one creating a new RDD and passing it on to the next function

00:06:52.820 --> 00:06:57.999
The advantage of the RDDs is that they can be persisted in memory

00:06:58.000 --> 00:07:05.529
Which means that then it's more efficient to reuse them later in the computation. So one of the disadvantages of hadoop mapreduce, is that you

00:07:06.169 --> 00:07:13.329
It's every time you're writing and stuff to disk basically after your MapReduce computation if you want to reuse it

00:07:13.330 --> 00:07:15.669
You've then got to go and get it from disk again

00:07:16.639 --> 00:07:19.449
Whereas with SPARC you can just persist the rdd's in memory

00:07:19.450 --> 00:07:22.569
If you want to come back to them later, then you can do it really easily

00:07:24.050 --> 00:07:28.720
You're saying large amount volumes of data. Can we put some numbers on this? Well, what are we looking at here?

00:07:29.000 --> 00:07:36.519
See the volumes of data we're talking about can vary I guess depending on company. It's probably ranging gigabytes to terabytes and then the biggest

00:07:37.100 --> 00:07:39.369
We then just keep going up basically

