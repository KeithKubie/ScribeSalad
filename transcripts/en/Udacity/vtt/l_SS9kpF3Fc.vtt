WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:03.860
So we talked about the equations that defined expectation maximization,

00:00:03.860 --> 00:00:06.930
and we stepped through an example with some actual data

00:00:06.930 --> 00:00:08.760
in the sense that it was data points. They weren't

00:00:08.760 --> 00:00:11.632
actual, measured data points. But what I'd like to talk

00:00:11.632 --> 00:00:13.810
about now for a moment is some of the properties

00:00:13.810 --> 00:00:17.600
of the EM algorithm more generally. So one of the

00:00:17.600 --> 00:00:20.630
things that's good about it is that each time the

00:00:20.630 --> 00:00:26.085
iteration of EM runs, the likelihood of the data is monotonically

00:00:26.085 --> 00:00:28.620
non-decreasing, right? So it's not getting worse.

00:00:28.620 --> 00:00:30.830
Generally it's finding higher and higher likelihoods

00:00:30.830 --> 00:00:35.730
and it's kind of moving in a good direction that way. Unfortunately, even though

00:00:35.730 --> 00:00:39.650
that's true, it is not the case that the algorithm has to converge. I

00:00:39.650 --> 00:00:42.540
mean have you ever seen it not converge? I've never seen it not converge.

00:00:42.540 --> 00:00:44.770
&gt;&gt; No, because usually there's some kind of step

00:00:44.770 --> 00:00:46.280
that you take and you just, you make the

00:00:46.280 --> 00:00:48.570
weight lower and lower. So yeah, or something like

00:00:48.570 --> 00:00:50.810
that. You know, no, I've never seen it not converge.

00:00:50.810 --> 00:00:51.830
&gt;&gt; So it

00:00:51.830 --> 00:00:53.380
doesn't have to converge. I think you can

00:00:53.380 --> 00:00:55.850
construct really strange examples that make it do

00:00:55.850 --> 00:00:57.500
that. But on the other hand, so even

00:00:57.500 --> 00:01:00.320
though it doesn't converge, it can't diverge, right?

00:01:00.320 --> 00:01:02.030
It can't be that these numbers blow up

00:01:02.030 --> 00:01:04.400
and become infinitely large because it really is

00:01:04.400 --> 00:01:07.150
working in the space of probabilities. And it's,

00:01:07.150 --> 00:01:08.980
it's pretty well behaved as far as that's concerned.

00:01:08.980 --> 00:01:12.460
&gt;&gt; That's a difference between in k means, right? So, the argument for

00:01:12.460 --> 00:01:16.810
k means, if I recall, which feels like was about a week ago,

00:01:16.810 --> 00:01:19.980
when we talked about this. [LAUGH] But of course, it was, it was merely seconds

00:01:19.980 --> 00:01:22.264
ago. Is that there is a finite number

00:01:22.264 --> 00:01:24.748
of configurations and k means and since you

00:01:24.748 --> 00:01:28.364
are never getting worse in our error metric. So long as you have some way

00:01:28.364 --> 00:01:30.260
of breaking ties, eventually you have to stop.

00:01:30.260 --> 00:01:31.970
And, so, that's how you got convergence, right?

00:01:31.970 --> 00:01:32.670
&gt;&gt; Yeah.

00:01:32.670 --> 00:01:37.920
&gt;&gt; So, in EM, I guess the configurations are probabilities and I guess

00:01:37.920 --> 00:01:40.800
there is an infinite number of those. Yet you can do more than guess.

00:01:40.800 --> 00:01:41.820
&gt;&gt; So in fact,

00:01:41.820 --> 00:01:43.070
there are an infinite number of those.

00:01:43.070 --> 00:01:44.090
&gt;&gt; Exactly.

00:01:44.090 --> 00:01:47.410
&gt;&gt; It wouldn't necessarily follow that you wouldn't converge

00:01:47.410 --> 00:01:50.050
from that. But that alone is, is one big difference,

00:01:50.050 --> 00:01:52.390
I guess, between the k means and the, the

00:01:52.390 --> 00:01:53.910
EM. That's the trade off you get for being able

00:01:53.910 --> 00:01:57.460
to put probabilities on things. So you've got an

00:01:57.460 --> 00:02:01.640
infinite number of configurations. You never do worse but you're

00:02:01.640 --> 00:02:04.430
always trying to move closer and closer. So, I guess

00:02:04.430 --> 00:02:06.960
what could happen is you could keep moving closer every

00:02:06.960 --> 00:02:10.070
single time, but because there's a infinite number of configurations,

00:02:10.070 --> 00:02:13.500
the step by which you get better could keep getting smaller,

00:02:13.500 --> 00:02:17.150
and so you never actually approach the final best configuration. I

00:02:17.150 --> 00:02:20.120
suppose that's possible. But for all intents and purposes, it converges.

00:02:20.120 --> 00:02:23.310
&gt;&gt; Right. Exactly. However, it can get stuck. And this

00:02:23.310 --> 00:02:26.010
you see all the time. I almost never not see it

00:02:26.010 --> 00:02:29.590
do this. Which is to say that, if there's multiple

00:02:29.590 --> 00:02:32.040
ways of assigning things to clusters, it could find a way

00:02:32.040 --> 00:02:35.630
that doesn't have very good likelihood but can't really improve

00:02:35.630 --> 00:02:38.410
on very well. So there's a local optima problem that

00:02:38.410 --> 00:02:40.190
is pretty common, and so what do we do when

00:02:40.190 --> 00:02:43.850
we, get stuck in local optima with a randomized algorithm?

00:02:43.850 --> 00:02:44.300
&gt;&gt; Cry.

00:02:44.300 --> 00:02:45.480
&gt;&gt; No.

00:02:45.480 --> 00:02:48.210
&gt;&gt; We take all of our toys home and randomly restart.

00:02:48.210 --> 00:02:48.870
&gt;&gt; There we go.

00:02:48.870 --> 00:02:49.540
&gt;&gt; Okay.

00:02:49.540 --> 00:02:51.980
&gt;&gt; And the last thing to mention is this, is, is what you

00:02:51.980 --> 00:02:57.160
just suggested a moment ago in the previous slide. Which is that, it's nothing,

00:02:57.160 --> 00:02:59.720
there's nothing specific about Gaussians in here. It really is an

00:02:59.720 --> 00:03:02.810
algorithm that can be applied anytime that we can work with

00:03:02.810 --> 00:03:06.770
probability distribution. And so there's just a ton of different algorithms

00:03:06.770 --> 00:03:08.440
that work in different scenarios

00:03:08.440 --> 00:03:11.090
by defining different probability distributions, and

00:03:11.090 --> 00:03:13.010
then all you have to do is figure out what the

00:03:13.010 --> 00:03:15.930
E step and the M step are. How do you expectation

00:03:15.930 --> 00:03:19.070
to work out the probability of the latent variables. And then,

00:03:19.070 --> 00:03:22.470
how do you do maximization to use those latent variables to

00:03:22.470 --> 00:03:25.930
estimate parameters? And usually it's the case

00:03:25.930 --> 00:03:28.260
that it's the estimation that's expensive. It's

00:03:28.260 --> 00:03:31.530
difficult because it involves probabilistic inference. Right?

00:03:31.530 --> 00:03:33.320
So it's just like Bayes net stuff.

00:03:33.320 --> 00:03:33.690
&gt;&gt; Mm.

00:03:33.690 --> 00:03:35.450
&gt;&gt; And the maximization is often just,

00:03:35.450 --> 00:03:37.360
you know, counting things. But it isn't,

00:03:37.360 --> 00:03:40.805
in general, the case that it's always harder to do E than M. There's some

00:03:40.805 --> 00:03:45.220
well-known examples where M is hard and E is actually quite easy. You know, for

00:03:45.220 --> 00:03:47.640
any given problem you have some work to do to derive what the E and

00:03:47.640 --> 00:03:49.510
the M steps are. But it's very general, it's a

00:03:49.510 --> 00:03:51.310
really it's a good tool to have in your toolbox.

00:03:51.310 --> 00:03:53.230
&gt;&gt; I like that. So, basically it's not that

00:03:53.230 --> 00:03:55.440
hard because it's just a small matter of math programming.

00:03:55.440 --> 00:03:56.300
&gt;&gt; Indeed.

