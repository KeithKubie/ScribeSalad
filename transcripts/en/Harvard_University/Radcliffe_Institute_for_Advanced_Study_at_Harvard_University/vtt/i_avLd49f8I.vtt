WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:04.275
[MUSIC PLAYING]

00:00:04.275 --> 00:00:06.520
- Good afternoon.

00:00:06.520 --> 00:00:07.780
Thank you for coming.

00:00:07.780 --> 00:00:12.320
And thank you for the very,
very gracious introduction.

00:00:12.320 --> 00:00:17.980
So as Meredith pointed out,
as computers and computer

00:00:17.980 --> 00:00:22.090
algorithms reach ever more
deeply into our lives,

00:00:22.090 --> 00:00:26.500
there is an increasing
concern that they

00:00:26.500 --> 00:00:31.620
embody societal values, such
as privacy, and fairness,

00:00:31.620 --> 00:00:34.710
and statistical
validity, and so on.

00:00:34.710 --> 00:00:38.320
And my work does
strive to try to put

00:00:38.320 --> 00:00:41.830
these goals on a firm
mathematical foundation.

00:00:41.830 --> 00:00:44.980
And my hope today is to
give you a little bit

00:00:44.980 --> 00:00:49.480
of the flavor of how someone
like me goes about trying

00:00:49.480 --> 00:00:51.460
to answer a question like this.

00:00:51.460 --> 00:00:53.830
So what is some of my thinking?

00:00:53.830 --> 00:00:57.310
I'll give two-- if time
permits, three examples.

00:00:57.310 --> 00:01:02.350
And I will include work that
is very much in progress

00:01:02.350 --> 00:01:04.340
right now.

00:01:04.340 --> 00:01:08.910
So we are going to in the
end use coins for fairness.

00:01:08.910 --> 00:01:13.120
But my goal in talking about
them right this very minute

00:01:13.120 --> 00:01:18.370
is to just give you a little bit
of the language of probability

00:01:18.370 --> 00:01:21.130
in case this is foreign to
you because not everybody here

00:01:21.130 --> 00:01:24.110
has had a probability class.

00:01:24.110 --> 00:01:27.130
So what do we mean
by a fair coin?

00:01:27.130 --> 00:01:30.430
Most of you would
say, well, it'll

00:01:30.430 --> 00:01:32.770
come up heads with
probability a half.

00:01:32.770 --> 00:01:35.170
What does that statement mean?

00:01:35.170 --> 00:01:38.020
Implicit in it is the idea
that you have some sort

00:01:38.020 --> 00:01:40.000
of repeatable experiment.

00:01:40.000 --> 00:01:41.800
You can flip the
coin many times.

00:01:41.800 --> 00:01:45.190
If you flip it 1,000
times, your belief--

00:01:45.190 --> 00:01:47.680
your guess-- is
that if it's fair,

00:01:47.680 --> 00:01:52.540
it should come up
about 500 heads and not

00:01:52.540 --> 00:01:54.350
too far away from that.

00:01:54.350 --> 00:01:58.810
So that's one view of the
definition of a fair coin.

00:01:58.810 --> 00:02:04.180
And you might wonder
if you have a coin

00:02:04.180 --> 00:02:06.500
can you test for fairness.

00:02:06.500 --> 00:02:08.280
And the answer is sure.

00:02:08.280 --> 00:02:10.350
And you have to think about why.

00:02:10.350 --> 00:02:13.150
But you can ask me about
that later if you want.

00:02:13.150 --> 00:02:15.810
But when we talk about
whether an algorithm is fair,

00:02:15.810 --> 00:02:17.890
you'll also maybe be
thinking, can I actually

00:02:17.890 --> 00:02:19.210
test for fairness?

00:02:19.210 --> 00:02:22.540
So it's the same
kind of exercise.

00:02:22.540 --> 00:02:27.850
So very, very briefly, we need a
sample space of our experiment.

00:02:27.850 --> 00:02:30.940
So the sample space is simply
all the possible outcomes.

00:02:30.940 --> 00:02:33.040
If I flip a coin
once, the sample space

00:02:33.040 --> 00:02:34.790
is just head or tail.

00:02:34.790 --> 00:02:35.290
That's it.

00:02:35.290 --> 00:02:36.250
It's the pair.

00:02:36.250 --> 00:02:38.740
Those are the two elements
in the sample space.

00:02:38.740 --> 00:02:43.260
If I roll a single die, there
are six possible outcomes.

00:02:43.260 --> 00:02:46.570
If I'm looking at
medicine, then I

00:02:46.570 --> 00:02:51.610
may be concerned with how many
months will somebody survive

00:02:51.610 --> 00:02:53.770
in a five-year follow-up study.

00:02:53.770 --> 00:02:57.220
So the number of
answers there is 61.

00:02:57.220 --> 00:03:00.580
It could be 0, 1, 2, 3
months, 4 months, and so on,

00:03:00.580 --> 00:03:01.860
up to 60 months.

00:03:01.860 --> 00:03:05.180
So notice that when we had
the die and we rolled it,

00:03:05.180 --> 00:03:08.080
the repeatable experiment
was the multiple roles.

00:03:08.080 --> 00:03:11.650
When it comes to number of
months survived in a follow-up

00:03:11.650 --> 00:03:16.660
study, that's not one person
who's being tested many times.

00:03:16.660 --> 00:03:17.980
It's many people.

00:03:17.980 --> 00:03:19.720
So we have to
understand a little bit

00:03:19.720 --> 00:03:23.260
about where our repetitions
are coming from.

00:03:23.260 --> 00:03:25.150
So that's the sample
space of an experiment.

00:03:25.150 --> 00:03:30.370
Oh, and there are also
uncountably infinite sets.

00:03:30.370 --> 00:03:33.070
If you're used to thinking
about a draw from a Gaussian

00:03:33.070 --> 00:03:36.550
distribution, that would be
an uncountably infinite set.

00:03:36.550 --> 00:03:39.010
We won't need that today.

00:03:39.010 --> 00:03:42.920
Now, not all outcomes have
the same probabilities.

00:03:42.920 --> 00:03:44.590
If I flipped a fair
coin, of course,

00:03:44.590 --> 00:03:48.160
it would be 50/50-- the
same probability-- the same

00:03:48.160 --> 00:03:50.370
with a die-- one in six.

00:03:50.370 --> 00:03:52.600
But suppose I flip two coins.

00:03:52.600 --> 00:03:57.340
So the number of heads in
the two flips of a fair coin

00:03:57.340 --> 00:04:00.170
is going to be 0, 1, or 2.

00:04:00.170 --> 00:04:02.590
But they don't all have
the same probability.

00:04:02.590 --> 00:04:05.140
And the best way
to think about this

00:04:05.140 --> 00:04:09.220
is to think about
flipping two fair coins.

00:04:09.220 --> 00:04:11.650
We'll call one of them the
green coin and one of them

00:04:11.650 --> 00:04:12.310
the blue coin.

00:04:12.310 --> 00:04:14.840
And they can be flipped
in either order.

00:04:14.840 --> 00:04:19.959
So there's a 50/50 chance
of 0 or 1 on the green coin.

00:04:19.959 --> 00:04:24.070
And then independently there's
a 50/50 chance of 0 or 1

00:04:24.070 --> 00:04:25.690
on the blue coin.

00:04:25.690 --> 00:04:29.110
So each of the paths
through this diagram

00:04:29.110 --> 00:04:31.240
has probability one fourth.

00:04:31.240 --> 00:04:34.780
But two of those paths
result in one head.

00:04:34.780 --> 00:04:39.070
So it's twice as
likely that we'll

00:04:39.070 --> 00:04:42.100
see one head than that
we'll see zero heads.

00:04:42.100 --> 00:04:45.550
That seems so obvious to us.

00:04:45.550 --> 00:04:47.740
But this device of
thinking about the blue

00:04:47.740 --> 00:04:49.090
and the green coin--

00:04:49.090 --> 00:04:51.100
this wasn't even
known to Leibniz,

00:04:51.100 --> 00:04:54.446
who developed calculus.

00:04:54.446 --> 00:04:56.340
So a few hundred years ago--

00:04:56.340 --> 00:05:00.340
350 years ago or so-- people
didn't understand this so well.

00:05:00.340 --> 00:05:03.390
Now it's absolutely
bread and butter.

00:05:03.390 --> 00:05:06.720
We'll need the notion of a
probability distribution, which

00:05:06.720 --> 00:05:11.370
is simply a list for each
possible outcome of how

00:05:11.370 --> 00:05:12.480
probable it is.

00:05:12.480 --> 00:05:16.740
So these are numbers between
0 and 1 that sum to 1.

00:05:16.740 --> 00:05:19.980
And that's the basic tutorial.

00:05:19.980 --> 00:05:23.970
Notice that sometimes
we really have

00:05:23.970 --> 00:05:30.480
no way of knowing which
outcome is the case.

00:05:30.480 --> 00:05:33.880
And that doesn't mean that
they're equally likely.

00:05:33.880 --> 00:05:36.000
So for example, we
might talk about what's

00:05:36.000 --> 00:05:39.660
our belief about the probability
of intelligent life on Mars.

00:05:39.660 --> 00:05:42.600
Either there is intelligent
life on Mars or there isn't.

00:05:42.600 --> 00:05:44.880
Just because we don't
know which is which

00:05:44.880 --> 00:05:47.010
doesn't mean that
they're equally likely.

00:05:47.010 --> 00:05:49.360
Given all of the other
things we know about life,

00:05:49.360 --> 00:05:52.190
they will not have
equal likelihood.

00:05:52.190 --> 00:05:56.440
And the last notion we'll need
is conditional probability.

00:05:56.440 --> 00:05:59.650
So I could talk about what's
the probability of rain.

00:05:59.650 --> 00:06:02.070
Well, already you should
say, what do you mean,

00:06:02.070 --> 00:06:03.450
what's the probability of rain?

00:06:03.450 --> 00:06:06.330
What's my sample space
am I talking about--

00:06:06.330 --> 00:06:08.270
days in the year in Cambridge?

00:06:08.270 --> 00:06:12.270
Am I talking about days
in the year in California?

00:06:12.270 --> 00:06:16.180
Am I talking about days in
Cambridge Union, California?

00:06:16.180 --> 00:06:18.420
So let's say
Cambridge in October--

00:06:18.420 --> 00:06:21.120
what's the probability of rain
on a random day in Cambridge

00:06:21.120 --> 00:06:22.480
in October?

00:06:22.480 --> 00:06:26.040
And what's the probability
of rain given that we

00:06:26.040 --> 00:06:27.550
observe that it's cloudy.

00:06:27.550 --> 00:06:32.430
So conditional probability talks
about how we update our beliefs

00:06:32.430 --> 00:06:34.360
in the light of evidence.

00:06:34.360 --> 00:06:36.390
And this is
absolutely fundamental

00:06:36.390 --> 00:06:37.560
to the scientific method.

00:06:40.760 --> 00:06:43.034
There are mathematical
ways of doing it.

00:06:43.034 --> 00:06:44.450
But you should
just know that this

00:06:44.450 --> 00:06:47.594
is what conditional
probability will do for us.

00:06:47.594 --> 00:06:50.180
OK, so now you're
armed with the language

00:06:50.180 --> 00:06:52.896
that you'll need to understand
at least the definition

00:06:52.896 --> 00:06:53.895
of differential privacy.

00:06:57.450 --> 00:07:01.150
Here's for a moment going to
be my computational model.

00:07:01.150 --> 00:07:05.140
I have a data analyst who
interacts with a database

00:07:05.140 --> 00:07:07.360
in a very abstract way.

00:07:07.360 --> 00:07:10.600
She asks a question like--

00:07:10.600 --> 00:07:12.040
what happened to Q1--

00:07:12.040 --> 00:07:13.960
she asks a question
like, what is

00:07:13.960 --> 00:07:15.640
the fraction of
people in the data

00:07:15.640 --> 00:07:18.582
set that are over six feet tall?

00:07:18.582 --> 00:07:19.540
And she gets an answer.

00:07:19.540 --> 00:07:23.290
The fraction is 40%.

00:07:23.290 --> 00:07:28.210
And then based on that, she
might choose a new question.

00:07:28.210 --> 00:07:31.534
How many people like muffin tops
and gets an approximate answer

00:07:31.534 --> 00:07:32.950
to that or gets
an answer to that.

00:07:32.950 --> 00:07:36.370
So that's our model of a
data analyst interacting

00:07:36.370 --> 00:07:38.260
with the database.

00:07:38.260 --> 00:07:41.600
The question could be something
very, very complicated.

00:07:41.600 --> 00:07:43.790
It could be a multi-part study.

00:07:43.790 --> 00:07:47.170
It could be a
computer algorithm.

00:07:47.170 --> 00:07:48.125
It doesn't matter.

00:07:48.125 --> 00:07:50.500
We're just going to abstract
it away-- question, answer--

00:07:50.500 --> 00:07:53.890
question, answer-- first
heart study on the data

00:07:53.890 --> 00:07:56.330
set-- second heart
study on the data set,

00:07:56.330 --> 00:07:58.870
which is informed by the
outcomes of the first heart

00:07:58.870 --> 00:08:00.420
study, and so on.

00:08:00.420 --> 00:08:04.490
OK, so this is a
very general model.

00:08:04.490 --> 00:08:06.580
It's used in some sense--

00:08:06.580 --> 00:08:09.700
you can imagine in the
census an epidemic detection

00:08:09.700 --> 00:08:12.790
based on over-the-counter
drug purchases.

00:08:12.790 --> 00:08:15.820
You might want to analyze
loan application data

00:08:15.820 --> 00:08:19.492
for evidence of discrimination.

00:08:19.492 --> 00:08:22.630
So there can be all sorts
of rich and juicy things

00:08:22.630 --> 00:08:23.500
in this data set.

00:08:23.500 --> 00:08:27.400
And we want to do these things
while preserving privacy.

00:08:27.400 --> 00:08:30.220
Now, this is a problem
that's quite old.

00:08:30.220 --> 00:08:33.789
And very specifically
we can find some results

00:08:33.789 --> 00:08:36.380
on these sorts of
things from 1965.

00:08:36.380 --> 00:08:38.950
And we'll talk about that later.

00:08:38.950 --> 00:08:42.309
Now, as Meredith
mentioned, many people

00:08:42.309 --> 00:08:44.350
think about de-identifying data.

00:08:44.350 --> 00:08:46.840
So the dream in
de-identified data

00:08:46.840 --> 00:08:51.850
is you start with a database
and some algorithm, M,

00:08:51.850 --> 00:08:54.190
chomps on the
database for a while,

00:08:54.190 --> 00:08:57.220
stripping out certain things,
and averaging and aggregating

00:08:57.220 --> 00:09:00.850
others, and produces a
de-identified data set.

00:09:00.850 --> 00:09:02.520
That's the hope.

00:09:02.520 --> 00:09:05.410
And in fact, once that
happens, we don't even

00:09:05.410 --> 00:09:07.000
need the original database.

00:09:07.000 --> 00:09:08.500
The algorithm can go away.

00:09:08.500 --> 00:09:10.390
And the analyst can
knock herself out

00:09:10.390 --> 00:09:14.280
by looking at the
de-identified data set.

00:09:14.280 --> 00:09:18.040
And the problem with this
approach is that a number

00:09:18.040 --> 00:09:22.460
of studies have shown that
de-identified data isn't.

00:09:22.460 --> 00:09:28.020
Either it isn't de-identified,
or it is no longer data.

00:09:28.020 --> 00:09:28.823
So it's just--

00:09:28.823 --> 00:09:30.940
[LAUGHTER]

00:09:30.940 --> 00:09:32.500
So you might say,
well, why don't we

00:09:32.500 --> 00:09:35.110
just restrict ourselves
to statistical queries?

00:09:35.110 --> 00:09:37.680
Now, this is really intuitive.

00:09:37.680 --> 00:09:41.530
A statistic is a quantity
computed from a sample.

00:09:41.530 --> 00:09:44.410
The reason statistics
works as a field

00:09:44.410 --> 00:09:47.650
is it really doesn't care
whom you have sampled.

00:09:47.650 --> 00:09:49.450
It just cares that
you've sort of

00:09:49.450 --> 00:09:52.000
sampled enough
people with enough

00:09:52.000 --> 00:09:54.598
of the right distribution.

00:09:54.598 --> 00:09:57.890
So that seems really
privacy preserving.

00:09:57.890 --> 00:10:00.150
And yet there are problems.

00:10:00.150 --> 00:10:03.350
So a simple example would
be a differencing attack.

00:10:03.350 --> 00:10:05.250
Suppose we ask the
following question.

00:10:07.910 --> 00:10:10.880
How many living
physics Nobel laureates

00:10:10.880 --> 00:10:12.410
floss their teeth regularly?

00:10:15.150 --> 00:10:16.950
We get the exact answer to this.

00:10:16.950 --> 00:10:19.930
And then we ask the
question, how many

00:10:19.930 --> 00:10:24.820
male, living, physics Nobel
laureates floss regularly?

00:10:24.820 --> 00:10:27.270
So both of these are
pretty large sets.

00:10:27.270 --> 00:10:32.490
There are a quite a number of
male, living, physics Nobel

00:10:32.490 --> 00:10:33.670
laureates.

00:10:33.670 --> 00:10:37.140
But there's only one female,
living, physics Nobel

00:10:37.140 --> 00:10:37.680
laureate--

00:10:37.680 --> 00:10:40.480
Donna Strickland--
as of October 2.

00:10:40.480 --> 00:10:43.770
And so if you took one
answer and subtracted it

00:10:43.770 --> 00:10:47.304
from the other, you would
learn whether or not

00:10:47.304 --> 00:10:51.240
Donna Strickland flosses
regularly, which of course,

00:10:51.240 --> 00:10:53.900
is a very personal
piece of information.

00:10:53.900 --> 00:10:55.710
So this is a simple example.

00:10:55.710 --> 00:10:56.880
It's a simple attack.

00:10:56.880 --> 00:11:01.680
But one thing you
should worry about--

00:11:01.680 --> 00:11:03.900
one thing we've learned
from cryptography

00:11:03.900 --> 00:11:06.900
is if anybody finds a way
of breaking something,

00:11:06.900 --> 00:11:10.800
there are probably a ton of
other ways of breaking that

00:11:10.800 --> 00:11:14.670
and simple variations of it, and
complicated variations of it.

00:11:14.670 --> 00:11:17.610
Like, it's always the
tip of the iceberg.

00:11:17.610 --> 00:11:20.070
And indeed, what we
call the fundamental law

00:11:20.070 --> 00:11:21.420
of information recovery--

00:11:21.420 --> 00:11:22.980
I missed the citation here--

00:11:22.980 --> 00:11:26.940
the first version was due to
Dinur and Nissim in 2003--

00:11:26.940 --> 00:11:32.120
essentially says that
overly accurate estimates

00:11:32.120 --> 00:11:37.110
of too many statistics
completely destroys privacy.

00:11:37.110 --> 00:11:39.560
And the definition
of overly accurate

00:11:39.560 --> 00:11:42.380
is tied to the
definition of too many.

00:11:42.380 --> 00:11:46.700
So if we're going
to break privacy

00:11:46.700 --> 00:11:49.970
and we can ask lots
and lots, then we

00:11:49.970 --> 00:11:52.220
can make them pretty
inaccurate and still

00:11:52.220 --> 00:11:53.750
manage to break privacy.

00:11:53.750 --> 00:11:56.630
If we're going to just be
able to ask a few questions,

00:11:56.630 --> 00:12:01.190
then to prove this theorem,
you need pretty tight accuracy.

00:12:01.190 --> 00:12:04.725
So those two are connected.

00:12:04.725 --> 00:12:09.500
OK, so here we are at Radcliffe.

00:12:09.500 --> 00:12:14.870
This is a picture of
Cecilia Payne-Gaposchkin.

00:12:14.870 --> 00:12:18.500
So maybe the astronomers
know who she is.

00:12:18.500 --> 00:12:22.920
She is the first woman to
obtain a PhD in astronomy

00:12:22.920 --> 00:12:28.650
and the first person to obtain
a PhD in astronomy at Harvard.

00:12:28.650 --> 00:12:33.650
She was responsible for
estimating the great abundance

00:12:33.650 --> 00:12:38.431
of hydrogen in
stars and realizing

00:12:38.431 --> 00:12:40.430
that the stars have a
very different composition

00:12:40.430 --> 00:12:43.190
from the Earth.

00:12:43.190 --> 00:12:46.580
OK, so here is Cecilia Payne.

00:12:46.580 --> 00:12:48.180
She's in the data set.

00:12:48.180 --> 00:12:51.200
And the analyst is
talking to the data set.

00:12:51.200 --> 00:12:57.510
And the question is, what does
privacy mean for Dr. Payne?

00:12:57.510 --> 00:13:00.410
So what are the
guarantees or the promises

00:13:00.410 --> 00:13:02.891
that we'd like to make to her?

00:13:02.891 --> 00:13:07.820
The one possibility would be
to say the analyst interacting

00:13:07.820 --> 00:13:11.030
with the database can't learn
anything new about Cecilia

00:13:11.030 --> 00:13:12.580
Payne.

00:13:12.580 --> 00:13:18.870
And this is in fact the
heart of the desideratum that

00:13:18.870 --> 00:13:21.030
was articulated by
the statistician

00:13:21.030 --> 00:13:25.320
Tordelinius in 1977 when the
statistics community was trying

00:13:25.320 --> 00:13:30.470
to get its hands on what is
a statistical database-- what

00:13:30.470 --> 00:13:32.220
does it mean that it's
statistical and not

00:13:32.220 --> 00:13:34.950
about individuals.

00:13:34.950 --> 00:13:37.980
So the problem
with that is if we

00:13:37.980 --> 00:13:40.870
can't learn anything new about
the people in the database,

00:13:40.870 --> 00:13:41.910
what is the point?

00:13:41.910 --> 00:13:44.040
So suppose that
I'm from Mars and I

00:13:44.040 --> 00:13:48.230
believe that all humans
have two left feet.

00:13:48.230 --> 00:13:51.030
And they interact with
the statistical database.

00:13:51.030 --> 00:13:54.600
And I learn that the
vast majority of humans

00:13:54.600 --> 00:13:57.320
have one left foot
and one right foot.

00:13:57.320 --> 00:14:03.090
So I used to believe that
Dr. Payne had two left feet.

00:14:03.090 --> 00:14:05.010
Erroneously, I believed this.

00:14:05.010 --> 00:14:06.700
And then I interacted
with the database.

00:14:06.700 --> 00:14:09.160
And now my beliefs
about her have changed.

00:14:09.160 --> 00:14:12.000
I've learned something
new about her.

00:14:12.000 --> 00:14:16.478
Have I compromised
her privacy or not?

00:14:16.478 --> 00:14:20.560
Well, suppose she had
not been in the data set.

00:14:20.560 --> 00:14:22.275
Suppose she had opted out.

00:14:22.275 --> 00:14:25.920
And she'd been replaced in
the data set by Henrietta Swan

00:14:25.920 --> 00:14:29.250
Leavitt, another
Harvard astronomer, who

00:14:29.250 --> 00:14:34.530
observed a certain mathematical
relationship between the period

00:14:34.530 --> 00:14:40.450
of variable stars and
their peak brightnesses.

00:14:40.450 --> 00:14:42.900
And this relationship
turned out to be

00:14:42.900 --> 00:14:48.408
very valuable for estimating
distances across space.

00:14:48.408 --> 00:14:53.080
So presumably, because this
is a statistical database,

00:14:53.080 --> 00:14:56.170
we would learn the
same things if Payne

00:14:56.170 --> 00:14:59.130
had been replaced by Leavitt.

00:14:59.130 --> 00:15:01.670
All we needed was
enough people to learn

00:15:01.670 --> 00:15:04.250
that the vast majority of people
have one left foot and one

00:15:04.250 --> 00:15:05.120
right foot.

00:15:05.120 --> 00:15:07.760
So ideally, we'd
learn the same things

00:15:07.760 --> 00:15:09.980
if Payne is replaced by
another random member

00:15:09.980 --> 00:15:12.720
of the population.

00:15:12.720 --> 00:15:16.400
And that's what differential
privacy captures.

00:15:16.400 --> 00:15:19.880
It says in English that
the outcome of any analysis

00:15:19.880 --> 00:15:23.750
is essentially equally
likely, independent of

00:15:23.750 --> 00:15:26.330
whether any individual
joins or refrains

00:15:26.330 --> 00:15:27.860
from joining the data set.

00:15:27.860 --> 00:15:31.910
In other words, the probability
distribution on outcomes

00:15:31.910 --> 00:15:36.050
is very robust to small
changes in the data set.

00:15:36.050 --> 00:15:37.520
Payne goes away.

00:15:37.520 --> 00:15:38.390
Leavitt joins.

00:15:38.390 --> 00:15:40.190
Payne's replaced by Leavitt.

00:15:40.190 --> 00:15:42.980
Annie Jump Cannon joins.

00:15:42.980 --> 00:15:45.170
So you've seen some
of these things--

00:15:45.170 --> 00:15:46.510
these terms.

00:15:46.510 --> 00:15:50.130
The likelihood here-- what's
our repeatable experiment?

00:15:50.130 --> 00:15:53.150
Our algorithms are
going to flip coins.

00:15:53.150 --> 00:15:55.430
They're going to
introduce randomness.

00:15:55.430 --> 00:15:58.760
And when we talk about
measuring likelihood,

00:15:58.760 --> 00:16:01.670
we mean with the
database fixed--

00:16:01.670 --> 00:16:05.720
likelihood as we vary the
coins of the algorithm.

00:16:05.720 --> 00:16:08.360
So you could run it many
times with different coins.

00:16:08.360 --> 00:16:11.990
And you'd get probability
distribution and outcomes.

00:16:11.990 --> 00:16:14.510
And essentially,
equally likely says

00:16:14.510 --> 00:16:17.870
simply that similar
databases should give rise

00:16:17.870 --> 00:16:21.950
to similar probability
distributions on outputs.

00:16:21.950 --> 00:16:25.420
And that's basically
the definition.

00:16:25.420 --> 00:16:29.540
So what it means is if a bad
event from my perspective

00:16:29.540 --> 00:16:32.510
is very unlikely when
I'm not in the data set,

00:16:32.510 --> 00:16:35.120
it's going to be
very unlikely even

00:16:35.120 --> 00:16:38.480
if I am because the probability
distribution on the outputs

00:16:38.480 --> 00:16:42.230
will not change much, whether
I'm in there or not in there.

00:16:42.230 --> 00:16:45.890
So if one is analyzing
power consumption

00:16:45.890 --> 00:16:49.460
data and a bad outcome for me
is that the police come and bust

00:16:49.460 --> 00:16:53.810
me for maybe use of
grow lights, well,

00:16:53.810 --> 00:16:56.630
if that's very unlikely to
happen if my data are not

00:16:56.630 --> 00:16:59.390
included, then it will
be unlikely to happen

00:16:59.390 --> 00:17:02.420
if they are included.

00:17:02.420 --> 00:17:05.060
The next example has to do
with genome wide association

00:17:05.060 --> 00:17:05.569
studies.

00:17:05.569 --> 00:17:06.650
I'm going to skip it now.

00:17:06.650 --> 00:17:09.380
But ask me about both of these
examples during the questions

00:17:09.380 --> 00:17:13.000
if you wish.

00:17:13.000 --> 00:17:15.630
So what differential
privacy does

00:17:15.630 --> 00:17:20.240
is it allows us to
separate harms that

00:17:20.240 --> 00:17:23.849
can happen to us based on
the teachings of the database

00:17:23.849 --> 00:17:28.040
from harms that happen to us
as a result of our choosing

00:17:28.040 --> 00:17:30.000
to participate in the database.

00:17:30.000 --> 00:17:34.190
So let's think about a medical
data set where we are analyzing

00:17:34.190 --> 00:17:35.870
the effects of smoking.

00:17:35.870 --> 00:17:38.360
And we learn by
analyzing this data set

00:17:38.360 --> 00:17:40.760
that smoking causes cancer.

00:17:40.760 --> 00:17:43.970
From the perspective of
a smoker in the data set,

00:17:43.970 --> 00:17:47.667
let's say, in some
sense, I mean, it's

00:17:47.667 --> 00:17:50.000
really good news because the
smoker then learns to quit.

00:17:50.000 --> 00:17:53.480
But for a moment, let's say that
this is bad news for the smoker

00:17:53.480 --> 00:17:59.118
because it means her insurance
premiums are going to rise.

00:17:59.118 --> 00:18:04.350
So what the differential
privacy ensures

00:18:04.350 --> 00:18:06.390
is that those same
teachings would

00:18:06.390 --> 00:18:09.930
be learned independent of
whether she participated

00:18:09.930 --> 00:18:11.030
or not.

00:18:11.030 --> 00:18:18.060
Now, of course, the whole point
of a medical database for study

00:18:18.060 --> 00:18:21.060
is to learn things like
smoking causes cancer precisely

00:18:21.060 --> 00:18:23.640
so that people
learn not to smoke

00:18:23.640 --> 00:18:26.720
or that they should enter a
smoking cessation program.

00:18:26.720 --> 00:18:30.930
So the hope was that by having
very strong privacy protection

00:18:30.930 --> 00:18:33.600
that makes things
essentially the same

00:18:33.600 --> 00:18:36.240
in terms of their
privacy loss risk,

00:18:36.240 --> 00:18:38.310
whether you are
participating or not,

00:18:38.310 --> 00:18:42.180
that you would feel
safe to participate.

00:18:42.180 --> 00:18:44.570
So it's a strong
definition of privacy.

00:18:44.570 --> 00:18:47.270
And you might wonder, why
is anything possible at all?

00:18:47.270 --> 00:18:50.090
How can you possibly get
any utility whatsoever

00:18:50.090 --> 00:18:53.240
when you have such a
strong privacy guarantee?

00:18:53.240 --> 00:19:00.670
And one thing that we've
learned from machine learning--

00:19:00.670 --> 00:19:04.120
from that whole field of
AI and machine learning--

00:19:04.120 --> 00:19:08.170
is that stability to small
changes in the data set

00:19:08.170 --> 00:19:11.230
allows you to
properly generalize,

00:19:11.230 --> 00:19:15.790
which means to port the
things that you have

00:19:15.790 --> 00:19:18.190
learned by analyzing
your data set

00:19:18.190 --> 00:19:20.602
to the population as a whole.

00:19:20.602 --> 00:19:22.920
That's the whole point
of machine learning.

00:19:22.920 --> 00:19:25.620
And it's also the whole point
of statistical learning.

00:19:25.620 --> 00:19:28.600
And so stability is both
necessary and sufficient

00:19:28.600 --> 00:19:29.160
for that.

00:19:29.160 --> 00:19:32.200
So stability or
differential privacy

00:19:32.200 --> 00:19:36.190
will preserve Payne's
privacy and protect

00:19:36.190 --> 00:19:38.210
against over-fitting
of the data.

00:19:38.210 --> 00:19:42.550
So in some very real
sense, privacy and utility

00:19:42.550 --> 00:19:46.510
are actually aligned,
as opposed to being

00:19:46.510 --> 00:19:47.860
in conflict with each other.

00:19:51.580 --> 00:19:54.050
So here's a simple example.

00:19:54.050 --> 00:19:56.240
It's not quite in the
model that we discussed.

00:19:56.240 --> 00:19:59.360
But it's just a simple example
that actually dates back

00:19:59.360 --> 00:20:00.680
to 1965.

00:20:00.680 --> 00:20:03.350
But we're going to analyze
it in modern terms.

00:20:03.350 --> 00:20:07.760
So it was developed by Warner
in order to question people--

00:20:07.760 --> 00:20:10.880
survey people-- about
embarrassing or even illegal

00:20:10.880 --> 00:20:11.840
behaviors.

00:20:11.840 --> 00:20:14.930
It's going to give
plausible deniability.

00:20:14.930 --> 00:20:17.440
So the question is, did
you floss last night?

00:20:17.440 --> 00:20:19.160
So what I'm interested
in is the fraction

00:20:19.160 --> 00:20:21.690
of people in the room
who flossed last night.

00:20:21.690 --> 00:20:24.680
And we could carry out this
study in a privacy preserving

00:20:24.680 --> 00:20:26.690
way as follows.

00:20:26.690 --> 00:20:28.760
I would give you the
following instructions.

00:20:28.760 --> 00:20:31.743
And I would tell you to carry
them out very privately.

00:20:31.743 --> 00:20:32.610
OK.

00:20:32.610 --> 00:20:35.826
So flip a fair coin.

00:20:35.826 --> 00:20:37.840
If that coin is
heads, you're going

00:20:37.840 --> 00:20:40.060
to answer completely
randomly having

00:20:40.060 --> 00:20:41.320
nothing to do with the truth.

00:20:41.320 --> 00:20:42.130
You flip again.

00:20:42.130 --> 00:20:44.980
And you respond yes if
your second coin came up

00:20:44.980 --> 00:20:47.170
heads and no otherwise.

00:20:47.170 --> 00:20:50.470
But if the outcome of
the first coin was tails,

00:20:50.470 --> 00:20:53.750
you're going to answer honestly.

00:20:53.750 --> 00:20:57.070
So you can see that
whether the truth is yes,

00:20:57.070 --> 00:21:00.520
you flossed, or no, you
didn't, in both cases,

00:21:00.520 --> 00:21:02.350
you might end up saying yes.

00:21:02.350 --> 00:21:05.800
And in both cases, you
might end up saying no.

00:21:05.800 --> 00:21:09.752
So this is where the plausible
deniability comes from.

00:21:09.752 --> 00:21:13.240
And the privacy analysis
is straightforward.

00:21:13.240 --> 00:21:14.890
The probability
that you say yes,

00:21:14.890 --> 00:21:16.780
given that the truth is yes--

00:21:16.780 --> 00:21:19.027
we're interested in this ratio--

00:21:19.027 --> 00:21:20.860
divided by the probability
that you say yes,

00:21:20.860 --> 00:21:22.210
given that the truth is no.

00:21:22.210 --> 00:21:24.036
That's going to be 3.

00:21:24.036 --> 00:21:29.540
If the truth is yes, you say
yes if the first coin is tails--

00:21:29.540 --> 00:21:30.980
probability in half.

00:21:30.980 --> 00:21:33.140
Or the first coin is
heads, and the second coin

00:21:33.140 --> 00:21:35.060
is heads--
probability a quarter.

00:21:35.060 --> 00:21:37.530
Total probability is 3/4.

00:21:37.530 --> 00:21:40.220
If the truth is
no, you'll say yes

00:21:40.220 --> 00:21:42.080
only if the first
and second coins

00:21:42.080 --> 00:21:44.580
are heads, which
as we saw earlier,

00:21:44.580 --> 00:21:46.940
has probability, a fourth.

00:21:46.940 --> 00:21:50.740
So the ratio between them is 3--

00:21:50.740 --> 00:21:54.340
same deal for outputting no.

00:21:54.340 --> 00:21:57.760
So it turns out
it's very, very easy

00:21:57.760 --> 00:22:00.100
to reverse engineer
the noise that we've

00:22:00.100 --> 00:22:02.320
added by all of
this coin flipping

00:22:02.320 --> 00:22:04.300
and get quite nice
approximations

00:22:04.300 --> 00:22:07.210
to the true fraction of
people in the room who've

00:22:07.210 --> 00:22:09.946
flossed last night.

00:22:09.946 --> 00:22:15.820
And if I want to have better
privacy protection, what I'll

00:22:15.820 --> 00:22:22.840
do is I'll flip a coin that
has a very small probability

00:22:22.840 --> 00:22:26.020
of tails, so you're
very unlikely to have

00:22:26.020 --> 00:22:27.100
to answer honestly.

00:22:27.100 --> 00:22:29.440
Most of the time
most of the people

00:22:29.440 --> 00:22:32.030
will end up answering randomly.

00:22:32.030 --> 00:22:34.660
So you have to recalculate
the privacy analysis.

00:22:34.660 --> 00:22:38.740
But you can force that
ratio to be very close to 1

00:22:38.740 --> 00:22:44.240
by answering honestly with
much smaller probability.

00:22:44.240 --> 00:22:45.180
So let's go back.

00:22:45.180 --> 00:22:48.350
Remember that
conditional probability

00:22:48.350 --> 00:22:52.450
let us say how we're going to
update our beliefs in light

00:22:52.450 --> 00:22:54.410
of the evidence.

00:22:54.410 --> 00:22:58.650
So now suppose the evidence
is Meredith has flipped

00:22:58.650 --> 00:23:00.520
and she says no.

00:23:00.520 --> 00:23:03.130
Well, it's not
very good evidence

00:23:03.130 --> 00:23:07.510
because maybe if I had
chosen my parameters nicely,

00:23:07.510 --> 00:23:09.670
I'd be essentially
equally likely to see

00:23:09.670 --> 00:23:13.090
that no whether she
flossed last night or not.

00:23:13.090 --> 00:23:16.240
So in this updating
of the beliefs,

00:23:16.240 --> 00:23:18.805
very little is changing.

00:23:18.805 --> 00:23:22.500
There's very little information.

00:23:22.500 --> 00:23:26.730
OK, and for the mathematicians
here, this is the definition.

00:23:26.730 --> 00:23:29.540
Adjacent data sets are
data sets that differ

00:23:29.540 --> 00:23:32.320
in the data of just one person.

00:23:32.320 --> 00:23:37.520
And we say that our algorithm
gives us epsilon differential

00:23:37.520 --> 00:23:40.970
privacy if this equation
holds where the probability

00:23:40.970 --> 00:23:45.000
spaces over the randomness
introduced by our algorithms.

00:23:45.000 --> 00:23:47.600
And this parameter
epsilon here is

00:23:47.600 --> 00:23:50.030
a measure of the
privacy loss, or it's

00:23:50.030 --> 00:23:53.078
an upper bound on
the privacy loss.

00:23:53.078 --> 00:23:56.020
OK, couple properties of this--

00:23:56.020 --> 00:23:57.290
it's future proof.

00:23:57.290 --> 00:24:01.600
So let's say that we run
a differentially private

00:24:01.600 --> 00:24:03.400
algorithm from this.

00:24:03.400 --> 00:24:07.340
Even if we knew which were the
two possible adjacent data sets

00:24:07.340 --> 00:24:09.760
and all we care
about is learning

00:24:09.760 --> 00:24:13.480
about the data of the
one person who is not

00:24:13.480 --> 00:24:18.010
in both of those
databases, no matter

00:24:18.010 --> 00:24:20.230
what we learn in
the future won't

00:24:20.230 --> 00:24:25.040
erode the privacy guarantee
that we got at the beginning.

00:24:25.040 --> 00:24:28.540
So you can't leverage
this to make things worse

00:24:28.540 --> 00:24:30.500
in post-processing.

00:24:30.500 --> 00:24:33.670
Also, we understand how
differentially private

00:24:33.670 --> 00:24:35.110
algorithms compose.

00:24:35.110 --> 00:24:37.300
That's the formal
term for saying,

00:24:37.300 --> 00:24:40.330
what is the cumulative
privacy loss as we

00:24:40.330 --> 00:24:43.030
run through one differentially
private algorithm

00:24:43.030 --> 00:24:46.420
after another with their
own privacy parameters?

00:24:46.420 --> 00:24:48.705
So we understand
how it composes.

00:24:48.705 --> 00:24:53.140
It composes gracefully,
automatically, and adaptively.

00:24:53.140 --> 00:24:55.990
So the fact that the
adversary or the analyst

00:24:55.990 --> 00:24:59.560
get to ask her questions
based on previous questions

00:24:59.560 --> 00:25:01.975
and answers doesn't
harm anything.

00:25:01.975 --> 00:25:04.360
There may or may not
be time to tell you

00:25:04.360 --> 00:25:06.820
about the importance of
that observation later.

00:25:09.990 --> 00:25:13.500
As a result, because we
understand composition,

00:25:13.500 --> 00:25:16.260
we can now program in a
differentially private way.

00:25:16.260 --> 00:25:19.620
What that means is we can
create little differentially

00:25:19.620 --> 00:25:24.000
private building blocks-- little
basic, you know, primitives.

00:25:24.000 --> 00:25:27.450
And now we can put them together
in creative and interesting

00:25:27.450 --> 00:25:31.050
ways to carry out
complicated computations

00:25:31.050 --> 00:25:36.130
while trying to minimize
the cumulative privacy loss.

00:25:36.130 --> 00:25:39.750
And that is really what sets
differential privacy aside

00:25:39.750 --> 00:25:45.420
from all other techniques
because in some sense

00:25:45.420 --> 00:25:49.180
potentially the sky's the limit.

00:25:49.180 --> 00:25:52.254
So now we get to fairness.

00:25:52.254 --> 00:25:56.470
The concern in algorithmic
fairness is the following.

00:25:56.470 --> 00:25:58.660
We have a population
which is diverse.

00:25:58.660 --> 00:26:01.350
There are ethnic divisions,
religious divisions,

00:26:01.350 --> 00:26:04.760
geographic, medical, gender,
class, sexual preference,

00:26:04.760 --> 00:26:07.136
and all sorts of things.

00:26:07.136 --> 00:26:10.460
And what we want is we
want to have algorithms

00:26:10.460 --> 00:26:12.620
that are somehow fair.

00:26:12.620 --> 00:26:14.900
Now, we will spend
some time thinking

00:26:14.900 --> 00:26:16.580
about how to define fair.

00:26:16.580 --> 00:26:18.530
But you all probably
have some sort

00:26:18.530 --> 00:26:21.866
of intuitive notion
of what fair means.

00:26:21.866 --> 00:26:27.140
Now, how would you be sure
that your algorithm is not,

00:26:27.140 --> 00:26:29.790
say, discriminating
on the basis of sex?

00:26:29.790 --> 00:26:34.550
So one possibility might be to
hide the sensitive information

00:26:34.550 --> 00:26:37.550
from the algorithm-- just not
have that information as one

00:26:37.550 --> 00:26:38.760
of the inputs--

00:26:38.760 --> 00:26:42.650
not have it in the description
of [? Cynthia, ?] for example.

00:26:42.650 --> 00:26:44.180
And that doesn't work.

00:26:44.180 --> 00:26:47.180
And the reason is
sensitive attributes

00:26:47.180 --> 00:26:50.960
can be sort of holographically
embedded in our data.

00:26:50.960 --> 00:26:54.650
And a classic example of
this is the correlation

00:26:54.650 --> 00:26:58.430
between race and zip
code, which comes up

00:26:58.430 --> 00:27:06.260
in redlining, which was an
illegal practice in the housing

00:27:06.260 --> 00:27:10.970
loan industry where
zip codes were

00:27:10.970 --> 00:27:13.850
being used as proxies for race.

00:27:13.850 --> 00:27:17.270
And I strongly recommend
reading Rothstein's book,

00:27:17.270 --> 00:27:21.440
The Color of Law, talking about
the history of this and showing

00:27:21.440 --> 00:27:25.250
that this was du jour--
not just de facto--

00:27:25.250 --> 00:27:29.660
and that it was a result of
government actions, which has

00:27:29.660 --> 00:27:32.502
a lot of implications legally.

00:27:32.502 --> 00:27:33.494
OK.

00:27:33.494 --> 00:27:37.550
Now, another problem
with the idea

00:27:37.550 --> 00:27:39.590
of hiding the
sensitive information

00:27:39.590 --> 00:27:44.430
is that algorithms that
have more information

00:27:44.430 --> 00:27:46.480
can do a better job.

00:27:46.480 --> 00:27:50.310
So suppose you have
a minority group

00:27:50.310 --> 00:27:57.280
in which hearing voices is a
common religious experience.

00:27:57.280 --> 00:27:59.520
But in the majority
community, hearing voices

00:27:59.520 --> 00:28:02.523
is diagnostic of schizophrenia.

00:28:02.523 --> 00:28:05.860
So given somebody
is hearing voices,

00:28:05.860 --> 00:28:08.030
one of the first things
you'd want to know is, hey,

00:28:08.030 --> 00:28:09.640
is this a normal
religious experience

00:28:09.640 --> 00:28:14.600
for you or in your community
before jumping to conclusions.

00:28:14.600 --> 00:28:18.440
So a culturally aware algorithm
will be more accurate.

00:28:18.440 --> 00:28:20.410
Now, I'm going to
introduce my majority

00:28:20.410 --> 00:28:24.100
and my minority groups
for the rest of the talk.

00:28:24.100 --> 00:28:26.350
So the population
is divided according

00:28:26.350 --> 00:28:29.770
to what herbs we like to
use for flavoring our food.

00:28:29.770 --> 00:28:31.810
And the majority
likes to use thyme.

00:28:31.810 --> 00:28:34.000
And the minority
likes to use sage.

00:28:34.000 --> 00:28:38.003
And they'll be referred to
as T and S, respectively.

00:28:41.720 --> 00:28:43.940
Many algorithms in
machine learning,

00:28:43.940 --> 00:28:46.820
particularly supervised
machine learning,

00:28:46.820 --> 00:28:51.390
are given historical
examples to learn from.

00:28:51.390 --> 00:28:54.670
So we will have
data and outcomes--

00:28:54.670 --> 00:28:58.430
for example, loan applications
and whether or not

00:28:58.430 --> 00:29:01.130
the loan was granted.

00:29:01.130 --> 00:29:05.180
And the goal of the
algorithm is to figure out

00:29:05.180 --> 00:29:09.530
to the best of its
ability how to imitate

00:29:09.530 --> 00:29:12.120
what has been done in the past.

00:29:12.120 --> 00:29:15.500
So you can see that if
there were biased decisions

00:29:15.500 --> 00:29:21.200
in the past, then this bias will
be just imbibed in the mother's

00:29:21.200 --> 00:29:23.340
milk of training data.

00:29:23.340 --> 00:29:29.440
And so you have to do something
to watch out for this.

00:29:29.440 --> 00:29:32.430
And this brings up a
really serious problem,

00:29:32.430 --> 00:29:36.080
which is in general, when
we think about these things,

00:29:36.080 --> 00:29:39.320
we don't have a source
of ground truth.

00:29:39.320 --> 00:29:41.450
We know what was
done in the past

00:29:41.450 --> 00:29:43.630
or how things would have
been done in the past.

00:29:43.630 --> 00:29:46.427
But we don't know what should
be done-- what's the right thing

00:29:46.427 --> 00:29:48.510
to be done-- what's the
accurate thing to be done.

00:29:52.450 --> 00:29:54.120
So I'm going to
focus specifically

00:29:54.120 --> 00:29:57.640
on classification algorithms.

00:29:57.640 --> 00:30:01.650
So this is from
the Sistine Chapel.

00:30:01.650 --> 00:30:03.570
This is God deciding
who's going to heaven

00:30:03.570 --> 00:30:04.590
and who's going to hell.

00:30:04.590 --> 00:30:07.080
It's a binary classification.

00:30:07.080 --> 00:30:07.980
[LAUGHTER]

00:30:07.980 --> 00:30:10.290
But there's also risk
assessment scoring--

00:30:10.290 --> 00:30:11.570
a scoring notion.

00:30:11.570 --> 00:30:16.140
So in this case, the goal is
to classify arrested children

00:30:16.140 --> 00:30:18.510
according to some
measure of risk.

00:30:18.510 --> 00:30:20.700
Children are
scored, for example,

00:30:20.700 --> 00:30:24.690
based on their current offense--

00:30:24.690 --> 00:30:26.660
their previous offenses.

00:30:26.660 --> 00:30:29.970
They'll have points
deducted if there

00:30:29.970 --> 00:30:31.410
are mitigating circumstances.

00:30:31.410 --> 00:30:35.580
But they end up
with a score that

00:30:35.580 --> 00:30:39.864
is supposed to say something
about how likely they are to--

00:30:39.864 --> 00:30:41.280
I don't know exactly
what the risk

00:30:41.280 --> 00:30:43.240
is that's being measured here.

00:30:43.240 --> 00:30:48.600
So sometimes these
scores are translated

00:30:48.600 --> 00:30:51.000
into binary decisions.

00:30:51.000 --> 00:30:52.920
In this case, the
binary decision

00:30:52.920 --> 00:30:56.340
is whether you should detain
the child or release the child.

00:30:59.004 --> 00:31:00.760
So you can have
scoring algorithms.

00:31:00.760 --> 00:31:04.800
They can be turned into binary
classification algorithms.

00:31:04.800 --> 00:31:07.640
So now let's try some
definitions of fairness

00:31:07.640 --> 00:31:10.580
with these motivating
scenarios in mind.

00:31:10.580 --> 00:31:14.070
Broadly speaking, there are two
general kinds of definitions.

00:31:14.070 --> 00:31:15.710
One is group fairness.

00:31:15.710 --> 00:31:17.810
And one is individual fairness.

00:31:17.810 --> 00:31:20.000
So group fairness is
the first thing that

00:31:20.000 --> 00:31:21.450
will come to everybody's mind.

00:31:21.450 --> 00:31:24.750
So we'll give some examples
or at least one example.

00:31:24.750 --> 00:31:28.010
These are statistical
properties about the treatment

00:31:28.010 --> 00:31:29.450
of the groups as a whole.

00:31:29.450 --> 00:31:33.140
So one of them, which I called
here a statistical parity--

00:31:33.140 --> 00:31:36.170
it's also called demographic
parity in the literature--

00:31:36.170 --> 00:31:40.880
says that, say, in the binary
classification setting,

00:31:40.880 --> 00:31:44.750
the demographics of the people
with a positive classification

00:31:44.750 --> 00:31:47.420
should be the same as
the demographics of those

00:31:47.420 --> 00:31:49.260
in the general population.

00:31:49.260 --> 00:31:51.259
If a third of your
population is sage eating,

00:31:51.259 --> 00:31:53.300
then a third of the people
that you give loans to

00:31:53.300 --> 00:31:54.640
should be sage eating.

00:31:54.640 --> 00:31:56.900
And similarly, a third of
the people that you deny

00:31:56.900 --> 00:31:58.310
should be sage eating.

00:31:58.310 --> 00:32:03.450
That's demographic parity, a
very normal, natural notion.

00:32:03.450 --> 00:32:07.970
For the most part, it's like
a red flag in the breach.

00:32:07.970 --> 00:32:13.430
So if you find that
the sage eaters are

00:32:13.430 --> 00:32:16.790
being disproportionately
turned down for loans,

00:32:16.790 --> 00:32:20.780
then maybe it's an
indication of bias.

00:32:20.780 --> 00:32:23.700
And you might want to
investigate further.

00:32:23.700 --> 00:32:26.900
But it's not necessarily
so meaningful

00:32:26.900 --> 00:32:30.800
when we actually have
demographic parity.

00:32:30.800 --> 00:32:33.470
So one example would
be in an advertising

00:32:33.470 --> 00:32:37.910
setting in which let's say
you're advertising a spa.

00:32:37.910 --> 00:32:42.020
And you really don't want
the sage eaters at your spa.

00:32:42.020 --> 00:32:44.720
But you have to advertise
to them in proportion

00:32:44.720 --> 00:32:45.780
for the population.

00:32:45.780 --> 00:32:48.500
So what you do is you advertise
to the sage eaters who

00:32:48.500 --> 00:32:49.960
can't afford your spa.

00:32:49.960 --> 00:32:52.370
And you advertise to the
thyme eaters who can.

00:32:52.370 --> 00:32:55.070
And now you preserve
your discrimination

00:32:55.070 --> 00:32:59.550
while appearing to have complied
with the demographic parity.

00:32:59.550 --> 00:33:03.860
So another issue that
arises is sometimes

00:33:03.860 --> 00:33:05.900
called fairness gerrymandering.

00:33:05.900 --> 00:33:10.360
So let's say that
the algorithm has

00:33:10.360 --> 00:33:14.880
demographic parity with respect
to sage eaters and thyme

00:33:14.880 --> 00:33:15.600
eaters.

00:33:15.600 --> 00:33:19.150
And maybe it has demographic
parity with respect

00:33:19.150 --> 00:33:23.120
to coffee drinkers
and tea drinkers.

00:33:23.120 --> 00:33:31.540
But it's really not treating
the tea drinking, sage eaters

00:33:31.540 --> 00:33:32.610
very well.

00:33:32.610 --> 00:33:35.710
So they're disproportionately
discriminated against.

00:33:38.302 --> 00:33:40.330
And that's a problem.

00:33:40.330 --> 00:33:44.320
That says, oh, my gosh, I
might have had 10 attributes,

00:33:44.320 --> 00:33:45.992
and now I have 2 to the 10.

00:33:45.992 --> 00:33:49.218
I have 1,024 things
I have to look at.

00:33:49.218 --> 00:33:53.860
So another issue I'm not
going to go through--

00:33:53.860 --> 00:33:56.530
other definitions-- other
notions of group fairness.

00:33:56.530 --> 00:34:00.400
But another issue is that
group fairness criteria,

00:34:00.400 --> 00:34:02.440
each of which are
very desirable,

00:34:02.440 --> 00:34:04.240
may be mutually incompatible.

00:34:04.240 --> 00:34:06.680
And you can't have
all of them at once.

00:34:06.680 --> 00:34:12.280
And this is at the heart
of the controversies

00:34:12.280 --> 00:34:15.370
surrounding recidivism
prediction algorithms--

00:34:15.370 --> 00:34:18.969
and a particular controversy
surrounding recidivism

00:34:18.969 --> 00:34:19.960
prediction algorithms.

00:34:19.960 --> 00:34:23.461
And again, I invite
you to ask about that.

00:34:23.461 --> 00:34:28.210
OK, well, a different notion
is individual fairness.

00:34:28.210 --> 00:34:31.159
And the intuition behind
individual fairness

00:34:31.159 --> 00:34:35.389
is that people who are similar
with respect to a given

00:34:35.389 --> 00:34:40.170
classification task should
be classified similarly.

00:34:40.170 --> 00:34:43.580
So in order to make any
sense of this whatsoever,

00:34:43.580 --> 00:34:46.370
we have to understand for
a given classification

00:34:46.370 --> 00:34:50.929
task what does it mean to
be similar or dissimilar?

00:34:50.929 --> 00:34:54.170
How do we figure out how
dissimilar people are?

00:34:54.170 --> 00:34:57.770
So credit scores in some sense
are an effort to do this.

00:34:57.770 --> 00:34:59.979
If people are very
different in credit scores,

00:34:59.979 --> 00:35:02.270
then we feel that they are
very different when it comes

00:35:02.270 --> 00:35:04.377
to their eligibility for loans.

00:35:04.377 --> 00:35:05.960
If they're very
similar, then we think

00:35:05.960 --> 00:35:08.765
they should be
treated similarly.

00:35:08.765 --> 00:35:11.660
So we need the right
notion of dissimilarity.

00:35:11.660 --> 00:35:14.006
And we need to get
our hands on it.

00:35:14.006 --> 00:35:15.650
And another issue
here is that you

00:35:15.650 --> 00:35:18.890
might think that, oh,
gee, there are going

00:35:18.890 --> 00:35:20.390
to be these boundary cases.

00:35:20.390 --> 00:35:22.070
It's like if you've
graded students,

00:35:22.070 --> 00:35:23.865
you've got students
who are very similar.

00:35:23.865 --> 00:35:26.240
And one of them is going to
get an A. And the other one's

00:35:26.240 --> 00:35:28.656
going to get a B. And what are
you going to do about this?

00:35:28.656 --> 00:35:30.950
So imagine that credit
scores, for example,

00:35:30.950 --> 00:35:35.730
are sort of smeared neatly
across the population.

00:35:35.730 --> 00:35:37.740
So there aren't big
gaps in credit scores.

00:35:37.740 --> 00:35:42.600
So you'll have a pair of
people, U and V, one of whom

00:35:42.600 --> 00:35:46.500
is going to get a loan approved
and the other one will not.

00:35:46.500 --> 00:35:48.240
And yet they're
really, really close

00:35:48.240 --> 00:35:52.010
to each other in
their credit scores.

00:35:52.010 --> 00:35:55.740
So problem-- you have to
draw the line somewhere.

00:35:55.740 --> 00:35:59.730
But not if you were thinking
about flipping coins.

00:35:59.730 --> 00:36:03.690
What you can do instead
is you can map individuals

00:36:03.690 --> 00:36:07.490
to probability distributions
over the outcomes.

00:36:07.490 --> 00:36:11.010
And you can require that similar
people have similar probability

00:36:11.010 --> 00:36:13.620
distributions over
outcomes, just

00:36:13.620 --> 00:36:15.780
as we did with
differential privacy

00:36:15.780 --> 00:36:18.300
when we required that
similar databases would

00:36:18.300 --> 00:36:21.810
have similar probabilities--

00:36:21.810 --> 00:36:25.115
distributions.

00:36:25.115 --> 00:36:28.430
So of course, a
big problem still

00:36:28.430 --> 00:36:31.670
is where do you get
your metric from?

00:36:31.670 --> 00:36:34.980
But also, what should
the metric capture?

00:36:34.980 --> 00:36:38.360
So let's say that we
have a Nice News company.

00:36:44.530 --> 00:36:48.560
An individual, x, is going to
apply for a job at Nice News.

00:36:48.560 --> 00:36:51.320
x will interact with
the environment there.

00:36:51.320 --> 00:36:53.120
There's randomness
in the environment.

00:36:53.120 --> 00:36:55.360
There's randomness in x's life.

00:36:55.360 --> 00:36:57.770
But there's a
well-defined probability.

00:36:57.770 --> 00:37:01.280
You can't find it, but there's a
well-defined probability that x

00:37:01.280 --> 00:37:03.620
will succeed in the
job where, let's say,

00:37:03.620 --> 00:37:05.690
success is something
very specific--

00:37:05.690 --> 00:37:07.640
stays in the job two years--

00:37:07.640 --> 00:37:10.640
has at least one promotion
during that time.

00:37:10.640 --> 00:37:14.300
So what we would want is that
if the probability of success

00:37:14.300 --> 00:37:17.000
for x and y are very
similar to each other,

00:37:17.000 --> 00:37:19.310
then they should have similar
probabilities of being

00:37:19.310 --> 00:37:21.830
classified as higher
by an algorithm--

00:37:21.830 --> 00:37:23.180
makes perfect sense.

00:37:23.180 --> 00:37:27.024
Similar people should
be treated similarly.

00:37:27.024 --> 00:37:31.396
But what if the environment
is hostile to sage eaters?

00:37:31.396 --> 00:37:32.930
So this is not Nice News.

00:37:32.930 --> 00:37:35.300
This is Nasty News.

00:37:35.300 --> 00:37:42.110
And now what should the
metric be measuring?

00:37:42.110 --> 00:37:44.720
Should it be measuring talent?

00:37:44.720 --> 00:37:47.810
Should it be measuring their
probability of success the way

00:37:47.810 --> 00:37:48.560
it was before?

00:37:48.560 --> 00:37:51.060
They are not the
same thing anymore.

00:37:51.060 --> 00:37:53.000
You need some judgment here.

00:37:53.000 --> 00:37:57.800
So this is a very poorly
explored area so far,

00:37:57.800 --> 00:38:01.480
but that I hope to
change this semester.

00:38:01.480 --> 00:38:04.970
So we've seen sort of
group fairness notions

00:38:04.970 --> 00:38:06.500
and individual fairness notions.

00:38:06.500 --> 00:38:09.740
There's now work on
trying to bridge the gap.

00:38:09.740 --> 00:38:14.780
And one of the
simplest approaches--

00:38:14.780 --> 00:38:17.810
it's not going to seem super
simple, but I promise you

00:38:17.810 --> 00:38:20.010
this is the simplest
version of it--

00:38:20.010 --> 00:38:22.250
is something called
multi-accuracy,

00:38:22.250 --> 00:38:25.610
proposed by Hebert-Johnson,
Kim, Reingold, and Roth.

00:38:25.610 --> 00:38:29.010
So imagine a world
where each individual

00:38:29.010 --> 00:38:31.220
is holding a secret coin.

00:38:31.220 --> 00:38:32.960
The coin is biased.

00:38:32.960 --> 00:38:37.190
It captures this
individual's probability of,

00:38:37.190 --> 00:38:39.320
let's say, defaulting on a loan.

00:38:39.320 --> 00:38:42.260
So some people will have very
low probabilities of defaulting

00:38:42.260 --> 00:38:43.640
because they're very organized.

00:38:43.640 --> 00:38:46.680
And they're very secure
financially in many ways--

00:38:46.680 --> 00:38:49.670
big families, outreach
network, and so on.

00:38:49.670 --> 00:38:52.410
Other people are in much
more precarious situations.

00:38:52.410 --> 00:38:55.040
So that's captured by the
different biases of the coin.

00:38:55.040 --> 00:38:56.690
Bias is not an evil term here.

00:38:56.690 --> 00:39:00.060
I just mean it's
not a fair coin--

00:39:00.060 --> 00:39:02.390
not a 50/50 coin.

00:39:02.390 --> 00:39:05.850
Now, our algorithm
is going to try

00:39:05.850 --> 00:39:09.690
to learn to estimate the
bias of each person's coin.

00:39:09.690 --> 00:39:13.020
But it doesn't actually
get to see coins.

00:39:13.020 --> 00:39:16.980
It just gets to see the
results of various coin flips.

00:39:16.980 --> 00:39:18.630
This is the historical data.

00:39:18.630 --> 00:39:22.260
So this is learning
how to predict risk

00:39:22.260 --> 00:39:23.580
based on training data.

00:39:23.580 --> 00:39:26.250
So I've seen many, many
people who look a lot like you

00:39:26.250 --> 00:39:29.010
in terms of financials.

00:39:29.010 --> 00:39:33.844
And those people-- you
know, about a third of them

00:39:33.844 --> 00:39:35.260
default on the
loans, so I'm going

00:39:35.260 --> 00:39:38.062
to guess that your probability
of defaulting is about a third.

00:39:38.062 --> 00:39:39.270
That's what's going on there.

00:39:39.270 --> 00:39:41.790
That's perfectly fine.

00:39:41.790 --> 00:39:46.670
So you could say that
a prediction algorithm

00:39:46.670 --> 00:39:50.970
has approximately accurate
expectation for a group--

00:39:50.970 --> 00:39:52.680
S, like the sage eaters--

00:39:52.680 --> 00:39:55.440
if the predictions
when I average them

00:39:55.440 --> 00:39:59.600
over the elements of S are
approximately accurate.

00:39:59.600 --> 00:40:01.180
Well, that sounds pretty good.

00:40:01.180 --> 00:40:02.970
But it turns out for
technical reasons,

00:40:02.970 --> 00:40:05.250
it's not as nice as it seems.

00:40:05.250 --> 00:40:09.870
So instead, the requirement
is strengthened.

00:40:09.870 --> 00:40:12.330
They require a
multi-accurate expectation

00:40:12.330 --> 00:40:14.190
for a collection of groups.

00:40:14.190 --> 00:40:16.770
What that means is
for every group that's

00:40:16.770 --> 00:40:19.050
listed in this
collection, it's going

00:40:19.050 --> 00:40:22.860
to be approximately accurate--
have approximately accurate

00:40:22.860 --> 00:40:26.364
expectation for that group.

00:40:26.364 --> 00:40:31.180
So that tends to make
things a lot better,

00:40:31.180 --> 00:40:33.250
although I didn't have
a chance to explain

00:40:33.250 --> 00:40:37.520
why it was a weak notion when
I was only looking at one set.

00:40:37.520 --> 00:40:41.290
And the nice thing about this is
that these kinds of predictors

00:40:41.290 --> 00:40:44.320
can be constructed via
machine learning techniques

00:40:44.320 --> 00:40:48.261
for large enough sets
in the collection.

00:40:48.261 --> 00:40:50.540
So if our sets are
things like the sage

00:40:50.540 --> 00:40:52.500
eaters, the thyme
eaters, the sage

00:40:52.500 --> 00:40:56.360
eating coffee drinkers,
people who wear purple,

00:40:56.360 --> 00:41:00.170
and so on, you're
going to get something

00:41:00.170 --> 00:41:04.546
that's pretty nice for all of
these groups, which is great.

00:41:04.546 --> 00:41:09.460
But which groups should you
actually be thinking about?

00:41:09.460 --> 00:41:14.380
So this is also the question
that you would ask yourself

00:41:14.380 --> 00:41:17.950
if somebody hands you a
predictor and a whole bunch

00:41:17.950 --> 00:41:20.140
of data, and says, go ahead.

00:41:20.140 --> 00:41:21.220
Audit my predictor.

00:41:21.220 --> 00:41:22.660
And see if it's behaving well.

00:41:22.660 --> 00:41:23.560
Here's the data.

00:41:23.560 --> 00:41:25.480
See if it's behaving fairly.

00:41:25.480 --> 00:41:27.730
You probably would
think, well, I'll

00:41:27.730 --> 00:41:30.940
be sure that it's even-handed
with respect to men and women,

00:41:30.940 --> 00:41:32.530
and sage eaters
and thyme eaters,

00:41:32.530 --> 00:41:34.540
and coffee drinkers
and tea drinkers,

00:41:34.540 --> 00:41:39.460
and people who wear dresses
versus people who prefer pants,

00:41:39.460 --> 00:41:42.370
and all sorts of things, and
straight and not straight,

00:41:42.370 --> 00:41:43.420
and so on.

00:41:43.420 --> 00:41:46.528
So which groups?

00:41:46.528 --> 00:41:51.060
Now, the problem that
I was really struggling

00:41:51.060 --> 00:41:56.130
with over the summer was how do
you know that you're supposed

00:41:56.130 --> 00:41:57.790
to look at a particular group?

00:41:57.790 --> 00:42:00.330
So if you have a
disadvantaged group--

00:42:00.330 --> 00:42:02.670
an oppressed group--
you can't expect

00:42:02.670 --> 00:42:06.509
them to stand up
and say, look at us.

00:42:06.509 --> 00:42:07.800
We're not being treated fairly.

00:42:07.800 --> 00:42:10.500
They may have actually
taken in the message

00:42:10.500 --> 00:42:14.094
that they are supposed
to be underachieving.

00:42:14.094 --> 00:42:18.600
So how do you use
technology somehow

00:42:18.600 --> 00:42:20.970
to step outside the
frame of reference

00:42:20.970 --> 00:42:23.370
and be sure you're checking
all of the right things

00:42:23.370 --> 00:42:29.455
or demanding all of the
right groups to be covered?

00:42:29.455 --> 00:42:36.070
So this is where we say
complexity theory rocks.

00:42:36.070 --> 00:42:39.430
We don't have to actually
list these groups.

00:42:39.430 --> 00:42:42.910
We can classify them implicitly.

00:42:42.910 --> 00:42:47.410
We can say I want to
consider all groups that

00:42:47.410 --> 00:42:52.480
can be defined by small circuits
or little tiny programs--

00:42:52.480 --> 00:42:54.520
well, simple
programs, let's say--

00:42:54.520 --> 00:42:56.230
very simple programs.

00:42:56.230 --> 00:43:02.660
Now, if you do that, all
of the machinery works.

00:43:02.660 --> 00:43:05.250
You will almost
surely capture all

00:43:05.250 --> 00:43:08.460
of the normal types
of discrimination

00:43:08.460 --> 00:43:09.900
that people think about.

00:43:09.900 --> 00:43:13.010
But you'll also have a
net for many more types.

00:43:13.010 --> 00:43:16.030
You'll catch them
in your net as well.

00:43:16.030 --> 00:43:21.330
So this is why I fell in
love with the multi-accuracy

00:43:21.330 --> 00:43:27.960
and its strengthening to
what's called multi-calibration

00:43:27.960 --> 00:43:30.894
because it gave me an answer
to this question of how

00:43:30.894 --> 00:43:33.060
do you decide which groups
you should be looking at.

00:43:33.060 --> 00:43:34.940
You don't have to.

00:43:34.940 --> 00:43:38.960
And a interesting
psychology question

00:43:38.960 --> 00:43:44.000
is how much work do
we do computationally

00:43:44.000 --> 00:43:46.310
when we are discriminating?

00:43:46.310 --> 00:43:48.260
Do we do a lot of
work to place someone

00:43:48.260 --> 00:43:50.420
into the bin of
discriminate against them?

00:43:50.420 --> 00:43:53.550
Or is it a very easy
computation in our mind?

00:43:53.550 --> 00:43:57.150
And my conjecture is that
it's a simple computation.

00:43:57.150 --> 00:43:57.860
But I don't know.

00:43:57.860 --> 00:44:01.220
The flip side of this
also is if there's

00:44:01.220 --> 00:44:03.530
a group that's
discriminated against,

00:44:03.530 --> 00:44:06.420
but you have to compute
really hard to find them,

00:44:06.420 --> 00:44:09.300
maybe in practice, it's just
not happening that much.

00:44:09.300 --> 00:44:15.540
OK, so the last
couple of minutes I'll

00:44:15.540 --> 00:44:18.570
spend on affirmative action--

00:44:18.570 --> 00:44:19.850
group remedies.

00:44:19.850 --> 00:44:25.360
And again, this also is
very much in progress.

00:44:25.360 --> 00:44:28.650
So one way that we can
do affirmative action

00:44:28.650 --> 00:44:30.270
is if we have rankings.

00:44:30.270 --> 00:44:32.210
And there are real
life examples of this.

00:44:32.210 --> 00:44:35.220
So in the University of
Texas and in the University

00:44:35.220 --> 00:44:39.540
of California
system, students who

00:44:39.540 --> 00:44:44.340
are ranked in the top 10% of
their own high school's class

00:44:44.340 --> 00:44:48.000
are automatically
granted admission

00:44:48.000 --> 00:44:52.170
to one of the
state universities.

00:44:52.170 --> 00:44:56.740
So that's great.

00:44:56.740 --> 00:45:01.860
And there's another example
of similar but more nuanced

00:45:01.860 --> 00:45:07.050
thinking by John Roemer,
who talked about stratifying

00:45:07.050 --> 00:45:10.470
students according to the
education level of the mother,

00:45:10.470 --> 00:45:15.180
and within each stratum,
ranking each student

00:45:15.180 --> 00:45:19.260
by the number of hours they
spend on homework per week,

00:45:19.260 --> 00:45:21.770
and then taking
to the university

00:45:21.770 --> 00:45:25.380
the top appropriately chosen
K percent from each stratum.

00:45:25.380 --> 00:45:28.130
So you fix K so that the
number of admitted students

00:45:28.130 --> 00:45:31.030
is the right answer.

00:45:31.030 --> 00:45:32.930
And his point was
that a student who

00:45:32.930 --> 00:45:34.940
comes from an uneducated
home may not even

00:45:34.940 --> 00:45:38.510
realize it's possible to
spend 10 or 15 hours a week

00:45:38.510 --> 00:45:39.020
on homework.

00:45:39.020 --> 00:45:42.574
They've never maybe
seen it modeled.

00:45:42.574 --> 00:45:48.690
Now, a different way
if you have a metric

00:45:48.690 --> 00:45:50.290
is something like this.

00:45:50.290 --> 00:45:51.930
And this is highly simplified.

00:45:51.930 --> 00:45:54.000
Suppose for this
simple version of it,

00:45:54.000 --> 00:45:57.430
we have the same number of
sage eaters and thyme eaters.

00:45:57.430 --> 00:45:59.670
And we have a distance
for each sage eater.

00:45:59.670 --> 00:46:02.400
We know how far they are
from each thyme eater.

00:46:02.400 --> 00:46:04.470
The idea is that we
will pair up the sage

00:46:04.470 --> 00:46:07.980
eaters and the thyme
eaters so that we minimize

00:46:07.980 --> 00:46:10.350
the sum of the distances over--

00:46:10.350 --> 00:46:14.640
for each sage eater how far it
is to its corresponding thyme

00:46:14.640 --> 00:46:15.870
eater.

00:46:15.870 --> 00:46:19.050
And then we will
classify the sage eater.

00:46:19.050 --> 00:46:21.330
We first learn how to
classify the thyme eaters.

00:46:21.330 --> 00:46:23.580
And then when we want to
classify a sage eater,

00:46:23.580 --> 00:46:25.890
we just follow it to its
paired up thyme eater.

00:46:25.890 --> 00:46:27.660
And we see how
they're classified.

00:46:27.660 --> 00:46:29.370
So we do something like that.

00:46:29.370 --> 00:46:31.080
We use earth mover distances.

00:46:31.080 --> 00:46:33.600
And the sage eaters are
mapped to distributions

00:46:33.600 --> 00:46:34.620
over thyme eaters.

00:46:34.620 --> 00:46:36.590
And similar, sage eaters
are mapped similarly.

00:46:36.590 --> 00:46:37.090
OK.

00:46:40.380 --> 00:46:41.730
So that actually works.

00:46:41.730 --> 00:46:43.527
But again, there's
this problem, where

00:46:43.527 --> 00:46:44.610
does the metric come from?

00:46:47.250 --> 00:46:52.610
So we know how to handle
multiple disjoint minority

00:46:52.610 --> 00:46:53.720
groups.

00:46:53.720 --> 00:46:56.150
If we have a metric, we
just do the same things

00:46:56.150 --> 00:46:59.120
for both the multiple
different groups.

00:46:59.120 --> 00:47:02.090
And I should point out
here that if, let's say,

00:47:02.090 --> 00:47:04.430
a third of the thyme eaters
are given loans, then

00:47:04.430 --> 00:47:06.920
a third of the sage eaters
will because of that 1 to 1

00:47:06.920 --> 00:47:07.790
matching.

00:47:07.790 --> 00:47:12.280
And that also happens in
the more rigorous version.

00:47:12.280 --> 00:47:16.500
If we don't have a metric,
quite surprisingly,

00:47:16.500 --> 00:47:19.810
we can build something,
which we call a fair ranking,

00:47:19.810 --> 00:47:22.480
from any multi-accurate
predictor.

00:47:22.480 --> 00:47:26.250
And then we can play those
ranking games that were done,

00:47:26.250 --> 00:47:30.060
say, by the University
of California.

00:47:30.060 --> 00:47:30.890
Right.

00:47:30.890 --> 00:47:31.940
OK.

00:47:31.940 --> 00:47:37.750
So the intersectional
case, though, remains open.

00:47:37.750 --> 00:47:41.500
We don't know how to do these
affirmative action games--

00:47:41.500 --> 00:47:43.710
games-- I mean, strategies--

00:47:43.710 --> 00:47:48.490
if our sets of minority
groups intersect.

00:47:48.490 --> 00:47:51.100
And of course, that's the
really interesting case

00:47:51.100 --> 00:47:52.930
when you have someone
who is both a sage

00:47:52.930 --> 00:47:54.910
eater and a rosemary eater.

00:47:57.940 --> 00:48:00.250
OK, so I think I'm out of time.

00:48:00.250 --> 00:48:04.574
And so I'll skip to the end.

00:48:04.574 --> 00:48:07.450
And I'll be happy to
take some questions.

00:48:07.450 --> 00:48:09.550
[APPLAUSE]

00:48:09.550 --> 00:48:12.600
[MUSIC PLAYING]

