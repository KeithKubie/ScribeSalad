WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:01.500
&gt;&gt; Anne-Imelda Radice:
Good afternoon.

00:00:01.500 --> 00:00:08.170
I'm Anne-Imelda Radice, and I am
a senior advisor to the chairman

00:00:08.170 --> 00:00:12.740
and director of the Division
of Public Programs at the NEH,

00:00:12.740 --> 00:00:15.610
the National Endowment
for the Humanities,

00:00:15.610 --> 00:00:18.140
which is the sponsor
of this room.

00:00:18.140 --> 00:00:22.520
I am so pleased to
welcome some amazing voices

00:00:22.520 --> 00:00:24.720
that have been heard
all day long.

00:00:24.720 --> 00:00:27.510
I wish we could just
do this all the time.

00:00:27.510 --> 00:00:33.110
[ Applause ]

00:00:33.110 --> 00:00:37.930
I was going to proudly say,
thinking that I would be

00:00:37.930 --> 00:00:43.500
in a special position, this
is my 16th Book Festival.

00:00:43.500 --> 00:00:46.900
However, the woman who
was our stage manager

00:00:46.900 --> 00:00:52.010
and directs this room, this
is her 18th, so I am humbled.

00:00:52.010 --> 00:00:54.510
[ Laughter ]

00:00:54.510 --> 00:00:57.940
[ Applause ]

00:00:57.940 --> 00:01:00.630
We are in for a treat.

00:01:00.630 --> 00:01:03.830
Professor Tenner's book,
The Efficiency Paradox:

00:01:03.830 --> 00:01:06.870
What Big Data Can't Do,

00:01:06.870 --> 00:01:10.730
absolutely was a
feel-good book for me.

00:01:10.730 --> 00:01:16.130
Now in business school,
there are all kinds of books

00:01:16.130 --> 00:01:20.040
and papers and things you've
got to absorb that tell you how

00:01:20.040 --> 00:01:22.060
to do things in a better way.

00:01:22.060 --> 00:01:27.340
The word efficiency is on every
other page, and it's something

00:01:27.340 --> 00:01:32.040
that is intimidating but you
feel is absolutely essential

00:01:32.040 --> 00:01:35.380
if you're going to be a success.

00:01:35.380 --> 00:01:40.940
Doctor Tenner is a very
respected professor.

00:01:40.940 --> 00:01:44.170
At the moment, he's at the
Lemelson Center for the Study

00:01:44.170 --> 00:01:47.590
of Invention and Innovation
at the Smithsonian.

00:01:47.590 --> 00:01:51.990
He has an AB from Princeton
and a PhD from Chicago.

00:01:51.990 --> 00:01:57.050
His writings, thank
heavens, are everywhere --

00:01:57.050 --> 00:01:59.460
the New York Times,
the Washington Post,

00:01:59.460 --> 00:02:01.460
the Wall Street Journal,
the Atlantic Monthly,

00:02:01.460 --> 00:02:03.120
Edward Tenner: cetera.

00:02:03.120 --> 00:02:05.370
Two other books that I
would recommend you read

00:02:05.370 --> 00:02:10.850
that he has done are
Techspeak, which is 1986,

00:02:10.850 --> 00:02:14.490
and then one that's
really, really amazing,

00:02:14.490 --> 00:02:19.330
Why Things Bite Back:
Technology and the Revenge

00:02:19.330 --> 00:02:21.670
of Unintentional Consequences.

00:02:21.670 --> 00:02:23.020
In fact, he's going
to be talking

00:02:23.020 --> 00:02:26.540
about unintentional
consequences,

00:02:26.540 --> 00:02:29.030
I'm sure, with us today.

00:02:29.030 --> 00:02:32.960
He's going to be exploring
fields like media and culture,

00:02:32.960 --> 00:02:36.110
education, transportation,
and medicine,

00:02:36.110 --> 00:02:41.550
and I had to hold myself back
from quoting from this book

00:02:41.550 --> 00:02:45.260
because I want him to
tell you all the things

00:02:45.260 --> 00:02:50.930
that just blew my mind away,
but I have to say two things.

00:02:50.930 --> 00:02:55.870
He has made me feel so much
better that I write on paper

00:02:55.870 --> 00:02:59.730
as opposed to do things
through a computer all the time

00:02:59.730 --> 00:03:01.030
and that --

00:03:01.030 --> 00:03:02.330
[ Applause ]

00:03:02.330 --> 00:03:03.630
Yes, there are a few of us left,

00:03:03.630 --> 00:03:06.640
and he thinks it's
important even

00:03:06.640 --> 00:03:10.310
if you're a technology person
to do that, and second of all,

00:03:10.310 --> 00:03:14.730
I no longer feel guilty when I
get into an Uber and I get upset

00:03:14.730 --> 00:03:18.280
with the driver who wants
to follow the GPS instead

00:03:18.280 --> 00:03:21.110
of being logical and
following directions,

00:03:21.110 --> 00:03:23.310
so you have no idea
how you've lifted

00:03:23.310 --> 00:03:25.980
that pall from my shoulders.

00:03:25.980 --> 00:03:33.300
In a way, the professor, through
his wisdom and his research

00:03:33.300 --> 00:03:36.320
and I think a great
sense of humor as well,

00:03:36.320 --> 00:03:39.170
allows us to stop
and smell the roses.

00:03:39.170 --> 00:03:40.970
So Professor.

00:03:40.970 --> 00:03:49.120
[ Applause ]

00:03:49.120 --> 00:03:51.780
&gt;&gt; Edward Tenner: Thank you for
that wonderful introduction,

00:03:51.780 --> 00:03:55.120
and thanks to the Library
of Congress, and especially

00:03:55.120 --> 00:03:57.690
to the National Endowment
for the Humanities

00:03:57.690 --> 00:03:59.270
in making this possible.

00:03:59.270 --> 00:04:01.390
It's really quite
an honor to be here.

00:04:01.390 --> 00:04:03.690
I should start with
a confession.

00:04:03.690 --> 00:04:07.150
I am not a professor, but
I play one on YouTube,

00:04:07.150 --> 00:04:13.230
and I've been an independent
writer for over 25 years now,

00:04:13.230 --> 00:04:18.310
and I've really loved it ever
since the employment crisis

00:04:18.310 --> 00:04:21.020
of the 1970s happened
after I received my PhD,

00:04:21.020 --> 00:04:24.910
and I went from the fast
track to the scenic route,

00:04:24.910 --> 00:04:26.930
and I haven't looked back.

00:04:26.930 --> 00:04:33.980
So I'd like to introduce you
to some of the ideas in my book

00:04:33.980 --> 00:04:41.400
and to show you why I
have a half enthusiasm

00:04:41.400 --> 00:04:48.990
and half equivocation
about big data.

00:04:48.990 --> 00:04:52.320
Are there any Chicagoans here?

00:04:52.320 --> 00:04:56.970
Could you show the
slide, please?

00:05:02.050 --> 00:05:05.900
Yes. So I guess it's
not really behind me.

00:05:05.900 --> 00:05:11.830
The people in the center will
have to look at either side,

00:05:11.830 --> 00:05:17.880
but that building -- does
anybody recognize that building?

00:05:17.880 --> 00:05:21.340
So I don't think we
have Chicagoans here,

00:05:21.340 --> 00:05:22.980
but that building is one

00:05:22.980 --> 00:05:26.240
of the most interesting
buildings in the United States.

00:05:26.240 --> 00:05:31.760
This is the former Ruben
R. Donnelley Calumet Plant

00:05:31.760 --> 00:05:37.780
that was built from 1912 to the
late 1920s, and it was once one

00:05:37.780 --> 00:05:40.900
of the largest if the not
largest printing plant

00:05:40.900 --> 00:05:45.480
in the world, and here is
where Sears, Roebuck catalogs

00:05:45.480 --> 00:05:50.210
and Bell System telephone
directories, Life Magazine,

00:05:50.210 --> 00:05:54.320
Time Magazine, they all
came off presses that worked

00:05:54.320 --> 00:05:58.750
with massive rolls of
paper, and this was part

00:05:58.750 --> 00:06:03.280
of what I call the
continuous process economy,

00:06:03.280 --> 00:06:07.130
and today this building is one
of the largest server farms

00:06:07.130 --> 00:06:14.140
of the world, and it is used to
route all kinds of information

00:06:14.140 --> 00:06:18.040
and transactions on the web, and
so the fate of this building,

00:06:18.040 --> 00:06:22.700
which was very well-equipped
with extra reinforced floors

00:06:22.700 --> 00:06:25.910
and with wonderful
electrical service --

00:06:25.910 --> 00:06:28.830
the fate of this building shows
what's happened to our society.

00:06:28.830 --> 00:06:31.720
We've gone from a
continuous production society

00:06:31.720 --> 00:06:34.680
to a platform society,
and the companies

00:06:34.680 --> 00:06:38.970
that dominate our attention
now are no longer AT and T

00:06:38.970 --> 00:06:43.350
and especially not Sears,
Roebuck but companies

00:06:43.350 --> 00:06:46.380
like Amazon, Google,
social media companies,

00:06:46.380 --> 00:06:48.440
and the streaming companies.

00:06:48.440 --> 00:06:53.080
People have written a lot
about this transition,

00:06:53.080 --> 00:06:56.730
whether it's good or it's
bad, its effect on inequality,

00:06:56.730 --> 00:06:59.440
gender, all kinds
of other things.

00:06:59.440 --> 00:07:01.990
My book, though, is different

00:07:01.990 --> 00:07:03.820
from other critiques
of Silicon Valley.

00:07:03.820 --> 00:07:06.530
My book is not about
all of the things

00:07:06.530 --> 00:07:09.140
that Silicon Valley
doesn't really care about.

00:07:09.140 --> 00:07:11.300
It's about what really
matters to Silicon Valley,

00:07:11.300 --> 00:07:15.360
which is efficiency, and so
my argument is that pursuit

00:07:15.360 --> 00:07:17.470
of too much efficiency

00:07:17.470 --> 00:07:21.590
in the short run can make us
less efficient in the long run,

00:07:21.590 --> 00:07:23.850
that efficiency is
good up to a point,

00:07:23.850 --> 00:07:25.780
but it can undermine itself.

00:07:25.780 --> 00:07:31.580
Let me give you a story
about my experience of this.

00:07:31.580 --> 00:07:35.300
I use a program called Waze
that is now sold by Google.

00:07:35.300 --> 00:07:39.970
It's actually not sold, it's
free, and/or it's in return

00:07:39.970 --> 00:07:42.800
for advertising that's really
impossible to look at even

00:07:42.800 --> 00:07:47.410
when you're stopped, but
who can object to this?

00:07:47.410 --> 00:07:50.990
In fact, even starting out as
a skeptic of Waze, I felt I had

00:07:50.990 --> 00:07:53.400
to try it, and I've
now worked myself

00:07:53.400 --> 00:07:56.830
up to their highest category.

00:07:56.830 --> 00:08:00.810
I am Waze Royalty, so if
you see a little crown --

00:08:00.810 --> 00:08:05.600
if you use Waze and you see a
little crown, that might be me.

00:08:05.600 --> 00:08:10.180
But I discovered something using
Waze to visit, at long last,

00:08:10.180 --> 00:08:14.080
the Thomas Edison Historic
Site in northern New Jersey,

00:08:14.080 --> 00:08:17.270
which is really almost
impossible to navigate

00:08:17.270 --> 00:08:21.440
without using a GPS, and it was
really great getting up there,

00:08:21.440 --> 00:08:25.000
but on my way back
after admiring all

00:08:25.000 --> 00:08:28.080
of the wonderful laboratories
that Edison established

00:08:28.080 --> 00:08:33.380
and that are now so beautifully
preserved, Waze told me

00:08:33.380 --> 00:08:37.670
to go north when I
should be going south.

00:08:37.670 --> 00:08:42.720
And fortunately, as the author
of a book praising the value

00:08:42.720 --> 00:08:45.700
of intuition against total
reliance on big data,

00:08:45.700 --> 00:08:47.200
I was kind of primed for this,

00:08:47.200 --> 00:08:49.760
and I disregarded
the instructions,

00:08:49.760 --> 00:08:53.170
and I headed south, and
I arrived safely at home.

00:08:53.170 --> 00:08:56.880
So this was a lesson
to me in the value

00:08:56.880 --> 00:09:01.370
of being a technology
writer and being able

00:09:01.370 --> 00:09:03.930
to follow my own advice.

00:09:03.930 --> 00:09:10.330
So my book is really not really
another one of the attacks

00:09:10.330 --> 00:09:12.620
on Silicon Valley, but
it's really an attempt

00:09:12.620 --> 00:09:16.720
to go beyond utopianism
and dystopianism.

00:09:16.720 --> 00:09:21.070
It's an attempt to be a realist,

00:09:21.070 --> 00:09:24.850
and I'd like to give you
a half-dozen problems

00:09:24.850 --> 00:09:30.860
of a platform society that
this book calls attention to.

00:09:30.860 --> 00:09:34.150
The first is asymmetric
knowledge.

00:09:34.150 --> 00:09:38.520
Now that just means that the
platform companies know an awful

00:09:38.520 --> 00:09:42.040
lot more about us than
we know about them.

00:09:42.040 --> 00:09:44.320
They have so much
information from our data,

00:09:44.320 --> 00:09:47.620
they know so much more
about our behavior,

00:09:47.620 --> 00:09:51.460
that it is actually difficult to
criticize them, even for a lot

00:09:51.460 --> 00:09:55.370
of academics who get very
selective access to their data.

00:09:55.370 --> 00:09:57.660
This makes it hard to legislate.

00:09:57.660 --> 00:10:00.060
It makes it hard to regulate.

00:10:00.060 --> 00:10:06.830
And congressional investigations
might mean we'll learn more,

00:10:06.830 --> 00:10:08.490
but the thing to keep in mind is

00:10:08.490 --> 00:10:11.790
that we don't really know very
much about the way they work

00:10:11.790 --> 00:10:15.640
and we, for example, still
don't know very much about what,

00:10:15.640 --> 00:10:21.940
if any, influence they really
had on the election of 2016.

00:10:21.940 --> 00:10:26.190
The second problem is the
importance of tacit knowledge.

00:10:26.190 --> 00:10:29.730
This is one of the
most underrated factors

00:10:29.730 --> 00:10:33.320
in intelligence, and
yet it's ubiquitous.

00:10:33.320 --> 00:10:37.430
We saw that when Watson
was first demonstrated

00:10:37.430 --> 00:10:39.690
in playing Jeopardy,
and Watson got a lot

00:10:39.690 --> 00:10:44.230
of difficult questions right,
more than human champions,

00:10:44.230 --> 00:10:47.320
but it also got a
very simple one wrong

00:10:47.320 --> 00:10:50.410
because it really didn't
have a robust understanding

00:10:50.410 --> 00:10:51.710
of the world.

00:10:51.710 --> 00:10:55.360
It had multiple processes
working on different answers,

00:10:55.360 --> 00:10:58.540
and then it would poll
each other and kind

00:10:58.540 --> 00:11:02.430
of reach a majority vote, but
that isn't how our minds work.

00:11:02.430 --> 00:11:04.960
I'll give you an example
from everyday life.

00:11:04.960 --> 00:11:07.380
Take everyday proverbs.

00:11:07.380 --> 00:11:11.960
Take "a stitch in time
saves nine," or my favorite

00:11:11.960 --> 00:11:16.270
as a kid was "a rolling
stone gathers no moss."

00:11:16.270 --> 00:11:18.860
Now what I wondered about is
what does that really mean?

00:11:18.860 --> 00:11:22.130
Does that mean that it is
good to be a rolling stone

00:11:22.130 --> 00:11:25.500
because moss is a kind
of undesirable accretion,

00:11:25.500 --> 00:11:30.400
and if you keep going, you will
not get this growing on you,

00:11:30.400 --> 00:11:31.700
or is moss something good?

00:11:31.700 --> 00:11:34.010
Is moss a stand-in for money?

00:11:34.010 --> 00:11:37.250
Does that mean if you kind
of stay where you are,

00:11:37.250 --> 00:11:41.140
the money will kind
of grow on itself?

00:11:41.140 --> 00:11:44.610
So I still haven't figured
that out, but my point is

00:11:44.610 --> 00:11:50.450
that if you give a child an
unfamiliar proverb from anywhere

00:11:50.450 --> 00:11:52.580
in the world, the
chances are that child

00:11:52.580 --> 00:11:56.350
with no previous exposure to
it will have an understanding

00:11:56.350 --> 00:11:58.190
of what this metaphor is about.

00:11:58.190 --> 00:12:01.080
We are metaphorical beings.

00:12:01.080 --> 00:12:05.450
Our art, our literature
is based on that.

00:12:05.450 --> 00:12:11.710
I remember hearing once a great
philosopher asking a great

00:12:11.710 --> 00:12:14.800
literary critic who was
expounding a particularly

00:12:14.800 --> 00:12:16.100
difficult poem.

00:12:16.100 --> 00:12:18.520
He said, tell me then
who was this fellow

00:12:18.520 --> 00:12:20.980
who can't express
himself clearly?

00:12:20.980 --> 00:12:24.710
And so that ambiguity is so much
of a part of the texture of life

00:12:24.710 --> 00:12:27.330
that we take it for granted,
and yet that's one of the things

00:12:27.330 --> 00:12:31.560
that artificial intelligence has
so much trouble dealing with.

00:12:31.560 --> 00:12:35.260
Then there are false positives.

00:12:35.260 --> 00:12:41.300
I was at the Smithsonian
Gallery of American Art,

00:12:41.300 --> 00:12:44.670
the Smithsonian Museum of
American Art this afternoon,

00:12:44.670 --> 00:12:48.920
and I saw a wonderful exhibition
which I urge you all to see

00:12:48.920 --> 00:12:55.360
of Trevor Paglen, who is an
artist who is exploring all

00:12:55.360 --> 00:12:56.660
of the mysteries

00:12:56.660 --> 00:12:59.910
of the military-industrial
surveillance complex.

00:12:59.910 --> 00:13:03.040
It was really a fantastic
exhibition,

00:13:03.040 --> 00:13:07.840
and one of the features of it
was a film of a string quartet

00:13:07.840 --> 00:13:14.720
as interpreted by software
that recognizes, supposedly,

00:13:14.720 --> 00:13:19.990
the genders and age and other
particulars of the players,

00:13:19.990 --> 00:13:22.260
and as I was watching
it, I was waiting

00:13:22.260 --> 00:13:28.080
for this really very disturbing
knowledge, this ability

00:13:28.080 --> 00:13:32.380
of the software to
identify people,

00:13:32.380 --> 00:13:35.670
and I was actually
somewhat relieved when I saw

00:13:35.670 --> 00:13:37.560
that the software said

00:13:37.560 --> 00:13:44.410
of one female violinist this was
a woman holding a baseball bat.

00:13:44.410 --> 00:13:46.050
So there might be
some risk in this.

00:13:46.050 --> 00:13:49.360
After all, you know, a
violinist might be arrested

00:13:49.360 --> 00:13:51.140
as a potential terrorist
or something.

00:13:51.140 --> 00:13:55.100
It could be a serious problem,
but the serious point of it,

00:13:55.100 --> 00:14:02.140
though, is that while efficiency
can work 99% of the time,

00:14:02.140 --> 00:14:05.680
especially in medical cases,
if it malfunctions just 1%

00:14:05.680 --> 00:14:09.400
of the time, the problems of
dealing with that, the human

00:14:09.400 --> 00:14:12.240
and material cost of
dealing with false positives,

00:14:12.240 --> 00:14:16.130
can very easily offset
all of the gains

00:14:16.130 --> 00:14:18.320
from the 99% efficiency.

00:14:18.320 --> 00:14:21.010
So when you hear about
high rates of efficiency,

00:14:21.010 --> 00:14:23.740
you also have to ask,
well, what is the cost

00:14:23.740 --> 00:14:25.130
of the false positives?

00:14:25.130 --> 00:14:29.660
What is the cost of the
results that have to be cleared

00:14:29.660 --> 00:14:34.590
or investigated at
great expense?

00:14:34.590 --> 00:14:40.060
There's also what social
psychologists call competitor

00:14:40.060 --> 00:14:42.310
neglect, which is a
big factor in business.

00:14:42.310 --> 00:14:49.870
So very often, people using
their big data will have a

00:14:49.870 --> 00:14:54.060
strategy, but what they may not
realize is the other side has

00:14:54.060 --> 00:14:55.360
big data, too.

00:14:55.360 --> 00:14:57.920
So we have a situation
now where more

00:14:57.920 --> 00:15:01.300
and more organizations are
competing with each other,

00:15:01.300 --> 00:15:03.670
and each of them believes
that it has the answer

00:15:03.670 --> 00:15:07.320
through its own use of
data, and yet when you look

00:15:07.320 --> 00:15:12.000
at that competitive world,
sometimes when those two users

00:15:12.000 --> 00:15:15.310
of big data are interacting,
the result can be something

00:15:15.310 --> 00:15:18.300
that is actually foreseen
by neither of them,

00:15:18.300 --> 00:15:19.960
and this has been
happening in baseball.

00:15:19.960 --> 00:15:22.990
There was a column by
Jared Diamond recently

00:15:22.990 --> 00:15:26.920
in the Wall Street Journal
about how the rule changes

00:15:26.920 --> 00:15:31.000
in baseball have been producing
differences in the game and,

00:15:31.000 --> 00:15:34.190
well, I can't go into that
in detail, that have resulted

00:15:34.190 --> 00:15:39.130
in lower attendance and thus
lower revenue, lower interest

00:15:39.130 --> 00:15:43.410
by younger fans, and
this is all as a result

00:15:43.410 --> 00:15:48.850
of the Moneyball idea that
any team can use these tools

00:15:48.850 --> 00:15:50.470
and somehow become a champion.

00:15:50.470 --> 00:15:54.010
Well, there's always an
advantage of the first adopter,

00:15:54.010 --> 00:16:04.010
but once everybody has it,
it turns out to be different.

00:16:04.010 --> 00:16:08.970
The fifth of the
issues that I talk

00:16:08.970 --> 00:16:13.330
about in The Efficiency
Paradox is Campbell's law.

00:16:13.330 --> 00:16:18.120
Campbell's Law is the
Heisenberg uncertainty principle

00:16:18.120 --> 00:16:19.980
of social science.

00:16:19.980 --> 00:16:22.470
Campbell's law is the
idea that once you start

00:16:22.470 --> 00:16:25.950
to measure something and
give people incentives

00:16:25.950 --> 00:16:31.310
to change their behavior to
optimize what is being measured,

00:16:31.310 --> 00:16:35.400
that behavior may
change in unwelcome ways.

00:16:35.400 --> 00:16:38.010
You may get results
that are good

00:16:38.010 --> 00:16:39.470
for the people being measured

00:16:39.470 --> 00:16:41.980
but not necessarily
delivering what they're supposed

00:16:41.980 --> 00:16:43.280
to be measuring.

00:16:43.280 --> 00:16:44.580
We've seen that a
lot in business.

00:16:44.580 --> 00:16:50.150
In the 1980s, critics would
say of corporate executives

00:16:50.150 --> 00:16:53.160
that they were bureaucrats
that were being paid not

00:16:53.160 --> 00:16:56.380
for performance, so we
need to measure the profits

00:16:56.380 --> 00:16:58.600
that they're generating
and pay them accordingly,

00:16:58.600 --> 00:17:00.850
but of course these executives,

00:17:00.850 --> 00:17:03.300
once they had these
compensation schemes,

00:17:03.300 --> 00:17:08.540
found ways to maximize their
profits in the short term

00:17:08.540 --> 00:17:11.740
with accounting techniques
that put off the results

00:17:11.740 --> 00:17:13.610
for their successors,
and we've been living

00:17:13.610 --> 00:17:15.630
with the results of that.

00:17:15.630 --> 00:17:19.480
And finally and most seriously
for me, there's a trend

00:17:19.480 --> 00:17:21.740
that I call counter-serendipity.

00:17:21.740 --> 00:17:26.160
We've all known the advantages
of accidental discovery.

00:17:26.160 --> 00:17:31.770
We've all been someplace as a
result of some kind of error,

00:17:31.770 --> 00:17:37.610
something unexpected that
has brought us into places

00:17:37.610 --> 00:17:39.590
or acquainted us with people

00:17:39.590 --> 00:17:42.960
that we've been very
glad to know.

00:17:42.960 --> 00:17:44.990
I certainly have.

00:17:44.990 --> 00:17:50.670
And the problem with
too much efficiency is

00:17:50.670 --> 00:17:53.060
that by following
existing patterns,

00:17:53.060 --> 00:17:57.100
which artificial intelligence
can do very well, it can get us

00:17:57.100 --> 00:18:00.320
into a groove that is
hard to escape from.

00:18:00.320 --> 00:18:03.520
It can institutionalize
the status quo.

00:18:03.520 --> 00:18:07.020
Far from being a
radical departure,

00:18:07.020 --> 00:18:11.490
pattern recognition can
be ultra-conservative,

00:18:11.490 --> 00:18:14.520
and it can keep us from
the kinds of breakthroughs

00:18:14.520 --> 00:18:18.300
that have really enriched
our lives over the centuries.

00:18:18.300 --> 00:18:22.640
Think, for example, of
what traditionally made

00:18:22.640 --> 00:18:27.030
for a successful
career in the Navy.

00:18:27.030 --> 00:18:31.410
Well, Hyman Rickover was a
very different kind of person.

00:18:31.410 --> 00:18:34.220
He did not correspond
to the pattern

00:18:34.220 --> 00:18:36.540
of successful Navy officers,

00:18:36.540 --> 00:18:40.630
but if you want an
atomic submarine invented,

00:18:40.630 --> 00:18:42.730
you need a different
kind of person.

00:18:42.730 --> 00:18:45.770
So even though he had great
difficulties in his career

00:18:45.770 --> 00:18:47.070
because of his background,

00:18:47.070 --> 00:18:49.200
he was able to contribute
something,

00:18:49.200 --> 00:18:52.700
and there is a Hyman
Rickover Hall now

00:18:52.700 --> 00:18:54.800
at Annapolis that reflects that.

00:18:54.800 --> 00:18:56.970
But it isn't only the military.

00:18:56.970 --> 00:19:02.920
Think of how many brilliant
breakthrough books did not

00:19:02.920 --> 00:19:05.750
conform to what publishers
and their editors

00:19:05.750 --> 00:19:11.310
or even most readers initially
believed a book should be

00:19:11.310 --> 00:19:14.500
and became future
bestsellers and classics.

00:19:14.500 --> 00:19:18.330
Moby Dick was a huge
commercial failure at first.

00:19:18.330 --> 00:19:21.730
People really wondered what had
gotten into Herman Melville,

00:19:21.730 --> 00:19:24.950
and it's very well-known that
Harry Potter was turned down by

00:19:24.950 --> 00:19:26.290
at least 20 publishers.

00:19:26.290 --> 00:19:28.800
So something that's radical,
something that's new,

00:19:28.800 --> 00:19:33.420
something that breaks with
these established ways

00:19:33.420 --> 00:19:36.270
of doing things can often
have a difficult time,

00:19:36.270 --> 00:19:38.690
and yet if we're not careful,

00:19:38.690 --> 00:19:43.110
artificial intelligence can
institutionalize a certain kind

00:19:43.110 --> 00:19:46.050
of stagnation.

00:19:46.050 --> 00:19:48.380
So what am I suggesting?

00:19:48.380 --> 00:19:53.400
I think that we should actually
learn from Silicon Valley,

00:19:53.400 --> 00:19:54.990
and when I wrote the book,

00:19:54.990 --> 00:19:58.860
one of my most interesting
surprises was how many Silicon

00:19:58.860 --> 00:20:02.060
Valley people really had a
secret analog side to them.

00:20:02.060 --> 00:20:09.490
For example, many parents
send their children to schools

00:20:09.490 --> 00:20:16.850
on the principles of a European
movement called anthroposophy,

00:20:16.850 --> 00:20:21.330
in which the emphasis is
on art and spirituality

00:20:21.330 --> 00:20:25.500
and technology is taught
very late, if at all.

00:20:25.500 --> 00:20:27.370
And we can also learn that some

00:20:27.370 --> 00:20:31.220
of the most important
decisions are really those made

00:20:31.220 --> 00:20:35.800
from instinct rather than
from a rational evaluation

00:20:35.800 --> 00:20:41.340
of all factors, and Jeff
Bezos himself made this point.

00:20:41.340 --> 00:20:45.490
People asked him about
how he established Amazon,

00:20:45.490 --> 00:20:49.220
and he said that he
went over it and he saw

00:20:49.220 --> 00:20:50.670
that the odds were against him.

00:20:50.670 --> 00:20:56.070
It looked like a losing bet,
but he saw that in the long run,

00:20:56.070 --> 00:21:00.320
he would regret it
if he lived to be 90

00:21:00.320 --> 00:21:02.720
and wondered what
it would have been

00:21:02.720 --> 00:21:07.200
like if he had taken the
plunge to the web, and his boss

00:21:07.200 --> 00:21:09.440
at the hedge fund where
he was working tried

00:21:09.440 --> 00:21:10.800
to talk him out of it.

00:21:10.800 --> 00:21:12.930
He did not seem to
be interested in it.

00:21:12.930 --> 00:21:16.280
There is no evidence
that he ever invested in.

00:21:16.280 --> 00:21:17.680
He was the more rational one.

00:21:17.680 --> 00:21:20.580
He was the numbers
person, and yet he lost

00:21:20.580 --> 00:21:23.720
out on what was clearly
the greatest chance

00:21:23.720 --> 00:21:29.820
of his career if this was so.

00:21:29.820 --> 00:21:37.910
So in conclusion, I would say

00:21:37.910 --> 00:21:41.080
that I'm not sure
what I can contribute

00:21:41.080 --> 00:21:44.950
to understanding the world
around us, which is the theme

00:21:44.950 --> 00:21:50.370
of this series of lectures,
but I really was impressed

00:21:50.370 --> 00:21:53.930
with the idea of one of my
contemporaries at Harvard

00:21:53.930 --> 00:21:58.620
who is now a professor of
law at Yale, Robert Gordon,

00:21:58.620 --> 00:22:02.470
and he defined an
intellectual in a way

00:22:02.470 --> 00:22:05.240
that I would make my own credo.

00:22:05.240 --> 00:22:09.120
He said an intellectual is
somebody who believes that one,

00:22:09.120 --> 00:22:13.920
the world is run by fools and
two, I could do no better.

00:22:13.920 --> 00:22:15.720
Thank you very much.

00:22:15.720 --> 00:22:32.180
[ Applause ]

00:22:32.180 --> 00:22:33.480
&gt;&gt; Anne-Imelda Radice: Please
step up to the microphone

00:22:33.480 --> 00:22:38.310
if you'd like to
ask some questions.

00:22:38.310 --> 00:22:39.610
&gt;&gt; Hello. My name is Maria Avera

00:22:39.610 --> 00:22:41.830
and I attend Carnegie
Mellon University,

00:22:41.830 --> 00:22:44.940
and I'm doing my masters in
public policy and management.

00:22:44.940 --> 00:22:46.810
I'm right here.

00:22:46.810 --> 00:22:49.660
And my question is, what
do you think about the idea

00:22:49.660 --> 00:22:55.750
of using the Moneyball
method for public policy,

00:22:55.750 --> 00:22:58.750
and what would be
its implications?

00:22:58.750 --> 00:23:00.050
&gt;&gt; Edward Tenner: I'm sorry.

00:23:00.050 --> 00:23:01.350
The method?

00:23:01.350 --> 00:23:02.650
&gt;&gt; Yeah, the Moneyball.

00:23:02.650 --> 00:23:03.950
&gt;&gt; Edward Tenner:
Moneyball method?

00:23:03.950 --> 00:23:05.250
&gt;&gt; Yeah, for public policy.

00:23:05.250 --> 00:23:06.550
&gt;&gt; Edward Tenner:
For public policy?

00:23:06.550 --> 00:23:09.330
Well, I'm not sure how Moneyball

00:23:09.330 --> 00:23:14.830
for public policy would
work in specific cases.

00:23:14.830 --> 00:23:20.700
Moneyball, at least
initially, was very useful

00:23:20.700 --> 00:23:26.000
in finding people who did not
perform that well according

00:23:26.000 --> 00:23:29.850
to the customary measures
of evaluating people.

00:23:29.850 --> 00:23:32.480
It was finding more
sophisticated ones.

00:23:32.480 --> 00:23:36.040
So I would suppose
that you could try

00:23:36.040 --> 00:23:40.770
to use Moneyball techniques
in assessing candidates

00:23:40.770 --> 00:23:47.220
or assessing people who are job
candidates in the public sector.

00:23:47.220 --> 00:23:51.190
It's harder to apply
Moneyball directly to policies

00:23:51.190 --> 00:23:55.380
because there is no historical
database on what policies work

00:23:55.380 --> 00:23:58.550
and what policies don't, so
the air has changed so much

00:23:58.550 --> 00:24:03.450
that even if you compile
that, you could say that well,

00:24:03.450 --> 00:24:05.550
a policy that was
really successful

00:24:05.550 --> 00:24:08.100
when the Calumet
Plant was turning

00:24:08.100 --> 00:24:12.170
out phone books would not
necessarily be successful

00:24:12.170 --> 00:24:18.610
when they were facilitating
transactions online.

00:24:18.610 --> 00:24:20.220
&gt;&gt; Okay. Thank you.

00:24:20.220 --> 00:24:22.520
&gt;&gt; Edward Tenner: Thank you.

00:24:26.380 --> 00:24:28.530
&gt;&gt; Okay. You were in
line before I was.

00:24:28.530 --> 00:24:29.830
&gt;&gt; All right.

00:24:29.830 --> 00:24:32.000
Thank you so much for
taking your time to be here.

00:24:32.000 --> 00:24:34.110
My name is Noah and
I'm a rising senior

00:24:34.110 --> 00:24:35.900
at American University
here in D.C.,

00:24:35.900 --> 00:24:40.750
and one of my biggest insights
from your lecture today was

00:24:40.750 --> 00:24:44.190
about the fact that a false
positive could just destroy the

00:24:44.190 --> 00:24:49.250
efficiency of an entire
system, and I think ironically,

00:24:49.250 --> 00:24:51.890
based on, you know, all the
readings that I've read,

00:24:51.890 --> 00:24:54.580
there isn't really an
institutionalized way

00:24:54.580 --> 00:24:57.270
of creating efficient
-- or, you know,

00:24:57.270 --> 00:25:00.590
creating like a resistance
to false positives.

00:25:00.590 --> 00:25:06.190
So do you think -- how
could we solve that?

00:25:06.190 --> 00:25:08.620
&gt;&gt; Edward Tenner: Well, there
is a psychologist named Gerd

00:25:08.620 --> 00:25:12.710
Gigerenzer, G-I-G-E-R-E-N-Z-E-R,

00:25:12.710 --> 00:25:16.950
who has received less
attention than Daniel Kahneman.

00:25:16.950 --> 00:25:19.560
He is not the winner
of a Nobel Prize.

00:25:19.560 --> 00:25:21.790
I have great respect for
Kahneman, by the way,

00:25:21.790 --> 00:25:25.530
but I think that Kahneman's
ideas have sometimes been

00:25:25.530 --> 00:25:27.540
applied in a one-sided way,

00:25:27.540 --> 00:25:31.460
and so Gigerenzer is a very good
counterweight, and Gigerenzer

00:25:31.460 --> 00:25:34.890
in his works has some
very concrete suggestions

00:25:34.890 --> 00:25:41.810
about statistical techniques
for discounting false positives

00:25:41.810 --> 00:25:47.660
and for reaching decisions that
are more intuitively grounded.

00:25:47.660 --> 00:25:52.310
One major branch of statistics,
you know, Bayesian statistics,

00:25:52.310 --> 00:25:56.670
has intuition, your
prior judgment

00:25:56.670 --> 00:26:00.450
of how likely something is, as
actually one of its foundations.

00:26:00.450 --> 00:26:04.470
So there is a tendency sometimes
to use Bayesian statistics

00:26:04.470 --> 00:26:06.510
as a panacea for everything.

00:26:06.510 --> 00:26:08.480
I am not a Bayesian
in that sense,

00:26:08.480 --> 00:26:11.820
but I think you'll find once
you look into Gigerenzer's work

00:26:11.820 --> 00:26:14.650
that there are some very
concrete suggestions

00:26:14.650 --> 00:26:18.700
about dealing with the
problems of false positives.

00:26:18.700 --> 00:26:21.880
&gt;&gt; Thank you so much.

00:26:21.880 --> 00:26:24.910
&gt;&gt; So a few years ago, I
started noticing that a couple

00:26:24.910 --> 00:26:27.360
of neighborhoods had speed
bumps starting to show

00:26:27.360 --> 00:26:30.510
up in their roads, and
I'm a civil engineer,

00:26:30.510 --> 00:26:34.260
and I was kind of, you know,
wondering why that was.

00:26:34.260 --> 00:26:37.330
I wasn't using Google
Maps at the time.

00:26:37.330 --> 00:26:38.820
Then I started using
Google Maps,

00:26:38.820 --> 00:26:40.460
and I was in a different city,

00:26:40.460 --> 00:26:42.500
one that I hadn't
driven through,

00:26:42.500 --> 00:26:45.570
and so it was directing
me on these roads to get

00:26:45.570 --> 00:26:47.230
to where I had to go,
and all of a sudden,

00:26:47.230 --> 00:26:50.020
I end up in this neighborhood,
and I'm hitting what appear

00:26:50.020 --> 00:26:52.630
to be newly constructed
speed bumps.

00:26:52.630 --> 00:26:55.050
So it became pretty clear
to me that what was going

00:26:55.050 --> 00:26:57.510
on was this was the
neighborhood's reaction

00:26:57.510 --> 00:27:04.300
to suddenly being turned into
an efficient route by big data.

00:27:04.300 --> 00:27:07.280
So you know, as civil engineers,
roads are designed, you know,

00:27:07.280 --> 00:27:10.710
interstates, you know,
state highway, you know,

00:27:10.710 --> 00:27:13.590
and you have big
buffers, things like that,

00:27:13.590 --> 00:27:16.540
and so one of the goals
is actually to keep all

00:27:16.540 --> 00:27:20.040
that traffic off of those
little side streets,

00:27:20.040 --> 00:27:22.420
not running through little
neighborhoods with little kids

00:27:22.420 --> 00:27:25.220
and stuff playing in front
yards, and keep as much

00:27:25.220 --> 00:27:30.560
of the traffic as possible
elsewhere, but time is money,

00:27:30.560 --> 00:27:32.290
and as a result, it appeared

00:27:32.290 --> 00:27:34.320
that everything had
been collapsed

00:27:34.320 --> 00:27:39.140
down to simple you can save 20
seconds of time on your route

00:27:39.140 --> 00:27:42.100
by doing this, which is not
really what the urban planners

00:27:42.100 --> 00:27:43.420
were intending.

00:27:43.420 --> 00:27:45.320
So anyway, it's just
an observation

00:27:45.320 --> 00:27:49.720
about how big data is
starting to drive that type

00:27:49.720 --> 00:27:52.680
of decision-making, you
know, based on what I can --

00:27:52.680 --> 00:27:57.080
as far as I can tell, maybe
like 20-second increments.

00:27:57.080 --> 00:27:58.380
&gt;&gt; Edward Tenner: Yeah.

00:27:58.380 --> 00:28:02.580
I mean I have followed the
story of Google Maps and Waze

00:28:02.580 --> 00:28:06.330
and the battles of neighborhoods
to deal with the traffic.

00:28:06.330 --> 00:28:10.940
In New Jersey, there have been
towns that have posted signs,

00:28:10.940 --> 00:28:15.230
had, you know, police
there to ticket people

00:28:15.230 --> 00:28:20.090
who were not there, but it turns
out because of this interaction

00:28:20.090 --> 00:28:25.100
between the technology and the
unexpected reactions of people,

00:28:25.100 --> 00:28:29.220
it is really impossible to
predict the actual behavior

00:28:29.220 --> 00:28:32.800
that will result from a
technological innovation.

00:28:35.030 --> 00:28:37.330
Thank you.

00:28:41.040 --> 00:28:43.760
&gt;&gt; I guess I would just
like to get your insight

00:28:43.760 --> 00:28:46.700
and what your opinion would be.

00:28:46.700 --> 00:28:51.380
If you look at the production of
food using agricultural methods,

00:28:51.380 --> 00:28:57.310
so to speak, going from using
pesticides to now going organic,

00:28:57.310 --> 00:29:00.360
I guess the equivalence of
that would be being aware

00:29:00.360 --> 00:29:05.610
of how the systems change and
trying to be mindful of that,

00:29:05.610 --> 00:29:07.840
because I see that a
lot in Metro stations,

00:29:07.840 --> 00:29:09.530
where I'm a software developer,

00:29:09.530 --> 00:29:11.320
so I'm on the computer all day
-- I'm one of those people,

00:29:11.320 --> 00:29:15.110
by the way -- but the last
thing I want to do when I get

00:29:15.110 --> 00:29:16.780
out of work is look at a screen,

00:29:16.780 --> 00:29:18.560
and I'll be at the
Metro station,

00:29:18.560 --> 00:29:22.320
and the only thing
you see is this.

00:29:22.320 --> 00:29:25.210
I mean it's literally like
mind-drooling zombies.

00:29:25.210 --> 00:29:30.140
No offense to anybody, but do
you think there will be some --

00:29:30.140 --> 00:29:33.840
I guess that's the
only question I have.

00:29:33.840 --> 00:29:38.540
Do you there will be some
sort of awareness movement

00:29:38.540 --> 00:29:41.130
when the time comes
due to understand

00:29:41.130 --> 00:29:43.490
that there are implications
to this kind of behavior?

00:29:43.490 --> 00:29:46.590
&gt;&gt; Edward Tenner: You mean
for the overuse of screens?

00:29:46.590 --> 00:29:48.790
&gt;&gt; Yes. Well, technology,
so to speak.

00:29:48.790 --> 00:29:50.230
Big data usage.

00:29:50.230 --> 00:29:51.530
&gt;&gt; Edward Tenner: Yeah.

00:29:51.530 --> 00:29:57.090
Well, I think one surprise
has been that people

00:29:57.090 --> 00:30:03.370
in general are not as ready
to give up analog experience

00:30:03.370 --> 00:30:06.180
as the Silicon Valley and
many journalists have said.

00:30:06.180 --> 00:30:10.830
For example, at one point,
Jeff Bezos was talking

00:30:10.830 --> 00:30:15.000
about Kindle replacing printing
books entirely, and in fact,

00:30:15.000 --> 00:30:17.460
there's been a mild
resurgence of printed books.

00:30:17.460 --> 00:30:20.140
Now the publishing
industry has other problems,

00:30:20.140 --> 00:30:23.600
but there hasn't been a mass
exodus, and one reason for that,

00:30:23.600 --> 00:30:26.720
as I document in The
Efficiency Paradox,

00:30:26.720 --> 00:30:28.860
is it turns out that
young people,

00:30:28.860 --> 00:30:32.600
far from being great enthusiasts
for electronic textbooks,

00:30:32.600 --> 00:30:35.360
very strongly prefer
the print textbooks,

00:30:35.360 --> 00:30:38.800
and the textbook publishers
would much rather have

00:30:38.800 --> 00:30:42.150
electronic-only, because it
could be more much profitable.

00:30:42.150 --> 00:30:46.050
The book could go dead after
the subscription period

00:30:46.050 --> 00:30:47.350
and so forth.

00:30:47.350 --> 00:30:49.370
There are many, you know, many,
many more things you can do

00:30:49.370 --> 00:30:55.300
to increase your gross margin
with an electronic copy,

00:30:55.300 --> 00:30:58.390
but the point is that
even among people

00:30:58.390 --> 00:31:00.340
who are so-called
digital natives,

00:31:00.340 --> 00:31:03.380
there is an instinctive
realization that for a lot

00:31:03.380 --> 00:31:05.620
of things analog is better.

00:31:05.620 --> 00:31:11.630
The sales of pencils, for
example, have continued

00:31:11.630 --> 00:31:16.030
to rise decades after decades
and decades of computing

00:31:16.030 --> 00:31:17.330
because there are many,

00:31:17.330 --> 00:31:21.380
many things for which it is
simply better to use a pencil

00:31:21.380 --> 00:31:27.450
and paper than to try to convert
things to electronic form.

00:31:27.450 --> 00:31:30.250
On the other hand, there
are so many advantages

00:31:30.250 --> 00:31:32.590
to electronic writing.

00:31:32.590 --> 00:31:34.020
I was a very early convert.

00:31:34.020 --> 00:31:36.640
So to me, we don't
have to choose.

00:31:36.640 --> 00:31:38.170
That's one of the big
points of my book.

00:31:38.170 --> 00:31:40.970
We don't have to choose
between analog or digital,

00:31:40.970 --> 00:31:47.360
and my emphasis is really
on looking objectively

00:31:47.360 --> 00:31:50.710
on what each technology
is good for and using it

00:31:50.710 --> 00:31:52.930
for what it's good
for rather than trying

00:31:52.930 --> 00:31:56.830
to make everything an
ideological discussion.

00:31:56.830 --> 00:32:00.980
I mean that's an obvious point,
and what I said in my preface

00:32:00.980 --> 00:32:04.820
and what reviewers have
pointed out that I've said is

00:32:04.820 --> 00:32:08.450
that this is all obvious
once you've read it.

00:32:08.450 --> 00:32:12.580
So L. Ron Hubbard said the only
way for a writer to get rich is

00:32:12.580 --> 00:32:16.060
to start a religion, so
sometimes I've been tempted.

00:32:16.060 --> 00:32:17.370
Sometimes I've been tempted

00:32:17.370 --> 00:32:22.040
to start a new faith called
obviology [phonetic spelling]

00:32:22.040 --> 00:32:25.990
and to take the advantage
of all the religious liberty

00:32:25.990 --> 00:32:28.530
that people have been
talking about, so stay tuned.

00:32:28.530 --> 00:32:33.730
I may return in a
new incarnation.

00:32:33.730 --> 00:32:35.030
&gt;&gt; Thank you.

00:32:35.030 --> 00:32:36.330
&gt;&gt; Edward Tenner: Thanks.

00:32:36.330 --> 00:32:37.630
&gt;&gt; Very insightful.

00:32:37.630 --> 00:32:40.960
&gt;&gt; Hi. Thank you for
an interesting talk.

00:32:40.960 --> 00:32:43.910
I'm a software engineer
by profession,

00:32:43.910 --> 00:32:48.390
and my question is a
reaction to what you said,

00:32:48.390 --> 00:32:52.320
that you don't wish your talk to
be a critique of Silicon Valley.

00:32:52.320 --> 00:32:56.100
I was reminded of what one
of the founders of the AI lab

00:32:56.100 --> 00:32:59.460
in MIT, Joseph Weizenbaum,
wrote 30 years ago,

00:32:59.460 --> 00:33:03.330
a book called Computer
Power and Human Reason,

00:33:03.330 --> 00:33:08.250
and he wrote his disillusionment
with computers arose

00:33:08.250 --> 00:33:12.470
out of the fact that he wrote
it a toy AI program called ELIZA

00:33:12.470 --> 00:33:16.150
which would do some natural
language interaction with people

00:33:16.150 --> 00:33:22.700
and would do a very rudimentary
analysis of medical problems,

00:33:22.700 --> 00:33:26.760
and that simple toy
program seduced thousands

00:33:26.760 --> 00:33:31.350
of his customers into believing
that he was a real doctor,

00:33:31.350 --> 00:33:36.190
and he had to attend to their
questions every day about all --

00:33:36.190 --> 00:33:37.730
they had questions,
medical questions --

00:33:37.730 --> 00:33:40.250
thinking that the toy program
would give the solution.

00:33:40.250 --> 00:33:44.270
So my question to you is,
in a very general sense --

00:33:44.270 --> 00:33:45.570
I mean he believed

00:33:45.570 --> 00:33:49.960
that computers have not really
fundamentally changed society

00:33:49.960 --> 00:33:53.590
for the better, and you
see a reaction these days

00:33:53.590 --> 00:33:57.740
to all this gadgetry that is
being pushed by Silicon Valley,

00:33:57.740 --> 00:34:02.520
smartphones, 24 by
seven total, you know,

00:34:02.520 --> 00:34:06.220
involvement with
electronic devices.

00:34:06.220 --> 00:34:10.840
So my general question to
you is, in the overall scheme

00:34:10.840 --> 00:34:16.930
of things, is Silicon Valley
really doing a good job?

00:34:16.930 --> 00:34:19.690
&gt;&gt; Edward Tenner: Has
there been progress?

00:34:19.690 --> 00:34:20.990
Is that what you're asking?

00:34:20.990 --> 00:34:23.680
Well, I think, no, I would
like to put it another way.

00:34:23.680 --> 00:34:27.450
I would like to say,
rather than asking, well,

00:34:27.450 --> 00:34:29.800
what is the balance so
far -- what I would like,

00:34:29.800 --> 00:34:31.490
the way I would like to put it,

00:34:31.490 --> 00:34:34.770
is what is the prospect
for the future?

00:34:34.770 --> 00:34:38.140
What will be the best
way to use technology?

00:34:38.140 --> 00:34:40.760
Because I don't think that
anything is inevitable

00:34:40.760 --> 00:34:42.360
about how technology applies.

00:34:42.360 --> 00:34:44.510
I believe that you
and I and all of us

00:34:44.510 --> 00:34:48.570
in this room really are the
people, along with millions

00:34:48.570 --> 00:34:50.250
or billions of other people,

00:34:50.250 --> 00:34:53.640
who determine what
technology does, what it means.

00:34:53.640 --> 00:34:56.340
Our decisions can make
it absolutely terrible.

00:34:56.340 --> 00:35:00.340
Our decisions also can use
it to do a lot of good,

00:35:00.340 --> 00:35:03.890
to do real good in education
instead of being a fad

00:35:03.890 --> 00:35:07.950
in education, to make real
improvements in health,

00:35:07.950 --> 00:35:11.840
although I document
how the misapplication

00:35:11.840 --> 00:35:15.210
of it has resulted in actually
more paperwork for doctors.

00:35:15.210 --> 00:35:19.110
So I'm trying not to say that
there's anything inherent

00:35:19.110 --> 00:35:23.110
in the technology that makes
it one way or the other,

00:35:23.110 --> 00:35:27.160
and the great quotation
was from Mel Kranzberg,

00:35:27.160 --> 00:35:30.250
one of the founders of
the history of technology,

00:35:30.250 --> 00:35:35.620
and he said that
technology is not good

00:35:35.620 --> 00:35:37.650
or bad, nor is it neutral.

00:35:37.650 --> 00:35:40.100
So it's really what
we make of it.

00:35:40.100 --> 00:35:42.050
&gt;&gt; Thank you.

00:35:42.050 --> 00:35:44.080
[ Inaudible Question ]

00:35:44.080 --> 00:35:45.380
&gt;&gt; Edward Tenner: Yeah.

00:35:45.380 --> 00:35:46.680
&gt;&gt; Small question, please.

00:35:46.680 --> 00:35:47.980
It's more like a request.

00:35:47.980 --> 00:35:51.100
As an ordinary person, you
know, as far as I know,

00:35:51.100 --> 00:35:55.720
I am nobody and, you know, so
I'm sure there are lots and lots

00:35:55.720 --> 00:35:59.390
of us around the world as
that category, you know,

00:35:59.390 --> 00:36:02.220
which we belong to that
category rather than, let's see,

00:36:02.220 --> 00:36:04.510
I'm your daughter, because
you're so tech-savvy,

00:36:04.510 --> 00:36:07.370
and you know what is going on
and what's so analog and what's

00:36:07.370 --> 00:36:09.770
so digital -- that
sort of stuff.

00:36:09.770 --> 00:36:13.530
So from my understanding, I
know there's not much to speak

00:36:13.530 --> 00:36:16.790
about our time in terms of,
you know, explaining my psyche

00:36:16.790 --> 00:36:20.460
or what's going on behind
my head or my motivation,

00:36:20.460 --> 00:36:23.130
but I feel that I'm compared
to -- I'm an immigrant,

00:36:23.130 --> 00:36:26.720
also Tibetan, from India, and we
are political refugees in India,

00:36:26.720 --> 00:36:30.150
and Silicon Valley of
India is Bangladesh.

00:36:30.150 --> 00:36:31.560
So I belong to that state.

00:36:31.560 --> 00:36:34.220
You know, I'm from a village
five hours from the city,

00:36:34.220 --> 00:36:37.060
but that is where some of
the software, you know,

00:36:37.060 --> 00:36:38.680
bring producers down there.

00:36:38.680 --> 00:36:41.750
So therefore, compared
to other immigrants,

00:36:41.750 --> 00:36:45.150
I feel I'm more fortunate
kind of background, possibly.

00:36:45.150 --> 00:36:49.290
You know, even if I don't know,
I might have a lot of help all

00:36:49.290 --> 00:36:52.110
around the world, sort of, but
then there are so many others,

00:36:52.110 --> 00:36:55.190
and it's so heartbreaking,
and I know I have thousands

00:36:55.190 --> 00:36:56.490
of questions in my head.

00:36:56.490 --> 00:36:59.890
So how should we proceed if it
is too late or it's too foolish

00:36:59.890 --> 00:37:01.880
to be struggling so hard?

00:37:01.880 --> 00:37:03.180
Why don't you get it?

00:37:03.180 --> 00:37:04.860
And this is how the
world is organized,

00:37:04.860 --> 00:37:08.110
and this is how it has been
going on since the 1960s

00:37:08.110 --> 00:37:10.300
or the Cold War or this,
blah, blah, blah, you know,

00:37:10.300 --> 00:37:12.530
and probably more than a
hundred years -- who knows --

00:37:12.530 --> 00:37:14.630
or maybe within the last ten
years or, you know, as you say,

00:37:14.630 --> 00:37:16.820
Amazon and Jeff Bezos
and all that, you know,

00:37:16.820 --> 00:37:21.020
taking over Washington Post,
and so how should we think

00:37:21.020 --> 00:37:24.840
and function to be safe
and also, you know,

00:37:24.840 --> 00:37:26.470
if it is still not too late,

00:37:26.470 --> 00:37:28.190
and if I believe the
way I believe that,

00:37:28.190 --> 00:37:30.390
it looks like we are
still kind of part

00:37:30.390 --> 00:37:33.280
of the normal so-called --

00:37:33.280 --> 00:37:35.330
you know, I'm a very
religious person --

00:37:35.330 --> 00:37:38.170
so that there's still
humanity left, that, you know,

00:37:38.170 --> 00:37:40.800
if we walk the right walk
and talk the right talk,

00:37:40.800 --> 00:37:44.450
we should be pretty safe, or
else this is what is the reality

00:37:44.450 --> 00:37:47.050
and you should not
fight but rather respect

00:37:47.050 --> 00:37:49.840
and also seek help and not
challenging and all that.

00:37:49.840 --> 00:37:52.710
If you do that, you see that
that's how it can be done,

00:37:52.710 --> 00:37:54.480
and it's one second,
or it's two second,

00:37:54.480 --> 00:37:56.740
and the whole D.C.
could be wiped out

00:37:56.740 --> 00:37:58.050
or the whole American
could be wiped out

00:37:58.050 --> 00:38:00.640
or the whole world
could be gone,

00:38:00.640 --> 00:38:02.650
so therefore you'd
better behave.

00:38:02.650 --> 00:38:03.950
Do not disrespect.

00:38:03.950 --> 00:38:06.300
Do not be racially biased,
this, that, you know,

00:38:06.300 --> 00:38:08.250
just know their power
or capability.

00:38:08.250 --> 00:38:10.340
How are we supposed
to, you know --

00:38:10.340 --> 00:38:13.440
please, if you haven't thought
about it, which I find it

00:38:13.440 --> 00:38:17.680
so funny, because as educated
and as so forward thinkers

00:38:17.680 --> 00:38:21.140
as you guys, it looks like,
you know, not much focus

00:38:21.140 --> 00:38:22.440
on the ordinary stuff.

00:38:22.440 --> 00:38:26.470
Rather you are too immersed in
the very, very specific thing,

00:38:26.470 --> 00:38:29.260
you know, which is so great,
but okay, don't you feel like,

00:38:29.260 --> 00:38:31.150
you know, you are neglecting us?

00:38:31.150 --> 00:38:33.860
And so therefore, how
should we perceive, sir,

00:38:33.860 --> 00:38:36.300
that there's still humanity
left, as in, you know,

00:38:36.300 --> 00:38:38.740
I'm a Tibetan, or you're
Chinese, or you're Nepalese,

00:38:38.740 --> 00:38:41.310
or you're Bangladeshi,
or there's no such thing?

00:38:41.310 --> 00:38:43.040
Just get on with it, you know?

00:38:43.040 --> 00:38:44.340
&gt;&gt; Edward Tenner: Yeah.

00:38:44.340 --> 00:38:45.640
&gt;&gt; Thank you so much.

00:38:45.640 --> 00:38:46.940
&gt;&gt; Edward Tenner: Thank you.

00:38:46.940 --> 00:38:48.240
Thank you.

00:38:48.240 --> 00:38:49.540
My book really does not try

00:38:49.540 --> 00:38:50.840
to answer ultimate
questions, so it's really --

00:38:50.840 --> 00:38:52.140
&gt;&gt; Yes, please do, and let
others do, please, you know,

00:38:52.140 --> 00:38:54.030
so that we can get more
education to people.

00:38:54.030 --> 00:38:55.330
&gt;&gt; Edward Tenner: Yeah.

00:38:55.330 --> 00:38:56.630
&gt;&gt; I'm kind of a little
bit fortunate one.

00:38:56.630 --> 00:38:57.930
&gt;&gt; Edward Tenner: Thank you.

00:38:57.930 --> 00:38:59.230
Thanks for your remarks.

00:38:59.230 --> 00:39:00.530
&gt;&gt; Thank you.

00:39:00.530 --> 00:39:02.130
&gt;&gt; Edward Tenner: I can't really
answer the deepest questions.

00:39:02.130 --> 00:39:03.490
I stick to the obvious.

00:39:03.490 --> 00:39:04.790
So thank you.

00:39:04.790 --> 00:39:07.120
&gt;&gt; Great. Thanks.

00:39:07.120 --> 00:39:09.730
So I really appreciated
what you said before

00:39:09.730 --> 00:39:13.520
about technology being neither
good nor evil nor neutral,

00:39:13.520 --> 00:39:17.030
and I'm just wondering
what your thoughts are

00:39:17.030 --> 00:39:21.660
on engaging policy makers who
may or may not know the details

00:39:21.660 --> 00:39:26.530
of any particular new emerging
technology and how these kind

00:39:26.530 --> 00:39:29.120
of people who are making
important decisions can balance

00:39:29.120 --> 00:39:34.650
precaution with taking
advantage of improved efficiency

00:39:34.650 --> 00:39:38.380
and opportunities that
new technologies present.

00:39:38.380 --> 00:39:39.860
&gt;&gt; Edward Tenner:
That's a great question.

00:39:39.860 --> 00:39:44.320
When I was an editor at
Princeton University Press,

00:39:44.320 --> 00:39:46.950
I would always cringe
when somebody had a book

00:39:46.950 --> 00:39:49.430
that was intended
for policy makers

00:39:49.430 --> 00:39:52.820
because the policy makers, I'm
not sure if they read books,

00:39:52.820 --> 00:39:54.190
but they certainly
don't buy them.

00:39:54.190 --> 00:39:58.860
&gt;&gt; Certainly not that
kind of book, right?

00:39:58.860 --> 00:40:01.240
&gt;&gt; Edward Tenner: It was
not a very favorable thing

00:40:01.240 --> 00:40:03.050
when I heard about it, and
I generally don't really --

00:40:03.050 --> 00:40:07.620
I generally don't write
about government policy,

00:40:07.620 --> 00:40:09.820
and one reason for
that is that so often,

00:40:09.820 --> 00:40:12.150
policy has had unintended
consequences.

00:40:12.150 --> 00:40:16.420
For example, in Ellen Brant's
book Cigarette Century,

00:40:16.420 --> 00:40:19.890
which is a wonderful account
of the unintended consequences

00:40:19.890 --> 00:40:22.870
of an automated cigarette
rolling machine

00:40:22.870 --> 00:40:28.380
that was patented about 1880
for the rise of world deaths

00:40:28.380 --> 00:40:34.530
in smoking, this was probably
the most lethal invention

00:40:34.530 --> 00:40:39.920
of all time, greater,
probably, than nuclear weapons.

00:40:39.920 --> 00:40:43.250
And so the problem
that I've seen is

00:40:43.250 --> 00:40:46.750
that very often these
policy changes --

00:40:46.750 --> 00:40:50.700
for example, breaking up James
B. Duke's American Tobacco.

00:40:50.700 --> 00:40:53.700
Duke was the one who really
worked with the inventor,

00:40:53.700 --> 00:40:56.970
Bonsack, to turn
cigarette-making

00:40:56.970 --> 00:41:02.250
from a hand-rolling operation
like cigars to something

00:41:02.250 --> 00:41:05.400
where machines could
produce, you know,

00:41:05.400 --> 00:41:10.330
millions of cigarettes a day,
and the federal government,

00:41:10.330 --> 00:41:11.830
they didn't object to smoking.

00:41:11.830 --> 00:41:15.410
They just thought that
Duke was really hogging it

00:41:15.410 --> 00:41:18.480
and was abusing the
monopoly of the machine,

00:41:18.480 --> 00:41:22.650
and so we had American
Tobacco broken up just

00:41:22.650 --> 00:41:28.700
as Standard Oil was broken up,
and the result was that instead

00:41:28.700 --> 00:41:32.110
of a tobacco monopoly,
you had an oligarchy

00:41:32.110 --> 00:41:34.590
that was even harder
to dislodge.

00:41:34.590 --> 00:41:36.170
It took many years.

00:41:36.170 --> 00:41:40.490
It took decades until the
surgeon general's report.

00:41:40.490 --> 00:41:44.690
So my problem with policy is

00:41:44.690 --> 00:41:47.380
that always there are these
unintended consequences,

00:41:47.380 --> 00:41:53.590
and so my emphasis is on helping
readers to understand policy

00:41:53.590 --> 00:41:58.720
for themselves, and possibly,
if there's enough influence,

00:41:58.720 --> 00:42:01.430
if there are enough
people, then I might be able

00:42:01.430 --> 00:42:04.090
to do a little bit to
help move the culture.

00:42:04.090 --> 00:42:05.570
&gt;&gt; Thank you very much.

00:42:05.570 --> 00:42:06.870
&gt;&gt; Edward Tenner: Thank you.

00:42:06.870 --> 00:42:10.590
&gt;&gt; I wanted you to put
together your research

00:42:10.590 --> 00:42:14.220
on Waze a little bit and
some of your conclusions.

00:42:14.220 --> 00:42:16.850
I have two observations
that I want.

00:42:16.850 --> 00:42:22.100
One is that when I don't
use Waze, it's hurt my life.

00:42:22.100 --> 00:42:24.140
I'm a cyclist who
goes to beach towns,

00:42:24.140 --> 00:42:27.330
and Waze is now routing
car traffic

00:42:27.330 --> 00:42:30.880
through the less traveled
roads that I used to enjoy.

00:42:30.880 --> 00:42:34.440
And two is it'll often
in a rural area put you

00:42:34.440 --> 00:42:38.760
on one very small route,
but they put a lot of people

00:42:38.760 --> 00:42:41.020
on the same route, and then
the whole thing kind of breaks

00:42:41.020 --> 00:42:43.630
down if there's a
traffic problem.

00:42:43.630 --> 00:42:46.530
So just broadly, if you
can say a couple of things

00:42:46.530 --> 00:42:49.840
about what you're finding, and
more broadly, the implications

00:42:49.840 --> 00:42:54.200
of other -- you alluded
to certain key findings,

00:42:54.200 --> 00:42:56.970
and I thought you were sort
of going to maybe mention

00:42:56.970 --> 00:42:59.750
where else you've seen what
you see happening with Waze.

00:42:59.750 --> 00:43:01.050
&gt;&gt; Edward Tenner: Thanks.

00:43:01.050 --> 00:43:06.390
I wish I had more time, but the
signal is that the time is up,

00:43:06.390 --> 00:43:10.860
so I leave it to individuals
to experiment with Waze.

00:43:10.860 --> 00:43:12.160
Really, it's free.

00:43:12.160 --> 00:43:13.460
It's worth trying.

00:43:13.460 --> 00:43:14.760
Who knows?

00:43:14.760 --> 00:43:16.830
You may be like me and work
your way up to Royalty.

00:43:16.830 --> 00:43:21.400
And I'll be glad
to talk one-on-one,

00:43:21.400 --> 00:43:23.320
but I think we have to --

00:43:23.320 --> 00:43:24.930
&gt;&gt; There's one question.

00:43:24.930 --> 00:43:26.230
&gt;&gt; Edward Tenner: I'm sorry.

00:43:26.230 --> 00:43:27.530
Yeah.

00:43:27.530 --> 00:43:28.830
&gt;&gt; Very short question.

00:43:28.830 --> 00:43:30.780
Yeah. What's your view and also
the expectation for the AI?

00:43:30.780 --> 00:43:34.090
You know, AI developing
very rapidly right now,

00:43:34.090 --> 00:43:35.470
and for the future.

00:43:35.470 --> 00:43:38.570
Somebody say so AI maybe
control the whole world.

00:43:38.570 --> 00:43:41.380
What's your view and
the expectation for AI,

00:43:41.380 --> 00:43:43.390
artificial intelligence?

00:43:43.390 --> 00:43:47.680
&gt;&gt; Edward Tenner: Well,
as far as world control

00:43:47.680 --> 00:43:51.300
through artificial intelligence,
I mean I have some views

00:43:51.300 --> 00:43:56.210
on that, but with the limits
on time, I think I would have

00:43:56.210 --> 00:44:01.080
to talk about that privately,
so I hope you'll understand,

00:44:01.080 --> 00:44:03.450
you know, that we
have a very, very,

00:44:03.450 --> 00:44:08.860
very persuasive analog
time keeper,

00:44:08.860 --> 00:44:11.150
and I need to follow
her direction.

00:44:11.150 --> 00:44:13.510
So thank you very much.

00:44:13.510 --> 00:44:16.660
[ Applause ]

