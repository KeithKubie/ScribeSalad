WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.720
there are some similarities between the

00:00:02.720 --> 00:00:02.730
there are some similarities between the
 

00:00:02.730 --> 00:00:04.340
there are some similarities between the
sequence the sequence machine

00:00:04.340 --> 00:00:04.350
sequence the sequence machine
 

00:00:04.350 --> 00:00:06.860
sequence the sequence machine
translation model and the language

00:00:06.860 --> 00:00:06.870
translation model and the language
 

00:00:06.870 --> 00:00:08.600
translation model and the language
models that you have worked with in the

00:00:08.600 --> 00:00:08.610
models that you have worked with in the
 

00:00:08.610 --> 00:00:11.509
models that you have worked with in the
first week of this course but there are

00:00:11.509 --> 00:00:11.519
first week of this course but there are
 

00:00:11.519 --> 00:00:13.490
first week of this course but there are
some significant differences as well

00:00:13.490 --> 00:00:13.500
some significant differences as well
 

00:00:13.500 --> 00:00:16.220
some significant differences as well
let's take a look so you can think of

00:00:16.220 --> 00:00:16.230
let's take a look so you can think of
 

00:00:16.230 --> 00:00:18.370
let's take a look so you can think of
machine translation as building a

00:00:18.370 --> 00:00:18.380
machine translation as building a
 

00:00:18.380 --> 00:00:21.260
machine translation as building a
conditional language model here's what I

00:00:21.260 --> 00:00:21.270
conditional language model here's what I
 

00:00:21.270 --> 00:00:23.810
conditional language model here's what I
mean in language modeling this was the

00:00:23.810 --> 00:00:23.820
mean in language modeling this was the
 

00:00:23.820 --> 00:00:26.810
mean in language modeling this was the
network we had built in the first week

00:00:26.810 --> 00:00:26.820
network we had built in the first week
 

00:00:26.820 --> 00:00:31.580
network we had built in the first week
and this model allows you to estimate

00:00:31.580 --> 00:00:31.590
and this model allows you to estimate
 

00:00:31.590 --> 00:00:35.540
and this model allows you to estimate
the probability of a sentence that's

00:00:35.540 --> 00:00:35.550
the probability of a sentence that's
 

00:00:35.550 --> 00:00:38.900
the probability of a sentence that's
what a language model does and you can

00:00:38.900 --> 00:00:38.910
what a language model does and you can
 

00:00:38.910 --> 00:00:41.000
what a language model does and you can
also use this to generate novel

00:00:41.000 --> 00:00:41.010
also use this to generate novel
 

00:00:41.010 --> 00:00:44.750
also use this to generate novel
sentences and sometimes we were writing

00:00:44.750 --> 00:00:44.760
sentences and sometimes we were writing
 

00:00:44.760 --> 00:00:48.200
sentences and sometimes we were writing
x1 and x2 here where in this example x2

00:00:48.200 --> 00:00:48.210
x1 and x2 here where in this example x2
 

00:00:48.210 --> 00:00:52.040
x1 and x2 here where in this example x2
would be equal to y1 or equal to Y hat 1

00:00:52.040 --> 00:00:52.050
would be equal to y1 or equal to Y hat 1
 

00:00:52.050 --> 00:00:55.220
would be equal to y1 or equal to Y hat 1
is just a feedback but X 1 X 2 and so on

00:00:55.220 --> 00:00:55.230
is just a feedback but X 1 X 2 and so on
 

00:00:55.230 --> 00:00:57.139
is just a feedback but X 1 X 2 and so on
weren't important so just to clean this

00:00:57.139 --> 00:00:57.149
weren't important so just to clean this
 

00:00:57.149 --> 00:00:59.119
weren't important so just to clean this
up for this slide I'm gonna just cross

00:00:59.119 --> 00:00:59.129
up for this slide I'm gonna just cross
 

00:00:59.129 --> 00:01:00.950
up for this slide I'm gonna just cross
these all right where X 1 could be the

00:01:00.950 --> 00:01:00.960
these all right where X 1 could be the
 

00:01:00.960 --> 00:01:03.650
these all right where X 1 could be the
vector of all zeros and X 2 X 3 are just

00:01:03.650 --> 00:01:03.660
vector of all zeros and X 2 X 3 are just
 

00:01:03.660 --> 00:01:05.810
vector of all zeros and X 2 X 3 are just
the previous output you were generating

00:01:05.810 --> 00:01:05.820
the previous output you were generating
 

00:01:05.820 --> 00:01:09.679
the previous output you were generating
okay so that was the language model the

00:01:09.679 --> 00:01:09.689
okay so that was the language model the
 

00:01:09.689 --> 00:01:11.780
okay so that was the language model the
machine translation model looks as

00:01:11.780 --> 00:01:11.790
machine translation model looks as
 

00:01:11.790 --> 00:01:13.190
machine translation model looks as
follows I'm going to use a couple

00:01:13.190 --> 00:01:13.200
follows I'm going to use a couple
 

00:01:13.200 --> 00:01:16.160
follows I'm going to use a couple
different colors green and purple to

00:01:16.160 --> 00:01:16.170
different colors green and purple to
 

00:01:16.170 --> 00:01:18.560
different colors green and purple to
denote respectively the encoding network

00:01:18.560 --> 00:01:18.570
denote respectively the encoding network
 

00:01:18.570 --> 00:01:20.870
denote respectively the encoding network
in green and the decoding network in

00:01:20.870 --> 00:01:20.880
in green and the decoding network in
 

00:01:20.880 --> 00:01:24.800
in green and the decoding network in
purple and you notice that the decoder

00:01:24.800 --> 00:01:24.810
purple and you notice that the decoder
 

00:01:24.810 --> 00:01:28.789
purple and you notice that the decoder
network looks pretty much identical to

00:01:28.789 --> 00:01:28.799
network looks pretty much identical to
 

00:01:28.799 --> 00:01:33.249
network looks pretty much identical to
the language model that we had up there

00:01:33.249 --> 00:01:33.259
the language model that we had up there
 

00:01:33.259 --> 00:01:36.050
the language model that we had up there
so what the machine translation model is

00:01:36.050 --> 00:01:36.060
so what the machine translation model is
 

00:01:36.060 --> 00:01:38.630
so what the machine translation model is
is very similar to the language model

00:01:38.630 --> 00:01:38.640
is very similar to the language model
 

00:01:38.640 --> 00:01:40.850
is very similar to the language model
except that Oh instead of always

00:01:40.850 --> 00:01:40.860
except that Oh instead of always
 

00:01:40.860 --> 00:01:42.830
except that Oh instead of always
starting along with the vector of all

00:01:42.830 --> 00:01:42.840
starting along with the vector of all
 

00:01:42.840 --> 00:01:45.859
starting along with the vector of all
zeros it instead has an encoding network

00:01:45.859 --> 00:01:45.869
zeros it instead has an encoding network
 

00:01:45.869 --> 00:01:48.950
zeros it instead has an encoding network
that figures out some representation for

00:01:48.950 --> 00:01:48.960
that figures out some representation for
 

00:01:48.960 --> 00:01:50.539
that figures out some representation for
the input sentence and then takes that

00:01:50.539 --> 00:01:50.549
the input sentence and then takes that
 

00:01:50.549 --> 00:01:53.090
the input sentence and then takes that
input sentence and starts off the

00:01:53.090 --> 00:01:53.100
input sentence and starts off the
 

00:01:53.100 --> 00:01:55.249
input sentence and starts off the
decoding network with a representation

00:01:55.249 --> 00:01:55.259
decoding network with a representation
 

00:01:55.259 --> 00:01:57.889
decoding network with a representation
of the input sentence rather than with

00:01:57.889 --> 00:01:57.899
of the input sentence rather than with
 

00:01:57.899 --> 00:02:02.209
of the input sentence rather than with
the representation of all zeros so

00:02:02.209 --> 00:02:02.219
the representation of all zeros so
 

00:02:02.219 --> 00:02:05.410
the representation of all zeros so
that's why I call this a conditional

00:02:05.410 --> 00:02:05.420
that's why I call this a conditional
 

00:02:05.420 --> 00:02:08.930
that's why I call this a conditional
language model and instead of modeling

00:02:08.930 --> 00:02:08.940
language model and instead of modeling
 

00:02:08.940 --> 00:02:11.390
language model and instead of modeling
the probability of any sentence it is

00:02:11.390 --> 00:02:11.400
the probability of any sentence it is
 

00:02:11.400 --> 00:02:14.150
the probability of any sentence it is
now modeling the probability of

00:02:14.150 --> 00:02:14.160
now modeling the probability of
 

00:02:14.160 --> 00:02:17.410
now modeling the probability of
say the output English translation

00:02:17.410 --> 00:02:17.420
say the output English translation
 

00:02:17.420 --> 00:02:22.270
say the output English translation
conditions on some input French sentence

00:02:22.270 --> 00:02:22.280
conditions on some input French sentence
 

00:02:22.280 --> 00:02:24.260
conditions on some input French sentence
so in other words you're trying to

00:02:24.260 --> 00:02:24.270
so in other words you're trying to
 

00:02:24.270 --> 00:02:27.440
so in other words you're trying to
estimate the probability of a english

00:02:27.440 --> 00:02:27.450
estimate the probability of a english
 

00:02:27.450 --> 00:02:29.630
estimate the probability of a english
translation like what's the chance that

00:02:29.630 --> 00:02:29.640
translation like what's the chance that
 

00:02:29.640 --> 00:02:32.240
translation like what's the chance that
the translation is Jane is visiting

00:02:32.240 --> 00:02:32.250
the translation is Jane is visiting
 

00:02:32.250 --> 00:02:35.180
the translation is Jane is visiting
Africa in September but conditions on

00:02:35.180 --> 00:02:35.190
Africa in September but conditions on
 

00:02:35.190 --> 00:02:39.620
Africa in September but conditions on
the input French census like John busy

00:02:39.620 --> 00:02:39.630
the input French census like John busy
 

00:02:39.630 --> 00:02:42.920
the input French census like John busy
cooler freak on Sept on so this is

00:02:42.920 --> 00:02:42.930
cooler freak on Sept on so this is
 

00:02:42.930 --> 00:02:45.020
cooler freak on Sept on so this is
really the probability of an English

00:02:45.020 --> 00:02:45.030
really the probability of an English
 

00:02:45.030 --> 00:02:48.020
really the probability of an English
sentence conditions on an input French

00:02:48.020 --> 00:02:48.030
sentence conditions on an input French
 

00:02:48.030 --> 00:02:49.310
sentence conditions on an input French
sentence which is why it is a

00:02:49.310 --> 00:02:49.320
sentence which is why it is a
 

00:02:49.320 --> 00:02:52.430
sentence which is why it is a
conditional language model now if you

00:02:52.430 --> 00:02:52.440
conditional language model now if you
 

00:02:52.440 --> 00:02:54.800
conditional language model now if you
want to apply this model to actually

00:02:54.800 --> 00:02:54.810
want to apply this model to actually
 

00:02:54.810 --> 00:02:57.650
want to apply this model to actually
translate a sentence from French into

00:02:57.650 --> 00:02:57.660
translate a sentence from French into
 

00:02:57.660 --> 00:03:01.480
translate a sentence from French into
English given this input French sentence

00:03:01.480 --> 00:03:01.490
English given this input French sentence
 

00:03:01.490 --> 00:03:04.430
English given this input French sentence
the model might tell you what is the

00:03:04.430 --> 00:03:04.440
the model might tell you what is the
 

00:03:04.440 --> 00:03:06.770
the model might tell you what is the
probability of different corresponding

00:03:06.770 --> 00:03:06.780
probability of different corresponding
 

00:03:06.780 --> 00:03:09.650
probability of different corresponding
English translations right so this is X

00:03:09.650 --> 00:03:09.660
English translations right so this is X
 

00:03:09.660 --> 00:03:12.620
English translations right so this is X
is the French sentence zombies equal

00:03:12.620 --> 00:03:12.630
is the French sentence zombies equal
 

00:03:12.630 --> 00:03:15.290
is the French sentence zombies equal
frequency long and this now tells you

00:03:15.290 --> 00:03:15.300
frequency long and this now tells you
 

00:03:15.300 --> 00:03:18.740
frequency long and this now tells you
was the probability of different English

00:03:18.740 --> 00:03:18.750
was the probability of different English
 

00:03:18.750 --> 00:03:22.960
was the probability of different English
translations of that French input and

00:03:22.960 --> 00:03:22.970
translations of that French input and
 

00:03:22.970 --> 00:03:26.180
translations of that French input and
what you do not want is to sample

00:03:26.180 --> 00:03:26.190
what you do not want is to sample
 

00:03:26.190 --> 00:03:30.080
what you do not want is to sample
outputs at random if you sample words

00:03:30.080 --> 00:03:30.090
outputs at random if you sample words
 

00:03:30.090 --> 00:03:33.230
outputs at random if you sample words
from this distribution P of Y given X

00:03:33.230 --> 00:03:33.240
from this distribution P of Y given X
 

00:03:33.240 --> 00:03:35.570
from this distribution P of Y given X
maybe one time you get a pretty good

00:03:35.570 --> 00:03:35.580
maybe one time you get a pretty good
 

00:03:35.580 --> 00:03:37.280
maybe one time you get a pretty good
translation jane is visiting Africa in

00:03:37.280 --> 00:03:37.290
translation jane is visiting Africa in
 

00:03:37.290 --> 00:03:39.770
translation jane is visiting Africa in
September but maybe another time you get

00:03:39.770 --> 00:03:39.780
September but maybe another time you get
 

00:03:39.780 --> 00:03:41.420
September but maybe another time you get
a different translation Jane's clearly

00:03:41.420 --> 00:03:41.430
a different translation Jane's clearly
 

00:03:41.430 --> 00:03:43.270
a different translation Jane's clearly
visiting Africa in September which is

00:03:43.270 --> 00:03:43.280
visiting Africa in September which is
 

00:03:43.280 --> 00:03:45.710
visiting Africa in September which is
sounds a little awkward but it's not a

00:03:45.710 --> 00:03:45.720
sounds a little awkward but it's not a
 

00:03:45.720 --> 00:03:47.480
sounds a little awkward but it's not a
terrible translation just not the best

00:03:47.480 --> 00:03:47.490
terrible translation just not the best
 

00:03:47.490 --> 00:03:50.690
terrible translation just not the best
one and sometimes just by chance you get

00:03:50.690 --> 00:03:50.700
one and sometimes just by chance you get
 

00:03:50.700 --> 00:03:53.180
one and sometimes just by chance you get
study others it's a tempering this

00:03:53.180 --> 00:03:53.190
study others it's a tempering this
 

00:03:53.190 --> 00:03:55.580
study others it's a tempering this
Africa and maybe just by chance

00:03:55.580 --> 00:03:55.590
Africa and maybe just by chance
 

00:03:55.590 --> 00:03:57.110
Africa and maybe just by chance
sometimes you sample a really bad

00:03:57.110 --> 00:03:57.120
sometimes you sample a really bad
 

00:03:57.120 --> 00:03:59.060
sometimes you sample a really bad
translation African frame welcome Jane

00:03:59.060 --> 00:03:59.070
translation African frame welcome Jane
 

00:03:59.070 --> 00:04:02.270
translation African frame welcome Jane
in September so when you're using this

00:04:02.270 --> 00:04:02.280
in September so when you're using this
 

00:04:02.280 --> 00:04:04.940
in September so when you're using this
model for machine translation you're not

00:04:04.940 --> 00:04:04.950
model for machine translation you're not
 

00:04:04.950 --> 00:04:07.699
model for machine translation you're not
trying to some poor random from this

00:04:07.699 --> 00:04:07.709
trying to some poor random from this
 

00:04:07.709 --> 00:04:10.430
trying to some poor random from this
distribution instead what you're like is

00:04:10.430 --> 00:04:10.440
distribution instead what you're like is
 

00:04:10.440 --> 00:04:13.580
distribution instead what you're like is
to find a English sentence Y that

00:04:13.580 --> 00:04:13.590
to find a English sentence Y that
 

00:04:13.590 --> 00:04:16.330
to find a English sentence Y that
maximizes that conditional probability

00:04:16.330 --> 00:04:16.340
maximizes that conditional probability
 

00:04:16.340 --> 00:04:19.430
maximizes that conditional probability
so in developing a machine translation

00:04:19.430 --> 00:04:19.440
so in developing a machine translation
 

00:04:19.440 --> 00:04:22.219
so in developing a machine translation
system one of the things you need to do

00:04:22.219 --> 00:04:22.229
system one of the things you need to do
 

00:04:22.229 --> 00:04:24.460
system one of the things you need to do
is come up with an algorithm that

00:04:24.460 --> 00:04:24.470
is come up with an algorithm that
 

00:04:24.470 --> 00:04:28.200
is come up with an algorithm that
and actually find the value of y that

00:04:28.200 --> 00:04:28.210
and actually find the value of y that
 

00:04:28.210 --> 00:04:31.510
and actually find the value of y that
maximizes this term over here

00:04:31.510 --> 00:04:31.520
maximizes this term over here
 

00:04:31.520 --> 00:04:33.520
maximizes this term over here
the most common algorithm for doing this

00:04:33.520 --> 00:04:33.530
the most common algorithm for doing this
 

00:04:33.530 --> 00:04:35.500
the most common algorithm for doing this
called the beam search is something you

00:04:35.500 --> 00:04:35.510
called the beam search is something you
 

00:04:35.510 --> 00:04:38.440
called the beam search is something you
see in the next video but before moving

00:04:38.440 --> 00:04:38.450
see in the next video but before moving
 

00:04:38.450 --> 00:04:40.090
see in the next video but before moving
on to describe beam search you might

00:04:40.090 --> 00:04:40.100
on to describe beam search you might
 

00:04:40.100 --> 00:04:43.510
on to describe beam search you might
wonder why not just use greedy search so

00:04:43.510 --> 00:04:43.520
wonder why not just use greedy search so
 

00:04:43.520 --> 00:04:45.070
wonder why not just use greedy search so
what a speedy search what greedy search

00:04:45.070 --> 00:04:45.080
what a speedy search what greedy search
 

00:04:45.080 --> 00:04:47.530
what a speedy search what greedy search
is an algorithm from computer science

00:04:47.530 --> 00:04:47.540
is an algorithm from computer science
 

00:04:47.540 --> 00:04:49.900
is an algorithm from computer science
which says to generate the first word

00:04:49.900 --> 00:04:49.910
which says to generate the first word
 

00:04:49.910 --> 00:04:51.580
which says to generate the first word
just pick whatever is the most likely

00:04:51.580 --> 00:04:51.590
just pick whatever is the most likely
 

00:04:51.590 --> 00:04:54.310
just pick whatever is the most likely
first word according to your conditional

00:04:54.310 --> 00:04:54.320
first word according to your conditional
 

00:04:54.320 --> 00:04:56.820
first word according to your conditional
language model could go into your

00:04:56.820 --> 00:04:56.830
language model could go into your
 

00:04:56.830 --> 00:04:59.740
language model could go into your
machine translation model and then after

00:04:59.740 --> 00:04:59.750
machine translation model and then after
 

00:04:59.750 --> 00:05:02.020
machine translation model and then after
having paid the first word you then pick

00:05:02.020 --> 00:05:02.030
having paid the first word you then pick
 

00:05:02.030 --> 00:05:03.730
having paid the first word you then pick
whatever is the second word that seems

00:05:03.730 --> 00:05:03.740
whatever is the second word that seems
 

00:05:03.740 --> 00:05:05.920
whatever is the second word that seems
most likely and then pick the third word

00:05:05.920 --> 00:05:05.930
most likely and then pick the third word
 

00:05:05.930 --> 00:05:07.450
most likely and then pick the third word
that seems to most likely and this

00:05:07.450 --> 00:05:07.460
that seems to most likely and this
 

00:05:07.460 --> 00:05:11.640
that seems to most likely and this
algorithm is called greedy search and

00:05:11.640 --> 00:05:11.650
algorithm is called greedy search and
 

00:05:11.650 --> 00:05:14.380
algorithm is called greedy search and
what you're reading life is to pick the

00:05:14.380 --> 00:05:14.390
what you're reading life is to pick the
 

00:05:14.390 --> 00:05:17.980
what you're reading life is to pick the
entire sequence of words you know y 1 y

00:05:17.980 --> 00:05:17.990
entire sequence of words you know y 1 y
 

00:05:17.990 --> 00:05:24.190
entire sequence of words you know y 1 y
2 up to y t y paths there that maximizes

00:05:24.190 --> 00:05:24.200
2 up to y t y paths there that maximizes
 

00:05:24.200 --> 00:05:26.530
2 up to y t y paths there that maximizes
the joint probability of that whole

00:05:26.530 --> 00:05:26.540
the joint probability of that whole
 

00:05:26.540 --> 00:05:29.620
the joint probability of that whole
thing and it turns out that the greedy

00:05:29.620 --> 00:05:29.630
thing and it turns out that the greedy
 

00:05:29.630 --> 00:05:31.300
thing and it turns out that the greedy
approach where you just pick the best

00:05:31.300 --> 00:05:31.310
approach where you just pick the best
 

00:05:31.310 --> 00:05:33.550
approach where you just pick the best
first word and then after having paid

00:05:33.550 --> 00:05:33.560
first word and then after having paid
 

00:05:33.560 --> 00:05:34.960
first word and then after having paid
the best first we're trying to pick the

00:05:34.960 --> 00:05:34.970
the best first we're trying to pick the
 

00:05:34.970 --> 00:05:37.240
the best first we're trying to pick the
best second word and then after that

00:05:37.240 --> 00:05:37.250
best second word and then after that
 

00:05:37.250 --> 00:05:39.490
best second word and then after that
trying to pick the best third word that

00:05:39.490 --> 00:05:39.500
trying to pick the best third word that
 

00:05:39.500 --> 00:05:41.740
trying to pick the best third word that
approach doesn't really work to

00:05:41.740 --> 00:05:41.750
approach doesn't really work to
 

00:05:41.750 --> 00:05:43.480
approach doesn't really work to
illustrate that let's consider following

00:05:43.480 --> 00:05:43.490
illustrate that let's consider following
 

00:05:43.490 --> 00:05:45.790
illustrate that let's consider following
two translations the first one is a

00:05:45.790 --> 00:05:45.800
two translations the first one is a
 

00:05:45.800 --> 00:05:48.730
two translations the first one is a
better translation so hopefully in our

00:05:48.730 --> 00:05:48.740
better translation so hopefully in our
 

00:05:48.740 --> 00:05:51.159
better translation so hopefully in our
machine translation model it will say

00:05:51.159 --> 00:05:51.169
machine translation model it will say
 

00:05:51.169 --> 00:05:55.330
machine translation model it will say
that P of Y given X is higher for the

00:05:55.330 --> 00:05:55.340
that P of Y given X is higher for the
 

00:05:55.340 --> 00:05:57.580
that P of Y given X is higher for the
first sentence it's just a better more

00:05:57.580 --> 00:05:57.590
first sentence it's just a better more
 

00:05:57.590 --> 00:05:59.350
first sentence it's just a better more
succinct translation of the French

00:05:59.350 --> 00:05:59.360
succinct translation of the French
 

00:05:59.360 --> 00:06:01.719
succinct translation of the French
inputs and the second one is not a bad

00:06:01.719 --> 00:06:01.729
inputs and the second one is not a bad
 

00:06:01.729 --> 00:06:03.850
inputs and the second one is not a bad
translations is just more verbose has

00:06:03.850 --> 00:06:03.860
translations is just more verbose has
 

00:06:03.860 --> 00:06:07.180
translations is just more verbose has
more unnecessary words but if the

00:06:07.180 --> 00:06:07.190
more unnecessary words but if the
 

00:06:07.190 --> 00:06:09.550
more unnecessary words but if the
algorithm has paid Jane is as the first

00:06:09.550 --> 00:06:09.560
algorithm has paid Jane is as the first
 

00:06:09.560 --> 00:06:13.030
algorithm has paid Jane is as the first
two words because going is a more common

00:06:13.030 --> 00:06:13.040
two words because going is a more common
 

00:06:13.040 --> 00:06:16.540
two words because going is a more common
English words probably the chance of

00:06:16.540 --> 00:06:16.550
English words probably the chance of
 

00:06:16.550 --> 00:06:21.640
English words probably the chance of
Jane is going given the French input

00:06:21.640 --> 00:06:21.650
Jane is going given the French input
 

00:06:21.650 --> 00:06:24.130
Jane is going given the French input
this might actually be higher then the

00:06:24.130 --> 00:06:24.140
this might actually be higher then the
 

00:06:24.140 --> 00:06:28.600
this might actually be higher then the
chance of Jane is visiting given the

00:06:28.600 --> 00:06:28.610
chance of Jane is visiting given the
 

00:06:28.610 --> 00:06:32.800
chance of Jane is visiting given the
input sentence input French sentence and

00:06:32.800 --> 00:06:32.810
input sentence input French sentence and
 

00:06:32.810 --> 00:06:35.440
input sentence input French sentence and
so it's quite possible that if you just

00:06:35.440 --> 00:06:35.450
so it's quite possible that if you just
 

00:06:35.450 --> 00:06:37.120
so it's quite possible that if you just
pick the third word based on whatever

00:06:37.120 --> 00:06:37.130
pick the third word based on whatever
 

00:06:37.130 --> 00:06:37.959
pick the third word based on whatever
maximize

00:06:37.959 --> 00:06:37.969
maximize
 

00:06:37.969 --> 00:06:39.459
maximize
is the probability of just the first

00:06:39.459 --> 00:06:39.469
is the probability of just the first
 

00:06:39.469 --> 00:06:42.339
is the probability of just the first
three words you end up choosing option

00:06:42.339 --> 00:06:42.349
three words you end up choosing option
 

00:06:42.349 --> 00:06:47.229
three words you end up choosing option
number two but this ultimately ends up

00:06:47.229 --> 00:06:47.239
number two but this ultimately ends up
 

00:06:47.239 --> 00:06:50.739
number two but this ultimately ends up
resulting in a less optimal sentence in

00:06:50.739 --> 00:06:50.749
resulting in a less optimal sentence in
 

00:06:50.749 --> 00:06:52.509
resulting in a less optimal sentence in
the less good sentence as measured by

00:06:52.509 --> 00:06:52.519
the less good sentence as measured by
 

00:06:52.519 --> 00:06:58.059
the less good sentence as measured by
this model for P of Y given X I know

00:06:58.059 --> 00:06:58.069
this model for P of Y given X I know
 

00:06:58.069 --> 00:07:00.179
this model for P of Y given X I know
this was maybe a slightly hand wavy

00:07:00.179 --> 00:07:00.189
this was maybe a slightly hand wavy
 

00:07:00.189 --> 00:07:03.850
this was maybe a slightly hand wavy
arguments but this is an example of a

00:07:03.850 --> 00:07:03.860
arguments but this is an example of a
 

00:07:03.860 --> 00:07:06.489
arguments but this is an example of a
broader phenomenon where if you want to

00:07:06.489 --> 00:07:06.499
broader phenomenon where if you want to
 

00:07:06.499 --> 00:07:09.309
broader phenomenon where if you want to
find the sequence of words y 1 y 2 all

00:07:09.309 --> 00:07:09.319
find the sequence of words y 1 y 2 all
 

00:07:09.319 --> 00:07:11.199
find the sequence of words y 1 y 2 all
the way up to the final word that

00:07:11.199 --> 00:07:11.209
the way up to the final word that
 

00:07:11.209 --> 00:07:12.839
the way up to the final word that
together and maximize the probability

00:07:12.839 --> 00:07:12.849
together and maximize the probability
 

00:07:12.849 --> 00:07:16.299
together and maximize the probability
it's not always optimal to just pick one

00:07:16.299 --> 00:07:16.309
it's not always optimal to just pick one
 

00:07:16.309 --> 00:07:19.929
it's not always optimal to just pick one
word at a time and of course the total

00:07:19.929 --> 00:07:19.939
word at a time and of course the total
 

00:07:19.939 --> 00:07:22.419
word at a time and of course the total
number of combinations of words in a

00:07:22.419 --> 00:07:22.429
number of combinations of words in a
 

00:07:22.429 --> 00:07:25.419
number of combinations of words in a
English sentence is exponentially large

00:07:25.419 --> 00:07:25.429
English sentence is exponentially large
 

00:07:25.429 --> 00:07:28.929
English sentence is exponentially large
so if you have just 10,000 words in the

00:07:28.929 --> 00:07:28.939
so if you have just 10,000 words in the
 

00:07:28.939 --> 00:07:31.949
so if you have just 10,000 words in the
dictionary and if you're contemplating

00:07:31.949 --> 00:07:31.959
dictionary and if you're contemplating
 

00:07:31.959 --> 00:07:34.689
dictionary and if you're contemplating
translations are up to ten words long

00:07:34.689 --> 00:07:34.699
translations are up to ten words long
 

00:07:34.699 --> 00:07:38.829
translations are up to ten words long
then there are ten thousand to the ten

00:07:38.829 --> 00:07:38.839
then there are ten thousand to the ten
 

00:07:38.839 --> 00:07:41.619
then there are ten thousand to the ten
possible sentences that are ten words

00:07:41.619 --> 00:07:41.629
possible sentences that are ten words
 

00:07:41.629 --> 00:07:44.499
possible sentences that are ten words
long picking words from vocabulary size

00:07:44.499 --> 00:07:44.509
long picking words from vocabulary size
 

00:07:44.509 --> 00:07:47.319
long picking words from vocabulary size
of from dictionary size of ten thousand

00:07:47.319 --> 00:07:47.329
of from dictionary size of ten thousand
 

00:07:47.329 --> 00:07:49.749
of from dictionary size of ten thousand
words so this is just a huge space of

00:07:49.749 --> 00:07:49.759
words so this is just a huge space of
 

00:07:49.759 --> 00:07:52.059
words so this is just a huge space of
possible sentences and it's impossible

00:07:52.059 --> 00:07:52.069
possible sentences and it's impossible
 

00:07:52.069 --> 00:07:54.609
possible sentences and it's impossible
to enumerate them all which is why the

00:07:54.609 --> 00:07:54.619
to enumerate them all which is why the
 

00:07:54.619 --> 00:07:56.559
to enumerate them all which is why the
most common thing to do is to use an

00:07:56.559 --> 00:07:56.569
most common thing to do is to use an
 

00:07:56.569 --> 00:08:00.850
most common thing to do is to use an
approximate search algorithm and what an

00:08:00.850 --> 00:08:00.860
approximate search algorithm and what an
 

00:08:00.860 --> 00:08:02.979
approximate search algorithm and what an
approximate search algorithm does is it

00:08:02.979 --> 00:08:02.989
approximate search algorithm does is it
 

00:08:02.989 --> 00:08:05.469
approximate search algorithm does is it
will try it won't always succeed but it

00:08:05.469 --> 00:08:05.479
will try it won't always succeed but it
 

00:08:05.479 --> 00:08:08.319
will try it won't always succeed but it
will try to pick the sentence Y that

00:08:08.319 --> 00:08:08.329
will try to pick the sentence Y that
 

00:08:08.329 --> 00:08:11.289
will try to pick the sentence Y that
maximizes that conditional probability

00:08:11.289 --> 00:08:11.299
maximizes that conditional probability
 

00:08:11.299 --> 00:08:14.439
maximizes that conditional probability
and even though it's not guaranteed to

00:08:14.439 --> 00:08:14.449
and even though it's not guaranteed to
 

00:08:14.449 --> 00:08:16.779
and even though it's not guaranteed to
find the value of y that maximizes this

00:08:16.779 --> 00:08:16.789
find the value of y that maximizes this
 

00:08:16.789 --> 00:08:19.389
find the value of y that maximizes this
it usually does a good enough job so the

00:08:19.389 --> 00:08:19.399
it usually does a good enough job so the
 

00:08:19.399 --> 00:08:21.129
it usually does a good enough job so the
sunrise in this video you saw how

00:08:21.129 --> 00:08:21.139
sunrise in this video you saw how
 

00:08:21.139 --> 00:08:23.519
sunrise in this video you saw how
machine translation can be posed as a

00:08:23.519 --> 00:08:23.529
machine translation can be posed as a
 

00:08:23.529 --> 00:08:26.109
machine translation can be posed as a
conditional language modeling problem

00:08:26.109 --> 00:08:26.119
conditional language modeling problem
 

00:08:26.119 --> 00:08:28.659
conditional language modeling problem
but one major difference between this

00:08:28.659 --> 00:08:28.669
but one major difference between this
 

00:08:28.669 --> 00:08:30.429
but one major difference between this
and the early language modeling problems

00:08:30.429 --> 00:08:30.439
and the early language modeling problems
 

00:08:30.439 --> 00:08:32.740
and the early language modeling problems
is rather than wanting to generate a

00:08:32.740 --> 00:08:32.750
is rather than wanting to generate a
 

00:08:32.750 --> 00:08:35.170
is rather than wanting to generate a
sentence at random you maybe want to try

00:08:35.170 --> 00:08:35.180
sentence at random you maybe want to try
 

00:08:35.180 --> 00:08:37.749
sentence at random you maybe want to try
to find the most likely English sentence

00:08:37.749 --> 00:08:37.759
to find the most likely English sentence
 

00:08:37.759 --> 00:08:40.600
to find the most likely English sentence
the most likely English translation but

00:08:40.600 --> 00:08:40.610
the most likely English translation but
 

00:08:40.610 --> 00:08:42.999
the most likely English translation but
the set of all English sentences of a

00:08:42.999 --> 00:08:43.009
the set of all English sentences of a
 

00:08:43.009 --> 00:08:45.059
the set of all English sentences of a
certain length is too large to

00:08:45.059 --> 00:08:45.069
certain length is too large to
 

00:08:45.069 --> 00:08:48.100
certain length is too large to
exhaustively enumerate so we have to

00:08:48.100 --> 00:08:48.110
exhaustively enumerate so we have to
 

00:08:48.110 --> 00:08:51.370
exhaustively enumerate so we have to
resort to a search algorithm so of that

00:08:51.370 --> 00:08:51.380
resort to a search algorithm so of that
 

00:08:51.380 --> 00:08:51.940
resort to a search algorithm so of that
the

00:08:51.940 --> 00:08:51.950
the
 

00:08:51.950 --> 00:08:53.410
the
go on to the next video where you

00:08:53.410 --> 00:08:53.420
go on to the next video where you
 

00:08:53.420 --> 00:08:57.250
go on to the next video where you
learned about the beam search arrow

