WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:03.370
So in order to deal with those problems,
we're going to introduce an extension to

00:00:03.370 --> 00:00:08.098
markup decision processes called
TDD MDPs as opposed to plain old MDPs.

00:00:08.098 --> 00:00:09.310
&gt;&gt; TDD MDPs.

00:00:09.310 --> 00:00:11.845
&gt;&gt; TDD MDPs because it
was off the tongue.

00:00:11.845 --> 00:00:13.241
[LAUGH]
&gt;&gt; What does it mean?

00:00:13.241 --> 00:00:17.130
&gt;&gt; Well it means
Targeted Trajectory Distribution

00:00:17.130 --> 00:00:17.940
&gt;&gt; Obviously.

00:00:17.940 --> 00:00:21.140
&gt;&gt; And that actually you can see that
when you think about how we go from

00:00:21.140 --> 00:00:22.880
an MDP to a TTDMDP.

00:00:22.880 --> 00:00:25.520
So what we are trying to do is we want
to make explicit those things we said

00:00:25.520 --> 00:00:29.240
before, so instead of having states,
we now have Trajectories and

00:00:29.240 --> 00:00:34.710
that's just all partial plot sequences
that we can't have, that we can't reach,

00:00:34.710 --> 00:00:37.770
that we can't get to, so it's the
sequence so far, it's the story so far.

00:00:38.970 --> 00:00:40.930
I try to go back and
forth between trajectories and

00:00:40.930 --> 00:00:43.940
sequences as opposed to stories because
this is actually more general than

00:00:43.940 --> 00:00:45.250
the story example that we're using.

00:00:45.250 --> 00:00:47.260
Actions are exactly the same
as they were before.

00:00:47.260 --> 00:00:50.700
We have a model, but now because we
have trajectories instead of states,

00:00:50.700 --> 00:00:53.750
we have a model that says well,
if I'm in a trajectory and

00:00:53.750 --> 00:00:57.690
I take a particular action, what's
the next trajectory that I will see?

00:00:57.690 --> 00:01:00.200
But it's still basically the same
things except that really

00:01:00.200 --> 00:01:04.390
it elevates this notion of trajectories
to sort of to a first class citizen and

00:01:04.390 --> 00:01:06.610
that we're reasoning
over trajectories now.

00:01:06.610 --> 00:01:11.050
And now instead of having a simple
reward, we have a target distribution.

00:01:11.050 --> 00:01:15.520
Now here I've made a sort of capital
T instead of a lowercase T and that's

00:01:15.520 --> 00:01:18.760
because the lower case T here was sort
of standing for trajectories the capital

00:01:18.760 --> 00:01:23.780
T stands for final trajectories
you know that is ends of stories.

00:01:23.780 --> 00:01:26.040
So you don't have a distribution or

00:01:26.040 --> 00:01:28.920
I guess because they have a probability
of zero for a partial dish but

00:01:28.920 --> 00:01:31.930
all which we really care about
is stories that we care about.

00:01:31.930 --> 00:01:35.860
So things that have higher probability
are stories that you might consider good

00:01:35.860 --> 00:01:39.540
and things that have lower probability,
stories that you might consider bad or

00:01:39.540 --> 00:01:40.210
less desirable.

00:01:40.210 --> 00:01:41.244
Does that make sense?

00:01:41.244 --> 00:01:44.848
&gt;&gt; I guess so, but why not just pick
the one that we want and just and

00:01:44.848 --> 00:01:49.472
call that I guess the distribution where
the right story has probability of 1 and

00:01:49.472 --> 00:01:51.933
everything else is 0
seems to make sense.

00:01:51.933 --> 00:01:54.146
&gt;&gt; Right so that's a sort of strict
generalization of what we were

00:01:54.146 --> 00:01:54.742
talking about.

00:01:54.742 --> 00:01:57.290
This is a strict generalization of
what we're talking about before.

00:01:57.290 --> 00:02:01.120
So this allows us to say,
some things are better than others and

00:02:01.120 --> 00:02:04.990
it turns out by relaxing this hard
constraint that I'm trying to find, I'm

00:02:04.990 --> 00:02:09.990
trying to maximize reward but instead
I'm trying to match a distribution.

00:02:09.990 --> 00:02:14.220
It allows more replay ability,
allows players to do a few more things,

00:02:14.220 --> 00:02:20.940
because now our goal is not to find
a policy that maps states to actions in

00:02:20.940 --> 00:02:26.230
order to maximize long term reward,
it's to find a probabilistic policy.

00:02:26.230 --> 00:02:30.220
That is a probability
distribution over actions, okay.

00:02:30.220 --> 00:02:33.270
And what is the optimal policy?

00:02:33.270 --> 00:02:37.340
It's the one, that if I use
this probability distribution,

00:02:37.340 --> 00:02:40.210
would lead to be matching
the target distribution.

00:02:40.210 --> 00:02:40.880
&gt;&gt; I see.

00:02:40.880 --> 00:02:41.500
&gt;&gt; Right?

00:02:41.500 --> 00:02:44.680
&gt;&gt; And just just so
that I'm still on board here.

00:02:44.680 --> 00:02:48.400
The actions here are the things
that the story can do?

00:02:48.400 --> 00:02:50.540
&gt;&gt; Right.
The story or the story manager or

00:02:50.540 --> 00:02:51.970
the system can do.

00:02:51.970 --> 00:02:54.600
&gt;&gt; And the uncertainty for that thing.

00:02:54.600 --> 00:02:57.290
The uncertainty is, well,
what is the player actually going to do?

00:02:57.290 --> 00:02:57.980
We don't know.

00:02:57.980 --> 00:02:58.800
&gt;&gt; Right.
&gt;&gt; Okay, so

00:02:58.800 --> 00:03:01.940
it's not the players actions
it's the story's actions

00:03:01.940 --> 00:03:03.730
&gt;&gt; Right, exactly it's story actions or

00:03:04.730 --> 00:03:08.220
system actions or manager actions or
lots of ways to sort of think about.

00:03:08.220 --> 00:03:11.115
&gt;&gt; Things like we're going to make
a wolf appear right about now.

00:03:11.115 --> 00:03:14.426
&gt;&gt; [SOUND] Yes exactly,
that was me doing a wolf.

00:03:14.426 --> 00:03:15.950
&gt;&gt; Shoot I was really scared.

00:03:16.980 --> 00:03:21.970
&gt;&gt; Or I'm going to make a key appear or
I'm going to unlock a door.

00:03:21.970 --> 00:03:24.050
I'm going to lock a door because I
don't want you to go in this room.

00:03:24.050 --> 00:03:24.800
&gt;&gt; Got it.
Right.

00:03:24.800 --> 00:03:26.610
&gt;&gt; So, these are the kind of actions
that you might be able to take in

00:03:26.610 --> 00:03:27.550
the system.

00:03:27.550 --> 00:03:29.500
The model says well if I'm
in some trajectory and

00:03:29.500 --> 00:03:32.450
I take one of these story actions,
what plot point will I end up in next?

00:03:32.450 --> 00:03:33.890
Which rejectory will I see next?

00:03:33.890 --> 00:03:36.280
That depends upon the player and so

00:03:36.280 --> 00:03:40.770
all of your uncertainty here,
sort of probably comes from the player.

00:03:40.770 --> 00:03:42.110
That's where our inter P comes from.

00:03:42.110 --> 00:03:42.680
&gt;&gt; Okay.

00:03:42.680 --> 00:03:43.820
&gt;&gt; And
I'm trying to get some stories and

00:03:43.820 --> 00:03:45.850
some stories I want to happen
more often than others, but

00:03:45.850 --> 00:03:49.400
this allows the player
agency sort of published.

00:03:49.400 --> 00:03:54.120
Now there's a lot of math that I could
do here, but at the end of the day what

00:03:54.120 --> 00:03:58.960
you might think is that, well now
that I no longer have this sort of

00:03:58.960 --> 00:04:02.150
simple constraint of maximizing
my long term expected reward but

00:04:02.150 --> 00:04:05.600
I have probabilities and
randomness and probabilistic policies.

00:04:05.600 --> 00:04:08.260
This is going to be harder,
much harder to deal with.

00:04:08.260 --> 00:04:09.200
But it turns out it's not.

00:04:09.200 --> 00:04:11.830
This is often the case in
the sheen learning in AI.

00:04:11.830 --> 00:04:15.380
If you relax a hard constraint and
make it a soft constraint

00:04:15.380 --> 00:04:17.660
you make it something that's
more about probabilities and

00:04:17.660 --> 00:04:21.649
probabilistic sort of search, you
actually get more power out of it and

00:04:21.649 --> 00:04:22.960
you're able to solve hard problems.

00:04:22.960 --> 00:04:26.030
So without going into the details I
can point you to a paper that you can

00:04:26.030 --> 00:04:27.980
actually series of papers
that you can read here.

00:04:27.980 --> 00:04:33.090
It turns out that by going from a hard
constraint to probability distributions,

00:04:33.090 --> 00:04:36.210
you can actually solve
TDMDP in linear time.

00:04:37.300 --> 00:04:38.580
&gt;&gt; The linear in what?

00:04:38.580 --> 00:04:40.950
&gt;&gt; Linear in the length of a story.

00:04:40.950 --> 00:04:42.790
&gt;&gt; That seems unlikely.

00:04:42.790 --> 00:04:43.660
&gt;&gt; Well, I can prove it to you, but

00:04:43.660 --> 00:04:45.590
instead I'm just going to
have to read the paper.

