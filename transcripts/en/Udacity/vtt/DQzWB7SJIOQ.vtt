WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:03.420
Consider the problem of
the matrix-matrix multiply operation,

00:00:03.420 --> 00:00:05.860
C gets C+A*B.

00:00:05.860 --> 00:00:08.500
Suppose we run this operation
on an ideal cash machine.

00:00:08.500 --> 00:00:11.750
And let's make our usual
simplifying assumptions.

00:00:11.750 --> 00:00:16.100
Namely, that the matrix is square, and
stuff conveniently divides other stuff.

00:00:16.100 --> 00:00:19.310
Now if you want to minimize data
transfers in a memory hierarchy

00:00:19.310 --> 00:00:24.310
one way is to reorganize the computation
to multiply b by b sub-blocks.

00:00:24.310 --> 00:00:27.520
Again for simplicity let's
assume that b divides n.

00:00:27.520 --> 00:00:31.060
The corresponding pseudocode algorithm
would look something like this.

00:00:31.060 --> 00:00:34.720
Now to make this algorithm cache
efficient, you would just choose b so

00:00:34.720 --> 00:00:38.340
that these three b by
b blocks fit in cache.

00:00:38.340 --> 00:00:42.250
In other words, b ought to be
proportional to the square root of z.

00:00:42.250 --> 00:00:46.270
Now there's a subtle detail here
about what fitting in cache means.

00:00:46.270 --> 00:00:50.250
To see it, suppose the matrices
are stored in column major order.

00:00:50.250 --> 00:00:53.330
That means the elements of each
column are laid out consecutively

00:00:53.330 --> 00:00:56.810
in linear memory addresses,
with one column following the other.

00:00:56.810 --> 00:00:59.290
This layout is common in
linear algebra packages,

00:00:59.290 --> 00:01:01.970
though note that languages like C and
C++ assume,

00:01:01.970 --> 00:01:06.370
by default, that a two dimensional
array uses row major order.

00:01:06.370 --> 00:01:10.860
A little more formally,
the ij element maps to a linear address.

00:01:10.860 --> 00:01:17.300
The offset is i + j * s, where s is the
stride or sometimes leading dimension.

00:01:17.300 --> 00:01:19.270
It has to be at least n.

00:01:19.270 --> 00:01:22.620
Now let's take any b by b
block of one of the matrices.

00:01:22.620 --> 00:01:25.570
Let's say we just want to
fit this block into cache.

00:01:25.570 --> 00:01:29.670
Then it has to be the case
that z is at least L squared.

00:01:29.670 --> 00:01:33.110
In literature, this is sometimes
written as the following lower bound.

00:01:33.110 --> 00:01:36.423
This condition is called
the tall-cache assumption.

00:01:36.423 --> 00:01:40.120
The tall-cache assumption says
the cache should be taller in terms of

00:01:40.120 --> 00:01:44.940
the number of lines than it is wide in
terms of the number of words per line.

00:01:44.940 --> 00:01:46.640
Let's take a simple example.

00:01:46.640 --> 00:01:50.140
Suppose our matrix block is
exactly the size of the cache z

00:01:50.140 --> 00:01:52.400
which holds 16 words.

00:01:52.400 --> 00:01:55.370
Now suppose the line size is four words.

00:01:55.370 --> 00:01:59.480
In this case, the Tall-Cache Assumption
holds assuming the matrix is

00:01:59.480 --> 00:02:04.090
stored in column major layout
columns would map to complete lines.

00:02:04.090 --> 00:02:07.520
Now suppose instead that L
is equal to eight words,

00:02:07.520 --> 00:02:10.354
in this case the Tall-Cache Assumption
does not hold.

00:02:10.354 --> 00:02:15.175
Columns no longer fill full lines,
thus the four by four block

00:02:15.175 --> 00:02:19.315
doesn't actually fit in cache even
though the cache has enough capacity.

00:02:19.315 --> 00:02:22.480
Now the Tall-Cache Assumption is
actually a really interesting one.

00:02:22.480 --> 00:02:26.580
It's basically an artifact of how
we chose to lay out the matrices.

00:02:26.580 --> 00:02:30.890
Put differently, in a memory hierarchy
model and efficient algorithm might be

00:02:30.890 --> 00:02:34.990
linked to your choice of data structure
and how you choose to lay out the data.

00:02:34.990 --> 00:02:36.640
We'll see more examples
of this soon enough.

