WEBVTT
Kind: captions
Language: en

00:00:03.560 --> 00:00:06.410
--a lot of research showing
that people learn best

00:00:06.410 --> 00:00:09.350
when they, themselves,
become teachers

00:00:09.350 --> 00:00:14.690
and when they work interactively
in a hands-on kind of way.

00:00:14.690 --> 00:00:17.660
And also research showing
that people learn well

00:00:17.660 --> 00:00:19.430
with empirical feedback.

00:00:19.430 --> 00:00:21.860
So we designed a project
that combined all three

00:00:21.860 --> 00:00:26.270
of those elements to
help students become more

00:00:26.270 --> 00:00:31.340
expert at debiasing their own,
as well as others', judgments

00:00:31.340 --> 00:00:33.110
and decisions.

00:00:33.110 --> 00:00:37.100
We always began by having
students complete a decision

00:00:37.100 --> 00:00:38.010
exercise.

00:00:38.010 --> 00:00:41.540
And in this exercise,
there's usually

00:00:41.540 --> 00:00:45.230
three people who are depicted.

00:00:45.230 --> 00:00:49.310
And, for example, they
might be three surgeons.

00:00:49.310 --> 00:00:52.040
And each surgeon is
described just the way

00:00:52.040 --> 00:00:55.230
they would be on a regular
website in real life.

00:00:55.230 --> 00:00:58.400
So you see a little
picture about this surgeon

00:00:58.400 --> 00:01:02.030
and a description
of the surgeon.

00:01:02.030 --> 00:01:07.670
And then the participant
or student, in this case,

00:01:07.670 --> 00:01:09.920
has to decide which
surgeon they would

00:01:09.920 --> 00:01:13.850
like to see, imagining that
they needed a particular kind

00:01:13.850 --> 00:01:16.410
of surgery, like foot surgery.

00:01:16.410 --> 00:01:19.910
Now, we, ahead of
time, have inserted

00:01:19.910 --> 00:01:24.380
key diagnostic information into
the description of each surgeon

00:01:24.380 --> 00:01:27.470
so that a person
reading it can tell

00:01:27.470 --> 00:01:30.470
which surgeon is likely to
have the best success rate.

00:01:30.470 --> 00:01:32.510
And that's what we
call in decision theory

00:01:32.510 --> 00:01:34.920
a diagnostic or normative cue.

00:01:34.920 --> 00:01:37.390
It's the cue we
want them to use,

00:01:37.390 --> 00:01:41.810
it's the one that helps them
maximize their expected value.

00:01:41.810 --> 00:01:46.880
Then we also embedded a bunch of
non-diagnostic distractor cues

00:01:46.880 --> 00:01:50.670
that often lead people
to make biased choices.

00:01:50.670 --> 00:01:53.540
And these are things like
what the person looks

00:01:53.540 --> 00:01:57.930
like, their race, their
class, their ethnicity,

00:01:57.930 --> 00:02:01.850
their religion and, really,
every judgment and decision

00:02:01.850 --> 00:02:04.910
making bias we could
possibly load into it.

00:02:04.910 --> 00:02:07.370
Things like order
effects, the order

00:02:07.370 --> 00:02:10.160
in which different
information appears.

00:02:10.160 --> 00:02:16.700
And so, what we found is that
most respondents ended up

00:02:16.700 --> 00:02:21.800
choosing a surgeon who looked
like the American stereotype

00:02:21.800 --> 00:02:25.730
of what a surgeon looks
like, a Caucasian male.

00:02:25.730 --> 00:02:28.340
But, in fact, that
surgeon did not

00:02:28.340 --> 00:02:31.460
have the best expected
value or success rate.

00:02:31.460 --> 00:02:36.170
And so then we revealed
the data to students

00:02:36.170 --> 00:02:41.400
and they saw for themselves that
they had fallen into this trap.

00:02:41.400 --> 00:02:47.330
And what's nice about this
exercise is that, regardless

00:02:47.330 --> 00:02:49.640
of what your views
are politically

00:02:49.640 --> 00:02:52.460
about affirmative action
or no affirmative action,

00:02:52.460 --> 00:02:55.470
everyone agrees that
there is a right way,

00:02:55.470 --> 00:02:57.320
there is a right choice.

00:02:57.320 --> 00:03:01.160
And that you shouldn't be
using something like race

00:03:01.160 --> 00:03:03.860
to make the choice,
you should be using

00:03:03.860 --> 00:03:05.670
who has the best success rate.

00:03:05.670 --> 00:03:09.050
So that's just the
first part of the task.

00:03:09.050 --> 00:03:11.120
Once people see
their own responses

00:03:11.120 --> 00:03:12.770
and they see that
the class as a whole

00:03:12.770 --> 00:03:14.960
has made a suboptimal
choice, they

00:03:14.960 --> 00:03:17.960
feel highly motivated
to want to correct it

00:03:17.960 --> 00:03:21.350
and to want to teach
others how to avoid it.

00:03:21.350 --> 00:03:25.190
So that's when the real
innovation comes in.

00:03:25.190 --> 00:03:31.610
And, essentially, what we do
is we give them the opportunity

00:03:31.610 --> 00:03:34.790
to first, identify
all of the biases

00:03:34.790 --> 00:03:37.460
that are embedded
within the task.

00:03:37.460 --> 00:03:41.610
They have to name each
one and define each one.

00:03:41.610 --> 00:03:44.090
And then they have
to design what

00:03:44.090 --> 00:03:46.330
we call choice architecture.

00:03:46.330 --> 00:03:49.940
It's a term borrowed by Cass
Sunstein and Richard Thaler's

00:03:49.940 --> 00:03:56.240
book, where it's just a small
piece of information that

00:03:56.240 --> 00:04:00.380
helps other new
respondents to make

00:04:00.380 --> 00:04:05.450
a rational choice or a choice
that maximizes expected value.

00:04:05.450 --> 00:04:09.800
Amazon Mechanical Turk is
this online marketplace

00:04:09.800 --> 00:04:13.310
where you have somewhere
between probably 10 and 50,000

00:04:13.310 --> 00:04:17.450
individuals a day who are,
in this marketplace, who

00:04:17.450 --> 00:04:20.550
are taking surveys of
all different types.

00:04:20.550 --> 00:04:24.710
Some of them are serving as
validation for machine learning

00:04:24.710 --> 00:04:29.060
algorithms, others are
psychology experiments.

00:04:29.060 --> 00:04:31.040
So ours is just one
of hundreds, if not

00:04:31.040 --> 00:04:33.770
thousands, that are on
there that individuals

00:04:33.770 --> 00:04:35.180
are paid to take the survey.

00:04:35.180 --> 00:04:37.520
And that's broadly
nationally representative

00:04:37.520 --> 00:04:39.320
with a few differences.

00:04:39.320 --> 00:04:42.180
Students are split into teams.

00:04:42.180 --> 00:04:46.490
Each team, after listing all
of the decision biases that are

00:04:46.490 --> 00:04:51.170
embedded within the scenario,
work together to create a kind

00:04:51.170 --> 00:04:55.430
of choice architecture that
will be used in a real-world

00:04:55.430 --> 00:04:58.220
scenario-- actually
run online on MTurk--

00:04:58.220 --> 00:05:06.980
to try to debias participants
in that activity online.

00:05:06.980 --> 00:05:11.420
So they come up with a lot of
different ways of doing this.

00:05:11.420 --> 00:05:15.020
Sometimes they'll include
some sort of table

00:05:15.020 --> 00:05:18.020
to try to teach people how to
calculate an expected value

00:05:18.020 --> 00:05:19.370
through numbers.

00:05:19.370 --> 00:05:23.600
Sometimes they'll tell
people, don't consider

00:05:23.600 --> 00:05:25.550
the race of a doctor,
that shouldn't matter,

00:05:25.550 --> 00:05:29.300
or the gender of a doctor,
that shouldn't matter.

00:05:29.300 --> 00:05:31.880
And they'll decide as a
group which method they think

00:05:31.880 --> 00:05:37.790
will be the best way to
debias these people online.

00:05:37.790 --> 00:05:40.265
And we'll try to coach
them through that.

00:05:40.265 --> 00:05:43.160
What the HILT money
has enabled us to do

00:05:43.160 --> 00:05:48.410
is systematize a whole
set of aha moments.

00:05:48.410 --> 00:05:53.210
Because there are a number
of these moments as you

00:05:53.210 --> 00:05:55.760
unfold the exercise
with students,

00:05:55.760 --> 00:05:59.660
where there's the first aha,
where it's not such a good aha.

00:05:59.660 --> 00:06:03.110
People are like, ooh,
ooh, I made a bad choice.

00:06:03.110 --> 00:06:08.960
And then there's the, I
get to try to fix this.

00:06:08.960 --> 00:06:12.200
And then there's
the good aha, where

00:06:12.200 --> 00:06:14.820
they see how well
they did and then say,

00:06:14.820 --> 00:06:18.470
oh, so this worked
better than that.

00:06:18.470 --> 00:06:22.790
And then there's
even another aha,

00:06:22.790 --> 00:06:25.310
which is they see
what other teams did.

00:06:25.310 --> 00:06:31.190
And so that's really, I
would say, the third aha.

00:06:31.190 --> 00:06:37.260
And then the last aha,
actually, it occurs to me,

00:06:37.260 --> 00:06:39.540
they don't necessarily
see for themselves

00:06:39.540 --> 00:06:44.040
but we try to, at a meta level,
encourage them to understand

00:06:44.040 --> 00:06:46.530
that they can use
this kind of process

00:06:46.530 --> 00:06:50.220
for testing their
assumptions empirically

00:06:50.220 --> 00:06:52.380
in whatever setting they're in.

00:06:52.380 --> 00:06:55.770
And so we use this with
executive education students as

00:06:55.770 --> 00:07:00.150
well as Harvard
enrolled students

00:07:00.150 --> 00:07:02.250
to help them think
about, they don't

00:07:02.250 --> 00:07:06.120
have to just hold on to their
existing beliefs about what

00:07:06.120 --> 00:07:10.410
creates change, but actually
submit them to empirical tests.

00:07:10.410 --> 00:07:14.900
So we began this project
before the HILT funding

00:07:14.900 --> 00:07:16.860
in executive education.

00:07:16.860 --> 00:07:19.650
And we had one
scenario, we were having

00:07:19.650 --> 00:07:21.060
participants choose a surgeon.

00:07:21.060 --> 00:07:23.528
But we had this idea
that, this isn't just

00:07:23.528 --> 00:07:24.570
about choosing a surgeon.

00:07:24.570 --> 00:07:28.200
This can be about any choice
where there is a correct answer

00:07:28.200 --> 00:07:30.480
that people aren't getting to.

00:07:30.480 --> 00:07:34.080
And what HILT allowed us to
do and the reason we applied

00:07:34.080 --> 00:07:36.870
was we wanted to move this
beyond executive education

00:07:36.870 --> 00:07:41.730
to our degree program classes,
where we have undergrads,

00:07:41.730 --> 00:07:44.310
MD's, people from the School
of Public Health, the business

00:07:44.310 --> 00:07:47.220
school, and then test
this across a whole host

00:07:47.220 --> 00:07:50.010
of different scenarios,
so not just surgeons.

00:07:50.010 --> 00:07:53.790
And what HILT funding allowed
us to do was test this idea

00:07:53.790 --> 00:07:57.000
and see how it could move
beyond just executive education,

00:07:57.000 --> 00:07:59.873
in this one scenario, to a
whole host of different types

00:07:59.873 --> 00:08:02.040
of students and a whole
host of different scenarios,

00:08:02.040 --> 00:08:05.280
and really expand the
exercise and grow it.

00:08:05.280 --> 00:08:07.740
Without the structure
that HILT brings

00:08:07.740 --> 00:08:09.630
and the funding
that HILT brings,

00:08:09.630 --> 00:08:13.800
we probably wouldn't
press ourselves to really

00:08:13.800 --> 00:08:20.160
think hard about what are the
key learning mechanisms here.

00:08:20.160 --> 00:08:24.030
And what is exportable,
what is generalizable,

00:08:24.030 --> 00:08:26.760
what isn't, where
are the weak points?

00:08:26.760 --> 00:08:30.930
And we also certainly would
never have had a coach to help

00:08:30.930 --> 00:08:34.250
us have an outside
perspective on it.

