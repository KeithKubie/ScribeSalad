WEBVTT
Kind: captions
Language: en

00:00:03.090 --> 00:00:05.910
hi everybody my name is Hirini and i'm

00:00:05.910 --> 00:00:05.920
hi everybody my name is Hirini and i'm
 

00:00:05.920 --> 00:00:07.470
hi everybody my name is Hirini and i'm
gonna be talking about how to use neural

00:00:07.470 --> 00:00:07.480
gonna be talking about how to use neural
 

00:00:07.480 --> 00:00:10.170
gonna be talking about how to use neural
networks to model sequences in the

00:00:10.170 --> 00:00:10.180
networks to model sequences in the
 

00:00:10.180 --> 00:00:11.699
networks to model sequences in the
previous lecture you saw how you could

00:00:11.699 --> 00:00:11.709
previous lecture you saw how you could
 

00:00:11.709 --> 00:00:13.619
previous lecture you saw how you could
use a neural network to model a data set

00:00:13.619 --> 00:00:13.629
use a neural network to model a data set
 

00:00:13.629 --> 00:00:15.810
use a neural network to model a data set
of many examples the difference with

00:00:15.810 --> 00:00:15.820
of many examples the difference with
 

00:00:15.820 --> 00:00:18.240
of many examples the difference with
sequences is that each example consists

00:00:18.240 --> 00:00:18.250
sequences is that each example consists
 

00:00:18.250 --> 00:00:20.790
sequences is that each example consists
of multiple data points there can be a

00:00:20.790 --> 00:00:20.800
of multiple data points there can be a
 

00:00:20.800 --> 00:00:22.890
of multiple data points there can be a
variable number of these data points per

00:00:22.890 --> 00:00:22.900
variable number of these data points per
 

00:00:22.900 --> 00:00:25.290
variable number of these data points per
example and the data points can depend

00:00:25.290 --> 00:00:25.300
example and the data points can depend
 

00:00:25.300 --> 00:00:30.359
example and the data points can depend
on each other in complicated ways so a

00:00:30.359 --> 00:00:30.369
on each other in complicated ways so a
 

00:00:30.369 --> 00:00:32.040
on each other in complicated ways so a
sequence could be something like a

00:00:32.040 --> 00:00:32.050
sequence could be something like a
 

00:00:32.050 --> 00:00:34.170
sequence could be something like a
sentence like this morning I took the

00:00:34.170 --> 00:00:34.180
sentence like this morning I took the
 

00:00:34.180 --> 00:00:36.750
sentence like this morning I took the
dog for a walk this is one example but

00:00:36.750 --> 00:00:36.760
dog for a walk this is one example but
 

00:00:36.760 --> 00:00:39.030
dog for a walk this is one example but
it consists of multiple words and the

00:00:39.030 --> 00:00:39.040
it consists of multiple words and the
 

00:00:39.040 --> 00:00:41.220
it consists of multiple words and the
words depend on each other another

00:00:41.220 --> 00:00:41.230
words depend on each other another
 

00:00:41.230 --> 00:00:42.450
words depend on each other another
example would be something like a

00:00:42.450 --> 00:00:42.460
example would be something like a
 

00:00:42.460 --> 00:00:44.970
example would be something like a
medical record one medical record would

00:00:44.970 --> 00:00:44.980
medical record one medical record would
 

00:00:44.980 --> 00:00:47.340
medical record one medical record would
be one example but it consists of many

00:00:47.340 --> 00:00:47.350
be one example but it consists of many
 

00:00:47.350 --> 00:00:50.130
be one example but it consists of many
measurements another example would be

00:00:50.130 --> 00:00:50.140
measurements another example would be
 

00:00:50.140 --> 00:00:51.630
measurements another example would be
something like a speech waveform where

00:00:51.630 --> 00:00:51.640
something like a speech waveform where
 

00:00:51.640 --> 00:00:53.610
something like a speech waveform where
this one waveform is an example but

00:00:53.610 --> 00:00:53.620
this one waveform is an example but
 

00:00:53.620 --> 00:00:54.870
this one waveform is an example but
again it consists of many many

00:00:54.870 --> 00:00:54.880
again it consists of many many
 

00:00:54.880 --> 00:00:58.170
again it consists of many many
measurements you've probably encountered

00:00:58.170 --> 00:00:58.180
measurements you've probably encountered
 

00:00:58.180 --> 00:01:00.540
measurements you've probably encountered
sequence modeling tasks in your everyday

00:01:00.540 --> 00:01:00.550
sequence modeling tasks in your everyday
 

00:01:00.550 --> 00:01:01.950
sequence modeling tasks in your everyday
life especially if you've used things

00:01:01.950 --> 00:01:01.960
life especially if you've used things
 

00:01:01.960 --> 00:01:03.840
life especially if you've used things
like Google Translate alexa or siri

00:01:03.840 --> 00:01:03.850
like Google Translate alexa or siri
 

00:01:03.850 --> 00:01:05.999
like Google Translate alexa or siri
tasks like machine translation and

00:01:05.999 --> 00:01:06.009
tasks like machine translation and
 

00:01:06.009 --> 00:01:07.980
tasks like machine translation and
question answering are all sequence

00:01:07.980 --> 00:01:07.990
question answering are all sequence
 

00:01:07.990 --> 00:01:10.020
question answering are all sequence
modeling tasks and the state of the art

00:01:10.020 --> 00:01:10.030
modeling tasks and the state of the art
 

00:01:10.030 --> 00:01:11.609
modeling tasks and the state of the art
in these tasks is mostly deep-learning

00:01:11.609 --> 00:01:11.619
in these tasks is mostly deep-learning
 

00:01:11.619 --> 00:01:16.099
in these tasks is mostly deep-learning
based

00:01:16.099 --> 00:01:16.109
 

00:01:16.109 --> 00:01:18.599
another interesting example I saw

00:01:18.599 --> 00:01:18.609
another interesting example I saw
 

00:01:18.609 --> 00:01:21.090
another interesting example I saw
recently was the self-parking car by

00:01:21.090 --> 00:01:21.100
recently was the self-parking car by
 

00:01:21.100 --> 00:01:23.190
recently was the self-parking car by
Audi when you think about it parking is

00:01:23.190 --> 00:01:23.200
Audi when you think about it parking is
 

00:01:23.200 --> 00:01:25.349
Audi when you think about it parking is
also a sequence modeling task because

00:01:25.349 --> 00:01:25.359
also a sequence modeling task because
 

00:01:25.359 --> 00:01:27.419
also a sequence modeling task because
parking is just a sequence of movements

00:01:27.419 --> 00:01:27.429
parking is just a sequence of movements
 

00:01:27.429 --> 00:01:29.639
parking is just a sequence of movements
and the next movement depends on all the

00:01:29.639 --> 00:01:29.649
and the next movement depends on all the
 

00:01:29.649 --> 00:01:32.789
and the next movement depends on all the
previous movements you can watch the

00:01:32.789 --> 00:01:32.799
previous movements you can watch the
 

00:01:32.799 --> 00:01:37.200
previous movements you can watch the
rest of this video online ok so a

00:01:37.200 --> 00:01:37.210
rest of this video online ok so a
 

00:01:37.210 --> 00:01:39.599
rest of this video online ok so a
sequence modeling problem now I'm just

00:01:39.599 --> 00:01:39.609
sequence modeling problem now I'm just
 

00:01:39.609 --> 00:01:40.859
sequence modeling problem now I'm just
gonna walk through a sequence modeling

00:01:40.859 --> 00:01:40.869
gonna walk through a sequence modeling
 

00:01:40.869 --> 00:01:43.260
gonna walk through a sequence modeling
problem to kind of motivate why we need

00:01:43.260 --> 00:01:43.270
problem to kind of motivate why we need
 

00:01:43.270 --> 00:01:45.660
problem to kind of motivate why we need
a different framework for specifically

00:01:45.660 --> 00:01:45.670
a different framework for specifically
 

00:01:45.670 --> 00:01:47.879
a different framework for specifically
for modeling sequences and what we

00:01:47.879 --> 00:01:47.889
for modeling sequences and what we
 

00:01:47.889 --> 00:01:50.629
for modeling sequences and what we
should be looking for in that framework

00:01:50.629 --> 00:01:50.639
should be looking for in that framework
 

00:01:50.639 --> 00:01:53.190
should be looking for in that framework
so the problem is predicting the next

00:01:53.190 --> 00:01:53.200
so the problem is predicting the next
 

00:01:53.200 --> 00:01:55.919
so the problem is predicting the next
word given these words we want to

00:01:55.919 --> 00:01:55.929
word given these words we want to
 

00:01:55.929 --> 00:01:57.410
word given these words we want to
predict what comes next

00:01:57.410 --> 00:01:57.420
predict what comes next
 

00:01:57.420 --> 00:02:00.090
predict what comes next
the first problem we run into is that

00:02:00.090 --> 00:02:00.100
the first problem we run into is that
 

00:02:00.100 --> 00:02:02.219
the first problem we run into is that
machine learning models that are not

00:02:02.219 --> 00:02:02.229
machine learning models that are not
 

00:02:02.229 --> 00:02:03.870
machine learning models that are not
explicitly designed to deal with

00:02:03.870 --> 00:02:03.880
explicitly designed to deal with
 

00:02:03.880 --> 00:02:07.260
explicitly designed to deal with
sequences take as input a fixed length

00:02:07.260 --> 00:02:07.270
sequences take as input a fixed length
 

00:02:07.270 --> 00:02:09.749
sequences take as input a fixed length
vector think back to the feed-forward

00:02:09.749 --> 00:02:09.759
vector think back to the feed-forward
 

00:02:09.759 --> 00:02:11.670
vector think back to the feed-forward
neural network from the first lecture

00:02:11.670 --> 00:02:11.680
neural network from the first lecture
 

00:02:11.680 --> 00:02:14.610
neural network from the first lecture
that Alexander introduced we have to

00:02:14.610 --> 00:02:14.620
that Alexander introduced we have to
 

00:02:14.620 --> 00:02:16.559
that Alexander introduced we have to
specify the size of the input

00:02:16.559 --> 00:02:16.569
specify the size of the input
 

00:02:16.569 --> 00:02:18.240
specify the size of the input
right at the outset we can't sometimes

00:02:18.240 --> 00:02:18.250
right at the outset we can't sometimes
 

00:02:18.250 --> 00:02:20.399
right at the outset we can't sometimes
feed in a vector of length ten other

00:02:20.399 --> 00:02:20.409
feed in a vector of length ten other
 

00:02:20.409 --> 00:02:22.319
feed in a vector of length ten other
times feed an elector a vector of length

00:02:22.319 --> 00:02:22.329
times feed an elector a vector of length
 

00:02:22.329 --> 00:02:26.039
times feed an elector a vector of length
20 it has to be fixed length so this is

00:02:26.039 --> 00:02:26.049
20 it has to be fixed length so this is
 

00:02:26.049 --> 00:02:27.539
20 it has to be fixed length so this is
kind of an issue with sequences because

00:02:27.539 --> 00:02:27.549
kind of an issue with sequences because
 

00:02:27.549 --> 00:02:29.520
kind of an issue with sequences because
sometimes we might have seen ten words

00:02:29.520 --> 00:02:29.530
sometimes we might have seen ten words
 

00:02:29.530 --> 00:02:30.569
sometimes we might have seen ten words
and we want to predict the next word

00:02:30.569 --> 00:02:30.579
and we want to predict the next word
 

00:02:30.579 --> 00:02:32.369
and we want to predict the next word
sometimes we might have seen four words

00:02:32.369 --> 00:02:32.379
sometimes we might have seen four words
 

00:02:32.379 --> 00:02:34.229
sometimes we might have seen four words
and we want to predict the next word so

00:02:34.229 --> 00:02:34.239
and we want to predict the next word so
 

00:02:34.239 --> 00:02:35.759
and we want to predict the next word so
we have to get that variable length

00:02:35.759 --> 00:02:35.769
we have to get that variable length
 

00:02:35.769 --> 00:02:39.479
we have to get that variable length
input into a fixed length vector one

00:02:39.479 --> 00:02:39.489
input into a fixed length vector one
 

00:02:39.489 --> 00:02:42.119
input into a fixed length vector one
simple way to do this would be to just

00:02:42.119 --> 00:02:42.129
simple way to do this would be to just
 

00:02:42.129 --> 00:02:43.979
simple way to do this would be to just
cut off the vector so say okay we're

00:02:43.979 --> 00:02:43.989
cut off the vector so say okay we're
 

00:02:43.989 --> 00:02:46.530
cut off the vector so say okay we're
gonna just take a fixed window force

00:02:46.530 --> 00:02:46.540
gonna just take a fixed window force
 

00:02:46.540 --> 00:02:47.940
gonna just take a fixed window force
this vector to be fixed length by only

00:02:47.940 --> 00:02:47.950
this vector to be fixed length by only
 

00:02:47.950 --> 00:02:50.460
this vector to be fixed length by only
considering the previous two words no

00:02:50.460 --> 00:02:50.470
considering the previous two words no
 

00:02:50.470 --> 00:02:52.020
considering the previous two words no
matter where we're making the prediction

00:02:52.020 --> 00:02:52.030
matter where we're making the prediction
 

00:02:52.030 --> 00:02:53.580
matter where we're making the prediction
we'll just take the previous two words

00:02:53.580 --> 00:02:53.590
we'll just take the previous two words
 

00:02:53.590 --> 00:02:56.119
we'll just take the previous two words
and then try to predict the next word

00:02:56.119 --> 00:02:56.129
and then try to predict the next word
 

00:02:56.129 --> 00:02:59.129
and then try to predict the next word
now we can represent these two words as

00:02:59.129 --> 00:02:59.139
now we can represent these two words as
 

00:02:59.139 --> 00:03:02.550
now we can represent these two words as
a fixed length vector by creating a

00:03:02.550 --> 00:03:02.560
a fixed length vector by creating a
 

00:03:02.560 --> 00:03:05.099
a fixed length vector by creating a
larger vector and then allocating space

00:03:05.099 --> 00:03:05.109
larger vector and then allocating space
 

00:03:05.109 --> 00:03:06.750
larger vector and then allocating space
in it for the first word and for the

00:03:06.750 --> 00:03:06.760
in it for the first word and for the
 

00:03:06.760 --> 00:03:09.479
in it for the first word and for the
second word we have a fixed length

00:03:09.479 --> 00:03:09.489
second word we have a fixed length
 

00:03:09.489 --> 00:03:10.710
second word we have a fixed length
vector now no matter what two words

00:03:10.710 --> 00:03:10.720
vector now no matter what two words
 

00:03:10.720 --> 00:03:13.229
vector now no matter what two words
we're using and we can feed this into a

00:03:13.229 --> 00:03:13.239
we're using and we can feed this into a
 

00:03:13.239 --> 00:03:14.789
we're using and we can feed this into a
machine learning model like a

00:03:14.789 --> 00:03:14.799
machine learning model like a
 

00:03:14.799 --> 00:03:15.960
machine learning model like a
feed-forward neural network or a

00:03:15.960 --> 00:03:15.970
feed-forward neural network or a
 

00:03:15.970 --> 00:03:17.819
feed-forward neural network or a
logistic regression or any other model

00:03:17.819 --> 00:03:17.829
logistic regression or any other model
 

00:03:17.829 --> 00:03:21.360
logistic regression or any other model
and try to make a prediction one thing

00:03:21.360 --> 00:03:21.370
and try to make a prediction one thing
 

00:03:21.370 --> 00:03:23.670
and try to make a prediction one thing
you might be noticing here is that by

00:03:23.670 --> 00:03:23.680
you might be noticing here is that by
 

00:03:23.680 --> 00:03:25.710
you might be noticing here is that by
using this fixed window we're giving

00:03:25.710 --> 00:03:25.720
using this fixed window we're giving
 

00:03:25.720 --> 00:03:28.140
using this fixed window we're giving
ourselves a very limited history we're

00:03:28.140 --> 00:03:28.150
ourselves a very limited history we're
 

00:03:28.150 --> 00:03:30.119
ourselves a very limited history we're
trying to predict the word walk having

00:03:30.119 --> 00:03:30.129
trying to predict the word walk having
 

00:03:30.129 --> 00:03:32.759
trying to predict the word walk having
only seen the words four and a this is

00:03:32.759 --> 00:03:32.769
only seen the words four and a this is
 

00:03:32.769 --> 00:03:35.879
only seen the words four and a this is
almost impossible but differently it's

00:03:35.879 --> 00:03:35.889
almost impossible but differently it's
 

00:03:35.889 --> 00:03:37.559
almost impossible but differently it's
really hard to model long term

00:03:37.559 --> 00:03:37.569
really hard to model long term
 

00:03:37.569 --> 00:03:39.509
really hard to model long term
dependencies to see this clearly

00:03:39.509 --> 00:03:39.519
dependencies to see this clearly
 

00:03:39.519 --> 00:03:41.970
dependencies to see this clearly
consider the word in sorry consider the

00:03:41.970 --> 00:03:41.980
consider the word in sorry consider the
 

00:03:41.980 --> 00:03:43.920
consider the word in sorry consider the
sentence in France I had a great time

00:03:43.920 --> 00:03:43.930
sentence in France I had a great time
 

00:03:43.930 --> 00:03:45.689
sentence in France I had a great time
and I learned some of the blank language

00:03:45.689 --> 00:03:45.699
and I learned some of the blank language
 

00:03:45.699 --> 00:03:46.860
and I learned some of the blank language
where we're trying to predict the word

00:03:46.860 --> 00:03:46.870
where we're trying to predict the word
 

00:03:46.870 --> 00:03:48.990
where we're trying to predict the word
in the blank I knew it was French but

00:03:48.990 --> 00:03:49.000
in the blank I knew it was French but
 

00:03:49.000 --> 00:03:50.490
in the blank I knew it was French but
that's because I looked very far back at

00:03:50.490 --> 00:03:50.500
that's because I looked very far back at
 

00:03:50.500 --> 00:03:51.659
that's because I looked very far back at
the word France that appeared in the

00:03:51.659 --> 00:03:51.669
the word France that appeared in the
 

00:03:51.669 --> 00:03:53.309
the word France that appeared in the
beginning of the sentence if we were

00:03:53.309 --> 00:03:53.319
beginning of the sentence if we were
 

00:03:53.319 --> 00:03:55.140
beginning of the sentence if we were
only looking at the past two words or

00:03:55.140 --> 00:03:55.150
only looking at the past two words or
 

00:03:55.150 --> 00:03:56.400
only looking at the past two words or
the past three words or even the past

00:03:56.400 --> 00:03:56.410
the past three words or even the past
 

00:03:56.410 --> 00:03:57.960
the past three words or even the past
five words it would be really hard to

00:03:57.960 --> 00:03:57.970
five words it would be really hard to
 

00:03:57.970 --> 00:04:01.649
five words it would be really hard to
guess the word in that blank so we don't

00:04:01.649 --> 00:04:01.659
guess the word in that blank so we don't
 

00:04:01.659 --> 00:04:04.559
guess the word in that blank so we don't
want to limit ourselves so much we want

00:04:04.559 --> 00:04:04.569
want to limit ourselves so much we want
 

00:04:04.569 --> 00:04:06.809
want to limit ourselves so much we want
to ideally use all of the information

00:04:06.809 --> 00:04:06.819
to ideally use all of the information
 

00:04:06.819 --> 00:04:09.390
to ideally use all of the information
that we have in the sequence but we also

00:04:09.390 --> 00:04:09.400
that we have in the sequence but we also
 

00:04:09.400 --> 00:04:12.210
that we have in the sequence but we also
need a fixed length vector so one way we

00:04:12.210 --> 00:04:12.220
need a fixed length vector so one way we
 

00:04:12.220 --> 00:04:14.009
need a fixed length vector so one way we
could do this is by using the entire

00:04:14.009 --> 00:04:14.019
could do this is by using the entire
 

00:04:14.019 --> 00:04:16.170
could do this is by using the entire
sequence but representing it as a set of

00:04:16.170 --> 00:04:16.180
sequence but representing it as a set of
 

00:04:16.180 --> 00:04:18.479
sequence but representing it as a set of
counts in language this representation

00:04:18.479 --> 00:04:18.489
counts in language this representation
 

00:04:18.489 --> 00:04:21.240
counts in language this representation
is also known as a bag of words all this

00:04:21.240 --> 00:04:21.250
is also known as a bag of words all this
 

00:04:21.250 --> 00:04:23.520
is also known as a bag of words all this
is is a vector in which each slot

00:04:23.520 --> 00:04:23.530
is is a vector in which each slot
 

00:04:23.530 --> 00:04:25.920
is is a vector in which each slot
represents a word and the number in that

00:04:25.920 --> 00:04:25.930
represents a word and the number in that
 

00:04:25.930 --> 00:04:28.320
represents a word and the number in that
slot represents the number of times that

00:04:28.320 --> 00:04:28.330
slot represents the number of times that
 

00:04:28.330 --> 00:04:29.760
slot represents the number of times that
that word occurs in the sentence

00:04:29.760 --> 00:04:29.770
that word occurs in the sentence
 

00:04:29.770 --> 00:04:31.830
that word occurs in the sentence
so here the second slot represents the

00:04:31.830 --> 00:04:31.840
so here the second slot represents the
 

00:04:31.840 --> 00:04:33.809
so here the second slot represents the
word this and there's a 1 because this

00:04:33.809 --> 00:04:33.819
word this and there's a 1 because this
 

00:04:33.819 --> 00:04:36.450
word this and there's a 1 because this
appears once in the sentence now we have

00:04:36.450 --> 00:04:36.460
appears once in the sentence now we have
 

00:04:36.460 --> 00:04:38.040
appears once in the sentence now we have
a fixed length vector no matter how many

00:04:38.040 --> 00:04:38.050
a fixed length vector no matter how many
 

00:04:38.050 --> 00:04:40.260
a fixed length vector no matter how many
words we have the vector will always be

00:04:40.260 --> 00:04:40.270
words we have the vector will always be
 

00:04:40.270 --> 00:04:41.939
words we have the vector will always be
the same size the counts will just be

00:04:41.939 --> 00:04:41.949
the same size the counts will just be
 

00:04:41.949 --> 00:04:44.249
the same size the counts will just be
different we can feed this into a

00:04:44.249 --> 00:04:44.259
different we can feed this into a
 

00:04:44.259 --> 00:04:45.749
different we can feed this into a
machine learning model and try to make a

00:04:45.749 --> 00:04:45.759
machine learning model and try to make a
 

00:04:45.759 --> 00:04:48.029
machine learning model and try to make a
prediction the problem you may be

00:04:48.029 --> 00:04:48.039
prediction the problem you may be
 

00:04:48.039 --> 00:04:50.070
prediction the problem you may be
noticing here is that we're losing all

00:04:50.070 --> 00:04:50.080
noticing here is that we're losing all
 

00:04:50.080 --> 00:04:52.680
noticing here is that we're losing all
of the sequential information these

00:04:52.680 --> 00:04:52.690
of the sequential information these
 

00:04:52.690 --> 00:04:54.570
of the sequential information these
counts don't preserve any order that we

00:04:54.570 --> 00:04:54.580
counts don't preserve any order that we
 

00:04:54.580 --> 00:04:57.059
counts don't preserve any order that we
had in the sequence to see why this is

00:04:57.059 --> 00:04:57.069
had in the sequence to see why this is
 

00:04:57.069 --> 00:04:58.350
had in the sequence to see why this is
really bad considered these two

00:04:58.350 --> 00:04:58.360
really bad considered these two
 

00:04:58.360 --> 00:05:00.900
really bad considered these two
sentences the food was good not bad at

00:05:00.900 --> 00:05:00.910
sentences the food was good not bad at
 

00:05:00.910 --> 00:05:01.260
sentences the food was good not bad at
all

00:05:01.260 --> 00:05:01.270
all
 

00:05:01.270 --> 00:05:04.080
all
versus the food was bad not good at all

00:05:04.080 --> 00:05:04.090
versus the food was bad not good at all
 

00:05:04.090 --> 00:05:06.570
versus the food was bad not good at all
these are completely opposite sentences

00:05:06.570 --> 00:05:06.580
these are completely opposite sentences
 

00:05:06.580 --> 00:05:08.040
these are completely opposite sentences
but their bag of words representation

00:05:08.040 --> 00:05:08.050
but their bag of words representation
 

00:05:08.050 --> 00:05:10.890
but their bag of words representation
would be exactly the same because they

00:05:10.890 --> 00:05:10.900
would be exactly the same because they
 

00:05:10.900 --> 00:05:14.460
would be exactly the same because they
contain the same set of words so by

00:05:14.460 --> 00:05:14.470
contain the same set of words so by
 

00:05:14.470 --> 00:05:16.860
contain the same set of words so by
representing our sentences counts we're

00:05:16.860 --> 00:05:16.870
representing our sentences counts we're
 

00:05:16.870 --> 00:05:18.450
representing our sentences counts we're
losing all of the sequential information

00:05:18.450 --> 00:05:18.460
losing all of the sequential information
 

00:05:18.460 --> 00:05:19.950
losing all of the sequential information
which is really important because we're

00:05:19.950 --> 00:05:19.960
which is really important because we're
 

00:05:19.960 --> 00:05:23.129
which is really important because we're
trying to model sequences ok so what do

00:05:23.129 --> 00:05:23.139
trying to model sequences ok so what do
 

00:05:23.139 --> 00:05:25.860
trying to model sequences ok so what do
we know now we want to preserve order in

00:05:25.860 --> 00:05:25.870
we know now we want to preserve order in
 

00:05:25.870 --> 00:05:27.689
we know now we want to preserve order in
the sequence but we also don't want to

00:05:27.689 --> 00:05:27.699
the sequence but we also don't want to
 

00:05:27.699 --> 00:05:31.469
the sequence but we also don't want to
cut it off to to a very short length you

00:05:31.469 --> 00:05:31.479
cut it off to to a very short length you
 

00:05:31.479 --> 00:05:33.330
cut it off to to a very short length you
might be saying well why don't we just

00:05:33.330 --> 00:05:33.340
might be saying well why don't we just
 

00:05:33.340 --> 00:05:35.279
might be saying well why don't we just
use a really big fixed window before we

00:05:35.279 --> 00:05:35.289
use a really big fixed window before we
 

00:05:35.289 --> 00:05:36.719
use a really big fixed window before we
were having issues because we were just

00:05:36.719 --> 00:05:36.729
were having issues because we were just
 

00:05:36.729 --> 00:05:39.270
were having issues because we were just
using a fixed window of size 2 what if

00:05:39.270 --> 00:05:39.280
using a fixed window of size 2 what if
 

00:05:39.280 --> 00:05:40.920
using a fixed window of size 2 what if
we extended that to be a fixed window of

00:05:40.920 --> 00:05:40.930
we extended that to be a fixed window of
 

00:05:40.930 --> 00:05:43.620
we extended that to be a fixed window of
size 7 and we think that by looking at 7

00:05:43.620 --> 00:05:43.630
size 7 and we think that by looking at 7
 

00:05:43.630 --> 00:05:45.240
size 7 and we think that by looking at 7
words we can get most of the context

00:05:45.240 --> 00:05:45.250
words we can get most of the context
 

00:05:45.250 --> 00:05:47.670
words we can get most of the context
that we need well yeah ok we can do that

00:05:47.670 --> 00:05:47.680
that we need well yeah ok we can do that
 

00:05:47.680 --> 00:05:50.070
that we need well yeah ok we can do that
now we have another fixed length vector

00:05:50.070 --> 00:05:50.080
now we have another fixed length vector
 

00:05:50.080 --> 00:05:51.570
now we have another fixed length vector
just like before it's bigger but it's

00:05:51.570 --> 00:05:51.580
just like before it's bigger but it's
 

00:05:51.580 --> 00:05:52.950
just like before it's bigger but it's
still fixed length we have allocated

00:05:52.950 --> 00:05:52.960
still fixed length we have allocated
 

00:05:52.960 --> 00:05:55.110
still fixed length we have allocated
space for each of the 7 words we can

00:05:55.110 --> 00:05:55.120
space for each of the 7 words we can
 

00:05:55.120 --> 00:05:56.550
space for each of the 7 words we can
feed this into a model and try to make a

00:05:56.550 --> 00:05:56.560
feed this into a model and try to make a
 

00:05:56.560 --> 00:06:00.659
feed this into a model and try to make a
prediction the problem here is that and

00:06:00.659 --> 00:06:00.669
prediction the problem here is that and
 

00:06:00.669 --> 00:06:02.070
prediction the problem here is that and
consider this in the scenario where

00:06:02.070 --> 00:06:02.080
consider this in the scenario where
 

00:06:02.080 --> 00:06:04.020
consider this in the scenario where
we're feeding this input vector into a

00:06:04.020 --> 00:06:04.030
we're feeding this input vector into a
 

00:06:04.030 --> 00:06:06.420
we're feeding this input vector into a
feed-forward neural network each of

00:06:06.420 --> 00:06:06.430
feed-forward neural network each of
 

00:06:06.430 --> 00:06:08.100
feed-forward neural network each of
those inputs each of those ones and

00:06:08.100 --> 00:06:08.110
those inputs each of those ones and
 

00:06:08.110 --> 00:06:10.559
those inputs each of those ones and
zeros has a separate weight connecting

00:06:10.559 --> 00:06:10.569
zeros has a separate weight connecting
 

00:06:10.569 --> 00:06:13.020
zeros has a separate weight connecting
it to the network if we see the words

00:06:13.020 --> 00:06:13.030
it to the network if we see the words
 

00:06:13.030 --> 00:06:14.939
it to the network if we see the words
this morning at the beginning of the

00:06:14.939 --> 00:06:14.949
this morning at the beginning of the
 

00:06:14.949 --> 00:06:17.820
this morning at the beginning of the
sentence very very commonly the network

00:06:17.820 --> 00:06:17.830
sentence very very commonly the network
 

00:06:17.830 --> 00:06:19.770
sentence very very commonly the network
will learn that this morning represents

00:06:19.770 --> 00:06:19.780
will learn that this morning represents
 

00:06:19.780 --> 00:06:22.860
will learn that this morning represents
a time or a setting if this morning then

00:06:22.860 --> 00:06:22.870
a time or a setting if this morning then
 

00:06:22.870 --> 00:06:25.080
a time or a setting if this morning then
appears at the end of the sentence we'll

00:06:25.080 --> 00:06:25.090
appears at the end of the sentence we'll
 

00:06:25.090 --> 00:06:26.879
appears at the end of the sentence we'll
have a lot of trouble recognizing that

00:06:26.879 --> 00:06:26.889
have a lot of trouble recognizing that
 

00:06:26.889 --> 00:06:28.950
have a lot of trouble recognizing that
because the weights at the end of the

00:06:28.950 --> 00:06:28.960
because the weights at the end of the
 

00:06:28.960 --> 00:06:31.770
because the weights at the end of the
vector never saw that phrase before and

00:06:31.770 --> 00:06:31.780
vector never saw that phrase before and
 

00:06:31.780 --> 00:06:33.839
vector never saw that phrase before and
the weights from the beginning of the

00:06:33.839 --> 00:06:33.849
the weights from the beginning of the
 

00:06:33.849 --> 00:06:36.649
the weights from the beginning of the
vector and not being shared with the end

00:06:36.649 --> 00:06:36.659
vector and not being shared with the end
 

00:06:36.659 --> 00:06:39.390
vector and not being shared with the end
in other words things we learn about the

00:06:39.390 --> 00:06:39.400
in other words things we learn about the
 

00:06:39.400 --> 00:06:41.189
in other words things we learn about the
sequence won't transfer if they appear

00:06:41.189 --> 00:06:41.199
sequence won't transfer if they appear
 

00:06:41.199 --> 00:06:43.200
sequence won't transfer if they appear
at different points in the sequence

00:06:43.200 --> 00:06:43.210
at different points in the sequence
 

00:06:43.210 --> 00:06:47.670
at different points in the sequence
were not sharing any parameters all

00:06:47.670 --> 00:06:47.680
were not sharing any parameters all
 

00:06:47.680 --> 00:06:49.350
were not sharing any parameters all
right so you kind of see all the

00:06:49.350 --> 00:06:49.360
right so you kind of see all the
 

00:06:49.360 --> 00:06:51.749
right so you kind of see all the
problems that arise with sequences now

00:06:51.749 --> 00:06:51.759
problems that arise with sequences now
 

00:06:51.759 --> 00:06:52.920
problems that arise with sequences now
and why we need a different framework

00:06:52.920 --> 00:06:52.930
and why we need a different framework
 

00:06:52.930 --> 00:06:55.110
and why we need a different framework
specifically we want to be able to deal

00:06:55.110 --> 00:06:55.120
specifically we want to be able to deal
 

00:06:55.120 --> 00:06:58.110
specifically we want to be able to deal
with variable length sequences we want

00:06:58.110 --> 00:06:58.120
with variable length sequences we want
 

00:06:58.120 --> 00:06:59.820
with variable length sequences we want
to maintain sequence order so we can

00:06:59.820 --> 00:06:59.830
to maintain sequence order so we can
 

00:06:59.830 --> 00:07:02.400
to maintain sequence order so we can
keep all about sequential information we

00:07:02.400 --> 00:07:02.410
keep all about sequential information we
 

00:07:02.410 --> 00:07:03.900
keep all about sequential information we
want to keep track of longer term

00:07:03.900 --> 00:07:03.910
want to keep track of longer term
 

00:07:03.910 --> 00:07:05.969
want to keep track of longer term
dependencies rather than cutting it off

00:07:05.969 --> 00:07:05.979
dependencies rather than cutting it off
 

00:07:05.979 --> 00:07:08.370
dependencies rather than cutting it off
too short and we want to be able to

00:07:08.370 --> 00:07:08.380
too short and we want to be able to
 

00:07:08.380 --> 00:07:10.469
too short and we want to be able to
share parameters across the sequence so

00:07:10.469 --> 00:07:10.479
share parameters across the sequence so
 

00:07:10.479 --> 00:07:12.210
share parameters across the sequence so
we don't have to relearn things across

00:07:12.210 --> 00:07:12.220
we don't have to relearn things across
 

00:07:12.220 --> 00:07:14.670
we don't have to relearn things across
the sequence because this is a class

00:07:14.670 --> 00:07:14.680
the sequence because this is a class
 

00:07:14.680 --> 00:07:15.900
the sequence because this is a class
about deep learning I'm gonna talk about

00:07:15.900 --> 00:07:15.910
about deep learning I'm gonna talk about
 

00:07:15.910 --> 00:07:17.909
about deep learning I'm gonna talk about
how to address these problems with

00:07:17.909 --> 00:07:17.919
how to address these problems with
 

00:07:17.919 --> 00:07:20.279
how to address these problems with
neural networks but know that time

00:07:20.279 --> 00:07:20.289
neural networks but know that time
 

00:07:20.289 --> 00:07:22.710
neural networks but know that time
series modeling and sequential modeling

00:07:22.710 --> 00:07:22.720
series modeling and sequential modeling
 

00:07:22.720 --> 00:07:24.600
series modeling and sequential modeling
is a very active field in machine

00:07:24.600 --> 00:07:24.610
is a very active field in machine
 

00:07:24.610 --> 00:07:26.070
is a very active field in machine
learning and it has been and there are

00:07:26.070 --> 00:07:26.080
learning and it has been and there are
 

00:07:26.080 --> 00:07:27.749
learning and it has been and there are
lots of other machine learning methods

00:07:27.749 --> 00:07:27.759
lots of other machine learning methods
 

00:07:27.759 --> 00:07:28.950
lots of other machine learning methods
that have been developed to deal with

00:07:28.950 --> 00:07:28.960
that have been developed to deal with
 

00:07:28.960 --> 00:07:31.589
that have been developed to deal with
these problems but for now I'll talk

00:07:31.589 --> 00:07:31.599
these problems but for now I'll talk
 

00:07:31.599 --> 00:07:36.450
these problems but for now I'll talk
about recurrent neural networks okay so

00:07:36.450 --> 00:07:36.460
about recurrent neural networks okay so
 

00:07:36.460 --> 00:07:38.390
about recurrent neural networks okay so
a recurrent neural network is

00:07:38.390 --> 00:07:38.400
a recurrent neural network is
 

00:07:38.400 --> 00:07:41.550
a recurrent neural network is
architected in the same way as a normal

00:07:41.550 --> 00:07:41.560
architected in the same way as a normal
 

00:07:41.560 --> 00:07:43.620
architected in the same way as a normal
neural network we have some inputs we

00:07:43.620 --> 00:07:43.630
neural network we have some inputs we
 

00:07:43.630 --> 00:07:45.899
neural network we have some inputs we
have some hidden layers and we have some

00:07:45.899 --> 00:07:45.909
have some hidden layers and we have some
 

00:07:45.909 --> 00:07:48.510
have some hidden layers and we have some
outputs the only difference is that each

00:07:48.510 --> 00:07:48.520
outputs the only difference is that each
 

00:07:48.520 --> 00:07:50.850
outputs the only difference is that each
hidden unit is doing a slightly

00:07:50.850 --> 00:07:50.860
hidden unit is doing a slightly
 

00:07:50.860 --> 00:07:52.589
hidden unit is doing a slightly
different function so let's take a look

00:07:52.589 --> 00:07:52.599
different function so let's take a look
 

00:07:52.599 --> 00:07:54.540
different function so let's take a look
at this one hidden unit to see exactly

00:07:54.540 --> 00:07:54.550
at this one hidden unit to see exactly
 

00:07:54.550 --> 00:07:59.490
at this one hidden unit to see exactly
what it's doing a recurrent hidden unit

00:07:59.490 --> 00:07:59.500
what it's doing a recurrent hidden unit
 

00:07:59.500 --> 00:08:03.089
what it's doing a recurrent hidden unit
computes a function of an input and its

00:08:03.089 --> 00:08:03.099
computes a function of an input and its
 

00:08:03.099 --> 00:08:06.209
computes a function of an input and its
own previous output its own previous

00:08:06.209 --> 00:08:06.219
own previous output its own previous
 

00:08:06.219 --> 00:08:08.430
own previous output its own previous
output is also known as the cell state

00:08:08.430 --> 00:08:08.440
output is also known as the cell state
 

00:08:08.440 --> 00:08:11.850
output is also known as the cell state
and in the diagram it's denoted by s the

00:08:11.850 --> 00:08:11.860
and in the diagram it's denoted by s the
 

00:08:11.860 --> 00:08:14.580
and in the diagram it's denoted by s the
subscript is the time step so at the

00:08:14.580 --> 00:08:14.590
subscript is the time step so at the
 

00:08:14.590 --> 00:08:17.550
subscript is the time step so at the
very first time stuff T equals zero the

00:08:17.550 --> 00:08:17.560
very first time stuff T equals zero the
 

00:08:17.560 --> 00:08:19.469
very first time stuff T equals zero the
recurrent unit computes a function of

00:08:19.469 --> 00:08:19.479
recurrent unit computes a function of
 

00:08:19.479 --> 00:08:22.920
recurrent unit computes a function of
the input at T equals zero and of its

00:08:22.920 --> 00:08:22.930
the input at T equals zero and of its
 

00:08:22.930 --> 00:08:27.029
the input at T equals zero and of its
initial state similarly at the next time

00:08:27.029 --> 00:08:27.039
initial state similarly at the next time
 

00:08:27.039 --> 00:08:29.490
initial state similarly at the next time
step it computes a function of the new

00:08:29.490 --> 00:08:29.500
step it computes a function of the new
 

00:08:29.500 --> 00:08:33.510
step it computes a function of the new
input and its previous cell state if you

00:08:33.510 --> 00:08:33.520
input and its previous cell state if you
 

00:08:33.520 --> 00:08:35.069
input and its previous cell state if you
look at the function at the bottom the

00:08:35.069 --> 00:08:35.079
look at the function at the bottom the
 

00:08:35.079 --> 00:08:36.779
look at the function at the bottom the
function to compute s 2 you'll see it's

00:08:36.779 --> 00:08:36.789
function to compute s 2 you'll see it's
 

00:08:36.789 --> 00:08:38.610
function to compute s 2 you'll see it's
really similar to the function for

00:08:38.610 --> 00:08:38.620
really similar to the function for
 

00:08:38.620 --> 00:08:41.449
really similar to the function for
consider unit in a feed-forward Network

00:08:41.449 --> 00:08:41.459
consider unit in a feed-forward Network
 

00:08:41.459 --> 00:08:44.250
consider unit in a feed-forward Network
computes the only difference is that

00:08:44.250 --> 00:08:44.260
computes the only difference is that
 

00:08:44.260 --> 00:08:46.019
computes the only difference is that
we're adding in an additional term to

00:08:46.019 --> 00:08:46.029
we're adding in an additional term to
 

00:08:46.029 --> 00:08:49.740
we're adding in an additional term to
incorporate its own previous state

00:08:49.740 --> 00:08:49.750
incorporate its own previous state
 

00:08:49.750 --> 00:08:52.710
incorporate its own previous state
a common way of viewing recurrent neural

00:08:52.710 --> 00:08:52.720
a common way of viewing recurrent neural
 

00:08:52.720 --> 00:08:55.140
a common way of viewing recurrent neural
networks is by unfolding them across

00:08:55.140 --> 00:08:55.150
networks is by unfolding them across
 

00:08:55.150 --> 00:08:57.780
networks is by unfolding them across
time so this is the same hidden unit at

00:08:57.780 --> 00:08:57.790
time so this is the same hidden unit at
 

00:08:57.790 --> 00:09:00.300
time so this is the same hidden unit at
different points in time here you can

00:09:00.300 --> 00:09:00.310
different points in time here you can
 

00:09:00.310 --> 00:09:02.340
different points in time here you can
see that at every point in time it takes

00:09:02.340 --> 00:09:02.350
see that at every point in time it takes
 

00:09:02.350 --> 00:09:05.700
see that at every point in time it takes
as input its own previous state and the

00:09:05.700 --> 00:09:05.710
as input its own previous state and the
 

00:09:05.710 --> 00:09:09.810
as input its own previous state and the
new input at that time step one thing to

00:09:09.810 --> 00:09:09.820
new input at that time step one thing to
 

00:09:09.820 --> 00:09:11.760
new input at that time step one thing to
notice here is that throughout the

00:09:11.760 --> 00:09:11.770
notice here is that throughout the
 

00:09:11.770 --> 00:09:13.410
notice here is that throughout the
sequence we're using the same weight

00:09:13.410 --> 00:09:13.420
sequence we're using the same weight
 

00:09:13.420 --> 00:09:17.100
sequence we're using the same weight
matrices W and u this solves our problem

00:09:17.100 --> 00:09:17.110
matrices W and u this solves our problem
 

00:09:17.110 --> 00:09:19.410
matrices W and u this solves our problem
of parameter sharing we don't have new

00:09:19.410 --> 00:09:19.420
of parameter sharing we don't have new
 

00:09:19.420 --> 00:09:20.640
of parameter sharing we don't have new
parameters for every point of the

00:09:20.640 --> 00:09:20.650
parameters for every point of the
 

00:09:20.650 --> 00:09:22.350
parameters for every point of the
sequence once we learn something it can

00:09:22.350 --> 00:09:22.360
sequence once we learn something it can
 

00:09:22.360 --> 00:09:25.200
sequence once we learn something it can
apply at any point in the sequence this

00:09:25.200 --> 00:09:25.210
apply at any point in the sequence this
 

00:09:25.210 --> 00:09:26.550
apply at any point in the sequence this
also helps us deal with variable length

00:09:26.550 --> 00:09:26.560
also helps us deal with variable length
 

00:09:26.560 --> 00:09:28.440
also helps us deal with variable length
sequences because we're not pre

00:09:28.440 --> 00:09:28.450
sequences because we're not pre
 

00:09:28.450 --> 00:09:30.780
sequences because we're not pre
specifying the length of the sequence we

00:09:30.780 --> 00:09:30.790
specifying the length of the sequence we
 

00:09:30.790 --> 00:09:32.460
specifying the length of the sequence we
don't have separate parameters for every

00:09:32.460 --> 00:09:32.470
don't have separate parameters for every
 

00:09:32.470 --> 00:09:33.780
don't have separate parameters for every
point in the sequence so in some cases

00:09:33.780 --> 00:09:33.790
point in the sequence so in some cases
 

00:09:33.790 --> 00:09:36.540
point in the sequence so in some cases
we could unroll this RNN to four time

00:09:36.540 --> 00:09:36.550
we could unroll this RNN to four time
 

00:09:36.550 --> 00:09:39.120
we could unroll this RNN to four time
steps in other cases we can unroll it to

00:09:39.120 --> 00:09:39.130
steps in other cases we can unroll it to
 

00:09:39.130 --> 00:09:43.200
steps in other cases we can unroll it to
ten time steps a final thing to notice

00:09:43.200 --> 00:09:43.210
ten time steps a final thing to notice
 

00:09:43.210 --> 00:09:45.750
ten time steps a final thing to notice
is that S sub n the self state at time n

00:09:45.750 --> 00:09:45.760
is that S sub n the self state at time n
 

00:09:45.760 --> 00:09:48.330
is that S sub n the self state at time n
can contain information from all of the

00:09:48.330 --> 00:09:48.340
can contain information from all of the
 

00:09:48.340 --> 00:09:51.240
can contain information from all of the
past time steps notice that each cell

00:09:51.240 --> 00:09:51.250
past time steps notice that each cell
 

00:09:51.250 --> 00:09:53.400
past time steps notice that each cell
state is a function of the previous self

00:09:53.400 --> 00:09:53.410
state is a function of the previous self
 

00:09:53.410 --> 00:09:54.960
state is a function of the previous self
state which is the function which is a

00:09:54.960 --> 00:09:54.970
state which is the function which is a
 

00:09:54.970 --> 00:09:56.520
state which is the function which is a
function of the previous cell state and

00:09:56.520 --> 00:09:56.530
function of the previous cell state and
 

00:09:56.530 --> 00:09:58.950
function of the previous cell state and
so on so this kind of solves our issue

00:09:58.950 --> 00:09:58.960
so on so this kind of solves our issue
 

00:09:58.960 --> 00:10:02.010
so on so this kind of solves our issue
of long-term dependencies because add a

00:10:02.010 --> 00:10:02.020
of long-term dependencies because add a
 

00:10:02.020 --> 00:10:05.610
of long-term dependencies because add a
time step very far in the future that

00:10:05.610 --> 00:10:05.620
time step very far in the future that
 

00:10:05.620 --> 00:10:08.340
time step very far in the future that
self state encompasses information about

00:10:08.340 --> 00:10:08.350
self state encompasses information about
 

00:10:08.350 --> 00:10:13.500
self state encompasses information about
all of the previous cell states all

00:10:13.500 --> 00:10:13.510
all of the previous cell states all
 

00:10:13.510 --> 00:10:16.200
all of the previous cell states all
right so now that you kind of understand

00:10:16.200 --> 00:10:16.210
right so now that you kind of understand
 

00:10:16.210 --> 00:10:18.210
right so now that you kind of understand
what a recurrent neural network is and

00:10:18.210 --> 00:10:18.220
what a recurrent neural network is and
 

00:10:18.220 --> 00:10:20.070
what a recurrent neural network is and
just to clarify I've shown you one

00:10:20.070 --> 00:10:20.080
just to clarify I've shown you one
 

00:10:20.080 --> 00:10:22.020
just to clarify I've shown you one
hidden unit in the previous slide but in

00:10:22.020 --> 00:10:22.030
hidden unit in the previous slide but in
 

00:10:22.030 --> 00:10:24.000
hidden unit in the previous slide but in
a full network you would have many many

00:10:24.000 --> 00:10:24.010
a full network you would have many many
 

00:10:24.010 --> 00:10:25.590
a full network you would have many many
of those hidden units and even many

00:10:25.590 --> 00:10:25.600
of those hidden units and even many
 

00:10:25.600 --> 00:10:28.950
of those hidden units and even many
layers of many hidden units so now we

00:10:28.950 --> 00:10:28.960
layers of many hidden units so now we
 

00:10:28.960 --> 00:10:30.630
layers of many hidden units so now we
can talk about how you would train a

00:10:30.630 --> 00:10:30.640
can talk about how you would train a
 

00:10:30.640 --> 00:10:33.570
can talk about how you would train a
recurrent neural network it's really

00:10:33.570 --> 00:10:33.580
recurrent neural network it's really
 

00:10:33.580 --> 00:10:35.640
recurrent neural network it's really
similar to how you train a normal neural

00:10:35.640 --> 00:10:35.650
similar to how you train a normal neural
 

00:10:35.650 --> 00:10:37.530
similar to how you train a normal neural
network it's back propagation there's

00:10:37.530 --> 00:10:37.540
network it's back propagation there's
 

00:10:37.540 --> 00:10:41.280
network it's back propagation there's
just an additional time dimension as a

00:10:41.280 --> 00:10:41.290
just an additional time dimension as a
 

00:10:41.290 --> 00:10:43.890
just an additional time dimension as a
reminder in back propagation we want to

00:10:43.890 --> 00:10:43.900
reminder in back propagation we want to
 

00:10:43.900 --> 00:10:47.100
reminder in back propagation we want to
find the parameters that minimize some

00:10:47.100 --> 00:10:47.110
find the parameters that minimize some
 

00:10:47.110 --> 00:10:49.410
find the parameters that minimize some
loss function the way that we do this is

00:10:49.410 --> 00:10:49.420
loss function the way that we do this is
 

00:10:49.420 --> 00:10:51.420
loss function the way that we do this is
by first taking the derivative of the

00:10:51.420 --> 00:10:51.430
by first taking the derivative of the
 

00:10:51.430 --> 00:10:53.370
by first taking the derivative of the
loss with respect to each of the

00:10:53.370 --> 00:10:53.380
loss with respect to each of the
 

00:10:53.380 --> 00:10:55.620
loss with respect to each of the
parameters and then shifting the

00:10:55.620 --> 00:10:55.630
parameters and then shifting the
 

00:10:55.630 --> 00:10:57.690
parameters and then shifting the
parameters in the opposite direction in

00:10:57.690 --> 00:10:57.700
parameters in the opposite direction in
 

00:10:57.700 --> 00:11:01.110
parameters in the opposite direction in
order to try and minimize the loss this

00:11:01.110 --> 00:11:01.120
order to try and minimize the loss this
 

00:11:01.120 --> 00:11:04.630
order to try and minimize the loss this
process is called gradient descent

00:11:04.630 --> 00:11:04.640
 

00:11:04.640 --> 00:11:08.180
so one difference with RNN is that we

00:11:08.180 --> 00:11:08.190
so one difference with RNN is that we
 

00:11:08.190 --> 00:11:10.880
so one difference with RNN is that we
have many time steps so we can produce

00:11:10.880 --> 00:11:10.890
have many time steps so we can produce
 

00:11:10.890 --> 00:11:13.340
have many time steps so we can produce
an output at every time step because we

00:11:13.340 --> 00:11:13.350
an output at every time step because we
 

00:11:13.350 --> 00:11:14.780
an output at every time step because we
have an output at every time step we can

00:11:14.780 --> 00:11:14.790
have an output at every time step we can
 

00:11:14.790 --> 00:11:17.900
have an output at every time step we can
have a loss at every time step rather

00:11:17.900 --> 00:11:17.910
have a loss at every time step rather
 

00:11:17.910 --> 00:11:20.740
have a loss at every time step rather
than just one single loss at the end

00:11:20.740 --> 00:11:20.750
than just one single loss at the end
 

00:11:20.750 --> 00:11:22.880
than just one single loss at the end
because and the way that we deal with

00:11:22.880 --> 00:11:22.890
because and the way that we deal with
 

00:11:22.890 --> 00:11:25.070
because and the way that we deal with
this is pretty simple the total loss is

00:11:25.070 --> 00:11:25.080
this is pretty simple the total loss is
 

00:11:25.080 --> 00:11:26.870
this is pretty simple the total loss is
just the sum of the losses at every time

00:11:26.870 --> 00:11:26.880
just the sum of the losses at every time
 

00:11:26.880 --> 00:11:27.970
just the sum of the losses at every time
step

00:11:27.970 --> 00:11:27.980
step
 

00:11:27.980 --> 00:11:30.890
step
similarly the total gradient is just the

00:11:30.890 --> 00:11:30.900
similarly the total gradient is just the
 

00:11:30.900 --> 00:11:35.680
similarly the total gradient is just the
sum of the gradients at every time step

00:11:35.680 --> 00:11:35.690
 

00:11:35.690 --> 00:11:39.110
so we can try this out by walking

00:11:39.110 --> 00:11:39.120
so we can try this out by walking
 

00:11:39.120 --> 00:11:41.780
so we can try this out by walking
through this gradient computation for a

00:11:41.780 --> 00:11:41.790
through this gradient computation for a
 

00:11:41.790 --> 00:11:44.510
through this gradient computation for a
single parameter W W is the weight

00:11:44.510 --> 00:11:44.520
single parameter W W is the weight
 

00:11:44.520 --> 00:11:45.920
single parameter W W is the weight
matrix that were multiplying by our

00:11:45.920 --> 00:11:45.930
matrix that were multiplying by our
 

00:11:45.930 --> 00:11:49.610
matrix that were multiplying by our
inputs we know that the total loss the

00:11:49.610 --> 00:11:49.620
inputs we know that the total loss the
 

00:11:49.620 --> 00:11:51.890
inputs we know that the total loss the
the total gradient so the derivative of

00:11:51.890 --> 00:11:51.900
the total gradient so the derivative of
 

00:11:51.900 --> 00:11:54.170
the total gradient so the derivative of
the loss with respect to W will be the

00:11:54.170 --> 00:11:54.180
the loss with respect to W will be the
 

00:11:54.180 --> 00:11:56.350
the loss with respect to W will be the
sum of the gradients at every time step

00:11:56.350 --> 00:11:56.360
sum of the gradients at every time step
 

00:11:56.360 --> 00:11:58.940
sum of the gradients at every time step
so for now we can focus on a single time

00:11:58.940 --> 00:11:58.950
so for now we can focus on a single time
 

00:11:58.950 --> 00:12:01.160
so for now we can focus on a single time
step knowing that at the end we would do

00:12:01.160 --> 00:12:01.170
step knowing that at the end we would do
 

00:12:01.170 --> 00:12:02.570
step knowing that at the end we would do
this for each of the time steps and then

00:12:02.570 --> 00:12:02.580
this for each of the time steps and then
 

00:12:02.580 --> 00:12:06.260
this for each of the time steps and then
sum them up to get the total gradient so

00:12:06.260 --> 00:12:06.270
sum them up to get the total gradient so
 

00:12:06.270 --> 00:12:08.720
sum them up to get the total gradient so
let's take time step two we can solve

00:12:08.720 --> 00:12:08.730
let's take time step two we can solve
 

00:12:08.730 --> 00:12:11.270
let's take time step two we can solve
this gradient using the chain rule so

00:12:11.270 --> 00:12:11.280
this gradient using the chain rule so
 

00:12:11.280 --> 00:12:12.710
this gradient using the chain rule so
the derivative of the loss with respect

00:12:12.710 --> 00:12:12.720
the derivative of the loss with respect
 

00:12:12.720 --> 00:12:16.010
the derivative of the loss with respect
to W is the derivative of the loss with

00:12:16.010 --> 00:12:16.020
to W is the derivative of the loss with
 

00:12:16.020 --> 00:12:18.470
to W is the derivative of the loss with
respect to the output the derivative of

00:12:18.470 --> 00:12:18.480
respect to the output the derivative of
 

00:12:18.480 --> 00:12:20.180
respect to the output the derivative of
the output with respect to the cell

00:12:20.180 --> 00:12:20.190
the output with respect to the cell
 

00:12:20.190 --> 00:12:22.430
the output with respect to the cell
state at time two and then the

00:12:22.430 --> 00:12:22.440
state at time two and then the
 

00:12:22.440 --> 00:12:23.750
state at time two and then the
derivative of the cell state with

00:12:23.750 --> 00:12:23.760
derivative of the cell state with
 

00:12:23.760 --> 00:12:27.340
derivative of the cell state with
respect to W so this seems fine but

00:12:27.340 --> 00:12:27.350
respect to W so this seems fine but
 

00:12:27.350 --> 00:12:29.780
respect to W so this seems fine but
let's take a closer look at this last

00:12:29.780 --> 00:12:29.790
let's take a closer look at this last
 

00:12:29.790 --> 00:12:34.970
let's take a closer look at this last
term you'll notice that s2 also depends

00:12:34.970 --> 00:12:34.980
term you'll notice that s2 also depends
 

00:12:34.980 --> 00:12:38.780
term you'll notice that s2 also depends
on s 1 and s 1 also depends on W so we

00:12:38.780 --> 00:12:38.790
on s 1 and s 1 also depends on W so we
 

00:12:38.790 --> 00:12:40.790
on s 1 and s 1 also depends on W so we
can't just leave that last term as a

00:12:40.790 --> 00:12:40.800
can't just leave that last term as a
 

00:12:40.800 --> 00:12:43.220
can't just leave that last term as a
constant we actually have to expand it

00:12:43.220 --> 00:12:43.230
constant we actually have to expand it
 

00:12:43.230 --> 00:12:46.130
constant we actually have to expand it
out farther ok so how do we expand this

00:12:46.130 --> 00:12:46.140
out farther ok so how do we expand this
 

00:12:46.140 --> 00:12:48.140
out farther ok so how do we expand this
out farther what we really want to know

00:12:48.140 --> 00:12:48.150
out farther what we really want to know
 

00:12:48.150 --> 00:12:51.200
out farther what we really want to know
is how exactly does the cell state at

00:12:51.200 --> 00:12:51.210
is how exactly does the cell state at
 

00:12:51.210 --> 00:12:55.940
is how exactly does the cell state at
time step 2 depend on W well it depends

00:12:55.940 --> 00:12:55.950
time step 2 depend on W well it depends
 

00:12:55.950 --> 00:12:57.860
time step 2 depend on W well it depends
directly on W because it feeds right in

00:12:57.860 --> 00:12:57.870
directly on W because it feeds right in
 

00:12:57.870 --> 00:13:00.770
directly on W because it feeds right in
we also saw that s 2 depends on s 1

00:13:00.770 --> 00:13:00.780
we also saw that s 2 depends on s 1
 

00:13:00.780 --> 00:13:04.100
we also saw that s 2 depends on s 1
which depends on W and you can also see

00:13:04.100 --> 00:13:04.110
which depends on W and you can also see
 

00:13:04.110 --> 00:13:06.260
which depends on W and you can also see
that s 2 depends on s 0 which also

00:13:06.260 --> 00:13:06.270
that s 2 depends on s 0 which also
 

00:13:06.270 --> 00:13:10.460
that s 2 depends on s 0 which also
depends on W in other words and here I'm

00:13:10.460 --> 00:13:10.470
depends on W in other words and here I'm
 

00:13:10.470 --> 00:13:12.440
depends on W in other words and here I'm
just writing it as a summation

00:13:12.440 --> 00:13:12.450
just writing it as a summation
 

00:13:12.450 --> 00:13:14.420
just writing it as a summation
those the the some that used on the

00:13:14.420 --> 00:13:14.430
those the the some that used on the
 

00:13:14.430 --> 00:13:18.319
those the the some that used on the
previous slide as a summation form and

00:13:18.319 --> 00:13:18.329
previous slide as a summation form and
 

00:13:18.329 --> 00:13:20.509
previous slide as a summation form and
you can see that the last two terms are

00:13:20.509 --> 00:13:20.519
you can see that the last two terms are
 

00:13:20.519 --> 00:13:23.120
you can see that the last two terms are
basically summing the contributions of W

00:13:23.120 --> 00:13:23.130
basically summing the contributions of W
 

00:13:23.130 --> 00:13:25.970
basically summing the contributions of W
in previous time steps to the error at

00:13:25.970 --> 00:13:25.980
in previous time steps to the error at
 

00:13:25.980 --> 00:13:30.170
in previous time steps to the error at
time step T this is key to how we model

00:13:30.170 --> 00:13:30.180
time step T this is key to how we model
 

00:13:30.180 --> 00:13:32.930
time step T this is key to how we model
longer term dependencies this gradient

00:13:32.930 --> 00:13:32.940
longer term dependencies this gradient
 

00:13:32.940 --> 00:13:34.579
longer term dependencies this gradient
is how we shift our parameters and our

00:13:34.579 --> 00:13:34.589
is how we shift our parameters and our
 

00:13:34.589 --> 00:13:37.040
is how we shift our parameters and our
parameters define our network by

00:13:37.040 --> 00:13:37.050
parameters define our network by
 

00:13:37.050 --> 00:13:39.800
parameters define our network by
shifting our parameters such that they

00:13:39.800 --> 00:13:39.810
shifting our parameters such that they
 

00:13:39.810 --> 00:13:42.560
shifting our parameters such that they
include contributions to the error from

00:13:42.560 --> 00:13:42.570
include contributions to the error from
 

00:13:42.570 --> 00:13:46.910
include contributions to the error from
past time steps they're shifted to model

00:13:46.910 --> 00:13:46.920
past time steps they're shifted to model
 

00:13:46.920 --> 00:13:52.009
past time steps they're shifted to model
longer term dependencies and here I'm

00:13:52.009 --> 00:13:52.019
longer term dependencies and here I'm
 

00:13:52.019 --> 00:13:54.199
longer term dependencies and here I'm
just writing it as a general sum not

00:13:54.199 --> 00:13:54.209
just writing it as a general sum not
 

00:13:54.209 --> 00:13:58.579
just writing it as a general sum not
just for time step two

00:13:58.579 --> 00:13:58.589
 

00:13:58.589 --> 00:14:01.009
okay so this is basically the process of

00:14:01.009 --> 00:14:01.019
okay so this is basically the process of
 

00:14:01.019 --> 00:14:03.230
okay so this is basically the process of
back propagation to through time you

00:14:03.230 --> 00:14:03.240
back propagation to through time you
 

00:14:03.240 --> 00:14:04.759
back propagation to through time you
would do this for every parameter in

00:14:04.759 --> 00:14:04.769
would do this for every parameter in
 

00:14:04.769 --> 00:14:07.129
would do this for every parameter in
your network and then use that in the

00:14:07.129 --> 00:14:07.139
your network and then use that in the
 

00:14:07.139 --> 00:14:11.090
your network and then use that in the
process of gradient descent in practice

00:14:11.090 --> 00:14:11.100
process of gradient descent in practice
 

00:14:11.100 --> 00:14:14.060
process of gradient descent in practice
ions are a bit difficult to train so I

00:14:14.060 --> 00:14:14.070
ions are a bit difficult to train so I
 

00:14:14.070 --> 00:14:16.639
ions are a bit difficult to train so I
kind of want to go through why that is

00:14:16.639 --> 00:14:16.649
kind of want to go through why that is
 

00:14:16.649 --> 00:14:20.660
kind of want to go through why that is
and what some ways some ways that we can

00:14:20.660 --> 00:14:20.670
and what some ways some ways that we can
 

00:14:20.670 --> 00:14:25.040
and what some ways some ways that we can
address these issues so let's go back to

00:14:25.040 --> 00:14:25.050
address these issues so let's go back to
 

00:14:25.050 --> 00:14:27.829
address these issues so let's go back to
this summation as a reminder this is the

00:14:27.829 --> 00:14:27.839
this summation as a reminder this is the
 

00:14:27.839 --> 00:14:29.900
this summation as a reminder this is the
derivative of the loss with respect to W

00:14:29.900 --> 00:14:29.910
derivative of the loss with respect to W
 

00:14:29.910 --> 00:14:32.480
derivative of the loss with respect to W
and this is what we would use to shift

00:14:32.480 --> 00:14:32.490
and this is what we would use to shift
 

00:14:32.490 --> 00:14:37.269
and this is what we would use to shift
our parameters W the last two terms are

00:14:37.269 --> 00:14:37.279
our parameters W the last two terms are
 

00:14:37.279 --> 00:14:42.350
our parameters W the last two terms are
considering the error of W at all of the

00:14:42.350 --> 00:14:42.360
considering the error of W at all of the
 

00:14:42.360 --> 00:14:45.170
considering the error of W at all of the
previous time steps let's take a look at

00:14:45.170 --> 00:14:45.180
previous time steps let's take a look at
 

00:14:45.180 --> 00:14:47.540
previous time steps let's take a look at
this one term this is how we this is the

00:14:47.540 --> 00:14:47.550
this one term this is how we this is the
 

00:14:47.550 --> 00:14:49.250
this one term this is how we this is the
derivative of the cell state at time

00:14:49.250 --> 00:14:49.260
derivative of the cell state at time
 

00:14:49.260 --> 00:14:50.810
derivative of the cell state at time
step two with respect to each of the

00:14:50.810 --> 00:14:50.820
step two with respect to each of the
 

00:14:50.820 --> 00:14:53.990
step two with respect to each of the
previous cell states you might notice

00:14:53.990 --> 00:14:54.000
previous cell states you might notice
 

00:14:54.000 --> 00:14:55.670
previous cell states you might notice
that this itself is also a chain rule

00:14:55.670 --> 00:14:55.680
that this itself is also a chain rule
 

00:14:55.680 --> 00:14:58.100
that this itself is also a chain rule
because s 2 depends on s 1 and s 1

00:14:58.100 --> 00:14:58.110
because s 2 depends on s 1 and s 1
 

00:14:58.110 --> 00:14:59.990
because s 2 depends on s 1 and s 1
depends on a zero we can expand this out

00:14:59.990 --> 00:15:00.000
depends on a zero we can expand this out
 

00:15:00.000 --> 00:15:03.380
depends on a zero we can expand this out
farther this is just for the derivative

00:15:03.380 --> 00:15:03.390
farther this is just for the derivative
 

00:15:03.390 --> 00:15:05.600
farther this is just for the derivative
of s 2 with respect to s 0 but what if

00:15:05.600 --> 00:15:05.610
of s 2 with respect to s 0 but what if
 

00:15:05.610 --> 00:15:07.490
of s 2 with respect to s 0 but what if
we were looking at a time step very far

00:15:07.490 --> 00:15:07.500
we were looking at a time step very far
 

00:15:07.500 --> 00:15:11.689
we were looking at a time step very far
in the future like time step n that term

00:15:11.689 --> 00:15:11.699
in the future like time step n that term
 

00:15:11.699 --> 00:15:15.040
in the future like time step n that term
would expand into a product of n terms

00:15:15.040 --> 00:15:15.050
would expand into a product of n terms
 

00:15:15.050 --> 00:15:18.920
would expand into a product of n terms
and ok you might be thinking so what

00:15:18.920 --> 00:15:18.930
and ok you might be thinking so what
 

00:15:18.930 --> 00:15:22.160
and ok you might be thinking so what
well as notice that as the gap between

00:15:22.160 --> 00:15:22.170
well as notice that as the gap between
 

00:15:22.170 --> 00:15:24.199
well as notice that as the gap between
time steps gets bigger and bigger this

00:15:24.199 --> 00:15:24.209
time steps gets bigger and bigger this
 

00:15:24.209 --> 00:15:26.090
time steps gets bigger and bigger this
product in the grade

00:15:26.090 --> 00:15:26.100
product in the grade
 

00:15:26.100 --> 00:15:29.360
product in the grade
gets longer and longer and if we look at

00:15:29.360 --> 00:15:29.370
gets longer and longer and if we look at
 

00:15:29.370 --> 00:15:31.520
gets longer and longer and if we look at
each of these terms what what are you -

00:15:31.520 --> 00:15:31.530
each of these terms what what are you -
 

00:15:31.530 --> 00:15:33.020
each of these terms what what are you -
these terms they all kind of take the

00:15:33.020 --> 00:15:33.030
these terms they all kind of take the
 

00:15:33.030 --> 00:15:35.240
these terms they all kind of take the
same form it's the derivative of a cell

00:15:35.240 --> 00:15:35.250
same form it's the derivative of a cell
 

00:15:35.250 --> 00:15:37.220
same form it's the derivative of a cell
state with respect to the previous cell

00:15:37.220 --> 00:15:37.230
state with respect to the previous cell
 

00:15:37.230 --> 00:15:43.010
state with respect to the previous cell
state that term can be written like this

00:15:43.010 --> 00:15:43.020
state that term can be written like this
 

00:15:43.020 --> 00:15:46.100
state that term can be written like this
and the actual form that actual formula

00:15:46.100 --> 00:15:46.110
and the actual form that actual formula
 

00:15:46.110 --> 00:15:47.930
and the actual form that actual formula
isn't that important just notice that

00:15:47.930 --> 00:15:47.940
isn't that important just notice that
 

00:15:47.940 --> 00:15:51.260
isn't that important just notice that
it's a product of two terms double use

00:15:51.260 --> 00:15:51.270
it's a product of two terms double use
 

00:15:51.270 --> 00:15:53.080
it's a product of two terms double use
and F Prime's

00:15:53.080 --> 00:15:53.090
and F Prime's
 

00:15:53.090 --> 00:15:57.620
and F Prime's
double use are our weight matrices these

00:15:57.620 --> 00:15:57.630
double use are our weight matrices these
 

00:15:57.630 --> 00:15:59.540
double use are our weight matrices these
are sampled mostly from a standard

00:15:59.540 --> 00:15:59.550
are sampled mostly from a standard
 

00:15:59.550 --> 00:16:01.850
are sampled mostly from a standard
normal distribution so most of the terms

00:16:01.850 --> 00:16:01.860
normal distribution so most of the terms
 

00:16:01.860 --> 00:16:05.000
normal distribution so most of the terms
will be less than one F prime is the

00:16:05.000 --> 00:16:05.010
will be less than one F prime is the
 

00:16:05.010 --> 00:16:07.010
will be less than one F prime is the
derivative of our activation function if

00:16:07.010 --> 00:16:07.020
derivative of our activation function if
 

00:16:07.020 --> 00:16:09.500
derivative of our activation function if
we use an activation function such as

00:16:09.500 --> 00:16:09.510
we use an activation function such as
 

00:16:09.510 --> 00:16:11.510
we use an activation function such as
the hyperbolic tangent or a sigmoid F

00:16:11.510 --> 00:16:11.520
the hyperbolic tangent or a sigmoid F
 

00:16:11.520 --> 00:16:16.340
the hyperbolic tangent or a sigmoid F
prime will always be less than one in

00:16:16.340 --> 00:16:16.350
prime will always be less than one in
 

00:16:16.350 --> 00:16:18.440
prime will always be less than one in
other words we're multiplying a lot of

00:16:18.440 --> 00:16:18.450
other words we're multiplying a lot of
 

00:16:18.450 --> 00:16:22.060
other words we're multiplying a lot of
small numbers together in this product

00:16:22.060 --> 00:16:22.070
small numbers together in this product
 

00:16:22.070 --> 00:16:24.950
small numbers together in this product
okay so what does this mean

00:16:24.950 --> 00:16:24.960
okay so what does this mean
 

00:16:24.960 --> 00:16:27.830
okay so what does this mean
basically recall that this product is

00:16:27.830 --> 00:16:27.840
basically recall that this product is
 

00:16:27.840 --> 00:16:30.950
basically recall that this product is
how we're adding the gradient from

00:16:30.950 --> 00:16:30.960
how we're adding the gradient from
 

00:16:30.960 --> 00:16:33.860
how we're adding the gradient from
future time steps to the gradient sorry

00:16:33.860 --> 00:16:33.870
future time steps to the gradient sorry
 

00:16:33.870 --> 00:16:35.150
future time steps to the gradient sorry
how we're adding the gradient from past

00:16:35.150 --> 00:16:35.160
how we're adding the gradient from past
 

00:16:35.160 --> 00:16:37.370
how we're adding the gradient from past
time steps to the gradient at a future

00:16:37.370 --> 00:16:37.380
time steps to the gradient at a future
 

00:16:37.380 --> 00:16:41.480
time steps to the gradient at a future
time step what's happening then is that

00:16:41.480 --> 00:16:41.490
time step what's happening then is that
 

00:16:41.490 --> 00:16:43.310
time step what's happening then is that
air is due to further and further back

00:16:43.310 --> 00:16:43.320
air is due to further and further back
 

00:16:43.320 --> 00:16:45.650
air is due to further and further back
time steps have increasingly smaller

00:16:45.650 --> 00:16:45.660
time steps have increasingly smaller
 

00:16:45.660 --> 00:16:48.110
time steps have increasingly smaller
gradients because that product for

00:16:48.110 --> 00:16:48.120
gradients because that product for
 

00:16:48.120 --> 00:16:50.480
gradients because that product for
further back time steps will be longer

00:16:50.480 --> 00:16:50.490
further back time steps will be longer
 

00:16:50.490 --> 00:16:52.580
further back time steps will be longer
and since the numbers are all decimals

00:16:52.580 --> 00:16:52.590
and since the numbers are all decimals
 

00:16:52.590 --> 00:16:54.260
and since the numbers are all decimals
they'll be it will be it will it'll

00:16:54.260 --> 00:16:54.270
they'll be it will be it will it'll
 

00:16:54.270 --> 00:16:59.180
they'll be it will be it will it'll
become increasingly smaller what this

00:16:59.180 --> 00:16:59.190
become increasingly smaller what this
 

00:16:59.190 --> 00:17:02.300
become increasingly smaller what this
what this ends up meaning at a high

00:17:02.300 --> 00:17:02.310
what this ends up meaning at a high
 

00:17:02.310 --> 00:17:04.220
what this ends up meaning at a high
level is that our parameters will become

00:17:04.220 --> 00:17:04.230
level is that our parameters will become
 

00:17:04.230 --> 00:17:05.840
level is that our parameters will become
biased to capture shorter term

00:17:05.840 --> 00:17:05.850
biased to capture shorter term
 

00:17:05.850 --> 00:17:08.689
biased to capture shorter term
dependencies the errors that arise from

00:17:08.689 --> 00:17:08.699
dependencies the errors that arise from
 

00:17:08.699 --> 00:17:10.370
dependencies the errors that arise from
further and further back time steps will

00:17:10.370 --> 00:17:10.380
further and further back time steps will
 

00:17:10.380 --> 00:17:12.710
further and further back time steps will
be harder and harder to propagate into

00:17:12.710 --> 00:17:12.720
be harder and harder to propagate into
 

00:17:12.720 --> 00:17:18.050
be harder and harder to propagate into
the gradient at future time steps recall

00:17:18.050 --> 00:17:18.060
the gradient at future time steps recall
 

00:17:18.060 --> 00:17:20.179
the gradient at future time steps recall
that recall this example that I showed

00:17:20.179 --> 00:17:20.189
that recall this example that I showed
 

00:17:20.189 --> 00:17:22.850
that recall this example that I showed
at the beginning the whole point of

00:17:22.850 --> 00:17:22.860
at the beginning the whole point of
 

00:17:22.860 --> 00:17:24.740
at the beginning the whole point of
using recurrent neural networks is

00:17:24.740 --> 00:17:24.750
using recurrent neural networks is
 

00:17:24.750 --> 00:17:26.120
using recurrent neural networks is
because we wanted to model long term

00:17:26.120 --> 00:17:26.130
because we wanted to model long term
 

00:17:26.130 --> 00:17:28.280
because we wanted to model long term
dependencies but if our parameters are

00:17:28.280 --> 00:17:28.290
dependencies but if our parameters are
 

00:17:28.290 --> 00:17:29.600
dependencies but if our parameters are
biased to capture short term

00:17:29.600 --> 00:17:29.610
biased to capture short term
 

00:17:29.610 --> 00:17:31.640
biased to capture short term
dependencies even if they see the whole

00:17:31.640 --> 00:17:31.650
dependencies even if they see the whole
 

00:17:31.650 --> 00:17:33.140
dependencies even if they see the whole
sequence they'll be but the parameters

00:17:33.140 --> 00:17:33.150
sequence they'll be but the parameters
 

00:17:33.150 --> 00:17:35.120
sequence they'll be but the parameters
will become biased to predict things

00:17:35.120 --> 00:17:35.130
will become biased to predict things
 

00:17:35.130 --> 00:17:40.210
will become biased to predict things
based mostly on the past couple words

00:17:40.210 --> 00:17:40.220
 

00:17:40.220 --> 00:17:44.570
okay so now I'm gonna go through some a

00:17:44.570 --> 00:17:44.580
okay so now I'm gonna go through some a
 

00:17:44.580 --> 00:17:47.960
okay so now I'm gonna go through some a
couple methods that are used to address

00:17:47.960 --> 00:17:47.970
couple methods that are used to address
 

00:17:47.970 --> 00:17:49.789
couple methods that are used to address
this issue in practice that work pretty

00:17:49.789 --> 00:17:49.799
this issue in practice that work pretty
 

00:17:49.799 --> 00:17:58.310
this issue in practice that work pretty
well the first one is the choice of

00:17:58.310 --> 00:17:58.320
well the first one is the choice of
 

00:17:58.320 --> 00:18:01.159
well the first one is the choice of
activation function so you saw that one

00:18:01.159 --> 00:18:01.169
activation function so you saw that one
 

00:18:01.169 --> 00:18:02.360
activation function so you saw that one
of the terms that was making that

00:18:02.360 --> 00:18:02.370
of the terms that was making that
 

00:18:02.370 --> 00:18:04.250
of the terms that was making that
product really small was the F prime

00:18:04.250 --> 00:18:04.260
product really small was the F prime
 

00:18:04.260 --> 00:18:06.830
product really small was the F prime
term F prime is the derivative of

00:18:06.830 --> 00:18:06.840
term F prime is the derivative of
 

00:18:06.840 --> 00:18:08.810
term F prime is the derivative of
whatever activation function we choose

00:18:08.810 --> 00:18:08.820
whatever activation function we choose
 

00:18:08.820 --> 00:18:11.630
whatever activation function we choose
to use here I've plotted the derivatives

00:18:11.630 --> 00:18:11.640
to use here I've plotted the derivatives
 

00:18:11.640 --> 00:18:14.180
to use here I've plotted the derivatives
of some common activation functions you

00:18:14.180 --> 00:18:14.190
of some common activation functions you
 

00:18:14.190 --> 00:18:15.380
of some common activation functions you
can see that the derivative of

00:18:15.380 --> 00:18:15.390
can see that the derivative of
 

00:18:15.390 --> 00:18:17.720
can see that the derivative of
hyperbolic tangent and sigmoid is always

00:18:17.720 --> 00:18:17.730
hyperbolic tangent and sigmoid is always
 

00:18:17.730 --> 00:18:19.159
hyperbolic tangent and sigmoid is always
less than one in fact for sigmoid it's

00:18:19.159 --> 00:18:19.169
less than one in fact for sigmoid it's
 

00:18:19.169 --> 00:18:21.950
less than one in fact for sigmoid it's
always less than 0.25 and instead we

00:18:21.950 --> 00:18:21.960
always less than 0.25 and instead we
 

00:18:21.960 --> 00:18:23.539
always less than 0.25 and instead we
choose to use an activation function

00:18:23.539 --> 00:18:23.549
choose to use an activation function
 

00:18:23.549 --> 00:18:27.500
choose to use an activation function
like relu it's always 1 above zero so

00:18:27.500 --> 00:18:27.510
like relu it's always 1 above zero so
 

00:18:27.510 --> 00:18:29.180
like relu it's always 1 above zero so
that will at least prevent the F prime

00:18:29.180 --> 00:18:29.190
that will at least prevent the F prime
 

00:18:29.190 --> 00:18:31.630
that will at least prevent the F prime
terms from shrinking the gradient

00:18:31.630 --> 00:18:31.640
terms from shrinking the gradient
 

00:18:31.640 --> 00:18:33.680
terms from shrinking the gradient
another solution would be how we

00:18:33.680 --> 00:18:33.690
another solution would be how we
 

00:18:33.690 --> 00:18:36.230
another solution would be how we
initialize our weights if we initialize

00:18:36.230 --> 00:18:36.240
initialize our weights if we initialize
 

00:18:36.240 --> 00:18:37.460
initialize our weights if we initialize
the weights from a normal distribution

00:18:37.460 --> 00:18:37.470
the weights from a normal distribution
 

00:18:37.470 --> 00:18:39.049
the weights from a normal distribution
they'll be mostly less than 1 and

00:18:39.049 --> 00:18:39.059
they'll be mostly less than 1 and
 

00:18:39.059 --> 00:18:40.880
they'll be mostly less than 1 and
they'll immediately shrink the gradients

00:18:40.880 --> 00:18:40.890
they'll immediately shrink the gradients
 

00:18:40.890 --> 00:18:44.419
they'll immediately shrink the gradients
if instead we initialize the weights to

00:18:44.419 --> 00:18:44.429
if instead we initialize the weights to
 

00:18:44.429 --> 00:18:46.700
if instead we initialize the weights to
something like the identity matrix it'll

00:18:46.700 --> 00:18:46.710
something like the identity matrix it'll
 

00:18:46.710 --> 00:18:49.370
something like the identity matrix it'll
at least prevent that W term from

00:18:49.370 --> 00:18:49.380
at least prevent that W term from
 

00:18:49.380 --> 00:18:51.740
at least prevent that W term from
shrinking that product at least at the

00:18:51.740 --> 00:18:51.750
shrinking that product at least at the
 

00:18:51.750 --> 00:18:55.430
shrinking that product at least at the
beginning the next solution is very

00:18:55.430 --> 00:18:55.440
beginning the next solution is very
 

00:18:55.440 --> 00:18:58.310
beginning the next solution is very
different it involves actually adding a

00:18:58.310 --> 00:18:58.320
different it involves actually adding a
 

00:18:58.320 --> 00:19:01.100
different it involves actually adding a
lot more complexity to the network using

00:19:01.100 --> 00:19:01.110
lot more complexity to the network using
 

00:19:01.110 --> 00:19:03.409
lot more complexity to the network using
a more complex type of cell called a

00:19:03.409 --> 00:19:03.419
a more complex type of cell called a
 

00:19:03.419 --> 00:19:06.620
a more complex type of cell called a
gated cell rather than here rather than

00:19:06.620 --> 00:19:06.630
gated cell rather than here rather than
 

00:19:06.630 --> 00:19:08.960
gated cell rather than here rather than
each node just being that simple our n n

00:19:08.960 --> 00:19:08.970
each node just being that simple our n n
 

00:19:08.970 --> 00:19:11.060
each node just being that simple our n n
unit that I showed at the beginning will

00:19:11.060 --> 00:19:11.070
unit that I showed at the beginning will
 

00:19:11.070 --> 00:19:12.590
unit that I showed at the beginning will
replace it with a much more complicated

00:19:12.590 --> 00:19:12.600
replace it with a much more complicated
 

00:19:12.600 --> 00:19:15.320
replace it with a much more complicated
cell a very common gated cell is

00:19:15.320 --> 00:19:15.330
cell a very common gated cell is
 

00:19:15.330 --> 00:19:17.510
cell a very common gated cell is
something called an L STM or a long

00:19:17.510 --> 00:19:17.520
something called an L STM or a long
 

00:19:17.520 --> 00:19:19.940
something called an L STM or a long
short term memory so like its name

00:19:19.940 --> 00:19:19.950
short term memory so like its name
 

00:19:19.950 --> 00:19:22.450
short term memory so like its name
implies L STM cells are able to keep

00:19:22.450 --> 00:19:22.460
implies L STM cells are able to keep
 

00:19:22.460 --> 00:19:25.669
implies L STM cells are able to keep
memory within the cell state unchanged

00:19:25.669 --> 00:19:25.679
memory within the cell state unchanged
 

00:19:25.679 --> 00:19:28.250
memory within the cell state unchanged
for many time steps this allows them to

00:19:28.250 --> 00:19:28.260
for many time steps this allows them to
 

00:19:28.260 --> 00:19:29.570
for many time steps this allows them to
effectively model longer-term

00:19:29.570 --> 00:19:29.580
effectively model longer-term
 

00:19:29.580 --> 00:19:32.690
effectively model longer-term
dependencies so I'm gonna go through a

00:19:32.690 --> 00:19:32.700
dependencies so I'm gonna go through a
 

00:19:32.700 --> 00:19:35.180
dependencies so I'm gonna go through a
very high-level overview of how Alice

00:19:35.180 --> 00:19:35.190
very high-level overview of how Alice
 

00:19:35.190 --> 00:19:37.220
very high-level overview of how Alice
TMS work but if you're interested feel

00:19:37.220 --> 00:19:37.230
TMS work but if you're interested feel
 

00:19:37.230 --> 00:19:39.049
TMS work but if you're interested feel
free to email me or ask me afterwards

00:19:39.049 --> 00:19:39.059
free to email me or ask me afterwards
 

00:19:39.059 --> 00:19:40.820
free to email me or ask me afterwards
and I can direct you to some more

00:19:40.820 --> 00:19:40.830
and I can direct you to some more
 

00:19:40.830 --> 00:19:43.310
and I can direct you to some more
resources to read about Alice gems in a

00:19:43.310 --> 00:19:43.320
resources to read about Alice gems in a
 

00:19:43.320 --> 00:19:46.490
resources to read about Alice gems in a
lot more detail alright

00:19:46.490 --> 00:19:46.500
lot more detail alright
 

00:19:46.500 --> 00:19:48.500
lot more detail alright
so Alice geum's basically you have a

00:19:48.500 --> 00:19:48.510
so Alice geum's basically you have a
 

00:19:48.510 --> 00:19:51.590
so Alice geum's basically you have a
three-step process the first step is to

00:19:51.590 --> 00:19:51.600
three-step process the first step is to
 

00:19:51.600 --> 00:19:52.430
three-step process the first step is to
forget

00:19:52.430 --> 00:19:52.440
forget
 

00:19:52.440 --> 00:19:54.890
forget
irrelevant parts of the cell state for

00:19:54.890 --> 00:19:54.900
irrelevant parts of the cell state for
 

00:19:54.900 --> 00:19:56.690
irrelevant parts of the cell state for
example if we're modeling a sentence and

00:19:56.690 --> 00:19:56.700
example if we're modeling a sentence and
 

00:19:56.700 --> 00:19:59.270
example if we're modeling a sentence and
we see a new subject we might want to

00:19:59.270 --> 00:19:59.280
we see a new subject we might want to
 

00:19:59.280 --> 00:20:01.040
we see a new subject we might want to
forget things about the old subject

00:20:01.040 --> 00:20:01.050
forget things about the old subject
 

00:20:01.050 --> 00:20:03.320
forget things about the old subject
because we know that future words will

00:20:03.320 --> 00:20:03.330
because we know that future words will
 

00:20:03.330 --> 00:20:04.670
because we know that future words will
be conjugated according to the new

00:20:04.670 --> 00:20:04.680
be conjugated according to the new
 

00:20:04.680 --> 00:20:08.840
be conjugated according to the new
subject the next state the next step is

00:20:08.840 --> 00:20:08.850
subject the next state the next step is
 

00:20:08.850 --> 00:20:11.000
subject the next state the next step is
an update step here's where we actually

00:20:11.000 --> 00:20:11.010
an update step here's where we actually
 

00:20:11.010 --> 00:20:13.580
an update step here's where we actually
update the cell state to reflect the new

00:20:13.580 --> 00:20:13.590
update the cell state to reflect the new
 

00:20:13.590 --> 00:20:15.920
update the cell state to reflect the new
the information from the new input in

00:20:15.920 --> 00:20:15.930
the information from the new input in
 

00:20:15.930 --> 00:20:17.390
the information from the new input in
this example like I said if we've just

00:20:17.390 --> 00:20:17.400
this example like I said if we've just
 

00:20:17.400 --> 00:20:20.240
this example like I said if we've just
seen a new subject we might want to this

00:20:20.240 --> 00:20:20.250
seen a new subject we might want to this
 

00:20:20.250 --> 00:20:21.650
seen a new subject we might want to this
is where we actually update the cell

00:20:21.650 --> 00:20:21.660
is where we actually update the cell
 

00:20:21.660 --> 00:20:23.330
is where we actually update the cell
state with the gender or whether the new

00:20:23.330 --> 00:20:23.340
state with the gender or whether the new
 

00:20:23.340 --> 00:20:27.650
state with the gender or whether the new
subject is plural or singular finally we

00:20:27.650 --> 00:20:27.660
subject is plural or singular finally we
 

00:20:27.660 --> 00:20:29.960
subject is plural or singular finally we
want to output certain parts of the cell

00:20:29.960 --> 00:20:29.970
want to output certain parts of the cell
 

00:20:29.970 --> 00:20:33.500
want to output certain parts of the cell
state so if we've just seen a subject we

00:20:33.500 --> 00:20:33.510
state so if we've just seen a subject we
 

00:20:33.510 --> 00:20:35.120
state so if we've just seen a subject we
have an idea that the next word might be

00:20:35.120 --> 00:20:35.130
have an idea that the next word might be
 

00:20:35.130 --> 00:20:37.280
have an idea that the next word might be
a verb so we'll output information

00:20:37.280 --> 00:20:37.290
a verb so we'll output information
 

00:20:37.290 --> 00:20:40.550
a verb so we'll output information
relevant to predicting a verb like the

00:20:40.550 --> 00:20:40.560
relevant to predicting a verb like the
 

00:20:40.560 --> 00:20:45.440
relevant to predicting a verb like the
tense each of these three steps is

00:20:45.440 --> 00:20:45.450
tense each of these three steps is
 

00:20:45.450 --> 00:20:47.210
tense each of these three steps is
implemented using a set of logic gates

00:20:47.210 --> 00:20:47.220
implemented using a set of logic gates
 

00:20:47.220 --> 00:20:49.310
implemented using a set of logic gates
and the logic gates are implemented

00:20:49.310 --> 00:20:49.320
and the logic gates are implemented
 

00:20:49.320 --> 00:20:53.270
and the logic gates are implemented
using sigmoid functions to give you some

00:20:53.270 --> 00:20:53.280
using sigmoid functions to give you some
 

00:20:53.280 --> 00:20:55.550
using sigmoid functions to give you some
intuition on ylst ms help with the

00:20:55.550 --> 00:20:55.560
intuition on ylst ms help with the
 

00:20:55.560 --> 00:20:58.490
intuition on ylst ms help with the
vanishing gradient problem is that first

00:20:58.490 --> 00:20:58.500
vanishing gradient problem is that first
 

00:20:58.500 --> 00:21:01.430
vanishing gradient problem is that first
the forget gate the first step can

00:21:01.430 --> 00:21:01.440
the forget gate the first step can
 

00:21:01.440 --> 00:21:03.560
the forget gate the first step can
equivalently be called the remember gate

00:21:03.560 --> 00:21:03.570
equivalently be called the remember gate
 

00:21:03.570 --> 00:21:05.810
equivalently be called the remember gate
because there you're choosing what to

00:21:05.810 --> 00:21:05.820
because there you're choosing what to
 

00:21:05.820 --> 00:21:07.940
because there you're choosing what to
forget and what to keep in the cell

00:21:07.940 --> 00:21:07.950
forget and what to keep in the cell
 

00:21:07.950 --> 00:21:11.750
forget and what to keep in the cell
state the forget gate can choose to keep

00:21:11.750 --> 00:21:11.760
state the forget gate can choose to keep
 

00:21:11.760 --> 00:21:14.150
state the forget gate can choose to keep
information in the cell state for many

00:21:14.150 --> 00:21:14.160
information in the cell state for many
 

00:21:14.160 --> 00:21:15.670
information in the cell state for many
many time steps

00:21:15.670 --> 00:21:15.680
many time steps
 

00:21:15.680 --> 00:21:18.020
many time steps
there's no activation function or

00:21:18.020 --> 00:21:18.030
there's no activation function or
 

00:21:18.030 --> 00:21:20.950
there's no activation function or
anything else shrinking that information

00:21:20.950 --> 00:21:20.960
anything else shrinking that information
 

00:21:20.960 --> 00:21:23.180
anything else shrinking that information
the second step the second thing is that

00:21:23.180 --> 00:21:23.190
the second step the second thing is that
 

00:21:23.190 --> 00:21:25.280
the second step the second thing is that
the cell state is separate from what's

00:21:25.280 --> 00:21:25.290
the cell state is separate from what's
 

00:21:25.290 --> 00:21:28.670
the cell state is separate from what's
outputted we're made this is not true of

00:21:28.670 --> 00:21:28.680
outputted we're made this is not true of
 

00:21:28.680 --> 00:21:31.700
outputted we're made this is not true of
normal recurrent units like I showed you

00:21:31.700 --> 00:21:31.710
normal recurrent units like I showed you
 

00:21:31.710 --> 00:21:34.190
normal recurrent units like I showed you
before in a simple recurrent unit the

00:21:34.190 --> 00:21:34.200
before in a simple recurrent unit the
 

00:21:34.200 --> 00:21:35.930
before in a simple recurrent unit the
cell state is the same thing as what

00:21:35.930 --> 00:21:35.940
cell state is the same thing as what
 

00:21:35.940 --> 00:21:38.360
cell state is the same thing as what
that cell outputs with an LS TM it has a

00:21:38.360 --> 00:21:38.370
that cell outputs with an LS TM it has a
 

00:21:38.370 --> 00:21:40.700
that cell outputs with an LS TM it has a
separate cell state and it only needs to

00:21:40.700 --> 00:21:40.710
separate cell state and it only needs to
 

00:21:40.710 --> 00:21:42.890
separate cell state and it only needs to
output information relevant to the

00:21:42.890 --> 00:21:42.900
output information relevant to the
 

00:21:42.900 --> 00:21:45.680
output information relevant to the
prediction at that time step because of

00:21:45.680 --> 00:21:45.690
prediction at that time step because of
 

00:21:45.690 --> 00:21:47.570
prediction at that time step because of
this it can keep information in the cell

00:21:47.570 --> 00:21:47.580
this it can keep information in the cell
 

00:21:47.580 --> 00:21:49.550
this it can keep information in the cell
state which might not be relevant at

00:21:49.550 --> 00:21:49.560
state which might not be relevant at
 

00:21:49.560 --> 00:21:51.290
state which might not be relevant at
this time stuff but might be relevant at

00:21:51.290 --> 00:21:51.300
this time stuff but might be relevant at
 

00:21:51.300 --> 00:21:54.140
this time stuff but might be relevant at
a much later time step so we can keep

00:21:54.140 --> 00:21:54.150
a much later time step so we can keep
 

00:21:54.150 --> 00:21:55.640
a much later time step so we can keep
that information without being penalized

00:21:55.640 --> 00:21:55.650
that information without being penalized
 

00:21:55.650 --> 00:21:56.500
that information without being penalized
for that

00:21:56.500 --> 00:21:56.510
for that
 

00:21:56.510 --> 00:21:58.340
for that
finally I didn't indicate this

00:21:58.340 --> 00:21:58.350
finally I didn't indicate this
 

00:21:58.350 --> 00:22:00.050
finally I didn't indicate this
explicitly in the diagram but the way

00:22:00.050 --> 00:22:00.060
explicitly in the diagram but the way
 

00:22:00.060 --> 00:22:01.850
explicitly in the diagram but the way
that the update step happens is through

00:22:01.850 --> 00:22:01.860
that the update step happens is through
 

00:22:01.860 --> 00:22:03.200
that the update step happens is through
an additive function not through a

00:22:03.200 --> 00:22:03.210
an additive function not through a
 

00:22:03.210 --> 00:22:05.270
an additive function not through a
multiplicative function so when we take

00:22:05.270 --> 00:22:05.280
multiplicative function so when we take
 

00:22:05.280 --> 00:22:05.760
multiplicative function so when we take
the

00:22:05.760 --> 00:22:05.770
the
 

00:22:05.770 --> 00:22:11.190
the
there's not a huge expansion so now I

00:22:11.190 --> 00:22:11.200
there's not a huge expansion so now I
 

00:22:11.200 --> 00:22:14.670
there's not a huge expansion so now I
just want to move on to going over some

00:22:14.670 --> 00:22:14.680
just want to move on to going over some
 

00:22:14.680 --> 00:22:18.320
just want to move on to going over some
possible tasks so the first task is

00:22:18.320 --> 00:22:18.330
possible tasks so the first task is
 

00:22:18.330 --> 00:22:20.970
possible tasks so the first task is
classification so here we want to

00:22:20.970 --> 00:22:20.980
classification so here we want to
 

00:22:20.980 --> 00:22:23.700
classification so here we want to
classify tweets as positive negative or

00:22:23.700 --> 00:22:23.710
classify tweets as positive negative or
 

00:22:23.710 --> 00:22:27.420
classify tweets as positive negative or
neutral and this task is also known as

00:22:27.420 --> 00:22:27.430
neutral and this task is also known as
 

00:22:27.430 --> 00:22:30.900
neutral and this task is also known as
sentiment analysis the way that we would

00:22:30.900 --> 00:22:30.910
sentiment analysis the way that we would
 

00:22:30.910 --> 00:22:32.550
sentiment analysis the way that we would
design a recurrent neural network to do

00:22:32.550 --> 00:22:32.560
design a recurrent neural network to do
 

00:22:32.560 --> 00:22:34.590
design a recurrent neural network to do
this is actually not by having an output

00:22:34.590 --> 00:22:34.600
this is actually not by having an output
 

00:22:34.600 --> 00:22:36.270
this is actually not by having an output
at every time step we only want one

00:22:36.270 --> 00:22:36.280
at every time step we only want one
 

00:22:36.280 --> 00:22:40.920
at every time step we only want one
output for the entire sequence and so

00:22:40.920 --> 00:22:40.930
output for the entire sequence and so
 

00:22:40.930 --> 00:22:43.080
output for the entire sequence and so
we'll take in the entire sequence the

00:22:43.080 --> 00:22:43.090
we'll take in the entire sequence the
 

00:22:43.090 --> 00:22:45.600
we'll take in the entire sequence the
entire tweet one word at a time and at

00:22:45.600 --> 00:22:45.610
entire tweet one word at a time and at
 

00:22:45.610 --> 00:22:47.610
entire tweet one word at a time and at
the very end we'll produce an output

00:22:47.610 --> 00:22:47.620
the very end we'll produce an output
 

00:22:47.620 --> 00:22:49.680
the very end we'll produce an output
which would which will actually be a

00:22:49.680 --> 00:22:49.690
which would which will actually be a
 

00:22:49.690 --> 00:22:52.050
which would which will actually be a
probability distribution over possible

00:22:52.050 --> 00:22:52.060
probability distribution over possible
 

00:22:52.060 --> 00:22:54.540
probability distribution over possible
classes where our classes in this case

00:22:54.540 --> 00:22:54.550
classes where our classes in this case
 

00:22:54.550 --> 00:22:56.270
classes where our classes in this case
would be positive negative or neutral

00:22:56.270 --> 00:22:56.280
would be positive negative or neutral
 

00:22:56.280 --> 00:23:00.440
would be positive negative or neutral
note that the only information that is

00:23:00.440 --> 00:23:00.450
note that the only information that is
 

00:23:00.450 --> 00:23:02.910
note that the only information that is
producing the output at the end is the

00:23:02.910 --> 00:23:02.920
producing the output at the end is the
 

00:23:02.920 --> 00:23:07.110
producing the output at the end is the
final cell state and so that final cell

00:23:07.110 --> 00:23:07.120
final cell state and so that final cell
 

00:23:07.120 --> 00:23:08.880
final cell state and so that final cell
state kind of has to summarize all of

00:23:08.880 --> 00:23:08.890
state kind of has to summarize all of
 

00:23:08.890 --> 00:23:10.800
state kind of has to summarize all of
the information from the begin the

00:23:10.800 --> 00:23:10.810
the information from the begin the
 

00:23:10.810 --> 00:23:13.590
the information from the begin the
entire sequence into that final cell

00:23:13.590 --> 00:23:13.600
entire sequence into that final cell
 

00:23:13.600 --> 00:23:16.290
entire sequence into that final cell
state so we can imagine if we have very

00:23:16.290 --> 00:23:16.300
state so we can imagine if we have very
 

00:23:16.300 --> 00:23:18.510
state so we can imagine if we have very
complicated tweets or well I don't know

00:23:18.510 --> 00:23:18.520
complicated tweets or well I don't know
 

00:23:18.520 --> 00:23:19.590
complicated tweets or well I don't know
if that's possible but very complicated

00:23:19.590 --> 00:23:19.600
if that's possible but very complicated
 

00:23:19.600 --> 00:23:22.590
if that's possible but very complicated
paragraphs or sentences

00:23:22.590 --> 00:23:22.600
paragraphs or sentences
 

00:23:22.600 --> 00:23:25.230
paragraphs or sentences
we might want to create a bigger network

00:23:25.230 --> 00:23:25.240
we might want to create a bigger network
 

00:23:25.240 --> 00:23:28.020
we might want to create a bigger network
with more hidden states to allow that

00:23:28.020 --> 00:23:28.030
with more hidden states to allow that
 

00:23:28.030 --> 00:23:33.420
with more hidden states to allow that
last state to be more expressive the

00:23:33.420 --> 00:23:33.430
last state to be more expressive the
 

00:23:33.430 --> 00:23:34.890
last state to be more expressive the
next task would be something like music

00:23:34.890 --> 00:23:34.900
next task would be something like music
 

00:23:34.900 --> 00:23:37.020
next task would be something like music
generation and I'll see if this will

00:23:37.020 --> 00:23:37.030
generation and I'll see if this will
 

00:23:37.030 --> 00:23:50.140
generation and I'll see if this will
play you can kind of hear it

00:23:50.140 --> 00:23:50.150
 

00:23:50.150 --> 00:23:52.549
okay so that was music generated by an

00:23:52.549 --> 00:23:52.559
okay so that was music generated by an
 

00:23:52.559 --> 00:23:54.320
okay so that was music generated by an
RNN which is pretty cool and something

00:23:54.320 --> 00:23:54.330
RNN which is pretty cool and something
 

00:23:54.330 --> 00:23:57.080
RNN which is pretty cool and something
you're actually also gonna do in the lab

00:23:57.080 --> 00:23:57.090
you're actually also gonna do in the lab
 

00:23:57.090 --> 00:24:01.130
you're actually also gonna do in the lab
today but music generator you can on RNN

00:24:01.130 --> 00:24:01.140
today but music generator you can on RNN
 

00:24:01.140 --> 00:24:03.049
today but music generator you can on RNN
can produce music because music is just

00:24:03.049 --> 00:24:03.059
can produce music because music is just
 

00:24:03.059 --> 00:24:07.570
can produce music because music is just
a sequence and the way that you would

00:24:07.570 --> 00:24:07.580
 

00:24:07.580 --> 00:24:10.210
the way that you would construct a new

00:24:10.210 --> 00:24:10.220
the way that you would construct a new
 

00:24:10.220 --> 00:24:12.220
the way that you would construct a new
recurrent neural network to do this

00:24:12.220 --> 00:24:12.230
recurrent neural network to do this
 

00:24:12.230 --> 00:24:15.110
recurrent neural network to do this
would be at every time point taking in a

00:24:15.110 --> 00:24:15.120
would be at every time point taking in a
 

00:24:15.120 --> 00:24:18.500
would be at every time point taking in a
note and producing the most likely next

00:24:18.500 --> 00:24:18.510
note and producing the most likely next
 

00:24:18.510 --> 00:24:20.450
note and producing the most likely next
note given the notes that you've seen so

00:24:20.450 --> 00:24:20.460
note given the notes that you've seen so
 

00:24:20.460 --> 00:24:21.950
note given the notes that you've seen so
far so here you would produce an output

00:24:21.950 --> 00:24:21.960
far so here you would produce an output
 

00:24:21.960 --> 00:24:28.279
far so here you would produce an output
at every time step the final task is

00:24:28.279 --> 00:24:28.289
at every time step the final task is
 

00:24:28.289 --> 00:24:31.190
at every time step the final task is
machine translation machine translation

00:24:31.190 --> 00:24:31.200
machine translation machine translation
 

00:24:31.200 --> 00:24:33.320
machine translation machine translation
is interesting because it's actually two

00:24:33.320 --> 00:24:33.330
is interesting because it's actually two
 

00:24:33.330 --> 00:24:36.590
is interesting because it's actually two
recurrent neural networks side-by-side

00:24:36.590 --> 00:24:36.600
recurrent neural networks side-by-side
 

00:24:36.600 --> 00:24:38.960
recurrent neural networks side-by-side
the first is an encoder the encoder

00:24:38.960 --> 00:24:38.970
the first is an encoder the encoder
 

00:24:38.970 --> 00:24:42.830
the first is an encoder the encoder
takes as input a sentence in a source

00:24:42.830 --> 00:24:42.840
takes as input a sentence in a source
 

00:24:42.840 --> 00:24:46.760
takes as input a sentence in a source
language like English it then there's

00:24:46.760 --> 00:24:46.770
language like English it then there's
 

00:24:46.770 --> 00:24:48.560
language like English it then there's
done a decoder which produces the same

00:24:48.560 --> 00:24:48.570
done a decoder which produces the same
 

00:24:48.570 --> 00:24:50.570
done a decoder which produces the same
sentence in a target language like

00:24:50.570 --> 00:24:50.580
sentence in a target language like
 

00:24:50.580 --> 00:24:54.560
sentence in a target language like
French notice in this architecture that

00:24:54.560 --> 00:24:54.570
French notice in this architecture that
 

00:24:54.570 --> 00:24:56.960
French notice in this architecture that
the only information passed from the

00:24:56.960 --> 00:24:56.970
the only information passed from the
 

00:24:56.970 --> 00:24:59.360
the only information passed from the
encoder to the decoder is the final cell

00:24:59.360 --> 00:24:59.370
encoder to the decoder is the final cell
 

00:24:59.370 --> 00:25:02.870
encoder to the decoder is the final cell
state and the idea is that that final

00:25:02.870 --> 00:25:02.880
state and the idea is that that final
 

00:25:02.880 --> 00:25:05.060
state and the idea is that that final
state should be kind of a summary of the

00:25:05.060 --> 00:25:05.070
state should be kind of a summary of the
 

00:25:05.070 --> 00:25:07.909
state should be kind of a summary of the
entire encoder sentence and given that

00:25:07.909 --> 00:25:07.919
entire encoder sentence and given that
 

00:25:07.919 --> 00:25:09.710
entire encoder sentence and given that
summary the decoder should be able to

00:25:09.710 --> 00:25:09.720
summary the decoder should be able to
 

00:25:09.720 --> 00:25:11.960
summary the decoder should be able to
figure out what the encoder sentence was

00:25:11.960 --> 00:25:11.970
figure out what the encoder sentence was
 

00:25:11.970 --> 00:25:13.640
figure out what the encoder sentence was
about and then produce the same sentence

00:25:13.640 --> 00:25:13.650
about and then produce the same sentence
 

00:25:13.650 --> 00:25:15.440
about and then produce the same sentence
in a different language you can imagine

00:25:15.440 --> 00:25:15.450
in a different language you can imagine
 

00:25:15.450 --> 00:25:17.630
in a different language you can imagine
though that okay maybe this is possible

00:25:17.630 --> 00:25:17.640
though that okay maybe this is possible
 

00:25:17.640 --> 00:25:19.940
though that okay maybe this is possible
for a sentence a really simple sentence

00:25:19.940 --> 00:25:19.950
for a sentence a really simple sentence
 

00:25:19.950 --> 00:25:22.010
for a sentence a really simple sentence
like the dog eats maybe we can encode

00:25:22.010 --> 00:25:22.020
like the dog eats maybe we can encode
 

00:25:22.020 --> 00:25:23.960
like the dog eats maybe we can encode
that in the final cell state but if we

00:25:23.960 --> 00:25:23.970
that in the final cell state but if we
 

00:25:23.970 --> 00:25:26.450
that in the final cell state but if we
had a much more complicated sentence or

00:25:26.450 --> 00:25:26.460
had a much more complicated sentence or
 

00:25:26.460 --> 00:25:27.799
had a much more complicated sentence or
a much longer sentence that would be

00:25:27.799 --> 00:25:27.809
a much longer sentence that would be
 

00:25:27.809 --> 00:25:30.350
a much longer sentence that would be
very difficult to try and summarize the

00:25:30.350 --> 00:25:30.360
very difficult to try and summarize the
 

00:25:30.360 --> 00:25:32.750
very difficult to try and summarize the
whole thing in that one cell state so

00:25:32.750 --> 00:25:32.760
whole thing in that one cell state so
 

00:25:32.760 --> 00:25:34.370
whole thing in that one cell state so
what's typically done in practice for a

00:25:34.370 --> 00:25:34.380
what's typically done in practice for a
 

00:25:34.380 --> 00:25:36.500
what's typically done in practice for a
machine translation is something called

00:25:36.500 --> 00:25:36.510
machine translation is something called
 

00:25:36.510 --> 00:25:39.620
machine translation is something called
attention with attention rather than

00:25:39.620 --> 00:25:39.630
attention with attention rather than
 

00:25:39.630 --> 00:25:42.049
attention with attention rather than
just taking in the final cell state to

00:25:42.049 --> 00:25:42.059
just taking in the final cell state to
 

00:25:42.059 --> 00:25:44.960
just taking in the final cell state to
the decoder at each time step we take in

00:25:44.960 --> 00:25:44.970
the decoder at each time step we take in
 

00:25:44.970 --> 00:25:46.850
the decoder at each time step we take in
a weighted sum of all of the previous

00:25:46.850 --> 00:25:46.860
a weighted sum of all of the previous
 

00:25:46.860 --> 00:25:49.760
a weighted sum of all of the previous
cell states so in this case we're trying

00:25:49.760 --> 00:25:49.770
cell states so in this case we're trying
 

00:25:49.770 --> 00:25:52.820
cell states so in this case we're trying
to produce the first word we'll take in

00:25:52.820 --> 00:25:52.830
to produce the first word we'll take in
 

00:25:52.830 --> 00:25:55.340
to produce the first word we'll take in
a weighted sum of all of the encoder

00:25:55.340 --> 00:25:55.350
a weighted sum of all of the encoder
 

00:25:55.350 --> 00:25:57.440
a weighted sum of all of the encoder
States most of the weight will probably

00:25:57.440 --> 00:25:57.450
States most of the weight will probably
 

00:25:57.450 --> 00:25:59.120
States most of the weight will probably
be on the first state because that's

00:25:59.120 --> 00:25:59.130
be on the first state because that's
 

00:25:59.130 --> 00:26:00.890
be on the first state because that's
what would be most relevant to producing

00:26:00.890 --> 00:26:00.900
what would be most relevant to producing
 

00:26:00.900 --> 00:26:01.930
what would be most relevant to producing
the first

00:26:01.930 --> 00:26:01.940
the first
 

00:26:01.940 --> 00:26:04.070
the first
then when we produced the second word

00:26:04.070 --> 00:26:04.080
then when we produced the second word
 

00:26:04.080 --> 00:26:06.289
then when we produced the second word
most of the weight will probably be on

00:26:06.289 --> 00:26:06.299
most of the weight will probably be on
 

00:26:06.299 --> 00:26:07.759
most of the weight will probably be on
the second cell state but we might have

00:26:07.759 --> 00:26:07.769
the second cell state but we might have
 

00:26:07.769 --> 00:26:10.100
the second cell state but we might have
some on the first and the third to try

00:26:10.100 --> 00:26:10.110
some on the first and the third to try
 

00:26:10.110 --> 00:26:12.590
some on the first and the third to try
and get an idea for the tenths or the

00:26:12.590 --> 00:26:12.600
and get an idea for the tenths or the
 

00:26:12.600 --> 00:26:16.519
and get an idea for the tenths or the
gender of this now and the same thing

00:26:16.519 --> 00:26:16.529
gender of this now and the same thing
 

00:26:16.529 --> 00:26:18.710
gender of this now and the same thing
for all of the cell states the way that

00:26:18.710 --> 00:26:18.720
for all of the cell states the way that
 

00:26:18.720 --> 00:26:20.389
for all of the cell states the way that
you implement this is just by including

00:26:20.389 --> 00:26:20.399
you implement this is just by including
 

00:26:20.399 --> 00:26:22.639
you implement this is just by including
those weight parameters in the weighted

00:26:22.639 --> 00:26:22.649
those weight parameters in the weighted
 

00:26:22.649 --> 00:26:25.730
those weight parameters in the weighted
sum as additional parameters that you

00:26:25.730 --> 00:26:25.740
sum as additional parameters that you
 

00:26:25.740 --> 00:26:27.740
sum as additional parameters that you
train using back propagation just like

00:26:27.740 --> 00:26:27.750
train using back propagation just like
 

00:26:27.750 --> 00:26:33.350
train using back propagation just like
everything else okay so I hope that you

00:26:33.350 --> 00:26:33.360
everything else okay so I hope that you
 

00:26:33.360 --> 00:26:35.539
everything else okay so I hope that you
have an idea now why we need a different

00:26:35.539 --> 00:26:35.549
have an idea now why we need a different
 

00:26:35.549 --> 00:26:37.090
have an idea now why we need a different
framework to model sequences and how

00:26:37.090 --> 00:26:37.100
framework to model sequences and how
 

00:26:37.100 --> 00:26:39.379
framework to model sequences and how
recurrent neural networks can solve some

00:26:39.379 --> 00:26:39.389
recurrent neural networks can solve some
 

00:26:39.389 --> 00:26:40.490
recurrent neural networks can solve some
of the issues that we saw at the

00:26:40.490 --> 00:26:40.500
of the issues that we saw at the
 

00:26:40.500 --> 00:26:42.830
of the issues that we saw at the
beginning as well as an idea of how to

00:26:42.830 --> 00:26:42.840
beginning as well as an idea of how to
 

00:26:42.840 --> 00:26:44.629
beginning as well as an idea of how to
train them and solve some of the

00:26:44.629 --> 00:26:44.639
train them and solve some of the
 

00:26:44.639 --> 00:26:47.720
train them and solve some of the
vanishing gradient problems I've talked

00:26:47.720 --> 00:26:47.730
vanishing gradient problems I've talked
 

00:26:47.730 --> 00:26:50.320
vanishing gradient problems I've talked
a lot about language but you can imagine

00:26:50.320 --> 00:26:50.330
a lot about language but you can imagine
 

00:26:50.330 --> 00:26:53.840
a lot about language but you can imagine
using these exact same neural net

00:26:53.840 --> 00:26:53.850
using these exact same neural net
 

00:26:53.850 --> 00:26:55.519
using these exact same neural net
recurrent neural networks for modeling

00:26:55.519 --> 00:26:55.529
recurrent neural networks for modeling
 

00:26:55.529 --> 00:26:57.889
recurrent neural networks for modeling
time series or waveforms or doing other

00:26:57.889 --> 00:26:57.899
time series or waveforms or doing other
 

00:26:57.899 --> 00:26:59.930
time series or waveforms or doing other
interesting sequence prediction tasks

00:26:59.930 --> 00:26:59.940
interesting sequence prediction tasks
 

00:26:59.940 --> 00:27:01.580
interesting sequence prediction tasks
like predicting stock market trends or

00:27:01.580 --> 00:27:01.590
like predicting stock market trends or
 

00:27:01.590 --> 00:27:04.850
like predicting stock market trends or
summarizing books or articles and maybe

00:27:04.850 --> 00:27:04.860
summarizing books or articles and maybe
 

00:27:04.860 --> 00:27:06.259
summarizing books or articles and maybe
you'll consider some sequence modeling

00:27:06.259 --> 00:27:06.269
you'll consider some sequence modeling
 

00:27:06.269 --> 00:27:08.480
you'll consider some sequence modeling
tasks for your final project so thank

00:27:08.480 --> 00:27:08.490
tasks for your final project so thank
 

00:27:08.490 --> 00:27:10.130
tasks for your final project so thank
you

00:27:10.130 --> 00:27:10.140
you
 

00:27:10.140 --> 00:27:14.579
you
[Applause]

