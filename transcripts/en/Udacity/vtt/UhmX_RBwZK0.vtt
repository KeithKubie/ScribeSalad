WEBVTT
Kind: captions
Language: en

00:00:00.038 --> 00:00:02.735
I find it helpful to think of the memory system as a pipe.

00:00:02.735 --> 00:00:07.090
Threads issuing requests stuff memory transactions into that pipe.

00:00:07.090 --> 00:00:11.396
For example, these could be load instructions to read in a certain address in memory.

00:00:11.396 --> 00:00:17.106
And the result of that transaction eventually falls out the bottom of the pipe, to head back to those threads.

00:00:17.106 --> 00:00:22.940
Now, the pipe is really deep. It takes 200 or 300 clock cycles for a transaction to move through this pipe.

00:00:22.940 --> 00:00:26.091
And the pipe is also really thick and wide.

00:00:26.091 --> 00:00:29.762
Okay, it's designed to be filled with many transactions at the same time

00:00:29.762 --> 00:00:32.726
from lots of SM's running lots of threads.

00:00:32.726 --> 00:00:38.157
So when you only have a few threads issuing transactions, the pipe is mostly empty.

00:00:38.157 --> 00:00:44.396
This could happen, for example, if you don't have all of your SM's, actively filled with threads that

00:00:44.396 --> 00:00:52.191
are issuing transactions, or if the latency between the transactions coming from each thread is too high.

00:00:52.191 --> 00:00:55.563
So if we only have a few threads issuing transactions,

00:00:55.563 --> 00:00:59.198
the pipe is mostly empty and not many bytes are being delivered.

00:00:59.198 --> 00:01:02.960
And Little's law tells us that if not many bytes are being delivered,

00:01:02.960 --> 00:01:05.698
then our total bandwidth is going to suffer.

00:01:05.698 --> 00:01:08.701
So you can make this better by having more threads issuing memory transactions,

00:01:08.701 --> 00:01:11.560
or by having more memory in flight per thread.

00:01:11.560 --> 00:01:14.343
So here's one of those strategies.

00:01:14.343 --> 00:01:19.261
This is measured data from taking a GPU and copying a bunch of data

00:01:19.261 --> 00:01:22.834
from one matrix to another, just like we're doing.

00:01:22.834 --> 00:01:28.379
And the important thing to notice is that we're doing it in 3 different styles.

00:01:28.379 --> 00:01:31.152
We're doing a 4 byte word--so this is the equivalent

00:01:31.152 --> 00:01:34.093
of a single precision floating point, which is what our code is doing--

00:01:34.093 --> 00:01:37.998
an 8 byte word--for example, if we were moving double precision floating point around,

00:01:37.998 --> 00:01:41.481
that's what we would be getting--or a 16 byte word.

00:01:41.481 --> 00:01:47.391
And there exist in CUDA data types in the 4, 8, and 16 byte, variety.

00:01:47.391 --> 00:01:50.104
For example, float, which is what we've been using so far,

00:01:50.104 --> 00:01:55.295
float 2, which is 2 adjacent floating point numbers, float 4, which is 4 adjacent floating point numbers.

00:01:55.295 --> 00:01:57.425
And so one option that we could do

00:01:57.425 --> 00:02:02.506
is we could try to restructure our code around the idea of having a single

00:02:02.506 --> 00:02:07.581
thread pull in 4 floating point numbers at a time and do its transpose on that.

00:02:07.581 --> 00:02:11.083
That would move us up from this line.

00:02:11.083 --> 00:02:19.037
In terms which is a percent of achieved bandwidth, up to this line, that's a healthy increase.

00:02:19.037 --> 00:02:25.129
On the other hand, the sort of torquing the code around to do something that's less natural--

00:02:25.129 --> 00:02:29.118
like manipulate 4 single floating point values at a time--

00:02:29.118 --> 00:02:32.341
in this case, it's not a particularly natural thing to do.

00:02:32.341 --> 00:02:37.934
What we have coming in--an array of floating point numbers or a matrix of floating point numbers.

00:02:37.934 --> 00:02:42.091
And to read them 4 at a time, in little bitty rows, and transpose those into little

00:02:42.091 --> 00:02:45.703
bitty columns, we can certainly do it, but I think it borders on a Ninja topic.

00:02:45.703 --> 00:02:50.723
While this would help, this is the kind of Ninja optimization that will prove useful.

00:02:50.723 --> 00:02:54.862
I don't think that it's vital. Let's talk about other things that we can do.

00:02:54.862 --> 00:02:59.226
So if we can't make all of our memory transactions wider very easily,

00:02:59.226 --> 00:03:02.315
then we can try to have more transactions.

00:03:02.315 --> 00:03:04.292
Let's have a quiz.

00:03:04.292 --> 00:03:08.187
Which of our transpose kernels, so far, probably suffer from too few transactions?

00:03:08.187 --> 00:03:11.500
Our first version had a single thread for the entire matrix.

00:03:11.500 --> 00:03:15.479
Our next version had a single thread for every row of the matrix.

00:03:15.479 --> 00:03:18.886
Our third version had a single thread for every element,

00:03:18.886 --> 00:03:21.398
and our fourth version was tiled.

00:03:21.398 --> 00:03:24.509
Check those which probably suffered from too few transactions.

