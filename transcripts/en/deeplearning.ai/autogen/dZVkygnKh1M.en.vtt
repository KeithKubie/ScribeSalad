WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.419
 
in this video you learn about some of

00:00:02.419 --> 00:00:02.429
in this video you learn about some of
 

00:00:02.429 --> 00:00:04.400
in this video you learn about some of
the classic neural network architecture

00:00:04.400 --> 00:00:04.410
the classic neural network architecture
 

00:00:04.410 --> 00:00:07.610
the classic neural network architecture
statue of Lynette 5 and then Alex adds

00:00:07.610 --> 00:00:07.620
statue of Lynette 5 and then Alex adds
 

00:00:07.620 --> 00:00:09.850
statue of Lynette 5 and then Alex adds
and then GGG net let's take a look

00:00:09.850 --> 00:00:09.860
and then GGG net let's take a look
 

00:00:09.860 --> 00:00:13.070
and then GGG net let's take a look
here's team Lynette 5 architecture you

00:00:13.070 --> 00:00:13.080
here's team Lynette 5 architecture you
 

00:00:13.080 --> 00:00:16.460
here's team Lynette 5 architecture you
sawed-off of an image which say 32 by 32

00:00:16.460 --> 00:00:16.470
sawed-off of an image which say 32 by 32
 

00:00:16.470 --> 00:00:19.460
sawed-off of an image which say 32 by 32
by one and the Goblin at five was to

00:00:19.460 --> 00:00:19.470
by one and the Goblin at five was to
 

00:00:19.470 --> 00:00:21.740
by one and the Goblin at five was to
recognize handwritten digits Olivia

00:00:21.740 --> 00:00:21.750
recognize handwritten digits Olivia
 

00:00:21.750 --> 00:00:26.029
recognize handwritten digits Olivia
image of a digit like that and Lynette

00:00:26.029 --> 00:00:26.039
image of a digit like that and Lynette
 

00:00:26.039 --> 00:00:28.700
image of a digit like that and Lynette
five was trained on grayscale images

00:00:28.700 --> 00:00:28.710
five was trained on grayscale images
 

00:00:28.710 --> 00:00:32.299
five was trained on grayscale images
which is why it's 32 by 32 by one this

00:00:32.299 --> 00:00:32.309
which is why it's 32 by 32 by one this
 

00:00:32.309 --> 00:00:34.160
which is why it's 32 by 32 by one this
neural network architecture is actually

00:00:34.160 --> 00:00:34.170
neural network architecture is actually
 

00:00:34.170 --> 00:00:37.069
neural network architecture is actually
quite similar to the last example you

00:00:37.069 --> 00:00:37.079
quite similar to the last example you
 

00:00:37.079 --> 00:00:40.310
quite similar to the last example you
saw last week in the first step you use

00:00:40.310 --> 00:00:40.320
saw last week in the first step you use
 

00:00:40.320 --> 00:00:43.220
saw last week in the first step you use
a set of six five by five filters with a

00:00:43.220 --> 00:00:43.230
a set of six five by five filters with a
 

00:00:43.230 --> 00:00:45.500
a set of six five by five filters with a
stride of one because you use six

00:00:45.500 --> 00:00:45.510
stride of one because you use six
 

00:00:45.510 --> 00:00:47.479
stride of one because you use six
filters you end up with a twenty by

00:00:47.479 --> 00:00:47.489
filters you end up with a twenty by
 

00:00:47.489 --> 00:00:50.540
filters you end up with a twenty by
twenty eight by six over there and with

00:00:50.540 --> 00:00:50.550
twenty eight by six over there and with
 

00:00:50.550 --> 00:00:53.420
twenty eight by six over there and with
a stride of one and no padding the image

00:00:53.420 --> 00:00:53.430
a stride of one and no padding the image
 

00:00:53.430 --> 00:00:56.569
a stride of one and no padding the image
dimensions reduces from 32 by 32 down to

00:00:56.569 --> 00:00:56.579
dimensions reduces from 32 by 32 down to
 

00:00:56.579 --> 00:01:00.470
dimensions reduces from 32 by 32 down to
28 by 28 then the Lynette neural network

00:01:00.470 --> 00:01:00.480
28 by 28 then the Lynette neural network
 

00:01:00.480 --> 00:01:04.039
28 by 28 then the Lynette neural network
applies pooling and Bank then when this

00:01:04.039 --> 00:01:04.049
applies pooling and Bank then when this
 

00:01:04.049 --> 00:01:06.020
applies pooling and Bank then when this
paper was written people used average

00:01:06.020 --> 00:01:06.030
paper was written people used average
 

00:01:06.030 --> 00:01:08.420
paper was written people used average
pooling much more if you're building a

00:01:08.420 --> 00:01:08.430
pooling much more if you're building a
 

00:01:08.430 --> 00:01:10.580
pooling much more if you're building a
modern variant you probably use max

00:01:10.580 --> 00:01:10.590
modern variant you probably use max
 

00:01:10.590 --> 00:01:13.429
modern variant you probably use max
pooling instead but in this example your

00:01:13.429 --> 00:01:13.439
pooling instead but in this example your
 

00:01:13.439 --> 00:01:16.640
pooling instead but in this example your
average pool and with a filter width of

00:01:16.640 --> 00:01:16.650
average pool and with a filter width of
 

00:01:16.650 --> 00:01:19.280
average pool and with a filter width of
2 in Australia - you wind up reducing

00:01:19.280 --> 00:01:19.290
2 in Australia - you wind up reducing
 

00:01:19.290 --> 00:01:21.710
2 in Australia - you wind up reducing
the dimensions the height and width by a

00:01:21.710 --> 00:01:21.720
the dimensions the height and width by a
 

00:01:21.720 --> 00:01:23.990
the dimensions the height and width by a
factor of two so you now end up with a

00:01:23.990 --> 00:01:24.000
factor of two so you now end up with a
 

00:01:24.000 --> 00:01:29.539
factor of two so you now end up with a
14 by 14 by 6 volume I guess the height

00:01:29.539 --> 00:01:29.549
14 by 14 by 6 volume I guess the height
 

00:01:29.549 --> 00:01:31.340
14 by 14 by 6 volume I guess the height
and width of these volumes on entirely

00:01:31.340 --> 00:01:31.350
and width of these volumes on entirely
 

00:01:31.350 --> 00:01:33.800
and width of these volumes on entirely
drones a scale you know technically if I

00:01:33.800 --> 00:01:33.810
drones a scale you know technically if I
 

00:01:33.810 --> 00:01:35.330
drones a scale you know technically if I
were drawing his volumes to scale the

00:01:35.330 --> 00:01:35.340
were drawing his volumes to scale the
 

00:01:35.340 --> 00:01:36.920
were drawing his volumes to scale the
height and width would be stronger by a

00:01:36.920 --> 00:01:36.930
height and width would be stronger by a
 

00:01:36.930 --> 00:01:39.140
height and width would be stronger by a
factor of 2 next you apply another

00:01:39.140 --> 00:01:39.150
factor of 2 next you apply another
 

00:01:39.150 --> 00:01:42.469
factor of 2 next you apply another
convolutional layer this time you use a

00:01:42.469 --> 00:01:42.479
convolutional layer this time you use a
 

00:01:42.479 --> 00:01:45.530
convolutional layer this time you use a
set of 16 filters that 5x5 so you end up

00:01:45.530 --> 00:01:45.540
set of 16 filters that 5x5 so you end up
 

00:01:45.540 --> 00:01:48.520
set of 16 filters that 5x5 so you end up
with 16 channels to the next volume and

00:01:48.520 --> 00:01:48.530
with 16 channels to the next volume and
 

00:01:48.530 --> 00:01:52.609
with 16 channels to the next volume and
back when this paper was written in 1998

00:01:52.609 --> 00:01:52.619
back when this paper was written in 1998
 

00:01:52.619 --> 00:01:54.770
back when this paper was written in 1998
people didn't really use having or

00:01:54.770 --> 00:01:54.780
people didn't really use having or
 

00:01:54.780 --> 00:01:56.899
people didn't really use having or
you're always using valid convolutions

00:01:56.899 --> 00:01:56.909
you're always using valid convolutions
 

00:01:56.909 --> 00:01:58.609
you're always using valid convolutions
which is why every time you apply a

00:01:58.609 --> 00:01:58.619
which is why every time you apply a
 

00:01:58.619 --> 00:02:00.740
which is why every time you apply a
convolutional layer the height and width

00:02:00.740 --> 00:02:00.750
convolutional layer the height and width
 

00:02:00.750 --> 00:02:02.929
convolutional layer the height and width
strengths so that's why

00:02:02.929 --> 00:02:02.939
strengths so that's why
 

00:02:02.939 --> 00:02:05.870
strengths so that's why
here you go from 14 to 14 down to 10 by

00:02:05.870 --> 00:02:05.880
here you go from 14 to 14 down to 10 by
 

00:02:05.880 --> 00:02:08.809
here you go from 14 to 14 down to 10 by
10 then another pulling layer so that

00:02:08.809 --> 00:02:08.819
10 then another pulling layer so that
 

00:02:08.819 --> 00:02:09.839
10 then another pulling layer so that
reduces a height of

00:02:09.839 --> 00:02:09.849
reduces a height of
 

00:02:09.849 --> 00:02:11.729
reduces a height of
by factor of two and you end up with

00:02:11.729 --> 00:02:11.739
by factor of two and you end up with
 

00:02:11.739 --> 00:02:14.280
by factor of two and you end up with
five by five over here and if we

00:02:14.280 --> 00:02:14.290
five by five over here and if we
 

00:02:14.290 --> 00:02:17.420
five by five over here and if we
multiply out these numbers 5/16 this

00:02:17.420 --> 00:02:17.430
multiply out these numbers 5/16 this
 

00:02:17.430 --> 00:02:20.339
multiply out these numbers 5/16 this
multiplies out to four hundred and

00:02:20.339 --> 00:02:20.349
multiplies out to four hundred and
 

00:02:20.349 --> 00:02:24.690
multiplies out to four hundred and
that's 25 times 16 is 400 and the next

00:02:24.690 --> 00:02:24.700
that's 25 times 16 is 400 and the next
 

00:02:24.700 --> 00:02:27.390
that's 25 times 16 is 400 and the next
layer is then a fully connected layer

00:02:27.390 --> 00:02:27.400
layer is then a fully connected layer
 

00:02:27.400 --> 00:02:32.339
layer is then a fully connected layer
that fully connects each of these 400

00:02:32.339 --> 00:02:32.349
that fully connects each of these 400
 

00:02:32.349 --> 00:02:35.640
that fully connects each of these 400
notes with every one of a hundred and

00:02:35.640 --> 00:02:35.650
notes with every one of a hundred and
 

00:02:35.650 --> 00:02:37.740
notes with every one of a hundred and
twenty neurons so there's a fully coming

00:02:37.740 --> 00:02:37.750
twenty neurons so there's a fully coming
 

00:02:37.750 --> 00:02:40.319
twenty neurons so there's a fully coming
to layer and sometimes that would draw

00:02:40.319 --> 00:02:40.329
to layer and sometimes that would draw
 

00:02:40.329 --> 00:02:44.280
to layer and sometimes that would draw
out you know explicitly a layer 400

00:02:44.280 --> 00:02:44.290
out you know explicitly a layer 400
 

00:02:44.290 --> 00:02:46.289
out you know explicitly a layer 400
noodles from skipping that here there's

00:02:46.289 --> 00:02:46.299
noodles from skipping that here there's
 

00:02:46.299 --> 00:02:48.149
noodles from skipping that here there's
a fully connected layer and then another

00:02:48.149 --> 00:02:48.159
a fully connected layer and then another
 

00:02:48.159 --> 00:02:50.580
a fully connected layer and then another
fully connected layer and then the final

00:02:50.580 --> 00:02:50.590
fully connected layer and then the final
 

00:02:50.590 --> 00:02:52.259
fully connected layer and then the final
step is it uses these you know

00:02:52.259 --> 00:02:52.269
step is it uses these you know
 

00:02:52.269 --> 00:02:55.530
step is it uses these you know
essentially 84 features and uses it with

00:02:55.530 --> 00:02:55.540
essentially 84 features and uses it with
 

00:02:55.540 --> 00:02:57.929
essentially 84 features and uses it with
one final output I guess you could draw

00:02:57.929 --> 00:02:57.939
one final output I guess you could draw
 

00:02:57.939 --> 00:02:59.909
one final output I guess you could draw
one more node here to make a prediction

00:02:59.909 --> 00:02:59.919
one more node here to make a prediction
 

00:02:59.919 --> 00:03:03.750
one more node here to make a prediction
for Y hat and Y hat took on 10 possible

00:03:03.750 --> 00:03:03.760
for Y hat and Y hat took on 10 possible
 

00:03:03.760 --> 00:03:06.509
for Y hat and Y hat took on 10 possible
values corresponding to recognizing each

00:03:06.509 --> 00:03:06.519
values corresponding to recognizing each
 

00:03:06.519 --> 00:03:09.629
values corresponding to recognizing each
of the digits from 0 to 9 a modern

00:03:09.629 --> 00:03:09.639
of the digits from 0 to 9 a modern
 

00:03:09.639 --> 00:03:12.000
of the digits from 0 to 9 a modern
version of this new network would use a

00:03:12.000 --> 00:03:12.010
version of this new network would use a
 

00:03:12.010 --> 00:03:15.869
version of this new network would use a
soft max layer with a 10-way

00:03:15.869 --> 00:03:15.879
soft max layer with a 10-way
 

00:03:15.879 --> 00:03:18.770
soft max layer with a 10-way
classification output although back then

00:03:18.770 --> 00:03:18.780
classification output although back then
 

00:03:18.780 --> 00:03:21.149
classification output although back then
Lynnette 5 actually used a different

00:03:21.149 --> 00:03:21.159
Lynnette 5 actually used a different
 

00:03:21.159 --> 00:03:23.849
Lynnette 5 actually used a different
classifier at the output layer one data

00:03:23.849 --> 00:03:23.859
classifier at the output layer one data
 

00:03:23.859 --> 00:03:26.999
classifier at the output layer one data
useless today so this new network was

00:03:26.999 --> 00:03:27.009
useless today so this new network was
 

00:03:27.009 --> 00:03:30.080
useless today so this new network was
small by modern standards had about

00:03:30.080 --> 00:03:30.090
small by modern standards had about
 

00:03:30.090 --> 00:03:34.469
small by modern standards had about
60,000 parameters and today you often

00:03:34.469 --> 00:03:34.479
60,000 parameters and today you often
 

00:03:34.479 --> 00:03:36.659
60,000 parameters and today you often
see neural networks with anywhere from

00:03:36.659 --> 00:03:36.669
see neural networks with anywhere from
 

00:03:36.669 --> 00:03:38.280
see neural networks with anywhere from
10 million to a hundred million

00:03:38.280 --> 00:03:38.290
10 million to a hundred million
 

00:03:38.290 --> 00:03:40.830
10 million to a hundred million
parameters and is not unusual to see

00:03:40.830 --> 00:03:40.840
parameters and is not unusual to see
 

00:03:40.840 --> 00:03:42.629
parameters and is not unusual to see
networks there are literally about a

00:03:42.629 --> 00:03:42.639
networks there are literally about a
 

00:03:42.639 --> 00:03:44.879
networks there are literally about a
thousand times bigger than this network

00:03:44.879 --> 00:03:44.889
thousand times bigger than this network
 

00:03:44.889 --> 00:03:47.849
thousand times bigger than this network
but one thing you do see is that as you

00:03:47.849 --> 00:03:47.859
but one thing you do see is that as you
 

00:03:47.859 --> 00:03:49.890
but one thing you do see is that as you
go deeper and then that networks as you

00:03:49.890 --> 00:03:49.900
go deeper and then that networks as you
 

00:03:49.900 --> 00:03:52.679
go deeper and then that networks as you
go from left to right the height and

00:03:52.679 --> 00:03:52.689
go from left to right the height and
 

00:03:52.689 --> 00:03:55.770
go from left to right the height and
width tend to go down so you went from

00:03:55.770 --> 00:03:55.780
width tend to go down so you went from
 

00:03:55.780 --> 00:04:00.240
width tend to go down so you went from
32 by 32 to 28 to 14 to 10 to 5 where as

00:04:00.240 --> 00:04:00.250
32 by 32 to 28 to 14 to 10 to 5 where as
 

00:04:00.250 --> 00:04:02.699
32 by 32 to 28 to 14 to 10 to 5 where as
the number of channels tends to increase

00:04:02.699 --> 00:04:02.709
the number of channels tends to increase
 

00:04:02.709 --> 00:04:08.759
the number of channels tends to increase
it goes from 1 to 6 to 16 as you go

00:04:08.759 --> 00:04:08.769
it goes from 1 to 6 to 16 as you go
 

00:04:08.769 --> 00:04:10.949
it goes from 1 to 6 to 16 as you go
deeper into the layers of the network

00:04:10.949 --> 00:04:10.959
deeper into the layers of the network
 

00:04:10.959 --> 00:04:13.080
deeper into the layers of the network
one other pattern you see in this new

00:04:13.080 --> 00:04:13.090
one other pattern you see in this new
 

00:04:13.090 --> 00:04:14.729
one other pattern you see in this new
network there's still often repeated

00:04:14.729 --> 00:04:14.739
network there's still often repeated
 

00:04:14.739 --> 00:04:17.219
network there's still often repeated
today is that you may have some come one

00:04:17.219 --> 00:04:17.229
today is that you may have some come one
 

00:04:17.229 --> 00:04:19.740
today is that you may have some come one
or more complex followed by pooling

00:04:19.740 --> 00:04:19.750
or more complex followed by pooling
 

00:04:19.750 --> 00:04:21.270
or more complex followed by pooling
layer and then

00:04:21.270 --> 00:04:21.280
layer and then
 

00:04:21.280 --> 00:04:23.790
layer and then
one or sometimes more than one can flare

00:04:23.790 --> 00:04:23.800
one or sometimes more than one can flare
 

00:04:23.800 --> 00:04:26.010
one or sometimes more than one can flare
followed by a pooling layer and then

00:04:26.010 --> 00:04:26.020
followed by a pooling layer and then
 

00:04:26.020 --> 00:04:28.350
followed by a pooling layer and then
some fully connected layers and then the

00:04:28.350 --> 00:04:28.360
some fully connected layers and then the
 

00:04:28.360 --> 00:04:31.470
some fully connected layers and then the
output right so this type of arrangement

00:04:31.470 --> 00:04:31.480
output right so this type of arrangement
 

00:04:31.480 --> 00:04:34.770
output right so this type of arrangement
of layers is quite common now finally

00:04:34.770 --> 00:04:34.780
of layers is quite common now finally
 

00:04:34.780 --> 00:04:36.720
of layers is quite common now finally
this is maybe only for those of you that

00:04:36.720 --> 00:04:36.730
this is maybe only for those of you that
 

00:04:36.730 --> 00:04:39.750
this is maybe only for those of you that
want to try reading the paper there are

00:04:39.750 --> 00:04:39.760
want to try reading the paper there are
 

00:04:39.760 --> 00:04:40.920
want to try reading the paper there are
a couple of other things that were

00:04:40.920 --> 00:04:40.930
a couple of other things that were
 

00:04:40.930 --> 00:04:43.800
a couple of other things that were
different the rest of this slide I'm

00:04:43.800 --> 00:04:43.810
different the rest of this slide I'm
 

00:04:43.810 --> 00:04:46.140
different the rest of this slide I'm
going to make a few more advanced

00:04:46.140 --> 00:04:46.150
going to make a few more advanced
 

00:04:46.150 --> 00:04:48.510
going to make a few more advanced
comments only for those of you that when

00:04:48.510 --> 00:04:48.520
comments only for those of you that when
 

00:04:48.520 --> 00:04:52.410
comments only for those of you that when
they try to meet this classic paper and

00:04:52.410 --> 00:04:52.420
they try to meet this classic paper and
 

00:04:52.420 --> 00:04:54.300
they try to meet this classic paper and
so everything I'm gonna write and read

00:04:54.300 --> 00:04:54.310
so everything I'm gonna write and read
 

00:04:54.310 --> 00:04:58.140
so everything I'm gonna write and read
you can safely skip on the slide and

00:04:58.140 --> 00:04:58.150
you can safely skip on the slide and
 

00:04:58.150 --> 00:04:59.430
you can safely skip on the slide and
this may be an interesting historical

00:04:59.430 --> 00:04:59.440
this may be an interesting historical
 

00:04:59.440 --> 00:05:02.159
this may be an interesting historical
footnote that is okay if you don't

00:05:02.159 --> 00:05:02.169
footnote that is okay if you don't
 

00:05:02.169 --> 00:05:05.490
footnote that is okay if you don't
follow fully so it turns out that if you

00:05:05.490 --> 00:05:05.500
follow fully so it turns out that if you
 

00:05:05.500 --> 00:05:08.340
follow fully so it turns out that if you
read the original paper back then people

00:05:08.340 --> 00:05:08.350
read the original paper back then people
 

00:05:08.350 --> 00:05:11.940
read the original paper back then people
use sigmoid and tannic nonlinearities

00:05:11.940 --> 00:05:11.950
use sigmoid and tannic nonlinearities
 

00:05:11.950 --> 00:05:14.760
use sigmoid and tannic nonlinearities
and people weren't using really

00:05:14.760 --> 00:05:14.770
and people weren't using really
 

00:05:14.770 --> 00:05:16.920
and people weren't using really
nonlinearities back then so if you local

00:05:16.920 --> 00:05:16.930
nonlinearities back then so if you local
 

00:05:16.930 --> 00:05:18.810
nonlinearities back then so if you local
paper you see sigmoid and chanak

00:05:18.810 --> 00:05:18.820
paper you see sigmoid and chanak
 

00:05:18.820 --> 00:05:22.290
paper you see sigmoid and chanak
referred to and there are also some

00:05:22.290 --> 00:05:22.300
referred to and there are also some
 

00:05:22.300 --> 00:05:24.720
referred to and there are also some
funny ways about this network was Maya

00:05:24.720 --> 00:05:24.730
funny ways about this network was Maya
 

00:05:24.730 --> 00:05:27.030
funny ways about this network was Maya
at least funny by modern standards so

00:05:27.030 --> 00:05:27.040
at least funny by modern standards so
 

00:05:27.040 --> 00:05:30.420
at least funny by modern standards so
for example you've seen how if you have

00:05:30.420 --> 00:05:30.430
for example you've seen how if you have
 

00:05:30.430 --> 00:05:34.170
for example you've seen how if you have
a image by NW by NC network with NC

00:05:34.170 --> 00:05:34.180
a image by NW by NC network with NC
 

00:05:34.180 --> 00:05:38.520
a image by NW by NC network with NC
channels then you use F by F by the same

00:05:38.520 --> 00:05:38.530
channels then you use F by F by the same
 

00:05:38.530 --> 00:05:41.760
channels then you use F by F by the same
NC dimensional filter where every filter

00:05:41.760 --> 00:05:41.770
NC dimensional filter where every filter
 

00:05:41.770 --> 00:05:44.700
NC dimensional filter where every filter
looks at every one of these channels but

00:05:44.700 --> 00:05:44.710
looks at every one of these channels but
 

00:05:44.710 --> 00:05:47.130
looks at every one of these channels but
back then computers were much slower and

00:05:47.130 --> 00:05:47.140
back then computers were much slower and
 

00:05:47.140 --> 00:05:49.320
back then computers were much slower and
so to save on computation as well as on

00:05:49.320 --> 00:05:49.330
so to save on computation as well as on
 

00:05:49.330 --> 00:05:52.409
so to save on computation as well as on
parameters the original n85 has some

00:05:52.409 --> 00:05:52.419
parameters the original n85 has some
 

00:05:52.419 --> 00:05:54.390
parameters the original n85 has some
crazy complicated where where different

00:05:54.390 --> 00:05:54.400
crazy complicated where where different
 

00:05:54.400 --> 00:05:56.370
crazy complicated where where different
filters look at different channels of

00:05:56.370 --> 00:05:56.380
filters look at different channels of
 

00:05:56.380 --> 00:05:59.340
filters look at different channels of
the input block and so the paper talks

00:05:59.340 --> 00:05:59.350
the input block and so the paper talks
 

00:05:59.350 --> 00:06:01.110
the input block and so the paper talks
about those details but a more modern

00:06:01.110 --> 00:06:01.120
about those details but a more modern
 

00:06:01.120 --> 00:06:03.270
about those details but a more modern
implementation you know wouldn't have

00:06:03.270 --> 00:06:03.280
implementation you know wouldn't have
 

00:06:03.280 --> 00:06:06.570
implementation you know wouldn't have
that type of complexity these days and

00:06:06.570 --> 00:06:06.580
that type of complexity these days and
 

00:06:06.580 --> 00:06:10.350
that type of complexity these days and
then one last thing that was done back

00:06:10.350 --> 00:06:10.360
then one last thing that was done back
 

00:06:10.360 --> 00:06:12.270
then one last thing that was done back
then I guess but isn't really done right

00:06:12.270 --> 00:06:12.280
then I guess but isn't really done right
 

00:06:12.280 --> 00:06:14.610
then I guess but isn't really done right
now is that you're almost on the net

00:06:14.610 --> 00:06:14.620
now is that you're almost on the net
 

00:06:14.620 --> 00:06:18.840
now is that you're almost on the net
five had a non-linearity after pooling

00:06:18.840 --> 00:06:18.850
five had a non-linearity after pooling
 

00:06:18.850 --> 00:06:21.300
five had a non-linearity after pooling
right and I think that you use a sigmoid

00:06:21.300 --> 00:06:21.310
right and I think that you use a sigmoid
 

00:06:21.310 --> 00:06:25.170
right and I think that you use a sigmoid
non-linearity after the pooling layer so

00:06:25.170 --> 00:06:25.180
non-linearity after the pooling layer so
 

00:06:25.180 --> 00:06:27.659
non-linearity after the pooling layer so
if you do read this paper and this is

00:06:27.659 --> 00:06:27.669
if you do read this paper and this is
 

00:06:27.669 --> 00:06:29.550
if you do read this paper and this is
one of the harder ones to beat of the

00:06:29.550 --> 00:06:29.560
one of the harder ones to beat of the
 

00:06:29.560 --> 00:06:31.629
one of the harder ones to beat of the
ones we'll go over in the next

00:06:31.629 --> 00:06:31.639
ones we'll go over in the next
 

00:06:31.639 --> 00:06:33.729
ones we'll go over in the next
the next one might be an easier one to

00:06:33.729 --> 00:06:33.739
the next one might be an easier one to
 

00:06:33.739 --> 00:06:35.709
the next one might be an easier one to
start with most of the ideas on this

00:06:35.709 --> 00:06:35.719
start with most of the ideas on this
 

00:06:35.719 --> 00:06:38.919
start with most of the ideas on this
slide are described in sections 2 &amp; 3 of

00:06:38.919 --> 00:06:38.929
slide are described in sections 2 &amp; 3 of
 

00:06:38.929 --> 00:06:41.139
slide are described in sections 2 &amp; 3 of
the paper and later sections in the

00:06:41.139 --> 00:06:41.149
the paper and later sections in the
 

00:06:41.149 --> 00:06:44.739
the paper and later sections in the
paper talk about some other ideas it

00:06:44.739 --> 00:06:44.749
paper talk about some other ideas it
 

00:06:44.749 --> 00:06:46.299
paper talk about some other ideas it
talked about something called the draft

00:06:46.299 --> 00:06:46.309
talked about something called the draft
 

00:06:46.309 --> 00:06:48.519
talked about something called the draft
transform and network which isn't widely

00:06:48.519 --> 00:06:48.529
transform and network which isn't widely
 

00:06:48.529 --> 00:06:50.229
transform and network which isn't widely
used today so if you do try to meet this

00:06:50.229 --> 00:06:50.239
used today so if you do try to meet this
 

00:06:50.239 --> 00:06:53.049
used today so if you do try to meet this
paper I recommend focusing on really on

00:06:53.049 --> 00:06:53.059
paper I recommend focusing on really on
 

00:06:53.059 --> 00:06:54.759
paper I recommend focusing on really on
section 2 which talks about this

00:06:54.759 --> 00:06:54.769
section 2 which talks about this
 

00:06:54.769 --> 00:06:57.040
section 2 which talks about this
architecture and maybe take a quick look

00:06:57.040 --> 00:06:57.050
architecture and maybe take a quick look
 

00:06:57.050 --> 00:06:59.769
architecture and maybe take a quick look
at section 3 which has a bunch of

00:06:59.769 --> 00:06:59.779
at section 3 which has a bunch of
 

00:06:59.779 --> 00:07:01.119
at section 3 which has a bunch of
experimental results which are pretty

00:07:01.119 --> 00:07:01.129
experimental results which are pretty
 

00:07:01.129 --> 00:07:03.309
experimental results which are pretty
interesting the second example of a

00:07:03.309 --> 00:07:03.319
interesting the second example of a
 

00:07:03.319 --> 00:07:04.959
interesting the second example of a
neural network I want to show you is

00:07:04.959 --> 00:07:04.969
neural network I want to show you is
 

00:07:04.969 --> 00:07:07.989
neural network I want to show you is
Alex net named after Alex rejewski who

00:07:07.989 --> 00:07:07.999
Alex net named after Alex rejewski who
 

00:07:07.999 --> 00:07:10.509
Alex net named after Alex rejewski who
was the first author of the paper

00:07:10.509 --> 00:07:10.519
was the first author of the paper
 

00:07:10.519 --> 00:07:13.359
was the first author of the paper
describing this work the other authors

00:07:13.359 --> 00:07:13.369
describing this work the other authors
 

00:07:13.369 --> 00:07:15.100
describing this work the other authors
were Elias us cover and Geoffrey Hinton

00:07:15.100 --> 00:07:15.110
were Elias us cover and Geoffrey Hinton
 

00:07:15.110 --> 00:07:18.730
were Elias us cover and Geoffrey Hinton
so Alex net inputs starts with 2 to 7 by

00:07:18.730 --> 00:07:18.740
so Alex net inputs starts with 2 to 7 by
 

00:07:18.740 --> 00:07:21.939
so Alex net inputs starts with 2 to 7 by
2 - 7 by 3 images oh and if you read the

00:07:21.939 --> 00:07:21.949
2 - 7 by 3 images oh and if you read the
 

00:07:21.949 --> 00:07:24.999
2 - 7 by 3 images oh and if you read the
paper the paper refers to 2 to 4 by 2 to

00:07:24.999 --> 00:07:25.009
paper the paper refers to 2 to 4 by 2 to
 

00:07:25.009 --> 00:07:27.999
paper the paper refers to 2 to 4 by 2 to
4 by 3 images but if you look at numbers

00:07:27.999 --> 00:07:28.009
4 by 3 images but if you look at numbers
 

00:07:28.009 --> 00:07:29.919
4 by 3 images but if you look at numbers
I think that the numbers make sense only

00:07:29.919 --> 00:07:29.929
I think that the numbers make sense only
 

00:07:29.929 --> 00:07:33.790
I think that the numbers make sense only
Urvashi to 27 by 227 and then the first

00:07:33.790 --> 00:07:33.800
Urvashi to 27 by 227 and then the first
 

00:07:33.800 --> 00:07:38.019
Urvashi to 27 by 227 and then the first
layer applies a sense of 96 11 by 11

00:07:38.019 --> 00:07:38.029
layer applies a sense of 96 11 by 11
 

00:07:38.029 --> 00:07:40.419
layer applies a sense of 96 11 by 11
filters where they strive for and

00:07:40.419 --> 00:07:40.429
filters where they strive for and
 

00:07:40.429 --> 00:07:42.909
filters where they strive for and
because it uses a large either for the

00:07:42.909 --> 00:07:42.919
because it uses a large either for the
 

00:07:42.919 --> 00:07:45.999
because it uses a large either for the
dimensions shrinks to 55 by 55 so

00:07:45.999 --> 00:07:46.009
dimensions shrinks to 55 by 55 so
 

00:07:46.009 --> 00:07:48.519
dimensions shrinks to 55 by 55 so
roughly going down by a factor of four

00:07:48.519 --> 00:07:48.529
roughly going down by a factor of four
 

00:07:48.529 --> 00:07:51.790
roughly going down by a factor of four
because of the last ride and then

00:07:51.790 --> 00:07:51.800
because of the last ride and then
 

00:07:51.800 --> 00:07:54.369
because of the last ride and then
applies max pulling with a three by

00:07:54.369 --> 00:07:54.379
applies max pulling with a three by
 

00:07:54.379 --> 00:07:56.499
applies max pulling with a three by
three filter so F equals three and a

00:07:56.499 --> 00:07:56.509
three filter so F equals three and a
 

00:07:56.509 --> 00:07:59.499
three filter so F equals three and a
stride of two so this reduces the volume

00:07:59.499 --> 00:07:59.509
stride of two so this reduces the volume
 

00:07:59.509 --> 00:08:06.249
stride of two so this reduces the volume
to 27 by 27 by 96 and then it performs a

00:08:06.249 --> 00:08:06.259
to 27 by 27 by 96 and then it performs a
 

00:08:06.259 --> 00:08:09.129
to 27 by 27 by 96 and then it performs a
5 by 5 same convolution so we're adding

00:08:09.129 --> 00:08:09.139
5 by 5 same convolution so we're adding
 

00:08:09.139 --> 00:08:15.279
5 by 5 same convolution so we're adding
C and F were in from 27 by 27 by 276 max

00:08:15.279 --> 00:08:15.289
C and F were in from 27 by 27 by 276 max
 

00:08:15.289 --> 00:08:17.529
C and F were in from 27 by 27 by 276 max
pooling again this then reduces the

00:08:17.529 --> 00:08:17.539
pooling again this then reduces the
 

00:08:17.539 --> 00:08:20.829
pooling again this then reduces the
height and width to 13 and then another

00:08:20.829 --> 00:08:20.839
height and width to 13 and then another
 

00:08:20.839 --> 00:08:24.339
height and width to 13 and then another
same convolution so same padding so it's

00:08:24.339 --> 00:08:24.349
same convolution so same padding so it's
 

00:08:24.349 --> 00:08:30.669
same convolution so same padding so it's
13 by 13 by now 384 filters and then 3

00:08:30.669 --> 00:08:30.679
13 by 13 by now 384 filters and then 3
 

00:08:30.679 --> 00:08:34.480
13 by 13 by now 384 filters and then 3
by 3 same convolution again gives you

00:08:34.480 --> 00:08:34.490
by 3 same convolution again gives you
 

00:08:34.490 --> 00:08:38.829
by 3 same convolution again gives you
that then 3 by 3 same convolution gives

00:08:38.829 --> 00:08:38.839
that then 3 by 3 same convolution gives
 

00:08:38.839 --> 00:08:42.999
that then 3 by 3 same convolution gives
you that max pool brings it down to 6 by

00:08:42.999 --> 00:08:43.009
you that max pool brings it down to 6 by
 

00:08:43.009 --> 00:08:44.490
you that max pool brings it down to 6 by
6 by 256

00:08:44.490 --> 00:08:44.500
6 by 256
 

00:08:44.500 --> 00:08:47.370
6 by 256
if you multiply out these numbers six

00:08:47.370 --> 00:08:47.380
if you multiply out these numbers six
 

00:08:47.380 --> 00:08:52.470
if you multiply out these numbers six
times six times 256 that's 9216 so we're

00:08:52.470 --> 00:08:52.480
times six times 256 that's 9216 so we're
 

00:08:52.480 --> 00:08:55.560
times six times 256 that's 9216 so we're
gonna unroll this into nine thousand

00:08:55.560 --> 00:08:55.570
gonna unroll this into nine thousand
 

00:08:55.570 --> 00:08:58.110
gonna unroll this into nine thousand
thirteen sixteen notes and then finally

00:08:58.110 --> 00:08:58.120
thirteen sixteen notes and then finally
 

00:08:58.120 --> 00:09:00.650
thirteen sixteen notes and then finally
it has a view fully connected layers and

00:09:00.650 --> 00:09:00.660
it has a view fully connected layers and
 

00:09:00.660 --> 00:09:04.020
it has a view fully connected layers and
then finally uses the softmax to output

00:09:04.020 --> 00:09:04.030
then finally uses the softmax to output
 

00:09:04.030 --> 00:09:07.890
then finally uses the softmax to output
which one of 1,000 courses the object

00:09:07.890 --> 00:09:07.900
which one of 1,000 courses the object
 

00:09:07.900 --> 00:09:12.060
which one of 1,000 courses the object
could be so this new network actually

00:09:12.060 --> 00:09:12.070
could be so this new network actually
 

00:09:12.070 --> 00:09:17.730
could be so this new network actually
had a lot of similarities to the net but

00:09:17.730 --> 00:09:17.740
had a lot of similarities to the net but
 

00:09:17.740 --> 00:09:20.240
had a lot of similarities to the net but
it was much bigger

00:09:20.240 --> 00:09:20.250
it was much bigger
 

00:09:20.250 --> 00:09:23.520
it was much bigger
so whereas Linette Linette five from the

00:09:23.520 --> 00:09:23.530
so whereas Linette Linette five from the
 

00:09:23.530 --> 00:09:26.130
so whereas Linette Linette five from the
previous slide had sixty thousand about

00:09:26.130 --> 00:09:26.140
previous slide had sixty thousand about
 

00:09:26.140 --> 00:09:28.650
previous slide had sixty thousand about
sixty thousand parameters this Alec's

00:09:28.650 --> 00:09:28.660
sixty thousand parameters this Alec's
 

00:09:28.660 --> 00:09:30.660
sixty thousand parameters this Alec's
net had about sixty million parameters

00:09:30.660 --> 00:09:30.670
net had about sixty million parameters
 

00:09:30.670 --> 00:09:34.140
net had about sixty million parameters
and the fact that they could take meal

00:09:34.140 --> 00:09:34.150
and the fact that they could take meal
 

00:09:34.150 --> 00:09:36.320
and the fact that they could take meal
pretty similar basic building blocks but

00:09:36.320 --> 00:09:36.330
pretty similar basic building blocks but
 

00:09:36.330 --> 00:09:39.090
pretty similar basic building blocks but
have a lot more hidden units in training

00:09:39.090 --> 00:09:39.100
have a lot more hidden units in training
 

00:09:39.100 --> 00:09:41.070
have a lot more hidden units in training
on a lot more data was he trained on the

00:09:41.070 --> 00:09:41.080
on a lot more data was he trained on the
 

00:09:41.080 --> 00:09:43.350
on a lot more data was he trained on the
image net data set that allowed it to

00:09:43.350 --> 00:09:43.360
image net data set that allowed it to
 

00:09:43.360 --> 00:09:47.090
image net data set that allowed it to
have this remarkable performance another

00:09:47.090 --> 00:09:47.100
have this remarkable performance another
 

00:09:47.100 --> 00:09:49.560
have this remarkable performance another
aspect of this architecture they made it

00:09:49.560 --> 00:09:49.570
aspect of this architecture they made it
 

00:09:49.570 --> 00:09:51.510
aspect of this architecture they made it
much better than the net was using the

00:09:51.510 --> 00:09:51.520
much better than the net was using the
 

00:09:51.520 --> 00:09:54.000
much better than the net was using the
revenue activation function and then

00:09:54.000 --> 00:09:54.010
revenue activation function and then
 

00:09:54.010 --> 00:09:56.130
revenue activation function and then
again just if you read the baby paper

00:09:56.130 --> 00:09:56.140
again just if you read the baby paper
 

00:09:56.140 --> 00:09:58.410
again just if you read the baby paper
some more advanced details that you

00:09:58.410 --> 00:09:58.420
some more advanced details that you
 

00:09:58.420 --> 00:09:59.760
some more advanced details that you
don't really need to worry about if you

00:09:59.760 --> 00:09:59.770
don't really need to worry about if you
 

00:09:59.770 --> 00:10:02.250
don't really need to worry about if you
don't read the paper one is that when

00:10:02.250 --> 00:10:02.260
don't read the paper one is that when
 

00:10:02.260 --> 00:10:04.410
don't read the paper one is that when
those people was written GPUs were still

00:10:04.410 --> 00:10:04.420
those people was written GPUs were still
 

00:10:04.420 --> 00:10:07.110
those people was written GPUs were still
a little bit slower so it had a

00:10:07.110 --> 00:10:07.120
a little bit slower so it had a
 

00:10:07.120 --> 00:10:10.440
a little bit slower so it had a
complicated way of training on two GPUs

00:10:10.440 --> 00:10:10.450
complicated way of training on two GPUs
 

00:10:10.450 --> 00:10:13.740
complicated way of training on two GPUs
and the basic idea was that a lot of

00:10:13.740 --> 00:10:13.750
and the basic idea was that a lot of
 

00:10:13.750 --> 00:10:16.380
and the basic idea was that a lot of
these layers was actually split across

00:10:16.380 --> 00:10:16.390
these layers was actually split across
 

00:10:16.390 --> 00:10:18.240
these layers was actually split across
two different GPUs and there was a

00:10:18.240 --> 00:10:18.250
two different GPUs and there was a
 

00:10:18.250 --> 00:10:20.670
two different GPUs and there was a
thoughtful way for when the two GPUs

00:10:20.670 --> 00:10:20.680
thoughtful way for when the two GPUs
 

00:10:20.680 --> 00:10:23.280
thoughtful way for when the two GPUs
would communicate with each other and

00:10:23.280 --> 00:10:23.290
would communicate with each other and
 

00:10:23.290 --> 00:10:26.460
would communicate with each other and
the paper also the original Alex net

00:10:26.460 --> 00:10:26.470
the paper also the original Alex net
 

00:10:26.470 --> 00:10:29.160
the paper also the original Alex net
architecture also had a node type of

00:10:29.160 --> 00:10:29.170
architecture also had a node type of
 

00:10:29.170 --> 00:10:32.360
architecture also had a node type of
layer called a local response

00:10:32.360 --> 00:10:32.370
layer called a local response
 

00:10:32.370 --> 00:10:35.310
layer called a local response
normalization and this type of layer

00:10:35.310 --> 00:10:35.320
normalization and this type of layer
 

00:10:35.320 --> 00:10:37.320
normalization and this type of layer
isn't really used much which is why I

00:10:37.320 --> 00:10:37.330
isn't really used much which is why I
 

00:10:37.330 --> 00:10:39.900
isn't really used much which is why I
didn't talk about it but the basic idea

00:10:39.900 --> 00:10:39.910
didn't talk about it but the basic idea
 

00:10:39.910 --> 00:10:42.300
didn't talk about it but the basic idea
of local response normalization is if

00:10:42.300 --> 00:10:42.310
of local response normalization is if
 

00:10:42.310 --> 00:10:45.270
of local response normalization is if
you look at one of these blocks one of

00:10:45.270 --> 00:10:45.280
you look at one of these blocks one of
 

00:10:45.280 --> 00:10:47.160
you look at one of these blocks one of
these volumes that we have on top let's

00:10:47.160 --> 00:10:47.170
these volumes that we have on top let's
 

00:10:47.170 --> 00:10:48.930
these volumes that we have on top let's
say for the sake of argument this one

00:10:48.930 --> 00:10:48.940
say for the sake of argument this one
 

00:10:48.940 --> 00:10:52.980
say for the sake of argument this one
you know 13 by 13 by 256 what local

00:10:52.980 --> 00:10:52.990
you know 13 by 13 by 256 what local
 

00:10:52.990 --> 00:10:56.280
you know 13 by 13 by 256 what local
response normalization ORN does is you

00:10:56.280 --> 00:10:56.290
response normalization ORN does is you
 

00:10:56.290 --> 00:10:57.420
response normalization ORN does is you
look at one position

00:10:57.420 --> 00:10:57.430
look at one position
 

00:10:57.430 --> 00:11:00.090
look at one position
so one position heightened with and look

00:11:00.090 --> 00:11:00.100
so one position heightened with and look
 

00:11:00.100 --> 00:11:03.000
so one position heightened with and look
down this across all the channels look

00:11:03.000 --> 00:11:03.010
down this across all the channels look
 

00:11:03.010 --> 00:11:06.660
down this across all the channels look
at all 256 numbers and normalize them

00:11:06.660 --> 00:11:06.670
at all 256 numbers and normalize them
 

00:11:06.670 --> 00:11:08.970
at all 256 numbers and normalize them
and the motivation for this local

00:11:08.970 --> 00:11:08.980
and the motivation for this local
 

00:11:08.980 --> 00:11:10.920
and the motivation for this local
response normalization was that for each

00:11:10.920 --> 00:11:10.930
response normalization was that for each
 

00:11:10.930 --> 00:11:15.330
response normalization was that for each
position in this 13 by 13 image maybe

00:11:15.330 --> 00:11:15.340
position in this 13 by 13 image maybe
 

00:11:15.340 --> 00:11:18.360
position in this 13 by 13 image maybe
you don't want too many neurons with a

00:11:18.360 --> 00:11:18.370
you don't want too many neurons with a
 

00:11:18.370 --> 00:11:21.000
you don't want too many neurons with a
very high activation but subsequently

00:11:21.000 --> 00:11:21.010
very high activation but subsequently
 

00:11:21.010 --> 00:11:23.100
very high activation but subsequently
many researchers have found that this

00:11:23.100 --> 00:11:23.110
many researchers have found that this
 

00:11:23.110 --> 00:11:26.040
many researchers have found that this
doesn't help that much so this is one of

00:11:26.040 --> 00:11:26.050
doesn't help that much so this is one of
 

00:11:26.050 --> 00:11:27.660
doesn't help that much so this is one of
those ideas that guess I'm drawing in

00:11:27.660 --> 00:11:27.670
those ideas that guess I'm drawing in
 

00:11:27.670 --> 00:11:30.270
those ideas that guess I'm drawing in
red because it's less important for you

00:11:30.270 --> 00:11:30.280
red because it's less important for you
 

00:11:30.280 --> 00:11:32.790
red because it's less important for you
to understand this one and in practice I

00:11:32.790 --> 00:11:32.800
to understand this one and in practice I
 

00:11:32.800 --> 00:11:34.440
to understand this one and in practice I
don't really use vocal response

00:11:34.440 --> 00:11:34.450
don't really use vocal response
 

00:11:34.450 --> 00:11:37.230
don't really use vocal response
normalizations you know really in the

00:11:37.230 --> 00:11:37.240
normalizations you know really in the
 

00:11:37.240 --> 00:11:39.000
normalizations you know really in the
networks that I were trained today so if

00:11:39.000 --> 00:11:39.010
networks that I were trained today so if
 

00:11:39.010 --> 00:11:40.830
networks that I were trained today so if
you're interested in the history of deep

00:11:40.830 --> 00:11:40.840
you're interested in the history of deep
 

00:11:40.840 --> 00:11:43.560
you're interested in the history of deep
learning I think even before Alex that

00:11:43.560 --> 00:11:43.570
learning I think even before Alex that
 

00:11:43.570 --> 00:11:45.330
learning I think even before Alex that
deep learning was starting to gain

00:11:45.330 --> 00:11:45.340
deep learning was starting to gain
 

00:11:45.340 --> 00:11:47.730
deep learning was starting to gain
traction in speech recognition in a few

00:11:47.730 --> 00:11:47.740
traction in speech recognition in a few
 

00:11:47.740 --> 00:11:50.940
traction in speech recognition in a few
other areas but it was really this paper

00:11:50.940 --> 00:11:50.950
other areas but it was really this paper
 

00:11:50.950 --> 00:11:53.280
other areas but it was really this paper
that convinced a lot of the computer

00:11:53.280 --> 00:11:53.290
that convinced a lot of the computer
 

00:11:53.290 --> 00:11:55.800
that convinced a lot of the computer
vision community to take a serious look

00:11:55.800 --> 00:11:55.810
vision community to take a serious look
 

00:11:55.810 --> 00:11:57.630
vision community to take a serious look
at deep learning to convince them that

00:11:57.630 --> 00:11:57.640
at deep learning to convince them that
 

00:11:57.640 --> 00:11:59.340
at deep learning to convince them that
deep learning really works in computer

00:11:59.340 --> 00:11:59.350
deep learning really works in computer
 

00:11:59.350 --> 00:12:01.200
deep learning really works in computer
vision and then they grew on to have a

00:12:01.200 --> 00:12:01.210
vision and then they grew on to have a
 

00:12:01.210 --> 00:12:03.420
vision and then they grew on to have a
huge impact not just in computer vision

00:12:03.420 --> 00:12:03.430
huge impact not just in computer vision
 

00:12:03.430 --> 00:12:05.550
huge impact not just in computer vision
but beyond computer vision as well and

00:12:05.550 --> 00:12:05.560
but beyond computer vision as well and
 

00:12:05.560 --> 00:12:07.680
but beyond computer vision as well and
if you want to try reading some of these

00:12:07.680 --> 00:12:07.690
if you want to try reading some of these
 

00:12:07.690 --> 00:12:09.570
if you want to try reading some of these
papers yourself and you really don't

00:12:09.570 --> 00:12:09.580
papers yourself and you really don't
 

00:12:09.580 --> 00:12:12.450
papers yourself and you really don't
have to for this course but if you want

00:12:12.450 --> 00:12:12.460
have to for this course but if you want
 

00:12:12.460 --> 00:12:14.520
have to for this course but if you want
to try reading some of these papers this

00:12:14.520 --> 00:12:14.530
to try reading some of these papers this
 

00:12:14.530 --> 00:12:16.980
to try reading some of these papers this
one is one of the easier ones to read so

00:12:16.980 --> 00:12:16.990
one is one of the easier ones to read so
 

00:12:16.990 --> 00:12:18.480
one is one of the easier ones to read so
this might be a good one to take a look

00:12:18.480 --> 00:12:18.490
this might be a good one to take a look
 

00:12:18.490 --> 00:12:21.690
this might be a good one to take a look
at so where as Alex Ned had a relatively

00:12:21.690 --> 00:12:21.700
at so where as Alex Ned had a relatively
 

00:12:21.700 --> 00:12:23.850
at so where as Alex Ned had a relatively
complicated architecture there's just a

00:12:23.850 --> 00:12:23.860
complicated architecture there's just a
 

00:12:23.860 --> 00:12:26.400
complicated architecture there's just a
lot of hyper parameters right where you

00:12:26.400 --> 00:12:26.410
lot of hyper parameters right where you
 

00:12:26.410 --> 00:12:30.420
lot of hyper parameters right where you
have all these numbers that Alex

00:12:30.420 --> 00:12:30.430
have all these numbers that Alex
 

00:12:30.430 --> 00:12:32.370
have all these numbers that Alex
Rusedski and his co-authors had to come

00:12:32.370 --> 00:12:32.380
Rusedski and his co-authors had to come
 

00:12:32.380 --> 00:12:34.710
Rusedski and his co-authors had to come
up with let me show you a third and

00:12:34.710 --> 00:12:34.720
up with let me show you a third and
 

00:12:34.720 --> 00:12:36.720
up with let me show you a third and
final example in this video called the

00:12:36.720 --> 00:12:36.730
final example in this video called the
 

00:12:36.730 --> 00:12:40.380
final example in this video called the
VGA or the bgg 16 network and a

00:12:40.380 --> 00:12:40.390
VGA or the bgg 16 network and a
 

00:12:40.390 --> 00:12:43.440
VGA or the bgg 16 network and a
remarkable thing about the vgg 16 is

00:12:43.440 --> 00:12:43.450
remarkable thing about the vgg 16 is
 

00:12:43.450 --> 00:12:45.930
remarkable thing about the vgg 16 is
that they said instead of having so many

00:12:45.930 --> 00:12:45.940
that they said instead of having so many
 

00:12:45.940 --> 00:12:48.420
that they said instead of having so many
hyper parameters list use a much simpler

00:12:48.420 --> 00:12:48.430
hyper parameters list use a much simpler
 

00:12:48.430 --> 00:12:51.540
hyper parameters list use a much simpler
network where you focus on just having

00:12:51.540 --> 00:12:51.550
network where you focus on just having
 

00:12:51.550 --> 00:12:54.090
network where you focus on just having
conflicts that are just three by three

00:12:54.090 --> 00:12:54.100
conflicts that are just three by three
 

00:12:54.100 --> 00:12:56.280
conflicts that are just three by three
filters with us try the one and always

00:12:56.280 --> 00:12:56.290
filters with us try the one and always
 

00:12:56.290 --> 00:13:00.300
filters with us try the one and always
use same padding and make all your max

00:13:00.300 --> 00:13:00.310
use same padding and make all your max
 

00:13:00.310 --> 00:13:03.090
use same padding and make all your max
pooling layers two by two over stride of

00:13:03.090 --> 00:13:03.100
pooling layers two by two over stride of
 

00:13:03.100 --> 00:13:07.180
pooling layers two by two over stride of
two and so one very nice thing about the

00:13:07.180 --> 00:13:07.190
two and so one very nice thing about the
 

00:13:07.190 --> 00:13:10.000
two and so one very nice thing about the
G network was really simplified these

00:13:10.000 --> 00:13:10.010
G network was really simplified these
 

00:13:10.010 --> 00:13:12.760
G network was really simplified these
neural network architectures so let's go

00:13:12.760 --> 00:13:12.770
neural network architectures so let's go
 

00:13:12.770 --> 00:13:15.160
neural network architectures so let's go
through the architecture so you start up

00:13:15.160 --> 00:13:15.170
through the architecture so you start up
 

00:13:15.170 --> 00:13:17.140
through the architecture so you start up
with an image and then the first two

00:13:17.140 --> 00:13:17.150
with an image and then the first two
 

00:13:17.150 --> 00:13:21.760
with an image and then the first two
layers are convolutions which are

00:13:21.760 --> 00:13:21.770
layers are convolutions which are
 

00:13:21.770 --> 00:13:23.670
layers are convolutions which are
therefore these three by three filters

00:13:23.670 --> 00:13:23.680
therefore these three by three filters
 

00:13:23.680 --> 00:13:26.350
therefore these three by three filters
and in the first layers first two layers

00:13:26.350 --> 00:13:26.360
and in the first layers first two layers
 

00:13:26.360 --> 00:13:28.750
and in the first layers first two layers
you use 64 filters so you end up with a

00:13:28.750 --> 00:13:28.760
you use 64 filters so you end up with a
 

00:13:28.760 --> 00:13:30.880
you use 64 filters so you end up with a
two to four by two to four because using

00:13:30.880 --> 00:13:30.890
two to four by two to four because using
 

00:13:30.890 --> 00:13:34.960
two to four by two to four because using
same convolutions and then with 64

00:13:34.960 --> 00:13:34.970
same convolutions and then with 64
 

00:13:34.970 --> 00:13:37.750
same convolutions and then with 64
channels so because VG 16 is a

00:13:37.750 --> 00:13:37.760
channels so because VG 16 is a
 

00:13:37.760 --> 00:13:40.480
channels so because VG 16 is a
relatively deep network I'm going to not

00:13:40.480 --> 00:13:40.490
relatively deep network I'm going to not
 

00:13:40.490 --> 00:13:42.880
relatively deep network I'm going to not
draw all the volumes here so what does

00:13:42.880 --> 00:13:42.890
draw all the volumes here so what does
 

00:13:42.890 --> 00:13:45.490
draw all the volumes here so what does
little picture de knows is what we would

00:13:45.490 --> 00:13:45.500
little picture de knows is what we would
 

00:13:45.500 --> 00:13:48.730
little picture de knows is what we would
previously have drawn as this 2 to 4 by

00:13:48.730 --> 00:13:48.740
previously have drawn as this 2 to 4 by
 

00:13:48.740 --> 00:13:53.020
previously have drawn as this 2 to 4 by
2 to 4 by 3 and then a convolution that

00:13:53.020 --> 00:13:53.030
2 to 4 by 3 and then a convolution that
 

00:13:53.030 --> 00:13:57.160
2 to 4 by 3 and then a convolution that
results in I guess a 2 to 4 by 2 to 4 by

00:13:57.160 --> 00:13:57.170
results in I guess a 2 to 4 by 2 to 4 by
 

00:13:57.170 --> 00:14:00.280
results in I guess a 2 to 4 by 2 to 4 by
64 some picture on this a deeper volume

00:14:00.280 --> 00:14:00.290
64 some picture on this a deeper volume
 

00:14:00.290 --> 00:14:02.920
64 some picture on this a deeper volume
and then another layer that results in

00:14:02.920 --> 00:14:02.930
and then another layer that results in
 

00:14:02.930 --> 00:14:07.750
and then another layer that results in
you know 2 to 4 by 2 to 4 by 64 so this

00:14:07.750 --> 00:14:07.760
you know 2 to 4 by 2 to 4 by 64 so this
 

00:14:07.760 --> 00:14:11.380
you know 2 to 4 by 2 to 4 by 64 so this
con 64 times 2 represents that you're

00:14:11.380 --> 00:14:11.390
con 64 times 2 represents that you're
 

00:14:11.390 --> 00:14:14.800
con 64 times 2 represents that you're
doing 2 layers to conflate with 64

00:14:14.800 --> 00:14:14.810
doing 2 layers to conflate with 64
 

00:14:14.810 --> 00:14:17.650
doing 2 layers to conflate with 64
filters and as I mentioned earlier the

00:14:17.650 --> 00:14:17.660
filters and as I mentioned earlier the
 

00:14:17.660 --> 00:14:21.820
filters and as I mentioned earlier the
filters are always 3 by 3 with Australia

00:14:21.820 --> 00:14:21.830
filters are always 3 by 3 with Australia
 

00:14:21.830 --> 00:14:24.220
filters are always 3 by 3 with Australia
1 and they're always same compositions

00:14:24.220 --> 00:14:24.230
1 and they're always same compositions
 

00:14:24.230 --> 00:14:26.230
1 and they're always same compositions
so rather than drawing all these volumes

00:14:26.230 --> 00:14:26.240
so rather than drawing all these volumes
 

00:14:26.240 --> 00:14:27.970
so rather than drawing all these volumes
I'm just not use text to represent this

00:14:27.970 --> 00:14:27.980
I'm just not use text to represent this
 

00:14:27.980 --> 00:14:31.870
I'm just not use text to represent this
network mix then uses a pulling layer so

00:14:31.870 --> 00:14:31.880
network mix then uses a pulling layer so
 

00:14:31.880 --> 00:14:33.880
network mix then uses a pulling layer so
the pooling layer will reduce anything

00:14:33.880 --> 00:14:33.890
the pooling layer will reduce anything
 

00:14:33.890 --> 00:14:35.890
the pooling layer will reduce anything
about it goes from 2 to 4 by 2 to 4 down

00:14:35.890 --> 00:14:35.900
about it goes from 2 to 4 by 2 to 4 down
 

00:14:35.900 --> 00:14:39.970
about it goes from 2 to 4 by 2 to 4 down
to what rate goes to 1 12 and 112 by 64

00:14:39.970 --> 00:14:39.980
to what rate goes to 1 12 and 112 by 64
 

00:14:39.980 --> 00:14:43.840
to what rate goes to 1 12 and 112 by 64
and then it has a couple more conflicts

00:14:43.840 --> 00:14:43.850
and then it has a couple more conflicts
 

00:14:43.850 --> 00:14:48.040
and then it has a couple more conflicts
so this means that it has a 128 filters

00:14:48.040 --> 00:14:48.050
so this means that it has a 128 filters
 

00:14:48.050 --> 00:14:50.260
so this means that it has a 128 filters
and because he's the same convolutions

00:14:50.260 --> 00:14:50.270
and because he's the same convolutions
 

00:14:50.270 --> 00:14:52.780
and because he's the same convolutions
let's see what's the new dimension all

00:14:52.780 --> 00:14:52.790
let's see what's the new dimension all
 

00:14:52.790 --> 00:14:56.650
let's see what's the new dimension all
right it'll be 1 2 up by 112 by 128 and

00:14:56.650 --> 00:14:56.660
right it'll be 1 2 up by 112 by 128 and
 

00:14:56.660 --> 00:14:59.590
right it'll be 1 2 up by 112 by 128 and
then pulling layer so you can figure out

00:14:59.590 --> 00:14:59.600
then pulling layer so you can figure out
 

00:14:59.600 --> 00:15:01.540
then pulling layer so you can figure out
what's the new dimension right it'll be

00:15:01.540 --> 00:15:01.550
what's the new dimension right it'll be
 

00:15:01.550 --> 00:15:04.750
what's the new dimension right it'll be
damned and now three comp layers with

00:15:04.750 --> 00:15:04.760
damned and now three comp layers with
 

00:15:04.760 --> 00:15:08.079
damned and now three comp layers with
256 dimensional filters you have to 256

00:15:08.079 --> 00:15:08.089
256 dimensional filters you have to 256
 

00:15:08.089 --> 00:15:10.900
256 dimensional filters you have to 256
filters sorry - the pooling layer and

00:15:10.900 --> 00:15:10.910
filters sorry - the pooling layer and
 

00:15:10.910 --> 00:15:15.180
filters sorry - the pooling layer and
then a few more conflicts pulling layer

00:15:15.180 --> 00:15:15.190
then a few more conflicts pulling layer
 

00:15:15.190 --> 00:15:17.230
then a few more conflicts pulling layer
more conflicts

00:15:17.230 --> 00:15:17.240
more conflicts
 

00:15:17.240 --> 00:15:18.890
more conflicts
pooling layer

00:15:18.890 --> 00:15:18.900
pooling layer
 

00:15:18.900 --> 00:15:23.030
pooling layer
and then it takes this final seven by

00:15:23.030 --> 00:15:23.040
and then it takes this final seven by
 

00:15:23.040 --> 00:15:25.490
and then it takes this final seven by
seven by five to twelve fees it's a

00:15:25.490 --> 00:15:25.500
seven by five to twelve fees it's a
 

00:15:25.500 --> 00:15:27.680
seven by five to twelve fees it's a
fully connected layer fully connected

00:15:27.680 --> 00:15:27.690
fully connected layer fully connected
 

00:15:27.690 --> 00:15:31.490
fully connected layer fully connected
with four engine 4096 units and in a

00:15:31.490 --> 00:15:31.500
with four engine 4096 units and in a
 

00:15:31.500 --> 00:15:33.920
with four engine 4096 units and in a
soft next output one of a thousand

00:15:33.920 --> 00:15:33.930
soft next output one of a thousand
 

00:15:33.930 --> 00:15:38.030
soft next output one of a thousand
classes the by the way does sixteen in

00:15:38.030 --> 00:15:38.040
classes the by the way does sixteen in
 

00:15:38.040 --> 00:15:41.090
classes the by the way does sixteen in
the name vgg sixteen refers to the fact

00:15:41.090 --> 00:15:41.100
the name vgg sixteen refers to the fact
 

00:15:41.100 --> 00:15:43.730
the name vgg sixteen refers to the fact
that this has 16 layers that have some

00:15:43.730 --> 00:15:43.740
that this has 16 layers that have some
 

00:15:43.740 --> 00:15:46.550
that this has 16 layers that have some
weights and this is a pretty large

00:15:46.550 --> 00:15:46.560
weights and this is a pretty large
 

00:15:46.560 --> 00:15:49.100
weights and this is a pretty large
network this network has a total of

00:15:49.100 --> 00:15:49.110
network this network has a total of
 

00:15:49.110 --> 00:15:51.140
network this network has a total of
about hundred and thirty-eight million

00:15:51.140 --> 00:15:51.150
about hundred and thirty-eight million
 

00:15:51.150 --> 00:15:53.900
about hundred and thirty-eight million
parameters and that's pretty long even

00:15:53.900 --> 00:15:53.910
parameters and that's pretty long even
 

00:15:53.910 --> 00:15:56.540
parameters and that's pretty long even
by modern standards but the simplicity

00:15:56.540 --> 00:15:56.550
by modern standards but the simplicity
 

00:15:56.550 --> 00:15:59.930
by modern standards but the simplicity
of the vgg 16 architecture made it quite

00:15:59.930 --> 00:15:59.940
of the vgg 16 architecture made it quite
 

00:15:59.940 --> 00:16:02.660
of the vgg 16 architecture made it quite
appealing you can tell this architecture

00:16:02.660 --> 00:16:02.670
appealing you can tell this architecture
 

00:16:02.670 --> 00:16:04.370
appealing you can tell this architecture
is really quite uniform there's a few

00:16:04.370 --> 00:16:04.380
is really quite uniform there's a few
 

00:16:04.380 --> 00:16:06.920
is really quite uniform there's a few
conflicts followed by a pooling layer

00:16:06.920 --> 00:16:06.930
conflicts followed by a pooling layer
 

00:16:06.930 --> 00:16:09.710
conflicts followed by a pooling layer
which reduces the height and width right

00:16:09.710 --> 00:16:09.720
which reduces the height and width right
 

00:16:09.720 --> 00:16:12.380
which reduces the height and width right
so the pooling layers reduce the height

00:16:12.380 --> 00:16:12.390
so the pooling layers reduce the height
 

00:16:12.390 --> 00:16:15.200
so the pooling layers reduce the height
and width you have a few of them here

00:16:15.200 --> 00:16:15.210
and width you have a few of them here
 

00:16:15.210 --> 00:16:18.019
and width you have a few of them here
but and also if you look at the number

00:16:18.019 --> 00:16:18.029
but and also if you look at the number
 

00:16:18.029 --> 00:16:20.720
but and also if you look at the number
of filters in the conflicts here you

00:16:20.720 --> 00:16:20.730
of filters in the conflicts here you
 

00:16:20.730 --> 00:16:23.810
of filters in the conflicts here you
have 64 filters and then you double to

00:16:23.810 --> 00:16:23.820
have 64 filters and then you double to
 

00:16:23.820 --> 00:16:27.790
have 64 filters and then you double to
one to eight double to 256 double to 512

00:16:27.790 --> 00:16:27.800
one to eight double to 256 double to 512
 

00:16:27.800 --> 00:16:30.500
one to eight double to 256 double to 512
and then I guess the authors thought 5-2

00:16:30.500 --> 00:16:30.510
and then I guess the authors thought 5-2
 

00:16:30.510 --> 00:16:32.300
and then I guess the authors thought 5-2
was big enough and didn't jump over the

00:16:32.300 --> 00:16:32.310
was big enough and didn't jump over the
 

00:16:32.310 --> 00:16:34.400
was big enough and didn't jump over the
game here but this you know so they're

00:16:34.400 --> 00:16:34.410
game here but this you know so they're
 

00:16:34.410 --> 00:16:36.290
game here but this you know so they're
roughly doubling on every step or

00:16:36.290 --> 00:16:36.300
roughly doubling on every step or
 

00:16:36.300 --> 00:16:39.050
roughly doubling on every step or
doubling through every stack of

00:16:39.050 --> 00:16:39.060
doubling through every stack of
 

00:16:39.060 --> 00:16:41.540
doubling through every stack of
conflicts was another simple principle

00:16:41.540 --> 00:16:41.550
conflicts was another simple principle
 

00:16:41.550 --> 00:16:43.640
conflicts was another simple principle
used to design the architecture of this

00:16:43.640 --> 00:16:43.650
used to design the architecture of this
 

00:16:43.650 --> 00:16:46.940
used to design the architecture of this
network and so I think the relative

00:16:46.940 --> 00:16:46.950
network and so I think the relative
 

00:16:46.950 --> 00:16:49.910
network and so I think the relative
uniformity of this architecture made it

00:16:49.910 --> 00:16:49.920
uniformity of this architecture made it
 

00:16:49.920 --> 00:16:52.760
uniformity of this architecture made it
quite attractive to researchers the main

00:16:52.760 --> 00:16:52.770
quite attractive to researchers the main
 

00:16:52.770 --> 00:16:55.250
quite attractive to researchers the main
downside was that it was a pretty large

00:16:55.250 --> 00:16:55.260
downside was that it was a pretty large
 

00:16:55.260 --> 00:16:56.750
downside was that it was a pretty large
network in terms in the number of

00:16:56.750 --> 00:16:56.760
network in terms in the number of
 

00:16:56.760 --> 00:16:59.900
network in terms in the number of
parameters you had to Train oh and if

00:16:59.900 --> 00:16:59.910
parameters you had to Train oh and if
 

00:16:59.910 --> 00:17:01.820
parameters you had to Train oh and if
you read the literature you sometimes

00:17:01.820 --> 00:17:01.830
you read the literature you sometimes
 

00:17:01.830 --> 00:17:04.819
you read the literature you sometimes
see people talk about the GG 19 there's

00:17:04.819 --> 00:17:04.829
see people talk about the GG 19 there's
 

00:17:04.829 --> 00:17:08.120
see people talk about the GG 19 there's
an even bigger version of this network

00:17:08.120 --> 00:17:08.130
an even bigger version of this network
 

00:17:08.130 --> 00:17:11.090
an even bigger version of this network
and you can see the details in the paper

00:17:11.090 --> 00:17:11.100
and you can see the details in the paper
 

00:17:11.100 --> 00:17:14.000
and you can see the details in the paper
and cited at the bottom by karen simeon

00:17:14.000 --> 00:17:14.010
and cited at the bottom by karen simeon
 

00:17:14.010 --> 00:17:18.230
and cited at the bottom by karen simeon
and andrew zisserman but because VG 16

00:17:18.230 --> 00:17:18.240
and andrew zisserman but because VG 16
 

00:17:18.240 --> 00:17:21.140
and andrew zisserman but because VG 16
does almost as well as the GG 19 a lot

00:17:21.140 --> 00:17:21.150
does almost as well as the GG 19 a lot
 

00:17:21.150 --> 00:17:24.350
does almost as well as the GG 19 a lot
of people will use eg 16 but the thing I

00:17:24.350 --> 00:17:24.360
of people will use eg 16 but the thing I
 

00:17:24.360 --> 00:17:26.510
of people will use eg 16 but the thing I
liked most about this was that this made

00:17:26.510 --> 00:17:26.520
liked most about this was that this made
 

00:17:26.520 --> 00:17:27.400
liked most about this was that this made
this

00:17:27.400 --> 00:17:27.410
this
 

00:17:27.410 --> 00:17:29.320
this
patent of how as you go deeper

00:17:29.320 --> 00:17:29.330
patent of how as you go deeper
 

00:17:29.330 --> 00:17:31.750
patent of how as you go deeper
heightened Grif goes down it just goes

00:17:31.750 --> 00:17:31.760
heightened Grif goes down it just goes
 

00:17:31.760 --> 00:17:33.370
heightened Grif goes down it just goes
down by a factor of two each time for

00:17:33.370 --> 00:17:33.380
down by a factor of two each time for
 

00:17:33.380 --> 00:17:35.440
down by a factor of two each time for
the pooling layers whereas the number of

00:17:35.440 --> 00:17:35.450
the pooling layers whereas the number of
 

00:17:35.450 --> 00:17:38.290
the pooling layers whereas the number of
channels increases and here it you know

00:17:38.290 --> 00:17:38.300
channels increases and here it you know
 

00:17:38.300 --> 00:17:40.810
channels increases and here it you know
roughly goes up by a factor of two every

00:17:40.810 --> 00:17:40.820
roughly goes up by a factor of two every
 

00:17:40.820 --> 00:17:43.150
roughly goes up by a factor of two every
time you have a new set of confluence so

00:17:43.150 --> 00:17:43.160
time you have a new set of confluence so
 

00:17:43.160 --> 00:17:46.810
time you have a new set of confluence so
by making the rate at which these goes

00:17:46.810 --> 00:17:46.820
by making the rate at which these goes
 

00:17:46.820 --> 00:17:49.060
by making the rate at which these goes
down and that go up very easy semantics

00:17:49.060 --> 00:17:49.070
down and that go up very easy semantics
 

00:17:49.070 --> 00:17:52.090
down and that go up very easy semantics
I thought this paper was very very

00:17:52.090 --> 00:17:52.100
I thought this paper was very very
 

00:17:52.100 --> 00:17:54.270
I thought this paper was very very
attractive from that perspective so

00:17:54.270 --> 00:17:54.280
attractive from that perspective so
 

00:17:54.280 --> 00:17:56.740
attractive from that perspective so
that's it for the three classic

00:17:56.740 --> 00:17:56.750
that's it for the three classic
 

00:17:56.750 --> 00:17:58.660
that's it for the three classic
architectures if you want you should

00:17:58.660 --> 00:17:58.670
architectures if you want you should
 

00:17:58.670 --> 00:18:01.000
architectures if you want you should
even now meet some of these papers I

00:18:01.000 --> 00:18:01.010
even now meet some of these papers I
 

00:18:01.010 --> 00:18:02.590
even now meet some of these papers I
recommend starting with the Alex net

00:18:02.590 --> 00:18:02.600
recommend starting with the Alex net
 

00:18:02.600 --> 00:18:05.200
recommend starting with the Alex net
paper followed by the vgg net paper and

00:18:05.200 --> 00:18:05.210
paper followed by the vgg net paper and
 

00:18:05.210 --> 00:18:07.360
paper followed by the vgg net paper and
then the Linette paper is a bit harder

00:18:07.360 --> 00:18:07.370
then the Linette paper is a bit harder
 

00:18:07.370 --> 00:18:09.010
then the Linette paper is a bit harder
to read but it is a good classic let's

00:18:09.010 --> 00:18:09.020
to read but it is a good classic let's
 

00:18:09.020 --> 00:18:11.410
to read but it is a good classic let's
take a look at that but Nick's let's go

00:18:11.410 --> 00:18:11.420
take a look at that but Nick's let's go
 

00:18:11.420 --> 00:18:13.240
take a look at that but Nick's let's go
beyond these classic networks and look

00:18:13.240 --> 00:18:13.250
beyond these classic networks and look
 

00:18:13.250 --> 00:18:15.340
beyond these classic networks and look
at some even more advanced even more

00:18:15.340 --> 00:18:15.350
at some even more advanced even more
 

00:18:15.350 --> 00:18:17.080
at some even more advanced even more
powerful neural network architectures

00:18:17.080 --> 00:18:17.090
powerful neural network architectures
 

00:18:17.090 --> 00:18:20.650
powerful neural network architectures
let's go onto the next video

