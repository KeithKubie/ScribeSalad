WEBVTT

1
00:00:00.730 --> 00:00:04.260
You are listening to radiolab radio from

2
00:00:05.790 --> 00:00:06.623
W. N y.

3
00:00:11.790 --> 00:00:12.623
S. E. N. N. P. R. Hi there.

4
00:00:12.690 --> 00:00:15.720
We're going to start today's program with a fellow named Robert.

5
00:00:15.810 --> 00:00:20.460
Is it Epstein or Epstein? Uh, just think Einstein with an app. Okay.

6
00:00:20.820 --> 00:00:22.860
That would make an Epstein, I guess that's right.

7
00:00:22.890 --> 00:00:26.910
And where are we reaching you right now? I am in a, the San Diego area.

8
00:00:27.090 --> 00:00:29.040
Robert Epstein is a psychologist,

9
00:00:29.270 --> 00:00:31.440
our editor in chief of psychology today magazine.

10
00:00:31.441 --> 00:00:33.870
He's written a ton of books on relationships and love.

11
00:00:33.900 --> 00:00:38.900
And he also happens to be one of the world's leading researchers in computer

12
00:00:39.691 --> 00:00:44.040
human interactions, like artificial intelligence, basically. That is correct,

13
00:00:44.041 --> 00:00:44.874
yes.

14
00:00:48.060 --> 00:00:51.960
So when did you decide to go onto the computer to get a date?

15
00:00:53.760 --> 00:00:58.470
2006 maybe. Why do you ask? Oh,

16
00:00:58.471 --> 00:01:02.280
no reason. No. What happened? You,

17
00:01:02.281 --> 00:01:06.120
you or you had gotten divorced? Yeah, I was single at the time. Yeah,

18
00:01:06.121 --> 00:01:06.954
I was divorced.

19
00:01:07.740 --> 00:01:12.150
And you decided that you'd try loving all the right places or what? Oh, sure.

20
00:01:12.330 --> 00:01:13.800
Bill online dating of everyone was doing it.

21
00:01:13.801 --> 00:01:17.700
My cousin actually convinced me to try it. So I did

22
00:01:20.480 --> 00:01:24.230
went online and I looked at photos and I looked at profiles and uh, you know,

23
00:01:24.231 --> 00:01:28.880
and I communicated with various people who, who, who were willing to talk to me.

24
00:01:30.680 --> 00:01:33.700
Yeah. One of the women I was communicating with, uh,

25
00:01:33.760 --> 00:01:37.480
lived in southern California where I do so I thought that's great cause you

26
00:01:37.481 --> 00:01:41.530
know, you want someone to be nearby. And she had a very attractive photo online

27
00:01:43.250 --> 00:01:47.660
and her English was poor, which at first bothered me. And then she said,

28
00:01:47.661 --> 00:01:50.840
well she's not really in California. She's really in Russia. Oh.

29
00:01:50.900 --> 00:01:55.190
But all four of my grandparents came from Russia. So I thought,

30
00:01:55.191 --> 00:01:56.024
well I'll go with it.

31
00:01:58.180 --> 00:02:01.780
So I continued to write to hurt. I suites the Atlanta.

32
00:02:01.990 --> 00:02:04.930
It's really warned you now. And I've been doing a lot of swimming.

33
00:02:05.500 --> 00:02:07.840
I've also been writing and doing the computer programming.

34
00:02:07.900 --> 00:02:11.620
She wrote back to me and very poor English. Hello Dan Rollback. Do you mind?

35
00:02:11.621 --> 00:02:14.190
I hate procedure later. I in theory can't be.

36
00:02:14.350 --> 00:02:18.670
I remember that she liked to walk in park and tell him to walk with the

37
00:02:18.671 --> 00:02:20.980
girlfriend and the bent and walked in.

38
00:02:21.270 --> 00:02:25.810
[inaudible] telling me about her family and her mom. Let me about youth today.

39
00:02:26.170 --> 00:02:29.470
MV spoke much and long time. They lived in a small apartment.

40
00:02:29.680 --> 00:02:31.600
I knew where in Russia they lived.

41
00:02:34.260 --> 00:02:36.460
It felt like we were bonding for sure. Hello.

42
00:02:36.610 --> 00:02:41.320
I might be able to come to Moscow on Sunday, April 15th departing Thursday,

43
00:02:41.321 --> 00:02:45.760
April 19th with love Robert. No, so it was getting serious. Oh yeah,

44
00:02:45.820 --> 00:02:48.270
of course. Then what happened? Well,

45
00:02:48.271 --> 00:02:51.480
two months passed and I began to feel uncomfortable.

46
00:02:51.810 --> 00:02:55.910
Something wasn't right. There were no phone calls.

47
00:02:57.310 --> 00:03:01.030
[inaudible] some point I began to suggest phone call, but there weren't any,

48
00:03:01.300 --> 00:03:05.860
but the main problem was I would say the something like,

49
00:03:05.890 --> 00:03:10.570
did you get my letter about me coming to Moscow in April or tell me more about

50
00:03:10.690 --> 00:03:15.190
this friend of yours that you mentioned and she did not, didn't mind.

51
00:03:15.220 --> 00:03:18.220
I'm very glad to your letter. She did not,

52
00:03:18.221 --> 00:03:21.040
she was still replying with fairly long emails.

53
00:03:21.620 --> 00:03:23.830
Find vetted my CT very bad,

54
00:03:24.580 --> 00:03:26.650
but they were kind of rambling in general.

55
00:03:26.680 --> 00:03:31.120
I think of you all days much and I very much want to see more likely you.

56
00:03:31.180 --> 00:03:34.390
I already gave you some dates for our visit to Moscow, my love.

57
00:03:34.570 --> 00:03:35.800
What do you think about that?

58
00:03:36.390 --> 00:03:41.390
Then at some point a little bell went off in my head finally and I started to

59
00:03:41.521 --> 00:03:44.090
send some emails, which uh,

60
00:03:44.430 --> 00:03:49.110
let's say included random alphabet letters. So you say, how,

61
00:03:49.111 --> 00:03:53.610
what are you wearing tonight? Are you wearing a DBG, g, G, l, p?

62
00:03:53.670 --> 00:03:58.670
Exactly. And it didn't make any difference. Halo,

63
00:03:58.800 --> 00:04:01.750
do you have Robert? Your letters do mean very hippy.

64
00:04:01.751 --> 00:04:05.230
When I open a letter box and that's when I realized

65
00:04:06.880 --> 00:04:10.320
Ivana was not a person. Ivana was a computer program

66
00:04:12.290 --> 00:04:16.820
I had been had. Wow. So what did you think?

67
00:04:17.360 --> 00:04:19.820
I felt like a fool. I felt like an incredible fool,

68
00:04:20.060 --> 00:04:25.060
especially given my background that I had been fooled that long.

69
00:04:25.460 --> 00:04:29.210
Now I can tell you now this is, this is something I've never made public about.

70
00:04:29.930 --> 00:04:31.130
The other example.

71
00:04:32.120 --> 00:04:35.360
Robert went on to tell us that not long after that first incident he was

72
00:04:35.390 --> 00:04:40.230
corresponding with someone woman I thought who also turned out to be a robot and

73
00:04:40.231 --> 00:04:45.231
he discovered at this time because a programmer contacted me from the UK and

74
00:04:46.201 --> 00:04:48.180
said, I know who you are.

75
00:04:48.240 --> 00:04:52.950
You have not been communicating with a person you've been communicating with a

76
00:04:52.951 --> 00:04:56.580
chat Bot. You've been now undressed twice by robots so to speak.

77
00:04:56.610 --> 00:05:01.100
Well and maybe more than twice. Well, how common do you think this is?

78
00:05:01.101 --> 00:05:04.700
Do you think that match.com and all those places are like swarming with these

79
00:05:04.730 --> 00:05:06.950
bots? You know, you know, I bet you they are

80
00:05:09.890 --> 00:05:11.300
[inaudible] that's what you have to understand.

81
00:05:11.301 --> 00:05:13.930
There are hundreds of these things out there. There might be thousands.

82
00:05:16.610 --> 00:05:17.443
That's what's coming

83
00:05:20.610 --> 00:05:25.300
through in a world like this. We are surrounded by part of the short life.

84
00:05:25.580 --> 00:05:29.780
You look like things can get a little confusing. In fact,

85
00:05:29.781 --> 00:05:33.050
we're going to do a whole show about that confusion about the sometimes

86
00:05:33.200 --> 00:05:34.010
peculiar,

87
00:05:34.010 --> 00:05:38.560
sometimes strange things that can happen when humans and machines collide.

88
00:05:39.160 --> 00:05:43.930
Collide, but don't quite know who's on what side of the road. Yeah,

89
00:05:44.190 --> 00:05:46.780
I don't know. Jet. Boom. Rod. That was good. That was good. Just go with it.

90
00:05:46.790 --> 00:05:50.310
Okay. I'm Robert Krulwich. This is radio lab and we're talking to machines.

91
00:05:51.570 --> 00:05:55.190
Worse. Should send me your credit card.

92
00:06:00.810 --> 00:06:01.643
[inaudible]

93
00:06:05.630 --> 00:06:08.390
<v 2>so keep talking. Just start things off.</v>

94
00:06:08.391 --> 00:06:12.860
Let's introduce you to the person who really hooked us on this whole idea of

95
00:06:12.861 --> 00:06:16.130
human robot Chit Chat. My name is Brian Christian. He's a writer.

96
00:06:16.200 --> 00:06:19.380
Are you Christian religiously? Uh, no man,

97
00:06:19.700 --> 00:06:23.720
that's not at all related to anything that's wrong with you. That's his name,

98
00:06:23.840 --> 00:06:24.291
but didn't know.

99
00:06:24.291 --> 00:06:26.990
What's important is that he wrote a book called the most human human,

100
00:06:27.080 --> 00:06:28.220
which is all about uh,

101
00:06:28.380 --> 00:06:32.150
the confusing things that can happen when people and machines interact.

102
00:06:32.180 --> 00:06:35.120
How did you, this is such a curious thing to get. Yeah.

103
00:06:35.210 --> 00:06:39.350
How did you get into this? I played with Ms dos intently when I was a child.

104
00:06:39.351 --> 00:06:42.980
Yeah, there you go. Yeah. Dos is kind of the early version of windows.

105
00:06:43.010 --> 00:06:45.680
I was programming these sort of rudimentary maze games,

106
00:06:45.740 --> 00:06:47.620
like a curse or going through a maze. Yeah.

107
00:06:47.660 --> 00:06:50.870
Basically did this by any chance mean you did not develop best friends?

108
00:06:51.230 --> 00:06:53.780
A lot of my best friends were also into that. Yeah. Wow.

109
00:06:53.810 --> 00:06:57.560
We were not the coolest, but we had a lot of fun.

110
00:06:57.561 --> 00:07:02.030
So there you are and and you just had a, you just have a talent for this. Yeah.

111
00:07:02.240 --> 00:07:03.890
I don't know what it was. I mean I was just,

112
00:07:03.891 --> 00:07:07.460
there was something I think fascinating to me that that you could take a process

113
00:07:07.461 --> 00:07:08.570
that you knew how to do,

114
00:07:08.660 --> 00:07:11.510
but in breaking it down to steps that were that explicit,

115
00:07:11.660 --> 00:07:16.550
you often learned something about how the process actually works. For me,

116
00:07:16.551 --> 00:07:20.630
programming is surprisingly linked to introspection.

117
00:07:21.230 --> 00:07:24.590
How exactly? Well, you know, if a computer were a person,

118
00:07:24.591 --> 00:07:27.740
you can imagine someone sitting in your living room and you say, you know,

119
00:07:27.741 --> 00:07:31.010
can you hand me that book? And it would say, no,

120
00:07:31.040 --> 00:07:33.770
I can't do that because there's a coffee cup on it. And you say, okay,

121
00:07:33.771 --> 00:07:36.980
well pick up the Coffee Cup and hand me the book. And it says, well,

122
00:07:36.981 --> 00:07:39.860
I can't do that because now I'm holding the cup. And you say, okay,

123
00:07:39.861 --> 00:07:44.760
put down the cup, then pick up the book and what you quickly learned,

124
00:07:45.180 --> 00:07:45.270
<v 1>Brian,</v>

125
00:07:45.270 --> 00:07:50.270
is it even really simple human behaviors are made up of a thousand sub routines.

126
00:07:51.190 --> 00:07:52.450
I mean, if you really think about it,

127
00:07:52.451 --> 00:07:56.200
the book task requires knowing what is a book you have to learn how to about

128
00:07:56.260 --> 00:08:00.400
elbows and wrists, how to grab something. What is a book? I already said that,

129
00:08:00.620 --> 00:08:03.160
oh, you need to know about gravity. If it's a machine,

130
00:08:03.161 --> 00:08:07.030
you have to teach it physics everything in the world in order for it to just

131
00:08:07.031 --> 00:08:09.940
pick up a spoon or buck. I knew that.

132
00:08:11.480 --> 00:08:14.750
<v 2>So now think of that Svetlana bought earlier. Okay.</v>

133
00:08:14.810 --> 00:08:19.430
Trying to make something that could actually mimic human conversation. Kinda,

134
00:08:19.450 --> 00:08:22.850
sorta imagine all this stuff you'd have to throw into that. Okay.

135
00:08:22.910 --> 00:08:27.410
English grammar, syntax and text tone, mood,

136
00:08:27.440 --> 00:08:30.290
sarcasm, higher ne adverbs. Turn taking.

137
00:08:30.690 --> 00:08:33.950
Oh well it's not actually as impossible as you'd imagine.

138
00:08:34.240 --> 00:08:35.270
This is kind of startling.

139
00:08:35.450 --> 00:08:39.250
If you go back to the very early days of software programming in the mid

140
00:08:39.250 --> 00:08:39.300
sixties,

141
00:08:39.300 --> 00:08:44.300
1964 and 1965 this was actually done with a little program called Eliza,

142
00:08:44.870 --> 00:08:48.920
and it was developed by Joseph Weizenbaum at MIT. What in Wiseman bomb's case.

143
00:08:49.190 --> 00:08:51.260
His model was not a Russian hottie instead,

144
00:08:51.261 --> 00:08:55.100
it was a non-directive Rogerian.

145
00:08:56.320 --> 00:08:59.790
The one surface, it's a particular school of therapy.

146
00:08:59.940 --> 00:09:03.300
The kind of where the therapist basically mirrors what you're saying,

147
00:09:03.301 --> 00:09:06.000
what you're saying, like you're saying, this is Sherry Turkle's,

148
00:09:06.001 --> 00:09:09.510
she's an anthropologist at the Massachusetts Institute of Technology and she

149
00:09:09.511 --> 00:09:13.410
worked with joeys and Bam. Where's it Weizenbaum? It's Weizenbaum at MIT.

150
00:09:13.440 --> 00:09:17.130
So if you say, you know, I, I'm feeling depressed. A therapist says,

151
00:09:17.131 --> 00:09:20.070
I'm sorry to hear you're feeling depressed. Tell me more. Um,

152
00:09:20.100 --> 00:09:21.870
Joseph Weizenbaum decides, you know,

153
00:09:21.871 --> 00:09:25.980
I think that's an easy enough type of conversation that I can program that into

154
00:09:25.981 --> 00:09:28.800
my computer. And so he writes up a simple little program,

155
00:09:28.830 --> 00:09:31.080
just about a hundred lines of code,

156
00:09:31.500 --> 00:09:33.870
which does sort of what your therapist does,

157
00:09:33.871 --> 00:09:37.860
where it looks for a keyword and what you're saying as an I'm feeling depressed,

158
00:09:37.890 --> 00:09:41.160
keyword, depressed, latches onto it, and then basically flips it back to you.

159
00:09:41.220 --> 00:09:43.920
I'm sorry to hear that you're feeling keyword depressed, right?

160
00:09:43.990 --> 00:09:47.280
It's basically a program that inverts your words and it's, it's a,

161
00:09:47.281 --> 00:09:49.860
it's a language game. So here's what he did.

162
00:09:49.861 --> 00:09:52.800
He put a computer on a desk running this little program and then he invited

163
00:09:52.801 --> 00:09:55.500
people to sit down and talk to it.

164
00:09:55.530 --> 00:09:58.980
They'd write in something and it would spit back the keyword and then you would

165
00:09:58.981 --> 00:10:02.010
then repeat and on and on. You wouldn't think this would be very interesting,

166
00:10:02.011 --> 00:10:05.490
right? Nope. But he found that his students want,

167
00:10:05.491 --> 00:10:10.320
who knew that the program didn't know or understand who had helped him write the

168
00:10:10.840 --> 00:10:15.150
program. Sophisticated students nevertheless wanted to be alone with it. Wow.

169
00:10:15.151 --> 00:10:16.950
They actually helped them write the program.

170
00:10:17.220 --> 00:10:20.700
I can read you if you like one of the conversations. Yes, please.

171
00:10:20.910 --> 00:10:25.530
This is an actual transcript between Eliza and a young woman. Men are all alike.

172
00:10:25.950 --> 00:10:29.010
In what way? They're always bugging us about something or other.

173
00:10:29.340 --> 00:10:30.173
Can you think of this?

174
00:10:31.250 --> 00:10:34.520
<v 3>Well, my boyfriend made me come here. Your boyfriend made you come here.</v>

175
00:10:34.760 --> 00:10:38.540
She says I'm depressed much of the time. I'm so sorry to hear you are depressed.

176
00:10:39.050 --> 00:10:40.910
It's true. How am I unhappy?

177
00:10:40.970 --> 00:10:43.380
Do you think coming here will help you not to be unhappy? Oh,

178
00:10:43.390 --> 00:10:45.950
I need some help that much team certain.

179
00:10:45.951 --> 00:10:47.660
What would it mean to you if you got some help?

180
00:10:47.980 --> 00:10:50.120
Perhaps I could learn to get along with my mother.

181
00:10:50.150 --> 00:10:51.160
Tell me more about your family.

182
00:10:51.790 --> 00:10:56.320
<v 2>Wow. And so it's this vacuum go on for hours. Well, and in fact it did.</v>

183
00:10:56.370 --> 00:11:00.450
<v 3>My mother takes care of me most in your family takes care of you. My father,</v>

184
00:11:00.710 --> 00:11:03.570
his father. You're like my father in some ways. You see,

185
00:11:04.110 --> 00:11:07.260
while you're not very aggressive, what makes you think I'm not very aggressive?

186
00:11:07.320 --> 00:11:10.120
You don't argue with me. Why do you think I don't argue with you?

187
00:11:10.150 --> 00:11:11.700
You are afraid of me.

188
00:11:12.280 --> 00:11:15.570
Leads you to believe that there's a woman in this transcript messing around or

189
00:11:15.571 --> 00:11:16.800
is she really pouring her heart out?

190
00:11:16.860 --> 00:11:19.530
We know that this is a woman who works for Joe Weizenbaum.

191
00:11:19.740 --> 00:11:22.620
She's sitting in the office and she just can't stop talking to it.

192
00:11:22.621 --> 00:11:23.550
That's all we know.

193
00:11:23.580 --> 00:11:27.240
What else comes to mind when you think of your father bullies and why's your mom

194
00:11:27.241 --> 00:11:29.510
is watching all this and he,

195
00:11:29.860 --> 00:11:33.390
he first thought it was funny and then he didn't think it was funny because they

196
00:11:33.391 --> 00:11:35.280
were actually having conversations with it.

197
00:11:36.020 --> 00:11:41.020
<v 2>One day he comes into the office and his secretary is on the computer divulging</v>

198
00:11:41.511 --> 00:11:43.970
her life story to it. According to Weizenbaum,

199
00:11:43.971 --> 00:11:47.840
she even told him to please leave the room so she could be alone with it and

200
00:11:47.841 --> 00:11:52.520
talk to it. And he, he, um, he was very upset.

201
00:11:54.580 --> 00:11:55.480
<v 4>Nevertheless,</v>

202
00:11:55.540 --> 00:11:57.370
<v 0>when word about Eliza got out,</v>

203
00:11:57.690 --> 00:12:02.580
<v 2>the medical community sort of latches onto it really and says, oh,</v>

204
00:12:02.581 --> 00:12:04.410
this is going to be the next revolution in therapy.

205
00:12:04.560 --> 00:12:07.770
<v 5>Pick something new and promising in the field of psychotherapy.</v>

206
00:12:07.800 --> 00:12:09.720
<v 2>This is from a newscast around that time,</v>

207
00:12:09.810 --> 00:12:12.330
therapists in like phone booths and cities.

208
00:12:12.331 --> 00:12:15.840
And you're going to walk in and put a quarter in the slot and have, you know,

209
00:12:15.841 --> 00:12:18.240
half an hour of therapy with this automatic program.

210
00:12:18.490 --> 00:12:22.710
<v 5>Either time can be rented for $5 an hour and that's every reason to suspect that</v>

211
00:12:22.760 --> 00:12:24.000
it will go down significantly.

212
00:12:24.300 --> 00:12:27.810
<v 2>People really thought that they were going to replace therapists with computers.</v>

213
00:12:28.440 --> 00:12:30.690
Absolutely. Really did. Absolutely. Yeah.

214
00:12:31.020 --> 00:12:36.020
And it was just this really appalling moment for Weizenbaum of there's

215
00:12:37.501 --> 00:12:41.310
something, the genie is out of the bottle,

216
00:12:41.311 --> 00:12:46.290
maybe in a in a bad way, and he does this one ATF his entire career,

217
00:12:46.320 --> 00:12:47.910
so he pulls the plug on the program,

218
00:12:47.911 --> 00:12:52.911
he cuts the funding and he goes from being one of the main advocates for

219
00:12:52.951 --> 00:12:57.951
artificial intelligence to basically committing the rest of his career to

220
00:12:58.350 --> 00:13:00.690
fighting against artificial intelligence.

221
00:13:01.160 --> 00:13:04.820
<v 0>Rude to admit and sign in for COPD for your [inaudible].</v>

222
00:13:05.240 --> 00:13:10.240
This is Joseph Weizenbaum interviewed in German just before he died in 2008 it

223
00:13:10.491 --> 00:13:13.390
was on the German documentary plug and pray. Mine,

224
00:13:13.770 --> 00:13:18.430
Cowen [inaudible] is my main objection. He's a Venda sting.

225
00:13:18.680 --> 00:13:19.431
The thing says,

226
00:13:19.431 --> 00:13:23.030
I understand that if somebody typed in something and the machine says,

227
00:13:23.060 --> 00:13:26.360
I understand that is talk new man, Duh,

228
00:13:26.510 --> 00:13:29.740
there's no one there as a high Luger.

229
00:13:29.960 --> 00:13:32.560
So it's a lie niche when, uh,

230
00:13:32.900 --> 00:13:35.870
each can be a niche flush down and I can't imagine that people who are

231
00:13:35.871 --> 00:13:40.871
emotionally imbalanced could be effectively treated by systematic email lines to

232
00:13:41.450 --> 00:13:42.283
message undo.

233
00:13:42.340 --> 00:13:47.340
<v 4>I must say that my reaction to the Eliza program at the time was to try to</v>

234
00:13:47.441 --> 00:13:52.441
reassure him at the time what I thought people were doing was using it as a kind

235
00:13:53.471 --> 00:13:57.370
of interactive diary, knowing that it was a machine,

236
00:13:57.700 --> 00:14:02.700
but using it as an occasion to breathe life into it in order to get their

237
00:14:03.611 --> 00:14:04.444
feelings out.

238
00:14:08.830 --> 00:14:12.060
<v 3>I think she's right to have said that to him too. Yeah,</v>

239
00:14:12.070 --> 00:14:15.950
because he says it's a lie. Well, it is a lie.

240
00:14:16.490 --> 00:14:20.500
How the machine can't love anything. Yes. And if you are a sensible human being,

241
00:14:20.501 --> 00:14:22.900
you know that and he's sitting right there on the desk.

242
00:14:22.901 --> 00:14:23.950
It's not pretending well,

243
00:14:23.951 --> 00:14:26.950
these are sensible human beings that we're already a little bit seduced.

244
00:14:26.951 --> 00:14:28.660
Let Matt just go forward a hundred years.

245
00:14:28.810 --> 00:14:33.130
Imagine a machine that is very sophisticated, very fluent,

246
00:14:33.131 --> 00:14:36.080
very convincingly human. I'm talking about blade runner basically. Yeah,

247
00:14:36.100 --> 00:14:36.910
exactly.

248
00:14:36.910 --> 00:14:41.910
At that point I think I would require some kind of label to remind me that this

249
00:14:43.090 --> 00:14:43.470
is [inaudible]

250
00:14:43.470 --> 00:14:46.290
<v 4>the thing. It's not a being, it's just a thing. Okay.</v>

251
00:14:46.291 --> 00:14:50.430
But if here's something to think about. If the machines get to that point,

252
00:14:50.580 --> 00:14:52.100
which is the big where you'd

253
00:14:52.290 --> 00:14:53.150
<v 2>want to label them, well,</v>

254
00:14:53.151 --> 00:14:58.070
you're going to need a way to know when they've crossed that line and become

255
00:14:59.900 --> 00:15:04.460
mindful. Yeah. So I should back up for a sec and say that in 1950,

256
00:15:04.980 --> 00:15:04.981
they're,

257
00:15:04.981 --> 00:15:08.630
they're just starting to develop the computer and they're already asking these

258
00:15:08.631 --> 00:15:12.410
philosophical questions like, can these machines think, you know,

259
00:15:12.560 --> 00:15:17.000
will be someday be able to make a machine that could think, uh, and if we did,

260
00:15:17.001 --> 00:15:17.834
how would we know?

261
00:15:18.140 --> 00:15:22.940
And so a British mathematician named Alan Turing proposed a simple thought

262
00:15:22.941 --> 00:15:26.180
experiment. Here's how we'll know when the machines make it across the line.

263
00:15:26.210 --> 00:15:28.430
Get a person, sit him down at a computer,

264
00:15:28.550 --> 00:15:32.960
have them start a conversation and text, hi, how are you? Enter good.

265
00:15:32.961 --> 00:15:35.340
Pops up on the screen. Sort of like Internet chat. Yep.

266
00:15:35.390 --> 00:15:38.630
So after that first conversation, have them do it again and then again,

267
00:15:38.631 --> 00:15:41.190
you know, hi, hello, how are you? Et cetera. [inaudible] back and forth.

268
00:15:41.220 --> 00:15:43.990
Then again, right over and over. But here's the catch.

269
00:15:44.020 --> 00:15:46.600
Half of these conversations will be with real people.

270
00:15:47.110 --> 00:15:51.520
Half will be with these computer programs that are basically impersonating

271
00:15:51.521 --> 00:15:53.500
people and the person in the seat,

272
00:15:53.501 --> 00:15:56.580
the human has to judge which of the conversations were with people,

273
00:15:56.581 --> 00:15:57.630
which were with humans.

274
00:15:58.020 --> 00:16:02.490
Turing's idea was that if those computer fakes could fool the human judge a

275
00:16:02.491 --> 00:16:06.750
certain percentage of the time Turing's magic threshold was 30% then at that

276
00:16:06.751 --> 00:16:09.750
point we can basically consider machines intelligent. Cause you know,

277
00:16:09.870 --> 00:16:11.820
if you can't tell the machine isn't human,

278
00:16:11.940 --> 00:16:15.000
then you can't say it's not intelligent. Yeah. That's basically, yeah.

279
00:16:15.030 --> 00:16:16.790
You said 30% of the time. Yeah.

280
00:16:17.360 --> 00:16:19.260
Touring is the natural number to me would be half. You know,

281
00:16:19.350 --> 00:16:22.620
51% would seem to be like the coaching moment, right?

282
00:16:22.650 --> 00:16:27.650
30% oh well 51% is actually a horrifying number in the context of the touring

283
00:16:28.441 --> 00:16:32.070
test because you've got these two conversations and you're trying to decide

284
00:16:32.071 --> 00:16:35.460
which is the real person. So if the computer were indistinguishable,

285
00:16:35.670 --> 00:16:38.940
that would be 50% you know the judge is doing no better than chance.

286
00:16:38.970 --> 00:16:43.970
So if a computer hits 51% that means they're they've out human to the humility

287
00:16:44.300 --> 00:16:48.140
of that is horrifying. Now,

288
00:16:48.780 --> 00:16:51.210
something to keep in mind when touring thought this whole thing up,

289
00:16:51.240 --> 00:16:53.400
the technology was so new.

290
00:16:53.520 --> 00:16:57.810
Computers barely existed that it was sort of a leap of imagination really.

291
00:16:57.840 --> 00:17:01.450
But no longer Robert Bring it. Can you give me like some excitement music here?

292
00:17:01.560 --> 00:17:03.900
<v 7>Absolutely good. Because every year</v>

293
00:17:06.360 --> 00:17:09.840
the greatest technologist on the planet, hi,

294
00:17:09.841 --> 00:17:13.410
I'm really copping to meet in a small room with folding chairs.

295
00:17:13.550 --> 00:17:18.550
I developed in Java and put Alan Turing's questions to the ultimate test.

296
00:17:23.230 --> 00:17:23.770
<v 2>It really,</v>

297
00:17:23.770 --> 00:17:27.970
it's just a couple of dudes you know who haven't seen the sun in 10 years in a

298
00:17:27.971 --> 00:17:32.020
room, but we do now have this thing called the lobe enterprise,

299
00:17:32.021 --> 00:17:35.560
which is essentially a yearly actual touring test.

300
00:17:35.800 --> 00:17:40.390
<v 7>Judge judges' table is going to be communicating with two entities,</v>

301
00:17:40.450 --> 00:17:42.780
one Schumann and one trainer.

302
00:17:42.960 --> 00:17:46.530
<v 2>The way the stage is set up is you've got the judges at a table on the left on</v>

303
00:17:46.531 --> 00:17:47.740
laptops, Ben,

304
00:17:47.760 --> 00:17:52.590
a bunch of giant server looking machines in the middle that programmers are

305
00:17:52.591 --> 00:17:56.640
fiddling with and then there's a curtain on the right hand side and we're behind

306
00:17:56.641 --> 00:17:57.474
the curtain.

307
00:17:57.920 --> 00:18:02.250
Brian actually participated in the 2009 Lobe Nour prize competition but not as a

308
00:18:02.251 --> 00:18:05.730
programmer as one of the four quote confederates,

309
00:18:05.731 --> 00:18:09.390
the confederates are the real people that the judges are talking to.

310
00:18:09.590 --> 00:18:12.240
Cause remember a half the conversations that have are with people,

311
00:18:12.241 --> 00:18:15.660
half are with computers and then Brian decided to participate that year because

312
00:18:15.661 --> 00:18:18.570
the year before 2008 the top program minister fool,

313
00:18:18.630 --> 00:18:22.590
25% of the judging panel pretty close to Turing's number. Exactly.

314
00:18:22.620 --> 00:18:26.520
One vote away. And so I felt to some extent,

315
00:18:26.970 --> 00:18:31.500
how can I get involved on behalf of humanity?

316
00:18:31.501 --> 00:18:35.580
How can I screw it up? Take a stand by this position for you.

317
00:18:36.320 --> 00:18:41.180
All right, machines, please hold. Your places are now representing all Human

318
00:18:42.860 --> 00:18:46.950
Brian, Chris Christian. Now in terms of what Bryan is up against,

319
00:18:46.951 --> 00:18:50.660
the computer programs have a variety of different strategies. For example,

320
00:18:50.661 --> 00:18:53.540
there was one program Brian's year that would do kind of a double fake out

321
00:18:54.110 --> 00:18:56.030
[inaudible] where it would pretend not to be a person,

322
00:18:56.180 --> 00:19:00.740
but a person who is sarcastically pretending to be a robot. Oh,

323
00:19:00.770 --> 00:19:03.590
people would ask it a simple question and it would say,

324
00:19:03.950 --> 00:19:07.400
I don't have enough ram to answer that question. Smiley face,

325
00:19:07.880 --> 00:19:11.870
and everyone would be like, oh, this is such a wise guy. [inaudible] clips.

326
00:19:13.190 --> 00:19:17.940
I want to tell you now about one particular Bot that competed. Brian's here.

327
00:19:18.140 --> 00:19:20.210
Hi, I'm really carpenter. That's the guy who made it.

328
00:19:20.211 --> 00:19:22.640
My program is called clever bots and that's the BOT.

329
00:19:22.910 --> 00:19:26.870
This is a program that employs a very 20 minutes, spooky, spooky, the right,

330
00:19:27.070 --> 00:19:28.460
right. A very spooky strategy.

331
00:19:28.600 --> 00:19:33.600
<v 4>You may be surprised to hear that despite the fact that it's called Clever Bot,</v>

332
00:19:33.940 --> 00:19:35.410
it states that it is a Bot.

333
00:19:35.411 --> 00:19:38.020
It states that it is never a human right there in front of them.

334
00:19:38.110 --> 00:19:39.340
Despite those facts.

335
00:19:39.550 --> 00:19:44.550
I received several emails a day from people who believe that actually they are

336
00:19:44.951 --> 00:19:46.090
being connected to humans.

337
00:19:47.720 --> 00:19:48.120
<v 8>[inaudible]</v>

338
00:19:48.120 --> 00:19:50.700
<v 4>oh, like they think they've been tricked. Yes.</v>

339
00:19:50.970 --> 00:19:54.030
Tricked into coming to a site that claims to be a Bot when in fact they're

340
00:19:54.031 --> 00:19:54.864
talking to humans.

341
00:19:56.170 --> 00:19:56.760
<v 8>[inaudible]</v>

342
00:19:56.760 --> 00:19:57.260
<v 4>but no,</v>

343
00:19:57.260 --> 00:20:02.260
no program could possibly respond in this way and there is a certain element of

344
00:20:02.541 --> 00:20:05.180
truth in that to explain Rolo carb mature,

345
00:20:05.240 --> 00:20:08.870
like Brian was one of those kids who is completely obsessed by computers.

346
00:20:08.900 --> 00:20:13.790
I was indeed a computer kid and when he was just a teenager age about 16 or so,

347
00:20:13.850 --> 00:20:17.720
wrote his first Chat Bot, I created a program that talked to me. No kidding.

348
00:20:17.750 --> 00:20:21.040
Yes. You typed in something and it would say something back though.

349
00:20:21.041 --> 00:20:24.590
At that time the responses were essentially pre-program and really simple.

350
00:20:24.870 --> 00:20:27.680
Kind of like Eliza, but one evening,

351
00:20:27.681 --> 00:20:31.670
I think fast forward many years he is in his apartment and one night he says,

352
00:20:31.750 --> 00:20:34.010
a switch suddenly flipped in my, in my mind

353
00:20:35.680 --> 00:20:36.290
<v 8>[inaudible]</v>

354
00:20:36.290 --> 00:20:41.290
<v 4>guys had any sore how to make the machine learn on its own.</v>

355
00:20:41.530 --> 00:20:42.670
What if you thought,

356
00:20:42.730 --> 00:20:47.560
what if it just started at zero like a little baby and it would grow in these

357
00:20:48.250 --> 00:20:52.070
discrete little increments every time you talk to it, right? Basically, uh,

358
00:20:52.100 --> 00:20:55.780
w the first thing that was said to that program that I created,

359
00:20:55.781 --> 00:21:00.040
the first version of that night, um, was said back by it.

360
00:21:00.070 --> 00:21:04.300
Meaning if he said to add hello, it now knew one thing, the word hello.

361
00:21:04.301 --> 00:21:06.130
So it would say hello back.

362
00:21:06.160 --> 00:21:10.090
The second thing it said was a choice of the first two things said to it.

363
00:21:10.091 --> 00:21:13.330
So if the second thing you said was, how are you doing it now? New Two things.

364
00:21:13.331 --> 00:21:15.340
The word hello and the phrase, how are you doing?

365
00:21:15.341 --> 00:21:19.570
So it could either say hello back again or no, how are you doing?

366
00:21:19.630 --> 00:21:23.170
The third thing it said was a choice of the first three things and so on.

367
00:21:23.350 --> 00:21:25.750
Ad Infinitum will not quite ad infinitum.

368
00:21:25.751 --> 00:21:30.120
But between 1988 and 1997, uh,

369
00:21:30.130 --> 00:21:34.810
a few thousand conversations took place between myself in it and a few of my

370
00:21:34.811 --> 00:21:35.740
friends and it,

371
00:21:35.920 --> 00:21:38.980
he and his friends would sit there and type things to it as a way of teaching

372
00:21:38.981 --> 00:21:42.840
it, new things, but it was just them. So it was slow going,

373
00:21:43.090 --> 00:21:44.440
languished for quite a long time.

374
00:21:46.060 --> 00:21:48.600
But then I started working with the Internet,

375
00:21:49.710 --> 00:21:54.390
putting it online where anyone could talk to it within the next 10 years.

376
00:21:54.480 --> 00:21:58.890
It had learned something like 5 million lines of conversation.

377
00:22:01.110 --> 00:22:06.110
Now eighties frequently handling around 200,000 requests an outlaw.

378
00:22:09.600 --> 00:22:13.560
And it's talking to more than 3 million people a month,

379
00:22:14.220 --> 00:22:17.670
3 million conversations a month. And after each one,

380
00:22:18.060 --> 00:22:20.490
clever Bot knows a little bit more than it did before.

381
00:22:20.670 --> 00:22:23.930
And every time you say something to it like, hey, clever about town,

382
00:22:24.030 --> 00:22:28.950
why am I so sad? Eighties accessing.

383
00:22:29.280 --> 00:22:34.280
I think the conversations that millions of people have had in the past asking

384
00:22:34.911 --> 00:22:38.210
itself, what is the best overlap? Where is the best correlation?

385
00:22:38.240 --> 00:22:41.390
How do people usually answer this question? Why am I so sad? That's right.

386
00:22:41.480 --> 00:22:42.313
And then

387
00:22:44.440 --> 00:22:48.750
I response everybody answers just because, hmm.

388
00:22:49.070 --> 00:22:53.410
All right. Well why? There must be a reason why I'm so sad

389
00:22:56.440 --> 00:22:59.680
because you have been sitting in the same place for too long.

390
00:23:00.550 --> 00:23:03.400
Is that WHO's tight? Who's saying that? Exactly?

391
00:23:03.460 --> 00:23:04.660
Where does that response come from?

392
00:23:05.020 --> 00:23:08.650
And the answer is it is one human being at some point in the past.

393
00:23:08.650 --> 00:23:12.610
Having said that, so that is one moment of human conversation from one person.

394
00:23:12.611 --> 00:23:14.770
Yes. So it's like I'm talking to a ghost.

395
00:23:14.830 --> 00:23:18.220
You are talking to it's intelligence if you like,

396
00:23:18.280 --> 00:23:21.490
is borrowed from millions of people in the past.

397
00:23:21.610 --> 00:23:25.270
I'm a little bit of their conversational knowledge.

398
00:23:25.271 --> 00:23:28.930
Their conversational intelligence goes into forming your reply.

399
00:23:29.170 --> 00:23:32.110
Now what's interesting says Rollo is that when you start a conversation with

400
00:23:32.111 --> 00:23:35.860
clever Bot, it doesn't really have a personality or no one personality.

401
00:23:36.300 --> 00:23:39.400
Everbody's is everything to everyone. It's just this big hive really.

402
00:23:39.430 --> 00:23:40.220
I have seen,

403
00:23:40.220 --> 00:23:43.360
but as you keep talking to it and it's sort of pulling forward from the high of

404
00:23:43.361 --> 00:23:46.010
these little ghost fragments of

405
00:23:46.010 --> 00:23:49.460
<v 2>past conversations, stitching them together, a form does</v>

406
00:23:49.540 --> 00:23:53.780
<v 4>kind of emerge it, it reflects the person that is speaking to.</v>

407
00:23:54.100 --> 00:23:58.870
It becomes somewhat like that person. Someone familiar already.

408
00:23:58.900 --> 00:24:02.120
People have very emotional conversations with it. People, um,

409
00:24:02.350 --> 00:24:05.320
have complete arguments with it and um,

410
00:24:06.130 --> 00:24:10.650
of course they try to, um, to get it into bed by talking dirty to it.

411
00:24:10.690 --> 00:24:15.460
Yeah. One thing I can tell you is that I have seen a single person,

412
00:24:15.490 --> 00:24:20.230
a teenage girl, um, speaking for 11 hours with just, uh,

413
00:24:20.500 --> 00:24:23.140
three 15 minute breaks. Whoa.

414
00:24:24.610 --> 00:24:26.380
About what everything.

415
00:24:26.770 --> 00:24:31.770
The day will come not too far down the road where clever Bot becomes so

416
00:24:32.920 --> 00:24:37.920
interesting to talk to that people will be talking to it all day,

417
00:24:38.141 --> 00:24:38.974
every day,

418
00:24:39.470 --> 00:24:42.440
<v 2>but we're not there yet because the same thing that makes clever bots so</v>

419
00:24:42.441 --> 00:24:46.580
interesting to talk to also can make it kind of ridiculous.

420
00:24:47.060 --> 00:24:48.740
For example, in our interview with Brian,

421
00:24:49.010 --> 00:24:52.190
he was the first person that turned us on to this program. As you were talking,

422
00:24:52.191 --> 00:24:55.970
Soren just sort of suggested, well, why don't we just try it right now?

423
00:24:56.480 --> 00:24:59.570
You want to talk? You want to tell it? Say to clarify it. I feel blue. Sure.

424
00:24:59.600 --> 00:25:03.830
Yeah. Are you pulling clever Bot up? Is it just clever bot.org or something?

425
00:25:04.040 --> 00:25:07.280
calm.com I feel, can you say,

426
00:25:07.281 --> 00:25:11.120
I feel blue because an asteroid hit my house this morning because, so this is,

427
00:25:11.190 --> 00:25:16.010
you've hit on a perfect strategy of a, of dealing with these bonds. Absurdity.

428
00:25:16.490 --> 00:25:17.450
Yes. Well,

429
00:25:17.451 --> 00:25:21.950
it basically saying something that has never been said before to clever Bot.

430
00:25:22.580 --> 00:25:26.630
So it's likely that no one has ever claimed an asteroid hit their house.

431
00:25:26.780 --> 00:25:30.410
It's weird enough that it may not be in the database. Okay. Alright.

432
00:25:30.860 --> 00:25:32.810
Let's see what it says. Hit My house this morning.

433
00:25:34.790 --> 00:25:36.250
<v 1>Clever. [inaudible]</v>

434
00:25:38.360 --> 00:25:40.150
<v 2>I woke up at 1:00 PM this afternoon.</v>

435
00:25:44.090 --> 00:25:48.980
Well, there we go. It's not quite so clever. You don't have to worry yet.

436
00:25:49.730 --> 00:25:51.020
Which in fact,

437
00:25:51.021 --> 00:25:55.040
when I went online to youtube and watched the lobe new competition that Brian

438
00:25:55.041 --> 00:25:58.490
attended [inaudible] it turns out none of the computers fooled the judges at

439
00:25:58.491 --> 00:26:02.390
all. None. Any? Well, I don't know if none, none, but they did really badly

440
00:26:07.670 --> 00:26:08.211
for me.

441
00:26:08.211 --> 00:26:12.470
One of the strange takeaways of thinking so much about artificial intelligence

442
00:26:12.920 --> 00:26:17.920
is this feeling of how complex it is to sit across the table from someone and

443
00:26:19.401 --> 00:26:23.960
communicate with body language and tone and rhythm and all of these things.

444
00:26:24.260 --> 00:26:28.580
What happens when those conversations are working out well is that we're willing

445
00:26:28.581 --> 00:26:33.320
to move the conversation in ways that allows us to be sort of perpetually

446
00:26:33.680 --> 00:26:37.910
startling to one another. That's a good word. Startling.

447
00:26:38.340 --> 00:26:39.173
<v 1>Yeah.</v>

448
00:26:39.410 --> 00:26:42.200
<v 2>You learn someone through these small surprises.</v>

449
00:26:51.230 --> 00:26:54.440
Thanks to Brian Christian, his excellent book, which inspired this hour.

450
00:26:54.560 --> 00:26:58.550
It's called the most human human. Go to radiolab.org for more info.

451
00:26:58.880 --> 00:27:03.350
Thanks also to our actors, Sarah Thyer, Andy Richter, and Susan Blackwell.

