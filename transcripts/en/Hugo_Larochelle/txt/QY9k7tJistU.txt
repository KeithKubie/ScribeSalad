Speaker 1:          00:00          Okay.

Speaker 2:          00:01          In this video, we'll quickly talk about how, uh, training would work in a more general conditional random fields set up.

Speaker 1:          00:11          Yeah.

Speaker 2:          00:11          We've mentioned in previous videos that we do not need to restrict ourselves to linear chain, a conditional random fields. If it were a modeling, a pixel labeling problem, we could use a grid structure where each node would correspond to one of the Pixel in the image. And we'd have factors that would express preferences on the joint value of neighboring pixels in the image. A or we could have a very, uh, complicated structure a over our graph where we'd have several types of loops and we can either be able to have factors that would involve more than just a two variables, could even have a structured at varies for different pairs of training inputs and targets. So, uh, I'm mentioning had an example where we have webpages, uh, that and we're modeling the label between a web pages that share a link. And if we had different groups of web pages, then a different groups of webpages might have a different, we'll of course have different network structures. So from modeling the webpages from, uh, the pages of one, uh, on, in one university website versus, and other University of websites, of course these would be different pages and the connected differently. So, um, they're conditional random fields. A framework is very general for modeling the interaction between random variables. So concepts that we're trying to model. So what about training such general conditional random fields?

Speaker 1:          01:47          Okay.

Speaker 2:          01:47          Um, well, uh, to do that, we'd still perform great in descent. So for a given pair of a target and input, we would need to derive what is the expression for the partial derivative of the negative log. Likelihood of observing some given target, given some input with respect to any parameter theta in our model. And uh, it also, uh, it actually takes a fairly simple expression, uh, that's very general. So that's partial. There, there will always be minus the sum over all the factors of the partial they live with respect to my parameter of interests of the luck factors. So over all the factors I'm taking the, uh, some of the partial list of the each luck factor with respect to my parameter. And I'm, I'm subtracting two that the expectation over the, uh, what could be the true label, why? Uh, I did this is the expectation with respect to uh, the model again, the sum of the partial [inaudible] of the luck factors.

Speaker 2:          02:57          So here we have a difference between what a agreement of what is a based on what is observed minus a gradient based on what the model thinks. It's conditioned on the input but we're doing an expectation over what the model thinks is most likely. And uh, if you think about this expression when it's trying to do is that it's trying to make the luck factor larger for the values of y that are in the training set and an expectation is trying to make the luck factors smaller in expectation for essentially every value of y based on what the model, um, currently thinks the way the science probability to other values of y. So if the model becomes very good than p of y, given x t is going to be peaked again, assigned a very high probability to the true, uh, uh, target yt and then zero to everything else, in which case these two terms with cancel out and otherwise training with proceed until, uh, we have converged to a conditional distribution that's a very close to has I'm essentially perfect probability to the, uh, the true answer.

Speaker 2:          04:08          Now in this expression, uh, the main problem is computing the expectation over why here. So this expectation here of this whole expression. So for one thing, and it's an expectation over all values for all the elements in the why vector, uh, but usually since each of the lock factors will only involved or maybe a pair or a triplet or just a subset of the why's, then usually this expectation reduces to an expectation over just a subset of the barrel goals. Uh, so, you know, use fun sense in linear chain conditional random fields. We saw that essentially these expectations are only over either a single yk variable or a pair of white key variables. So often it's only going to involve a few of these white variables. Uh, that being said, uh, computing the conditional distribution over these subset of y variables. Uh, if the graph is not a tree or a chain, that's still attractive, but we have to approximate that conditional distribution. And what we can use is, uh, the loopy variant of belief propagation, uh, to approximate that conditional distribution.

Speaker 2:          05:20          So is it just a reminder of how we can use, uh, the log messages as computed by belief propagation? So we're running a belief propagation, usually in luck space. We run it where we iterate over all the messages that are being passed between factors and variables and vice versa. And we go over these messages several times until the value of the messages doesn't change. And once this has converged, then we can compute, for instance, the marginal probability over just a single y given x as just the exponential of the luck factor. That only involves why k plus the sum for all the other factors of the log messages, uh, that comes from them. So this way we have a way of approximating, in this case, the marginal distribution, but we can generalize this formula to any subset of, of why gays. Um, so I won't go more into the details of this. This is just give you a headstart. If you want to go look at the literature for how to perform and, uh, train general a conditional random fields, models using loopy belief propagation.