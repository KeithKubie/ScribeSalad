WEBVTT
Kind: captions
Language: en

00:00:00.140 --> 00:00:03.810
 this capsule we will see the Algeria of 

00:00:02.280 --> 00:00:08.840
 learning necessarily active parents 

00:00:03.810 --> 00:00:11.070
 cloning so now we're ready to 

00:00:08.840 --> 00:00:13.110
 also considered commented what one 

00:00:11.070 --> 00:00:15.269
 can do active learning in the 

00:00:13.110 --> 00:00:19.230
 body of learning by differences 

00:00:15.269 --> 00:00:22.170
 temporal so in active learning 

00:00:19.230 --> 00:00:24.660
 in fact we are our agent to himself 

00:00:22.170 --> 00:00:26.400
 policy that updates so he can 

00:00:24.660 --> 00:00:27.840
 make his own decisions on 

00:00:26.400 --> 00:00:31.470
 that she should be 

00:00:27.840 --> 00:00:33.809
 the action that takes 1 and you gave and 

00:00:31.470 --> 00:00:35.840
 and usefully this tale what we want 

00:00:33.809 --> 00:00:39.000
 to do is to take the action that 

00:00:35.840 --> 00:00:42.329
 maximizes the expectation of the sum of 

00:00:39.000 --> 00:00:45.180
 future rewards and who this formula 1 

00:00:42.329 --> 00:00:47.610
 here now the problem in 

00:00:45.180 --> 00:00:51.570
 learning of deaths that we estimate 

00:00:47.610 --> 00:00:54.230
 only the value of a policy but 

00:00:51.570 --> 00:00:57.750
 we are not going to destine ourselves 

00:00:54.230 --> 00:01:00.030
 the transition distribution so the 

00:00:57.750 --> 00:01:03.420
 given transition model you gave 

00:01:00.030 --> 00:01:05.909
 your fencing if I'm an action learns 

00:01:03.420 --> 00:01:08.760
 take an active approach 

00:01:05.909 --> 00:01:09.930
 we could parallel to take a 

00:01:08.760 --> 00:01:11.630
 transition model how 

00:01:09.930 --> 00:01:15.479
 learning by pda 

00:01:11.630 --> 00:01:17.280
 but in fact it will go a little against the 

00:01:15.479 --> 00:01:18.750
 objective learning by differences 

00:01:17.280 --> 00:01:21.420
 temporal that of not precisely 

00:01:18.750 --> 00:01:23.790
 learn this model but learn 

00:01:21.420 --> 00:01:29.280
 rather directly associated values 

00:01:23.790 --> 00:01:31.049
 to states so if that's who 

00:01:29.280 --> 00:01:32.880
 learning will try to solve as 

00:01:31.049 --> 00:01:34.259
 problem so comment on what is avoided 

00:01:32.880 --> 00:01:37.200
 learning a model of 

00:01:34.259 --> 00:01:39.240
 transition and the idea is actually going 

00:01:37.200 --> 00:01:43.680
 to introduce and learn 

00:01:39.240 --> 00:01:49.259
 rather an action function values ​​than 

00:01:43.680 --> 00:01:52.890
 s to q 2 r yes and 2 so rather than 

00:01:49.259 --> 00:01:55.049
 to learn v2 is what hopes it years 

00:01:52.890 --> 00:01:57.869
 the sum of the reinforcements from a 

00:01:55.049 --> 00:02:00.450
 state's until the end for politics 

00:01:57.869 --> 00:02:03.149
 optimal we will learn a function that 

00:02:00.450 --> 00:02:04.649
 of s1 which is also the hope of the 

00:02:03.149 --> 00:02:05.250
 sum of reinforcements from a 

00:02:04.649 --> 00:02:08.789
 state s 

00:02:05.250 --> 00:02:12.690
 but in the specific case we run 

00:02:08.789 --> 00:02:13.560
 the action to the state and that continues 

00:02:12.690 --> 00:02:15.930
 then 

00:02:13.560 --> 00:02:18.150
 until the end by following the policy 

00:02:15.930 --> 00:02:20.640
 optimal so that's what it corresponds to 

00:02:18.150 --> 00:02:25.530
 the definition of our action function 

00:02:20.640 --> 00:02:28.800
 barras is actually the yoyo link between the 

00:02:25.530 --> 00:02:33.510
 function that in the usa and life function 

00:02:28.800 --> 00:02:34.890
 it's just that the value of s 

00:02:33.510 --> 00:02:38.519
 for the optimal plan 

00:02:34.890 --> 00:02:41.790
 it's just the match so the value 

00:02:38.519 --> 00:02:43.950
 maximum of my function where I 

00:02:41.790 --> 00:02:47.340
 maximizes relative to the action so 

00:02:43.950 --> 00:02:49.230
 indeed, it is hope 

00:02:47.340 --> 00:02:52.470
 the sum of the reinforcements from 

00:02:49.230 --> 00:02:54.390
 if I perform the action at it's 

00:02:52.470 --> 00:02:56.459
 the best I can do for the 

00:02:54.390 --> 00:02:59.040
 optimal policy is to take the 

00:02:56.459 --> 00:03:02.069
 best action so its value to 

00:02:59.040 --> 00:03:03.440
 sausage with the maximum value for 

00:03:02.069 --> 00:03:09.209
 my function 

00:03:03.440 --> 00:03:11.130
 q is this with respect to in the plane of 

00:03:09.209 --> 00:03:13.620
 the agent if I have my job that 

00:03:11.130 --> 00:03:17.940
 seals here estimated simply so of 

00:03:13.620 --> 00:03:19.829
 take the action that maximizes my 

00:03:17.940 --> 00:03:21.269
 function which therefore has my policy that 

00:03:19.829 --> 00:03:24.390
 I can follow at this moment it is 

00:03:21.269 --> 00:03:27.299
 the hac max of mac function that by 

00:03:24.390 --> 00:03:29.880
 report to so I may have needed for 

00:03:27.299 --> 00:03:32.160
 get an action executed when 

00:03:29.880 --> 00:03:35.120
 learning to esteemed my model of 

00:03:32.160 --> 00:03:37.440
 transition and expresses his luck and to 

00:03:35.120 --> 00:03:42.230
 m6 of this approach where we go 

00:03:37.440 --> 00:03:42.230
 use in the near by tulasne 

00:03:43.990 --> 00:03:49.060
 so to get and learning 

00:03:45.760 --> 00:03:52.270
 well we'll see how do we 

00:03:49.060 --> 00:03:56.380
 can write recurrently the 

00:03:52.270 --> 00:03:58.600
 definition of q2 is will be alas I 

00:03:56.380 --> 00:04:01.450
 reminder is the sum of the rewards 

00:03:58.600 --> 00:04:03.150
 future hoped from there you have to 

00:04:01.450 --> 00:04:07.960
 from state it if i execute 

00:04:03.150 --> 00:04:10.600
 i'm running action a and so it's okay 

00:04:07.960 --> 00:04:13.750
 to recurrently correspond to the 

00:04:10.600 --> 00:04:17.739
 reward was you going to have more my factor 

00:04:13.750 --> 00:04:20.489
 the second time the future reward 

00:04:17.739 --> 00:04:25.750
 hope they just so quickly the 

00:04:20.489 --> 00:04:28.660
 the hope of actually my value to 

00:04:25.750 --> 00:04:31.180
 policy-weighted mind state 

00:04:28.660 --> 00:04:34.650
 I arrive at the premium ethos from s 

00:04:31.180 --> 00:04:37.330
 if I execute the action to which my argument 

00:04:34.650 --> 00:04:40.000
 is now empty es premium 

00:04:37.330 --> 00:04:42.340
 Well, it's actually equivalent to 

00:04:40.000 --> 00:04:45.370
 maximum compared to the action that I 

00:04:42.340 --> 00:04:46.530
 will be able to execute a spirit even of my 

00:04:45.370 --> 00:04:51.280
 function 

00:04:46.530 --> 00:04:54.040
 q2 is this bonus after so I managed to 

00:04:51.280 --> 00:04:56.080
 recurrently write the value of 

00:04:54.040 --> 00:04:58.840
 q2 s up compared to all my others 

00:04:56.080 --> 00:05:00.330
 qi values ​​of his so had two 

00:04:58.840 --> 00:05:03.220
 minds 

00:05:00.330 --> 00:05:05.250
 and as in the projecting of what we 

00:05:03.220 --> 00:05:09.790
 will do is we will use that for 

00:05:05.250 --> 00:05:12.850
 deduce a way to update my 

00:05:09.790 --> 00:05:14.980
 function my estimate my function qds 

00:05:12.850 --> 00:05:16.870
 I take my job one hour 

00:05:14.980 --> 00:05:19.660
 current is frozen incremented by a 

00:05:16.870 --> 00:05:23.260
 learning rate times the difference 

00:05:19.660 --> 00:05:26.200
 between the information I gained due 

00:05:23.260 --> 00:05:29.470
 to the fact that I went from state s to 

00:05:26.200 --> 00:05:34.030
 the state expresses by executing an action to 

00:05:29.470 --> 00:05:39.490
 give - my present value my 

00:05:34.030 --> 00:05:40.900
 current estimate of q2 is ok and so 

00:05:39.490 --> 00:05:43.840
 actually because I have this 

00:05:40.900 --> 00:05:46.030
 relationship here in q2 and the others that 

00:05:43.840 --> 00:05:48.610
 minds my premium if I observe that 

00:05:46.030 --> 00:05:51.250
 it's possible to go from s to a 

00:05:48.610 --> 00:05:52.690
 mind state give me but I could 

00:05:51.250 --> 00:05:57.430
 try to get closer to the 

00:05:52.690 --> 00:05:59.230
 reward rds more gamma times my 

00:05:57.430 --> 00:06:00.480
 a current estimate of which 

00:05:59.230 --> 00:06:04.390
 would be the awards photo is 

00:06:00.480 --> 00:06:06.580
 hoped from express so emptied this 

00:06:04.390 --> 00:06:10.750
 premium then that's equivalent or at 

00:06:06.580 --> 00:06:13.240
 maximum barbara prime of q is this 

00:06:10.750 --> 00:06:16.390
 crime says so we see the similarity 

00:06:13.240 --> 00:06:18.060
 with the rule td we have here from 

00:06:16.390 --> 00:06:22.210
 temporal differences 

00:06:18.060 --> 00:06:24.280
 so I look like my most haf x range 

00:06:22.210 --> 00:06:26.890
 folks but the generic successful 

00:06:24.280 --> 00:06:30.760
 according to my function that I 

00:06:26.890 --> 00:06:33.100
 know of esteemed and here instead of videos 

00:06:30.760 --> 00:06:35.230
 is this qds player to then this is the place 

00:06:33.100 --> 00:06:35.740
 to have said this vehicle is 

00:06:35.230 --> 00:06:37.360
 thing here 

00:06:35.740 --> 00:06:39.730
 and so that's going to be the rule 

00:06:37.360 --> 00:06:45.010
 the learning that we will use in 

00:06:39.730 --> 00:06:46.720
 learning that offered them a 

00:06:45.010 --> 00:06:49.480
 example still actually our 

00:06:46.720 --> 00:06:52.830
 environment and three states so at 

00:06:49.480 --> 00:06:57.850
 early feeder read my table of 

00:06:52.830 --> 00:07:02.140
 so my function than zero so for s 0 

00:06:57.850 --> 00:07:06.250
 the action at 1 to 0 but same thing for l0 

00:07:02.140 --> 00:07:08.470
 the action at 2.26 at 0 saad also 

00:07:06.250 --> 00:07:10.260
 same thing for is simple for s2 seen 

00:07:08.470 --> 00:07:11.440
 that it's a terminal state I'm going 

00:07:10.260 --> 00:07:13.090
 specific 

00:07:11.440 --> 00:07:16.030
 the action it's going to be always the same 

00:07:13.090 --> 00:07:17.080
 action the man action then i'm going to me 

00:07:16.030 --> 00:07:18.520
 serve to 0 range 

00:07:17.080 --> 00:07:20.740
 I will use like everything 

00:07:18.520 --> 00:07:24.620
 learning point 5 then a rate 

00:07:20.740 --> 00:07:26.690
 of 5 0 points 5 also just for 

00:07:24.620 --> 00:07:30.139
 what this example if not these debates 

00:07:26.690 --> 00:07:31.490
 their loans specified there is no reason 

00:07:30.139 --> 00:07:37.100
 particular to use these values 

00:07:31.490 --> 00:07:39.610
 there here then at the very beginning so we 

00:07:37.100 --> 00:07:42.590
 start the simulation 

00:07:39.610 --> 00:07:44.870
 I arrive at the start state s 0 general 

00:07:42.590 --> 00:07:46.940
 we think of them - 10.1 and when if 

00:07:44.870 --> 00:07:49.340
 in fact we do nothing we want everything 

00:07:46.940 --> 00:07:53.060
 just remember that we've been to 

00:07:49.340 --> 00:07:57.050
 start you got 0 then we got the reward 

00:07:53.060 --> 00:07:58.220
 at 2 - 0 point 1 and so we can not 

00:07:57.050 --> 00:08:00.169
 update because we have 

00:07:58.220 --> 00:08:01.970
 need to choose what is the action 

00:08:00.169 --> 00:08:03.560
 that I'm going to take the state is what I 

00:08:01.970 --> 00:08:05.900
 can choose when so if the first 

00:08:03.560 --> 00:08:07.970
 thing I'm going to do I'm going to choose 

00:08:05.900 --> 00:08:12.620
 what actions I'm going to perform in 

00:08:07.970 --> 00:08:18.229
 s 0 and the idea if here to take a max 

00:08:12.620 --> 00:08:19.910
 from q2 0 to 1 or q 2 0 to 2 when if i have 

00:08:18.229 --> 00:08:21.860
 used both to zero so 

00:08:19.910 --> 00:08:23.630
 arbitrarily I will choose 2 

00:08:21.860 --> 00:08:25.010
 I could have chosen 1 

00:08:23.630 --> 00:08:30.950
 because if I decided to choose the 

00:08:25.010 --> 00:08:33.529
 radio I take the action to two lots 

00:08:30.950 --> 00:08:35.930
 I observe that it brings me to the state is 

00:08:33.529 --> 00:08:39.169
 zero and when I'm going to do that is 

00:08:35.930 --> 00:08:44.000
 I will update my function 

00:08:39.169 --> 00:08:46.760
 to go you have 0 and assuming that I took 

00:08:44.000 --> 00:08:49.279
 the action at 2 according to the formula of 

00:08:46.760 --> 00:08:51.740
 day 2 of the nerve nor so I will do my 

00:08:49.279 --> 00:08:55.089
 old value and I have a crime between 

00:08:51.740 --> 00:08:57.709
 the learning rate 0.5 point 

00:08:55.089 --> 00:09:02.900
 information based on the fact that I 

00:08:57.709 --> 00:09:04.820
 makes a transition from s 0 to 0 - my 

00:09:02.900 --> 00:09:08.300
 current estimate of the function 

00:09:04.820 --> 00:09:11.390
 my previous state s 0 and the fact that 

00:09:08.300 --> 00:09:14.390
 I took action at everything so I'm going 

00:09:11.390 --> 00:09:17.360
 just replaced so kind here r20 which 

00:09:14.390 --> 00:09:21.380
 is less 0 point when I was in 06 

00:09:17.360 --> 00:09:23.540
 I got the reward month 0.1 plus my 

00:09:21.380 --> 00:09:27.170
 discount rate times the maximum between 

00:09:23.540 --> 00:09:29.270
 the two values ​​of the function qu to a1 

00:09:27.170 --> 00:09:32.209
 and a2 game the initialized to zero so 

00:09:29.270 --> 00:09:34.550
 it's the max of 0 0 so if I do everything 

00:09:32.209 --> 00:09:38.290
 this calculation to I get a value for q 

00:09:34.550 --> 00:09:40.730
 2 0 to 2 2 - 0 points 0 5 

00:09:38.290 --> 00:09:43.070
 to know if I have to take an action 

00:09:40.730 --> 00:09:44.750
 in my new state which is in is 

00:09:43.070 --> 00:09:47.900
 still 0 because I made a 

00:09:44.750 --> 00:09:50.540
 transition from 10 is reopened to 0 and there 

00:09:47.900 --> 00:09:53.270
 I have to choose then to action I have to 

00:09:50.540 --> 00:09:56.750
 take it so I'm still going to arms 

00:09:53.270 --> 00:09:59.270
 axis between q 2 0 to a shield 2 0 to 2 but 

00:09:56.750 --> 00:10:02.690
 now that from 0 to 2 is equal to 

00:09:59.270 --> 00:10:04.430
 less than 0.05 so in that if in 

00:10:02.690 --> 00:10:06.080
 done I'm going to have a change in the 

00:10:04.430 --> 00:10:09.110
 politics I will rather choose 

00:10:06.080 --> 00:10:15.530
 action at one year the state and unfolding 

00:10:09.110 --> 00:10:18.170
 before I had chosen the action on May 2 

00:10:15.530 --> 00:10:20.840
 of love action suppose that in my 

00:10:18.170 --> 00:10:23.570
 simulation it can not pass 2 0 as 1 

00:10:20.840 --> 00:10:25.790
 so now I can make a bet to 

00:10:23.570 --> 00:10:28.640
 day of my value of my job that 

00:10:25.790 --> 00:10:31.340
 go 2-0 and the action has one that is 

00:10:28.640 --> 00:10:33.320
 the previous action that I took that 

00:10:31.340 --> 00:10:34.820
 I have just taken one more time 

00:10:33.320 --> 00:10:37.850
 I will update it in writing 

00:10:34.820 --> 00:10:41.180
 liar by alpha times information 

00:10:37.850 --> 00:10:45.680
 related to the fact that I went from s 0 as 

00:10:41.180 --> 00:10:47.480
 one is my present value so here here I 

00:10:45.680 --> 00:10:49.910
 have to compare these quadras of my function 

00:10:47.480 --> 00:10:52.370
 that in insurge by action to 2 where 

00:10:49.910 --> 00:10:53.840
 the action to three I take maximum because 

00:10:52.370 --> 00:10:56.060
 only if both are zero so that's fine 

00:10:53.840 --> 00:10:58.460
 give 0 anyway so my new 

00:10:56.060 --> 00:11:02.690
 0 culoz station to a headband if I 

00:10:58.460 --> 00:11:04.460
 do the math - 0.05 household to state 

00:11:02.690 --> 00:11:06.890
 is healthy I have to make a decision I 

00:11:04.460 --> 00:11:11.060
 do the haf match between q drawing smells 

00:11:06.890 --> 00:11:12.730
 and qds 1 to 3 so it's in 2003 because 

00:11:11.060 --> 00:11:15.560
 that is trying out possible actions 

00:11:12.730 --> 00:11:17.660
 according to my environment it is the action 

00:11:15.560 --> 00:11:19.130
 to two where the action to you both have 

00:11:17.660 --> 00:11:23.170
 been initialized to zero so 

00:11:19.130 --> 00:11:23.170
 arbitrarily here I choose to two 

00:11:23.980 --> 00:11:32.570
 with the action to two it brings me to pass 

00:11:28.250 --> 00:11:34.490
 in my simulation as 0 and so their 

00:11:32.570 --> 00:11:36.680
 household can update my values ​​of 

00:11:34.490 --> 00:11:38.990
 the function that in s1 for the action to 

00:11:36.680 --> 00:11:41.270
 two with the same formula as before so 

00:11:38.990 --> 00:11:45.830
 I use the fact that I observed a 

00:11:41.270 --> 00:11:48.200
 reverse s voltage 0 so here I 

00:11:45.830 --> 00:11:50.900
 will compare that value here with 

00:11:48.200 --> 00:11:51.290
 my current estimate because if 

00:11:50.900 --> 00:11:55.130
 in the East 

00:11:51.290 --> 00:12:00.250
 two causes for qds 0 to 1 I had less 

00:11:55.130 --> 00:12:04.820
 0.05 that 2 0 it must be not them here 

00:12:00.250 --> 00:12:06.440
 so I also had less 0.05 so 

00:12:04.820 --> 00:12:09.040
 for both actions actually my value 

00:12:06.440 --> 00:12:11.750
 of q &amp; a f zero is equivalent to the hand 

00:12:09.040 --> 00:12:13.880
 and I do the math all that gives me 

00:12:11.750 --> 00:12:15.880
 a new charge value to 2 2 - 

00:12:13.880 --> 00:12:18.050
 0 point 0 6 2 5 

00:12:15.880 --> 00:12:19.699
 now I am happy 0 to take 

00:12:18.050 --> 00:12:21.380
 an action of which it is still a 

00:12:19.699 --> 00:12:23.660
 time as you are around but 

00:12:21.380 --> 00:12:27.230
 value of 40 times I succeed them it goes 

00:12:23.660 --> 00:12:29.389
 to be two so my value in 0 has 

00:12:27.230 --> 00:12:33.800
 function that for action and action to 

00:12:29.389 --> 00:12:38.509
 2 c - 0.05 so arbitrarily I 

00:12:33.800 --> 00:12:40.880
 to choose one will do us business 

00:12:38.509 --> 00:12:42.790
 a new transition from sg reopened 

00:12:40.880 --> 00:12:46.100
 s16 that I observe in the simulation 

00:12:42.790 --> 00:12:48.290
 having chosen the action at 1 previously 

00:12:46.100 --> 00:12:50.769
 it allows me to update 

00:12:48.290 --> 00:12:54.860
 my function that in s 0 for the action to 

00:12:50.769 --> 00:12:58.910
 1.6 here I'm going to do an associated calculation 

00:12:54.860 --> 00:13:01.490
 at a pull 2 ​​0 loses s1 and I compare 

00:12:58.910 --> 00:13:03.889
 the body and I will take action two 

00:13:01.490 --> 00:13:06.069
 breast or three stops they are according to 

00:13:03.889 --> 00:13:09.350
 what I estimated by my function that 

00:13:06.069 --> 00:13:12.019
 whereas if I max in 10 0 and - 

00:13:09.350 --> 00:13:14.660
 0 point 1 0625 if I do all the calculation 

00:13:12.019 --> 00:13:17.410
 it gives me new value for 2 

00:13:14.660 --> 00:13:20.180
 011 2 - 0 points 0.75 

00:13:17.410 --> 00:13:21.680
 that leads me now to take a 

00:13:20.180 --> 00:13:25.220
 decision at s1 

00:13:21.680 --> 00:13:28.100
 I do the max between q2 is a pure 

00:13:25.220 --> 00:13:30.949
 two-man action and three-man action 

00:13:28.100 --> 00:13:33.920
 that's it so the airmac feel a 

00:13:30.949 --> 00:13:35.720
 negative value and 0 which brings me to 

00:13:33.920 --> 00:13:36.709
 choose the action at 3 so I have a 

00:13:35.720 --> 00:13:39.079
 political change 

00:13:36.709 --> 00:13:41.209
 when I was at a reverse spell I had 

00:13:39.079 --> 00:13:44.269
 chose the two meta action I'm going 

00:13:41.209 --> 00:13:45.380
 take action at 3 and now 

00:13:44.269 --> 00:13:49.430
 suppose that in the stimulation 

00:13:45.380 --> 00:13:51.790
 I observe that at 3 May nice 2 we get 

00:13:49.430 --> 00:13:54.110
 recalls that for the terminal state 

00:13:51.790 --> 00:13:57.170
 yes so for the terminals I can 

00:13:54.110 --> 00:13:59.689
 always put all lassigny its value 

00:13:57.170 --> 00:14:01.160
 to the reward is observed in a two 

00:13:59.689 --> 00:14:01.939
 so this is the special case when 

00:14:01.160 --> 00:14:03.589
 I expected 

00:14:01.939 --> 00:14:05.659
 upstream mines algorithms i'm going all of 

00:14:03.589 --> 00:14:09.409
 following signed the value of my function 

00:14:05.659 --> 00:14:11.709
 than the reward observed and that's 

00:14:09.409 --> 00:14:14.749
 besides there that I'm going to study here 

00:14:11.709 --> 00:14:17.209
 in my update because if I have a 

00:14:14.749 --> 00:14:18.439
 mac on a single action because I 

00:14:17.209 --> 00:14:20.419
 make a transition from the holy fathers 

00:14:18.439 --> 00:14:21.409
 the terminal state or in fact I can 

00:14:20.419 --> 00:14:23.329
 of actions to take so I'm going 

00:14:21.409 --> 00:14:25.789
 use the ass value for action 

00:14:23.329 --> 00:14:27.049
 danone which is equal to 1 

00:14:25.789 --> 00:14:29.599
 if I do the math that's our ass 

00:14:27.049 --> 00:14:30.979
 sarts at 3 which is zero point and 

00:14:29.599 --> 00:14:33.169
 I'll be able to do it again 

00:14:30.979 --> 00:14:35.029
 to interest and continue to do as much 

00:14:33.169 --> 00:14:37.729
 of tests that I wish it according to a 

00:14:35.029 --> 00:14:40.069
 stop criterion given either a number 

00:14:37.729 --> 00:14:43.699
 fixed iterations or until 

00:14:40.069 --> 00:14:44.029
 did my kuhn values ​​change not 

00:14:43.699 --> 00:14:46.159
 a lot 

00:14:44.029 --> 00:14:49.639
 or that in fact I could also 

00:14:46.159 --> 00:14:51.649
 to watch is what my policy has stopped 

00:14:49.639 --> 00:14:52.699
 to change in different states that 

00:14:51.649 --> 00:14:57.529
 could be a stop hospital 

00:14:52.699 --> 00:15:00.589
 possible but again we can 

00:14:57.529 --> 00:15:02.209
 before the problem of one still the 

00:15:00.589 --> 00:15:04.579
 dilemma in the exploration phase versus 

00:15:02.209 --> 00:15:06.589
 exploitation so let's imagine I would have 

00:15:04.579 --> 00:15:09.049
 could also take an action at 4 which 

00:15:06.589 --> 00:15:11.029
 bring me back a state is king and that 

00:15:09.049 --> 00:15:13.479
 it was ps3 house a reward 

00:15:11.029 --> 00:15:18.949
 associates 2000 

00:15:13.479 --> 00:15:22.249
 now since q2 s1 off cat is 

00:15:18.949 --> 00:15:24.470
 equal to zero so his selection to 4 which 

00:15:22.249 --> 00:15:26.269
 brought me one of 13.3 so at 

00:15:24.470 --> 00:15:28.699
 initialization game reindeer initialized 

00:15:26.269 --> 00:15:32.059
 the parry of qsr at 4 1 0 

00:15:28.699 --> 00:15:34.549
 and since in my essay I put to 

00:15:32.059 --> 00:15:36.649
 qds day to 3.9 while they were over 

00:15:34.549 --> 00:15:37.999
 great than 0 but it's true that the 

00:15:36.649 --> 00:15:41.029
 close thanks to each time she 

00:15:37.999 --> 00:15:44.509
 would arrive at s1 she was never exploring 

00:15:41.029 --> 00:15:47.479
 the action at 4 so never explore the state 

00:15:44.509 --> 00:15:50.379
 is king so we also have here is a 

00:15:47.479 --> 00:15:55.279
 problem with a potential problem of 

00:15:50.379 --> 00:15:57.619
 lack of exploration your adaptation 

00:15:55.279 --> 00:15:59.779
 to control this balance between 

00:15:57.619 --> 00:16:02.269
 exploration and exploitation in the 

00:15:59.779 --> 00:16:05.239
 ass learning it's worth it all 

00:16:02.269 --> 00:16:07.220
 simply when I make the choice of 

00:16:05.239 --> 00:16:10.339
 my support action that serves me 

00:16:07.220 --> 00:16:13.939
 of exploration here so we see on my 

00:16:10.339 --> 00:16:15.270
 value of 2 my function that rather than 

00:16:13.939 --> 00:16:16.770
 fall of max 

00:16:15.270 --> 00:16:18.390
 compared to actions on my function 

00:16:16.770 --> 00:16:21.180
 that directly I wanted to go to 

00:16:18.390 --> 00:16:23.520
 through my exploration function where I 

00:16:21.180 --> 00:16:27.000
 consider, among other things, the number 

00:16:23.520 --> 00:16:29.250
 of times that I have reached the express state 

00:16:27.000 --> 00:16:31.380
 and that I did a potential action 

00:16:29.250 --> 00:16:34.230
 to the premiums so that allows me 

00:16:31.380 --> 00:16:37.320
 to artificially increase the 

00:16:34.230 --> 00:16:38.910
 future reward hoped to associate with being 

00:16:37.320 --> 00:16:40.440
 in the state and this price ends the action 

00:16:38.910 --> 00:16:41.660
 off premiums by his father and 

00:16:40.440 --> 00:16:44.100
 artificially increase it for 

00:16:41.660 --> 00:16:47.220
 try a few times to do all 

00:16:44.100 --> 00:16:48.540
 the different actions possible or 

00:16:47.220 --> 00:16:51.690
 number of times I'm going to do this 

00:16:48.540 --> 00:16:55.140
 action there is going to be associated their wheels to 

00:16:51.690 --> 00:16:57.810
 threshold is only the handcuff in my 

00:16:55.140 --> 00:16:59.280
 exploration function so if that's 

00:16:57.810 --> 00:17:00.960
 so we can introduce the 

00:16:59.280 --> 00:17:03.780
 concept of exploitation and exploration 

00:17:00.960 --> 00:17:06.650
 which to control this balance there in the 

00:17:03.780 --> 00:17:06.650
 panty box 

