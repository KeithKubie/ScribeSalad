1
00:00:00,990 --> 00:00:03,420
The Joe Rogan experience.
We're also,

2
00:00:03,450 --> 00:00:07,110
there's a concern about those systems
being vulnerable to third party

3
00:00:07,240 --> 00:00:11,710
attacks. Yeah. So hacking. Yeah, that's a,

4
00:00:11,711 --> 00:00:13,000
that's a fascinating question.

5
00:00:13,300 --> 00:00:18,300
I think there is a whole discipline
called adversarial machine learning in AI,

6
00:00:19,630 --> 00:00:24,130
which basically any kind of system
you can think of how we can feed it.

7
00:00:24,131 --> 00:00:25,030
Examples,

8
00:00:25,990 --> 00:00:30,490
how we can add a little bit of noise
to the system to fool it completely.

9
00:00:30,850 --> 00:00:33,820
So there's been demonstrations
on Alexa for example,

10
00:00:34,060 --> 00:00:38,230
where you can take a,
you can take a,

11
00:00:38,260 --> 00:00:42,790
you can feed noise into the system that's
imperceptible to us humans and make it

12
00:00:42,791 --> 00:00:47,140
believe you said anything.
So fool the system into thinking,

13
00:00:47,530 --> 00:00:52,270
ordering extra toilet paper. I
Dunno. Uh, in the same for cars,

14
00:00:52,271 --> 00:00:57,271
you can feed noise into the cameras
to make us believe that there is,

15
00:00:57,401 --> 00:00:58,600
or there isn't a pedestrian,

16
00:00:59,100 --> 00:01:00,960
there is or there isn't lane markings.

17
00:01:01,230 --> 00:01:04,740
So someone could do this in theory,
at least

18
00:01:04,940 --> 00:01:08,840
in a, in theory that's the big
difference is in theory is doable.

19
00:01:08,841 --> 00:01:12,980
You can do demonstrations in practices
actually really difficult to do in the

20
00:01:12,981 --> 00:01:14,930
real world.
So in the lab you can do it,

21
00:01:15,020 --> 00:01:18,440
you can construct a situation where a
pedestrian can wear certain types of

22
00:01:18,441 --> 00:01:21,800
clothing or put up a certain kind of sign
where they disappear from the system.

23
00:01:22,360 --> 00:01:24,190
I have to ask you this because
now I just remembered this.

24
00:01:24,191 --> 00:01:25,960
You'd be the perfect
person to talk about this.

25
00:01:26,090 --> 00:01:27,880
I'm not sure if you remember this case,

26
00:01:28,120 --> 00:01:31,570
but there was a guy named
Michael Hastings, Michael
Hastings with a journalist.

27
00:01:31,660 --> 00:01:36,160
And, uh, he was, I believe
in Iraq or Afghanistan.

28
00:01:36,161 --> 00:01:41,161
He was somewhere overseas and he was
stuck there because of this volcano that

29
00:01:41,891 --> 00:01:43,480
erupted and I believe Iceland.

30
00:01:44,170 --> 00:01:47,590
And he was over there for the Rolling
Stone magazine, uh, and doing,

31
00:01:47,591 --> 00:01:49,990
doing an article about a general,
well,

32
00:01:49,991 --> 00:01:53,950
he stayed there for a long time because
they were stranded because of the

33
00:01:53,980 --> 00:01:58,090
volcano and they got real
comfortable around him. And, uh,

34
00:01:58,390 --> 00:02:03,390
he reported a lot of the stuff that they
said and did that maybe they thought

35
00:02:03,730 --> 00:02:06,430
that he probably wouldn't
have reported on,

36
00:02:06,790 --> 00:02:11,620
including them saying disparaging things
about President Obama at the time.

37
00:02:11,800 --> 00:02:16,150
Anyway, comes back, the general
was forced to resign. Um,

38
00:02:16,210 --> 00:02:18,520
he was a beloved general and,
uh,

39
00:02:18,550 --> 00:02:21,970
Michael Hastings was in fearing for his
life because he thought that they were

40
00:02:21,971 --> 00:02:25,930
going to come and get him because these
people were very, very angry at him.

41
00:02:26,560 --> 00:02:31,560
He wound up driving his car into a tree
going like 120 miles an hour and the car

42
00:02:32,201 --> 00:02:34,540
exploded and the engine went flying.

43
00:02:35,260 --> 00:02:40,260
And people that were the
conspiracy theorists were
saying they believe that that

44
00:02:42,221 --> 00:02:47,221
car had been rigged to work autonomously
or that someone for some third party

45
00:02:48,970 --> 00:02:53,020
bad person decided to, or good
person, depending on your perspective,

46
00:02:53,320 --> 00:02:57,520
decided to drive that guy's car into
a fucking tree at 120 miles an hour.

47
00:02:57,820 --> 00:02:59,410
Do you think that that and is

48
00:03:00,610 --> 00:03:02,920
2011,

49
00:03:04,090 --> 00:03:07,360
Michael Hastings def 12,

50
00:03:07,420 --> 00:03:10,300
maybe 2012 I think that sounds great.

51
00:03:11,260 --> 00:03:15,310
Let's see what it says.
2013, 2013. June, 2013.

52
00:03:15,340 --> 00:03:19,360
Do you think that in 2013
that would have been possible?

53
00:03:21,680 --> 00:03:22,513
Okay.

54
00:03:24,450 --> 00:03:27,360
It's entirely possible. No,
I just wanted to say that.

55
00:03:27,390 --> 00:03:32,280
[inaudible] shout out to the
Joe Rogan sub reddit. Okay.

56
00:03:32,340 --> 00:03:35,190
Uh,
check that one off the list.

57
00:03:38,520 --> 00:03:42,860
Jamie. Pull that up, chat off. Um,

58
00:03:46,080 --> 00:03:46,913
Aye.

59
00:03:47,180 --> 00:03:49,940
Whether it's possible is
an interesting question,

60
00:03:49,970 --> 00:03:52,350
whether it's likely is another question.
I,

61
00:03:52,351 --> 00:03:56,780
I think it's very unlikely and the
other most important questions,

62
00:03:56,781 --> 00:03:59,690
that's something we should worry at scale,

63
00:03:59,691 --> 00:04:04,460
but our future is cars being used
to assassinate essentially people.

64
00:04:05,270 --> 00:04:06,590
I'm Russian,
so I've,

65
00:04:06,591 --> 00:04:11,120
I've heard of those things being
done by our friend Putin of it. Um,

66
00:04:12,260 --> 00:04:13,220
I think,

67
00:04:13,610 --> 00:04:17,780
I think it's very unlikely that this
kind of thing would happen at scale,

68
00:04:18,170 --> 00:04:19,220
that people would use this.

69
00:04:19,221 --> 00:04:23,690
I think there'll be more effective ways
to achieve this kind of end for sure.

70
00:04:24,080 --> 00:04:24,630
And I,

71
00:04:24,630 --> 00:04:29,360
I just think it's a very difficult
technical challenge that uh,

72
00:04:30,140 --> 00:04:31,910
uh,
if hacking happens,

73
00:04:32,240 --> 00:04:37,240
it would be at a different level
than hacking the AI systems.

74
00:04:37,790 --> 00:04:39,750
It'll be just hacking software,
right?

75
00:04:39,770 --> 00:04:42,980
And hacking software is the kind of the,

76
00:04:42,981 --> 00:04:45,740
the kind of thing that can happen
with anything. An elevator saw,

77
00:04:45,920 --> 00:04:50,090
elevate a software or, uh, any
kind of software that operates,

78
00:04:50,091 --> 00:04:54,100
any aspect of our lives could be hacked.
And that same kind of way. Right. My,

79
00:04:54,180 --> 00:04:56,930
my question though was in 2013,

80
00:04:56,960 --> 00:05:00,020
was that technology available where
they could take over someone's car?

81
00:05:02,680 --> 00:05:06,070
Do you know what car it was?
Mercedes, I think it was an s class,

82
00:05:06,071 --> 00:05:10,690
C C C C class as is. Yes, yes,

83
00:05:11,110 --> 00:05:15,740
yes. But I, I don't think, oh
boy, this is like a no, it's,

84
00:05:15,860 --> 00:05:18,400
listen, this has been
widely speculated. I know,

85
00:05:18,460 --> 00:05:20,590
I'm just asking you because
you're actually an expert. I mean,

86
00:05:20,591 --> 00:05:25,000
it's very rare that you get an expert in
autonomous vehicles and you get to run

87
00:05:25,001 --> 00:05:28,300
a conspiracy theory behind them to see
if they can just put a stamp on it being

88
00:05:28,570 --> 00:05:31,500
possible or not. Let me just say
that Alex Jones is a fishing,

89
00:05:31,510 --> 00:05:36,510
not allowed to say MIT scientists says
she's exactly what he's going to try to

90
00:05:37,511 --> 00:05:40,150
do. Uh, no, I, um, first of all,

91
00:05:40,151 --> 00:05:43,210
let me back off and say I
am not a security expert,

92
00:05:43,300 --> 00:05:45,850
which is a very important difference.
That is important.

93
00:05:45,920 --> 00:05:47,710
So then a autonomous vehicle,

94
00:05:47,711 --> 00:05:51,280
I'd build a autonomous vehicle systems.

95
00:05:51,340 --> 00:05:55,440
I don't know how to make
them extremely robust.

96
00:05:55,441 --> 00:05:58,520
The security to hacking attacks, right.
And have a lot of really good friends,

97
00:05:58,550 --> 00:06:00,690
which are some of the coolest people.
Uh,

98
00:06:00,691 --> 00:06:04,310
I know who are basically hackers
converted to security experts.

99
00:06:04,760 --> 00:06:08,570
I would say though, loosely speaking,
I think the technology was there yes.

100
00:06:08,660 --> 00:06:12,830
For with physical access to the car
to be able to control it. But I don't,

101
00:06:13,370 --> 00:06:15,890
I think it's extremely unlikely.
That's what happened.

102
00:06:16,390 --> 00:06:19,130
I agree.
I see where you're coming from.

103
00:06:19,470 --> 00:06:23,520
I'm not asking you whether or not
it's likely that it happened and I'm,

104
00:06:23,530 --> 00:06:26,020
I'm sure you don't even
have much information on the
case cause I had to explain

105
00:06:26,021 --> 00:06:30,550
it to you. Right. That's right.
I'm the guy also had, uh,

106
00:06:30,980 --> 00:06:35,870
some serious amphetamines in the system.
Um, they compared it to crystal Meth,

107
00:06:35,871 --> 00:06:39,320
but the reality is he was a
journalist and most journalists,

108
00:06:39,350 --> 00:06:44,350
I don't want to say most all lot are
on Adderall and Adderall is essentially

109
00:06:45,500 --> 00:06:48,440
amphetamines. I mean, that's
what it is. It's real. It's like,

110
00:06:48,950 --> 00:06:52,760
it's like next door neighbors
to crystal meth really is. Um,

111
00:06:54,160 --> 00:06:56,020
he is it,

112
00:06:56,440 --> 00:06:59,980
well you said it's possible they could
actually get it to turn the wheel.

113
00:07:01,270 --> 00:07:03,490
Yes. I have to look at the
exact system. Like I said,

114
00:07:03,491 --> 00:07:07,840
drive by wire thing that I
mentioned. Some systems are not, uh,

115
00:07:08,170 --> 00:07:10,900
it's not so easy to turn.
The wheel actually could

116
00:07:11,780 --> 00:07:13,400
get them to just
accelerate out of control.

117
00:07:13,401 --> 00:07:16,760
He's going like 120 something miles
now and just slammed into a tree

118
00:07:17,340 --> 00:07:21,180
entirely possible. Ah, you
can't do it twice. The, um,

119
00:07:21,840 --> 00:07:26,580
the systems back then though, we're
far more primitive. Correct? Yeah. Uh,

120
00:07:26,581 --> 00:07:30,700
yeah, but it's, it's really, again,
the, the attack vectors here. The,

121
00:07:30,710 --> 00:07:33,120
so the way you hack these systems,

122
00:07:33,121 --> 00:07:37,650
I have more to do with the software Lola
of software that can be primitive than

123
00:07:37,651 --> 00:07:39,290
the high level AI stuff.
Right.

124
00:07:39,330 --> 00:07:42,510
But my issue with was there's no
cameras on the outside of the vehicle,

125
00:07:42,511 --> 00:07:47,370
like there is on Tesla today, which has
autonomous driving as an option as a, so,

126
00:07:47,520 --> 00:07:48,600
okay.
I see your point now.

127
00:07:48,750 --> 00:07:52,980
So he wouldn't be hacking the system
that perceives the world and act in the

128
00:07:52,981 --> 00:07:53,460
world.

129
00:07:53,460 --> 00:07:58,460
It would literally be malfunction that
forces it to not be able to brake,

130
00:07:59,040 --> 00:08:02,040
accelerate uncontrollably,
which is, uh, you know,

131
00:08:02,060 --> 00:08:07,060
it's a more basic kind of attack and then
control then making the car steer auto

132
00:08:07,410 --> 00:08:09,900
lane. Yes, yes. That's a different,

133
00:08:10,110 --> 00:08:13,410
that's what people worry about with
autonomous vehicles. Once more and more,

134
00:08:13,650 --> 00:08:18,000
you're talking about potentially 10,
20 million lines of source code.

135
00:08:18,090 --> 00:08:22,250
So there's all this code and
so obviously becomes, uh,

136
00:08:22,800 --> 00:08:23,460
amenable,

137
00:08:23,460 --> 00:08:28,460
susceptible to bugs that can
be exploited to hack the code.

138
00:08:29,430 --> 00:08:31,320
And so people are worried,
uh,

139
00:08:31,410 --> 00:08:35,130
legitimately so that these
security attacks would, uh,

140
00:08:35,190 --> 00:08:37,920
would lead to these kind of,
um,

141
00:08:38,550 --> 00:08:43,420
well at the worst case assassinations,
but really sort of just basic, uh,

142
00:08:43,500 --> 00:08:47,940
basic attacks, basic a hacking
attacks. And I think it's,

143
00:08:48,480 --> 00:08:51,330
I think that's something that people in
the automotive industry and certainly

144
00:08:51,331 --> 00:08:56,331
Tesla's really working hard on and making
sure that the ad that everything is

145
00:08:56,641 --> 00:09:00,600
secure, there's going to be, of course
vulnerabilities always. But, uh,

146
00:09:00,601 --> 00:09:02,970
I think they're really
serious about preventing them.

147
00:09:03,450 --> 00:09:05,460
But in the demonstration space,

148
00:09:05,461 --> 00:09:10,461
you'd be able to demonstrate
some interesting ways to
trick the system in terms

149
00:09:10,741 --> 00:09:11,530
of computer vision.

150
00:09:11,530 --> 00:09:16,470
This all boils down to that
these systems are actually,

151
00:09:16,880 --> 00:09:18,480
that are ones that are camera based,

152
00:09:18,870 --> 00:09:23,700
are not as robust as our human eyes
are to the world. So like I said,

153
00:09:23,730 --> 00:09:24,960
if you add a little bit of noise,

154
00:09:24,961 --> 00:09:29,580
you can convince it to see anything to
us humans that look like the same road,

155
00:09:29,670 --> 00:09:31,700
like the same three pedestrians,
costumes,

156
00:09:31,710 --> 00:09:33,450
trawler a little person
on the camera lens,

157
00:09:34,910 --> 00:09:39,840
the little cameras, right. You could get
down there with a sharpie. That's, that's,

158
00:09:40,160 --> 00:09:45,000
that's the one attack vector. That's a
as draw stuff. But you jokingly say that,

159
00:09:45,001 --> 00:09:49,470
but that's I believe is possible. The Sun
plays tricks on Cadillac, Super Cruise,

160
00:09:49,740 --> 00:09:52,380
next generation system.
We'll address camera problem.

161
00:09:52,620 --> 00:09:56,160
Oh well as long as the next generation
addresses that you fucking assholes.

162
00:09:56,730 --> 00:09:58,500
The Sun plays tricks on it.

163
00:09:58,860 --> 00:10:02,130
So next gen system is something you're
going to have to bring that Cadillac into

164
00:10:02,131 --> 00:10:04,570
the dealership and they're going to have
to update the software, update it yet.

165
00:10:04,650 --> 00:10:07,240
Whereas Tessa would just handle
that shit over the area. Yeah,

166
00:10:07,290 --> 00:10:10,410
I got an update the other day. I was
like, all right, all right. Question.

167
00:10:10,880 --> 00:10:15,410
Same as, so that, that's an
exciting, powerful capability.

168
00:10:15,800 --> 00:10:19,910
But then the Boeing, the
flip side is, you know, uh,

169
00:10:19,911 --> 00:10:22,760
it can significantly change the
behavior of the system. And there,

170
00:10:22,970 --> 00:10:26,660
there could be a glitch that could be
a glitch that could be a bug that the

171
00:10:26,661 --> 00:10:31,610
Boeing one's terrifying, especially
with a lot of, I mean, that number,

172
00:10:31,670 --> 00:10:36,020
whatever it is, it's like 300
combined, 300 plus people dead,

173
00:10:36,230 --> 00:10:39,030
maybe even 400. I mean, it's,

174
00:10:40,430 --> 00:10:43,430
I don't even know how to think.
Think about that number. Yeah.

175
00:10:43,590 --> 00:10:44,840
All from a software glitch,

176
00:10:44,930 --> 00:10:48,050
the guy who created it or the girl who
quoted it must feel fucking terrible.

177
00:10:49,640 --> 00:10:53,660
Yeah. And you kind of, you, you walk, man.

178
00:10:54,200 --> 00:10:55,850
It's, it's, uh,

179
00:10:56,150 --> 00:11:00,890
it's a lot of burden and it's one of the
reasons it's one of the most exciting

180
00:11:00,891 --> 00:11:02,120
things to work on actually,

181
00:11:02,480 --> 00:11:07,130
is the code we write has the
capability to save human life.

182
00:11:07,131 --> 00:11:10,580
But the terrifying thing is also has
the capability to take human life.

183
00:11:11,090 --> 00:11:15,860
And that's a, that's a weird place
to be as an engineer or directly,

184
00:11:16,190 --> 00:11:20,210
a little piece of code. You know, I'll
write thousands of them a day. You know,

185
00:11:20,211 --> 00:11:23,900
basically notes you're taking
could eventually lead to a,

186
00:11:24,410 --> 00:11:25,340
somebody dying

187
00:11:29,830 --> 00:11:33,020
[inaudible].

