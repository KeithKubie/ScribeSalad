WEBVTT
Kind: captions
Language: en

00:00:00.742 --> 00:00:03.580
All right, so let's see if we can think
through, or maybe guess what's going to

00:00:03.580 --> 00:00:10.100
happen if TD lambda is run as an update
algorithm on some finite amount of data.

00:00:10.100 --> 00:00:13.300
So we it, we give the system
some finite amount of data, and

00:00:13.300 --> 00:00:16.170
then we run TD lambda with different
possible values of lambda.

00:00:16.170 --> 00:00:19.060
And then we ask, alright,
the V that you've learned,

00:00:19.060 --> 00:00:22.235
how far is that from what the V should
have been, if we had infinite data?

00:00:22.235 --> 00:00:24.055
&gt;&gt; Hm-mm.

00:00:24.055 --> 00:00:27.377
&gt;&gt; Alright so, I don't know,
let's see if we can figure out, or

00:00:27.377 --> 00:00:29.837
maybe even guess,
what this might look like.

00:00:29.837 --> 00:00:34.300
So, let's say if we use lambda
equals zero, so we do TD zero.

00:00:34.300 --> 00:00:35.605
So after some finite amount data,

00:00:35.605 --> 00:00:37.140
there's going to be
some amount of error.

00:00:37.140 --> 00:00:42.180
Let's just you know,
I don't know, call it that.

00:00:42.180 --> 00:00:42.790
&gt;&gt; Okay.

00:00:42.790 --> 00:00:47.622
&gt;&gt; Now it turns out, empirically, if you
play with this, it's pretty typical for

00:00:47.622 --> 00:00:52.880
TD one to give you worse error,
given finite data.

00:00:52.880 --> 00:00:55.460
And now the important thing
is what happens in between.

00:00:55.460 --> 00:00:58.081
So it could be like a straight line
if it was literally interpolating,

00:00:58.081 --> 00:01:02.130
it could be kind of bowed up, but
it's actually typically bowed down.

00:01:05.510 --> 00:01:10.400
So it's not so atypical to see a curve
kind of like this, that gets better.

00:01:10.400 --> 00:01:14.060
The amount of error goes down as
lambda gets to some intermediate value

00:01:14.060 --> 00:01:15.240
let's call it, I don't know .7.

00:01:15.240 --> 00:01:18.840
It depends of course on this,
the particular problem.

00:01:18.840 --> 00:01:22.060
And then kind of start to shoot up again
and it eventually hit the error for

00:01:22.060 --> 00:01:23.269
TD one as lambda approaches one.

00:01:24.430 --> 00:01:28.080
And so this is kind of the justification
for why we want to consider TD lambda.

00:01:28.080 --> 00:01:31.480
That it's actually combining these
two different sources of information.

00:01:31.480 --> 00:01:32.880
What happens at the end
of the episode and

00:01:32.880 --> 00:01:37.490
what happens at each step, in a way
that actually takes advantage of both.

00:01:37.490 --> 00:01:38.830
It gets you the benefits of both.

00:01:38.830 --> 00:01:43.150
We're rapidly propagating information
and we're not getting a biased estimate,

00:01:43.150 --> 00:01:45.660
because of the way that we're
using one step information.

00:01:45.660 --> 00:01:48.216
So, you know,
it has some attractiveness to it.

00:01:48.216 --> 00:01:51.320
&gt;&gt; Huh, did you pick 0.7 on purpose?

00:01:51.320 --> 00:01:54.020
I remember reading a paper once
that suggested that basically

00:01:54.020 --> 00:01:55.990
everyone uses lambda equals to 0.7.

00:01:55.990 --> 00:02:00.871
&gt;&gt; I typically see 0.3 to 0.7 as values
where things tend to work out where

00:02:00.871 --> 00:02:05.160
this bottom tends to fall,
somewhere between 0.3 and 0.7.

00:02:05.160 --> 00:02:10.100
Personally I always use zero, because
it works better with the control.

00:02:10.100 --> 00:02:12.180
I have had better performance and

00:02:12.180 --> 00:02:14.530
better understanding of what
happens in the zero case.

00:02:14.530 --> 00:02:17.530
But it is true that if you actually
run it with a finite amount of data,

00:02:17.530 --> 00:02:21.598
it tends to bottom out
somewhere between 0.3 and 0.7.

00:02:21.598 --> 00:02:23.340
&gt;&gt; Hmm, okay, that's worth note.

00:02:23.340 --> 00:02:26.387
&gt;&gt; So that's actually all I wanted
to tell you about TD lambda.

00:02:26.387 --> 00:02:30.110
I wanted to give you a sense of how the
equations are defined, and how they're

00:02:30.110 --> 00:02:34.350
derived, and empirically, why this
is something we actually care about.

00:02:34.350 --> 00:02:36.350
So, maybe we should sum up?

00:02:36.350 --> 00:02:37.000
&gt;&gt; I'm big fan of sum.

00:02:37.000 --> 00:02:38.055
Is this going to be an infinite sum?

00:02:38.055 --> 00:02:40.520
&gt;&gt; [LAUGH]
&gt;&gt; Thank you, I've been here all week.

