WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.007
The other major thing that you can do to minimize the time that your program

00:00:03.007 --> 00:00:06.212
spends in its memory accesses is what's called coalescing.

00:00:06.212 --> 00:00:09.079
We want to coalesce your accesses to global memory.

00:00:09.079 --> 00:00:10.547
Let me explain what that means.

00:00:10.547 --> 00:00:15.187
Whenever a thread on the GPU reads or writes global memory,

00:00:15.187 --> 00:00:18.070
it always accesses a large chunk of memory at once.

00:00:18.070 --> 00:00:22.694
Even if that thread only needs to read or write a small subset of the data in that large chunk.

00:00:22.694 --> 00:00:26.265
But if other threads are making similar axises at the same time

00:00:26.265 --> 00:00:29.779
then the GPU can exploit that and reuse this larger chunk

00:00:29.779 --> 00:00:33.336
for all of the threads that're trying to access that memory.

00:00:33.336 --> 00:00:39.645
This means the GPU is at its most efficient when threads read or write contiguous global memory locations.

00:00:39.645 --> 00:00:42.236
We say such an access pattern is coalesced.

00:00:42.236 --> 00:00:45.116
In this example every thread is reading or writing from a chunk of memory

00:00:45.116 --> 00:00:48.619
that's basically given by the index of the thread plus some offset.

00:00:48.619 --> 00:00:51.320
This is a coalesced access. This is good.

00:00:51.320 --> 00:00:55.281
You'll get very high performance on a memory read or memory write in this setting.

00:00:55.281 --> 00:01:01.514
In this example every adjacent thread is accessing every other memory location,

00:01:01.514 --> 00:01:03.765
and so this is not coalesced.

00:01:03.765 --> 00:01:07.057
We would call this strided because there is a stride

00:01:07.057 --> 00:01:10.614
between every threads access and this pattern is not so good.

00:01:10.614 --> 00:01:14.798
If you think about it, the way that I drew this dotted line here sort of implies

00:01:14.798 --> 00:01:20.672
that the GPU in this case was accessing memory in chunks of five memory locations.

00:01:20.672 --> 00:01:24.009
If I were to just draw out the next five memory locations,

00:01:24.009 --> 00:01:28.532
you could see that here, to service the needs of four threads making a request

00:01:28.532 --> 00:01:33.318
each to an adjacent memory location, I was able to service that with a single memory transaction.

00:01:33.318 --> 00:01:34.485
This dotted line.

00:01:34.485 --> 00:01:40.270
Whereas in this case the same four threads are striding across memory,

00:01:40.270 --> 00:01:46.639
and I actually need to pull in 2 memory transactions to these chunks of memory in order to service that.

00:01:46.639 --> 00:01:50.081
I'm going to get half of the speed of out of my global memory here.

00:01:50.081 --> 00:01:53.682
You can probably see that the larger the stride between my threads,

00:01:53.682 --> 00:01:57.223
the more total memory transactions I'm going to have to do

00:01:57.223 --> 00:01:59.357
and the lower my performance will get.

00:01:59.357 --> 00:02:03.474
At the limit you can get to a place where each thread is accessing spots

00:02:03.474 --> 00:02:06.298
so far in memory or so unrelated to each other in memory

00:02:06.298 --> 00:02:09.570
that every single thread gets its own memory transaction.

00:02:09.570 --> 00:02:13.237
This, as you can imagine, will lead to pretty bad memory performance from the memory system.

00:02:13.237 --> 00:02:16.005
We'll talk more about memory optimizations later.

00:02:16.005 --> 00:02:18.739
For now, just know that global memory is going to be fastest

00:02:18.739 --> 00:02:22.739
when successive threads read or write adjacent locations in a continuous stretch of memory.

