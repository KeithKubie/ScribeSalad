WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:04.330
Now let's look at the relationships
between SMT and cache performance.

00:00:04.330 --> 00:00:06.110
The cache of the core,

00:00:06.110 --> 00:00:10.520
is shared by all of the SMT threads
that are currently active on for

00:00:10.520 --> 00:00:15.180
example, if you have 2 SMT then the
cache is shared by both of the threads.

00:00:15.180 --> 00:00:18.510
That gives us some good and
some bad properties.

00:00:18.510 --> 00:00:23.720
The good property Is that we get
fast data sharing among the threads.

00:00:23.720 --> 00:00:26.370
When Thread 0 stores something, and

00:00:26.370 --> 00:00:30.490
then Thread 1 wants to read it,
it gets to get a cache shared.

00:00:30.490 --> 00:00:34.530
So any communication between these
threads that happens within a short span

00:00:34.530 --> 00:00:37.355
of time like we store and
then we quickly load in another thread,

00:00:37.355 --> 00:00:40.260
will go through the cache and
have really,

00:00:40.260 --> 00:00:44.190
really good performance because the load
here will always be a cache hit,

00:00:44.190 --> 00:00:47.120
assuming that the store
happened recently.

00:00:47.120 --> 00:00:50.840
So the communication now no
longer goes through shared memory

00:00:50.840 --> 00:00:52.260
because we have a cache.

00:00:52.260 --> 00:00:55.530
The communication tends to
go through just the cache.

00:00:55.530 --> 00:01:00.770
But when we have an SMT processor,
the cache capacity that we have and

00:01:00.770 --> 00:01:04.650
its associativity is shared
by all of the threads.

00:01:04.650 --> 00:01:09.470
So if the working set of thread
0 plus the working set of

00:01:09.470 --> 00:01:15.140
thread 1 minus the part of the working
set that they have in common,

00:01:15.140 --> 00:01:17.820
which will be counted
twice if we did this,

00:01:17.820 --> 00:01:23.330
exceeds the data size,
we get what is called cache trashing.

00:01:23.330 --> 00:01:26.200
Pretty much the data no
longer fits in the cache, and

00:01:26.200 --> 00:01:28.670
we get a lot of cache misses.

00:01:28.670 --> 00:01:34.070
If each of the working sets would fit
individually in the data cache size,

00:01:34.070 --> 00:01:37.650
then the performance
of the SMT can be and

00:01:37.650 --> 00:01:42.200
usually is significantly worse than
doing the threads one at a time.

00:01:42.200 --> 00:01:47.310
So we were assuming that when we did
SMT that we will get some benefits from

00:01:47.310 --> 00:01:51.200
using resources that would otherwise be
underutilized like the issue slots and

00:01:51.200 --> 00:01:53.460
the reservation stations and so on.

00:01:53.460 --> 00:01:56.630
But that assumes we are getting
similar cash performance.

00:01:56.630 --> 00:02:00.360
However, if the cash
has a given size and

00:02:00.360 --> 00:02:05.540
threads are written to fit in that cash,
then running two such threads

00:02:05.540 --> 00:02:10.530
if they don't share enough data,
might exceed the cash size in which case

00:02:10.530 --> 00:02:15.400
suddenly we have a lot more misses
then we expected and we make payback.

00:02:15.400 --> 00:02:19.330
In cache misses a lot more than
we gain by overlapping execution

00:02:19.330 --> 00:02:20.580
in the SMT fashion.

