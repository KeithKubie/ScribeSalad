1
00:00:00,260 --> 00:00:03,830
[inaudible]

2
00:00:04,150 --> 00:00:06,510
so what would you say to
the person listening that's
going, well, wait a minute,

3
00:00:06,520 --> 00:00:09,280
1000 bucks,
I can't do anything with 1000 bucks.

4
00:00:09,281 --> 00:00:11,410
There's basically nowhere I can get rent.
You know,

5
00:00:11,411 --> 00:00:13,690
how am I going to do
anything close to a living?

6
00:00:14,150 --> 00:00:17,800
Right? So for most people now,
$1,000 is not going to be enough,

7
00:00:17,801 --> 00:00:21,610
but it will provide a cushion. Right?
So that's sort of the whole idea.

8
00:00:21,611 --> 00:00:24,010
We're starting off with $1,000
and that's not going to be enough.

9
00:00:24,011 --> 00:00:25,030
So you're still have to work.

10
00:00:25,090 --> 00:00:27,730
So is that the thing? Is that the
biggest confusion related to all this?

11
00:00:27,731 --> 00:00:32,650
That I think people hear ubi and they
think that it's just enough. Just enough.

12
00:00:32,651 --> 00:00:33,610
So you can get by.

13
00:00:33,670 --> 00:00:36,880
But you're saying that's actually
not really what's going on here.

14
00:00:37,270 --> 00:00:39,760
It may not be enough initially.
Um,

15
00:00:40,480 --> 00:00:43,900
I mean there may be some places
in the country, not Los Angeles,

16
00:00:43,901 --> 00:00:45,850
but some places you could live on $1,000.

17
00:00:45,851 --> 00:00:49,150
So if you're really in a bad
situation and you're living in La,

18
00:00:49,151 --> 00:00:52,990
you could pack up and move somewhere
where maybe a thousand dollars we'll allow

19
00:00:52,991 --> 00:00:55,150
you to survive. Right?
That's a possibility.

20
00:00:55,151 --> 00:00:59,710
But I think but most people will do is
they will take that thousand dollars and

21
00:00:59,711 --> 00:01:03,370
they will use that to sort of
cushion, um, the difficult times.

22
00:01:03,371 --> 00:01:07,480
But they will still be motivated
to find a job, to start a business,

23
00:01:07,481 --> 00:01:09,340
to do something there.
We just have more options,

24
00:01:09,341 --> 00:01:12,280
more freedom in terms of the
choices that they make. Um,

25
00:01:12,450 --> 00:01:15,460
and that's why you mentioned you're
having Andrew Yang and he actually calls

26
00:01:15,461 --> 00:01:18,700
this program the freedom dividends.
That's what he's named it.

27
00:01:18,701 --> 00:01:22,000
And that's exactly what it is. It
gives you more options, options,

28
00:01:22,001 --> 00:01:25,840
especially for someone that's living
month to month and really has no income.

29
00:01:26,500 --> 00:01:29,680
You know, the, the number of choices
that you have at in that scenario,

30
00:01:29,681 --> 00:01:30,514
which is very limited.

31
00:01:30,580 --> 00:01:34,090
Yeah. I guess so much of this has to do
with just the strange way we deal with

32
00:01:34,091 --> 00:01:37,300
politics. So, for example,
you'll have, you know,

33
00:01:37,301 --> 00:01:41,500
politicians saying we have to have $15
minimum wage and then you can walk into

34
00:01:41,501 --> 00:01:45,400
Burger King or Mcdonald's now in order
on an iPad because they're basically

35
00:01:45,401 --> 00:01:47,800
saying, all right, we're not
going to pay our people this much.

36
00:01:47,801 --> 00:01:51,430
So everything just becomes sort
of Uber politicized, right?

37
00:01:51,850 --> 00:01:53,300
That's right. And, and you know that,

38
00:01:53,800 --> 00:01:58,270
that's why a basic income is maybe a
better approach because we just give it to

39
00:01:58,271 --> 00:02:02,650
everyone. And the problem with raising
the minimum wage is that it may be,

40
00:02:03,370 --> 00:02:05,560
you know,
good for many workers,

41
00:02:05,980 --> 00:02:09,790
but it can actually also increase
the incentive to automate or,

42
00:02:10,240 --> 00:02:13,660
or to do other things, right? So it
could actually result in fewer jobs.

43
00:02:13,661 --> 00:02:17,560
So giving people a basic income and
preserving the incentive for them to still

44
00:02:17,561 --> 00:02:21,250
work or to do other things, I think
is, is a very attractive proposition.

45
00:02:21,520 --> 00:02:26,060
W when you sort of play out, you
know, five, 10, 20 years from now,

46
00:02:26,720 --> 00:02:30,320
do you think things are going to just
be beyond drastically different in a way

47
00:02:30,321 --> 00:02:34,550
we really can't think about because
of the speed of all of this because of

48
00:02:34,910 --> 00:02:38,300
everything we've talked about here that
we really can't even envision the way is,

49
00:02:38,810 --> 00:02:42,650
I think if you go out maybe a little
further than that 2030 years, that really,

50
00:02:42,651 --> 00:02:46,760
really gets hard to imagine
what the future looks like. Um,

51
00:02:47,300 --> 00:02:50,020
my latest book, architects of
intelligence, one of the people, you know,

52
00:02:50,280 --> 00:02:52,580
it's a series of interviews
with the top people in AI.

53
00:02:52,880 --> 00:02:56,910
One of the people I talk to is Ray
Kurzweil. Chris is the big futurist.

54
00:02:56,950 --> 00:03:01,160
He thinks that it will be alive then we
won't be long guy. Yeah, he absolutely,

55
00:03:01,170 --> 00:03:02,380
he thinks he's going to live forever.

56
00:03:02,381 --> 00:03:05,290
He expects what he calls the singularity
is something that is going to

57
00:03:05,291 --> 00:03:09,130
completely change the whole paradigm.
Um,

58
00:03:09,370 --> 00:03:12,850
he thinks that within just 10 years
we're going to have human level,

59
00:03:12,880 --> 00:03:16,360
artificial intelligence and so
forth. So that's possible. Um,

60
00:03:16,600 --> 00:03:18,190
the problem is that is very unpredictable.

61
00:03:18,191 --> 00:03:20,260
We don't know how fast
all of this is coming.

62
00:03:20,650 --> 00:03:25,650
What I've really focused on is sort of
the practical impacts of AI and robotics

63
00:03:26,351 --> 00:03:27,820
and the impact on the job market.

64
00:03:27,820 --> 00:03:31,030
So what I would say is that
within five to 10 years,

65
00:03:31,031 --> 00:03:36,031
we're going to definitely see a fairly
dramatic and unambiguous impact on the

66
00:03:36,671 --> 00:03:38,200
job market and on the economy.

67
00:03:38,201 --> 00:03:41,830
And I think ray is basically gonna live
long enough to see the robots take over

68
00:03:41,880 --> 00:03:45,970
one way or another. That's
what he believes. You know,
ray is, he's already 70.

69
00:03:45,971 --> 00:03:48,760
But uh,
if you've seen his photos recently,

70
00:03:49,300 --> 00:03:53,250
he looks a lot younger than then
he did a while ago. So He's,

71
00:03:53,251 --> 00:03:57,730
he's at least he's had some work
done. So whether the, you know,

72
00:03:57,731 --> 00:03:59,920
the stuff under the hood
is, is, is better or not,

73
00:03:59,921 --> 00:04:03,130
but is he the only person
doing that sort of thing?

74
00:04:03,131 --> 00:04:06,160
There must be some other people. Oh, no,
there are a lot, you know, silicon valley,

75
00:04:06,161 --> 00:04:10,870
there are many people very interested
in this idea of living forever and,

76
00:04:10,910 --> 00:04:15,450
and advanced it, you know, the Google
People Larry Page and Sergei and, uh,

77
00:04:16,090 --> 00:04:18,310
Peter Teal is very into this as well,

78
00:04:18,590 --> 00:04:22,930
is even I think played
around with supposedly blood
transfusions and stuff like

79
00:04:22,931 --> 00:04:26,300
that. So yes, the, the silicon
valley elite is, you know,

80
00:04:26,350 --> 00:04:30,970
there are true believers in terms of
this idea that technology is going to

81
00:04:30,971 --> 00:04:33,460
completely transform things in the future,

82
00:04:33,461 --> 00:04:37,090
is going to be dramatically different
from the past and then we're going to

83
00:04:37,270 --> 00:04:40,570
potentially even have the
possibility of living forever.

84
00:04:40,750 --> 00:04:42,040
Can you explain the singularity?

85
00:04:42,420 --> 00:04:47,420
The singularity basically means a point
at which technology takes off and begins

86
00:04:48,130 --> 00:04:52,290
accelerating at a rate that becomes
an comprehensible to us. So they,

87
00:04:52,350 --> 00:04:55,120
it comes from basically a black hole,
right?

88
00:04:55,121 --> 00:04:58,780
The center of a black hole was what's
called a singularity where the laws of

89
00:04:58,781 --> 00:05:03,540
physics breakdown and you can't
see beyond that point. Um, so this,

90
00:05:03,550 --> 00:05:06,330
the term singularity was coined,
um,

91
00:05:06,430 --> 00:05:11,260
as a way to express the idea that technol
technology reaches a point where it's

92
00:05:11,320 --> 00:05:15,190
just completely unpredictable beyond
that point because things are moving so

93
00:05:15,191 --> 00:05:16,024
fast.

94
00:05:16,060 --> 00:05:20,680
And most people that think about this
associate that with the advance of what's

95
00:05:20,681 --> 00:05:24,970
called superintelligence are
machines that are smarter than us,

96
00:05:24,971 --> 00:05:29,490
not only human level intelligence,
but a machine that, that you know,

97
00:05:29,500 --> 00:05:32,050
is smarter than any human being,
maybe dramatically.

98
00:05:32,051 --> 00:05:35,860
So maybe so smart that it makes
us look like a mouse or an insect.

99
00:05:35,861 --> 00:05:40,750
And that's where my Psi Phi brain and
every movie that I've ever seen and every

100
00:05:40,751 --> 00:05:45,170
dystopian future says, well, why would
the robots need us at that point? Exactly.

101
00:05:45,180 --> 00:05:45,701
If anything,

102
00:05:45,701 --> 00:05:49,030
wouldn't they just see us in it as an
annoying hindrance or a vestige of the

103
00:05:49,031 --> 00:05:51,460
past? And why wouldn't they
want to get rid of us? Right.

104
00:05:51,461 --> 00:05:53,590
And that's a real concern,
that concern,

105
00:05:53,800 --> 00:05:57,840
which is what you see in the
terminator movie. Um, and,

106
00:05:58,010 --> 00:05:59,390
and even more than that,

107
00:05:59,391 --> 00:06:02,240
the related concern of what's
called the control problem,

108
00:06:02,241 --> 00:06:06,860
which is that if we created a
superintelligence, something
that's far beyond us,

109
00:06:07,370 --> 00:06:09,860
maybe it won't actively
want to destroy us,

110
00:06:09,861 --> 00:06:13,250
but it might act in ways that are not,
you know,

111
00:06:13,920 --> 00:06:15,430
what our by a robot,

112
00:06:15,600 --> 00:06:20,130
right. Um, and there are very, very
serious thinkers that are focused on this.

113
00:06:20,131 --> 00:06:24,210
The money, the most prominent ones is
Nick Bostrom, who I also interviewed, um,

114
00:06:24,270 --> 00:06:27,750
in my latest book, architects of
intelligence. So he, you know,

115
00:06:27,751 --> 00:06:32,751
believes that this is a real issue and
he's working on finding ways to build

116
00:06:34,080 --> 00:06:38,460
systems that will be controllable
even if they become super intelligent.

117
00:06:39,080 --> 00:06:40,950
And so this is an issue
that he's focused on.

118
00:06:41,050 --> 00:06:44,200
Oh, but the inherent problem being that
if you create the super intelligence,

119
00:06:44,230 --> 00:06:46,990
it probably at some point
can get around that. I mean,

120
00:06:46,991 --> 00:06:50,380
I know that's not mind blowing to him, but
like, right, like that's the whole idea.

121
00:06:50,790 --> 00:06:53,710
Yeah. Once we have a super
intelligence, then, you know,

122
00:06:53,711 --> 00:06:57,100
it's so far beyond us that we
can't control it anymore. So what,

123
00:06:57,520 --> 00:06:59,870
what people are working on,
um,

124
00:06:59,920 --> 00:07:04,150
is basically a principles of computer
science that will hopefully allow them to

125
00:07:04,510 --> 00:07:06,340
build these systems in a way that,

126
00:07:06,400 --> 00:07:11,210
that will remain aligned with what we
want them to do even when they're super

127
00:07:11,211 --> 00:07:11,760
intelligent.

128
00:07:11,760 --> 00:07:14,100
Right. But it's the problem with
that, what we hit on earlier,

129
00:07:14,101 --> 00:07:18,660
which is maybe we here in America
hopefully figure out some systems that are

130
00:07:18,661 --> 00:07:19,560
going to make some sense,

131
00:07:19,561 --> 00:07:22,740
but if the Chinese figure out a system
that's a little bit different or just

132
00:07:22,741 --> 00:07:27,150
some random guy in his garage in Mexico
figures out some other thing that we

133
00:07:27,151 --> 00:07:30,330
still have that basically
there's just no way to manage.

134
00:07:30,331 --> 00:07:32,420
It seems like a big problems.
One of

135
00:07:32,420 --> 00:07:37,190
the scariest aspects of this is that
competition and the fact that there would

136
00:07:37,191 --> 00:07:41,240
be an incredible first mover
advantage to whoever gets there first.

137
00:07:41,630 --> 00:07:45,760
Whoever builds the first
superintelligent system,

138
00:07:46,480 --> 00:07:50,090
you know, is, is going to be way ahead
of everyone else. And the reason is,

139
00:07:50,150 --> 00:07:54,350
is that most people believe in what's
called an intelligence explosion or kind

140
00:07:54,351 --> 00:07:55,550
of,
um,

141
00:07:55,850 --> 00:07:59,990
iterative improvement where basically
once the machine reaches human level

142
00:08:00,290 --> 00:08:02,090
intelligence or it gets beyond that,

143
00:08:02,330 --> 00:08:05,630
it's going to turn its attention
to its own code, right,

144
00:08:05,631 --> 00:08:07,400
to building better versions of itself.

145
00:08:07,401 --> 00:08:11,870
So it's going to continuously engineer
a smarter version of itself and that's

146
00:08:11,871 --> 00:08:14,660
something that could explode rapidly.
So whoever gets there first,

147
00:08:15,170 --> 00:08:16,420
they're essentially uncatchable.

148
00:08:16,490 --> 00:08:20,270
So that is going to set up a competitive
environment between the US and China

149
00:08:20,271 --> 00:08:23,300
and Russia and so forth. So that's
something to worry about. Um,

150
00:08:24,080 --> 00:08:27,050
but there are a lot of people
working on doing this in a safe way.

151
00:08:27,050 --> 00:08:29,760
Open Ai is another good example.
That's the,

152
00:08:29,820 --> 00:08:33,050
the organization it was
set up by Elon Musk, right?

153
00:08:33,051 --> 00:08:37,640
And some other people and they're actively
working on building systems that they

154
00:08:37,641 --> 00:08:40,670
basically, they're trying to get
there first to be the first one to,

155
00:08:40,700 --> 00:08:45,440
to create a generally intelligent system
and to do it in a way that is safe.

156
00:08:45,441 --> 00:08:47,700
So, um, I think that's
a good thing. There's,

157
00:08:47,701 --> 00:08:51,500
there's some real focus on that
and investment in that area.

158
00:08:51,501 --> 00:08:56,280
But at the same time there's also a lot
of hype people like Ilan saying some

159
00:08:56,281 --> 00:08:57,510
pretty over the top things.

160
00:08:57,550 --> 00:09:02,490
I do think that to some extent that's
a bit dangerous because it, again,

161
00:09:02,491 --> 00:09:05,250
this is something that is
probably pretty far in the future.

162
00:09:05,251 --> 00:09:09,250
I would say probably at
least 50 years of away. Um,

163
00:09:09,420 --> 00:09:11,010
there is a big debate over that.
Again,

164
00:09:11,011 --> 00:09:13,860
in my latest book I interviewed all
these people. I asked them this question,

165
00:09:14,220 --> 00:09:18,690
how soon are we going to have a computer
that is at the level of a human being

166
00:09:18,691 --> 00:09:23,691
in terms of intelligence
and the predictions I got
ranged from 10 years from Ray

167
00:09:24,480 --> 00:09:26,760
Kurzweil to nearly 200 years.
Wow.

168
00:09:26,780 --> 00:09:30,960
So there's a wide variation that the
average guest was about 80 years.

169
00:09:30,961 --> 00:09:32,640
So at the end of this century,
so pretty cool.

170
00:09:32,910 --> 00:09:35,750
What are the markers that cause people
to have a different response to that

171
00:09:35,751 --> 00:09:36,111
question?

172
00:09:36,111 --> 00:09:39,560
So why would someone like Kurzweil
say 10 and then someone else says 200

173
00:09:39,740 --> 00:09:42,770
well, you know, there are a number of
breakthroughs that you have to have.

174
00:09:42,771 --> 00:09:46,130
You have to have machines that can
learn the way people learn. Um,

175
00:09:46,131 --> 00:09:49,820
right now as I said, we've got
machine learning, deep learning,

176
00:09:49,821 --> 00:09:53,960
which is highly dependent on lots and
lots of data in particular labeled data.

177
00:09:53,961 --> 00:09:58,961
So you can train one of these algorithms
to recognize pictures of a dog and you

178
00:09:59,091 --> 00:10:03,020
would give it maybe a million
photographs that had, that was, you know,

179
00:10:03,080 --> 00:10:03,591
these photos.

180
00:10:03,591 --> 00:10:06,590
Would he labeled either there's a
dog there or there's not a dog there.

181
00:10:06,591 --> 00:10:11,280
And based on this he could
learn and eventually recognize
dogs that a superhuman

182
00:10:11,330 --> 00:10:16,160
level, but that's not the way a human
child learns, right? Um, a human child,

183
00:10:16,161 --> 00:10:16,431
you can,

184
00:10:16,431 --> 00:10:20,330
you can point to a dog and maybe you
only need to do that once before the kid

185
00:10:20,331 --> 00:10:20,931
needs to learn.

186
00:10:20,931 --> 00:10:25,040
And so one of the biggest initiatives
is teaching machines to learn from less

187
00:10:25,041 --> 00:10:29,600
data in, in an unsupervised way, in
the way that that people can. Um,

188
00:10:29,960 --> 00:10:33,830
and then you've got to have the ability
to think generally to conceive new ideas,

189
00:10:33,831 --> 00:10:35,120
to be creative,

190
00:10:35,510 --> 00:10:39,620
to understand that one thing causes
another thing is as opposed to just two

191
00:10:39,621 --> 00:10:44,621
things being correlated to
develop counterfactuals to
imagine I've got this plan

192
00:10:45,651 --> 00:10:47,750
for the future,
but if I tweak this one thing,

193
00:10:47,751 --> 00:10:49,340
then this is what's going to be different.

194
00:10:49,700 --> 00:10:53,870
These are all uniquely human ways of
thinking and it's going to take a lot of

195
00:10:53,871 --> 00:10:57,950
breakthroughs, um, before we have a
machine that can do all of those things.

196
00:10:57,951 --> 00:11:02,480
And there's just a lot of disagreement
even between the very smartest people

197
00:11:02,870 --> 00:11:04,730
working in this field,
um,

198
00:11:04,760 --> 00:11:07,880
about how long it's going to take for
those kinds of breakthroughs to happen.

199
00:11:08,110 --> 00:11:10,240
How concerned are you about,
uh,

200
00:11:10,450 --> 00:11:14,980
the unbiased thing that seems to be
happening when it comes to the algorithm?

201
00:11:15,190 --> 00:11:16,120
So for example,
you know,

202
00:11:16,121 --> 00:11:19,510
the famous case that everyone talks
about is that if you Google American

203
00:11:19,511 --> 00:11:22,540
scientists, that there
is, that it happens to be,

204
00:11:22,541 --> 00:11:26,710
it's just a function of things that most
American famous American scientists who

205
00:11:26,711 --> 00:11:30,460
have done most of the breakthroughs,
most, not all happened to be white men.

206
00:11:30,790 --> 00:11:33,580
But that Google is an
biasing the searches.

207
00:11:33,581 --> 00:11:36,610
So it includes more black people
are more women or things like that,

208
00:11:36,820 --> 00:11:38,170
which nobody has a problem with.

209
00:11:38,200 --> 00:11:40,930
No one in their right mind has a real
problem acknowledging that there are

210
00:11:41,080 --> 00:11:44,320
scientists of every color and
gender and all of those things. Um,

211
00:11:44,350 --> 00:11:46,720
but they're unbiased and things that are,
that are not.

212
00:11:46,810 --> 00:11:50,740
So it's not really factual sort of what
we're putting in the algorithm and that

213
00:11:50,741 --> 00:11:52,170
where that could lead us see,

214
00:11:52,340 --> 00:11:54,070
right. It's pretty, that's

215
00:11:54,310 --> 00:11:59,080
ultimately a decision for
society I suppose how we
want to address those issues.

216
00:11:59,081 --> 00:12:04,081
I remember the whole issue of bias
in algorithms is a huge issue in,

217
00:12:04,171 --> 00:12:05,260
in artificial intelligence.

218
00:12:05,261 --> 00:12:08,620
People are working on risking that
and that that operates in both ways.

219
00:12:08,621 --> 00:12:12,370
I mean there are definitely an
absolutely have been legitimate cases of

220
00:12:12,371 --> 00:12:16,630
algorithms that are biased against people
of color and so forth. For example. Um,

221
00:12:17,050 --> 00:12:21,880
and gender too. I mean, I know that um,
one company for example, stopped using,

222
00:12:22,270 --> 00:12:26,950
um, an AI system that we choose to screen
resumes because it was biased against

223
00:12:26,951 --> 00:12:29,570
women and so forth.
And there've been other things.

224
00:12:29,600 --> 00:12:33,250
Where does that bias come from in
a situation with bias? You know,

225
00:12:33,520 --> 00:12:37,870
what happens is that, again,
these are systems that are
learning from data, right?

226
00:12:38,140 --> 00:12:40,780
But where does that data come from
originally? It comes from people.

227
00:12:40,960 --> 00:12:45,250
So if people are biased in some way
and they're generating this data,

228
00:12:45,820 --> 00:12:48,130
then an algorithm, a machine
learning, I was driven,

229
00:12:48,131 --> 00:12:51,550
comes along and is trained on that data,
it will pick up that bias.

230
00:12:51,550 --> 00:12:54,340
So basically we're, we're the
flaw in the system. Absolutely.

231
00:12:54,341 --> 00:12:58,510
More than anything else really. Right.
Um, but there is a hopeful note there,

232
00:12:58,630 --> 00:13:03,040
which is that fixing bias in the
human being is very hard, right?

233
00:13:03,041 --> 00:13:03,670
I mean,

234
00:13:03,670 --> 00:13:07,600
we don't really know how to do that
and we know that it does exist to some

235
00:13:07,601 --> 00:13:11,710
extent, but fixing it in an algorithm
could be a lot easier. Right.

236
00:13:11,711 --> 00:13:15,270
It's basically tweaking some bits.
So as we bought,

237
00:13:15,271 --> 00:13:17,880
I guess it depends who's doing it
though, right? Exactly. As long as,

238
00:13:17,881 --> 00:13:21,190
as long as it's done in,
in, in a careful proper way.

239
00:13:21,310 --> 00:13:26,310
But we can't imagine a future
where algorithms as they
were employed more maybe

240
00:13:26,561 --> 00:13:30,910
as, as kind of a check on decisions
or maybe in some cases actually making

241
00:13:30,911 --> 00:13:35,500
decisions. Um, it can actually be a less
biased world and not a more biased world.

242
00:13:35,520 --> 00:13:36,011
But you're right,

243
00:13:36,011 --> 00:13:39,820
there are huge numbers of issues
running in both directions there. Um,

244
00:13:40,250 --> 00:13:43,010
well it's funny that that's sort of the
theme of all of this because I'm even

245
00:13:43,011 --> 00:13:47,480
trying to sort of figure out as you're
talking, are you optimistic about this or,

246
00:13:47,540 --> 00:13:50,420
or, or pessimistic about sort of
where this could all lead? And I,

247
00:13:50,540 --> 00:13:51,860
I definitely sense both sides.

248
00:13:52,280 --> 00:13:57,080
Yeah. I mean, I tend to be speaking
more holistically including, you know,

249
00:13:57,140 --> 00:13:59,450
there are many, many issues with Ai, um,

250
00:13:59,690 --> 00:14:02,990
things that we should be concerned
about biases. One security, the ability,

251
00:14:03,380 --> 00:14:06,610
the ability of people, bad people
to hack into a system and, and,

252
00:14:06,800 --> 00:14:09,290
and do evil things with it.
Um,

253
00:14:09,710 --> 00:14:13,880
the potential for weaponization is
another thing that many people are really,

254
00:14:13,881 --> 00:14:17,430
really worried about. The idea that
you can have autonomous weapons, um,

255
00:14:17,480 --> 00:14:22,480
and not just one autonomous weapon that
might independently kill people without

256
00:14:22,491 --> 00:14:25,910
a human in the loop, but do, you could
have thousands of them swarming, right,

257
00:14:25,911 --> 00:14:28,670
which be this guy.
And that truly terrifying.

258
00:14:28,740 --> 00:14:33,560
And this is something that you know,
is, is not really science fiction.

259
00:14:33,590 --> 00:14:33,801
I mean,

260
00:14:33,801 --> 00:14:38,120
we were talking earlier about super
intelligence and the terminator where the

261
00:14:38,121 --> 00:14:40,700
machine's actively or
making a choice to kill us,

262
00:14:41,060 --> 00:14:43,310
that science fiction that
lies far in the future,

263
00:14:43,610 --> 00:14:48,610
but the idea of having thousands of
swarming autonomous drones that were not

264
00:14:49,190 --> 00:14:50,510
intelligent independently,

265
00:14:50,511 --> 00:14:55,511
but we're programmed by somebody else
to do something to attack someone or so.

266
00:14:55,780 --> 00:14:57,740
So this is something that could happen.

267
00:14:57,820 --> 00:14:59,290
So the idea being that,
okay,

268
00:14:59,291 --> 00:15:02,890
Amazon moves to drone delivery and
then someone hacks into the system and

269
00:15:02,891 --> 00:15:05,740
instead of these drones
dropping packages at our door,

270
00:15:05,741 --> 00:15:07,570
they're flying through our windows and

271
00:15:07,830 --> 00:15:10,820
the attack on people on the streets. Or
it could be hacking or whatever. Yeah.

272
00:15:10,940 --> 00:15:13,010
Or it could be someone,
you know,

273
00:15:13,011 --> 00:15:17,540
manufacturing a huge numbers of these
drones and then installing autonomous

274
00:15:17,541 --> 00:15:22,070
software. Um, because you
know, the barrier to entry
here is pretty low. I mean,

275
00:15:22,071 --> 00:15:22,904
these are,

276
00:15:23,240 --> 00:15:26,690
these are weapons that some people could
be like weapons of mass disruption.

277
00:15:26,691 --> 00:15:31,190
If you had enough autonomous drones,
that would be incredibly dangerous.

278
00:15:31,191 --> 00:15:34,910
Right now if you look at something
like nuclear weapons, um,

279
00:15:34,970 --> 00:15:36,930
in order for our country to have
nuclear weapons that, you know,

280
00:15:36,960 --> 00:15:39,140
you've got to be a nation state,
you've got to have left, you know,

281
00:15:39,141 --> 00:15:42,680
resources on that level in order
to develop nuclear weapons.

282
00:15:42,710 --> 00:15:46,430
But these kinds of technologies
where you're talking, you know,

283
00:15:46,431 --> 00:15:48,020
and there's a lot of um,

284
00:15:48,050 --> 00:15:51,590
overlap between the commercial sector
and things that could be done on the

285
00:15:51,591 --> 00:15:54,590
security or military side.
You could go on Amazon,

286
00:15:54,591 --> 00:15:58,820
you could purchase a thousand drones
and then maybe you could, um, you know,

287
00:15:58,821 --> 00:16:02,420
engineered them to be, to be weapons
or something. These are something that,

288
00:16:02,780 --> 00:16:04,820
you know,
there's a much lower threshold there.

289
00:16:04,830 --> 00:16:07,580
People in the basement somewhere could
be doing this kind of stuff, right?

290
00:16:08,030 --> 00:16:12,680
So it is quite, well I think they
know already, but it is quite scary.

291
00:16:12,681 --> 00:16:15,300
And many people in the Ai community,

292
00:16:15,301 --> 00:16:17,090
they were very passionate
about this in particular.

293
00:16:17,100 --> 00:16:21,260
There there's an initiative in the
United Nations to actually been fully

294
00:16:21,261 --> 00:16:22,830
autonomous weapons for example.
And,

295
00:16:22,860 --> 00:16:27,290
and the real worry is not
just the military is would
use these kinds of weapons,

296
00:16:27,860 --> 00:16:30,440
but it would go beyond that
and you would have, you know,

297
00:16:30,441 --> 00:16:35,000
there's kind of shady arms dealers
that now sell machine guns are selling

298
00:16:35,060 --> 00:16:37,340
autonomous drones.
Um,

299
00:16:37,341 --> 00:16:40,610
and so that they then become available
to terrorists and all kinds of people.

300
00:16:40,611 --> 00:16:44,290
And this is, this is a really
scary scenario. One of the
people I interviewed, uh,

301
00:16:44,840 --> 00:16:48,770
uh, Stuart Russell, who's
a professor at UC Berkeley,

302
00:16:49,010 --> 00:16:51,830
created a youtube video
code slaughter bots.

303
00:16:51,870 --> 00:16:56,060
You can go on and watch and it's really
quite terrifying and it shows you

304
00:16:56,061 --> 00:17:01,061
exactly what could be done with huge
numbers of swarming autonomous drones.

305
00:17:01,131 --> 00:17:03,740
And it's not, again,
it's not science fiction.

306
00:17:03,741 --> 00:17:07,430
It's something that could happen
in the next five, 10 years. Do you,

307
00:17:07,760 --> 00:17:11,930
uh, are you familiar at all with just
sort of the anti technology movement?

308
00:17:12,020 --> 00:17:15,380
The more that you pay attention to the
technology movement and the amount of

309
00:17:15,381 --> 00:17:19,010
people that are trying to either get
off the grid or limit the amount of time

310
00:17:19,011 --> 00:17:20,390
online and,
and that whole thing,

311
00:17:20,680 --> 00:17:22,870
right? I mean, I, you know, that,

312
00:17:22,871 --> 00:17:27,340
that's I think a natural response to
a lot of this. I mean, uh, the, the,

313
00:17:28,060 --> 00:17:31,910
the worst example of that is, is Ted
Kaczynski, right? The Unabomber, right?

314
00:17:32,020 --> 00:17:35,260
Who actually wrote a manifesto that he,

315
00:17:35,261 --> 00:17:37,540
he was published I think
in the New York Times.

316
00:17:38,020 --> 00:17:41,920
But if you go and read that manifesto,
this is guided. Okay. He's crazy, right?

317
00:17:41,921 --> 00:17:43,240
He's a murderer,
all of this.

318
00:17:43,810 --> 00:17:47,320
But if you read that manifesto and
not know that it was written by him,

319
00:17:47,321 --> 00:17:51,260
he's raising a lot of the issues that
we are now talking about. You know,

320
00:17:51,300 --> 00:17:54,370
the issues that technology
could be a threat. Um,

321
00:17:54,420 --> 00:17:59,340
the issue that we might
become so dependent on this
technology that we lose our

322
00:17:59,400 --> 00:18:04,200
agency, right? Or ability to think
for ourselves. Um, and so far,

323
00:18:04,201 --> 00:18:07,410
so, you know, even back then these people,
like we're, we're thinking about this.

324
00:18:07,410 --> 00:18:11,910
And so this is a natural response in one
of the things I fear the most is that

325
00:18:12,900 --> 00:18:17,900
if we don't find a way to adapt to these
technologies and find a way to leverage

326
00:18:18,241 --> 00:18:21,540
all this progress on behalf of everyone
so that everyone is better off,

327
00:18:21,900 --> 00:18:24,060
there's going to be a
bigger and bigger backlash.

328
00:18:24,420 --> 00:18:26,550
People are going to turn
against the technology. Um,

329
00:18:26,670 --> 00:18:31,320
and that might mean not just going
off the grid and living as a hermit,

330
00:18:31,350 --> 00:18:36,000
but actually be, you know, becoming
much more adversarial to the system.

331
00:18:36,030 --> 00:18:38,040
And that might happen politically.

332
00:18:38,041 --> 00:18:42,750
It might happen in some places even in
the form of social unrest and so forth as

333
00:18:42,751 --> 00:18:45,060
things get, get bad enough if
we really have unemployment.

334
00:18:45,061 --> 00:18:48,600
So I just think it's critically important
that we begin to really address these

335
00:18:48,601 --> 00:18:52,530
issues and have a honest discussion
about them so that we can avoid that

336
00:18:52,531 --> 00:18:57,030
scenario. I assume you're a fan of black
mirror. I haven't really watched that,

337
00:18:57,031 --> 00:18:59,400
but I've heard a lot about
it. But um, but yeah,

338
00:18:59,450 --> 00:19:03,810
those kinds of scenarios or you know,
science fiction now,

339
00:19:03,811 --> 00:19:06,630
but they are every day becoming reality.

340
00:19:06,670 --> 00:19:10,150
Yeah. Is there a Scifi movie
that you think handles some
of this in the best way?

341
00:19:10,151 --> 00:19:12,400
So not going all the way
to terminator tomorrow,

342
00:19:12,401 --> 00:19:16,150
but like there's some movies that you
think of sort of teased out some of the

343
00:19:16,460 --> 00:19:18,420
more realistic features or closer,

344
00:19:18,520 --> 00:19:21,190
yeah, there, there are
several. There's one movie, um,

345
00:19:21,430 --> 00:19:23,320
a few years ago called Elysium,

346
00:19:23,870 --> 00:19:27,010
which really got at the issue of
inequality because what happen,

347
00:19:27,310 --> 00:19:28,560
oh,
that's the one with the,

348
00:19:28,620 --> 00:19:33,430
they built this love artificial sugar
and all the rich people migrated.

349
00:19:33,431 --> 00:19:36,580
Aaron Earth became, became basically, um,

350
00:19:38,030 --> 00:19:40,720
you know, I dystopian nightmare. All the,

351
00:19:40,740 --> 00:19:43,360
all the regular people were
stuck on earth. And that's,

352
00:19:43,361 --> 00:19:47,140
you're seeing that already of course
with wealthy people moving to gated

353
00:19:47,141 --> 00:19:50,920
communities and so forth and elite cds
like San Francisco where things are

354
00:19:50,921 --> 00:19:52,660
becoming so unequal.

355
00:19:52,661 --> 00:19:56,740
And I really worry that that's the kind
of future we could have if we don't

356
00:19:56,741 --> 00:20:00,760
adapt to this where you literally have
got a small number of incredibly wealthy

357
00:20:00,761 --> 00:20:04,060
people that are benefiting from this
technology and are maybe using the

358
00:20:04,061 --> 00:20:07,540
technology to protect themselves
from the masses. Right.

359
00:20:07,541 --> 00:20:09,160
And everyone else is
literally left behind.

360
00:20:09,180 --> 00:20:12,420
So that that's sort of the ultimate irony
of what's happening in Silicon Valley,

361
00:20:12,421 --> 00:20:14,580
right? I mean, you just said
it. It's like San Francisco,

362
00:20:14,581 --> 00:20:18,090
they've got all these great minds are
up there creating all of this incredible

363
00:20:18,091 --> 00:20:20,130
technology,
absurd amounts of wealth.

364
00:20:20,430 --> 00:20:22,380
And then if you go out on
the streets of San Francisco,

365
00:20:22,381 --> 00:20:24,600
the amount of homelessness is,
is through the roof.

366
00:20:24,930 --> 00:20:26,880
I mean crime and everything else.
San Francisco

367
00:20:26,880 --> 00:20:31,350
and the bay area is ground zero
for this technological revolution.

368
00:20:31,860 --> 00:20:35,850
And then right in their backyard, you
see, you see the inequality, right?

369
00:20:35,851 --> 00:20:39,930
You see what's happening and um, this
is something that's going to scale out,

370
00:20:39,960 --> 00:20:42,780
right. Um, to everywhere basically. Um,

371
00:20:43,110 --> 00:20:47,650
so we really need to get control
of that. Um, and if we don't, it's,

372
00:20:47,680 --> 00:20:50,740
I think it's got a tear off society apart,
right?

373
00:20:50,741 --> 00:20:55,741
It's going to ultimately lead to some
real problems in the United States and in

374
00:20:56,051 --> 00:20:59,380
other countries that are less
stable, that have less, you know,

375
00:20:59,830 --> 00:21:02,980
solid institutions that we have.
It, it could be even worse.

376
00:21:02,981 --> 00:21:06,580
You're going to see governments fall and
things like that in some countries as a

377
00:21:06,581 --> 00:21:10,330
result of this. I think so it's really
something to be concerned about. We need,

378
00:21:10,331 --> 00:21:12,400
we need to have some sort of a plan.

379
00:21:12,790 --> 00:21:16,270
Yeah. Well that's what everyone's
trying to figure out right now is,

380
00:21:16,300 --> 00:21:17,320
is what is that plan

381
00:21:17,890 --> 00:21:18,371
exactly.

382
00:21:18,371 --> 00:21:22,780
And I think this is one of the biggest
challenges we're gonna face in the future.

383
00:21:22,781 --> 00:21:23,614
You know,
this is,

384
00:21:23,930 --> 00:21:27,880
AI is going to be one of the primary
forces that shapes the future is going to

385
00:21:27,881 --> 00:21:29,170
be incredibly disruptive.

386
00:21:29,560 --> 00:21:32,920
And of course it's going
to happen in parallel with
other things. Climate Change,

387
00:21:33,370 --> 00:21:37,780
um, geo politically, the rise
of China, um, migration, right?

388
00:21:37,810 --> 00:21:39,790
These are all huge things
that are happening already.

389
00:21:40,240 --> 00:21:42,550
All these things are
coming at us in parallel.

390
00:21:42,551 --> 00:21:46,450
They're going to hit all together and
I really worry about kind of a perfect

391
00:21:46,451 --> 00:21:49,520
storm. So we really need to begin
to get a handle on all that.

392
00:21:49,630 --> 00:21:53,080
How does the information war
factor into all of this? You know,

393
00:21:53,081 --> 00:21:55,360
one of the things that people that watch
this show are always taught, you know,

394
00:21:55,510 --> 00:21:58,570
everyone's talking about fake news all
the time or that were just being handed

395
00:21:58,571 --> 00:21:59,410
things.
You know,

396
00:21:59,411 --> 00:22:03,220
the algorithm pushes us stories that
are favorable maybe to one side of the

397
00:22:03,221 --> 00:22:05,110
political aisle or something like that.

398
00:22:05,111 --> 00:22:10,111
And then we're all going to also siphon
off into our own informational realities

399
00:22:10,301 --> 00:22:13,890
basically, and sort of will live in the
same physical world, but digitally we're

400
00:22:14,110 --> 00:22:17,110
going to just accept different
truths. Exactly. And that, that,

401
00:22:17,220 --> 00:22:20,700
that's what makes it even more scary as
to all of this disruption is coming at

402
00:22:20,701 --> 00:22:24,240
us at a time when we are
just incredibly polarized,

403
00:22:24,270 --> 00:22:27,000
where to some extent we're
living in different universes.

404
00:22:27,690 --> 00:22:32,100
Our ability to even talk to each other,
it seems to be limited.

405
00:22:32,101 --> 00:22:34,860
How are we going to, you
know, address these issues?

406
00:22:34,861 --> 00:22:39,570
How are we going to respond to these
incredibly disruptive forces when we can't

407
00:22:39,571 --> 00:22:43,740
even sit down and have a conversation
and agree on the same facts? Right. Um,

408
00:22:43,790 --> 00:22:44,820
that's a real problem.
And,

409
00:22:44,821 --> 00:22:49,140
and there is evidence that it's getting
worse and worse. That's largely the, the,

410
00:22:49,230 --> 00:22:52,680
and that's why I'm doing this. This is,
so this is my little firewall right here.

411
00:22:52,740 --> 00:22:53,281
Exactly.

412
00:22:53,281 --> 00:22:56,970
I hope that there can be more of this
because we really need everyone and that

413
00:22:56,971 --> 00:23:00,900
includes people on the left and people
on the right to be able to talk to each

414
00:23:00,901 --> 00:23:04,560
other about this because otherwise we're
going to have what we have now we've

415
00:23:04,561 --> 00:23:07,290
got to, we've got a congress
that literally can do nothing.

416
00:23:07,620 --> 00:23:11,700
And I'm concerned no matter who
wins in 2020 the presidency,

417
00:23:12,410 --> 00:23:17,190
what, you know, it's likely that the
congress will still be divided, right?

418
00:23:17,191 --> 00:23:22,191
The same political polarization and
social polarization and social media

419
00:23:22,291 --> 00:23:25,010
polarization will be there. So how are
we going to address these kinds of,

420
00:23:25,180 --> 00:23:26,770
so that gets to what you
were talking about earlier,

421
00:23:26,771 --> 00:23:30,460
that you'd need something sort of separate
from the government in a way to sort

422
00:23:30,461 --> 00:23:30,870
of be,
if,

423
00:23:30,870 --> 00:23:35,870
if something could oversee some of our
ability to deal with this technology,

424
00:23:36,220 --> 00:23:36,701
it almost,

425
00:23:36,701 --> 00:23:41,290
in a way it can't be politicized
because of the way our system is,

426
00:23:41,840 --> 00:23:44,870
right? I mean I, it's stagnant
in terms of having a basic,

427
00:23:45,170 --> 00:23:48,140
I think there are good reasons to
put that in the hands of a separate

428
00:23:48,141 --> 00:23:51,470
institution. And the Federal Reserve
would be a good example of that.

429
00:23:51,471 --> 00:23:54,710
You've seen the Federal Reserve,
which controls interest rates, right?

430
00:23:55,280 --> 00:23:59,270
Is relatively independent right now. Um,
although, you know, Trump has tried to,

431
00:24:00,110 --> 00:24:01,790
to, to play around with
that. But you know,

432
00:24:01,791 --> 00:24:05,150
if it weren't for the fact that we
had an independent Federal Reserve,

433
00:24:05,151 --> 00:24:10,140
I think we would be in much bigger
trouble now than we are. Um, and so there,

434
00:24:10,160 --> 00:24:14,930
there is an argument for maybe taking
some essential functions of government

435
00:24:14,990 --> 00:24:19,730
away from the political process and having
a kind of a technocratic approach to

436
00:24:19,731 --> 00:24:20,300
that.

437
00:24:20,300 --> 00:24:22,850
All right,
so my last question will be a two parter.

438
00:24:23,510 --> 00:24:27,830
Paint me a future if we get some of
this under control and we deal with this

439
00:24:27,831 --> 00:24:31,460
technology maturely and
properly give me sort of where,

440
00:24:31,461 --> 00:24:33,290
what do we look like in about 10 years?

441
00:24:33,350 --> 00:24:37,040
And then if we lose control and we don't
do the proper things and don't have the

442
00:24:37,041 --> 00:24:39,560
proper firewalls, what does the
future look like? In 10 years?

443
00:24:40,010 --> 00:24:44,870
Well, I think that, you know,
maybe looking even beyond
10 years, we know one that,

444
00:24:45,260 --> 00:24:48,240
how far do you want to go that say 15,
20 years ago.

445
00:24:48,290 --> 00:24:51,560
But I think that within
that kind of a timeframe,

446
00:24:52,130 --> 00:24:55,340
there's going to be an unambiguous
impact on, on the job market.

447
00:24:55,341 --> 00:24:59,150
So if we don't get control of this,
we will see unemployment,

448
00:24:59,151 --> 00:25:00,560
at least among some workers.

449
00:25:00,790 --> 00:25:04,400
We will see greatly increased
inequality even beyond what we see now.

450
00:25:04,401 --> 00:25:08,990
We will see more and more anger,
um, more people left behind,

451
00:25:09,050 --> 00:25:11,840
possibly even social unrest.
As a result of that,

452
00:25:12,260 --> 00:25:14,690
we will see the rise of people like Trump,

453
00:25:14,691 --> 00:25:17,120
which you might characterize
as kind of a demagogue,

454
00:25:17,121 --> 00:25:21,870
someone that preys on the fear
that people have, right? Um,

455
00:25:21,950 --> 00:25:22,880
maybe,
you know,

456
00:25:22,881 --> 00:25:27,881
points to two immigrants as opposed to
technology as being the primary cause of

457
00:25:28,311 --> 00:25:29,180
this and so forth.
Right?

458
00:25:29,180 --> 00:25:30,730
Those other people are stealing your,

459
00:25:31,070 --> 00:25:36,070
it's a lot easier to always point to
another human being than it is to point to

460
00:25:37,181 --> 00:25:42,010
an intangible force like technology. So
we could have a future where, you know,

461
00:25:42,220 --> 00:25:46,030
everything is just very, very ugly. On a
lot of people are, are really struggling,

462
00:25:46,690 --> 00:25:48,810
uh, me on that future. And,

463
00:25:48,930 --> 00:25:51,760
and the other part of that though is
that there's also an economic aspect to

464
00:25:51,761 --> 00:25:55,390
that, that, that as people are
unemployed or have lower incomes,

465
00:25:55,391 --> 00:25:59,140
they have less money to spend right
now they're not driving the economy,

466
00:25:59,141 --> 00:26:02,960
so the whole economy suffers.
So we could have also a financial crisis,

467
00:26:02,961 --> 00:26:04,460
the recession,
um,

468
00:26:04,510 --> 00:26:08,140
perhaps people can't pay their debts and
we get into a situation like we had in

469
00:26:08,650 --> 00:26:11,710
2008. Right? So that's, that's
sort of the worst case scenario.

470
00:26:12,100 --> 00:26:15,250
The best case scenario is that
we find a way to adapt to this.

471
00:26:15,280 --> 00:26:16,990
Maybe it was something
like a basic income.

472
00:26:16,990 --> 00:26:21,490
So we addressed this issue of people not
having jobs or not having an adequate

473
00:26:21,491 --> 00:26:24,760
incomes.
So they still have money to spend.

474
00:26:25,060 --> 00:26:27,250
They go out and they spend
their money in the economy.

475
00:26:27,251 --> 00:26:31,000
There are all kinds of new products and
services and exciting things for people

476
00:26:31,001 --> 00:26:35,020
to spend money on. There is incredible
opportunity for entrepreneurs,

477
00:26:35,021 --> 00:26:39,700
for people like Elon Musk's or the next
Steve and Steve jobs to create things.

478
00:26:40,150 --> 00:26:44,450
Um, based on this new technology and
people have income that they need to,

479
00:26:44,470 --> 00:26:48,530
to buy these products. Um, things do, um,

480
00:26:48,570 --> 00:26:52,020
in large measure get less
expensive. So people you know, have,

481
00:26:52,021 --> 00:26:54,900
have greater purchasing power.
Um,

482
00:26:55,140 --> 00:26:59,820
we have enormous breakthroughs
in science and medicine. Um,

483
00:26:59,850 --> 00:27:02,100
we all live longer and healthier lives.

484
00:27:02,460 --> 00:27:06,300
So there are enormous benefits
to artificial intelligence.

485
00:27:06,301 --> 00:27:10,350
It's going to become the primary tool
that's used in scientific research and

486
00:27:10,351 --> 00:27:15,150
solving problems like climate change,
developing new forms of clean energy,

487
00:27:15,151 --> 00:27:16,860
you know, medical
breakthroughs, all of that.

488
00:27:17,250 --> 00:27:21,630
The key thing is that we want to make
sure that we get that stuff and then we

489
00:27:21,631 --> 00:27:23,340
get it for everyone.
In other words,

490
00:27:23,341 --> 00:27:26,550
we want to be able to leverage it on
behalf of everyone rather than just a few

491
00:27:26,551 --> 00:27:30,300
people. And I think that if we can
find a way to navigate through this,

492
00:27:30,360 --> 00:27:33,600
that it's incredibly, um, optimistic.

493
00:27:33,601 --> 00:27:37,350
And where it kind of ends up in the far
future is maybe something like Star Trek,

494
00:27:37,380 --> 00:27:37,971
right?
Where,

495
00:27:37,971 --> 00:27:42,300
where you've got what has been called
kind of the post scarcity economy,

496
00:27:42,340 --> 00:27:46,980
an economy of abundance where you know
people don't have to worry about the

497
00:27:46,981 --> 00:27:51,660
basics of life anymore than people you
know, focused on other things, right?

498
00:27:51,661 --> 00:27:54,570
I mean, and star track, you've
got to materialize or right you,

499
00:27:55,110 --> 00:27:56,310
everything is basically free.

500
00:27:56,311 --> 00:27:58,620
People don't have to work a
nine to five job to survive.

501
00:27:58,660 --> 00:28:01,320
They are out traveling
the universe or whatever.

502
00:28:01,770 --> 00:28:05,190
I'm doing things that are genuinely
meaningful to them and I think that's sort

503
00:28:05,191 --> 00:28:10,191
of the vision that we should have as
we anticipate the development of these

504
00:28:10,501 --> 00:28:14,270
technologies. But in order
to get there, we've got

505
00:28:14,320 --> 00:28:16,810
to be honest,
I'm serious lifter

506
00:28:17,090 --> 00:28:19,310
what the implications of this is.

507
00:28:19,311 --> 00:28:21,320
We need to have an honest
discussion and come up,

508
00:28:21,650 --> 00:28:25,730
I think ultimately with some real
policies to address the risks and the

509
00:28:25,731 --> 00:28:30,080
downsides that are going to come
with this progress. So we shall see.

510
00:28:30,620 --> 00:28:33,650
I hope these are the three words that'll
start to take us out of this one.

511
00:28:33,920 --> 00:28:35,450
We should probably do this every year.

512
00:28:35,451 --> 00:28:39,680
You want to do one of these every year
and just pick up on all the incremental

513
00:28:39,681 --> 00:28:42,470
progress. And then I suppose if
everything you're saying is right,

514
00:28:42,800 --> 00:28:46,190
one year we're going to do
it and everything will look
so absolutely different.

515
00:28:46,310 --> 00:28:49,760
We won't even be able to look back and
make any sense of what we're talking

516
00:28:49,761 --> 00:28:52,310
about. Once we get past that singularity,
you're going to be different.

517
00:28:52,670 --> 00:28:54,860
I look forward to it
and for more on Martin,

518
00:28:54,861 --> 00:28:58,640
you can follow him on
Twitter at m forward future.

