WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.526
Hey, I'm John McCutchan,
a software engineer

00:00:02.526 --> 00:00:04.025
working on the Dart
virtual machine.

00:00:04.025 --> 00:00:06.870
In this talk, I'm going to
introduce the Dart Observatory,

00:00:06.870 --> 00:00:08.970
a powerful tool for
profiling and analyzing

00:00:08.970 --> 00:00:10.470
Dart applications.

00:00:10.470 --> 00:00:13.860
Finally, I will explain how to
use the CPU Profiler to guide

00:00:13.860 --> 00:00:15.780
you while optimizing Dart code.

00:00:15.780 --> 00:00:18.160
This talk is aimed at an
intermediate Dart programmer

00:00:18.160 --> 00:00:20.920
who wants to make
her programs fly.

00:00:20.920 --> 00:00:23.710
Observatory is a powerful tool
for analyzing and profiling

00:00:23.710 --> 00:00:24.656
running Dart programs.

00:00:24.656 --> 00:00:26.280
It's a new type of
tool which gives you

00:00:26.280 --> 00:00:28.650
an always-live, hyperlinked
view into the running

00:00:28.650 --> 00:00:30.670
state of your program.

00:00:30.670 --> 00:00:33.610
Presently, Observatory
consists of six tools.

00:00:33.610 --> 00:00:35.650
The first is a
View and Evaluate.

00:00:35.650 --> 00:00:37.880
You can view any
object inside the VM--

00:00:37.880 --> 00:00:41.860
an isolate, a library, a class,
or any instance of a class.

00:00:41.860 --> 00:00:43.690
You can also
evaluate expressions

00:00:43.690 --> 00:00:46.480
in the context of these
objects, including expressions

00:00:46.480 --> 00:00:49.250
that modify the object
that you're looking at.

00:00:49.250 --> 00:00:52.150
There's Code Coverage, which you
can view alongside the source

00:00:52.150 --> 00:00:53.720
directly in Observatory.

00:00:53.720 --> 00:00:55.840
The CPU Profiler is
always on, always

00:00:55.840 --> 00:00:58.650
sampling the running program,
collecting statistics

00:00:58.650 --> 00:01:00.910
about where it's executing.

00:01:00.910 --> 00:01:04.269
The Allocation Profile collects
statistics for each class,

00:01:04.269 --> 00:01:06.840
allowing you to figure out how
many instances your program

00:01:06.840 --> 00:01:08.910
allocates of each one.

00:01:08.910 --> 00:01:11.190
Now, we can give you a
stack trace at any time.

00:01:11.190 --> 00:01:13.160
And this is a snapshot
of the call stack,

00:01:13.160 --> 00:01:14.870
including local stack
variables, which

00:01:14.870 --> 00:01:18.080
you can view and manipulate even
after the program has continued

00:01:18.080 --> 00:01:18.890
running.

00:01:18.890 --> 00:01:23.120
And if you're interested, you
can even visualize the Heap.

00:01:23.120 --> 00:01:26.032
Code Coverage is an important
tool in testing your program.

00:01:26.032 --> 00:01:27.990
That is, if there are
code paths that are never

00:01:27.990 --> 00:01:29.770
executed by your
tests, you don't really

00:01:29.770 --> 00:01:32.300
know if those code paths
are correct or not.

00:01:32.300 --> 00:01:34.970
Now, Observatory gives you
up-to-date code coverage

00:01:34.970 --> 00:01:36.540
on demand.

00:01:36.540 --> 00:01:39.870
Code Coverage is displayed next
to your program source lines.

00:01:39.870 --> 00:01:42.940
A green box indicates that
the code is executable and has

00:01:42.940 --> 00:01:45.620
executed, whereas
a red box indicates

00:01:45.620 --> 00:01:48.232
that the code is executable
but hasn't executed.

00:01:48.232 --> 00:01:49.440
And this is not a good thing.

00:01:49.440 --> 00:01:51.820
If you see a lot of red boxes
after you've run your test

00:01:51.820 --> 00:01:54.110
suite, you don't
have the coverage

00:01:54.110 --> 00:01:56.620
necessary to really know for
certain that your program is

00:01:56.620 --> 00:01:57.950
correct.

00:01:57.950 --> 00:02:00.460
Lines without a green or
red box are not executable.

00:02:00.460 --> 00:02:03.460
Typically, these are
comments or other white space

00:02:03.460 --> 00:02:04.880
inside your program.

00:02:04.880 --> 00:02:07.240
Remember, Code Coverage
can be refreshed on demand,

00:02:07.240 --> 00:02:08.880
so even in the middle of
running you test suite,

00:02:08.880 --> 00:02:10.421
you can go ahead
and just take a peek

00:02:10.421 --> 00:02:13.270
and see how much coverage
you've got already.

00:02:13.270 --> 00:02:15.190
Observatory's Allocation
Profile gives you

00:02:15.190 --> 00:02:18.460
live, up-to-date statistics
about what type of objects

00:02:18.460 --> 00:02:20.640
your program is allocating.

00:02:20.640 --> 00:02:22.450
The Allocation Profile
displays information

00:02:22.450 --> 00:02:24.100
about both old and new space.

00:02:24.100 --> 00:02:26.750
And this information is
displayed the same for each.

00:02:26.750 --> 00:02:28.520
So we're just going
to focus on new space.

00:02:28.520 --> 00:02:30.520
So you can see on the
slide, we have a pie chart

00:02:30.520 --> 00:02:33.310
showing how much of the space
is used and how much of it

00:02:33.310 --> 00:02:34.540
is free.

00:02:34.540 --> 00:02:36.580
And alongside the
pie chart, we have

00:02:36.580 --> 00:02:39.070
a table of statistics about
collections that have occurred

00:02:39.070 --> 00:02:41.386
in this space-- the total
number of collections that

00:02:41.386 --> 00:02:43.330
have occurred since
the program started,

00:02:43.330 --> 00:02:44.990
the average time each
collection took--

00:02:44.990 --> 00:02:47.240
that's the amount of time
that your program has paused

00:02:47.240 --> 00:02:50.230
for when we're doing the GC
pause-- and the cumulative time

00:02:50.230 --> 00:02:52.650
that all of the
GCs in this space

00:02:52.650 --> 00:02:56.050
has taken out of your
program's execution time.

00:02:56.050 --> 00:03:00.000
Now, below the pie chart is a
table showing for each class

00:03:00.000 --> 00:03:02.849
how many instances your
program has allocated,

00:03:02.849 --> 00:03:05.015
how many are alive in a
moment, and how many of them

00:03:05.015 --> 00:03:07.100
have survived the last GC.

00:03:07.100 --> 00:03:09.090
Remember, in the
Allocation Profiler,

00:03:09.090 --> 00:03:10.885
you can go ahead and
manually trigger GCs,

00:03:10.885 --> 00:03:12.260
so you can be
certain that you're

00:03:12.260 --> 00:03:16.050
looking at up-to-date
memory usage information.

00:03:16.050 --> 00:03:17.510
Reducing the number
of allocations

00:03:17.510 --> 00:03:19.968
your program performs can have
a big impact on performance.

00:03:19.968 --> 00:03:22.080
Because by allocating
fewer objects,

00:03:22.080 --> 00:03:24.490
you're going to incur
shorter and less frequent

00:03:24.490 --> 00:03:28.990
GC pauses, which is always good
for your program's performance.

00:03:28.990 --> 00:03:32.030
Now let's take a quick look
at an Observatory CPU Profile

00:03:32.030 --> 00:03:34.640
capture of a game that
uses the popular 2D physics

00:03:34.640 --> 00:03:36.760
library, Box2D.

00:03:36.760 --> 00:03:39.400
Dart's Profiler aggregates
samples into buckets

00:03:39.400 --> 00:03:40.790
that we call Tags.

00:03:40.790 --> 00:03:43.270
And the root of this
tree is the Default Tag

00:03:43.270 --> 00:03:46.970
that is set automatically when
your program begins executing.

00:03:46.970 --> 00:03:50.690
If we expanded the Default
Tag and we see the VM Tags.

00:03:50.690 --> 00:03:53.020
And VM Tags are
categories of execution

00:03:53.020 --> 00:03:55.910
that the VM sets while
your program is running.

00:03:55.910 --> 00:03:58.490
So sometimes, the VM has to
transition over and compile

00:03:58.490 --> 00:04:01.410
some functions that have
never been executed before.

00:04:01.410 --> 00:04:03.520
And so you'll see the
Compile Tag show up.

00:04:03.520 --> 00:04:06.010
But we can see from
this profile that 95%

00:04:06.010 --> 00:04:08.405
of the time, the
Script Tag is set.

00:04:08.405 --> 00:04:10.030
And that's good,
because the Script Tab

00:04:10.030 --> 00:04:13.000
is set when we're actually
executing Dart code.

00:04:13.000 --> 00:04:17.279
Now below Script, we see that
the next highest tag is 3%,

00:04:17.279 --> 00:04:18.909
and that's GC New Space.

00:04:18.909 --> 00:04:21.200
So right away, we can see
that this program is spending

00:04:21.200 --> 00:04:25.940
95% of its time executing, and
only 3% of its time garbage

00:04:25.940 --> 00:04:26.860
collecting new space.

00:04:26.860 --> 00:04:30.190
And they're spending
little time elsewhere.

00:04:30.190 --> 00:04:34.730
Now expanding Script shows us
the hot functions in Box2D.

00:04:34.730 --> 00:04:37.500
We see at the top solve velocity
constraints, solve position

00:04:37.500 --> 00:04:39.390
constraints, sin, and cos.

00:04:39.390 --> 00:04:41.910
The seven functions highlighted
on this slide account

00:04:41.910 --> 00:04:45.580
for more than 40% of
all CPU time used.

00:04:45.580 --> 00:04:47.122
These are some
pretty hot functions.

00:04:47.122 --> 00:04:49.080
But sometimes you're
going to look at a profile

00:04:49.080 --> 00:04:50.621
and there won't be
any hot functions.

00:04:50.621 --> 00:04:52.320
And we call this a flat profile.

00:04:52.320 --> 00:04:54.680
Now in this case, you
can use custom tags

00:04:54.680 --> 00:04:59.010
to help aggregate and reveal
where you're spending time.

00:04:59.010 --> 00:05:02.074
If we want to see who called
solve velocity constraints,

00:05:02.074 --> 00:05:03.490
we can expand it's
node and we can

00:05:03.490 --> 00:05:07.450
see that 100% of the time
it's called from island.solve.

00:05:07.450 --> 00:05:10.190
And further if we want to see
who called the island.solve,

00:05:10.190 --> 00:05:13.030
we can expand its node and we
can see that 100% of the time,

00:05:13.030 --> 00:05:14.780
it's called from world.solve.

00:05:14.780 --> 00:05:17.010
So we can already see
that Box2D is spending

00:05:17.010 --> 00:05:19.680
most of its time solving the
world's velocity and position

00:05:19.680 --> 00:05:20.590
constraints.

00:05:20.590 --> 00:05:23.140
And if your game is suffering
from a performance problem,

00:05:23.140 --> 00:05:26.504
looking at the constraints you
use is the obvious next step.

00:05:26.504 --> 00:05:27.920
With only a few
clicks, we've been

00:05:27.920 --> 00:05:31.000
able to gain a high-level
understanding of how

00:05:31.000 --> 00:05:33.150
the program is
running inside the VM

00:05:33.150 --> 00:05:35.270
and what functions are hot.

00:05:35.270 --> 00:05:36.830
Now that we've done
a quick overview,

00:05:36.830 --> 00:05:38.220
let's take a
slightly deeper look

00:05:38.220 --> 00:05:40.837
at the data displayed
by the Profiler.

00:05:40.837 --> 00:05:42.420
The Observatory
Profiler is structured

00:05:42.420 --> 00:05:45.100
as a tree, rooted with the
categories of execution which

00:05:45.100 --> 00:05:47.330
we call tags,
followed by functions.

00:05:47.330 --> 00:05:49.940
You can configure how
the tree is displayed,

00:05:49.940 --> 00:05:53.480
putting VM Tags before User
Tags, or the other way around.

00:05:53.480 --> 00:05:57.490
Or you can hide either set of
tags if you're not interested.

00:05:57.490 --> 00:06:00.520
Above the Profile tree, you'll
notice a summary is displayed.

00:06:00.520 --> 00:06:02.700
In this case, the
samples were collected

00:06:02.700 --> 00:06:05.870
over two minutes and 11
seconds of wall clock time.

00:06:05.870 --> 00:06:11.260
We collected 120,000
samples at 1,000 Hertz.

00:06:11.260 --> 00:06:14.640
This is also where you configure
how the tags are displayed.

00:06:14.640 --> 00:06:17.190
Below the summary, you'll
see the Profiler tree.

00:06:17.190 --> 00:06:19.200
Each row in the
tree is either a tag

00:06:19.200 --> 00:06:20.790
or a function whose
name is displayed

00:06:20.790 --> 00:06:22.350
in the center of the tree.

00:06:22.350 --> 00:06:23.900
For each tag or
function, there's

00:06:23.900 --> 00:06:25.430
two numbers associated with it.

00:06:25.430 --> 00:06:27.900
The first number displayed
to the left of the name

00:06:27.900 --> 00:06:29.995
is the percentage of
its parent's node.

00:06:29.995 --> 00:06:32.370
And the second number displayed
on the right of the table

00:06:32.370 --> 00:06:35.630
is the global
exclusive CPU usage.

00:06:35.630 --> 00:06:38.990
Here, we've expanded the
Default Tag and the Script Tag

00:06:38.990 --> 00:06:41.680
and you can see that solve
velocity constraints is

00:06:41.680 --> 00:06:47.900
8.92% of all script execution
and 8.6% of global CPU usage.

00:06:47.900 --> 00:06:51.510
Because the Script Tag is so
close to 100% of the program's

00:06:51.510 --> 00:06:54.404
execution time, these two
numbers are really close.

00:06:54.404 --> 00:06:56.320
Otherwise, there might
be a larger discrepancy

00:06:56.320 --> 00:06:58.276
between the two.

00:06:58.276 --> 00:07:00.070
This screen shot
shows the call tree

00:07:00.070 --> 00:07:02.025
expanded to show more
of the call chain

00:07:02.025 --> 00:07:03.970
to solve velocity constraints.

00:07:03.970 --> 00:07:08.440
Note that the exclusive column
for the callers is close to 0%.

00:07:08.440 --> 00:07:10.910
Intuitively, this makes
sense because these functions

00:07:10.910 --> 00:07:13.300
do very little work
themselves other

00:07:13.300 --> 00:07:16.450
than call to other functions
that do the real work

00:07:16.450 --> 00:07:18.630
all the way up to solve
velocity constraints, which

00:07:18.630 --> 00:07:22.510
is a hot function
in this program.

00:07:22.510 --> 00:07:24.879
Optimizing programs is
an art and a science.

00:07:24.879 --> 00:07:27.170
The science is understanding
exactly what your Profiler

00:07:27.170 --> 00:07:29.370
is measuring, which
we just covered.

00:07:29.370 --> 00:07:32.020
The art is making effective
optimizations, which

00:07:32.020 --> 00:07:35.280
often requires intuition about
your own program and the system

00:07:35.280 --> 00:07:36.500
it is running on.

00:07:36.500 --> 00:07:39.800
Let's talk about the cycle
of optimization in general.

00:07:39.800 --> 00:07:42.270
The cycle begins with
measuring using your Profiler.

00:07:42.270 --> 00:07:44.260
And at first, you're
measuring to identify

00:07:44.260 --> 00:07:45.580
the hot spots in your program.

00:07:45.580 --> 00:07:46.830
These hot spots
are the functions

00:07:46.830 --> 00:07:48.413
that you're spending
a lot of time in,

00:07:48.413 --> 00:07:51.240
or the tags that you're
spending a lot of time in.

00:07:51.240 --> 00:07:52.870
Once you've identified
the hot spot,

00:07:52.870 --> 00:07:55.550
you want to create a
representative benchmark.

00:07:55.550 --> 00:07:59.290
This benchmark executes the
hot spot with typical inputs.

00:07:59.290 --> 00:08:00.960
Technically, this
step is optional.

00:08:00.960 --> 00:08:02.960
But it can really help
with your iteration times

00:08:02.960 --> 00:08:04.810
if you have a
canned program that

00:08:04.810 --> 00:08:08.490
exercises the slow
path of your program.

00:08:08.490 --> 00:08:11.710
Using this representative
benchmark, measure again

00:08:11.710 --> 00:08:12.590
with the Profiler.

00:08:12.590 --> 00:08:14.340
This is going to
give you a baseline

00:08:14.340 --> 00:08:16.214
to measure your
optimizations against.

00:08:16.214 --> 00:08:17.630
Without a good
baseline, you can't

00:08:17.630 --> 00:08:21.280
tell if the changes you've made
made things better or worse.

00:08:21.280 --> 00:08:24.290
So with your baseline,
you're ready to optimize.

00:08:24.290 --> 00:08:26.469
This is where your own
intuition comes into play.

00:08:26.469 --> 00:08:28.260
Your goal is to minimize
the amount of time

00:08:28.260 --> 00:08:30.884
in the hot spots, and there are
many different approaches here.

00:08:30.884 --> 00:08:33.440
You can experiment with reducing
the amount of work done,

00:08:33.440 --> 00:08:37.617
caching results, or using
different algorithms entirely.

00:08:37.617 --> 00:08:39.200
Now that you've made
a change that you

00:08:39.200 --> 00:08:41.059
think will speed
your program up,

00:08:41.059 --> 00:08:45.290
prove it by measuring again and
comparing with your baseline.

00:08:45.290 --> 00:08:46.900
That way, you can
say definitively

00:08:46.900 --> 00:08:50.950
that you've made your program
5% faster, 10% faster,

00:08:50.950 --> 00:08:53.234
or it's had no effect at all.

00:08:53.234 --> 00:08:54.650
You repeat this
cycle until you're

00:08:54.650 --> 00:08:57.187
happy with the performance
of your program.

00:08:57.187 --> 00:08:59.520
Before I go, I'd like to leave
you with a couple of tips

00:08:59.520 --> 00:09:01.780
on optimizing your
Dart programs.

00:09:01.780 --> 00:09:04.120
In the Profiler tree,
optimized Dart functions

00:09:04.120 --> 00:09:05.970
are prefixed with an asterisk.

00:09:05.970 --> 00:09:07.885
If your hot functions
aren't optimized,

00:09:07.885 --> 00:09:08.760
you've got a problem.

00:09:08.760 --> 00:09:12.190
You're probably triggering
excessive deoptimizations

00:09:12.190 --> 00:09:14.410
or prohibiting
optimizations from occurring

00:09:14.410 --> 00:09:15.560
in the first place.

00:09:15.560 --> 00:09:17.560
You need to understand
why that is,

00:09:17.560 --> 00:09:19.890
because optimized
code executes much

00:09:19.890 --> 00:09:21.340
faster than unoptimized code.

00:09:21.340 --> 00:09:23.740
And so you want to make sure
that your hot functions can

00:09:23.740 --> 00:09:26.291
and are staying optimized.

00:09:26.291 --> 00:09:28.790
You can get a high-level idea
of how your program is running

00:09:28.790 --> 00:09:30.560
simply by looking
at the VM Tags.

00:09:30.560 --> 00:09:33.260
And there are certain patterns
that you should come to expect.

00:09:33.260 --> 00:09:35.880
For applications which
do lots of computation,

00:09:35.880 --> 00:09:38.610
you should see Script
very high up there.

00:09:38.610 --> 00:09:40.300
You're probably
allocating too much

00:09:40.300 --> 00:09:44.470
if GC is high, in which case
hop over the Allocation Profiler

00:09:44.470 --> 00:09:46.410
to help understand
what you're allocating

00:09:46.410 --> 00:09:48.510
so you can minimize that.

00:09:48.510 --> 00:09:51.560
Applications that do lots
of file and network I/O

00:09:51.560 --> 00:09:54.500
should have a hot Native
Tag, because the actual I/O

00:09:54.500 --> 00:09:57.180
is done by calling out
to the operating system.

00:09:57.180 --> 00:10:02.054
So for server applications, you
want to see a hot Native Tag.

00:10:02.054 --> 00:10:03.720
I'm John McCutchan,
and you just learned

00:10:03.720 --> 00:10:06.300
about Observatory and
the Dart CPU Profiler.

00:10:06.300 --> 00:10:08.390
You can learn more about
Dart and Observatory

00:10:08.390 --> 00:10:10.440
by following the
links on this slide.

00:10:10.440 --> 00:10:12.410
Thanks for watching.

