WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:02.459
MALE SPEAKER: So today we're
here to see Jerry Kaplan.

00:00:02.459 --> 00:00:04.420
He's co-founded four
startups, two of which

00:00:04.420 --> 00:00:07.820
have gone public-- serial
entrepreneur, widely

00:00:07.820 --> 00:00:09.830
respected as a
technical innovator,

00:00:09.830 --> 00:00:11.352
and a bestselling author.

00:00:11.352 --> 00:00:12.810
In the interest of
time though, I'm

00:00:12.810 --> 00:00:15.647
going to abridge his long
list of many accomplishments

00:00:15.647 --> 00:00:17.980
and talk about a few things
that I think will especially

00:00:17.980 --> 00:00:21.020
interest you before he talks
about things that especially

00:00:21.020 --> 00:00:22.849
interest him.

00:00:22.849 --> 00:00:25.140
So one especially interesting
thing that he's done-- he

00:00:25.140 --> 00:00:27.600
was the co-founder, alongside
his friends Kevin Doren

00:00:27.600 --> 00:00:31.610
and Robert Carr, of the
GO Corporation in 1987.

00:00:31.610 --> 00:00:34.110
They were pioneers in the work
of tablet and stylus-based

00:00:34.110 --> 00:00:37.650
computing, the precursors to the
Apple Newton, the Palm Pilot,

00:00:37.650 --> 00:00:40.380
and later smartphones
and tablets of today.

00:00:40.380 --> 00:00:42.214
If you chronicled,
actually, his time there,

00:00:42.214 --> 00:00:44.296
they have a very interesting
book called "Startup:

00:00:44.296 --> 00:00:45.790
The Silicon Valley Adventure."

00:00:45.790 --> 00:00:48.310
Some of you may
have heard of this.

00:00:48.310 --> 00:00:49.480
A fun little fact.

00:00:49.480 --> 00:00:51.330
Omid Kordestani-- some
of you may know him

00:00:51.330 --> 00:00:53.390
as our former chief
business officer, started

00:00:53.390 --> 00:00:59.047
at Google in 1999-- got his
start, actually, at Go Corp.

00:00:59.047 --> 00:01:00.130
Jerry may talk about that.

00:01:00.130 --> 00:01:00.610
I don't know.

00:01:00.610 --> 00:01:01.193
It's possible.

00:01:03.740 --> 00:01:07.070
Here he is today again to talk
about artificial intelligence

00:01:07.070 --> 00:01:10.620
and the changing world
of work automation.

00:01:10.620 --> 00:01:14.954
So we are here for
"Humans Need Not Apply."

00:01:14.954 --> 00:01:16.870
Give a warm welcome to
Jerry Kaplan, everyone.

00:01:16.870 --> 00:01:18.750
[APPLAUSE]

00:01:22.670 --> 00:01:24.170
JERRY KAPLAN: Thanks,
how's the mic?

00:01:24.170 --> 00:01:26.280
Oh, that's good.

00:01:26.280 --> 00:01:28.405
All right, well, a mentor
of mine used to say never

00:01:28.405 --> 00:01:29.560
give a talk the first time.

00:01:29.560 --> 00:01:31.060
I want you to know
I've put together

00:01:31.060 --> 00:01:32.530
a special talk for you guys.

00:01:32.530 --> 00:01:34.650
This is the first I'm giving it.

00:01:34.650 --> 00:01:37.070
We'll see what happens.

00:01:37.070 --> 00:01:38.130
I should leave some time.

00:01:38.130 --> 00:01:41.490
I have lots of weird
anecdotes about Google

00:01:41.490 --> 00:01:44.134
that I will be happy to tell
when I'm not on the camera,

00:01:44.134 --> 00:01:47.990
as long as I'm not
being recorded.

00:01:47.990 --> 00:01:54.670
OK, so now for something
completely different, as

00:01:54.670 --> 00:01:56.360
they used to say
on "Monty Python."

00:01:56.360 --> 00:01:58.420
The common wisdom about
artificial intelligence

00:01:58.420 --> 00:02:00.570
is that we're building
increasingly intelligent

00:02:00.570 --> 00:02:02.600
machines that are
ultimately going

00:02:02.600 --> 00:02:05.750
to surpass human capabilities
and steal our jobs

00:02:05.750 --> 00:02:07.810
and maybe even
escape human control

00:02:07.810 --> 00:02:09.870
and take over the world.

00:02:09.870 --> 00:02:12.620
So I'm going to
present the case today

00:02:12.620 --> 00:02:15.450
that that narrative
is both misguided and

00:02:15.450 --> 00:02:18.300
counterproductive-- that a
more appropriate way to frame

00:02:18.300 --> 00:02:20.620
this, which is really
better supported

00:02:20.620 --> 00:02:22.660
by actual historical
and current events,

00:02:22.660 --> 00:02:25.680
is that AI is simply
a natural extension

00:02:25.680 --> 00:02:29.560
of long-standing efforts to
automate tasks that date back

00:02:29.560 --> 00:02:32.850
at least to the start of
the Industrial Revolution.

00:02:32.850 --> 00:02:35.840
And then I want to talk
about the consequences

00:02:35.840 --> 00:02:37.905
if you think about it
in that particular way.

00:02:37.905 --> 00:02:40.030
But let me ask about the
audience-- how many of you

00:02:40.030 --> 00:02:40.613
are engineers?

00:02:43.050 --> 00:02:48.122
OK, how many of you
are not engineers?

00:02:48.122 --> 00:02:49.550
Two.

00:02:49.550 --> 00:02:52.131
How many people haven't
raised their hand yet?

00:02:52.131 --> 00:02:52.630
Nobody.

00:02:52.630 --> 00:02:53.440
OK.

00:02:53.440 --> 00:02:56.420
And that's called
closure, right?

00:02:56.420 --> 00:02:59.980
OK, and how many of
you are doing anything

00:02:59.980 --> 00:03:02.800
even vaguely related to AI?

00:03:02.800 --> 00:03:05.000
Oh, not that many, OK.

00:03:05.000 --> 00:03:06.200
Cool.

00:03:06.200 --> 00:03:09.475
At least you won't admit it by
the time I'm done with my talk,

00:03:09.475 --> 00:03:09.975
I think.

00:03:13.590 --> 00:03:16.260
OK, so let me start with a
little bit of a history lesson.

00:03:16.260 --> 00:03:20.370
I'm teaching Impact of
Artificial Intelligence

00:03:20.370 --> 00:03:21.120
at Stanford.

00:03:21.120 --> 00:03:23.060
And much to my
shock, the students

00:03:23.060 --> 00:03:24.840
who studied artificial
intelligence

00:03:24.840 --> 00:03:26.550
don't know much
about its history.

00:03:26.550 --> 00:03:28.830
So here's a kind
of irreverent view.

00:03:28.830 --> 00:03:33.540
I'm going to start with an
unorthodox history of AI.

00:03:33.540 --> 00:03:35.840
Now, here's a news
flash for you.

00:03:35.840 --> 00:03:39.040
Science does not
proceed scientifically.

00:03:39.040 --> 00:03:42.810
So it's like the making of
legislation and sausage.

00:03:42.810 --> 00:03:47.920
Perhaps this is better done
outside of the public view.

00:03:47.920 --> 00:03:49.610
More than you might
want to believe,

00:03:49.610 --> 00:03:51.810
progress is often due
to the clash of egos

00:03:51.810 --> 00:03:53.670
and ideas and institutions.

00:03:53.670 --> 00:03:55.030
You guys work in an institution.

00:03:55.030 --> 00:03:57.310
I'm sure you see
that occasionally.

00:03:57.310 --> 00:03:59.150
And artificial intelligence
is no exception,

00:03:59.150 --> 00:04:02.070
so let me start right
at the beginning.

00:04:02.070 --> 00:04:04.520
Dartmouth College, 1956.

00:04:04.520 --> 00:04:06.910
A group of scientists--
they got together

00:04:06.910 --> 00:04:08.910
for an extended working session.

00:04:08.910 --> 00:04:11.710
How many of you who
John McCarthy is?

00:04:11.710 --> 00:04:13.400
Oh, man, OK.

00:04:13.400 --> 00:04:16.677
He's a mathematician who was
then employed at Dartmouth.

00:04:16.677 --> 00:04:19.010
Now, he hosted this meeting
along with-- raise your hand

00:04:19.010 --> 00:04:21.589
if you know these
guys-- Marvin Minsky.

00:04:21.589 --> 00:04:23.430
Oh, more than John, OK.

00:04:23.430 --> 00:04:24.630
He was then at Harvard.

00:04:24.630 --> 00:04:26.690
Claude Shannon?

00:04:26.690 --> 00:04:27.420
That's good.

00:04:27.420 --> 00:04:30.160
You guys should
know who Shannon is.

00:04:30.160 --> 00:04:31.690
He was at Bell Laboratories.

00:04:31.690 --> 00:04:33.766
And Nathaniel Rochester?

00:04:33.766 --> 00:04:34.620
Probably no hands.

00:04:34.620 --> 00:04:35.200
One hand.

00:04:35.200 --> 00:04:37.610
Are you his son?

00:04:37.610 --> 00:04:38.110
Sorry?

00:04:38.110 --> 00:04:39.540
AUDIENCE: [INAUDIBLE].

00:04:39.540 --> 00:04:42.120
JERRY KAPLAN: OK, there you go.

00:04:42.120 --> 00:04:43.650
He was at IBM.

00:04:43.650 --> 00:04:46.070
Now, here's what
these guys had to say,

00:04:46.070 --> 00:04:47.950
or John McCarthy had to say.

00:04:47.950 --> 00:04:50.440
He called his
proposal "A Proposal

00:04:50.440 --> 00:04:52.170
for the Dartmouth
Summer Research Project

00:04:52.170 --> 00:04:54.740
on Artificial Intelligence."

00:04:54.740 --> 00:04:58.060
Now, this was the first known
use of the term artificial

00:04:58.060 --> 00:04:59.370
intelligence.

00:04:59.370 --> 00:05:04.960
But what's not commonly known
is why did John McCarthy choose

00:05:04.960 --> 00:05:06.140
that particular name?

00:05:06.140 --> 00:05:08.340
He explained this
later-- much later,

00:05:08.340 --> 00:05:10.220
actually-- his motivation.

00:05:10.220 --> 00:05:12.694
He said, "As for myself,
one of the reasons

00:05:12.694 --> 00:05:14.610
for inventing the term
artificial intelligence

00:05:14.610 --> 00:05:17.810
was to escape the
association with cybernetics.

00:05:17.810 --> 00:05:21.420
Its concentration on analog
feedback seemed misguided,

00:05:21.420 --> 00:05:23.220
and I wished to
avoid having either

00:05:23.220 --> 00:05:26.870
to accept Norbert
Wiener as a guru

00:05:26.870 --> 00:05:30.180
or having to argue with him."

00:05:30.180 --> 00:05:32.770
Now, Norbert Wiener,
as you may know,

00:05:32.770 --> 00:05:35.610
was a highly respected--
Norbert Wiener?

00:05:35.610 --> 00:05:36.540
Anybody?

00:05:36.540 --> 00:05:37.560
Oh, my god.

00:05:37.560 --> 00:05:38.580
OK.

00:05:38.580 --> 00:05:40.450
Cybernetics.

00:05:40.450 --> 00:05:41.776
Cybernetics.

00:05:41.776 --> 00:05:43.980
Good, you've heard
the term at least.

00:05:43.980 --> 00:05:46.840
He was a highly respected
senior mathematician

00:05:46.840 --> 00:05:48.760
and a philosopher at MIT.

00:05:48.760 --> 00:05:52.280
Now, while he was that,
McCarthy, this guy,

00:05:52.280 --> 00:05:54.120
was just a junior
professor at Dartmouth.

00:05:54.120 --> 00:05:58.660
So he didn't want to have to go
up against the powers that be.

00:05:58.660 --> 00:06:01.690
So to understand the original
intention of the founding

00:06:01.690 --> 00:06:03.720
fathers of AI,
it's worth reading

00:06:03.720 --> 00:06:06.810
some of the actual text of
this conference proposal.

00:06:06.810 --> 00:06:09.460
I think it's on the screen.

00:06:09.460 --> 00:06:13.230
"The study is to proceed on
the basis of the conjecture

00:06:13.230 --> 00:06:16.700
that every aspect of
learning or any other feature

00:06:16.700 --> 00:06:21.280
of intelligence can in principle
be so precisely described

00:06:21.280 --> 00:06:24.922
that a machine can be
made to simulate it.

00:06:24.922 --> 00:06:26.380
An attempt will be
made to find out

00:06:26.380 --> 00:06:27.796
how to make machines
use language,

00:06:27.796 --> 00:06:30.510
form abstractions
and concepts, solve

00:06:30.510 --> 00:06:32.880
kinds of problems now
reserved for humans,

00:06:32.880 --> 00:06:34.910
and improve themselves."

00:06:34.910 --> 00:06:39.010
It's 1950-- what was it, 1956?

00:06:39.010 --> 00:06:42.120
"We think that a
significant advance

00:06:42.120 --> 00:06:44.590
could be made in one or more
these problems if a carefully

00:06:44.590 --> 00:06:47.670
selected group of scientists
work on it together

00:06:47.670 --> 00:06:51.560
for a summer."

00:06:51.560 --> 00:06:56.380
Now, that's a pretty dubious
agenda for a summer break.

00:06:56.380 --> 00:06:58.520
Now, many of the Dartmouth
conference participants

00:06:58.520 --> 00:07:01.710
had their own view about
how to best approach

00:07:01.710 --> 00:07:02.895
artificial intelligence.

00:07:02.895 --> 00:07:06.800
But John McCarthy's specialty
was mathematical logic.

00:07:06.800 --> 00:07:11.750
In particular, he believed
that logical inference

00:07:11.750 --> 00:07:17.060
was the key to, using his
words, simulated intelligence.

00:07:17.060 --> 00:07:19.540
That's what he thought AI was.

00:07:19.540 --> 00:07:22.874
Now, his approach, skipping
ahead quite a ways,

00:07:22.874 --> 00:07:24.290
but his approach
eventually became

00:07:24.290 --> 00:07:27.140
known as what's called the
physical symbol systems

00:07:27.140 --> 00:07:28.650
hypothesis.

00:07:28.650 --> 00:07:31.470
Anybody here have heard of that?

00:07:31.470 --> 00:07:32.280
One.

00:07:32.280 --> 00:07:36.080
Good man, OK, you can take
over for the rest of the talk.

00:07:36.080 --> 00:07:38.540
Now, that was the
dominant paradigm

00:07:38.540 --> 00:07:41.240
in the field of artificial
intelligence for the first 30

00:07:41.240 --> 00:07:46.070
years or so after the
Dartmouth Conference.

00:07:46.070 --> 00:07:50.300
Now, here's John McCarthy.

00:07:50.300 --> 00:07:52.830
I'm old enough to have
known John McCarthy when

00:07:52.830 --> 00:07:55.560
I was a postdoc at Stanford,
where he founded the Stanford

00:07:55.560 --> 00:07:56.830
Artificial Intelligence Lab.

00:07:56.830 --> 00:07:59.690
Now, John was definitely
a brilliant scientist.

00:07:59.690 --> 00:08:03.540
He invented the
programming language Lisp.

00:08:03.540 --> 00:08:05.490
Good.

00:08:05.490 --> 00:08:08.270
And he invented the
concept of time sharing.

00:08:08.270 --> 00:08:10.980
Not too many people know that.

00:08:10.980 --> 00:08:13.350
But he definitely had the
mad professor thing going.

00:08:13.350 --> 00:08:15.360
Let's see if this works.

00:08:15.360 --> 00:08:16.100
Almost.

00:08:16.100 --> 00:08:18.405
I'm using somebody
else's computer.

00:08:21.300 --> 00:08:23.590
You know, he had the
wild eyes and the hair.

00:08:23.590 --> 00:08:25.173
The guy on the right,
as you may know,

00:08:25.173 --> 00:08:29.710
is Professor Emmett Brown,
who invented the-- what is it?

00:08:29.710 --> 00:08:31.670
The flux capacitor time machine.

00:08:31.670 --> 00:08:33.360
How many people know
the flux capacitor?

00:08:33.360 --> 00:08:35.352
OK, good.

00:08:35.352 --> 00:08:37.310
I'm just checking to make
sure this talk works.

00:08:39.870 --> 00:08:45.400
But I'm confident that John
McCarthy, having met him,

00:08:45.400 --> 00:08:48.170
never really expected that
his clever name emerging

00:08:48.170 --> 00:08:49.790
field is going to
turn out to be one

00:08:49.790 --> 00:08:54.090
of the great accidental
marketing coups of all time.

00:08:54.090 --> 00:08:58.460
So it's not only inspired
generations of researchers,

00:08:58.460 --> 00:09:02.580
including myself, but it
spawned a virtual industry

00:09:02.580 --> 00:09:05.640
of science fiction and
Hollywood blockbusters and media

00:09:05.640 --> 00:09:11.987
attention and pontificating
pundits, also including myself.

00:09:11.987 --> 00:09:14.070
How do you name the field
something less arousing,

00:09:14.070 --> 00:09:17.850
like logical programming,
or symbolic systems?

00:09:17.850 --> 00:09:19.850
I doubt very many of us
would have ever heard

00:09:19.850 --> 00:09:21.756
of the field today.

00:09:21.756 --> 00:09:23.130
The field would
have just motored

00:09:23.130 --> 00:09:26.190
along automating various
tasks while we marvelled

00:09:26.190 --> 00:09:31.300
at the cleverness not of
what the creations were,

00:09:31.300 --> 00:09:33.429
but of the engineers.

00:09:33.429 --> 00:09:35.220
I'm getting a little
bit ahead of my story.

00:09:35.220 --> 00:09:37.630
In any case,
McCarthy's hypothesis

00:09:37.630 --> 00:09:41.170
that logic was the basis of
human intelligence is, at best,

00:09:41.170 --> 00:09:42.890
questionable.

00:09:42.890 --> 00:09:44.920
Today, in fact,
most AI researchers

00:09:44.920 --> 00:09:47.560
have abandoned this
approach and believe

00:09:47.560 --> 00:09:50.420
it was just plain wrong.

00:09:50.420 --> 00:09:53.870
The symbolic system approach has
been almost entirely abandoned

00:09:53.870 --> 00:09:56.040
in favor of generally
what's now referred

00:09:56.040 --> 00:09:57.520
to as machine learning.

00:09:57.520 --> 00:09:59.630
How many people here are
doing machine learning?

00:09:59.630 --> 00:10:00.310
Good.

00:10:00.310 --> 00:10:03.520
OK, or you certainly
know about it.

00:10:03.520 --> 00:10:07.010
But rejecting that old approach
is throwing the baby out

00:10:07.010 --> 00:10:08.500
with the bathwater.

00:10:08.500 --> 00:10:10.860
Some truly important
advances in computing

00:10:10.860 --> 00:10:13.470
came out of symbolic
systems, including

00:10:13.470 --> 00:10:16.460
things like heuristic search
algorithms, logical problem

00:10:16.460 --> 00:10:20.130
solvers, game players,
reasoning systems.

00:10:20.130 --> 00:10:23.090
These were all the old approach.

00:10:23.090 --> 00:10:25.600
And many of the results
of all that work

00:10:25.600 --> 00:10:28.500
are in wide practical use today.

00:10:28.500 --> 00:10:31.690
For example, formulating
driving directions-- I got

00:10:31.690 --> 00:10:33.972
lost coming here.

00:10:33.972 --> 00:10:36.180
I didn't know the difference
between the express lane

00:10:36.180 --> 00:10:37.565
and the regular lane.

00:10:37.565 --> 00:10:38.940
I thought I was
in the other one.

00:10:38.940 --> 00:10:41.090
Take this exit.

00:10:41.090 --> 00:10:42.790
No exit.

00:10:42.790 --> 00:10:45.650
Laying out factories
and warehouses,

00:10:45.650 --> 00:10:47.990
proving that complex
computer chips actually

00:10:47.990 --> 00:10:51.990
meet their specifications-- this
all uses early AI techniques.

00:10:51.990 --> 00:10:54.780
And I'm sure that there are
many more of these to come.

00:10:54.780 --> 00:10:58.470
Now, did I mention
machine learning?

00:10:58.470 --> 00:11:01.140
It's certainly the focus
of most current research,

00:11:01.140 --> 00:11:04.390
and in some circles,
at least where I am,

00:11:04.390 --> 00:11:06.100
it's considered a
serious candidate

00:11:06.100 --> 00:11:09.970
for the real basis of
human intelligence.

00:11:09.970 --> 00:11:12.530
Now, my personal opinion
is that while it's

00:11:12.530 --> 00:11:14.790
a very powerful
technology, and it's

00:11:14.790 --> 00:11:17.080
going to have a very
significant practical impact,

00:11:17.080 --> 00:11:20.990
it's very unlikely to be
the computational equivalent

00:11:20.990 --> 00:11:23.450
of the human mind.

00:11:23.450 --> 00:11:25.000
And whatever your
view, you might

00:11:25.000 --> 00:11:28.465
be surprised to learn a
little more about what

00:11:28.465 --> 00:11:31.460
are the fundamental concepts
that underlie what's

00:11:31.460 --> 00:11:34.560
called the connectionist or
neural networking approach

00:11:34.560 --> 00:11:35.860
to machine learning came from.

00:11:35.860 --> 00:11:39.150
There are some other approaches,
mainly in the statistical area.

00:11:39.150 --> 00:11:39.820
So let's see.

00:11:42.600 --> 00:11:45.250
Frank Rosenblatt,
anybody heard of him?

00:11:45.250 --> 00:11:48.270
Wow, OK, great.

00:11:48.270 --> 00:11:51.110
I didn't until I started
researching this.

00:11:51.110 --> 00:11:53.610
Back in the late
1950s, John McCarthy

00:11:53.610 --> 00:11:55.440
wasn't the only one
interested in building

00:11:55.440 --> 00:11:57.220
intelligent machines.

00:11:57.220 --> 00:12:00.020
There was another highly
optimistic proponent,

00:12:00.020 --> 00:12:02.000
and that was Professor
Frank Rosenblatt

00:12:02.000 --> 00:12:05.390
at Cornell-- another competing
prominent institution.

00:12:05.390 --> 00:12:07.600
You've got Cornell, and
you've got Dartmouth--

00:12:07.600 --> 00:12:09.600
and lots of people at MIT.

00:12:09.600 --> 00:12:12.690
And Rosenblatt was intrigued
by some pioneering research

00:12:12.690 --> 00:12:16.290
by psychologists Warren
McCulloch and Walter Pitts

00:12:16.290 --> 00:12:17.860
at the University of Chicago.

00:12:17.860 --> 00:12:20.480
And McCulloch and
Pitts had observed

00:12:20.480 --> 00:12:23.290
that a network of
brain neurons could

00:12:23.290 --> 00:12:28.700
be modeled by, of all
things, logical expressions.

00:12:28.700 --> 00:12:30.412
So Rosenblatt got
the bright idea

00:12:30.412 --> 00:12:32.620
to implement their ideas in
a computer program, which

00:12:32.620 --> 00:12:35.610
he rebranded as a perceptron.

00:12:35.610 --> 00:12:37.650
Anybody heard of perceptrons?

00:12:37.650 --> 00:12:38.320
Oh, good.

00:12:38.320 --> 00:12:39.750
Cool.

00:12:39.750 --> 00:12:41.610
He built an early
version of what today,

00:12:41.610 --> 00:12:45.425
we would call a
simple neural network.

00:12:45.425 --> 00:12:46.800
This is the geekiest
looking guy.

00:12:46.800 --> 00:12:48.690
He looks like he's 12 years old.

00:12:48.690 --> 00:12:50.860
That's him.

00:12:50.860 --> 00:12:57.770
That's his actual photo
cell array right there.

00:13:04.090 --> 00:13:08.430
Now, he wasn't about to be
outdone by McCarthy and Minsky,

00:13:08.430 --> 00:13:12.085
so Rosenblatt heavily promoted
his work in the popular press.

00:13:12.085 --> 00:13:14.210
For instance, he was quoted
in "The New York Times"

00:13:14.210 --> 00:13:22.370
in 1958 saying, the machine
that he is going to build

00:13:22.370 --> 00:13:28.350
would be the first device
to think as the human brain.

00:13:28.350 --> 00:13:31.310
In principle, it would be
possible to build brains

00:13:31.310 --> 00:13:34.140
that could reproduce
themselves on an assembly line,

00:13:34.140 --> 00:13:36.140
and which would be conscious
of their existence.

00:13:36.140 --> 00:13:42.260
This is 1958.

00:13:42.260 --> 00:13:44.900
The article went on
to say that the embryo

00:13:44.900 --> 00:13:48.530
of an electronic
computer-- the embryo

00:13:48.530 --> 00:13:50.030
of an electronic
computer today that

00:13:50.030 --> 00:13:53.860
will be able to walk, talk, see,
write, reproduce itself, and be

00:13:53.860 --> 00:13:56.400
conscious of its existence.

00:13:56.400 --> 00:13:58.170
And here's what I love.

00:13:58.170 --> 00:14:00.900
"It is expected to be
finished in about a year

00:14:00.900 --> 00:14:05.760
at a cost of about $100,000."

00:14:05.760 --> 00:14:08.960
So much for the journalistic
accuracy of "The New York

00:14:08.960 --> 00:14:10.250
Times."

00:14:10.250 --> 00:14:14.110
By the way, I'm usually
debating John Markoff.

00:14:14.110 --> 00:14:15.360
He's the science writer there.

00:14:15.360 --> 00:14:16.485
We love to beat each other.

00:14:16.485 --> 00:14:17.280
I wish he was here.

00:14:17.280 --> 00:14:19.260
He'd go crazy.

00:14:19.260 --> 00:14:21.654
Now, that might seem a
little bit optimistic,

00:14:21.654 --> 00:14:23.570
given that Rosenblatt's
demonstration included

00:14:23.570 --> 00:14:27.530
only 400 photo cells connected
to 1,000 perceptrons, which

00:14:27.530 --> 00:14:31.640
after 50 trials, was able
to tell whether a card had

00:14:31.640 --> 00:14:35.680
a square marked on the right
side or on the left side.

00:14:35.680 --> 00:14:36.750
That's what he could do.

00:14:36.750 --> 00:14:38.930
Now, on a more
positive note, and this

00:14:38.930 --> 00:14:41.550
is also pretty
remarkable, I can't help

00:14:41.550 --> 00:14:44.070
but notice that
many of his wilder

00:14:44.070 --> 00:14:47.550
prophecies in the article have
actually now become reality.

00:14:47.550 --> 00:14:51.230
He went on to say-- listen to
this closely-- remember, 1958.

00:14:51.230 --> 00:14:56.510
"Later, perceptrons will be able
to recognize people, call out

00:14:56.510 --> 00:15:00.230
their names, instantly
translate speech

00:15:00.230 --> 00:15:05.070
from one language to speech or
writing in another language."

00:15:05.070 --> 00:15:06.520
He was right.

00:15:06.520 --> 00:15:10.820
It only took 50 years
longer than he predicted.

00:15:10.820 --> 00:15:17.900
OK, now, Rosenblatt's work was
well known to at least some

00:15:17.900 --> 00:15:22.440
of the participants at
that Dartmouth Conference.

00:15:22.440 --> 00:15:25.390
In particular, he attended the
Bronx High School of Science--

00:15:25.390 --> 00:15:27.080
anybody here go there?

00:15:27.080 --> 00:15:30.620
Not one, wrong coast--
with Marvin Minsky.

00:15:30.620 --> 00:15:32.320
They were one year apart.

00:15:32.320 --> 00:15:34.730
So they later wound up going
to these different forums

00:15:34.730 --> 00:15:36.830
and debating each
other, promoting

00:15:36.830 --> 00:15:39.170
their respectively
favored approaches

00:15:39.170 --> 00:15:40.320
to artificial intelligence.

00:15:40.320 --> 00:15:44.979
Until in 1969, Minsky,
who's now at MIT-- remember,

00:15:44.979 --> 00:15:46.520
one guy's at Cornell,
the other guy's

00:15:46.520 --> 00:15:52.330
at MIT-- along with a colleague
of Marvin Minsky's called

00:15:52.330 --> 00:15:54.490
Seymour Papert,
published a book called

00:15:54.490 --> 00:15:58.570
"Perceptrons." in which he went
to pains to discredit, somewhat

00:15:58.570 --> 00:16:00.370
unfairly I might
add, a simplified

00:16:00.370 --> 00:16:03.742
version of Rosenblatt's work.

00:16:03.742 --> 00:16:05.450
Now, here's the way
science really works.

00:16:05.450 --> 00:16:10.160
Now, Rosenblatt was unable
to mount a proper defense

00:16:10.160 --> 00:16:11.980
for a very simple reason.

00:16:11.980 --> 00:16:14.690
Anybody guess what it was?

00:16:14.690 --> 00:16:18.540
He died in a boating accident
in 1971, two years later.

00:16:18.540 --> 00:16:20.450
He couldn't defend himself.

00:16:20.450 --> 00:16:23.180
Now, the book, however,
proved highly influential,

00:16:23.180 --> 00:16:26.290
effectively foreclosing
funding and research

00:16:26.290 --> 00:16:29.400
on perceptrons and
artificial neural networks

00:16:29.400 --> 00:16:30.920
in general for
more than a decade.

00:16:35.810 --> 00:16:38.840
So after 50 years,
which is better,

00:16:38.840 --> 00:16:44.170
the symbolic systems approach or
the machine learning approach?

00:16:44.170 --> 00:16:45.920
The plain fact is both
of these approaches

00:16:45.920 --> 00:16:47.860
have different strengths
and weaknesses.

00:16:47.860 --> 00:16:50.084
In general, symbolic
reasoning is

00:16:50.084 --> 00:16:51.500
more appropriate
for problems that

00:16:51.500 --> 00:16:53.670
require abstract reasoning.

00:16:53.670 --> 00:16:55.540
And machine learning,
on the other hand,

00:16:55.540 --> 00:16:58.290
is better for problems that
require sensory perception

00:16:58.290 --> 00:17:01.010
or extracting patterns
from large collections

00:17:01.010 --> 00:17:02.784
of noisy data.

00:17:02.784 --> 00:17:04.200
So you might ask
the question, why

00:17:04.200 --> 00:17:05.690
was the symbolic
approach dominant

00:17:05.690 --> 00:17:08.310
in the last half of the 20th
century and machine learning

00:17:08.310 --> 00:17:09.890
is dominant today?

00:17:09.890 --> 00:17:13.950
The answer is fairly
simple-- the machines.

00:17:13.950 --> 00:17:18.119
They are literally a million
times faster, cheaper,

00:17:18.119 --> 00:17:20.750
and have a million times
more memory at the same price

00:17:20.750 --> 00:17:22.280
as they did back then.

00:17:22.280 --> 00:17:25.869
That's a qualitative difference.

00:17:25.869 --> 00:17:29.395
In the early days
of AI, machines just

00:17:29.395 --> 00:17:31.727
weren't powerful enough
to automatically learn

00:17:31.727 --> 00:17:33.810
anything of interest-- the
square is on the right,

00:17:33.810 --> 00:17:35.620
the square is on the left.

00:17:35.620 --> 00:17:38.370
They had n only a minuscule
fraction of the processing

00:17:38.370 --> 00:17:41.360
speed and a vanishingly
small amount of memory

00:17:41.360 --> 00:17:44.250
in which to store data
compared to today's computers.

00:17:44.250 --> 00:17:46.320
But most importantly,
there simply

00:17:46.320 --> 00:17:48.560
weren't many sources
of machine readable

00:17:48.560 --> 00:17:51.230
data available to learn from.

00:17:51.230 --> 00:17:53.850
What were you going to learn?

00:17:53.850 --> 00:17:56.330
For real time learning, most
communication at that time

00:17:56.330 --> 00:17:58.610
was on paper.

00:17:58.610 --> 00:18:01.990
And for real-time learning,
the data from sensors

00:18:01.990 --> 00:18:04.630
was equally primitive and
only available, usually,

00:18:04.630 --> 00:18:08.735
in an analog form that really
resisted processing digitally.

00:18:08.735 --> 00:18:10.235
So you had four
trends-- improvement

00:18:10.235 --> 00:18:13.530
in computing speed,
memory, the transition

00:18:13.530 --> 00:18:18.540
from physical to electronically
stored data, and easier access

00:18:18.540 --> 00:18:19.600
to large bodies of data.

00:18:19.600 --> 00:18:21.510
God knows you guys
know about that.

00:18:21.510 --> 00:18:25.490
It's mainly due to the internet
and low-cost, high-resolution

00:18:25.490 --> 00:18:26.370
digital sensors.

00:18:26.370 --> 00:18:30.220
I don't know how I came up
with five, but I can't count.

00:18:30.220 --> 00:18:33.070
These were the prime
drivers-- never

00:18:33.070 --> 00:18:34.960
give a talk the first time.

00:18:34.960 --> 00:18:37.890
These were prime drivers in
the refocusing of efforts

00:18:37.890 --> 00:18:40.070
from the symbolic reasoning
approach to the machine

00:18:40.070 --> 00:18:41.440
learning approach.

00:18:41.440 --> 00:18:44.150
OK, there's a little
bit of history for you.

00:18:44.150 --> 00:18:47.769
Now let me get to
the main issue.

00:18:47.769 --> 00:18:48.560
Can machines think?

00:18:52.770 --> 00:18:55.980
So what is artificial
intelligence, really?

00:18:55.980 --> 00:18:58.780
After a lifetime of
work in this field

00:18:58.780 --> 00:19:01.990
and a great deal of
reflection on this question,

00:19:01.990 --> 00:19:07.830
my reluctant and disappointing
answer is simple.

00:19:07.830 --> 00:19:09.240
No.

00:19:09.240 --> 00:19:12.930
Or at least they can't
think the way people think.

00:19:12.930 --> 00:19:16.070
So far, at least, there's no
obvious road map from here

00:19:16.070 --> 00:19:17.300
to there.

00:19:17.300 --> 00:19:19.900
Machines are not people.

00:19:19.900 --> 00:19:21.810
And there's simply no
persuasive argument

00:19:21.810 --> 00:19:23.870
that they're on the
same path to becoming

00:19:23.870 --> 00:19:26.840
generally intelligent,
sentient beings,

00:19:26.840 --> 00:19:29.560
despite what you
see in the movies.

00:19:29.560 --> 00:19:32.360
Now, wait a minute,
you might say.

00:19:32.360 --> 00:19:36.070
Jerry, can't they solve all
sorts of complex reasoning

00:19:36.070 --> 00:19:38.340
and perception problems?

00:19:38.340 --> 00:19:39.450
Sure they can.

00:19:39.450 --> 00:19:42.040
They can perform
tasks that humans

00:19:42.040 --> 00:19:44.930
solve using intelligence.

00:19:44.930 --> 00:19:47.890
But that doesn't mean that
the machines are intelligent.

00:19:47.890 --> 00:19:51.390
It merely means that many
tasks that we thought

00:19:51.390 --> 00:19:56.790
required general intelligence
are in fact subject to solution

00:19:56.790 --> 00:19:59.860
by other kinds of
mechanical means.

00:19:59.860 --> 00:20:02.180
Now, there's an old
joke in AI, which

00:20:02.180 --> 00:20:05.570
is that once an AI problem
is solved, it's no longer AI.

00:20:05.570 --> 00:20:07.220
Anybody heard that?

00:20:07.220 --> 00:20:09.360
A couple of people, good.

00:20:09.360 --> 00:20:12.400
Now, personally, I don't think
that's any longer a joke.

00:20:12.400 --> 00:20:14.250
I'm going to look at
some of the signature

00:20:14.250 --> 00:20:16.620
accomplishments of
artificial intelligence

00:20:16.620 --> 00:20:18.660
from this different perspective.

00:20:18.660 --> 00:20:25.240
Let's start with computer chess.

00:20:25.240 --> 00:20:27.820
Now, for decades--
most of you guys

00:20:27.820 --> 00:20:29.460
weren't around to
see this, but I

00:20:29.460 --> 00:20:33.210
was-- the archetypal test
of the coming of age of AI

00:20:33.210 --> 00:20:34.950
wasn't the Turing test.

00:20:34.950 --> 00:20:36.660
It was, could a
machine ever beat

00:20:36.660 --> 00:20:39.312
the world's chess champion?

00:20:39.312 --> 00:20:40.770
For a long time,
you see, chess was

00:20:40.770 --> 00:20:42.860
considered the
quintessential demonstration

00:20:42.860 --> 00:20:44.740
of human intelligence.

00:20:44.740 --> 00:20:49.970
So surely when a computer
was world chess champion,

00:20:49.970 --> 00:20:51.340
AI would have arrived.

00:20:51.340 --> 00:20:51.840
That's it.

00:20:51.840 --> 00:20:53.880
We'd have smart machines.

00:20:53.880 --> 00:20:57.590
Well, it happened in
1997 when IBM's Deep Blue

00:20:57.590 --> 00:20:59.215
beat the then champion,
Garry Kasparov.

00:21:01.870 --> 00:21:06.440
Lots of ink was spilled in the
media lamenting the arrival

00:21:06.440 --> 00:21:08.340
of super-intelligent machines.

00:21:08.340 --> 00:21:10.550
There was a lot of hand
wringing or what this meant

00:21:10.550 --> 00:21:13.120
for the future of mankind.

00:21:13.120 --> 00:21:15.870
But the truth is it meant
nothing other than that you

00:21:15.870 --> 00:21:17.370
could do a lot of
clever programming

00:21:17.370 --> 00:21:22.090
and use the increases in speed
of computers to play chess.

00:21:22.090 --> 00:21:23.720
The techniques used
have applications

00:21:23.720 --> 00:21:26.570
to similar classes of problems.

00:21:26.570 --> 00:21:30.580
But they hardly proved to be
the harbingers of the robot

00:21:30.580 --> 00:21:33.290
apocalypse.

00:21:33.290 --> 00:21:35.020
So let me tell you
what people said

00:21:35.020 --> 00:21:39.100
after that non-event happened.

00:21:39.100 --> 00:21:41.610
They said, OK, sure,
computers can play chess.

00:21:41.610 --> 00:21:45.460
But they'll never be
able to drive a car.

00:21:45.460 --> 00:21:47.252
This really was what happened.

00:21:47.252 --> 00:21:48.710
That requires a
broad understanding

00:21:48.710 --> 00:21:51.390
of the real world-- the ability
to make split-second judgments

00:21:51.390 --> 00:21:53.510
in chaotic circumstances.

00:21:53.510 --> 00:21:57.957
And, of course, common sense--
machines will never have that.

00:21:57.957 --> 00:22:00.040
Well, as you know, this
bulwark of human supremacy

00:22:00.040 --> 00:22:04.630
was breached in 2004
with the DARPA Grand

00:22:04.630 --> 00:22:06.649
Challenge for
autonomous vehicles,

00:22:06.649 --> 00:22:08.440
which are soon coming,
if they're not here,

00:22:08.440 --> 00:22:10.167
to a parking lot near you.

00:22:10.167 --> 00:22:11.750
How many of you guys
have taken a ride

00:22:11.750 --> 00:22:14.690
in the Google self-driving cars?

00:22:14.690 --> 00:22:15.190
What?

00:22:18.900 --> 00:22:20.630
Oh, they should
send one up here.

00:22:24.070 --> 00:22:26.220
Have you been at least down
to the Tesla dealership

00:22:26.220 --> 00:22:28.280
to take a test drive?

00:22:28.280 --> 00:22:31.310
I did that over the weekend.

00:22:31.310 --> 00:22:32.620
The self-driving car was cool.

00:22:36.090 --> 00:22:39.150
OK, now our self-driving
cars do just that.

00:22:39.150 --> 00:22:40.880
They drive cars.

00:22:40.880 --> 00:22:41.880
They don't build houses.

00:22:41.880 --> 00:22:42.796
They don't cook meals.

00:22:42.796 --> 00:22:43.830
They don't make beds.

00:22:43.830 --> 00:22:46.180
That's what they do.

00:22:46.180 --> 00:22:49.860
So computers can play
chess and drive cars.

00:22:49.860 --> 00:22:51.850
But then they
said-- people said,

00:22:51.850 --> 00:22:55.760
but they could
never play Jeopardy.

00:22:55.760 --> 00:22:59.040
OK, well, that requires
too much world knowledge

00:22:59.040 --> 00:23:02.300
and understanding metaphors
and clever wordplay.

00:23:02.300 --> 00:23:05.800
Well, thanks again to be
ingenious people at IBM,

00:23:05.800 --> 00:23:07.530
this hurdle has
also been cleared.

00:23:07.530 --> 00:23:09.740
As undoubtedly you know,
IBM's Watson system

00:23:09.740 --> 00:23:15.590
beat Ken Jennings, the world
Jeopardy champion in 2011.

00:23:15.590 --> 00:23:19.130
Now, what is Watson?

00:23:19.130 --> 00:23:22.550
The reality is it's a collection
of facts and figures encoded

00:23:22.550 --> 00:23:25.440
into cleverly
organized modules that

00:23:25.440 --> 00:23:27.650
can quickly and
accurately answer

00:23:27.650 --> 00:23:30.430
various types of common
Jeopardy questions.

00:23:30.430 --> 00:23:34.500
Watson's main advantage
over the human contestants,

00:23:34.500 --> 00:23:36.900
believe it or not, was
that it could ring in

00:23:36.900 --> 00:23:39.350
before they could when it
estimated a high likelihood

00:23:39.350 --> 00:23:42.387
that it had a correct answer.

00:23:42.387 --> 00:23:44.470
I would love to go in this
in more detail for you.

00:23:44.470 --> 00:23:47.540
It turns out most of
the Jeopardy champions

00:23:47.540 --> 00:23:48.900
know the answers.

00:23:48.900 --> 00:23:50.950
They're just not that fast.

00:23:50.950 --> 00:23:53.540
And so the machine had
numerous advantages.

00:23:53.540 --> 00:23:57.960
It's a long-- it was
kind of a magic show.

00:23:57.960 --> 00:24:00.090
It's a wonderful accomplishment.

00:24:00.090 --> 00:24:03.810
It's a really remarkable
and very sophisticated

00:24:03.810 --> 00:24:07.280
knowledge-based retrieval
system and an inference system

00:24:07.280 --> 00:24:08.990
that was honed, at
least at that time,

00:24:08.990 --> 00:24:10.189
to a particular problem set.

00:24:10.189 --> 00:24:12.230
Now they're trying to
apply it to lots of others.

00:24:12.230 --> 00:24:17.370
Now, how many of you saw
that, or pictures of it?

00:24:17.370 --> 00:24:22.020
OK, now here's what bothers me.

00:24:22.020 --> 00:24:23.820
Is this supposed to be animated?

00:24:23.820 --> 00:24:24.320
OK.

00:24:28.260 --> 00:24:30.830
Now, in my opinion, IBM
didn't do the field of AI

00:24:30.830 --> 00:24:34.500
any favors by wrapping
Watson in a theatrical suite

00:24:34.500 --> 00:24:36.530
of anthropomorphic features.

00:24:36.530 --> 00:24:38.060
There's really no
technical reason

00:24:38.060 --> 00:24:41.617
to have a system say its
responses in a calm, didactic

00:24:41.617 --> 00:24:42.200
tone of voice.

00:24:42.200 --> 00:24:46.010
Yes, Alex, the answer
is such and such.

00:24:46.010 --> 00:24:49.820
Much less to put up a head-like
graphic of swirling lights,

00:24:49.820 --> 00:24:51.840
suggesting that the
machine had a mind

00:24:51.840 --> 00:24:54.440
and was thinking
about the problem.

00:24:54.440 --> 00:24:57.980
These were incidental adornments
to a tremendous technical

00:24:57.980 --> 00:24:59.490
achievement.

00:24:59.490 --> 00:25:00.950
Now, without a
deep understanding

00:25:00.950 --> 00:25:03.370
of how these systems
work, and with humans

00:25:03.370 --> 00:25:06.220
as the only available
exemplars with which

00:25:06.220 --> 00:25:07.950
to interpret the
results, the temptation

00:25:07.950 --> 00:25:12.700
to view them as human-like
is really irresistible.

00:25:12.700 --> 00:25:15.740
But they aren't those things.

00:25:15.740 --> 00:25:19.814
OK, so let me give you a couple
more interesting examples--

00:25:19.814 --> 00:25:21.730
more contemporary, things
that you're probably

00:25:21.730 --> 00:25:23.560
more familiar with.

00:25:23.560 --> 00:25:25.310
What about these machine
learning systems?

00:25:25.310 --> 00:25:27.160
Aren't they more like
human intelligence?

00:25:27.160 --> 00:25:29.950
Well, not really.

00:25:29.950 --> 00:25:32.970
True, I could argue
this for two hours here.

00:25:32.970 --> 00:25:35.320
Lots of people
sticking their hand up.

00:25:35.320 --> 00:25:37.510
In reality, the use of the
term neural networks is

00:25:37.510 --> 00:25:40.880
little more than an analogy,
in the same sense as saying

00:25:40.880 --> 00:25:44.780
airplane design was
inspired by birds.

00:25:44.780 --> 00:25:47.170
It's in the same category.

00:25:47.170 --> 00:25:50.000
Consider how machines
and people learn.

00:25:50.000 --> 00:25:53.139
You can teach a computer
to recognize cats

00:25:53.139 --> 00:25:54.430
by showing it a million images.

00:25:54.430 --> 00:25:57.595
You guys know Andrew Ng?

00:25:57.595 --> 00:26:02.736
He was at Google when
he did that work.

00:26:02.736 --> 00:26:04.110
You can show it
a million images,

00:26:04.110 --> 00:26:06.780
or you could simply point
one out to a three-year-old

00:26:06.780 --> 00:26:09.230
and get the same job done.

00:26:09.230 --> 00:26:11.270
That's a cat.

00:26:11.270 --> 00:26:12.500
Oh, that's it.

00:26:12.500 --> 00:26:15.960
Now, from then on, the three
year old knows what a cat is.

00:26:15.960 --> 00:26:20.810
Obviously, humans and machines
do not learn the same way.

00:26:20.810 --> 00:26:22.990
And let me give you another
interesting example.

00:26:22.990 --> 00:26:24.615
Anybody here doing
machine translation?

00:26:26.810 --> 00:26:29.544
One Google site.

00:26:29.544 --> 00:26:33.100
OK, I'm going into the lion's
den in about two weeks.

00:26:33.100 --> 00:26:35.675
I'm going to talk to the
machine translation people,

00:26:35.675 --> 00:26:36.615
among others.

00:26:36.615 --> 00:26:37.990
Now, tremendous
strides have been

00:26:37.990 --> 00:26:40.060
made in this field
in the past few years

00:26:40.060 --> 00:26:42.580
mainly by applying statistical
and machine learning

00:26:42.580 --> 00:26:47.800
techniques to large
bodies of concordant text.

00:26:47.800 --> 00:26:53.052
But how do people perform
this difficult task?

00:26:53.052 --> 00:26:54.260
Think about how people do it.

00:26:54.260 --> 00:26:55.635
They learn two or
more languages,

00:26:55.635 --> 00:26:58.800
along with the respective
cultures and conventions.

00:26:58.800 --> 00:27:01.030
Then they read some
text in one language,

00:27:01.030 --> 00:27:03.600
they understand what it says,
and they render the meaning

00:27:03.600 --> 00:27:08.550
as closely as possible
in another language.

00:27:08.550 --> 00:27:11.330
Now, machine translation, as
successful as it is today,

00:27:11.330 --> 00:27:16.730
there's no relationship to
the human translation process.

00:27:16.730 --> 00:27:18.970
Its success simply means
there's another way

00:27:18.970 --> 00:27:21.630
to approximate the same results.

00:27:21.630 --> 00:27:24.940
It's mostly just
concordances of text.

00:27:24.940 --> 00:27:28.070
It doesn't relate to the way
people solve that problem.

00:27:28.070 --> 00:27:30.000
What do we learn from this?

00:27:30.000 --> 00:27:32.130
It's just a way
to-- we just didn't

00:27:32.130 --> 00:27:34.920
think there was another
solution, but there is,

00:27:34.920 --> 00:27:37.430
besides having people
understand that.

00:27:37.430 --> 00:27:41.240
Now, let me go on to one
you are all carrying around.

00:27:41.240 --> 00:27:43.200
You carry around smartphones.

00:27:43.200 --> 00:27:46.320
They're reminiscent
of the capabilities

00:27:46.320 --> 00:27:50.290
of the computer on the Star
Trek Enterprise-- "Star Trek?"

00:27:50.290 --> 00:27:51.230
Everybody?

00:27:51.230 --> 00:27:53.779
Good, OK.

00:27:53.779 --> 00:27:55.320
I started talking
about "The Jetsons"

00:27:55.320 --> 00:27:59.240
in my class at Stanford, nobody
knew what I was talking about.

00:27:59.240 --> 00:27:59.910
What's that?

00:27:59.910 --> 00:28:03.637
You know, Rosie
and-- you know, OK.

00:28:03.637 --> 00:28:04.720
That's called getting old.

00:28:08.220 --> 00:28:14.120
So this is more like--
I lost complete track,

00:28:14.120 --> 00:28:16.090
and I got the whole thing
right in front of me.

00:28:16.090 --> 00:28:17.240
Hey, Siri, you know?

00:28:17.240 --> 00:28:20.050
You can talk to your
phone, and it talks back.

00:28:20.050 --> 00:28:22.460
It also becomes more
capable every day

00:28:22.460 --> 00:28:25.902
as you download new apps and
upgrade the operating system.

00:28:25.902 --> 00:28:28.046
So I'm using examples
of [INAUDIBLE].

00:28:30.359 --> 00:28:31.900
But do you really
think of your phone

00:28:31.900 --> 00:28:34.010
as getting smarter
in the human sense

00:28:34.010 --> 00:28:37.204
when you download an app or
you enable voice recognition?

00:28:37.204 --> 00:28:38.870
Certainly not in the
same sense that you

00:28:38.870 --> 00:28:40.328
get smarter when
you learn calculus

00:28:40.328 --> 00:28:42.870
or when you learn philosophy.

00:28:42.870 --> 00:28:46.880
It's the electronic equivalent
of a Swiss Army knife.

00:28:46.880 --> 00:28:49.250
It's a bunch of different
information processing tools

00:28:49.250 --> 00:28:51.100
that are bound together
into a single unit,

00:28:51.100 --> 00:28:54.210
taking advantage of some
commonalities, like detailed

00:28:54.210 --> 00:28:58.270
maps, and like internet access.

00:28:58.270 --> 00:29:01.530
Now, you have one
integrated mind,

00:29:01.530 --> 00:29:03.990
while your phone
has no mind at all.

00:29:03.990 --> 00:29:06.460
There's no one home.

00:29:06.460 --> 00:29:10.040
So I try to make the case
that machines perform

00:29:10.040 --> 00:29:12.440
an increasingly
diverse array of tasks

00:29:12.440 --> 00:29:15.480
that people perform by applying
their native intelligence.

00:29:15.480 --> 00:29:18.300
Now, does that mean
that machines are smart?

00:29:18.300 --> 00:29:20.540
Well, now things
get interesting.

00:29:20.540 --> 00:29:23.034
Let's talk about how you
might measure supposed machine

00:29:23.034 --> 00:29:23.575
intelligence.

00:29:26.865 --> 00:29:28.490
I pulled that picture
off the internet.

00:29:28.490 --> 00:29:31.490
So I didn't make it up.

00:29:31.490 --> 00:29:34.169
That's part of the point
I'm trying to make.

00:29:34.169 --> 00:29:36.710
We can start by looking at how
we measure human intelligence.

00:29:36.710 --> 00:29:41.660
Now, a common method is with
IQ tests, but even for humans,

00:29:41.660 --> 00:29:45.470
this is a deeply flawed concept.

00:29:45.470 --> 00:29:48.900
We love to measure and
rank things with numbers.

00:29:48.900 --> 00:29:51.030
But let's face it,
reducing human intelligence

00:29:51.030 --> 00:29:54.500
to a flat, linear scale
is highly questionable.

00:29:54.500 --> 00:29:57.110
Little Sally did two more
arithmetic problems than Johnny

00:29:57.110 --> 00:30:00.590
did in time allotted, so her
IQ is seven points higher

00:30:00.590 --> 00:30:02.281
than his.

00:30:02.281 --> 00:30:04.686
Bull.

00:30:04.686 --> 00:30:06.900
But that's not to say that
some people aren't smarter

00:30:06.900 --> 00:30:09.970
than others-- only that simple
numerical measures provide

00:30:09.970 --> 00:30:14.370
an inappropriate patina of
objectivity and precision.

00:30:14.370 --> 00:30:18.412
As psychologists are
fond of pointing out,

00:30:18.412 --> 00:30:20.370
there are many different
kinds of intelligence.

00:30:20.370 --> 00:30:23.522
There's social and emotional,
analytic, athletic, musical, et

00:30:23.522 --> 00:30:24.790
cetera.

00:30:24.790 --> 00:30:28.990
But what on Earth does it mean
to say that Mozart and Einstein

00:30:28.990 --> 00:30:29.970
have the same IQ?

00:30:32.700 --> 00:30:34.720
Now, suppose we gave
the same intelligence

00:30:34.720 --> 00:30:36.361
tests to a machine.

00:30:36.361 --> 00:30:36.860
Wow!

00:30:36.860 --> 00:30:39.210
And it only took one millisecond
to accurately complete

00:30:39.210 --> 00:30:43.810
all of the sums that took
Sally, and Johnny, and Alan.

00:30:43.810 --> 00:30:46.530
It must be super-smart.

00:30:46.530 --> 00:30:50.110
It also outperforms all
humans on memory tests,

00:30:50.110 --> 00:30:53.670
logical reasoning tests,
and god knows what else.

00:30:53.670 --> 00:30:56.520
Maybe it can shoot
straighter, read faster,

00:30:56.520 --> 00:30:58.430
and can outrun
the fastest human.

00:30:58.430 --> 00:30:59.910
Oh my god.

00:30:59.910 --> 00:31:02.110
Robots can outperform us.

00:31:02.110 --> 00:31:04.840
What are we all going to do?

00:31:04.840 --> 00:31:06.390
So are the robots taking over?

00:31:11.265 --> 00:31:14.580
Are the robots taking over?

00:31:14.580 --> 00:31:16.430
Of course, by the
logic I just gave you,

00:31:16.430 --> 00:31:18.900
machines took over a long time
ago whether they are smart

00:31:18.900 --> 00:31:19.714
or not.

00:31:19.714 --> 00:31:20.630
They move our freight.

00:31:20.630 --> 00:31:21.504
They score our tests.

00:31:21.504 --> 00:31:22.730
They explore the cosmos.

00:31:22.730 --> 00:31:25.450
They plant and pick
most of our crops.

00:31:25.450 --> 00:31:26.630
They trade stocks.

00:31:26.630 --> 00:31:31.065
They store and retrieve our
documents, as Jacob knows,

00:31:31.065 --> 00:31:32.920
in petabytes.

00:31:32.920 --> 00:31:35.110
They manufacture just
about everything,

00:31:35.110 --> 00:31:37.310
including themselves.

00:31:37.310 --> 00:31:39.150
And sometimes they do
it with human help,

00:31:39.150 --> 00:31:41.220
and sometimes without
human intervention.

00:31:41.220 --> 00:31:45.160
And yet, they aren't
taking over our businesses.

00:31:45.160 --> 00:31:48.190
They aren't marrying
our children.

00:31:48.190 --> 00:31:52.290
They are not watching the SyFy
channel when we're not around.

00:31:52.290 --> 00:31:56.160
So what's wrong with the
traditional picture of AI?

00:31:56.160 --> 00:31:58.660
We can build machines and
write programs and perform

00:31:58.660 --> 00:32:00.945
tasks that previously
required human intelligence

00:32:00.945 --> 00:32:06.690
and attention, but there's
really nothing new about that.

00:32:06.690 --> 00:32:09.190
Each new technological
breakthrough

00:32:09.190 --> 00:32:12.640
from the invention of
the plow to to the CGI

00:32:12.640 --> 00:32:16.170
rendering of Rapunzel's
hair is better

00:32:16.170 --> 00:32:18.530
understood as an
advance in automation,

00:32:18.530 --> 00:32:22.360
not as a usurpation
of human primacy.

00:32:22.360 --> 00:32:26.370
We can program machines to
solve very complex problems,

00:32:26.370 --> 00:32:29.560
and they may operate with
increasing independence.

00:32:29.560 --> 00:32:32.770
But as a friend of
mine once observed,

00:32:32.770 --> 00:32:35.350
a vehicle will
really be autonomous

00:32:35.350 --> 00:32:37.380
when you instruct it to
take you to the office,

00:32:37.380 --> 00:32:39.130
and it decides to go
to the beach instead.

00:32:42.650 --> 00:32:44.914
My point is simple.

00:32:44.914 --> 00:32:47.080
Lots of problems we think
require human intelligence

00:32:47.080 --> 00:32:49.279
to solve actually don't.

00:32:49.279 --> 00:32:51.070
There are lots of other
ways to solve them,

00:32:51.070 --> 00:32:53.170
and that's what the
machines are doing.

00:32:53.170 --> 00:32:55.500
Calculating used
to be the province

00:32:55.500 --> 00:32:57.140
of highly trained specialists.

00:32:57.140 --> 00:32:58.120
Did you guys know that?

00:32:58.120 --> 00:32:59.910
You used to go see
somebody when you

00:32:59.910 --> 00:33:03.790
wanted to do some calculation.

00:33:03.790 --> 00:33:06.600
Now all it takes is
the $0.99 calculator.

00:33:06.600 --> 00:33:08.180
Making money in the
stock market used

00:33:08.180 --> 00:33:09.830
to be the province of experts.

00:33:09.830 --> 00:33:14.030
Now the majority of trading
is initiated by computers.

00:33:14.030 --> 00:33:16.440
It's the same for driving
directions, picking and packing

00:33:16.440 --> 00:33:19.270
orders in warehouses, and
designing more efficient wings

00:33:19.270 --> 00:33:21.230
for airplanes.

00:33:21.230 --> 00:33:24.920
But you don't have to worry
about the robots taking over.

00:33:24.920 --> 00:33:28.130
Robots don't have feelings,
except in the movies.

00:33:28.130 --> 00:33:29.410
Here's a news flash for you.

00:33:29.410 --> 00:33:30.805
They aren't male or female.

00:33:33.500 --> 00:33:35.740
As I like to say to
my Stanford students,

00:33:35.740 --> 00:33:39.730
what does it mean for
a robot to be gay?

00:33:39.730 --> 00:33:46.010
So robots don't have
independent goals and desires.

00:33:46.010 --> 00:33:48.490
A robot that's designed
to wash and fold laundry

00:33:48.490 --> 00:33:51.730
isn't going to wake up one day
and say, oh my god, what a fool

00:33:51.730 --> 00:33:52.300
I've been.

00:33:52.300 --> 00:33:54.771
I really want to play the
great concert halls of Europe.

00:33:57.600 --> 00:34:03.750
So just as we can teach
bears to ride bikes,

00:34:03.750 --> 00:34:06.940
and we can teach chimps
to use sign language,

00:34:06.940 --> 00:34:08.520
we could build
machines to perform

00:34:08.520 --> 00:34:11.239
tasks the way
people do, and even

00:34:11.239 --> 00:34:13.040
to simulate human emotions.

00:34:13.040 --> 00:34:16.060
We can make them say ouch
when you pinch them or wag

00:34:16.060 --> 00:34:17.885
their tails when you pet them.

00:34:17.885 --> 00:34:19.510
But there's simply
no compelling reason

00:34:19.510 --> 00:34:22.139
to believe this bears any
meaningful relationship

00:34:22.139 --> 00:34:24.620
to human behavior or experience.

00:34:24.620 --> 00:34:27.260
Machines aren't
people, even if we

00:34:27.260 --> 00:34:32.230
build them to talk and walk and
chew gum the way that we do.

00:34:32.230 --> 00:34:37.171
OK, now I've given
you a new way to think

00:34:37.171 --> 00:34:38.420
about artificial intelligence.

00:34:38.420 --> 00:34:40.370
Let's talk about
the implications

00:34:40.370 --> 00:34:43.050
of this new perspective.

00:34:43.050 --> 00:34:44.550
I'm going to try
to run through this

00:34:44.550 --> 00:34:47.420
pretty quickly because
I was warned people like

00:34:47.420 --> 00:34:48.362
to ask questions.

00:34:48.362 --> 00:34:49.820
Now, it's certainly
true that AI is

00:34:49.820 --> 00:34:52.040
going to have a
serious impact on labor

00:34:52.040 --> 00:34:54.429
markets and employment.

00:34:54.429 --> 00:34:56.530
But perhaps not in the
way that people expect.

00:34:56.530 --> 00:35:00.510
If you think of machines as
becoming even more intelligent

00:35:00.510 --> 00:35:04.210
and threatening our livelihoods,
the obvious solution

00:35:04.210 --> 00:35:06.990
is to prevent them from getting
smarter, and to lock our doors

00:35:06.990 --> 00:35:09.397
and arm ourselves with Tasers
against these robots that

00:35:09.397 --> 00:35:11.770
are coming to take our jobs.

00:35:11.770 --> 00:35:17.990
Well, the robots are coming,
but not exactly for our jobs.

00:35:17.990 --> 00:35:21.670
Machines and computers
don't perform jobs.

00:35:21.670 --> 00:35:23.640
They automate tasks.

00:35:23.640 --> 00:35:27.050
Now, except in extreme cases,
you don't roll in a robot

00:35:27.050 --> 00:35:29.030
and show an employee
to the door.

00:35:29.030 --> 00:35:33.640
Instead, the new technologies
hollow out and change the jobs

00:35:33.640 --> 00:35:35.689
that people perform.

00:35:35.689 --> 00:35:37.480
Even experts spend most
of their time doing

00:35:37.480 --> 00:35:39.070
mundane, repetitive tasks.

00:35:39.070 --> 00:35:40.850
They review lab tests.

00:35:40.850 --> 00:35:42.460
They draft simple contracts.

00:35:42.460 --> 00:35:44.785
They write straightforward
press releases.

00:35:44.785 --> 00:35:46.855
They fill out
paperwork and forms.

00:35:46.855 --> 00:35:48.830
On the blue collar
side, lots of workers

00:35:48.830 --> 00:35:52.400
lay bricks, paint houses, mow
lawns, drive cars, load trucks,

00:35:52.400 --> 00:35:54.430
pack boxes, and
take blood samples.

00:35:54.430 --> 00:35:58.380
They fight fires, deliver
direct traffic, et cetera.

00:35:58.380 --> 00:36:01.860
And many of these intellectual
and physical tasks

00:36:01.860 --> 00:36:07.950
require straightforward logic
or simple hand-eye coordination.

00:36:07.950 --> 00:36:09.950
Now, the new technologies,
mainly driven

00:36:09.950 --> 00:36:13.970
by artificial intelligence, are
poised to automate these tasks,

00:36:13.970 --> 00:36:16.580
not to replace the jobs.

00:36:16.580 --> 00:36:20.960
Now, if your job involves
a narrow, well-defined set

00:36:20.960 --> 00:36:23.490
of duties, and many
do, then indeed,

00:36:23.490 --> 00:36:25.520
your employment is at risk.

00:36:25.520 --> 00:36:27.555
If you have a broader
set of responsibilities,

00:36:27.555 --> 00:36:30.380
or if your job requires a
human touch such as expressing

00:36:30.380 --> 00:36:34.080
sympathy or providing
companionship,

00:36:34.080 --> 00:36:36.692
I don't think you have
too much to worry about.

00:36:36.692 --> 00:36:38.150
Now, just check
out this comparison

00:36:38.150 --> 00:36:42.100
of the job duties between
licensed practical nurses

00:36:42.100 --> 00:36:43.590
and bricklayers.

00:36:43.590 --> 00:36:47.770
Whose job do you think is
most at risk from automation?

00:36:47.770 --> 00:36:50.670
By the way, this
list is hilarious.

00:36:50.670 --> 00:36:56.190
"Monitoring fluid and
food intake and output."

00:36:56.190 --> 00:36:58.825
I was like, OK, I didn't
know they measure output.

00:37:02.070 --> 00:37:04.583
"Providing emotional support."

00:37:04.583 --> 00:37:06.416
What you guys are working
on-- (ROBOT VOICE)

00:37:06.416 --> 00:37:08.174
I am so sorry
about your problem.

00:37:08.174 --> 00:37:08.840
I mean, come on.

00:37:11.940 --> 00:37:15.120
Most jobs, as opposed
to tasks, involve

00:37:15.120 --> 00:37:18.060
a mix of general capabilities
and specific skills.

00:37:18.060 --> 00:37:20.780
And as machines perform
the more routine tasks,

00:37:20.780 --> 00:37:22.850
the plain fact is
that fewer people are

00:37:22.850 --> 00:37:24.520
needed to get the jobs done.

00:37:24.520 --> 00:37:27.970
So one person's
productivity enhancing tool

00:37:27.970 --> 00:37:31.420
is in fact another's pink
slip, or more likely,

00:37:31.420 --> 00:37:35.370
a job opening that no
longer needs to be filled.

00:37:35.370 --> 00:37:37.580
Now, this is called
structural unemployment.

00:37:37.580 --> 00:37:40.300
Automation, whether it's driven
by artificial intelligence

00:37:40.300 --> 00:37:42.320
or not, it changes
the skills that

00:37:42.320 --> 00:37:44.866
are necessary to perform work.

00:37:44.866 --> 00:37:48.450
I need to move ahead, because
we're running out of time.

00:37:48.450 --> 00:37:50.930
So this is called
structural unemployment,

00:37:50.930 --> 00:37:55.110
and it's the mismatch of skills
against the needs of employers.

00:37:55.110 --> 00:37:59.800
People get put out of
work because it's not

00:37:59.800 --> 00:38:01.660
so much that there's
a lack of jobs,

00:38:01.660 --> 00:38:04.830
but the training that people
need to perform those jobs--

00:38:04.830 --> 00:38:06.250
there's a disconnect.

00:38:06.250 --> 00:38:09.340
Now, historically, as
automation has eliminated

00:38:09.340 --> 00:38:15.990
the need for workers, the
resulting increase in wealth

00:38:15.990 --> 00:38:17.790
has eventually generated
new kinds of jobs

00:38:17.790 --> 00:38:18.810
to take up the slack.

00:38:18.810 --> 00:38:21.820
And I see no reason that pattern
is not going to continue,

00:38:21.820 --> 00:38:24.600
but the keyword
there is eventually.

00:38:24.600 --> 00:38:27.100
Let's talk about
farm employment.

00:38:27.100 --> 00:38:28.900
This stuff is amazing,
if you look into it.

00:38:28.900 --> 00:38:33.990
200 years ago, more than
90% of the US population

00:38:33.990 --> 00:38:35.790
worked in agriculture.

00:38:35.790 --> 00:38:39.550
Basically, almost all anyone
did was grow and prepare food.

00:38:39.550 --> 00:38:41.720
That's what it meant to work.

00:38:41.720 --> 00:38:43.590
Now, today, less than
2% of the population

00:38:43.590 --> 00:38:46.528
is required to feed
everybody, as you can

00:38:46.528 --> 00:38:49.940
see in the free food over here.

00:38:49.940 --> 00:38:51.940
Oh my god, is
everybody out of work?

00:38:51.940 --> 00:38:52.830
Of course not.

00:38:52.830 --> 00:38:54.620
We've had plenty
of time to adapt,

00:38:54.620 --> 00:38:57.580
and as our standard of living
has relentlessly increased,

00:38:57.580 --> 00:39:00.160
which I'll get to in a
minute, new opportunities

00:39:00.160 --> 00:39:03.340
have always arisen for
people to fill the expanding

00:39:03.340 --> 00:39:08.564
expectations of our ever
richer and greedier society.

00:39:08.564 --> 00:39:10.480
Now, if a person from
1800 could see us today,

00:39:10.480 --> 00:39:12.630
they'd think we'd all gone nuts.

00:39:12.630 --> 00:39:16.482
Why not work a few hours a
week, buy a sack of potatoes

00:39:16.482 --> 00:39:18.440
and a jug of wine , build
a shack in the woods,

00:39:18.440 --> 00:39:23.560
dig a hole for an outhouse,
and live a life of leisure?

00:39:23.560 --> 00:39:26.040
Somehow, our rising
expectations seem

00:39:26.040 --> 00:39:32.410
to be magically out of
pace due to our wealth.

00:39:32.410 --> 00:39:34.940
OK, so what are the
jobs of the future?

00:39:34.940 --> 00:39:39.820
I don't see why we can't be a
society of competitive gamers,

00:39:39.820 --> 00:39:44.340
artisans, personal shoppers,
flower arrangers, tennis pros,

00:39:44.340 --> 00:39:45.960
party planners,
and no doubt a lot

00:39:45.960 --> 00:39:48.520
of other things that
don't exist yet.

00:39:48.520 --> 00:39:50.770
You might say, well, who is
going to do the real work?

00:39:50.770 --> 00:39:52.470
Well, our great
grandchildren may

00:39:52.470 --> 00:39:56.020
think of our idea of real
work as so 21st century.

00:39:56.020 --> 00:40:00.740
It may take, as we think of
with agriculture-- it may only

00:40:00.740 --> 00:40:03.770
take 2% of the population,
assisted by some pretty

00:40:03.770 --> 00:40:06.660
remarkable automation,
to accomplish what's

00:40:06.660 --> 00:40:09.640
taking 90% of our labor today.

00:40:09.640 --> 00:40:11.910
So what?

00:40:11.910 --> 00:40:13.680
It may be as important
to them to have

00:40:13.680 --> 00:40:15.630
fresh flowers in
the house each day

00:40:15.630 --> 00:40:18.770
as it is for us to take
a shower every day,

00:40:18.770 --> 00:40:21.960
which 70% of the
US population does.

00:40:21.960 --> 00:40:26.260
By the way, in 1900, the
average was once a week.

00:40:26.260 --> 00:40:28.360
I'm glad I'm not there.

00:40:32.090 --> 00:40:33.200
So let me move ahead.

00:40:35.710 --> 00:40:38.060
That's the good news.

00:40:38.060 --> 00:40:39.810
The bad news is it
it's going to take time

00:40:39.810 --> 00:40:41.470
for this transition to happen.

00:40:41.470 --> 00:40:44.330
And there's a new wave of AI
enabled applications that's

00:40:44.330 --> 00:40:47.325
likely to accelerate the
normal cycle of job creation

00:40:47.325 --> 00:40:48.776
and destruction.

00:40:48.776 --> 00:40:50.400
So we're going to
need to find new ways

00:40:50.400 --> 00:40:54.564
to retrain displaced workers.

00:40:54.564 --> 00:40:55.730
I was going to go into this.

00:40:55.730 --> 00:40:57.146
I know Jacob's
interested in this,

00:40:57.146 --> 00:41:02.986
but hopefully, we're going to
have to skip over this idea.

00:41:02.986 --> 00:41:04.860
Our problem is our
vocational training system

00:41:04.860 --> 00:41:06.200
is really messed up.

00:41:06.200 --> 00:41:08.450
It's mainly because the
government today is the lender

00:41:08.450 --> 00:41:10.400
of first resort for students.

00:41:10.400 --> 00:41:12.290
So the skills that
people learn are

00:41:12.290 --> 00:41:15.050
disconnected from the
needs of the employers

00:41:15.050 --> 00:41:16.097
in the marketplace.

00:41:16.097 --> 00:41:17.930
So we're not actually
investing in education

00:41:17.930 --> 00:41:19.800
so much as we're
heading out money

00:41:19.800 --> 00:41:22.897
to people to learn things that
won't help them pay it back.

00:41:22.897 --> 00:41:23.730
You can't get a job?

00:41:23.730 --> 00:41:24.420
It's too bad.

00:41:24.420 --> 00:41:26.002
Your student loan is still due.

00:41:26.002 --> 00:41:27.668
How many of you guys
have student loans?

00:41:27.668 --> 00:41:30.432
OK, not bad.

00:41:30.432 --> 00:41:32.640
So there are different
ways to do this,

00:41:32.640 --> 00:41:34.900
and we need to create
new financial instruments

00:41:34.900 --> 00:41:36.820
that tie the
development of capital,

00:41:36.820 --> 00:41:40.289
the deployment of capital, to
the return on the investment.

00:41:40.289 --> 00:41:41.830
And I've got this
concept that I talk

00:41:41.830 --> 00:41:44.890
about in my book, which
is somewhere around here,

00:41:44.890 --> 00:41:47.120
that I call a job mortgage.

00:41:47.120 --> 00:41:51.190
So you get a mortgage
for your education,

00:41:51.190 --> 00:41:54.810
and it is payable solely out
of your future earnings stream.

00:41:54.810 --> 00:41:57.080
And that causes all
the right incentives

00:41:57.080 --> 00:41:59.511
to align so that we're teaching
people the right things.

00:41:59.511 --> 00:42:01.302
Otherwise, people aren't
going to give them

00:42:01.302 --> 00:42:04.640
the money if they don't know
that there's going to be

00:42:04.640 --> 00:42:07.210
a likelihood of a payback.

00:42:07.210 --> 00:42:09.950
Finally, there's one
other dark cloud.

00:42:09.950 --> 00:42:14.470
I painted a very optimistic
view of the future.

00:42:14.470 --> 00:42:19.430
While it's true that automation
makes a society richer,

00:42:19.430 --> 00:42:22.210
there are serious questions
about whose pockets

00:42:22.210 --> 00:42:24.640
are filled by that wealth.

00:42:24.640 --> 00:42:27.591
You may be aware while
we were on high tech,

00:42:27.591 --> 00:42:29.090
we tend to believe
we are developing

00:42:29.090 --> 00:42:33.150
dazzling technologies for
a needy and grateful world,

00:42:33.150 --> 00:42:34.890
and indeed, we've
made great progress

00:42:34.890 --> 00:42:36.931
in raising the standard
of living for the poorest

00:42:36.931 --> 00:42:37.720
people on Earth.

00:42:37.720 --> 00:42:40.160
But for the developed world,
the news is not so good.

00:42:40.160 --> 00:42:42.750
Up until about 1970,
on and off, we've

00:42:42.750 --> 00:42:44.520
found ways to
distribute at least some

00:42:44.520 --> 00:42:47.580
of those economic
benefits across society,

00:42:47.580 --> 00:42:50.200
and this was the rise in the
supposed-- the mythical middle

00:42:50.200 --> 00:42:51.230
class.

00:42:51.230 --> 00:42:54.470
But it doesn't take much to
see that those days are over.

00:42:54.470 --> 00:42:57.050
They began to diverge.

00:42:57.050 --> 00:43:00.140
So as economists
know, automation

00:43:00.140 --> 00:43:03.392
is the substitute of
capital for labor.

00:43:03.392 --> 00:43:05.405
And Karl Marx was right.

00:43:05.405 --> 00:43:07.390
The struggle between
capital and labor

00:43:07.390 --> 00:43:10.417
is a losing proposition
for workers.

00:43:10.417 --> 00:43:12.500
What that means is that
the benefits of automation

00:43:12.500 --> 00:43:16.790
naturally accrue to those who
can invest in the new systems.

00:43:16.790 --> 00:43:19.632
And why not?

00:43:19.632 --> 00:43:22.090
People aren't really working
harder than they used to work.

00:43:22.090 --> 00:43:27.290
In fact, they aren't really
smarter than they used to be.

00:43:27.290 --> 00:43:29.120
Working hours have
actually decreased

00:43:29.120 --> 00:43:32.550
slowly but consistently for
about the last 100 years.

00:43:32.550 --> 00:43:34.710
The reason we can
do more with less

00:43:34.710 --> 00:43:37.940
is that the business owners
invest some of their capital

00:43:37.940 --> 00:43:40.960
into the process and
productivity for improvements.

00:43:40.960 --> 00:43:45.840
And they reap the
most of the rewards.

00:43:45.840 --> 00:43:49.795
So what has all this
got to do with AI?

00:43:49.795 --> 00:43:51.670
Now, the technologies
that are on the drawing

00:43:51.670 --> 00:43:54.790
boards in our labs are
quickening the hearts

00:43:54.790 --> 00:43:57.130
of entrepreneurs and
investors everywhere,

00:43:57.130 --> 00:43:58.870
as you guys are well aware.

00:43:58.870 --> 00:44:01.320
And they are the ones
who stand to benefit

00:44:01.320 --> 00:44:03.760
while they export more
and more of the risk out

00:44:03.760 --> 00:44:06.030
to the rest of society.

00:44:06.030 --> 00:44:08.000
Workers are less secure today.

00:44:08.000 --> 00:44:09.430
Wages are stagnant.

00:44:09.430 --> 00:44:12.140
Pension funds can go bust.

00:44:12.140 --> 00:44:14.470
We're raising a
generation of contractors

00:44:14.470 --> 00:44:16.570
for the gig economy.

00:44:16.570 --> 00:44:18.730
They're working variable
hours, and health benefits

00:44:18.730 --> 00:44:19.880
are their own problem.

00:44:19.880 --> 00:44:21.700
That's not true for you guys.

00:44:21.700 --> 00:44:23.720
You have regular
employment jobs.

00:44:23.720 --> 00:44:25.140
But if you really
find out what's

00:44:25.140 --> 00:44:28.926
going on in the rest of
the world, this is true.

00:44:28.926 --> 00:44:30.800
Now, some people have
the mistaken impression

00:44:30.800 --> 00:44:32.050
that the free market
will naturally

00:44:32.050 --> 00:44:34.550
address these problems if only
we can get the government out

00:44:34.550 --> 00:44:35.156
of the way.

00:44:35.156 --> 00:44:38.340
And I'm here to tell you
that our economy is hardly

00:44:38.340 --> 00:44:41.580
an example of
unfettered capitalism.

00:44:41.580 --> 00:44:46.802
The fact is that there are all
sorts of rules and policies

00:44:46.802 --> 00:44:49.010
that drive where the capital
goes, how it's deployed,

00:44:49.010 --> 00:44:51.490
and who gets the returns.

00:44:51.490 --> 00:44:57.070
And the basic problem
is-- ah, this is great.

00:44:57.070 --> 00:45:00.620
I should show the slides
while I give the talk.

00:45:00.620 --> 00:45:03.010
The basic problem is that
our economic and regulatory

00:45:03.010 --> 00:45:06.770
policies have become decoupled
from our social goals.

00:45:06.770 --> 00:45:07.990
And we have to fix that.

00:45:07.990 --> 00:45:10.089
But the question is how?

00:45:10.089 --> 00:45:11.130
Now here's the good news.

00:45:15.330 --> 00:45:18.390
Most people have
no idea about this.

00:45:18.390 --> 00:45:20.810
The good news is that
the economy isn't static.

00:45:20.810 --> 00:45:22.590
It doubles about every 40 years.

00:45:22.590 --> 00:45:25.610
You guys are familiar with the
singularity and the Moore's

00:45:25.610 --> 00:45:26.810
curve and all that.

00:45:26.810 --> 00:45:29.220
That's happening
with the economy too,

00:45:29.220 --> 00:45:30.300
not just with computers.

00:45:30.300 --> 00:45:33.920
It doubles about every 40 years,
and it's done that reliably

00:45:33.920 --> 00:45:35.300
since the start
of the Industrial

00:45:35.300 --> 00:45:37.604
Revolution in the 1700s.

00:45:37.604 --> 00:45:42.130
In 1800, the average
household income was $1,000.

00:45:42.130 --> 00:45:43.770
And that's about
the same as it is

00:45:43.770 --> 00:45:47.751
today in Malawi and Mozambique.

00:45:47.751 --> 00:45:49.750
And probably not
coincidentally, their economies

00:45:49.750 --> 00:45:53.670
look surprisingly similar to
what the US was 200 years ago.

00:45:53.670 --> 00:45:57.270
Yet I doubt that people
in Ben Franklin's time

00:45:57.270 --> 00:45:59.890
thought of themselves
as dirt poor--

00:45:59.890 --> 00:46:03.450
that they were barely
scratching out an existence.

00:46:03.450 --> 00:46:06.907
So what this means is that 40
years from now, most likely

00:46:06.907 --> 00:46:08.990
there's literally going
to be twice as much wealth

00:46:08.990 --> 00:46:10.190
to go around.

00:46:10.190 --> 00:46:14.370
So the challenge for us
is to implement policies

00:46:14.370 --> 00:46:17.825
that will encourage that wealth
to be more broadly distributed.

00:46:17.825 --> 00:46:20.200
We don't have to take from
the rich and give to the poor.

00:46:20.200 --> 00:46:22.750
We need to provide
incentives for entrepreneurs

00:46:22.750 --> 00:46:24.540
and businesses to
find ways to benefit

00:46:24.540 --> 00:46:27.020
ever larger swaths of society.

00:46:27.020 --> 00:46:30.820
So in my book, again,
I just give you

00:46:30.820 --> 00:46:32.870
an example of the
kinds of policies

00:46:32.870 --> 00:46:36.075
that smart folks like
you could come up with.

00:46:36.075 --> 00:46:38.450
And the idea here is to make
corporate taxes progressive.

00:46:38.450 --> 00:46:40.730
I'm not saying this is the
answer or even an answer.

00:46:40.730 --> 00:46:43.820
It's just the kind of
thinking we need to do.

00:46:43.820 --> 00:46:46.270
You can make corporate
taxes progressive

00:46:46.270 --> 00:46:49.610
based on how widely distributed
the equity in a company is.

00:46:49.610 --> 00:46:53.320
So companies that have
larger stockholder bases

00:46:53.320 --> 00:46:55.640
have a lower tax rate.

00:46:55.640 --> 00:46:57.490
Microsoft, to use
them as an example,

00:46:57.490 --> 00:46:59.160
they should pay a
far lower tax rate

00:46:59.160 --> 00:47:03.050
than Bechtel, which
is privately held.

00:47:03.050 --> 00:47:04.850
Now, progressive
policies like this,

00:47:04.850 --> 00:47:06.558
to promote our social
goals-- by the way,

00:47:06.558 --> 00:47:08.930
I flesh that out in the book
in quite a bit of detail,

00:47:08.930 --> 00:47:14.500
how it would work, and I
encourage to you to buy a copy,

00:47:14.500 --> 00:47:17.540
if not read one.

00:47:17.540 --> 00:47:21.260
Progressive policies like that
can promote our social goals

00:47:21.260 --> 00:47:22.690
without stifling the economy.

00:47:22.690 --> 00:47:25.106
We just have to get on with
it and stop believing the myth

00:47:25.106 --> 00:47:28.440
that unfettered capitalism
is the answer to the world's

00:47:28.440 --> 00:47:29.630
problems.

00:47:29.630 --> 00:47:34.080
So let me wrap
things up and recap.

00:47:34.080 --> 00:47:35.720
I don't want you to
think I'm anti-AI.

00:47:35.720 --> 00:47:37.094
Nothing's further
from the truth.

00:47:37.094 --> 00:47:42.260
I think the potential
impact on world is similar,

00:47:42.260 --> 00:47:45.580
and I'm not exaggerating
this-- the potential impact is

00:47:45.580 --> 00:47:49.760
about the same as the
invention of the wheel.

00:47:49.760 --> 00:47:51.400
We need to think of
it not of some sort

00:47:51.400 --> 00:47:54.630
of magical discontinuity in the
development of intelligent life

00:47:54.630 --> 00:47:58.310
on earth, but as a powerful
collection of automation tools

00:47:58.310 --> 00:48:01.230
with the potential to
transform our livelihoods

00:48:01.230 --> 00:48:03.680
and to vastly
increase our wealth.

00:48:03.680 --> 00:48:06.550
The challenge we face is that
our existing institutions,

00:48:06.550 --> 00:48:08.305
without some
enlightened rethinking,

00:48:08.305 --> 00:48:13.260
run a serious risk of making
a mess of this opportunity.

00:48:13.260 --> 00:48:15.050
I'm supremely confident
that our future

00:48:15.050 --> 00:48:17.060
is very bright--
that it it's more

00:48:17.060 --> 00:48:19.120
"Star Trek" than "Terminator."

00:48:19.120 --> 00:48:22.780
But the transition is going
to be protracted and brutal

00:48:22.780 --> 00:48:24.520
unless we pay
attention to the issues

00:48:24.520 --> 00:48:26.750
that I tried to raise
with you here today.

00:48:26.750 --> 00:48:28.530
We have to find
new and better ways

00:48:28.530 --> 00:48:31.950
to ensure that our
economy doesn't motor on,

00:48:31.950 --> 00:48:35.000
going faster and faster, while
throwing ever more people

00:48:35.000 --> 00:48:36.760
overboard.

00:48:36.760 --> 00:48:40.400
Our technology and our
economy should serve us, not

00:48:40.400 --> 00:48:42.610
the other way around.

00:48:42.610 --> 00:48:43.260
So thank you.

00:48:43.260 --> 00:48:45.610
I'm sorry to run so long.

00:48:45.610 --> 00:48:49.020
Next time I give the talk,
it would be half this long.

00:48:49.020 --> 00:48:51.390
[APPLAUSE]

00:48:56.610 --> 00:48:58.411
Do we have time for questions?

00:48:58.411 --> 00:49:00.660
MALE SPEAKER: Yeah, so we
do have a little bit of time

00:49:00.660 --> 00:49:01.290
for questions.

00:49:01.290 --> 00:49:03.290
There is a mic placed
right over there

00:49:03.290 --> 00:49:06.350
for those who want to ask
questions, please line up.

00:49:06.350 --> 00:49:08.970
You guys need to do that.

00:49:08.970 --> 00:49:10.420
Go for it.

00:49:10.420 --> 00:49:14.470
AUDIENCE: So for the task
of taking over the world,

00:49:14.470 --> 00:49:19.460
are there other means except
for having human intelligence?

00:49:19.460 --> 00:49:22.090
JERRY KAPLAN: Yeah, it's
a very dangerous issue,

00:49:22.090 --> 00:49:24.760
as a matter of fact, because
a lot of the AI technologies,

00:49:24.760 --> 00:49:27.480
we talk about the
productivity and all of that.

00:49:27.480 --> 00:49:32.690
But they have very serious
applications in, for example,

00:49:32.690 --> 00:49:34.570
military use.

00:49:34.570 --> 00:49:36.720
And this is a very
difficult problem.

00:49:36.720 --> 00:49:38.950
A lot of very smart
people are actually--

00:49:38.950 --> 00:49:42.775
I wouldn't say secretly,
but not publicly working on.

00:49:42.775 --> 00:49:44.900
A friend of mine is on his
way to Geneva right now.

00:49:48.240 --> 00:49:51.140
There's meetings
regularly with the UN.

00:49:51.140 --> 00:49:53.870
There's a lot going
on in the US military,

00:49:53.870 --> 00:49:57.010
because they recognize that just
going ahead and implementing

00:49:57.010 --> 00:50:01.150
the kinds of technologies to the
battlefield that are currently

00:50:01.150 --> 00:50:04.400
being applied to driving
cars and other things

00:50:04.400 --> 00:50:07.200
might backfire, because
it would be a lot easier

00:50:07.200 --> 00:50:10.860
for non-state actors,
to put it politely,

00:50:10.860 --> 00:50:15.130
and dictators to-- today it
takes enormous investment

00:50:15.130 --> 00:50:18.070
to make and buy bombers
and all that kind of stuff.

00:50:18.070 --> 00:50:20.610
It's going to be really cheap,
just like everything else,

00:50:20.610 --> 00:50:21.660
like cellphones.

00:50:21.660 --> 00:50:24.610
And there are some
really creepy things

00:50:24.610 --> 00:50:27.970
that can be done to
take over the world

00:50:27.970 --> 00:50:32.177
and wipe out humanity
at a very low cost,

00:50:32.177 --> 00:50:34.085
and that's going to
be a big problem.

00:50:36.950 --> 00:50:39.190
AUDIENCE: So you gave a
lot of examples of problems

00:50:39.190 --> 00:50:40.898
that we thought that
were innately human,

00:50:40.898 --> 00:50:43.470
but we were later able to
describe it in a different way.

00:50:43.470 --> 00:50:47.750
So why should we doubt
that unbounded learning

00:50:47.750 --> 00:50:50.200
is a problem we can't
describe in a different way?

00:50:50.200 --> 00:50:52.050
By unbounded learning,
I mean the example

00:50:52.050 --> 00:50:53.680
you gave-- oh, that's a cat.

00:50:53.680 --> 00:50:55.880
Or for humans, you know,
if you ask them, pass me

00:50:55.880 --> 00:50:59.240
the purple cup, they'll
learn that's purple

00:50:59.240 --> 00:51:01.190
if they didn't know that word.

00:51:01.190 --> 00:51:03.736
Why can't that unbounded
learning be described in a way

00:51:03.736 --> 00:51:05.860
that we can train machines
to learn in the same way

00:51:05.860 --> 00:51:07.420
that babies learn?

00:51:07.420 --> 00:51:10.820
JERRY KAPLAN: Well, I'm
going to turn your question

00:51:10.820 --> 00:51:13.120
around a little bit to put
it in the context of what

00:51:13.120 --> 00:51:13.890
I said here.

00:51:13.890 --> 00:51:16.730
I'm not saying that any
particular task is completely

00:51:16.730 --> 00:51:20.010
impervious to machine learning.

00:51:20.010 --> 00:51:22.420
In fact, it might very well be.

00:51:22.420 --> 00:51:23.950
However, machine
learning is really

00:51:23.950 --> 00:51:26.920
good at picking out patterns
in large volumes of data

00:51:26.920 --> 00:51:28.390
as it's practiced today.

00:51:28.390 --> 00:51:31.400
It could be a future form of
software technology, which

00:51:31.400 --> 00:51:33.380
could do something more
into what you said,

00:51:33.380 --> 00:51:37.280
but that doesn't mean that they
have human-like characteristics

00:51:37.280 --> 00:51:38.034
or human learning.

00:51:38.034 --> 00:51:39.700
And it doesn't mean
that all of our jobs

00:51:39.700 --> 00:51:42.330
will go away because
a lot of our jobs

00:51:42.330 --> 00:51:49.080
require face-to-face interaction
or the expression of empathy.

00:51:49.080 --> 00:51:53.010
And I don't buy the idea
that machines can effectively

00:51:53.010 --> 00:51:57.080
express empathy in their
dealings with other people.

00:51:57.080 --> 00:51:59.310
So the tasks can go
away, and maybe they

00:51:59.310 --> 00:52:00.750
will be able to learn as well.

00:52:00.750 --> 00:52:01.699
That would be great.

00:52:01.699 --> 00:52:02.240
That's a cat.

00:52:02.240 --> 00:52:03.140
That's a chair.

00:52:03.140 --> 00:52:05.960
That's that-- boom, boom,
boom, the machine's got it.

00:52:05.960 --> 00:52:09.480
But that's just another step
in a long line of things

00:52:09.480 --> 00:52:11.360
where people looked,
and they went, wow!

00:52:11.360 --> 00:52:13.030
It used to take
people to do that.

00:52:13.030 --> 00:52:15.160
Now a machine can do it.

00:52:15.160 --> 00:52:17.380
It's just the next step.

00:52:17.380 --> 00:52:19.520
So a lot of that stuff
is going to go away.

00:52:19.520 --> 00:52:21.260
And if we all wind
up-- nobody wants

00:52:21.260 --> 00:52:24.560
to watch a robot play
competitive tennis.

00:52:24.560 --> 00:52:26.270
It's just not interesting.

00:52:26.270 --> 00:52:29.340
So I mean, there are lots of
these jobs that inherently

00:52:29.340 --> 00:52:30.810
require human beings.

00:52:30.810 --> 00:52:33.890
So I'm trying to separate
the task of automation

00:52:33.890 --> 00:52:38.650
from what will people do.

00:52:38.650 --> 00:52:40.959
And I hope that began--

00:52:40.959 --> 00:52:43.500
AUDIENCE: Yeah, I guess it was
even just the empathy example.

00:52:43.500 --> 00:52:47.125
You say that robots can't be
empathetic, but maybe they can.

00:52:47.125 --> 00:52:49.250
Maybe we just think that's
an innately human thing.

00:52:49.250 --> 00:52:51.458
The tennis example actually
is very convincing to me,

00:52:51.458 --> 00:52:54.049
like, I would never watch
a robot play tennis.

00:52:54.049 --> 00:52:55.590
But if a robot was
just as empathetic

00:52:55.590 --> 00:52:58.190
and had a human form,
and you couldn't

00:52:58.190 --> 00:53:01.040
tell that it was a robot when
you walked into the doctor.

00:53:01.040 --> 00:53:03.781
We think it's a human
problem, but maybe it's not.

00:53:03.781 --> 00:53:06.030
JERRY KAPLAN: Boy, this is
a really complicated topic.

00:53:06.030 --> 00:53:07.310
What you said is right.

00:53:07.310 --> 00:53:09.210
We can fool people.

00:53:09.210 --> 00:53:10.820
And we do this all the time.

00:53:10.820 --> 00:53:13.634
We can build a way-- this has
gone on since the 16th century,

00:53:13.634 --> 00:53:14.800
where they built automatons.

00:53:14.800 --> 00:53:16.854
We went, oh, my god,
it's just like a person.

00:53:16.854 --> 00:53:17.770
And they were amazing.

00:53:17.770 --> 00:53:18.811
Have you ever seen these?

00:53:18.811 --> 00:53:21.440
These mechanical devices are
absolutely incredible-- 16th

00:53:21.440 --> 00:53:22.300
and 17th century.

00:53:22.300 --> 00:53:27.820
It was fun entertainment
for the courts of kings.

00:53:27.820 --> 00:53:32.953
But if you know it's a machine,
the fact that the screen comes

00:53:32.953 --> 00:53:35.135
up and says, thank you,
I really appreciate

00:53:35.135 --> 00:53:36.320
that you placed your order.

00:53:36.320 --> 00:53:37.960
I mean, come on.

00:53:37.960 --> 00:53:43.850
It just doesn't
compute emotionally.

00:53:43.850 --> 00:53:45.420
We're not going
to buy that story.

00:53:45.420 --> 00:53:52.040
So a lot of it has to do with,
like, toys that look like dogs

00:53:52.040 --> 00:53:54.070
or look like children or play.

00:53:54.070 --> 00:53:56.340
It's all very complicated,
because play--

00:53:56.340 --> 00:54:00.990
if you're doing it knowingly,
that's perfectly reasonable.

00:54:00.990 --> 00:54:06.667
If you're doing play because
you're being fooled, or being

00:54:06.667 --> 00:54:09.000
persuaded to buy something
because the machine has gone,

00:54:09.000 --> 00:54:10.000
oh, come on.

00:54:10.000 --> 00:54:12.090
I got a whole bunch of
hungry mouths to feed.

00:54:12.090 --> 00:54:16.400
Oh, please, buy this car for me.

00:54:16.400 --> 00:54:17.930
We're not going to like that.

00:54:17.930 --> 00:54:18.930
That's my point.

00:54:18.930 --> 00:54:19.785
AUDIENCE: Thank you.

00:54:19.785 --> 00:54:20.660
JERRY KAPLAN: Thanks.

00:54:20.660 --> 00:54:21.420
Yes, sir.

00:54:21.420 --> 00:54:23.834
No waiting on check stand 2.

00:54:23.834 --> 00:54:26.000
AUDIENCE: So I agreed with
most everything you said.

00:54:26.000 --> 00:54:28.566
I had a problem with
one of your examples,

00:54:28.566 --> 00:54:29.940
and this may seem
like a nitpick.

00:54:29.940 --> 00:54:32.600
But I'm going to
flip this around.

00:54:32.600 --> 00:54:38.220
You said teaching a machine
a new task was like teaching

00:54:38.220 --> 00:54:41.320
a primate to use sign language.

00:54:41.320 --> 00:54:45.270
So do you think primates
don't use intelligence the way

00:54:45.270 --> 00:54:48.374
we do and don't understand
what's being said?

00:54:48.374 --> 00:54:49.790
JERRY KAPLAN:
That's a good point.

00:54:49.790 --> 00:54:52.070
I think that the point
I was trying to make

00:54:52.070 --> 00:54:55.280
is different than the one
that you-- I'm not saying I

00:54:55.280 --> 00:54:58.060
didn't say what you said, but
that's not really what I meant.

00:54:58.060 --> 00:54:59.730
What I meant was you
can take something

00:54:59.730 --> 00:55:04.420
that has no natural affinity
for that particular task,

00:55:04.420 --> 00:55:06.420
and you can get
it to do that task

00:55:06.420 --> 00:55:07.970
to some level of competence.

00:55:07.970 --> 00:55:09.340
That's what I was trying to say.

00:55:09.340 --> 00:55:12.320
Your point about chimp
stuff is pretty interesting,

00:55:12.320 --> 00:55:14.740
because obviously
they have brains.

00:55:14.740 --> 00:55:17.140
And I think most
reasonable people think

00:55:17.140 --> 00:55:20.240
they have rudimentary minds.

00:55:20.240 --> 00:55:24.940
But the point is they don't
naturally use sign language,

00:55:24.940 --> 00:55:29.190
and as I say, you teach
a bear to ride a bike.

00:55:29.190 --> 00:55:31.720
That's not like, oh, my god,
what are we going to do?

00:55:31.720 --> 00:55:34.990
Next thing you know we'll be
teaching bears to drive cars.

00:55:34.990 --> 00:55:39.460
It's that we can make machines
that also appear human-like

00:55:39.460 --> 00:55:40.960
and do these
human-like activities,

00:55:40.960 --> 00:55:44.900
but it's not a natural
part of the process.

00:55:44.900 --> 00:55:47.510
Machines have certain
characteristics,

00:55:47.510 --> 00:55:49.344
and I can give another
talk on that subject.

00:55:49.344 --> 00:55:50.468
What are those characters ?

00:55:50.468 --> 00:55:52.030
And they're different
than people,

00:55:52.030 --> 00:55:53.971
and we need to
understand that and stop

00:55:53.971 --> 00:55:56.220
thinking about ourselves as
we're making more and more

00:55:56.220 --> 00:55:57.470
intelligent machines.

00:55:57.470 --> 00:55:59.140
I'm just giving you
the framing to help

00:55:59.140 --> 00:56:02.224
us to understand the economic
results that are appropriate.

00:56:02.224 --> 00:56:02.724
Thank you.

00:56:02.724 --> 00:56:04.140
MALE SPEAKER: If
we go quickly, we

00:56:04.140 --> 00:56:05.760
have time for two
more questions.

00:56:05.760 --> 00:56:07.590
JERRY KAPLAN: Two more, OK.

00:56:07.590 --> 00:56:10.460
AUDIENCE: You were talking a
bit about the social impacts,

00:56:10.460 --> 00:56:14.040
and in the end, the people
who own the machines

00:56:14.040 --> 00:56:15.625
get the benefit
from the machines.

00:56:15.625 --> 00:56:17.500
And I agreed there when
you talked about ways

00:56:17.500 --> 00:56:19.875
to change policy,
but historically,

00:56:19.875 --> 00:56:21.620
social-focused
policies have come

00:56:21.620 --> 00:56:24.130
from things like labor
movements-- people

00:56:24.130 --> 00:56:26.130
controlling the means of
production or whatever.

00:56:26.130 --> 00:56:28.360
These kinds of things
where people go on strike.

00:56:28.360 --> 00:56:31.250
Who is going to strike if
all the people doing tasks

00:56:31.250 --> 00:56:33.114
have just been replaced?

00:56:33.114 --> 00:56:33.780
I own a machine.

00:56:33.780 --> 00:56:34.400
I don't have any workers, right?

00:56:34.400 --> 00:56:36.316
I got a factory that
builds everything I need,

00:56:36.316 --> 00:56:37.610
and I try to sell it to people.

00:56:37.610 --> 00:56:40.223
And eventually it might
implode, I don't know,

00:56:40.223 --> 00:56:41.630
if everybody is doing it.

00:56:41.630 --> 00:56:43.280
But the remaining
jobs are really

00:56:43.280 --> 00:56:48.210
just kind of these neat,
supervisory things or the 1%

00:56:48.210 --> 00:56:51.880
of the population that can
get a big audience on whatever

00:56:51.880 --> 00:56:53.495
medium.

00:56:53.495 --> 00:56:55.370
How do you run a whole
economy based on that?

00:56:55.370 --> 00:56:57.911
JERRY KAPLAN: Well, there are
two basic points you're making.

00:56:57.911 --> 00:57:00.090
Let me try to respond
to them each briefly.

00:57:00.090 --> 00:57:04.285
Because you're right
in a lot of senses.

00:57:04.285 --> 00:57:08.390
When you automate people
out of jobs, for those jobs,

00:57:08.390 --> 00:57:09.632
we don't need people.

00:57:09.632 --> 00:57:11.340
And the question is
when are the new jobs

00:57:11.340 --> 00:57:13.072
going to arrive, if ever?

00:57:13.072 --> 00:57:15.030
Most people are thinking
about this statically,

00:57:15.030 --> 00:57:17.450
like, well, we're just going to
automate away 90% of the jobs

00:57:17.450 --> 00:57:19.241
when we can build
machines that dig ditches

00:57:19.241 --> 00:57:20.800
and do all that kind of stuff.

00:57:20.800 --> 00:57:22.760
But I think historically
what happens

00:57:22.760 --> 00:57:26.910
is new jobs are created
that require humans

00:57:26.910 --> 00:57:28.780
for one reason or another.

00:57:28.780 --> 00:57:31.420
And I tried to kind
of make that point.

00:57:31.420 --> 00:57:33.380
We really can be
a leisure society.

00:57:33.380 --> 00:57:35.690
What we would think of
as a leisure society,

00:57:35.690 --> 00:57:38.480
that's what you'll get
paid for in the future.

00:57:38.480 --> 00:57:42.415
In 80 years, the average
American, if these trends hold,

00:57:42.415 --> 00:57:43.915
the average American
household would

00:57:43.915 --> 00:57:46.560
be making $200,000 a year.

00:57:46.560 --> 00:57:51.490
Now, most of you guys make
$200,000 a year, I understand.

00:57:51.490 --> 00:57:54.210
It pays well here, and
I was making a joke.

00:57:54.210 --> 00:57:57.561
But my point about
that is that there

00:57:57.561 --> 00:57:59.060
are going to be
people-- when you're

00:57:59.060 --> 00:58:00.990
making that kind of money,
you want those fresh flowers

00:58:00.990 --> 00:58:03.031
every day, and you may be
willing to pay somebody

00:58:03.031 --> 00:58:05.170
to do that and pay them
a living wage to do it.

00:58:05.170 --> 00:58:07.810
So I think the nature of
work is going to shift.

00:58:07.810 --> 00:58:10.485
Those people from 1800 who
look at us today and think

00:58:10.485 --> 00:58:14.980
we're crazy because
we are we're doing

00:58:14.980 --> 00:58:19.290
stuff we don't need to do for
people who don't need it done.

00:58:19.290 --> 00:58:22.150
And that is the way
they would look at it.

00:58:22.150 --> 00:58:25.715
And I can't say for sure,
but I think that pattern

00:58:25.715 --> 00:58:26.980
is likely to continue.

00:58:26.980 --> 00:58:29.210
It's just very hard to
visualize what that's

00:58:29.210 --> 00:58:32.450
going to be like in 80 years.

00:58:32.450 --> 00:58:34.030
AUDIENCE: My question
is when do you

00:58:34.030 --> 00:58:36.150
think AI will get
to the point where

00:58:36.150 --> 00:58:38.801
it can predict the behavior
of other AI actors?

00:58:38.801 --> 00:58:41.050
Because I think that's the
heart of human intelligence

00:58:41.050 --> 00:58:44.290
in the social context,
and we haven't really

00:58:44.290 --> 00:58:47.270
seen much in that task space.

00:58:47.270 --> 00:58:50.340
JERRY KAPLAN: Wow, again,
a very complicated-- I

00:58:50.340 --> 00:58:53.950
could go on for a
long time on this.

00:58:53.950 --> 00:58:56.030
This has come up already
in things like the flash

00:58:56.030 --> 00:58:59.370
crash of 2010, which I cover
in my book, which you're all

00:58:59.370 --> 00:59:00.792
encouraged to take a look at.

00:59:03.680 --> 00:59:06.010
It's a real problem,
because people are stealthy

00:59:06.010 --> 00:59:07.676
developing these
systems, and it creates

00:59:07.676 --> 00:59:09.160
what's called systemic risk.

00:59:09.160 --> 00:59:13.650
Because these machines are like
gods in terms of the damage

00:59:13.650 --> 00:59:16.060
they can inflict
in milliseconds.

00:59:16.060 --> 00:59:19.440
And so it shut down
the US power grid.

00:59:19.440 --> 00:59:22.490
You can bet that China today,
or I shouldn't pick out

00:59:22.490 --> 00:59:26.820
China-- powerful players
today have the ability

00:59:26.820 --> 00:59:29.720
to completely decimate our
economy for a fair period

00:59:29.720 --> 00:59:33.150
of time almost instantly--
almost like a press

00:59:33.150 --> 00:59:33.970
of a button.

00:59:33.970 --> 00:59:36.992
And so these are
difficult issues,

00:59:36.992 --> 00:59:38.200
because you get two of those.

00:59:38.200 --> 00:59:41.360
You ever seen the old movie
"Colossus-- The Forbin

00:59:41.360 --> 00:59:43.410
Project"?

00:59:43.410 --> 00:59:43.971
Anybody?

00:59:43.971 --> 00:59:46.220
So one guy, two guys will
know what I'm talking about.

00:59:46.220 --> 00:59:47.180
It's about just that.

00:59:47.180 --> 00:59:48.920
They created an
intelligent system.

00:59:48.920 --> 00:59:51.527
The Russians-- this was like
1960 when they made the film,

00:59:51.527 --> 00:59:52.807
it was great-- also did that.

00:59:52.807 --> 00:59:55.140
And the two of them figured
there had to be another one,

00:59:55.140 --> 00:59:57.110
and they got together, and
they took over the world.

00:59:57.110 --> 00:59:58.485
It's actually a
pretty cool film.

00:59:58.485 --> 01:00:01.040
It's not as stupid as it sounds.

01:00:01.040 --> 01:00:03.480
So it's a real issue.

01:00:03.480 --> 01:00:05.150
These side effects,
the unintended

01:00:05.150 --> 01:00:07.170
consequences-- the
kinds of technology

01:00:07.170 --> 01:00:09.802
we're developing-- that's
another hour-long talk.

01:00:09.802 --> 01:00:11.260
It doesn't mean we
shouldn't do it.

01:00:11.260 --> 01:00:12.676
It means we need
to be aware of it

01:00:12.676 --> 01:00:15.340
to figure out how to control
it in reasonable ways.

01:00:15.340 --> 01:00:16.870
So I apologize [INAUDIBLE].

01:00:16.870 --> 01:00:20.990
If anybody wants to stay and
hear stories about early Google

01:00:20.990 --> 01:00:25.020
like I would-- well here he is.

01:00:25.020 --> 01:00:27.340
I won't do it on camera
because I don't want anybody

01:00:27.340 --> 01:00:28.217
to record it.

01:00:28.217 --> 01:00:28.800
But thank you.

01:00:28.800 --> 01:00:33.150
Thank you so much for
listening to my crazy rants.

01:00:33.150 --> 01:00:35.900
[APPLAUSE]

