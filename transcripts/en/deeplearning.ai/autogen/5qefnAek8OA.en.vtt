WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.600
 
bachelor on processes or data one me me

00:00:02.600 --> 00:00:02.610
bachelor on processes or data one me me
 

00:00:02.610 --> 00:00:04.999
bachelor on processes or data one me me
batch at the time but at times you might

00:00:04.999 --> 00:00:05.009
batch at the time but at times you might
 

00:00:05.009 --> 00:00:07.010
batch at the time but at times you might
need to process the examples one at a

00:00:07.010 --> 00:00:07.020
need to process the examples one at a
 

00:00:07.020 --> 00:00:08.960
need to process the examples one at a
time let's see how you can adapt your

00:00:08.960 --> 00:00:08.970
time let's see how you can adapt your
 

00:00:08.970 --> 00:00:11.270
time let's see how you can adapt your
network to do that recall that during

00:00:11.270 --> 00:00:11.280
network to do that recall that during
 

00:00:11.280 --> 00:00:13.879
network to do that recall that during
training here are the equations you just

00:00:13.879 --> 00:00:13.889
training here are the equations you just
 

00:00:13.889 --> 00:00:16.129
training here are the equations you just
implement national within a single

00:00:16.129 --> 00:00:16.139
implement national within a single
 

00:00:16.139 --> 00:00:19.640
implement national within a single
mini-batches sum over that mini batch of

00:00:19.640 --> 00:00:19.650
mini-batches sum over that mini batch of
 

00:00:19.650 --> 00:00:22.730
mini-batches sum over that mini batch of
the zi values to compute the mean so

00:00:22.730 --> 00:00:22.740
the zi values to compute the mean so
 

00:00:22.740 --> 00:00:24.320
the zi values to compute the mean so
here you're just summing over the

00:00:24.320 --> 00:00:24.330
here you're just summing over the
 

00:00:24.330 --> 00:00:26.480
here you're just summing over the
examples in one mini batch I'm using M

00:00:26.480 --> 00:00:26.490
examples in one mini batch I'm using M
 

00:00:26.490 --> 00:00:28.580
examples in one mini batch I'm using M
to denote the number of examples in the

00:00:28.580 --> 00:00:28.590
to denote the number of examples in the
 

00:00:28.590 --> 00:00:30.620
to denote the number of examples in the
mini batch not not in the whole training

00:00:30.620 --> 00:00:30.630
mini batch not not in the whole training
 

00:00:30.630 --> 00:00:33.410
mini batch not not in the whole training
set then you compute the variance and

00:00:33.410 --> 00:00:33.420
set then you compute the variance and
 

00:00:33.420 --> 00:00:36.680
set then you compute the variance and
then you compute the norm by scaling by

00:00:36.680 --> 00:00:36.690
then you compute the norm by scaling by
 

00:00:36.690 --> 00:00:38.840
then you compute the norm by scaling by
the mean and standard deviation what

00:00:38.840 --> 00:00:38.850
the mean and standard deviation what
 

00:00:38.850 --> 00:00:40.490
the mean and standard deviation what
that's on added for numerical stability

00:00:40.490 --> 00:00:40.500
that's on added for numerical stability
 

00:00:40.500 --> 00:00:44.270
that's on added for numerical stability
and then V tilde is taking Z norm and

00:00:44.270 --> 00:00:44.280
and then V tilde is taking Z norm and
 

00:00:44.280 --> 00:00:48.229
and then V tilde is taking Z norm and
rescaling by gamma and beta so notice

00:00:48.229 --> 00:00:48.239
rescaling by gamma and beta so notice
 

00:00:48.239 --> 00:00:52.189
rescaling by gamma and beta so notice
that mu and Sigma squared which you need

00:00:52.189 --> 00:00:52.199
that mu and Sigma squared which you need
 

00:00:52.199 --> 00:00:54.619
that mu and Sigma squared which you need
for this scaling calculation are

00:00:54.619 --> 00:00:54.629
for this scaling calculation are
 

00:00:54.629 --> 00:00:57.920
for this scaling calculation are
computed on the entire mini value but at

00:00:57.920 --> 00:00:57.930
computed on the entire mini value but at
 

00:00:57.930 --> 00:01:00.250
computed on the entire mini value but at
times you might not have a mini batch of

00:01:00.250 --> 00:01:00.260
times you might not have a mini batch of
 

00:01:00.260 --> 00:01:03.740
times you might not have a mini batch of
64 128 alternative Pacific examples to

00:01:03.740 --> 00:01:03.750
64 128 alternative Pacific examples to
 

00:01:03.750 --> 00:01:05.750
64 128 alternative Pacific examples to
process at the same time so you need

00:01:05.750 --> 00:01:05.760
process at the same time so you need
 

00:01:05.760 --> 00:01:07.640
process at the same time so you need
some different way of coming up with mu

00:01:07.640 --> 00:01:07.650
some different way of coming up with mu
 

00:01:07.650 --> 00:01:09.679
some different way of coming up with mu
and Sigma squared and if just one

00:01:09.679 --> 00:01:09.689
and Sigma squared and if just one
 

00:01:09.689 --> 00:01:11.570
and Sigma squared and if just one
example taking the mean and variance of

00:01:11.570 --> 00:01:11.580
example taking the mean and variance of
 

00:01:11.580 --> 00:01:15.200
example taking the mean and variance of
that one example doesn't make sense so

00:01:15.200 --> 00:01:15.210
that one example doesn't make sense so
 

00:01:15.210 --> 00:01:18.469
that one example doesn't make sense so
what's actually done in order to apply

00:01:18.469 --> 00:01:18.479
what's actually done in order to apply
 

00:01:18.479 --> 00:01:21.200
what's actually done in order to apply
your neural network at test time is to

00:01:21.200 --> 00:01:21.210
your neural network at test time is to
 

00:01:21.210 --> 00:01:23.179
your neural network at test time is to
come up with some separate estimate of

00:01:23.179 --> 00:01:23.189
come up with some separate estimate of
 

00:01:23.189 --> 00:01:26.570
come up with some separate estimate of
mu and Sigma squared and in typical

00:01:26.570 --> 00:01:26.580
mu and Sigma squared and in typical
 

00:01:26.580 --> 00:01:28.609
mu and Sigma squared and in typical
implementations of national what you do

00:01:28.609 --> 00:01:28.619
implementations of national what you do
 

00:01:28.619 --> 00:01:33.620
implementations of national what you do
is estimate this using a exponentially

00:01:33.620 --> 00:01:33.630
is estimate this using a exponentially
 

00:01:33.630 --> 00:01:36.830
is estimate this using a exponentially
weighted average where the average is

00:01:36.830 --> 00:01:36.840
weighted average where the average is
 

00:01:36.840 --> 00:01:44.120
weighted average where the average is
across the mini batches so to be very

00:01:44.120 --> 00:01:44.130
across the mini batches so to be very
 

00:01:44.130 --> 00:01:45.679
across the mini batches so to be very
concrete here's what I mean

00:01:45.679 --> 00:01:45.689
concrete here's what I mean
 

00:01:45.689 --> 00:01:48.530
concrete here's what I mean
let's pick some layer L and let's say

00:01:48.530 --> 00:01:48.540
let's pick some layer L and let's say
 

00:01:48.540 --> 00:01:52.399
let's pick some layer L and let's say
you're going through mini batches x1 x2

00:01:52.399 --> 00:01:52.409
you're going through mini batches x1 x2
 

00:01:52.409 --> 00:01:54.679
you're going through mini batches x1 x2
together with the corresponding values

00:01:54.679 --> 00:01:54.689
together with the corresponding values
 

00:01:54.689 --> 00:01:56.929
together with the corresponding values
of Y and so on

00:01:56.929 --> 00:01:56.939
of Y and so on
 

00:01:56.939 --> 00:02:01.850
of Y and so on
so when training on x1 for that layer L

00:02:01.850 --> 00:02:01.860
so when training on x1 for that layer L
 

00:02:01.860 --> 00:02:06.649
so when training on x1 for that layer L
you get some new L and in fact I'm going

00:02:06.649 --> 00:02:06.659
you get some new L and in fact I'm going
 

00:02:06.659 --> 00:02:10.070
you get some new L and in fact I'm going
to write this as new for the first mini

00:02:10.070 --> 00:02:10.080
to write this as new for the first mini
 

00:02:10.080 --> 00:02:11.860
to write this as new for the first mini
batch and that lane

00:02:11.860 --> 00:02:11.870
batch and that lane
 

00:02:11.870 --> 00:02:13.780
batch and that lane
and then when you train on the second

00:02:13.780 --> 00:02:13.790
and then when you train on the second
 

00:02:13.790 --> 00:02:16.360
and then when you train on the second
mini batch for that layer and that mean

00:02:16.360 --> 00:02:16.370
mini batch for that layer and that mean
 

00:02:16.370 --> 00:02:17.890
mini batch for that layer and that mean
about you and there was some second

00:02:17.890 --> 00:02:17.900
about you and there was some second
 

00:02:17.900 --> 00:02:20.500
about you and there was some second
value of you and then for the third mini

00:02:20.500 --> 00:02:20.510
value of you and then for the third mini
 

00:02:20.510 --> 00:02:24.070
value of you and then for the third mini
batch in this hidden layer you end up

00:02:24.070 --> 00:02:24.080
batch in this hidden layer you end up
 

00:02:24.080 --> 00:02:30.250
batch in this hidden layer you end up
with some third value for MU so just as

00:02:30.250 --> 00:02:30.260
with some third value for MU so just as
 

00:02:30.260 --> 00:02:32.620
with some third value for MU so just as
means for how to use the exponentially

00:02:32.620 --> 00:02:32.630
means for how to use the exponentially
 

00:02:32.630 --> 00:02:35.550
means for how to use the exponentially
weighted average to compute the mean of

00:02:35.550 --> 00:02:35.560
weighted average to compute the mean of
 

00:02:35.560 --> 00:02:39.370
weighted average to compute the mean of
theta1 theta2 theta3 when you are trying

00:02:39.370 --> 00:02:39.380
theta1 theta2 theta3 when you are trying
 

00:02:39.380 --> 00:02:41.770
theta1 theta2 theta3 when you are trying
to compute a exponentially weighted

00:02:41.770 --> 00:02:41.780
to compute a exponentially weighted
 

00:02:41.780 --> 00:02:44.259
to compute a exponentially weighted
average of the current temperature you

00:02:44.259 --> 00:02:44.269
average of the current temperature you
 

00:02:44.269 --> 00:02:47.110
average of the current temperature you
will do that to keep track of so what's

00:02:47.110 --> 00:02:47.120
will do that to keep track of so what's
 

00:02:47.120 --> 00:02:49.630
will do that to keep track of so what's
the latest average value of this mean

00:02:49.630 --> 00:02:49.640
the latest average value of this mean
 

00:02:49.640 --> 00:02:52.870
the latest average value of this mean
vector your seat so that exponentially

00:02:52.870 --> 00:02:52.880
vector your seat so that exponentially
 

00:02:52.880 --> 00:02:55.210
vector your seat so that exponentially
weighted average becomes your estimate

00:02:55.210 --> 00:02:55.220
weighted average becomes your estimate
 

00:02:55.220 --> 00:02:58.449
weighted average becomes your estimate
for what the mean of the B's is for that

00:02:58.449 --> 00:02:58.459
for what the mean of the B's is for that
 

00:02:58.459 --> 00:03:01.539
for what the mean of the B's is for that
hidden layer and similarly you'd use an

00:03:01.539 --> 00:03:01.549
hidden layer and similarly you'd use an
 

00:03:01.549 --> 00:03:03.100
hidden layer and similarly you'd use an
exponentially weighted average to keep

00:03:03.100 --> 00:03:03.110
exponentially weighted average to keep
 

00:03:03.110 --> 00:03:06.789
exponentially weighted average to keep
track of these values of Sigma squared

00:03:06.789 --> 00:03:06.799
track of these values of Sigma squared
 

00:03:06.799 --> 00:03:09.130
track of these values of Sigma squared
that you see on the first mini batch in

00:03:09.130 --> 00:03:09.140
that you see on the first mini batch in
 

00:03:09.140 --> 00:03:11.770
that you see on the first mini batch in
that layer Sigma squared then you see on

00:03:11.770 --> 00:03:11.780
that layer Sigma squared then you see on
 

00:03:11.780 --> 00:03:13.960
that layer Sigma squared then you see on
a second mini batch and so on so you

00:03:13.960 --> 00:03:13.970
a second mini batch and so on so you
 

00:03:13.970 --> 00:03:17.860
a second mini batch and so on so you
keep a running average of the MU and the

00:03:17.860 --> 00:03:17.870
keep a running average of the MU and the
 

00:03:17.870 --> 00:03:20.140
keep a running average of the MU and the
Sigma square that you're seeing for each

00:03:20.140 --> 00:03:20.150
Sigma square that you're seeing for each
 

00:03:20.150 --> 00:03:22.420
Sigma square that you're seeing for each
layer as you train the neural network

00:03:22.420 --> 00:03:22.430
layer as you train the neural network
 

00:03:22.430 --> 00:03:24.640
layer as you train the neural network
across different mini batches then

00:03:24.640 --> 00:03:24.650
across different mini batches then
 

00:03:24.650 --> 00:03:28.030
across different mini batches then
finally at test time what you do is in

00:03:28.030 --> 00:03:28.040
finally at test time what you do is in
 

00:03:28.040 --> 00:03:31.030
finally at test time what you do is in
place of this equation you would just

00:03:31.030 --> 00:03:31.040
place of this equation you would just
 

00:03:31.040 --> 00:03:34.599
place of this equation you would just
compute Z norm using whatever value you

00:03:34.599 --> 00:03:34.609
compute Z norm using whatever value you
 

00:03:34.609 --> 00:03:37.259
compute Z norm using whatever value you
see you have and using your

00:03:37.259 --> 00:03:37.269
see you have and using your
 

00:03:37.269 --> 00:03:40.539
see you have and using your
exponentially weighted average of the MU

00:03:40.539 --> 00:03:40.549
exponentially weighted average of the MU
 

00:03:40.549 --> 00:03:41.949
exponentially weighted average of the MU
and Sigma squared whatever was the

00:03:41.949 --> 00:03:41.959
and Sigma squared whatever was the
 

00:03:41.959 --> 00:03:44.710
and Sigma squared whatever was the
latest value you have to do the scaling

00:03:44.710 --> 00:03:44.720
latest value you have to do the scaling
 

00:03:44.720 --> 00:03:48.430
latest value you have to do the scaling
here and then you would compute each

00:03:48.430 --> 00:03:48.440
here and then you would compute each
 

00:03:48.440 --> 00:03:51.759
here and then you would compute each
other on your one test example using

00:03:51.759 --> 00:03:51.769
other on your one test example using
 

00:03:51.769 --> 00:03:53.620
other on your one test example using
that Z norm that we just computed on the

00:03:53.620 --> 00:03:53.630
that Z norm that we just computed on the
 

00:03:53.630 --> 00:03:57.490
that Z norm that we just computed on the
left and using the beta and gamma

00:03:57.490 --> 00:03:57.500
left and using the beta and gamma
 

00:03:57.500 --> 00:04:00.190
left and using the beta and gamma
parameters then you'll you have learned

00:04:00.190 --> 00:04:00.200
parameters then you'll you have learned
 

00:04:00.200 --> 00:04:01.449
parameters then you'll you have learned
during your neural network training

00:04:01.449 --> 00:04:01.459
during your neural network training
 

00:04:01.459 --> 00:04:04.330
during your neural network training
process so the takeaway from this is

00:04:04.330 --> 00:04:04.340
process so the takeaway from this is
 

00:04:04.340 --> 00:04:07.360
process so the takeaway from this is
that during training time mu and Sigma

00:04:07.360 --> 00:04:07.370
that during training time mu and Sigma
 

00:04:07.370 --> 00:04:09.400
that during training time mu and Sigma
squared are computed on an entire mini

00:04:09.400 --> 00:04:09.410
squared are computed on an entire mini
 

00:04:09.410 --> 00:04:12.129
squared are computed on an entire mini
batch of you know say 64 and June 28 or

00:04:12.129 --> 00:04:12.139
batch of you know say 64 and June 28 or
 

00:04:12.139 --> 00:04:15.220
batch of you know say 64 and June 28 or
some number of examples but at test time

00:04:15.220 --> 00:04:15.230
some number of examples but at test time
 

00:04:15.230 --> 00:04:16.690
some number of examples but at test time
you might need to process a single

00:04:16.690 --> 00:04:16.700
you might need to process a single
 

00:04:16.700 --> 00:04:19.509
you might need to process a single
example at a time so the way to do that

00:04:19.509 --> 00:04:19.519
example at a time so the way to do that
 

00:04:19.519 --> 00:04:21.759
example at a time so the way to do that
is to estimate mu and Sigma squared from

00:04:21.759 --> 00:04:21.769
is to estimate mu and Sigma squared from
 

00:04:21.769 --> 00:04:22.430
is to estimate mu and Sigma squared from
your training

00:04:22.430 --> 00:04:22.440
your training
 

00:04:22.440 --> 00:04:25.490
your training
and there many ways to do that you

00:04:25.490 --> 00:04:25.500
and there many ways to do that you
 

00:04:25.500 --> 00:04:27.410
and there many ways to do that you
couldn't clearly run your whole training

00:04:27.410 --> 00:04:27.420
couldn't clearly run your whole training
 

00:04:27.420 --> 00:04:29.720
couldn't clearly run your whole training
set through your final network to get mu

00:04:29.720 --> 00:04:29.730
set through your final network to get mu
 

00:04:29.730 --> 00:04:31.760
set through your final network to get mu
and Sigma squared but in practice what

00:04:31.760 --> 00:04:31.770
and Sigma squared but in practice what
 

00:04:31.770 --> 00:04:33.500
and Sigma squared but in practice what
people usually do is implement an

00:04:33.500 --> 00:04:33.510
people usually do is implement an
 

00:04:33.510 --> 00:04:35.480
people usually do is implement an
exponentially weighted average where you

00:04:35.480 --> 00:04:35.490
exponentially weighted average where you
 

00:04:35.490 --> 00:04:37.730
exponentially weighted average where you
just keep track of the new and Sigma

00:04:37.730 --> 00:04:37.740
just keep track of the new and Sigma
 

00:04:37.740 --> 00:04:39.140
just keep track of the new and Sigma
squared values you've seen during

00:04:39.140 --> 00:04:39.150
squared values you've seen during
 

00:04:39.150 --> 00:04:41.210
squared values you've seen during
training and use an exponentially

00:04:41.210 --> 00:04:41.220
training and use an exponentially
 

00:04:41.220 --> 00:04:43.040
training and use an exponentially
weighted average also sometimes called a

00:04:43.040 --> 00:04:43.050
weighted average also sometimes called a
 

00:04:43.050 --> 00:04:45.170
weighted average also sometimes called a
running average to just get a rough

00:04:45.170 --> 00:04:45.180
running average to just get a rough
 

00:04:45.180 --> 00:04:47.030
running average to just get a rough
estimate of mu and Sigma squared and

00:04:47.030 --> 00:04:47.040
estimate of mu and Sigma squared and
 

00:04:47.040 --> 00:04:48.950
estimate of mu and Sigma squared and
then you use those values of MU and

00:04:48.950 --> 00:04:48.960
then you use those values of MU and
 

00:04:48.960 --> 00:04:51.830
then you use those values of MU and
Sigma square that test time to do the

00:04:51.830 --> 00:04:51.840
Sigma square that test time to do the
 

00:04:51.840 --> 00:04:54.530
Sigma square that test time to do the
scaling you need of the hidden unit

00:04:54.530 --> 00:04:54.540
scaling you need of the hidden unit
 

00:04:54.540 --> 00:04:57.650
scaling you need of the hidden unit
values z in practice this process is

00:04:57.650 --> 00:04:57.660
values z in practice this process is
 

00:04:57.660 --> 00:05:00.560
values z in practice this process is
pretty robust to the exact way you use

00:05:00.560 --> 00:05:00.570
pretty robust to the exact way you use
 

00:05:00.570 --> 00:05:03.560
pretty robust to the exact way you use
to estimate mu and Sigma squared so I

00:05:03.560 --> 00:05:03.570
to estimate mu and Sigma squared so I
 

00:05:03.570 --> 00:05:05.750
to estimate mu and Sigma squared so I
wouldn't worry too much about exactly

00:05:05.750 --> 00:05:05.760
wouldn't worry too much about exactly
 

00:05:05.760 --> 00:05:08.270
wouldn't worry too much about exactly
how you do this and if you're using a

00:05:08.270 --> 00:05:08.280
how you do this and if you're using a
 

00:05:08.280 --> 00:05:10.130
how you do this and if you're using a
deep learning framework they'll usually

00:05:10.130 --> 00:05:10.140
deep learning framework they'll usually
 

00:05:10.140 --> 00:05:14.600
deep learning framework they'll usually
have some default way to estimate mu and

00:05:14.600 --> 00:05:14.610
have some default way to estimate mu and
 

00:05:14.610 --> 00:05:16.580
have some default way to estimate mu and
Sigma squared tension work reasonably

00:05:16.580 --> 00:05:16.590
Sigma squared tension work reasonably
 

00:05:16.590 --> 00:05:18.940
Sigma squared tension work reasonably
well as well but in practice any

00:05:18.940 --> 00:05:18.950
well as well but in practice any
 

00:05:18.950 --> 00:05:21.560
well as well but in practice any
reasonable way to estimate the mean and

00:05:21.560 --> 00:05:21.570
reasonable way to estimate the mean and
 

00:05:21.570 --> 00:05:26.270
reasonable way to estimate the mean and
variance of your hidden unit values of Z

00:05:26.270 --> 00:05:26.280
variance of your hidden unit values of Z
 

00:05:26.280 --> 00:05:29.840
variance of your hidden unit values of Z
should work fine and test so that's it -

00:05:29.840 --> 00:05:29.850
should work fine and test so that's it -
 

00:05:29.850 --> 00:05:32.000
should work fine and test so that's it -
dome and using it I think you'll be able

00:05:32.000 --> 00:05:32.010
dome and using it I think you'll be able
 

00:05:32.010 --> 00:05:34.430
dome and using it I think you'll be able
to train much deeper networks and get

00:05:34.430 --> 00:05:34.440
to train much deeper networks and get
 

00:05:34.440 --> 00:05:36.320
to train much deeper networks and get
your learning album to run much more

00:05:36.320 --> 00:05:36.330
your learning album to run much more
 

00:05:36.330 --> 00:05:38.630
your learning album to run much more
quickly before we wrap up for this video

00:05:38.630 --> 00:05:38.640
quickly before we wrap up for this video
 

00:05:38.640 --> 00:05:40.430
quickly before we wrap up for this video
I want to share you some thoughts on

00:05:40.430 --> 00:05:40.440
I want to share you some thoughts on
 

00:05:40.440 --> 00:05:43.190
I want to share you some thoughts on
deep learning frameworks as well let's

00:05:43.190 --> 00:05:43.200
deep learning frameworks as well let's
 

00:05:43.200 --> 00:05:44.690
deep learning frameworks as well let's
start to talk about that in the next

00:05:44.690 --> 00:05:44.700
start to talk about that in the next
 

00:05:44.700 --> 00:05:47.060
start to talk about that in the next
video

