WEBVTT
Kind: captions
Language: en

00:00:00.089 --> 00:00:04.200
I’m working on the ethics and governance
of artificial intelligence project, here at

00:00:04.200 --> 00:00:05.770
Berkman Klein.

00:00:05.770 --> 00:00:13.160
There are a lot of questions as to how exactly
incorporating this new technology into different

00:00:13.160 --> 00:00:17.510
social environments is really going to affect
people, and I think one of the most important

00:00:17.510 --> 00:00:22.350
things is getting people’s perspectives
who are actually going to be impacted.

00:00:22.350 --> 00:00:28.010
So, I’m looking forward to participating
in some early educational initiatives and

00:00:28.010 --> 00:00:33.680
some discussions that we can post online in
blog posts and things, to help people feel

00:00:33.680 --> 00:00:37.330
like they’re more familiar with this subject
and more comfortable, because it can be really

00:00:37.330 --> 00:00:38.330
intimidating.

00:00:38.330 --> 00:00:44.190
Right now, this technology or early versions
of machine learning and artificial intelligence

00:00:44.190 --> 00:00:49.620
applications are being used in institutions
ranging from the judicial system, to financial

00:00:49.620 --> 00:00:53.300
institutions, and they’re really going to
impact everyone.

00:00:53.300 --> 00:00:59.489
I think it’s important for people to talk
about how they’re being implemented and

00:00:59.489 --> 00:01:05.319
what the consequences of that are for them,
and that we should have an open discussion,

00:01:05.319 --> 00:01:11.409
and that people can’t do that if they’re
unfamiliar with the technology or why it’s

00:01:11.409 --> 00:01:13.290
being employed.

00:01:13.290 --> 00:01:18.159
I think that everyone needs to have at least
a basic familiarity with these things because

00:01:18.159 --> 00:01:20.950
in ten years there’s not going to be an
institution that doesn’t use it in some

00:01:20.950 --> 00:01:22.289
way.

00:01:22.289 --> 00:01:27.780
I grew up in a pretty low income community
that didn’t have a lot of access to these

00:01:27.780 --> 00:01:34.609
technologies initially, and so I was very
new to even using a computer when I got into

00:01:34.609 --> 00:01:35.609
college.

00:01:35.609 --> 00:01:40.670
It’s something that was hard for me initially,
but that I started really getting interested

00:01:40.670 --> 00:01:45.039
in, partially because I’m a huge sci-fi
fan now, and so I think that sci-fi and fiction

00:01:45.039 --> 00:01:53.929
really opens up your eyes to both the opportunities
and the potential costs of using different

00:01:53.929 --> 00:01:55.499
advanced technologies.

00:01:55.499 --> 00:01:59.939
I wanted to be part of the conversation about
how we would actually approach a future where

00:01:59.939 --> 00:02:02.950
these things were possible and to make sure
that we would use them in a way that would

00:02:02.950 --> 00:02:09.189
benefit us and not this scarier, more dystopian
views of what could happen.

00:02:09.189 --> 00:02:13.620
Software, so scalable, that we can offer more
resources and more information to so many

00:02:13.620 --> 00:02:15.709
more people at a lower cost.

00:02:15.709 --> 00:02:19.150
We’re also at a time where we have so much
more information than we’ve ever had in

00:02:19.150 --> 00:02:24.040
history, so things like machine learning and
artificial intelligence can really help to

00:02:24.040 --> 00:02:30.069
open up the answers that we can get from all
of that data and maybe some very non-intuitive

00:02:30.069 --> 00:02:35.120
answers that people just have not been able
to find themselves.

00:02:35.120 --> 00:02:40.799
I think that the thing that scares me most
is that artificial intelligence software is

00:02:40.799 --> 00:02:48.170
going to be employed in institutions and around
populations that don’t understand both ends

00:02:48.170 --> 00:02:53.129
of the things it has to offer, but also its
limitations.

00:02:53.129 --> 00:02:58.819
It will just be taken as objective fact or
a scientific opinion that you can’t question,

00:02:58.819 --> 00:03:05.220
when it’s important to realize that this
is something that is crafted by humans, that

00:03:05.220 --> 00:03:09.819
can be fallible, that can be employed in different
ways and have different outcomes.

00:03:09.819 --> 00:03:14.379
I think my biggest fear is that we won’t
question it and that these things will just

00:03:14.379 --> 00:03:20.640
be able to be deployed without having any
kind of public dialogue or pushback if it

00:03:20.640 --> 00:03:21.650
has negative consequences.

