WEBVTT
Kind: captions
Language: en

00:00:00.470 --> 00:00:05.129
The last step here is how
do we learn a model for R?

00:00:05.129 --> 00:00:09.639
Now remember,
when we execute an action a in state s,

00:00:09.639 --> 00:00:13.758
we get an immediate reward, little r.

00:00:13.758 --> 00:00:16.500
So again, big R, s,

00:00:16.500 --> 00:00:21.850
a as are expected reward if we're
in state s and we execute action a.

00:00:21.850 --> 00:00:27.660
Little r is our immediate reward when
we experience this in the real world.

00:00:27.660 --> 00:00:32.650
So big R is our model, little r is
what we get in an experience tuple.

00:00:32.650 --> 00:00:39.070
So we want to update this model every
time we have a real experience.

00:00:39.070 --> 00:00:45.520
And it's a simple equation, very much
like the q table update equation.

00:00:45.520 --> 00:00:50.887
What we have is one minus alpha
where alpha is our learning rate and

00:00:50.887 --> 00:00:55.780
again that can typically be
something like zero point two.

00:00:55.780 --> 00:01:01.810
Anyways we multiply that times
our current value for R and

00:01:01.810 --> 00:01:08.130
then we add in of course our new
estimate of what that value ought to be.

00:01:08.130 --> 00:01:10.760
And we just use r for the new estimate.

00:01:10.760 --> 00:01:14.310
So it's alpha times little r,
which is our immediate reward, or

00:01:14.310 --> 00:01:17.230
our new best estimate of
what the value should be,

00:01:17.230 --> 00:01:23.000
plus what the value was
before times 1 minus alpha.

00:01:23.000 --> 00:01:26.220
So we're waiting presumably.

00:01:26.220 --> 00:01:32.050
Our old value more than our new value,
so we converge more slowly.

00:01:32.050 --> 00:01:32.720
But that's it.

00:01:32.720 --> 00:01:35.770
That's a simple way
to build a model of R

00:01:35.770 --> 00:01:38.370
from observations of interactions
with the real world.

