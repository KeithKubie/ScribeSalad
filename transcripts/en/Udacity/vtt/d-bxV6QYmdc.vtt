WEBVTT
Kind: captions
Language: en

00:00:00.610 --> 00:00:02.920
Let's talk a little bit more about a specific example

00:00:02.920 --> 00:00:06.430
of supervised learning. We might want to ask the question, can we

00:00:06.430 --> 00:00:08.990
write an equation that takes as input a bunch of

00:00:08.990 --> 00:00:14.050
information. For example, height, weight, birth year or position about baseball

00:00:14.050 --> 00:00:17.540
players and predicts their lifetime number of home runs. To

00:00:17.540 --> 00:00:19.670
do this, we want to write an algorithm that does the

00:00:19.670 --> 00:00:22.340
following. Takes in the data points for which we have all

00:00:22.340 --> 00:00:26.260
these input attributes, and the player's lifetime number of home runs.

00:00:26.260 --> 00:00:29.100
Then builds the most accurate equation to predict lifetime number of

00:00:29.100 --> 00:00:32.960
home runs, using these input variables. We can then use this equation

00:00:32.960 --> 00:00:35.680
to predict the lifetime number of home runs for players for

00:00:35.680 --> 00:00:38.620
whom we did not initially have number of home runs. But we

00:00:38.620 --> 00:00:41.300
do have all the other data. So, their height and weight

00:00:41.300 --> 00:00:44.450
and birth year and position maybe. One way that we might be

00:00:44.450 --> 00:00:47.650
able to solve this problem is using linear regression and one

00:00:47.650 --> 00:00:49.580
basic implementation on machine learning is

00:00:49.580 --> 00:00:51.570
to perform linear regression by using

00:00:51.570 --> 00:00:55.225
gradient descent. What is this? How does this work? let's learn about

00:00:55.225 --> 00:00:57.205
gradient descent by working through an

00:00:57.205 --> 00:00:59.550
example that utilizes our baseball data set.

