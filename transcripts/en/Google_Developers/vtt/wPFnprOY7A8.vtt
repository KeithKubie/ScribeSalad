WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.940
[MUSIC PLAYING]

00:00:02.940 --> 00:00:04.440
BARATUNDE THURSTON:
I/O highlights--

00:00:04.440 --> 00:00:10.740
It's been a really big momentous
and mostly beautiful gathering.

00:00:10.740 --> 00:00:14.640
I saw one of the creators of
machine learning and neural

00:00:14.640 --> 00:00:16.379
networks this morning.

00:00:16.379 --> 00:00:20.280
I declare with 100%
certainty that machines

00:00:20.280 --> 00:00:21.960
will gain consciousness.

00:00:21.960 --> 00:00:23.880
And none of you
reacted, and that

00:00:23.880 --> 00:00:25.130
made me more alarmed, right?

00:00:25.130 --> 00:00:27.250
[LAUGHTER] What's
going on there?

00:00:27.250 --> 00:00:29.430
I saw a space pioneer.

00:00:29.430 --> 00:00:33.630
Dr. Mae Jemison reminds us all
that we live under one sky,

00:00:33.630 --> 00:00:37.500
under one roof, and remind us
to connect with each other.

00:00:37.500 --> 00:00:41.610
I saw fruit used last
night to make music.

00:00:41.610 --> 00:00:44.280
And it made it hard for me to
eat that Apple during lunch

00:00:44.280 --> 00:00:47.250
today because every time
I took a bite I heard

00:00:47.250 --> 00:00:49.200
it screaming apple, apple.

00:00:49.200 --> 00:00:51.150
Like fruit shouldn't talk.

00:00:51.150 --> 00:00:52.740
And if it does talk,
it shouldn't talk

00:00:52.740 --> 00:00:55.200
in a self-aware kind of way.

00:00:55.200 --> 00:00:56.070
It's just creepy.

00:00:56.070 --> 00:00:57.280
[LAUGHTER]

00:00:57.280 --> 00:01:00.770
There's been a lot of amazing
news announcements out of I/O

00:01:00.770 --> 00:01:01.270
this year.

00:01:01.270 --> 00:01:04.840
We have Android queue
updates, Flutter for the web,

00:01:04.840 --> 00:01:09.460
something called a Nest Hub
Max, and of course, the biggest

00:01:09.460 --> 00:01:12.730
technical innovation,
Dark Theme, which

00:01:12.730 --> 00:01:15.610
seems to really excite the
super geeks in the room.

00:01:15.610 --> 00:01:18.310
I got you to glance
away from your screens,

00:01:18.310 --> 00:01:20.984
so I should say Dark Theme.

00:01:20.984 --> 00:01:23.980
[APPLAUSE]

00:01:23.980 --> 00:01:25.030
You're so easy.

00:01:25.030 --> 00:01:27.200
You're so easy.

00:01:27.200 --> 00:01:31.000
Now my presentation will run
at 30% higher energy efficiency

00:01:31.000 --> 00:01:32.590
and push fewer pixels.

00:01:32.590 --> 00:01:34.990
So I was paying attention.

00:01:34.990 --> 00:01:35.953
I really was.

00:01:35.953 --> 00:01:37.120
I learned a bunch of things.

00:01:37.120 --> 00:01:40.450
I learned that in
NBU is Google's way

00:01:40.450 --> 00:01:43.480
of saying next billion users.

00:01:43.480 --> 00:01:46.980
That's how this company thinks
about growth, a humble brag.

00:01:46.980 --> 00:01:48.790
The Pixel 3a is out.

00:01:48.790 --> 00:01:53.610
And we got
privacy-ish, which I'm

00:01:53.610 --> 00:01:56.020
going to come back to later.

00:01:56.020 --> 00:01:58.140
So the first
conclusion I came to

00:01:58.140 --> 00:02:01.623
after many of the sessions and
overhearing people in lines

00:02:01.623 --> 00:02:03.040
was that machine
learning is going

00:02:03.040 --> 00:02:05.790
to take all the human jobs.

00:02:05.790 --> 00:02:09.150
I saw very specific ones where
machine learning algorithms

00:02:09.150 --> 00:02:12.300
outperform or nearly
outperform humans

00:02:12.300 --> 00:02:15.240
in things like translation
and transcription,

00:02:15.240 --> 00:02:18.090
in driving, in
picking things up,

00:02:18.090 --> 00:02:22.530
and most interestingly of
all, in developing machine

00:02:22.530 --> 00:02:24.570
learning algorithms.

00:02:24.570 --> 00:02:28.170
I want to show you a slide that
I saw yesterday, which really--

00:02:28.170 --> 00:02:29.550
it looks kind of banal.

00:02:29.550 --> 00:02:31.290
But this is a terrifying image.

00:02:31.290 --> 00:02:35.280
The red curve is the performance
of a machine learning

00:02:35.280 --> 00:02:38.160
algorithm built by a
machine learning algorithm.

00:02:38.160 --> 00:02:41.880
The black line is what
puny humans are capable of.

00:02:41.880 --> 00:02:43.980
And here's why that concerns me.

00:02:43.980 --> 00:02:47.070
I have assumed a
level of self-interest

00:02:47.070 --> 00:02:49.170
and self-preservation
on the part

00:02:49.170 --> 00:02:51.060
of the people designing
and developing

00:02:51.060 --> 00:02:52.560
this new world we live in.

00:02:52.560 --> 00:02:54.730
So taking away certain
jobs and occupations,

00:02:54.730 --> 00:02:56.730
cool, but not my job.

00:02:56.730 --> 00:02:58.890
And yet, the people
with the super power

00:02:58.890 --> 00:03:00.570
are like you know
what would be cool?

00:03:00.570 --> 00:03:03.510
To create a superpower capable
of creating superpowers

00:03:03.510 --> 00:03:05.640
greater than my superpower.

00:03:05.640 --> 00:03:06.940
Stop.

00:03:06.940 --> 00:03:08.220
You don't have to do it.

00:03:08.220 --> 00:03:10.770
Like literally, no
one's making you do it.

00:03:10.770 --> 00:03:12.540
Do something else.

00:03:12.540 --> 00:03:13.530
Just a thought.

00:03:13.530 --> 00:03:14.250
Just a thought.

00:03:14.250 --> 00:03:18.750
That curves should be a warning
line we've gone too far.

00:03:18.750 --> 00:03:23.230
But at the same time, I had
another opposite conclusion,

00:03:23.230 --> 00:03:27.520
which is that machine learning
will bring us closer together.

00:03:27.520 --> 00:03:30.300
I spent a lot of time
in the experiments dome.

00:03:30.300 --> 00:03:34.630
It's one of several
domes here on planet I/O.

00:03:34.630 --> 00:03:38.250
And I witnessed and participated
in some really beautiful uses

00:03:38.250 --> 00:03:39.510
of technology.

00:03:39.510 --> 00:03:43.140
I saw a conversation between
two people, one of whom

00:03:43.140 --> 00:03:44.130
was hearing impaired.

00:03:44.130 --> 00:03:47.880
Yet that conversation itself
was not impaired at all.

00:03:47.880 --> 00:03:50.820
I took part in a dance lesson
from one of the greatest

00:03:50.820 --> 00:03:53.820
dancers we as a species
have ever produced,

00:03:53.820 --> 00:03:57.030
Bill T Jones, thanks to
the PoseNet algorithm.

00:03:57.030 --> 00:04:01.260
And I'm as good as he is now
as submitted by exhibit a here.

00:04:01.260 --> 00:04:04.980
I would literally empty a
wallet or max out a credit card

00:04:04.980 --> 00:04:08.490
if I could get personal dance
lessons from Beyonce, right?

00:04:08.490 --> 00:04:09.900
That would be really successful.

00:04:09.900 --> 00:04:12.720
I met this developer
Ezra from Egypt.

00:04:12.720 --> 00:04:16.380
And we made random beautiful
art together using something

00:04:16.380 --> 00:04:18.990
called Rotavo.

00:04:18.990 --> 00:04:21.300
Now I'm assuming this isn't
some kind of weapon system.

00:04:21.300 --> 00:04:24.300
It's just used to make
cute art, but we made this.

00:04:24.300 --> 00:04:26.310
And as I stared
into it, I recognize

00:04:26.310 --> 00:04:29.520
it as a metaphor for the
arc of human existence

00:04:29.520 --> 00:04:33.260
and that we peak sometime
around Candy Crush

00:04:33.260 --> 00:04:35.880
and are now in the climate
catastrophe decline,

00:04:35.880 --> 00:04:37.860
unless we intervene.

00:04:37.860 --> 00:04:41.490
I also recognize that one of the
greatest innovations of I/O '19

00:04:41.490 --> 00:04:45.180
was to get hundreds and I
mean hundreds of developers

00:04:45.180 --> 00:04:46.130
to go outside.

00:04:46.130 --> 00:04:48.540
[LAUGHTER]

00:04:48.540 --> 00:04:50.010
Just go outside.

00:04:50.010 --> 00:04:52.530
[APPLAUSE]

00:04:52.530 --> 00:04:56.220
And then be forced to
exercise because everything

00:04:56.220 --> 00:04:58.980
is 1.3 miles apart
from everything else.

00:04:58.980 --> 00:05:01.170
I've walked 30 miles
in the past two days.

00:05:01.170 --> 00:05:03.330
I have calluses now.

00:05:03.330 --> 00:05:07.500
This was also a test of in what
situations and circumstances

00:05:07.500 --> 00:05:09.825
will people stare
at their screens.

00:05:09.825 --> 00:05:12.010
It turns out
everywhere, everywhere,

00:05:12.010 --> 00:05:14.878
including the
digital detox zone.

00:05:14.878 --> 00:05:16.535
[LAUGHTER]

00:05:16.535 --> 00:05:17.910
People are like,
I'm here to tox.

00:05:17.910 --> 00:05:19.327
I don't know what
you're here for.

00:05:19.327 --> 00:05:21.450
I'm a toxer.

00:05:21.450 --> 00:05:24.510
We're just sitting on the
street with nothing else to do.

00:05:24.510 --> 00:05:26.290
Thank you, [INAUDIBLE],,
for the photo.

00:05:26.290 --> 00:05:30.210
And maybe, I was one of the few
to notice the sort of police

00:05:30.210 --> 00:05:33.230
overwatch situation.

00:05:33.230 --> 00:05:36.510
And as I took advantage of
digital imaging technology

00:05:36.510 --> 00:05:38.730
and zoomed in, I
realized there's not

00:05:38.730 --> 00:05:40.810
a human in this car.

00:05:40.810 --> 00:05:43.120
It's already here.

00:05:43.120 --> 00:05:45.570
Skynet starts with
an empty police

00:05:45.570 --> 00:05:49.060
car overlooking a bunch of super
powerful technologies creating

00:05:49.060 --> 00:05:50.220
the future.

00:05:50.220 --> 00:05:51.390
It's too late to run.

00:05:51.390 --> 00:05:54.720
Enjoy captivity.

00:05:54.720 --> 00:05:58.260
The greatest invention,
though, other than dark theme

00:05:58.260 --> 00:05:59.610
are these porta potties.

00:05:59.610 --> 00:06:03.090
Can we give it up
for the ultra lav?

00:06:03.090 --> 00:06:04.020
Come on now.

00:06:04.020 --> 00:06:05.340
This is amazing.

00:06:05.340 --> 00:06:06.660
[APPLAUSE]

00:06:06.660 --> 00:06:09.790
Actually bigger and better than
my New York City apartment.

00:06:09.790 --> 00:06:10.290
So--

00:06:10.290 --> 00:06:12.000
[LAUGHTER]

00:06:12.000 --> 00:06:15.690
--that's most other reflection
on I/O. Again, my name,

00:06:15.690 --> 00:06:19.350
my full name is
Baratunde Rafiq Thurston.

00:06:19.350 --> 00:06:23.400
And I am hereby way of
this woman, Arnita Lorraine

00:06:23.400 --> 00:06:26.100
Thurston, who raised
me and my older sister

00:06:26.100 --> 00:06:29.730
Belinda on her own, who
was a multitude of people--

00:06:29.730 --> 00:06:33.840
a survivor of sexual
assault, a paralegal,

00:06:33.840 --> 00:06:37.620
a computer programmer, an
activist, and environmentalist.

00:06:37.620 --> 00:06:40.170
And it was her working
as a systems analyst

00:06:40.170 --> 00:06:42.900
for the federal government
in the early 1980s that

00:06:42.900 --> 00:06:45.660
brought the first
computer into our house.

00:06:45.660 --> 00:06:49.560
A computer that helped introduce
me to the early internet.

00:06:49.560 --> 00:06:52.130
Clap if you remember
this internet?

00:06:52.130 --> 00:06:52.700
Yeah!

00:06:52.700 --> 00:06:53.990
[APPLAUSE]

00:06:53.990 --> 00:06:55.500
Text, baby.

00:06:55.500 --> 00:06:56.310
No GIFs.

00:06:56.310 --> 00:06:57.060
No cats.

00:06:57.060 --> 00:06:58.190
No ads.

00:06:58.190 --> 00:07:00.660
It was a simpler
and beautiful time.

00:07:00.660 --> 00:07:04.920
And I witnessed and was a result
of the power of technology

00:07:04.920 --> 00:07:06.690
to upgrade my life.

00:07:06.690 --> 00:07:08.640
My mother's financial
life got upgraded,

00:07:08.640 --> 00:07:12.120
which meant my education,
quality of food got better,

00:07:12.120 --> 00:07:15.780
and my sense of creativity of
literally what was possible

00:07:15.780 --> 00:07:18.910
was powered by technology
from an early age.

00:07:18.910 --> 00:07:22.110
This is a image of an article
I wrote in high school

00:07:22.110 --> 00:07:27.390
in Washington DC in 1993,
headlined Upper School Joins

00:07:27.390 --> 00:07:30.450
Internet because we
got a full time always

00:07:30.450 --> 00:07:35.430
on t1 connection, which
changed everything.

00:07:35.430 --> 00:07:38.440
And among the observations
I had in this article,

00:07:38.440 --> 00:07:39.600
this one stood out to me.

00:07:39.600 --> 00:07:43.560
I wrote students have used
the computer room because we

00:07:43.560 --> 00:07:47.790
segregated them back then
to write papers, solve math

00:07:47.790 --> 00:07:50.160
problems, conduct
science experiments,

00:07:50.160 --> 00:07:53.550
and connect to local
libraries and universities.

00:07:53.550 --> 00:07:57.960
All legal, make-your-momma
proud type activities.

00:07:57.960 --> 00:08:01.620
What I did not anticipate
was Fortnite, cyber attacks

00:08:01.620 --> 00:08:03.540
on our election
systems, selfies,

00:08:03.540 --> 00:08:07.310
or dark theme of course.

00:08:07.310 --> 00:08:09.890
The path that I
have walked has been

00:08:09.890 --> 00:08:13.370
enabled by what technology
has brought to us.

00:08:13.370 --> 00:08:18.050
I registered my
domain name in 1998.

00:08:18.050 --> 00:08:21.530
Most of the jobs I've
ever had were directly

00:08:21.530 --> 00:08:24.360
influenced by or
working with technology,

00:08:24.360 --> 00:08:28.160
including working for America's
finest news source, The Onion,

00:08:28.160 --> 00:08:28.740
where I was--

00:08:28.740 --> 00:08:29.240
[APPLAUSE]

00:08:29.240 --> 00:08:32.280
Yes, for the good
kind of fake news.

00:08:32.280 --> 00:08:35.510
Yes, I was director of
digital there for years.

00:08:35.510 --> 00:08:37.760
Right after that, I
helped start a business,

00:08:37.760 --> 00:08:40.940
which merged
technology with humor

00:08:40.940 --> 00:08:42.860
and tried to bring
a level of humanity

00:08:42.860 --> 00:08:44.390
to some of these cool tools.

00:08:44.390 --> 00:08:47.360
And our signature action
was a series of hackathons

00:08:47.360 --> 00:08:49.860
that we called Comedy
Hack Day and straight--

00:08:49.860 --> 00:08:50.360
[APPLAUSE]

00:08:50.360 --> 00:08:50.860
Yes!

00:08:50.860 --> 00:08:52.650
We got the-- yes!

00:08:52.650 --> 00:08:54.200
That's what I'm talking about.

00:08:54.200 --> 00:08:56.110
We got one person
who knew about it.

00:08:56.110 --> 00:08:57.530
I'm very excited
about the human--

00:08:57.530 --> 00:08:58.702
it's a human connection.

00:08:58.702 --> 00:08:59.410
It was beautiful.

00:08:59.410 --> 00:09:00.410
[LAUGHTER]

00:09:00.410 --> 00:09:01.490
Should do more of that.

00:09:01.490 --> 00:09:02.930
I like you.

00:09:02.930 --> 00:09:05.030
So we would bring
developers and designers

00:09:05.030 --> 00:09:08.420
and comedians together
to imagine and then build

00:09:08.420 --> 00:09:11.000
working prototypes of jokes.

00:09:11.000 --> 00:09:15.590
So someone once made a digital
assistant, voice enabled,

00:09:15.590 --> 00:09:17.540
but only in the
body of a Furby that

00:09:17.540 --> 00:09:19.040
had to be attached
to your shoulder.

00:09:19.040 --> 00:09:20.720
So you'd walk around with that.

00:09:20.720 --> 00:09:22.460
There was a team that
made an app, which

00:09:22.460 --> 00:09:26.187
was for a long time in the
app stores, called EquiTable.

00:09:26.187 --> 00:09:28.520
And that would allow you to
split your bill with friends

00:09:28.520 --> 00:09:29.850
after a night out.

00:09:29.850 --> 00:09:31.910
But it wouldn't split
the bill equally.

00:09:31.910 --> 00:09:34.070
It would split the
bill equitably,

00:09:34.070 --> 00:09:37.430
taking into account the
pay gap based on gender

00:09:37.430 --> 00:09:39.560
and race in the United States.

00:09:39.560 --> 00:09:41.840
So different people would
pay different amounts.

00:09:41.840 --> 00:09:45.710
It was reparations one meal at
a time is what they call that.

00:09:45.710 --> 00:09:48.330
[APPLAUSE]

00:09:48.330 --> 00:09:50.180
And then I helped
in the first year

00:09:50.180 --> 00:09:52.430
of "The Daily Show
under Trevor Noah"

00:09:52.430 --> 00:09:56.240
to reimagine how that now
global institution deploys

00:09:56.240 --> 00:09:59.750
technology for more than
publishing video feeds online

00:09:59.750 --> 00:10:02.230
but to get way more
interactive with its community

00:10:02.230 --> 00:10:05.320
and more creative with the
possibilities for jokes.

00:10:05.320 --> 00:10:08.370
Now technology can be very
personal for all of us.

00:10:08.370 --> 00:10:11.440
And it has been for me through
a little company up the road

00:10:11.440 --> 00:10:13.220
called 23andMe.

00:10:13.220 --> 00:10:15.680
As I mentioned, I have
an older sister, Belinda.

00:10:15.680 --> 00:10:17.660
She's nine years
ahead of me in life

00:10:17.660 --> 00:10:19.520
in more ways than just age.

00:10:19.520 --> 00:10:22.130
She is also-- we have
different fathers.

00:10:22.130 --> 00:10:24.170
And after our
mother passed away,

00:10:24.170 --> 00:10:28.370
we had even more questions that
a living human couldn't answer.

00:10:28.370 --> 00:10:31.790
So we went to the great
database of 23andMe.

00:10:31.790 --> 00:10:35.090
And I got incredible
bragging rights

00:10:35.090 --> 00:10:38.420
thanks to the comparison
capabilities within 23andMe.

00:10:38.420 --> 00:10:40.600
I got to be able
to find out that I

00:10:40.600 --> 00:10:42.780
am less Neanderthal
than my older sister.

00:10:42.780 --> 00:10:44.120
[LAUGHTER]

00:10:44.120 --> 00:10:45.650
Clap if you're a
younger sibling,

00:10:45.650 --> 00:10:46.730
if your younger sibling.

00:10:46.730 --> 00:10:47.510
[APPLAUSE]

00:10:47.510 --> 00:10:49.502
We're the good ones.

00:10:49.502 --> 00:10:50.210
I'm just kidding.

00:10:50.210 --> 00:10:51.260
Everyone's good.

00:10:51.260 --> 00:10:53.780
But it is really powerful to
be able to say to your older

00:10:53.780 --> 00:10:57.060
sibling, no, you're the
Neanderthal because science,

00:10:57.060 --> 00:10:57.560
right?

00:10:57.560 --> 00:11:00.410
Like you have backup.

00:11:00.410 --> 00:11:04.700
But big sis had a genetic
clap back I did not foresee.

00:11:04.700 --> 00:11:06.050
She said that's cool, baby bro.

00:11:06.050 --> 00:11:08.740
But you're also
way whiter than me.

00:11:08.740 --> 00:11:11.780
And I was like no,
not that there's

00:11:11.780 --> 00:11:14.450
anything wrong with being white
in a room full of white people.

00:11:14.450 --> 00:11:15.860
But no!

00:11:15.860 --> 00:11:20.490
Like I've got a lot invested
in the blackness thing.

00:11:20.490 --> 00:11:21.860
And this is way late.

00:11:21.860 --> 00:11:23.310
And how do I not know this?

00:11:23.310 --> 00:11:24.570
And what does that even mean?

00:11:24.570 --> 00:11:27.380
And it's extra awkward
because I wrote a book called

00:11:27.380 --> 00:11:28.610
"How To Be Black."

00:11:28.610 --> 00:11:29.450
Like that's not--

00:11:29.450 --> 00:11:30.420
[LAUGHTER]

00:11:30.420 --> 00:11:32.030
It's kind of
undermines my brand.

00:11:32.030 --> 00:11:33.072
You know what I'm saying?

00:11:33.072 --> 00:11:34.730
Like this is not good.

00:11:34.730 --> 00:11:38.240
I didn't write a book on
how to be 81.6% black.

00:11:38.240 --> 00:11:40.820
That was a totally different
book, not a bestseller.

00:11:40.820 --> 00:11:41.825
Very different author.

00:11:47.840 --> 00:11:51.120
My life is mostly great,
mostly incredible,

00:11:51.120 --> 00:11:54.320
because I've been filled with
great and incredible people,

00:11:54.320 --> 00:11:56.400
and that includes
the end of last year

00:11:56.400 --> 00:11:59.600
my then girlfriend
proposed to me.

00:11:59.600 --> 00:12:01.340
And I had the good
sense to say yes.

00:12:01.340 --> 00:12:02.870
So we're getting married.

00:12:02.870 --> 00:12:03.500
I'm engaged.

00:12:03.500 --> 00:12:04.460
It's very exciting.

00:12:04.460 --> 00:12:06.760
[APPLAUSE]

00:12:06.760 --> 00:12:10.310
And I'm going to take this
opportunity to share with

00:12:10.310 --> 00:12:11.470
y'all in the world.

00:12:11.470 --> 00:12:12.470
We're starting a family.

00:12:12.470 --> 00:12:15.690
It's very exciting
time to do that.

00:12:15.690 --> 00:12:19.280
It's an Amazon family,
and what we did

00:12:19.280 --> 00:12:22.200
is it means we merged
our Amazon account.

00:12:22.200 --> 00:12:25.430
So she can use my prime
but with her credit card.

00:12:25.430 --> 00:12:27.470
It's great invasion.

00:12:27.470 --> 00:12:28.705
And it really changes--

00:12:28.705 --> 00:12:30.080
It's also-- it's
just really nice

00:12:30.080 --> 00:12:33.630
to have our union recognized
by Chairman Bezos,

00:12:33.630 --> 00:12:35.330
you know, head of
one of the largest

00:12:35.330 --> 00:12:38.100
nongovernmental military
operations in the world.

00:12:38.100 --> 00:12:44.240
It's really cool to just be
seen, truly seen like that.

00:12:44.240 --> 00:12:47.438
There was a time when to create
a child, which we are not

00:12:47.438 --> 00:12:48.980
actually doing, but
to create a child

00:12:48.980 --> 00:12:50.420
involves a lot of physical work.

00:12:50.420 --> 00:12:53.390
But you all know disruption,
machine learning.

00:12:53.390 --> 00:12:56.810
And Amazon makes it
much easier to just add

00:12:56.810 --> 00:12:59.420
a child to your family.

00:12:59.420 --> 00:13:02.180
They've got a slick
UX, and then you

00:13:02.180 --> 00:13:04.880
have a bunch of parameters
you can fill out

00:13:04.880 --> 00:13:10.080
to kind of optimize your child
for your particular situation.

00:13:10.080 --> 00:13:12.950
Now, I'm not a fan of the
binary gendery choice,

00:13:12.950 --> 00:13:15.780
but that's a simple
update on the back end.

00:13:15.780 --> 00:13:18.260
So what I did was I
created a little girl

00:13:18.260 --> 00:13:22.310
after my favorite Roller Derby
character's name, Beyonslay.

00:13:22.310 --> 00:13:25.010
And I just skipped those early
years where you don't sleep.

00:13:25.010 --> 00:13:28.250
So she's almost 11 now.

00:13:28.250 --> 00:13:30.150
And she's a cute little unicorn.

00:13:30.150 --> 00:13:32.150
But once you've got a
child, this is the beauty.

00:13:32.150 --> 00:13:35.450
Amazon gives you options, as
you can add another child.

00:13:35.450 --> 00:13:38.320
Or if you're not happy, you
can edit your existing child.

00:13:38.320 --> 00:13:39.220
[LAUGHTER]

00:13:39.220 --> 00:13:41.750
I mean, clap if you ever
wanted to edit a child, right?

00:13:41.750 --> 00:13:43.260
Like that's an amazing--

00:13:43.260 --> 00:13:46.010
we need that IRL, you know?

00:13:46.010 --> 00:13:48.283
So you choose to edit the child.

00:13:48.283 --> 00:13:49.700
And when you click
through, you're

00:13:49.700 --> 00:13:51.620
given even more
dramatic options.

00:13:51.620 --> 00:13:54.140
Do you save the child?

00:13:54.140 --> 00:13:56.447
Or do you remove
the child, which

00:13:56.447 --> 00:13:58.280
I'm pretty sure is a
human rights violation.

00:13:58.280 --> 00:13:59.790
But it's cloud based.

00:13:59.790 --> 00:14:00.290
It's Amazon.

00:14:00.290 --> 00:14:00.800
Who knows.

00:14:00.800 --> 00:14:03.840
It's beyond the jurisdiction
of any government.

00:14:03.840 --> 00:14:07.970
It brings me to Waze,
one of my favorite tools

00:14:07.970 --> 00:14:10.100
for always letting me
know where I am but

00:14:10.100 --> 00:14:12.240
denying me the knowledge
of how to get anywhere.

00:14:12.240 --> 00:14:13.940
It's an amazing
combination where

00:14:13.940 --> 00:14:16.310
I know less the more I know.

00:14:16.310 --> 00:14:18.650
And one thing I took
a look at somewhat

00:14:18.650 --> 00:14:22.700
recently is just the
level of chaos and mayhem

00:14:22.700 --> 00:14:24.470
that happens inside of Waze.

00:14:24.470 --> 00:14:26.988
I'm not sure how many of
you all payed attention

00:14:26.988 --> 00:14:28.280
to what happens inside of Waze.

00:14:28.280 --> 00:14:32.120
But this is an average route
through a busy Los Angeles

00:14:32.120 --> 00:14:33.080
interchange.

00:14:33.080 --> 00:14:36.560
But when you zoom
in, you see violence.

00:14:36.560 --> 00:14:40.310
Like there is a blood
thirsty, sword wielding

00:14:40.310 --> 00:14:43.040
nut chasing Royalty.

00:14:43.040 --> 00:14:46.220
There's someone laughing in the
background protected, though,

00:14:46.220 --> 00:14:48.230
probably why they're laughing.

00:14:48.230 --> 00:14:50.640
And the police aren't
doing anything about it.

00:14:50.640 --> 00:14:52.010
[LAUGHTER]

00:14:52.010 --> 00:14:54.970
Oddly reminiscent sadly of
what happens in the real world.

00:14:54.970 --> 00:14:56.930
So maybe Waze is actually real.

00:14:56.930 --> 00:15:01.052
When I got here to I/O, I wanted
to see what I could learn,

00:15:01.052 --> 00:15:02.510
see what some of
the sessions were.

00:15:02.510 --> 00:15:05.162
So I loaded the
app onto my phone.

00:15:05.162 --> 00:15:06.620
I found out that
the app is created

00:15:06.620 --> 00:15:10.800
by some unverified developer
who wants access to a view

00:15:10.800 --> 00:15:12.770
and edit all the
events on my calendar.

00:15:12.770 --> 00:15:14.600
And once I just
click through blindly

00:15:14.600 --> 00:15:17.060
through those permissions,
I started searching.

00:15:17.060 --> 00:15:20.450
And I searched for machine
and I found so many options

00:15:20.450 --> 00:15:22.310
and things to learn about.

00:15:22.310 --> 00:15:23.840
I searched for ethics.

00:15:23.840 --> 00:15:28.040
I found one, which
is better than zero.

00:15:28.040 --> 00:15:29.015
Progress.

00:15:29.015 --> 00:15:31.640
So Y'all should definitely find
the stream on that one and like

00:15:31.640 --> 00:15:34.010
embody everything
in that session.

00:15:34.010 --> 00:15:36.860
And then I searched
for avoid apocalypse.

00:15:36.860 --> 00:15:41.090
And I didn't find
anything in the session.

00:15:41.090 --> 00:15:44.850
It brings me to a moment
in my life three years ago.

00:15:44.850 --> 00:15:47.330
I was invited to South
by Southwest interactive.

00:15:47.330 --> 00:15:50.270
I've been going for almost
a decade at that point.

00:15:50.270 --> 00:15:52.130
And this particular
year, they brought me in

00:15:52.130 --> 00:15:55.370
to be inducted into
the Hall of Fame,

00:15:55.370 --> 00:16:00.410
joining people like Danah Boyd
and Ze Frank and Kara Swisher.

00:16:00.410 --> 00:16:02.630
And in my brief
acceptance speech,

00:16:02.630 --> 00:16:04.670
I made some remarks,
which I think

00:16:04.670 --> 00:16:07.400
are still relevant to the
world we're living in today.

00:16:07.400 --> 00:16:10.700
I want to share a brief
moment of that with you.

00:16:10.700 --> 00:16:12.560
And you can read along.

00:16:12.560 --> 00:16:14.810
I said to them and I
say to you still now

00:16:14.810 --> 00:16:17.300
the algorithms are coming.

00:16:17.300 --> 00:16:20.750
And we know they aren't
pure or objective.

00:16:20.750 --> 00:16:23.450
Like journalists, they're
embedded with the values

00:16:23.450 --> 00:16:24.140
of their makers.

00:16:24.140 --> 00:16:27.530
They reflect the
society around them.

00:16:27.530 --> 00:16:29.210
But this renovation
is all about making

00:16:29.210 --> 00:16:30.252
the world a better place.

00:16:30.252 --> 00:16:33.140
And the algorithms and
code that claim to do so

00:16:33.140 --> 00:16:36.260
derived from this
very imperfect world,

00:16:36.260 --> 00:16:38.810
sick with racism and sexism,
and crippling poverty.

00:16:38.810 --> 00:16:43.610
Then isn't it possible that they
might make the world a worse

00:16:43.610 --> 00:16:44.660
place?

00:16:44.660 --> 00:16:48.560
Could we end up with virtual
reality racism or machine

00:16:48.560 --> 00:16:51.030
learned sexism?

00:16:51.030 --> 00:16:54.900
And today, could poverty
be policed by drones

00:16:54.900 --> 00:16:56.820
and an internet of crap?

00:16:56.820 --> 00:17:00.280
Possibly, maybe
the answer is yes.

00:17:00.280 --> 00:17:04.349
So today we live in some version
of the world, the world that

00:17:04.349 --> 00:17:08.550
gives us headlines about social
media disrupting our democracy,

00:17:08.550 --> 00:17:12.450
about yet another data leak from
yet another organization taking

00:17:12.450 --> 00:17:15.180
advantage of access to
information about us,

00:17:15.180 --> 00:17:18.960
about our legislators selling us
out to ISPs who are selling us

00:17:18.960 --> 00:17:21.119
out to advertisers,
who are selling us out

00:17:21.119 --> 00:17:24.720
to brands who just want our
money, about self-driving cars

00:17:24.720 --> 00:17:31.610
who literally can't see black
people, that's terrible,

00:17:31.610 --> 00:17:34.850
about machine learning applied
to resumes, which conveniently

00:17:34.850 --> 00:17:38.720
sort out all the women because
that's also what history has

00:17:38.720 --> 00:17:42.920
done, so we've just scaled that
into our present and future,

00:17:42.920 --> 00:17:45.020
about an automated
response system

00:17:45.020 --> 00:17:49.610
that was goaded into mentioning
the n-word publicly on Twitter,

00:17:49.610 --> 00:17:53.300
and about the cloud based
apology company known

00:17:53.300 --> 00:17:56.150
as Facebook, apologizing
yet again for something

00:17:56.150 --> 00:18:00.330
it deservedly should
apologize for.

00:18:00.330 --> 00:18:02.790
I have been working
over the past year

00:18:02.790 --> 00:18:06.120
to try to integrate my own
thinking around technology.

00:18:06.120 --> 00:18:09.720
I was a big booster in
the '90s and early 2000s.

00:18:09.720 --> 00:18:12.630
I have seen the harms as well.

00:18:12.630 --> 00:18:17.070
And last year, I wrote a bit of
a manifesto I went on a journey

00:18:17.070 --> 00:18:20.070
to try to understand
how all my data existed

00:18:20.070 --> 00:18:23.400
amongst the major platforms,
amongst app developers,

00:18:23.400 --> 00:18:26.310
amongst the very websites
or web browsers that I visit

00:18:26.310 --> 00:18:27.540
and that I use.

00:18:27.540 --> 00:18:30.480
And what came out of that
was a set of principles

00:18:30.480 --> 00:18:32.500
that I then open sourced
with a Google Doc

00:18:32.500 --> 00:18:35.160
that others have
contributed to to help

00:18:35.160 --> 00:18:40.060
guide us more conscientiously
into the future.

00:18:40.060 --> 00:18:42.090
And so I'm going to
walk you through several

00:18:42.090 --> 00:18:44.520
of those principles
and hope you take them

00:18:44.520 --> 00:18:47.340
in the spirit of generosity
and embed them into the code

00:18:47.340 --> 00:18:48.900
and into the values
as you go out

00:18:48.900 --> 00:18:51.420
and build this world that
we all want to live in.

00:18:51.420 --> 00:18:56.400
The first is about transparency
and what I call trust scores.

00:18:56.400 --> 00:18:59.280
Now look we all get a
score from a system.

00:18:59.280 --> 00:19:01.620
We get credit scores
from the financial system

00:19:01.620 --> 00:19:04.350
that determine if we can get
a job, if we can get a home,

00:19:04.350 --> 00:19:05.640
if we can get a car.

00:19:05.640 --> 00:19:08.280
And I think it's time to flip
that scoring system around

00:19:08.280 --> 00:19:10.890
to create something like
a trust score, which

00:19:10.890 --> 00:19:14.850
rates the organizations that we
are in relationship with based

00:19:14.850 --> 00:19:19.530
on how they handle us, how
they handle our information,

00:19:19.530 --> 00:19:22.500
based on what's inside
of the technology.

00:19:22.500 --> 00:19:25.980
And we have a good metaphor
from the world of food.

00:19:25.980 --> 00:19:28.470
When I want to know
what's in my food,

00:19:28.470 --> 00:19:31.770
I don't drag a chemistry
set to the grocery store

00:19:31.770 --> 00:19:34.890
and inspect every
item point by point.

00:19:34.890 --> 00:19:36.660
I read the nutrition label.

00:19:36.660 --> 00:19:40.360
I know the content, the
calories, the ratings.

00:19:40.360 --> 00:19:41.530
This is possible.

00:19:41.530 --> 00:19:44.658
And with that knowledge, we
can jump to another world

00:19:44.658 --> 00:19:46.200
and look at the
building traits where

00:19:46.200 --> 00:19:49.770
we get broad ratings of lead
generation for sustainability

00:19:49.770 --> 00:19:51.700
and energy efficiency.

00:19:51.700 --> 00:19:55.050
I shouldn't have to guess about
what's inside the product.

00:19:55.050 --> 00:19:59.070
I certainly shouldn't have
to read 33,000 word legalese

00:19:59.070 --> 00:20:03.570
terms of service to figure out
what's really happening inside.

00:20:03.570 --> 00:20:07.290
All of that should be as
usable as the UX and systems

00:20:07.290 --> 00:20:09.720
that got me onto the
platforms, into the apps,

00:20:09.720 --> 00:20:12.520
into the services
in the first place.

00:20:12.520 --> 00:20:15.300
Now the second principle
is about defaults.

00:20:15.300 --> 00:20:19.290
Changing those defaults
from open to close, from

00:20:19.290 --> 00:20:22.130
opt out to opt in.

00:20:22.130 --> 00:20:24.110
Defaults matter, y'all.

00:20:24.110 --> 00:20:27.020
Most of us don't
change the default.

00:20:27.020 --> 00:20:30.770
As in life, we accept mostly
the world that we walk into.

00:20:30.770 --> 00:20:33.500
We accept the family beliefs,
the religious beliefs,

00:20:33.500 --> 00:20:35.240
and the settings
and the programs

00:20:35.240 --> 00:20:37.340
that we're using as
far as what rights they

00:20:37.340 --> 00:20:39.150
claim to have with our data.

00:20:39.150 --> 00:20:43.130
I think we need to totally
switch to a minimal data

00:20:43.130 --> 00:20:47.480
default, to something more akin
to data conservation rather

00:20:47.480 --> 00:20:50.407
than data extraction
and exploitation.

00:20:50.407 --> 00:20:52.490
There was a great series
of guidelines and a model

00:20:52.490 --> 00:20:56.480
from Mozilla, which they call
their Lean Data Practices.

00:20:56.480 --> 00:21:00.330
I encourage you all to look at
those and apply them and ask,

00:21:00.330 --> 00:21:02.420
do I really need
this information

00:21:02.420 --> 00:21:04.760
about the customer,
the user, the person,

00:21:04.760 --> 00:21:07.745
if you choose to call them
a person every now and then?

00:21:07.745 --> 00:21:11.180
And maybe we should treat
data like other things we want

00:21:11.180 --> 00:21:16.370
to limit-- sugar, Netflix, Trump
tweets, carbon, fossil fuels--

00:21:16.370 --> 00:21:19.880
and see how far we
can go with how little

00:21:19.880 --> 00:21:23.470
of that information about
the user as possible.

00:21:23.470 --> 00:21:26.740
That brings me to the third
point about data ownership

00:21:26.740 --> 00:21:28.960
and data portability.

00:21:28.960 --> 00:21:32.560
Now I think of this in a
couple of different layers.

00:21:32.560 --> 00:21:36.280
I think about the
data that I generate

00:21:36.280 --> 00:21:39.730
or the data that is
derived from my actions,

00:21:39.730 --> 00:21:43.330
about the data of us
and the data about us,

00:21:43.330 --> 00:21:45.650
about the content
and the metadata.

00:21:45.650 --> 00:21:48.370
So when I take a photo and
upload it, OK, that's mine.

00:21:48.370 --> 00:21:50.240
That's user generated content.

00:21:50.240 --> 00:21:52.930
But when I walk
from here to here

00:21:52.930 --> 00:21:56.020
and create a digital
trail, a little history,

00:21:56.020 --> 00:21:58.320
that is also data of me.

00:21:58.320 --> 00:22:00.550
And I think we need to
start living in a world

00:22:00.550 --> 00:22:03.130
where that is mine, where
that is a part of me,

00:22:03.130 --> 00:22:05.380
where I have a
level of sovereignty

00:22:05.380 --> 00:22:07.060
and self-determination.

00:22:07.060 --> 00:22:10.210
As I do with this
body, so should I

00:22:10.210 --> 00:22:14.260
do with what this body
represents in the virtual world

00:22:14.260 --> 00:22:16.720
because things are going
both ways in business.

00:22:16.720 --> 00:22:19.970
They should go both
ways in rights as well.

00:22:19.970 --> 00:22:23.410
And when we start to think about
ownership of data in a more

00:22:23.410 --> 00:22:25.780
expansive way, it
makes something

00:22:25.780 --> 00:22:28.390
like this a bit more
interesting, these capture

00:22:28.390 --> 00:22:30.430
codes, these little
quizzes and tests

00:22:30.430 --> 00:22:32.763
to prove you're not a robot,
which pretty soon we're all

00:22:32.763 --> 00:22:35.347
going to fail because the robots
are going to design the tests

00:22:35.347 --> 00:22:36.850
and then they're
going to pass it.

00:22:36.850 --> 00:22:41.550
But I've identified so many
hills and stairs and cars

00:22:41.550 --> 00:22:44.020
that I'm essentially
a co-founder of Waymo

00:22:44.020 --> 00:22:47.110
and every other self-driving
technology out there.

00:22:47.110 --> 00:22:51.370
We should all be considered
co owners and partners

00:22:51.370 --> 00:22:54.520
in the products and services
that are being built.

00:22:54.520 --> 00:22:56.960
Machine learning
depends on data.

00:22:56.960 --> 00:23:01.550
Artificial intelligence without
data is artificial stupidity.

00:23:01.550 --> 00:23:03.860
So every user is
also a contributor.

00:23:03.860 --> 00:23:07.090
This is much more of a
cooperative economic model

00:23:07.090 --> 00:23:10.000
than a top down capitalistic
extraction and exploitation

00:23:10.000 --> 00:23:10.640
model.

00:23:10.640 --> 00:23:12.710
Let's have that
framework in mind.

00:23:12.710 --> 00:23:15.370
And I know it's complicated
but so is everything

00:23:15.370 --> 00:23:17.080
else we've ever done.

00:23:17.080 --> 00:23:20.230
And we use technology
for the express purpose

00:23:20.230 --> 00:23:22.090
of dealing with the
complexity of trying

00:23:22.090 --> 00:23:26.350
to abstract it and find
ways to still approach it.

00:23:26.350 --> 00:23:28.300
Number four, I think
we should start

00:23:28.300 --> 00:23:33.940
shifting our terminology around
permission, not just privacy.

00:23:33.940 --> 00:23:38.140
Privacy has this vague yet
hard edged binary field.

00:23:38.140 --> 00:23:40.000
It's private or it's public.

00:23:40.000 --> 00:23:41.620
Privacy or not.

00:23:41.620 --> 00:23:44.050
Permission has layers.

00:23:44.050 --> 00:23:46.690
When I think about permission,
I think about the first Unix

00:23:46.690 --> 00:23:49.990
system that I ever used back at
that high school and the chmod

00:23:49.990 --> 00:23:53.770
command, C-H-M-O-D, changing
the access permissions

00:23:53.770 --> 00:23:57.620
on directories, on files, for
users, groups, and others,

00:23:57.620 --> 00:23:58.780
including are the owner.

00:23:58.780 --> 00:24:01.450
And I think we need to apply
that as well to information

00:24:01.450 --> 00:24:02.020
about us.

00:24:02.020 --> 00:24:06.010
I'm cool entering into a
relationship with a developer

00:24:06.010 --> 00:24:10.240
around this use for this
app for this amount of time.

00:24:10.240 --> 00:24:12.940
That doesn't mean that
seven years from now,

00:24:12.940 --> 00:24:14.530
the business that
created that app got

00:24:14.530 --> 00:24:17.680
acquired by a company I
want nothing to do with,

00:24:17.680 --> 00:24:20.200
and they just automatically
get me, right?

00:24:20.200 --> 00:24:22.150
I married you, not your cousin.

00:24:22.150 --> 00:24:24.190
It doesn't translate that way.

00:24:24.190 --> 00:24:27.520
So giving much more fine
tooth controls around

00:24:27.520 --> 00:24:30.820
what apps and technologies
have permission

00:24:30.820 --> 00:24:35.380
to do with whom for how long,
in what context, including

00:24:35.380 --> 00:24:38.740
location, will bring much more
self-determination control

00:24:38.740 --> 00:24:42.790
and respect to the
underlying relationship.

00:24:42.790 --> 00:24:45.100
And I can't talk about
respect without talking

00:24:45.100 --> 00:24:49.300
about inclusion,
systemic inclusion.

00:24:49.300 --> 00:24:51.940
We all live in this world
of systemic exclusion.

00:24:51.940 --> 00:24:57.530
We're sitting atop centuries
of all kinds of exclusion.

00:24:57.530 --> 00:25:00.398
And so where we are
is a result in part

00:25:00.398 --> 00:25:02.440
of where our ancestors
were, where the people who

00:25:02.440 --> 00:25:04.810
came before us were, of
where the system that

00:25:04.810 --> 00:25:07.290
was designed by people put us.

00:25:07.290 --> 00:25:11.260
And we know too much now to say
it's all equal for everybody.

00:25:11.260 --> 00:25:14.240
We literally have the data
to prove that's not true.

00:25:14.240 --> 00:25:16.180
So we should be using
that information

00:25:16.180 --> 00:25:19.540
to create inclusive systems
across many dimensions.

00:25:19.540 --> 00:25:21.958
And I want to give some
kudos to this organization

00:25:21.958 --> 00:25:23.500
and many others for
the steps they've

00:25:23.500 --> 00:25:26.290
taken toward accessibility,
which have gotten so much

00:25:26.290 --> 00:25:27.970
better than they have been.

00:25:27.970 --> 00:25:30.400
They can still get better still.

00:25:30.400 --> 00:25:33.240
ProjectInclude is
one such effort

00:25:33.240 --> 00:25:35.590
that's focused on the
technology industry.

00:25:35.590 --> 00:25:39.800
They got literal work books that
I think you should be using.

00:25:39.800 --> 00:25:41.920
It's not about charity for me.

00:25:41.920 --> 00:25:44.220
Doing the right thing
is a good thing,

00:25:44.220 --> 00:25:45.560
but it's good for business.

00:25:45.560 --> 00:25:47.170
It's good for human rights.

00:25:47.170 --> 00:25:50.990
It's good for designing
not just technology,

00:25:50.990 --> 00:25:53.470
but the world that we're
all going to be living in.

00:25:53.470 --> 00:25:55.930
We're not operating
in a vertical anymore.

00:25:55.930 --> 00:25:58.490
This stuff is infecting
every area of life.

00:25:58.490 --> 00:26:01.600
And so if people from
every area of life

00:26:01.600 --> 00:26:05.320
are not participating in the
creativity and the construction

00:26:05.320 --> 00:26:08.080
of the rules and the
systems, then they're

00:26:08.080 --> 00:26:10.090
subjects, not citizens.

00:26:10.090 --> 00:26:12.610
And that's not the type of
world that I think any of us

00:26:12.610 --> 00:26:14.290
want to be living in.

00:26:14.290 --> 00:26:18.500
Number six, imagine harder.

00:26:18.500 --> 00:26:20.650
I want everyone
to think actively

00:26:20.650 --> 00:26:24.790
about what the worst thing that
can happen with your technology

00:26:24.790 --> 00:26:25.600
is.

00:26:25.600 --> 00:26:28.040
Don't just think about the
hockey stick and the valuation

00:26:28.040 --> 00:26:31.120
and the user growth and
the delightful personas

00:26:31.120 --> 00:26:35.230
and testimonial stories come to
life when something goes well.

00:26:35.230 --> 00:26:38.020
Think about the
tragedy and the horror

00:26:38.020 --> 00:26:40.130
if something can go wrong.

00:26:40.130 --> 00:26:42.910
And then use this
tool, ethical OS,

00:26:42.910 --> 00:26:44.500
developed in
partnership with Omidyar

00:26:44.500 --> 00:26:46.120
and a bunch of
other organizations,

00:26:46.120 --> 00:26:48.040
to literally come
up with a framework

00:26:48.040 --> 00:26:50.650
to think about
worst case scenarios

00:26:50.650 --> 00:26:53.060
and how to manage against them.

00:26:53.060 --> 00:26:55.390
One of my favorite examples
from their workbook

00:26:55.390 --> 00:26:57.370
says could your product
or business do anything

00:26:57.370 --> 00:26:59.890
your users are unaware of?

00:26:59.890 --> 00:27:02.440
If so, why are you not
sharing that with them?

00:27:02.440 --> 00:27:04.240
Would it pose a risk
to your business

00:27:04.240 --> 00:27:06.970
if it showed up in
tomorrow's news?

00:27:06.970 --> 00:27:09.640
If you'd rather it not be
written about publicly,

00:27:09.640 --> 00:27:11.170
maybe you're ashamed of it.

00:27:11.170 --> 00:27:13.660
Maybe someone in the
organization is ashamed of it,

00:27:13.660 --> 00:27:16.620
and that is not a good
way to operate in,

00:27:16.620 --> 00:27:19.210
again, any relationship.

00:27:19.210 --> 00:27:23.390
Number seven, we've got to
break open the black box.

00:27:23.390 --> 00:27:25.810
So much of what is
happening with technology

00:27:25.810 --> 00:27:29.840
is beyond us, almost literally
beyond comprehension.

00:27:29.840 --> 00:27:31.930
So we've got to start
building in ways

00:27:31.930 --> 00:27:35.890
to understand what seems
to be not understandable.

00:27:35.890 --> 00:27:38.560
There's an analogy in food
inspectors in that industry

00:27:38.560 --> 00:27:40.990
and auditors in the
financial industry and peer

00:27:40.990 --> 00:27:42.190
review and academia.

00:27:42.190 --> 00:27:45.670
We rarely just let
people skate by.

00:27:45.670 --> 00:27:49.180
So please, start building
in even more ways

00:27:49.180 --> 00:27:51.100
to inspect,
interrogate, and measure

00:27:51.100 --> 00:27:54.130
the impact of the
tools we're building.

00:27:54.130 --> 00:27:58.050
When someone commits
driving under the influence

00:27:58.050 --> 00:28:02.800
offense, a DUI or DWI,
we take away the car

00:28:02.800 --> 00:28:05.230
at least for a certain
amount of time.

00:28:05.230 --> 00:28:08.110
I would love a world
where someone abuses

00:28:08.110 --> 00:28:10.810
the data of millions
of users and so

00:28:10.810 --> 00:28:13.810
was not allowed to
continue to have access

00:28:13.810 --> 00:28:16.000
to the data of
millions of users.

00:28:16.000 --> 00:28:18.160
That's the most logical
approach that we take

00:28:18.160 --> 00:28:20.110
in every other area of life.

00:28:20.110 --> 00:28:23.042
Technology is just
another area of life.

00:28:23.042 --> 00:28:25.000
And "The New York Times"
launched an experience

00:28:25.000 --> 00:28:28.540
last week that did
this for advertisers.

00:28:28.540 --> 00:28:31.090
I can't be here and not
talk about advertising.

00:28:31.090 --> 00:28:32.630
We're here because ad money.

00:28:32.630 --> 00:28:35.450
84% of Google's
money is ad money.

00:28:35.450 --> 00:28:39.490
So let's talk a bit more about
how those ads even get made.

00:28:39.490 --> 00:28:41.800
What the "Times" did
with pretty brilliant.

00:28:41.800 --> 00:28:43.600
They bought an ad campaign.

00:28:43.600 --> 00:28:46.150
And the content
of the ads was how

00:28:46.150 --> 00:28:50.350
revealing how those ads were
derived as in what that ad

00:28:50.350 --> 00:28:52.300
buyer knew about
the user when they

00:28:52.300 --> 00:28:53.570
booked the ad in the system.

00:28:53.570 --> 00:28:54.430
Here's one example.

00:28:54.430 --> 00:28:56.570
This ad thinks you're
trying to lose weight

00:28:56.570 --> 00:28:58.810
but still love
bakeries, as derived

00:28:58.810 --> 00:29:02.770
from browsing history probably
and credit card history.

00:29:02.770 --> 00:29:05.710
That level of
opaqueness-- if more of us

00:29:05.710 --> 00:29:08.220
understood that, not just
as the creators of the tech

00:29:08.220 --> 00:29:11.950
but the users of it, we'd want
some more breaks, some more

00:29:11.950 --> 00:29:14.680
visibility, some more
controls to prevent that.

00:29:14.680 --> 00:29:16.420
That's probably a bit too much.

00:29:16.420 --> 00:29:21.210
We can trust, but we need
mechanisms to verify.

00:29:21.210 --> 00:29:23.550
We also need to
upgrade and enforce

00:29:23.550 --> 00:29:25.560
the rules around all of this.

00:29:25.560 --> 00:29:28.950
Again, I return to the idea
of once you identify abuse,

00:29:28.950 --> 00:29:30.940
you stop the underlying
abusive behavior.

00:29:30.940 --> 00:29:32.940
You don't just say, please,
don't do that again,

00:29:32.940 --> 00:29:35.795
and I trust that you will
mind your own business.

00:29:35.795 --> 00:29:39.000
So I'm glad to see the industry
calling for regulation.

00:29:39.000 --> 00:29:43.050
We need independents as
that comes into play.

00:29:43.050 --> 00:29:45.930
Finally, this is the ninth
and final for now principal,

00:29:45.930 --> 00:29:48.780
and it's actually the
more inspiring one for me.

00:29:48.780 --> 00:29:51.570
But I think of this
as encouraging folks

00:29:51.570 --> 00:29:54.570
to think beyond
just consumption.

00:29:54.570 --> 00:29:57.720
A lot of the models
that were designed,

00:29:57.720 --> 00:29:59.550
the business model and
the technical models

00:29:59.550 --> 00:30:01.770
to support them was
about getting people

00:30:01.770 --> 00:30:04.860
to spend time with,
to log on for longer,

00:30:04.860 --> 00:30:09.870
to spend money with, to suck
life out of, to consume.

00:30:09.870 --> 00:30:13.500
And I say let me do what you do.

00:30:13.500 --> 00:30:18.270
I remember the first time I
used the ad buying capabilities

00:30:18.270 --> 00:30:21.480
of one of the platforms
where I was merely a user.

00:30:21.480 --> 00:30:26.040
And I saw my friends in a much
more empowering and interesting

00:30:26.040 --> 00:30:26.580
way.

00:30:26.580 --> 00:30:27.900
I could slice and dice them.

00:30:27.900 --> 00:30:31.140
I could understand them in a
way that I couldn't as a friend.

00:30:31.140 --> 00:30:33.480
I could only see
them that way if I

00:30:33.480 --> 00:30:37.050
chose to see them as an object,
as the target of an advertising

00:30:37.050 --> 00:30:39.780
campaign both enabled
by this platform.

00:30:39.780 --> 00:30:43.380
Let's close that distance
and allow creativity

00:30:43.380 --> 00:30:46.920
on the part of the users, the
contributors in this new world

00:30:46.920 --> 00:30:48.150
that I am imagining.

00:30:48.150 --> 00:30:50.220
Here's a couple of examples
of beautiful things

00:30:50.220 --> 00:30:51.720
that have been done
with technology

00:30:51.720 --> 00:30:53.910
that allow us to see
ourselves a bit more

00:30:53.910 --> 00:30:55.990
honestly and a bit better.

00:30:55.990 --> 00:30:58.800
There's a group called the
Equal Justice Initiative, which

00:30:58.800 --> 00:31:01.440
does great work around
unearthing and revealing

00:31:01.440 --> 00:31:03.570
some of the dark history
of the United States

00:31:03.570 --> 00:31:05.850
with respect to racial
terror lynching.

00:31:05.850 --> 00:31:07.990
They partnered up with
Google, google.org,

00:31:07.990 --> 00:31:11.310
to build Lynching in America
as a virtual experience

00:31:11.310 --> 00:31:15.390
to accompany their museum
in Montgomery, Alabama.

00:31:15.390 --> 00:31:17.970
There's the Center for
Policing Equity, which

00:31:17.970 --> 00:31:20.250
is bringing machine
learning and big data

00:31:20.250 --> 00:31:23.280
analysis to one of the most
painful and intractable

00:31:23.280 --> 00:31:27.510
problems in this country, which
is discriminate use of police

00:31:27.510 --> 00:31:30.580
force where we know that
police use force far

00:31:30.580 --> 00:31:32.940
more disproportionately
against black people

00:31:32.940 --> 00:31:34.140
than against white people.

00:31:34.140 --> 00:31:37.530
Science and math are
helping us understand why,

00:31:37.530 --> 00:31:40.830
and with police, are
reducing that delta.

00:31:40.830 --> 00:31:43.350
In New York City, where
I lived for 12 years,

00:31:43.350 --> 00:31:46.950
JustFix.nyc is putting
power in the hands

00:31:46.950 --> 00:31:49.620
of tenants to argue
against their slumlords,

00:31:49.620 --> 00:31:52.980
especially tenants in low
income and public housing.

00:31:52.980 --> 00:31:55.770
So they can do things like get
repairs in their apartments,

00:31:55.770 --> 00:32:00.090
find out who owns the building,
respond to an eviction notice.

00:32:00.090 --> 00:32:03.150
This tool is using that
power with a more of a sense

00:32:03.150 --> 00:32:05.430
of spider-man rules, right?

00:32:05.430 --> 00:32:08.340
You can think about the
ability of all of this

00:32:08.340 --> 00:32:12.630
to create power to create
wealth and then ask for who?

00:32:12.630 --> 00:32:13.603
Who needs it more?

00:32:13.603 --> 00:32:15.270
Who needs Cloud music
banging right now?

00:32:15.270 --> 00:32:18.090
Clearly we do because
we just got it.

00:32:18.090 --> 00:32:20.320
I timed that perfectly
to have like a Cloud

00:32:20.320 --> 00:32:23.160
under beat to my super
serious moral talk

00:32:23.160 --> 00:32:25.480
at the end of the session.

00:32:25.480 --> 00:32:28.290
But yeah, make sure that
these tools are not merely

00:32:28.290 --> 00:32:32.250
accruing wealth and power to the
already wealthy and powerful.

00:32:32.250 --> 00:32:33.270
That's not a fun world.

00:32:33.270 --> 00:32:36.510
That's a fun world for like six
people, literally six people,

00:32:36.510 --> 00:32:38.457
and the rest of
us suffer in that.

00:32:38.457 --> 00:32:41.040
So I'm going to leave you with
some resources, some things you

00:32:41.040 --> 00:32:42.030
can follow up with.

00:32:42.030 --> 00:32:45.880
All these slides are online,
baratunde.com/googleio.

00:32:45.880 --> 00:32:47.775
They're in a Google slideshow.

00:32:47.775 --> 00:32:49.650
And you can download
them and manipulate them

00:32:49.650 --> 00:32:50.850
to your own degree.

00:32:50.850 --> 00:32:53.760
But I want to talk
through ethicalOS.org.

00:32:53.760 --> 00:32:58.860
Do that-- dotEveryone.co.uk,
The Data Society and Research

00:32:58.860 --> 00:33:00.550
Institute out of
New York City where

00:33:00.550 --> 00:33:03.510
I'm an advisor, the AI
now Institute, which

00:33:03.510 --> 00:33:06.360
asks deep questions and does
independent study of some

00:33:06.360 --> 00:33:09.270
of these systems, Project
Include, which I already

00:33:09.270 --> 00:33:12.710
mentioned, and then a couple
of books, "Winners Take All"

00:33:12.710 --> 00:33:15.810
by Anand Giridharadas,
read that, yo,

00:33:15.810 --> 00:33:19.020
"The Age Of Surveillance
Capitalism," read that first.

00:33:19.020 --> 00:33:21.030
And an inspirational
twist on all

00:33:21.030 --> 00:33:23.200
of this that goes back
much farther than either

00:33:23.200 --> 00:33:26.430
of those two is a book
called "Decolonizing Wealth."

00:33:26.430 --> 00:33:29.400
And it's based on the ideas
of indigenous wisdom brought

00:33:29.400 --> 00:33:32.830
to bear on a system that
we've built which is, again,

00:33:32.830 --> 00:33:35.330
very extractive,
very exploitative,

00:33:35.330 --> 00:33:38.270
but could be much more
contributory, circular,

00:33:38.270 --> 00:33:39.660
and balanced.

00:33:39.660 --> 00:33:43.020
For inspiration, The Verge
has a beautiful podcast series

00:33:43.020 --> 00:33:45.450
called Better World, which
I think of as an inverted

00:33:45.450 --> 00:33:46.610
"Black Mirror."

00:33:46.610 --> 00:33:48.210
It's thinking of
positive futures

00:33:48.210 --> 00:33:50.010
that we can use
with this technology

00:33:50.010 --> 00:33:52.350
and actually giving you
characters and narrative

00:33:52.350 --> 00:33:54.210
to be able to inhabit.

00:33:54.210 --> 00:33:56.645
The biggest book
of all, "Drawdown--

00:33:56.645 --> 00:33:59.700
The Most Comprehensive Plan
Ever To Reverse Climate Change."

00:33:59.700 --> 00:34:03.180
I think if you're not working
on the climate crisis, do that.

00:34:03.180 --> 00:34:04.740
It's like the
biggest problem that

00:34:04.740 --> 00:34:07.350
connects all the other
problems, and there's so much

00:34:07.350 --> 00:34:10.440
being done right now to improve
energy efficiency and use

00:34:10.440 --> 00:34:12.000
machines to find these spots.

00:34:12.000 --> 00:34:15.719
We can do and should
be doing a lot more.

00:34:15.719 --> 00:34:18.510
To me, these are
questions not of tech.

00:34:18.510 --> 00:34:22.900
They're questions of life
and reality and of power.

00:34:22.900 --> 00:34:24.389
And as we're building
these things,

00:34:24.389 --> 00:34:26.130
we're also building
the world we're going

00:34:26.130 --> 00:34:27.770
to live in where we coexist.

00:34:27.770 --> 00:34:31.097
And so ask what's the
world I want to live in?

00:34:31.097 --> 00:34:32.639
What's the world
you want to live in?

00:34:32.639 --> 00:34:34.350
What's the world
we want to live in?

00:34:34.350 --> 00:34:39.520
Let's imagine harder and imagine
better and build that world.

00:34:39.520 --> 00:34:40.690
Thank you very much.

00:34:40.690 --> 00:34:42.230
I'm Baratunde Thurston.

00:34:42.230 --> 00:34:42.900
Wakonda forever.

00:34:42.900 --> 00:34:44.940
[APPLAUSE]

00:34:44.940 --> 00:34:46.970
Enjoy the rest of I/O.
Thank you, livestream.

00:34:46.970 --> 00:34:51.220
[MUSIC PLAYING]

