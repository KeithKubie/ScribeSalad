WEBVTT
Kind: captions
Language: en

00:00:11.340 --> 00:00:17.700
Hi, welcome to Udacity. I have the privilege and honor of interviewing Dr. Ashwin Ram,

00:00:17.700 --> 00:00:23.215
senior manager of AI Science at Amazon Alexa and Dr. Sebastian Thrun,

00:00:23.215 --> 00:00:26.110
president and co-founder of Udacity.

00:00:26.110 --> 00:00:27.865
Can you tell us a little bit about yourselves,

00:00:27.865 --> 00:00:29.110
and how you got into AI,

00:00:29.110 --> 00:00:31.450
and some of the projects you worked on in the past?

00:00:31.450 --> 00:00:36.455
So my first introduction to AI was Doug Hofstadter's book, GÃ¶del, Escher, Bach.

00:00:36.455 --> 00:00:39.340
I came to the US to study engineering in

00:00:39.340 --> 00:00:43.210
grad school and rapidly discovered that AI was my passion.

00:00:43.210 --> 00:00:45.980
I also found my wife here, which is nice.

00:00:45.980 --> 00:00:47.315
I had an interesting experience,

00:00:47.315 --> 00:00:51.300
I did a PhD in AI at a liberal arts school at Yale.

00:00:51.300 --> 00:00:54.790
So I was studying science and technology but also being

00:00:54.790 --> 00:00:58.360
exposed to the human side of technology: so looking at philosophy,

00:00:58.360 --> 00:01:03.570
psychology, design, the human experience and that colored the way I look at AI.

00:01:03.570 --> 00:01:05.415
You know, my story is somewhat similar.

00:01:05.415 --> 00:01:08.950
I was in college and I wanted to understand people.

00:01:08.950 --> 00:01:11.470
And I studied philosophy, medicine,

00:01:11.470 --> 00:01:14.650
some psychology, a little biology.

00:01:14.650 --> 00:01:17.100
And I studied computer science as my major.

00:01:17.100 --> 00:01:22.510
And in computer science I found AI to be an interesting study of the human intelligence.

00:01:22.510 --> 00:01:24.895
And I got really frustrated with philosophy because

00:01:24.895 --> 00:01:27.790
people make these outrageous claims and they can't back it up.

00:01:27.790 --> 00:01:31.285
They can't say, here's a system and the system does exactly the following thing.

00:01:31.285 --> 00:01:33.425
Because in AI you couldn't make an outrageous claim at the time.

00:01:33.425 --> 00:01:35.920
It's very small but at least could build an entire system.

00:01:35.920 --> 00:01:38.860
So I got really intrigued about building robots that do something good I can understand.

00:01:38.860 --> 00:01:42.950
And what are some of your past projects that you're most proud of in AI?

00:01:42.950 --> 00:01:45.445
Well, I've worked on self-driving cars.

00:01:45.445 --> 00:01:47.950
I've built a huge number of different robots.

00:01:47.950 --> 00:01:52.035
I've built robots that would explore abandoned mines and build mine maps.

00:01:52.035 --> 00:01:56.931
I've built robots that went to Smithsonian Museum and gave tours to tourists,

00:01:56.931 --> 00:01:59.415
robots in the medical sector that help

00:01:59.415 --> 00:02:02.790
elderly age and be able to be more mobile and so on.

00:02:02.790 --> 00:02:04.165
And these are all AI projects.

00:02:04.165 --> 00:02:07.870
All the way to more like intellectual perception projects.

00:02:07.870 --> 00:02:09.810
I did a project recently on fighting

00:02:09.810 --> 00:02:13.091
skin cancer where we could show that an adequately trained AI is better

00:02:13.091 --> 00:02:15.000
in fighting skin cancer than the best human doctors.

00:02:15.000 --> 00:02:16.000
How about you Dr. Ram?

00:02:16.000 --> 00:02:19.090
I think my most previous project is

00:02:19.090 --> 00:02:22.000
the first one I did and it's also the last one I'm doing now.

00:02:22.000 --> 00:02:24.070
I've worked on a range of things.

00:02:24.070 --> 00:02:26.185
I've worked on robotics.

00:02:26.185 --> 00:02:28.310
I've worked on computer games, AI for games.

00:02:28.310 --> 00:02:30.765
I've worked on machine diagnostics.

00:02:30.765 --> 00:02:34.330
But I started my life in language understanding and in looking at

00:02:34.330 --> 00:02:38.040
how AI might understand human language and learn from human language.

00:02:38.040 --> 00:02:39.520
Several years later, full circle,

00:02:39.520 --> 00:02:42.080
now at Alexa I'm working on the same problem again.

00:02:42.080 --> 00:02:43.470
The progress has been amazing.

00:02:43.470 --> 00:02:47.250
The progress has been amazing and there's still a lot of hard problems to solve.

00:02:47.250 --> 00:02:49.500
Alexa, what time is it?

00:02:49.500 --> 00:02:52.970
The time is 2:07 PM.

00:02:52.970 --> 00:02:54.860
It's so cool.

00:02:54.860 --> 00:02:59.560
So you're currently working on Alexa and voice user interfaces.

00:02:59.560 --> 00:03:01.675
How are these two different?

00:03:01.675 --> 00:03:04.840
And, also, how do you explain to the lay audience

00:03:04.840 --> 00:03:08.370
the difference between conversational AI and voice recognition?

00:03:08.370 --> 00:03:10.930
The voice user interface part of Alexa or

00:03:10.930 --> 00:03:14.330
any other system is the speech recognition part.

00:03:14.330 --> 00:03:17.239
Right? We have to understand what words you just said

00:03:17.239 --> 00:03:20.417
from the sound waves to turn it into text.

00:03:20.417 --> 00:03:23.265
And then we have to also understand what you are asking.

00:03:23.265 --> 00:03:26.176
In this case, Sebastian was asking about the time,

00:03:26.176 --> 00:03:28.480
and so we have to then respond back with the time.

00:03:28.480 --> 00:03:33.760
That gives Sebastian a voice interface to a well known application.

00:03:33.760 --> 00:03:35.480
In this case, the time application.

00:03:35.480 --> 00:03:37.282
When you get into conversational AI,

00:03:37.282 --> 00:03:41.620
now he is talking about truly understanding what language means.

00:03:41.620 --> 00:03:43.420
Why the person is asking me this?

00:03:43.420 --> 00:03:45.115
And what response he might want?

00:03:45.115 --> 00:03:48.084
So if I ask Alexa to wake me up in the morning,

00:03:48.084 --> 00:03:51.040
what I'm really asking is set an alarm in the morning. I didn't say that.

00:03:51.040 --> 00:03:52.330
People don't say what they mean,

00:03:52.330 --> 00:03:54.150
they expect you to infer that.

00:03:54.150 --> 00:03:56.635
So you can ask Alexa something quite complicated.

00:03:56.635 --> 00:04:03.715
Like, "Alexa, who was the president when Obama was a teenager?"

00:04:03.715 --> 00:04:06.775
Richard Nixon, Ronald Reagan, Jimmy Carter,

00:04:06.775 --> 00:04:11.075
and Gerald Ford were the US presidents when Barack Obama was a teenager.

00:04:11.075 --> 00:04:12.970
So there's a lot of inference going on here.

00:04:12.970 --> 00:04:14.635
Firstly, what kind of president?

00:04:14.635 --> 00:04:18.375
Not a company president like Sebastian, the country's president.

00:04:18.375 --> 00:04:21.190
We know that because we're talking about Obama. Which Obama?

00:04:21.190 --> 00:04:26.500
We have shared knowledge of the world around us and we know exactly who Obama refers to.

00:04:26.500 --> 00:04:27.940
We also now, we can't look up

00:04:27.940 --> 00:04:31.585
a web page to find the answer to this question, it doesn't exist.

00:04:31.585 --> 00:04:33.160
We have to go back and do some inference.

00:04:33.160 --> 00:04:36.955
There's many steps involved in figuring out what time period were we talking about?

00:04:36.955 --> 00:04:40.770
Finding the individual piece of information and collecting it together on the fly.

00:04:40.770 --> 00:04:44.435
All of these are hard problems and it all has to happen in about a second.

00:04:44.435 --> 00:04:47.380
What are the implications of voice user interfaces

00:04:47.380 --> 00:04:51.540
for interfaces in the future and for human computer interaction?

00:04:51.540 --> 00:04:55.150
I would say we trained ourselves to use our fingers and,

00:04:55.150 --> 00:04:58.420
in the beginning, it was a very cryptic language to talk to a computer.

00:04:58.420 --> 00:04:59.440
In the early days,

00:04:59.440 --> 00:05:02.950
you had to do PIP and LS and crazy commands.

00:05:02.950 --> 00:05:04.924
And then we got to a point where we can move stuff with our hand mouse around.

00:05:04.924 --> 00:05:07.990
We trained our hand to move left and right,

00:05:07.990 --> 00:05:10.265
and up and down, or your finger on the trackpad.

00:05:10.265 --> 00:05:13.820
And now it's coming full circle, now we can just talk to our house

00:05:13.820 --> 00:05:17.258
the same way we talk to our family. So there's no instructions.

00:05:17.258 --> 00:05:20.735
They can unpack Alexa as example, and put on your desk.

00:05:20.735 --> 00:05:22.905
And the only thing you have to learn is their name.

00:05:22.905 --> 00:05:26.110
So you call Alexa and then you can talk to her natural language.

00:05:26.110 --> 00:05:29.090
And I think it's a great ability to take

00:05:29.090 --> 00:05:34.270
this compute idea and make it so pervasive in our lives without any training whatsoever.

00:05:34.270 --> 00:05:36.405
That just everyone in the world can benefit from it.

00:05:36.405 --> 00:05:37.850
Yeah, very similar story.

00:05:37.850 --> 00:05:41.955
For years, we've been typing to our computers and other devices.

00:05:41.955 --> 00:05:44.630
You know, we type on microwave control panels,

00:05:44.630 --> 00:05:47.045
and TV remotes, and all kinds of things.

00:05:47.045 --> 00:05:49.025
And for the last few years, 10 or so years,

00:05:49.025 --> 00:05:52.660
we've had touchscreens so we've been able to touch and swipe and so forth.

00:05:52.660 --> 00:05:54.245
But that's not how we communicate.

00:05:54.245 --> 00:05:56.240
When we want to talk we are not typing at each other.

00:05:56.240 --> 00:05:57.865
We are not touching and swiping.

00:05:57.865 --> 00:05:59.535
We're just talking and you're talking back.

00:05:59.535 --> 00:06:01.520
Right? Why can't our devices do that?

00:06:01.520 --> 00:06:03.280
Not just computers, all devices.

00:06:03.280 --> 00:06:05.750
Why can't they talk to my thermostat, and my microwave,

00:06:05.750 --> 00:06:09.290
and my car, and every other piece of technology?

00:06:09.290 --> 00:06:11.510
Imagine the world of technology being

00:06:11.510 --> 00:06:14.880
accessible to everyone not just to the techno savvy.

00:06:14.880 --> 00:06:16.220
I think that's the future of AI.

00:06:16.220 --> 00:06:20.013
I just spent in a hotel and Alexa was part of the hotel room,

00:06:20.013 --> 00:06:21.394
we've talked about this before - with

00:06:21.394 --> 00:06:25.320
this recording and I could go in and open the curtains using voice.

00:06:25.320 --> 00:06:28.965
So do you think we're upon the end of keyboards as we know it?

00:06:28.965 --> 00:06:30.825
Would you say that it's happening soon?

00:06:30.825 --> 00:06:32.584
I think there's a sweet spot for IV mortality,

00:06:32.584 --> 00:06:34.785
they're never going entirely away.

00:06:34.785 --> 00:06:36.595
If we look at the ability to,

00:06:36.595 --> 00:06:39.060
the number of word counts per minute and so on,

00:06:39.060 --> 00:06:40.655
keyboards will have the sweet spot.

00:06:40.655 --> 00:06:42.920
But I feel bothered by the fact that I have to

00:06:42.920 --> 00:06:45.110
move my fingers to talk to a computer, honestly.

00:06:45.110 --> 00:06:46.500
I'd rather talk the way we're talking right now.

00:06:46.500 --> 00:06:49.335
I think this is the first time it's become a reality.

00:06:49.335 --> 00:06:51.090
That's why I'm such a big fan of Alexa.

00:06:51.090 --> 00:06:53.960
And I really am not just because he is sitting here and working with us.

00:06:53.960 --> 00:06:55.055
I'm a big fan of Alexa.

00:06:55.055 --> 00:06:58.040
I have many of them at home because it is,

00:06:58.040 --> 00:06:59.860
for me, a complete new era of computing.

00:06:59.860 --> 00:07:02.770
Yes. So, again, the sweet spot.

00:07:02.770 --> 00:07:05.330
We still need to type things.

00:07:05.330 --> 00:07:06.695
We still need a mouse and keyboard.

00:07:06.695 --> 00:07:09.800
If I'm making a PowerPoint presentation or writing a word document,

00:07:09.800 --> 00:07:11.210
I'm not going to speak it out.

00:07:11.210 --> 00:07:13.895
And I certainly can't draw using my voice.

00:07:13.895 --> 00:07:16.145
But that's a very niche use case.

00:07:16.145 --> 00:07:17.330
I think from 80, 90,

00:07:17.330 --> 00:07:20.775
95% of what we do in our daily lives voice should be just fine.

00:07:20.775 --> 00:07:26.115
That's amazing. We actually do some speech to text dictation when teaching.

00:07:26.115 --> 00:07:28.439
It's a tool for rapid prototyping sometimes at

00:07:28.439 --> 00:07:30.830
Udacity when we're scripting out our courses or content.

00:07:30.830 --> 00:07:33.280
So there's also that use case too.

00:07:33.280 --> 00:07:37.220
So you're solving a major challenge with conversational AI and what are

00:07:37.220 --> 00:07:38.570
the product challenges and

00:07:38.570 --> 00:07:42.140
the technical challenges that you're encountering along the way?

00:07:42.140 --> 00:07:43.940
So there are several science challenges and,

00:07:43.940 --> 00:07:46.255
of course, engineering challenges as well.

00:07:46.255 --> 00:07:50.090
On the science side, I mentioned some of them earlier: understanding meaning,

00:07:50.090 --> 00:07:55.415
understanding the speaker's intent in talking about something.

00:07:55.415 --> 00:08:00.508
Another tough one is in a conversation where you have multiple tones in a dialogue,

00:08:00.508 --> 00:08:02.790
you have to track context across dialogue.

00:08:02.790 --> 00:08:07.610
A lot of our conversational agents nowadays are very good at transactional interaction.

00:08:07.610 --> 00:08:10.655
Sort of one-step: play music, tell me the time.

00:08:10.655 --> 00:08:13.070
But in order to carry on a multi-step conversation,

00:08:13.070 --> 00:08:17.400
we have to track the context of the dialogue along as we go.

00:08:17.400 --> 00:08:19.410
That's another really hard problem.

00:08:19.410 --> 00:08:20.985
Personalization is a hard problem.

00:08:20.985 --> 00:08:23.675
Personality in the dialogue is to every individual.

00:08:23.675 --> 00:08:25.320
On the technical side,

00:08:25.320 --> 00:08:27.320
there are a lot of challenges in training some

00:08:27.320 --> 00:08:30.035
of the machine learning systems that I use to do this,

00:08:30.035 --> 00:08:32.000
it requires a lot of data.

00:08:32.000 --> 00:08:34.635
It requires a lot of computer power and lot of times.

00:08:34.635 --> 00:08:39.970
So we have highly paralyzed training systems that are used to train models.

00:08:39.970 --> 00:08:41.960
These models have to run in real time,

00:08:41.960 --> 00:08:43.470
that's a hard problem as well.

00:08:43.470 --> 00:08:46.310
And they're typically too large to put on a device.

00:08:46.310 --> 00:08:49.340
So you also need connectivity to the cloud at

00:08:49.340 --> 00:08:53.210
all times with fast enough response rate and low enough latency.

00:08:53.210 --> 00:08:55.035
Dr. Thrun you have any questions?

00:08:55.035 --> 00:09:00.416
Well, I would say, as much as I'm fan of Amazon Echo it's still in the infancy.

00:09:00.416 --> 00:09:02.440
So this box, at some point,

00:09:02.440 --> 00:09:04.695
should be smarter than the smartest Stanford professor.

00:09:04.695 --> 00:09:07.400
In fact, my several PhD students would say I'd

00:09:07.400 --> 00:09:10.365
rather study with Alexa than study with Sebastian.

00:09:10.365 --> 00:09:11.492
If you look at the spectrum of [inaudible] ,

00:09:11.492 --> 00:09:13.490
now the point being, we have good transactional models.

00:09:13.490 --> 00:09:15.710
I actually like this much over the previous generation of

00:09:15.710 --> 00:09:17.995
speech systems where you had to learn the vocabulary.

00:09:17.995 --> 00:09:20.780
My car had to understand which word to use.

00:09:20.780 --> 00:09:22.104
This is now flexible enough.

00:09:22.104 --> 00:09:24.560
It understand stand me without any training.

00:09:24.560 --> 00:09:26.980
But as you said, I think it's becoming really wise and smart.

00:09:26.980 --> 00:09:30.870
And being real coached with me and understand what I want. It's a long step.

00:09:30.870 --> 00:09:34.500
And the reason I mention this is because I know you guys are hiring.

00:09:34.500 --> 00:09:36.895
It's an amazingly great feat to be in right now.

00:09:36.895 --> 00:09:38.788
I mean, this is amazing.

00:09:38.788 --> 00:09:41.585
But just think about where it is going to be 10 years from now.

00:09:41.585 --> 00:09:45.460
When I can come home and it knows exactly what my favorite food is,

00:09:45.460 --> 00:09:49.005
and knows that the spinach is spoiled this day, and it reminds me of this.

00:09:49.005 --> 00:09:50.715
Or maybe even ordered already and

00:09:50.715 --> 00:09:53.780
Amazon drones came by and delivered it before you even thought about it.

00:09:53.780 --> 00:09:57.925
That's an amazing vision. This can become really a personal assistant in some sense.

00:09:57.925 --> 00:10:02.330
Yeah, that's absolutely a future that we can all look forward to.

00:10:02.330 --> 00:10:05.345
Again several hard challenges in realizing that.

00:10:05.345 --> 00:10:08.660
Speech recognition is near solved.

00:10:08.660 --> 00:10:11.845
We get very high accuracy with no training on most systems now.

00:10:11.845 --> 00:10:13.170
The next step, of course,

00:10:13.170 --> 00:10:16.105
is the conversational language understanding I talked about.

00:10:16.105 --> 00:10:18.300
Once you've done that, the beyond that,

00:10:18.300 --> 00:10:21.425
to get to the kind of vision you're talking about, we now need reasoning.

00:10:21.425 --> 00:10:24.580
We now need these systems to be able to understand us and reason about

00:10:24.580 --> 00:10:28.990
our world not just the language part of it but listen about our goals, and our beliefs,

00:10:28.990 --> 00:10:33.700
and our intents, and our desires and recognize that you're home late,

00:10:33.700 --> 00:10:36.850
and want a quick meal, and have that drone deliver to you ahead of time.

00:10:36.850 --> 00:10:39.990
I think the next big challenge in AI is going to be reasoning system,

00:10:39.990 --> 00:10:41.590
natural language or otherwise.

00:10:41.590 --> 00:10:43.248
Yeah, it's going to be complete transformation.

00:10:43.248 --> 00:10:45.805
We're going to look at these devices as dinosaurs.

00:10:45.805 --> 00:10:49.695
You could touch them and so on and you could probably text somebody else?

00:10:49.695 --> 00:10:51.704
And it does show me the time,

00:10:51.704 --> 00:10:53.806
but it was in my pocket.

00:10:53.806 --> 00:10:58.470
And if this technology becomes pervasive and you never ever touch a device again.

00:10:58.470 --> 00:11:00.730
And you don't even know what devices are anymore because

00:11:00.730 --> 00:11:03.175
they're with you when you go to your office or go jogging.

00:11:03.175 --> 00:11:06.460
And then there's another device with you and it just seamlessly integrates.

00:11:06.460 --> 00:11:08.800
That is going to be the future of computer science.

00:11:08.800 --> 00:11:10.840
And so many of us get stuck.

00:11:10.840 --> 00:11:13.820
Like, they think, OK, in the mainframe time it was mainframes.

00:11:13.820 --> 00:11:17.435
In the smartphone, smartphone is the ultimate thing to do.

00:11:17.435 --> 00:11:19.090
And they can't really think ahead what's next.

00:11:19.090 --> 00:11:20.960
I think this is showing us the way ahead.

00:11:20.960 --> 00:11:23.110
I agree with that. I think the future of computing is

00:11:23.110 --> 00:11:25.745
where we won't see computers anymore.

00:11:25.745 --> 00:11:27.610
We have that with motors,

00:11:27.610 --> 00:11:29.965
electrical motors, and everything around us.

00:11:29.965 --> 00:11:32.140
We never see them, we don't talk about them,

00:11:32.140 --> 00:11:33.610
we take it for granted that they exist,

00:11:33.610 --> 00:11:35.950
and they do the right things for us the right time.

00:11:35.950 --> 00:11:37.470
Computing should be like that.

00:11:37.470 --> 00:11:41.530
Do you both imagine that computers will be embedded in our environment all around us?

00:11:41.530 --> 00:11:43.585
Or do you imagine it being embedded within us?

00:11:43.585 --> 00:11:48.310
Or we're constantly carrying it as we're carrying that dinosaur brick.

00:11:48.310 --> 00:11:51.605
I for one can't wait until I have at least one chip embedded in me.

00:11:51.605 --> 00:11:54.280
It really, really upsets me to have to authenticate.

00:11:54.280 --> 00:11:57.430
And I would love to pay this, on this way, honestly.

00:11:57.430 --> 00:12:01.720
And I'm super happy to have a chip implanted if that's what it takes.

00:12:01.720 --> 00:12:04.180
So that I can just move around without, like,

00:12:04.180 --> 00:12:07.755
every so and so often I have to go to my phone and read some two factor number,

00:12:07.755 --> 00:12:09.500
and then punch it back into a browser.

00:12:09.500 --> 00:12:11.045
That's just so wrong, in my opinion.

00:12:11.045 --> 00:12:12.290
Will I go further?

00:12:12.290 --> 00:12:15.190
I mean, if we really go far

00:12:15.190 --> 00:12:18.710
out I think that's an opportunity to have a brain interface that's better than speech.

00:12:18.710 --> 00:12:23.635
Right now, our brains are limited in part because our own I/O is not very good.

00:12:23.635 --> 00:12:26.770
We are very slow in speaking, very slow in listening.

00:12:26.770 --> 00:12:28.975
So maybe we cannot get to the system that

00:12:28.975 --> 00:12:30.760
newborn babies know a lot of stuff about

00:12:30.760 --> 00:12:33.360
the world already because they have a brain interface. I don't know.

00:12:33.360 --> 00:12:35.065
But before that happens,

00:12:35.065 --> 00:12:37.030
I just envision that my clothing,

00:12:37.030 --> 00:12:38.710
my environment, my building,

00:12:38.710 --> 00:12:40.885
and so on will have this pervasive network of

00:12:40.885 --> 00:12:44.485
whatever evolves out of Amazon Echo that is able to listen to me,

00:12:44.485 --> 00:12:46.630
understand me, and assist me.

00:12:46.630 --> 00:12:48.630
Yeah, I agree with that vision as well.

00:12:48.630 --> 00:12:51.550
I think there is a value to

00:12:51.550 --> 00:12:55.150
having a conversational interface in addition to having a brain interface.

00:12:55.150 --> 00:12:56.920
Just like typewriters didn't go away I

00:12:56.920 --> 00:12:59.655
don't think voice will go away once we have brain interfaces.

00:12:59.655 --> 00:13:01.660
There are many use cases where you want,

00:13:01.660 --> 00:13:04.050
for example, a conversational companion at home.

00:13:04.050 --> 00:13:04.810
Yeah.

00:13:04.810 --> 00:13:07.225
You know, a babysitter for your kid.

00:13:07.225 --> 00:13:09.595
Or a companion for an older person.

00:13:09.595 --> 00:13:12.618
You don't necessarily need a computer reading your great-grandmother's brain.

00:13:12.618 --> 00:13:19.915
When people live to be 250 years old you want them to be able to talk to them.

00:13:19.915 --> 00:13:21.427
So I think this use case will follow them,

00:13:21.427 --> 00:13:23.440
but I think computers will be everywhere.

00:13:23.440 --> 00:13:25.290
They'll be on us and they'll be around us.

00:13:25.290 --> 00:13:27.760
There's a great book, regarding the companion topic,

00:13:27.760 --> 00:13:28.980
it's called Marjorie Prime,

00:13:28.980 --> 00:13:31.465
or it was a play that was recently turned into a movie.

00:13:31.465 --> 00:13:34.390
And it really goes into AI and companions.

00:13:34.390 --> 00:13:38.230
I think it's a great film and a great play. I highly recommend it.

00:13:38.230 --> 00:13:40.570
On the topic of hiring, which you mentioned earlier,

00:13:40.570 --> 00:13:43.540
and the topic of the challenges in AI, as you know,

00:13:43.540 --> 00:13:46.360
we have an Artificial Intelligence Nanodegree program at Udacity,

00:13:46.360 --> 00:13:50.155
and we have this self-driving car Nanodegree program that Sebastian pioneered.

00:13:50.155 --> 00:13:53.560
What are the challenges and skills that you think are most important that

00:13:53.560 --> 00:13:57.630
students need to have to tackle these challenges in the near future?

00:13:57.630 --> 00:14:01.735
So we're very excited to be partnering with the AI Nanodegree at Udacity.

00:14:01.735 --> 00:14:03.460
And I've spoken to many of the students.

00:14:03.460 --> 00:14:06.280
I think they're bright and they have great futures ahead of them.

00:14:06.280 --> 00:14:08.710
One of the interesting things about AI is that,

00:14:08.710 --> 00:14:12.535
when you think of this future world with AI being embedded everywhere,

00:14:12.535 --> 00:14:14.940
AI jobs aren't just in AI.

00:14:14.940 --> 00:14:16.650
They're in every part of life.

00:14:16.650 --> 00:14:19.340
You know, you will need some knowledge and understanding of AI.

00:14:19.340 --> 00:14:21.754
So what is that knowledge and understanding?

00:14:21.754 --> 00:14:25.795
You need to understand how to make sense of large amounts of data and

00:14:25.795 --> 00:14:28.295
build models that will enable you to then

00:14:28.295 --> 00:14:32.295
predict the future based on other new data you might not have seen yet.

00:14:32.295 --> 00:14:36.330
You need to be able to do that kind of reasoning and inference that I talked about.

00:14:36.330 --> 00:14:40.270
You need to be able to do problem solving and decision making that is

00:14:40.270 --> 00:14:44.800
really data driven and also contextually knowledge driven.

00:14:44.800 --> 00:14:47.825
And so the Nanodegrees that you're creating have

00:14:47.825 --> 00:14:51.384
a lot of the core technology and a lot of the core understanding.

00:14:51.384 --> 00:14:54.645
The thing is, you do need to understand the algorithms and the math,

00:14:54.645 --> 00:14:56.440
but it's really more than just that.

00:14:56.440 --> 00:14:59.050
It's trying to understand what are the kinds

00:14:59.050 --> 00:15:02.860
of conceptual frameworks you bring to bear when trying to solve a problem?

00:15:02.860 --> 00:15:04.620
In computer science, well,

00:15:04.620 --> 00:15:07.615
I used to be a professor, we call it computational thinking.

00:15:07.615 --> 00:15:11.515
Just like you have design thinking and you have other kinds of mathematical thinking.

00:15:11.515 --> 00:15:15.150
Computational thinking is a way of thinking about problems and this AI thinking is

00:15:15.150 --> 00:15:19.975
another newfangled way now that's just emerging and will become pervasive.

00:15:19.975 --> 00:15:22.170
What I find fascinating is the way

00:15:22.170 --> 00:15:26.230
recent work in AI is changing the way we program computers.

00:15:26.230 --> 00:15:29.280
In the past, a software engineer would sit

00:15:29.280 --> 00:15:33.175
down and think about every possible contingency.

00:15:33.175 --> 00:15:34.830
So if you write, say,

00:15:34.830 --> 00:15:36.567
an Android operating system there's

00:15:36.567 --> 00:15:40.980
12 million possible contingencies and you write this amazingly long kitchen recipe like,

00:15:40.980 --> 00:15:43.125
if this happens so this, if this happens so this,

00:15:43.125 --> 00:15:44.550
if this happens so this,

00:15:44.550 --> 00:15:46.643
and then iterate to this.

00:15:46.643 --> 00:15:48.380
And because it is so hard to anticipate

00:15:48.380 --> 00:15:50.745
every contingency you would pay these people lots of money.

00:15:50.745 --> 00:15:52.515
Of course, many of Udacity students

00:15:52.515 --> 00:15:55.725
expect to make a lot of money and hopefully make lots of money doing this.

00:15:55.725 --> 00:15:57.595
But the transition to machine learning

00:15:57.595 --> 00:16:00.660
now is kind of much more, I think, much more humane.

00:16:00.660 --> 00:16:04.080
We don't tell kids in

00:16:04.080 --> 00:16:05.310
the first 18 years of life

00:16:05.310 --> 00:16:08.415
every possible contingency and then let them out. We let them learn.

00:16:08.415 --> 00:16:09.930
If you give them examples and then make up

00:16:09.930 --> 00:16:12.415
their own rules and they'll learn from their own data.

00:16:12.415 --> 00:16:16.030
And now we're at the point where our AI systems we can actually teach them based on data.

00:16:16.030 --> 00:16:19.140
So rather than telling it, this is exactly what this vowel looks like,

00:16:19.140 --> 00:16:21.020
this is exactly how we process documents,

00:16:21.020 --> 00:16:22.570
how to exactly we respond,

00:16:22.570 --> 00:16:24.170
let them look at data.

00:16:24.170 --> 00:16:26.965
That's what deep learning really has done to us in artificial intelligence.

00:16:26.965 --> 00:16:29.005
And I think it's a paradigm shift not just for AI.

00:16:29.005 --> 00:16:31.275
It's a paradigm shift for all of computer science.

00:16:31.275 --> 00:16:35.430
In the future, I think, there will be a much strong component of trusting

00:16:35.430 --> 00:16:37.675
our machines to find their own rules than

00:16:37.675 --> 00:16:40.565
having to sit down and give them all the rules by hand.

00:16:40.565 --> 00:16:42.420
One of the effects of that is going to be that

00:16:42.420 --> 00:16:44.455
these machines will all be different from each other.

00:16:44.455 --> 00:16:47.490
Even today, your Alexas that are in your house are different from mine.

00:16:47.490 --> 00:16:49.345
Because they're personalized, they're learning about me,

00:16:49.345 --> 00:16:50.430
they know what music I have,

00:16:50.430 --> 00:16:53.440
and what time I wake up in the morning.

00:16:53.440 --> 00:16:55.395
As you fast forward that into the future,

00:16:55.395 --> 00:16:58.235
these self-learning machines are all going to be different.

00:16:58.235 --> 00:17:01.920
So nowadays if you go out and buy any piece of technology,

00:17:01.920 --> 00:17:03.180
a car or a computer,

00:17:03.180 --> 00:17:05.520
they're all identical if you buy the same model.

00:17:05.520 --> 00:17:06.990
That will no longer be the case.

00:17:06.990 --> 00:17:09.785
Different training data in every household?

00:17:09.785 --> 00:17:12.410
Identical twins turn out to be often very different.

00:17:12.410 --> 00:17:14.100
And it's not the DNA that makes them different.

00:17:14.100 --> 00:17:14.832
It's the environment.

00:17:14.832 --> 00:17:16.937
Different environment, different experiences.

00:17:16.937 --> 00:17:19.872
So these AI's have experiences as well and as Sebastian said

00:17:19.872 --> 00:17:23.585
it's all about learning from those experiences.

00:17:23.585 --> 00:17:26.700
So the programmers of today are people who know how to design

00:17:26.700 --> 00:17:30.665
models that can learn from experiences as opposed to building code.

00:17:30.665 --> 00:17:33.915
And that's a complete new skill set that doesn't exist today.

00:17:33.915 --> 00:17:35.385
If you go to any major college,

00:17:35.385 --> 00:17:37.170
you learn how to code and so on.

00:17:37.170 --> 00:17:41.640
But this new kind of programming is so brand new and so interesting and I think it's just

00:17:41.640 --> 00:17:46.290
now coming to the point where you have really interesting results like self-driving cars,

00:17:46.290 --> 00:17:48.450
cancer diagnostics, legal document discovery,

00:17:48.450 --> 00:17:50.595
speech recognition, natural language understanding.

00:17:50.595 --> 00:17:52.170
All these things are now getting to the point

00:17:52.170 --> 00:17:54.270
where they become really really interesting.

00:17:54.270 --> 00:17:56.005
Is Amazon hiring, Dr. Ram?

00:17:56.005 --> 00:17:57.470
We're hiring a ton.

00:17:57.470 --> 00:18:02.520
Amazon is hiring 100,000 people this year across all the different businesses.

00:18:02.520 --> 00:18:02.580
That's a lot of people.

00:18:02.580 --> 00:18:05.635
Alexa is hiring as well. And we are

00:18:05.635 --> 00:18:09.255
certainly super excited about our partnership with Udacity.

00:18:09.255 --> 00:18:11.490
We have our recruiters talking to your folks

00:18:11.490 --> 00:18:13.830
and we have a pipeline of really good people.

00:18:13.830 --> 00:18:16.336
I see more and more candidates with some kind of

00:18:16.336 --> 00:18:19.470
Udacity Nanodegree on them which is nice.

00:18:19.470 --> 00:18:22.470
I have a question for you? If you had a magic wand to

00:18:22.470 --> 00:18:26.085
solve one hard technology problem, what would it be?

00:18:26.085 --> 00:18:28.785
Like, something that bugs you when you wake up in the morning and say,

00:18:28.785 --> 00:18:31.200
this is me in the way of meeting needs whatever you want to build?

00:18:31.200 --> 00:18:35.490
I think the big gap right

00:18:35.490 --> 00:18:41.220
now in AI is what people sometimes call the knowledge graph,

00:18:41.220 --> 00:18:43.135
is knowing enough about the world.

00:18:43.135 --> 00:18:45.930
So the data driven vision that we talked about is

00:18:45.930 --> 00:18:49.120
great when you have all that data and you have all that computer horsepower.

00:18:49.120 --> 00:18:51.030
And you can train these machines with

00:18:51.030 --> 00:18:53.544
all possible contingencies that you might have seen.

00:18:53.544 --> 00:18:56.590
When you're in something new, new situation,

00:18:56.590 --> 00:19:00.090
new use case, new problem you want to address, or when, again,

00:19:00.090 --> 00:19:01.995
back to the example when we're just talking,

00:19:01.995 --> 00:19:04.703
we share a lot of knowledge about the world,

00:19:04.703 --> 00:19:08.245
how do we capture that shared knowledge and make it available to computers?

00:19:08.245 --> 00:19:10.715
Really, nteresting. When I studied AI and so I'm used to AI,

00:19:10.715 --> 00:19:12.955
we were much more than knowledge-based world,

00:19:12.955 --> 00:19:16.380
[inaudible] building out this big knowledge-based and so on.

00:19:16.380 --> 00:19:18.728
And now, we seem to be much more in the perceptual world when we can

00:19:18.728 --> 00:19:22.220
recognize voice and recognize images and so on.

00:19:22.220 --> 00:19:24.200
Do you think the pendulum is swinging back?

00:19:24.200 --> 00:19:27.380
I think it'll emerge. I want to see hybrid solutions.

00:19:27.380 --> 00:19:28.710
I've worked in for years,

00:19:28.710 --> 00:19:30.895
as have you, in the knowledge-based paradigm.

00:19:30.895 --> 00:19:32.855
I know its strengths and its weaknesses.

00:19:32.855 --> 00:19:35.630
And now with the data-driven and machine learning paradigms,

00:19:35.630 --> 00:19:37.570
there are certainly limitations there as well.

00:19:37.570 --> 00:19:39.070
We're going to need both.

00:19:39.070 --> 00:19:42.295
I think the big problem we haven't solved yet is how we merge them.

00:19:42.295 --> 00:19:44.795
There's some sort of ad hoc solutions out there,

00:19:44.795 --> 00:19:46.340
we use a little of this, a little of that,

00:19:46.340 --> 00:19:48.360
but no truly integrated approach as yet.

00:19:48.360 --> 00:19:51.405
One more question, in the media,

00:19:51.405 --> 00:19:52.770
AI is often discussed in

00:19:52.770 --> 00:19:56.147
some very ambivalent terms like if you talk to people like Elon Musk,

00:19:56.147 --> 00:20:01.755
they believe AI will enslave us and kill us possibly or whatever.

00:20:01.755 --> 00:20:07.170
You built probably the most direct system that is in touch with all of us.

00:20:07.170 --> 00:20:09.745
My Alexas listen to me all day long.

00:20:09.745 --> 00:20:12.186
I have a concern that some smart AI may break lose,

00:20:12.186 --> 00:20:14.815
listen to me, and take control of me?

00:20:14.815 --> 00:20:16.360
I'm not concerned about that.

00:20:16.360 --> 00:20:19.350
I'm not anymore concerned about that.

00:20:19.350 --> 00:20:20.530
I'm concerned about, you know,

00:20:20.530 --> 00:20:22.290
smart people breaking loose,

00:20:22.290 --> 00:20:25.120
my kids, or friends, or strangers as well.

00:20:25.120 --> 00:20:28.380
Occasionally, there are rouge people and they maybe rouge AIs,

00:20:28.380 --> 00:20:31.230
but that's not the dominant future.

00:20:31.230 --> 00:20:33.860
I think these AIs they grow up in our world,

00:20:33.860 --> 00:20:35.195
they're socialized in our world.

00:20:35.195 --> 00:20:38.890
And as we get more and more towards AI's that truly learn from experience,

00:20:38.890 --> 00:20:41.050
they're going to learn our values as well.

00:20:41.050 --> 00:20:43.345
And then a lot of good people working on

00:20:43.345 --> 00:20:46.620
the ethics of AI and building ethics into the AI,

00:20:46.620 --> 00:20:50.170
I don't know that we necessarily have to build it in as a rule.

00:20:50.170 --> 00:20:52.060
I think it's something that emerges from the kind

00:20:52.060 --> 00:20:54.775
of data that they encounter in the world that they live.

00:20:54.775 --> 00:20:57.430
They learn from us because they interact with us.

00:20:57.430 --> 00:20:58.800
I'm with you. I'm totally with you.

00:20:58.800 --> 00:21:03.250
So, in addition, to being a founder of a very successful e-learning company,

00:21:03.250 --> 00:21:04.775
you're also a technologist.

00:21:04.775 --> 00:21:06.925
If you have a magic wand and could wave that,

00:21:06.925 --> 00:21:10.415
what technology, or science problem would you like to have solved?

00:21:10.415 --> 00:21:16.485
I would literally love to get computers into my brain.

00:21:16.485 --> 00:21:18.730
I mean, if I look at my example,

00:21:18.730 --> 00:21:20.230
the Google self-driving car was,

00:21:20.230 --> 00:21:25.475
that the self-driving car would surpass human driving and it was predictable.

00:21:25.475 --> 00:21:28.000
And the reason is, when we people make

00:21:28.000 --> 00:21:31.425
a driving mistake I learn from it or maybe you learn from it.

00:21:31.425 --> 00:21:33.520
But I won't learn from your mistakes and you won't

00:21:33.520 --> 00:21:36.725
from my mistakes so all of us have to make the same mistakes.

00:21:36.725 --> 00:21:39.350
And that's an impediment to progress.

00:21:39.350 --> 00:21:41.592
Let's say if Google self-driving car makes a mistake,

00:21:41.592 --> 00:21:44.410
all the other ones know from it including all the unborn cars.

00:21:44.410 --> 00:21:48.696
So I would love to have a world where we birth a child and they're as

00:21:48.696 --> 00:21:53.495
smart as a high school graduate or have a PhD and they know something very interesting.

00:21:53.495 --> 00:21:55.840
Of course, it's physiologically impossible today.

00:21:55.840 --> 00:21:58.315
But perhaps instead of having to

00:21:58.315 --> 00:22:01.420
train all these things all our life maybe we can just know them.

00:22:01.420 --> 00:22:03.475
And we've gotten very close, honestly.

00:22:03.475 --> 00:22:05.445
And I think if I can Google anything right now or I can Echo

00:22:05.445 --> 00:22:08.100
anything and I get the facts right there.

00:22:08.100 --> 00:22:11.680
And it's a big game changer for me because now I know about Afghanistan,

00:22:11.680 --> 00:22:13.210
know about the Civil War,

00:22:13.210 --> 00:22:14.837
and all these things just by googling it when I need it.

00:22:14.837 --> 00:22:18.395
But it's still effort involved,

00:22:18.395 --> 00:22:21.035
[inaudible] know these things.

00:22:21.035 --> 00:22:24.910
So can I make a device that I recognize every face I ever seen.

00:22:24.910 --> 00:22:27.520
I recognize any conversation I ever had.

00:22:27.520 --> 00:22:32.065
And really put my brain on steroids so I can learn from others mistakes much faster.

00:22:32.065 --> 00:22:35.170
There would, it's hard to think about today before

00:22:35.170 --> 00:22:38.790
you involve some crazy implant and I'm not sure people want to have a crazy implant.

00:22:38.790 --> 00:22:41.170
But, you know, everything is possible.

00:22:41.170 --> 00:22:43.625
I'll beta test for sure.

00:22:43.625 --> 00:22:48.280
Sign me up. My friend Eli Brazer is very passionate about the same topic.

00:22:48.280 --> 00:22:49.870
He's a data scientist at Netflix,

00:22:49.870 --> 00:22:54.900
and he's very passionate about the fact that humans will pass away.

00:22:54.900 --> 00:22:56.290
And when a human being passes away,

00:22:56.290 --> 00:22:59.095
we lose all of that knowledge and all that experience and all of

00:22:59.095 --> 00:23:01.960
the intuition and the intuitive choices we make.

00:23:01.960 --> 00:23:05.020
So, if you're a surfer and you go to the beach and you know

00:23:05.020 --> 00:23:09.000
the waves are choppy you have this intuitive feeling,

00:23:09.000 --> 00:23:12.653
"Oh, it feels like sharky waters," or "All this experience is lost."

00:23:12.653 --> 00:23:15.045
So I hope that that our future comes to pass.

00:23:15.045 --> 00:23:17.720
Yeah. I'm confident. I mean, go back I don't know about a little more than 100 years ago,

00:23:17.720 --> 00:23:20.700
the world's expert on flights met.

00:23:20.700 --> 00:23:23.110
And they determined, absolutely,

00:23:23.110 --> 00:23:25.515
flight is impossible after many decades of trying.

00:23:25.515 --> 00:23:28.270
Secretly in Kittyhawk,

00:23:28.270 --> 00:23:31.960
two brothers invent the flight at the same time and they didn't know about it and so on.

00:23:31.960 --> 00:23:34.250
All of a sudden, "Oh, my god, it's possible."

00:23:34.250 --> 00:23:36.993
So all these things when people say it's impossible,

00:23:36.993 --> 00:23:38.800
my take is just wait.

00:23:38.800 --> 00:23:42.460
OK. Because typically things are possible, in my opinion.

00:23:42.460 --> 00:23:45.145
And many things that we've deemed impossible like, today,

00:23:45.145 --> 00:23:48.440
my voice is strong enough that it carries to Australia.

00:23:48.440 --> 00:23:52.100
And so, I take this thing over here and I talk with someone in Australia. It's amazing.

00:23:52.100 --> 00:23:56.355
That was deemed to be impossible for the vast majority of human history.

00:23:56.355 --> 00:23:57.570
So having a brain interface,

00:23:57.570 --> 00:23:58.780
to me, seems very possible.

00:23:58.780 --> 00:24:00.841
And we're pretty close in some some areas.

00:24:00.841 --> 00:24:03.130
There's some very nice brain interfaces, for example,

00:24:03.130 --> 00:24:06.095
for disabled people and locked-in patients.

00:24:06.095 --> 00:24:12.230
And the next step is the kind of world that might allow all of us to augment our brain.

00:24:12.230 --> 00:24:13.915
Not just to control things,

00:24:13.915 --> 00:24:17.425
like in the case of disabled people today, but

00:24:17.425 --> 00:24:20.650
also in terms of to ingest the sum total of human knowledge.

00:24:20.650 --> 00:24:21.640
That'd be amazing.

00:24:21.640 --> 00:24:23.140
That'd be like The Matrix.

00:24:23.140 --> 00:24:23.324
No, better.

00:24:23.324 --> 00:24:24.429
Better.

00:24:24.429 --> 00:24:26.920
Much better. I don't think we would use human bodies

00:24:26.920 --> 00:24:29.500
for energy because it makes no sense to me whatsoever.

00:24:29.500 --> 00:24:32.305
I meant when Neo learns kung fu in, like, an instant.

00:24:32.305 --> 00:24:35.290
You can learn anything in seconds. That's amazing.

00:24:35.290 --> 00:24:39.130
Well, that's it for time. That's all the questions we have today. Thank you, Ashwin.

00:24:39.130 --> 00:24:39.973
And thank you.

00:24:39.973 --> 00:24:42.530
Thank you, Sebastian. Thank you both for being here today.

00:24:42.530 --> 00:24:42.704
Pleasure.

00:24:42.704 --> 00:24:46.200
Greatly appreciate it. And thank you for watching.

00:24:46.200 --> 00:24:48.570
Join us next time.

