WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.280
[GOOGLE LOGO MUSIC]

00:00:04.560 --> 00:00:05.805
JESSE ENGEL: I am Jesse.

00:00:05.805 --> 00:00:06.930
ADAM ROBERTS: And I'm Adam.

00:00:06.930 --> 00:00:08.180
JESSE ENGEL: And we are from--

00:00:08.180 --> 00:00:11.400
thanks-- we're from a
project within Google

00:00:11.400 --> 00:00:13.050
called Project Magenta.

00:00:13.050 --> 00:00:15.870
It's part of the machine
learning group within Google.

00:00:15.870 --> 00:00:19.800
And we work specifically on--

00:00:19.800 --> 00:00:21.820
it's an open source
research project.

00:00:21.820 --> 00:00:24.000
So we do cutting edge
machine learning research.

00:00:24.000 --> 00:00:26.790
But we're really
interested in the role

00:00:26.790 --> 00:00:30.210
that machine learning can
play for creative technologies

00:00:30.210 --> 00:00:32.159
and for artists and musicians.

00:00:32.159 --> 00:00:35.550
So everything we do we
put out in open source.

00:00:35.550 --> 00:00:39.540
And we also focus on
building tools for developers

00:00:39.540 --> 00:00:43.110
and for artists so that they
can actually actively explore

00:00:43.110 --> 00:00:48.130
using AI and machine learning
in the creative process.

00:00:48.130 --> 00:00:50.492
And so, if you're interested
more in the project,

00:00:50.492 --> 00:00:51.450
just a real quick plug.

00:00:51.450 --> 00:00:54.300
You can go to g.co/magenta.

00:00:54.300 --> 00:00:55.860
If you're more on
the research side,

00:00:55.860 --> 00:00:58.490
you can find all sorts of
research papers and data sets.

00:00:58.490 --> 00:01:01.200
If you're a musician, there's
a lot of integrated tools

00:01:01.200 --> 00:01:02.340
you can try.

00:01:02.340 --> 00:01:05.910
And we also have JavaScript
in other libraries

00:01:05.910 --> 00:01:07.423
to use for coders.

00:01:07.423 --> 00:01:09.090
And so I'm going to
pass it over to Adam

00:01:09.090 --> 00:01:11.670
who's going to talk about what
this means more in practice.

00:01:11.670 --> 00:01:12.420
ADAM ROBERTS: Yes.

00:01:12.420 --> 00:01:14.010
So let's look at
one concrete example

00:01:14.010 --> 00:01:16.020
of the type of work we do.

00:01:16.020 --> 00:01:17.730
This is a project that
just-- we released

00:01:17.730 --> 00:01:19.680
last week called groove.

00:01:19.680 --> 00:01:23.730
And it actually started with
just inviting and actually

00:01:23.730 --> 00:01:25.470
hiring drummers,
professional drummers

00:01:25.470 --> 00:01:26.920
to come into the office.

00:01:26.920 --> 00:01:30.210
We recorded them playing on
an electronic drum kit that

00:01:30.210 --> 00:01:34.500
allowed us to capture the
symbolic representation

00:01:34.500 --> 00:01:35.768
of their performances.

00:01:35.768 --> 00:01:37.560
And then we trained
machine learning models

00:01:37.560 --> 00:01:40.260
to do various tasks with
this data that we thought

00:01:40.260 --> 00:01:43.260
could be interesting to
use in a creative context.

00:01:43.260 --> 00:01:45.520
So I'll get into some of
those tasks in just a moment.

00:01:45.520 --> 00:01:50.248
But first, I just want to kind
of go through how we actually

00:01:50.248 --> 00:01:51.540
seed this stuff into the world.

00:01:51.540 --> 00:01:54.640
So first, we're
a research group.

00:01:54.640 --> 00:01:56.160
So we write an academic paper.

00:01:56.160 --> 00:01:58.480
We submit it to a machine
learning conference.

00:01:58.480 --> 00:02:01.670
We also release the data sets
so that other researchers can

00:02:01.670 --> 00:02:03.420
take that and either
reproduce our results

00:02:03.420 --> 00:02:06.580
or even hopefully expand
on it, improve upon it.

00:02:06.580 --> 00:02:09.539
But we also always put our
stuff into open source.

00:02:09.539 --> 00:02:12.750
So we want coders to have access
to this technology, as well.

00:02:12.750 --> 00:02:17.380
So we'll release a TensorFlow
implementation of our models.

00:02:17.380 --> 00:02:19.790
We, also, typically,
and in this case,

00:02:19.790 --> 00:02:22.660
we re-implemented
in TensorFlow JS.

00:02:22.660 --> 00:02:24.900
And this is just a
really useful technology

00:02:24.900 --> 00:02:27.990
for building interfaces
and applications

00:02:27.990 --> 00:02:29.755
on top of these methods.

00:02:29.755 --> 00:02:31.380
It makes it really
easy to do that when

00:02:31.380 --> 00:02:32.820
you put it into JavaScript.

00:02:32.820 --> 00:02:34.550
And then, lastly,
in this case, we

00:02:34.550 --> 00:02:36.840
put the data set into
TensorFlow data sets, which

00:02:36.840 --> 00:02:39.920
is just like a single line API
for accessing that data set

00:02:39.920 --> 00:02:42.600
so you can train new
models, or you can use it

00:02:42.600 --> 00:02:44.830
for whatever purpose you'd like.

00:02:44.830 --> 00:02:47.610
And then finally, we
typically build some sort

00:02:47.610 --> 00:02:49.483
of tools for either
musicians or artists,

00:02:49.483 --> 00:02:51.400
or whatever types of
creators we're targeting.

00:02:51.400 --> 00:02:54.090
And in this case, we took our
JavaScript implementation,

00:02:54.090 --> 00:02:57.780
and we built some plugins called
magenta/studio for Ableton

00:02:57.780 --> 00:02:58.330
live.

00:02:58.330 --> 00:03:00.240
So if you're not familiar
with Ableton Live,

00:03:00.240 --> 00:03:03.210
it's a professional
software package

00:03:03.210 --> 00:03:07.230
that people use to produce
music or to compose music.

00:03:07.230 --> 00:03:10.920
And these plugins
add new functionality

00:03:10.920 --> 00:03:14.320
to that library that
didn't exist before.

00:03:14.320 --> 00:03:15.750
So I want to focus
in on Drumify,

00:03:15.750 --> 00:03:19.290
which is one of the two plugins
that we built from the models

00:03:19.290 --> 00:03:22.320
we made with our group data set.

00:03:22.320 --> 00:03:24.840
And what this plug-in
does is it actually

00:03:24.840 --> 00:03:28.050
lets you take any
sort of rhythmic music

00:03:28.050 --> 00:03:30.480
and turn it into a drumbeat,
or to create a drumbeat that

00:03:30.480 --> 00:03:32.410
kind of accompanies it well.

00:03:32.410 --> 00:03:34.005
So imagine you're a producer.

00:03:34.005 --> 00:03:35.130
You're starting a new song.

00:03:35.130 --> 00:03:37.020
And you have a bass line
that you like a lot.

00:03:37.020 --> 00:03:39.780
But maybe you either don't
have access to a drum kit

00:03:39.780 --> 00:03:42.840
or you're not a talented or
skillful drummer yourself.

00:03:42.840 --> 00:03:46.710
You can use the Drumify
plug-in to take that bass line

00:03:46.710 --> 00:03:50.010
and create an accompanying
drumbeat to continue

00:03:50.010 --> 00:03:51.870
your compositional process.

00:03:51.870 --> 00:03:54.520
So let's just hear a quick
example of this in practice.

00:03:54.520 --> 00:03:58.140
So first, you're going to hear
a bass line that somebody made.

00:03:58.140 --> 00:04:01.322
[BASS LINE PLAYING]

00:04:01.322 --> 00:04:03.030
So now we're going to
take that and we're

00:04:03.030 --> 00:04:06.480
going to turn it into a drum
beat to accompany with Drumify.

00:04:10.194 --> 00:04:13.146
[BASS LINE WITH DRUMS PLAYING]

00:04:19.050 --> 00:04:21.018
[LAUGHTER]

00:04:21.018 --> 00:04:24.960
[APPLAUSE]

00:04:24.960 --> 00:04:28.200
So that was just using the bass
line, the onsets of the bass

00:04:28.200 --> 00:04:29.760
notes extracting that rhythm.

00:04:29.760 --> 00:04:31.645
And then with just
a few clicks, you

00:04:31.645 --> 00:04:33.520
can create a drum beat
to go along with that.

00:04:33.520 --> 00:04:35.145
So that's just one
example of the types

00:04:35.145 --> 00:04:36.510
of things we're working on.

00:04:36.510 --> 00:04:39.183
There's a lot more g.co/magenta.

00:04:39.183 --> 00:04:40.350
We have a lot more projects.

00:04:40.350 --> 00:04:42.870
Everything is free, open
source, easy to use.

00:04:42.870 --> 00:04:44.980
So please check that out
if you're interested.

00:04:44.980 --> 00:04:47.280
But now we want to transition
over and focus a bit

00:04:47.280 --> 00:04:49.030
on some of the creators
that have actually

00:04:49.030 --> 00:04:51.910
started using this technology
and their artistic practice.

00:04:51.910 --> 00:04:54.180
So, specifically, we have
two musicians here today

00:04:54.180 --> 00:04:56.050
that we're really excited about.

00:04:56.050 --> 00:04:57.960
The first is Claire Evans.

00:04:57.960 --> 00:05:00.720
So she's one third
of the band YACHT.

00:05:00.720 --> 00:05:03.540
She's also an artist in
her own right and a very

00:05:03.540 --> 00:05:05.020
accomplished author.

00:05:05.020 --> 00:05:06.870
Her book is called "Broad Band--

00:05:06.870 --> 00:05:10.230
The Untold Story of the
Women Who Made the Internet."

00:05:10.230 --> 00:05:12.350
And I highly
recommend this book.

00:05:12.350 --> 00:05:12.850
Yeah.

00:05:12.850 --> 00:05:14.640
[APPLAUSE]

00:05:14.640 --> 00:05:16.380
I consider it to be
required reading.

00:05:16.380 --> 00:05:17.740
So definitely check that out.

00:05:17.740 --> 00:05:19.140
But today she's not going
to be talking about that.

00:05:19.140 --> 00:05:21.570
She's going to be talking
about how her band has recently

00:05:21.570 --> 00:05:23.910
adopted some of these
machine learning technologies

00:05:23.910 --> 00:05:25.200
in their process.

00:05:25.200 --> 00:05:28.500
And we can see where that's
taking them as a band.

00:05:28.500 --> 00:05:30.300
So let's invite Claire
up to the stage.

00:05:33.220 --> 00:05:34.540
CLAIRE EVANS: Thanks, guys.

00:05:34.540 --> 00:05:35.040
Hi.

00:05:37.902 --> 00:05:39.540
Hi, everyone.

00:05:39.540 --> 00:05:40.870
Hi, I'm Claire, obviously.

00:05:40.870 --> 00:05:43.943
And I'm here as a
representative of my band.

00:05:43.943 --> 00:05:45.860
Yeah, we're playing
tonight on the main stage.

00:05:45.860 --> 00:05:47.510
So if you like this, if
you find this interesting,

00:05:47.510 --> 00:05:49.343
we're going to be playing
a lot of the songs

00:05:49.343 --> 00:05:51.700
that I'm talking about here
today on the main stage.

00:05:51.700 --> 00:05:54.100
So, for anyone who isn't
familiar with Yacht,

00:05:54.100 --> 00:05:56.690
I'm just going to start this
with a quick sort of preface

00:05:56.690 --> 00:05:57.940
so you get a sense of
who we are and where

00:05:57.940 --> 00:05:59.950
we're coming from when I
start talking about how

00:05:59.950 --> 00:06:01.390
we get into machine learning.

00:06:01.390 --> 00:06:05.770
So Yacht was founded in 2002
by my partner Jona Bechtolt,

00:06:05.770 --> 00:06:07.190
who's sitting over there.

00:06:07.190 --> 00:06:08.920
It was named after
this kind of decrepit

00:06:08.920 --> 00:06:12.010
looking sign that he saw on the
street in Portland, Oregon--

00:06:12.010 --> 00:06:15.130
YACHT, Young Americans
Challenging High Technology.

00:06:15.130 --> 00:06:18.910
We have no idea what
this business did.

00:06:18.910 --> 00:06:20.740
It is actually
frankly unGoogleable.

00:06:20.740 --> 00:06:21.692
Some things are.

00:06:21.692 --> 00:06:23.650
We tried to find out many
times over the years.

00:06:23.650 --> 00:06:26.720
But even though we've had a
lot of different incarnations

00:06:26.720 --> 00:06:29.290
of the band, it's been
17 years of making music,

00:06:29.290 --> 00:06:30.460
we've kept this acronym.

00:06:30.460 --> 00:06:32.418
Because it kind of
articulates something really

00:06:32.418 --> 00:06:34.090
core about who we
are, which is that we

00:06:34.090 --> 00:06:38.350
want to stay in constant engaged
dialogue with technology.

00:06:38.350 --> 00:06:41.350
I mean, obviously we're not
Luddites, because I'm here.

00:06:41.350 --> 00:06:44.170
And we're not particularly
adversarial, either.

00:06:44.170 --> 00:06:46.090
It's just challenging
in the sense

00:06:46.090 --> 00:06:48.520
that we want to remain
engaged, always.

00:06:48.520 --> 00:06:50.830
And we always want to be
aware of the kind of push

00:06:50.830 --> 00:06:55.360
and pull between using tools
and having our work be affected

00:06:55.360 --> 00:06:59.070
by the tools that we use,
the tools shaping our work.

00:06:59.070 --> 00:07:00.820
I want to say that I'm
not a coder at all.

00:07:00.820 --> 00:07:03.400
I only understood, like, 20% of
what Adam and Jesse were even

00:07:03.400 --> 00:07:04.930
saying just now, and we've
been working with them

00:07:04.930 --> 00:07:05.597
for three years.

00:07:05.597 --> 00:07:08.320
So our relationship
to technology

00:07:08.320 --> 00:07:10.630
in the context of
art making has always

00:07:10.630 --> 00:07:13.040
been from the
outside looking in.

00:07:13.040 --> 00:07:15.910
And we're interested in
tools, in getting access

00:07:15.910 --> 00:07:18.490
to interesting tools, and
then in finding our own novel

00:07:18.490 --> 00:07:22.330
ways of using those tools,
ways that are kind of sideways.

00:07:22.330 --> 00:07:24.190
We like to force
creative applications

00:07:24.190 --> 00:07:26.650
of noncreative technologist,
both consumer facing

00:07:26.650 --> 00:07:29.573
and nonconsumer-facing just to
see what we can do with them

00:07:29.573 --> 00:07:31.990
and see how they can be applied
to our historically pretty

00:07:31.990 --> 00:07:35.810
DIY, punk rock operation.

00:07:35.810 --> 00:07:38.170
If we have one basic
prime directive that

00:07:38.170 --> 00:07:41.230
dominates what we do, it's
to do as much as possible

00:07:41.230 --> 00:07:43.480
with as little as possible,
which is something

00:07:43.480 --> 00:07:46.270
that we picked up from reading
a lot of Buckminster Fuller

00:07:46.270 --> 00:07:49.342
and drawing the requisite
analogies to our own background

00:07:49.342 --> 00:07:51.550
in decentralized punk rock
communities in the Pacific

00:07:51.550 --> 00:07:52.710
Northwest.

00:07:52.710 --> 00:07:54.210
I'm going to tell
you about a couple

00:07:54.210 --> 00:07:56.585
of projects we've done to give
you a sense of who we are.

00:07:56.585 --> 00:07:59.050
So, a couple of years ago,
we revealed new album cover

00:07:59.050 --> 00:08:01.715
artwork exclusively via fax.

00:08:01.715 --> 00:08:04.090
We did that by building a web
application that identified

00:08:04.090 --> 00:08:06.640
nearby fax machines to
our fans, like at FedEx

00:08:06.640 --> 00:08:10.480
or UPS or their parents' offices
and sent the artwork directly

00:08:10.480 --> 00:08:12.860
to them with an
additioned cover letter.

00:08:12.860 --> 00:08:15.220
You know, a fax machine
transmits information

00:08:15.220 --> 00:08:17.320
through sound, which is
basically what music does.

00:08:17.320 --> 00:08:19.870
And we liked the
idea of activating

00:08:19.870 --> 00:08:22.390
a dormant technology,
or a latent technology,

00:08:22.390 --> 00:08:24.910
as a way of showing what
creative possibilities exist

00:08:24.910 --> 00:08:27.210
at the brink of obsolescence.

00:08:27.210 --> 00:08:29.860
And, on that step, we recently
wrapped a four year project

00:08:29.860 --> 00:08:32.080
to reactivate a really
dormant piece of technology

00:08:32.080 --> 00:08:36.580
in downtown Los Angeles,
a public artwork from 1975

00:08:36.580 --> 00:08:38.860
called the "Triforium",
which was originally

00:08:38.860 --> 00:08:42.130
designed as the world's first
polyphonoptic instrument,

00:08:42.130 --> 00:08:45.790
so an instrument that could
synchronize light and sound

00:08:45.790 --> 00:08:48.910
into an original, new art form.

00:08:48.910 --> 00:08:51.130
The computer system that
it was built with in 1975,

00:08:51.130 --> 00:08:52.780
obviously, was not up to snuff.

00:08:52.780 --> 00:08:55.390
So it had been broken
for a very long time.

00:08:55.390 --> 00:08:58.120
But we got some money, and we
got an interdisciplinary team

00:08:58.120 --> 00:09:01.360
together, and managed
to bring the lights back

00:09:01.360 --> 00:09:03.610
using a custom built
LED installation.

00:09:03.610 --> 00:09:06.280
And we managed to salvage
the original 8-bit paper tape

00:09:06.280 --> 00:09:08.110
code that ran the
original computer

00:09:08.110 --> 00:09:10.090
system so that it could
be responsive to live

00:09:10.090 --> 00:09:11.840
musical input once again.

00:09:11.840 --> 00:09:14.080
So, again, this
is a cohabitation

00:09:14.080 --> 00:09:16.060
of old and new technologies.

00:09:16.060 --> 00:09:18.430
And we love making use of
things, things that are just

00:09:18.430 --> 00:09:21.910
waiting to be re-explored
and reimagined, and again,

00:09:21.910 --> 00:09:25.227
with as little
resources as possible.

00:09:25.227 --> 00:09:26.810
We became really
interested in machine

00:09:26.810 --> 00:09:28.070
learning about four years ago.

00:09:28.070 --> 00:09:31.530
Because we felt like it was
maybe the next step for us.

00:09:31.530 --> 00:09:35.330
We were interested both in the
reflective qualities of machine

00:09:35.330 --> 00:09:36.830
learning, like the
way that it would

00:09:36.830 --> 00:09:40.015
help us to understand ourselves,
and the generative qualities

00:09:40.015 --> 00:09:42.140
of machine learning, the
ways that it would help us

00:09:42.140 --> 00:09:44.700
to make something entirely new.

00:09:44.700 --> 00:09:47.840
So I'm speaking to you now
from basically the tail end

00:09:47.840 --> 00:09:50.630
of a year long project of
trying to find a machine

00:09:50.630 --> 00:09:52.970
learning driven compositional
process that would work

00:09:52.970 --> 00:09:55.880
for our purposes, that would
allow us to make music that

00:09:55.880 --> 00:09:59.330
wasn't just passable as
human being generated music,

00:09:59.330 --> 00:10:02.390
but as something genuinely
interesting and meaningful

00:10:02.390 --> 00:10:06.498
and in line with our back
catalog of recordings.

00:10:06.498 --> 00:10:08.540
And so we didn't just want
to make a record using

00:10:08.540 --> 00:10:09.270
machine learning.

00:10:09.270 --> 00:10:11.880
We wanted to make a YACHT
record using machine learning.

00:10:11.880 --> 00:10:14.248
And that's a much
different proposition.

00:10:14.248 --> 00:10:15.540
I'll get into the nitty gritty.

00:10:15.540 --> 00:10:18.390
Because I figure
that's the theme.

00:10:18.390 --> 00:10:20.522
This is basically what we
used to make our record.

00:10:20.522 --> 00:10:22.730
We experimented with a bunch
of different strategies.

00:10:22.730 --> 00:10:24.897
But we found that the best
compositional tool for us

00:10:24.897 --> 00:10:28.080
was Magenta's MusicVAE model,
which is a latent space

00:10:28.080 --> 00:10:30.918
interpolation model that allowed
us to find, essentially, like--

00:10:30.918 --> 00:10:33.210
I mean, I know this isn't a
technical way of explaining

00:10:33.210 --> 00:10:35.070
it-- but it allowed
us to find melodies

00:10:35.070 --> 00:10:38.040
hidden in between songs
from our own back catalog.

00:10:38.040 --> 00:10:40.590
And this is what the user
facing side of that model

00:10:40.590 --> 00:10:41.970
looks like when
we were initially

00:10:41.970 --> 00:10:43.610
recording the record last May.

00:10:43.610 --> 00:10:45.930
It's a CO lab notebook,
so not exactly the kind

00:10:45.930 --> 00:10:49.200
of thing musicians are used
to bringing into the studio.

00:10:49.200 --> 00:10:51.120
And, unfortunately,
we started doing this

00:10:51.120 --> 00:10:53.640
before Magenta made user
friendly Ableton Live

00:10:53.640 --> 00:10:54.828
plugins for musicians.

00:10:54.828 --> 00:10:56.870
But you know, whatever--
it gives us street cred.

00:10:56.870 --> 00:10:57.620
So I'm OK with it.

00:11:00.117 --> 00:11:02.700
So in order to work this way,
to bring something like a CO lab

00:11:02.700 --> 00:11:05.200
notebook into the studio, we
have to do a lot of prep work.

00:11:05.200 --> 00:11:08.550
So, first, we manually annotated
our entire back catalog

00:11:08.550 --> 00:11:10.320
of music-- that's 82 songs--

00:11:10.320 --> 00:11:11.280
into MIDI.

00:11:11.280 --> 00:11:14.070
And then we broke out
all of the bass lines,

00:11:14.070 --> 00:11:18.690
vocal melodies, keyboard lines,
drum parts into four bar loops.

00:11:18.690 --> 00:11:22.050
Then we ran pairs of those loops
through the CO lab notebook

00:11:22.050 --> 00:11:23.730
at different
temperatures, sometimes

00:11:23.730 --> 00:11:25.680
dozens, if not
hundreds, of times

00:11:25.680 --> 00:11:27.780
in order to generate
this massive body

00:11:27.780 --> 00:11:29.520
of melodic information
that we could then

00:11:29.520 --> 00:11:34.350
use as sort of source material
for creating new songs.

00:11:34.350 --> 00:11:37.950
When we had this massive
amount of musical information,

00:11:37.950 --> 00:11:39.900
that's when the human
being process began.

00:11:39.900 --> 00:11:42.780
This is when we started
manually culling through all

00:11:42.780 --> 00:11:45.940
of this MIDI data trying to
find interesting moments,

00:11:45.940 --> 00:11:48.420
things that spoke to us,
things that felt interesting,

00:11:48.420 --> 00:11:50.590
things that we wanted
to explore further.

00:11:50.590 --> 00:11:52.890
As some of you might know,
using machine learning

00:11:52.890 --> 00:11:55.720
to make a song with structure,
with a beginning, middle,

00:11:55.720 --> 00:11:58.650
and end, with a verse, chorus,
verse, is a little bit, still,

00:11:58.650 --> 00:11:59.490
out of our reach.

00:11:59.490 --> 00:12:02.395
But that's a good thing because
the melody was the model's job,

00:12:02.395 --> 00:12:04.020
but the arrangement
and the performance

00:12:04.020 --> 00:12:05.962
was entirely our job.

00:12:05.962 --> 00:12:08.420
So to demonstrate what I mean
a little bit more concretely,

00:12:08.420 --> 00:12:10.712
let's focus on just a single
melody from a single song.

00:12:10.712 --> 00:12:12.590
So I'm going to play
you a melody that came

00:12:12.590 --> 00:12:14.660
straight out of
the MusicVAE model.

00:12:14.660 --> 00:12:17.810
It was one of several different
MIDI sequences generated

00:12:17.810 --> 00:12:20.413
by an interpolation between
two different YACHT songs,

00:12:20.413 --> 00:12:21.830
one called "Hologram"
and one that

00:12:21.830 --> 00:12:23.840
has a swear word in the title,
so I feel like I probably

00:12:23.840 --> 00:12:25.060
shouldn't say it out loud.

00:12:25.060 --> 00:12:26.000
AUDIENCE: Say it!

00:12:26.000 --> 00:12:26.470
[ONE NOTE MELODY WITH METRONOME
 PLAYING]

00:12:26.470 --> 00:12:28.512
CLAIRE EVANS: "I Want to
Fuck You Till I'm Dead."

00:12:28.512 --> 00:12:30.114
[LAUGHS]

00:12:50.620 --> 00:12:51.120
OK.

00:12:51.120 --> 00:12:52.800
So this particularly
melody for us

00:12:52.800 --> 00:12:55.080
was exceptionally
aesthetically interesting.

00:12:55.080 --> 00:12:58.290
But like every melody generated
by the MusicVAE model,

00:12:58.290 --> 00:13:00.780
it's just an endless
sequence of notes that

00:13:00.780 --> 00:13:02.400
goes on and on until it stops.

00:13:02.400 --> 00:13:05.232
It's not exactly pop material.

00:13:05.232 --> 00:13:06.690
So this is where
the rules come in.

00:13:06.690 --> 00:13:08.065
And I don't mean
technical rules.

00:13:08.065 --> 00:13:11.940
I mean human rules, working
rules for our specific process.

00:13:11.940 --> 00:13:14.380
We have always thrived, like
many artists, I believe,

00:13:14.380 --> 00:13:15.672
under self-imposed constraints.

00:13:15.672 --> 00:13:17.338
Because when you sit
down to write a pop

00:13:17.338 --> 00:13:19.720
song about anything in the
world, it's overwhelming.

00:13:19.720 --> 00:13:21.570
But if you have some
boundaries in place,

00:13:21.570 --> 00:13:23.890
you can begin to think
about it more concretely.

00:13:23.890 --> 00:13:26.525
So for us we decided that
every single song that we're

00:13:26.525 --> 00:13:27.900
going to create
with this process

00:13:27.900 --> 00:13:30.180
had to be interpolated
from existing

00:13:30.180 --> 00:13:32.370
melodies from our back catalog.

00:13:32.370 --> 00:13:34.140
We hoped that this
would result in songs

00:13:34.140 --> 00:13:36.510
that had that kind
of indefinable YACHT

00:13:36.510 --> 00:13:38.550
feeling, which we don't
know how to quantify

00:13:38.550 --> 00:13:40.217
and I don't think the
model can, either.

00:13:40.217 --> 00:13:43.200
But that's what we decided
would be our parameters.

00:13:43.200 --> 00:13:46.440
We decided also that we
could not add any notes.

00:13:46.440 --> 00:13:48.510
We could not add any harmonies.

00:13:48.510 --> 00:13:51.180
We could not jam or
improvise or otherwise

00:13:51.180 --> 00:13:54.520
interpret or, essentially,
be creative in any way.

00:13:54.520 --> 00:13:58.230
There was no additive
alteration, only subtractive

00:13:58.230 --> 00:13:59.600
or transpositional changes.

00:13:59.600 --> 00:14:02.640
So we could assign any
melody to any instrument.

00:14:02.640 --> 00:14:05.070
So that melody we just heard
could have been a keyboard

00:14:05.070 --> 00:14:06.990
line, could've been a bass line,
could've been a guitar line,

00:14:06.990 --> 00:14:08.198
could've been a vocal melody.

00:14:08.198 --> 00:14:09.750
That was our decision to make.

00:14:09.750 --> 00:14:12.490
We could transpose melodies
to our working key.

00:14:12.490 --> 00:14:14.820
And we could structure
and cut up and collage

00:14:14.820 --> 00:14:17.220
as much as we wanted.

00:14:17.220 --> 00:14:19.650
Now we'll talk about lyrics,
another important element

00:14:19.650 --> 00:14:20.820
of any song.

00:14:20.820 --> 00:14:22.620
So for this project,
we collaborated

00:14:22.620 --> 00:14:25.530
with a different creative
technologist, Ross Goodwin,

00:14:25.530 --> 00:14:27.780
who was a free agent when
we started working with him.

00:14:27.780 --> 00:14:29.350
He's now with Google's Artists
and Machine Intelligence

00:14:29.350 --> 00:14:32.220
Group, which is something
that happened kind of a lot

00:14:32.220 --> 00:14:34.240
during our process.

00:14:34.240 --> 00:14:36.760
We worked with Ross to
create a lyrics model

00:14:36.760 --> 00:14:38.850
with sort of the same
ethos as our melodic model.

00:14:38.850 --> 00:14:40.830
So we wanted to
have it be kind of

00:14:40.830 --> 00:14:43.050
reflective of our
own inspiration,

00:14:43.050 --> 00:14:45.800
our own background, our own
history, our own back catalog.

00:14:45.800 --> 00:14:47.610
So the model that
we built with Ross

00:14:47.610 --> 00:14:51.460
was trained on a corpus
of 20 megabytes of text.

00:14:51.460 --> 00:14:54.750
So that's about 500,000 pages
or approximately two million

00:14:54.750 --> 00:14:55.260
words.

00:14:55.260 --> 00:14:57.150
And these are all
from bands that we

00:14:57.150 --> 00:15:00.360
considered to be our influences,
music we grew up listening to,

00:15:00.360 --> 00:15:03.630
music our parents listened to,
our own music, our friends,

00:15:03.630 --> 00:15:05.740
and collaborators, and peers.

00:15:05.740 --> 00:15:08.220
We saw this as an opportunity
to kind of teach the machine

00:15:08.220 --> 00:15:11.280
our values, our history,
our community, and where

00:15:11.280 --> 00:15:13.450
we come from as artists.

00:15:13.450 --> 00:15:14.980
The end result was this.

00:15:14.980 --> 00:15:17.410
So this is one
instance, one block

00:15:17.410 --> 00:15:20.275
of output from the lyrics model
that Ross created with us.

00:15:20.275 --> 00:15:22.150
Which we printed out on
a single sheet of dot

00:15:22.150 --> 00:15:23.080
matrix printer paper.

00:15:23.080 --> 00:15:25.420
Because, did you know, they
still make dot matrix printers,

00:15:25.420 --> 00:15:26.770
and you can buy them on Amazon.

00:15:26.770 --> 00:15:29.540
And we wanted to visualize it
as something really physical.

00:15:29.540 --> 00:15:31.233
So we had this
massive block of text,

00:15:31.233 --> 00:15:33.400
one continuous sheet that
we brought into the studio

00:15:33.400 --> 00:15:33.900
with us.

00:15:33.900 --> 00:15:35.890
And I literally sat down
on the studio floor,

00:15:35.890 --> 00:15:38.080
highlighting
interesting passages.

00:15:38.080 --> 00:15:39.580
It's interesting
because it contains

00:15:39.580 --> 00:15:42.120
a range of low to high
temperature material.

00:15:42.120 --> 00:15:45.250
So the low temperature stuff,
because it's taking less risks,

00:15:45.250 --> 00:15:48.085
is much more repetitive,
much more simplistic.

00:15:48.085 --> 00:15:49.710
It's kind of a punk
rock lyrics engine.

00:15:49.710 --> 00:15:52.450
It taps into the more
elemental things in songs.

00:15:52.450 --> 00:15:54.700
So there's entire pages
and pages and pages

00:15:54.700 --> 00:15:57.070
of repetitive phrases,
like, I want your brain,

00:15:57.070 --> 00:15:59.748
or I want to rock,
or I love you.

00:15:59.748 --> 00:16:01.540
That's the really low
temperature material.

00:16:01.540 --> 00:16:03.220
And then the high
temperature material

00:16:03.220 --> 00:16:05.890
is full of nonsensical
run on sentences

00:16:05.890 --> 00:16:09.680
and lots of really weird proper
nouns and names of things.

00:16:09.680 --> 00:16:11.350
And so in order
to make songs that

00:16:11.350 --> 00:16:13.540
had a range of emotional
sort of feeling,

00:16:13.540 --> 00:16:15.040
we combined a lot
of low temperature

00:16:15.040 --> 00:16:18.280
and high temperature
material into the same songs.

00:16:18.280 --> 00:16:20.890
And as with the melodies, we
didn't take anything as is.

00:16:20.890 --> 00:16:23.680
We really went through
manually and combed through

00:16:23.680 --> 00:16:26.680
and looked for exceptionally
interesting phrases, or images,

00:16:26.680 --> 00:16:28.430
or passages, or things
that spoke to us

00:16:28.430 --> 00:16:30.430
and felt like they were
meaningful to who we are

00:16:30.430 --> 00:16:32.380
and where we're coming from.

00:16:32.380 --> 00:16:35.410
The biggest influences on our
working method with the text

00:16:35.410 --> 00:16:38.220
were really kind of like low
tech, anti-technological,

00:16:38.220 --> 00:16:38.720
really.

00:16:38.720 --> 00:16:41.260
I mean, we were looking at
William S Burroughs cut up

00:16:41.260 --> 00:16:44.020
writing methods
and the Dadaists.

00:16:44.020 --> 00:16:46.900
High tech, low tech is
kind of our operandus.

00:16:46.900 --> 00:16:49.330
So in order to
actually make songs

00:16:49.330 --> 00:16:52.270
with this giant block of
text and this giant pile

00:16:52.270 --> 00:16:55.990
of melodies, we actually had to
take the interesting passages

00:16:55.990 --> 00:16:57.790
and then place them
on top of the melodies

00:16:57.790 --> 00:17:00.610
that we decided would be the
interesting vocal melodies.

00:17:00.610 --> 00:17:04.000
The problem is the
melodies generated by VAE

00:17:04.000 --> 00:17:06.520
don't have any relationship
to the human body, least

00:17:06.520 --> 00:17:09.369
of all to our human bodies,
or to our competencies

00:17:09.369 --> 00:17:10.535
as performers and singers.

00:17:10.535 --> 00:17:12.160
And they certainly
have no relationship

00:17:12.160 --> 00:17:14.839
to the internal rhythms
of the English language.

00:17:14.839 --> 00:17:17.550
So a lot of the time we had
to break the lyrics on top

00:17:17.550 --> 00:17:20.050
of the melodies in order to
sort of force them into working.

00:17:20.050 --> 00:17:22.599
And that meant we had to pull
apart syllables and pronounce

00:17:22.599 --> 00:17:24.460
things in really weird
ways and do things

00:17:24.460 --> 00:17:26.702
that were deeply unintuitive
and which will certainly

00:17:26.702 --> 00:17:28.660
lead to a lot of people
listening to this music

00:17:28.660 --> 00:17:30.670
and mishearing
lyrics constantly.

00:17:30.670 --> 00:17:33.082
It's like [INAUDIBLE] maker.

00:17:33.082 --> 00:17:34.540
So let's take a
closer look at some

00:17:34.540 --> 00:17:36.410
of the lyrics we
decided to work with.

00:17:36.410 --> 00:17:38.470
Here's a passage from a song.

00:17:38.470 --> 00:17:40.330
"I want your phone to my brain.

00:17:40.330 --> 00:17:41.770
I want you to call my name.

00:17:41.770 --> 00:17:43.420
I want you to do it, too.

00:17:43.420 --> 00:17:44.650
Oh, won't you come?

00:17:44.650 --> 00:17:45.550
Won't you come?

00:17:45.550 --> 00:17:48.400
Won't you work on my
head, be my number nine?

00:17:48.400 --> 00:17:51.490
To be alive, to be
with you, like a weed.

00:17:51.490 --> 00:17:55.010
I can feel it in my
head like a dog in bed."

00:17:55.010 --> 00:17:55.510
I know.

00:17:55.510 --> 00:17:57.650
[APPLAUSE]

00:17:57.650 --> 00:18:00.490
So speaking as someone who
normally writes songs that

00:18:00.490 --> 00:18:03.280
have a relationship
to meaning or cadence

00:18:03.280 --> 00:18:07.480
or are personal in some way,
singing these kinds of lyrics

00:18:07.480 --> 00:18:10.420
really forced me to step
outside of my embodied habits

00:18:10.420 --> 00:18:14.200
and develop a relationship
to words first as sounds.

00:18:14.200 --> 00:18:16.750
And then to grow to love and
appreciate the meaning that

00:18:16.750 --> 00:18:18.690
comes after sound.

00:18:18.690 --> 00:18:20.480
It's pretty liberating.

00:18:20.480 --> 00:18:22.480
But the lyrics also contain
these really strong,

00:18:22.480 --> 00:18:24.730
strange images that I
would never have written.

00:18:24.730 --> 00:18:27.160
Like, "I can feel it in my
head like a dog in bed."

00:18:27.160 --> 00:18:29.080
I mean, what a phrase.

00:18:29.080 --> 00:18:31.570
It has the form of
idiomatic English.

00:18:31.570 --> 00:18:34.180
But the meaning is
completely sideways.

00:18:34.180 --> 00:18:36.670
And yet, it also still
kind of means something.

00:18:36.670 --> 00:18:38.080
Because I think
we can all easily

00:18:38.080 --> 00:18:41.590
imagine a feeling that's
as warm and willful

00:18:41.590 --> 00:18:44.810
and present as a dog that's
sneaking into bed at night.

00:18:44.810 --> 00:18:46.810
And that's the magic of
working with this stuff.

00:18:46.810 --> 00:18:49.600
It really opens you up to new
ways of thinking about language

00:18:49.600 --> 00:18:52.000
or thinking about music and
of thinking of the interplay

00:18:52.000 --> 00:18:54.240
between those two things.

00:18:54.240 --> 00:18:54.740
OK.

00:18:54.740 --> 00:18:56.805
So let's hear how these
lyrics fit on that melody

00:18:56.805 --> 00:18:59.180
that I just played you, which
we determined at the outset

00:18:59.180 --> 00:19:03.015
would be a good vocal melody.

00:19:03.015 --> 00:19:05.370
[METRONOME AND MUSIC PLAYING]

00:19:28.302 --> 00:19:28.802
[LAUGHS]

00:19:28.802 --> 00:19:29.302
[APPLAUSE]

00:19:29.302 --> 00:19:32.280
Thanks.

00:19:32.280 --> 00:19:34.500
So one of the most interesting
and challenging things

00:19:34.500 --> 00:19:37.140
of working in this
way is actually

00:19:37.140 --> 00:19:39.330
performing the
generative material.

00:19:39.330 --> 00:19:42.567
Like I said earlier, it's often
far beyond our competencies.

00:19:42.567 --> 00:19:44.400
And sometimes the things
that sound simple--

00:19:44.400 --> 00:19:47.730
I mean, this sounds simple,
but it's like sideways

00:19:47.730 --> 00:19:51.078
from the embodied patterns of
play of singing and performance

00:19:51.078 --> 00:19:52.120
that we're accustomed to.

00:19:52.120 --> 00:19:53.578
And I can't tell
you how many times

00:19:53.578 --> 00:19:56.580
we are in the studio just trying
to nail, like, some seemingly

00:19:56.580 --> 00:19:57.563
simple guitar line.

00:19:57.563 --> 00:19:59.730
But just because it was
slightly different than what

00:19:59.730 --> 00:20:02.530
we were accustomed to doing,
it was impossible to do.

00:20:02.530 --> 00:20:03.792
And that happened a lot.

00:20:03.792 --> 00:20:05.250
And it was kind of
brain breakingly

00:20:05.250 --> 00:20:06.480
difficult in many moments.

00:20:06.480 --> 00:20:08.730
But at the same time,
it often forcibly

00:20:08.730 --> 00:20:11.490
pushed us outside of our
comfort zone, pushed us

00:20:11.490 --> 00:20:14.460
outside of the patterns
that we had fallen into,

00:20:14.460 --> 00:20:16.530
and often patterns we
hadn't even perceived

00:20:16.530 --> 00:20:17.620
were there to begin with.

00:20:17.620 --> 00:20:19.120
And it forced us
to play differently

00:20:19.120 --> 00:20:21.620
and think differently
about how we work.

00:20:21.620 --> 00:20:22.120
OK.

00:20:22.120 --> 00:20:24.840
Finally, I want to play you the
first minute of the final song

00:20:24.840 --> 00:20:26.080
with everything together.

00:20:26.080 --> 00:20:28.320
So you'll hear the
first chorus, which

00:20:28.320 --> 00:20:31.065
has its own, like,
amazing, made up idiom.

00:20:31.065 --> 00:20:33.190
And then you'll hear the
verse I played you before.

00:20:33.190 --> 00:20:35.240
So again, melodically,
everything you hear

00:20:35.240 --> 00:20:37.995
was generated by
the MusicVAE model.

00:20:37.995 --> 00:20:39.870
But the performance,
arrangement, production,

00:20:39.870 --> 00:20:42.750
structure, everything
else is ours.

00:20:42.750 --> 00:20:46.650
And this is kind of what we see
as a collaborative strategy.

00:20:46.650 --> 00:20:49.260
It's not so much about,
like, as an artist

00:20:49.260 --> 00:20:51.040
being replaced by
machine learning,

00:20:51.040 --> 00:20:53.070
but rather being
given the opportunity

00:20:53.070 --> 00:20:54.985
to focus our energies
in different directions

00:20:54.985 --> 00:20:56.860
in different places than
we're accustomed to.

00:20:56.860 --> 00:20:59.060
It's not about revoking
control at all.

00:20:59.060 --> 00:21:00.150
It's not about letting go.

00:21:00.150 --> 00:21:05.540
It's about holding on and
letting the process change you.

00:21:05.540 --> 00:21:06.920
You could pump it, maybe.

00:21:06.920 --> 00:21:10.856
[MUSIC PLAYING]

00:22:16.642 --> 00:22:17.690
It's like a real song!

00:22:21.300 --> 00:22:23.142
So obviously, this
is just one way

00:22:23.142 --> 00:22:25.100
of working with machine
learning to make music.

00:22:25.100 --> 00:22:28.032
And it's not even, like, really
the right way, I don't think.

00:22:28.032 --> 00:22:29.240
There's countless approaches.

00:22:29.240 --> 00:22:30.920
And many of them are going
to be far more technical.

00:22:30.920 --> 00:22:32.990
Again, we are getting
in where we fit in.

00:22:32.990 --> 00:22:35.120
We are engaging at the
level that we know how.

00:22:35.120 --> 00:22:37.658
But beyond the challenges
that it faces, like,

00:22:37.658 --> 00:22:39.950
that it brings to workflow,
because it's definitely not

00:22:39.950 --> 00:22:44.150
intuitive or fun to pull up the
CO lab notebook in the browser,

00:22:44.150 --> 00:22:45.740
in the studio,
the challenges are

00:22:45.740 --> 00:22:47.120
really satisfying and exciting.

00:22:47.120 --> 00:22:48.745
Because they're the
kinds of challenges

00:22:48.745 --> 00:22:51.630
that make you stop and consider
what you're actually doing.

00:22:51.630 --> 00:22:54.710
And, for us, the process has
been infuriating at times,

00:22:54.710 --> 00:22:57.500
but ultimately, really
deeply gratifying.

00:22:57.500 --> 00:22:59.027
And the best way
I can describe it

00:22:59.027 --> 00:23:00.860
is that it feels like
you're doing a puzzle.

00:23:00.860 --> 00:23:04.340
And then when you're done,
the picture on the puzzle

00:23:04.340 --> 00:23:06.060
is not what's on the box.

00:23:06.060 --> 00:23:06.770
But who cares?

00:23:06.770 --> 00:23:08.780
Because who cares
what's on the box?

00:23:08.780 --> 00:23:09.390
OK.

00:23:09.390 --> 00:23:10.120
There's more to talk about.

00:23:10.120 --> 00:23:11.537
We can talk about
it in the panel.

00:23:11.537 --> 00:23:12.048
Thank you.

00:23:12.048 --> 00:23:15.638
[APPLAUSE]

00:23:15.638 --> 00:23:16.805
JESSE ENGEL: That's awesome.

00:23:16.805 --> 00:23:18.230
Cool.

00:23:18.230 --> 00:23:18.752
Yeah.

00:23:18.752 --> 00:23:19.900
You can sit here.

00:23:19.900 --> 00:23:20.400
Yeah.

00:23:20.400 --> 00:23:24.030
So we're really grateful
to YACHT for coming to us

00:23:24.030 --> 00:23:25.840
so early in this process.

00:23:25.840 --> 00:23:28.470
Because you can see
that a lot of our tools,

00:23:28.470 --> 00:23:31.920
through your story, were just
sort of in their early stages.

00:23:31.920 --> 00:23:33.900
And we got a lot of
really useful feedback

00:23:33.900 --> 00:23:36.330
about how an artist
would actually

00:23:36.330 --> 00:23:38.720
want to interact with
different types of things

00:23:38.720 --> 00:23:39.970
in machine learning.

00:23:39.970 --> 00:23:42.420
And this next project
we're going to introduce I

00:23:42.420 --> 00:23:46.830
feel sort of shows how we've
come along this process

00:23:46.830 --> 00:23:49.560
to now make these tools
available to the point

00:23:49.560 --> 00:23:51.930
where someone can just
do a project in a shorter

00:23:51.930 --> 00:23:53.660
amount of time.

00:23:53.660 --> 00:23:56.370
And so we've done a project
with the Flaming Lips

00:23:56.370 --> 00:23:57.957
that's very specifically for--

00:23:57.957 --> 00:23:58.540
AUDIENCE: Woo!

00:23:58.540 --> 00:24:02.160
ADAM ROBERTS: --hey-- that's
very specifically for I/O.

00:24:02.160 --> 00:24:06.300
And so we're going to give
you a sneak peak of things

00:24:06.300 --> 00:24:09.325
that are going to be happening
for the concert tonight.

00:24:09.325 --> 00:24:11.700
And then we're going to have
a nice discussion panel here

00:24:11.700 --> 00:24:13.300
talking about all these things.

00:24:13.300 --> 00:24:14.552
So we'll play a video.

00:24:14.552 --> 00:24:15.510
CLAIRE EVANS: One more.

00:24:15.510 --> 00:24:16.468
ADAM ROBERTS: One more.

00:24:16.468 --> 00:24:18.130
[VIDEO PLAYBACK]

00:24:18.130 --> 00:24:20.110
- Any time that the
Flaming Lips have

00:24:20.110 --> 00:24:22.780
stumbled upon a new
little instrument,

00:24:22.780 --> 00:24:24.585
it's changed what we created.

00:24:29.900 --> 00:24:31.580
- The goal of the
Magenta team is

00:24:31.580 --> 00:24:35.000
to really explore the role of
machine learning in creativity,

00:24:35.000 --> 00:24:37.640
in the creative process to
enable people to express

00:24:37.640 --> 00:24:38.870
themselves in new ways.

00:24:38.870 --> 00:24:41.630
Piano Genie is the great
work of an intern we had,

00:24:41.630 --> 00:24:42.620
Chris Donahue.

00:24:42.620 --> 00:24:46.370
He designed an algorithm to
be able to take piano playing

00:24:46.370 --> 00:24:49.490
and try to reproduce the piano
playing only hitting a couple

00:24:49.490 --> 00:24:50.822
buttons on a controller.

00:24:50.822 --> 00:24:52.280
And it naturally
comes out sounding

00:24:52.280 --> 00:24:54.230
a lot more like a
professional piano player.

00:24:54.230 --> 00:24:55.190
- One of the things
that we really

00:24:55.190 --> 00:24:57.607
focus on that's very different
from a lot of other machine

00:24:57.607 --> 00:25:00.320
learning projects is
how can we let people

00:25:00.320 --> 00:25:01.863
manipulate these algorithms.

00:25:01.863 --> 00:25:04.280
- It's been really exciting
collaborating with the Flaming

00:25:04.280 --> 00:25:04.780
Lips.

00:25:04.780 --> 00:25:07.063
Because they're so creative
in their approach to music

00:25:07.063 --> 00:25:09.230
that we just sort of showed
them everything we have.

00:25:09.230 --> 00:25:11.210
We're hoping to create
a new experience where

00:25:11.210 --> 00:25:14.000
the audience can co-create music
with the band in real time,

00:25:14.000 --> 00:25:15.170
using Piano Genie.

00:25:15.170 --> 00:25:17.990
- So we made an intelligent
musical instrument and melody

00:25:17.990 --> 00:25:20.840
creator out of
fruit with Google.

00:25:20.840 --> 00:25:23.225
[MUSIC PLAYING]

00:25:26.087 --> 00:25:27.530
[APPLAUSE]

00:25:27.530 --> 00:25:29.250
When I played the
fruit, I'm touching it.

00:25:29.250 --> 00:25:31.792
And I didn't know exactly what
it was going to do every time.

00:25:31.792 --> 00:25:35.180
- Each one is
announcing what it is.

00:25:35.180 --> 00:25:36.170
- Banana.

00:25:36.170 --> 00:25:37.250
- Green apple.

00:25:37.250 --> 00:25:38.570
- Maybe it's in the key of G.

00:25:38.570 --> 00:25:39.653
[STRANGE ELECTRONIC VOICE]

00:25:39.653 --> 00:25:41.030
- So we worked with Google AI.

00:25:41.030 --> 00:25:44.210
And they sent us the
software called Piano Genie.

00:25:44.210 --> 00:25:46.780
You hit a note, and it
automatically plays music.

00:25:46.780 --> 00:25:49.870
[MUSIC PLAYING]

00:25:49.870 --> 00:25:51.410
- And the more that
we play with it,

00:25:51.410 --> 00:25:53.917
the more it understands
what it's playing against

00:25:53.917 --> 00:25:55.000
and who it's playing with.

00:25:58.000 --> 00:26:00.210
- So you play a different
rhythm or a different note,

00:26:00.210 --> 00:26:01.400
and this goes on and on.

00:26:01.400 --> 00:26:04.960
So it actually wrote a melody
that we would not have written.

00:26:04.960 --> 00:26:07.198
- Instead of the machine
doing it for you,

00:26:07.198 --> 00:26:09.490
you're kind of encouraging
the machine to do something.

00:26:09.490 --> 00:26:10.735
It's kind of cool.

00:26:10.735 --> 00:26:15.960
1, 2, 3-- if you're
a banana, that's

00:26:15.960 --> 00:26:17.866
pretty good for a banana.

00:26:17.866 --> 00:26:20.306
[LAUGHTER]

00:26:25.091 --> 00:26:25.674
[END PLAYBACK]

00:26:25.674 --> 00:26:29.090
[APPLAUSE]

00:26:34.572 --> 00:26:35.280
JESSE ENGEL: So--

00:26:35.280 --> 00:26:35.600
WAYNE COYNE: All right.

00:26:35.600 --> 00:26:36.650
Am I-- am I working?

00:26:36.650 --> 00:26:36.920
There you go.

00:26:36.920 --> 00:26:37.150
Yeah.

00:26:37.150 --> 00:26:37.440
Yeah.

00:26:37.440 --> 00:26:38.100
JESSE ENGEL: Yeah,
there you are.

00:26:38.100 --> 00:26:39.180
WAYNE COYNE: All right.

00:26:39.180 --> 00:26:40.200
JESSE ENGEL: So
this is Wayne Coyne.

00:26:40.200 --> 00:26:40.800
WAYNE COYNE: Hello, everybody.

00:26:40.800 --> 00:26:41.300
[APPLAUSE]

00:26:41.300 --> 00:26:43.830
JESSE ENGEL: Yes, hello.

00:26:43.830 --> 00:26:47.400
So we wanted to start this
discussion because there's

00:26:47.400 --> 00:26:50.353
obviously a lot of hype
about machine learning

00:26:50.353 --> 00:26:51.520
and artificial intelligence.

00:26:51.520 --> 00:26:53.312
And when you come to
one of these projects,

00:26:53.312 --> 00:26:55.080
there can be a lot
of preconceptions

00:26:55.080 --> 00:26:57.510
about what it is that's
going to be like to interact

00:26:57.510 --> 00:26:58.260
with these things.

00:26:58.260 --> 00:27:00.180
So, maybe, Claire, I
want to start with you,

00:27:00.180 --> 00:27:04.260
just talking about how
those preconceptions met

00:27:04.260 --> 00:27:06.732
or what was different
than what you expected.

00:27:06.732 --> 00:27:07.440
CLAIRE EVANS: Oh.

00:27:07.440 --> 00:27:09.450
Yeah, I mean, I'm not ashamed
to say that, at the outset,

00:27:09.450 --> 00:27:11.700
we thought we would just
push a button and make songs.

00:27:11.700 --> 00:27:13.765
We thought that's where
we were at in terms

00:27:13.765 --> 00:27:15.390
of the technological
development of AI.

00:27:15.390 --> 00:27:17.850
I mean, that's maybe what
the hype of the mainstream

00:27:17.850 --> 00:27:18.720
makes us believe.

00:27:18.720 --> 00:27:20.280
That it's going to come
for our jobs in this way

00:27:20.280 --> 00:27:21.072
that's so visceral.

00:27:21.072 --> 00:27:23.790
But we really thought we could
sort of put all of our songs

00:27:23.790 --> 00:27:26.552
into a machine and then it
would give us a new YACHT song.

00:27:26.552 --> 00:27:28.260
And we found out very
quickly, of course,

00:27:28.260 --> 00:27:30.040
that that's not at
all where we're at,

00:27:30.040 --> 00:27:30.780
which was really exciting.

00:27:30.780 --> 00:27:33.270
Because it meant that we got
to be the humans in the loop.

00:27:33.270 --> 00:27:35.437
And we got to have way more
control over the process

00:27:35.437 --> 00:27:37.170
than we initially
believed we would.

00:27:37.170 --> 00:27:38.712
I mean, initially
we thought it would

00:27:38.712 --> 00:27:41.460
be about committing to
whatever the machine made

00:27:41.460 --> 00:27:43.050
and then we would have to play
it and perform it and make it

00:27:43.050 --> 00:27:43.550
our own.

00:27:43.550 --> 00:27:47.173
But actually, we got to kind
of co-create with the models.

00:27:47.173 --> 00:27:48.840
And we had to take a
much stronger hand.

00:27:48.840 --> 00:27:50.840
And we had to come up
with rule sets and systems

00:27:50.840 --> 00:27:52.890
and processes that
were uniquely our own.

00:27:52.890 --> 00:27:54.762
So that, even if
you gave the same--

00:27:54.762 --> 00:27:56.970
we gave other musicians or
really anyone in this room

00:27:56.970 --> 00:27:59.970
the exact same lyrics
generated machine output

00:27:59.970 --> 00:28:02.682
and the same notation data,
we'd all make different records.

00:28:02.682 --> 00:28:04.890
Because it's really about
the personal interpretation

00:28:04.890 --> 00:28:06.973
of what to do with this,
all this source material.

00:28:06.973 --> 00:28:12.450
So I was pleasantly surprised
at the lack of sophistication,

00:28:12.450 --> 00:28:13.500
I suppose.

00:28:13.500 --> 00:28:14.020
Yeah.

00:28:14.020 --> 00:28:14.470
JESSE ENGEL: Yeah.

00:28:14.470 --> 00:28:16.330
Wayne, does that vibe
with your experience?

00:28:19.980 --> 00:28:22.620
WAYNE COYNE: I would
say our thing had

00:28:22.620 --> 00:28:25.810
so much momentum going.

00:28:25.810 --> 00:28:26.950
And I always have--

00:28:26.950 --> 00:28:28.960
I feel, like, I have
too many questions.

00:28:28.960 --> 00:28:29.550
You know?

00:28:29.550 --> 00:28:31.850
I have questions
even to ask you.

00:28:31.850 --> 00:28:33.200
What was the word?

00:28:33.200 --> 00:28:33.700
OK.

00:28:33.700 --> 00:28:38.160
So there's the light thing
that's in Los Angeles, right?

00:28:38.160 --> 00:28:39.160
CLAIRE EVANS: Triforium.

00:28:39.160 --> 00:28:39.570
Yeah.

00:28:39.570 --> 00:28:40.320
WAYNE COYNE: Yeah.

00:28:40.320 --> 00:28:41.933
So what's the word
it says it's--

00:28:41.933 --> 00:28:43.100
CLAIRE EVANS: Polyphonoptic.

00:28:43.100 --> 00:28:43.270
I know.

00:28:43.270 --> 00:28:43.820
I figured you would like that.

00:28:43.820 --> 00:28:44.630
WAYNE COYNE: What is that?

00:28:44.630 --> 00:28:45.470
I mean, is it even a word?

00:28:45.470 --> 00:28:46.330
CLAIRE EVANS: It's,
like, phonoptic.

00:28:46.330 --> 00:28:46.700
It's not a real--

00:28:46.700 --> 00:28:47.480
I mean, it's a word.

00:28:47.480 --> 00:28:47.910
WAYNE COYNE: OK.

00:28:47.910 --> 00:28:48.900
CLAIRE EVANS: So
any word is a word.

00:28:48.900 --> 00:28:50.180
WAYNE COYNE: But
it's a made up word?

00:28:50.180 --> 00:28:52.400
CLAIRE EVANS: It's-- every
word is a made up word.

00:28:52.400 --> 00:28:53.850
WAYNE COYNE: I was, like, what?

00:28:53.850 --> 00:28:54.350
[LAUGHTER]

00:28:54.350 --> 00:28:57.130
Well, that's true.

00:28:57.130 --> 00:29:05.490
So, I mean, from my experience,
I didn't really have any idea--

00:29:05.490 --> 00:29:08.238
how-- really what
we were going to do.

00:29:08.238 --> 00:29:09.780
I think we were
deciding what we were

00:29:09.780 --> 00:29:13.110
going to do based on what
we did five minutes ago.

00:29:13.110 --> 00:29:14.820
I mean, everything
that we've done

00:29:14.820 --> 00:29:20.010
starts to accelerate with
ideas and energy and momentum.

00:29:20.010 --> 00:29:23.550
And I think that's why you
guys wanted the Flaming Lips,

00:29:23.550 --> 00:29:24.050
you know?

00:29:24.050 --> 00:29:26.730
Because, like, they're going
to do something, you know?

00:29:26.730 --> 00:29:33.690
And you talked about these
self-imposed rules or whatever.

00:29:33.690 --> 00:29:38.150
And I think we were lucky that
the only rule we had is you

00:29:38.150 --> 00:29:39.230
have to do it now.

00:29:39.230 --> 00:29:40.870
You have to get going.

00:29:40.870 --> 00:29:41.960
And I love that.

00:29:41.960 --> 00:29:50.020
I mean, a lot of times you do,
with time, you second guess.

00:29:50.020 --> 00:29:50.632
What is this?

00:29:50.632 --> 00:29:51.340
Is this any good?

00:29:51.340 --> 00:29:53.050
And you go back and redo it.

00:29:53.050 --> 00:29:57.010
And, sometimes, with that
energy of do it, do it, do it,

00:29:57.010 --> 00:29:58.960
you make decisions and
you're 20 decisions

00:29:58.960 --> 00:30:02.260
in before you've had time
to be too insecure about it

00:30:02.260 --> 00:30:03.650
or whatever.

00:30:03.650 --> 00:30:06.070
So I think the more that--

00:30:06.070 --> 00:30:09.280
and we're very lucky because
we had you guys sitting

00:30:09.280 --> 00:30:13.910
there every step of the way.

00:30:13.910 --> 00:30:16.930
If something didn't
work, we just blamed you.

00:30:16.930 --> 00:30:19.030
And said you know, you've
got to fix this thing.

00:30:19.030 --> 00:30:20.530
You know?

00:30:20.530 --> 00:30:24.260
And to me that's always
the intimidating part

00:30:24.260 --> 00:30:27.870
of a new thing.

00:30:27.870 --> 00:30:30.470
We have a brand new
Volvo and I don't quite

00:30:30.470 --> 00:30:36.018
know how to turn on the Sirius
stations in the car yet.

00:30:36.018 --> 00:30:37.060
You know, you're driving.

00:30:37.060 --> 00:30:38.477
You're trying not
to kill anybody.

00:30:38.477 --> 00:30:41.290
And then you're trying to get
to the last Beatles station

00:30:41.290 --> 00:30:42.250
that you were on.

00:30:42.250 --> 00:30:44.440
And so I always get intimidated
that I'm going to just turn

00:30:44.440 --> 00:30:45.490
the car off or something.

00:30:45.490 --> 00:30:48.280
So if I don't know how it
works, I'm always afraid.

00:30:48.280 --> 00:30:52.600
But having you guys there sort
of showing us how it worked.

00:30:52.600 --> 00:30:54.850
And then, we kind of immediately
want to go, oh, well,

00:30:54.850 --> 00:30:55.600
I want to do this.

00:30:55.600 --> 00:30:56.720
I want to do that.

00:30:56.720 --> 00:31:02.210
And I think that's not a luxury
that very many people have.

00:31:02.210 --> 00:31:04.820
But I felt like
that encouraged us

00:31:04.820 --> 00:31:10.280
to be as absurd as
you guys, you know,

00:31:10.280 --> 00:31:11.750
thankfully allowed us to be.

00:31:11.750 --> 00:31:16.020
I mean, the idea that
this quickly went from--

00:31:16.020 --> 00:31:18.470
there's a piano there
with this little device.

00:31:18.470 --> 00:31:21.170
And then 30 seconds later,
there's a bowl of fruit there,

00:31:21.170 --> 00:31:26.260
and Steven's playing this
amazing classical kind

00:31:26.260 --> 00:31:30.850
of piece using bananas and
strawberries and oranges.

00:31:30.850 --> 00:31:33.010
And when you're there
and all that's happening,

00:31:33.010 --> 00:31:34.720
it's exhilarating.

00:31:34.720 --> 00:31:39.053
And I think for someone
like me that idea

00:31:39.053 --> 00:31:40.720
that we're not playing
it on a keyboard,

00:31:40.720 --> 00:31:43.270
we're not playing it on a
guitar, we're not playing it--

00:31:43.270 --> 00:31:45.790
we're playing it on
fruit, just takes it

00:31:45.790 --> 00:31:51.220
in to this other realm.

00:31:51.220 --> 00:31:54.820
And so, some people say, well,
that's what, maybe-- that's

00:31:54.820 --> 00:31:56.080
what children would do.

00:31:56.080 --> 00:31:59.530
You know, that say yeah,
can I play musical notes

00:31:59.530 --> 00:32:01.030
on an orange or a strawberry.

00:32:01.030 --> 00:32:03.190
And I'd be like, yeah.

00:32:03.190 --> 00:32:03.820
Yeah.

00:32:03.820 --> 00:32:06.640
ADAM ROBERTS: Yeah, so even
though we were there, right,

00:32:06.640 --> 00:32:10.720
so we could help get around sort
of any issues you came across,

00:32:10.720 --> 00:32:12.557
there is some level
of unpredictability

00:32:12.557 --> 00:32:13.390
to these algorithms.

00:32:13.390 --> 00:32:13.780
Right?

00:32:13.780 --> 00:32:14.020
WAYNE COYNE: Yeah.

00:32:14.020 --> 00:32:15.853
ADAM ROBERTS: And I
think you guys have both

00:32:15.853 --> 00:32:17.140
experienced that in your work.

00:32:17.140 --> 00:32:19.150
So I'm curious to
hear, maybe starting

00:32:19.150 --> 00:32:20.650
with Claire from your process.

00:32:20.650 --> 00:32:22.330
Because there was
parts of things

00:32:22.330 --> 00:32:23.920
that you're giving
up some agency to,

00:32:23.920 --> 00:32:26.808
right, like, you were letting
it write lyrics for you.

00:32:26.808 --> 00:32:29.350
And in your case, you're going
to be doing a live performance

00:32:29.350 --> 00:32:31.740
where you don't know exactly
what it's going to play.

00:32:31.740 --> 00:32:32.020
WAYNE COYNE: Yeah.

00:32:32.020 --> 00:32:34.270
ADAM ROBERTS: How do you,
even though you're giving up

00:32:34.270 --> 00:32:35.895
a little bit of this
agency, how do you

00:32:35.895 --> 00:32:39.340
keep control and make sure that
your artistic vision is still

00:32:39.340 --> 00:32:40.828
shining through on top of this?

00:32:40.828 --> 00:32:41.620
CLAIRE EVANS: Yeah.

00:32:41.620 --> 00:32:43.270
I mean, I think
ultimately nothing

00:32:43.270 --> 00:32:44.660
goes out into the world
with our name on it

00:32:44.660 --> 00:32:46.285
that hasn't been
sussed over for months

00:32:46.285 --> 00:32:48.193
on the computer in
our living room.

00:32:48.193 --> 00:32:49.360
We have the ultimate agency.

00:32:49.360 --> 00:32:51.220
And the fact that we're even
doing the project to begin with

00:32:51.220 --> 00:32:52.845
is like a kind of
aesthetic provocation

00:32:52.845 --> 00:32:54.730
that we've decided
we want to do.

00:32:54.730 --> 00:32:56.375
But at the same time--

00:32:56.375 --> 00:32:58.750
I don't know-- I mean, I think
what's being replaced here

00:32:58.750 --> 00:33:01.540
in terms of our process
is the initial jam,

00:33:01.540 --> 00:33:03.580
the initial sound gathering,
the initial filling

00:33:03.580 --> 00:33:05.540
up the notebooks
with lyrics ideas,

00:33:05.540 --> 00:33:06.790
the sort of generative moment.

00:33:06.790 --> 00:33:09.310
And instead of jamming in
a room with each other,

00:33:09.310 --> 00:33:11.410
we're jamming in a server
with a model, right?

00:33:11.410 --> 00:33:15.520
And what we end up with is
a mass of source material

00:33:15.520 --> 00:33:16.630
to work with.

00:33:16.630 --> 00:33:18.760
And in my mind, that's
when the work begins.

00:33:18.760 --> 00:33:20.500
That's when the
creative work begins.

00:33:20.500 --> 00:33:22.420
And I'm a writer, too,
so I believe strongly

00:33:22.420 --> 00:33:24.250
that writing is editing.

00:33:24.250 --> 00:33:26.400
What you put on the page
in the very beginning

00:33:26.400 --> 00:33:28.900
is just like this thing that
happens in a fugue state that's

00:33:28.900 --> 00:33:29.890
a total mess.

00:33:29.890 --> 00:33:32.750
And then it's useless until you
actually do something with it.

00:33:32.750 --> 00:33:34.083
And I think it's the same thing.

00:33:34.083 --> 00:33:36.333
I mean, a notebook full of
lyrics ideas isn't a song.

00:33:36.333 --> 00:33:38.500
A jam you did in the studio
that sounded really cool

00:33:38.500 --> 00:33:39.672
in the moment isn't a song.

00:33:39.672 --> 00:33:42.130
What you do with those things,
how you bring them together,

00:33:42.130 --> 00:33:43.600
how you structure them, how
you arrange them, produce

00:33:43.600 --> 00:33:45.068
them, perform them,
commit to them,

00:33:45.068 --> 00:33:47.110
and then perform them for
possibly years to come,

00:33:47.110 --> 00:33:48.180
that's the song.

00:33:48.180 --> 00:33:48.400
WAYNE COYNE: Yeah.

00:33:48.400 --> 00:33:49.858
CLAIRE EVANS: And
so I don't really

00:33:49.858 --> 00:33:52.150
feel that much like we
gave up anything, really.

00:33:52.150 --> 00:33:53.800
We just sort of
sped up or changed

00:33:53.800 --> 00:33:55.040
the nature of the process.

00:33:55.040 --> 00:33:55.970
I don't even think
it was faster.

00:33:55.970 --> 00:33:57.303
Actually, I think it was slower.

00:33:57.303 --> 00:34:00.040
I think it took way longer
to manually annotate

00:34:00.040 --> 00:34:02.650
our back catalog in
the MIDI and come up

00:34:02.650 --> 00:34:05.025
with a corpus of 2
million words of things

00:34:05.025 --> 00:34:07.150
that we thought would be
the same things that we're

00:34:07.150 --> 00:34:09.537
kicking around in our
head on some level.

00:34:09.537 --> 00:34:11.620
I think it was actually
more tedious in many ways.

00:34:11.620 --> 00:34:14.469
So yeah, it's, like, you
decide what you give up.

00:34:14.469 --> 00:34:16.870
But you also really
determine the parameters

00:34:16.870 --> 00:34:17.679
of how you do it.

00:34:17.679 --> 00:34:19.360
And then what you do
with it afterwards

00:34:19.360 --> 00:34:20.753
is what really matters.

00:34:20.753 --> 00:34:23.170
JESSE ENGEL: And that was more
in a compositional process,

00:34:23.170 --> 00:34:25.659
where you have to deal with
how much are you controlling

00:34:25.659 --> 00:34:27.199
and how much are you editing.

00:34:27.199 --> 00:34:29.679
And so Wayne, with this thing
that's more of a performance,

00:34:29.679 --> 00:34:32.260
right, you have this
[INAUDIBLE] interaction

00:34:32.260 --> 00:34:33.850
where it's, like,
how much are you

00:34:33.850 --> 00:34:36.308
controlling what's happening
and how much is there a chance

00:34:36.308 --> 00:34:37.840
element in the performance?

00:34:37.840 --> 00:34:38.632
WAYNE COYNE: Right.

00:34:38.632 --> 00:34:41.650
I mean, the way we looked
at it, and even the way

00:34:41.650 --> 00:34:42.699
we looked at even--

00:34:42.699 --> 00:34:48.270
you guys are-- we're
accepting that we're

00:34:48.270 --> 00:34:50.100
bringing a collaborator in.

00:34:50.100 --> 00:34:52.290
You know what I mean?

00:34:52.290 --> 00:34:54.090
Which we do that all the time.

00:34:54.090 --> 00:34:58.912
And there's sometimes
you regret it, you know?

00:34:58.912 --> 00:35:01.708
[LAUGHTER]

00:35:01.708 --> 00:35:08.078
We had a digital trumpet
part on a song once.

00:35:08.078 --> 00:35:09.620
We thought it was
great and whatever.

00:35:09.620 --> 00:35:14.090
And we had a trumpet
player come to the studio.

00:35:14.090 --> 00:35:16.490
And we said, yeah, play
whatever you want, whatever.

00:35:16.490 --> 00:35:16.990
You know?

00:35:16.990 --> 00:35:22.010
And little by little,
we didn't like what

00:35:22.010 --> 00:35:24.240
he played, for whatever reason.

00:35:24.240 --> 00:35:27.538
It wasn't his fault. We were
stuck on a certain idea.

00:35:27.538 --> 00:35:29.080
And, by the end of
it, we said, well,

00:35:29.080 --> 00:35:31.240
can you play exactly
what we played,

00:35:31.240 --> 00:35:34.920
only play it on a real trumpet?

00:35:34.920 --> 00:35:38.040
And, after he left, we actually
didn't even use his trumpet,

00:35:38.040 --> 00:35:39.780
you know?

00:35:39.780 --> 00:35:45.010
But all this is a process of
saying do I like this thing?

00:35:45.010 --> 00:35:46.190
How much do I care about it?

00:35:46.190 --> 00:35:47.440
How much does it matter to me?

00:35:47.440 --> 00:35:50.130
And you go through all
these different reasons.

00:35:50.130 --> 00:35:51.790
And there's no real reason.

00:35:51.790 --> 00:35:56.800
Your reason at the end
of it is I just like it.

00:35:56.800 --> 00:35:59.510
I don't have any
deep meaning to it.

00:35:59.510 --> 00:36:01.270
And so for us, I
mean, we went into it

00:36:01.270 --> 00:36:05.170
knowing we want to do
this collaboration.

00:36:05.170 --> 00:36:08.770
And unlike the trumpet
player, everything

00:36:08.770 --> 00:36:10.090
that we would touch--

00:36:10.090 --> 00:36:11.560
and I said this to
you guys there--

00:36:11.560 --> 00:36:14.500
we were starting with
sounds that I really

00:36:14.500 --> 00:36:16.420
liked to begin with.

00:36:16.420 --> 00:36:20.070
And a lot of times I'll
start to hear the sounds

00:36:20.070 --> 00:36:22.037
and I'm already starting
to think of a song.

00:36:22.037 --> 00:36:23.370
I'm, like, hey, play that again.

00:36:23.370 --> 00:36:25.045
You know, you're
going off on things.

00:36:25.045 --> 00:36:25.670
I'm, like wait.

00:36:25.670 --> 00:36:27.240
Go back to this other sound.

00:36:27.240 --> 00:36:30.910
So, to me, that's really
all I'm ever doing.

00:36:30.910 --> 00:36:32.717
It's sparking something
that's saying,

00:36:32.717 --> 00:36:34.050
oh, I can turn that into a song.

00:36:34.050 --> 00:36:36.630
I already know what I want
to sing to that or whatever.

00:36:36.630 --> 00:36:41.190
And knowing that we're going to
turn it into something that we

00:36:41.190 --> 00:36:42.330
have to perform--

00:36:42.330 --> 00:36:45.210
we never consider that
when we're thinking

00:36:45.210 --> 00:36:46.790
of what it's going to be.

00:36:46.790 --> 00:36:48.870
Yeah, to me, those are
just two different worlds.

00:36:48.870 --> 00:36:49.200
You know?

00:36:49.200 --> 00:36:49.950
JESSE ENGEL: Yeah.

00:36:49.950 --> 00:36:52.800
WAYNE COYNE: And
knowing that we know

00:36:52.800 --> 00:36:57.300
that we want the audience to be
part of it, I think as we went,

00:36:57.300 --> 00:37:02.070
our initial surge of an idea
quickly turned into a way

00:37:02.070 --> 00:37:05.060
to make that something
that we thought--

00:37:05.060 --> 00:37:06.698
when we're there
with an audience

00:37:06.698 --> 00:37:08.240
and they're doing
this thing with us,

00:37:08.240 --> 00:37:13.540
they'll understand their
contribution to the thing.

00:37:13.540 --> 00:37:16.850
We talked about
interactive things

00:37:16.850 --> 00:37:21.230
where you're not quite sure
what you're doing to make it go.

00:37:21.230 --> 00:37:24.290
You know, you walk into a lot of
things where you sort of feel,

00:37:24.290 --> 00:37:26.840
like, you're supposed
to move your arm

00:37:26.840 --> 00:37:29.065
or stand in a certain place
and it does something.

00:37:29.065 --> 00:37:31.830
But you can't quite
tell what it's doing.

00:37:31.830 --> 00:37:33.650
And I think we've
gone to great effort

00:37:33.650 --> 00:37:36.690
to say when this
thing is happening,

00:37:36.690 --> 00:37:41.430
good, bad, indifferent,
you know you contributed

00:37:41.430 --> 00:37:46.830
to its growing, its dying,
its boredom, its excitement.

00:37:46.830 --> 00:37:48.180
You contributed to it.

00:37:48.180 --> 00:37:51.540
Which to me, is
already what we want.

00:37:51.540 --> 00:37:53.430
Because to me, that's
where the fun is.

00:37:53.430 --> 00:37:55.690
That's where the
energy comes from.

00:37:55.690 --> 00:37:58.320
We're not really saying we're
going to create the greatest

00:37:58.320 --> 00:37:59.490
piece of music ever.

00:37:59.490 --> 00:38:00.420
We might.

00:38:00.420 --> 00:38:04.180
But to me, that's not what
concerts or anything is.

00:38:04.180 --> 00:38:06.958
It's we're all going to
participate in this thing.

00:38:06.958 --> 00:38:08.500
We're going to create
our own energy,

00:38:08.500 --> 00:38:11.350
create our own time,
our own happening.

00:38:11.350 --> 00:38:13.650
JESSE ENGEL: And
so you mentioned

00:38:13.650 --> 00:38:15.833
there's some unpredictability
in the interaction

00:38:15.833 --> 00:38:16.500
with this thing.

00:38:16.500 --> 00:38:18.960
And a lot of music
and technology

00:38:18.960 --> 00:38:21.570
has always co-evolved with
that unpredictability.

00:38:21.570 --> 00:38:23.365
Guitar amplifiers
weren't meant to distort.

00:38:23.365 --> 00:38:24.990
But then people turned
them up and they

00:38:24.990 --> 00:38:26.250
found this sounds great.

00:38:26.250 --> 00:38:26.850
You know?

00:38:26.850 --> 00:38:28.950
Or the 808 drum was
originally supposed

00:38:28.950 --> 00:38:31.795
to be an accurate reproduction
of a drum set, which it's not.

00:38:31.795 --> 00:38:34.170
But, man, it sounds really
good in electronic and hip hop

00:38:34.170 --> 00:38:34.500
music.

00:38:34.500 --> 00:38:35.550
WAYNE COYNE: For sure, yeah.

00:38:35.550 --> 00:38:37.500
JESSE ENGEL: So with
these new technologies,

00:38:37.500 --> 00:38:39.960
machine learning,
like, Claire, did

00:38:39.960 --> 00:38:44.310
you have any experiences where
they failed in ways that, you

00:38:44.310 --> 00:38:47.050
know, maybe the failure
was the interesting aspect?

00:38:47.050 --> 00:38:47.200
CLAIRE EVANS: Yeah.

00:38:47.200 --> 00:38:48.360
I mean, I think it's
all about the failure.

00:38:48.360 --> 00:38:50.830
The minute it's perfect is
when I stop being interested,

00:38:50.830 --> 00:38:51.330
I think.

00:38:51.330 --> 00:38:55.060
I think those moments where the
melodic information deviates

00:38:55.060 --> 00:38:57.060
from anything a human
being would do or perform,

00:38:57.060 --> 00:38:58.920
or those moments
where a tool doesn't

00:38:58.920 --> 00:39:01.210
meet your expectations of
what it's supposed to do,

00:39:01.210 --> 00:39:02.070
but it does something
completely different

00:39:02.070 --> 00:39:04.197
that's more interesting
like the 808 or the NSynth,

00:39:04.197 --> 00:39:06.780
by the way, which is an amazing
neural synthesizer that Google

00:39:06.780 --> 00:39:07.280
makes.

00:39:07.280 --> 00:39:09.060
But, yeah, those are
the moments where

00:39:09.060 --> 00:39:11.970
I think that's the most
interesting output.

00:39:11.970 --> 00:39:14.400
Because it allows
you to determine what

00:39:14.400 --> 00:39:16.040
your own taste is sometimes.

00:39:16.040 --> 00:39:19.140
I think that taste is often
a response to something

00:39:19.140 --> 00:39:21.450
rather than something that's
coming directly from you.

00:39:21.450 --> 00:39:21.990
You see something.

00:39:21.990 --> 00:39:22.690
You're, like, I like it.

00:39:22.690 --> 00:39:23.520
I don't like it.

00:39:23.520 --> 00:39:25.240
And that's how you kind
of determine who you are,

00:39:25.240 --> 00:39:27.690
what you want to be doing, how
you want to express yourself.

00:39:27.690 --> 00:39:29.940
And I think it's very
interesting and helpful to have

00:39:29.940 --> 00:39:31.420
this sort of semi-neutral--

00:39:31.420 --> 00:39:33.420
I mean, I know saying
that AI is neutral is kind

00:39:33.420 --> 00:39:35.910
of a loaded thing-- but
semi-neutral other party

00:39:35.910 --> 00:39:38.130
in the room that is not
one of your band mates

00:39:38.130 --> 00:39:39.570
that proposes an idea.

00:39:39.570 --> 00:39:40.830
And we can all agree with it.

00:39:40.830 --> 00:39:41.910
And we can all disagree with it.

00:39:41.910 --> 00:39:44.430
No one's feelings are hurt if
we dispense with that idea,

00:39:44.430 --> 00:39:46.013
because it didn't
come from any of us.

00:39:46.013 --> 00:39:48.180
We can fall in love with
it and move forward with it

00:39:48.180 --> 00:39:49.150
as a group or not.

00:39:49.150 --> 00:39:53.010
And we can determine who we are
in terms of our relationship

00:39:53.010 --> 00:39:55.650
to each other by
agreeing or disagreeing

00:39:55.650 --> 00:39:57.228
on some other generated output.

00:39:57.228 --> 00:39:58.770
But, I mean, the
NSynth, for example,

00:39:58.770 --> 00:40:00.600
is kind of like the
808 in many ways.

00:40:00.600 --> 00:40:03.463
Because it's this tool that
is supposed to do one thing,

00:40:03.463 --> 00:40:05.880
it's supposed to sort of split
the difference between lots

00:40:05.880 --> 00:40:07.630
of different sounds
using light and space.

00:40:07.630 --> 00:40:09.850
And it kind of, in my opinion,
sort of fails at that.

00:40:09.850 --> 00:40:11.975
I mean, it doesn't convincingly
spot the difference

00:40:11.975 --> 00:40:14.070
between a car horn and a flute.

00:40:14.070 --> 00:40:15.510
Because the sort
of sample rate is

00:40:15.510 --> 00:40:17.385
so low that it ends up
sounding kind of reedy

00:40:17.385 --> 00:40:19.020
and wonky and weird.

00:40:19.020 --> 00:40:21.050
And initially we thought
that that was, like--

00:40:21.050 --> 00:40:22.300
we thought that was a failure.

00:40:22.300 --> 00:40:23.580
We thought it was
not interesting.

00:40:23.580 --> 00:40:25.997
It wasn't until we started
thinking about it like the 808,

00:40:25.997 --> 00:40:28.680
where we realized that it was
a tool with its own aesthetic,

00:40:28.680 --> 00:40:30.823
its own kind of weird,
wonky, reedy sound

00:40:30.823 --> 00:40:32.490
that it became really
interesting to us.

00:40:32.490 --> 00:40:34.115
And now it's a huge
part of our record.

00:40:34.115 --> 00:40:35.903
Because it is, again,
one of these objects

00:40:35.903 --> 00:40:38.070
that's both high tech and
low tech at the same time.

00:40:38.070 --> 00:40:39.602
And it sounds really low fi.

00:40:39.602 --> 00:40:42.060
But it takes, like, millions
of dollars of machine learning

00:40:42.060 --> 00:40:43.920
research to make that sound.

00:40:43.920 --> 00:40:46.240
That juxtaposition is
pretty fascinating to us.

00:40:46.240 --> 00:40:48.450
ADAM ROBERTS: Yeah, so I
think, unfortunately, we're

00:40:48.450 --> 00:40:49.240
out of time.

00:40:49.240 --> 00:40:50.822
But let's thank Claire--

00:40:50.822 --> 00:40:51.530
WAYNE COYNE: Yay!

00:40:51.530 --> 00:40:52.387
ADAM ROBERTS: --and
Wayne for coming.

00:40:52.387 --> 00:40:52.814
[APPLAUSE]

00:40:52.814 --> 00:40:53.241
WAYNE COYNE: Yep.

00:40:53.241 --> 00:40:53.670
ADAM ROBERTS: Thanks.

00:40:53.670 --> 00:40:54.490
All right, thank you.

00:40:54.490 --> 00:40:55.865
And they're both
playing tonight.

00:40:55.865 --> 00:40:57.330
So check out the show.

00:40:57.330 --> 00:41:01.280
[GOOGLE LOGO MUSIC]

