WEBVTT

1
00:00:00.760 --> 00:00:05.540
In this video,
we'll see a first algorithm for adapting and learning the,
uh,

2
00:00:05.650 --> 00:00:08.020
dictionary matrix in this par scoring model,

3
00:00:08.230 --> 00:00:12.610
assuming that we have found some good spots,
representations,
uh,

4
00:00:12.640 --> 00:00:15.970
for our inputs.
Okay.

5
00:00:16.240 --> 00:00:18.790
So in our parks coding objective,

6
00:00:18.820 --> 00:00:22.660
we have to minimization one where we perform inference of the Layton

7
00:00:22.661 --> 00:00:26.320
representations and one where we're trying to find a,
uh,

8
00:00:26.340 --> 00:00:29.590
the best dictionary,
a matrix,

9
00:00:29.710 --> 00:00:31.750
given these representations here.

10
00:00:32.060 --> 00:00:35.140
So now we'll look at how we can perform this minimization.

11
00:00:38.930 --> 00:00:42.060
So another way to ride the previous objective is,
uh,

12
00:00:42.230 --> 00:00:46.870
to define the loss per example,
which is the,
uh,
uh,

13
00:00:46.910 --> 00:00:51.040
which is the sparsity,
sorry,
the reconstruction and the sparsity term,

14
00:00:51.050 --> 00:00:54.470
and just say we'll minimize it,
uh,
with respect to age.

15
00:00:54.500 --> 00:00:58.340
And we've seen in the previous video that,
uh,

16
00:00:58.341 --> 00:01:02.420
how to get this representation.
We were called the minimizer here,

17
00:01:02.630 --> 00:01:04.040
h of XD.

18
00:01:04.310 --> 00:01:09.310
And so we just put it back into the reconstruction objective and that we have to

19
00:01:09.501 --> 00:01:10.550
minimize with respected.

20
00:01:11.510 --> 00:01:15.960
So let's assume that the later representation that we extract the sparse

21
00:01:15.961 --> 00:01:17.440
representation from,
uh,

22
00:01:17.510 --> 00:01:21.590
the input x t actually doesn't depend on the dictionary matrix.

23
00:01:21.620 --> 00:01:25.820
This is of course,
falls,
uh,
for different,
uh,

24
00:01:25.940 --> 00:01:29.480
Matrix d we'll get different spars,
schooling representations,

25
00:01:29.930 --> 00:01:33.200
but let's see whether we could just assume that these are fixed and try to

26
00:01:33.230 --> 00:01:37.190
update d so that we get,
uh,
uh,
a better,
uh,

27
00:01:37.280 --> 00:01:42.130
improve our objective.
I'm pretty improve it at least with respect to TD.

28
00:01:43.480 --> 00:01:48.430
So now what do we have to do is only minimize the reconstruction part because
if,

29
00:01:48.820 --> 00:01:53.290
uh,
h of x is fixed,
then this here doesn't depend on d,

30
00:01:53.291 --> 00:01:55.450
so we can just scrap it and ignore it.

31
00:01:55.660 --> 00:01:58.750
So we only have to minimize the spread reconstruction.

32
00:01:59.170 --> 00:02:02.230
That's actually a fairly nice problem.
With respect to d,

33
00:02:02.231 --> 00:02:06.040
it's actually a convex optimization problem.
But again,

34
00:02:06.041 --> 00:02:10.660
we have a constraint in a four d and the constraint is that,

35
00:02:10.960 --> 00:02:14.050
uh,
we want it's columns to be of unit norm.

36
00:02:14.080 --> 00:02:18.370
So we have to do this so that the motto cannot cheat by,
uh,

37
00:02:18.400 --> 00:02:23.190
making h as small as possible and compensating by multiplying the,
by,
uh,
uh,

38
00:02:23.290 --> 00:02:27.160
the current funding factor.
Okay.
So again,

39
00:02:27.161 --> 00:02:30.640
we could do some grading dissent,
but we'll have to adapt it,
uh,

40
00:02:30.730 --> 00:02:35.670
to respect the constraint that the columns of d must remain,
uh,

41
00:02:35.750 --> 00:02:37.090
uh,
with a uniform.

42
00:02:40.750 --> 00:02:43.240
So,
uh,
this great in the sense,
uh,

43
00:02:43.241 --> 00:02:48.241
adapted for this constraint is a known as a projected grade in dissent

44
00:02:48.550 --> 00:02:50.800
algorithm.
This is what we present here.

45
00:02:51.280 --> 00:02:56.140
So what we'll do is that as long as our dictionary,
a matrix keeps changing,

46
00:02:56.930 --> 00:03:00.490
uh,
and,
and now we'll be using later representations.

47
00:03:00.491 --> 00:03:03.550
We've extracted for all of our training example,

48
00:03:03.620 --> 00:03:07.030
we keep those fixed and we only update the d matrix.

49
00:03:07.450 --> 00:03:11.490
And then we will do that is with the regular green in the sense of data.
We'll,

50
00:03:11.560 --> 00:03:15.550
so we'll take d and that will subtract alpha times the gradient of the

51
00:03:15.551 --> 00:03:19.570
reconstruction air.
And so I'll let you do the math.
But here,

52
00:03:19.820 --> 00:03:20.620
uh,

53
00:03:20.620 --> 00:03:24.550
you can see that you should be able to see fairly easily that disc responds to

54
00:03:24.580 --> 00:03:28.270
minus the great and the reconstruction term times alpha.

55
00:03:29.320 --> 00:03:32.200
And with regular grading dissent,
we wouldn't have this part here.

56
00:03:32.230 --> 00:03:34.990
We just keep updating the matrix like this,

57
00:03:35.170 --> 00:03:37.150
using this formula until it has converged.

58
00:03:37.510 --> 00:03:40.360
But now we have to take into account the constraint.

59
00:03:40.660 --> 00:03:45.660
And so what we'll do is that we will simply renormalize the columns of these so

60
00:03:46.331 --> 00:03:49.380
that they,
after the update,
uh,

61
00:03:49.570 --> 00:03:51.910
even though after the update they might not be of unit norm,

62
00:03:51.911 --> 00:03:56.260
we'll put them back into,
there are constraints set of that.
We're exploring.

63
00:03:56.270 --> 00:04:01.270
We'll renormalize the columns of d and make sure that there are normal one.

64
00:04:01.630 --> 00:04:06.340
So here what we're doing is,
is that for each column,
uh,
uh,
the,

65
00:04:06.400 --> 00:04:10.420
uh,
dot j,
uh,
we'll replace it by the column,

66
00:04:10.421 --> 00:04:14.880
but divided by its norms.
So now it's,
we normalize it.
It's a,
again,
in unit norm,

67
00:04:15.610 --> 00:04:20.350
so it's called a projected great in the sense algorithm because every time we do

68
00:04:20.351 --> 00:04:20.980
an update,

69
00:04:20.980 --> 00:04:25.980
right after we read project our solution into the space of values that we allow

70
00:04:28.841 --> 00:04:33.700
for optimization problem.
And the way we do that is through a projection.

71
00:04:34.220 --> 00:04:36.340
Um,
so it's more formally,

72
00:04:36.341 --> 00:04:41.140
you can define this as a projection into a set and a,

73
00:04:41.141 --> 00:04:43.660
so I won't go into the mathematical details for why this is,

74
00:04:43.661 --> 00:04:46.600
but in this particular algorithm,
projected great in the sense,

75
00:04:46.601 --> 00:04:51.160
simply corresponds to doing the regular grid in the set update followed by a

76
00:04:51.161 --> 00:04:55.750
renormalization of the columns,
and you iterate until convergence,
right?

77
00:04:55.751 --> 00:04:59.470
So that's our first algorithm for performing dictionary update,

78
00:04:59.830 --> 00:05:02.320
a known as projected projected grade in dissent.

