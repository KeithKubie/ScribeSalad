WEBVTT
Kind: captions
Language: en

00:00:04.850 --> 00:00:08.210
and you know one of the things that Carl

00:00:08.210 --> 00:00:08.220
and you know one of the things that Carl
 

00:00:08.220 --> 00:00:10.400
and you know one of the things that Carl
Jung pointed out was that he had this

00:00:10.400 --> 00:00:10.410
Jung pointed out was that he had this
 

00:00:10.410 --> 00:00:14.720
Jung pointed out was that he had this
idea that when science that that LP you

00:00:14.720 --> 00:00:14.730
idea that when science that that LP you
 

00:00:14.730 --> 00:00:16.490
idea that when science that that LP you
know alchemy is the root of science in

00:00:16.490 --> 00:00:16.500
know alchemy is the root of science in
 

00:00:16.500 --> 00:00:18.800
know alchemy is the root of science in
some sense it's this dreamlike substrate

00:00:18.800 --> 00:00:18.810
some sense it's this dreamlike substrate
 

00:00:18.810 --> 00:00:21.109
some sense it's this dreamlike substrate
out of which science emerged and but

00:00:21.109 --> 00:00:21.119
out of which science emerged and but
 

00:00:21.119 --> 00:00:22.700
out of which science emerged and but
alchemy was a kind of a weird add

00:00:22.700 --> 00:00:22.710
alchemy was a kind of a weird add
 

00:00:22.710 --> 00:00:24.349
alchemy was a kind of a weird add
mixture of religious thinking and

00:00:24.349 --> 00:00:24.359
mixture of religious thinking and
 

00:00:24.359 --> 00:00:26.240
mixture of religious thinking and
scientific thinking because those two

00:00:26.240 --> 00:00:26.250
scientific thinking because those two
 

00:00:26.250 --> 00:00:28.279
scientific thinking because those two
things hadn't been differentiated back

00:00:28.279 --> 00:00:28.289
things hadn't been differentiated back
 

00:00:28.289 --> 00:00:29.900
things hadn't been differentiated back
when there were alchemists and Jung

00:00:29.900 --> 00:00:29.910
when there were alchemists and Jung
 

00:00:29.910 --> 00:00:31.550
when there were alchemists and Jung
believed that what had happened in in

00:00:31.550 --> 00:00:31.560
believed that what had happened in in
 

00:00:31.560 --> 00:00:34.069
believed that what had happened in in
Europe at least first was that the

00:00:34.069 --> 00:00:34.079
Europe at least first was that the
 

00:00:34.079 --> 00:00:36.229
Europe at least first was that the
scientific end of alchemy blew up and

00:00:36.229 --> 00:00:36.239
scientific end of alchemy blew up and
 

00:00:36.239 --> 00:00:38.240
scientific end of alchemy blew up and
expanded at an exponential rate and that

00:00:38.240 --> 00:00:38.250
expanded at an exponential rate and that
 

00:00:38.250 --> 00:00:40.340
expanded at an exponential rate and that
led to this advanced technological

00:00:40.340 --> 00:00:40.350
led to this advanced technological
 

00:00:40.350 --> 00:00:42.110
led to this advanced technological
civilization that we have but that the

00:00:42.110 --> 00:00:42.120
civilization that we have but that the
 

00:00:42.120 --> 00:00:43.910
civilization that we have but that the
moral dimension that was embedded in the

00:00:43.910 --> 00:00:43.920
moral dimension that was embedded in the
 

00:00:43.920 --> 00:00:45.799
moral dimension that was embedded in the
religious symbolism didn't develop at

00:00:45.799 --> 00:00:45.809
religious symbolism didn't develop at
 

00:00:45.809 --> 00:00:47.810
religious symbolism didn't develop at
all and so we're in this unstable

00:00:47.810 --> 00:00:47.820
all and so we're in this unstable
 

00:00:47.820 --> 00:00:50.020
all and so we're in this unstable
situation where were far more

00:00:50.020 --> 00:00:50.030
situation where were far more
 

00:00:50.030 --> 00:00:52.130
situation where were far more
technologically proficient than we are

00:00:52.130 --> 00:00:52.140
technologically proficient than we are
 

00:00:52.140 --> 00:00:54.260
technologically proficient than we are
wise and that that's actually a big

00:00:54.260 --> 00:00:54.270
wise and that that's actually a big
 

00:00:54.270 --> 00:00:56.119
wise and that that's actually a big
problem because obviously the more

00:00:56.119 --> 00:00:56.129
problem because obviously the more
 

00:00:56.129 --> 00:00:59.049
problem because obviously the more
powerful the tools you generate the more

00:00:59.049 --> 00:00:59.059
powerful the tools you generate the more
 

00:00:59.059 --> 00:01:02.090
powerful the tools you generate the more
intelligent ethically you better be or

00:01:02.090 --> 00:01:02.100
intelligent ethically you better be or
 

00:01:02.100 --> 00:01:05.030
intelligent ethically you better be or
things are going to really are going to

00:01:05.030 --> 00:01:05.040
things are going to really are going to
 

00:01:05.040 --> 00:01:07.100
things are going to really are going to
go to hell in a handbasket very very

00:01:07.100 --> 00:01:07.110
go to hell in a handbasket very very
 

00:01:07.110 --> 00:01:09.320
go to hell in a handbasket very very
rapidly you know I had this thought I

00:01:09.320 --> 00:01:09.330
rapidly you know I had this thought I
 

00:01:09.330 --> 00:01:10.700
rapidly you know I had this thought I
think I shared it a little bit last

00:01:10.700 --> 00:01:10.710
think I shared it a little bit last
 

00:01:10.710 --> 00:01:12.649
think I shared it a little bit last
night that you know in the next five

00:01:12.649 --> 00:01:12.659
night that you know in the next five
 

00:01:12.659 --> 00:01:14.270
night that you know in the next five
years six years we're gonna develop

00:01:14.270 --> 00:01:14.280
years six years we're gonna develop
 

00:01:14.280 --> 00:01:16.460
years six years we're gonna develop
pretty viciously intelligent AI systems

00:01:16.460 --> 00:01:16.470
pretty viciously intelligent AI systems
 

00:01:16.470 --> 00:01:17.899
pretty viciously intelligent AI systems
and that's already happening you know I

00:01:17.899 --> 00:01:17.909
and that's already happening you know I
 

00:01:17.909 --> 00:01:19.700
and that's already happening you know I
mean they're monitoring YouTube and

00:01:19.700 --> 00:01:19.710
mean they're monitoring YouTube and
 

00:01:19.710 --> 00:01:20.899
mean they're monitoring YouTube and
they're monitoring Facebook and they're

00:01:20.899 --> 00:01:20.909
they're monitoring Facebook and they're
 

00:01:20.909 --> 00:01:22.130
they're monitoring Facebook and they're
monitoring Google and they're trying to

00:01:22.130 --> 00:01:22.140
monitoring Google and they're trying to
 

00:01:22.140 --> 00:01:24.260
monitoring Google and they're trying to
make ethical decisions these AI systems

00:01:24.260 --> 00:01:24.270
make ethical decisions these AI systems
 

00:01:24.270 --> 00:01:26.420
make ethical decisions these AI systems
and the problem is is that the ethical

00:01:26.420 --> 00:01:26.430
and the problem is is that the ethical
 

00:01:26.430 --> 00:01:28.310
and the problem is is that the ethical
presuppositions of the programmers are

00:01:28.310 --> 00:01:28.320
presuppositions of the programmers are
 

00:01:28.320 --> 00:01:30.140
presuppositions of the programmers are
being embedded into the infrastructure

00:01:30.140 --> 00:01:30.150
being embedded into the infrastructure
 

00:01:30.150 --> 00:01:32.270
being embedded into the infrastructure
of the net and that's a hell of a thing

00:01:32.270 --> 00:01:32.280
of the net and that's a hell of a thing
 

00:01:32.280 --> 00:01:33.859
of the net and that's a hell of a thing
to think because it means that for

00:01:33.859 --> 00:01:33.869
to think because it means that for
 

00:01:33.869 --> 00:01:35.899
to think because it means that for
better or worse we're building automated

00:01:35.899 --> 00:01:35.909
better or worse we're building automated
 

00:01:35.909 --> 00:01:37.670
better or worse we're building automated
intelligences that reflect our own

00:01:37.670 --> 00:01:37.680
intelligences that reflect our own
 

00:01:37.680 --> 00:01:40.280
intelligences that reflect our own
morality and we better be very careful

00:01:40.280 --> 00:01:40.290
morality and we better be very careful
 

00:01:40.290 --> 00:01:41.630
morality and we better be very careful
about what our morality is if we're

00:01:41.630 --> 00:01:41.640
about what our morality is if we're
 

00:01:41.640 --> 00:01:43.609
about what our morality is if we're
going to automate it because automated

00:01:43.609 --> 00:01:43.619
going to automate it because automated
 

00:01:43.619 --> 00:01:46.340
going to automate it because automated
systems are incredibly powerful so so

00:01:46.340 --> 00:01:46.350
systems are incredibly powerful so so
 

00:01:46.350 --> 00:01:48.020
systems are incredibly powerful so so
that's that's kind of that's where we're

00:01:48.020 --> 00:01:48.030
that's that's kind of that's where we're
 

00:01:48.030 --> 00:01:50.210
that's that's kind of that's where we're
at at least to some degree in terms of

00:01:50.210 --> 00:01:50.220
at at least to some degree in terms of
 

00:01:50.220 --> 00:01:52.670
at at least to some degree in terms of
the new technological transformations

00:01:52.670 --> 00:01:52.680
the new technological transformations
 

00:01:52.680 --> 00:01:55.700
the new technological transformations
with in in communication technology you

00:01:55.700 --> 00:01:55.710
with in in communication technology you
 

00:01:55.710 --> 00:01:57.649
with in in communication technology you
know it puts each of us at the at the

00:01:57.649 --> 00:01:57.659
know it puts each of us at the at the
 

00:01:57.659 --> 00:01:59.600
know it puts each of us at the at the
center of a wide web of connections and

00:01:59.600 --> 00:01:59.610
center of a wide web of connections and
 

00:01:59.610 --> 00:02:01.580
center of a wide web of connections and
makes the consequences of our moral

00:02:01.580 --> 00:02:01.590
makes the consequences of our moral
 

00:02:01.590 --> 00:02:03.740
makes the consequences of our moral
decisions much more immediately manifest

00:02:03.740 --> 00:02:03.750
decisions much more immediately manifest
 

00:02:03.750 --> 00:02:06.320
decisions much more immediately manifest
to each of us

