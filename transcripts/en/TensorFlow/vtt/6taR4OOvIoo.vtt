WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.662
[MUSIC PLAYING]

00:00:01.662 --> 00:00:03.120
LAURENCE MORONEY:
Bring the Magnus.

00:00:03.120 --> 00:00:03.930
MAGNUS HYTTSTEN: I'm
not sure you want that.

00:00:03.930 --> 00:00:05.638
LAURENCE MORONEY:
Let's bring the Magnus.

00:00:10.707 --> 00:00:11.790
MAGNUS HYTTSTEN: Hi there.

00:00:11.790 --> 00:00:15.090
And welcome to "#AskTensorFlow,"
where we answer questions you

00:00:15.090 --> 00:00:17.550
may have around
everything TensorFlow.

00:00:17.550 --> 00:00:20.190
I'm Magnus Hyttsten, a developer
advocate on the TensorFlow

00:00:20.190 --> 00:00:20.820
team.

00:00:20.820 --> 00:00:21.720
LAURENCE MORONEY: And
I'm Laurence Moroney,

00:00:21.720 --> 00:00:23.762
also a developer advocate
on the TensorFlow team.

00:00:23.762 --> 00:00:25.470
MAGNUS HYTTSTEN: So
let's get right to it

00:00:25.470 --> 00:00:26.590
with the first question.

00:00:26.590 --> 00:00:27.360
LAURENCE MORONEY: So
the first question

00:00:27.360 --> 00:00:29.040
comes from Aaron on Twitter.

00:00:29.040 --> 00:00:32.070
And it is what's the "best
starting point for learning

00:00:32.070 --> 00:00:34.050
the tools of ML and AI?

00:00:34.050 --> 00:00:36.180
I feel like diving
right into TensorFlow

00:00:36.180 --> 00:00:38.762
could be a little bit like
getting into the deep end!"

00:00:38.762 --> 00:00:40.470
You know, Aaron, that's
a question that's

00:00:40.470 --> 00:00:42.390
close to my heart and
I agree with you a lot.

00:00:42.390 --> 00:00:44.520
So there's so many
things to learn

00:00:44.520 --> 00:00:47.160
because this is such a
nascent area in so many ways.

00:00:47.160 --> 00:00:49.170
So my recommendation
would be to look

00:00:49.170 --> 00:00:51.400
at doing two different
things in parallel.

00:00:51.400 --> 00:00:54.660
So the first of these is to
look at pretrained models that

00:00:54.660 --> 00:00:56.100
are already out there.

00:00:56.100 --> 00:00:57.990
And you can see how
machine learning

00:00:57.990 --> 00:01:01.180
models will change the overall
paradigm of programming.

00:01:01.180 --> 00:01:02.880
So instead of you
writing a lot of code

00:01:02.880 --> 00:01:05.480
to do rules for things
like classification,

00:01:05.480 --> 00:01:07.260
a machine learning
model has inferred

00:01:07.260 --> 00:01:09.600
patterns between lots
of input features

00:01:09.600 --> 00:01:11.700
to do the
classification for you.

00:01:11.700 --> 00:01:13.500
You'll then load that
model, provide it

00:01:13.500 --> 00:01:16.080
with a set of inputs, and it
just gives you the results.

00:01:16.080 --> 00:01:19.230
It's not just fun, it's
actually great exposure

00:01:19.230 --> 00:01:22.120
into the overall field of
of ML and how it works.

00:01:22.120 --> 00:01:24.600
What you can also do
is then get low level

00:01:24.600 --> 00:01:27.090
and start building
models yourself.

00:01:27.090 --> 00:01:30.000
And that's what TensorFlow
is really, really good at.

00:01:30.000 --> 00:01:31.590
It might be daunting
when you first

00:01:31.590 --> 00:01:33.465
take a look at it because
there's just simply

00:01:33.465 --> 00:01:34.620
so much to learn.

00:01:34.620 --> 00:01:38.042
So my advice would be to start
doing two different things.

00:01:38.042 --> 00:01:40.500
The first of these is to look
at some samples for something

00:01:40.500 --> 00:01:43.920
called classification and
the classification done

00:01:43.920 --> 00:01:45.330
using neural networks.

00:01:45.330 --> 00:01:47.460
It sounds complicated, but
it's actually quite easy

00:01:47.460 --> 00:01:48.640
to get started.

00:01:48.640 --> 00:01:51.740
Check out the TensorFlow blog
for some samples and tutorials.

00:01:51.740 --> 00:01:53.490
The first of these, I
think I wrote myself

00:01:53.490 --> 00:01:55.680
when I started playing
with TensorFlow,

00:01:55.680 --> 00:01:58.040
was the equivalent
of a Hello World.

00:01:58.040 --> 00:02:00.360
And in this Hello
World for TensorFlow,

00:02:00.360 --> 00:02:02.940
I actually ended up
training a neural network

00:02:02.940 --> 00:02:04.140
for rain detection.

00:02:04.140 --> 00:02:05.360
It wasn't very accurate.

00:02:05.360 --> 00:02:07.860
It probably would have been
more accurate to open the window

00:02:07.860 --> 00:02:10.500
to see if it was raining,
but it was a learned model.

00:02:10.500 --> 00:02:13.770
I took data about pressure, and
temperature, and other stuff

00:02:13.770 --> 00:02:15.955
and trained a model
that said when

00:02:15.955 --> 00:02:17.580
it was this pressure,
this temperature,

00:02:17.580 --> 00:02:19.117
was it raining-- yes or no?

00:02:19.117 --> 00:02:21.200
And then when I measured
pressure and temperature,

00:02:21.200 --> 00:02:22.658
it would tell me
if it was raining.

00:02:22.658 --> 00:02:24.690
It was about 75% accurate.

00:02:24.690 --> 00:02:26.580
So the second thing
then is to start

00:02:26.580 --> 00:02:29.850
looking at some samples for
something called regression.

00:02:29.850 --> 00:02:32.160
And these can be
used for prediction.

00:02:32.160 --> 00:02:34.200
In many ways, regression
tools like this

00:02:34.200 --> 00:02:36.000
was the starting
point for companies

00:02:36.000 --> 00:02:38.040
providing AI services.

00:02:38.040 --> 00:02:41.040
So for example, given a set
of data like characteristics

00:02:41.040 --> 00:02:43.680
about a house, ML
regression models

00:02:43.680 --> 00:02:45.930
have become
staggeringly accurate

00:02:45.930 --> 00:02:48.258
at predicting the price
or value of that house.

00:02:48.258 --> 00:02:49.800
Obviously, not just
houses-- anything

00:02:49.800 --> 00:02:51.900
like that where you
have some prediction.

00:02:51.900 --> 00:02:53.670
Learning regression
will help you

00:02:53.670 --> 00:02:55.290
getting started
into understanding

00:02:55.290 --> 00:02:56.620
how these things work.

00:02:56.620 --> 00:02:57.660
I hope that helps.

00:02:57.660 --> 00:02:59.220
That was a fascinating question.

00:02:59.220 --> 00:03:01.800
Huge area of stuff to learn.

00:03:01.800 --> 00:03:03.630
I know it seems
overwhelming, but I

00:03:03.630 --> 00:03:05.130
promise it will be worth it.

00:03:05.130 --> 00:03:06.450
Shall we take a look
at the next question?

00:03:06.450 --> 00:03:07.992
MAGNUS HYTTSTEN:
Yeah, let's do that.

00:03:07.992 --> 00:03:10.920
So the next question is from
Ashan on Stack Overflow.

00:03:10.920 --> 00:03:14.250
And the question is,
"SKLearn has a labelencoder,

00:03:14.250 --> 00:03:16.590
is there anything
similar in TensorFlow

00:03:16.590 --> 00:03:18.600
to manage categorical input?"

00:03:18.600 --> 00:03:21.120
And I'm happy to say
that there actually is.

00:03:21.120 --> 00:03:26.250
TensorFlow has a package called
tf.feature_columns that has

00:03:26.250 --> 00:03:28.830
many, many functions
to describe your input,

00:03:28.830 --> 00:03:32.050
including bucketizing,
managing categories,

00:03:32.050 --> 00:03:34.930
and in fact even to
train embeddings.

00:03:34.930 --> 00:03:38.220
There is also a blog
post that describes

00:03:38.220 --> 00:03:41.430
all of this stuff in
quite a lot of detail,

00:03:41.430 --> 00:03:44.290
so you should definitely
check out the link here below.

00:03:44.290 --> 00:03:46.800
LAURENCE MORONEY: The next
one comes from Kaique da Silva

00:03:46.800 --> 00:03:48.150
and it's on Twitter.

00:03:48.150 --> 00:03:51.660
And Kaique was asking,
"what's the best way to start

00:03:51.660 --> 00:03:53.610
contributing to TensorFlow?"

00:03:53.610 --> 00:03:54.420
Oh, I like that.

00:03:54.420 --> 00:03:54.840
MAGNUS HYTTSTEN: Yeah.

00:03:54.840 --> 00:03:56.710
LAURENCE MORONEY: We always
love it when people contribute.

00:03:56.710 --> 00:03:58.660
So I think there's lots of
ways that you can do it.

00:03:58.660 --> 00:04:00.398
So the first and most
obvious, of course,

00:04:00.398 --> 00:04:01.940
is to take a look
at the source code.

00:04:01.940 --> 00:04:03.528
It is open source after all.

00:04:03.528 --> 00:04:05.820
And maybe you can find
something there that you can add

00:04:05.820 --> 00:04:06.810
or you can improve.

00:04:06.810 --> 00:04:08.970
There's also lots of
issues that we've tagged,

00:04:08.970 --> 00:04:10.440
contributions welcome.

00:04:10.440 --> 00:04:13.530
So check in and take a look
to see if they're for you.

00:04:13.530 --> 00:04:16.438
MAGNUS HYTTSTEN: That's right,
but everyone cannot program

00:04:16.438 --> 00:04:17.480
and create pull requests.

00:04:17.480 --> 00:04:19.522
LAURENCE MORONEY: Or
they're deep AI specialists.

00:04:19.522 --> 00:04:21.690
MAGNUS HYTTSTEN: Exactly,
using Python and C++.

00:04:21.690 --> 00:04:24.113
So if you're not a
deep AI scientist who

00:04:24.113 --> 00:04:25.530
can improve the
framework, there's

00:04:25.530 --> 00:04:29.430
still a lot of options
that could work for you.

00:04:29.430 --> 00:04:31.830
You could, for example,
write a blog post on Medium

00:04:31.830 --> 00:04:34.140
and tell us all about
it because we're

00:04:34.140 --> 00:04:38.760
looking to add things to the
official TensorFlow Medium

00:04:38.760 --> 00:04:40.750
blog property all the time.

00:04:40.750 --> 00:04:42.930
So we'd love to check
out any contributions

00:04:42.930 --> 00:04:44.597
that you would be
interested in sharing.

00:04:44.597 --> 00:04:46.097
LAURENCE MORONEY:
And if you've done

00:04:46.097 --> 00:04:48.540
something cool in TensorFlow,
do let us know all about it.

00:04:48.540 --> 00:04:50.880
All the time, we're looking
to highlight projects.

00:04:50.880 --> 00:04:53.140
We have a show called
"TensorFlow Meets,"

00:04:53.140 --> 00:04:54.450
where we'd love to have you on.

00:04:54.450 --> 00:04:55.960
We'll talk to you about
what you're doing.

00:04:55.960 --> 00:04:57.750
We'll get to showcase
what you're doing.

00:04:57.750 --> 00:05:00.330
And then you just maybe would
be able to inspire and inform

00:05:00.330 --> 00:05:03.600
lots of other people to succeed
themselves in TensorFlow.

00:05:03.600 --> 00:05:04.800
And there's one more, right?

00:05:04.800 --> 00:05:05.250
MAGNUS HYTTSTEN: Right.

00:05:05.250 --> 00:05:06.240
LAURENCE MORONEY: There's one
more thing that you can do.

00:05:06.240 --> 00:05:08.483
And that is ask
questions on here, right?

00:05:08.483 --> 00:05:09.150
You never know--

00:05:09.150 --> 00:05:09.990
MAGNUS HYTTSTEN: That's right.

00:05:09.990 --> 00:05:11.040
LAURENCE MORONEY:
You never know who

00:05:11.040 --> 00:05:13.260
might be struggling with
the same stuff that you are.

00:05:13.260 --> 00:05:14.760
And sometimes, we're
even struggling

00:05:14.760 --> 00:05:16.593
with it ourselves and
hearing your questions

00:05:16.593 --> 00:05:18.270
is great to help us focus.

00:05:18.270 --> 00:05:20.010
And the more we
see your question,

00:05:20.010 --> 00:05:21.760
the more we'll try to answer it.

00:05:21.760 --> 00:05:22.635
So thank you so much.

00:05:22.635 --> 00:05:24.843
Those are lots of great ways
that you can contribute.

00:05:24.843 --> 00:05:26.740
We'd love to see what
you do with them.

00:05:26.740 --> 00:05:27.573
MAGNUS HYTTSTEN: OK.

00:05:27.573 --> 00:05:30.030
Next question-- "I
keep training a DNN

00:05:30.030 --> 00:05:32.070
classifier on the
same data, but I

00:05:32.070 --> 00:05:34.110
get different accuracy results.

00:05:34.110 --> 00:05:35.000
Why?"

00:05:35.000 --> 00:05:38.030
And this is from
Laurence in Seattle.

00:05:38.030 --> 00:05:40.520
Hey, is that you?

00:05:40.520 --> 00:05:41.600
LAURENCE MORONEY: Maybe.

00:05:41.600 --> 00:05:42.980
Yes, yes, OK.

00:05:42.980 --> 00:05:43.590
You got me.

00:05:43.590 --> 00:05:44.460
That is me.

00:05:44.460 --> 00:05:47.550
This one did drive
me crazy for a while,

00:05:47.550 --> 00:05:50.590
but the solution to this
is actually very simple.

00:05:50.590 --> 00:05:53.910
It's common practice to shuffle
your training and test sets.

00:05:53.910 --> 00:05:56.010
But of course, if you
shuffle and randomize them

00:05:56.010 --> 00:05:59.100
before you split them, you'll
end up with different training

00:05:59.100 --> 00:06:02.790
and test sets every time, so of
course, your results will vary.

00:06:02.790 --> 00:06:05.310
And not only that, we
learned from the engineers

00:06:05.310 --> 00:06:08.700
that some TensorFlow
operations are deliberately

00:06:08.700 --> 00:06:11.373
not deterministic for
performance reasons.

00:06:11.373 --> 00:06:13.290
So even if you haven't
shuffled your data sets

00:06:13.290 --> 00:06:15.582
and you're always using the
same data set for training,

00:06:15.582 --> 00:06:17.940
you may also sometimes
see some small differences

00:06:17.940 --> 00:06:18.808
in your results.

00:06:18.808 --> 00:06:20.850
But the main reason if
you have large differences

00:06:20.850 --> 00:06:23.830
is because you shuffled
before you split.

00:06:23.830 --> 00:06:26.062
So I'm guilty of that mea culpa.

00:06:26.062 --> 00:06:28.020
MAGNUS HYTTSTEN: And
that's it for this version

00:06:28.020 --> 00:06:28.920
of "#AskTensorFlow."

00:06:28.920 --> 00:06:31.350
Now if you have a question
you would like to ask us,

00:06:31.350 --> 00:06:35.250
then file it on Twitter with
the hashtag #AskTensorFlow.

00:06:35.250 --> 00:06:38.130
Now, we're really happy that you
were here today and check out

00:06:38.130 --> 00:06:40.080
the next version as well.

00:06:40.080 --> 00:06:42.830
[MUSIC PLAYING]

