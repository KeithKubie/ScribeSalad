WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.868
[MUSIC PLAYING]

00:00:07.180 --> 00:00:08.360
DONG CHEN: Hello everyone.

00:00:08.360 --> 00:00:10.180
Welcome to the talk.

00:00:10.180 --> 00:00:11.590
My name is Dong Chen.

00:00:11.590 --> 00:00:14.750
I'm a tech lead of a
ML Kit team at Google.

00:00:14.750 --> 00:00:18.550
I'm here today to talk about
ML Kit, Google's machine

00:00:18.550 --> 00:00:21.430
learning SDK for mobile.

00:00:21.430 --> 00:00:24.640
I would provide a review
of what ML Kit does

00:00:24.640 --> 00:00:28.670
and also what's new since
its inception six months ago.

00:00:28.670 --> 00:00:32.350
I also share some tips
and the best practices

00:00:32.350 --> 00:00:35.650
to improve performance,
accuracy, reduce

00:00:35.650 --> 00:00:38.170
size, and a few other things.

00:00:38.170 --> 00:00:41.560
In the end, I will talk
about what we're working on.

00:00:44.770 --> 00:00:48.805
Two years ago, Google
CEO Sundar Pichai said,

00:00:48.805 --> 00:00:53.320
we will move from mobile-first
to an AI-first world.

00:00:53.320 --> 00:00:55.950
The intersection
of these two worlds

00:00:55.950 --> 00:00:59.220
means machine
learning on mobile.

00:00:59.220 --> 00:01:02.220
In recent years, more
and more mobile apps

00:01:02.220 --> 00:01:05.190
are using machine learning
to produce fascinating user

00:01:05.190 --> 00:01:06.750
experiences.

00:01:06.750 --> 00:01:08.550
With smartphones
becoming increasingly

00:01:08.550 --> 00:01:12.210
powerful and capable, more and
more machine learning logic

00:01:12.210 --> 00:01:14.810
is shifting from the
server running the cloud

00:01:14.810 --> 00:01:17.971
to the mobile device
in your pocket.

00:01:17.971 --> 00:01:21.460
Running on-device machine
learning has many advantages.

00:01:21.460 --> 00:01:25.250
It's fast, it does not need to
send very expensive RPC costs

00:01:25.250 --> 00:01:27.270
and incur long network latency.

00:01:27.270 --> 00:01:31.760
It also runs anytime, anywhere
with and without network

00:01:31.760 --> 00:01:33.770
connectivity.

00:01:33.770 --> 00:01:36.940
It also provides better
protection for user privacy

00:01:36.940 --> 00:01:40.690
since the data does not
have to leave the device.

00:01:40.690 --> 00:01:42.430
Let me take a quick poll here.

00:01:42.430 --> 00:01:44.640
How many of you travel
from outside of California

00:01:44.640 --> 00:01:46.950
to attend this Dev Summit?

00:01:46.950 --> 00:01:47.450
Wow.

00:01:47.450 --> 00:01:48.920
Welcome.

00:01:48.920 --> 00:01:50.830
Welcome to Silicon Valley.

00:01:50.830 --> 00:01:54.550
I hope you get a chance to visit
Golden Gate Bridge, Fisherman's

00:01:54.550 --> 00:01:58.270
Wharf, and also enjoy some
good food at a local restaurant

00:01:58.270 --> 00:02:00.220
around the Bay Area.

00:02:00.220 --> 00:02:04.330
Talking about food,
my biological watch

00:02:04.330 --> 00:02:06.580
tells me it's time for dinner.

00:02:06.580 --> 00:02:08.983
So where should I eat?

00:02:08.983 --> 00:02:13.320
Well, I will pull out my
phone, ask Google Assistant.

00:02:13.320 --> 00:02:14.180
Hi, Google.

00:02:14.180 --> 00:02:14.980
I'm hungry.

00:02:14.980 --> 00:02:17.700
Any good restaurants
around here?

00:02:17.700 --> 00:02:19.370
Google Assistant
uses machine learning

00:02:19.370 --> 00:02:22.610
to understand my question
and make recommendation

00:02:22.610 --> 00:02:26.390
where to go for dinner around
the area and location I'm

00:02:26.390 --> 00:02:28.820
currently at.

00:02:28.820 --> 00:02:31.560
When I travel to a
new place, I would

00:02:31.560 --> 00:02:34.690
like to take a lot of photos.

00:02:34.690 --> 00:02:37.700
And Google Photos
use image recognition

00:02:37.700 --> 00:02:39.600
to understand image content.

00:02:39.600 --> 00:02:44.550
You can classify photos quickly
into thousands of categories.

00:02:44.550 --> 00:02:48.950
You can also detect individual
faces, paths, objects,

00:02:48.950 --> 00:02:52.880
text in the photo and then make
them searchable by keywords

00:02:52.880 --> 00:02:55.490
or categories later on.

00:02:55.490 --> 00:02:59.690
Mobile devices can now
really increasingly

00:02:59.690 --> 00:03:02.214
[? suffix ?] the
machine learning tasks.

00:03:02.214 --> 00:03:07.350
Last spring, I took my
family to Tuscany in Italy.

00:03:07.350 --> 00:03:10.485
We drove around those
beautiful hilltop towns.

00:03:10.485 --> 00:03:13.810
In each of the towns,
there are many signs.

00:03:13.810 --> 00:03:16.065
But I don't speak
Italian, unfortunately.

00:03:16.065 --> 00:03:18.220
I wish that I do.

00:03:18.220 --> 00:03:21.690
So how do I know what they mean?

00:03:21.690 --> 00:03:26.010
Here is where the Google
Translate came to the rescue.

00:03:26.010 --> 00:03:28.580
Google Translate can perform
multiple machine learning tasks

00:03:28.580 --> 00:03:30.330
in a single journey.

00:03:30.330 --> 00:03:34.110
When I pull out my phone,
pointing the camera at a sign,

00:03:34.110 --> 00:03:36.810
it will use optical
capture recognition

00:03:36.810 --> 00:03:39.740
to detect the text in the sign.

00:03:39.740 --> 00:03:43.020
It will also use natural
language processing

00:03:43.020 --> 00:03:46.834
to translate the text from
one language into another.

00:03:46.834 --> 00:03:50.570
Finally, it will use speech
synthesis to convert text

00:03:50.570 --> 00:03:54.170
into voice and using
speaker to tell me what

00:03:54.170 --> 00:03:56.666
the sign is in my own language.

00:03:56.666 --> 00:03:59.390
All these tasks involves
machine learning.

00:03:59.390 --> 00:04:02.720
Combined, they produce a
seamless and a powerful user

00:04:02.720 --> 00:04:04.164
experience.

00:04:04.164 --> 00:04:06.490
This all looks great.

00:04:06.490 --> 00:04:10.720
But as a developer, how do
I build something like this?

00:04:10.720 --> 00:04:13.802
Well, it's doable but not easy.

00:04:13.802 --> 00:04:17.959
Machine learning often requires
specialized knowledge and years

00:04:17.959 --> 00:04:21.790
of experience and training
in building ML models.

00:04:21.790 --> 00:04:23.990
The model training,
a lot of time,

00:04:23.990 --> 00:04:28.630
requires a large amount
of good high-quality data.

00:04:28.630 --> 00:04:31.790
The mobile device higher
[INAUDIBLE] [? side ?]

00:04:31.790 --> 00:04:33.950
has very limited
computing power.

00:04:33.950 --> 00:04:36.800
The models running on
a server in the cloud

00:04:36.800 --> 00:04:39.950
are often too large or too
complex to run directly

00:04:39.950 --> 00:04:41.330
on the mobile device.

00:04:41.330 --> 00:04:42.950
You need to spend
a lot of effort

00:04:42.950 --> 00:04:46.382
to optimize model
for the mobile usage.

00:04:46.382 --> 00:04:49.280
After finally the
app is built, you

00:04:49.280 --> 00:04:51.250
need to worry about
how do I deploy?

00:04:51.250 --> 00:04:54.790
How do I maintain ongoing
instrumentation of the models?

00:04:54.790 --> 00:04:56.856
That becomes another headache.

00:04:56.856 --> 00:05:01.930
To tackle all these
problems, we launched ML Kit,

00:05:01.930 --> 00:05:04.750
Google's Machine
Learning SDK for mobile,

00:05:04.750 --> 00:05:09.700
which helps mobile developers
to build Android and iOS

00:05:09.700 --> 00:05:13.540
apps using machine
learning technologies.

00:05:13.540 --> 00:05:17.250
ML Kit is aimed at making
machine learning easy

00:05:17.250 --> 00:05:19.320
for mobile developers.

00:05:19.320 --> 00:05:22.470
Just because you want to use
machine learning on mobile,

00:05:22.470 --> 00:05:24.630
it does not mean
you need to worry

00:05:24.630 --> 00:05:28.200
about collecting data,
building models, training,

00:05:28.200 --> 00:05:31.800
model compression, optimization,
hosting, deployment,

00:05:31.800 --> 00:05:34.320
downloading model--
all this headache.

00:05:34.320 --> 00:05:37.800
ML Kit will take care
of all this for you.

00:05:37.800 --> 00:05:43.080
We provide common turnkey models
that work just out-of-box.

00:05:43.080 --> 00:05:47.490
All the models are optimized
for speed, accuracy, as well as

00:05:47.490 --> 00:05:51.460
efficiency for
the mobile device.

00:05:51.460 --> 00:05:55.980
We provide one consistent API
across both Android and iOS.

00:05:55.980 --> 00:05:59.970
It's very easy to integrate
ML Kit with Firebase tools

00:05:59.970 --> 00:06:04.110
like remote config
or A/B testing.

00:06:04.110 --> 00:06:06.980
For commonly needed
machine learning tasks,

00:06:06.980 --> 00:06:11.410
we have Base APIs that come
with pre-trained Google models

00:06:11.410 --> 00:06:13.420
that work out-of-box.

00:06:13.420 --> 00:06:16.540
Currently there are five
APIs we are supporting.

00:06:16.540 --> 00:06:21.100
The test recognition API is
supported on both on-device

00:06:21.100 --> 00:06:22.560
and the cloud.

00:06:22.560 --> 00:06:26.260
The on-device API can
recognize Latin characters.

00:06:26.260 --> 00:06:30.820
And the cloud API supports
a wide range of languages

00:06:30.820 --> 00:06:32.744
and special characters.

00:06:32.744 --> 00:06:36.190
Face detection is
another API we support,

00:06:36.190 --> 00:06:39.220
which can be used to detect
faces in both static image

00:06:39.220 --> 00:06:42.112
as well as live video streaming.

00:06:42.112 --> 00:06:45.100
Now we also launched
contour detection,

00:06:45.100 --> 00:06:48.300
which can help you to identify
different parts of the face

00:06:48.300 --> 00:06:52.172
and you can then apply
face mask or filters.

00:06:52.172 --> 00:06:55.190
The barcode scanning
can be used to detect

00:06:55.190 --> 00:06:58.700
both one-dimensional
two-dimensional barcodes, as

00:06:58.700 --> 00:07:00.830
well as a QR code.

00:07:00.830 --> 00:07:05.190
Image labeling API can detect
objects of various entities

00:07:05.190 --> 00:07:08.010
inside the photo.

00:07:08.010 --> 00:07:10.270
This supports both
on-device and the cloud.

00:07:10.270 --> 00:07:12.782
The on-device API supports
more than 400 labels,

00:07:12.782 --> 00:07:14.490
which cover most of
the common things you

00:07:14.490 --> 00:07:19.890
see in photos, while the
cloud API can support 10,000

00:07:19.890 --> 00:07:22.770
labels across many categories.

00:07:22.770 --> 00:07:27.210
Finally, our landmark API can
recognize well-known places

00:07:27.210 --> 00:07:31.950
in the photo, like the White
House or the Eiffel Tower.

00:07:31.950 --> 00:07:34.370
If this pre-trained model
does not fit your need

00:07:34.370 --> 00:07:38.385
and you are an experienced
developer with the knowledge

00:07:38.385 --> 00:07:40.650
and you know how to
build and train models,

00:07:40.650 --> 00:07:43.860
you're more than welcome to
bring your own custom model.

00:07:43.860 --> 00:07:47.010
We run custom models
on TensorFlow Lite.

00:07:47.010 --> 00:07:49.650
As you might know, TensorFlow
is the open-source framework

00:07:49.650 --> 00:07:50.880
for machine learning.

00:07:50.880 --> 00:07:53.400
And TensorFlow Lite is
a lightweight version

00:07:53.400 --> 00:07:56.810
of TensorFlow optimized
for mobile platforms.

00:07:56.810 --> 00:07:58.980
For models trained
with TensorFlow,

00:07:58.980 --> 00:08:02.220
we provide you tools
to convert and compress

00:08:02.220 --> 00:08:05.856
into a format compatible
with TensorFlow Lite.

00:08:05.856 --> 00:08:08.230
While you're using a custom
model, you have two options--

00:08:08.230 --> 00:08:12.100
either bounded inside your
app or hosted in the cloud.

00:08:12.100 --> 00:08:14.440
If you choose the latter
option to host in the cloud,

00:08:14.440 --> 00:08:17.410
it does not mean you need to
build your own cloud server.

00:08:17.410 --> 00:08:21.680
ML Kit and Firebase will
provide a way for you.

00:08:21.680 --> 00:08:24.460
We will manage the model
hosting, deployment,

00:08:24.460 --> 00:08:30.195
downloading, upgrade, as well
as the ongoing experimentations.

00:08:30.195 --> 00:08:34.200
Since ML Kit was launched
six months ago at Google I/O,

00:08:34.200 --> 00:08:37.049
we have made several
enhancements.

00:08:37.049 --> 00:08:41.929
First, we greatly enhanced
our face detection models,

00:08:41.929 --> 00:08:48.366
which is now 18 times faster
and 13% to 24% more accurate.

00:08:48.366 --> 00:08:51.930
We also polished our
text recognition API

00:08:51.930 --> 00:08:54.930
by making them more
streamlined and consistent

00:08:54.930 --> 00:08:57.916
across both on-device
and the cloud.

00:08:57.916 --> 00:09:01.640
In addition, we launched
133-point face contour

00:09:01.640 --> 00:09:04.022
detection.

00:09:04.022 --> 00:09:07.110
As shown in this
image, you can see,

00:09:07.110 --> 00:09:09.810
now you can use our face
contour detection API

00:09:09.810 --> 00:09:13.590
to identify the contours of
various parts of your face

00:09:13.590 --> 00:09:15.690
or anyone's face in the photo.

00:09:15.690 --> 00:09:17.730
And that includes
the entire face--

00:09:17.730 --> 00:09:20.820
both eyebrows, eyes,
nose, and the lips.

00:09:20.820 --> 00:09:26.850
This is where the real-time apps
can put a customer face mask

00:09:26.850 --> 00:09:27.810
and the filters--

00:09:27.810 --> 00:09:31.290
like a goggle or
some funny nose--

00:09:31.290 --> 00:09:34.350
on the face, and
make the mask move

00:09:34.350 --> 00:09:39.372
with the face in
live video streaming.

00:09:39.372 --> 00:09:42.860
Next, I'm going to share
some tips and best practices

00:09:42.860 --> 00:09:45.350
for how to use ML
Kit so that I can

00:09:45.350 --> 00:09:48.602
build impressive mobile
apps using on-device machine

00:09:48.602 --> 00:09:49.550
learning.

00:09:49.550 --> 00:09:51.560
First of all, I will
share a couple of tips

00:09:51.560 --> 00:09:53.800
for achieving best accuracy.

00:09:53.800 --> 00:09:56.690
There are two things
you should make sure.

00:09:56.690 --> 00:10:01.030
One, you should take a very
sharp and well-focused image.

00:10:01.030 --> 00:10:05.554
Poor image focus can
really hurt the accuracy.

00:10:05.554 --> 00:10:08.010
Second, you should
ensure the options

00:10:08.010 --> 00:10:11.370
you want to detect in the
image has sufficient size.

00:10:11.370 --> 00:10:14.520
For example, for face detection,
you should have at least 100

00:10:14.520 --> 00:10:17.190
by 100 pixels for each face.

00:10:17.190 --> 00:10:19.980
And if you want the contour
detection, in the selfie mode,

00:10:19.980 --> 00:10:26.375
for example, then the face
should be 200 by 200 pixels.

00:10:26.375 --> 00:10:30.230
For the text API
for Latin languages,

00:10:30.230 --> 00:10:34.215
each character should be
at least 16 by 16 in size.

00:10:34.215 --> 00:10:38.410
And if you use our cloud
API to detect and recognize

00:10:38.410 --> 00:10:41.850
Chinese, Japanese, and
Korean, then each character

00:10:41.850 --> 00:10:45.850
in those oriental languages
should be at least 24 by 24.

00:10:45.850 --> 00:10:49.060
Similarly, barcodes also
have the size requirement.

00:10:49.060 --> 00:10:53.900
Please check out our online
documentation for more details.

00:10:53.900 --> 00:10:57.170
Machine learning and
libraries can be large, which

00:10:57.170 --> 00:10:59.330
can slow down the app download.

00:10:59.330 --> 00:11:04.094
There are two ways to
reduce the APK size.

00:11:04.094 --> 00:11:08.590
First, you can build your
app as a Android App Bundle.

00:11:08.590 --> 00:11:12.720
By doing that, you enable Google
Play to automatically generate

00:11:12.720 --> 00:11:16.460
APKs for specific screen
density, CPU architecture,

00:11:16.460 --> 00:11:17.770
as well as the languages.

00:11:17.770 --> 00:11:20.010
Then your users only
have to download

00:11:20.010 --> 00:11:22.290
the APKs and native call
libraries that match

00:11:22.290 --> 00:11:24.840
their device configuration.

00:11:24.840 --> 00:11:27.390
Another way you
can reduce APK size

00:11:27.390 --> 00:11:30.870
is, if the machine learning
feature on your app

00:11:30.870 --> 00:11:33.630
is not the primary
purpose, then you

00:11:33.630 --> 00:11:36.370
could move machine
learning features which

00:11:36.370 --> 00:11:40.350
requires the ML Kit into
a dynamic feature module.

00:11:40.350 --> 00:11:42.300
In that way, you'll
prevent users

00:11:42.300 --> 00:11:45.090
from unnecessary
downloading ML models,

00:11:45.090 --> 00:11:48.460
which can sometimes be large.

00:11:48.460 --> 00:11:52.970
We all know machine learning
involves a lot of computation.

00:11:52.970 --> 00:11:55.650
So the speed becomes
really important.

00:11:55.650 --> 00:11:59.440
Here are some tips how you
could improve the performance

00:11:59.440 --> 00:12:02.980
especially for real-time
inference on streaming video.

00:12:02.980 --> 00:12:05.106
You can reduce the image
resolution as well as

00:12:05.106 --> 00:12:09.040
the video frame rate to limit
the amount of computation

00:12:09.040 --> 00:12:10.950
it involves to the inference.

00:12:10.950 --> 00:12:13.200
With the current
frame stream process,

00:12:13.200 --> 00:12:16.060
you should also throttle
incoming video frames

00:12:16.060 --> 00:12:18.760
to avoid any backup,
which increases memory

00:12:18.760 --> 00:12:21.330
as well as slow down
the performance.

00:12:21.330 --> 00:12:23.800
For real-time face
detection, you

00:12:23.800 --> 00:12:28.510
should use fast mode, which
likely is the default mode.

00:12:28.510 --> 00:12:31.390
Oftentimes 480 by
360 resolution is

00:12:31.390 --> 00:12:33.602
sufficient for face detection.

00:12:33.602 --> 00:12:36.270
For real-time processing,
you should also

00:12:36.270 --> 00:12:40.650
choose between contour
detection versus classification

00:12:40.650 --> 00:12:44.040
or landmark detection,
but not both because doing

00:12:44.040 --> 00:12:46.470
both could be
expensive and may not

00:12:46.470 --> 00:12:52.071
be fit for real-time
processing in a slow device.

00:12:52.071 --> 00:12:54.610
Another tip and
trick you should use

00:12:54.610 --> 00:12:56.450
is, you should
wait for detection

00:12:56.450 --> 00:12:59.705
to finish before rendering
the face and contour together.

00:13:02.285 --> 00:13:06.060
You can check out our
online QuickStart sample app

00:13:06.060 --> 00:13:08.332
on GitHub for more details.

00:13:08.332 --> 00:13:13.391
To illustrate what I mean,
I will do a live demo.

00:13:13.391 --> 00:13:16.820
Let's switch to the demo mode.

00:13:16.820 --> 00:13:23.150
So for the purpose of demo, I'm
using a slower 3-year-old Nexus

00:13:23.150 --> 00:13:26.450
5X phone, because using
the latest Pixel phone,

00:13:26.450 --> 00:13:29.660
the differences are
not as prominent.

00:13:29.660 --> 00:13:32.450
In the first video,
I'm going to show you

00:13:32.450 --> 00:13:34.730
without any performance tips.

00:13:34.730 --> 00:13:37.670
And we do not do
videos throttling.

00:13:37.670 --> 00:13:41.780
We do not do any drawing to make
sure the contour and the face

00:13:41.780 --> 00:13:42.900
are together.

00:13:42.900 --> 00:13:48.750
So if you just call the
API without any performance

00:13:48.750 --> 00:13:53.570
improvement, you can
see the contours are not

00:13:53.570 --> 00:13:55.300
following the face.

00:13:55.300 --> 00:13:59.910
And there's a big gap
between these two.

00:13:59.910 --> 00:14:04.375
All right, so now I'm going
to switch to another version

00:14:04.375 --> 00:14:07.410
after applying performance tips.

00:14:11.410 --> 00:14:17.640
In this version, it's using
the exact same Nexus 5X phone,

00:14:17.640 --> 00:14:21.030
but we throttle the
incoming video frame

00:14:21.030 --> 00:14:23.590
when we're still processing
the current frame.

00:14:23.590 --> 00:14:25.620
And also we wait
for the detection

00:14:25.620 --> 00:14:30.282
to finish before rendering
both the face and the contour.

00:14:30.282 --> 00:14:34.789
As you can see now, the
contours are following the face

00:14:34.789 --> 00:14:35.330
all the time.

00:14:35.330 --> 00:14:36.567
They match each other.

00:14:36.567 --> 00:14:37.400
There's no more gap.

00:14:41.032 --> 00:14:42.490
So let's switch
back to the slides.

00:14:42.490 --> 00:14:44.164
[APPLAUSE]

00:14:44.164 --> 00:14:45.092
Thank you.

00:14:48.340 --> 00:14:53.230
If you're using our class model
API, how to include a model

00:14:53.230 --> 00:14:55.950
becomes something
you should consider.

00:14:55.950 --> 00:14:57.100
There are two ways.

00:14:57.100 --> 00:14:59.970
You can either bundle
the model inside your app

00:14:59.970 --> 00:15:02.280
or host it on the cloud.

00:15:02.280 --> 00:15:04.635
If you bundle your
model in your app,

00:15:04.635 --> 00:15:07.160
it is available immediately.

00:15:07.160 --> 00:15:09.870
It also does not require
any model downloading.

00:15:09.870 --> 00:15:11.550
So it works offline.

00:15:11.550 --> 00:15:14.130
But the downside is
you get a bigger app

00:15:14.130 --> 00:15:16.210
because the app
contains the model.

00:15:16.210 --> 00:15:18.390
It may slow down
the app download.

00:15:18.390 --> 00:15:22.260
Also, you cannot change
the model without a new app

00:15:22.260 --> 00:15:24.470
release.

00:15:24.470 --> 00:15:27.950
On the other hand, if you
host the model in the cloud,

00:15:27.950 --> 00:15:30.890
we provide all the
hosting support for you.

00:15:30.890 --> 00:15:34.070
You get a smaller app size
because the app itself does not

00:15:34.070 --> 00:15:36.770
contain the model, which
translates into a faster

00:15:36.770 --> 00:15:38.390
installation.

00:15:38.390 --> 00:15:42.530
You also can choose the download
model only if it's needed.

00:15:42.530 --> 00:15:45.590
The model updates
can come over the air

00:15:45.590 --> 00:15:48.830
into the app without
any new app release.

00:15:48.830 --> 00:15:52.070
You can also use remote config
and A/B testing framework

00:15:52.070 --> 00:15:54.990
provided by the Firebase
to do phase rollout

00:15:54.990 --> 00:15:57.590
as well as control the pilots.

00:15:57.590 --> 00:16:01.010
The drawback of hosting
the model on the cloud

00:16:01.010 --> 00:16:03.950
is obviously it
needs connectivity.

00:16:03.950 --> 00:16:08.270
When there is no connectivity,
you cannot download the model.

00:16:08.270 --> 00:16:09.890
Also the model will
not be available

00:16:09.890 --> 00:16:11.810
until they are downloaded.

00:16:11.810 --> 00:16:15.230
So a third option is
using a hybrid approach.

00:16:15.230 --> 00:16:17.720
You can bundle the
initial version

00:16:17.720 --> 00:16:18.740
of the model in the app.

00:16:18.740 --> 00:16:22.450
So it will make it usable
right away after installation.

00:16:22.450 --> 00:16:23.900
Then you can receive
model updates

00:16:23.900 --> 00:16:25.400
over the air from the cloud.

00:16:28.106 --> 00:16:31.030
If you are using
our base API, it's

00:16:31.030 --> 00:16:32.980
providing in two
different forms.

00:16:32.980 --> 00:16:34.925
For the lack of
better terms, I call

00:16:34.925 --> 00:16:38.736
it thick SDK and the thin SDK.

00:16:38.736 --> 00:16:41.260
In the thin SDK, the
model is actually

00:16:41.260 --> 00:16:43.330
provided by the Google
Play service, which means

00:16:43.330 --> 00:16:44.800
they're shared across all apps.

00:16:44.800 --> 00:16:49.660
The app itself does not have
to contain the model, which

00:16:49.660 --> 00:16:51.690
will make your app smaller.

00:16:51.690 --> 00:16:54.730
Text recognition and the
barcode scanning APIs

00:16:54.730 --> 00:16:58.086
are provided through
the thin SDK.

00:16:58.086 --> 00:17:01.030
The second type is
called thick SDKs.

00:17:01.030 --> 00:17:04.000
The models are
bundled inside SDK.

00:17:04.000 --> 00:17:07.490
Each app will have their
own copy of the model, which

00:17:07.490 --> 00:17:09.380
will increase the app size.

00:17:09.380 --> 00:17:11.900
Face detection
and image labeling

00:17:11.900 --> 00:17:15.300
are supported through
the thick SDKs.

00:17:15.300 --> 00:17:19.369
To use these two types of SDKs
and use various APIs provided

00:17:19.369 --> 00:17:23.450
by the ML Kit, you need to
include the appropriate ML Kit

00:17:23.450 --> 00:17:26.630
dependencies in your
app-level build.gradle file.

00:17:26.630 --> 00:17:30.020
Inside that file-- inside
this dependency section--

00:17:30.020 --> 00:17:33.830
if you want to use the API
support through the thin SDK,

00:17:33.830 --> 00:17:36.890
you should add the dependency
called Firebase-ml-vision.

00:17:40.038 --> 00:17:43.310
And in addition, if you
want to use thick SDK,

00:17:43.310 --> 00:17:46.520
you should still keep this line,
because automation API entry

00:17:46.520 --> 00:17:50.190
points are coming from
this thin SDK dependency.

00:17:50.190 --> 00:17:53.150
But you also need to add
additional dependencies.

00:17:53.150 --> 00:17:58.880
For example, if you want
to use image detection,

00:17:58.880 --> 00:18:03.320
then you need to add the
Firebase-ml-vision face model

00:18:03.320 --> 00:18:07.196
dependency because
it's a thick SDK.

00:18:07.196 --> 00:18:10.930
Similarly, for image labeling
if you're using that feature,

00:18:10.930 --> 00:18:14.011
then you need to add image
label model dependency.

00:18:16.837 --> 00:18:19.170
Next, I will talk
about a few new areas

00:18:19.170 --> 00:18:21.575
we are currently working on.

00:18:21.575 --> 00:18:25.380
A few new ML Kit features
are either under development

00:18:25.380 --> 00:18:27.870
or in early testing phase.

00:18:27.870 --> 00:18:31.710
We expanded beyond Vision
starting with natural language

00:18:31.710 --> 00:18:33.990
processing with Smart Reply.

00:18:33.990 --> 00:18:36.450
Smart Reply is a
feature which can

00:18:36.450 --> 00:18:39.180
enable you to produce
meaningful responses based

00:18:39.180 --> 00:18:41.910
on the current
conversation context.

00:18:41.910 --> 00:18:46.160
We are also planning to go
into other areas, like speech.

00:18:46.160 --> 00:18:49.440
At the same time, we'll
continue to enhance performance

00:18:49.440 --> 00:18:52.724
and accuracy of base APIs.

00:18:52.724 --> 00:18:56.260
We launched model compression
and conversion service

00:18:56.260 --> 00:19:00.640
to our alpha users, which
help them to convert

00:19:00.640 --> 00:19:04.210
and compress large models into
smaller and faster versions

00:19:04.210 --> 00:19:05.590
for mobile usage.

00:19:05.590 --> 00:19:09.170
The conversion service,
currently [? saying ?] alpha,

00:19:09.170 --> 00:19:13.210
uses pruning quantization,
distillation,

00:19:13.210 --> 00:19:16.930
as well as to transfer learning
to retrain the large models

00:19:16.930 --> 00:19:19.990
and make them smaller and
faster without sacrificing

00:19:19.990 --> 00:19:22.288
too much accuracy.

00:19:22.288 --> 00:19:25.180
Fishbrain is a
community-based app.

00:19:25.180 --> 00:19:29.080
It enables the user to share
the photos of their catch

00:19:29.080 --> 00:19:31.350
within their social networks.

00:19:31.350 --> 00:19:35.020
With the machine learning
model, it can identify any fish

00:19:35.020 --> 00:19:36.670
with just a photo.

00:19:36.670 --> 00:19:39.700
When they first came
to us, their model

00:19:39.700 --> 00:19:43.300
is more than 80 megabytes.

00:19:43.300 --> 00:19:46.310
By using our conversion
and compression service,

00:19:46.310 --> 00:19:50.230
they're able to trim down
the model to under 1 meg.

00:19:50.230 --> 00:19:55.000
As you can see, they not only
maintain same-level accuracy,

00:19:55.000 --> 00:19:58.350
it's actually slightly better.

00:19:58.350 --> 00:20:01.370
If you are interested in trying
out our model compression

00:20:01.370 --> 00:20:05.780
service, please join our alpha
program by signing up today

00:20:05.780 --> 00:20:10.726
at g.co/firebase/signup.

00:20:10.726 --> 00:20:13.240
No matter if you are
new to machine learning

00:20:13.240 --> 00:20:17.410
or experienced AI expert, I
hope you enjoyed the talk today

00:20:17.410 --> 00:20:19.420
and can take home some tips.

00:20:19.420 --> 00:20:23.414
And I can't wait to see what
you will build with ML Kit.

00:20:23.414 --> 00:20:25.330
If you have any questions,
I will wait outside

00:20:25.330 --> 00:20:27.040
in the [? Ask Android ?] lounge.

00:20:27.040 --> 00:20:28.954
And we also have
office hours tomorrow.

00:20:28.954 --> 00:20:30.370
Thank you very
much for listening.

00:20:30.370 --> 00:20:33.420
[MUSIC PLAYING]

