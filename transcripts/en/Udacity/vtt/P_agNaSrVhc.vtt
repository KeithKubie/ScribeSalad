WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:01.630
So the way I like to think about it,

00:00:01.630 --> 00:00:05.290
is there's three main families of
reinforcement learning algorithms.

00:00:05.290 --> 00:00:07.260
So, the one that's most like
what we've talked about so

00:00:07.260 --> 00:00:10.760
far is what we could call model based
reinforcement learning algorithm.

00:00:10.760 --> 00:00:14.610
So, model based reinforcement learning
algorithms takes the state action reward

00:00:14.610 --> 00:00:16.610
tupples that it gets, and

00:00:16.610 --> 00:00:21.349
sends them to a model learner, which
learns the transitions and rewards.

00:00:22.498 --> 00:00:24.580
Those transition and rewards,
once you've learned them,

00:00:24.580 --> 00:00:27.700
you can put through an MDPSolver,
like we talked about,

00:00:27.700 --> 00:00:32.170
which could be used to spit out a Q*,
a optimal value function.

00:00:32.170 --> 00:00:35.850
And once you have the optimal
value function, you can use,

00:00:35.850 --> 00:00:38.920
just by taking the argmax of the state
that you are in, you can choose which

00:00:38.920 --> 00:00:42.100
action you should take in any given
state and that gives you the policy.

00:00:42.100 --> 00:00:45.630
So it's still mapping the state action
reward sequence to policies, but

00:00:45.630 --> 00:00:48.210
it's doing it by creating all these
intermediate values in between.

00:00:52.180 --> 00:00:53.440
Cool, and
let me just add one more thing,

00:00:53.440 --> 00:00:57.980
which is that the model learner
takes the history that it's seeing.

00:00:57.980 --> 00:01:00.832
But it also takes its current estimate
of transition to rewards to produce

00:01:00.832 --> 00:01:02.582
the new estimate of
the transition rewards.

00:01:02.582 --> 00:01:05.769
So, this is how I want to kind of
represent the learning piece of

00:01:05.769 --> 00:01:06.578
the sequence.

00:01:06.578 --> 00:01:07.370
Does that make sense?

00:01:08.650 --> 00:01:09.820
All right, so this is one type.

00:01:09.820 --> 00:01:11.980
Now let me show you another type.

00:01:11.980 --> 00:01:14.770
So, the second class of
reinforcement learning

00:01:14.770 --> 00:01:16.510
algorithm that's
important to think about.

00:01:16.510 --> 00:01:19.568
They're referred to as
value-function-based, or actually,

00:01:19.568 --> 00:01:20.800
sometimes model-free.

00:01:20.800 --> 00:01:22.743
So, the beginning and
the end of it are still the same.

00:01:22.743 --> 00:01:26.964
We're taking sequences of state action
rewards and producing a policy and

00:01:26.964 --> 00:01:30.247
we even have this Q* in between
that we generate the policy

00:01:30.247 --> 00:01:31.588
from using the argmax.

00:01:31.588 --> 00:01:35.328
But now, instead of feeding back
the transitions and rewards,

00:01:35.328 --> 00:01:40.107
we're actually feeding back Q*, and we
have a direct value update equation that

00:01:40.107 --> 00:01:45.000
takes state action reward that it just
experienced, the current estimate of Q*.

00:01:45.000 --> 00:01:48.710
Actually, it's kind of more Q than Q*,

00:01:48.710 --> 00:01:52.719
and uses that to generate a new Q,
which is then used to generate a policy.

00:01:55.427 --> 00:01:58.278
Yeah.
So instead of explicitly building

00:01:58.278 --> 00:02:03.007
a model and using it,
it just somehow directly learns the Q

00:02:03.007 --> 00:02:06.303
values from state actions and rewards.

00:02:06.303 --> 00:02:09.794
And then the third class of
reinforcement learning algorithms comes

00:02:09.794 --> 00:02:13.405
from the idea that you can sometimes
actually take the policy itself and

00:02:13.405 --> 00:02:17.197
feed that back to a policy update that
directly modifies the policy based on

00:02:17.197 --> 00:02:19.450
the state action rewards
that you receive.

00:02:19.450 --> 00:02:22.960
So, in some sense, this is much, much
more direct, but the learning problem is

00:02:22.960 --> 00:02:26.380
very difficult because the kind of
feedback that you're getting about

00:02:26.380 --> 00:02:30.700
the policy isn't really very useful for
directly modifying the policy.

00:02:30.700 --> 00:02:33.789
So, here's these three models all
together so you can compare and

00:02:33.789 --> 00:02:34.351
contrast.

00:02:34.351 --> 00:02:35.029
You can see,

00:02:35.029 --> 00:02:39.105
in some sense, they're getting simpler
as [LAUGH] we go down to policy search.

00:02:39.105 --> 00:02:42.968
But you could also say that
the learning is more direct.

00:02:42.968 --> 00:02:48.290
And as we go up this way, the learning
problems become more supervised

00:02:48.290 --> 00:02:53.270
in the sense that you can imagine
learning to predict next dates and next

00:02:53.270 --> 00:02:57.800
rewards from previous dates and previous
rewards to learn T/R pretty directly.

00:02:57.800 --> 00:03:00.010
It's basically a supervised
learning problem.

00:03:00.010 --> 00:03:02.611
You get to see what
the output's supposed to be.

00:03:02.611 --> 00:03:05.905
Here, you don't quite get to see what
the output's supposed to be, but

00:03:05.905 --> 00:03:08.769
you do get values that you can
imagine propagating backwards.

00:03:08.769 --> 00:03:11.336
And here,
you get very little useful feedback,

00:03:11.336 --> 00:03:14.655
in terms of how to change your
policy to make it better.

00:03:14.655 --> 00:03:19.610
&gt;&gt; Wait, so, the way you put that
suggests that you should always do one.

00:03:19.610 --> 00:03:20.930
Well, so, yeah, but

00:03:20.930 --> 00:03:23.680
look at all the extra stuff
that you're doing in between.

00:03:23.680 --> 00:03:26.820
So, in fact, different problems
have different trade offs,

00:03:26.820 --> 00:03:30.410
as to whether or not which of
these is the best thing to do.

00:03:30.410 --> 00:03:32.540
A lot of the research has been
focused on number two, and

00:03:32.540 --> 00:03:35.770
that's what we're going to be diving
into, because in some ways it strikes

00:03:35.770 --> 00:03:39.630
a nice balance between keeping
the computations relatively simple and

00:03:39.630 --> 00:03:42.000
keeping the learning
updates relatively simple.

00:03:42.000 --> 00:03:44.040
But there are plenty of cases
where you'd rather do one and

00:03:44.040 --> 00:03:44.880
where you'd rather do three.

00:03:46.580 --> 00:03:48.310
&gt;&gt; I look forward to you
convincing me that that's true.

