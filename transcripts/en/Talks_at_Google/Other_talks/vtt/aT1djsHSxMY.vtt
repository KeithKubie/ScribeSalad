WEBVTT
Kind: captions
Language: en

00:00:09.100 --> 00:00:11.460
RICHARD: Hi, welcome
to Talks at Google

00:00:11.460 --> 00:00:13.640
in Cambridge, Massachusetts.

00:00:13.640 --> 00:00:17.980
We have a very important
topic for us today.

00:00:17.980 --> 00:00:21.450
Those of us in the high
tech community, of course,

00:00:21.450 --> 00:00:26.380
are oriented towards thinking
of technology as a good thing--

00:00:26.380 --> 00:00:31.110
something that brings
benefits to people.

00:00:31.110 --> 00:00:33.420
But it's not always that way.

00:00:33.420 --> 00:00:36.670
And it's our responsibility
to also think

00:00:36.670 --> 00:00:40.420
about what it means to
deploy that technology.

00:00:40.420 --> 00:00:42.670
Is it really for
good or for not.

00:00:42.670 --> 00:00:46.150
Is it good for
all people or does

00:00:46.150 --> 00:00:48.800
it have different effects on
different segments of society

00:00:48.800 --> 00:00:51.070
and who gets to
decide these issues.

00:00:51.070 --> 00:00:53.530
Is it just us as technologists.

00:00:53.530 --> 00:00:57.820
To what extent is there a
role for a broader public.

00:00:57.820 --> 00:01:00.580
So Professor Sheila
Jasanoff has written

00:01:00.580 --> 00:01:03.910
a very intelligent
and thoughtful book

00:01:03.910 --> 00:01:04.640
on these issues.

00:01:04.640 --> 00:01:09.239
And so we're very pleased
to have her here to speak.

00:01:09.239 --> 00:01:10.030
Professor Jasanoff.

00:01:14.140 --> 00:01:15.640
SHEILA JASANOFF:
Thank you, Richard.

00:01:15.640 --> 00:01:17.060
It's a pleasure to be here.

00:01:17.060 --> 00:01:20.256
And thank you all for
coming during the aftermath

00:01:20.256 --> 00:01:22.410
of the lunch hour.

00:01:22.410 --> 00:01:26.080
So let me begin by telling you
that this where this book came

00:01:26.080 --> 00:01:29.390
from and where it sits in
relation to the sorts of things

00:01:29.390 --> 00:01:34.530
I normally do and then take
you through pieces of it that

00:01:34.530 --> 00:01:38.150
are maybe the most removed from
what the people in this room

00:01:38.150 --> 00:01:39.560
ordinarily do.

00:01:39.560 --> 00:01:41.430
Because although
there is a chapter

00:01:41.430 --> 00:01:45.280
in the book on information and
communication technologies,

00:01:45.280 --> 00:01:48.630
I'm not specifically going to
tell you about that chapter.

00:01:48.630 --> 00:01:51.910
If you're interested, you
can always read the book.

00:01:51.910 --> 00:01:57.000
So I write for a very diverse
interdisciplinary audience

00:01:57.000 --> 00:02:01.420
because my work sits at
the intersection of law,

00:02:01.420 --> 00:02:05.200
social sciences, and
science and technology.

00:02:05.200 --> 00:02:07.260
But I've never written
for a trade press

00:02:07.260 --> 00:02:09.960
and never written for a
totally general audience.

00:02:09.960 --> 00:02:12.830
But Norton was
beginning a series

00:02:12.830 --> 00:02:17.030
on ethics connected with
various public issues.

00:02:17.030 --> 00:02:20.250
They were originally doing
this in collaboration

00:02:20.250 --> 00:02:22.820
with a non-governmental
organization.

00:02:22.820 --> 00:02:24.600
And the people at
Norton were not

00:02:24.600 --> 00:02:26.940
thinking about science
and technology.

00:02:26.940 --> 00:02:30.000
But this NGO partner that
they were working with

00:02:30.000 --> 00:02:34.750
said that they wanted something
about the ethics of technology.

00:02:34.750 --> 00:02:38.420
And then this was translated
to me by the Norton editor

00:02:38.420 --> 00:02:40.230
as meaning risk.

00:02:40.230 --> 00:02:43.670
Now, I didn't want to write
a book exactly about risk.

00:02:43.670 --> 00:02:46.710
I think I agonized more
about the title of this book

00:02:46.710 --> 00:02:51.726
than I have of any of my
other dozen plus books.

00:02:51.726 --> 00:02:56.200
So it became something of
an exploration of the way

00:02:56.200 --> 00:03:01.360
decisions are made and
inclusion issues are worked out

00:03:01.360 --> 00:03:04.490
at the frontiers of technology.

00:03:04.490 --> 00:03:09.030
But to do anything like this,
one has to begin somewhere.

00:03:09.030 --> 00:03:10.940
That is, there's a
lot of [? vogue ?]

00:03:10.940 --> 00:03:14.130
if we're talking about new
and emerging technologies.

00:03:14.130 --> 00:03:17.130
In fact, there's a whole
society given the name

00:03:17.130 --> 00:03:22.220
SNET, the Society for Nano
and Emerging Technologies.

00:03:22.220 --> 00:03:24.260
But you can't really
say anything sensible

00:03:24.260 --> 00:03:26.450
unless you know something
about the history of where

00:03:26.450 --> 00:03:27.400
this came from.

00:03:27.400 --> 00:03:30.900
So in the half hour or
so that I'll talk to you,

00:03:30.900 --> 00:03:34.000
I'll take you a little
bit through the history

00:03:34.000 --> 00:03:37.860
of American efforts to
grapple with the value

00:03:37.860 --> 00:03:40.260
dimensions of science
and technology

00:03:40.260 --> 00:03:42.800
and then go into
what I see as some

00:03:42.800 --> 00:03:44.370
of the challenges of today.

00:03:44.370 --> 00:03:46.520
Because I think
that in this century

00:03:46.520 --> 00:03:50.120
the kinds of ethical
and value questions,

00:03:50.120 --> 00:03:52.600
political questions
that we're confronting

00:03:52.600 --> 00:03:54.760
are different in
all kinds of ways.

00:03:54.760 --> 00:03:56.330
And I'm hoping
that there be time

00:03:56.330 --> 00:03:59.450
for a discussion in
which I get to hear

00:03:59.450 --> 00:04:02.310
from you what strikes
you as interesting,

00:04:02.310 --> 00:04:05.520
wrong, misguided, whatever.

00:04:05.520 --> 00:04:09.020
So the little roadmap.

00:04:09.020 --> 00:04:11.080
People tend to
think that the tech

00:04:11.080 --> 00:04:12.890
world just sort of arrived.

00:04:12.890 --> 00:04:16.720
I mean, I still remember when
my daughter first taught me

00:04:16.720 --> 00:04:18.959
how to quote, "Google", unquote.

00:04:18.959 --> 00:04:21.839
That's within living memory.

00:04:21.839 --> 00:04:24.990
And so people forget
that invention has

00:04:24.990 --> 00:04:26.520
been with us for a long time.

00:04:26.520 --> 00:04:30.680
And so many of the things that
we think about in connection

00:04:30.680 --> 00:04:33.810
with invention are
not in themselves new,

00:04:33.810 --> 00:04:39.560
but invention happens inside of
a society, embedded in society.

00:04:39.560 --> 00:04:43.760
And as history
changes, things change.

00:04:43.760 --> 00:04:47.750
And the question as invention
speeds up to some extent

00:04:47.750 --> 00:04:50.940
as it's felt to be doing at the
moment, the question of what

00:04:50.940 --> 00:04:54.640
we want, what we want to
retain of the past world, what

00:04:54.640 --> 00:04:57.250
we want to give up--
these things also

00:04:57.250 --> 00:04:59.220
change to some degree.

00:04:59.220 --> 00:05:01.860
People talk about unintended
consequences a lot.

00:05:01.860 --> 00:05:04.610
In the Q&amp;A, people are
interested in why I really

00:05:04.610 --> 00:05:05.800
dislike that term.

00:05:05.800 --> 00:05:07.490
We can go into that.

00:05:07.490 --> 00:05:11.240
I prefer to talk about
undesired consequences.

00:05:11.240 --> 00:05:13.730
And then the question
that Richard has already

00:05:13.730 --> 00:05:16.840
brought up-- if there are all
these questions, than there

00:05:16.840 --> 00:05:17.920
are answers.

00:05:17.920 --> 00:05:21.170
And who is in the
position of authority

00:05:21.170 --> 00:05:24.710
to be giving answers that should
be binding to other people.

00:05:24.710 --> 00:05:26.500
So that's a rough
structure of what

00:05:26.500 --> 00:05:28.820
I want to talk to you about.

00:05:28.820 --> 00:05:31.930
So when I say we've always
been inventors, of course,

00:05:31.930 --> 00:05:35.540
we can go back into
the depths of mythology

00:05:35.540 --> 00:05:40.550
to find that people have been
thinking about invention,

00:05:40.550 --> 00:05:44.110
power, the connection
between inventors

00:05:44.110 --> 00:05:47.160
and states for a very long time.

00:05:47.160 --> 00:05:50.440
Daedalus, seen in
Western mythology

00:05:50.440 --> 00:05:53.330
as the primal
figure illustrating

00:05:53.330 --> 00:05:59.250
what the very processes of
engineering might be about,

00:05:59.250 --> 00:06:01.670
was in the employ of King Midas.

00:06:01.670 --> 00:06:04.910
So there was a very close
hand and glove relationship

00:06:04.910 --> 00:06:11.290
between the ruler of the state
and the person whose ingenuity

00:06:11.290 --> 00:06:12.860
he was enrolling.

00:06:12.860 --> 00:06:16.000
And we all know the myth
of Icarus and the fact

00:06:16.000 --> 00:06:19.250
that things didn't always
work out in the right way,

00:06:19.250 --> 00:06:21.070
although it's
questionable whether it

00:06:21.070 --> 00:06:23.240
was the technology that
failed or whether it

00:06:23.240 --> 00:06:26.030
was human error-- it's
usually represented

00:06:26.030 --> 00:06:29.630
as having been Icarus'
problem and not the problem

00:06:29.630 --> 00:06:33.120
that wax was the material
that was being used.

00:06:33.120 --> 00:06:36.280
So we have all of
these kinds of stories

00:06:36.280 --> 00:06:38.660
that we've grown up with.

00:06:38.660 --> 00:06:40.580
And when I say we--
it's interesting

00:06:40.580 --> 00:06:43.090
that when my father was
teaching me English,

00:06:43.090 --> 00:06:46.160
one of the first pieces
of English language

00:06:46.160 --> 00:06:49.140
prose that I remember
was him making

00:06:49.140 --> 00:06:52.040
me read a story about Icarus.

00:06:52.040 --> 00:06:53.980
And I still remember
the sentence,

00:06:53.980 --> 00:06:56.640
"Icarus flew too near
the sun and the heat

00:06:56.640 --> 00:06:59.000
of the sun melted the wax."

00:06:59.000 --> 00:07:01.880
I mean, this is embedded
in my five-year-old brain,

00:07:01.880 --> 00:07:03.350
as what I was taught.

00:07:03.350 --> 00:07:08.750
This is what being
post-colonial means, in a sense.

00:07:08.750 --> 00:07:12.880
So technology has been,
as the Icarus illustrates,

00:07:12.880 --> 00:07:15.230
the dream about liberation.

00:07:15.230 --> 00:07:18.050
Not being wedded
to various things.

00:07:18.050 --> 00:07:21.830
And all of you are deeply
familiar with ideas

00:07:21.830 --> 00:07:22.870
like singularity.

00:07:22.870 --> 00:07:28.720
The singularity that imagine
us being not interestingly

00:07:28.720 --> 00:07:33.600
situated-- no two people sitting
exactly next to each other

00:07:33.600 --> 00:07:34.680
or very few.

00:07:34.680 --> 00:07:36.890
And sort of dotted about.

00:07:36.890 --> 00:07:40.770
But maybe it's because the union
that we imagine among people

00:07:40.770 --> 00:07:43.460
is a different kind
of one not constrained

00:07:43.460 --> 00:07:45.130
by physical presences.

00:07:45.130 --> 00:07:47.360
So what we want to
be liberated from

00:07:47.360 --> 00:07:50.290
has also changed over
the years and that

00:07:50.290 --> 00:07:53.750
raises new kinds of questions
for ethics and governance.

00:07:53.750 --> 00:07:56.830
And what I was saying is
that make sense of the new,

00:07:56.830 --> 00:08:01.080
you kind of have to
delve back into the old.

00:08:01.080 --> 00:08:05.210
So a moment of old in American
science and technology policy

00:08:05.210 --> 00:08:09.760
begins shortly after World War
II with, among other things,

00:08:09.760 --> 00:08:12.990
the formation of our National
Science Foundation in 1950.

00:08:12.990 --> 00:08:15.170
But the National
Science Foundation

00:08:15.170 --> 00:08:18.560
grew out of the experience
of the wartime years

00:08:18.560 --> 00:08:21.890
in the kind of partnership
that the federal government

00:08:21.890 --> 00:08:24.810
had built up with
scientists and engineers,

00:08:24.810 --> 00:08:29.330
very prominently including those
in this particular geographical

00:08:29.330 --> 00:08:30.360
area.

00:08:30.360 --> 00:08:35.150
So a quotation that's often
used to be a kind of emblem

00:08:35.150 --> 00:08:39.280
of the spirit of technological
entrepreneurship in that period

00:08:39.280 --> 00:08:41.730
is this from one of
the first commissioners

00:08:41.730 --> 00:08:46.070
of the Atomic Energy
Commission, Lewis Strauss.

00:08:46.070 --> 00:08:50.680
And he imagines nuclear power
as producing this phrase

00:08:50.680 --> 00:08:53.640
that many people actually
know-- "electrical energy too

00:08:53.640 --> 00:08:54.990
cheap to meter."

00:08:54.990 --> 00:08:57.560
But it's the rest
of the sentence that

00:08:57.560 --> 00:09:00.090
catches my attention
because it's

00:09:00.090 --> 00:09:02.790
a liberationist dream--
"People will know

00:09:02.790 --> 00:09:05.340
of great periodic
regional famines

00:09:05.340 --> 00:09:08.820
only as matters of history
will travel effortlessly

00:09:08.820 --> 00:09:13.140
of overseas and under them,
through the air with a minimum

00:09:13.140 --> 00:09:15.510
of danger and at great speeds."

00:09:15.510 --> 00:09:18.680
So this is being said in 1954.

00:09:18.680 --> 00:09:23.070
I came to the US in 1956
and it took 17 hours

00:09:23.070 --> 00:09:25.740
to come from London to New York.

00:09:25.740 --> 00:09:29.190
So you know, civil aviation
was in a different state.

00:09:29.190 --> 00:09:31.900
The question of speed
meant something different

00:09:31.900 --> 00:09:33.350
at that time.

00:09:33.350 --> 00:09:37.290
Letters didn't go
back and forth in what

00:09:37.290 --> 00:09:42.140
we would consider to be normal,
let alone at electronic speeds.

00:09:42.140 --> 00:09:46.200
So one can look at
this kind of quotation

00:09:46.200 --> 00:09:50.970
and see in it a set
of visions about what

00:09:50.970 --> 00:09:57.110
the state imagined itself
doing in this modernist period.

00:09:57.110 --> 00:09:58.130
There would be progress.

00:09:58.130 --> 00:10:01.210
Science and technology would
be used for human betterment.

00:10:01.210 --> 00:10:05.030
And Vannecar Bush, who was
an MIT professor but was

00:10:05.030 --> 00:10:08.000
FDR's as de-facto
science advisor

00:10:08.000 --> 00:10:10.240
wrote "Science, the
Endless Frontier",

00:10:10.240 --> 00:10:15.930
he imagined that this was
indeed an endless frontier.

00:10:15.930 --> 00:10:18.840
But at that frontier,
you would get efficiency

00:10:18.840 --> 00:10:21.070
in the energy too
cheap to meter.

00:10:21.070 --> 00:10:24.290
And people were writing
about how things could

00:10:24.290 --> 00:10:25.880
be made extremely efficient.

00:10:25.880 --> 00:10:28.870
There was this very popular book
called "Cheaper by the Dozen"

00:10:28.870 --> 00:10:33.000
about how a guy had-- well,
two efficiency experts, a man

00:10:33.000 --> 00:10:35.340
and his wife-- had
had a dozen children

00:10:35.340 --> 00:10:38.670
and they could manage things
much more readily for 12

00:10:38.670 --> 00:10:41.010
children than for only one.

00:10:41.010 --> 00:10:45.050
Accessibility, that's all
over Lewis Strauss' statement

00:10:45.050 --> 00:10:47.510
that I quoted for you.

00:10:47.510 --> 00:10:48.490
Eradication.

00:10:48.490 --> 00:10:50.120
The way you get
rid of bad things

00:10:50.120 --> 00:10:52.500
is you just-- it's
like your iPhone

00:10:52.500 --> 00:10:55.300
where you just flick the
screen and it goes away.

00:10:55.300 --> 00:10:57.700
And this was the
sort of imagination

00:10:57.700 --> 00:11:04.700
of how technology would liberate
us from the scourges of hunger

00:11:04.700 --> 00:11:07.165
and disease but also
from physical barriers--

00:11:07.165 --> 00:11:11.040
the over the seas and
under them kind of point.

00:11:11.040 --> 00:11:14.060
And also, a point that's not
made much of-- that all of this

00:11:14.060 --> 00:11:16.110
would somehow be transparent.

00:11:16.110 --> 00:11:20.490
Not with governments
sequestering information.

00:11:20.490 --> 00:11:23.230
So this is an ideal that
has persisted through

00:11:23.230 --> 00:11:24.885
to the present in some ways.

00:11:27.440 --> 00:11:31.250
I think of this as somewhat
the vision of one worldism--

00:11:31.250 --> 00:11:34.240
that everybody was
united in wanting

00:11:34.240 --> 00:11:36.710
exactly these sorts of things.

00:11:36.710 --> 00:11:38.850
And there's a little
piece of folk culture

00:11:38.850 --> 00:11:41.110
that no one in
this room will know

00:11:41.110 --> 00:11:46.010
because it was the poem that
was the motto of the UN Women's

00:11:46.010 --> 00:11:49.990
Guild that I like to put as
a bracket on the other side

00:11:49.990 --> 00:11:53.130
of Lewis Strauss because
it expect expresses,

00:11:53.130 --> 00:11:55.600
in a completely
different register, some

00:11:55.600 --> 00:11:59.430
of the same high modernist
dreams of what progress

00:11:59.430 --> 00:12:00.650
is about.

00:12:00.650 --> 00:12:04.900
So this is the UN Women's
Guild poem, the motto.

00:12:04.900 --> 00:12:07.900
"There shall be peace on Earth,
but not until all children

00:12:07.900 --> 00:12:08.910
daily eat their fill.

00:12:08.910 --> 00:12:12.700
Does the hunger
eradication go warmly clad.

00:12:12.700 --> 00:12:15.370
It should be against
the winter wind.

00:12:15.370 --> 00:12:18.980
And thus released from
hunger, fear, and need."

00:12:18.980 --> 00:12:27.060
There's the eradication myth and
then a unity and universalism

00:12:27.060 --> 00:12:29.360
at the social level, as well.

00:12:29.360 --> 00:12:32.530
So this was by a an
anonymous American poet

00:12:32.530 --> 00:12:34.440
written in the 1950s.

00:12:34.440 --> 00:12:38.290
And it started appearing
on every piece of stuff

00:12:38.290 --> 00:12:42.610
that the UN Women's Guild used
to sell-- the linen tea towels

00:12:42.610 --> 00:12:44.800
and various things of that sort.

00:12:44.800 --> 00:12:47.520
But at the sort of high
governmental level,

00:12:47.520 --> 00:12:50.010
you have a person like
Lewis Strauss embodying

00:12:50.010 --> 00:12:53.970
a set of visions and at the
sort of popular culture level

00:12:53.970 --> 00:12:57.570
almost, you have the UN
Women's Guild talking

00:12:57.570 --> 00:12:59.720
about these sorts of things.

00:12:59.720 --> 00:13:02.770
Thematically, it's the same.

00:13:02.770 --> 00:13:05.110
It's the imagination
of a world united

00:13:05.110 --> 00:13:07.950
through a particular
kind of progress that

00:13:07.950 --> 00:13:12.570
has gotten rid of ancient
evils of one sort or another.

00:13:12.570 --> 00:13:14.190
So where are we now.

00:13:14.190 --> 00:13:16.230
So people sometimes
call this the era

00:13:16.230 --> 00:13:18.930
of the convergent technologies.

00:13:18.930 --> 00:13:21.640
And again, we can point
to historical moments

00:13:21.640 --> 00:13:23.890
and documents that
sort of embody where

00:13:23.890 --> 00:13:26.490
these ideas come into being.

00:13:26.490 --> 00:13:29.610
In this case, it's a
governmental author,

00:13:29.610 --> 00:13:32.560
Bainbridge, who wrote a report
from the National Science

00:13:32.560 --> 00:13:36.920
Foundation that's often taken
as a sort of originary moment

00:13:36.920 --> 00:13:41.620
for talking about NBIC,
which are nanotechnology,

00:13:41.620 --> 00:13:44.260
biotechnology,
information technology,

00:13:44.260 --> 00:13:46.200
and cognitive science.

00:13:46.200 --> 00:13:51.250
And you see that there's a whole
list of areas of application

00:13:51.250 --> 00:13:54.060
where it's no longer a
science in the abstract,

00:13:54.060 --> 00:13:57.310
no longer a particular
technology-- but everything

00:13:57.310 --> 00:14:01.270
coming together in this
moment of convergence.

00:14:01.270 --> 00:14:04.080
And it's a very ambitious idea.

00:14:04.080 --> 00:14:05.800
So the National
Science Foundation

00:14:05.800 --> 00:14:09.900
is going to put its money behind
these convergent technologies

00:14:09.900 --> 00:14:13.590
and you're going to get
scaler improvements,

00:14:13.590 --> 00:14:16.480
of a sort, that people
had not imagined before.

00:14:16.480 --> 00:14:19.030
So just going back
to that passage,

00:14:19.030 --> 00:14:22.960
one can drag out a bit
different scales of intervention

00:14:22.960 --> 00:14:26.930
at which technology is supposed
to be working say human health.

00:14:26.930 --> 00:14:29.910
All of human health,
human cognition.

00:14:29.910 --> 00:14:32.770
Not one mind, but
human cognition.

00:14:32.770 --> 00:14:33.970
Human communication.

00:14:33.970 --> 00:14:35.980
So it's the entire
species that's

00:14:35.980 --> 00:14:39.550
going to be made better
through these improvements

00:14:39.550 --> 00:14:41.830
and the outcomes
are going to benefit

00:14:41.830 --> 00:14:46.370
particular groups, particular
societies, national security,

00:14:46.370 --> 00:14:47.070
as mentioned.

00:14:47.070 --> 00:14:49.550
So certainly, at
the national level.

00:14:49.550 --> 00:14:52.730
And through unifying
science and education,

00:14:52.730 --> 00:14:54.470
we're going to
get a sort of bump

00:14:54.470 --> 00:14:58.070
up in the capabilities
of the society

00:14:58.070 --> 00:15:01.200
to do all of these things
through the convergent

00:15:01.200 --> 00:15:04.260
technology lens.

00:15:04.260 --> 00:15:08.520
So one can then stop and
say, OK, so technologically,

00:15:08.520 --> 00:15:11.980
we are now in a
formal ambitious era.

00:15:11.980 --> 00:15:14.480
We're talking about
benefiting everybody

00:15:14.480 --> 00:15:17.140
at the sort of elevated scales.

00:15:17.140 --> 00:15:19.040
And we're talking
about doing it moreover

00:15:19.040 --> 00:15:22.140
through a convergence among
technological frontiers

00:15:22.140 --> 00:15:25.290
that previously might not
have been in contact with one

00:15:25.290 --> 00:15:26.260
another.

00:15:26.260 --> 00:15:29.010
So one can step back
and say, well fine,

00:15:29.010 --> 00:15:32.350
there will be consequences
that we may not be happy about

00:15:32.350 --> 00:15:38.990
and what tools do we have to get
to at these kinds of frontiers

00:15:38.990 --> 00:15:41.090
and make sure that
they're evolving

00:15:41.090 --> 00:15:46.640
to the benefit of the societies
they're intended to serve.

00:15:46.640 --> 00:15:50.220
So in public policy,
you can step back

00:15:50.220 --> 00:15:52.010
and say, well,
there are these sort

00:15:52.010 --> 00:15:55.190
of standard models
for how you go

00:15:55.190 --> 00:15:57.880
about regulating
undesired consequences

00:15:57.880 --> 00:16:00.710
or regulating to prevent
them or manage them

00:16:00.710 --> 00:16:02.360
at one level or another.

00:16:02.360 --> 00:16:04.870
And to sort of
paradigmatic approaches

00:16:04.870 --> 00:16:08.080
can be separated a
little bit from another.

00:16:08.080 --> 00:16:10.660
One is about risks--
familiar, I'm sure,

00:16:10.660 --> 00:16:12.190
to everybody in this room.

00:16:12.190 --> 00:16:16.600
But you can parse out what
the key elements of the risk

00:16:16.600 --> 00:16:18.240
paradigm might be.

00:16:18.240 --> 00:16:21.150
So first of all,
there is, in the risk

00:16:21.150 --> 00:16:24.890
based way of going
about doing regulation,

00:16:24.890 --> 00:16:27.180
a kind of built in
commitment to the idea

00:16:27.180 --> 00:16:30.150
of technological progress
the things will evolve

00:16:30.150 --> 00:16:33.600
and the way you need to manage
them is through regulation.

00:16:33.600 --> 00:16:38.030
So risk, by definition, focuses
on the harmful consequences

00:16:38.030 --> 00:16:42.200
and says, OK, things will
have to evolve as they will.

00:16:42.200 --> 00:16:45.890
What we need to stay focused
on is the bad consequences

00:16:45.890 --> 00:16:47.980
and we need to prevent them.

00:16:47.980 --> 00:16:51.720
And all we need for that is
the right kinds of expertise.

00:16:51.720 --> 00:16:54.840
So we have risk assessment,
we have other expert based

00:16:54.840 --> 00:16:59.860
evaluations, and then we can
set in place policies of control

00:16:59.860 --> 00:17:03.250
that are going to make sure
that these risks are somehow

00:17:03.250 --> 00:17:06.060
contained-- that
they do not escape.

00:17:06.060 --> 00:17:10.910
And this word, containment, is
all of the policy discourse,

00:17:10.910 --> 00:17:13.650
whether you're talking
about nuclear or bio

00:17:13.650 --> 00:17:16.010
or informational risks
of different sorts

00:17:16.010 --> 00:17:18.220
that we can come back to.

00:17:18.220 --> 00:17:21.990
The other major paradigm
begins somewhere else

00:17:21.990 --> 00:17:23.550
and is focused on rights.

00:17:23.550 --> 00:17:26.589
And this one says
that what we should

00:17:26.589 --> 00:17:29.170
begin with is what human
beings need and want

00:17:29.170 --> 00:17:31.650
and make sure that whatever
we're doing in public policy

00:17:31.650 --> 00:17:35.320
and regulation-- it is
centered on developing

00:17:35.320 --> 00:17:37.490
these capabilities
of people, the ones

00:17:37.490 --> 00:17:39.390
that we want to protect.

00:17:39.390 --> 00:17:44.030
And this is focused on
freedoms and undue constraints.

00:17:44.030 --> 00:17:45.670
How can we eliminate them.

00:17:45.670 --> 00:17:47.980
This is where you would put
the concern for privacy,

00:17:47.980 --> 00:17:51.200
for instance, although
you could put privacy

00:17:51.200 --> 00:17:54.250
on both sides-- on the risk
side and on the right side

00:17:54.250 --> 00:17:56.480
and we can talk about that.

00:17:56.480 --> 00:17:58.520
And we need new
institutions, therefore,

00:17:58.520 --> 00:18:00.260
to give effect to these rights.

00:18:00.260 --> 00:18:03.450
And we need policies
that liberate people, not

00:18:03.450 --> 00:18:06.148
constrain them.

00:18:06.148 --> 00:18:09.670
Now, the problem with
these NBIC technologies

00:18:09.670 --> 00:18:13.780
is that they don't fit these
regulatory ideas all that well

00:18:13.780 --> 00:18:17.030
partly because what
makes them valuable, what

00:18:17.030 --> 00:18:21.020
makes them powerful is precisely
that they can't be contained.

00:18:21.020 --> 00:18:23.070
Now there's a
historical moment when

00:18:23.070 --> 00:18:26.160
one sees this very clearly--
when the biotechnologies come

00:18:26.160 --> 00:18:27.020
into being.

00:18:27.020 --> 00:18:31.240
And we have a conference
in 1975 at Asilomar

00:18:31.240 --> 00:18:33.410
and the molecular
biologists are debating

00:18:33.410 --> 00:18:35.880
what they're supposed to
do with biotechnology.

00:18:35.880 --> 00:18:38.390
They come up with the
idea of containment

00:18:38.390 --> 00:18:41.020
because they borrow it
from the nuclear industry.

00:18:41.020 --> 00:18:43.220
And they say we need
two kinds of control--

00:18:43.220 --> 00:18:47.700
biological containment
and physical containment.

00:18:47.700 --> 00:18:49.410
Physical containment
to make sure things

00:18:49.410 --> 00:18:50.850
don't escape from the lab.

00:18:50.850 --> 00:18:53.720
Biological containment to
make sure that the organisms

00:18:53.720 --> 00:18:57.820
themselves don't multiply
in ways we don't want.

00:18:57.820 --> 00:18:59.990
And they just have
no imagination

00:18:59.990 --> 00:19:04.450
that what the biotech industry
wants to do is non-containment.

00:19:04.450 --> 00:19:06.790
They want their
crops to be all over.

00:19:06.790 --> 00:19:10.340
They want their anti-Zika
mosquitoes to be all over.

00:19:10.340 --> 00:19:12.470
And so the whole
idea that they enter

00:19:12.470 --> 00:19:16.310
the biotech revolutionary era
with this idea of containment

00:19:16.310 --> 00:19:18.880
is a little ironic
in retrospect.

00:19:18.880 --> 00:19:22.445
And it took them two years to
abandon that as a principle,

00:19:22.445 --> 00:19:25.130
but that they didn't come
up with anything necessarily

00:19:25.130 --> 00:19:26.740
significantly better.

00:19:26.740 --> 00:19:30.040
So the technologies
of this moment

00:19:30.040 --> 00:19:32.870
are characterized by
things that violate

00:19:32.870 --> 00:19:35.630
the notion of containment--
right, left, and center.

00:19:35.630 --> 00:19:38.430
They're notable
because of their powers

00:19:38.430 --> 00:19:42.870
of diffusion and penetration
because of their pervasiveness

00:19:42.870 --> 00:19:45.410
and because of their
evolutionary capability.

00:19:45.410 --> 00:19:47.410
These are not
static technologies,

00:19:47.410 --> 00:19:50.950
you can't box them in and
say undue consequences

00:19:50.950 --> 00:19:54.350
or undesirable consequences
will be mitigated because we've

00:19:54.350 --> 00:19:57.970
erected a box around them.

00:19:57.970 --> 00:20:01.320
So this poses a bunch of
governance challenges.

00:20:01.320 --> 00:20:03.090
It's interesting
that people see even

00:20:03.090 --> 00:20:05.630
the history of
regulating biotechnology

00:20:05.630 --> 00:20:09.040
in the period from
1975 to 2000 has

00:20:09.040 --> 00:20:11.930
a story of massive
regulatory failure

00:20:11.930 --> 00:20:14.420
and therefore, people
are trying to think

00:20:14.420 --> 00:20:18.080
of new paradigmatic
approaches to how we should

00:20:18.080 --> 00:20:20.110
go about regulating things.

00:20:20.110 --> 00:20:22.160
And in that context,
it's interesting

00:20:22.160 --> 00:20:25.470
that people are no longer so
much talking about science

00:20:25.470 --> 00:20:28.230
as they're talking about
technology, which, obviously,

00:20:28.230 --> 00:20:35.350
is of central significance to
the tech companies of today.

00:20:35.350 --> 00:20:41.460
So I want to do the
last third of this talk

00:20:41.460 --> 00:20:43.600
by saying that in a way, We?

00:20:43.600 --> 00:20:46.840
Ought to be more focused
on the rights paradigm.

00:20:46.840 --> 00:20:51.460
But the rights paradigm also
has destabilizing forces

00:20:51.460 --> 00:20:52.600
attached to it.

00:20:52.600 --> 00:20:55.740
The rights paradigm
was created for people

00:20:55.740 --> 00:20:58.800
who look kind of like
us and it's imagined

00:20:58.800 --> 00:21:03.380
that we begin and end with the
physical persona that we have.

00:21:03.380 --> 00:21:06.110
So for instance, if you
look at all of the privacy

00:21:06.110 --> 00:21:08.960
decisions of the
US Supreme Court,

00:21:08.960 --> 00:21:14.280
they're all premised on a person
in physical space, a contained

00:21:14.280 --> 00:21:15.270
human being.

00:21:15.270 --> 00:21:18.340
So why should we not
have the government

00:21:18.340 --> 00:21:19.880
regulating contraceptives.

00:21:19.880 --> 00:21:23.500
Well, it's like the cops
kicking down the bedroom door.

00:21:23.500 --> 00:21:25.560
This is the sort of imagination.

00:21:25.560 --> 00:21:31.000
Why should be not have
tracking of potentially

00:21:31.000 --> 00:21:35.240
criminal engagements
inside of telephone booths.

00:21:35.240 --> 00:21:38.100
Because telephone booths
are kind of like rooms

00:21:38.100 --> 00:21:40.720
and therefore, people should
be entitled to the same kind

00:21:40.720 --> 00:21:42.950
of privacy in the room.

00:21:42.950 --> 00:21:46.080
So I would suggest
that we live in an era

00:21:46.080 --> 00:21:49.170
when this idea of a physically
contained human being sitting

00:21:49.170 --> 00:21:51.780
in physically contained
space has broken down

00:21:51.780 --> 00:21:53.820
for all sorts of reasons.

00:21:53.820 --> 00:21:56.430
You find it in fiction.

00:21:56.430 --> 00:21:59.900
I don't know if anybody here is
familiar with Philip Pullman's

00:21:59.900 --> 00:22:02.640
famous trilogy,
His Dark Materials,

00:22:02.640 --> 00:22:08.330
in which the central characters
all have a demon, as Pullman

00:22:08.330 --> 00:22:10.560
calls them, attached to them.

00:22:10.560 --> 00:22:13.680
This is an animal
avatar like persona

00:22:13.680 --> 00:22:16.130
that is part of that human.

00:22:16.130 --> 00:22:18.010
And if you kill that
demon for instance,

00:22:18.010 --> 00:22:19.490
you're killing the person.

00:22:19.490 --> 00:22:22.660
And it is said that
Pullman was profoundly

00:22:22.660 --> 00:22:27.190
inspired by this famous
painting of Leonardo-- the Lady

00:22:27.190 --> 00:22:30.080
with the Ermine, where
there's this organic unity

00:22:30.080 --> 00:22:32.690
between the lady and her ermine.

00:22:32.690 --> 00:22:36.380
And he got this idea
of the animal demon

00:22:36.380 --> 00:22:39.780
by looking at that painting.

00:22:39.780 --> 00:22:43.500
But if you think about
who we are today,

00:22:43.500 --> 00:22:47.810
you could think of us as being
an era of overlapping subjects.

00:22:47.810 --> 00:22:52.280
So there's the classically
the depicted human subject

00:22:52.280 --> 00:22:55.050
who is the human body.

00:22:55.050 --> 00:23:01.040
But there is the genomic
genetics subject, the product--

00:23:01.040 --> 00:23:05.180
you could think of it as the
23 and me subject if you want.

00:23:05.180 --> 00:23:08.550
And then there's the one that
you all are more involved with,

00:23:08.550 --> 00:23:11.670
which is the subject
that is constituted

00:23:11.670 --> 00:23:15.400
by the informational traces
that are left behind.

00:23:15.400 --> 00:23:17.020
And the point is
that these are not

00:23:17.020 --> 00:23:18.930
separate-- they're
not separable.

00:23:18.930 --> 00:23:22.210
But it does throw up a
set of questions for law

00:23:22.210 --> 00:23:23.140
on public policy.

00:23:23.140 --> 00:23:27.180
Which if these personas
are trying to protect

00:23:27.180 --> 00:23:30.600
and what rights do they
have, vis-a-vis us.

00:23:30.600 --> 00:23:34.630
So that case like Google
Spain-- which, if all of you

00:23:34.630 --> 00:23:38.290
know about it-- is about
the memory preservation

00:23:38.290 --> 00:23:42.070
through Google could
be seen as being logged

00:23:42.070 --> 00:23:45.170
in that area between the
physical human subject

00:23:45.170 --> 00:23:47.030
and the informational
human subject.

00:23:47.030 --> 00:23:50.230
And so is some kind of
set of understandings

00:23:50.230 --> 00:23:53.200
of what is due to that
subject in the middle going

00:23:53.200 --> 00:23:56.000
to carry over to the
informational traces.

00:23:56.000 --> 00:23:57.800
And if so, how.

00:23:57.800 --> 00:24:03.420
So it is in that trification
of the subject-- I mean,

00:24:03.420 --> 00:24:06.260
I sometimes like to say we
have become the Trinity,

00:24:06.260 --> 00:24:09.650
in a sentence--
where on that sort

00:24:09.650 --> 00:24:13.970
of parsing out of the human do
you overlay public policy that

00:24:13.970 --> 00:24:17.970
has evolved over millennia
dealing with only

00:24:17.970 --> 00:24:21.010
that figure in the middle.

00:24:21.010 --> 00:24:25.530
So very quickly, I think
that our classical frameworks

00:24:25.530 --> 00:24:29.540
for regulating all
run into problems.

00:24:29.540 --> 00:24:31.470
We live in what
is popularly known

00:24:31.470 --> 00:24:35.620
as the neoliberal era, in which
states have decided that they

00:24:35.620 --> 00:24:37.910
need to let markets
do more, which means

00:24:37.910 --> 00:24:40.110
let the private sector do more.

00:24:40.110 --> 00:24:42.630
But we know that when
markets come into action

00:24:42.630 --> 00:24:46.380
it's often at the time
when products have already

00:24:46.380 --> 00:24:49.090
been designed and they're
already out there.

00:24:49.090 --> 00:24:51.580
And it's often too
late at that point

00:24:51.580 --> 00:24:54.800
to get the kind of
public involvement

00:24:54.800 --> 00:24:57.030
that might actually
lead to changes

00:24:57.030 --> 00:25:00.300
in the design of a
commodity, whatever it is.

00:25:00.300 --> 00:25:05.000
And the ambivalences
are not apparent

00:25:05.000 --> 00:25:08.020
when you're living in a
society of early adopters.

00:25:08.020 --> 00:25:10.300
What are the people thinking
who don't want that.

00:25:10.300 --> 00:25:12.160
And so by the time
these concerns

00:25:12.160 --> 00:25:15.160
mature, often,
markets are already

00:25:15.160 --> 00:25:17.590
to set have, too
many sunk costs,

00:25:17.590 --> 00:25:22.980
and are not able to do
things in a reasonable way.

00:25:22.980 --> 00:25:26.690
And then markets, as business
historians will know,

00:25:26.690 --> 00:25:28.430
don't remember
their own mistakes.

00:25:28.430 --> 00:25:31.010
I mean, in this respect,
markets are bit like science.

00:25:31.010 --> 00:25:32.800
The false [INAUDIBLE]
are forgotten,

00:25:32.800 --> 00:25:35.080
they were just those things
that those people did.

00:25:35.080 --> 00:25:36.630
They were mistakes,
we do better,

00:25:36.630 --> 00:25:38.970
and therefore, only
the progressive history

00:25:38.970 --> 00:25:41.810
gets written and not
the regressive, the one

00:25:41.810 --> 00:25:44.700
that made for complications.

00:25:44.700 --> 00:25:47.380
On regulation, we don't need
to say very much because this

00:25:47.380 --> 00:25:49.080
is so much the mantra
of the moment--

00:25:49.080 --> 00:25:51.000
that governments
can't do anything.

00:25:51.000 --> 00:25:54.950
And therefore, all we should
do is remind ourselves

00:25:54.950 --> 00:25:56.250
that there are arguments here.

00:25:56.250 --> 00:25:59.650
It's not just that they
are governments, it's why.

00:25:59.650 --> 00:26:02.300
And partly,
governmental regulation

00:26:02.300 --> 00:26:05.580
presumes a degree of
availability of knowledge

00:26:05.580 --> 00:26:08.220
and a level playing field
of knowledge that turns out

00:26:08.220 --> 00:26:11.480
not to be available
in most governments.

00:26:11.480 --> 00:26:14.920
The rollout of the Affordable
Care Act as one recent case

00:26:14.920 --> 00:26:18.230
study that says something
about governmental capability

00:26:18.230 --> 00:26:20.650
in these areas.

00:26:20.650 --> 00:26:24.350
But on top of it, there are
very specific US policy choices

00:26:24.350 --> 00:26:25.090
we've made.

00:26:25.090 --> 00:26:28.300
Which is to say that regulation
should happen very far

00:26:28.300 --> 00:26:29.260
downstream.

00:26:29.260 --> 00:26:32.410
It should address the
product and not the process.

00:26:32.410 --> 00:26:34.690
And we've said
this over and over.

00:26:34.690 --> 00:26:38.390
But that also means that
early corrective steps

00:26:38.390 --> 00:26:41.220
are less likely to be taken.

00:26:41.220 --> 00:26:44.260
And furthermore, of course, we
live in a globalizing world.

00:26:44.260 --> 00:26:46.410
So when we turn
to regulation, we

00:26:46.410 --> 00:26:50.400
create patchwork systems and
multinational enterprises,

00:26:50.400 --> 00:26:54.360
well aware of that phenomenon.

00:26:54.360 --> 00:26:57.070
So one step that we've
increasingly been taking

00:26:57.070 --> 00:26:59.890
is saying, well OK, all
these problems-- they're

00:26:59.890 --> 00:27:01.320
a little bit about values.

00:27:01.320 --> 00:27:03.040
So why don't we do ethics.

00:27:03.040 --> 00:27:07.180
And there's been, in fact, a
sort of statistical bursting

00:27:07.180 --> 00:27:12.340
out of ethics fields and
ethics deliberative bodies

00:27:12.340 --> 00:27:15.750
around most of these
frontiers areas of science

00:27:15.750 --> 00:27:16.780
and technology.

00:27:16.780 --> 00:27:20.340
There are almost as many
hyphenated ethics fields

00:27:20.340 --> 00:27:22.970
as there are sciences
and technologies.

00:27:22.970 --> 00:27:27.220
And my research group
for the last two years--

00:27:27.220 --> 00:27:31.580
I had somebody who was a
philosopher of neuroethics

00:27:31.580 --> 00:27:35.620
and has a job in
being a neuroethicist.

00:27:35.620 --> 00:27:38.080
But you know, what's a
neuroethicist as opposed

00:27:38.080 --> 00:27:40.020
to some other kind of ethicist.

00:27:40.020 --> 00:27:44.870
So ethics deliberations
end up committing a lot

00:27:44.870 --> 00:27:46.360
to private bodies.

00:27:46.360 --> 00:27:48.680
And so I sit on an
ethics committee

00:27:48.680 --> 00:27:52.130
at Harvard dealing with
bio-related issues.

00:27:52.130 --> 00:27:55.700
How people get appointed
to these bodies, what

00:27:55.700 --> 00:27:59.720
their deliberations are--
this is all quite secretive.

00:27:59.720 --> 00:28:02.730
It's not meant to
be out in the open

00:28:02.730 --> 00:28:05.804
where publics can
have some kind of say.

00:28:05.804 --> 00:28:07.220
So that's the sort
of thing I mean

00:28:07.220 --> 00:28:10.380
by saying that questions
of value are privatized.

00:28:10.380 --> 00:28:12.520
And moreover, ethics
itself becomes

00:28:12.520 --> 00:28:14.510
a new kind of expertise.

00:28:14.510 --> 00:28:16.380
But we might argue
that everybody

00:28:16.380 --> 00:28:19.370
has values about what they
consider to be good or bad.

00:28:19.370 --> 00:28:24.670
And that by training people to
become ethical philosophers,

00:28:24.670 --> 00:28:28.540
we may be weeding out some
of that ambivalence that we

00:28:28.540 --> 00:28:30.120
should be paying attention to.

00:28:30.120 --> 00:28:32.850
And some public values
may never get picked up

00:28:32.850 --> 00:28:35.230
in a thing called ethics.

00:28:35.230 --> 00:28:39.510
So even these ethical ways
of talking and thinking

00:28:39.510 --> 00:28:40.980
may not be enough.

00:28:40.980 --> 00:28:46.250
In my own work, I tend to
think that context-- what

00:28:46.250 --> 00:28:49.370
other people call context--
is extremely important.

00:28:49.370 --> 00:28:53.290
That we need to pay attention
to the different historical

00:28:53.290 --> 00:28:56.680
pathways by which
cultures have evolved.

00:28:56.680 --> 00:29:00.240
And the kinds of central
commitments norms,

00:29:00.240 --> 00:29:02.330
values that they adhere
to, which are not

00:29:02.330 --> 00:29:04.390
the same across countries.

00:29:04.390 --> 00:29:10.730
So here is a little example
from a tech company, in a sense.

00:29:10.730 --> 00:29:13.220
This is an article
that caught my eye just

00:29:13.220 --> 00:29:14.670
in January of this year.

00:29:14.670 --> 00:29:17.540
Of course, I knew about
the phenomenon in general

00:29:17.540 --> 00:29:21.650
that Uber was having a hard
time penetrating into Germany.

00:29:21.650 --> 00:29:25.540
But the New York Times
writer's explanation

00:29:25.540 --> 00:29:30.290
for why miscalculated and
had to pull out of Frankfurt

00:29:30.290 --> 00:29:34.300
was quite interesting to me.

00:29:34.300 --> 00:29:36.390
"With a thriving
financial center-- so this

00:29:36.390 --> 00:29:39.720
is the pullout of Frankfurt--
and cosmopolitan population,

00:29:39.720 --> 00:29:42.560
the city, Frankfurt,
seemed like an ideal place

00:29:42.560 --> 00:29:43.720
to operate and grow.

00:29:43.720 --> 00:29:45.670
Yet the company was
forced out by a mix

00:29:45.670 --> 00:29:47.690
of cultural and legal missteps."

00:29:47.690 --> 00:29:50.400
And the sentence,
specifically, "miscalculated

00:29:50.400 --> 00:29:54.800
how best to gain the support of
skeptical locals unaccustomed

00:29:54.800 --> 00:29:58.400
to its win at all costs
tactics and it underestimated

00:29:58.400 --> 00:30:00.380
the regulatory hurdles
of doing business

00:30:00.380 --> 00:30:02.230
in Europe's largest economy."

00:30:02.230 --> 00:30:06.130
So these are three
items, three variables.

00:30:06.130 --> 00:30:09.660
Skeptical locals, win
at all costs tactics,

00:30:09.660 --> 00:30:12.970
and regulatory hurdles of
doing business in Europe.

00:30:12.970 --> 00:30:16.310
So you know, these are
absolute truisms to anybody

00:30:16.310 --> 00:30:19.660
who thinks about public policy
and these are not novelty.

00:30:19.660 --> 00:30:23.450
I mean, I, myself would love
to go to the right people

00:30:23.450 --> 00:30:26.770
and ask who did you
have scoping out

00:30:26.770 --> 00:30:28.980
the regulatory environment
in which you were going

00:30:28.980 --> 00:30:30.630
to launch this technology.

00:30:30.630 --> 00:30:32.190
And then a bit later
in the article,

00:30:32.190 --> 00:30:35.310
it also talks about how the
Germans don't like credit.

00:30:35.310 --> 00:30:36.900
Well, anybody who's
lived in Germany

00:30:36.900 --> 00:30:38.960
knows Germans don't like credit.

00:30:38.960 --> 00:30:40.630
You go to a restaurant,
they'd rather

00:30:40.630 --> 00:30:43.300
have you come in with
400 euros in your pocket

00:30:43.300 --> 00:30:45.230
than accept your credit card.

00:30:45.230 --> 00:30:50.940
And you won't get mugged so
you don't need your 20 euro

00:30:50.940 --> 00:30:53.980
urban wallet in case
you do get mugged.

00:30:53.980 --> 00:30:59.790
They evaluate the risk of credit
default in a different way

00:30:59.790 --> 00:31:02.770
from what they evaluate the risk
of being mugged on the street

00:31:02.770 --> 00:31:05.820
as you're carrying your
dinner money around with you.

00:31:05.820 --> 00:31:08.790
So these are kind
of obvious points

00:31:08.790 --> 00:31:11.510
and one can then do
the analysis to think

00:31:11.510 --> 00:31:15.510
about why Germany, with its
thriving financial centers

00:31:15.510 --> 00:31:17.770
and so on and so
forth, has ended up

00:31:17.770 --> 00:31:21.040
with such a different idea of
the risks and benefits of doing

00:31:21.040 --> 00:31:24.400
business from where
we happen to be.

00:31:24.400 --> 00:31:27.820
So this all prompts
me to think harder

00:31:27.820 --> 00:31:31.270
about a sort of patented
phrase in my work, which

00:31:31.270 --> 00:31:33.560
has turned out to
be a kind of sleeper

00:31:33.560 --> 00:31:36.590
when I wrote an article called,
"Technologies of Humility."

00:31:36.590 --> 00:31:39.750
I didn't expect that it
would become, practically,

00:31:39.750 --> 00:31:42.040
my most cited piece.

00:31:42.040 --> 00:31:45.410
Most humorously, somebody
sent me a clipping

00:31:45.410 --> 00:31:49.660
from a paper in
Oxford, UK one saying,

00:31:49.660 --> 00:31:52.470
"Harvard professor
advocates humility."

00:31:52.470 --> 00:31:57.060
So it has gotten a little
bit of circulation.

00:31:57.060 --> 00:32:00.550
But what I say in that piece
is that we should pay attention

00:32:00.550 --> 00:32:03.760
to where questions come
from for public policy.

00:32:03.760 --> 00:32:07.330
But I think investment is a
lot like public policy in terms

00:32:07.330 --> 00:32:09.610
of where it's trying to go.

00:32:09.610 --> 00:32:12.890
So framing-- how is the
question being posed and is it

00:32:12.890 --> 00:32:15.890
being posed the right way and
what are other ways of posing

00:32:15.890 --> 00:32:17.350
the question.

00:32:17.350 --> 00:32:20.070
I need not tell
engineers that this

00:32:20.070 --> 00:32:23.350
is a sort of fundamentally
important set of things

00:32:23.350 --> 00:32:25.350
that people understand
when they're there

00:32:25.350 --> 00:32:28.080
in the guts of a design
process but do not

00:32:28.080 --> 00:32:31.990
understand when they get out
into public policy world.

00:32:31.990 --> 00:32:34.910
And then other questions
of ethics and values--

00:32:34.910 --> 00:32:38.030
you're going to innovate,
but who's going to get it.

00:32:38.030 --> 00:32:39.834
Everybody's heard
about the Luddites

00:32:39.834 --> 00:32:41.250
and how bad they
were because they

00:32:41.250 --> 00:32:43.120
went around breaking machines.

00:32:43.120 --> 00:32:45.650
Very few people say,
who was out there

00:32:45.650 --> 00:32:47.930
to give the Luddites
a different job when

00:32:47.930 --> 00:32:50.660
their hand crafted
looms were going

00:32:50.660 --> 00:32:53.210
to be driven out of
business and instead, they

00:32:53.210 --> 00:32:54.700
were going to have
something else.

00:32:54.700 --> 00:32:57.260
Well, those skeptical
publics and fans

00:32:57.260 --> 00:32:59.760
don't want to be driven out.

00:32:59.760 --> 00:33:03.670
So that's part of the
vulnerability point.

00:33:03.670 --> 00:33:04.480
Distribution.

00:33:04.480 --> 00:33:07.170
Richard mentioned this
at the outset-- who loses

00:33:07.170 --> 00:33:10.660
and who wins and learning
how do you actually

00:33:10.660 --> 00:33:14.340
historize experiences and
learn to do things better.

00:33:14.340 --> 00:33:17.170
So I think one can carry
this idea of humility

00:33:17.170 --> 00:33:21.870
forward into thinking about
technology in and for society.

00:33:21.870 --> 00:33:25.920
I think one can pull out some
policy prescriptions that

00:33:25.920 --> 00:33:28.410
may sound a little
bit banal, put

00:33:28.410 --> 00:33:30.150
at this level of abstraction.

00:33:30.150 --> 00:33:32.860
But if you get into the nuts
and bolts, the nitty gritty

00:33:32.860 --> 00:33:36.860
of historical cases of business
success and business failure,

00:33:36.860 --> 00:33:40.840
you find, repeatedly, the same
pattern things showing up.

00:33:40.840 --> 00:33:45.110
So people do not attend
to the communities

00:33:45.110 --> 00:33:47.610
and the social
norms of the places

00:33:47.610 --> 00:33:49.450
where things will be tried out.

00:33:49.450 --> 00:33:52.030
They do not engage with
history and culture

00:33:52.030 --> 00:33:54.210
because it's assumed
that the new will

00:33:54.210 --> 00:33:58.890
be valued exactly the same
way as where it was invented

00:33:58.890 --> 00:34:01.670
in the first place.

00:34:01.670 --> 00:34:06.290
Concerns about distribution
tend to get it back seat

00:34:06.290 --> 00:34:09.570
and we've seen that just
over and over again.

00:34:09.570 --> 00:34:12.170
And then this idea
that communication

00:34:12.170 --> 00:34:15.570
needs to be two way-- that
it's not just the people who

00:34:15.570 --> 00:34:18.270
have the new thing that are
going to go out and sell

00:34:18.270 --> 00:34:23.030
to the people who surely will
be demanding this innovation,

00:34:23.030 --> 00:34:27.280
but that technologies
like legal systems

00:34:27.280 --> 00:34:31.080
and like political
systems also have publics.

00:34:31.080 --> 00:34:33.500
And how can those
publics actually

00:34:33.500 --> 00:34:37.239
be heard back in
that design process.

00:34:37.239 --> 00:34:40.840
So I'd be happy to talk in
more detail about any of this

00:34:40.840 --> 00:34:43.179
that interests you
and above all, I'm

00:34:43.179 --> 00:34:46.380
interested in the kinds of
questions and observations

00:34:46.380 --> 00:34:47.139
you may have.

00:34:47.139 --> 00:34:48.560
So let me stop there.

00:34:54.513 --> 00:34:55.054
AUDIENCE: Hi.

00:34:55.054 --> 00:34:56.887
I'm trying to think
about how to phrase this

00:34:56.887 --> 00:34:59.280
correctly, but in
the modern world

00:34:59.280 --> 00:35:03.410
where technology is not
subject to a single locality--

00:35:03.410 --> 00:35:05.820
so there isn't a single
entity that can regulate

00:35:05.820 --> 00:35:09.590
entire industry--
how would you propose

00:35:09.590 --> 00:35:14.480
that the technologies that
are going global be regulated

00:35:14.480 --> 00:35:15.220
or not?

00:35:15.220 --> 00:35:16.990
And in particular,
I'm thinking about

00:35:16.990 --> 00:35:19.770
of technologies
like Twitter, who

00:35:19.770 --> 00:35:24.300
we praised a lot of when the
revolution in the Arab springs

00:35:24.300 --> 00:35:28.260
were happening and everybody was
like, oh, Twitter is amazing.

00:35:28.260 --> 00:35:31.470
But now that is being
used by a group like ISIS,

00:35:31.470 --> 00:35:32.980
we have the opposite
view, right?

00:35:35.202 --> 00:35:37.410
SHEILA JASANOFF: I mean,
that's an excellent question

00:35:37.410 --> 00:35:39.420
and I think it
can be taken apart

00:35:39.420 --> 00:35:40.850
into several different things.

00:35:40.850 --> 00:35:44.450
So one point is just
the evolutionary point.

00:35:44.450 --> 00:35:50.050
That regulation often operates
in a kind of linear mindset

00:35:50.050 --> 00:35:54.610
that there is a moment--
usually, the marketing moment--

00:35:54.610 --> 00:35:57.520
where something leaves the
lab and goes out into society,

00:35:57.520 --> 00:35:59.750
which is the right
moment to regulate.

00:35:59.750 --> 00:36:00.770
And that's that.

00:36:00.770 --> 00:36:03.120
But we know that
pretty much every kind

00:36:03.120 --> 00:36:05.900
of technological
system-- that that is not

00:36:05.900 --> 00:36:09.430
how it works because the
amount of information

00:36:09.430 --> 00:36:12.220
we have at that very moment
may be extremely limited.

00:36:12.220 --> 00:36:14.460
Think about pharmaceutical
drugs for instance--

00:36:14.460 --> 00:36:17.300
you do clinical trials,
but the clinical trials

00:36:17.300 --> 00:36:21.350
give you only an approximation
of the actual effect

00:36:21.350 --> 00:36:24.330
of long term use in
bodies of different sorts.

00:36:24.330 --> 00:36:25.710
And Twitter is no different.

00:36:25.710 --> 00:36:28.830
You can think of it as a
medication being let loose

00:36:28.830 --> 00:36:31.120
on society and the
clinical trials

00:36:31.120 --> 00:36:33.580
revealed that there are
different population

00:36:33.580 --> 00:36:37.150
groups that are going to react
and respond in different ways

00:36:37.150 --> 00:36:38.810
to the availability
of the technology.

00:36:38.810 --> 00:36:41.200
So I think that one
needs to sort of have

00:36:41.200 --> 00:36:45.510
the idea that the regulatory
process has to be an ongoing

00:36:45.510 --> 00:36:46.800
and recursive one.

00:36:46.800 --> 00:36:49.440
This is what I mean about
the learning experience.

00:36:49.440 --> 00:36:54.400
And how you built that
in-- many businesses

00:36:54.400 --> 00:36:58.460
think that the competitive
movement of getting something

00:36:58.460 --> 00:37:00.910
out there is
incredibly important

00:37:00.910 --> 00:37:04.610
and don't want to look
back and rethink things.

00:37:04.610 --> 00:37:06.700
Who has the
responsibility, anyway,

00:37:06.700 --> 00:37:08.900
for taking things on board.

00:37:08.900 --> 00:37:11.610
So that's one kind
of point, I think,

00:37:11.610 --> 00:37:15.981
to wrestle with the need
for more recursion in policy

00:37:15.981 --> 00:37:17.090
making.

00:37:17.090 --> 00:37:21.900
The second point
is that it's not

00:37:21.900 --> 00:37:25.390
going to be the case that
regulation will be universal

00:37:25.390 --> 00:37:28.040
in the same way that the
technology itself, in a way,

00:37:28.040 --> 00:37:29.170
isn't universal.

00:37:29.170 --> 00:37:31.330
So all sort of
studies of technology

00:37:31.330 --> 00:37:34.520
out of the field of STS,
science and technology studies,

00:37:34.520 --> 00:37:37.770
show that uses are continually
adapting technology.

00:37:37.770 --> 00:37:42.930
So one can say a phrase like
technological capability

00:37:42.930 --> 00:37:45.940
that may have some
universal dimensions.

00:37:45.940 --> 00:37:50.350
But what people will take and do
with that in their own context

00:37:50.350 --> 00:37:51.370
is going to vary.

00:37:51.370 --> 00:37:56.360
And then again,
being attuned to that

00:37:56.360 --> 00:38:00.040
and accepting the fact that
maybe there isn't going

00:38:00.040 --> 00:38:02.720
to be a one size fits all.

00:38:02.720 --> 00:38:06.345
And so what degree of
variation then, is tolerable.

00:38:09.420 --> 00:38:17.980
Why does Trump's idea about
not having the Pacific

00:38:17.980 --> 00:38:24.020
treaty or the European treaty,
the disastrous consequences

00:38:24.020 --> 00:38:26.560
NAFTA-- why do those
have so much traction.

00:38:26.560 --> 00:38:28.970
It's partly because
people are actually

00:38:28.970 --> 00:38:31.820
rebelling from a different
side against the idea

00:38:31.820 --> 00:38:33.920
that once you get a
technology on the books,

00:38:33.920 --> 00:38:37.210
it should be free
flowing completely.

00:38:37.210 --> 00:38:40.340
But this is, of course,
a topic that, especially

00:38:40.340 --> 00:38:45.370
at this company, I don't need
to belabor because when Google

00:38:45.370 --> 00:38:47.900
talks to China, Google
is saying something very

00:38:47.900 --> 00:38:50.700
different from when
Google Docs to Spain

00:38:50.700 --> 00:38:55.070
or when Google talks to
Congress on Capitol Hill.

00:38:55.070 --> 00:39:00.690
But the democracy implications
are not necessarily

00:39:00.690 --> 00:39:01.580
being heard.

00:39:01.580 --> 00:39:04.890
These are very top to top
kinds of deliberations.

00:39:04.890 --> 00:39:08.190
And I think that there needs
to be much more opening up

00:39:08.190 --> 00:39:09.750
about the kinds of values.

00:39:09.750 --> 00:39:11.420
How much contradiction
we actually

00:39:11.420 --> 00:39:14.364
tolerate across the world.

00:39:14.364 --> 00:39:16.030
AUDIENCE: So I actually
have a question.

00:39:16.030 --> 00:39:20.500
The field of GMO, genetically
modified organisms, I think,

00:39:20.500 --> 00:39:23.870
is an interesting prism
for a lot of this.

00:39:23.870 --> 00:39:27.120
So from a first
world perspective,

00:39:27.120 --> 00:39:30.230
I've got the money to feed
my family organic foods

00:39:30.230 --> 00:39:34.610
and I might say it's my right
to exist in GMO-free world

00:39:34.610 --> 00:39:37.240
because I have these fears and
who knows what bad things may

00:39:37.240 --> 00:39:38.057
happen and so on.

00:39:38.057 --> 00:39:39.640
And then there's the
other perspective

00:39:39.640 --> 00:39:42.590
the GMOs may turn
out to be the thing

00:39:42.590 --> 00:39:48.380
to give the other billions
of the world's population

00:39:48.380 --> 00:39:50.810
these benefits of
enough food to eat

00:39:50.810 --> 00:39:55.200
and freedom from disease
and starvation and whatnot.

00:39:55.200 --> 00:40:03.150
How does one make progress
with these opposed principles?

00:40:03.150 --> 00:40:06.740
SHEILA JASANOFF: Well, I
think that other recurrent--

00:40:06.740 --> 00:40:09.530
an excellent question--
goes back to the point

00:40:09.530 --> 00:40:12.300
about framing that
I was making before.

00:40:12.300 --> 00:40:16.640
So the debate in GMO is often
couched in exactly that way--

00:40:16.640 --> 00:40:19.350
that the Europeans can afford
to be resistant to GMOs

00:40:19.350 --> 00:40:23.140
because they also have the
capacity to pay extra money,

00:40:23.140 --> 00:40:24.880
buy their organic products.

00:40:24.880 --> 00:40:27.480
But this is not the kind
of technological solution

00:40:27.480 --> 00:40:30.344
that's going to benefit
the rest of the world.

00:40:30.344 --> 00:40:32.260
You may have seen that
one of the big business

00:40:32.260 --> 00:40:36.490
stories of the recent past is
the deal that Bayer is making

00:40:36.490 --> 00:40:40.600
in buying Monsanto and I'm
very interested to see what

00:40:40.600 --> 00:40:45.350
two different corporate cultures
that have grown up and evolved

00:40:45.350 --> 00:40:48.210
under a very different
regulatory presumptions

00:40:48.210 --> 00:40:53.060
are going to do when they're now
married into the single entity.

00:40:53.060 --> 00:40:58.690
But the kinds of
points that people

00:40:58.690 --> 00:41:02.790
would make if they looked
deeply into the GMO controversy

00:41:02.790 --> 00:41:06.670
would say that that initial
idea-- that it's a binary.

00:41:06.670 --> 00:41:10.880
That some people are rich and
can afford to do without GMOs

00:41:10.880 --> 00:41:13.090
and that other people
are poor and need GMOs

00:41:13.090 --> 00:41:17.160
was not a very helpful
framing to start with.

00:41:17.160 --> 00:41:21.390
That is not the case that people
with a genetic technologies

00:41:21.390 --> 00:41:25.590
have deeply investigated
what the nutritional and crop

00:41:25.590 --> 00:41:27.900
needs of the parts
of the world are,

00:41:27.900 --> 00:41:31.360
where GMOs are going
to be used heavily.

00:41:31.360 --> 00:41:35.650
Many people whom will say-- this
is quite a credible argument--

00:41:35.650 --> 00:41:39.440
that the precursor to the GMO
story is the Green Revolution

00:41:39.440 --> 00:41:42.380
and that the Green Revolution
tells a shaded story.

00:41:42.380 --> 00:41:44.850
That there's no question
that the Green Revolution

00:41:44.850 --> 00:41:51.670
produced high yielding farms and
massively raised the capacity

00:41:51.670 --> 00:41:55.700
to generate food crops in
parts of the world that had

00:41:55.700 --> 00:41:57.760
previously been in a deficit.

00:41:57.760 --> 00:41:59.650
But they will also
say that the long term

00:41:59.650 --> 00:42:03.340
environmental consequences,
which now are to paying back

00:42:03.340 --> 00:42:07.550
to damage agriculture, were
not foreseen and calculated

00:42:07.550 --> 00:42:10.220
because you could only
grow these germs with very

00:42:10.220 --> 00:42:16.110
high inputs of electricity,
fertilizer, and water.

00:42:16.110 --> 00:42:20.520
And therefore, these were
subsidized elevations of grain

00:42:20.520 --> 00:42:24.440
and had very unequal effects
across the areas of the world

00:42:24.440 --> 00:42:26.590
where the Green
Revolution was introduced.

00:42:26.590 --> 00:42:30.220
So I think that the
technology story always

00:42:30.220 --> 00:42:33.810
needs to be told in a
more differentiated way

00:42:33.810 --> 00:42:37.310
than the
proponent-opponent version.

00:42:37.310 --> 00:42:41.130
I mean, I really think that this
is not a black and white issue.

00:42:41.130 --> 00:42:44.600
This is a who loses,
who wins issue

00:42:44.600 --> 00:42:46.740
and even that is too binary.

00:42:46.740 --> 00:42:49.310
Because you need to
throw in temporality over

00:42:49.310 --> 00:42:50.630
what period of time.

00:42:50.630 --> 00:42:51.770
You might gain.

00:42:51.770 --> 00:42:55.540
I've heard biotech people
from the third world talking

00:42:55.540 --> 00:42:59.220
about the fact that they
planted this kind of cotton

00:42:59.220 --> 00:43:02.090
and the first year, the
yields were improved,

00:43:02.090 --> 00:43:05.670
the second year, the yields
were vastly improved.

00:43:05.670 --> 00:43:08.090
The third year was stabilized.

00:43:08.090 --> 00:43:12.980
The fourth year a bug came
and got rid of the mono crops

00:43:12.980 --> 00:43:16.090
because it was resistant to
the particular change that

00:43:16.090 --> 00:43:17.380
had been introduced.

00:43:17.380 --> 00:43:21.450
So one has to take those
stories on board as well

00:43:21.450 --> 00:43:23.720
and think about how
technology should

00:43:23.720 --> 00:43:26.480
function in a differentiation.

00:43:26.480 --> 00:43:29.100
I guess that's one
of my main issues.

00:43:31.780 --> 00:43:35.480
AUDIENCE: So I wanted
to ask about the sort

00:43:35.480 --> 00:43:43.200
of perverse incentives that
some of the public policies

00:43:43.200 --> 00:43:49.720
have in driving ethics out
of the role of invention,

00:43:49.720 --> 00:43:58.160
such as regulations that prevent
questioning of things that

00:43:58.160 --> 00:43:59.850
are being done by
companies or that

00:43:59.850 --> 00:44:02.010
limit their liability
in such a case

00:44:02.010 --> 00:44:06.270
that they don't have to consider
the negative consequences

00:44:06.270 --> 00:44:09.710
of what they do because
they're protected

00:44:09.710 --> 00:44:12.420
by the regulatory
industry as opposed

00:44:12.420 --> 00:44:21.480
to incentives that would
drive them to be concerned.

00:44:21.480 --> 00:44:27.650
SHEILA JASANOFF: Yes, so
how should one-- in essence,

00:44:27.650 --> 00:44:30.240
you're asking what one
can do about capture.

00:44:30.240 --> 00:44:34.800
That cozy relations built
up between governments

00:44:34.800 --> 00:44:37.140
eager to protect
their industries

00:44:37.140 --> 00:44:42.960
and therefore, they're protected
against the worst consequences

00:44:42.960 --> 00:44:44.750
of what they're up to.

00:44:44.750 --> 00:44:47.650
So the piece of this
that my book is most

00:44:47.650 --> 00:44:52.610
focused on is not, in
a sense, the phenomenon

00:44:52.610 --> 00:44:55.890
in the first place, but how
the phenomenon that's actually

00:44:55.890 --> 00:44:58.900
put it as if it's rational.

00:44:58.900 --> 00:45:02.610
So where people
sort of blatantly

00:45:02.610 --> 00:45:05.060
get together in a
room and say, we're

00:45:05.060 --> 00:45:07.940
going to come out giving
you everything you want.

00:45:07.940 --> 00:45:16.220
So how Trump and Christie
managed certain dealings

00:45:16.220 --> 00:45:20.310
with casinos and debts in
the state of New Jersey.

00:45:20.310 --> 00:45:23.290
I mean, the public can
understand those stories

00:45:23.290 --> 00:45:24.240
when they hear them.

00:45:24.240 --> 00:45:27.780
But if you tell people, I
conducted a risk assessment

00:45:27.780 --> 00:45:31.080
and discovered that this
company's product is safe,

00:45:31.080 --> 00:45:34.110
then you're resorting to science
to produce a kind of argument

00:45:34.110 --> 00:45:37.240
that everybody buys into
the rationality of science.

00:45:37.240 --> 00:45:39.520
But if you look
harder, you often

00:45:39.520 --> 00:45:45.990
find that the technical
arguments that were made

00:45:45.990 --> 00:45:50.640
shoved under the carpet fairly
important uncertainties,

00:45:50.640 --> 00:45:53.770
unresolved questions,
which might

00:45:53.770 --> 00:46:03.030
have come to light if the debate
had been in a more open avenue.

00:46:03.030 --> 00:46:05.200
One such thing
that's going on right

00:46:05.200 --> 00:46:09.920
now is around this extremely
promising gene editing

00:46:09.920 --> 00:46:12.090
technology that's
a suite of things

00:46:12.090 --> 00:46:14.560
that goes by the name CRISPR.

00:46:14.560 --> 00:46:17.160
I'm sure many of you
know all about it.

00:46:17.160 --> 00:46:23.090
So who is leading, who is
spearheading the debate on how

00:46:23.090 --> 00:46:24.650
to regulate CRISPR.

00:46:24.650 --> 00:46:28.350
So far, it's been a few
national science academies

00:46:28.350 --> 00:46:31.150
that, actually,
have held meetings

00:46:31.150 --> 00:46:33.800
that they call summit
meetings to work out

00:46:33.800 --> 00:46:39.930
a global regulatory package
on the implications of CRISPR.

00:46:39.930 --> 00:46:42.720
But I think people already
know that, for instance, even

00:46:42.720 --> 00:46:44.850
with CRISPR, the question
about whether you're

00:46:44.850 --> 00:46:47.860
wanting to do human genome
editing that affects

00:46:47.860 --> 00:46:50.200
the next generation of the
human or whether you're

00:46:50.200 --> 00:46:56.850
wanting to do genes drives that
will alter a species so that it

00:46:56.850 --> 00:46:59.680
behaves in a different way in
the ecosystem-- these are going

00:46:59.680 --> 00:47:03.700
to carry around them,
different forms of debate.

00:47:03.700 --> 00:47:07.860
And we have forums in which
these things could get raised,

00:47:07.860 --> 00:47:11.170
but it's not yet happening
because the people who

00:47:11.170 --> 00:47:13.740
control the frontiers
of invention

00:47:13.740 --> 00:47:16.910
are usually setting the terms
about how the debates ought

00:47:16.910 --> 00:47:18.030
to be conducted.

00:47:18.030 --> 00:47:21.250
So my work is more around
that sort of phenomenon,

00:47:21.250 --> 00:47:24.810
the sort of premature
taking out of politics

00:47:24.810 --> 00:47:26.480
under the guise of
technical reasons.

00:47:31.560 --> 00:47:33.840
AUDIENCE: Several
inventions-- and I won't even

00:47:33.840 --> 00:47:38.500
say technology-- over
the past few years

00:47:38.500 --> 00:47:41.590
have been in the form of
provocations, probably

00:47:41.590 --> 00:47:43.090
intentionally, like Uber.

00:47:43.090 --> 00:47:45.630
I think the people at Uber
knew that what they were doing

00:47:45.630 --> 00:47:48.830
was against the existing
regulations on taxes

00:47:48.830 --> 00:47:52.940
and so on-- or
certainly suspected it.

00:47:52.940 --> 00:47:55.200
Google, when it scanned
millions of books,

00:47:55.200 --> 00:47:58.480
knew that they were in a
legal gray area on what they

00:47:58.480 --> 00:47:59.670
were doing.

00:47:59.670 --> 00:48:01.920
And I think they thought
they were doing a good thing,

00:48:01.920 --> 00:48:06.160
but legally, it
was unsettled law.

00:48:06.160 --> 00:48:09.420
Wall Street, when
it bundled mortgages

00:48:09.420 --> 00:48:14.140
into these packages that could
then be traded on exchanges,

00:48:14.140 --> 00:48:17.080
obviously, they were doing it to
make more money for themselves.

00:48:17.080 --> 00:48:20.120
But they also made the market
for mortgages more liquid

00:48:20.120 --> 00:48:21.150
in some ways.

00:48:21.150 --> 00:48:23.330
And then of course,
there's catastrophe later.

00:48:26.150 --> 00:48:31.780
I don't see how we can
advance in any of these areas

00:48:31.780 --> 00:48:33.930
without making mistakes.

00:48:33.930 --> 00:48:36.660
I don't know if
long term, Uber is

00:48:36.660 --> 00:48:44.000
going to make taxi drivers
poorer, or make traffic worse,

00:48:44.000 --> 00:48:45.070
or make things better.

00:48:45.070 --> 00:48:47.750
I don't know.

00:48:47.750 --> 00:48:49.560
I have my opinion
on Google Books,

00:48:49.560 --> 00:48:52.550
but let's leave that
aside because you'll think

00:48:52.550 --> 00:48:55.260
I am partisan.

00:48:55.260 --> 00:48:59.390
The bundling of securities seems
like a good thing in many ways.

00:48:59.390 --> 00:49:04.350
Are we condemned to oscillate
to have almost business

00:49:04.350 --> 00:49:08.450
cycles of inventions, where
something is introduced?

00:49:08.450 --> 00:49:12.000
We have the reaction,
it's over-regulated,

00:49:12.000 --> 00:49:13.350
under-regulated.

00:49:13.350 --> 00:49:18.960
And I don't see it happening
in any evolutionary way.

00:49:18.960 --> 00:49:22.350
I don't see it possible
for Uber or Google Books

00:49:22.350 --> 00:49:27.430
or securitization to go
before some wise committee

00:49:27.430 --> 00:49:29.630
at the Senate and say, we
want to do this experiment

00:49:29.630 --> 00:49:33.780
with taxes, let us do
it and see what happens.

00:49:33.780 --> 00:49:37.470
So how do we go forward
without making huge mistakes

00:49:37.470 --> 00:49:40.092
and oscillating in this way?

00:49:40.092 --> 00:49:41.800
SHEILA JASANOFF: Thanks
for the question.

00:49:41.800 --> 00:49:46.390
I mean, oscillation
implies that we've already

00:49:46.390 --> 00:49:50.680
parsed the world out
into a right-left binary.

00:49:50.680 --> 00:49:53.460
First of all, I don't think
that's how technology evolves.

00:49:53.460 --> 00:49:57.890
I think, arguably, all of
these destructive cycles

00:49:57.890 --> 00:50:01.940
that you're describing
began in experimental ways.

00:50:01.940 --> 00:50:07.360
And it's not that somebody
blanketed the entire US

00:50:07.360 --> 00:50:11.230
with Uber type solutions.

00:50:11.230 --> 00:50:13.410
They were introduced.

00:50:13.410 --> 00:50:16.070
And market research was done
and people went forward.

00:50:16.070 --> 00:50:18.560
So you know, I think
there's something

00:50:18.560 --> 00:50:23.160
to be done in conceptual
terms in recognizing

00:50:23.160 --> 00:50:25.140
how the introduction
of a technology

00:50:25.140 --> 00:50:28.420
like an experimental
system and then one

00:50:28.420 --> 00:50:31.360
can ask, well with
experiments, we

00:50:31.360 --> 00:50:36.410
have a long history of thinking
about how not to violate

00:50:36.410 --> 00:50:44.040
people's rights and expectations
in biomedicine, for instance.

00:50:44.040 --> 00:50:46.490
We've evolved a set of
ethical understandings

00:50:46.490 --> 00:50:48.350
about how to do experiments.

00:50:48.350 --> 00:50:52.060
It's just that we don't regard
throwing somebody out of a job

00:50:52.060 --> 00:50:55.960
as being similar
in characteristics

00:50:55.960 --> 00:51:05.490
to causing somebody to have
fatal side effects from a drug

00:51:05.490 --> 00:51:06.312
treatment.

00:51:06.312 --> 00:51:08.020
Although if you listen
to the economists,

00:51:08.020 --> 00:51:12.650
they would say that the
chances of your lifespan being

00:51:12.650 --> 00:51:16.930
drastically altered by having
no job are actually quite high.

00:51:16.930 --> 00:51:18.940
So I'm not sure that the
conceptual distinction

00:51:18.940 --> 00:51:24.580
between the economic disruption
and the putting at risk

00:51:24.580 --> 00:51:28.300
of health and safety
should be seen as so

00:51:28.300 --> 00:51:29.930
different from one another.

00:51:29.930 --> 00:51:35.330
So I think that these same
people with the talent

00:51:35.330 --> 00:51:37.920
to imagine the
disruptive technology

00:51:37.920 --> 00:51:40.340
should themselves be
taught not to think

00:51:40.340 --> 00:51:43.440
in a black and white world--
as if it is a binary, as

00:51:43.440 --> 00:51:46.320
if it is either/or or nothing.

00:51:46.320 --> 00:51:51.180
In fact, I think all
of industry development

00:51:51.180 --> 00:51:55.520
is full of experiments
and failed experiments.

00:51:55.520 --> 00:51:57.897
It's full of choices
of one sort or another.

00:51:57.897 --> 00:52:00.230
AUDIENCE: But I'm not sure
what you're suggesting there.

00:52:00.230 --> 00:52:03.660
Are you suggesting that
Uber should petition

00:52:03.660 --> 00:52:07.290
the state of Massachusetts that
Brookline will be an Uber zone

00:52:07.290 --> 00:52:09.340
and we'll try it
out and see what

00:52:09.340 --> 00:52:11.185
happens to the tax industry?

00:52:11.185 --> 00:52:12.560
I'm not sure what
you mean there.

00:52:12.560 --> 00:52:14.601
SHEILA JASANOFF: Well,
the first thing I'm saying

00:52:14.601 --> 00:52:17.250
is that Uber doesn't have
to petition Brookline.

00:52:17.250 --> 00:52:19.640
At the introduction
of a new technology,

00:52:19.640 --> 00:52:23.050
you see the companies are
actually making experiments.

00:52:23.050 --> 00:52:24.190
That's what they're doing.

00:52:24.190 --> 00:52:27.250
So I'm not making
up the experiment.

00:52:27.250 --> 00:52:30.160
I'm not saying go tell people
that they are experimenting.

00:52:30.160 --> 00:52:31.180
They are experiments.

00:52:31.180 --> 00:52:32.370
AUDIENCE: But at
the margin, they're

00:52:32.370 --> 00:52:33.820
not throwing anyone out of work.

00:52:33.820 --> 00:52:35.653
At the margin, they're
not changing traffic.

00:52:35.653 --> 00:52:37.520
It's only when they
become successful enough

00:52:37.520 --> 00:52:39.010
that those effects
might kick in.

00:52:39.010 --> 00:52:41.190
SHEILA JASANOFF:
Well, at that point,

00:52:41.190 --> 00:52:45.350
we should think about
what the scale up involves

00:52:45.350 --> 00:52:47.630
in terms of displacement.

00:52:47.630 --> 00:52:52.030
So for instance, if you take
an agricultural region that

00:52:52.030 --> 00:52:56.080
is organized around many,
many, many small holdings

00:52:56.080 --> 00:53:00.540
instead of one that's organized
around huge agribusinesses

00:53:00.540 --> 00:53:04.667
the way commodity
crops are in the US

00:53:04.667 --> 00:53:06.375
and you take that
technology and you say,

00:53:06.375 --> 00:53:08.450
I'm going to take it
to this other place

00:53:08.450 --> 00:53:11.070
and you know that the
economies of scale

00:53:11.070 --> 00:53:15.130
are going to privilege the
people with the larger holdings

00:53:15.130 --> 00:53:19.070
at the expense of the people
with the smaller holdings.

00:53:19.070 --> 00:53:22.050
That's where an ethical
burden may kick in--

00:53:22.050 --> 00:53:24.630
that you should be
thinking about what you're

00:53:24.630 --> 00:53:28.605
going to do with the people who
are displaced because they are

00:53:28.605 --> 00:53:32.160
the more vulnerable
and they are the ones

00:53:32.160 --> 00:53:37.840
whose conditions of life will be
so adversely affected that you

00:53:37.840 --> 00:53:39.460
ought to be thinking about it.

00:53:39.460 --> 00:53:42.440
When countries embrace
technology policy

00:53:42.440 --> 00:53:45.200
in a transitional
mode, they often

00:53:45.200 --> 00:53:49.549
take pains to think about
what to do with people who are

00:53:49.549 --> 00:53:50.840
going to be thrown out of work.

00:53:50.840 --> 00:53:52.820
So when Germany, for
instance, decided

00:53:52.820 --> 00:53:55.670
to phase out its
depot coal mines,

00:53:55.670 --> 00:53:58.560
it did start thinking
about how to do

00:53:58.560 --> 00:54:01.420
reskilling work with
some of those people

00:54:01.420 --> 00:54:04.220
who had been in the mines.

00:54:04.220 --> 00:54:07.520
And whose responsibility
this is-- it's

00:54:07.520 --> 00:54:10.870
not just a question of
Brookline versus Uber.

00:54:10.870 --> 00:54:14.480
It's really an open
question for us

00:54:14.480 --> 00:54:17.290
as members of civilized
technologically

00:54:17.290 --> 00:54:21.340
advanced, striving,
innovating societies to keep

00:54:21.340 --> 00:54:23.460
on asking those questions.

00:54:23.460 --> 00:54:29.040
Acting as if it doesn't
matter, as if all disruption is

00:54:29.040 --> 00:54:31.389
the same anyway--
which I think it isn't.

00:54:31.389 --> 00:54:32.930
I mean, whether you
disrupt something

00:54:32.930 --> 00:54:35.920
that was unjustly created
or whether you disrupt

00:54:35.920 --> 00:54:39.350
something that has
democratic backing behind it

00:54:39.350 --> 00:54:42.520
are two different kinds
of considerations.

00:54:42.520 --> 00:54:47.171
In any case, I think nobody
should be off the hook, OK?

00:54:47.171 --> 00:54:47.670
that?

00:54:47.670 --> 00:54:50.660
Is, to think forward about
the consequences-- just giving

00:54:50.660 --> 00:54:53.176
somebody a free pass, saying,
well you're doing technology,

00:54:53.176 --> 00:54:54.800
you're doing innovation,
therefore, you

00:54:54.800 --> 00:54:56.420
don't have a
responsibility-- that

00:54:56.420 --> 00:54:58.170
is clearly not a way to go.

00:54:58.170 --> 00:55:00.230
And we do a lot
of different ways

00:55:00.230 --> 00:55:02.170
in which these debates
can be organized.

00:55:02.170 --> 00:55:05.410
That is, there are
zillions of experiments

00:55:05.410 --> 00:55:09.480
that have been done with how to
engage a diversity of publics.

00:55:09.480 --> 00:55:11.800
I don't think that we're
operating in a vacuum.

00:55:11.800 --> 00:55:14.200
I don't think it's
a blank slate.

00:55:14.200 --> 00:55:17.290
I think that if you take
the challenge of how

00:55:17.290 --> 00:55:20.250
to do democracy
seriously, recognizing

00:55:20.250 --> 00:55:22.530
that the introduction
of technology

00:55:22.530 --> 00:55:26.780
disturbs the dynamics of
politics in the same way

00:55:26.780 --> 00:55:29.460
that the introduction of
a new piece of legislation

00:55:29.460 --> 00:55:31.300
or a constitutional
amendment does.

00:55:31.300 --> 00:55:34.690
Then I think that it
liberates people to go back

00:55:34.690 --> 00:55:38.190
to the theme of liberation to
rethink how to do democracy

00:55:38.190 --> 00:55:40.600
in more creative ways.

