Speaker 1:          00:00          You are listening to radiolab radio from W. N y. S. E. N. N. P. R. Hi there. We're going to start today's program with a fellow named Robert. Is it Epstein or Epstein? Uh, just think Einstein with an app. Okay. That would make an Epstein, I guess that's right. And where are we reaching you right now? I am in a, the San Diego area. Robert Epstein is a psychologist, our editor in chief of psychology today magazine. He's written a ton of books on relationships and love. And he also happens to be one of the world's leading researchers in computer human interactions, like artificial intelligence, basically. That is correct, yes.

Speaker 1:          00:48          So when did you decide to go onto the computer to get a date? 2006 maybe. Why do you ask? Oh, no reason. No. What happened? You, you or you had gotten divorced? Yeah, I was single at the time. Yeah, I was divorced. And you decided that you'd try loving all the right places or what? Oh, sure. Bill online dating of everyone was doing it. My cousin actually convinced me to try it. So I did went online and I looked at photos and I looked at profiles and uh, you know, and I communicated with various people who, who, who were willing to talk to me. Yeah. One of the women I was communicating with, uh, lived in southern California where I do so I thought that's great cause you know, you want someone to be nearby. And she had a very attractive photo online and her English was poor, which at first bothered me. And then she said, well she's not really in California. She's really in Russia. Oh. But all four of my grandparents came from Russia. So I thought, well I'll go with it.

Speaker 1:          01:58          So I continued to write to hurt. I suites the Atlanta. It's really warned you now. And I've been doing a lot of swimming. I've also been writing and doing the computer programming. She wrote back to me and very poor English. Hello Dan Rollback. Do you mind? I hate procedure later. I in theory can't be. I remember that she liked to walk in park and tell him to walk with the girlfriend and the bent and walked in. [inaudible] telling me about her family and her mom. Let me about youth today. MV spoke much and long time. They lived in a small apartment. I knew where in Russia they lived.

Speaker 1:          02:34          It felt like we were bonding for sure. Hello. I might be able to come to Moscow on Sunday, April 15th departing Thursday, April 19th with love Robert. No, so it was getting serious. Oh yeah, of course. Then what happened? Well, two months passed and I began to feel uncomfortable. Something wasn't right. There were no phone calls. [inaudible] some point I began to suggest phone call, but there weren't any, but the main problem was I would say the something like, did you get my letter about me coming to Moscow in April or tell me more about this friend of yours that you mentioned and she did not, didn't mind. I'm very glad to your letter. She did not, she was still replying with fairly long emails. Find vetted my CT very bad, but they were kind of rambling in general. I think of you all days much and I very much want to see more likely you.

Speaker 1:          03:31          I already gave you some dates for our visit to Moscow, my love. What do you think about that? Then at some point a little bell went off in my head finally and I started to send some emails, which uh, let's say included random alphabet letters. So you say, how, what are you wearing tonight? Are you wearing a DBG, g, G, l, p? Exactly. And it didn't make any difference. Halo, do you have Robert? Your letters do mean very hippy. When I open a letter box and that's when I realized Ivana was not a person. Ivana was a computer program I had been had. Wow. So what did you think? I felt like a fool. I felt like an incredible fool, especially given my background that I had been fooled that long. Now I can tell you now this is, this is something I've never made public about.

Speaker 1:          04:29          The other example. Robert went on to tell us that not long after that first incident he was corresponding with someone woman I thought who also turned out to be a robot and he discovered at this time because a programmer contacted me from the UK and said, I know who you are. You have not been communicating with a person you've been communicating with a chat Bot. You've been now undressed twice by robots so to speak. Well and maybe more than twice. Well, how common do you think this is? Do you think that match.com and all those places are like swarming with these bots? You know, you know, I bet you they are [inaudible] that's what you have to understand. There are hundreds of these things out there. There might be thousands.

Speaker 1:          05:16          That's what's coming through in a world like this. We are surrounded by part of the short life. You look like things can get a little confusing. In fact, we're going to do a whole show about that confusion about the sometimes peculiar, sometimes strange things that can happen when humans and machines collide. Collide, but don't quite know who's on what side of the road. Yeah, I don't know. Jet. Boom. Rod. That was good. That was good. Just go with it. Okay. I'm Robert Krulwich. This is radio lab and we're talking to machines. Worse. Should send me your credit card. [inaudible]

Speaker 2:          06:05          so keep talking. Just start things off. Let's introduce you to the person who really hooked us on this whole idea of human robot Chit Chat. My name is Brian Christian. He's a writer. Are you Christian religiously? Uh, no man, that's not at all related to anything that's wrong with you. That's his name, but didn't know. What's important is that he wrote a book called the most human human, which is all about uh, the confusing things that can happen when people and machines interact. How did you, this is such a curious thing to get. Yeah. How did you get into this? I played with Ms dos intently when I was a child. Yeah, there you go. Yeah. Dos is kind of the early version of windows. I was programming these sort of rudimentary maze games, like a curse or going through a maze. Yeah. Basically did this by any chance mean you did not develop best friends?

Speaker 2:          06:51          A lot of my best friends were also into that. Yeah. Wow. We were not the coolest, but we had a lot of fun. So there you are and and you just had a, you just have a talent for this. Yeah. I don't know what it was. I mean I was just, there was something I think fascinating to me that that you could take a process that you knew how to do, but in breaking it down to steps that were that explicit, you often learned something about how the process actually works. For me, programming is surprisingly linked to introspection. How exactly? Well, you know, if a computer were a person, you can imagine someone sitting in your living room and you say, you know, can you hand me that book? And it would say, no, I can't do that because there's a coffee cup on it. And you say, okay, well pick up the Coffee Cup and hand me the book. And it says, well, I can't do that because now I'm holding the cup. And you say, okay, put down the cup, then pick up the book and what you quickly learned,

Speaker 3:          07:45          Brian, is it even really simple human behaviors are made up of a thousand sub routines. I mean, if you really think about it, the book task requires knowing what is a book you have to learn how to about elbows and wrists, how to grab something. What is a book? I already said that, oh, you need to know about gravity. If it's a machine, you have to teach it physics everything in the world in order for it to just pick up a spoon or buck. I knew that.

Speaker 2:          08:11          So now think of that Svetlana bought earlier. Okay. Trying to make something that could actually mimic human conversation. Kinda, sorta imagine all this stuff you'd have to throw into that. Okay. English grammar, syntax and text tone, mood, sarcasm, higher ne adverbs. Turn taking. Oh well it's not actually as impossible as you'd imagine. This is kind of startling. If you go back to the very early days of software programming in the mid sixties, 1964 and 1965 this was actually done with a little program called Eliza, and it was developed by Joseph Weizenbaum at MIT. What in Wiseman bomb's case. His model was not a Russian hottie instead, it was a non-directive Rogerian. The one surface, it's a particular school of therapy. The kind of where the therapist basically mirrors what you're saying, what you're saying, like you're saying, this is Sherry Turkle's, she's an anthropologist at the Massachusetts Institute of Technology and she worked with joeys and Bam.

Speaker 2:          09:10          Where's it Weizenbaum? It's Weizenbaum at MIT. So if you say, you know, I, I'm feeling depressed. A therapist says, I'm sorry to hear you're feeling depressed. Tell me more. Um, Joseph Weizenbaum decides, you know, I think that's an easy enough type of conversation that I can program that into my computer. And so he writes up a simple little program, just about a hundred lines of code, which does sort of what your therapist does, where it looks for a keyword and what you're saying as an I'm feeling depressed, keyword, depressed, latches onto it, and then basically flips it back to you. I'm sorry to hear that you're feeling keyword depressed, right? It's basically a program that inverts your words and it's, it's a, it's a language game. So here's what he did. He put a computer on a desk running this little program and then he invited people to sit down and talk to it.

Speaker 2:          09:55          They'd write in something and it would spit back the keyword and then you would then repeat and on and on. You wouldn't think this would be very interesting, right? Nope. But he found that his students want, who knew that the program didn't know or understand who had helped him write the program. Sophisticated students nevertheless wanted to be alone with it. Wow. They actually helped them write the program. I can read you if you like one of the conversations. Yes, please. This is an actual transcript between Eliza and a young woman. Men are all alike. In what way? They're always bugging us about something or other. Can you think of this?

Speaker 4:          10:31          Well, my boyfriend made me come here. Your boyfriend made you come here. She says I'm depressed much of the time. I'm so sorry to hear you are depressed. It's true. How am I unhappy? Do you think coming here will help you not to be unhappy? Oh, I need some help that much team certain. What would it mean to you if you got some help? Perhaps I could learn to get along with my mother. Tell me more about your family.

Speaker 2:          10:51          Wow. And so it's this vacuum go on for hours. Well, and in fact it did.

Speaker 4:          10:56          My mother takes care of me most in your family takes care of you. My father, his father. You're like my father in some ways. You see, while you're not very aggressive, what makes you think I'm not very aggressive? You don't argue with me. Why do you think I don't argue with you? You are afraid of me. Leads you to believe that there's a woman in this transcript messing around or is she really pouring her heart out? We know that this is a woman who works for Joe Weizenbaum. She's sitting in the office and she just can't stop talking to it. That's all we know. What else comes to mind when you think of your father bullies and why's your mom is watching all this and he, he first thought it was funny and then he didn't think it was funny because they were actually having conversations with it.

Speaker 2:          11:36          One day he comes into the office and his secretary is on the computer divulging her life story to it. According to Weizenbaum, she even told him to please leave the room so she could be alone with it and talk to it. And he, he, um, he was very upset.

Speaker 5:          11:54          Nevertheless,

Speaker 1:          11:55          when word about Eliza got out,

Speaker 2:          11:57          the medical community sort of latches onto it really and says, oh, this is going to be the next revolution in therapy.

Speaker 6:          12:04          Pick something new and promising in the field of psychotherapy.

Speaker 2:          12:07          This is from a newscast around that time, therapists in like phone booths and cities. And you're going to walk in and put a quarter in the slot and have, you know, half an hour of therapy with this automatic program.

Speaker 6:          12:18          Either time can be rented for $5 an hour and that's every reason to suspect that it will go down significantly.

Speaker 2:          12:24          People really thought that they were going to replace therapists with computers. Absolutely. Really did. Absolutely. Yeah. And it was just this really appalling moment for Weizenbaum of there's something, the genie is out of the bottle, maybe in a in a bad way, and he does this one ATF his entire career, so he pulls the plug on the program, he cuts the funding and he goes from being one of the main advocates for artificial intelligence to basically committing the rest of his career to fighting against artificial intelligence.

Speaker 1:          13:01          Rude to admit and sign in for COPD for your [inaudible]. This is Joseph Weizenbaum interviewed in German just before he died in 2008 it was on the German documentary plug and pray. Mine, Cowen [inaudible] is my main objection. He's a Venda sting. The thing says, I understand that if somebody typed in something and the machine says, I understand that is talk new man, Duh, there's no one there as a high Luger. So it's a lie niche when, uh, each can be a niche flush down and I can't imagine that people who are emotionally imbalanced could be effectively treated by systematic email lines to message undo.

Speaker 5:          13:42          I must say that my reaction to the Eliza program at the time was to try to reassure him at the time what I thought people were doing was using it as a kind of interactive diary, knowing that it was a machine, but using it as an occasion to breathe life into it in order to get their feelings out.

Speaker 4:          14:08          I think she's right to have said that to him too. Yeah, because he says it's a lie. Well, it is a lie. How the machine can't love anything. Yes. And if you are a sensible human being, you know that and he's sitting right there on the desk. It's not pretending well, these are sensible human beings that we're already a little bit seduced. Let Matt just go forward a hundred years. Imagine a machine that is very sophisticated, very fluent, very convincingly human. I'm talking about blade runner basically. Yeah, exactly. At that point I think I would require some kind of label to remind me that this is [inaudible]

Speaker 5:          14:43          the thing. It's not a being, it's just a thing. Okay. But if here's something to think about. If the machines get to that point, which is the big where you'd

Speaker 2:          14:52          want to label them, well, you're going to need a way to know when they've crossed that line and become mindful. Yeah. So I should back up for a sec and say that in 1950, they're, they're just starting to develop the computer and they're already asking these philosophical questions like, can these machines think, you know, will be someday be able to make a machine that could think, uh, and if we did, how would we know? And so a British mathematician named Alan Turing proposed a simple thought experiment. Here's how we'll know when the machines make it across the line. Get a person, sit him down at a computer, have them start a conversation and text, hi, how are you? Enter good. Pops up on the screen. Sort of like Internet chat. Yep. So after that first conversation, have them do it again and then again, you know, hi, hello, how are you?

Speaker 2:          15:40          Et cetera. [inaudible] back and forth. Then again, right over and over. But here's the catch. Half of these conversations will be with real people. Half will be with these computer programs that are basically impersonating people and the person in the seat, the human has to judge which of the conversations were with people, which were with humans. Turing's idea was that if those computer fakes could fool the human judge a certain percentage of the time Turing's magic threshold was 30% then at that point we can basically consider machines intelligent. Cause you know, if you can't tell the machine isn't human, then you can't say it's not intelligent. Yeah. That's basically, yeah. You said 30% of the time. Yeah. Touring is the natural number to me would be half. You know, 51% would seem to be like the coaching moment, right? 30% oh well 51% is actually a horrifying number in the context of the touring test because you've got these two conversations and you're trying to decide which is the real person. So if the computer were indistinguishable, that would be 50% you know the judge is doing no better than chance. So if a computer hits 51% that means they're they've out human to the humility of that is horrifying.

Speaker 2:          16:48          Now, something to keep in mind when touring thought this whole thing up, the technology was so new. Computers barely existed that it was sort of a leap of imagination really. But no longer Robert Bring it. Can you give me like some excitement music here?

Speaker 7:          17:01          Absolutely good. Because every year the greatest technologist on the planet, hi, I'm really copping to meet in a small room with folding chairs. I developed in Java and put Alan Turing's questions to the ultimate test.

Speaker 2:          17:23          It really, it's just a couple of dudes you know who haven't seen the sun in 10 years in a room, but we do now have this thing called the lobe enterprise, which is essentially a yearly actual touring test.

Speaker 7:          17:35          Judge judges' table is going to be communicating with two entities, one Schumann and one trainer.

Speaker 2:          17:42          The way the stage is set up is you've got the judges at a table on the left on laptops, Ben, a bunch of giant server looking machines in the middle that programmers are fiddling with and then there's a curtain on the right hand side and we're behind the curtain. Brian actually participated in the 2009 Lobe Nour prize competition but not as a programmer as one of the four quote confederates, the confederates are the real people that the judges are talking to. Cause remember a half the conversations that have are with people, half are with computers and then Brian decided to participate that year because the year before 2008 the top program minister fool, 25% of the judging panel pretty close to Turing's number. Exactly. One vote away. And so I felt to some extent, how can I get involved on behalf of humanity? How can I screw it up?

Speaker 2:          18:32          Take a stand by this position for you. All right, machines, please hold. Your places are now representing all Human Brian, Chris Christian. Now in terms of what Bryan is up against, the computer programs have a variety of different strategies. For example, there was one program Brian's year that would do kind of a double fake out [inaudible] where it would pretend not to be a person, but a person who is sarcastically pretending to be a robot. Oh, people would ask it a simple question and it would say, I don't have enough ram to answer that question. Smiley face, and everyone would be like, oh, this is such a wise guy. [inaudible] clips. I want to tell you now about one particular Bot that competed. Brian's here. Hi, I'm really carpenter. That's the guy who made it. My program is called clever bots and that's the BOT. This is a program that employs a very 20 minutes, spooky, spooky, the right, right. A very spooky strategy.

Speaker 5:          19:28          You may be surprised to hear that despite the fact that it's called Clever Bot, it states that it is a Bot. It states that it is never a human right there in front of them. Despite those facts. I received several emails a day from people who believe that actually they are being connected to humans.

Speaker 8:          19:47          [inaudible]

Speaker 5:          19:48          oh, like they think they've been tricked. Yes. Tricked into coming to a site that claims to be a Bot when in fact they're talking to humans.

Speaker 8:          19:56          [inaudible]

Speaker 5:          19:56          but no, no program could possibly respond in this way and there is a certain element of truth in that to explain Rolo carb mature, like Brian was one of those kids who is completely obsessed by computers. I was indeed a computer kid and when he was just a teenager age about 16 or so, wrote his first Chat Bot, I created a program that talked to me. No kidding. Yes. You typed in something and it would say something back though. At that time the responses were essentially pre-program and really simple. Kind of like Eliza, but one evening, I think fast forward many years he is in his apartment and one night he says, a switch suddenly flipped in my, in my mind

Speaker 8:          20:35          [inaudible]

Speaker 5:          20:36          guys had any sore how to make the machine learn on its own. What if you thought, what if it just started at zero like a little baby and it would grow in these discrete little increments every time you talk to it, right? Basically, uh, w the first thing that was said to that program that I created, the first version of that night, um, was said back by it. Meaning if he said to add hello, it now knew one thing, the word hello. So it would say hello back. The second thing it said was a choice of the first two things said to it. So if the second thing you said was, how are you doing it now? New Two things. The word hello and the phrase, how are you doing? So it could either say hello back again or no, how are you doing? The third thing it said was a choice of the first three things and so on. Ad Infinitum will not quite ad infinitum. But between 1988 and 1997, uh, a few thousand conversations took place between myself in it and a few of my friends and it, he and his friends would sit there and type things to it as a way of teaching it, new things, but it was just them. So it was slow going, languished for quite a long time. But then I started working with the Internet, putting it online where anyone could talk to it within the next 10 years. It had learned something like 5 million lines of conversation.

Speaker 5:          22:01          Now eighties frequently handling around 200,000 requests an outlaw. And it's talking to more than 3 million people a month, 3 million conversations a month. And after each one, clever Bot knows a little bit more than it did before. And every time you say something to it like, hey, clever about town, why am I so sad? Eighties accessing. I think the conversations that millions of people have had in the past asking itself, what is the best overlap? Where is the best correlation? How do people usually answer this question? Why am I so sad? That's right. And then I response everybody answers just because, hmm. All right. Well why? There must be a reason why I'm so sad because you have been sitting in the same place for too long. Is that WHO's tight? Who's saying that? Exactly? Where does that response come from? And the answer is it is one human being at some point in the past.

Speaker 5:          23:08          Having said that, so that is one moment of human conversation from one person. Yes. So it's like I'm talking to a ghost. You are talking to it's intelligence if you like, is borrowed from millions of people in the past. I'm a little bit of their conversational knowledge. Their conversational intelligence goes into forming your reply. Now what's interesting says Rollo is that when you start a conversation with clever Bot, it doesn't really have a personality or no one personality. Everbody's is everything to everyone. It's just this big hive really. I have seen, but as you keep talking to it and it's sort of pulling forward from the high of these little ghost fragments of

Speaker 2:          23:46          past conversations, stitching them together, a form does

Speaker 5:          23:49          kind of emerge it, it reflects the person that is speaking to. It becomes somewhat like that person. Someone familiar already. People have very emotional conversations with it. People, um, have complete arguments with it and um, of course they try to, um, to get it into bed by talking dirty to it. Yeah. One thing I can tell you is that I have seen a single person, a teenage girl, um, speaking for 11 hours with just, uh, three 15 minute breaks. Whoa. About what everything. The day will come not too far down the road where clever Bot becomes so interesting to talk to that people will be talking to it all day, every day,

Speaker 2:          24:39          but we're not there yet because the same thing that makes clever bots so interesting to talk to also can make it kind of ridiculous. For example, in our interview with Brian, he was the first person that turned us on to this program. As you were talking, Soren just sort of suggested, well, why don't we just try it right now? You want to talk? You want to tell it? Say to clarify it. I feel blue. Sure. Yeah. Are you pulling clever Bot up? Is it just clever bot.org or something? calm.com I feel, can you say, I feel blue because an asteroid hit my house this morning because, so this is, you've hit on a perfect strategy of a, of dealing with these bonds. Absurdity. Yes. Well, it basically saying something that has never been said before to clever Bot. So it's likely that no one has ever claimed an asteroid hit their house. It's weird enough that it may not be in the database. Okay. Alright. Let's see what it says. Hit My house this morning.

Speaker 3:          25:34          Clever. [inaudible]

Speaker 2:          25:38          I woke up at 1:00 PM this afternoon. Well, there we go. It's not quite so clever. You don't have to worry yet. Which in fact, when I went online to youtube and watched the lobe new competition that Brian attended [inaudible] it turns out none of the computers fooled the judges at all. None. Any? Well, I don't know if none, none, but they did really badly for me. One of the strange takeaways of thinking so much about artificial intelligence is this feeling of how complex it is to sit across the table from someone and communicate with body language and tone and rhythm and all of these things. What happens when those conversations are working out well is that we're willing to move the conversation in ways that allows us to be sort of perpetually startling to one another. That's a good word. Startling.

Speaker 3:          26:38          Yeah.

Speaker 2:          26:39          You learn someone through these small surprises. Thanks to Brian Christian, his excellent book, which inspired this hour. It's called the most human human. Go to radiolab.org for more info. Thanks also to our actors, Sarah Thyer, Andy Richter, and Susan Blackwell.