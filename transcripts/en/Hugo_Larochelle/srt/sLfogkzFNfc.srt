1
00:00:00,490 --> 00:00:01,001
In this video,

2
00:00:01,001 --> 00:00:04,510
we'll look at a common way of initializing
the parameters of a neural network.

3
00:00:06,050 --> 00:00:08,870
So we're not the final step in,
uh,

4
00:00:08,900 --> 00:00:13,520
our recipe for obtaining
the whole cicastic gradient
descent training algorithm

5
00:00:13,521 --> 00:00:17,560
for neural net. Uh, so I have to
look at the initialization methods.

6
00:00:17,561 --> 00:00:20,080
So how do we initialize the,
uh,

7
00:00:20,090 --> 00:00:23,600
initial value of all of our parameters,
all of our,

8
00:00:23,770 --> 00:00:28,610
a hidden layer weights and biases as
well as the output weights and biases?

9
00:00:31,360 --> 00:00:34,000
Okay. Well, um, typically, uh,

10
00:00:34,001 --> 00:00:38,370
we often first initialized
devices to a zero. Uh,

11
00:00:38,380 --> 00:00:41,530
so we don't express any
preference for the, uh,

12
00:00:41,560 --> 00:00:46,360
hidden units taking values that
are above or below, uh, you know,

13
00:00:46,361 --> 00:00:48,510
closer to the saturation point,

14
00:00:48,511 --> 00:00:53,470
the lower saturation point or the
higher saturation point. Um, if,

15
00:00:53,560 --> 00:00:55,340
uh,
if you're using a,

16
00:00:55,341 --> 00:00:59,890
a sigmoid activation function and a,

17
00:00:59,891 --> 00:01:02,920
if you would like most hidden
needs to be close to zero,

18
00:01:02,921 --> 00:01:07,900
taps us more sparsity in the
activation of, uh, your, uh,

19
00:01:07,930 --> 00:01:11,710
layers. Uh, it's sparsity something
we haven't discussed so much, uh,

20
00:01:11,711 --> 00:01:15,160
in later videos we'll talk about
that and why that might be desirable.

21
00:01:15,370 --> 00:01:16,240
But if you do this,

22
00:01:16,241 --> 00:01:19,950
then you might want to initialize the
biases to a large negative violin.

23
00:01:20,020 --> 00:01:24,470
Like say, you know, the bias could
initially be minus 10 plus, you know,

24
00:01:24,490 --> 00:01:27,880
the activation that would
give you the value of a unit.

25
00:01:28,600 --> 00:01:32,650
And so because of this, uh, if this
term is not too large, initially,

26
00:01:32,651 --> 00:01:34,420
most units would be close to zero.

27
00:01:34,421 --> 00:01:38,410
So that's one way of initializing and
neural network in a situation where it's

28
00:01:38,440 --> 00:01:42,600
initially sparse in its hidden
activations. Uh, but, uh,

29
00:01:42,640 --> 00:01:45,190
that's for special cases where
you think that's a good idea,

30
00:01:45,191 --> 00:01:50,120
just in general a good recipes
to just initialized to zero. Um,

31
00:01:50,550 --> 00:01:54,130
now for the weights, uh,
well let's see. First, uh,

32
00:01:54,190 --> 00:01:58,100
could we initialize them to zero?
Well, uh, if we use the, uh,

33
00:01:58,220 --> 00:02:00,130
tench activation function,

34
00:02:00,430 --> 00:02:05,350
we can actually show that all gradients
will be zero at the first time.

35
00:02:05,350 --> 00:02:09,250
You compute your gradients. And
so, uh, if the grain size zero,

36
00:02:09,251 --> 00:02:13,210
then you're updating your weights with
me in the direction that the vector of

37
00:02:13,211 --> 00:02:16,180
zero, so you're not
changing the weights. Uh,

38
00:02:16,181 --> 00:02:19,810
and so this essentially corresponds
to settle point. You've converged,

39
00:02:20,200 --> 00:02:23,410
quote unquote, and uh, you can't
move away from that initialization.

40
00:02:23,500 --> 00:02:28,270
So that's obviously a
bad idea. Um, well, okay,

41
00:02:28,300 --> 00:02:28,661
could we,

42
00:02:28,661 --> 00:02:33,340
should I initialize all the weights to
non zero value but to exactly that same

43
00:02:33,341 --> 00:02:36,190
non zero value,
but it's also a bad idea.

44
00:02:36,191 --> 00:02:41,191
And that's because in that case we can
show that all hidden units in all layers,

45
00:02:41,620 --> 00:02:43,780
uh,
we'll always behave the same.

46
00:02:43,930 --> 00:02:48,520
All the Hinns will compute
exactly the same, uh, uh,

47
00:02:48,550 --> 00:02:51,130
activation function and a,

48
00:02:51,140 --> 00:02:55,990
essentially all of the units there are
connections with the layer below will

49
00:02:55,991 --> 00:03:00,190
always stay the same. And,
uh, and intuitively though,

50
00:03:00,191 --> 00:03:03,190
we want the neural net where we have
different than Munis that do different

51
00:03:03,191 --> 00:03:07,240
things. And so we need to break the
initial symmetry that we've, uh,

52
00:03:07,260 --> 00:03:11,320
that would have enforced if we had used
the same identical value for all weights

53
00:03:11,330 --> 00:03:15,910
of all the units.
And so the recipe that people use,

54
00:03:15,960 --> 00:03:20,590
the usually follow is that they're
going to samples the cast actually the

55
00:03:20,591 --> 00:03:24,310
initial value for all the weights.
Uh,

56
00:03:24,340 --> 00:03:29,340
and one thing that we propose here is
to use a uniform distribution in some

57
00:03:29,951 --> 00:03:34,540
interval minus B two B. So,
uh, around centered at zero.

58
00:03:34,960 --> 00:03:39,490
And the value that suggested that we
propose here is the square root of six

59
00:03:39,491 --> 00:03:44,491
divided by the square root of the sum
of the number of units in layer k and

60
00:03:45,041 --> 00:03:49,920
number of units in layer
came on this one. Uh, and um,

61
00:03:50,200 --> 00:03:55,000
so that might seem like a strange
formula. Uh, let's first no, this,

62
00:03:55,001 --> 00:03:59,770
that, um, one of assemble
around zero because we want
the initially small weights.

63
00:03:59,770 --> 00:04:03,280
We want to start with a simple neural
network that's not to nonlinear.

64
00:04:03,850 --> 00:04:08,530
And because the probability that
I'll weights are sampled, you know,

65
00:04:08,650 --> 00:04:13,240
uh, get a initial value that's exactly
the same as essentially zero. Uh,

66
00:04:13,300 --> 00:04:14,620
then we can break symmetry.

67
00:04:14,740 --> 00:04:18,790
So now each in the unit is going to be
initialized with a slightly different, uh,

68
00:04:18,920 --> 00:04:22,360
initials, state initial
set of weights. Um,

69
00:04:22,540 --> 00:04:25,030
so other values for B could work well,

70
00:04:25,060 --> 00:04:27,970
this is just one proposal
and this isn't like,

71
00:04:28,060 --> 00:04:32,040
this isn't an exact science determining
good initialization for their own that,

72
00:04:32,070 --> 00:04:35,080
uh, the parameters of the neural
net can look at this paper.

73
00:04:35,081 --> 00:04:39,700
This is where I got this
formula here. And, uh,

74
00:04:39,730 --> 00:04:44,730
they can show under certain conditions
that the hidden layer values at the end

75
00:04:45,640 --> 00:04:49,630
layer activations and also the
gradients are back propagated,

76
00:04:49,870 --> 00:04:53,710
will tend to have a
similar ranges of values,

77
00:04:53,711 --> 00:04:58,440
similar variants across the
different layers. And that's
something that, uh, means,

78
00:04:58,530 --> 00:04:58,810
uh,

79
00:04:58,810 --> 00:05:02,440
so if you have very big gradients at say
the top hidden layer in small green and

80
00:05:02,441 --> 00:05:05,330
set the Laura hidden layer,
then uh,

81
00:05:05,950 --> 00:05:10,570
intuitively this means that the top in
layer we'll train much faster because

82
00:05:10,700 --> 00:05:11,120
it's,

83
00:05:11,120 --> 00:05:14,320
it's pushed further away from its current
value because the gradients are bigger

84
00:05:14,520 --> 00:05:18,160
than the, uh, lower hidden there
if they had those smaller gradient.

85
00:05:18,820 --> 00:05:22,970
And so they tried to correct for this at
least initially by using this value in,

86
00:05:22,971 --> 00:05:26,680
they show under certain conditions
that this is well behaved. All right,

87
00:05:26,681 --> 00:05:29,740
so this is a very reasonable,
uh,

88
00:05:29,800 --> 00:05:34,210
initialization recipe to use and
I encourage you to use that. And,

89
00:05:34,211 --> 00:05:36,160
uh, but, uh, as I stated,

90
00:05:36,680 --> 00:05:40,750
there are other papers that might suggest
other values which might work well.

91
00:05:40,930 --> 00:05:45,580
One thing that she would want to do is
perhaps use always the same formula,

92
00:05:45,610 --> 00:05:48,870
but at least try different
seed for how you, Jay,

93
00:05:49,030 --> 00:05:52,200
for your random number generator
that generates, uh, uh,

94
00:05:52,240 --> 00:05:53,830
numbers between minus B and B.

95
00:05:54,280 --> 00:05:58,950
And so we'll try different values and
then see which one works better, uh,

96
00:05:59,080 --> 00:06:03,610
as a solution for, in terms of the quality
of the neural net you get. All right,

97
00:06:03,611 --> 00:06:07,750
so this is a good recipe to follow
for initializing your neural network.

