WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.500
The following content is
provided under a Creative

00:00:02.500 --> 00:00:03.910
Commons license.

00:00:03.910 --> 00:00:06.950
Your support will help MIT
OpenCourseWare continue to

00:00:06.950 --> 00:00:10.600
offer high-quality educational
resources for free.

00:00:10.600 --> 00:00:13.500
To make a donation or view
additional materials from

00:00:13.500 --> 00:00:18.480
hundreds of MIT courses, visit
MIT OpenCourseWare at

00:00:18.480 --> 00:00:19.730
ocw.mit.edu.

00:00:22.570 --> 00:00:25.080
JOHN DONG: All right, so I'm
sure everybody's kind of

00:00:25.080 --> 00:00:30.750
curious about Project 2.2
Beta, so here's the

00:00:30.750 --> 00:00:32.310
preliminary performance
results.

00:00:32.310 --> 00:00:34.800
There's still a bit more work
to do on finalizing these

00:00:34.800 --> 00:00:37.210
numbers, but here's
how they look.

00:00:37.210 --> 00:00:40.270
But before I show you guys that,
I'd like to yell at you

00:00:40.270 --> 00:00:41.790
guys for a little bit.

00:00:41.790 --> 00:00:46.100
So this is a timeline of the
submission deadline and when

00:00:46.100 --> 00:00:47.680
people submitted things.

00:00:47.680 --> 00:00:52.610
So from this zone to this zone
is an hour, and I see like 50%

00:00:52.610 --> 00:00:55.830
of the commits made during
that period.

00:00:55.830 --> 00:01:00.660
So a little bit of background,
the submission checking system

00:01:00.660 --> 00:01:04.269
automatically clones a
repository up to the deadline

00:01:04.269 --> 00:01:05.880
and not a second after.

00:01:05.880 --> 00:01:10.200
So if you look at from this
to this, some people took

00:01:10.200 --> 00:01:11.870
quite a jump back.

00:01:11.870 --> 00:01:15.810
So just as a warning, please try
to submit things on time.

00:01:15.810 --> 00:01:21.370
11:59 means 11:59, and to drive
that point further, this

00:01:21.370 --> 00:01:23.690
is an example of a commit
that I saw in somebody's

00:01:23.690 --> 00:01:27.340
repository, who's name
I blanked out.

00:01:27.340 --> 00:01:30.340
Obviously, it's seven seconds
past the deadline, so the

00:01:30.340 --> 00:01:32.970
automatic repository cloner
didn't grab it.

00:01:32.970 --> 00:01:37.170
And the previous commit to that
was like 10 days ago when

00:01:37.170 --> 00:01:40.760
Reid pushed out pentominoes
grades.

00:01:40.760 --> 00:01:44.340
So don't do that either.

00:01:44.340 --> 00:01:48.140
A little break down on how
often people commit.

00:01:48.140 --> 00:01:50.490
About a quarter of the class
only made one commit to their

00:01:50.490 --> 00:01:52.490
repository.

00:01:52.490 --> 00:01:54.780
Half of you guys did three
to 10 commits, which

00:01:54.780 --> 00:01:56.280
seems to be an up.

00:01:56.280 --> 00:02:00.190
Under 21+, there was somebody
who did 100 some commits,

00:02:00.190 --> 00:02:01.440
which was pretty impressive.

00:02:05.150 --> 00:02:06.830
Yeah, very smart dude.

00:02:06.830 --> 00:02:09.310
Committing often is a good idea,
so that you don't run

00:02:09.310 --> 00:02:10.850
into a situation like before.

00:02:10.850 --> 00:02:14.440
And next time definitely, there
is going to be next to

00:02:14.440 --> 00:02:17.800
zero tolerance for people who
don't commit things on time

00:02:17.800 --> 00:02:19.360
for the deadlines.

00:02:19.360 --> 00:02:22.320
Now, the numbers that
people want.

00:02:22.320 --> 00:02:25.520
So for rotate, just as an
interesting data point, this

00:02:25.520 --> 00:02:28.570
is the 512 by 512 case.

00:02:28.570 --> 00:02:33.400
It seems like not everybody
remembered to carry over their

00:02:33.400 --> 00:02:36.950
optimizations for the 512 case
over to the final, otherwise

00:02:36.950 --> 00:02:38.670
you would expect the numbers
to be a little bit more

00:02:38.670 --> 00:02:43.800
similar and not off by
a factor of eight.

00:02:43.800 --> 00:02:49.860
And for rotate overall, that's
the distribution.

00:02:49.860 --> 00:02:52.340
The speed up factor is
normalized to some constant

00:02:52.340 --> 00:02:55.760
that gives everybody a
reasonable number.

00:02:55.760 --> 00:02:58.780
And there were a lot
of groups that had

00:02:58.780 --> 00:03:00.020
code that didn't build.

00:03:00.020 --> 00:03:00.540
Yes?

00:03:00.540 --> 00:03:02.188
AUDIENCE: Does the speedup
only include

00:03:02.188 --> 00:03:05.250
the rotate dot 64.

00:03:05.250 --> 00:03:06.920
JOHN DONG: Yes.

00:03:06.920 --> 00:03:09.560
Performance was only tested on
the rotate dot 64, which is

00:03:09.560 --> 00:03:12.280
what we said in the
handout also.

00:03:12.280 --> 00:03:14.900
So there were a lot of groups
that didn't build, which

00:03:14.900 --> 00:03:18.260
really surprised me, and I think
it's probably because

00:03:18.260 --> 00:03:20.760
half of you guys pushed things
after the deadline, and I

00:03:20.760 --> 00:03:23.900
presume those things contained
important commits towards

00:03:23.900 --> 00:03:25.730
making your code
actually work.

00:03:25.730 --> 00:03:28.910
But in this case, there was no
header files, there was no

00:03:28.910 --> 00:03:30.590
cross-testing.

00:03:30.590 --> 00:03:34.720
All we did was run your make
file on your code, and we

00:03:34.720 --> 00:03:39.510
replaced your test bed dot c,
and your k timing, so I'm not

00:03:39.510 --> 00:03:42.380
quite sure why people have
code that doesn't build.

00:03:46.210 --> 00:03:51.410
For sort, this was the maximum
size input that we allowed you

00:03:51.410 --> 00:03:52.660
guys to run.

00:03:58.250 --> 00:04:01.210
One group did really well.

00:04:01.210 --> 00:04:03.640
So all of these are correct.

00:04:03.640 --> 00:04:08.120
The correctness test is built
in, and I replaced that with a

00:04:08.120 --> 00:04:10.730
clean copy that contains
a couple additional

00:04:10.730 --> 00:04:13.630
checks, by the way.

00:04:13.630 --> 00:04:19.151
So I tried the other extreme,
a relatively small array.

00:04:19.151 --> 00:04:22.310
AUDIENCE: Is the top
the same person?

00:04:22.310 --> 00:04:23.650
JOHN DONG: I'm not sure whether
or not the top is the

00:04:23.650 --> 00:04:25.080
same person.

00:04:25.080 --> 00:04:29.650
But the distribution wise seems
like people didn't quite

00:04:29.650 --> 00:04:32.750
remember to optimize for
the smallest case.

00:04:32.750 --> 00:04:37.240
And this is the current overall
speedup factors for

00:04:37.240 --> 00:04:38.640
sort averaging.

00:04:38.640 --> 00:04:44.320
We did about 10 to 15 cases
for rotate and sort.

00:04:44.320 --> 00:04:50.240
And that's the overall
speedup distribution.

00:04:50.240 --> 00:04:56.650
SAMAN AMARASINGHE: OK, so now
you're done with individual

00:04:56.650 --> 00:04:59.440
project, so you already did the
last project individually,

00:04:59.440 --> 00:05:02.090
and then we are moving into,
again, a group project.

00:05:02.090 --> 00:05:05.190
So the first thing we have,
setting up automated system

00:05:05.190 --> 00:05:08.490
for you to say who your
group members are.

00:05:08.490 --> 00:05:11.610
So we will send you information,
and with that,

00:05:11.610 --> 00:05:15.150
what you have to do is run a
script saying who your group

00:05:15.150 --> 00:05:17.600
members are, both group members
have to do it, and

00:05:17.600 --> 00:05:20.600
then we will basically clear
that account in there.

00:05:20.600 --> 00:05:24.490
That said, a lot of you didn't
know, in the first project,

00:05:24.490 --> 00:05:27.150
how to work and what's
the right mode of

00:05:27.150 --> 00:05:29.070
operations with the group.

00:05:29.070 --> 00:05:33.360
OK if we gave you to write
100,000 lines of code, it

00:05:33.360 --> 00:05:35.550
makes sense to say, OK, I'm
going to divide the problem

00:05:35.550 --> 00:05:37.910
into half, one person do
one, the other person

00:05:37.910 --> 00:05:39.840
do the other half.

00:05:39.840 --> 00:05:43.500
The reason for doing the group
is to try to get you to do

00:05:43.500 --> 00:05:46.260
pair programming, because
talking to a lot of you,

00:05:46.260 --> 00:05:49.440
getting a lot of feedback, it
looks like most of you spent a

00:05:49.440 --> 00:05:52.330
huge amount of time debugging.

00:05:52.330 --> 00:05:57.080
And since you're only writing
a little amount of code, it

00:05:57.080 --> 00:06:00.990
makes a lot more sense to sit
with your partner next to the

00:06:00.990 --> 00:06:04.010
screen, one person types, other
person looks over, and

00:06:04.010 --> 00:06:06.510
then you have a much faster
way of getting through the

00:06:06.510 --> 00:06:07.500
debugging process.

00:06:07.500 --> 00:06:11.000
So the next one, don't try to
divide the problem by half.

00:06:11.000 --> 00:06:15.200
Just try to find some time,
sit with each other.

00:06:15.200 --> 00:06:18.220
Then the other really disturbing
thing is there have

00:06:18.220 --> 00:06:22.540
been a couple of groups that
were completely dysfunctional.

00:06:22.540 --> 00:06:26.010
We get emails saying, OK, my
group member didn't talk to

00:06:26.010 --> 00:06:29.880
me, or they didn't do any
work, or they were very

00:06:29.880 --> 00:06:31.940
condescending.

00:06:31.940 --> 00:06:35.710
And that's really sad, because
from my experience with MIT

00:06:35.710 --> 00:06:39.080
students, when you guys go to
company, you guys probably

00:06:39.080 --> 00:06:41.640
will be the best programmers
there.

00:06:41.640 --> 00:06:42.510
There's no question about it.

00:06:42.510 --> 00:06:43.590
I have seen that.

00:06:43.590 --> 00:06:45.950
To the point that some people
might even resent it, to have

00:06:45.950 --> 00:06:46.670
this best programmer.

00:06:46.670 --> 00:06:50.700
But what I have seen is the fact
a lot of you cannot work

00:06:50.700 --> 00:06:53.530
in the group, if you haven't
developed that skill, you will

00:06:53.530 --> 00:06:55.590
not be the most impactful
person.

00:06:55.590 --> 00:06:58.700
I have seen that again and again
in my experience with

00:06:58.700 --> 00:06:59.780
doing a start-up.

00:06:59.780 --> 00:07:03.410
Our MIT students, their way of
impacting is put all night do

00:07:03.410 --> 00:07:05.820
the entire project
by themselves.

00:07:05.820 --> 00:07:08.480
Doable when you're doing a
small change to a large

00:07:08.480 --> 00:07:10.690
project, but if you want
to have a big change,

00:07:10.690 --> 00:07:11.380
you can't do that.

00:07:11.380 --> 00:07:13.390
You have to work with the
group, figure out how to

00:07:13.390 --> 00:07:15.630
impact, how to communicate.

00:07:15.630 --> 00:07:18.620
This is more important learning
than, say, trying to

00:07:18.620 --> 00:07:20.390
figure out how you can
optimize something.

00:07:20.390 --> 00:07:26.170
So being individual contributors
and able to do

00:07:26.170 --> 00:07:29.450
amazing things is important,
but the fact that you can't

00:07:29.450 --> 00:07:32.980
work with a group is going to
really make the impact you can

00:07:32.980 --> 00:07:34.700
make much less.

00:07:34.700 --> 00:07:38.840
So please, please learn how to
work with your group members.

00:07:38.840 --> 00:07:42.160
Some of them might not be as
good as you are, and that will

00:07:42.160 --> 00:07:45.330
be probably true in real life,
too, but that doesn't mean you

00:07:45.330 --> 00:07:48.200
can be condescending towards
them, make them feel inferior.

00:07:48.200 --> 00:07:50.260
That doesn't cut it.

00:07:50.260 --> 00:07:53.240
You have to learn how to
work with these people.

00:07:53.240 --> 00:07:59.970
So part of your learning is
working with others, and

00:07:59.970 --> 00:08:01.570
that's a large part
of your learning.

00:08:01.570 --> 00:08:04.190
Don't consider that to be this
external thing, even though

00:08:04.190 --> 00:08:05.710
you might think you can
do a better job.

00:08:05.710 --> 00:08:08.200
Just work with the other person,
especially in pair

00:08:08.200 --> 00:08:09.670
programming, because where
both of you are sticking

00:08:09.670 --> 00:08:12.530
together, it's much easier.

00:08:12.530 --> 00:08:16.310
Because there are four eyes on
your project, they might see

00:08:16.310 --> 00:08:18.510
something you don't see, and
see whether you can work

00:08:18.510 --> 00:08:19.110
together like that.

00:08:19.110 --> 00:08:24.030
And I don't want to hear any
more stories saying, look, my

00:08:24.030 --> 00:08:27.420
partner was too dumb, or my
partner didn't show up, he

00:08:27.420 --> 00:08:30.320
couldn't deal with that.

00:08:30.320 --> 00:08:32.289
Those are not great excuses.

00:08:32.289 --> 00:08:33.940
And so we wonder.

00:08:33.940 --> 00:08:37.299
Some of them, we will pay
attention to it, because it

00:08:37.299 --> 00:08:39.539
might be one person's
unilateral actions

00:08:39.539 --> 00:08:40.850
that lead to that.

00:08:40.850 --> 00:08:43.620
Still, please, try to figure
out how we can

00:08:43.620 --> 00:08:44.910
work with your partners.

00:08:44.910 --> 00:08:47.110
And hope you have a good
partner experience.

00:08:47.110 --> 00:08:48.540
Use pair programming.

00:08:48.540 --> 00:08:51.390
Use a lot of good debugging
techniques, and the next

00:08:51.390 --> 00:08:52.640
project will be fine.

00:09:13.430 --> 00:09:14.260
CHARLES LEISERSON: Great.

00:09:14.260 --> 00:09:17.780
We're going to talk
more about caches.

00:09:17.780 --> 00:09:18.620
Whoo-whoo!

00:09:18.620 --> 00:09:20.170
OK.

00:09:20.170 --> 00:09:24.880
So for those who weren't here
last time, we talked about the

00:09:24.880 --> 00:09:27.980
ideal cache model.

00:09:27.980 --> 00:09:30.840
As you recall, it has a
two-level hierarchy and a

00:09:30.840 --> 00:09:35.040
cache size M bytes, and a cache
line length of B bytes.

00:09:35.040 --> 00:09:37.860
It's fully associative
and is optimal

00:09:37.860 --> 00:09:41.750
omniscient replacement strategy.

00:09:41.750 --> 00:09:45.850
However, we also learned that
LRU was a good substitute, and

00:09:45.850 --> 00:09:49.380
that any of the asymptotic
results that you can get with

00:09:49.380 --> 00:09:52.800
optimal, you could also
get with LRU.

00:09:52.800 --> 00:09:56.120
The two performance measures
that we talked about were the

00:09:56.120 --> 00:09:59.800
work, which deals with what the
processor ends up doing,

00:09:59.800 --> 00:10:04.490
and the cache misses, which is
the transfers between cache

00:10:04.490 --> 00:10:06.590
and main memory.

00:10:06.590 --> 00:10:08.900
You only have to count one
direction, because what goes

00:10:08.900 --> 00:10:16.230
in basically goes out, so more
or less, it's the same number.

00:10:16.230 --> 00:10:21.570
OK, so I'd like to start today
by talking about some very

00:10:21.570 --> 00:10:25.860
basic algorithms that you have
seen in your algorithms and

00:10:25.860 --> 00:10:31.890
data structures class, but which
may look new when we

00:10:31.890 --> 00:10:34.140
start taking caches
into account.

00:10:34.140 --> 00:10:36.960
So the first one here
is the problem of

00:10:36.960 --> 00:10:40.490
merging two sorted arrays.

00:10:40.490 --> 00:10:42.160
So as you recall, you
can basically do

00:10:42.160 --> 00:10:43.660
this in linear time.

00:10:43.660 --> 00:10:48.130
The way that the algorithm works
is that it looks at the

00:10:48.130 --> 00:10:52.330
first element of the two
arrays to be sorted and

00:10:52.330 --> 00:10:55.910
whichever is smaller, it
puts in the output.

00:10:55.910 --> 00:10:59.440
And then it advances the pointer
to the next element,

00:10:59.440 --> 00:11:03.160
and then whichever is smaller,
it puts in the output.

00:11:03.160 --> 00:11:05.600
And in every step, it's doing
just a constant amount of

00:11:05.600 --> 00:11:12.730
work, there are n items, so by
the time this process is done,

00:11:12.730 --> 00:11:15.770
we basically spent time
proportional to the number of

00:11:15.770 --> 00:11:17.270
items in the output list.

00:11:17.270 --> 00:11:20.360
So this should be
fairly familiar.

00:11:20.360 --> 00:11:24.270
So the time to emerge n
elements is order n.

00:11:27.330 --> 00:11:30.320
Now, the reason merging is
useful is because you can use

00:11:30.320 --> 00:11:34.010
it in a sorting algorithm, a
merge sorting algorithm.

00:11:34.010 --> 00:11:39.550
The way merge sort works is it
essentially does divide and

00:11:39.550 --> 00:11:40.740
conquer on the array.

00:11:40.740 --> 00:11:44.980
It divides the array into two
pieces, and it divides those

00:11:44.980 --> 00:11:47.450
each into two pieces, and those
into two, until it gets

00:11:47.450 --> 00:11:49.880
down to something
of unit size.

00:11:49.880 --> 00:11:52.600
And then what it does
is it merges the

00:11:52.600 --> 00:11:55.560
two pairs of arrays.

00:11:55.560 --> 00:12:03.850
So for example here, the 19 and
3 got merged together to

00:12:03.850 --> 00:12:06.260
be 3 and 19.

00:12:06.260 --> 00:12:08.790
The 12 and 46 were already in
order, but you still had to do

00:12:08.790 --> 00:12:11.300
work to get it there
and so forth.

00:12:11.300 --> 00:12:15.480
So it puts everything in order
in pairs, and then for each of

00:12:15.480 --> 00:12:18.790
those, it puts it together into
fours, and for each of

00:12:18.790 --> 00:12:21.730
those, it puts it together
into the final list.

00:12:21.730 --> 00:12:25.010
Now of course, the way it does
this is not in the order I

00:12:25.010 --> 00:12:25.510
showed you.

00:12:25.510 --> 00:12:30.690
It actually goes down and does
a walk of this tree.

00:12:30.690 --> 00:12:36.940
But conceptually, you can see
that it essentially comes down

00:12:36.940 --> 00:12:41.850
to merging pairs, merging
quadruples, emerging octuples,

00:12:41.850 --> 00:12:50.100
and so forth, all the way until
the program is done.

00:12:50.100 --> 00:12:54.790
So to calculate the work of
Merge Sort, this is something

00:12:54.790 --> 00:12:57.300
you've seen before because it's
exactly what you do in

00:12:57.300 --> 00:13:00.290
your algorithms class.

00:13:00.290 --> 00:13:04.160
You get a recurrence that
says that the work,

00:13:04.160 --> 00:13:05.470
in this case is--

00:13:05.470 --> 00:13:07.620
well, if you have only one
element, it's a constant

00:13:07.620 --> 00:13:11.350
amount of work, and otherwise,
I solve two problems of half

00:13:11.350 --> 00:13:15.320
the size doing order n work,
which is the time to merge the

00:13:15.320 --> 00:13:16.440
two elements.

00:13:16.440 --> 00:13:19.020
So classic divide and conquer.

00:13:19.020 --> 00:13:22.380
And I'm sure you're familiar
with what the solution to this

00:13:22.380 --> 00:13:23.320
recurrence is.

00:13:23.320 --> 00:13:26.990
What's the solution to
this recurrence?

00:13:26.990 --> 00:13:28.840
n log n.

00:13:28.840 --> 00:13:32.200
I want to, though, step through
it, just to get

00:13:32.200 --> 00:13:35.520
everybody warmed up to the
way that I want to solve

00:13:35.520 --> 00:13:39.780
recurrences so that you are in
a position, when we do the

00:13:39.780 --> 00:13:41.030
caching analysis--

00:13:43.560 --> 00:13:47.050
we have a common framework for
understanding how the caching

00:13:47.050 --> 00:13:49.130
analysis will work.

00:13:49.130 --> 00:13:53.200
So we're going to solve this
recurrence, and if the base

00:13:53.200 --> 00:13:56.330
case is constant, we
usually omit it.

00:13:56.330 --> 00:13:58.940
It's assumed.

00:13:58.940 --> 00:14:02.800
So we start out with W of n, and
what we do is we replace

00:14:02.800 --> 00:14:08.610
it by the right hand side, where
we put the constant term

00:14:08.610 --> 00:14:10.230
on the top and then
the two children.

00:14:10.230 --> 00:14:14.630
So here I've gotten rid of the
theta, because conceptually

00:14:14.630 --> 00:14:17.600
when I'm done, I can put a big
theta around the whole thing,

00:14:17.600 --> 00:14:20.610
around the whole tree, and it
just makes the math a little

00:14:20.610 --> 00:14:21.990
bit easier and a little
bit clearer to see

00:14:21.990 --> 00:14:23.320
what's going on.

00:14:23.320 --> 00:14:28.270
So then I take each of those
and I split those, and this

00:14:28.270 --> 00:14:31.460
time I've got n over 4.

00:14:31.460 --> 00:14:32.430
Correct?

00:14:32.430 --> 00:14:34.580
I checked for that
one this time.

00:14:34.580 --> 00:14:37.180
It's funny because it was still
there, actually, just a

00:14:37.180 --> 00:14:41.340
few minutes before class
as I was going through.

00:14:41.340 --> 00:14:44.640
And we keep doing that until
we get down to something of

00:14:44.640 --> 00:14:48.600
size one, until the recurrence
bottoms out.

00:14:48.600 --> 00:14:51.590
So when you look at a recursion
tree of this nature,

00:14:51.590 --> 00:14:54.470
the first thing that you
typically want to do is take a

00:14:54.470 --> 00:14:57.340
look at what the height
of the tree is.

00:14:57.340 --> 00:15:00.560
In this case, we're taking a
problem of size n, and we're

00:15:00.560 --> 00:15:02.600
halving it at every step.

00:15:02.600 --> 00:15:07.910
And so the number of times we
have to halve the argument--

00:15:07.910 --> 00:15:10.910
which turns out to be also equal
to the work, but that's

00:15:10.910 --> 00:15:12.100
just coincidence--

00:15:12.100 --> 00:15:14.730
is log n times.

00:15:14.730 --> 00:15:18.730
So the height is log
base 2 of n.

00:15:18.730 --> 00:15:23.610
Now what we do typically is we
add things up across the rows,

00:15:23.610 --> 00:15:24.900
across the levels.

00:15:24.900 --> 00:15:27.185
So on the top level,
we have n.

00:15:27.185 --> 00:15:31.010
On the next level, we have n.

00:15:31.010 --> 00:15:33.940
The next level, hey, n.

00:15:36.660 --> 00:15:41.890
To add up the bottom, just to
make sure, we have to count up

00:15:41.890 --> 00:15:44.930
how many leaves there are, and
the number of leaves, since

00:15:44.930 --> 00:15:47.720
this is a binary tree, is
just 2 to the height.

00:15:47.720 --> 00:15:53.310
So it's 2 to the log
n, which is n.

00:15:53.310 --> 00:15:58.640
So then I add across all the
leaves, and I get the order 1

00:15:58.640 --> 00:16:03.580
at the base times n,
which is order n.

00:16:03.580 --> 00:16:07.370
And so now I'm in a position
to add up the total work,

00:16:07.370 --> 00:16:11.855
which is basically log n
levels of n is total

00:16:11.855 --> 00:16:13.040
of order n log n.

00:16:13.040 --> 00:16:16.090
So hopefully, this
is all review.

00:16:16.090 --> 00:16:18.190
Hopefully this is all review.

00:16:18.190 --> 00:16:19.310
You haven't seen this before.

00:16:19.310 --> 00:16:21.260
It's really neat, isn't it?

00:16:21.260 --> 00:16:22.950
But you've missed something
along the way.

00:16:26.860 --> 00:16:28.110
So now with caching.

00:16:30.860 --> 00:16:35.500
So the first thing to observe is
that merge subroutine, the

00:16:35.500 --> 00:16:43.190
number of cache misses that it
has is order n over B. So as

00:16:43.190 --> 00:16:45.800
you're going through, these
arrays are laid out

00:16:45.800 --> 00:16:48.530
continuously in memory.

00:16:48.530 --> 00:16:50.900
The number of misses--

00:16:50.900 --> 00:16:52.740
you're just going through
the data once--

00:16:52.740 --> 00:16:56.950
is order n data, all going
through contiguously.

00:16:56.950 --> 00:17:00.210
And so every time you bring in
data, you get full spatial

00:17:00.210 --> 00:17:04.730
locality, there are n elements,
it costs n over B.

00:17:04.730 --> 00:17:06.400
So is that plain?

00:17:06.400 --> 00:17:10.990
Hopefully that part's plain,
because you bring in things of

00:17:10.990 --> 00:17:13.970
each axis, you get the
same factor of B.

00:17:13.970 --> 00:17:15.690
So now merge sort--

00:17:15.690 --> 00:17:18.109
and this is, once again, the
hard part is coming up with a

00:17:18.109 --> 00:17:21.099
recurrence, and then the other
hard part is solving it.

00:17:21.099 --> 00:17:23.560
So there's two hard parts
to recurrences.

00:17:23.560 --> 00:17:27.310
OK, so the merge sort algorithm
solves two problems

00:17:27.310 --> 00:17:30.160
of size n over 2, and
then does a merge.

00:17:30.160 --> 00:17:32.610
So the second line here is
pretty straightforward.

00:17:32.610 --> 00:17:34.930
I take the cache misses
that I need to do,

00:17:34.930 --> 00:17:35.830
and I take a merge.

00:17:35.830 --> 00:17:38.810
I may have a few other accesses
in there, but they're

00:17:38.810 --> 00:17:41.300
going to be dominated
by the merge.

00:17:41.300 --> 00:17:45.080
OK, so it's still going
to be theta n over B.

00:17:45.080 --> 00:17:50.410
Now the hard part, generally, of
dealing with cache analysis

00:17:50.410 --> 00:17:53.590
is how to do the base case,
because the base case is more

00:17:53.590 --> 00:17:56.680
complicated than when you just
do running time, you get that

00:17:56.680 --> 00:17:59.610
to run down to a base case
of constant size.

00:17:59.610 --> 00:18:01.720
Here, you don't get to
run down to base

00:18:01.720 --> 00:18:04.080
case of constant size.

00:18:04.080 --> 00:18:08.900
So what it says here is that
we're going to run down until

00:18:08.900 --> 00:18:13.580
I have a sorting problem that
fits in cache, n is going to

00:18:13.580 --> 00:18:17.190
be less than some constant times
n, for some sufficiently

00:18:17.190 --> 00:18:21.080
small constant c less than 1.

00:18:21.080 --> 00:18:25.420
When in finally fits in cache,
how many cache misses does it

00:18:25.420 --> 00:18:27.300
take me to sort it?

00:18:27.300 --> 00:18:30.490
Well, I only need to have the
cold misses to bring that

00:18:30.490 --> 00:18:36.500
array into memory, and so that's
just proportional to n

00:18:36.500 --> 00:18:41.450
over B, because for all the rest
of the levels of merging,

00:18:41.450 --> 00:18:44.610
you're inside the cache.

00:18:44.610 --> 00:18:46.430
That make sense?

00:18:46.430 --> 00:18:48.190
So that's where we get
this recurrence.

00:18:48.190 --> 00:18:51.980
This is always a tricky thing
to figure out how to write

00:18:51.980 --> 00:18:54.780
that recurrence for
a given thing.

00:18:54.780 --> 00:18:57.620
Then, as I say, the other
tricky thing is how

00:18:57.620 --> 00:18:58.890
do you solve it?

00:18:58.890 --> 00:19:01.420
But we're going to solve it
essentially the same way as we

00:19:01.420 --> 00:19:04.500
did before.

00:19:04.500 --> 00:19:06.880
I'm not going to go through all
the steps here, except to

00:19:06.880 --> 00:19:07.940
just elaborate.

00:19:07.940 --> 00:19:12.140
So what we're doing is we're
taking, and we have n over B

00:19:12.140 --> 00:19:16.200
at the top, and then we divide
it into two problems, and for

00:19:16.200 --> 00:19:18.580
each of those--

00:19:18.580 --> 00:19:22.600
whoops, there's a c there
that doesn't belong.

00:19:22.600 --> 00:19:26.560
It should just be n over
2B on both, and then n

00:19:26.560 --> 00:19:30.170
over 4B, and so forth.

00:19:30.170 --> 00:19:34.500
And we keep going down until
we get to our base case.

00:19:34.500 --> 00:19:39.060
Now in our base case, what I
claim is that when I hit this

00:19:39.060 --> 00:19:43.120
base case, it's going to be the
case that n is, in fact, a

00:19:43.120 --> 00:19:48.800
constant factor times n, so that
n over B is almost the

00:19:48.800 --> 00:19:53.280
same as m over B. And the reason
is because before I hit

00:19:53.280 --> 00:20:00.120
the base case, when I was at
size twice n, and that was

00:20:00.120 --> 00:20:01.410
bigger than m.

00:20:01.410 --> 00:20:04.900
So if twice n is
bigger than m--

00:20:04.900 --> 00:20:07.960
than my constant times m--

00:20:07.960 --> 00:20:17.710
but n is smaller than m, then
it's the case that n and m are

00:20:17.710 --> 00:20:21.210
essentially the same size to
within a constant factor,

00:20:21.210 --> 00:20:23.880
within a factor of
two, in fact.

00:20:23.880 --> 00:20:30.110
And so therefore, here I can say
that it's order m over B.

00:20:30.110 --> 00:20:32.850
And now the question is, how
many levels did I have to go

00:20:32.850 --> 00:20:37.690
down cutting things in half
before I got to something of

00:20:37.690 --> 00:20:40.460
size m over B?

00:20:40.460 --> 00:20:43.270
Well, the way that I usually
think about this is--

00:20:43.270 --> 00:20:46.230
you can do it by taking the
difference as I did before.

00:20:46.230 --> 00:20:49.590
The height of the whole tree is
going to be log base 2 of

00:20:49.590 --> 00:20:55.220
n, and the height of this
is basically log of--

00:20:55.220 --> 00:21:00.250
what's the size of n?

00:21:00.250 --> 00:21:02.725
It's going to be log of the size
of n when this occurs.

00:21:02.725 --> 00:21:07.650
Well, n at that point is
something like cm.

00:21:07.650 --> 00:21:11.590
So it's basically log n minus
log cm, which is basically log

00:21:11.590 --> 00:21:14.840
of n over cm.

00:21:14.840 --> 00:21:16.090
How about some questions.

00:21:22.520 --> 00:21:23.195
Yeah, question.

00:21:23.195 --> 00:21:24.924
AUDIENCE: --the reason for why
you just substituted on the

00:21:24.924 --> 00:21:28.820
left, [INAUDIBLE] over B, but
on the right [INAUDIBLE].

00:21:28.820 --> 00:21:29.630
CHARLES LEISERSON: Here?

00:21:29.630 --> 00:21:30.650
AUDIENCE: No.

00:21:30.650 --> 00:21:31.180
CHARLES LEISERSON:
Or you mean here?

00:21:31.180 --> 00:21:32.430
AUDIENCE: [INAUDIBLE].

00:21:36.100 --> 00:21:36.930
CHARLES LEISERSON: On
the right side.

00:21:36.930 --> 00:21:38.914
AUDIENCE: On the right-most
leaf, it's n over B. Is that

00:21:38.914 --> 00:21:41.180
all of the leaves added up
because [INAUDIBLE]?

00:21:41.180 --> 00:21:42.820
CHARLES LEISERSON: No, no, no,
this is going to be all of the

00:21:42.820 --> 00:21:44.710
leaves added up here.

00:21:44.710 --> 00:21:47.690
This is the stack I have
on the right hand side.

00:21:47.690 --> 00:21:48.520
So we'll get there.

00:21:48.520 --> 00:21:51.625
So the point is, the number of
leaves is 2 to this-- so

00:21:51.625 --> 00:21:54.020
that's just n over cm--

00:21:54.020 --> 00:21:59.660
times m over B. Well, n over
cm times m over B, the m's

00:21:59.660 --> 00:22:03.500
cancel, and I get essentially
n over b with whatever that

00:22:03.500 --> 00:22:06.920
constant is here.

00:22:06.920 --> 00:22:11.490
And so now I have n over b
across every level, and then

00:22:11.490 --> 00:22:18.340
when I add those up, I have to
go log of n over cm levels,

00:22:18.340 --> 00:22:20.560
which is the same as
log of n over m.

00:22:20.560 --> 00:22:24.230
So I have n over B times
log of n over m.

00:22:24.230 --> 00:22:25.281
Yeah, question?

00:22:25.281 --> 00:22:28.718
AUDIENCE: Initial assumption
that c is some sufficiently

00:22:28.718 --> 00:22:31.418
small number, so 1
over c would be a

00:22:31.418 --> 00:22:32.660
rather large factor.

00:22:32.660 --> 00:22:36.570
CHARLES LEISERSON: It could
potentially be a large factor,

00:22:36.570 --> 00:22:37.690
but it's a constant.

00:22:37.690 --> 00:22:41.520
In other words, it can't
vary with n.

00:22:41.520 --> 00:22:44.310
So in fact, typically for
something like merge sort, the

00:22:44.310 --> 00:22:47.930
constant is going to
be very close to--

00:22:47.930 --> 00:22:51.840
for most things, the constant
there is typically only a few.

00:22:51.840 --> 00:22:54.460
Because the question is, how
many other things do you need

00:22:54.460 --> 00:22:56.070
in order to make sure
it fits in?

00:22:56.070 --> 00:22:57.170
In this case, you have n.

00:22:57.170 --> 00:23:00.490
Well, you have both the input
and the output, so here, it's

00:23:00.490 --> 00:23:02.750
going to be, you have to fit
both the input and the output

00:23:02.750 --> 00:23:05.850
into cache in order not
to have the cache it.

00:23:05.850 --> 00:23:07.880
So it's basically going
to be like a factor

00:23:07.880 --> 00:23:09.170
of 2 for merge sort.

00:23:09.170 --> 00:23:12.500
For the matrix multiplication,
it was like a factor of three.

00:23:12.500 --> 00:23:14.600
So generally a fairly
small number.

00:23:14.600 --> 00:23:15.502
Question?

00:23:15.502 --> 00:23:16.978
AUDIENCE: I guess that
makes sense.

00:23:16.978 --> 00:23:19.438
But I [INAUDIBLE].

00:23:19.438 --> 00:23:24.358
So on the order of the size of
the leaves, can you assume

00:23:24.358 --> 00:23:27.320
that n is more than cm, so
you can substitute--

00:23:27.320 --> 00:23:30.510
CHARLES LEISERSON: Yeah, because
basically, when it

00:23:30.510 --> 00:23:31.680
hits this condition--

00:23:31.680 --> 00:23:32.155
AUDIENCE: Right.

00:23:32.155 --> 00:23:33.105
I understand that.

00:23:33.105 --> 00:23:37.754
Then you're also saying, why
isn't there more or less just

00:23:37.754 --> 00:23:42.173
one B, because there's n over
cm, n is the same as cm.

00:23:42.173 --> 00:23:44.137
That's [INAUDIBLE].

00:23:44.137 --> 00:23:46.120
At the bottom level,
there should be--

00:23:46.120 --> 00:23:49.340
CHARLES LEISERSON: Oh, did I
do something wrong here?

00:23:49.340 --> 00:23:50.590
Number of leaves is--

00:23:50.590 --> 00:23:51.840
AUDIENCE: [INAUDIBLE].

00:23:55.260 --> 00:23:55.900
CHARLES LEISERSON:
Right, right.

00:23:55.900 --> 00:23:56.220
Sorry.

00:23:56.220 --> 00:23:58.480
This is the n at the top.

00:23:58.480 --> 00:24:00.610
You always have to
be careful here.

00:24:00.610 --> 00:24:07.520
So this is the n in this case,
so this is some n, little n.

00:24:07.520 --> 00:24:08.900
So this is not the same n.

00:24:08.900 --> 00:24:10.910
This is the n that we
had at the top.

00:24:10.910 --> 00:24:13.650
This is the notion of
recurrences, like the n keeps

00:24:13.650 --> 00:24:16.030
recurring, but you
know which ones--

00:24:16.030 --> 00:24:18.530
so it can be confusing,
because if you're--

00:24:18.530 --> 00:24:19.470
so yeah.

00:24:19.470 --> 00:24:22.270
So this is not the
n that's here.

00:24:22.270 --> 00:24:25.260
This is the n that started
out at the top.

00:24:25.260 --> 00:24:27.110
So we're analyzing it
in terms of the n.

00:24:27.110 --> 00:24:29.380
Some people write these things
where they would write this in

00:24:29.380 --> 00:24:34.410
terms of k, and then analyze it
for n, and for some people,

00:24:34.410 --> 00:24:39.380
that can be helpful to do, to
disambiguate the two things.

00:24:39.380 --> 00:24:43.330
I always find it wastes a
variable, and you know, those

00:24:43.330 --> 00:24:44.580
variables are hard to come by.

00:24:44.580 --> 00:24:45.980
There's only a finite
number of them.

00:24:50.510 --> 00:24:53.310
OK, so are we good for this?

00:24:53.310 --> 00:24:56.210
So here, we ended up with
n over b, log of n

00:24:56.210 --> 00:24:59.980
over m cache misses.

00:24:59.980 --> 00:25:01.990
So how does that compare?

00:25:01.990 --> 00:25:06.500
Let's just do a little
thinking about this.

00:25:06.500 --> 00:25:10.770
Here's the recurrence, and
I solved it out to this.

00:25:10.770 --> 00:25:12.530
So let's just look to
see what this means.

00:25:12.530 --> 00:25:15.890
If I have a really big n, much
bigger than the size of my

00:25:15.890 --> 00:25:22.020
cache, then I'm going to do a
factor of B log n less misses

00:25:22.020 --> 00:25:27.060
than work, because the work
is n over B log n.

00:25:31.380 --> 00:25:37.990
So if n compared to m-- let's
say n was m squared or

00:25:37.990 --> 00:25:39.240
something--

00:25:41.370 --> 00:25:48.060
then n over m would
still be n, if n

00:25:48.060 --> 00:25:50.380
was as big as m squared.

00:25:50.380 --> 00:25:56.180
So if n was as big as m squared,
this log of n over m

00:25:56.180 --> 00:25:59.590
would still be log n.

00:25:59.590 --> 00:26:07.435
And so I would basically have n
over B log n for a factor of

00:26:07.435 --> 00:26:09.160
B log n less misses than work.

00:26:09.160 --> 00:26:10.625
If they're about the same--

00:26:10.625 --> 00:26:11.650
did I get this right?

00:26:11.650 --> 00:26:16.950
If they're about the same size,
if n is approximately m,

00:26:16.950 --> 00:26:21.760
maybe it's just a little bit
bigger, then the log here

00:26:21.760 --> 00:26:27.020
disappears completely, and
so I basically just

00:26:27.020 --> 00:26:28.570
have n over B misses.

00:26:31.237 --> 00:26:32.487
AUDIENCE: [INAUDIBLE].

00:26:36.380 --> 00:26:38.060
CHARLES LEISERSON: Yeah,
but if n is like--

00:26:38.060 --> 00:26:39.800
AUDIENCE: [INAUDIBLE].

00:26:39.800 --> 00:26:40.740
CHARLES LEISERSON: Yeah.

00:26:40.740 --> 00:26:42.980
In fact, for this, you have to
be careful as you get to the

00:26:42.980 --> 00:26:43.860
base cases.

00:26:43.860 --> 00:26:47.470
Technically, for some of this,
I should be saying 1 plus log

00:26:47.470 --> 00:26:50.610
of n over m, and in some of the
things I do later, I will

00:26:50.610 --> 00:26:52.480
put in the ones.

00:26:52.480 --> 00:26:57.030
But if you're looking at it
asymptotically and n gets big,

00:26:57.030 --> 00:26:58.920
you don't have to worry
about those cases.

00:26:58.920 --> 00:27:02.110
That just handles the case
whether you're looking at n

00:27:02.110 --> 00:27:04.130
getting large or whether you're
trying to handle a

00:27:04.130 --> 00:27:06.950
formula for all n, even
if and n is small.

00:27:06.950 --> 00:27:07.850
Question?

00:27:07.850 --> 00:27:09.100
AUDIENCE: [INAUDIBLE]?

00:27:11.770 --> 00:27:15.150
CHARLES LEISERSON: The work
was n log n, yes.

00:27:15.150 --> 00:27:17.050
The work was n log n.

00:27:17.050 --> 00:27:21.890
So here we basically have n over
B log n, so I'm saving a

00:27:21.890 --> 00:27:25.380
factor of B in the case where
it's about the same.

00:27:25.380 --> 00:27:26.830
Did I get this right?

00:27:26.830 --> 00:27:28.880
I'm just looking at this, and
now I'm trying to reverse

00:27:28.880 --> 00:27:32.480
engineer what my argument is.

00:27:32.480 --> 00:27:38.890
So we're looking at n log n
versus n over B log n over m.

00:27:38.890 --> 00:27:41.830
AUDIENCE: [INAUDIBLE PHRASE].

00:27:41.830 --> 00:27:45.995
So that you get a factor of B
less misses, because you would

00:27:45.995 --> 00:27:49.180
be getting n over B like log
of n, that's the only way

00:27:49.180 --> 00:27:51.630
you're getting a factor
of B less misses.

00:27:51.630 --> 00:27:54.406
So I don't understand how you're
saying, for n more or

00:27:54.406 --> 00:27:55.060
less equal to m.

00:27:55.060 --> 00:27:58.000
You would want something
more like, for n--

00:27:58.000 --> 00:28:00.760
CHARLES LEISERSON: Well, if n
and m are about the same size,

00:28:00.760 --> 00:28:07.070
the number of cache misses is
just n over B. And the number

00:28:07.070 --> 00:28:12.430
of cache misses is n over B,
and the work is n log n.

00:28:12.430 --> 00:28:17.106
So I've saved a factor
of B times log n, OK?

00:28:21.320 --> 00:28:22.730
What did I say?

00:28:22.730 --> 00:28:23.980
AUDIENCE: [INAUDIBLE].

00:28:26.960 --> 00:28:28.180
CHARLES LEISERSON: B log m?

00:28:28.180 --> 00:28:31.020
No, I was saying that's
for the case when n is

00:28:31.020 --> 00:28:32.830
much bigger than m.

00:28:32.830 --> 00:28:34.080
So let's take a look
at the case--

00:28:36.750 --> 00:28:38.180
let me just do it on
the board here.

00:28:44.420 --> 00:28:54.440
Let's suppose that n is like m
squared, just as an example,

00:28:54.440 --> 00:28:56.510
big number.

00:28:56.510 --> 00:29:03.120
So I'm going to look at,
essentially, n over B times

00:29:03.120 --> 00:29:06.502
log of n over m.

00:29:09.650 --> 00:29:15.360
So log of n over m, so what is
n over m is about m, which is

00:29:15.360 --> 00:29:20.960
about square root of n, right?

00:29:20.960 --> 00:29:25.950
So this basically ends up being
approximately n over B

00:29:25.950 --> 00:29:34.250
log of square root of n, which
is the same as log n, to

00:29:34.250 --> 00:29:35.290
within a constant factor.

00:29:35.290 --> 00:29:38.540
I'm going to leave out the
constant factors here.

00:29:38.540 --> 00:29:43.546
Then I want to compare
that with n log n.

00:29:50.610 --> 00:29:53.310
So I get a factor of
B less misses.

00:29:53.310 --> 00:29:56.780
So the first one, yes, OK.

00:29:56.780 --> 00:29:59.530
So I get a factor of B less
misses, you're right.

00:29:59.530 --> 00:30:01.230
Then I get a factor
of B less misses.

00:30:01.230 --> 00:30:02.480
So I think I've got
these switched.

00:30:10.330 --> 00:30:13.400
So this is the case I'm doing
is for n much bigger than m.

00:30:16.260 --> 00:30:17.590
So let's do the other case.

00:30:17.590 --> 00:30:19.790
I think I've got the two
things switched.

00:30:19.790 --> 00:30:21.400
I'll fix it in the notes.

00:30:21.400 --> 00:30:25.550
If n and m are approximately
the same, then the log is a

00:30:25.550 --> 00:30:28.230
constant, right?

00:30:28.230 --> 00:30:33.860
So this ends up being
approximately n over B. And

00:30:33.860 --> 00:30:36.470
now when I take a look at the
difference between the number

00:30:36.470 --> 00:30:39.110
of things, I get B log n.

00:30:39.110 --> 00:30:40.660
So I've got the two
things mixed.

00:30:40.660 --> 00:30:41.820
Yeah?

00:30:41.820 --> 00:30:44.640
AUDIENCE: As n approaches m,
then the log would approach

00:30:44.640 --> 00:30:47.460
zero, but were you talking
about how it technically

00:30:47.460 --> 00:30:49.020
should be--

00:30:49.020 --> 00:30:50.247
CHARLES LEISERSON:
1 plus n, yes.

00:30:50.247 --> 00:30:53.217
AUDIENCE: So technically, that
approaches one when the log

00:30:53.217 --> 00:30:54.360
approaches zero.

00:30:54.360 --> 00:30:57.008
CHARLES LEISERSON: Yeah.

00:30:57.008 --> 00:30:58.976
AUDIENCE: These things are
really hard for me, because

00:30:58.976 --> 00:30:59.960
they are really arbitrary.

00:30:59.960 --> 00:31:01.600
And then you're like, oh
yeah, you can just put

00:31:01.600 --> 00:31:02.420
a 1 on top of there.

00:31:02.420 --> 00:31:05.126
And for example, I always miss
those, because I usually try

00:31:05.126 --> 00:31:07.586
to do the math as rigorously
as I can, and those ones

00:31:07.586 --> 00:31:08.356
generally do not appear,
and you're

00:31:08.356 --> 00:31:09.834
like, oh, sure whatever.

00:31:09.834 --> 00:31:13.412
So how am I supposed to know
that the log is actually not

00:31:13.412 --> 00:31:15.771
going to be zero, and I'm going
to be like, yeah, you're

00:31:15.771 --> 00:31:18.140
not going to do any caches.

00:31:18.140 --> 00:31:19.590
CHARLES LEISERSON: Because
generally, what we're doing is

00:31:19.590 --> 00:31:21.950
we're looking at how things
scale, so we're generally

00:31:21.950 --> 00:31:25.410
looking at n being big, in which
case it doesn't matter.

00:31:25.410 --> 00:31:28.530
These things only matter if
n's-- for example, notice here

00:31:28.530 --> 00:31:34.000
that if n goes less than m,
we're in real trouble, right?

00:31:34.000 --> 00:31:35.820
Because now the log
is negative.

00:31:35.820 --> 00:31:37.190
Wait, what does that mean?

00:31:37.190 --> 00:31:41.340
Well, the answer is the analysis
was assuming that n

00:31:41.340 --> 00:31:44.150
was sufficiently large
compared with m.

00:31:44.150 --> 00:31:46.712
AUDIENCE: Why can't you just
be like, oh, when n is for

00:31:46.712 --> 00:31:49.518
less than one, you can assume,
well, n is 2n.

00:31:49.518 --> 00:31:50.982
In that case, you get log
of two, which is still

00:31:50.982 --> 00:31:52.934
something or other.

00:31:52.934 --> 00:31:53.920
CHARLES LEISERSON:
Yeah, exactly.

00:31:53.920 --> 00:31:56.390
So what happens in these things
is if you get right on

00:31:56.390 --> 00:32:01.350
the cusp of fitting in memory,
then these analyses like,

00:32:01.350 --> 00:32:07.020
well, what exactly is the
answer, is dicey.

00:32:07.020 --> 00:32:09.580
But if you assume that
it doesn't fit in,

00:32:09.580 --> 00:32:10.690
what's going to happen?

00:32:10.690 --> 00:32:13.930
Or that does fit in, what
is going to happen?

00:32:13.930 --> 00:32:16.350
And then the analysis
right on the edge is

00:32:16.350 --> 00:32:17.600
somewhere between there.

00:32:20.100 --> 00:32:21.000
Good.

00:32:21.000 --> 00:32:23.040
So I switched these.

00:32:23.040 --> 00:32:25.190
I said this the other
way around.

00:32:25.190 --> 00:32:25.670
That's funny.

00:32:25.670 --> 00:32:30.530
I went through this, and then
in my notes, I had them

00:32:30.530 --> 00:32:34.990
switched, and I said, oh my
gosh, I did this wrong.

00:32:34.990 --> 00:32:37.020
And I've just gone through it,
and it turns out I was right

00:32:37.020 --> 00:32:40.090
in my notes.

00:32:40.090 --> 00:32:42.850
Now, one of the things, if you
look at what's going on--

00:32:42.850 --> 00:32:45.220
let's just go back to
this picture here.

00:32:45.220 --> 00:32:49.410
What's going on here is each one
of the passes that we're

00:32:49.410 --> 00:32:56.010
doing to do a merge is basically
taking n over B

00:32:56.010 --> 00:32:58.100
misses to do a binary merge.

00:32:58.100 --> 00:33:05.890
We're going through all the data
to merge just two things,

00:33:05.890 --> 00:33:08.490
and traversing all the data.

00:33:08.490 --> 00:33:11.530
So you can imagine, what would
happen if I did, say, a

00:33:11.530 --> 00:33:12.780
four-way merge?

00:33:15.210 --> 00:33:20.770
With a four-way merge, I could
actually merge four things

00:33:20.770 --> 00:33:23.505
with only a little bit more
than n over B misses.

00:33:26.210 --> 00:33:29.280
In fact, that's what we're going
to analyze in general.

00:33:29.280 --> 00:33:34.850
So the idea is that we can
improve our cache efficiency

00:33:34.850 --> 00:33:38.650
by doing multi-way merging.

00:33:38.650 --> 00:33:42.350
So the idea here is, let's merge
R, which is, let's say,

00:33:42.350 --> 00:33:44.370
less than n subarrays
with a tournament.

00:33:44.370 --> 00:33:51.250
So here are R subarrays, and
here's they're each, let's

00:33:51.250 --> 00:33:56.370
say, is of size n over R. And
what we're going to do is

00:33:56.370 --> 00:33:58.940
merge them with a tournament, so
this is a tournament where

00:33:58.940 --> 00:34:02.070
we say, who's the winner of
these two, who's the winner of

00:34:02.070 --> 00:34:03.740
these two, et cetera.

00:34:03.740 --> 00:34:07.710
And then whoever wins at the top
here, we take them and put

00:34:07.710 --> 00:34:09.810
them in the output, and then
we repeat the tournament.

00:34:12.320 --> 00:34:13.810
Now let's just look
what happens.

00:34:13.810 --> 00:34:18.830
It takes order R work to produce
the forced output.

00:34:18.830 --> 00:34:20.630
So we got R things here.

00:34:20.630 --> 00:34:23.600
To playoff this tournament,
there are R nodes here.

00:34:23.600 --> 00:34:26.630
They each have to do a constant
amount of comparing

00:34:26.630 --> 00:34:30.399
before I end up with a single
value to put in the output.

00:34:30.399 --> 00:34:34.100
So it costs me R to get
this thing warmed up.

00:34:34.100 --> 00:34:38.540
But once I find the winner,
and I remove the winner

00:34:38.540 --> 00:34:42.120
whatever chain he might have
come along, how quickly can I

00:34:42.120 --> 00:34:46.320
repopulate the tournament
with the next guy?

00:34:46.320 --> 00:34:51.260
The next guy only has to play
the tournament on the path

00:34:51.260 --> 00:34:51.780
that was there.

00:34:51.780 --> 00:34:53.610
All the other matches,
we know who won.

00:34:56.570 --> 00:35:03.810
So the second guy only cost me
log R to produce the next guy.

00:35:03.810 --> 00:35:08.910
And the next guy is log R, and
so once we get going, to do an

00:35:08.910 --> 00:35:16.620
R-way merge only costs me
log R work per element.

00:35:16.620 --> 00:35:20.930
So the total work in merging is
R, to get started, plus n

00:35:20.930 --> 00:35:26.530
log R. Well, R is less than n,
so that's just n log R total

00:35:26.530 --> 00:35:27.880
to do the merging.

00:35:27.880 --> 00:35:29.130
That's the work.

00:35:32.360 --> 00:35:38.340
Now, let's take a look at what
happens if I now do merge sort

00:35:38.340 --> 00:35:39.590
with R-way merges.

00:35:42.630 --> 00:35:51.660
So what I do is if I have only
one element, then it's going

00:35:51.660 --> 00:35:54.830
to cost me order one time to
merge it, because there's

00:35:54.830 --> 00:35:57.580
nothing to do, just put
it in the output.

00:35:57.580 --> 00:36:04.160
Otherwise, I've got R problems
of size n over R that I'm

00:36:04.160 --> 00:36:07.050
going to merge, and my
merge takes n log R

00:36:07.050 --> 00:36:12.270
time to do the merge.

00:36:12.270 --> 00:36:15.310
So if I look at the recursion
tree, I have n log R here

00:36:15.310 --> 00:36:20.900
starting here, then I branch R
ways, and then I have n over R

00:36:20.900 --> 00:36:26.510
log R to do the R-way branching
at the next level, n

00:36:26.510 --> 00:36:29.200
over R squared log R at the
next level, et cetera.

00:36:35.183 --> 00:36:38.500
AUDIENCE: You said that the
cost of processing is R.

00:36:38.500 --> 00:36:39.560
CHARLES LEISERSON:
--is n log R.

00:36:39.560 --> 00:36:43.040
AUDIENCE: But is--

00:36:43.040 --> 00:36:46.940
CHARLES LEISERSON: Upfront,
there's an order R cost, but

00:36:46.940 --> 00:36:51.520
the order R cost is dominated
by the n log R, so we don't

00:36:51.520 --> 00:36:54.000
have to count that separately.

00:36:54.000 --> 00:36:55.360
We just have to worry
about this guy.

00:36:59.380 --> 00:37:05.240
So as I go through here, I
basically end up having a tree

00:37:05.240 --> 00:37:09.780
which is only log base R of n
tall, because I'm dividing

00:37:09.780 --> 00:37:13.440
things into R pieces
each time, rather

00:37:13.440 --> 00:37:15.330
than into two pieces.

00:37:15.330 --> 00:37:20.030
So I only go log base R steps
till I get to the base case.

00:37:20.030 --> 00:37:22.610
But I'm doing an R-way merge,
so the number of

00:37:22.610 --> 00:37:23.860
leaves is still n.

00:37:26.390 --> 00:37:31.110
But now when I add across here,
I get n times log R, and

00:37:31.110 --> 00:37:33.940
I go across here, I get n times
log R, because I got R

00:37:33.940 --> 00:37:35.490
copies of the same thing.

00:37:35.490 --> 00:37:38.480
Now I've got R squared times
n over R squared

00:37:38.480 --> 00:37:40.150
log R, and so forth.

00:37:40.150 --> 00:37:44.740
And so at every level, I have
n log R. So I have n log R

00:37:44.740 --> 00:37:48.550
times the number of levels here,
which is log base R of

00:37:48.550 --> 00:37:53.000
n, plus the order n work at
the bottom, which we can

00:37:53.000 --> 00:37:55.060
ignore because it's going
to be dominated.

00:37:55.060 --> 00:37:58.560
And so what you notice here is,
what's log base R of n?

00:37:58.560 --> 00:38:03.490
That's just log n over log R.
So the log Rs cancel, and I

00:38:03.490 --> 00:38:08.570
get n log n plus n, which
is just n log n.

00:38:08.570 --> 00:38:11.980
So after all that work, we still
do the same amount of

00:38:11.980 --> 00:38:15.760
work, whether I do binary
merging or R-way merging, the

00:38:15.760 --> 00:38:18.880
work is the same.

00:38:18.880 --> 00:38:21.600
But there's kind of
a big difference

00:38:21.600 --> 00:38:27.260
when it comes to caching.

00:38:27.260 --> 00:38:30.670
So it's the same work as
binary merge sort.

00:38:30.670 --> 00:38:33.980
So let's take a look
at the caching.

00:38:33.980 --> 00:38:40.390
So let's assume that my
tournament fits in the cache.

00:38:40.390 --> 00:38:46.670
So I want to make sure that R is
less than m over B for some

00:38:46.670 --> 00:38:50.360
constant R. So when I do
constant way, when I consider

00:38:50.360 --> 00:38:53.320
the R-way merging of contiguous
arrays of total

00:38:53.320 --> 00:38:58.150
size n, the entire tournament
plus 1 block from each array

00:38:58.150 --> 00:39:00.090
can fit in cache.

00:39:00.090 --> 00:39:04.660
So the tournament is never going
to be responsible for

00:39:04.660 --> 00:39:08.440
generating cache misses, because
I'm going to leave the

00:39:08.440 --> 00:39:10.790
tournament in memory.

00:39:10.790 --> 00:39:15.470
So if I'm the optimal algorithm,
it's going to say,

00:39:15.470 --> 00:39:20.290
let's just leave the tournament
in memory and bring

00:39:20.290 --> 00:39:23.200
in all the other things as
we do the operation.

00:39:23.200 --> 00:39:23.990
Question?

00:39:23.990 --> 00:39:26.290
AUDIENCE: [INAUDIBLE]

00:39:26.290 --> 00:39:28.060
CHARLES LEISERSON: Those circles
that I had, the tree.

00:39:28.060 --> 00:39:31.154
AUDIENCE: Is that a cumulative
list of the elements that

00:39:31.154 --> 00:39:32.820
you've merged in already?

00:39:36.460 --> 00:39:36.880
CHARLES LEISERSON: I'm sorry.

00:39:36.880 --> 00:39:38.825
Is the--

00:39:38.825 --> 00:39:44.363
AUDIENCE: Is it a cumulative
list of the length arrays that

00:39:44.363 --> 00:39:45.368
you've merged already?

00:39:45.368 --> 00:39:45.770
CHARLES LEISERSON: No, no, no.

00:39:45.770 --> 00:39:46.630
You haven't merged them.

00:39:46.630 --> 00:39:47.790
Let's just go back
and make sure we

00:39:47.790 --> 00:39:49.040
understand the algorithm.

00:39:49.040 --> 00:39:55.000
The algorithm says that what we
do is we compare the head

00:39:55.000 --> 00:40:01.240
of this pair and we produce a
single value here, for which

00:40:01.240 --> 00:40:02.560
whatever is-- because
these are already

00:40:02.560 --> 00:40:05.890
sorted to do the merge.

00:40:05.890 --> 00:40:06.780
These are already sorted.

00:40:06.780 --> 00:40:09.460
So I just have the minimum of
these two here, and the

00:40:09.460 --> 00:40:12.020
minimum of these two here,
and the minimum of all

00:40:12.020 --> 00:40:13.600
four of them here.

00:40:13.600 --> 00:40:15.510
So it's repeated.

00:40:15.510 --> 00:40:19.500
When we get to the top, we have
now the minimum of all of

00:40:19.500 --> 00:40:22.770
these guys, and that's the guy
that's the minimum overall, we

00:40:22.770 --> 00:40:24.400
put him in the output array.

00:40:24.400 --> 00:40:29.560
And now we walk back down the
path that he came from, and

00:40:29.560 --> 00:40:34.150
what we do is we say, oh, this
guy had a-- let's walk down

00:40:34.150 --> 00:40:37.680
the path, let's say we
get to this guy.

00:40:37.680 --> 00:40:39.350
Let's advance the pointer
in here and

00:40:39.350 --> 00:40:42.180
bring out another element.

00:40:42.180 --> 00:40:45.180
And now we play off the
tournament here, play off the

00:40:45.180 --> 00:40:48.360
guy here, and he advances, and
he advances, whatever.

00:40:48.360 --> 00:40:50.585
And now some other path
may be the minimum.

00:40:50.585 --> 00:40:52.370
But it only took
me log n work.

00:40:52.370 --> 00:40:56.740
I'm only keeping copies of the
element, if you will, or the

00:40:56.740 --> 00:41:01.800
results of the comparisons along
this in the tree here.

00:41:04.370 --> 00:41:07.440
And that tree, we're saying,
fits in the cache, plus 1

00:41:07.440 --> 00:41:09.240
block, the first block.

00:41:09.240 --> 00:41:12.660
Whatever cache block fits in
each of these arrays, no

00:41:12.660 --> 00:41:15.590
matter how much we've gone
down, one of those

00:41:15.590 --> 00:41:18.260
is fitting in memory.

00:41:18.260 --> 00:41:23.450
So then what happens here is the
entire tournament plus one

00:41:23.450 --> 00:41:28.450
block from each memory can fit
in cache, and so therefore the

00:41:28.450 --> 00:41:31.950
number of cache misses that I'm
going to have when I do

00:41:31.950 --> 00:41:35.780
the merge it's just essentially
the time to take

00:41:35.780 --> 00:41:40.630
faults on that one cache block
whenever I exceed it in each

00:41:40.630 --> 00:41:45.120
array, plus the one for the
output that's similar.

00:41:45.120 --> 00:41:48.060
And so the total number of cache
misses that I'm going to

00:41:48.060 --> 00:41:52.810
have is going to be n over B,
because I'm just striding

00:41:52.810 --> 00:41:57.880
straight through memory, and
that tournament, I don't have

00:41:57.880 --> 00:42:00.170
to worry about, because
it's sitting in cache.

00:42:00.170 --> 00:42:02.650
And there's enough sitting in
cache that all the other

00:42:02.650 --> 00:42:05.350
stuff, I can just keep one block
from each of them in

00:42:05.350 --> 00:42:08.910
memory and still expect
to get it.

00:42:08.910 --> 00:42:14.130
In fact, you need the tall cache
assumption to assume

00:42:14.130 --> 00:42:16.480
that they all fit in memory.

00:42:16.480 --> 00:42:20.880
So therefore, the R-way merge
sort is then, if it's

00:42:20.880 --> 00:42:23.340
sufficiently small, once again,
we have the case that

00:42:23.340 --> 00:42:25.830
it fits in memory so I only have
the cold misses to get

00:42:25.830 --> 00:42:29.120
there, n over B, if
n is less than cm.

00:42:29.120 --> 00:42:32.730
And otherwise, it's R copies of
the number of cache misses

00:42:32.730 --> 00:42:42.220
for n over R, plus n over B.
Because this is what it took

00:42:42.220 --> 00:42:44.050
us here to do the merge.

00:42:44.050 --> 00:42:48.400
We get only n over B faults when
we merge, as long as the

00:42:48.400 --> 00:42:50.390
tournament fits in cache.

00:42:50.390 --> 00:42:52.410
If the tournament doesn't fit
in cache, it's a more

00:42:52.410 --> 00:42:54.626
complicated analysis.

00:42:54.626 --> 00:42:57.972
AUDIENCE: --n over B,
that's cold misses.

00:42:57.972 --> 00:43:00.840
You're getting the stuff--

00:43:00.840 --> 00:43:03.070
CHARLES LEISERSON: Yeah,
basically, it's the cold

00:43:03.070 --> 00:43:05.910
misses on the data,
yes, basically.

00:43:11.200 --> 00:43:12.030
Good.

00:43:12.030 --> 00:43:16.230
So now, let's do the recursion
tree for this.

00:43:16.230 --> 00:43:19.090
So we basically have n over B
that we're going to pay at

00:43:19.090 --> 00:43:24.780
every level, dividing by R, et
cetera, down to the point

00:43:24.780 --> 00:43:26.215
where things fit in cache.

00:43:30.290 --> 00:43:32.960
And by the time it fits in
cache, it's going to be m over

00:43:32.960 --> 00:43:35.890
B, because n will be
approximately m, just as we

00:43:35.890 --> 00:43:38.630
had before when we were
doing the binary case.

00:43:38.630 --> 00:43:41.800
As soon as the subarray
completely fits in memory, I

00:43:41.800 --> 00:43:43.750
don't have to when I'm
doing the sorting.

00:43:43.750 --> 00:43:46.390
So this is now analyzing not the
merging, this is analyzing

00:43:46.390 --> 00:43:49.360
the sorting now.

00:43:49.360 --> 00:43:50.920
This is the sorting,
not the merging.

00:43:54.330 --> 00:43:58.930
So we get down to m over B, and
I've gone now log base R,

00:43:58.930 --> 00:44:01.180
not log base 2 as we
did before, but log

00:44:01.180 --> 00:44:05.010
base R n over cm.

00:44:05.010 --> 00:44:08.360
The number of leaves is n over
cm, and so when I multiply

00:44:08.360 --> 00:44:11.330
this out, I get the same n over
B here, and I've got n

00:44:11.330 --> 00:44:12.730
over B at every level here.

00:44:15.420 --> 00:44:17.060
So where's the win?

00:44:17.060 --> 00:44:21.440
The win is that I have only
log base R of n over cm,

00:44:21.440 --> 00:44:26.040
rather than log base 2 levels
in the tree, because the

00:44:26.040 --> 00:44:29.640
amount that every level
cost me was the same,

00:44:29.640 --> 00:44:30.890
asymptotically.

00:44:33.710 --> 00:44:37.520
So when I add it up, I get n
over B log base R of n over m,

00:44:37.520 --> 00:44:40.600
instead of n over B log
base 2 of n over m.

00:44:40.600 --> 00:44:42.530
So how do we tune R?

00:44:45.100 --> 00:44:49.460
Well if we just look at this
formula here, if I want to

00:44:49.460 --> 00:44:51.580
tune R, what should I
do to R to make this

00:44:51.580 --> 00:44:52.830
be as small as possible?

00:44:55.100 --> 00:44:57.300
Make it as big as possible.

00:44:57.300 --> 00:45:01.360
But I had to assume that R was
less than some constant times

00:45:01.360 --> 00:45:04.890
m so that it fits in cache.

00:45:04.890 --> 00:45:12.850
So that's, in fact, what I do,
is I say R is m over B. So it

00:45:12.850 --> 00:45:17.890
fits in cache, we have at least
one block for each thing

00:45:17.890 --> 00:45:19.020
that we're merging.

00:45:19.020 --> 00:45:22.150
And then when we do the analysis
now, I take log base

00:45:22.150 --> 00:45:27.210
m over B here, and that's
compared to the binary one,

00:45:27.210 --> 00:45:31.910
which was log base 2, which is
a factor of-- because this is

00:45:31.910 --> 00:45:34.110
just log 2 over log base--

00:45:34.110 --> 00:45:38.050
of log of m over B savings
in cache misses.

00:45:38.050 --> 00:45:39.700
Now, is that a significant
number?

00:45:39.700 --> 00:45:41.110
Let's take a look.

00:45:41.110 --> 00:45:57.830
So if your l one cache is 32
kilobytes, and we have cache

00:45:57.830 --> 00:46:01.410
lines of 64 bytes, that is
basically the difference in

00:46:01.410 --> 00:46:04.420
the exponents, 9x savings.

00:46:04.420 --> 00:46:07.230
For l two cache, we get
about a 12x savings.

00:46:07.230 --> 00:46:09.430
For l three, we get about
a 17x savings.

00:46:09.430 --> 00:46:11.910
Now of course, there are some
other constants going on in

00:46:11.910 --> 00:46:14.360
here, so you can't be absolutely
sure that it's

00:46:14.360 --> 00:46:17.320
exactly these numbers, but it's
going to be proportional

00:46:17.320 --> 00:46:20.200
to these numbers.

00:46:20.200 --> 00:46:24.750
So that's pretty good savings
to do multi-way merging.

00:46:24.750 --> 00:46:28.030
So generally when you merge,
don't merge pairs.

00:46:28.030 --> 00:46:30.270
Not a very good way of doing
it if you want to take good

00:46:30.270 --> 00:46:33.210
advantage of cache.

00:46:33.210 --> 00:46:36.680
May give you some ideas for how
to improve some sorts that

00:46:36.680 --> 00:46:37.930
you might have looked at.

00:46:40.890 --> 00:46:43.830
Now it turns out that there's
a cache oblivious sorting

00:46:43.830 --> 00:46:47.360
algorithm, where you don't
actually have--

00:46:47.360 --> 00:46:50.360
that was a cache aware algorithm
that knew the size

00:46:50.360 --> 00:46:52.635
of the cache, and we tune
R to get there.

00:46:52.635 --> 00:46:57.630
There is an algorithm called
funnelsort, which is based on

00:46:57.630 --> 00:47:00.510
recursively sorting and
n to the 1/3 groups of

00:47:00.510 --> 00:47:03.200
n to the 2/3 items.

00:47:03.200 --> 00:47:06.930
And then you merge the sorted
groups with a merging process

00:47:06.930 --> 00:47:08.780
called an n to the 1/3 funnel.

00:47:11.540 --> 00:47:15.230
So this is more for fun,
although the sorting

00:47:15.230 --> 00:47:20.710
algorithm, in my experience,
from what others have told me

00:47:20.710 --> 00:47:24.110
about implementing it and so
forth, is probably about 30%

00:47:24.110 --> 00:47:27.750
slower than the best hand-tuned
algorithm.

00:47:27.750 --> 00:47:30.020
Whereas with matrix
multiplication, the cache

00:47:30.020 --> 00:47:33.450
oblivious algorithms are as
good as any cache aware

00:47:33.450 --> 00:47:37.350
algorithm as a practical matter,
here, they're off by

00:47:37.350 --> 00:47:39.660
about 20% or 30%.

00:47:39.660 --> 00:47:43.580
So interesting research topic
is build one of these things

00:47:43.580 --> 00:47:45.980
and make it really efficient
so that it can compete with

00:47:45.980 --> 00:47:48.030
real sorts.

00:47:48.030 --> 00:47:51.890
So the k funnel merges k cubed
items in k sorted lists,

00:47:51.890 --> 00:47:54.070
incurring this many
cache funnels.

00:47:54.070 --> 00:47:58.090
Here, I did put in the one, for
people who are concerned

00:47:58.090 --> 00:48:00.550
about the ones.

00:48:00.550 --> 00:48:04.400
And so then, you get this
recurrence for the cache

00:48:04.400 --> 00:48:08.330
misses, because you solve n to
the 1/3 problems of size n to

00:48:08.330 --> 00:48:13.900
the 2/3 recursively, plus
this amount for merging.

00:48:13.900 --> 00:48:16.550
And that ends up giving you this
bound, which turns out to

00:48:16.550 --> 00:48:17.800
be asymptotically optimal.

00:48:20.740 --> 00:48:25.050
And the way it works is there's
basically, a k funnel

00:48:25.050 --> 00:48:27.520
is constructed recursively.

00:48:27.520 --> 00:48:32.120
And the idea is that what we
have is, we have recursive k

00:48:32.120 --> 00:48:36.570
funnels, so this is going to be
a merging process, that is

00:48:36.570 --> 00:48:49.400
going to produce k cubed items
by having k to the 3/2 buffers

00:48:49.400 --> 00:48:53.380
that each are taking the square
root of k guys and

00:48:53.380 --> 00:48:56.050
producing k guys out.

00:48:56.050 --> 00:48:58.380
So each of these guys is going
to produce k, and the square

00:48:58.380 --> 00:49:04.000
root of k of them for a total
of k to the 3/2, but each of

00:49:04.000 --> 00:49:06.290
these is going to be length
square root of k, so we end up

00:49:06.290 --> 00:49:09.260
with k cubed.

00:49:09.260 --> 00:49:12.870
and And they basically feed each
other, and then they get

00:49:12.870 --> 00:49:17.330
merged with their own k thing,
and each of these then

00:49:17.330 --> 00:49:19.290
recursively is constructed
the same way.

00:49:21.980 --> 00:49:24.930
And the basic idea is that you
keep filling the buffers, I

00:49:24.930 --> 00:49:28.315
think I say this here, so that
all these buffers end up being

00:49:28.315 --> 00:49:30.050
in contiguous storage.

00:49:30.050 --> 00:49:33.710
And the idea is, rather than
going and just getting one

00:49:33.710 --> 00:49:37.660
element out as you do in a
typical tournament, as long as

00:49:37.660 --> 00:49:41.860
you're going to go merge, let's
merge a lot of stuff and

00:49:41.860 --> 00:49:43.390
put it into our buffer
so we don't have to

00:49:43.390 --> 00:49:45.350
go back here again.

00:49:45.350 --> 00:49:48.880
So you sort of batch your
merging in local regions, and

00:49:48.880 --> 00:49:51.250
that ends up using the
cache efficiently

00:49:51.250 --> 00:49:52.500
in the local regions.

00:49:54.960 --> 00:49:56.520
Enough of sorting.

00:49:56.520 --> 00:50:01.280
Let's go on to physics.

00:50:01.280 --> 00:50:04.260
So many of you are probably
studying in your linear

00:50:04.260 --> 00:50:07.630
algebra class or elsewhere,
the heat equation.

00:50:07.630 --> 00:50:11.130
So, people familiar with
heat diffusion?

00:50:11.130 --> 00:50:14.680
So it's a common one to
do, and these were--

00:50:14.680 --> 00:50:18.110
I have a former student,
Matteo Frigo, who is a

00:50:18.110 --> 00:50:24.750
brilliant coder on anything
cache oblivious.

00:50:24.750 --> 00:50:26.860
He's got the best
code out there.

00:50:30.680 --> 00:50:35.660
So the 2D heat equation, what we
do is let's let u(t, x, y)

00:50:35.660 --> 00:50:39.730
be the temperature at time
t at point (x, y).

00:50:39.730 --> 00:50:41.910
And now you can go through the
physics and come up with an

00:50:41.910 --> 00:50:46.210
equation that looks like this,
which says that basically the

00:50:46.210 --> 00:50:51.010
partial of u with respect to t
is proportional to the sum of

00:50:51.010 --> 00:50:55.240
the second partials with
respect to x and

00:50:55.240 --> 00:50:58.510
with respect to y.

00:50:58.510 --> 00:51:01.380
So basically what that says is,
the hotter the difference

00:51:01.380 --> 00:51:04.210
between two things, the quicker
things are going to

00:51:04.210 --> 00:51:07.890
adjust, the quicker
the temperature

00:51:07.890 --> 00:51:09.170
moves between them.

00:51:09.170 --> 00:51:14.220
And alpha is the thermal
diffusivity, which has--

00:51:14.220 --> 00:51:18.220
different materials have
different thermal

00:51:18.220 --> 00:51:20.840
diffusivities.

00:51:20.840 --> 00:51:24.660
Say that three times fast.

00:51:24.660 --> 00:51:31.520
So if we do a simulation, we
can end up with heats, say,

00:51:31.520 --> 00:51:35.840
put like this, and after
it looks like this.

00:51:35.840 --> 00:51:37.170
See if we can get this
running here.

00:51:37.170 --> 00:51:38.420
So now, let me see.

00:51:45.500 --> 00:51:51.940
So I can move my cursor around
and make things.

00:51:51.940 --> 00:51:54.080
You can just sort of see
that it simulates.

00:51:54.080 --> 00:51:57.280
You can see the simulation
is actually pretty slow.

00:51:57.280 --> 00:52:01.940
Now, on my slide, I have a
thing here that says--

00:52:01.940 --> 00:52:04.920
let's see if this breaks
when we do it again.

00:52:04.920 --> 00:52:06.170
There we go.

00:52:08.960 --> 00:52:13.220
So we're getting around 100
frames per minute in doing

00:52:13.220 --> 00:52:14.470
this simulation.

00:52:17.550 --> 00:52:21.120
And so how does this
simulation work?

00:52:21.120 --> 00:52:22.380
So let's take a look at that.

00:52:22.380 --> 00:52:27.040
It's kind of a neat problem.

00:52:27.040 --> 00:52:30.310
So this is what happened
when I did 6.172

00:52:30.310 --> 00:52:31.050
for a little while.

00:52:31.050 --> 00:52:34.510
It basically gave me that after
a while, because it just

00:52:34.510 --> 00:52:37.450
sort of averages things,
smears it out.

00:52:37.450 --> 00:52:38.290
So what's going on?

00:52:38.290 --> 00:52:40.700
Let's look at it in one
dimension, because it's easier

00:52:40.700 --> 00:52:45.410
to understand than if we
take on two dimensions.

00:52:45.410 --> 00:52:48.940
So assuming that we have,
say, a bar which has no

00:52:48.940 --> 00:52:50.190
differential in this direction,

00:52:50.190 --> 00:52:52.250
only in this direction.

00:52:52.250 --> 00:52:58.420
So then we get to drop the
partials with respect to y.

00:52:58.420 --> 00:53:01.270
So if I take a look at that,
what I can do is what's called

00:53:01.270 --> 00:53:02.110
a finite difference

00:53:02.110 --> 00:53:03.870
approximation, which you probably--

00:53:03.870 --> 00:53:06.600
who's studied finite
differences?

00:53:06.600 --> 00:53:07.760
So a few people.

00:53:07.760 --> 00:53:10.080
It's OK if you haven't.

00:53:10.080 --> 00:53:12.420
That's OK if you haven't, I'll
teach it to you now.

00:53:12.420 --> 00:53:15.530
And then you're free to forget
it, because that's not the

00:53:15.530 --> 00:53:17.520
part that I want you to
understand, but it is

00:53:17.520 --> 00:53:18.780
interesting.

00:53:18.780 --> 00:53:22.660
So what I can do is look at the
partial, for example, with

00:53:22.660 --> 00:53:26.890
respect to t, and just do an
approximation the says, well,

00:53:26.890 --> 00:53:29.120
let me perturb t
a little bit--

00:53:29.120 --> 00:53:30.580
that's what it means.

00:53:30.580 --> 00:53:38.040
So I add delta t minus u of t
divided by t plus delta t

00:53:38.040 --> 00:53:41.300
minus t, which gives
me delta t.

00:53:41.300 --> 00:53:42.270
And I can use that as an

00:53:42.270 --> 00:53:44.350
approximation for this partial.

00:53:47.410 --> 00:53:50.090
Then on the right hand side--
well first of all, let me get

00:53:50.090 --> 00:53:52.930
the first derivative
with respect to x.

00:53:52.930 --> 00:53:55.470
And basically here what I'll do
is I'll do an approximation

00:53:55.470 --> 00:54:00.740
where I take x plus delta x over
2 minus x minus delta x

00:54:00.740 --> 00:54:04.780
over 2, and once again, the
differences in the terms there

00:54:04.780 --> 00:54:07.100
ends up being delta x.

00:54:07.100 --> 00:54:11.470
And now I use that to
take the next one.

00:54:11.470 --> 00:54:14.540
So basically, to take this
one, I basically take the

00:54:14.540 --> 00:54:19.600
partial with respect to delta
x over 2, minus the partial

00:54:19.600 --> 00:54:24.440
with x minus delta x over 2, and
take the partial of that,

00:54:24.440 --> 00:54:25.850
do the approximation.

00:54:25.850 --> 00:54:29.830
And what happens is, if you
look at it, when I take a

00:54:29.830 --> 00:54:34.920
partial here I'm adding delta
x over 2 twice, so I end up

00:54:34.920 --> 00:54:38.640
getting just a delta x here,
and then the two things on

00:54:38.640 --> 00:54:41.830
either side combined give me my
original one, 2 times u(t,

00:54:41.830 --> 00:54:45.720
x), and then another one here,
and now the whole thing over

00:54:45.720 --> 00:54:48.900
delta x squared.

00:54:48.900 --> 00:54:52.500
And so what I can do is to
reduce this heat equation,

00:54:52.500 --> 00:54:55.670
which is continuous, to
something that we can handle

00:54:55.670 --> 00:54:59.740
in a computer, which is
discrete, by saying OK, let's

00:54:59.740 --> 00:55:03.090
just do this approximation that
says that this term must

00:55:03.090 --> 00:55:06.350
be equal to that term.

00:55:06.350 --> 00:55:08.326
And if you've studied the linear
algebra that said that

00:55:08.326 --> 00:55:10.920
there are all kinds of
conditions on convergence, and

00:55:10.920 --> 00:55:13.690
stability, and stuff like that,
that are actually quite

00:55:13.690 --> 00:55:15.522
interesting from a numerical
point of view, but we're not

00:55:15.522 --> 00:55:17.590
going to get into it.

00:55:17.590 --> 00:55:20.550
But basically, I've just taken
that equation here and said,

00:55:20.550 --> 00:55:23.950
OK, that's my approximation
for this one.

00:55:23.950 --> 00:55:25.210
And now what do I have here?

00:55:25.210 --> 00:55:31.640
I've got u of t plus delta t,
and u things u of t, and then

00:55:31.640 --> 00:55:36.970
over here, they're all with t,
but now the deltas are in--

00:55:36.970 --> 00:55:39.880
whoops, that should have
been a delta x there.

00:55:39.880 --> 00:55:40.870
I don't know how
that got there.

00:55:40.870 --> 00:55:43.370
That should be a
delta x there.

00:55:43.370 --> 00:55:45.340
They're all in spatial
over here.

00:55:48.520 --> 00:55:56.820
So what I can do is take this,
and do an iterative process to

00:55:56.820 --> 00:55:58.730
compute this.

00:55:58.730 --> 00:56:04.170
And so the idea is, let me take
this and throw this term

00:56:04.170 --> 00:56:08.590
onto the right hand side, and
look at u of t plus delta t as

00:56:08.590 --> 00:56:09.770
if it's t plus 1.

00:56:09.770 --> 00:56:13.880
Let me make my delta be
one, essentially.

00:56:13.880 --> 00:56:19.730
Throw the delta t over here
times the alpha over delta x1,

00:56:19.730 --> 00:56:24.540
and then I get basically u of
tx is based on u of t of x

00:56:24.540 --> 00:56:27.640
plus 1 and of x and x minus 1.

00:56:27.640 --> 00:56:28.840
As I say, there's a typo here.

00:56:28.840 --> 00:56:33.080
That should be a delta t.

00:56:33.080 --> 00:56:36.370
So what that says is that if I
look at my one-dimensional

00:56:36.370 --> 00:56:40.410
process proceeding through
time, what I'm doing is

00:56:40.410 --> 00:56:44.320
updating every point here based
on the three points

00:56:44.320 --> 00:56:48.840
below it, diagonally
to the right, and

00:56:48.840 --> 00:56:51.640
diagonally to the left.

00:56:51.640 --> 00:56:54.640
So this guy can be updated
because of those.

00:56:54.640 --> 00:56:58.390
These we're not going update,
because they're the boundary.

00:56:58.390 --> 00:56:59.810
So these can be fixed.

00:56:59.810 --> 00:57:02.455
In a periodic stencil,
they may even wrap

00:57:02.455 --> 00:57:05.660
around like a torus.

00:57:05.660 --> 00:57:08.570
So basically, I can go through
and update all these with

00:57:08.570 --> 00:57:11.190
whatever that hairy
equation is.

00:57:11.190 --> 00:57:13.410
And this is basically what
the code is that I

00:57:13.410 --> 00:57:14.660
showed you is doing.

00:57:17.130 --> 00:57:21.360
It just keeps updating everyone
based on three until

00:57:21.360 --> 00:57:25.160
I've gone through a bunch of
time, and that's how the

00:57:25.160 --> 00:57:26.410
system evolves.

00:57:31.330 --> 00:57:37.080
So any questions about
how I got to here?

00:57:37.080 --> 00:57:39.250
So we're going to now
look at this purely

00:57:39.250 --> 00:57:41.080
computer sciencey now.

00:57:41.080 --> 00:57:42.970
We don't have to understand
any of those equations.

00:57:42.970 --> 00:57:44.680
We just have to understand
the structure.

00:57:44.680 --> 00:57:47.980
The structure is that we're
updating t plus 1 based on

00:57:47.980 --> 00:57:52.300
stuff on three points with
some function that some

00:57:52.300 --> 00:57:58.530
physicist oracle gave
us out of the blue.

00:57:58.530 --> 00:58:03.180
And so here is a pretty simple
algorithm to do it.

00:58:03.180 --> 00:58:05.750
I basically have what's called
the kernel, which does this

00:58:05.750 --> 00:58:11.200
updating, basically updating
each one based on things.

00:58:11.200 --> 00:58:13.430
And what I'm going to do for
computer science is I don't

00:58:13.430 --> 00:58:17.400
need to keep all the
intermediate values.

00:58:17.400 --> 00:58:18.660
And so what I'm going
to do is do what's

00:58:18.660 --> 00:58:21.300
called an even-odd trick.

00:58:21.300 --> 00:58:24.870
Basically if I have one row,
I compute the next row into

00:58:24.870 --> 00:58:29.160
another array, and then I'll
reuse that first array-- it's

00:58:29.160 --> 00:58:30.770
all been used up--

00:58:30.770 --> 00:58:33.020
and go back to the first one.

00:58:33.020 --> 00:58:36.410
So basically here, I'm just
going to update t plus 1 mod

00:58:36.410 --> 00:58:43.550
2, and just allocate two arrays
of size n, and just do

00:58:43.550 --> 00:58:46.596
modding all the way up.

00:58:46.596 --> 00:58:48.760
Is that clear?

00:58:48.760 --> 00:58:50.880
And other than that, it's
basically doing the same

00:58:50.880 --> 00:58:53.120
thing, and I'm doing a little
bit of fancy arithmetic here

00:58:53.120 --> 00:58:54.910
by passing--

00:58:54.910 --> 00:58:59.450
see stuff, where I'm passing the
pointer to where I am in

00:58:59.450 --> 00:59:02.010
the array, so I only
have to update it

00:59:02.010 --> 00:59:04.900
locally within the array.

00:59:04.900 --> 00:59:06.870
So I don't have to double
indexing once I'm in the

00:59:06.870 --> 00:59:09.530
array, because I'm already
indexed into the part of the

00:59:09.530 --> 00:59:12.470
array that I'm going to use, and
then I am doing flipping.

00:59:12.470 --> 00:59:14.270
So this is just a little
bit of cleverness.

00:59:14.270 --> 00:59:17.330
You might want to study
this later.

00:59:17.330 --> 00:59:20.700
So what's happening then is I
have this double nested loop

00:59:20.700 --> 00:59:23.260
where I have a time loop on the
outside, and a space loop

00:59:23.260 --> 00:59:25.920
on the inside, and I'm basically
going through and

00:59:25.920 --> 00:59:30.270
using a stencil of this shape,
this is called a three point

00:59:30.270 --> 00:59:34.190
stencil, because you're
basically taking three points

00:59:34.190 --> 00:59:36.570
to update one point.

00:59:36.570 --> 00:59:41.950
And now if I imagine that this
dimension is bigger, n here is

00:59:41.950 --> 00:59:46.490
bigger than my cache size,
what's going to happen?

00:59:46.490 --> 00:59:48.840
I'm going to take a cache fault
here, these are all cold

00:59:48.840 --> 00:59:49.850
misses, et cetera.

00:59:49.850 --> 00:59:55.760
But when I get back to the
beginning here, if I use LRU,

00:59:55.760 --> 00:59:58.350
nothing is going to be in memory
that I happened to

00:59:58.350 --> 00:59:59.430
update over here.

00:59:59.430 --> 01:00:01.580
So I have to go and I
take a cache fault

01:00:01.580 --> 01:00:04.070
on every cache line.

01:00:04.070 --> 01:00:10.810
And so if I'm going t steps into
the future from where I

01:00:10.810 --> 01:00:15.620
started, I basically have n
times t updates, and I save a

01:00:15.620 --> 01:00:20.935
factor of B, because I get the
spatial locality because the u

01:00:20.935 --> 01:00:25.490
of t minus 1, u of t, and u of t
plus 1, are all generally on

01:00:25.490 --> 01:00:30.500
the same-- are nearby, and all
within one cache line.

01:00:30.500 --> 01:00:32.170
Question?

01:00:32.170 --> 01:00:34.020
AUDIENCE: The x's, what
are the x's for?

01:00:36.660 --> 01:00:37.055
CHARLES LEISERSON: Sorry.

01:00:37.055 --> 01:00:38.110
I should have put the
legend on here.

01:00:38.110 --> 01:00:40.660
The x's are a miss.

01:00:40.660 --> 01:00:44.030
So I do a miss when I update
these, and then these I don't

01:00:44.030 --> 01:00:45.300
miss on, because it
was brought in

01:00:45.300 --> 01:00:48.420
when I accessed that.

01:00:48.420 --> 01:00:52.110
And then I do a miss,
and I'll do it--

01:00:52.110 --> 01:00:55.660
so basically I do it, then I
shift over the stencil by one,

01:00:55.660 --> 01:00:58.440
and then I won't get a miss.

01:00:58.440 --> 01:01:01.060
So I'm just looking at the
misses on the reads, not

01:01:01.060 --> 01:01:03.000
misses on the writes.

01:01:03.000 --> 01:01:05.500
I should have made
that clear, too.

01:01:05.500 --> 01:01:07.340
But the point is, the writes
don't help you, because it's

01:01:07.340 --> 01:01:11.320
all out of memory by the
time I get up here.

01:01:11.320 --> 01:01:15.740
To the second row, if this is
longer than my cache size,

01:01:15.740 --> 01:01:17.220
none of that's there
if I'm using LRU.

01:01:21.358 --> 01:01:24.214
AUDIENCE: You have also a miss,
like you need to get two

01:01:24.214 --> 01:01:27.045
[INAUDIBLE].

01:01:27.045 --> 01:01:27.830
CHARLES LEISERSON: Yeah, but
what I'm saying is I'm only

01:01:27.830 --> 01:01:30.270
looking at the read misses.

01:01:30.270 --> 01:01:33.260
Yes, there are write misses as
well, but basically, I'm only

01:01:33.260 --> 01:01:34.060
doing the read misses.

01:01:34.060 --> 01:01:35.900
You can look at the write
misses as well.

01:01:35.900 --> 01:01:37.150
It makes the picture messier.

01:01:40.120 --> 01:01:42.670
So we've basically
have nt over b.

01:01:42.670 --> 01:01:44.950
However this, let me tell
you, is the way that

01:01:44.950 --> 01:01:46.200
everybody codes it.

01:01:48.530 --> 01:01:51.410
and And if you have a machine
where you have any bandwidth

01:01:51.410 --> 01:01:55.010
issues to memory, especially for
these large problems, this

01:01:55.010 --> 01:01:58.810
is not a very good way to
do it, as it turns out.

01:01:58.810 --> 01:02:02.030
So it turns out that what you
want to do is, as we've seen,

01:02:02.030 --> 01:02:04.960
divide and conquer is a really
good way to do it.

01:02:04.960 --> 01:02:08.970
But in this case, when we're
doing divide and conquer,

01:02:08.970 --> 01:02:13.980
we're actually not going to use
rectangles, we're going to

01:02:13.980 --> 01:02:15.230
use trapezoids.

01:02:17.760 --> 01:02:19.920
And the reason is that a
trapezoid has the nice

01:02:19.920 --> 01:02:22.010
property that--

01:02:22.010 --> 01:02:27.100
notice that if I have all these
points in memory, then

01:02:27.100 --> 01:02:29.760
notice that I can compute all
the guys that are read on the

01:02:29.760 --> 01:02:33.530
next level, and then I can
compute all the guys that are

01:02:33.530 --> 01:02:35.660
next on the next level.

01:02:35.660 --> 01:02:37.910
And so for example, if you
imagine that this part here

01:02:37.910 --> 01:02:41.380
fit within cache, I could
actually keep going.

01:02:41.380 --> 01:02:44.580
I didn't have to stop here, I
could keep going right up to a

01:02:44.580 --> 01:02:48.480
triangle if I wanted to, and
compute all the values without

01:02:48.480 --> 01:02:52.190
having any more cache misses
than those needed to bring in,

01:02:52.190 --> 01:02:53.480
essentially, one row--

01:02:53.480 --> 01:02:57.390
two rows, actually, because
I'm reusing the

01:02:57.390 --> 01:02:59.660
rows as I go up.

01:02:59.660 --> 01:03:02.490
So what we're going to do is
traverse trapezoidal regions

01:03:02.490 --> 01:03:07.720
of space-time points such that
the points are between an

01:03:07.720 --> 01:03:11.300
upper limit, T1, and a low one,
T0, and between an x0 and

01:03:11.300 --> 01:03:15.330
an x1, where now I have slopes
here that are going to be, in

01:03:15.330 --> 01:03:18.480
general, this is
plus 1 minus 1.

01:03:18.480 --> 01:03:22.680
And in fact, sometimes it will
be straight, in which case

01:03:22.680 --> 01:03:23.600
we'll call it 0.

01:03:23.600 --> 01:03:29.760
It's really the inverse of the
slope, but we'll still call it

01:03:29.760 --> 01:03:32.220
zero rather than infinity.

01:03:32.220 --> 01:03:33.380
So it's 1 over the slope.

01:03:33.380 --> 01:03:35.260
There's a name for
that, right?

01:03:35.260 --> 01:03:37.730
Is that called the
run or something?

01:03:37.730 --> 01:03:39.620
I forget, I don't remember
my calculus.

01:03:42.570 --> 01:03:44.840
So that's what we're
going to do.

01:03:44.840 --> 01:03:49.230
And we're going to leave the
upper and right borders undone

01:03:49.230 --> 01:03:51.270
and include, so it's going
to be a sort of half open

01:03:51.270 --> 01:03:54.960
trapezoid on the left and
bottom, closed on the left and

01:03:54.960 --> 01:03:58.200
bottom, and open on
the top and right.

01:03:58.200 --> 01:04:03.120
So the width is basically the
midpoint here, and the height

01:04:03.120 --> 01:04:05.220
is the height, because they're
always going to have

01:04:05.220 --> 01:04:09.420
parallel axes here.

01:04:09.420 --> 01:04:13.790
So here's how are our recursion
is going to work.

01:04:13.790 --> 01:04:18.390
If the height is 1, then we
can compute all space-time

01:04:18.390 --> 01:04:22.300
points in any way we want.

01:04:22.300 --> 01:04:25.900
I can just go through them if
I want, because they're all

01:04:25.900 --> 01:04:26.570
independent.

01:04:26.570 --> 01:04:28.930
None depends on anybody else.

01:04:28.930 --> 01:04:30.540
So that's going to
be our base case.

01:04:33.730 --> 01:04:38.100
If the width is greater than
twice the height, however,

01:04:38.100 --> 01:04:39.930
then what we're going to do
is we're going to cut the

01:04:39.930 --> 01:04:45.740
trapezoid through the middle
of the slope of minus 1.

01:04:48.380 --> 01:04:51.680
And that will produce two new
trapezoids, which we then will

01:04:51.680 --> 01:04:58.936
recursively compute all
the elements of.

01:04:58.936 --> 01:05:02.000
So I'll start out with
a trapezoid.

01:05:02.000 --> 01:05:07.000
Basically, if it ends up that
it's a long and wide one, I'm

01:05:07.000 --> 01:05:10.700
going to make what's called a
space cut, and cut it this

01:05:10.700 --> 01:05:13.410
way, and then I'm going to
recursively do this one and

01:05:13.410 --> 01:05:15.470
then this one.

01:05:15.470 --> 01:05:18.950
And notice that I can
do that because--

01:05:18.950 --> 01:05:21.790
all these guys I can do, but
then when I get to the border

01:05:21.790 --> 01:05:24.830
here, this will already have
been done by the time I'm

01:05:24.830 --> 01:05:26.080
computing these guys.

01:05:28.930 --> 01:05:32.510
So the requirement is that
I've got to do things

01:05:32.510 --> 01:05:36.360
according to that map of triples
that I showed you

01:05:36.360 --> 01:05:38.750
before, but I don't have to
do them in the same order.

01:05:38.750 --> 01:05:41.300
I don't have to do the whole
bottom row first.

01:05:41.300 --> 01:05:44.820
In this case, I can compute the
whole trapezoid here, and

01:05:44.820 --> 01:05:48.850
then I can compute this
trapezoid here, and then all

01:05:48.850 --> 01:05:50.940
the values that I'll need will
have already been computed

01:05:50.940 --> 01:05:54.870
over here, that are on the
boundary of this trapezoid.

01:05:54.870 --> 01:05:58.030
The other type of cut I'll
do is what happens when a

01:05:58.030 --> 01:06:01.680
trapezoid gets too
tall for me.

01:06:01.680 --> 01:06:04.470
So if the trapezoid is too tall,
then what we'll do is

01:06:04.470 --> 01:06:06.580
we'll slice it through the
middle, but the other way.

01:06:06.580 --> 01:06:08.380
We call that a time cut.

01:06:08.380 --> 01:06:11.570
So we won't take it all the way
through time, we'll only

01:06:11.570 --> 01:06:12.990
take it partially
through time.

01:06:16.430 --> 01:06:19.000
Now you can show, and I'm not
going to show this in detail,

01:06:19.000 --> 01:06:21.910
but you can show that if I do
this, my trapezoids are always

01:06:21.910 --> 01:06:23.620
sort of medium sized.

01:06:23.620 --> 01:06:26.520
I never get long, long
skinny ones.

01:06:26.520 --> 01:06:29.490
If I start with something that's
sort of got a good

01:06:29.490 --> 01:06:33.660
aspect ratio, I maintain a good
aspect ratio through the

01:06:33.660 --> 01:06:34.910
entire code.

01:06:38.820 --> 01:06:40.070
So here's the implementation.

01:06:43.700 --> 01:06:48.040
This is what Matteo Frigo wrote,
and I've modified it a

01:06:48.040 --> 01:06:48.900
little bit.

01:06:48.900 --> 01:06:53.020
So basically, we pass in it
the values that let us

01:06:53.020 --> 01:06:59.320
identify the trapezoid, t0, t1,
x0, and then the slope on

01:06:59.320 --> 01:07:04.870
the left side, x1 in the slope
on the right side, where the

01:07:04.870 --> 01:07:11.290
dx0 and the dx1s are all either
0, 1, or minus 1.

01:07:11.290 --> 01:07:15.980
And then what I do is I look at
what the height is that my

01:07:15.980 --> 01:07:17.760
trapezoid is going
to operate on.

01:07:17.760 --> 01:07:22.920
And if the height is 1, well,
then I just run through all

01:07:22.920 --> 01:07:27.830
the elements, and I just
compute the kernel--

01:07:27.830 --> 01:07:30.050
that program that I showed
you before, that kernel--

01:07:30.050 --> 01:07:31.690
on all the elements.

01:07:31.690 --> 01:07:33.920
Nothing really to be done there,
just go through and

01:07:33.920 --> 01:07:37.900
compute them individually
with a four loop.

01:07:37.900 --> 01:07:46.910
Otherwise, if I've got the
situation where the trapezoid

01:07:46.910 --> 01:07:51.330
is big, then I do this
comparison, which I promise

01:07:51.330 --> 01:07:53.310
you-- you can work out the
math if you wish--

01:07:53.310 --> 01:07:56.950
which I promise you tells you
whether or not it's more than

01:07:56.950 --> 01:07:59.570
twice the height, as I said
before, whether the width is

01:07:59.570 --> 01:08:01.130
more than twice the height.

01:08:01.130 --> 01:08:07.360
And if so, I compute the middle
point, and then I

01:08:07.360 --> 01:08:09.120
partition it into two
trapezoids, and I

01:08:09.120 --> 01:08:12.790
recursively call them.

01:08:12.790 --> 01:08:16.770
And otherwise, I simply cut the
time in half, and then I

01:08:16.770 --> 01:08:20.550
do the bottom half and
then the upper half.

01:08:20.550 --> 01:08:25.979
So getting all those parameters
exactly right takes

01:08:25.979 --> 01:08:30.310
a little bit of thinking,
makes my brain hurt, but

01:08:30.310 --> 01:08:33.149
Matteo is brilliant at
this kind of coding.

01:08:36.490 --> 01:08:38.540
So let's see how
well this does.

01:08:38.540 --> 01:08:41.479
So I'm not going to do a
detailed analysis that I did

01:08:41.479 --> 01:08:47.319
before, but basically what's
going on is at this level, if

01:08:47.319 --> 01:08:49.960
I'm doing divide and conquering,
I'm only doing a

01:08:49.960 --> 01:08:54.350
constant amount of work
managing this stuff.

01:08:54.350 --> 01:08:58.500
So my caches that I'm taking
in the internal part of the

01:08:58.500 --> 01:08:59.840
tree are all going
to be order one.

01:09:03.140 --> 01:09:10.729
Now each leaf is going to
represent a trapezoid, which

01:09:10.729 --> 01:09:15.990
is going to be approximately h
times w, where h and w are

01:09:15.990 --> 01:09:19.260
approximately equal, because
they're going to be shaped--

01:09:19.260 --> 01:09:22.140
This is assuming I start out
with a number of iterations to

01:09:22.140 --> 01:09:28.720
do that is at least as large as
the number of points that I

01:09:28.720 --> 01:09:29.500
have to go on.

01:09:29.500 --> 01:09:32.170
If I start out with something
that's really thin and flat,

01:09:32.170 --> 01:09:34.290
then it's not going
to be the case.

01:09:34.290 --> 01:09:36.410
But if I start out with
something that's deep enough,

01:09:36.410 --> 01:09:39.950
then I'm going to be able
to make progress in an

01:09:39.950 --> 01:09:43.620
unconventional order into
time by moving the time

01:09:43.620 --> 01:09:46.550
non-uniformly through
the space.

01:09:46.550 --> 01:09:53.340
So each leaf represents a fairly
balanced trapezoid.

01:09:53.340 --> 01:09:57.350
Each leaf basically
is going to--

01:09:57.350 --> 01:10:03.050
if you look that the direction
of the trapezoid is in time,

01:10:03.050 --> 01:10:06.960
so this represents the spatial
dimension, and if I have

01:10:06.960 --> 01:10:10.430
something of size w, I
can access it with

01:10:10.430 --> 01:10:13.720
only w over B misses.

01:10:13.720 --> 01:10:18.810
And when that fits in cache,
where w is some constant less

01:10:18.810 --> 01:10:20.830
than m, so w is order m.

01:10:20.830 --> 01:10:24.230
So each of these things that's
a leaf is only going to occur

01:10:24.230 --> 01:10:25.480
w over B misses.

01:10:30.080 --> 01:10:32.720
Now, the total space number
of points I have to go

01:10:32.720 --> 01:10:34.490
after is n times t.

01:10:34.490 --> 01:10:37.610
N is going to be the full
dimension this way, t is the

01:10:37.610 --> 01:10:39.450
height that way.

01:10:39.450 --> 01:10:42.810
And so since each leaf
has hw points, I

01:10:42.810 --> 01:10:44.570
have nt over hw leaves.

01:10:47.130 --> 01:10:49.860
And the number of internal nodes
is just the leaves minus

01:10:49.860 --> 01:10:52.640
1, so they can't contribute
substantially, because there's

01:10:52.640 --> 01:10:57.840
only order one misses I'm taking
here, whereas I've got

01:10:57.840 --> 01:11:02.720
something on the order of w
over B misses for this.

01:11:02.720 --> 01:11:05.330
So therefore, now I
can do my math.

01:11:05.330 --> 01:11:08.100
The number of cache misses
I'm going to take is--

01:11:08.100 --> 01:11:11.300
well, how many leaves
do I have?

01:11:11.300 --> 01:11:13.780
nt over hw.

01:11:13.780 --> 01:11:15.420
And what does each
one cost us?

01:11:15.420 --> 01:11:19.220
W over B.

01:11:19.220 --> 01:11:24.830
And so now, when I multiply that
out, well, hw is about m

01:11:24.830 --> 01:11:34.540
squared, and w over B is about m
over B. And so I get nt over

01:11:34.540 --> 01:11:38.200
MB as being the total
number of savings.

01:11:38.200 --> 01:11:42.270
so whereas the original
algorithm only got nt over B,

01:11:42.270 --> 01:11:49.610
we've got this factor of a
memory cache size in there

01:11:49.610 --> 01:11:53.100
showing us that we have far
fewer cache misses.

01:11:53.100 --> 01:11:55.640
So the cache misses end up not
being an issue for this.

01:11:58.360 --> 01:11:59.610
Any questions about that?

01:12:06.230 --> 01:12:11.410
So I want to show you a
simulation of this three point

01:12:11.410 --> 01:12:14.330
stencil and comparing
the two things.

01:12:14.330 --> 01:12:16.650
So this is going to be the
looping version, where the red

01:12:16.650 --> 01:12:21.170
dots are where the cache misses
are, and this is going

01:12:21.170 --> 01:12:23.590
to be the trapezoidal one.

01:12:23.590 --> 01:12:27.950
And basically, I have an n of 95
and a t of 87, and what I'm

01:12:27.950 --> 01:12:31.440
going to do is assume a fully
associative LRU cache that

01:12:31.440 --> 01:12:35.750
fits four points on a cache
line, where the cache size is

01:12:35.750 --> 01:12:39.520
32, two to the fifth as opposed
to two to the 15th,

01:12:39.520 --> 01:12:42.100
it's really little.

01:12:42.100 --> 01:12:44.910
If I get a cache hit, I'm going
to call it one cycle.

01:12:44.910 --> 01:12:48.070
If I get a cache miss, I'm going
to call it 10 cycles.

01:12:48.070 --> 01:12:49.070
We're going to race them.

01:12:49.070 --> 01:12:52.840
So on the left is the
current world

01:12:52.840 --> 01:12:54.510
champion, the looping algorithm.

01:12:54.510 --> 01:12:59.130
And on the right is the cache
oblivious trapezoid algorithm.

01:12:59.130 --> 01:13:00.380
So let's go.

01:13:07.750 --> 01:13:12.480
So you can see that it's
basically, it's made a space

01:13:12.480 --> 01:13:18.060
cut there, but it's made a time
cut across the top there.

01:13:18.060 --> 01:13:20.730
It said, this is too tall, so
let me cut it this way.

01:13:38.460 --> 01:13:40.764
And that guy's, meanwhile,
taking all those-- you can see

01:13:40.764 --> 01:13:42.140
how many cache misses
he's taking.

01:13:45.850 --> 01:13:47.100
Let's speed him up.

01:13:52.770 --> 01:13:54.460
That's one way you can do
it, is make it think.

01:14:09.050 --> 01:14:12.670
So let's see what happens if I
have a cache of size eight.

01:14:18.020 --> 01:14:19.270
So here we go.

01:14:31.320 --> 01:14:32.830
I think I'm just doing
the same thing.

01:14:32.830 --> 01:14:34.160
I know I can show
you the cuts.

01:14:34.160 --> 01:14:37.050
Can I show you the cuts?

01:14:37.050 --> 01:14:37.270
I know.

01:14:37.270 --> 01:14:39.540
I think it's because I'm not--

01:14:39.540 --> 01:14:41.350
OK, let's try it.

01:14:41.350 --> 01:14:41.600
There we go.

01:14:41.600 --> 01:14:44.700
Now I'm showing the cuts
as they go on.

01:14:44.700 --> 01:14:47.678
Let's do that again.

01:14:47.678 --> 01:14:49.516
We'll go fast and do it again.

01:14:58.230 --> 01:15:00.400
So those are the cuts that it's
making to begin with as

01:15:00.400 --> 01:15:01.760
it's doing the divide
and conquer.

01:15:06.450 --> 01:15:09.570
So I think this is the
same size cache.

01:15:14.100 --> 01:15:16.595
So now I think I'm doing
a bigger cache.

01:15:25.010 --> 01:15:26.830
I think I did a bigger cache,
but I'm not sure I gave the

01:15:26.830 --> 01:15:28.080
other guy a bigger cache.

01:15:46.790 --> 01:15:48.330
Yeah, because it doesn't matter
for the guy on the

01:15:48.330 --> 01:15:49.100
left, right?

01:15:49.100 --> 01:15:51.470
As long as the cache line is the
same length and as long as

01:15:51.470 --> 01:15:55.910
it's not big enough, he's going
to do the same thing.

01:15:55.910 --> 01:15:57.600
He didn't get to take advantage
of the fact that the

01:15:57.600 --> 01:16:00.350
cache was bigger, because it
was still smaller than the

01:16:00.350 --> 01:16:02.050
array that he's striping
out there.

01:16:05.460 --> 01:16:06.880
Anyway, we can play with
these all day.

01:16:13.910 --> 01:16:16.240
So if you make the cache lines
bigger, then of course it'll

01:16:16.240 --> 01:16:19.790
go faster, because he'll
have fewer misses.

01:16:19.790 --> 01:16:21.680
He'll get to bring it in.

01:16:21.680 --> 01:16:22.930
So let's see here.

01:16:25.330 --> 01:16:29.030
So let's now do it for real.

01:16:29.030 --> 01:16:30.820
So this is a two-dimensional
problem.

01:16:30.820 --> 01:16:34.720
You can use the same thing
to do what end up being

01:16:34.720 --> 01:16:37.710
three-dimensional trapezoids.

01:16:37.710 --> 01:16:40.130
In fact, you can generalize
this trapezoid method to

01:16:40.130 --> 01:16:41.790
multiple dimensions.

01:16:41.790 --> 01:16:43.320
So this is the looping one.

01:16:43.320 --> 01:16:45.240
So let's start that one out.

01:16:51.020 --> 01:16:53.820
So it's going about 104
frames a minute.

01:17:08.470 --> 01:17:12.590
I think by resizing it, the
calibration is off.

01:17:15.160 --> 01:17:18.000
But in any case, let's switch to
the cash oblivious version.

01:17:27.840 --> 01:17:29.090
Anybody notice something?

01:17:31.460 --> 01:17:32.710
Slower.

01:17:35.040 --> 01:17:36.290
Why is that?

01:17:41.460 --> 01:17:43.010
I gave code exactly
as I had up there.

01:17:46.120 --> 01:17:47.640
No, it's not because it's
two dimensions.

01:17:47.640 --> 01:17:48.890
AUDIENCE: [INAUDIBLE].

01:17:53.090 --> 01:17:54.090
CHARLES LEISERSON: I'm sorry?

01:17:54.090 --> 01:17:54.390
[INTERPOSING VOICES]

01:17:54.390 --> 01:17:58.505
CHARLES LEISERSON: Yeah, so now
it's the trapezoiding at

01:17:58.505 --> 01:17:59.755
only 86 frames.

01:18:02.080 --> 01:18:04.150
What do you suppose
is going on there?

01:18:04.150 --> 01:18:06.750
AUDIENCE: You have
[INAUDIBLE].

01:18:06.750 --> 01:18:08.000
CHARLES LEISERSON: Yeah.

01:18:11.120 --> 01:18:14.870
So this is a case where if you
look at the code I wrote, I

01:18:14.870 --> 01:18:24.790
went down to a t, a delta t,
of one in my recursion.

01:18:27.510 --> 01:18:28.810
I recursed all the way down.

01:18:28.810 --> 01:18:32.160
Let's see what happens if
instead of going all the way

01:18:32.160 --> 01:18:34.830
down, playing the trapezoid
game on little tiny

01:18:34.830 --> 01:18:41.070
trapezoids, suppose I go down
only to, say, when t is 10,

01:18:41.070 --> 01:18:45.280
and then do essentially
the row major ones.

01:18:45.280 --> 01:18:50.920
So I'm basically coarsening
the leaves of the thing.

01:18:50.920 --> 01:18:52.460
So to do that, I do this.

01:18:52.460 --> 01:18:53.590
So now we go--

01:18:53.590 --> 01:18:54.840
ah.

01:19:00.660 --> 01:19:02.670
So I have to coarsen in
order to overcome the

01:19:02.670 --> 01:19:04.250
procedure call overhead.

01:19:04.250 --> 01:19:06.050
It has nothing to do
with the cache.

01:19:06.050 --> 01:19:08.440
It has to the fact that the
way that you implement

01:19:08.440 --> 01:19:10.770
recursion, recursion
and function calls

01:19:10.770 --> 01:19:11.770
have a cost to them.

01:19:11.770 --> 01:19:15.600
And if what you're going to do
is do a little tiny update of

01:19:15.600 --> 01:19:18.860
those few floating point
operations--

01:19:18.860 --> 01:19:22.380
let's go back to the looping
just to see.

01:19:22.380 --> 01:19:30.540
The looping is going
about 107, 108, and

01:19:30.540 --> 01:19:37.310
trapezoiding at 136.

01:19:37.310 --> 01:19:39.520
So unfortunately, you need a
voodoo variable, but it's a

01:19:39.520 --> 01:19:42.660
voodoo variable not to overcome
the cache, but rather

01:19:42.660 --> 01:19:45.470
to deal with what's the
overhead in using the

01:19:45.470 --> 01:19:48.420
processor when you do
function calls.

01:19:48.420 --> 01:19:51.150
So let's see.

01:19:51.150 --> 01:19:52.290
How coarse can we make it?

01:19:52.290 --> 01:19:54.110
Let's try five, a coarsening
of five?

01:19:57.370 --> 01:19:58.620
That's still pretty good.

01:20:02.040 --> 01:20:02.790
That's still pretty good.

01:20:02.790 --> 01:20:04.040
How about four?

01:20:07.940 --> 01:20:11.750
Still doing 131 frames
a minute.

01:20:11.750 --> 01:20:15.080
How about three?

01:20:15.080 --> 01:20:16.620
Oh, we lost something there.

01:20:19.540 --> 01:20:20.790
How about two?

01:20:24.020 --> 01:20:30.070
So at a coarsening of two, I
go 138, whereas the looping

01:20:30.070 --> 01:20:32.890
goes at about the same.

01:20:32.890 --> 01:20:33.720
I can't do 20.

01:20:33.720 --> 01:20:34.780
I didn't program that in.

01:20:34.780 --> 01:20:37.820
I just programmed up to 10.

01:20:37.820 --> 01:20:42.330
So if I go down to one, however,
then you see it's not

01:20:42.330 --> 01:20:42.970
that efficient.

01:20:42.970 --> 01:20:46.780
But if I pick any number that's
even slightly larger,

01:20:46.780 --> 01:20:50.490
that gives me just enough that
the function call overhead

01:20:50.490 --> 01:20:51.300
ends up not being a

01:20:51.300 --> 01:20:57.600
substantial cost of the things.

01:20:57.600 --> 01:20:59.630
So let me just wrap up now.

01:20:59.630 --> 01:21:00.870
So I just have a couple
more things.

01:21:00.870 --> 01:21:05.750
So I'm not going to really talk
about these, but there

01:21:05.750 --> 01:21:09.640
are lots of cash oblivious
algorithms that have been

01:21:09.640 --> 01:21:18.250
discovered in the last 10 or 15
years for doing things like

01:21:18.250 --> 01:21:23.340
matrix transposition, which is
similar to rotating a matrix.

01:21:23.340 --> 01:21:26.720
You can do that in a cache
oblivious fashion.

01:21:26.720 --> 01:21:32.270
Strassen's algorithm, which
does matrix multiplication

01:21:32.270 --> 01:21:34.720
using fewer than n
cubed operations.

01:21:34.720 --> 01:21:40.440
The FFT can be computed in a
cache oblivious fashion.

01:21:40.440 --> 01:21:44.770
And LUD composition
is a popular

01:21:44.770 --> 01:21:48.740
thing to solve systems.

01:21:48.740 --> 01:21:52.690
In addition, there are cache
oblivious data structures, and

01:21:52.690 --> 01:21:53.760
here are just a few of them.

01:21:53.760 --> 01:21:57.060
There's cache oblivious B-Trees
and priority queues,

01:21:57.060 --> 01:22:00.820
and doing things called
ordered-file maintenance.

01:22:00.820 --> 01:22:01.850
There's a whole raft.

01:22:01.850 --> 01:22:05.740
There's probably now several
hundred papers written on

01:22:05.740 --> 01:22:09.240
cache oblivious algorithms, so
something you should be aware

01:22:09.240 --> 01:22:12.970
of and understand how it is that
you go about designing an

01:22:12.970 --> 01:22:14.040
algorithm of this nature.

01:22:14.040 --> 01:22:17.290
Not all of them are
straightforward.

01:22:17.290 --> 01:22:22.910
For example, the FFT one does
divide and conquer but not by

01:22:22.910 --> 01:22:24.010
dividing it into two.

01:22:24.010 --> 01:22:26.750
It divides it into square root
of n pieces of size square

01:22:26.750 --> 01:22:31.460
root of n each in order to get
a good cache efficient

01:22:31.460 --> 01:22:34.060
algorithm that doesn't have
any tuning parameters.

01:22:34.060 --> 01:22:38.010
But almost all of them, since
they're recursive, do have

01:22:38.010 --> 01:22:41.090
this annoying thing that you
have to still coarsen the base

01:22:41.090 --> 01:22:45.250
case in order to get really good
performance if you're not

01:22:45.250 --> 01:22:50.600
doing a lot of work in the
leaves of the recursion.

01:22:50.600 --> 01:22:51.850
So any questions?

