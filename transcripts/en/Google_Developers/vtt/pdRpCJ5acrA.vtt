WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.297
[MUSIC PLAYING]

00:00:04.920 --> 00:00:07.390
BENJAMIN SCHROM: Hello,
and good afternoon.

00:00:07.390 --> 00:00:08.460
My name is Ben Schrom.

00:00:08.460 --> 00:00:10.830
I'm a product manager
on the AR team.

00:00:10.830 --> 00:00:13.782
I'm joined here by two of my
colleagues, Leon and Christina,

00:00:13.782 --> 00:00:15.240
and we're here to
talk to you today

00:00:15.240 --> 00:00:19.230
about what we've been up
to lately with ARCore.

00:00:19.230 --> 00:00:21.990
I wanted to start out, though,
by giving some quick context.

00:00:21.990 --> 00:00:24.870
It's been a big year
for AR, but we're really

00:00:24.870 --> 00:00:26.010
just getting started.

00:00:26.010 --> 00:00:28.950
Last year at I/O, our AR
platform wasn't even a year

00:00:28.950 --> 00:00:30.660
old.

00:00:30.660 --> 00:00:32.640
But everything
we've done before,

00:00:32.640 --> 00:00:34.350
and everything we're
doing now, really

00:00:34.350 --> 00:00:36.000
goes back to a pretty
simple question.

00:00:38.600 --> 00:00:41.120
That smartphone camera
is less and less

00:00:41.120 --> 00:00:43.490
a camera in any
traditional sense, almost

00:00:43.490 --> 00:00:45.500
like your smartphone is
less and less a phone

00:00:45.500 --> 00:00:46.998
in any traditional sense.

00:00:46.998 --> 00:00:49.040
You really don't need to
take much of a step back

00:00:49.040 --> 00:00:51.410
to remember that it was
pretty recently that cameras

00:00:51.410 --> 00:00:54.860
were for taking snapshots
and not much more.

00:00:54.860 --> 00:00:58.670
But this idea of AR stemmed
from a collective realization

00:00:58.670 --> 00:01:01.640
that these cameras we all
carry around in our pockets,

00:01:01.640 --> 00:01:04.190
they happen to be attached to
these little super computers

00:01:04.190 --> 00:01:06.740
that are also
packed with sensors.

00:01:06.740 --> 00:01:09.140
And so the question AR
asks is, what could we

00:01:09.140 --> 00:01:11.930
do if we thought about
these cameras themselves

00:01:11.930 --> 00:01:15.800
as one of the richest of all
the available sensors, a sensor

00:01:15.800 --> 00:01:17.960
that we can hold up
to the world and use

00:01:17.960 --> 00:01:21.970
what we see as core input?

00:01:21.970 --> 00:01:24.420
And from that stems, also,
a really pretty simple

00:01:24.420 --> 00:01:26.940
observation, that the
richer the set of inputs

00:01:26.940 --> 00:01:29.760
you give a computing device,
the richer the set of outputs

00:01:29.760 --> 00:01:30.865
it can produce.

00:01:30.865 --> 00:01:31.990
And this isn't a new thing.

00:01:31.990 --> 00:01:34.470
This is really true for the
whole history of computing.

00:01:34.470 --> 00:01:37.610
The punch card gives way to the
keyboard, the mouse, the touch

00:01:37.610 --> 00:01:40.510
screen, voice, so on and so on.

00:01:40.510 --> 00:01:45.370
But given that AR experiences
can be within the world itself,

00:01:45.370 --> 00:01:47.970
there's really no limit to the
richness of the experiences

00:01:47.970 --> 00:01:49.350
we can ultimately produce.

00:01:52.570 --> 00:01:55.160
To get more specific,
when I say richer inputs,

00:01:55.160 --> 00:01:57.440
I mean going way beyond the
sorts of things we mostly

00:01:57.440 --> 00:01:58.857
feed into phones
right now, things

00:01:58.857 --> 00:02:04.040
like touches, swipes,
numbers, letters, emojis.

00:02:04.040 --> 00:02:07.010
These are distinctly
not the inputs

00:02:07.010 --> 00:02:09.350
that humans use to
perceive the world.

00:02:09.350 --> 00:02:14.060
Humans, we perceive that the
world has three dimensions,

00:02:14.060 --> 00:02:18.360
that it has things in it
that have shapes and mass.

00:02:18.360 --> 00:02:20.930
And we're super attuned
to things like light

00:02:20.930 --> 00:02:23.480
coming from different
directions and different sources

00:02:23.480 --> 00:02:25.700
and reflecting off
different materials.

00:02:25.700 --> 00:02:28.220
But our phones are basically
clueless about these things

00:02:28.220 --> 00:02:30.220
until quite recently.

00:02:30.220 --> 00:02:34.710
So when we combine the incoming
visual data from a camera

00:02:34.710 --> 00:02:37.680
with a bunch of other sensors
and computing resources,

00:02:37.680 --> 00:02:42.120
like IMUs and GPUs and ML
models and software algorithms,

00:02:42.120 --> 00:02:45.960
we can begin to give our phones
some of the same understanding

00:02:45.960 --> 00:02:48.050
of the world that we have.

00:02:48.050 --> 00:02:51.720
So in AR terms, this means
things like 6DoF tracking,

00:02:51.720 --> 00:02:54.300
or plane finding,
light estimation,

00:02:54.300 --> 00:02:57.142
or super precise
shared localization.

00:03:01.390 --> 00:03:05.250
And once you give a computer
these sorts of richer inputs,

00:03:05.250 --> 00:03:08.210
you can produce really
powerful new outputs.

00:03:08.210 --> 00:03:11.390
You can do what stream is doing
and overlay a how-to video

00:03:11.390 --> 00:03:13.652
about how to change the
oil in your car directly

00:03:13.652 --> 00:03:15.110
above the place
where the oil goes.

00:03:19.150 --> 00:03:21.400
You can even call
on a remote expert

00:03:21.400 --> 00:03:23.830
to help you find specific
parts of the engine

00:03:23.830 --> 00:03:26.330
right in front of you.

00:03:26.330 --> 00:03:29.690
Or you could turn a smartphone
into the world's quickest

00:03:29.690 --> 00:03:32.660
and easiest tape measure, one
that's always with you, one

00:03:32.660 --> 00:03:35.420
that doesn't always get
lost in your junk drawer

00:03:35.420 --> 00:03:38.070
just when you need it.

00:03:38.070 --> 00:03:39.990
Well, within Google
Maps, you can

00:03:39.990 --> 00:03:42.270
take a set of
walking directions,

00:03:42.270 --> 00:03:45.480
and you can place navigational
aids right where you need them,

00:03:45.480 --> 00:03:47.610
within the world itself.

00:03:47.610 --> 00:03:50.460
It's like a set of dynamic
road signs placed just for you.

00:03:53.080 --> 00:03:56.430
You can also create wholly
new types of gameplay,

00:03:56.430 --> 00:03:59.130
like "Tendar," a game
from Tender Claws, that

00:03:59.130 --> 00:04:02.280
uses facial expressions and
emotions to create a narrative

00:04:02.280 --> 00:04:04.210
around a virtual object.

00:04:04.210 --> 00:04:08.176
In this case, it's a guppy that
feeds off of human emotion.

00:04:08.176 --> 00:04:11.310
Can you imagine how crazy this
pitch would have sounded even

00:04:11.310 --> 00:04:13.380
in 2015?

00:04:13.380 --> 00:04:17.470
But it's totally possible
now, and it's really fun.

00:04:17.470 --> 00:04:19.990
Or you can create
the types of scenes

00:04:19.990 --> 00:04:23.020
previously only possible
within the CGI lab

00:04:23.020 --> 00:04:24.760
within a Hollywood
studio, where you

00:04:24.760 --> 00:04:27.058
can place digital characters
into the world such

00:04:27.058 --> 00:04:29.100
that they feel like they're
right there with you.

00:04:32.810 --> 00:04:35.480
We are working so
hard on AR at Google

00:04:35.480 --> 00:04:38.090
because we believe it unlocks
a new set of applications that

00:04:38.090 --> 00:04:41.030
are creative and helpful
in ways we could only

00:04:41.030 --> 00:04:44.070
imagine a few years ago.

00:04:44.070 --> 00:04:46.670
AR can be uniquely
helpful, because it

00:04:46.670 --> 00:04:50.060
can present useful, important
information within the most

00:04:50.060 --> 00:04:53.870
relevant context,
the world itself.

00:04:53.870 --> 00:04:56.140
And as we said
before, it gives you

00:04:56.140 --> 00:04:59.230
the ability to use what you
see as a fundamental input

00:04:59.230 --> 00:05:00.850
into your computing.

00:05:00.850 --> 00:05:02.770
And at the very
least, it saves you

00:05:02.770 --> 00:05:07.440
from having to type those 1,000
words each picture is worth.

00:05:07.440 --> 00:05:09.630
On the creative
side, what you see

00:05:09.630 --> 00:05:11.490
and what the camera
perceives can

00:05:11.490 --> 00:05:15.810
become a wholly new mechanic
for games or self-expression.

00:05:15.810 --> 00:05:19.440
And the creative digital outputs
of artists, storytellers,

00:05:19.440 --> 00:05:23.010
and game designers can now
inhabit the same spaces

00:05:23.010 --> 00:05:23.940
we do as people.

00:05:27.150 --> 00:05:29.110
Enabling exactly these
sorts of experiences

00:05:29.110 --> 00:05:32.060
is why we are building ARCore.

00:05:32.060 --> 00:05:35.600
Our goal with ARCore is to
give developers like yourselves

00:05:35.600 --> 00:05:38.540
simple and powerful
tools for bridging

00:05:38.540 --> 00:05:40.897
the digital and physical world.

00:05:40.897 --> 00:05:43.480
So today we're going to give you
a recap of the progress we've

00:05:43.480 --> 00:05:45.188
made over the last
year, and then

00:05:45.188 --> 00:05:47.230
we're going to walk through
a bunch of new things

00:05:47.230 --> 00:05:50.500
we're bringing to the platform.

00:05:50.500 --> 00:05:53.350
To start with, I'm most excited
to say that since last year,

00:05:53.350 --> 00:05:55.360
we've almost
quadrupled the number

00:05:55.360 --> 00:05:58.030
of ARCore-enabled devices,
bringing that number

00:05:58.030 --> 00:06:00.130
to an estimated 400 million.

00:06:00.130 --> 00:06:02.800
And we did this by working
really closely with top Android

00:06:02.800 --> 00:06:07.630
OEMs to ensure new devices are
ARCore-compatible at launch.

00:06:07.630 --> 00:06:10.150
That also means the
number of ARCore devices

00:06:10.150 --> 00:06:15.040
will just keep growing as these
new phones sell in the market.

00:06:15.040 --> 00:06:18.340
We've also worked with lots of
developers, like all of you,

00:06:18.340 --> 00:06:21.102
to expand the number of
AR applications available.

00:06:21.102 --> 00:06:23.560
In fact, there's now a dedicated
section of the Google Play

00:06:23.560 --> 00:06:28.810
Store that features over
3,000 ARCore applications.

00:06:28.810 --> 00:06:30.660
Here are some of our
recent favorites.

00:06:30.660 --> 00:06:33.390
Here's "Pharos," which
uses cloud anchors to let

00:06:33.390 --> 00:06:36.000
multiple players share a journey
through a universe created

00:06:36.000 --> 00:06:38.180
by Childish Gambino.

00:06:38.180 --> 00:06:40.105
Or there's the
ColorSnap visualizer.

00:06:40.105 --> 00:06:41.480
It's an app from
Sherwin-Williams

00:06:41.480 --> 00:06:43.272
that lets you see what
a new shade of paint

00:06:43.272 --> 00:06:48.370
looks like on your walls without
having to slap the paint on.

00:06:48.370 --> 00:06:50.520
Or there's GeoGebra's
3D graphing calculator

00:06:50.520 --> 00:06:53.280
that lets you create
3D math plots,

00:06:53.280 --> 00:06:57.065
place them in your space, and
explore them by walking around.

00:06:57.065 --> 00:06:58.690
The simple breadth
of things that we've

00:06:58.690 --> 00:07:00.815
seen developers create in
a relatively short period

00:07:00.815 --> 00:07:02.680
has been remarkable,
and we really

00:07:02.680 --> 00:07:04.730
hope to see even more
interesting things,

00:07:04.730 --> 00:07:07.282
some of the improvements
we'll dive into now.

00:07:07.282 --> 00:07:08.740
With that, I'd like
to turn it over

00:07:08.740 --> 00:07:11.907
to one of the lead
engineers on our team, Leon.

00:07:11.907 --> 00:07:13.374
LEON WONG: Thanks, Ben.

00:07:13.374 --> 00:07:16.310
[APPLAUSE]

00:07:16.310 --> 00:07:19.130
Thanks a lot, Ben.

00:07:19.130 --> 00:07:20.720
Well, I can't really
believe that it's

00:07:20.720 --> 00:07:24.920
been a whole year since the last
Google I/O. And over that time,

00:07:24.920 --> 00:07:27.680
we've released six
updates to ARCore,

00:07:27.680 --> 00:07:29.780
and we've made improvements
to almost every part

00:07:29.780 --> 00:07:32.330
of the platform, from
algorithmic quality

00:07:32.330 --> 00:07:34.190
to developer tools,
and we've also

00:07:34.190 --> 00:07:36.958
added some great
new capabilities.

00:07:36.958 --> 00:07:39.000
I'd love to share some of
the highlights with you

00:07:39.000 --> 00:07:41.190
today, starting with
improvements to some

00:07:41.190 --> 00:07:46.640
of the fundamentals upon which
all AR experiences are built.

00:07:46.640 --> 00:07:49.190
Continuing to improve
the quality of our motion

00:07:49.190 --> 00:07:51.740
tracking and environmental
understanding algorithms

00:07:51.740 --> 00:07:54.110
has been a top focus for us.

00:07:54.110 --> 00:07:58.670
Not only does this create more
reliable and enjoyable user

00:07:58.670 --> 00:08:02.630
experiences, but we've seen that
improvements to the algorithms

00:08:02.630 --> 00:08:06.260
that underlie ARCore
have boosted ARCore user

00:08:06.260 --> 00:08:10.470
engagement and retention across
a broad range of applications.

00:08:10.470 --> 00:08:12.560
One of our biggest
achievements in the last year

00:08:12.560 --> 00:08:17.030
was improving ARCore motion
tracking robustness by 30%,

00:08:17.030 --> 00:08:20.300
with a large part of that coming
from better sensor calibration

00:08:20.300 --> 00:08:22.670
algorithms that
have helped ARCore

00:08:22.670 --> 00:08:25.430
adapt to the
diversity of hardware

00:08:25.430 --> 00:08:27.710
in the Android ecosystem.

00:08:27.710 --> 00:08:30.920
Now, there are always going
to be some cases where

00:08:30.920 --> 00:08:32.630
tracking simply fails.

00:08:32.630 --> 00:08:34.580
People will put their
phones in their pockets.

00:08:34.580 --> 00:08:36.860
People will shake
their phones too hard.

00:08:36.860 --> 00:08:39.350
And it won't always be possible
for us to maintain tracking

00:08:39.350 --> 00:08:41.580
quality in those cases.

00:08:41.580 --> 00:08:44.390
So we feel that one of the
most important things we can do

00:08:44.390 --> 00:08:46.550
is educate users about
how tracking works

00:08:46.550 --> 00:08:49.620
and what they can do to
improve their own experiences.

00:08:49.620 --> 00:08:51.950
That's why we introduced
an API to report

00:08:51.950 --> 00:08:54.115
tracking failure reasons.

00:08:54.115 --> 00:08:56.240
For example, when there's
not enough visual texture

00:08:56.240 --> 00:09:00.230
in a scene to allow our
cameras to track motion,

00:09:00.230 --> 00:09:03.890
or there's simply too little
light in the environment,

00:09:03.890 --> 00:09:06.980
or when there's excessive motion
that saturates inertial sensors

00:09:06.980 --> 00:09:10.010
or can cause camera motion blur.

00:09:10.010 --> 00:09:12.250
By providing this kind of
feedback to applications,

00:09:12.250 --> 00:09:15.050
we hope that apps will be able
to guide users toward more

00:09:15.050 --> 00:09:17.120
successful AR experiences.

00:09:19.730 --> 00:09:21.920
Plane finding is
another experience

00:09:21.920 --> 00:09:26.150
that's been a key focus
for our engineering team.

00:09:26.150 --> 00:09:30.050
Now, plane finding, for a
very large percentage of apps,

00:09:30.050 --> 00:09:31.970
is one of those things
you have to do in order

00:09:31.970 --> 00:09:34.880
to place AR content and
begin the overall AR

00:09:34.880 --> 00:09:38.190
experience that people
are trying to get into.

00:09:38.190 --> 00:09:39.810
But what's not
really clear to users

00:09:39.810 --> 00:09:42.240
is that we need particular
kinds of camera movements

00:09:42.240 --> 00:09:45.060
in order to find
planes successfully.

00:09:45.060 --> 00:09:47.400
ARCore triangulates
where visual features

00:09:47.400 --> 00:09:50.280
are in three-dimensional
space by seeing them

00:09:50.280 --> 00:09:52.270
from multiple
different perspectives.

00:09:52.270 --> 00:09:55.365
So large, gentle motions
focusing on the target area

00:09:55.365 --> 00:09:59.060
of interest really work best.

00:09:59.060 --> 00:10:01.650
Rather than making users
learn how to do this better,

00:10:01.650 --> 00:10:05.060
however, we've focused on
reducing the amount of user

00:10:05.060 --> 00:10:07.550
motion that our
algorithms require

00:10:07.550 --> 00:10:11.060
by increasing the number
and types of visual features

00:10:11.060 --> 00:10:13.610
that we're using
to locate planes.

00:10:13.610 --> 00:10:16.730
This has improved plane-finding
speed and success rates

00:10:16.730 --> 00:10:18.060
dramatically.

00:10:18.060 --> 00:10:21.260
For example, in Google's own
AR measurement application,

00:10:21.260 --> 00:10:24.380
we saw a 50% reduction
in the amount of time

00:10:24.380 --> 00:10:27.000
it takes to find
an initial plane.

00:10:27.000 --> 00:10:29.130
To see this in
action, take a look

00:10:29.130 --> 00:10:31.660
at the graphic on the screen.

00:10:31.660 --> 00:10:33.540
It's really hard
to see, but if you

00:10:33.540 --> 00:10:36.420
look at how little camera
motion is needed before you

00:10:36.420 --> 00:10:39.828
see the dots that indicate
we've found the floor plan,

00:10:39.828 --> 00:10:41.370
you'll see just what
kind of progress

00:10:41.370 --> 00:10:44.040
we've made over the last year.

00:10:44.040 --> 00:10:45.030
Keep looking.

00:10:45.030 --> 00:10:46.020
There it is.

00:10:46.020 --> 00:10:48.030
It's almost instantaneous
in a lot of cases.

00:10:51.060 --> 00:10:53.700
Camera quality is
another fundamental part

00:10:53.700 --> 00:10:56.400
of nearly every AR experience.

00:10:56.400 --> 00:11:00.030
When ARCore launched, we
optimized camera configurations

00:11:00.030 --> 00:11:02.130
for visual tracking performance.

00:11:02.130 --> 00:11:04.050
So we did things
like we fixed focus

00:11:04.050 --> 00:11:08.340
at infinity to make it easier
to model camera focal lengths,

00:11:08.340 --> 00:11:11.640
and we tightly controlled
exposure settings, frame rates,

00:11:11.640 --> 00:11:16.650
and resolutions to prevent
motion blur and limit compute.

00:11:16.650 --> 00:11:18.540
This made our computer
vision challenges

00:11:18.540 --> 00:11:20.640
a lot more tractable,
but it really

00:11:20.640 --> 00:11:24.410
wasn't ideal for many
end user applications.

00:11:24.410 --> 00:11:26.810
For example, AR
photography has grown

00:11:26.810 --> 00:11:29.200
into a really important
use case for us,

00:11:29.200 --> 00:11:31.640
and we really wanted to
let AR take advantage

00:11:31.640 --> 00:11:33.920
of more of the
camera capabilities

00:11:33.920 --> 00:11:36.930
that users expect
from their devices.

00:11:36.930 --> 00:11:39.620
So over the last year,
we launched a number

00:11:39.620 --> 00:11:42.580
of important camera updates.

00:11:42.580 --> 00:11:46.930
We launched autofocus so that
AR photographs are sharp,

00:11:46.930 --> 00:11:49.060
even when scenes are close up.

00:11:49.060 --> 00:11:52.180
And we launched a feature
called Shared Camera Control.

00:11:52.180 --> 00:11:54.490
This is a feature that
lets applications quickly

00:11:54.490 --> 00:11:56.530
switch between
Visual Tracking mode

00:11:56.530 --> 00:11:59.983
for the camera and a mode that's
controlled by the application

00:11:59.983 --> 00:12:01.900
so that they can choose
to do things like take

00:12:01.900 --> 00:12:04.240
higher-resolution photographs.

00:12:04.240 --> 00:12:06.460
And then finally,
we doubled your fun

00:12:06.460 --> 00:12:08.530
by adding front-facing
camera support,

00:12:08.530 --> 00:12:10.600
so you can take those
all-important AR selfies.

00:12:13.600 --> 00:12:16.450
So in addition to working on
the quality and reliability

00:12:16.450 --> 00:12:19.240
of ARCore end user
experiences, we also

00:12:19.240 --> 00:12:21.250
invested in our
development tools

00:12:21.250 --> 00:12:24.310
to help application creators
work more efficiently

00:12:24.310 --> 00:12:27.920
and take advantage of
our latest features.

00:12:27.920 --> 00:12:31.040
For Java developers,
dealing with 3D graphics

00:12:31.040 --> 00:12:32.640
can be a real challenge.

00:12:32.640 --> 00:12:35.770
So we launched Sceneform
at I/O last year.

00:12:35.770 --> 00:12:38.950
Sceneform makes it easy
to create 3D scene graphs

00:12:38.950 --> 00:12:42.220
and render them realistically,
all without the complexity

00:12:42.220 --> 00:12:44.260
of OpenGL.

00:12:44.260 --> 00:12:46.870
Since our launch of
Sceneform last year,

00:12:46.870 --> 00:12:50.480
we expanded its capabilities
in a lot of different ways.

00:12:50.480 --> 00:12:55.510
So for example, we added support
for external dynamic textures

00:12:55.510 --> 00:12:57.730
to allow you to do
high-quality video

00:12:57.730 --> 00:12:59.660
playbacks in your applications.

00:12:59.660 --> 00:13:02.200
We added screen recording
to help developers

00:13:02.200 --> 00:13:05.950
capture demo videos and
let users share screenshots

00:13:05.950 --> 00:13:07.790
on social media.

00:13:07.790 --> 00:13:10.450
And we added animation
support so that your 3D assets

00:13:10.450 --> 00:13:12.560
can come to life in AR.

00:13:12.560 --> 00:13:14.680
And then finally, we've
tried to keep Sceneform

00:13:14.680 --> 00:13:17.290
up-to-date by supporting
the latest ARCore features,

00:13:17.290 --> 00:13:18.205
like Augmented Faces.

00:13:21.320 --> 00:13:24.500
Now, for developers working
in Unity instead of Java

00:13:24.500 --> 00:13:26.600
for their application
development workflow,

00:13:26.600 --> 00:13:30.170
we've been regularly updating
our ARCore SDK for Unity,

00:13:30.170 --> 00:13:33.710
so it always showcases the best
of ARCore's growing platform

00:13:33.710 --> 00:13:35.300
capabilities.

00:13:35.300 --> 00:13:37.820
But because we know that many
application developers are

00:13:37.820 --> 00:13:40.580
building cross-platform
applications,

00:13:40.580 --> 00:13:42.320
we've also worked
closely with Unity

00:13:42.320 --> 00:13:45.140
on their AR Foundation package.

00:13:45.140 --> 00:13:47.210
AR Foundation lets
developers use

00:13:47.210 --> 00:13:49.670
a core set of augmented
reality features

00:13:49.670 --> 00:13:54.900
across both ARKit on iOS,
and ARCore on Android,

00:13:54.900 --> 00:13:58.710
all using a common API so that
you can maintain a single code

00:13:58.710 --> 00:14:00.930
base for your apps.

00:14:00.930 --> 00:14:04.020
And to make those cross-platform
experiences even better,

00:14:04.020 --> 00:14:07.290
we brought key ARCore
features to iOS,

00:14:07.290 --> 00:14:13.590
like Cloud anchors, which lets
developers create multi-user AR

00:14:13.590 --> 00:14:15.450
experiences that
are all anchored

00:14:15.450 --> 00:14:16.770
in the same physical location.

00:14:19.950 --> 00:14:24.460
Now, effective user interaction
design is just as much

00:14:24.460 --> 00:14:27.850
of a challenge for AR
as software development.

00:14:27.850 --> 00:14:29.920
And designers are
still figuring out

00:14:29.920 --> 00:14:34.360
what's working best for their
applications and use cases.

00:14:34.360 --> 00:14:36.190
In order to help
with this problem,

00:14:36.190 --> 00:14:38.980
we introduced ARCore Elements.

00:14:38.980 --> 00:14:41.560
ARCore Elements is a
set of UI components

00:14:41.560 --> 00:14:45.550
that Google has designed and
validated with user testing.

00:14:45.550 --> 00:14:49.540
You can use ARCore Elements to
insert common AR interaction

00:14:49.540 --> 00:14:52.570
patterns, like plane finding
and object manipulation,

00:14:52.570 --> 00:14:55.030
into your Unity apps,
all without having

00:14:55.030 --> 00:14:57.250
to reinvent the wheel.

00:14:57.250 --> 00:14:59.590
This helps users learn
actions that they

00:14:59.590 --> 00:15:01.780
can perform across
different applications,

00:15:01.780 --> 00:15:05.410
and it also makes it easier to
follow Google's recommended AR

00:15:05.410 --> 00:15:09.280
user experience guidelines.

00:15:09.280 --> 00:15:11.980
So those were some examples
of the many updates

00:15:11.980 --> 00:15:15.140
and improvements we've made
to ARCore in the last year.

00:15:15.140 --> 00:15:17.570
Now we'd like to share some
of our newest capabilities,

00:15:17.570 --> 00:15:21.670
including several that
are launching this week.

00:15:21.670 --> 00:15:23.650
From the start, the
mission of ARCore

00:15:23.650 --> 00:15:25.600
has been to give
developers the ability

00:15:25.600 --> 00:15:28.150
to create more realistic
experiences that

00:15:28.150 --> 00:15:32.230
are available to more
users and in more places.

00:15:32.230 --> 00:15:34.240
And we wanted to give
our devices the ability

00:15:34.240 --> 00:15:37.930
to see and understand the world
in much the same way that we

00:15:37.930 --> 00:15:43.780
do, and to fully engage our
own human senses by rendering

00:15:43.780 --> 00:15:49.525
digital content in context with
the highest levels of realism.

00:15:49.525 --> 00:15:50.900
So let's start by
looking at some

00:15:50.900 --> 00:15:53.300
of the new visual
perception capabilities

00:15:53.300 --> 00:15:55.040
that we've added to
ARCore to make it

00:15:55.040 --> 00:15:58.280
more useful in more contexts.

00:15:58.280 --> 00:16:00.350
By human standards,
ARCore launched

00:16:00.350 --> 00:16:03.860
with some pretty limited
visual perception capabilities.

00:16:03.860 --> 00:16:06.380
We could detect
horizontal planes,

00:16:06.380 --> 00:16:10.040
and soon after, we added
support for vertical planes.

00:16:10.040 --> 00:16:12.500
And this was really important
for allowing applications

00:16:12.500 --> 00:16:16.610
to place AR objects
in places like floors,

00:16:16.610 --> 00:16:19.840
on tables, and on walls,
where real objects often lie.

00:16:22.830 --> 00:16:25.980
But what we care about so
much more than objects in many

00:16:25.980 --> 00:16:29.260
of our life
experiences are people.

00:16:29.260 --> 00:16:30.960
So with this in mind,
we felt that one

00:16:30.960 --> 00:16:33.510
of the most important
canvases for AR

00:16:33.510 --> 00:16:35.940
should be the human face.

00:16:35.940 --> 00:16:39.340
And this really isn't a very
new idea, if you think about it.

00:16:39.340 --> 00:16:43.920
We've been augmenting faces with
masks, makeup, and face paint

00:16:43.920 --> 00:16:46.050
for as long as we
can all remember,

00:16:46.050 --> 00:16:48.120
so it's not really
that surprising

00:16:48.120 --> 00:16:51.120
that people are really excited
to take these experiences

00:16:51.120 --> 00:16:54.780
to a new level in AR.

00:16:54.780 --> 00:16:57.540
But high-quality,
three-dimensional face

00:16:57.540 --> 00:17:00.930
perception is a really
difficult technical challenge.

00:17:00.930 --> 00:17:04.380
Faces are complex 3D
surfaces, and people

00:17:04.380 --> 00:17:08.280
are highly attuned to the
smallest shifts in expression.

00:17:08.280 --> 00:17:10.980
Furthermore, faces
are deformable,

00:17:10.980 --> 00:17:12.869
and face-tracking
solutions need to work

00:17:12.869 --> 00:17:16.680
across diverse face shapes,
hairstyles, skin colors,

00:17:16.680 --> 00:17:18.980
and age groups.

00:17:18.980 --> 00:17:22.329
So to solve these problems and
help a wide range of developers

00:17:22.329 --> 00:17:25.240
be able to launch
face-based AR applications,

00:17:25.240 --> 00:17:28.329
we launched Augmented
Faces recently.

00:17:28.329 --> 00:17:30.760
Augmented Faces for
front-facing cameras

00:17:30.760 --> 00:17:34.270
provides a high-quality,
468-point, three-dimensional

00:17:34.270 --> 00:17:37.360
mesh that tracks head
movements and changing

00:17:37.360 --> 00:17:39.070
facial expressions.

00:17:39.070 --> 00:17:40.940
Best of all, we use
machine learning,

00:17:40.940 --> 00:17:45.740
so this works on devices
without depth sensors.

00:17:45.740 --> 00:17:48.500
Now, if you think about
the level of realism

00:17:48.500 --> 00:17:50.150
that you can achieve between--

00:17:50.150 --> 00:17:51.710
the difference in
realism that you

00:17:51.710 --> 00:17:55.640
can achieve between a
plastic mask and motion

00:17:55.640 --> 00:17:57.620
capture-based CGI
effects, you'll

00:17:57.620 --> 00:18:00.920
start to understand why we're
so excited about what developers

00:18:00.920 --> 00:18:05.590
are going to do with these
high-quality face meshes.

00:18:05.590 --> 00:18:08.140
Augmented Faces is
unlocking new use cases

00:18:08.140 --> 00:18:11.392
for photography, social
media, and commerce.

00:18:11.392 --> 00:18:12.850
We're seeing really
strong interest

00:18:12.850 --> 00:18:16.270
from brands and retailers
for use cases like makeup

00:18:16.270 --> 00:18:20.500
and trying on makeup,
hair colors, eyeglasses,

00:18:20.500 --> 00:18:21.730
and accessories.

00:18:21.730 --> 00:18:23.500
And of course,
people are creating

00:18:23.500 --> 00:18:26.770
tons of fun photos with
everything from beauty effects

00:18:26.770 --> 00:18:28.720
to face morphing,
and so much more.

00:18:32.540 --> 00:18:34.580
In fact, interest
in Augmented Faces

00:18:34.580 --> 00:18:36.980
has been so high
that we've decided

00:18:36.980 --> 00:18:39.800
to bring Augmented
Faces and make it

00:18:39.800 --> 00:18:42.470
available on iOS this summer.

00:18:42.470 --> 00:18:47.840
It'll have the same high-quality
468-point face mesh as Android

00:18:47.840 --> 00:18:51.080
and will work on all
ARKit-capable devices

00:18:51.080 --> 00:18:53.780
without requiring
a depth sensor.

00:18:53.780 --> 00:18:56.000
And after we launch,
developers will

00:18:56.000 --> 00:18:58.880
be able to create Augmented
Faces applications that

00:18:58.880 --> 00:19:02.390
have the potential to reach
a billion users across iOS

00:19:02.390 --> 00:19:04.930
and Android.

00:19:04.930 --> 00:19:08.440
One of the first cross-platform
experiences to take advantage

00:19:08.440 --> 00:19:11.570
of this will be Meitu's
BeautyPlus application,

00:19:11.570 --> 00:19:13.270
which will feature--

00:19:13.270 --> 00:19:15.040
I see there's some
Meitu fans back there.

00:19:15.040 --> 00:19:17.110
Right on.

00:19:17.110 --> 00:19:20.620
And so BeautyPlus will feature
a number of great face effects,

00:19:20.620 --> 00:19:23.590
including this example, which
is a little gem that Ben's

00:19:23.590 --> 00:19:25.270
been playing with.

00:19:25.270 --> 00:19:28.420
In the experience here,
it tosses a birthday cake

00:19:28.420 --> 00:19:29.110
in your face.

00:19:29.110 --> 00:19:32.050
And true story-- we
actually couldn't figure out

00:19:32.050 --> 00:19:34.220
how to make it work.

00:19:34.220 --> 00:19:36.520
We tried and tried,
and then we realized

00:19:36.520 --> 00:19:38.620
that the trigger for
the birthday cake

00:19:38.620 --> 00:19:41.260
is actually when you open your
mouth in order to blow out

00:19:41.260 --> 00:19:42.430
the birthday candle.

00:19:42.430 --> 00:19:44.980
Perfectly natural, but
it's that little moment

00:19:44.980 --> 00:19:49.330
of surprise and delight when you
figure that out that represents

00:19:49.330 --> 00:19:52.595
the kind of moments we hope
more developers will create

00:19:52.595 --> 00:19:53.845
when they use Augmented Faces.

00:19:57.050 --> 00:19:59.080
And if you're here
with us on site,

00:19:59.080 --> 00:20:02.270
please try Augmented
Faces out for yourselves.

00:20:02.270 --> 00:20:06.070
There's a demo out in the
sandbox area behind the tent,

00:20:06.070 --> 00:20:08.072
and you can try a
photo booth experience

00:20:08.072 --> 00:20:10.030
where you can create some
neat selfies that you

00:20:10.030 --> 00:20:14.270
can share with your
friends and followers.

00:20:14.270 --> 00:20:17.000
So now I'd like to talk
about a different class

00:20:17.000 --> 00:20:20.030
of visual perception that
we're adding to phones.

00:20:20.030 --> 00:20:23.240
And we think it has just as
much potential to be helpful

00:20:23.240 --> 00:20:26.390
and fun as Augmented Faces.

00:20:26.390 --> 00:20:31.000
And that category
is Augmented Images.

00:20:31.000 --> 00:20:35.200
Think about all the 2D
images in our space.

00:20:35.200 --> 00:20:39.340
There's maps, signs,
posters, labels.

00:20:39.340 --> 00:20:42.130
These are the main ways we
annotate our physical space

00:20:42.130 --> 00:20:44.080
with information.

00:20:44.080 --> 00:20:48.340
They're cheap, they're easy to
print, they're easy to place,

00:20:48.340 --> 00:20:51.110
and they're everywhere.

00:20:51.110 --> 00:20:53.420
Now, think about
how much more useful

00:20:53.420 --> 00:20:56.420
these things would be if
your phone could recognize

00:20:56.420 --> 00:21:00.620
and transform each one into the
anchor for an interactive 3D

00:21:00.620 --> 00:21:02.590
experience.

00:21:02.590 --> 00:21:04.180
That was the vision
that prompted

00:21:04.180 --> 00:21:06.890
us to develop Augmented
Faces and launch it

00:21:06.890 --> 00:21:07.780
earlier this year.

00:21:11.060 --> 00:21:15.050
Version 1 of Augmented Faces
had some limitations, however.

00:21:15.050 --> 00:21:17.240
It used image
detection and object

00:21:17.240 --> 00:21:18.980
pose estimation
algorithms that were

00:21:18.980 --> 00:21:23.910
too expensive for us to run
on every single camera frame.

00:21:23.910 --> 00:21:26.990
So what we did was we
actually used ARCore motion

00:21:26.990 --> 00:21:33.620
tracking to do 3D
updates to render

00:21:33.620 --> 00:21:37.640
3D perspectives on your AR
content as your device moved.

00:21:37.640 --> 00:21:39.950
So the results here
were very high-quality,

00:21:39.950 --> 00:21:43.410
but this only works when
your target image is static,

00:21:43.410 --> 00:21:46.670
so it limited the set of
use cases we could support.

00:21:46.670 --> 00:21:49.340
To overcome this
limitation, we significantly

00:21:49.340 --> 00:21:51.560
revamped the computer
vision algorithms

00:21:51.560 --> 00:21:53.780
behind Augmented Images.

00:21:53.780 --> 00:21:57.950
And in ARCore version 1.9,
which is launching this week,

00:21:57.950 --> 00:22:01.970
we've added the ability to
track moving target images.

00:22:01.970 --> 00:22:03.710
Along with these
algorithmic changes,

00:22:03.710 --> 00:22:07.300
we've improved image
detection recall by 15%.

00:22:07.300 --> 00:22:10.340
We've boosted tracking
precision by 30%.

00:22:10.340 --> 00:22:13.190
And we've gained the ability
to track multiple objects

00:22:13.190 --> 00:22:14.015
in the same frame.

00:22:17.100 --> 00:22:19.350
So with moving
augmented images, you

00:22:19.350 --> 00:22:22.500
can now do things like attach
AR content to movable objects,

00:22:22.500 --> 00:22:26.190
like product boxes, printed
documents, and game pieces,

00:22:26.190 --> 00:22:28.920
like in this
example from JD.com,

00:22:28.920 --> 00:22:31.170
which is a children's
spelling game.

00:22:31.170 --> 00:22:34.770
In this game, once you
spell a word correctly,

00:22:34.770 --> 00:22:36.780
it gives you positive
feedback by showing you

00:22:36.780 --> 00:22:38.070
that word in action.

00:22:40.660 --> 00:22:44.620
You can also do things
like alter physical reality

00:22:44.620 --> 00:22:46.960
with moving augmented images.

00:22:46.960 --> 00:22:50.260
So in this example called
Notable Women, which

00:22:50.260 --> 00:22:53.050
is a collaboration between
Google Cloud Creative Lab

00:22:53.050 --> 00:22:58.450
and Rosie Rios, who was the 43rd
treasurer of the United States,

00:22:58.450 --> 00:23:00.940
the application highlights
the achievements

00:23:00.940 --> 00:23:03.760
of notable historical
American women

00:23:03.760 --> 00:23:06.730
by swapping their
faces onto US currency.

00:23:06.730 --> 00:23:08.620
The level of
realism here is only

00:23:08.620 --> 00:23:11.665
possible with the improvements
we've made to augmented images.

00:23:14.690 --> 00:23:17.630
So that was just a
quick introduction

00:23:17.630 --> 00:23:20.830
to Augmented Faces and
Augmented Images, but if you're

00:23:20.830 --> 00:23:23.330
interested in learning more
about these technologies

00:23:23.330 --> 00:23:25.520
and using them in
your own applications,

00:23:25.520 --> 00:23:28.490
please join us at tomorrow
afternoon's dedicated session.

00:23:31.297 --> 00:23:32.880
So now that we've
had a chance to look

00:23:32.880 --> 00:23:36.060
at some of ARCore's new visual
perception capabilities,

00:23:36.060 --> 00:23:38.010
I'd like to turn things
over to Christina,

00:23:38.010 --> 00:23:41.670
who's going to talk about the
ways we're bringing greater

00:23:41.670 --> 00:23:45.045
realism and utility to ARCore.

00:23:45.045 --> 00:23:48.510
[APPLAUSE]

00:23:49.500 --> 00:23:51.450
CHRISTINA TONG: Thank you, Leon.

00:23:51.450 --> 00:23:55.080
One of AR's fundamental
goals is to blend the virtual

00:23:55.080 --> 00:23:56.550
with the real.

00:23:56.550 --> 00:23:59.490
I want to really believe
that that virtual pet is here

00:23:59.490 --> 00:24:00.810
with me.

00:24:00.810 --> 00:24:03.180
And that couch that I'm
thinking about buying,

00:24:03.180 --> 00:24:05.820
I want to see it in
AR in my living room

00:24:05.820 --> 00:24:08.760
as if it were actually there.

00:24:08.760 --> 00:24:11.670
And realism really
matters for immersion.

00:24:11.670 --> 00:24:15.420
Just think about the difference
between great and not-so-great

00:24:15.420 --> 00:24:17.970
CGI in the movies.

00:24:17.970 --> 00:24:20.100
Having realism
really helps to keep

00:24:20.100 --> 00:24:22.320
users grounded in
that experience

00:24:22.320 --> 00:24:25.620
and engaged with
that experience.

00:24:25.620 --> 00:24:28.290
And one of the key
parts to making AR real

00:24:28.290 --> 00:24:30.780
is to get the lighting right.

00:24:30.780 --> 00:24:33.800
Let's take a look at why.

00:24:33.800 --> 00:24:35.660
So take a look at this picture.

00:24:35.660 --> 00:24:36.920
What do you see?

00:24:36.920 --> 00:24:38.560
I think it's a
pretty simple photo.

00:24:38.560 --> 00:24:41.230
We have a chair, a
plant, and a mirror,

00:24:41.230 --> 00:24:45.260
all against a simple
wooden wall here.

00:24:45.260 --> 00:24:48.160
But there are actually so
many human perceptual cues

00:24:48.160 --> 00:24:51.100
that we see in the scene,
and we think about these

00:24:51.100 --> 00:24:53.590
and use them subconsciously
to understand

00:24:53.590 --> 00:24:55.690
what's happening in the scene.

00:24:55.690 --> 00:24:59.170
One of those perceptual
cues is specular highlights,

00:24:59.170 --> 00:25:03.400
which are shiny spots that
appear on objects when

00:25:03.400 --> 00:25:06.130
light illuminates them.

00:25:06.130 --> 00:25:08.320
Another perceptual
cue is shadows,

00:25:08.320 --> 00:25:10.930
areas that are darker
because less light falls

00:25:10.930 --> 00:25:15.800
on them, because that light
is blocked by other objects.

00:25:15.800 --> 00:25:19.150
We also see
differences in shading.

00:25:19.150 --> 00:25:22.010
Some objects are angled
differently from the camera.

00:25:22.010 --> 00:25:24.580
They are farther
away or closer to us,

00:25:24.580 --> 00:25:26.380
or they have different
material properties,

00:25:26.380 --> 00:25:30.740
like being less or
more reflective.

00:25:30.740 --> 00:25:33.900
So we can see that even in
a simple image like this,

00:25:33.900 --> 00:25:36.230
we actually have so many
different perceptual cues

00:25:36.230 --> 00:25:38.850
that we use to understand
what's going on.

00:25:38.850 --> 00:25:41.600
These are the inputs
to our understanding.

00:25:41.600 --> 00:25:43.280
And the output is
that we actually

00:25:43.280 --> 00:25:46.280
understand that in this
scene, the light is coming

00:25:46.280 --> 00:25:48.350
from the front-right
of the scene,

00:25:48.350 --> 00:25:51.500
and that the light is
actually pretty bright.

00:25:51.500 --> 00:25:55.730
Now, what if we wanted to add
an AR object into this scene,

00:25:55.730 --> 00:25:58.940
but maybe an AR object
that we wouldn't normally

00:25:58.940 --> 00:26:02.780
see in a scene like
this, like this rocket?

00:26:02.780 --> 00:26:05.930
Now, note here that the
rocket is pretty shiny.

00:26:05.930 --> 00:26:07.340
It's very reflective.

00:26:07.340 --> 00:26:09.320
It has details like
the rivets, and it's

00:26:09.320 --> 00:26:11.330
got a reflective window.

00:26:11.330 --> 00:26:14.330
The rocket's also casting
a nice, soft shadow

00:26:14.330 --> 00:26:16.580
on the ground.

00:26:16.580 --> 00:26:18.140
We're going to put this scene--

00:26:18.140 --> 00:26:21.050
this rocket-- into the
scene that we just saw,

00:26:21.050 --> 00:26:25.160
and we're going to use a very
common heuristic to light it.

00:26:25.160 --> 00:26:28.730
That heuristic will be to
take the average brightness

00:26:28.730 --> 00:26:30.900
of the pixels in
the scene, and we're

00:26:30.900 --> 00:26:34.160
going to apply that to the
rocket's surface equally

00:26:34.160 --> 00:26:37.080
as ambient illumination.

00:26:37.080 --> 00:26:39.890
Now, ambient light gives
the same light and intensity

00:26:39.890 --> 00:26:42.710
to every object and every
surface in the scene

00:26:42.710 --> 00:26:45.260
from no particular direction.

00:26:45.260 --> 00:26:47.420
For a shiny object
like this, we're

00:26:47.420 --> 00:26:50.720
going to see that using ambient
illumination only to light

00:26:50.720 --> 00:26:57.350
the object doesn't necessarily
result in the prettiest result.

00:26:57.350 --> 00:27:00.650
As you can see here, this
doesn't look quite right.

00:27:00.650 --> 00:27:02.990
Now, what is really
great is that the rocket

00:27:02.990 --> 00:27:05.120
is anchored onto the floor.

00:27:05.120 --> 00:27:07.880
One of the first steps
towards having a realistic AR

00:27:07.880 --> 00:27:10.610
experience is to actually
have your objects

00:27:10.610 --> 00:27:14.190
look like they're sitting there
and grounded on the floor.

00:27:14.190 --> 00:27:16.770
But this rocket
doesn't have a shadow,

00:27:16.770 --> 00:27:19.370
and it looks really dark,
because its material here

00:27:19.370 --> 00:27:22.220
is kind of dark, and
there isn't enough energy

00:27:22.220 --> 00:27:27.080
from the ambient intensity
only to create that shininess.

00:27:27.080 --> 00:27:28.550
Now, there are some
tricks we could

00:27:28.550 --> 00:27:33.110
use to fix this, like
lightening the overall material.

00:27:33.110 --> 00:27:37.045
But then the rocket might look
too bright in other scenes,

00:27:37.045 --> 00:27:38.420
and we still
wouldn't capture any

00:27:38.420 --> 00:27:40.220
of the shininess or
the shadows that we

00:27:40.220 --> 00:27:43.770
want to see on a real object.

00:27:43.770 --> 00:27:46.160
What we would really
want, ideally,

00:27:46.160 --> 00:27:48.170
is to be able to
actually understand

00:27:48.170 --> 00:27:52.760
where light is coming from
in the scene from 360 degrees

00:27:52.760 --> 00:27:55.610
and in high dynamic range,
which is the range of light

00:27:55.610 --> 00:27:57.440
that humans see.

00:27:57.440 --> 00:28:00.350
Ideally, we would
also want ARCore to do

00:28:00.350 --> 00:28:03.440
this for us out of the box.

00:28:03.440 --> 00:28:07.190
The result might look
something like this.

00:28:07.190 --> 00:28:09.020
On the right-hand
side, we can see

00:28:09.020 --> 00:28:10.700
that this rocket
looks much more real

00:28:10.700 --> 00:28:13.070
and integrated into the scene.

00:28:13.070 --> 00:28:15.800
Notice how the shadows
on the legs of the rocket

00:28:15.800 --> 00:28:18.800
actually match the shadow
direction coming off

00:28:18.800 --> 00:28:21.920
of the legs of the
planter in the scene.

00:28:21.920 --> 00:28:24.410
Notice how the specular
highlights on the rocket

00:28:24.410 --> 00:28:28.012
also match the direction
of the light coming in.

00:28:28.012 --> 00:28:30.470
As much as you might be able
to believe that a rocket would

00:28:30.470 --> 00:28:32.390
actually be in a
scene like this,

00:28:32.390 --> 00:28:36.030
this one really looks
like it's there.

00:28:36.030 --> 00:28:39.680
So today we're excited to
announce new ARCore APIs that

00:28:39.680 --> 00:28:42.830
will allow you to render
realistic AR assets

00:28:42.830 --> 00:28:44.970
like the one on the right.

00:28:44.970 --> 00:28:48.590
In fact, the image on the right
was captured live on a Pixel 3

00:28:48.590 --> 00:28:51.380
running our new APIs.

00:28:51.380 --> 00:28:54.860
Out of the box, these new APIs
will provide three things--

00:28:54.860 --> 00:28:58.700
directional lighting, ambient
spherical harmonics, and a cube

00:28:58.700 --> 00:29:00.830
map for reflections.

00:29:00.830 --> 00:29:03.290
So let's walk through
what each of those APIs

00:29:03.290 --> 00:29:05.795
can provide us by
looking at a live demo.

00:29:05.795 --> 00:29:08.420
We're going to welcome Ben back
to the stage to help with that.

00:29:13.766 --> 00:29:15.071
[APPLAUSE]

00:29:15.071 --> 00:29:16.196
BENJAMIN SCHROM: Thank you.

00:29:18.672 --> 00:29:21.130
CHRISTINA TONG: Now, the first
thing that we're going to do

00:29:21.130 --> 00:29:24.580
is to place the rocket
back into the scene

00:29:24.580 --> 00:29:28.540
with ambient illumination only.

00:29:28.540 --> 00:29:30.130
As we can see here,
it doesn't look

00:29:30.130 --> 00:29:32.950
very grounded or realistic.

00:29:32.950 --> 00:29:34.172
There's no shadow.

00:29:34.172 --> 00:29:36.130
But one of the simplest
things that we could do

00:29:36.130 --> 00:29:38.230
is to add in a shadow.

00:29:38.230 --> 00:29:40.455
Unfortunately, without
these new APIs,

00:29:40.455 --> 00:29:42.580
we wouldn't actually know
where the strongest light

00:29:42.580 --> 00:29:44.600
in the scene is coming from.

00:29:44.600 --> 00:29:47.360
But we could do the simplest
and naive thing here,

00:29:47.360 --> 00:29:50.110
which is to put a single
directional light from directly

00:29:50.110 --> 00:29:52.520
above, and going
down at the ground.

00:29:52.520 --> 00:29:55.420
So let's take a look at
what that looks like.

00:29:55.420 --> 00:29:56.780
It's looking a little better.

00:29:56.780 --> 00:29:58.210
We can see the
specular highlights

00:29:58.210 --> 00:30:01.120
on the top of the rocket,
and we can see a soft shadow

00:30:01.120 --> 00:30:03.460
on the bottom of the rocket.

00:30:03.460 --> 00:30:05.620
But as Ben will show
you, that shadow

00:30:05.620 --> 00:30:07.510
doesn't actually
match the direction

00:30:07.510 --> 00:30:12.430
of the shadows that are on
the table and on the chair.

00:30:12.430 --> 00:30:13.930
So in a second,
we're actually going

00:30:13.930 --> 00:30:17.140
to turn on the
environmental HDR lighting,

00:30:17.140 --> 00:30:20.050
and we're going to see how
this shadow will actually

00:30:20.050 --> 00:30:22.487
immediately change to match
the shadows in the scene.

00:30:22.487 --> 00:30:24.820
And we're going to see that,
because we're understanding

00:30:24.820 --> 00:30:26.960
light's direction in the scene.

00:30:26.960 --> 00:30:29.530
So as you can see here,
using machine learning,

00:30:29.530 --> 00:30:33.130
our algorithm has actually
noticed where the strongest

00:30:33.130 --> 00:30:35.410
directional light is coming
from and rendered that

00:30:35.410 --> 00:30:37.900
onto the rocket.

00:30:37.900 --> 00:30:41.540
Now, you can see the shadows
in the same direction,

00:30:41.540 --> 00:30:43.330
but you'll recall
that the rocket is

00:30:43.330 --> 00:30:45.627
supposed to be pretty shiny.

00:30:45.627 --> 00:30:47.710
And this rocket that we
see in the scene right now

00:30:47.710 --> 00:30:49.870
isn't quite shiny yet.

00:30:49.870 --> 00:30:52.870
What we need is a way to
get realistic reflections

00:30:52.870 --> 00:30:54.520
from all directions.

00:30:54.520 --> 00:30:57.190
And to do that, we're going to
turn on the cube map, which is

00:30:57.190 --> 00:30:59.170
also provided by our new APIs.

00:31:01.890 --> 00:31:04.570
Now we can see that
it's turned on.

00:31:04.570 --> 00:31:06.870
You can see that the
shadows, reflections,

00:31:06.870 --> 00:31:09.360
and the lighting on
the rocket really match

00:31:09.360 --> 00:31:11.155
those of the surroundings.

00:31:14.430 --> 00:31:16.170
Even in the stage
lighting that we

00:31:16.170 --> 00:31:18.390
have here, which is a
fairly unnatural environment

00:31:18.390 --> 00:31:21.150
to be standing on
a stage, we can

00:31:21.150 --> 00:31:24.030
see that our estimation of where
the light is coming from really

00:31:24.030 --> 00:31:24.982
works.

00:31:24.982 --> 00:31:26.190
Thank you, Ben, for the demo.

00:31:26.190 --> 00:31:29.606
[CHEERING AND APPLAUSE]

00:31:33.520 --> 00:31:36.480
So one other thing to note is
that the lighting on the stage

00:31:36.480 --> 00:31:38.980
was pretty static
during the demo.

00:31:38.980 --> 00:31:42.370
But in the real world,
lights are often dynamic.

00:31:42.370 --> 00:31:43.890
Let's go back to
the slides here,

00:31:43.890 --> 00:31:45.390
and we're going to take
a look what happens

00:31:45.390 --> 00:31:46.560
when the lights are dynamic.

00:31:49.610 --> 00:31:51.650
So environmental
HDR lighting also

00:31:51.650 --> 00:31:54.750
works even when the lights are
moving around in the scene,

00:31:54.750 --> 00:31:56.880
as you can see right here.

00:31:56.880 --> 00:31:58.580
Now, one of these
figures is virtual,

00:31:58.580 --> 00:32:02.600
and we're going to take a quick
vote to see if you can tell.

00:32:02.600 --> 00:32:07.990
Raise your hand if you think
the one on your left is virtual.

00:32:07.990 --> 00:32:08.670
All right.

00:32:08.670 --> 00:32:12.530
Raise your hand if you
think the one on this side,

00:32:12.530 --> 00:32:16.050
on your right, is virtual.

00:32:16.050 --> 00:32:16.620
All right.

00:32:16.620 --> 00:32:20.000
And raise your hand if
you can't quite tell.

00:32:20.000 --> 00:32:20.720
All right.

00:32:20.720 --> 00:32:22.970
So we fooled some of you.

00:32:22.970 --> 00:32:26.840
The one on your right-hand
side is the real mannequin,

00:32:26.840 --> 00:32:29.540
and the one on
your left-hand side

00:32:29.540 --> 00:32:32.703
is a virtual mannequin
placed in AR.

00:32:32.703 --> 00:32:34.370
And we can see that
the lighting on both

00:32:34.370 --> 00:32:37.220
is reacting dynamically,
realistically,

00:32:37.220 --> 00:32:40.240
as the light in the scene
pans from left to right.

00:32:40.240 --> 00:32:44.492
And this footage was also
captured live on a smartphone.

00:32:44.492 --> 00:32:46.700
If you're at I/O, you can
see this scene for yourself

00:32:46.700 --> 00:32:49.340
today in the sandbox.

00:32:49.340 --> 00:32:51.770
Now, you might be
wondering, do I

00:32:51.770 --> 00:32:55.010
have to know all about
specular highlights,

00:32:55.010 --> 00:32:59.330
and shadows, and the rest to
be able to use this in my app?

00:32:59.330 --> 00:33:00.800
Don't worry, you don't.

00:33:00.800 --> 00:33:02.730
We take care of that for you.

00:33:02.730 --> 00:33:05.380
So these APIs will work
for you out of the box.

00:33:05.380 --> 00:33:07.130
But I want to give you
a little sneak peek

00:33:07.130 --> 00:33:09.080
at the challenges
we overcame and some

00:33:09.080 --> 00:33:12.050
of the machine learning magic
that we use for this feature.

00:33:15.300 --> 00:33:18.090
So we really had to
overcome two key challenges.

00:33:18.090 --> 00:33:20.300
The first challenge is
that cell phones have

00:33:20.300 --> 00:33:22.770
a really limited field of view.

00:33:22.770 --> 00:33:27.200
In fact, your cell phone only
sees 6% of the 360 degrees

00:33:27.200 --> 00:33:30.000
around you.

00:33:30.000 --> 00:33:31.620
The second challenge
is that phones

00:33:31.620 --> 00:33:34.000
see in low dynamic range.

00:33:34.000 --> 00:33:36.030
So you and I, humans,
we can generally

00:33:36.030 --> 00:33:38.430
see very bright and
very dark, and we

00:33:38.430 --> 00:33:41.220
can tell the difference
between those extremes.

00:33:41.220 --> 00:33:45.390
But your cell phone can see
some bright and some dark,

00:33:45.390 --> 00:33:47.800
and the range between
those is not very large.

00:33:47.800 --> 00:33:49.840
That's low dynamic range.

00:33:49.840 --> 00:33:53.520
So our challenge was to convert
a single, low dynamic range

00:33:53.520 --> 00:33:57.150
frame that sees 6% of the
world, and extrapolate

00:33:57.150 --> 00:34:01.290
from that 360-degree
lighting in HDR.

00:34:01.290 --> 00:34:03.120
So we do this with
machine learning

00:34:03.120 --> 00:34:05.310
using a TensorFlow-lite
neural net

00:34:05.310 --> 00:34:08.370
and training samples like
the one pictured here.

00:34:08.370 --> 00:34:11.190
So you don't have to worry
about sensing perceptual cues

00:34:11.190 --> 00:34:13.620
and translating that
into AR lighting.

00:34:13.620 --> 00:34:16.670
We'll take care of that for you.

00:34:16.670 --> 00:34:19.340
If you're interested in learning
the details in how we developed

00:34:19.340 --> 00:34:21.949
this feature and how
to use it in your app,

00:34:21.949 --> 00:34:26.610
please attend tomorrow
morning's dedicated session.

00:34:26.610 --> 00:34:29.270
So again, that's
environmental HDR lighting,

00:34:29.270 --> 00:34:32.020
coming this summer
to all ARCore phones.

00:34:32.020 --> 00:34:34.730
It will help your users
to experience true realism

00:34:34.730 --> 00:34:37.100
and immersion in the
apps that you build.

00:34:39.909 --> 00:34:41.920
Now, we've just talked
about exciting features,

00:34:41.920 --> 00:34:44.170
like realistic lighting,
Augmented Faces,

00:34:44.170 --> 00:34:46.550
Augmented Images, and more.

00:34:46.550 --> 00:34:49.070
But we also want to make
it really easy to bring

00:34:49.070 --> 00:34:51.560
ARCore features to your users.

00:34:51.560 --> 00:34:52.989
We want to make
it easy for people

00:34:52.989 --> 00:34:57.760
to access compelling AR
experiences from your website.

00:34:57.760 --> 00:35:02.500
So today, we're
introducing Scene Viewer.

00:35:02.500 --> 00:35:05.170
Scene Viewer is a
3D and AR viewer

00:35:05.170 --> 00:35:08.830
that runs natively on Android
and allows users to seamlessly

00:35:08.830 --> 00:35:12.820
put any 3D content from your
website into your space,

00:35:12.820 --> 00:35:14.770
just like this penguin.

00:35:14.770 --> 00:35:16.420
Now, this penguin
is being launched

00:35:16.420 --> 00:35:21.288
straight from Google Search into
my space in its lifelike size.

00:35:21.288 --> 00:35:23.830
You may have seen something like
this in the consumer keynote

00:35:23.830 --> 00:35:25.690
this morning.

00:35:25.690 --> 00:35:28.720
Now, what's really important
is that the best of ARCore

00:35:28.720 --> 00:35:31.570
will work out of the
box with Scene Viewer.

00:35:31.570 --> 00:35:35.580
That includes motion tracking,
plane finding, and more.

00:35:35.580 --> 00:35:38.000
And the environmental
HDR lighting you just saw

00:35:38.000 --> 00:35:40.200
will be coming soon to
Scene Viewer as well.

00:35:42.750 --> 00:35:45.170
You can even add
in-context titles

00:35:45.170 --> 00:35:48.385
and call to actions
for your users.

00:35:48.385 --> 00:35:49.760
In the flow that's
pictured here,

00:35:49.760 --> 00:35:51.680
I'm thinking about
buying a new chair,

00:35:51.680 --> 00:35:54.080
and I want to put
it in my bedroom.

00:35:54.080 --> 00:35:56.540
To do that, I want to use
Scene Viewer to actually view

00:35:56.540 --> 00:35:58.070
that chair in my space.

00:35:58.070 --> 00:36:00.200
So in the right-most
image here, we

00:36:00.200 --> 00:36:03.310
have the AR chair placed
into the real bedroom.

00:36:03.310 --> 00:36:05.900
And the nice thing is
that from that AR view,

00:36:05.900 --> 00:36:09.320
I can actually directly intent
to the URL that will allow

00:36:09.320 --> 00:36:12.030
me to purchase that chair.

00:36:12.030 --> 00:36:16.760
Now, let's take a look at how
easy it is to use Scene Viewer.

00:36:16.760 --> 00:36:18.890
You may know about
Model Viewer, which

00:36:18.890 --> 00:36:22.130
is an open source,
cross-browser web component that

00:36:22.130 --> 00:36:25.950
allows users to see objects
in 3D in the browser.

00:36:25.950 --> 00:36:29.900
Scene Viewer will work
hand-in-hand with Model Viewer.

00:36:29.900 --> 00:36:31.940
Whenever Scene
Viewer is available,

00:36:31.940 --> 00:36:36.620
Model Viewer will
seamlessly intent out to it.

00:36:36.620 --> 00:36:40.590
All it takes to enable Scene
Viewer is to add two letters--

00:36:40.590 --> 00:36:43.550
two letters only--
to the HTML tag.

00:36:43.550 --> 00:36:48.490
Not surprisingly, those
two letters are A and R.

00:36:48.490 --> 00:36:50.570
Next, let's take a
look at an example

00:36:50.570 --> 00:36:55.920
where Model Viewer intents out
seamlessly to Scene Viewer.

00:36:55.920 --> 00:36:57.960
We've been working
with partners like NASA

00:36:57.960 --> 00:37:00.690
to bring their web
content to life.

00:37:00.690 --> 00:37:03.120
This model of the
Mars Curiosity Rover

00:37:03.120 --> 00:37:07.750
goes straight from the
web into your house.

00:37:07.750 --> 00:37:10.470
Whether it be for
shopping or education,

00:37:10.470 --> 00:37:12.690
it is often dramatically
more compelling

00:37:12.690 --> 00:37:15.540
to see objects in
their real-life size,

00:37:15.540 --> 00:37:17.640
to get up close to
them, and to view them

00:37:17.640 --> 00:37:21.300
as if they were actually
there in your space.

00:37:21.300 --> 00:37:24.420
If you're at I/O, you can also
experience the Mars Curiosity

00:37:24.420 --> 00:37:27.330
Rover in Scene Viewer
in the AR sandbox.

00:37:30.440 --> 00:37:33.980
To learn more about how you can
build amazing AR experiences,

00:37:33.980 --> 00:37:35.930
attend some of these
upcoming sessions

00:37:35.930 --> 00:37:38.000
over the next few days.

00:37:38.000 --> 00:37:40.160
And you can check
out our ARCore demos

00:37:40.160 --> 00:37:43.550
in Sandbox B. You can
also come to our code labs

00:37:43.550 --> 00:37:45.590
for some detailed
tutorials on how to use

00:37:45.590 --> 00:37:49.200
these features in your apps.

00:37:49.200 --> 00:37:51.760
So thank you so, so much
for listening to this talk.

00:37:51.760 --> 00:37:54.910
We're really looking forward to
see what you build in ARCore.

00:37:54.910 --> 00:37:55.450
Thank you.

00:37:55.450 --> 00:37:58.800
[MUSIC PLAYING]

