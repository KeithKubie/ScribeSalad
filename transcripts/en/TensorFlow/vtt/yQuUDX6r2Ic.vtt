WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.772
[MUSIC PLAYING]

00:00:04.083 --> 00:00:05.500
DANIEL SITUNAYAKE:
Hey, everybody.

00:00:05.500 --> 00:00:07.210
So my name's Daniel.

00:00:07.210 --> 00:00:08.910
LAURENCE MORONEY:
And I'm Lawrence.

00:00:08.910 --> 00:00:11.220
DANIEL SITUNAYAKE: And
we have an awesome talk

00:00:11.220 --> 00:00:12.390
for you, this afternoon.

00:00:12.390 --> 00:00:14.370
So I've been super
excited watching

00:00:14.370 --> 00:00:16.810
the keynote, because there's
just been so much stuff

00:00:16.810 --> 00:00:18.310
that is relevant
to what we're going

00:00:18.310 --> 00:00:20.700
to be talking about today,
which is running machine

00:00:20.700 --> 00:00:23.400
learning on devices.

00:00:23.400 --> 00:00:25.890
So we've seen the
kind of amazing things

00:00:25.890 --> 00:00:28.200
that you can do if you're
running ML at the edge

00:00:28.200 --> 00:00:29.280
and on device.

00:00:29.280 --> 00:00:31.590
But there are a ton of
options that developers have

00:00:31.590 --> 00:00:33.487
for doing this type of stuff.

00:00:33.487 --> 00:00:35.320
And it can be a little
bit hard to navigate.

00:00:35.320 --> 00:00:37.470
So we decided to put
this session together

00:00:37.470 --> 00:00:39.063
to give you an overview.

00:00:39.063 --> 00:00:40.980
We're going to be showing
you all the options,

00:00:40.980 --> 00:00:43.020
walking you through some
code, and showing you

00:00:43.020 --> 00:00:44.160
some awesome demos.

00:00:44.160 --> 00:00:47.580
So hopefully, you'll enjoy.

00:00:47.580 --> 00:00:50.400
So first, we're going to
walk through using TensorFlow

00:00:50.400 --> 00:00:54.330
to train models and then
export saved models, which

00:00:54.330 --> 00:00:57.677
you can then convert
to deploy on devices.

00:00:57.677 --> 00:00:59.760
We're then going to see a
number of different ways

00:00:59.760 --> 00:01:03.007
that you can deploy to
Android and iOS devices.

00:01:03.007 --> 00:01:04.590
And finally, we're
going to talk about

00:01:04.590 --> 00:01:07.410
some new and super exciting
hardware devices that you

00:01:07.410 --> 00:01:10.790
can use to run your models.

00:01:10.790 --> 00:01:12.460
So first of all,
I want to give you

00:01:12.460 --> 00:01:15.070
an overview of some different
technologies and the device

00:01:15.070 --> 00:01:17.590
types they each
allow you to support.

00:01:17.590 --> 00:01:19.930
So to begin with,
we have ML Kit,

00:01:19.930 --> 00:01:22.690
which is designed to make
it super easy to deploy

00:01:22.690 --> 00:01:25.800
ML inside of mobile apps.

00:01:25.800 --> 00:01:28.340
We then have TensorFlow.js,
which basically

00:01:28.340 --> 00:01:31.700
lets you target any device that
has a JavaScript interpreter,

00:01:31.700 --> 00:01:34.490
whether that's in browser
or through Node.js.

00:01:34.490 --> 00:01:37.710
So that even supports
embedded platforms.

00:01:37.710 --> 00:01:39.950
And finally, TensorFlow
Lite gives you

00:01:39.950 --> 00:01:43.400
high performance inference
across any device or embedded

00:01:43.400 --> 00:01:45.680
platform, all the way
from mobile phones

00:01:45.680 --> 00:01:48.700
to microcontrollers.

00:01:48.700 --> 00:01:50.620
So before we get
any further, let's

00:01:50.620 --> 00:01:54.070
talk a little bit about
TensorFlow itself.

00:01:54.070 --> 00:01:58.090
So TensorFlow is Google's tool
chain for absolutely everything

00:01:58.090 --> 00:01:59.830
to do with machine learning.

00:01:59.830 --> 00:02:02.680
And as you can see, there
are TensorFlow tools

00:02:02.680 --> 00:02:05.680
for basically every
part of the ML workflow,

00:02:05.680 --> 00:02:08.470
from loading data through
to building models

00:02:08.470 --> 00:02:12.380
and then deploying them
to devices and servers.

00:02:12.380 --> 00:02:14.020
So for this section
of the talk, we're

00:02:14.020 --> 00:02:16.750
going to focus on building
a model with TensorFlow

00:02:16.750 --> 00:02:21.380
and then deploying it as
a TensorFlow Lite model.

00:02:21.380 --> 00:02:23.270
There are actually
tons of ways to get up

00:02:23.270 --> 00:02:25.190
and running with
TensorFlow on device.

00:02:25.190 --> 00:02:28.580
So the quickest way is to try
out our demo apps and sample

00:02:28.580 --> 00:02:29.300
code.

00:02:29.300 --> 00:02:31.850
And we also have a big
library of pretrained models

00:02:31.850 --> 00:02:34.890
that you can drop into your
apps that are ready to use.

00:02:34.890 --> 00:02:37.250
You can also take
these and retrain them

00:02:37.250 --> 00:02:41.050
based on your own data
using transfer learning.

00:02:41.050 --> 00:02:42.900
You can, as you've
seen this morning,

00:02:42.900 --> 00:02:45.270
use Federated Learning
to train models

00:02:45.270 --> 00:02:48.840
based on distributed data
across a pool of devices.

00:02:48.840 --> 00:02:51.850
And you can finally build
models from scratch,

00:02:51.850 --> 00:02:54.390
which is what Laurence
is now going to show off.

00:02:54.390 --> 00:02:56.790
LAURENCE MORONEY:
Thank you, Daniel.

00:02:56.790 --> 00:02:58.240
Quick question for everybody.

00:02:58.240 --> 00:03:02.320
How many of you have ever
built a machine learn model?

00:03:02.320 --> 00:03:02.820
Oh, wow.

00:03:02.820 --> 00:03:03.795
DANIEL SITUNAYAKE: Wow.

00:03:03.795 --> 00:03:05.443
LAURENCE MORONEY: Oh, wow.

00:03:05.443 --> 00:03:06.360
Big round of applause.

00:03:06.360 --> 00:03:08.152
So hopefully, this
isn't too basic for you,

00:03:08.152 --> 00:03:09.600
what I'm going to be showing.

00:03:09.600 --> 00:03:11.820
But I want to show
just the process

00:03:11.820 --> 00:03:13.670
of building a model
and some of the stops

00:03:13.670 --> 00:03:15.420
that you can then do
to prepare that model

00:03:15.420 --> 00:03:18.390
to run on the mobile devices
that Daniel was talking about.

00:03:18.390 --> 00:03:20.220
Can we switch to
the laptop, please?

00:03:22.900 --> 00:03:25.360
Can folks at the
back read that code?

00:03:25.360 --> 00:03:27.490
Just wave your hands if you can.

00:03:27.490 --> 00:03:29.950
OK, good.

00:03:29.950 --> 00:03:32.740
Wave them like this
if you need it bigger.

00:03:32.740 --> 00:03:36.750
OK, some do, or you
just want to stretch.

00:03:36.750 --> 00:03:37.710
Let's see.

00:03:37.710 --> 00:03:39.230
How's that?

00:03:39.230 --> 00:03:40.400
OK, cool.

00:03:40.400 --> 00:03:42.820
So I'm just going to show
some very basic TensorFlow

00:03:42.820 --> 00:03:43.830
code, here.

00:03:43.830 --> 00:03:45.490
And I wanted to
show the simplest

00:03:45.490 --> 00:03:47.832
possible neural
network that I could.

00:03:47.832 --> 00:03:50.290
So for those of you who've
never built something in machine

00:03:50.290 --> 00:03:52.630
learning or have never
built a machine learn model,

00:03:52.630 --> 00:03:55.120
the idea is like with
a neural network,

00:03:55.120 --> 00:03:57.430
you can do some basic
pattern matching

00:03:57.430 --> 00:03:58.875
from inputs to outputs.

00:03:58.875 --> 00:04:00.250
We're at Google
I/O, so I'm going

00:04:00.250 --> 00:04:02.290
to talk about inputs
and outputs a lot.

00:04:02.290 --> 00:04:04.420
And in this case, I'm
creating the simplest

00:04:04.420 --> 00:04:06.280
possible neural network I can.

00:04:06.280 --> 00:04:08.530
And this is a neural
network with a single layer

00:04:08.530 --> 00:04:11.320
and a single neuron in
that neural network.

00:04:11.320 --> 00:04:14.020
And that's this line
of code, right here.

00:04:14.020 --> 00:04:16.885
The Keras.layers.Dense
units equal 1.

00:04:16.885 --> 00:04:18.309
Input shape equals 1.

00:04:18.309 --> 00:04:21.095
And I'm going to then train this
neural network on some data,

00:04:21.095 --> 00:04:22.720
and that's what you
can see in the line

00:04:22.720 --> 00:04:25.030
underneath-- the Xs and the Ys.

00:04:25.030 --> 00:04:29.560
Now, there is a relationship
between these data points.

00:04:29.560 --> 00:04:32.440
Can anybody guess what
that relationship is?

00:04:32.440 --> 00:04:36.462
There's a clue in the 0 and 32.

00:04:36.462 --> 00:04:37.926
AUDIENCE: Temperature.

00:04:37.926 --> 00:04:40.430
LAURENCE MORONEY: Yeah, a
temperature conversion, right?

00:04:40.430 --> 00:04:43.580
So the idea is I could write
code that's like 9 over 5

00:04:43.580 --> 00:04:45.890
plus whatever, plus 32.

00:04:45.890 --> 00:04:47.990
But I want to do it as a
machine learn model, just

00:04:47.990 --> 00:04:49.320
to give as an example.

00:04:49.320 --> 00:04:51.480
So in this case, I'm going
to create this model.

00:04:51.480 --> 00:04:53.188
And with this model,
I'm just training it

00:04:53.188 --> 00:04:54.500
with six pairs of data.

00:04:54.500 --> 00:04:57.110
And then what it will
do is it will start then

00:04:57.110 --> 00:05:00.770
trying to infer the relationship
between these data points.

00:05:00.770 --> 00:05:02.600
And then from that,
going forward,

00:05:02.600 --> 00:05:05.450
it's a super simple model to
be able to do a temperature

00:05:05.450 --> 00:05:06.500
conversion.

00:05:06.500 --> 00:05:11.210
So how it's going to work is
it's going to make a guess.

00:05:11.210 --> 00:05:13.210
And this is how machine
learning actually works.

00:05:13.210 --> 00:05:16.430
It just makes a wild guess
as to what the relationship

00:05:16.430 --> 00:05:18.020
between these data is.

00:05:18.020 --> 00:05:20.180
And then it's got something
called a loss function.

00:05:20.180 --> 00:05:21.930
And what that loss
function is going to do

00:05:21.930 --> 00:05:26.390
is it's going to see how good or
how bad that guess actually is.

00:05:26.390 --> 00:05:28.730
And then based on the
data from the guess

00:05:28.730 --> 00:05:30.560
and the data from
the loss function,

00:05:30.560 --> 00:05:33.000
it then has an
optimizer, which is this.

00:05:33.000 --> 00:05:36.055
And what the optimizer does
is it creates another guess,

00:05:36.055 --> 00:05:38.180
and then it will measure
that guess to see how well

00:05:38.180 --> 00:05:39.120
or how badly it did.

00:05:39.120 --> 00:05:41.412
It will create another guess,
and it will measure that,

00:05:41.412 --> 00:05:44.330
and so on, and so on,
until I ask it to stop

00:05:44.330 --> 00:05:47.980
or until it does it 500 times,
which is what this line of code

00:05:47.980 --> 00:05:49.040
is actually doing.

00:05:49.040 --> 00:05:52.830
So if I'm going to create
this model quite simply,

00:05:52.830 --> 00:05:54.080
we'll see it's going to train.

00:05:54.080 --> 00:05:55.622
There was one that
I created earlier,

00:05:55.622 --> 00:05:59.510
so my workbook's taking
a moment to get running.

00:05:59.510 --> 00:06:01.247
My network
connection's gone down.

00:06:01.247 --> 00:06:03.930
Hang on.

00:06:03.930 --> 00:06:06.130
Let me refresh and reload.

00:06:06.130 --> 00:06:08.490
You love it when you dry run
a demo, and it works great.

00:06:11.770 --> 00:06:12.840
We get that warning.

00:06:12.840 --> 00:06:13.660
I'll run that.

00:06:13.660 --> 00:06:14.400
I'll do that.

00:06:14.400 --> 00:06:16.930
And now, it starts
training, hopefully.

00:06:16.930 --> 00:06:17.953
There we go.

00:06:17.953 --> 00:06:19.120
It's starting to train, now.

00:06:19.120 --> 00:06:21.100
It's going through
all these epochs.

00:06:21.100 --> 00:06:22.930
So it's going to
do that 500 times.

00:06:22.930 --> 00:06:24.790
And then at the end
of the 500 times,

00:06:24.790 --> 00:06:26.570
it's going to have
this trained model.

00:06:26.570 --> 00:06:28.112
And then this trained
model, I'm just

00:06:28.112 --> 00:06:29.530
going to ask it to predict.

00:06:29.530 --> 00:06:32.470
So for example, if I give
it 100 degrees centigrade,

00:06:32.470 --> 00:06:34.310
what's that going
to be in Fahrenheit?

00:06:34.310 --> 00:06:36.980
The real answer is
212, but it's going

00:06:36.980 --> 00:06:40.217
to give me 211 and
something, because this

00:06:40.217 --> 00:06:41.050
isn't very accurate.

00:06:41.050 --> 00:06:43.580
Because I've only trained
it on six points of data.

00:06:43.580 --> 00:06:45.910
So if you think about it,
there's a linear relationship

00:06:45.910 --> 00:06:49.570
between the data from Fahrenheit
to centigrade on those six,

00:06:49.570 --> 00:06:51.050
but the computer
doesn't know that.

00:06:51.050 --> 00:06:52.870
It doesn't go linearly forever.

00:06:52.870 --> 00:06:54.970
It could go like this,
or it could change.

00:06:54.970 --> 00:06:57.160
So it's giving me a
very high probability

00:06:57.160 --> 00:07:00.910
that for 100 degrees centigrade,
it would be 212 Fahrenheit.

00:07:00.910 --> 00:07:03.800
And that comes out as
211 degrees centigrade.

00:07:03.800 --> 00:07:05.300
So I've just built a model.

00:07:05.300 --> 00:07:07.390
And what we're going
to take a look at next

00:07:07.390 --> 00:07:09.672
is, how do I get that
model to work on mobile?

00:07:09.672 --> 00:07:11.380
Can we switch back to
the slides, please?

00:07:16.200 --> 00:07:18.520
So the process is pretty simple.

00:07:18.520 --> 00:07:20.550
The idea is like
using Keras or using

00:07:20.550 --> 00:07:21.690
an estimator in TensorFlow.

00:07:21.690 --> 00:07:22.780
You build a model.

00:07:22.780 --> 00:07:25.050
You then save that model
out in a file format

00:07:25.050 --> 00:07:26.560
called SavedModel.

00:07:26.560 --> 00:07:29.130
And in TensorFlow 2, we're
standardizing on that file

00:07:29.130 --> 00:07:32.970
format to make it easier for
us to be able to go across

00:07:32.970 --> 00:07:35.580
different types of
runtimes, like JavaScript,

00:07:35.580 --> 00:07:37.770
TFX in the web, or
TensorFlow Lite.

00:07:37.770 --> 00:07:40.560
By the way, the QR code on
this slide is to the workbook

00:07:40.560 --> 00:07:42.160
that I just showed a moment ago.

00:07:42.160 --> 00:07:44.790
So if you want to experiment
with that workbook

00:07:44.790 --> 00:07:47.040
for yourself, if you're just
learning, please go ahead

00:07:47.040 --> 00:07:47.760
and do so.

00:07:47.760 --> 00:07:50.340
It's a public URL, so feel
free to have fun with it.

00:07:50.340 --> 00:07:51.870
And I put a whole
bunch of QR codes

00:07:51.870 --> 00:07:53.400
in the rest of the slides.

00:07:53.400 --> 00:07:55.548
Now, once you've done
that, in TensorFlow Lite,

00:07:55.548 --> 00:07:57.840
there's something called the
TensorFlow Lite Converter.

00:07:57.840 --> 00:08:01.260
And that will convert
our SavedModel

00:08:01.260 --> 00:08:03.150
into a TensorFlow Lite model.

00:08:03.150 --> 00:08:04.860
So the process of
converting means

00:08:04.860 --> 00:08:06.300
it's going to shrink the model.

00:08:06.300 --> 00:08:09.030
It's going to optimize the model
for running on small devices,

00:08:09.030 --> 00:08:11.250
for running on devices
where battery life is

00:08:11.250 --> 00:08:13.170
a concern, and things like that.

00:08:13.170 --> 00:08:16.020
So out of that process, I
get a TensorFlow Lite model,

00:08:16.020 --> 00:08:18.380
which I can then run
on different devices.

00:08:18.380 --> 00:08:20.200
And here's the code
to actually do that.

00:08:20.200 --> 00:08:22.117
So we've got a little
bit of a breaking change

00:08:22.117 --> 00:08:24.660
between TensorFlow
1 and TensorFlow 2.

00:08:24.660 --> 00:08:27.300
So in the workbook that
was on that QR code,

00:08:27.300 --> 00:08:30.956
I've put both pieces of code on
how to create the SavedModel.

00:08:30.956 --> 00:08:33.539
And then once you've done that,
the third line from the bottom

00:08:33.539 --> 00:08:35.490
here is the TF Lite Converter.

00:08:35.490 --> 00:08:37.200
And all you have
to do is say here's

00:08:37.200 --> 00:08:39.059
the SavedModel directory.

00:08:39.059 --> 00:08:42.210
Run the TF Lite Converter from
SavedModel in that directory,

00:08:42.210 --> 00:08:45.790
and it will generate
a .tflite file for me.

00:08:45.790 --> 00:08:49.135
And that .tflite file is what
I can then use on mobile.

00:08:49.135 --> 00:08:50.760
So let's take a look
at that in action,

00:08:50.760 --> 00:08:52.260
if we can switch
back to the laptop.

00:08:55.570 --> 00:08:57.990
So all I'm going to do
within the same workbook

00:08:57.990 --> 00:09:00.570
is I'm going to run that
code that I just saw.

00:09:00.570 --> 00:09:03.900
And I'm using TensorFlow
1.x and Colab here.

00:09:03.900 --> 00:09:06.120
And we shall see
that it actually

00:09:06.120 --> 00:09:09.360
has saved out a model
for me in this directory.

00:09:09.360 --> 00:09:12.470
And I need that in the
next piece of code,

00:09:12.470 --> 00:09:16.910
because I have to tell it the
directory that it got saved to.

00:09:16.910 --> 00:09:19.760
So I'll just paste that in,
and then I'll run this out.

00:09:19.760 --> 00:09:21.830
And we can see the TF
Lite Converter is what

00:09:21.830 --> 00:09:23.760
will do the conversion for us.

00:09:23.760 --> 00:09:27.073
So if I run that, it
gives me the number 612.

00:09:27.073 --> 00:09:29.115
Can anybody guess why it
gives me the number 612?

00:09:32.300 --> 00:09:33.638
It's not an HTTP code.

00:09:33.638 --> 00:09:35.180
I thought it was
that, at first, too.

00:09:35.180 --> 00:09:37.320
That's actually just
the size of the model.

00:09:37.320 --> 00:09:41.210
So the model that I just trained
off those six pieces of data,

00:09:41.210 --> 00:09:44.570
when that got compiled
down, it's a 612 byte model.

00:09:44.570 --> 00:09:46.760
So if I go in there,
you can see I saved it

00:09:46.760 --> 00:09:49.890
in /tmp/model.tflite.

00:09:49.890 --> 00:09:54.110
And if in my Colab, if I go
and I look at /tmp directory,

00:09:54.110 --> 00:09:56.510
we'll see model.tflite is there.

00:09:56.510 --> 00:09:59.010
And I could download that
then to start using it

00:09:59.010 --> 00:10:00.682
in my mobile apps if I like.

00:10:00.682 --> 00:10:02.390
Can we switch back to
the slides, please?

00:10:06.060 --> 00:10:07.720
So now, we have the model.

00:10:07.720 --> 00:10:09.330
We've trained the model.

00:10:09.330 --> 00:10:10.480
Obviously, the models
you're going to train

00:10:10.480 --> 00:10:12.090
are hopefully a little bit
more complicated than the one

00:10:12.090 --> 00:10:13.140
that I did.

00:10:13.140 --> 00:10:15.392
You've been able to convert
that model to TF Lite.

00:10:15.392 --> 00:10:17.100
And now, what can you
do with that model,

00:10:17.100 --> 00:10:18.630
particularly on mobile?

00:10:18.630 --> 00:10:21.053
Well, there's three sets of
options that I want to cover.

00:10:21.053 --> 00:10:23.220
The first one, if you were
at the developer keynote,

00:10:23.220 --> 00:10:24.660
you probably saw ML Kit.

00:10:24.660 --> 00:10:27.090
And ML Kit is super cool.

00:10:27.090 --> 00:10:30.000
For me, in particular, it uses
the Firebase programming API.

00:10:30.000 --> 00:10:32.220
Any Firebase fans, here?

00:10:32.220 --> 00:10:34.140
Yeah, woo!

00:10:34.140 --> 00:10:36.330
The Firebase API
for programming,

00:10:36.330 --> 00:10:37.890
I find particularly cool.

00:10:37.890 --> 00:10:40.660
It's got a really
nice asynchronous API.

00:10:40.660 --> 00:10:43.030
And when you think about
it, when I'm using a model,

00:10:43.030 --> 00:10:44.980
I'm going to be passing
data to the model.

00:10:44.980 --> 00:10:46.320
The model is going to
run some inference,

00:10:46.320 --> 00:10:48.153
and it's going to send
something back to me.

00:10:48.153 --> 00:10:49.770
So it's perfect for
Firebase, and it's

00:10:49.770 --> 00:10:53.580
perfect for that asynchronous
API that Firebase gives us.

00:10:53.580 --> 00:10:55.980
If we don't want to use
Firebase-- and remember,

00:10:55.980 --> 00:10:58.470
Firebase ships with a bunch
of models that work out

00:10:58.470 --> 00:11:01.830
of the box for vision detection
and some of the AutoML

00:11:01.830 --> 00:11:03.880
stuff that we saw today.

00:11:03.880 --> 00:11:06.420
But you can also ship
your custom TF Lite model

00:11:06.420 --> 00:11:08.180
into Firebase if you want.

00:11:08.180 --> 00:11:12.025
But if you don't want to use
Firebase, or maybe your model

00:11:12.025 --> 00:11:14.400
is going to be deployed in a
country where Firebase isn't

00:11:14.400 --> 00:11:17.310
supported, or you want it
to work completely offline,

00:11:17.310 --> 00:11:20.240
and things like that, then the
idea is you can still deploy

00:11:20.240 --> 00:11:22.080
a model directly to your app.

00:11:22.080 --> 00:11:24.510
And I'm going to show a
TensorFlow Lite for that,

00:11:24.510 --> 00:11:27.540
and getting low level, and
using TensorFlow Lite directly

00:11:27.540 --> 00:11:29.830
instead of going through
the ML Kit wrapper.

00:11:29.830 --> 00:11:31.950
And then finally, there's
the mobile browser.

00:11:31.950 --> 00:11:33.540
So you can actually
deploy a model.

00:11:33.540 --> 00:11:35.490
You can convert it
to JSON, and you

00:11:35.490 --> 00:11:38.280
can deploy it to run, actually,
in a mobile browser, which

00:11:38.280 --> 00:11:39.660
I find pretty cool.

00:11:39.660 --> 00:11:42.150
So first, let's take
a look at ML Kit.

00:11:42.150 --> 00:11:46.260
So ML Kit is Google's solution
for Firebase developers

00:11:46.260 --> 00:11:48.090
and for mobile
developers who want

00:11:48.090 --> 00:11:52.290
to have machine learning models
running in their applications.

00:11:52.290 --> 00:11:55.110
Any ML Kit users
here, out of interest?

00:11:55.110 --> 00:11:55.790
Oh, not many.

00:11:55.790 --> 00:11:56.360
Wow.

00:11:56.360 --> 00:11:58.652
Well, you're in for a treat
if you haven't used it yet.

00:11:58.652 --> 00:12:01.278
Go check out the Firebase
booth, the Firebase sandbox.

00:12:01.278 --> 00:12:03.570
They got some really cool
stuff that you can play with.

00:12:03.570 --> 00:12:05.430
But just to show how
it works, the idea

00:12:05.430 --> 00:12:08.370
is that in the Firebase
console, you can either

00:12:08.370 --> 00:12:11.520
pick one of the preexisting
models that Firebase gives you,

00:12:11.520 --> 00:12:14.238
or you can upload the model
that you just created.

00:12:14.238 --> 00:12:15.780
So in Firebase,
you've got the option

00:12:15.780 --> 00:12:18.090
to say a custom model
I've uploaded-- here,

00:12:18.090 --> 00:12:20.310
you can see one that I
did a couple of weeks ago

00:12:20.310 --> 00:12:22.170
of this model that I uploaded.

00:12:22.170 --> 00:12:24.430
It's now in the
Firebase console,

00:12:24.430 --> 00:12:26.640
and I can use it
within my Firebase app.

00:12:26.640 --> 00:12:29.400
And I can use it alongside a lot
of the other Firebase goodies

00:12:29.400 --> 00:12:30.720
like analytics.

00:12:30.720 --> 00:12:33.090
Or a really cool
one is A/B testing.

00:12:33.090 --> 00:12:35.130
So I can have two
versions of my model.

00:12:35.130 --> 00:12:37.630
I could A/B test to
see which works best.

00:12:37.630 --> 00:12:39.630
Those kind of services
are available to Firebase

00:12:39.630 --> 00:12:42.330
developers, and when integrated
with machine learning,

00:12:42.330 --> 00:12:44.050
I find that makes
it pretty cool.

00:12:44.050 --> 00:12:45.540
And then once I've
done that, now,

00:12:45.540 --> 00:12:47.880
when I start building
my application,

00:12:47.880 --> 00:12:51.790
I do get all of the goodness of
the Firebase programming API.

00:12:51.790 --> 00:12:56.760
So if this is on Android, the
idea is with TensorFlow Lite,

00:12:56.760 --> 00:12:59.220
there's a TensorFlow
Lite runtime

00:12:59.220 --> 00:13:01.990
object that we'll often
call the interpreter.

00:13:01.990 --> 00:13:05.670
And here, you can see, I'm
just calling interpreter.run.

00:13:05.670 --> 00:13:07.590
I'm passing it my inputs.

00:13:07.590 --> 00:13:09.720
So in this case, if it's
Fahrenheit to centigrade

00:13:09.720 --> 00:13:12.090
conversion, I'm just
going to pass it a float.

00:13:12.090 --> 00:13:16.253
And then in its
onSuccessListener,

00:13:16.253 --> 00:13:18.420
it's going to give me a
call back when the model has

00:13:18.420 --> 00:13:19.453
finished executing.

00:13:19.453 --> 00:13:21.120
So it's really nice
in the sense that it

00:13:21.120 --> 00:13:22.542
can be very asynchronous.

00:13:22.542 --> 00:13:24.000
If you have a really
big model that

00:13:24.000 --> 00:13:26.590
might take a long time to
run, instead of you locking up

00:13:26.590 --> 00:13:29.640
your UI thread, it's going to be
working nice and asynchronously

00:13:29.640 --> 00:13:31.200
through ML Kit.

00:13:31.200 --> 00:13:34.800
So in my addOnSuccessListener,
I'm adding a SuccessListener.

00:13:34.800 --> 00:13:37.170
It's going to give me a
call back with the results.

00:13:37.170 --> 00:13:39.240
And then that
result, I can parse

00:13:39.240 --> 00:13:42.703
to get my output from
the machine learn model.

00:13:42.703 --> 00:13:44.120
And it's really
as simple as that.

00:13:44.120 --> 00:13:46.110
And in this case, I'm
passing it in a float.

00:13:46.110 --> 00:13:47.640
It's converting the temperature.

00:13:47.640 --> 00:13:49.240
It's sending a float back to me.

00:13:49.240 --> 00:13:51.720
And that's why my getOutput
array is a float array

00:13:51.720 --> 00:13:53.520
with a single element in it.

00:13:53.520 --> 00:13:55.950
That's one thing if you haven't
worked in machine learning

00:13:55.950 --> 00:13:58.710
and if you haven't built machine
learning models before, one

00:13:58.710 --> 00:14:00.510
of the things that
you'll encounter a lot

00:14:00.510 --> 00:14:02.220
is that when you're
passing data in,

00:14:02.220 --> 00:14:04.290
you pass data in as tensors.

00:14:04.290 --> 00:14:06.660
But when you are mapping
those tensors to a high level

00:14:06.660 --> 00:14:09.090
programming language,
like Java or Kotlin,

00:14:09.090 --> 00:14:10.303
you tend to use arrays.

00:14:10.303 --> 00:14:11.970
And when it's passing
stuff back to you,

00:14:11.970 --> 00:14:13.567
it's passing back a tensor.

00:14:13.567 --> 00:14:15.150
And again, they tend
to map to arrays,

00:14:15.150 --> 00:14:19.570
and that's why in the code
here, you're seeing arrays.

00:14:19.570 --> 00:14:20.580
So iOS.

00:14:20.580 --> 00:14:23.030
Any iOS fans, here?

00:14:23.030 --> 00:14:23.870
Oh, a few.

00:14:23.870 --> 00:14:24.950
Hey, nobody booed.

00:14:24.950 --> 00:14:26.232
You said they would boo.

00:14:26.232 --> 00:14:28.700
[CHUCKLING]

00:14:28.700 --> 00:14:30.180
So in iOS, it also works.

00:14:30.180 --> 00:14:33.200
So for example, again, I
have my interpreter in iOS.

00:14:33.200 --> 00:14:35.000
This is Swift code.

00:14:35.000 --> 00:14:37.490
I'll call the .run
method on my interpreter.

00:14:37.490 --> 00:14:41.090
I'll pass it the inputs, and
I will get the outputs back.

00:14:41.090 --> 00:14:43.140
And again, in this
very simple model,

00:14:43.140 --> 00:14:45.300
I'm just getting a
single value back.

00:14:45.300 --> 00:14:47.775
So it's just my outputs at
index 0 I'm going to read.

00:14:47.775 --> 00:14:49.400
If you're doing
something more complex,

00:14:49.400 --> 00:14:51.108
your data in and your
data out structures

00:14:51.108 --> 00:14:53.390
are going to be a bit
more complex than this.

00:14:53.390 --> 00:14:54.980
But as Daniel
mentioned earlier on,

00:14:54.980 --> 00:14:56.780
we have a bunch of
sample applications

00:14:56.780 --> 00:14:59.270
that you can dissect to take
a look at how they actually

00:14:59.270 --> 00:15:00.860
do it.

00:15:00.860 --> 00:15:02.120
So that's ML Kit.

00:15:02.120 --> 00:15:04.510
And that's a rough
look at how it

00:15:04.510 --> 00:15:07.450
can work with the custom
models that you build

00:15:07.450 --> 00:15:09.430
and convert to run
in TensorFlow Lite.

00:15:09.430 --> 00:15:12.920
But let's take a look at the
TensorFlow Lite runtime itself.

00:15:12.920 --> 00:15:15.520
So now, if I'm building
an Android application,

00:15:15.520 --> 00:15:17.500
and I've built my
model, and I don't

00:15:17.500 --> 00:15:20.340
want to depend on an exterior
service like the Firebase

00:15:20.340 --> 00:15:22.690
service to deploy
the model for me,

00:15:22.690 --> 00:15:24.700
I want to bundle the
model with my app.

00:15:24.700 --> 00:15:27.270
And then, however, the user
gets the app, via the Play Store

00:15:27.270 --> 00:15:30.310
or via other means, the
model is a part of that.

00:15:30.310 --> 00:15:32.120
Then it's very easy
for me to do that.

00:15:32.120 --> 00:15:36.070
So that .tflite file that I
created earlier on, all I have

00:15:36.070 --> 00:15:40.090
to do is put that in my assets
folder in Android as an asset,

00:15:40.090 --> 00:15:42.420
just like any other-- like
any image, or any JPEG,

00:15:42.420 --> 00:15:43.770
or any of those kind of things.

00:15:43.770 --> 00:15:46.030
It's just an asset.

00:15:46.030 --> 00:15:48.008
But the one thing
that's really important,

00:15:48.008 --> 00:15:50.050
and it's the number one
bug that most people will

00:15:50.050 --> 00:15:52.240
hit when they first
start doing this,

00:15:52.240 --> 00:15:56.920
is that when Android deploys
your app to the device

00:15:56.920 --> 00:15:58.600
to run it, it will zip up.

00:15:58.600 --> 00:16:01.120
It will compress everything
in the Assets folder.

00:16:01.120 --> 00:16:03.940
The model will not work
if it is compressed.

00:16:03.940 --> 00:16:05.620
It has to be uncompressed.

00:16:05.620 --> 00:16:09.610
So when you build out Gradle,
you just specify aaptOptions.

00:16:09.610 --> 00:16:12.430
You say noCompress
"tflite", and then it

00:16:12.430 --> 00:16:14.680
won't compress the
tflite file for you.

00:16:14.680 --> 00:16:16.960
And then you'll be able to
run it and do inference.

00:16:16.960 --> 00:16:19.790
So many times, I've worked with
people building their first TF

00:16:19.790 --> 00:16:22.295
Lite application, it
failed unloading it

00:16:22.295 --> 00:16:23.170
into the interpreter.

00:16:23.170 --> 00:16:24.200
They had no idea why.

00:16:24.200 --> 00:16:26.120
And it's they've forgotten
to put this line.

00:16:26.120 --> 00:16:27.740
So if you only take one
thing away from this talk,

00:16:27.740 --> 00:16:29.565
take this slide
away, because it will

00:16:29.565 --> 00:16:30.940
solve a lot of
your problems when

00:16:30.940 --> 00:16:33.420
you get started with TF Lite.

00:16:33.420 --> 00:16:35.940
Then of course, still
in build.gradle,

00:16:35.940 --> 00:16:37.920
all you have to do is,
in your dependencies,

00:16:37.920 --> 00:16:41.622
you add the implementation of
the TensorFlow-lite runtime.

00:16:41.622 --> 00:16:43.080
And what that's
going to do is it's

00:16:43.080 --> 00:16:45.380
going to give you the latest
version of TensorFlow Lite,

00:16:45.380 --> 00:16:47.547
and then that will give you
the interpreter that you

00:16:47.547 --> 00:16:48.070
can use it.

00:16:48.070 --> 00:16:49.350
And this QR code is a link.

00:16:49.350 --> 00:16:50.850
I've put the full
app that I'm going

00:16:50.850 --> 00:16:53.670
to show in a moment on GitHub,
so you can go and have a play

00:16:53.670 --> 00:16:57.350
and hack around
with it if you like.

00:16:57.350 --> 00:17:00.330
So now, if I want to
actually do inference--

00:17:00.330 --> 00:17:02.480
so this is Kotlin code.

00:17:02.480 --> 00:17:04.986
And there's a few things
to take a look at here

00:17:04.986 --> 00:17:07.069
on how you'll do inference
and how you'll actually

00:17:07.069 --> 00:17:09.690
be able to get your model up
and running to begin with.

00:17:09.690 --> 00:17:12.920
So first of all, there are two
things that I'm declaring here.

00:17:12.920 --> 00:17:15.230
Remember earlier, we tend
to use the term interpreter

00:17:15.230 --> 00:17:16.730
for the TF Lite runtime.

00:17:16.730 --> 00:17:20.030
So I'm creating a TF
Lite object, which

00:17:20.030 --> 00:17:21.638
I'm going to call interpreter.

00:17:21.638 --> 00:17:23.930
Sorry, I'm going to create
an interpreter object, which

00:17:23.930 --> 00:17:25.490
I'm going to call TF Lite.

00:17:25.490 --> 00:17:28.490
And then I'm going to create a
MappedByteBuffer object, which

00:17:28.490 --> 00:17:30.200
is the TF Lite model.

00:17:30.200 --> 00:17:32.600
Now earlier, remember, I
said you put the TF Lite

00:17:32.600 --> 00:17:34.610
model into your Assets folder.

00:17:34.610 --> 00:17:37.250
How you read it out
of the Assets folder

00:17:37.250 --> 00:17:39.170
is as a MappedByteBuffer.

00:17:39.170 --> 00:17:41.480
I'm not going to show the
code for that in the slides,

00:17:41.480 --> 00:17:42.938
but it's available
on the download,

00:17:42.938 --> 00:17:44.900
if you want have a look
at it for yourself.

00:17:44.900 --> 00:17:47.810
And then you're also going to
need a TF Lite Options object.

00:17:47.810 --> 00:17:49.460
And that TF Lite
Options object is

00:17:49.460 --> 00:17:51.590
used to set things like
the number of threads

00:17:51.590 --> 00:17:54.000
that you wanted to execute on.

00:17:54.000 --> 00:17:55.610
So now, to
instantiate your model

00:17:55.610 --> 00:17:58.560
so that you can start using
it, it's as easy as this.

00:17:58.560 --> 00:18:01.280
So first of all, I'm going to
call a loadModelfile function.

00:18:01.280 --> 00:18:02.840
That loadModelfile
function is what

00:18:02.840 --> 00:18:06.620
reads the TF Lite model
out of the Assets folder

00:18:06.620 --> 00:18:07.952
as a MappedByteBuffer.

00:18:07.952 --> 00:18:09.410
And it gives me my
MappedByteBuffer

00:18:09.410 --> 00:18:10.910
called tflitemodel.

00:18:10.910 --> 00:18:12.860
In my options, I'm going
to say, for example,

00:18:12.860 --> 00:18:14.960
I just want this to
run on one thread.

00:18:14.960 --> 00:18:18.680
And then when I instantiate
my interpreter like this,

00:18:18.680 --> 00:18:21.290
by giving it that
MappedByteBuffer of the model

00:18:21.290 --> 00:18:24.560
and giving it the options,
I now have an interpreter

00:18:24.560 --> 00:18:27.230
that I can run inference
on in Android itself.

00:18:27.230 --> 00:18:28.880
And what does the
inference look like?

00:18:28.880 --> 00:18:31.450
It will look
something like this.

00:18:31.450 --> 00:18:33.060
So remember earlier,
when I mentioned

00:18:33.060 --> 00:18:36.190
a neural network takes in a
number of inputs as tensors,

00:18:36.190 --> 00:18:38.770
it gives you a number
of outputs as tensors.

00:18:38.770 --> 00:18:42.460
Those tensors, in a higher level
language like Kotlin or Java

00:18:42.460 --> 00:18:44.800
or Swift, will map to arrays.

00:18:44.800 --> 00:18:48.010
So even though I'm
feeding in a single float,

00:18:48.010 --> 00:18:50.120
I have to feed that
in as an array.

00:18:50.120 --> 00:18:51.880
So that's why here,
my input value

00:18:51.880 --> 00:18:54.800
is a float array with
a single value in it.

00:18:54.800 --> 00:18:57.190
So if I want to convert
100, for example,

00:18:57.190 --> 00:18:59.820
that's going to be a float array
with a single value containing

00:18:59.820 --> 00:19:00.590
100.

00:19:00.590 --> 00:19:03.300
And that F is for float,
not for Fahrenheit.

00:19:03.300 --> 00:19:05.050
When I was rehearsing
these slides before,

00:19:05.050 --> 00:19:06.760
somebody was like, oh,
how'd you put Fahrenheit

00:19:06.760 --> 00:19:07.700
into code like that.

00:19:07.700 --> 00:19:08.590
But it's a float.

00:19:08.590 --> 00:19:10.413
It's not Fahrenheit.

00:19:10.413 --> 00:19:11.830
And then when I'm
reading, we have

00:19:11.830 --> 00:19:13.780
to get down a little
low level here,

00:19:13.780 --> 00:19:16.870
because the model's going to
send me out a stream of bytes.

00:19:16.870 --> 00:19:21.430
I know that those bytes map to
a float, but Kotlin doesn't.

00:19:21.430 --> 00:19:23.230
Java doesn't.

00:19:23.230 --> 00:19:26.350
So those stream of bytes, I
know they're mapping to a float.

00:19:26.350 --> 00:19:29.890
And a float has 4 bytes, so I'm
going to create a byte buffer.

00:19:29.890 --> 00:19:32.590
I'm going to allocate 4
bytes to that byte buffer,

00:19:32.590 --> 00:19:35.080
and I'm just going to set
its order to be native order.

00:19:35.080 --> 00:19:36.622
Because there's
different orders like

00:19:36.622 --> 00:19:38.200
big-endian and little-endian.

00:19:38.200 --> 00:19:41.710
But when you're using TF Lite,
always just use native order.

00:19:41.710 --> 00:19:45.280
And then to do my inference,
I call tflite.run.

00:19:45.280 --> 00:19:46.810
I give it my input value.

00:19:46.810 --> 00:19:48.330
I give it my output value.

00:19:48.330 --> 00:19:49.780
It'll read from the input value.

00:19:49.780 --> 00:19:51.830
It'll write to the output file.

00:19:51.830 --> 00:19:55.120
And then on the output file,
if I want to get my prediction,

00:19:55.120 --> 00:19:56.770
it's written those 4 bytes.

00:19:56.770 --> 00:19:59.285
I have to rewind them, and
then I'm going to read a float.

00:19:59.285 --> 00:20:00.910
And what Kotlin will
do is say, OK, I'm

00:20:00.910 --> 00:20:03.320
taking those 4 bytes
out of that buffer.

00:20:03.320 --> 00:20:05.510
And I'm going to give you
back a float from them.

00:20:05.510 --> 00:20:07.540
So that's how I would
do an inference.

00:20:07.540 --> 00:20:11.290
It seems very complex for a
very simple task, like float in,

00:20:11.290 --> 00:20:14.380
float out, but the structure
is the same regardless

00:20:14.380 --> 00:20:18.337
of how complex your input is
and how complex your output is.

00:20:18.337 --> 00:20:20.920
So while this might seem to be
the 20 pound hammer for the one

00:20:20.920 --> 00:20:22.990
pound nail, it's
also the same hammer

00:20:22.990 --> 00:20:26.070
when you have a 20 pound nail.

00:20:26.070 --> 00:20:29.020
So that was Android,
iOS is very similar.

00:20:29.020 --> 00:20:33.220
So in iOS, all I have to do is I
put my model in my application.

00:20:33.220 --> 00:20:35.420
So I just put my TF Lite model.

00:20:35.420 --> 00:20:37.540
It's an asset like any other.

00:20:37.540 --> 00:20:40.080
And then in code, first
of all, I create a pod.

00:20:40.080 --> 00:20:44.980
And in my pod file, I'll have
a pod for TensorFlow Lite.

00:20:44.980 --> 00:20:48.480
I've spoken at I/O for
the last five years,

00:20:48.480 --> 00:20:50.808
and this is my first time
ever showing C++ code.

00:20:50.808 --> 00:20:52.350
I'm kind of geeking
out a little bit.

00:20:55.500 --> 00:20:57.960
Right now, it supports
objective C++.

00:20:57.960 --> 00:21:00.750
We do have a Swift wrapper
in some of our sample

00:21:00.750 --> 00:21:02.910
applications, but the
Swift wrapper, right now,

00:21:02.910 --> 00:21:04.260
only works in a few scenarios.

00:21:04.260 --> 00:21:06.180
We're working at
generalizing that.

00:21:06.180 --> 00:21:09.600
So for now, I'm just
going to show C++ code.

00:21:09.600 --> 00:21:11.870
Any C++ fans, here?

00:21:11.870 --> 00:21:12.390
Oh, wow.

00:21:12.390 --> 00:21:13.223
More than I thought.

00:21:13.223 --> 00:21:14.130
Nice.

00:21:14.130 --> 00:21:17.340
So then your C++ code, it's
exactly the same model as I was

00:21:17.340 --> 00:21:18.240
just showing.

00:21:18.240 --> 00:21:21.000
So first of all, I'm going
to create an interpreter,

00:21:21.000 --> 00:21:22.920
and I'm going to call
that interpreter.

00:21:22.920 --> 00:21:25.560
And now, I'm going to
create two buffers.

00:21:25.560 --> 00:21:27.470
So these are buffers
of unsigned ints.

00:21:27.470 --> 00:21:30.570
One buffer is my input
buffer that I call ibuffer.

00:21:30.570 --> 00:21:33.322
The other buffer is my output
buffer that I call obuffer.

00:21:33.322 --> 00:21:34.780
And both of these,
the interpreter,

00:21:34.780 --> 00:21:37.440
I'm just going to say, hey,
use a typed_tensor for these.

00:21:37.440 --> 00:21:38.250
So that's my input.

00:21:38.250 --> 00:21:39.520
That's my output.

00:21:39.520 --> 00:21:42.090
And when I tflite.run, it's
going to read from the input,

00:21:42.090 --> 00:21:43.050
write to the output.

00:21:43.050 --> 00:21:46.620
Now, I have an output buffer,
and I can just get my inference

00:21:46.620 --> 00:21:48.540
back from that output buffer.

00:21:48.540 --> 00:21:52.740
So that was a quick
tour of TF Lite, how

00:21:52.740 --> 00:21:56.550
you can build your model,
save it as a TF Lite,

00:21:56.550 --> 00:21:58.330
and I forgot to show--

00:21:58.330 --> 00:21:58.830
oh, no.

00:21:58.830 --> 00:22:01.620
I did show, sorry, where you can
actually download as a TF Lite.

00:22:01.620 --> 00:22:03.900
But I can demo it now
running an Android.

00:22:03.900 --> 00:22:07.190
So if we can switch
back to the laptop?

00:22:07.190 --> 00:22:09.300
So I'm going to go
to Android Studio.

00:22:09.300 --> 00:22:11.210
And I've tried to make
the font big enough.

00:22:11.210 --> 00:22:13.640
We can all see the font.

00:22:13.640 --> 00:22:15.870
And let me just scroll
that down a little bit.

00:22:15.870 --> 00:22:18.680
So this was the Kotlin code
that I showed a moment ago,

00:22:18.680 --> 00:22:21.350
and the simplest possible
application that I could build

00:22:21.350 --> 00:22:22.340
is this one.

00:22:22.340 --> 00:22:24.260
So it's a simple
Android application.

00:22:24.260 --> 00:22:25.370
It's got one button on it.

00:22:25.370 --> 00:22:27.290
That button says do inference.

00:22:27.290 --> 00:22:28.790
When you push that,
it's hard code.

00:22:28.790 --> 00:22:30.890
It'll pass 100 to the
model and get back

00:22:30.890 --> 00:22:32.450
the response for the model.

00:22:32.450 --> 00:22:33.620
I have it in debug mode.

00:22:33.620 --> 00:22:34.940
I have some breakpoints set.

00:22:34.940 --> 00:22:37.678
So let's take a look
at what happens.

00:22:37.678 --> 00:22:40.220
So once I click Do Inference,
I'm hitting this breakpoint now

00:22:40.220 --> 00:22:41.630
in Android Studio.

00:22:41.630 --> 00:22:43.770
And I've set up
my input inputVal,

00:22:43.770 --> 00:22:47.050
and we can see my inputVal
is containing just 100.

00:22:47.050 --> 00:22:51.140
And if I step over, my
outputVal has been set up.

00:22:51.140 --> 00:22:53.000
It's a direct byte buffer.

00:22:53.000 --> 00:22:54.260
Right now, position is 0.

00:22:54.260 --> 00:22:56.200
Its capacity is 4.

00:22:56.200 --> 00:22:58.220
I'm going to set its
order, and then I'm

00:22:58.220 --> 00:23:01.080
going to pass the
inputVal containing

00:23:01.080 --> 00:23:03.680
100, the outputVal,
which is my empty 4 byte

00:23:03.680 --> 00:23:05.930
buffer, to tflite.run.

00:23:05.930 --> 00:23:09.560
Execute that, and the TF Lite
interpreter has done its job,

00:23:09.560 --> 00:23:12.430
and it's written
back to my outputVal.

00:23:12.430 --> 00:23:14.270
But I can't read that yet.

00:23:14.270 --> 00:23:16.460
Remember earlier,
the position was 1.

00:23:16.460 --> 00:23:17.150
The limit was 4.

00:23:17.150 --> 00:23:18.290
The capacity was 4.

00:23:18.290 --> 00:23:19.445
It's written to it, now.

00:23:19.445 --> 00:23:21.170
So that buffer is full.

00:23:21.170 --> 00:23:26.030
So when I rewind,
now, we can see

00:23:26.030 --> 00:23:28.100
my position has gone back to 0.

00:23:28.100 --> 00:23:30.350
So I know I can start
reading from that buffer.

00:23:30.350 --> 00:23:32.770
So I'm going to say
outputVal.getFloat,

00:23:32.770 --> 00:23:36.590
and we'll see the prediction
that comes back is that 211.31.

00:23:36.590 --> 00:23:40.280
So that model has been wrapped
by the TF Lite runtime.

00:23:40.280 --> 00:23:41.720
I've given it the input buffer.

00:23:41.720 --> 00:23:43.137
I've given that
the output buffer.

00:23:43.137 --> 00:23:45.650
I've executed it, and it's
given me back that code.

00:23:45.650 --> 00:23:48.920
And actually, there's one
really cool Kotlin language

00:23:48.920 --> 00:23:51.455
feature that I want
to demonstrate, here.

00:23:51.455 --> 00:23:53.330
I don't know if anybody
has seen this before.

00:23:53.330 --> 00:23:55.122
This might be the first
time we've actually

00:23:55.122 --> 00:23:56.700
ever shown this on stage.

00:23:56.700 --> 00:23:58.720
But if I want to run
this model again,

00:23:58.720 --> 00:24:00.870
you'll notice that there
are line numbers here.

00:24:00.870 --> 00:24:02.460
All I have to do
is type goto 50.

00:24:07.550 --> 00:24:09.860
I'm seeing who's still awake.

00:24:09.860 --> 00:24:11.270
Of course, it's gosub 50.

00:24:14.980 --> 00:24:17.400
So that's just a very
quick and very simple

00:24:17.400 --> 00:24:21.430
example of how this
would run in Android.

00:24:21.430 --> 00:24:23.770
And again, that
sample is online.

00:24:23.770 --> 00:24:24.415
It's on GitHub.

00:24:24.415 --> 00:24:26.790
I've put it on GitHub so that
we can have a play with it.

00:24:26.790 --> 00:24:30.000
All right, if we can
switch back to the slides?

00:24:30.000 --> 00:24:32.510
So the third of the options
that I had mentioned-- first

00:24:32.510 --> 00:24:33.290
was ML Kit.

00:24:33.290 --> 00:24:34.705
Second was to TF Lite.

00:24:34.705 --> 00:24:36.080
The third of the
options was then

00:24:36.080 --> 00:24:38.480
to be able to use JavaScript
and to be able to run

00:24:38.480 --> 00:24:40.890
your model in a browser.

00:24:40.890 --> 00:24:43.270
So TensorFlow.js is your friend.

00:24:43.270 --> 00:24:45.990
So the idea is that
with TensorFlow.js,

00:24:45.990 --> 00:24:48.360
in your Python, when
you're building the model,

00:24:48.360 --> 00:24:51.360
you've PIP installed a
library called TensorFlow.js.

00:24:51.360 --> 00:24:53.070
And then that
gives you a command

00:24:53.070 --> 00:24:55.240
called TensorFlow.js converter.

00:24:55.240 --> 00:24:57.570
With TensorFlow.js
converter, if you'd

00:24:57.570 --> 00:25:00.360
saved that as a saved model,
as we showed earlier on,

00:25:00.360 --> 00:25:02.920
you just say, hey, my input
format's a SavedModel.

00:25:02.920 --> 00:25:05.010
Here's the directory
the SavedModel is in.

00:25:05.010 --> 00:25:08.170
Here's the directory I
want you to write it to.

00:25:08.170 --> 00:25:09.617
So now, once it's
done that, it's

00:25:09.617 --> 00:25:11.200
actually going to
take that SavedModel

00:25:11.200 --> 00:25:13.880
and convert that
into a JSON object.

00:25:13.880 --> 00:25:17.230
So now, in a super, super simple
web page-- and this QR code,

00:25:17.230 --> 00:25:18.890
again, has that web page--

00:25:18.890 --> 00:25:20.530
now, all I have to
do is say, here's

00:25:20.530 --> 00:25:23.050
the URL of that model.json.

00:25:23.050 --> 00:25:27.880
And I will say const model
= await tf.loadLayersModel,

00:25:27.880 --> 00:25:29.350
giving it that URL.

00:25:29.350 --> 00:25:32.160
So if you're using
TensorFlow.js in your browser

00:25:32.160 --> 00:25:34.840
with that script tag right
at the top of the page, now,

00:25:34.840 --> 00:25:38.020
that model is loaded from
a JSON serialization.

00:25:38.020 --> 00:25:39.790
And I can start
running inference

00:25:39.790 --> 00:25:41.750
on that model in the browser.

00:25:41.750 --> 00:25:43.510
So here was how I would use it.

00:25:43.510 --> 00:25:46.530
Again, I'm setting up my inputs.

00:25:46.530 --> 00:25:48.277
And JavaScript,
you know earlier,

00:25:48.277 --> 00:25:50.110
I was saying you're
going to pass in arrays,

00:25:50.110 --> 00:25:53.155
and you get out arrays, except
in a high level language?

00:25:53.155 --> 00:25:54.280
Sorry, you pass in tensors.

00:25:54.280 --> 00:25:55.240
You get out tensors.

00:25:55.240 --> 00:25:57.360
High level language tends
to wrap them in arrays.

00:25:57.360 --> 00:26:00.850
TensorFlow.js actually gives
you a tensor 2D object,

00:26:00.850 --> 00:26:02.260
and that's what I'm using here.

00:26:02.260 --> 00:26:05.420
So the tensor 2D object
takes two parameters.

00:26:05.420 --> 00:26:08.230
The first parameter is the
array that you want to pass in,

00:26:08.230 --> 00:26:10.570
and you can see here that
array is just the value 10.

00:26:10.570 --> 00:26:12.610
It's a single item array.

00:26:12.610 --> 00:26:16.370
And then the second parameter
is the shape of that array.

00:26:16.370 --> 00:26:18.660
So here, the first
parameter is a 10.

00:26:18.660 --> 00:26:20.122
The second parameter is 1, 1.

00:26:20.122 --> 00:26:21.580
And that's the
shape of that array.

00:26:21.580 --> 00:26:23.590
It's just a 1 by 1 array.

00:26:23.590 --> 00:26:26.140
So once I've done that,
and I have my input, now,

00:26:26.140 --> 00:26:29.260
if I want to run inference using
the model, all I have to do

00:26:29.260 --> 00:26:30.950
is say model.predictinput.

00:26:30.950 --> 00:26:32.710
And it will give
me back my results.

00:26:32.710 --> 00:26:34.820
In this case, I was
alerting the results.

00:26:34.820 --> 00:26:36.790
But in my demo, I'm
actually going to write it.

00:26:36.790 --> 00:26:38.498
So if we can switch
back to the demo box?

00:26:42.030 --> 00:26:46.100
And I have that super simple
web page hosted on the web.

00:26:46.100 --> 00:26:48.972
And I've put that model in
there, and it's going to run.

00:26:48.972 --> 00:26:50.430
This is a slightly
different model.

00:26:50.430 --> 00:26:52.180
I'll show training
that model in a moment.

00:26:52.180 --> 00:26:55.065
This was just the model
where y equals 2x minus 1.

00:26:55.065 --> 00:26:58.290
So I'm doing an inference
where x equals 10.

00:26:58.290 --> 00:27:01.450
And if x equals 10, y equals
2x minus 1 will give you 19.

00:27:01.450 --> 00:27:03.450
And when I train the model
on six items of data,

00:27:03.450 --> 00:27:05.550
it says 18.97.

00:27:05.550 --> 00:27:11.460
So again, all I do is, in
Python, I can train the model.

00:27:11.460 --> 00:27:14.160
With TensorFlow.js, I can
then convert that model

00:27:14.160 --> 00:27:15.720
to a JSON object.

00:27:15.720 --> 00:27:19.370
And then in TensorFlow.js, I
can instantiate a model off

00:27:19.370 --> 00:27:21.660
of that JSON and then
start doing predictions

00:27:21.660 --> 00:27:22.588
in that model.

00:27:22.588 --> 00:27:24.880
If we can switch back to the
demo machine for a moment?

00:27:24.880 --> 00:27:25.080
Oh, no.

00:27:25.080 --> 00:27:26.880
I'm still on the demo
machine, aren't I?

00:27:26.880 --> 00:27:31.835
I can show that in
action in a notebook.

00:27:34.830 --> 00:27:35.670
I lost my notebook.

00:27:41.620 --> 00:27:44.470
So this notebook
is also available,

00:27:44.470 --> 00:27:46.470
where I gave those QR codes.

00:27:46.470 --> 00:27:48.670
And this notebook, again,
is a very similar one

00:27:48.670 --> 00:27:51.460
to the one I showed earlier
on super, super simple neural

00:27:51.460 --> 00:27:53.917
network, single layer
with a single neuron.

00:27:53.917 --> 00:27:55.750
I'm not going to step
through all of it now.

00:27:55.750 --> 00:27:59.480
But the idea is if you PIP
install TensorFlow.js right now

00:27:59.480 --> 00:28:03.230
in Google Colab, it will upgrade
Google Colab from TensorFlow

00:28:03.230 --> 00:28:06.140
1.13 to TensorFlow 2.

00:28:06.140 --> 00:28:08.530
So if you run through
this and install that,

00:28:08.530 --> 00:28:09.890
you'll see that happening.

00:28:09.890 --> 00:28:13.790
And then once you have
TensorFlow 2 on your machine,

00:28:13.790 --> 00:28:17.290
then you can use the
TensorFlow.js converter,

00:28:17.290 --> 00:28:19.990
as shown here, giving
it the input format

00:28:19.990 --> 00:28:22.882
and giving it the SavedModel
from the directory

00:28:22.882 --> 00:28:23.840
as I'd done earlier on.

00:28:23.840 --> 00:28:26.620
And it will write
out to /temp/linear.

00:28:26.620 --> 00:28:28.390
The one thing to
take note of, though,

00:28:28.390 --> 00:28:31.990
if you are doing this yourself
is that when it writes to that,

00:28:31.990 --> 00:28:34.240
it won't just write
the JSON file.

00:28:34.240 --> 00:28:36.740
It also writes a binary file.

00:28:36.740 --> 00:28:39.790
So when you upload the JSON
file to the web server--

00:28:39.790 --> 00:28:43.270
to be able to create a model
off of that JSON file, make sure

00:28:43.270 --> 00:28:46.690
the binary file is in the same
directory as the JSON file.

00:28:46.690 --> 00:28:50.230
Or the model off
of the JSON file

00:28:50.230 --> 00:28:52.300
is going to give you some
really weird results.

00:28:52.300 --> 00:28:54.730
That's also the number one
bug that I found when people

00:28:54.730 --> 00:28:57.190
have been using TensorFlow.js.

00:28:57.190 --> 00:28:59.500
It's that they will
convert to the JSON file.

00:28:59.500 --> 00:29:01.000
They'll upload it
to their server.

00:29:01.000 --> 00:29:03.760
They'll have no idea what
that random binary file was,

00:29:03.760 --> 00:29:06.040
and they're getting
indeterminate results back

00:29:06.040 --> 00:29:06.850
from that model.

00:29:06.850 --> 00:29:09.658
So make sure when you do
that, you get that model.

00:29:09.658 --> 00:29:11.950
I don't know if I have one
here that I prepared earlier

00:29:11.950 --> 00:29:14.750
that I can show you
what it looks like.

00:29:14.750 --> 00:29:15.250
I don't.

00:29:15.250 --> 00:29:16.288
It's empty, right now.

00:29:16.288 --> 00:29:18.080
But when you run this
and you write it out,

00:29:18.080 --> 00:29:21.380
you'll see that the model.json
and a binary file are there.

00:29:21.380 --> 00:29:24.742
Make sure you upload
both of them to use it.

00:29:24.742 --> 00:29:26.450
Can we switch back to
the slides, please?

00:29:29.650 --> 00:29:31.450
So that was a quick
summary, where

00:29:31.450 --> 00:29:35.260
we saw that model that you build
using Python can be converted

00:29:35.260 --> 00:29:36.537
to TensorFlow Lite.

00:29:36.537 --> 00:29:37.870
You can save it as a SavedModel.

00:29:37.870 --> 00:29:40.330
You can convert it to TensorFlow
Lite and use it in ML Kit,

00:29:40.330 --> 00:29:43.190
or use it directly in
TensorFlow Lite itself.

00:29:43.190 --> 00:29:45.190
Alternatively, if you
want to convert it to .js,

00:29:45.190 --> 00:29:46.905
then it will save it
out as a JSON file.

00:29:46.905 --> 00:29:48.280
You can convert
it to a JSON file

00:29:48.280 --> 00:29:50.020
and then use that in JavaScript.

00:29:50.020 --> 00:29:51.880
So that's the summary
of being able to use

00:29:51.880 --> 00:29:53.662
those models on mobile devices.

00:29:53.662 --> 00:29:55.120
But now, Daniel is
going to tell us

00:29:55.120 --> 00:29:57.155
all about going beyond
phones and the web.

00:29:57.155 --> 00:29:58.030
So thank you, Daniel.

00:29:58.030 --> 00:29:58.190
Thank you.

00:29:58.190 --> 00:29:59.815
DANIEL SITUNAYAKE:
Thank you, Laurence.

00:29:59.815 --> 00:30:02.596
[APPLAUSE]

00:30:07.020 --> 00:30:07.620
Awesome.

00:30:07.620 --> 00:30:10.950
So like Lawrence says so far,
we've talked about phones.

00:30:10.950 --> 00:30:14.230
But these aren't the only
devices that we use every day.

00:30:14.230 --> 00:30:16.020
So I'm going to talk
about some new tools

00:30:16.020 --> 00:30:18.990
that our team has designed to
help developers use machine

00:30:18.990 --> 00:30:21.660
learning everywhere.

00:30:21.660 --> 00:30:24.390
So our homes and
cities are filled

00:30:24.390 --> 00:30:27.260
with devices that contain
embedded computing power.

00:30:27.260 --> 00:30:30.990
And in fact, every year,
literally billions of devices

00:30:30.990 --> 00:30:34.110
are manufactured that contain
small but highly capable

00:30:34.110 --> 00:30:37.230
computation devices
called microcontrollers.

00:30:37.230 --> 00:30:39.420
So microcontrollers
are at the core

00:30:39.420 --> 00:30:41.760
of most of our digital
gadgets, everything

00:30:41.760 --> 00:30:43.530
from the buttons
on your microwave

00:30:43.530 --> 00:30:46.380
through to the electronics
controlling your car.

00:30:46.380 --> 00:30:48.240
And our team
started to ask, what

00:30:48.240 --> 00:30:50.610
if developers could
deploy machine learning

00:30:50.610 --> 00:30:53.910
to all of these objects?

00:30:53.910 --> 00:30:56.180
So at the TensorFlow
Dev Summit, we

00:30:56.180 --> 00:30:58.370
announced an
experimental interpreter

00:30:58.370 --> 00:31:02.360
that will run TensorFlow
models on microcontrollers.

00:31:02.360 --> 00:31:05.330
So this is actually a
new frontier for AI.

00:31:05.330 --> 00:31:09.020
We have super cheap hardware
with super huge battery life

00:31:09.020 --> 00:31:11.190
and no need for an
internet connection,

00:31:11.190 --> 00:31:13.380
because we're doing
offline inference.

00:31:13.380 --> 00:31:16.430
So this enables some incredible
potential applications,

00:31:16.430 --> 00:31:19.910
where AI can become truly
personal while still preserving

00:31:19.910 --> 00:31:22.540
privacy.

00:31:22.540 --> 00:31:24.990
We want to make it ridiculously
easy for developers

00:31:24.990 --> 00:31:27.180
to build these new
types of products,

00:31:27.180 --> 00:31:29.400
so we've actually
worked with SparkFun

00:31:29.400 --> 00:31:31.440
to design a
microcontroller development

00:31:31.440 --> 00:31:33.570
board that you can buy today.

00:31:33.570 --> 00:31:35.730
It's called the
SparkFun Edge, and it's

00:31:35.730 --> 00:31:39.120
powered by an ultra efficient
ARM processor that's packed

00:31:39.120 --> 00:31:40.920
with sensors and I/O ports.

00:31:40.920 --> 00:31:45.060
So you can use it to prototype
embedded machine learning code.

00:31:45.060 --> 00:31:47.100
And we have example
code available

00:31:47.100 --> 00:31:49.670
that shows how you can
run speech recognition

00:31:49.670 --> 00:31:53.470
in a model that takes up less
than 20 kilobytes of memory,

00:31:53.470 --> 00:31:56.000
which is crazy.

00:31:56.000 --> 00:31:58.970
So I'm now going to give you
a quick demo of the device,

00:31:58.970 --> 00:32:00.740
and I'll show you
what some of this code

00:32:00.740 --> 00:32:02.700
looks like for
running inference.

00:32:02.700 --> 00:32:05.150
So you should remember before
we do this, all of this

00:32:05.150 --> 00:32:08.570
is all available on our website,
along with documentation

00:32:08.570 --> 00:32:09.860
and tutorials.

00:32:09.860 --> 00:32:12.620
And the really cool thing,
while you're here at I/O,

00:32:12.620 --> 00:32:14.780
you should head over
to the Codelabs area.

00:32:14.780 --> 00:32:16.460
And you can try
hands on development

00:32:16.460 --> 00:32:18.480
with the SparkFun Edge boards.

00:32:18.480 --> 00:32:21.120
So let's switch over
to the camera, here.

00:32:21.120 --> 00:32:23.370
LAURENCE MORONEY: I think
that actual image was bigger

00:32:23.370 --> 00:32:24.420
than 20 kilobytes.

00:32:24.420 --> 00:32:26.720
DANIEL SITUNAYAKE:
Yeah, definitely.

00:32:26.720 --> 00:32:28.410
It's kind of
mind-blowing that you

00:32:28.410 --> 00:32:32.410
can fit a speech model into
such a small amount of memory.

00:32:32.410 --> 00:32:34.960
So this is the device itself.

00:32:34.960 --> 00:32:37.260
So it's just a little dev board.

00:32:37.260 --> 00:32:38.700
I'm going to slide
the battery in.

00:32:42.090 --> 00:32:44.160
So the program we
have here, basically,

00:32:44.160 --> 00:32:45.810
is just running inference.

00:32:45.810 --> 00:32:48.570
And every second of
audio that comes in,

00:32:48.570 --> 00:32:51.540
it's running through a
little model that looks

00:32:51.540 --> 00:32:53.310
for a couple of hot words.

00:32:53.310 --> 00:32:54.880
You can see this
light is flashing.

00:32:54.880 --> 00:32:57.820
It flashes once every
time inference is run.

00:32:57.820 --> 00:32:59.910
So we're getting a pretty
decent frame right,

00:32:59.910 --> 00:33:02.790
even though it's a tiny,
low powered microcontroller

00:33:02.790 --> 00:33:05.660
with a coin cell battery.

00:33:05.660 --> 00:33:08.480
So what I'm going to do now
is take my life in my hands

00:33:08.480 --> 00:33:10.730
and try and get it to
trigger with the hot words.

00:33:10.730 --> 00:33:14.830
And hopefully, you'll
see some lights flash.

00:33:14.830 --> 00:33:17.850
Yes, yes, yes.

00:33:17.850 --> 00:33:20.240
First time, not lucky.

00:33:20.240 --> 00:33:23.411
Yes, yes, yes.

00:33:23.411 --> 00:33:25.790
Yes, yes, yes.

00:33:25.790 --> 00:33:30.680
So it's not working so great
when we've got the AC going,

00:33:30.680 --> 00:33:32.630
but you saw the lights
lighting up there.

00:33:32.630 --> 00:33:34.520
And I basically got a
really simple program

00:33:34.520 --> 00:33:36.830
that looks at the
confidence score

00:33:36.830 --> 00:33:39.920
that we get from the model
that the word yes was detected.

00:33:39.920 --> 00:33:42.350
And the higher the confidence,
the more lights appear.

00:33:42.350 --> 00:33:43.980
So we got three lights there.

00:33:43.980 --> 00:33:46.910
So it's pretty good.

00:33:46.910 --> 00:33:48.190
Let's have a look at the code.

00:33:48.190 --> 00:33:49.648
So if we can go
back to the slides?

00:33:58.040 --> 00:34:01.820
So all we do, basically,
to make this work is

00:34:01.820 --> 00:34:04.490
we've got our model, which is
just a plain old TensorFlow

00:34:04.490 --> 00:34:06.980
Lite model that you trained
however you wanted to

00:34:06.980 --> 00:34:09.620
with the rest of our
TensorFlow tool chain.

00:34:09.620 --> 00:34:13.130
And we have this model
available as an array

00:34:13.130 --> 00:34:15.469
of bytes within our app.

00:34:15.469 --> 00:34:18.199
We're going to pull
in some objects

00:34:18.199 --> 00:34:20.480
that we're going to use
to run the interpreters.

00:34:20.480 --> 00:34:23.330
So first of all, we
create a resolver,

00:34:23.330 --> 00:34:25.580
which is able to pull
in the TensorFlow ops

00:34:25.580 --> 00:34:28.020
that we need to run the model.

00:34:28.020 --> 00:34:30.530
We then create some
memory that is allocated

00:34:30.530 --> 00:34:32.480
for some of the
working processes that

00:34:32.480 --> 00:34:35.040
are going to happen
as we input data

00:34:35.040 --> 00:34:37.040
and run some of the operations.

00:34:37.040 --> 00:34:39.330
And then we build and
interpret the object,

00:34:39.330 --> 00:34:41.090
which we pass all
this stuff into,

00:34:41.090 --> 00:34:45.400
that is actually going to
execute the model for us.

00:34:45.400 --> 00:34:48.960
So the next thing
we do is basically

00:34:48.960 --> 00:34:50.699
generate some
features that we're

00:34:50.699 --> 00:34:51.940
going to pass into the model.

00:34:51.940 --> 00:34:53.730
So we have some
code not pictured

00:34:53.730 --> 00:34:57.180
here, which takes audio
from the microphones that

00:34:57.180 --> 00:35:01.140
are on the board and transforms
that into a spectrogram

00:35:01.140 --> 00:35:03.910
that we then feed
into the model.

00:35:03.910 --> 00:35:07.590
Once we have done that,
we invoke the model,

00:35:07.590 --> 00:35:09.880
and we get an output.

00:35:09.880 --> 00:35:11.922
So the output is
just another tensor,

00:35:11.922 --> 00:35:13.380
and we can look
through that tensor

00:35:13.380 --> 00:35:18.480
to find which of our classes
were able to be matched.

00:35:18.480 --> 00:35:20.850
And hopefully, in
this case, it was

00:35:20.850 --> 00:35:24.090
the yes that showed up as
the highest probability.

00:35:24.090 --> 00:35:26.950
So all of this code
is available online.

00:35:26.950 --> 00:35:29.610
We have documentation
that walks you through it.

00:35:29.610 --> 00:35:33.090
And like I said, the device
is available here, in I/O,

00:35:33.090 --> 00:35:36.630
in the Codelabs labs area, if
you'd like to try it yourself.

00:35:36.630 --> 00:35:39.920
So tiny computers are great,
but sometimes, you just

00:35:39.920 --> 00:35:41.480
need more power.

00:35:41.480 --> 00:35:44.270
So imagine you have a
manufacturing plant that

00:35:44.270 --> 00:35:48.050
is using computer vision to spot
faulty parts on a fast moving

00:35:48.050 --> 00:35:49.680
production line.

00:35:49.680 --> 00:35:52.830
So we recently announced
the Coral platform,

00:35:52.830 --> 00:35:55.130
which provides hardware
for accelerated

00:35:55.130 --> 00:35:56.600
inference at the Edge.

00:35:56.600 --> 00:35:58.647
So these are small
devices still,

00:35:58.647 --> 00:36:00.230
but they use something
called the Edge

00:36:00.230 --> 00:36:04.500
TPU to run machine learning
models incredibly fast.

00:36:04.500 --> 00:36:06.520
So one of our
development boards here

00:36:06.520 --> 00:36:10.520
can run image classification
on several simultaneous video

00:36:10.520 --> 00:36:12.900
streams at 60 frames per second.

00:36:12.900 --> 00:36:14.480
So it's super awesome.

00:36:14.480 --> 00:36:17.120
We have these devices
available to check out

00:36:17.120 --> 00:36:19.250
in the Codelabs area, as well.

00:36:19.250 --> 00:36:22.040
And in addition, in
the ML and AI sandbox,

00:36:22.040 --> 00:36:25.580
there's a demo of
showing a use case,

00:36:25.580 --> 00:36:29.030
spotting faulty parts
in manufacturing.

00:36:29.030 --> 00:36:32.260
So once again, it's super easy
to run TensorFlow Lite models

00:36:32.260 --> 00:36:33.550
on Coral devices.

00:36:33.550 --> 00:36:36.160
And this example shows
how you can load a model,

00:36:36.160 --> 00:36:39.670
grab camera input, run
inference, and annotate

00:36:39.670 --> 00:36:42.080
an output image in just
a few lines of code.

00:36:42.080 --> 00:36:44.110
So all of this,
again, is available

00:36:44.110 --> 00:36:47.550
online on the Coral site.

00:36:47.550 --> 00:36:51.210
So we've shown a ton of exciting
stuff today, and all of it

00:36:51.210 --> 00:36:55.380
is available on TensorFlow.org
and the Coral site, right now.

00:36:55.380 --> 00:36:59.370
So you'll be able to find
example code, example apps,

00:36:59.370 --> 00:37:01.170
pretrained models,
and everything

00:37:01.170 --> 00:37:05.120
you need to get started
with deploying to device.

00:37:05.120 --> 00:37:07.800
And I've got some
links up here for you.

00:37:07.800 --> 00:37:09.690
But while you're
here at I/O, there

00:37:09.690 --> 00:37:13.320
are a ton of other opportunities
to play with on device ML.

00:37:13.320 --> 00:37:15.840
So we have a couple of sessions
that I'd like to call out,

00:37:15.840 --> 00:37:16.620
here.

00:37:16.620 --> 00:37:19.650
We have the TensorFlow Lite
official talk, tomorrow,

00:37:19.650 --> 00:37:23.010
which is going to go into a lot
more depth around TensorFlow

00:37:23.010 --> 00:37:25.650
Lite and the tools we have
available for on device

00:37:25.650 --> 00:37:27.930
inference and converting models.

00:37:27.930 --> 00:37:29.790
And we also have
a talk on What's

00:37:29.790 --> 00:37:33.990
New in Android ML, which
is this evening at 6:00 PM.

00:37:33.990 --> 00:37:36.690
So you should definitely
check both of those out.

00:37:36.690 --> 00:37:39.310
And in the Codelabs area,
we have a load of content.

00:37:39.310 --> 00:37:41.070
So if you're just
learning TensorFlow,

00:37:41.070 --> 00:37:44.460
we have a six part course you
can take to basically go end

00:37:44.460 --> 00:37:48.528
to end from nothing to knowing
what you're talking about.

00:37:48.528 --> 00:37:50.070
And then we have a
couple of Codelabs

00:37:50.070 --> 00:37:51.810
you can use for on
device ML, and I

00:37:51.810 --> 00:37:54.600
think there's a Coral
codelab, as well.

00:37:54.600 --> 00:37:57.663
So thank you so
much for showing up.

00:37:57.663 --> 00:37:59.080
And I hope this
has been exciting,

00:37:59.080 --> 00:38:01.810
and you've got a glimpse of
how you can do on device ML.

00:38:01.810 --> 00:38:03.310
Like you saw in the
keynote, there's

00:38:03.310 --> 00:38:06.040
some amazing applications,
and it's up to you

00:38:06.040 --> 00:38:08.945
to build this
amazing, new future.

00:38:08.945 --> 00:38:11.590
So thank you so
much for being here.

00:38:11.590 --> 00:38:14.940
[MUSIC PLAYING]

