WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.579
all right I think there's been an

00:00:01.579 --> 00:00:01.589
all right I think there's been an
 

00:00:01.589 --> 00:00:03.770
all right I think there's been an
exciting video in this video you see how

00:00:03.770 --> 00:00:03.780
exciting video in this video you see how
 

00:00:03.780 --> 00:00:06.230
exciting video in this video you see how
to implement gradient descent for your

00:00:06.230 --> 00:00:06.240
to implement gradient descent for your
 

00:00:06.240 --> 00:00:08.419
to implement gradient descent for your
neural network with one hidden layer in

00:00:08.419 --> 00:00:08.429
neural network with one hidden layer in
 

00:00:08.429 --> 00:00:10.520
neural network with one hidden layer in
this video I'm going to just give you

00:00:10.520 --> 00:00:10.530
this video I'm going to just give you
 

00:00:10.530 --> 00:00:12.799
this video I'm going to just give you
the equations you need to implement in

00:00:12.799 --> 00:00:12.809
the equations you need to implement in
 

00:00:12.809 --> 00:00:14.810
the equations you need to implement in
order to get that propagation of the jet

00:00:14.810 --> 00:00:14.820
order to get that propagation of the jet
 

00:00:14.820 --> 00:00:17.029
order to get that propagation of the jet
gradient descent working and then in the

00:00:17.029 --> 00:00:17.039
gradient descent working and then in the
 

00:00:17.039 --> 00:00:19.429
gradient descent working and then in the
video after this one or give some more

00:00:19.429 --> 00:00:19.439
video after this one or give some more
 

00:00:19.439 --> 00:00:21.500
video after this one or give some more
intuition about why these particular

00:00:21.500 --> 00:00:21.510
intuition about why these particular
 

00:00:21.510 --> 00:00:24.140
intuition about why these particular
equations are the accurate equations or

00:00:24.140 --> 00:00:24.150
equations are the accurate equations or
 

00:00:24.150 --> 00:00:26.060
equations are the accurate equations or
the correct equations for computing the

00:00:26.060 --> 00:00:26.070
the correct equations for computing the
 

00:00:26.070 --> 00:00:27.620
the correct equations for computing the
gradients you need for your neural

00:00:27.620 --> 00:00:27.630
gradients you need for your neural
 

00:00:27.630 --> 00:00:30.050
gradients you need for your neural
network so your neural network with a

00:00:30.050 --> 00:00:30.060
network so your neural network with a
 

00:00:30.060 --> 00:00:32.510
network so your neural network with a
single hidden layer for now will have

00:00:32.510 --> 00:00:32.520
single hidden layer for now will have
 

00:00:32.520 --> 00:00:39.920
single hidden layer for now will have
parameters W 1 V 1 W 2 and B 2 and so as

00:00:39.920 --> 00:00:39.930
parameters W 1 V 1 W 2 and B 2 and so as
 

00:00:39.930 --> 00:00:44.750
parameters W 1 V 1 W 2 and B 2 and so as
a reminder if you have an X alternative

00:00:44.750 --> 00:00:44.760
a reminder if you have an X alternative
 

00:00:44.760 --> 00:00:50.389
a reminder if you have an X alternative
the UM n 0 input features and N 1 hidden

00:00:50.389 --> 00:00:50.399
the UM n 0 input features and N 1 hidden
 

00:00:50.399 --> 00:00:56.660
the UM n 0 input features and N 1 hidden
units and n 2 output units in our

00:00:56.660 --> 00:00:56.670
units and n 2 output units in our
 

00:00:56.670 --> 00:00:59.139
units and n 2 output units in our
example so following it n 2 equals 1

00:00:59.139 --> 00:00:59.149
example so following it n 2 equals 1
 

00:00:59.149 --> 00:01:06.500
example so following it n 2 equals 1
then the matrix W 1 will be N 1 by n 0 B

00:01:06.500 --> 00:01:06.510
then the matrix W 1 will be N 1 by n 0 B
 

00:01:06.510 --> 00:01:08.870
then the matrix W 1 will be N 1 by n 0 B
1 will be an N 1 dimensional vector so

00:01:08.870 --> 00:01:08.880
1 will be an N 1 dimensional vector so
 

00:01:08.880 --> 00:01:11.240
1 will be an N 1 dimensional vector so
you can write down as a 10 1 by 1

00:01:11.240 --> 00:01:11.250
you can write down as a 10 1 by 1
 

00:01:11.250 --> 00:01:13.340
you can write down as a 10 1 by 1
dimensional matrix really a column

00:01:13.340 --> 00:01:13.350
dimensional matrix really a column
 

00:01:13.350 --> 00:01:16.490
dimensional matrix really a column
vector the dimensions of W 2 will be n 2

00:01:16.490 --> 00:01:16.500
vector the dimensions of W 2 will be n 2
 

00:01:16.500 --> 00:01:20.830
vector the dimensions of W 2 will be n 2
by N 1 and the dimension of B 2 will be

00:01:20.830 --> 00:01:20.840
by N 1 and the dimension of B 2 will be
 

00:01:20.840 --> 00:01:26.749
by N 1 and the dimension of B 2 will be
n 2 by 1 right where again so far we've

00:01:26.749 --> 00:01:26.759
n 2 by 1 right where again so far we've
 

00:01:26.759 --> 00:01:28.609
n 2 by 1 right where again so far we've
only seen examples where n 2 is equal to

00:01:28.609 --> 00:01:28.619
only seen examples where n 2 is equal to
 

00:01:28.619 --> 00:01:30.800
only seen examples where n 2 is equal to
one where you have just one a single

00:01:30.800 --> 00:01:30.810
one where you have just one a single
 

00:01:30.810 --> 00:01:36.920
one where you have just one a single
hidden unit so you also have a cost

00:01:36.920 --> 00:01:36.930
hidden unit so you also have a cost
 

00:01:36.930 --> 00:01:39.560
hidden unit so you also have a cost
function for a neural network and for

00:01:39.560 --> 00:01:39.570
function for a neural network and for
 

00:01:39.570 --> 00:01:41.359
function for a neural network and for
now I'm just going to assume that you're

00:01:41.359 --> 00:01:41.369
now I'm just going to assume that you're
 

00:01:41.369 --> 00:01:44.210
now I'm just going to assume that you're
doing binary classification so in that

00:01:44.210 --> 00:01:44.220
doing binary classification so in that
 

00:01:44.220 --> 00:01:49.100
doing binary classification so in that
case the cost of your parameters as

00:01:49.100 --> 00:01:49.110
case the cost of your parameters as
 

00:01:49.110 --> 00:01:51.760
case the cost of your parameters as
follows is going to be 1 over m of the

00:01:51.760 --> 00:01:51.770
follows is going to be 1 over m of the
 

00:01:51.770 --> 00:01:57.080
follows is going to be 1 over m of the
average of that loss function and so L

00:01:57.080 --> 00:01:57.090
average of that loss function and so L
 

00:01:57.090 --> 00:01:59.959
average of that loss function and so L
here is the loss when your new network

00:01:59.959 --> 00:01:59.969
here is the loss when your new network
 

00:01:59.969 --> 00:02:03.020
here is the loss when your new network
predicts y hat all right this is really

00:02:03.020 --> 00:02:03.030
predicts y hat all right this is really
 

00:02:03.030 --> 00:02:05.899
predicts y hat all right this is really
a a to when the ground should label is e

00:02:05.899 --> 00:02:05.909
a a to when the ground should label is e
 

00:02:05.909 --> 00:02:07.639
a a to when the ground should label is e
to the Y and if you're doing binary

00:02:07.639 --> 00:02:07.649
to the Y and if you're doing binary
 

00:02:07.649 --> 00:02:09.499
to the Y and if you're doing binary
classification the loss function can be

00:02:09.499 --> 00:02:09.509
classification the loss function can be
 

00:02:09.509 --> 00:02:12.500
classification the loss function can be
exactly what you use for logistic

00:02:12.500 --> 00:02:12.510
exactly what you use for logistic
 

00:02:12.510 --> 00:02:15.020
exactly what you use for logistic
earlier so to crane the parameters your

00:02:15.020 --> 00:02:15.030
earlier so to crane the parameters your
 

00:02:15.030 --> 00:02:18.410
earlier so to crane the parameters your
algorithms you need to perform gradient

00:02:18.410 --> 00:02:18.420
algorithms you need to perform gradient
 

00:02:18.420 --> 00:02:21.589
algorithms you need to perform gradient
descent when training your network is

00:02:21.589 --> 00:02:21.599
descent when training your network is
 

00:02:21.599 --> 00:02:23.750
descent when training your network is
important to initialize the parameters

00:02:23.750 --> 00:02:23.760
important to initialize the parameters
 

00:02:23.760 --> 00:02:25.910
important to initialize the parameters
randomly rather than to all the URLs

00:02:25.910 --> 00:02:25.920
randomly rather than to all the URLs
 

00:02:25.920 --> 00:02:28.130
randomly rather than to all the URLs
will say later why that's the case but

00:02:28.130 --> 00:02:28.140
will say later why that's the case but
 

00:02:28.140 --> 00:02:30.020
will say later why that's the case but
after initializing the parameter to

00:02:30.020 --> 00:02:30.030
after initializing the parameter to
 

00:02:30.030 --> 00:02:32.330
after initializing the parameter to
something each loop or gradient descent

00:02:32.330 --> 00:02:32.340
something each loop or gradient descent
 

00:02:32.340 --> 00:02:34.130
something each loop or gradient descent
would compute the predictions

00:02:34.130 --> 00:02:34.140
would compute the predictions
 

00:02:34.140 --> 00:02:36.770
would compute the predictions
so you basically compute you know y hat

00:02:36.770 --> 00:02:36.780
so you basically compute you know y hat
 

00:02:36.780 --> 00:02:42.349
so you basically compute you know y hat
I for I equals 1 through m say and then

00:02:42.349 --> 00:02:42.359
I for I equals 1 through m say and then
 

00:02:42.359 --> 00:02:44.479
I for I equals 1 through m say and then
you need to compute the derivative so

00:02:44.479 --> 00:02:44.489
you need to compute the derivative so
 

00:02:44.489 --> 00:02:49.460
you need to compute the derivative so
you need to compute DW 1 and that's this

00:02:49.460 --> 00:02:49.470
you need to compute DW 1 and that's this
 

00:02:49.470 --> 00:02:51.410
you need to compute DW 1 and that's this
is a derivative of the cost function

00:02:51.410 --> 00:02:51.420
is a derivative of the cost function
 

00:02:51.420 --> 00:02:54.349
is a derivative of the cost function
with respect to the parameter w1 you

00:02:54.349 --> 00:02:54.359
with respect to the parameter w1 you
 

00:02:54.359 --> 00:02:56.240
with respect to the parameter w1 you
need to compute another variable which

00:02:56.240 --> 00:02:56.250
need to compute another variable which
 

00:02:56.250 --> 00:02:59.210
need to compute another variable which
is going to call B b1 which is the

00:02:59.210 --> 00:02:59.220
is going to call B b1 which is the
 

00:02:59.220 --> 00:03:00.860
is going to call B b1 which is the
derivative or the slope of your cost

00:03:00.860 --> 00:03:00.870
derivative or the slope of your cost
 

00:03:00.870 --> 00:03:04.129
derivative or the slope of your cost
function with respect to the variable B

00:03:04.129 --> 00:03:04.139
function with respect to the variable B
 

00:03:04.139 --> 00:03:07.339
function with respect to the variable B
1 and so on similarly for the other

00:03:07.339 --> 00:03:07.349
1 and so on similarly for the other
 

00:03:07.349 --> 00:03:10.430
1 and so on similarly for the other
parameters W 2 and B 2 and then finally

00:03:10.430 --> 00:03:10.440
parameters W 2 and B 2 and then finally
 

00:03:10.440 --> 00:03:12.619
parameters W 2 and B 2 and then finally
the gradient descent update would be to

00:03:12.619 --> 00:03:12.629
the gradient descent update would be to
 

00:03:12.629 --> 00:03:17.869
the gradient descent update would be to
update W 1 as W 1 minus alpha the

00:03:17.869 --> 00:03:17.879
update W 1 as W 1 minus alpha the
 

00:03:17.879 --> 00:03:23.119
update W 1 as W 1 minus alpha the
learning rate times d w1 b1 gets updated

00:03:23.119 --> 00:03:23.129
learning rate times d w1 b1 gets updated
 

00:03:23.129 --> 00:03:27.500
learning rate times d w1 b1 gets updated
as b1 minus the learning rate times D b1

00:03:27.500 --> 00:03:27.510
as b1 minus the learning rate times D b1
 

00:03:27.510 --> 00:03:32.020
as b1 minus the learning rate times D b1
and similarly for W 2 and B 2 and

00:03:32.020 --> 00:03:32.030
and similarly for W 2 and B 2 and
 

00:03:32.030 --> 00:03:34.729
and similarly for W 2 and B 2 and
sometimes I use colon equals and

00:03:34.729 --> 00:03:34.739
sometimes I use colon equals and
 

00:03:34.739 --> 00:03:36.319
sometimes I use colon equals and
sometimes equals as either either the

00:03:36.319 --> 00:03:36.329
sometimes equals as either either the
 

00:03:36.329 --> 00:03:38.479
sometimes equals as either either the
notation works fine and so this would be

00:03:38.479 --> 00:03:38.489
notation works fine and so this would be
 

00:03:38.489 --> 00:03:40.819
notation works fine and so this would be
one iteration of gradient descent and

00:03:40.819 --> 00:03:40.829
one iteration of gradient descent and
 

00:03:40.829 --> 00:03:42.500
one iteration of gradient descent and
then you repeat this some number of

00:03:42.500 --> 00:03:42.510
then you repeat this some number of
 

00:03:42.510 --> 00:03:44.270
then you repeat this some number of
times until your parameters look like

00:03:44.270 --> 00:03:44.280
times until your parameters look like
 

00:03:44.280 --> 00:03:46.069
times until your parameters look like
they're converging so in previous videos

00:03:46.069 --> 00:03:46.079
they're converging so in previous videos
 

00:03:46.079 --> 00:03:47.839
they're converging so in previous videos
we talked about how to compute

00:03:47.839 --> 00:03:47.849
we talked about how to compute
 

00:03:47.849 --> 00:03:50.089
we talked about how to compute
predictions how to compute the outputs

00:03:50.089 --> 00:03:50.099
predictions how to compute the outputs
 

00:03:50.099 --> 00:03:51.619
predictions how to compute the outputs
and we saw how to do that in a

00:03:51.619 --> 00:03:51.629
and we saw how to do that in a
 

00:03:51.629 --> 00:03:54.289
and we saw how to do that in a
vectorized way as well so the key is to

00:03:54.289 --> 00:03:54.299
vectorized way as well so the key is to
 

00:03:54.299 --> 00:03:56.259
vectorized way as well so the key is to
know how to compute these partial

00:03:56.259 --> 00:03:56.269
know how to compute these partial
 

00:03:56.269 --> 00:04:00.140
know how to compute these partial
derivative terms the DW 1 DB 1 as well

00:04:00.140 --> 00:04:00.150
derivative terms the DW 1 DB 1 as well
 

00:04:00.150 --> 00:04:04.069
derivative terms the DW 1 DB 1 as well
as the derivatives BW 2 and DP 2 so what

00:04:04.069 --> 00:04:04.079
as the derivatives BW 2 and DP 2 so what
 

00:04:04.079 --> 00:04:06.770
as the derivatives BW 2 and DP 2 so what
I'd like to do is just give you the

00:04:06.770 --> 00:04:06.780
I'd like to do is just give you the
 

00:04:06.780 --> 00:04:09.409
I'd like to do is just give you the
equations you need in order to compute

00:04:09.409 --> 00:04:09.419
equations you need in order to compute
 

00:04:09.419 --> 00:04:12.140
equations you need in order to compute
these derivatives and I'll defer to the

00:04:12.140 --> 00:04:12.150
these derivatives and I'll defer to the
 

00:04:12.150 --> 00:04:14.689
these derivatives and I'll defer to the
next video which is an optional video to

00:04:14.689 --> 00:04:14.699
next video which is an optional video to
 

00:04:14.699 --> 00:04:17.420
next video which is an optional video to
go greater into Jeff about how we came

00:04:17.420 --> 00:04:17.430
go greater into Jeff about how we came
 

00:04:17.430 --> 00:04:20.060
go greater into Jeff about how we came
up with those formulas so then just

00:04:20.060 --> 00:04:20.070
up with those formulas so then just
 

00:04:20.070 --> 00:04:23.080
up with those formulas so then just
summarize again the equations for

00:04:23.080 --> 00:04:23.090
summarize again the equations for
 

00:04:23.090 --> 00:04:30.310
summarize again the equations for
for propagation so you have z1 equals W

00:04:30.310 --> 00:04:30.320
for propagation so you have z1 equals W
 

00:04:30.320 --> 00:04:36.090
for propagation so you have z1 equals W
1x plus B 1 and then a 1 equals the

00:04:36.090 --> 00:04:36.100
1x plus B 1 and then a 1 equals the
 

00:04:36.100 --> 00:04:39.480
1x plus B 1 and then a 1 equals the
activation function in that layer

00:04:39.480 --> 00:04:39.490
activation function in that layer
 

00:04:39.490 --> 00:04:43.180
activation function in that layer
applied other than Y since V 1 and then

00:04:43.180 --> 00:04:43.190
applied other than Y since V 1 and then
 

00:04:43.190 --> 00:04:51.159
applied other than Y since V 1 and then
Z 2 equals W 2 A 1 plus B 2 and then

00:04:51.159 --> 00:04:51.169
Z 2 equals W 2 A 1 plus B 2 and then
 

00:04:51.169 --> 00:04:54.280
Z 2 equals W 2 A 1 plus B 2 and then
finally this is all vectorize across

00:04:54.280 --> 00:04:54.290
finally this is all vectorize across
 

00:04:54.290 --> 00:04:56.320
finally this is all vectorize across
your training set right a 2 is equal to

00:04:56.320 --> 00:04:56.330
your training set right a 2 is equal to
 

00:04:56.330 --> 00:05:01.870
your training set right a 2 is equal to
G 2 of Z 2 the game for now if we assume

00:05:01.870 --> 00:05:01.880
G 2 of Z 2 the game for now if we assume
 

00:05:01.880 --> 00:05:03.700
G 2 of Z 2 the game for now if we assume
you're doing binary classification then

00:05:03.700 --> 00:05:03.710
you're doing binary classification then
 

00:05:03.710 --> 00:05:05.920
you're doing binary classification then
this activation function really should

00:05:05.920 --> 00:05:05.930
this activation function really should
 

00:05:05.930 --> 00:05:07.510
this activation function really should
be the sigmoid function so I'm just

00:05:07.510 --> 00:05:07.520
be the sigmoid function so I'm just
 

00:05:07.520 --> 00:05:10.060
be the sigmoid function so I'm just
throw that in 0 so that the forward

00:05:10.060 --> 00:05:10.070
throw that in 0 so that the forward
 

00:05:10.070 --> 00:05:13.060
throw that in 0 so that the forward
propagation or the left-to-right forward

00:05:13.060 --> 00:05:13.070
propagation or the left-to-right forward
 

00:05:13.070 --> 00:05:15.190
propagation or the left-to-right forward
computation for your neural network next

00:05:15.190 --> 00:05:15.200
computation for your neural network next
 

00:05:15.200 --> 00:05:17.379
computation for your neural network next
let's compute the derivatives so this is

00:05:17.379 --> 00:05:17.389
let's compute the derivatives so this is
 

00:05:17.389 --> 00:05:22.450
let's compute the derivatives so this is
the back propagation step we're going to

00:05:22.450 --> 00:05:22.460
the back propagation step we're going to
 

00:05:22.460 --> 00:05:29.320
the back propagation step we're going to
compute D Z 2 equals a 2 minus the

00:05:29.320 --> 00:05:29.330
compute D Z 2 equals a 2 minus the
 

00:05:29.330 --> 00:05:32.380
compute D Z 2 equals a 2 minus the
ground truth Y and just just as a

00:05:32.380 --> 00:05:32.390
ground truth Y and just just as a
 

00:05:32.390 --> 00:05:34.450
ground truth Y and just just as a
reminder all this is vectorize across

00:05:34.450 --> 00:05:34.460
reminder all this is vectorize across
 

00:05:34.460 --> 00:05:38.320
reminder all this is vectorize across
example so the matrix Y is the sum 1 by

00:05:38.320 --> 00:05:38.330
example so the matrix Y is the sum 1 by
 

00:05:38.330 --> 00:05:41.860
example so the matrix Y is the sum 1 by
M matrix then this all of your M

00:05:41.860 --> 00:05:41.870
M matrix then this all of your M
 

00:05:41.870 --> 00:05:45.279
M matrix then this all of your M
examples stacked horizontally then it

00:05:45.279 --> 00:05:45.289
examples stacked horizontally then it
 

00:05:45.289 --> 00:05:51.370
examples stacked horizontally then it
turns out DW 2 is equal to this in fact

00:05:51.370 --> 00:05:51.380
turns out DW 2 is equal to this in fact
 

00:05:51.380 --> 00:05:55.270
turns out DW 2 is equal to this in fact
um these first three equations are very

00:05:55.270 --> 00:05:55.280
um these first three equations are very
 

00:05:55.280 --> 00:05:58.870
um these first three equations are very
similar to gradient descent for logistic

00:05:58.870 --> 00:05:58.880
similar to gradient descent for logistic
 

00:05:58.880 --> 00:05:59.730
similar to gradient descent for logistic
regression

00:05:59.730 --> 00:05:59.740
regression
 

00:05:59.740 --> 00:06:06.610
regression
come on X is equals 1 comma um chickens

00:06:06.610 --> 00:06:06.620
come on X is equals 1 comma um chickens
 

00:06:06.620 --> 00:06:10.330
come on X is equals 1 comma um chickens
equals true and just a little detail

00:06:10.330 --> 00:06:10.340
equals true and just a little detail
 

00:06:10.340 --> 00:06:14.409
equals true and just a little detail
this NP dot some is a Python numpy

00:06:14.409 --> 00:06:14.419
this NP dot some is a Python numpy
 

00:06:14.419 --> 00:06:17.080
this NP dot some is a Python numpy
come-on for something across one

00:06:17.080 --> 00:06:17.090
come-on for something across one
 

00:06:17.090 --> 00:06:19.320
come-on for something across one
dimension of a matrix in this case

00:06:19.320 --> 00:06:19.330
dimension of a matrix in this case
 

00:06:19.330 --> 00:06:22.090
dimension of a matrix in this case
something horizontally and what keep

00:06:22.090 --> 00:06:22.100
something horizontally and what keep
 

00:06:22.100 --> 00:06:25.480
something horizontally and what keep
dims does is it prevents python from

00:06:25.480 --> 00:06:25.490
dims does is it prevents python from
 

00:06:25.490 --> 00:06:29.129
dims does is it prevents python from
outputting one of those funny rank 1

00:06:29.129 --> 00:06:29.139
outputting one of those funny rank 1
 

00:06:29.139 --> 00:06:31.750
outputting one of those funny rank 1
arrays where where the dimensions was

00:06:31.750 --> 00:06:31.760
arrays where where the dimensions was
 

00:06:31.760 --> 00:06:33.790
arrays where where the dimensions was
you know n comma

00:06:33.790 --> 00:06:33.800
you know n comma
 

00:06:33.800 --> 00:06:35.529
you know n comma
so by having keep them as equals true

00:06:35.529 --> 00:06:35.539
so by having keep them as equals true
 

00:06:35.539 --> 00:06:39.640
so by having keep them as equals true
this ensures that Python outputs for db2

00:06:39.640 --> 00:06:39.650
this ensures that Python outputs for db2
 

00:06:39.650 --> 00:06:42.999
this ensures that Python outputs for db2
a vector that is sum n by one

00:06:42.999 --> 00:06:43.009
a vector that is sum n by one
 

00:06:43.009 --> 00:06:46.270
a vector that is sum n by one
technically this will be I guess n to

00:06:46.270 --> 00:06:46.280
technically this will be I guess n to
 

00:06:46.280 --> 00:06:48.369
technically this will be I guess n to
buy one in this case is just a one by

00:06:48.369 --> 00:06:48.379
buy one in this case is just a one by
 

00:06:48.379 --> 00:06:51.390
buy one in this case is just a one by
one number so maybe it doesn't matter

00:06:51.390 --> 00:06:51.400
one number so maybe it doesn't matter
 

00:06:51.400 --> 00:06:54.070
one number so maybe it doesn't matter
but later on we'll see when it really

00:06:54.070 --> 00:06:54.080
but later on we'll see when it really
 

00:06:54.080 --> 00:06:57.279
but later on we'll see when it really
matters so so far what we've done is

00:06:57.279 --> 00:06:57.289
matters so so far what we've done is
 

00:06:57.289 --> 00:07:00.189
matters so so far what we've done is
very similar to logistic regression but

00:07:00.189 --> 00:07:00.199
very similar to logistic regression but
 

00:07:00.199 --> 00:07:02.890
very similar to logistic regression but
now as you compute two new to run back

00:07:02.890 --> 00:07:02.900
now as you compute two new to run back
 

00:07:02.900 --> 00:07:09.100
now as you compute two new to run back
propagation you would compute this easy

00:07:09.100 --> 00:07:09.110
propagation you would compute this easy
 

00:07:09.110 --> 00:07:17.170
propagation you would compute this easy
two times G one prime of Z 1 so this

00:07:17.170 --> 00:07:17.180
two times G one prime of Z 1 so this
 

00:07:17.180 --> 00:07:20.200
two times G one prime of Z 1 so this
quantity G 1 prime is the derivative of

00:07:20.200 --> 00:07:20.210
quantity G 1 prime is the derivative of
 

00:07:20.210 --> 00:07:21.939
quantity G 1 prime is the derivative of
whatever was the activation function you

00:07:21.939 --> 00:07:21.949
whatever was the activation function you
 

00:07:21.949 --> 00:07:25.089
whatever was the activation function you
use for the hidden layer and for the

00:07:25.089 --> 00:07:25.099
use for the hidden layer and for the
 

00:07:25.099 --> 00:07:26.830
use for the hidden layer and for the
output layer I assume that you're doing

00:07:26.830 --> 00:07:26.840
output layer I assume that you're doing
 

00:07:26.840 --> 00:07:28.959
output layer I assume that you're doing
binary classification with the sigmoid

00:07:28.959 --> 00:07:28.969
binary classification with the sigmoid
 

00:07:28.969 --> 00:07:30.459
binary classification with the sigmoid
function so that's already baked into

00:07:30.459 --> 00:07:30.469
function so that's already baked into
 

00:07:30.469 --> 00:07:34.600
function so that's already baked into
that formula for DZ 2 and this times is

00:07:34.600 --> 00:07:34.610
that formula for DZ 2 and this times is
 

00:07:34.610 --> 00:07:39.249
that formula for DZ 2 and this times is
a element-wise product so this here is

00:07:39.249 --> 00:07:39.259
a element-wise product so this here is
 

00:07:39.259 --> 00:07:44.529
a element-wise product so this here is
going to be an N 1 by M matrix and this

00:07:44.529 --> 00:07:44.539
going to be an N 1 by M matrix and this
 

00:07:44.539 --> 00:07:47.800
going to be an N 1 by M matrix and this
here this element wise derivative thing

00:07:47.800 --> 00:07:47.810
here this element wise derivative thing
 

00:07:47.810 --> 00:07:51.189
here this element wise derivative thing
is also going to be an N 1 by n matrix

00:07:51.189 --> 00:07:51.199
is also going to be an N 1 by n matrix
 

00:07:51.199 --> 00:07:53.439
is also going to be an N 1 by n matrix
and so this times there is an element

00:07:53.439 --> 00:07:53.449
and so this times there is an element
 

00:07:53.449 --> 00:07:56.379
and so this times there is an element
wise products of two matrices then

00:07:56.379 --> 00:07:56.389
wise products of two matrices then
 

00:07:56.389 --> 00:08:02.290
wise products of two matrices then
finally DW 1 is equal to that and DB 1

00:08:02.290 --> 00:08:02.300
finally DW 1 is equal to that and DB 1
 

00:08:02.300 --> 00:08:13.769
finally DW 1 is equal to that and DB 1
is equal to this and P dot some D Z 1 X

00:08:13.769 --> 00:08:13.779
is equal to this and P dot some D Z 1 X
 

00:08:13.779 --> 00:08:16.320
is equal to this and P dot some D Z 1 X
is equals 1

00:08:16.320 --> 00:08:16.330
is equals 1
 

00:08:16.330 --> 00:08:21.890
is equals 1
keep this equals true so we're

00:08:21.890 --> 00:08:21.900
keep this equals true so we're
 

00:08:21.900 --> 00:08:23.749
keep this equals true so we're
previously the keep dinners maybe matter

00:08:23.749 --> 00:08:23.759
previously the keep dinners maybe matter
 

00:08:23.759 --> 00:08:27.680
previously the keep dinners maybe matter
less if n 2 is equal to 1 so just one by

00:08:27.680 --> 00:08:27.690
less if n 2 is equal to 1 so just one by
 

00:08:27.690 --> 00:08:30.640
less if n 2 is equal to 1 so just one by
one thing is there's a real number here

00:08:30.640 --> 00:08:30.650
one thing is there's a real number here
 

00:08:30.650 --> 00:08:36.529
one thing is there's a real number here
pp 1 will be a N 1 by 1 vector and so

00:08:36.529 --> 00:08:36.539
pp 1 will be a N 1 by 1 vector and so
 

00:08:36.539 --> 00:08:38.930
pp 1 will be a N 1 by 1 vector and so
you want Python you want n P dot some

00:08:38.930 --> 00:08:38.940
you want Python you want n P dot some
 

00:08:38.940 --> 00:08:40.610
you want Python you want n P dot some
output something of this dimension

00:08:40.610 --> 00:08:40.620
output something of this dimension
 

00:08:40.620 --> 00:08:44.240
output something of this dimension
rather than a family right one array of

00:08:44.240 --> 00:08:44.250
rather than a family right one array of
 

00:08:44.250 --> 00:08:46.640
rather than a family right one array of
that dimension which could end up

00:08:46.640 --> 00:08:46.650
that dimension which could end up
 

00:08:46.650 --> 00:08:48.290
that dimension which could end up
messing up some of your basic

00:08:48.290 --> 00:08:48.300
messing up some of your basic
 

00:08:48.300 --> 00:08:50.570
messing up some of your basic
calculations the other way would be to

00:08:50.570 --> 00:08:50.580
calculations the other way would be to
 

00:08:50.580 --> 00:08:53.300
calculations the other way would be to
not have to keep them parameters but to

00:08:53.300 --> 00:08:53.310
not have to keep them parameters but to
 

00:08:53.310 --> 00:08:56.900
not have to keep them parameters but to
explicitly call in a reshape to reshape

00:08:56.900 --> 00:08:56.910
explicitly call in a reshape to reshape
 

00:08:56.910 --> 00:09:00.050
explicitly call in a reshape to reshape
the output of and P dot some into this

00:09:00.050 --> 00:09:00.060
the output of and P dot some into this
 

00:09:00.060 --> 00:09:04.390
the output of and P dot some into this
dimension which you would like dB so how

00:09:04.390 --> 00:09:04.400
dimension which you would like dB so how
 

00:09:04.400 --> 00:09:08.300
dimension which you would like dB so how
so that was more propagation in I guess

00:09:08.300 --> 00:09:08.310
so that was more propagation in I guess
 

00:09:08.310 --> 00:09:11.329
so that was more propagation in I guess
four equations and back propagation in I

00:09:11.329 --> 00:09:11.339
four equations and back propagation in I
 

00:09:11.339 --> 00:09:14.300
four equations and back propagation in I
guess six equations I knew I just wrote

00:09:14.300 --> 00:09:14.310
guess six equations I knew I just wrote
 

00:09:14.310 --> 00:09:16.670
guess six equations I knew I just wrote
down these equations but in the next

00:09:16.670 --> 00:09:16.680
down these equations but in the next
 

00:09:16.680 --> 00:09:18.860
down these equations but in the next
optional video let's go over some

00:09:18.860 --> 00:09:18.870
optional video let's go over some
 

00:09:18.870 --> 00:09:22.040
optional video let's go over some
intuitions for how the six equations for

00:09:22.040 --> 00:09:22.050
intuitions for how the six equations for
 

00:09:22.050 --> 00:09:23.930
intuitions for how the six equations for
the back propagation algorithm were

00:09:23.930 --> 00:09:23.940
the back propagation algorithm were
 

00:09:23.940 --> 00:09:25.850
the back propagation algorithm were
derived please feel free to watch that

00:09:25.850 --> 00:09:25.860
derived please feel free to watch that
 

00:09:25.860 --> 00:09:27.860
derived please feel free to watch that
or not but either way if you implement

00:09:27.860 --> 00:09:27.870
or not but either way if you implement
 

00:09:27.870 --> 00:09:29.990
or not but either way if you implement
these algorithms you will have a correct

00:09:29.990 --> 00:09:30.000
these algorithms you will have a correct
 

00:09:30.000 --> 00:09:32.300
these algorithms you will have a correct
implementation of four prop and back

00:09:32.300 --> 00:09:32.310
implementation of four prop and back
 

00:09:32.310 --> 00:09:34.640
implementation of four prop and back
prop and you'll be able to compute the

00:09:34.640 --> 00:09:34.650
prop and you'll be able to compute the
 

00:09:34.650 --> 00:09:36.740
prop and you'll be able to compute the
derivative you need in order to apply

00:09:36.740 --> 00:09:36.750
derivative you need in order to apply
 

00:09:36.750 --> 00:09:39.019
derivative you need in order to apply
gradient descent to learn the parameters

00:09:39.019 --> 00:09:39.029
gradient descent to learn the parameters
 

00:09:39.029 --> 00:09:41.510
gradient descent to learn the parameters
of your neural network it is possible to

00:09:41.510 --> 00:09:41.520
of your neural network it is possible to
 

00:09:41.520 --> 00:09:43.699
of your neural network it is possible to
implement design room and get it to work

00:09:43.699 --> 00:09:43.709
implement design room and get it to work
 

00:09:43.709 --> 00:09:45.199
implement design room and get it to work
without deeply understanding the

00:09:45.199 --> 00:09:45.209
without deeply understanding the
 

00:09:45.209 --> 00:09:47.300
without deeply understanding the
calculus a lot of successful deep

00:09:47.300 --> 00:09:47.310
calculus a lot of successful deep
 

00:09:47.310 --> 00:09:50.510
calculus a lot of successful deep
learning practitioners do so but if you

00:09:50.510 --> 00:09:50.520
learning practitioners do so but if you
 

00:09:50.520 --> 00:09:52.310
learning practitioners do so but if you
want you can also watch the next video

00:09:52.310 --> 00:09:52.320
want you can also watch the next video
 

00:09:52.320 --> 00:09:54.170
want you can also watch the next video
just to get a bit more intuition about

00:09:54.170 --> 00:09:54.180
just to get a bit more intuition about
 

00:09:54.180 --> 00:09:56.570
just to get a bit more intuition about
the derivation of these of these

00:09:56.570 --> 00:09:56.580
the derivation of these of these
 

00:09:56.580 --> 00:09:58.820
the derivation of these of these
equations

