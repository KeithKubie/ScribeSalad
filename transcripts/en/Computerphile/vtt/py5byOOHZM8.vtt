WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.520
This is kind-of a follow up to Brais' videos on deep learning

00:00:02.520 --> 00:00:04.760
So deep learning is kind of a big thing at the moment

00:00:04.760 --> 00:00:11.120
and there's some disagreement between research over whether this is gonna be - the, this is *it*

00:00:11.120 --> 00:00:13.000
This is the big thing that's gonna change everything

00:00:13.000 --> 00:00:15.940
or whether this is another flash in the pan, like artificial neural networks were in the 80s

00:00:15.940 --> 00:00:18.360
Everyone got very excited and they got quite a good results

00:00:18.360 --> 00:00:21.020
and when they realized that they couldn't solve all the problems with them, I don't know

00:00:21.020 --> 00:00:23.560
For what it's worth, these are a big deal, I think

00:00:26.620 --> 00:00:29.880
[offscreen] Let's talk about convoluted neural networks, have I said that right?

00:00:29.880 --> 00:00:30.660
Convolutional neural networks.

00:00:30.660 --> 00:00:31.540
[offscreen] Ah, right, ok

00:00:31.540 --> 00:00:35.560
They combine both deep neural networks, which is what Brais was talking about

00:00:35.560 --> 00:00:38.900
and kernel convolutions, which is what I talked about in a previous video.

00:00:38.900 --> 00:00:41.020
I would thoroughly recommend people watch that video

00:00:41.020 --> 00:00:43.380
You know, it's got an entertaining host, right?

00:00:43.380 --> 00:00:45.120
*laugh from offscreen*

00:00:45.120 --> 00:00:48.080
So, but, but, because, if you don't know what a kernel convolution is, this isn't gonna make much sense to you

00:00:48.080 --> 00:00:49.480
So watch that video first

00:00:49.480 --> 00:00:53.400
[offscreen] So that's the kernel convolutions we did on graphics and things like, uh, sobel op-

00:00:53.400 --> 00:00:55.280
Yeah, Sobel operations, Gaussian blurs, and things like this.

00:00:55.280 --> 00:00:57.760
Sobel operations in particular, and edge detection

00:00:58.600 --> 00:01:03.840
So, if we think back to a traditional artificial neural network

00:01:03.840 --> 00:01:07.640
ok, what we've got is we've got some kind of input we're trying to learn, ok

00:01:07.640 --> 00:01:11.380
we've got some hidden layers, alright, and then we've got some output layers

00:01:11.380 --> 00:01:12.360
maybe just this one, I don't know

00:01:12.360 --> 00:01:16.080
And these are fully connected, so we have lots of connections from here

00:01:16.080 --> 00:01:19.160
and here and here and these are connected to here

00:01:19.160 --> 00:01:21.760
and so on, I'm just drawing in a few of them,

00:01:21.760 --> 00:01:23.480
and then these are all connected to the end.

00:01:23.480 --> 00:01:26.800
ok, Now using Brais's analogy, we were talking about house prices.

00:01:26.800 --> 00:01:28.540
ok, so, this will be something like number of bedrooms,

00:01:28.540 --> 00:01:30.820
and this would be something like "has it got a pool"

00:01:30.820 --> 00:01:33.240
and this would be, you know, what floor space is it

00:01:33.240 --> 00:01:35.080
and has it got a good garden and so on

00:01:35.080 --> 00:01:39.980
lots of these, ok, lots of inner nodes that we don't really care about particularly, or I don't

00:01:39.980 --> 00:01:43.740
Uh, right, and so on and so forth, and then finally at the end we have a house price.

00:01:43.740 --> 00:01:47.760
Now what this house price is, is a complicated function of these inputs.

00:01:47.760 --> 00:01:49.060
It's complicated because

00:01:49.060 --> 00:01:54.860
this node here is some linear, or non-linear now, combination of

00:01:54.940 --> 00:01:56.360
these. ok, so, a bit of this

00:01:56.360 --> 00:01:58.140
plus a bit of this, plus a bit of this, plus a bit of this

00:01:58.140 --> 00:02:00.060
through some non-linearity function.

00:02:00.060 --> 00:02:02.420
This is a different combination of these.

00:02:02.420 --> 00:02:05.820
This, again, different combination of these, and so on, right

00:02:05.820 --> 00:02:07.600
And then this, a different combination of these

00:02:07.600 --> 00:02:10.380
So, you can see, you're building up some kind of level of abstraction here

00:02:10.380 --> 00:02:12.360
where you've got combinations of combinations

00:02:12.360 --> 00:02:14.860
And that function is very complicated

00:02:14.860 --> 00:02:18.060
When Brais talked about a black box, in some ways that's exactly what it is

00:02:18.060 --> 00:02:23.700
because we can't look at these individual weights and say, "well that's got .2 of this one, so that must mean this"

00:02:23.700 --> 00:02:25.400
because we just don't know what it means, right

00:02:25.400 --> 00:02:29.600
In the grand scheme of things, in this whole network, we don't know what that individual weight means

00:02:29.600 --> 00:02:32.700
And to be honest, we might not even care that much

00:02:32.700 --> 00:02:37.620
What we really care about is how well does it predict house price, how accurate is that based on that

00:02:37.620 --> 00:02:40.600
for a different input, so we change this, read the output - is it good?

00:02:40.800 --> 00:02:42.140
Yes? Brilliant!

00:02:42.140 --> 00:02:47.100
ok, now, for images, which is obviously what I spend most of my time around,

00:02:47.400 --> 00:02:51.240
this is a start, but it's not very useful to me.

00:02:51.240 --> 00:02:59.120
If you think that this is our inputs, ok, and I give you a picture of a house, and I say "right, tell me how much this house is worth"

00:03:00.540 --> 00:03:01.580
ok, well, what?

00:03:01.580 --> 00:03:04.240
So how do I, ok, so there's two things I could do, right

00:03:04.240 --> 00:03:08.120
first of all is I could try and calculate things like number of bedrooms and stuff, based on the image

00:03:08.120 --> 00:03:08.900
and put them in here

00:03:08.900 --> 00:03:11.820
In some way, I'd be calculating some features

00:03:11.820 --> 00:03:13.940
and then I'd be putting them in here and learning on those features.

00:03:13.940 --> 00:03:17.380
That is quite a smart way of doing it, because, apart from that's obviously quite difficult

00:03:17.380 --> 00:03:21.120
um, it's smart because we don't have to have that many more neurons

00:03:21.120 --> 00:03:25.380
In anything, we can actually use the same network as we used before for our model

00:03:25.380 --> 00:03:29.100
on our house, all we have to do is work out the bit of code that does the image analysis

00:03:29.100 --> 00:03:36.000
Now, anyone that's tried to find out the number of rooms in a house based only on one picture of the outside of the house will tell me that that can't be done, right

00:03:36.000 --> 00:03:36.740
That's hard.

00:03:36.740 --> 00:03:40.220
Ok, so you could naively think, what we could do instead

00:03:40.540 --> 00:03:42.460
is just put the image in here.

00:03:42.460 --> 00:03:43.680
Make this the first pixel,

00:03:43.680 --> 00:03:45.400
and this the second pixel,

00:03:45.400 --> 00:03:46.560
and this the third pixel,

00:03:46.600 --> 00:03:47.480
and so on, ok.

00:03:47.480 --> 00:03:51.060
Then, this has got all the information it could ever need, right

00:03:51.740 --> 00:03:53.740
But it, but that's the problem

00:03:54.740 --> 00:04:01.300
7 megapixel image, that's 7 million input nodes, let's say we have 7 million nodes on the next layer

00:04:01.300 --> 00:04:05.180
each one connects to each other one, you can see that that's just gonna melt my computer

00:04:05.180 --> 00:04:08.700
it's not even gonna try and create it, it's too much information

00:04:08.700 --> 00:04:11.720
That's why we downsample our space a little bit.

00:04:11.720 --> 00:04:14.400
What we would usually do is calculate some small subset of features

00:04:14.400 --> 00:04:16.280
and then we would put them in at this end.

00:04:16.280 --> 00:04:19.740
So that's quite important. So, traditional machine learning is done a bit like that.

00:04:19.740 --> 00:04:23.440
So Michel's done some videos on this. Calculate some features about someone's face

00:04:23.440 --> 00:04:25.520
and put that in to some machine learning algorithm

00:04:25.520 --> 00:04:29.020
What you don't do is try and put the machine learning algorithm just on the face

00:04:29.020 --> 00:04:30.800
because it's too much information there

00:04:30.800 --> 00:04:35.600
Until now, ok, right? That's where convolutional neural networks step in.

00:04:35.600 --> 00:04:40.940
So, convolutional neural networks replace each of these nodes with a kernel convolution.

00:04:40.960 --> 00:04:43.040
So, like, a Sobel edge detector

00:04:43.040 --> 00:04:48.460
Now, so instead of what I would've done before, which was run a Sobel over something and then machine learn on that,

00:04:49.460 --> 00:04:55.420
I just give this the opportunity to learn which features are interesting

00:04:55.420 --> 00:04:57.780
maybe it is an edge detection, maybe it is a corner detection

00:04:57.780 --> 00:05:01.260
maybe it's something that highlights whatever's in the middle of the picture

00:05:01.260 --> 00:05:03.600
Or something that highlights the top left-hand corner

00:05:03.600 --> 00:05:07.040
it doesn't really matter, and the point is I don't know what they are

00:05:07.040 --> 00:05:11.180
right, if I give you, you know, two thousand pictures of houses

00:05:11.180 --> 00:05:13.520
and ask you to predict house prices based on the pictures

00:05:13.520 --> 00:05:18.720
I don't know for sure, I can guess, but it might be - that how many windows it has and things like this

00:05:18.960 --> 00:05:20.640
but I don't know for sure.

00:05:20.640 --> 00:05:24.100
And a computer can brute-force through those things much quicker than I can and tell me

00:05:24.100 --> 00:05:29.300
And then I can go, I can both predict it, and I can look back and say, "oh, it was windows after all"

00:05:29.300 --> 00:05:31.980
So, let's imagine that what we have is our image, ok, so

00:05:31.980 --> 00:05:35.440
I'm gonna move away from the house analogy now because I'm gonna have to draw a lot of pictures of houses

00:05:35.440 --> 00:05:38.920
if I do that. Ok, so let's talk about CNN works

00:05:38.920 --> 00:05:41.860
Um, and why it's useful. So, we have an image of something

00:05:41.860 --> 00:05:45.480
Now, I have seen convolutional neural networks used for non-images

00:05:45.480 --> 00:05:47.520
but for now, we'll just talk about images

00:05:47.520 --> 00:05:52.120
This is a picture of, let's say me. It's, you know, it's not a great likeness but I'll stick by it

00:05:52.200 --> 00:05:54.840
Now, there are three channels here, ok.

00:05:54.840 --> 00:05:57.120
So this is actually a 3D volume, in some sense

00:05:57.120 --> 00:05:58.920
remember when we talked about 3D images,

00:05:59.240 --> 00:06:01.880
you could view RGB as a, in some sense, 3D

00:06:01.880 --> 00:06:05.700
So, the first plane is our R, G, and B, or vice versa

00:06:05.700 --> 00:06:08.420
What we do is, if we performed a Sobel edge detection on this,

00:06:08.420 --> 00:06:11.580
what it would do is produce another image that was  slightly smaller than this

00:06:11.580 --> 00:06:12.780
and only one deep.

00:06:12.780 --> 00:06:17.740
So hypothetically, it would be another image where the edges, let's say the horizontal edges, were highlighted

00:06:17.740 --> 00:06:20.520
So it would kinda look like, that, or something, I don't know

00:06:20.520 --> 00:06:23.420
some half of my face where the horizontal edges are highlighted, ok

00:06:23.420 --> 00:06:24.860
It's not a great diagram

00:06:24.860 --> 00:06:30.320
But there would only be one output, because Sobel just outputs a number between 0 and 255, as soon as you scale it, ok

00:06:30.320 --> 00:06:34.620
Now the problem is that I don't know that Sobel's the best thing for this task, ok

00:06:34.620 --> 00:06:38.900
It might be, right, it might be useful to detect edges on houses, to work out what their prices are

00:06:38.900 --> 00:06:42.720
or if you want to detect faces, to detect the size of the face, that kind of makes sense

00:06:42.720 --> 00:06:46.620
On the other hand, it's gonna produce a lot of erroneous bits

00:06:46.620 --> 00:06:50.360
if I was sitting in front of a tree, there's gonna be loads of edge stuff going on there that I don't care about

00:06:50.360 --> 00:06:54.680
In a convolutional neural network, what we do is we do, let's say, 60 of these on the first layer.

00:06:54.760 --> 00:07:01.760
So we have one, and then behind it we have another one, and behind it we have another one, and behind it we another one, and so on, going this way.

00:07:01.760 --> 00:07:05.820
So the first one will be some convolution process applied to this whole image

00:07:05.820 --> 00:07:09.560
that takes three input channels and outputs one output channel

00:07:09.560 --> 00:07:12.140
The next one will be a different kernel convolution operation

00:07:12.140 --> 00:07:13.500
so each of these will have a different kernel

00:07:13.500 --> 00:07:15.660
those are our weights, those are these values here

00:07:15.660 --> 00:07:17.940
In sort of our analogy back to normal learning

00:07:18.000 --> 00:07:21.440
Um, and so let's say we have 60 of those, or 64 of those

00:07:21.440 --> 00:07:25.340
One of them might be detecting edges, one of them might be detecting corners

00:07:25.500 --> 00:07:28.940
Um, and then we use them as our features for learning

00:07:28.940 --> 00:07:33.440
Now that's a start, but we're - this is is deep learning now, right, so what do we do now

00:07:33.440 --> 00:07:36.440
well, what we now do is we do more features based on these features

00:07:36.440 --> 00:07:41.160
So we find combinations of corners, combinations of edges, that make something interesting

00:07:41.400 --> 00:07:48.940
My face is not just a circle of edges, what it is is a number of corners and edges and bits of texture and things

00:07:48.940 --> 00:07:55.400
all in a specific shape that is unique to, uh, well, certainly to a human face, but even unique to me

00:07:55.400 --> 00:07:58.060
right, because we're capable of distinguishing between different people

00:07:58.220 --> 00:08:01.920
So, this kernel window will go down to this pixel here

00:08:01.920 --> 00:08:05.320
ok, so this will slide about this image and produce this output image

00:08:05.320 --> 00:08:07.780
and then the next one will do the same, and the next one will do the same

00:08:08.000 --> 00:08:10.320
Then we do the same thing on this one

00:08:10.320 --> 00:08:12.200
let me do it in a different pen so we can see better.

00:08:12.200 --> 00:08:13.820
Here's my red kernel convolution

00:08:13.820 --> 00:08:17.140
and this slides about and produces another image,

00:08:17.680 --> 00:08:21.600
which is a slight combination of, maybe, corners and edges

00:08:21.600 --> 00:08:25.060
or something. I mean, this second level, it's not gonna be too abstract, but we'll get the idea

00:08:25.060 --> 00:08:27.640
So there's gonna be some sort of shape that's gonna be sort of...

00:08:27.640 --> 00:08:30.260
It's not gonna make much sense to us, but it'll make some sense to this machine.

00:08:30.260 --> 00:08:34.080
And there'll be another set of these, so there'll be lots of these, right, going back

00:08:34.080 --> 00:08:40.140
All of these will look different and be some different representation of my face transformed in some way, to be useful

00:08:40.140 --> 00:08:45.820
And again, I haven't picked these, these have been learned, just like a normal deep learning algorithm

00:08:45.820 --> 00:08:49.640
So I haven't had to say, "I definitely think edges are important for this"

00:08:49.640 --> 00:08:50.960
cause I don't know for sure.

00:08:50.960 --> 00:08:55.480
So this goes on, and we keep doing this, and sometimes we also downsample the size of these images,

00:08:55.480 --> 00:08:58.340
just to save memory, ok, but we won't dwell on that too much.

00:08:58.340 --> 00:09:04.000
And, because of the way that we downsample, and the way that sometimes these convolution operations slightly shrink the image,

00:09:04.000 --> 00:09:05.720
cause they don't go all the way to the edge, right

00:09:05.720 --> 00:09:09.780
If you've got a 5 by 5 kernel, you can't go to the edge 2 pixels cause you're going off the edge

00:09:09.780 --> 00:09:12.040
so we don't worry about that, we just get slightly smaller

00:09:12.040 --> 00:09:17.480
In the end, we end up with a much smaller image, and lots of features going all the way back

00:09:17.480 --> 00:09:21.860
So these are my different convolutions of convolutions, of convolutions, of convolutions

00:09:21.860 --> 00:09:25.880
And each one will look different, and represent something different

00:09:25.880 --> 00:09:26.480
and we don't know what that is.

00:09:26.480 --> 00:09:30.760
So this one, could be highlighted when it's a face in the middle, or it could be dark when there isn't

00:09:30.760 --> 00:09:34.900
This one might be highlighted when there's an ear at a certain position, and so on.

00:09:34.900 --> 00:09:38.680
Eventually, these will get down to being just one pixel, and very very long

00:09:38.680 --> 00:09:42.760
So essentially what we've done there is we completely removed the spatial dimension.

00:09:42.760 --> 00:09:49.100
There's no more spatial information left, we don't know where anything is. But we know what it is, because it's listed in all these features.

00:09:49.620 --> 00:09:52.020
These now are our neurons at the end.

00:09:52.020 --> 00:09:57.760
So we have a couple more layers that point to these, and then finally, we have one at the end that says

00:09:57.760 --> 00:09:59.580
"Is this a picture of Mike's face?"

00:09:59.580 --> 00:10:05.160
And it produces a 1 if it is, and a 0 if it isn't. And then what we do is, just like a normal network, we train it.

00:10:05.160 --> 00:10:10.560
So we say, "here's a picture of me", ok , so this should be a 1. And let's say it's 0.5.

00:10:10.560 --> 00:10:15.900
cause it's kind of random. So we adjust these weights, and we adjust the weights inside all these kernel convolutions.

00:10:15.900 --> 00:10:17.660
[offscreen] So does that adjustment happen manually?

00:10:18.020 --> 00:10:27.520
No, it happens using a, uh, well, it's coded in, um, but it's usually performed by a library, and it's using a process called back propagation.

00:10:27.520 --> 00:10:31.800
So what we do is we basically predict what direction we have to move the weights in to improve our output,

00:10:31.800 --> 00:10:33.680
and then we move them over slightly in that direction.

00:10:33.680 --> 00:10:36.480
And we have to do it in reverse order, because these ones

00:10:36.480 --> 00:10:40.720
depend on these ones, depend on these ones, and vice versa, what we do is we say,

00:10:40.720 --> 00:10:43.560
well, look, given that I've said it's 0.5 chance of Mike and we want a 1,

00:10:43.560 --> 00:10:47.400
how do I change these weights here to get slightly closer to 1?

00:10:47.400 --> 00:10:51.060
and I do it. And then I say, "how do I change these again to do even better?" and so on

00:10:51.060 --> 00:10:54.380
and then I work my way back, ok, that kind of maths we're not gonna go into, right.

00:10:54.380 --> 00:10:56.480
A lot of these things are, are implemented in libraries

00:10:56.480 --> 00:11:02.440
So as a researcher, I mean, much as I'd like to implement some of these things, it takes quite a long time

00:11:02.440 --> 00:11:04.380
just because programming takes a while, right, and

00:11:04.380 --> 00:11:08.680
And, it's better for me just to apply these things and get good results

00:11:08.680 --> 00:11:13.860
than it is for me to reinvent the wheel all the time, constantly, if everyone was programming the same things over and over again,

00:11:13.860 --> 00:11:14.620
no one would get anything done

00:11:14.620 --> 00:11:19.420
So, I'd have to start by programming up Linux, to get, to get, I'm not claiming I can, by the way,

00:11:19.420 --> 00:11:23.020
and, and so on. So, you know, let's not reinvent the wheel.

00:11:23.020 --> 00:11:27.220
Um, so I do this, I send in, let's say 1000 pictures, 500 of which are me

00:11:27.220 --> 00:11:29.320
you know, so I've been to a photo shoot or something, right

00:11:29.320 --> 00:11:30.740
and 500 of which are not.

00:11:30.740 --> 00:11:35.280
And I train it so the convolutions and these weights on the output are such that

00:11:35.280 --> 00:11:37.780
it gets 1 when it's a picture of me and 0 when it isn't.

00:11:37.780 --> 00:11:42.780
And then I can look at these convolutions and say "what is it about me that's distinctive?"

00:11:43.680 --> 00:11:48.480
And it's probably gonna be finding, um, you know, weird shapes on my face, right

00:11:48.480 --> 00:11:52.240
cause it's a bit of a weird shape, so it - things that are unique to me

00:11:52.240 --> 00:11:55.620
Now in a more general situation, there's a big database called ImageNet

00:11:55.620 --> 00:11:59.160
They have a competition every year to see who can classify these images the best.

00:11:59.600 --> 00:12:02.160
So dogs, cats, planes, trees, and so on

00:12:02.160 --> 00:12:05.300
OK, they're all in there, and there's a thousand or so images of each

00:12:05.300 --> 00:12:10.060
right, so, we have a really big network that's much bigger than this little one I drew

00:12:10.060 --> 00:12:14.380
and we say, "right, let's throw millions of images at this", right, thousands of cats, thousands of dogs

00:12:14.380 --> 00:12:16.620
and we have lots more outputs than just the one,

00:12:16.620 --> 00:12:19.400
and we say "what is it?" and it says "it's a dog" and it is. *dog bark*

00:12:19.400 --> 00:12:21.400
Convolutional neural networks have been around for a little while

00:12:21.400 --> 00:12:28.380
but, they've really started to be big in about 2012 when it - when someone came along, applied one of these to ImageNet, and got incredible results.

00:12:28.380 --> 00:12:32.700
And so on and so forth. And now there's this big push and everyone's trying to get even better results, and even better results.

00:12:32.700 --> 00:12:38.220
Now, I work on more of the applied end of computer science, so I'm more interested in how this affects plant science and things like this

00:12:38.220 --> 00:12:39.420
So that's what we're working on.

00:12:40.420 --> 00:12:45.140
Um, but, the kind of results we're seeing are really really impressive

00:12:45.140 --> 00:12:51.720
So, I mean, case in point, I've done, I've done some root tip detection, so detection of root tips in images, right, of plants

00:12:52.020 --> 00:12:56.100
and, um, I've got some software that I've already programmed

00:12:56.100 --> 00:13:00.220
and I've kind of done a low-level feature detector approach to detecting root tips

00:13:00.220 --> 00:13:05.120
and it's about 70%, ok, which is what you would expect, because maybe some root hair gets confused as to root tip

00:13:05.120 --> 00:13:10.840
or a bit of blotch of dirt, or maybe there's just two root tips really close together and it gets confused

00:13:11.960 --> 00:13:15.160
This, the CNN that I trained, um, is 98% accurate

00:13:15.280 --> 00:13:17.680
And it finds them with 99% accuracy.

00:13:17.680 --> 00:13:19.700
It doesn't make many mistakes.

00:13:19.700 --> 00:13:21.620
And that's over thousands of images.

00:13:21.620 --> 00:13:25.340
[offscreen] So does that mean the work you've done already just goes out the window?

00:13:26.460 --> 00:13:29.980
Yep. Uh, no, to an extent, yes, and to an extent, no.

00:13:29.980 --> 00:13:33.140
You need expertise to be able to craft a network and train it and prepare the images.

00:13:33.140 --> 00:13:39.600
And there's obviously work to be done, and there's some disagreement over how much of a problem you can solve with a convolutional neural network

00:13:39.600 --> 00:13:43.720
So, there are lots more things you can do with roots beyond finding tips.

00:13:43.720 --> 00:13:45.940
Can you do all of them with a convolutional neural network?

00:13:45.940 --> 00:13:48.320
I don't know, we'll see. Are we trying, but, we'll see.

00:13:48.320 --> 00:13:55.200
Maybe not. So maybe what you do is you use this as a tool, just like other machine learning algorithms, within a package that does lots of other things as well.

00:13:55.200 --> 00:14:00.760
On the other hand, if you're just doing cat and dog detection, you might as well use a CNN, cause it's gonna do better than anything else.

00:14:03.740 --> 00:14:09.160
The other purpose for ways the botnet can use its parts is for distributed computing [fades out]

00:14:09.160 --> 00:14:16.120
[fades in] Now, some objects obviously are more amenable to this than others, but the more images we get, the better it is. There's no depth involved here at all, ok

