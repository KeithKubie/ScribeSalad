WEBVTT
Kind: captions
Language: en

00:00:00.800 --> 00:00:03.560
LAGOS MOLNAR: In today's
world, video is everywhere.

00:00:03.560 --> 00:00:06.880
You are an app developer, and
you want to wow your users.

00:00:06.880 --> 00:00:09.850
But how do you create the
detailed, fine-tuned media

00:00:09.850 --> 00:00:12.250
magic that you have envisioned?

00:00:12.250 --> 00:00:14.830
I am Lajos, from the Android
Video Framework Team,

00:00:14.830 --> 00:00:17.510
and I'm going to talk about
how to create great, even

00:00:17.510 --> 00:00:20.790
impressive multimedia
experiences on Android.

00:00:20.790 --> 00:00:23.856
Sure, Android has great
high-level media APIs.

00:00:23.856 --> 00:00:26.480
But if you want to do something
other than playing or capturing

00:00:26.480 --> 00:00:28.860
video, or you need a
feature that is not

00:00:28.860 --> 00:00:31.660
supported by MediaPlayer
or MediaRecorder,

00:00:31.660 --> 00:00:33.420
you will need to
get down and dirty

00:00:33.420 --> 00:00:35.020
with the nuts and
bolts of Android

00:00:35.020 --> 00:00:37.860
Media-- the low-level APIs.

00:00:37.860 --> 00:00:39.930
They consist of
the MediaCodec that

00:00:39.930 --> 00:00:42.940
is used to both decode--
compress audio and video--

00:00:42.940 --> 00:00:46.220
and to encode raw
audio and video.

00:00:46.220 --> 00:00:48.760
MediaExtractor parses
media container files

00:00:48.760 --> 00:00:50.810
into audio and video packets.

00:00:50.810 --> 00:00:54.250
Conversely, MediaMuxer packages
audio and video packets

00:00:54.250 --> 00:00:56.120
into media container.

00:00:56.120 --> 00:00:58.880
MediaCrypto and
MediaDRM are used

00:00:58.880 --> 00:01:00.980
to decrypt protected
media content

00:01:00.980 --> 00:01:03.370
and to manage the
digital rights.

00:01:03.370 --> 00:01:06.220
You can do everything that
MediaPlayer and MediaRecorder

00:01:06.220 --> 00:01:08.220
does using the low level APIs.

00:01:08.220 --> 00:01:10.360
I know it, because we did it.

00:01:10.360 --> 00:01:12.400
For example, to
build a media player,

00:01:12.400 --> 00:01:15.850
you need two MediaCodec
objects and a MediaExtractor

00:01:15.850 --> 00:01:18.670
to parse the individual
audio and video packets out

00:01:18.670 --> 00:01:20.060
of the data source.

00:01:20.060 --> 00:01:23.430
To play back secure content,
you will need a MediaDRM object

00:01:23.430 --> 00:01:25.250
to manage the secure session.

00:01:25.250 --> 00:01:27.120
And you will need to
provide a MediaCrypto

00:01:27.120 --> 00:01:29.890
object to the video decoder.

00:01:29.890 --> 00:01:32.250
You can implement
MediaRecorder using

00:01:32.250 --> 00:01:34.850
two MediaCodecs
and a MediaMuxer.

00:01:34.850 --> 00:01:37.100
In fact, you can
build a wide variety

00:01:37.100 --> 00:01:38.860
of multimedia applications.

00:01:38.860 --> 00:01:41.340
And, might I add,
quite cool ones.

00:01:41.340 --> 00:01:42.940
Then you combine
these media blocks

00:01:42.940 --> 00:01:46.250
with others, such as
textures and texture shaders.

00:01:46.250 --> 00:01:49.510
For example, this is how you
could do video editing as it is

00:01:49.510 --> 00:01:52.610
done in Auto Awesome
Movie in Google+.

00:01:52.610 --> 00:01:55.840
Decode the two source
videos onto GL Textures.

00:01:55.840 --> 00:01:58.740
Composite them
using GLES Shaders

00:01:58.740 --> 00:02:01.610
onto the input surface
of a video encoder.

00:02:01.610 --> 00:02:02.500
Viola.

00:02:02.500 --> 00:02:03.840
That was easy.

00:02:03.840 --> 00:02:06.580
So how does MediaCodec work?

00:02:06.580 --> 00:02:09.250
MediaCodec, at its core,
is a signal processor

00:02:09.250 --> 00:02:11.900
that produces output
data from input data.

00:02:11.900 --> 00:02:14.120
It is processing
data asynchronously

00:02:14.120 --> 00:02:16.790
and uses a set of input
and output buffers.

00:02:16.790 --> 00:02:20.040
At a simplistic level, the
client requests an empty input

00:02:20.040 --> 00:02:22.580
buffer, fills it up
with data, and sends it

00:02:22.580 --> 00:02:24.960
to the codec for processing.

00:02:24.960 --> 00:02:28.470
The codec uses up the
data and transforms it

00:02:28.470 --> 00:02:31.230
into one of its
empty output buffers.

00:02:31.230 --> 00:02:34.730
Finally, the client requests
a filled output buffer,

00:02:34.730 --> 00:02:38.480
consumes its contents, and
sends it back to the codec.

00:02:38.480 --> 00:02:41.910
The real operation of a codec
is a little more complex

00:02:41.910 --> 00:02:44.950
as each of these operations
is happening in parallel.

00:02:44.950 --> 00:02:47.740
Also, codecs that
produce raw video buffers

00:02:47.740 --> 00:02:50.340
can be connected to a surface.

00:02:50.340 --> 00:02:52.650
Let's go over the
states of MediaCodec.

00:02:52.650 --> 00:02:54.780
When you create a
codec, it starts

00:02:54.780 --> 00:02:56.610
in the uninitialized state.

00:02:56.610 --> 00:02:58.920
First, you need to configure it.

00:02:58.920 --> 00:03:02.370
When you call Start, the codec
moves to the flushed state

00:03:02.370 --> 00:03:04.820
where it holds all the buffers.

00:03:04.820 --> 00:03:06.870
As soon as you dequeue
an input buffer,

00:03:06.870 --> 00:03:09.310
the codec moves to
the running state.

00:03:09.310 --> 00:03:11.850
It spends most of its
useful life here processing

00:03:11.850 --> 00:03:14.120
input buffers,
generating output.

00:03:14.120 --> 00:03:17.250
If you queue an input buffer
with the End of Stream flag,

00:03:17.250 --> 00:03:19.830
the codec moves to the
End of Stream state.

00:03:19.830 --> 00:03:23.220
Here it no longer accepts
further input buffers,

00:03:23.220 --> 00:03:26.030
but still generates output
until the End of Stream buffer

00:03:26.030 --> 00:03:27.320
is reached.

00:03:27.320 --> 00:03:29.210
You can move back to
the flushed states

00:03:29.210 --> 00:03:32.970
from any of the executing
states by calling Flush.

00:03:32.970 --> 00:03:36.110
Or call Stop to go back
to the configure state.

00:03:36.110 --> 00:03:38.230
If you call Reset
from any state,

00:03:38.230 --> 00:03:41.360
the codec moves back to
the uninitialized state.

00:03:41.360 --> 00:03:44.510
On rare occasions, the
codec may encounter an error

00:03:44.510 --> 00:03:46.140
and move to the error state.

00:03:46.140 --> 00:03:48.500
This is communicated
using an invalid return

00:03:48.500 --> 00:03:53.160
value from a queueing operation
or sometimes via an exception.

00:03:53.160 --> 00:03:56.200
Call Reset to make the
codec usable again.

00:03:56.200 --> 00:03:57.990
After you finish
using a codec, you

00:03:57.990 --> 00:04:00.120
have to release it
by calling Release.

00:04:00.120 --> 00:04:01.620
Boom.

00:04:01.620 --> 00:04:03.650
By default, codecs
use [? buffers ?]

00:04:03.650 --> 00:04:05.620
for both input and output data.

00:04:05.620 --> 00:04:07.820
However, video codecs
can be configured

00:04:07.820 --> 00:04:12.070
to use hardware accelerated
surface buffers for raw video.

00:04:12.070 --> 00:04:14.070
This is the preferred
mode to operate hardware

00:04:14.070 --> 00:04:16.380
accelerated video
codecs as it allows

00:04:16.380 --> 00:04:18.529
them to work without memcpy.

00:04:18.529 --> 00:04:21.190
In either mode, MediaCodec
uses buffer indices

00:04:21.190 --> 00:04:22.610
to refer to buffers.

00:04:22.610 --> 00:04:24.690
You get the index of
an empty input buffer

00:04:24.690 --> 00:04:26.800
by calling Dequeue Input Buffer.

00:04:26.800 --> 00:04:28.860
You can then fill
the buffer with data,

00:04:28.860 --> 00:04:32.250
apply a time stamp or flags,
and sand in to the codec

00:04:32.250 --> 00:04:34.320
by using Queue Input Buffer.

00:04:34.320 --> 00:04:37.700
To get a filled output buffer,
call Dequeue Output Buffer

00:04:37.700 --> 00:04:39.140
to get its index.

00:04:39.140 --> 00:04:41.410
Note how to time stamp is
propagated from the input

00:04:41.410 --> 00:04:42.310
buffer.

00:04:42.310 --> 00:04:45.620
If using [? buffers, ?] you
can then process the data

00:04:45.620 --> 00:04:49.040
and release the buffer by
calling Release Output Buffer.

00:04:49.040 --> 00:04:50.790
A couple of cautions.

00:04:50.790 --> 00:04:54.520
Although buffers are referred
to by indices in a buffer array,

00:04:54.520 --> 00:04:57.150
not all indices may be
valid so there is really

00:04:57.150 --> 00:05:00.470
no indication of the
number of buffers used.

00:05:00.470 --> 00:05:02.950
All output buffers
are read only.

00:05:02.950 --> 00:05:05.360
You should only access
a buffer by your holding

00:05:05.360 --> 00:05:09.720
onto it with Dequeue and
Queue, or Release Buffer calls.

00:05:09.720 --> 00:05:11.960
Though buffers are
handled asynchronously,

00:05:11.960 --> 00:05:13.940
release each buffer promptly.

00:05:13.940 --> 00:05:17.720
Doing otherwise may
stall some codecs.

00:05:17.720 --> 00:05:21.150
As mentioned before, you
can set up a video encoder

00:05:21.150 --> 00:05:24.080
to process hardware
accelerated surface input.

00:05:24.080 --> 00:05:28.150
You do this by calling Create
Input Surface after Configure.

00:05:28.150 --> 00:05:29.700
Pass this surface
to the producer

00:05:29.700 --> 00:05:33.410
of the video frames which will
now directly talk to the codec.

00:05:33.410 --> 00:05:35.440
Similarly to the
normal operation,

00:05:35.440 --> 00:05:38.280
the producer will request
and empty input buffer,

00:05:38.280 --> 00:05:40.530
fill it with data,
apply your time stamp,

00:05:40.530 --> 00:05:43.650
and send it back to the
codec for processing.

00:05:43.650 --> 00:05:45.640
When you want to stop
the encoding session,

00:05:45.640 --> 00:05:49.410
call Signal End of Input Stream
to apply the end of stream flag

00:05:49.410 --> 00:05:52.260
to effectively the
last frame received.

00:05:52.260 --> 00:05:55.300
Likewise, you can
configure video decoders

00:05:55.300 --> 00:05:58.020
to use hardware
accelerated output buffers

00:05:58.020 --> 00:06:01.290
by specifying an output
surface in Configure.

00:06:01.290 --> 00:06:03.540
Use the familiar
Dequeue Output Buffer

00:06:03.540 --> 00:06:06.130
to get the index of a
filled output buffer.

00:06:06.130 --> 00:06:09.450
However, in contrast to
[? buffers, ?] you cannot

00:06:09.450 --> 00:06:12.770
inspect the contents of
surface output buffers.

00:06:12.770 --> 00:06:14.910
You can only decide
whether or not

00:06:14.910 --> 00:06:17.890
to send a buffer on
to the output surface.

00:06:17.890 --> 00:06:21.300
To discard it, call Release
Output Buffer with render

00:06:21.300 --> 00:06:22.570
set to False.

00:06:22.570 --> 00:06:25.350
Or sets render to
True to display

00:06:25.350 --> 00:06:28.230
the buffer on the
output surface.

00:06:28.230 --> 00:06:31.630
In the L Developer Preview,
the added support to optionally

00:06:31.630 --> 00:06:35.830
set the time stamp of a surface
buffer about to be rendered.

00:06:35.830 --> 00:06:39.460
If set, some buffer consumers
will wait for a time stamp

00:06:39.460 --> 00:06:43.230
to pass before
using that buffer.

00:06:43.230 --> 00:06:45.010
We have recently
added a few features

00:06:45.010 --> 00:06:47.030
to make MediaCodec more useful.

00:06:47.030 --> 00:06:49.410
One of these, as
demonstrated briefly before,

00:06:49.410 --> 00:06:51.370
is the ability to
precisely schedule

00:06:51.370 --> 00:06:53.490
the display of video frames.

00:06:53.490 --> 00:06:56.500
Up until now, A/V sync has
been difficult to achieve.

00:06:56.500 --> 00:06:58.530
With the addition of
surface time stamp

00:06:58.530 --> 00:07:01.860
support in MediaCodec and
high-resolution time stamp

00:07:01.860 --> 00:07:05.000
support an AudioTrack, this
is now straightforward.

00:07:05.000 --> 00:07:09.000
Scheduling of video frames
only works on surface view.

00:07:09.000 --> 00:07:12.860
Use the precise audio time stamp
to calculate the exact system

00:07:12.860 --> 00:07:16.510
time when you want the video
frame to appear on the display.

00:07:16.510 --> 00:07:20.080
Then set the time stamp by
calling Release Output Buffer

00:07:20.080 --> 00:07:22.860
about two VSYNC periods prior.

00:07:22.860 --> 00:07:25.060
The frame will be shown
at the first VSYNC

00:07:25.060 --> 00:07:27.840
after the requested time stamp.

00:07:27.840 --> 00:07:29.800
Another recent
improvement to MediaCodec

00:07:29.800 --> 00:07:32.330
is the support for
adaptive playback.

00:07:32.330 --> 00:07:35.930
Adapted playback is an optional
feature of video decoders

00:07:35.930 --> 00:07:39.440
that enables seamless change
in resolution during playback.

00:07:39.440 --> 00:07:42.530
Whereas the client can
start to feed the decoder,

00:07:42.530 --> 00:07:45.960
input video frames of a new
resolution, and the resolution

00:07:45.960 --> 00:07:48.850
of the output buffers
change automatically

00:07:48.850 --> 00:07:50.810
and without a significant gap.

00:07:50.810 --> 00:07:53.160
It is only supported if
the codec is configured

00:07:53.160 --> 00:07:56.180
to use hardware accelerated
surface buffers.

00:07:56.180 --> 00:07:58.930
If a codec does not
support adaptive playback,

00:07:58.930 --> 00:08:00.910
you can still change
the resolution.

00:08:00.910 --> 00:08:03.660
First, mark the last frame
of the old resolution

00:08:03.660 --> 00:08:05.760
with an End of Stream flag.

00:08:05.760 --> 00:08:08.440
Then wait until all
frames are decoded.

00:08:08.440 --> 00:08:10.950
Then stop the codec,
or simply flush it

00:08:10.950 --> 00:08:13.900
if using API level 19 or higher.

00:08:13.900 --> 00:08:16.520
Then configure it for
the new resolution.

00:08:16.520 --> 00:08:19.600
Finally, start the codec again.

00:08:19.600 --> 00:08:20.940
Wow.

00:08:20.940 --> 00:08:24.080
And I used to think
that was convenient.

00:08:24.080 --> 00:08:26.980
Before you can rely on the
adaptive playback features,

00:08:26.980 --> 00:08:28.930
you need to verify
that it is supported

00:08:28.930 --> 00:08:33.350
by the codec you are using by
calling [INAUDIBLE] Feature

00:08:33.350 --> 00:08:37.100
Supported API.

00:08:37.100 --> 00:08:40.080
You enable adaptive playback
during codec configuration

00:08:40.080 --> 00:08:43.330
using two special keys,
max width and max height.

00:08:43.330 --> 00:08:45.780
These for a hint for the
maximum resolution that

00:08:45.780 --> 00:08:48.080
has to be supported
by the codec.

00:08:48.080 --> 00:08:50.350
If the configuration
succeeds, the codec

00:08:50.350 --> 00:08:56.770
is expected to switch
resolutions smaller

00:08:56.770 --> 00:08:58.650
than the hinge in
a seamless fashion.

00:08:58.650 --> 00:09:01.830
Switching to larger resolutions
is still possible dynamically,

00:09:01.830 --> 00:09:03.590
but it may not be seamless.

00:09:03.590 --> 00:09:06.130
Also, if you try to switch
to a resolution that is not

00:09:06.130 --> 00:09:09.460
supported by the codec, it
will enter its error state.

00:09:09.460 --> 00:09:12.080
A word of advice,
to conserve memory,

00:09:12.080 --> 00:09:13.980
use the smallest
resolution hinge

00:09:13.980 --> 00:09:16.090
required for your use case.

00:09:16.090 --> 00:09:18.760
To change the video resolution
while the codec is running,

00:09:18.760 --> 00:09:22.220
simply cue a SYNC frame
using Queue Input Buffer.

00:09:22.220 --> 00:09:24.430
A SYNC frame is a
special key frame

00:09:24.430 --> 00:09:26.640
that also contains the
configuration change

00:09:26.640 --> 00:09:27.300
parameters.

00:09:27.300 --> 00:09:31.530
For H264, this means
that the SPS and PPS

00:09:31.530 --> 00:09:35.316
are supplied together with the
IDR frame in a single buffer.

00:09:35.316 --> 00:09:36.290
Woof.

00:09:36.290 --> 00:09:38.220
That was a lot of information.

00:09:38.220 --> 00:09:40.700
We have published a streaming
video player example

00:09:40.700 --> 00:09:42.830
on the developer website
that demonstrates

00:09:42.830 --> 00:09:46.550
the use of the low level APIs
as well as adaptive playback.

00:09:46.550 --> 00:09:48.210
There's also video
by [? Ollie ?]

00:09:48.210 --> 00:09:51.210
that describes this
sample in detail.

00:09:51.210 --> 00:09:53.360
Be sure to check them out.

00:09:53.360 --> 00:09:55.530
And for those of
you who want more,

00:09:55.530 --> 00:09:57.890
the low-level
media APIs are also

00:09:57.890 --> 00:10:00.190
available in the Android NDK.

00:10:00.190 --> 00:10:03.390
These CAPIs mimic
the Java ones and are

00:10:03.390 --> 00:10:07.480
available for both 32 and
64-bit native binaries.

00:10:07.480 --> 00:10:08.940
Sweet.

00:10:08.940 --> 00:10:11.134
With that, I must say farewell.

00:10:11.134 --> 00:10:13.300
I leave you with the challenge
of building something

00:10:13.300 --> 00:10:17.920
fantastic for your users using
the Media Building Blocks.

00:10:17.920 --> 00:10:21.040
For more information, visit
the Android developer website

00:10:21.040 --> 00:10:24.540
for API reference, guides,
and sample applications.

00:10:24.540 --> 00:10:26.790
Thank you for your time.

