WEBVTT
Kind: captions
Language: en

00:00:00.034 --> 00:00:03.423
All right, it's time to wrap up. Here's what I hope you've taken away from this unit.

00:00:03.423 --> 00:00:09.640
Remember APOD--Analyze, Parallelize, Optimize, and Deploy.

00:00:09.640 --> 00:00:15.901
The important points here are do profile-guided optimization at every step and deploy early and often

00:00:15.901 --> 00:00:19.391
rather than optimizing forever in a vacuum. I can't emphasize this enough.

00:00:19.391 --> 00:00:22.838
Optimization takes effort and often complicates code,

00:00:22.838 --> 00:00:27.580
so optimize only where and when you need it and go around this cycle multiple times.

00:00:27.580 --> 00:00:30.399
Now, most codes are limited by memory bandwidth.

00:00:30.399 --> 00:00:33.765
So compare your performance to the theoretical peak bandwidth

00:00:33.765 --> 00:00:36.340
and if it's lacking, see what you can do about it.

00:00:36.340 --> 00:00:39.928
Things that will help improve in order from most to least important

00:00:39.928 --> 00:00:45.088
are assure sufficient occupancy, make sure you have enough threads to keep the machine busy.

00:00:45.088 --> 00:00:48.628
This doesn't mean having as many threads as you can possibly fit on the machine,

00:00:48.628 --> 00:00:51.623
but you do need enough that the machine is basically busy.

00:00:51.623 --> 00:00:54.465
Coalesce global memory accesses.

00:00:54.465 --> 00:00:59.004
Really strive to see if you can find a way to cast your algorithm so that you achieve perfect coalescing.

00:00:59.004 --> 00:01:02.181
And if you can't, consider whether you can do a transpose operation

00:01:02.181 --> 00:01:05.711
or something that will get poor coalescing once

00:01:05.711 --> 00:01:11.458
but then put the data into a memory form where all your subsequent accesses will get good coalescing.

00:01:11.458 --> 00:01:13.493
Remember Little's law.

00:01:13.493 --> 00:01:15.360
In order to achieve the maximum bandwidth,

00:01:15.360 --> 00:01:18.513
you may need to reduce the latency between your memory accesses.

00:01:18.513 --> 00:01:23.292
And so for example, we saw that in 1 case we spent too much time waiting at barriers.

00:01:23.292 --> 00:01:25.085
By reducing the number of threads in a block,

00:01:25.085 --> 00:01:28.270
we are able to reduce the average time spent waiting at a barrier

00:01:28.270 --> 00:01:31.985
and help saturate that global memory bandwidth.

00:01:31.985 --> 00:01:36.212
We've talked about minimizing the branch divergence experience by threads.

00:01:36.212 --> 00:01:39.910
Remember that this really applies to threads that diverge within a warp.

00:01:39.910 --> 00:01:44.590
If the warps themselves diverge--in other words, if all the threads within a warp take the same branch,

00:01:44.590 --> 00:01:48.287
go in the same code path--then that comes for free.

00:01:48.287 --> 00:01:51.345
There's no additional penalty for threads in different warps diverging.

00:01:51.345 --> 00:01:56.480
It's only when threads within a warp diverge that you have to execute both sides of the branch.

00:01:56.480 --> 00:02:00.178
As a rule, you should generally try to avoid branchy code--

00:02:00.178 --> 00:02:03.874
code with of lots of if statements, switch statements, and so on.

00:02:03.874 --> 00:02:08.342
And you should generally be thinking about avoiding thread workload imbalance.

00:02:08.342 --> 00:02:10.685
In other words, if you have loops in your kernels

00:02:10.685 --> 00:02:14.468
that might execute a very different number of times between threads,

00:02:14.468 --> 00:02:18.018
then that 1 thread that's taking much longer than average thread

00:02:18.018 --> 00:02:20.729
can end holding the rest of the threads hostage.

00:02:20.729 --> 00:02:23.829
All that said, don't let a little bit of thread divergence freak you out.

00:02:23.829 --> 00:02:27.594
Remember we analyzed a real-world example of dealing with boundary conditions

00:02:27.594 --> 00:02:31.333
at the edge of an image and figured out that in fact the if statements

00:02:31.333 --> 00:02:35.699
to guard the edge of the images weren't really costing us very much.

00:02:35.699 --> 00:02:39.276
Only a few warps ended up being divergent.

00:02:39.276 --> 00:02:42.604
If you're limited by the actual computational performance of your kernel

00:02:42.604 --> 00:02:45.574
rather than the time it takes to get the data to and from your kernel,

00:02:45.574 --> 00:02:48.427
then consider using fast math operations.

00:02:48.427 --> 00:02:53.416
This includes things like the intrinsics for sine and cosine and so forth.

00:02:53.416 --> 00:02:56.157
They go quite a bit faster than their math.h counterparts,

00:02:56.157 --> 00:02:58.931
at the cost of a few bits of precision.

00:02:58.931 --> 00:03:02.282
And remember that when you use double precision it should be on purpose.

00:03:02.282 --> 00:03:08.164
So just typing the literal 3.14, well, that's a 64-bit double precision number,

00:03:08.164 --> 00:03:10.571
and the compiler will treat it as such,

00:03:10.571 --> 00:03:15.012
whereas typing 3.14f tells the compiler, hey, this is a single precision operation,

00:03:15.012 --> 00:03:19.157
you don't have to promote everything I multiply this by or add this to

00:03:19.157 --> 00:03:21.497
to be a double precision number.

00:03:21.497 --> 00:03:24.552
Finally, if you're limited by host device memory transfer time,

00:03:24.552 --> 00:03:29.863
consider using streams and asynchronous memcpys to overlap computation and memory transfers.

00:03:30.647 --> 00:03:33.672
And that's it. Now go forth and optimize your codes.

