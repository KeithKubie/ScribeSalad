Speaker 1:          00:00          In this video, we'll see a simple type of conditional random fields, no that as the linear chain conditional random fields. So we motivated the use of conditional random fields in the context of sequence classification where we want to be able to jointly model the full sequence of labels associated with a sequence of inputs. So, uh, this here is essentially another way of writing the probability of a given value for y one up to y capital k, capital k is the number of elements in the sequence given the first input in this sequence up to the last input in the sequence.

Speaker 1:          00:45          Now let's look at how we, uh, will a particular parametrization of this conditional distribution. So perhaps the most simple, a conditional random fields or CRF is the linear chain a CRF. Now let's see, uh, what is, uh, what we do in the context of a linear CRF, not in regular classification where we don't, uh, assume there is any dependence in our model between, uh, the, uh, the adjacent positions in a sequence. Uh, in this context, it mean it means that our distribution, the probability that our model assigned to a particular sequence why given a sequence of input x will just be the product of the probability that our neural network assigned to each label. Why k given its associated input x k. Now, uh, in the neural network that we remodeled this is that we take the soft max output activation function. So we take the exponential of the pre activation value for the label for which we are evaluating and probability that's why came and then we divide that by normalization constant or partition function, which is just the sum of all potential value of the numerator.

Speaker 1:          02:03          So this would just be the sum of the exponential of DP activation function for any value of Waikiki. And now we are taking the product of this because we are assuming our uh, our model does not model dependencies between adjacent positions in the sequence. And now because the product of the exponential is the exponential of the sum, we can also write this as the exponential of is some over positions, k of the pre activation that the output layer for each label. Why K as predicted. So the reactivation as computed based on the associated input, ex gay and a. And then we also divide by the product of the partition functions or normalization constants here. Now in the linear chain CRF in the context of sequence classification will essentially add a term that will also express a preference for a particular sequences or of AGSM and values for a yk and why k plus one for any position king.

Speaker 1:          03:04          So in other words, the conditional distribution of y given some sequence x, it's going to be the exponential of again, so here and here we have exactly the same term. So this is essentially saying how we think, why k the particular value of why gay that were for which we are valuing your probability is uh, is, is likely a given the input and given the associated input x games. So that's why we have ex gay here. Plus another term which is going to say how much our model likes that, whyK is followed by why k plus one and this so how much we like seeing why k followed by why k plus one is going to be parent tries by Matrix v where it's entry yk Yq plus one uh, tells us how much we liked this particular sequence of yk. Why K plus one.

Speaker 1:          04:01          So just to be clear, why k plus one is a variable and it has a particular value. If we have 10 classes, then why k could be one, two, three up to 10 and similarly why k plus one is also a variable, but it's the variable that represents the label at the next step, k plus one. And it also takes a value from one to 10 and so v, y, k y k plus one if a y k four in this sequence here, if Mikey was equal to say five and then y k plus one was equal to three, then this term would for this particular value of k would look at the entry at the fifth row and the third column of Matrix speed. Okay. So to sum up in a linear chain, a conditional random fields, we keep the per position term that expresses how much we think, why he should be likely given its associated but x, k.

Speaker 1:          04:57          But we add another term which also expresses a preference as to whether we like seeing why k followed by why k plus one for a particular position over position k. And then notice that here also we some from k equals one up to came and as one. So we add our preferences for uh, for each position. Kay. So if we had the sequence of size, say three, and then we had that were valuing the probability for y one equals se two, and then y two equals five, and then wide three equals uh, let's say three, then it means that this term here would correspond to a right, it just here that correspond to v two five Plus v five three. Okay. So if we were to compute this expression and actually this part of the expression for these values for this sequence of wise, then I'd have this term plus district.

Speaker 1:          06:05          So to sum up the probability of a sequence y given x is going to be the exponential of a, some of terms which express a preference about the value for each element in the sequence. Why Gay? Uh, in the sequence of labels given the associated input plus the, uh, some of pairwise preferences between adjacent labels. And so the, probably the is going to be proportional to the exponential of the, some of these, uh, two types of terms. And now because we want a probability distribution that sums to zero, we have to divide by something we call the partition function. Or sometimes people use the name normalization constant. It's a partition function in particular because, uh, it depends, uh, for one thing on x, but it also depends on the parameters of the, uh, of the models for different parameters. We'll get a different value of, of this normalization constant. And what this zed is here is really just the sum of the numerator but over all possible sequence values. Uh, so all possible values of that sequence. Why? So there's actually a number, an exponential number of such sequences. And so a naive method for computing it would be intractable, but, uh, we'll see in a future videos out to actually write a dynamic program for computing the partition function for linear chain conditional random fields.

Speaker 2:          07:34          Yeah.

Speaker 1:          07:35          Now, visually, if you look at the, uh, uh, the, the flow graph for computation. So before in the regular neural net from the input, we'd get a computation graph up to the pre activation layer and there would be no interaction between this computation for say the uh, input that position came on as one and the computation of the neural net at Persia at position k that we computed the pre activation for that particular input. Now what we're adding is a matrix of undirected connections between adjacent position such that the pre, uh, such that essentially the probability will assign here will also depend on the, uh, on what's computed here and so on. And so this is very informal. There's just to illustrate the dependencies here, and this is sort of a informal graph of computations, but we'll formalize this a bit more in the next videos.

Speaker 1:          08:38          What's important to realize that between each adjacent step, we always use the same Matrix v as our set of parameters that control the, uh, interaction between, uh, and the essentially control how we model, uh, observations of each sent in the sequence. So the Matrix we hear is the same as this matrix and so on. And similarly, the set of weights of my neural network. So the matrix w here and be here. And also all the parameters that would be between the input and the pre activation at the output layer. These parameters for this neural net is the same as these, uh, these parameters and as these parameters. So it's sort of the, uh, uh, the same copy of the same neural network, uh, with a shared weight and a, so this will have implications when, for instance, we compute gradients. Uh, it means that we'll get great answers that will affect, uh, told that the gradients, when do backpropagation there will be gradient that will go on this matrix here, which is the same as this here. So we'll have to cumulate gradients for each position. Okay. But this is just to really highlight the fact that we using the same neural network for each position and between adjacent position, we using the same matrix of parameter v. All right? So that's the linear chain conditional random fields.