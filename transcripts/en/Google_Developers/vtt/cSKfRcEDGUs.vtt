WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.972
[MUSIC PLAYING]

00:00:06.375 --> 00:00:07.500
JOSH GORDON: Hey, everyone.

00:00:07.500 --> 00:00:08.484
Welcome back.

00:00:08.484 --> 00:00:09.900
In this episode,
I'll show you how

00:00:09.900 --> 00:00:13.030
to train your own image
classifier starting from just

00:00:13.030 --> 00:00:14.844
a directory of images.

00:00:14.844 --> 00:00:17.010
For example, say you want
to build a classifier that

00:00:17.010 --> 00:00:19.280
can tell the difference
between a picture of a T.

00:00:19.280 --> 00:00:21.214
rex and a triceratops.

00:00:21.214 --> 00:00:22.880
Or maybe you want to
classify a painting

00:00:22.880 --> 00:00:25.490
as being a Monet or Picasso.

00:00:25.490 --> 00:00:27.320
To do that, we'll
work with a code lab

00:00:27.320 --> 00:00:29.410
called TensorFlow for Poets.

00:00:29.410 --> 00:00:31.020
And this is a great
way to get started

00:00:31.020 --> 00:00:34.550
learning about and working
with image classification.

00:00:34.550 --> 00:00:36.390
Now two things off the bat.

00:00:36.390 --> 00:00:38.590
First, this code
lab is high level.

00:00:38.590 --> 00:00:40.800
To train our classifier,
we'll basically just need

00:00:40.800 --> 00:00:42.720
to run a couple of scripts.

00:00:42.720 --> 00:00:45.254
But what's impressive is what
the classifier will create

00:00:45.254 --> 00:00:46.920
is better than what
I could have written

00:00:46.920 --> 00:00:49.300
myself just a few years ago.

00:00:49.300 --> 00:00:51.770
As we go, I'll show you how
the code lab looks in action,

00:00:51.770 --> 00:00:53.470
and I'll add context
and background

00:00:53.470 --> 00:00:55.530
on why it works so well.

00:00:55.530 --> 00:00:57.270
OK, let's get started.

00:00:57.270 --> 00:00:59.680
To train an image classifier
with TensorFlow for Poets,

00:00:59.680 --> 00:01:03.180
we'll only need to provide
one thing-- training data.

00:01:03.180 --> 00:01:06.080
Here that's just
directories full of images.

00:01:06.080 --> 00:01:07.820
My plan is to
create a classifier

00:01:07.820 --> 00:01:10.600
to tell the difference between
five types of flowers--

00:01:10.600 --> 00:01:12.870
roses, sunflowers, and so on.

00:01:12.870 --> 00:01:16.010
And here's what my
training data looks like.

00:01:16.010 --> 00:01:17.880
Notice that I have
five directories, one

00:01:17.880 --> 00:01:20.080
for each type of flower.

00:01:20.080 --> 00:01:22.740
Inside each directory
are lots of pictures.

00:01:22.740 --> 00:01:24.520
And the reason I'm
working with flowers

00:01:24.520 --> 00:01:26.850
is we provided this
data set in the code lab

00:01:26.850 --> 00:01:29.210
so you can get
started right away.

00:01:29.210 --> 00:01:31.660
If you want to use your own
images, say, for dinosaurs

00:01:31.660 --> 00:01:34.050
or paintings, all you need
to do is create a directory

00:01:34.050 --> 00:01:36.040
and fill it with
images from the web.

00:01:36.040 --> 00:01:39.376
We'll need about 100 images
in each directory to start.

00:01:39.376 --> 00:01:42.000
Once we have our training data,
the next thing we'll need to do

00:01:42.000 --> 00:01:43.240
is train our classifier.

00:01:43.240 --> 00:01:45.920
And for that, we'll
use TensorFlow.

00:01:45.920 --> 00:01:48.700
TensorFlow as an open source
machine learning library.

00:01:48.700 --> 00:01:50.620
And it's especially
useful for working

00:01:50.620 --> 00:01:53.916
with a branch of machine
learning called deep learning.

00:01:53.916 --> 00:01:56.290
Deep learning has led to great
results in the last couple

00:01:56.290 --> 00:01:59.830
years, especially in domains
like image classification,

00:01:59.830 --> 00:02:01.830
which we'll be
working with today.

00:02:01.830 --> 00:02:04.240
And here's one reason why.

00:02:04.240 --> 00:02:06.180
Recall that way
back in episode one,

00:02:06.180 --> 00:02:08.509
we talked about telling the
difference between apples

00:02:08.509 --> 00:02:09.630
and oranges.

00:02:09.630 --> 00:02:11.290
We saw it's
impossible to do this

00:02:11.290 --> 00:02:14.730
by hand because there's too
much variation in the world.

00:02:14.730 --> 00:02:17.500
But now we also know that
classifiers take features

00:02:17.500 --> 00:02:18.630
as input.

00:02:18.630 --> 00:02:20.590
And with images,
it's incredibly hard

00:02:20.590 --> 00:02:24.027
to write code to extract
useful features by hand.

00:02:24.027 --> 00:02:25.860
For example, you wouldn't
want to write code

00:02:25.860 --> 00:02:28.850
to detect the texture
of a piece of fruit.

00:02:28.850 --> 00:02:31.150
To get around this,
we use deep learning

00:02:31.150 --> 00:02:34.280
because it has a major advantage
when working with images.

00:02:34.280 --> 00:02:35.470
And it's this.

00:02:35.470 --> 00:02:38.280
You don't need to extract
features manually.

00:02:38.280 --> 00:02:41.750
Instead, you can use the raw
pixels of the image's features,

00:02:41.750 --> 00:02:44.180
and the classifier
will do the rest.

00:02:44.180 --> 00:02:45.930
To see the difference
in our training data

00:02:45.930 --> 00:02:48.170
looks, let's compare
the Iris data set

00:02:48.170 --> 00:02:50.150
with our directories of images.

00:02:50.150 --> 00:02:52.120
In Iris, each
column is a feature

00:02:52.120 --> 00:02:53.417
that describes the flower.

00:02:53.417 --> 00:02:55.500
And you can imagine we
came up with these features

00:02:55.500 --> 00:02:58.870
manually, say, by measuring
the flower with a ruler.

00:02:58.870 --> 00:03:00.850
Now by contrast, here's
our training data

00:03:00.850 --> 00:03:02.710
in TensorFlow for Poets.

00:03:02.710 --> 00:03:05.330
It's just a list
of labeled images.

00:03:05.330 --> 00:03:07.420
And again, a classifier
is just a function.

00:03:07.420 --> 00:03:09.040
f of x equals y.

00:03:09.040 --> 00:03:11.820
Here x is a 2D array of
pixels from the image.

00:03:11.820 --> 00:03:14.170
And y is a label like rose.

00:03:14.170 --> 00:03:16.170
Now when we're talking
about deep learning,

00:03:16.170 --> 00:03:19.440
the classifier we'll be using
is called a neural network.

00:03:19.440 --> 00:03:22.010
At a high level, that's just
another type of classifier,

00:03:22.010 --> 00:03:24.920
like the nearest neighbor
one wrote last time.

00:03:24.920 --> 00:03:26.360
The difference is
a neural network

00:03:26.360 --> 00:03:28.640
can learn more
complex functions.

00:03:28.640 --> 00:03:30.610
In this code lab,
TensorFlow for Poets

00:03:30.610 --> 00:03:33.160
takes care of setting up and
training the neural network

00:03:33.160 --> 00:03:35.290
for you behind the scenes.

00:03:35.290 --> 00:03:37.580
That doesn't mean that
TensorFlow code is any harder

00:03:37.580 --> 00:03:39.780
to write than what
we've seen so far.

00:03:39.780 --> 00:03:42.470
In fact, my favorite way of
writing TensorFlow programs

00:03:42.470 --> 00:03:44.520
is by using TF Learn.

00:03:44.520 --> 00:03:47.250
And TF Learn is a high level
machine learning library

00:03:47.250 --> 00:03:48.960
on top of TensorFlow.

00:03:48.960 --> 00:03:51.530
And the syntax is
similar to scikit-learn

00:03:51.530 --> 00:03:54.364
like we've seen so far.

00:03:54.364 --> 00:03:55.780
For example, here's
a code snippet

00:03:55.780 --> 00:03:58.450
that shows you how to
import a neural network,

00:03:58.450 --> 00:04:01.790
train it, and use it
to classify new data.

00:04:01.790 --> 00:04:05.619
And you can do this using the
skills you've already learned.

00:04:05.619 --> 00:04:07.410
If you want to learn
more, no pun intended,

00:04:07.410 --> 00:04:09.630
about this stuff
right now, I put links

00:04:09.630 --> 00:04:12.520
in the description
you can check out.

00:04:12.520 --> 00:04:14.590
OK, now let's return
to TensorFlow for Poets

00:04:14.590 --> 00:04:16.209
and train our classifier.

00:04:16.209 --> 00:04:18.356
To do that, we'll kick
it off with this script.

00:04:18.356 --> 00:04:19.980
It's covered in detail
in the code lab,

00:04:19.980 --> 00:04:21.899
so I won't say too
much about it here.

00:04:21.899 --> 00:04:24.140
But I will give you
context on two more things

00:04:24.140 --> 00:04:26.020
you might want to know about.

00:04:26.020 --> 00:04:28.030
First, the script
takes about 20 minutes

00:04:28.030 --> 00:04:29.330
to train the classifier.

00:04:29.330 --> 00:04:31.680
Now ask yourself,
is that a long time?

00:04:31.680 --> 00:04:33.710
The answer turns out to be no.

00:04:33.710 --> 00:04:35.570
Under the hood,
TensorFlow for Poets

00:04:35.570 --> 00:04:38.600
isn't actually training a
classifier from scratch.

00:04:38.600 --> 00:04:41.530
Instead, it's starting with
an existing classifier called

00:04:41.530 --> 00:04:42.680
Inception.

00:04:42.680 --> 00:04:45.820
And Inception is one of
Google's best image classifiers.

00:04:45.820 --> 00:04:47.360
And it's open source.

00:04:47.360 --> 00:04:50.200
Whereas we have just a couple
thousand images in our training

00:04:50.200 --> 00:04:54.270
data, Inception was trained
on 1.2 million images

00:04:54.270 --> 00:04:56.490
from 1,000 different categories.

00:04:56.490 --> 00:05:00.160
Training Inception took about
two weeks on a fast desktop

00:05:00.160 --> 00:05:01.920
with eight GPUs.

00:05:01.920 --> 00:05:04.970
In TensorFlow for Poets,
we'll begin with Inception

00:05:04.970 --> 00:05:07.360
and then use a technique
called retraining

00:05:07.360 --> 00:05:09.720
to adjust it to work
with our images.

00:05:09.720 --> 00:05:12.270
This lets us re-use some
of the parameters Inception

00:05:12.270 --> 00:05:15.970
has previously learned so we
can create a new high accuracy

00:05:15.970 --> 00:05:19.510
classifier with far
less training data.

00:05:19.510 --> 00:05:21.810
I'll fast forward til
our training finishes.

00:05:21.810 --> 00:05:25.310
And once we have a trained
classifier, we can try it out.

00:05:25.310 --> 00:05:27.660
To do that, I'll download
this image of a rose

00:05:27.660 --> 00:05:30.100
from Wikimedia Commons
and use our classifier

00:05:30.100 --> 00:05:32.880
to predict what type
of flower it is.

00:05:32.880 --> 00:05:34.840
As we can see, it gets it right.

00:05:34.840 --> 00:05:36.820
And we can see the
confidence distribution

00:05:36.820 --> 00:05:39.370
for the other types
of flowers as well.

00:05:39.370 --> 00:05:41.390
Now keep in mind
our classifier only

00:05:41.390 --> 00:05:43.600
knows about the training
data we've shown it.

00:05:43.600 --> 00:05:45.410
So if we ask it to
classify an image,

00:05:45.410 --> 00:05:48.020
say, of the Roman
Colosseum, it must predict

00:05:48.020 --> 00:05:49.850
that it's a type of flower.

00:05:49.850 --> 00:05:52.910
Hopefully, though, the
confidence will be low.

00:05:52.910 --> 00:05:55.850
Now let me give you one or
two more closing thoughts.

00:05:55.850 --> 00:05:57.630
To train a good
image classifier,

00:05:57.630 --> 00:06:01.300
the name of the game is
diversity and quantity.

00:06:01.300 --> 00:06:03.120
By diversity, I
mean the more images

00:06:03.120 --> 00:06:06.990
of different types of roses we
have, the better off we'll be.

00:06:06.990 --> 00:06:08.760
For example, our
training data includes

00:06:08.760 --> 00:06:12.400
pictures of red, white,
and yellow roses.

00:06:12.400 --> 00:06:14.820
We also have pictures
taken at different angles,

00:06:14.820 --> 00:06:17.630
say, from above or to the side.

00:06:17.630 --> 00:06:20.610
And we've included pictures
of roses in the foreground

00:06:20.610 --> 00:06:22.540
as well as the background.

00:06:22.540 --> 00:06:24.990
Now by quantity, I mean the
more training data we have,

00:06:24.990 --> 00:06:28.320
the better a classifier
we're likely to create.

00:06:28.320 --> 00:06:31.550
There are several hundred
images inside the roses folder.

00:06:31.550 --> 00:06:33.360
That's enough to
retrain Inception.

00:06:33.360 --> 00:06:36.010
And you can probably get
away with even fewer images,

00:06:36.010 --> 00:06:38.480
though your accuracy
might decrease.

00:06:38.480 --> 00:06:39.601
OK, that's it for now.

00:06:39.601 --> 00:06:41.100
As a next step,
you'll probably want

00:06:41.100 --> 00:06:44.370
to dig deeper and try writing
your own TensorFlow code.

00:06:44.370 --> 00:06:47.380
Here's a link to a tutorial that
will show you how to do that.

00:06:47.380 --> 00:06:50.702
And you can use exactly the
same technology we saw here.

00:06:50.702 --> 00:06:52.410
As always, thanks very
much for watching.

00:06:52.410 --> 00:06:53.743
And I'll see you guys next time.

00:06:53.743 --> 00:06:56.070
[MUSIC PLAYING]

