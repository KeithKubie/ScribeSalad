WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.554
♪ (music) ♪

00:00:06.148 --> 00:00:07.468
Hi there, everyone.

00:00:07.468 --> 00:00:10.488
I'm excited to introduce a new project
that we've been working on

00:00:10.929 --> 00:00:17.829
that takes a new project
to improve usability of TensorFlow.

00:00:18.347 --> 00:00:20.267
And we care so much about usability here

00:00:20.267 --> 00:00:22.367
that we're going all the way back
to first principles

00:00:22.367 --> 00:00:24.227
of the computation we're performing.

00:00:24.227 --> 00:00:26.036
But first, why usability?

00:00:27.125 --> 00:00:29.105
I felt that everyone here agrees

00:00:29.105 --> 00:00:32.265
that productivity
in machine learning is critical,

00:00:32.616 --> 00:00:35.676
because it leads to a faster pace
of innovation and progress in our field.

00:00:36.376 --> 00:00:39.366
Of course, we just want to build
beautiful things for TensorFlow users,

00:00:39.366 --> 00:00:41.066
so that's a big piece of it as well.

00:00:41.256 --> 00:00:43.056
But if you look
at machine learning frameworks

00:00:43.056 --> 00:00:44.526
there's two major approaches.

00:00:45.090 --> 00:00:47.470
The most familiar
are the graph building approaches

00:00:47.470 --> 00:00:49.040
where you explicitly define a graph

00:00:49.040 --> 00:00:51.150
and then execute it
to run your computation.

00:00:51.620 --> 00:00:54.790
This is great for performance,
but isn't always the best for usability.

00:00:56.654 --> 00:00:59.884
In contrast, the define-by-run approaches
like eager execution

00:00:59.884 --> 00:01:04.154
aren't always the best for performance,
but allow you to use language control flow

00:01:04.154 --> 00:01:06.714
and other things that make them
a lot easier to use.

00:01:07.074 --> 00:01:08.834
The interesting thing
about both approaches

00:01:08.834 --> 00:01:12.314
is that they're really
about allowing Python to understand

00:01:12.314 --> 00:01:14.724
the difference between
the Tensor computation in your code

00:01:14.724 --> 00:01:18.144
and all the other non-Tensor stuff,
like command line option processing

00:01:18.144 --> 00:01:21.024
and visualization,
and the other things you do.

00:01:22.153 --> 00:01:24.673
So, I think it's interesting
to look at how these actually work.

00:01:25.182 --> 00:01:28.032
In the case of eager execution,
for example, you write your model

00:01:28.359 --> 00:01:30.209
and Python starts out by parsing it,

00:01:30.209 --> 00:01:33.599
and then it feeds every statement
at a time to the interpreter.

00:01:33.599 --> 00:01:35.259
And if it's a Tensor operation,

00:01:35.260 --> 00:01:37.170
it fires up TensorFlow and hands it off,

00:01:37.170 --> 00:01:39.410
and TensorFlow takes care
of your Tensor operations.

00:01:39.410 --> 00:01:41.630
Otherwise, the Python interpreter runs it.

00:01:43.016 --> 00:01:46.136
The key thing about eager execution
and graph building

00:01:46.136 --> 00:01:48.346
is that they're designed
within the constraints

00:01:48.346 --> 00:01:50.666
of what you can do as a Python library.

00:01:51.498 --> 00:01:54.228
Now, if you crack open the compiler

00:01:54.228 --> 00:01:59.118
and look at it from what could we do
with the compiler and language involved,

00:01:59.118 --> 00:02:01.178
there is a whole other set of approaches

00:02:01.178 --> 00:02:03.728
that can be applied to solving 
this particular problem

00:02:04.059 --> 00:02:05.619
and that's what we're doing.

00:02:06.505 --> 00:02:09.105
The cool thing about a compiler
is that after you parse your code,

00:02:09.105 --> 00:02:12.125
the compiler can see the entire program
and all the Tensor ops in it.

00:02:12.133 --> 00:02:15.403
And so what we're doing
is we'll add a new stage to the compiler

00:02:15.403 --> 00:02:17.493
that automatically takes
these Tensor operations

00:02:17.493 --> 00:02:18.788
out of your program,

00:02:18.788 --> 00:02:21.633
builds a standard
TensorFlow graph for you,

00:02:21.633 --> 00:02:23.303
and then hands it to TensorFlow.

00:02:23.303 --> 00:02:24.873
Since it's a standard TensorFlow graph,

00:02:24.873 --> 00:02:27.603
you get full access to all things
that TensorFlow can do,

00:02:27.603 --> 00:02:29.023
including all the devices.

00:02:29.499 --> 00:02:32.279
Now, we think that this approach
is a really great combination,

00:02:32.279 --> 00:02:34.989
because you get all the power
and flexibility of TensorFlow,

00:02:34.989 --> 00:02:38.832
but you also get the usability
of your execution as well.

00:02:39.470 --> 00:02:41.850
But there's a catch.
There's always a catch, right?

00:02:42.116 --> 00:02:44.756
The catch here is that
we can't do this with Python.

00:02:45.018 --> 00:02:47.278
At least, not with
the reliability we expect,

00:02:47.278 --> 00:02:50.838
because it doesn't support
the kind of compiler analysis we need.

00:02:51.247 --> 00:02:52.637
And what do we mean by that?

00:02:52.637 --> 00:02:54.767
The compiler has to be able
to reason about values

00:02:54.767 --> 00:02:56.177
as they flow through your program.

00:02:56.177 --> 00:02:59.007
It has to reason about control flow
and function calls.

00:02:59.007 --> 00:03:02.197
It has to reason about variable aliasing
and other things like that.

00:03:02.204 --> 00:03:05.514
If we can't directly use Python,
then, as users, of course,

00:03:05.514 --> 00:03:08.994
we care a lot about all the nice things
we've come to love about Python,

00:03:09.388 --> 00:03:12.328
including the ability to use
all the standard Python APIs.

00:03:13.565 --> 00:03:14.915
So, I know what you're thinking.

00:03:14.915 --> 00:03:17.855
"Does this mean that we're talking
about doing a new language?"

00:03:18.903 --> 00:03:21.893
That's definitely an approach to solve
the technical requirements we want,

00:03:21.893 --> 00:03:23.003
and if we do new language,

00:03:23.003 --> 00:03:26.373
we can obviously build
all the nice things we want into it.

00:03:26.373 --> 00:03:29.053
But this comes at a cost
because it turns out

00:03:29.053 --> 00:03:32.382
that we'd be foregoing
the benefits of having a community.

00:03:32.382 --> 00:03:36.087
That includes things like tools
and libraries, but also things like books,

00:03:36.087 --> 00:03:37.917
which some people still use.

00:03:39.367 --> 00:03:41.347
And even more significantly,

00:03:41.347 --> 00:03:43.524
this would take years of time to get right

00:03:43.524 --> 00:03:45.764
and machine learning just moves too fast.

00:03:46.524 --> 00:03:48.724
No, we think it's better
to use an existing language.

00:03:48.727 --> 00:03:51.557
But here we have to be careful,
because to do this right,

00:03:51.557 --> 00:03:55.487
we have to make significant improvements
to the compiler and the language

00:03:55.490 --> 00:03:58.350
and we need to be able
to do this in a reasonable amount of time.

00:03:59.355 --> 00:04:02.845
So, of course, this brings us
to the Swift programming language.

00:04:04.734 --> 00:04:07.604
Now, I assume that most of you
are not very familiar with Swift,

00:04:07.604 --> 00:04:09.114
so here's a quick introduction.

00:04:09.591 --> 00:04:12.031
Swift is designed
with a lightweight syntax

00:04:12.239 --> 00:04:16.659
and it's really geared toward
being easy to use and learn.

00:04:17.152 --> 00:04:20.042
Swift draws together best practices
from lots of different places,

00:04:20.042 --> 00:04:22.342
including things
like functional programming and generics.

00:04:23.765 --> 00:04:27.035
Swift builds on LVM, which means, 
of course, that it has an interpreter

00:04:27.035 --> 00:04:30.225
and it has scripting capabilities as well.

00:04:30.768 --> 00:04:33.088
Swift is great in notebook environments,

00:04:33.088 --> 00:04:34.268
and these are really awesome

00:04:34.268 --> 00:04:36.625
when you're interactively
developing in real time.

00:04:37.655 --> 00:04:39.375
Swift is also open source.

00:04:39.826 --> 00:04:43.076
It's portable to lots of platforms
and has a big community of people.

00:04:43.334 --> 00:04:45.214
The number one thing
that's most important to us

00:04:45.214 --> 00:04:50.614
is it has a fully open design environment,
called "Swift Evolution,"

00:04:50.614 --> 00:04:51.884
which allows us to propose

00:04:51.884 --> 00:04:54.884
first-class machine learning
language and compiler features

00:04:54.884 --> 00:04:56.874
directly for integration into Swift.

00:04:57.775 --> 00:05:00.555
If you bring all of this together
that we've been talking about here,

00:05:00.939 --> 00:05:03.909
I'm happy to introduce
Swift for TensorFlow.

00:05:05.205 --> 00:05:08.445
Swift for TensorFlow gives you
the full performance of graphs,

00:05:09.394 --> 00:05:11.194
you can use native language control flow,

00:05:11.194 --> 00:05:13.364
has built-in support
for automatic differentiation,

00:05:13.364 --> 00:05:15.844
it can detect errors in your code
without even running it,

00:05:15.844 --> 00:05:18.134
and it has full access to Python APIs.

00:05:19.105 --> 00:05:22.645
Instead of telling you about it,
here is Richard Wei to show it to you now.

00:05:23.074 --> 00:05:24.444
Thank you, Chris.

00:05:25.794 --> 00:05:28.064
(applause)

00:05:30.940 --> 00:05:33.630
I'm thrilled to show you
Swift for TensorFlow.

00:05:33.636 --> 00:05:36.826
Swift is a high-performance,
modern programming language.

00:05:37.434 --> 00:05:39.329
And today, for the very first time,

00:05:39.329 --> 00:05:42.889
Swift has a full-power
TensorFlow built right in.

00:05:42.889 --> 00:05:46.269
I'm going to walk through
three major styles of programming:

00:05:48.249 --> 00:05:51.479
scripting, interpreting, and notebooks.

00:05:51.928 --> 00:05:55.888
So, first let me show you
the Swift interpreter.

00:05:58.638 --> 00:06:00.598
This is a Swift interpreter.

00:06:01.230 --> 00:06:06.390
When I type some code, Swift evaluates it
and prints the result, just like Python.

00:06:07.193 --> 00:06:09.233
Now, let's import TensorFlow.

00:06:09.794 --> 00:06:11.454
(keyboard clacking)

00:06:11.814 --> 00:06:14.434
I can create a Tensor from some scalers.

00:06:14.863 --> 00:06:17.303
(keyboard clacking)

00:06:25.632 --> 00:06:28.582
I can do any TensorFlow operation directly

00:06:28.582 --> 00:06:29.782
and see the result,

00:06:29.782 --> 00:06:32.162
just like I would with eager execution.

00:06:32.509 --> 00:06:34.691
For example, a + a,

00:06:35.851 --> 00:06:40.001
or [ACE] matrix product with itself.

00:06:41.531 --> 00:06:44.581
Of course, loops just work.

00:06:45.080 --> 00:06:47.020
I can print the result [of it].

00:06:48.505 --> 00:06:52.805
Interpreter is a lot of fun to work with,
but I like using TensorFlow

00:06:52.805 --> 00:06:56.265
in a more interactive environment,
just like Jupyter Notebook,

00:06:56.648 --> 00:06:58.208
so let's see how they work.

00:07:00.176 --> 00:07:01.836
This is a Swift notebook.

00:07:05.642 --> 00:07:07.902
It shows all the results on the right.

00:07:08.684 --> 00:07:10.995
So, here's a more interesting code:

00:07:12.325 --> 00:07:14.335
<i>Fun with Functions.</i>

00:07:14.335 --> 00:07:17.735
So, here I have
a sigmoid function inside a loop.

00:07:18.471 --> 00:07:21.421
Now, as I click on this button,

00:07:21.421 --> 00:07:26.511
it shows a trace of all values
produced by this function over time.

00:07:27.167 --> 00:07:31.567
Now, as a machine learning developer,
I often like to differentiate functions.

00:07:32.411 --> 00:07:34.751
Now, when I type in--

00:07:35.721 --> 00:07:40.211
Well, since we were able to improve
the programming language,

00:07:40.211 --> 00:07:42.421
we built first-class
automatic differentiation

00:07:42.421 --> 00:07:43.951
right into Swift.

00:07:44.390 --> 00:07:51.050
Now, when I type in "gradient effects,"
it shows the gradient.

00:07:51.050 --> 00:07:55.820
Swift computes the gradient automatically
and gives me the result.

00:07:56.344 --> 00:07:58.084
So, here's the gradient sigmoid.

00:08:00.110 --> 00:08:03.310
Now, let's look at some Python code.

00:08:03.854 --> 00:08:05.084
Let's think about Python.

00:08:05.550 --> 00:08:09.320
Well, as a machine learning developer, 
I've been using Python a lot

00:08:09.320 --> 00:08:12.290
and I know there are
many great Python libraries.

00:08:12.825 --> 00:08:17.485
Just today, my colleague, Dan,
sent me a data set in pickle format.

00:08:19.239 --> 00:08:21.629
I can directly use Python APIs to load it.

00:08:22.354 --> 00:08:29.204
All I have to do
is to type in "import Python,"

00:08:30.183 --> 00:08:36.943
and Swift uses a Python API,
<i>pickle</i>, to be specific, to load the data.

00:08:37.506 --> 00:08:42.106
Here you can see the data
right in the Swift notebook.

00:08:50.136 --> 00:08:51.696
So, here's the Swift notebook.

00:08:52.041 --> 00:08:55.331
Now, some people
like to run training script

00:08:55.331 --> 00:08:57.901
directly in command line.

00:08:59.102 --> 00:09:02.782
So, let me show you how to train
a simple model in command line.

00:09:07.621 --> 00:09:10.561
So, here is a simple MNIST model.

00:09:12.119 --> 00:09:16.639
In this model, I'm using TensorFlow's
data set API to load training data.

00:09:17.207 --> 00:09:21.417
As I scroll down, I have the forward pass,
backward pass, gradient descent,

00:09:21.417 --> 00:09:23.537
all defined in the training loop.

00:09:24.441 --> 00:09:27.551
Now, I usually like to work on the go,

00:09:27.551 --> 00:09:32.621
so this code has been working
on a CPU on my laptop.

00:09:33.624 --> 00:09:37.254
But when I want to get
more performance, what do I do?

00:09:37.621 --> 00:09:40.191
Well, why don't I just enable Cloud TPU?

00:09:41.493 --> 00:09:46.863
So, all I have to do is add one line
to enable TPU execution.

00:09:49.230 --> 00:09:52.920
When I save this file, open a terminal

00:09:53.912 --> 00:09:59.672
to run this training script,
it's initializing TPU

00:09:59.673 --> 00:10:02.843
and the Swift compiler
automatically partitions this program

00:10:02.843 --> 00:10:06.413
into a normal subprogram
and a TensorFlow graph.

00:10:06.413 --> 00:10:11.278
And TensorFlow is sending this graph
to the TensorFlow XLA compiler

00:10:11.278 --> 00:10:12.798
for TPU execution.

00:10:13.468 --> 00:10:18.828
Now, it's running and we're waiting
for the TPU to give the result.

00:10:22.754 --> 00:10:26.994
Look, loss is going down!
All right!

00:10:29.533 --> 00:10:36.383
Why don't we simply open TensorBoard
and see the training curve.

00:10:39.394 --> 00:10:44.514
Now I can see the entire
training history in TensorBoard.

00:10:45.995 --> 00:10:47.885
So, this is looking great.

00:10:48.500 --> 00:10:51.150
Now, this is Swift for TensorFlow.

00:10:51.841 --> 00:10:54.771
It's an interactive programming experience

00:10:55.401 --> 00:10:59.081
with supercomputing performance
at your fingertips.

00:11:00.475 --> 00:11:01.965
Back to you, Chris.

00:11:05.055 --> 00:11:06.785
(Chris) Thanks, Richard.

00:11:06.785 --> 00:11:08.918
(applause)

00:11:10.548 --> 00:11:13.128
All right, to recap quickly,
Richard showed you

00:11:13.128 --> 00:11:15.798
that Swift has an interpreter
and it works just like you'd expect.

00:11:17.352 --> 00:11:21.472
Now, I know that it's super frustrating
to be working on a program,

00:11:21.472 --> 00:11:25.142
and two hours into a training run
you get a shape error or a type mismatch.

00:11:25.142 --> 00:11:28.656
Swift is designed to catch errors early,
and we've started building in support

00:11:28.656 --> 00:11:32.936
for catching TensorFlow-specific mistakes
right into TensorFlow.

00:11:34.426 --> 00:11:37.416
We showed that you can directly use
arbitrary Python APIs

00:11:37.416 --> 00:11:40.076
and other dynamic languages
directly from Swift,

00:11:40.076 --> 00:11:42.446
which gives you full access
to the data science ecosystem

00:11:42.446 --> 00:11:45.226
and any other Python APIs
that you just love to use.

00:11:46.588 --> 00:11:49.618
Swift is also generating
standard TensorFlow graphs,

00:11:49.618 --> 00:11:51.378
including control flow,

00:11:51.378 --> 00:11:54.598
which give you the full performance
of the session API.

00:11:54.991 --> 00:11:56.601
Of course, graphs are also awesome

00:11:56.601 --> 00:11:59.201
because they give you access
to everything that TensorFlow can do,

00:11:59.201 --> 00:12:02.451
including devices spanning the range
from the tiniest Raspberry Pi

00:12:02.451 --> 00:12:05.271
all the way up to a TPU supercomputer.

00:12:05.271 --> 00:12:08.161
Now, you may be wondering
when do you get this, what does this mean?

00:12:08.462 --> 00:12:11.882
This is still an early-stage project,
but I'm happy to say

00:12:11.882 --> 00:12:14.882
that we're looking forward 
to our open source release next month.

00:12:15.322 --> 00:12:17.182
And not only are we releasing the code,

00:12:17.182 --> 00:12:20.370
we're releasing technical white papers
and documents that explain how it works.

00:12:20.370 --> 00:12:22.700
And we're also moving
our design discussions

00:12:22.700 --> 00:12:24.740
out into the public onto Google Groups,

00:12:24.740 --> 00:12:27.020
so that everybody
who wants to can participate.

00:12:28.135 --> 00:12:29.615
Now, we're not done yet.

00:12:29.615 --> 00:12:32.623
So, we have basic support
for automatic differentiation

00:12:32.623 --> 00:12:34.826
built right into the compiler
on the language.

00:12:34.826 --> 00:12:37.716
But we're also really interested
in extending it to support exotic cases,

00:12:37.716 --> 00:12:41.066
like recursion, and even differentiating
custom data structures.

00:12:42.535 --> 00:12:44.865
Compatibility issues
are also super frustrating,

00:12:44.865 --> 00:12:47.257
particularly if you accidentally
use an op or a dtype

00:12:47.257 --> 00:12:49.147
that's not supported by your device.

00:12:49.147 --> 00:12:52.457
Swift has great support for detecting
availability issues like this,

00:12:52.457 --> 00:12:56.157
and we're looking forward to wiring this
right into support TensorFlow.

00:12:57.278 --> 00:12:59.359
We're also interested in high-level APIs,

00:12:59.359 --> 00:13:01.329
but with this, we want to be
a little bit cautious.

00:13:01.329 --> 00:13:02.749
We have some prototypes now,

00:13:02.749 --> 00:13:04.939
but we'd really like to work
with the community

00:13:04.939 --> 00:13:08.709
and evaluate and iterate
and design multiple different approaches

00:13:08.709 --> 00:13:10.669
and actually experiment with these,

00:13:10.669 --> 00:13:14.369
and then settle on the best one
based on real-world experience.

00:13:15.008 --> 00:13:18.558
This has been a super quick tour
of Swift for TensorFlow.

00:13:19.155 --> 00:13:22.425
Swift for TensorFlow combines
the power and flexibility of TensorFlow

00:13:22.425 --> 00:13:24.845
with a whole new standard for usability.

00:13:24.847 --> 00:13:28.157
We think it will take your productivity
and shoot it to the roof.

00:13:29.013 --> 00:13:31.293
Swift for TensorFlow
is also an early-stage project

00:13:31.892 --> 00:13:33.812
and so we'd really love
for you to get interested

00:13:33.812 --> 00:13:35.392
and help us to build this future.

00:13:35.918 --> 00:13:37.098
Thank you.

00:13:37.579 --> 00:13:40.109
(applause)

00:13:41.210 --> 00:13:44.140
♪ (music) ♪

