WEBVTT
Kind: captions
Language: en

00:00:06.159 --> 00:00:08.450
LAURENCE MORONEY: So Lucasz,
you work on Tensor2Tensor,

00:00:08.450 --> 00:00:09.950
which is like a set.

00:00:09.950 --> 00:00:11.450
It's a library
dataset that's really

00:00:11.450 --> 00:00:12.560
designed to make
machine learning

00:00:12.560 --> 00:00:13.580
more accessible, right?

00:00:13.580 --> 00:00:15.170
Could you tell me all about it?

00:00:15.170 --> 00:00:17.630
LUCASZ KAISER: Yes, it is.

00:00:17.630 --> 00:00:22.587
To me, it's a follow-up
on the work on TensorFlow.

00:00:22.587 --> 00:00:24.170
TensorFlow is now
used by many people.

00:00:24.170 --> 00:00:26.600
It's a great system, at
least the foundations

00:00:26.600 --> 00:00:28.100
for machine learning.

00:00:28.100 --> 00:00:31.010
We found that it's still
quite hard for people

00:00:31.010 --> 00:00:34.400
to get into machine learning,
to start, get their first model,

00:00:34.400 --> 00:00:36.620
get some system working.

00:00:36.620 --> 00:00:40.940
And then you can start going
from there, tweaking, tuning.

00:00:40.940 --> 00:00:44.230
There are a few tutorials, but
then you get stuck on them.

00:00:44.230 --> 00:00:46.100
There is a limit
to what you can do.

00:00:46.100 --> 00:00:48.620
So in Tensor2Tensor,
it's a library

00:00:48.620 --> 00:00:54.590
where you can get the data,
get the models, train them.

00:00:54.590 --> 00:00:56.120
Get a working
system, but then we

00:00:56.120 --> 00:00:59.510
know you can also go further,
get more complicated systems.

00:00:59.510 --> 00:01:01.910
We do research on it
so you can really try

00:01:01.910 --> 00:01:03.290
to do difficult things with it.

00:01:03.290 --> 00:01:04.590
LAURENCE MORONEY: So when
you mentioned get the data,

00:01:04.590 --> 00:01:06.548
so there's known datasets
out there that you've

00:01:06.548 --> 00:01:07.880
packaged as part of this?

00:01:07.880 --> 00:01:08.671
LUCASZ KAISER: Yes.

00:01:08.671 --> 00:01:11.930
We've packaged a lot
of the datasets that

00:01:11.930 --> 00:01:14.930
are used in academia, image
classification datasets.

00:01:14.930 --> 00:01:17.180
There is a dataset
of digits that's

00:01:17.180 --> 00:01:19.370
like the "Hello World"
of machine learning.

00:01:19.370 --> 00:01:19.730
LAURENCE MORONEY: MNIST.

00:01:19.730 --> 00:01:20.360
LUCASZ KAISER:
It's called MNIST.

00:01:20.360 --> 00:01:21.560
LAURENCE MORONEY: Yeah.

00:01:21.560 --> 00:01:23.530
LUCASZ KAISER: And
there are many more.

00:01:23.530 --> 00:01:24.305
There's CIFAR.

00:01:24.305 --> 00:01:28.140
There are larger datasets
like ImageNet, text datasets,

00:01:28.140 --> 00:01:29.390
captioning.

00:01:29.390 --> 00:01:31.520
There are a lot of
them so we package them

00:01:31.520 --> 00:01:34.395
so you don't need to know
where to download them from.

00:01:34.395 --> 00:01:35.420
It's there.

00:01:35.420 --> 00:01:37.795
But you also don't need to
know how to preprocess them.

00:01:37.795 --> 00:01:39.170
Because there are
a lot of tricks

00:01:39.170 --> 00:01:42.320
that people use for
preprocessing, for adjusting.

00:01:42.320 --> 00:01:45.070
It's sometimes hard to find
them if you don't know them.

00:01:45.070 --> 00:01:47.390
In Tensor2Tensor,
they're all in the code.

00:01:47.390 --> 00:01:49.610
We explicitly point out,
here is the preprocessing

00:01:49.610 --> 00:01:51.530
so you can read it,
know how it's done,

00:01:51.530 --> 00:01:53.120
or you can just run it.

00:01:53.120 --> 00:01:54.710
Then you get the data.

00:01:54.710 --> 00:01:55.730
You have it.

00:01:55.730 --> 00:01:57.290
You can run your
own models, or you

00:01:57.290 --> 00:02:00.070
can run the models that
are there in Tensor2Tensor.

00:02:00.070 --> 00:02:01.160
LAURENCE MORONEY: And that's
really useful to have,

00:02:01.160 --> 00:02:03.660
because sometimes, you just
want to get straight to training

00:02:03.660 --> 00:02:05.000
and executing on models, right?

00:02:05.000 --> 00:02:07.787
Rather than needing to figure
out all the things that I need

00:02:07.787 --> 00:02:08.995
to do for a specific dataset.

00:02:08.995 --> 00:02:10.689
So if that's already
done for us--

00:02:10.689 --> 00:02:11.480
LUCASZ KAISER: Yes.

00:02:11.480 --> 00:02:15.140
On the other hand, sometimes
you have your own dataset

00:02:15.140 --> 00:02:17.450
and you just want to run
existing models on it,

00:02:17.450 --> 00:02:20.110
it can be harder
than people think.

00:02:20.110 --> 00:02:22.070
In Tensor2Tensor,
you have the API.

00:02:22.070 --> 00:02:25.410
You just make your dataset
adhere to this very simple API.

00:02:25.410 --> 00:02:28.214
And all the same preprocessing,
all the same models

00:02:28.214 --> 00:02:28.880
will be applied.

00:02:28.880 --> 00:02:32.342
So you should get reasonably
good results just running

00:02:32.342 --> 00:02:33.175
the existing models.

00:02:33.175 --> 00:02:35.216
LAURENCE MORONEY: A common
concept in programming

00:02:35.216 --> 00:02:36.524
is don't repeat yourself.

00:02:36.524 --> 00:02:37.940
So if you've already
done the work

00:02:37.940 --> 00:02:41.480
for preprocessing something, if
I put my data in that format,

00:02:41.480 --> 00:02:42.600
then I'm not repeating.

00:02:42.600 --> 00:02:43.391
LUCASZ KAISER: Yes.

00:02:43.391 --> 00:02:46.040
It's an effort to put the
machine learning components

00:02:46.040 --> 00:02:49.020
and best practices
on a single platform

00:02:49.020 --> 00:02:51.807
so that not everyone has
to redo it every time.

00:02:51.807 --> 00:02:53.390
LAURENCE MORONEY:
When I first started

00:02:53.390 --> 00:02:55.760
looking into Tensor2Tensor,
one of the things I noticed

00:02:55.760 --> 00:02:57.870
is that a lot of the
things in Tensor2Tensor

00:02:57.870 --> 00:03:00.150
are described as "problems."

00:03:00.150 --> 00:03:03.244
And can you describe what you
mean actually by a problem?

00:03:03.244 --> 00:03:04.910
LUCASZ KAISER: Yes,
in the simplest case

00:03:04.910 --> 00:03:06.620
we say MNIST is a problem.

00:03:06.620 --> 00:03:07.952
LAURENCE MORONEY: OK.

00:03:07.952 --> 00:03:10.160
LUCASZ KAISER: You want to
put in an image of a digit

00:03:10.160 --> 00:03:12.800
and get out which digit that is.

00:03:12.800 --> 00:03:15.590
And MNIST is a dataset
that has supervised

00:03:15.590 --> 00:03:18.200
examples of this kind.

00:03:18.200 --> 00:03:19.700
But some problems
are unsupervised.

00:03:19.700 --> 00:03:22.760
You just get the image
and then your network

00:03:22.760 --> 00:03:24.270
should figure out the digits.

00:03:24.270 --> 00:03:26.450
Some problems are
kind of reinforcement

00:03:26.450 --> 00:03:27.390
learning problems.

00:03:27.390 --> 00:03:28.670
You just get some signal.

00:03:28.670 --> 00:03:29.460
This was good.

00:03:29.460 --> 00:03:30.620
This was bad.

00:03:30.620 --> 00:03:33.090
But it's coming from an
external environment.

00:03:33.090 --> 00:03:35.780
So we are trying to make a
single interface so we can

00:03:35.780 --> 00:03:38.180
package these different things.

00:03:38.180 --> 00:03:40.940
If you put it as a
problem in Tensor2Tensor,

00:03:40.940 --> 00:03:43.489
you're guaranteed that the
models we have will run on it.

00:03:43.489 --> 00:03:44.530
LAURENCE MORONEY: Got it.

00:03:44.530 --> 00:03:46.446
LUCASZ KAISER: If you're
coming with your data

00:03:46.446 --> 00:03:50.090
or your reinforcement learning
problem, put it there,

00:03:50.090 --> 00:03:53.150
and the existing models,
the baselines that we have,

00:03:53.150 --> 00:03:54.152
can be run on it.

00:03:54.152 --> 00:03:55.110
LAURENCE MORONEY: Cool.

00:03:55.110 --> 00:03:56.790
Yeah, because for me, like
as I've been doing it,

00:03:56.790 --> 00:03:57.680
one of the things
I've been doing

00:03:57.680 --> 00:03:59.630
is I've been just
trying to find raw data

00:03:59.630 --> 00:04:01.410
and then I'm
preprocessing that data.

00:04:01.410 --> 00:04:02.660
And then I'm training a model.

00:04:02.660 --> 00:04:04.451
But I don't have any
context like, what are

00:04:04.451 --> 00:04:06.295
the best ways to preprocess it?

00:04:06.295 --> 00:04:07.670
The best ways to
format the data?

00:04:07.670 --> 00:04:08.500
Do I normalize it?

00:04:08.500 --> 00:04:09.120
Do I not?

00:04:09.120 --> 00:04:10.578
You know, all those
kind of things.

00:04:10.578 --> 00:04:12.690
So if, instead of
thinking in those terms,

00:04:12.690 --> 00:04:15.130
what I'm able to do
is just shape my data

00:04:15.130 --> 00:04:18.219
according to Tensor2Tensor,
and then you'll have--

00:04:18.219 --> 00:04:19.010
LUCASZ KAISER: Yes.

00:04:19.010 --> 00:04:21.310
LAURENCE MORONEY: --prewritten
code that will work for me.

00:04:21.310 --> 00:04:22.101
LUCASZ KAISER: Yes.

00:04:22.101 --> 00:04:25.355
It's certainly a great thing
to do once or twice to learn.

00:04:25.355 --> 00:04:27.610
Do everything
end-to-end on your own.

00:04:27.610 --> 00:04:28.270
Take a course.

00:04:28.270 --> 00:04:29.630
Do a tutorial.

00:04:29.630 --> 00:04:31.370
But what we found out in Brain--

00:04:31.370 --> 00:04:35.060
we've been doing research on
these topics for years now--

00:04:35.060 --> 00:04:38.300
after your 10th model, you
don't want to redo everything

00:04:38.300 --> 00:04:39.740
from scratch.

00:04:39.740 --> 00:04:42.170
And maybe you want to
compare with your model

00:04:42.170 --> 00:04:43.520
from a year ago.

00:04:43.520 --> 00:04:47.210
It's really great to have
these things organized, tested,

00:04:47.210 --> 00:04:49.490
maintained, working.

00:04:49.490 --> 00:04:51.860
And I think this really
makes machine learning

00:04:51.860 --> 00:04:55.130
more accessible to a wider
group of people, which is one

00:04:55.130 --> 00:04:56.510
of the goals in Google, right?

00:04:56.510 --> 00:04:57.060
LAURENCE MORONEY: Yeah, exactly.

00:04:57.060 --> 00:04:59.594
And also, it's like-- because
it's open source, right?

00:04:59.594 --> 00:05:01.760
So some of the things that
you do for preprocessing,

00:05:01.760 --> 00:05:03.069
I can go and take a look.

00:05:03.069 --> 00:05:03.860
LUCASZ KAISER: Yes!

00:05:03.860 --> 00:05:04.890
LAURENCE MORONEY: And
I can learn from that.

00:05:04.890 --> 00:05:06.640
So if I want to write
my own preprocessor,

00:05:06.640 --> 00:05:07.900
I can learn from
what you've done.

00:05:07.900 --> 00:05:09.983
LUCASZ KAISER: And we
strive for a little bit more

00:05:09.983 --> 00:05:11.520
than just open sourcing.

00:05:11.520 --> 00:05:14.950
We really try to keep
the code readable.

00:05:14.950 --> 00:05:17.882
Because you can open source
code that's very hard to read.

00:05:17.882 --> 00:05:19.530
LAURENCE MORONEY: That's true.

00:05:19.530 --> 00:05:21.530
LUCASZ KAISER: We're
trying to keep it organized

00:05:21.530 --> 00:05:22.940
so you'd know where to look.

00:05:22.940 --> 00:05:25.300
There is a preprocess function
in every problem class.

00:05:25.300 --> 00:05:27.640
And you know, OK, I need
to know the preprocessing

00:05:27.640 --> 00:05:28.730
for this kind of data.

00:05:28.730 --> 00:05:29.440
I look there.

00:05:29.440 --> 00:05:30.520
LAURENCE MORONEY: Some
of the coolest things

00:05:30.520 --> 00:05:32.740
that I've seen
Tensor2Tensor do is actually

00:05:32.740 --> 00:05:34.570
generation content, right?

00:05:34.570 --> 00:05:36.959
You've done some
Wikipedia generation.

00:05:36.959 --> 00:05:37.750
LUCASZ KAISER: Yes.

00:05:37.750 --> 00:05:38.589
So we--

00:05:38.589 --> 00:05:40.630
LAURENCE MORONEY: Or should
I say Wiki generation

00:05:40.630 --> 00:05:41.606
rather than, right?

00:05:41.606 --> 00:05:43.480
LUCASZ KAISER: We have
classification models,

00:05:43.480 --> 00:05:48.620
but we work on
generating text a lot.

00:05:48.620 --> 00:05:51.340
And one example of generating
text is translation.

00:05:51.340 --> 00:05:53.230
You take a sentence
in one language

00:05:53.230 --> 00:05:55.630
and then generate, say,
a sentence in English

00:05:55.630 --> 00:05:57.210
that's a translation.

00:05:57.210 --> 00:05:59.930
You generate it,
usually, word by word.

00:05:59.930 --> 00:06:03.670
But it's a lot of
fun to just let

00:06:03.670 --> 00:06:05.830
it generate without
conditioning on anything,

00:06:05.830 --> 00:06:08.230
just say generate
some sentences.

00:06:08.230 --> 00:06:11.530
And for a single sentence, it
might not be that fun to read.

00:06:11.530 --> 00:06:15.180
But we train this on
whole Wikipedia articles.

00:06:15.180 --> 00:06:17.140
Wiki is a great dataset.

00:06:17.140 --> 00:06:17.800
It's large.

00:06:17.800 --> 00:06:21.220
It has a lot of articles
with nice structure.

00:06:21.220 --> 00:06:25.210
And we see that it can generate
wonderfully coherent things

00:06:25.210 --> 00:06:28.600
that are really hard to
tell from real articles,

00:06:28.600 --> 00:06:29.930
but totally made up.

00:06:29.930 --> 00:06:32.180
LAURENCE MORONEY: So what
do you write articles about?

00:06:32.180 --> 00:06:33.400
LUCASZ KAISER: We had--

00:06:33.400 --> 00:06:35.800
every time you generate
because you sample,

00:06:35.800 --> 00:06:37.175
something different
can come out.

00:06:37.175 --> 00:06:38.050
LAURENCE MORONEY: OK.

00:06:38.050 --> 00:06:40.570
LUCASZ KAISER: So we sampled
and once came out an article

00:06:40.570 --> 00:06:42.430
about a Japanese music team.

00:06:42.430 --> 00:06:44.530
It has a composer.

00:06:44.530 --> 00:06:47.230
It has a name--

00:06:47.230 --> 00:06:48.610
I think Motohiro Ohta--

00:06:48.610 --> 00:06:50.300
totally made up with Google.

00:06:50.300 --> 00:06:53.680
There doesn't seem to
be anyone like this.

00:06:53.680 --> 00:06:56.751
LAURENCE MORONEY: If you're
watching, Motohiro, thank you.

00:06:56.751 --> 00:06:58.250
LUCASZ KAISER: It
sounds reasonable.

00:06:58.250 --> 00:07:01.540
And it has sections, when
the band formed, who joined.

00:07:01.540 --> 00:07:03.460
It has a section on
when they performed.

00:07:03.460 --> 00:07:05.320
They performed
with Led Zeppelin.

00:07:05.320 --> 00:07:07.630
So it clearly learned some
knowledge about the world,

00:07:07.630 --> 00:07:09.010
like what are band names?

00:07:09.010 --> 00:07:11.800
What do they-- who
do they perform with?

00:07:11.800 --> 00:07:14.380
It has a period in
which it exists.

00:07:14.380 --> 00:07:18.220
And the dates make sense.

00:07:18.220 --> 00:07:21.360
And then the band is to break
apart with Motohiro in tears.

00:07:21.360 --> 00:07:22.750
[CHUCKLES]

00:07:22.750 --> 00:07:24.715
So it generates a longish story.

00:07:24.715 --> 00:07:27.256
LAURENCE MORONEY: I'm in tears,
too, I never got to see them.

00:07:27.256 --> 00:07:27.990
[LAUGHS]

00:07:27.990 --> 00:07:30.730
LUCASZ KAISER: Yes, but
then you run this again,

00:07:30.730 --> 00:07:34.520
and you get some fake book
or some fake TV series.

00:07:34.520 --> 00:07:35.970
I like the fake TV series.

00:07:35.970 --> 00:07:39.310
It had like seasons
and names of episodes.

00:07:39.310 --> 00:07:40.430
LAURENCE MORONEY: Oh, wow.

00:07:40.430 --> 00:07:42.970
LUCASZ KAISER: It was like
about a boy in the suburbs.

00:07:42.970 --> 00:07:44.130
It has a summary.

00:07:44.130 --> 00:07:45.630
LAURENCE MORONEY:
Oh, this is great.

00:07:45.630 --> 00:07:47.800
So I mean, if I'm a
writer, I can get ideas

00:07:47.800 --> 00:07:51.062
by having this thing
throw articles at me.

00:07:51.062 --> 00:07:53.270
Another thing that I saw
that you've been working on,

00:07:53.270 --> 00:07:56.410
there was a blog post recently
about generation of poetry.

00:07:56.410 --> 00:07:58.360
LUCASZ KAISER: That was
done together with Lak

00:07:58.360 --> 00:08:00.190
from Google Cloud.

00:08:00.190 --> 00:08:01.624
And he's a technical writer.

00:08:01.624 --> 00:08:03.790
And he said, well, you're
saying machine learning is

00:08:03.790 --> 00:08:05.680
accessible with this library.

00:08:05.680 --> 00:08:06.795
Let me be the guinea pig.

00:08:06.795 --> 00:08:07.670
LAURENCE MORONEY: OK.

00:08:07.670 --> 00:08:10.000
LUCASZ KAISER: Let me try
to do something I make up

00:08:10.000 --> 00:08:12.950
and see whether it's
really easy enough to do.

00:08:12.950 --> 00:08:15.640
And he said, OK, why
not generate poetry?

00:08:15.640 --> 00:08:17.650
You're generating text.

00:08:17.650 --> 00:08:19.800
I would like to
generate some poems.

00:08:19.800 --> 00:08:21.490
He says, there is the
Project Gutenberg,

00:08:21.490 --> 00:08:25.450
which is a large database
of open literature.

00:08:25.450 --> 00:08:27.850
He wrote a script that
selected some poetry and said,

00:08:27.850 --> 00:08:29.320
let's train on it.

00:08:29.320 --> 00:08:30.530
Let me try.

00:08:30.530 --> 00:08:33.490
Can I hook up this
data into the system?

00:08:33.490 --> 00:08:34.960
Train one of the models on it.

00:08:34.960 --> 00:08:35.650
Will it work?

00:08:35.650 --> 00:08:38.570
And he did it, and
it didn't work.

00:08:38.570 --> 00:08:40.909
Well, he trained, but
the model was not good.

00:08:40.909 --> 00:08:44.230
It was not tuned to
this kind of data.

00:08:44.230 --> 00:08:47.020
He says, well, but we
have automatic tuning

00:08:47.020 --> 00:08:48.430
for the models.

00:08:48.430 --> 00:08:50.620
Let me see how I can do that.

00:08:50.620 --> 00:08:54.740
And he tried and tried,
and then it worked.

00:08:54.740 --> 00:08:56.800
And he wrote a whole
blog post how to--

00:08:56.800 --> 00:08:59.010
I love this,
because it shows you

00:08:59.010 --> 00:09:01.670
if you come with your own data
and want to get something,

00:09:01.670 --> 00:09:03.850
how do I actually do this?

00:09:03.850 --> 00:09:05.810
And it goes through
the whole experience.

00:09:05.810 --> 00:09:07.210
And at the end,
there is actually

00:09:07.210 --> 00:09:11.887
a service which you type a line
and it continues your poem.

00:09:11.887 --> 00:09:13.220
LAURENCE MORONEY: I've tried it.

00:09:13.220 --> 00:09:13.900
That was fun.

00:09:13.900 --> 00:09:16.300
And I've put a link in
the description below.

00:09:16.300 --> 00:09:18.960
And it wrote better
poetry than I can.

00:09:18.960 --> 00:09:20.720
[LAUGHS]

00:09:20.720 --> 00:09:24.100
So if I wanted to get started
to do this kind of thing--

00:09:24.100 --> 00:09:24.970
maybe it's poetry.

00:09:24.970 --> 00:09:26.970
Maybe it's song
lyrics, or whatever--

00:09:26.970 --> 00:09:29.170
and I bring my own data,
what advice would you give?

00:09:29.170 --> 00:09:31.370
How should I get
started with it?

00:09:31.370 --> 00:09:33.966
LUCASZ KAISER: Well, go to
Tensor2Tensor on GitHub.

00:09:33.966 --> 00:09:34.840
LAURENCE MORONEY: OK.

00:09:34.840 --> 00:09:37.330
LUCASZ KAISER: Or check
out the poetry blog post.

00:09:37.330 --> 00:09:41.480
You can install it and try
to run from the command line.

00:09:41.480 --> 00:09:44.440
We also have Jupyter
notebooks, Spyder notebooks

00:09:44.440 --> 00:09:46.870
where you can see step
by step what's happening.

00:09:46.870 --> 00:09:49.450
Also, inspect the data,
especially if you do images.

00:09:49.450 --> 00:09:52.609
Actually seeing what's
coming on input, what's

00:09:52.609 --> 00:09:54.400
supposed to come on
output, what's actually

00:09:54.400 --> 00:09:56.920
coming on output is
a very useful thing.

00:09:56.920 --> 00:09:59.890
So there is a notebook
where we step by step show

00:09:59.890 --> 00:10:01.320
you what to do.

00:10:01.320 --> 00:10:02.350
Go to the website.

00:10:02.350 --> 00:10:03.550
Try it out.

00:10:03.550 --> 00:10:06.970
And also, thanks to a
lot of my colleagues,

00:10:06.970 --> 00:10:08.770
it's a very active project.

00:10:08.770 --> 00:10:10.660
We work on it every day.

00:10:10.660 --> 00:10:11.560
We work with it.

00:10:11.560 --> 00:10:12.970
We work on it.

00:10:12.970 --> 00:10:15.670
So you can start with the
small things, like poetry.

00:10:15.670 --> 00:10:17.020
It's a very small dataset.

00:10:17.020 --> 00:10:19.180
You can train in a few hours.

00:10:19.180 --> 00:10:21.787
MNIST, you can train
in two minutes.

00:10:21.787 --> 00:10:22.620
But you can move on.

00:10:22.620 --> 00:10:26.920
Like with the Wikipedia, we also
did just Wikipedia generation,

00:10:26.920 --> 00:10:30.722
but we also did Wikipedia
conditioned on web searches.

00:10:30.722 --> 00:10:31.930
So it doesn't make things up.

00:10:31.930 --> 00:10:33.460
It's actually factual.

00:10:33.460 --> 00:10:36.721
It generates things that the
internet says about people.

00:10:36.721 --> 00:10:37.720
LAURENCE MORONEY: I see.

00:10:37.720 --> 00:10:40.780
LUCASZ KAISER: And this
data set is 8 terabytes.

00:10:40.780 --> 00:10:45.880
It takes a bit longer to train,
but we still open-source it,

00:10:45.880 --> 00:10:47.770
thanks to the
common crawl, which

00:10:47.770 --> 00:10:52.090
has a large part
of websites online.

00:10:52.090 --> 00:10:54.829
It's possible to actually
do this-- well, at home,

00:10:54.829 --> 00:10:55.370
I don't know.

00:10:55.370 --> 00:11:01.240
But at a university, you can
get this really large dataset.

00:11:01.240 --> 00:11:03.800
You can run
state-of-the-art models.

00:11:03.800 --> 00:11:06.760
So it's a platform where you
can start small, but really

00:11:06.760 --> 00:11:08.082
go big with time.

00:11:08.082 --> 00:11:09.040
LAURENCE MORONEY: Cool.

00:11:09.040 --> 00:11:11.410
So Tensor2Tensor,
and it's on GitHub

00:11:11.410 --> 00:11:12.830
as part of the
TensorFlow project.

00:11:12.830 --> 00:11:13.180
LUCASZ KAISER: Yes.

00:11:13.180 --> 00:11:13.890
LAURENCE MORONEY: So cool.

00:11:13.890 --> 00:11:15.400
Well, thank you so much, Lucasz.

00:11:15.400 --> 00:11:16.780
This has been so much fun.

00:11:16.780 --> 00:11:17.740
I've learned so much.

00:11:17.740 --> 00:11:19.990
And thank you, everybody,
for watching this episode

00:11:19.990 --> 00:11:21.190
of "Coffee with a Googler."

00:11:21.190 --> 00:11:22.480
If you have any questions
for me or if you

00:11:22.480 --> 00:11:24.100
have any questions for
Lucasz, please leave them

00:11:24.100 --> 00:11:25.056
in the comments below.

00:11:25.056 --> 00:11:27.430
And also, don't forget to take
a look at the description.

00:11:27.430 --> 00:11:29.710
We've put links to everything
that we spoke about.

00:11:29.710 --> 00:11:32.350
And also, we have a brand new
TensorFlow channel on YouTube.

00:11:32.350 --> 00:11:33.640
So be sure to check it out.

00:11:33.640 --> 00:11:35.430
Thank you.

