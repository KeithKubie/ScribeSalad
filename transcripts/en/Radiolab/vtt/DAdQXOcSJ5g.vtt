WEBVTT

1
00:00:02.050 --> 00:00:02.950
Oh wait, you're listening.

2
00:00:08.730 --> 00:00:13.730
<v 1>You are listening to radio lab radio from W N y.</v>

3
00:00:13.791 --> 00:00:14.980
S. T e.

4
00:00:18.980 --> 00:00:22.550
All right. Hello? Hello? Hello? Hello? Hello? Hello. I can hear you.

5
00:00:22.790 --> 00:00:25.250
They can't hear me? No, we can hear you. We can hear you.

6
00:00:25.760 --> 00:00:30.650
We can also hear it twice. Yeah. Hey, I'm Jen. I boom. Rod.

7
00:00:30.680 --> 00:00:35.360
I'm Robert Krulwich, Radiolab, and today, oops. Oops. You don't, you don't hear.

8
00:00:35.400 --> 00:00:38.620
You don't hear us. We have a story about how, hmm.

9
00:00:38.900 --> 00:00:43.850
The echoes of you can go out into the world and come back and bite you in.

10
00:00:43.851 --> 00:00:48.680
All of us. Really in the bud. Hmm. Uh, oh wait, wait.

11
00:00:48.681 --> 00:00:53.480
Maybe we're fine now. Is My echo gone? Yes.

12
00:00:53.690 --> 00:00:56.780
Okay. Okay. We're good. We're good. And it comes to us from our producers.

13
00:00:56.781 --> 00:01:01.220
Simon Adler. Yeah. Nick. Hello. Hello. Sorry. Okay,

14
00:01:01.221 --> 00:01:04.550
so this is Nick Bilton. My name is Nick Bilton.

15
00:01:04.551 --> 00:01:07.190
I'm a special correspondent for Vanity Fair and is beat,

16
00:01:07.250 --> 00:01:11.060
you could say is trying to predict the future of technology to look into the

17
00:01:11.061 --> 00:01:11.894
future.

18
00:01:11.980 --> 00:01:16.940
I'm into this kind of crystal ball and try to predict what the next five, 10,

19
00:01:16.941 --> 00:01:19.100
15 years would look like for the media industry.

20
00:01:19.160 --> 00:01:21.470
Do you have a good batting record? Like did you,

21
00:01:21.500 --> 00:01:23.450
did you call some big well yeah, you know,

22
00:01:23.570 --> 00:01:27.410
phones in our pockets that would be like super computers that social media would

23
00:01:27.411 --> 00:01:32.000
drive news, not newspapers and so on and things like that. So it's, it's,

24
00:01:32.120 --> 00:01:32.953
it's been pretty good.

25
00:01:33.080 --> 00:01:36.890
I reached out to you because I came across this article that you wrote an

26
00:01:36.891 --> 00:01:37.910
article that, uh,

27
00:01:38.330 --> 00:01:43.190
sent shivers down my spine and I'm not one to typically be given shivers by

28
00:01:43.191 --> 00:01:43.641
articles.

29
00:01:43.641 --> 00:01:48.410
So I guess how did you stumble into all of this and where did,

30
00:01:48.411 --> 00:01:51.180
where does this start for you? So, uh,

31
00:01:51.210 --> 00:01:55.530
I was sitting around with some friends in my living room and friend of mine

32
00:01:55.531 --> 00:01:57.810
mentioned, Oh, did you see this thing that Adobe put out recently?

33
00:02:00.370 --> 00:02:04.960
<v 2>We live in a time when more people than ever before believe that they can change</v>

34
00:02:04.961 --> 00:02:05.794
the world.

35
00:02:05.900 --> 00:02:08.230
<v 1>Conversation led nick to a video,</v>

36
00:02:08.410 --> 00:02:11.940
a video online of the Adobe Max 2016.

37
00:02:13.970 --> 00:02:16.070
<v 2>There are tons and tons of people in the audience. Yeah,</v>

38
00:02:16.180 --> 00:02:18.150
<v 1>he's amazing. And, and up in front of them,</v>

39
00:02:18.340 --> 00:02:22.150
it looks like the stage of a apple product launch,

40
00:02:22.180 --> 00:02:26.170
but sort of beach themed. Why Beach? I have absolutely no idea.

41
00:02:26.600 --> 00:02:28.660
That's a little bit to your mind. I got, Hey,

42
00:02:29.230 --> 00:02:32.830
there are two hosts that are sitting in these like lifeguard chairs,

43
00:02:34.270 --> 00:02:38.350
comedian Jordan Peele, Jordan Peele as in key and Peele, Jordan Peele. Yes.

44
00:02:38.590 --> 00:02:42.880
And then the other host is this woman Kim Chambers who is a marathon swimmer and

45
00:02:43.060 --> 00:02:47.020
uh, an Adobe employee. And then please welcome to the stage

46
00:02:48.930 --> 00:02:49.763
[inaudible] Jay.

47
00:02:49.920 --> 00:02:52.680
<v 2>Hello everyone. Young guy glasses.</v>

48
00:02:52.710 --> 00:02:56.220
You guys have been making weird stuff online.

49
00:02:58.290 --> 00:02:59.620
Great photo ID chain

50
00:02:59.900 --> 00:03:02.480
<v 1>and he says Adobe is known for Photoshop.</v>

51
00:03:02.481 --> 00:03:05.120
We're known for editing a photos and

52
00:03:05.170 --> 00:03:07.930
<v 2>doing magical things visually. Well,</v>

53
00:03:08.610 --> 00:03:12.940
we all do the next thing today. Let's do something to humans.

54
00:03:13.010 --> 00:03:15.830
Speech pulls up the screen on a MAC computer. Wow.

55
00:03:16.190 --> 00:03:19.570
I have obtained today's piece of audio where, um,

56
00:03:19.700 --> 00:03:24.410
there's Meco key talking to peel about his feeling after getting nominated.

57
00:03:24.930 --> 00:03:27.720
<v 1>Keegan Michael Key had been nominated for an Emmy and a,</v>

58
00:03:27.721 --> 00:03:29.760
he and Jordan Peele were talking about it.

59
00:03:30.230 --> 00:03:34.280
<v 2>Ah, there's a pretty interesting, uh, joke here. So let's, uh,</v>

60
00:03:34.310 --> 00:03:39.200
let's just get yet, uh, I jumped on the bed and um,

61
00:03:39.410 --> 00:03:43.130
and uh, uh, I kissed my dogs and my wife in that order.

62
00:03:44.360 --> 00:03:47.360
Not a bad joke. So that's a, that's do something here. Okay.

63
00:03:47.450 --> 00:03:51.440
So suppose I'm Michael K one, 2000. There's a audio to his life.

64
00:03:51.770 --> 00:03:52.400
<v 1>In other words,</v>

65
00:03:52.400 --> 00:03:56.780
what if Keegan Michael Key with feeling like that was a little bit rough on my

66
00:03:56.781 --> 00:03:59.880
wife? That was a little bit mean, uh, you know, maybe, uh,

67
00:04:00.080 --> 00:04:04.820
he wanted to go and rewrite history and say that he kissed his wife before the

68
00:04:04.821 --> 00:04:05.310
dog.

69
00:04:05.310 --> 00:04:09.590
<v 2>Okay. Actually want his wife to go before the dogs. So,</v>

70
00:04:11.010 --> 00:04:12.200
okay. So what do we do

71
00:04:12.890 --> 00:04:15.140
<v 1>easily? So Xavier Clicks a button in the program,</v>

72
00:04:15.170 --> 00:04:18.620
automatically generates a transcript of the audio and projects it up on the

73
00:04:18.621 --> 00:04:22.790
screen behind him. You know, just text of what Keegan Michael Key said. Okay,

74
00:04:22.990 --> 00:04:25.790
let me zooming a little bit and then copy paste.

75
00:04:26.060 --> 00:04:29.870
He just highlights the word wife and pastes it over in front of dogs.

76
00:04:30.120 --> 00:04:35.100
<v 2>Okay, let's listen to it. Clicks play. And uh, uh, I kissed my wife and my dogs.</v>

77
00:04:36.010 --> 00:04:36.843
Woo.

78
00:04:37.620 --> 00:04:37.940
<v 3>[inaudible]</v>

79
00:04:37.940 --> 00:04:40.730
<v 4>oh, so he was able to move the,</v>

80
00:04:40.760 --> 00:04:44.900
edit the audio by moving the text around in a text box. Yes, exactly. Okay.

81
00:04:44.901 --> 00:04:46.490
Well that's kind of cool. Kind of impressive.

82
00:04:46.820 --> 00:04:50.720
<v 2>Wait, but then here's more, here's more. Uh,</v>

83
00:04:50.900 --> 00:04:55.210
we can actually type something that's not here. So, wait, wait, what?

84
00:04:55.640 --> 00:05:00.400
Just hang on just again. I have heard that, um, actually that on that day, uh,

85
00:05:00.770 --> 00:05:04.570
Michael actually the hour Jordan,

86
00:05:05.410 --> 00:05:08.810
so sorry to recover the truth. Let's go with you.

87
00:05:08.820 --> 00:05:12.970
Goes back into that little word box though. Let's remove the award of my here,

88
00:05:13.330 --> 00:05:18.040
your secrets out to type the word Jordan.

89
00:05:18.280 --> 00:05:21.280
<v 1>So he typed it out. J, o. R. D. A. N. N. Just to be clear,</v>

90
00:05:21.580 --> 00:05:24.340
Keegan Michael Key did not see Jordan anywhere in this clip.

91
00:05:24.590 --> 00:05:28.550
<v 2>And here we go. And, uh, I kissed Jordan and my dogs.</v>

92
00:05:31.920 --> 00:05:32.161
<v 4>Wait,</v>

93
00:05:32.161 --> 00:05:36.690
he just typed in a word that the guy never said and it made the guy say the word

94
00:05:36.960 --> 00:05:39.150
that he never said as if he actually said it

95
00:05:39.670 --> 00:05:44.670
<v 2>exactly. Well you will, which jumps out of this lifeguard.</v>

96
00:05:44.800 --> 00:05:44.830
Yeah.

97
00:05:44.830 --> 00:05:47.260
<v 1>Chair started sort of stomping around the stage.</v>

98
00:05:47.570 --> 00:05:49.170
<v 2>You were, you were demon. Oh yeah.</v>

99
00:05:49.890 --> 00:05:52.620
I have a magic and to the lot magic,

100
00:05:52.621 --> 00:05:57.230
I cannot show you guys as we calculate types, small phrases. So that's

101
00:05:59.090 --> 00:06:02.930
<v 1>okay. So what were your mood? He deletes the words. My dogs. Any types? Three,</v>

102
00:06:02.940 --> 00:06:07.400
three times. Oh Man.

103
00:06:07.740 --> 00:06:11.090
Tea Bag and a, I kissed Jordan three times.

104
00:06:15.390 --> 00:06:15.750
<v 5>[inaudible]</v>

105
00:06:15.750 --> 00:06:20.670
<v 1>all right. Wait, you are saying that Keegan Michael Key never said,</v>

106
00:06:20.850 --> 00:06:25.440
ever said Jordan. Never said three. Never said time's never,

107
00:06:25.770 --> 00:06:29.670
never said any of those words. And somehow just from the typing in of it,

108
00:06:29.671 --> 00:06:33.450
the guy is now saying them and we're hearing them in his voice.

109
00:06:33.480 --> 00:06:37.080
That's what just happened. Yup. That is exactly what the demo claims.

110
00:06:37.140 --> 00:06:41.900
It's essentially Photoshop for audio. Nick built. And again, you can take, uh,

111
00:06:42.230 --> 00:06:45.750
uh, as little as 20 minutes of someone's voice type the words.

112
00:06:45.751 --> 00:06:50.000
And it creates in that voice, um, that sentence with,

113
00:06:50.010 --> 00:06:53.000
with just 20 minutes of the guy talking. Yes. What, how,

114
00:06:53.060 --> 00:06:57.640
how in heaven do you do this? Okay. Well,

115
00:06:57.650 --> 00:07:01.850
and so we're, we're here to Adobe. Um, what, what exactly do you do here? Sure.

116
00:07:01.851 --> 00:07:05.160
I'm the product manager for audio. This is during Gleaves. Um,

117
00:07:05.270 --> 00:07:08.780
I flew out to Seattle and tracked him down to ask him exactly that question.

118
00:07:08.780 --> 00:07:12.350
So essentially what it does is it does an analysis of the speech and it creates

119
00:07:12.351 --> 00:07:16.400
a models and it, it basically, uh, and he explained to me that this program,

120
00:07:16.401 --> 00:07:18.520
which they call it Voco by the way, um,

121
00:07:18.710 --> 00:07:22.910
what it does is it takes 20 minutes or actually 40 if you want the best results,

122
00:07:23.150 --> 00:07:26.480
uh, of you talking. And it figures out all of the phonetics of your speech.

123
00:07:26.510 --> 00:07:29.960
All of the sounds that you make finds each little block of sound and speech that

124
00:07:29.961 --> 00:07:31.880
is in the recordings, chops them all up.

125
00:07:31.940 --> 00:07:35.750
And then when you go and type things in, it will recombine those, um,

126
00:07:35.890 --> 00:07:39.590
into that new word. But what if it encounters a sound that I've never made?

127
00:07:40.220 --> 00:07:43.430
Well a, the theory is in 40 minutes of speech,

128
00:07:43.460 --> 00:07:45.620
which is the amount they recommend you feed in,

129
00:07:45.650 --> 00:07:50.450
you're going to probably say just about every sound in the English language.

130
00:07:50.480 --> 00:07:53.420
So if really, so like phonetically I go,

131
00:07:53.421 --> 00:07:57.290
I run through the gamut and in 40 minutes, yes. Wow.

132
00:07:58.490 --> 00:08:02.120
Well, and like what would you, or what are you hoping people will,

133
00:08:02.150 --> 00:08:07.150
would use a product like Foco for a,

134
00:08:07.191 --> 00:08:11.120
so for the video production tools and for what audition is used for a lot as

135
00:08:11.540 --> 00:08:12.110
dialogue editing,

136
00:08:12.110 --> 00:08:16.190
the whole idea during said is to help people that work in movies and TV.

137
00:08:16.191 --> 00:08:21.191
A lot of our customers record great audio on set the actors and the dialogues

138
00:08:21.321 --> 00:08:24.080
and everything. Uh, and when they come back,

139
00:08:24.410 --> 00:08:26.630
if sometimes there's a mistake or they make a change,

140
00:08:26.900 --> 00:08:29.710
like the actor on set said shoe. Uh,

141
00:08:30.200 --> 00:08:33.830
but what he was pointing at was obviously a boot. And right now there's,

142
00:08:33.831 --> 00:08:36.800
they do what's called ADR. They'll bring the actor in,

143
00:08:36.801 --> 00:08:40.670
they'll rerecord some lines and they'll try and drop that into the video.

144
00:08:40.790 --> 00:08:43.280
But there's a lot of, you're not using the same microphones,

145
00:08:43.281 --> 00:08:46.400
you're not in the same location. The actor might be sick that day.

146
00:08:46.401 --> 00:08:48.560
So up his voice sounds different and things,

147
00:08:48.740 --> 00:08:51.800
a lot of times you can really hear that stand out in productions if they don't

148
00:08:51.801 --> 00:08:56.610
get it just right or with Voco you just delete the word shoe type in boot and

149
00:08:56.611 --> 00:08:57.180
boom,

150
00:08:57.180 --> 00:09:01.710
there it is using the same source media and the same characteristics and have it

151
00:09:01.711 --> 00:09:05.040
just sound seamless and natural. And so it, it,

152
00:09:05.910 --> 00:09:07.440
it's going to be a sort of,

153
00:09:07.810 --> 00:09:11.570
the hope is that it will make the lives of professional post-production editors

154
00:09:11.610 --> 00:09:14.610
easier the world over. That's our hope right now. Yeah,

155
00:09:16.200 --> 00:09:20.190
but that's not exactly, well, it's, I mean it's, um,

156
00:09:20.490 --> 00:09:24.630
what Nick Bilton thought when, um, when he saw this video,

157
00:09:24.720 --> 00:09:27.660
it could be Donald Trump's voice or Vladimir Putin. Um,

158
00:09:28.110 --> 00:09:30.480
so I saw that and I thought, wow,

159
00:09:30.540 --> 00:09:34.710
if imagine if audio clips start getting shared around the Internet,

160
00:09:35.590 --> 00:09:40.170
um, as fake news about fake conversation between, you know,

161
00:09:40.200 --> 00:09:44.130
Vladimir Putin and Paul Manafort about trying to get Trump into the white house

162
00:09:44.131 --> 00:09:47.130
or something like that. Right now. I was like, Whoa, this is,

163
00:09:47.370 --> 00:09:51.390
this is scary stuff, but we're just getting started.

164
00:09:52.200 --> 00:09:55.140
In the words of John Raymond Arnold played by Samuel L.

165
00:09:55.140 --> 00:09:58.110
Jackson in the movie Jurassic Park in his own voice. Hold on to you,

166
00:09:58.111 --> 00:10:02.280
but things are about to get a lot crazy

167
00:10:05.250 --> 00:10:06.083
<v 5>[inaudible]</v>

168
00:10:10.580 --> 00:10:14.330
<v 1>so forget voices for a second because now one, two, three, four, five, one, two,</v>

169
00:10:14.331 --> 00:10:17.870
three, four, five. It's facetime. All right. We are at the Paul G.

170
00:10:17.870 --> 00:10:20.420
Allen Center at the University of Washington in Seattle,

171
00:10:20.450 --> 00:10:25.450
so I left Adobe and went across town to talk to the head of the grail lab hero.

172
00:10:27.590 --> 00:10:31.580
Yeah. Very nice to meet you. Dr Eric [inaudible].

173
00:10:31.950 --> 00:10:34.610
I'm a professor in the computer science department at the University of

174
00:10:34.611 --> 00:10:37.580
Washington and Elsa work at Facebook. Can I just have you come a little closer?

175
00:10:38.600 --> 00:10:43.250
Okay. Just to back up for a second, when nick first saw the VOCO demonstration,

176
00:10:43.280 --> 00:10:46.580
he started to wonder, okay, like how could this be used down the road?

177
00:10:46.760 --> 00:10:48.470
And my original thesis was, oh, well,

178
00:10:48.471 --> 00:10:53.471
maybe what'll happen is that you will be able to create three d actors just like

179
00:10:54.531 --> 00:10:55.490
you didn't star wars.

180
00:10:55.580 --> 00:11:00.580
Then join it with the voco stuff to create a fake Hillary Clinton and you know,

181
00:11:01.040 --> 00:11:04.550
Donald Trump having a conversation or making out or whatever it is you want to

182
00:11:04.551 --> 00:11:08.120
do. And that led him to investigate the type of work that era does.

183
00:11:08.390 --> 00:11:12.140
Should I've been using these terms like facial reenactment and facial

184
00:11:12.141 --> 00:11:13.430
manipulation? Are those the,

185
00:11:13.431 --> 00:11:17.450
are those the right words and then what the hell do these words mean? Yeah, so,

186
00:11:18.020 --> 00:11:19.610
um, I mean it's all,

187
00:11:19.611 --> 00:11:24.611
it's all a way of animating faces and it started from the movies,

188
00:11:25.071 --> 00:11:29.120
right? The concept is to drive these remotely controlled bodies called avatars.

189
00:11:29.400 --> 00:11:31.580
I think like the aptly named movie Avatar

190
00:11:33.470 --> 00:11:36.320
or a Sargent. Yes. Going a little further back.

191
00:11:36.360 --> 00:11:39.170
No sign of intelligent life anywhere, toy story,

192
00:11:40.380 --> 00:11:41.840
and to make the characters come alive.

193
00:11:41.990 --> 00:11:44.840
What you need is the expressions of the actors playing them.

194
00:11:45.080 --> 00:11:49.160
This is the movie space. It means that you will bring the person to a studio.

195
00:11:49.370 --> 00:11:53.320
Then you covered their face with these sticky sensory marker things and then

196
00:11:53.321 --> 00:11:54.700
they will spend hours, hours,

197
00:11:54.701 --> 00:11:59.270
hours capturing the person's little dynamics like smiled up in miles teeth,

198
00:11:59.530 --> 00:12:04.280
claws, mas, no teeth had surprised disturbance like that. Angry, bloated. Yeah,

199
00:12:04.360 --> 00:12:05.193
frustrated.

200
00:12:05.200 --> 00:12:10.200
And from that they create a virtual character capable of emoting all those

201
00:12:10.211 --> 00:12:12.820
expressions. And to make that character believable,

202
00:12:13.030 --> 00:12:17.470
the animators sometimes have to model a bone structure and muscles.

203
00:12:17.590 --> 00:12:20.350
And as you can imagine, this can get very, very expensive.

204
00:12:20.351 --> 00:12:22.690
And so what people like eras started to wonder was like,

205
00:12:22.870 --> 00:12:24.400
can this be done on a budget?

206
00:12:24.430 --> 00:12:29.380
So she and others in the field started feeding videos of faces into computers

207
00:12:29.381 --> 00:12:33.700
and trained those computers to break down the face into a series of points.

208
00:12:33.880 --> 00:12:38.880
Our model started about a two 50 by two 50 that is 62,500 points on one human

209
00:12:40.331 --> 00:12:43.790
face. And the once we know that right, we can track the points. Okay.

210
00:12:43.980 --> 00:12:48.980
So once you can track how my face moves through a video clip by these 250 by 250

211
00:12:50.640 --> 00:12:55.300
data points, what can you then do with that information? Well,

212
00:12:55.480 --> 00:13:00.480
I can apply the points on the face on a different model of a different person.

213
00:13:01.360 --> 00:13:04.320
Now this is, this is where things get a quite strange, uh,

214
00:13:04.600 --> 00:13:09.600
because instead of being able to map all of your facial movements onto a

215
00:13:09.881 --> 00:13:14.040
computer generated a virtual character or person, uh,

216
00:13:14.410 --> 00:13:19.150
with era and others in this field of facial reenactment have figured out how to

217
00:13:19.151 --> 00:13:24.151
do is to map your facial movements onto a real person,

218
00:13:24.690 --> 00:13:28.800
a, a prerecorded real person. What, what does that even mean? What,

219
00:13:28.840 --> 00:13:31.660
how does that work? The best example of this is a,

220
00:13:31.661 --> 00:13:33.990
is this piece of software that nick showed us. Uh,

221
00:13:34.000 --> 00:13:38.200
this software that I found from these university students called face to face

222
00:13:38.201 --> 00:13:38.891
sweet present,

223
00:13:38.891 --> 00:13:42.700
a novel realtime facial reenactment method that works with any commodity Webcam.

224
00:13:42.980 --> 00:13:45.700
There's a video demo of this and when you open it up, uh,

225
00:13:45.701 --> 00:13:48.790
this very monotone voice comes in saying,

226
00:13:48.791 --> 00:13:52.450
since our method only uses RGB data for both the source and target actor,

227
00:13:52.480 --> 00:13:54.760
and you're like, what the heck is this? And the screen pops up here,

228
00:13:54.761 --> 00:13:57.250
we demonstrate our method in a live setup. On the right,

229
00:13:57.251 --> 00:14:01.420
you've got this heavyset man, a goatee, spiked hair on the right,

230
00:14:01.450 --> 00:14:05.170
a source actor is captured with the standard web, is arching his eyebrows,

231
00:14:05.171 --> 00:14:08.980
he's pursing his lips, he's opening his mouth widely. The sorta like a,

232
00:14:09.010 --> 00:14:11.980
like if you're making funny faces for a two year old kind of thing. Yeah.

233
00:14:12.190 --> 00:14:15.700
And then this input drives the animation of the face and the video shown on the

234
00:14:15.701 --> 00:14:16.660
Monitor to the left.

235
00:14:16.770 --> 00:14:21.770
On the left you've got this Dell computer screen displaying a CNN clip of George

236
00:14:22.260 --> 00:14:27.240
Bush. This is a real clip of Bush back from 2013 and his face is,

237
00:14:27.600 --> 00:14:30.890
they're looking right at the camera, occupies most of that screen.

238
00:14:30.980 --> 00:14:32.620
Significant difference to previous methods.

239
00:14:32.621 --> 00:14:36.880
What you start to notice is when the man with the goatee smiles,

240
00:14:37.150 --> 00:14:40.600
George Bush in the CNN clip also smiles.

241
00:14:40.750 --> 00:14:42.430
And when the man raises his eyebrows,

242
00:14:42.431 --> 00:14:47.431
George Bush raises his eyebrows and you realize this man is controlling George

243
00:14:48.530 --> 00:14:50.250
Bush's face. Wait, so this is a guy in,

244
00:14:50.380 --> 00:14:54.140
in the present controlling a past George Bush.

245
00:14:54.141 --> 00:14:56.210
A real George Bush from an old video clip.

246
00:14:56.330 --> 00:14:59.930
<v 6>Yeah. Okay. I pulled up a video for you here. Okay, cool.</v>

247
00:14:59.990 --> 00:15:02.210
And a little while back when we were just learning about this,

248
00:15:02.390 --> 00:15:05.900
we happen to have our friend Andrew Morantz who writes for the New Yorker in the

249
00:15:05.901 --> 00:15:06.734
studio.

250
00:15:07.290 --> 00:15:11.940
<v 1>So that is George Bushes face. [inaudible] what?</v>

251
00:15:12.870 --> 00:15:16.050
Oh God. Oh God.

252
00:15:16.380 --> 00:15:20.070
That's terrifying. His,

253
00:15:20.970 --> 00:15:24.870
okay. So yeah, I cannot stop watching George Bush's face. Oh,

254
00:15:24.871 --> 00:15:29.490
they're doing with Putin now. Holy God. So I just have a guy just sort of going

255
00:15:31.620 --> 00:15:36.210
[inaudible] and then that's what Putin is doing. Yeah. Oh, now it's Trump.

256
00:15:36.270 --> 00:15:40.920
You know, I mean those videos online, how my mouth agape again,

257
00:15:41.010 --> 00:15:45.930
Nick Bilton, this is this, this is a form of puppetry where your face is the,

258
00:15:46.050 --> 00:15:49.530
is the puppeteer than the only thing is, is, is that George W.

259
00:15:49.530 --> 00:15:51.900
Bush as the puppet. So I sit in front of a camera,

260
00:15:51.930 --> 00:15:55.530
I smile and the business is taken care of. That's real time.

261
00:15:55.560 --> 00:15:58.590
This isn't like you have to render some software on your computer.

262
00:15:58.620 --> 00:16:03.620
It's literally you download a clip or you take a clip from the cable news and

263
00:16:03.991 --> 00:16:07.500
you turn on your Webcam and however long it takes you to do it and you're done.

264
00:16:08.070 --> 00:16:11.980
It's the same as to shooting a video on your phone. What is this for? What?

265
00:16:12.060 --> 00:16:13.280
So what, what are the applications,

266
00:16:13.710 --> 00:16:15.320
<v 7>Jason? Um,</v>

267
00:16:15.380 --> 00:16:20.270
I want to be able to help cal develop telepresence. This is era again.

268
00:16:20.480 --> 00:16:25.100
So I love telepresence. Yeah. So for example,

269
00:16:25.520 --> 00:16:28.460
so my mom leaves in these rail, um,

270
00:16:28.760 --> 00:16:33.680
and I'm here and, um, what is it be cool if I could, um,

271
00:16:34.130 --> 00:16:36.230
have some, it's kind of crazy, but right.

272
00:16:36.231 --> 00:16:41.231
But if I could have some kind of Hologram of her sitting on my couch here and we

273
00:16:42.981 --> 00:16:44.450
can have a conversation

274
00:16:44.660 --> 00:16:46.890
<v 6>and going one step further. One of your colleagues,</v>

275
00:16:46.891 --> 00:16:48.600
a guy by the name of Steve Sights,

276
00:16:48.601 --> 00:16:51.540
I'm a professor at the University of Washington and I also work part time at

277
00:16:51.541 --> 00:16:51.990
Google.

278
00:16:51.990 --> 00:16:56.460
He told me that they see this technology as like a building block that could one

279
00:16:56.461 --> 00:17:00.780
day be used to essentially virtually bring someone back from the dead.

280
00:17:01.020 --> 00:17:05.370
I just think this technology combined with the virtual reality and other

281
00:17:05.371 --> 00:17:07.170
innovations could help me, you know,

282
00:17:07.171 --> 00:17:11.700
just be there in the room with Albert Einstein or Carl Sagan. You know,

283
00:17:12.060 --> 00:17:15.000
that that's sort of the motivation. That's what they want to do.

284
00:17:15.001 --> 00:17:18.930
That's the motivation to ghosts. Well, for them. Yes. And uh,

285
00:17:19.230 --> 00:17:21.690
when I was talking to some folks who work in commercials,

286
00:17:21.691 --> 00:17:26.691
they're developing their own version of this and the idea is that they're going

287
00:17:26.991 --> 00:17:31.980
to make a million or $1 billion off of this because a, say you bring, I dunno,

288
00:17:32.700 --> 00:17:33.533
uh,

289
00:17:34.200 --> 00:17:39.120
Jennifer Anniston in to film some makeup commercial and in the makeup commercial

290
00:17:39.240 --> 00:17:41.610
in English. She says, so come and buy this product.

291
00:17:41.611 --> 00:17:45.750
This is the best sort of whatever product around right now. You've got China,

292
00:17:45.751 --> 00:17:46.920
which is a booming market.

293
00:17:47.070 --> 00:17:50.070
You maybe want to market things to China and really like to be able to use

294
00:17:50.071 --> 00:17:54.630
Jennifer Anniston. Problem is Jennifer Annison doesn't speak Mandarin.

295
00:17:54.870 --> 00:17:59.870
So either you use the same audio clip and you have someone come in and speak a

296
00:18:00.570 --> 00:18:03.030
mandarin over her and the lips don't line up.

297
00:18:03.240 --> 00:18:05.760
Or You have to hire a Mandarin speaking, uh,

298
00:18:05.850 --> 00:18:10.500
actor to come in and do the part of Jennifer Anniston with this technology.

299
00:18:10.740 --> 00:18:13.050
All you have to do is record Jennifer Aniston.

300
00:18:13.051 --> 00:18:18.051
Once you can hire a mandarin speaker and the mandarin speakers voice will be

301
00:18:18.361 --> 00:18:22.980
coming out of Jennifer Aniston's mouth is if she had said it and in front of the

302
00:18:22.981 --> 00:18:27.330
camera, her lips would be moving as if she were a perfect mandarin speaker.

303
00:18:27.390 --> 00:18:31.870
Exactly. Exactly. Wow. I think that part of it is actually, that's,

304
00:18:32.460 --> 00:18:34.860
that's amazing. Yeah. Oh my God.

305
00:18:35.640 --> 00:18:39.270
I'm amazed and completely frightened by what you're telling me.

306
00:18:39.450 --> 00:18:42.500
And that's the whole point of what nick was writing about that, uh,

307
00:18:42.750 --> 00:18:47.750
that gave me shivers that someday if you join the video manipulation with the

308
00:18:48.451 --> 00:18:52.200
Voco voice manipulation, you, you're, you're the,

309
00:18:52.320 --> 00:18:54.480
the ultimate puppeteer. You can create

310
00:18:55.470 --> 00:18:59.850
<v 8>anyone talking about anything that you want in their own voice and having any</v>

311
00:18:59.851 --> 00:19:04.470
kind of emotion around it and you'd have it right there for everyone to see in

312
00:19:04.471 --> 00:19:09.420
video. And all you need to do is take that and put it on Twitter or Facebook.

313
00:19:10.110 --> 00:19:13.770
And if it's shocking enough, I minutes later it's everywhere.

314
00:19:18.240 --> 00:19:19.073
<v 5>[inaudible]</v>

315
00:19:22.080 --> 00:19:26.940
<v 8>like the timing of you guys making this thing and then this explosion of fake</v>

316
00:19:26.941 --> 00:19:31.230
news like, Huh? How do you guys think about,

317
00:19:31.260 --> 00:19:34.020
about how it could be used for nefarious purposes?

318
00:19:35.730 --> 00:19:40.570
<v 7>Uh, yeah, it's a good question. Um, again, you're a camel mockers lizard man.</v>

319
00:19:40.810 --> 00:19:43.210
I feel like when every technology is developed, then there,

320
00:19:43.211 --> 00:19:47.140
there is this danger of, uh, with our technology, you, um,

321
00:19:47.200 --> 00:19:50.530
you can create fake videos and so on, or I don't want to call it fake videos,

322
00:19:50.531 --> 00:19:53.890
but like to create video from audio. Right. But they are fake videos. Yeah.

323
00:19:54.180 --> 00:19:57.490
Yeah. But the way that I think about it is that like,

324
00:19:57.491 --> 00:19:59.470
scientists are doing their job in science,

325
00:19:59.530 --> 00:20:02.470
like inventing the technology and sign it off.

326
00:20:02.500 --> 00:20:06.640
And then we all need to like think about the next steps. Obviously. I mean,

327
00:20:06.880 --> 00:20:10.420
people shouldn't work on that. Um, and the answer is not clear.

328
00:20:10.421 --> 00:20:12.460
Maybe it's education. Maybe it's, um,

329
00:20:12.520 --> 00:20:16.060
every video should come up with some code now that this is,

330
00:20:16.330 --> 00:20:20.290
this is like authentic video or authentic attacks and you don't believe anything

331
00:20:20.291 --> 00:20:24.850
else. I mean, yeah, but like it maybe it was the timing more than anything,

332
00:20:24.851 --> 00:20:28.510
but I saw this video and it really felt like, oh my God,

333
00:20:28.511 --> 00:20:33.030
like America can't handle this right now. Like we're in a moment where we're,

334
00:20:33.060 --> 00:20:36.680
we're truth seems to be sort of a, an open disk.

335
00:20:36.710 --> 00:20:41.710
What is true is has become an open discussion and this seems to be adding fuel

336
00:20:41.771 --> 00:20:43.960
on the fire of sort of, uh,

337
00:20:45.040 --> 00:20:48.070
competing narratives in a way that I find

338
00:20:48.070 --> 00:20:50.680
<v 1>troubling. And I'm just curious that you don't,</v>

339
00:20:53.600 --> 00:20:55.040
<v 7>um, a thing that</v>

340
00:20:57.130 --> 00:21:01.120
I think that people, if people know the touch technology exists,

341
00:21:01.810 --> 00:21:05.260
then they will be more skeptical. My guess, I don't know.

342
00:21:05.710 --> 00:21:09.610
But if people know that fake news exists, if they know that fake texts exists,

343
00:21:09.910 --> 00:21:11.890
fake videos exist, fake photos exist,

344
00:21:11.891 --> 00:21:16.080
then everyone is more skeptical in what they read and see.

345
00:21:16.900 --> 00:21:20.860
<v 1>But like, Eh, a man in North Carolina, I think he was from North Carolina,</v>

346
00:21:20.861 --> 00:21:24.970
believed from a fake print article that Hillary Clinton was running a sex ring

347
00:21:24.971 --> 00:21:28.930
out of a pizza parlor in DC, which is like insane.

348
00:21:29.260 --> 00:21:31.120
This man believed it and showed up with a gun.

349
00:21:31.390 --> 00:21:35.110
And if people are at a moment where they are willing to believe stories as

350
00:21:35.111 --> 00:21:36.250
ludicrous as that,

351
00:21:36.640 --> 00:21:41.060
like I don't expect them to wonder if this video is real or not.

352
00:21:43.930 --> 00:21:44.763
<v 5>[inaudible]</v>

353
00:21:46.480 --> 00:21:48.220
<v 7>um, so whether you asking,</v>

354
00:21:48.750 --> 00:21:52.500
<v 1>I'm asking, I'm asking, do you, are you afraid of the power of [inaudible]?</v>

355
00:21:52.810 --> 00:21:55.190
<v 7>Yes. And if not, why?</v>

356
00:21:57.880 --> 00:22:01.880
Just I'm just giving my, I don't know, just, uh, I'm answering your questions,

357
00:22:01.881 --> 00:22:06.710
but I'm a technologist. I'm a computer scientist, so I'm not really,

358
00:22:06.711 --> 00:22:09.750
because I know how to read and I know that because I know that this technology

359
00:22:09.751 --> 00:22:14.600
is reversible. I mean nobody. Well, there is not,

360
00:22:18.810 --> 00:22:23.010
not worried too much

361
00:22:30.360 --> 00:22:30.590
<v 5>[inaudible].</v>

362
00:22:30.590 --> 00:22:34.000
<v 1>Have you seen these videos otherwise? I can text it, yeah. Okay. Yeah,</v>

363
00:22:34.550 --> 00:22:37.040
and as we were feeling worried and more than that,

364
00:22:37.850 --> 00:22:40.280
surprised that the folks making these technologies weren't,

365
00:22:40.970 --> 00:22:42.950
we decided to do a third gut check.

366
00:22:43.010 --> 00:22:46.580
See if we were totally off base and get in touch with one of the guys who's on

367
00:22:46.610 --> 00:22:47.900
the front lines of this.

368
00:22:48.380 --> 00:22:51.200
Can you describe what was going through your head when you were watching bushes

369
00:22:51.710 --> 00:22:54.830
face? I can tell you exactly what I was thinking. I was thinking,

370
00:22:55.100 --> 00:22:57.800
how are we going to develop a forensic technique to detect this?

371
00:23:00.170 --> 00:23:01.490
This is honey for reed.

372
00:23:01.550 --> 00:23:04.880
I am a professor of computer science at Dartmouth College.

373
00:23:05.010 --> 00:23:08.120
He started like a Sherlock Holmes of digital misdeeds,

374
00:23:08.121 --> 00:23:11.150
which means that it spends a lot of time sitting around looking at pictures and

375
00:23:11.151 --> 00:23:13.910
videos, trying to understand where has this come from?

376
00:23:13.911 --> 00:23:15.860
Has it been manipulated and should we trust it?

377
00:23:16.040 --> 00:23:18.770
He's been worked for all sorts of organizations. The AP,

378
00:23:18.771 --> 00:23:23.000
The Times Reuters who want to know if say a picture is fake or not,

379
00:23:23.090 --> 00:23:26.510
they often will ask me, you have to ugly when this just happened.

380
00:23:26.670 --> 00:23:29.020
Actually yesterday images came out of North Korea. Um,

381
00:23:29.160 --> 00:23:32.660
and every time images come out of these regimes where there's a history of photo

382
00:23:32.661 --> 00:23:34.970
manipulation, there are real concerns about this.

383
00:23:35.030 --> 00:23:38.750
So I was asked to determine if they'd been manipulated in some way and if so,

384
00:23:38.751 --> 00:23:42.950
how had they been manipulated and what, how the heck would you do that? Wow.

385
00:23:43.370 --> 00:23:46.040
Every time you manipulate data, you're going to leave something

386
00:23:46.040 --> 00:23:49.280
<v 9>behind. So let's say you do some funny business to a photo,</v>

387
00:23:49.340 --> 00:23:52.700
you might create some noticeable distortion in the picture itself,

388
00:23:52.730 --> 00:23:55.100
but you also might distort the data.

389
00:23:55.130 --> 00:23:59.390
And we're in the business of basically finding those distortions in the data.

390
00:23:59.990 --> 00:24:03.980
For example, imagine he gets sent a photo, it's probably a Jpeg, jpeg,

391
00:24:03.981 --> 00:24:08.520
which now is 99% of the image formats that we see out there is what is called

392
00:24:08.600 --> 00:24:10.250
the lossy compression scheme.

393
00:24:10.490 --> 00:24:14.900
Just a fancy way to say that when a photo is taken and stored as a Jpeg,

394
00:24:15.110 --> 00:24:19.220
the camera, you know, just to save space throws a little bit of the data away.

395
00:24:19.520 --> 00:24:20.780
So for example,

396
00:24:20.810 --> 00:24:23.900
if I went out to the Dartmouth green right now took a picture of the grass,

397
00:24:24.680 --> 00:24:28.820
the camera isn't going to store all those millions of little variations of green

398
00:24:28.821 --> 00:24:31.880
hidden in the grass because that would be just a huge file.

399
00:24:32.120 --> 00:24:35.720
It's going to save space by throwing some of those greens away.

400
00:24:36.200 --> 00:24:40.340
You just don't notice if it changes like a lot or a little bit less than that.

401
00:24:40.370 --> 00:24:43.280
It just grass as far as you can tell. Now here's how next trick.

402
00:24:43.310 --> 00:24:47.270
Every camera has a subtly different Palette of Greens that's going to keep and

403
00:24:47.271 --> 00:24:48.650
greens that's going to throw away.

404
00:24:48.680 --> 00:24:51.710
This varies tremendously from device to device.

405
00:24:51.920 --> 00:24:56.390
An iPhone compresses the image and much more so less Greens and a high end icon

406
00:24:56.391 --> 00:24:59.540
or a high on cannon, which would keep more of those variations of green.

407
00:24:59.870 --> 00:25:02.870
Now if you hold these two pictures side by side,

408
00:25:02.871 --> 00:25:06.200
you might not be able to tell the difference. But honey says,

409
00:25:06.201 --> 00:25:08.450
when you look at the underlying pixels,

410
00:25:08.660 --> 00:25:11.430
there are different recognizable patterns.

411
00:25:11.490 --> 00:25:13.160
If you take an image off of your iPhone,

412
00:25:13.220 --> 00:25:17.000
I should be able to go into that jpeg and look at the packaging and say, ah yes,

413
00:25:17.001 --> 00:25:18.620
this should have come out of an iPhone.

414
00:25:18.710 --> 00:25:22.790
But if that image is uploaded to Facebook and then redownloaded or put into

415
00:25:22.791 --> 00:25:26.930
Photoshop and re saved, it will not look like jpeg consistent with an iPhone.

416
00:25:27.020 --> 00:25:30.500
So basically he can see at the level of the pixels or data whether the picture

417
00:25:30.501 --> 00:25:33.500
has been messed with in any way. Huh. And this is of course,

418
00:25:33.501 --> 00:25:37.610
just one of many different ways that honey can spot a fake. Yeah. Let me ask,

419
00:25:37.880 --> 00:25:42.880
if you could go up against the top 100 best counterfeiters,

420
00:25:44.750 --> 00:25:47.750
do you think you'd catch them 10% of the time?

421
00:25:47.780 --> 00:25:50.930
50% of the time just to have kids? What's your sense? Um,

422
00:25:51.170 --> 00:25:54.620
I would say we could probably catch 75% of the fakes,

423
00:25:54.650 --> 00:25:58.640
but I would say that would take a long time to do this is not an easy task.

424
00:25:58.970 --> 00:25:59.950
And so, you know, the,

425
00:25:59.951 --> 00:26:04.951
the pace at which the media moves does not lend itself to careful forensic

426
00:26:05.721 --> 00:26:09.530
analysis of images. Um, I'm always amazed that you get these emails,

427
00:26:09.531 --> 00:26:12.740
you're like, all right, you got 20 minutes and, um, you would need, you know,

428
00:26:12.741 --> 00:26:17.270
half a day, a day per image, still a very manual and a very human process.

429
00:26:17.680 --> 00:26:18.610
It's so are,

430
00:26:18.950 --> 00:26:23.690
is this video editing and this audio editing that's coming down the pipeline

431
00:26:23.691 --> 00:26:27.710
here? Yeah, I guess, should I be, should I be terrified?

432
00:26:29.000 --> 00:26:33.680
Um, yes, you should. Um, oh no. Did you really mean that?

433
00:26:34.190 --> 00:26:35.000
Yeah, I think it's,

434
00:26:35.000 --> 00:26:38.150
I think it's going to raise the fake news thing to a whole new level.

435
00:26:38.540 --> 00:26:41.900
I did see some artifacts by the way, and the videos, they are not perfect,

436
00:26:41.960 --> 00:26:46.050
but that's neither here nor there because the ability of technology to

437
00:26:46.051 --> 00:26:50.290
manipulate and alter reality is growing at a breakneck speed. Um,

438
00:26:50.550 --> 00:26:54.250
and the ability to disseminate that information is phenomenal. So I,

439
00:26:54.251 --> 00:26:55.830
I can't stop that by the way,

440
00:26:55.860 --> 00:26:59.190
because at the end of the day it's always going to be easier to create a fake

441
00:26:59.520 --> 00:27:00.660
but to detect a fake. Hmm.

442
00:27:06.440 --> 00:27:07.273
<v 5>[inaudible]</v>

443
00:27:07.720 --> 00:27:09.190
<v 9>thank you very much. We're going to,</v>

444
00:27:10.240 --> 00:27:13.030
Chad himself just handed me a cup of water,

445
00:27:13.031 --> 00:27:16.270
which shows none of you have gotten too big for your britches and that could be

446
00:27:16.271 --> 00:27:17.830
a serious problem.

447
00:27:17.860 --> 00:27:21.550
I would like to have seen Peter Jennings do that ever for this guy.

448
00:27:21.610 --> 00:27:26.200
My name is John Klein, Co founder and CEO of tap media. Before that,

449
00:27:26.201 --> 00:27:27.250
president of CNN,

450
00:27:27.251 --> 00:27:32.251
u s before that I was executive vice president of CBS News where I was executive

451
00:27:32.651 --> 00:27:34.540
in charge of 60 minutes,

452
00:27:34.541 --> 00:27:39.370
48 hours and a bunch of other things and he's had to react to some serious

453
00:27:39.371 --> 00:27:41.510
evolutions in the media industry. Uh,

454
00:27:41.710 --> 00:27:44.380
he was manning the helm as social media exploded,

455
00:27:44.381 --> 00:27:46.390
as smartphones became ubiquitous.

456
00:27:46.540 --> 00:27:51.540
And consequently he had to deal with figuring out how and if to trust thousands

457
00:27:51.940 --> 00:27:55.270
of hours of video taken on these smart phones incented by viewers,

458
00:27:55.510 --> 00:27:57.430
what to broadcast and what not.

459
00:27:57.820 --> 00:28:01.870
And so we wanted to know how someone in his position would think about these

460
00:28:01.871 --> 00:28:02.704
fake videos.

461
00:28:02.950 --> 00:28:07.000
So we sent him all of the different demos and videos we'd come across just to

462
00:28:07.001 --> 00:28:07.810
see what he thought.

463
00:28:07.810 --> 00:28:12.810
First thought was that this is the kind of thing that a James Bond villain would

464
00:28:13.390 --> 00:28:16.570
put to use or a the joker in Batman.

465
00:28:16.810 --> 00:28:20.710
Now we're an eighth grade girl who right. Wants to be most popular. Exactly.

466
00:28:20.800 --> 00:28:22.540
Yeah. You know, I mean, this is,

467
00:28:22.810 --> 00:28:26.800
there's so many ways to abuse this blows your mind.

468
00:28:28.180 --> 00:28:29.680
I mean, it,

469
00:28:30.520 --> 00:28:35.520
it goes to the very core of communication of any sort,

470
00:28:38.100 --> 00:28:42.670
whether it's television or radio or interpersonal.

471
00:28:43.720 --> 00:28:48.040
Um, it is what I'm seeing. True is what I'm hearing real,

472
00:28:48.730 --> 00:28:50.680
uh, in, in your, over the course of your career,

473
00:28:50.681 --> 00:28:55.510
you've seen multiple technological developments that have impacted the media in

474
00:28:55.511 --> 00:28:59.110
rather profound ways. Where is your terror level right now,

475
00:28:59.111 --> 00:29:03.070
or your fear level caused by this relative to all of the other sort of

476
00:29:03.071 --> 00:29:07.450
advancements that have occurred over, over your career? It's terrifying.

477
00:29:08.650 --> 00:29:09.910
And it,

478
00:29:10.110 --> 00:29:14.820
it hurdles us even faster toward that point where no one believes anything.

479
00:29:17.260 --> 00:29:22.260
<v 1>How do you have a democracy in a country where people can't trust anything that</v>

480
00:29:23.321 --> 00:29:25.740
they see or read anymore? What,

481
00:29:26.140 --> 00:29:30.950
what we saw happen with the fake news during the election cycle was that, um,

482
00:29:31.000 --> 00:29:35.290
all the, the, it doesn't, it didn't even need to matter if anyone, you know,

483
00:29:35.500 --> 00:29:38.890
would rebuff it afterwards. This is nick built. And again,

484
00:29:39.100 --> 00:29:44.050
it would reach millions and millions of in me or seconds and,

485
00:29:44.290 --> 00:29:47.290
and you, and that was it. You've done it had done as j it's job.

486
00:29:47.291 --> 00:29:52.060
And I think that with this audio stuff and the video stuff that's gonna that's

487
00:29:52.061 --> 00:29:55.990
gonna come down, uh, on online in the next few years. Um,

488
00:29:56.020 --> 00:29:57.200
it's gonna do the same thing, but,

489
00:29:57.550 --> 00:29:59.160
but no one's going to know what's real and what's not.

490
00:29:59.300 --> 00:30:01.670
<v 10>I moved on her actually, you know, she was down in Palm Beach.</v>

491
00:30:01.671 --> 00:30:02.900
I moved on her NFL.

492
00:30:03.020 --> 00:30:04.400
<v 1>And what's more, Nick says,</v>

493
00:30:04.490 --> 00:30:09.090
if you think about the video that came out of Donald Trump from access Hollywood

494
00:30:09.360 --> 00:30:11.280
<v 10>automatically attracted to beautiful. I just started.</v>

495
00:30:11.660 --> 00:30:14.270
<v 1>Um, the thing that was really interesting about that video</v>

496
00:30:14.350 --> 00:30:16.900
<v 10>when you were started, they let you do it. You can do anything,</v>

497
00:30:16.960 --> 00:30:18.160
whatever you want, grab by the,

498
00:30:18.550 --> 00:30:23.470
<v 1>you don't actually see Donald Trump until the very last second when he gets off</v>

499
00:30:23.471 --> 00:30:27.280
the bus. How were you high? You only hear him make me a soapstone.

500
00:30:27.281 --> 00:30:31.420
I have a little hug with Donnelley and so if that technology existed today,

501
00:30:31.540 --> 00:30:36.100
I can guarantee you that Donald Trump would have responded by saying, oh,

502
00:30:36.101 --> 00:30:40.060
it's fake. It's fake news, fake audio. You can't see me. I didn't say that.

503
00:30:40.720 --> 00:30:43.660
And it would just be this video, his word against his

504
00:30:45.700 --> 00:30:46.090
<v 5>[inaudible].</v>

505
00:30:46.090 --> 00:30:48.890
<v 11>Okay. Actually that's kind of like for me, that's sort of the real problem here.</v>

506
00:30:48.980 --> 00:30:49.850
Like you create this,

507
00:30:50.090 --> 00:30:55.090
this possibility for like plausible deniability that's so broad.

508
00:30:56.060 --> 00:30:58.010
You know what I mean? It's like, you know,

509
00:30:58.011 --> 00:31:01.760
it's like the tobacco industry in the sixties and seventies I was just reading

510
00:31:01.761 --> 00:31:02.870
this great article by a,

511
00:31:02.890 --> 00:31:07.040
the writer Tim Harford about this in the sixties and seventies the tobacco

512
00:31:07.041 --> 00:31:12.041
industry led this very calculated effort to sort of push back against cancer

513
00:31:12.861 --> 00:31:14.550
science by, you know,

514
00:31:14.570 --> 00:31:19.460
just injecting a little bit of doubt here. A little bit of doubt there.

515
00:31:19.461 --> 00:31:23.000
Right. On the other hand, on the other hand, this and on the other hand that,

516
00:31:23.001 --> 00:31:26.240
and the idea was to create just enough wiggle room that nothing happens.

517
00:31:26.480 --> 00:31:28.640
They do that with climate change too. Exactly.

518
00:31:29.000 --> 00:31:33.110
And it's that little bit of doubt that creates paralysis.

519
00:31:33.560 --> 00:31:36.770
And is that what's going to happen that like there's going to be paralysis now

520
00:31:37.340 --> 00:31:41.360
writ large because now we're talking about the very things we see,

521
00:31:41.361 --> 00:31:42.470
the very things we hear

522
00:31:43.570 --> 00:31:43.700
<v 5>[inaudible]</v>

523
00:31:43.700 --> 00:31:44.091
<v 1>but wait,</v>

524
00:31:44.091 --> 00:31:47.540
but we don't you think that before we get completely carried away with the

525
00:31:47.541 --> 00:31:49.610
threat of this technology because, you know,

526
00:31:49.640 --> 00:31:53.900
maybe we should just find out literally where we are now. Yeah.

527
00:31:53.930 --> 00:31:57.230
We should give it a spin. Yeah. So at this moment,

528
00:31:57.231 --> 00:32:00.080
do you think making one of these eclipses is, is possible?

529
00:32:01.080 --> 00:32:04.610
I think it's entirely possible. Um, it just, I would be careful what it is.

530
00:32:11.990 --> 00:32:12.150
<v 5>[inaudible]</v>

531
00:32:12.150 --> 00:32:13.830
<v 11>after the break, things get</v>

532
00:32:15.090 --> 00:32:15.923
<v 5>fake.</v>

533
00:32:30.090 --> 00:32:30.490
[inaudible]

534
00:32:30.490 --> 00:32:34.230
<v 12>how do you everyone, it's Angela calling from Dallas, Texas.</v>

535
00:32:34.380 --> 00:32:39.210
Radiolab is supported in part by the outfit piece. Sloan Foundation,

536
00:32:39.260 --> 00:32:43.340
enhancing public understanding of science and technology in the modern world.

537
00:32:43.520 --> 00:32:48.520
More information aboutSloan@wwwdotsloan.org thanks.

538
00:32:50.631 --> 00:32:51.530
Radiolab.

539
00:32:56.840 --> 00:33:00.980
<v 6>Jad Robert Radiolab. So we're back. We're going to now fake something.</v>

540
00:33:01.040 --> 00:33:04.820
We're going to build our own video from scratch. Fake words, fake faces,

541
00:33:05.000 --> 00:33:07.280
because we want to know like in use,

542
00:33:07.340 --> 00:33:12.050
how dangerous are these technologies really? Can they make a convincing fake?

543
00:33:12.080 --> 00:33:14.390
Are they as easy as advertised?

544
00:33:14.450 --> 00:33:19.450
So we will find out by giving the assignment as always to our long suffering.

545
00:33:20.060 --> 00:33:24.430
Simon Adler. So while I was in Seattle talking to during Gleaves, Dolby polite,

546
00:33:24.470 --> 00:33:29.390
not so subtly hinted that I would really like to, uh, to give vocal world.

547
00:33:29.400 --> 00:33:34.050
Let's say I had my hands on it somehow. What can I do with it? Well,

548
00:33:34.080 --> 00:33:38.040
right now, nothing, because we haven't shared it with them. At first,

549
00:33:38.041 --> 00:33:40.740
I just thought he didn't want me to be able to play around with it.

550
00:33:40.741 --> 00:33:44.550
But then I realized that, and I didn't even have a personal copy for myself,

551
00:33:44.610 --> 00:33:48.080
but yet. Oh, so it's not even on the premises here? No,

552
00:33:48.230 --> 00:33:51.240
it's still very much a contained to research.

553
00:33:52.030 --> 00:33:54.340
<v 13>Oh, but</v>

554
00:33:55.430 --> 00:33:58.910
<v 6>hi. Hi there. Hey Matt. Oh yeah, yeah, I'm here. Great.</v>

555
00:33:59.250 --> 00:34:02.750
Eventually I got in touch with this guy. So I'm Dr Matthew Elites.

556
00:34:02.780 --> 00:34:05.660
I'm the Chief Science Officer at Sarah Proc Limited,

557
00:34:05.720 --> 00:34:09.220
which is the vocal synthesis research company based in Edinburgh. Yeah. Okay.

558
00:34:09.221 --> 00:34:14.221
So I called you up because I was hoping that you could help me to make a video

559
00:34:15.511 --> 00:34:16.770
clip that has, I don't know,

560
00:34:16.771 --> 00:34:21.771
like George Bush or Barack Obama saying things that they have never said.

561
00:34:23.960 --> 00:34:27.830
Yep. That sounds great. That's it. He was just game. Yeah, no. See,

562
00:34:27.860 --> 00:34:31.670
the thing is, what his company does is not quite the same as Voco.

563
00:34:31.940 --> 00:34:33.630
What they do is like for a client,

564
00:34:33.890 --> 00:34:37.760
they'll create a voice that you can then just type in words or sentences and

565
00:34:37.761 --> 00:34:40.970
make that voice say whatever you want it to say. I feel sad.

566
00:34:40.971 --> 00:34:42.960
That's an interesting idea. They've, uh,

567
00:34:43.010 --> 00:34:45.020
created voices with a variety of accents.

568
00:34:45.050 --> 00:34:47.840
Great for you said last summer in a variety of languages.

569
00:34:50.400 --> 00:34:53.600
[inaudible] and in their spare time when they're not making voices for clients,

570
00:34:54.000 --> 00:34:54.970
govern knowledge. What's N***a?

571
00:34:55.130 --> 00:35:00.130
I think they're building celebrity voices future and it just so happens they've

572
00:35:00.681 --> 00:35:03.620
got a Barack Obama and a George Bush bought. Yeah.

573
00:35:03.800 --> 00:35:07.240
How did did you create a George Bush robot? Well,

574
00:35:07.390 --> 00:35:11.050
a great thing about George Bush is that he was president of the United States

575
00:35:11.051 --> 00:35:13.240
for some time the morning, good morning, good morning,

576
00:35:13.270 --> 00:35:17.200
which means he had to give the weekly presidential address. We can go today.

577
00:35:17.201 --> 00:35:20.430
I received a great honor and the other great thing about the addresses,

578
00:35:20.440 --> 00:35:22.000
it's completely copyright free,

579
00:35:22.001 --> 00:35:25.090
so we were allowed to do anything we liked with that audio for the people of

580
00:35:25.091 --> 00:35:25.924
America,

581
00:35:26.020 --> 00:35:28.540
maybe things that they haven't envisaged that we're going to do with that.

582
00:35:28.630 --> 00:35:31.450
Real quick digression here just because it's absolutely fascinating,

583
00:35:31.930 --> 00:35:36.310
it looks like we're actually about to enter this really sticky gray area when it

584
00:35:36.311 --> 00:35:39.210
comes to ownership. For example, in audiobook,

585
00:35:39.300 --> 00:35:43.320
if you record an audio book and you've signed over the rights to those audio

586
00:35:43.321 --> 00:35:47.250
files to the publisher, the publisher has the copyright. You, you don't own it.

587
00:35:47.251 --> 00:35:51.300
You do not own your own voice. Is that really true? Yeah. Anyway,

588
00:35:51.610 --> 00:35:56.110
back to Bush. So I took all those weekly addresses about six hours worth,

589
00:35:56.140 --> 00:35:58.660
which is a lot more tape than Volvo's 20 minutes.

590
00:35:58.720 --> 00:36:01.210
But what he did with it is pretty similar, right?

591
00:36:01.211 --> 00:36:04.840
You Fed them into this machine learning algorithm along with their transcripts.

592
00:36:05.020 --> 00:36:07.770
And then with the program we'll do a,

593
00:36:08.020 --> 00:36:11.800
it will take the text and it will analyze it in terms of the linguistics that

594
00:36:11.890 --> 00:36:14.790
will say this is the word social security, social,

595
00:36:15.590 --> 00:36:19.780
social is made up of the sound, sir. Oh, oh right.

596
00:36:20.170 --> 00:36:25.170
And so we'll cut those sounds out into lots of little tiny pieces.

597
00:36:25.360 --> 00:36:27.490
And it did that for all of the words

598
00:36:29.000 --> 00:36:33.010
in all of these addresses around 80,000 in total.

599
00:36:34.330 --> 00:36:38.290
Put them all in this database with tons of info about what sound came before it,

600
00:36:38.440 --> 00:36:41.740
after, etc. And once that database is built,

601
00:36:42.160 --> 00:36:44.890
all that's left to do by typing some text. And then I push,

602
00:36:44.980 --> 00:36:49.980
go and try and find a set of little sounds which will join together really

603
00:36:50.141 --> 00:36:54.460
nicely. And then I pushed play and see how well they came out.

604
00:36:56.310 --> 00:36:59.960
So what we did was we found an old video of former presidents,

605
00:36:59.970 --> 00:37:02.180
George Bush and Barack Obama together, right?

606
00:37:04.090 --> 00:37:06.300
They're shaking hands, making generic statements.

607
00:37:06.470 --> 00:37:07.830
The exact clip isn't important,

608
00:37:07.950 --> 00:37:12.950
but we wondered could we turn that clip from a boring meet and greet to a

609
00:37:13.501 --> 00:37:16.650
scenario where Bush is telling Obama a joke.

610
00:37:16.950 --> 00:37:21.660
So we convinced a comedy writer Rachel Axler who works for the show veep to

611
00:37:21.661 --> 00:37:25.410
write as few jokes and then sent it off to Matt and this is what the computer

612
00:37:25.411 --> 00:37:26.244
spat out

613
00:37:26.300 --> 00:37:29.390
<v 14>and well it goes something like knock knock,</v>

614
00:37:29.391 --> 00:37:31.970
who's there overall overall,

615
00:37:32.210 --> 00:37:35.720
I think it's something about the Oval Office. Probably.

616
00:37:36.170 --> 00:37:41.090
That was a a very good joke. Mr President, my wife Lori tells it better.

617
00:37:44.410 --> 00:37:48.190
<v 6>What the hell is that? Wait, what was that? That was terrible.</v>

618
00:37:50.730 --> 00:37:53.290
That was like was it, I don't even get it. Hey,

619
00:37:53.291 --> 00:37:54.700
I don't understand that joke at all.

620
00:37:55.060 --> 00:37:57.430
And that's literally what the computer spit out.

621
00:37:57.520 --> 00:37:59.830
That is what the computer sped out. And truth be told,

622
00:37:59.831 --> 00:38:00.900
I don't think it's anywhere.

623
00:38:00.940 --> 00:38:04.900
It is not worthy of the negative response that you are getting. Terrible.

624
00:38:05.200 --> 00:38:07.210
Terrible. Let me show you another one.

625
00:38:07.510 --> 00:38:11.830
<v 14>So happy to be joining forces with this good man to put cortisone in your</v>

626
00:38:11.831 --> 00:38:12.664
drinking water.

627
00:38:13.490 --> 00:38:17.200
What gets to help protect people's teeth so they don't get fillings?

628
00:38:17.590 --> 00:38:21.340
Isn't that fluoride? Oh shoot. I think I signed the wrong bill.

629
00:38:22.680 --> 00:38:26.050
<v 6>That's good. That's a legitimate, no, the robots are terrible.</v>

630
00:38:26.720 --> 00:38:28.270
The joke is funny. Yeah, I like the joke,

631
00:38:28.710 --> 00:38:32.080
but the robots just massacred that joke, which is in itself kind of a joke.

632
00:38:32.250 --> 00:38:33.910
Well, I do think it, yeah, I'll let me get it. Well,

633
00:38:33.911 --> 00:38:36.700
I think the youtube are far more than you should be and you're far more critical

634
00:38:36.701 --> 00:38:41.020
than the average listener. However, Matt is so wrong about it. But anyway.

635
00:38:41.170 --> 00:38:41.620
Anyhow,

636
00:38:41.620 --> 00:38:46.620
Matt did tell me that conversations I getting people to s to talk back and forth

637
00:38:46.960 --> 00:38:50.390
to each other are still really difficult for a synthesizer to do.

638
00:38:50.510 --> 00:38:50.901
<v 15>But you know,</v>

639
00:38:50.901 --> 00:38:53.780
conversational stuff is always difficult and in fact we're going to see,

640
00:38:53.781 --> 00:38:56.210
it's going to be a long time before we get really,

641
00:38:56.330 --> 00:38:59.690
really easy conversational synthesis. There's all sorts of barriers to that.

642
00:39:00.290 --> 00:39:03.640
<v 6>There's a human quality, uh, to a, to a conversation that, uh,</v>

643
00:39:03.690 --> 00:39:07.720
that the synthesizers can't quite capture yet. But he also told us that, um,

644
00:39:07.930 --> 00:39:11.260
you know, if we add, once we add the video or if we add a video to this, uh,

645
00:39:11.500 --> 00:39:14.200
it will smooth out a lot of, a lot of the problems

646
00:39:14.500 --> 00:39:16.570
<v 15>when you have the faces as well speaking.</v>

647
00:39:16.571 --> 00:39:20.890
People are not focusing in the audio and you can't hear that the areas in the

648
00:39:20.891 --> 00:39:21.724
same way.

649
00:39:21.730 --> 00:39:22.563
<v 6>So</v>

650
00:39:26.190 --> 00:39:26.550
<v 15>hello?</v>

651
00:39:26.550 --> 00:39:30.150
<v 6>Hey, is this Kyle? Oh yeah, great, great. I found these two Grad students.</v>

652
00:39:30.180 --> 00:39:33.570
My name is Sinskey cyto. I am Tyler Shefsky from uh,

653
00:39:33.600 --> 00:39:36.240
the University of southern California USC.

654
00:39:36.300 --> 00:39:40.380
They also do a lot of facial reenactment research and agreed to help us. But,

655
00:39:40.760 --> 00:39:45.270
uh, making these visuals also turned out to be way harder than we thought.

656
00:39:45.500 --> 00:39:49.490
It turned out the clip we chose posed some serious challenges. Uh,

657
00:39:49.530 --> 00:39:52.220
there were too many side shots of Obama's face. Uh,

658
00:39:52.230 --> 00:39:54.450
the lighting was all wrong and, uh,

659
00:39:54.480 --> 00:39:59.400
eventually I got an email one late Sunday night saying, uh, it's not gonna work.

660
00:39:59.580 --> 00:40:04.020
Okay. So now I think I can draw a line here and I can point out that this,

661
00:40:04.050 --> 00:40:07.380
that we maybe got overexcited about this technology.

662
00:40:07.710 --> 00:40:10.500
It is not yet ready for true deceit.

663
00:40:10.530 --> 00:40:13.980
You have been fumbling and fumbling and fumbling here.

664
00:40:14.010 --> 00:40:17.940
I have with them fumble [inaudible] people. Okay.

665
00:40:18.240 --> 00:40:21.420
I find it interesting psychologically that Simon feels like it's a person who

666
00:40:21.840 --> 00:40:26.280
failure. I don't like to fail. You should. This is failure.

667
00:40:26.590 --> 00:40:29.980
<v 11>So, okay, just, just, uh, just on Simon's behalf,</v>

668
00:40:30.010 --> 00:40:32.710
on the behalf of actually trying to answer the question, we felt like, okay,

669
00:40:32.711 --> 00:40:36.190
maybe, maybe we should try this one last time.

670
00:40:36.400 --> 00:40:40.720
Let's find a simpler Obama video and with the audio rather than like whole

671
00:40:40.721 --> 00:40:44.170
phrases. Let's just do a couple of word replacements here or there. By the way,

672
00:40:44.171 --> 00:40:46.510
the only reason we're using Obama is that he seems to be the cow.

673
00:40:46.511 --> 00:40:48.420
All of these technologies are built around it.

674
00:40:48.490 --> 00:40:53.490
In case we chose the video of Obama's last weekly address and we chose the audio

675
00:40:53.621 --> 00:40:58.490
from a talk he'd given in Chicago, uh, after he'd left off. What's,

676
00:40:59.420 --> 00:41:02.170
what's been going on in his speech.

677
00:41:02.171 --> 00:41:04.150
He sort of talks about what he's going to do next,

678
00:41:04.151 --> 00:41:06.790
how he's still gonna keep fighting for what he believes is right.

679
00:41:06.940 --> 00:41:11.940
Filled with idealism and absolutely certain that somehow I was going to change

680
00:41:12.491 --> 00:41:14.020
the world. But we thought,

681
00:41:14.021 --> 00:41:16.840
what if in an alternate reality he didn't want to keep fighting.

682
00:41:17.350 --> 00:41:20.920
What if he could at that moment see the divisions ahead? And he was just like,

683
00:41:20.980 --> 00:41:22.330
that's too much. I give up.

684
00:41:23.890 --> 00:41:26.560
No truth is we didn't think too hard about this because we didn't have much

685
00:41:26.561 --> 00:41:29.290
time. We just whipped it together, did a script based on words.

686
00:41:29.291 --> 00:41:32.260
Obama used with a few changes, send it off to the guys at USC.

687
00:41:32.760 --> 00:41:36.760
<v 9>Right. And I myself, uh, seeing this new script so that, uh,</v>

688
00:41:36.980 --> 00:41:41.750
we can use that video of my face to pop a ties, a the former president. And, uh,

689
00:41:41.780 --> 00:41:43.880
when, when we got the final video back,

690
00:41:47.100 --> 00:41:48.270
<v 11>I have to say it was,</v>

691
00:41:50.900 --> 00:41:54.080
I was expecting it to be horrible and where did it have a good laugh?

692
00:41:54.110 --> 00:41:58.550
But I, it went from like Laffy Keighley to, to,

693
00:41:58.850 --> 00:42:02.360
oh wait, this is creepy.

694
00:42:02.490 --> 00:42:06.400
<v 9>Well, yeah, no, I, I was suddenly, uh, I had been gang busters.</v>

695
00:42:06.480 --> 00:42:07.830
We got to release this thing, uh,

696
00:42:08.250 --> 00:42:11.550
and not tell anybody and try to fake out the entire world. Uh, but when,

697
00:42:11.580 --> 00:42:15.860
when I saw it, there was, uh, a reluctancy. You mean you went, oh no, I went,

698
00:42:15.870 --> 00:42:17.880
Oh God, yeah, yeah. I thought, oh,

699
00:42:17.960 --> 00:42:20.390
<v 11>this, this, you know, my personal thought was like,</v>

700
00:42:20.391 --> 00:42:23.630
it was convincing enough that I got genuinely spooked. But you know,

701
00:42:23.631 --> 00:42:24.351
just in fairness,

702
00:42:24.351 --> 00:42:27.110
we shouldn't sit around talking about something people can't see.

703
00:42:27.770 --> 00:42:32.510
Go to future of fake news.com and check it out for yourself. It's all one word,

704
00:42:32.511 --> 00:42:36.860
future of fake news.com and it'll pop right up.

705
00:42:36.900 --> 00:42:40.520
You can see, tell us what you think. You can see how Simon made the video.

706
00:42:41.570 --> 00:42:43.450
Check it out. Anyhow,

707
00:42:43.490 --> 00:42:47.620
the whole process got us all thinking like, Oh wow,

708
00:42:47.780 --> 00:42:52.660
what if we bunch of idiots can do this, uh, for no money very, very quickly.

709
00:42:53.410 --> 00:42:58.090
What will this mean to like a newsroom, for example, just to start there.

710
00:42:58.700 --> 00:43:00.240
<v 9>Uh, we're at the level now with,</v>

711
00:43:00.270 --> 00:43:05.270
with this kind of thing where we need technologists to,

712
00:43:05.460 --> 00:43:10.080
to verify or knocked down. And again, news executive, John Klein,

713
00:43:10.320 --> 00:43:15.260
I don't think journalists, English majors are, are, are gonna be the ones to,

714
00:43:15.390 --> 00:43:19.140
to solve this. You know, you may have been editor of your school paper,

715
00:43:19.141 --> 00:43:22.080
but this is beyond your, your capability.

716
00:43:22.081 --> 00:43:27.081
But if you're good at collaborating with engineers and scientists,

717
00:43:28.110 --> 00:43:29.490
uh, you know, you'll,

718
00:43:29.520 --> 00:43:33.240
you'll have a good chance of working together to you to figure it out.

719
00:43:33.330 --> 00:43:34.110
So we need,

720
00:43:34.110 --> 00:43:38.550
we need technical expertise more than we ever have.

721
00:43:38.970 --> 00:43:41.780
<v 11>Okay. Can I ask you in your heart, this isn't the only,</v>

722
00:43:41.860 --> 00:43:44.920
let me compare your heart to my heart for a second. In my heart.

723
00:43:45.640 --> 00:43:50.200
I want somebody to tell the researchers he has, sorry, you can't do that.

724
00:43:50.440 --> 00:43:52.840
Sorry. You know, I know. It's really cool. I know you,

725
00:43:52.870 --> 00:43:56.530
I know you probably a really proud of that algorithm, but, um,

726
00:43:57.280 --> 00:43:59.710
some men in black are gonna walk in right now and they're going to take your

727
00:43:59.711 --> 00:44:03.100
computers away. And and you, you just can't, sorry.

728
00:44:03.190 --> 00:44:06.340
Society is going to overrule you right now. Did you,

729
00:44:06.430 --> 00:44:10.030
is there a part of you that just dictatorial, he wants to just like squash this?

730
00:44:10.990 --> 00:44:14.050
<v 9>Sure, but when you still have the, what are they,</v>

731
00:44:14.051 --> 00:44:19.051
the FSB and Moscow or the CIA utilizing this and developing it anyway,

732
00:44:20.560 --> 00:44:25.090
weaponizing it so to speak, probably.

733
00:44:25.770 --> 00:44:30.070
I think that the top down model could never contain that.

734
00:44:30.100 --> 00:44:34.650
John says ultimately happening is probably going to be bigger than any one

735
00:44:34.651 --> 00:44:37.770
organization or any one newsroom can solve.

736
00:44:38.340 --> 00:44:43.020
He said it'll probably end up coming down to the 14 and 15 year olds of tomorrow

737
00:44:43.200 --> 00:44:46.950
who will grow up using this technology, making fake videos,

738
00:44:46.951 --> 00:44:51.951
being the victims of fake videos and that maybe in the maze of them having to

739
00:44:53.131 --> 00:44:56.160
parse truth from fiction in such a personal way.

740
00:44:56.700 --> 00:45:01.260
Some kind of code will develop. I'm an optimist by nature. I do.

741
00:45:01.320 --> 00:45:03.990
I look at this and I say, well, somebody's going to figure it out.

742
00:45:04.110 --> 00:45:09.110
What worries me is the larger context within this state within which this takes

743
00:45:09.181 --> 00:45:14.130
place a way. This is all occurring within a context of massive news.

744
00:45:14.131 --> 00:45:16.410
Illiteracy and the,

745
00:45:16.470 --> 00:45:21.470
the consumers seem to be just throwing their hands up and tiring of trying to

746
00:45:21.961 --> 00:45:25.610
even figure it out. And so just the, the, the,

747
00:45:25.930 --> 00:45:30.930
the work involved in getting to the bottom of the truth is unappealing to a

748
00:45:32.941 --> 00:45:36.150
growing percentage of the audience.

749
00:45:36.180 --> 00:45:41.180
And I'm not sure where Gen z the teenagers of today come out on this.

750
00:45:42.840 --> 00:45:46.380
Let's hope that they are more willing to do the work.

751
00:45:46.590 --> 00:45:51.280
Maybe out of self interest may be so that they're not dissed by, you know,

752
00:45:51.390 --> 00:45:56.280
the girl in social studies. But that's our best hope for overcoming it.

753
00:45:56.830 --> 00:45:57.050
<v 5>Okay.</v>

754
00:45:57.050 --> 00:46:00.080
<v 9>Because everybody else seems to be sick of trying</v>

755
00:46:08.420 --> 00:46:09.253
<v 5>[inaudible]</v>

756
00:46:15.730 --> 00:46:16.490
uh,

757
00:46:16.490 --> 00:46:20.680
<v 18>reporter Simon Adler, this piece was produced by Simon and Andy McCune.</v>

758
00:46:32.240 --> 00:46:36.110
Very special thanks to Kyle Olszewski and the entire team at

759
00:46:38.420 --> 00:46:39.980
USC Institute for Creative Technology for all their work,

760
00:46:40.160 --> 00:46:43.010
manipulating that video of President Obama.

761
00:46:43.550 --> 00:46:45.980
And thanks to Matthew Aylett for [inaudible]

762
00:46:46.110 --> 00:46:48.450
<v 11>the sizing. So, so many words for us.</v>

763
00:46:49.470 --> 00:46:51.570
Rachel Axler for writing us the jokes that we tried to use.

764
00:46:52.290 --> 00:46:56.130
So whom power for building? It's an amazing website. Angus, Neil, Amy, pearl,

765
00:46:56.160 --> 00:47:01.160
everybody in the WWE NYC newsroom for advising us and giving us reaction shots

766
00:47:01.741 --> 00:47:05.040
to the face to face video and to David Carroll for putting us in touch with Nick

767
00:47:05.040 --> 00:47:05.880
Bilton in the first place.

768
00:47:06.060 --> 00:47:09.540
And to Nick Dalton for inspiring this whole story with his article,

769
00:47:09.870 --> 00:47:10.703
he's got a new one,

770
00:47:10.870 --> 00:47:15.240
a book actually American kingpin about the founder of a black market website

771
00:47:15.241 --> 00:47:19.230
called the Silk Road and to superstar in switching the coin computer scientist

772
00:47:19.231 --> 00:47:22.830
who works in eras lab, who helped us understand what the heck was going on.

773
00:47:23.040 --> 00:47:24.000
And finally,

774
00:47:24.001 --> 00:47:28.200
you can see the video that we created as well as a bunch of other kind of a

775
00:47:28.530 --> 00:47:30.970
crazy that we mentioned throughout this episode.

776
00:47:30.971 --> 00:47:34.240
It's at future of fake news.com all one word,

777
00:47:34.540 --> 00:47:39.250
future of fake news.com and uh, with that,

778
00:47:40.390 --> 00:47:45.000
my real cohosts and I will bet you a Jew. I'm jet Booman.

779
00:47:45.160 --> 00:47:48.340
I'm Robert Krulwich, who we really are.

780
00:47:49.750 --> 00:47:52.840
I'm glad we could finally be honest about that all these years.

