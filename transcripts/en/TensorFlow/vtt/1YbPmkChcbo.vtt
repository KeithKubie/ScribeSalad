WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.409
[MUSIC PLAYING]

00:00:07.792 --> 00:00:10.770
KRZYSZTOF OSTROWSKI: All
right, TensorFlow Federated--

00:00:10.770 --> 00:00:14.070
what's exciting about TFF
is that it enables everyone

00:00:14.070 --> 00:00:18.830
to experiment with computations
on decentralized data.

00:00:18.830 --> 00:00:20.370
And decentralized
data is exciting,

00:00:20.370 --> 00:00:22.080
because it's everywhere.

00:00:22.080 --> 00:00:23.700
It's in intelligent
home devices,

00:00:23.700 --> 00:00:26.243
in sensor networks, in
distributed medical databases.

00:00:26.243 --> 00:00:27.660
And of course,
there's a ton of it

00:00:27.660 --> 00:00:29.820
on personal devices
like cell phones.

00:00:29.820 --> 00:00:31.140
And we love our cell phones.

00:00:31.140 --> 00:00:32.770
We want them to be intelligent.

00:00:32.770 --> 00:00:34.610
This data could help.

00:00:34.610 --> 00:00:36.760
Traditionally, the way
we implement intelligence

00:00:36.760 --> 00:00:37.830
is on the server.

00:00:37.830 --> 00:00:39.830
So here we have a
model on the server.

00:00:39.830 --> 00:00:42.300
The clients all talk to the
server to make predictions.

00:00:42.300 --> 00:00:44.730
So all the data accumulates
on the server as well.

00:00:44.730 --> 00:00:49.170
So the model, the data, it's
all in one place-- super easy.

00:00:49.170 --> 00:00:51.440
The downside of this is
that all this back and forth

00:00:51.440 --> 00:00:55.130
communication can
hurt user experience

00:00:55.130 --> 00:00:59.120
due to network latency, lack of
connectivity, shortened battery

00:00:59.120 --> 00:00:59.640
life.

00:00:59.640 --> 00:01:01.527
And of course,
there's a ton of data

00:01:01.527 --> 00:01:03.860
that would be really useful
in implementing intelligence

00:01:03.860 --> 00:01:07.270
but that, for various reasons,
you may choose not to collect.

00:01:07.270 --> 00:01:09.280
So what can we do?

00:01:09.280 --> 00:01:12.420
Well, one idea is take all
the TensorFlow machinery

00:01:12.420 --> 00:01:14.110
and put it on-device.

00:01:14.110 --> 00:01:16.560
So here we have each
client independently

00:01:16.560 --> 00:01:20.040
training its own model using
only its own local data.

00:01:20.040 --> 00:01:23.820
No communication
necessarily-- great.

00:01:23.820 --> 00:01:25.083
Well, maybe not so great--

00:01:25.083 --> 00:01:27.000
actually, we realize
that, very often, there's

00:01:27.000 --> 00:01:29.640
just not enough data on
each individual device

00:01:29.640 --> 00:01:31.620
to learn a good model.

00:01:31.620 --> 00:01:33.120
And unlike before,
even though there

00:01:33.120 --> 00:01:35.250
might be millions
of clients, you

00:01:35.250 --> 00:01:38.350
can't benefit from the data.

00:01:38.350 --> 00:01:41.050
We can mitigate this by
pre-training the model

00:01:41.050 --> 00:01:43.860
on the server on
some proxy data.

00:01:43.860 --> 00:01:45.400
But just think of
a smart keyboard.

00:01:45.400 --> 00:01:47.620
If, today, everyone
starts using a new word,

00:01:47.620 --> 00:01:51.023
then a smart model trained
on yesterday's data

00:01:51.023 --> 00:01:52.190
won't be able to pick it up.

00:01:52.190 --> 00:01:55.280
So this technique
has limitations.

00:01:55.280 --> 00:01:56.710
OK, so now what?

00:01:56.710 --> 00:01:58.010
Do we just give up?

00:01:58.010 --> 00:02:00.310
We have to choose between
more intelligence versus more

00:02:00.310 --> 00:02:01.300
privacy?

00:02:01.300 --> 00:02:03.640
Or can we have both?

00:02:03.640 --> 00:02:06.010
Until a few years ago, we
didn't know the answer.

00:02:06.010 --> 00:02:09.380
It turns out the
answer is yes, we can.

00:02:09.380 --> 00:02:10.910
In fact, it's very simple.

00:02:10.910 --> 00:02:14.110
It goes like this-- you start
with the model on the server.

00:02:14.110 --> 00:02:16.510
You distribute it to
some of the clients.

00:02:16.510 --> 00:02:18.370
Now each line trains
the model locally using

00:02:18.370 --> 00:02:20.090
its own local data--

00:02:20.090 --> 00:02:22.340
and that doesn't have to
mean training to convergence.

00:02:22.340 --> 00:02:24.220
It could be just
training a little bit--

00:02:24.220 --> 00:02:25.905
produces a new model,
locally trained,

00:02:25.905 --> 00:02:27.030
and sends it to the server.

00:02:27.030 --> 00:02:30.940
And in practice, we would
send updates and not models,

00:02:30.940 --> 00:02:33.190
but that's an
implementation detail.

00:02:33.190 --> 00:02:37.150
All right, so now server
gets locally trained models

00:02:37.150 --> 00:02:38.510
from all the clients.

00:02:38.510 --> 00:02:41.160
And now is the crazy part.

00:02:41.160 --> 00:02:43.690
We just average them out--

00:02:43.690 --> 00:02:46.650
so simple.

00:02:46.650 --> 00:02:49.830
So OK, the average
model, trivially, it

00:02:49.830 --> 00:02:52.350
reflects the training
from every client, right?

00:02:52.350 --> 00:02:53.108
So it's good.

00:02:53.108 --> 00:02:55.650
But how do we know it's a good
model, that this procedures is

00:02:55.650 --> 00:02:56.990
doing something meaningful?

00:02:56.990 --> 00:02:58.698
In fact, you would
think it's too simple.

00:02:58.698 --> 00:03:02.220
There's just no way-- no
way-- this can possibly work.

00:03:02.220 --> 00:03:03.340
And you would be correct.

00:03:03.340 --> 00:03:04.590
It's not enough to do it once.

00:03:04.590 --> 00:03:05.423
You have to earn it.

00:03:05.423 --> 00:03:06.657
So we repeat the process.

00:03:06.657 --> 00:03:08.490
The combined model
becomes the initial model

00:03:08.490 --> 00:03:09.630
for the next round.

00:03:09.630 --> 00:03:11.460
And so it goes in rounds.

00:03:11.460 --> 00:03:14.010
In every round,
the combined model

00:03:14.010 --> 00:03:16.050
gets a little bit better
thanks to the data

00:03:16.050 --> 00:03:17.500
from all the clients.

00:03:17.500 --> 00:03:21.860
And now hundreds or thousands,
many, many rounds later,

00:03:21.860 --> 00:03:25.568
your smart keyboard begins to
show signs of intelligence.

00:03:25.568 --> 00:03:26.610
So this is quite amazing.

00:03:26.610 --> 00:03:29.280
It's mind-boggling that
something this incredibly

00:03:29.280 --> 00:03:31.020
simple can actually
work in practice.

00:03:31.020 --> 00:03:33.620
And yet it does.

00:03:33.620 --> 00:03:35.150
And then it gets
even more crazy.

00:03:35.150 --> 00:03:37.970
You can do things like
compress the update

00:03:37.970 --> 00:03:39.650
from each client
down to one bit,

00:03:39.650 --> 00:03:42.860
or add some random noise to
it to implement differential

00:03:42.860 --> 00:03:43.760
privacy.

00:03:43.760 --> 00:03:45.020
Many extensions are possible.

00:03:45.020 --> 00:03:46.955
And it still works.

00:03:46.955 --> 00:03:49.080
And you can apply it to
other things than learning.

00:03:49.080 --> 00:03:51.163
For example, you can use
it to compute a statistic

00:03:51.163 --> 00:03:54.370
over sensitive data.

00:03:54.370 --> 00:03:56.363
So experimenting with
all the different things

00:03:56.363 --> 00:03:57.780
you can do with
federated learning

00:03:57.780 --> 00:03:59.100
is actually a lot of fun.

00:03:59.100 --> 00:04:02.490
And TFF is here basically
just so that everyone

00:04:02.490 --> 00:04:05.300
can have fun doing it.

00:04:05.300 --> 00:04:06.870
It is open source.

00:04:06.870 --> 00:04:09.620
It's inspired by our experiences
with federal learning

00:04:09.620 --> 00:04:12.380
at Google, but now generalized
to non-learning use cases

00:04:12.380 --> 00:04:14.710
as well.

00:04:14.710 --> 00:04:16.690
We're doing it in the
open, in the public.

00:04:16.690 --> 00:04:17.410
It's on GitHub.

00:04:17.410 --> 00:04:18.500
We just recently started.

00:04:18.500 --> 00:04:20.740
So now is actually a great
time to jump in and contribute,

00:04:20.740 --> 00:04:22.657
because you can have
influence on the way this

00:04:22.657 --> 00:04:26.150
goes from the early stages.

00:04:26.150 --> 00:04:28.360
We want to create an
ecosystem, so TFF is all

00:04:28.360 --> 00:04:30.710
about composability.

00:04:30.710 --> 00:04:32.168
If you're building
a new extension,

00:04:32.168 --> 00:04:34.793
you should be able to combine it
with all of the existing ones.

00:04:34.793 --> 00:04:36.970
If you're interfacing a new
platform for deployment,

00:04:36.970 --> 00:04:39.630
you should be able to deploy
all of the existing code to it.

00:04:39.630 --> 00:04:41.380
So we've made a number
of design decisions

00:04:41.380 --> 00:04:45.970
to really promote composability.

00:04:45.970 --> 00:04:48.350
And speaking of
deployments, in order

00:04:48.350 --> 00:04:50.530
to enable flexibility
in this regard,

00:04:50.530 --> 00:04:54.112
TFF compiles all your code into
an abstract representation,

00:04:54.112 --> 00:04:55.820
which, today, you can
run in a simulator,

00:04:55.820 --> 00:04:57.620
but that, in the future,
could potentially

00:04:57.620 --> 00:04:59.210
run on real devices--

00:04:59.210 --> 00:05:00.470
no promises here.

00:05:00.470 --> 00:05:02.928
In the first release, we only
provide a simulation runtime.

00:05:04.940 --> 00:05:09.180
I mentioned that TFF was all
about having fun experimenting.

00:05:09.180 --> 00:05:11.230
In our past work on
federated learning--

00:05:11.230 --> 00:05:13.590
and that's before TFF was born--

00:05:13.590 --> 00:05:17.190
we've discovered certain
things that consistently

00:05:17.190 --> 00:05:18.570
get in the way of having fun.

00:05:18.570 --> 00:05:22.050
And the worst offender was
really all the different types

00:05:22.050 --> 00:05:23.580
of logic getting interleaved.

00:05:23.580 --> 00:05:26.035
So it's model logic,
communication, checkpointing,

00:05:26.035 --> 00:05:26.910
differential privacy.

00:05:26.910 --> 00:05:29.530
All this stuff gets mixed up,
and it gets very confusing.

00:05:29.530 --> 00:05:34.530
So in order to avoid this, to
preserve the joy of creation

00:05:34.530 --> 00:05:37.530
for you, we've designed
programming abstractions

00:05:37.530 --> 00:05:40.170
that will allow you to write
your federated learning

00:05:40.170 --> 00:05:44.250
code at a similar level as
when you write pseudocode

00:05:44.250 --> 00:05:46.320
or draw in a whiteboard.

00:05:46.320 --> 00:05:48.660
You'll see an example of
this later in the talk.

00:05:48.660 --> 00:05:51.370
And I hope that it
will work for you.

00:05:51.370 --> 00:05:52.590
OK, so what's in the box?

00:05:52.590 --> 00:05:54.000
You get two sets of interfaces.

00:05:54.000 --> 00:05:55.810
The upper layer
allows you to create

00:05:55.810 --> 00:05:58.230
a system that can perform
federated training

00:05:58.230 --> 00:06:01.570
or evaluation using
your existing model.

00:06:01.570 --> 00:06:05.230
And this sits on top of a
layer of lower-level modularity

00:06:05.230 --> 00:06:07.780
abstractions that
allow you to express

00:06:07.780 --> 00:06:11.850
and simulate custom
types of computations.

00:06:11.850 --> 00:06:13.600
And this layered
architecture is designed

00:06:13.600 --> 00:06:17.360
to enable a clean
separation of concerns

00:06:17.360 --> 00:06:20.200
so that developers who
specialize in different areas,

00:06:20.200 --> 00:06:21.820
whether that be
federated learning,

00:06:21.820 --> 00:06:25.358
machine learning, compiler
theory, or systems integration,

00:06:25.358 --> 00:06:27.400
can all independently
contribute without stepping

00:06:27.400 --> 00:06:30.260
on each other's toes.

00:06:30.260 --> 00:06:33.660
OK, federated learning, we've
talked about this as an idea.

00:06:33.660 --> 00:06:36.230
Now let's look at the code.

00:06:36.230 --> 00:06:40.100
We provide interfaces to
represent federated data sets

00:06:40.100 --> 00:06:43.325
for simulations and a couple
of data sets for experiments.

00:06:43.325 --> 00:06:45.700
If you have a Keras model,
you can wrap it like this with

00:06:45.700 --> 00:06:47.440
a one-liner for use with TFF--

00:06:47.440 --> 00:06:48.955
very easy.

00:06:48.955 --> 00:06:50.830
And now we can use one
of the build functions

00:06:50.830 --> 00:06:53.290
we provide to
construct various kinds

00:06:53.290 --> 00:06:54.820
of federated computations.

00:06:54.820 --> 00:06:56.920
And those are essentially
abstract representations

00:06:56.920 --> 00:07:00.400
of systems that can perform
various federated tasks.

00:07:00.400 --> 00:07:03.220
And I'll explain what
that means in a minute.

00:07:03.220 --> 00:07:06.280
Training, for instance,
is represented

00:07:06.280 --> 00:07:08.920
as a pair of computations,
one of them that constructs

00:07:08.920 --> 00:07:11.410
the initial state of a
federated training system,

00:07:11.410 --> 00:07:14.140
and the other one that
executes a single round

00:07:14.140 --> 00:07:16.198
of federated averaging.

00:07:16.198 --> 00:07:17.740
And those are still
kind of abstract.

00:07:17.740 --> 00:07:21.060
But you can also invoke them
just like functions in Python.

00:07:21.060 --> 00:07:23.200
And when you do,
they, by default,

00:07:23.200 --> 00:07:25.000
execute in a local
simulation runtime.

00:07:25.000 --> 00:07:28.240
So this is actually how you can
write little experiment loops.

00:07:28.240 --> 00:07:30.490
You can do things like pick
a different set of clients

00:07:30.490 --> 00:07:33.200
in each round and so on.

00:07:33.200 --> 00:07:37.210
The state of the system includes
the model of the training.

00:07:37.210 --> 00:07:39.350
So this is how you
can very easily

00:07:39.350 --> 00:07:43.220
simulate federated
evaluation of your model.

00:07:43.220 --> 00:07:45.530
All of this sits on
top of FC API, which

00:07:45.530 --> 00:07:47.750
is basically a language for
constructing distributed

00:07:47.750 --> 00:07:49.190
systems.

00:07:49.190 --> 00:07:50.630
It is embedded in Python.

00:07:50.630 --> 00:07:53.570
So you just write
Python code as usual.

00:07:53.570 --> 00:07:56.210
It does introduce a couple
abstract, new concepts

00:07:56.210 --> 00:07:58.220
that are worth explaining.

00:07:58.220 --> 00:08:01.640
So maybe let's take
a deep dive and look.

00:08:01.640 --> 00:08:03.620
All right, first concept--

00:08:03.620 --> 00:08:05.980
imagine you have a
group of clients again.

00:08:05.980 --> 00:08:09.010
Each of them has a
temperature sensor

00:08:09.010 --> 00:08:11.890
that generates a reading,
some floating-point number.

00:08:11.890 --> 00:08:15.280
I'm going to refer to the
collective of all these sensor

00:08:15.280 --> 00:08:18.290
readings as a federated
value, a single value.

00:08:18.290 --> 00:08:21.780
So you can think of a
federated value as a multi-set.

00:08:21.780 --> 00:08:25.400
Now in TFF, values like this
are first-class citizens,

00:08:25.400 --> 00:08:29.010
which means, among other
things, that they have types.

00:08:29.010 --> 00:08:32.789
The types of those kinds of
values consist of the identity

00:08:32.789 --> 00:08:35.429
of the group of devices
that are hosting the value--

00:08:35.429 --> 00:08:37.080
we call that the placement--

00:08:37.080 --> 00:08:39.750
and the local type, type of
the local data items that are

00:08:39.750 --> 00:08:42.651
hosted by each
member of the group.

00:08:42.651 --> 00:08:45.390
All right, now let's throw
the server into the mix.

00:08:45.390 --> 00:08:46.710
There's a number on the server.

00:08:46.710 --> 00:08:49.038
We can also give it
a federated type.

00:08:49.038 --> 00:08:50.830
In this case, I'm
dropping the curly braces

00:08:50.830 --> 00:08:52.913
to indicate that there's
actually just one number,

00:08:52.913 --> 00:08:54.170
not many.

00:08:54.170 --> 00:08:57.870
OK, now let's introduce
a distributed aggregation

00:08:57.870 --> 00:09:00.738
protocol that runs among
these system participants.

00:09:00.738 --> 00:09:03.030
So let's say it computes the
number on the server based

00:09:03.030 --> 00:09:04.488
on all the numbers
on the client's.

00:09:04.488 --> 00:09:09.170
Now in TFF, we can think
of that as a function

00:09:09.170 --> 00:09:10.920
even though the inputs
and outputs of that

00:09:10.920 --> 00:09:12.570
function reside in
different places--

00:09:12.570 --> 00:09:15.750
the inputs on the clients
and the output on the server.

00:09:15.750 --> 00:09:18.030
Indeed, we can give it a
functional-type signature

00:09:18.030 --> 00:09:20.450
that looks like this.

00:09:20.450 --> 00:09:23.650
So in TFF, you can think
of distributed systems,

00:09:23.650 --> 00:09:25.470
or components of
distributed systems,

00:09:25.470 --> 00:09:28.600
distributed protocols,
as functions, simply.

00:09:28.600 --> 00:09:30.220
We also provide
a library of what

00:09:30.220 --> 00:09:34.220
we call federated operators
that represent, abstractly,

00:09:34.220 --> 00:09:37.030
very common types of building
blocks like, in this case,

00:09:37.030 --> 00:09:38.950
computing an average
among client values

00:09:38.950 --> 00:09:41.500
and putting their
result in the server.

00:09:41.500 --> 00:09:43.540
Now with all this that
I've just described,

00:09:43.540 --> 00:09:47.758
you can actually draw system
diagrams in code, so to speak.

00:09:47.758 --> 00:09:50.050
It goes like this-- you
declare the federated type that

00:09:50.050 --> 00:09:53.860
represents the inputs to
your distributed system.

00:09:53.860 --> 00:10:00.057
Now you pass it as an argument
to a special function decorator

00:10:00.057 --> 00:10:02.140
to indicate that, in a
system you're constructing,

00:10:02.140 --> 00:10:03.820
this is going to be the input.

00:10:03.820 --> 00:10:05.860
Now in the body of the
decorated function,

00:10:05.860 --> 00:10:08.810
you invoke all the
different valid operators

00:10:08.810 --> 00:10:14.050
to essentially populate your
data flow diagram like this.

00:10:14.050 --> 00:10:16.180
It works conceptually in
very much the same way

00:10:16.180 --> 00:10:20.430
as when you construct
non-eager TensorFlow graphs.

00:10:20.430 --> 00:10:23.510
OK, now let's look at something
more complex and more exciting.

00:10:23.510 --> 00:10:25.180
So again, we have
a group of clients.

00:10:25.180 --> 00:10:27.220
They have temperature sensors.

00:10:27.220 --> 00:10:31.390
Suppose you want to compute
what fraction of your clients

00:10:31.390 --> 00:10:34.010
have temperatures
exceeding some threshold.

00:10:34.010 --> 00:10:37.690
So in this system, in this
computation I'm constructing,

00:10:37.690 --> 00:10:38.770
there are two inputs.

00:10:38.770 --> 00:10:41.780
One is the temperature
readings in the clients.

00:10:41.780 --> 00:10:43.810
The other output is the
threshold in the server.

00:10:43.810 --> 00:10:45.852
And again, the inputs can
be in different places,

00:10:45.852 --> 00:10:46.762
and that's OK.

00:10:46.762 --> 00:10:48.330
All right, how do
I execute this?

00:10:48.330 --> 00:10:52.498
First, we probably want to
just broadcast the thresholds

00:10:52.498 --> 00:10:53.290
to all the clients.

00:10:53.290 --> 00:10:56.760
So that's our first
federated operator in action.

00:10:56.760 --> 00:10:58.890
Now that each client
has both the threshold

00:10:58.890 --> 00:11:00.787
and its own local
temperature reading,

00:11:00.787 --> 00:11:02.370
you can run a little
bit of TensorFlow

00:11:02.370 --> 00:11:07.110
to compute 1 if it's over
the threshold, 0 otherwise.

00:11:07.110 --> 00:11:08.980
OK, you can think of
this as basically a map

00:11:08.980 --> 00:11:10.680
step in MapReduce.

00:11:10.680 --> 00:11:13.050
And the result of that
is a federated float, yet

00:11:13.050 --> 00:11:14.142
another one.

00:11:14.142 --> 00:11:15.850
OK, now we have all
these ones and zeros.

00:11:15.850 --> 00:11:17.642
Actually, the only
thing that remains to do

00:11:17.642 --> 00:11:20.700
is to perform a
distributed aggregation

00:11:20.700 --> 00:11:22.720
to compute the average
of those ones and zeros

00:11:22.720 --> 00:11:24.200
and place the result
in the server.

00:11:24.200 --> 00:11:27.047
OK, that's the third federated
operator in our system.

00:11:27.047 --> 00:11:27.630
And that's it.

00:11:27.630 --> 00:11:29.980
That's a complete example.

00:11:29.980 --> 00:11:34.730
Now let's look at how this
example works in the code.

00:11:34.730 --> 00:11:37.880
Again, you declare the
federated types of your inputs.

00:11:37.880 --> 00:11:41.450
You pass them as inputs to the
other arguments to the function

00:11:41.450 --> 00:11:42.570
decorator.

00:11:42.570 --> 00:11:45.350
And now, in the body of
the decorated function,

00:11:45.350 --> 00:11:47.540
you simply invoke all
the federated operators

00:11:47.540 --> 00:11:50.720
you need in the proper sequence
so the broadcast, the map,

00:11:50.720 --> 00:11:52.855
and the average are all there.

00:11:52.855 --> 00:11:54.230
And that piece of
TensorFlow that

00:11:54.230 --> 00:11:58.010
was a parameter to
the mapping operator

00:11:58.010 --> 00:12:03.020
is expressed using ordinary
TensorFlow ops just as normal.

00:12:03.020 --> 00:12:04.430
And this is the
complete example.

00:12:04.430 --> 00:12:07.400
It's working code that you
can copy-paste into a code lab

00:12:07.400 --> 00:12:08.220
and try it out.

00:12:08.220 --> 00:12:11.060
OK, so this example
obviously has nothing

00:12:11.060 --> 00:12:12.360
to do with federated learning.

00:12:12.360 --> 00:12:16.450
However, in tutorials
on our website,

00:12:16.450 --> 00:12:21.190
you can find examples of fully
implemented federated training

00:12:21.190 --> 00:12:23.380
and federated evaluation
code that look, basically,

00:12:23.380 --> 00:12:24.550
just like this.

00:12:24.550 --> 00:12:27.610
They also [INAUDIBLE]
some variable renaming.

00:12:27.610 --> 00:12:30.600
So they also fit on one screen.

00:12:30.600 --> 00:12:35.870
So yeah, in TFF, you can express
your federated learning logic

00:12:35.870 --> 00:12:39.020
very concisely in a way
that you can just look at it

00:12:39.020 --> 00:12:40.880
and understand what it does.

00:12:40.880 --> 00:12:45.000
And it's actually
really easy to modify.

00:12:45.000 --> 00:12:47.930
Yeah, and I personally--

00:12:47.930 --> 00:12:50.690
I feel it's liberating
to be able to express

00:12:50.690 --> 00:12:53.720
my ideas at this level without
getting bogged down in all

00:12:53.720 --> 00:12:56.480
the unnecessary detail.

00:12:56.480 --> 00:12:58.820
And this empowers
me to try and create

00:12:58.820 --> 00:13:00.590
an experiment with new things.

00:13:00.590 --> 00:13:03.290
And I hope that you will
check it out, and try it,

00:13:03.290 --> 00:13:05.930
and that you'll feel the same.

00:13:05.930 --> 00:13:07.150
And that's all I have.

00:13:07.150 --> 00:13:09.442
Everything you've
seen is on GitHub.

00:13:09.442 --> 00:13:10.900
As I mentioned,
there are many ways

00:13:10.900 --> 00:13:14.200
to contribute depending on
what your interests are.

00:13:14.200 --> 00:13:15.160
Thank you very much.

00:13:15.160 --> 00:13:18.510
[MUSIC PLAYING]

