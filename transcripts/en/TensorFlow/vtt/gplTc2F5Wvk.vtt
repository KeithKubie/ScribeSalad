WEBVTT
Kind: captions
Language: en

00:05:39.685 --> 00:05:41.685
Test.

00:11:18.451 --> 00:11:21.087
&gt;&gt; Hi, everybody.

00:11:31.270 --> 00:11:33.270
We have a big show for you 
today.

00:11:46.970 --> 00:11:48.970
So, if you have -- now would be 
a 

00:11:51.882 --> 00:11:53.882
great time to turn -- the exit 
behind you.

00:12:35.371 --> 00:12:37.371
[ Applause ]

00:13:48.989 --> 00:13:50.989
&gt;&gt; Hi, everybody.

00:13:52.446 --> 00:13:54.446
Welcome.

00:14:08.318 --> 00:14:10.318
Welcome to the 2018 TensorFlow 
Lite summit.

00:14:11.973 --> 00:14:13.973
We have a good day with lots of 
cool talks.

00:14:17.698 --> 00:14:19.698
As you know -- we are embarking 
on the 

00:14:20.704 --> 00:14:22.704
--

00:15:18.318 --> 00:15:20.318
and the controllers in Europe 
are 

00:15:21.333 --> 00:15:23.333
useing

00:15:27.685 --> 00:15:29.685
to project the trajectory of 
flight 

00:15:32.019 --> 00:15:34.850
through the air of Belgium, 
Luxembourg, Germany and the 

00:15:34.851 --> 00:15:37.701
Netherlands.  This has more than
1.

00:15:43.193 --> 00:15:45.193
8 million flights and it is one 
of the 

00:15:48.485 --> 00:15:50.485
most dense air spaces in the 
world.

00:15:54.383 --> 00:15:56.383
And teary farming.

00:15:59.293 --> 00:16:01.293
We know that a cow's health is 
vital 

00:16:03.768 --> 00:16:05.768
to the survival of the dairy 
industry.

00:16:07.816 --> 00:16:10.476
And -- connected our company in 
the Netherlands, they wondered 

00:16:10.477 --> 00:16:12.477
if they can 

00:16:14.739 --> 00:16:16.739
use machine learning to track 
the health 

00:16:22.773 --> 00:16:25.433
of cows and be able to provide 
insights to farmers and 

00:16:25.434 --> 00:16:27.434
veterinarians on actions 

00:16:30.320 --> 00:16:32.320
to be taken to ensure we have 
happy, 

00:16:35.197 --> 00:16:37.197
healthy cows that are high 
yielding.

00:16:49.014 --> 00:16:51.014
In California, and also from the
Netherlands.

00:16:54.058 --> 00:16:56.058
And

00:17:06.720 --> 00:17:08.720
--

00:17:23.252 --> 00:17:25.252
music, the machine learning 
algorithm, 

00:17:30.959 --> 00:17:32.959
the neural networks -- 

00:17:37.814 --> 00:17:40.445
&gt;&gt; And changed by machine 
learning.

00:17:43.503 --> 00:17:45.736
The popular Google home, or the 
pixel or search or YouTube or 

00:17:45.737 --> 00:17:49.992
even maps.  Do you know what is 
fascinateing in all of these 

00:17:49.993 --> 00:17:51.993
examples?

00:17:54.282 --> 00:17:55.729
TensorFlow is at the forefront 
of them.  Makeing it all 

00:17:55.730 --> 00:17:57.730
possible.

00:17:58.977 --> 00:18:01.635
A machine learning platform that
can solve challengeing problems 

00:18:01.636 --> 00:18:03.636
for all of us.

00:18:05.092 --> 00:18:07.544
Join us on this incredible 
journey to make TensorFlow 

00:18:07.545 --> 00:18:10.006
powerful, scaleable and the best
machine learning platform for 

00:18:10.207 --> 00:18:12.207
everybody.

00:18:17.789 --> 00:18:20.626
I now -- with TensorFlow to tell
us more about this.  Thank you.

00:18:28.167 --> 00:18:32.255
&gt;&gt; So, let's take a look at what
we have been doing over the last

00:18:32.256 --> 00:18:34.256
few years.  It's been really 
amazing.

00:18:37.130 --> 00:18:39.972
There's lots of new -- we have 
seen the popularity of 

00:18:39.973 --> 00:18:41.616
TensorFlow Lite grow.  
Especially over the last year, 

00:18:41.617 --> 00:18:43.882
we focused on makeing TensorFlow
easy to 

00:18:49.993 --> 00:18:51.993
use, and the degrees -- and new 

00:18:53.042 --> 00:18:55.075
programming paradigms like -- 
execution really make that 

00:18:55.076 --> 00:18:57.927
easyier.  Earlier this year, we 
hit the 

00:19:01.173 --> 00:19:04.014
milestone of 11 million 
downloads.  We are really 

00:19:04.015 --> 00:19:06.662
exciteed to see how much users 
are uses this and how much 

00:19:06.663 --> 00:19:08.663
impact it's had in the world.

00:19:12.141 --> 00:19:14.141
Here's a map showing 
self-identifyied 

00:19:16.424 --> 00:19:18.424
locations of folks on Git hub 
that 

00:19:20.892 --> 00:19:24.755
started TV -- TensorFlow.  It 
goes up and down.  In fact, 

00:19:24.756 --> 00:19:27.219
TensorFlow is useed in every 
time zone in the world.

00:19:32.722 --> 00:19:34.968
An important part of any open 
source product is the 

00:19:34.969 --> 00:19:37.608
contributeors themselves.  The 
people who make this project 

00:19:37.816 --> 00:19:41.880
successful.  I'm exciteed to see
over a thousand contributeors 

00:19:41.881 --> 00:19:43.881
from outside Google who 

00:19:44.970 --> 00:19:46.625
are makeing contributions not 
just by improveing code, but 

00:19:46.626 --> 00:19:48.873
also by helping the rest of the
committee by answering 

00:19:48.874 --> 00:19:50.874
questions, 

00:19:51.937 --> 00:19:53.937
responding to queries and so on.

00:19:55.885 --> 00:19:57.885
Our commitment to this community
is by 

00:19:58.892 --> 00:20:00.892
share

00:20:04.282 --> 00:20:06.282
-- sharing our direction in the 

00:20:07.957 --> 00:20:09.957
roadmap, have the design 
direction, and 

00:20:11.407 --> 00:20:13.451
focus on the key needs like 
TensorBoard.  We will be talking

00:20:13.452 --> 00:20:16.092
about this later this afternoon 
in detail.

00:20:19.967 --> 00:20:22.440
Today we are launching a new 
TensorFlow Lite blog.  We'll be 

00:20:22.441 --> 00:20:23.865
shareing work by the team in the
community on this blog, and we 

00:20:23.866 --> 00:20:27.536
would like to invite you to 
participate in this as well.

00:20:30.794 --> 00:20:33.454
We're also launching a new 
YouTube channel for TensorFlow 

00:20:33.455 --> 00:20:37.758
that brings together all the 
great content for TensorFlow.  

00:20:37.759 --> 00:20:40.219
Again, all of these are for the 
community to really help build 

00:20:40.220 --> 00:20:45.338
and communicate.  All day today 
we will be shareing a number of 

00:20:45.339 --> 00:20:47.339
posts on the blog and videos on 
the channel.

00:20:48.995 --> 00:20:50.851
The talks you are hear hearing 
here will be made available 

00:20:50.852 --> 00:20:54.721
there as well, along with lots 
of conversations and interviews 

00:20:54.722 --> 00:20:56.722
with the speakers.

00:20:58.802 --> 00:21:01.037
To make views and shareing 
easyier, today we are launching 

00:21:01.038 --> 00:21:03.038
TensorFlow hub.

00:21:04.290 --> 00:21:05.509
This library of components is 
easyily integrateed into your 

00:21:05.510 --> 00:21:07.756
models.  Now, again, goes back 
to really 

00:21:10.825 --> 00:21:13.703
makeing things easy for you.

00:21:43.743 --> 00:21:45.743
Library.

00:21:46.998 --> 00:21:49.250
With the focus on deep learning 
and neural networks.  It's a 

00:21:49.251 --> 00:21:51.251
rich collection of machine 
learning environments.

00:21:54.151 --> 00:21:56.151
It includes items like 
regressions and 

00:21:57.835 --> 00:21:59.267
decision trees commonly used for
many structured data 

00:21:59.268 --> 00:22:01.302
classification problems.  
There's a broad collection of 

00:22:01.303 --> 00:22:03.303
state of 

00:22:06.987 --> 00:22:08.987
the art tools for stats and

00:22:11.742 --> 00:22:13.742
Baysian analysis.

00:22:14.809 --> 00:22:16.809
You can check out the blog post 
for details.

00:22:18.668 --> 00:22:20.704
As I mentioned earlier, one of 
the big key focus points for us 

00:22:20.705 --> 00:22:24.399
is to make TensorFlow it easy to
use.  And we have been pushing 

00:22:24.400 --> 00:22:29.103
on simpler APIs, and making them
more intuitive.  The lowest 

00:22:29.104 --> 00:22:30.927
level -- our focus is to 
consolidate a lot of the APIs we

00:22:30.928 --> 00:22:33.995
have and make it easier to build
these models and train them.

00:22:38.471 --> 00:22:41.308
At the noise level the 
TensorFlow APIs are really 

00:22:41.309 --> 00:22:43.309
flexible and let users build 
anything they want to.

00:22:46.594 --> 00:22:48.594
But these same APIs are easier 
to use.

00:22:50.481 --> 00:22:52.481
TensorFlow contains a full 

00:22:54.536 --> 00:22:57.385
implementation of Keras.  You 
can offer lots of layers to 

00:22:57.386 --> 00:22:59.386
train them as well.

00:23:02.879 --> 00:23:06.951
Keras works with both executions
as well.  For distributed 

00:23:06.952 --> 00:23:08.952
execution, we provide 

00:23:10.010 --> 00:23:11.027
estimators so you can take 
models and distribute them 

00:23:11.028 --> 00:23:13.028
across machines.

00:23:14.522 --> 00:23:16.522
You could also get estimators 
from the Keras models.

00:23:19.493 --> 00:23:21.938
And finally, we provide premade 
estimators.  A library of ready 

00:23:21.939 --> 00:23:25.992
to go implementations of common 
machine learning environments.  

00:23:25.993 --> 00:23:28.644
So, let's take a look at how 
this works.  So, first, you 

00:23:28.645 --> 00:23:30.645
would often define were model.  
This is a

00:23:31.290 --> 00:23:33.528
nice and easy way to define your
model.  Shows a convolution 

00:23:33.529 --> 00:23:35.529
model here with just a few lines
here.

00:23:38.206 --> 00:23:40.206
Now, once you've  defined that, 
often 

00:23:41.686 --> 00:23:43.686
you want to do some input 
processing.

00:23:45.764 --> 00:23:47.764
We have a great idea of the data
introduced in 1.

00:23:50.257 --> 00:23:53.106
4 that makes it easy to process 
inputs and lets us do lots of 

00:23:53.107 --> 00:23:55.967
optimizations behind the scenes.
And you will see a lot more 

00:23:55.968 --> 00:24:00.848
detail on this later today as 
well.  Once you have those, the 

00:24:00.849 --> 00:24:03.105
model and the info data, now, 
you can put them 

00:24:06.777 --> 00:24:09.407
together by equating the 
data-set, computing gradients 

00:24:09.408 --> 00:24:11.408
and updating parameters 
themselves.

00:24:13.109 --> 00:24:15.109
You need a few lines to put 
these together.

00:24:19.438 --> 00:24:20.645
And you can use your debugger to
debug that and involve problems 

00:24:20.646 --> 00:24:22.646
as well.

00:24:24.122 --> 00:24:26.767
And, of course, you can do it 
even fewer lines by using the 

00:24:26.768 --> 00:24:28.768
pre-defined lines we have in 
Keras.

00:24:32.281 --> 00:24:34.111
In this case, it executes the 
model as a graph with all the 

00:24:34.112 --> 00:24:37.158
optimizations that come with it.
This is great for a single 

00:24:37.159 --> 00:24:39.159
machine or a single device.

00:24:41.236 --> 00:24:43.236
Now, often, given the high, 
heavy computation needs for

00:24:45.094 --> 00:24:47.094
deep learning or machine  
learning, we 

00:24:49.749 --> 00:24:54.049
want to use more than one 
actuator.  We have estimators.  

00:24:54.050 --> 00:24:56.050
The same datasets that you had, 
you 

00:24:57.737 --> 00:24:59.737
can build an estimator and 
really use 

00:25:00.782 --> 00:25:02.620
that to train across the cluster
or multiple devices on a single 

00:25:02.621 --> 00:25:06.321
machine.  That's great.  Why not
use a  cloud cluster?

00:25:09.785 --> 00:25:11.785
Why not use a single block box 
if you can do it faster?

00:25:15.677 --> 00:25:17.917
This is used for training ML 
models at scale.  And the focus 

00:25:17.918 --> 00:25:20.375
is to take everything you have 
been doing and build a TPU 

00:25:23.236 --> 00:25:25.236
estimator to allow you to scale 
the same model.

00:25:26.716 --> 00:25:28.716
And finally, once you have 
trained 

00:25:30.214 --> 00:25:32.214
that model, use that one line at
the 

00:25:33.232 --> 00:25:35.567
bottom for the deployment 
itself.  Your deployment is 

00:25:35.568 --> 00:25:39.022
important, you often do that in 
data centers.  But more and more

00:25:39.023 --> 00:25:43.695
we are seeing the need to deploy
this on the phones, on other 

00:25:43.696 --> 00:25:45.696
devices as well.

00:25:49.344 --> 00:25:51.430
And so, for that, we have 
TensorFlow  lithe.

00:25:55.100 --> 00:25:56.727
And we have a custom format 
that's designed for devices and 

00:25:56.728 --> 00:25:59.991
lightweight and really fast to 
get started with.  And then once

00:25:59.992 --> 00:26:02.838
you have that format,
 you can include that in your 

00:26:07.677 --> 00:26:09.677
application, integrate

00:26:10.825 --> 00:26:13.080
TensorFlow Lite with a few 
lines, and you have an 

00:26:13.081 --> 00:26:15.931
application to do predictions 
and include ML.  Whatever task 

00:26:15.932 --> 00:26:17.932
you want to perform.

00:26:19.005 --> 00:26:20.837
So, TensorFlow runs not just on 
many platforms, but in many 

00:26:20.838 --> 00:26:22.838
languages as well.

00:26:24.307 --> 00:26:27.168
Today I'm excited to add Swift 
to the mix.  And it brings a 

00:26:27.169 --> 00:26:29.169
fresh approach to machine 
learning.

00:26:31.505 --> 00:26:33.736
Don't miss the talk by Chris 
Lattner this afternoon that 

00:26:33.737 --> 00:26:35.737
covers the exciting 

00:26:37.866 --> 00:26:40.518
details of how we are doing 
this.  JavaScript is a language 

00:26:40.519 --> 00:26:42.756
that's synonymous with the web 
development community.

00:26:46.981 --> 00:26:50.993
I'm excited to announce 
TensorFlow.JS, bringing it to 

00:26:50.994 --> 00:26:53.446
the web developers.  Let's take 
a brief look at this.

00:26:56.493 --> 00:26:59.328
The same TensorFlow  
applications in JavaScript, you 

00:26:59.329 --> 00:27:01.329
can call them just as plain 
JavaScript code.

00:27:03.807 --> 00:27:06.256
And a full-fledged layer of API 
on top.  And full support for 

00:27:06.257 --> 00:27:08.257
TensorFlow and 

00:27:11.493 --> 00:27:13.804
Keras models so you can pick the
best deployment for you.

00:27:18.085 --> 00:27:20.085
And under the covers, these APIs
are actuated.

00:27:21.591 --> 00:27:23.591
And we have NodeJS support 
coming

00:27:25.045 --> 00:27:27.045
soon, which will give you the 
power to 

00:27:28.947 --> 00:27:30.947
actuate on CPUs and GPUs.

00:27:32.817 --> 00:27:34.817
And I would like to welcome 
Megan 

00:27:37.113 --> 00:27:40.585
Kacholia to talk about how 
TensorFlow does performance.

00:27:41.183 --> 00:27:43.183
[ Applause ]
&gt;&gt; Thank you.  All right.

00:27:47.759 --> 00:27:49.183
Thanks, Rajat so, performance 
across all platforms is critical

00:27:49.184 --> 00:27:51.672
to TensorFlow Lite's success.  I
want to take a quick step back 

00:27:51.673 --> 00:27:55.749
and talk about some of the 
things we think about when 

00:27:55.750 --> 00:27:58.804
measuring and assessing 
TensorFlow's performance.  One 

00:27:58.805 --> 00:28:01.239
of the things we want to do is 
focus on real world data and 

00:28:01.240 --> 00:28:04.918
time to accuracy.  We want to 
have reproducible benchmarks and

00:28:04.919 --> 00:28:07.180
make sure they're realistic of 
the workloads and types of 

00:28:08.595 --> 00:28:12.294
things that users like you are 
doing on a daily basis.  Another

00:28:12.295 --> 00:28:14.750
thing, like Rajat talked about, 
is we want to make sure we have 

00:28:14.950 --> 00:28:16.950
clean APIs.

00:28:18.411 --> 00:28:21.240
And we don't want to have a fast
version and a pretty version.  

00:28:21.241 --> 00:28:23.241
The fast version is the pretty 
version.

00:28:24.527 --> 00:28:26.783
All the APIs that we talked 
about that we're talking about 

00:28:26.784 --> 00:28:31.293
through various talks, these are
the things you can use to get 

00:28:31.294 --> 00:28:33.942
the best performance out of 
TensorFlow.  You don't have to 

00:28:33.943 --> 00:28:35.943
worry about what is fast or 
pretty, use the pretty one, it 

00:28:36.621 --> 00:28:38.621
is fast.

00:28:39.684 --> 00:28:41.684
TF Data from Derek after the 
keynote.  As well as

00:28:42.326 --> 00:28:44.762
distribution strategy from Igor.
And these are great examples of 

00:28:44.763 --> 00:28:46.763
things 

00:28:48.614 --> 00:28:51.056
we have been pushing on to 
ensure good performance and good

00:28:51.057 --> 00:28:55.336
APIs.  We want good performance,
whether it's a large data center

00:28:55.337 --> 00:28:57.764
like here, or maybe you're using
something like on the image 

00:28:57.977 --> 00:28:59.977
here.

00:29:01.441 --> 00:29:03.441
A GPU or CPU box under your 
desk.

00:29:06.929 --> 00:29:08.929
Making use of a cloud platform 
or a mobile or  embedded device.

00:29:10.808 --> 00:29:14.068
We want TensorFlow to perform 
well across all of them.  Now 

00:29:14.069 --> 00:29:15.493
the numbers, because what is a 
performance talk if I don't show

00:29:15.494 --> 00:29:18.141
you slides and numbers.  First, 
look at things on the mobile 

00:29:18.558 --> 00:29:20.558
side.

00:29:21.990 --> 00:29:26.124
This is highlighting TensorFlow 
Lite performance.  There's a 

00:29:26.125 --> 00:29:28.158
talk giving a lot more detail 
how it works and the things we 

00:29:29.784 --> 00:29:31.784
were thinking of when making it 
later 

00:29:32.795 --> 00:29:34.795
today

00:29:36.097 --> 00:29:38.097
by Sarah.

00:29:42.809 --> 00:29:45.657
And weft the speed yum with  Qu 
-- and it's critical to have 

00:29:45.658 --> 00:29:49.516
strong performance regardless of
the platform, and we're really 

00:29:49.517 --> 00:29:51.974
excited to see these gains in 
mobile.  In looking past mobile,

00:29:51.975 --> 00:29:54.831
just beyond, there are a
number of companies in the 

00:29:54.832 --> 00:29:58.089
hardware space which continues 
to expand.  The contributions 

00:29:58.090 --> 00:29:59.102
that come out of the  
collaborations that we have with

00:29:59.103 --> 00:30:03.212
these companies, the 
contributions they give back to 

00:30:03.213 --> 00:30:05.044
TensorFlow and back to the 
community at large, are critical

00:30:05.045 --> 00:30:09.322
to  making sure that TensorFlow 
performs well on these specific 

00:30:09.323 --> 00:30:13.390
platforms for the users that 
each group really cares about.  

00:30:13.391 --> 00:30:15.391
One of the first ones I want to 
highlight is Intel.

00:30:20.921 --> 00:30:22.921
So, the Intel MKL-DNN library, 
open 

00:30:24.180 --> 00:30:26.180
sourced and highly optimized for
TensorFlow.

00:30:30.111 --> 00:30:32.357
We have a 3X inference speedup 
on Intel platforms, as well as 

00:30:32.358 --> 00:30:35.427
great scaling efficiency on 
training.  And this is one of 

00:30:35.428 --> 00:30:40.543
those things that highlights how
important it is to have strong 

00:30:40.544 --> 00:30:43.587
collaborations with different 
folks in the community.  And 

00:30:43.588 --> 00:30:45.624
we're excited to see things like
this to go back to all the 

00:30:45.625 --> 00:30:47.625
users.

00:30:49.499 --> 00:30:51.353
And I want to call out a new of 
the collaborations with NVIDIA 

00:30:51.354 --> 00:30:53.354
as well.

00:30:56.601 --> 00:30:59.341
And Tensor RT, an inference 
Optimizer and we have been 

00:30:59.959 --> 00:31:02.406
working on this
for a long time.  It's been 

00:31:02.407 --> 00:31:04.407
around for a little while.  But 
with the TensorFlow 1.

00:31:08.100 --> 00:31:10.100
7 release, we have native 
support built in.

00:31:11.758 --> 00:31:13.758
You can get low latency, high 
throughput.

00:31:17.680 --> 00:31:19.680
You can see an inference speedup

00:31:21.532 --> 00:31:24.781
versus native 32 with standard 
TensorFlow.  It's great to see 

00:31:24.782 --> 00:31:26.782
the collaborations and the 
contributions and the great 

00:31:27.226 --> 00:31:30.070
numbers delivered by it.  
Looking past inference and going

00:31:30.071 --> 00:31:32.071
on to some of the training  
things.

00:31:34.791 --> 00:31:36.791
So, mixed-precision training is 
important.

00:31:39.277 --> 00:31:41.722
As faster and more hardware 
comes out, if you use the 

00:31:41.723 --> 00:31:43.723
support, that's how to get the 
best out of the hardware.

00:31:47.628 --> 00:31:49.628
One of the examples is the Tesla
V100 that NVIDIA has.

00:31:52.332 --> 00:31:54.332
And we want the mixed-precision 

00:31:55.590 --> 00:31:57.590
training support get the best 
performance out of the hardware.

00:31:58.447 --> 00:32:00.447
You can see a training speedup.

00:32:02.943 --> 00:32:04.943
This is on an 8X Tesla V100 box.
You can see the performance 

00:32:08.237 --> 00:32:10.061
improvement moving to 
mixed-precision training versus 

00:32:10.062 --> 00:32:11.696
the standard TensorFlow.  
Scaling efficiency is really 

00:32:11.697 --> 00:32:14.146
important as well.  Obviously we
want to make sure

00:32:16.771 --> 00:32:19.439
TensorFlow flows well, maybe a 
single GPU.  But keep going 

00:32:19.440 --> 00:32:21.884
regardless of what you throw at 
it.  We want to make sure that, 

00:32:21.885 --> 00:32:26.779
again,  looking at examples for 
real-world data as well as 

00:32:26.780 --> 00:32:28.780
synthetic data, it's great to 

00:32:31.888 --> 00:32:34.797
benchmark on synthetic data, but
real-world needs to perform as 

00:32:34.798 --> 00:32:36.798
expected.

00:32:39.474 --> 00:32:41.474
90% with real data and 95% with 
synthetic data.

00:32:43.738 --> 00:32:45.966
This is a V100 box that has one 
1 and 8GPUs.  And you can see 

00:32:45.967 --> 00:32:47.999
the scaling here.  But this is 
something we care about, 

00:32:51.267 --> 00:32:54.126
and you're going to hear more 
about scaling efficiency with 

00:32:54.127 --> 00:32:56.127
internal APIs later today.

00:32:59.402 --> 00:33:02.248
Moving past -- moving on to 
cloud frameworks.  I want to 

00:33:02.249 --> 00:33:04.682
talk about cloud TPUs.  Cloud 
TPU was  launched in beta in 

00:33:04.901 --> 00:33:06.901
February.

00:33:07.950 --> 00:33:09.950
Just a month and a half ago.

00:33:11.835 --> 00:33:14.077
This is Google's V2 TPU.  It's 
available by Google's cloud 

00:33:15.502 --> 00:33:17.330
platform, like I mentioned.  
It's exciting to look at the 

00:33:17.331 --> 00:33:19.363
numbers here.  This picture is 
showing a device.

00:33:23.708 --> 00:33:26.160
And on a single device, you can 
get 180 teraflops of 

00:33:26.161 --> 00:33:28.195
computation.  But it's not just 
about the power, it's

00:33:30.030 --> 00:33:31.846
about what you can do on this.  
Doesn't matter if you can't run 

00:33:31.847 --> 00:33:33.847
the types of models that you 
want.

00:33:35.920 --> 00:33:37.354
I want to highlight the 
reference models that we have 

00:33:37.355 --> 00:33:40.841
open sourced that are available 
today as well as a bunch more 

00:33:40.842 --> 00:33:43.795
types of models coming soon.  
Again, just to highlight the 

00:33:43.796 --> 00:33:48.078
breadth of things you can run on
the hardware and get great 

00:33:48.079 --> 00:33:50.079
performance and great accuracy.

00:33:52.022 --> 00:33:53.447
We have an internal team 
continually making sure that 

00:33:53.448 --> 00:33:57.526
these models perform well, they 
perform fast and they are also 

00:33:57.527 --> 00:33:59.527
training to accuracy in the 
expected amount of time.

00:34:03.863 --> 00:34:06.726
It's not just about putting a 
model out and open sourcing it, 

00:34:06.727 --> 00:34:10.190
but it's making it work as the 
community expects it to work.  

00:34:10.191 --> 00:34:13.642
Again, some numbers.  What good 
is it if I don't show numbers in

00:34:13.643 --> 00:34:15.643
the performance talk?

00:34:16.686 --> 00:34:18.686
One of the numbers I want to 
call out 

00:34:22.364 --> 00:34:26.711
on this slide, the cost to 
ImageNet is under  $85.  It's 

00:34:26.712 --> 00:34:28.712
exciting to see what you can 

00:34:29.975 --> 00:34:32.010
achieve but making use of this 
platform.  And if you want more 

00:34:32.011 --> 00:34:34.011
numbers, you can 

00:34:36.500 --> 00:34:38.962
look at the DAWNBench
rentry that was submitted for 

00:34:38.963 --> 00:34:42.442
the ImageNet training.  One 
final exciting thing to call out

00:34:44.495 --> 00:34:47.976
is the available of pods, later 
this year.  So, what's a pod?  A

00:34:47.977 --> 00:34:50.010
pod is actually 64 of the 
devices, like I showed earlier, 

00:34:50.011 --> 00:34:52.499
all wired up together.  And you 
get about 11.

00:34:56.966 --> 00:34:59.409
5 petaflops of computation in a 
pod.  That is a lot of compute 

00:34:59.410 --> 00:35:03.882
power that is going to be 
available in this.  What can you

00:35:03.883 --> 00:35:08.958
do with that?  The team has been
pushing training resident 50 on 

00:35:08.959 --> 00:35:12.633
a pod to accuracy in less than 
15 minutes.  We're very excited 

00:35:12.634 --> 00:35:15.902
what can be done with this type 
of hardware.  And just the 

00:35:15.903 --> 00:35:17.734
amazing speed that you can get 
that wasn't really possible 

00:35:17.735 --> 00:35:19.735
before.

00:35:20.828 --> 00:35:23.054
So, Rajat talked about the APIs 
and just the ease of use and 

00:35:23.055 --> 00:35:27.994
things we're focusing on.  I 
have given you some numbers.  

00:35:27.995 --> 00:35:29.613
But what happens when you put it
together?  What can TensorFlow 

00:35:29.614 --> 00:35:34.201
do?  I want to invite Jeff Dean,
the  leader of the brain team, 

00:35:34.202 --> 00:35:38.108
to come up and talk a bit more 
about how TensorFlow addresses 

00:35:38.109 --> 00:35:41.774
real problems.
[ Applause ]

00:35:43.024 --> 00:35:46.884
&gt;&gt; Thanks, Megan.  So, I think 
one of the really remarkable 

00:35:46.885 --> 00:35:49.764
things about machine learning is
it's capacity to solve real 

00:35:49.765 --> 00:35:53.644
problems in the world.  And 
we've seen tremendous progress 

00:35:53.645 --> 00:35:58.345
in the last two years.  In 2008,
though, the U.S. National 

00:35:58.346 --> 00:36:00.346
academy of engineering put out 
this list of grand  engineering 

00:36:03.028 --> 00:36:04.860
challenges that they were hoping
to be solved by the end of the 

00:36:04.861 --> 00:36:06.861
21st century.  It's 14 different
challenges.

00:36:09.356 --> 00:36:11.215
And I think it's a really nice 
list of things we should be 

00:36:11.216 --> 00:36:13.216
aspiring to work on as a 
society.

00:36:15.129 --> 00:36:17.365
If we solved all these problems,
our planet would be healthier, 

00:36:17.366 --> 00:36:19.366
people would 

00:36:20.626 --> 00:36:24.284
live longer, we would be happier
and things would be better.  And

00:36:24.285 --> 00:36:26.076
I think machine learning is 
going to help us in all of 

00:36:26.077 --> 00:36:29.801
these.  Some in small ways.  
Machine learning influencing our

00:36:31.657 --> 00:36:35.736
understanding of chemical 
molecules.  Some in major ways. 

00:36:35.737 --> 00:36:38.381
I'm going to talk about two 
today.  But I think machine 

00:36:38.382 --> 00:36:40.382
learning is a key to tackling 
these areas.

00:36:45.337 --> 00:36:47.008
The two are advancing health 
informatics and engineering of

00:36:47.009 --> 00:36:51.312
tools for scientific discovery. 
Clearly machine learning is a 

00:36:51.313 --> 00:36:53.313
big 

00:36:54.366 --> 00:36:56.592
component, and TensorFlow itself
you can think of as a tool for 

00:36:56.593 --> 00:37:00.294
helping us engineer some of 
these discovers.  But one of the

00:37:00.295 --> 00:37:02.723
things that I think is really 
important is that there's a lot 

00:37:04.146 --> 00:37:05.979
more opportunity for machine 
learning than there is machine 

00:37:05.980 --> 00:37:07.980
learning expertise in the world.

00:37:09.444 --> 00:37:11.482
The way you solve a machine 
learning problem today is, you 

00:37:11.483 --> 00:37:13.483
have some data.

00:37:14.768 --> 00:37:16.768
You have some computation, maybe
GPUs 

00:37:19.480 --> 00:37:22.964
or TPUs or CPUs, and then a 
machine learning expert.  

00:37:22.965 --> 00:37:25.207
Someone who has taken a graduate
class in machine learning.  

00:37:25.208 --> 00:37:27.208
Downloaded TensorFlow and 
familiar enough to play with it.

00:37:27.835 --> 00:37:29.682
But that's a small set of people
in the world.  And then you stir

00:37:29.683 --> 00:37:32.337
all this together and you get a 
solution, hopefully.

00:37:35.588 --> 00:37:37.424
So, that's -- the unfortunate 
thing about that is there's 

00:37:37.425 --> 00:37:40.905
probably tens of thousands of 
organizations in the world today

00:37:40.906 --> 00:37:42.956
that are actually effectively  
using machine learning in 

00:37:42.957 --> 00:37:44.979
production environments and 
really making use of it

00:37:45.797 --> 00:37:47.797
to solve problems.  But there's 
probably tens of millions 

00:37:51.515 --> 00:37:53.515
of organizations in the world 
that have 

00:37:54.567 --> 00:37:55.994
data in a form that could be 
used for machine learning but 

00:37:55.995 --> 00:38:01.104
don't have the internal 
expertise and skills.  How can 

00:38:01.105 --> 00:38:03.765
we make machine learning much 
easier to use so you don't need 

00:38:03.766 --> 00:38:05.766
nearly as much expertise to 
apply it?

00:38:08.864 --> 00:38:10.493
Can we use computation to 
replace a lot of the need for 

00:38:10.494 --> 00:38:14.354
the machine learning expertise? 
We have been working on a suite 

00:38:14.355 --> 00:38:18.232
of techniques, AutoML.  And 
neural architecture search is 

00:38:18.233 --> 00:38:23.332
one example of this.  One of the
things a machine learning expert

00:38:23.333 --> 00:38:25.378
does is they sit down and decide
for a particular problem what 

00:38:25.379 --> 00:38:27.379
kind of model structure they're 
going to use for the problem.

00:38:31.114 --> 00:38:33.744
A resident 50 architecture, a 
nine-layer  CNM with these 

00:38:33.745 --> 00:38:35.745
filter sizes and so on.

00:38:38.302 --> 00:38:40.302
It turns out that you can use 
machine 

00:38:42.404 --> 00:38:42.808
learning to optimize a 
controller that proposes machine

00:38:42.809 --> 00:38:47.726
learning models.  You can have 
the controller propose machine 

00:38:47.727 --> 00:38:49.976
learning models, train it on the
problems you care about, see 

00:38:49.977 --> 00:38:54.648
which work well and which don't 
and use that feedback as a 

00:38:54.649 --> 00:38:58.329
reinforcement signal for the 
Model generating model.  You can

00:38:58.330 --> 00:39:00.330
steer it towards models 

00:39:02.206 --> 00:39:03.616
working well for particular 
problems and away from the space

00:39:03.617 --> 00:39:06.276
where they're not working well. 
If you repeat this, you can get 

00:39:08.104 --> 00:39:12.825
powerful, high-quality models.  
And they look a little weird.  

00:39:12.826 --> 00:39:14.866
So, this is not something a 
human machine learning expert 

00:39:14.867 --> 00:39:16.867
would sit down and sort of 
extract.

00:39:18.808 --> 00:39:21.246
But it has characteristics of 
things we know human machine 

00:39:21.247 --> 00:39:24.336
experts have discovered are  
helpful.  If you think of the 

00:39:24.337 --> 00:39:28.012
resident architecture, it has 
skip connections that allow you 

00:39:28.013 --> 00:39:30.013
to skip every layer.

00:39:31.092 --> 00:39:32.525
These more organic-looking 
connections are the same 

00:39:32.526 --> 00:39:34.862
fundamental idea, which is you 
want to allow input data to flow

00:39:36.150 --> 00:39:38.396
more to the output
without going through as many 

00:39:38.807 --> 00:39:40.807
computational layers.

00:39:43.086 --> 00:39:46.950
So, the interesting thing is, 
AutoML does quite well here.  

00:39:46.951 --> 00:39:48.951
Every dot here -- this is a 
graph 

00:39:50.626 --> 00:39:54.315
showing computational cost 
versus accuracy for ImageNet.  

00:39:54.316 --> 00:39:57.395
And every dot shows different 
kinds of tradeoffs.  And 

00:39:57.396 --> 00:39:59.867
generally, as you expend more 
computation, you get higher 

00:39:59.868 --> 00:40:01.868
accuracy.  But every dot here is
sort of the work 

00:40:07.670 --> 00:40:09.717
of years of effort -- a 
cumulative effort by top machine

00:40:09.718 --> 00:40:11.718
learning experts in the world.

00:40:15.107 --> 00:40:17.359
And if you run AutoML, you get 
better accuracy and better 

00:40:17.360 --> 00:40:21.227
computational tradeoffs than all
those  models.  That's true at 

00:40:21.228 --> 00:40:23.228
the high end, you care 

00:40:24.682 --> 00:40:26.682
about utmost accuracy and don't 
care about computational budget.

00:40:29.981 --> 00:40:32.209
But at the low end, you have low
weight with low computational 

00:40:32.210 --> 00:40:35.518
cost and high accuracy.  That is
exciting.  I think this is a 

00:40:35.519 --> 00:40:37.519
real opportunity to use more 
computation to solve real 

00:40:39.817 --> 00:40:41.043
machine learning problems in a 
much more automated way so that 

00:40:41.044 --> 00:40:43.890
we could solve more problems 
more quickly.  We have released 

00:40:43.891 --> 00:40:45.516
this in
collaboration with the cloud 

00:40:45.517 --> 00:40:47.957
group at Google as a -- as a 
product that 

00:40:51.410 --> 00:40:52.228
customers can use for solving 
their own computer vision 

00:40:52.229 --> 00:40:54.229
problems.

00:40:55.728 --> 00:40:57.348
And obviously the one I have has
lots of other categories of 

00:40:57.349 --> 00:41:00.610
problems.  Okay.  Advance health
informatics.

00:41:03.868 --> 00:41:05.494
So, machine learning and health 
care is going to be a really 

00:41:05.495 --> 00:41:07.495
impactful combination.  One of 
the areas that we have been 

00:41:09.176 --> 00:41:11.176
working on is a variety of 
different 

00:41:13.659 --> 00:41:15.493
medical imaging problems, 
including one problem in -- 

00:41:15.494 --> 00:41:17.494
where you're trying to 

00:41:18.558 --> 00:41:20.558
look at an image and diagnose if
that 

00:41:21.601 --> 00:41:24.464
image shows science of diabetic 
retinopathy.  400 million people

00:41:24.465 --> 00:41:28.326
are at risk of this around the 
world.  It's very treatable if 

00:41:28.327 --> 00:41:31.180
it's caught in time.  But if 
it's not, you can suffer vision 

00:41:31.379 --> 00:41:33.379
loss.

00:41:34.447 --> 00:41:36.680
So, in many parts of the world, 
there aren't enough 

00:41:36.681 --> 00:41:39.949
ophthalmologists to inspect 
these images.  And so, we have 

00:41:39.950 --> 00:41:44.839
done work, and with work that 
we've published in the very end 

00:41:44.840 --> 00:41:47.073
of 2016, we showed that we had a
model that was on par with board

00:41:48.096 --> 00:41:50.096
certified ophthalmologists.  And
since then,

00:41:50.348 --> 00:41:52.385
we have been continuing to work 
on this.  And we've changed how 

00:41:52.386 --> 00:41:54.386
we sort of label our training 
data.

00:41:56.680 --> 00:41:58.680
We've gotten retinal specialists
to 

00:42:00.214 --> 00:42:03.474
label the training data rather 
than general ophthalmologists.  

00:42:03.475 --> 00:42:05.920
And we have it on par with 
retinal specialists, a higher 

00:42:05.921 --> 00:42:09.344
standard of care.  We can bring 
this and deliver this to lots 

00:42:09.345 --> 00:42:12.218
and lots of places around the 
world.  But more interestingly, 

00:42:12.219 --> 00:42:14.253
I'm going to tell you a tale of 
scientific discovery.

00:42:17.730 --> 00:42:19.730
So, we had a new person join the
retinopathy team.

00:42:22.831 --> 00:42:24.925
And as a  warmup exercise, Lily 
who leads this work, said to 

00:42:24.926 --> 00:42:29.024
this new person, hey, why don't 
you go off and see if you can 

00:42:29.025 --> 00:42:31.070
predict age and gender from the 
images.

00:42:34.745 --> 00:42:36.745
Maybe age within a couple of 
decades, and no gender.

00:42:41.226 --> 00:42:46.131
The AUC should be .5.  They came
back, I can predict gender with 

00:42:46.132 --> 00:42:49.793
AUC of .7.  That must be wrong. 
Go off and come back later.

00:42:53.867 --> 00:42:55.701
And they came back and said my 
AUC is now .85.  That got us 

00:42:55.702 --> 00:42:57.740
thinking.  And we investigated 
what other kinds 

00:43:01.010 --> 00:43:02.023
of things we could predict from 
these retinal

00:43:02.024 --> 00:43:07.337
images.  You can predict a 
variety of things indeed caytive

00:43:07.338 --> 00:43:09.338
of cardiovascular health.

00:43:11.477 --> 00:43:14.735
Your age and gender are signs of
cardiovascular health.  Your 

00:43:14.736 --> 00:43:16.736
hemoglobin level, lots of things
like this.

00:43:18.013 --> 00:43:21.504
We have a new, non-invasive test
for cardiovascular health.  

00:43:21.505 --> 00:43:23.941
Normally, you have to draw blood
and do lab tests, but we can do 

00:43:23.942 --> 00:43:25.975
this just from an image.  Which 
is pretty cool.

00:43:29.643 --> 00:43:31.689
We're also doing a bunch of work
on predictive tasks for health 

00:43:31.690 --> 00:43:34.767
care given a patient's medical 
record, can we predict the 

00:43:34.768 --> 00:43:37.404
future?  This is something 
doctors want to do.  Understand 

00:43:37.405 --> 00:43:39.242
how your patient is going to 
progress.  And you want to be 

00:43:39.243 --> 00:43:44.158
able to answer lots of kinds of 
questions.  Will the patient be 

00:43:44.159 --> 00:43:46.159
readmitted if I release them 
now?

00:43:48.038 --> 00:43:48.842
What are the most likely 
diagnosis I should be thinking 

00:43:48.843 --> 00:43:51.692
about?  What tests for this 
patient?  Lots of questions like

00:43:51.693 --> 00:43:55.752
that.  And we have been 
collaborating with several 

00:43:55.753 --> 00:43:57.753
health care organizations to 

00:43:59.235 --> 00:44:01.235
work on identified health care 
records to predict these things.

00:44:04.343 --> 00:44:06.343
In January, we posted a many

00:44:08.611 --> 00:44:10.034
author archive paper and looked 
at these tasks.  Highlighting 

00:44:10.035 --> 00:44:12.035
one here.

00:44:13.514 --> 00:44:15.344
Predicting which patients are 
most at risk for mortality, and 

00:44:15.345 --> 00:44:17.345
using this, 

00:44:18.849 --> 00:44:20.075
we're able to predict which 
patients are most seriously at 

00:44:20.076 --> 00:44:22.733
risk 24 hours earlier than the 
clinical baselines that are 

00:44:23.334 --> 00:44:28.236
currently in use.  That means 
that doctors get 24 hours of 

00:44:28.237 --> 00:44:31.093
advanced notice to pay attention
to the patients critically ill 

00:44:31.094 --> 00:44:33.336
and need their close attention 
and close watching.

00:44:37.628 --> 00:44:39.628
This is indicative of what 
machine learning can do.

00:44:44.369 --> 00:44:47.009
The Google brain team's mission 
is to make machines intent and 

00:44:47.010 --> 00:44:49.654
approve lives.  I'm going to 
close with a bit of a story.

00:44:53.719 --> 00:44:55.147
So, when I was 5 years old, I 
lived in northwestern Uganda for

00:44:55.148 --> 00:44:57.148
a year.

00:44:58.382 --> 00:45:00.429
And the local crop there is a 
root called cassava.  And I was 

00:45:00.430 --> 00:45:02.430
5.

00:45:03.479 --> 00:45:06.343
So, I liked to go out and help 
people pick cassava.  But it 

00:45:06.344 --> 00:45:08.344
turns out that machine learning 

00:45:09.804 --> 00:45:12.682
and cassava have a kind of cool 
twist together.  Please roll the

00:45:12.683 --> 00:45:14.683
video.

00:45:20.249 --> 00:45:24.515
&gt;&gt; Cassava is a really important
crop.  It provides for over 500 

00:45:24.516 --> 00:45:28.571
million Africans every day.
&gt;&gt; When all other crops failed, 

00:45:28.572 --> 00:45:30.572
farmers 

00:45:33.266 --> 00:45:35.518
know they could rely on their 
cassava plants to provide them 

00:45:35.519 --> 00:45:38.780
food.
&gt;&gt; We have several diseases that

00:45:38.781 --> 00:45:41.823
affect  cassava, and these 
diseases make the roots 

00:45:41.824 --> 00:45:43.824
inedible.

00:45:45.892 --> 00:45:47.517
It is very crucial to actually 
control and manage these 

00:45:47.518 --> 00:45:48.946
diseases.
&gt;&gt; We are using machine learning

00:45:48.947 --> 00:45:50.947
to respond to the diseases.

00:45:53.620 --> 00:45:55.620
&gt;&gt; And TensorFlow is the best 
foundation for our solutions.

00:45:58.697 --> 00:46:00.697
The app can diagnose multiple 
diseases.

00:46:04.614 --> 00:46:07.468
It's nuru, Swahili for light.  
You wave your phone over a leaf,

00:46:07.469 --> 00:46:10.310
if it has a symptom, the box 
will pop up, you have this 

00:46:10.311 --> 00:46:14.807
problem.  When we get a 
diagnosis, we have an option for

00:46:14.808 --> 00:46:16.808
advice and learn about 
management practices.

00:46:18.678 --> 00:46:20.912
: the object we use through 
TensorFlow relies upon our team 

00:46:20.913 --> 00:46:23.755
annotating images.
&gt;&gt; We have collected over 5,000 

00:46:26.406 --> 00:46:27.423
high-quality images of different
cassava diseases throughout this

00:46:27.424 --> 00:46:29.424
project.

00:46:30.496 --> 00:46:33.748
We use a single model on the 
mobile net architecture.  It's 

00:46:33.749 --> 00:46:35.749
able to make predictions in less
than one second.

00:46:36.429 --> 00:46:39.275
&gt;&gt; Instead of implementing 
thousands of lines of code, 

00:46:39.276 --> 00:46:41.509
TensorFlow has a library of 
functions to allow us to build 

00:46:42.727 --> 00:46:44.976
architectures in much less time.
&gt;&gt; We need something that can be

00:46:46.607 --> 00:46:48.607
deployed on a phone without any 
connection.

00:46:50.654 --> 00:46:53.321
TensorFlow is able to shrink 
these neural networks.

00:46:55.147 --> 00:46:56.783
&gt;&gt; The human input is critical. 
We're building something that 

00:46:56.784 --> 00:46:59.424
augments your experience and 
makes you better at your job.

00:46:59.632 --> 00:47:04.120
&gt;&gt; So, with AI tools and machine
learning, you can improve the 

00:47:04.121 --> 00:47:07.980
yields, you can protect your 
crops, and you can have a much 

00:47:07.981 --> 00:47:09.981
more reliable source of  food.

00:47:11.838 --> 00:47:13.472
&gt;&gt; AI offers the prospect to 
fundamentally transform the life

00:47:13.473 --> 00:47:15.473
of 

00:47:17.125 --> 00:47:19.775
hundreds of millions of farms 
around the world.

00:47:22.024 --> 00:47:24.461
&gt;&gt; You can see a product that 
can actually make someone's life

00:47:24.462 --> 00:47:26.462
better.

00:47:30.116 --> 00:47:32.116
This is kind of revolutionary

00:47:33.390 --> 00:47:35.390
.

00:47:37.909 --> 00:47:39.909
&gt;&gt; Cool.  And I think we have 
some members of 

00:47:42.814 --> 00:47:45.454
the Penn State and IITA teams 
from Tanzania here today.  If 

00:47:45.455 --> 00:47:47.892
you could all stand up or wave. 
And I'm sure they would be happy

00:47:47.893 --> 00:47:51.549
to chat with you.
[ Applause ]

00:47:53.784 --> 00:47:55.416
I'm sure they would be happy to 
chat you at the break about that

00:47:55.417 --> 00:47:57.460
work.  With that, I would like 
to introduce 

00:48:00.505 --> 00:48:02.505
Derek Murray who is going to 
talk to you about tf.data.

00:48:06.193 --> 00:48:08.193
That's a way to describe an 
input line.  Derek.  Thanks.

00:48:10.066 --> 00:48:12.066
[ Applause ]
&gt;&gt; Okay.  Thank you, Jeff.

00:48:14.767 --> 00:48:16.791
And, wow, it's amazing to see 
how people are using TensorFlow 

00:48:16.792 --> 00:48:18.792
to make the world a better 
place.

00:48:21.698 --> 00:48:23.698
As Jeff said, my name is Derek 
Murray, 

00:48:25.160 --> 00:48:27.160
I'm thrilled to be here to tell 
you about tf.data.

00:48:30.657 --> 00:48:32.657
It helps you get your data from 
cat 

00:48:33.874 --> 00:48:38.039
pictures to cassava pictures 
into TensorFlow.  They're 

00:48:38.040 --> 00:48:40.703
usually overshadowed by the more
glamorous aspects of machine 

00:48:44.980 --> 00:48:47.419
learning, matrix, convolution, 
but I would argue they're 

00:48:47.420 --> 00:48:50.068
extremely important.  Input data
is the life blood of machine 

00:48:50.069 --> 00:48:52.069
learning.  And the current 
algorithms and 

00:48:55.161 --> 00:48:57.185
hardware are so thirsty for 
data, we need a powerful input 

00:48:57.186 --> 00:48:59.237
pipeline to keep up with them.  
There we go.

00:49:03.101 --> 00:49:05.540
So, when I kicked off the tf.
data project last year, 

00:49:05.541 --> 00:49:08.792
TensorFlow had room to improve. 
You could feed in data from 

00:49:08.793 --> 00:49:10.793
Python at 

00:49:13.282 --> 00:49:15.720
each step, kind of slow, or set 
up curators to feed in data.  

00:49:15.721 --> 00:49:17.721
These were challenging to use.

00:49:19.213 --> 00:49:21.264
So, I decided to focus on three 
themes, the main focus of my 

00:49:21.265 --> 00:49:23.265
talk today.  First is 
performance.

00:49:27.135 --> 00:49:29.135
When we're training models on 
state of

00:49:31.626 --> 00:49:33.626
the art accelerator hardware, we
have a 

00:49:36.315 --> 00:49:38.138
moral imperative to keep them 
busy with new data at all times.

00:49:38.139 --> 00:49:40.804
The second is flexibility.  We 
want to handle any data in 

00:49:41.210 --> 00:49:43.856
TensorFlow itself.  We don't 
want to have to rely on 

00:49:46.702 --> 00:49:48.702
different tools so you can 
experiment with different views.

00:49:53.060 --> 00:49:55.060
And third, we have ease of use.

00:49:56.718 --> 00:49:58.251
With TensorFlow, we want to open
up machine learning to users of 

00:49:58.252 --> 00:50:03.137
all abilities.  And it can be a 
big leap from following along 

00:50:03.138 --> 00:50:05.138
with your first tutorial 

00:50:06.619 --> 00:50:08.619
to training your first on your 
own data.

00:50:12.091 --> 00:50:14.091
And we can help smooth this 
transition.  Tf.

00:50:15.450 --> 00:50:16.660
data is the only library you 
need for TensorFlow.  How to do 

00:50:16.661 --> 00:50:19.310
that?  We took inspiration from 
the worlds of 

00:50:24.600 --> 00:50:26.652
data bastes and designed tf.data
as tools to divide up into 

00:50:26.653 --> 00:50:30.538
three.  First, the tools to 
extract data from a wide range 

00:50:30.539 --> 00:50:32.539
of sources.

00:50:34.232 --> 00:50:36.232
These can range from in-memory 
arrays to multi

00:50:38.111 --> 00:50:40.365
Terabyte files across a 
distributed system.  Then we 

00:50:40.366 --> 00:50:42.366
have the tools to transform your
data.

00:50:44.856 --> 00:50:47.308
These enable you to extract 
features, perform data 

00:50:47.309 --> 00:50:49.962
augmentation and ultimately 
convert your raw data into the 

00:50:49.963 --> 00:50:52.812
tensors you will use to train 
your model.

00:51:01.999 --> 00:51:04.452
And finally, loading into the 
accelerators.  That is important

00:51:04.453 --> 00:51:07.726
for performance.  That's the 
high level pitch.  What's it 

00:51:07.727 --> 00:51:09.727
look like in real today?

00:51:10.778 --> 00:51:12.778
This is the standard tf.

00:51:15.092 --> 00:51:17.092
data input pipeline for example 
protos for example files.

00:51:18.760 --> 00:51:19.978
I bet that 90% of all TensorFlow
input pipelines start out this 

00:51:19.979 --> 00:51:24.476
way.  It's so common, we have 
wrapped this pipeline up in a 

00:51:24.477 --> 00:51:26.477
single utility.

00:51:28.536 --> 00:51:31.794
For petago logical reasons, it's
important to start out there.  

00:51:31.795 --> 00:51:33.795
We get a list of  files on your 
local 

00:51:39.113 --> 00:51:42.375
disk or in GCS or S3, and 
extract the tf records.  We use 

00:51:42.376 --> 00:51:44.376
functional transformations to 
pre-process the data.

00:51:47.067 --> 00:51:49.331
We took inspiration from C 
Sharp, Java and Scala which use 

00:51:49.332 --> 00:51:54.015
method chaining to build up a 
pipeline of data set objects and

00:51:54.016 --> 00:51:56.284
a higher order of functional
operators like map and filter to

00:51:56.285 --> 00:51:59.136
help you customize the behavior 
of that pipeline.

00:52:02.196 --> 00:52:03.626
And finally, the load phase, 
tell TensorFlow how to get data 

00:52:03.627 --> 00:52:07.495
out of the set.  And one of the 
easiest ways is to create an 

00:52:07.496 --> 00:52:09.496
iterator.

00:52:10.559 --> 00:52:12.559
Just like the name sake in 
Python, 

00:52:13.569 --> 00:52:17.878
gives you sequential access.  
And we will see ways to soup up 

00:52:17.879 --> 00:52:22.546
this part of the pipeline later 
on.  I have given you an 

00:52:22.547 --> 00:52:26.615
overview, I want to come back to
the key themes and tell you how 

00:52:26.616 --> 00:52:27.635
we have been advancing each of 
them.  Let's start with 

00:52:27.636 --> 00:52:31.487
performance.  So, you remember 
all those exciting performance 

00:52:31.488 --> 00:52:35.359
results that Megan told you 
about in the keynote?  Well, 

00:52:35.360 --> 00:52:37.360
every one of them was measured 

00:52:40.439 --> 00:52:41.854
using real input data and  ad 
tf.data input pipeline you can 

00:52:41.855 --> 00:52:44.329
download and use in your 
programs.  And there's one that 

00:52:44.330 --> 00:52:46.400
I personally like, and it 
measures the performance of 

00:52:49.649 --> 00:52:51.649
training an infinitely fast 
image model 

00:52:52.692 --> 00:52:54.719
or real data in order to tease 
out any bottlenecks in the 

00:52:54.720 --> 00:52:56.720
pipeline.

00:52:58.185 --> 00:53:00.185
When we ran this last week on an

00:53:02.475 --> 00:53:04.475
NVIDIA feed, it processed over 
30,000

00:53:04.515 --> 00:53:06.965
ImageNet images per second.  
That's much faster than we can 

00:53:06.966 --> 00:53:10.829
train on current hardware, but 
it's exciting for a couple of 

00:53:10.830 --> 00:53:15.550
reasons.  And this throughput 
more than doubled other the last

00:53:15.551 --> 00:53:17.582
eight months.  And a testament 
to the great job the 

00:53:20.632 --> 00:53:22.632
team has done in optimizing 
TensorFlow performance.

00:53:24.688 --> 00:53:27.543
And the accelerators are getting
faster all the time.  And we 

00:53:27.544 --> 00:53:29.544
have this extremely useful 

00:53:32.621 --> 00:53:33.652
benchmark that guides us as we 
continue to approve tf.data 

00:53:33.653 --> 00:53:38.334
performance.  How do you get 
that kind of performance?  Well,

00:53:38.335 --> 00:53:40.335
one option is you can go on 

00:53:41.811 --> 00:53:43.811
GitHub, the TensorFlow 
benchmarks 

00:53:44.854 --> 00:53:46.272
projects, and use it in your 
program.  You should probably 

00:53:46.273 --> 00:53:49.334
just do that.  But maybe you 
have a different problem to 

00:53:49.335 --> 00:53:52.182
solve.  We have recently 
launched tf.data on TensorFlow.

00:53:54.630 --> 00:53:57.057
org, and this guide is full of 
useful, theoretical, and 

00:53:57.058 --> 00:53:59.095
practical information that gives
you the ability to put 

00:54:01.124 --> 00:54:03.970
optimizations into practice on 
your own pipelines.  And to 

00:54:03.971 --> 00:54:06.406
support this on the technical 
side, adding a raft of new 

00:54:06.407 --> 00:54:08.100
features to tf.data to achieve 
this

00:54:08.101 --> 00:54:10.101
performance.

00:54:11.755 --> 00:54:13.385
One I critically want to call 
out is this part of TensorFlow 

00:54:13.386 --> 00:54:18.888
1.8.  You can start playing with
it in the builds right away.  

00:54:18.889 --> 00:54:20.717
And up to this point, tf.data 
has been exclusively for code 

00:54:20.718 --> 00:54:24.774
that runs on the CPU.  This 
marks our first foray into 

00:54:27.006 --> 00:54:29.042
running on GPUs as well, there's
a lot more I can say on this 

00:54:29.043 --> 00:54:32.107
topic and we are developing the 
features.  But let's go back to 

00:54:32.108 --> 00:54:35.352
look again at program from 
earlier on.  Show you how to put

00:54:35.353 --> 00:54:37.353
the techniques into practice.

00:54:39.215 --> 00:54:41.848
First off, when dealing with a 
large dataset like  GCS or S3, 

00:54:41.849 --> 00:54:44.516
you can speed things up by 
reading multiple files in 

00:54:46.140 --> 00:54:49.813
parallel by increasing the level
of throughput into your model.  

00:54:49.814 --> 00:54:51.814
And you can turn this on with 
the 

00:54:54.574 --> 00:54:56.602
single record, the call, numb 
parallel -- and then you can 

00:54:56.603 --> 00:54:58.603
improve performance 

00:55:00.278 --> 00:55:02.278
by switching to fused version of
various  transformations.

00:55:04.761 --> 00:55:06.761
And I can repeat -- between the 
boxes, the buffers.

00:55:09.248 --> 00:55:11.078
And fused together the match and
batch, the execution of the 

00:55:11.079 --> 00:55:13.079
function and the map and the 
data transfer

00:55:14.942 --> 00:55:17.379
of each element into the  batch.
And together, these two 

00:55:17.380 --> 00:55:19.380
optimizations 

00:55:20.429 --> 00:55:22.429
get big speedups for models that
consume a large volume of data.

00:55:24.894 --> 00:55:26.731
And last, but not least, we have
the GPU prefetch that I 

00:55:26.732 --> 00:55:28.732
mentioned.

00:55:30.026 --> 00:55:32.670
This ensures that the next batch
of input data is in memory on 

00:55:32.671 --> 00:55:34.671
the GPU when it's ready to 
begun.

00:55:36.986 --> 00:55:38.986
There is a crucial part of the 
CNN benchmarks.

00:55:42.688 --> 00:55:44.688
But achieving it had buffers 
from CPU to GPU.

00:55:46.759 --> 00:55:47.998
And the new device API gives you
the same performance and only 

00:55:47.999 --> 00:55:49.999
requires you 

00:55:51.264 --> 00:55:53.264
to add one line of code to your 
input 

00:55:54.734 --> 00:55:56.758
line to get the benefits.  This 
is the cliffs notes version.

00:55:59.848 --> 00:56:01.677
I would encourage you to watch 
my colleague's talk later, he's 

00:56:01.678 --> 00:56:03.678
going to 

00:56:04.678 --> 00:56:06.678
show you

00:56:07.984 --> 00:56:09.984
a more specific approach.  And 
restructure check it out.

00:56:12.375 --> 00:56:14.375
Now, let's switch gears and move
on to the second.

00:56:17.453 --> 00:56:18.922
Originally the flexibility in 
tf.data stemmed from the 

00:56:18.923 --> 00:56:22.374
functional transformation.  
Allowed you to put any 

00:56:22.375 --> 00:56:27.669
TensorFlow graph, at any point 
in your pipeline.   So, for 

00:56:27.670 --> 00:56:29.670
example, if you have existing 
TensorFlow

00:56:31.138 --> 00:56:33.583
code for pre-processing images, 
you can stick it in a data flow 

00:56:33.584 --> 00:56:36.636
map and start using it right 
away.  The original version of 

00:56:36.637 --> 00:56:41.122
tf.data traded on this and let 
you pass a list of tensors in 

00:56:41.123 --> 00:56:46.237
and get a list of tensors out 
from these transformations.  We 

00:56:46.238 --> 00:56:48.238
heard back from the users, they 
had 

00:56:50.068 --> 00:56:52.068
more sophisticated things and 
complicated structures.

00:56:55.806 --> 00:56:58.243
We had in TensorFlow, added 
native support, and it's useful 

00:56:58.244 --> 00:57:01.297
for dealing with complex 
categorical data and  trading in

00:57:01.298 --> 00:57:03.298
models.

00:57:04.953 --> 00:57:05.971
So, at this point, if TensorFlow
is doing everything you want to 

00:57:05.972 --> 00:57:10.860
do, you're all set.  But one 
thing we have learned over last 

00:57:10.861 --> 00:57:13.702
few years is not everything is 
most naturally expressed in a 

00:57:13.703 --> 00:57:17.596
TensorFlow graph.  We have been 
working to give you alternative 

00:57:17.597 --> 00:57:20.616
ways to build up the tf.data 
pipelines.  The first is to add 

00:57:20.617 --> 00:57:22.617
data set.  And this allows you 
to build a 

00:57:26.719 --> 00:57:28.719
pipeline from a Python function 
that 

00:57:30.389 --> 00:57:32.878
generates -- and you can wrap 
existing Python code and benefit

00:57:32.879 --> 00:57:36.751
from the performance transitions
like prefetch and GPUs.  The 

00:57:36.752 --> 00:57:38.752
other way

00:57:40.000 --> 00:57:42.000
might be more appealing to power
users.

00:57:43.455 --> 00:57:45.455
We have openedded up a backend 
API, 

00:57:47.314 --> 00:57:49.314
and you can build the plugins in
C++.

00:57:52.603 --> 00:57:54.603
And I've heard from some of our

00:57:56.488 --> 00:57:59.165
partners, this is useful for 
custom and we're dogfooding this

00:57:59.166 --> 00:58:01.192
approach for some of the 
implementations like the 

00:58:01.193 --> 00:58:04.858
recently added Kafka data set.  
I'm looking forward to what some

00:58:04.859 --> 00:58:09.970
of you will build with this new 
API, and encourage you to 

00:58:09.971 --> 00:58:13.231
contribute back via pull 
requests.  We're excited about 

00:58:13.232 --> 00:58:15.232
the contributions from the 
community at this point in the 

00:58:15.269 --> 00:58:17.269
project.

00:58:18.744 --> 00:58:22.416
Okay, final thing I want to 
cover is ease of use.  I want to

00:58:22.417 --> 00:58:24.855
speak to folks like you who have
used TensorFlow for a year or 

00:58:24.856 --> 00:58:29.151
more, and struggled with the 
data into TensorFlow, I don't 

00:58:29.152 --> 00:58:31.216
have to make much of a case.  
But the users have high 

00:58:31.217 --> 00:58:33.712
expectations and there are 
people getting their first 

00:58:35.332 --> 00:58:37.368
exposure to every day.  We 
continue to push hard on 

00:58:37.369 --> 00:58:42.647
usability.  I want to share a 
few highlights.  First off, as 

00:58:42.648 --> 00:58:44.648
Rajat told you in the 

00:58:46.941 --> 00:58:47.144
keynote, eager execution is 
here, makes using

00:58:47.145 --> 00:58:49.571
tf.data a lot more pleasant.

00:58:52.846 --> 00:58:55.704
Alex is going to tell you more, 
but from my admittedly biased 

00:58:55.705 --> 00:58:57.705
perspective, 

00:58:58.757 --> 00:59:00.757
you can start treating data sets
just like any object.

00:59:03.638 --> 00:59:05.638
You can look over them with the 
regular fore loop and there's no

00:59:05.873 --> 00:59:07.873
iteration required.

00:59:09.189 --> 00:59:11.189
What's neat about this, it works
together with tf.

00:59:13.067 --> 00:59:15.100
data like GPU prefetch so you 
can combine the efficiency of 

00:59:15.101 --> 00:59:19.614
execution for your input 
pipeline and the flexibility of 

00:59:19.615 --> 00:59:21.615
Eager execution for your model 
code.

00:59:22.664 --> 00:59:24.664
Next, return feedback.

00:59:25.911 --> 00:59:27.911
Power users like the 
composebility and 

00:59:29.362 --> 00:59:31.362
figurability of the data API, 
many had 

00:59:33.660 --> 00:59:36.112
users just want an easy way to 
pull best practices.  TensorFlow

00:59:36.113 --> 00:59:38.113
1.

00:59:39.591 --> 00:59:41.591
8 will have new protocol buffers
and for 

00:59:42.631 --> 00:59:44.076
CSV data to make it easier to 
handle the formats and apply all

00:59:44.077 --> 00:59:46.077
the best practices from the 
performance.

00:59:47.935 --> 00:59:51.385
So, let's go for one last time 
back to the standard problem.  

00:59:51.386 --> 00:59:53.211
As I promised in the beginning, 
it can be replaced by a single 

00:59:53.212 --> 00:59:55.212
call.

00:59:57.280 --> 00:59:59.917
And this performs all the 
parallel IO, shuffling,  

00:59:59.918 --> 01:00:01.918
batching and fetching for you.  
Gives

01:00:02.769 --> 01:00:05.415
you back a data set you can 
continue to transform using map 

01:00:05.416 --> 01:00:07.853
filter and transformations.  And
if you have a workload, use a 

01:00:11.313 --> 01:00:13.313
binary format like tf.records.

01:00:15.204 --> 01:00:16.827
But those who tend to have 
smaller data, they prefer 

01:00:16.828 --> 01:00:18.828
something simple.

01:00:23.359 --> 01:00:25.359
And the CSV format fits that  
fine.

01:00:26.635 --> 01:00:28.058
There are thousands of different
CSV data  sets that are 

01:00:28.059 --> 01:00:30.923
available to download for free. 
And this snippet shows you how 

01:00:30.924 --> 01:00:32.924
to use 

01:00:35.623 --> 01:00:37.461
an API, installing them to 
download with just a couple 

01:00:37.462 --> 01:00:42.340
simple commands.  Once you have 
done that, you can use the new 

01:00:42.341 --> 01:00:44.376
data set function in TensorFlow 
to get the data out of the 

01:00:44.377 --> 01:00:46.377
loaded files.

01:00:50.305 --> 01:00:52.745
In this case, it's a data set of
a million use headlines.  What I

01:00:52.746 --> 01:00:54.746
particularly like about this 

01:00:56.195 --> 01:00:59.084
new API  is it takes care of 
figuring out the types of common

01:00:59.085 --> 01:01:01.932
names and dramatically cuts down
the boilerplate you have to run.

01:01:02.129 --> 01:01:06.805
Finally, we have been working to
improve the integration between 

01:01:06.806 --> 01:01:08.806
tf.

01:01:10.053 --> 01:01:12.700
data and high-level APIs like 
estimators and Keras.  The Keras

01:01:12.701 --> 01:01:14.940
support is still in the 
pipeline, if you'll excuse the 

01:01:14.941 --> 01:01:16.941
pun.

01:01:18.392 --> 01:01:20.638
But if we want to switch our CSV
parsing code for estimators, 

01:01:20.639 --> 01:01:25.317
it's a simple matter of 
returning the data set from an 

01:01:25.318 --> 01:01:27.318
estimator's input function.  No 
more iterator required.

01:01:30.598 --> 01:01:31.817
And pass the input function to 
the estimators train method and 

01:01:31.818 --> 01:01:33.818
we're good to go.

01:01:37.111 --> 01:01:39.751
The road we're taking, make the 
data sets and the integrators as

01:01:39.752 --> 01:01:41.752
natural as possible.

01:01:43.858 --> 01:01:46.518
Features like Eager execution 
and the high level APIs are 

01:01:46.519 --> 01:01:48.998
making this easier.  The 
eventual goal is to make it  

01:01:51.866 --> 01:01:53.096
seamless so it's a natural 
extension of your TensorFlow 

01:01:53.097 --> 01:01:55.542
program.  Well, that is about 
all the time I have.

01:01:58.808 --> 01:02:00.027
So, just to recap, I told you in
the beginning that our mission 

01:02:00.028 --> 01:02:04.502
for tf.data was to make a 
library for input processing 

01:02:04.503 --> 01:02:07.761
that was fast,  flexible, and 
easy to use.  I hope I have 

01:02:07.762 --> 01:02:10.621
convinced you in the last 15 
minutes we

01:02:11.839 --> 01:02:13.839
have achieved these three goals.
I hope you understand that tf.

01:02:16.319 --> 01:02:18.992
data is the one library all 
input processing.  If you want 

01:02:18.993 --> 01:02:23.259
to find out more, there is a ton
of documentation about tf.data 

01:02:23.260 --> 01:02:26.752
on TensorFlow.org.  Cover how 
toes and performance guidance I 

01:02:26.753 --> 01:02:28.980
mentioned earlier.  And the 
benchmarks and the official 

01:02:31.049 --> 01:02:33.922
models and repositories, 
examples of high performance and

01:02:33.923 --> 01:02:35.923
readable pipelines written in 
tf.data.

01:02:37.370 --> 01:02:39.612
And with all of this information
and knowing the creativity, I'm 

01:02:39.613 --> 01:02:42.481
really looking forward to seeing
what all of you build with this 

01:02:42.482 --> 01:02:46.601
library.  Thanks a lot for 
listening.

01:02:49.606 --> 01:02:51.606
[ Applause ]
&gt;&gt; Okay.

01:02:55.584 --> 01:02:57.835
And now it is my great pleasure 
to introduce Alex Passos who is 

01:02:57.836 --> 01:02:59.836
going to 

01:03:01.083 --> 01:03:02.910
tell you all about TensorFlow 
Eager execution.

01:03:02.911 --> 01:03:04.911
&gt;&gt; Hello.  My name is Alex.

01:03:09.406 --> 01:03:11.917
And I'm here to tell you about 
Eager execution, you have heard 

01:03:11.918 --> 01:03:13.918
the last two talks.  But I'm 
here to tell you what it's 

01:03:13.983 --> 01:03:15.983
about.

01:03:19.110 --> 01:03:20.916
This new imperative 
object-oriented way of using 

01:03:20.917 --> 01:03:23.560
TensorFlow.  We're introducing 
today as part of TensorFlow 

01:03:23.561 --> 01:03:27.625
core.  Because you're here or 
watching on the live stream, I 

01:03:27.626 --> 01:03:30.461
hope, that TensorFlow has been 
this, like, graph execution 

01:03:30.462 --> 01:03:34.642
engine for machine learning that
lets you run graphs in high 

01:03:34.643 --> 01:03:38.915
scale and all sorts of other 
nice things.  But has it?  And 

01:03:38.916 --> 01:03:42.558
why did we choose to go with 
graphs in the first place?  

01:03:42.559 --> 01:03:46.634
Since now we're -- I'm going to 
tell you about Eager Execution. 

01:03:46.635 --> 01:03:49.286
We moved beyond what we can 
achieve with graphs, it's a good

01:03:49.287 --> 01:03:51.949
idea to recap why we bothered.  
And like a really good reason 

01:03:51.950 --> 01:03:54.419
why you want to have your 
computation respected 

01:03:57.720 --> 01:03:59.720
as a platform-independent graph,
once you have that, it's easy to

01:04:00.179 --> 01:04:02.179
differentiate the graph.

01:04:03.242 --> 01:04:04.462
I went to grad school before all
of this was standard in machine 

01:04:04.463 --> 01:04:09.151
learning tool kits and I
do not wish that on anyone.  

01:04:09.152 --> 01:04:11.999
Life is much better now, trust 
me.  And if you have a 

01:04:12.000 --> 01:04:14.000
platform-independent abstract 
representation of your 

01:04:15.466 --> 01:04:16.681
computation, you can just go and
deploy it to pretty much 

01:04:16.682 --> 01:04:22.382
anything you want.  You can run 
it on the TPU, you can run it on

01:04:22.383 --> 01:04:24.446
the GPU, put it on a phone or a 
Raspberry Pi.  There are all 

01:04:24.447 --> 01:04:26.325
sorts of cool deployments that 
you are going to hear about 

01:04:26.326 --> 01:04:28.326
today.

01:04:29.777 --> 01:04:32.674
And this is -- it's really 
valuable to have this kind of 

01:04:32.675 --> 01:04:35.725
platform-independent view.  
Compilers work with data and 

01:04:35.726 --> 01:04:41.016
graphs generally.  And they know
how to do nice optimizations 

01:04:41.017 --> 01:04:43.017
that rely on a global view of 
the computation.

01:04:47.323 --> 01:04:50.189
Expressing the -- data  laying 
and all things like that.  And 

01:04:50.190 --> 01:04:52.190
these are  deep-learning 
specific.

01:04:53.253 --> 01:04:55.694
We can choose how to  properly 
lay out your channels and height

01:04:55.695 --> 01:04:59.626
and width so your convolutions 
are faster.  And finally, like a

01:04:59.627 --> 01:05:04.300
key reason that's very important
to us at Google and important to

01:05:04.301 --> 01:05:06.339
you as well, I hope, once you 
have a platform-independent 

01:05:07.146 --> 01:05:09.779
representation, you can
deploy it and distribute it 

01:05:09.780 --> 01:05:14.486
across hundreds of machines or a
TPU like you saw earlier.  And 

01:05:14.487 --> 01:05:16.508
this is a seamless process.  
Since graphs are so good, what 

01:05:16.509 --> 01:05:18.509
made us 

01:05:19.764 --> 01:05:21.176
think it's a good idea to move 
beyond them and do Eager 

01:05:21.177 --> 01:05:23.840
Execution?  A good place to 
start, you don't have 

01:05:28.323 --> 01:05:30.323
to give up automatic 
differentiation.

01:05:31.580 --> 01:05:33.580
Like Python's autograph -- 
sorry, 

01:05:35.070 --> 01:05:37.750
autograph, that lets you shape 
dynamic code.  You don't need to

01:05:37.751 --> 01:05:40.586
have the computation to 
differentiate it.  You can build

01:05:40.587 --> 01:05:42.820
up a trace as you go and walk 
back the trace to compute 

01:05:42.821 --> 01:05:44.821
gradients.

01:05:47.298 --> 01:05:49.332
Also, if you don't stop to build
a platform like in this 

01:05:49.333 --> 01:05:53.616
computational graph, you can 
iterate a lot more quickly.  You

01:05:53.617 --> 01:05:55.451
can play with your model as you 
build it, inspect it, poke and 

01:05:55.452 --> 01:06:00.112
prod at it.  And this can let 
you just be more productive when

01:06:00.113 --> 01:06:03.384
you're like, making all these 
things.  Also, you can run your 

01:06:03.385 --> 01:06:05.839
model for debuggers and 
profilers and add all 

01:06:09.287 --> 01:06:10.717
sorts of, like, analysis to them
to just really understand how 

01:06:10.718 --> 01:06:12.718
they're doing what they're 
doing.  And finally,

01:06:14.583 --> 01:06:16.203
if we don't force you to 
represent your computation in a 

01:06:16.204 --> 01:06:18.449
separate way than the host 
programming language you're 

01:06:18.450 --> 01:06:20.450
using, 

01:06:21.507 --> 01:06:24.388
you can just use machinery of 
your host programming language 

01:06:24.389 --> 01:06:26.389
to do control flow 

01:06:28.053 --> 01:06:30.101
and complicated data structures 
which for some models is key to 

01:06:30.102 --> 01:06:33.565
making the model work at all.  
So, I hope you're not wondering 

01:06:33.566 --> 01:06:35.566
how do I get to use this?

01:06:38.254 --> 01:06:40.254
The way to use this is  
super-easy.

01:06:41.502 --> 01:06:43.502
Import TensorFlow and have Eager
Execution.

01:06:46.170 --> 01:06:48.402
And what happens is any time you
run a TensorFlow application, 

01:06:48.403 --> 01:06:52.685
instead of TensorFlow  building 
a graph that later when executed

01:06:52.686 --> 01:06:54.686
is going to run that 

01:06:55.736 --> 01:06:57.154
matrix multiplication, we run 
that for you and give you the 

01:06:57.155 --> 01:07:01.825
result.  You can print it, you 
can slice it, dice it, do 

01:07:01.826 --> 01:07:03.866
whatever you want with it.  And 
because things are happening 

01:07:05.922 --> 01:07:08.361
immediately, you can have highly
dynamic control flow that 

01:07:08.362 --> 01:07:10.008
depends on the actual values of 
the computation you're  

01:07:10.009 --> 01:07:14.696
executing.  And here is just a 
simple conditions line search 

01:07:14.697 --> 01:07:15.706
example that I wrote, and it 
doesn't

01:07:15.707 --> 01:07:17.707
matter.

01:07:20.388 --> 01:07:21.411
It just is loops that have 
complicated values based on the 

01:07:21.412 --> 01:07:24.872
computation.  And this runs just
fine on whatever device you 

01:07:24.873 --> 01:07:26.873
have.

01:07:28.754 --> 01:07:30.995
And together with this enable 
Eager Execution theme, we're 

01:07:30.996 --> 01:07:36.074
bringing you a few symbols in 
TensorFlow that make it easier 

01:07:36.075 --> 01:07:38.075
for you to write code both 

01:07:39.123 --> 01:07:41.760
building graphs and executing 
Eagerly.  We're bringing in a 

01:07:41.761 --> 01:07:43.761
new way of doing gradients.

01:07:46.255 --> 01:07:46.861
And you're familiar with how you
do gradients in normal 

01:07:46.862 --> 01:07:49.509
TensorFlow.  Great the variable 
and the loss function.

01:07:52.550 --> 01:07:54.550
I hope you can think of a better
loss 

01:07:55.599 --> 01:07:56.211
function than this one, and you 
call gradients to differentiate 

01:07:56.212 --> 01:07:58.212
it.

01:07:59.663 --> 01:08:01.919
But when you have eager 
execution, we try to be as 

01:08:01.920 --> 01:08:03.920
efficient as you can.

01:08:05.367 --> 01:08:08.002
And if you're going to 
differentiate, you need to keep 

01:08:08.003 --> 01:08:09.634
track of the memory of 
information of what's happening 

01:08:09.635 --> 01:08:13.092
so far.  Like your activation.  
But I don't want you to pay for 

01:08:13.093 --> 01:08:18.201
the cost of this tracking when 
you're not computing gradients. 

01:08:18.202 --> 01:08:20.237
Performance, the whole reason 
we're doing this, we want to use

01:08:20.238 --> 01:08:25.761
these big, nice
pieces hardware to train models 

01:08:26.169 --> 01:08:28.602
super-fast.  When you want to 
compute gradients, 

01:08:32.477 --> 01:08:34.509
you use this context manager and
records all the operations you 

01:08:34.510 --> 01:08:37.567
execute so we can play it back. 
Otherwise, the API  is the same.

01:08:41.625 --> 01:08:44.467
Also,  training loops in Eager, 
as Derek pointed out, is much --

01:08:44.468 --> 01:08:48.140
it's very easy and 
straightforward.  You can just 

01:08:48.141 --> 01:08:53.018
use a 5.4 loop to iterate over 
your data sets.  And data sets 

01:08:53.019 --> 01:08:55.019
work in Eager just fine and work
with the same high performance 

01:08:57.344 --> 01:08:59.573
you get in the graph execution 
engine.  Then you can do your 

01:08:59.574 --> 01:09:03.253
predictions, supply your 
gradients and do other things 

01:09:03.254 --> 01:09:05.254
you're  used to doing.  But 
really, the interesting thing 

01:09:08.374 --> 01:09:09.994
about Eager Execution is not 
just when you're writing the 

01:09:09.995 --> 01:09:14.163
code that it's finished, that 
it's done that we already know 

01:09:14.164 --> 01:09:16.420
works, but you're still 
developing.  You want to do 

01:09:16.421 --> 01:09:21.926
things like debug.  So, when 
Eager Execution is enabled, you 

01:09:21.927 --> 01:09:23.927
can just take any model code and
I 

01:09:24.974 --> 01:09:26.974
use my simple example here.

01:09:28.851 --> 01:09:30.851
Add notes to, like, to anywhere 
you want, and once you're in

01:09:33.127 --> 01:09:35.986
the debugger, you have the full 
power of debugging available.  

01:09:35.987 --> 01:09:38.409
You can print the value of any 
tensor, change the value of any 

01:09:38.410 --> 01:09:40.410
tensor, run any operation you 
want on any tensor.

01:09:43.338 --> 01:09:44.552
And this will hopefully empower 
you to really understand what's 

01:09:44.553 --> 01:09:46.996
going on in your models.  And 
you'll be able to fix any 

01:09:46.997 --> 01:09:48.997
problems you have.

01:09:50.654 --> 01:09:52.890
You can also take Eager 
Execution code and profile it 

01:09:52.891 --> 01:09:54.891
using whatever profiling tool 
you are most familiar and 

01:09:55.127 --> 01:09:57.127
comfortable with.

01:09:59.021 --> 01:10:01.021
So, here, I have a little dump 
model 

01:10:02.482 --> 01:10:04.927
that just does an app.  And 
let's pretend I don't know which

01:10:08.578 --> 01:10:10.578
is going to be the lower, this 
one is more expensive.

01:10:13.460 --> 01:10:15.460
But you can run the code for the

01:10:18.758 --> 01:10:20.758
Payton profiler and find out the
matmul 

01:10:22.210 --> 01:10:24.453
is 15 times more expensive.  
Also, by the  way, those 

01:10:24.454 --> 01:10:26.454
examples are 

01:10:30.576 --> 01:10:32.576
run on the Google collaborate 
thing, 

01:10:33.842 --> 01:10:35.453
which is a completely public 
shared for notebooks hosted on 

01:10:35.454 --> 01:10:40.353
Google prod.  And I think we 
have a demo on Eager that's 

01:10:40.354 --> 01:10:42.990
hosted on that you can play out 
with later.  If you're on live 

01:10:42.991 --> 01:10:47.919
stream, you can play with it now
if you can find the link.  But 

01:10:47.920 --> 01:10:49.920
together with Eager, we're 

01:10:51.366 --> 01:10:53.366
bringing a lot of new APIs to 
make it 

01:10:55.437 --> 01:10:57.468
easier to make graphs and 
execute models.  They are 

01:10:57.469 --> 01:11:01.356
compatible with Eager Execution 
and graph modeling.  A low 

01:11:01.357 --> 01:11:03.588
priority feature request is how 
to customize gradients in 

01:11:03.589 --> 01:11:06.237
TensorFlow.  And I'm sure you're
familiar with

01:11:08.502 --> 01:11:11.154
the tricks, stop gradients and 
functions.  But we're 

01:11:11.155 --> 01:11:12.786
introducing a new API that works
in both eager and graph 

01:11:12.787 --> 01:11:16.861
execution.  What I like about 
this example is it's a thing 

01:11:16.862 --> 01:11:18.862
being asked by many, many people
how to do it.

01:11:20.521 --> 01:11:22.757
If I want to have my forward 
pass and the backward pass, take

01:11:22.758 --> 01:11:24.758
the gradient from a particular 
TensorFlow and clip it.

01:11:28.289 --> 01:11:30.289
Keep it small to prevent it from
exploding.

01:11:31.945 --> 01:11:33.563
It just takes six lines of code 
to clip the gradient.  And I 

01:11:33.564 --> 01:11:35.564
think this is cool.  I look 
forward to seeing what you can 

01:11:37.248 --> 01:11:39.248
do with this when you're doing 
more than 

01:11:40.743 --> 01:11:41.560
six lines of code and solving 
all new and interesting research

01:11:41.561 --> 01:11:46.030
problems.   A big, big change 
when programming with Eager from

01:11:46.031 --> 01:11:48.675
graph that I really want you to 
stop and think about is we're 

01:11:51.728 --> 01:11:54.381
trying to make everything as 
Pythonic and object-oriented as 

01:11:54.382 --> 01:11:56.382
possible.

01:11:58.058 --> 01:11:59.487
So, variables in had TensorFlow 
are usually a complicated thing 

01:11:59.488 --> 01:12:01.488
to think 

01:12:02.953 --> 01:12:04.953
about, but when execution is 
enabled, simpler.

01:12:07.253 --> 01:12:08.269
It's just a Python object.  You 
can change the value, read the 

01:12:08.270 --> 01:12:10.910
value. 
When the last reference to it 

01:12:10.911 --> 01:12:13.957
goes away, you get the memory 
back.  Even if it's the GPU 

01:12:13.958 --> 01:12:18.020
memory.  So, if you want to 
share variables, you just reuse 

01:12:18.021 --> 01:12:22.900
those objects.  You don't worry 
about variable scopes or any 

01:12:22.901 --> 01:12:25.753
other complicated structure.  
And because we have this, like, 

01:12:28.390 --> 01:12:30.390
object-oriented approach to 
variables, 

01:12:31.440 --> 01:12:33.440
you can look at the APUs in 
TensorFlow 

01:12:35.751 --> 01:12:36.562
and rethink them in a way that's
object-oriented and easier to 

01:12:36.563 --> 01:12:39.614
use.  And one is the overhaul 
with the metrics API.

01:12:44.691 --> 01:12:46.691
So, we're  introducing this new 
tfe.

01:12:47.961 --> 01:12:49.961
metrics, one has an updoubt of 
value, and one gives the result.

01:12:52.059 --> 01:12:54.089
And hopefully this is an API 
that everyone is going to find 

01:12:54.090 --> 01:12:58.565
familiar.  Please don't try to 
compare this to the other 

01:12:58.566 --> 01:13:00.566
metrics API.

01:13:01.606 --> 01:13:03.053
We're giving you a way to do 
object-oriented saving of 

01:13:03.054 --> 01:13:07.316
TensorFlow models.  If you tried
looking at TensorFlow check 

01:13:07.317 --> 01:13:10.162
points, you know they depend on 
variable names.  And variable 

01:13:10.163 --> 01:13:12.219
names depend not just on the 
game show variable, but all 

01:13:12.220 --> 01:13:14.447
other variables present in the 
graph. 

01:13:16.703 --> 01:13:18.324
This can make it hard to save 
and load subsets of the model 

01:13:18.325 --> 01:13:22.210
and really control what's in the
check point.  We're introducing 

01:13:22.211 --> 01:13:24.211
a completely 

01:13:25.671 --> 01:13:27.671
object-oriented,  python-object 
based 

01:13:29.332 --> 01:13:32.004
saving API where you -- it's 
like Python -- any model gets 

01:13:32.005 --> 01:13:35.445
saved, you can save any subset 
of your model.  You can load any

01:13:35.446 --> 01:13:39.968
subset of your model.  You can 
even use this tfe.checkpoint 

01:13:39.969 --> 01:13:42.620
object to build things you want 
to save that have more than a 

01:13:42.621 --> 01:13:44.621
model.

01:13:47.541 --> 01:13:49.775
Here we have an optimizer and a 
global stack.  You can put 

01:13:49.776 --> 01:13:52.009
whatever you want in there.  The
object graph is something you 

01:13:52.010 --> 01:13:57.123
can save and load.  You can save
and load your discriminators and

01:13:57.124 --> 01:13:59.124
generators separate.  And take 
the discriminator and load it 

01:14:01.207 --> 01:14:02.830
back up as another network that 
you can use on another part of 

01:14:02.831 --> 01:14:07.348
the model.  This should give you
a lot more control to get a lot 

01:14:07.349 --> 01:14:11.014
more out of, like, TensorFlow 
checkpoint.  But if you have a 

01:14:11.015 --> 01:14:15.283
question that everybody asks me 
when I tell them to work with 

01:14:15.284 --> 01:14:18.336
the Eager Execution, is it fast?
Graphs have this high

01:14:18.532 --> 01:14:20.532
performance.

01:14:21.994 --> 01:14:24.628
How fast can I make this run 
Python code all the time?  We 

01:14:24.629 --> 01:14:27.473
can make it fast enough.  For 
models that are highly  

01:14:29.313 --> 01:14:31.313
computationally intensive, you 
don't see 

01:14:33.369 --> 01:14:36.633
any Python overhead and we are 
fast with the TensorFlow.  

01:14:36.634 --> 01:14:37.653
Sometimes slightly faster, and 
reasons that I don't fully 

01:14:37.654 --> 01:14:39.654
understand.

01:14:41.519 --> 01:14:43.519
Even for highly dynamic models, 
you 

01:14:44.759 --> 01:14:46.759
have comparative performance 
with anything else you can find.

01:14:48.223 --> 01:14:50.223
And please don't get attached to
these numbers.

01:14:53.740 --> 01:14:55.374
We have many more benchmarks and
we're optimizing Eager 

01:14:55.375 --> 01:14:58.439
performance aggressively.  But I
hope you know if your model can 

01:15:01.490 --> 01:15:03.115
keep it busy, you're doing large
matrix computations, there's no 

01:15:03.116 --> 01:15:07.189
cost in experimenting and doing 
your research and model building

01:15:07.190 --> 01:15:10.033
with Eager Execution turned on. 
When you're doing smaller 

01:15:10.034 --> 01:15:15.117
things, there are overheads.  
Don't get attached to them.  

01:15:15.118 --> 01:15:17.118
We're being aggressive about 
optimizing this.

01:15:20.601 --> 01:15:22.853
If you run an identity, it takes
a microsecond.  If you run it 

01:15:22.854 --> 01:15:26.105
with Eager Execution turned on, 
there's an extramicrosecond. 

01:15:26.106 --> 01:15:29.760
If you're tracing gradients, 
another three microseconds.

01:15:34.064 --> 01:15:36.531
But just enqueuing something on 
the  GPU screen, that takes a 

01:15:36.532 --> 01:15:38.532
single digit microsecond.

01:15:39.806 --> 01:15:42.244
So, if you can execute enough 
computation to keep a GPU busy, 

01:15:42.245 --> 01:15:47.126
you're unlikely to see anything 
bad from using Eager Execution. 

01:15:47.127 --> 01:15:50.191
And, again, these numbers are 
improving very quickly.  Please 

01:15:50.192 --> 01:15:52.192
don't get too attached to them.

01:15:53.862 --> 01:15:55.862
But there is this large 
ecosystem of 

01:15:56.902 --> 01:15:58.522
TensorFlow code libraries, 
models, frameworks, check 

01:15:58.523 --> 01:16:01.581
points, that I don't think 
anyone wants to give up.  And I 

01:16:01.582 --> 01:16:04.634
don't want you it give up if you
want to used Eager Execution.  

01:16:04.635 --> 01:16:06.635
So, we're also thinking really 
hard 

01:16:08.907 --> 01:16:09.922
about how can you -- how you can
interoperate between Eager and 

01:16:09.923 --> 01:16:13.168
graph.  One way is to call into 
graphs from Eager code.

01:16:17.439 --> 01:16:21.315
And you can do that with tfe.
make template.  We build a graph

01:16:21.316 --> 01:16:23.343
for that little Python function 
and you can use it and 

01:16:25.580 --> 01:16:28.043
manipulate and call the graph 
from Eager Execution.  We also 

01:16:28.044 --> 01:16:30.887
have the reverse, which is how 
to call into Eager from a graph.

01:16:31.285 --> 01:16:33.138
Let's say
you have a big graph and you 

01:16:33.139 --> 01:16:36.377
understand everything in it, but
there's a little chunk of your 

01:16:36.378 --> 01:16:38.836
computation that you really 
don't know how to express in -- 

01:16:40.457 --> 01:16:42.457
either don't know, or you don't 
want to 

01:16:43.919 --> 01:16:45.919
bother expressing it in using 
liar TensorFlow graphs.

01:16:49.846 --> 01:16:51.846
So, you can wrap it in a tfe 
graph, 

01:16:53.113 --> 01:16:55.113
and you can run any TensorFlow 
in there, 

01:16:56.146 --> 01:16:58.146
including convolutions and other
things not available.

01:17:00.629 --> 01:17:03.286
And you can look at the values 
and use dynamic control.  I hope

01:17:03.287 --> 01:17:05.287
with these two things together, 

01:17:06.742 --> 01:17:08.742
you can reuse Eager and graph 
code across.

01:17:11.611 --> 01:17:13.611
But the easiest way to get Eager
and 

01:17:15.070 --> 01:17:16.479
graph compatibility is to write 
model code that can go both 

01:17:16.480 --> 01:17:18.480
ways.

01:17:19.730 --> 01:17:21.730
Once the code is written and 
debugged 

01:17:24.014 --> 01:17:26.855
and tested, there's nothing to 
tell you to build a graph or 

01:17:26.856 --> 01:17:30.512
execute Eagerly.  Debug in Eager
and impart that same code into 

01:17:30.513 --> 01:17:32.513
graph.

01:17:34.172 --> 01:17:37.025
Put it in estimator, deploy it 
on the GPU, distribute it.  Do 

01:17:37.026 --> 01:17:38.858
whatever you want.  This is what
we've done in the example 

01:17:38.859 --> 01:17:41.107
models.  And there's going to be
a link in the end of the of

01:17:42.534 --> 01:17:43.136
presentation so you don't need 
to worry about writing this 

01:17:43.137 --> 01:17:45.137
down.

01:17:47.245 --> 01:17:49.245
So, here is some practical 
advice for you.

01:17:51.190 --> 01:17:53.619
Write code that's going to work 
well when executing Eagerly and 

01:17:53.620 --> 01:17:58.092
building graphs.  To do that, 
use the Keras layers, they're  

01:17:58.093 --> 01:17:59.722
object-oriented, easy to 
understand, manipulate and play 

01:17:59.723 --> 01:18:01.723
around with.

01:18:03.187 --> 01:18:04.656
Use the Keras model, that will 
give you saving and loading and 

01:18:04.657 --> 01:18:08.969
training and all sorts of things
automatically if you want.  But 

01:18:08.970 --> 01:18:10.970
you're not forced to use those.

01:18:13.443 --> 01:18:14.250
Use config summary, they will 
move to the TensorFlow package 

01:18:14.251 --> 01:18:16.886
soon.  If you're watching this 
on video, probably already 

01:18:16.887 --> 01:18:18.887
happened.

01:18:21.571 --> 01:18:24.011
Use the tfe metrics instead of 
the  tf.metrics, these are 

01:18:24.012 --> 01:18:27.259
object-oriented and friendier 
and eager to use.  And use the 

01:18:27.260 --> 01:18:29.260
object-based saving.

01:18:30.337 --> 01:18:32.337
Which is a much nicer user 
experience anyway.

01:18:34.427 --> 01:18:37.331
So, you're going to want to do 
this all the time, it's how your

01:18:37.332 --> 01:18:40.616
code is going to work well in 
Eager execution and graph 

01:18:40.617 --> 01:18:43.867
building.  So, now, I would like
to take some time to tell you 

01:18:43.868 --> 01:18:45.868
why you

01:18:47.532 --> 01:18:49.532
should enable Eager execution, 
and like 

01:18:50.813 --> 01:18:52.813
a real good importance reason 
that led 

01:18:54.279 --> 01:18:56.723
us to build this in the first 
place, being able to play these 

01:18:56.724 --> 01:19:01.812
objects and manipulate them 
directly is just a much nicer 

01:19:01.813 --> 01:19:04.063
experience than having to build 
a graph and interact later in 

01:19:04.064 --> 01:19:08.124
the session.  It's a lot more 
intuitive, let's you understand 

01:19:08.125 --> 01:19:10.125
what's going on better.

01:19:11.730 --> 01:19:13.730
If you're

01:19:17.865 --> 01:19:19.865
new, play around

01:20:01.789 --> 01:20:06.496
&gt;&gt; Now I would like to point to 
a few things.  Some of my 

01:20:06.497 --> 01:20:09.334
colleagues, they're going to be 
in the demo room during the 

01:20:09.335 --> 01:20:11.335
break 

01:20:13.418 --> 01:20:15.086
with laptops, with  notebooks to
let you type and try Eager mode 

01:20:15.087 --> 01:20:18.761
there.  Please go and give it a 
try.  Or if you're watching on 

01:20:18.762 --> 01:20:20.762
the live stream, type that short
link.

01:20:23.248 --> 01:20:26.095
Hopefully it will stay long 
enough for you to type it.  And 

01:20:26.096 --> 01:20:29.544
play with it right now.  It's 
really nice.  We have a getting 

01:20:29.545 --> 01:20:34.664
started guide on TensorFlow that
should be live now.  That tells 

01:20:34.665 --> 01:20:37.113
you what you need to know about 
Eager execution and starting to 

01:20:38.950 --> 01:20:41.181
use TensorFlow using Eager 
Execution.  We have a ton of 

01:20:41.182 --> 01:20:43.182
example models like 

01:20:45.257 --> 01:20:47.686
from RNN to net to all purposing
that are available behind that 

01:20:47.687 --> 01:20:49.687
link, and I 

01:20:51.353 --> 01:20:53.200
encourage you to look at them 
and how easy to write the model 

01:20:53.201 --> 01:20:55.201
and how easy it 

01:20:57.282 --> 01:21:00.133
is to reuse the same code from 
graphs to deployment.  We have 

01:21:00.134 --> 01:21:02.582
deployment for graph from all 
models except for the highly 

01:21:02.583 --> 01:21:05.850
dynamic ones which are hard to 
write in a graph form.  Give it 

01:21:05.851 --> 01:21:11.174
a try.  Let us know how it went.
We're super-excited to share 

01:21:11.175 --> 01:21:16.049
with you and I hope you have a 
good time playing with this.  

01:21:16.050 --> 01:21:18.050
Thank you.

01:21:27.283 --> 01:21:29.962
And now it's time to have a 
treat.  Introducing Nick hill 

01:21:29.963 --> 01:21:31.963
and Daniel.

01:21:33.208 --> 01:21:36.065
They have a cool demo set up, 
but I don't to spoil it.

01:21:45.021 --> 01:21:48.083
&gt;&gt; Hi, everyone, my name is 
Daniel.

01:21:50.109 --> 01:21:52.958
&gt;&gt; My anymore is Nikhil.
&gt;&gt; We're from the Google brain 

01:21:52.959 --> 01:21:55.586
team.  And  today, we're 
delighted to talk about 

01:21:55.587 --> 01:21:59.069
JavaScript.  Python has been one
of the mainstream languages for 

01:21:59.070 --> 01:22:01.070
scientific computing.  And it's 
been like that for a while.

01:22:03.951 --> 01:22:05.951
And there's a lot of tools and 
libraries around Python.

01:22:07.610 --> 01:22:09.610
But that's where it ends.

01:22:10.854 --> 01:22:12.486
We're here today to talk -- to 
convince you that JavaScript and

01:22:12.487 --> 01:22:14.487
the 

01:22:16.548 --> 01:22:18.548
browser have a lot to offer.

01:22:20.207 --> 01:22:22.207
And TensorFlow Playground is a 
great example of that.

01:22:24.283 --> 01:22:26.098
I'm curious, how many people 
have seen TensorFlow  Playground

01:22:26.099 --> 01:22:28.997
before?  Oh, wow.  Quite a few. 
I'm very glad.

01:22:32.080 --> 01:22:34.937
Those of you who haven't seen 
it, check it out after the talk 

01:22:34.938 --> 01:22:36.938
at playground.tensorFlow  
Lite.org.

01:22:40.280 --> 01:22:42.280
It's a visualization of a small 
neural network.

01:22:44.139 --> 01:22:46.139
And it shows in real-I'm the 
neural network as it's training.

01:22:47.399 --> 01:22:49.438
And this is a lot of fun to make
and had a huge educational 

01:22:49.439 --> 01:22:52.108
success.  We have been getting 
emails from high schools and  

01:22:52.109 --> 01:22:54.575
universities that have been 
using this to teach students 

01:22:54.576 --> 01:22:58.844
about machine learning. 
After we launched playground, we

01:22:58.845 --> 01:23:03.111
were wondering, why was it so  
successful?  And we think one 

01:23:03.112 --> 01:23:06.776
big reason was because it was in
the browser.  And the browser is

01:23:06.777 --> 01:23:11.851
this unique platform where you 
-- the things you build, you can

01:23:11.852 --> 01:23:14.899
share with anyone with just the 
link.  And those people that 

01:23:14.900 --> 01:23:16.900
open your app 

01:23:18.357 --> 01:23:19.781
don't have to install any 
drivers or any software.  It 

01:23:19.782 --> 01:23:25.059
just works.  Another thing is, 
it's -- the browser is highly 

01:23:25.060 --> 01:23:28.950
interactive.  And so, the user 
is going to be  engaged with 

01:23:28.951 --> 01:23:30.951
whatever you're building.

01:23:32.209 --> 01:23:34.650
Another big thing is that 
browsers -- we didn't take 

01:23:34.651 --> 01:23:36.651
advantage of this in the 

01:23:38.500 --> 01:23:40.130
Playground, but browsers have 
access to sensors like the 

01:23:40.131 --> 01:23:45.623
microphone and the camera and 
the accelerometer.  And all 

01:23:45.624 --> 01:23:49.525
these are behind standardized 
APIs that work on all browsers. 

01:23:49.526 --> 01:23:51.569
And the last and most important 
thing, is the data that comes 

01:23:51.570 --> 01:23:55.449
from these sensors doesn't ever 
have to leave the client.  You 

01:23:55.450 --> 01:23:57.450
don't have to upload anything to

01:23:58.506 --> 01:24:00.506
the server, which preserves 
privacy.

01:24:01.982 --> 01:24:03.982
Now, the  Playground that we

01:24:07.686 --> 01:24:09.686
built is powered by a small 
neural 

01:24:10.738 --> 01:24:12.764
network, 300 lines of vanilla 
JavaScript that we built as a 

01:24:12.765 --> 01:24:16.016
one-off library.  It doesn't 
scale.  It's a simple loop and 

01:24:16.017 --> 01:24:18.017
wasn't engineered to be 
reusable.

01:24:20.533 --> 01:24:22.533
But it was clear to us that if 
we were 

01:24:23.784 --> 01:24:25.845
to open the door for people to 
merge machine learning and the 

01:24:25.846 --> 01:24:28.290
browser, we had to build a 
library.  And we did it.

01:24:31.949 --> 01:24:33.949
We released Deep Learn JS.

01:24:37.653 --> 01:24:39.653
A JavaScript library that is 

01:24:41.728 --> 01:24:44.357
GPU-accelerated and does that 
via WebGL, standards in the 

01:24:44.358 --> 01:24:46.815
browser, and allows it to render
graphics.

01:24:50.066 --> 01:24:52.066
And deeplearn .

01:24:53.113 --> 01:24:54.036
js allows it to both run 
inference and training in the 

01:24:54.037 --> 01:24:57.881
browser.  When we released it, 
we had an incredible momentum.

01:25:02.308 --> 01:25:04.308
The community took deep 
deeplearn.

01:25:06.219 --> 01:25:08.055
js and forwarded it the into 
browser and built fun things 

01:25:08.056 --> 01:25:12.742
with it.  One example is the 
file transfer.  Another had the 

01:25:12.743 --> 01:25:17.218
character RNN and built a novel 
interface that  allows you to 

01:25:17.219 --> 01:25:19.219
explore all the different 
possible  endings of a sentence.

01:25:20.268 --> 01:25:22.268
All generated by the model in 
real-time.  Another

01:25:25.739 --> 01:25:27.582
example is the model -- this was
a post about this one, that the 

01:25:27.583 --> 01:25:29.848
person that built it allowed 
users to explore the 

01:25:33.541 --> 01:25:34.760
hidden dimensions, the 
interesting dimensions in the  

01:25:34.761 --> 01:25:36.761
embedding space.

01:25:38.015 --> 01:25:41.275
And you can see how they relate 
to boldness of the font.  And 

01:25:41.276 --> 01:25:43.276
there was even education 
examples 

01:25:44.326 --> 01:25:46.326
like teachable machines that 
built this 

01:25:47.985 --> 01:25:49.985
fun little game that taught 
people how 

01:25:51.673 --> 01:25:53.504
computer vision models work so 
people could interact directly 

01:25:53.505 --> 01:25:58.998
with the webcam.  Now, all the 
examples I showed you point to 

01:25:58.999 --> 01:26:03.285
the incredible momentum we have 
with  deeplearn.js.  And 

01:26:03.286 --> 01:26:05.286
building on that momentum, we're

01:26:06.946 --> 01:26:08.946
very excited today to announce 
that deeplearn.

01:26:10.001 --> 01:26:12.856
js is joining the TensorFlow 
family.  And with that, we are 

01:26:12.857 --> 01:26:14.857
releasing a new 

01:26:17.563 --> 01:26:19.563
ecosystem of libraries and tools
and 

01:26:21.847 --> 01:26:24.692
machine learning with JavaScript
calls TensorFlow.js.  Now, 

01:26:24.693 --> 01:26:26.693
before we get into the details, 
I 

01:26:28.553 --> 01:26:31.253
want to go over three main use 
cases of how you can use 

01:26:31.254 --> 01:26:33.300
TensorFlow.js today.

01:26:37.162 --> 01:26:38.794
The first use case is write 
models directly in the browser. 

01:26:38.795 --> 01:26:40.622
Huge implications. 
Think of the playground they 

01:26:40.623 --> 01:26:46.309
just showed.  The second use 
case is -- a major use  case -- 

01:26:46.310 --> 01:26:49.161
is you can take a pre-trained 
model in Python, use a script, 

01:26:49.162 --> 01:26:51.162
and you 

01:26:52.406 --> 01:26:54.406
can import it into the browser 
to inference.

01:26:55.860 --> 01:26:57.860
And a related use case is the 
same 

01:26:59.317 --> 01:27:01.367
model that you take during prep,
you can  re-train it, 

01:27:01.368 --> 01:27:03.368
potentially with private data 
that comes from those censors of

01:27:07.263 --> 01:27:09.263
the browser -- in the browser 
itself.

01:27:12.254 --> 01:27:14.254
Now, to give a schematic view, 
we have 

01:27:15.699 --> 01:27:17.699
the bruiser that uses WebGL to 
do fast algebra.

01:27:22.628 --> 01:27:25.679
And two sets of APIs, the ops 
API, which was deeplearn.js, and

01:27:25.680 --> 01:27:27.680
we worked hard to align with 
TensorFlow Python.

01:27:31.997 --> 01:27:34.433
It is powered by an automatic 
differentiation library.  And on

01:27:34.434 --> 01:27:36.925
top of that, we have a 
high-level API work layers API 

01:27:36.926 --> 01:27:40.982
that allows you to use best 
practices and high-level 

01:27:40.983 --> 01:27:42.983
building blocks to write models.

01:27:44.437 --> 01:27:47.089
But I'm also very excited today 
to announce is that we're 

01:27:47.090 --> 01:27:49.090
releasing tools 

01:27:51.573 --> 01:27:53.573
that can take an existing Keras 
model, 

01:27:54.831 --> 01:27:56.245
or TensorFlow  savedmodel and 
forward it automatically

01:27:56.246 --> 01:27:58.246
for execution in the browser.

01:27:59.497 --> 01:28:01.497
Now, to show you an example of 
our 

01:28:02.761 --> 01:28:04.761
API, we're going to go over a 
small 

01:28:07.863 --> 01:28:09.863
program that tries to learn a 
quadratic function.

01:28:11.111 --> 01:28:13.111
They're trying to learn A, B, 
and C from data.

01:28:14.982 --> 01:28:16.982
So, we have our import tf from 
TensorFlow JS.

01:28:20.473 --> 01:28:22.473
This is a standard ES in 
JavaScript.

01:28:24.984 --> 01:28:26.984
We have the three, A, B, C, we 
mark them as variable.

01:28:30.049 --> 01:28:30.875
Which means they are viewable 
and the optimizer can change 

01:28:30.876 --> 01:28:32.876
them.

01:28:35.345 --> 01:28:37.999
We have the function that does 
the computation.  You can see 

01:28:38.000 --> 01:28:43.698
tf.add and tf.square like 
TensorFlow.  Notion that  API, 

01:28:43.699 --> 01:28:46.342
we have a chaining API which 
allows you to pull these math 

01:28:48.577 --> 01:28:50.828
operations on tensors 
themselves, and this leads to 

01:28:50.829 --> 01:28:52.829
better readable code that 

01:28:54.085 --> 01:28:56.085
is closer to how we write it.

01:28:57.152 --> 01:28:58.370
It is very popular in JavaScript
world.  That's that part of the 

01:28:58.371 --> 01:29:00.371
model.

01:29:03.257 --> 01:29:06.551
Now, for the training part, we 
need a loss function.  This is a

01:29:06.552 --> 01:29:08.552
loss function, an error between 
the prediction and the label.

01:29:13.061 --> 01:29:15.061
We have our optimizer, the edge 
of the optimizer.

01:29:17.154 --> 01:29:20.001
And we train the model, 
optimizer.minimize for some 

01:29:20.002 --> 01:29:22.229
model.  And I want to emphasize 
for those who 

01:29:25.891 --> 01:29:27.891
have used

01:29:29.986 --> 01:29:32.624
tf before, Alex's talk, it's 
aligned with the Eager API in 

01:29:32.625 --> 01:29:34.625
Python.  All right.

01:29:36.703 --> 01:29:38.703
So, clearly, that's not how most
people write machine learning.

01:29:43.828 --> 01:29:47.294
Because those low-level algebras
can be quite verbose.  For that,

01:29:47.295 --> 01:29:49.295
we have our layered API.

01:29:51.164 --> 01:29:53.807
So show you an example of that, 
we're going to build a recounter

01:29:53.808 --> 01:29:55.808
neural network that learns the 
numbers.

01:29:58.474 --> 01:30:00.474
But the complicated part is that
those 

01:30:03.022 --> 01:30:04.245
numbers, like the number 90 plus
10, are being set character by 

01:30:04.246 --> 01:30:07.095
character.  And then the neural 
network has to 

01:30:11.202 --> 01:30:13.202
maintain an internal space with 
an LSTM cell.

01:30:14.866 --> 01:30:16.866
And that gets passed into a 
decoder.

01:30:19.969 --> 01:30:22.809
And the decoder has 100,  
character-by-character.  It's a 

01:30:22.810 --> 01:30:24.810
sequence-by-sequence model.

01:30:26.892 --> 01:30:28.714
This may sound complicated, but 
the layered APU is not that much

01:30:28.715 --> 01:30:30.715
code.

01:30:31.795 --> 01:30:33.795
We have the import in 
TensorFlow.js.

01:30:37.906 --> 01:30:40.760
We have the sequential model.  
Those familiar with Keras, this 

01:30:40.761 --> 01:30:42.989
API looks very familiar.  We 
have the first two layers of the

01:30:42.990 --> 01:30:48.293
encoder, the last three layers 
are the decoder.  And that's our

01:30:48.294 --> 01:30:50.294
model.

01:30:52.760 --> 01:30:54.381
We then compile it with a loss, 
an  optimizer, and a metric we 

01:30:54.382 --> 01:30:56.382
want to monitor, like accuracy.

01:30:59.911 --> 01:31:03.161
And we call model.fit with our 
data.  What I want to point out 

01:31:03.162 --> 01:31:05.162
here is the "Await"  keyword.

01:31:07.436 --> 01:31:10.096
This is an asynchronous call 
which  means -- because in 

01:31:10.097 --> 01:31:14.598
practice, that can take 30-40 
seconds in the browser.  And in 

01:31:14.599 --> 01:31:17.080
those 30-40 seconds, you don't 
want the main UI thread of the 

01:31:17.081 --> 01:31:21.166
browser to be locked.  And this 
is why you get a call back with 

01:31:21.167 --> 01:31:23.873
a history object after that's 
done.  And in between the GPU is

01:31:23.874 --> 01:31:25.874
going to do the work.

01:31:29.990 --> 01:31:32.221
Now, the code I showed you is 
when you are trying to write 

01:31:32.222 --> 01:31:34.674
models directly -- when you want
to write model the  directly in 

01:31:34.675 --> 01:31:36.675
the browser.

01:31:38.124 --> 01:31:40.124
But, as I said before, a major 
use 

01:31:42.004 --> 01:31:44.004
case -- even with deeplearn.

01:31:46.516 --> 01:31:48.960
js, were people importing models
that were pre-trained and they 

01:31:48.961 --> 01:31:50.999
wanted to do it in the browser. 
Before the details of that, I 

01:31:51.000 --> 01:31:53.000
want to 

01:31:55.685 --> 01:31:57.685
show you a fun little game that 
our

01:31:57.738 --> 01:31:59.738
friends built that takes 
advantage of an 

01:32:01.624 --> 01:32:03.624
automatically pre-trained model 
and imports into the becauser.

01:32:05.100 --> 01:32:07.100
It's called emoji scavenger 
hunt.

01:32:09.989 --> 01:32:11.406
I'm going to show you a real 
demo with the phone.  It's in 

01:32:11.407 --> 01:32:14.868
the browser.  Let's see.  And 
you can see here.  So, you can 

01:32:14.869 --> 01:32:16.869
see I have a Chrome 

01:32:18.788 --> 01:32:20.205
browser  opened up on a Pixel 
phone.  You can see it at the 

01:32:20.206 --> 01:32:22.206
top.

01:32:23.246 --> 01:32:25.246
And the game uses the webcam and
shows 

01:32:26.255 --> 01:32:28.255
me

01:32:29.994 --> 01:32:32.244
an emoji and I have some number 
of seconds to find the emoji 

01:32:32.245 --> 01:32:34.245
before the time runs out.

01:32:38.345 --> 01:32:39.553
Nikhil is going to help me 
identify the objects.  Are you 

01:32:39.554 --> 01:32:41.394
ready?
&gt;&gt; I'm ready.

01:32:41.395 --> 01:32:44.442
&gt;&gt; All right.  Let's go.  All 
right.  Watch.

01:32:45.854 --> 01:32:51.773
&gt;&gt; Have a watch.
&gt;&gt; Nice.  Yay!  We got that.

01:32:55.426 --> 01:32:57.426
Let's see what our next item is.

01:32:59.287 --> 01:33:02.742
Shoe.
&gt;&gt; Shoe.

01:33:04.568 --> 01:33:06.568
&gt;&gt; Help me out here, buddy.  We 
got the shoe!

01:33:06.822 --> 01:33:08.856
&gt;&gt; What's next?
&gt;&gt; That's a banana.

01:33:10.875 --> 01:33:13.732
&gt;&gt; Does anyone -- this guy's got
a banana.

01:33:15.368 --> 01:33:17.836
&gt;&gt; Come over here.  Yay!
&gt;&gt; All right.

01:33:18.037 --> 01:33:20.037
&gt;&gt; All right.
&gt;&gt; I'm ready.

01:33:21.699 --> 01:33:23.699
&gt;&gt; We're going to have a high 
score here.   Beer.

01:33:24.144 --> 01:33:28.613
&gt;&gt; Beer.  It's 10:30 in the 
morning, Daniel.  Step out --

01:33:28.614 --> 01:33:32.887
&gt;&gt; All right.  All right.  So, 
I'm going to jump into some of 

01:33:32.888 --> 01:33:39.998
the technical details of how we 
actually built that game.  Stand

01:33:39.999 --> 01:33:41.999
by, please.

01:33:43.689 --> 01:33:45.689
So, what we did was we trained a
model 

01:33:46.993 --> 01:33:49.649
in TensorFlow to be an object 
recognizer for the game.  We 

01:33:49.650 --> 01:33:50.868
chose about 400 different 
classes that would be reasonable

01:33:50.869 --> 01:33:52.869
for a game like this.

01:33:55.336 --> 01:33:57.336
You know, watches and baa 
bananas and beer.

01:34:00.069 --> 01:34:02.069
We used the TensorFlow for poets
code  lab.

01:34:03.536 --> 01:34:06.786
And in that code lab, you take a
pre-trained mobile net model.  

01:34:06.787 --> 01:34:09.641
If you don't know what MobileNet
is, it's a state of the art 

01:34:09.642 --> 01:34:11.642
computer model for edge devices.

01:34:13.710 --> 01:34:15.710
We took that model and retrained
it.

01:34:17.174 --> 01:34:20.225
Now we have an object detector 
in the pipeline.  How to get 

01:34:20.226 --> 01:34:22.226
this into the browser?  We 
provided this tool today to help

01:34:22.455 --> 01:34:24.455
you do that.

01:34:26.325 --> 01:34:27.737
Once it's in, you get the same 
and make the computer talk and 

01:34:27.738 --> 01:34:31.620
all that kind of fun stuff.  
Let's jump into how but convert 

01:34:31.621 --> 01:34:37.303
that model.  As Daniel mentioned
earlier, we support two types of

01:34:37.304 --> 01:34:39.304
models.  TensorFlow saved 
models, we have

01:34:41.175 --> 01:34:43.175
a converter for that, and a 
converter for Keras saved model.

01:34:44.834 --> 01:34:47.669
You define the model and define 
it with the saved model.  The 

01:34:47.670 --> 01:34:49.670
standard way to do that.

01:34:50.722 --> 01:34:53.197
Similarly, this is the code for 
Keras.  The next piece is that 

01:34:53.198 --> 01:34:55.198
we actually convert it to the 
web.

01:35:00.156 --> 01:35:02.156
Today, we're releasing a 
package, TensorFlow.

01:35:04.419 --> 01:35:06.468
js, a script lets you point to 
the TensorFlow save model and 

01:35:06.469 --> 01:35:08.469
lets you point to the output 
director.

01:35:10.949 --> 01:35:13.189
That's where the static built 
art facts go.  Keras is the same

01:35:13.190 --> 01:35:15.190
flow.

01:35:16.857 --> 01:35:18.857
Point to the output and you have
a directory.

01:35:20.942 --> 01:35:22.155
Now, you statically host those 
on the website, simple static 

01:35:22.156 --> 01:35:25.858
hosting.  And on the JavaScript 
side, we provide an API that 

01:35:25.859 --> 01:35:28.090
lets you load that model.  So, 
this is what it looks like for 

01:35:28.301 --> 01:35:33.020
TensorFlow.  And the TensorFlow 
save model, we noticed that it 

01:35:33.021 --> 01:35:35.021
was a model, we don't right now 
support continuing training of 

01:35:35.259 --> 01:35:39.725
this model.  While in the Keras 
case we actually let you 

01:35:39.726 --> 01:35:41.726
continue  training.

01:35:44.200 --> 01:35:45.620
And we're working hard to let 
you keep these APIs alive in the

01:35:45.621 --> 01:35:48.888
future.  Under the
cover, what are we actually 

01:35:48.889 --> 01:35:50.889
doing?  Graph optimization.

01:35:52.739 --> 01:35:54.226
Which essentially means we prune
out nodes you don't need to make

01:35:54.227 --> 01:35:56.265
the prediction.  You don't need 
them.

01:36:00.343 --> 01:36:02.343
We optimize waits for browser 
caching.

01:36:03.585 --> 01:36:05.616
We park in 4 megabytes, helps 
the browser be quick the next 

01:36:05.617 --> 01:36:07.617
time you load.

01:36:08.666 --> 01:36:11.108
Today we support about 90 of the
most commonly used TensorFlow 

01:36:11.109 --> 01:36:13.790
ops, and we're working hard to 
control more flow ops.

01:36:17.040 --> 01:36:18.265
And we support 32 of the most 
commonly used Keras layers 

01:36:18.266 --> 01:36:22.965
today.  And as I mentioned, we 
let you continue training for 

01:36:22.966 --> 01:36:25.816
Keras models and let you do 
evaluation as well as make 

01:36:25.817 --> 01:36:27.817
predictions.  Okay.

01:36:30.290 --> 01:36:32.725
So, obviously there's a lot you 
can do with just porting your 

01:36:32.726 --> 01:36:34.726
models to the web.

01:36:36.182 --> 01:36:39.062
Since the beginning of 
deeplearn.js, we have made it a 

01:36:39.063 --> 01:36:42.970
high priority to train directly 
in the browser.  This opens up 

01:36:42.971 --> 01:36:45.012
the door for education and 
interactive tools like we saw 

01:36:45.013 --> 01:36:47.013
with the playground.

01:36:49.479 --> 01:36:50.690
As well as lets you train with 
data that never leaves your 

01:36:50.691 --> 01:36:52.691
client.  This is huge.

01:36:54.983 --> 01:36:57.813
To show off what you can do with
something like this, we built 

01:36:57.814 --> 01:36:59.814
another little game game.

01:37:02.287 --> 01:37:04.287
The goal of the game is to play 
Pac-Man.

01:37:06.570 --> 01:37:10.026
Daniel is much, much better at 
this game than I am.  Say hi.  

01:37:10.027 --> 01:37:12.681
There's three phases of the 
game.  Phase one, we're going to

01:37:12.682 --> 01:37:17.155
collect frames from the webcam 
and associate those with up, 

01:37:17.156 --> 01:37:20.641
down, left and right.  Daniel's 
going to move his head up, down,

01:37:20.642 --> 01:37:24.117
left and right and he's going to
play the game like that.  And 

01:37:24.118 --> 01:37:26.182
you'll notice, as he's 
collecting frames, he's kind of 

01:37:26.183 --> 01:37:28.420
moving around a little bit, 
which helps the model see 

01:37:30.450 --> 01:37:31.255
different angles for that class 
and generalize a little bit 

01:37:31.256 --> 01:37:36.571
better.  So, after he's done 
collecting these frames, we're 

01:37:36.572 --> 01:37:39.210
going to go and train our model.
So, we're not actually training 

01:37:39.211 --> 01:37:42.055
from scratch here when we hit 
that "Train" button.

01:37:45.363 --> 01:37:47.363
We're taking a pre-trained 
mobile net, 

01:37:48.816 --> 01:37:50.816
porting that to the web, and 
doing a re-training phase.

01:37:53.937 --> 01:37:56.601
And using a  layered APU to do 
this that the browser.  You want

01:37:56.602 --> 01:37:58.671
to hit the train button.  It's 
going down, looks like we're 

01:37:58.866 --> 01:38:01.721
learning
something.  That's great.  So, 

01:38:01.722 --> 01:38:03.543
as soon as we press that play 
button, what's going to lap is 

01:38:03.544 --> 01:38:07.009
we're going to make predictions 
from the webcam.  Those are 

01:38:07.010 --> 01:38:08.833
going to get plugged into those 
controls and it's going to 

01:38:08.834 --> 01:38:12.704
control the Pac-Man game.  
Ready?  All right.

01:38:15.745 --> 01:38:16.761
So, you can see in the bottom 
right, he's highlighting the 

01:38:16.762 --> 01:38:20.019
class that is could  inned.  And
he moves his head around, you'll

01:38:21.029 --> 01:38:23.475
see a change by class.  And he's
off.

01:38:33.041 --> 01:38:37.102
So -- so, all of this code is 
online and you can go fork it.  

01:38:37.103 --> 01:38:40.169
We invite you to do so.  
Obviously this is just a game.  

01:38:40.170 --> 01:38:41.821
But you can imagine, you know, 
other types of applications of 

01:38:41.822 --> 01:38:45.704
this, like make a browser 
extension that lets you control 

01:38:45.705 --> 01:38:47.952
a page for accessibility 
purposes.  So, again, all this 

01:38:47.953 --> 01:38:51.411
code is online.  Please go fork 
if and play and make something 

01:38:51.412 --> 01:38:54.868
else with it.  Okay.  Daniel, I 
know this is fun.

01:38:58.113 --> 01:39:00.113
&gt;&gt; I got it.

01:39:02.395 --> 01:39:07.675
&gt;&gt; Okay.  So, let's talk a 
little bit about performance.  

01:39:07.676 --> 01:39:11.939
So, what we're looking at here 
is a benchmark of MobileNet 1.0 

01:39:11.940 --> 01:39:13.940
running with TensorFlow.  
TensorFlow classic, not with 

01:39:14.805 --> 01:39:18.461
TensorFlow.js.  And I want to 
point out, this is a batch size 

01:39:18.462 --> 01:39:20.462
of one.  This is important 
because we're 

01:39:22.980 --> 01:39:23.593
thinking about this in the 
context of an interactive 

01:39:23.594 --> 01:39:28.486
application.  Maybe this Pac-Man
game, feeding in webcam data, 

01:39:28.487 --> 01:39:30.719
you want to know the prediction 
time for one.  Can't really 

01:39:30.720 --> 01:39:32.720
batch it.

01:39:36.236 --> 01:39:38.888
In the top row, TV Cuda running 
on a 1080 GT X, it's about 3 

01:39:38.889 --> 01:39:40.914
milliseconds.  And the shorter 
the bar, the faster.

01:39:45.204 --> 01:39:47.204
The second row we have 
TensorFlow CPU 

01:39:50.886 --> 01:39:54.357
running with AVX512 on a Macbook
pro here.  And 60 seconds for 

01:39:54.358 --> 01:39:56.358
that.

01:39:58.866 --> 01:40:03.784
Where does TensorFlow.js come 
into the picture?  We're getting

01:40:03.785 --> 01:40:06.014
about 11 milliseconds for this. 
Which is pretty good if you 

01:40:06.015 --> 01:40:10.690
think about this in the context 
of an interactive game.  So, on 

01:40:10.691 --> 01:40:12.929
the laptop we just showed the 
game, we're getting about 100 

01:40:14.133 --> 01:40:16.782
milliseconds for that.  And 
that's still pretty good.  Like,

01:40:20.039 --> 01:40:21.656
you can build a whole 
interactive game with what's 

01:40:21.657 --> 01:40:23.920
running on there.  The web is 
only going to get faster and 

01:40:23.921 --> 01:40:26.755
faster.  There's a whole new set
of standards  

01:40:29.767 --> 01:40:31.767
coming,

01:40:32.832 --> 01:40:35.714
like web GPU to push the 
boundaries.  But the browser has

01:40:35.715 --> 01:40:38.357
limitations.  You can only get 
access to the GPU 

01:40:41.603 --> 01:40:45.714
through WebGL on these APIs.  
How do we scale beyond that?  

01:40:45.715 --> 01:40:46.520
How do we scale beyond the 
limitations we have in the 

01:40:46.521 --> 01:40:48.521
browser?

01:40:50.393 --> 01:40:52.428
There's a whole ecosystem of 
server-side JavaScript tools 

01:40:52.429 --> 01:40:54.472
using NodeJS that we would love 
to take advantage of.

01:40:57.527 --> 01:40:59.527
So, today I'm really happy to 
tell you 

01:41:01.808 --> 01:41:04.650
that we're working on NodeJS 
binding to the TensorFlow C API.

01:41:08.503 --> 01:41:10.503
That means you'll be able to 
write the 

01:41:12.803 --> 01:41:14.803
same with the Eager mode, the 
poll 

01:41:15.848 --> 01:41:17.848
polynomial, and the Pac-Man 
example, and 

01:41:19.733 --> 01:41:21.974
bind to TensorFlow C and have 
your TensorFlow running with 

01:41:21.975 --> 01:41:23.975
CUDA installed.

01:41:25.023 --> 01:41:27.861
Eventual we will run the 
TensorFlow opt backend, that 

01:41:27.862 --> 01:41:31.353
same JS code.  These bindings 
are under active development.  

01:41:31.354 --> 01:41:34.040
Stay tuned.  All right.  So, 
let's recap

01:41:38.068 --> 01:41:40.068
some of the things we launched 
today and talked about.

01:41:43.007 --> 01:41:45.007
This  low-level ops API which 
does 

01:41:46.070 --> 01:41:48.070
hard level linear algebra and 
the Eager mode.

01:41:51.366 --> 01:41:53.366
This is previously known as 
deeplearn.js, rebranding today.

01:41:56.460 --> 01:41:59.099
We  released these high-level 
layers API.  That mirrors 

01:41:59.100 --> 01:42:01.100
TensorFlow layers.

01:42:02.347 --> 01:42:04.165
And we saw that with the 
addition of RNN and the Pac-Man 

01:42:04.166 --> 01:42:06.166
demo.

01:42:08.461 --> 01:42:10.461
We also  showed you how you can 
import 

01:42:12.929 --> 01:42:14.929
TensorFlow saved Model and Keras
for re-training in the browser.

01:42:16.442 --> 01:42:19.291
We have released a bunch of 
demos and examples on GitHub.  

01:42:19.292 --> 01:42:21.532
These are not the only two.  
There is a whole repository that

01:42:21.533 --> 01:42:23.533
can get you started.

01:42:26.637 --> 01:42:28.877
They have live links and you can
poke around and play.  I invite 

01:42:28.878 --> 01:42:31.738
you to do that.  We really want 
to see you get involved in this 

01:42:31.739 --> 01:42:33.739
project.  We have a bunch of 
links here.

01:42:37.829 --> 01:42:39.829
JS.tensorFlow.org is the 
website.

01:42:42.513 --> 01:42:44.513
There's tutorials, 
documentation, et cetera.

01:42:46.991 --> 01:42:51.068
Our code is open source under 
TensorFlow.js, play there too.  

01:42:51.069 --> 01:42:52.281
And we started a community 
mailing list today.  That's the 

01:42:52.282 --> 01:42:55.579
short link
here.  And the community  

01:42:55.580 --> 01:42:59.451
mailing list is for people to 
post demos and ask questions.  

01:42:59.452 --> 01:43:02.082
So, this project was not just 
Daniel and myself.  This is a 

01:43:02.083 --> 01:43:04.131
larger team effort between many 
of our amazing colleagues at 

01:43:04.132 --> 01:43:09.034
Google.  We want to thank them. 
And we want to thank all of the 

01:43:12.506 --> 01:43:15.352
amazing open source contributor 
for deeplearn.js.  And we're 

01:43:15.353 --> 01:43:17.353
really excited to build the 

01:43:19.008 --> 01:43:19.829
next chapter of machine learning
in JavaScript with you.  Thank 

01:43:19.830 --> 01:43:22.095
you.
&gt;&gt; Thank you.

01:43:36.203 --> 01:43:38.203
&gt;&gt; We can

01:43:44.210 --> 01:43:49.509
&gt;&gt; Okay.  We are now going to 
take a break.  We have about a 

01:43:49.510 --> 01:43:52.151
little under half an hour.  We 
are going to be back here at 

01:43:52.152 --> 01:43:54.173
11:30 for a great talk on 
performance.

01:43:57.232 --> 01:43:59.232
Head on over to the auditorium.

01:44:00.473 --> 01:44:02.473
We have food, demos, we have the
speakers there to ask questions.

01:44:03.931 --> 01:44:05.931
Have fun.

02:10:14.291 --> 02:10:16.291
Test.  Test.

02:11:22.585 --> 02:11:28.881
&gt;&gt; So, we've got a few more 
seating down over here.  We can 

02:11:28.882 --> 02:11:30.961
help you out on that end.  We 
have a few seats left over here.

02:11:31.571 --> 02:11:36.244
&gt;&gt; Hi.
&gt;&gt; A few seats over there.

02:11:37.499 --> 02:11:39.499
&gt;&gt; I would like to get started 
again.

02:11:41.564 --> 02:11:43.218
So, hi, my name is Brennan.  I'm
talking about training 

02:11:43.219 --> 02:11:45.219
performance today.

02:11:46.877 --> 02:11:48.495
Now, this talk is centered 
around a user's guide to how to 

02:11:48.496 --> 02:11:50.496
improve performance.

02:11:51.966 --> 02:11:54.205
So, there's -- performance is 
very complicated, a lot of 

02:11:54.206 --> 02:11:57.888
internals to TensorFlow as to 
things we're going to optimize 

02:11:57.889 --> 02:12:00.348
your training time.  But I'm 
going to talk today about how 

02:12:02.620 --> 02:12:04.244
you can make the most of the 
TensorFlow you know and love to 

02:12:04.245 --> 02:12:06.245
converge faster.  Now, before I 
go further, I want to 

02:12:08.916 --> 02:12:10.916
take a moment to acknowledge the
great 

02:12:12.170 --> 02:12:13.584
work that's done not just by our
engineering TensorFlow team and 

02:12:13.585 --> 02:12:18.264
not just by other teams at 
Google, but by our partner 

02:12:18.265 --> 02:12:20.265
teams, for example, the partner 

02:12:21.537 --> 02:12:23.386
team at NVIDIA, doing a lot of 
work to make TensorFlow work 

02:12:23.387 --> 02:12:27.455
fast and I want to acknowledge  
that.  With that, dig into 

02:12:27.456 --> 02:12:30.328
motivation.  Why do we need 
performance and how to improve 

02:12:30.329 --> 02:12:32.369
performance.  Is it just fine 
today?

02:12:36.657 --> 02:12:38.918
Some folks at Baidu put together
research.  If you want to 

02:12:38.919 --> 02:12:40.919
improve the quality of your 
models, just

02:12:41.583 --> 02:12:43.238
train on larger data sets.  
These beautiful straight lines 

02:12:43.239 --> 02:12:45.284
are  showing for multiple 
different models, 

02:12:48.532 --> 02:12:50.167
as you give more and more 
training data, you get linearly 

02:12:50.168 --> 02:12:52.168
more accurate.

02:12:53.215 --> 02:12:55.215
Now, I'm being slightly 
facetious 

02:12:56.261 --> 02:12:58.261
here, if you look closely, the 
axis on 

02:12:59.539 --> 02:13:01.539
the graph are log rhythmic, not 
linear.

02:13:02.594 --> 02:13:04.594
We don't need linearly 
increasing 

02:13:06.654 --> 02:13:09.727
amounts of data, we need 
exponentially more data.  This 

02:13:09.728 --> 02:13:11.728
holds not just for model classes

02:13:13.609 --> 02:13:15.438
on -- in this case, it was 
sequence-to-sequence-type work, 

02:13:15.439 --> 02:13:20.152
but they found this applied to 
images and translation.  To 

02:13:20.153 --> 02:13:22.153
multiple different areas across 
multiple model tops.

02:13:24.241 --> 02:13:26.891
We're going to need to train on 
exponentially more data to 

02:13:26.892 --> 02:13:30.138
improve our model quality.  
Unfortunately, we have quite the

02:13:32.994 --> 02:13:35.846
obstinate adversary, physics.  
Here's a graph of  

02:13:35.847 --> 02:13:39.955
microprocessor trend data over 
40 years.  And we can see that 

02:13:39.956 --> 02:13:42.599
clock frequency has hit a wall. 
And are performance is not 

02:13:42.600 --> 02:13:47.065
getting that much faster 
compared to how it used to.  We 

02:13:47.066 --> 02:13:49.066
are going to have to work a lot 
harder

02:13:49.329 --> 02:13:51.329
to meet the challenges of today 
and tomorrow with performance.

02:13:53.605 --> 02:13:55.605
Silicon itself is not going to 
get us 

02:13:56.659 --> 02:13:58.659
there without a little bit of 
clever cleverness.

02:14:00.315 --> 02:14:02.315
The result of these two forces 
coming 

02:14:07.255 --> 02:14:10.932
together has resulted in a 
Cambrian explosion of hardware. 

02:14:10.933 --> 02:14:12.933
We have TPUs and other exciting 
things coming.

02:14:15.838 --> 02:14:18.327
There's startups like Nirvana is
part of Intel, and the IPU from 

02:14:18.328 --> 02:14:23.004
Graphcore.  Taking points in the
design space and trying 

02:14:23.005 --> 02:14:25.005
different hardware methodologies

02:14:26.695 --> 02:14:27.095
and layouts to get to get the 
best machine learning 

02:14:27.096 --> 02:14:30.979
performance.  This is going to 
be a very exciting, exciting 

02:14:30.980 --> 02:14:32.980
area coming forward as we think 
about performance in the future.

02:14:34.642 --> 02:14:36.642
Now, before I dig into the meat 
of my 

02:14:38.707 --> 02:14:40.940
talk, I do want to give  a -- I 
do want to acknowledge, this is 

02:14:40.941 --> 02:14:42.941
the Jurassic 

02:14:46.620 --> 02:14:48.620
period and not of the pre-
Cambrian era.

02:14:54.212 --> 02:14:56.461
You have a picture of tetal 
bytes that's not licensed, send 

02:14:56.462 --> 02:15:00.154
it my way.  Across models and 
paradigms, it looks  roughly as 

02:15:00.155 --> 02:15:03.830
follows.  You have the training 
data.  You need to load that if 

02:15:03.831 --> 02:15:05.666
in.
This is phase one, read from 

02:15:05.667 --> 02:15:08.940
neither disk or generate from a 
reinforcement learning 

02:15:08.941 --> 02:15:10.941
environment.

02:15:12.228 --> 02:15:14.228
Decompress it, parse it, image 
model, 

02:15:18.953 --> 02:15:21.802
image augmentations, random 
flips, color distortions.  And 

02:15:21.803 --> 02:15:23.803
compute the forward pass, the 

02:15:24.874 --> 02:15:26.874
loss, the backwards pass, your 
gradients.

02:15:28.776 --> 02:15:30.804
And after the gradients, update 
the things you're trying to 

02:15:30.805 --> 02:15:32.805
learn and repeat the cycle 
again.

02:15:36.047 --> 02:15:38.964
Now, again, there's a wide 
variety of accelerators.

02:15:43.239 --> 02:15:45.239
The phase one happens mostly on 
CPUs.

02:15:46.735 --> 02:15:49.807
The accelerator takes over phase
two and phase three.  With that,

02:15:49.808 --> 02:15:51.808
let's dig in.

02:15:54.094 --> 02:15:56.562
In my experience, as people 
migrate to modern accelerators, 

02:15:56.563 --> 02:15:58.563
new TPUs, new 

02:15:59.806 --> 02:16:01.645
generation of GPUs, et cetera, 
phase one is actually where the 

02:16:01.646 --> 02:16:04.485
most performance problems are.  
Things everyone  hits.  There's 

02:16:04.486 --> 02:16:06.722
a problem with phase one.  We're
going to spend a bit of time 

02:16:07.974 --> 02:16:09.974
digging into input  pipelines.

02:16:12.034 --> 02:16:14.687
Now, you heard earlier from 
Derek about tf.data.  This is 

02:16:14.688 --> 02:16:16.688
the far and away recommended 

02:16:18.330 --> 02:16:20.330
API and way to load data into 
TensorFlow.  And here is if

02:16:21.592 --> 02:16:23.592
you're doing simple image model,
for 

02:16:24.604 --> 02:16:27.164
example, ResNet50, it will start
like this.

02:16:32.677 --> 02:16:34.092
Images batched together into tf 
.record files.  You can shuffle 

02:16:34.093 --> 02:16:39.182
and repeat.  The parser function
you map across every input 

02:16:39.183 --> 02:16:41.183
image, that will do things 

02:16:43.046 --> 02:16:45.046
like parse the examples, jpg 
decode, and 

02:16:46.105 --> 02:16:48.105
you batch it up and return the 
dataset.

02:16:50.170 --> 02:16:52.170
Now, if you run this on a 
fancy-pants 

02:16:54.866 --> 02:16:57.912
CloudTPU, a modern accelerator, 
only 150 images a second.  This 

02:16:57.913 --> 02:16:59.913
is nowhere near what you should 
expect.

02:17:02.779 --> 02:17:04.821
Now, before you think, wow, 
cloud TPUs must be garbage, I'm 

02:17:04.822 --> 02:17:08.907
going back to what I was doing 
before, it behooves you to try 

02:17:08.908 --> 02:17:11.143
to optimize performance.  When 
you're optimizing performance, 

02:17:13.160 --> 02:17:15.406
it's important to follow a 
methodology.  You have to 

02:17:15.407 --> 02:17:17.842
measure your performance, find 
your bottleneck, optimize your 

02:17:18.656 --> 02:17:20.656
bottleneck and repeat.

02:17:21.911 --> 02:17:23.911
What does that look like with a 
cloud TPU.

02:17:27.631 --> 02:17:29.631
There's tools in TensorFlow for 
GPUs and TPUs.

02:17:32.136 --> 02:17:34.136
But you have capture TPU 
profile.

02:17:36.010 --> 02:17:38.658
And you can run this to -- 
pointing it to your TPU.  In 

02:17:38.659 --> 02:17:40.659
this case, the

02:17:41.712 --> 02:17:44.776
TPU's name is Seta.  And capture
into a profile log directly.

02:17:48.860 --> 02:17:50.484
The same as with TensorBoard.  
With that, I would like to 

02:17:50.485 --> 02:17:52.485
switch to 

02:17:53.737 --> 02:17:54.962
the laptop where you can see 
what the profileing tools look 

02:17:54.963 --> 02:17:56.963
like.

02:17:58.226 --> 02:18:00.897
Here it a trace from actually 
the very same input pipeline, or

02:18:00.898 --> 02:18:04.769
similar to the input pipeline I 
just showed.  And you can see 

02:18:04.770 --> 02:18:07.631
here that your step time graph, 
you have this tiny bit of orange

02:18:07.632 --> 02:18:13.544
at the bottom.  And this is the 
compute time on the cloud TPU.  

02:18:13.545 --> 02:18:14.562
And everything up here, it had 
blue, that's actually waiting 

02:18:14.563 --> 02:18:19.035
for data.  That's input pipeline
processing.  So, our TPU is 

02:18:19.036 --> 02:18:21.036
sitting idle and this 

02:18:22.096 --> 02:18:24.341
is telling you 92% of the  time.
Totally, totally not what we 

02:18:24.342 --> 02:18:26.371
want to be  doing.  So, let's 
dig in.

02:18:29.408 --> 02:18:31.225
Now, what I recommend  using -- 
we have a bunch of tools and 

02:18:31.226 --> 02:18:34.490
they're constantly improving and
getting better.  To really 

02:18:34.491 --> 02:18:36.525
understand what's going on 
underneath the hood, we're going

02:18:36.526 --> 02:18:38.526
to use the trace viewer.

02:18:40.205 --> 02:18:43.069
So, here I've loaded it up.  One
thing I should note, the trace 

02:18:43.891 --> 02:18:46.889
viewer is
designed for power users.  It 

02:18:46.890 --> 02:18:49.544
may be a little bit 
unapproachable.  Let me walk you

02:18:49.545 --> 02:18:51.582
through where to look in the 
trace viewer.  In the top, you 

02:18:51.583 --> 02:18:53.583
have the TPU.

02:18:54.851 --> 02:18:57.714
Now, one cloud TPU has eight 
compute cores.  And these 

02:18:57.715 --> 02:18:59.715
operate independently, although 
typically they're operating on 

02:19:01.213 --> 02:19:03.656
the same sorts of data, just in 
parallel.  So, these are on the 

02:19:03.657 --> 02:19:06.512
top you can see your step 
number.  You can see the 

02:19:06.513 --> 02:19:09.191
TensorFlow ops that it's 
executing, and finally with the 

02:19:09.192 --> 02:19:11.192
XLA ops.

02:19:13.458 --> 02:19:15.693
So, TPUs are programmed by XLA 
and you can see what's going on 

02:19:15.694 --> 02:19:20.581
underneath the hood.  Below 
that, the CPU compute threads.  

02:19:20.582 --> 02:19:23.030
These are the general TensorFlow
thread pool threads.  The  

02:19:23.031 --> 02:19:26.694
iterator thread.  And finally, a
set of threads for in-feed and 

02:19:26.695 --> 02:19:28.695
out-feed.

02:19:30.772 --> 02:19:33.622
These are managing DMAs to and 
from the TPU device.  Now, it's 

02:19:33.623 --> 02:19:35.866
a little hard to see at the top,
so, we're going to need to zoom 

02:19:35.867 --> 02:19:39.326
in.  We're going to need to look
in a little bit more depth.

02:19:42.388 --> 02:19:44.626
In order to navigate around 
within the  trace, the keyboard 

02:19:44.627 --> 02:19:47.297
shortcuts are the
most useful.  Keyboard shortcuts

02:19:47.298 --> 02:19:50.772
are from the left hand.  A and D
move left and right.  W and S 

02:19:50.773 --> 02:19:52.810
move in and out.  So, this is a 
little bit like the 

02:19:56.281 --> 02:19:57.705
arrow on like a key pad, just 
like with your left hand on the 

02:19:57.706 --> 02:20:01.367
home row.  There's a couple 
other keyboard shortcuts.  So, 

02:20:01.368 --> 02:20:02.581
for example, if you click on 
something, you can see the 

02:20:02.582 --> 02:20:05.620
details about what this is.  And
if you press F, you'll focus in 

02:20:05.621 --> 02:20:08.077
on just that sort of element of 
the timeline.

02:20:11.121 --> 02:20:12.949
So, you can zoom in and navigate
around really easily.  If you 

02:20:12.950 --> 02:20:16.830
want to see a little bit more 
about it, you can press M.  This

02:20:16.831 --> 02:20:18.831
marks it on the UI.

02:20:20.075 --> 02:20:22.104
You can see that our step time, 
our training step took 5.4 

02:20:22.105 --> 02:20:24.105
seconds.

02:20:25.174 --> 02:20:27.174
We go to the infeed queue, that 
was also 5.4 seconds.

02:20:30.069 --> 02:20:32.069
So, let's dig into what's going 
on the 

02:20:33.509 --> 02:20:35.976
CPU since the TPU is sitting 
idle  waiting for data.  There's

02:20:35.977 --> 02:20:40.488
a lot of things going on.  We 
need to zoom in a lot farther.  

02:20:40.489 --> 02:20:41.723
Not just at the second range, 
but down to the millisecond 

02:20:41.724 --> 02:20:45.377
range.  Here we can see each of 
those vertical bars, that's 5

02:20:45.378 --> 02:20:47.378
milliseconds, okay?

02:20:49.847 --> 02:20:52.296
And if we zoom in this far, we 
can see that our iterator is 

02:20:52.297 --> 02:20:55.558
running  continuously, and the 
map function is what's taking 

02:20:55.559 --> 02:20:59.028
the longest amount of time.  
Okay?  The map  function, 

02:20:59.029 --> 02:21:04.358
there's a bunch of other little 
ops that are happening here that

02:21:04.359 --> 02:21:06.359
are your batching or your 
repeating or whatnot.

02:21:08.834 --> 02:21:10.866
But the map function is the bulk
of the time.  That's are the 

02:21:10.867 --> 02:21:13.310
focus of the optimization 
efforts.  And the map function 

02:21:13.311 --> 02:21:15.311
runs the elements 

02:21:16.573 --> 02:21:18.573
of the map on your normal, 
standard TensorFlow thread pool.

02:21:20.244 --> 02:21:22.283
And look closely, we can zoom in
further, no two ops running at 

02:21:22.284 --> 02:21:27.160
the same  time.  We're using 
multiple threads in the thread 

02:21:27.161 --> 02:21:28.990
pool, this is processing single 
threaded.  That leads to the 

02:21:28.991 --> 02:21:30.991
first optimization.

02:21:32.242 --> 02:21:34.476
I'm going to switch back to the 
slides.  This is what you need 

02:21:34.477 --> 02:21:37.932
to do to use multiple threads 
for your input pipeline for your

02:21:37.933 --> 02:21:39.933
map function.

02:21:42.016 --> 02:21:44.871
Hit numb parallel calls to 64 
and you'll be using up to 64 

02:21:44.872 --> 02:21:46.872
threads, and 

02:21:48.942 --> 02:21:50.793
because cloud TPUs are hooked up
to a

02:21:50.794 --> 02:21:51.813
powerful machine, you can use 
all of these threads 

02:21:51.814 --> 02:21:53.814
concurrently.

02:21:55.916 --> 02:21:58.567
If you do this and rerun your 
model, you have a 4X 

02:21:58.568 --> 02:22:01.824
improvement, over 600 images a 
second.  That's pretty great.  

02:22:01.825 --> 02:22:04.660
But, we're not done.  An 
important part of the 

02:22:04.661 --> 02:22:07.542
performance methodology is step 
three.  Repeat.  You have to 

02:22:07.543 --> 02:22:12.022
repeat again.  So, we take a new
trace.  I'm not going to do it 

02:22:12.023 --> 02:22:15.507
live on the  laptop.  Because we
want to go through.  We have a 

02:22:15.508 --> 02:22:19.788
lot of stuff to cover.  We now 
see right here.  We have a lot 

02:22:19.789 --> 02:22:21.789
more compute threads 

02:22:22.843 --> 02:22:25.284
going on, but we're still very 
much input-bounder.  And if we 

02:22:25.285 --> 02:22:27.285
zoom in a lot, you can actually 
see that the bottom element 

02:22:29.011 --> 02:22:31.011
here, this tf.

02:22:32.098 --> 02:22:34.553
record, waiting for data to load
from the file system.  We 

02:22:34.554 --> 02:22:36.554
process things in parallel 
quickly 

02:22:37.599 --> 02:22:40.643
and take a while to transfer 
them to the device over PCIE.  

02:22:40.644 --> 02:22:42.644
This presents a pipelining 
opportunity.

02:22:43.900 --> 02:22:46.742
Now, to give you a bit of 
intuition for what I mean, input

02:22:46.743 --> 02:22:51.424
pipelines you should mentally 
associate with ETL.  Extract is 

02:22:51.425 --> 02:22:54.928
the first phase where you
load the data from storage.  

02:22:54.929 --> 02:22:56.929
Transform phases to prepare for 
training.

02:22:59.163 --> 02:23:01.163
And finally load into the 
accelerator.

02:23:03.021 --> 02:23:03.836
Not just in the API, but a 
useful mental model for 

02:23:03.837 --> 02:23:09.369
performance.  To give you a bit 
of intuition for that, each of 

02:23:09.370 --> 02:23:11.660
the different phases of ETL use 
different hardware components in

02:23:12.275 --> 02:23:14.275
your server system.

02:23:16.140 --> 02:23:18.178
The extract phase is  
emphasizing the disk in the 

02:23:18.179 --> 02:23:20.216
storage system or the network 
link if you're from a remote 

02:23:20.833 --> 02:23:22.833
storage system.

02:23:23.870 --> 02:23:27.952
Transform typically happens on 
CPU and it's  CPU-hungry.  And 

02:23:27.953 --> 02:23:30.809
your load phase is emphasizing 
the DMA, your connections to 

02:23:30.810 --> 02:23:32.810
your accelerator.

02:23:35.894 --> 02:23:37.112
This is true with a GPU, TPU or 
any other accelerators you might

02:23:37.113 --> 02:23:41.797
be using.  And so what's going 
on is if you map this out over 

02:23:41.798 --> 02:23:44.036
time, you're extracting and 
while you're extracting, you're 

02:23:45.675 --> 02:23:48.307
doing nothing with the  CPU.  
And during a transform phase, 

02:23:48.308 --> 02:23:51.370
doing nothing with the 
connection between the CPU 

02:23:51.371 --> 02:23:54.020
memory and the accelerator.  And
training, the entire CPU and the

02:23:55.249 --> 02:23:56.673
rest of the machine is sitting 
idle. 

02:23:56.674 --> 02:23:58.674
This is incredibly wasteful.

02:23:59.932 --> 02:24:01.568
Because they're all using 
different components in the 

02:24:01.569 --> 02:24:03.569
system, you can actually overlap
all of this in a 

02:24:06.697 --> 02:24:08.697
technique called software 
pipelining.

02:24:10.763 --> 02:24:13.409
So, you are extracting for step 
five, transforming for step 

02:24:13.410 --> 02:24:15.410
four, loading data for step 
three and training for step two.

02:24:17.292 --> 02:24:19.934
This is an efficient use of your
compute resources.  You can 

02:24:19.935 --> 02:24:21.968
train faster.  You'll notice in 
a well-pipelined 

02:24:25.642 --> 02:24:27.642
model, your accelerator will be 
100% utilized.

02:24:28.683 --> 02:24:29.897
But it's possible that your CPU 
or disk will be a little bit 

02:24:29.898 --> 02:24:31.898
idle.  That's okay.

02:24:35.230 --> 02:24:36.650
Your accelerator is typically 
your most protecter resource.  

02:24:36.651 --> 02:24:38.651
That's the bottleneck.

02:24:39.937 --> 02:24:41.357
It's okay if the others are 
slightly faster than you need 

02:24:41.358 --> 02:24:43.358
them to be.

02:24:45.062 --> 02:24:47.062
How do you enable software 
pipelining with datasets?

02:24:50.346 --> 02:24:52.346
Set numb Juried parallel Juried 
reads to 32.

02:24:53.609 --> 02:24:55.609
And it's using party interleaf.

02:24:59.296 --> 02:25:02.562
A key dataset transformation 
that anaI believes pipelining.  

02:25:02.563 --> 02:25:04.563
And you can set the prefetches 
right at the end.

02:25:06.011 --> 02:25:07.029
And that means everything is  
pipelined with

02:25:07.030 --> 02:25:09.263
everything below.  Your 
extraction is pipelined from 

02:25:09.264 --> 02:25:11.264
your loading into your 
accelerator.

02:25:14.182 --> 02:25:16.182
Now, one thing I want to 
mention, when 

02:25:19.490 --> 02:25:21.490
you net numb numb_parallel_reads
is 

02:25:23.416 --> 02:25:26.072
equal to 32, you're using 
parallel_reads.  We have 

02:25:26.073 --> 02:25:27.295
conflated these in the API 
because we believe that 

02:25:27.296 --> 02:25:30.341
distributed storage is critical 
going forward for machine 

02:25:30.342 --> 02:25:34.005
learning  workloads.  Why is 
that?  As we have the research, 

02:25:34.006 --> 02:25:36.247
we see that datasets are going 
to become larger and larger over

02:25:36.248 --> 02:25:39.900
time.  So, you're really going 
to  need -- they just won't fit 

02:25:39.901 --> 02:25:41.901
on a single machine.  You need 
to distribute them across a 

02:25:42.129 --> 02:25:44.129
cluster.

02:25:46.004 --> 02:25:47.832
Additionally, when you have data
disaggregated from your 

02:25:47.833 --> 02:25:49.833
accelerator 

02:25:51.282 --> 02:25:52.937
nodes, it means you can more 
efficiently share your 

02:25:52.938 --> 02:25:55.801
accelerator nodes.  If you're 
training on them today or 

02:25:57.230 --> 02:25:59.896
tomorrow and in three minutes 
someone else wants to train, 

02:25:59.897 --> 02:26:03.978
you're not  copying datasets 
around.  It's easier to use.  

02:26:03.979 --> 02:26:06.842
And finally, it makes it a lot 
nicer doing large-scale 

02:26:06.843 --> 02:26:08.843
hyperparameter searches.  You 
have one

02:26:11.510 --> 02:26:13.347
cluster and a fungible pool of 
resources.  We believe that 

02:26:13.348 --> 02:26:14.795
districted resources are 
important and we have  worked 

02:26:14.796 --> 02:26:19.063
hard to make that fast with tf.
data.  So, what happens when you

02:26:19.064 --> 02:26:21.064
do this?

02:26:22.115 --> 02:26:24.355
It turns out, the cloud TPU, 
you'll get over 1700 images a 

02:26:24.356 --> 02:26:26.356
second.  With these 
optimizations.

02:26:28.636 --> 02:26:31.113
So, we're now about 12 times 
faster than our initial input 

02:26:31.114 --> 02:26:34.594
pipeline with less than 60 
characters worth of typing.  So,

02:26:34.595 --> 02:26:39.877
that's pretty good.  But we can 
do better.  If you capture the 

02:26:39.878 --> 02:26:44.996
trace, you'll actually see that 
our transform step is slightly 

02:26:44.997 --> 02:26:47.427
longer than our accelerator 
trading model time.  The TPU is 

02:26:47.428 --> 02:26:50.083
just too fast.  And we need to 
break out some advanced 

02:26:51.703 --> 02:26:55.362
optimization techniques that are
available today.  One of the 

02:26:55.363 --> 02:26:57.363
most powerful ones is to 

02:26:58.618 --> 02:27:00.618
use these fuse dataset 
operators, map 

02:27:01.682 --> 02:27:03.304
and batch, shuffle and repeat, 
fusing together the operations 

02:27:03.305 --> 02:27:08.643
to improve performance on your 
CPU.  Tf.data works hard to 

02:27:08.644 --> 02:27:10.885
ensure that the elements 
produced out of your dataset by 

02:27:13.351 --> 02:27:15.751
your iterator are in a 
deterministic

02:27:13.351 --> 02:27:18.028
order.  But if you give tf.data 
the opportunity to reorder, we 

02:27:18.029 --> 02:27:20.678
can enable performance 
optimizations.  We can use 

02:27:20.679 --> 02:27:22.679
sloppy interleave 

02:27:23.935 --> 02:27:25.991
underneath the hood, working 
around variability in similar 

02:27:25.992 --> 02:27:30.689
storage systems.  There's other 
tweaks you can do.  And apply 

02:27:30.690 --> 02:27:32.738
them together in this optimized 
input pipeline, we get over 

02:27:35.787 --> 02:27:37.787
2,000 images a second and we are
now accelerator bound.

02:27:41.701 --> 02:27:45.836
Here is TensorBoard, and 
everything is 100% orange.  We 

02:27:45.837 --> 02:27:48.488
are entirely accelerator bound 
and it's churning 100% of the 

02:27:48.489 --> 02:27:51.760
time.  This is great.  We can 
now start looking into 

02:27:54.217 --> 02:27:57.593
optimizations that we can do on 
the TPU to make the TPU faster. 

02:27:57.594 --> 02:28:00.269
We can see that our CPU is idle.
We can see that there's some 

02:28:00.270 --> 02:28:02.270
overhead, reshape and copy that 
we might think 

02:28:05.989 --> 02:28:07.989
about optimizing away with some 
device-specific optimizations.

02:28:09.851 --> 02:28:11.685
Which brings me to phase two.  
Now, as I mentioned before, 

02:28:11.686 --> 02:28:13.686
we're in 

02:28:15.172 --> 02:28:17.442
this sort of Cambrian explosion.
We're still in the early days, 

02:28:17.443 --> 02:28:21.303
we're finding that a lot of the 
accelerators work differently.  

02:28:21.304 --> 02:28:24.190
Some chips
are  smaller, some chips are 

02:28:24.191 --> 02:28:26.191
bigger, 

02:28:28.673 --> 02:28:32.736
some chips use HBM- HBM-2, for 
example, TPUs and CPUs.  Some do

02:28:32.737 --> 02:28:34.737
away with that entirely, 

02:28:35.811 --> 02:28:38.663
Graphcore's IPU and optimized 
for communication.  And it's 

02:28:38.664 --> 02:28:39.899
hard to provide out of the box 
performance recommendations that

02:28:39.900 --> 02:28:42.567
are going to apply to all of 
these different hardware  

02:28:42.568 --> 02:28:47.659
platforms.  That said, there's a
few common things if we peer 

02:28:47.660 --> 02:28:49.660
into the future, gaze into 

02:28:51.310 --> 02:28:52.329
our crystal balls, we think this
is what we're going to see more 

02:28:52.330 --> 02:28:54.330
of.

02:28:55.592 --> 02:28:58.046
What I expect to see a lot of is
interesting numerical formats.

02:29:01.739 --> 02:29:03.739
This is a 32-bit format we know 
and love.

02:29:05.836 --> 02:29:07.836
Most models today have been 
trained in  fp32.

02:29:11.544 --> 02:29:13.544
There is great work at NVIDIA 
and 

02:29:18.727 --> 02:29:20.972
Baidu, and you can train an 
fp16, pf32, but the activations 

02:29:20.973 --> 02:29:25.475
in fp16.  This is a big win on 
two dimensions.  You can run a 

02:29:25.476 --> 02:29:29.344
larger model because more layers
fits in memory.  But 

02:29:29.345 --> 02:29:31.345
additionally, your model tends 
to 

02:29:32.624 --> 02:29:34.908
run faster, because accelerators
today, GPUs and TPUs are not 

02:29:34.909 --> 02:29:36.909
compute-bound.

02:29:40.836 --> 02:29:42.836
They're actually memory 
bandwidth bound. 

02:29:43.118 --> 02:29:45.118
The memory is too slow.

02:29:48.018 --> 02:29:50.064
So fp16 can unlock a lot of 
great performance on devices.

02:29:54.935 --> 02:29:59.237
But one other floating point 
format, there's bfloat 16.  And 

02:29:59.238 --> 02:30:01.678
this is different than fp16.  
Even though it uses just 16 

02:30:01.679 --> 02:30:06.937
bits.  The range is us same as 
fp32.  You don't worry as much 

02:30:06.938 --> 02:30:08.938
about 

02:30:09.954 --> 02:30:12.430
vanishing or exploding gradients
and NANs that you might when 

02:30:12.431 --> 02:30:16.446
using fp16.  There are a number 
of numerical formats such as 

02:30:16.447 --> 02:30:18.447
flex point.

02:30:24.933 --> 02:30:28.809
Folks from Intel parenned at 
NIPS is in a poster section.  We

02:30:28.810 --> 02:30:31.685
are going to make it easier to 
use the numerical floating 

02:30:31.686 --> 02:30:33.686
formats.

02:30:35.154 --> 02:30:37.611
Stay tuned for APIs in this 
space.  Another hardware trend 

02:30:37.612 --> 02:30:39.612
is optimization 

02:30:40.867 --> 02:30:42.867
nor matrix optimization.  
Especially in this mode.

02:30:48.382 --> 02:30:50.382
In Volta GPUs, they have tensor 
cores with.

02:30:53.715 --> 02:30:55.715
TPUs are built around a 128X128 
matrix unit.

02:30:57.375 --> 02:30:59.375
This is a systolic array, this 
tran 

02:31:01.855 --> 02:31:03.855
sisser configs ration that makes
it easy 

02:31:05.085 --> 02:31:07.085
to compute convolutions.

02:31:09.795 --> 02:31:12.459
And here is showing a systolic 
array does.  It's named after 

02:31:12.460 --> 02:31:14.507
the heart, which is pumping 
blood in these cycles.

02:31:17.758 --> 02:31:19.830
Plumbs
 through the data and you get 

02:31:19.831 --> 02:31:23.094
fast matrix multiplication.  
What this means, because we're 

02:31:23.095 --> 02:31:25.095
seeing 

02:31:26.358 --> 02:31:28.595
hardware-supported matrix 
multiplication at different 

02:31:28.596 --> 02:31:30.472
sizes and scales, the way you 
lay out the data and implement 

02:31:30.473 --> 02:31:34.735
it, can make a huge difference 
on performance on these 

02:31:34.736 --> 02:31:36.736
accelerators.

02:31:37.938 --> 02:31:39.971
Here I'm calling out two 
different models running on 

02:31:39.972 --> 02:31:41.972
GPUs.

02:31:43.666 --> 02:31:45.298
You use channels last, you end 
up losing a fair bit of 

02:31:45.299 --> 02:31:47.976
performance compared to a 
channels-first implementation.

02:31:51.260 --> 02:31:53.533
If you compare different LSTM 
cell implementations, the folk 

02:31:53.534 --> 02:31:56.214
the at NVIDIA worked really hard
to make really fast  

02:32:00.475 --> 02:32:01.094
kernels for LSTMs and make it 
available as part of the 

02:32:01.095 --> 02:32:03.095
package.

02:32:04.546 --> 02:32:07.033
If you're using a GPU and want 
better performance, use the 

02:32:07.034 --> 02:32:11.309
optimized libraries for your 
platform.  This is true.  You 

02:32:11.310 --> 02:32:13.310
need to use the latest version 
of TensorFlow.

02:32:16.021 --> 02:32:18.684
We're constantly working on 
performance improvements, and 

02:32:18.685 --> 02:32:23.808
cuDNN and Intel MKL we talked 
about today.  And investigate 

02:32:23.809 --> 02:32:26.044
16-bit numerical 
representations.  We see a lot 

02:32:26.045 --> 02:32:28.045
of potential
performance advantages there.

02:32:30.766 --> 02:32:32.722
And doing inference, we have 
talked about TensorFlow RT, 

02:32:32.723 --> 02:32:34.723
which is available 

02:32:36.193 --> 02:32:38.270
for NVIDIA platforms that can 
quantize and make inference 

02:32:38.271 --> 02:32:43.150
really fast.  And as part of the
technique, if you see, for 

02:32:43.151 --> 02:32:45.600
example, that a particular 
computation is a bottleneck, you

02:32:45.601 --> 02:32:49.110
can substitute it with 
somethings that computationally 

02:32:49.111 --> 02:32:51.111
faster.

02:32:52.602 --> 02:32:53.613
You have Tor careful because you
may change the quality of the 

02:32:53.614 --> 02:32:56.707
model doing this.  With that, I 
would like to move on to phase 

02:32:56.708 --> 02:33:01.432
three.  Now, typically, when you
use an accelerator, you're 

02:33:01.433 --> 02:33:04.507
actually using more than one 
accelerator.  You're using a 

02:33:04.508 --> 02:33:06.345
single device, it may have 
different components that 

02:33:06.346 --> 02:33:08.346
operate in parallel.

02:33:11.646 --> 02:33:13.646
Here is, for example, a picture 
of the 

02:33:15.517 --> 02:33:17.517
NVIDIA DGX-1 and shows the 
connectivity between the GPUs.

02:33:21.438 --> 02:33:24.302
You have two of four each with 
connectivity between them.  If 

02:33:24.303 --> 02:33:26.303
you don't take advantage of the 

02:33:28.781 --> 02:33:30.781
topology, if you do a naive 
gradient 

02:33:34.503 --> 02:33:36.941
within the server going via the 
CPU or TCIP switches, it will be

02:33:36.942 --> 02:33:38.942
a significant

02:33:40.624 --> 02:33:42.655
disadvantage due to a clever 
utilization by nickel and CCL-2.

02:33:45.917 --> 02:33:47.958
We have an optimizeed 
implementation available as part

02:33:47.959 --> 02:33:49.959
of the benchmarks, but 

02:33:51.018 --> 02:33:52.644
it's a little tricky to use.  We
are working on making this easy 

02:33:52.645 --> 02:33:56.702
for everyone to use in 
distribution strategies.  And 

02:33:56.703 --> 02:33:59.542
you'll hear more about 
distribution strategies in just 

02:33:59.543 --> 02:34:01.543
a few minutes.

02:34:02.813 --> 02:34:04.813
TPUs, you also need to carefully
aggregate your gradients.

02:34:08.313 --> 02:34:10.313
And we have the cross John 
Boehner shoredoptimizer.

02:34:12.189 --> 02:34:14.189
You take the existing SGD and 
just 

02:34:17.428 --> 02:34:19.428
wrap it with a TPU cross
shardoptimizer.

02:34:20.581 --> 02:34:21.606
This will aggregate across the 
compute shards within a single 

02:34:21.607 --> 02:34:23.875
device.  But the exact same code
works all the 

02:34:27.376 --> 02:34:30.035
way up to a whole cloud TPU pod 
across 64 different devices.

02:34:33.125 --> 02:34:34.356
Now, I want to take one moment 
to actually talk a little bit 

02:34:34.357 --> 02:34:38.026
about measuring performance.  I 
guess the saying goes, there's 

02:34:38.027 --> 02:34:40.027
lies, damn lies and statistics.

02:34:43.567 --> 02:34:45.567
Well, I'm going do add a fourth 
one.  Performance benchmarks.

02:34:48.870 --> 02:34:50.098
The Internet is pleat with 
shoddy benchmarks and 

02:34:50.099 --> 02:34:52.979
misinformation.  And
this irks me to no end.

02:34:56.645 --> 02:34:58.645
We have seen benchmarks, they 
use 

02:34:59.914 --> 02:35:02.161
synthetic data or measuring only
certain subsets.  You have 

02:35:02.162 --> 02:35:04.389
incomplete comparisons.  One 
benchmark is comparing the full 

02:35:04.390 --> 02:35:08.294
device.  One is comparing only 
one part of the device.  We've 

02:35:08.295 --> 02:35:11.142
seen bugs in the machine 
learning where they've optimized

02:35:11.143 --> 02:35:15.023
a way, or done performance 
tricks that make it run faster, 

02:35:15.024 --> 02:35:17.269
but actually make it not 
converge to the same accuracy.

02:35:20.522 --> 02:35:22.522
You've lost quality of your 
model.

02:35:23.986 --> 02:35:26.618
Additionally, as we look 
forward, this is actually, to be

02:35:26.619 --> 02:35:28.867
fair, a nuanced space.  And 
hardware is harder and harder to

02:35:30.899 --> 02:35:32.899
give an apples to apples 
comparison.

02:35:34.374 --> 02:35:36.616
We have different numerical 
formats and different algorithms

02:35:36.617 --> 02:35:39.297
fit better on different 
hardware.  Some chips have small

02:35:39.298 --> 02:35:45.399
amounts of memory.  If you have 
a very, very big model that 

02:35:45.400 --> 02:35:48.050
can't fit, that's a very unfair 
comparison.  As a result, I 

02:35:48.051 --> 02:35:49.878
strongly encourage you, if 
you're trying to choose and 

02:35:49.879 --> 02:35:51.935
evaluate different hardware 
platforms, take your 

02:35:59.476 --> 02:36:00.702
workloads and measure them end 
to end

02:36:00.703 --> 02:36:02.368
to the accuracy you want in the 
application.  That's a fair 

02:36:02.369 --> 02:36:04.369
amount of work.

02:36:05.412 --> 02:36:07.412
If you can't run your own 
workloads, 

02:36:08.500 --> 02:36:09.524
look to quality end to end 
benchmarks that measure 

02:36:09.525 --> 02:36:13.221
accuracy.  And I think a good 
example is Stanford's DAWNBench.

02:36:15.657 --> 02:36:17.657
There's nuance in the parameters
of how they're set.

02:36:21.553 --> 02:36:24.009
For example the dataset size, a 
small dataset size or a large 

02:36:24.010 --> 02:36:26.858
dataset size.  And there's 
nuance in how to set the 

02:36:27.877 --> 02:36:29.877
accuracy threshold.

02:36:30.942 --> 02:36:33.388
Despite these, it's a lot harder
to perform well on an end to end

02:36:33.389 --> 02:36:38.506
benchmark and not work on real 
work.  You're less likely to be 

02:36:38.507 --> 02:36:40.741
mislead looking at the end to 
end benchmarks.

02:36:44.221 --> 02:36:46.476
That said, we're pushing for the
end- end-to-end benchmarks, 

02:36:46.477 --> 02:36:48.477
there's a lot of 

02:36:49.951 --> 02:36:52.043
utility in microbenchmarks to 
understand how fast are the 

02:36:52.044 --> 02:36:54.044
components.

02:36:58.567 --> 02:37:00.567
When I was optimizeing ResNet50 
for 

02:37:02.256 --> 02:37:04.256
the cloud TPU case, how did I 
know it was slow?

02:37:06.119 --> 02:37:08.356
As Derek mentioned, input 
pipelines can have over  13,000 

02:37:08.357 --> 02:37:13.447
images a second.  That's using 
VDG pre-processing.  This 

02:37:13.448 --> 02:37:15.448
pre-processing is  
computationally

02:37:17.535 --> 02:37:19.766
cheaper than the ResNet on 
Inception pre-processing, but 

02:37:19.767 --> 02:37:21.767
shows we can go really, really 
fast.

02:37:26.674 --> 02:37:28.674
ResNet50 on a

02:37:30.836 --> 02:37:34.301
DGS-1, this is using a mixed 
precision 16.  This is have 

02:37:34.302 --> 02:37:36.345
TensorFlow nightly.  This is the
performance you can expect in 

02:37:36.346 --> 02:37:40.396
the  future.  Now, if you want 
to test the performance of the 

02:37:40.397 --> 02:37:44.460
GPUs themselves in isolation, 
that's about 6.1 synthetic 

02:37:44.461 --> 02:37:47.553
images a second.  You are 
excluding the cost of your input

02:37:47.554 --> 02:37:49.554
pipeline.

02:37:51.024 --> 02:37:54.473
For a cloud TPU we have a few 
other microbenchmarks.  For 

02:37:54.474 --> 02:37:56.713
TensorFlow 1  1.7 that's 
available today, you can expect 

02:37:59.984 --> 02:38:01.984
to achieve 2 .

02:38:05.873 --> 02:38:07.873
6-5,000 images a second, mixed 
flow B16.

02:38:09.150 --> 02:38:11.150
You're streaming the data in 
with a 

02:38:13.252 --> 02:38:17.158
batch size of  32, and get over 
76% accuracy in about 13 hours. 

02:38:17.159 --> 02:38:19.191
If you lop off the input 
pipeline and just test the 

02:38:19.192 --> 02:38:21.229
device performance, you're 
actually over 3200 images a 

02:38:21.230 --> 02:38:24.123
second, which is very cool with 
TensorFlow 1.7.

02:38:27.400 --> 02:38:29.400
And with TensorFlow nightly, 
coming in TensorFlow  1.

02:38:31.268 --> 02:38:33.503
8, we have optimized the input 
pipeline performance, and you go

02:38:33.504 --> 02:38:35.504
from 2600

02:38:38.079 --> 02:38:40.731
images a second to over 300 
image 0 images a second in 

02:38:40.732 --> 02:38:43.774
TensorFlow 1.8.  Very exciting. 
A lot of work happening 

02:38:43.775 --> 02:38:46.044
underneath the hood.  As we 
stare into the future even 

02:38:50.476 --> 02:38:54.079
further, what is coming in 
TensorFlow with performance?  

02:38:54.080 --> 02:38:56.315
This is actually our optimized 
input pipeline, or something 

02:38:56.316 --> 02:38:59.177
very close to it, and you'll 
notice there's a lot of magic 

02:39:01.224 --> 02:39:02.637
numbers we have hand-tuned and 
picked out.  How did we choose 

02:39:02.638 --> 02:39:06.948
them?  We spent a lot of time 
playing around with it and 

02:39:06.949 --> 02:39:10.208
picking them out.  Do you need 
to do that?  There's no reason 

02:39:10.209 --> 02:39:12.474
you need to do that.  We can 
autotune a lot of these values 

02:39:15.320 --> 02:39:17.320
and we're working on adding in 
smarts to 

02:39:19.266 --> 02:39:21.507
TensorFlow to tune your 
pipelines.  This is true not 

02:39:21.508 --> 02:39:23.508
just for the magic 

02:39:26.005 --> 02:39:28.664
numbers, but for these fused  
dataset operations functions, 

02:39:28.665 --> 02:39:30.665
we'll be working 

02:39:31.742 --> 02:39:32.969
on switching a naive 
straightforward implementation 

02:39:32.970 --> 02:39:35.643
to use the functions underneath 
the hood.  If you give us 

02:39:35.644 --> 02:39:37.644
permission, we'll be 

02:39:38.916 --> 02:39:40.133
able to adjust things where 
we're not preserving necessarily

02:39:40.134 --> 02:39:42.134
the order,

02:39:43.198 --> 02:39:45.198
but we can do the right things 
for you.

02:39:46.472 --> 02:39:48.714
Automatically  tuning the 
prefetch buffer size, that last 

02:39:48.715 --> 02:39:53.425
line, that's available and 
coming in TensorFlow 1.8.  We're

02:39:53.426 --> 02:39:55.426
not just working on optimizing 

02:39:57.279 --> 02:39:59.279
the input  pipelines, we're 
working on 

02:40:00.792 --> 02:40:02.792
optimizing in-device 
performance.

02:40:07.476 --> 02:40:09.810
There's XLA and grab grappler.  
And they're rewriting the model 

02:40:09.811 --> 02:40:14.313
to work well on different 
platforms.  There's exciting 

02:40:14.314 --> 02:40:15.943
work here that we will be 
excited to share with you over 

02:40:15.944 --> 02:40:17.944
time.

02:40:18.996 --> 02:40:21.026
There's a lot more reading and 
there's a huge amount of 

02:40:21.027 --> 02:40:23.474
literature on this.  Check out 
some of these things.

02:40:27.585 --> 02:40:29.618
If you want to learn about 
reduced precision  training, 

02:40:29.619 --> 02:40:34.539
here are references.  And I'll 
Tweet out this link shortly.  

02:40:34.540 --> 02:40:37.014
They'll be available and you can
load them up into TensorBoard 

02:40:37.015 --> 02:40:40.102
and play around with them 
yourself.  With that, thank you 

02:40:40.103 --> 02:40:42.103
very much for listening to me 
today.

02:40:43.826 --> 02:40:45.826
[ Applause ]

02:40:48.093 --> 02:40:50.093
Next up is Mustafa who is going 
to talk 

02:40:51.754 --> 02:40:53.754
about TensorFlow's high-level 
APIs.

02:40:54.617 --> 02:40:56.617
&gt;&gt; Thank you, Brennan.  Hello, 
everybody.

02:40:57.278 --> 02:40:59.278
[ Applause ]

02:41:00.557 --> 02:41:02.557
My name is Mustafa.

02:41:04.667 --> 02:41:06.667
Today I'll talk about  
high-level 

02:41:07.757 --> 02:41:09.757
APIs, but keep practitioners 
like you in mind.

02:41:12.654 --> 02:41:14.890
To keep you in mind, we'll 
provide an example to increase 

02:41:14.891 --> 02:41:16.891
user happiness by the power of 
machine learning.

02:41:21.018 --> 02:41:23.018
After defining the example 
project, 

02:41:24.081 --> 02:41:27.518
we'll use pre-made estimators to
start our first experiment.  

02:41:27.519 --> 02:41:29.519
Then we'll experiment more with 
every 

02:41:30.795 --> 02:41:33.627
feature we have by feature 
columns.  And we'll introduce a 

02:41:33.628 --> 02:41:35.628
couple of pre-made estimators 
that you can experiment more.

02:41:38.577 --> 02:41:40.211
And we learn, how can you 
experiment with other modeling 

02:41:40.212 --> 02:41:42.212
ideas too?

02:41:46.722 --> 02:41:48.547
So, those are the topics we will
cover in this talk, and talk 

02:41:48.548 --> 02:41:51.605
about how to scale it up and how
you can use it in your 

02:41:51.606 --> 02:41:56.104
production.  Let's talk about 
estimators.  So, it's a library 

02:41:56.105 --> 02:41:58.558
that lets you focus on your 
experiment.  There are thousands

02:41:58.559 --> 02:42:00.559
of engineers.

02:42:01.612 --> 02:42:03.612
This is not a small number.

02:42:05.922 --> 02:42:08.774
And hundreds of projects in 
Google who use estimators.  So, 

02:42:08.775 --> 02:42:10.775
we learned a lot from their 
experiments.

02:42:12.872 --> 02:42:15.527
And we created our APIs so that 
the time from an idea to an

02:42:17.617 --> 02:42:19.617
experiment will be as short as 
possible.

02:42:20.879 --> 02:42:22.879
So -- and I'm really happy to 
share 

02:42:23.946 --> 02:42:25.985
all this experience with all of 
you.  Whatever we are using 

02:42:25.986 --> 02:42:27.986
internal at 

02:42:29.043 --> 02:42:31.043
Google is the same as the open 
source.

02:42:32.739 --> 02:42:35.455
So, you all have the same 
things.  Estimator keeps the 

02:42:35.456 --> 02:42:37.456
model function.  We'll talk 
about what the model 

02:42:39.753 --> 02:42:42.202
function is later, but it 
defines your network and how can

02:42:42.203 --> 02:42:44.203
you train or what is 

02:42:45.267 --> 02:42:46.074
the behavior during the 
evaluation or during the export 

02:42:46.075 --> 02:42:48.075
time?

02:42:49.555 --> 02:42:52.037
And it provides you some loops 
such as training, evaluation, 

02:42:52.038 --> 02:42:54.038
and it provides 

02:42:57.970 --> 02:43:00.424
you some interface to integrate 
with tf.  Also, estimator keeps 

02:43:00.425 --> 02:43:02.425
sessions so you 

02:43:03.507 --> 02:43:05.507
don't need to learn what is  tf.
session.  It handles it for you.

02:43:08.037 --> 02:43:10.037
But you need to provide data.

02:43:12.557 --> 02:43:14.998
And as Derek mentioned, you can 
return a tf.data set from your 

02:43:14.999 --> 02:43:17.683
input function.  So, let's 
define our project and start 

02:43:18.294 --> 02:43:21.762
with our experiment.  I love 
hiking.  This is one of the 

02:43:21.763 --> 02:43:23.763
pictures I took in one of my 
hikes.

02:43:26.018 --> 02:43:28.018
And let's imagine there's a 
website, 

02:43:29.900 --> 02:43:31.961
hiking website, similar to IMDB,
but it's for hiking.

02:43:33.813 --> 02:43:36.660
And that website has information
for each hike, and users are 

02:43:36.661 --> 02:43:40.538
labeling those hikes by saying, 
I like this hike, I don't like 

02:43:40.539 --> 02:43:44.037
this hike, this is my rating, 
and all this stuff.  And we want

02:43:44.038 --> 02:43:47.528
to use this data.  Let's imagine
you have this data from that 

02:43:47.529 --> 02:43:49.529
website.

02:43:50.594 --> 02:43:52.594
To recommend hikes for users.  
How can we do that?

02:43:54.887 --> 02:43:56.887
There are many ways of doing it.

02:43:58.137 --> 02:44:00.137
Let's define one way machine 
learning can help us.

02:44:03.236 --> 02:44:05.236
In this case, we want to predict
probability of like.

02:44:07.100 --> 02:44:09.100
Whether a given user will like a
given hike or not.

02:44:14.467 --> 02:44:16.923
What you have, you have hike 
features and user feature 

02:44:16.924 --> 02:44:21.643
features.  And where can you 
learn from?  The label data, you

02:44:21.644 --> 02:44:23.644
can have whether users like that
hike or not.

02:44:26.750 --> 02:44:28.750
So, whack we use to predict if 
they like a hike?

02:44:30.283 --> 02:44:32.283
You can use one of the pre-made 
estimators.

02:44:34.188 --> 02:44:36.188
In this case, the  pre-made 
estimator.

02:44:40.167 --> 02:44:42.167
It's a binary estimation 
problem.

02:44:43.636 --> 02:44:45.636
We designed estimators to go to 
this kind of problem.

02:44:48.143 --> 02:44:50.143
This means you can use it as a 
black box solution.

02:44:52.022 --> 02:44:54.022
Pre-made estimators are 
surprisingly

02:44:54.456 --> 02:44:59.991
popular within Google and in 
many projects.  Why?  The 

02:44:59.992 --> 02:45:01.266
engineers are using pre-made 
solutions instead of building 

02:45:01.267 --> 02:45:03.519
their own models.

02:45:06.989 --> 02:45:09.258
I think, first of  all, it 
works.  It handles many 

02:45:09.259 --> 02:45:11.259
implementations so you can focus
on your experiment.

02:45:14.784 --> 02:45:17.046
It has reasonable defaults for 
initialization, partitioning, or

02:45:18.687 --> 02:45:20.760
optimization so you have a 
reasonable baseline as quick as 

02:45:20.761 --> 02:45:26.045
possible.  And it is easy to 
experiment with new features.  

02:45:26.046 --> 02:45:28.046
So, we learn about that.

02:45:29.943 --> 02:45:31.943
You can experiment with all of 
your 

02:45:33.223 --> 02:45:36.118
data by using the same estimator
without changing it.  So, let's 

02:45:36.119 --> 02:45:38.119
jump into our first 

02:45:39.216 --> 02:45:41.216
experiment so that we can have a

02:45:42.263 --> 02:45:45.150
baseline that we will improve.  
I will talk about  it, but in 

02:45:45.151 --> 02:45:47.151
this 

02:45:52.048 --> 02:45:54.048
case, you are useing hike

02:45:55.835 --> 02:45:57.835
_id, it might be hike name, as 
an identification to your model.

02:46:00.777 --> 02:46:02.633
And say you have hidden_units 
one.  What this will learning 

02:46:02.634 --> 02:46:04.711
with it will learn the label for
each hike idea.

02:46:07.783 --> 02:46:09.783
That may be a good baseline for 
your overall progress.

02:46:11.248 --> 02:46:12.474
You need to say what is your 
evaluation data, what is your

02:46:12.475 --> 02:46:14.475
training data?

02:46:16.770 --> 02:46:18.811
Then you can call train and 
evaluate.  Just by this couple 

02:46:18.812 --> 02:46:20.812
of lines of code, you should be 
able to experiment.

02:46:25.393 --> 02:46:29.091
And you can see the results on 
the TensorBoard.  For example, 

02:46:29.092 --> 02:46:31.949
you can see training and 
evaluation.  Or how the metric 

02:46:31.950 --> 02:46:36.273
is moving.  Since this is a 
classification problem, you will

02:46:36.274 --> 02:46:38.274
see accuracy metric.

02:46:41.235 --> 02:46:44.297
And this is a binary estimation,
you will see that.  All of these

02:46:44.298 --> 02:46:49.196
things are free and ready to be 
used.  Let's experiment more.  

02:46:49.197 --> 02:46:51.826
Let's start with the data.  
Experimenting with the data 

02:46:51.827 --> 02:46:55.297
itself.  The design feature 
columns with the same mind-set.

02:46:59.373 --> 02:47:02.210
We want to make it -- make it 
easy to experiment with your 

02:47:02.211 --> 02:47:04.211
features, with your data.

02:47:06.061 --> 02:47:08.114
And based on our experience -- 
internal experience -- it 

02:47:08.115 --> 02:47:10.781
reduces the lines of code and 
may improve the model.

02:47:14.063 --> 02:47:16.098
There are a bunch of 
transformations you can handle 

02:47:16.099 --> 02:47:18.099
via feature columns.

02:47:19.409 --> 02:47:21.409
These are bucketing, crossing, 
hashing and embedding.

02:47:24.107 --> 02:47:25.934
Each of these needs a careful 
explanation.  Unfortunately I 

02:47:25.935 --> 02:47:27.935
don't have enough time

02:47:30.836 --> 02:47:32.859
here, but you can check Magnus's
tutorial and the video, they are

02:47:32.860 --> 02:47:36.952
very good.  Let's experiment 
with all the hike features we 

02:47:36.953 --> 02:47:38.953
have.

02:47:40.231 --> 02:47:41.680
Each hike may have text such as 
kid friendly, dog  friendly, 

02:47:41.681 --> 02:47:43.681
birding.

02:47:46.399 --> 02:47:48.878
You may choose indicator column 
instead of embedding column in 

02:47:48.879 --> 02:47:54.411
this case because you don't have
a huge number of tags.  You 

02:47:54.412 --> 02:47:57.275
don't need the dimension.  And 
for a numerical column, such as 

02:47:59.911 --> 02:48:02.767
each hike may have elevation 
gain, you need to normalize so 

02:48:02.768 --> 02:48:04.768
that optimization will be  
well-conditioned.

02:48:06.844 --> 02:48:08.844
Your problem with condition.

02:48:10.129 --> 02:48:12.129
And you can use a normalizer 
function here.

02:48:13.636 --> 02:48:15.877
Or, you may choose buckettizing.
In this case, the distance of 

02:48:15.878 --> 02:48:17.878
the 

02:48:19.352 --> 02:48:22.003
hike, we bucket-ize it so it 
will learn the model for 

02:48:22.004 --> 02:48:24.686
different things for different 
segments.  You can concentrate 

02:48:24.687 --> 02:48:28.197
as a different kind of 
normalization too.  How can you 

02:48:28.198 --> 02:48:30.198
use all of these things 
together?

02:48:33.509 --> 02:48:35.337
Just putting them into a list, 
that's it.  Then your system 

02:48:35.338 --> 02:48:38.602
should work.  So, let's 
experiment with personalization.

02:48:40.235 --> 02:48:42.057
What we mean by personal
ization is instead of 

02:48:42.058 --> 02:48:44.714
recommending the same hikes to 
all users, let's recommend 

02:48:45.127 --> 02:48:47.127
different hierarchies for 
different 

02:48:49.264 --> 02:48:51.732
users based on their interests. 
And the way -- one way to do 

02:48:51.733 --> 02:48:53.733
that is  using user features.

02:48:55.831 --> 02:48:57.854
In this case, we are using user 
embedding by embedding_column.

02:49:01.568 --> 02:49:03.568
So, this will let the model to 
learn a 

02:49:06.432 --> 02:49:09.184
vector for each user and put the
users closer if their hike 

02:49:09.185 --> 02:49:11.185
preferences are similar.  And 
how can you use that?

02:49:14.536 --> 02:49:16.536
Again, it's just depending into 
your list.

02:49:19.021 --> 02:49:21.263
And you need to also play with 
hidden units because rough 

02:49:21.264 --> 02:49:25.758
minimal  features now and you 
need to let your model to learn 

02:49:25.759 --> 02:49:27.995
different transformations.  And 
the rest of the pipeline should 

02:49:28.421 --> 02:49:30.421
work.

02:49:31.728 --> 02:49:33.369
You will hear this a lot during 
this talk because it's based on 

02:49:33.370 --> 02:49:36.664
that.  The rest of the pipeline 
should work and you should be 

02:49:36.665 --> 02:49:40.510
able to analyze your 
experiments.  Let's experiment 

02:49:40.511 --> 02:49:42.511
more.

02:49:44.915 --> 02:49:47.572
We have a couple of pre-made 
solutions.  I mentioned it's 

02:49:47.573 --> 02:49:50.245
very popular and I picked only 
two of them here to show.

02:49:53.324 --> 02:49:55.324
One is wide-n-deep,

02:49:56.404 --> 02:49:59.896
it's a joint training of the 
neural network and the model.  

02:49:59.897 --> 02:50:03.786
You may like it or not.  So 
let's start the experiment.  You

02:50:03.787 --> 02:50:05.787
need to define what are the 

02:50:06.846 --> 02:50:08.846
features you want to fit to the 
neural network.

02:50:11.134 --> 02:50:13.134
Again, via feature column, it's 
the list.

02:50:15.001 --> 02:50:16.825
And you need to define the 
features to feed into the linear

02:50:16.826 --> 02:50:18.826
part.

02:50:22.120 --> 02:50:24.754
In this  case, user id and 
picks.  For example, if a user 

02:50:24.755 --> 02:50:26.755
always picks 

02:50:30.655 --> 02:50:31.882
dog of had friendly-friendly 
hikes, the model will learn this

02:50:31.883 --> 02:50:33.883
feature.

02:50:39.011 --> 02:50:41.444
And you can instantiate dnn and 
the rest of the pipeline should 

02:50:41.445 --> 02:50:43.445
work.

02:50:46.916 --> 02:50:48.916
Baseed on the

02:50:52.354 --> 02:50:55.411
2017 Kaggle survey, they are 
very popular.  And we are 

02:50:55.412 --> 02:50:57.412
introducing gradient 

02:51:00.304 --> 02:51:02.304
boosted trees as a pre-made 
estimator.

02:51:04.193 --> 02:51:07.247
And you can experiment without 
changing your pipeline.  Let's 

02:51:07.248 --> 02:51:09.248
start our experimentation.

02:51:10.936 --> 02:51:13.998
In the current version, we only 
support buckettized column.  And

02:51:13.999 --> 02:51:15.999
we are working to support 

02:51:19.581 --> 02:51:22.034
numerical column and cot 
categorical column too.  Here is

02:51:22.035 --> 02:51:24.035
hike distance and hike elevation
gain.

02:51:27.330 --> 02:51:29.330
And we buckettize them.  And 
then you can

02:51:31.647 --> 02:51:33.647
have the classifier and the rest
of thepipe line should work.

02:51:37.813 --> 02:51:38.845
We know that  trees are those at
computationally expensive -- or 

02:51:38.846 --> 02:51:40.846
training 

02:51:42.957 --> 02:51:44.381
trees is not as computationally 
expensive as training neural 

02:51:44.382 --> 02:51:48.058
networks.  And they fit into 
memory.  So, by leveraging that,

02:51:48.059 --> 02:51:50.059
we provide you 

02:51:51.136 --> 02:51:53.136
a utility so that you can train 
your 

02:51:55.217 --> 02:51:59.490
model in order of magnitude 
faster than the usual one.  And 

02:51:59.491 --> 02:52:01.491
the rest of the pipeline should 
work.

02:52:06.050 --> 02:52:07.495
So, let's say this solutions are
not enough for you and you want 

02:52:07.496 --> 02:52:09.953
to experiment more ideas.  Let's
talk about them.

02:52:14.667 --> 02:52:16.667
Before delve into this 
high-level 

02:52:18.561 --> 02:52:21.052
solutions that you can use, 
let's look at a network in a 

02:52:21.053 --> 02:52:23.053
supervised  setting.  In this 
case, you have a network which 

02:52:23.716 --> 02:52:25.968
you fit the features.  And based
on the output of network and 

02:52:29.048 --> 02:52:31.485
the labels, you need to decide 
what is the loss?  What is the 

02:52:31.486 --> 02:52:35.147
objective you want to minimize? 
And what is -- what are the 

02:52:35.148 --> 02:52:37.148
metrics 

02:52:39.663 --> 02:52:42.538
that you will use as a success 
metric for your  evaluation?  

02:52:42.539 --> 02:52:44.539
And your predictions on serving 
time may be

02:52:45.004 --> 02:52:47.004
didn't than in training time.

02:52:48.455 --> 02:52:50.696
For example, if you have a large
setting, you may want to use 

02:52:50.697 --> 02:52:52.697
just the 

02:52:54.774 --> 02:52:56.418
ranking of the classes instead 
of the priorities in the serving

02:52:56.419 --> 02:52:58.419
time.

02:53:01.138 --> 02:53:04.637
For that you don't need to 
calculate the priority.  You can

02:53:04.638 --> 02:53:07.285
use the losses to rank them.  
For all of these, we can  

02:53:07.286 --> 02:53:09.286
sectored 

02:53:10.286 --> 02:53:14.697
them out under head API.  It 
expects you to give the label 

02:53:14.698 --> 02:53:20.217
and out of your network and 
provides these things for you.  

02:53:20.218 --> 02:53:22.218
You'll see it in action.  And 
model function is an 

02:53:25.125 --> 02:53:27.125
implementation of this help and 
network together.

02:53:29.210 --> 02:53:31.897
We talk about the DNN class 5.  
DNN class 5 has a model 

02:53:31.898 --> 02:53:33.898
function.

02:53:36.435 --> 02:53:38.435
It has a specific implementation
for head network.

02:53:40.943 --> 02:53:43.028
Let's implement this with the 
head API.  In this case, DNN  

02:53:43.029 --> 02:53:45.272
estimator.  And we can 
instantiate ahead.

02:53:48.369 --> 02:53:49.807
In this case, it's binary 
classification head because we 

02:53:49.808 --> 02:53:52.050
are trying to predict whether 
it's like or not like.

02:53:56.110 --> 02:53:58.110
And why are we introducing this 
head 

02:53:59.569 --> 02:54:03.049
since these two lines are the 
same as DNN classifier?  Why to 

02:54:03.050 --> 02:54:05.292
introduce this head?  So,
 you can experiment with 

02:54:05.293 --> 02:54:08.768
different ideas by combining 
different network architectures 

02:54:08.769 --> 02:54:10.769
and different heads.

02:54:13.125 --> 02:54:15.806
For example, you can use 
wide-ended, or DNN estimator 

02:54:15.807 --> 02:54:17.807
with a  multi-label head.

02:54:19.932 --> 02:54:22.158
You can even combine different 
heads together, we introduced 

02:54:22.159 --> 02:54:24.159
multi-head here.

02:54:25.428 --> 02:54:27.288
So, it's the one way of 
experimenting with the  

02:54:27.289 --> 02:54:29.289
multi-task learning.

02:54:32.629 --> 02:54:35.297
With a couple lines of code you 
can experiment with multi-task 

02:54:35.298 --> 02:54:38.140
learning.  Please check it out. 
Say the architectures are not 

02:54:38.141 --> 02:54:42.655
enough for you and you want to 
expand more, you can write your 

02:54:42.656 --> 02:54:44.656
own model function.

02:54:48.806 --> 02:54:50.868
We strongly recommend to you to 
use TFK layers to build your 

02:54:50.869 --> 02:54:54.557
network.  You can do whatever 
you want there.  You can be as 

02:54:54.558 --> 02:54:56.805
creative as possible.  And after
you have the output of 

02:55:00.077 --> 02:55:01.914
network, you need to pick one of
the optimizers available in 

02:55:01.915 --> 02:55:03.915
TensorFlow.

02:55:06.776 --> 02:55:08.776
And you can use one of the heads
we mentioned which will convert 

02:55:10.981 --> 02:55:12.981
your network had output and the 
labels 

02:55:14.042 --> 02:55:16.042
into training behaviors or 
evaluation 

02:55:18.478 --> 02:55:19.336
metrics or expart behavior 
export

02:55:19.337 --> 02:55:24.039
behaviors.  Then you can fit to 
the estimator.  Again, the rest 

02:55:24.040 --> 02:55:26.040
of the pipeline should work.

02:55:27.718 --> 02:55:29.718
Keras model is another way of 
creating your model.

02:55:31.842 --> 02:55:35.294
And it's very popular, it's very
intuitive to use.  For example, 

02:55:35.295 --> 02:55:37.295
this is one of the Keras models 
you can build.

02:55:40.001 --> 02:55:41.250
So, how can you get this 
estimator so that the rest of 

02:55:41.251 --> 02:55:44.524
the pipeline should work?  You 
can use model to estimator which

02:55:46.570 --> 02:55:48.200
gives you the estimator so you 
can run your experiments without

02:55:48.201 --> 02:55:50.269
changing your pipeline.

02:55:53.730 --> 02:55:56.761
Transfer learning is another 
popular technique we  do.  

02:55:56.762 --> 02:55:58.762
Experiment with.

02:56:00.281 --> 02:56:02.281
One way of extending is using 
model A, 

02:56:03.546 --> 02:56:05.601
which is already the trend, to 
improve the prediction of model 

02:56:05.602 --> 02:56:07.602
B.  How can you do that?

02:56:11.602 --> 02:56:13.602
Surprisingly, just copying and 

02:56:14.853 --> 02:56:17.298
transferring from model A to 
model B works.  That's simple, 

02:56:17.299 --> 02:56:19.299
but it works.

02:56:21.588 --> 02:56:23.588
And we provide that for you.

02:56:25.876 --> 02:56:27.876
You can use one start -- this 
one line 

02:56:28.971 --> 02:56:30.971
is saying that transfer all of 
the model A into model B.

02:56:34.294 --> 02:56:36.294
Or, you can define a subset of 
model A 

02:56:37.981 --> 02:56:40.015
to transfer from model A to 
model B.  Let's talk

02:56:40.830 --> 02:56:42.830
about image features.

02:56:45.557 --> 02:56:47.190
We talk about embedding 
categorical column and image  

02:56:47.191 --> 02:56:49.643
features.  But what if you have 
image features, 

02:56:52.701 --> 02:56:54.340
how can you use them in your 
pipeline with a couple of lines 

02:56:54.341 --> 02:56:56.341
of code?

02:56:59.502 --> 02:57:01.502
You can implement one of the 
image classifiers.

02:57:03.779 --> 02:57:05.779
Which is not a couple of lines 
of  code.

02:57:08.876 --> 02:57:11.721
Or you can, thanks to TF-Hub, 
you will learn later, you can 

02:57:11.722 --> 02:57:17.001
use one line from their hub to 
instantiate the feature column, 

02:57:17.002 --> 02:57:19.002
called image  embedding column.

02:57:20.864 --> 02:57:22.864
In this case, you may remember 
Jeff 

02:57:23.940 --> 02:57:26.848
mentioned AutoML.  It is one of 
the AutoML models.  It's really 

02:57:26.849 --> 02:57:28.895
good.  It's one of the top 
models you can use.

02:57:35.802 --> 02:57:38.572
Here you will use nasnet as a 
feature.

02:57:42.685 --> 02:57:46.782
It will use only the optimizeer 
of nasnet into your feature.  

02:57:46.783 --> 02:57:48.783
How can you use it as the 
classifier?

02:57:50.064 --> 02:57:52.493
Just depending it into your 
feature columns, and then done. 

02:57:52.494 --> 02:57:54.533
You can experiment with it.  
Let's say you experimented and 

02:57:54.534 --> 02:57:59.014
you find some models, but you 
need to scale it up.  Not all of

02:57:59.015 --> 02:58:01.015
you, but some of you may need to
scale your training.

02:58:05.157 --> 02:58:07.807
You can use multi-GPU, means 
multiplication on different 

02:58:07.808 --> 02:58:09.808
GPUs.

02:58:11.072 --> 02:58:13.137
You can learn about that after 
my talk.  You don't need to 

02:58:13.138 --> 02:58:16.216
change the estimator or model 
code.  Everything should work 

02:58:16.217 --> 02:58:20.300
with just a single line of 
configuration change.  Or, you 

02:58:20.301 --> 02:58:22.328
may want to distribute your 
training to multiple machines by

02:58:22.329 --> 02:58:24.581
saying these are workers, these 
are primary 

02:58:27.871 --> 02:58:30.332
servers and there's very little 
going on.  Same.  You don't need

02:58:30.333 --> 02:58:34.616
to change your estimator or 
model code.  Everything should 

02:58:34.617 --> 02:58:36.617
work based on the configuration.

02:58:40.984 --> 02:58:43.828
Or, you may want to use TPU 
estimator for TPU.  There's a 

02:58:43.829 --> 02:58:46.298
minimal change in the model 
function.  Hopefully later we 

02:58:46.299 --> 02:58:48.725
will fix that too.  But now 
there's a minimal change in 

02:58:51.813 --> 02:58:53.813
your model function you need to 
do.

02:58:55.687 --> 02:58:58.557
To use this in your production, 
you need to expart.  Or you can 

02:58:58.558 --> 02:59:00.558
-- you need to serve.

02:59:02.463 --> 02:59:04.505
And we recommend you to use TF 
serving.  In the serving time, 

02:59:04.506 --> 02:59:10.013
instead of reading data from the
files, you have a request.  And 

02:59:10.014 --> 02:59:12.014
you need to define the receiver

02:59:13.896 --> 02:59:16.751
function, which is defining how 
can you connect that into the 

02:59:16.752 --> 02:59:20.837
model?  In this case -- and 
after that, what would be the 

02:59:20.838 --> 02:59:23.097
output of that model?  Which is 
defined by a signature 

02:59:23.293 --> 02:59:29.690
definition.  So, here, again, a 
couple of lines of code.  You 

02:59:29.691 --> 02:59:31.941
will export your chain model 
with the TF serving.

02:59:35.844 --> 02:59:37.844
For example, if your request is 
tf.

02:59:38.915 --> 02:59:40.140
example, you will use this 
function to get your receiver 

02:59:40.141 --> 02:59:42.141
function.

02:59:44.436 --> 02:59:46.436
And you can use export set model
so 

02:59:47.902 --> 02:59:49.902
that it will be used by TF 
serving.

02:59:54.019 --> 02:59:56.457
These are the modules I 
mentioned,  tf.estimator, tf.

02:59:58.505 --> 03:00:02.586
feature Curdy column.  Don't use
tf.learn, we are deprecating it.

03:00:05.443 --> 03:00:06.663
And these are a couple that I 
picked.  You can check it out.  

03:00:06.664 --> 03:00:10.986
Thank you.  I hope some of you 
will improve your products with 

03:00:10.987 --> 03:00:13.454
the tools that we mentioned.  
Thank you.

03:00:19.184 --> 03:00:21.184
And I'll introduce Igor.

03:00:23.051 --> 03:00:25.051
Igor will talk about how you can
do distribution with TensorFlow.

03:00:26.339 --> 03:00:28.339
And he's coming.  Yes.  He's 
coming.

03:00:30.041 --> 03:00:32.041
&gt;&gt; Hey.

03:00:34.528 --> 03:00:38.212
Hello, everyone.  My name is 
Igor.  I'm going to -- I work in

03:00:38.213 --> 03:00:40.213
the TensorFlow team and I'm 
going to talk to 

03:00:43.103 --> 03:00:45.757
you today about distributed 
TensorFlow.  Well, why would you

03:00:45.758 --> 03:00:49.234
care about distributed 
TensorFlow?  Many of you know 

03:00:49.235 --> 03:00:51.235
the  answer, probably.

03:00:53.773 --> 03:00:56.631
But just in case, it's a way for
your models to train faster and 

03:00:56.632 --> 03:00:58.632
be more parallel.

03:01:00.945 --> 03:01:02.945
It's a way for you to get more 
things done, iterate quicker.

03:01:06.032 --> 03:01:08.680
When you train in models, it can
take a long time.  And when I 

03:01:08.681 --> 03:01:10.681
say a long time, I mean week 
weeks.

03:01:13.708 --> 03:01:18.026
Without  With all the available 
hardware to you out there, 

03:01:18.027 --> 03:01:20.027
scaling up to hundreds of 

03:01:22.102 --> 03:01:23.535
CPUs or GPUs are really make a 
difference.  How could you scale

03:01:23.536 --> 03:01:25.536
up?

03:01:26.841 --> 03:01:29.081
Well, you could just add a GPU 
to your machine.  In this case, 

03:01:29.082 --> 03:01:31.082
this is just plug and play.

03:01:33.383 --> 03:01:35.428
You insert a GPU, TensorFlow 
handles all the details for you 

03:01:35.429 --> 03:01:38.287
and you see a nice bump in the 
training speed.

03:01:42.571 --> 03:01:44.571
You could also insert multiple 
GPUs.

03:01:46.491 --> 03:01:48.725
In this case -- in this case, 
you would have to write 

03:01:48.726 --> 03:01:50.726
additional code.  You need to 
replicate

03:01:51.164 --> 03:01:53.800
your model.  You need to combine
gradients from 

03:01:59.305 --> 03:02:01.136
every GPU and if you're using 
batchnorm layer, you have the 

03:02:01.137 --> 03:02:03.609
tricky question of what to do 
with the statistics on each GPU.

03:02:06.459 --> 03:02:08.317
The point I'm trying to make is 
that you need to do additional 

03:02:08.318 --> 03:02:10.994
work to make this work.  And you
need to learn stuff that you 

03:02:13.275 --> 03:02:18.592
didn't plan on learning.  You 
can also use multiple machines. 

03:02:18.593 --> 03:02:21.449
And this situation is similar to
the one before.  But in this 

03:02:21.450 --> 03:02:23.491
case, your bottleneck is 
probably going to be that 

03:02:23.492 --> 03:02:28.600
communication between the 
machines.  You'll start thinking

03:02:28.601 --> 03:02:31.283
about minimizing that 
communication and probably doing

03:02:32.096 --> 03:02:34.096
more work locally.

03:02:35.149 --> 03:02:37.989
For example, combining the 
gradients in the local GPUs 

03:02:37.990 --> 03:02:42.280
before exchanging them with the 
remote GPUs.  Unless specialized

03:02:42.281 --> 03:02:46.772
network and hardware is used.  
The coordination costs in this 

03:02:46.773 --> 03:02:48.773
setup 

03:02:50.055 --> 03:02:51.902
are probably going to limit your
scaling.  But there is an 

03:02:51.903 --> 03:02:56.593
approach -- there is a solution 
to this.  This approach is  

03:02:56.594 --> 03:02:58.644
called parameter server.

03:03:02.966 --> 03:03:04.966
Some hosts we call them 
parameter servers. 

03:03:07.492 --> 03:03:10.172
They're going to only hold  
training weights.  Other hosts, 

03:03:10.173 --> 03:03:14.508
workers, they're going to have a
copy of the TensorFlow graph.  

03:03:14.509 --> 03:03:17.170
They're going to get their own 
input, compute their own 

03:03:17.171 --> 03:03:19.171
gradient, and then 

03:03:20.668 --> 03:03:21.884
just go ahead and update the 
training waits without any 

03:03:21.885 --> 03:03:23.885
coordination with other workers.

03:03:25.945 --> 03:03:27.183
So, this is an approach with low
coordination between a large 

03:03:27.184 --> 03:03:33.906
number of hosts.  And there you 
go.  This scales well.  And we 

03:03:33.907 --> 03:03:37.809
have been doing this at Google 
for a long time.  But there is a

03:03:37.810 --> 03:03:42.312
wrinkle with this approach.  You
give up synchronicity.  And that

03:03:42.313 --> 03:03:44.374
has benefits.  And if you think 
about it, parameter 

03:03:47.704 --> 03:03:49.704
server approach, it's an 
approach from 

03:03:50.986 --> 03:03:53.571
the -- from the CPU era.  With 
all the reliable communication 

03:03:58.266 --> 03:04:01.125
between GPUs, we can consider 
designs which have tighter 

03:04:01.126 --> 03:04:03.772
coupleing and more coordination 
between the workers.

03:04:08.266 --> 03:04:10.100
One such approach is based on 
overages.  That's not a new 

03:04:10.101 --> 03:04:12.101
idea.

03:04:15.602 --> 03:04:17.837
The general goal of all-reduce 
is to combine all the values and

03:04:17.838 --> 03:04:21.764
distribute results to all the 
processes.  All-

03:04:24.430 --> 03:04:26.430
reduce is kind of tricky to 
explain in this light.

03:04:30.555 --> 03:04:33.419
But you can think of its results
as reduce stipulation followed 

03:04:33.420 --> 03:04:36.276
by a broadcast stipulation.  But
don't think of it that way in 

03:04:36.890 --> 03:04:41.816
terms of performance.  It's a 
fused algorithm.  And it's way 

03:04:41.817 --> 03:04:44.258
more efficient than those two 
operations together.

03:04:51.663 --> 03:04:53.663
In addition to it, hardware 
vendors -- 

03:04:55.555 --> 03:04:57.395
hardware vendors specialized or 
reduce implementations that 

03:04:57.396 --> 03:05:02.701
TensorFlow could secretly use 
behind the scenes to help you.  

03:05:02.702 --> 03:05:04.955
Alternative approaches.  They 
typically send all data to a 

03:05:06.418 --> 03:05:08.418
central place.

03:05:09.479 --> 03:05:11.308
All-reduce is not going to have 
such a bottleneck because it 

03:05:11.309 --> 03:05:14.353
distributed coordination between
GPUs way more  evenly.

03:05:18.040 --> 03:05:19.862
With every tick of all-reduce, 
each GPU sends and receives a 

03:05:19.863 --> 03:05:21.908
part of the final answer.

03:05:25.813 --> 03:05:27.813
So, how could all-reduce help us
with our models.

03:05:29.313 --> 03:05:31.313
Well, consider -- let's say you 
have two GPUs.

03:05:34.059 --> 03:05:36.297
You copied layers and the 
variables in every GPU and you 

03:05:36.298 --> 03:05:38.298
performed the forward pass, nice
and parallel.

03:05:41.193 --> 03:05:43.193
But then during the backward 
pass, as the gradients

03:05:44.064 --> 03:05:46.064
become available, we can reduce 
-- we 

03:05:47.747 --> 03:05:49.579
can use  all-reduce to combine 
those gradients with the 

03:05:49.580 --> 03:05:53.279
counterparts from other GPUs.  
In addition to that, because of 

03:05:53.280 --> 03:05:55.280
the 

03:05:56.964 --> 03:05:58.964
way -- in addition to that, 
gradients 

03:06:01.055 --> 03:06:02.501
from the other layers are 
available before the gradients 

03:06:02.502 --> 03:06:04.502
from the other layers.

03:06:06.420 --> 03:06:08.420
So, we could overlap backward 

03:06:09.750 --> 03:06:12.001
propagation computation and 
reduce communication.  That 

03:06:12.002 --> 03:06:14.002
gives you even more per second.

03:06:15.676 --> 03:06:17.910
The bottom line is, when all -- 
when communication between GPUs 

03:06:17.911 --> 03:06:22.212
is reliable, all-reduce can be 
fast and allow you to scale 

03:06:22.213 --> 03:06:24.213
well.

03:06:25.541 --> 03:06:27.541
How could you use  all-reduce in
TensorFlow?

03:06:29.437 --> 03:06:32.344
Well, so far in this talk I told
you to take advantage of 

03:06:32.345 --> 03:06:36.841
multiple GPUs you need to write 
additional code, change your 

03:06:36.842 --> 03:06:38.842
model and learn stuff.

03:06:42.789 --> 03:06:45.044
Chances are you're using -- 
you're following -- of using the

03:06:45.045 --> 03:06:47.071
highest level API that works for
your use case.

03:06:51.721 --> 03:06:56.260
That probably isette is  
Estimator.  It is the model 

03:06:56.261 --> 03:06:58.261
function, it has no knowledge 
about GPUs or devices.

03:07:01.172 --> 03:07:03.172
So, to have that model use 
multiple

03:07:03.206 --> 03:07:05.206
GPUs, you just need to add one 
line.

03:07:06.253 --> 03:07:08.253
You need to pass an instance of 
a new 

03:07:13.620 --> 03:07:15.620
class called 
MirrormirrorStrategy.

03:07:17.951 --> 03:07:20.396
And it is one implementation of 
our new distribution strategy 

03:07:20.397 --> 03:07:25.727
API.  TensorFlow, how to 
replicate your model?  Oops.  

03:07:25.728 --> 03:07:27.728
Sorry.

03:07:30.680 --> 03:07:32.325
Another thing I want to say is 
that MirrorStrategy could take a

03:07:32.326 --> 03:07:36.399
number of GPUs or a list of GPUs
to use.  Or you cannot give it 

03:07:36.400 --> 03:07:41.098
any arguments at all and then it
will just figure out what GPUs 

03:07:41.099 --> 03:07:43.099
to use.

03:07:46.592 --> 03:07:48.592
MirrorStrategy works in a way 
exactly as I described before.

03:07:51.534 --> 03:07:53.784
It replicates your model, it 
uses  all-reduce for 

03:07:53.785 --> 03:07:55.785
communication.

03:08:02.778 --> 03:08:05.238
So, gradient updates from every 
GPUs, from all GPUs, they're 

03:08:05.239 --> 03:08:09.547
going to be combined before 
updating the waits.  And each 

03:08:09.548 --> 03:08:12.008
copy of your model, an average 
GPU, is part of a single 

03:08:12.613 --> 03:08:15.305
TensorFlow graph.  That means 
there is the replication 

03:08:19.996 --> 03:08:20.818
with synchronous training that 
uses all- all-reduce on many 

03:08:20.819 --> 03:08:25.509
GPUs.  Now, the last ten minutes
are kind of a waste of time for 

03:08:25.510 --> 03:08:27.510
you if this doesn't perform 
well.

03:08:29.388 --> 03:08:31.388
And it does perform well.  As

03:08:34.138 --> 03:08:37.409
you add GPUs, this 
implementation scales well.  We 

03:08:37.410 --> 03:08:39.636
have a team at TensorFlow that 
specifically works on fast 

03:08:43.727 --> 03:08:44.754
implementations of all-reduce 
for various machine 

03:08:44.755 --> 03:08:49.880
configurations.  And this 
implementation gets 90% scaling 

03:08:49.881 --> 03:08:51.881
on 8 GPUs.

03:08:52.943 --> 03:08:54.943
And, again, it didn't require 
any 

03:08:56.858 --> 03:08:58.858
change to the -- to the user's 
model.

03:09:00.357 --> 03:09:02.815
It didn't require any change are
because we changed everything in

03:09:03.840 --> 03:09:05.840
TensorFlow that's not your 
model.

03:09:09.578 --> 03:09:11.425
Things like optimizer, bench 
norm, summaries, everything that

03:09:11.426 --> 03:09:13.930
rides state, now needs to become
distribution aware.

03:09:16.968 --> 03:09:19.622
That means it needs to learn how
to combine its state with other 

03:09:19.623 --> 03:09:21.623
GPUs.

03:09:24.939 --> 03:09:26.993
And this is important because 
alternative APIs out there, they

03:09:30.689 --> 03:09:32.689
typically ask you to rephrase 
your model 

03:09:33.785 --> 03:09:35.785
to supply optimizeers, for 
example, 

03:09:36.843 --> 03:09:39.515
separately so that they can do 
all the state coordination 

03:09:39.516 --> 03:09:41.516
behind the scenes.

03:09:44.420 --> 03:09:47.089
And if you have some experience 
with training your models on 

03:09:47.090 --> 03:09:49.090
multiple GPUs, 

03:09:52.049 --> 03:09:54.702
you might be wondering, well, 
can  I -- can I save my model 

03:09:54.703 --> 03:09:56.703
own a computer with 

03:09:57.764 --> 03:10:00.209
8 GPUs and then do an
evaluation on it on a computer 

03:10:00.210 --> 03:10:04.280
with, say, no GPUs?  Typically 
this causes a problem.

03:10:09.994 --> 03:10:11.994
But with distribution strategy 
API, we 

03:10:13.666 --> 03:10:15.666
maintain backward compatibility 
on the checkpoint level.

03:10:20.815 --> 03:10:25.094
So, Mirror Mirrorstrategy, it 
has multiple copies on the GPU. 

03:10:25.095 --> 03:10:27.562
It's going to save one coupe, 
and then at the restore time, 

03:10:27.563 --> 03:10:31.430
it's only going to restore that 
state to a required number of 

03:10:31.431 --> 03:10:33.431
GPUs.

03:10:35.087 --> 03:10:37.087
So, this use case is supported.

03:10:39.770 --> 03:10:43.426
Distribution strategy works with
Eager mode as well.  But we are 

03:10:43.427 --> 03:10:45.427
still fine-tuning the 
performance.

03:10:48.485 --> 03:10:51.190
And distribution strategies are 
a very general API that I hope 

03:10:51.191 --> 03:10:53.191
in the future will support many 
use cases.

03:10:56.280 --> 03:10:58.280
It's not tied to Estimator and 
we are 

03:10:59.771 --> 03:11:01.771
looking into ways of creating 
even more 

03:11:04.307 --> 03:11:06.307
-- better APIs based on 
distribution strategy.

03:11:08.580 --> 03:11:11.241
We -- in the future, soon, 
pretty soon -- we intend to 

03:11:11.242 --> 03:11:13.675
support all kinds of -- many 
kinds of distributed training.

03:11:20.203 --> 03:11:22.203
Synchronous, asynchronous, 
multi-node, 

03:11:23.878 --> 03:11:25.100
parallelism, all of that is as 
part of distribution strategy 

03:11:25.101 --> 03:11:27.760
API.  Until then, for

03:11:31.254 --> 03:11:33.254
multi-node, use estimator.train 
and evaluate.

03:11:37.592 --> 03:11:39.592
Or Horovod, that offers a 
multi-node solution.

03:11:44.325 --> 03:11:46.977
MirrorStrategy is available for 
you in our nightly build.  And 

03:11:46.978 --> 03:11:48.978
we are very, very actively 
working on it.

03:11:52.128 --> 03:11:55.379
And it's a product of work of 
many people.  And I would really

03:11:55.380 --> 03:11:59.286
encourage you to try it out and 
let us know what you think about

03:11:59.287 --> 03:12:01.287
it.

03:12:04.405 --> 03:12:06.867
By GitHub or talk to us after my
talk.  All right.  Thank you.  

03:12:06.868 --> 03:12:09.745
Thanks for your attention.
[ Applause ]

03:12:18.747 --> 03:12:20.747
Next up is Justine and Shanging 
to tell 

03:12:21.834 --> 03:12:23.880
you how to debug TensorFlow 
using TensorBoard.

03:12:27.572 --> 03:12:30.833
&gt;&gt; Well, thank you, everybody, 
for being here today.

03:12:36.547 --> 03:12:38.547
We're going to be giving a talk 
about 

03:12:41.379 --> 03:12:43.917
the new TensorFlow debugger,, 
which comes included with 

03:12:43.918 --> 03:12:46.573
TensorBoard.  It's basically a 
debugger like you 

03:12:49.835 --> 03:12:51.679
would see in an IDE that lets 
you step in separate points and 

03:12:51.680 --> 03:12:53.705
models and watch tensors.  But 
before we do that, I would like 

03:12:53.706 --> 03:12:55.706
to 

03:12:57.588 --> 03:12:59.417
give you some background on 
TensorBoard and some of the 

03:12:59.418 --> 03:13:01.657
other developments which 
happened in the last year.  

03:13:01.658 --> 03:13:03.658
Which we unfortunately don't 
have too 

03:13:05.157 --> 03:13:07.157
much time to go into.

03:13:08.225 --> 03:13:10.225
But TensorBoard is basically a 
weapon application.

03:13:13.797 --> 03:13:15.627
It's a suite of weapon 
applications that was authored 

03:13:15.628 --> 03:13:17.628
by about 20 people.

03:13:20.674 --> 03:13:22.674
And it's all packed into a 2

03:13:24.820 --> 03:13:27.059
megabyte command web server that
works offline.  And TensorBoard 

03:13:27.060 --> 03:13:28.892
can be used for many purposes 
with the different plugins baked

03:13:28.893 --> 03:13:30.893
into it.

03:13:32.172 --> 03:13:34.436
The one you're all most familiar
with for those who have used 

03:13:34.437 --> 03:13:37.289
TensorBoard is the scalers 
dashboard.  You can plot 

03:13:37.290 --> 03:13:40.801
anything you want.  It could be 
like loss curves, et cetera, 

03:13:40.802 --> 03:13:43.479
accuracy.  And these
things like sort of help us 

03:13:43.480 --> 03:13:47.981
understand, like, whether or not
our model is converging on 

03:13:47.982 --> 03:13:49.982
optimal solutions.

03:13:52.249 --> 03:13:54.277
And here is the really 
interesting, underutilized 

03:13:54.278 --> 03:13:56.732
feature called the embedding 
projector.  And this was 

03:13:56.733 --> 03:13:59.408
originally written by Google so 
we could do things like, you 

03:14:03.261 --> 03:14:05.089
know, project our data into a 3D
space, see how things cluster.

03:14:09.976 --> 03:14:12.256
doing MNIST, the 7s here and the
9s here.  And we actually 

03:14:12.257 --> 03:14:14.257
recently -- what you 

03:14:15.731 --> 03:14:17.731
see on the screen is we got a 
really 

03:14:20.207 --> 03:14:22.207
cool contribution from Francois 
at IBM research.

03:14:24.703 --> 03:14:26.142
He sent pull requests on the 
GitHub repository, since we are 

03:14:26.143 --> 03:14:31.659
in the open.  He  added 
interactive label editing.  So, 

03:14:31.660 --> 03:14:33.660
you can sort of like go in there

03:14:36.150 --> 03:14:38.150
and change things as algorithms 
like 

03:14:40.463 --> 03:14:41.481
TSNE, give your data -- sort of 
reveal the structure of your 

03:14:41.482 --> 03:14:43.482
data.

03:14:45.970 --> 03:14:47.604
To learn more, search Google for
interactive super vision with 

03:14:47.605 --> 03:14:51.286
TensorBoard.  This is another 
really amazing contribution that

03:14:51.287 --> 03:14:55.559
we received from a university 
student named Chris Anderson.  

03:14:55.560 --> 03:14:57.560
It's called the  holder plugin.

03:14:59.235 --> 03:15:01.235
And this basically gives you a 
real 

03:15:02.897 --> 03:15:06.588
real-time visual glimpse into 
TensorFlow data structures.  

03:15:06.589 --> 03:15:07.992
Like, for example, as you're 
training script is running, it's

03:15:07.993 --> 03:15:12.492
real-time.  It doesn't require a
hard drive.  It doesn't work 

03:15:12.493 --> 03:15:16.193
with something like GCS at this 
point in time.  I think this 

03:15:16.194 --> 03:15:18.232
would be a very useful tool 
going forward in terms of model 

03:15:20.064 --> 03:15:22.912
explainability.
Now, TensorBoard also has some 

03:15:22.913 --> 03:15:24.913
new plugins for optimization.

03:15:29.640 --> 03:15:32.487
Cloud recently contributed a TPU
profiling plugin.  And TPU 

03:15:32.488 --> 03:15:35.332
hardware is a little different 
from what many of you might be 

03:15:35.333 --> 03:15:39.035
used to.  And TensorBoard, with 
this plugin, can really help you

03:15:39.036 --> 03:15:41.291
get the most out of your 
hardware and ensure that it's 

03:15:41.292 --> 03:15:43.530
being properly utilize utilized.

03:15:46.581 --> 03:15:48.813
Now, the TensorBoard ecosystem, 
part of the goal of this talk, 

03:15:48.814 --> 03:15:50.814
before we get 

03:15:52.474 --> 03:15:55.130
into the demo, is I want to 
attract more folks in the 

03:15:55.131 --> 03:15:59.016
community to get involveed with 
TensorBoard development.  We use

03:15:59.017 --> 03:16:01.646
many of the tools you're 
familiar with such as TypeScript

03:16:01.647 --> 03:16:03.647
and Polymer.

03:16:05.947 --> 03:16:07.947
We also use some tools you might
not 

03:16:10.439 --> 03:16:12.439
be familiar with, luck Bazel, 
for good reasons.

03:16:15.336 --> 03:16:17.336
You can go to the Readmes for 
the plugins we wrote originally.

03:16:19.442 --> 03:16:21.675
Now, with TensorBoard, the 
reason this is just a little bit

03:16:21.676 --> 03:16:23.676
more challenging compared to 
some of the other web 

03:16:26.173 --> 03:16:28.001
application you may have used or
written in the past, is we deal 

03:16:28.002 --> 03:16:30.262
with very challenging 
requirements.  Like, this thing

03:16:30.263 --> 03:16:32.741
needs to work offline.

03:16:37.015 --> 03:16:39.254
It needs to be able to build, 
regardless of, like, corporate 

03:16:39.255 --> 03:16:44.319
or national firewalls that may 
block certain URLs when it's 

03:16:44.320 --> 03:16:46.167
downloading things.  For 
example, one of the first things

03:16:46.168 --> 03:16:50.637
I did when I joined the 
TensorBoard team wasn't actually

03:16:50.638 --> 03:16:53.492
visualizing machine learning, 
but adding a contribution to 

03:16:58.597 --> 03:16:59.400
Bazel which allows  downloads to
be carrier grade 

03:16:59.401 --> 03:17:02.664
internationally.  And there are 
a whole variety of challenges 

03:17:02.665 --> 03:17:04.665
like when it comes to an 
application like this.

03:17:06.729 --> 03:17:08.755
But those  burdens are things 
we've mostly solved for you, and

03:17:08.756 --> 03:17:10.756
here is a concrete example.

03:17:15.272 --> 03:17:17.272
Writing that toilsome thousand 
line 

03:17:18.275 --> 03:17:20.275
file

03:17:22.838 --> 03:17:24.256
was what it took to make 
TensorBoard look good anywhere 

03:17:24.257 --> 03:17:28.125
in the world without having to 
ping.  That is one of the many 

03:17:28.126 --> 03:17:30.979
burdens that the TensorBoard 
team carries on behalf of plugin

03:17:30.980 --> 03:17:32.980
authors.

03:17:35.069 --> 03:17:36.898
Now, I want to give you a quick 
introduction for Shanging who is

03:17:36.899 --> 03:17:38.899
the 

03:17:41.576 --> 03:17:43.576
author of this TensorFlow 
debugger.

03:17:46.892 --> 03:17:48.892
And with the help of Che  Che 
Zhang.

03:17:51.787 --> 03:17:55.387
As I mentioned earlier,
TensorBoard has been the flash 

03:17:51.787 --> 03:17:54.434
light that's giving broad 
overviews of what's 

03:17:57.905 --> 03:17:59.905
happening inside these black box
models.

03:18:01.791 --> 03:18:03.833
What the TensorFlow debuggers 
does, it turns that flash light 

03:18:03.834 --> 03:18:08.936
into an X had-ray.  Using this 
plugin, you can literally watch 

03:18:08.937 --> 03:18:10.776
the tensors as they flow in 
real-time while having complete 

03:18:10.777 --> 03:18:15.726
control over the entire process.
This X-ray is what's going to 

03:18:15.727 --> 03:18:17.727
make it 

03:18:18.787 --> 03:18:20.440
possible for you to pinpoint 
problems we've previously found 

03:18:20.441 --> 03:18:22.441
difficult to identify.

03:18:25.769 --> 03:18:26.793
Perhaps down to the tiniest nan 
at the precise moments they 

03:18:26.794 --> 03:18:29.034
happen.  That's why we call it 
an X-ray.

03:18:33.371 --> 03:18:35.371
It reveals the graph and math 
beneath 

03:18:36.638 --> 03:18:38.638
the abstractions we love, Keras 
or 

03:18:40.710 --> 03:18:42.335
Estimator, or as was announced 
today,  swift.  Whatever tools 

03:18:42.336 --> 03:18:44.336
you're using, this 

03:18:45.610 --> 03:18:48.675
could potentially be a very 
helpful troubleshooting tool.  I

03:18:48.676 --> 03:18:51.343
would like to introduce its 
author, Shanging, who can show 

03:18:51.344 --> 03:18:53.344
you a demo.
&gt;&gt; Thank you very much.

03:18:54.438 --> 03:18:56.438
[ Applause ]
Okay.

03:19:00.983 --> 03:19:02.983
So, in a moment the screencast 
will start.  Great.

03:19:05.701 --> 03:19:07.701
Thank you, Justine for the 
generous intro.  I'm Shanging.

03:19:12.047 --> 03:19:14.693
And I'm glad and honored to 
present the feature plugin for 

03:19:14.694 --> 03:19:16.694
TensorBoard.

03:19:17.953 --> 03:19:20.232
Among the many createed for 
TensorBoard so far.  For those 

03:19:20.233 --> 03:19:22.233
that know TensorFlow 

03:19:24.931 --> 03:19:26.752
debugger or TFDGB, it's only had
a can command line interface 

03:19:26.753 --> 03:19:29.204
until recently.  Like the 
command line interface, the 

03:19:31.257 --> 03:19:33.912
debugger plugin allows you to 
look into internals of the run 

03:19:33.913 --> 03:19:35.913
in TensorFlow 

03:19:38.425 --> 03:19:39.440
model, but in a much more 
intuitive and richer environment

03:19:39.441 --> 03:19:44.156
in the browser.  In this talk 
I'm going to show two examples. 

03:19:44.157 --> 03:19:45.620
One example of how to use the 
tool to understand and probe and

03:19:45.621 --> 03:19:50.531
visualize a working model that 
doesn't have any bugs in it.  

03:19:50.532 --> 03:19:53.384
I'm also going to show you how 
to use the tool to debug a model

03:19:53.385 --> 03:19:57.473
with a bug in it so you can see 
how to use the tool to catch the

03:19:57.474 --> 03:19:59.474
road cause of problems and fix 
them.

03:20:01.194 --> 03:20:03.423
So, first, let's look at the 
first example.  And that's the 

03:20:03.424 --> 03:20:06.137
example on the right part of the
screen right now.  It's a

03:20:07.776 --> 03:20:09.776
simple TensorFlow program that 
does some 

03:20:10.841 --> 03:20:12.841
regression using some generateed
synthetic data.

03:20:17.595 --> 03:20:19.595
And if we run program in the con

03:20:20.860 --> 03:20:22.093
console, we can see a constant 
decrease in the loss value 

03:20:22.094 --> 03:20:25.383
during training.  Even though 
the model works, we have no 

03:20:25.384 --> 03:20:27.810
knowledge how the model works.  
That's mainly because in graph 

03:20:27.811 --> 03:20:32.113
mode, the sessions are run as a 
black box.  That wraps all the 

03:20:32.114 --> 03:20:35.987
computation in one single line 
of Python code.  What if we want

03:20:35.988 --> 03:20:37.988
to look in the model?

03:20:41.516 --> 03:20:43.556
Look at the matrix 
multiplication in a dense layer 

03:20:43.557 --> 03:20:47.023
and have a gradient and so 
forth?  The TensorFlow debugger 

03:20:47.024 --> 03:20:49.058
or TensorBoard debugger plugin 
as a tool can allow you to do 

03:20:49.059 --> 03:20:51.059
that.

03:20:52.773 --> 03:20:54.592
To start the tool, we start the 
TensorBoard with the flat 

03:20:54.593 --> 03:20:56.883
debugger port.  We specify the 
port to be 7,000.

03:21:01.579 --> 03:21:02.193
Once it's running, we can 
navigate to our TensorBoard URL 

03:21:02.194 --> 03:21:07.966
in the browser.  At the startup,
the plugin tells you it's 

03:21:07.967 --> 03:21:11.059
waiting for connections from 
TensorFlow to run.  That's 

03:21:11.060 --> 03:21:13.060
because we haven't started 
program yet.

03:21:15.411 --> 03:21:17.411
And code snippets, estimators or
Keras models.

03:21:20.964 --> 03:21:23.018
In this model, we're using tf.
session.  The first line is an 

03:21:23.019 --> 03:21:27.730
import line, and the second line
is a line that wraps are the 

03:21:27.731 --> 03:21:30.173
original objects with a special 
wrapper that has the information

03:21:30.174 --> 03:21:32.174
where to connect the port 
number.

03:21:36.098 --> 03:21:37.117
Now, without our -- with our 
program implemented we can start

03:21:37.118 --> 03:21:39.361
program again.  Now, as soon as 
program  starts, we 

03:21:42.620 --> 03:21:44.455
can see the graphical user 
interface in the browser switch 

03:21:44.456 --> 03:21:46.910
to a mode that shows you a graph
of the sessions that are run in 

03:21:46.911 --> 03:21:49.549
two ways.  In a tree view on the
left and in a graph on the 

03:21:49.550 --> 03:21:54.253
right.  On the bottom left 
corner, you can also see what 

03:21:54.254 --> 03:21:56.495
session is currently executing. 
The tree structure on the right 

03:21:58.531 --> 03:22:00.531
corresponds to name scopes in 
your model.

03:22:03.489 --> 03:22:05.104
For example, the dense layer -- 
the dense name scope corresponds

03:22:05.105 --> 03:22:10.067
to the dense layer.  You can 
open the source code to look at 

03:22:10.068 --> 03:22:12.515
the correspondence between the 
graph nodes and the lines of the

03:22:12.516 --> 03:22:14.516
path and program created those 
programs.

03:22:19.052 --> 03:22:21.052
If you click  nasnal, you can 
see which line of the

03:22:21.755 --> 03:22:22.371
path and source code is 
responsible for creating that 

03:22:22.372 --> 03:22:25.452
node.  In this case, it's dense 
layer.  As expected.

03:22:30.789 --> 03:22:32.817
If you click another -- if you 
click the last tenser, you will 

03:22:32.818 --> 03:22:35.467
see the corresponding node in 
the graph.  You can see where 

03:22:35.468 --> 03:22:39.559
it's created in the path and 
source code.  It's where we call

03:22:39.560 --> 03:22:41.560
the me squared error.

03:22:44.898 --> 03:22:47.347
And the gradients, name scope 
corresponds to the back 

03:22:47.348 --> 03:22:50.455
propagation part of the model.  
You can click around, poke 

03:22:50.456 --> 03:22:52.456
around and 

03:22:54.167 --> 03:22:56.843
explore how a TensorFlow model 
does optimization and 

03:22:56.844 --> 03:22:59.124
propagation if you are 
interested.  And these nodes are

03:22:59.125 --> 03:23:03.407
created when we created the 
decent optimizer.  You can 

03:23:03.408 --> 03:23:05.408
continue to any node of the 
graph and pause there.

03:23:09.309 --> 03:23:12.585
So, we have just continued to 
the m node in the dense layer.  

03:23:12.586 --> 03:23:15.446
And you can continue to the 
gradient at the matmul.  And we 

03:23:15.447 --> 03:23:20.191
did.  And you can see the 
summaries of the tensor values. 

03:23:20.192 --> 03:23:22.241
You can look although the data 
type, their shape and also the 

03:23:22.242 --> 03:23:24.242
range of their values.

03:23:27.967 --> 03:23:29.997
So, in the so-called health 
pills, you can look at how many 

03:23:29.998 --> 03:23:31.852
of those
values are zero and negative or 

03:23:31.853 --> 03:23:35.966
positive and so forth.  You 
hover over those, you can get 

03:23:35.967 --> 03:23:38.901
more information such as the 
mean and the standard deviation 

03:23:38.902 --> 03:23:40.902
of the values in the tensor.

03:23:43.800 --> 03:23:45.800
So, next, we can click these 
links to 

03:23:46.839 --> 03:23:48.839
open a detailed view of the 
tensors.

03:23:49.945 --> 03:23:52.610
You can apply  slicing to reduce
the dimensionality so it's easy 

03:23:52.611 --> 03:23:58.174
to look at the values.  We have 
reduced the dimension from 2 to 

03:23:58.175 --> 03:24:00.238
1, looking at it as a curve.  
Now continue to the loss tensor.

03:24:01.248 --> 03:24:05.142
Which is a scaler.  And yep.  
It's a scaler.  And the shape is

03:24:05.143 --> 03:24:08.846
an empty list as we can see 
here.  We can switch to the 

03:24:08.847 --> 03:24:11.742
history -- full history modes.  
We can look at how the value 

03:24:11.743 --> 03:24:13.743
changes as it is being trained.

03:24:16.260 --> 03:24:17.676
So, with the full history mode 
enabled, we can continue other 

03:24:17.677 --> 03:24:21.798
the sessions.  Like 50 of them. 
We can see in real-time how the 

03:24:21.799 --> 03:24:26.530
loss value decreases and how the
value on the Matmul changes.

03:24:30.213 --> 03:24:33.084
That's how you can use it as an 
X-ray animater for the models to

03:24:33.085 --> 03:24:36.973
have a better  understanding how
your model works.  Next, let's

03:24:36.974 --> 03:24:39.012
look at a broken model.

03:24:42.515 --> 03:24:45.767
That's the debug model we ship 
with TensorFlow.  That's the 

03:24:45.768 --> 03:24:47.768
only broken model we ship with 
TensorFlow as far as I know, and

03:24:49.877 --> 03:24:51.321
I'm proud to be the author of 
it.  We can see that the model 

03:24:51.322 --> 03:24:53.999
doesn't quite work.  After two 
iterations of training, the 

03:24:55.839 --> 03:24:57.839
accuracy is stuck at about 10%.

03:24:58.892 --> 03:25:01.566
We suspect there might be bad 
numerical values like not a 

03:25:01.567 --> 03:25:04.009
number or infinities.  But we're
not sure which nodes of the 

03:25:06.865 --> 03:25:08.865
graph are responsible for 
generating those infinities.

03:25:11.016 --> 03:25:14.085
To answer that question, we can 
use the debugger tool.  We do a 

03:25:14.086 --> 03:25:16.086
refresh in our browser.

03:25:18.803 --> 03:25:21.479
And then we can start our debug 
MNIST example to connect to the 

03:25:21.480 --> 03:25:23.480
debugger plugin.

03:25:24.768 --> 03:25:27.220
So, again, we're looking at the 
graph.  Now, in order to find 

03:25:27.221 --> 03:25:29.221
the nodes 

03:25:32.969 --> 03:25:34.969
responsible for that infinities,
we can look at the watch points.

03:25:36.022 --> 03:25:38.022
And use the conditional break 
points 

03:25:40.143 --> 03:25:42.011
feature to continue running the 
model until any tenser includes 

03:25:42.012 --> 03:25:44.012
it.

03:25:45.082 --> 03:25:47.533
You are  seeing a list of tensor
values.  Those a complete list 

03:25:47.534 --> 03:25:49.184
of tensors involved in training 
the

03:25:49.185 --> 03:25:51.255
model.  In a moment, the model 
is stopping 

03:25:54.507 --> 03:25:56.507
because it hit an infinity in 
the 

03:25:58.402 --> 03:26:00.028
tensor, cross entropy/log.  We 
can see in the health pill and 

03:26:00.029 --> 03:26:02.029
see 

03:26:03.131 --> 03:26:04.968
in the detailed tenser view 
those orange lines.  Showing you

03:26:04.969 --> 03:26:07.028
the infinity values.  Now, the 
question is, why do those 

03:26:08.264 --> 03:26:10.331
infinity values happen?  So, we 
can go back to the source code 

03:26:12.404 --> 03:26:14.404
and find the line of Python code
where 

03:26:16.098 --> 03:26:19.370
it's created and that's where 
it's tf.lot.  And we can open up

03:26:19.371 --> 03:26:22.230
the graph view and we see the 
inputs.  So, we can trace the 

03:26:22.231 --> 03:26:24.231
inputs.

03:26:26.526 --> 03:26:29.589
In this case, the input is the 
softmax tensor.  We can expand 

03:26:29.590 --> 03:26:32.057
and highlight and look at the 
value of the inputs, which is 

03:26:32.058 --> 03:26:34.058
softmax.

03:26:36.177 --> 03:26:39.049
There are, indeed, five values 
of the is tensor.  And the 

03:26:39.050 --> 03:26:42.325
reason for infinity is because 
we have log of zero.  With that 

03:26:42.326 --> 03:26:46.217
knowledge, we can go back to the
source code and fix it.  We're 

03:26:46.218 --> 03:26:48.218
not going to do this in this 
demo here.  All right.

03:26:52.384 --> 03:26:54.411
So, that's the TensorBoard 
debugging.  And I encourage you 

03:26:54.412 --> 03:26:56.679
to use it, explore it.  And 
hopefully it will help you 

03:26:57.293 --> 03:26:59.344
understand your
model better and help you fix 

03:26:59.345 --> 03:27:02.471
bugs much more quickly.  You can
just use this simple command 

03:27:03.925 --> 03:27:05.992
line, TensorBoard, with a 
special flag.  With that, I 

03:27:05.993 --> 03:27:07.993
would like to hand this back to 
Justine.

03:27:13.300 --> 03:27:15.300
[ Applause ]

03:27:17.446 --> 03:27:20.114
&gt;&gt; Well, thank you, Shanging.  I
thought that was a really 

03:27:20.725 --> 03:27:22.725
interesting demo.

03:27:27.067 --> 03:27:28.524
And it is -- and it was a great 
leap forward for TensorBoard.  

03:27:28.525 --> 03:27:30.572
And it really shows that one of 
the things we have been doing 

03:27:30.573 --> 03:27:35.288
recently is, rather than being a
read-only reporting tool, we're 

03:27:35.289 --> 03:27:37.289
trying to explore more 

03:27:38.993 --> 03:27:40.993
interactive directions as we 
have shown you today.

03:27:42.274 --> 03:27:44.274
These are things folks who are 

03:27:45.539 --> 03:27:46.975
productionizing TensorBoard, 
such as cube flow, should take 

03:27:46.976 --> 03:27:48.976
into consideration.

03:27:50.501 --> 03:27:53.163
We want to attract more  
contributors.  We have two 

03:27:53.164 --> 03:27:55.415
approaches for this where you 
can develop an artificial repo 

03:27:55.416 --> 03:28:00.518
and send us pull requests.  We 
do our work in the open.  This 

03:28:00.519 --> 03:28:05.209
does need approval on security 
footprint, et cetera.  And there

03:28:05.210 --> 03:28:07.210
is an escape hatch if that 
doesn't work out.

03:28:09.732 --> 03:28:12.422
You can independently develop 
plugins, you can create custom 

03:28:12.423 --> 03:28:14.270
static builds without anyone's 
approval.  You can do whatever 

03:28:14.271 --> 03:28:16.271
you want.  Because part of the 
goal on this team 

03:28:19.166 --> 03:28:21.166
is to liberate the tools.

03:28:23.314 --> 03:28:27.652
With that said, I want to thank
all of you for attending.  And 

03:28:27.653 --> 03:28:29.653
thank you watching on YouTube.

03:28:31.974 --> 03:28:33.974
If you like this talk, hashtag 
Tweeter or, you know, reach out.

03:28:34.421 --> 03:28:36.454
Thank you, again.

03:28:39.468 --> 03:28:41.468
[ Applause ]

04:30:15.243 --> 04:30:17.765
Test.  Test.

04:41:16.163 --> 04:41:18.163
&gt;&gt; Hi, everyone.

04:41:23.085 --> 04:41:25.085
Hope everybody had a good lunch.

04:41:28.746 --> 04:41:31.741
I'm Sarah Sirajuddin, an 
engineer on activity.  And my 

04:41:31.742 --> 04:41:34.406
colleague, Andrew Aselle, also 
on the same team.  And we are 

04:41:34.407 --> 04:41:36.063
excited to talk about the work 
we have been doing to bring 

04:41:36.064 --> 04:41:38.064
machine learning to mobile 
devices.

04:41:41.607 --> 04:41:43.607
So, in today's talk, we'll cover
three areas.

04:41:44.696 --> 04:41:45.729
First, how machine learning on 
devices is different and 

04:41:45.730 --> 04:41:47.730
important.  Then we'll talk 
about all the work 

04:41:52.670 --> 04:41:54.670
that we have been doing

04:41:57.039 --> 04:41:59.039
on TensorFlow Lite.

04:42:00.318 --> 04:42:02.003
And then how you can use it in 
your apps.  Let's talk about 

04:42:02.004 --> 04:42:04.004
your devices.

04:42:05.483 --> 04:42:08.173
Usually a device is a mobile 
device, basically a phone.  Our 

04:42:08.174 --> 04:42:10.174
phones are with us all the 
times.

04:42:12.469 --> 04:42:14.111
These days they have lots of 
censors giving rich data about 

04:42:14.112 --> 04:42:16.112
the world around us.

04:42:17.410 --> 04:42:19.410
And lastly, we use the phones 
all the time.

04:42:20.919 --> 04:42:22.919
Another category of devices is 
edge devices.

04:42:24.606 --> 04:42:26.650
And this industry has seen a 
huge growth in the last few  

04:42:26.651 --> 04:42:29.353
years.  By some estimates that 
are 23 billion 

04:42:33.269 --> 04:42:35.269
connected devices, smart 
speakers, smart 

04:42:37.406 --> 04:42:37.617
watches, smart sensors, with 
what

04:42:37.618 --> 04:42:42.387
have you.  And technology that 
only was available on the most 

04:42:42.388 --> 04:42:44.388
expensive  devices is now 
available on the cheaper ones.

04:42:47.322 --> 04:42:49.582
So, this rapid increase in the 
availability of these more and 

04:42:49.583 --> 04:42:51.583
more 

04:42:52.905 --> 04:42:54.555
capable devices has now opened 
up many opportunities for doing 

04:42:54.556 --> 04:42:57.833
machine learning on device.  In 
addition to that, though, there 

04:42:57.834 --> 04:43:01.114
are several other reasons why 
you may consider doing on device

04:43:01.115 --> 04:43:04.229
machine learning.  And probably 
the most important one is 

04:43:04.841 --> 04:43:06.841
latency.

04:43:07.963 --> 04:43:09.963
So, if you're processing 
streaming 

04:43:11.225 --> 04:43:13.077
data such as audio or video, 
then you don't want to be making

04:43:13.078 --> 04:43:16.220
calls back and forth to the 
server.  Other reasons are that 

04:43:16.221 --> 04:43:18.221
your processing 

04:43:19.308 --> 04:43:22.438
can happen even when your device
is offline.  Sensitive data can 

04:43:22.439 --> 04:43:25.119
stay on device.  It's more 
power-efficient because the 

04:43:27.157 --> 04:43:30.259
device is not sending data back 
and forth.  And lastly, we are 

04:43:30.260 --> 04:43:32.260
in a position to 

04:43:33.332 --> 04:43:34.990
take advantage of all the sensor
data that is already present on 

04:43:34.991 --> 04:43:39.331
the device.  So, all that is 
great.  But there's also a 

04:43:39.332 --> 04:43:41.332
catch.

04:43:42.608 --> 04:43:43.023
And the catch is that on-device 
machine

04:43:43.024 --> 04:43:45.499
learning is hard.  And the 
reason it is hard is that many 

04:43:48.198 --> 04:43:50.198
of these devices have some 
pretty tight constraints.

04:43:52.553 --> 04:43:54.810
Small batteries, low compute 
power, tight memory.

04:43:58.129 --> 04:44:01.289
And TensorFlow wasn't a great 
fit for this.  And that is the 

04:44:01.290 --> 04:44:03.290
reason we built TensorFlow Lite.

04:44:05.407 --> 04:44:07.648
Which is a light-weight library 
and tools for doing machine 

04:44:07.649 --> 04:44:11.202
learning on embedded and small 
platforms.  So, we launched 

04:44:11.203 --> 04:44:14.926
TensorFlow Lite late last year 
in developer preview.  And since

04:44:14.927 --> 04:44:17.205
then we have been working on 
adding more features and support

04:44:17.206 --> 04:44:21.955
to it.  So, I'll just walk you 
through the high-level design of

04:44:21.956 --> 04:44:27.088
the system.  We have the 
TensorFlow Lite format.  This is

04:44:27.089 --> 04:44:29.126
different from what TensorFlow 
uses and we had to do so for 

04:44:29.127 --> 04:44:31.623
reasons of efficiency.  Then 
there's the interpreter, which  

04:44:32.247 --> 04:44:34.247
runs on device.

04:44:35.585 --> 04:44:37.648
Then there are a set of 
optimized kernels.  And they are

04:44:37.649 --> 04:44:39.649
there are interface which is you
can use to take advantage of 

04:44:41.337 --> 04:44:43.337
hardware acceleration when it is
available.

04:44:45.283 --> 04:44:47.587
It's  cross-platform, so, it 
supports Android and iOS.  And

04:44:47.588 --> 04:44:52.095
I'm really happy to say today 
that we also have support for 

04:44:52.096 --> 04:44:54.136
Raspberry Pi and pretty much 
most other devices which are 

04:44:54.757 --> 04:44:56.757
running Linux.

04:44:57.803 --> 04:44:59.900
So, the developer workflow 
roughly is that you take a 

04:44:59.901 --> 04:45:02.185
trained TensorFlow model and 
then you convert it to the 

04:45:03.817 --> 04:45:06.490
TensorFlow Lite format using a 
converter.  And then you update 

04:45:06.491 --> 04:45:08.491
your apps to 

04:45:09.761 --> 04:45:13.101
invoke the interpreter using the
Java or C++ APIs.  One other 

04:45:13.102 --> 04:45:15.772
thing that I want to call out 
here is that iOS developers have

04:45:17.001 --> 04:45:19.001
another option.

04:45:20.914 --> 04:45:23.161
They can convert the trained 
TensorFlow graph into the CoreML

04:45:23.162 --> 04:45:25.162
graph 

04:45:26.677 --> 04:45:28.677
and use the CoreML run time 
directly.

04:45:31.203 --> 04:45:33.469
And this TensorFlow to CoreML 
converter is something we worked

04:45:33.470 --> 04:45:35.470
on 

04:45:36.542 --> 04:45:39.422
together with the folks that 
built CoreML.  There are top of 

04:45:39.423 --> 04:45:41.423
the mind every time you talk 
about TensorFlow Lite.

04:45:43.566 --> 04:45:47.245
The two most common, is it small
in size?  And is it fast?  Let's

04:45:47.246 --> 04:45:49.710
talk about the first one.  
Keeping TensorFlow Lite small 

04:45:49.711 --> 04:45:55.445
was a key goal for us when we 
started building this.  So, the 

04:45:55.446 --> 04:45:58.522
size of our interpreter is only 
75 kilobytes.  When you include

04:45:58.523 --> 04:46:03.901
all the supported ops, this is 
400 kilobytes.  Another thing 

04:46:03.902 --> 04:46:08.216
worth noting here is a feature 
called selective registration.  

04:46:08.217 --> 04:46:09.839
So, developers have the option 
to only include the ops that 

04:46:09.840 --> 04:46:13.539
their models need and link 
those.  And thereby keep the 

04:46:13.540 --> 04:46:18.268
footprint small.  So, how do we 
do this?  So, first of all, we 

04:46:18.269 --> 04:46:20.269
have been pretty 

04:46:23.411 --> 04:46:24.230
careful in terms of which 
dispense dependencies we 

04:46:24.231 --> 04:46:26.677
include.  And TensorFlow Lite 
uses flat buffers 

04:46:31.023 --> 04:46:34.523
which are more memory efficient 
than critical buffers.  And 

04:46:34.524 --> 04:46:36.784
moving on to the next question, 
which is performance.  

04:46:36.785 --> 04:46:39.262
Performance, a super-important 
goal for us.  And we made design

04:46:39.263 --> 04:46:41.263
choices throughout the system to
make it so.

04:46:45.002 --> 04:46:46.232
So, let's look at the first 
thing, which is the TensorFlow 

04:46:46.233 --> 04:46:48.233
Lite format.

04:46:50.745 --> 04:46:52.745
So, we use flat buffers to 
represent models.

04:46:57.024 --> 04:47:00.009
And FlatBuffer is it a  
cross-platform serialization 

04:47:00.010 --> 04:47:02.083
library developed for game 
performance, and since used in 

04:47:04.536 --> 04:47:06.801
other sensitive applications.  
And the advantage of using  

04:47:08.656 --> 04:47:10.707
FlatBuffers is we are able to
access data without going 

04:47:10.708 --> 04:47:15.669
through heavy weight, parsing or
steps of the large files which 

04:47:15.670 --> 04:47:18.523
contain weights.  Another thing 
we do at the conversion 

04:47:21.617 --> 04:47:24.298
is we look at the biases and 
that allows us to execute faster

04:47:24.299 --> 04:47:26.299
later on.

04:47:27.355 --> 04:47:29.232
The TensorFlow Lite interpreter 
uses static memory and execution

04:47:29.233 --> 04:47:31.233
plans which allows us to load up
faster.

04:47:35.395 --> 04:47:36.425
There are a set of optimized 
kernels which have been 

04:47:36.426 --> 04:47:38.426
optimized to run fast on 

04:47:40.963 --> 04:47:43.455
the NEON and ARM platforms.  We 
wanted to build TensorFlow Lite 

04:47:43.456 --> 04:47:45.456
so 

04:47:46.532 --> 04:47:48.790
that we can take advantage of 
all the innovations that are 

04:47:48.791 --> 04:47:53.287
happening in silicon for these 
devices.  So, the first thing 

04:47:53.288 --> 04:47:55.288
here is that 

04:47:57.533 --> 04:48:00.323
TensorFlow Lite supports the 
Android neural network API.

04:48:06.875 --> 04:48:08.875
The Qualcomm HVX is coming out 
soon.

04:48:10.612 --> 04:48:11.856
And MediaTek and others have 
announceed their integration 

04:48:11.857 --> 04:48:15.728
with Android neural network API.
So we should be seeing those in 

04:48:15.729 --> 04:48:19.665
the coming months as well.  
Second thing here is we have 

04:48:19.666 --> 04:48:22.776
also been working on adding 
direct GPU acceleration.

04:48:27.106 --> 04:48:29.106
And useing Metal on iOS.

04:48:30.598 --> 04:48:32.220
So, quantization is the last bit
that I want to talk about in the

04:48:32.221 --> 04:48:34.221
context of performance.

04:48:38.619 --> 04:48:40.619
So, roughly speaking, 
quantization is 

04:48:42.341 --> 04:48:44.393
this technique to store numbers 
and perform calculations on them

04:48:44.394 --> 04:48:47.563
in representations that are more
exact than 32-bit floating point

04:48:47.564 --> 04:48:51.863
numbers.  This is important for 
two reasons.  One, the smaller 

04:48:51.864 --> 04:48:53.864
the model, the better it is for 
these small devices.

04:48:57.380 --> 04:48:59.641
Second, many processers have 
specialized instruction sets 

04:48:59.642 --> 04:49:03.761
which process fixed point 
operations much  faster than 

04:49:03.762 --> 04:49:05.762
they process floating point 
numbers.

04:49:07.258 --> 04:49:09.326
So, a very naive way to do point
optimization would be to shrink 

04:49:09.327 --> 04:49:11.327
the 

04:49:13.411 --> 04:49:15.411
weights and activations after 
you're done training.

04:49:17.112 --> 04:49:19.334
But that leads to suboptimal 
accuracies.  So we have been 

04:49:19.335 --> 04:49:21.335
working on doing quantityization
at training time.

04:49:23.902 --> 04:49:27.204
And we have recently released a 
script which does this.  What we

04:49:27.205 --> 04:49:29.205
have seen is for architectures 

04:49:30.485 --> 04:49:32.104
and inception, we are able to 
get accuracies that are similar 

04:49:32.105 --> 04:49:37.206
to their floating point 
counterparts while seeing pretty

04:49:37.207 --> 04:49:39.207
impressive gains in the 
latencies.  So, I've

04:49:40.688 --> 04:49:41.495
talked about a bunch of 
different performance  

04:49:41.496 --> 04:49:46.838
optimizations.  Now let's see 
what all of these translate to 

04:49:46.839 --> 04:49:48.839
together in terms of numbers.

04:49:50.956 --> 04:49:53.428
So, these are two models that we
benchmarked and we ran them on 

04:49:53.429 --> 04:49:57.530
the Android Pixel 2 phone.  We 
were running these with four 

04:49:59.381 --> 04:50:01.381
threads and using all the four 
large cores of the Pixel 2.

04:50:04.513 --> 04:50:06.513
And what you see is that we are 
seeing 

04:50:09.166 --> 04:50:10.837
that these quantized models run 
three times faster on TensorFlow

04:50:10.838 --> 04:50:12.875
Lite than their floating point 
counterparts on TensorFlow.

04:50:16.342 --> 04:50:17.565
So, I will move on now and talk 
about whats supported on 

04:50:17.566 --> 04:50:19.566
TensorFlow Lite.

04:50:20.816 --> 04:50:22.450
So, currently, it is limited to 
inference only, although we are 

04:50:22.451 --> 04:50:24.711
going to be working on  
supporting training in the 

04:50:24.712 --> 04:50:29.488
future.  We support 50 
commonly-used operations which 

04:50:29.489 --> 04:50:32.343
developers can use in their own 
models.  In addition, they can 

04:50:32.344 --> 04:50:34.990
use any of these popular open 
source models that we support.

04:50:38.456 --> 04:50:41.142
One thing to note here is that 
we have an extensible design, 

04:50:41.143 --> 04:50:43.143
so, if a developer 

04:50:45.079 --> 04:50:46.957
is trying to use a model which
has an app not currently 

04:50:46.958 --> 04:50:48.958
supported, they 

04:50:50.014 --> 04:50:53.946
have the option to use what we 
call a custom op and use that.  

04:50:53.947 --> 04:50:56.183
And later in this talk, we will 
show you some code snippets on 

04:50:56.184 --> 04:50:58.184
how you can do that yourself.

04:51:00.490 --> 04:51:03.147
So, this is all theory about 
TensorFlow Lite.  Let me show 

04:51:03.148 --> 04:51:05.607
you a quick video of TensorFlow 
Lite in practice.

04:51:09.903 --> 04:51:11.903
So, we took this simple 
mobile-like 

04:51:13.183 --> 04:51:14.621
model and we retrain on some 
common objects that we could 

04:51:14.622 --> 04:51:16.890
find around our office.  And 
this is our demo classification 

04:51:18.746 --> 04:51:21.200
app, which is already open 
sourced.  As you can see, it is 

04:51:21.201 --> 04:51:23.487
able to classify these object 
objects.

04:51:28.436 --> 04:51:33.151
[ Laughter ]
So, that was demo.  Now let's 

04:51:33.152 --> 04:51:35.152
talk about production stuff.

04:51:36.651 --> 04:51:39.552
So, I'm very excited to say we 
have been working with other 

04:51:39.553 --> 04:51:44.487
teams in Google to bring 
TensorFlow Lite to Google apps. 

04:51:44.488 --> 04:51:46.488
So, the portrait mode on Android

04:51:52.275 --> 04:51:55.136
camera, 'Hey Google' on Google 
Assistant, Smart reply on Google

04:51:55.137 --> 04:51:57.645
OS, these are going to be 
powered by TensorFlow Lite in 

04:51:57.646 --> 04:51:59.646
the future.

04:52:01.124 --> 04:52:02.539
And I'm going to hand it off to 
Andrew who can tell you how to 

04:52:02.540 --> 04:52:07.176
use TensorFlow Lite.
&gt;&gt; Thanks for the introduction. 

04:52:07.177 --> 04:52:09.836
So, now that we see what 
TensorFlow is, or TensorFlow 

04:52:09.837 --> 04:52:13.298
Lite is, in particular, let's 
find out how to use it.  Let's 

04:52:13.299 --> 04:52:15.945
jump into the code.  So, the 
first step of the  four-step 

04:52:17.162 --> 04:52:19.626
process is to get a model.  You 
can download it off the Internet

04:52:20.647 --> 04:52:22.716
or you can train it yourself.  
Once you have a model, you need 

04:52:22.717 --> 04:52:26.433
to convert it into TensorFlow 
Lite format using our converter.

04:52:29.492 --> 04:52:31.492
There might be ops you want to 
spot 

04:52:33.700 --> 04:52:36.176
optimize using special 
intrinsics or hardware specific 

04:52:36.177 --> 04:52:38.177
to your application.  Or we 
don't support.

04:52:39.721 --> 04:52:42.401
write custom ops for that.  You 
can go to the app and write it 

04:52:45.059 --> 04:52:46.714
using the client API of your  
choice.  Look at the conversion 

04:52:46.715 --> 04:52:48.715
process.

04:52:51.678 --> 04:52:53.678
We support save model or frozen 
graph def.

04:52:55.351 --> 04:52:57.351
And we are showing the Python 
interface.

04:52:59.046 --> 04:53:01.046
We give it a direct directory of
a 

04:53:02.917 --> 04:53:05.594
save model, and it gives us a 
FlatBuffer out.  Before that's 

04:53:05.595 --> 04:53:07.595
done, there might be 

04:53:08.701 --> 04:53:10.701
some things you need to use to 
make this work better.

04:53:14.638 --> 04:53:19.966
The first thing is you need to 
use a frozen graphdef.  A lot of

04:53:19.967 --> 04:53:22.423
times a training graph has 
conditional logic or checks that

04:53:22.424 --> 04:53:24.424
are not 

04:53:25.898 --> 04:53:27.898
necessary for inference.

04:53:29.081 --> 04:53:32.179
Sometimes it's useful to create 
a special inference script.  

04:53:32.180 --> 04:53:34.858
Lastly, if you need to look at 
what the model is doing, the 

04:53:34.859 --> 04:53:39.199
TensorFlow is good, but we have 
the TensorFlow Lite visualizer 

04:53:39.200 --> 04:53:42.696
and looking at these compared to
each other can help you.  If you

04:53:42.697 --> 04:53:44.697
find issues, file them with 

04:53:45.990 --> 04:53:47.990
GitHub and we will respond to 
them as we get needs.

04:53:50.286 --> 04:53:53.181
So, lastly, write custom 
operator.  Let's see how to do 

04:53:53.182 --> 04:53:55.182
that.

04:53:57.318 --> 04:53:57.724
To write it in TensorFlow Lite 
is relatively

04:53:57.725 --> 04:54:02.459
simple.  The main function is 
invoke.  I've defined an 

04:54:02.460 --> 04:54:06.984
operator that returns PII.  A 
one scaler Pi.  Once you have 

04:54:06.985 --> 04:54:10.287
done that, you need to register 
the new operations.  There's a 

04:54:10.288 --> 04:54:12.288
number of ways to register 
operations.

04:54:14.405 --> 04:54:17.087
If you don't have custom ops and
don't need overriding, you can 

04:54:17.088 --> 04:54:21.189
use in the built-in resolver.  
But you might want to ship 

04:54:21.190 --> 04:54:24.677
binary that's much smaller.  You
might want selective 

04:54:24.678 --> 04:54:26.956
registration.  You should ship a
needed ops resolver 

04:54:30.056 --> 04:54:32.310
or include your custom ops in 
that same thing.  Once you have 

04:54:32.311 --> 04:54:36.834
the ops set, you just plug it 
into the interpreter.  Okay.  We

04:54:36.835 --> 04:54:39.723
have talked about custom 
operations.  Let's see how we 

04:54:39.724 --> 04:54:41.724
put this into 

04:54:43.648 --> 04:54:45.648
practice in the Java API.

04:54:47.133 --> 04:54:49.212
In Java, you put it in the 
interpreter, fill in the inputs 

04:54:49.213 --> 04:54:51.475
and outputs, and run run, which 
will 

04:54:55.770 --> 04:54:57.844
populate with the results of the
inference.  Really simple.  

04:54:57.845 --> 04:55:01.952
Next, how to include this?  
Compile a bunch of code?  We're 

04:55:01.953 --> 04:55:04.429
working hard to make it so you 
can use TensorFlow from the PIP 

04:55:04.430 --> 04:55:06.483
and do the training and you 
don't need

04:55:08.549 --> 04:55:10.549
to compile TensorFlow.

04:55:12.620 --> 04:55:15.498
We parade an Android Gradle file
and you don't need to compile 

04:55:15.499 --> 04:55:17.499
for an Android app.

04:55:22.953 --> 04:55:25.002
We have a similar thing for  
Cocoapods.  Once we know how to 

04:55:25.003 --> 04:55:27.897
use TensorFlow, look at the 
roadmap.  As we move forward, we

04:55:27.898 --> 04:55:31.788
are going to support more and 
more TensorFlow models out of 

04:55:31.789 --> 04:55:37.139
the box with more ops.  Second, 
add on-device training and look 

04:55:37.140 --> 04:55:39.140
at so you can do hybrid 
training.

04:55:40.221 --> 04:55:42.735
Some of it on your server, some 
on your device.  Wherefore it 

04:55:42.736 --> 04:55:46.458
makes sense.  Should be an 
option.  And include tooling to 

04:55:46.459 --> 04:55:48.547
analyze graphs better to do more
optimizations.

04:55:52.261 --> 04:55:54.133
We have more that we can talk 
about that we're working on, but

04:55:54.134 --> 04:55:56.413
hope this will make you 
interested and excited to try 

04:55:56.414 --> 04:55:58.414
it.  So, there's one remaining 
question 

04:56:02.385 --> 04:56:04.040
left, which is, should I use 
TensorFlow mobile or TensorFlow 

04:56:04.041 --> 04:56:06.504
Lite?  TensorFlow mobile is a 
stripped down 

04:56:10.980 --> 04:56:13.252
set of TensorFlow that  uses a 
subset of the ops.  We are going

04:56:13.253 --> 04:56:15.731
to improve TensorFlow Lite and 
its ability to map to custom 

04:56:16.540 --> 04:56:18.540
hardware.

04:56:20.115 --> 04:56:20.740
We recommend you target
TensorFlow Lite as soon as 

04:56:20.741 --> 04:56:23.818
possible if it's possible.  If 
there's functionality you need 

04:56:23.819 --> 04:56:27.764
in TensorFlow mobile, let us 
know and we'll work to improve 

04:56:27.765 --> 04:56:29.408
TensorFlow Lite ins a 
commensurate way.  Okay.  Demo 

04:56:29.409 --> 04:56:34.106
time.  So, nothing like a live 
demo, right?  So, let's switch 

04:56:34.107 --> 04:56:36.986
over to the demo feed and we'll 
talk about it.

04:56:40.048 --> 04:56:41.889
And, so, we saw some mobile 
phones.  Mobile phones are 

04:56:41.890 --> 04:56:44.757
really exciting, you know, 
because everybody has them.  But

04:56:44.758 --> 04:56:46.616
another thing that's happening 
is these edge  computing 

04:56:46.617 --> 04:56:48.617
devices.

04:56:50.537 --> 04:56:54.657
One of the most popular ones for
hobbyists is the Raspberry Pi.  

04:56:54.658 --> 04:56:57.799
I have built hardware around the
Raspberry Pi.  As we zoom in, we

04:56:57.800 --> 04:57:00.464
have the Raspberry Pi mobile 
board.  This is a system on 

04:57:00.465 --> 04:57:02.739
chip, similar to a cell phone 
chip.  And one of the great 

04:57:02.740 --> 04:57:06.616
things about the Raspberry Pi is
that they're really cheap.  

04:57:06.617 --> 04:57:09.916
Another great thing, they can 
interface to hardware.  Here 

04:57:09.917 --> 04:57:11.917
we're interfaced to a 
microcontroller that allow us to

04:57:12.576 --> 04:57:14.576
basically control these motors.

04:57:15.681 --> 04:57:17.681
These are server motors, common 
in RC cars.

04:57:17.927 --> 04:57:19.357
And they allow us to move the 
camera left and right and up and

04:57:19.358 --> 04:57:21.358
down.

04:57:24.216 --> 04:57:26.961
Essentially it's a camera 
gimbal.  And it's  connected to 

04:57:26.962 --> 04:57:32.274
a Raspberry Pi compatible 
camera.  What to do with this?  

04:57:32.275 --> 04:57:34.275
We showed the classification 
demo before.

04:57:36.197 --> 04:57:38.197
Let's look at an SSD example.  
Single shot detection.

04:57:40.910 --> 04:57:44.018
It can identify bounding boxes 
in an image.  Given an image, I 

04:57:44.019 --> 04:57:46.664
get multiple bounding  boxes.  
So, for example, we have an 

04:57:46.665 --> 04:57:48.665
apple here.  And it identifies 
an apple.

04:57:51.366 --> 04:57:53.259
Now, the really cool thing we 
can do with this, now that we 

04:57:53.260 --> 04:57:57.361
have the motor, we can tell it 
to center the apple.  We turned 

04:57:57.362 --> 04:58:00.250
on the motors, and they're 
active.  And as I move it  

04:58:00.251 --> 04:58:04.198
around, it's going to keep the 
apple as centered as possible.  

04:58:04.199 --> 04:58:06.862
If I go up, it will go up, if I 
go down, it will go down.  So, 

04:58:06.863 --> 04:58:10.938
this is really fun.  What could 
you use this for?  Well, if 

04:58:10.939 --> 04:58:13.184
you're a person, it can identify
you.  Currently I have that 

04:58:13.185 --> 04:58:16.063
filtered out.  So, if I stand 
back, it's going to center on 

04:58:16.064 --> 04:58:18.945
me.  So, I could use this as 
sort of a virtual

04:58:19.541 --> 04:58:24.065
videographer.  Imagine a 
professor wants to tape their 

04:58:24.066 --> 04:58:26.117
lecture, but they don't have a 
camera person.  This would be a 

04:58:26.118 --> 04:58:28.555
great way to do that.  I'm sure 
that all the hobbyists around 

04:58:31.043 --> 04:58:33.043
can now use TensorFlow in a 
really 

04:58:34.739 --> 04:58:36.579
simple way, can come up with 
many better applications than 

04:58:36.580 --> 04:58:38.580
what I'm showing here.

04:58:39.869 --> 04:58:41.869
But I find it fun and I theme 
you do.

04:58:43.352 --> 04:58:45.000
And I'm not an electrical or 
mechanical engineer, so you can 

04:58:45.001 --> 04:58:47.001
do this too.  All right.  Thanks
for showing the demo.

04:58:49.922 --> 04:58:51.922
Let's go back to the slides, 
please.

04:58:53.209 --> 04:58:55.671
So, I had a backup video just in
case it didn't work.  It's 

04:58:55.672 --> 04:58:56.935
always a good plan.
[ Laughter ]

04:58:56.936 --> 04:58:59.430
But we didn't need it.  So, 
that's great.  So, let's 

04:58:59.431 --> 04:59:01.431
summarize.

04:59:02.443 --> 04:59:04.443
So, what should you do?  I'm 
sure you want to use TensorFlow 

04:59:04.474 --> 04:59:06.474
Lite.  Where can you get it?

04:59:09.392 --> 04:59:10.013
You can get it on GitHub right 
around the TensorFlow 

04:59:10.014 --> 04:59:14.355
repository.  How you can find 
out about it is looking at the 

04:59:14.356 --> 04:59:16.356
TensorFlow Lite 

04:59:17.676 --> 04:59:19.676
documentation on the TensorFlow.
org website.

04:59:26.545 --> 04:59:29.615
And we have a  mailing list, 
tflite@tensorFlow.org.  And you 

04:59:29.616 --> 04:59:31.616
can tell us about issues and 

04:59:32.932 --> 04:59:35.398
what you use TensorFlow Lite 
for.  I hope to hear from all 

04:59:35.399 --> 04:59:37.399
you.  One at a time, please.

04:59:39.725 --> 04:59:42.194
Thanks, everybody, thanks Sarah 
for her presentation.  Thank 

04:59:42.195 --> 04:59:45.074
you, everybody around the world 
listening to this.  And in 

04:59:45.075 --> 04:59:46.735
addition, this was work that we 
worked very hard with other 

04:59:46.736 --> 04:59:50.439
members of the Google team, lots
of different teams.  So, there's

04:59:50.440 --> 04:59:54.308
a lot of work that went into 
this.  So, thanks a lot.

04:59:57.313 --> 04:59:59.313
[ Applause ]

05:00:04.276 --> 05:00:06.936
So, with that -- we have our 
next talk.  And where is our 

05:00:06.937 --> 05:00:08.937
next speaker?

05:00:10.042 --> 05:00:12.042
Our next speaker will be Vijay.

05:00:13.526 --> 05:00:15.526
And he's going to be talking 
about AutoML.

05:00:18.627 --> 05:00:21.091
&gt;&gt; Thank you very much.  Hi, 
everybody.  My name is Vijay.

05:00:24.197 --> 05:00:26.041
And today I'll be talking to 
you, or hopefully convincing 

05:00:26.042 --> 05:00:28.304
you, that when we try to apply 
machine learning to solving 

05:00:29.729 --> 05:00:32.403
problems, that we should really 
be thinking about designing 

05:00:32.404 --> 05:00:36.363
search spaces over solutions to 
those problems.  And then we can

05:00:36.364 --> 05:00:38.818
use automated machine learning 
techniques in order to evaluate 

05:00:40.501 --> 05:00:42.782
our ideas much more efficiently.
I think a big reason why a lot 

05:00:42.783 --> 05:00:44.836
of us are here today is due to 
the incredible 

05:00:49.375 --> 05:00:52.508
impact that machine learning can
have on practical problems.  Two

05:00:52.509 --> 05:00:54.509
often-cited reasons is that we 

05:00:55.591 --> 05:00:56.834
have increasing amounts of 
compute capability and access to

05:00:56.835 --> 05:00:59.901
data to train on.  But I think 
one other aspect is all of you, 

05:00:59.902 --> 05:01:03.442
right?  There's so many more 
people involved in many machine 

05:01:03.443 --> 05:01:05.948
learning today that are 
contributing and publishing 

05:01:05.949 --> 05:01:07.949
ideas.

05:01:09.007 --> 05:01:11.282
So, this graph tries to put this
into perspective by measuring 

05:01:11.283 --> 05:01:14.784
how many machine learning papers
are published on archive every 

05:01:14.785 --> 05:01:16.863
year since 2009.  And plotting 
that

05:01:20.935 --> 05:01:23.995
against a Moore's Law 
exponential growth curve.  As 

05:01:23.996 --> 05:01:26.624
you can see here, we have been 
keeping up with Moore's law, 2X,

05:01:26.625 --> 05:01:30.496
every year.  And this is 
demonstrating how many new ideas

05:01:30.497 --> 05:01:31.710
are being developed in the 
field.  This is a great thing, 

05:01:31.711 --> 05:01:33.711
right?

05:01:35.186 --> 05:01:37.691
So, one concrete way of looking 
at this is in the field of 

05:01:37.692 --> 05:01:39.692
computer vision, 

05:01:41.204 --> 05:01:43.869
we have seen top one image and 
accuracy start from the 50% 

05:01:43.870 --> 05:01:45.870
range from the 

05:01:47.793 --> 05:01:49.672
AlexNet architecture, which, by 
the way, revolutionized the 

05:01:49.673 --> 05:01:51.313
field of image classification.  
And every year we have been 

05:01:51.314 --> 05:01:53.314
getting 

05:01:55.226 --> 05:01:57.729
better and better up until 2017.
Now, these improvements haven't 

05:01:57.730 --> 05:02:02.257
come just because we have been 
training bigger models, right?  

05:02:02.258 --> 05:02:04.310
These improvements have also 
come from the fact that we have 

05:02:04.311 --> 05:02:06.311
lots of great ideas, right?

05:02:07.820 --> 05:02:09.820
Things like, batch  
normalization, 

05:02:11.323 --> 05:02:11.925
residual or skip connections and
various regularization 

05:02:11.926 --> 05:02:16.285
techniques.  Now, each of these 
points, like Jeff mentioned 

05:02:16.286 --> 05:02:19.795
earlier, is the result of years 
of research effort.  And we 

05:02:19.796 --> 05:02:21.865
build on each other's ideas. 
But one of the challenging 

05:02:21.866 --> 05:02:25.781
things is how do we keep up with
so much -- so many ideas that 

05:02:25.782 --> 05:02:31.541
are being produced?  And Is want
to zoom in a little bit in terms

05:02:31.542 --> 05:02:33.542
of the complexity of some of 
these models.

05:02:34.642 --> 05:02:36.642
So, we're going to zoom in a 
little 

05:02:39.817 --> 05:02:41.817
bit on InceptionV4 and look at 
the idea  embedded in there.

05:02:45.194 --> 05:02:47.194
These are models within the 
architecture.

05:02:49.686 --> 05:02:51.134
Every one of the arrows and  
operations was designed by a 

05:02:51.135 --> 05:02:53.135
human.  Somebody wrote some code
in order to 

05:02:55.891 --> 05:02:58.124
specify all of these little 
details.  Now, there are 

05:02:58.125 --> 05:03:00.125
high-level reasons why this kind
of architecture might make 

05:03:00.813 --> 05:03:02.813
sense.

05:03:03.935 --> 05:03:06.220
But our theory doesn't really 
explain with so much certainty 

05:03:06.221 --> 05:03:09.322
how every detail seems to 
matter.  And as a field, I 

05:03:09.323 --> 05:03:12.612
think, we're definitely working 
on trying to improve the theory 

05:03:12.613 --> 05:03:14.613
behind this.

05:03:16.147 --> 05:03:17.577
But for many of us, we're happy 
to use this kind of complexity 

05:03:17.578 --> 05:03:20.475
out of the box if we can.  
Because it really helps to solve

05:03:20.476 --> 05:03:24.581
problems.  Now, this isn't too 
surprising.  We know that 

05:03:24.582 --> 05:03:26.659
because machine
learning has had such an impact 

05:03:26.660 --> 05:03:30.397
on real products, that we're 
going to be willing to use 

05:03:30.398 --> 05:03:32.475
anything we possibly can.  And 
even if we don't understand all 

05:03:34.137 --> 05:03:36.380
the little minor details.  As 
long as it solves our problems 

05:03:36.381 --> 05:03:38.381
well 

05:03:39.493 --> 05:03:41.493
and hopefully are 
understandable.

05:03:43.042 --> 05:03:44.712
So, given all these ideas, how 
can we harness this explosion of

05:03:44.713 --> 05:03:46.713
ideas much more efficiently?

05:03:48.671 --> 05:03:50.671
So, let's step back and kind of 
ask a 

05:03:51.936 --> 05:03:54.183
few questions that we might have
heard when we were trying to 

05:03:54.184 --> 05:03:58.097
train machine learning models.  
Simple, but hard questions.  

05:03:58.098 --> 05:04:00.992
What learning rate should I 
apply for my optimization?  If 

05:04:00.993 --> 05:04:03.256
I'm training a deep neural 
network model, what dropout rate

05:04:03.257 --> 05:04:07.977
should I apply?  How do we 
answer this question today?  I 

05:04:07.978 --> 05:04:09.978
think we combine a few different
types of benefits.

05:04:12.890 --> 05:04:14.927
One of them is leveraging 
research or intuition and 

05:04:14.928 --> 05:04:17.368
engineering intuition.  What 
this means is that we start with

05:04:19.602 --> 05:04:21.602
code, or we ask our colleagues, 
hey, 

05:04:23.088 --> 05:04:24.915
what are good settings for these
fields?  If it were the case 

05:04:24.916 --> 05:04:26.916
that there was one setting that 
worked for

05:04:27.562 --> 05:04:29.430
everybody, we wouldn't be 
looking at these  parameters.  

05:04:29.431 --> 05:04:31.686
But it does matter.  So, then, 
we move on to some trial and 

05:04:32.291 --> 05:04:36.615
error process.  We try a certain
setting and see how well it 

05:04:36.616 --> 05:04:39.503
works on our problem and we 
continue to iterate.  And I 

05:04:39.504 --> 05:04:41.504
think the other aspect, which is

05:04:42.576 --> 05:04:44.848
becoming more common,  
hopefully, is increasing access 

05:04:44.849 --> 05:04:49.567
to compute and data by which we 
can evaluate these ideas.  So, 

05:04:49.568 --> 05:04:52.877
this combination is really ripe 
for automation, right?  And not 

05:04:52.878 --> 05:04:54.878
surprisingly, this exists today.

05:04:56.800 --> 05:04:58.800
It's  called hyperparameter 
optimization.

05:05:00.296 --> 05:05:02.770
And this kind of setup, we have 
a tuner giving out these 

05:05:02.771 --> 05:05:05.469
hyperparameter settings.  We 
have a train their trains our 

05:05:05.470 --> 05:05:10.001
model on our dataset and then 
tries to give some kind of 

05:05:10.002 --> 05:05:13.357
signal about how good those  
settings were.  And it might 

05:05:13.358 --> 05:05:17.085
give a validation accuracy of 
some value.  And the tuner can 

05:05:17.086 --> 05:05:19.140
then learn from this feedback to
find better points from the 

05:05:19.345 --> 05:05:21.394
search space.  And, you know, 
this is an  existing 

05:05:25.967 --> 05:05:27.608
big field and there are existing
systems

05:05:27.609 --> 05:05:28.837
like those shown at the very 
bottom that can help you to do 

05:05:28.838 --> 05:05:34.096
this.  But now let's ask a few 
more complicated or detailed 

05:05:34.097 --> 05:05:36.162
questions that I think people do
often ask as well.

05:05:39.665 --> 05:05:41.924
Why do you use batchnorm before 
relu?  I switched the order and 

05:05:41.925 --> 05:05:45.059
it seems to work better.  If 
you're trying to train a 

05:05:45.060 --> 05:05:50.003
completely new model, use one 
type of sub architecture or 

05:05:50.004 --> 05:05:52.459
another type of sub 
architecture?  Now, if you think

05:05:52.460 --> 05:05:55.122
about it, these questions aren't
really that different from 

05:05:55.123 --> 05:05:57.123
hyperparameter settings.

05:05:58.811 --> 05:06:00.655
So, if we think of 
hyperparameter optimization as 

05:06:00.656 --> 05:06:05.349
searching over a specific domain
of ideas, then it seems possible

05:06:05.350 --> 05:06:06.991
that maybe we can actually treat
the decisions made in this type 

05:06:06.992 --> 05:06:11.281
of model as another form of 
searching over a domain of 

05:06:11.282 --> 05:06:15.235
ideas.  And we can therefore 
think about deemphasizing any 

05:06:15.236 --> 05:06:17.236
specific decision that we make 
on our architectures.

05:06:20.203 --> 05:06:20.827
And instead think about the 
surplus of ideas that we might 

05:06:20.828 --> 05:06:22.828
have.

05:06:24.360 --> 05:06:26.027
So, let's take a concrete 
example of a search space design

05:06:26.028 --> 05:06:28.081
that my colleague Barrett did 
where

05:06:32.619 --> 05:06:35.881
he tried to design a search 
space for a convolutional cell. 

05:06:35.882 --> 05:06:39.573
I'll walk you through how you 
might design such a work space. 

05:06:39.574 --> 05:06:43.230
So, the first question is, you 
have to get your inputs.  Might 

05:06:43.231 --> 05:06:45.880
say you have access to the 
previous input.  And if you want

05:06:45.881 --> 05:06:48.114
support for skip connections, 
you might have the previous, 

05:06:48.115 --> 05:06:52.857
previous input.  So, the first 
job in the search space is to 

05:06:52.858 --> 05:06:55.308
define which inputs I'm going 
select.  And then, once you have

05:06:55.309 --> 05:06:59.240
those inputs selected, you want 
to then figure out what 

05:06:59.241 --> 05:07:02.106
operation should I apply to each
of those inputs before  summing 

05:07:02.107 --> 05:07:05.184
them together?  So, I might 
select something like 

05:07:10.137 --> 05:07:12.824
three by three convolution or 
three by three maxpooling and 

05:07:12.825 --> 05:07:17.954
combine those together.  We can 
then recursively turn that crank

05:07:17.955 --> 05:07:19.816
and apply it several more times 
where we use different 

05:07:19.817 --> 05:07:21.817
operations for different inputs.

05:07:23.517 --> 05:07:24.946
And we can even use the 
intermediate outputs of previous

05:07:24.947 --> 05:07:27.624
decisions in our search.  And 
then finally, you take all of 

05:07:27.625 --> 05:07:29.625
your 

05:07:31.140 --> 05:07:33.140
outputs that are unused and you 
concatenate them together.

05:07:34.642 --> 05:07:36.642
And that is your convolutional 
cell.

05:07:38.973 --> 05:07:40.973
And if you want to build your 
model, 

05:07:42.439 --> 05:07:43.908
like ResNet, stack them 
together.  This is one point 

05:07:43.909 --> 05:07:45.909
from the search space of ideas.

05:07:48.020 --> 05:07:50.020
There are a billion possible 
ways to 

05:07:51.078 --> 05:07:53.078
construct a cell like this in 
the search space.

05:07:57.079 --> 05:08:00.785
Changing the list and the way 
the connections can be made.  

05:08:00.786 --> 05:08:02.786
Now that we've designed our 
search 

05:08:04.721 --> 05:08:06.943
space, we go back to the 
hyperparameter tuning system.  

05:08:06.944 --> 05:08:09.614
We have a program generator on 
the left that generates samples 

05:08:09.615 --> 05:08:11.615
from this search space.

05:08:13.322 --> 05:08:15.615
We then train and evaluate on 
the task at hand.  Oftentimes a 

05:08:15.616 --> 05:08:19.893
proxy task.  And iterate to 
quickly find what are the best 

05:08:19.894 --> 05:08:22.367
programs from our search space? 
And the system on the left, in 

05:08:22.368 --> 05:08:24.632
program generator, can 
optionally learn from feedback.

05:08:27.701 --> 05:08:30.157
So, it might use something like 
reinforcement learning, 

05:08:30.158 --> 05:08:32.158
revolutionary 

05:08:34.083 --> 05:08:36.083
algorithms, or even search can 
work well in certain situations.

05:08:37.169 --> 05:08:39.848
So, we did this type of 
approach.  We took this 

05:08:39.849 --> 05:08:41.849
convolutional cell.

05:08:44.437 --> 05:08:45.867
Trained it on proxy task to make
quick

05:08:45.868 --> 05:08:47.926
progress on the evaluation of an
idea.  And then we took the best

05:08:47.927 --> 05:08:52.212
candidate cells that with found 
from that search.  We enlarged 

05:08:52.213 --> 05:08:54.467
in terms of the number of 
filters and the number of times 

05:08:54.468 --> 05:08:56.468
we 

05:08:57.748 --> 05:09:02.068
stacked it and applied it to the
ImageNet dataset.  These are two

05:09:02.069 --> 05:09:06.209
found from the search.  Looking 
at the results, you can see that

05:09:06.210 --> 05:09:07.883
we were able to do better than 
the existing state of the art 

05:09:07.884 --> 05:09:09.884
models in 

05:09:13.456 --> 05:09:15.718
terms of top line accuracy top 1
accuracy.  This effort was an 

05:09:15.719 --> 05:09:17.763
example where we took a model 
where decisions were pretty 

05:09:20.229 --> 05:09:21.449
complex and we honestly found 
another complex model that was 

05:09:21.450 --> 05:09:23.450
better.

05:09:25.185 --> 05:09:27.461
But next I'll show you an 
example where we can use this 

05:09:27.462 --> 05:09:29.462
general technique 

05:09:30.538 --> 05:09:32.538
to find even more interpretable 
outputs.

05:09:34.006 --> 05:09:36.006
So, let's look at optimization 
update rules.

05:09:40.340 --> 05:09:43.286
Most of you are probably 
familiar with stochastic 

05:09:43.287 --> 05:09:45.287
gradient decent.

05:09:46.612 --> 05:09:48.435
And shown on the left, gradient 
by the learning weight and 

05:09:48.436 --> 05:09:50.436
delta.

05:09:51.725 --> 05:09:53.725
And then we have Adam, these can
be  

05:09:55.246 --> 05:09:57.328
expressed fairly concisely just 
by being given the moving 

05:09:57.329 --> 05:09:59.329
average of the
gradient and so forth.

05:10:02.060 --> 05:10:04.513
But we really only have a 
handful of these type of 

05:10:04.514 --> 05:10:07.187
optimization update rules that 
we typically apply for deep 

05:10:07.795 --> 05:10:12.530
learning, for example.  What if 
we,  instead, treat these update

05:10:12.531 --> 05:10:14.531
equation rules as part of a 
larger search space?

05:10:18.698 --> 05:10:21.588
And so, you can take these 
expressions and turn them into a

05:10:21.589 --> 05:10:27.201
data flow graph that uses the 
optimization update rule.  We 

05:10:27.202 --> 05:10:29.840
can express them using this 
simple tree, but also a lot of 

05:10:29.841 --> 05:10:32.086
other ideas.  And so, you can 
then turn this crank 

05:10:35.378 --> 05:10:37.378
on this new search space and try
to find 

05:10:38.459 --> 05:10:40.459
a better optimization update 
rule.

05:10:43.792 --> 05:10:46.490
So, my colleagues ran that 
experiment.  They took a fixed  

05:10:46.491 --> 05:10:48.491
convolutional model 

05:10:50.379 --> 05:10:52.426
and tried to search over the  
fixed rules.  They found 

05:10:52.427 --> 05:10:54.688
optimizations that did better 
than what I have shown you on 

05:10:55.506 --> 05:10:58.370
this particular task.  One nice 
feature of this search space, 

05:11:00.653 --> 05:11:02.338
the results are more 
interpretable.  The fourth 

05:11:02.339 --> 05:11:04.339
update rule here.

05:11:05.411 --> 05:11:07.411
Taking the gradient and 
multiplying it by an expression.

05:11:08.695 --> 05:11:10.695
The gradient and the moving 
average

05:11:11.400 --> 05:11:13.047
the gradient agree in your 
direction, you should take a 

05:11:13.048 --> 05:11:16.126
bigger step in that direction.  
And that they disagree that we 

05:11:16.127 --> 05:11:18.974
should make a smaller step.  
This is actually a form of 

05:11:18.975 --> 05:11:21.641
momentum.  And so, one thing we 
can get from this 

05:11:24.941 --> 05:11:26.941
is maybe we should be designing 
search 

05:11:28.259 --> 05:11:30.100
spaces that have more notions 
encoded in the search space 

05:11:30.101 --> 05:11:37.546
ideas.  We may be able to find 
even better results.  So, so far

05:11:37.547 --> 05:11:41.682
I have focused on techniques and
search space ideas where we care

05:11:41.683 --> 05:11:43.683
about accuracy.

05:11:44.844 --> 05:11:46.688
But what's great about searching
over many ideas, we night have 

05:11:46.689 --> 05:11:52.229
the potential to search over 
more than just accuracy.  For 

05:11:52.230 --> 05:11:55.318
example, a lot of us care about 
inference speed.  We want to 

05:11:55.319 --> 05:11:57.319
take a model and deploy it 

05:11:58.438 --> 05:12:00.914
on real hardware, real mobile 
platform.  And we take a lot of 

05:12:00.915 --> 05:12:03.596
time and try to figure out how 
to take one idea and make it 

05:12:03.597 --> 05:12:08.721
fast enough.  But what if could,
as part of the search space of 

05:12:08.722 --> 05:12:11.842
ideas, finds ones that balance 
both speed and accuracy?  So, we

05:12:11.843 --> 05:12:13.843
tried to do this experiment 

05:12:14.944 --> 05:12:16.806
where we included the run time 
on a

05:12:16.807 --> 05:12:17.226
real mobile device as part of 
this inner loop of the 

05:12:17.227 --> 05:12:19.227
evaluation.

05:12:21.953 --> 05:12:23.603
So, we tried to focus to 
optimize on both accuracy as 

05:12:23.604 --> 05:12:25.905
well as inference speed.  And as
this process goes on over time, 

05:12:28.572 --> 05:12:30.434
program generator is able to 
find faster models while also 

05:12:30.435 --> 05:12:32.435
figuring out how to 

05:12:34.154 --> 05:12:36.154
make those models even more 
accurate.

05:12:37.905 --> 05:12:39.905
One interesting side effect of 
this is 

05:12:41.175 --> 05:12:44.028
that when you run searches over 
ideas, the output is actually 

05:12:44.029 --> 05:12:46.304
not just one model, it's a 
culture of models that 

05:12:47.936 --> 05:12:49.781
implicitly codes this tradeoff. 
This shows you we have points 

05:12:49.782 --> 05:12:51.782
along 

05:12:53.297 --> 05:12:54.328
the space that provide a 
tradeoff between inference speed

05:12:54.329 --> 05:12:58.877
on a mobile platform and 
accuracy on the dataset that 

05:12:58.878 --> 05:13:01.580
we're trying to solve.  Rather 
than manually engineering the 

05:13:03.665 --> 05:13:05.709
one point I want to get working,
I can get a result that can 

05:13:05.710 --> 05:13:10.896
maybe be  deployed on various 
types of platforms.  So, I'll 

05:13:10.897 --> 05:13:14.371
emphasize this in maybe a 
slightly different way.  Which 

05:13:14.372 --> 05:13:16.822
is that we could define a search
space of ideas in TensorFlow, 

05:13:16.823 --> 05:13:18.823
and through this automatic 
machine learning

05:13:21.720 --> 05:13:22.336
process, we could get models 
that have a guarantied run time 

05:13:22.337 --> 05:13:25.893
performance target on a target 
platform device.  And one of the

05:13:25.894 --> 05:13:27.894
nice things about 

05:13:28.965 --> 05:13:30.009
having an integrated ecosystem 
like  TensorFlow, you can just 

05:13:30.010 --> 05:13:32.010
use the 

05:13:33.089 --> 05:13:34.533
libraries that convert from 
program program to program to 

05:13:34.534 --> 05:13:37.221
you can get this end to end 
pipeline working well together.

05:13:40.530 --> 05:13:43.185
There's nothing required to 
specifically tune a model.  Let 

05:13:43.186 --> 05:13:46.070
me conclude by returning to this
process of  evaluating ideas in 

05:13:46.071 --> 05:13:52.057
this world where we're trying to
explore different ideas.  The 

05:13:52.058 --> 05:13:54.058
first is that we designed search

05:13:56.610 --> 05:13:59.468
spaces to try to test out a 
large set of possible ideas.  

05:13:59.469 --> 05:14:01.734
Note that when we designed the 
search space, that required 

05:14:01.735 --> 05:14:06.073
human intuition.  There's a need
for human ingenuity as part of 

05:14:06.074 --> 05:14:08.074
this process.

05:14:09.183 --> 05:14:11.183
So, designing the search space 

05:14:12.280 --> 05:14:14.544
properly takes a lot of efforts,
but you can evaluate many more 

05:14:14.545 --> 05:14:17.663
ideas much more quickly.  When 
it comes to trial and error, we 

05:14:19.305 --> 05:14:20.952
had to think about how software 
should be changed so that we can

05:14:20.953 --> 05:14:24.642
permit this type of search 
process.  So, for example, I 

05:14:24.643 --> 05:14:28.994
think many of us have probably 
written scripts where you take 

05:14:28.995 --> 05:14:30.861
things like learning rate and 
dropout rate as command line 

05:14:30.862 --> 05:14:32.862
flags.

05:14:34.354 --> 05:14:37.661
What if you wanted to test out 
deeper ideas in your programs?  

05:14:37.662 --> 05:14:39.757
How do you design a program 
that's much more tuneable at all

05:14:39.758 --> 05:14:42.658
levels of your program?  I think
this is a big question for us to

05:14:42.659 --> 05:14:48.253
tackle.  And lastly, we think 
these ideas will become 

05:14:48.254 --> 05:14:50.760
increasingly relevant as many of
you get access to more and more 

05:14:53.027 --> 05:14:54.871
computation capabilities such as
TPU pods.  Imagine a world where

05:14:54.872 --> 05:14:57.354
all you have to do is take your 
idea, submit it to an 

05:15:01.334 --> 05:15:03.373
idea bank and you have a pod of 
TPUs crunching overnight to 

05:15:03.374 --> 05:15:05.374
figure out which solution 
organization ideas are the best 

05:15:06.271 --> 05:15:08.987
and then waking up in the 
morning and it telling you, 

05:15:08.988 --> 05:15:10.988
these were the good ideas, these
were the bad ideas and so forth.

05:15:12.304 --> 05:15:14.568
I think part of the reason this 
excites me is that automatic 

05:15:14.569 --> 05:15:17.449
machine learning can keep these 
machines much more busy than we 

05:15:17.450 --> 05:15:19.450
can.  We have to
sleep.

05:15:23.714 --> 05:15:25.393
But machines can keep on 
churning 24/7.  So, with that, 

05:15:25.394 --> 05:15:29.495
thanks for listening.
[ Applause ]

05:15:33.804 --> 05:15:35.804
And next up is Ian who will be 
talking 

05:15:37.504 --> 05:15:39.504
to you about fusion plasmas.

05:15:47.392 --> 05:15:49.392
So, I want to talk to you about 

05:15:50.487 --> 05:15:52.487
something that's very important 
to me.

05:15:53.597 --> 05:15:56.100
And that's how will civilization
power itself for the next 

05:15:56.101 --> 05:15:58.101
hundred years?

05:15:59.791 --> 05:16:02.917
So, in 2100, the projected 
world's population is 11.2 

05:16:02.918 --> 05:16:04.918
billion.  If all 11.

05:16:07.853 --> 05:16:09.853
2 billion people to want enjoy 
the same 

05:16:12.207 --> 05:16:14.207
power usage that we do now in 
the United 

05:16:15.968 --> 05:16:17.968
States, that's going to require 
burning around .

05:16:21.591 --> 05:16:23.259
2yata joules of energy over the 
next hundred years.  That's a 

05:16:23.260 --> 05:16:25.260
whole lot.

05:16:26.786 --> 05:16:29.091
So, to put that in perspective, 
if we wanted to do that with oil

05:16:29.092 --> 05:16:31.092
alone, we 

05:16:32.201 --> 05:16:34.249
would are have to ramp up oil 
production by a factor of 10 for

05:16:34.250 --> 05:16:40.339
the next hundred years.  There's
no way that's going to happen.  

05:16:40.340 --> 05:16:42.340
Besides being infeasible, that 
would 

05:16:43.914 --> 05:16:45.914
contribute to catastrophic 
climate change.

05:16:49.925 --> 05:16:51.925
If we want to keep climate 
change to a 

05:16:55.103 --> 05:16:57.103
not ideal, but reasonable, say, 
2 

05:16:59.426 --> 05:17:01.426
degrees temperature increase, 
only 1.

05:17:03.751 --> 05:17:05.751
2% can come from coal or oil.

05:17:07.262 --> 05:17:09.262
Where does the other come from?

05:17:10.362 --> 05:17:12.362
One possible source would be 
nuclear fusion. 

05:17:14.471 --> 05:17:17.910
So, fusion involves pushing 
together two smaller nuclei.  

05:17:17.911 --> 05:17:19.943
What you get out is a whole lot 
of energy.  And no greenhouse 

05:17:19.944 --> 05:17:21.944
gas.

05:17:23.897 --> 05:17:25.897
So, right now the sun runs on 
nuclear fusion.

05:17:29.827 --> 05:17:31.827
And the reaction is so 
energy-dense that .

05:17:40.107 --> 05:17:43.882
2 yatajewels would require a 
trivial amount of boron.

05:17:48.277 --> 05:17:50.277
So, so far it sounds like a 
miracle fuel.  What's the catch?

05:17:50.956 --> 05:17:53.206
Well, the difficulty is that 
people have been trying this for

05:17:53.207 --> 05:17:56.301
70 years and so far no one has 
gotten out more energy than they

05:17:56.302 --> 05:17:58.302
put in.

05:17:59.568 --> 05:18:01.660
So, to understand this, you have
to imagine that the -- well, the

05:18:01.661 --> 05:18:04.119
reaction takes place inside of a
plasma.

05:18:07.474 --> 05:18:09.474
And the plasma is a million plus

05:18:11.623 --> 05:18:14.104
degree swarm of charged 
particles.  And these particles 

05:18:14.105 --> 05:18:16.105
don't to want stay in place.

05:18:17.581 --> 05:18:19.426
The sun uses a gravitational 
force to keep everything in 

05:18:19.427 --> 05:18:21.427
place.  We can't do that.

05:18:24.415 --> 05:18:26.902
So, instead, we use magnets.  
Now, magnets, you try to squeeze

05:18:26.903 --> 05:18:30.782
it with magnets and they can pop
out the end.  And you can get 

05:18:30.783 --> 05:18:33.263
little turbulent ripples.  And 
what happens is the plasma 

05:18:33.264 --> 05:18:35.540
breaks up, it gets unstable.  It
gets

05:18:35.959 --> 05:18:40.244
cooler.  And then the reaction 
stops.  And that's what's been 

05:18:40.245 --> 05:18:44.164
happening for 70 years.  So, 
this is the kind of problem that

05:18:44.165 --> 05:18:46.165
I like.

05:18:48.921 --> 05:18:50.579
It combines physics, 
probability, computation, 

05:18:50.580 --> 05:18:52.580
mathematics.

05:18:53.661 --> 05:18:55.661
And so, I was like, I want to 
work on this.

05:18:57.606 --> 05:18:59.606
How can we accelerate progress?

05:19:01.756 --> 05:19:05.870
Well, so, Google is not  
building a fusion reactor.  What

05:19:05.871 --> 05:19:07.871
we have done is we have 
partnered 

05:19:10.160 --> 05:19:12.602
with TAE technologies, the 
world's largest private fusion 

05:19:12.603 --> 05:19:15.913
energy company.  And we have 
been working with them since 

05:19:15.914 --> 05:19:17.914
2015.

05:19:21.084 --> 05:19:23.084
So, pictured here is their fifth

05:19:24.146 --> 05:19:26.146
generation plasma generation 
device.  And this thing is huge.

05:19:29.477 --> 05:19:31.477
It would fill up a large part of
this room.

05:19:33.202 --> 05:19:35.202
And then in the center we have 
-- is 

05:19:36.328 --> 05:19:38.328
where the  Applause ] many 
plasma is kept.

05:19:40.217 --> 05:19:42.217
This is elongated toriod.

05:19:43.283 --> 05:19:44.302
And the goal is to keel this in 
its place and prevent 

05:19:44.303 --> 05:19:49.449
turbulence.  If it gets out of 
place, then the reaction stops. 

05:19:49.450 --> 05:19:51.550
So, there's magnets and neutral 
beams and a host of other 

05:19:51.551 --> 05:19:53.601
technologies to keep it in 
place.

05:19:56.805 --> 05:19:58.805
Now, what's Google's job 
specifically? 

05:20:00.529 --> 05:20:02.593
Well, our goal is to take the  
measurements that come from this

05:20:04.649 --> 05:20:06.943
experimental  reactor.  And 
every time the physicists do an 

05:20:09.402 --> 05:20:11.696
experiment, within five minutes,
we want to tell them the plasma 

05:20:11.697 --> 05:20:14.377
density, temperature and 
magnetic field on a 

05:20:15.411 --> 05:20:18.078
three-dimensional grid.  So, how
hard is that?

05:20:22.660 --> 05:20:24.660
Well, first of all, the plasma 
is very, very hot.

05:20:27.164 --> 05:20:31.099
So, you can't just poke it with 
a thermometer like a turkey.  

05:20:31.100 --> 05:20:33.134
The thermometer would melt and 
you would disrupt the plasma and

05:20:33.135 --> 05:20:36.200
ruin the experiment.  What you 
do have are measurements along 

05:20:36.201 --> 05:20:38.648
the  boundary.  But there's only
so many measurements 

05:20:41.699 --> 05:20:43.337
you can take, because you can't 
cut to -- can't cut, you know, 

05:20:43.338 --> 05:20:45.338
that many holes 

05:20:46.838 --> 05:20:48.838
in the side of this device.  So,
let's look closely at one.

05:20:53.104 --> 05:20:55.104
Let's look at measuring of 
electron 

05:20:56.175 --> 05:20:58.175
density, that's done with a 
device known 

05:21:04.398 --> 05:21:05.844
as as an inner pherometer, it's 
proportional to the average 

05:21:05.845 --> 05:21:07.845
density along that ray.

05:21:11.253 --> 05:21:12.066
So, we have 14 lasers shining 
through the center of the 

05:21:12.067 --> 05:21:14.396
plasma.  We know the average 
density along

05:21:14.597 --> 05:21:16.597
14 lines.

05:21:17.927 --> 05:21:21.647
And from that, we want to know 
the density everywhere.  So, 

05:21:21.648 --> 05:21:23.648
clearly there's no one unique 
solution to this problem.

05:21:26.366 --> 05:21:28.236
And instead we'll have a 
distribution over possible 

05:21:28.237 --> 05:21:30.237
solutions.

05:21:33.234 --> 05:21:35.234
So, we do this in a Basian 
sense, and 

05:21:37.957 --> 05:21:39.957
the final output is a 
probability 

05:21:41.067 --> 05:21:42.686
density function for the density
of the electrons given the 

05:21:42.687 --> 05:21:44.687
measurements.

05:21:45.784 --> 05:21:47.664
And we can  visualize that with 
a graph where you have a mean 

05:21:47.665 --> 05:21:49.733
and some air bars.

05:21:53.006 --> 05:21:55.490
How does TensorFlow help with 
this?  Well, so, the first place

05:21:55.491 --> 05:21:59.363
is translating measurement 
physics into code.  So, let's 

05:21:59.364 --> 05:22:01.364
consider the distribution for 
the camera measurement.

05:22:03.731 --> 05:22:06.618
So, the cameras measure photons.
And say we have some photons 

05:22:06.619 --> 05:22:11.181
being emitted from the plasma.  
The mean number of photons 

05:22:11.182 --> 05:22:13.182
reaching 

05:22:18.991 --> 05:22:21.670
the camera is given by a sparse 
tenser, dense matmul.  But we 

05:22:21.671 --> 05:22:23.671
don't realize the mean.

05:22:25.770 --> 05:22:28.841
Instead what we realize is a 
noisy mean.  There's noise due 

05:22:28.842 --> 05:22:30.842
to a finite number of photons.

05:22:35.171 --> 05:22:38.873
And we have discreetization 
noise, we have space.  The 

05:22:38.874 --> 05:22:41.750
TensorFlow normal distribution
library gives you access, so 

05:22:41.751 --> 05:22:44.420
this noisy flux represents a 
normal distribution.  It has a 

05:22:44.421 --> 05:22:47.085
mean.  It has -- you can draw 
samples.

05:22:51.643 --> 05:22:53.643
You can compute the PDF and so 
on.

05:22:55.125 --> 05:22:57.621
That's not all, though, we also 
have analog to digital 

05:22:57.622 --> 05:23:00.345
conversion process that we model
as passing this normal 

05:23:02.643 --> 05:23:05.104
distribution through a 
non-linear response curve and 

05:23:05.105 --> 05:23:07.105
digitizing it to 8 bits.

05:23:09.639 --> 05:23:11.083
So, at the end, this digitized 
charge is another distribution 

05:23:11.084 --> 05:23:15.168
object that has the ability to 
take samples.  You can compute 

05:23:15.169 --> 05:23:19.097
the probability mass function 
because it's discreet.  And so 

05:23:19.098 --> 05:23:21.098
on.

05:23:22.572 --> 05:23:24.572
And since we want to be 
Bayesian, we 

05:23:27.745 --> 05:23:30.007
want to reassemble a number of 
these distributions giving us a 

05:23:30.008 --> 05:23:32.008
likelihood and 

05:23:33.108 --> 05:23:35.108
a prior and so on with the goal 
of producing a posterior.

05:23:39.964 --> 05:23:41.207
And then we do Bayesian 
inference.  To do inference, we 

05:23:41.208 --> 05:23:43.208
do this in two different ways.

05:23:46.129 --> 05:23:48.129
The first way is variational 

05:23:50.165 --> 05:23:51.642
inference, which amounts to min 
nic minimizing the loss 

05:23:51.643 --> 05:23:53.643
function.

05:23:55.352 --> 05:23:57.409
And you can get the true 
posterior.  This is done like 

05:23:57.410 --> 05:23:59.410
any other TensorFlow
minimization.

05:24:01.868 --> 05:24:03.868
For example, we use Adam 
Optimizer.

05:24:05.169 --> 05:24:07.169
The second way is using 
Hamiltonian Monte Carlo.

05:24:09.293 --> 05:24:11.293
So the TensorFlow probability 
library 

05:24:13.190 --> 05:24:15.190
gives you a number of Monte 
Carlo sample 

05:24:17.768 --> 05:24:18.994
hers, and the Ham ill toneon 
allows you to take samples 

05:24:18.995 --> 05:24:23.518
faster.  Notice in both cases, 
it's autodifferentiation.

05:24:27.012 --> 05:24:29.012
Whether we're taking afraid why 
notes 

05:24:30.073 --> 05:24:32.341
for the loss or to do the 
Hamiltonian Monte Carlo 

05:24:32.342 --> 05:24:36.431
sampling.  Popping up a level, 
you'll notice, we're not doing 

05:24:36.432 --> 05:24:38.432
deep learning.

05:24:39.926 --> 05:24:42.889
As I said, we're doing an 
inverse problem, measurements 

05:24:42.890 --> 05:24:45.560
given by physicists are into a 
reconstruction of some physical 

05:24:45.561 --> 05:24:48.430
state.  So, there's a few 
differences I want to highlight.

05:24:50.289 --> 05:24:52.289
First of all, there are no  
labels that are given to us.

05:24:55.226 --> 05:24:57.919
The natch natural label here 
would be a three-dimensional 

05:24:57.920 --> 05:25:01.444
image of the actual plasma.  But
we're the ones who are telling 

05:25:03.102 --> 05:25:04.957
people what the plasma looks 
like, so, we're the ones 

05:25:04.958 --> 05:25:09.920
actually producing the labels.  
So, begin that there's no 

05:25:09.921 --> 05:25:11.921
labels, you 

05:25:14.453 --> 05:25:16.327
might be tempted to say, this is
an

05:25:16.328 --> 05:25:19.005
unsupervised learning technique 
like word clustering.  Here 

05:25:19.006 --> 05:25:20.845
there really is a right answer. 
There really was a plasma out 

05:25:20.846 --> 05:25:26.168
there.  And if -- or the plasma 
doesn't fall within our air 

05:25:26.169 --> 05:25:29.084
bars, we have made a mistake.  
And also you'll notice that our 

05:25:29.085 --> 05:25:31.085
graph 

05:25:32.177 --> 05:25:34.177
here models physics rather than 
generic functions.

05:25:36.994 --> 05:25:40.041
So, it's a bit more constrained 
on these deep neural networks.  

05:25:40.042 --> 05:25:42.042
But that allows us to get the 
right answer with no labels.

05:25:44.744 --> 05:25:46.984
At the end of the day TensorFlow
does, despite it not being deep 

05:25:46.985 --> 05:25:49.627
learning, TensorFlow adds value 
with the TensorFlow 

05:25:49.628 --> 05:25:53.142
distributions and the 
probability library.  We have 

05:25:53.143 --> 05:25:55.178
autodifferentiation to do 
inference.  And in order to 

05:25:55.179 --> 05:25:57.179
provide answers to 

05:25:59.064 --> 05:26:00.483
many measurements at once,  GPUs
and distributed computing is 

05:26:00.484 --> 05:26:03.361
very important.  So, thank you 
very much.

05:26:06.861 --> 05:26:09.735
[ Applause ]

05:26:13.404 --> 05:26:15.404
And next up we have Cory talking
about 

05:26:16.599 --> 05:26:18.599
machine learning and

05:26:19.837 --> 05:26:21.837
genomics.

05:26:23.751 --> 05:26:27.492
&gt;&gt; Hello, everyone.  My name is 
Cory McLean and I'm an 

05:26:29.759 --> 05:26:32.441
engineering on the genomics team
in Google brain.  And today I'm 

05:26:32.442 --> 05:26:35.078
excited to tell you about 
nucleus, which is a library 

05:26:35.079 --> 05:26:37.079
we've 

05:26:40.236 --> 05:26:42.101
released today to make it easy 
to bring genomics data to 

05:26:42.102 --> 05:26:44.102
TensorFlow.

05:26:46.657 --> 05:26:48.083
So, genomics is the study of the
structure and function of 

05:26:48.084 --> 05:26:52.599
genomes.  In every cell in your 
body, you have two copies of the

05:26:52.600 --> 05:26:55.663
genome, one from each parent.  
And this is strings of DNA, 

05:26:55.664 --> 05:26:59.352
which is a four-letter alphabet.
And about 3 billion letters in 

05:26:59.353 --> 05:27:01.353
the genome.

05:27:03.658 --> 05:27:06.148
So, here is a picture of a 
snapshot on chromosome-1, 

05:27:06.149 --> 05:27:11.121
150,000 letters.  What we can 
see is there's a number of known

05:27:11.122 --> 05:27:13.122
things about this area.

05:27:14.819 --> 05:27:17.288
One, there are functional 
elements, like the genes 

05:27:17.289 --> 05:27:21.025
depicted in that second row.  
Biological measurements allow us

05:27:21.026 --> 05:27:24.113
to analyze what our different 
things that are active in cells.

05:27:26.958 --> 05:27:29.215
So, on that third row, we can 
see the amount of gene 

05:27:29.216 --> 05:27:31.752
expression across different 
tissue types is quantified 

05:27:31.967 --> 05:27:33.967
there.

05:27:35.873 --> 05:27:37.873
And at the bottom, through 
sequencing many

05:27:38.533 --> 05:27:41.431
people, we can identify places 
where there's variation across 

05:27:41.432 --> 05:27:43.432
individuals.

05:27:44.533 --> 05:27:46.164
And there's many different  
computational algorithmic 

05:27:46.165 --> 05:27:48.165
challenges in developing that 
image.

05:27:51.144 --> 05:27:52.388
This ranges from, on the 
experimental data generation 

05:27:52.389 --> 05:27:54.389
side.

05:27:56.503 --> 05:27:58.571
Can we better take the output of
these physical measurements to 

05:27:58.572 --> 05:28:02.526
get accurate DNA readings.  
We'll reduce noise in the 

05:28:02.527 --> 05:28:05.196
experiments that quantify this 
expression.

05:28:08.480 --> 05:28:11.157
Can we take the DNA sequence and
interpret where our functional 

05:28:11.158 --> 05:28:14.923
elements like these genes?  Or 
predict how active are they in 

05:28:16.358 --> 05:28:18.358
different tissue types?

05:28:20.480 --> 05:28:23.168
And can we identify places where
individuals vary compared to our

05:28:23.365 --> 05:28:25.365
reference?

05:28:27.892 --> 05:28:28.099
And how is that different in 
small variants versus say in 

05:28:28.100 --> 05:28:30.100
cancer?

05:28:31.606 --> 05:28:34.096
And how do those changes 
influence human traits?  So, one

05:28:34.097 --> 05:28:36.097
thing that is really exciting 

05:28:37.372 --> 05:28:39.372
for us is, there are many 
opportunities 

05:28:41.061 --> 05:28:42.704
for deep learning in genomics.  
And a lot of that is driven by 

05:28:42.705 --> 05:28:46.590
the increase in the amount of 
data available.  This graph 

05:28:46.591 --> 05:28:48.591
shows the dramatic reduction in 
cost to

05:28:49.897 --> 05:28:51.897
sequence a million bases of DNA 
over the past  decade.

05:28:53.383 --> 05:28:56.242
But also, there's a lot of 
structure in these datasets that

05:28:56.243 --> 05:28:58.243
is often complex 

05:28:59.506 --> 05:29:01.506
and difficult to represent with 
relatively simple models.

05:29:05.516 --> 05:29:07.516
But this made us display 
convolutional 

05:29:08.594 --> 05:29:10.018
structure, so we can use 
techniques from image 

05:29:10.019 --> 05:29:12.726
classification as well as 
sequence models.  And there have

05:29:12.727 --> 05:29:15.206
been a number of proven 
successes of applying deep 

05:29:15.207 --> 05:29:19.567
learning to problems in genomics
such as deep variant, which is a

05:29:19.568 --> 05:29:21.568
tool our group 

05:29:23.487 --> 05:29:25.987
developed to identify small  
variants using convolutional 

05:29:25.988 --> 05:29:32.214
neural networks.  So, our goals 
in genomics are multi-faceted.  

05:29:32.215 --> 05:29:34.244
One is to make it easy to apply 
TensorFlow to problems in 

05:29:34.245 --> 05:29:38.539
genomics.  And do this by 
creating libraries to make it 

05:29:38.540 --> 05:29:40.540
easy to work with genomics data.

05:29:41.613 --> 05:29:43.907
We're also interested in 
developing tools and pushing the

05:29:43.908 --> 05:29:46.208
boundaries on some of these 
scientific questions  using 

05:29:47.455 --> 05:29:49.455
those things that we've built.

05:29:50.524 --> 05:29:52.524
And then want to make all of 
that 

05:29:54.893 --> 05:29:55.317
publicly available as tools that
can be used by the

05:29:55.318 --> 05:30:00.460
community.  So, today, I'll 
focus on the first part of 

05:30:00.461 --> 05:30:03.531
making it easy to bring  
genomics data to TensorFlow.  

05:30:03.532 --> 05:30:05.532
So, what is a major problem?

05:30:07.474 --> 05:30:09.474
One major difficulty is that 
there are 

05:30:10.776 --> 05:30:12.246
many different types of data 
that are generated for genomics 

05:30:12.247 --> 05:30:14.247
research.

05:30:15.961 --> 05:30:17.961
You can see here on the right a 
subset of different types used.

05:30:20.925 --> 05:30:22.925
And these different file formats
have 

05:30:23.995 --> 05:30:25.995
varying amounts of support and 
in 

05:30:28.100 --> 05:30:30.146
general no uniform APIs.  We 
have some concerns about 

05:30:30.147 --> 05:30:32.147
efficiency and language support 
where we would like 

05:30:34.698 --> 05:30:36.698
to be able to express some 
manipulations 

05:30:39.884 --> 05:30:41.960
in Python, but need some 
effective ways to efficiently go

05:30:41.961 --> 05:30:44.848
through this data such that 
native Python wouldn't make that

05:30:45.270 --> 05:30:48.122
possible.  So, to address these 
challenges, we 

05:30:52.297 --> 05:30:54.998
developed Nucleus, which is a 
C++ and Python library for 

05:30:54.999 --> 05:30:56.999
reading and writing 

05:30:58.303 --> 05:31:00.303
genomic data to make it easy to 
bring to 

05:31:02.857 --> 05:31:05.742
TensorFlow models and then feed 
through the tf.data API that 

05:31:05.743 --> 05:31:07.743
Derek talked about today 

05:31:09.244 --> 05:31:11.315
for training models for your 
particular task of interest.  

05:31:11.316 --> 05:31:13.979
And we support the
reading of many of the most 

05:31:13.980 --> 05:31:15.980
common data 

05:31:17.113 --> 05:31:19.987
formats in genomics and provide 
a unified API across the 

05:31:19.988 --> 05:31:22.454
different data  types.  So, 
we're able to iterate through 

05:31:22.455 --> 05:31:26.142
the different records of these 
different types and be able to 

05:31:26.143 --> 05:31:28.143
query on specific 

05:31:31.860 --> 05:31:33.860
regions of the genome to access 
the data there.

05:31:36.062 --> 05:31:38.062
The way that we developed this 
uses 

05:31:41.034 --> 05:31:42.681
protocol buffers under the hood 
so that we can implement all the

05:31:42.682 --> 05:31:44.682
general parsing 

05:31:47.211 --> 05:31:49.211
in C++ and then make those 
available to 

05:31:51.132 --> 05:31:53.804
other languages like Python.  
And for those of you particular 

05:31:53.805 --> 05:31:55.805
with 

05:32:00.866 --> 05:32:02.866
genomics, we end up using HTS

05:32:04.902 --> 05:32:07.408
lib which is a conical parser 
for the high-through put 

05:32:07.409 --> 05:32:09.444
sequencing data formats with the
variants and then wrap that to 

05:32:12.108 --> 05:32:14.752
generate the protocol buffers 
and then use CLIF on top of this

05:32:14.753 --> 05:32:16.753
to make the data available to 
Python.

05:32:19.876 --> 05:32:21.947
And finally, we use some of the 
TensorFlow core libraries so we 

05:32:21.948 --> 05:32:23.948
can 

05:32:27.126 --> 05:32:30.020
write out this data as T  
FRecords so they can be ingested

05:32:30.021 --> 05:32:35.013
by the API.  The data types we 
currently support are the 

05:32:35.014 --> 05:32:37.014
following, raking from general 
genome

05:32:38.273 --> 05:32:41.371
annotation to reference genomes 
and different sequencer reads.  

05:32:41.372 --> 05:32:43.372
Whether they're director off the

05:32:44.506 --> 05:32:46.562
sequencer or mapped as well as 
genetic variants.

05:32:51.070 --> 05:32:53.121
So, to give an example of the 
reading API, it's quite 

05:32:53.122 --> 05:32:57.403
straightforward.  So, this is 
kind of a toy example, but it is

05:32:57.404 --> 05:33:00.063
essentially similar to what is 
used for deep variant where we 

05:33:00.064 --> 05:33:02.064
want to 

05:33:04.374 --> 05:33:06.374
train a model to identify actual
genome 

05:33:08.108 --> 05:33:09.133
variations based on maps, 
sequence reads and a reference 

05:33:09.134 --> 05:33:12.469
genome.  So, you have three 
different data types that we 

05:33:12.470 --> 05:33:14.968
need.  We import the different 
reader types.

05:33:19.274 --> 05:33:21.274
And then say, in this renal 
region 

05:33:22.382 --> 05:33:24.438
that we're interested in, we can
issue queries to each of the 

05:33:24.439 --> 05:33:26.439
different reader 

05:33:27.860 --> 05:33:29.860
types and then have

05:33:31.675 --> 05:33:33.509
iterables that we can manipulate
and turn into TensorFlow 

05:33:33.510 --> 05:33:37.827
examples.  On the writing side, 
it's similarly straightforward.

05:33:43.193 --> 05:33:45.193
So, if we have a list of 
variants for 

05:33:47.338 --> 05:33:49.789
the -- the common vcf format, 
we'll have an associated header 

05:33:49.790 --> 05:33:54.324
which provides metadata about 
this, and then open a writer 

05:33:54.325 --> 05:33:55.770
with that header and then just 
loop

05:33:55.771 --> 05:33:57.771
through the variants and write 
them.

05:34:01.129 --> 05:34:03.129
And note that we support writing
to 

05:34:04.165 --> 05:34:08.198
block format which is for the 
subsequent indexing by other 

05:34:08.199 --> 05:34:10.199
tools.

05:34:11.878 --> 05:34:13.923
We can directly write to TF 
records and write the methods to

05:34:13.924 --> 05:34:15.994
write out charted data which we 
found helps 

05:34:19.312 --> 05:34:21.312
avoiding certain hot spots in 
the genome 

05:34:22.982 --> 05:34:24.982
using very a very similar API.

05:34:29.175 --> 05:34:31.469
Finally, we have been working 
with the Google Cloud team which

05:34:31.470 --> 05:34:35.982
has some tools for analyzing 
variant data.  And so, they have

05:34:35.983 --> 05:34:37.983
developed a tool 

05:34:40.113 --> 05:34:42.113
called Variant Transforms, which
allows 

05:34:44.840 --> 05:34:46.840
you to load the variant files to
big 

05:34:47.968 --> 05:34:49.968
query using Apache Beam.

05:34:51.014 --> 05:34:53.014
And you can do queries over that
data.

05:34:56.120 --> 05:34:57.349
We're implementing here to have 
Nucleus under the hood providing

05:34:57.350 --> 05:35:00.614
that generation of the variants 
and to learn more about that 

05:35:00.615 --> 05:35:02.881
tool you can go to the link 
below.

05:35:10.100 --> 05:35:12.536
So, to summarize, we have 
developed Nucleus, which is a 

05:35:12.537 --> 05:35:14.537
C++ and Python 

05:35:16.250 --> 05:35:18.336
library to make it easy to bring
genomics data to TensorFlow to 

05:35:18.337 --> 05:35:20.337
train your models of interest 
for genomic problems.

05:35:23.279 --> 05:35:25.279
And we have the ability to 
interoperate

05:35:26.739 --> 05:35:28.587
with cloud genomics and being 
integrated into the various 

05:35:28.588 --> 05:35:30.621
transforms at the moment.  And 
this ended up being the 

05:35:30.622 --> 05:35:32.622
foundation 

05:35:36.186 --> 05:35:37.205
of our CNN-based variant caller 
which is available at the link 

05:35:37.206 --> 05:35:41.715
below.  So, with that, I would 
like to thank you all for your 

05:35:41.716 --> 05:35:43.716
attention today.

05:35:46.811 --> 05:35:48.811
[ Applause ]

05:35:52.230 --> 05:35:54.926
Next up we'll have Edd to talk 
about open source 

05:35:54.927 --> 05:35:56.927
collaborations.

05:35:58.627 --> 05:36:02.511
.
&gt;&gt; Thank you.  Hi, everyone.  

05:36:02.512 --> 05:36:04.512
Now, I was going to talk to you 
about 

05:36:06.535 --> 05:36:08.739
my plans to reanimate dinosaurs 
with TensorFlow, but I don't 

05:36:10.782 --> 05:36:13.038
want to steal those  guy's 
thunder.  Actually, I'm here to 

05:36:13.039 --> 05:36:14.279
talk about open source 
collaboration in the TensorFlow 

05:36:14.280 --> 05:36:18.847
project.  That's my job at 
Google.  To work on growing the 

05:36:18.848 --> 05:36:20.848
participation and the 
collaboration in the project in 

05:36:22.159 --> 05:36:24.217
the whole community.  So, you 
guys all here and everybody 

05:36:27.493 --> 05:36:31.261
watching on the livestream are a
huge part of this already.  If 

05:36:31.262 --> 05:36:33.133
you saw a slide like this at the
beginning of the day in the 

05:36:33.134 --> 05:36:37.903
keynote, you can see the numbers
ticked up.  In the five days I 

05:36:37.904 --> 05:36:43.175
have been monitoring this slide,
I am increasing the numbers.  

05:36:43.176 --> 05:36:45.858
The amount of participation is 
staggering.  As an open source 

05:36:45.859 --> 05:36:49.331
project, it blows my mind when I
came to work on it.  And so much

05:36:49.332 --> 05:36:51.414
of that is due to the 
participation of everybody here 

05:36:51.415 --> 05:36:53.456
in the community.  There are 
parts of TensorFlow that 

05:36:56.340 --> 05:36:59.409
wouldn't exist, many of them, 
without that collaboration.  For

05:36:59.410 --> 05:37:01.873
instance, whether it's Spark
connectors, whether it's support

05:37:01.874 --> 05:37:05.567
for particular architectures and
accelerators, or maybe certain 

05:37:05.568 --> 05:37:10.731
language bindings.  We not only 
benefit from a huge amount of 

05:37:10.732 --> 05:37:12.619
adoption by being open source, 
but as the adoption grows, this 

05:37:12.620 --> 05:37:15.293
is the way we sustain and grow 
the project.

05:37:18.466 --> 05:37:20.466
You saw this map earlier as 
well.

05:37:22.148 --> 05:37:23.981
This is just some of the GitHub 
stars that gave their locations 

05:37:23.982 --> 05:37:25.982
that we could map.

05:37:28.744 --> 05:37:30.812
And it was as far north as 
Norway and as far south as the 

05:37:30.813 --> 05:37:32.813
Antarctic.

05:37:34.090 --> 05:37:36.184
And what's obvious right now, 
although there's a large team at

05:37:36.185 --> 05:37:38.185
Google 

05:37:39.475 --> 05:37:41.475
developing TensorFlow, there are
for 

05:37:42.569 --> 05:37:47.091
many more people and in far many
more places using it .  And the 

05:37:47.092 --> 05:37:50.783
open source projects, more 
adoption were more demand.  

05:37:50.784 --> 05:37:52.784
There's so much we can do 
together to grow TensorFlow.

05:37:54.079 --> 05:37:55.548
Now, you remember that thing 
where you turn up to a party and

05:37:55.549 --> 05:38:00.505
everyone is having a good time 
and they all seem to know what 

05:38:00.506 --> 05:38:02.506
they're doing and why it's such 
fun.

05:38:04.623 --> 05:38:05.629
But who am I going to talk to 
with and what are they looking 

05:38:05.630 --> 05:38:08.916
at?  And sometimes a large open
source project can be a bit like

05:38:08.917 --> 05:38:12.204
that.  You want to get involved 
and contributing to TensorFlow, 

05:38:12.205 --> 05:38:14.205
but where do you start?

05:38:17.166 --> 05:38:19.863
You think this module, -- this 
feature and something you want 

05:38:19.864 --> 05:38:24.010
to work on.  How do you find the
right person to talk to?  How 

05:38:24.011 --> 05:38:26.011
you learn what we're thinking 
about the direction for this?

05:38:29.351 --> 05:38:31.845
We have heard some of those 
things and we recognize that we 

05:38:31.846 --> 05:38:33.876
want to improve our openness, 
our transparency and our 

05:38:34.069 --> 05:38:37.205
participation.  We're trying to 
work to make it easier to get 

05:38:37.206 --> 05:38:41.353
involved in TensorFlow.  We have
already, for instance, refreshed

05:38:41.354 --> 05:38:43.009
our roadmap which you can find 
on the TensorFlow website about 

05:38:43.010 --> 05:38:46.664
the general direction of a lot 
of the code.  And we'll do that 

05:38:46.665 --> 05:38:48.665
a lot more regularly.

05:38:49.982 --> 05:38:51.635
But I want to talk about four 
initiatives that we have going 

05:38:51.636 --> 05:38:54.914
that will enable us to work 
together more effectively and 

05:38:54.915 --> 05:38:56.977
faster.  The first of these is 
simple.

05:39:00.229 --> 05:39:01.895
It's a central community for 
everyone who is working on and 

05:39:01.896 --> 05:39:03.896
contributing to TensorFlow.

05:39:05.419 --> 05:39:07.419
GitHub has so much energy going 
on.  There's

05:39:09.320 --> 05:39:11.983
so much great debate in all the 
issues.  Look in there, in the 

05:39:11.984 --> 05:39:13.984
pull requests, 

05:39:15.053 --> 05:39:17.053
really  thoughtful conversation 
conversations and contributions.

05:39:17.528 --> 05:39:19.528
Thank you for being part of 
that.

05:39:21.870 --> 05:39:22.920
But what we don't have is a 
central place you can 

05:39:22.921 --> 05:39:24.921
collaborate.

05:39:28.088 --> 05:39:30.088
We have a mailing list, 
developers @it feel.org.

05:39:31.510 --> 05:39:33.510
That's a

05:39:37.964 --> 05:39:40.641
developer@tensorFlow.org.  We 
can work together as a community

05:39:40.642 --> 05:39:45.412
to get feedback and coordinate 
together.  Many of the projects 

05:39:45.413 --> 05:39:48.099
have mailing lists that you can 
find at TensorFlow.

05:39:48.708 --> 05:39:50.708
org/community.

05:39:53.436 --> 05:39:55.436
Whether it's TF  Lite, or 
TensorFlow.JS.

05:39:57.373 --> 05:39:59.830
So, that's collaboration.  Now, 
we talked about the fact there 

05:40:03.123 --> 05:40:04.130
are many use cases outside of 
Google that the core team don't 

05:40:04.131 --> 05:40:06.131
see.  Many more.

05:40:08.897 --> 05:40:10.897
Many  Much more happens outside 
than inside the core team.

05:40:13.659 --> 05:40:15.529
So, we want to make it possible 
for people with shared interests

05:40:15.530 --> 05:40:18.024
in projects to work together.  
This is where the beauty of open

05:40:18.884 --> 05:40:20.884
source comes in.

05:40:21.971 --> 05:40:23.971
How do we do that?  We're 
setting up a structure for 

05:40:24.872 --> 05:40:26.339
groups to work
together.  Special interest 

05:40:26.340 --> 05:40:28.830
groups.  We have been piloting 
the first of these for a few 

05:40:28.831 --> 05:40:30.831
months now.

05:40:31.953 --> 05:40:34.803
This is called SIG build.  It's 
about building, packaging and 

05:40:35.621 --> 05:40:37.267
distributing TensorFlow.  
Familiar with TensorFlow, you 

05:40:37.268 --> 05:40:39.767
know we built it in a certain 
way.  Guess what?

05:40:42.887 --> 05:40:44.713
Not every architecture or 
application finds that the best 

05:40:44.714 --> 05:40:46.714
way for them.

05:40:47.800 --> 05:40:49.436
For instance, the Linux wants to
build against the shared 

05:40:49.437 --> 05:40:53.363
libraries in the distribution.  
That's not something we do.  So,

05:40:53.364 --> 05:40:55.364
we brought together a bunch of 

05:40:56.848 --> 05:40:58.848
stakeholders over Linux, 
companies like 

05:41:03.245 --> 05:41:04.474
redhat, IBM, and NVIDIA, Intel, 
to collaborate in a group to 

05:41:04.475 --> 05:41:07.945
look at the build and make it 
work for effectively for more 

05:41:07.946 --> 05:41:10.401
people in the future.  That's 
just one group.  The pilot.

05:41:14.054 --> 05:41:15.487
But, we want to pave the cow 
paths.  Where there is energy 

05:41:15.488 --> 05:41:18.986
and people collaborating on a 
particular thing, that's a great

05:41:18.987 --> 05:41:20.987
candidate to bring a special 
interest group together.

05:41:24.471 --> 05:41:26.471
We're also bringing online a 
group for TensorBoard where key 

05:41:28.165 --> 05:41:30.165
stakeholders of the TensorBoard 
ecosystem

05:41:30.866 --> 05:41:32.866
can work together on 
collaboration.

05:41:35.163 --> 05:41:37.010
And the bindings, completely 
built by the group for 

05:41:37.011 --> 05:41:40.117
TensorFlow.  And each will have 
a different way of working, a 

05:41:40.118 --> 05:41:44.235
different community.  But the 
common thing is, we're going to 

05:41:44.236 --> 05:41:45.456
provide  forms, if you have a 
shared interest in a particular 

05:41:45.457 --> 05:41:47.710
area, we can focus on it.

05:41:51.620 --> 05:41:54.464
Now, I'd like to talk about the 
design of TensorFlow.  You know,

05:41:54.465 --> 05:41:56.093
one of the most amazing things 
and the benefits of TensorFlow 

05:41:56.094 --> 05:41:58.094
is that the code that we release
is the 

05:42:01.031 --> 05:42:03.462
code that Google uses on a daily
basis.  It's kind of remarkable 

05:42:03.463 --> 05:42:06.555
for an open source project.  And
so, we're really causal about 

05:42:06.951 --> 05:42:10.036
changes.  We're really careful 
about design.  We have a 

05:42:10.037 --> 05:42:13.557
commitment, obviously, to the 
API through the 1.X series  

05:42:13.558 --> 05:42:18.903
release.  And we have design 
reviews internally.   So, when 

05:42:18.904 --> 05:42:23.198
things change, we have 
proposals, we get feedback.  But

05:42:23.199 --> 05:42:25.043
by now you are thinking, well, 
you just said that so many use 

05:42:25.044 --> 05:42:28.988
cases and so many users are 
outside of Google.  Yet you're 

05:42:28.989 --> 05:42:30.989
having these design reviews 
inside. 

05:42:31.657 --> 05:42:34.327
So, what we're going to do is 
open up a public feedback phase 

05:42:34.328 --> 05:42:39.029
to our design process so we can 
engage much more broadly with 

05:42:39.030 --> 05:42:40.467
every contributor and user about
how that might affect their 

05:42:40.468 --> 05:42:43.316
needs and what their opinions 
are.

05:42:47.432 --> 05:42:49.432
Keep an eye on the developers at
TensorFlow.org  mailing list.

05:42:51.343 --> 05:42:52.800
That's where we'll notify it 
coming online in the next couple

05:42:52.801 --> 05:42:57.315
months.  My hope is this process
will be a way that everybody can

05:42:57.316 --> 05:42:59.969
communicate and discuss about 
the future direction of  

05:42:59.970 --> 05:43:01.970
TensorFlow.  Whether you're in 
the core team at 

05:43:05.133 --> 05:43:07.133
Google or in the broader 
community.

05:43:08.653 --> 05:43:11.299
So,  contributing to TensorFlow 
isn't just about issues or PR 

05:43:11.300 --> 05:43:16.449
requests.  In fact, I would say 
I reckon that there's so much 

05:43:16.450 --> 05:43:20.738
more -- more energy going into  
blogging, running meetups, doing

05:43:20.739 --> 05:43:22.799
presentations, teaching, doing 
courses.  So many universities 

05:43:22.800 --> 05:43:24.800
around the world.

05:43:27.928 --> 05:43:29.998
And we want to amp up and 
support the amount of content 

05:43:29.999 --> 05:43:32.248
that educates and highlights 
TensorFlow.  We're really 

05:43:32.249 --> 05:43:34.508
excited already that so
many of you do such amazing 

05:43:34.509 --> 05:43:37.417
jobs.  We would like to be able 
to point everybody in the 

05:43:37.418 --> 05:43:40.912
TensorFlow community to the work
that you're doing.  So, there's 

05:43:40.913 --> 05:43:44.188
a couple things that we launched
to support this.  The first you 

05:43:44.189 --> 05:43:48.560
probably already heard.  We now 
have a blog for TensorFlow.  A 

05:43:48.561 --> 05:43:50.561
blog.tensorFlow.org.

05:43:52.294 --> 05:43:54.294
One of the things I'm most 
excited 

05:43:55.377 --> 05:43:57.021
about with this blog is that as 
well as important announcements 

05:43:57.022 --> 05:44:00.887
and education, we're setting it 
up from the beginning to involve

05:44:00.888 --> 05:44:03.567
content from around the web and 
into the community.  That's one 

05:44:03.568 --> 05:44:05.568
of the reason we're using 

05:44:07.305 --> 05:44:08.517
the Medium platform to make it 
easy to integrate content around

05:44:08.518 --> 05:44:10.518
the web and give you the credit 
for the work you have done.

05:44:14.739 --> 05:44:17.200
So, we would really like to hear
from you.  If you have a blog 

05:44:17.201 --> 05:44:22.407
post to get into the TensorFlow 
publication, get in touch.  

05:44:22.408 --> 05:44:24.481
Secondly, and if you're on the 
livestream watching this, you've

05:44:24.482 --> 05:44:27.600
kind of found out about this, we
have a YouTube channel that's 

05:44:27.601 --> 05:44:29.601
launched today.

05:44:31.271 --> 05:44:33.271
Now, one of the things I'm most 
excited

05:44:34.781 --> 05:44:37.315
about this in this is a show 
called TensorFlow Meets.  We are

05:44:37.316 --> 05:44:39.316
able to get out into the world 

05:44:40.593 --> 05:44:42.593
of contributors and users and 
highlight some of the use cases.

05:44:43.692 --> 05:44:47.784
Highlight the work of everybody.
This is a chance for you too.  

05:44:47.785 --> 05:44:49.830
We would love to meet you and 
chat with you about what you're 

05:44:49.831 --> 05:44:53.764
up to and have you featured on 
the YouTube channel.  Again, 

05:44:53.765 --> 05:44:58.502
reach out to us.  We would love 
you to be a part of it.  There 

05:44:58.503 --> 05:45:01.380
is one URL to get  involved in 
all these things that I 

05:45:01.381 --> 05:45:03.381
mentioned, TensorFlow.
org/community.

05:45:05.906 --> 05:45:07.906
So, if anyone's mentioned a 
mailing 

05:45:08.993 --> 05:45:10.617
list or group to you today, 
please go to that URL and you 

05:45:10.618 --> 05:45:13.308
will find resources there.  It's
my hope that TensorFlow is going

05:45:15.561 --> 05:45:17.617
to continue to be a party, but 
maybe one you can find yourself 

05:45:17.618 --> 05:45:21.931
part of a lot sooner and have 
more fun.  Please, feel free to 

05:45:21.932 --> 05:45:23.932
reach out to me.

05:45:28.857 --> 05:45:31.832
There's my people address, ew 
ewj@google.com.  And talk about 

05:45:31.833 --> 05:45:33.833
your experiences collaborating 
around open source and 

05:45:33.889 --> 05:45:35.889
TensorFlow.  I would love to 
hear about it.  Thank you so

05:45:35.950 --> 05:45:38.653
much.
[ Applause ]

05:45:41.941 --> 05:45:43.790
Now, our next speaker is Chris 
Lattner.  He's going to talk 

05:45:43.791 --> 05:45:45.843
about a first principles 
approach to machine learning.

05:45:46.043 --> 05:45:49.926
&gt;&gt; All right.  Thank you, Edd.

05:45:54.065 --> 05:45:56.117
Hi there, everyone.  I'm excited
to introduce a new project 

05:46:02.476 --> 05:46:05.151
that we have been working on 
that takes a new project to 

05:46:05.152 --> 05:46:07.400
improve usability of TensorFlow.
And we care so much about 

05:46:07.401 --> 05:46:10.893
usability here that we're going 
all the way back to first 

05:46:10.894 --> 05:46:13.803
principles of the computation 
that we're performing.  But 

05:46:13.804 --> 05:46:15.804
first, why usability?

05:46:17.072 --> 05:46:18.538
I hope that everyone here agrees
that productivity and machine 

05:46:18.539 --> 05:46:21.020
learning is critical.  Because 
it leads to a faster pace of 

05:46:22.867 --> 05:46:24.313
innovation and progress in our 
field.  And, of course, question

05:46:24.314 --> 05:46:27.594
are just want to build beautiful
things for TensorFlow users 

05:46:27.595 --> 05:46:29.632
since that's a big piece of it 
as well.  But if you look at 

05:46:29.633 --> 05:46:34.159
machine learning frameworks, 
there's two major approaches.  

05:46:34.160 --> 05:46:36.160
The most familiar are the graph 

05:46:38.263 --> 05:46:40.263
building approaches where you 
explicitly 

05:46:41.332 --> 05:46:44.448
define a graph and execute it to
run a computation.  It's great 

05:46:44.449 --> 05:46:49.176
for performance, but not always 
for usability.  In define by run

05:46:49.177 --> 05:46:51.459
approach, eager execution, not 
always the best 

05:46:55.964 --> 05:46:58.228
performance, but you can use it
easier.  And both approaches are

05:46:58.229 --> 05:47:02.144
really about allowing Python to 
understand the difference 

05:47:02.145 --> 05:47:04.145
between the tensor computation 
of your code and all the 

05:47:07.302 --> 05:47:09.545
other non-tenser stuff like 
command line  processing and 

05:47:09.546 --> 05:47:11.192
visualization and what you do.  
I think it's interesting to look

05:47:11.193 --> 05:47:15.527
at how these actually work.  In 
the case of Eager Execution, you

05:47:17.196 --> 05:47:19.196
write the model and Python 
parses it.

05:47:21.103 --> 05:47:22.552
And then it feeds every 
statement at a time to the 

05:47:22.553 --> 05:47:25.251
interpreter.  If it's a tenser 
operation, it hands 

05:47:29.790 --> 05:47:32.250
it to TensorFlow and  takes care
of the tensor applications 

05:47:32.251 --> 05:47:34.754
otherwise Python runs it.

05:47:39.885 --> 05:47:41.778
The key thing about Eager 
Execution is they're designed 

05:47:41.779 --> 05:47:46.098
within the constraints of a 
Python library.  With the 

05:47:46.099 --> 05:47:48.989
compiler, what we can do.
compiler and language involved, 

05:47:48.990 --> 05:47:53.069
there's a whole other set of 
approaches that can be applied 

05:47:53.070 --> 05:47:55.927
to  solving this problem.  
That's what we're doing.  The 

05:47:55.928 --> 05:47:57.949
cool thing about a compiler, 
after you parse your code, the 

05:47:57.950 --> 05:48:01.217
compiler can see the entire 
program and all the tensor ops 

05:48:01.218 --> 05:48:03.218
in it.

05:48:04.881 --> 05:48:06.881
We're all thing a new stage to 
the 

05:48:08.898 --> 05:48:11.220
compiler, takes the tensor 
applications out, and because 

05:48:15.092 --> 05:48:17.955
it's a standard TensorFlow 
graph, you can get access to all

05:48:17.956 --> 05:48:22.837
the things that TensorFlow can 
do, including the devices.  You 

05:48:22.838 --> 05:48:24.878
get the power and flexibility of
TensorFlow, but you get the 

05:48:24.879 --> 05:48:28.991
usability of your execution as 
well.  But there's a catch.  

05:48:28.992 --> 05:48:31.046
There's always a catch,ing 
right?  The catch here is that 

05:48:31.047 --> 05:48:34.124
we can't do this with Python.  
At least not with the 

05:48:34.125 --> 05:48:37.834
reliability we expect, because 
it doesn't support the kind of 

05:48:37.835 --> 05:48:39.835
compiler analysis we need.  And 
what do we mean by that?

05:48:43.413 --> 05:48:46.673
Well, the compiler has to be 
able to reason about values.  

05:48:46.674 --> 05:48:48.674
Has to reason about control flow
and function calls.

05:48:52.382 --> 05:48:54.382
Variable listing and thing

05:48:55.528 --> 05:49:00.272
Variable alassing.  And we have 
come to know the things about 

05:49:00.273 --> 05:49:03.142
Python, including using all the 
standard Python APIs.  I know 

05:49:03.143 --> 05:49:06.015
what you're thinking.  Does this
mean we're talking about doing a

05:49:06.016 --> 05:49:10.526
new language?  Well, that's 
definitely an approach to solve 

05:49:10.527 --> 05:49:12.818
the technical requirements we 
want.  With a new language, we 

05:49:12.819 --> 05:49:15.675
can build all the nice things we
want into it.  But this comes at

05:49:15.676 --> 05:49:17.676
a cost.

05:49:19.400 --> 05:49:20.422
It turns out we would be 
foregoing the benefits of a 

05:49:20.423 --> 05:49:24.146
community.  That includes tools 
and libraries, but also things 

05:49:24.147 --> 05:49:28.480
like books, which some people 
still use.  And even more 

05:49:28.481 --> 05:49:32.800
significantly, this would take 
years of time to get right.  And

05:49:32.801 --> 05:49:34.873
machine learning just moves too 
fast.  No, we think it's better 

05:49:34.874 --> 05:49:39.431
to use an existing language.  
But here we have to be careful. 

05:49:39.432 --> 05:49:41.432
Because to do this right, we 
have to 

05:49:42.728 --> 05:49:45.001
make significant improvements to
the compiler and the language 

05:49:45.002 --> 05:49:48.538
and do it in a reasonable amount
of time.  And so, of course, 

05:49:48.539 --> 05:49:53.047
this brings us to the Swift 
programming language.  Now, I 

05:49:53.048 --> 05:49:55.752
assume that most of
you are not very familiar with 

05:49:55.753 --> 05:50:00.964
Swift, so, I'll give you a quick
introduction.  Swift is designed

05:50:00.965 --> 05:50:03.218
with a lightweight syntax.  It's
geared towards being easy to use

05:50:04.038 --> 05:50:07.740
and learn.  Swift draws together
best practices from lots of 

05:50:07.741 --> 05:50:09.741
different places, including 

05:50:10.826 --> 05:50:12.826
things like functional 
programming and generics.

05:50:15.979 --> 05:50:18.222
Swift builds on LLVM, it has an 
interpreter and scripting 

05:50:18.223 --> 05:50:23.801
capabilities as well.  Swift is 
great in notebook environments. 

05:50:23.802 --> 05:50:25.257
These are really awesome when 
you're interactively developing 

05:50:25.258 --> 05:50:28.593
in real-time.  Swift is also 
open source.  It's part of lots 

05:50:28.594 --> 05:50:32.558
of platforms.  It has a big 
community of people.  But the 

05:50:32.559 --> 05:50:34.559
number one thing that's most 

05:50:36.914 --> 05:50:39.168
important to us is it has a 
fully open design environment 

05:50:39.169 --> 05:50:41.169
called Swift  Evolution.

05:50:43.075 --> 05:50:45.075
Allows us to propose machine 
learning 

05:50:46.355 --> 05:50:47.574
and compiler features 
directorially for integration 

05:50:47.575 --> 05:50:52.462
into Swift.  When you bring all 
of this together, I'm happy to 

05:50:52.463 --> 05:50:54.697
introduce Swift for TensorFlow. 
Swift for TensorFlow gives you 

05:50:54.698 --> 05:50:58.561
the full performance of Graphs. 
You can use native language

05:50:59.171 --> 05:51:01.171
control flow.

05:51:03.426 --> 05:51:06.069
Has built-ins for automatic 
differentiation.  You can detect

05:51:06.070 --> 05:51:08.070
errors without running.

05:51:10.175 --> 05:51:12.175
And full interaction with APIs.

05:51:14.044 --> 05:51:15.673
I would like to welcome Richard 
Wei to tell you about it now.

05:51:15.674 --> 05:51:18.132
&gt;&gt; Thank you, Chris.
[ Applause ]

05:51:21.980 --> 05:51:23.811
I'm thrilled to show you Swift 
for TensorFlow.  Swift is a 

05:51:23.812 --> 05:51:26.254
high-performance, modern 
programming language.  And 

05:51:26.255 --> 05:51:28.706
today, for the very first time, 
Swift has a full powered 

05:51:28.707 --> 05:51:30.707
TensorFlow built right in.

05:51:33.403 --> 05:51:35.660
I'm going to walk through three 
major styles of programming.

05:51:39.390 --> 05:51:42.068
Scripting, interpreting, and  
notebooks.  So, first let me 

05:51:42.069 --> 05:51:44.144
show you the Swift interpreter.

05:51:47.860 --> 05:51:50.726
This is a Swift interpreter.  
When I type some code, swift 

05:51:50.727 --> 05:51:53.616
evaluates it and prints a 
result.  Just like Python.

05:51:58.189 --> 05:52:00.189
Now, let's import TensorFlow.

05:52:03.744 --> 05:52:05.744
I can create a tensor from some 
scale scalers.

05:52:15.762 --> 05:52:16.580
Now, I can do any TensorFlow 
operation directly and see the 

05:52:16.581 --> 05:52:18.581
result.

05:52:21.520 --> 05:52:23.520
Just like I would with Eager 
Execution.

05:52:28.330 --> 05:52:30.330
For example, A plus A, or ace 
matrix product with itself.

05:52:32.233 --> 05:52:34.526
Of course, loops just work.  I 
can print the result.

05:52:39.278 --> 05:52:41.278
Now, interpreter is a lot of fun
to work with.

05:52:43.409 --> 05:52:44.845
But I like using TensorFlow in a
more interactive environment 

05:52:44.846 --> 05:52:47.118
just like Jupiter notebook.  So,
let's see how they work.

05:52:50.451 --> 05:52:53.312
This is a Swift notebook.  It 
shows

05:52:55.344 --> 05:52:57.344
all the results on the right.

05:52:59.409 --> 05:53:01.409
So, here's some more interesting
code.  Fun with functions.

05:53:04.118 --> 05:53:06.118
So, here I have a sigmoid 
function inside a loop.

05:53:09.489 --> 05:53:11.948
Now, as I click on this button, 
it shows a trace of all values 

05:53:11.949 --> 05:53:16.042
produced by this function over 
time.  Now, as a machine 

05:53:16.043 --> 05:53:18.742
learning developer, I often like
to differentiate functions.

05:53:24.255 --> 05:53:26.506
Now, when I type in -- well, 
since we were able to improve 

05:53:26.507 --> 05:53:28.507
that programming 

05:53:30.012 --> 05:53:31.038
language, we built first class 
automatic differentiation right 

05:53:31.039 --> 05:53:33.039
into Swift.

05:53:36.053 --> 05:53:38.929
Now, when I type in gradient 
effects, it shows the gradients.

05:53:40.980 --> 05:53:42.980
Swift computes the gradient 

05:53:44.097 --> 05:53:46.097
automatically and gives me the 
result.

05:53:47.199 --> 05:53:49.199
So, here is the gradient in the 
sigmoid.

05:53:50.503 --> 05:53:52.503
Now, let's look at some Python 
code.  Let's think about Python.

05:53:54.857 --> 05:53:56.114
Well, as a machine learning 
developer, I have been using 

05:53:56.115 --> 05:53:58.115
Python a lot.

05:53:59.177 --> 05:54:02.097
And I know there are many great 
Python libraries.  Just today my

05:54:02.098 --> 05:54:04.098
colleague, Dan, sent me 

05:54:05.206 --> 05:54:07.206
a dataset in pickle format.

05:54:09.955 --> 05:54:11.955
Well, I can directly use Python 
APIs to load it.

05:54:16.147 --> 05:54:17.387
All I have to do is just type in
"Import

05:54:17.388 --> 05:54:19.388
Python.

05:54:23.602 --> 05:54:25.886
"  And Swift uses a Python API, 
Pickle, to be specific, to load 

05:54:25.887 --> 05:54:31.247
the data.  In here, you can see 
the data right in the Swift 

05:54:31.248 --> 05:54:33.248
notebook.

05:54:39.414 --> 05:54:41.414
Now, -- so, here's a Swift 
notebook.

05:54:43.359 --> 05:54:45.225
Now, some people like to run 
training scripts  directly in 

05:54:45.226 --> 05:54:47.904
command line.  So, let me show 
you how to train a 

05:54:53.252 --> 05:54:55.252
simple model from command line.

05:55:01.966 --> 05:55:03.966
So, here is a simple model.

05:55:06.288 --> 05:55:08.288
I'm using TensorFlow dataset to 
load the API.

05:55:11.034 --> 05:55:12.052
I have the forward pass and the 
backward pass defined in the 

05:55:12.053 --> 05:55:14.755
training loop.  Now, I usually 
like to work on the go, 

05:55:19.336 --> 05:55:22.663
so, this code has been working 
on the CPU on my laptop.  But 

05:55:22.664 --> 05:55:24.664
when I want to get more 
performance, what do I do?

05:55:26.811 --> 05:55:28.811
Well, why don't I just enable 
cloud TPU?

05:55:31.753 --> 05:55:33.753
So, all I have to do is add one 
line 

05:55:35.927 --> 05:55:37.927
to enable TPU execution.

05:55:40.340 --> 05:55:42.340
When I save this file, open the 

05:55:44.512 --> 05:55:49.588
terminal to run this training 
script.  It's initializing TPU. 

05:55:49.589 --> 05:55:51.589
And the Swift compiler 
automatically 

05:55:53.765 --> 05:55:57.552
partitions this program into a 
program and a TensorFlow graph. 

05:55:57.553 --> 05:55:59.553
And TensorFlow is sending this 
graph to

05:56:00.441 --> 05:56:01.875
the TensorFlow  SLA compiler for
TPU execution.  Now, it's 

05:56:01.876 --> 05:56:06.295
running.  And we're waiting for 
the TPU to give the result.

05:56:11.059 --> 05:56:14.173
Look!  Loss is going down.  All 
right.

05:56:20.437 --> 05:56:22.437
So, why don't we simply

05:56:23.785 --> 05:56:25.785
open TensorBoard and see the 
training curve?

05:56:32.485 --> 05:56:34.485
So, now I can see the entire  
training history in TensorBoard.

05:56:35.232 --> 05:56:37.232
So, this is  looking great!

05:56:39.094 --> 05:56:41.094
Now, this is Swift for 
TensorFlow.

05:56:42.744 --> 05:56:44.979
It's an interactive programming 
experience with super-computing 

05:56:47.242 --> 05:56:49.888
performance at your fingertips. 
Back to you, Chris.

05:56:51.722 --> 05:56:53.722
&gt;&gt; Thanks, Richard.  Thanks, 
Richard.

05:56:55.182 --> 05:56:57.626
[ Applause ]
All right.

05:57:00.673 --> 05:57:02.703
To recap quickly, Richard showed
you that Swift has an 

05:57:02.704 --> 05:57:07.577
interpreter.  And it works just 
like you would expect.  Now, I 

05:57:07.578 --> 05:57:09.578
know that it's super-frustrating
to be working on a 

05:57:12.454 --> 05:57:15.136
program and two hours into a 
training run, get a shape error 

05:57:15.137 --> 05:57:17.137
are or type mismatch.

05:57:21.283 --> 05:57:23.665
Swift is catching it early.  We 
built catching mistakes right 

05:57:23.666 --> 05:57:25.666
into TensorFlow.

05:57:27.788 --> 05:57:29.788
And you can use APIs and other 
languages from Swift.

05:57:32.116 --> 05:57:34.609
And give use full access to any 
of the python APIs that you love

05:57:34.610 --> 05:57:38.347
to use.  Swift is generateing 
standard TensorFlow graphs, 

05:57:38.348 --> 05:57:40.636
including control flow.  Which 
give you the full performance of

05:57:41.456 --> 05:57:45.773
the session API.  Of course, 
graphs are also awesome because 

05:57:45.774 --> 05:57:48.272
they give you access to 
everything that TensorFlow can 

05:57:48.273 --> 05:57:52.194
do, including devices spanning 
the range from the tiniest 

05:57:52.195 --> 05:57:54.691
Raspberry Pi all the way up to a
TPU super computer.

05:57:57.809 --> 05:58:00.108
You may wonder, what does this 
mean?  This is an early stage 

05:58:00.109 --> 05:58:03.144
project.  But we're looking 
forward to our open source 

05:58:03.145 --> 05:58:06.034
release next month.  And not 
only are we releasing the code, 

05:58:06.035 --> 05:58:08.905
but
technical white papers and 

05:58:08.906 --> 05:58:13.659
documents to explain how it 
works and moving our design 

05:58:13.660 --> 05:58:15.543
discussions out into the public 
on the Google group so everyone 

05:58:15.544 --> 05:58:19.473
can participate.  We're not done
yet.  We have basic support for 

05:58:19.474 --> 05:58:21.474
automatic 

05:58:23.934 --> 05:58:25.934
differentiation built right into
the compiler and the language.

05:58:29.030 --> 05:58:31.113
But we want to have exotic cases
like recursion and data  

05:58:31.114 --> 05:58:33.114
structures.

05:58:34.241 --> 05:58:36.740
Compatibility issues are  
super-frustrating.  Especially 

05:58:36.741 --> 05:58:41.324
if you use an op or  D-type not 
supported by your device.  Swift

05:58:41.325 --> 05:58:42.806
has great support for detecting 
issues like this, and we are 

05:58:42.807 --> 05:58:49.462
looking forward to wiring this 
into supporting TensorFlow.  We 

05:58:49.463 --> 05:58:51.463
are interested in high-level 
APIs.

05:58:55.088 --> 05:58:57.088
We have some prototypes now, but
we 

05:58:58.601 --> 05:59:00.464
would like to design multiple 
approaches and experiment and 

05:59:00.465 --> 05:59:02.465
settle on the best one based on 
real-world experience.

05:59:06.613 --> 05:59:08.613
This has been a super-quick tour
of Swift for TensorFlow.

05:59:10.736 --> 05:59:13.036
Swift for TensorFlow combines 
the power and flexibility of 

05:59:13.037 --> 05:59:16.137
TensorFlow with a whole knew 
standard of usability.  We think

05:59:16.138 --> 05:59:18.138
it's

05:59:19.221 --> 05:59:20.883
going to take your ability to 
the roof.  It's an early stage 

05:59:20.884 --> 05:59:25.213
project.  We would like you to 
get interested and help us to 

05:59:25.214 --> 05:59:27.489
build this future.  Thank you.

05:59:30.500 --> 05:59:32.500
[ Applause ]

06:33:45.950 --> 06:33:54.302
&gt;&gt; Hello, everyone.  Welcome 
back.  Welcome back.  I'm 

06:33:54.303 --> 06:33:56.303
Jeremiah, and this is Andrew.

06:33:57.390 --> 06:33:59.390
We are here from the TensorFlow 
Hub team.

06:34:01.110 --> 06:34:03.582
We are based in Zurich, 
Switzerland, and we're excited 

06:34:03.583 --> 06:34:07.889
to share TensorFlow Hub today.  
So, this first slide is actually

06:34:07.890 --> 06:34:09.890
one that I stole.

06:34:12.190 --> 06:34:14.846
I took it from a colleague, Noah
Fiedel who leads TensorFlow 

06:34:14.847 --> 06:34:16.847
serving.

06:34:19.551 --> 06:34:22.854
And Noah uses this slide to tell
a personal story.  It kind of 

06:34:22.855 --> 06:34:24.855
shows the growth of things 

06:34:27.689 --> 06:34:29.689
-- the type of tools that we use
to do software engineering.

06:34:33.916 --> 06:34:36.015
And it shows how they mature 
over time.  He connects this to 

06:34:36.016 --> 06:34:38.511
a similar thing happening, the 
tools we use to do machine 

06:34:38.512 --> 06:34:40.512
learning.  And he draws these 
connections.

06:34:43.471 --> 06:34:45.471
We're rediscovering things as we
grow our machine learning tools.

06:34:47.439 --> 06:34:49.439
Things like the machine learning

06:34:51.556 --> 06:34:54.228
equivalent of source control.  
Machine learning equivalent of 

06:34:54.856 --> 06:34:56.856
continuous integration.

06:34:59.412 --> 06:35:01.878
And Noah make this is 
observation that this is lagging

06:35:01.879 --> 06:35:06.463
behind the software engineering 
side by 15-20 years.  So, this 

06:35:06.464 --> 06:35:09.397
creates a really interesting
opportunity, right?  We can look

06:35:09.398 --> 06:35:11.398
at software engineering.  We can
look at some of the things that 

06:35:11.686 --> 06:35:14.417
have happened there.  And think 
about what kind of impact 

06:35:18.747 --> 06:35:20.747
they may have on machine 
learning.  Right?

06:35:23.764 --> 06:35:26.045
So, looking at software  
engineering, there's something 

06:35:26.046 --> 06:35:29.783
so fundamental, it's almost easy
to skip over.  That's this idea 

06:35:29.784 --> 06:35:31.784
of  sharing code.  Shared 
repositories.

06:35:34.348 --> 06:35:37.270
On the surface, this makes us 
immediately more productive.  We

06:35:37.271 --> 06:35:40.580
can search for code, download 
it, use it.  But has really 

06:35:40.581 --> 06:35:44.777
powerful second order effects, 
right?  This changes the way we 

06:35:44.778 --> 06:35:49.306
write code.  We refactor our  
code.  We put it in libraries.  

06:35:49.307 --> 06:35:51.307
We share those libraries.

06:35:53.091 --> 06:35:56.008
And this really makes people 
even more productive.  And it's 

06:35:56.009 --> 06:35:58.894
the same dynamic that we want to
create for machine learning with

06:36:00.950 --> 06:36:02.950
TensorFlow Hub.

06:36:05.538 --> 06:36:07.538
TensorFlow Hub lets you build, 
share 

06:36:09.290 --> 06:36:11.366
and use pieces of machine 
learning.  So, why is this 

06:36:11.367 --> 06:36:15.293
important?  Well, anyone who has
done machine learning from 

06:36:15.294 --> 06:36:16.522
scratch knows you need a lot to 
do it

06:36:16.523 --> 06:36:22.352
well.  Now media an algorithm.  
You need data.  You need compute

06:36:22.353 --> 06:36:24.846
power and expertise.  And if 
you're missing any of these, 

06:36:26.527 --> 06:36:28.527
you're out of luck.

06:36:33.367 --> 06:36:35.367
So, TensorFlow Hub lets you 
distill 

06:36:36.900 --> 06:36:38.900
all these things down into a 
reusable 

06:36:41.866 --> 06:36:43.953
package called a module.  They 
can be  easily reused.

06:36:47.687 --> 06:36:51.393
So, you'll notice I'm saying  
"module" instead of "Model."  It

06:36:51.394 --> 06:36:53.460
turns out that a model is a 
little bit too big to encourage 

06:36:53.461 --> 06:36:58.032
sharing.  If you have a model, 
you can use that model if you 

06:36:58.033 --> 06:37:00.122
have the exact inputs it wants 
and you expect the exact outputs

06:37:00.739 --> 06:37:04.498
it provides.  If there's any 
little difference ises, you're 

06:37:04.499 --> 06:37:06.499
kind of out of luck.

06:37:07.611 --> 06:37:10.497
So, modules are a small piece, 
right?  If you think of a model,

06:37:10.498 --> 06:37:12.498
like a 

06:37:14.233 --> 06:37:16.233
binary, think of a module like a
library.

06:37:19.223 --> 06:37:21.223
So, on the inside, a module is 
actually a saved model.

06:37:23.377 --> 06:37:25.642
So this lets us package up the 
algorithm in the form of a 

06:37:25.643 --> 06:37:30.189
graph.  Package up the weights, 
you can do things like 

06:37:30.190 --> 06:37:33.098
initialize, use assets.  And our
libraries make it very easy to 

06:37:34.976 --> 06:37:38.097
instantiate these in
your TensorFlow code.  So, you 

06:37:38.098 --> 06:37:42.256
can compose these in interesting
ways.  This makes things very 

06:37:42.257 --> 06:37:46.226
reusable.  You can produce one 
of these and share it.  These 

06:37:46.227 --> 06:37:48.290
are also retrainable.  Once you 
patch it into your bigger 

06:37:50.764 --> 06:37:53.275
program, you can back propagate 
through it just like normal.  

06:37:53.276 --> 06:37:55.946
And this is really powerful 
because, if you do happen to 

06:37:55.947 --> 06:37:57.947
have enough data, 

06:37:59.053 --> 06:38:01.342
you can customize the tf.hub 
module for your own application.

06:38:02.810 --> 06:38:04.810
And to tell us a little bit more
about 

06:38:05.929 --> 06:38:08.829
some of those applications, I'll
hand it over to Andrew.

06:38:10.665 --> 06:38:12.665
&gt;&gt; Thanks Jer reMaya.

06:38:13.801 --> 06:38:16.682
Let's look at a specific example
of using a tf .

06:38:20.029 --> 06:38:22.029
hub module for image  
retraining.

06:38:24.008 --> 06:38:24.653
Say we're going to make an app 
to classify rabbit breeds from 

06:38:24.654 --> 06:38:26.654
photos.

06:38:27.784 --> 06:38:30.723
We have a couple hundred 
examples, not enough to train an

06:38:30.724 --> 06:38:32.724
entire image classification 
model from scratch.

06:38:36.132 --> 06:38:37.159
But we could start from an 
existing general purpose 

06:38:37.160 --> 06:38:39.202
classification model.  Most of 
the high-performing ones are 

06:38:40.683 --> 06:38:42.550
trained on millions of examples 
and they can easily classify 

06:38:42.551 --> 06:38:44.551
thousands of categories.

06:38:47.352 --> 06:38:49.631
So, we want to reuse the 
architecture and the trained 

06:38:49.632 --> 06:38:51.632
weights of that model without 
the classification  layers, and 

06:38:54.602 --> 06:38:57.281
in that way, we can add our own 
rabbit classifier on top and we 

06:38:57.282 --> 06:38:59.282
can train it on our own rabbit 
examples.

06:39:02.245 --> 06:39:04.245
And keep the re-used weights fix
fixed.

06:39:06.094 --> 06:39:08.094
So, since we're

06:39:09.538 --> 06:39:12.274
using TensorFlow Hub, our first 
stop is TensorFlow.org/hub.  We 

06:39:12.275 --> 06:39:14.554
can find the list of the newly 
released, state of the art, and 

06:39:14.555 --> 06:39:17.223
also the well-known image 
modules.  Some of them include 

06:39:17.224 --> 06:39:19.224
the classification layers.  And

06:39:21.726 --> 06:39:23.602
some of them remove them, just 
providing a feature vector as 

06:39:23.603 --> 06:39:28.776
output.  And so, we'll choose 
one of the feature vector ones 

06:39:28.777 --> 06:39:30.777
for this case.

06:39:33.140 --> 06:39:35.870
Let's use NASNet, a state of the
art image module created by a 

06:39:35.871 --> 06:39:39.244
neural network architecture 
search.  You paste the URL of 

06:39:39.245 --> 06:39:44.614
the module.  And TensorFlow Hub 
downloads the graph and all of 

06:39:44.615 --> 06:39:47.730
its weights and importing it 
into your model.  In that one 

06:39:47.731 --> 06:39:51.047
line, you're ready to use the 
module like any function.  So, 

06:39:51.048 --> 06:39:52.686
here we just provide a batch of 
inputs and we get back our 

06:39:52.687 --> 06:39:54.687
feature vectors.

06:39:56.412 --> 06:40:00.520
We add a classification layer on
top and output our predictions. 

06:40:00.521 --> 06:40:03.558
But in that one line, you get a 
huge amount of value.  In this 

06:40:03.559 --> 06:40:05.559
particular  case, more than 

06:40:07.249 --> 06:40:09.249
62,000 hours of GPU time went 
into 

06:40:11.729 --> 06:40:13.966
finding the best architecture 
for NASNet and  training the 

06:40:13.967 --> 06:40:15.967
result.

06:40:17.691 --> 06:40:19.750
All the expertise, the  testing,
the research that the author put

06:40:19.751 --> 06:40:23.484
into that, that's all built into
the module.  Plus that module 

06:40:23.485 --> 06:40:26.773
can be fine-tuned with your 
model.  So, if you have enough 

06:40:26.774 --> 06:40:28.619
examples, you
can potentially get better 

06:40:28.620 --> 06:40:33.412
performance if you use a 
low-learning rate, if you set 

06:40:33.413 --> 06:40:35.413
the trainable parameter to true,
and 

06:40:37.121 --> 06:40:39.121
if you use the training version 
of the graph.

06:40:40.918 --> 06:40:42.803
So, NASNet is available in a 
large size as well as a mobile 

06:40:42.804 --> 06:40:46.777
size module.  And then there's 
also the new progressive NASNet.

06:40:51.775 --> 06:40:53.858
And then a number of new 
MobileNet modules for doing 

06:40:53.859 --> 06:40:55.945
on-device image classification, 
as well as some industry 

06:40:58.210 --> 06:41:01.108
standard ones like Inception and
ResNet.  That list is at 

06:41:01.109 --> 06:41:06.723
TensorFlow.org/hub.  All those 
modules are pre-trained using 

06:41:06.724 --> 06:41:08.578
the TFslim check points and 
ready to be used for 

06:41:08.579 --> 06:41:11.314
classification or as feature 
vector inputs to your own model.

06:41:11.728 --> 06:41:15.685
Okay.  Let's look at another 
example.  In this case, doing a 

06:41:15.686 --> 06:41:17.686
little bit of text 
classification.

06:41:19.207 --> 06:41:21.207
So, we'd like to know whether a 

06:41:22.333 --> 06:41:24.333
restaurant review is a positive 
or negative sentiment.

06:41:26.469 --> 06:41:28.469
So, as Jeremiah mentioned, one 
of the 

06:41:30.612 --> 06:41:32.887
great things about TensorFlow 
Hub is it packages the graph 

06:41:32.888 --> 06:41:34.888
with the data.  For our modules,
that means that all 

06:41:40.407 --> 06:41:42.407
Doing things like normalizing 
and  tokennizing operations.

06:41:44.322 --> 06:41:45.942
So, we can use a  pre-trained 
sentence embedding module to map

06:41:45.943 --> 06:41:50.888
a full sentence to an embedding 
vector.  So, if we want to 

06:41:50.889 --> 06:41:54.853
classify some restaurant 
reviews, then we just take one 

06:41:54.854 --> 06:41:56.714
of those sentence embedding 
modules, we add our own 

06:41:56.715 --> 06:41:59.837
classification layer on top.  
And then we train with our 

06:41:59.838 --> 06:42:05.209
reviews.  We keep the sentence 
modules weight's fix weights 

06:42:05.210 --> 06:42:07.210
fixed.

06:42:09.548 --> 06:42:11.823
And just like for the image 
modules, TensorFlow.org/hub 

06:42:11.824 --> 06:42:13.824
lists a number of different text
modules.

06:42:16.415 --> 06:42:18.415
We have neural network language 
models 

06:42:21.951 --> 06:42:23.951
that are trained for English, 
Japanese, 

06:42:27.065 --> 06:42:29.065
Spanish, we haveword2vec trained
on Wikipedia.

06:42:36.648 --> 06:42:39.926
And EL MO that looks how words 
are used across context.  And 

06:42:39.927 --> 06:42:41.965
something really new, today, you
may have seen a new paper this 

06:42:41.966 --> 06:42:46.284
morning from the team, this is 
the universal sentence encoder.

06:42:53.085 --> 06:42:55.739
It's a sentence-level training 
module and enables a variety of 

06:42:55.740 --> 06:42:57.740
tasks, in other words, 
universal.

06:42:58.241 --> 06:43:00.241
Some of the things it's good 
for, 

06:43:01.529 --> 06:43:03.969
semantic similarity, custom text
classification, clustering and 

06:43:03.970 --> 06:43:05.970
semantic seven.

06:43:07.862 --> 06:43:12.779
search.  But the best thing is 
how little training is required 

06:43:12.780 --> 06:43:14.780
to adapt to your problem.  That 
sounds great in our particular 

06:43:14.840 --> 06:43:17.520
case.  Let's try it on the 
restaurant review task.

06:43:21.009 --> 06:43:23.009
So, we just paste that URL from 
the 

06:43:24.247 --> 06:43:27.120
paper, and like before, 
TensorFlow Hub downloads the 

06:43:27.121 --> 06:43:29.121
module and inserts it into your 
graph.

06:43:32.857 --> 06:43:34.857
But this time we're using the 
text 

06:43:36.962 --> 06:43:39.870
embedding column to feed into a 
classifier.  And this module can

06:43:39.871 --> 06:43:44.006
be  fine-tuned with your model 
by setting trainable to true.  

06:43:44.007 --> 06:43:46.686
Of course, you have to lower the
learning rate so that you don't 

06:43:46.687 --> 06:43:49.993
ruin the existing weights that 
are in there, but it's something

06:43:49.994 --> 06:43:54.352
worth  exploring if you have 
enough data.  Now, let's take a 

06:43:54.353 --> 06:43:57.454
closer look at that URL.  As 
Jeremiah mentioned, a module is 

06:43:57.455 --> 06:44:03.017
a program.  So, make sure what 
you're executing is from a 

06:44:03.018 --> 06:44:05.018
location that you trust.

06:44:07.156 --> 06:44:09.543
In this case, the module is from
tf.hub.dev. 

06:44:15.725 --> 06:44:18.644
That's our new news for 
Google-provided modules like 

06:44:18.645 --> 06:44:20.707
NASNet and the encoder.  We 
would like to make a place where

06:44:22.368 --> 06:44:24.652
you can publish the modules that
you create.  In this case, 

06:44:24.653 --> 06:44:26.920
Google is the publisher.  And 
universal sentencing encoder is 

06:44:28.966 --> 06:44:30.966
the name of the module.

06:44:32.256 --> 06:44:34.256
And finally, the version number 
is 1.

06:44:36.113 --> 06:44:41.786
So, TensorFlow Hub considers 
modules to be immutable.  That 

06:44:41.787 --> 06:44:43.427
way you don't have to worry 
about the weights changing 

06:44:43.428 --> 06:44:45.428
between training sessions.

06:44:47.603 --> 06:44:52.146
So, that module URL, and all of 
the module URLs on tf.hub.dev 

06:44:52.147 --> 06:44:54.147
include a version number.

06:44:55.482 --> 06:44:57.750
And you can take that URL and 
paste it into your browser and 

06:44:57.751 --> 06:45:02.517
see the complete documentation 
for any module that's hosted on 

06:45:02.518 --> 06:45:07.089
tf.hub .Dev.  Here's the 
particular one for the universal

06:45:07.090 --> 06:45:12.141
sentence encoder.  And then we 
also have modules for other 

06:45:12.142 --> 06:45:14.008
domains besides text 
classification and imagery 

06:45:14.009 --> 06:45:16.943
training, like a generative 
image module that 

06:45:20.873 --> 06:45:22.873
contains a progressive GAN that 
was trained on celeb A.

06:45:26.060 --> 06:45:28.060
And another model based on deep 
local 

06:45:29.351 --> 06:45:31.351
features network that can
identify landmark images.

06:45:37.067 --> 06:45:39.761
Both have great co-lab notebooks
on  TensorFlow.org/hub.  The 

06:45:39.762 --> 06:45:41.595
images here were created from 
them.  And we're adding more 

06:45:41.596 --> 06:45:45.689
modules over time for tasks like
audio and video over the next 

06:45:45.690 --> 06:45:47.798
few months.  But most 
importantly, we're really 

06:45:51.727 --> 06:45:53.727
excited to see what you build 
with TensorFlow Hub.

06:45:59.354 --> 06:46:02.174
Use the hashtag t tfhub, and 
visit TensorFlow.

06:46:05.920 --> 06:46:08.361
org/hub for examples of 
tutorials, interactive notebooks

06:46:08.362 --> 06:46:10.362
and code labs and 

06:46:12.212 --> 06:46:14.212
our new discussion mailing list.

06:46:15.270 --> 06:46:16.699
For everyone from our team in 
Zurich, I want to thank you so 

06:46:16.700 --> 06:46:18.744
much.

06:46:21.956 --> 06:46:23.956
[ Applause ]
Okay.

06:46:28.881 --> 06:46:30.391
Next up, Clemens and Raz will 
tell you about TensorFlow 

06:46:30.392 --> 06:46:35.561
Extended.
&gt;&gt; Thank you.  Hello, everyone.

06:46:40.546 --> 06:46:42.189
First, thanks, everyone, for 
coming to the TensorFlow Dev 

06:46:42.190 --> 06:46:46.958
Summit.  And second, thanks for 
staying around this long.  I 

06:46:46.959 --> 06:46:48.226
know it's been a long day and 
there's a lot of information 

06:46:48.227 --> 06:46:50.227
that we have been throwing at 
you.

06:46:54.018 --> 06:46:55.678
But we have much, much more and 
many more announce  wants.  

06:46:55.679 --> 06:46:58.658
Stick with me.  My name is 
Clemenss, and this is Ras.

06:47:01.778 --> 06:47:04.661
And we are going to talk about 
TensorFlow Extended today.  I'm 

06:47:04.662 --> 06:47:06.500
going to do a quick survey.  How
many of you do machine learning 

06:47:06.501 --> 06:47:10.625
in a research or academic 
setting?  Okay.  Quite a big 

06:47:10.626 --> 06:47:14.336
number.  And how many much you 
do machine learning in a 

06:47:14.337 --> 06:47:19.987
production setting?  Okay.  That
looks about half/half.  

06:47:19.988 --> 06:47:22.323
Obviously, also, a lot of 
overlap.  For those who do 

06:47:22.324 --> 06:47:25.848
machine learning in a production
setting, how many of you agree 

06:47:25.849 --> 06:47:29.556
with this statement?  Yeah?  
Some?  Okay.  I see a lot of 

06:47:29.557 --> 06:47:34.078
hands coming up.  So, everyone 
that I speak with who is doing 

06:47:34.079 --> 06:47:35.909
machine learning in production 
agrees with this statement.  

06:47:35.910 --> 06:47:38.362
Doing machine learning
in production is hard.  And it's

06:47:38.363 --> 06:47:40.363
too hard.

06:47:43.013 --> 06:47:45.013
Because after all, we want to

06:47:46.624 --> 06:47:48.067
democratize machine learning and
allow people to deploy machine 

06:47:48.068 --> 06:47:51.400
learning in their products.  One
of the reasons it's still hard 

06:47:51.401 --> 06:47:53.401
is 

06:47:54.474 --> 06:47:57.431
that in addition to the actual 
machine learning, the small, 

06:47:57.432 --> 06:48:00.123
orange box where you use 
TensorFlow, you may use Keras to

06:48:02.823 --> 06:48:03.859
put the layers in the model, you
need to worry about so much 

06:48:03.860 --> 06:48:05.860
more.  There's all of these 
other things you 

06:48:09.059 --> 06:48:11.744
need to learn about to actually 
deploy machine learning in a 

06:48:11.745 --> 06:48:13.745
production setting and serve it 
in your product.

06:48:16.907 --> 06:48:20.420
This is exact my what TensorFlow
Extended is about.  It's a 

06:48:20.421 --> 06:48:25.190
Google machine learning platform
that allows the users to go from

06:48:25.191 --> 06:48:27.256
data to a production serving 
machine learning models as fast 

06:48:27.257 --> 06:48:29.257
as possible.

06:48:32.444 --> 06:48:34.099
Now, before we introduced TFX, 
we saw that going through this 

06:48:34.100 --> 06:48:36.605
process of writing some of these
components, some 

06:48:39.753 --> 06:48:41.188
of them didn't exist before, 
glueing them together and 

06:48:41.189 --> 06:48:43.189
actually getting to a launch 
took anywhere between six and 

06:48:44.917 --> 06:48:46.917
nine months,
sometimes even a year.

06:48:52.152 --> 06:48:54.214
Once we deployed TFX and allowed
developers to use it, most can 

06:48:54.215 --> 06:48:59.425
use it and get up and running in
a day and get to a deployable 

06:48:59.426 --> 06:49:01.426
model in production in a matter 
of weeks or just a month.

06:49:05.205 --> 06:49:07.205
Now, TFX is a very large system 
and 

06:49:09.981 --> 06:49:12.487
platform that consists consists 
of of a lot of components and  

06:49:12.488 --> 06:49:17.117
services, so I can't talk about 
it all in the next 25 minutes.  

06:49:17.118 --> 06:49:19.585
We can only cover a small part. 
But talking about the things we 

06:49:19.586 --> 06:49:24.182
have open sourced and made 
available to you.  First, we're 

06:49:24.183 --> 06:49:27.275
going to talk about TensorFlow 
transform.  And how to apply 

06:49:27.276 --> 06:49:29.276
transformations on your data 
consistently between training 

06:49:29.540 --> 06:49:31.999
and serving.  Next, Ras is going
to introduce you to 

06:49:35.162 --> 06:49:36.176
a new product that we are open 
sourcing called TensorFlow model

06:49:36.177 --> 06:49:40.732
analysis.  We're going to give a
demo of how all of this works 

06:49:40.733 --> 06:49:45.510
together end-to-end.  And then 
we can make a broad announcement

06:49:45.511 --> 06:49:48.235
for our plans for TensorFlow 
Extended and sharing it with the

06:49:48.236 --> 06:49:52.179
community.  So, let's jump into 
TensorFlow transform first.  A

06:49:55.290 --> 06:49:58.191
typical ML  pipeline that you 
may see in the wild is during 

06:49:58.192 --> 06:50:01.765
training you do a -- you have a 
distributed data pipeline that 

06:50:01.766 --> 06:50:04.693
applies transformations to your 
data.  Because usually you train

06:50:04.694 --> 06:50:07.796
on a large amount of data.  This
needs to be distributed.

06:50:11.101 --> 06:50:12.963
And you run that  pipeline and 
sometimes materialize the output

06:50:12.964 --> 06:50:14.964
before you actually put it into 
your trainer.

06:50:17.941 --> 06:50:20.020
Now, at serving time, you need 
to somehow replay those exact 

06:50:20.846 --> 06:50:22.846
transformations online.

06:50:24.542 --> 06:50:27.288
As a new request comes in, it 
needs to be sent to your model. 

06:50:27.289 --> 06:50:30.655
Now, there's a couple of 
challenges with this.  The first

06:50:30.656 --> 06:50:32.294
one is, usually those two things
are very different code paths, 

06:50:32.295 --> 06:50:36.891
right?  The data distribution 
systems that you would use for 

06:50:36.892 --> 06:50:38.334
batch processing are very 
different from the libraries and

06:50:38.335 --> 06:50:42.878
tools that you would use to, in 
real-time, transform data to 

06:50:42.879 --> 06:50:44.879
make a request to your are 
model, right?

06:50:46.596 --> 06:50:48.596
So, now, you have two different 
code paths.

06:50:50.324 --> 06:50:52.608
Second, in many cases, it's very
hard to keep those two in sync. 

06:50:52.609 --> 06:50:56.732
And I'm sure a
lot of you have seen this.  You 

06:50:56.733 --> 06:50:59.631
change your batch processing 
pipeline and add a new feature 

06:50:59.632 --> 06:51:03.508
or change how it behaves.  And 
you need to make sure that the 

06:51:05.600 --> 06:51:06.869
code in the production system is
changed at the same time and 

06:51:06.870 --> 06:51:08.870
kept in sync.  And the third 
problem is, sometimes 

06:51:11.849 --> 06:51:13.730
you actually want to deploy your
TensorFlow machine learning 

06:51:13.731 --> 06:51:16.640
model in many different 
environments.  So, you want to 

06:51:16.641 --> 06:51:20.192
deploy it on a mobile device, on
a server, maybe you want to put 

06:51:20.193 --> 06:51:22.675
it on a car.  Now suddenly you 
have three different 

06:51:25.179 --> 06:51:26.833
environments where you want to 
apply these transformations, and

06:51:26.834 --> 06:51:31.006
maybe different languages for 
those and it's very hard to keep

06:51:31.007 --> 06:51:33.669
those in sync.  And this 
introduces something that we 

06:51:37.800 --> 06:51:39.040
call training serveing skew, the
transformations at training time

06:51:39.041 --> 06:51:41.041
might 

06:51:42.771 --> 06:51:44.771
be different than serving time, 
which 

06:51:45.868 --> 06:51:47.868
leads to bad quality of your 
serving model.

06:51:50.192 --> 06:51:52.525
TensorFlow transform addresses 
this by helping you write your 

06:51:52.526 --> 06:51:56.269
data processing log at training 
time.  Help you create those 

06:51:56.270 --> 06:51:58.270
data pipelines 

06:52:01.481 --> 06:52:03.535
And at the same time, it emits a
TensorFlow graph that can be in 

06:52:03.536 --> 06:52:05.536
line with your  training and 
serving model.

06:52:08.690 --> 06:52:10.578
Now, what this does is, it 
actually hermetically seals the 

06:52:10.579 --> 06:52:12.579
model.

06:52:13.697 --> 06:52:16.187
And your model takes a raw data 
request as input.  And all of 

06:52:16.188 --> 06:52:18.032
the transformations are actually
happening within the TensorFlow 

06:52:18.033 --> 06:52:22.608
graph.  Now, this is a lot of 
advantages.  And one of them is 

06:52:22.609 --> 06:52:24.609
that you no longer 

06:52:25.657 --> 06:52:27.692
have any code in your serving 
environment that does these 

06:52:28.922 --> 06:52:31.799
transformations, because they're
all being done in the TensorFlow

06:52:31.800 --> 06:52:37.380
graph.  Another one is wherever 
you deploy this TensorFlow 

06:52:37.381 --> 06:52:39.381
model, all of those 
transformations are applied in a

06:52:40.646 --> 06:52:43.110
consistent way, no matter where 
this graph is being evaluated.  

06:52:43.111 --> 06:52:45.651
Let's see how that looks.  This 
is a code snippet of a 

06:52:46.784 --> 06:52:50.904
pre-processing function that you
would write with tf.transform.  

06:52:50.905 --> 06:52:52.317
I'm going to walk you through 
what happens here and what we 

06:52:52.318 --> 06:52:55.054
need to do for this.  So, the 
first thing we do is normalize 

06:52:55.847 --> 06:52:58.509
this feature.  And as all
of you know, in order to 

06:52:58.510 --> 06:53:01.626
normalize a feature, we need to 
compute the mean and the 

06:53:01.627 --> 06:53:03.627
standard deviation.

06:53:07.808 --> 06:53:09.898
And to apply this 
transformation, we need to 

06:53:09.899 --> 06:53:12.396
divide by the standard 
deviation.  For the input 

06:53:12.397 --> 06:53:14.397
feature X, we have to compute 
the statistics.

06:53:16.317 --> 06:53:18.214
Which is a trivial task if the 
data fits into a single machine.

06:53:18.215 --> 06:53:20.215
You can do it easily.

06:53:21.528 --> 06:53:24.029
It's a non-trivial task if you 
have a gigantic training dataset

06:53:24.030 --> 06:53:28.604
and you have to compute these 
metrics effectively.  Once we 

06:53:28.605 --> 06:53:30.389
have these metrics, we can apply
this transformation to the 

06:53:30.390 --> 06:53:32.691
feature.  This is to show you 
that the output of 

06:53:36.897 --> 06:53:38.897
this transformation can then be,
again, 

06:53:40.160 --> 06:53:41.831
multiplied with another tensor, 
which is a regular TensorFlow 

06:53:41.832 --> 06:53:46.618
transformation.  And in order to
bucketize a  feature, you also, 

06:53:46.619 --> 06:53:49.088
again, need to compute the 
bucket boundaries to actually 

06:53:49.089 --> 06:53:52.183
apply this transformation.  And,
again, this is a distributed 

06:53:52.184 --> 06:53:56.701
data job to compute those 
metrics for the result of an  

06:53:56.702 --> 06:54:00.392
already-transformed feature.  
This is another benefit.  To 

06:54:00.393 --> 06:54:02.273
then actually
apply this transformation 

06:54:02.274 --> 06:54:07.019
transformation.   The next 
examples show you in the same 

06:54:07.020 --> 06:54:09.020
function you can apply any other

06:54:10.119 --> 06:54:12.372
tensor and tensor route 
function.  And there's also some

06:54:12.373 --> 06:54:14.373
of what we call 

06:54:19.950 --> 06:54:21.766
"Mappers" itpers" in tf .
transform that we don't have to 

06:54:21.767 --> 06:54:23.767
run a data pipeline to compute 
anything.

06:54:26.680 --> 06:54:28.536
Now, what happens is the orange 
boxes are what we call 

06:54:28.537 --> 06:54:32.906
analyzers.  We realize those as 
actual data pipelines to compute

06:54:32.907 --> 06:54:34.907
over your data.

06:54:37.913 --> 06:54:38.528
They're implemented using Apache
Beam.  We can talk about this 

06:54:38.529 --> 06:54:42.463
more later.  But this allows us 
to actually run this distributed

06:54:42.464 --> 06:54:45.569
data pipeline in different 
environments.  There's different

06:54:45.570 --> 06:54:48.693
runners for Apache Beam.  And 
all of the transforms are just 

06:54:50.751 --> 06:54:52.843
single instance-to-instance 
transformations using pure 

06:54:52.844 --> 06:54:55.528
TensorFlow code.  And what 
happens when you run 

06:54:58.877 --> 06:55:00.877
TensorFlow transform is that we 
actually 

06:55:02.170 --> 06:55:03.618
run these analyze phases, 
compute the results of the 

06:55:03.619 --> 06:55:05.682
analyze phases, and inject the 
result as a constant in the 

06:55:06.081 --> 06:55:08.742
TensorFlow graph.  So, this is 
on the right.  And in this graph

06:55:08.743 --> 06:55:13.771
is a hermetic TensorFlow graph 
that  applies all of the 

06:55:13.772 --> 06:55:18.964
transformations.  And it can be 
inlined in your serving graph.  

06:55:18.965 --> 06:55:21.424
Now your serving graph has the 
transform graph as part of it 

06:55:21.425 --> 06:55:25.963
and can play through these 
transforms wherever you want to 

06:55:25.964 --> 06:55:28.675
deploy this TensorFlow model.  
So, what can be done with 

06:55:28.676 --> 06:55:30.951
TensorFlow Transform?  At 
training time, for the batch 

06:55:32.649 --> 06:55:35.380
processing, really anything that
you can do with a distributed 

06:55:35.381 --> 06:55:39.357
data pipeline.  So, there's a 
lot of flexibility here, what 

06:55:39.358 --> 06:55:41.006
types of statistics you can 
compute.  We provide a lot of 

06:55:41.007 --> 06:55:43.007
utility functions for you.

06:55:44.744 --> 06:55:46.809
But you can also write custom 
data pipelines.  And at serving 

06:55:46.810 --> 06:55:49.313
time, because we generate a 
TensorFlow graph that applies 

06:55:51.207 --> 06:55:53.066
these transformations, we have 
limited to what you can do with 

06:55:53.067 --> 06:55:55.379
the TensorFlow graph.  But for 
all of you in TensorFlow, 

06:55:56.851 --> 06:55:58.851
there's a lot of flexibility in 
there as well.

06:56:00.593 --> 06:56:02.593
And so anything that you can do 
in a 

06:56:03.887 --> 06:56:07.202
TensorFlow graph you can do with
tf.transformations.   So, some 

06:56:07.203 --> 06:56:08.849
of the common use cases, the 
ones

06:56:08.850 --> 06:56:10.850
on the left I just spoke about.

06:56:12.381 --> 06:56:15.359
You can scale and continue value
to the core.  A value between 

06:56:15.360 --> 06:56:17.360
zero and one.

06:56:19.319 --> 06:56:21.605
You can bucketize a continuous 
value.  If you have text, you 

06:56:21.606 --> 06:56:25.106
can apply a bag of words or 
N-grams.  Or for future crosses,

06:56:25.107 --> 06:56:27.107
you can cross 

06:56:28.374 --> 06:56:31.419
those strings and then generate 
the result of those crosses.  

06:56:31.420 --> 06:56:33.040
And as mentioned before, the 
transformer is extremely 

06:56:33.041 --> 06:56:38.430
powerful in being able to chain 
look at these transforms.  You 

06:56:38.431 --> 06:56:41.752
can apply transform on the 
result of a transform as well.  

06:56:41.753 --> 06:56:43.609
And another particularly 
interesting transform is 

06:56:43.610 --> 06:56:45.917
applying another TensorFlow 
model.  You have heard about the

06:56:45.918 --> 06:56:51.295
safe model before.  If you have 
a safe model that you can apply 

06:56:51.296 --> 06:56:54.435
as a transformation, you can use
this it f.transform.

06:56:58.170 --> 06:57:00.727
You want to apply an Inception 
model and combine it with 

06:57:00.728 --> 06:57:02.728
another feature or 

06:57:04.476 --> 06:57:06.149
use it as an input feature to 
the model.  You can use any 

06:57:06.150 --> 06:57:08.150
TensorFlow model 

06:57:09.294 --> 06:57:12.691
that's in line in had your graph
and in your serving graph.  So, 

06:57:12.692 --> 06:57:15.002
all of this is available today. 
And you can go check it out on

06:57:15.003 --> 06:57:20.392
GitHub at  TensorFlow/transform.
I'm going to hand it over to Ras

06:57:20.393 --> 06:57:22.660
who is going to talk about 
TensorFlow model analysis.

06:57:25.415 --> 06:57:27.415
&gt;&gt; All right, thanks, Clemens.  
Hi, everyone.

06:57:30.166 --> 06:57:32.166
I'm really excited to talk about
TensorFlow model analysis today.

06:57:33.099 --> 06:57:38.462
We're going to talk little bit 
about metrics.  Let's see.  Next

06:57:38.463 --> 06:57:41.358
slide.  All right.  So, we can 
already get metrics today, 

06:57:41.359 --> 06:57:43.836
right?  We use TensorBoard.  
TensorBoard's awesome.

06:57:47.998 --> 06:57:49.238
You saw an earlier presentation 
today boutons.  It's a great 

06:57:49.239 --> 06:57:52.185
tool.  While you're training you
can watch your metrics, right?

06:57:56.320 --> 06:57:57.985
And if you're training isn't 
going well, you can save 

06:57:57.986 --> 06:58:01.098
yourself a couple of hours of 
your life, right?  Terminate the

06:58:01.099 --> 06:58:03.349
training, fix some things.  But 
let's say you have a trained 

06:58:03.350 --> 06:58:09.370
model already.  Are we done with
metrics?  Is that it?  Is there 

06:58:09.371 --> 06:58:12.825
anymore to be said about metrics
when we're done training well?  

06:58:12.826 --> 06:58:16.897
Of course there is.  We want to 
know how well our trained model 

06:58:16.898 --> 06:58:21.355
actually does for our target 
population.  Right?  And I would

06:58:21.356 --> 06:58:23.398
argue that we want to do this in
a distributed fashion over the 

06:58:24.013 --> 06:58:29.135
entire dataset.  Now, why 
wouldn't we just sample?  Why 

06:58:29.136 --> 06:58:34.228
wouldn't we just save more
hours of hour lives, right?  

06:58:34.229 --> 06:58:36.350
Just sample, make things fast 
and easy, right?  Start with a 

06:58:36.351 --> 06:58:40.210
large dataset.  You're going to 
slice that dataset.  I'm going 

06:58:40.211 --> 06:58:42.874
to look at people at noon.  
Noontime, right?  That's a 

06:58:42.875 --> 06:58:47.581
feature.  From Chicago.  My 
hometown.  Okay?   Running on 

06:58:47.582 --> 06:58:49.582
this particular device.

06:58:51.460 --> 06:58:53.460
Well, each of these slices 
reduced the 

06:58:55.338 --> 06:58:58.861
size of your evaluation dataset 
by a factor, right?  This is an 

06:58:58.862 --> 06:59:00.862
exponential decline.  By the 
time you're looking at the 

06:59:02.375 --> 06:59:05.055
experience for a particular, you
know, set of users, you're not 

06:59:05.056 --> 06:59:08.361
left with very much data, right?
And the error bars on your 

06:59:08.362 --> 06:59:12.334
performance measures, they're 
huge.  I mean, how do you know 

06:59:12.335 --> 06:59:14.616
that the noise doesn't exceed 
your signal by that point, 

06:59:14.617 --> 06:59:18.166
right?   So, really, you want to
start with your larger  dataset 

06:59:18.167 --> 06:59:22.992
before you start sliceing.  All 
right.  So, let's talk about a 

06:59:22.993 --> 06:59:29.215
particular metric.  I'm not sure
if, you know, who has heard of 

06:59:29.216 --> 06:59:31.216
the ROC Curve?

06:59:33.195 --> 06:59:34.239
It's kind of an unknown thing in
machine learning these days.  

06:59:34.240 --> 06:59:38.182
Okay.  So, we have our ROC
Curve.  And I'm going to talk 

06:59:38.183 --> 06:59:43.791
about a concept that you may or 
may not be familiar with, which 

06:59:43.792 --> 06:59:45.792
is ML fairness.  Okay.

06:59:47.524 --> 06:59:49.524
What is father fairness?

06:59:50.837 --> 06:59:52.837
Fairness is a complicated topic.

06:59:54.167 --> 06:59:56.446
Fairness is basically how well 
does our machine learning model 

06:59:56.447 --> 06:59:58.447
do for 

06:59:59.765 --> 07:00:01.765
different segments of our 
population?

07:00:02.870 --> 07:00:04.870
You don't have one ROC Curve, 
you have 

07:00:08.071 --> 07:00:10.581
an ROC Curve for everybody 
segment and group of  users.  

07:00:10.582 --> 07:00:12.467
Who here would run their 
business based on their top line

07:00:12.468 --> 07:00:14.468
metrics?  No one, right?  That's
crazy.

07:00:17.273 --> 07:00:19.273
You have to slice your metric 
metric.

07:00:20.781 --> 07:00:22.781
You have to dive in and find out
how things are going.

07:00:24.503 --> 07:00:26.503
That lucky user, black curve on 
the top, great  experience.

07:00:28.641 --> 07:00:29.891
The unlucky user, the blue 
curve, not such a great 

07:00:29.892 --> 07:00:31.892
experience.

07:00:33.221 --> 07:00:35.324
So, when can our models be 
unfair to various users, okay?

07:00:38.450 --> 07:00:40.737
Well, one instance is if you 
simply don't have a lot of data 

07:00:40.738 --> 07:00:42.738
from where which to draw your 
inferences, right?

07:00:48.923 --> 07:00:51.250
So, we use  tochiastic 
optimizers, and if we retrain 

07:00:51.251 --> 07:00:52.486
the model, it does something 
slightly

07:00:52.487 --> 07:00:57.240
different every time.  And you 
get high variance for some users

07:00:57.241 --> 07:00:59.965
because you don't have a lot of 
data there.  We have been 

07:00:59.966 --> 07:01:03.716
incorporating data from a lot of
data sources.  Some data sources

07:01:03.717 --> 07:01:05.458
are more biased than others.  
Some  users get the sort end of 

07:01:05.459 --> 07:01:09.599
the deal.  Other users get the 
ideal experience.  Our labels 

07:01:09.600 --> 07:01:15.022
could be wrong, right?  All of 
these things can happen.  And 

07:01:15.023 --> 07:01:17.336
here's TensorFlow model 
analysis.  You're looking here 

07:01:17.337 --> 07:01:21.757
at the UI hosted within a 
Jupiter notebook.  On the X axis

07:01:21.758 --> 07:01:23.829
we have our loss.  And you can 
see there's some natural 

07:01:25.509 --> 07:01:27.555
variance in the metrics, you 
know?  And we're not always 

07:01:27.556 --> 07:01:32.336
going to get spot on the same 
precision and recall for every 

07:01:32.337 --> 07:01:34.410
segment of  population.  But 
sometimes you'll see, you know, 

07:01:35.875 --> 07:01:38.213
what about those guys at the top
there?  Experiencing the highest

07:01:38.214 --> 07:01:42.365
amount of loss.  Do they have 
something in common?  We want to

07:01:42.366 --> 07:01:44.366
know this.

07:01:45.488 --> 07:01:48.004
Sometimes our -- our users that 
are our most -- that get the 

07:01:48.005 --> 07:01:52.106
poorest experience, they're 
sometimes our most vocal users,

07:01:52.107 --> 07:01:54.586
right?  We all know this.

07:01:58.967 --> 07:02:02.712
I'd like to invite you to come 
visit ml-fairness.com.  There's 

07:02:02.713 --> 07:02:04.713
a deep literature about the 

07:02:06.456 --> 07:02:08.456
mathematical side of ML 
Fairness.

07:02:11.681 --> 07:02:13.681
Once you have figureed out 
thousand measure fairness.

07:02:15.613 --> 07:02:17.272
How does TensorFlow model 
analysis give you the sliced 

07:02:17.273 --> 07:02:19.273
metrics?

07:02:20.582 --> 07:02:22.207
How do you go about getting the 
metrics?  Today, a saved model 

07:02:22.208 --> 07:02:24.489
for serving.  That's a familiar 
thing.

07:02:27.779 --> 07:02:29.779
TensorFlow model analysis is 
simple -- is similar.

07:02:31.105 --> 07:02:32.774
You export a saved model for 
evaluation.  Why are these 

07:02:32.775 --> 07:02:34.775
models different?  Why export 
two?

07:02:38.821 --> 07:02:41.110
Well, the eval graph that we 
serialized as a save model has 

07:02:41.111 --> 07:02:44.860
some additional annotations that
allow our evaluation batch job 

07:02:44.861 --> 07:02:47.110
to find the features, to find 
the prediction, to find the 

07:02:47.111 --> 07:02:51.295
label.  And we don't want those 
things, you know, mixed in with 

07:02:51.296 --> 07:02:55.823
our  serving graph.  So, we 
export a second one.  So, this 

07:02:55.824 --> 07:02:59.338
is the GitHub.  We just opened 
it I think last night at 4:30 

07:02:59.339 --> 07:03:02.657
p.m.  Check it out.  We have 
been  using it internally for 

07:03:03.711 --> 07:03:10.089
quite some time now.  Now it's
available externally as well.  

07:03:10.090 --> 07:03:12.090
The GitHub has an example that 
kind of puts it all together.

07:03:15.130 --> 07:03:16.150
So, that you can try all these 
components that we're talking 

07:03:16.151 --> 07:03:18.448
about from your local machine.  
You don't have to get an account

07:03:18.681 --> 07:03:22.399
anywhere.  You just get  cloned 
and run the scripts and run the 

07:03:22.400 --> 07:03:24.400
code lab.  This is the Chicago 
taxi example.

07:03:28.245 --> 07:03:29.903
So, we're using public data from
-- publicly-had available data 

07:03:29.904 --> 07:03:31.904
to determine 

07:03:34.658 --> 07:03:36.658
which riders will tip their 
driver and 

07:03:38.249 --> 07:03:40.249
which riders, shall we say, 
don't have 

07:03:43.473 --> 07:03:46.175
enough money to tiptoed tip 
today, right?  What does 

07:03:46.176 --> 07:03:48.176
fairness mean in this context?

07:03:50.423 --> 07:03:53.115
So, our model is going to make 
some predictions.  We may want 

07:03:53.116 --> 07:03:56.247
to slice these predictions by 
time of day.  During rush hour, 

07:03:56.248 --> 07:03:59.820
we're going to have a lot of 
data.  So, hopefully, our model 

07:03:59.821 --> 07:04:03.525
is going to be fair if that data
is not biased.  At the very 

07:04:03.526 --> 07:04:06.637
least, it's not going to have a 
lot of  variance.  But how is it

07:04:06.638 --> 07:04:09.780
going to do at 4 a.m. in the 
morning?  Maybe not so well.

07:04:12.890 --> 07:04:13.098
How is it going to do when the 
bars

07:04:13.099 --> 07:04:16.577
close?  An interesting question.
I don't know yet.  But I 

07:04:16.578 --> 07:04:19.443
challenge you to find out.  All 
right.  So, this is what you can

07:04:19.444 --> 07:04:27.052
run using your local scripts.  
We start with our raw data.  We 

07:04:27.053 --> 07:04:29.539
run the tf.transform.  The tf.

07:04:32.623 --> 07:04:34.481
transform emits a transform 
function and our transform 

07:04:34.482 --> 07:04:36.482
examples.  We train our model.

07:04:39.257 --> 07:04:41.257
Our model, again, emits two 
saved 

07:04:42.992 --> 07:04:44.871
models, one for serving and one 
for eval.  You can try this 

07:04:44.872 --> 07:04:47.334
locally.  Run scripts and play 
with this stuff.

07:04:50.664 --> 07:04:52.664
Clemens talked a little bit 
about transform.

07:04:53.973 --> 07:04:55.671
We want to take our dense 
features and scale them to a 

07:04:55.672 --> 07:05:01.724
particular Z-score.  We don't 
want to do that batch by batch. 

07:05:01.725 --> 07:05:05.894
The mean for each batch is going
to differ, may be fluctuations. 

07:05:05.895 --> 07:05:07.895
We want to normalize these 
things 

07:05:09.231 --> 07:05:11.287
across the entire dataset.  
We've built a vocabulary, we 

07:05:11.288 --> 07:05:15.784
bucket for the wide part of our 
model, and we emit our transform

07:05:15.785 --> 07:05:18.424
function, and into the trainer 
we go, right?

07:05:22.542 --> 07:05:25.661
You heard earlier today about tf
estimators.  And here is a wide 

07:05:25.662 --> 07:05:27.662
and deep estimator that takes 
our transform

07:05:29.428 --> 07:05:32.352
features and emits two save 
models.  And now we're in 

07:05:32.353 --> 07:05:37.135
TensorFlow model analysis which 
reads in the save model.  And 

07:05:37.136 --> 07:05:39.784
runs it against all of the raw 
data.  We call render slicing 

07:05:39.785 --> 07:05:44.149
metrics from the Jupiter 
notebook.  And you see the UI.  

07:05:44.150 --> 07:05:46.150
And the thing to notice here is 
that 

07:05:47.744 --> 07:05:50.025
this UI  is immersive, right?  
It's not just a static picture 

07:05:50.026 --> 07:05:55.627
that you can look at and go, 
huh, and walk away from.  It 

07:05:55.628 --> 07:05:58.522
lets you see your errors broken 
down by bucket or broken down by

07:05:58.523 --> 07:06:00.586
feature.  And it lets you drill 
in and ask 

07:06:04.138 --> 07:06:07.019
questions and be curious about 
how your models are actually 

07:06:07.020 --> 07:06:09.020
treating various subsets of your
population.

07:06:11.777 --> 07:06:14.696
Those subsets may be the 
lucrative subsets.  Right?  You 

07:06:14.697 --> 07:06:16.163
really want to drill in.  And 
then you want to serve your 

07:06:16.164 --> 07:06:18.164
model.

07:06:19.329 --> 07:06:21.329
So, our demo, our example, has a

07:06:24.257 --> 07:06:26.330
one-liner here that you can run 
to serve your model.

07:06:30.909 --> 07:06:32.909
And make a client request.

07:06:35.934 --> 07:06:38.215
The thing to notice here is 
we're mic making a GRPC request 

07:06:38.216 --> 07:06:40.216
to that server.

07:06:44.477 --> 07:06:46.477
We're takeing our feature 
tensors, 

07:06:47.680 --> 07:06:48.730
sending them to the server, and 
back

07:06:48.731 --> 07:06:51.637
comes probability, right?  
That's not quite enough.  We 

07:06:51.638 --> 07:06:55.203
have heard a little bit of 
feedback about this server.  And

07:06:55.204 --> 07:06:57.204
the thing we have heard is that 

07:06:58.905 --> 07:07:00.905
GRPC is cool, but Rust is really
cool.

07:07:04.072 --> 07:07:07.795
.  But  REST is really cool.  I 
tried.  This is one of the top 

07:07:07.796 --> 07:07:09.796
feature 

07:07:11.949 --> 07:07:13.949
requests on GitHub for model 
serveing.

07:07:16.498 --> 07:07:19.171
You can now pack your tensors 
into a JSON object.  Send that 

07:07:19.172 --> 07:07:21.172
JSON object to the server 

07:07:22.454 --> 07:07:25.126
and get a response back via 
HTTP.  Much more convenient.  

07:07:25.127 --> 07:07:29.708
And I'm very excited to say that
it will be released very soon.  

07:07:29.709 --> 07:07:31.709
Very soon.

07:07:35.136 --> 07:07:38.030
I see the excitement out there. 
Back to the end-to-end.  So, 

07:07:38.031 --> 07:07:40.031
yeah.

07:07:41.125 --> 07:07:43.595
So, you can try all of these 
pieces, end-to-end, all on your 

07:07:43.596 --> 07:07:45.596
local machine.

07:07:48.695 --> 07:07:51.245
Because they're using Apache 
Beam director rinser.  And 

07:07:51.246 --> 07:07:53.899
direct runners allow you to take
your distributed job and run 

07:07:53.900 --> 07:07:55.900
them all locally.

07:07:58.816 --> 07:08:01.517
If you swap in Apache Beam's 
Dataflow runner, you can run 

07:08:01.518 --> 07:08:04.254
against the entire dataset in 
the cloud.  The example also 

07:08:04.255 --> 07:08:07.157
shows you how to run the big job
against the cloud version as

07:08:07.369 --> 07:08:09.369
well.

07:08:11.097 --> 07:08:13.097
We're currently working with the

07:08:15.962 --> 07:08:17.594
community to develop a runner 
for Apache Flink, a runner for 

07:08:17.595 --> 07:08:22.937
Spark.  Stay tuned to the 
TensorFlow blog and to our 

07:08:22.938 --> 07:08:24.938
GitHub.

07:08:27.288 --> 07:08:31.554
And you can find the example at 
TensorFlow/model-analysis.  And 

07:08:31.555 --> 07:08:35.624
back to Clemens.
&gt;&gt; Thank you, Ras.

07:08:38.634 --> 07:08:40.634
[ Applause ]

07:08:43.861 --> 07:08:45.861
All right.

07:08:47.805 --> 07:08:49.707
We have heard about transform 
and how to use model analysis 

07:08:49.708 --> 07:08:54.479
and how to serve them.  You say 
you want more.  Is that enough? 

07:08:54.480 --> 07:08:56.480
You want more?  All right.  You 
want more.

07:08:58.850 --> 07:09:00.850
And I can think of why you want 
more.

07:09:02.372 --> 07:09:04.372
Maybe you read the paper we 
published 

07:09:05.494 --> 07:09:08.998
and presented last year about 
TensorFlow expended.  We laid 

07:09:08.999 --> 07:09:11.452
out a broad vision of how this 
platform works within Google and

07:09:11.453 --> 07:09:13.453
the 

07:09:14.969 --> 07:09:17.246
features and impact we have 
using it.  And that figure one, 

07:09:17.247 --> 07:09:19.924
which allows these boxes and 
described what 

07:09:24.281 --> 07:09:26.281
TensorFlow extrovert Extended.

07:09:28.259 --> 07:09:31.335
It's overly simplified, but much
more than today.  We spoke about

07:09:31.336 --> 07:09:33.336
these four components of 
TensorFlow Extended.

07:09:36.179 --> 07:09:38.179
This is not yet an end-to-end 
machine learning platform.

07:09:40.527 --> 07:09:42.527
This is just a very small piece.

07:09:43.836 --> 07:09:45.889
These are the libraryies we have
open sourced for you to use, but

07:09:45.890 --> 07:09:49.389
we haven't yet released the 
entire platform.  We are working

07:09:49.390 --> 07:09:52.544
hard on this.  We have seen the 
profound impact it had 

07:09:52.545 --> 07:09:56.844
internally.  How people could 
start using this platform and 

07:09:56.845 --> 07:09:59.552
deploying
machine learning in production 

07:09:59.553 --> 07:10:02.042
using TFX.  And we are working 
hard to make more 

07:10:05.660 --> 07:10:09.702
of those come don'ts components 
available to you.  And we are 

07:10:09.703 --> 07:10:11.793
looking at the data components 
and you can analyze the data, 

07:10:14.295 --> 07:10:16.390
visualize the distributions and 
detect anomalies.  That's an 

07:10:16.391 --> 07:10:20.541
important part of any machine 
learning pipeline.  To detect 

07:10:20.542 --> 07:10:22.542
changes and shifts in your data 
and anomalies.

07:10:24.695 --> 07:10:26.695
After this, looking at the 
horizontal 

07:10:28.043 --> 07:10:30.529
pieces that help tie all of 
these components together.  If 

07:10:30.530 --> 07:10:34.470
they're all single  libraries, 
you have to get them together.  

07:10:34.471 --> 07:10:36.765
You have to use them 
individually.  They have 

07:10:36.766 --> 07:10:38.195
well-defined interfaces, but you
have to combine them by  

07:10:38.196 --> 07:10:42.130
yourself.  Internally, we have 
the shared configuration 

07:10:42.131 --> 07:10:46.256
framework that allows you to 
configure the entire pipeline.  

07:10:46.257 --> 07:10:48.139
And the nice thing on the front 
end, allows you to monitor the 

07:10:48.140 --> 07:10:53.087
status of these pipelines and 
see progress and inspect the 

07:10:53.088 --> 07:10:54.736
different artifacts that have 
been produced by all of the 

07:10:54.737 --> 07:10:57.456
components.  So, this
is something we're also  looking

07:10:57.457 --> 07:10:59.457
to release later this year.  And
I think you get the idea.

07:11:02.499 --> 07:11:04.499
Eventual we want to make all of 
this available to the community.

07:11:06.861 --> 07:11:08.963
Because internally, hundreds of 
teams use this to improve our 

07:11:08.964 --> 07:11:10.964
products.

07:11:12.523 --> 07:11:14.598
We really believe that this will
be as transformative to the 

07:11:14.599 --> 07:11:16.599
community as it is at Google.

07:11:20.394 --> 07:11:22.394
And we're working very hard to 
release 

07:11:23.490 --> 07:11:25.552
these technologies in the 
platform to see what you can do 

07:11:25.553 --> 07:11:27.553
with them in your products and 
companies.

07:11:32.714 --> 07:11:34.714
Keep watching the TensorFlow 
blogs for our future plans.

07:11:37.175 --> 07:11:39.175
And as mentioned, you can use 
some 

07:11:42.466 --> 07:11:44.746
today, transform is released, 
model-analysis yesterday, 

07:11:44.747 --> 07:11:46.747
serving is released.

07:11:48.282 --> 07:11:51.912
And the end to end example is 
available under this link.  You 

07:11:51.913 --> 07:11:54.352
can find it under the model 
analysis repo.  Thank you from 

07:11:54.353 --> 07:11:58.300
myself and Ras.  I'm going to 
ask you to join me in welcoming 

07:11:58.301 --> 07:12:00.301
a special external guest, 
Patrick Brandt, joining us from 

07:12:02.465 --> 07:12:04.500
Coca-Cola who is going to talk 
about applied AI at Coca-Cola.  

07:12:04.501 --> 07:12:07.426
Thank you.
[ Applause ]

07:12:08.878 --> 07:12:13.901
&gt;&gt; Hey, great job.  Thanks, 
Clemens.  All  right.  So, yes, 

07:12:13.902 --> 07:12:15.902
I'm Patrick.

07:12:17.599 --> 07:12:20.318
I'm a solutions strategyist for 
Coca-Cola.  I'm going to share 

07:12:20.319 --> 07:12:21.964
with you how we're using 
TensorFlow to support some of 

07:12:21.965 --> 07:12:23.965
our 

07:12:27.573 --> 07:12:29.656
large largest and most popular 
digital marketing programs in 

07:12:29.657 --> 07:12:32.978
North America.  We are going off
on a marketing tangent before we

07:12:32.979 --> 07:12:36.886
come back.  As background, what 
is proof of purchase and the 

07:12:36.887 --> 07:12:39.963
relationship to marketing?  As 
an example, back in the day, 

07:12:39.964 --> 07:12:44.454
folks could clip the bar codes 
off their cereal boxes and mail 

07:12:44.455 --> 07:12:46.487
the bar codes back into the 
cereal company to receive a 

07:12:46.894 --> 07:12:52.405
reward.  Some kind of coupon or 
prize back through the mail.  

07:12:52.406 --> 07:12:54.435
And this is basic loyalty 
marketing.  Brands, in this 

07:12:54.436 --> 07:12:56.866
case, the cereal company, 
rewarding consumers who 

07:12:57.285 --> 07:13:01.001
purchase.  And at the same time,
opening up a line of 

07:13:01.002 --> 07:13:03.002
communication between the brand 
and the consumer.

07:13:04.748 --> 07:13:06.748
Now, over the last 15-some odd 
years 

07:13:08.263 --> 07:13:11.151
of marketing digitization, this 
concept has evolved into digital

07:13:11.152 --> 07:13:13.152
engagement marketing.

07:13:14.702 --> 07:13:16.702
Consumers in the moment in 
real-time

07:13:16.976 --> 07:13:18.840
through web and mobile channels.
But proof of purchase is still 

07:13:18.841 --> 07:13:20.841
an important component of the 
experience.

07:13:23.415 --> 07:13:25.415
We have an active digital 
marketing 

07:13:26.629 --> 07:13:28.629
program at Coca-Cola.

07:13:31.082 --> 07:13:33.082
They can earn a magazine sub 

07:13:35.265 --> 07:13:37.265
corruption or the chance to 
subscription 

07:13:39.054 --> 07:13:41.054
or a chance to win.

07:13:44.485 --> 07:13:46.485
We printed these 14-character 
product Pincodes.

07:13:47.846 --> 07:13:51.176
And these are what our consumers
enter into our promotions.  You 

07:13:51.177 --> 07:13:53.177
can enter them in by hand.  But 
on your mobile device, you can 

07:13:53.415 --> 07:13:56.105
scan them.  This had been the 
holy grail of 

07:13:59.216 --> 07:14:01.481
marketing IT at Coke for a long 
time.  We looked at commercial 

07:14:01.482 --> 07:14:03.482
and open source optical 
character recognition 

07:14:07.546 --> 07:14:08.348
software, OCR, but it could 
never read these codes very 

07:14:08.349 --> 07:14:10.628
well.  And the problem has to do
with the code.

07:14:16.194 --> 07:14:18.194
These are 4x7, 
dot-matrix-printed.

07:14:19.948 --> 07:14:22.408
The printer head is an inch 
under the cap, and they are 

07:14:22.409 --> 07:14:24.668
flying under the printer at a 
rapid rate.

07:14:28.806 --> 07:14:30.920
Creates visual artifacts, things
that normal OCR can't handle 

07:14:30.921 --> 07:14:34.246
well.  We knew if we wanted to 
unlock this experience, we were 

07:14:34.247 --> 07:14:37.339
going to have to build something
from scratch.  When I look at 

07:14:37.340 --> 07:14:41.641
these codes, a couple of 
characteristics jump out at me. 

07:14:41.642 --> 07:14:43.642
We're using a small alphabet.  
Let's say ten characters.

07:14:46.419 --> 07:14:47.894
And there's a decedent amount of
variability in the presentation 

07:14:47.895 --> 07:14:49.895
of these characters.

07:14:51.597 --> 07:14:53.920
This reminds me of MNIST, the 
online database of 60,000 

07:14:53.921 --> 07:14:55.921
handwritten digital images.

07:14:59.887 --> 07:15:01.887
And convolutional neural 
networks, 

07:15:03.200 --> 07:15:05.200
extremely good at extracting the
text.

07:15:06.936 --> 07:15:09.272
I'm probably going to tell you 
something you know, they work by

07:15:11.140 --> 07:15:12.809
breaking it down into smaller 
pieces and looking for edges and

07:15:12.810 --> 07:15:14.810
textures and colors.

07:15:16.514 --> 07:15:18.993
And these very granular feature 
activations are pooled up, 

07:15:18.994 --> 07:15:21.700
often, into a more general 
feature layer.  And then that's 

07:15:21.701 --> 07:15:27.061
filtered.  And those activations
are pulled up and so on until 

07:15:27.062 --> 07:15:29.513
the output of the neural net is 
run through a function which 

07:15:30.728 --> 07:15:33.373
creates a probability 
distribution of the likelihood 

07:15:33.374 --> 07:15:35.374
that a set of objects 

07:15:36.430 --> 07:15:38.430
exists within the image.

07:15:41.298 --> 07:15:43.157
But they had a nice property, 
handleing the nature of images 

07:15:43.158 --> 07:15:45.406
well.  From our perspective, 
they can handle

07:15:48.255 --> 07:15:49.884
the tilt and twist of a bottle 
cap held in someone's hand.  

07:15:49.885 --> 07:15:53.324
It's perfect.  So, this is what 
we're going to use.  We're going

07:15:53.325 --> 07:15:55.325
to move forward.

07:15:56.403 --> 07:15:57.639
Now we need to build our 
platform.  That begins with 

07:15:57.640 --> 07:16:02.816
training.  The  beating heart of
any applied AI solution.  And we

07:16:02.817 --> 07:16:05.503
knew we needed high quality 
images with accurate labels of 

07:16:05.504 --> 07:16:07.504
the codes 

07:16:08.567 --> 07:16:10.567
and we likely needed a lot of 
them.

07:16:12.280 --> 07:16:14.280
We started with a synthetic 
dataset of 

07:16:16.592 --> 07:16:18.444
random strings over blank images
and superimposed over blank 

07:16:18.445 --> 07:16:22.629
backgrounds.  This was a base 
for transfer learning once we 

07:16:22.630 --> 07:16:24.630
created our real-world data set.

07:16:26.165 --> 07:16:28.639
We did a production run of caps 
and fridge packs and 

07:16:28.640 --> 07:16:30.640
distributing those to 

07:16:32.367 --> 07:16:34.367
multiple third-party  displayers
along 

07:16:35.497 --> 07:16:36.537
with custom tools to scan the 
cap and label it with the pin 

07:16:36.538 --> 07:16:41.907
Code.  But an important 
component was an existing 

07:16:41.908 --> 07:16:43.349
pincode validation service we 
have had in production for a 

07:16:43.350 --> 07:16:46.709
long time to support our 
programs.  So, any time a 

07:16:46.710 --> 07:16:48.790
trainer labeled an image, we 
would send that label through 

07:16:50.430 --> 07:16:52.430
our validation service, if it

07:16:54.569 --> 07:16:57.255
was a valid pincode, we knew we 
had an accurate label.  This 

07:16:57.256 --> 07:16:59.328
gets the model trained and now 
we need to release it to the 

07:16:59.329 --> 07:17:01.617
wild.  We had some aggressive 
performance requirements.

07:17:05.987 --> 07:17:07.616
We wanted 1 second average 
processing time.  95% accuracy 

07:17:07.617 --> 07:17:12.574
at launch.  And host the model 
remotely, for the web, and embed

07:17:12.575 --> 07:17:16.137
it natively on mobile devices to
support mobile apps.  So, this 

07:17:16.138 --> 07:17:18.138
means that our model has to be 
small.

07:17:20.475 --> 07:17:22.761
Small enough to support over the
air updates as the model 

07:17:22.762 --> 07:17:27.112
improves over time.  And to help
us improve that model over time,

07:17:27.113 --> 07:17:29.606
we created an active learning 
UI, a user interface, that 

07:17:29.607 --> 07:17:32.097
allows our consumers to train 
the model once it's in 

07:17:32.098 --> 07:17:34.098
production.  And that's what 
this looks  like.

07:17:37.478 --> 07:17:39.478
So, if I was a consumer, scan a 
cap, 

07:17:40.588 --> 07:17:42.839
and the model cannot infer a 
valid pincode, it sends a  

07:17:42.840 --> 07:17:44.840
per-character 

07:17:46.367 --> 07:17:48.618
confidence of every character at
every position.  It can render a

07:17:48.619 --> 07:17:50.619
screen like what you see here.

07:17:53.359 --> 07:17:56.045
I, as a user, am only directed 
to address the particularly 

07:17:56.046 --> 07:17:58.319
low-confidence characters.  I 
see the

07:18:00.590 --> 07:18:02.841
characters, tap the red ones, 
bring up the keyboard, I tap 

07:18:02.842 --> 07:18:04.908
them, entered into the 
promotion.  It's a good user 

07:18:04.909 --> 07:18:06.909
experience for me.

07:18:08.450 --> 07:18:10.450
I scan a code and I'm only a few
taps 

07:18:11.943 --> 07:18:14.030
away from being entered into a 
promotion.  But we have 

07:18:14.031 --> 07:18:16.724
extremely valuable data for 
training.  Because we have the 

07:18:16.725 --> 07:18:18.725
image that created 

07:18:20.654 --> 07:18:22.289
the invalid difference as well 
as the user-corrected label they

07:18:22.290 --> 07:18:26.243
needed to correct to get into 
the promotion.  We can throw 

07:18:26.244 --> 07:18:27.946
this into the hopper for future 
rounds of training to improve 

07:18:27.947 --> 07:18:30.237
the model.  All right.  When you
put it all together, this is 

07:18:31.267 --> 07:18:33.267
what it looks like.

07:18:35.232 --> 07:18:37.232
User takes a picture of a cap, 
the 

07:18:38.346 --> 07:18:41.043
image is  normalized, then sent 
into our confident model.   The 

07:18:41.044 --> 07:18:43.044
output of which is a character 
probability matrix.

07:18:45.447 --> 07:18:47.312
This is the per-character 
competence of every character at

07:18:47.313 --> 07:18:51.082
every position.  That is further
analyzed to create a top ten 

07:18:51.083 --> 07:18:54.597
prediction.  Each one of those 
are fed into our pincode 

07:18:54.598 --> 07:18:56.880
validation  service.  The first 
one that's valid, often the 

07:18:58.336 --> 07:19:00.336
first one on the list, is
entered into the  promotion.

07:19:04.089 --> 07:19:05.302
And if none are valid, our user 
sees the active learning 

07:19:05.303 --> 07:19:10.005
experience.  So, our model 
development effort went through 

07:19:10.006 --> 07:19:12.036
three big  iterations.  In an 
effort to keep the model size 

07:19:15.095 --> 07:19:17.095
small up front, the data team 
used 

07:19:18.109 --> 07:19:20.109
binary

07:19:21.880 --> 07:19:23.880
binary  normalization and it 
didn't 

07:19:24.959 --> 07:19:26.992
produce enough data to get the 
model.  They switched and the 

07:19:26.993 --> 07:19:28.993
model size was 

07:19:30.903 --> 07:19:32.903
too large to support over the 
air updates.  They start over.

07:19:35.701 --> 07:19:38.361
They just completely rearchitect
the net using SqueezeNet, 

07:19:38.362 --> 07:19:41.907
designed to reduce the size by 
reducing the number of learnable

07:19:41.908 --> 07:19:43.908
parameters within the model.

07:19:45.409 --> 07:19:47.409
After making this move, we had a
problem.

07:19:50.368 --> 07:19:52.448
We started to experience 
internal  co-variant shift, the 

07:19:52.449 --> 07:19:54.449
result of reducing the number of
learnable parameters.

07:19:57.619 --> 07:20:00.539
That means that very small 
changes to upstream parameter 

07:20:00.540 --> 07:20:02.540
values cascaded to 

07:20:04.053 --> 07:20:06.357
huge gyrations in downstream 
parameter values.  This slowed 

07:20:06.358 --> 07:20:08.358
our training process.

07:20:09.482 --> 07:20:11.330
We had to grind through this 
co-variant shift to get the 

07:20:11.331 --> 07:20:13.331
model to

07:20:14.649 --> 07:20:16.918
converge, if it would converge 
at all.  We  introduced batch 

07:20:16.919 --> 07:20:19.418
normalization, which sped up  
training.  It got the model to 

07:20:19.419 --> 07:20:21.928
converge.  And now we're exactly
where we want to be.

07:20:26.084 --> 07:20:28.153
We have a model, a 25 fold 
decrease where we started with 

07:20:28.154 --> 07:20:32.923
accuracy greater than 95%.  And 
the results are impressive.  

07:20:32.924 --> 07:20:36.021
These are some screen grabs from
a test site that I built.  And 

07:20:36.022 --> 07:20:38.113
you can see across the top row 
how the model handled different 

07:20:38.114 --> 07:20:43.544
types of occlusion.  It handles 
translation, tilting the cap, 

07:20:43.545 --> 07:20:49.143
rotation, twisting the cap.  And
camera focus issues.  So, you 

07:20:49.144 --> 07:20:51.144
can try this out for yourself.

07:20:52.899 --> 07:20:54.779
I'm going to pitch the 
newly-launched Coca-Cola USA 

07:20:54.780 --> 07:20:56.780
app.

07:21:00.380 --> 07:21:03.974
It hit Android and iPhone app 
stores a couple days ago.  It 

07:21:03.975 --> 07:21:06.871
does many things, and you can 
scan a code.  You can go on with

07:21:06.872 --> 07:21:09.775
the mobile browser, take a 
picture of a cap code to be 

07:21:11.756 --> 07:21:13.756
entered into a promotion.  Quick
shoutouts.

07:21:17.435 --> 07:21:19.440
I can't not mention these folks.

07:21:23.801 --> 07:21:25.801
Quantiphi built our model.

07:21:26.935 --> 07:21:27.761
Ellen Duncan,  spearheaded this 
from the

07:21:27.762 --> 07:21:29.762
marketing side.

07:21:31.286 --> 07:21:33.286
And my people in IT, my 
colleague, 

07:21:34.593 --> 07:21:37.087
Andy Donaldson,  shepherded this
into production.  Thank you.  

07:21:37.088 --> 07:21:39.088
It's been a privilege to speak 
with you.

07:21:40.801 --> 07:21:42.801
I covered a lot of ground in ten
short minutes.

07:21:44.356 --> 07:21:47.459
There's a lot of stuff I didn't 
talk about.  Please feel free to

07:21:47.460 --> 07:21:49.460
reach out on 

07:21:50.745 --> 07:21:53.090
Twitter,@patrickbrandt.  And 
wpb.

07:21:56.967 --> 07:21:59.798
is wpb.is/linkedin.  You can 
read an article I  published 

07:22:01.851 --> 07:22:03.506
last year on this solution, on 
the Google developers

07:22:03.507 --> 07:22:05.507
&gt;&gt; AUDIENCE: .

07:22:07.239 --> 07:22:12.028
And you can get there at wpb. 
wpb.is/tensorFlow.  Thank you.  

07:22:12.029 --> 07:22:14.029
Next up is Alex.

07:22:15.137 --> 07:22:17.137
And Alex is going to talk to us 
about applied ML with robotics.

07:22:22.108 --> 07:22:24.108
[ Applause ]

07:22:29.824 --> 07:22:34.364
&gt;&gt; All right.  Hi, everybody.  
I'm Alex from the brain robotics

07:22:34.365 --> 07:22:36.365
team.  And in this presentation 
I'll be 

07:22:39.651 --> 07:22:42.080
talking about how we use 
simulation and adaptation in 

07:22:42.081 --> 07:22:44.081
some of our real-world robot 
learning problems.

07:22:46.550 --> 07:22:49.383
So, first, let me start by 
introducing robot learning.  The

07:22:49.384 --> 07:22:51.384
goal of robot learning is to use

07:22:52.460 --> 07:22:54.291
machine learning to learn 
robotic skills that work in 

07:22:54.292 --> 07:22:58.400
general environments.  What we 
have seen so far is that if you 

07:22:58.401 --> 07:23:00.499
control your environment a lot, 
you can get robots to do pretty 

07:23:00.500 --> 07:23:02.500
impressive things.

07:23:05.477 --> 07:23:07.477
And where the techniques break 
down 

07:23:08.553 --> 07:23:09.376
is when you try to apply these 
same techniques to more general 

07:23:09.377 --> 07:23:13.774
environments.  And the thinking 
is if you use machine learning, 

07:23:13.775 --> 07:23:15.775
this can help learn from the 
environment and address the 

07:23:15.841 --> 07:23:21.027
generalization issues.  So, as a
step in this direction, we have 

07:23:21.028 --> 07:23:24.144
been  looking at the problem of 
robotic grasping.  This is a 

07:23:24.145 --> 07:23:25.382
project we have been working on 
in collaboration with people at 

07:23:25.383 --> 07:23:27.383
X.

07:23:28.933 --> 07:23:31.623
And to explain the product a 
bit, we have the real robot arm 

07:23:31.624 --> 07:23:34.677
learning to pick
up objects out of a bin.  There 

07:23:34.678 --> 07:23:36.341
is going to be a camera looking 
down over the shoulder of the 

07:23:36.342 --> 07:23:38.342
arm into the bin.

07:23:39.664 --> 07:23:42.176
And from this RGB image, we're 
going to train a neural network 

07:23:42.177 --> 07:23:44.450
to learn what commands it should
send to the robot to 

07:23:45.284 --> 07:23:47.999
successfully pick up objects.  
Now, we to want try to solve 

07:23:48.000 --> 07:23:51.756
this task using as few 
assumptions as possible.  So, 

07:23:51.757 --> 07:23:53.757
importantly, we're not going to 

07:23:55.293 --> 07:23:57.293
give any information about the 
geometry 

07:23:58.380 --> 07:24:00.457
of what kinds of objects we're 
going pick up.  And no 

07:24:00.458 --> 07:24:03.159
information about the depth of 
the scene.  So, in order to 

07:24:03.160 --> 07:24:05.160
solve the task, the 

07:24:06.638 --> 07:24:07.934
model needs to learn hand-eye 
coordination or see where it is 

07:24:07.935 --> 07:24:11.438
in the camera image and then 
figure out where in the seam it 

07:24:11.439 --> 07:24:13.708
is and then combine these two to
figure out how it can move 

07:24:13.709 --> 07:24:15.709
around.

07:24:16.841 --> 07:24:18.290
Now, in order to train this 
model, we're going to need a lot

07:24:18.291 --> 07:24:22.896
of data because it's a pretty 
large-scale image model.  And 

07:24:22.897 --> 07:24:25.579
our solution at the time for 
this was to simply use more 

07:24:25.580 --> 07:24:27.580
robots.

07:24:28.681 --> 07:24:30.324
So this is what we called the 
arm farm.  These are six robots

07:24:30.325 --> 07:24:33.448
collecting data in parallel.  
And if you have six robots, you 

07:24:33.449 --> 07:24:36.768
can collect data a lot faster 
than if you only have one robot.

07:24:40.524 --> 07:24:42.566
So,  using these robots, we were
able to collect over a million 

07:24:42.567 --> 07:24:44.567
attempted 

07:24:46.099 --> 07:24:48.807
grasps over a million total of 
robot hours.  And we were able 

07:24:48.808 --> 07:24:50.808
to successfully train models to 
pick up objects.

07:24:52.763 --> 07:24:54.600
Now, this works, but it still 
took a lot of time to collect 

07:24:54.601 --> 07:24:59.127
this dataset.  This motivated 
looking into ways to reduce the 

07:24:59.128 --> 07:25:03.311
amount of real-world data needed
to learn these behaviors.  One 

07:25:03.312 --> 07:25:05.802
approach for doing this is 
simulation.  So, in the left 

07:25:05.803 --> 07:25:08.486
video here, you can see the 
images that are going into our 

07:25:10.327 --> 07:25:12.327
model in our  real-world setup.

07:25:14.728 --> 07:25:16.384
And on the right here you can 
see our simulated recreation of 

07:25:16.385 --> 07:25:21.153
that setup.  Now, the advantage 
of moving things into 

07:25:21.154 --> 07:25:24.470
simulation, is that simulated 
robots are easier to scale.  We 

07:25:24.471 --> 07:25:26.471
have been able to spin up 
thousands 

07:25:27.579 --> 07:25:29.469
of simulateed robots  grasping 
various objects.  And we were 

07:25:29.470 --> 07:25:32.365
able to collect millions of 
grasps in just over eight hours

07:25:35.479 --> 07:25:36.312
instead of the weeks that were 
required for the original 

07:25:36.313 --> 07:25:40.242
dataset.  Now, this is good for 
getting a lot of data, but 

07:25:40.243 --> 07:25:42.114
unfortunately, models trained in
simulation tend not to transfer 

07:25:42.115 --> 07:25:45.448
to the actual  real-world robot.
There are a lot of systemic 

07:25:46.897 --> 07:25:48.897
differences between the two.

07:25:50.237 --> 07:25:50.445
One big one is the visual 
appearancens of different 

07:25:50.446 --> 07:25:52.446
things.

07:25:53.539 --> 07:25:55.539
And another big one is just 
physical 

07:25:57.079 --> 07:25:58.955
differences between our 
real-world physics and our 

07:25:58.956 --> 07:26:03.899
simulated physics.   What we did
was we were able to train a 

07:26:03.900 --> 07:26:08.261
model in simulation to get to 
around 90% grasp success.  We 

07:26:08.262 --> 07:26:10.749
then deployed to the real robot 
and it succeeds just over 20% of

07:26:10.750 --> 07:26:15.673
the time, which is a very big 
performance drop.  So, in order 

07:26:15.674 --> 07:26:17.674
to get good performance, you 
need to do something a bit more 

07:26:17.704 --> 07:26:19.704
clever.

07:26:22.010 --> 07:26:24.080
This motivated looking into 
similar-to-real transfer, using 

07:26:24.081 --> 07:26:28.400
simulate the data to improve 
your real-world sample 

07:26:28.401 --> 07:26:33.797
efficiency.  Now, there are a 
few different ways you can do 

07:26:33.798 --> 07:26:35.798
this.

07:26:38.536 --> 07:26:40.230
One way is adding randomization 
into the simulator.  You can 

07:26:40.231 --> 07:26:42.505
change
around the textures you apply to

07:26:45.192 --> 07:26:46.632
objects, changing their colors, 
changing how lighting is 

07:26:46.633 --> 07:26:49.320
interacting with your scene.  
And you can play around with 

07:26:49.321 --> 07:26:52.229
changing the geometry of what 
kinds of objects you're trying 

07:26:52.230 --> 07:26:54.319
to pick up.  Another way of 
doing this is the main 

07:26:54.924 --> 07:26:56.924
adaptation.

07:26:58.257 --> 07:27:00.257
Which is a set of  techniques 
for 

07:27:02.165 --> 07:27:04.870
learning when rough you have two
domains of data that have 

07:27:04.871 --> 07:27:06.871
structure but are different.

07:27:08.583 --> 07:27:10.583
The two domains are the 
simulated and real robot data.

07:27:13.977 --> 07:27:17.691
And there are feature-level and 
pixel-level ways of doing this. 

07:27:17.692 --> 07:27:20.813
In this work, we tried all of 
these approaches.  And I'm going

07:27:20.814 --> 07:27:22.814
to focus primary on the domain 
adaptation side of things.

07:27:25.753 --> 07:27:28.017
So, in  feature-level domain 
adaptation, what we're going to 

07:27:28.018 --> 07:27:32.602
do is take our simulated data, 
take our real data, train the 

07:27:32.603 --> 07:27:34.603
same model on both datasets.

07:27:36.254 --> 07:27:38.254
But then

07:27:41.550 --> 07:27:44.031
at an intermediate feature 
layer, a similarity loss.  

07:27:44.032 --> 07:27:46.333
That's going to make the 
distribution of features to be 

07:27:46.334 --> 07:27:48.826
the same across both
domains.

07:27:52.763 --> 07:27:56.892
One approach for doing this is 
domain adversarial networks.  

07:27:56.893 --> 07:27:59.787
This is implemented as a small 
neural net that tries to predict

07:27:59.788 --> 07:28:01.788
the domain based on the 
interview questions put 

07:28:04.119 --> 07:28:07.049
features, and the rest of the 
model is trying to confuse the 

07:28:07.050 --> 07:28:09.135
domain classifier as much as 
possible.

07:28:14.700 --> 07:28:16.700
Now pixel methods look at it 
from a different point of view.

07:28:19.313 --> 07:28:21.313
Instead of features, we're going
to 

07:28:22.445 --> 07:28:24.445
transform at the pixel level to 
look for realistic.

07:28:26.330 --> 07:28:28.363
We take a general adversarial 
network.  We feed it an image 

07:28:28.364 --> 07:28:32.174
from our simulator.  It's going 
to output an image that looks 

07:28:32.175 --> 07:28:34.175
more realistic.

07:28:36.275 --> 07:28:37.939
And then we're going to use the 
generator to train whatever task

07:28:37.940 --> 07:28:41.239
model that we want to train.  
Now, we're going to train both 

07:28:41.240 --> 07:28:45.421
the generator and the task model
at the same time.  We found that

07:28:45.422 --> 07:28:47.921
in practice this was useful 
because it helps ground the 

07:28:50.623 --> 07:28:52.689
generator output to be useful 
for training your downstream 

07:28:52.690 --> 07:28:54.735
task.  All right.  So, taking a 
step back.

07:28:58.033 --> 07:29:00.033
Feature-level methods can learn 

07:29:01.558 --> 07:29:03.558
domain-invariant features when 
you

07:29:03.829 --> 07:29:05.829
have data from related domains 
that aren't  identical.

07:29:07.556 --> 07:29:08.994
Meanwhile, pixel-level methods 
can transform your data to look 

07:29:08.995 --> 07:29:14.371
more like your  real-world data,
but in practice, they don't work

07:29:14.372 --> 07:29:16.675
perfectly and there's small 
artifacts and  inaccuracieses 

07:29:16.676 --> 07:29:18.676
from 

07:29:20.921 --> 07:29:22.921
generateor

07:29:24.344 --> 07:29:26.344
output.

07:29:27.694 --> 07:29:30.187
You can use both methods.  Not 
getting all the way there, but 

07:29:33.310 --> 07:29:35.310
attach a feature-level to close 
the reality gap.

07:29:36.848 --> 07:29:38.848
And combine what we call the 
grasp, a 

07:29:40.211 --> 07:29:42.695
combination of pixel-level and 
feature-level.  In the left half

07:29:42.696 --> 07:29:46.676
of the video, a simulated grasp.
In the right half, the output of

07:29:46.677 --> 07:29:50.799
the generator.  You can see it's
learning cool things in terms of

07:29:50.800 --> 07:29:53.701
drawing what the tray should 
look like.  Drawing more 

07:29:53.702 --> 07:29:59.284
realistic textures on the arm.  
Drawing shadows.  It's learned 

07:29:59.285 --> 07:30:01.990
how to draw shadows as the arm 
is moving around in the scene.  

07:30:01.991 --> 07:30:04.914
It's not perfect.  There are 
still odd splotches of color 

07:30:07.206 --> 07:30:08.656
around, but it's definitely  
learning something about what it

07:30:08.657 --> 07:30:12.645
means for an image to
look for realistic.  Now, this 

07:30:12.646 --> 07:30:14.750
is good for  getting a lot of 
pretty images, but what matters 

07:30:14.751 --> 07:30:19.122
for our problem is whether these
images are actually useful for 

07:30:19.123 --> 07:30:23.034
reducing the amount of 
real-world data required.  And 

07:30:23.035 --> 07:30:25.911
we find that it does.  So, to 
explain this chart a bit, on 

07:30:29.284 --> 07:30:31.284
the X axis is the number of 
real-world samples used.

07:30:32.605 --> 07:30:34.910
And we compared the performance 
of different methods as we vary 

07:30:34.911 --> 07:30:38.605
the amount of real-world data 
given to the model.  The blue 

07:30:38.606 --> 07:30:40.606
bar is the performance with only
simulated data.

07:30:43.187 --> 07:30:45.444
The red bar is the performance 
when we use only real data, and 

07:30:45.445 --> 07:30:47.935
the orange bar is the 
performance when we use both 

07:30:50.401 --> 07:30:52.894
simulated and real data and the 
domain adaptation methods I have

07:30:52.895 --> 07:30:54.895
been talking about.

07:30:57.837 --> 07:31:00.301
When we use just 2% of the 
original real world data set, 

07:31:00.302 --> 07:31:03.610
we're able to get the same level
of performance.  This reduces 

07:31:03.611 --> 07:31:06.080
the number of real-world samples
we needed by up to 50 times, 

07:31:08.342 --> 07:31:09.560
which is really exciting in 
terms of not needing to run 

07:31:09.561 --> 07:31:11.561
robots for a long amount of time
to learn these grasping

07:31:12.479 --> 07:31:16.590
behaviors.  Additionally, we 
found that even when we give all

07:31:16.591 --> 07:31:18.466
of the real-world data to the 
model, when we give simulated 

07:31:18.467 --> 07:31:20.916
data as well, we're still able 
to see improved performance.

07:31:24.653 --> 07:31:26.508
So, that implies that we haven't
hit data capacity limits for 

07:31:26.509 --> 07:31:28.989
this grasping problem.  And 
finally, there's a way to train 

07:31:32.062 --> 07:31:34.062
this setup without having 
real-world  labels.

07:31:35.190 --> 07:31:37.247
And when we train the model in 
this  setsing, we found that we 

07:31:37.248 --> 07:31:41.407
were still able to get pretty 
good performance on the 

07:31:41.408 --> 07:31:43.408
real-world robot.

07:31:45.353 --> 07:31:47.023
Now, this was a work of a large 
team across both brain as well 

07:31:47.024 --> 07:31:51.606
as X.  I would like to thank all
of my collaborators.  Here is a 

07:31:51.607 --> 07:31:54.473
link to the original  paper.  
And I believe there is also a 

07:31:54.474 --> 07:31:56.946
blog post if people are 
interested in hearing more 

07:31:56.947 --> 07:32:00.027
details.  Thanks.

07:32:03.030 --> 07:32:08.263
[ Applause ]
All right.  And lastly, we have 

07:32:08.264 --> 07:32:12.236
Sherol who is going to talk to 
you about art and music with 

07:32:12.237 --> 07:32:14.299
machine learning.
&gt;&gt; Amazing.  Just amazing.

07:32:18.676 --> 07:32:20.962
It's really just in awe of what 
machine learning is capable of 

07:32:20.963 --> 07:32:23.457
and how we can extend human 
capabilities.

07:32:27.428 --> 07:32:28.874
And we want to think more than 
just about, you know, 

07:32:28.875 --> 07:32:30.748
discovering new approaches and 
new ways of using the 

07:32:30.749 --> 07:32:33.433
technology.  We to want see how 
it's being used how 

07:32:37.950 --> 07:32:39.950
it impacts the human creative 
process.

07:32:42.034 --> 07:32:44.034
So, imagine you need to find or 
compose a drum pattern.

07:32:47.584 --> 07:32:50.324
You have some idea of -- of a 
drum beat that you would like to

07:32:50.325 --> 07:32:52.325
compose.

07:32:53.394 --> 07:32:56.279
And all you need to do now is go
to a website where there's a 

07:32:56.280 --> 07:32:58.280
pre-trained 

07:33:00.036 --> 07:33:02.946
model of drum patterns sitting 
on the online.  You just need a 

07:33:02.947 --> 07:33:08.137
web browser.  You give it some 
human input and you can generate

07:33:08.138 --> 07:33:11.240
a space of expressive 
variations.  You can tune and 

07:33:11.241 --> 07:33:13.241
control the type of outputs that
you're getting from this 

07:33:13.319 --> 07:33:18.510
generative model.  And if you 
don't like it, you can continue 

07:33:18.511 --> 07:33:24.316
going through it exploring this 
generative space.  So, this is 

07:33:24.317 --> 07:33:26.823
the type of work that project 
Magenta focuses on.

07:33:30.318 --> 07:33:32.318
To give you a bird's eye view of
what Project

07:33:32.611 --> 07:33:34.714
Magenta is about, it  basically 
is a group of researchers and 

07:33:34.715 --> 07:33:37.416
developers and creative 
technologists that engage in 

07:33:38.658 --> 07:33:40.658
generative models research.

07:33:41.970 --> 07:33:43.970
So, you'll see this work pub u 
pub 

07:33:44.980 --> 07:33:48.772
lushed published at machine 
learning conferences, a lot of 

07:33:48.773 --> 07:33:52.926
research contributions from 
Magenta.  And you'll see the  

07:33:52.927 --> 07:33:55.840
code, after it's been published,
put into open source 

07:33:58.908 --> 07:34:01.522
repository on GitHub in the 
Magenta repo.  And from there, 

07:34:01.523 --> 07:34:03.523
we'll see ways of thinking and 
designing creative tools 

07:34:06.601 --> 07:34:09.160
that can enhance and extend 
human expressive creative 

07:34:09.161 --> 07:34:11.161
process.

07:34:13.181 --> 07:34:15.863
And eventually ending up into 
the hands of artists and 

07:34:15.864 --> 07:34:19.823
musicians.  Inventing new ways 
we can create.  And inventing 

07:34:19.824 --> 07:34:21.824
new types of artists.

07:34:22.876 --> 07:34:24.298
So, I'm going to give three 
brief overviews of the 

07:34:24.299 --> 07:34:26.299
highlights of some of our recent
work.

07:34:28.003 --> 07:34:31.049
So, this is performance RNN.  
How many have seen this?  This 

07:34:31.050 --> 07:34:32.682
is one of the demos earlier 
today.  A lot of people have 

07:34:32.683 --> 07:34:34.683
seen and heard of 

07:34:36.000 --> 07:34:38.715
this kind of work.  This is what
people think of as a generative 

07:34:38.716 --> 07:34:42.879
model.  How can we build a
computer that has the kind of 

07:34:42.880 --> 07:34:44.880
intuition 

07:34:47.540 --> 07:34:49.540
to know the qualities of

07:34:51.180 --> 07:34:54.075
melody and harmony and 
expressing dynamics.  It's more 

07:34:54.076 --> 07:34:56.957
interesting to explore this in 
the browser enabled by 

07:34:56.958 --> 07:35:01.529
TensorFlow.js.  This is a demo 
we have running online.  We have

07:35:01.530 --> 07:35:03.401
the ability to tune and control 
some of the output that we're 

07:35:03.402 --> 07:35:07.363
getting.  So, in a second, I'm 
going to show you this video of 

07:35:07.364 --> 07:35:10.020
what that looks like.  You would
have seen it out on the demo 

07:35:10.445 --> 07:35:16.048
floor.  But we'll show you, and 
all of you watching online.  We 

07:35:16.049 --> 07:35:18.049
were able to bring it even more 

07:35:20.205 --> 07:35:22.205
alive by connecting a baby grand
piano 

07:35:24.395 --> 07:35:26.696
that is also a midi controller 
and we have the ability to 

07:35:26.697 --> 07:35:30.615
perform alongside the generative
model, reading in the inputs 

07:35:30.616 --> 07:35:32.616
from the human playing the 
piano.

07:35:33.949 --> 07:35:35.949
So, let's take a look.

07:35:39.134 --> 07:35:41.174
So, this is trained on classical
music data from actual live 

07:35:41.175 --> 07:35:43.175
performers.

07:35:45.100 --> 07:35:47.100
This is from a dataset that we 
got 

07:35:48.846 --> 07:35:50.846
from a piano competition.

07:35:54.454 --> 07:35:56.454
[piano playing] -- I don't know 
if 

07:35:57.827 --> 07:35:59.464
you noticed, this is Nikhil from
earlier today.  He's a talented 

07:35:59.465 --> 07:36:01.465
young man.  He

07:36:04.543 --> 07:36:06.543
helped build out the browser 
version.

07:36:12.743 --> 07:36:14.828
[piano playing] and so, we're 
thinking of ways that we take 

07:36:14.829 --> 07:36:16.829
bodies of 

07:36:18.097 --> 07:36:20.540
work, we train a model off of 
the data, then we create these 

07:36:20.541 --> 07:36:22.541
open source tools 

07:36:23.608 --> 07:36:24.828
that enable new forms of 
interaction, of creativity and 

07:36:24.829 --> 07:36:29.916
of expression.  And this is all 
these points of engagement are 

07:36:29.917 --> 07:36:34.607
enabled by TensorFlow.  The next
tool I want to talk about that 

07:36:34.608 --> 07:36:38.135
we have been working on is the 
variational autoencoders.  How 

07:36:38.136 --> 07:36:40.205
many people are familiar with 
latent space interpolation?  

07:36:40.206 --> 07:36:45.073
Quite a few of you.  If you're 
not, it's quite simple.  You 

07:36:45.074 --> 07:36:47.120
take human inputs and you train 
it through a neural network, 

07:36:47.121 --> 07:36:51.414
compressing it down to an 
embedding space.  You compress 

07:36:51.415 --> 07:36:54.477
it down to some dimensionality 
and then you reconstruct it.  

07:36:54.478 --> 07:36:55.488
So, you're comparing the 
reconstruction with the 

07:36:55.489 --> 07:37:01.207
original.  And trying to train 
-- build a space around that.  

07:37:01.208 --> 07:37:03.043
And what that does, is that 
creates the ability to 

07:37:03.044 --> 07:37:05.491
interpolate from one point to 
another, touching on the 

07:37:08.330 --> 07:37:08.944
intermediate points where human 
may have not

07:37:08.945 --> 07:37:10.945
given input.

07:37:12.210 --> 07:37:13.845
So, the machine learning model 
may have never seen an example 

07:37:13.846 --> 07:37:16.488
it's able to generate.  It's 
building an intuition off of 

07:37:17.492 --> 07:37:21.555
these examples.  You can imagine
if you're an animator, there's 

07:37:21.556 --> 07:37:23.792
so many ways of going from cat 
to pig.  How would you animate 

07:37:23.793 --> 07:37:27.858
that?  There's an intuition the 
artist would have in creating 

07:37:27.859 --> 07:37:30.505
that sort of morphing from one 
to the other.  We're able to 

07:37:30.506 --> 07:37:34.789
have the machine learning model 
also do this now.  We can also 

07:37:34.790 --> 07:37:36.842
do this with sound, right?  This
technology actually carries over

07:37:38.064 --> 07:37:40.064
to multiple domains.  So, this 
is NSynth.

07:37:42.995 --> 07:37:45.848
And we've released this I think 
sometime last year.  And what it

07:37:45.849 --> 07:37:50.118
does, it takes that same idea of
moving one input to another.  

07:37:50.119 --> 07:37:52.119
Let's take a look.  You'll get a
sense of it.

07:37:56.381 --> 07:37:58.381
Piccolo to electric guitar

07:38:03.145 --> 07:38:06.041
.  [sound moving back and forth]
-- so, rather than recompose, or

07:38:06.042 --> 07:38:10.217
fading from one sound to the 
other, what we're actually able 

07:38:10.218 --> 07:38:12.218
to do is we're able to 

07:38:13.522 --> 07:38:14.808
find these intermediary, 
recomposed sound samples and 

07:38:14.809 --> 07:38:17.264
produce that.  So, it looks, you
know, there's a lot of 

07:38:17.265 --> 07:38:19.265
components to 

07:38:20.284 --> 07:38:22.815
that, there's a wave net 
decoder.  But really it's the 

07:38:22.816 --> 07:38:24.816
same technology 

07:38:26.800 --> 07:38:29.512
underlying the encoder, decoder.
When we think about the types of

07:38:29.513 --> 07:38:33.699
tools that musicians use, we 
think less about  training 

07:38:33.700 --> 07:38:37.572
machine learning models.  We 
think about drum pedals.  Not 

07:38:37.573 --> 07:38:41.824
drum pedals.  Guitar pedals.  
They are used to refine sound to

07:38:44.126 --> 07:38:47.410
cultivate the art and flavor the
musician is looking for.  We 

07:38:47.411 --> 07:38:49.411
don't think about parameter 
flags 

07:38:50.531 --> 07:38:53.071
or trying to write lines of 
Python code to create the sort 

07:38:53.072 --> 07:38:55.072
of art, you know, in general.

07:38:57.082 --> 07:38:58.333
So, what we've done, not just 
are we interested in finding and

07:38:58.334 --> 07:39:03.134
discovering new things, we're 
also interested in how those 

07:39:03.135 --> 07:39:07.297
things get used in general.  
Used by practitioners.  Used by 

07:39:07.298 --> 07:39:12.272
specialists.  And so, we've 
created a hardware.  We've taken

07:39:12.273 --> 07:39:16.203
the piece of hardware.  We have 
taken the machine learning model

07:39:16.204 --> 07:39:18.204
and put it into a box where a 

07:39:19.299 --> 07:39:20.340
musician can plug in and explore
this latent space and 

07:39:20.341 --> 07:39:22.623
performance.  Let's look at what
musicians feel

07:39:26.171 --> 07:39:28.171
and what thing in this process.

07:39:33.885 --> 07:39:36.773
[synth music]
&gt;&gt; It feels like we're turning a

07:39:36.774 --> 07:39:39.909
new corner of new possibilities.
It could generate a sound that 

07:39:39.910 --> 07:39:42.991
might inspire us.
&gt;&gt; The fun part is you think you

07:39:42.992 --> 07:39:46.322
know what you're doing, there's 
a weird interaction happening 

07:39:46.323 --> 07:39:48.997
that could give you something 
totally unexpected.

07:39:51.718 --> 07:39:53.718
&gt;&gt; I mean, it's great research, 
and it's 

07:39:54.839 --> 07:39:57.760
really fun and it's amazing to 
discover new things, but even 

07:39:57.761 --> 07:40:01.302
more amazing to see how it gets 
used how people think to create 

07:40:01.303 --> 07:40:05.047
alongside it.  And so, what's 
even better is it's just 

07:40:05.048 --> 07:40:10.573
released.  And in collaboration 
with the creative lab London.  

07:40:10.574 --> 07:40:14.496
NSynth super.  It's open source.
All the specs are on GitHub.

07:40:20.790 --> 07:40:22.459
We talk about potential to the 
touch, and the code and what 

07:40:22.460 --> 07:40:24.760
hardware it's running on.  This 
is all available to everyone 

07:40:24.973 --> 07:40:29.912
today.  You can go online and 
check it out yourself.  Now, 

07:40:29.913 --> 07:40:31.343
music is more than just sound,  
right?  It's actually a sequence

07:40:31.344 --> 07:40:33.344
of things that goes on.

07:40:35.402 --> 07:40:37.402
So, when we think about the -- 
this 

07:40:38.685 --> 07:40:41.167
idea of what it needs to have a 
generative music space, we think

07:40:41.168 --> 07:40:44.512
also about melodies.  And so, 
just like we have cat to pig, 

07:40:46.572 --> 07:40:48.572
what is it like to go from one 
melody to the next?

07:40:49.878 --> 07:40:51.985
And moreover, once we have that 
technology, how does it -- what 

07:40:51.986 --> 07:40:55.239
does it look like to create with
that?  You have this expressive 

07:40:55.240 --> 07:40:58.831
space of variations.  How
do we design an expressive tool 

07:40:58.832 --> 07:41:03.996
that takes advantage of it?  And
what will we get out of it?  

07:41:03.997 --> 07:41:06.528
This is another tool that's 
developed by another team at 

07:41:06.529 --> 07:41:08.529
Google to make use of melodies 
in a latent space.

07:41:11.929 --> 07:41:13.395
How interpolation works and then
building a song or a composition

07:41:13.396 --> 07:41:15.396
with it.  Take a listen.

07:41:18.198 --> 07:41:20.198
Say you have two melodies.

07:41:26.052 --> 07:41:27.903
[twinkle, twinkle little star 
--]

07:41:27.904 --> 07:41:29.904
And the middle.

07:41:33.701 --> 07:41:35.701
[melody is morphing]

07:41:37.027 --> 07:41:39.027
And extended.

07:41:40.836 --> 07:41:42.836
[melody

07:41:48.429 --> 07:41:52.502
is becomeing more complex]
And we really are just 

07:41:52.503 --> 07:41:56.694
scratching the surface of what's
possible.  How do we continue to

07:41:56.695 --> 07:41:59.814
have the machine learn and have 
a better intuition for what 

07:41:59.815 --> 07:42:01.903
melodies are about?  So, again, 
to bring it back full 

07:42:05.683 --> 07:42:07.744
circle, we have, using different
compositions and musical works, 

07:42:07.745 --> 07:42:09.745
we're 

07:42:11.466 --> 07:42:13.343
able to train a variational 
autoencoder to create an 

07:42:13.344 --> 07:42:15.452
embedding space that builds 
tools that enable open source 

07:42:18.531 --> 07:42:21.416
communities to design creative 
artist tools.  To look at new 

07:42:21.417 --> 07:42:24.341
ways of  pushing the expressive 
boundaries that we currently 

07:42:24.549 --> 07:42:28.675
have.  This is, again, just 
released.  It's on our blog.  

07:42:28.676 --> 07:42:33.298
All the code is open source.  
And made available to you.  And 

07:42:33.299 --> 07:42:35.373
also enabled by TensorFlow.  In 
addition to all these other 

07:42:35.374 --> 07:42:37.374
things, 

07:42:40.133 --> 07:42:42.657
including Nikhil, here enabled 
by the type of work and 

07:42:42.658 --> 07:42:45.759
creativity and expressivity.  
And so, in wrapping  up, I want 

07:42:45.760 --> 07:42:49.251
to take us back to this demo 
that we saw.  Now, the most 

07:42:49.252 --> 07:42:51.310
interesting and maybe the 
coolest thing about this demo 

07:42:51.311 --> 07:42:55.072
was that we didn't even know 
that it was being built until

07:42:59.794 --> 07:43:02.697
it was Tweeted by T Tero, a 
developer from Finland.  And the

07:43:02.698 --> 07:43:04.698
fact of the matter is, this just
-- we're barely scratching the 

07:43:05.170 --> 07:43:09.120
surface.  There's so much to do,
so much to engage in and so much

07:43:09.121 --> 07:43:13.238
to discover.  And we want to see
so much more of this.  We want 

07:43:13.239 --> 07:43:15.087
to see more developers, more 
people sharing things and more 

07:43:15.088 --> 07:43:18.157
people getting engaged.  Not 
just developers, but artists and

07:43:18.766 --> 07:43:23.489
creatives as well.  We to want 
explore and invent and imagine 

07:43:23.490 --> 07:43:25.334
what we can do with machine 
learning together as an 

07:43:25.335 --> 07:43:27.335
expressive tool.

07:43:29.458 --> 07:43:31.458
And so, go to our website, g.

07:43:33.332 --> 07:43:36.009
co/magenta, you'll find our 
publications and these demos.  

07:43:36.010 --> 07:43:38.010
You can experience it yourself 
and more.

07:43:39.118 --> 07:43:41.169
And you can also join our 
discussion group.  So, here's g.

07:43:42.402 --> 07:43:45.053
co/magenta.  Join our discussion
group.  Become part of the 

07:43:45.054 --> 07:43:47.745
community and share the things 
that you're building so we can 

07:43:47.746 --> 07:43:51.045
do this alongside together.  
Thank you so much.

07:43:54.257 --> 07:43:58.925
[ Applause ]
So, that's it for the talks 

07:43:58.926 --> 07:44:04.064
today.  We've had an amazing, 
amazing show.  Amazing spread of

07:44:04.065 --> 07:44:06.764
speakers and topics.  Now let's 
take a look at a highlight 

07:44:09.241 --> 07:44:11.241
review of the day.

07:44:23.247 --> 07:44:25.247
[Music playing]

07:44:38.774 --> 07:44:43.036
&gt;&gt; Earlier this year, we hit the
milestone of 11 million 

07:44:43.037 --> 07:44:45.037
downloads.

07:44:47.952 --> 07:44:49.816
We are reallied really excited 
to see how much users are using 

07:44:49.817 --> 07:44:51.817
this and the impact in fact 
world.

07:44:54.782 --> 07:44:56.782
&gt;&gt; We're very excited today to 
announce 

07:44:58.086 --> 07:45:00.086
that we are joining the 
TensorFlow 

07:45:01.612 --> 07:45:06.193
family, deeplearn .js.
&gt;&gt; TensorFlow is an early stage 

07:45:06.194 --> 07:45:08.194
project, 

07:45:09.698 --> 07:45:11.698
we would like you to help build 
this future.

07:45:14.296 --> 07:45:17.414
&gt;&gt; I told you at the beginning, 
the mission for tf.data was to 

07:45:17.415 --> 07:45:19.524
make a library that was fast, 
flexible and easy to use.

07:45:21.165 --> 07:45:22.596
&gt;&gt; So, I'm very excited to say 
that we have been working with 

07:45:22.597 --> 07:45:27.097
other teams in Google to bring 
TensorFlow Lite to Google apps.

07:45:28.520 --> 07:45:30.552
&gt;&gt; In general, the Google brain 
team's 

07:45:34.435 --> 07:45:36.504
mission is to make machines 
intelligent and use that ability

07:45:36.505 --> 07:45:38.505
to improve people's lives.

07:45:40.033 --> 07:45:41.271
I think that's good examples of 
where there's real opportunity 

07:45:41.272 --> 07:45:46.218
for this.
[ Applause ]

07:45:50.184 --> 07:45:52.184
&gt;&gt; So, hold on just a minute.

