Speaker 1:          00:00          You are listening to radio lab radio from W. N y. S. A. N. N. P. R. All right. Hey, I'm Jad Abumrad. I'm Robert Krulwich. This is radio lab, the podcast. So here's the story. We've been falling for a while. Yes, it comes from a friend of mine, Andrew Zali who is a great thinker and writer. He wrote a book called resilience, why things bounce back. And he's a guy who thinks a lot about technology. I have been interested in a long time, uh, for a long time in the relationship between, uh, technology and emotion. Uh, and uh, because, um, well I've thrown more than one cell phone to the ground. Uh, Andrew and I were having breakfast one day and he pitched me on this idea of doing a story about Facebook. Remember iron, not a huge, no believer in doing stories about Facebook, but this story was wickedly interesting and profound in its way. So he and I had been following it for a couple of years, um, up and down through this roller coaster of events. It really begins in 2011 well, let me back up for a minute. One of the challenges talking about Facebook is just the scale of the thing.

Speaker 2:          01:15          So you know, there's, there's 1.3 billion people on Earth as of March, 2014. Those are active monthly users. There's a billion people who access the site through, uh, through mobile devices. [inaudible]

Speaker 1:          01:32          just to put that in perspective, there's more Facebook users than there are Catholics. That can't be true. Yeah, yeah, yeah. It turns out it is true, but they're neck and neck

Speaker 3:          01:42          [inaudible].

Speaker 1:          01:43          Anyhow, the overall point is that when you have one out of every seven people on the planet in the same space trying to connect across time and geography, you are bound to create problems. Sometimes Facebook making headlines again tonight, big issue this time prior. Before we go there, we should introduce you to the guy in our story. Who is the problem solver? My name is [inaudible] and I am the director of engineering at Facebook. Story Begins Christmas 2011.

Speaker 4:          02:13          People are doing what they do every holiday season. It's just they're getting back together with their families and they're going to family parties and they're taking lots and lots of pictures. Um, and they're all uploading them to Facebook. And at the time, the number of photos that were getting uploaded, what's going pretty crazy. In fact, in just those few days between Christmas and new years, there are more images uploaded to Facebook than the word, the entirety of flicker. Wait,

Speaker 1:          02:41          you're saying more images were uploaded in a week to Facebook then? All a flicker all time. Yeah. Oh. Which created a situation, the number of photos was going up and along with the number of photos going up, the number of reports was going up. What he means by reports is this, back in 2011, if you saw something on Facebook that really upset you, you could click a button to report it. You could tell Facebook to take it down, which from their perspective is a really important mechanism because if you're Facebook, you don't want certain kinds of content on your site.

Speaker 4:          03:18          You don't want nudity, you don't want like drug use, um, hate speech, things like that. So a day or so after Christmas, thereabouts, Facebook engineers come back to work and they find waiting for them. Literally millions of photo reports. Yes, the number of people that would be necessary to review everything that was coming in. Um, it kind of boggled my mind. How many people would you have needed? I think at the time we were looking at it, which was two years ago, and again, all of this has grown much since then. We're looking at like thousands, thousands,

Speaker 1:          03:48          like some giant facility in Nevada filled with nothing but humans looking at Christmas porn. We were actually joking about this, but we found out later, um, there actually are thousands of people across the world who do this for Internet companies all day long, which clearly warrants its own show. But for our purposes, just know that when our photo is reported, a human being has to look at it

Speaker 4:          04:11          exactly right because there needs to be a judgment on the image and humans are the best at that. So Arturo decided, before we do anything, let's just figure out what we're dealing with. And so we sat down with a team of people and we started going through the photos that people were reporting.

Speaker 1:          04:27          And what they found was that about 97% of these million or so photo reports were drastically

Speaker 4:          04:35          ms categorized. They were seeing moms holding little babies reported for harassment, pictures of families and matching Christmas sweaters reported for nudity. Pictures of puppies reported for hate speech. Puppies reported as hate speech. Yes. And we're like, what's going on? Right. Hm. So they decide, let's investigate. Okay, so step one for Facebook, just

Speaker 1:          04:59          ask a few of these people, why don't you like this photo? Why did you report this responses come back. And the first thing they realize is that almost always the person complaining about the image was in the image they were complaining about and they just hate the picture. Like maybe they were doing a goofy dance, someone snapped a photo and that's why did you post that? Take it down. Maybe they were at a party, they got a little too drunk. They hooked up with their ex. Somebody took a picture and that person says, Oh, you know, that's a one time thing and it's never happened. And again, take it down. Arturo said they were definitely a lot of reports from people who used to be couples and then they broke up

Speaker 4:          05:35          and then they're asking to take the photos down and the puppy. What would, what would be the reason for that? Oh, because it was maybe a shared puppy,

Speaker 1:          05:41          you know, maybe it's your ex wife's puppy, you see it makes you sad

Speaker 4:          05:46          get down. So once we've begun investigating, you find that there's all of this relationship, things that happen that are like really complicated. You're talking about stuff that's the kind of natural detritus of human dramas.

Speaker 1:          06:01          And the only reason that the person reporting it flagged it, it's like hate speech is because that was one of the only options. They were just picking because they needed to get to the next screen to submit the report.

Speaker 4:          06:12          So we edit a step, our turn, his team set it up so that when people were choosing that option, I want this photo to be removed from Facebook. Some of them would see a little box on the screen that said, how does the photo make you feel in the box gave several choices. The options were embarrass, embarrassing, upsetting, bad photo, and then we always put in an other where you could write in whatever you wanted about the image and it worked incredibly well. I mean like 50% of people would select an emotion like for instance, embarrassing and then 34% of people would select other and we read those. We sit down and we were reading the other and what was the most frequent thing that people were typing into other? It was, it's embarrassing. It's embarrassing, but you had embarrassing on the list. I know. That's weird. I know.

Speaker 1:          07:05          Yes, it's on our Touro is like, okay, maybe we should just put, it's in front of the choices as in, please describe this piece of content. It's embarrassing.

Speaker 4:          07:16          It's about four of me. It makes me sad, etc. And when they wrote out the choices that way with that extra word, we went from 50% of people selecting an emotion to 78% people selecting an emotion.

Speaker 1:          07:28          In other words, the word it's all by itself boosted the response by 28% from 50 to 78 and in Facebook land that means thousands and thousands of people. I'm trying to think of what could, what could that be? It's to people like full sentences or here's thinking. Um, it's always good to mirror the way people talk. Right? Arturo's idea though, which I find kind of interesting is that, um, when you just say embarrassing and there's no subject, it's silently implied that you are embarrassing. But if you say it's embarrassing, well then that shifts the sort of emotional energy to this photograph thing. And so then it's less hot and it's easier to deal with. Oh, how interesting. That thing is embarrassing. I'm fine. It's embarrassing. It is responsible, not me. Good for our Taro. That's like, it's interesting, right? It's very subtle, but it still doesn't solve their basic problem because even if Facebook now knows why the person flagged the photo that it was embarrassing and not actually hate speech, they still can't take it down. I mean there's nothing in the policy, the terms of service that says you can't put up embarrassing photos. And in fact if they took it down, they'd be violating the rights of the person who posted it.

Speaker 4:          08:41          Like there's nothing we can do. I'm sorry.

Speaker 1:          08:43          Oh, so they'd actually fence themselves in a little bit. Yeah. For me, I'd always put it in another, I would just be like, just go deal with it yourself. But I would say talk to the person know. Honestly that's the solution. I, he wouldn't put it that way, but I think what he needed to have happen was for the person who posted the picture and the person who was pissed about it, they'll talk to each other to work it out themselves. So Arturo and his team made a tweak where if you said this photo was embarrassing or whatever, a new screen would pop up and it would ask, do you want your friend to take the photo down? And if you said yes, I would like my stupid friend to take the photo down. We put up an empty message box, just an empty box that said, we think it's a good idea for you to tell the person who upset you that they upset you and only 20% of people would type something in and send that message.

Speaker 1:          09:36          They just didn't do it. They just said, I, I'd rather you deal with this. So our tour in his team were like, okay, let's take it one step further. When that message box popped up, we gave people a default message that we that'd be crafted to start the conversation. Just get the conversation going and it's kind of funny. The first question of the message that we did was like, hey, I didn't like this photo. Take it down. Hey, I don't like that photo. That's a little aggressive. It is. But when they started presenting people with a message box with that sentence pre-written in your, almost immediately we went from 20% of people sending a message to 50% of people sending a message. Really as it's been surprising to all of us, like we weren't expecting to see that big of a shift. Okay, so this means that people just don't want to, right.

Speaker 1:          10:19          They'll sign up for pretty much anything? No, not necessarily. No. Maybe it's just that it's so easy to shirk the responsibility of confronting another person that you need every little stupid nudge you can get to see, okay, that's how I see it. Okay, so they put out this pre-written message, it seems to really have an effect, so they're like, okay, if that works so well, why don't we try some different wording instead of, hey, I didn't like this photo. Take it down. Why don't we try? Hey Robert, I didn't like this photo. Take it down. Just putting in your name works about 7% better than leaving it out. Meaning what? It means that you're 7% more likely either to get the person to do what you asked them to do. Take down the photo or to start a conversation about how to resolve your feelings about, oh, we're now measuring the effectiveness of the message.

Speaker 1:          11:08          If I'm objecting, will the other party pull it off the computer, pull it off, or just talk to you about it? They also tried variations like, Hey Robert, would you please take it down throwing in the word please, or would you mind taking it down? And it turns out that would you please performance 4% better than what your mind, you're not totally sure why, but there they tried dozens of phrases like, would you please mind? Would you mind, I'm sorry to bring this up, but would you please take it down? I'm sorry to bring this up, but would you mind taking it down? And at a certain point, Andrew and I got recruited to see part true. We just want to see this whole process. They're going through up close. So we took a trip out to Facebook headquarters, Menlo Park, California, and this was about a year ago. No, I have done, just before the hubbub, we met up with Arturo who sort of walked us through the campus. [inaudible]

Speaker 1:          11:58          like the Hammock is kind of a little like hang out. It's one of these sort of like a socialist utopic silicon valley campuses where people are like in hammocks and there there's volleyball happening. We actually have our baby foxes here really? And they had foxes running around at one point. So we were there on a Friday because every Friday afternoon Arturo assembles this really big group welcome to the Union to review all the data and you've got about 15 people crammed into a conference room like technical folks, which Dev on software engineer. Interesting thing at Facebook, Dan feral. I'm a data scientist. I'm also an engineer and a lot of these guys called themselves trust engineers and every Friday the trust engineers are joined by a bunch of outside scientists. Dacher Keltner, professor psychology, UC Berkeley, Matt Killingsworth. I studied the causes and nature of human happiness. Liliana, Simon Thomas and my background is neuroscience.

Speaker 1:          12:55          This is the meeting where the team was reviewing all the data about these phrases and so everybody was looking at a giant graph projected on the wall, kind of supporting your, your slightly u shaped curve there in that, especially in the deletion numbers, the hey, I don't like this photo, take it down and the, hey I don't like this photo, would you please take it down? Our are kind of the winters here. [inaudible] it's kind of interesting that you see the person that's receiving a more direct message is higher 11% versus 4% one of the things they noticed is that anytime they used the word sorry and a phrase like, Hey Robert, sorry to bring this up, but would you please take it down? Turns out the m sorry doesn't actually help make the numbers go down. Really the seven and nine are the low. Some of the low points and those are the ones that say sorry. Okay. So like don't just don't apologize, just don't apologize cause it's like it shifts the responsibility back to you I guess. No it doesn't. It's just, it's just, it's like it's a, it's a linguistic psychology subtle thing. I'm making that up. I am kind of, yeah, but one of the things that really struck me at this meeting on different subject is that the scientists in the room as they were looking at the graph, taken in the numbers sufficient. A lot of them had this look on their face of like, oh, brain's holy.

Speaker 5:          14:06          Um, I'm just stunned and humbled at the numbers that we generally get in in these studies. It's Emilio, Simon Thomas from Berkeley. My background is in neuroscience and I'm used to studies where we look at 20 people and that's sufficient to say something general about how brains work.

Speaker 1:          14:21          Like in general at Facebook, like people would scoff at sample sizes that small. That's rob Boyle, who's a project manager at Facebook. The main and shoes that we're used to working with are in the hundreds of thousands to millions. This is kind of an interesting moment because there's been a lot of criticism recently, especially in social science, about the sample sizes, how they're too small and how there's, they're too often filled with white undergraduate college kids. And how can you generalize from that? So you could tell that some of the scientists in the room, like for example, Dacher, he's a psychologist at UC Berkeley and we're like, oh my God, look at what we can do now. We can get all these different people of different class backgrounds, different countries to him, this kind of work with Facebook. This could be the future of social science right here. There has never been a human community like this in human history.

Speaker 1:          15:12          It's somewhere in the middle of all the, uh, excitement about the data and the speed at which they can now test things. The bottleneck is no longer how fast we can test, how things work. It's coming up with the right things to test. Andrew threw out a question. What is the statistical likelihood that I have been a Guinea pig in one of your experiments? Uh, is I believe 100%, but [inaudible] any given person that Stan feral data scientist and when we look at the data, any given person is probably currently involved in what, 10 different experiments and they'd been exposed to to 10 different experimental things. Yup.

Speaker 6:          15:54          That kind of [inaudible]

Speaker 1:          15:55          blew me back a little bit. I was like, uh, may I have been a research subject and I had no idea coming up. Yeah. Everybody gets the idea and the lab rats revolt. Stay with us.

Speaker 7:          16:12          Hi there. This is feather [inaudible] calling from London, England. Radiolab is supported in part by the National Science Foundation and by the offered p Sloan Foundation enhancing public understanding of science and technology in the modern world. More information about sloane@wwwdotsloan.org oh, that's a bit of a tongue twister.

Speaker 8:          16:34          Hey guys, this is Alison calling from Luxembourg. Radiolab is supported by rocket mortgage by quicken loans. Rocket mortgage brings the mortgage process into the 21st century. With this completely online process, safely share your bank statements and pay stubs with the touch of a button. Ditch the paperwork and use your phone or tablet to get approved for purchase or refinance in minutes. Get accustomed mortgage solution whenever and wherever you want. Check out rocket mortgage today@quickenloansdotcomslashradiolabequalhousinglenderlicensedinallfiftystatesandmlsconsumeraccess.org number 30 30 hi,

Speaker 9:          17:11          I'm Robert Krulwich. Radiolab is supported by Google and Squarespace. Make your business official with Google and square space. When you create a custom domain and a beautiful business website with Squarespace, you'll receive a free year of business, email and professional tools from Google. It's the simplest way to look professional online. Visit squarespace.com/google to start your free trial. Use the offer code document for 10% off on your first purchase. Google and Squarespace. Make it professional. Make it beautiful.

Speaker 1:          17:45          This is Radiolab and uh, we'll pick up the story with Andrews. Ali and I sitting in a meeting at Facebook headquarters. This was about a year and a half ago. We had just learned that at any given moment, any given Facebook user is part of 10 experiments at once without really their knowledge and sitting there in that meeting, you know, this was a while ago, we both were like, did we just hear that correctly?

Speaker 10:         18:10          That kind of blew me back a little bit and I was like, I, I've been a research subject and I had no idea and I had that moment of discovery on a Friday and literally the next day, Saturday,

Speaker 11:         18:27          this is scary. The world had that experience. Facebook using you and me as lab rats for a Facebook experiment on emotion. Barely a day after we'd gotten off the plane from Facebook headquarters, the kerfuffle occurred, Facebook exposed for using us as lab rats, as lab rat lab rats probably say Facebook messing with your emotion. You may remember the story cause for a hot second. It was everywhere. Facebook altered the amount of you said that it was all over. Facebook story was an academic paper had come out that showed that with some scientists, the company had intentionally manipulated user newsfeeds to study a person's emotional response. Seriously. They wanted to see how emotion is spread on social media. They basically tinkered with the news feeds about 700,000 people, 700,000 users to test how they react if they saw more positive versus negative posts and vice versa, and they found an effect that when people saw more positive stuff in their news feeds, they would post more positive things themselves and vice versa was a tiny effect, tiny effect, but the results weren't really the story.

Speaker 11:         19:32          The real story was that Facebook was messing with us. Yeah, you pause and scares me when you think that they were just doing an experiment to manipulate how people were feeling and how they then reacted on Facebook if people went apoplectic. [inaudible] has this big brother element to it that I think people are going to be very uncomfortable with and some people went so far as to argue, I wonder if Facebook killed anyone with their emotional manipulation. Stunned as if a person had a psychological psychiatric disorder manipulating their social world could cause them real harm. Make sure you read those terms and conditions. My friends asked what

Speaker 1:          20:12          what you hear is a sense of betrayal that I really wasn't aware that this space of mine was being treated in these ways and that I was part of your psychological experimentation. That's Kate Crawford. I'm a principal researcher at Microsoft research visiting professor at MIT, strong critic of Facebook. Throughout the Kerfuffle, there is a power imbalance at work. I think when we look at the way that that experiment was done, it's an example of highly centralized power and highly opaque power at work. And I don't want to see us in a situation where we just have to blindly trust that platforms are looking out for us here. I'm thinking of one of an earlier Facebook study back in 2010 where they did a study looking at whether they could increase voter turnout. They had this quite simple design. They, they came up with, you know, a little uh, box that would pop up and show you where your nearest voting booth was.

Speaker 1:          21:01          And then they said, oh, well in addition to that, when you voted, here's a button you can press that says I voted. And then you'll also see the pictures of six of your friends who'd also voted that day. What this change. The number of people who went out to vote the day, and Facebook found that it did, that if you saw a bunch of pictures of your friends who had voted and you felt those pictures on election day, you were then 2% more likely to click the I voted button yourself. Presumably because you two had gone out and voted. Now 2% might not sound like a lot, but it was not insignificant. Again, I think by the order of 340,000 votes, whether the votes that they estimate, they actually, uh, shifted by getting people to go on that. These are people who wouldn't have, it, wouldn't have voted and who they have said in their own paper and published paper that they increased the number of votes that day by 340,000 simply by saying that your neighbors did it too.

Speaker 1:          21:51          Yeah. By your friends. No. My first reaction to this, I must admit, was okay. I mean, we're at historic lows when it comes to voter turnout. This sounds like a good thing. Yes, but what happens if someone's running a platform that a lot of people are on and they say, Hey, you know, I'm, I'm really interested in this candidate. This candidate is going to look out not just for my interest, but the interest of the technology sector and I think they're, you know that they're a great candidate. Why don't we just show that, get out to vote message and that that little system design that we have to the people who clearly because we already have their political preferences, the ones who kind of agree with us and the people who disagree with that candidate, they won't get those little nudges. Now that is a profound democratic palette that you have.

Speaker 1:          22:37          Kate's basic position is that when it comes to social engineering, which is what this is companies and the people that use them, you need to be really, really careful. In fact, when, when Andrew mentioned to her that Arturo had this group and the group had a name, he actually runs a group called the trust engineering group. His job is to engineer trust. When Andrew told her that Facebook users, that's easier. You're smacking your forehead. I think we call that a face palm. She face palm really hard. These ideas that we could somehow engineer compassion, I think to some degree have a kind of Hubris in them.

Speaker 12:         23:15          [inaudible]

Speaker 1:          23:16          who are we to decide whether we can make somebody more compassionate or not.

Speaker 12:         23:23          [inaudible]

Speaker 1:          23:26          how do you want to set this up? Let's see. How do we do this? A couple months after our first interview, we spoke to our Touro Bay Har. Again, at this point the kerfuffle was dying down. We asked them about all the uproar. I know this is not your work, this is the emotional contagion stuff. But literally like hours after we got from that meeting that thing erupted. Do you understand the backlash? No, I mean I think that, I mean we, we,

Speaker 4:          23:56          we really care about the people who use Facebook. I don't think that there's such a thing as a as uh, I mean if anything I've learned in this work is that you really have to respect people's response and emotions no matter what they are.

Speaker 1:          24:16          He says the whole thing. Definitely

Speaker 4:          24:19          made them take stock. Um, there was a moment of concern of what it would mean to the work and there there was like this is this gonna is this gonna mean that um, that we can't do this? Hmm. Part of me like being on this committee here is I actually wanna reclaim back the word emotion, um, and reclaimed back the ability to do um, very thoughtful and careful experiments and when it come back the word experiment. Okay.

Speaker 1:          24:45          Where do you want to reclaim it from? From what? Um, well suddenly like the word emotion and the word experiment only things became really charged. Well, yeah, because people thought that Facebook was manipulating emotion and they were like, yes. [inaudible] case in our case,

Speaker 4:          25:00          right then in, in, in, in the work that we're talking about right now, all of the work that we do begins with a person asking us for help.

Speaker 1:          25:08          This was Arturo's most emphatic point. He said it over and over that, you know, Facebook isn't just doing this for fun. People are asking for help. They need help, which points to one of the biggest challenges of living online, which is that, you know, offline, you know, when we try and engineer trust offline, or at least just read one another, we do it in these super subtle ways using eye contact, facial expressions, and posture and tone of voice, all this nonverbal stuff. And of course when we go online, we don't have access to any of that. In the absence of that feedback, how do we communicate what, what does communication turn into

Speaker 4:          25:49          me too? This isn't just because of the other stupid kid's best riff on this, I got to say is a Louis C K on Conan for kids. It's just this thing. It's bad. And he did this great bit about technology and kids, you know, kids are mean and it's cause they're trying it out. They look at a kid, they go, you're fat. And then they see the kid's face scrunched up and they go, Ooh, that doesn't feel good to make a person do that. But they got to start with doing the mean thing. But when they write your fat, then they just go, hmm. That was fun. I like that. Anyhow, back to Arturo. I mean I think about like what it means to be in the presence of a friend or a loved one.

Speaker 1:          26:32          Um, and, and how will you build experiences that facilitate that when you cannot be physically together? Arturo says that's really all up to, he's just trying to nudge people a tiny bit so that their online cells are a little bit closer to how they are offline. And I got to say if he can do that by engineering, a couple of phrases like, Hey Robert, would you mind? Et Cetera, et cetera. Well then I'm all for it.

Speaker 9:          26:59          Why not take the position that to create a company that stands between two people who who are interacting and then giving them boxes and statuses and, and, and little, um, and advertising and stuff like this is not doing a service, this is just, this is, this is a way to wedge yourself into the ordinary business of social intercourse and make money on it. And you're acting like this group of people now is going to try to, to create the, the moral equivalent of an actual conversation. First of all, it's probably not engineerable and of all, I don't believe that for a moment. All I'm thinking is they're going to just go and figure out other ways in which to make a revenue enhancer.

Speaker 1:          27:46          No, I don't think it's one or the other. I think they're in it for the money. In fact, if they can figure this out and make the Internet universe, uh, more inducive to trust and less annoying, it could mean trillions of dollars. So yeah, it's the money. But still, that doesn't negate the fact that we have to build these systems, right, that we have to make the Internet a little bit better.

Speaker 9:          28:07          Does, that's fine. This idea, however, that, um, you're going to have to coach people into the subtleties of the relationship. Tell him you're sorry. Tone. Just, you know, here's the formula for this. Here's you. He doesn't want, he did something. You, you need to repair that. Here are the seven ways you might repair that to do all that. It's, it's as if the hallmark card company instead of living only on mother's Day, Father's day and birthdays just spread its evil wings out into the whole rest of your life.

Speaker 1:          28:44          I don't know. I think that's a wonderful thing. I think, you know, I have a slightly different opinion of it. I mean, you've got to keep in mind how this thing came about. I mean, they tried to get people to talk to each other. They gave them the blank text box, but nobody used it. Right. So they're like, okay, let's come up with some stock phrases that yes, are generic, but think about the next step after you send the message saying, you know, Jad p I don't like the photo. Police take it down. Presumably then you and I get into a conversation. Maybe I explained myself. I say, Oh my God, I'm so sorry. I didn't realize that you didn't like that photo. I just thought that that was an amazing night. I just thought that was a great night. I didn't realize you thought you look so sorry. I'll take it down. It's cool. See now presumably we're having that conversation as a next step.

Speaker 9:          29:27          I presume that how many of the birthday cards that you've sent to first cousins have resulted in a conversation, maybe [inaudible] these things are actually not, they're really the opposite of what you're saying. Are Conversation substitutes.

Speaker 1:          29:40          Maybe, maybe

Speaker 4:          29:42          they're conversation starters. Maybe that's the deep experiment. Are they conversation starters or were substitutes we'll I hope they're conversation starters. Yeah, cause maybe that would be a beginning. It kind of in my mind goes back to like the beginning of the automobile age [inaudible] Andrew puts it, there was a time when automobiles were new and you know, they didn't have turn signals. The tools they did have, like the horn didn't necessarily indicate all the things that we use. It indicated it wasn't clear what the Horn was actually there to do. Is it there to say hello or is it there to say get out of the way. And over time we created norms. We created roads with lanes. We created turn signals that are primarily there for other people so that we can coexist in this great flow without crashing into each other. And we still have road rage and we still have road rage. We still have places where those tools are incomplete.