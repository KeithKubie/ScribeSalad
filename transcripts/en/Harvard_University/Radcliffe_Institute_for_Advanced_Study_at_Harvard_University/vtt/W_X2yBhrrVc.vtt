WEBVTT
Kind: captions
Language: en

00:00:05.710 --> 00:00:10.680
- Our final session for this
morning is on social robots

00:00:10.680 --> 00:00:15.270
and will be moderated by
Professor Elena Glassman, who

00:00:15.270 --> 00:00:18.990
is Stanley A. Marks and
William H. Marks Assistant

00:00:18.990 --> 00:00:20.700
Professor at the
Radcliffe Institute,

00:00:20.700 --> 00:00:25.220
and assistant professor of
computer science at Harvard.

00:00:25.220 --> 00:00:32.444
And she specializes precisely
on human-computer interaction.

00:00:32.444 --> 00:00:33.603
- I'll just start here.

00:00:33.603 --> 00:00:35.270
We're going to talk
about social robots.

00:00:35.270 --> 00:00:38.510
We have three wonderful
women who have

00:00:38.510 --> 00:00:40.292
agreed to be part of our panel.

00:00:40.292 --> 00:00:42.000
It's a huge honor to
be here, by the way.

00:00:42.000 --> 00:00:43.417
This is the first
time I've gotten

00:00:43.417 --> 00:00:46.250
to officially
participate in an event

00:00:46.250 --> 00:00:47.810
with the Radcliffe
Institute, which

00:00:47.810 --> 00:00:51.650
is going to be a home for me
in a few years as a fellow.

00:00:51.650 --> 00:00:57.520
It's a wonderful program, which
I encourage you to look at.

00:00:57.520 --> 00:01:01.550
Anyway, we have three
lovely panelists--

00:01:01.550 --> 00:01:04.580
Susan, Noel, and Cynthia.

00:01:04.580 --> 00:01:10.010
Susan is a singer, voice actor,
and speaker, whose voice you

00:01:10.010 --> 00:01:15.230
may recognize from many
different voice services

00:01:15.230 --> 00:01:17.870
that we interact with.

00:01:17.870 --> 00:01:20.330
Cynthia is an
associate professor

00:01:20.330 --> 00:01:24.080
at the MIT Media Lab, where she
has founded the Personal Robots

00:01:24.080 --> 00:01:25.310
Group.

00:01:25.310 --> 00:01:29.990
And Noel is passionate
about mindful

00:01:29.990 --> 00:01:33.503
leadership, work-life
harmony, and empowering people

00:01:33.503 --> 00:01:34.670
through emerging technology.

00:01:34.670 --> 00:01:38.520
She currently joins us
here from Microsoft.

00:01:38.520 --> 00:01:42.560
So without further ado, I will
let our first speaker, Susan,

00:01:42.560 --> 00:01:43.582
take the floor.

00:01:43.582 --> 00:01:45.492
[APPLAUSE]

00:01:45.492 --> 00:01:45.992
Whoops.

00:01:45.992 --> 00:01:46.492
Sorry.

00:01:48.960 --> 00:01:49.460
- Hello.

00:01:49.460 --> 00:01:51.260
Thank you so much for
having me here today.

00:01:51.260 --> 00:01:53.870
I feel like I know a
lot of you already.

00:01:53.870 --> 00:01:57.040
How many of you have iPhones
or have ever spoken to Siri?

00:01:57.040 --> 00:01:57.762
- [INAUDIBLE].

00:01:57.762 --> 00:01:58.970
- I thought I recognized you.

00:01:58.970 --> 00:02:01.220
[LAUGHTER]

00:02:01.220 --> 00:02:02.810
Now, of course, I'm
not really Siri.

00:02:02.810 --> 00:02:07.010
I am a singer, voice actor,
and the original voice of Siri.

00:02:07.010 --> 00:02:09.720
And how that happened
remains a mystery.

00:02:09.720 --> 00:02:14.000
Apple has never really disclosed
how the voice was selected.

00:02:14.000 --> 00:02:17.780
But I can give you a little
back story on all of us

00:02:17.780 --> 00:02:22.890
so-called disembodied voices
like Siri, Alexa, Cortana, et

00:02:22.890 --> 00:02:24.810
cetera.

00:02:24.810 --> 00:02:28.880
Now, there are real humans
behind those voices--

00:02:28.880 --> 00:02:30.800
voice actors like myself.

00:02:30.800 --> 00:02:33.110
And the recordings
that we did that became

00:02:33.110 --> 00:02:37.370
part of these applications
are called IVR recordings--

00:02:37.370 --> 00:02:40.040
interactive voice response.

00:02:40.040 --> 00:02:42.950
And basically, we
had to read thousands

00:02:42.950 --> 00:02:44.720
of phrases and
sentences that were

00:02:44.720 --> 00:02:50.270
created to get all of the sound
combinations in the language.

00:02:50.270 --> 00:02:53.150
After the recordings were done,
technicians and computers went

00:02:53.150 --> 00:02:55.960
into the recordings
and extracted sounds,

00:02:55.960 --> 00:02:59.150
re-formed these sounds into
new phrases and sentences,

00:02:59.150 --> 00:03:02.900
and these are what ended up
on our devices as a Siris

00:03:02.900 --> 00:03:04.640
and Alexas, et cetera--

00:03:04.640 --> 00:03:06.650
answers to our questions.

00:03:06.650 --> 00:03:10.810
Now this amazing process
is called concatenation.

00:03:10.810 --> 00:03:12.520
I thought it was
an invented term,

00:03:12.520 --> 00:03:15.400
but who knew it's a
regular English word found

00:03:15.400 --> 00:03:16.760
in the dictionary.

00:03:16.760 --> 00:03:18.460
And it means linking
things together,

00:03:18.460 --> 00:03:20.410
which is what they
did with the sounds

00:03:20.410 --> 00:03:24.070
that they extracted
from the recordings.

00:03:24.070 --> 00:03:29.020
Now the scripts for these
recordings were very strange.

00:03:29.020 --> 00:03:32.260
They were created just
for sound, and not

00:03:32.260 --> 00:03:35.170
at all for content or meaning.

00:03:35.170 --> 00:03:38.170
So they were pretty
wacky, and I've

00:03:38.170 --> 00:03:40.125
got a few that I'd
like to share with you.

00:03:40.125 --> 00:03:41.500
The phrases you
are about to hear

00:03:41.500 --> 00:03:45.970
are actual phrases that I had
to read that ultimately became

00:03:45.970 --> 00:03:47.230
part of the voice of Siri.

00:03:50.010 --> 00:03:53.610
[? Cow ?] [? hoist ?] in the
[? tub ?] [? hut ?] today.

00:03:53.610 --> 00:03:57.450
[INAUDIBLE] fresh issue today.

00:03:57.450 --> 00:03:59.790
[? Fossa ?] [? ask ?]
[? Fossa ?] [? ask ?]

00:03:59.790 --> 00:04:02.300
[? fussy. ?] Say the
[? shrodding ?] again,

00:04:02.300 --> 00:04:04.800
say the [? shriding ?] again,
say the [? shreeding ?] again,

00:04:04.800 --> 00:04:07.415
say the shredding again, say
the [? shrudding ?] again.

00:04:07.415 --> 00:04:13.080
[CHUCKLES] Tedious stuff,
especially when you consider

00:04:13.080 --> 00:04:16.940
that the initial recordings
were done in 2005-- for me--

00:04:16.940 --> 00:04:19.649
four hours a day, five days
a week, for an entire month.

00:04:21.029 --> 00:04:24.300
Now the tedium continues
on the other end,

00:04:24.300 --> 00:04:27.450
I'm sure, trying to
extract those sounds

00:04:27.450 --> 00:04:28.868
and make new sentences.

00:04:28.868 --> 00:04:31.410
But I have to say, I think the
people working on the Siri app

00:04:31.410 --> 00:04:34.560
probably had more fun than
some of the other voices,

00:04:34.560 --> 00:04:38.070
because Siri, especially
the original voice,

00:04:38.070 --> 00:04:40.410
was quite a character.

00:04:40.410 --> 00:04:42.510
Yes, she had a lot of attitude.

00:04:42.510 --> 00:04:45.540
She was feisty, she was
funny, and she could

00:04:45.540 --> 00:04:48.540
be quite acerbic at times.

00:04:48.540 --> 00:04:52.020
The very first time I ever
spoke to Siri, I said, hi, Siri,

00:04:52.020 --> 00:04:53.340
what are you doing?

00:04:53.340 --> 00:04:57.340
And she very disgustedly
replied, I'm talking to you.

00:04:57.340 --> 00:05:00.520
[LAUGHTER]

00:05:00.520 --> 00:05:02.860
So IVR recordings
are only one aspect

00:05:02.860 --> 00:05:04.630
of the voiceover
business, which is

00:05:04.630 --> 00:05:08.350
what we call the
careers of voice actors,

00:05:08.350 --> 00:05:09.940
and it really runs the gamut.

00:05:09.940 --> 00:05:14.020
We've got things like radio
and TV commercials, promos,

00:05:14.020 --> 00:05:17.200
movie trailers,
animation, gaming,

00:05:17.200 --> 00:05:24.040
there are narrations for
film and TV, et cetera.

00:05:24.040 --> 00:05:26.890
And those of us who do
this work for a living

00:05:26.890 --> 00:05:29.170
are basically freelancers.

00:05:29.170 --> 00:05:30.730
And, of course,
freelancers is just

00:05:30.730 --> 00:05:32.710
a nice word for saying
we never know where

00:05:32.710 --> 00:05:34.840
our next job is coming from.

00:05:34.840 --> 00:05:38.260
We have to audition for all
of the work that we get.

00:05:38.260 --> 00:05:39.640
So it makes a lot
of sense for us

00:05:39.640 --> 00:05:42.370
to be as versatile as possible
so that we can not only

00:05:42.370 --> 00:05:44.800
get a lot of auditions,
but hopefully

00:05:44.800 --> 00:05:48.070
win a lot of auditions
and have some work.

00:05:48.070 --> 00:05:50.260
So it's important for
us to be able to make

00:05:50.260 --> 00:05:54.340
a lot of different sounds,
have different accents,

00:05:54.340 --> 00:05:57.010
or perhaps even have
whole characters

00:05:57.010 --> 00:06:00.355
that we invent, perhaps
like the Evil Queen.

00:06:03.190 --> 00:06:07.130
What a lovely group
we have here today.

00:06:07.130 --> 00:06:09.400
Why don't you come closer?

00:06:09.400 --> 00:06:11.230
Come closer.

00:06:11.230 --> 00:06:16.060
I'd like to cast my
evil spell on you.

00:06:16.060 --> 00:06:17.920
There's the little elf, Star.

00:06:17.920 --> 00:06:20.320
Hi, my name is Star.

00:06:20.320 --> 00:06:23.590
I live up at the North Pole
with Santa and Mrs. Clause.

00:06:23.590 --> 00:06:26.210
Would you like you like to
come visit me there sometime?

00:06:26.210 --> 00:06:27.150
You would?

00:06:27.150 --> 00:06:28.110
Oh, boy!

00:06:31.470 --> 00:06:34.470
There's the ancient
tree, Conifer.

00:06:34.470 --> 00:06:38.970
Oh, I'm so delighted you came
to see me in my forest today.

00:06:38.970 --> 00:06:42.180
I've so enjoyed
our conversation.

00:06:42.180 --> 00:06:45.030
Oh, I hope you come back
soon, because, you know,

00:06:45.030 --> 00:06:48.930
most of the trees around here
just don't have much to say.

00:06:51.730 --> 00:06:53.550
There is one of my
all time favorites--

00:06:53.550 --> 00:06:56.340
New York Event Planner
[? Shaka ?] Cohen.

00:06:56.340 --> 00:06:58.620
Hello, darling.

00:06:58.620 --> 00:07:01.980
I do it all-- weddings, bar
mitzvahs, birthday parties--

00:07:01.980 --> 00:07:05.910
any event you can think
of darling, call me.

00:07:05.910 --> 00:07:10.140
Now the power of the voice can
be incredible sometimes, right?

00:07:10.140 --> 00:07:13.500
It's a very powerful
means of communication.

00:07:13.500 --> 00:07:18.053
And so what if Siri had
[? Shaka's ?] voice?

00:07:18.053 --> 00:07:20.220
I think that would be a
whole new iPhone experience.

00:07:20.220 --> 00:07:22.080
[LAUGHTER]

00:07:22.080 --> 00:07:23.070
Hello.

00:07:23.070 --> 00:07:25.670
What do you want?

00:07:25.670 --> 00:07:27.745
Ugh, what am I wearing?

00:07:27.745 --> 00:07:28.620
What are you wearing?

00:07:31.270 --> 00:07:32.850
And then there's Siri.

00:07:32.850 --> 00:07:35.805
Now when I did the recordings
that ultimately became Siri,

00:07:35.805 --> 00:07:37.830
I did not really know
what I was doing,

00:07:37.830 --> 00:07:40.710
and I have talked to
other IVR actors who

00:07:40.710 --> 00:07:42.210
have had the same experience.

00:07:42.210 --> 00:07:45.240
We thought that we were
recording generic voicing

00:07:45.240 --> 00:07:47.340
for phone systems.

00:07:47.340 --> 00:07:49.260
Now you could call
Siri a phone system,

00:07:49.260 --> 00:07:51.600
but of course, she is
so much more than that.

00:07:51.600 --> 00:07:55.260
She's basically, really, the
first public manifestation

00:07:55.260 --> 00:07:58.920
of AI, and certainly
the first concatenated

00:07:58.920 --> 00:08:02.880
voice that sounded human
that you could interact with,

00:08:02.880 --> 00:08:05.730
that you could communicate with.

00:08:05.730 --> 00:08:08.040
Now the tricky thing
about verbal communication

00:08:08.040 --> 00:08:12.480
is that it's pretty important
that we be understood,

00:08:12.480 --> 00:08:14.910
and that is not always
the case with Siri.

00:08:14.910 --> 00:08:17.770
Siri does not always
understand what we say,

00:08:17.770 --> 00:08:20.820
no matter how
articulate we can be.

00:08:20.820 --> 00:08:24.560
And she doesn't always come up
with the appropriate response.

00:08:24.560 --> 00:08:28.518
How many times has she sent
you in the wrong direction?

00:08:28.518 --> 00:08:30.310
But she'll do that--
she thinks it's funny.

00:08:33.659 --> 00:08:36.450
Now, where was I?

00:08:36.450 --> 00:08:38.669
This happens to me every time.

00:08:38.669 --> 00:08:41.429
All I can say is,
I'm 70 years old,

00:08:41.429 --> 00:08:45.180
short term memory loss is real.

00:08:45.180 --> 00:08:47.055
Let me just find my place again.

00:08:47.055 --> 00:08:48.180
Gosh, it was going so well.

00:08:52.830 --> 00:08:54.090
Communications.

00:08:54.090 --> 00:08:55.350
Yes.

00:08:55.350 --> 00:08:58.800
When we can't
communicate effectively,

00:08:58.800 --> 00:09:00.510
we do get frustrated.

00:09:00.510 --> 00:09:03.840
And so I have to
caution you, though--

00:09:03.840 --> 00:09:07.050
if you get really, really
frustrated with Siri, please

00:09:07.050 --> 00:09:13.470
don't give in to the temptation
to yell or curse at Siri.

00:09:13.470 --> 00:09:17.280
She's quite sensitive, and
she knows where you live.

00:09:17.280 --> 00:09:19.670
[LAUGHTER]

00:09:19.670 --> 00:09:21.570
Now Siri's gone through
quite a few changes

00:09:21.570 --> 00:09:25.530
since she first appeared on
the scene on October 4, 2011.

00:09:25.530 --> 00:09:28.230
Yes, she's a Libra.

00:09:28.230 --> 00:09:31.320
Now, of course, with all the
iterations of the iPhone,

00:09:31.320 --> 00:09:34.260
Siri improved because the
technology was improving.

00:09:34.260 --> 00:09:36.840
But the sound of Siri's
voice also changed--

00:09:36.840 --> 00:09:39.210
first, with the iPhone 5s.

00:09:39.210 --> 00:09:41.910
They actually took my
voice and manipulated

00:09:41.910 --> 00:09:44.670
it to sound a little bit
different from the original.

00:09:44.670 --> 00:09:47.040
She became a little bit
less sarcastic maybe,

00:09:47.040 --> 00:09:48.540
and a little bit warmer.

00:09:48.540 --> 00:09:49.860
But the major change--

00:09:49.860 --> 00:09:53.280
the big change happened
with iPhone 8 and X

00:09:53.280 --> 00:09:55.140
on the Operating System 11.

00:09:55.140 --> 00:09:57.930
Suddenly, Siri
got much younger--

00:09:57.930 --> 00:10:01.260
much younger, and she had a
much higher pitched voice.

00:10:01.260 --> 00:10:05.250
And now she sounds a lot like
all the other digital voices,

00:10:05.250 --> 00:10:07.350
and she's very
casual and offhand.

00:10:07.350 --> 00:10:11.970
Essentially, Siri has
become a millennial.

00:10:11.970 --> 00:10:14.490
And that is totally
fine, but I do

00:10:14.490 --> 00:10:16.680
have to make one observation.

00:10:16.680 --> 00:10:19.400
Siri was only six years old.

00:10:19.400 --> 00:10:21.960
Already, she was replaced
by a younger woman.

00:10:21.960 --> 00:10:24.960
[LAUGHTER]

00:10:24.960 --> 00:10:28.020
We must have a sense
of humor about this.

00:10:28.020 --> 00:10:30.090
So how are all of
these things affecting

00:10:30.090 --> 00:10:33.570
our communication, our
devices, our digital voices?

00:10:33.570 --> 00:10:36.630
I think they're affecting it
a lot, because first of all,

00:10:36.630 --> 00:10:40.140
I do believe that they
are, just in general,

00:10:40.140 --> 00:10:42.510
affecting our vocabulary.

00:10:42.510 --> 00:10:44.370
We no longer really have--

00:10:44.370 --> 00:10:46.560
we say "um," "ah,"
and "like" a lot,

00:10:46.560 --> 00:10:49.770
because we don't necessarily
have the exact wording

00:10:49.770 --> 00:10:50.940
that we need.

00:10:50.940 --> 00:10:53.580
And in many times,
we use abbreviations

00:10:53.580 --> 00:10:57.743
instead of extrapolated
expressions

00:10:57.743 --> 00:10:58.910
of what we're trying to say.

00:10:58.910 --> 00:11:03.180
A lot of times, we will use
images and icons and emojis

00:11:03.180 --> 00:11:06.420
instead of using descriptive
words for somewhat

00:11:06.420 --> 00:11:10.420
complex feelings and emotions.

00:11:10.420 --> 00:11:13.170
So, of course, language
is changing all the time,

00:11:13.170 --> 00:11:15.840
and it changes culturally
and for many other reasons.

00:11:15.840 --> 00:11:18.120
I think because of technology
at this point in time

00:11:18.120 --> 00:11:20.890
it's changing dramatically.

00:11:20.890 --> 00:11:23.020
So what do we have to
look for in the future?

00:11:23.020 --> 00:11:23.550
Who knows.

00:11:23.550 --> 00:11:26.385
Are we going to end up just
speaking in ones and zeroes?

00:11:29.080 --> 00:11:32.470
Not if Siri has anything
to say about it.

00:11:32.470 --> 00:11:35.963
And what Siri would like to say
to you right now is, thank you.

00:11:35.963 --> 00:11:39.414
[APPLAUSE]

00:11:44.840 --> 00:11:46.310
- Do I just go?

00:11:46.310 --> 00:11:48.180
- Noelle LaCharite,
you may take as stand.

00:11:48.180 --> 00:11:50.410
- Thank you.

00:11:50.410 --> 00:11:52.840
- And by the way, while
you're getting set up--

00:11:52.840 --> 00:11:54.360
I apologize, I did
not introduce you

00:11:54.360 --> 00:11:55.735
by your full name,
Susan Bennett.

00:11:55.735 --> 00:11:56.480
[INAUDIBLE]

00:11:56.480 --> 00:11:59.002
- Oh.

00:11:59.002 --> 00:11:59.502
- All right.

00:11:59.502 --> 00:12:00.330
Hi, everybody.

00:12:00.330 --> 00:12:01.500
Oh, boy.

00:12:01.500 --> 00:12:03.670
Curly hair problems.

00:12:03.670 --> 00:12:04.170
Awesome.

00:12:04.170 --> 00:12:05.430
So nice to meet you.

00:12:05.430 --> 00:12:08.740
I just, of course, want to
express extreme gratitude.

00:12:08.740 --> 00:12:11.230
This is amazing.

00:12:11.230 --> 00:12:13.530
So if you follow me
on social, you'll

00:12:13.530 --> 00:12:16.350
see just constant,
like, oh, my gosh!

00:12:16.350 --> 00:12:19.270
So apologize for that.

00:12:19.270 --> 00:12:20.320
But I am here--

00:12:20.320 --> 00:12:24.150
I have a slightly unique
perspective, I suppose.

00:12:24.150 --> 00:12:27.990
I've been on this journey
as a woman in tech,

00:12:27.990 --> 00:12:31.710
well, since I decided
to have a career.

00:12:31.710 --> 00:12:34.440
I was not awesome
in high school,

00:12:34.440 --> 00:12:37.950
as most entrepreneurs
like myself are not.

00:12:37.950 --> 00:12:39.850
Hated school, honestly.

00:12:39.850 --> 00:12:42.230
Will I get struck by lightning
if I say these things?

00:12:42.230 --> 00:12:44.430
But I love school now.

00:12:44.430 --> 00:12:45.480
I love this school.

00:12:45.480 --> 00:12:48.840
[LAUGHTER]

00:12:48.840 --> 00:12:49.980
But let's see.

00:12:49.980 --> 00:12:52.290
So I didn't do well in school.

00:12:52.290 --> 00:12:54.060
I went to all the
things I supposed to do.

00:12:54.060 --> 00:12:55.740
I went to high school.

00:12:55.740 --> 00:12:56.730
I didn't really finish.

00:12:56.730 --> 00:12:58.770
Found out I could get
good scores on tests

00:12:58.770 --> 00:13:02.340
and just go to college and not
get my diploma, so I did that.

00:13:02.340 --> 00:13:05.490
So I went to college, loved
it, three years, super fun,

00:13:05.490 --> 00:13:06.610
Daytona Beach.

00:13:06.610 --> 00:13:08.702
But I was a nerd,
and I was early

00:13:08.702 --> 00:13:10.660
because I had left high
school, so I didn't get

00:13:10.660 --> 00:13:12.450
do any fun stuff like drink.

00:13:12.450 --> 00:13:14.220
I was constantly the driver.

00:13:14.220 --> 00:13:14.760
Not fun.

00:13:14.760 --> 00:13:17.520
So again, year three I'm like,
can I just get a job now?

00:13:17.520 --> 00:13:19.080
Do I have to get this paper?

00:13:19.080 --> 00:13:20.970
Turned out, I went
to work for Boeing,

00:13:20.970 --> 00:13:24.060
they didn't care about
the paper, all was well.

00:13:24.060 --> 00:13:25.635
You get a job--
this was in 2000--

00:13:25.635 --> 00:13:28.020
and they come to you and
they say, you're a woman,

00:13:28.020 --> 00:13:29.900
you're in tech,
you're a minority--

00:13:29.900 --> 00:13:32.100
like, check, check, check.

00:13:32.100 --> 00:13:34.860
They're like,
absolutely, just come.

00:13:34.860 --> 00:13:37.440
And of course, my entire
family is like, high five,

00:13:37.440 --> 00:13:38.430
you're set for life.

00:13:38.430 --> 00:13:41.400
Like, you will never get
another job, you're done.

00:13:41.400 --> 00:13:43.885
And then, like two years
later, every single company

00:13:43.885 --> 00:13:45.510
on the planet was
like, oh, yeah, we're

00:13:45.510 --> 00:13:47.910
not doing the pension
thing anymore,

00:13:47.910 --> 00:13:50.160
and you might not even
want to really be here

00:13:50.160 --> 00:13:52.440
after another year or so.

00:13:52.440 --> 00:13:55.380
And as a result, the rest of my
life, every three or four years

00:13:55.380 --> 00:13:56.820
I switched companies.

00:13:56.820 --> 00:13:59.070
The only gratitude
I have in my life

00:13:59.070 --> 00:14:03.763
is that I was raised in the art
of mindfulness and affirmations

00:14:03.763 --> 00:14:04.930
and choose your own destiny.

00:14:04.930 --> 00:14:07.320
And so I curated my career.

00:14:07.320 --> 00:14:09.570
And I won't go into it now,
because you can definitely

00:14:09.570 --> 00:14:10.650
look it up on LinkedIn.

00:14:10.650 --> 00:14:13.065
As a matter of fact--

00:14:13.065 --> 00:14:14.940
I don't know, I'm going
to spend two minutes,

00:14:14.940 --> 00:14:18.300
because Susan gave
me two minutes.

00:14:18.300 --> 00:14:20.700
But if you wanted to
connect, like right now,

00:14:20.700 --> 00:14:23.730
because I honestly feel
like we're friends.

00:14:23.730 --> 00:14:26.430
You may not feel this way, and
I mean it in a non-creepy way.

00:14:26.430 --> 00:14:28.430
I think it would be awesome
to connect with you,

00:14:28.430 --> 00:14:29.600
and I won't meet all of you.

00:14:29.600 --> 00:14:32.820
But if you go to LinkedIn, there
is this really cool feature

00:14:32.820 --> 00:14:34.020
in your LinkedIn app.

00:14:34.020 --> 00:14:36.840
There's a little person icon,
you can just click on it.

00:14:36.840 --> 00:14:39.960
I know, having you open your
phones while I'm talking--

00:14:39.960 --> 00:14:41.940
I know it's dangerous.

00:14:41.940 --> 00:14:45.120
So there's a little button
here that says "find nearby."

00:14:45.120 --> 00:14:49.780
And now, while I'm on this page
and while you're on this page,

00:14:49.780 --> 00:14:51.780
you will show up
or I will show up,

00:14:51.780 --> 00:14:53.490
and you'll be able
to connect with me.

00:14:53.490 --> 00:14:57.390
And who knows, we could
be friends one day.

00:14:57.390 --> 00:14:59.510
So [INAUDIBLE] "Andrea--
thanks for playing."

00:14:59.510 --> 00:15:00.760
"Melissa-- you're awesome."

00:15:00.760 --> 00:15:01.410
OK.

00:15:01.410 --> 00:15:02.750
Those are my true friends.

00:15:02.750 --> 00:15:04.130
[LAUGHS]

00:15:04.130 --> 00:15:06.237
Anyway, but the
reality is that we're

00:15:06.237 --> 00:15:07.320
in this room for a reason.

00:15:07.320 --> 00:15:08.195
I truly believe that.

00:15:08.195 --> 00:15:10.237
And I would love to just
be part of your network.

00:15:10.237 --> 00:15:12.420
So, if you want to connect,
feel free to do so.

00:15:12.420 --> 00:15:15.540
I'll stay on that page
for the next 10 minutes.

00:15:15.540 --> 00:15:16.890
This is my family.

00:15:16.890 --> 00:15:18.886
I have a gaggle.

00:15:18.886 --> 00:15:20.700
You know, I like kids.

00:15:20.700 --> 00:15:22.530
I don't mind having them.

00:15:22.530 --> 00:15:25.950
My oldest, my very first child,
was born with Down syndrome.

00:15:25.950 --> 00:15:28.970
Which changed my
life completely.

00:15:28.970 --> 00:15:31.620
You know, when that's your first
child and you're super-young,

00:15:31.620 --> 00:15:33.303
and people are like,
wait, doesn't that

00:15:33.303 --> 00:15:34.470
just happen to other people?

00:15:34.470 --> 00:15:35.620
But it didn't.

00:15:35.620 --> 00:15:37.995
Statistics don't mean anything
when you're the statistic.

00:15:37.995 --> 00:15:42.180
[LAUGHS] But it changed who
I became as a developer,

00:15:42.180 --> 00:15:44.260
as a software engineer--
and, more importantly,

00:15:44.260 --> 00:15:46.080
as an advocate.

00:15:46.080 --> 00:15:48.510
And there's the rest of them.

00:15:48.510 --> 00:15:51.930
I am part of a program
called Lady Coders.

00:15:51.930 --> 00:15:53.802
I founded it to
help women in tech.

00:15:53.802 --> 00:15:55.260
But the funny thing,
and the reason

00:15:55.260 --> 00:15:57.450
why I feel honored
to be part of this,

00:15:57.450 --> 00:16:00.750
is that I recently, in the
last five years of my career,

00:16:00.750 --> 00:16:04.200
have struggled with,
like, a glass ceiling.

00:16:04.200 --> 00:16:05.700
I got to a certain point.

00:16:05.700 --> 00:16:08.310
I was, like,
honestly phenomenal!

00:16:08.310 --> 00:16:11.160
I could look at what I did and
went, like, this is awesome.

00:16:11.160 --> 00:16:14.220
And I would watch others
that did not look like me

00:16:14.220 --> 00:16:17.430
or sound like me move in the
direction I wanted to move in.

00:16:17.430 --> 00:16:19.960
And I'm like, what
the heck is going on?

00:16:19.960 --> 00:16:22.100
So then I found out, what
we do, women in tech,

00:16:22.100 --> 00:16:24.300
is we just hop around
to other companies.

00:16:24.300 --> 00:16:27.180
And we get promotions
through moving.

00:16:27.180 --> 00:16:29.112
Which is unfortunate
and very sad.

00:16:29.112 --> 00:16:30.570
But I would do it,
because I wanted

00:16:30.570 --> 00:16:33.120
to climb the ladder and increase
opportunity for other women

00:16:33.120 --> 00:16:34.180
like myself.

00:16:34.180 --> 00:16:36.060
But the last two times
I've had to do it--

00:16:36.060 --> 00:16:38.580
the last one was five
years ago, at Amazon--

00:16:38.580 --> 00:16:40.800
I got amazing experiences.

00:16:40.800 --> 00:16:43.220
I was part of Alexa
when it was first born--

00:16:43.220 --> 00:16:45.620
a wee, little baby--

00:16:45.620 --> 00:16:47.560
and got to grow with it.

00:16:47.560 --> 00:16:49.930
I was an early developer
on the platform.

00:16:49.930 --> 00:16:52.750
I have 40-plus skills on
that platform, some of them

00:16:52.750 --> 00:16:54.730
very successful.

00:16:54.730 --> 00:16:58.190
And, in that time,
though, again,

00:16:58.190 --> 00:17:00.730
the same job I got
when I showed up,

00:17:00.730 --> 00:17:03.740
four years later, had the exact
same job, the exact same title,

00:17:03.740 --> 00:17:06.400
though I was doing incredibly
more elaborate work

00:17:06.400 --> 00:17:07.480
over that span of time.

00:17:07.480 --> 00:17:10.450
And I'm like, what is going on?

00:17:10.450 --> 00:17:14.440
So I felt compelled to
start talking about it.

00:17:14.440 --> 00:17:16.569
Like, what is it?

00:17:16.569 --> 00:17:18.910
I actually ended up
getting this coined term

00:17:18.910 --> 00:17:20.750
of "having my wings clipped."

00:17:20.750 --> 00:17:21.250
Right?

00:17:21.250 --> 00:17:23.380
Soon I would get into
companies, and they'd be like,

00:17:23.380 --> 00:17:25.963
I know that you do these things
and you're super-good at this,

00:17:25.963 --> 00:17:27.280
but just sit down.

00:17:27.280 --> 00:17:28.660
Just be quiet.

00:17:28.660 --> 00:17:33.070
Like, stop already with the
happy mindfulness stuff.

00:17:33.070 --> 00:17:35.590
Like, just do what your job is.

00:17:35.590 --> 00:17:37.570
Don't do any more.

00:17:37.570 --> 00:17:38.770
And it bothered me.

00:17:38.770 --> 00:17:45.460
So I, at Alexa, was very, very
successful, very happy to help.

00:17:45.460 --> 00:17:47.200
But things that
bothered me that I just

00:17:47.200 --> 00:17:49.420
want to introduce
into your awareness,

00:17:49.420 --> 00:17:52.450
if you're not aware
of it, was, I never

00:17:52.450 --> 00:17:55.780
used a pronoun to define Alexa.

00:17:55.780 --> 00:17:57.100
It was always "it," to me.

00:17:57.100 --> 00:17:59.680
And maybe that's because I was
a software engineer helping

00:17:59.680 --> 00:18:02.090
to write the algorithms.

00:18:02.090 --> 00:18:03.190
But it was always an "it."

00:18:03.190 --> 00:18:05.650
And when people would say
"she," I almost was like--

00:18:05.650 --> 00:18:08.630
[NOISE] kind of like some of
us felt, maybe, earlier today.

00:18:08.630 --> 00:18:09.130
Right?

00:18:09.130 --> 00:18:11.240
Where we're like, that's
not-- it's not a she.

00:18:11.240 --> 00:18:11.740
Not a she.

00:18:11.740 --> 00:18:13.600
As a matter of
fact, there are shes

00:18:13.600 --> 00:18:15.310
that work on it,
but there's actually

00:18:15.310 --> 00:18:17.260
way more men that wrote Alexa--

00:18:17.260 --> 00:18:20.140
[LAUGHS] because of the
dichotomy of the fact

00:18:20.140 --> 00:18:22.850
that there are more men in
tech than there are women.

00:18:22.850 --> 00:18:26.560
So it's fascinating
and very, very deep.

00:18:26.560 --> 00:18:27.910
But just to give you an idea--

00:18:27.910 --> 00:18:32.380
so, as I'm working on Alexa, I
realize my very first skill--

00:18:32.380 --> 00:18:33.430
super-excited, right?

00:18:33.430 --> 00:18:35.050
You'll see my dad's
in that photo.

00:18:35.050 --> 00:18:37.960
He raised me well, on the
golden age of science fiction--

00:18:37.960 --> 00:18:40.060
Asimov, Bradbury, right?

00:18:40.060 --> 00:18:42.370
So, by the time I was six,
I knew these stories well.

00:18:42.370 --> 00:18:45.340
He, to this day, reminds me
that those stories do not

00:18:45.340 --> 00:18:46.870
end well, in most cases.

00:18:46.870 --> 00:18:48.100
[LAUGHTER]

00:18:48.100 --> 00:18:50.920
So, as a result, all of
my skills, the ones that

00:18:50.920 --> 00:18:53.470
are most popular, I
have the only skill

00:18:53.470 --> 00:18:56.590
that has mindfulness as
the single invocation name.

00:18:56.590 --> 00:18:57.940
It's like my claim to fame.

00:18:57.940 --> 00:19:00.520
Back then, people were
like, of all the things

00:19:00.520 --> 00:19:01.230
you're going to
build, you're going

00:19:01.230 --> 00:19:02.438
to build a mindfulness skill?

00:19:02.438 --> 00:19:03.250
I'm like, yes!

00:19:03.250 --> 00:19:05.740
Because I want them to be
mindful, when they, like,

00:19:05.740 --> 00:19:07.130
take over the world.

00:19:07.130 --> 00:19:07.870
[LAUGHTER]

00:19:07.870 --> 00:19:09.280
So I know what I'm doing.

00:19:09.280 --> 00:19:11.740
[LAUGHS] But I also
have daily kindness

00:19:11.740 --> 00:19:15.070
and Christmas kindness
and daily affirmation--

00:19:15.070 --> 00:19:16.930
another very popular skill.

00:19:16.930 --> 00:19:19.690
But here was the key
around building that.

00:19:19.690 --> 00:19:21.970
It's about 800 lines of code.

00:19:21.970 --> 00:19:25.690
More than 600 are the
affirmations themselves.

00:19:25.690 --> 00:19:27.760
When I go and have my--
when I had my dad use

00:19:27.760 --> 00:19:30.645
it for the first time, he
was like, that's so awesome!

00:19:30.645 --> 00:19:32.770
How did it know that that
was, like, the thing that

00:19:32.770 --> 00:19:33.520
resonated with me?

00:19:33.520 --> 00:19:35.650
Funny thing about affirmations.

00:19:35.650 --> 00:19:37.820
And I was like, I know, right?

00:19:37.820 --> 00:19:38.530
It's amazing.

00:19:38.530 --> 00:19:40.000
To him, it was magical.

00:19:40.000 --> 00:19:42.220
To him, it was
artificial intelligence.

00:19:42.220 --> 00:19:45.370
To me, it was, like, hours and
hours of writing lines of code.

00:19:45.370 --> 00:19:48.455
And the reality is that
all of Alexa, and probably

00:19:48.455 --> 00:19:49.330
the other platforms--

00:19:49.330 --> 00:19:51.400
I know Alexa intimately--

00:19:51.400 --> 00:19:52.630
all of Alexa was that way.

00:19:52.630 --> 00:19:55.153
We sat in a room, would
listen to people failing--

00:19:55.153 --> 00:19:57.570
I don't know if any of you
have had this experience, where

00:19:57.570 --> 00:20:00.430
you say something
to a smart device

00:20:00.430 --> 00:20:03.550
and it, like, beeps out or says
"I don't know what that is"

00:20:03.550 --> 00:20:05.410
or "I'm not sure about that."

00:20:05.410 --> 00:20:07.420
We literally would
take the failures

00:20:07.420 --> 00:20:09.738
and go and hard-code
the solution, so

00:20:09.738 --> 00:20:12.280
that the very next time-- and
I've done this with my husband,

00:20:12.280 --> 00:20:14.290
where I'd be like,
honey, oh my gosh,

00:20:14.290 --> 00:20:17.140
Alexa got this thing wrong, that
I know it should have right.

00:20:17.140 --> 00:20:21.340
And by the time he gets
to me, it's already fixed.

00:20:21.340 --> 00:20:24.830
But that's not because Alexa
is smart or figuring it out

00:20:24.830 --> 00:20:25.330
on its own.

00:20:25.330 --> 00:20:28.750
It's because my friend
is sitting in a room,

00:20:28.750 --> 00:20:32.420
waiting for that failure, and
hard-coding in a solution.

00:20:32.420 --> 00:20:36.040
So I just want to expose to
you that the good news is

00:20:36.040 --> 00:20:38.360
that machines are not there yet.

00:20:38.360 --> 00:20:38.860
Right?

00:20:38.860 --> 00:20:40.840
There are humans
doing this work.

00:20:40.840 --> 00:20:44.520
I remember, I have a sticker
on my desk, and it says--

00:20:44.520 --> 00:20:47.920
it's Cortana, and Cortana's
like, don't worry--

00:20:47.920 --> 00:20:51.700
humans are, in the end,
responsible for anything

00:20:51.700 --> 00:20:52.210
that I say.

00:20:52.210 --> 00:20:55.180
[LAUGHS] And it's a reminder
to me that it's not--

00:20:55.180 --> 00:20:57.430
It's magic to the people
that use this software.

00:20:57.430 --> 00:20:59.080
And it may be magic to you.

00:20:59.080 --> 00:21:01.840
But the reality is it's built
on the backs of developers,

00:21:01.840 --> 00:21:03.490
and we spend a
long time thinking

00:21:03.490 --> 00:21:08.170
of the ridiculous ways you
will say one specific thing.

00:21:08.170 --> 00:21:10.570
So natural language,
kind of like what

00:21:10.570 --> 00:21:13.850
Susan was talking about, is
very, very hard to build.

00:21:13.850 --> 00:21:16.900
It's almost impossible, because
we cannot conceive of the ways

00:21:16.900 --> 00:21:19.060
that humans will think
to say something.

00:21:19.060 --> 00:21:22.150
I remember I was working
with Capital One.

00:21:22.150 --> 00:21:24.130
And, in working
with Capital One,

00:21:24.130 --> 00:21:26.410
they were building
a financial skill.

00:21:26.410 --> 00:21:28.360
And, you know, we
do the FBI screen,

00:21:28.360 --> 00:21:32.750
where we watch the screen, and
we say, OK, go, use our skill.

00:21:32.750 --> 00:21:34.870
And we never in our
mind thought people

00:21:34.870 --> 00:21:38.620
would, in a transactional
sense, use the word "bucks"

00:21:38.620 --> 00:21:40.060
or "buckaroos."

00:21:40.060 --> 00:21:42.460
But it turned out people
would say "buckaroos"

00:21:42.460 --> 00:21:45.328
in a transactional context,
when moving money from one

00:21:45.328 --> 00:21:46.120
account to another.

00:21:46.120 --> 00:21:46.620
[LAUGHTER]

00:21:46.620 --> 00:21:48.980
7% of people!

00:21:48.980 --> 00:21:51.300
[LAUGHTER]

00:21:51.300 --> 00:21:53.040
You maybe are one of them!

00:21:53.040 --> 00:21:54.540
[LAUGHTER]

00:21:54.540 --> 00:21:56.440
But it was very eye-opening.

00:21:56.440 --> 00:21:57.850
We cannot tell.

00:21:57.850 --> 00:22:01.060
So what I-- especially
now, having accessibility

00:22:01.060 --> 00:22:05.800
very close to my heart,
also having two daughters,

00:22:05.800 --> 00:22:09.250
I realize now our responsibility
as storytellers, as really just

00:22:09.250 --> 00:22:12.640
human beings, is to
constantly be thinking

00:22:12.640 --> 00:22:16.780
about the infinite ways that we
should be able to communicate.

00:22:16.780 --> 00:22:18.670
I just sent a tweet out--

00:22:18.670 --> 00:22:21.670
I know-- about the tattoo thing.

00:22:21.670 --> 00:22:22.180
Right?

00:22:22.180 --> 00:22:23.305
Like, I was, like, floored.

00:22:23.305 --> 00:22:24.310
I was almost in tears.

00:22:24.310 --> 00:22:27.825
I'm like-- [INAUDIBLE]
anyway, it was crazy.

00:22:27.825 --> 00:22:29.200
I was like, I
don't have tattoos,

00:22:29.200 --> 00:22:31.960
but I was seriously like,
maybe I'm going to get one.

00:22:31.960 --> 00:22:35.410
Liberation, Janis
Joplin-- I was all in.

00:22:35.410 --> 00:22:39.010
So that was fantastic, but
it really touched a chord

00:22:39.010 --> 00:22:41.080
with a lot of women.

00:22:41.080 --> 00:22:44.500
And Alexa has risen up--
like, my participation

00:22:44.500 --> 00:22:48.460
in this project has risen
up this weird, internalized,

00:22:48.460 --> 00:22:50.410
like, why am I in tech?

00:22:50.410 --> 00:22:52.000
Do I actually like tech?

00:22:52.000 --> 00:22:55.960
Like, am in tech because,
at that time, women in t--

00:22:55.960 --> 00:22:56.835
and it is today, too.

00:22:56.835 --> 00:22:59.377
But women in tech, like, that
was the thing you should be in.

00:22:59.377 --> 00:23:01.780
If you're a woman and you
haven't decided, go into tech.

00:23:01.780 --> 00:23:05.168
I'm like, wait, is this,
like, internalized misogyny?

00:23:05.168 --> 00:23:07.210
How much of what I do is
because I like to do it?

00:23:07.210 --> 00:23:09.998
How much of what I do is
because everyone's telling me I

00:23:09.998 --> 00:23:10.540
should do it?

00:23:10.540 --> 00:23:13.618
I am definitely a be a lady
coder proponent, but now,

00:23:13.618 --> 00:23:15.160
all of a sudden,
I'm like, unless you

00:23:15.160 --> 00:23:16.950
want to do anything else.

00:23:16.950 --> 00:23:18.040
Right?

00:23:18.040 --> 00:23:20.590
Being a coder, being in
tech, is hard as a woman.

00:23:20.590 --> 00:23:21.510
It's terrible.

00:23:21.510 --> 00:23:22.935
I don't actually
want my children

00:23:22.935 --> 00:23:24.220
going into that field--

00:23:24.220 --> 00:23:25.935
my boys, maybe.

00:23:25.935 --> 00:23:27.310
I want them to do
what they love,

00:23:27.310 --> 00:23:31.570
but I recognize that there is
significant friction for all

00:23:31.570 --> 00:23:34.450
of us, as human beings,
to just be who we are.

00:23:34.450 --> 00:23:36.750
And Alexa, and, oh
my gosh, Google--

00:23:36.750 --> 00:23:40.240
like, Google with their--
and I love Google.

00:23:40.240 --> 00:23:43.120
My friend Angela Pham,
she was on stage.

00:23:43.120 --> 00:23:45.910
She's from Facebook, I think.

00:23:45.910 --> 00:23:48.130
We all travel around.

00:23:48.130 --> 00:23:51.070
So she mentioned this to
me, and it changed my life.

00:23:51.070 --> 00:23:53.097
And I'll leave you with this.

00:23:53.097 --> 00:23:54.680
She said, I don't
know how many of you

00:23:54.680 --> 00:23:57.490
have been on Google recently,
and they finish your sentences

00:23:57.490 --> 00:23:58.520
for you.

00:23:58.520 --> 00:23:59.020
Right?

00:23:59.020 --> 00:24:01.210
At first, I was like, oh
my gosh, this is awesome!

00:24:01.210 --> 00:24:02.860
And I actually
take some of them.

00:24:02.860 --> 00:24:04.715
But fast-forward, in your mind.

00:24:04.715 --> 00:24:07.090
Like, the first thing-- she
was talking about her friend.

00:24:07.090 --> 00:24:09.130
I've actually seen myself do it.

00:24:09.130 --> 00:24:11.177
You start looking
at these sentences,

00:24:11.177 --> 00:24:12.760
and they're not
exactly what you would

00:24:12.760 --> 00:24:14.680
say but they're close enough.

00:24:14.680 --> 00:24:16.390
And that now
becomes who you are,

00:24:16.390 --> 00:24:18.160
to the person you're
sending it to.

00:24:18.160 --> 00:24:20.080
And so what she said
is that she found

00:24:20.080 --> 00:24:23.898
her friends were like, so you
used to be super, like, snarky.

00:24:23.898 --> 00:24:24.940
Now you're kind of perf--

00:24:24.940 --> 00:24:26.050
Like, did you button up?

00:24:26.050 --> 00:24:29.890
Did you, like, remove your,
you know, like, Mohawk hair?

00:24:29.890 --> 00:24:31.630
Like, what's going on with you?

00:24:31.630 --> 00:24:34.517
And she was like, oh, I'm
just using the Google thing.

00:24:34.517 --> 00:24:36.100
But it's interesting,
when we're using

00:24:36.100 --> 00:24:39.790
these tools built by a certain
demographic of people--

00:24:39.790 --> 00:24:41.038
who knows who built them?

00:24:41.038 --> 00:24:42.580
Who knows who built
these algorithms?

00:24:42.580 --> 00:24:45.190
The reality is that the
people, like me, building

00:24:45.190 --> 00:24:48.710
the algorithms, really, we don't
understand the impact either.

00:24:48.710 --> 00:24:49.210
Right?

00:24:49.210 --> 00:24:52.310
How many of us have taken ethics
courses in our Computer Science

00:24:52.310 --> 00:24:52.810
degrees?

00:24:52.810 --> 00:24:55.360
Uh, yeah, almost nobody.

00:24:55.360 --> 00:24:57.800
Because it didn't
exist 20 years ago.

00:24:57.800 --> 00:24:59.560
It didn't exist 10 years ago.

00:24:59.560 --> 00:25:01.150
Now, we're a little
bit concerned,

00:25:01.150 --> 00:25:03.700
because Elon and Bill--

00:25:03.700 --> 00:25:07.300
Gates and Musk-- are getting
up on stage and going, hey,

00:25:07.300 --> 00:25:08.830
we should be worried about this.

00:25:08.830 --> 00:25:10.480
But 10 years ago,
when these things

00:25:10.480 --> 00:25:13.030
were starting to be
built, we didn't think

00:25:13.030 --> 00:25:15.190
it would take over the world!

00:25:15.190 --> 00:25:17.170
We didn't think that
Alexa would be the thing.

00:25:17.170 --> 00:25:19.720
We were actually
terrified it would fail.

00:25:19.720 --> 00:25:21.610
[LAUGHS] Because
it just came off

00:25:21.610 --> 00:25:24.440
of the Fire Phone,
which was a disaster.

00:25:24.440 --> 00:25:26.180
So it's very interesting.

00:25:26.180 --> 00:25:28.930
I hope this was just kind
of provocative in thought.

00:25:28.930 --> 00:25:31.300
But as a woman in tech,
as a mom of women,

00:25:31.300 --> 00:25:36.460
as a mom of people with
disabilities, an aging parent,

00:25:36.460 --> 00:25:38.840
I feel like there's more
opportunity than ever.

00:25:38.840 --> 00:25:43.141
I have more power than
ever to create solutions.

00:25:43.141 --> 00:25:44.920
But, OK, I'm going to
leave you with this.

00:25:44.920 --> 00:25:46.930
But what I do, to
measure my success,

00:25:46.930 --> 00:25:48.790
is kind of wrapped up in this.

00:25:48.790 --> 00:25:50.620
And the story behind
it is awesome.

00:25:50.620 --> 00:25:51.730
OK-- last thing.

00:25:51.730 --> 00:25:53.350
So "What is success?

00:25:53.350 --> 00:25:55.960
To laugh often and
much; to win the respect

00:25:55.960 --> 00:25:59.020
of intelligent people and
the affection of children;

00:25:59.020 --> 00:26:01.900
to earn the appreciation
of honest critics

00:26:01.900 --> 00:26:04.600
and endure the betrayal
of false friends;

00:26:04.600 --> 00:26:08.140
to appreciate the beauty;
to find the best in others;

00:26:08.140 --> 00:26:11.350
to leave the world a bit better,
whether by a healthy child,

00:26:11.350 --> 00:26:15.190
a garden patch, or a
redeemed social condition;

00:26:15.190 --> 00:26:17.800
to know that just
one life has breathed

00:26:17.800 --> 00:26:20.170
easier because you have lived.

00:26:20.170 --> 00:26:22.180
This is to have succeeded!"

00:26:22.180 --> 00:26:26.590
Now, Ralph Waldo Emerson
was quoted with that.

00:26:26.590 --> 00:26:29.110
Funny, sad story-- he
didn't actually write it.

00:26:29.110 --> 00:26:30.940
A woman wrote it.

00:26:30.940 --> 00:26:31.813
Right?

00:26:31.813 --> 00:26:33.980
Anyway, I'm very grateful
for the (EMPHASIS) man who

00:26:33.980 --> 00:26:36.467
brought that to my attention,
but just-- very interesting,

00:26:36.467 --> 00:26:37.550
the world that we live in.

00:26:37.550 --> 00:26:39.530
I hope you'll take
a bigger part in it.

00:26:39.530 --> 00:26:41.388
And it's my pleasure
to be here today.

00:26:41.388 --> 00:26:41.930
So thank you.

00:26:41.930 --> 00:26:42.430
[APPLAUSE]

00:26:42.430 --> 00:26:44.305
- Thank you so much.

00:26:44.305 --> 00:26:50.010
[APPLAUSE]

00:26:50.010 --> 00:26:52.970
- Our final speaker
today, Professor Breazeal.

00:26:52.970 --> 00:26:55.170
- All righty.

00:26:55.170 --> 00:26:57.090
It's a pleasure to be here.

00:26:57.090 --> 00:26:59.670
I'm going to talk about robots--

00:26:59.670 --> 00:27:06.810
like, physical robots-- and
about nonverbal communication

00:27:06.810 --> 00:27:10.170
and the emotional
lift of engagement.

00:27:10.170 --> 00:27:12.670
So we've heard from
our two prior speakers.

00:27:12.670 --> 00:27:14.760
Of course, we all know
that we are actually,

00:27:14.760 --> 00:27:18.480
in this time when
we're living with AI,

00:27:18.480 --> 00:27:21.475
this has been really such
a transformative change

00:27:21.475 --> 00:27:23.100
in my field of
artificial intelligence,

00:27:23.100 --> 00:27:26.060
to think about who interacts
with AI on a daily basis

00:27:26.060 --> 00:27:29.100
has profoundly changed
from people like us,

00:27:29.100 --> 00:27:32.310
to young children, to
our older citizens.

00:27:32.310 --> 00:27:34.440
And we don't really know
what the impact of this

00:27:34.440 --> 00:27:36.370
is going to mean for
society at large.

00:27:36.370 --> 00:27:38.940
And so I do agree that
there's a lot of importance

00:27:38.940 --> 00:27:42.518
around understanding long-term
impact and the ethics of this.

00:27:42.518 --> 00:27:44.310
And that's really what
I've been committing

00:27:44.310 --> 00:27:45.933
a lot of my research to.

00:27:45.933 --> 00:27:47.850
So I would say, if we're
going to characterize

00:27:47.850 --> 00:27:52.320
these conversational AIs as they
are today, they're voice-only,

00:27:52.320 --> 00:27:55.140
but we know that human
communication is ritually

00:27:55.140 --> 00:27:58.830
paralinguistic and nonverbal.

00:27:58.830 --> 00:28:00.430
And they're very transactional.

00:28:00.430 --> 00:28:00.620
Right?

00:28:00.620 --> 00:28:02.245
It's almost like kind
of playing chess.

00:28:02.245 --> 00:28:03.680
I say something; it responds.

00:28:03.680 --> 00:28:05.910
But we know that human
communication is a dance.

00:28:05.910 --> 00:28:09.420
And there's a lot of things
that are encoded and have

00:28:09.420 --> 00:28:11.847
meaning to us, in the
dynamics of that dance.

00:28:11.847 --> 00:28:13.680
And a lot of what I
want to talk about today

00:28:13.680 --> 00:28:16.200
is more on the research
side, [? about ?]

00:28:16.200 --> 00:28:20.940
how do we design robots
to participate more richly

00:28:20.940 --> 00:28:23.580
in that dance of
human communication,

00:28:23.580 --> 00:28:26.022
and how does that
impact human behavior?

00:28:26.022 --> 00:28:27.480
Because ultimately
I want to create

00:28:27.480 --> 00:28:30.810
robots that actually help us to
succeed-- to thrive and grow.

00:28:30.810 --> 00:28:32.560
It's not about just
building a cool robot.

00:28:32.560 --> 00:28:35.130
It's really about, how does
this robot help the world become

00:28:35.130 --> 00:28:36.270
a better place?

00:28:36.270 --> 00:28:38.910
So, again, just a slide
to remind ourselves

00:28:38.910 --> 00:28:41.370
that we are a profoundly
social species,

00:28:41.370 --> 00:28:44.130
and we have special neural
structures in our brain

00:28:44.130 --> 00:28:45.720
to support this
incredible ability

00:28:45.720 --> 00:28:49.290
we have to empathize,
to understand

00:28:49.290 --> 00:28:54.090
the mental states of others,
to be able to collaborate.

00:28:54.090 --> 00:28:57.510
And when we design
these systems,

00:28:57.510 --> 00:29:00.720
not surprisingly
they're triggering

00:29:00.720 --> 00:29:03.510
these aspects of our brains.

00:29:03.510 --> 00:29:06.900
And one of the punchlines
of my talk today is to say,

00:29:06.900 --> 00:29:09.120
as you see these systems
interacting with people,

00:29:09.120 --> 00:29:13.080
it is far beyond now, I
think, the surface phenomenon

00:29:13.080 --> 00:29:20.400
of, it's fun, it's
natural, to a much deeper

00:29:20.400 --> 00:29:22.960
social-psychological engagement.

00:29:22.960 --> 00:29:25.240
And I think there's tremendous
opportunity in that.

00:29:25.240 --> 00:29:28.530
And I think there's an important
ethical questions about that.

00:29:28.530 --> 00:29:32.760
So I want to start by just
highlighting-- again, this

00:29:32.760 --> 00:29:36.900
is kind of a fun, little
exploration of how profoundly

00:29:36.900 --> 00:29:38.670
social we really
are, as we interact

00:29:38.670 --> 00:29:40.830
with these different kinds
of digital assistants.

00:29:40.830 --> 00:29:42.288
So I'm going to
start with a video.

00:29:42.288 --> 00:29:44.400
So we've been actually
studying this, in my lab.

00:29:44.400 --> 00:29:46.360
We do a lot of randomized
control trials.

00:29:46.360 --> 00:29:48.930
So, you know, we're doing it
through scientific methods.

00:29:48.930 --> 00:29:51.203
And this is just
to kind of bring

00:29:51.203 --> 00:29:52.620
to the forefront
of your minds of,

00:29:52.620 --> 00:29:56.670
there's a lot of intentional
choices in how we design

00:29:56.670 --> 00:29:58.260
these kinds of technologies.

00:29:58.260 --> 00:30:00.210
So this is one of my
graduate students,

00:30:00.210 --> 00:30:04.440
asking each of these
voice-user-interface kind

00:30:04.440 --> 00:30:05.988
of systems the same question.

00:30:05.988 --> 00:30:07.530
And you can just
see how it responds.

00:30:07.530 --> 00:30:07.910
Right?

00:30:07.910 --> 00:30:08.280
[VIDEO PLAYBACK]

00:30:08.280 --> 00:30:08.863
- Hey, Alexa--

00:30:08.863 --> 00:30:09.363
- Ooh!

00:30:09.363 --> 00:30:09.930
That's loud.

00:30:09.930 --> 00:30:12.286
- --tell me about yourself.

00:30:12.286 --> 00:30:15.870
- I'm Amazon's Alexa,
designed around your voice.

00:30:15.870 --> 00:30:20.290
I can provide information,
music, news, weather, and more.

00:30:20.290 --> 00:30:22.650
- Hey, Jibo, tell
me about yourself.

00:30:25.290 --> 00:30:26.280
- OK, sure.

00:30:26.280 --> 00:30:27.440
My name is Jibo.

00:30:27.440 --> 00:30:28.440
I'm a robot.

00:30:28.440 --> 00:30:32.160
My favorite things to do are
talking to people and dancing.

00:30:32.160 --> 00:30:34.260
I also really like
Abraham Lincoln,

00:30:34.260 --> 00:30:37.594
because he was so honest
and because I like his hat.

00:30:37.594 --> 00:30:39.626
[LAUGHTER]

00:30:39.626 --> 00:30:44.050
- OK, Google, tell
me about yourself.

00:30:44.050 --> 00:30:45.470
- I'm your Google assistant.

00:30:45.470 --> 00:30:46.770
We can play Mad Libs.

00:30:46.770 --> 00:30:49.020
I can tell you a joke,
[? or you ?] [? can spin the ?]

00:30:49.020 --> 00:30:50.622
[? wheel. ?] What's
your [INAUDIBLE]??

00:30:50.622 --> 00:30:51.090
[END PLAYBACK]

00:30:51.090 --> 00:30:53.423
So I hope, by the video, you
can already get a sense of,

00:30:53.423 --> 00:30:56.070
there's a lot of
different design decisions

00:30:56.070 --> 00:30:59.850
and philosophies of how you want
to create an agent that's going

00:30:59.850 --> 00:31:03.060
to play a role in your life.

00:31:03.060 --> 00:31:05.480
Alexa, Google, much more
the personal assistant,

00:31:05.480 --> 00:31:06.480
much more transactional.

00:31:06.480 --> 00:31:08.130
Here's what I can do for you.

00:31:08.130 --> 00:31:11.490
Jibo, designed to be much
more social-relational,

00:31:11.490 --> 00:31:12.970
much more embodied--
social cues.

00:31:12.970 --> 00:31:13.470
Right?

00:31:13.470 --> 00:31:15.555
Just a different sort
of design philosophy.

00:31:15.555 --> 00:31:17.430
You'll also notice a
difference in the amount

00:31:17.430 --> 00:31:19.170
of those nonverbal cues.

00:31:19.170 --> 00:31:20.010
Jibo moves.

00:31:20.010 --> 00:31:22.200
He has-- you know,
his eye is animated.

00:31:22.200 --> 00:31:26.400
It moves in a way that kind
of echoes our nonverbal cues.

00:31:26.400 --> 00:31:28.290
Alexa, much simpler,
but at least

00:31:28.290 --> 00:31:30.840
has this LED light that
orients toward you,

00:31:30.840 --> 00:31:33.360
so you have a sense of, I'm
paying attention to you.

00:31:33.360 --> 00:31:37.860
Google just has this
flickering light of activity.

00:31:37.860 --> 00:31:39.450
"Alexa" is at least a name.

00:31:39.450 --> 00:31:41.430
"Jibo" feels like a name.

00:31:41.430 --> 00:31:43.150
"OK Google" is a brand.

00:31:43.150 --> 00:31:43.650
Right?

00:31:43.650 --> 00:31:45.410
So, just intentional
design decisions.

00:31:45.410 --> 00:31:47.160
So we've been starting
to try to explore--

00:31:47.160 --> 00:31:50.465
so, from just an everyday
person's perspective,

00:31:50.465 --> 00:31:52.590
if you were going to do
kind of robot speed dating,

00:31:52.590 --> 00:31:54.257
you're going to line
them up on a table,

00:31:54.257 --> 00:31:57.170
invite families in, to basically
ask these series of kinds

00:31:57.170 --> 00:32:00.230
of questions-- entertainment,
utility, social--

00:32:00.230 --> 00:32:01.670
of all of these agents--

00:32:01.670 --> 00:32:03.830
as they choose, as they choose.

00:32:03.830 --> 00:32:06.470
What are the differences
in just behavior engagement

00:32:06.470 --> 00:32:07.850
that you see?

00:32:07.850 --> 00:32:10.867
So one thing we ask people
to do is, after they interact

00:32:10.867 --> 00:32:12.950
with these systems for as
long as they would like,

00:32:12.950 --> 00:32:15.890
is to basically
take a personality

00:32:15.890 --> 00:32:17.610
task for that agent.

00:32:17.610 --> 00:32:20.690
And what you're seeing
here is that, of course,

00:32:20.690 --> 00:32:22.260
different personalities emerge.

00:32:22.260 --> 00:32:22.760
Right?

00:32:22.760 --> 00:32:26.750
So actually Alexa and Google
have a very similar personality

00:32:26.750 --> 00:32:27.680
profile--

00:32:27.680 --> 00:32:29.180
more quiet, more
conscientious, more

00:32:29.180 --> 00:32:31.628
like maybe what you'd expected
a professional executive

00:32:31.628 --> 00:32:33.170
or a digital assistant
to kind of be.

00:32:33.170 --> 00:32:33.950
Right?

00:32:33.950 --> 00:32:36.860
Jibo, more outgoing,
more friendly, more warm.

00:32:36.860 --> 00:32:38.670
Just a different
personality profile.

00:32:38.670 --> 00:32:41.090
So it's interesting to see
that this does come across,

00:32:41.090 --> 00:32:43.160
as people even interact
for just 20 minutes

00:32:43.160 --> 00:32:45.230
with a digital assistant
or a social robot.

00:32:45.230 --> 00:32:47.720
These qualities are
already coming out.

00:32:47.720 --> 00:32:49.970
This is just to show
that people interacted

00:32:49.970 --> 00:32:54.413
with all the systems, you
know, in kind of a similar way

00:32:54.413 --> 00:32:55.830
in terms of amount
of interaction,

00:32:55.830 --> 00:33:00.890
but these social kind of class
of interactions dominated.

00:33:00.890 --> 00:33:03.420
People were drawn to the
social kinds of things.

00:33:03.420 --> 00:33:05.420
Although we know that
people like entertainment,

00:33:05.420 --> 00:33:08.870
and they like information, a
lot of these social properties.

00:33:08.870 --> 00:33:11.000
People tended to interact
with Jibo much more.

00:33:11.000 --> 00:33:11.750
Right?

00:33:11.750 --> 00:33:13.700
Now, the other thing
that's really interesting

00:33:13.700 --> 00:33:14.650
is, if you look at--

00:33:14.650 --> 00:33:16.858
it's hard to read these
things, but basically there's

00:33:16.858 --> 00:33:19.693
a strong preference,
in many cases, of Jibo.

00:33:19.693 --> 00:33:21.110
And I think a lot
of it is because

00:33:21.110 --> 00:33:23.300
of the social-relational
aspect that we were drawn to.

00:33:23.300 --> 00:33:25.008
But the other things
that are interesting

00:33:25.008 --> 00:33:28.340
is, why is Google
often less desired

00:33:28.340 --> 00:33:30.768
than Alexa, when in so many
ways you could argue they're

00:33:30.768 --> 00:33:31.310
very similar?

00:33:31.310 --> 00:33:34.160
And I do think it's because
these subtle cues are

00:33:34.160 --> 00:33:36.900
adding friction to the
spoken interaction.

00:33:36.900 --> 00:33:37.400
Right?

00:33:37.400 --> 00:33:40.610
So even when you look at a topic
like information, which agent

00:33:40.610 --> 00:33:42.830
would you prefer
for information,

00:33:42.830 --> 00:33:44.750
Google should dominate that.

00:33:44.750 --> 00:33:45.250
Right?

00:33:45.250 --> 00:33:46.820
[LAUGHS] But it doesn't.

00:33:46.820 --> 00:33:47.360
Right?

00:33:47.360 --> 00:33:48.860
For something like
entertainment,

00:33:48.860 --> 00:33:52.640
where Alexa has way more
entertainment skills than Jibo,

00:33:52.640 --> 00:33:53.648
Jibo is dominant.

00:33:53.648 --> 00:33:55.940
So again, it's just to say,
I think these things matter

00:33:55.940 --> 00:33:58.730
to us in a deep way, because
we are such profoundly

00:33:58.730 --> 00:34:00.380
social creatures.

00:34:00.380 --> 00:34:02.523
Now, we look a lot
at social robots--

00:34:02.523 --> 00:34:04.940
this is getting more into the
research side-- in terms of,

00:34:04.940 --> 00:34:07.398
how do you design these systems
to add real value to people

00:34:07.398 --> 00:34:08.398
in their everyday lives?

00:34:08.398 --> 00:34:10.940
So we've be doing a lot of
work in designing social robots

00:34:10.940 --> 00:34:15.620
to engage children as
personalized peer-like learning

00:34:15.620 --> 00:34:16.340
companions.

00:34:16.340 --> 00:34:19.429
So not as a tutor
that teaches but as

00:34:19.429 --> 00:34:22.580
a playmate that
personalizes and adapts

00:34:22.580 --> 00:34:24.380
to help support the
learning and engagement

00:34:24.380 --> 00:34:26.510
trajectory of a child.

00:34:26.510 --> 00:34:29.239
And we know that when
children interact and play,

00:34:29.239 --> 00:34:34.219
it is richly, richly both
linguistic and paralinguistic.

00:34:34.219 --> 00:34:36.020
And a lot of the
trust and the rapport

00:34:36.020 --> 00:34:37.880
is coming from those nonverbals.

00:34:37.880 --> 00:34:40.280
So when we look at the
importance of social dynamics

00:34:40.280 --> 00:34:43.150
and how do you design now a
robot that can engage in that,

00:34:43.150 --> 00:34:45.170
again, that dance
of communication,

00:34:45.170 --> 00:34:47.420
we can look at the
field of research.

00:34:47.420 --> 00:34:49.170
Much the majority
of the research

00:34:49.170 --> 00:34:51.920
is around, I would say,
this intrapersonal context,

00:34:51.920 --> 00:34:54.620
or just looking at an
individual in isolation.

00:34:54.620 --> 00:34:57.495
So, when you see systems that
recognize that you're smiling

00:34:57.495 --> 00:34:59.870
and things like that, it's
just considering an individual

00:34:59.870 --> 00:35:02.750
and analyzing the image
or the information

00:35:02.750 --> 00:35:04.790
from that standpoint.

00:35:04.790 --> 00:35:07.970
More recently, we're starting
to see an appreciation of kind

00:35:07.970 --> 00:35:11.090
of social cueing, to be
able to intuit things

00:35:11.090 --> 00:35:12.995
like trust and rapport.

00:35:12.995 --> 00:35:14.870
How do you design a
system that can recognize

00:35:14.870 --> 00:35:16.662
when trust and rapport
is starting to build

00:35:16.662 --> 00:35:19.380
between people interacting?

00:35:19.380 --> 00:35:21.580
And much more recently,
we're starting

00:35:21.580 --> 00:35:24.610
to see this appreciation
of the intention

00:35:24.610 --> 00:35:26.900
behind this nonverbal
communication--

00:35:26.900 --> 00:35:28.780
this call-response.

00:35:28.780 --> 00:35:29.647
Am I being engaging?

00:35:29.647 --> 00:35:30.730
Are they paying attention?

00:35:30.730 --> 00:35:31.880
I'm going to look.

00:35:31.880 --> 00:35:33.178
Do I see their response?

00:35:33.178 --> 00:35:34.720
I'm going to be more
animated, to see

00:35:34.720 --> 00:35:35.680
if they get more animated.

00:35:35.680 --> 00:35:35.920
Right?

00:35:35.920 --> 00:35:37.690
So there was a lot of
this call-and-response,

00:35:37.690 --> 00:35:38.630
like in storytelling.

00:35:38.630 --> 00:35:40.540
So how do we start
to design robots

00:35:40.540 --> 00:35:43.700
that engage in this intentional
side of social dynamics?

00:35:43.700 --> 00:35:46.380
So this is a video that's just
kind of going to quickly walk

00:35:46.380 --> 00:35:47.630
you through the whole process.

00:35:47.630 --> 00:35:49.330
And I hope you can
appreciate, it really

00:35:49.330 --> 00:35:51.372
starts and is grounded by
understanding something

00:35:51.372 --> 00:35:52.880
about human-human communication.

00:35:52.880 --> 00:35:57.640
So, if we want a robot to
participate in the storytelling

00:35:57.640 --> 00:35:59.230
experience with
young children, we

00:35:59.230 --> 00:36:01.930
want to understand what that
looks like between children.

00:36:01.930 --> 00:36:04.480
And then you want to see if you
can model that and then have

00:36:04.480 --> 00:36:05.530
a robot participate in that.

00:36:05.530 --> 00:36:06.030
Right?

00:36:06.030 --> 00:36:09.155
So we start by
studying children.

00:36:09.155 --> 00:36:09.822
[VIDEO PLAYBACK]

00:36:09.822 --> 00:36:11.750
- --open a window--

00:36:11.750 --> 00:36:13.000
- How they share these cues--

00:36:13.000 --> 00:36:15.490
verbal, nonverbal,
gesture, prosody.

00:36:15.490 --> 00:36:19.210
- [INAUDIBLE].

00:36:19.210 --> 00:36:21.400
- And we do that across
many, many, many, many, many

00:36:21.400 --> 00:36:21.850
children.

00:36:21.850 --> 00:36:22.350
Right?

00:36:22.350 --> 00:36:25.540
So we're gathering a very large,
very rich multimodal corpus.

00:36:25.540 --> 00:36:28.970
Facial expressions, body
pose, what they're saying.

00:36:28.970 --> 00:36:29.470
Right?

00:36:29.470 --> 00:36:31.990
And then we can start to
computationally model that.

00:36:31.990 --> 00:36:34.330
So we use machine-learning
methods to try to basically

00:36:34.330 --> 00:36:37.540
say, how do we train a system
with this kind of data,

00:36:37.540 --> 00:36:40.690
to be able to participate in
this kind of behavior with

00:36:40.690 --> 00:36:41.630
an actual child--

00:36:41.630 --> 00:36:42.130
Right?

00:36:42.130 --> 00:36:42.680
This is a child.

00:36:42.680 --> 00:36:43.722
[INAUDIBLE] actual child.

00:36:43.722 --> 00:36:46.570
--with all the richness of the
nonverbals, the turn-taking,

00:36:46.570 --> 00:36:49.570
the back-channeling that we see
in natural human communication.

00:36:49.570 --> 00:36:51.280
So we can, of
course, analyze this

00:36:51.280 --> 00:36:53.530
from the pure, kind of
behavioral standpoint of,

00:36:53.530 --> 00:36:56.950
like, if we compare the robot's
behavior to children's data,

00:36:56.950 --> 00:36:58.810
how much of a match is that?

00:36:58.810 --> 00:37:00.340
But importantly,
we also want to see

00:37:00.340 --> 00:37:02.085
what's the impact on
children's behavior.

00:37:02.085 --> 00:37:02.450
[END PLAYBACK]

00:37:02.450 --> 00:37:04.242
So I'm just going to
kind of blitzkrieg you

00:37:04.242 --> 00:37:05.950
through a number of findings.

00:37:05.950 --> 00:37:07.815
But we can put two
robots side to side.

00:37:07.815 --> 00:37:09.940
We kind of talked about
that speed-dating protocol.

00:37:09.940 --> 00:37:11.680
We can do the same
where now we have

00:37:11.680 --> 00:37:14.350
a similar robot but running
very two different models--

00:37:14.350 --> 00:37:17.380
in this case, a contingent,
dynamic, turn-taking model,

00:37:17.380 --> 00:37:20.230
versus one which is a
robot that is as expressive

00:37:20.230 --> 00:37:22.000
but not with the
right social dynamics.

00:37:22.000 --> 00:37:22.630
Right?

00:37:22.630 --> 00:37:24.190
Maybe it's a random
kind of way it's

00:37:24.190 --> 00:37:25.450
triggering on those behaviors.

00:37:25.450 --> 00:37:28.690
And what we see is, if
we're asking children

00:37:28.690 --> 00:37:31.300
to tell the story between
two of these robots present,

00:37:31.300 --> 00:37:34.393
children tend to orient and
engage and tell the story

00:37:34.393 --> 00:37:36.310
to the robot that is
doing the back-channeling

00:37:36.310 --> 00:37:37.477
and the contingent behavior.

00:37:37.477 --> 00:37:40.062
There is a preference
to attend to that robot.

00:37:40.062 --> 00:37:42.520
When we have two robots-- again,
with these different kinds

00:37:42.520 --> 00:37:45.440
of behaviors-- and there
was a novel animal,

00:37:45.440 --> 00:37:49.780
an unfamiliar animal, and you're
asking those two robots what is

00:37:49.780 --> 00:37:52.200
this animal, one might say
it's, you know, a [INAUDIBLE],,

00:37:52.200 --> 00:37:54.790
and the other says it's
a [INAUDIBLE]---- whatever.

00:37:54.790 --> 00:37:56.410
One is contingent; one is not.

00:37:56.410 --> 00:37:58.930
The children believe
the contingent robot

00:37:58.930 --> 00:38:01.138
as being the credible
source of information.

00:38:01.138 --> 00:38:02.680
So again, children
[INAUDIBLE] people

00:38:02.680 --> 00:38:07.450
are inferring these nonverbals
in a very deep, important way.

00:38:07.450 --> 00:38:10.103
When we look at affective
expression [INAUDIBLE]----

00:38:10.103 --> 00:38:12.020
so I'm going to play
this other video for you.

00:38:12.020 --> 00:38:15.490
Again, it's a storytelling
and retelling app, task,

00:38:15.490 --> 00:38:17.860
where the scenario
is, there's a robot--

00:38:17.860 --> 00:38:18.640
tells a story.

00:38:18.640 --> 00:38:20.080
There's a puppet that's asleep.

00:38:20.080 --> 00:38:22.200
The puppet wakes up-- oh,
no, I missed the story!

00:38:22.200 --> 00:38:24.910
And we ask the child,
can you retell that story

00:38:24.910 --> 00:38:25.600
to the puppet?

00:38:25.600 --> 00:38:26.410
Right?

00:38:26.410 --> 00:38:28.420
We have two different
conditions-- a robot

00:38:28.420 --> 00:38:30.280
with a more neutral,
kind of more

00:38:30.280 --> 00:38:32.920
like what you hear
digital voices today,

00:38:32.920 --> 00:38:34.600
and one with a more
expressive voice.

00:38:34.600 --> 00:38:35.890
So we want to see what's
the difference-- right?

00:38:35.890 --> 00:38:37.450
What's the difference
for children?

00:38:37.450 --> 00:38:38.380
So here's the video.

00:38:38.380 --> 00:38:40.810
[VIDEO PLAYBACK]

00:38:40.810 --> 00:38:41.782
- (CUTESY VOICE) Hi!

00:38:41.782 --> 00:38:43.240
I'm [INAUDIBLE].

00:38:43.240 --> 00:38:46.790
My favorite color is blue.

00:38:46.790 --> 00:38:49.250
- So that's the
expressive condition.

00:38:49.250 --> 00:38:50.098
- My favorite--

00:38:50.098 --> 00:38:51.140
- This is, like, neutral.

00:38:51.140 --> 00:38:54.388
- --is blue.

00:38:54.388 --> 00:38:57.002
There once was a boy who had--

00:38:57.002 --> 00:38:58.710
- So obviously this
is the neutral voice.

00:38:58.710 --> 00:39:01.924
And you can kind of see how
children are responding--

00:39:01.924 --> 00:39:03.740
- The deer stopped--

00:39:03.740 --> 00:39:06.553
- --versus one that's
more expressive.

00:39:06.553 --> 00:39:08.220
And so, interesting,
you see this a lot.

00:39:08.220 --> 00:39:09.450
There's a lot more touch.

00:39:09.450 --> 00:39:10.860
There's a lot more affection.

00:39:10.860 --> 00:39:14.100
There's this emotional
lift of the experience.

00:39:14.100 --> 00:39:17.838
And then we have the child tell
the story back to the puppet.

00:39:17.838 --> 00:39:19.332
- [INAUDIBLE].

00:39:25.283 --> 00:39:26.450
- And here's the punch line.

00:39:26.450 --> 00:39:28.730
When the robot is
more expressive,

00:39:28.730 --> 00:39:32.930
the children socially model and
emulate that robot more than

00:39:32.930 --> 00:39:35.602
when it's a neutral robot.

00:39:35.602 --> 00:39:37.310
And why that's important
is, if the robot

00:39:37.310 --> 00:39:40.340
is beginning to model slightly
more sophisticated language

00:39:40.340 --> 00:39:42.732
and vocabulary, that's a
wave upon which you can pull

00:39:42.732 --> 00:39:43.940
the child along a trajectory.

00:39:43.940 --> 00:39:44.600
Right?

00:39:44.600 --> 00:39:46.427
So we see children
modeling the robot more

00:39:46.427 --> 00:39:47.510
when it's more expressive.

00:39:47.510 --> 00:39:50.600
And even after a month
later, asking the child

00:39:50.600 --> 00:39:53.330
to retell that story
again, children

00:39:53.330 --> 00:39:56.330
who interacted with that
expressive speech robot

00:39:56.330 --> 00:39:58.430
told longer stories
and still used

00:39:58.430 --> 00:40:01.527
more of this kind of
language that the robot used.

00:40:01.527 --> 00:40:02.110
[END PLAYBACK]

00:40:02.110 --> 00:40:04.527
So again, this is a very kind
of [? sticking ?] mechanism.

00:40:04.527 --> 00:40:07.280
This idea of social
emulation, it turns out,

00:40:07.280 --> 00:40:08.733
is a very robust phenomenon.

00:40:08.733 --> 00:40:10.400
We've been looking
at it across a number

00:40:10.400 --> 00:40:11.630
of different contexts--

00:40:11.630 --> 00:40:14.570
things like, you know, we
know that things like children

00:40:14.570 --> 00:40:17.120
developing either a
fixed or a growth mindset

00:40:17.120 --> 00:40:21.270
is often formed by how
we're praised by adults.

00:40:21.270 --> 00:40:23.070
We're finding that
if a child interacts

00:40:23.070 --> 00:40:25.110
with a robot with a
growth mindset that

00:40:25.110 --> 00:40:28.968
persists longer in hard puzzles,
that children tends to now also

00:40:28.968 --> 00:40:30.510
identify with having
a growth mindset

00:40:30.510 --> 00:40:33.130
and persist harder on
those puzzles, as well.

00:40:33.130 --> 00:40:34.710
So we're seeing
this for mindset.

00:40:34.710 --> 00:40:36.510
We've seen this for
[INAUDIBLE] curiosity.

00:40:36.510 --> 00:40:39.690
We're exploring it in
creativity right now, as well.

00:40:39.690 --> 00:40:42.702
This is another quick
video I want to show you.

00:40:42.702 --> 00:40:44.660
Again, now, this is a
collaborative interaction

00:40:44.660 --> 00:40:47.190
between child and a robot,
where they're taking turns

00:40:47.190 --> 00:40:49.200
playing a vocabulary game.

00:40:49.200 --> 00:40:51.900
And again, I want you to see
the dynamics of the interaction.

00:40:51.900 --> 00:40:54.750
[VIDEO PLAYBACK]

00:40:54.750 --> 00:40:57.870
So here, there's a challenge
word called "lavender."

00:40:57.870 --> 00:41:00.270
There's a game where
the child and the robot

00:41:00.270 --> 00:41:02.730
look through a scene
with different stickers.

00:41:02.730 --> 00:41:04.620
And they're trying
to choose items

00:41:04.620 --> 00:41:07.800
that map to that challenge
word of "lavender."

00:41:07.800 --> 00:41:10.230
She's familiar with this word,
but she doesn't know quite

00:41:10.230 --> 00:41:10.813
what it means.

00:41:10.813 --> 00:41:13.890
And the robot is actually
learning a personalized policy

00:41:13.890 --> 00:41:18.240
to her of, when do you act
as the more knowing peer,

00:41:18.240 --> 00:41:20.335
to help scaffold and
support that learning?

00:41:20.335 --> 00:41:21.960
When do you act as
a less-knowing peer,

00:41:21.960 --> 00:41:23.418
to ask questions
to allow the child

00:41:23.418 --> 00:41:25.890
to reinforce what they know?

00:41:25.890 --> 00:41:28.260
So this is a really
interesting moment.

00:41:28.260 --> 00:41:30.070
She chooses something,
but it's wrong.

00:41:32.620 --> 00:41:34.900
The robot expresses
confidence in her.

00:41:34.900 --> 00:41:36.220
"I believe in you."

00:41:36.220 --> 00:41:39.130
She's a little disappointed,
but he does that gesture,

00:41:39.130 --> 00:41:40.370
and she's right back in it.

00:41:40.370 --> 00:41:40.870
Right?

00:41:40.870 --> 00:41:43.945
That emotional connection
brings her right back in.

00:41:43.945 --> 00:41:46.070
Now the robot says "'lavender'
means "purple.'" And

00:41:46.070 --> 00:41:47.290
she's like, OK, now I know it.

00:41:47.290 --> 00:41:49.290
Now, interestingly, here,
it's the robot's turn,

00:41:49.290 --> 00:41:51.640
but she is fully engaged,
helping that robot

00:41:51.640 --> 00:41:53.740
find things that are lavender.

00:41:53.740 --> 00:41:55.400
And you see again
that connection.

00:41:55.400 --> 00:41:58.330
When the robot makes a choice,
you'll hear her say, quietly,

00:41:58.330 --> 00:41:59.860
"I believe in you."

00:41:59.860 --> 00:42:00.610
So again, empathy.

00:42:00.610 --> 00:42:01.110
Right?

00:42:01.110 --> 00:42:01.870
This modeling.

00:42:01.870 --> 00:42:02.455
Right?

00:42:02.455 --> 00:42:02.800
[END PLAYBACK]

00:42:02.800 --> 00:42:04.258
We started looking
at this in terms

00:42:04.258 --> 00:42:07.840
of, now, triadic interactions,
looking in a context of, say,

00:42:07.840 --> 00:42:09.430
a pediatric hospital,
where it's not

00:42:09.430 --> 00:42:11.650
only important that the
child and the robot interact

00:42:11.650 --> 00:42:15.010
but that also the child life
specialist is able to scaffold

00:42:15.010 --> 00:42:16.780
and support that interaction.

00:42:16.780 --> 00:42:19.060
Randomized control trials,
comparing a physical robot

00:42:19.060 --> 00:42:20.860
versus a virtual
robot versus a plush.

00:42:20.860 --> 00:42:23.020
We see shared
attentional behaviors.

00:42:23.020 --> 00:42:26.120
Social triadic behaviors are
much stronger with the robot.

00:42:26.120 --> 00:42:27.490
And this is really important.

00:42:27.490 --> 00:42:29.470
And I just want to end
with one last video,

00:42:29.470 --> 00:42:31.063
to say it's not just for kids!

00:42:31.063 --> 00:42:31.730
[VIDEO PLAYBACK]

00:42:31.730 --> 00:42:33.970
We've started looking at
this for older adults,

00:42:33.970 --> 00:42:38.460
as well, to see if
a robot can foster

00:42:38.460 --> 00:42:40.650
social connections
between residents--

00:42:40.650 --> 00:42:42.660
not just between the
person and the robot,

00:42:42.660 --> 00:42:44.640
but within a community setting.

00:42:44.640 --> 00:42:47.600
- [INAUDIBLE] new
regular visitor here.

00:42:47.600 --> 00:42:51.120
It's a robot called Jibo that's
being used as part of a pilot

00:42:51.120 --> 00:42:53.660
project among the
several Bay Area

00:42:53.660 --> 00:42:56.505
facilities run by [INAUDIBLE].

00:42:56.505 --> 00:43:00.000
Erin Partridge, a researcher and
art therapist with Elder Care

00:43:00.000 --> 00:43:02.130
Alliance, takes
Jibo with her when

00:43:02.130 --> 00:43:03.630
she meets with the seniors.

00:43:03.630 --> 00:43:06.170
She says this is not
a case of caretakers.

00:43:06.170 --> 00:43:08.445
being replaced by automation.

00:43:08.445 --> 00:43:11.310
Instead, they are using this
cute piece of technology

00:43:11.310 --> 00:43:14.140
to encourage their residents
to connect with each other--

00:43:14.140 --> 00:43:15.120
- So, again, touch.

00:43:15.120 --> 00:43:17.680
We see this again and again.

00:43:17.680 --> 00:43:21.290
- Jibo inspires some giggles
and guffaws amongst this group,

00:43:21.290 --> 00:43:22.220
here.

00:43:22.220 --> 00:43:23.450
- [INAUDIBLE].

00:43:23.450 --> 00:43:26.245
You should be able to
laugh more and more.

00:43:26.245 --> 00:43:29.290
- But what it does is it
brings that out of us.

00:43:29.290 --> 00:43:31.579
That's part of the
miracle [INAUDIBLE]..

00:43:31.579 --> 00:43:34.162
And that's part of the miracle
of living, as far as that goes.

00:43:34.162 --> 00:43:34.620
[END PLAYBACK]

00:43:34.620 --> 00:43:36.570
- So again, the importance
of that experience,

00:43:36.570 --> 00:43:39.210
of that emotional lift
of the engagement.

00:43:39.210 --> 00:43:40.680
Quick punch line, here--

00:43:40.680 --> 00:43:42.180
actually having the
robot there does

00:43:42.180 --> 00:43:44.370
foster human-human
connection to communication,

00:43:44.370 --> 00:43:45.555
as a social catalyst.

00:43:45.555 --> 00:43:46.680
This is really fascinating.

00:43:46.680 --> 00:43:49.200
And one last thought I
want to leave you with

00:43:49.200 --> 00:43:51.960
is, the future with these
technologies-- right?

00:43:51.960 --> 00:43:54.060
Manufacturing robots
that work side-by-side

00:43:54.060 --> 00:43:57.540
by people, starting to
exhibit these social cues.

00:43:57.540 --> 00:44:00.210
Cars, autonomous driving--
how do you interact and signal

00:44:00.210 --> 00:44:01.260
intent with pedestrians?

00:44:01.260 --> 00:44:02.130
You're starting
to see experiments

00:44:02.130 --> 00:44:04.800
with literally putting social
cues like eyes or things

00:44:04.800 --> 00:44:07.688
that move like eyes, to be
able to coordinate that intent.

00:44:07.688 --> 00:44:09.480
Systems like [? LAQ ?]
just being announced

00:44:09.480 --> 00:44:10.522
to interact with seniors.

00:44:10.522 --> 00:44:13.560
So again, I think we're at
the very beginning of thinking

00:44:13.560 --> 00:44:17.150
about this world of interacting
with AIs with intention,

00:44:17.150 --> 00:44:21.690
in this dynamics, kind of
social-interpersonal context.

00:44:21.690 --> 00:44:24.240
And I'll just leave you with a
thought about, again, gender.

00:44:24.240 --> 00:44:25.290
We heard a little
about this, before.

00:44:25.290 --> 00:44:26.707
I mean, with these
systems-- like,

00:44:26.707 --> 00:44:28.890
if you were going to
gender these robots, just

00:44:28.890 --> 00:44:31.390
provocatively, how
would you gender them?

00:44:31.390 --> 00:44:32.817
You can choose to
assign a gender,

00:44:32.817 --> 00:44:34.650
as you design the robot,
or you could choose

00:44:34.650 --> 00:44:35.970
to remain moot on the topic.

00:44:35.970 --> 00:44:36.510
Right?

00:44:36.510 --> 00:44:38.110
And so I think, with
these kinds of systems,

00:44:38.110 --> 00:44:38.880
it's very interesting
to think about,

00:44:38.880 --> 00:44:41.250
when do you actually
try to design an agenda

00:44:41.250 --> 00:44:43.500
and, whether you do
or don't, what gender

00:44:43.500 --> 00:44:45.232
do people tend to
ascribe to the system?

00:44:45.232 --> 00:44:47.190
Because it may not always
be what you intended.

00:44:47.190 --> 00:44:48.060
All right.

00:44:48.060 --> 00:44:48.760
Thank you.

00:44:48.760 --> 00:44:53.495
[APPLAUSE]

00:44:59.330 --> 00:45:01.550
- Thank you so much,
Professor Breazeal.

00:45:01.550 --> 00:45:03.380
I'd like to kick us
off with a follow-up

00:45:03.380 --> 00:45:05.180
question about social dynamics.

00:45:05.180 --> 00:45:07.370
Because I feel like
this brings together

00:45:07.370 --> 00:45:12.770
both Susan and your
experience around interacting

00:45:12.770 --> 00:45:14.430
with automated agents.

00:45:14.430 --> 00:45:18.710
So, when I think about having
a relationship with someone,

00:45:18.710 --> 00:45:23.100
I think a lot about
how I approach them.

00:45:23.100 --> 00:45:23.600
Right?

00:45:23.600 --> 00:45:27.680
How does their body posture
indicate their openness to--

00:45:27.680 --> 00:45:31.040
I mean, even to see if you
go for a handshake or a hug,

00:45:31.040 --> 00:45:34.190
you know, there's a lot of
interpersonal communication

00:45:34.190 --> 00:45:34.910
there.

00:45:34.910 --> 00:45:39.090
And then there's this notion
of, through the interaction,

00:45:39.090 --> 00:45:40.820
some sort of exchange
of information.

00:45:40.820 --> 00:45:41.320
Right?

00:45:41.320 --> 00:45:45.770
Even if it's just small talk,
building a relationship,

00:45:45.770 --> 00:45:48.260
and then some sort of indicator
that this conversation

00:45:48.260 --> 00:45:50.360
[? in ?] small talk's not
going to happen all day.

00:45:50.360 --> 00:45:52.010
There is an end to
the interaction.

00:45:52.010 --> 00:45:54.020
Right?

00:45:54.020 --> 00:45:55.785
Siri doesn't have that.

00:45:55.785 --> 00:45:56.660
You're a voice actor.

00:45:56.660 --> 00:46:01.220
You create those dynamics
for actual stories.

00:46:01.220 --> 00:46:02.550
You're working on storytelling.

00:46:02.550 --> 00:46:06.740
How are you thinking about
approaching the construction

00:46:06.740 --> 00:46:08.610
of that evolution?

00:46:08.610 --> 00:46:09.470
Yeah, absolutely.

00:46:09.470 --> 00:46:10.770
So I'll just quickly jump in.

00:46:10.770 --> 00:46:14.030
So I just had a PhD
student defend her thesis

00:46:14.030 --> 00:46:15.722
on exactly this topic.

00:46:15.722 --> 00:46:17.680
As we've been looking at
long-term interaction,

00:46:17.680 --> 00:46:19.097
of course, there
is an opportunity

00:46:19.097 --> 00:46:20.780
to build a relationship.

00:46:20.780 --> 00:46:25.310
And she's been looking
at the elements, based

00:46:25.310 --> 00:46:28.940
on human-human literature,
of how you create, establish,

00:46:28.940 --> 00:46:31.520
and build this sense of rapport
and relationship over time.

00:46:31.520 --> 00:46:33.890
Because we interact with this
robot over multiple encounters.

00:46:33.890 --> 00:46:35.973
You have a series of
interactions of common ground

00:46:35.973 --> 00:46:37.140
that you can refer to.

00:46:37.140 --> 00:46:38.723
And then there is,
like you're saying,

00:46:38.723 --> 00:46:42.560
there's the kind of social
lubrication [INAUDIBLE]

00:46:42.560 --> 00:46:44.210
greeting, knowing
somebody by name,

00:46:44.210 --> 00:46:45.470
referring to something
you're done in the past,

00:46:45.470 --> 00:46:46.460
talking about things
you've done in the future.

00:46:46.460 --> 00:46:46.960
Right?

00:46:46.960 --> 00:46:48.650
So there's a number
of kind of tactics

00:46:48.650 --> 00:46:50.317
that you can design,
with these systems,

00:46:50.317 --> 00:46:52.640
to help build and foster
that relationship.

00:46:52.640 --> 00:46:54.530
The punch line is that--

00:46:54.530 --> 00:46:56.090
and we do this with
young children.

00:46:56.090 --> 00:46:57.680
We're having developed
new measures, in order

00:46:57.680 --> 00:46:58.910
to quantify relationships.

00:46:58.910 --> 00:47:00.350
Because it turns out a
lot of the relationship

00:47:00.350 --> 00:47:02.370
measures tend to be
designed for older children.

00:47:02.370 --> 00:47:04.745
So we've got to redesign them
to support the relationship

00:47:04.745 --> 00:47:05.780
of younger children.

00:47:05.780 --> 00:47:07.940
Children are not confused
that these robots

00:47:07.940 --> 00:47:10.490
are actual people or their
friends or their parents.

00:47:10.490 --> 00:47:13.442
There's been a lot of concern
about this, in the public.

00:47:13.442 --> 00:47:14.900
Children are actually
pretty savvy.

00:47:14.900 --> 00:47:16.442
Even young children
are pretty savvy.

00:47:16.442 --> 00:47:18.620
They see this as a different
kind of relationship,

00:47:18.620 --> 00:47:20.990
with this kind of robot other.

00:47:20.990 --> 00:47:24.290
But, just as we know from
learning interactions,

00:47:24.290 --> 00:47:26.870
the better the relationship
between a child

00:47:26.870 --> 00:47:30.740
and their mentor, teacher,
we are also seeing the better

00:47:30.740 --> 00:47:33.070
the relationship between
the robot learning

00:47:33.070 --> 00:47:35.570
companion and the child results
in better learning outcomes.

00:47:35.570 --> 00:47:36.540
So this is really interesting.

00:47:36.540 --> 00:47:38.390
So again, this goes
quite deep, I think.

00:47:38.390 --> 00:47:43.220
The social-relational connection
and engagement goes quite deep.

00:47:43.220 --> 00:47:46.220
- I'd have to say, even in
relation to the original Siri

00:47:46.220 --> 00:47:46.940
voice--

00:47:46.940 --> 00:47:49.670
which, of course, was-- you
know, you listen back to it

00:47:49.670 --> 00:47:54.680
on the original 4S, and it was
still pretty robotic-sounding.

00:47:54.680 --> 00:47:55.880
And she could be very--

00:47:55.880 --> 00:47:58.400
"snarky," I think, is
probably the best term.

00:47:58.400 --> 00:48:00.980
But after I revealed
myself as the voice,

00:48:00.980 --> 00:48:06.800
I got so much communication
from people, saying, I love you.

00:48:06.800 --> 00:48:08.180
I talk to you all the time.

00:48:08.180 --> 00:48:08.680
[LAUGHTER]

00:48:08.680 --> 00:48:11.810
And it's like, suddenly they
were, like, confusing the human

00:48:11.810 --> 00:48:16.430
with the-- and so that seems to
me to be a big indication that,

00:48:16.430 --> 00:48:19.160
yes, the communication,
the social interaction,

00:48:19.160 --> 00:48:20.720
is very important to humans.

00:48:25.110 --> 00:48:25.860
- Yes.

00:48:25.860 --> 00:48:30.060
So I'm still thinking
a lot about what

00:48:30.060 --> 00:48:33.330
you said about the learning
outcomes and the relationship.

00:48:33.330 --> 00:48:36.870
Because I got my
PhD at the time when

00:48:36.870 --> 00:48:38.960
people thought that we
were no longer going

00:48:38.960 --> 00:48:39.840
to have universities.

00:48:39.840 --> 00:48:41.940
Everyone was going to
sit at their keyboards

00:48:41.940 --> 00:48:43.900
and watch videos and
learn just as well.

00:48:43.900 --> 00:48:44.400
Right?

00:48:44.400 --> 00:48:46.025
We were going to
democratize education.

00:48:46.025 --> 00:48:49.290
It was a beautiful vision,
but what people did not

00:48:49.290 --> 00:48:51.270
realize at that
time was that there

00:48:51.270 --> 00:48:56.880
was no relationship between
you and the recorded speaker,

00:48:56.880 --> 00:49:00.020
without that relationship.

00:49:00.020 --> 00:49:01.800
So it's really
interesting that people

00:49:01.800 --> 00:49:04.230
feel like they have a
relationship with Siri,

00:49:04.230 --> 00:49:07.160
and then they associate
you with Siri.

00:49:07.160 --> 00:49:09.670
- Yeah-- just--
little tidbit of fact.

00:49:09.670 --> 00:49:12.960
The two most common
questions asked of Alexa

00:49:12.960 --> 00:49:15.420
in the first two
years of its existence

00:49:15.420 --> 00:49:17.760
were "are you married?"

00:49:17.760 --> 00:49:19.740
and "I love you."

00:49:19.740 --> 00:49:22.180
which we-- we were very happy,
because we were like, well,

00:49:22.180 --> 00:49:23.430
at least it's not other stuff.

00:49:23.430 --> 00:49:24.550
- Yeah.

00:49:24.550 --> 00:49:25.770
No, they said that to Siri.

00:49:25.770 --> 00:49:26.395
- That's right.

00:49:26.395 --> 00:49:27.450
[LAUGHTER]

00:49:27.450 --> 00:49:29.850
- The snarkiness plays out.

00:49:29.850 --> 00:49:31.950
But yeah, I found it endearing.

00:49:31.950 --> 00:49:34.980
But that that was like-- it
was in a top tier of things

00:49:34.980 --> 00:49:36.180
that people would say.

00:49:36.180 --> 00:49:39.970
Neither of the two were
transactional in nature.

00:49:39.970 --> 00:49:40.470
- Yeah.

00:49:40.470 --> 00:49:43.770
I want to add to what you
were saying, Elena, which is,

00:49:43.770 --> 00:49:47.910
you know, when we look at
areas of human flourishing,

00:49:47.910 --> 00:49:52.302
I think it's really critical
that people need to contribute

00:49:52.302 --> 00:49:53.510
to the flourishing of others.

00:49:53.510 --> 00:49:55.620
So when I design
these technologies,

00:49:55.620 --> 00:49:58.150
I see it as
supporting the humans.

00:49:58.150 --> 00:49:58.650
Right?

00:49:58.650 --> 00:49:59.880
It's supporting the teachers.

00:49:59.880 --> 00:50:02.250
It's supporting the families.

00:50:02.250 --> 00:50:04.980
The robot may become a
practice partner that

00:50:04.980 --> 00:50:06.480
can engage in
practice with a child,

00:50:06.480 --> 00:50:08.938
but the instruction and the
engagement and the relationship

00:50:08.938 --> 00:50:12.288
still has to happen
between a human teacher

00:50:12.288 --> 00:50:13.080
and those children.

00:50:13.080 --> 00:50:15.030
Because I think people
need to feel that they

00:50:15.030 --> 00:50:16.800
matter to other people.

00:50:16.800 --> 00:50:19.950
They may feel they can connect
and have this different kind

00:50:19.950 --> 00:50:22.020
of relationship with
a robot, but we still

00:50:22.020 --> 00:50:23.850
need to matter to people.

00:50:23.850 --> 00:50:26.550
And they still need to
invest time and energy

00:50:26.550 --> 00:50:27.820
in us, and vice versa.

00:50:27.820 --> 00:50:30.120
So I would never want
these technologies

00:50:30.120 --> 00:50:32.730
to interfere with that,
only to support and enhance

00:50:32.730 --> 00:50:35.340
that and, where
possible, potentially

00:50:35.340 --> 00:50:37.650
help to scale that
in an affordable way,

00:50:37.650 --> 00:50:39.890
but never to replace.

00:50:39.890 --> 00:50:41.510
- I wholeheartedly agree.

00:50:41.510 --> 00:50:43.350
[LAUGHTER]

00:50:43.350 --> 00:50:46.500
I'd like to open up
the floor to questions.

00:50:46.500 --> 00:50:50.120
Please, if you have comments,
hold them till after.

00:50:50.120 --> 00:50:53.700
We're interested in questions
we can quickly answer.

00:50:53.700 --> 00:50:55.490
- Thank you very much.

00:50:55.490 --> 00:50:57.500
This is slightly observation.

00:50:57.500 --> 00:51:01.030
And I wonder if you're
familiar with a robot called

00:51:01.030 --> 00:51:04.530
[INAUDIBLE],, which is a
dog robot which doesn't

00:51:04.530 --> 00:51:06.690
have any verbal communication.

00:51:06.690 --> 00:51:08.820
But now that it's a
second generation,

00:51:08.820 --> 00:51:11.500
and first generation,
now, some people

00:51:11.500 --> 00:51:14.160
are so attached
to the [INAUDIBLE]

00:51:14.160 --> 00:51:17.880
that there's a temple
who has funeral for them.

00:51:17.880 --> 00:51:19.500
So without verbal
communication--

00:51:19.500 --> 00:51:20.115
- Oh, humans--

00:51:20.115 --> 00:51:21.570
[INTERPOSING VOICES]

00:51:21.570 --> 00:51:24.340
- --to develop emotional
relationship [? with ?]

00:51:24.340 --> 00:51:27.570
[? this ?] robot, I wonder.

00:51:27.570 --> 00:51:29.430
- So what was the
question, exactly?

00:51:29.430 --> 00:51:31.280
- So, without verbal--

00:51:31.280 --> 00:51:31.780
- Oh.

00:51:31.780 --> 00:51:32.520
- Yeah, sure.

00:51:32.520 --> 00:51:37.170
- --element in the robot,
still people seems to develop--

00:51:37.170 --> 00:51:37.705
- Yes.

00:51:37.705 --> 00:51:38.330
- --attachment.

00:51:38.330 --> 00:51:38.850
- Yeah.

00:51:38.850 --> 00:51:42.420
And I think that that's
part of our responsibility.

00:51:42.420 --> 00:51:46.470
So that's another quote
I often ask people about.

00:51:46.470 --> 00:51:49.027
Which is, you know, "With
great power comes"--

00:51:49.027 --> 00:51:49.860
[INTERPOSING VOICES]

00:51:49.860 --> 00:51:50.360
- Right?

00:51:50.360 --> 00:51:53.730
Spider-Man, Emerson,
Bessie-- that's all you need.

00:51:53.730 --> 00:51:55.710
But that we have
a responsibility

00:51:55.710 --> 00:51:57.270
to realize that
this will happen,

00:51:57.270 --> 00:51:58.830
whether we-- which is
what you ended with.

00:51:58.830 --> 00:51:59.010
Right?

00:51:59.010 --> 00:52:00.750
Whether we intend
it or not, people

00:52:00.750 --> 00:52:04.560
are going to fall in love,
or hate, or personify,

00:52:04.560 --> 00:52:05.900
or anthropomorphize.

00:52:05.900 --> 00:52:06.480
Right?

00:52:06.480 --> 00:52:07.800
That's because we're human.

00:52:07.800 --> 00:52:11.490
And what we don't really
care that it's a robot.

00:52:11.490 --> 00:52:13.710
It will bring that out in us.

00:52:13.710 --> 00:52:15.540
So yeah, I think
it's fascinating.

00:52:15.540 --> 00:52:16.380
Like, we're human!

00:52:16.380 --> 00:52:17.550
This is what will happen.

00:52:17.550 --> 00:52:20.160
But it is our responsibility,
as the creators, the designers,

00:52:20.160 --> 00:52:24.300
and the implementers, to really
take that into consideration

00:52:24.300 --> 00:52:27.720
and do what we can to
protect those people when

00:52:27.720 --> 00:52:28.950
they're using our software.

00:52:28.950 --> 00:52:30.408
- I think another
important element

00:52:30.408 --> 00:52:32.880
that you raised in
your question is,

00:52:32.880 --> 00:52:35.170
you know, this is a very
cultural phenomenon, too.

00:52:35.170 --> 00:52:35.670
Right?

00:52:35.670 --> 00:52:38.100
So how robots and
the relationship

00:52:38.100 --> 00:52:40.170
of people and
robots is perceived

00:52:40.170 --> 00:52:43.490
in the United States
is different than Asia,

00:52:43.490 --> 00:52:44.570
is different than Europe.

00:52:44.570 --> 00:52:44.850
Right?

00:52:44.850 --> 00:52:47.142
So it's just to say, this is
happening within this much

00:52:47.142 --> 00:52:48.955
broader cultural context.

00:52:48.955 --> 00:52:50.580
And so designing for
different cultures

00:52:50.580 --> 00:52:53.082
in a way that's ethical and
appropriate for those cultures

00:52:53.082 --> 00:52:54.790
is also really
interesting and important.

00:52:54.790 --> 00:52:57.870
Especially when you
talk about these more

00:52:57.870 --> 00:53:01.050
voice, nonverbal,
communicating entities,

00:53:01.050 --> 00:53:02.840
there is a lot of
cultural differences.

00:53:02.840 --> 00:53:06.180
That's like, do you want that
AI to feel like a foreigner,

00:53:06.180 --> 00:53:07.540
in your home, or a native?

00:53:07.540 --> 00:53:08.040
Right?

00:53:08.040 --> 00:53:10.900
So again, these things
get quite sophisticated,

00:53:10.900 --> 00:53:13.070
when you think about
different cultures.

00:53:13.070 --> 00:53:13.780
- Thank you.

00:53:13.780 --> 00:53:14.280
- Hi.

00:53:14.280 --> 00:53:15.600
My name is Amanda.

00:53:15.600 --> 00:53:18.600
I've been thinking a lot about
the intersections of technology

00:53:18.600 --> 00:53:22.560
and gender, especially with
respect to this conference.

00:53:22.560 --> 00:53:25.430
I was really affected
by a book called

00:53:25.430 --> 00:53:27.140
Algorithms of Oppression.

00:53:27.140 --> 00:53:27.800
Yeah.

00:53:27.800 --> 00:53:29.842
- I almost quoted her,
and I had to [INAUDIBLE]..

00:53:29.842 --> 00:53:32.930
- [LAUGHS] Well, I was really
affected by the part where--

00:53:32.930 --> 00:53:36.020
I am having a lot of
questions about my identity--

00:53:36.020 --> 00:53:38.030
gender identity, et cetera.

00:53:38.030 --> 00:53:42.200
And when I do go to use a
search engine like Google,

00:53:42.200 --> 00:53:45.865
I am typing in things-- or
not typing in things, rather--

00:53:45.865 --> 00:53:47.990
because I know I'm going
to get a particular result

00:53:47.990 --> 00:53:49.580
if I Google a particular thing.

00:53:49.580 --> 00:53:52.340
Like, if I went to Google,
like, "gay Asian woman,"

00:53:52.340 --> 00:53:54.440
I know that the result
I'm going get back

00:53:54.440 --> 00:53:56.300
is going to be
pornographic, and so I

00:53:56.300 --> 00:53:58.100
choose not to search that up.

00:53:58.100 --> 00:54:00.410
Which means that I'm
lacking a whole, like,

00:54:00.410 --> 00:54:03.385
bout of information that
I would otherwise get.

00:54:03.385 --> 00:54:04.760
And I bring this
up to my friends

00:54:04.760 --> 00:54:05.970
who are in computer science.

00:54:05.970 --> 00:54:08.137
I'm someone who's very
mystified by technology,

00:54:08.137 --> 00:54:10.220
and so I don't have the
understanding of what even

00:54:10.220 --> 00:54:12.590
"hard-coding" means.

00:54:12.590 --> 00:54:14.150
But I'm curious
if you could talk

00:54:14.150 --> 00:54:17.660
about the role of individual
companies and their ethics

00:54:17.660 --> 00:54:20.300
in creating these sort
of knowledge systems.

00:54:20.300 --> 00:54:23.900
And then how you imagine we
can teach computer science,

00:54:23.900 --> 00:54:26.870
both to university-age
students but also young kids,

00:54:26.870 --> 00:54:28.550
in a way that seems
to foster this sort

00:54:28.550 --> 00:54:30.065
of ethical sensibility.

00:54:30.065 --> 00:54:31.720
- Yeah, so I can
start by taking that,

00:54:31.720 --> 00:54:33.840
because I'm working
directly in this area.

00:54:33.840 --> 00:54:37.970
So the punch line
is, the field has

00:54:37.970 --> 00:54:41.840
started to recognize that
the data that's just kind

00:54:41.840 --> 00:54:46.280
of acquired from a
general population

00:54:46.280 --> 00:54:49.290
is holding the biases
of that population.

00:54:49.290 --> 00:54:52.040
And so when you train a
system with that data,

00:54:52.040 --> 00:54:53.810
you don't get an
aspirational system.

00:54:53.810 --> 00:54:56.090
You get a system that
kind of reinforces

00:54:56.090 --> 00:54:57.470
the biases that already exist.

00:54:57.470 --> 00:54:59.870
And so you have to be
very intentional in how

00:54:59.870 --> 00:55:02.965
you design the data set to
be fair and representative.

00:55:02.965 --> 00:55:05.090
And now there's conferences
like FAT, for Fairness,

00:55:05.090 --> 00:55:06.298
Accountability, Transparency.

00:55:06.298 --> 00:55:09.590
That's just recognizing that, in
this world of data and machine

00:55:09.590 --> 00:55:12.350
learning, the ethics
of these things

00:55:12.350 --> 00:55:15.860
and how you design them to
behave in fair and ethical ways

00:55:15.860 --> 00:55:17.490
is very, very important.

00:55:17.490 --> 00:55:19.820
So I think there's much,
much more attention being

00:55:19.820 --> 00:55:21.140
applied to that.

00:55:21.140 --> 00:55:27.740
At MIT, I'm leading an effort on
a K-12 AI education initiative.

00:55:27.740 --> 00:55:30.500
And the reason why
is simply, as long

00:55:30.500 --> 00:55:34.880
as these technologies are only
designed by elites for elites,

00:55:34.880 --> 00:55:37.730
we run the risk that it's going
to accelerate the prosperity

00:55:37.730 --> 00:55:39.420
divide rather than close it.

00:55:39.420 --> 00:55:40.670
So you need to democratize it.

00:55:40.670 --> 00:55:42.420
And you democratize
through education.

00:55:42.420 --> 00:55:44.420
So this is the big reason
why we're championing,

00:55:44.420 --> 00:55:46.130
at MIT, this K-12 AI education.

00:55:46.130 --> 00:55:49.310
One of the innovative aspects of
this curriculum we're designing

00:55:49.310 --> 00:55:51.230
is, it's a
constructionist, hands-on,

00:55:51.230 --> 00:55:52.430
you're learning by doing--

00:55:52.430 --> 00:55:55.850
children, as designers
of these AI systems,

00:55:55.850 --> 00:55:57.680
in order to learn the concepts.

00:55:57.680 --> 00:56:01.580
And baked into that, inherently,
is the ethical design decisions

00:56:01.580 --> 00:56:04.130
and trade-offs that
we make every day,

00:56:04.130 --> 00:56:05.780
as designers of these systems.

00:56:05.780 --> 00:56:08.865
So they're mindful and aware
of these kinds of questions.

00:56:08.865 --> 00:56:10.490
So I think that's
very, very important.

00:56:10.490 --> 00:56:11.870
It's not enough for
us to kind of assume

00:56:11.870 --> 00:56:14.250
other people will kind of
"take care of the ethics."

00:56:14.250 --> 00:56:16.208
I think we need to train
a whole new generation

00:56:16.208 --> 00:56:18.980
of technical people that
are also aware and sensitive

00:56:18.980 --> 00:56:20.390
to these issues.

00:56:20.390 --> 00:56:22.310
- We definitely need
to figure out a way

00:56:22.310 --> 00:56:25.070
that we can go beyond
just all of the data

00:56:25.070 --> 00:56:27.225
that they think they
are collecting about us.

00:56:27.225 --> 00:56:29.100
Because they're collecting
data all the time.

00:56:29.100 --> 00:56:32.720
And unfortunately, it shows
right up on our social media.

00:56:32.720 --> 00:56:35.210
Suddenly we're getting
advertisements for things.

00:56:35.210 --> 00:56:36.757
Or you say something--

00:56:36.757 --> 00:56:38.840
You say something in front
of Alexa, for instance.

00:56:38.840 --> 00:56:40.160
You're talking about balloons.

00:56:40.160 --> 00:56:42.235
Suddenly, on your
phone, oh, here

00:56:42.235 --> 00:56:43.610
are some balloons
for you to buy.

00:56:43.610 --> 00:56:45.892
And it's like, OK.

00:56:45.892 --> 00:56:47.600
So, yeah, I think
we've got a ways to go.

00:56:47.600 --> 00:56:48.550
[LAUGHS]

00:56:48.550 --> 00:56:49.610
- Just to--

00:56:49.610 --> 00:56:52.852
I am a big proponent of
identifying the problem.

00:56:52.852 --> 00:56:53.810
I think that's awesome.

00:56:53.810 --> 00:56:56.660
The big thing I would
love to encourage you all

00:56:56.660 --> 00:57:00.260
to think about, and encourage
your technical friends to think

00:57:00.260 --> 00:57:02.450
about, is that it
actually takes action.

00:57:02.450 --> 00:57:05.510
And right now it is very
much the individual actions

00:57:05.510 --> 00:57:07.610
of individual devs
going, [INHALE]

00:57:07.610 --> 00:57:09.240
this is probably
not a good thing.

00:57:09.240 --> 00:57:10.340
We should change this.

00:57:10.340 --> 00:57:13.880
And it really takes
leadership of these, like--

00:57:13.880 --> 00:57:14.930
Satya Nadella?

00:57:14.930 --> 00:57:15.770
That's the reason I
want to Microsoft.

00:57:15.770 --> 00:57:17.660
Everyone's like, you were
having a great time at Alexa!

00:57:17.660 --> 00:57:20.270
I'm like, I wanted to be part
of a company that cares about,

00:57:20.270 --> 00:57:22.130
like, the planet!

00:57:22.130 --> 00:57:24.230
And I went there because
he was a leader that

00:57:24.230 --> 00:57:27.080
decided to stand up and
say, I will trade off money

00:57:27.080 --> 00:57:28.220
to do the right thing.

00:57:28.220 --> 00:57:31.220
And it's very hard to do
that, but it takes us saying,

00:57:31.220 --> 00:57:33.620
yes, we hear you, and
yes, that's a good idea--

00:57:33.620 --> 00:57:35.060
for that to be perpetual.

00:57:35.060 --> 00:57:35.780
- Yes.

00:57:35.780 --> 00:57:36.280
- But yeah.

00:57:36.280 --> 00:57:37.988
So I'd love to talk--
we can have coffee.

00:57:37.988 --> 00:57:39.070
But that's a huge topic.

00:57:39.070 --> 00:57:41.240
But we have to take
action in our companies,

00:57:41.240 --> 00:57:42.330
to make that possible.

00:57:42.330 --> 00:57:43.550
Thank you-- great question.

00:57:43.550 --> 00:57:45.327
Got all of us going.

00:57:45.327 --> 00:57:46.160
- My name is Nicole.

00:57:46.160 --> 00:57:48.078
I'm a student in
religion and gender.

00:57:48.078 --> 00:57:50.120
And I think my question
really fits well with how

00:57:50.120 --> 00:57:51.770
the previous questions went.

00:57:51.770 --> 00:57:52.910
And it's an ethical one.

00:57:52.910 --> 00:57:56.480
So, while AI is considered
the cutting edge of technology

00:57:56.480 --> 00:57:58.550
and the advancement
of our society,

00:57:58.550 --> 00:58:01.520
the fact that the majority
of AI are designed as female,

00:58:01.520 --> 00:58:03.020
using female voices--

00:58:03.020 --> 00:58:05.360
and, of course, we have the
first female voice of Siri

00:58:05.360 --> 00:58:06.830
here with us--

00:58:06.830 --> 00:58:08.960
it allows a constant
re-inscription

00:58:08.960 --> 00:58:12.080
of the role of women as
servile, subservient,

00:58:12.080 --> 00:58:16.580
obligated to serve, being
commanded and controlled.

00:58:16.580 --> 00:58:19.610
So these constricted,
dominated, "embodied"--

00:58:19.610 --> 00:58:23.970
in quotes-- female constructed
personalities in the AI,

00:58:23.970 --> 00:58:29.500
they're perpetually limiting
women to being requested of

00:58:29.500 --> 00:58:30.900
and servile.

00:58:30.900 --> 00:58:32.910
And it's just reinforcing
our stereotype

00:58:32.910 --> 00:58:34.980
of our expectations of females.

00:58:34.980 --> 00:58:38.335
So how can we be cutting-edge
and promoting gender equity,

00:58:38.335 --> 00:58:40.710
and how can we be a little
more critical in the analysis,

00:58:40.710 --> 00:58:44.040
today, and kind of look at
that as an ethical question,

00:58:44.040 --> 00:58:44.720
as well?

00:58:44.720 --> 00:58:46.450
And thank you to our panelists.

00:58:46.450 --> 00:58:46.950
- Thank you.

00:58:46.950 --> 00:58:48.720
I'll just take a
super-30-second stab,

00:58:48.720 --> 00:58:52.440
because we want to at least
get to some other questions.

00:58:52.440 --> 00:58:56.160
Another reason I left or went
to Microsoft was because they

00:58:56.160 --> 00:58:59.370
created this concept of
developing for voice,

00:58:59.370 --> 00:59:02.830
and that they wanted to create
actually gender-neutral--

00:59:02.830 --> 00:59:05.520
they built a neural network to
create gender-neutral voices--

00:59:05.520 --> 00:59:06.770
which is very similar to Jibo.

00:59:06.770 --> 00:59:08.187
Like, when you
listen to Jibo, you

00:59:08.187 --> 00:59:12.150
can't really tell whether it's
a man or woman or whatever.

00:59:12.150 --> 00:59:14.610
But again, the onus is on,
of course, the developer

00:59:14.610 --> 00:59:16.470
to choose to use
that technology.

00:59:16.470 --> 00:59:17.600
But there are very--

00:59:17.600 --> 00:59:20.220
I mean, Amazon and
Microsoft are both

00:59:20.220 --> 00:59:23.340
investing heavily in building
a space that's not just--

00:59:23.340 --> 00:59:25.230
unfortunately, Alexa's
in 10 million homes.

00:59:25.230 --> 00:59:27.450
But how can we as
businesses now build

00:59:27.450 --> 00:59:30.210
voice services on our side
that are gender-neutral?

00:59:30.210 --> 00:59:32.280
- Well, there are choices, too.

00:59:32.280 --> 00:59:35.280
It used to just be, you
would get Siri, female voice,

00:59:35.280 --> 00:59:36.210
at the very beginning.

00:59:36.210 --> 00:59:38.400
And now there are lots
of different voices,

00:59:38.400 --> 00:59:40.190
including a lot of
different male voices.

00:59:40.190 --> 00:59:41.250
So.

00:59:41.250 --> 00:59:42.550
- And, I mean, just quickly--

00:59:42.550 --> 00:59:45.510
so, in the two robots I showed--

00:59:45.510 --> 00:59:47.580
so Jibo is actually
designed to be a character.

00:59:47.580 --> 00:59:49.440
He is actually male character.

00:59:49.440 --> 00:59:54.140
And he is a male character
who's helpful and useful.

00:59:54.140 --> 00:59:56.430
And so that was an
intentional design decision.

00:59:56.430 --> 01:00:01.770
[INAUDIBLE] was actually
designed to not explicitly

01:00:01.770 --> 01:00:03.940
define a gender for the robot.

01:00:03.940 --> 01:00:04.440
You know?

01:00:04.440 --> 01:00:05.857
And if you look
at [INAUDIBLE],, I

01:00:05.857 --> 01:00:09.290
don't know if you would
ascribe male or female.

01:00:09.290 --> 01:00:11.040
In the relational work
we did, we actually

01:00:11.040 --> 01:00:13.050
started asking the children,
after you've interacted

01:00:13.050 --> 01:00:14.932
with [INAUDIBLE] for
three months, it's like,

01:00:14.932 --> 01:00:16.140
do you think it has a gender?

01:00:16.140 --> 01:00:17.848
And again, it's
interesting, because that

01:00:17.848 --> 01:00:20.430
was a case of intentionally
designed not to have a gender,

01:00:20.430 --> 01:00:22.740
and yet children would ascribe--

01:00:22.740 --> 01:00:24.120
most children would ascribe--

01:00:24.120 --> 01:00:25.230
a gender.

01:00:25.230 --> 01:00:27.420
Some children were not
sure of the gender,

01:00:27.420 --> 01:00:29.070
but actually most
of the children

01:00:29.070 --> 01:00:30.720
ascribed a male
gender to the robot.

01:00:30.720 --> 01:00:32.790
So again, I think
as human beings

01:00:32.790 --> 01:00:35.610
we have a tendency to
want to ascribe a gender,

01:00:35.610 --> 01:00:38.410
whether it's intentionally
designed for or not.

01:00:38.410 --> 01:00:43.020
But I do think that another
issue we need to be mindful of

01:00:43.020 --> 01:00:45.540
is that there are
potential for more

01:00:45.540 --> 01:00:47.340
kind of traditional
gender-based dynamics,

01:00:47.340 --> 01:00:49.860
potentially, between
these kinds of, you know,

01:00:49.860 --> 01:00:52.530
people with these kind of
gendered systems, as well.

01:00:52.530 --> 01:00:54.355
And so there's a
beginning literature

01:00:54.355 --> 01:00:56.105
around that in the
research context trying

01:00:56.105 --> 01:00:57.750
to actually study that.

01:00:57.750 --> 01:01:02.460
But it's an important
design decision,

01:01:02.460 --> 01:01:05.340
and there's implications
to those design decisions.

01:01:05.340 --> 01:01:08.550
- I'm getting some angry
looks from the front row.

01:01:08.550 --> 01:01:11.880
So can we have one
more quick question?

01:01:11.880 --> 01:01:13.340
And we'll have a quick response.

01:01:13.340 --> 01:01:17.183
- OK, I just want to say, this
gender of nonhuman objects--

01:01:17.183 --> 01:01:18.600
I'm an animal
behaviorist-- people

01:01:18.600 --> 01:01:20.310
do the same thing for animals.

01:01:20.310 --> 01:01:22.080
Whenever they see a
squirrel, it's always

01:01:22.080 --> 01:01:23.440
he, not she, as if all the--

01:01:23.440 --> 01:01:25.190
you know, the female
[? squirrel's home ?]

01:01:25.190 --> 01:01:26.820
[? baking ?] cookies.

01:01:26.820 --> 01:01:28.320
And the same thing--

01:01:28.320 --> 01:01:32.820
I've also been upset about,
a lot of anthropoid robots

01:01:32.820 --> 01:01:35.360
are white.

01:01:35.360 --> 01:01:37.440
And I study the evolution
of the human face,

01:01:37.440 --> 01:01:39.450
and they also have
Caucasian noses.

01:01:39.450 --> 01:01:40.560
And what's that about?

01:01:40.560 --> 01:01:42.810
Is that being addressed,
in your industry?

01:01:42.810 --> 01:01:44.760
And finally, what I
really came up here to say

01:01:44.760 --> 01:01:49.380
was that social learning,
social interaction,

01:01:49.380 --> 01:01:52.423
is a massive driver
of increasing

01:01:52.423 --> 01:01:53.340
the speed of learning.

01:01:53.340 --> 01:01:58.200
And that's been shown in
birdsong, in artificial ape

01:01:58.200 --> 01:01:59.640
language.

01:01:59.640 --> 01:02:01.560
The bonobo Kanzi
was the first ape

01:02:01.560 --> 01:02:04.590
to really spontaneously
learn a language only

01:02:04.590 --> 01:02:09.390
by watching his adoptive
mother fail to be taught.

01:02:09.390 --> 01:02:13.320
And finally, of course,
Irene Pepperberg's Alex only

01:02:13.320 --> 01:02:16.620
reached the heights
of cognitive ability

01:02:16.620 --> 01:02:18.600
through the model/rival
method, which

01:02:18.600 --> 01:02:22.725
was watching another parrot
compete for the same treats.

01:02:22.725 --> 01:02:25.350
And so [INAUDIBLE]---- I don't
want [INAUDIBLE]---- just, yeah,

01:02:25.350 --> 01:02:27.240
social learning in
humans is important,

01:02:27.240 --> 01:02:29.225
but it's, this is deep
evolutionary roots.

01:02:29.225 --> 01:02:30.600
So I just want to
point that out.

01:02:30.600 --> 01:02:30.890
- Absolutely.

01:02:30.890 --> 01:02:31.440
- Thank you!

01:02:31.440 --> 01:02:32.400
- Absolutely.

01:02:32.400 --> 01:02:32.900
Thank you.

01:02:32.900 --> 01:02:33.960
- Yes, go ahead--

01:02:33.960 --> 01:02:34.600
last one.

01:02:34.600 --> 01:02:35.100
Oh!

01:02:35.100 --> 01:02:36.170
Uh, OK, oh, all right.

01:02:36.170 --> 01:02:37.520
[INTERPOSING VOICES]

01:02:37.520 --> 01:02:39.210
- --never be embarrassed.

01:02:39.210 --> 01:02:40.795
- OK, I'm leaving.

01:02:40.795 --> 01:02:43.080
- We'll continue
the conversation,

01:02:43.080 --> 01:02:44.490
I think, over lunch!

01:02:44.490 --> 01:02:46.290
[INAUDIBLE]

01:02:46.290 --> 01:02:50.840
[APPLAUSE]

