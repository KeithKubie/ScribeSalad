WEBVTT
Kind: captions
Language: en

00:00:08.550 --> 00:00:10.840
SUSANNA: Authors at Google
today is very pleased

00:00:10.840 --> 00:00:13.090
to invite David Mindell.

00:00:13.090 --> 00:00:15.150
David Mindell is
the Dibner professor

00:00:15.150 --> 00:00:18.750
of the history of engineering
and manufacturing at MIT.

00:00:18.750 --> 00:00:21.490
He has 25 years of
experience as an engineer

00:00:21.490 --> 00:00:24.420
in the field of undersea
robotic exploration

00:00:24.420 --> 00:00:29.090
as a veteran of more than 30
oceanographic expeditions,

00:00:29.090 --> 00:00:31.650
and more recently, as an
airplane pilot and engineer

00:00:31.650 --> 00:00:33.370
of autonomous aircraft.

00:00:33.370 --> 00:00:36.040
He is the award winning
author of "Iron Coffin-- War,

00:00:36.040 --> 00:00:39.400
Technology, and Experience
aboard the USS Monitor,"

00:00:39.400 --> 00:00:42.400
and "Digital Apollo-- Human
and Machine in Spaceflight."

00:00:42.400 --> 00:00:45.400
And his most recent book,
"Our Robots, Ourselves"

00:00:45.400 --> 00:00:48.820
was published by Viking on
October 13 of this year.

00:00:48.820 --> 00:00:51.020
Please join me in
welcoming David Mindell.

00:00:51.020 --> 00:00:56.031
[APPLAUSE]

00:00:56.031 --> 00:00:58.530
DAVID MINDELL: Thank you Susanna
for that nice introduction.

00:00:58.530 --> 00:01:00.810
It's a pleasure to be here.

00:01:00.810 --> 00:01:03.170
And I'm going to talk
about my new book,

00:01:03.170 --> 00:01:07.920
"Our Robots, Ourselves."

00:01:07.920 --> 00:01:11.750
I come to this book sort
of out of the experience

00:01:11.750 --> 00:01:14.900
of my previous book, which
was called "Digital Apollo."

00:01:14.900 --> 00:01:17.240
And "Digital Apollo"
was about the computers

00:01:17.240 --> 00:01:20.980
and the software that were used
inside both the command module

00:01:20.980 --> 00:01:23.350
and the lunar module for
the Apollo lunar landings

00:01:23.350 --> 00:01:25.309
in the '60s, how
they were designed,

00:01:25.309 --> 00:01:26.350
how they were engineered.

00:01:26.350 --> 00:01:28.800
It was really the first
embedded computer.

00:01:28.800 --> 00:01:31.100
It was certainly
the first computer

00:01:31.100 --> 00:01:34.820
that software became
central to human life

00:01:34.820 --> 00:01:39.880
and was life critical software,
and one of the first real time

00:01:39.880 --> 00:01:43.370
control computers, and the first
digital fly by wire system.

00:01:43.370 --> 00:01:47.410
And you can see in this image
over on the right, which

00:01:47.410 --> 00:01:51.590
is the cover image-- it was made
actually by John Knoll who you

00:01:51.590 --> 00:01:55.530
may know from Industrial Light
&amp; Magic-- and a little bit more

00:01:55.530 --> 00:01:57.340
clearly presented here.

00:01:57.340 --> 00:02:01.700
The book focuses on this kind of
climactic moment in the Apollo

00:02:01.700 --> 00:02:05.240
11 lunar landing where
the mythology went,

00:02:05.240 --> 00:02:07.980
Armstrong reaches
up and turns off

00:02:07.980 --> 00:02:13.400
the computer the last minute
and lands the spacecraft by hand

00:02:13.400 --> 00:02:15.520
to avoid this crater
that you can see there

00:02:15.520 --> 00:02:17.410
out the window, West crater.

00:02:17.410 --> 00:02:20.020
And the book sort of takes
that moment as a starting point

00:02:20.020 --> 00:02:22.610
for why would he turn
off the computer,

00:02:22.610 --> 00:02:24.530
and why was that important?

00:02:24.530 --> 00:02:27.640
And now it turns out that he
didn't turn off the computer.

00:02:27.640 --> 00:02:32.440
He turned it from a fairly
highly automated targeting

00:02:32.440 --> 00:02:34.640
mode that kind of allowed
him a kind of cursor

00:02:34.640 --> 00:02:37.870
control around the moon
to a still fairly highly

00:02:37.870 --> 00:02:40.480
semi-automated
mode, attitude hold

00:02:40.480 --> 00:02:42.320
in his right hand,
rate of descent

00:02:42.320 --> 00:02:44.010
with a switch in his left hand.

00:02:44.010 --> 00:02:46.820
Still very much a fly by
wire kind of automated mode.

00:02:46.820 --> 00:02:49.930
That's actually not
so far from how pilots

00:02:49.930 --> 00:02:52.247
fly Airbus airliners today.

00:02:52.247 --> 00:02:53.580
He didn't turn off the computer.

00:02:53.580 --> 00:02:56.290
He moved it to a different
level of automation

00:02:56.290 --> 00:02:59.750
to suit what he felt was
the situation at the time.

00:02:59.750 --> 00:03:02.880
And this was a very interesting
moment because I learned,

00:03:02.880 --> 00:03:04.880
in the course of writing
this book, at the time,

00:03:04.880 --> 00:03:09.800
the Soviet spacecraft were
controlled by analog computers.

00:03:09.800 --> 00:03:11.510
And they were very
highly automated.

00:03:11.510 --> 00:03:15.010
They left very little discretion
and role for the astronauts.

00:03:15.010 --> 00:03:17.170
The American computer
was a general purpose

00:03:17.170 --> 00:03:19.930
digital computer, one of the
first integrated circuits--

00:03:19.930 --> 00:03:21.130
uses of integrated circuits.

00:03:21.130 --> 00:03:23.790
In fact, this
computer consumed 60%

00:03:23.790 --> 00:03:25.990
of the national output
of integrated circuits

00:03:25.990 --> 00:03:29.780
for a couple years
during the '60s.

00:03:29.780 --> 00:03:32.540
So it was a very advanced,
forward looking thing to do,

00:03:32.540 --> 00:03:34.880
including all the
complexities of the software.

00:03:34.880 --> 00:03:36.970
And yet all that
advanced technology

00:03:36.970 --> 00:03:39.270
did not make the
machine more automated.

00:03:39.270 --> 00:03:44.380
It just made it automated in a
more nuanced, sophisticated way

00:03:44.380 --> 00:03:48.160
that gave the human better
control over the spacecraft.

00:03:48.160 --> 00:03:50.520
And that gave me this
idea, which I then

00:03:50.520 --> 00:03:53.480
pursued throughout
this new book,

00:03:53.480 --> 00:03:56.990
that often the highest
level of automation--

00:03:56.990 --> 00:03:59.030
we talk about levels
of automation--

00:03:59.030 --> 00:04:01.460
is not necessarily
full autonomy.

00:04:01.460 --> 00:04:03.140
And the most
difficult challenging

00:04:03.140 --> 00:04:06.310
technological problem
is not full autonomy,

00:04:06.310 --> 00:04:09.120
but rather what I've come
to call a perfect five.

00:04:11.710 --> 00:04:14.310
If you think about level
one as fully manual,

00:04:14.310 --> 00:04:19.620
level 10 as fully
automated, the perfect five

00:04:19.620 --> 00:04:21.990
is really the most
complicated, difficult,

00:04:21.990 --> 00:04:25.940
and I think also socially
and financially rewarding

00:04:25.940 --> 00:04:30.660
place to have automated systems
where the human is in control.

00:04:30.660 --> 00:04:34.110
There's trusted,
transparent autonomy.

00:04:34.110 --> 00:04:37.820
And the system can be moved
up and down various levels,

00:04:37.820 --> 00:04:39.730
turning things on,
turning things off,

00:04:39.730 --> 00:04:43.080
in order to suit the
particular situation.

00:04:43.080 --> 00:04:46.290
And what you see through
the Apollo story,

00:04:46.290 --> 00:04:48.800
and many of the stories in the
book is that a lot of systems

00:04:48.800 --> 00:04:53.240
start out in the
engineering phase

00:04:53.240 --> 00:04:55.070
as imagining full autonomy.

00:04:55.070 --> 00:04:57.460
The engineers on
the Apollo computer

00:04:57.460 --> 00:05:00.010
thought there would only be
two buttons on the interface.

00:05:00.010 --> 00:05:04.065
One would be go to moon, and
one would be take me home.

00:05:04.065 --> 00:05:05.690
And instead of course
what you ended up

00:05:05.690 --> 00:05:10.300
with was this very rich,
very carefully designed

00:05:10.300 --> 00:05:12.790
mix of instruments and controls.

00:05:12.790 --> 00:05:16.030
As these systems frequently,
time and time again,

00:05:16.030 --> 00:05:18.420
move from laboratory
to field, there

00:05:18.420 --> 00:05:20.630
are human interventions
and human controls

00:05:20.630 --> 00:05:24.350
put in at critical moments.

00:05:24.350 --> 00:05:26.490
So again, I've been talking
about the perfect five.

00:05:26.490 --> 00:05:29.040
So the subtitle of the book
is "The Myths of Autonomy."

00:05:29.040 --> 00:05:31.180
I want to read you a
little bit from chapter one

00:05:31.180 --> 00:05:34.450
about what those myths are.

00:05:34.450 --> 00:05:37.110
First there's the myth
of linear progress,

00:05:37.110 --> 00:05:40.990
the idea that technology evolves
from direct human involvement

00:05:40.990 --> 00:05:44.940
to remote presence, and then
to fully autonomous robots.

00:05:44.940 --> 00:05:47.300
Political scientist
Peter Singer--

00:05:47.300 --> 00:05:49.960
you may be familiar with
his book "Wired for War"--

00:05:49.960 --> 00:05:52.970
epitomizes this pathology
when he writes that quote,

00:05:52.970 --> 00:05:55.540
"this concept of keeping the
human in the loop is already

00:05:55.540 --> 00:05:59.080
being eroded by both
policymakers and the technology

00:05:59.080 --> 00:06:02.470
itself, which are both moving
rapidly toward pushing humans

00:06:02.470 --> 00:06:04.640
out of the loop," unquote.

00:06:04.640 --> 00:06:06.360
But there's no
evidence to suggest

00:06:06.360 --> 00:06:08.510
that this is a
natural evolution,

00:06:08.510 --> 00:06:11.720
that the technology
itself, as Singer puts it,

00:06:11.720 --> 00:06:12.707
does any such thing.

00:06:12.707 --> 00:06:14.540
In fact, there's good
evidence-- a lot of it

00:06:14.540 --> 00:06:17.580
is presented in this book-- that
people are moving into deeper

00:06:17.580 --> 00:06:19.990
intimacy with their machines.

00:06:19.990 --> 00:06:21.770
Second is the myth
of replacement,

00:06:21.770 --> 00:06:27.220
the idea that machines take
over human jobs one for one.

00:06:27.220 --> 00:06:30.250
But human factors, researchers,
and cognitive systems engineers

00:06:30.250 --> 00:06:33.530
have found that really
does automation simply

00:06:33.530 --> 00:06:35.610
mechanize a human task.

00:06:35.610 --> 00:06:38.280
Rather, it tends to make
the task more complex,

00:06:38.280 --> 00:06:42.056
often increases the workload,
or at least shifts it around.

00:06:42.056 --> 00:06:43.930
And finally, as I
mentioned, we have the myth

00:06:43.930 --> 00:06:46.690
of full autonomy,
the Utopian idea that

00:06:46.690 --> 00:06:50.030
robots today and in the
future should operate entirely

00:06:50.030 --> 00:06:51.330
on their own.

00:06:51.330 --> 00:06:53.350
Yes, automation
can certainly take

00:06:53.350 --> 00:06:55.210
on parts of tests
that were previously

00:06:55.210 --> 00:06:56.700
accomplished by humans.

00:06:56.700 --> 00:06:58.840
Machines do act on
their own in response

00:06:58.840 --> 00:07:02.130
to their environments for
certain periods of time.

00:07:02.130 --> 00:07:04.570
But the machine that operates
entirely independently

00:07:04.570 --> 00:07:07.880
of human direction
is a useless machine.

00:07:07.880 --> 00:07:11.090
And I used to say that only
a rock is truly autonomous.

00:07:11.090 --> 00:07:12.940
But then my geologist
friends reminded me

00:07:12.940 --> 00:07:17.080
that even rocks are shaped and
formed by their environments.

00:07:17.080 --> 00:07:20.550
Automation changes the type
of human involvement required,

00:07:20.550 --> 00:07:23.700
transforms it, but
does not eliminate it.

00:07:23.700 --> 00:07:26.250
For any apparently
autonomous system,

00:07:26.250 --> 00:07:29.260
you can always find the
wrapper of human control

00:07:29.260 --> 00:07:33.212
that makes it useful and
returns meaningful data.

00:07:33.212 --> 00:07:34.920
The questions that
I'm interested in then

00:07:34.920 --> 00:07:38.150
are not manned versus
unmanned, human control

00:07:38.150 --> 00:07:42.320
versus autonomous, but
rather where are the people?

00:07:42.320 --> 00:07:43.880
Which people are they?

00:07:43.880 --> 00:07:46.660
What are they doing, and
when are they doing it?

00:07:46.660 --> 00:07:51.810
And so you can trace through
networks of-- in some sense,

00:07:51.810 --> 00:07:57.300
any programming task
is a kind of placing

00:07:57.300 --> 00:07:59.350
of human intention,
and human design,

00:07:59.350 --> 00:08:02.500
and human views of the world
into a piece of software that

00:08:02.500 --> 00:08:06.740
is later executed
at some future date.

00:08:06.740 --> 00:08:11.630
So the book covers four
extreme environments.

00:08:11.630 --> 00:08:14.710
And the idea is that in these
extreme environments like space

00:08:14.710 --> 00:08:17.900
flight, people have been
forced to use robotics for 30

00:08:17.900 --> 00:08:19.779
or 40 years because,
in many cases,

00:08:19.779 --> 00:08:21.320
it was the only way
to do a job where

00:08:21.320 --> 00:08:23.130
people couldn't physically be.

00:08:23.130 --> 00:08:25.440
And we can look at
those environments

00:08:25.440 --> 00:08:29.070
and see something about
our robotic future

00:08:29.070 --> 00:08:31.870
in more ordinary
environments like automobiles

00:08:31.870 --> 00:08:33.870
and other aspects of daily life.

00:08:33.870 --> 00:08:37.260
So I begin with the deep ocean
which is where my career began.

00:08:37.260 --> 00:08:42.970
And the move from scientists
visiting the seafloor

00:08:42.970 --> 00:08:45.640
in submarines-- you may
be familiar with Alvin,

00:08:45.640 --> 00:08:48.750
it was a three man submarine
operated by the Oceanographic

00:08:48.750 --> 00:08:50.330
Institute at Woods
Hole where I used

00:08:50.330 --> 00:08:53.400
to work-- toward remote robots.

00:08:53.400 --> 00:08:55.470
And this was the
evolution that I

00:08:55.470 --> 00:08:58.060
was involved in starting
in the late '80s,

00:08:58.060 --> 00:08:59.740
moving into the 1990s.

00:08:59.740 --> 00:09:02.540
And we were all convinced
that the world was moving

00:09:02.540 --> 00:09:05.130
from human, to
remote, to autonomous.

00:09:05.130 --> 00:09:08.370
There was a great deal of
tension over that issue.

00:09:08.370 --> 00:09:12.020
This image appeared in
National Geographic in 1981

00:09:12.020 --> 00:09:15.260
in an article written by my
mentor in that field, Dr.

00:09:15.260 --> 00:09:18.530
Robert Ballard, you may know as
the discoverer of the Titanic.

00:09:18.530 --> 00:09:22.050
And in this article, Ballard
laid out this new vision

00:09:22.050 --> 00:09:25.930
for exploring the seafloor which
was remote or telepresence.

00:09:25.930 --> 00:09:29.650
And you can see there's now
a traditional oceanographic

00:09:29.650 --> 00:09:33.330
ship-- can you see
that out there--

00:09:33.330 --> 00:09:35.350
traditional oceanographic
ship on the surface,

00:09:35.350 --> 00:09:37.990
a fiber optic cable,
itself a rather

00:09:37.990 --> 00:09:40.080
new technology in the 1980s.

00:09:40.080 --> 00:09:45.390
Then a kind of basic towed
sled called Argo in this case,

00:09:45.390 --> 00:09:47.310
and then a little
robot called Jason

00:09:47.310 --> 00:09:50.220
that would come off this
vehicle and explore,

00:09:50.220 --> 00:09:53.190
in this case, the mid-ocean
ridges or hydrothermal vents.

00:09:53.190 --> 00:09:55.459
And Ballard started
a lab at Woods Hole

00:09:55.459 --> 00:09:57.000
called the Deep
Submergence Lab which

00:09:57.000 --> 00:09:58.860
spent the '80s
kind of developing

00:09:58.860 --> 00:10:00.600
this particular system.

00:10:00.600 --> 00:10:03.730
Now interestingly, before
the Jason robot came online,

00:10:03.730 --> 00:10:05.540
we had just the sled Argo.

00:10:05.540 --> 00:10:07.970
And that's the thing that
actually discovered the Titanic

00:10:07.970 --> 00:10:10.680
wreck in 1985.

00:10:10.680 --> 00:10:15.700
In 1986 though, the
Woods Hole groups

00:10:15.700 --> 00:10:19.330
together went back and
revisited the Titanic wreck

00:10:19.330 --> 00:10:21.590
and sent this little
kind of proto Jason

00:10:21.590 --> 00:10:23.520
called Jason Junior.

00:10:23.520 --> 00:10:27.700
It connected to Alvin, the three
man submarine, little robot,

00:10:27.700 --> 00:10:31.000
human vehicle,
remote robot, down

00:10:31.000 --> 00:10:32.720
the grand staircase
of the Titanic,

00:10:32.720 --> 00:10:34.095
took tremendous pictures.

00:10:34.095 --> 00:10:38.690
It was here on the cover of the
"National Geographic" magazine.

00:10:38.690 --> 00:10:40.410
But there was a great
deal of tension,

00:10:40.410 --> 00:10:42.490
in some cases literally
tension pulling

00:10:42.490 --> 00:10:45.000
on the cable between
the folks who

00:10:45.000 --> 00:10:47.160
operated the manned
submersible, Alvin,

00:10:47.160 --> 00:10:50.230
and the folks who felt
that remote presence was

00:10:50.230 --> 00:10:51.230
the way of the future.

00:10:51.230 --> 00:10:54.700
In fact, I use these two
covers because the "Geographic"

00:10:54.700 --> 00:10:56.600
article, which
Ballard wrote, shows

00:10:56.600 --> 00:11:00.260
only the little remote robot,
doesn't deal with Alvin at all.

00:11:00.260 --> 00:11:03.500
And the "Time" magazine cover--
which actually, Ken Marschall

00:11:03.500 --> 00:11:06.640
lives out here in LA, did
this painting off-- shows

00:11:06.640 --> 00:11:07.350
only Alvin.

00:11:07.350 --> 00:11:09.900
And this is why a lot of people
still to this day think that

00:11:09.900 --> 00:11:13.720
Alvin discovered the
Titanic, which it didn't.

00:11:13.720 --> 00:11:16.630
But what you see here
is a sense that actually

00:11:16.630 --> 00:11:18.560
the human and the
robotic vehicles

00:11:18.560 --> 00:11:21.930
are evolving together
and kind of playing off

00:11:21.930 --> 00:11:25.090
each other in the evolution.

00:11:25.090 --> 00:11:27.980
This image, which is a family
tree of undersea vehicles

00:11:27.980 --> 00:11:31.390
from Woods Hole, kind
of implies by it's sort

00:11:31.390 --> 00:11:34.580
of Darwinian ascent
nature that you

00:11:34.580 --> 00:11:36.740
have the manned
submersible here, Alvin,

00:11:36.740 --> 00:11:39.070
then the remote
vehicles Jason, then

00:11:39.070 --> 00:11:41.712
this whole panoply of
autonomous vehicles.

00:11:41.712 --> 00:11:42.920
I worked a little bit on ABE.

00:11:42.920 --> 00:11:44.320
I talk about it in the book.

00:11:44.320 --> 00:11:45.820
The REMUS vehicles
are the ones that

00:11:45.820 --> 00:11:48.930
discovered the wreck
of the Air France crash

00:11:48.930 --> 00:11:51.160
that I also discuss
in the book, moving

00:11:51.160 --> 00:11:54.190
toward this level of
higher and higher autonomy.

00:11:54.190 --> 00:11:56.120
But at the top of
this is vehicles

00:11:56.120 --> 00:11:58.260
that called hybrid ROVs.

00:11:58.260 --> 00:12:01.830
They are sometimes remote, and
they're sometimes autonomous.

00:12:01.830 --> 00:12:03.939
And so the real
evolution is actually,

00:12:03.939 --> 00:12:05.355
if anything, a
kind of convergence

00:12:05.355 --> 00:12:08.180
and a blurring of the
lines between what's

00:12:08.180 --> 00:12:10.950
a human operated vehicle,
what's a remote vehicle,

00:12:10.950 --> 00:12:14.500
and what's an
autonomous vehicle.

00:12:14.500 --> 00:12:17.010
These are a few different images
from some of my colleagues

00:12:17.010 --> 00:12:19.180
at Woods Hole
about what the kind

00:12:19.180 --> 00:12:22.940
of currently engineered future
of oceanography looks like.

00:12:22.940 --> 00:12:25.750
Here you have what we used to
think of an autonomous vehicle.

00:12:25.750 --> 00:12:27.060
Send it off the ship.

00:12:27.060 --> 00:12:29.380
It'll go down like
an autonomous robot.

00:12:29.380 --> 00:12:31.210
Run a bunch of track lines.

00:12:31.210 --> 00:12:32.110
Keep some algorithms.

00:12:32.110 --> 00:12:34.040
Do some mapping and come back.

00:12:34.040 --> 00:12:36.450
But of course, as
those vehicles evolved,

00:12:36.450 --> 00:12:41.500
they developed
acoustic technologies

00:12:41.500 --> 00:12:43.600
to stay in touch with
what's on the ship.

00:12:43.600 --> 00:12:46.350
And every expedition
you go on is really

00:12:46.350 --> 00:12:49.400
a collaboration between
a manned vehicle, a ship,

00:12:49.400 --> 00:12:50.860
and an autonomous vehicle.

00:12:50.860 --> 00:12:55.255
And you send the robotic
system out in the morning.

00:12:55.255 --> 00:12:58.070
It maybe comes back
a day or two later.

00:12:58.070 --> 00:13:00.170
Exchanges energy
for information.

00:13:00.170 --> 00:13:02.390
Talks to its human
overlords a little bit.

00:13:02.390 --> 00:13:03.830
Goes back out, does it again.

00:13:03.830 --> 00:13:06.440
And you have this kind of
constantly going out and coming

00:13:06.440 --> 00:13:06.940
back.

00:13:06.940 --> 00:13:10.580
And the autonomy is actually
pretty well bounded in time.

00:13:10.580 --> 00:13:13.650
And there's always, again, this
kind of wrapper of, go out,

00:13:13.650 --> 00:13:15.980
do this, come back.

00:13:15.980 --> 00:13:18.560
Sometimes they just run
straight track lines.

00:13:18.560 --> 00:13:23.170
But simply getting the
vehicle in six miles of water

00:13:23.170 --> 00:13:25.470
to go out and do
something and return

00:13:25.470 --> 00:13:27.760
is an amazing feat
of technology.

00:13:27.760 --> 00:13:29.640
It requires all
kinds of subtlety

00:13:29.640 --> 00:13:36.180
in the software, and algorithms,
and system design, and whatnot.

00:13:36.180 --> 00:13:39.970
But the challenge is to
keep it under control.

00:13:39.970 --> 00:13:41.840
I'll read you a little
passage from the book

00:13:41.840 --> 00:13:44.700
about my experience
with the remote vehicles

00:13:44.700 --> 00:13:48.610
and how it transformed the
presence that we experienced.

00:13:48.610 --> 00:13:51.960
"On a summer day in 1988,
two years after the Titanic

00:13:51.960 --> 00:13:55.860
exploration, I walked down the
stairs of an old green aluminum

00:13:55.860 --> 00:14:02.190
building in Woods Hole
called the Deep Submergence

00:14:02.190 --> 00:14:03.030
Laboratory.

00:14:03.030 --> 00:14:06.610
I was there looking for a
job straight out of college,

00:14:06.610 --> 00:14:08.820
and I was there to meet a
man named Skip Marquet, one

00:14:08.820 --> 00:14:10.804
of the original
designers of Alvin

00:14:10.804 --> 00:14:12.720
and the co-founder of
the Deep Submergence Lab

00:14:12.720 --> 00:14:14.440
with Bob Ballard.

00:14:14.440 --> 00:14:17.400
Touring around this lab, I saw
exotic robots, heavy pressure

00:14:17.400 --> 00:14:19.780
housings, and other
things foreign to me.

00:14:19.780 --> 00:14:22.350
'This thing's been inside the
Titanic,' Marquet said as he

00:14:22.350 --> 00:14:25.580
pointed out Jason Junior,
opened up on a lab bench with

00:14:25.580 --> 00:14:28.120
electronic guts spilled out.

00:14:28.120 --> 00:14:30.096
But inside the robots
and surrounding

00:14:30.096 --> 00:14:31.470
them were things
that were deeply

00:14:31.470 --> 00:14:34.220
familiar to me--
electronics, microprocessors,

00:14:34.220 --> 00:14:35.530
software manuals.

00:14:35.530 --> 00:14:36.780
And in a moment, I was hooked.

00:14:36.780 --> 00:14:39.520
I could bring my interests in
electronics and my engineering

00:14:39.520 --> 00:14:42.830
degree to this unusual
alien adventure.

00:14:42.830 --> 00:14:45.880
I was eager to travel the
world doing engineering

00:14:45.880 --> 00:14:47.440
and build electronics
that would find

00:14:47.440 --> 00:14:49.330
their way into
extreme environments,

00:14:49.330 --> 00:14:52.200
and not have to report
to work in a cubicle.

00:14:52.200 --> 00:14:57.340
I joined the Deep Submergence
Lab as a junior engineer.

00:14:57.340 --> 00:15:00.230
What was it like then to
operate a remote robot

00:15:00.230 --> 00:15:01.680
in the deep ocean?

00:15:01.680 --> 00:15:03.980
First of all, we should
qualify the term robot.

00:15:03.980 --> 00:15:06.520
The term was commonly
used for the vehicle,

00:15:06.520 --> 00:15:09.750
but there was very little
resembling autonomy about it.

00:15:09.750 --> 00:15:12.000
In fact, it was something
of a blank slate technically

00:15:12.000 --> 00:15:12.500
speaking.

00:15:12.500 --> 00:15:15.122
There was relatively little
computing power on board,

00:15:15.122 --> 00:15:17.580
only enough to flick the lights
and instruments on and off,

00:15:17.580 --> 00:15:20.620
turn on the thrusters, and
do some local housekeeping.

00:15:20.620 --> 00:15:22.900
The video and most of the
instrumentation signals

00:15:22.900 --> 00:15:25.100
went straight up
the optical fibers

00:15:25.100 --> 00:15:27.690
and was multiplexed
through the computer

00:15:27.690 --> 00:15:29.710
to go to the top for processing.

00:15:29.710 --> 00:15:31.700
Even when Jason was
doing something automatic

00:15:31.700 --> 00:15:34.770
such as holding its
own constant depth,

00:15:34.770 --> 00:15:38.170
the feedback loops were usually
closed up on the surface

00:15:38.170 --> 00:15:41.220
through a computer on the ship.

00:15:41.220 --> 00:15:43.130
Jason's control room
though consisted

00:15:43.130 --> 00:15:49.200
of five or six 27 inch video
monitors mounted on the wall,

00:15:49.200 --> 00:15:52.290
displaying imagery from the
multiple cameras navigation

00:15:52.290 --> 00:15:52.996
data.

00:15:52.996 --> 00:15:54.890
A series of control
stations were

00:15:54.890 --> 00:15:58.470
arranged beneath them, one for
the pilot, one for an engineer.

00:15:58.470 --> 00:16:01.000
A data logger changed
the videotapes.

00:16:01.000 --> 00:16:02.740
This left plenty
of room in the van

00:16:02.740 --> 00:16:05.400
for a chief scientist who
usually sat behind the pilot

00:16:05.400 --> 00:16:06.720
to direct the dive.

00:16:06.720 --> 00:16:10.860
Then 10 or 20 other people,
other scientists, engineers,

00:16:10.860 --> 00:16:14.090
graduate students, and
film teams from the media.

00:16:14.090 --> 00:16:16.950
When all was stable,
the whole control van

00:16:16.950 --> 00:16:20.010
would become concentrated
on the seafloor.

00:16:20.010 --> 00:16:22.580
'Now that's the world of
telepresence,' the pilot Martin

00:16:22.580 --> 00:16:23.870
Bowen said.

00:16:23.870 --> 00:16:25.510
'That's where I
forget about my body.

00:16:25.510 --> 00:16:28.260
And I project myself
into the ocean floor,

00:16:28.260 --> 00:16:32.027
and have to make
that vehicle dance.'

00:16:32.027 --> 00:16:33.860
The pilots learned to
narrow their attention

00:16:33.860 --> 00:16:36.130
to hear just a few voices.

00:16:36.130 --> 00:16:38.560
If the compass didn't look
right or the surface weather

00:16:38.560 --> 00:16:41.630
was flaring up, the pilot could
ask the navigator about it.

00:16:41.630 --> 00:16:43.580
I tended to stand
the navigation watch,

00:16:43.580 --> 00:16:45.530
and I learned to
anticipate much of what

00:16:45.530 --> 00:16:47.810
the pilot needed and
when he needed it--

00:16:47.810 --> 00:16:51.250
move the ship a bit, change the
navigation quality, watch out

00:16:51.250 --> 00:16:53.870
you're getting a little
bit close to this.

00:16:53.870 --> 00:16:56.490
'I just started mapping things
in my own head,' Martin Bowen

00:16:56.490 --> 00:17:00.090
said. 'What the obstacles are,
how I need to fly, how low.

00:17:00.090 --> 00:17:02.700
I have the advantage of being
surrounded by people who are

00:17:02.700 --> 00:17:05.849
also helping to take care.'

00:17:05.849 --> 00:17:08.930
So our presence on the
seafloor deeply related to what

00:17:08.930 --> 00:17:11.650
was going on that
darkened control room.

00:17:11.650 --> 00:17:15.210
As Jason pinged and photographed
its way around shipwreck sites,

00:17:15.210 --> 00:17:17.609
or hydrothermal vents,
or other places,

00:17:17.609 --> 00:17:21.230
the group in the control room
was in constant conversation,

00:17:21.230 --> 00:17:24.040
observing, questioning,
speculating on what

00:17:24.040 --> 00:17:26.680
the cameras and sensors showed.

00:17:26.680 --> 00:17:31.180
This constant real time seminar
about the ongoing exploration,

00:17:31.180 --> 00:17:32.790
combined with the
beautiful haunting

00:17:32.790 --> 00:17:34.490
images we were seeing
on the monitors,

00:17:34.490 --> 00:17:37.420
is what transported us
into this other world.

00:17:37.420 --> 00:17:39.560
It was the most fundamental
surprising difference

00:17:39.560 --> 00:17:42.050
from Alvin, where even
though you were physically

00:17:42.050 --> 00:17:44.100
near that other
world, you only had

00:17:44.100 --> 00:17:50.530
two people plus the pilot
talking to each other."

00:17:50.530 --> 00:17:53.370
So the rest of the book
goes through a series

00:17:53.370 --> 00:17:55.975
of similar experiences
in other environments.

00:17:58.680 --> 00:18:00.250
I mentioned the
undersea environment

00:18:00.250 --> 00:18:02.784
with both remote and
autonomous vehicles.

00:18:02.784 --> 00:18:04.200
There's a chapter
on space where I

00:18:04.200 --> 00:18:07.050
talk about the groups who
use the Mars Exploration

00:18:07.050 --> 00:18:11.120
rovers, not too far from here,
up at JPL, and how there, even

00:18:11.120 --> 00:18:14.060
though they're experiencing a
20 minute time delay from when

00:18:14.060 --> 00:18:18.490
they get their signals to when
they originated on Mars, a much

00:18:18.490 --> 00:18:20.350
longer time delay
to issue a command

00:18:20.350 --> 00:18:23.950
and see a response,
they still, over time,

00:18:23.950 --> 00:18:26.880
develop an immensely
intense feeling of presence

00:18:26.880 --> 00:18:28.180
on the red planet.

00:18:28.180 --> 00:18:30.870
And they feel like they are
present in the landscape.

00:18:30.870 --> 00:18:34.341
The robots up there are not
doing any geology on their own.

00:18:34.341 --> 00:18:36.090
They're remote controlled
from the ground.

00:18:36.090 --> 00:18:37.631
They have certain
autonomous features

00:18:37.631 --> 00:18:39.580
that are used at certain times.

00:18:39.580 --> 00:18:42.180
But they build a picture
on the ground, in this case

00:18:42.180 --> 00:18:47.170
as much through large scale
paper maps as through computer

00:18:47.170 --> 00:18:50.130
screens that the
group on the ground

00:18:50.130 --> 00:18:52.830
can kind of explore together.

00:18:52.830 --> 00:18:55.050
Similar stories
about the repairs

00:18:55.050 --> 00:18:56.470
of the Hubble Space Telescope.

00:18:56.470 --> 00:19:01.205
There were five missions
over a 15 year period

00:19:01.205 --> 00:19:03.120
that conducted those repairs.

00:19:03.120 --> 00:19:06.730
Beautiful choreographed
dance between humans, robots,

00:19:06.730 --> 00:19:10.310
all networked through these
kind of extended networks.

00:19:10.310 --> 00:19:14.530
There's a chapter on the
Predator, the remotely piloted

00:19:14.530 --> 00:19:17.700
aircraft that the
Air Force uses both

00:19:17.700 --> 00:19:20.450
for persistent surveillance,
real time video,

00:19:20.450 --> 00:19:22.520
in Iraq, and Afghanistan,
and elsewhere,

00:19:22.520 --> 00:19:25.950
and also for firing missiles
and shooting people.

00:19:25.950 --> 00:19:29.070
And what you found there is very
similar to what we experienced

00:19:29.070 --> 00:19:30.580
in the Jason control room.

00:19:30.580 --> 00:19:33.070
Their bodies, the
operators of Predator

00:19:33.070 --> 00:19:35.520
are physically removed
from the battlefield.

00:19:35.520 --> 00:19:39.540
But their experience of warfare
is actually very, very intense,

00:19:39.540 --> 00:19:41.510
partly because
they are doing what

00:19:41.510 --> 00:19:44.620
aircraft pilots who are
literally above the battlefield

00:19:44.620 --> 00:19:45.470
don't do.

00:19:45.470 --> 00:19:47.745
The Predator pilots
will circle for hours.

00:19:47.745 --> 00:19:50.850
They have a camera on
a single farmhouse,

00:19:50.850 --> 00:19:55.050
or compound, or
social organization.

00:19:55.050 --> 00:19:57.090
And because of the
social relationships

00:19:57.090 --> 00:20:01.690
they form through the networks,
sometimes with friendly forces

00:20:01.690 --> 00:20:04.105
on the ground who they're
trying to support,

00:20:04.105 --> 00:20:05.480
other times of
people who they're

00:20:05.480 --> 00:20:07.438
surveilling and potentially
attacking-- they're

00:20:07.438 --> 00:20:09.740
still social
relationships-- gives them

00:20:09.740 --> 00:20:11.670
a very intense
feeling of presence.

00:20:11.670 --> 00:20:14.190
And in fact, the
Predator operators

00:20:14.190 --> 00:20:16.250
experience post-traumatic
stress disorder

00:20:16.250 --> 00:20:20.920
at about the same rate that the
Air Force airplane pilots do,

00:20:20.920 --> 00:20:24.410
which is remarkable considering
they're physically not at risk.

00:20:24.410 --> 00:20:26.980
And it's a very, very
complicated conversation--

00:20:26.980 --> 00:20:29.330
I'm sure you've read about
in the media-- going on

00:20:29.330 --> 00:20:32.690
within the Air Force about,
are these people warriors?

00:20:32.690 --> 00:20:33.870
Do they deserve medals?

00:20:33.870 --> 00:20:36.280
Do they deserve combat pay?

00:20:36.280 --> 00:20:38.690
There's very high
rates of burnout.

00:20:38.690 --> 00:20:41.760
It's a very kind
of unsettling-- I

00:20:41.760 --> 00:20:44.040
hate to use this word--
but disruptive experience

00:20:44.040 --> 00:20:45.780
for the people involved.

00:20:45.780 --> 00:20:49.490
Similar story aboard
airliners where

00:20:49.490 --> 00:20:56.060
I talk about how, in some
airlines, pilots are actually--

00:20:56.060 --> 00:20:58.900
and the airlines
themselves-- are backing away

00:20:58.900 --> 00:21:02.650
from highly automated
systems like autoland

00:21:02.650 --> 00:21:05.460
which will land an airplane
without the pilot touching

00:21:05.460 --> 00:21:09.690
the controls, but is a fairly
brittle, kind of high stress

00:21:09.690 --> 00:21:15.830
situation, in favor of things
like heads up displays where

00:21:15.830 --> 00:21:18.470
in any kind of weather,
in any kind of situation,

00:21:18.470 --> 00:21:21.390
the pilot always does
the same procedures

00:21:21.390 --> 00:21:24.580
and is given a flight
path vector and a guidance

00:21:24.580 --> 00:21:28.430
cue overlaid on
the actual runway

00:21:28.430 --> 00:21:31.340
or where the runway will be if
he can't see it for weather,

00:21:31.340 --> 00:21:33.100
and lands that way.

00:21:33.100 --> 00:21:36.520
Very much dependent on
software, very highly automated,

00:21:36.520 --> 00:21:38.650
integrated technological system.

00:21:38.650 --> 00:21:41.700
But the perception and
the motor coordination

00:21:41.700 --> 00:21:43.970
are still going
through the human body.

00:21:43.970 --> 00:21:47.440
And actually interestingly,
the space shuttle

00:21:47.440 --> 00:21:51.790
had an autoland system that our
taxpayer dollars all paid for.

00:21:51.790 --> 00:21:53.895
It was never used.

00:21:53.895 --> 00:21:59.020
The pilots never, just like
the lunar landing pilots,

00:21:59.020 --> 00:22:01.180
didn't like the
idea or could never

00:22:01.180 --> 00:22:03.670
even test the autoland
system on the shuttle.

00:22:03.670 --> 00:22:07.634
But they had the same guidance
cues and flight path vectors

00:22:07.634 --> 00:22:09.550
in their heads up display,
which were actually

00:22:09.550 --> 00:22:11.490
generated by the autoland.

00:22:11.490 --> 00:22:13.670
But instead of the
autoland operating

00:22:13.670 --> 00:22:16.500
the flight controls
directly, it sort of went

00:22:16.500 --> 00:22:18.200
through the cognition
and the body

00:22:18.200 --> 00:22:21.230
of the commander of
the mission and flew it

00:22:21.230 --> 00:22:23.160
by joystick
following those cues.

00:22:26.320 --> 00:22:30.130
So one of the things you see
is all autonomous systems--

00:22:30.130 --> 00:22:33.570
this is actually a quote
from a DOD report in 2012.

00:22:33.570 --> 00:22:36.910
"All autonomous systems are
joint human machine cognitive

00:22:36.910 --> 00:22:37.840
systems.

00:22:37.840 --> 00:22:39.890
There are no fully
autonomous systems

00:22:39.890 --> 00:22:44.160
just as there are no fully
autonomous sailors, soldiers,

00:22:44.160 --> 00:22:45.560
airmen, or Marines."

00:22:45.560 --> 00:22:48.250
And you can extend that
into the civilian word world

00:22:48.250 --> 00:22:52.190
as, there are no autonomous
pilots, drivers, other things.

00:22:52.190 --> 00:22:54.360
Everybody's embedded
in a network.

00:22:54.360 --> 00:22:57.080
There's always these kinds
of trades of autonomy,

00:22:57.080 --> 00:23:01.750
particularly depending on
things like spatial positioning,

00:23:01.750 --> 00:23:04.250
bandwidth, what kind
of networks you're in.

00:23:04.250 --> 00:23:07.130
Those things vary over time.

00:23:07.130 --> 00:23:09.250
So I want to read you
a more modern story

00:23:09.250 --> 00:23:20.690
from the world of oceanography
about autonomous vehicles

00:23:20.690 --> 00:23:22.910
to show you how this plays out.

00:23:22.910 --> 00:23:24.970
"James Kinsey, a young
engineering scientist

00:23:24.970 --> 00:23:27.280
at the Deep Submergence
Lab 20 years

00:23:27.280 --> 00:23:30.890
after I started working
there, came to the job

00:23:30.890 --> 00:23:32.750
with great plans
for the autonomy

00:23:32.750 --> 00:23:35.740
he planned to bestow
on his vehicles.

00:23:35.740 --> 00:23:38.280
He began to build up
probabilistic models of how

00:23:38.280 --> 00:23:41.830
the hydrothermal vent plumes
propagate through the ocean,

00:23:41.830 --> 00:23:43.910
and to try to instruct
the vehicles to follow

00:23:43.910 --> 00:23:47.430
minute detections from their
sensors down to the vents.

00:23:47.430 --> 00:23:49.380
Over time, however,
Kinsey realized

00:23:49.380 --> 00:23:52.580
that trying to imbue that
much autonomy into the vehicle

00:23:52.580 --> 00:23:54.810
was likely a problem
from the beginning.

00:23:54.810 --> 00:23:57.910
Because of the nature of
oceanographic exploration,

00:23:57.910 --> 00:24:00.180
the tasks are poorly
defined, and the environment

00:24:00.180 --> 00:24:01.610
is always changing.

00:24:01.610 --> 00:24:03.270
Anything programmed
into the vehicle

00:24:03.270 --> 00:24:07.630
constituted assumptions, models
about how the world worked

00:24:07.630 --> 00:24:10.800
that might not be
valid in a new context

00:24:10.800 --> 00:24:13.030
where the people could see more.

00:24:13.030 --> 00:24:15.680
'I think it focused on the
wrong aspects of autonomy

00:24:15.680 --> 00:24:17.250
perhaps,' Kinsey told me.

00:24:17.250 --> 00:24:19.910
'You're requiring the vehicle
to understand a lot of context

00:24:19.910 --> 00:24:22.690
that may not be
available to it.'"

00:24:22.690 --> 00:24:24.770
Kinsey had his own
version of the surprise

00:24:24.770 --> 00:24:27.690
that I had talked about in a
previous chapter, experienced

00:24:27.690 --> 00:24:30.340
by space engineers
as the abstractions

00:24:30.340 --> 00:24:33.350
of autonomy from a kind of
design and research phase

00:24:33.350 --> 00:24:35.540
met real applications.

00:24:35.540 --> 00:24:38.410
"One of the problems with
having a vehicle that

00:24:38.410 --> 00:24:41.480
makes its own decisions,
Kinsey realized, is quote,

00:24:41.480 --> 00:24:44.160
'there's a certain amount of
opaqueness to what it's doing.

00:24:44.160 --> 00:24:49.210
Even if you're monitoring
it, you say gee,

00:24:49.210 --> 00:24:52.410
it just suddenly wandered
off to the southwest.

00:24:52.410 --> 00:24:56.150
Is that a malfunction, or is it
a part of the decision tree?'

00:24:56.150 --> 00:24:58.290
Operating in the deep
ocean is expensive,

00:24:58.290 --> 00:25:00.590
and autonomous vehicles,
even though they're unmanned,

00:25:00.590 --> 00:25:02.480
are far from disposable.

00:25:02.480 --> 00:25:05.780
Kinsey observes, 'people like
to know where their stuff is,

00:25:05.780 --> 00:25:09.600
especially when they pay
a lot of money for it.'

00:25:09.600 --> 00:25:11.900
Overall in the ocean,
the lines between

00:25:11.900 --> 00:25:14.910
human, remote, and autonomous
vehicles are blurring.

00:25:14.910 --> 00:25:19.290
Engineers envision now an ocean
with many vehicles working

00:25:19.290 --> 00:25:20.360
in concert.

00:25:20.360 --> 00:25:21.780
Some may contain people.

00:25:21.780 --> 00:25:23.880
Others will be
remote or autonomous.

00:25:23.880 --> 00:25:27.930
All will be capable of shifting
modes at any given time.

00:25:27.930 --> 00:25:29.650
Alvin recently had an upgrade.

00:25:29.650 --> 00:25:32.140
And the new software was
actually originally designed

00:25:32.140 --> 00:25:34.990
for autonomous vehicles.

00:25:34.990 --> 00:25:38.040
The chain challenges are to
coordinate all these machines,

00:25:38.040 --> 00:25:39.750
keep the humans
informed, and ensure

00:25:39.750 --> 00:25:42.890
that the robots' actions
reflect human intentions.

00:25:42.890 --> 00:25:45.090
Some will operate through
high bandwidth channels

00:25:45.090 --> 00:25:48.110
like optical fibers, others
through more constrained

00:25:48.110 --> 00:25:49.080
channels.

00:25:49.080 --> 00:25:52.500
Some will circle close to a
node to flash up their data,

00:25:52.500 --> 00:25:54.480
then slide back into the abyss.

00:25:54.480 --> 00:25:56.810
Each will do as it's told
and make some decisions

00:25:56.810 --> 00:25:59.230
on its own accord
to structures imbued

00:25:59.230 --> 00:26:01.690
by its human programmers.

00:26:01.690 --> 00:26:04.610
In this world undersea,
but also on land,

00:26:04.610 --> 00:26:07.830
we can imagine autonomy as
a strangely shaped three

00:26:07.830 --> 00:26:10.770
dimensional cloud, with
vehicles constantly

00:26:10.770 --> 00:26:13.950
moving back and forth
across its boundaries.

00:26:13.950 --> 00:26:16.360
Now imagine that one of
these vehicles is your car,

00:26:16.360 --> 00:26:19.330
and the 3D cloud of autonomy
is your neighborhood.

00:26:19.330 --> 00:26:21.090
At certain times
in certain places,

00:26:21.090 --> 00:26:23.310
the car has some
kinds of autonomy--

00:26:23.310 --> 00:26:27.610
stay within a highway lane or
drive in a high speed convoy.

00:26:27.610 --> 00:26:30.750
At other times, such as if
you're far from a cell tower

00:26:30.750 --> 00:26:34.190
or driving in snow and the ice
obscures the car's sensors,

00:26:34.190 --> 00:26:36.150
the autonomous
capabilities are reduced,

00:26:36.150 --> 00:26:38.190
and the driver must
be more involved.

00:26:38.190 --> 00:26:40.250
You drive in and
out of the cloud,

00:26:40.250 --> 00:26:43.930
delicately switching in and
out of automatic modes."

00:26:43.930 --> 00:26:45.590
So what does this
mean for engineering?

00:26:45.590 --> 00:26:47.610
I'm an engineer as
well as a scholar,

00:26:47.610 --> 00:26:51.190
and I like to think
that all of this work

00:26:51.190 --> 00:26:54.280
tells us something about how
to build autonomous systems.

00:26:54.280 --> 00:26:55.220
One of them is this.

00:26:57.742 --> 00:26:59.450
I've interviewed a
lot of airline pilots.

00:26:59.450 --> 00:27:01.116
And I've asked every
single one of them,

00:27:01.116 --> 00:27:02.940
have you ever asked
this question?

00:27:02.940 --> 00:27:05.880
And every single one
of them says yes.

00:27:05.880 --> 00:27:08.070
In fact, one of them
said, "oh, that's

00:27:08.070 --> 00:27:11.330
only what the new guys
say, what's it doing now.

00:27:11.330 --> 00:27:18.730
The experienced people say,
oh it does that sometimes."

00:27:18.730 --> 00:27:22.140
And so I've been involved
with a partner of mine,

00:27:22.140 --> 00:27:24.100
a company called
Aurora Flight Sciences,

00:27:24.100 --> 00:27:28.980
building full size autonomous
vehicles for research projects.

00:27:28.980 --> 00:27:30.770
And one of them was
this program called

00:27:30.770 --> 00:27:35.500
AACUS, which was a full
size autonomous cargo

00:27:35.500 --> 00:27:40.560
helicopter designed to deliver
cargo to some remote area

00:27:40.560 --> 00:27:43.420
without putting a
human pilot at risk.

00:27:43.420 --> 00:27:45.570
But again, there's
always a wrapper

00:27:45.570 --> 00:27:50.220
of human activity around
any autonomous system.

00:27:50.220 --> 00:27:52.230
And in this case,
part of that wrapper

00:27:52.230 --> 00:27:54.350
was the people who needed
the supplies, right?

00:27:54.350 --> 00:27:55.780
There's no reason
to go somewhere

00:27:55.780 --> 00:27:57.870
if you don't have
something to bring someone.

00:27:57.870 --> 00:28:00.740
And so there's a marine
landing support specialist

00:28:00.740 --> 00:28:01.700
on the ground.

00:28:01.700 --> 00:28:05.170
And now we were required by
ONR, this person is not a pilot,

00:28:05.170 --> 00:28:08.370
should not have a kind of
flight control station,

00:28:08.370 --> 00:28:11.030
and should be able to work
with the system with only

00:28:11.030 --> 00:28:12.880
15 minutes of training.

00:28:12.880 --> 00:28:15.380
So we talk to these folks and
we said, how would you like it

00:28:15.380 --> 00:28:18.260
if we could build an autonomous
cargo helicopter that

00:28:18.260 --> 00:28:20.900
has a lidar that could come
in and find a landing zone,

00:28:20.900 --> 00:28:23.060
and land where you
want your supplies.

00:28:23.060 --> 00:28:25.340
And they all, all of
them had experience

00:28:25.340 --> 00:28:26.490
in Iraq and Afghanistan.

00:28:26.490 --> 00:28:29.570
They said, oh my goodness,
no way, horrifying,

00:28:29.570 --> 00:28:33.820
terrifying to have
a 10 ton Black Hawk

00:28:33.820 --> 00:28:37.940
helicopter bearing down on
you at 100 knots, not fun.

00:28:37.940 --> 00:28:39.680
They said, you have
no idea how scary it

00:28:39.680 --> 00:28:43.140
is to be in a combat zone and
see unmanned aircraft flying

00:28:43.140 --> 00:28:45.535
around when you don't
know who's they are,

00:28:45.535 --> 00:28:47.660
what their intentions are,
what they're doing even.

00:28:47.660 --> 00:28:51.290
If they're friendly
they're scary.

00:28:51.290 --> 00:28:53.620
So we had to build
a system that had

00:28:53.620 --> 00:28:56.070
a kind of situated
or embedded autonomy

00:28:56.070 --> 00:28:59.420
where the people on the
ground could at least

00:28:59.420 --> 00:29:03.670
have some basic kind of state
control of the aircraft.

00:29:03.670 --> 00:29:05.860
The simplest one is
if it's coming in

00:29:05.860 --> 00:29:08.120
and it actually
has a lidar on it,

00:29:08.120 --> 00:29:11.220
you can just see some of
these operational modes here.

00:29:11.220 --> 00:29:14.090
Here's the helicopter
scanning the terrain

00:29:14.090 --> 00:29:17.810
and choosing a landing zone.

00:29:17.810 --> 00:29:20.010
But maybe the landing zone
the helicopter chooses

00:29:20.010 --> 00:29:22.480
is not one that's acceptable
to the person on the ground,

00:29:22.480 --> 00:29:23.890
or vice versa.

00:29:23.890 --> 00:29:27.770
So you have about a minute to
do a kind of little negotiation.

00:29:27.770 --> 00:29:29.070
Do this, do that.

00:29:29.070 --> 00:29:32.780
And the aircraft has to
be able to respond in ways

00:29:32.780 --> 00:29:36.250
that the human finds
predictable, understandable.

00:29:36.250 --> 00:29:39.510
And that basically
means relatively simple.

00:29:39.510 --> 00:29:41.710
What we ended up with in
this case was actually

00:29:41.710 --> 00:29:44.930
designing the kind of core
state machine for the autonomous

00:29:44.930 --> 00:29:49.260
system-- in this case, it
was in MATLAB Stateflow--

00:29:49.260 --> 00:29:51.960
to have these kind of
macro states of autonomy

00:29:51.960 --> 00:29:55.700
that were very simple, very well
understood, very predictable,

00:29:55.700 --> 00:29:58.060
and then kind of
autocoding both the user

00:29:58.060 --> 00:30:01.940
interface and the core autonomy
mission manager out of that

00:30:01.940 --> 00:30:05.520
to give the system a very
kind of predictable look

00:30:05.520 --> 00:30:07.230
and feel to the person.

00:30:07.230 --> 00:30:10.360
There's a few things they could
command the helicopter to do.

00:30:10.360 --> 00:30:15.270
Go away, abort, circle for a
little, choose another zone.

00:30:15.270 --> 00:30:19.070
And there were a lot
of complex algorithms

00:30:19.070 --> 00:30:20.980
buried in each of those states.

00:30:20.980 --> 00:30:22.455
But the states had
to be relatively

00:30:22.455 --> 00:30:23.580
simple and straightforward.

00:30:26.490 --> 00:30:28.445
The fly off for that
vehicle was in 2014.

00:30:28.445 --> 00:30:29.770
It went very well.

00:30:29.770 --> 00:30:32.200
We beat Lockheed
Martin at that game.

00:30:32.200 --> 00:30:34.600
That program is now
progressing into other phases.

00:30:34.600 --> 00:30:36.462
We are very proud
of our work on it.

00:30:36.462 --> 00:30:38.920
And yet, when it was reported
in the "Wall Street Journal,"

00:30:38.920 --> 00:30:40.970
this was the
headline-- "Navy Drones

00:30:40.970 --> 00:30:43.370
With a Mind of Their Own."

00:30:43.370 --> 00:30:44.920
And so there's
still a great kind

00:30:44.920 --> 00:30:49.310
of public narrative
of, Star Wars-y,

00:30:49.310 --> 00:30:51.680
20th century science
fiction, the drones

00:30:51.680 --> 00:30:54.780
are coming to kill
us kinds of things,

00:30:54.780 --> 00:30:57.560
despite the fact that
people who are closest to it

00:30:57.560 --> 00:31:00.492
on the front lines, that's
the last thing they want.

00:31:03.290 --> 00:31:07.380
I'll just briefly close
with another program

00:31:07.380 --> 00:31:08.460
that we're working on.

00:31:08.460 --> 00:31:10.720
This is a DARPA
program called ALIAS

00:31:10.720 --> 00:31:13.450
where the assignment is really
to build a kind of robot that

00:31:13.450 --> 00:31:17.380
sits in the co-pilot seat--
not really a robotic co-pilot

00:31:17.380 --> 00:31:19.800
because you actually end up
changing the roles of both

00:31:19.800 --> 00:31:22.480
the pilot and the co-pilot.

00:31:22.480 --> 00:31:25.530
And then can either convert
an existing aircraft

00:31:25.530 --> 00:31:27.810
into a remotely
piloted aircraft,

00:31:27.810 --> 00:31:31.220
or can kind of
help a single pilot

00:31:31.220 --> 00:31:34.580
and kind of-- so really,
one of the terms people

00:31:34.580 --> 00:31:38.460
use these days is a co-robot
that allows support and help,

00:31:38.460 --> 00:31:40.900
but still keeps the
human in the loop.

00:31:40.900 --> 00:31:44.100
Very challenging program from
a bunch of different points

00:31:44.100 --> 00:31:46.850
of view, but one of them that
I think is the most worthwhile

00:31:46.850 --> 00:31:49.080
is learning how to
build a system that's

00:31:49.080 --> 00:31:52.310
truly collaborative
and allows the pilot

00:31:52.310 --> 00:31:56.480
to sort of go through checklists
and different procedures.

00:31:56.480 --> 00:31:59.080
This program is written up by
John Markoff in "The New York

00:31:59.080 --> 00:32:03.010
Times" this summer, "A Machine
in the Co-pilot's Seat."

00:32:03.010 --> 00:32:06.090
There's a kind of prototype of
it that Aurora has already made

00:32:06.090 --> 00:32:09.670
called Centaur, which is an
optionally piloted aircraft.

00:32:09.670 --> 00:32:13.700
It can be flown by a human pilot
like a traditional aircraft.

00:32:13.700 --> 00:32:16.450
It can be remotely operated
from the ground station.

00:32:16.450 --> 00:32:18.580
Or the mode that I
think is the coolest,

00:32:18.580 --> 00:32:21.440
you can fly the ground
station from the backseat.

00:32:21.440 --> 00:32:23.040
And so you're like
a UAV operator,

00:32:23.040 --> 00:32:24.530
but you're sitting in the seat.

00:32:24.530 --> 00:32:26.710
And again, you see
this convergence

00:32:26.710 --> 00:32:35.430
of remote, human occupied,
and autonomous vehicles.

00:32:35.430 --> 00:32:39.208
So what can we
conclude from all this?

00:32:39.208 --> 00:32:42.060
I'll read a little bit
from the conclusion.

00:32:42.060 --> 00:32:44.670
"The fully autonomous
land robot making its way

00:32:44.670 --> 00:32:47.300
through a landscape
under computer control

00:32:47.300 --> 00:32:49.890
remains an attractive
idea for engineers.

00:32:49.890 --> 00:32:52.350
Perceiving the environment,
classifying, matching it

00:32:52.350 --> 00:32:54.540
up to models and
prior experience,

00:32:54.540 --> 00:32:57.840
and making plans to move
forward resemble our daily acts

00:32:57.840 --> 00:32:58.870
of living.

00:32:58.870 --> 00:33:01.700
Uncertainties in the world
and within the machines,

00:33:01.700 --> 00:33:05.050
the unexpected that will always
foil our prior assumptions

00:33:05.050 --> 00:33:07.290
make the problem not only
harder, but even more

00:33:07.290 --> 00:33:08.740
interesting.

00:33:08.740 --> 00:33:10.770
Thinking these
problems through, aided

00:33:10.770 --> 00:33:13.960
by the medium of technology
is a noble effort, engineering

00:33:13.960 --> 00:33:15.930
at its philosophical best.

00:33:15.930 --> 00:33:19.340
How do we observe, decide,
and act in the world?

00:33:19.340 --> 00:33:21.900
How should we live
with uncertainty?

00:33:21.900 --> 00:33:24.830
But we should not confuse
technical thought experiments

00:33:24.830 --> 00:33:27.450
with what's useful
in a human context.

00:33:27.450 --> 00:33:30.880
When lives and resources
are at stake, time and time

00:33:30.880 --> 00:33:34.360
again, over decades of
examples given in this book,

00:33:34.360 --> 00:33:36.120
from the deep ocean
to the outer planets,

00:33:36.120 --> 00:33:38.070
we have reined in the autonomy.

00:33:38.070 --> 00:33:40.620
It's not a story about
progress, that one day we'll

00:33:40.620 --> 00:33:42.510
just advance enough
to get it right,

00:33:42.510 --> 00:33:45.650
but a story about the move
from laboratory and R&amp;D

00:33:45.650 --> 00:33:47.010
into the field.

00:33:47.010 --> 00:33:49.360
That transition
tempers the autonomy,

00:33:49.360 --> 00:33:51.770
whether the task is to
respond to interactions

00:33:51.770 --> 00:33:54.820
and return scientific data,
or to protect and defend

00:33:54.820 --> 00:33:56.950
human life.

00:33:56.950 --> 00:33:59.770
In retrospect, Neil
Armstrong's last minute manual

00:33:59.770 --> 00:34:03.730
intervention, turning off the
automation on his moon landing,

00:34:03.730 --> 00:34:08.280
signaled the limits of the 20th
century vision of full autonomy

00:34:08.280 --> 00:34:12.010
and foretold the slow advent
of potent human collaboration

00:34:12.010 --> 00:34:13.340
and presence.

00:34:13.340 --> 00:34:15.590
The lone autonomous
drone is as much

00:34:15.590 --> 00:34:19.020
an anachronism as the
lone unconnected computer.

00:34:19.020 --> 00:34:21.159
The challenges of robotics
in the 20th century

00:34:21.159 --> 00:34:22.989
are those of situating
machines within

00:34:22.989 --> 00:34:25.159
human and social systems.

00:34:25.159 --> 00:34:28.400
They're challenges
of relationships."

00:34:28.400 --> 00:34:30.820
So I'll just close
with a little startup

00:34:30.820 --> 00:34:33.810
that I've been working
on called Humatics which

00:34:33.810 --> 00:34:36.929
aims to take what we've done
in those two projects for ONR

00:34:36.929 --> 00:34:40.429
and DARPA and bring them to
a larger world of robotics.

00:34:40.429 --> 00:34:43.280
How do we enable robotics
and autonomous systems

00:34:43.280 --> 00:34:46.810
to work within
human environments?

00:34:46.810 --> 00:34:50.530
And how do we make autonomy
transparent, trusted,

00:34:50.530 --> 00:34:53.889
and by extension, safe
and useful for people

00:34:53.889 --> 00:34:56.380
where other people are around?

00:34:56.380 --> 00:34:58.920
I often say, if you look
at the full autonomy

00:34:58.920 --> 00:35:02.530
problem in aviation, build
a fully unmanned aircraft

00:35:02.530 --> 00:35:04.660
to take off, fly
somewhere, and land,

00:35:04.660 --> 00:35:06.270
we solved that
problem 20 years ago.

00:35:06.270 --> 00:35:07.600
We know how to do that.

00:35:07.600 --> 00:35:09.790
To do that same
task from an airport

00:35:09.790 --> 00:35:12.050
that other people are
using, through air

00:35:12.050 --> 00:35:15.190
space that other people are
flying through, over places

00:35:15.190 --> 00:35:17.410
where people are living,
landing at another airport

00:35:17.410 --> 00:35:18.860
that other people
are using, we're

00:35:18.860 --> 00:35:21.020
just beginning to think
about that problem.

00:35:21.020 --> 00:35:22.430
That's a very
challenging problem

00:35:22.430 --> 00:35:25.080
that we really don't
even have answers to yet.

00:35:25.080 --> 00:35:26.965
So I'll just leave at that.

00:35:26.965 --> 00:35:28.590
And there's a bunch
of books out there.

00:35:28.590 --> 00:35:30.200
And we have some
time for questions.

00:35:30.200 --> 00:35:30.868
Thank you.

00:35:30.868 --> 00:35:34.214
[APPLAUSE]

00:35:37.570 --> 00:35:41.180
AUDIENCE: So I'm really
interested in the failure modes

00:35:41.180 --> 00:35:43.840
between, as the autonomy
is transitioning

00:35:43.840 --> 00:35:46.920
between-- I think the Air
France example is a great one.

00:35:46.920 --> 00:35:49.460
So you put all
these safety modes,

00:35:49.460 --> 00:35:52.400
as we're now putting them
in cars, to prevent crashes.

00:35:52.400 --> 00:35:54.650
And over the statistics,
they're safer.

00:35:54.650 --> 00:35:57.890
But then you start to see these
really catastrophic failures

00:35:57.890 --> 00:35:59.555
when but they don't work.

00:35:59.555 --> 00:36:00.680
So how do we prevent those?

00:36:00.680 --> 00:36:04.520
How do we keep the
pilots having context

00:36:04.520 --> 00:36:06.740
when the autopilot
says, I need to shut off

00:36:06.740 --> 00:36:07.850
and you're in control?

00:36:07.850 --> 00:36:09.651
Like, it's in your lap--

00:36:09.651 --> 00:36:10.900
DAVID MINDELL: Great question.

00:36:10.900 --> 00:36:12.507
Obviously, really
essential issue.

00:36:12.507 --> 00:36:14.340
I've talked a lot about
the Air France crash

00:36:14.340 --> 00:36:18.970
because it's sort of the
classic hand off tragedy.

00:36:18.970 --> 00:36:20.935
First of all, you can
look into that story

00:36:20.935 --> 00:36:25.810
and you can see that there are
things that the airliner could

00:36:25.810 --> 00:36:29.440
have been programmed to do to
make the transition smoother.

00:36:29.440 --> 00:36:34.780
For example, it lost its air
data from the pitot tubes

00:36:34.780 --> 00:36:36.770
when they iced over.

00:36:36.770 --> 00:36:39.200
And at that point,
just checked out,

00:36:39.200 --> 00:36:41.580
said, OK, no more
fly by wire at all.

00:36:41.580 --> 00:36:43.510
You're in a manual
reversion mode.

00:36:43.510 --> 00:36:44.165
Here you go.

00:36:44.165 --> 00:36:44.665
Right?

00:36:47.580 --> 00:36:50.610
Again, if you think about
engineering the relationship--

00:36:50.610 --> 00:36:53.870
which wasn't really done when
that airliner was designed--

00:36:53.870 --> 00:36:56.310
you can think about a
lot of alternatives.

00:36:56.310 --> 00:36:59.240
Anybody who programs
flight controls for UAVs

00:36:59.240 --> 00:37:01.800
will tell you that you
could fly perfectly flying

00:37:01.800 --> 00:37:03.880
without pitot tubes, right?

00:37:03.880 --> 00:37:06.600
You have a sort of basic
internal flight model.

00:37:06.600 --> 00:37:08.440
You knew you were going
at a certain rate.

00:37:08.440 --> 00:37:10.140
It's perfectly
capable of holding

00:37:10.140 --> 00:37:12.920
the airplane stable for
a while, quite a while

00:37:12.920 --> 00:37:15.410
actually, or descending
to a safe altitude

00:37:15.410 --> 00:37:18.500
while the crew can sort of--

00:37:18.500 --> 00:37:20.460
Secondly, the
interfaces on airliners

00:37:20.460 --> 00:37:23.620
are really not all that well
designed by modern standards.

00:37:23.620 --> 00:37:27.080
And they are designed to
kind of shift liability

00:37:27.080 --> 00:37:30.850
as much as they are to
provide insight into what's

00:37:30.850 --> 00:37:33.190
going on inside the system.

00:37:33.190 --> 00:37:34.720
Thirdly, and this
has come up quite

00:37:34.720 --> 00:37:37.730
a bit since that crash and a
couple other crashes as well,

00:37:37.730 --> 00:37:40.160
the pilot's manual skills
had clearly degraded.

00:37:40.160 --> 00:37:42.770
I mean, flying an airliner
at a very high altitude,

00:37:42.770 --> 00:37:44.780
often autopilot, is
not an easy task.

00:37:44.780 --> 00:37:46.000
The air is very thin.

00:37:46.000 --> 00:37:49.790
The line between
stalling and overspeeding

00:37:49.790 --> 00:37:51.170
is a very narrow line.

00:37:51.170 --> 00:37:54.620
It's not something
that's typically done.

00:37:54.620 --> 00:37:57.405
At the same time, every
beginner pilot is taught,

00:37:57.405 --> 00:37:59.780
when the airplane is stalling,
don't pull the stick back.

00:37:59.780 --> 00:38:01.930
And in the Air France case,
one guy pulled it back

00:38:01.930 --> 00:38:05.450
and one guy pushed it forward.

00:38:05.450 --> 00:38:08.340
So the hand off is challenging.

00:38:08.340 --> 00:38:11.190
There are a lot of places
that it's done well.

00:38:11.190 --> 00:38:13.490
There are other places
that it's done poorly.

00:38:13.490 --> 00:38:15.610
There's certainly room
for innovation there.

00:38:15.610 --> 00:38:20.220
In fact, I think, again,
if we see the perfect five

00:38:20.220 --> 00:38:22.930
as a goal of the
technology, then

00:38:22.930 --> 00:38:25.330
we should focus on improving
the hand offs in lots

00:38:25.330 --> 00:38:27.060
of different directions.

00:38:27.060 --> 00:38:29.310
One of the goals of the book
is to at least point out

00:38:29.310 --> 00:38:32.520
places that it's happened
in these other cases that

00:38:32.520 --> 00:38:35.040
are useful examples for
people who are working

00:38:35.040 --> 00:38:36.700
and other technologies today.

00:38:36.700 --> 00:38:38.389
So absolutely essential thing.

00:38:38.389 --> 00:38:39.097
AUDIENCE: Thanks.

00:38:41.821 --> 00:38:43.570
AUDIENCE: So it's not
a huge tactical leap

00:38:43.570 --> 00:38:46.130
to go from the
assisted helicopter,

00:38:46.130 --> 00:38:51.870
for instance, landing at the
human supervision to adding

00:38:51.870 --> 00:38:54.280
forward looking
infrared and a minigun

00:38:54.280 --> 00:38:57.745
to start blasting
anything with heat.

00:38:57.745 --> 00:38:59.120
Are there things
that can be done

00:38:59.120 --> 00:39:04.190
to prevent that, or
prevent the evil genius,

00:39:04.190 --> 00:39:08.295
or the little subshot
of CIA from developing

00:39:08.295 --> 00:39:10.420
these technologies that
really, technically, aren't

00:39:10.420 --> 00:39:12.628
that difficult after these
other problems are solved?

00:39:12.628 --> 00:39:16.650
DAVID MINDELL: Right,
that's a great question too.

00:39:16.650 --> 00:39:21.490
The current US military official
policy is we don't do that.

00:39:21.490 --> 00:39:24.040
Weapons are always
released under some kind

00:39:24.040 --> 00:39:26.190
of human approval process.

00:39:26.190 --> 00:39:28.620
And I think, if
anything, I would almost

00:39:28.620 --> 00:39:31.600
guess that that policy will
become stronger because there's

00:39:31.600 --> 00:39:34.330
a fear among general
military professionals

00:39:34.330 --> 00:39:36.620
like, what do we actually do?

00:39:36.620 --> 00:39:39.240
If actually joysticking
the aircraft maybe

00:39:39.240 --> 00:39:40.990
isn't our great skill
anymore, and we're

00:39:40.990 --> 00:39:44.110
getting kind of pushed
around in the networks, well,

00:39:44.110 --> 00:39:48.500
deciding who to kill and when,
that's an important thing.

00:39:48.500 --> 00:39:52.750
That said, there's a long
history of autonomy in weapons.

00:39:52.750 --> 00:39:55.390
And a guided missile
from the 1950s

00:39:55.390 --> 00:39:59.700
is, again, a kind of situated
autonomy where a human aims

00:39:59.700 --> 00:40:01.610
a device and releases it.

00:40:01.610 --> 00:40:04.370
And it has some sort
of automatic function.

00:40:04.370 --> 00:40:06.320
And the debates are
really about what

00:40:06.320 --> 00:40:09.430
is an appropriate and acceptable
line for those things.

00:40:09.430 --> 00:40:12.127
Landmines are autonomous
vehicles, right?

00:40:12.127 --> 00:40:13.960
And they're autonomous
weapons without a lot

00:40:13.960 --> 00:40:15.250
of discrimination.

00:40:15.250 --> 00:40:18.580
And a lot of people feel they're
beyond an ethical line that's

00:40:18.580 --> 00:40:19.180
acceptable.

00:40:19.180 --> 00:40:24.550
So some of these issues have a
long rooting in other questions

00:40:24.550 --> 00:40:27.310
about weapons.

00:40:27.310 --> 00:40:30.220
I personally feel you're a
lot more likely to get killed

00:40:30.220 --> 00:40:32.390
by a poorly designed
robot then you are

00:40:32.390 --> 00:40:34.860
to get killed by an evil robot.

00:40:34.860 --> 00:40:38.920
And that kind of poor
design and poor situating

00:40:38.920 --> 00:40:41.070
is really a much more
immediate concern

00:40:41.070 --> 00:40:43.790
for us, a la Air
France or other things,

00:40:43.790 --> 00:40:47.560
than the evil robots
coming to take over.

00:40:50.875 --> 00:40:53.250
AUDIENCE: I think this may be
a little bit of a follow up

00:40:53.250 --> 00:40:56.180
on the Air France,
except on the ground.

00:40:56.180 --> 00:40:58.370
There's a lot of work
and a lot of press

00:40:58.370 --> 00:41:01.550
lately about the development
of these autonomous vehicles,

00:41:01.550 --> 00:41:02.930
the self driving cars.

00:41:02.930 --> 00:41:06.800
And I was wondering if you have
any comments or ideas of how

00:41:06.800 --> 00:41:08.300
you think that's
going to go, or how

00:41:08.300 --> 00:41:09.680
you think it should develop.

00:41:09.680 --> 00:41:11.180
DAVID MINDELL: Yeah,
great question.

00:41:11.180 --> 00:41:14.550
And I do discuss
this in the book.

00:41:14.550 --> 00:41:17.130
I believe that we should
be able to improve

00:41:17.130 --> 00:41:21.770
the safety of automobiles,
and even radically change

00:41:21.770 --> 00:41:25.530
the driver experience using
all the good stuff that you've

00:41:25.530 --> 00:41:28.250
read about-- machine
vision, and lidars,

00:41:28.250 --> 00:41:31.670
other kinds of advanced
sensors, path planning.

00:41:31.670 --> 00:41:33.330
But I think it
should be organized

00:41:33.330 --> 00:41:38.650
around keeping the human deeply
involved in the experience.

00:41:38.650 --> 00:41:40.230
Reducing workload, yes.

00:41:40.230 --> 00:41:43.810
Allowing you to text, yes.

00:41:43.810 --> 00:41:48.980
Allowing relief of
boring activities

00:41:48.980 --> 00:41:51.560
like sitting in traffic
jams, by all means.

00:41:51.560 --> 00:41:54.760
Sleeping in the trunk, no.

00:41:54.760 --> 00:41:59.250
And sleeping in the trunk with
your kids in the backseat,

00:41:59.250 --> 00:42:01.720
again, there are
30 or 40 examples

00:42:01.720 --> 00:42:03.520
about how that's
really not a safe way

00:42:03.520 --> 00:42:06.120
to think about a system.

00:42:06.120 --> 00:42:09.005
It's a very interesting, very
important debate as you know.

00:42:09.005 --> 00:42:11.520
In the last week, since
the book came out,

00:42:11.520 --> 00:42:18.560
Tesla released one of its
early autopilot features,

00:42:18.560 --> 00:42:22.590
and a lot of people
feel released it

00:42:22.590 --> 00:42:26.354
without the proper--
beta testing

00:42:26.354 --> 00:42:28.270
is not something you
really do with your users

00:42:28.270 --> 00:42:30.730
when they're going down the
highway at 90 miles an hour.

00:42:30.730 --> 00:42:34.280
It's very different from
software in other realms.

00:42:34.280 --> 00:42:36.730
And there are some
interesting stuff popping up

00:42:36.730 --> 00:42:42.750
about various ways that there
are problems potentially

00:42:42.750 --> 00:42:43.960
with that algorithm.

00:42:43.960 --> 00:42:46.350
I think that particular
issue has more

00:42:46.350 --> 00:42:50.170
to do with software
testing and release policy

00:42:50.170 --> 00:42:51.771
than it does with
what's the right way

00:42:51.771 --> 00:42:52.770
to automate the vehicle.

00:42:52.770 --> 00:42:56.130
But it's still very
much on people's minds.

00:42:56.130 --> 00:42:59.790
And I know there are people
in the driverless car

00:42:59.790 --> 00:43:02.289
community who are horrified.

00:43:02.289 --> 00:43:03.830
I think they're
right that if there's

00:43:03.830 --> 00:43:05.920
an accident in the
next week or two

00:43:05.920 --> 00:43:08.290
with this early release
of this software,

00:43:08.290 --> 00:43:11.810
it will set back a lot of
people's idea of progress

00:43:11.810 --> 00:43:13.400
for a long time.

00:43:13.400 --> 00:43:16.400
That said, I think it's
amazing and exciting to think

00:43:16.400 --> 00:43:19.920
about, how can we use
technology and driving to expand

00:43:19.920 --> 00:43:23.100
your experience of the
world, and bring you more

00:43:23.100 --> 00:43:26.380
richly out into the environment
that you're in, whether it's

00:43:26.380 --> 00:43:29.180
your situatedness vis
a vis other people,

00:43:29.180 --> 00:43:32.330
or the geographic environment,
or other parts of the driving

00:43:32.330 --> 00:43:36.150
task, while at the same time
relieving you of-- I'm a pilot

00:43:36.150 --> 00:43:38.290
and I fly long
distances in my plane.

00:43:38.290 --> 00:43:41.304
But I can mostly do it like
this where my hands are crossed

00:43:41.304 --> 00:43:41.970
and I'm looking.

00:43:41.970 --> 00:43:44.200
And things don't
change all that fast

00:43:44.200 --> 00:43:46.350
unless there's a small
number of emergencies.

00:43:46.350 --> 00:43:48.670
Whereas when I drive,
I'm, you know, got

00:43:48.670 --> 00:43:50.015
my shoulders tensed up.

00:43:50.015 --> 00:43:53.020
And a car or a hit could run our
in front of you at any moment.

00:43:53.020 --> 00:43:56.930
That's a much more kind
of tense experience.

00:43:56.930 --> 00:43:58.900
I think there's any
every reason to believe

00:43:58.900 --> 00:44:03.045
we can relieve that kind
of hyperactive nervousness

00:44:03.045 --> 00:44:07.610
of driving into things that
will both reduce error, reduce

00:44:07.610 --> 00:44:10.060
fatigue, reduce workload.

00:44:10.060 --> 00:44:12.800
But again, I have a lot
of trouble imagining

00:44:12.800 --> 00:44:14.440
sleeping in the
back of the trunk

00:44:14.440 --> 00:44:16.916
while the car is barrelling
down the highway.

00:44:16.916 --> 00:44:18.290
AUDIENCE: Also,
I'm wondering is,

00:44:18.290 --> 00:44:21.320
is there likely a
human factors part?

00:44:21.320 --> 00:44:24.370
Because one of the issues with
some of the autopilots when

00:44:24.370 --> 00:44:26.510
they kick out is that
the highly trained pilots

00:44:26.510 --> 00:44:31.756
have trouble figuring out how
to move into the mode of flying.

00:44:31.756 --> 00:44:33.130
And is that going
to be a problem

00:44:33.130 --> 00:44:34.720
particularly with drivers?

00:44:34.720 --> 00:44:36.840
DAVID MINDELL: You get
manual skill degradation

00:44:36.840 --> 00:44:39.830
if you don't practice it enough.

00:44:39.830 --> 00:44:41.930
And that's something
to certainly consider.

00:44:41.930 --> 00:44:44.890
Although again, when you're
going in a fly by wire world,

00:44:44.890 --> 00:44:48.510
the manual skills are things
that you can kind of construct,

00:44:48.510 --> 00:44:50.400
technically.

00:44:50.400 --> 00:44:52.720
There's nothing
that says that this

00:44:52.720 --> 00:44:54.679
has to be how you drive a car.

00:44:54.679 --> 00:44:56.720
People have been trying
these things for decades.

00:44:56.720 --> 00:45:01.764
But two joysticks down near
your waist as a possible way

00:45:01.764 --> 00:45:02.680
you can drive the car.

00:45:02.680 --> 00:45:07.050
That's actually how they
flew the lunar module.

00:45:07.050 --> 00:45:10.110
And interfaces need
to be designed well.

00:45:10.110 --> 00:45:13.380
There's always challenges
with human factors issues.

00:45:13.380 --> 00:45:15.190
The regulatory environment
is challenging.

00:45:17.890 --> 00:45:22.030
But it's very hard to picture
a fully autonomous car

00:45:22.030 --> 00:45:24.590
that doesn't have a red
stop button, or at least

00:45:24.590 --> 00:45:27.380
some kind of intervention.

00:45:27.380 --> 00:45:31.200
And if you think about
what that constitutes,

00:45:31.200 --> 00:45:35.820
it constitutes the saying
that the driving task

00:45:35.820 --> 00:45:40.240
has been understood completely,
in some prior space and time,

00:45:40.240 --> 00:45:41.910
by other people.

00:45:41.910 --> 00:45:46.160
And there's no possibility that
you being in the environment

00:45:46.160 --> 00:45:50.530
at the particular time have
nothing to add to the story.

00:45:50.530 --> 00:45:55.030
That's just something we know is
not true, because being there,

00:45:55.030 --> 00:45:57.260
and being the most immediate
person, and the one

00:45:57.260 --> 00:46:01.260
with your rear end on the line,
provides you with information

00:46:01.260 --> 00:46:05.910
that the system
may benefit from.

00:46:05.910 --> 00:46:08.320
And so there are
other ways to think

00:46:08.320 --> 00:46:10.550
about how you might do that.

00:46:14.780 --> 00:46:15.910
AUDIENCE: Hi.

00:46:15.910 --> 00:46:19.220
So if I was half my age
and entering college,

00:46:19.220 --> 00:46:21.320
what field of study
should I go into if I'm

00:46:21.320 --> 00:46:24.160
interested in robotics
and this field?

00:46:24.160 --> 00:46:26.820
Is it more software these
days, or is there still

00:46:26.820 --> 00:46:29.200
a lot of hardware,
or a combination?

00:46:29.200 --> 00:46:31.260
DAVID MINDELL: That's
a great question.

00:46:31.260 --> 00:46:34.090
Obviously the software is
critical, and exciting,

00:46:34.090 --> 00:46:35.640
and interesting.

00:46:35.640 --> 00:46:40.760
And at the same time,
all of these problems

00:46:40.760 --> 00:46:43.700
actually point to
improvements in sensors

00:46:43.700 --> 00:46:46.200
that are needed that
you might not even

00:46:46.200 --> 00:46:49.425
see if you weren't thinking
about them in this way.

00:46:49.425 --> 00:46:50.800
That's one of the
things Humanics

00:46:50.800 --> 00:46:54.570
is going to do is build new
kinds of sensors that enable

00:46:54.570 --> 00:46:56.470
autonomous systems
to work in closer

00:46:56.470 --> 00:46:59.180
proximity to human environments.

00:46:59.180 --> 00:47:00.760
I'm an electrical engineer.

00:47:00.760 --> 00:47:04.110
I like building embedded
systems and I do some of both.

00:47:04.110 --> 00:47:06.780
But I do think that the
hardware innovation is

00:47:06.780 --> 00:47:09.320
equally interesting to
the software innovation.

00:47:09.320 --> 00:47:15.070
I think on just the autonomous
car side, as much at Google

00:47:15.070 --> 00:47:18.090
as anywhere else, there's been
amazing improvement in lidar

00:47:18.090 --> 00:47:21.430
and bringing the cost down,
simply because of the needs

00:47:21.430 --> 00:47:23.760
that driving has presented.

00:47:23.760 --> 00:47:25.660
And lidar has limits.

00:47:25.660 --> 00:47:29.450
So there are other systems that
are going to come out as well.

00:47:29.450 --> 00:47:32.470
I don't know that it breaks
down either hardware or software

00:47:32.470 --> 00:47:34.110
per se.

00:47:34.110 --> 00:47:37.890
I do think that all of those
innovations kind of need

00:47:37.890 --> 00:47:41.470
to be coupled with these higher
level system configurations.

00:47:41.470 --> 00:47:44.890
And one of the things that
was really important to me

00:47:44.890 --> 00:47:48.470
in that AACUS helicopter program
was that what I'm talking about

00:47:48.470 --> 00:47:50.700
is not just an interface issue.

00:47:50.700 --> 00:47:52.470
Yes, we need better interfaces.

00:47:52.470 --> 00:47:55.610
Yes we need good interfaces.

00:47:55.610 --> 00:47:58.430
But no amount of good
interface is going on

00:47:58.430 --> 00:48:01.860
put a Band-Aid on a system
that behaves unpredictably

00:48:01.860 --> 00:48:04.360
or in ways that people
can't relate to.

00:48:04.360 --> 00:48:07.030
It's really about how
you design the system

00:48:07.030 --> 00:48:10.970
and kind of conceptualize
the core autonomy.

00:48:10.970 --> 00:48:15.070
AUDIENCE: So you were talking
about the self-piloting cars

00:48:15.070 --> 00:48:17.430
and how you would never be
comfortable without having

00:48:17.430 --> 00:48:21.380
a big red stop
button in your car.

00:48:21.380 --> 00:48:23.160
And that would be very nice.

00:48:23.160 --> 00:48:24.030
I like that button.

00:48:24.030 --> 00:48:27.265
But what I really want is that
button in everybody else's car.

00:48:27.265 --> 00:48:28.640
DAVID MINDELL:
That you can stop?

00:48:28.640 --> 00:48:29.723
AUDIENCE: That I can stop.

00:48:29.723 --> 00:48:32.640
So if a car comes barreling
down the road with some crazy

00:48:32.640 --> 00:48:38.340
driver, and I cannot react in
a way that will save me inside

00:48:38.340 --> 00:48:41.110
of my vehicle, for
that vehicle to stop.

00:48:41.110 --> 00:48:43.970
That's really my greater
fear when I drive around

00:48:43.970 --> 00:48:47.240
Los Angeles is not the software
or even my own reactions,

00:48:47.240 --> 00:48:49.510
but just the crazy things
that sometimes happen.

00:48:49.510 --> 00:48:51.110
DAVID MINDELL: Yep, that's
certainly true anyway.

00:48:51.110 --> 00:48:51.620
AUDIENCE: In a
way, all the time.

00:48:51.620 --> 00:48:52.995
DAVID MINDELL: No
matter what car

00:48:52.995 --> 00:48:54.967
you make, with no
matter what technology,

00:48:54.967 --> 00:48:56.800
at some level you're
only going to eliminate

00:48:56.800 --> 00:48:58.633
half the accidents,
which are the ones where

00:48:58.633 --> 00:49:01.590
you run into other people.

00:49:01.590 --> 00:49:05.406
A lot of accidents are people
running into things, actually.

00:49:05.406 --> 00:49:07.280
AUDIENCE: Well like,
you can't stop the wall.

00:49:07.280 --> 00:49:09.530
DAVID MINDELL: Well another
way to say it is you

00:49:09.530 --> 00:49:12.842
can build a car that
won't hit anything.

00:49:12.842 --> 00:49:14.800
And if you build a car
that won't hit anything,

00:49:14.800 --> 00:49:18.640
that doesn't mean it needs
to be fully autonomous.

00:49:18.640 --> 00:49:20.970
It can sort of do
collision avoidance

00:49:20.970 --> 00:49:22.160
or collision prevention.

00:49:22.160 --> 00:49:25.160
Those are probably never
going to be perfect.

00:49:25.160 --> 00:49:29.030
My Volvo right now
will slam on the brakes

00:49:29.030 --> 00:49:30.760
if I rear end someone.

00:49:30.760 --> 00:49:31.640
AUDIENCE: Sure.

00:49:31.640 --> 00:49:35.510
But my question is, one thing
the technology can really do

00:49:35.510 --> 00:49:37.992
is it has these
communication protocols that

00:49:37.992 --> 00:49:40.200
are much faster than the
ways humans can communicate.

00:49:40.200 --> 00:49:43.637
So there is a way, using
software, using robots,

00:49:43.637 --> 00:49:45.970
to make the driving experience
in this case considerably

00:49:45.970 --> 00:49:50.260
safer by ensuring other
people won't hit you.

00:49:50.260 --> 00:49:52.430
And that's something
that I think--

00:49:52.430 --> 00:49:55.340
I'm sorry to interrupt you--
is something that is really

00:49:55.340 --> 00:50:00.890
absent from this discussion
right now of all this exercise

00:50:00.890 --> 00:50:03.010
safety potential of
autonomous decision making.

00:50:03.010 --> 00:50:04.830
So how do you think we can
change that conversation?

00:50:04.830 --> 00:50:06.871
DAVID MINDELL: Well I
think that's a great point.

00:50:06.871 --> 00:50:10.090
And again, I think you have
to think about the autonomy as

00:50:10.090 --> 00:50:12.890
situated in a human setting.

00:50:12.890 --> 00:50:16.660
That means other cars, other
drivers with their pluses

00:50:16.660 --> 00:50:21.800
and minuses as
operators, and thinking

00:50:21.800 --> 00:50:23.820
about those relationships,
whether it's

00:50:23.820 --> 00:50:29.380
V to V communications, or radar
sensors, or ultrasonic sensors,

00:50:29.380 --> 00:50:33.200
or whatever, are all things
that you do need to think about.

00:50:33.200 --> 00:50:35.840
I doubt we'll achieve
perfection in any of them.

00:50:35.840 --> 00:50:38.110
But I think there's some
low handing fruit probably

00:50:38.110 --> 00:50:39.380
that can be addressed.

00:50:39.380 --> 00:50:41.307
So by all means.

00:50:41.307 --> 00:50:42.140
AUDIENCE: Thank you.

00:50:44.619 --> 00:50:46.160
DAVID MINDELL: Might
you like to know

00:50:46.160 --> 00:50:49.260
if the person who's
behind you on the highway

00:50:49.260 --> 00:50:52.907
has a history of drunk
driving convictions?

00:50:52.907 --> 00:50:54.240
Yeah, I would like to know that.

00:50:54.240 --> 00:50:57.510
But there are privacy
and equity issues

00:50:57.510 --> 00:50:58.910
around that kind of thing.

00:51:00.569 --> 00:51:02.360
AUDIENCE: So I have a
quick question that's

00:51:02.360 --> 00:51:04.840
related to the hand off issue.

00:51:04.840 --> 00:51:08.840
It seems like to make the
hand off, whether it's planned

00:51:08.840 --> 00:51:12.010
or emergency safe,
you're going to have

00:51:12.010 --> 00:51:15.660
to build up some kind of model
of whether to believe and trust

00:51:15.660 --> 00:51:18.550
the user, the actual
human input or not, which

00:51:18.550 --> 00:51:19.740
stress as problematic.

00:51:19.740 --> 00:51:22.740
Like in a self
driving car situation,

00:51:22.740 --> 00:51:26.840
if something strange
happens, hand off occurs.

00:51:26.840 --> 00:51:29.950
User is maybe not
paying attention

00:51:29.950 --> 00:51:32.430
and does something like slam
on the break in an unsafe way

00:51:32.430 --> 00:51:35.640
or swerve in an unsafe
way, at a certain point

00:51:35.640 --> 00:51:38.990
the machine is going to have to
decide, no, I don't trust you.

00:51:38.990 --> 00:51:40.510
Is that problematic?

00:51:40.510 --> 00:51:41.760
DAVID MINDELL: It is actually.

00:51:41.760 --> 00:51:42.640
It's an old question.

00:51:42.640 --> 00:51:45.300
They asked this question
about the Apollo computer.

00:51:45.300 --> 00:51:47.550
They asked it about
an Airbus airliner.

00:51:47.550 --> 00:51:49.640
Should the computer
allow the person

00:51:49.640 --> 00:51:51.884
to do something dangerous?

00:51:51.884 --> 00:51:53.800
It's a great question,
really interesting one.

00:51:53.800 --> 00:51:55.530
AUDIENCE: And presuming
that you know the difference

00:51:55.530 --> 00:51:56.550
between dangerous and not--

00:51:56.550 --> 00:51:57.716
DAVID MINDELL: Well exactly.

00:51:57.716 --> 00:51:59.120
I mean, that's a human judgment.

00:51:59.120 --> 00:52:00.760
All you're really
saying in that case

00:52:00.760 --> 00:52:03.570
is that I'm shifting
the judgment of what's

00:52:03.570 --> 00:52:07.030
dangerous from the
person in the environment

00:52:07.030 --> 00:52:10.280
to a group of people
sitting around the table

00:52:10.280 --> 00:52:11.590
six months before.

00:52:11.590 --> 00:52:13.970
And there's good
reasons to do that.

00:52:13.970 --> 00:52:16.760
People sitting around the table
are probably not fatigued.

00:52:16.760 --> 00:52:18.660
They have a lot more
time to deliberate.

00:52:18.660 --> 00:52:23.160
There's collectively more brain
power, so on and so forth.

00:52:23.160 --> 00:52:26.110
And at the same time, the
person who's in there, however

00:52:26.110 --> 00:52:29.780
drunk, or tired, or
silly they may be,

00:52:29.780 --> 00:52:32.680
are seeing things that you
couldn't see otherwise.

00:52:32.680 --> 00:52:36.070
That's just a judgment call
really that needs to be made,

00:52:36.070 --> 00:52:40.490
that is being made every
time a parameter is set,

00:52:40.490 --> 00:52:43.700
a threshold is set,
a configuration

00:52:43.700 --> 00:52:45.510
file-- I actually have
a story in the book

00:52:45.510 --> 00:52:48.240
about the configuration
file for one

00:52:48.240 --> 00:52:50.320
of the DARPA Grand
Challenge cars, which

00:52:50.320 --> 00:52:54.010
is a source of a lot of--
it was 1,300 lines long.

00:52:54.010 --> 00:52:56.820
There were 1,300 parameters
that people had to go in

00:52:56.820 --> 00:52:59.300
and manually set to
make that thing work.

00:52:59.300 --> 00:53:02.280
I'm curious what the
modern cars are for that.

00:53:02.280 --> 00:53:03.620
And that's fine.

00:53:03.620 --> 00:53:05.860
And obviously you try to
eliminate them and make

00:53:05.860 --> 00:53:06.720
them more automated.

00:53:06.720 --> 00:53:09.890
But there's still a lot
of tweaking and setting

00:53:09.890 --> 00:53:13.200
that goes on inside of any
kind of AI system these days.

00:53:13.200 --> 00:53:16.890
And those judgments
should be transparent

00:53:16.890 --> 00:53:20.780
and understood who's making
them, why, for what reason.

00:53:20.780 --> 00:53:24.340
That gets as simple as, when
you drive to the grocery store,

00:53:24.340 --> 00:53:27.610
do you want to prioritize
speed of travel,

00:53:27.610 --> 00:53:29.720
fuel efficiency, or safety?

00:53:29.720 --> 00:53:31.200
Very often those
three things are

00:53:31.200 --> 00:53:32.590
in conflict with each other.

00:53:32.590 --> 00:53:34.680
Somebody's got to
make that decision.

00:53:34.680 --> 00:53:38.010
I'm just saying it might as
well be you rather than somebody

00:53:38.010 --> 00:53:39.296
behind closed doors.

00:53:42.284 --> 00:53:43.280
AUDIENCE: Hi.

00:53:43.280 --> 00:53:46.800
I have question regarded
the perfect five scenario.

00:53:46.800 --> 00:53:48.905
Will this be applicable
20 years down the line,

00:53:48.905 --> 00:53:51.700
or is just in the veil for
incremental development

00:53:51.700 --> 00:53:52.282
to autonomy?

00:53:52.282 --> 00:53:53.490
DAVID MINDELL: Applicable to?

00:53:53.490 --> 00:53:54.865
AUDIENCE: 20 years
down the line,

00:53:54.865 --> 00:53:57.237
will perfect five
would still be your--

00:53:57.237 --> 00:53:59.570
DAVID MINDELL: Yeah, I do
actually don't think-- I mean,

00:53:59.570 --> 00:54:03.360
again all the evidence you've
seen in the last 50 years

00:54:03.360 --> 00:54:06.450
is that systems that get
deployed where there are lives

00:54:06.450 --> 00:54:09.380
at stake end up in that state.

00:54:09.380 --> 00:54:14.610
So it's an empirical argument
based on many, many systems

00:54:14.610 --> 00:54:15.520
studies, basically.

00:54:18.240 --> 00:54:22.090
I could be wrong in 20 years.

00:54:22.090 --> 00:54:25.140
I mean, if had told
someone in 1960

00:54:25.140 --> 00:54:30.930
that they could make a computer
with a 10 megahertz clock rate

00:54:30.930 --> 00:54:34.446
and all of three
megabytes of RAM,

00:54:34.446 --> 00:54:36.070
they would have said
of course, that'll

00:54:36.070 --> 00:54:41.280
be plenty to get us
fully autonomous ships.

00:54:41.280 --> 00:54:44.570
And indeed that
happened, and computers

00:54:44.570 --> 00:54:46.390
have done all kinds
of amazing things,

00:54:46.390 --> 00:54:50.140
but there are still
human interventions

00:54:50.140 --> 00:54:51.530
in all of these systems.

00:54:51.530 --> 00:54:55.410
So I could be proven wrong.

00:54:55.410 --> 00:54:57.740
But it is an empirical
argument based

00:54:57.740 --> 00:55:02.635
on how people have done this
for many previous decades.

00:55:02.635 --> 00:55:04.990
AUDIENCE: But incrementally,
wouldn't human intervention

00:55:04.990 --> 00:55:08.160
would be like, we would
kind of forget the basics.

00:55:08.160 --> 00:55:10.280
Like, I came to
US two years back,

00:55:10.280 --> 00:55:12.300
and now I can't drive stick.

00:55:12.300 --> 00:55:15.260
So incrementally, even all these
things start getting added.

00:55:15.260 --> 00:55:18.320
Now all of these
semi-autonomous cars are here,

00:55:18.320 --> 00:55:21.280
and then I might just
forget the very basics

00:55:21.280 --> 00:55:23.660
of having that intervention.

00:55:23.660 --> 00:55:26.680
DAVID MINDELL: And again,
these things all go on.

00:55:26.680 --> 00:55:29.380
And ditto with the
anti-lock brakes.

00:55:29.380 --> 00:55:31.840
I'm sure none of you
ever have to stop on ice,

00:55:31.840 --> 00:55:35.970
but from where I come from, we
all brake on ice all the time.

00:55:35.970 --> 00:55:38.640
And you have to change the
way that you use the brakes

00:55:38.640 --> 00:55:40.260
actually when you have that.

00:55:40.260 --> 00:55:42.205
There are all these
little forms of autonomy

00:55:42.205 --> 00:55:44.010
that come up from the ground up.

00:55:44.010 --> 00:55:47.530
But I would argue they're still
sort of managed by the user.

00:55:47.530 --> 00:55:52.220
And in fact, your car still
has a low gear one and low gear

00:55:52.220 --> 00:55:52.760
two.

00:55:52.760 --> 00:55:56.010
I don't know if all cars have
that, but most cars have that.

00:55:56.010 --> 00:55:58.390
And so you have
the automatic mode.

00:55:58.390 --> 00:56:01.105
And then for whatever
reasons, again, it's

00:56:01.105 --> 00:56:03.140
no, sometimes there are reasons.

00:56:03.140 --> 00:56:04.800
You can go into those low gears.

00:56:04.800 --> 00:56:08.570
So you're still have a gear
shift in the car, interestingly

00:56:08.570 --> 00:56:09.310
enough.

00:56:09.310 --> 00:56:10.510
AUDIENCE: Thank you.

00:56:10.510 --> 00:56:10.740
DAVID MINDELL: Great.

00:56:10.740 --> 00:56:11.823
Thanks for your attention.

00:56:11.823 --> 00:56:13.330
[APPLAUSE]

