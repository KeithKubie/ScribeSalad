WEBVTT
Kind: captions
Language: en

00:00:00.470 --> 00:00:03.080
That's sort of how to do
sampling efficiently.

00:00:03.080 --> 00:00:07.240
The other thing is,
how often do you have to resample?

00:00:07.240 --> 00:00:10.810
Well, in the algorithm that I showed
you, we resample all the time, and

00:00:10.810 --> 00:00:14.340
that's sort of we have a whole bunch of
samples that are about uniform weight,

00:00:14.340 --> 00:00:15.240
and we spread them out.

00:00:16.379 --> 00:00:20.370
If I don't have too many samples
that have very large weights,

00:00:20.370 --> 00:00:21.550
maybe I don't have to resample.

00:00:21.550 --> 00:00:25.490
Maybe I could just reuse the weights
the same samples over again.

00:00:25.490 --> 00:00:26.090
Right?
I'd still have to

00:00:26.090 --> 00:00:30.270
apply the deterministic part, and
I have to apply the expansion part, but

00:00:30.270 --> 00:00:32.320
I don't have to do that resampling.

00:00:32.320 --> 00:00:34.960
And you can do that by just saying,

00:00:34.960 --> 00:00:37.580
let me take a look at sort of
the variation in the weights.

00:00:37.580 --> 00:00:39.480
I want them as uniform as possible.

00:00:39.480 --> 00:00:42.010
And as long as they have a certain
amount of uniformity to them,

00:00:42.010 --> 00:00:42.910
I won't resample.

00:00:42.910 --> 00:00:45.640
So that reduces the number
of times you do resampling.

00:00:45.640 --> 00:00:49.990
A second problem is
what if p of z given x.

00:00:49.990 --> 00:00:53.180
What if your likelihood
is incredibly peaked.

00:00:53.180 --> 00:00:57.750
Remember, you multiply the likelihood
times the weights of the samples.

00:00:57.750 --> 00:00:58.990
If it's incredibly peaked,

00:00:58.990 --> 00:01:02.369
that means that p of z given x
is zero almost everywhere else.

00:01:03.500 --> 00:01:06.270
That's not very helpful,
because what that means is,

00:01:06.270 --> 00:01:12.360
you've wiped out all of your your,
your possible predictions.

00:01:12.360 --> 00:01:17.380
In general, you want to allow for
noise in your p of z given x.

00:01:17.380 --> 00:01:20.370
Don't be so systematic about it.

00:01:20.370 --> 00:01:23.380
And you also might want to

00:01:23.380 --> 00:01:27.710
be better at sort of how you do your
your proposals to begin with, right?

00:01:27.710 --> 00:01:30.760
So you want to make sure
that particle filter

00:01:30.760 --> 00:01:32.510
sort of spreads out to
points where to begin with.

00:01:32.510 --> 00:01:35.650
And one way of doing that actually
is if you think of a sample,

00:01:35.650 --> 00:01:39.080
each sample you can think of as having
its own little mini Kalman filter.

00:01:39.080 --> 00:01:43.240
So you can think of it as having a
Gaussian that sort of spreads itself out

00:01:43.240 --> 00:01:44.110
and goes forward.

00:01:44.110 --> 00:01:49.440
In general, you're much better off
overestimating noise and let your

00:01:49.440 --> 00:01:54.390
measurements sort of narrow you down,
than underestimating noise and have

00:01:54.390 --> 00:01:57.140
your measurements think that they're
much more accurate than they are.

00:01:57.140 --> 00:01:59.680
Or I should say much more
constraining than they are.

00:01:59.680 --> 00:02:01.650
Another issue is recovery from failure.

00:02:01.650 --> 00:02:05.440
Suppose an object really
did disappear and reappear.

00:02:05.440 --> 00:02:09.840
If I have no particles there, I don't
have any way of tracking it, ever.

00:02:09.840 --> 00:02:14.460
So, a standard thing to do is
each iteration, you randomly put

00:02:14.460 --> 00:02:18.210
out some particles everywhere, or
sort of uniformly distribute it.

00:02:18.210 --> 00:02:21.720
Because if my measurements
suddenly have moved over here,

00:02:21.720 --> 00:02:24.120
there'll be some particles over there,
and

00:02:24.120 --> 00:02:28.500
eventually that will be
the distribution that will be tracked.

00:02:28.500 --> 00:02:33.780
So those three issues of sort
of making sure you're smart and

00:02:33.780 --> 00:02:35.910
efficient about your resampling.

00:02:35.910 --> 00:02:39.760
Not letting your system think that
measurements are too precise, and making

00:02:39.760 --> 00:02:43.100
sure you have enough noise so that you
don't kill off samples too quickly.

00:02:43.100 --> 00:02:46.970
And this idea of having some
random samples distributed in a,

00:02:46.970 --> 00:02:51.720
in a useful way, so you can track things
if things change very unexpectedly.

00:02:51.720 --> 00:02:54.380
All these things are necessary
to make particle filtering work.

