WEBVTT
Kind: captions
Language: en

00:00:00.090 --> 00:00:00.940
NICK: Hi, everyone.

00:00:00.940 --> 00:00:02.050
My name is Nick.

00:00:02.050 --> 00:00:04.290
I am a engineer on
the TensorBoard team.

00:00:04.290 --> 00:00:09.950
And I'm here today to talk
about TensorBoard and Summaries.

00:00:09.950 --> 00:00:12.860
So first off, just an outline
of what I'll be talking about.

00:00:12.860 --> 00:00:16.820
First, I'll give an overview
of TensorBoard, what it is

00:00:16.820 --> 00:00:21.110
and how it works, just
mostly sort of as background.

00:00:21.110 --> 00:00:24.920
Then I'll talk for a bit
about the tf.summary APIs.

00:00:24.920 --> 00:00:30.423
In particular, how they've
evolved from TF 1.x to TF 2.0.

00:00:30.423 --> 00:00:32.090
And then finally,
I'll talk a little bit

00:00:32.090 --> 00:00:35.630
about the summary data format,
log directories, event files,

00:00:35.630 --> 00:00:39.200
some best practices and tips.

00:00:39.200 --> 00:00:41.530
So let's go ahead
and get started.

00:00:41.530 --> 00:00:43.780
So TensorBoard--
hopefully, most of you

00:00:43.780 --> 00:00:45.190
have heard of TensorBoard.

00:00:45.190 --> 00:00:47.410
If you haven't, it's the
visualization toolkit

00:00:47.410 --> 00:00:48.290
for TensorFlow.

00:00:48.290 --> 00:00:52.660
That's a picture of the
web UI on the right.

00:00:52.660 --> 00:00:55.360
Typically, you run this from the
command line as the TensorBoard

00:00:55.360 --> 00:00:55.930
command.

00:00:55.930 --> 00:00:57.140
It prints out a URL.

00:00:57.140 --> 00:00:58.835
You view it in your browser.

00:00:58.835 --> 00:01:01.210
And from there on, you have
a bunch of different controls

00:01:01.210 --> 00:01:03.940
and visualizations.

00:01:03.940 --> 00:01:07.210
And the sort of key selling
point of TensorBoard

00:01:07.210 --> 00:01:11.050
is that it provides cool
visualizations out of the box,

00:01:11.050 --> 00:01:14.120
without a lot of extra work.

00:01:14.120 --> 00:01:15.970
you basically can just
run it on your data

00:01:15.970 --> 00:01:19.570
and get a bunch of
different kinds of tools

00:01:19.570 --> 00:01:23.900
and different sort of
analyses you can do.

00:01:23.900 --> 00:01:27.190
So let's dive into the
parts of TensorBoard

00:01:27.190 --> 00:01:29.975
from the user
perspective a little bit.

00:01:29.975 --> 00:01:31.600
First off, there's
multiple dashboards.

00:01:31.600 --> 00:01:33.430
So we have this
sort of tabs setup

00:01:33.430 --> 00:01:34.937
with dashboards across the top.

00:01:34.937 --> 00:01:37.270
In the screenshot, it shows
the scalers dashboard, which

00:01:37.270 --> 00:01:39.550
is kind of the default one.

00:01:39.550 --> 00:01:42.750
But there's also dashboards
for images, histogram, graphs,

00:01:42.750 --> 00:01:49.570
a whole bunch more are being
added every month almost.

00:01:49.570 --> 00:01:52.090
And one thing that many of
the dashboards have in common

00:01:52.090 --> 00:01:54.370
is this ability to
sort of slice and dice

00:01:54.370 --> 00:01:56.700
your data by run and by tag.

00:01:56.700 --> 00:01:58.450
And a run, you can
think of that as a sign

00:01:58.450 --> 00:02:00.430
of a run of your
TensorFlow program,

00:02:00.430 --> 00:02:02.860
or your TensorFlow job.

00:02:02.860 --> 00:02:07.510
And a tag corresponds to
a specific named metric,

00:02:07.510 --> 00:02:09.949
or a piece of summary data.

00:02:09.949 --> 00:02:11.910
So here, the runs,
we have a train

00:02:11.910 --> 00:02:16.090
and evolve run on the lower
left corner in the run selector.

00:02:16.090 --> 00:02:18.760
And then we have different tags,
including the cross [INAUDIBLE]

00:02:18.760 --> 00:02:20.890
tag is the one being visualized.

00:02:20.890 --> 00:02:23.825
And one more thing I'll
mention is that one thing a lot

00:02:23.825 --> 00:02:25.450
of TensorBoard
emphasizes is seeing how

00:02:25.450 --> 00:02:27.020
your data changes over time.

00:02:27.020 --> 00:02:30.910
So most of the data takes
the form of a time series.

00:02:30.910 --> 00:02:32.920
And in this case, with
the scalers dashboard,

00:02:32.920 --> 00:02:36.130
the time series is sort of as
a step count across the x-axis.

00:02:39.170 --> 00:02:41.740
So we might ask, what's
going on behind the scenes

00:02:41.740 --> 00:02:45.290
to make this all come together?

00:02:45.290 --> 00:02:50.890
And so here is our architecture
diagram for TensorBoard.

00:02:50.890 --> 00:02:54.850
We'll start over on the left
with your TensorFlow job.

00:02:54.850 --> 00:02:58.540
It writes data to disk
using the tf.summary API.

00:02:58.540 --> 00:03:01.960
And we'll talk both about the
summary API and the event file

00:03:01.960 --> 00:03:03.740
format a little more later.

00:03:03.740 --> 00:03:06.550
Then the center component
is TensorBoard itself.

00:03:06.550 --> 00:03:10.000
We have a background thread
that loads event file data.

00:03:10.000 --> 00:03:12.430
And because the event
file data itself

00:03:12.430 --> 00:03:17.590
isn't efficient for querying,
we construct a subsample

00:03:17.590 --> 00:03:20.680
of the data and memory that
we can query more efficiently.

00:03:20.680 --> 00:03:24.580
And then the rest so
TensorBoard is a web server that

00:03:24.580 --> 00:03:26.510
has a plugin architecture.

00:03:26.510 --> 00:03:30.320
So each dashboard
on the frontend--

00:03:30.320 --> 00:03:32.200
as a backend, it has
a specific plugin

00:03:32.200 --> 00:03:34.400
backend So for example,
the scalers dashboard talks

00:03:34.400 --> 00:03:37.390
to a scalers backend,
images to an image backend.

00:03:37.390 --> 00:03:41.380
And this allows the backends to
do pre-processing or otherwise

00:03:41.380 --> 00:03:43.090
structure the data
in an appropriate way

00:03:43.090 --> 00:03:45.610
for the frontend to display.

00:03:45.610 --> 00:03:48.770
And then each plugin has a
frontend dashboard component,

00:03:48.770 --> 00:03:52.030
which are all compiled
together by TensorBoard

00:03:52.030 --> 00:03:56.227
and served as a single
page and index.html.

00:03:56.227 --> 00:03:58.810
And that page communicates back
and forth through the backends

00:03:58.810 --> 00:04:01.905
through standard HTTP requests.

00:04:01.905 --> 00:04:04.030
And then finally, hopefully,
we have our happy user

00:04:04.030 --> 00:04:06.440
on the other end
seeing their data,

00:04:06.440 --> 00:04:10.780
analyzing it, getting
useful insights.

00:04:10.780 --> 00:04:13.570
And I'll talk a little more
about just some details

00:04:13.570 --> 00:04:15.975
about the frontend.

00:04:15.975 --> 00:04:18.100
The front end is built on
the Polymer web component

00:04:18.100 --> 00:04:21.269
framework, where you
define custom elements.

00:04:21.269 --> 00:04:25.800
So the entirety of TensorBoard
is one large custom element,

00:04:25.800 --> 00:04:27.880
tf-tensorboard.

00:04:27.880 --> 00:04:29.710
But that's just the top.

00:04:29.710 --> 00:04:32.800
From there on, each
plugin front end is--

00:04:32.800 --> 00:04:35.960
each dashboard is its
own frontend component.

00:04:35.960 --> 00:04:38.740
For example, there's
a tf-scaler dashboard.

00:04:38.740 --> 00:04:40.930
And then all the way
down to shared components

00:04:40.930 --> 00:04:43.280
for more basic UI elements.

00:04:43.280 --> 00:04:46.450
So we can think of this as
a button, or a selector,

00:04:46.450 --> 00:04:51.330
or a card element, or
a collapsible pane.

00:04:51.330 --> 00:04:53.920
And these components are shared
across many of the dashboards.

00:04:53.920 --> 00:04:57.340
And that's one of the key
ways in which TensorBoard

00:04:57.340 --> 00:05:00.520
achieves what is hopefully
a somewhat uniform look

00:05:00.520 --> 00:05:04.540
and feel from
dashboard to dashboard.

00:05:04.540 --> 00:05:06.760
The actual logic
for these components

00:05:06.760 --> 00:05:08.073
is implemented in JavaScript.

00:05:08.073 --> 00:05:09.490
Some of that's
actually TypeScript

00:05:09.490 --> 00:05:11.350
that we compile to JavaScript.

00:05:11.350 --> 00:05:13.570
Especially the more
complicated visualizations,

00:05:13.570 --> 00:05:16.900
TypeScript helps build
them up as libraries

00:05:16.900 --> 00:05:20.350
without having to worry
about some of the pitfalls

00:05:20.350 --> 00:05:23.140
you might get writing
them in pure JavaScript.

00:05:23.140 --> 00:05:25.180
And then the actual
visualizations

00:05:25.180 --> 00:05:28.960
are a mix of different
implementations.

00:05:28.960 --> 00:05:30.820
Many of them use
Plottable, which

00:05:30.820 --> 00:05:34.270
is a wrapper library over the
D3, the standard JavaScript

00:05:34.270 --> 00:05:35.470
visualization library.

00:05:35.470 --> 00:05:37.720
Some of them use native D3.

00:05:37.720 --> 00:05:40.390
And then for some of the
more complex visualizations,

00:05:40.390 --> 00:05:43.580
there are libraries that do
some of the heavy lifting.

00:05:43.580 --> 00:05:45.620
So the graph
visualization, for example,

00:05:45.620 --> 00:05:49.930
uses a directed graph
library to do layout.

00:05:49.930 --> 00:05:53.080
The projector uses a
WebGL wrapper library

00:05:53.080 --> 00:05:55.200
to do the 3D visualizations.

00:05:55.200 --> 00:05:57.190
And the recently introduced
What-If Tool plugin

00:05:57.190 --> 00:06:00.250
uses the facets library
from [INAUDIBLE] folks.

00:06:00.250 --> 00:06:03.220
So we bring a whole bunch
of different visualization

00:06:03.220 --> 00:06:07.840
technologies together under
one TensorBoard umbrella

00:06:07.840 --> 00:06:11.170
is how you can think
about the frontend.

00:06:11.170 --> 00:06:16.120
So now that we have a overview
of TensorBoard itself,

00:06:16.120 --> 00:06:19.480
I'll talk about how your data
actually gets to TensorBoard.

00:06:19.480 --> 00:06:23.290
So how do you unlock all
of this functionality?

00:06:23.290 --> 00:06:30.580
And the spoiler announcement
to that is the tf.summary API.

00:06:30.580 --> 00:06:33.310
So to summarize the
summary API, you

00:06:33.310 --> 00:06:37.240
can think of it as structured
logging for your model.

00:06:37.240 --> 00:06:39.890
The goal is really to make it
easy to instrument your model

00:06:39.890 --> 00:06:40.390
code.

00:06:40.390 --> 00:06:43.900
So to allow you to
log metrics, weights,

00:06:43.900 --> 00:06:47.350
details about predictions,
input data, performance metrics,

00:06:47.350 --> 00:06:50.620
pretty much anything that
you might want to instrument.

00:06:50.620 --> 00:06:52.120
And you can log
these all, save them

00:06:52.120 --> 00:06:54.430
to disk for later analysis.

00:06:54.430 --> 00:06:58.240
And you won't necessarily always
be calling the summary API

00:06:58.240 --> 00:06:59.030
directly.

00:06:59.030 --> 00:07:01.730
Some frameworks call
the summary API for you.

00:07:01.730 --> 00:07:05.670
So for examples, estimator
has the summary saver hook.

00:07:05.670 --> 00:07:07.390
Keras has a
TensorBoard callback,

00:07:07.390 --> 00:07:11.230
which takes care of some
of the nitty gritty.

00:07:11.230 --> 00:07:14.230
But underlying that is
still the summary API.

00:07:14.230 --> 00:07:17.860
So most data gets to
TensorBoard in this way.

00:07:17.860 --> 00:07:19.810
There are some exceptions.

00:07:19.810 --> 00:07:22.240
Some dashboards have
different data flows.

00:07:22.240 --> 00:07:24.160
The debugger is a
good example of this.

00:07:24.160 --> 00:07:26.410
The debugger dashboard
integrates with tfdbg.

00:07:26.410 --> 00:07:29.110
It has a separate back
channel that it uses

00:07:29.110 --> 00:07:30.250
to communicate information.

00:07:30.250 --> 00:07:31.960
It doesn't use the summary API.

00:07:31.960 --> 00:07:37.220
But many of the commonly
used dashboards do.

00:07:37.220 --> 00:07:41.260
And so the summary API
actually has sort of--

00:07:41.260 --> 00:07:43.030
there's several variations.

00:07:43.030 --> 00:07:44.900
And when talking
about the variations,

00:07:44.900 --> 00:07:49.300
it's useful to think of the
API as having two basic halves.

00:07:49.300 --> 00:07:52.880
On one half we have the
instrumentation surface.

00:07:52.880 --> 00:07:54.730
So these logging
these are like logging

00:07:54.730 --> 00:07:57.460
ops that you place
in your model code.

00:07:57.460 --> 00:07:59.170
They're pretty
familiar to people

00:07:59.170 --> 00:08:01.990
who have used the summary API,
things like scaler, histogram,

00:08:01.990 --> 00:08:03.860
image.

00:08:03.860 --> 00:08:06.040
And then the other
half of the summary API

00:08:06.040 --> 00:08:08.770
is about writing that
log data to disk.

00:08:08.770 --> 00:08:11.230
And creating a
specially formatted log

00:08:11.230 --> 00:08:16.420
file which TensorBoard can
read and extract the data from.

00:08:16.420 --> 00:08:19.750
And so, just to give a
sense of how those relate

00:08:19.750 --> 00:08:21.700
to the different
versions, there's

00:08:21.700 --> 00:08:27.970
four variations of the summary
API from TF 1.x to 2.0.

00:08:27.970 --> 00:08:31.640
And the two key dimensions
on which they vary

00:08:31.640 --> 00:08:34.250
are the instrumentation
side and the writing side.

00:08:34.250 --> 00:08:36.950
And we'll go into
this in more detail.

00:08:36.950 --> 00:08:40.929
But first off, let's start
with the most familiar summary

00:08:40.929 --> 00:08:43.419
API from TF 1.x.

00:08:43.419 --> 00:08:46.750
So just as a review--
again, if you've

00:08:46.750 --> 00:08:49.100
used the summary API before,
this will look familiar.

00:08:49.100 --> 00:08:50.770
But this is kind
of a code sample

00:08:50.770 --> 00:08:53.950
of using the summary API 1.x.

00:08:53.950 --> 00:08:57.550
The instrumentation ops, like
scaler, actually output summary

00:08:57.550 --> 00:08:58.600
protos directly.

00:08:58.600 --> 00:09:01.150
And then those are
merged together

00:09:01.150 --> 00:09:06.280
by a merge all op that generates
a combined proto output.

00:09:06.280 --> 00:09:09.550
The combined output, you can
fetch using session dot run.

00:09:09.550 --> 00:09:12.610
And then, that output, you
can write to a File Writer

00:09:12.610 --> 00:09:15.310
for a particular
log directory using

00:09:15.310 --> 00:09:18.850
this add summary call that
takes the summary proto itself

00:09:18.850 --> 00:09:21.640
and also a step.

00:09:21.640 --> 00:09:24.000
So this is, in a
nutshell, the flow

00:09:24.000 --> 00:09:27.430
for TF 1.x summary writing.

00:09:27.430 --> 00:09:30.540
There's some limitations
to this, which

00:09:30.540 --> 00:09:33.220
I'll describe in two parts.

00:09:33.220 --> 00:09:36.210
The first set of limitations
has to do with the kinds of data

00:09:36.210 --> 00:09:37.930
types that we can support.

00:09:37.930 --> 00:09:43.830
So in TF 1.x, there's a
fixed set of data types.

00:09:43.830 --> 00:09:47.100
And adding new ones
is a little involved.

00:09:47.100 --> 00:09:49.750
It requires changes to
TensorFlow in terms of you

00:09:49.750 --> 00:09:51.600
would need a new proto
definition field.

00:09:51.600 --> 00:09:56.820
You'd need a new op definition,
a new kernel, and a new Python

00:09:56.820 --> 00:09:59.340
API symbol.

00:09:59.340 --> 00:10:02.040
And this is a barrier
to sensibility

00:10:02.040 --> 00:10:05.700
for adding new data types
to support new TensorBoard

00:10:05.700 --> 00:10:06.750
plugins.

00:10:06.750 --> 00:10:10.260
It's led people to do
creative workarounds.

00:10:10.260 --> 00:10:13.180
For example, like
rendering a matplotlib plot

00:10:13.180 --> 00:10:14.670
in your training code.

00:10:14.670 --> 00:10:17.100
And then logging it
as an image summary.

00:10:17.100 --> 00:10:21.420
And the prompt here
is, what if we instead

00:10:21.420 --> 00:10:24.120
had a single op or a
set of ops that could

00:10:24.120 --> 00:10:28.080
generalize across data formats?

00:10:28.080 --> 00:10:31.030
And this brings us to
our first variation.

00:10:31.030 --> 00:10:33.510
Which is the
TensorBoard summary API,

00:10:33.510 --> 00:10:38.280
where we try and make this
extensible to new data types.

00:10:38.280 --> 00:10:41.370
And the TensorBoard
API, the mechanism

00:10:41.370 --> 00:10:44.010
here is that we use the tensor
itself as a generic data

00:10:44.010 --> 00:10:46.060
container.

00:10:46.060 --> 00:10:47.710
Which can correspond to--

00:10:47.710 --> 00:10:50.880
for example, we can represent
a histogram, an image,

00:10:50.880 --> 00:10:51.990
scaler itself.

00:10:51.990 --> 00:10:56.820
We can represent these all in
certain formats as tensors.

00:10:56.820 --> 00:11:01.320
And what this lets us do is
use a shared tensor summary API

00:11:01.320 --> 00:11:03.330
with some metadata
that we can use

00:11:03.330 --> 00:11:07.890
to describe the tensor
format for our one place

00:11:07.890 --> 00:11:11.160
to send summary data.

00:11:11.160 --> 00:11:13.290
So TensorBoard.summary,
the principle it takes

00:11:13.290 --> 00:11:17.610
is actually that you can
reimplement the tf.summary ops

00:11:17.610 --> 00:11:20.700
and APIs as Python
logic to call TensorFlow

00:11:20.700 --> 00:11:24.630
ops for pre-processing and
then a call to tensor summary.

00:11:24.630 --> 00:11:27.000
And this is a win in the
sense that you no longer need

00:11:27.000 --> 00:11:30.870
individual C++ kernels and proto
fields for each individual data

00:11:30.870 --> 00:11:32.770
type.

00:11:32.770 --> 00:11:34.860
So the TensorBoard plugins
today actually do this.

00:11:34.860 --> 00:11:36.000
They have for a while.

00:11:36.000 --> 00:11:40.410
They have their own summary
ops defined in TensorBoard.

00:11:40.410 --> 00:11:43.230
And the result of this has
been that for a new TensorBoard

00:11:43.230 --> 00:11:45.392
plugins, where this
is the only option,

00:11:45.392 --> 00:11:46.850
there's been quite
a bit of uptake.

00:11:46.850 --> 00:11:51.710
For example, the pr_curve
plugin has a pr_curve summary.

00:11:51.710 --> 00:11:54.020
And that's the main
route people use.

00:11:54.020 --> 00:11:56.520
But for existing
data types, there

00:11:56.520 --> 00:11:59.770
isn't really much reason
to stop using tf.summary.

00:11:59.770 --> 00:12:01.780
And so, for those,
it makes sense.

00:12:01.780 --> 00:12:04.170
That's been what
people have used.

00:12:04.170 --> 00:12:09.990
But then tf.summary, it still
has some other limitations.

00:12:09.990 --> 00:12:12.470
And so that's what we're
going to look at next.

00:12:15.330 --> 00:12:20.100
So the second set of
limitations in tf.summary

00:12:20.100 --> 00:12:23.190
is around this requirement
that the summary data

00:12:23.190 --> 00:12:25.650
flows through the graph itself.

00:12:25.650 --> 00:12:30.240
So merge_all uses the hidden
graph collection essentially

00:12:30.240 --> 00:12:33.150
to achieve the
effect to the user

00:12:33.150 --> 00:12:36.450
as though your summary ops have
side effects of writing data.

00:12:36.450 --> 00:12:38.250
Kind of like a
conventional-- the way

00:12:38.250 --> 00:12:41.190
you use a standard logging API.

00:12:41.190 --> 00:12:43.620
But because it's using
a graph collection,

00:12:43.620 --> 00:12:46.440
it's not really safe for
use inside control flow

00:12:46.440 --> 00:12:47.940
and functions.

00:12:47.940 --> 00:12:51.600
And also, with eager execution,
it's very cumbersome to use.

00:12:51.600 --> 00:12:53.400
You would have to
keep track of outputs

00:12:53.400 --> 00:12:58.580
by hand or in some way wait
to send them to the writer.

00:12:58.580 --> 00:13:03.150
And these limitations also
apply to TensorBoard.summary ops

00:13:03.150 --> 00:13:04.320
themselves.

00:13:04.320 --> 00:13:07.020
Because they don't really change
anything about the writing

00:13:07.020 --> 00:13:08.520
structure.

00:13:08.520 --> 00:13:12.240
And these limitations have
sort of led to the prompt of,

00:13:12.240 --> 00:13:14.940
what about if summary
recording was an actual side

00:13:14.940 --> 00:13:19.800
effect of op execution?

00:13:19.800 --> 00:13:22.530
And so this brings us
to tf.contrib summary,

00:13:22.530 --> 00:13:25.170
which has new writing
logic that achieves this.

00:13:28.010 --> 00:13:30.880
And so here's a code sample
for tf.contrib summary,

00:13:30.880 --> 00:13:34.630
which looks pretty different
from the original TF summary.

00:13:34.630 --> 00:13:37.210
It works with eager execution.

00:13:37.210 --> 00:13:40.930
But the change we
have to make is now

00:13:40.930 --> 00:13:44.727
we create a writer upfront
via create_file_writer.

00:13:44.727 --> 00:13:46.810
It's still tied to a
specific log directory, which

00:13:46.810 --> 00:13:48.910
we'll talk more about later.

00:13:48.910 --> 00:13:52.120
You set the writer as the
default writer in the context.

00:13:52.120 --> 00:13:54.100
You enable summary recording.

00:13:54.100 --> 00:13:57.610
And then the individual
instrumentation ops

00:13:57.610 --> 00:14:01.060
will actually write directly
to the writer when they run.

00:14:01.060 --> 00:14:05.740
So this gives you standard
usage pattern of a logging

00:14:05.740 --> 00:14:06.970
API that you would expect.

00:14:06.970 --> 00:14:09.160
And it's compatible
with eager execution

00:14:09.160 --> 00:14:10.540
and also with graph execution.

00:14:13.560 --> 00:14:18.180
So some details to how this
works with contrib summaries.

00:14:18.180 --> 00:14:21.990
The writer is backed in the
TensorFlow runtime by a C++

00:14:21.990 --> 00:14:24.450
resource called
SummaryWriterInterface.

00:14:24.450 --> 00:14:27.510
That essentially encapsulates
the actual writing logic.

00:14:27.510 --> 00:14:29.010
Which makes it
possible in principle

00:14:29.010 --> 00:14:32.250
to have different
implementations of this.

00:14:32.250 --> 00:14:37.680
The default writer, as conceived
of by the Python code that's

00:14:37.680 --> 00:14:40.680
executing, is just a
handle to that resource

00:14:40.680 --> 00:14:43.080
stored in the context.

00:14:43.080 --> 00:14:47.100
And then instrumentation
ops, like scaler and image,

00:14:47.100 --> 00:14:48.210
now are stateful ops.

00:14:48.210 --> 00:14:50.010
They have side effects.

00:14:50.010 --> 00:14:53.910
And they achieve this by taking
the default writer handle

00:14:53.910 --> 00:14:56.850
as input along with the data
they're supposed to write.

00:14:56.850 --> 00:14:59.370
And then the actual op kernel
implements the writing using

00:14:59.370 --> 00:15:02.400
the C++ resource object.

00:15:02.400 --> 00:15:05.580
And with this model, the
Python writer objects

00:15:05.580 --> 00:15:08.370
mostly manage this state.

00:15:08.370 --> 00:15:11.790
They don't quite completely
align because the C++ resource

00:15:11.790 --> 00:15:13.968
could actually be shared
across Python objects.

00:15:13.968 --> 00:15:15.510
Which is a little
bit different still

00:15:15.510 --> 00:15:18.390
from the TensorFlow
2.0 paradigm,

00:15:18.390 --> 00:15:22.290
where we want our Python
state to reflect runtime state

00:15:22.290 --> 00:15:24.180
1 to 1.

00:15:24.180 --> 00:15:26.850
And this was just one
example of a few things

00:15:26.850 --> 00:15:30.600
that we're changing with TF 2.0.

00:15:30.600 --> 00:15:33.660
And with TF 2.0, we
had this opportunity

00:15:33.660 --> 00:15:37.260
to stitch some of these
features together and make

00:15:37.260 --> 00:15:42.210
one unified new tf.summary API.

00:15:42.210 --> 00:15:44.880
And so here we are
completing our filling

00:15:44.880 --> 00:15:47.280
out of the space
of possibilities

00:15:47.280 --> 00:15:50.430
where we have the tf.summary
API bringing together features

00:15:50.430 --> 00:15:58.070
from really all three of
the existing APIs in 2.0.

00:15:58.070 --> 00:16:01.390
So TF summary and TF 2.0,
it really represents,

00:16:01.390 --> 00:16:05.190
like I said, this unification
of the three different APIs.

00:16:05.190 --> 00:16:07.030
The instrumentation
ops are actually

00:16:07.030 --> 00:16:08.530
provided by TensorBoard.

00:16:08.530 --> 00:16:11.770
And they use this generic
Tensor data format.

00:16:11.770 --> 00:16:15.340
That's the same format
as SensorBoard.summary.

00:16:15.340 --> 00:16:18.250
Which lets them extend to
multiple different kinds

00:16:18.250 --> 00:16:20.290
of data types.

00:16:20.290 --> 00:16:23.410
We borrowed the implementation
of the writing logic

00:16:23.410 --> 00:16:26.020
from tf.contrib summary
and some of the APIs.

00:16:26.020 --> 00:16:28.000
But with slightly
adjusted semantics in some

00:16:28.000 --> 00:16:32.230
places, mostly just so that
we align with the state

00:16:32.230 --> 00:16:34.870
management in TF 2.0.

00:16:34.870 --> 00:16:37.630
And then there's actually
a not trivial amount

00:16:37.630 --> 00:16:40.480
of just glue and
circular import fixes

00:16:40.480 --> 00:16:43.540
to get the two halves
of both TensorBoard

00:16:43.540 --> 00:16:45.760
and the original
TF summary writing

00:16:45.760 --> 00:16:47.770
API to talk to each other.

00:16:47.770 --> 00:16:51.920
And I'll go into a little
bit more detail about that.

00:16:51.920 --> 00:16:55.300
So the dependency structure
for tf.summary and TF 2.0

00:16:55.300 --> 00:16:58.880
is a little bit complicated.

00:16:58.880 --> 00:17:01.660
The actual Python module
contains API symbols

00:17:01.660 --> 00:17:06.109
from both TensorBoard and
TensorFlow fused together.

00:17:06.109 --> 00:17:08.140
But because the
TensorBoard symbols also

00:17:08.140 --> 00:17:10.780
depend on TensorFlow,
this creates

00:17:10.780 --> 00:17:13.599
this complicated
dependency relationship.

00:17:13.599 --> 00:17:17.920
And the way we linearize
this dependency relationship

00:17:17.920 --> 00:17:24.819
is that tf.summary in the
original TensorFlow code

00:17:24.819 --> 00:17:28.630
exposes the writing APIs,
like create_file_writer

00:17:28.630 --> 00:17:34.390
and the actual
underlying writing logic.

00:17:34.390 --> 00:17:37.930
Then we have what I call a
shim module in TensorFlow,

00:17:37.930 --> 00:17:41.590
that merges those symbols
via wildcard import

00:17:41.590 --> 00:17:45.820
with the regular imports of
the instrumentation APIs,

00:17:45.820 --> 00:17:49.510
like scaler and image that are
now defined in TensorBoard.

00:17:49.510 --> 00:17:52.600
And this produces the
combined namespace.

00:17:52.600 --> 00:17:55.250
But now it's a
TensorBoard module.

00:17:55.250 --> 00:17:59.380
So then TensorFlow, in it's
top level init__.py where

00:17:59.380 --> 00:18:01.840
it's assembling
the API together,

00:18:01.840 --> 00:18:04.640
imports the TensorBoard module
and sets that as the new

00:18:04.640 --> 00:18:07.040
tf.summary.

00:18:07.040 --> 00:18:11.330
And this does mean that the
API service depends directly

00:18:11.330 --> 00:18:12.230
on TensorBoard.

00:18:12.230 --> 00:18:15.250
But TensorFlow already has a
pip dependency on TensorBoard.

00:18:15.250 --> 00:18:18.120
So this isn't really a
change in that respect.

00:18:18.120 --> 00:18:20.630
But the API surface is
now being constructed

00:18:20.630 --> 00:18:23.750
through multiple components,
where the summary component is

00:18:23.750 --> 00:18:25.940
provided by TensorBoard.

00:18:25.940 --> 00:18:28.580
And what that gives us
is a single module that

00:18:28.580 --> 00:18:30.790
combines both sets of symbols.

00:18:30.790 --> 00:18:34.400
So that the delta
for users is smaller.

00:18:34.400 --> 00:18:39.390
But we can have the code live in
the appropriate places for now.

00:18:39.390 --> 00:18:43.220
And so these are some code
samples for the TF 2.0 summary

00:18:43.220 --> 00:18:44.220
API.

00:18:44.220 --> 00:18:47.120
The first one shows it
under eager execution.

00:18:47.120 --> 00:18:50.220
It should look fairly
similar to contrib.

00:18:50.220 --> 00:18:53.360
You create the writer
upfront, set it as default.

00:18:53.360 --> 00:18:55.350
You can call the
instrumentation ops directly.

00:18:55.350 --> 00:18:58.310
So you no longer need to enable
summary writing, which makes

00:18:58.310 --> 00:19:00.110
it a little more streamlined.

00:19:00.110 --> 00:19:03.250
And I should say
that the ops, they

00:19:03.250 --> 00:19:05.087
write when executed in theory.

00:19:05.087 --> 00:19:07.170
But there's actually some
buffering in the writer.

00:19:07.170 --> 00:19:09.560
So usually you want to make
sure to flush the writer

00:19:09.560 --> 00:19:12.000
to ensure the data is
actually written to disk.

00:19:12.000 --> 00:19:15.960
And this example shows
an explicit flush.

00:19:15.960 --> 00:19:18.090
In eager, it will
do flushing for you

00:19:18.090 --> 00:19:19.980
when you exit the
as default context.

00:19:19.980 --> 00:19:22.547
But it's good if you
care about making sure--

00:19:22.547 --> 00:19:25.130
like, for example, after every
iteration of the loop, that you

00:19:25.130 --> 00:19:29.720
have data persisted to disk,
it's good to flush the writer.

00:19:29.720 --> 00:19:35.060
And then this is an example with
the at tf.function decorator.

00:19:35.060 --> 00:19:37.340
Again, you create
the writer up front.

00:19:37.340 --> 00:19:40.947
One important thing to note
here is that the writer,

00:19:40.947 --> 00:19:42.530
you have to maintain
a reference to it

00:19:42.530 --> 00:19:45.610
as long as you have a
function that uses the writer.

00:19:45.610 --> 00:19:48.050
And this has to do with
the difference between when

00:19:48.050 --> 00:19:50.660
the function is
traced and executed.

00:19:50.660 --> 00:19:52.220
It's a limitation
that hopefully we

00:19:52.220 --> 00:19:54.180
can improve this a little bit.

00:19:54.180 --> 00:19:56.930
But for now, at least,
that's one caveat.

00:19:56.930 --> 00:19:59.300
So the best way to
handle that is you

00:19:59.300 --> 00:20:02.060
set the writer as default
in the function itself.

00:20:02.060 --> 00:20:04.660
And then call instrumentation
ops that you need.

00:20:04.660 --> 00:20:06.830
And these write,
again, when executed,

00:20:06.830 --> 00:20:09.770
meaning when the function
is actually called.

00:20:09.770 --> 00:20:13.610
So we can see that's happening
down with the my_func call.

00:20:13.610 --> 00:20:16.060
And then you can flush
the writer again.

00:20:18.970 --> 00:20:20.590
And then, here we
have an example

00:20:20.590 --> 00:20:23.710
with legacy graph execution,
since there are still

00:20:23.710 --> 00:20:27.020
folks who use the 2.0.

00:20:27.020 --> 00:20:28.450
This is a little
bit more verbose.

00:20:28.450 --> 00:20:30.340
But again, you
create the writer.

00:20:30.340 --> 00:20:34.060
You set it as default. You've
constructed your graph.

00:20:34.060 --> 00:20:37.000
And then, in this case, you
need to explicitly initialize

00:20:37.000 --> 00:20:39.640
the writer if you're
running init op.

00:20:39.640 --> 00:20:41.620
We have sign of a
compatibility shim

00:20:41.620 --> 00:20:44.890
that lets you run all
of the v2 summary ops.

00:20:44.890 --> 00:20:48.370
So that you don't have to
keep track of them manually.

00:20:48.370 --> 00:20:50.270
And then, again,
flushing the writer.

00:20:50.270 --> 00:20:57.090
So this is how you would use
it in legacy graph execution.

00:21:00.170 --> 00:21:04.050
So how do you get to TF 2.0?

00:21:04.050 --> 00:21:08.750
The contrib summary API is close
enough to the 2.0 summary API

00:21:08.750 --> 00:21:10.460
that we do--

00:21:10.460 --> 00:21:13.240
actually, mostly, we
auto-migrate this in the TF

00:21:13.240 --> 00:21:15.590
upgrade v2 migration script.

00:21:15.590 --> 00:21:19.147
But tf.summary in
1.0 is sufficiently

00:21:19.147 --> 00:21:21.230
different on the writing
side that we can't really

00:21:21.230 --> 00:21:24.720
do a safe auto-migration.

00:21:24.720 --> 00:21:27.860
So here is the
three bullet version

00:21:27.860 --> 00:21:30.350
of how to migrate by hand.

00:21:30.350 --> 00:21:34.040
The first thing is that the
writer now needs to be present,

00:21:34.040 --> 00:21:39.320
created, and set via as default
before using the summary ops.

00:21:39.320 --> 00:21:45.060
And this is a limitation that
it's a little bit tricky.

00:21:45.060 --> 00:21:47.510
We're hoping to relax
this a little bit.

00:21:47.510 --> 00:21:49.410
So it's possible to
set a writer later.

00:21:49.410 --> 00:21:52.980
But for now, you want to have
the default writer already

00:21:52.980 --> 00:21:53.480
present.

00:21:53.480 --> 00:21:56.000
Otherwise, the summary ops
basically just become no ops

00:21:56.000 --> 00:22:00.500
if there's no writer, since
they have no where to write to.

00:22:00.500 --> 00:22:04.880
Then each op takes its
own step argument now.

00:22:04.880 --> 00:22:10.370
This is because since
there's no later step where

00:22:10.370 --> 00:22:12.470
you add the summary
to the writer, that's

00:22:12.470 --> 00:22:15.050
where the step was
previously provided.

00:22:15.050 --> 00:22:17.760
And there's also no
global step in TF 2.0.

00:22:17.760 --> 00:22:20.490
So there isn't really a good
default variable to use.

00:22:20.490 --> 00:22:23.990
So for now, steps are
being passed explicitly.

00:22:23.990 --> 00:22:26.500
And I'll talk about
this a little more later

00:22:26.500 --> 00:22:27.840
on the next slide.

00:22:27.840 --> 00:22:30.440
And then the function signatures
for the instrumentation ops,

00:22:30.440 --> 00:22:33.350
like scaler and image,
have change slightly.

00:22:33.350 --> 00:22:35.360
The most obvious thing
being that they no longer

00:22:35.360 --> 00:22:36.890
return an output.

00:22:36.890 --> 00:22:38.420
Because they write
via side effect.

00:22:38.420 --> 00:22:41.450
But also there's
slight differences

00:22:41.450 --> 00:22:44.630
in the keyword arguments that
won't affect most people.

00:22:44.630 --> 00:22:46.280
But it's something
good to know about.

00:22:46.280 --> 00:22:49.700
And these details will all be
in the external migration guide

00:22:49.700 --> 00:22:50.200
soon.

00:22:52.940 --> 00:22:54.590
And so the other changes--

00:22:54.590 --> 00:22:57.950
and this and some of the
other stuff I was mentioning.

00:22:57.950 --> 00:22:59.630
One change is with
graph writing.

00:22:59.630 --> 00:23:05.120
Since there's no default
global graph in 2.0,

00:23:05.120 --> 00:23:08.810
there's no direct
instrumentation op

00:23:08.810 --> 00:23:10.310
to write the graph.

00:23:10.310 --> 00:23:12.230
Instead, the approach
here is there's

00:23:12.230 --> 00:23:17.150
a set of tracing style APIs
to enable and disable tracing.

00:23:17.150 --> 00:23:22.160
And what those do is they record
the graphs of executing TF

00:23:22.160 --> 00:23:22.690
functions.

00:23:22.690 --> 00:23:24.830
So functions that
execute well, the tracing

00:23:24.830 --> 00:23:27.550
is enabled to the
summary writer.

00:23:27.550 --> 00:23:31.340
And this better reflects
the TF 2.0 understanding

00:23:31.340 --> 00:23:33.800
of graphs as something that
are associated with functions

00:23:33.800 --> 00:23:36.920
as they execute.

00:23:36.920 --> 00:23:39.480
Then this is what
I was alluding to.

00:23:39.480 --> 00:23:42.140
It's still a little bit
tricky to use default writers

00:23:42.140 --> 00:23:44.110
with graph mode since
it's not always the case

00:23:44.110 --> 00:23:45.860
that you know which
writer you want to use

00:23:45.860 --> 00:23:47.700
as you're assembling the graph.

00:23:47.700 --> 00:23:52.310
So we're working on
making that a little bit

00:23:52.310 --> 00:23:54.200
more user friendly.

00:23:54.200 --> 00:23:57.493
And setting the step for each op
is also definitely boilerplate.

00:23:57.493 --> 00:23:58.910
So that's another
area where we're

00:23:58.910 --> 00:24:01.520
working to make it possible
to set the step, maybe in one

00:24:01.520 --> 00:24:04.400
place, or somehow in the context
to avoid the need to pass it

00:24:04.400 --> 00:24:07.820
into ops individually.

00:24:07.820 --> 00:24:11.330
And then, the event file binary
representation has changed.

00:24:11.330 --> 00:24:12.322
This only affects you.

00:24:12.322 --> 00:24:14.780
This doesn't affect TensorBoard
in that TensorBoard already

00:24:14.780 --> 00:24:16.700
supports this format.

00:24:16.700 --> 00:24:19.640
But if you were parsing event
files in any manual way,

00:24:19.640 --> 00:24:21.460
you might notice this change.

00:24:21.460 --> 00:24:23.840
And I'll talk a little
bit more about that change

00:24:23.840 --> 00:24:26.070
in the next section.

00:24:26.070 --> 00:24:30.140
And finally, as mentioned, the
writers now have a one to one

00:24:30.140 --> 00:24:32.840
mapping to the underlying
resource and event file.

00:24:32.840 --> 00:24:36.560
So there's no more sharing
of writer resources.

00:24:39.720 --> 00:24:40.410
OK.

00:24:40.410 --> 00:24:43.410
And then the last section
will be about the summary data

00:24:43.410 --> 00:24:44.170
format.

00:24:44.170 --> 00:24:47.850
So this is log
directories, event files,

00:24:47.850 --> 00:24:50.030
how your data is
actually persisted.

00:24:53.020 --> 00:24:56.880
So first off, what
is a log directory?

00:24:56.880 --> 00:25:00.590
The TensorBoard command expects
a required dash dash logdir

00:25:00.590 --> 00:25:01.090
flag.

00:25:01.090 --> 00:25:03.728
In fact, your first
introduction to TensorBoard

00:25:03.728 --> 00:25:05.020
may have been trying to run it.

00:25:05.020 --> 00:25:06.940
And then it spits
out an error that you

00:25:06.940 --> 00:25:08.950
didn't pass the logdir flag.

00:25:08.950 --> 00:25:13.300
So the log directory
flag is the location

00:25:13.300 --> 00:25:15.880
that TensorBoard expects
to read data from.

00:25:15.880 --> 00:25:19.510
And this is often the
primary output directory

00:25:19.510 --> 00:25:21.370
for a TensorFlow program.

00:25:21.370 --> 00:25:23.410
Frameworks, again, like
Estimator and Keras

00:25:23.410 --> 00:25:25.870
have different knobs
for where output goes.

00:25:25.870 --> 00:25:29.290
But often, people will put it
all under one root directory.

00:25:29.290 --> 00:25:32.610
And that's often what people
use is the log directory.

00:25:32.610 --> 00:25:35.170
But TensorBoard has this
flexible interpretation

00:25:35.170 --> 00:25:38.140
where really all it cares
about is that it's a directory

00:25:38.140 --> 00:25:41.090
tree containing summary data.

00:25:41.090 --> 00:25:43.780
And when I say directory
tree, I really do mean a tree.

00:25:43.780 --> 00:25:45.790
Because the data can
be arbitrarily deep.

00:25:45.790 --> 00:25:47.530
TensorBoard will
traverse the entire tree

00:25:47.530 --> 00:25:51.010
looking for summary data.

00:25:51.010 --> 00:25:54.040
And you might think, that could
sort of be a problem sometimes,

00:25:54.040 --> 00:25:58.320
especially if there's hundreds,
thousands of event files.

00:25:58.320 --> 00:25:59.280
And it's true.

00:25:59.280 --> 00:26:01.450
Yeah, log directories
can be pretty large.

00:26:01.450 --> 00:26:04.930
And so TensorBoard
tries to take advantage

00:26:04.930 --> 00:26:07.090
of structure in
the log directory

00:26:07.090 --> 00:26:09.910
by mapping sub
directories of the logdir

00:26:09.910 --> 00:26:12.100
to this notion of
runs, which we talked

00:26:12.100 --> 00:26:14.410
about a little bit
in the early section

00:26:14.410 --> 00:26:16.360
about the TensorBoard UI.

00:26:16.360 --> 00:26:17.830
So again, these are runs.

00:26:17.830 --> 00:26:21.070
Like, a run of a
program, they're

00:26:21.070 --> 00:26:24.350
not individual
session.run calls.

00:26:24.350 --> 00:26:29.050
And when TensorBaord loads a
run, the definition it uses

00:26:29.050 --> 00:26:31.990
is that it's any directory
in the logdir that has

00:26:31.990 --> 00:26:33.980
at least one event file in it.

00:26:33.980 --> 00:26:35.950
And in this case, we mean
only direct children.

00:26:35.950 --> 00:26:39.730
So the directory has to
contain an actual event file.

00:26:39.730 --> 00:26:41.710
And an event file
is just defined

00:26:41.710 --> 00:26:44.530
as a file that has the
name, has the string

00:26:44.530 --> 00:26:46.087
tfevents in the name.

00:26:46.087 --> 00:26:47.920
Which is just the
standard naming convention

00:26:47.920 --> 00:26:51.370
used by summary writers.

00:26:51.370 --> 00:26:54.760
So as an example of this,
we have this log directory

00:26:54.760 --> 00:26:57.835
structure which has a
root directory logs.

00:26:57.835 --> 00:27:01.420
It has two experiment
sub directories in it.

00:27:01.420 --> 00:27:03.340
The first experiment
contains an event file.

00:27:03.340 --> 00:27:05.620
So that makes that itself a run.

00:27:05.620 --> 00:27:08.620
It also contains two sub
directories, train and eval,

00:27:08.620 --> 00:27:09.710
with event files.

00:27:09.710 --> 00:27:12.580
So those two also become runs.

00:27:12.580 --> 00:27:14.200
Visually, they
look like sub runs.

00:27:14.200 --> 00:27:16.060
But they're all considered
independent runs

00:27:16.060 --> 00:27:20.110
for TensorBoard, at least in
the current interpretation.

00:27:20.110 --> 00:27:21.550
And then, in
experiment two, that

00:27:21.550 --> 00:27:23.850
doesn't contain an
event file directly.

00:27:23.850 --> 00:27:25.120
So it's not a run.

00:27:25.120 --> 00:27:27.580
But it has a train sub
directory under it.

00:27:27.580 --> 00:27:29.720
So TensorBoard looks
at this log directory

00:27:29.720 --> 00:27:32.920
and traverses it and
finds four different runs.

00:27:36.080 --> 00:27:40.850
And this traversal step
happens continuously.

00:27:40.850 --> 00:27:43.490
TensorBoard will pull a
log directory for new data.

00:27:43.490 --> 00:27:47.360
And this is to facilitate
using TensorBoard as a way

00:27:47.360 --> 00:27:50.840
to monitor the progress
of a running job,

00:27:50.840 --> 00:27:53.270
or even potentially a job
that hasn't started yet.

00:27:53.270 --> 00:27:55.730
You might start TensorBoard
and your job at the same time.

00:27:55.730 --> 00:27:59.030
So this directory may
not even exist yet.

00:27:59.030 --> 00:28:00.770
And we may expect
that different runs

00:28:00.770 --> 00:28:03.390
will be created as it proceeds.

00:28:03.390 --> 00:28:07.160
So we need to continuously
check for new directories being

00:28:07.160 --> 00:28:10.550
created, new data
being appended.

00:28:10.550 --> 00:28:13.640
In the case where you know
that your data is not changing,

00:28:13.640 --> 00:28:15.530
like you're just
viewing old data,

00:28:15.530 --> 00:28:19.420
you can disable this using
the reload interval flag.

00:28:19.420 --> 00:28:24.110
And you can also adjust the
interval at which it pulls.

00:28:24.110 --> 00:28:26.150
So when it's traversing
the log directory,

00:28:26.150 --> 00:28:28.256
it does this in two passes.

00:28:28.256 --> 00:28:30.350
The first pass is
finding new runs.

00:28:30.350 --> 00:28:33.830
So it searches the directory
tree for new directories

00:28:33.830 --> 00:28:37.110
with TF event files in them.

00:28:37.110 --> 00:28:39.770
This can be very expensive if
your tree is deeply nested,

00:28:39.770 --> 00:28:43.220
and especially if it's
on a remote file system.

00:28:43.220 --> 00:28:44.900
And especially if the
remote file system

00:28:44.900 --> 00:28:48.750
is on a different continent,
which I've seen sometimes.

00:28:48.750 --> 00:28:53.000
So a key here is that walking
the whole directory tree

00:28:53.000 --> 00:28:55.920
can be pretty slow.

00:28:55.920 --> 00:28:58.050
We have some
optimizations for this.

00:28:58.050 --> 00:29:01.820
So for example, on
Google Cloud Storage,

00:29:01.820 --> 00:29:04.070
rather than walking each
directory individually,

00:29:04.070 --> 00:29:07.350
we have this iterative
globbing approach.

00:29:07.350 --> 00:29:10.760
Which we basically use to find
all directories at a given

00:29:10.760 --> 00:29:14.060
depth at the same time, which
takes advantage of the fact

00:29:14.060 --> 00:29:17.090
that GCS doesn't actually
really have directories.

00:29:17.090 --> 00:29:19.100
They're sort of an illusion.

00:29:19.100 --> 00:29:22.040
And there's other file system
optimizations like this

00:29:22.040 --> 00:29:25.100
that we would like
to make as well.

00:29:25.100 --> 00:29:27.180
But that's just one example.

00:29:27.180 --> 00:29:30.380
And then the second pass, after
it's found all the new runs,

00:29:30.380 --> 00:29:33.650
is that it reads new event
file data from each run.

00:29:33.650 --> 00:29:36.380
And it goes through the
runs essentially in series.

00:29:36.380 --> 00:29:38.640
There is a limiting
factor in Python itself

00:29:38.640 --> 00:29:39.740
for paralyzing this.

00:29:39.740 --> 00:29:43.700
But again, something that
we are interested in working

00:29:43.700 --> 00:29:45.770
on improving.

00:29:45.770 --> 00:29:47.600
And then, when you
actually have the set

00:29:47.600 --> 00:29:50.090
of event files for
a run, TensorBoard

00:29:50.090 --> 00:29:54.650
iterates over them, basically,
in directory listing order.

00:29:54.650 --> 00:29:57.400
You might have noticed on the
previous slide with the example

00:29:57.400 --> 00:30:00.750
logdir that the event files
all have a fix, prefix,

00:30:00.750 --> 00:30:02.580
and then a time stamp.

00:30:02.580 --> 00:30:04.970
And so what this means
is that the directory

00:30:04.970 --> 00:30:08.930
order is essentially creation
order of the event files.

00:30:08.930 --> 00:30:12.200
And so, in each event file,
we read records from it

00:30:12.200 --> 00:30:14.870
sequentially until we get
to the end of the file.

00:30:14.870 --> 00:30:16.700
And then at that
point, TensorBoard

00:30:16.700 --> 00:30:19.310
checks to see if there's
a subsequent file already

00:30:19.310 --> 00:30:19.850
created.

00:30:19.850 --> 00:30:22.070
If so, it continues to that one.

00:30:22.070 --> 00:30:24.710
Otherwise, it says, OK,
I'm done with this run.

00:30:24.710 --> 00:30:27.320
And then it goes
to the next run.

00:30:27.320 --> 00:30:29.480
And after it finishes
all the runs,

00:30:29.480 --> 00:30:31.670
it waits for the
reload interval.

00:30:31.670 --> 00:30:34.430
And then it starts
the new reload cycle.

00:30:34.430 --> 00:30:37.520
And this reload resumes
the read from the same

00:30:37.520 --> 00:30:42.140
offset in every file per
run that it stopped in.

00:30:42.140 --> 00:30:45.950
And an important thing
to point out here

00:30:45.950 --> 00:30:49.070
is that TensorBoard won't
ever revisit an earlier

00:30:49.070 --> 00:30:50.600
file within a run.

00:30:50.600 --> 00:30:54.860
So if it finishes reading a file
and continues to a later one,

00:30:54.860 --> 00:30:58.520
it won't ever go back to check
if the previous file contains

00:30:58.520 --> 00:31:00.020
new data.

00:31:00.020 --> 00:31:02.540
And this is based
on the assumption

00:31:02.540 --> 00:31:06.770
that the last file is the only
active one, the only one being

00:31:06.770 --> 00:31:08.510
actively written to.

00:31:08.510 --> 00:31:13.610
And it's important to avoid
checking all event files, which

00:31:13.610 --> 00:31:15.110
can be-- sometimes
there's thousands

00:31:15.110 --> 00:31:17.840
of event files in a
single run directory.

00:31:17.840 --> 00:31:22.820
And so that's a mechanism for
avoiding wasted rechecking.

00:31:22.820 --> 00:31:25.850
But this assumption definitely
doesn't always hold.

00:31:25.850 --> 00:31:27.740
There are cases when
a single program

00:31:27.740 --> 00:31:30.680
is using multiple active
writers within a run.

00:31:30.680 --> 00:31:33.800
And in that case, it can seem
like the data is being skipped.

00:31:33.800 --> 00:31:36.290
Because you proceed
to a new file.

00:31:36.290 --> 00:31:40.070
And then data added to the
original file no longer appears

00:31:40.070 --> 00:31:41.360
in TensorBoard.

00:31:41.360 --> 00:31:44.067
And luckily, it's
fairly straightforward

00:31:44.067 --> 00:31:44.900
to work around this.

00:31:44.900 --> 00:31:46.070
You just restart TensorBoard.

00:31:46.070 --> 00:31:48.260
And it will always pick up
all the data that existed

00:31:48.260 --> 00:31:50.458
at the time that it started.

00:31:50.458 --> 00:31:52.250
But we're working on
a better fix for this.

00:31:52.250 --> 00:31:56.120
So that we can still detect
and read when there is

00:31:56.120 --> 00:31:58.955
data added to files
other than the last one.

00:31:58.955 --> 00:32:01.580
But this is something that
has bitten people before.

00:32:01.580 --> 00:32:05.310
So just a heads up.

00:32:05.310 --> 00:32:08.550
And then, so the actual
event file format,

00:32:08.550 --> 00:32:12.095
this is based on TFRecord,
which is the standard TensorFlow

00:32:12.095 --> 00:32:12.595
format.

00:32:12.595 --> 00:32:16.220
It's the same as
tf.io TFRecordWriter.

00:32:16.220 --> 00:32:19.910
And it's a pretty simple
format, enough that it fits

00:32:19.910 --> 00:32:22.310
on the left side of this slide.

00:32:22.310 --> 00:32:25.280
It's basically just a bunch
of binary strings prefixed

00:32:25.280 --> 00:32:29.060
by their length with
CRCs for data integrity.

00:32:29.060 --> 00:32:31.610
And one particular
thing that I'll note

00:32:31.610 --> 00:32:35.960
is that because there's
no specific length

00:32:35.960 --> 00:32:41.480
for each string, there's no real
way to seek ahead in the file.

00:32:41.480 --> 00:32:44.220
You basically have to
read it sequentially.

00:32:44.220 --> 00:32:48.740
And there's also no built
in compression of any kind.

00:32:48.740 --> 00:32:51.158
And TensorBoard, it's
possible in theory

00:32:51.158 --> 00:32:52.700
to have the whole
file be compressed.

00:32:52.700 --> 00:32:54.242
TensorBoard doesn't
support this yet.

00:32:54.242 --> 00:32:57.320
But it's something that
could help save space

00:32:57.320 --> 00:33:01.460
when there's a lot of redundant
strings within the event file.

00:33:04.510 --> 00:33:07.430
And then each
individual record--

00:33:07.430 --> 00:33:10.510
so TFRecord is the framing
structure for the event file.

00:33:10.510 --> 00:33:13.720
Each individual record is
a serialized event protocol

00:33:13.720 --> 00:33:15.830
buffer.

00:33:15.830 --> 00:33:18.790
And this simplified schema
for the protocol buffer

00:33:18.790 --> 00:33:21.110
is shown on the left.

00:33:21.110 --> 00:33:22.840
We have a wall time
and a step, which

00:33:22.840 --> 00:33:25.300
are used to construct
the time series.

00:33:25.300 --> 00:33:29.530
And then we have a few
different ways to store data.

00:33:29.530 --> 00:33:32.230
But the primary one is
a summary sub-message.

00:33:32.230 --> 00:33:35.710
The main exception is graph
data get stored in the GraphDef

00:33:35.710 --> 00:33:37.750
separately.

00:33:37.750 --> 00:33:41.920
And then we can look at
the summary sub-message,

00:33:41.920 --> 00:33:45.410
which is itself basically a
list of value sub-messages.

00:33:45.410 --> 00:33:47.740
That's where the actual
interesting part is.

00:33:47.740 --> 00:33:51.070
And each one of
these contains a tag.

00:33:51.070 --> 00:33:53.980
Again, from our
overview of the UI,

00:33:53.980 --> 00:33:56.050
that's the name or
idea of the summary

00:33:56.050 --> 00:33:57.550
as shown in TensorBoard.

00:33:57.550 --> 00:33:59.920
We have metadata, which is
used to describe the more

00:33:59.920 --> 00:34:01.390
generic tensor formats.

00:34:01.390 --> 00:34:04.690
And then specific
type fields, including

00:34:04.690 --> 00:34:10.060
ones for the original TF 1.x,
specific fields for each type.

00:34:10.060 --> 00:34:16.060
And then the tensor field, which
can be used with the new tensor

00:34:16.060 --> 00:34:21.159
style instrumentation ops to
hold general forms of data.

00:34:25.260 --> 00:34:27.420
And then, in terms of
loading the summary data

00:34:27.420 --> 00:34:29.460
into memory-- so I
mentioned this briefly

00:34:29.460 --> 00:34:31.739
in the architecture stage.

00:34:31.739 --> 00:34:34.590
But TensorBoard has to load
the summary data into memory.

00:34:34.590 --> 00:34:38.070
Because, like I said, there's no
real indexing or random access

00:34:38.070 --> 00:34:38.940
in the event file.

00:34:38.940 --> 00:34:42.120
You can think of them like
they're just like raw logs.

00:34:42.120 --> 00:34:44.880
And so TensorBoard
loads it into memory

00:34:44.880 --> 00:34:49.543
and creates its own indexes of
data by run, plugin, and tag.

00:34:49.543 --> 00:34:51.960
Which support the different
kinds of visualization queries

00:34:51.960 --> 00:34:54.030
that plugins need.

00:34:54.030 --> 00:34:56.460
And TensorBoard also does
downsampling in order

00:34:56.460 --> 00:34:59.070
to avoid running out of memory
since the log directory may

00:34:59.070 --> 00:35:01.140
contain far more data
than could reasonably

00:35:01.140 --> 00:35:04.010
fit in TensorBoard's RAM.

00:35:04.010 --> 00:35:05.850
And to do the
downsampling, it uses

00:35:05.850 --> 00:35:08.010
a reservoir sampling algorithm.

00:35:08.010 --> 00:35:10.110
It's essentially just
an algorithm for uniform

00:35:10.110 --> 00:35:12.660
sampling when you don't know
the size of your sequence

00:35:12.660 --> 00:35:17.580
in advance, which is the case
when we're consuming data

00:35:17.580 --> 00:35:19.330
from an active job.

00:35:19.330 --> 00:35:23.400
And because of this, it
has a random aspect which

00:35:23.400 --> 00:35:25.320
can be surprising to users.

00:35:25.320 --> 00:35:27.810
Where you might not
understand-- like,

00:35:27.810 --> 00:35:30.780
why is this step being
taken and not this one.

00:35:30.780 --> 00:35:34.650
And there's, like,
a gap between steps.

00:35:34.650 --> 00:35:37.480
This can be tuned with a
samples per plugin flag.

00:35:37.480 --> 00:35:39.608
So that tunes the
size of the reservoir.

00:35:39.608 --> 00:35:41.400
Basically, if you make
the reservoir larger

00:35:41.400 --> 00:35:43.025
than your total number
of steps, you'll

00:35:43.025 --> 00:35:45.315
always see all of your data.

00:35:45.315 --> 00:35:47.190
So that gives you a
certain amount of control

00:35:47.190 --> 00:35:48.780
over how about sampling works.

00:35:52.520 --> 00:35:54.920
And just to review
some of the section,

00:35:54.920 --> 00:35:56.990
there's some best
practices for-- at least

00:35:56.990 --> 00:36:02.030
in the current TensorBoard,
how to get the best performers.

00:36:02.030 --> 00:36:05.000
One of the basic ones is just
to avoid having enormous log

00:36:05.000 --> 00:36:07.160
directories, in
terms of the number

00:36:07.160 --> 00:36:10.610
of files, subdirectories,
quantity of data.

00:36:10.610 --> 00:36:13.460
This doesn't actually
mean that the overall log

00:36:13.460 --> 00:36:15.570
directory has to be small.

00:36:15.570 --> 00:36:17.960
It just means that
TensorBoard itself

00:36:17.960 --> 00:36:19.460
will run better if
you can launch it

00:36:19.460 --> 00:36:23.630
at a directory that just
contains relevant data for what

00:36:23.630 --> 00:36:24.690
you want to examine.

00:36:24.690 --> 00:36:26.750
So, a hierarchical
structure where

00:36:26.750 --> 00:36:29.180
you can pick sort of a
experiment sub-directory,

00:36:29.180 --> 00:36:30.800
or group a few
experiments together

00:36:30.800 --> 00:36:33.440
works really well here.

00:36:33.440 --> 00:36:35.030
I mentioned the reload interval.

00:36:35.030 --> 00:36:37.130
You can set it to 0
to disable reload.

00:36:37.130 --> 00:36:41.930
So for unchanging data, this
helps avoid extra overhead.

00:36:41.930 --> 00:36:45.860
And that's especially useful
on a remote file system case.

00:36:45.860 --> 00:36:48.440
In that case, it's
also useful if you

00:36:48.440 --> 00:36:51.230
can run TensorBoard
close to your data,

00:36:51.230 --> 00:36:55.460
or download a subset of it that
you need so that it doesn't all

00:36:55.460 --> 00:36:58.490
have to be fetched
over the network.

00:36:58.490 --> 00:37:03.670
And for now, due to the way that
superseded event files aren't

00:37:03.670 --> 00:37:07.240
re-read, it's best to avoid
multiple active writers pointed

00:37:07.240 --> 00:37:09.955
at the same directory.

00:37:09.955 --> 00:37:11.830
This is something, again,
that we're actively

00:37:11.830 --> 00:37:12.940
working on improving this.

00:37:12.940 --> 00:37:16.300
But at least for now, that
can lead to this appearance

00:37:16.300 --> 00:37:18.340
that some data gets skipped.

00:37:18.340 --> 00:37:21.130
And then, in general, stay
tuned for logdir performance

00:37:21.130 --> 00:37:21.880
improvements.

00:37:21.880 --> 00:37:26.980
We're hoping to make a number
of these improvements soon.

00:37:26.980 --> 00:37:29.680
And that pretty much
rounds out the content

00:37:29.680 --> 00:37:30.830
that I have for today.

00:37:30.830 --> 00:37:35.290
So I'd like to thank
everybody for attending.

00:37:35.290 --> 00:37:38.380
And if you want to find
out more about TensorBoard,

00:37:38.380 --> 00:37:42.790
we have a new sub site on
tensorflow.org/tensorboard.

00:37:42.790 --> 00:37:44.470
You can also find us on GitHub.

00:37:44.470 --> 00:37:46.690
And people interested
in contributing

00:37:46.690 --> 00:37:49.330
are welcome to join the
TensorBoard special interest

00:37:49.330 --> 00:37:51.120
group.

00:37:51.120 --> 00:37:52.120
Thank you all very much.

00:37:56.750 --> 00:37:59.704
AUDIENCE: What are the potential
uses for the multiple summary

00:37:59.704 --> 00:38:00.820
writers?

00:38:00.820 --> 00:38:03.070
NICK: The potential use cases
for the multiple summary

00:38:03.070 --> 00:38:07.450
writers, sometimes this is just
a matter of code structure.

00:38:07.450 --> 00:38:10.270
Or if you're using a library
that itself creates a summary

00:38:10.270 --> 00:38:14.410
writer, it's not always
straightforward to ensure

00:38:14.410 --> 00:38:18.940
that you can use
the same writer.

00:38:18.940 --> 00:38:24.580
So TF 1.x had this File
Writer cache, which was--

00:38:24.580 --> 00:38:27.558
the best practice then was
to use this shared cache

00:38:27.558 --> 00:38:29.350
since we only had one
writer per directory.

00:38:29.350 --> 00:38:32.098
And it was to work
around this problem.

00:38:32.098 --> 00:38:33.640
I think it's better
to work around it

00:38:33.640 --> 00:38:36.700
on the TensorBoard side and have
some ideas for how to do that.

00:38:36.700 --> 00:38:39.430
So hopefully, that part
will be out of date soon.

00:38:39.430 --> 00:38:40.870
Like, within a month or two.

00:38:40.870 --> 00:38:44.050
AUDIENCE: Are there plans to
change the event file format?

00:38:44.050 --> 00:38:44.650
NICK: Yeah.

00:38:44.650 --> 00:38:49.503
So I think a lot of
this depends on--

00:38:49.503 --> 00:38:50.920
I think that the
event file format

00:38:50.920 --> 00:38:57.250
itself could be a lot better
tailored to what TensorBoard

00:38:57.250 --> 00:38:59.710
actually needs.

00:38:59.710 --> 00:39:02.170
And some of the things I
mentioned would just be--

00:39:02.170 --> 00:39:04.030
even if we had an index
into the event file,

00:39:04.030 --> 00:39:06.460
that could potentially help--

00:39:06.460 --> 00:39:08.560
we could potentially
paralyze reads.

00:39:08.560 --> 00:39:12.287
Or we could sort of scan
ahead and do smarter sampling.

00:39:12.287 --> 00:39:13.870
Like, rather than
reading all the data

00:39:13.870 --> 00:39:15.537
and then down sampling
it, we could just

00:39:15.537 --> 00:39:20.050
pick different offsets
and sample from there.

00:39:20.050 --> 00:39:23.410
We're constrained right now
mostly by this being the legacy

00:39:23.410 --> 00:39:24.730
format.

00:39:24.730 --> 00:39:27.760
But I think it would be
pretty interesting to explore

00:39:27.760 --> 00:39:28.305
new formats.

00:39:31.480 --> 00:39:34.630
Particularly when you have
different data types mixed in,

00:39:34.630 --> 00:39:37.060
something sort of columnar
could be kind of useful,

00:39:37.060 --> 00:39:40.780
where you can read
only images if you

00:39:40.780 --> 00:39:42.550
need to read
images, or otherwise

00:39:42.550 --> 00:39:46.210
avoid the phenomenon where--
so, this happens sometimes

00:39:46.210 --> 00:39:49.690
where one particular
event file contains

00:39:49.690 --> 00:39:53.380
lots and lots of large
images or large graph defs.

00:39:53.380 --> 00:39:58.540
And this blocks the reading
of a lot of small scaler data.

00:39:58.540 --> 00:40:00.750
And that's, obviously,
not really--

00:40:00.750 --> 00:40:02.570
it doesn't really make sense.

00:40:02.570 --> 00:40:05.800
But again, it's a limitation
of having the data only be

00:40:05.800 --> 00:40:07.544
accessible sequentially.

00:40:07.544 --> 00:40:09.238
AUDIENCE: Can you
talk a little bit more

00:40:09.238 --> 00:40:10.448
about the graph dashboard?

00:40:10.448 --> 00:40:11.900
Is graph just another summary?

00:40:11.900 --> 00:40:12.870
Or how does--

00:40:12.870 --> 00:40:13.450
NICK: Yeah.

00:40:13.450 --> 00:40:16.390
So the graph visualization,
which was actually

00:40:16.390 --> 00:40:19.130
originally created
by Big Picture,

00:40:19.130 --> 00:40:23.470
It's a visualization of the
actual TensorFlow graph.

00:40:23.470 --> 00:40:26.260
So like, the computation
graph with ops and edges

00:40:26.260 --> 00:40:28.840
connecting them and
sort of the data flow.

00:40:28.840 --> 00:40:30.070
It's pretty cool.

00:40:30.070 --> 00:40:34.270
It's really a good way if you
want to visually understand

00:40:34.270 --> 00:40:36.670
what's going on.

00:40:36.670 --> 00:40:42.602
It works best when the code
uses scoping to delineate

00:40:42.602 --> 00:40:43.810
different parts of the model.

00:40:43.810 --> 00:40:46.330
If it's just a
giant soup of ops,

00:40:46.330 --> 00:40:49.600
it's a little hard to understand
the higher order structure.

00:40:49.600 --> 00:40:51.130
And there's actually
some cool work

00:40:51.130 --> 00:40:53.770
done for TF 2.0, which
isn't in the presentation,

00:40:53.770 --> 00:40:58.150
about for Keras showing the
Keras conceptual graph using

00:40:58.150 --> 00:41:03.315
Keras layers to give you a
better view into the high level

00:41:03.315 --> 00:41:04.690
structure of the
model like you'd

00:41:04.690 --> 00:41:08.715
expect to see in a diagram
written by a human.

00:41:08.715 --> 00:41:10.090
But the graph
dashboard can still

00:41:10.090 --> 00:41:14.450
be useful for understanding
exactly what ops are happening.

00:41:14.450 --> 00:41:17.470
And sometimes it's useful
for debugging cases.

00:41:17.470 --> 00:41:21.100
If some part of the graph
is behaving weirdly,

00:41:21.100 --> 00:41:23.500
maybe you didn't realize
that you actually

00:41:23.500 --> 00:41:25.990
have an edge between
two different ops that

00:41:25.990 --> 00:41:28.060
was unexpected.

00:41:28.060 --> 00:41:30.880
AUDIENCE: Are there any
plans to add support

00:41:30.880 --> 00:41:33.310
for basically that sort
of higher order structure

00:41:33.310 --> 00:41:33.810
annotation?

00:41:33.810 --> 00:41:35.400
So I'm imagining,
for instance, like,

00:41:35.400 --> 00:41:38.020
if you have [INAUDIBLE] having
the whole model, and then

00:41:38.020 --> 00:41:40.390
a block, and then a sub
block, and if there's

00:41:40.390 --> 00:41:42.640
like five layers of
structural depth, that would

00:41:42.640 --> 00:41:44.110
be nice to be able to imitate.

00:41:44.110 --> 00:41:44.610
NICK: Yeah.

00:41:44.610 --> 00:41:50.128
I think this is an
interesting question.

00:41:50.128 --> 00:41:52.420
So right now, the main tool
you have for that structure

00:41:52.420 --> 00:41:53.980
is just name scoping.

00:41:53.980 --> 00:41:56.650
But it only really works
if the part of the graph

00:41:56.650 --> 00:42:00.790
is all been defined in
the same place anyway.

00:42:00.790 --> 00:42:03.940
I think it would be really nice
to have the graph visualization

00:42:03.940 --> 00:42:10.090
support more strategies for
human friendly annotation

00:42:10.090 --> 00:42:12.542
and organization.

00:42:12.542 --> 00:42:14.500
I think the recent work
that we've done on this

00:42:14.500 --> 00:42:16.420
has been the Keras
conceptual graph, which

00:42:16.420 --> 00:42:18.790
[? Stephan ?] did over there.

00:42:18.790 --> 00:42:25.060
But I think having that work for
not just Keras layers, but more

00:42:25.060 --> 00:42:28.180
general model decomposition
approaches would

00:42:28.180 --> 00:42:29.443
be really nice.

00:42:29.443 --> 00:42:31.360
AUDIENCE: Often, I find
that the problem isn't

00:42:31.360 --> 00:42:32.530
that it's not capturing enough.

00:42:32.530 --> 00:42:33.710
It's actually that it's
capturing too much.

00:42:33.710 --> 00:42:35.623
So for instance, you'll
have a convolution.

00:42:35.623 --> 00:42:37.790
But then there's a bunch
of loss and regularization.

00:42:37.790 --> 00:42:40.840
There winds up being a bunch of
tensors that sort of clutter.

00:42:40.840 --> 00:42:42.940
And so actually, even the
ability to filter stuff

00:42:42.940 --> 00:42:44.845
out of the conceptual graph.

00:42:44.845 --> 00:42:47.440
NICK: So there id a feature
in the graph dashboard

00:42:47.440 --> 00:42:49.640
where you can remove
nodes from the main graph.

00:42:49.640 --> 00:42:52.570
But I believe it has
to be done by hand.

00:42:52.570 --> 00:42:54.970
It does a certain amount
of automatic extraction

00:42:54.970 --> 00:42:57.670
of things that are sort of less
important out of the graph.

00:42:57.670 --> 00:43:01.810
But maybe that's a place we
could look into having either

00:43:01.810 --> 00:43:03.370
a smarter procedure
for doing that,

00:43:03.370 --> 00:43:06.660
or a way to sort of tag
like, hey, this section of--

00:43:06.660 --> 00:43:08.410
I don't actually want
to see any of these.

00:43:08.410 --> 00:43:11.720
Or this should be
factored out in some way.

00:43:11.720 --> 00:43:12.220
Yeah.

00:43:12.220 --> 00:43:13.120
Thanks.

00:43:13.120 --> 00:43:17.970
[APPLAUSE]

