WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.720 align:start position:0%
 
hello<00:00:00.450><c> and</c><00:00:00.750><c> welcome</c><00:00:01.079><c> back</c><00:00:01.110><c> in</c><00:00:01.530><c> this</c><00:00:02.280><c> week</c><00:00:02.490><c> you</c>

00:00:02.720 --> 00:00:02.730 align:start position:0%
hello and welcome back in this week you
 

00:00:02.730 --> 00:00:04.760 align:start position:0%
hello and welcome back in this week you
learn<00:00:02.909><c> about</c><00:00:03.149><c> optimization</c><00:00:03.929><c> algorithms</c><00:00:04.440><c> that</c>

00:00:04.760 --> 00:00:04.770 align:start position:0%
learn about optimization algorithms that
 

00:00:04.770 --> 00:00:06.170 align:start position:0%
learn about optimization algorithms that
will<00:00:04.859><c> enable</c><00:00:05.069><c> you</c><00:00:05.400><c> to</c><00:00:05.430><c> train</c><00:00:05.910><c> in</c><00:00:06.060><c> your</c>

00:00:06.170 --> 00:00:06.180 align:start position:0%
will enable you to train in your
 

00:00:06.180 --> 00:00:08.780 align:start position:0%
will enable you to train in your
networks<00:00:06.660><c> much</c><00:00:06.960><c> faster</c><00:00:07.500><c> you've</c><00:00:08.370><c> heard</c><00:00:08.550><c> me</c><00:00:08.670><c> say</c>

00:00:08.780 --> 00:00:08.790 align:start position:0%
networks much faster you've heard me say
 

00:00:08.790 --> 00:00:10.700 align:start position:0%
networks much faster you've heard me say
before<00:00:08.849><c> that</c><00:00:09.150><c> apply</c><00:00:09.660><c> machine</c><00:00:09.870><c> learning</c><00:00:10.050><c> is</c><00:00:10.679><c> a</c>

00:00:10.700 --> 00:00:10.710 align:start position:0%
before that apply machine learning is a
 

00:00:10.710 --> 00:00:12.860 align:start position:0%
before that apply machine learning is a
highly<00:00:11.370><c> empirical</c><00:00:12.059><c> process</c><00:00:12.540><c> is</c><00:00:12.690><c> highly</c>

00:00:12.860 --> 00:00:12.870 align:start position:0%
highly empirical process is highly
 

00:00:12.870 --> 00:00:15.079 align:start position:0%
highly empirical process is highly
intuitive<00:00:13.410><c> process</c><00:00:13.950><c> it</c><00:00:14.370><c> which</c><00:00:14.580><c> you</c><00:00:14.730><c> just</c><00:00:14.910><c> have</c>

00:00:15.079 --> 00:00:15.089 align:start position:0%
intuitive process it which you just have
 

00:00:15.089 --> 00:00:16.910 align:start position:0%
intuitive process it which you just have
to<00:00:15.240><c> train</c><00:00:15.540><c> a</c><00:00:15.570><c> lot</c><00:00:15.660><c> of</c><00:00:15.900><c> models</c><00:00:16.289><c> to</c><00:00:16.470><c> find</c><00:00:16.619><c> one</c>

00:00:16.910 --> 00:00:16.920 align:start position:0%
to train a lot of models to find one
 

00:00:16.920 --> 00:00:18.740 align:start position:0%
to train a lot of models to find one
that<00:00:17.100><c> works</c><00:00:17.279><c> really</c><00:00:17.520><c> well</c><00:00:17.640><c> so</c><00:00:18.510><c> it</c><00:00:18.570><c> really</c>

00:00:18.740 --> 00:00:18.750 align:start position:0%
that works really well so it really
 

00:00:18.750 --> 00:00:21.320 align:start position:0%
that works really well so it really
helps<00:00:19.170><c> to</c><00:00:19.380><c> really</c><00:00:19.619><c> train</c><00:00:19.859><c> models</c><00:00:20.250><c> quickly</c><00:00:20.400><c> one</c>

00:00:21.320 --> 00:00:21.330 align:start position:0%
helps to really train models quickly one
 

00:00:21.330 --> 00:00:22.939 align:start position:0%
helps to really train models quickly one
thing<00:00:21.359><c> that</c><00:00:21.720><c> makes</c><00:00:21.869><c> it</c><00:00:22.020><c> more</c><00:00:22.170><c> difficult</c><00:00:22.380><c> is</c>

00:00:22.939 --> 00:00:22.949 align:start position:0%
thing that makes it more difficult is
 

00:00:22.949 --> 00:00:24.470 align:start position:0%
thing that makes it more difficult is
that<00:00:22.980><c> deep</c><00:00:23.430><c> learning</c><00:00:23.670><c> which</c><00:00:24.090><c> is</c><00:00:24.180><c> the</c><00:00:24.330><c> work</c>

00:00:24.470 --> 00:00:24.480 align:start position:0%
that deep learning which is the work
 

00:00:24.480 --> 00:00:26.750 align:start position:0%
that deep learning which is the work
best<00:00:24.750><c> in</c><00:00:25.019><c> the</c><00:00:25.320><c> regime</c><00:00:25.650><c> of</c><00:00:25.680><c> Big</c><00:00:26.070><c> Data</c><00:00:26.279><c> when</c>

00:00:26.750 --> 00:00:26.760 align:start position:0%
best in the regime of Big Data when
 

00:00:26.760 --> 00:00:27.920 align:start position:0%
best in the regime of Big Data when
you're<00:00:26.880><c> able</c><00:00:26.939><c> to</c><00:00:27.119><c> train</c><00:00:27.330><c> your</c><00:00:27.420><c> near</c><00:00:27.570><c> network</c>

00:00:27.920 --> 00:00:27.930 align:start position:0%
you're able to train your near network
 

00:00:27.930 --> 00:00:30.980 align:start position:0%
you're able to train your near network
on<00:00:28.109><c> a</c><00:00:28.170><c> huge</c><00:00:28.800><c> data</c><00:00:29.220><c> set</c><00:00:29.490><c> and</c><00:00:29.670><c> training</c><00:00:30.630><c> on</c><00:00:30.720><c> large</c>

00:00:30.980 --> 00:00:30.990 align:start position:0%
on a huge data set and training on large
 

00:00:30.990 --> 00:00:33.979 align:start position:0%
on a huge data set and training on large
data<00:00:31.230><c> sets</c><00:00:31.590><c> is</c><00:00:31.830><c> just</c><00:00:32.219><c> slow</c><00:00:32.460><c> so</c><00:00:33.210><c> what</c><00:00:33.719><c> you</c><00:00:33.840><c> find</c>

00:00:33.979 --> 00:00:33.989 align:start position:0%
data sets is just slow so what you find
 

00:00:33.989 --> 00:00:36.260 align:start position:0%
data sets is just slow so what you find
is<00:00:34.260><c> that</c><00:00:34.500><c> having</c><00:00:34.920><c> fast</c><00:00:35.460><c> optimization</c>

00:00:36.260 --> 00:00:36.270 align:start position:0%
is that having fast optimization
 

00:00:36.270 --> 00:00:37.520 align:start position:0%
is that having fast optimization
algorithms<00:00:36.750><c> having</c><00:00:37.079><c> good</c><00:00:37.260><c> optimization</c>

00:00:37.520 --> 00:00:37.530 align:start position:0%
algorithms having good optimization
 

00:00:37.530 --> 00:00:39.619 align:start position:0%
algorithms having good optimization
algorithms<00:00:38.280><c> can</c><00:00:38.730><c> really</c><00:00:39.000><c> speed</c><00:00:39.329><c> up</c><00:00:39.480><c> the</c>

00:00:39.619 --> 00:00:39.629 align:start position:0%
algorithms can really speed up the
 

00:00:39.629 --> 00:00:42.049 align:start position:0%
algorithms can really speed up the
efficiency<00:00:40.230><c> of</c><00:00:40.469><c> you</c><00:00:40.680><c> and</c><00:00:40.890><c> your</c><00:00:41.040><c> team</c><00:00:41.070><c> so</c><00:00:42.030><c> let's</c>

00:00:42.049 --> 00:00:42.059 align:start position:0%
efficiency of you and your team so let's
 

00:00:42.059 --> 00:00:44.479 align:start position:0%
efficiency of you and your team so let's
get<00:00:42.329><c> started</c><00:00:42.809><c> by</c><00:00:43.079><c> talking</c><00:00:43.140><c> about</c><00:00:43.559><c> mini-batch</c>

00:00:44.479 --> 00:00:44.489 align:start position:0%
get started by talking about mini-batch
 

00:00:44.489 --> 00:00:46.610 align:start position:0%
get started by talking about mini-batch
gradient<00:00:44.570><c> descent</c><00:00:45.570><c> you've</c><00:00:46.320><c> learned</c>

00:00:46.610 --> 00:00:46.620 align:start position:0%
gradient descent you've learned
 

00:00:46.620 --> 00:00:48.830 align:start position:0%
gradient descent you've learned
previously<00:00:46.890><c> the</c><00:00:47.460><c> vectorization</c><00:00:47.940><c> allows</c><00:00:48.570><c> you</c>

00:00:48.830 --> 00:00:48.840 align:start position:0%
previously the vectorization allows you
 

00:00:48.840 --> 00:00:51.049 align:start position:0%
previously the vectorization allows you
to<00:00:49.020><c> efficiently</c><00:00:49.530><c> compute</c><00:00:49.980><c> on</c><00:00:50.129><c> all</c><00:00:50.370><c> M</c><00:00:50.670><c> examples</c>

00:00:51.049 --> 00:00:51.059 align:start position:0%
to efficiently compute on all M examples
 

00:00:51.059 --> 00:00:53.930 align:start position:0%
to efficiently compute on all M examples
that<00:00:51.870><c> allows</c><00:00:52.230><c> you</c><00:00:52.500><c> to</c><00:00:52.640><c> process</c><00:00:53.640><c> your</c><00:00:53.760><c> whole</c>

00:00:53.930 --> 00:00:53.940 align:start position:0%
that allows you to process your whole
 

00:00:53.940 --> 00:00:55.970 align:start position:0%
that allows you to process your whole
training<00:00:54.300><c> set</c><00:00:54.539><c> without</c><00:00:54.750><c> an</c><00:00:55.110><c> explicit</c><00:00:55.469><c> for</c>

00:00:55.970 --> 00:00:55.980 align:start position:0%
training set without an explicit for
 

00:00:55.980 --> 00:00:58.580 align:start position:0%
training set without an explicit for
loop<00:00:56.239><c> so</c><00:00:57.239><c> that's</c><00:00:57.629><c> why</c><00:00:57.840><c> we</c><00:00:58.050><c> would</c><00:00:58.230><c> take</c><00:00:58.410><c> our</c>

00:00:58.580 --> 00:00:58.590 align:start position:0%
loop so that's why we would take our
 

00:00:58.590 --> 00:01:00.770 align:start position:0%
loop so that's why we would take our
training<00:00:58.949><c> examples</c><00:00:59.399><c> and</c><00:00:59.730><c> stack</c><00:01:00.149><c> them</c><00:01:00.390><c> into</c>

00:01:00.770 --> 00:01:00.780 align:start position:0%
training examples and stack them into
 

00:01:00.780 --> 00:01:05.420 align:start position:0%
training examples and stack them into
this<00:01:01.340><c> huge</c><00:01:02.340><c> matrix</c><00:01:02.930><c> capital</c><00:01:03.930><c> X</c><00:01:04.110><c> so</c><00:01:04.320><c> 6</c><00:01:04.860><c> 1</c><00:01:05.010><c> X</c><00:01:05.159><c> 2</c><00:01:05.309><c> X</c>

00:01:05.420 --> 00:01:05.430 align:start position:0%
this huge matrix capital X so 6 1 X 2 X
 

00:01:05.430 --> 00:01:08.539 align:start position:0%
this huge matrix capital X so 6 1 X 2 X
3<00:01:05.510><c> you</c><00:01:06.510><c> know</c><00:01:06.659><c> and</c><00:01:06.930><c> then</c><00:01:07.140><c> um</c><00:01:07.350><c> eventually</c><00:01:07.860><c> it</c>

00:01:08.539 --> 00:01:08.549 align:start position:0%
3 you know and then um eventually it
 

00:01:08.549 --> 00:01:11.960 align:start position:0%
3 you know and then um eventually it
goes<00:01:08.790><c> up</c><00:01:09.000><c> to</c><00:01:09.210><c> X</c><00:01:09.720><c> M</c><00:01:10.200><c> they</c><00:01:10.979><c> give</c><00:01:11.189><c> M</c><00:01:11.430><c> training</c>

00:01:11.960 --> 00:01:11.970 align:start position:0%
goes up to X M they give M training
 

00:01:11.970 --> 00:01:15.170 align:start position:0%
goes up to X M they give M training
examples<00:01:12.030><c> and</c><00:01:12.689><c> similarly</c><00:01:13.619><c> for</c><00:01:13.950><c> y</c><00:01:14.159><c> this</c><00:01:14.220><c> is</c><00:01:14.549><c> y</c><00:01:14.670><c> 1</c>

00:01:15.170 --> 00:01:15.180 align:start position:0%
examples and similarly for y this is y 1
 

00:01:15.180 --> 00:01:23.330 align:start position:0%
examples and similarly for y this is y 1
y<00:01:15.570><c> 2</c><00:01:15.710><c> y</c><00:01:16.710><c> 3</c><00:01:17.360><c> and</c><00:01:18.360><c> so</c><00:01:18.509><c> on</c><00:01:18.540><c> up</c><00:01:18.960><c> to</c><00:01:19.850><c> Y</c><00:01:20.850><c> M</c><00:01:22.130><c> so</c><00:01:23.130><c> the</c>

00:01:23.330 --> 00:01:23.340 align:start position:0%
y 2 y 3 and so on up to Y M so the
 

00:01:23.340 --> 00:01:27.109 align:start position:0%
y 2 y 3 and so on up to Y M so the
dimension<00:01:23.700><c> of</c><00:01:23.790><c> X</c><00:01:24.000><c> was</c><00:01:24.479><c> n</c><00:01:24.840><c> X</c><00:01:25.409><c> by</c><00:01:25.920><c> M</c><00:01:26.189><c> and</c><00:01:26.580><c> this</c><00:01:27.060><c> is</c>

00:01:27.109 --> 00:01:27.119 align:start position:0%
dimension of X was n X by M and this is
 

00:01:27.119 --> 00:01:30.649 align:start position:0%
dimension of X was n X by M and this is
1<00:01:27.619><c> by</c><00:01:28.619><c> M</c><00:01:28.829><c> vectorization</c><00:01:29.790><c> allows</c><00:01:30.390><c> you</c><00:01:30.540><c> to</c>

00:01:30.649 --> 00:01:30.659 align:start position:0%
1 by M vectorization allows you to
 

00:01:30.659 --> 00:01:32.840 align:start position:0%
1 by M vectorization allows you to
process<00:01:30.900><c> our</c><00:01:31.200><c> M</c><00:01:31.470><c> examples</c><00:01:31.850><c> quickly</c>

00:01:32.840 --> 00:01:32.850 align:start position:0%
process our M examples quickly
 

00:01:32.850 --> 00:01:35.840 align:start position:0%
process our M examples quickly
relatively<00:01:33.780><c> quickly</c><00:01:34.020><c> if</c><00:01:34.320><c> M</c><00:01:34.890><c> is</c><00:01:34.920><c> very</c><00:01:35.520><c> large</c>

00:01:35.840 --> 00:01:35.850 align:start position:0%
relatively quickly if M is very large
 

00:01:35.850 --> 00:01:38.600 align:start position:0%
relatively quickly if M is very large
then<00:01:36.450><c> it</c><00:01:36.479><c> can</c><00:01:36.810><c> still</c><00:01:37.079><c> be</c><00:01:37.110><c> slow</c><00:01:37.619><c> so</c><00:01:37.920><c> for</c><00:01:38.220><c> example</c>

00:01:38.600 --> 00:01:38.610 align:start position:0%
then it can still be slow so for example
 

00:01:38.610 --> 00:01:42.170 align:start position:0%
then it can still be slow so for example
what<00:01:38.850><c> if</c><00:01:39.030><c> M</c><00:01:39.270><c> was</c><00:01:40.229><c> 5</c><00:01:40.680><c> million</c><00:01:41.159><c> you</c><00:01:41.670><c> know</c><00:01:41.729><c> 50</c>

00:01:42.170 --> 00:01:42.180 align:start position:0%
what if M was 5 million you know 50
 

00:01:42.180 --> 00:01:45.260 align:start position:0%
what if M was 5 million you know 50
million<00:01:42.509><c> or</c><00:01:42.659><c> even</c><00:01:42.930><c> bigger</c><00:01:43.909><c> with</c><00:01:44.909><c> the</c>

00:01:45.260 --> 00:01:45.270 align:start position:0%
million or even bigger with the
 

00:01:45.270 --> 00:01:47.330 align:start position:0%
million or even bigger with the
implementation<00:01:46.049><c> of</c><00:01:46.170><c> gradient</c><00:01:46.439><c> sent</c><00:01:46.890><c> on</c><00:01:47.189><c> your</c>

00:01:47.330 --> 00:01:47.340 align:start position:0%
implementation of gradient sent on your
 

00:01:47.340 --> 00:01:49.550 align:start position:0%
implementation of gradient sent on your
training<00:01:47.939><c> set</c><00:01:48.210><c> what</c><00:01:48.869><c> you</c><00:01:48.960><c> have</c><00:01:49.110><c> to</c><00:01:49.229><c> do</c><00:01:49.350><c> is</c><00:01:49.470><c> you</c>

00:01:49.550 --> 00:01:49.560 align:start position:0%
training set what you have to do is you
 

00:01:49.560 --> 00:01:51.560 align:start position:0%
training set what you have to do is you
have<00:01:49.680><c> to</c><00:01:49.740><c> process</c><00:01:50.009><c> your</c><00:01:50.549><c> entire</c><00:01:50.909><c> training</c><00:01:51.299><c> set</c>

00:01:51.560 --> 00:01:51.570 align:start position:0%
have to process your entire training set
 

00:01:51.570 --> 00:01:53.389 align:start position:0%
have to process your entire training set
before<00:01:51.930><c> you</c><00:01:52.049><c> take</c><00:01:52.320><c> you</c><00:01:52.560><c> know</c><00:01:52.590><c> one</c><00:01:52.860><c> little</c><00:01:53.159><c> step</c>

00:01:53.389 --> 00:01:53.399 align:start position:0%
before you take you know one little step
 

00:01:53.399 --> 00:01:55.280 align:start position:0%
before you take you know one little step
for<00:01:53.610><c> gradient</c><00:01:53.939><c> descent</c><00:01:54.270><c> and</c><00:01:54.509><c> then</c><00:01:55.140><c> you</c><00:01:55.200><c> have</c>

00:01:55.280 --> 00:01:55.290 align:start position:0%
for gradient descent and then you have
 

00:01:55.290 --> 00:01:56.780 align:start position:0%
for gradient descent and then you have
to<00:01:55.380><c> process</c><00:01:55.560><c> your</c><00:01:55.890><c> entire</c><00:01:56.219><c> training</c><00:01:56.490><c> set</c><00:01:56.729><c> of</c>

00:01:56.780 --> 00:01:56.790 align:start position:0%
to process your entire training set of
 

00:01:56.790 --> 00:01:58.340 align:start position:0%
to process your entire training set of
five<00:01:57.090><c> million</c><00:01:57.360><c> training</c><00:01:57.750><c> examples</c><00:01:58.110><c> again</c>

00:01:58.340 --> 00:01:58.350 align:start position:0%
five million training examples again
 

00:01:58.350 --> 00:01:59.840 align:start position:0%
five million training examples again
before<00:01:58.560><c> you</c><00:01:58.740><c> take</c><00:01:58.920><c> another</c><00:01:59.189><c> little</c><00:01:59.430><c> step</c><00:01:59.729><c> of</c>

00:01:59.840 --> 00:01:59.850 align:start position:0%
before you take another little step of
 

00:01:59.850 --> 00:02:01.700 align:start position:0%
before you take another little step of
gradient<00:01:59.939><c> descent</c><00:02:00.210><c> so</c><00:02:00.869><c> it</c><00:02:00.990><c> turns</c><00:02:01.200><c> out</c><00:02:01.380><c> that</c>

00:02:01.700 --> 00:02:01.710 align:start position:0%
gradient descent so it turns out that
 

00:02:01.710 --> 00:02:04.340 align:start position:0%
gradient descent so it turns out that
you<00:02:02.130><c> can</c><00:02:02.369><c> get</c><00:02:02.579><c> a</c><00:02:02.610><c> faster</c><00:02:03.299><c> algorithm</c><00:02:03.659><c> if</c><00:02:04.170><c> you</c>

00:02:04.340 --> 00:02:04.350 align:start position:0%
you can get a faster algorithm if you
 

00:02:04.350 --> 00:02:05.539 align:start position:0%
you can get a faster algorithm if you
get<00:02:04.560><c> straightened</c><00:02:04.950><c> descent</c>

00:02:05.539 --> 00:02:05.549 align:start position:0%
get straightened descent
 

00:02:05.549 --> 00:02:07.639 align:start position:0%
get straightened descent
start<00:02:06.030><c> to</c><00:02:06.149><c> make</c><00:02:06.329><c> some</c><00:02:06.509><c> progress</c><00:02:06.570><c> even</c><00:02:07.469><c> before</c>

00:02:07.639 --> 00:02:07.649 align:start position:0%
start to make some progress even before
 

00:02:07.649 --> 00:02:10.279 align:start position:0%
start to make some progress even before
you<00:02:07.979><c> finish</c><00:02:08.310><c> processing</c><00:02:08.550><c> your</c><00:02:09.390><c> entire</c><00:02:09.780><c> your</c>

00:02:10.279 --> 00:02:10.289 align:start position:0%
you finish processing your entire your
 

00:02:10.289 --> 00:02:11.120 align:start position:0%
you finish processing your entire your
giant<00:02:10.830><c> tree</c>

00:02:11.120 --> 00:02:11.130 align:start position:0%
giant tree
 

00:02:11.130 --> 00:02:14.720 align:start position:0%
giant tree
in<00:02:11.190><c> size</c><00:02:11.400><c> of</c><00:02:11.550><c> five</c><00:02:11.730><c> million</c><00:02:12.030><c> examples</c><00:02:13.730><c> in</c>

00:02:14.720 --> 00:02:14.730 align:start position:0%
in size of five million examples in
 

00:02:14.730 --> 00:02:16.910 align:start position:0%
in size of five million examples in
particular<00:02:15.060><c> here's</c><00:02:15.570><c> what</c><00:02:15.690><c> you</c><00:02:15.810><c> can</c><00:02:15.960><c> do</c><00:02:16.140><c> let's</c>

00:02:16.910 --> 00:02:16.920 align:start position:0%
particular here's what you can do let's
 

00:02:16.920 --> 00:02:18.470 align:start position:0%
particular here's what you can do let's
say<00:02:17.100><c> that</c><00:02:17.250><c> you</c><00:02:17.340><c> split</c><00:02:17.670><c> up</c><00:02:17.790><c> your</c><00:02:17.820><c> training</c><00:02:18.180><c> set</c>

00:02:18.470 --> 00:02:18.480 align:start position:0%
say that you split up your training set
 

00:02:18.480 --> 00:02:20.990 align:start position:0%
say that you split up your training set
into<00:02:18.900><c> smaller</c><00:02:19.410><c> your</c><00:02:19.920><c> little</c><00:02:20.220><c> baby</c><00:02:20.460><c> training</c>

00:02:20.990 --> 00:02:21.000 align:start position:0%
into smaller your little baby training
 

00:02:21.000 --> 00:02:23.690 align:start position:0%
into smaller your little baby training
sets<00:02:21.540><c> and</c><00:02:21.780><c> these</c><00:02:22.320><c> baby</c><00:02:22.740><c> training</c><00:02:23.190><c> sets</c><00:02:23.460><c> are</c>

00:02:23.690 --> 00:02:23.700 align:start position:0%
sets and these baby training sets are
 

00:02:23.700 --> 00:02:24.050 align:start position:0%
sets and these baby training sets are
called

00:02:24.050 --> 00:02:24.060 align:start position:0%
called
 

00:02:24.060 --> 00:02:30.200 align:start position:0%
called
mini<00:02:24.720><c> batches</c><00:02:26.450><c> and</c><00:02:28.490><c> let's</c><00:02:29.490><c> say</c><00:02:29.640><c> each</c><00:02:29.820><c> of</c><00:02:30.000><c> your</c>

00:02:30.200 --> 00:02:30.210 align:start position:0%
mini batches and let's say each of your
 

00:02:30.210 --> 00:02:33.140 align:start position:0%
mini batches and let's say each of your
baby<00:02:30.540><c> training</c><00:02:30.930><c> sets</c><00:02:31.200><c> have</c><00:02:31.950><c> just</c><00:02:32.370><c> 1000</c>

00:02:33.140 --> 00:02:33.150 align:start position:0%
baby training sets have just 1000
 

00:02:33.150 --> 00:02:38.200 align:start position:0%
baby training sets have just 1000
examples<00:02:33.600><c> each</c><00:02:35.090><c> so</c><00:02:36.090><c> you</c><00:02:36.150><c> take</c><00:02:36.510><c> X</c><00:02:36.720><c> 1</c><00:02:37.020><c> through</c><00:02:37.350><c> X</c>

00:02:38.200 --> 00:02:38.210 align:start position:0%
examples each so you take X 1 through X
 

00:02:38.210 --> 00:02:41.300 align:start position:0%
examples each so you take X 1 through X
1000<00:02:39.210><c> and</c><00:02:39.450><c> you</c><00:02:40.140><c> call</c><00:02:40.410><c> that</c><00:02:40.440><c> your</c><00:02:40.770><c> first</c><00:02:41.040><c> little</c>

00:02:41.300 --> 00:02:41.310 align:start position:0%
1000 and you call that your first little
 

00:02:41.310 --> 00:02:43.010 align:start position:0%
1000 and you call that your first little
baby<00:02:41.520><c> training</c><00:02:42.000><c> session</c><00:02:42.360><c> also</c><00:02:42.480><c> called</c><00:02:42.750><c> a</c><00:02:42.810><c> mini</c>

00:02:43.010 --> 00:02:43.020 align:start position:0%
baby training session also called a mini
 

00:02:43.020 --> 00:02:45.800 align:start position:0%
baby training session also called a mini
batch<00:02:43.230><c> and</c><00:02:43.530><c> then</c><00:02:44.190><c> you</c><00:02:44.400><c> take</c><00:02:44.670><c> home</c><00:02:44.940><c> the</c><00:02:45.510><c> next</c>

00:02:45.800 --> 00:02:45.810 align:start position:0%
batch and then you take home the next
 

00:02:45.810 --> 00:02:52.400 align:start position:0%
batch and then you take home the next
1000<00:02:46.470><c> examples</c><00:02:46.880><c> X</c><00:02:47.880><c> 1000</c><00:02:48.870><c> 1</c><00:02:49.280><c> through</c><00:02:50.280><c> X</c><00:02:51.410><c> 2000</c>

00:02:52.400 --> 00:02:52.410 align:start position:0%
1000 examples X 1000 1 through X 2000
 

00:02:52.410 --> 00:02:54.080 align:start position:0%
1000 examples X 1000 1 through X 2000
that's<00:02:52.740><c> the</c><00:02:52.920><c> next</c><00:02:53.160><c> thousand</c><00:02:53.490><c> examples</c><00:02:53.730><c> and</c>

00:02:54.080 --> 00:02:54.090 align:start position:0%
that's the next thousand examples and
 

00:02:54.090 --> 00:02:57.110 align:start position:0%
that's the next thousand examples and
call<00:02:54.270><c> the</c><00:02:54.390><c> next</c><00:02:54.600><c> one</c><00:02:54.780><c> and</c><00:02:54.990><c> so</c><00:02:55.650><c> on</c><00:02:55.680><c> and</c><00:02:56.190><c> I'm</c>

00:02:57.110 --> 00:02:57.120 align:start position:0%
call the next one and so on and I'm
 

00:02:57.120 --> 00:02:58.700 align:start position:0%
call the next one and so on and I'm
going<00:02:57.300><c> to</c><00:02:57.360><c> introduce</c><00:02:57.510><c> a</c><00:02:57.810><c> new</c><00:02:57.840><c> notation</c><00:02:58.560><c> I'm</c>

00:02:58.700 --> 00:02:58.710 align:start position:0%
going to introduce a new notation I'm
 

00:02:58.710 --> 00:03:02.390 align:start position:0%
going to introduce a new notation I'm
going<00:02:58.770><c> to</c><00:02:58.950><c> call</c><00:02:59.190><c> this</c><00:02:59.900><c> X</c><00:03:00.900><c> superscript</c><00:03:01.710><c> with</c>

00:03:02.390 --> 00:03:02.400 align:start position:0%
going to call this X superscript with
 

00:03:02.400 --> 00:03:07.310 align:start position:0%
going to call this X superscript with
curly<00:03:02.970><c> braces</c><00:03:03.410><c> 1</c><00:03:04.410><c> and</c><00:03:04.860><c> I</c><00:03:05.760><c> want</c><00:03:05.880><c> to</c><00:03:06.000><c> call</c><00:03:06.180><c> this</c><00:03:06.450><c> X</c>

00:03:07.310 --> 00:03:07.320 align:start position:0%
curly braces 1 and I want to call this X
 

00:03:07.320 --> 00:03:12.380 align:start position:0%
curly braces 1 and I want to call this X
superscript<00:03:08.310><c> with</c><00:03:08.970><c> curly</c><00:03:09.420><c> braces</c><00:03:10.370><c> too</c><00:03:11.370><c> now</c><00:03:12.120><c> if</c>

00:03:12.380 --> 00:03:12.390 align:start position:0%
superscript with curly braces too now if
 

00:03:12.390 --> 00:03:14.600 align:start position:0%
superscript with curly braces too now if
you<00:03:12.510><c> have</c><00:03:12.630><c> five</c><00:03:12.990><c> million</c><00:03:13.290><c> training</c><00:03:14.220><c> examples</c>

00:03:14.600 --> 00:03:14.610 align:start position:0%
you have five million training examples
 

00:03:14.610 --> 00:03:16.220 align:start position:0%
you have five million training examples
total<00:03:15.060><c> and</c><00:03:15.240><c> each</c><00:03:15.420><c> of</c><00:03:15.570><c> these</c><00:03:15.690><c> little</c><00:03:15.930><c> mini</c>

00:03:16.220 --> 00:03:16.230 align:start position:0%
total and each of these little mini
 

00:03:16.230 --> 00:03:18.470 align:start position:0%
total and each of these little mini
batches<00:03:16.500><c> as</c><00:03:16.890><c> a</c><00:03:16.920><c> thousand</c><00:03:17.610><c> examples</c><00:03:17.970><c> that</c>

00:03:18.470 --> 00:03:18.480 align:start position:0%
batches as a thousand examples that
 

00:03:18.480 --> 00:03:21.050 align:start position:0%
batches as a thousand examples that
means<00:03:18.720><c> you</c><00:03:18.840><c> have</c><00:03:19.310><c> 5000</c><00:03:20.310><c> of</c><00:03:20.430><c> these</c><00:03:20.580><c> videos</c><00:03:20.940><c> you</c>

00:03:21.050 --> 00:03:21.060 align:start position:0%
means you have 5000 of these videos you
 

00:03:21.060 --> 00:03:24.590 align:start position:0%
means you have 5000 of these videos you
know<00:03:21.150><c> 5000</c><00:03:21.750><c> times</c><00:03:21.840><c> 1000</c><00:03:22.560><c> equals</c><00:03:22.830><c> 5</c><00:03:23.550><c> million</c><00:03:24.000><c> so</c>

00:03:24.590 --> 00:03:24.600 align:start position:0%
know 5000 times 1000 equals 5 million so
 

00:03:24.600 --> 00:03:28.400 align:start position:0%
know 5000 times 1000 equals 5 million so
altogether<00:03:24.990><c> you</c><00:03:25.530><c> would</c><00:03:25.650><c> have</c><00:03:26.660><c> 5000</c><00:03:27.660><c> of</c><00:03:27.900><c> these</c>

00:03:28.400 --> 00:03:28.410 align:start position:0%
altogether you would have 5000 of these
 

00:03:28.410 --> 00:03:31.940 align:start position:0%
altogether you would have 5000 of these
um<00:03:28.740><c> mini</c><00:03:29.430><c> batches</c><00:03:30.230><c> so</c><00:03:31.230><c> the</c><00:03:31.290><c> ends</c><00:03:31.590><c> of</c><00:03:31.680><c> X</c>

00:03:31.940 --> 00:03:31.950 align:start position:0%
um mini batches so the ends of X
 

00:03:31.950 --> 00:03:34.880 align:start position:0%
um mini batches so the ends of X
superscript<00:03:32.250><c> curly</c><00:03:33.000><c> braces</c><00:03:33.210><c> 5000</c><00:03:33.990><c> and</c><00:03:34.290><c> then</c>

00:03:34.880 --> 00:03:34.890 align:start position:0%
superscript curly braces 5000 and then
 

00:03:34.890 --> 00:03:36.980 align:start position:0%
superscript curly braces 5000 and then
similarly<00:03:35.310><c> you</c><00:03:35.430><c> do</c><00:03:35.700><c> the</c><00:03:35.790><c> same</c><00:03:35.970><c> thing</c><00:03:36.210><c> for</c><00:03:36.810><c> y</c>

00:03:36.980 --> 00:03:36.990 align:start position:0%
similarly you do the same thing for y
 

00:03:36.990 --> 00:03:39.470 align:start position:0%
similarly you do the same thing for y
you'd<00:03:37.320><c> also</c><00:03:37.680><c> split</c><00:03:37.980><c> up</c><00:03:38.250><c> your</c><00:03:38.910><c> training</c><00:03:39.210><c> data</c>

00:03:39.470 --> 00:03:39.480 align:start position:0%
you'd also split up your training data
 

00:03:39.480 --> 00:03:43.910 align:start position:0%
you'd also split up your training data
for<00:03:39.660><c> Y</c><00:03:40.430><c> accordingly</c><00:03:41.430><c> so</c><00:03:42.240><c> you</c><00:03:42.330><c> call</c><00:03:42.510><c> that</c><00:03:42.920><c> y1</c>

00:03:43.910 --> 00:03:43.920 align:start position:0%
for Y accordingly so you call that y1
 

00:03:43.920 --> 00:03:51.050 align:start position:0%
for Y accordingly so you call that y1
and<00:03:44.220><c> then</c><00:03:45.209><c> this</c><00:03:45.420><c> is</c><00:03:45.600><c> y</c><00:03:46.280><c> 1001</c><00:03:47.720><c> 3y</c><00:03:49.010><c> 2000</c><00:03:50.060><c> this</c>

00:03:51.050 --> 00:03:51.060 align:start position:0%
and then this is y 1001 3y 2000 this
 

00:03:51.060 --> 00:03:56.840 align:start position:0%
and then this is y 1001 3y 2000 this
becomes<00:03:51.630><c> called</c><00:03:53.330><c> y2</c><00:03:54.330><c> and</c><00:03:54.980><c> so</c><00:03:55.980><c> on</c><00:03:56.160><c> until</c><00:03:56.820><c> you</c>

00:03:56.840 --> 00:03:56.850 align:start position:0%
becomes called y2 and so on until you
 

00:03:56.850 --> 00:04:04.720 align:start position:0%
becomes called y2 and so on until you
have<00:03:57.170><c> y</c><00:03:59.090><c> 5000</c><00:04:00.170><c> so</c><00:04:01.170><c> now</c><00:04:03.530><c> we</c>

00:04:04.720 --> 00:04:04.730 align:start position:0%
have y 5000 so now we
 

00:04:04.730 --> 00:04:07.619 align:start position:0%
have y 5000 so now we
-<00:04:04.849><c> number</c><00:04:05.269><c> T</c><00:04:05.480><c> is</c><00:04:05.840><c> going</c><00:04:06.140><c> to</c><00:04:06.230><c> be</c><00:04:06.290><c> comprised</c><00:04:06.769><c> of</c><00:04:07.129><c> X</c>

00:04:07.619 --> 00:04:07.629 align:start position:0%
- number T is going to be comprised of X
 

00:04:07.629 --> 00:04:14.680 align:start position:0%
- number T is going to be comprised of X
T<00:04:08.629><c> and</c><00:04:09.549><c> Y</c><00:04:11.170><c> T</c><00:04:12.170><c> and</c><00:04:12.440><c> that</c><00:04:12.860><c> is</c><00:04:12.920><c> a</c><00:04:13.690><c> thousand</c>

00:04:14.680 --> 00:04:14.690 align:start position:0%
T and Y T and that is a thousand
 

00:04:14.690 --> 00:04:16.420 align:start position:0%
T and Y T and that is a thousand
training<00:04:15.080><c> examples</c><00:04:15.500><c> so</c><00:04:15.680><c> the</c><00:04:15.799><c> corresponding</c>

00:04:16.420 --> 00:04:16.430 align:start position:0%
training examples so the corresponding
 

00:04:16.430 --> 00:04:19.390 align:start position:0%
training examples so the corresponding
input<00:04:16.639><c> output</c><00:04:17.180><c> pairs</c><00:04:17.500><c> before</c><00:04:18.500><c> moving</c><00:04:18.859><c> on</c><00:04:19.100><c> just</c>

00:04:19.390 --> 00:04:19.400 align:start position:0%
input output pairs before moving on just
 

00:04:19.400 --> 00:04:20.349 align:start position:0%
input output pairs before moving on just
to<00:04:19.669><c> make</c><00:04:20.180><c> sure</c>

00:04:20.349 --> 00:04:20.359 align:start position:0%
to make sure
 

00:04:20.359 --> 00:04:22.990 align:start position:0%
to make sure
notation<00:04:21.199><c> is</c><00:04:21.440><c> clear</c><00:04:21.709><c> we</c><00:04:22.370><c> have</c><00:04:22.550><c> previously</c>

00:04:22.990 --> 00:04:23.000 align:start position:0%
notation is clear we have previously
 

00:04:23.000 --> 00:04:25.480 align:start position:0%
notation is clear we have previously
used<00:04:23.080><c> superscript</c><00:04:24.080><c> round</c><00:04:24.320><c> brackets</c><00:04:24.830><c> I</c><00:04:24.919><c> to</c>

00:04:25.480 --> 00:04:25.490 align:start position:0%
used superscript round brackets I to
 

00:04:25.490 --> 00:04:27.969 align:start position:0%
used superscript round brackets I to
index<00:04:25.880><c> on</c><00:04:26.030><c> the</c><00:04:26.120><c> training</c><00:04:26.360><c> set</c><00:04:26.600><c> so</c><00:04:26.900><c> X</c><00:04:27.139><c> is</c><00:04:27.350><c> d</c><00:04:27.800><c> I've</c>

00:04:27.969 --> 00:04:27.979 align:start position:0%
index on the training set so X is d I've
 

00:04:27.979 --> 00:04:30.280 align:start position:0%
index on the training set so X is d I've
trained<00:04:28.280><c> example</c><00:04:28.790><c> we</c><00:04:29.479><c> use</c><00:04:29.660><c> superscript</c>

00:04:30.280 --> 00:04:30.290 align:start position:0%
trained example we use superscript
 

00:04:30.290 --> 00:04:33.490 align:start position:0%
trained example we use superscript
square<00:04:31.070><c> brackets</c><00:04:31.490><c> L</c><00:04:31.639><c> to</c><00:04:32.479><c> index</c><00:04:32.990><c> into</c><00:04:33.260><c> the</c>

00:04:33.490 --> 00:04:33.500 align:start position:0%
square brackets L to index into the
 

00:04:33.500 --> 00:04:35.140 align:start position:0%
square brackets L to index into the
different<00:04:33.830><c> layers</c><00:04:33.979><c> of</c><00:04:34.130><c> a</c><00:04:34.340><c> neural</c><00:04:34.580><c> network</c><00:04:34.760><c> so</c>

00:04:35.140 --> 00:04:35.150 align:start position:0%
different layers of a neural network so
 

00:04:35.150 --> 00:04:39.550 align:start position:0%
different layers of a neural network so
VL<00:04:35.740><c> comes</c><00:04:36.740><c> from</c><00:04:36.979><c> the</c><00:04:37.630><c> Z</c><00:04:38.630><c> values</c><00:04:39.080><c> for</c><00:04:39.320><c> the</c><00:04:39.380><c> elf</c>

00:04:39.550 --> 00:04:39.560 align:start position:0%
VL comes from the Z values for the elf
 

00:04:39.560 --> 00:04:42.070 align:start position:0%
VL comes from the Z values for the elf
layer<00:04:39.860><c> of</c><00:04:40.070><c> in</c><00:04:40.220><c> your</c><00:04:40.370><c> network</c><00:04:40.760><c> and</c><00:04:41.060><c> here</c><00:04:41.810><c> we're</c>

00:04:42.070 --> 00:04:42.080 align:start position:0%
layer of in your network and here we're
 

00:04:42.080 --> 00:04:45.909 align:start position:0%
layer of in your network and here we're
introducing<00:04:42.500><c> the</c><00:04:42.919><c> curly</c><00:04:43.699><c> brackets</c><00:04:44.270><c> T</c><00:04:44.990><c> to</c>

00:04:45.909 --> 00:04:45.919 align:start position:0%
introducing the curly brackets T to
 

00:04:45.919 --> 00:04:48.010 align:start position:0%
introducing the curly brackets T to
index<00:04:46.430><c> into</c><00:04:46.610><c> different</c><00:04:47.090><c> mini</c><00:04:47.300><c> batches</c><00:04:47.570><c> so</c><00:04:47.960><c> you</c>

00:04:48.010 --> 00:04:48.020 align:start position:0%
index into different mini batches so you
 

00:04:48.020 --> 00:04:53.140 align:start position:0%
index into different mini batches so you
have<00:04:48.229><c> X</c><00:04:48.470><c> T</c><00:04:48.770><c> Y</c><00:04:50.139><c> T</c><00:04:51.139><c> and</c><00:04:51.460><c> to</c><00:04:52.460><c> check</c><00:04:52.820><c> your</c>

00:04:53.140 --> 00:04:53.150 align:start position:0%
have X T Y T and to check your
 

00:04:53.150 --> 00:04:56.230 align:start position:0%
have X T Y T and to check your
understanding<00:04:53.389><c> of</c><00:04:54.050><c> these</c><00:04:54.770><c> um</c><00:04:55.220><c> or</c><00:04:55.760><c> what's</c><00:04:56.120><c> the</c>

00:04:56.230 --> 00:04:56.240 align:start position:0%
understanding of these um or what's the
 

00:04:56.240 --> 00:05:02.800 align:start position:0%
understanding of these um or what's the
dimension<00:04:56.949><c> right</c><00:04:57.949><c> of</c><00:04:58.990><c> XT</c><00:04:59.990><c> and</c><00:05:00.229><c> YT</c><00:05:00.789><c> well</c><00:05:01.789><c> X</c><00:05:02.330><c> is</c>

00:05:02.800 --> 00:05:02.810 align:start position:0%
dimension right of XT and YT well X is
 

00:05:02.810 --> 00:05:07.120 align:start position:0%
dimension right of XT and YT well X is
NX<00:05:03.500><c> by</c><00:05:03.740><c> M</c><00:05:04.010><c> so</c><00:05:04.729><c> if</c><00:05:04.970><c> x1</c><00:05:05.510><c> is</c><00:05:05.930><c> a</c><00:05:06.020><c> thousand</c><00:05:06.800><c> training</c>

00:05:07.120 --> 00:05:07.130 align:start position:0%
NX by M so if x1 is a thousand training
 

00:05:07.130 --> 00:05:09.190 align:start position:0%
NX by M so if x1 is a thousand training
examples<00:05:07.639><c> or</c><00:05:07.940><c> the</c><00:05:08.030><c> X</c><00:05:08.210><c> values</c><00:05:08.660><c> for</c><00:05:08.810><c> a</c><00:05:08.870><c> thousand</c>

00:05:09.190 --> 00:05:09.200 align:start position:0%
examples or the X values for a thousand
 

00:05:09.200 --> 00:05:11.290 align:start position:0%
examples or the X values for a thousand
examples<00:05:09.440><c> then</c><00:05:10.310><c> this</c><00:05:10.490><c> dimension</c><00:05:11.000><c> should</c><00:05:11.030><c> be</c>

00:05:11.290 --> 00:05:11.300 align:start position:0%
examples then this dimension should be
 

00:05:11.300 --> 00:05:16.690 align:start position:0%
examples then this dimension should be
MX<00:05:11.690><c> by</c><00:05:12.470><c> 1,000</c><00:05:13.400><c> and</c><00:05:13.780><c> x2</c><00:05:14.780><c> should</c><00:05:15.200><c> also</c><00:05:15.380><c> be</c><00:05:15.770><c> an</c><00:05:16.010><c> X</c>

00:05:16.690 --> 00:05:16.700 align:start position:0%
MX by 1,000 and x2 should also be an X
 

00:05:16.700 --> 00:05:20.020 align:start position:0%
MX by 1,000 and x2 should also be an X
by<00:05:17.210><c> 1000</c><00:05:18.020><c> and</c><00:05:18.229><c> so</c><00:05:18.260><c> on</c><00:05:19.039><c> so</c><00:05:19.370><c> all</c><00:05:19.520><c> of</c><00:05:19.669><c> these</c><00:05:19.789><c> should</c>

00:05:20.020 --> 00:05:20.030 align:start position:0%
by 1000 and so on so all of these should
 

00:05:20.030 --> 00:05:23.080 align:start position:0%
by 1000 and so on so all of these should
have<00:05:20.300><c> to</c><00:05:20.479><c> mention</c><00:05:20.690><c> NX</c><00:05:21.380><c> /</c><00:05:21.560><c> 1000</c><00:05:22.250><c> and</c><00:05:22.430><c> these</c>

00:05:23.080 --> 00:05:23.090 align:start position:0%
have to mention NX / 1000 and these
 

00:05:23.090 --> 00:05:30.990 align:start position:0%
have to mention NX / 1000 and these
should<00:05:23.539><c> have</c><00:05:23.690><c> to</c><00:05:23.930><c> mention</c><00:05:24.110><c> 1</c><00:05:24.710><c> by</c><00:05:26.020><c> 1000</c><00:05:27.490><c> right</c>

00:05:30.990 --> 00:05:31.000 align:start position:0%
 
 

00:05:31.000 --> 00:05:32.490 align:start position:0%
 
2

00:05:32.490 --> 00:05:32.500 align:start position:0%
2
 

00:05:32.500 --> 00:05:35.580 align:start position:0%
2
the<00:05:32.860><c> name</c><00:05:33.100><c> of</c><00:05:33.130><c> this</c><00:05:33.490><c> algorithm</c><00:05:33.960><c> -</c><00:05:34.960><c> gradient</c>

00:05:35.580 --> 00:05:35.590 align:start position:0%
the name of this algorithm - gradient
 

00:05:35.590 --> 00:05:38.190 align:start position:0%
the name of this algorithm - gradient
descent<00:05:36.010><c> refers</c><00:05:36.670><c> to</c><00:05:36.700><c> the</c><00:05:37.240><c> gradient</c><00:05:37.780><c> descent</c>

00:05:38.190 --> 00:05:38.200 align:start position:0%
descent refers to the gradient descent
 

00:05:38.200 --> 00:05:39.600 align:start position:0%
descent refers to the gradient descent
algorithm<00:05:38.380><c> we've</c><00:05:38.860><c> been</c><00:05:39.010><c> talking</c><00:05:39.460><c> about</c>

00:05:39.600 --> 00:05:39.610 align:start position:0%
algorithm we've been talking about
 

00:05:39.610 --> 00:05:41.280 align:start position:0%
algorithm we've been talking about
previously<00:05:39.850><c> where</c><00:05:40.360><c> you</c><00:05:40.420><c> process</c><00:05:40.840><c> your</c><00:05:40.870><c> entire</c>

00:05:41.280 --> 00:05:41.290 align:start position:0%
previously where you process your entire
 

00:05:41.290 --> 00:05:43.200 align:start position:0%
previously where you process your entire
training<00:05:41.590><c> set</c><00:05:41.860><c> all</c><00:05:42.070><c> at</c><00:05:42.310><c> the</c><00:05:42.460><c> same</c><00:05:42.640><c> time</c><00:05:42.700><c> and</c>

00:05:43.200 --> 00:05:43.210 align:start position:0%
training set all at the same time and
 

00:05:43.210 --> 00:05:46.320 align:start position:0%
training set all at the same time and
the<00:05:43.870><c> name</c><00:05:44.050><c> comes</c><00:05:44.350><c> from</c><00:05:45.000><c> viewing</c><00:05:46.000><c> that</c><00:05:46.150><c> as</c>

00:05:46.320 --> 00:05:46.330 align:start position:0%
the name comes from viewing that as
 

00:05:46.330 --> 00:05:48.780 align:start position:0%
the name comes from viewing that as
processing<00:05:46.930><c> your</c><00:05:47.080><c> entire</c><00:05:47.470><c> batch</c><00:05:47.920><c> of</c><00:05:48.310><c> training</c>

00:05:48.780 --> 00:05:48.790 align:start position:0%
processing your entire batch of training
 

00:05:48.790 --> 00:05:50.670 align:start position:0%
processing your entire batch of training
examples<00:05:48.880><c> all</c><00:05:49.480><c> at</c><00:05:49.570><c> the</c><00:05:49.660><c> same</c><00:05:49.810><c> time</c><00:05:49.870><c> I'm</c><00:05:50.440><c> not</c>

00:05:50.670 --> 00:05:50.680 align:start position:0%
examples all at the same time I'm not
 

00:05:50.680 --> 00:05:52.410 align:start position:0%
examples all at the same time I'm not
such<00:05:50.860><c> a</c><00:05:50.890><c> great</c><00:05:51.130><c> name</c><00:05:51.310><c> but</c><00:05:51.580><c> that's</c><00:05:51.820><c> just</c><00:05:52.030><c> what</c>

00:05:52.410 --> 00:05:52.420 align:start position:0%
such a great name but that's just what
 

00:05:52.420 --> 00:05:54.570 align:start position:0%
such a great name but that's just what
is<00:05:52.570><c> called</c><00:05:52.840><c> mini</c><00:05:53.320><c> batch</c><00:05:53.590><c> period</c><00:05:53.950><c> descent</c><00:05:54.310><c> in</c>

00:05:54.570 --> 00:05:54.580 align:start position:0%
is called mini batch period descent in
 

00:05:54.580 --> 00:05:57.630 align:start position:0%
is called mini batch period descent in
contrast<00:05:55.120><c> refers</c><00:05:55.960><c> to</c><00:05:55.990><c> the</c><00:05:56.740><c> algorithm</c><00:05:57.430><c> which</c>

00:05:57.630 --> 00:05:57.640 align:start position:0%
contrast refers to the algorithm which
 

00:05:57.640 --> 00:05:59.220 align:start position:0%
contrast refers to the algorithm which
we'll<00:05:57.790><c> talk</c><00:05:57.910><c> about</c><00:05:58.120><c> on</c><00:05:58.270><c> the</c><00:05:58.360><c> next</c><00:05:58.390><c> slide</c><00:05:58.630><c> and</c>

00:05:59.220 --> 00:05:59.230 align:start position:0%
we'll talk about on the next slide and
 

00:05:59.230 --> 00:06:02.310 align:start position:0%
we'll talk about on the next slide and
which<00:05:59.380><c> you</c><00:05:59.500><c> process</c><00:05:59.740><c> is</c><00:06:00.160><c> single</c><00:06:00.670><c> mini</c><00:06:01.390><c> batch</c><00:06:01.660><c> X</c>

00:06:02.310 --> 00:06:02.320 align:start position:0%
which you process is single mini batch X
 

00:06:02.320 --> 00:06:04.890 align:start position:0%
which you process is single mini batch X
T<00:06:02.620><c> YT</c><00:06:03.220><c> at</c><00:06:03.850><c> the</c><00:06:04.000><c> same</c><00:06:04.210><c> time</c><00:06:04.390><c> rather</c><00:06:04.720><c> than</c>

00:06:04.890 --> 00:06:04.900 align:start position:0%
T YT at the same time rather than
 

00:06:04.900 --> 00:06:07.380 align:start position:0%
T YT at the same time rather than
processing<00:06:05.530><c> your</c><00:06:06.100><c> entire</c><00:06:06.430><c> training</c><00:06:06.730><c> set</c><00:06:07.000><c> X</c><00:06:07.210><c> Y</c>

00:06:07.380 --> 00:06:07.390 align:start position:0%
processing your entire training set X Y
 

00:06:07.390 --> 00:06:10.140 align:start position:0%
processing your entire training set X Y
at<00:06:07.450><c> the</c><00:06:07.570><c> same</c><00:06:07.750><c> time</c><00:06:08.520><c> so</c><00:06:09.520><c> let's</c><00:06:09.850><c> see</c><00:06:09.940><c> how</c><00:06:10.090><c> many</c>

00:06:10.140 --> 00:06:10.150 align:start position:0%
at the same time so let's see how many
 

00:06:10.150 --> 00:06:12.570 align:start position:0%
at the same time so let's see how many
batch<00:06:10.570><c> gradient</c><00:06:10.870><c> descent</c><00:06:11.140><c> works</c><00:06:11.620><c> to</c><00:06:12.400><c> run</c>

00:06:12.570 --> 00:06:12.580 align:start position:0%
batch gradient descent works to run
 

00:06:12.580 --> 00:06:14.340 align:start position:0%
batch gradient descent works to run
mini-batch<00:06:13.030><c> gradient</c><00:06:13.060><c> descent</c><00:06:13.690><c> on</c><00:06:13.900><c> your</c>

00:06:14.340 --> 00:06:14.350 align:start position:0%
mini-batch gradient descent on your
 

00:06:14.350 --> 00:06:17.610 align:start position:0%
mini-batch gradient descent on your
training<00:06:14.650><c> sets</c><00:06:15.010><c> you</c><00:06:15.220><c> would</c><00:06:15.370><c> run</c><00:06:16.150><c> for</c><00:06:16.870><c> t</c><00:06:17.140><c> equals</c>

00:06:17.610 --> 00:06:17.620 align:start position:0%
training sets you would run for t equals
 

00:06:17.620 --> 00:06:22.050 align:start position:0%
training sets you would run for t equals
1<00:06:18.030><c> to</c><00:06:19.169><c> 5000</c><00:06:20.169><c> because</c><00:06:20.919><c> we</c><00:06:21.070><c> had</c><00:06:21.220><c> 5000</c><00:06:21.880><c> mini</c>

00:06:22.050 --> 00:06:22.060 align:start position:0%
1 to 5000 because we had 5000 mini
 

00:06:22.060 --> 00:06:25.170 align:start position:0%
1 to 5000 because we had 5000 mini
batches<00:06:22.330><c> of</c><00:06:22.720><c> size</c><00:06:23.160><c> 1,000</c><00:06:24.160><c> each</c><00:06:24.310><c> and</c><00:06:24.550><c> what</c>

00:06:25.170 --> 00:06:25.180 align:start position:0%
batches of size 1,000 each and what
 

00:06:25.180 --> 00:06:26.550 align:start position:0%
batches of size 1,000 each and what
you're<00:06:25.300><c> going</c><00:06:25.390><c> to</c><00:06:25.510><c> do</c><00:06:25.690><c> inside</c><00:06:26.169><c> the</c><00:06:26.320><c> for</c><00:06:26.530><c> loop</c>

00:06:26.550 --> 00:06:26.560 align:start position:0%
you're going to do inside the for loop
 

00:06:26.560 --> 00:06:29.520 align:start position:0%
you're going to do inside the for loop
is<00:06:26.919><c> basically</c><00:06:27.610><c> implement</c><00:06:28.210><c> one</c><00:06:28.960><c> step</c><00:06:29.320><c> of</c>

00:06:29.520 --> 00:06:29.530 align:start position:0%
is basically implement one step of
 

00:06:29.530 --> 00:06:40.010 align:start position:0%
is basically implement one step of
gradient<00:06:29.560><c> descent</c><00:06:33.120><c> using</c><00:06:34.740><c> X</c><00:06:35.740><c> G</c><00:06:36.840><c> comma</c><00:06:37.979><c> Y</c><00:06:38.979><c> T</c><00:06:39.040><c> and</c>

00:06:40.010 --> 00:06:40.020 align:start position:0%
gradient descent using X G comma Y T and
 

00:06:40.020 --> 00:06:43.290 align:start position:0%
gradient descent using X G comma Y T and
it's<00:06:41.020><c> as</c><00:06:41.229><c> if</c><00:06:41.500><c> you</c><00:06:41.770><c> had</c><00:06:42.010><c> a</c><00:06:42.040><c> training</c><00:06:42.550><c> set</c><00:06:43.000><c> of</c>

00:06:43.290 --> 00:06:43.300 align:start position:0%
it's as if you had a training set of
 

00:06:43.300 --> 00:06:48.780 align:start position:0%
it's as if you had a training set of
size<00:06:44.070><c> 1,000</c><00:06:45.070><c> examples</c><00:06:45.160><c> and</c><00:06:46.770><c> it</c><00:06:47.770><c> was</c><00:06:47.950><c> as</c><00:06:48.160><c> if</c><00:06:48.430><c> you</c>

00:06:48.780 --> 00:06:48.790 align:start position:0%
size 1,000 examples and it was as if you
 

00:06:48.790 --> 00:06:51.030 align:start position:0%
size 1,000 examples and it was as if you
were<00:06:49.450><c> to</c><00:06:49.720><c> implement</c><00:06:50.169><c> the</c><00:06:50.530><c> algorithm</c><00:06:50.800><c> you're</c>

00:06:51.030 --> 00:06:51.040 align:start position:0%
were to implement the algorithm you're
 

00:06:51.040 --> 00:06:52.740 align:start position:0%
were to implement the algorithm you're
already<00:06:51.220><c> familiar</c><00:06:51.490><c> with</c><00:06:51.790><c> but</c><00:06:52.120><c> just</c><00:06:52.180><c> on</c><00:06:52.510><c> this</c>

00:06:52.740 --> 00:06:52.750 align:start position:0%
already familiar with but just on this
 

00:06:52.750 --> 00:06:55.409 align:start position:0%
already familiar with but just on this
you<00:06:53.350><c> know</c><00:06:53.470><c> little</c><00:06:53.800><c> training</c><00:06:54.040><c> set</c><00:06:54.340><c> size</c><00:06:54.550><c> of</c><00:06:54.880><c> M</c>

00:06:55.409 --> 00:06:55.419 align:start position:0%
you know little training set size of M
 

00:06:55.419 --> 00:06:58.380 align:start position:0%
you know little training set size of M
equals<00:06:55.450><c> 1000</c><00:06:56.440><c> rather</c><00:06:57.160><c> than</c><00:06:57.460><c> having</c><00:06:57.729><c> explicit</c>

00:06:58.380 --> 00:06:58.390 align:start position:0%
equals 1000 rather than having explicit
 

00:06:58.390 --> 00:07:01.080 align:start position:0%
equals 1000 rather than having explicit
for<00:06:58.630><c> loop</c><00:06:58.840><c> over</c><00:06:59.169><c> all</c><00:06:59.200><c> 1000</c><00:06:59.979><c> examples</c><00:07:00.130><c> you</c>

00:07:01.080 --> 00:07:01.090 align:start position:0%
for loop over all 1000 examples you
 

00:07:01.090 --> 00:07:03.180 align:start position:0%
for loop over all 1000 examples you
would<00:07:01.210><c> use</c><00:07:01.390><c> vectorization</c><00:07:01.870><c> to</c><00:07:02.440><c> process</c><00:07:02.919><c> all</c>

00:07:03.180 --> 00:07:03.190 align:start position:0%
would use vectorization to process all
 

00:07:03.190 --> 00:07:05.370 align:start position:0%
would use vectorization to process all
1,000<00:07:03.970><c> examples</c><00:07:04.030><c> sort</c><00:07:04.810><c> of</c><00:07:04.900><c> all</c><00:07:05.020><c> at</c><00:07:05.140><c> the</c><00:07:05.200><c> same</c>

00:07:05.370 --> 00:07:05.380 align:start position:0%
1,000 examples sort of all at the same
 

00:07:05.380 --> 00:07:08.969 align:start position:0%
1,000 examples sort of all at the same
time<00:07:06.479><c> so</c><00:07:07.479><c> let's</c><00:07:07.690><c> write</c><00:07:07.840><c> this</c><00:07:07.870><c> out</c><00:07:08.110><c> first</c><00:07:08.800><c> you</c>

00:07:08.969 --> 00:07:08.979 align:start position:0%
time so let's write this out first you
 

00:07:08.979 --> 00:07:15.300 align:start position:0%
time so let's write this out first you
implement<00:07:09.900><c> forward</c><00:07:10.900><c> prop</c><00:07:11.110><c> on</c><00:07:13.560><c> the</c><00:07:14.560><c> inputs</c><00:07:15.040><c> so</c>

00:07:15.300 --> 00:07:15.310 align:start position:0%
implement forward prop on the inputs so
 

00:07:15.310 --> 00:07:18.600 align:start position:0%
implement forward prop on the inputs so
just<00:07:16.270><c> on</c><00:07:16.330><c> XP</c><00:07:16.810><c> and</c><00:07:17.020><c> you</c><00:07:18.010><c> do</c><00:07:18.190><c> that</c><00:07:18.400><c> by</c>

00:07:18.600 --> 00:07:18.610 align:start position:0%
just on XP and you do that by
 

00:07:18.610 --> 00:07:24.390 align:start position:0%
just on XP and you do that by
implementing<00:07:19.240><c> you</c><00:07:19.990><c> know</c><00:07:20.200><c> Z</c><00:07:20.530><c> 1</c><00:07:21.000><c> equals</c><00:07:22.229><c> W</c><00:07:23.229><c> 1</c><00:07:23.440><c> now</c>

00:07:24.390 --> 00:07:24.400 align:start position:0%
implementing you know Z 1 equals W 1 now
 

00:07:24.400 --> 00:07:27.420 align:start position:0%
implementing you know Z 1 equals W 1 now
previously<00:07:25.030><c> we</c><00:07:25.450><c> just</c><00:07:25.479><c> have</c><00:07:25.870><c> X</c><00:07:26.140><c> there</c><00:07:26.500><c> right</c>

00:07:27.420 --> 00:07:27.430 align:start position:0%
previously we just have X there right
 

00:07:27.430 --> 00:07:29.400 align:start position:0%
previously we just have X there right
but<00:07:27.790><c> now</c><00:07:28.000><c> you're</c><00:07:28.540><c> on</c><00:07:28.630><c> process</c><00:07:29.020><c> the</c><00:07:29.110><c> entire</c>

00:07:29.400 --> 00:07:29.410 align:start position:0%
but now you're on process the entire
 

00:07:29.410 --> 00:07:30.659 align:start position:0%
but now you're on process the entire
training<00:07:29.650><c> set</c><00:07:29.919><c> and</c><00:07:29.979><c> you're</c><00:07:30.160><c> just</c><00:07:30.280><c> processing</c>

00:07:30.659 --> 00:07:30.669 align:start position:0%
training set and you're just processing
 

00:07:30.669 --> 00:07:33.649 align:start position:0%
training set and you're just processing
the<00:07:30.850><c> first</c><00:07:31.060><c> mini</c><00:07:31.330><c> batch</c><00:07:31.570><c> so</c><00:07:31.870><c> this</c><00:07:32.050><c> becomes</c><00:07:32.410><c> X</c>

00:07:33.649 --> 00:07:33.659 align:start position:0%
the first mini batch so this becomes X
 

00:07:33.659 --> 00:07:37.399 align:start position:0%
the first mini batch so this becomes X
tea<00:07:34.659><c> when</c><00:07:35.169><c> you</c><00:07:35.259><c> processing</c><00:07:35.740><c> mini-batch</c><00:07:36.219><c> tea</c>

00:07:37.399 --> 00:07:37.409 align:start position:0%
tea when you processing mini-batch tea
 

00:07:37.409 --> 00:07:43.189 align:start position:0%
tea when you processing mini-batch tea
and<00:07:38.409><c> then</c><00:07:38.650><c> you</c><00:07:38.830><c> would</c><00:07:38.979><c> have</c><00:07:39.840><c> a1</c><00:07:41.219><c> equals</c><00:07:42.219><c> G</c><00:07:42.639><c> 1</c><00:07:42.879><c> of</c>

00:07:43.189 --> 00:07:43.199 align:start position:0%
and then you would have a1 equals G 1 of
 

00:07:43.199 --> 00:07:48.390 align:start position:0%
and then you would have a1 equals G 1 of
Z<00:07:44.199><c> 1</c><00:07:45.270><c> District</c><00:07:46.270><c> Capital</c><00:07:46.870><c> Z</c><00:07:46.900><c> since</c><00:07:47.830><c> we're</c><00:07:48.009><c> this</c>

00:07:48.390 --> 00:07:48.400 align:start position:0%
Z 1 District Capital Z since we're this
 

00:07:48.400 --> 00:07:49.980 align:start position:0%
Z 1 District Capital Z since we're this
is<00:07:48.460><c> actually</c><00:07:48.819><c> a</c><00:07:48.849><c> vectorized</c><00:07:49.180><c> implementation</c>

00:07:49.980 --> 00:07:49.990 align:start position:0%
is actually a vectorized implementation
 

00:07:49.990 --> 00:07:57.929 align:start position:0%
is actually a vectorized implementation
and<00:07:50.849><c> so</c><00:07:51.849><c> on</c><00:07:52.060><c> until</c><00:07:52.569><c> you</c><00:07:52.659><c> end</c><00:07:52.840><c> up</c><00:07:52.990><c> with</c><00:07:53.050><c> a</c><00:07:55.560><c> l</c><00:07:56.939><c> you</c>

00:07:57.929 --> 00:07:57.939 align:start position:0%
and so on until you end up with a l you
 

00:07:57.939 --> 00:08:02.459 align:start position:0%
and so on until you end up with a l you
know<00:07:58.060><c> as</c><00:07:58.240><c> I</c><00:07:58.419><c> guess</c><00:07:58.599><c> GL</c><00:07:59.169><c> of</c><00:08:00.360><c> VL</c><00:08:01.360><c> and</c><00:08:01.689><c> then</c><00:08:02.349><c> this</c>

00:08:02.459 --> 00:08:02.469 align:start position:0%
know as I guess GL of VL and then this
 

00:08:02.469 --> 00:08:04.619 align:start position:0%
know as I guess GL of VL and then this
is<00:08:02.650><c> your</c><00:08:02.770><c> prediction</c><00:08:03.129><c> and</c><00:08:04.090><c> you</c><00:08:04.210><c> notice</c><00:08:04.599><c> that</c>

00:08:04.619 --> 00:08:04.629 align:start position:0%
is your prediction and you notice that
 

00:08:04.629 --> 00:08:06.619 align:start position:0%
is your prediction and you notice that
here<00:08:05.169><c> you</c><00:08:05.379><c> should</c><00:08:05.680><c> use</c><00:08:05.979><c> a</c><00:08:06.009><c> vectorized</c>

00:08:06.619 --> 00:08:06.629 align:start position:0%
here you should use a vectorized
 

00:08:06.629 --> 00:08:11.339 align:start position:0%
here you should use a vectorized
implementation<00:08:09.270><c> it's</c><00:08:10.270><c> just</c><00:08:10.569><c> that</c><00:08:10.779><c> this</c>

00:08:11.339 --> 00:08:11.349 align:start position:0%
implementation it's just that this
 

00:08:11.349 --> 00:08:13.860 align:start position:0%
implementation it's just that this
vectorized<00:08:11.889><c> implementation</c><00:08:12.870><c> processes</c>

00:08:13.860 --> 00:08:13.870 align:start position:0%
vectorized implementation processes
 

00:08:13.870 --> 00:08:17.279 align:start position:0%
vectorized implementation processes
1,000<00:08:14.860><c> examples</c><00:08:15.430><c> at</c><00:08:15.699><c> a</c><00:08:15.729><c> time</c><00:08:16.000><c> rather</c><00:08:16.330><c> than</c><00:08:16.509><c> 5</c>

00:08:17.279 --> 00:08:17.289 align:start position:0%
1,000 examples at a time rather than 5
 

00:08:17.289 --> 00:08:20.189 align:start position:0%
1,000 examples at a time rather than 5
million<00:08:17.710><c> examples</c><00:08:18.150><c> mixed</c><00:08:19.150><c> you</c><00:08:19.300><c> compute</c><00:08:19.840><c> the</c>

00:08:20.189 --> 00:08:20.199 align:start position:0%
million examples mixed you compute the
 

00:08:20.199 --> 00:08:25.679 align:start position:0%
million examples mixed you compute the
cost<00:08:21.069><c> function</c><00:08:21.250><c> J</c><00:08:23.370><c> which</c><00:08:24.370><c> I'm</c><00:08:25.089><c> going</c><00:08:25.419><c> to</c><00:08:25.479><c> write</c>

00:08:25.679 --> 00:08:25.689 align:start position:0%
cost function J which I'm going to write
 

00:08:25.689 --> 00:08:30.540 align:start position:0%
cost function J which I'm going to write
as<00:08:26.069><c> 1</c><00:08:27.069><c> over</c><00:08:27.779><c> 1000</c><00:08:28.779><c> since</c><00:08:29.199><c> 301</c><00:08:29.949><c> thousands</c><00:08:30.460><c> the</c>

00:08:30.540 --> 00:08:30.550 align:start position:0%
as 1 over 1000 since 301 thousands the
 

00:08:30.550 --> 00:08:33.149 align:start position:0%
as 1 over 1000 since 301 thousands the
size<00:08:30.789><c> of</c><00:08:31.000><c> your</c><00:08:31.150><c> little</c><00:08:31.509><c> training</c><00:08:31.779><c> set</c><00:08:32.159><c> sum</c>

00:08:33.149 --> 00:08:33.159 align:start position:0%
size of your little training set sum
 

00:08:33.159 --> 00:08:36.269 align:start position:0%
size of your little training set sum
from<00:08:33.339><c> I</c><00:08:33.519><c> equals</c><00:08:33.849><c> 1</c><00:08:33.909><c> through</c><00:08:34.089><c> L</c><00:08:34.510><c> of</c><00:08:35.250><c> really</c><00:08:36.250><c> the</c>

00:08:36.269 --> 00:08:36.279 align:start position:0%
from I equals 1 through L of really the
 

00:08:36.279 --> 00:08:43.829 align:start position:0%
from I equals 1 through L of really the
you<00:08:36.880><c> know</c><00:08:37.029><c> loss</c><00:08:37.750><c> of</c><00:08:38.169><c> Y</c><00:08:38.890><c> hat</c><00:08:39.159><c> I</c><00:08:39.750><c> Y</c><00:08:40.750><c> I</c><00:08:42.599><c> and</c><00:08:43.599><c> this</c>

00:08:43.829 --> 00:08:43.839 align:start position:0%
you know loss of Y hat I Y I and this
 

00:08:43.839 --> 00:08:46.650 align:start position:0%
you know loss of Y hat I Y I and this
notation<00:08:44.589><c> for</c><00:08:44.980><c> clarity</c><00:08:45.399><c> refers</c><00:08:45.820><c> to</c><00:08:45.850><c> examples</c>

00:08:46.650 --> 00:08:46.660 align:start position:0%
notation for clarity refers to examples
 

00:08:46.660 --> 00:08:54.060 align:start position:0%
notation for clarity refers to examples
from<00:08:47.560><c> the</c><00:08:47.860><c> mini-batch</c><00:08:48.480><c> XT</c><00:08:50.399><c> YT</c><00:08:52.800><c> and</c><00:08:53.800><c> then</c><00:08:53.920><c> if</c>

00:08:54.060 --> 00:08:54.070 align:start position:0%
from the mini-batch XT YT and then if
 

00:08:54.070 --> 00:08:55.710 align:start position:0%
from the mini-batch XT YT and then if
you're<00:08:54.190><c> using</c><00:08:54.399><c> regularization</c><00:08:54.850><c> you</c><00:08:55.449><c> can</c><00:08:55.570><c> also</c>

00:08:55.710 --> 00:08:55.720 align:start position:0%
you're using regularization you can also
 

00:08:55.720 --> 00:08:59.880 align:start position:0%
you're using regularization you can also
have<00:08:56.170><c> this</c><00:08:56.699><c> regularization</c><00:08:57.699><c> term</c><00:08:58.680><c> just</c><00:08:59.680><c> move</c>

00:08:59.880 --> 00:08:59.890 align:start position:0%
have this regularization term just move
 

00:08:59.890 --> 00:09:05.240 align:start position:0%
have this regularization term just move
over<00:08:59.920><c> to</c><00:09:00.310><c> the</c><00:09:00.519><c> denominator</c><00:09:00.959><c> time</c><00:09:01.959><c> sum</c><00:09:02.290><c> over</c><00:09:02.529><c> L</c>

00:09:05.240 --> 00:09:05.250 align:start position:0%
 
 

00:09:05.250 --> 00:09:07.139 align:start position:0%
 
Frobenius<00:09:06.250><c> norm</c><00:09:06.279><c> the</c><00:09:06.610><c> way</c><00:09:06.730><c> measures</c><00:09:07.120><c> a</c>

00:09:07.139 --> 00:09:07.149 align:start position:0%
Frobenius norm the way measures a
 

00:09:07.149 --> 00:09:09.269 align:start position:0%
Frobenius norm the way measures a
squared<00:09:07.570><c> so</c><00:09:08.290><c> because</c><00:09:08.649><c> this</c><00:09:08.860><c> is</c><00:09:09.010><c> really</c><00:09:09.190><c> the</c>

00:09:09.269 --> 00:09:09.279 align:start position:0%
squared so because this is really the
 

00:09:09.279 --> 00:09:13.259 align:start position:0%
squared so because this is really the
cost<00:09:09.810><c> on</c><00:09:11.040><c> just</c><00:09:12.040><c> one</c><00:09:12.279><c> rainy</c><00:09:12.519><c> batch</c><00:09:12.790><c> and</c><00:09:13.120><c> then</c><00:09:13.180><c> I</c>

00:09:13.259 --> 00:09:13.269 align:start position:0%
cost on just one rainy batch and then I
 

00:09:13.269 --> 00:09:17.040 align:start position:0%
cost on just one rainy batch and then I
index<00:09:13.750><c> this</c><00:09:13.959><c> cost</c><00:09:14.260><c> J</c><00:09:15.160><c> with</c><00:09:15.550><c> a</c><00:09:15.579><c> superscript</c><00:09:16.209><c> T</c>

00:09:17.040 --> 00:09:17.050 align:start position:0%
index this cost J with a superscript T
 

00:09:17.050 --> 00:09:19.740 align:start position:0%
index this cost J with a superscript T
in<00:09:17.350><c> curly</c><00:09:17.890><c> braces</c><00:09:18.100><c> so</c><00:09:19.089><c> you</c><00:09:19.149><c> notice</c><00:09:19.600><c> that</c>

00:09:19.740 --> 00:09:19.750 align:start position:0%
in curly braces so you notice that
 

00:09:19.750 --> 00:09:22.500 align:start position:0%
in curly braces so you notice that
everything<00:09:20.620><c> we're</c><00:09:20.860><c> doing</c><00:09:21.040><c> is</c><00:09:21.399><c> exactly</c><00:09:22.180><c> the</c>

00:09:22.500 --> 00:09:22.510 align:start position:0%
everything we're doing is exactly the
 

00:09:22.510 --> 00:09:24.630 align:start position:0%
everything we're doing is exactly the
same<00:09:22.750><c> as</c><00:09:22.990><c> when</c><00:09:23.949><c> we</c><00:09:24.160><c> were</c><00:09:24.310><c> previously</c>

00:09:24.630 --> 00:09:24.640 align:start position:0%
same as when we were previously
 

00:09:24.640 --> 00:09:26.759 align:start position:0%
same as when we were previously
implementing<00:09:25.449><c> gradient</c><00:09:25.839><c> descent</c><00:09:26.170><c> except</c>

00:09:26.759 --> 00:09:26.769 align:start position:0%
implementing gradient descent except
 

00:09:26.769 --> 00:09:29.610 align:start position:0%
implementing gradient descent except
that<00:09:26.920><c> instead</c><00:09:27.100><c> of</c><00:09:27.430><c> doing</c><00:09:27.760><c> it</c><00:09:27.940><c> on</c><00:09:28.180><c> X</c><00:09:28.779><c> Y</c><00:09:29.170><c> you're</c>

00:09:29.610 --> 00:09:29.620 align:start position:0%
that instead of doing it on X Y you're
 

00:09:29.620 --> 00:09:32.759 align:start position:0%
that instead of doing it on X Y you're
not<00:09:29.769><c> doing</c><00:09:30.040><c> it</c><00:09:30.160><c> on</c><00:09:30.250><c> X</c><00:09:30.550><c> T</c><00:09:31.029><c> YT</c><00:09:31.560><c> next</c><00:09:32.560><c> you'd</c>

00:09:32.759 --> 00:09:32.769 align:start position:0%
not doing it on X T YT next you'd
 

00:09:32.769 --> 00:09:38.020 align:start position:0%
not doing it on X T YT next you'd
implement<00:09:33.870><c> back</c><00:09:34.870><c> prop</c><00:09:35.430><c> to</c><00:09:36.430><c> compute</c>

00:09:38.020 --> 00:09:38.030 align:start position:0%
implement back prop to compute
 

00:09:38.030 --> 00:09:42.830 align:start position:0%
implement back prop to compute
gradients<00:09:40.100><c> with</c><00:09:41.100><c> respect</c><00:09:41.550><c> to</c><00:09:41.760><c> really</c><00:09:42.450><c> respect</c>

00:09:42.830 --> 00:09:42.840 align:start position:0%
gradients with respect to really respect
 

00:09:42.840 --> 00:09:46.880 align:start position:0%
gradients with respect to really respect
to<00:09:43.020><c> this</c><00:09:43.140><c> JT</c><00:09:43.640><c> so</c><00:09:44.640><c> you're</c><00:09:44.940><c> still</c><00:09:45.210><c> using</c><00:09:45.420><c> only</c><00:09:45.890><c> X</c>

00:09:46.880 --> 00:09:46.890 align:start position:0%
to this JT so you're still using only X
 

00:09:46.890 --> 00:09:54.170 align:start position:0%
to this JT so you're still using only X
T<00:09:47.900><c> YT</c><00:09:49.790><c> and</c><00:09:51.650><c> then</c><00:09:52.650><c> you</c><00:09:52.800><c> update</c><00:09:53.190><c> the</c><00:09:53.310><c> weights</c><00:09:53.490><c> you</c>

00:09:54.170 --> 00:09:54.180 align:start position:0%
T YT and then you update the weights you
 

00:09:54.180 --> 00:09:59.650 align:start position:0%
T YT and then you update the weights you
know<00:09:54.210><c> wre</c><00:09:55.020><c> every</c><00:09:55.410><c> WL</c><00:09:55.920><c> gets</c><00:09:56.700><c> updated</c><00:09:57.300><c> as</c><00:09:58.520><c> WL</c>

00:09:59.650 --> 00:09:59.660 align:start position:0%
know wre every WL gets updated as WL
 

00:09:59.660 --> 00:10:11.620 align:start position:0%
know wre every WL gets updated as WL
minus<00:10:00.660><c> alpha</c><00:10:01.080><c> D</c><00:10:01.670><c> WL</c><00:10:02.670><c> and</c><00:10:03.440><c> similarly</c><00:10:04.440><c> for</c><00:10:04.650><c> B</c><00:10:09.590><c> and</c>

00:10:11.620 --> 00:10:11.630 align:start position:0%
minus alpha D WL and similarly for B and
 

00:10:11.630 --> 00:10:15.290 align:start position:0%
minus alpha D WL and similarly for B and
so<00:10:12.630><c> this</c><00:10:12.930><c> is</c><00:10:13.920><c> one</c><00:10:14.310><c> pass</c><00:10:14.700><c> through</c><00:10:14.730><c> your</c>

00:10:15.290 --> 00:10:15.300 align:start position:0%
so this is one pass through your
 

00:10:15.300 --> 00:10:17.840 align:start position:0%
so this is one pass through your
training<00:10:15.720><c> set</c><00:10:16.400><c> using</c><00:10:17.400><c> mini-batch</c><00:10:17.820><c> gradient</c>

00:10:17.840 --> 00:10:17.850 align:start position:0%
training set using mini-batch gradient
 

00:10:17.850 --> 00:10:20.360 align:start position:0%
training set using mini-batch gradient
descent<00:10:18.570><c> the</c><00:10:19.260><c> code</c><00:10:19.500><c> i've</c><00:10:19.650><c> written</c><00:10:19.830><c> down</c><00:10:19.980><c> here</c>

00:10:20.360 --> 00:10:20.370 align:start position:0%
descent the code i've written down here
 

00:10:20.370 --> 00:10:23.770 align:start position:0%
descent the code i've written down here
is<00:10:20.610><c> also</c><00:10:20.820><c> called</c><00:10:21.330><c> doing</c><00:10:22.290><c> one</c><00:10:22.530><c> epoch</c><00:10:23.430><c> of</c>

00:10:23.770 --> 00:10:23.780 align:start position:0%
is also called doing one epoch of
 

00:10:23.780 --> 00:10:27.650 align:start position:0%
is also called doing one epoch of
training<00:10:24.780><c> and</c><00:10:24.990><c> epoch</c><00:10:25.830><c> is</c><00:10:26.160><c> a</c><00:10:26.460><c> word</c><00:10:27.360><c> that</c><00:10:27.390><c> just</c>

00:10:27.650 --> 00:10:27.660 align:start position:0%
training and epoch is a word that just
 

00:10:27.660 --> 00:10:31.430 align:start position:0%
training and epoch is a word that just
means<00:10:27.900><c> a</c><00:10:28.350><c> single</c><00:10:29.160><c> pass</c><00:10:29.900><c> through</c><00:10:30.900><c> the</c><00:10:31.050><c> training</c>

00:10:31.430 --> 00:10:31.440 align:start position:0%
means a single pass through the training
 

00:10:31.440 --> 00:10:38.570 align:start position:0%
means a single pass through the training
set<00:10:36.110><c> so</c><00:10:37.110><c> whereas</c><00:10:37.710><c> with</c><00:10:37.980><c> batch</c><00:10:38.250><c> gradient</c>

00:10:38.570 --> 00:10:38.580 align:start position:0%
set so whereas with batch gradient
 

00:10:38.580 --> 00:10:41.030 align:start position:0%
set so whereas with batch gradient
descent<00:10:38.820><c> a</c><00:10:39.300><c> single</c><00:10:40.290><c> pass</c><00:10:40.560><c> through</c><00:10:40.950><c> the</c>

00:10:41.030 --> 00:10:41.040 align:start position:0%
descent a single pass through the
 

00:10:41.040 --> 00:10:43.040 align:start position:0%
descent a single pass through the
training<00:10:41.340><c> set</c><00:10:41.550><c> allows</c><00:10:41.850><c> you</c><00:10:42.090><c> to</c><00:10:42.270><c> take</c><00:10:42.480><c> only</c><00:10:42.720><c> one</c>

00:10:43.040 --> 00:10:43.050 align:start position:0%
training set allows you to take only one
 

00:10:43.050 --> 00:10:45.320 align:start position:0%
training set allows you to take only one
gradient<00:10:43.500><c> descent</c><00:10:43.800><c> step</c><00:10:44.070><c> with</c><00:10:44.790><c> really</c><00:10:45.180><c> batch</c>

00:10:45.320 --> 00:10:45.330 align:start position:0%
gradient descent step with really batch
 

00:10:45.330 --> 00:10:47.360 align:start position:0%
gradient descent step with really batch
gradient<00:10:45.540><c> descent</c><00:10:45.780><c> a</c><00:10:46.290><c> single</c><00:10:46.890><c> pass</c><00:10:47.100><c> through</c>

00:10:47.360 --> 00:10:47.370 align:start position:0%
gradient descent a single pass through
 

00:10:47.370 --> 00:10:49.100 align:start position:0%
gradient descent a single pass through
the<00:10:47.490><c> training</c><00:10:47.790><c> set</c><00:10:48.000><c> that</c><00:10:48.030><c> is</c><00:10:48.420><c> one</c><00:10:48.660><c> epoch</c>

00:10:49.100 --> 00:10:49.110 align:start position:0%
the training set that is one epoch
 

00:10:49.110 --> 00:10:52.160 align:start position:0%
the training set that is one epoch
allows<00:10:49.800><c> you</c><00:10:50.010><c> to</c><00:10:50.250><c> take</c><00:10:50.490><c> 5000</c><00:10:51.240><c> gradient</c><00:10:51.870><c> descent</c>

00:10:52.160 --> 00:10:52.170 align:start position:0%
allows you to take 5000 gradient descent
 

00:10:52.170 --> 00:10:54.950 align:start position:0%
allows you to take 5000 gradient descent
steps<00:10:52.410><c> now</c><00:10:53.070><c> of</c><00:10:53.190><c> course</c><00:10:53.370><c> you</c><00:10:53.550><c> want</c><00:10:53.820><c> to</c><00:10:54.000><c> take</c>

00:10:54.950 --> 00:10:54.960 align:start position:0%
steps now of course you want to take
 

00:10:54.960 --> 00:10:56.540 align:start position:0%
steps now of course you want to take
multiple<00:10:55.440><c> passes</c><00:10:55.920><c> through</c><00:10:56.130><c> the</c><00:10:56.250><c> training</c>

00:10:56.540 --> 00:10:56.550 align:start position:0%
multiple passes through the training
 

00:10:56.550 --> 00:10:58.850 align:start position:0%
multiple passes through the training
sets<00:10:56.790><c> which</c><00:10:56.970><c> you</c><00:10:57.300><c> usually</c><00:10:57.540><c> want</c><00:10:57.960><c> to</c><00:10:58.050><c> you</c><00:10:58.710><c> might</c>

00:10:58.850 --> 00:10:58.860 align:start position:0%
sets which you usually want to you might
 

00:10:58.860 --> 00:11:01.220 align:start position:0%
sets which you usually want to you might
want<00:10:59.130><c> another</c><00:10:59.250><c> for</c><00:10:59.880><c> loop</c><00:11:00.090><c> or</c><00:11:00.270><c> another</c><00:11:00.540><c> your</c>

00:11:01.220 --> 00:11:01.230 align:start position:0%
want another for loop or another your
 

00:11:01.230 --> 00:11:03.440 align:start position:0%
want another for loop or another your
while<00:11:01.530><c> loop</c><00:11:01.770><c> out</c><00:11:02.010><c> there</c><00:11:02.400><c> so</c><00:11:02.940><c> you</c><00:11:03.030><c> keep</c><00:11:03.270><c> taking</c>

00:11:03.440 --> 00:11:03.450 align:start position:0%
while loop out there so you keep taking
 

00:11:03.450 --> 00:11:05.540 align:start position:0%
while loop out there so you keep taking
process<00:11:03.840><c> through</c><00:11:04.290><c> the</c><00:11:04.410><c> training</c><00:11:04.770><c> set</c><00:11:04.980><c> until</c>

00:11:05.540 --> 00:11:05.550 align:start position:0%
process through the training set until
 

00:11:05.550 --> 00:11:07.520 align:start position:0%
process through the training set until
hopefully<00:11:06.300><c> you</c><00:11:06.780><c> converge</c><00:11:07.200><c> or</c><00:11:07.440><c> it</c>

00:11:07.520 --> 00:11:07.530 align:start position:0%
hopefully you converge or it
 

00:11:07.530 --> 00:11:09.470 align:start position:0%
hopefully you converge or it
approximately<00:11:07.980><c> converged</c><00:11:08.520><c> when</c><00:11:09.210><c> you</c><00:11:09.300><c> have</c><00:11:09.450><c> a</c>

00:11:09.470 --> 00:11:09.480 align:start position:0%
approximately converged when you have a
 

00:11:09.480 --> 00:11:11.480 align:start position:0%
approximately converged when you have a
lost<00:11:09.750><c> training</c><00:11:10.110><c> set</c><00:11:10.350><c> meaning</c><00:11:10.920><c> batch</c><00:11:11.190><c> gradient</c>

00:11:11.480 --> 00:11:11.490 align:start position:0%
lost training set meaning batch gradient
 

00:11:11.490 --> 00:11:14.000 align:start position:0%
lost training set meaning batch gradient
descent<00:11:11.730><c> runs</c><00:11:12.450><c> much</c><00:11:12.870><c> faster</c><00:11:12.900><c> than</c><00:11:13.740><c> batch</c>

00:11:14.000 --> 00:11:14.010 align:start position:0%
descent runs much faster than batch
 

00:11:14.010 --> 00:11:15.950 align:start position:0%
descent runs much faster than batch
gradient<00:11:14.250><c> descent</c><00:11:14.490><c> and</c><00:11:15.030><c> it's</c><00:11:15.480><c> pretty</c><00:11:15.810><c> much</c>

00:11:15.950 --> 00:11:15.960 align:start position:0%
gradient descent and it's pretty much
 

00:11:15.960 --> 00:11:17.900 align:start position:0%
gradient descent and it's pretty much
what<00:11:16.290><c> everyone</c><00:11:16.620><c> in</c><00:11:17.040><c> deep</c><00:11:17.220><c> learning</c><00:11:17.370><c> will</c><00:11:17.670><c> use</c>

00:11:17.900 --> 00:11:17.910 align:start position:0%
what everyone in deep learning will use
 

00:11:17.910 --> 00:11:19.700 align:start position:0%
what everyone in deep learning will use
when<00:11:18.420><c> you're</c><00:11:18.570><c> training</c><00:11:18.780><c> on</c><00:11:19.020><c> a</c><00:11:19.050><c> large</c><00:11:19.320><c> dataset</c>

00:11:19.700 --> 00:11:19.710 align:start position:0%
when you're training on a large dataset
 

00:11:19.710 --> 00:11:22.100 align:start position:0%
when you're training on a large dataset
in<00:11:20.400><c> the</c><00:11:20.580><c> next</c><00:11:20.850><c> video</c><00:11:21.090><c> let's</c><00:11:21.420><c> delve</c><00:11:21.690><c> deeper</c>

00:11:22.100 --> 00:11:22.110 align:start position:0%
in the next video let's delve deeper
 

00:11:22.110 --> 00:11:24.440 align:start position:0%
in the next video let's delve deeper
into<00:11:22.140><c> mini</c><00:11:22.740><c> batch</c><00:11:22.950><c> goodness</c><00:11:23.400><c> and</c><00:11:23.610><c> so</c><00:11:24.090><c> you</c><00:11:24.150><c> can</c>

00:11:24.440 --> 00:11:24.450 align:start position:0%
into mini batch goodness and so you can
 

00:11:24.450 --> 00:11:26.360 align:start position:0%
into mini batch goodness and so you can
get<00:11:24.570><c> a</c><00:11:24.600><c> better</c><00:11:24.780><c> understanding</c><00:11:24.900><c> of</c><00:11:25.710><c> what</c><00:11:26.250><c> is</c>

00:11:26.360 --> 00:11:26.370 align:start position:0%
get a better understanding of what is
 

00:11:26.370 --> 00:11:30.050 align:start position:0%
get a better understanding of what is
doing<00:11:26.700><c> and</c><00:11:26.940><c> why</c><00:11:27.210><c> it</c><00:11:27.270><c> works</c><00:11:27.570><c> so</c><00:11:27.840><c> well</c>

