WEBVTT
Kind: captions
Language: en

00:00:00.512 --> 00:00:01.980
All right so that's supervised learning,
Michael, and

00:00:01.980 --> 00:00:03.860
there's a whole bunch of
stuff that we covered.

00:00:03.860 --> 00:00:05.400
What about unsupervised learning?

00:00:05.400 --> 00:00:08.010
Is there anything there in that section

00:00:08.010 --> 00:00:10.070
that you wish we had been
able to talk about, too?

00:00:10.070 --> 00:00:11.820
&gt;&gt; It certainly seems
like there ought to be,

00:00:11.820 --> 00:00:14.320
in the sense that it was a lot
shorter than the other sections.

00:00:14.320 --> 00:00:16.370
&gt;&gt; A lot shorter than
the other sections.

00:00:16.370 --> 00:00:20.035
&gt;&gt; Yeah, one thing that I thought would
be worth bringing up in this context is

00:00:20.035 --> 00:00:21.750
tf-idf.

00:00:21.750 --> 00:00:24.950
So tf-idf is something that
I think is in one version

00:00:24.950 --> 00:00:26.840
of this class that you teach.

00:00:26.840 --> 00:00:28.540
But we didn't get a chance
to talk about it in here.

00:00:28.540 --> 00:00:31.040
And so, what does it have to do
with unsupervised learning, or

00:00:31.040 --> 00:00:33.050
randomized optimization?

00:00:33.050 --> 00:00:33.790
&gt;&gt; I don't know, Michael.

00:00:33.790 --> 00:00:35.890
What does it have to do with it?

00:00:35.890 --> 00:00:37.580
&gt;&gt; All right, so tf-idf, it stands for

00:00:37.580 --> 00:00:40.300
term frequency inverse
document frequency.

00:00:40.300 --> 00:00:43.300
And it is a way of applying
weight when youâ€™re doing,

00:00:44.470 --> 00:00:48.580
kind of a nearest neighbor
operation in textual data.

00:00:48.580 --> 00:00:52.090
So, if you have a big collection of
text like, web pages and queries,

00:00:52.090 --> 00:00:55.898
and you're making queries against
the webpages, you might want to know how

00:00:55.898 --> 00:00:59.171
close is this query is to all
the various documents that are in your

00:00:59.171 --> 00:01:02.586
collection, all of the various
webpages you might want to return.

00:01:02.586 --> 00:01:07.463
And it turns out that there's lots of
different similarity measures that you

00:01:07.463 --> 00:01:08.070
can use.

00:01:08.070 --> 00:01:11.680
You can use Euclidian distance, you can
use some kind of dot product-y thing.

00:01:11.680 --> 00:01:14.030
&gt;&gt; You can use the cosine of
the angle between documents.

00:01:14.030 --> 00:01:14.600
&gt;&gt; Yeah there we go.

00:01:14.600 --> 00:01:17.400
As a way of measuring how similar
this query is to a document so

00:01:17.400 --> 00:01:19.300
we can return the most similar ones.

00:01:19.300 --> 00:01:23.349
But it turns out that there's better and
worse ways of doing this waiting.

00:01:23.349 --> 00:01:26.316
So it has become apparent
through the years,

00:01:26.316 --> 00:01:31.312
I think back into the 60's actually
with Jerry Salton, that a really good

00:01:31.312 --> 00:01:36.464
way of doing this that's quite simple
but incredibly powerful, is to say that

00:01:36.464 --> 00:01:41.304
the amount of weight that you put on
the appearance of a term in a document,

00:01:41.304 --> 00:01:46.100
should be proportional, or positively
related to the term frequency.

00:01:46.100 --> 00:01:47.630
The number of times that
word that term appears.

00:01:47.630 --> 00:01:51.240
So if I'm searching for things about
snow, and we have a webpage that just

00:01:51.240 --> 00:01:53.660
mentions snow in passing,
that's not as important.

00:01:53.660 --> 00:01:57.120
As if we have a document that just
mentions snow all over the place.

00:01:57.120 --> 00:01:59.800
That the importance of that word grows
with the number of times that it appears

00:01:59.800 --> 00:02:00.410
in the document.

00:02:00.410 --> 00:02:01.210
&gt;&gt; Well that makes sense, but

00:02:01.210 --> 00:02:05.010
then that would imply that probably
the most important word is the.

00:02:05.010 --> 00:02:05.600
&gt;&gt; Aye, yes.

00:02:05.600 --> 00:02:07.180
Well first of all, no.

00:02:07.180 --> 00:02:08.280
It's not really that important.

00:02:08.280 --> 00:02:11.008
At least as far as determining
whether some document is relevant.

00:02:11.008 --> 00:02:12.580
So, exactly so.

00:02:12.580 --> 00:02:14.430
So that's the tf part
is the term frequency.

00:02:14.430 --> 00:02:17.070
The idf part,
inverse document frequency, says,

00:02:17.070 --> 00:02:21.300
well how many documents in your entire
collection have that word in them?

00:02:21.300 --> 00:02:23.870
If it appears indiscriminately
across a large number

00:02:23.870 --> 00:02:25.550
of documents then we
want to down-weight it.

00:02:25.550 --> 00:02:26.130
&gt;&gt; Like the.

00:02:26.130 --> 00:02:28.680
&gt;&gt; Like the, so
the appears almost everywhere.

00:02:28.680 --> 00:02:31.790
And so the document frequency,
the number of documents it appears in,

00:02:31.790 --> 00:02:32.500
is huge.

00:02:32.500 --> 00:02:34.870
The inverse document frequency
is therefore very small,

00:02:34.870 --> 00:02:37.740
so that gets very little weight when
you're doing this kind of comparison.

00:02:37.740 --> 00:02:40.430
&gt;&gt; That makes sense,
heck did a whole thesis about this.

00:02:40.430 --> 00:02:41.126
&gt;&gt; Oh, so
maybe you've heard of this before.

00:02:41.126 --> 00:02:42.272
&gt;&gt; I have heard of this before, and

00:02:42.272 --> 00:02:44.830
it's actually one of these things
that are just completely accepted.

00:02:44.830 --> 00:02:48.780
And it makes sense in the unsupervised
learning case because you're effectively

00:02:48.780 --> 00:02:50.620
in the ad hoc retrieval task,
as they call it.

00:02:50.620 --> 00:02:53.810
We would probably call it the Google
task if it had been invented.

00:02:53.810 --> 00:02:54.810
&gt;&gt; You know what was funny?

00:02:54.810 --> 00:02:58.070
I worked in information retrieval
before Google, and what was funny is-

00:02:58.070 --> 00:02:59.010
&gt;&gt; They stole all your ideas?

00:02:59.010 --> 00:03:02.115
&gt;&gt; No, No, No, No, No, No, No, No, Yes.

00:03:02.115 --> 00:03:07.110
No it was hard sometimes
writing papers to make

00:03:07.110 --> 00:03:09.880
the case that it was important to be
able to do information retrieval.

00:03:09.880 --> 00:03:11.460
Like what are we retrieving on?

00:03:11.460 --> 00:03:13.320
There's only ten documents in the world.

00:03:13.320 --> 00:03:15.810
Like it was a pretty funny thing and
now its so

00:03:15.810 --> 00:03:17.560
obvious to everyone
that this is important.

00:03:17.560 --> 00:03:19.110
In fact, they don't even think
about it anymore because-

00:03:19.110 --> 00:03:20.190
&gt;&gt; It changed the world.

00:03:20.190 --> 00:03:20.750
&gt;&gt; It did.
Okay, so

00:03:20.750 --> 00:03:22.640
that was something that we
could have talked about,

00:03:22.640 --> 00:03:25.670
but mainly we didn't have time and
we were tired.

00:03:25.670 --> 00:03:28.680
Let's see, we've talked about semi
supervised learning already, and

00:03:28.680 --> 00:03:31.680
that is something that could
have made sense in the section.

00:03:31.680 --> 00:03:35.370
And I guess, but that's all sort of
the unsupervised learning space.

00:03:35.370 --> 00:03:38.347
Spectral clustering, where as different
kinds of clustering we could've talked

00:03:38.347 --> 00:03:40.630
about but, we didn't
&gt;&gt; I like spectral clustering

00:03:40.630 --> 00:03:43.367
&gt;&gt; Yeah, it's kind of a neat thing, and

00:03:43.367 --> 00:03:48.172
pretty good right, it does pretty
well in the real world, right?

00:03:48.172 --> 00:03:49.662
&gt;&gt; Yeah, I think so
&gt;&gt; Okay, so

00:03:49.662 --> 00:03:52.296
you should look that up
&gt;&gt; What's neat about it is it actually

00:03:52.296 --> 00:03:53.846
doesn't get stuck in local optima.

00:03:53.846 --> 00:03:57.543
So it's a whole series of methods that
are based more on linear algebra,

00:03:57.543 --> 00:04:00.453
where you can kind of invert
matrixes and get one answer,

00:04:00.453 --> 00:04:04.290
than it is on search, and EM, and
gradient decent, and things like that.

00:04:04.290 --> 00:04:06.865
&gt;&gt; Yes, there's a claim you could
make that a lot of machine learning,

00:04:06.865 --> 00:04:09.990
particularly the unsupervised learning
space, is effectively linear algebra.

00:04:09.990 --> 00:04:11.230
At bottom.

00:04:11.230 --> 00:04:14.340
&gt;&gt; Well, and spectral clustering,
they fess up to that.

00:04:14.340 --> 00:04:16.209
&gt;&gt; Right, anything else we,
we didn't say anything new.

00:04:16.209 --> 00:04:18.600
Is there anything in randomized
optimization that we didn't really have

00:04:18.600 --> 00:04:20.079
a chance for talking about-
&gt;&gt; I think it's worth at least

00:04:20.079 --> 00:04:21.540
mentioning cross entropy,

00:04:21.540 --> 00:04:23.720
which is a method that's
really simple to implement.

00:04:23.720 --> 00:04:25.080
It's incredibly effective.

00:04:25.080 --> 00:04:28.680
Students that I work with have
implemented it in a number of settings,

00:04:28.680 --> 00:04:30.440
and it just always does well.

00:04:30.440 --> 00:04:33.500
It does better than our
friends simulated and

00:04:33.500 --> 00:04:35.790
kneeling and hill climbing and
things like that.

00:04:35.790 --> 00:04:37.490
It does really well.

00:04:37.490 --> 00:04:39.895
Well, so here's the thing,
I think it's an awful lot like MIMIC.

00:04:39.895 --> 00:04:43.295
[CROSSTALK] It's a MIMIC mimic.

00:04:43.295 --> 00:04:43.934
Yeah, very nice.

00:04:43.934 --> 00:04:48.811
But it seems to me that the two
communities weren't really

00:04:48.811 --> 00:04:50.850
aware of each other.

00:04:50.850 --> 00:04:52.770
So maybe we need to put
these two things together.

00:04:52.770 --> 00:04:53.590
You can be MIMIC, and

00:04:53.590 --> 00:04:56.200
I can be cross-entropy and we can figure
out how they relate to each other.

00:04:56.200 --> 00:04:58.210
&gt;&gt; And
we can cross fertilize each other.

00:04:58.210 --> 00:04:59.435
I don't like the sound of that at all.

00:04:59.435 --> 00:05:01.964
&gt;&gt; [LAUGHTER] Okay, so on that note,
we'll leave unsupervised learning and

00:05:01.964 --> 00:05:02.910
randomized optimization.

00:05:02.910 --> 00:05:05.098
&gt;&gt; Yes, see and this is what
happens when we're not supervised.

00:05:05.098 --> 00:05:07.210
&gt;&gt; [LAUGHTER] Well done.

