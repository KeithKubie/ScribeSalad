WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:01.939
in the previous video you saw the

00:00:01.939 --> 00:00:01.949
in the previous video you saw the
 

00:00:01.949 --> 00:00:05.180
in the previous video you saw the
logistic regression model to train the

00:00:05.180 --> 00:00:05.190
logistic regression model to train the
 

00:00:05.190 --> 00:00:07.249
logistic regression model to train the
parameters W and B of your logistic

00:00:07.249 --> 00:00:07.259
parameters W and B of your logistic
 

00:00:07.259 --> 00:00:09.200
parameters W and B of your logistic
regression model you need to define a

00:00:09.200 --> 00:00:09.210
regression model you need to define a
 

00:00:09.210 --> 00:00:11.660
regression model you need to define a
cost function let's take a look at a

00:00:11.660 --> 00:00:11.670
cost function let's take a look at a
 

00:00:11.670 --> 00:00:13.249
cost function let's take a look at a
cost function you can use to Train

00:00:13.249 --> 00:00:13.259
cost function you can use to Train
 

00:00:13.259 --> 00:00:15.289
cost function you can use to Train
logistic regression to recap this is

00:00:15.289 --> 00:00:15.299
logistic regression to recap this is
 

00:00:15.299 --> 00:00:17.779
logistic regression to recap this is
what we had defined from the previous

00:00:17.779 --> 00:00:17.789
what we had defined from the previous
 

00:00:17.789 --> 00:00:20.570
what we had defined from the previous
slide so you output Y hat is sigmoid of

00:00:20.570 --> 00:00:20.580
slide so you output Y hat is sigmoid of
 

00:00:20.580 --> 00:00:23.300
slide so you output Y hat is sigmoid of
W transpose X plus B where sigmoid of z

00:00:23.300 --> 00:00:23.310
W transpose X plus B where sigmoid of z
 

00:00:23.310 --> 00:00:25.429
W transpose X plus B where sigmoid of z
is as defined here so to learn

00:00:25.429 --> 00:00:25.439
is as defined here so to learn
 

00:00:25.439 --> 00:00:27.650
is as defined here so to learn
parameters for your model you're given a

00:00:27.650 --> 00:00:27.660
parameters for your model you're given a
 

00:00:27.660 --> 00:00:30.470
parameters for your model you're given a
training set of M training examples and

00:00:30.470 --> 00:00:30.480
training set of M training examples and
 

00:00:30.480 --> 00:00:33.229
training set of M training examples and
it seems natural that you want to find

00:00:33.229 --> 00:00:33.239
it seems natural that you want to find
 

00:00:33.239 --> 00:00:35.600
it seems natural that you want to find
parameters W and B so that at least on

00:00:35.600 --> 00:00:35.610
parameters W and B so that at least on
 

00:00:35.610 --> 00:00:37.670
parameters W and B so that at least on
the training set the outputs you have

00:00:37.670 --> 00:00:37.680
the training set the outputs you have
 

00:00:37.680 --> 00:00:39.200
the training set the outputs you have
the predictions you have on the training

00:00:39.200 --> 00:00:39.210
the predictions you have on the training
 

00:00:39.210 --> 00:00:41.479
the predictions you have on the training
set we're going to write this Y hat I

00:00:41.479 --> 00:00:41.489
set we're going to write this Y hat I
 

00:00:41.489 --> 00:00:44.060
set we're going to write this Y hat I
that that would be close to the ground

00:00:44.060 --> 00:00:44.070
that that would be close to the ground
 

00:00:44.070 --> 00:00:46.670
that that would be close to the ground
truth labels Y either you got in the

00:00:46.670 --> 00:00:46.680
truth labels Y either you got in the
 

00:00:46.680 --> 00:00:49.700
truth labels Y either you got in the
training set so to fill in a little bit

00:00:49.700 --> 00:00:49.710
training set so to fill in a little bit
 

00:00:49.710 --> 00:00:52.459
training set so to fill in a little bit
more detail for the equation on top we

00:00:52.459 --> 00:00:52.469
more detail for the equation on top we
 

00:00:52.469 --> 00:00:55.189
more detail for the equation on top we
have said that Y has as defined at the

00:00:55.189 --> 00:00:55.199
have said that Y has as defined at the
 

00:00:55.199 --> 00:00:58.729
have said that Y has as defined at the
top for a training example X and of

00:00:58.729 --> 00:00:58.739
top for a training example X and of
 

00:00:58.739 --> 00:01:01.040
top for a training example X and of
course for each training example we're

00:01:01.040 --> 00:01:01.050
course for each training example we're
 

00:01:01.050 --> 00:01:03.740
course for each training example we're
using these super scripts with round

00:01:03.740 --> 00:01:03.750
using these super scripts with round
 

00:01:03.750 --> 00:01:05.810
using these super scripts with round
brackets with parentheses to index into

00:01:05.810 --> 00:01:05.820
brackets with parentheses to index into
 

00:01:05.820 --> 00:01:07.850
brackets with parentheses to index into
different training examples your

00:01:07.850 --> 00:01:07.860
different training examples your
 

00:01:07.860 --> 00:01:10.520
different training examples your
prediction on training example I which

00:01:10.520 --> 00:01:10.530
prediction on training example I which
 

00:01:10.530 --> 00:01:14.390
prediction on training example I which
is y hat I is going to be obtained by

00:01:14.390 --> 00:01:14.400
is y hat I is going to be obtained by
 

00:01:14.400 --> 00:01:16.609
is y hat I is going to be obtained by
taking the sigmoid function and applying

00:01:16.609 --> 00:01:16.619
taking the sigmoid function and applying
 

00:01:16.619 --> 00:01:19.789
taking the sigmoid function and applying
it to W transpose X I the input for the

00:01:19.789 --> 00:01:19.799
it to W transpose X I the input for the
 

00:01:19.799 --> 00:01:22.130
it to W transpose X I the input for the
training example plus B and you can also

00:01:22.130 --> 00:01:22.140
training example plus B and you can also
 

00:01:22.140 --> 00:01:26.899
training example plus B and you can also
define Z I as follows where Zi is equal

00:01:26.899 --> 00:01:26.909
define Z I as follows where Zi is equal
 

00:01:26.909 --> 00:01:30.289
define Z I as follows where Zi is equal
to you know W transpose X I plus B so

00:01:30.289 --> 00:01:30.299
to you know W transpose X I plus B so
 

00:01:30.299 --> 00:01:31.700
to you know W transpose X I plus B so
throughout this course we're going to

00:01:31.700 --> 00:01:31.710
throughout this course we're going to
 

00:01:31.710 --> 00:01:34.580
throughout this course we're going to
use this notational convention that the

00:01:34.580 --> 00:01:34.590
use this notational convention that the
 

00:01:34.590 --> 00:01:40.480
use this notational convention that the
superscript parentheses I refers to data

00:01:40.480 --> 00:01:40.490
superscript parentheses I refers to data
 

00:01:40.490 --> 00:01:43.580
superscript parentheses I refers to data
via X or Y or Z or something else

00:01:43.580 --> 00:01:43.590
via X or Y or Z or something else
 

00:01:43.590 --> 00:01:47.600
via X or Y or Z or something else
associated with the i'f training example

00:01:47.600 --> 00:01:47.610
associated with the i'f training example
 

00:01:47.610 --> 00:01:50.929
associated with the i'f training example
associated with the i'f example okay

00:01:50.929 --> 00:01:50.939
associated with the i'f example okay
 

00:01:50.939 --> 00:01:53.270
associated with the i'f example okay
that's what the superscript I in

00:01:53.270 --> 00:01:53.280
that's what the superscript I in
 

00:01:53.280 --> 00:01:55.850
that's what the superscript I in
parenthesis means now let's see what

00:01:55.850 --> 00:01:55.860
parenthesis means now let's see what
 

00:01:55.860 --> 00:01:58.340
parenthesis means now let's see what
loss function or an error function we

00:01:58.340 --> 00:01:58.350
loss function or an error function we
 

00:01:58.350 --> 00:02:00.230
loss function or an error function we
can use to measure how well our

00:02:00.230 --> 00:02:00.240
can use to measure how well our
 

00:02:00.240 --> 00:02:02.030
can use to measure how well our
algorithm is doing one thing you could

00:02:02.030 --> 00:02:02.040
algorithm is doing one thing you could
 

00:02:02.040 --> 00:02:04.819
algorithm is doing one thing you could
do is define the loss when your

00:02:04.819 --> 00:02:04.829
do is define the loss when your
 

00:02:04.829 --> 00:02:07.789
do is define the loss when your
algorithm outputs Y hat and the true

00:02:07.789 --> 00:02:07.799
algorithm outputs Y hat and the true
 

00:02:07.799 --> 00:02:10.279
algorithm outputs Y hat and the true
label is y to be maybe the squared error

00:02:10.279 --> 00:02:10.289
label is y to be maybe the squared error
 

00:02:10.289 --> 00:02:12.170
label is y to be maybe the squared error
or one hover square

00:02:12.170 --> 00:02:12.180
or one hover square
 

00:02:12.180 --> 00:02:15.140
or one hover square
it turns out that you could do this but

00:02:15.140 --> 00:02:15.150
it turns out that you could do this but
 

00:02:15.150 --> 00:02:17.000
it turns out that you could do this but
in logistic regression people don't

00:02:17.000 --> 00:02:17.010
in logistic regression people don't
 

00:02:17.010 --> 00:02:19.760
in logistic regression people don't
usually do this because when you come to

00:02:19.760 --> 00:02:19.770
usually do this because when you come to
 

00:02:19.770 --> 00:02:21.860
usually do this because when you come to
learn the parameters you find that the

00:02:21.860 --> 00:02:21.870
learn the parameters you find that the
 

00:02:21.870 --> 00:02:23.450
learn the parameters you find that the
optimization problem which we'll talk

00:02:23.450 --> 00:02:23.460
optimization problem which we'll talk
 

00:02:23.460 --> 00:02:25.910
optimization problem which we'll talk
about later becomes non convex so you

00:02:25.910 --> 00:02:25.920
about later becomes non convex so you
 

00:02:25.920 --> 00:02:28.190
about later becomes non convex so you
end up with ostentation problem it would

00:02:28.190 --> 00:02:28.200
end up with ostentation problem it would
 

00:02:28.200 --> 00:02:30.590
end up with ostentation problem it would
not talk local optima so gradient

00:02:30.590 --> 00:02:30.600
not talk local optima so gradient
 

00:02:30.600 --> 00:02:33.440
not talk local optima so gradient
descent may not find a global optimum if

00:02:33.440 --> 00:02:33.450
descent may not find a global optimum if
 

00:02:33.450 --> 00:02:35.090
descent may not find a global optimum if
you didn't understand the last couple of

00:02:35.090 --> 00:02:35.100
you didn't understand the last couple of
 

00:02:35.100 --> 00:02:36.650
you didn't understand the last couple of
comments don't worry about it we'll get

00:02:36.650 --> 00:02:36.660
comments don't worry about it we'll get
 

00:02:36.660 --> 00:02:38.600
comments don't worry about it we'll get
to it in the later video but the

00:02:38.600 --> 00:02:38.610
to it in the later video but the
 

00:02:38.610 --> 00:02:41.120
to it in the later video but the
intuition to take away is that this

00:02:41.120 --> 00:02:41.130
intuition to take away is that this
 

00:02:41.130 --> 00:02:43.850
intuition to take away is that this
function L called the loss function is a

00:02:43.850 --> 00:02:43.860
function L called the loss function is a
 

00:02:43.860 --> 00:02:45.890
function L called the loss function is a
function we need to define to measure

00:02:45.890 --> 00:02:45.900
function we need to define to measure
 

00:02:45.900 --> 00:02:49.490
function we need to define to measure
how good our output Y hat is when the

00:02:49.490 --> 00:02:49.500
how good our output Y hat is when the
 

00:02:49.500 --> 00:02:52.490
how good our output Y hat is when the
true label is y and squared error seems

00:02:52.490 --> 00:02:52.500
true label is y and squared error seems
 

00:02:52.500 --> 00:02:53.900
true label is y and squared error seems
like it might be a reasonable choice

00:02:53.900 --> 00:02:53.910
like it might be a reasonable choice
 

00:02:53.910 --> 00:02:56.360
like it might be a reasonable choice
except that it makes gradient descent

00:02:56.360 --> 00:02:56.370
except that it makes gradient descent
 

00:02:56.370 --> 00:02:59.330
except that it makes gradient descent
not work well so in logistic regression

00:02:59.330 --> 00:02:59.340
not work well so in logistic regression
 

00:02:59.340 --> 00:03:01.520
not work well so in logistic regression
we will actually define a different loss

00:03:01.520 --> 00:03:01.530
we will actually define a different loss
 

00:03:01.530 --> 00:03:04.130
we will actually define a different loss
function that plays a similar role as

00:03:04.130 --> 00:03:04.140
function that plays a similar role as
 

00:03:04.140 --> 00:03:06.920
function that plays a similar role as
squared error that will give us an

00:03:06.920 --> 00:03:06.930
squared error that will give us an
 

00:03:06.930 --> 00:03:09.620
squared error that will give us an
optimization problem that is convex and

00:03:09.620 --> 00:03:09.630
optimization problem that is convex and
 

00:03:09.630 --> 00:03:11.840
optimization problem that is convex and
so will see in a later video becomes

00:03:11.840 --> 00:03:11.850
so will see in a later video becomes
 

00:03:11.850 --> 00:03:15.140
so will see in a later video becomes
much easier to optimize so what we use

00:03:15.140 --> 00:03:15.150
much easier to optimize so what we use
 

00:03:15.150 --> 00:03:17.420
much easier to optimize so what we use
in logistic regression is actually the

00:03:17.420 --> 00:03:17.430
in logistic regression is actually the
 

00:03:17.430 --> 00:03:20.510
in logistic regression is actually the
following loss function which ones with

00:03:20.510 --> 00:03:20.520
following loss function which ones with
 

00:03:20.520 --> 00:03:27.250
following loss function which ones with
my rent out here is negative y log y hat

00:03:27.250 --> 00:03:27.260
my rent out here is negative y log y hat
 

00:03:27.260 --> 00:03:35.450
my rent out here is negative y log y hat
plus 1 minus y log 1 minus y hat here's

00:03:35.450 --> 00:03:35.460
plus 1 minus y log 1 minus y hat here's
 

00:03:35.460 --> 00:03:37.160
plus 1 minus y log 1 minus y hat here's
some intuition for why this loss

00:03:37.160 --> 00:03:37.170
some intuition for why this loss
 

00:03:37.170 --> 00:03:39.830
some intuition for why this loss
function exists keep in mind that if

00:03:39.830 --> 00:03:39.840
function exists keep in mind that if
 

00:03:39.840 --> 00:03:43.190
function exists keep in mind that if
we're using squared error then you want

00:03:43.190 --> 00:03:43.200
we're using squared error then you want
 

00:03:43.200 --> 00:03:44.990
we're using squared error then you want
this squared error to be as small as

00:03:44.990 --> 00:03:45.000
this squared error to be as small as
 

00:03:45.000 --> 00:03:47.060
this squared error to be as small as
possible and with this logistic

00:03:47.060 --> 00:03:47.070
possible and with this logistic
 

00:03:47.070 --> 00:03:49.490
possible and with this logistic
regression loss function we'll also want

00:03:49.490 --> 00:03:49.500
regression loss function we'll also want
 

00:03:49.500 --> 00:03:51.620
regression loss function we'll also want
this to be as small as possible to

00:03:51.620 --> 00:03:51.630
this to be as small as possible to
 

00:03:51.630 --> 00:03:53.720
this to be as small as possible to
understand why this makes sense let's

00:03:53.720 --> 00:03:53.730
understand why this makes sense let's
 

00:03:53.730 --> 00:03:56.270
understand why this makes sense let's
look at the two cases in the first case

00:03:56.270 --> 00:03:56.280
look at the two cases in the first case
 

00:03:56.280 --> 00:03:59.270
look at the two cases in the first case
let's say Y is equal to 1 then the loss

00:03:59.270 --> 00:03:59.280
let's say Y is equal to 1 then the loss
 

00:03:59.280 --> 00:04:03.560
let's say Y is equal to 1 then the loss
function Y has comma Y is just the

00:04:03.560 --> 00:04:03.570
function Y has comma Y is just the
 

00:04:03.570 --> 00:04:05.090
function Y has comma Y is just the
stress term right under the negative

00:04:05.090 --> 00:04:05.100
stress term right under the negative
 

00:04:05.100 --> 00:04:09.110
stress term right under the negative
sign so it's negative log Y hat if y is

00:04:09.110 --> 00:04:09.120
sign so it's negative log Y hat if y is
 

00:04:09.120 --> 00:04:11.210
sign so it's negative log Y hat if y is
equal to 1 because if y equals 1 then

00:04:11.210 --> 00:04:11.220
equal to 1 because if y equals 1 then
 

00:04:11.220 --> 00:04:13.850
equal to 1 because if y equals 1 then
the second term 1 minus y is equal to 0

00:04:13.850 --> 00:04:13.860
the second term 1 minus y is equal to 0
 

00:04:13.860 --> 00:04:16.550
the second term 1 minus y is equal to 0
so this says if y equals 1 you want

00:04:16.550 --> 00:04:16.560
so this says if y equals 1 you want
 

00:04:16.560 --> 00:04:19.130
so this says if y equals 1 you want
negative Y hat to be as big as possible

00:04:19.130 --> 00:04:19.140
negative Y hat to be as big as possible
 

00:04:19.140 --> 00:04:21.830
negative Y hat to be as big as possible
so that means you want

00:04:21.830 --> 00:04:21.840
so that means you want
 

00:04:21.840 --> 00:04:27.530
so that means you want
log y hat to be large to be as big as

00:04:27.530 --> 00:04:27.540
log y hat to be large to be as big as
 

00:04:27.540 --> 00:04:31.190
log y hat to be large to be as big as
possible and that means you want y hat

00:04:31.190 --> 00:04:31.200
possible and that means you want y hat
 

00:04:31.200 --> 00:04:35.000
possible and that means you want y hat
to be large but because y hat is you

00:04:35.000 --> 00:04:35.010
to be large but because y hat is you
 

00:04:35.010 --> 00:04:36.950
to be large but because y hat is you
know the sigmoid function it can never

00:04:36.950 --> 00:04:36.960
know the sigmoid function it can never
 

00:04:36.960 --> 00:04:39.409
know the sigmoid function it can never
be bigger than one so this is saying

00:04:39.409 --> 00:04:39.419
be bigger than one so this is saying
 

00:04:39.419 --> 00:04:42.620
be bigger than one so this is saying
that if Y is equal to one you once Y has

00:04:42.620 --> 00:04:42.630
that if Y is equal to one you once Y has
 

00:04:42.630 --> 00:04:44.480
that if Y is equal to one you once Y has
to be as big as possible but it can't

00:04:44.480 --> 00:04:44.490
to be as big as possible but it can't
 

00:04:44.490 --> 00:04:46.190
to be as big as possible but it can't
ever be bigger than one so saying you

00:04:46.190 --> 00:04:46.200
ever be bigger than one so saying you
 

00:04:46.200 --> 00:04:47.930
ever be bigger than one so saying you
want Y hat to be close to one as well

00:04:47.930 --> 00:04:47.940
want Y hat to be close to one as well
 

00:04:47.940 --> 00:04:51.170
want Y hat to be close to one as well
the other case is if y equals zero if y

00:04:51.170 --> 00:04:51.180
the other case is if y equals zero if y
 

00:04:51.180 --> 00:04:53.240
the other case is if y equals zero if y
equals zero then this first term in the

00:04:53.240 --> 00:04:53.250
equals zero then this first term in the
 

00:04:53.250 --> 00:04:55.550
equals zero then this first term in the
loss function is equal to zero because y

00:04:55.550 --> 00:04:55.560
loss function is equal to zero because y
 

00:04:55.560 --> 00:05:00.440
loss function is equal to zero because y
0 and in the second term defines the

00:05:00.440 --> 00:05:00.450
0 and in the second term defines the
 

00:05:00.450 --> 00:05:02.180
0 and in the second term defines the
loss function so the loss becomes

00:05:02.180 --> 00:05:02.190
loss function so the loss becomes
 

00:05:02.190 --> 00:05:08.210
loss function so the loss becomes
negative log 1 minus y hat and so if is

00:05:08.210 --> 00:05:08.220
negative log 1 minus y hat and so if is
 

00:05:08.220 --> 00:05:10.219
negative log 1 minus y hat and so if is
a learning procedure you try to make the

00:05:10.219 --> 00:05:10.229
a learning procedure you try to make the
 

00:05:10.229 --> 00:05:12.379
a learning procedure you try to make the
last function small what this means is

00:05:12.379 --> 00:05:12.389
last function small what this means is
 

00:05:12.389 --> 00:05:18.320
last function small what this means is
that you want log 1 minus y hat to be

00:05:18.320 --> 00:05:18.330
that you want log 1 minus y hat to be
 

00:05:18.330 --> 00:05:21.469
that you want log 1 minus y hat to be
large and because it's a negative sign

00:05:21.469 --> 00:05:21.479
large and because it's a negative sign
 

00:05:21.479 --> 00:05:23.210
large and because it's a negative sign
there and then through a similar piece

00:05:23.210 --> 00:05:23.220
there and then through a similar piece
 

00:05:23.220 --> 00:05:25.310
there and then through a similar piece
of reasoning you can conclude that this

00:05:25.310 --> 00:05:25.320
of reasoning you can conclude that this
 

00:05:25.320 --> 00:05:28.820
of reasoning you can conclude that this
loss function is trying to make y hat as

00:05:28.820 --> 00:05:28.830
loss function is trying to make y hat as
 

00:05:28.830 --> 00:05:31.880
loss function is trying to make y hat as
small as possible and again because Y

00:05:31.880 --> 00:05:31.890
small as possible and again because Y
 

00:05:31.890 --> 00:05:34.550
small as possible and again because Y
hat has to be between 0 &amp; 1 this is

00:05:34.550 --> 00:05:34.560
hat has to be between 0 &amp; 1 this is
 

00:05:34.560 --> 00:05:38.390
hat has to be between 0 &amp; 1 this is
saying that if Y is equal to 0 then your

00:05:38.390 --> 00:05:38.400
saying that if Y is equal to 0 then your
 

00:05:38.400 --> 00:05:40.190
saying that if Y is equal to 0 then your
loss function will push the parameters

00:05:40.190 --> 00:05:40.200
loss function will push the parameters
 

00:05:40.200 --> 00:05:42.650
loss function will push the parameters
so make Y hat as close to zero as

00:05:42.650 --> 00:05:42.660
so make Y hat as close to zero as
 

00:05:42.660 --> 00:05:44.570
so make Y hat as close to zero as
possible now there are a lot of

00:05:44.570 --> 00:05:44.580
possible now there are a lot of
 

00:05:44.580 --> 00:05:47.360
possible now there are a lot of
functions with roughly this effect that

00:05:47.360 --> 00:05:47.370
functions with roughly this effect that
 

00:05:47.370 --> 00:05:49.880
functions with roughly this effect that
if Y is equal to 1 you try to make Y hat

00:05:49.880 --> 00:05:49.890
if Y is equal to 1 you try to make Y hat
 

00:05:49.890 --> 00:05:51.830
if Y is equal to 1 you try to make Y hat
knowledge and Y is equal to 0 or try to

00:05:51.830 --> 00:05:51.840
knowledge and Y is equal to 0 or try to
 

00:05:51.840 --> 00:05:54.080
knowledge and Y is equal to 0 or try to
make Y hat small we just gave here in

00:05:54.080 --> 00:05:54.090
make Y hat small we just gave here in
 

00:05:54.090 --> 00:05:57.320
make Y hat small we just gave here in
green a somewhat informal justification

00:05:57.320 --> 00:05:57.330
green a somewhat informal justification
 

00:05:57.330 --> 00:05:59.420
green a somewhat informal justification
for this particular loss function will

00:05:59.420 --> 00:05:59.430
for this particular loss function will
 

00:05:59.430 --> 00:06:01.820
for this particular loss function will
provide an optional video later to give

00:06:01.820 --> 00:06:01.830
provide an optional video later to give
 

00:06:01.830 --> 00:06:04.460
provide an optional video later to give
a more formal justification for Y in

00:06:04.460 --> 00:06:04.470
a more formal justification for Y in
 

00:06:04.470 --> 00:06:06.320
a more formal justification for Y in
logistic regression we like to use the

00:06:06.320 --> 00:06:06.330
logistic regression we like to use the
 

00:06:06.330 --> 00:06:08.240
logistic regression we like to use the
loss function with this particular form

00:06:08.240 --> 00:06:08.250
loss function with this particular form
 

00:06:08.250 --> 00:06:10.940
loss function with this particular form
finally the loss function was defined

00:06:10.940 --> 00:06:10.950
finally the loss function was defined
 

00:06:10.950 --> 00:06:12.680
finally the loss function was defined
with respect to a single training

00:06:12.680 --> 00:06:12.690
with respect to a single training
 

00:06:12.690 --> 00:06:14.570
with respect to a single training
example it measures how well you're

00:06:14.570 --> 00:06:14.580
example it measures how well you're
 

00:06:14.580 --> 00:06:16.730
example it measures how well you're
doing on a single training example are

00:06:16.730 --> 00:06:16.740
doing on a single training example are
 

00:06:16.740 --> 00:06:18.830
doing on a single training example are
now going to define something called the

00:06:18.830 --> 00:06:18.840
now going to define something called the
 

00:06:18.840 --> 00:06:22.909
now going to define something called the
cost function which measures how are you

00:06:22.909 --> 00:06:22.919
cost function which measures how are you
 

00:06:22.919 --> 00:06:24.950
cost function which measures how are you
doing on the entire training set so the

00:06:24.950 --> 00:06:24.960
doing on the entire training set so the
 

00:06:24.960 --> 00:06:28.730
doing on the entire training set so the
cost function J which is applied to your

00:06:28.730 --> 00:06:28.740
cost function J which is applied to your
 

00:06:28.740 --> 00:06:31.820
cost function J which is applied to your
parameter W NP is going to be the

00:06:31.820 --> 00:06:31.830
parameter W NP is going to be the
 

00:06:31.830 --> 00:06:35.540
parameter W NP is going to be the
average ly 1 over m of the sum

00:06:35.540 --> 00:06:35.550
average ly 1 over m of the sum
 

00:06:35.550 --> 00:06:41.390
average ly 1 over m of the sum
of the last function applied to each of

00:06:41.390 --> 00:06:41.400
of the last function applied to each of
 

00:06:41.400 --> 00:06:44.059
of the last function applied to each of
the training examples in turn when here

00:06:44.059 --> 00:06:44.069
the training examples in turn when here
 

00:06:44.069 --> 00:06:46.520
the training examples in turn when here
Y has is of course the prediction output

00:06:46.520 --> 00:06:46.530
Y has is of course the prediction output
 

00:06:46.530 --> 00:06:48.100
Y has is of course the prediction output
by your logistic regression algorithm

00:06:48.100 --> 00:06:48.110
by your logistic regression algorithm
 

00:06:48.110 --> 00:06:50.779
by your logistic regression algorithm
using you know a particular set of

00:06:50.779 --> 00:06:50.789
using you know a particular set of
 

00:06:50.789 --> 00:06:53.990
using you know a particular set of
parameters W and B and so just to expand

00:06:53.990 --> 00:06:54.000
parameters W and B and so just to expand
 

00:06:54.000 --> 00:06:57.350
parameters W and B and so just to expand
this out this is equal to negative 1

00:06:57.350 --> 00:06:57.360
this out this is equal to negative 1
 

00:06:57.360 --> 00:07:00.830
this out this is equal to negative 1
over m sum from I equals 1 through m of

00:07:00.830 --> 00:07:00.840
over m sum from I equals 1 through m of
 

00:07:00.840 --> 00:07:02.960
over m sum from I equals 1 through m of
the definition of the last function

00:07:02.960 --> 00:07:02.970
the definition of the last function
 

00:07:02.970 --> 00:07:10.129
the definition of the last function
above so this is why I log Y hat I plus

00:07:10.129 --> 00:07:10.139
above so this is why I log Y hat I plus
 

00:07:10.139 --> 00:07:16.309
above so this is why I log Y hat I plus
1 minus y I log 1 minus y hat I because

00:07:16.309 --> 00:07:16.319
1 minus y I log 1 minus y hat I because
 

00:07:16.319 --> 00:07:18.110
1 minus y I log 1 minus y hat I because
it can put square brackets here so the

00:07:18.110 --> 00:07:18.120
it can put square brackets here so the
 

00:07:18.120 --> 00:07:20.659
it can put square brackets here so the
minus sign goes outside everything else

00:07:20.659 --> 00:07:20.669
minus sign goes outside everything else
 

00:07:20.669 --> 00:07:23.210
minus sign goes outside everything else
so the terminology I'm going to use is

00:07:23.210 --> 00:07:23.220
so the terminology I'm going to use is
 

00:07:23.220 --> 00:07:25.790
so the terminology I'm going to use is
that the loss function is applied to

00:07:25.790 --> 00:07:25.800
that the loss function is applied to
 

00:07:25.800 --> 00:07:28.730
that the loss function is applied to
just a single training example like so

00:07:28.730 --> 00:07:28.740
just a single training example like so
 

00:07:28.740 --> 00:07:31.790
just a single training example like so
and the cost function is the cost of

00:07:31.790 --> 00:07:31.800
and the cost function is the cost of
 

00:07:31.800 --> 00:07:34.610
and the cost function is the cost of
your parameters so in training your

00:07:34.610 --> 00:07:34.620
your parameters so in training your
 

00:07:34.620 --> 00:07:36.529
your parameters so in training your
logistic regression model we're going to

00:07:36.529 --> 00:07:36.539
logistic regression model we're going to
 

00:07:36.539 --> 00:07:38.899
logistic regression model we're going to
try to find parameters W and B that

00:07:38.899 --> 00:07:38.909
try to find parameters W and B that
 

00:07:38.909 --> 00:07:41.330
try to find parameters W and B that
minimize the overall cost function J

00:07:41.330 --> 00:07:41.340
minimize the overall cost function J
 

00:07:41.340 --> 00:07:44.420
minimize the overall cost function J
written at the bottom so you've just

00:07:44.420 --> 00:07:44.430
written at the bottom so you've just
 

00:07:44.430 --> 00:07:46.790
written at the bottom so you've just
seen the set up for the logistic

00:07:46.790 --> 00:07:46.800
seen the set up for the logistic
 

00:07:46.800 --> 00:07:48.920
seen the set up for the logistic
regression algorithm the loss function

00:07:48.920 --> 00:07:48.930
regression algorithm the loss function
 

00:07:48.930 --> 00:07:51.019
regression algorithm the loss function
for a training example and the overall

00:07:51.019 --> 00:07:51.029
for a training example and the overall
 

00:07:51.029 --> 00:07:52.850
for a training example and the overall
cost function for the parameters of your

00:07:52.850 --> 00:07:52.860
cost function for the parameters of your
 

00:07:52.860 --> 00:07:55.519
cost function for the parameters of your
algorithm it turns out that logistic

00:07:55.519 --> 00:07:55.529
algorithm it turns out that logistic
 

00:07:55.529 --> 00:07:57.589
algorithm it turns out that logistic
regression can be viewed as a very very

00:07:57.589 --> 00:07:57.599
regression can be viewed as a very very
 

00:07:57.599 --> 00:08:00.320
regression can be viewed as a very very
small neural network in the next video

00:08:00.320 --> 00:08:00.330
small neural network in the next video
 

00:08:00.330 --> 00:08:01.820
small neural network in the next video
we'll go over that so you can start

00:08:01.820 --> 00:08:01.830
we'll go over that so you can start
 

00:08:01.830 --> 00:08:03.769
we'll go over that so you can start
gaining intuition about what neural

00:08:03.769 --> 00:08:03.779
gaining intuition about what neural
 

00:08:03.779 --> 00:08:06.589
gaining intuition about what neural
networks do so that let's go on to the

00:08:06.589 --> 00:08:06.599
networks do so that let's go on to the
 

00:08:06.599 --> 00:08:08.870
networks do so that let's go on to the
next video about how to view logistic

00:08:08.870 --> 00:08:08.880
next video about how to view logistic
 

00:08:08.880 --> 00:08:11.059
next video about how to view logistic
regression as a very small neural

00:08:11.059 --> 00:08:11.069
regression as a very small neural
 

00:08:11.069 --> 00:08:13.430
regression as a very small neural
network

