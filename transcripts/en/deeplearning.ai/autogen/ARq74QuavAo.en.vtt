WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.810
 
drop out does this seemingly crazy thing

00:00:02.810 --> 00:00:02.820
drop out does this seemingly crazy thing
 

00:00:02.820 --> 00:00:05.480
drop out does this seemingly crazy thing
of randomly knocking out eun-seo Network

00:00:05.480 --> 00:00:05.490
of randomly knocking out eun-seo Network
 

00:00:05.490 --> 00:00:07.550
of randomly knocking out eun-seo Network
why does it work so well as a regulator

00:00:07.550 --> 00:00:07.560
why does it work so well as a regulator
 

00:00:07.560 --> 00:00:11.030
why does it work so well as a regulator
let's gain some better intuition in the

00:00:11.030 --> 00:00:11.040
let's gain some better intuition in the
 

00:00:11.040 --> 00:00:13.039
let's gain some better intuition in the
previous video I gave this intuition

00:00:13.039 --> 00:00:13.049
previous video I gave this intuition
 

00:00:13.049 --> 00:00:15.950
previous video I gave this intuition
that drop out randomly knocks out units

00:00:15.950 --> 00:00:15.960
that drop out randomly knocks out units
 

00:00:15.960 --> 00:00:17.930
that drop out randomly knocks out units
your network so it's as if on every

00:00:17.930 --> 00:00:17.940
your network so it's as if on every
 

00:00:17.940 --> 00:00:19.670
your network so it's as if on every
iteration you're working with the

00:00:19.670 --> 00:00:19.680
iteration you're working with the
 

00:00:19.680 --> 00:00:21.950
iteration you're working with the
smaller neural network and so using a

00:00:21.950 --> 00:00:21.960
smaller neural network and so using a
 

00:00:21.960 --> 00:00:23.630
smaller neural network and so using a
smaller neural network seems like it

00:00:23.630 --> 00:00:23.640
smaller neural network seems like it
 

00:00:23.640 --> 00:00:25.670
smaller neural network seems like it
should have a regular izing effect

00:00:25.670 --> 00:00:25.680
should have a regular izing effect
 

00:00:25.680 --> 00:00:28.339
should have a regular izing effect
here's the second intuition which is you

00:00:28.339 --> 00:00:28.349
here's the second intuition which is you
 

00:00:28.349 --> 00:00:29.750
here's the second intuition which is you
know let's look at it from the

00:00:29.750 --> 00:00:29.760
know let's look at it from the
 

00:00:29.760 --> 00:00:33.709
know let's look at it from the
perspective of a single unit all right

00:00:33.709 --> 00:00:33.719
perspective of a single unit all right
 

00:00:33.719 --> 00:00:35.840
perspective of a single unit all right
let's say this one now for this unit to

00:00:35.840 --> 00:00:35.850
let's say this one now for this unit to
 

00:00:35.850 --> 00:00:38.420
let's say this one now for this unit to
do is job as for input then needs to

00:00:38.420 --> 00:00:38.430
do is job as for input then needs to
 

00:00:38.430 --> 00:00:41.720
do is job as for input then needs to
generate some meaningful output now with

00:00:41.720 --> 00:00:41.730
generate some meaningful output now with
 

00:00:41.730 --> 00:00:44.389
generate some meaningful output now with
dropout the inputs can get randomly

00:00:44.389 --> 00:00:44.399
dropout the inputs can get randomly
 

00:00:44.399 --> 00:00:46.760
dropout the inputs can get randomly
eliminated you know sometimes those two

00:00:46.760 --> 00:00:46.770
eliminated you know sometimes those two
 

00:00:46.770 --> 00:00:48.500
eliminated you know sometimes those two
units will get eliminated sometimes a

00:00:48.500 --> 00:00:48.510
units will get eliminated sometimes a
 

00:00:48.510 --> 00:00:50.779
units will get eliminated sometimes a
different unit will get eliminated so

00:00:50.779 --> 00:00:50.789
different unit will get eliminated so
 

00:00:50.789 --> 00:00:52.880
different unit will get eliminated so
what these are at this unit which I'm

00:00:52.880 --> 00:00:52.890
what these are at this unit which I'm
 

00:00:52.890 --> 00:00:55.549
what these are at this unit which I'm
circling purple it can't rely on any one

00:00:55.549 --> 00:00:55.559
circling purple it can't rely on any one
 

00:00:55.559 --> 00:00:58.130
circling purple it can't rely on any one
feature because any one feature could go

00:00:58.130 --> 00:00:58.140
feature because any one feature could go
 

00:00:58.140 --> 00:01:01.099
feature because any one feature could go
away at random or any one of its own

00:01:01.099 --> 00:01:01.109
away at random or any one of its own
 

00:01:01.109 --> 00:01:04.100
away at random or any one of its own
influence could go away in random so in

00:01:04.100 --> 00:01:04.110
influence could go away in random so in
 

00:01:04.110 --> 00:01:06.320
influence could go away in random so in
particular be reluctant to put all of

00:01:06.320 --> 00:01:06.330
particular be reluctant to put all of
 

00:01:06.330 --> 00:01:10.370
particular be reluctant to put all of
this bets on say just this input right

00:01:10.370 --> 00:01:10.380
this bets on say just this input right
 

00:01:10.380 --> 00:01:12.679
this bets on say just this input right
the ways we reluctant to put too much

00:01:12.679 --> 00:01:12.689
the ways we reluctant to put too much
 

00:01:12.689 --> 00:01:15.020
the ways we reluctant to put too much
weight on any one input because army can

00:01:15.020 --> 00:01:15.030
weight on any one input because army can
 

00:01:15.030 --> 00:01:17.810
weight on any one input because army can
go away so this unit would be more

00:01:17.810 --> 00:01:17.820
go away so this unit would be more
 

00:01:17.820 --> 00:01:19.670
go away so this unit would be more
motivated to straight out this weight

00:01:19.670 --> 00:01:19.680
motivated to straight out this weight
 

00:01:19.680 --> 00:01:21.230
motivated to straight out this weight
and give you a little bit of weight to

00:01:21.230 --> 00:01:21.240
and give you a little bit of weight to
 

00:01:21.240 --> 00:01:25.969
and give you a little bit of weight to
each of the four inputs to this unit and

00:01:25.969 --> 00:01:25.979
each of the four inputs to this unit and
 

00:01:25.979 --> 00:01:27.710
each of the four inputs to this unit and
by spreading all the weights

00:01:27.710 --> 00:01:27.720
by spreading all the weights
 

00:01:27.720 --> 00:01:29.510
by spreading all the weights
this will tend to have an effect of

00:01:29.510 --> 00:01:29.520
this will tend to have an effect of
 

00:01:29.520 --> 00:01:33.830
this will tend to have an effect of
shrinking the squared norm of the waste

00:01:33.830 --> 00:01:33.840
shrinking the squared norm of the waste
 

00:01:33.840 --> 00:01:37.789
shrinking the squared norm of the waste
and so similar to what we saw with l2

00:01:37.789 --> 00:01:37.799
and so similar to what we saw with l2
 

00:01:37.799 --> 00:01:39.620
and so similar to what we saw with l2
regularization the effect of

00:01:39.620 --> 00:01:39.630
regularization the effect of
 

00:01:39.630 --> 00:01:41.510
regularization the effect of
implementing dropout is that it strings

00:01:41.510 --> 00:01:41.520
implementing dropout is that it strings
 

00:01:41.520 --> 00:01:43.429
implementing dropout is that it strings
aways and does similar to l2

00:01:43.429 --> 00:01:43.439
aways and does similar to l2
 

00:01:43.439 --> 00:01:45.080
aways and does similar to l2
regularization it helps to prevent

00:01:45.080 --> 00:01:45.090
regularization it helps to prevent
 

00:01:45.090 --> 00:01:47.090
regularization it helps to prevent
overfitting but it turns out that

00:01:47.090 --> 00:01:47.100
overfitting but it turns out that
 

00:01:47.100 --> 00:01:49.429
overfitting but it turns out that
dropout can formerly be shown to be an

00:01:49.429 --> 00:01:49.439
dropout can formerly be shown to be an
 

00:01:49.439 --> 00:01:52.340
dropout can formerly be shown to be an
adaptive form of l2 regularization but

00:01:52.340 --> 00:01:52.350
adaptive form of l2 regularization but
 

00:01:52.350 --> 00:01:54.350
adaptive form of l2 regularization but
the l2 penalty on different waves are

00:01:54.350 --> 00:01:54.360
the l2 penalty on different waves are
 

00:01:54.360 --> 00:01:56.780
the l2 penalty on different waves are
different depending on the size of the

00:01:56.780 --> 00:01:56.790
different depending on the size of the
 

00:01:56.790 --> 00:01:58.399
different depending on the size of the
activations being multiplied into that

00:01:58.399 --> 00:01:58.409
activations being multiplied into that
 

00:01:58.409 --> 00:02:01.100
activations being multiplied into that
weight but to summarize it is possible

00:02:01.100 --> 00:02:01.110
weight but to summarize it is possible
 

00:02:01.110 --> 00:02:03.289
weight but to summarize it is possible
to show that dropout has a no similar

00:02:03.289 --> 00:02:03.299
to show that dropout has a no similar
 

00:02:03.299 --> 00:02:07.520
to show that dropout has a no similar
effect to l2 regularization only the l2

00:02:07.520 --> 00:02:07.530
effect to l2 regularization only the l2
 

00:02:07.530 --> 00:02:09.109
effect to l2 regularization only the l2
regularization applied to different ways

00:02:09.109 --> 00:02:09.119
regularization applied to different ways
 

00:02:09.119 --> 00:02:10.969
regularization applied to different ways
can be a little bit different and even

00:02:10.969 --> 00:02:10.979
can be a little bit different and even
 

00:02:10.979 --> 00:02:12.740
can be a little bit different and even
more adaptive than scale of different

00:02:12.740 --> 00:02:12.750
more adaptive than scale of different
 

00:02:12.750 --> 00:02:14.440
more adaptive than scale of different
inputs one more detail

00:02:14.440 --> 00:02:14.450
inputs one more detail
 

00:02:14.450 --> 00:02:16.210
inputs one more detail
when you're implementing dropout she's a

00:02:16.210 --> 00:02:16.220
when you're implementing dropout she's a
 

00:02:16.220 --> 00:02:18.759
when you're implementing dropout she's a
network where you have three input

00:02:18.759 --> 00:02:18.769
network where you have three input
 

00:02:18.769 --> 00:02:24.790
network where you have three input
features this is 7 7 units 0 7 3 2 1 so

00:02:24.790 --> 00:02:24.800
features this is 7 7 units 0 7 3 2 1 so
 

00:02:24.800 --> 00:02:26.440
features this is 7 7 units 0 7 3 2 1 so
one of the parameters we have to choose

00:02:26.440 --> 00:02:26.450
one of the parameters we have to choose
 

00:02:26.450 --> 00:02:29.350
one of the parameters we have to choose
was the cheap profit which is a charm to

00:02:29.350 --> 00:02:29.360
was the cheap profit which is a charm to
 

00:02:29.360 --> 00:02:32.500
was the cheap profit which is a charm to
keeping a unit in each layer so it is

00:02:32.500 --> 00:02:32.510
keeping a unit in each layer so it is
 

00:02:32.510 --> 00:02:35.860
keeping a unit in each layer so it is
also feasible to very key prop by layer

00:02:35.860 --> 00:02:35.870
also feasible to very key prop by layer
 

00:02:35.870 --> 00:02:40.089
also feasible to very key prop by layer
so for the first layer your matrix W 1

00:02:40.089 --> 00:02:40.099
so for the first layer your matrix W 1
 

00:02:40.099 --> 00:02:42.009
so for the first layer your matrix W 1
will be 3 by 7

00:02:42.009 --> 00:02:42.019
will be 3 by 7
 

00:02:42.019 --> 00:02:45.780
will be 3 by 7
your second weight matrix will be 7 by 7

00:02:45.780 --> 00:02:45.790
your second weight matrix will be 7 by 7
 

00:02:45.790 --> 00:02:50.800
your second weight matrix will be 7 by 7
W 3 will be 7 by 3 and so on and so W 2

00:02:50.800 --> 00:02:50.810
W 3 will be 7 by 3 and so on and so W 2
 

00:02:50.810 --> 00:02:52.900
W 3 will be 7 by 3 and so on and so W 2
is actually the biggest weight matrix

00:02:52.900 --> 00:02:52.910
is actually the biggest weight matrix
 

00:02:52.910 --> 00:02:55.089
is actually the biggest weight matrix
right those are actually the largest of

00:02:55.089 --> 00:02:55.099
right those are actually the largest of
 

00:02:55.099 --> 00:02:56.920
right those are actually the largest of
the parameters will be in W 2 which is 7

00:02:56.920 --> 00:02:56.930
the parameters will be in W 2 which is 7
 

00:02:56.930 --> 00:03:00.309
the parameters will be in W 2 which is 7
by 7 so to prevent to reduce over

00:03:00.309 --> 00:03:00.319
by 7 so to prevent to reduce over
 

00:03:00.319 --> 00:03:02.890
by 7 so to prevent to reduce over
setting of that matrix maybe for this

00:03:02.890 --> 00:03:02.900
setting of that matrix maybe for this
 

00:03:02.900 --> 00:03:05.620
setting of that matrix maybe for this
layer I guess this is layer 2 you might

00:03:05.620 --> 00:03:05.630
layer I guess this is layer 2 you might
 

00:03:05.630 --> 00:03:08.890
layer I guess this is layer 2 you might
have a cheap cost as relatively low say

00:03:08.890 --> 00:03:08.900
have a cheap cost as relatively low say
 

00:03:08.900 --> 00:03:12.370
have a cheap cost as relatively low say
0.5 where's 4 different layers where you

00:03:12.370 --> 00:03:12.380
0.5 where's 4 different layers where you
 

00:03:12.380 --> 00:03:13.780
0.5 where's 4 different layers where you
might worry less well just again you

00:03:13.780 --> 00:03:13.790
might worry less well just again you
 

00:03:13.790 --> 00:03:15.250
might worry less well just again you
could have a higher key problem in

00:03:15.250 --> 00:03:15.260
could have a higher key problem in
 

00:03:15.260 --> 00:03:20.770
could have a higher key problem in
reducing 0.7 maybe this is 0.7 and if

00:03:20.770 --> 00:03:20.780
reducing 0.7 maybe this is 0.7 and if
 

00:03:20.780 --> 00:03:21.879
reducing 0.7 maybe this is 0.7 and if
the layers we don't worry about

00:03:21.879 --> 00:03:21.889
the layers we don't worry about
 

00:03:21.889 --> 00:03:23.289
the layers we don't worry about
overfitting at all you can have a keep

00:03:23.289 --> 00:03:23.299
overfitting at all you can have a keep
 

00:03:23.299 --> 00:03:26.650
overfitting at all you can have a keep
drop of 1.0 right so you know for

00:03:26.650 --> 00:03:26.660
drop of 1.0 right so you know for
 

00:03:26.660 --> 00:03:29.440
drop of 1.0 right so you know for
clarity these are numbers I'm drawing in

00:03:29.440 --> 00:03:29.450
clarity these are numbers I'm drawing in
 

00:03:29.450 --> 00:03:31.210
clarity these are numbers I'm drawing in
the purple boxes these could be

00:03:31.210 --> 00:03:31.220
the purple boxes these could be
 

00:03:31.220 --> 00:03:33.789
the purple boxes these could be
different key prompts for different

00:03:33.789 --> 00:03:33.799
different key prompts for different
 

00:03:33.799 --> 00:03:36.580
different key prompts for different
layers notice that the key problem 1.0

00:03:36.580 --> 00:03:36.590
layers notice that the key problem 1.0
 

00:03:36.590 --> 00:03:38.410
layers notice that the key problem 1.0
means that you're keeping every unit and

00:03:38.410 --> 00:03:38.420
means that you're keeping every unit and
 

00:03:38.420 --> 00:03:40.960
means that you're keeping every unit and
so you're really not using broad drop

00:03:40.960 --> 00:03:40.970
so you're really not using broad drop
 

00:03:40.970 --> 00:03:43.270
so you're really not using broad drop
out for that layer but the layers where

00:03:43.270 --> 00:03:43.280
out for that layer but the layers where
 

00:03:43.280 --> 00:03:44.530
out for that layer but the layers where
you're more worried about overfitting

00:03:44.530 --> 00:03:44.540
you're more worried about overfitting
 

00:03:44.540 --> 00:03:46.000
you're more worried about overfitting
really the layers of all the parameters

00:03:46.000 --> 00:03:46.010
really the layers of all the parameters
 

00:03:46.010 --> 00:03:48.460
really the layers of all the parameters
you could say key prompt to be smaller

00:03:48.460 --> 00:03:48.470
you could say key prompt to be smaller
 

00:03:48.470 --> 00:03:51.400
you could say key prompt to be smaller
to apply a more powerful form of dropout

00:03:51.400 --> 00:03:51.410
to apply a more powerful form of dropout
 

00:03:51.410 --> 00:03:52.990
to apply a more powerful form of dropout
it's kind of like cranking up the

00:03:52.990 --> 00:03:53.000
it's kind of like cranking up the
 

00:03:53.000 --> 00:03:55.449
it's kind of like cranking up the
regularization parameter lambda of l2

00:03:55.449 --> 00:03:55.459
regularization parameter lambda of l2
 

00:03:55.459 --> 00:03:57.069
regularization parameter lambda of l2
regularization when you try to

00:03:57.069 --> 00:03:57.079
regularization when you try to
 

00:03:57.079 --> 00:03:58.900
regularization when you try to
regularize some layers more than others

00:03:58.900 --> 00:03:58.910
regularize some layers more than others
 

00:03:58.910 --> 00:04:00.580
regularize some layers more than others
and technically you can also apply

00:04:00.580 --> 00:04:00.590
and technically you can also apply
 

00:04:00.590 --> 00:04:03.220
and technically you can also apply
dropout to the input layer where you can

00:04:03.220 --> 00:04:03.230
dropout to the input layer where you can
 

00:04:03.230 --> 00:04:05.050
dropout to the input layer where you can
have some cons of you know just acting

00:04:05.050 --> 00:04:05.060
have some cons of you know just acting
 

00:04:05.060 --> 00:04:06.610
have some cons of you know just acting
on one or more of the input features

00:04:06.610 --> 00:04:06.620
on one or more of the input features
 

00:04:06.620 --> 00:04:09.879
on one or more of the input features
although in practice usually don't do

00:04:09.879 --> 00:04:09.889
although in practice usually don't do
 

00:04:09.889 --> 00:04:13.449
although in practice usually don't do
that that often and so Chih problem 1.0

00:04:13.449 --> 00:04:13.459
that that often and so Chih problem 1.0
 

00:04:13.459 --> 00:04:15.309
that that often and so Chih problem 1.0
is quite common for the input layer you

00:04:15.309 --> 00:04:15.319
is quite common for the input layer you
 

00:04:15.319 --> 00:04:17.860
is quite common for the input layer you
might also use a very high value is 0.9

00:04:17.860 --> 00:04:17.870
might also use a very high value is 0.9
 

00:04:17.870 --> 00:04:20.229
might also use a very high value is 0.9
but it's much less likely that you know

00:04:20.229 --> 00:04:20.239
but it's much less likely that you know
 

00:04:20.239 --> 00:04:20.440
but it's much less likely that you know
you

00:04:20.440 --> 00:04:20.450
you
 

00:04:20.450 --> 00:04:21.970
you
once eliminate half of the input

00:04:21.970 --> 00:04:21.980
once eliminate half of the input
 

00:04:21.980 --> 00:04:24.820
once eliminate half of the input
features so usually key problem if you

00:04:24.820 --> 00:04:24.830
features so usually key problem if you
 

00:04:24.830 --> 00:04:27.400
features so usually key problem if you
apply that all will be a number close to

00:04:27.400 --> 00:04:27.410
apply that all will be a number close to
 

00:04:27.410 --> 00:04:30.880
apply that all will be a number close to
one if you even apply dropout at all to

00:04:30.880 --> 00:04:30.890
one if you even apply dropout at all to
 

00:04:30.890 --> 00:04:33.370
one if you even apply dropout at all to
the input layer so just to summarize if

00:04:33.370 --> 00:04:33.380
the input layer so just to summarize if
 

00:04:33.380 --> 00:04:35.020
the input layer so just to summarize if
you are more worried about some layers

00:04:35.020 --> 00:04:35.030
you are more worried about some layers
 

00:04:35.030 --> 00:04:37.030
you are more worried about some layers
overfitting than others you can set a

00:04:37.030 --> 00:04:37.040
overfitting than others you can set a
 

00:04:37.040 --> 00:04:39.550
overfitting than others you can set a
lower key prop for some layers than

00:04:39.550 --> 00:04:39.560
lower key prop for some layers than
 

00:04:39.560 --> 00:04:40.090
lower key prop for some layers than
others

00:04:40.090 --> 00:04:40.100
others
 

00:04:40.100 --> 00:04:41.740
others
the downside is this gives you even more

00:04:41.740 --> 00:04:41.750
the downside is this gives you even more
 

00:04:41.750 --> 00:04:43.720
the downside is this gives you even more
hyper parameters to search for using

00:04:43.720 --> 00:04:43.730
hyper parameters to search for using
 

00:04:43.730 --> 00:04:45.970
hyper parameters to search for using
cross-validation one other alternative

00:04:45.970 --> 00:04:45.980
cross-validation one other alternative
 

00:04:45.980 --> 00:04:48.130
cross-validation one other alternative
might be to have some layers where you

00:04:48.130 --> 00:04:48.140
might be to have some layers where you
 

00:04:48.140 --> 00:04:49.900
might be to have some layers where you
apply dropout in some ways we don't

00:04:49.900 --> 00:04:49.910
apply dropout in some ways we don't
 

00:04:49.910 --> 00:04:51.730
apply dropout in some ways we don't
apply dropout and in terms of one hyper

00:04:51.730 --> 00:04:51.740
apply dropout and in terms of one hyper
 

00:04:51.740 --> 00:04:53.770
apply dropout and in terms of one hyper
parameter which is the key prop for the

00:04:53.770 --> 00:04:53.780
parameter which is the key prop for the
 

00:04:53.780 --> 00:04:55.390
parameter which is the key prop for the
layers for which you do apply dropout

00:04:55.390 --> 00:04:55.400
layers for which you do apply dropout
 

00:04:55.400 --> 00:04:57.550
layers for which you do apply dropout
and before we wrap up just a couple

00:04:57.550 --> 00:04:57.560
and before we wrap up just a couple
 

00:04:57.560 --> 00:04:59.950
and before we wrap up just a couple
implementational tips many of the first

00:04:59.950 --> 00:04:59.960
implementational tips many of the first
 

00:04:59.960 --> 00:05:01.780
implementational tips many of the first
successful implementations of dropouts

00:05:01.780 --> 00:05:01.790
successful implementations of dropouts
 

00:05:01.790 --> 00:05:04.600
successful implementations of dropouts
were to computer vision so in computer

00:05:04.600 --> 00:05:04.610
were to computer vision so in computer
 

00:05:04.610 --> 00:05:07.180
were to computer vision so in computer
vision the input size is so big you in

00:05:07.180 --> 00:05:07.190
vision the input size is so big you in
 

00:05:07.190 --> 00:05:09.700
vision the input size is so big you in
putting all these pixels that you almost

00:05:09.700 --> 00:05:09.710
putting all these pixels that you almost
 

00:05:09.710 --> 00:05:12.130
putting all these pixels that you almost
never have enough data and so dropout is

00:05:12.130 --> 00:05:12.140
never have enough data and so dropout is
 

00:05:12.140 --> 00:05:14.110
never have enough data and so dropout is
very frequently used by it in computer

00:05:14.110 --> 00:05:14.120
very frequently used by it in computer
 

00:05:14.120 --> 00:05:15.910
very frequently used by it in computer
vision and there are some conservation

00:05:15.910 --> 00:05:15.920
vision and there are some conservation
 

00:05:15.920 --> 00:05:17.680
vision and there are some conservation
researchers that pretty much always use

00:05:17.680 --> 00:05:17.690
researchers that pretty much always use
 

00:05:17.690 --> 00:05:20.710
researchers that pretty much always use
it almost as a default but really the

00:05:20.710 --> 00:05:20.720
it almost as a default but really the
 

00:05:20.720 --> 00:05:23.200
it almost as a default but really the
thing to remember is that dropout is a

00:05:23.200 --> 00:05:23.210
thing to remember is that dropout is a
 

00:05:23.210 --> 00:05:25.270
thing to remember is that dropout is a
regularization technique it helps

00:05:25.270 --> 00:05:25.280
regularization technique it helps
 

00:05:25.280 --> 00:05:28.270
regularization technique it helps
prevent overfitting and so unless my

00:05:28.270 --> 00:05:28.280
prevent overfitting and so unless my
 

00:05:28.280 --> 00:05:31.810
prevent overfitting and so unless my
algorithm is over fitting I wouldn't

00:05:31.810 --> 00:05:31.820
algorithm is over fitting I wouldn't
 

00:05:31.820 --> 00:05:33.910
algorithm is over fitting I wouldn't
actually bother the use drop also is

00:05:33.910 --> 00:05:33.920
actually bother the use drop also is
 

00:05:33.920 --> 00:05:35.470
actually bother the use drop also is
used somewhat less often than other

00:05:35.470 --> 00:05:35.480
used somewhat less often than other
 

00:05:35.480 --> 00:05:37.630
used somewhat less often than other
application areas it's just the computer

00:05:37.630 --> 00:05:37.640
application areas it's just the computer
 

00:05:37.640 --> 00:05:39.790
application areas it's just the computer
vision you know you usually just don't

00:05:39.790 --> 00:05:39.800
vision you know you usually just don't
 

00:05:39.800 --> 00:05:40.960
vision you know you usually just don't
have an update or so you're almost

00:05:40.960 --> 00:05:40.970
have an update or so you're almost
 

00:05:40.970 --> 00:05:43.390
have an update or so you're almost
always overfitting which is why they

00:05:43.390 --> 00:05:43.400
always overfitting which is why they
 

00:05:43.400 --> 00:05:44.500
always overfitting which is why they
tend to be some computer vision

00:05:44.500 --> 00:05:44.510
tend to be some computer vision
 

00:05:44.510 --> 00:05:46.540
tend to be some computer vision
researchers square by drop out by the

00:05:46.540 --> 00:05:46.550
researchers square by drop out by the
 

00:05:46.550 --> 00:05:49.270
researchers square by drop out by the
intuition always doesn't always

00:05:49.270 --> 00:05:49.280
intuition always doesn't always
 

00:05:49.280 --> 00:05:51.750
intuition always doesn't always
generalize I think to other disciplines

00:05:51.750 --> 00:05:51.760
generalize I think to other disciplines
 

00:05:51.760 --> 00:05:56.920
generalize I think to other disciplines
one big downside of dropouts is that the

00:05:56.920 --> 00:05:56.930
one big downside of dropouts is that the
 

00:05:56.930 --> 00:05:59.680
one big downside of dropouts is that the
cost function J is no longer well

00:05:59.680 --> 00:05:59.690
cost function J is no longer well
 

00:05:59.690 --> 00:06:02.530
cost function J is no longer well
defined on every iteration they are

00:06:02.530 --> 00:06:02.540
defined on every iteration they are
 

00:06:02.540 --> 00:06:05.380
defined on every iteration they are
randomly you know killing off a bunch of

00:06:05.380 --> 00:06:05.390
randomly you know killing off a bunch of
 

00:06:05.390 --> 00:06:08.740
randomly you know killing off a bunch of
nodes and so if you are double checking

00:06:08.740 --> 00:06:08.750
nodes and so if you are double checking
 

00:06:08.750 --> 00:06:10.870
nodes and so if you are double checking
the performance or gradient descent it

00:06:10.870 --> 00:06:10.880
the performance or gradient descent it
 

00:06:10.880 --> 00:06:12.760
the performance or gradient descent it
is actually harder to double check that

00:06:12.760 --> 00:06:12.770
is actually harder to double check that
 

00:06:12.770 --> 00:06:15.910
is actually harder to double check that
and you have a well-defined cost

00:06:15.910 --> 00:06:15.920
and you have a well-defined cost
 

00:06:15.920 --> 00:06:18.460
and you have a well-defined cost
function J that is going downhill on

00:06:18.460 --> 00:06:18.470
function J that is going downhill on
 

00:06:18.470 --> 00:06:21.250
function J that is going downhill on
every iteration because the cost

00:06:21.250 --> 00:06:21.260
every iteration because the cost
 

00:06:21.260 --> 00:06:23.110
every iteration because the cost
function J that you're optimizing is

00:06:23.110 --> 00:06:23.120
function J that you're optimizing is
 

00:06:23.120 --> 00:06:25.540
function J that you're optimizing is
actually less less well defined there is

00:06:25.540 --> 00:06:25.550
actually less less well defined there is
 

00:06:25.550 --> 00:06:28.030
actually less less well defined there is
a surfing hard to calculate so you lose

00:06:28.030 --> 00:06:28.040
a surfing hard to calculate so you lose
 

00:06:28.040 --> 00:06:30.040
a surfing hard to calculate so you lose
the debugging tool through the plot a

00:06:30.040 --> 00:06:30.050
the debugging tool through the plot a
 

00:06:30.050 --> 00:06:31.810
the debugging tool through the plot a
graph like this

00:06:31.810 --> 00:06:31.820
graph like this
 

00:06:31.820 --> 00:06:34.420
graph like this
so what I usually do is turn off drop

00:06:34.420 --> 00:06:34.430
so what I usually do is turn off drop
 

00:06:34.430 --> 00:06:36.430
so what I usually do is turn off drop
out of you will set key property equals

00:06:36.430 --> 00:06:36.440
out of you will set key property equals
 

00:06:36.440 --> 00:06:38.770
out of you will set key property equals
one and it run my code make sure that it

00:06:38.770 --> 00:06:38.780
one and it run my code make sure that it
 

00:06:38.780 --> 00:06:41.440
one and it run my code make sure that it
is monotonically decreasing J and then

00:06:41.440 --> 00:06:41.450
is monotonically decreasing J and then
 

00:06:41.450 --> 00:06:43.810
is monotonically decreasing J and then
turn on dropout and hope that you know I

00:06:43.810 --> 00:06:43.820
turn on dropout and hope that you know I
 

00:06:43.820 --> 00:06:46.240
turn on dropout and hope that you know I
didn't introduce a welcome to my code

00:06:46.240 --> 00:06:46.250
didn't introduce a welcome to my code
 

00:06:46.250 --> 00:06:48.400
didn't introduce a welcome to my code
during drop out because you need other

00:06:48.400 --> 00:06:48.410
during drop out because you need other
 

00:06:48.410 --> 00:06:50.050
during drop out because you need other
ways I guess but not plotting these

00:06:50.050 --> 00:06:50.060
ways I guess but not plotting these
 

00:06:50.060 --> 00:06:51.880
ways I guess but not plotting these
figures to make sure that your code is

00:06:51.880 --> 00:06:51.890
figures to make sure that your code is
 

00:06:51.890 --> 00:06:54.250
figures to make sure that your code is
working the Granderson is working even

00:06:54.250 --> 00:06:54.260
working the Granderson is working even
 

00:06:54.260 --> 00:06:58.060
working the Granderson is working even
with drop out so with that there are so

00:06:58.060 --> 00:06:58.070
with drop out so with that there are so
 

00:06:58.070 --> 00:07:00.430
with drop out so with that there are so
a few more regularization techniques

00:07:00.430 --> 00:07:00.440
a few more regularization techniques
 

00:07:00.440 --> 00:07:02.380
a few more regularization techniques
that work your knowing let's talk about

00:07:02.380 --> 00:07:02.390
that work your knowing let's talk about
 

00:07:02.390 --> 00:07:03.910
that work your knowing let's talk about
a few more such techniques in the next

00:07:03.910 --> 00:07:03.920
a few more such techniques in the next
 

00:07:03.920 --> 00:07:06.370
a few more such techniques in the next
video

