WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.952
[MUSIC PLAYING]

00:00:06.692 --> 00:00:07.400
PAIGE BAILEY: Hi.

00:00:07.400 --> 00:00:10.670
I'm Paige, and I'm a Developer
Advocate for TensorFlow.

00:00:10.670 --> 00:00:13.820
Machine learning techniques like
Convolutional Neural Networks,

00:00:13.820 --> 00:00:18.110
also called CNNs, and Generative
Adversarial Networks, or GANs,

00:00:18.110 --> 00:00:21.750
have shown great promise in a
diverse range of applications--

00:00:21.750 --> 00:00:23.720
everything from
image classification

00:00:23.720 --> 00:00:26.780
to scene reconstruction
to speech recognition.

00:00:26.780 --> 00:00:28.670
To efficiently
train these models

00:00:28.670 --> 00:00:31.790
on massive amounts of data,
machine learning engineers

00:00:31.790 --> 00:00:35.090
often need to use specialized
hardware, such as Graphics

00:00:35.090 --> 00:00:39.910
Processing Units, GPUs, or
Tensor Processing Units, TPUs.

00:00:39.910 --> 00:00:43.010
GPUs and TPUs are
used as accelerators

00:00:43.010 --> 00:00:45.410
for the portions of the
model that can be broken up

00:00:45.410 --> 00:00:48.270
into parallelizable operations.

00:00:48.270 --> 00:00:51.230
Think of these chips as
very specialized tools

00:00:51.230 --> 00:00:55.790
that can do one particular task
extremely well and extremely

00:00:55.790 --> 00:00:56.660
quickly.

00:00:56.660 --> 00:00:58.790
When using this
specialized hardware,

00:00:58.790 --> 00:01:02.720
tasks that used to take days
or even weeks to complete now

00:01:02.720 --> 00:01:04.370
can only take minutes.

00:01:04.370 --> 00:01:07.370
The good news is that you can
develop deep learning models

00:01:07.370 --> 00:01:11.750
on Google Colab using
GPU and TPU at no cost.

00:01:11.750 --> 00:01:14.630
Let's dive into a
notebook and check it out.

00:01:14.630 --> 00:01:17.990
To change your runtime in
Google Colab, all you have to do

00:01:17.990 --> 00:01:21.890
is select Runtime, Change
Runtime Type, and then

00:01:21.890 --> 00:01:26.150
opt for None, GPU, or TPU.

00:01:26.150 --> 00:01:30.140
For this selection, we'll go
with GPU, and we'll hit Save.

00:01:30.140 --> 00:01:34.100
Let's install TensorFlow
2.0 with GPU support.

00:01:34.100 --> 00:01:37.700
We can confirm that TensorFlow
can see the GPU by running

00:01:37.700 --> 00:01:42.260
device name=tf.test.gpu
device name.

00:01:42.260 --> 00:01:44.850
Once the command has run,
we can see that the device

00:01:44.850 --> 00:01:47.540
is located at the 0 slot.

00:01:47.540 --> 00:01:49.790
To observe the TensorFlow
speed-up on GPU

00:01:49.790 --> 00:01:54.470
relative to CPU, let's use
this basic keras model.

00:01:54.470 --> 00:01:58.020
We hit Run, and
we find that mnest

00:01:58.020 --> 00:02:02.460
is able to train to completion
in around 43 seconds.

00:02:02.460 --> 00:02:06.660
With CPU, it would
take mnest 69 seconds

00:02:06.660 --> 00:02:08.550
to achieve the same accuracy.

00:02:08.550 --> 00:02:13.146
So you received almost a
third of a boost of speed.

00:02:13.146 --> 00:02:14.520
If you're interested
in obtaining

00:02:14.520 --> 00:02:16.740
additional information
about hardware,

00:02:16.740 --> 00:02:20.137
you can run these two commands
from any Colab notebook.

00:02:20.137 --> 00:02:21.720
This will tell you
everything that you

00:02:21.720 --> 00:02:25.830
need to know about
CPU, RAM, and GPU.

00:02:25.830 --> 00:02:29.740
For TPUs, let's try a bit more
interesting of an example.

00:02:29.740 --> 00:02:32.970
We'll change the runtime
type again, select TPU,

00:02:32.970 --> 00:02:34.230
and hit Save.

00:02:34.230 --> 00:02:36.540
Here, we're
predicting Shakespeare

00:02:36.540 --> 00:02:38.720
with TPUs and keras.

00:02:38.720 --> 00:02:42.000
In this Colab notebook, we'll
build a two-layer forward LSTM

00:02:42.000 --> 00:02:45.900
model, and we'll convert a keras
model to its equivalent TPU

00:02:45.900 --> 00:02:48.810
version, using the standard
keras methods Fit, Predict,

00:02:48.810 --> 00:02:50.130
and Evaluate.

00:02:50.130 --> 00:02:53.340
As we scroll down, we can see
that we're downloading data,

00:02:53.340 --> 00:02:56.730
we're building a data
generator with TF logging,

00:02:56.730 --> 00:02:59.370
we're checking to see the
size of the array coming in

00:02:59.370 --> 00:03:03.150
from Project Gutenberg, and
we're building our model.

00:03:03.150 --> 00:03:06.570
This model will take quite a
while to train, but remember,

00:03:06.570 --> 00:03:09.600
you're training against
every single aspect

00:03:09.600 --> 00:03:11.580
of Shakespeare's corpus.

00:03:11.580 --> 00:03:14.010
After we've cycled
through 10 [? epochs ?]

00:03:14.010 --> 00:03:16.470
and we have pretty
good accuracy,

00:03:16.470 --> 00:03:19.390
we can start making
predictions with our model.

00:03:19.390 --> 00:03:21.810
Let's take a look at what
a neural network thinks

00:03:21.810 --> 00:03:24.600
might be a Shakespearean play.

00:03:24.600 --> 00:03:26.900
So it's obviously not perfect.

00:03:26.900 --> 00:03:29.100
AI can't be the Bard yet.

00:03:29.100 --> 00:03:32.230
But this does look like
a traditional script.

00:03:32.230 --> 00:03:35.220
And you can see that if you
start adding layers, adding

00:03:35.220 --> 00:03:40.170
nodes, and even adding clusters
of TPUs as opposed to just one,

00:03:40.170 --> 00:03:42.600
you'll improve your
accuracy even more

00:03:42.600 --> 00:03:45.480
and start generating
Shakespeare-like plays

00:03:45.480 --> 00:03:47.350
all on your own.

00:03:47.350 --> 00:03:49.620
You just learned how to
accelerate your machine

00:03:49.620 --> 00:03:53.540
learning projects with GPUs
and TPUs in Google Colab.

00:03:53.540 --> 00:03:55.650
In the next video, I'll
walk you through how

00:03:55.650 --> 00:03:57.570
to upgrade your
existing TensorFlow

00:03:57.570 --> 00:03:59.740
code to TensorFlow 2.0.

00:03:59.740 --> 00:04:01.830
So subscribe to the
TensorFlow YouTube channel

00:04:01.830 --> 00:04:04.080
and be notified when
that video lands.

00:04:04.080 --> 00:04:07.639
In the meantime, keep building
cool projects with TensorFlow,

00:04:07.639 --> 00:04:09.930
and make sure to share them
on Twitter with the hashtag

00:04:09.930 --> 00:04:12.120
#PoweredbyTF.

00:04:12.120 --> 00:04:14.480
We're looking forward to
seeing what you create.

00:04:14.480 --> 00:04:15.990
And I'll see you next time.

00:04:15.990 --> 00:04:18.740
[MUSIC PLAYING]

