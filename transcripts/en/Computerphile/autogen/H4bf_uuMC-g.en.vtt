WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:02.700
Today we're going to be talking about big data. How big is big?

00:00:03.759 --> 00:00:04.900
so

00:00:04.900 --> 00:00:12.300
Well, first of all, there is no precise definition as a rule. So kind of be standard what people would say is

00:00:12.849 --> 00:00:16.409
When we can no longer reasonably deal with the data using traditional methods

00:00:16.990 --> 00:00:22.289
So that we kind of think what's a traditional method? Well, it might be can we process the data on a single computer?

00:00:23.140 --> 00:00:26.910
Can we store the data on a single computer? And if we can't then we're probably dealing

00:00:27.670 --> 00:00:33.090
With big data, so you need to have new methods in order to be able to handle and process this data

00:00:35.770 --> 00:00:42.839
As computers getting faster and bigger capacities and more memory and things that the concept of what becomes big is is changing, right?

00:00:42.940 --> 00:00:48.329
So kind of but a lot of it isn't really as I'll talk about later isn't how

00:00:48.579 --> 00:00:50.579
Much power you can get in a single computer

00:00:50.739 --> 00:00:58.529
It's more how we can use multiple computers to split the data up process everything and then throw it back like in the MapReduce framework

00:00:58.809 --> 00:01:00.839
Then we talked about the for in with big data

00:01:00.840 --> 00:01:07.439
There's something called the five es which kind of defines some features and problems that are common amongst any Big Data things

00:01:07.570 --> 00:01:11.729
We have the five es and the first three that were defined. I think these were defined in 2001

00:01:11.729 --> 00:01:18.269
So that's kind of how having talked about four. So first of all, we've got some volume. So this is the most obvious one

00:01:18.270 --> 00:01:23.729
It's just simply how large the dataset it's the second one is

00:01:24.340 --> 00:01:25.479
velocity

00:01:25.479 --> 00:01:31.559
So a lot of the time these days huge amounts of data are being generated in a very short amount of time

00:01:31.560 --> 00:01:37.439
So you think of how much data Facebook is generating people liking stuff people uploading content that's happening constantly

00:01:37.869 --> 00:01:40.709
All throughout the day the amount of data they generate every day

00:01:40.710 --> 00:01:44.339
It's just huge basically so they need to process that in real time

00:01:44.340 --> 00:01:47.369
And the third one is variety

00:01:47.829 --> 00:01:53.849
Traditionally the data we would have and we would store it in a traditional single database. It would be in a very structured format

00:01:53.850 --> 00:01:57.689
So you've got columns and rows everywhere. He would have values for the columns these days

00:01:57.689 --> 00:01:59.500
We've got data coming in in a lot of different formats

00:01:59.500 --> 00:02:03.599
So as well as the traditional kind of structured data, we have unstructured data

00:02:03.790 --> 00:02:08.639
So you've got stuff coming like web dream cliques, we've got like social media likes coming in

00:02:08.640 --> 00:02:11.479
We've got stuff like images and audio and video

00:02:12.480 --> 00:02:17.810
So we need to be able to handle all these different types of data and extract what we need from them

00:02:17.810 --> 00:02:19.909
and the first one is

00:02:20.459 --> 00:02:22.459
value

00:02:23.489 --> 00:02:28.069
Yeah, so there's no point in us collecting huge amounts of data and then doing nothing with it

00:02:28.290 --> 00:02:32.870
So we want to know what we want to obtain from the data and then think of ways to go about that

00:02:33.110 --> 00:02:38.179
So something some form of value could just be getting humans to understand what is happening

00:02:38.700 --> 00:02:42.950
In that data. So for example if you have a fleet of lorries

00:02:42.950 --> 00:02:47.539
They will all have telematics sensors in that we collecting sensor data of what the lawyers are doing

00:02:47.549 --> 00:02:51.439
So it's of a lot of value to the fleet manager to then be able to easily

00:02:51.569 --> 00:02:57.018
Visualize huge amounts of data coming in and see what it's happening. So as well as processing and storing this stuff

00:02:57.019 --> 00:03:01.249
We also want to be able to visualize it and show it humans in an easily understandable format

00:03:01.250 --> 00:03:05.509
Oh, the value stuff is just finding patterns machine learning algorithms from all of this data

00:03:06.120 --> 00:03:08.390
see then the fifth and final one is

00:03:08.910 --> 00:03:12.469
Veracity this is basically how trustworthy the data is how reliable it is

00:03:12.540 --> 00:03:15.079
So we've got data coming in from a lot of different sources

00:03:15.419 --> 00:03:18.079
So is it being generated with statistical bias?

00:03:18.480 --> 00:03:24.019
Are there missing values if we use think for example the sensor data, we need to realize that maybe the sensors are faulty

00:03:24.019 --> 00:03:26.010
They're giving slightly off readings

00:03:26.010 --> 00:03:28.010
So it's important to understand how?

00:03:28.139 --> 00:03:31.369
Reliable the data we're looking at is and so these are kind of the five

00:03:31.470 --> 00:03:38.389
Standard features of Big Data some people try and add more. There's another seven V's a big data at the 10 meter producer

00:03:38.389 --> 00:03:40.389
I see. I'm sure we will keep going up and up

00:03:40.530 --> 00:03:44.149
They are doing things like don't like vulnerability. So

00:03:45.450 --> 00:03:49.009
Obviously when we're storing a lot of data a lot of that is quite personal data

00:03:49.010 --> 00:03:52.489
So making sure that's secure but these are the kind of the five main ones

00:03:52.489 --> 00:03:55.309
The first thing the big big data obviously is just the sheer volume

00:03:55.530 --> 00:04:00.589
So one way of dealing with this is to split the data across multiple computers

00:04:01.169 --> 00:04:06.319
So you could think okay. So we've got too much data to fit on one machine. We'll just get a more powerful computer

00:04:06.359 --> 00:04:09.559
We'll get more CPU power. We'll get larger memory

00:04:10.230 --> 00:04:13.639
that very quickly becomes quite difficult to manage because every time you need to

00:04:13.919 --> 00:04:18.329
Scale it up again because you've got even more data you to buy computer or new hardware

00:04:18.329 --> 00:04:24.569
So what tends to happen instead and all like they see all companies or just have like a cluster of computers?

00:04:24.970 --> 00:04:27.749
So rather than a single machine

00:04:27.750 --> 00:04:31.169
They'll have say a massive mean warehouse

00:04:31.169 --> 00:04:31.620
basically

00:04:31.620 --> 00:04:37.410
If you wind loads and loads and loads of computers and what this means that we can do is we can do distributed storage

00:04:37.410 --> 00:04:41.579
so each of those machines will store a portion of the data and then we can also

00:04:42.520 --> 00:04:47.460
Do the computation split across those machines rather than having one computer going through?

00:04:47.460 --> 00:04:52.650
I know a billion database records you can have each computer going through a thousand of those database records

00:04:53.680 --> 00:04:59.519
Let me take a really naive way of saying right. Ok, let's do it. Alphabetically, I'll load more records. Come in for say Zed

00:04:59.890 --> 00:05:06.059
That's easy. Stick it on the end load more records coming for P. This Y in the middle, right? How do you manage that?

00:05:06.400 --> 00:05:08.290
and so there's

00:05:08.290 --> 00:05:09.880
Computing frameworks that will help with this

00:05:09.880 --> 00:05:15.809
So for example, if you're storing data industry to fashion than this the Hadoop distributed file system

00:05:16.150 --> 00:05:23.669
And that will manage kind of the cluster resources where the files are stored and those frameworks will also provide fault tolerance and reliability

00:05:23.669 --> 00:05:30.689
So if one of the nose goes down, then it you've not lost that data. There will have been some replication across other nodes

00:05:30.760 --> 00:05:34.799
So that yeah losing a single node isn't going to cause you a lot of problems

00:05:34.930 --> 00:05:38.579
And what using a cluster also allows you to do is whenever you want to scale it up

00:05:38.580 --> 00:05:43.620
All you do is just add more computers into the network and you're done and you can get by on

00:05:44.140 --> 00:05:46.140
relatively cheap

00:05:46.330 --> 00:05:50.310
Hardware rather than having to keep buying a new supercomputer in a big data

00:05:51.039 --> 00:05:53.249
System there tends to be a pretty standard workflow

00:05:53.860 --> 00:05:58.200
so the first thing you would want to do is have a measure to

00:05:58.750 --> 00:06:04.229
Ingest the data remember, we've got a huge variety of data coming in. It's all coming in from different sources

00:06:04.810 --> 00:06:08.760
So we need a way to kind of aggregators and move it on to further down the pipeline

00:06:08.950 --> 00:06:16.589
So there's some frameworks for this. There's an Apache Capra and like Apache flume for example and loads and loads of others as well

00:06:17.650 --> 00:06:22.289
So basically aggregate all the data push it on to the rest of the system

00:06:22.289 --> 00:06:25.859
so then the second thing that you probably want to do is

00:06:26.409 --> 00:06:30.119
Store that data so like we just spoke about the distributed file system

00:06:30.120 --> 00:06:34.380
you store is in a distributed manner across the cluster then you want to

00:06:34.900 --> 00:06:38.220
Process this data and you may skip out storage entirely

00:06:38.220 --> 00:06:40.859
So in some cases you may not want to store your data

00:06:40.990 --> 00:06:43.200
You just want to process it use it to update

00:06:43.600 --> 00:06:47.820
Some machine learning model somewhere and then discard it and we don't care about long-term storage

00:06:48.060 --> 00:06:54.480
So you're processing the data again do it in disputed fashion using frameworks such as MapReduce or Apache spark

00:06:54.790 --> 00:07:01.619
Designing the algorithms to do that processing requires a little bit more thought than maybe doing a traditional algorithm with the frameworks

00:07:01.620 --> 00:07:06.480
We'll hide some of it but you need to be thinking that even if we're doing it through a framework

00:07:06.490 --> 00:07:12.720
We've still got data on different computers if we need to share messages between these computers during the computation

00:07:12.970 --> 00:07:16.079
It becomes quite expensive if we keep moving a lot of data across the network

00:07:16.480 --> 00:07:23.069
So it's designing algorithms that limit data movement around and it's the principle of data locality

00:07:23.080 --> 00:07:26.370
So you want to keep the computation close to the data?

00:07:27.070 --> 00:07:28.690
Don't move the data around

00:07:28.690 --> 00:07:34.559
Sometimes it's unavoidable, but we limit it. So the other thing about processing is that there's different ways of doing it

00:07:34.560 --> 00:07:35.680
There's batch processing

00:07:35.680 --> 00:07:39.840
So you already have all of your data or whatever you protected so far

00:07:39.850 --> 00:07:45.299
You take all of that data across the cluster you process all of that get your results and you're done

00:07:45.669 --> 00:07:50.159
The other thing we can do is real-time processing. So again because the velocity of the data is coming in

00:07:50.160 --> 00:07:53.070
We don't want to constantly have to take all the day to Detective

00:07:53.070 --> 00:07:56.099
Well produce it get results and then we've got a ton more data

00:07:56.100 --> 00:08:00.119
I want to do the same get all the data bring it back process all of it

00:08:01.060 --> 00:08:03.060
So instead we would

00:08:03.790 --> 00:08:07.979
Do real-time processing so as each data item arrives?

00:08:07.979 --> 00:08:14.249
We process that we don't have to look at all the data we've got so far. We just incrementally process everything

00:08:14.890 --> 00:08:17.909
And that's coming up in another video when we talk about data streaming

00:08:18.160 --> 00:08:23.820
So the other thing that you might want to do before processing is something called pre-processing remember I talked about unstructured data

00:08:24.160 --> 00:08:30.749
So maybe getting that data into a format that we specifically can use for the purpose we want to so

00:08:30.750 --> 00:08:34.890
That would be a stage in the pipeline before processing the other thing with huge amounts of data

00:08:35.050 --> 00:08:38.550
There's likely to be a lot of noise a lot of outliers so we can remove those

00:08:40.150 --> 00:08:46.469
We can also remove one instances, so if you think we're getting a ton of instances in and we want them she learning algorithm

00:08:46.470 --> 00:08:51.660
There'll be a lot of instances that are very very similar see an instance is say in a database

00:08:51.660 --> 00:08:57.750
It's like a single line in the database. So for HTV sensor reading it would be everything for that

00:08:57.750 --> 00:09:03.839
Lorry at that point in time CS speed directions traveling reducing. The number of instances is about reducing the granularity

00:09:03.839 --> 00:09:05.839
so part of it is saying

00:09:05.889 --> 00:09:08.459
if we store a rather than storing data for a

00:09:08.620 --> 00:09:14.909
Continuous period of time so every minute for an hour if those states are very similar across that we can just say okay for this

00:09:14.910 --> 00:09:22.199
period this is what happens and put it in a single line or we could say for example a machine learning algorithm if there's

00:09:22.870 --> 00:09:26.549
Instances with very very similar features and then a very very similar class

00:09:26.550 --> 00:09:30.539
We can take a single one of those instances and that will suitably represent

00:09:30.760 --> 00:09:36.089
All of those instances so we can very very quickly reduce a huge data set down to a much smaller one

00:09:36.250 --> 00:09:40.469
By saying there's a lot of redundancy here and we don't need a hundred very similar instances

00:09:40.630 --> 00:09:42.779
When we one would do just as well

00:09:42.779 --> 00:09:44.350
So if you've got a hundred

00:09:44.350 --> 00:09:51.480
Instances and you reduce it down to one is does not have an impact on how important those instances are in the scheme of things

00:09:52.120 --> 00:09:54.120
Yes, so techniques

00:09:54.610 --> 00:10:00.449
That deal with this stuff. Some of them would just purely say okay now this is a single instance and

00:10:01.000 --> 00:10:03.419
That's all you ever know others of them would

00:10:04.000 --> 00:10:05.610
Have yet have a waiting?

00:10:05.610 --> 00:10:10.469
So some way of saying this is a more important one because it's very similar to 100 others that we got rid of this one's

00:10:10.569 --> 00:10:16.319
really not as important because there are least three others that were similar to it so we can wait instances to kind of reflect their

00:10:16.319 --> 00:10:20.009
Importance. There are specific frameworks with big data streaming as well

00:10:20.050 --> 00:10:26.669
so there's technologies such as the spark streaming module' for apache' spark or there's newer ones such as

00:10:27.069 --> 00:10:31.409
Apache plink that can be used to do that. So they kind of abstracts away from the

00:10:31.959 --> 00:10:33.959
streaming aspects of it so you can focus

00:10:34.209 --> 00:10:38.669
Just in what you want to do a little thinking all this data is coming through very fast, obviously

00:10:38.670 --> 00:10:45.449
My limited brain is thinking streaming relates to video. But you're talking about just data that is happening in real time. Is that right?

00:10:45.519 --> 00:10:47.500
yes, so

00:10:47.500 --> 00:10:53.149
Going back to the Lori's as they're driving down the motorway. They may be sending out a sense of read every

00:10:53.760 --> 00:10:55.019
minute or so and

00:10:55.019 --> 00:10:59.599
That since the reading goes back we get all the sense readings from all the lorries coming in as a data stream

00:10:59.600 --> 00:11:05.570
so it's kind of a very quick roundup of the basics of Big Data and there's a lot of applications this obviously so

00:11:06.600 --> 00:11:13.670
Thanks, we'll have huge volumes of transaction data that you can extract patterns of value from that and see what is normal they can do

00:11:13.769 --> 00:11:19.609
Kind of fraud detection on that again. The previous example of fleet managers understanding what is going on

00:11:19.610 --> 00:11:25.459
basically any industry will now have ways of being able to extract value from the data that they have so in the next video we're

00:11:25.459 --> 00:11:32.569
Going to talk about data stream processing and more about how we actually deal with the problems that we all time data can presenters

00:11:33.690 --> 00:11:35.250
over very very large BIOS

00:11:35.250 --> 00:11:41.390
This kind of computation is a lot more efficient if you can distribute at because doing this map phase of saying, okay

00:11:41.390 --> 00:11:45.769
This is one occurrence. The letter A that's independent of anything else and see most

00:11:46.440 --> 00:11:52.130
Interested in you're probably only interested when a button is pressed or so on the only times positive

