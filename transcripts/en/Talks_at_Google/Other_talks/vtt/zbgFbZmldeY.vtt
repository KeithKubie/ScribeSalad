WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.455
[APPLAUSE]

00:00:07.774 --> 00:00:09.190
PETER SINGER: Thank
you very much.

00:00:09.190 --> 00:00:10.731
It's great to see
so many of you here

00:00:10.731 --> 00:00:12.490
despite the temptations
of getting out

00:00:12.490 --> 00:00:14.230
in the sunshine on
a beautiful day.

00:00:14.230 --> 00:00:15.940
And thank you very
much, [? Christine, ?]

00:00:15.940 --> 00:00:18.980
for having set this up
and for that introduction.

00:00:18.980 --> 00:00:22.980
So I'm talking about
an article that I

00:00:22.980 --> 00:00:25.240
wrote a very long time ago.

00:00:25.240 --> 00:00:27.740
"Famine, Affluence, and
Morality" was originally

00:00:27.740 --> 00:00:30.690
published in 1972.

00:00:30.690 --> 00:00:35.040
And it's now been
republished together

00:00:35.040 --> 00:00:38.230
with a couple of more recent
essays, and a previously

00:00:38.230 --> 00:00:41.680
unpublished preface,
and a forward by Bill

00:00:41.680 --> 00:00:44.234
and Melinda Gates
as this little book,

00:00:44.234 --> 00:00:45.650
"Famine, Affluence,
and Morality."

00:00:45.650 --> 00:00:49.950
And I'm delighted that OUP have
had the idea that this essay is

00:00:49.950 --> 00:00:53.110
still relevant
today and that it's

00:00:53.110 --> 00:00:55.760
something that is worth getting
out there and reminding people

00:00:55.760 --> 00:00:57.420
about.

00:00:57.420 --> 00:01:00.590
But let me just take you back a
little bit to the circumstances

00:01:00.590 --> 00:01:03.880
in which it was written,
which most of you

00:01:03.880 --> 00:01:05.674
will not be able
to remember I can

00:01:05.674 --> 00:01:06.840
see looking around the room.

00:01:06.840 --> 00:01:08.740
[LAUGHTER]

00:01:08.740 --> 00:01:12.800
So for those who
don't know, there

00:01:12.800 --> 00:01:15.660
was a time when the country
that is now Bangladesh

00:01:15.660 --> 00:01:17.250
was a part of Pakistan.

00:01:17.250 --> 00:01:20.430
It was called East Pakistan.

00:01:20.430 --> 00:01:23.060
There was a movement
for independence

00:01:23.060 --> 00:01:28.330
from what was then West Pakistan
and is now just Pakistan.

00:01:28.330 --> 00:01:29.750
And that movement
for independence

00:01:29.750 --> 00:01:34.820
was very brutally repressed
by the Pakistani army.

00:01:34.820 --> 00:01:39.170
As a result of that
repression, nine million people

00:01:39.170 --> 00:01:43.830
fled across the border from
East Pakistan to India.

00:01:43.830 --> 00:01:47.400
And this is just a
small, tiny segment

00:01:47.400 --> 00:01:49.540
of that mass of
humanity that was

00:01:49.540 --> 00:01:54.600
trying to escape the repression
and widespread starvation that

00:01:54.600 --> 00:01:57.880
also had occurred because of
the disruption of infrastructure

00:01:57.880 --> 00:02:00.430
because of that repression.

00:02:00.430 --> 00:02:02.320
I was living in
Oxford at the time.

00:02:02.320 --> 00:02:06.620
I was a graduate student at
the University of Oxford.

00:02:06.620 --> 00:02:11.930
And I was troubled by the fact
that despite this vast number

00:02:11.930 --> 00:02:16.410
of people in great need,
affluent nations were not

00:02:16.410 --> 00:02:17.794
doing very much to help.

00:02:17.794 --> 00:02:19.960
It wasn't that they didn't
know about the situation.

00:02:19.960 --> 00:02:22.110
It was well publicized.

00:02:22.110 --> 00:02:25.880
The Beatle, George Harrison--
or ex-Beatle, I guess, by then,

00:02:25.880 --> 00:02:30.370
perhaps-- put on a
concert for Bangladesh,

00:02:30.370 --> 00:02:32.590
and tried to raise
money for it, and did

00:02:32.590 --> 00:02:33.740
raise some money for it.

00:02:33.740 --> 00:02:35.490
And Oxfam and
other organizations

00:02:35.490 --> 00:02:36.850
were fundraising for it.

00:02:36.850 --> 00:02:39.680
But they raised
I think something

00:02:39.680 --> 00:02:43.490
like 20 or 30 million pounds.

00:02:43.490 --> 00:02:47.560
And the World
Health Organization

00:02:47.560 --> 00:02:51.930
was saying that something
like half a billion pounds

00:02:51.930 --> 00:02:56.630
was needed to feed and
provide sanitation and shelter

00:02:56.630 --> 00:02:59.160
for this very large
number of refugees.

00:02:59.160 --> 00:03:02.850
And India was a much poorer
country then than it is today.

00:03:02.850 --> 00:03:06.380
So it was not really going to be
able to cope with this burden.

00:03:06.380 --> 00:03:10.690
So I wanted to write
something about this.

00:03:10.690 --> 00:03:13.320
This was at a time when
philosophy, at least

00:03:13.320 --> 00:03:15.070
English speaking
philosophy, was just

00:03:15.070 --> 00:03:20.090
starting to return to what I see
as its roots and true nature,

00:03:20.090 --> 00:03:23.750
going right back to Athens, and
ancient Athens, and Socrates,

00:03:23.750 --> 00:03:27.190
of suggesting how
we ought to live.

00:03:27.190 --> 00:03:30.180
It was emerging
from a period where

00:03:30.180 --> 00:03:32.630
it was really analyzing
the meanings of moral terms

00:03:32.630 --> 00:03:35.630
in what was sometimes
ordinary language philosophy

00:03:35.630 --> 00:03:38.039
and I think of as a
phase that-- you know,

00:03:38.039 --> 00:03:39.330
it wasn't completely worthless.

00:03:39.330 --> 00:03:43.170
But it was less
interesting than actually

00:03:43.170 --> 00:03:45.137
trying to grapple with
these questions of how

00:03:45.137 --> 00:03:47.220
we ought to live that
traditionally philosophy has

00:03:47.220 --> 00:03:48.340
been about.

00:03:48.340 --> 00:03:51.760
So I wanted to write
something about this.

00:03:51.760 --> 00:03:54.070
And this seemed a
good example to ask

00:03:54.070 --> 00:03:57.950
what are our obligations
as people living

00:03:57.950 --> 00:04:00.100
in an affluent society--
pretty comfortable,

00:04:00.100 --> 00:04:03.390
pretty secure-- in
terms of helping

00:04:03.390 --> 00:04:05.282
in a situation like this?

00:04:05.282 --> 00:04:06.990
But I didn't want to
limit it either just

00:04:06.990 --> 00:04:09.460
to this particular crisis,
which obviously at some point

00:04:09.460 --> 00:04:13.440
was going to be solved
one way or the other--

00:04:13.440 --> 00:04:18.260
but generalizing to what we
ought to do to help people

00:04:18.260 --> 00:04:22.150
in extreme poverty, which
existed all over the world

00:04:22.150 --> 00:04:24.490
and which was also taking lives.

00:04:24.490 --> 00:04:27.540
So the argument
that I put forward

00:04:27.540 --> 00:04:29.490
was really a very simple one.

00:04:29.490 --> 00:04:32.310
And I'll just run you
quickly through the premises

00:04:32.310 --> 00:04:33.920
in the argument.

00:04:33.920 --> 00:04:37.600
So the first premise I think
is very difficult to deny,

00:04:37.600 --> 00:04:41.140
that suffering and death
from lack of food, shelter,

00:04:41.140 --> 00:04:42.580
and medical care is a bad thing.

00:04:42.580 --> 00:04:46.760
And that was what these nine
million people were being

00:04:46.760 --> 00:04:48.470
threatened with at the time.

00:04:48.470 --> 00:04:50.080
So there was a bad
thing happening.

00:04:53.150 --> 00:04:55.340
Second premise, somewhat
more controversial--

00:04:55.340 --> 00:04:57.600
and I'll come back to
this and say a little bit

00:04:57.600 --> 00:04:59.040
in defense of it.

00:04:59.040 --> 00:05:01.280
But I wanted to claim
that if it's in our power

00:05:01.280 --> 00:05:05.000
to prevent something bad
happening without sacrificing

00:05:05.000 --> 00:05:08.880
something of comparable
moral significance, then

00:05:08.880 --> 00:05:11.590
we ought to do that.

00:05:11.590 --> 00:05:13.090
You're probably
saying, well, what's

00:05:13.090 --> 00:05:15.230
of comparable
moral significance?

00:05:15.230 --> 00:05:18.210
But I wanted to
leave that as a kind

00:05:18.210 --> 00:05:21.700
of open expression for people
to put their own values in.

00:05:21.700 --> 00:05:24.830
I didn't want to make my
judgment, in this article

00:05:24.830 --> 00:05:28.320
anyway, as to what might be of
comparable moral significance

00:05:28.320 --> 00:05:32.710
to suffering and death from
lack of food and so on.

00:05:32.710 --> 00:05:38.720
I wanted people to ask
themselves-- so, you know,

00:05:38.720 --> 00:05:40.299
I could do something.

00:05:40.299 --> 00:05:42.090
We're getting obviously
to the question of,

00:05:42.090 --> 00:05:45.040
I could donate to
Oxfam's appeal.

00:05:45.040 --> 00:05:46.210
I could do something.

00:05:46.210 --> 00:05:48.490
What would I be
sacrificing if I were

00:05:48.490 --> 00:05:51.350
to make a substantial
donation to that appeal?

00:05:51.350 --> 00:05:53.800
Would it be of comparable
moral significance

00:05:53.800 --> 00:05:57.390
to the death and suffering
that it would prevent?

00:05:57.390 --> 00:06:00.684
And I thought that most people
living in affluent countries

00:06:00.684 --> 00:06:02.850
if they were honest with
themselves would say, well,

00:06:02.850 --> 00:06:05.920
I could give quite a lot before
I reached the point where I was

00:06:05.920 --> 00:06:08.550
sacrificing anything
that even in terms

00:06:08.550 --> 00:06:10.720
of my own values,
whatever they might be,

00:06:10.720 --> 00:06:12.980
would be of comparable
moral significance

00:06:12.980 --> 00:06:17.050
to what we would be preventing.

00:06:17.050 --> 00:06:20.440
So that's the second premise.

00:06:20.440 --> 00:06:23.910
And the third is a factual
claim that it is in our power

00:06:23.910 --> 00:06:28.670
to prevent suffering and death
without thereby sacrificing

00:06:28.670 --> 00:06:30.730
anything of comparable
moral significance.

00:06:30.730 --> 00:06:32.900
So that's obviously
a claim that needs

00:06:32.900 --> 00:06:36.690
to be defended as well in terms
of what the factual situation

00:06:36.690 --> 00:06:37.700
in the world is.

00:06:37.700 --> 00:06:40.210
And I'll come back to that.

00:06:40.210 --> 00:06:43.430
But from those
three premises, we

00:06:43.430 --> 00:06:45.470
can draw the conclusion
that we ought

00:06:45.470 --> 00:06:48.530
to do what would prevent the
suffering and death from lack

00:06:48.530 --> 00:06:54.012
of food where we can
and if indeed it's

00:06:54.012 --> 00:06:56.470
the case, that when that's in
our power, we ought to do it.

00:06:56.470 --> 00:06:59.000
So that's the really
simple argument.

00:06:59.000 --> 00:07:01.420
And I think one of the
reasons why the article has

00:07:01.420 --> 00:07:06.170
been very successful,
and is still widely known

00:07:06.170 --> 00:07:10.570
and discussed, and reprinted in
many anthologies and textbooks

00:07:10.570 --> 00:07:13.050
is because that
argument is so simple.

00:07:13.050 --> 00:07:16.290
It doesn't require a great
philosophical sophistication

00:07:16.290 --> 00:07:19.270
to spell out what
the argument is.

00:07:19.270 --> 00:07:22.210
Now some people see
that as a disadvantage.

00:07:22.210 --> 00:07:24.960
Particularly if I go and
lecture on this sort of topic

00:07:24.960 --> 00:07:28.160
in a place like France, they
all think, oh, this can't really

00:07:28.160 --> 00:07:28.880
be philosophy.

00:07:28.880 --> 00:07:29.900
I can understand that.

00:07:29.900 --> 00:07:31.000
[LAUGHTER]

00:07:31.000 --> 00:07:34.170
It's not profound enough.

00:07:34.170 --> 00:07:37.690
But I think we can get into
deeper questions if we want to.

00:07:37.690 --> 00:07:42.510
And we don't need to make the
language more complicated.

00:07:42.510 --> 00:07:47.280
OK let's, though, look at
the defense of the premises.

00:07:47.280 --> 00:07:50.800
And we'll start with
the second premise.

00:07:50.800 --> 00:07:52.380
So in defending
the second premise,

00:07:52.380 --> 00:07:55.370
I told a little
story-- and the story

00:07:55.370 --> 00:07:57.290
might be the other reason
that the article has

00:07:57.290 --> 00:08:00.030
been so widely read--
called "The Drowning

00:08:00.030 --> 00:08:01.390
Child in the Shallow Pond."

00:08:01.390 --> 00:08:03.060
I couldn't find,
despite everything

00:08:03.060 --> 00:08:04.860
that's on the internet-- you'd
think everything is there.

00:08:04.860 --> 00:08:07.380
I could not find a photo of
a child drowning in a pond.

00:08:07.380 --> 00:08:08.340
[LAUGHTER]

00:08:08.340 --> 00:08:11.990
But I found a photo of a rather
happy toddler playing in water.

00:08:11.990 --> 00:08:13.410
And that's going to have to do.

00:08:13.410 --> 00:08:14.880
[LAUGHTER]

00:08:15.720 --> 00:08:21.560
So the story is--
it's laid out here--

00:08:21.560 --> 00:08:24.110
you're walking across a park.

00:08:24.110 --> 00:08:27.240
And there's a shallow
pond in the park.

00:08:27.240 --> 00:08:28.850
You know that the
pond is shallow.

00:08:28.850 --> 00:08:31.534
You've been walking through
the park on summer days

00:08:31.534 --> 00:08:32.700
when kids have been playing.

00:08:32.700 --> 00:08:35.640
And you know that if there's
a teenager in the water,

00:08:35.640 --> 00:08:39.159
it's only up to his waist.

00:08:39.159 --> 00:08:40.530
But today, it's not summer.

00:08:40.530 --> 00:08:42.690
There's nobody else around.

00:08:42.690 --> 00:08:45.230
You wouldn't expect anyone
to be in the water at all.

00:08:45.230 --> 00:08:46.982
But you do see
something in the water.

00:08:46.982 --> 00:08:48.440
And when you look
more closely, you

00:08:48.440 --> 00:08:52.010
see it's a very small
child, a toddler.

00:08:52.010 --> 00:08:54.280
And although the
pond, is shallow it's

00:08:54.280 --> 00:08:55.670
too deep for this child.

00:08:55.670 --> 00:08:59.420
And this child is
apparently drowning,

00:08:59.420 --> 00:09:00.810
in danger of drowning.

00:09:00.810 --> 00:09:02.310
Your first response
probably would

00:09:02.310 --> 00:09:05.150
be to look around and say,
who's looking after this child?

00:09:05.150 --> 00:09:08.850
Where's the father, or
mother, or babysitter?

00:09:08.850 --> 00:09:10.880
Somebody has to be taking
care of this child.

00:09:10.880 --> 00:09:12.334
But you can't see anyone.

00:09:12.334 --> 00:09:13.750
You don't know how
it could happen

00:09:13.750 --> 00:09:16.750
that this small child could be
alone and fallen in the pond.

00:09:16.750 --> 00:09:19.240
But that apparently
is what's happened.

00:09:19.240 --> 00:09:20.840
And there's nobody else there.

00:09:20.840 --> 00:09:24.460
So it looks like the only way
to stop this child drowning

00:09:24.460 --> 00:09:28.190
is for you to run into the pond,
and quickly grab the child,

00:09:28.190 --> 00:09:29.570
and pull the child out.

00:09:29.570 --> 00:09:32.400
Not a dangerous
thing to do, but you

00:09:32.400 --> 00:09:35.330
realize there is some
cost to you involved.

00:09:35.330 --> 00:09:37.110
As bad luck would
have it, you've

00:09:37.110 --> 00:09:38.830
dressed in your most
expensive outfit,

00:09:38.830 --> 00:09:42.250
because you're going to meet
someone you want to impress.

00:09:42.250 --> 00:09:43.900
And it's going to get ruined.

00:09:43.900 --> 00:09:47.260
Your expensive shoes, suit,
whatever else it might be

00:09:47.260 --> 00:09:50.800
is going to get ruined by
wading into this muddy pond.

00:09:50.800 --> 00:09:52.420
It's going to be
inconvenient for you.

00:09:52.420 --> 00:09:55.780
You're going to have to go back
and dry off, call your friend

00:09:55.780 --> 00:09:57.780
and say you're going
to be late, whatever.

00:09:57.780 --> 00:10:02.980
And you're up for the expense
of replacing your nice clothing

00:10:02.980 --> 00:10:05.770
that you bought recently.

00:10:05.770 --> 00:10:11.140
Nevertheless, most people
would think if you said,

00:10:11.140 --> 00:10:14.150
yeah, well, I don't
want to damage my shoes.

00:10:14.150 --> 00:10:17.740
And after all, this
is not my child.

00:10:17.740 --> 00:10:20.410
And I'm not responsible
for this child.

00:10:20.410 --> 00:10:22.890
Nobody said, you
know, please look

00:10:22.890 --> 00:10:25.170
after this child or
anything like that.

00:10:25.170 --> 00:10:28.440
So why don't I just forget
about it and go on my way?

00:10:28.440 --> 00:10:30.790
If you said that,
most people I think

00:10:30.790 --> 00:10:33.950
would think that you'd
done something really bad,

00:10:33.950 --> 00:10:35.180
something really wrong.

00:10:38.090 --> 00:10:40.240
I thought of this as a
purely hypothetical example.

00:10:40.240 --> 00:10:43.630
But there are
cases of people who

00:10:43.630 --> 00:10:49.210
neglect to take simple steps
that will save a child's life.

00:10:49.210 --> 00:10:51.890
There was one that got a
lot of attention in China

00:10:51.890 --> 00:10:55.210
three years ago because
it was caught on video.

00:10:55.210 --> 00:10:57.940
This is a street in
the city of Foshan.

00:10:57.940 --> 00:11:02.860
And there is a child here
who has been previously hit

00:11:02.860 --> 00:11:06.570
by a van driving
down the street.

00:11:06.570 --> 00:11:09.717
The child's mother is
not aware that this

00:11:09.717 --> 00:11:10.800
has happened to her child.

00:11:10.800 --> 00:11:12.330
She's doing something else.

00:11:12.330 --> 00:11:14.120
And the video camera
captures a number

00:11:14.120 --> 00:11:18.650
of people walking down
the street, like this man,

00:11:18.650 --> 00:11:20.640
basically looking
away from the child.

00:11:20.640 --> 00:11:22.640
It's pretty hard to imagine
that this person who

00:11:22.640 --> 00:11:25.060
has walked from down there
has not noticed that there's

00:11:25.060 --> 00:11:26.570
a child lying in the street.

00:11:26.570 --> 00:11:29.830
But he's paying no
attention to the child.

00:11:29.830 --> 00:11:31.860
And over a period--
I can't remember

00:11:31.860 --> 00:11:34.170
exactly how long-- but
over a period of 10 minutes

00:11:34.170 --> 00:11:37.390
or so, something
like a dozen people

00:11:37.390 --> 00:11:40.660
walk down the street without
paying any attention.

00:11:40.660 --> 00:11:44.370
And tragically, a second car
hit and ran over the child

00:11:44.370 --> 00:11:46.550
while she was lying
on the street.

00:11:46.550 --> 00:11:49.450
After that, a woman who
was cleaning the street

00:11:49.450 --> 00:11:53.600
did notice the child
and sounded the alarm.

00:11:53.600 --> 00:11:55.040
The child was taken to hospital.

00:11:55.040 --> 00:11:58.900
But the injuries were too
severe, and the child died.

00:11:58.900 --> 00:12:04.480
That was then shown in China
on national news programs.

00:12:04.480 --> 00:12:08.490
And there was a huge outcry
that this was a terrible thing.

00:12:08.490 --> 00:12:10.400
What's happening to China?

00:12:10.400 --> 00:12:13.580
Don't we care about each other?

00:12:13.580 --> 00:12:16.200
And there was very
widespread condemnation,

00:12:16.200 --> 00:12:19.410
as you'd expect of the
people who had done nothing

00:12:19.410 --> 00:12:20.920
to help the child.

00:12:20.920 --> 00:12:24.500
So it's not just a
sort of local thing.

00:12:24.500 --> 00:12:27.460
I think that if
we think, yes, you

00:12:27.460 --> 00:12:30.240
ought to have helped the child
in the pond and people in China

00:12:30.240 --> 00:12:32.790
also I think you ought to
help someone in the street,

00:12:32.790 --> 00:12:34.540
it may not always happen.

00:12:34.540 --> 00:12:38.260
But the moral judgment
that I am looking for,

00:12:38.260 --> 00:12:42.994
that I am inviting you to make
is one ought not to do this.

00:12:42.994 --> 00:12:44.910
I hope you would be
thinking that you yourself

00:12:44.910 --> 00:12:47.000
if you were in this
situation would not do this,

00:12:47.000 --> 00:12:48.640
that you would help the child.

00:12:48.640 --> 00:12:51.760
You would think that the cost
of replacing your clothes

00:12:51.760 --> 00:12:54.880
would not be anything of
comparable moral significance

00:12:54.880 --> 00:12:56.410
to saving the child's life.

00:12:56.410 --> 00:12:58.890
And therefore, that's
something you ought to do.

00:12:58.890 --> 00:13:03.800
So if I do have your agreement
on that, your support on that,

00:13:03.800 --> 00:13:08.880
then I can use that
as a way of saying,

00:13:08.880 --> 00:13:11.070
at least in that
particular case,

00:13:11.070 --> 00:13:13.656
the child in the pond sort of
case or the child in the street

00:13:13.656 --> 00:13:18.290
here, you agree that
if it's in your power

00:13:18.290 --> 00:13:22.040
to prevent something really bad
happening without sacrificing

00:13:22.040 --> 00:13:23.870
something comparably
significant,

00:13:23.870 --> 00:13:26.420
you ought to do it.

00:13:26.420 --> 00:13:29.900
And that is an important
step in the argument.

00:13:29.900 --> 00:13:34.390
It shows that you're not
taking the view that you only

00:13:34.390 --> 00:13:37.200
have obligations if
you have in some way

00:13:37.200 --> 00:13:40.120
some special responsibility--
let's say you promise to look

00:13:40.120 --> 00:13:43.080
after the child, or the
child is your child,

00:13:43.080 --> 00:13:44.120
or something like that.

00:13:44.120 --> 00:13:46.040
If you think that
this would be wrong,

00:13:46.040 --> 00:13:49.910
you're actually saying
we do have obligations

00:13:49.910 --> 00:13:53.770
to help strangers even when
we haven't voluntarily taken

00:13:53.770 --> 00:13:55.920
on those kinds of obligations.

00:13:55.920 --> 00:14:00.720
And that's part of
the judgment that

00:14:00.720 --> 00:14:03.210
lies behind the
second premise that I

00:14:03.210 --> 00:14:07.510
want to get you to agree to.

00:14:07.510 --> 00:14:09.210
But of course, you
might say, well,

00:14:09.210 --> 00:14:11.620
I agree in the case of
the child in the pond

00:14:11.620 --> 00:14:14.840
or the child in
the street there.

00:14:14.840 --> 00:14:17.410
But the analogy, if
you're going to use--

00:14:17.410 --> 00:14:19.160
as I presume you've
probably already seen

00:14:19.160 --> 00:14:21.570
the strategy here-- if
you're going to use this

00:14:21.570 --> 00:14:24.700
as an analogy for saying
I ought to help strangers

00:14:24.700 --> 00:14:29.480
in Bangladesh, or refugees
from Bangladesh or in India,

00:14:29.480 --> 00:14:31.500
or for that matter
right now that you

00:14:31.500 --> 00:14:35.500
or to help people in
developing countries who

00:14:35.500 --> 00:14:40.200
are in need who you can
help, then that analogy--

00:14:40.200 --> 00:14:43.420
there's too many differences
between the two situations.

00:14:43.420 --> 00:14:47.480
And so I want to say a
little bit about that now.

00:14:47.480 --> 00:14:50.020
So the question is,
yes, there are obviously

00:14:50.020 --> 00:14:53.950
differences between the
situation, many differences.

00:14:53.950 --> 00:14:57.170
Are they morally
relevant differences

00:14:57.170 --> 00:15:00.350
between those situations?

00:15:00.350 --> 00:15:02.357
And there's such a
lot of differences

00:15:02.357 --> 00:15:04.440
that I'm not going to be
able to mention them all.

00:15:04.440 --> 00:15:06.630
But I am going to
mention some that I

00:15:06.630 --> 00:15:14.050
think are psychologically
relevant in that they would

00:15:14.050 --> 00:15:20.460
affect, perhaps, the likelihood
that people will help--

00:15:20.460 --> 00:15:24.750
differences between the child in
the pond and the global poverty

00:15:24.750 --> 00:15:27.020
situation today.

00:15:27.020 --> 00:15:29.010
And maybe they affect
the moral judgments

00:15:29.010 --> 00:15:32.280
that people will make about
whether we ought to help

00:15:32.280 --> 00:15:33.430
or not.

00:15:33.430 --> 00:15:36.880
So I'll just go fairly
quickly through these,

00:15:36.880 --> 00:15:41.460
because I know we have a
limited amount of time.

00:15:41.460 --> 00:15:46.404
So in the child in the pond,
there's an identifiable child.

00:15:46.404 --> 00:15:48.820
You don't know this child's
name or much about this child,

00:15:48.820 --> 00:15:50.819
but you can see it's that
child I'll be helping.

00:15:50.819 --> 00:15:53.820
It's one particular individual.

00:15:53.820 --> 00:15:57.010
In global poverty, you don't
know who you'll be helping.

00:15:57.010 --> 00:16:00.380
You may donate to the
Against Malaria Foundation,

00:16:00.380 --> 00:16:04.090
a highly effective charity that
distributes bed nets in areas

00:16:04.090 --> 00:16:06.430
where malaria kills children.

00:16:06.430 --> 00:16:10.080
And it's been very
well documented

00:16:10.080 --> 00:16:14.230
by the best possible methods,
by randomized controlled trials,

00:16:14.230 --> 00:16:17.780
that distributing
bed nets does reduce

00:16:17.780 --> 00:16:21.830
child mortality at modest cost.

00:16:21.830 --> 00:16:24.330
But of course, if you contribute
to the Against Malaria

00:16:24.330 --> 00:16:26.280
Foundation, you're
never going to know

00:16:26.280 --> 00:16:28.670
which child's life you saved.

00:16:28.670 --> 00:16:30.110
Because it's a counterfactual.

00:16:30.110 --> 00:16:33.040
It's, well, if this child hadn't
been sleeping under a bed net,

00:16:33.040 --> 00:16:35.780
he or she would have got
malaria and would have died.

00:16:35.780 --> 00:16:38.060
And if you distribute
enough bed nets,

00:16:38.060 --> 00:16:41.020
there will be such a child
that fills that description.

00:16:41.020 --> 00:16:42.890
But you'll never
know which one it is.

00:16:42.890 --> 00:16:45.450
And psychologically,
we're much readier to help

00:16:45.450 --> 00:16:50.631
an identifiable individual
than a statistical individual.

00:16:53.150 --> 00:16:55.340
The futility aspect
is another little sort

00:16:55.340 --> 00:16:58.490
of psychological trick
that we play on ourselves.

00:16:58.490 --> 00:17:00.530
When we think of the
child in the pond,

00:17:00.530 --> 00:17:02.670
we think I can save that child.

00:17:02.670 --> 00:17:05.420
And when I've saved that
child, I've solved the problem.

00:17:05.420 --> 00:17:09.280
There's nobody else needing
to be saved around there.

00:17:09.280 --> 00:17:11.040
But when we think
about global poverty,

00:17:11.040 --> 00:17:15.550
we often think, oh, but there
are-- the current World Bank

00:17:15.550 --> 00:17:19.089
figure is 700 million people
living in extreme poverty.

00:17:19.089 --> 00:17:21.609
There's no way I can
help all of them.

00:17:21.609 --> 00:17:23.490
In fact, the difference
that I could make

00:17:23.490 --> 00:17:26.369
is insignificant compared
to the size of the problem.

00:17:26.369 --> 00:17:30.400
It's a drop in the
ocean, we sometimes say.

00:17:30.400 --> 00:17:32.630
And that is a
discouraging factor that

00:17:32.630 --> 00:17:34.415
makes us less likely to do it.

00:17:34.415 --> 00:17:36.290
But if you think of it
from the point of view

00:17:36.290 --> 00:17:38.740
of the individual
you have helped,

00:17:38.740 --> 00:17:42.210
it's just as much a
benefit for that individual

00:17:42.210 --> 00:17:44.850
that you've saved their life, or
saved the life of their child,

00:17:44.850 --> 00:17:48.340
or prevented them going blind,
or reduced their suffering

00:17:48.340 --> 00:17:50.150
from a disease.

00:17:50.150 --> 00:17:53.630
It doesn't reduce the
value of that benefit

00:17:53.630 --> 00:17:58.110
that sadly, there are hundreds
of millions of other people

00:17:58.110 --> 00:18:00.080
who are still in that situation.

00:18:00.080 --> 00:18:03.380
I think it's still just
as important a benefit.

00:18:03.380 --> 00:18:05.850
The diffusion of
responsibility--

00:18:05.850 --> 00:18:08.020
also in the child in
the pond, I said you're

00:18:08.020 --> 00:18:09.910
the only person who can help.

00:18:09.910 --> 00:18:13.190
But clearly, that's not the
case with global poverty.

00:18:13.190 --> 00:18:15.390
There are, again,
hundreds of millions

00:18:15.390 --> 00:18:18.891
of people who can help, some
of whom are wealthier than you.

00:18:18.891 --> 00:18:20.640
And some of those
people who are wealthier

00:18:20.640 --> 00:18:24.860
than you are helping like Bill
and Melinda Gates, for example.

00:18:24.860 --> 00:18:26.560
Others who are a lot
wealthier than you

00:18:26.560 --> 00:18:28.300
are not helping at all.

00:18:28.300 --> 00:18:30.350
So you might say, well, why me?

00:18:30.350 --> 00:18:33.980
Why am I the one who should
do something about this?

00:18:33.980 --> 00:18:35.510
And again,
psychologically, there

00:18:35.510 --> 00:18:37.870
are all sorts of
experiments psychologists

00:18:37.870 --> 00:18:44.460
have done that show that we
are less likely to help others

00:18:44.460 --> 00:18:47.110
if we are one of a group and
we see that others in the group

00:18:47.110 --> 00:18:48.790
are not helping.

00:18:48.790 --> 00:18:52.770
It's something that, in a
sense, deters us from helping.

00:18:52.770 --> 00:18:55.730
But mostly, we think that that's
wrong, at least in retrospect

00:18:55.730 --> 00:18:56.710
when we're outside it.

00:18:56.710 --> 00:18:59.985
I mean, we think that
people should have helped.

00:18:59.985 --> 00:19:02.110
And the fact that they were
part of a society where

00:19:02.110 --> 00:19:06.410
people didn't help very much
doesn't really excuse them.

00:19:06.410 --> 00:19:09.360
If we think about people who
turned a blind eye to what

00:19:09.360 --> 00:19:11.110
was happening in
Nazi Germany, we

00:19:11.110 --> 00:19:14.130
don't think the fact that
they were just one of many

00:19:14.130 --> 00:19:16.960
is a sufficient excuse.

00:19:16.960 --> 00:19:20.269
And I think here too, we
should think that, well, I

00:19:20.269 --> 00:19:21.560
can still make some difference.

00:19:21.560 --> 00:19:24.290
Even if other people won't,
I can make some difference.

00:19:24.290 --> 00:19:27.740
And maybe if I and a few
others start helping,

00:19:27.740 --> 00:19:30.350
that will make it easier
for others to join in.

00:19:30.350 --> 00:19:32.690
We'll build up the critical
mass of people helping.

00:19:32.690 --> 00:19:35.810
And we'll actually counteract
this psychological effect

00:19:35.810 --> 00:19:39.440
of diffusion of responsibility.

00:19:39.440 --> 00:19:41.270
The child in the pond is near.

00:19:41.270 --> 00:19:44.670
And the people, the refugees
in India, were far away.

00:19:44.670 --> 00:19:46.460
And other people
in extreme poverty

00:19:46.460 --> 00:19:50.137
are far away from
where we are now.

00:19:50.137 --> 00:19:51.720
Most people when
they think about that

00:19:51.720 --> 00:19:54.640
are pretty clear about
that doesn't really make

00:19:54.640 --> 00:19:56.560
a difference to my obligations.

00:19:56.560 --> 00:19:58.950
If the distance makes
it harder for me

00:19:58.950 --> 00:20:02.840
to actually do anything, then
sure, that makes a difference.

00:20:02.840 --> 00:20:05.050
But that's relevant
to the other premise

00:20:05.050 --> 00:20:08.180
that I showed you, the
one about whether it's

00:20:08.180 --> 00:20:10.850
in our power to do something.

00:20:10.850 --> 00:20:12.640
If it's just
distance, I think we

00:20:12.640 --> 00:20:17.550
can see pretty clearly that
it's not that critical.

00:20:17.550 --> 00:20:19.220
You can see the
child for yourself

00:20:19.220 --> 00:20:20.300
and sum up the situation.

00:20:20.300 --> 00:20:22.810
You don't have to rely on
information from others.

00:20:22.810 --> 00:20:25.370
Psychologically, that
makes a difference too.

00:20:25.370 --> 00:20:28.880
But again, I would say,
what really matters

00:20:28.880 --> 00:20:31.380
is the quality of information.

00:20:31.380 --> 00:20:32.810
Are you getting
good information?

00:20:32.810 --> 00:20:34.470
Is it reliable information?

00:20:34.470 --> 00:20:37.750
Could somebody be
trying to scam you

00:20:37.750 --> 00:20:41.000
into sending them a
donation or sending

00:20:41.000 --> 00:20:42.690
a donation to an
organization that isn't

00:20:42.690 --> 00:20:44.710
a bonafide organization at all?

00:20:44.710 --> 00:20:47.580
All of those are very
relevant and proper concerns.

00:20:47.580 --> 00:20:50.180
But otherwise,
whether you actually

00:20:50.180 --> 00:20:54.500
see it with your own eyes or
whether you get information

00:20:54.500 --> 00:20:58.250
from a source that you believe
is fully reliable I don't think

00:20:58.250 --> 00:21:00.060
should make a difference.

00:21:00.060 --> 00:21:04.310
So although psychologically
these are disanalogies,

00:21:04.310 --> 00:21:08.750
I want to argue that morally,
they're not really relevant.

00:21:08.750 --> 00:21:11.480
And this is my explanation
of what's going on,

00:21:11.480 --> 00:21:15.420
which owes something to
the Harvard psychologist

00:21:15.420 --> 00:21:19.600
Joshua Greene who has a book
called "Moral Tribes," which

00:21:19.600 --> 00:21:21.790
is about moral psychology
and its implications

00:21:21.790 --> 00:21:25.100
in this kind of area,
that I highly recommend.

00:21:25.100 --> 00:21:28.860
So why do we have
these responses

00:21:28.860 --> 00:21:31.090
that I just described?

00:21:31.090 --> 00:21:36.400
We have them because we evolved
in small face-to-face societies

00:21:36.400 --> 00:21:39.520
where basically we knew
the people we could help.

00:21:39.520 --> 00:21:41.540
They were identifiable
individuals.

00:21:41.540 --> 00:21:44.550
And they were part of our group.

00:21:44.550 --> 00:21:47.750
And to that extent,
some of these responses

00:21:47.750 --> 00:21:49.150
are hardwired into us.

00:21:49.150 --> 00:21:51.360
They're part of our biology.

00:21:51.360 --> 00:21:55.110
But the world has changed
in the last century or two

00:21:55.110 --> 00:21:57.090
very dramatically.

00:21:57.090 --> 00:21:59.750
It's changed in the sense
that we're living now

00:21:59.750 --> 00:22:02.060
in a much bigger community.

00:22:02.060 --> 00:22:04.450
And we have the
ability to know what's

00:22:04.450 --> 00:22:08.740
going on far away from us,
which we never had before.

00:22:08.740 --> 00:22:12.200
And we have the ability
to actually help and make

00:22:12.200 --> 00:22:14.380
a difference, not
quite as instantly

00:22:14.380 --> 00:22:16.630
as we have the ability
to know what's happening,

00:22:16.630 --> 00:22:20.900
but quickly enough.

00:22:20.900 --> 00:22:24.940
Of course, evolution
doesn't work that fast.

00:22:24.940 --> 00:22:26.620
The biology hasn't changed.

00:22:26.620 --> 00:22:28.950
We still have the
innate responses

00:22:28.950 --> 00:22:33.710
that are more suited to the many
millennia in which we lived,

00:22:33.710 --> 00:22:37.030
or millions of years, going
back to our pre-human ancestors

00:22:37.030 --> 00:22:40.110
even, in which we've lived
in small social groups.

00:22:40.110 --> 00:22:44.950
So that's why we
have these notions.

00:22:44.950 --> 00:22:48.180
But now we really need to go
beyond them-- not that I'm

00:22:48.180 --> 00:22:50.500
saying we shouldn't have
emotions in this area,

00:22:50.500 --> 00:22:54.370
but we need to use our
reason and our ability

00:22:54.370 --> 00:22:58.050
to reflect in order
to go beyond them

00:22:58.050 --> 00:23:02.980
and think about what we ought
to do in a different way.

00:23:02.980 --> 00:23:06.660
So the psychological
differences I'm saying

00:23:06.660 --> 00:23:08.810
are not always morally relevant.

00:23:08.810 --> 00:23:11.660
And I've just said-- what's
the second part of that slide,

00:23:11.660 --> 00:23:14.720
so I needn't repeat that.

00:23:14.720 --> 00:23:17.041
Briefly, I want to
make sure that you

00:23:17.041 --> 00:23:18.040
have time for questions.

00:23:18.040 --> 00:23:19.970
So I don't want
to go on too long.

00:23:19.970 --> 00:23:22.870
I'll just briefly run
through the factual claim

00:23:22.870 --> 00:23:25.380
I made that it's in our
power to do something--

00:23:25.380 --> 00:23:26.730
has been challenged.

00:23:26.730 --> 00:23:28.540
Some of you may have
read some critiques

00:23:28.540 --> 00:23:32.310
of aid-- Bill Easterly's book,
"The White Man's Burden,"

00:23:32.310 --> 00:23:34.760
Dambisa Moyo's "Dead Aid."

00:23:34.760 --> 00:23:37.110
And my Princeton
colleague Angus Deaton

00:23:37.110 --> 00:23:41.060
who this year got the
Nobel Prize for Economics

00:23:41.060 --> 00:23:45.010
also has some criticisms of
aid in his excellent book,

00:23:45.010 --> 00:23:47.890
"The Great Escape."

00:23:47.890 --> 00:23:51.870
But I think the more
sweeping critiques that

00:23:51.870 --> 00:23:54.459
come from Easterly
and Moyo are not

00:23:54.459 --> 00:23:56.000
applicable to what
I'm talking about.

00:23:56.000 --> 00:23:59.830
They're directed at government
aid, multilateral aid,

00:23:59.830 --> 00:24:07.070
not at the NGOs that I would
recommend you give your aid to.

00:24:07.070 --> 00:24:11.440
Very few of us say we want to
give money to the government

00:24:11.440 --> 00:24:13.160
so we can increase its aid.

00:24:13.160 --> 00:24:14.920
Even as far as
government's concerned,

00:24:14.920 --> 00:24:19.140
I think Easterly and Moyo are a
little unfair, especially where

00:24:19.140 --> 00:24:22.040
you have governments that have
been reasonably thoughtful,

00:24:22.040 --> 00:24:26.650
as Difford has in this country,
in terms of overcoming some

00:24:26.650 --> 00:24:30.330
of the objections that certainly
have existed in the past

00:24:30.330 --> 00:24:31.620
to aid.

00:24:31.620 --> 00:24:36.037
And as far as Deatons critique,
which is a more nuanced one,

00:24:36.037 --> 00:24:38.370
Deaton acknowledges that aid,
particularly in the health

00:24:38.370 --> 00:24:42.160
area, has saved
millions of lives.

00:24:42.160 --> 00:24:44.980
And I think there's
no doubt about that.

00:24:44.980 --> 00:24:47.410
You look at the figures--
child deaths have

00:24:47.410 --> 00:24:51.170
come down very dramatically
in the last 50 years

00:24:51.170 --> 00:24:57.740
or more from 20 million in 1960
to under six million today.

00:24:57.740 --> 00:25:01.030
So we had less than a third
of the number of children

00:25:01.030 --> 00:25:03.390
who die before their
fifth birthday as

00:25:03.390 --> 00:25:05.600
compared to 50 years
ago despite the fact

00:25:05.600 --> 00:25:08.080
that the world's population
has more than doubled.

00:25:08.080 --> 00:25:12.780
So effectively, the death
rate for children under five

00:25:12.780 --> 00:25:16.420
is less than one
sixth of what it was.

00:25:16.420 --> 00:25:18.634
That's very good news.

00:25:18.634 --> 00:25:20.300
Aid can't claim all
the credit for that.

00:25:20.300 --> 00:25:22.520
Obviously, economic
development in countries

00:25:22.520 --> 00:25:26.210
like China in particular has
made a huge difference here.

00:25:26.210 --> 00:25:29.160
But I think it's clear that
aid has also made a difference.

00:25:29.160 --> 00:25:33.170
And there's data on that
which I could go into.

00:25:33.170 --> 00:25:36.390
But I think that the data is
pretty clear in some cases

00:25:36.390 --> 00:25:40.210
that aid programs have made
an important difference

00:25:40.210 --> 00:25:42.760
in reducing child
mortality, also

00:25:42.760 --> 00:25:45.270
in doing other things
like reducing incidence

00:25:45.270 --> 00:25:49.150
of preventable
blindness from trachoma,

00:25:49.150 --> 00:25:52.130
and dealing with a whole
range of other conditions

00:25:52.130 --> 00:25:53.830
that cause a lot of
suffering, providing

00:25:53.830 --> 00:25:56.970
more education, particularly
education for girls,

00:25:56.970 --> 00:25:59.780
providing information
about family

00:25:59.780 --> 00:26:02.280
planning, a whole lot
of different things

00:26:02.280 --> 00:26:03.420
that aid has done.

00:26:03.420 --> 00:26:07.770
So I think the factual
premise is justified.

00:26:07.770 --> 00:26:09.670
We do have it in our
power to do things

00:26:09.670 --> 00:26:13.950
as long as we choose carefully
and thoughtfully about what

00:26:13.950 --> 00:26:17.550
we're doing with our resources.

00:26:17.550 --> 00:26:20.730
So the only thing that
I do want to correct

00:26:20.730 --> 00:26:24.247
here, having said that, is
in the original article,

00:26:24.247 --> 00:26:26.580
I said, like, for the cost
of your ruined pair of shoes,

00:26:26.580 --> 00:26:27.620
you could save a life.

00:26:27.620 --> 00:26:29.190
Well, you're going
to have to really

00:26:29.190 --> 00:26:31.815
be very much at the top
end of the shoe market--

00:26:31.815 --> 00:26:32.810
[LAUGHTER]

00:26:32.810 --> 00:26:35.030
--for that to be true on
the more recent research.

00:26:35.030 --> 00:26:37.750
Because whereas I
thought earlier maybe

00:26:37.750 --> 00:26:41.060
there was some suggestions that
for a couple of hundred dollars

00:26:41.060 --> 00:26:45.340
you could save a child's
life, more recent research

00:26:45.340 --> 00:26:47.710
suggests that it's
significantly more than that.

00:26:47.710 --> 00:26:51.840
It might be in
$1,000 or two range.

00:26:51.840 --> 00:26:55.714
But it's still, I think, not
something that's-- most of us

00:26:55.714 --> 00:26:57.630
would not have to give
up something comparably

00:26:57.630 --> 00:27:01.730
significant in order
to achieve that.

00:27:01.730 --> 00:27:05.010
OK, aren't I giving
through my taxes?

00:27:05.010 --> 00:27:08.597
Well, yes, if you
pay taxes in the UK,

00:27:08.597 --> 00:27:10.180
you are giving to
one of the countries

00:27:10.180 --> 00:27:13.470
that now, thanks to a bipartisan
pledge that's actually

00:27:13.470 --> 00:27:17.660
being fulfilled, is among the
better nations in the world.

00:27:17.660 --> 00:27:20.500
And that's really
good, much better

00:27:20.500 --> 00:27:23.200
than where I spend part
of each year, Australia,

00:27:23.200 --> 00:27:24.700
and better still
than where I spend

00:27:24.700 --> 00:27:25.970
the other part of each year.

00:27:25.970 --> 00:27:29.300
The US is really pretty
miserable on this scale.

00:27:29.300 --> 00:27:35.640
But still, this line is 0.7%
of gross national income,

00:27:35.640 --> 00:27:36.660
so not very much.

00:27:36.660 --> 00:27:40.670
70 pence in every 100 pounds the
nation earns is not very much.

00:27:40.670 --> 00:27:43.450
I think we could do a
lot better than that.

00:27:46.510 --> 00:27:49.550
So this is sort of
the big, big question

00:27:49.550 --> 00:27:52.170
about the whole thing is,
well, how much should we give?

00:27:52.170 --> 00:27:54.830
What is this level of
comparable moral significance?

00:27:54.830 --> 00:27:56.710
Doesn't it make
life very demanding

00:27:56.710 --> 00:27:59.160
to go all the way there?

00:27:59.160 --> 00:28:01.810
And when the article,
the "Famine, Affluence,

00:28:01.810 --> 00:28:05.280
and Morality" article,
first came out,

00:28:05.280 --> 00:28:08.330
it was used, as I said,
in a lot of classrooms.

00:28:08.330 --> 00:28:10.280
Professors gave it to
their students to read,

00:28:10.280 --> 00:28:12.162
because they could read it.

00:28:12.162 --> 00:28:13.620
And they could be
challenged by it.

00:28:13.620 --> 00:28:16.690
But I've heard from
a number of people

00:28:16.690 --> 00:28:18.960
that the-- the way
in which it was used

00:28:18.960 --> 00:28:21.520
was basically, look, here's
a rather plausible argument.

00:28:21.520 --> 00:28:24.490
The premises seem like
they're plausible.

00:28:24.490 --> 00:28:26.100
Certainly, the
conclusion follows

00:28:26.100 --> 00:28:27.340
if you accept the premise.

00:28:27.340 --> 00:28:29.900
But the conclusion
is so demanding,

00:28:29.900 --> 00:28:31.780
it must be wrong, right?

00:28:31.780 --> 00:28:36.740
So your job is to show where
the argument goes wrong.

00:28:36.740 --> 00:28:39.460
And that was something that to
work quite well as a teaching

00:28:39.460 --> 00:28:40.930
tool.

00:28:40.930 --> 00:28:42.931
But one of the
interesting things that's

00:28:42.931 --> 00:28:45.180
happened more recently, and
perhaps one of the reasons

00:28:45.180 --> 00:28:48.530
why OUP thought it would be good
to have the article out there

00:28:48.530 --> 00:28:53.210
again, is that, in
fact, more people now

00:28:53.210 --> 00:28:55.270
are seeing this
not as an objection

00:28:55.270 --> 00:28:59.060
but as a reason for
doing something about it.

00:28:59.060 --> 00:29:01.580
And that relates
to what's now known

00:29:01.580 --> 00:29:05.950
as effective altruism,
a new movement,

00:29:05.950 --> 00:29:07.790
certainly within the
last 10 years, that

00:29:07.790 --> 00:29:10.810
is trying to publicize
the idea of living

00:29:10.810 --> 00:29:14.040
as if that argument
actually mattered

00:29:14.040 --> 00:29:16.250
and as if we could do
something about it.

00:29:16.250 --> 00:29:18.770
So here's one of the
founders of this movement.

00:29:18.770 --> 00:29:22.010
It's sort of not a
single organization

00:29:22.010 --> 00:29:23.070
that's got it going.

00:29:23.070 --> 00:29:24.695
There are a number
of different people,

00:29:24.695 --> 00:29:26.260
different organizations.

00:29:26.260 --> 00:29:29.120
One of them is Toby Ord,
who is now a research

00:29:29.120 --> 00:29:32.830
fellow in philosophy at Oxford.

00:29:32.830 --> 00:29:34.780
He read the article
as a student.

00:29:34.780 --> 00:29:39.760
He decided that he wanted
to live so as to make

00:29:39.760 --> 00:29:41.150
the world a better place.

00:29:41.150 --> 00:29:44.940
So he thought, well, I'm
doing OK as a student.

00:29:44.940 --> 00:29:47.110
I'm on this graduate
studentship.

00:29:47.110 --> 00:29:50.990
I think it was around 16,000
pounds or maybe 14,000 pounds.

00:29:50.990 --> 00:29:53.260
He said, yeah, I could maybe
have a little bit more.

00:29:53.260 --> 00:29:55.740
But, you know, I'm not
really missing anything.

00:29:55.740 --> 00:29:58.020
And I'm heading for
an academic career.

00:29:58.020 --> 00:30:00.930
After I get my
DPhil, I'm probably

00:30:00.930 --> 00:30:02.410
going to get an academic job.

00:30:02.410 --> 00:30:04.570
I'll be earning a lot
more than my studentship.

00:30:04.570 --> 00:30:06.350
Suppose that I just
continue to live

00:30:06.350 --> 00:30:08.780
on this studentship,
or inflation

00:30:08.780 --> 00:30:10.360
adjusted amount of
this studentship,

00:30:10.360 --> 00:30:11.880
or a little bit more.

00:30:11.880 --> 00:30:16.047
So he's pledged to live on that
amount adjusted for inflation--

00:30:16.047 --> 00:30:17.880
it's probably a bit
higher than that by now,

00:30:17.880 --> 00:30:22.390
because he made that
pledge a few years ago--

00:30:22.390 --> 00:30:24.590
and to give away the rest.

00:30:24.590 --> 00:30:28.400
And he worked out what
he might do with that.

00:30:28.400 --> 00:30:31.770
He picked, as a highly
effective thing to do,

00:30:31.770 --> 00:30:36.990
preventing blindness from
trachoma, the largest

00:30:36.990 --> 00:30:38.990
cause of preventable
blindness in the world that

00:30:38.990 --> 00:30:41.710
affects people in
developing countries--

00:30:41.710 --> 00:30:45.070
pretty cheap to prevent.

00:30:45.070 --> 00:30:48.130
He, because he likes
doing maths, and sums,

00:30:48.130 --> 00:30:50.330
and so on, he
decided to work out

00:30:50.330 --> 00:30:52.330
how much he would
be able to give away

00:30:52.330 --> 00:30:55.270
if he had a normal academic
trajectory at the kind

00:30:55.270 --> 00:30:58.820
of salaries that academics are
likely to have until retirement

00:30:58.820 --> 00:31:02.510
and continued to
live on this amount.

00:31:02.510 --> 00:31:04.800
And so he got this
large sum of money,

00:31:04.800 --> 00:31:07.070
notional a large sum
of money, divided it

00:31:07.070 --> 00:31:09.600
by the cost of preventing
blindness from trachoma,

00:31:09.600 --> 00:31:14.090
and ended up with a figure
of 80,000-- 80,000 cases

00:31:14.090 --> 00:31:17.800
of blindness that he alone,
not a very rich person,

00:31:17.800 --> 00:31:20.120
no Bill Gates or Warren
Buffet-- he alone

00:31:20.120 --> 00:31:22.610
would be able to prevent.

00:31:22.610 --> 00:31:24.794
And he thought that
was very impressive.

00:31:24.794 --> 00:31:26.710
He was sort of thrilled
to think that he could

00:31:26.710 --> 00:31:28.870
do that much good in
the world-- and decided

00:31:28.870 --> 00:31:30.000
to tell people about it.

00:31:30.000 --> 00:31:33.600
So he founded this organization,
Giving What We Can,

00:31:33.600 --> 00:31:36.750
encouraged people not to take
a pledge as tough as the one he

00:31:36.750 --> 00:31:39.750
did, but to pledge to
give 10% of their income

00:31:39.750 --> 00:31:40.870
to effective charities.

00:31:43.977 --> 00:31:46.310
Here's somebody who came to
this completely independent,

00:31:46.310 --> 00:31:49.000
Julia Wise, a woman living
in Boston who sort of thought

00:31:49.000 --> 00:31:55.250
that learning how much better
she is than other people wanted

00:31:55.250 --> 00:31:58.730
to give quite a lot to help
people and could live on less--

00:31:58.730 --> 00:32:00.930
persuaded her then
boyfriend now husband

00:32:00.930 --> 00:32:02.759
to join her in doing this.

00:32:02.759 --> 00:32:04.550
And incidentally, he
worked-- Jeff Kauffman

00:32:04.550 --> 00:32:07.420
works for Google in Boston.

00:32:07.420 --> 00:32:10.610
But even before he got
this job at Google,

00:32:10.610 --> 00:32:12.910
they were already donating
a substantial amount

00:32:12.910 --> 00:32:15.510
of their income.

00:32:15.510 --> 00:32:20.070
They were I think donating
something like 30% of it

00:32:20.070 --> 00:32:24.800
even when their total income
was no more than around $50,000

00:32:24.800 --> 00:32:26.280
a year.

00:32:26.280 --> 00:32:27.860
Now that Jeff has
a nice Google job,

00:32:27.860 --> 00:32:32.230
they've upped this to
50% of their income.

00:32:32.230 --> 00:32:35.640
And if you want to read about
how Julia feels about it,

00:32:35.640 --> 00:32:39.330
she writes this engaging
kind of personal blog

00:32:39.330 --> 00:32:41.292
at givinggladly.com.

00:32:41.292 --> 00:32:45.020
And you can see what she
does, and how she lives,

00:32:45.020 --> 00:32:49.570
and why she thinks this is
important by looking at that.

00:32:49.570 --> 00:32:52.000
And this is probably
the most impressive sort

00:32:52.000 --> 00:32:56.960
of a case of the influence of
philosophy on direct behavior.

00:32:56.960 --> 00:32:59.560
This is not somebody
that I've ever met.

00:32:59.560 --> 00:33:03.447
But I got an email out of the
blue a few years ago telling me

00:33:03.447 --> 00:33:05.530
that as a result of
discussing "Famine, Affluence,

00:33:05.530 --> 00:33:08.470
and Morality" in class,
the sort of discussion

00:33:08.470 --> 00:33:10.560
went, well, you know,
there's this argument.

00:33:10.560 --> 00:33:12.405
And it leads to saying
you should give away

00:33:12.405 --> 00:33:13.030
a lot of money.

00:33:13.030 --> 00:33:15.390
And then somebody
else they read said,

00:33:15.390 --> 00:33:17.550
well, you know, if that
argument were true,

00:33:17.550 --> 00:33:19.525
it wouldn't just
apply to giving money.

00:33:19.525 --> 00:33:20.900
You could help
people for example

00:33:20.900 --> 00:33:22.860
by donating a kidney
to a stranger.

00:33:22.860 --> 00:33:25.680
And again, this was seen
as a reductio ad absurdum

00:33:25.680 --> 00:33:29.330
that morality couldn't be
that demanding that that's

00:33:29.330 --> 00:33:30.350
what we ought to do.

00:33:30.350 --> 00:33:31.767
But Chris, after
thinking about it

00:33:31.767 --> 00:33:33.308
and discussing it
with other people--

00:33:33.308 --> 00:33:35.280
he didn't rush into
this-- took some months

00:33:35.280 --> 00:33:38.550
to reach this decision-- decided
that was what he wanted to do.

00:33:38.550 --> 00:33:42.740
And you see him here
just after doing that.

00:33:42.740 --> 00:33:44.600
I've continued to be
in touch with him.

00:33:44.600 --> 00:33:47.000
And he's very happy
about what he's done.

00:33:47.000 --> 00:33:48.584
He's in good health.

00:33:48.584 --> 00:33:50.250
He's been in contact
with the person who

00:33:50.250 --> 00:33:52.730
received the kidney who
was a man in his 40s

00:33:52.730 --> 00:33:56.260
who was a schoolteacher teaching
in an underprivileged school

00:33:56.260 --> 00:33:58.770
in St. Louis, so he feels
really good about that.

00:33:58.770 --> 00:34:01.020
Of course, you never know
who you're going to give to.

00:34:01.020 --> 00:34:03.580
It could've been someone who
was a conservative Republican

00:34:03.580 --> 00:34:04.080
or whatever.

00:34:04.080 --> 00:34:04.320
[LAUGHTER]

00:34:04.320 --> 00:34:05.910
Chris might have
felt less happy.

00:34:05.910 --> 00:34:08.781
But that's-- it turned
out well for him, anyway.

00:34:08.781 --> 00:34:10.134
[LAUGHTER]

00:34:12.389 --> 00:34:16.184
So there are a few
people who do this.

00:34:16.184 --> 00:34:18.350
I think it's probably, for
most people including me,

00:34:18.350 --> 00:34:20.909
it may be a step too far.

00:34:20.909 --> 00:34:24.300
But it's an example of
the way that philosophy

00:34:24.300 --> 00:34:26.610
can make a difference.

00:34:26.610 --> 00:34:28.239
OK, I'm nearly done.

00:34:28.239 --> 00:34:30.230
So this is this movement,
effective altruism.

00:34:30.230 --> 00:34:31.965
You can look it up
on Wikipedia now.

00:34:31.965 --> 00:34:33.949
So it's pretty new.

00:34:33.949 --> 00:34:37.630
It wasn't there a few years
ago-- stresses the idea

00:34:37.630 --> 00:34:40.400
about applying
evidence and reason.

00:34:40.400 --> 00:34:42.440
And here's one
way of doing this.

00:34:42.440 --> 00:34:44.659
An organization-- this is
American rather than UK--

00:34:44.659 --> 00:34:46.600
but an organization
called GiveWell

00:34:46.600 --> 00:34:50.120
that reviews charities,
finds ones for which there's

00:34:50.120 --> 00:34:52.100
really clear evidence,
and recommends them--

00:34:52.100 --> 00:34:54.232
so just this thin slice.

00:34:54.232 --> 00:34:56.440
That doesn't mean that all
these other charities that

00:34:56.440 --> 00:34:59.832
have been reviewed are
actually not effective.

00:34:59.832 --> 00:35:01.290
What it means is
they have not been

00:35:01.290 --> 00:35:04.350
able to produce good
enough evidence to satisfy

00:35:04.350 --> 00:35:06.830
the rigorous assessment
that GiveWell

00:35:06.830 --> 00:35:08.880
does that they are effective.

00:35:08.880 --> 00:35:10.600
And that's a very
different question.

00:35:10.600 --> 00:35:14.510
But GiveWell is kind of driving
the movement to get more data,

00:35:14.510 --> 00:35:17.330
to get independent studies,
to get good analysis so

00:35:17.330 --> 00:35:20.800
that we can know which are the
really effective charities.

00:35:20.800 --> 00:35:24.080
And that's certainly helped
the whole effective altruism

00:35:24.080 --> 00:35:25.220
movement.

00:35:25.220 --> 00:35:28.000
Here again is the one
that Toby Ord started.

00:35:28.000 --> 00:35:31.570
And if you go to their
Where to Give tab,

00:35:31.570 --> 00:35:33.560
they also recommend
charities more

00:35:33.560 --> 00:35:36.010
suited for purposes
for the UK, if you're

00:35:36.010 --> 00:35:40.570
interested in tax deductibility
for donations in the UK.

00:35:40.570 --> 00:35:42.530
And there's one
that I've involved

00:35:42.530 --> 00:35:45.480
with that started as the title
of a earlier book of mine,

00:35:45.480 --> 00:35:49.411
"The Life You Can Save"--
somewhat more global

00:35:49.411 --> 00:35:50.910
in the charities
that it recommends,

00:35:50.910 --> 00:35:54.080
slightly less rigorous in the
evidence required than they

00:35:54.080 --> 00:35:57.830
GiveWell, because we wanted a
broader group of organizations.

00:35:57.830 --> 00:36:02.190
But if you're interested, have
a look at any of those websites.

00:36:02.190 --> 00:36:04.320
And at that, I'm going
to stop so that we still

00:36:04.320 --> 00:36:05.622
have some time for questions.

00:36:05.622 --> 00:36:06.330
Thanks very much.

00:36:06.330 --> 00:36:08.584
[APPLAUSE]

00:36:14.172 --> 00:36:14.880
SPEAKER 1: Great.

00:36:14.880 --> 00:36:16.570
So we have two microphones.

00:36:16.570 --> 00:36:18.410
We've got Jan with a
microphone over there.

00:36:18.410 --> 00:36:19.130
I've got one.

00:36:19.130 --> 00:36:22.550
I'm going to start with
a question, if I may.

00:36:22.550 --> 00:36:24.400
I love the link
between this book

00:36:24.400 --> 00:36:26.440
and the ideas around
effective altruism.

00:36:26.440 --> 00:36:28.310
I wonder though about
what your thoughts

00:36:28.310 --> 00:36:32.290
on the immediacy of social media
and the immediacy of what we're

00:36:32.290 --> 00:36:35.946
able to do now in order to bring
us closer to things happening

00:36:35.946 --> 00:36:38.570
on the other side of the globe--
does that change our behavior?

00:36:38.570 --> 00:36:42.040
Or do we suffer from
information overload?

00:36:42.040 --> 00:36:45.206
PETER SINGER: I'm hoping that
we'll change our behavior.

00:36:45.206 --> 00:36:47.080
It probably already is
changing our behavior,

00:36:47.080 --> 00:36:50.404
but I haven't seen good
enough data on that.

00:36:50.404 --> 00:36:52.070
Obviously, you know,
all of these things

00:36:52.070 --> 00:36:54.920
have positives and negatives.

00:36:54.920 --> 00:36:56.350
I would like sort
of the positive

00:36:56.350 --> 00:36:58.280
that we feel more closely
connected to people

00:36:58.280 --> 00:37:00.190
on the other side of the world.

00:37:00.190 --> 00:37:04.690
So I like the idea of, you know,
Mark Zuckerberg's internet.org,

00:37:04.690 --> 00:37:06.670
that everybody in
the world eventually

00:37:06.670 --> 00:37:08.260
is going to be on the net.

00:37:08.260 --> 00:37:10.680
And we can communicate
with them in some way.

00:37:10.680 --> 00:37:13.640
I think it will be
excellent if that spreads.

00:37:13.640 --> 00:37:17.960
Sometimes the social
media get people

00:37:17.960 --> 00:37:20.520
to focus on a particular
thing that goes viral.

00:37:20.520 --> 00:37:24.210
And it may not be the
most effective thing.

00:37:24.210 --> 00:37:28.020
We had the ice bucket
challenge a year or so ago.

00:37:28.020 --> 00:37:30.720
You know, that was
fine, but it was

00:37:30.720 --> 00:37:35.270
dealing with a fairly rare
disease in affluent countries.

00:37:35.270 --> 00:37:38.430
If we want to use our
resources most effectively

00:37:38.430 --> 00:37:41.130
to help people
suffering from diseases,

00:37:41.130 --> 00:37:44.210
there are others affecting
people in developing countries

00:37:44.210 --> 00:37:47.660
where it would be much more
cost effective to donate.

00:37:47.660 --> 00:37:49.810
SPEAKER 1: Perfect.

00:37:49.810 --> 00:37:51.820
AUDIENCE: So there's
two elements of this.

00:37:51.820 --> 00:37:52.320
Is this on?

00:37:52.320 --> 00:37:52.810
Yeah.

00:37:52.810 --> 00:37:53.190
PETER SINGER: Yeah, yeah.

00:37:53.190 --> 00:37:55.648
AUDIENCE: And one of them is
getting people to donate more.

00:37:55.648 --> 00:37:58.390
And the other one is donate
to effective charities.

00:37:58.390 --> 00:38:02.160
For example, Against Malaria
Foundation from what I know

00:38:02.160 --> 00:38:05.540
is about 100 times more
effective than the ALS ice

00:38:05.540 --> 00:38:07.820
bucket challenge per dollar.

00:38:07.820 --> 00:38:10.260
Which of those areas do
you find it is easier

00:38:10.260 --> 00:38:15.350
to convince people to
make an improvement?

00:38:15.350 --> 00:38:17.630
And what sort of
strategies have you learned

00:38:17.630 --> 00:38:19.540
from writing your
books and so on

00:38:19.540 --> 00:38:21.310
and how you have
managed to convince

00:38:21.310 --> 00:38:23.291
those people to do that?

00:38:23.291 --> 00:38:25.082
PETER SINGER: Yeah,
that's a good question.

00:38:28.380 --> 00:38:31.800
I don't really have good
data to say which of them

00:38:31.800 --> 00:38:33.040
have I found more successful.

00:38:33.040 --> 00:38:34.860
I've certainly--
I do know people

00:38:34.860 --> 00:38:38.990
who've started giving because
of arguments that I and others

00:38:38.990 --> 00:38:41.630
in the effective altruism
movement have put forward.

00:38:41.630 --> 00:38:43.840
I know quite a lot
of people who've

00:38:43.840 --> 00:38:46.566
taken that up and perhaps
in some way responded

00:38:46.566 --> 00:38:48.940
to that argument-- maybe
thought they should do something

00:38:48.940 --> 00:38:51.570
before, but didn't get
around to doing it.

00:38:51.570 --> 00:38:55.520
So that's certainly possible.

00:38:55.520 --> 00:38:58.667
Probably though, you
typically get more resistance

00:38:58.667 --> 00:39:00.500
when you tell people
they ought to be giving

00:39:00.500 --> 00:39:03.110
or they ought to be giving
more than if you tell them

00:39:03.110 --> 00:39:06.440
they ought to be giving more
effectively, at least the kind

00:39:06.440 --> 00:39:07.900
of audiences that I talk about.

00:39:07.900 --> 00:39:10.550
But I do get pushback
from that as well.

00:39:10.550 --> 00:39:14.840
I get pushback from--
some people say, but look,

00:39:14.840 --> 00:39:17.360
you're taking the emotional
component out of giving.

00:39:17.360 --> 00:39:19.220
You're telling people
to think about it.

00:39:19.220 --> 00:39:21.540
And if people don't
feel emotionally,

00:39:21.540 --> 00:39:24.530
then they're not
going to give at all.

00:39:24.530 --> 00:39:27.090
I've don't accept that I'm
taking the emotional component

00:39:27.090 --> 00:39:27.590
out of it.

00:39:27.590 --> 00:39:31.590
I'm trying to change
the emotional component

00:39:31.590 --> 00:39:32.520
in some way.

00:39:32.520 --> 00:39:34.030
And I also get
pushback, I should

00:39:34.030 --> 00:39:38.070
say, from people in
the philanthropy sector

00:39:38.070 --> 00:39:41.670
as such, so professional
philanthropy advisors, those

00:39:41.670 --> 00:39:43.570
who are involved
in organizations

00:39:43.570 --> 00:39:45.200
for promoting philanthropy.

00:39:45.200 --> 00:39:47.870
They want to be cause neutral.

00:39:47.870 --> 00:39:51.950
Because they don't want
to turn people away.

00:39:51.950 --> 00:39:55.520
If they're find to be
advisors, and potential clients

00:39:55.520 --> 00:39:58.600
come to them and say, we'd like
your advice on how to give,

00:39:58.600 --> 00:40:00.610
or we want to give
away some of our money,

00:40:00.610 --> 00:40:03.310
or leave our estate
to something--

00:40:03.310 --> 00:40:06.850
and then they say
to you, and we're

00:40:06.850 --> 00:40:11.090
really passionate about music,
so we want to endow a new opera

00:40:11.090 --> 00:40:12.930
hall for our city.

00:40:12.930 --> 00:40:14.430
And if you then say
to them, well, I

00:40:14.430 --> 00:40:15.888
don't really think
a new opera hall

00:40:15.888 --> 00:40:18.390
is what the world
most needs, you know,

00:40:18.390 --> 00:40:20.439
should be helping to
prevent blindness in Africa,

00:40:20.439 --> 00:40:22.480
then they're worried that
those people would just

00:40:22.480 --> 00:40:25.220
go away and find somebody
else to talk to so.

00:40:25.220 --> 00:40:27.200
So they kind of have
this official idea

00:40:27.200 --> 00:40:29.760
that we can't judge.

00:40:29.760 --> 00:40:32.220
We can't judge between
different causes.

00:40:32.220 --> 00:40:33.950
I think that's wrong,
but I can sort of

00:40:33.950 --> 00:40:36.850
understand from their point of
view why they're saying that.

00:40:36.850 --> 00:40:38.280
We had someone in
the back there.

00:40:38.280 --> 00:40:38.450
Yeah.

00:40:38.450 --> 00:40:39.530
AUDIENCE: Yeah, I was
going to ask actually

00:40:39.530 --> 00:40:41.530
a really similar question
but with the example

00:40:41.530 --> 00:40:43.900
of International
Rescue Committee who

00:40:43.900 --> 00:40:46.390
were really praised, because
they did this large scale

00:40:46.390 --> 00:40:48.270
monitoring and
evaluation process.

00:40:48.270 --> 00:40:51.100
And it turned out that their
most expensive, longest running

00:40:51.100 --> 00:40:53.210
program had no impact.

00:40:53.210 --> 00:40:56.390
And there was debate, should
she publish that or not?

00:40:56.390 --> 00:40:59.100
Because it will be so
psychologically discouraging.

00:40:59.100 --> 00:41:02.606
It is a moral thing to
publish that finding or not?

00:41:02.606 --> 00:41:04.006
You just answered it.

00:41:04.006 --> 00:41:04.760
But yeah--

00:41:04.760 --> 00:41:05.010
PETER SINGER: Yeah.

00:41:05.010 --> 00:41:05.260
AUDIENCE: Thank you.

00:41:05.260 --> 00:41:05.850
PETER SINGER: No,
I think it's great

00:41:05.850 --> 00:41:08.270
that people do those
trials and that they

00:41:08.270 --> 00:41:09.740
are prepared to publish them.

00:41:09.740 --> 00:41:13.760
Best of all is if
they commit themselves

00:41:13.760 --> 00:41:16.330
in advance of the
trial by announcing

00:41:16.330 --> 00:41:17.496
that they're doing a trial.

00:41:17.496 --> 00:41:19.370
And that's what some
organizations are doing.

00:41:19.370 --> 00:41:23.310
One very transparent
organization is GiveDirectly.

00:41:23.310 --> 00:41:27.820
GiveDirectly has pioneered
handing out one-off cash grants

00:41:27.820 --> 00:41:30.320
to poor families in East Africa.

00:41:30.320 --> 00:41:33.960
It sort of finds poor
families, gives them $1,000,

00:41:33.960 --> 00:41:35.900
makes it clear that
that's what they're

00:41:35.900 --> 00:41:39.680
going to get-- it's
not a permanent thing--

00:41:39.680 --> 00:41:42.016
and then sees what
they do with it

00:41:42.016 --> 00:41:43.390
and whether they
come out better.

00:41:43.390 --> 00:41:45.223
And they announced
beforehand that they were

00:41:45.223 --> 00:41:46.569
going to do a randomized trial.

00:41:46.569 --> 00:41:48.110
So they were committed
to publicizing

00:41:48.110 --> 00:41:50.740
the results of the trial
however they came out.

00:41:50.740 --> 00:41:52.450
They did come out well.

00:41:52.450 --> 00:41:56.110
They're now actually trying
a guaranteed minimum income

00:41:56.110 --> 00:42:00.940
scheme to see whether if they
do give regular minimum amounts,

00:42:00.940 --> 00:42:02.227
what that does to people.

00:42:02.227 --> 00:42:03.810
And again, they've
announced that they

00:42:03.810 --> 00:42:07.190
will do a trial on
that without knowing

00:42:07.190 --> 00:42:09.315
how it's going to come out.

00:42:09.315 --> 00:42:11.680
AUDIENCE: So you explain
your moral argument

00:42:11.680 --> 00:42:14.530
with sound logic
that applies to NGOs.

00:42:14.530 --> 00:42:16.890
But there are many
ways in which entities

00:42:16.890 --> 00:42:20.320
try to target helping the
developing world, sometimes

00:42:20.320 --> 00:42:23.230
even extreme such as
military interventions.

00:42:23.230 --> 00:42:25.550
So my question is,
how do you think--

00:42:25.550 --> 00:42:28.950
to what extent do you think
your moral framework applies

00:42:28.950 --> 00:42:33.010
to such ways of helping out
rather than just directly

00:42:33.010 --> 00:42:34.870
through NGOs?

00:42:34.870 --> 00:42:38.680
I think the overriding
framework applies.

00:42:38.680 --> 00:42:41.570
I'm focusing on
NGOs, because I'm

00:42:41.570 --> 00:42:44.350
addressing people who might
have decisions about what

00:42:44.350 --> 00:42:48.090
to do with surplus income,
or sometimes with time

00:42:48.090 --> 00:42:51.170
that they might be prepared to
volunteer, or whatever it is.

00:42:51.170 --> 00:42:53.730
Nobody-- none of you will
have the power to say,

00:42:53.730 --> 00:42:58.190
oh, we ought to be intervening
in Syria, so let's do that.

00:42:58.190 --> 00:43:01.690
I mean, you might decide to
write a letter to a paper.

00:43:01.690 --> 00:43:03.510
Or you might decide to
vote for someone who

00:43:03.510 --> 00:43:04.676
thinks you ought to do that.

00:43:04.676 --> 00:43:08.004
But very marginal effect
that you can have there.

00:43:08.004 --> 00:43:09.920
But I do think if you're
considering something

00:43:09.920 --> 00:43:13.750
like intervention, you
ought to be weighing out

00:43:13.750 --> 00:43:17.060
the costs and benefits
of what that ought to be.

00:43:17.060 --> 00:43:21.010
And that's why
some cases I think

00:43:21.010 --> 00:43:25.270
we've missed opportunities to
get huge benefits at rather

00:43:25.270 --> 00:43:26.320
small costs.

00:43:26.320 --> 00:43:28.850
Rwanda would be the
classic case of that

00:43:28.850 --> 00:43:33.550
where according to the Canadian
leader of the UN forces

00:43:33.550 --> 00:43:37.410
that were there before, another
5,000 well-trained troops could

00:43:37.410 --> 00:43:40.660
have stopped the massacre
that took 800,000 lives.

00:43:40.660 --> 00:43:43.741
I think it's very regrettable
that that didn't happen.

00:43:43.741 --> 00:43:46.240
But other people have called
for intervention-- for example,

00:43:46.240 --> 00:43:49.054
at the time of the Kosovo
intervention against Serbia,

00:43:49.054 --> 00:43:50.720
other people said,
well, what about what

00:43:50.720 --> 00:43:51.980
Russia is doing in Chechnya?

00:43:51.980 --> 00:43:54.822
Isn't that just as bad as what
the Serbs are doing in Kosovo?

00:43:54.822 --> 00:43:57.030
And the answer might well
be yes, it was just as bad.

00:43:57.030 --> 00:43:59.780
But who wants a war with a major
nuclear armed power, right?

00:43:59.780 --> 00:44:02.270
The costs are
going to be absurd.

00:44:02.270 --> 00:44:05.640
So I do think the
ultimate framework is

00:44:05.640 --> 00:44:07.385
applicable to those situations.

00:44:10.640 --> 00:44:12.890
AUDIENCE: With your argument,
how should we compare

00:44:12.890 --> 00:44:14.930
individuals to corporations?

00:44:14.930 --> 00:44:16.950
So if a company such
as Google, what level

00:44:16.950 --> 00:44:20.050
should we be giving a
percentage of our profits

00:44:20.050 --> 00:44:21.347
back to charities?

00:44:21.347 --> 00:44:23.180
PETER SINGER: What level
should Google give?

00:44:23.180 --> 00:44:23.680
[LAUGHTER]

00:44:23.680 --> 00:44:25.096
AUDIENCE: Corporations
in general.

00:44:25.096 --> 00:44:27.320
PETER SINGER: Yeah.

00:44:27.320 --> 00:44:31.290
You know, corporations are in
somewhat different situations

00:44:31.290 --> 00:44:34.170
in that they have
fiduciary responsibilities

00:44:34.170 --> 00:44:36.600
to their shareholders.

00:44:36.600 --> 00:44:38.240
Of course, they can give.

00:44:38.240 --> 00:44:41.250
And they can justify this
in terms of being good

00:44:41.250 --> 00:44:44.310
for the image of the company.

00:44:44.310 --> 00:44:47.480
So there is a fair
amount of flexibility.

00:44:47.480 --> 00:44:51.544
And I think Google would
enhance its reputation

00:44:51.544 --> 00:44:52.460
by giving quite a lot.

00:44:52.460 --> 00:44:54.680
I know that it does
have google.org.

00:44:54.680 --> 00:44:58.460
And its funding a lot of
projects through that.

00:44:58.460 --> 00:45:02.290
And some of those I hope
will do a lot of good.

00:45:02.290 --> 00:45:06.450
So it's hard for me to put
any kind of specific figure

00:45:06.450 --> 00:45:07.890
on that.

00:45:07.890 --> 00:45:09.520
But that's an important thing.

00:45:09.520 --> 00:45:12.120
And then also in terms of
what the company is doing,

00:45:12.120 --> 00:45:15.060
the sort of corporate social
responsibility policies,

00:45:15.060 --> 00:45:16.260
I think are also important.

00:45:16.260 --> 00:45:18.830
And there's quite
a lot of thought

00:45:18.830 --> 00:45:22.080
going into that has also
had beneficial consequences

00:45:22.080 --> 00:45:25.200
on a number of
different companies.

00:45:25.200 --> 00:45:27.700
So that's not a very
specific answer.

00:45:27.700 --> 00:45:28.740
I'm sorry.

00:45:28.740 --> 00:45:31.520
If you have suggestions
about what kind of level

00:45:31.520 --> 00:45:34.340
ought to be set, I'm interested
in listening to them.

00:45:34.340 --> 00:45:37.563
But I don't feel I have the
knowledge to be more specific.

00:45:37.563 --> 00:45:38.146
SPEAKER 1: OK.

00:45:38.146 --> 00:45:39.861
We've got time for
two more questions.

00:45:39.861 --> 00:45:40.360
Go ahead.

00:45:40.360 --> 00:45:43.510
AUDIENCE: I like your
initial three-step argument.

00:45:43.510 --> 00:45:45.840
I wonder, where do you stop?

00:45:45.840 --> 00:45:49.120
So in the example of a
child that is drawing,

00:45:49.120 --> 00:45:52.160
if somebody told me,
you can have a meal,

00:45:52.160 --> 00:45:54.600
or you can skip lunch today
and you'll save a child,

00:45:54.600 --> 00:45:56.600
I'll probably skip lunch
today and save a child.

00:45:56.600 --> 00:46:01.230
So if I give all my spare
money to charity today,

00:46:01.230 --> 00:46:04.671
I could still probably miss
a meal one day and save

00:46:04.671 --> 00:46:05.170
a child's.

00:46:05.170 --> 00:46:06.520
So I should do that.

00:46:06.520 --> 00:46:08.300
And then I could
probably get another job

00:46:08.300 --> 00:46:11.330
and donate that money to charity
and save even another life.

00:46:11.330 --> 00:46:14.067
Because that's what I would do
for a child if somebody told me

00:46:14.067 --> 00:46:15.775
you have to work an
extra two hours today

00:46:15.775 --> 00:46:17.330
to save a child here.

00:46:17.330 --> 00:46:20.625
And the same goes as why
only focus on donating money?

00:46:20.625 --> 00:46:23.120
You know, I walk home, and
there is a homeless person.

00:46:23.120 --> 00:46:26.290
And that person would probably
be hungry and cold tonight.

00:46:26.290 --> 00:46:28.280
I could take him home.

00:46:28.280 --> 00:46:30.225
So why shouldn't I do that?

00:46:30.225 --> 00:46:32.230
It's a bit of a tricky question.

00:46:32.230 --> 00:46:34.450
But I think I understand
your moral framework.

00:46:34.450 --> 00:46:35.290
And I like it.

00:46:35.290 --> 00:46:38.610
I just wonder as such a
simple moral framework,

00:46:38.610 --> 00:46:41.690
if you would do anything to save
a child that is drowning right

00:46:41.690 --> 00:46:44.100
here, then where do you stop?

00:46:44.100 --> 00:46:46.350
Like, where do you put the
boundary between how much

00:46:46.350 --> 00:46:47.350
are you willing to give?

00:46:47.350 --> 00:46:51.580
Because with such
a simple analogy,

00:46:51.580 --> 00:46:53.250
what occurs to me is
that I should stop

00:46:53.250 --> 00:46:55.740
or everybody should
stop doing everything

00:46:55.740 --> 00:47:00.030
and give 100% or more 100%--
like, make a massive effort

00:47:00.030 --> 00:47:00.905
to donate everything.

00:47:00.905 --> 00:47:03.238
PETER SINGER: Well, you
probably shouldn't stop working,

00:47:03.238 --> 00:47:04.947
because that's the
source of your income.

00:47:04.947 --> 00:47:05.446
[LAUGHTER]

00:47:05.446 --> 00:47:06.880
And, you know, you
actually should

00:47:06.880 --> 00:47:09.730
be trying to work to-- if
you're fortunate enough

00:47:09.730 --> 00:47:13.130
to have a well-paying job, you
should be using that income

00:47:13.130 --> 00:47:15.010
and perhaps maximizing
that income.

00:47:15.010 --> 00:47:17.210
Some people in effective
altruism movement

00:47:17.210 --> 00:47:20.030
have deliberately chosen
higher income careers.

00:47:20.030 --> 00:47:22.360
I had a student at
Princeton who could

00:47:22.360 --> 00:47:24.130
have gone to graduate
school and probably

00:47:24.130 --> 00:47:27.040
had a career as a
philosophy professor.

00:47:27.040 --> 00:47:28.920
But because he had a
strong maths background,

00:47:28.920 --> 00:47:31.810
he also had very good
offers from Wall Street

00:47:31.810 --> 00:47:34.660
to go and work there--
and decided to do that.

00:47:34.660 --> 00:47:37.600
And he's been doing that for
the past four or five years,

00:47:37.600 --> 00:47:40.590
donating half of his income,
which even in the first year

00:47:40.590 --> 00:47:44.740
enabled him to donate $100,000
to effective charities.

00:47:44.740 --> 00:47:48.420
So, you know, he sees that
as a path of doing good.

00:47:48.420 --> 00:47:49.870
Some people would say, well, no.

00:47:49.870 --> 00:47:51.800
I mean, if that wasn't
the kind of work

00:47:51.800 --> 00:47:56.000
that I really want to do, I
just couldn't face doing that.

00:47:56.000 --> 00:47:57.710
But they'll earn less,
but they'll still

00:47:57.710 --> 00:47:58.946
give significantly.

00:47:58.946 --> 00:48:00.570
But you're asking,
I suppose, you know,

00:48:00.570 --> 00:48:01.880
where do you draw this line?

00:48:01.880 --> 00:48:04.290
And I don't have a very
good answer to that.

00:48:04.290 --> 00:48:08.390
In the original essay,
I said, ultimately,

00:48:08.390 --> 00:48:11.240
the only line you can draw is
the point at which-- well, I

00:48:11.240 --> 00:48:11.970
said two things.

00:48:11.970 --> 00:48:13.810
One is, if there
are certain things

00:48:13.810 --> 00:48:16.864
that you need in order to
maintain your income-- so it

00:48:16.864 --> 00:48:18.280
doesn't apply to
Google obviously,

00:48:18.280 --> 00:48:21.210
but some jobs you need to
be able to dress in a suit,

00:48:21.210 --> 00:48:23.230
and wear a tie, and so on.

00:48:23.230 --> 00:48:27.800
So you can't give away so much
that you can no longer hold

00:48:27.800 --> 00:48:31.010
a job and do well in your job.

00:48:31.010 --> 00:48:31.740
That's one thing.

00:48:31.740 --> 00:48:35.520
But apart from that, I said,
really the ultimate line

00:48:35.520 --> 00:48:38.240
would just be where you've
impoverished yourself

00:48:38.240 --> 00:48:40.700
so much that if
you gave away more,

00:48:40.700 --> 00:48:44.824
you would be adding to
your own difficulties,

00:48:44.824 --> 00:48:46.490
suffering, whatever
you want to call it,

00:48:46.490 --> 00:48:49.380
as much as you would be
alleviating the difficulties

00:48:49.380 --> 00:48:50.670
or suffering of someone else.

00:48:50.670 --> 00:48:52.380
That's the ultimate line.

00:48:52.380 --> 00:48:55.740
But, you know, I was a lot
younger when I wrote that.

00:48:55.740 --> 00:48:58.187
And maybe I've become a
little more realistic in terms

00:48:58.187 --> 00:48:59.770
of what you can say
to people and what

00:48:59.770 --> 00:49:02.220
you can expect them to do.

00:49:02.220 --> 00:49:06.279
And I think that even if in some
sense that still is ultimately

00:49:06.279 --> 00:49:08.570
what you ought to do, there's
a difference between that

00:49:08.570 --> 00:49:10.750
and what we ought to
expect people to do,

00:49:10.750 --> 00:49:14.070
what we ought to blame
people for not doing.

00:49:14.070 --> 00:49:18.890
And I think that if people sort
of just start doing something,

00:49:18.890 --> 00:49:21.490
make a substantial
difference, and then say,

00:49:21.490 --> 00:49:23.840
I'm going to try this out.

00:49:23.840 --> 00:49:26.710
If I'm comfortable with it,
I'm going to increase it.

00:49:26.710 --> 00:49:29.540
Year after year,
I'll be doing more.

00:49:29.540 --> 00:49:31.060
I think that's
the kind of appeal

00:49:31.060 --> 00:49:34.120
that has more hope of attracting
a large number of people.

00:49:34.120 --> 00:49:37.450
So that's the least
the public answer

00:49:37.450 --> 00:49:39.760
that I give to your question.

00:49:39.760 --> 00:49:41.520
Thanks.

00:49:41.520 --> 00:49:42.770
And there was a last one here?

00:49:42.770 --> 00:49:43.492
AUDIENCE: Yes.

00:49:43.492 --> 00:49:44.950
Quick question--
in your slide when

00:49:44.950 --> 00:49:47.110
you were talking about
the psychological reasons

00:49:47.110 --> 00:49:48.780
against it and
things, you didn't

00:49:48.780 --> 00:49:51.700
touch at all I think on
sort of reciprocation, which

00:49:51.700 --> 00:49:54.820
seems to be quite
a big-- the idea

00:49:54.820 --> 00:49:57.550
that if you see a child
drowning, you can think,

00:49:57.550 --> 00:49:59.470
well, if my child
was drowning, I'd

00:49:59.470 --> 00:50:01.280
want one of my
neighbors to help me.

00:50:01.280 --> 00:50:03.540
But when you're seeing
a disaster, famines

00:50:03.540 --> 00:50:05.190
across the other
side of the world,

00:50:05.190 --> 00:50:07.530
people probably don't
feel like one day that

00:50:07.530 --> 00:50:09.270
could be me in the
famine if you're

00:50:09.270 --> 00:50:11.640
in sort of developed world.

00:50:11.640 --> 00:50:14.140
I think that can take some of
the sort of the-- at least the

00:50:14.140 --> 00:50:15.350
urgency out of things.

00:50:15.350 --> 00:50:16.480
PETER SINGER: Mhm.

00:50:16.480 --> 00:50:21.700
OK, so that's not-- when
you said reciprocation,

00:50:21.700 --> 00:50:24.000
I thought you meant,
you know, this person

00:50:24.000 --> 00:50:27.090
is actually likely to
help me in some way.

00:50:27.090 --> 00:50:29.630
That's one sense
of reciprocation.

00:50:29.630 --> 00:50:32.440
But that doesn't apply to
the child in the pond either.

00:50:32.440 --> 00:50:34.100
So what you're talking
about is rather

00:50:34.100 --> 00:50:37.522
the kind of imagine
yourself in that position,

00:50:37.522 --> 00:50:38.980
you know, that
could happen to you.

00:50:38.980 --> 00:50:40.470
Or your child could be drowning.

00:50:40.470 --> 00:50:42.400
And you'd want
somebody to do that.

00:50:42.400 --> 00:50:45.880
And then the question
would be, well, how far can

00:50:45.880 --> 00:50:47.610
we carry out that
exercise, right?

00:50:47.610 --> 00:50:50.750
OK, so I do have a child
let's say that age.

00:50:50.750 --> 00:50:52.190
I mean, I don't anymore.

00:50:52.190 --> 00:50:53.340
My children are grown up.

00:50:53.340 --> 00:50:55.530
But I guess I have a
grandchild of that age,

00:50:55.530 --> 00:50:56.900
so I could say that.

00:50:56.900 --> 00:50:59.280
I would want somebody
to rescue my grandchild.

00:51:02.050 --> 00:51:04.460
Could I say, well, I
could become a refugee

00:51:04.460 --> 00:51:06.710
or something like that?

00:51:06.710 --> 00:51:08.310
I think I could imagine that.

00:51:08.310 --> 00:51:11.510
In fact, you know, my parents
were refugees from the Nazis.

00:51:11.510 --> 00:51:14.570
They came to Australia when
the Nazis took over Austria.

00:51:14.570 --> 00:51:19.390
So I don't have to go that far
back to think that, well, I

00:51:19.390 --> 00:51:22.750
certainly am very glad
that people helped them

00:51:22.750 --> 00:51:24.600
and that Australians took them.

00:51:24.600 --> 00:51:27.620
And so it depends on how far
you're going to carry that.

00:51:27.620 --> 00:51:31.250
But I think we can put ourselves
in the position of others

00:51:31.250 --> 00:51:31.820
in some ways.

00:51:31.820 --> 00:51:32.900
We can form connections.

00:51:32.900 --> 00:51:35.680
And the original question
that [? Christine ?]

00:51:35.680 --> 00:51:38.500
asked about social media
I guess may make it easier

00:51:38.500 --> 00:51:43.210
for us to see how we could be in
that situation in some perhaps

00:51:43.210 --> 00:51:48.625
not really likely circumstances,
but imaginable circumstances.

00:51:48.625 --> 00:51:51.000
AUDIENCE: Just I don't think
that the reciprocation thing

00:51:51.000 --> 00:51:52.450
is-- I don't think
it's a good argument,

00:51:52.450 --> 00:51:54.740
but I think that sort of
intuitively it feels that

00:51:54.740 --> 00:51:55.240
that's--

00:51:55.240 --> 00:51:56.295
PETER SINGER: Uh-huh.

00:51:56.295 --> 00:51:57.670
AUDIENCE: --that
would inherently

00:51:57.670 --> 00:51:58.720
stop people taking that step.

00:51:58.720 --> 00:51:59.360
PETER SINGER: Right, OK.

00:51:59.360 --> 00:51:59.859
OK, right.

00:51:59.859 --> 00:52:02.427
So then it's also related to
the they're people like me.

00:52:02.427 --> 00:52:03.760
That part of my group and so on.

00:52:03.760 --> 00:52:03.940
AUDIENCE: Yeah.

00:52:03.940 --> 00:52:04.820
It's sort of a in-group,
out-group thing

00:52:04.820 --> 00:52:05.420
on a fundamental level.

00:52:05.420 --> 00:52:05.546
PETER SINGER: Yeah.

00:52:05.546 --> 00:52:06.010
Yeah, you're right.

00:52:06.010 --> 00:52:08.180
And that is something that I
could have added to that list

00:52:08.180 --> 00:52:08.560
but didn't.

00:52:08.560 --> 00:52:09.059
Yeah.

00:52:09.059 --> 00:52:10.470
Thanks.

00:52:10.470 --> 00:52:13.590
SPEAKER 1: I'm terribly
sorry, but we're out of time.

00:52:13.590 --> 00:52:16.220
This has been an incredible,
thought provoking

00:52:16.220 --> 00:52:17.829
talk with a wonderful
call to action.

00:52:17.829 --> 00:52:20.120
I hope we all stop and think
about what we ought to do,

00:52:20.120 --> 00:52:21.014
how we ought to live.

00:52:21.014 --> 00:52:22.180
Please join me in thanking--

00:52:22.180 --> 00:52:22.560
PETER SINGER: Thank you.

00:52:22.560 --> 00:52:24.060
SPEAKER 1: --Professor
Peter Singer.

00:52:24.060 --> 00:52:26.750
[APPLAUSE]

