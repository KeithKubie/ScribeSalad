WEBVTT
Kind: captions
Language: en

00:00:01.590 --> 00:00:02.910
CARTER MORGAN: Hi.

00:00:02.910 --> 00:00:04.666
You guys excited at all?

00:00:04.666 --> 00:00:06.160
[CHEERS]

00:00:06.160 --> 00:00:06.660
Very cool.

00:00:06.660 --> 00:00:08.701
Today, we're going to be
talking about Kubernetes

00:00:08.701 --> 00:00:10.510
and best practices
for orchestrating

00:00:10.510 --> 00:00:12.020
the cloud with it.

00:00:12.020 --> 00:00:14.750
We'll have some live demos of
using industry standard tools

00:00:14.750 --> 00:00:16.890
like Docker and Kubernetes.

00:00:16.890 --> 00:00:20.530
You'll see us break down an
app into maybe micro services,

00:00:20.530 --> 00:00:22.750
package that up in a
container, distribute it,

00:00:22.750 --> 00:00:26.260
and then we'll use
Kubernetes to manage that.

00:00:26.260 --> 00:00:30.054
I'm Carter Morgan, developer
programs engineer at Google.

00:00:30.054 --> 00:00:32.220
And I was going to give
this talk with someone else.

00:00:32.220 --> 00:00:33.053
He couldn't make it.

00:00:33.053 --> 00:00:36.790
So I'll be driving all the
live demos and whatnot.

00:00:36.790 --> 00:00:40.024
Now, before we get into it,
I want to tell you my story.

00:00:40.024 --> 00:00:41.690
When I started at
Google six months ago,

00:00:41.690 --> 00:00:44.120
I had never worked
on the cloud before.

00:00:44.120 --> 00:00:46.950
I had programmed emulators
and compilers at Microsoft.

00:00:46.950 --> 00:00:49.450
I had done thread
databases at the Air Force.

00:00:49.450 --> 00:00:51.250
But I'd never
worked at the cloud.

00:00:51.250 --> 00:00:54.570
So when I started trying to
ramp up, it was overwhelming.

00:00:54.570 --> 00:00:57.300
There were so many tools,
so many different processes.

00:00:57.300 --> 00:00:59.760
I didn't know where to
begin, where to start.

00:00:59.760 --> 00:01:01.772
Luckily, one day I
ended up on a bus

00:01:01.772 --> 00:01:03.230
next to this guy
that was literally

00:01:03.230 --> 00:01:05.019
writing the book on Kubernetes.

00:01:05.019 --> 00:01:05.950
I told him my problem.

00:01:05.950 --> 00:01:08.850
I said, I'm having a lot
of trouble ramping up.

00:01:08.850 --> 00:01:11.540
And he gave me two
words of advice.

00:01:11.540 --> 00:01:13.340
He said, use Kubernetes.

00:01:13.340 --> 00:01:15.920
[LAUGHTER]

00:01:15.920 --> 00:01:17.900
And I said, you want me
to learn another tool

00:01:17.900 --> 00:01:19.890
on top of everything
else I'm juggling?

00:01:19.890 --> 00:01:21.840
And he said, yeah.

00:01:21.840 --> 00:01:24.470
And so he showed me Kubernetes
over the next two-hour bus

00:01:24.470 --> 00:01:24.970
ride.

00:01:24.970 --> 00:01:26.530
And it turned out he was right.

00:01:26.530 --> 00:01:28.600
Kubernetes offered
abstractions that made sense.

00:01:28.600 --> 00:01:30.770
It made the cloud
more approachable.

00:01:30.770 --> 00:01:33.480
Instead of focusing
on the machines,

00:01:33.480 --> 00:01:36.270
I was able to focus on what
I knew, my application.

00:01:36.270 --> 00:01:38.019
And so we decided
to make a course.

00:01:38.019 --> 00:01:40.310
Over the next four months,
I got to work with and learn

00:01:40.310 --> 00:01:42.600
best practices for
approaching the three

00:01:42.600 --> 00:01:45.192
major hurdles to writing
scalable applications online.

00:01:45.192 --> 00:01:46.650
And so by the end
of this talk, you

00:01:46.650 --> 00:01:48.650
will have seen this in
practice with live demos.

00:01:51.120 --> 00:01:55.290
So the first hurdle is
the application itself.

00:01:55.290 --> 00:01:56.900
I'm not sure why
the image the split.

00:01:56.900 --> 00:01:58.300
Oh, it's not, OK.

00:01:58.300 --> 00:02:00.130
So you see on screen,
I have a picture

00:02:00.130 --> 00:02:03.330
of a monolith application
and a microservices.

00:02:03.330 --> 00:02:04.990
We're going to be
demoing these today.

00:02:04.990 --> 00:02:06.489
And they're
functionally equivalent.

00:02:06.489 --> 00:02:07.840
They do the exact same thing.

00:02:07.840 --> 00:02:10.067
We just took a different
design approach.

00:02:10.067 --> 00:02:12.650
So how this app works is you can
hit and endpoint and get back

00:02:12.650 --> 00:02:13.358
a Hello response.

00:02:13.358 --> 00:02:17.080
It's a very, very
polite application.

00:02:17.080 --> 00:02:18.864
If we hit a secure
endpoint, we're

00:02:18.864 --> 00:02:20.280
not going to be
able to get access

00:02:20.280 --> 00:02:22.210
unless we have the right token.

00:02:22.210 --> 00:02:23.639
Both of these do the same thing.

00:02:23.639 --> 00:02:25.930
But there are pros and cons
to the different approaches

00:02:25.930 --> 00:02:27.290
we took.

00:02:27.290 --> 00:02:29.557
With microservices,
some of the benefits

00:02:29.557 --> 00:02:31.390
are that we can have
independent deployment.

00:02:31.390 --> 00:02:33.880
So my off service
or my hello service

00:02:33.880 --> 00:02:35.900
can be compiled and
shipped out separately

00:02:35.900 --> 00:02:37.400
without recompiling
anything else.

00:02:37.400 --> 00:02:38.775
They don't touch.

00:02:38.775 --> 00:02:40.400
I can use different
languages for that.

00:02:40.400 --> 00:02:42.270
So I could use Go
for hello and off.

00:02:42.270 --> 00:02:45.570
And I could use Node.js for
the front end if I wanted.

00:02:45.570 --> 00:02:47.760
And then also, these
services are smaller,

00:02:47.760 --> 00:02:49.780
which means it's easier
to maintain and manage.

00:02:49.780 --> 00:02:52.920
One person or a small team
can own any of these services

00:02:52.920 --> 00:02:55.190
from end to end, from
building the code

00:02:55.190 --> 00:02:58.250
to running it in production.

00:02:58.250 --> 00:03:01.800
Now, these advances in
how we design applications

00:03:01.800 --> 00:03:04.910
were made possible by advances
in application isolation

00:03:04.910 --> 00:03:08.190
technologies like
containers and VMs.

00:03:08.190 --> 00:03:11.220
What containers and VMs
were developed to solve

00:03:11.220 --> 00:03:14.530
were the problem of it
works on my machine.

00:03:14.530 --> 00:03:16.790
We wanted something that was
reproducible and reliable

00:03:16.790 --> 00:03:17.950
and that would
work on any system.

00:03:17.950 --> 00:03:19.960
We wanted to be able to
build it once and ship

00:03:19.960 --> 00:03:22.760
an application to different
operating environments.

00:03:22.760 --> 00:03:25.750
So on screen, you see a
picture of a boat or ship.

00:03:25.750 --> 00:03:27.570
And on top of that
are containers.

00:03:27.570 --> 00:03:31.790
In this example, the
operating systems is our boat.

00:03:31.790 --> 00:03:34.870
And the containers work
within that to give isolation

00:03:34.870 --> 00:03:36.130
to our application.

00:03:36.130 --> 00:03:38.420
The containers are
application containers.

00:03:38.420 --> 00:03:41.620
And what happens is inside of
there, we have an application.

00:03:41.620 --> 00:03:44.414
All of its dependencies in its
environment are bundled up.

00:03:44.414 --> 00:03:46.330
They don't touch any
other running application

00:03:46.330 --> 00:03:47.360
on our system.

00:03:47.360 --> 00:03:50.982
So we could, say, bind
port 80 on both of these

00:03:50.982 --> 00:03:52.190
without any kind of conflict.

00:03:52.190 --> 00:03:53.280
It's very convenient.

00:03:53.280 --> 00:03:54.660
Or we could have
two applications

00:03:54.660 --> 00:03:56.780
that use the same library,
but different versions,

00:03:56.780 --> 00:03:58.350
without any conflict.

00:03:58.350 --> 00:04:01.090
So we're isolating
our application

00:04:01.090 --> 00:04:02.660
away from the
operating environment

00:04:02.660 --> 00:04:05.974
and away from any other running
application on our system.

00:04:05.974 --> 00:04:08.140
Containers are also very
lightweight because they're

00:04:08.140 --> 00:04:10.720
a construct within
our operating system.

00:04:10.720 --> 00:04:13.350
So this means that
starting a new application

00:04:13.350 --> 00:04:16.010
with the containers is simple
as loading or unloading

00:04:16.010 --> 00:04:18.560
a container onto a boat.

00:04:18.560 --> 00:04:20.680
If we were using
virtual machines,

00:04:20.680 --> 00:04:22.150
to get the same
kind of isolation,

00:04:22.150 --> 00:04:24.830
we would have to
rebuild the boat.

00:04:24.830 --> 00:04:26.890
And that's effective,
but maybe not efficient.

00:04:26.890 --> 00:04:28.690
So as a best
practice, we're going

00:04:28.690 --> 00:04:33.680
to use containers to package
and distribute our applications.

00:04:33.680 --> 00:04:37.060
And I'm actually going
to demo this now.

00:04:37.060 --> 00:04:41.080
So I'll be demoing Docker, which
is a container image format.

00:04:41.080 --> 00:04:42.310
It's very popular.

00:04:42.310 --> 00:04:44.700
The same principles would
work if we were using

00:04:44.700 --> 00:04:46.190
Rocket, another image format.

00:04:46.190 --> 00:04:48.384
The command lines
would be different.

00:04:48.384 --> 00:04:50.050
And so the very first
thing I want to do

00:04:50.050 --> 00:04:52.050
is build my application.

00:04:52.050 --> 00:04:54.580
I'm using Go version 1.6.

00:04:54.580 --> 00:04:56.790
There's no magic here.

00:04:56.790 --> 00:04:59.010
I'm going to build the
monolithic version first.

00:04:59.010 --> 00:05:01.010
And then later, we'll use
the microservices one.

00:05:01.010 --> 00:05:02.110
It's just easier
to show everything

00:05:02.110 --> 00:05:03.276
when it's bundled up as one.

00:05:10.520 --> 00:05:12.640
Normally, if you
build Go, it's not

00:05:12.640 --> 00:05:15.170
this complicated to write out.

00:05:15.170 --> 00:05:17.520
But I want to build
a static binary.

00:05:17.520 --> 00:05:20.250
And so that is a
little more involved.

00:05:31.371 --> 00:05:31.870
Oops.

00:05:38.690 --> 00:05:41.567
OK, now, once this
is built, we're

00:05:41.567 --> 00:05:43.400
going to have a binary
that we can test out.

00:05:43.400 --> 00:05:45.316
And I'll be able to show
you the functionality

00:05:45.316 --> 00:05:46.250
of this application.

00:05:46.250 --> 00:05:47.900
Both the monolith
and the microservices

00:05:47.900 --> 00:05:48.887
do the same thing.

00:05:48.887 --> 00:05:50.470
We've just broken
the microservices up

00:05:50.470 --> 00:05:51.470
into smaller components.

00:05:54.950 --> 00:05:59.400
So now that I have a
binary, I can run it.

00:05:59.400 --> 00:06:01.010
I'm going to find some ports.

00:06:09.034 --> 00:06:11.075
I'm actually going to run
that in the background.

00:06:13.477 --> 00:06:15.560
And now I can do things
like curl to the endpoint.

00:06:28.310 --> 00:06:29.912
And I get back a greeter.

00:06:34.832 --> 00:06:36.122
AUDIENCE: [INAUDIBLE].

00:06:36.122 --> 00:06:37.330
CARTER MORGAN: Ah, thank you.

00:06:39.958 --> 00:06:41.410
You guys are paying attention.

00:06:41.410 --> 00:06:42.970
I like it.

00:06:42.970 --> 00:06:44.450
So I got back a greeter.

00:06:44.450 --> 00:06:48.470
If I were to hit up
the secure route,

00:06:48.470 --> 00:06:51.023
I'm going to be denied access.

00:06:51.023 --> 00:06:52.856
So what we're going to
have to do is log in.

00:06:57.520 --> 00:07:01.400
I'm going to use the super
secret password-- "password."

00:07:01.400 --> 00:07:04.357
And I have a token now.

00:07:04.357 --> 00:07:06.190
So I'm going to pass
that token as a header.

00:07:18.680 --> 00:07:21.080
And this will give us access
to this secure endpoint.

00:07:28.822 --> 00:07:30.280
Now, that's the
only time I'm going

00:07:30.280 --> 00:07:32.050
through all of these steps
the rest of the demo.

00:07:32.050 --> 00:07:33.508
For everything
else, I'm just going

00:07:33.508 --> 00:07:35.050
to hit the regular endpoint.

00:07:35.050 --> 00:07:36.460
It's a lot of steps.

00:07:36.460 --> 00:07:37.665
But so now we have a binary.

00:07:37.665 --> 00:07:38.290
It works.

00:07:38.290 --> 00:07:39.264
We've tested it out.

00:07:39.264 --> 00:07:40.805
We want to be able
to run this binary

00:07:40.805 --> 00:07:42.446
on any machine on any system.

00:07:42.446 --> 00:07:43.820
And that's where
Docker comes in.

00:07:47.240 --> 00:07:48.650
So I have a Dockerfile.

00:07:48.650 --> 00:07:50.969
And what this is it's
steps to recreate an image.

00:07:50.969 --> 00:07:52.760
We're going to take
all of the dependencies

00:07:52.760 --> 00:07:53.840
to run an application.

00:07:53.840 --> 00:07:57.000
And we're going to bundle
this up into one image.

00:07:57.000 --> 00:07:59.860
So the very first line is
saying, what is our base image,

00:07:59.860 --> 00:08:02.290
or what is our base
operating system?

00:08:02.290 --> 00:08:04.540
Now, as a best practice,
we want to use something

00:08:04.540 --> 00:08:05.630
that's very, very small.

00:08:05.630 --> 00:08:07.379
We don't want to bundle
unnecessary things

00:08:07.379 --> 00:08:08.400
into our application.

00:08:08.400 --> 00:08:10.575
So Alpine Three One is
five megabytes in size.

00:08:10.575 --> 00:08:12.200
But it still gives
us the functionality

00:08:12.200 --> 00:08:14.519
where we could SSH
into a container

00:08:14.519 --> 00:08:16.310
and ping and make sure
that our internet is

00:08:16.310 --> 00:08:18.460
working, basic debugging
we'd need in production.

00:08:21.460 --> 00:08:24.700
Further down, you see that I'm
taking the binary I just built.

00:08:24.700 --> 00:08:27.957
And I'm going to store it in the
container at user bin monolith.

00:08:27.957 --> 00:08:29.540
And then when I
started the container,

00:08:29.540 --> 00:08:30.610
I'm going to run the monolith.

00:08:30.610 --> 00:08:32.590
So I can start this
container and run it pretty

00:08:32.590 --> 00:08:33.631
much like an application.

00:08:35.799 --> 00:08:39.299
So I'm actually
going to do that now.

00:08:39.299 --> 00:08:41.885
I'm going to build the image.

00:08:41.885 --> 00:08:43.509
I'm going to give it
a name-- Monolith.

00:08:43.509 --> 00:08:49.980
And I'm going to give
it a version-- 1.0.0.

00:08:49.980 --> 00:08:51.650
And what we see
here is we're taking

00:08:51.650 --> 00:08:53.827
the content of that directory,
and we're setting it

00:08:53.827 --> 00:08:54.910
up to a build [INAUDIBLE].

00:08:54.910 --> 00:08:57.080
And it's going to recreate
an image from this.

00:08:57.080 --> 00:08:59.410
And the image should be 13 megs.

00:08:59.410 --> 00:09:01.820
Because we have eight
megabytes for avocation

00:09:01.820 --> 00:09:04.540
and then five for Alpine.

00:09:08.620 --> 00:09:10.150
Now, one reason
this was so small

00:09:10.150 --> 00:09:13.222
is because we didn't use Docker
to build our application.

00:09:13.222 --> 00:09:15.680
We used Docker as an environment
to package and distribute.

00:09:15.680 --> 00:09:17.540
We built our
application outside.

00:09:17.540 --> 00:09:20.660
And that lets us do things like
use a continuous integration

00:09:20.660 --> 00:09:23.770
system or manually build an
application like I just did.

00:09:23.770 --> 00:09:27.680
So we keep our Docker
images really small.

00:09:27.680 --> 00:09:37.530
So now, I'm going to push
my image on to the cloud,

00:09:37.530 --> 00:09:38.536
or into the cloud.

00:09:38.536 --> 00:09:40.035
This is going to
go up to Docker Hub

00:09:40.035 --> 00:09:42.650
because I didn't specify
a different registry.

00:09:42.650 --> 00:09:45.680
We could use differ ones
like quay.io or gcr.io.

00:09:45.680 --> 00:09:47.770
And that's just prefixing it.

00:09:47.770 --> 00:09:49.480
Docker Hub is the default one.

00:09:49.480 --> 00:09:51.320
So that's just what I used.

00:09:51.320 --> 00:09:55.840
Now, what this lets
me do is if I get rid

00:09:55.840 --> 00:10:19.775
of the image on my
local machine-- oops--

00:10:19.775 --> 00:10:21.900
so now I don't have that
image of my local machine.

00:10:21.900 --> 00:10:22.608
It doesn't exist.

00:10:25.340 --> 00:10:26.840
If I were to run
and say Docker, I

00:10:26.840 --> 00:10:40.480
want to use that image-- let
me fix a typo right quick--

00:10:40.480 --> 00:10:42.440
it couldn't find
the image locally.

00:10:42.440 --> 00:10:43.780
So it went and pulled this down.

00:10:50.930 --> 00:10:58.040
So now I can do things
like find the image ID.

00:11:02.510 --> 00:11:04.615
I can inspect it to
get an IP address.

00:11:17.420 --> 00:11:18.920
And now I can hit
this endpoint just

00:11:18.920 --> 00:11:20.961
like the application I
had locally on my machine.

00:11:27.224 --> 00:11:28.140
So that's pretty cool.

00:11:28.140 --> 00:11:31.124
We now have an image that we
can ship to almost any operating

00:11:31.124 --> 00:11:32.540
environment and
it's going to run.

00:11:32.540 --> 00:11:34.456
If Docker is installed,
this is going to work.

00:11:37.480 --> 00:11:39.522
So at this point, I
thought I knew everything

00:11:39.522 --> 00:11:41.480
I needed to know to write
scalable applications

00:11:41.480 --> 00:11:42.250
for the cloud.

00:11:42.250 --> 00:11:44.540
I knew how to break it down
into smaller pieces that

00:11:44.540 --> 00:11:48.970
were easier to maintain or
easier to ship and distribute.

00:11:48.970 --> 00:11:51.170
I knew how to package
those into a container.

00:11:51.170 --> 00:11:53.789
And I knew how to ship those.

00:11:53.789 --> 00:11:56.330
But it turns out that packaging
and distributing applications

00:11:56.330 --> 00:11:57.860
is a small part of it.

00:11:57.860 --> 00:12:00.580
We have to worry about is your
container running in the wild?

00:12:00.580 --> 00:12:01.559
Is it online?

00:12:01.559 --> 00:12:03.350
Once it's up there,
how do you discover it?

00:12:03.350 --> 00:12:04.430
How do you access it?

00:12:04.430 --> 00:12:06.840
How do you connect
network to it?

00:12:06.840 --> 00:12:09.310
Security-- how do
you manage passwords

00:12:09.310 --> 00:12:10.970
and sensitive information?

00:12:10.970 --> 00:12:15.870
There's a lot of complexity that
comes with managing containers.

00:12:15.870 --> 00:12:18.730
And so as a best practice, we
want to automate and monitor

00:12:18.730 --> 00:12:19.589
everything we can.

00:12:19.589 --> 00:12:21.130
Because we don't
want to get woken up

00:12:21.130 --> 00:12:23.130
in the middle of the night
responding to a pager

00:12:23.130 --> 00:12:25.270
to jump on a terminal
or command line just

00:12:25.270 --> 00:12:27.960
to restart a container.

00:12:27.960 --> 00:12:29.820
We want to use a framework
or something that

00:12:29.820 --> 00:12:33.290
can handle this complexity for
us and monitor and automate

00:12:33.290 --> 00:12:36.020
as much as we possibly can.

00:12:36.020 --> 00:12:38.400
Now ideally, this
framework isn't

00:12:38.400 --> 00:12:40.700
going to lock us in into
any one way of doing things.

00:12:40.700 --> 00:12:43.420
We don't want to be locked into
a container format, a cloud

00:12:43.420 --> 00:12:46.979
provider, or again, any
specific set of tools.

00:12:46.979 --> 00:12:49.270
We want a framework that's
going to be flexible and let

00:12:49.270 --> 00:12:51.350
us tell our own story.

00:12:51.350 --> 00:12:53.560
Because as our
application evolves,

00:12:53.560 --> 00:12:56.740
the tool that we're going
to use is going to evolve.

00:12:56.740 --> 00:12:59.591
And this leads me to Kubernetes.

00:12:59.591 --> 00:13:03.330
Kubernetes is an open source
container automation framework.

00:13:03.330 --> 00:13:04.180
It's written in Go.

00:13:04.180 --> 00:13:05.638
And it's hosted
entirely on GitHub.

00:13:05.638 --> 00:13:09.430
So you can go and check it out
yourself, even commit to it.

00:13:09.430 --> 00:13:10.630
It's an open API.

00:13:10.630 --> 00:13:15.020
So CoreOS uses it with AWS, GCP.

00:13:15.020 --> 00:13:16.210
We use it.

00:13:16.210 --> 00:13:18.535
You'll see me using Google
container NGINX, which

00:13:18.535 --> 00:13:19.530
is a hosted version.

00:13:22.860 --> 00:13:23.970
Microsoft uses it too.

00:13:23.970 --> 00:13:26.630
So you're not locked in
any one cloud provider.

00:13:26.630 --> 00:13:27.330
It's pluggable.

00:13:27.330 --> 00:13:30.130
So you can write like your own
scheduler or your own tooling

00:13:30.130 --> 00:13:30.970
with it.

00:13:30.970 --> 00:13:33.150
And it's based on years
of Google's experiences

00:13:33.150 --> 00:13:34.919
with running
containers internally.

00:13:34.919 --> 00:13:37.460
So it's something that will let
you scale up from a small app

00:13:37.460 --> 00:13:41.400
like I'm demoing today
to production needs.

00:13:41.400 --> 00:13:44.150
You see that there's a lot
of enthusiasm with Kubernetes

00:13:44.150 --> 00:13:45.570
as well too.

00:13:45.570 --> 00:13:49.910
On screen, you see a snapshot
of the commits to GitHub.

00:13:49.910 --> 00:13:53.430
And for the version 1.2 release,
there was over 5,000 commits.

00:13:53.430 --> 00:13:55.160
So Kubernetes is
really fastly growing.

00:13:58.000 --> 00:14:00.110
Now, during this demo,
I don't have time

00:14:00.110 --> 00:14:03.820
to show you all of the
features in Kubernetes.

00:14:03.820 --> 00:14:06.580
Version 1.2 was designed
to make it easy to scale

00:14:06.580 --> 00:14:08.180
and easy to deploy.

00:14:08.180 --> 00:14:12.840
1.3 is adding a lot
of very cool features.

00:14:12.840 --> 00:14:14.020
Maybe after, come find me.

00:14:14.020 --> 00:14:15.370
We can talk about more of them.

00:14:15.370 --> 00:14:17.930
Today, I'm going to show you the
basics to get started and clear

00:14:17.930 --> 00:14:18.846
the remaining hurdles.

00:14:21.650 --> 00:14:26.050
And so you can see that there's
over 1,000 external projects

00:14:26.050 --> 00:14:27.660
that are based on Kubernetes.

00:14:27.660 --> 00:14:29.730
It's one of the top
projects on GitHub.

00:14:29.730 --> 00:14:32.300
And there's companies that
are using it with 100 nodes

00:14:32.300 --> 00:14:34.130
or 100 machines up to 1,000.

00:14:34.130 --> 00:14:36.220
So today, I'm showing a
very small application.

00:14:36.220 --> 00:14:37.636
But Kubernetes is
a framework that

00:14:37.636 --> 00:14:41.140
will let you grow from
little to much bigger.

00:14:41.140 --> 00:14:44.290
And the best way to see
the power of Kubernetes

00:14:44.290 --> 00:14:45.221
is to demo it.

00:14:58.480 --> 00:15:00.880
So what I'm going
to show off here

00:15:00.880 --> 00:15:04.030
is using Kubernetes to
run that same image.

00:15:04.030 --> 00:15:06.820
So before we use Docker
to run a monolith,

00:15:06.820 --> 00:15:09.260
now we're going
to use Kubernetes.

00:15:09.260 --> 00:15:11.990
So I could say kubectl, which
is the Kubernetes command line

00:15:11.990 --> 00:15:13.927
tool.

00:15:13.927 --> 00:15:14.510
Let's say run.

00:15:28.887 --> 00:15:30.720
Now, what happens here
is Kubernetes created

00:15:30.720 --> 00:15:31.890
what's called a deployment.

00:15:31.890 --> 00:15:34.000
I'm going to explain each and
every one of these concepts

00:15:34.000 --> 00:15:34.600
after this.

00:15:38.500 --> 00:15:41.674
But what this does is behind
the scenes it creates some pods.

00:15:41.674 --> 00:15:43.840
And pods, you can think of
them as our applications.

00:15:43.840 --> 00:15:45.617
So I have an instance
of our application

00:15:45.617 --> 00:15:46.700
running in the background.

00:15:49.460 --> 00:16:03.004
If we look further, I'm going
to actually describe it.

00:16:03.004 --> 00:16:04.420
We can see all the
steps that I've

00:16:04.420 --> 00:16:05.670
got running in the background.

00:16:23.620 --> 00:16:28.400
I work at Google, but spelling
is very difficult for me.

00:16:28.400 --> 00:16:31.900
OK, sorry.

00:16:31.900 --> 00:16:34.707
So I actually want
to describe this pod.

00:16:34.707 --> 00:16:36.290
Because we can see
the Docker commands

00:16:36.290 --> 00:16:38.607
I've got running in
the background for us

00:16:38.607 --> 00:16:39.190
by Kubernetes.

00:16:45.910 --> 00:16:48.780
So I'm going to make this
smaller for a second.

00:16:54.390 --> 00:16:55.870
Can you guys still see that?

00:16:55.870 --> 00:16:56.850
All right.

00:16:56.850 --> 00:17:00.150
So we'll see that after our
container was scheduled,

00:17:00.150 --> 00:17:01.480
we pulled the image.

00:17:01.480 --> 00:17:03.420
Just like I pulled
it manually before,

00:17:03.420 --> 00:17:05.990
Kubernetes pulled that for us.

00:17:05.990 --> 00:17:08.450
And then it actually created
a container with that image

00:17:08.450 --> 00:17:09.530
and started it.

00:17:09.530 --> 00:17:12.000
So Kubernetes handled
all of the Docker for us.

00:17:16.230 --> 00:17:19.250
Now, I want to be able
to access this pod.

00:17:19.250 --> 00:17:20.890
And pods get internal IP.

00:17:20.890 --> 00:17:22.750
So I could use port
forwarding or I could

00:17:22.750 --> 00:17:24.680
use some kind of init scripts.

00:17:24.680 --> 00:17:26.569
But there's other ways to do it.

00:17:26.569 --> 00:17:28.260
Kubectl gives you
an expose command.

00:17:32.920 --> 00:17:34.160
I'm going to open up port 80.

00:17:36.848 --> 00:17:38.825
And I'm going to make
the type load balancer.

00:17:44.790 --> 00:17:46.314
This takes a second to start up.

00:17:46.314 --> 00:17:47.730
What this did is
it created what's

00:17:47.730 --> 00:17:49.790
called a service, a
persistent endpoint for us

00:17:49.790 --> 00:17:51.930
to access our pods with.

00:17:51.930 --> 00:17:57.840
If I look at this
service, right now

00:17:57.840 --> 00:17:59.910
it doesn't have an
external IP address.

00:17:59.910 --> 00:18:00.980
It's creating one.

00:18:00.980 --> 00:18:02.413
So let's actually
check that out.

00:18:08.126 --> 00:18:09.500
You can see that
we automatically

00:18:09.500 --> 00:18:10.416
picked up an endpoint.

00:18:10.416 --> 00:18:11.350
It saw our pod.

00:18:14.200 --> 00:18:31.460
If I were to scale up my
application, or make three,

00:18:31.460 --> 00:18:33.590
we can see that we picked
up these endpoints.

00:18:33.590 --> 00:18:35.270
Our service pick those
up automatically.

00:18:35.270 --> 00:18:37.780
So it's feeding traffic
to our application

00:18:37.780 --> 00:18:41.910
without us having to
manually configure that.

00:18:41.910 --> 00:18:46.051
If I were to look behind the
scenes a little bit further,

00:18:46.051 --> 00:18:47.550
I can see that I
have three versions

00:18:47.550 --> 00:18:49.480
of our application running too.

00:18:49.480 --> 00:18:51.226
So let's send traffic to them.

00:18:55.790 --> 00:18:58.405
I now have an external IP
address that I can hit.

00:19:05.480 --> 00:19:06.980
And I get our
familiar message back.

00:19:09.680 --> 00:19:13.300
So what was happening
behind the scenes there?

00:19:13.300 --> 00:19:15.280
Everything starts
with an application.

00:19:15.280 --> 00:19:17.969
And in Kubernetes, the
application is the pod.

00:19:17.969 --> 00:19:20.510
You can think of it as one or
more containers housed together

00:19:20.510 --> 00:19:22.170
that know how to play
well with each other.

00:19:22.170 --> 00:19:23.794
They're going to be
scheduled together.

00:19:23.794 --> 00:19:25.580
They can communicate
with each other.

00:19:25.580 --> 00:19:26.580
They share a namespace.

00:19:26.580 --> 00:19:29.780
So if we have data files
that we want to access,

00:19:29.780 --> 00:19:32.290
every container in
this pod can access it.

00:19:32.290 --> 00:19:34.496
We also have one
IP address per pod,

00:19:34.496 --> 00:19:36.120
which is pretty nice
because I can have

00:19:36.120 --> 00:19:38.328
two versions of my application
running, and they can,

00:19:38.328 --> 00:19:41.960
again, both bind port
80 with no conflict.

00:19:41.960 --> 00:19:43.930
Now, every object
in Kubernetes gets

00:19:43.930 --> 00:19:45.804
this is arbitrary
key value data.

00:19:45.804 --> 00:19:47.095
And this is how we do grouping.

00:19:47.095 --> 00:19:48.886
This is how our service
was able to pick up

00:19:48.886 --> 00:19:51.660
the pods behind the scenes.

00:19:51.660 --> 00:19:54.720
So on screen, you see that
I have two applications.

00:19:54.720 --> 00:19:56.110
And they both have labels.

00:19:56.110 --> 00:19:58.940
They have a version,
and they have a track.

00:19:58.940 --> 00:20:01.010
If I were to select
the v1 version,

00:20:01.010 --> 00:20:03.330
I would select
every application.

00:20:03.330 --> 00:20:05.850
So before my service
was selecting everything

00:20:05.850 --> 00:20:08.750
that was at equals monolith.

00:20:08.750 --> 00:20:10.940
If I were just to
select track=stable,

00:20:10.940 --> 00:20:13.410
I'd only get one of those.

00:20:13.410 --> 00:20:16.810
Now, we talked about
services a little bit before.

00:20:16.810 --> 00:20:19.870
Services are persistent
endpoints for IP addresses.

00:20:19.870 --> 00:20:21.650
They can be internal
or external.

00:20:21.650 --> 00:20:23.150
So if we have back
ends that we want

00:20:23.150 --> 00:20:24.900
to be able to communicate
with each other,

00:20:24.900 --> 00:20:27.090
we would use a service for that.

00:20:27.090 --> 00:20:29.630
But if we wanted to access
these outside of our cluster,

00:20:29.630 --> 00:20:32.480
we would also use
a service for that.

00:20:32.480 --> 00:20:33.940
These are dynamically created.

00:20:33.940 --> 00:20:36.000
So our services can
dynamically pick up

00:20:36.000 --> 00:20:37.334
pods to send traffic to.

00:20:37.334 --> 00:20:38.750
And they can have
different types.

00:20:38.750 --> 00:20:40.150
We saw the load balancer one.

00:20:40.150 --> 00:20:42.590
And that's just going to round
robin traffic to pods that

00:20:42.590 --> 00:20:45.770
are targeted by our service.

00:20:45.770 --> 00:20:49.240
Now, to this point, I haven't
really talked about machines.

00:20:49.240 --> 00:20:52.280
Does anyone notice that?

00:20:52.280 --> 00:20:54.780
So the reason why is Kubernetes
abstracts away the machine

00:20:54.780 --> 00:20:57.250
so we can focus on
our application.

00:20:57.250 --> 00:20:59.470
Deployments are an
object that manage

00:20:59.470 --> 00:21:01.060
our pods or our
applications for us

00:21:01.060 --> 00:21:02.730
in order to manage
scheduling them.

00:21:02.730 --> 00:21:05.980
So on screen, you'll see that I
have two machines called nodes.

00:21:05.980 --> 00:21:07.730
And I have one version
of my application,

00:21:07.730 --> 00:21:09.760
one replica that I want running.

00:21:09.760 --> 00:21:11.900
Deployments are used
to drive our desired

00:21:11.900 --> 00:21:14.810
state or our current state
closer to our desired state.

00:21:14.810 --> 00:21:17.070
So if I want one, I've
got one, I'm good.

00:21:17.070 --> 00:21:19.420
But if I change that
and I say I want two,

00:21:19.420 --> 00:21:22.420
the deployment [INAUDIBLE]
is scheduling a second node.

00:21:22.420 --> 00:21:24.574
If a machine were to
go down, Kubernetes

00:21:24.574 --> 00:21:26.240
would handle rescheduling
the second one

00:21:26.240 --> 00:21:28.960
until our current state
matched our desired state.

00:21:28.960 --> 00:21:31.575
That's pretty powerful.

00:21:31.575 --> 00:21:33.700
So now I'm going to take
our application that we've

00:21:33.700 --> 00:21:35.120
been using, the monolith.

00:21:35.120 --> 00:21:36.930
And I'm going to use
Kubernetes to show you

00:21:36.930 --> 00:21:40.220
how to the set up deployments
for a microservices version

00:21:40.220 --> 00:21:41.060
of the app.

00:21:41.060 --> 00:21:43.060
So we're going to have
our external facing front

00:21:43.060 --> 00:21:45.380
end that can be accessed
outside of our cluster.

00:21:45.380 --> 00:21:47.640
And we'll have two
persistent back end

00:21:47.640 --> 00:21:49.810
services-- our off and
hello-- all communicating

00:21:49.810 --> 00:21:50.476
with each other.

00:21:57.210 --> 00:22:00.950
So the first thing I want to
do is show the configurations

00:22:00.950 --> 00:22:02.555
for our front end service.

00:22:08.480 --> 00:22:10.220
We're going to be
listed on port 443.

00:22:10.220 --> 00:22:12.460
And this is going to
be a secure endpoint.

00:22:12.460 --> 00:22:14.670
So we're going to have
to pass sensitive data.

00:22:14.670 --> 00:22:16.336
I'm not going to get
into this too much.

00:22:16.336 --> 00:22:19.020
I'm just going to
manually create it here.

00:22:19.020 --> 00:22:21.780
But Kubernetes gives you a
way to pass sensitive data

00:22:21.780 --> 00:22:23.530
without baking it
into your container

00:22:23.530 --> 00:22:26.480
or storing it
publicly on GitHub.

00:22:26.480 --> 00:22:31.277
And then I'm also going to
say if I hit my default route,

00:22:31.277 --> 00:22:33.610
I'm going to forward that
traffic to the hello endpoint.

00:22:33.610 --> 00:22:34.774
This is a named endpoint.

00:22:34.774 --> 00:22:36.690
And Kubernetes is going
to set this up for us.

00:22:36.690 --> 00:22:39.106
So I don't need to know the
IP address as long as I set up

00:22:39.106 --> 00:22:41.300
my services.

00:22:41.300 --> 00:22:43.210
If I hit the login
endpoint, then I'm

00:22:43.210 --> 00:22:45.251
going to pass that to the
authentication handler.

00:22:48.169 --> 00:22:50.460
So really quick, I'm going
to create that configuration

00:22:50.460 --> 00:22:51.168
data that I need.

00:23:03.787 --> 00:23:06.120
And I'm just going to make
that from a file we just saw.

00:23:12.810 --> 00:23:18.040
I'm also going to create a file
for our sensitive data as well.

00:23:18.040 --> 00:23:21.300
And all of these commands
are in GitHub publicly.

00:23:21.300 --> 00:23:22.549
So you can follow along.

00:23:22.549 --> 00:23:24.090
And they're also in
the space I think

00:23:24.090 --> 00:23:25.910
it's called that
they set up for I/O.

00:23:25.910 --> 00:23:28.451
So if you want to see everything
I'm running, check that out.

00:23:42.580 --> 00:23:44.280
OK, now that we have
that data, let's

00:23:44.280 --> 00:23:45.696
look at some of
our other systems.

00:23:58.380 --> 00:24:00.390
So for our off
service, we're going

00:24:00.390 --> 00:24:02.100
to create a service named off.

00:24:02.100 --> 00:24:05.480
We're going to forward traffic
from port 80 to port 80.

00:24:05.480 --> 00:24:06.670
This is an internal service.

00:24:06.670 --> 00:24:08.711
We're not saying it's a
load balance or anything.

00:24:08.711 --> 00:24:11.080
So we can only access
this internally.

00:24:11.080 --> 00:24:12.630
But if you notice,
this is where we

00:24:12.630 --> 00:24:14.130
got the name for our endpoint.

00:24:16.970 --> 00:24:24.030
If we look at the
deployment, we're

00:24:24.030 --> 00:24:28.330
using the askcarter off image.

00:24:28.330 --> 00:24:30.727
And we're just taking
traffic on port 80 and 80.

00:24:30.727 --> 00:24:32.810
So I'm actually going to
go ahead and create that.

00:24:47.380 --> 00:24:49.130
And I'm going to do
something very similar

00:24:49.130 --> 00:24:51.855
for the hello service.

00:24:56.470 --> 00:24:56.970
Oops.

00:25:11.530 --> 00:25:15.540
So if we were to
look at our service

00:25:15.540 --> 00:25:19.359
now for the front end, which
is a little more complicated,

00:25:19.359 --> 00:25:21.150
we can see that we're
going to take traffic

00:25:21.150 --> 00:25:23.790
on port 443, which our
NGINX is looking for,

00:25:23.790 --> 00:25:25.040
the internal container.

00:25:25.040 --> 00:25:27.240
And we're going to load
balance to any pods that

00:25:27.240 --> 00:25:28.490
are picked up by this service.

00:25:31.298 --> 00:25:39.000
And if we were to look at
our deployment as well,

00:25:39.000 --> 00:25:55.200
you can see that
we're just using--

00:25:55.200 --> 00:25:57.580
we're using NGINX for that.

00:25:57.580 --> 00:26:00.880
And so this data that we mounted
before, that we created before,

00:26:00.880 --> 00:26:02.780
this is going to be
mounted by our container.

00:26:02.780 --> 00:26:04.280
Kubernetes is going to
take care of loading

00:26:04.280 --> 00:26:06.196
all of our sensitive
data in our configuration

00:26:06.196 --> 00:26:07.862
before we even start our pods.

00:26:07.862 --> 00:26:09.570
So I'm actually going
to create that now.

00:26:23.834 --> 00:26:25.500
So let's look at the
state of the world.

00:26:28.641 --> 00:26:29.141
Oops.

00:26:32.841 --> 00:26:34.590
If we look at everything
we have going on,

00:26:34.590 --> 00:26:36.010
we have our monolith
that we created before

00:26:36.010 --> 00:26:37.210
with three replicas.

00:26:37.210 --> 00:26:40.260
We have the hello service
with seven replicas.

00:26:40.260 --> 00:26:45.190
We have a front end and we
have an off system, one each.

00:26:45.190 --> 00:26:54.960
If we were to look at the
services we have created,

00:26:54.960 --> 00:26:56.010
we have our monolith.

00:26:56.010 --> 00:26:59.851
And our front end is still
loading up its external IP.

00:26:59.851 --> 00:27:02.100
And once it does, we'll be
able to send traffic to it.

00:27:02.100 --> 00:27:03.910
And it's going to know how to
communicate with its back ends

00:27:03.910 --> 00:27:04.619
that were set up.

00:27:04.619 --> 00:27:06.618
Kubernetes handled that
for us so we didn't have

00:27:06.618 --> 00:27:07.870
to manually hook all that up.

00:27:07.870 --> 00:27:08.680
That's pretty cool.

00:27:08.680 --> 00:27:10.930
So I'm actually going to
wait for that to come online.

00:27:16.979 --> 00:27:18.520
And while I'm waiting,
I'm just going

00:27:18.520 --> 00:27:20.478
to change the number of
pods we have right now.

00:27:26.547 --> 00:27:28.130
So I'm going to say
my hello service--

00:27:28.130 --> 00:27:29.710
I think that seven is too many.

00:27:35.447 --> 00:27:37.030
So I'm going to
change that to be six.

00:27:47.240 --> 00:27:49.201
Does anyone here use vim?

00:27:49.201 --> 00:27:51.484
I need some of you guys
to teach me this after.

00:27:51.484 --> 00:27:54.388
[LAUGHTER]

00:27:59.720 --> 00:28:02.820
None of that.

00:28:02.820 --> 00:28:03.700
So, sorry.

00:28:03.700 --> 00:28:10.890
So I'm going to apply that
change to my deployment file.

00:28:17.010 --> 00:28:17.510
Thank you.

00:28:21.720 --> 00:28:26.810
And now if I look at the number
of pods, it's hard to tell.

00:28:29.990 --> 00:28:32.514
Now, if I look at
the deployments,

00:28:32.514 --> 00:28:34.430
I can see that I have
six of that application.

00:28:34.430 --> 00:28:36.810
So scaling is really easy
with these deployment files.

00:28:39.460 --> 00:28:42.410
So let's actually send
some traffic that way.

00:28:42.410 --> 00:28:43.240
I have a front end.

00:28:46.380 --> 00:28:47.260
This one is secure.

00:28:58.490 --> 00:29:02.430
I got that very
familiar message back.

00:29:02.430 --> 00:29:04.800
So I'll be honest.

00:29:04.800 --> 00:29:07.310
At this point, I thought I knew
everything I needed to know.

00:29:07.310 --> 00:29:08.934
I knew how to break
an application down

00:29:08.934 --> 00:29:10.170
into smaller bits.

00:29:10.170 --> 00:29:12.650
I knew how to package that into
application containers that

00:29:12.650 --> 00:29:14.350
could be shipped
and distributed.

00:29:14.350 --> 00:29:17.760
And I knew how to use an
automation tool to automate

00:29:17.760 --> 00:29:19.680
and monitor everything.

00:29:19.680 --> 00:29:21.880
What more was there
for me to learn?

00:29:21.880 --> 00:29:24.380
Well, someone asked me,
what about code updates?

00:29:24.380 --> 00:29:26.480
What if the intern Bob
is coding blindfolded

00:29:26.480 --> 00:29:29.030
again and he ships bad code out?

00:29:29.030 --> 00:29:31.030
It happens.

00:29:31.030 --> 00:29:33.120
Well, in that case, we
want a deployment story

00:29:33.120 --> 00:29:35.600
that's going to let us roll
out updates cautiously,

00:29:35.600 --> 00:29:37.190
but roll back
instantly if we need

00:29:37.190 --> 00:29:39.420
to if something goes wrong.

00:29:39.420 --> 00:29:40.940
Some best practices
in this field

00:29:40.940 --> 00:29:42.950
have turned out to be
blue-green deployments

00:29:42.950 --> 00:29:44.650
or canary deployment.

00:29:44.650 --> 00:29:46.270
And you do that with Kubernetes.

00:29:46.270 --> 00:29:48.457
But what I want to show
you is our rolling update.

00:29:48.457 --> 00:29:50.290
But as a best practice,
we want to make sure

00:29:50.290 --> 00:29:55.090
that we have a way
to deploy cautiously.

00:29:55.090 --> 00:29:58.190
So on our deployments,
if you push on an update,

00:29:58.190 --> 00:29:58.940
there's an option.

00:29:58.940 --> 00:30:01.030
And we could say we
want a rolling update.

00:30:01.030 --> 00:30:02.890
And what that'll let
us do is gradually

00:30:02.890 --> 00:30:05.605
scale up a new version of our
application, send traffic to it

00:30:05.605 --> 00:30:08.370
at the same time as our old
application is getting traffic,

00:30:08.370 --> 00:30:10.150
and then gradually
roll down the other one

00:30:10.150 --> 00:30:12.610
as we're scaling
up the new version.

00:30:12.610 --> 00:30:14.990
And what this lets us do
is if something goes wrong,

00:30:14.990 --> 00:30:16.060
we can minimize downtime.

00:30:16.060 --> 00:30:18.393
Because we still have our own
version of the application

00:30:18.393 --> 00:30:19.040
in production.

00:30:19.040 --> 00:30:22.600
So if I ran the command, I could
spin up my new application,

00:30:22.600 --> 00:30:23.910
goodbye.

00:30:23.910 --> 00:30:25.040
I'd start getting traffic.

00:30:25.040 --> 00:30:27.510
So now both are getting
traffic at the same time.

00:30:27.510 --> 00:30:30.190
And I gradually start
spinning down the old one

00:30:30.190 --> 00:30:32.440
until I have whatever the
desired state of the new one

00:30:32.440 --> 00:30:34.610
was-- in this case, three.

00:30:34.610 --> 00:30:35.840
So let's check that out.

00:30:47.050 --> 00:30:51.017
So this time, what
we're going to do

00:30:51.017 --> 00:30:52.475
is we're going to
change some code.

00:31:01.242 --> 00:31:02.950
And since we're near
the end of the talk,

00:31:02.950 --> 00:31:04.730
instead of saying
hello, let's say

00:31:04.730 --> 00:31:07.665
goodbye, which makes
me kind of sad, guys.

00:31:12.975 --> 00:31:14.600
We're going to rebuild
our application.

00:31:19.624 --> 00:31:21.040
And once that's
built, we're going

00:31:21.040 --> 00:31:23.760
to package this into a Docker
container and ship it back out.

00:31:27.160 --> 00:31:29.652
From there, we use Kubernetes
to roll out this new change.

00:31:29.652 --> 00:31:30.610
And we can see it live.

00:31:43.880 --> 00:31:45.580
So we're going to make 2.0.

00:31:45.580 --> 00:31:47.434
So far, we've been
using hello 1.0.

00:31:47.434 --> 00:31:48.725
I created those images earlier.

00:31:53.310 --> 00:31:53.810
Oops.

00:31:58.150 --> 00:32:00.525
While that's writing, I'm
going to start a little script.

00:32:19.360 --> 00:32:23.640
All this script does is it just
constantly pings the endpoint.

00:32:23.640 --> 00:32:25.340
There's no magic here.

00:32:25.340 --> 00:32:29.144
I'm grabbing the front
end's external IP address.

00:32:29.144 --> 00:32:31.060
And then I'm just going
to constantly curl it.

00:32:37.447 --> 00:32:39.155
So right now, I'm
getting hello messages.

00:32:42.130 --> 00:32:57.520
Once I push this
change-- so we're

00:32:57.520 --> 00:32:59.270
going to push this
change up to the cloud.

00:33:08.701 --> 00:33:10.200
And then what I'm
going to do is I'm

00:33:10.200 --> 00:33:12.534
going to actually modify
our deployment file.

00:33:12.534 --> 00:33:14.450
This time, I'm going to
tell the hello service

00:33:14.450 --> 00:33:26.100
to use a different
version of-- this time I'm

00:33:26.100 --> 00:33:37.969
going to tell the hello pod
container to use version 2.0.

00:33:37.969 --> 00:33:39.510
Once I apply that
change, we're going

00:33:39.510 --> 00:33:41.343
to start seeing the new
one getting traffic.

00:33:49.085 --> 00:33:50.210
So this takes a little bit.

00:33:50.210 --> 00:33:51.360
This is a gradual process.

00:33:51.360 --> 00:33:53.840
It's not instantaneous.

00:33:53.840 --> 00:33:57.850
What we'll start seeing
is goodbye message

00:33:57.850 --> 00:33:58.840
getting sent out.

00:33:58.840 --> 00:34:00.577
So we're getting
a few right now.

00:34:00.577 --> 00:34:02.160
Over the next few
seconds, we're going

00:34:02.160 --> 00:34:04.361
to see more and more
until eventually, all

00:34:04.361 --> 00:34:05.610
we're getting back is goodbye.

00:34:05.610 --> 00:34:07.764
So we just did a live
update of our code.

00:34:07.764 --> 00:34:09.305
It's a very small
sample application.

00:34:09.305 --> 00:34:10.650
But there was no down time.

00:34:13.730 --> 00:34:15.870
Now, if something
happened, and maybe I

00:34:15.870 --> 00:34:19.330
didn't want to tell
you guys goodbye,

00:34:19.330 --> 00:34:21.845
I could undo that roll out.

00:34:31.667 --> 00:34:33.750
And we're seeing the traffic
start to revert back.

00:34:33.750 --> 00:34:37.290
So Kubernetes makes it very easy
to push out updates and changes

00:34:37.290 --> 00:34:41.309
to code and have that
taken care of for you.

00:34:41.309 --> 00:34:43.350
And now we're starting to
see some of our hellos.

00:34:43.350 --> 00:34:45.250
And eventually, this is
going to go back to the state

00:34:45.250 --> 00:34:47.375
that we had before where
it's all the hello version

00:34:47.375 --> 00:34:48.389
of our application.

00:34:48.389 --> 00:34:51.530
And that's pretty cool.

00:34:51.530 --> 00:34:54.580
So to start wrap up, some
of the best practices

00:34:54.580 --> 00:34:56.920
we talked about-- there's
three major hurdles

00:34:56.920 --> 00:34:58.560
of getting an
application online.

00:34:58.560 --> 00:34:59.580
There's the application.

00:34:59.580 --> 00:35:01.830
We want to make sure to
package that in something that

00:35:01.830 --> 00:35:04.150
can be shipped everywhere.

00:35:04.150 --> 00:35:06.640
So we're going to use
containers as a best practice.

00:35:06.640 --> 00:35:09.410
For the infrastructure needed
to manage that complexity,

00:35:09.410 --> 00:35:11.452
we want to automate and
monitor whatever we can.

00:35:11.452 --> 00:35:13.077
And we want to make
sure that we're not

00:35:13.077 --> 00:35:15.601
locked into any set of tooling.

00:35:15.601 --> 00:35:18.100
Finally, we want to make sure
that once we're in production,

00:35:18.100 --> 00:35:20.680
we deploy cautiously with
care and that we can roll back

00:35:20.680 --> 00:35:22.030
when we need to.

00:35:22.030 --> 00:35:26.180
So Kubernetes gives
you all of this.

00:35:26.180 --> 00:35:29.100
Now, I learned these
things from an expert

00:35:29.100 --> 00:35:31.949
over four months
of making a course.

00:35:31.949 --> 00:35:33.490
But taking advice
from a guy on a bus

00:35:33.490 --> 00:35:36.280
is normally not good
for your career.

00:35:36.280 --> 00:35:38.090
So check out the
Scalable Microservices

00:35:38.090 --> 00:35:39.950
with Kubernetes course.

00:35:39.950 --> 00:35:43.510
Kelsey Hightower, myself,
Adrian Cockcroft are all in it.

00:35:43.510 --> 00:35:46.170
It's part of Udacity's
DevOps Nanodegree Program,

00:35:46.170 --> 00:35:47.839
which is just starting up.

00:35:47.839 --> 00:35:49.130
I've taken some of the classes.

00:35:49.130 --> 00:35:50.000
It's pretty cool.

00:35:50.000 --> 00:35:52.390
And in fact, it's
probably easier

00:35:52.390 --> 00:35:55.130
to just have the trailer
tell you instead of me.

00:35:55.130 --> 00:35:57.957
So I'm going to play that now.

00:35:57.957 --> 00:35:58.623
[VIDEO PLAYBACK]

00:35:58.623 --> 00:36:01.720
-Containers,
microservices, Kubernetes--

00:36:01.720 --> 00:36:04.680
these are a few of the hottest
buzzwords in scalable computing

00:36:04.680 --> 00:36:05.750
today.

00:36:05.750 --> 00:36:07.510
But what's driving the hype?

00:36:07.510 --> 00:36:10.395
In the last decade, user demand
for always-on applications

00:36:10.395 --> 00:36:12.550
have grown exponentially.

00:36:12.550 --> 00:36:15.270
Users expect their favorite
sites and favorite services

00:36:15.270 --> 00:36:18.450
to be up 24/7, no exceptions.

00:36:18.450 --> 00:36:20.910
Many developers are focused
on application patterns

00:36:20.910 --> 00:36:23.185
such as microservices
to meet this need.

00:36:23.185 --> 00:36:24.880
But what about
the infrastructure

00:36:24.880 --> 00:36:27.672
needed to support
these always-on apps?

00:36:27.672 --> 00:36:29.130
This course from
Google and Udacity

00:36:29.130 --> 00:36:32.330
is going to show you step by
step from theory to production

00:36:32.330 --> 00:36:34.210
how to meet and exceed
these demands using

00:36:34.210 --> 00:36:36.350
tools that are available today.

00:36:36.350 --> 00:36:39.650
Learn from industry experts
like Google's Kelsey Hightower

00:36:39.650 --> 00:36:43.060
and the former cloud architect
of Netflix, Adrian Cockcroft,

00:36:43.060 --> 00:36:45.600
about modern
application design, best

00:36:45.600 --> 00:36:48.080
practices for using Docker
to package and distribute

00:36:48.080 --> 00:36:50.840
containers, and how to
manage deployments and scale

00:36:50.840 --> 00:36:52.930
applications with Kubernetes.

00:36:52.930 --> 00:36:54.320
So what are you waiting for?

00:36:54.320 --> 00:36:56.280
If you want practical,
hands-on advice

00:36:56.280 --> 00:36:59.300
for containerizing your app
and managing it at scale,

00:36:59.300 --> 00:37:01.526
sign up today.

00:37:01.526 --> 00:37:02.747
[END PLAYBACK]

00:37:02.747 --> 00:37:04.830
CARTER MORGAN: So I'm
really proud of that course.

00:37:04.830 --> 00:37:07.570
And I don't know if you guys
saw the magical fist bump,

00:37:07.570 --> 00:37:11.980
but that's how you feel inside
if you take this course after.

00:37:11.980 --> 00:37:16.600
So to wrap up, I
just want to say

00:37:16.600 --> 00:37:20.570
Kubernetes is a container
automation framework

00:37:20.570 --> 00:37:21.800
system that's flexible.

00:37:21.800 --> 00:37:23.320
It's open.

00:37:23.320 --> 00:37:24.650
It's very powerful.

00:37:24.650 --> 00:37:26.650
And it lets you
tell your own story.

00:37:26.650 --> 00:37:28.520
So this was my Kubernetes story.

00:37:28.520 --> 00:37:30.530
I definitely want to hear yours.

00:37:30.530 --> 00:37:32.280
I'm not going to have
time for questions.

00:37:32.280 --> 00:37:34.692
So you can hit me up
on Twitter always.

00:37:34.692 --> 00:37:35.900
I do want to hear your story.

00:37:35.900 --> 00:37:37.955
Or you can find us at
the sandbox outside.

00:37:37.955 --> 00:37:39.413
Thanks for checking
that out, guys.

00:37:39.413 --> 00:37:41.630
[APPLAUSE]

00:37:41.630 --> 00:37:45.600
[MUSIC PLAYING]

