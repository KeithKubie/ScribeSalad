WEBVTT
Kind: captions
Language: en

00:00:00.580 --> 00:00:04.120
In the example in the quiz,
the math says the result should be 1.0.

00:00:04.120 --> 00:00:06.851
But the code says 0.95.

00:00:06.851 --> 00:00:09.080
That's a big difference.

00:00:09.080 --> 00:00:12.130
Go ahead, replace the one
billion with just one, and

00:00:12.130 --> 00:00:14.840
you'll see that the error becomes tiny.

00:00:14.840 --> 00:00:17.610
We're going to want the values
involved in the calculation of this

00:00:17.610 --> 00:00:23.270
big loss function that we care about
to never get too big or too small.

00:00:23.270 --> 00:00:26.230
One good guiding principle
is that we always want our

00:00:26.230 --> 00:00:31.300
variables to have zero mean and
equal variance whenever possible.

00:00:31.300 --> 00:00:33.110
On top of the numeric issues,

00:00:33.110 --> 00:00:37.100
there are also really good mathematical
reasons to keep values you compute.

00:00:37.100 --> 00:00:42.150
Roughly around a mean of zero, an equal
variance when you're doing optimization.

00:00:42.150 --> 00:00:45.060
A badly conditioned problem means
that the optimizer has to do

00:00:45.060 --> 00:00:48.290
a lot of searching to go and
find a good solution.

00:00:48.290 --> 00:00:50.980
A well conditioned problem
makes it a lot easier for

00:00:50.980 --> 00:00:53.030
the optimizer to do its job.

00:00:53.030 --> 00:00:55.760
If your dealing with images,
it's simple.

00:00:55.760 --> 00:00:59.000
You can take the pixel values of your
image, they are typically between 0 and

00:00:59.000 --> 00:00:59.710
255.

00:00:59.710 --> 00:01:03.750
And simply subtract 128 and
divide by 128.

00:01:03.750 --> 00:01:07.610
It doesn't change the content of your
image, but it makes it much easier for

00:01:07.610 --> 00:01:09.390
the optimization to proceed numerically.

00:01:10.560 --> 00:01:13.730
You also want your weights and
biases to be initialized at

00:01:13.730 --> 00:01:16.830
a good enough starting point for
the gradient descent to proceed.

00:01:16.830 --> 00:01:20.530
There are lots of fancy schemes to
find good initialization values, but

00:01:20.530 --> 00:01:23.940
we're going to focus on a simple,
general method.

00:01:23.940 --> 00:01:27.860
Draw the weights randomly from a
Gaussian distribution with mean zero and

00:01:27.860 --> 00:01:29.100
standard deviation sigma.

00:01:30.190 --> 00:01:34.300
The sigma value determines the order
of magnitude of your outputs

00:01:34.300 --> 00:01:37.500
at the initial point
of your optimization.

00:01:37.500 --> 00:01:40.770
Because of the soft max on top of it,
the order of magnitude

00:01:40.770 --> 00:01:45.510
also determines the peakiness of your
initial probability distribution.

00:01:45.510 --> 00:01:49.310
A large sigma means that your
distribution will have large peaks.

00:01:49.310 --> 00:01:51.350
It's going to be very opinionated.

00:01:51.350 --> 00:01:54.370
A small sigma means that your
distribution is very uncertain

00:01:54.370 --> 00:01:55.290
about things.

00:01:55.290 --> 00:01:58.930
It's usually better to begin with
an uncertain distribution and

00:01:58.930 --> 00:02:02.490
let the optimization become more
confident as the train progress.

00:02:02.490 --> 00:02:05.180
So use a small sigma to begin with.

00:02:05.180 --> 00:02:05.740
Okay, so

00:02:05.740 --> 00:02:09.830
now we actually have everything we need
to actually train this classifier.

00:02:09.830 --> 00:02:13.460
We've got our training data, which
is normalized to have zero mean and

00:02:13.460 --> 00:02:15.220
unit variance.

00:02:15.220 --> 00:02:20.180
We multiply it by a large matrix, which
is initialized with random weights.

00:02:20.180 --> 00:02:24.330
We apply the soft max and
then the cross entropy loss and

00:02:24.330 --> 00:02:28.180
we calculate the average of this
loss over the entire training data.

00:02:29.360 --> 00:02:31.940
Then our magical optimization
package computes

00:02:31.940 --> 00:02:36.680
the derivative of this loss with respect
to the weights and to the biases and

00:02:36.680 --> 00:02:40.920
takes a step back in the direction
opposite to that derivative.

00:02:40.920 --> 00:02:42.541
And then we start all over again, and

00:02:42.541 --> 00:02:45.430
repeat the process until we reach
a minimum of the loss function.

