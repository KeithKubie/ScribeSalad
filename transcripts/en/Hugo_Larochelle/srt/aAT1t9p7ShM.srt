1
00:00:00,580 --> 00:00:00,730
Okay.

2
00:00:00,730 --> 00:00:04,480
And this video will introduce the idea
of parameters sharing and convolutional

3
00:00:04,481 --> 00:00:05,170
neural networks.

4
00:00:05,170 --> 00:00:06,003
Yes.

5
00:00:08,430 --> 00:00:10,190
So we've seen before the,
uh,

6
00:00:10,200 --> 00:00:14,370
idea of local connectivity that
convolutional neural networks a leverage.

7
00:00:14,430 --> 00:00:16,710
And now we're ready to talk about the,
uh,

8
00:00:16,711 --> 00:00:21,300
an additional idea of parameter
sharing across hidden units.

9
00:00:23,760 --> 00:00:24,593
Yeah.

10
00:00:24,780 --> 00:00:25,091
All right.

11
00:00:25,091 --> 00:00:29,890
So this second idea that we're adding
on top of local connectivity is that,

12
00:00:30,070 --> 00:00:34,000
uh, we will be sharing the parameters.
So the way that connections,

13
00:00:34,001 --> 00:00:38,440
the values of the weights
of the connections between
the hidden units and the

14
00:00:38,441 --> 00:00:42,580
units and the layer below, uh,
between, uh, certain units.

15
00:00:43,150 --> 00:00:46,760
Specifically what we'll do is
that we'll take all of our, uh,

16
00:00:46,780 --> 00:00:51,780
hidden units in some given hidden there
and we will segment the hidden units

17
00:00:52,120 --> 00:00:54,730
into different so-called feature maps.

18
00:00:55,510 --> 00:01:00,190
The units in the feature map will actually
share their parameters. That is the,

19
00:01:00,250 --> 00:01:00,610
uh,

20
00:01:00,610 --> 00:01:05,610
value of the way that connection between
a hidden unit and it's receptive field.

21
00:01:06,360 --> 00:01:10,740
Uh, it's receptive to input
from the previous layer, uh,

22
00:01:10,900 --> 00:01:15,280
will be exactly the same. It will be the
same values have the same connections.

23
00:01:16,660 --> 00:01:21,550
And, uh, now the hidden units
within a same feature map,

24
00:01:21,551 --> 00:01:26,140
they will distinguish themselves by
being connected at a different position.

25
00:01:26,140 --> 00:01:29,920
So a different receptive fields centered
at a different possession in the

26
00:01:30,160 --> 00:01:34,090
previous layer. So, uh,
specifically, normally,

27
00:01:34,510 --> 00:01:39,490
uh, okay. So if we have a hidden
layer with three feature maps,

28
00:01:39,491 --> 00:01:43,390
one, two, and three, then say
for the first feature map,

29
00:01:44,050 --> 00:01:45,160
all of these hidden units,

30
00:01:45,161 --> 00:01:49,300
we'll be using the same
matrix of connections.

31
00:01:49,810 --> 00:01:53,210
Uh, so for instance, decision unit, uh,

32
00:01:53,230 --> 00:01:57,990
imagine there were two pixels here
than it would be, I would require a,

33
00:01:58,000 --> 00:02:00,440
a matrix two by two,
uh,

34
00:02:00,490 --> 00:02:05,490
specifying the connection
between a dissident unit and
each of the pixels within

35
00:02:05,711 --> 00:02:06,550
the receptive fields.

36
00:02:07,120 --> 00:02:11,900
And now say this hidden unit
connected with that, uh,

37
00:02:12,040 --> 00:02:12,791
receptive field,

38
00:02:12,791 --> 00:02:17,791
which to simplify things would be just
two by two would use the same matrix of

39
00:02:17,981 --> 00:02:20,710
weighted connections to
compute it's pre activation.

40
00:02:21,520 --> 00:02:25,270
And similarly for that in unit,
also within the feature map,

41
00:02:25,271 --> 00:02:29,620
we'll assume that we will actually
have a receptive field at each possible

42
00:02:29,621 --> 00:02:33,220
position, uh, within the image. Uh,

43
00:02:33,240 --> 00:02:37,780
there are extensions of convolutional
neural networks that are consider having

44
00:02:37,781 --> 00:02:41,650
skipping certain, uh, receptive fields.
But just to simplify the description,

45
00:02:41,680 --> 00:02:44,620
always assume where, uh, connecting, uh,

46
00:02:44,650 --> 00:02:48,340
a feature map to all possible
receptive fields within the image.

47
00:02:49,750 --> 00:02:53,890
And so when this feature map within
this feature and when connect, uh,

48
00:02:54,070 --> 00:02:56,440
computing the reactivation
of all hidden units,

49
00:02:56,770 --> 00:02:59,980
what we'll do is we'll use the
same set of parameters, compute it.

50
00:03:00,190 --> 00:03:04,600
And so that hidden layer with
multiply this weight here by December,

51
00:03:04,601 --> 00:03:07,060
put less this weight here by this input,

52
00:03:07,390 --> 00:03:11,410
does this weight here times disempowered
and plus this weight here at time,

53
00:03:11,440 --> 00:03:12,273
this input.

54
00:03:12,520 --> 00:03:17,160
And then this hidden unit here would
do a with compute this input here,

55
00:03:17,190 --> 00:03:21,690
times this weight, this input
here at times this weight, uh,

56
00:03:21,940 --> 00:03:25,070
with at times this way and this
input here times this week.

57
00:03:25,090 --> 00:03:27,850
So really I want to emphasize
that we're using the same matrix,

58
00:03:27,880 --> 00:03:31,330
the same set of parameters,
uh, for this, uh,

59
00:03:31,331 --> 00:03:33,430
this hidden unit and this hidden here.

60
00:03:34,450 --> 00:03:36,280
And now I'll say in
the second feature map,

61
00:03:36,340 --> 00:03:40,810
it would be using a different matrix of
parameters. So it would have its own, uh,

62
00:03:40,840 --> 00:03:45,150
it's own two by two matrix of parameters.
And then when it's computing, uh,

63
00:03:45,340 --> 00:03:46,610
say this hidden Munis computing,

64
00:03:46,620 --> 00:03:48,790
because activation based
on that receptive field,

65
00:03:48,970 --> 00:03:51,400
instead of multiplying
the inputs by this matrix,

66
00:03:51,530 --> 00:03:53,920
multiply its input by that matrix instead.

67
00:03:54,820 --> 00:03:57,130
And then same thing for
the third feature map.

68
00:03:59,420 --> 00:04:04,250
So I'll use the notation
Wij for refereeing to these
matrices of connection.

69
00:04:04,400 --> 00:04:09,400
So the Matrix that connects the iff input
channel in the previous layer with DJF

70
00:04:10,310 --> 00:04:13,900
feature map in the hidden
layer will be noted as w Ij.

71
00:04:13,940 --> 00:04:16,640
So WIP is actually a matrix here.

72
00:04:18,360 --> 00:04:22,840
And uh, so this, for
instance, would be w, uh,

73
00:04:22,860 --> 00:04:26,790
say we have just one, a one channel
in the input when Dotson color,

74
00:04:26,791 --> 00:04:30,870
imagine it was, uh, in gray.
So this would be w one,

75
00:04:31,350 --> 00:04:35,160
one. This matrix here would be w one, two,

76
00:04:35,161 --> 00:04:39,810
and then there'd be another matrix
here, presumably, uh, with, uh,

77
00:04:39,840 --> 00:04:41,850
which would correspond to one three.

78
00:04:45,960 --> 00:04:46,750
So

79
00:04:46,750 --> 00:04:47,161
I didn't,

80
00:04:47,161 --> 00:04:51,940
parameter sharing allows us to reduce the
number of parameters even more because

81
00:04:51,941 --> 00:04:55,270
now the different hidden units connected
at different positions actually have

82
00:04:55,271 --> 00:04:58,300
the same set of parameters that we all
need to have for each feature map to

83
00:04:58,301 --> 00:05:00,130
store one matrix of connections.

84
00:05:01,610 --> 00:05:06,020
And also it has the nice property that
effectively within one feature map will

85
00:05:06,021 --> 00:05:10,880
be extracting the same feature,
but at every position in the input.

86
00:05:11,390 --> 00:05:14,780
So say feature map
corresponded to a it filter.

87
00:05:14,781 --> 00:05:16,930
If we were to visualize it that uh,

88
00:05:17,090 --> 00:05:22,090
looked something like a positive weights
here and then some negative weights in

89
00:05:22,641 --> 00:05:26,520
that region.
Now it would mean that we would take a,

90
00:05:26,540 --> 00:05:28,070
we'll be extracting that feature,

91
00:05:28,071 --> 00:05:31,460
which is essentially detecting whether
there is an edge at this particular

92
00:05:31,461 --> 00:05:36,461
position within the receptive field will
be detecting that feature everywhere in

93
00:05:37,130 --> 00:05:41,240
the input because we'll be
comparing that, uh, set of, uh,

94
00:05:41,270 --> 00:05:46,010
that filters centrally that a set of
connections with that receptive field,

95
00:05:46,011 --> 00:05:49,940
but also the same filter with
this other receptive feeling,

96
00:05:49,970 --> 00:05:54,680
every receptive fields, uh, in the
image. And now maybe feature map too.

97
00:05:54,770 --> 00:05:59,270
So, okay. So in feature map one,
we extracting the actual feature,

98
00:05:59,271 --> 00:06:04,030
but everywhere. Whereas feature
map do might be extracting say, uh,

99
00:06:04,040 --> 00:06:08,960
that feature. So more like a,
perhaps a, uh, uh, horizontal,

100
00:06:09,540 --> 00:06:12,080
uh, uh, action. And
that's going like this.

101
00:06:12,470 --> 00:06:16,310
And in feature map three could be
something different like a diagonal.

102
00:06:19,540 --> 00:06:22,810
So, um, to describe this property,

103
00:06:22,811 --> 00:06:26,620
that feature map one is
extracting the same feature. Uh,

104
00:06:26,770 --> 00:06:31,150
it turns out we can say that the feature
map before to visualize it as an image

105
00:06:31,380 --> 00:06:35,170
would, uh, correspond to tens
formation. That is equity variant.

106
00:06:35,200 --> 00:06:38,530
That is if we, uh, once we, uh,

107
00:06:38,560 --> 00:06:43,560
say we take this input image and we apply
a transformation like a translation,

108
00:06:45,220 --> 00:06:47,650
well in the visualization
that feature map,

109
00:06:47,651 --> 00:06:50,080
we'll actually see exactly
the same transformation.

110
00:06:50,190 --> 00:06:54,310
That pattern of activations
will be translated based
on the same transformation

111
00:06:54,490 --> 00:06:58,750
as the one we applied in
the original input. For that
reason, we'll say that, uh,

112
00:06:59,060 --> 00:07:01,420
the, uh, feature map will be equivalent.

113
00:07:03,830 --> 00:07:04,663
Yeah.

114
00:07:04,920 --> 00:07:09,870
So putting the local connectivity and the
private or sharing, uh, ideas together,

115
00:07:10,530 --> 00:07:15,120
uh, then it means that we computing these
feature maps and these feature maps,

116
00:07:15,210 --> 00:07:19,170
actually the computation
of these feature maps, uh,

117
00:07:19,200 --> 00:07:23,700
actually corresponds to a
computing aid discreet convolution,

118
00:07:24,360 --> 00:07:26,820
which is a notion taken
from signal processing.

119
00:07:27,300 --> 00:07:30,660
This asterisk symbol is the
symbol for that operation.

120
00:07:31,440 --> 00:07:33,060
And so computing the feature maps,

121
00:07:33,061 --> 00:07:37,290
which are the pre activations for
the hidden layer, will correspond to,

122
00:07:37,380 --> 00:07:38,213
uh,

123
00:07:38,220 --> 00:07:43,220
doing a discrete convolution with a
channel from the previous layer using a

124
00:07:43,831 --> 00:07:48,810
kernel matrix,
which we note that Ski Ij where I is the,

125
00:07:48,850 --> 00:07:51,260
uh, uh, index of the
channel, the previous layer,

126
00:07:51,261 --> 00:07:53,760
and Jay's the index of
the feature map again.

127
00:07:54,300 --> 00:07:59,160
And that kernel is computed from
the, uh, hidden weights matrix.

128
00:07:59,161 --> 00:08:03,650
W Ij, and specifically I'll
just correspond to a, uh,

129
00:08:03,730 --> 00:08:07,830
Wij, but where we flipped the rows
and the columns in that Matrix.

130
00:08:08,610 --> 00:08:09,930
So in the next videos,

131
00:08:09,931 --> 00:08:12,910
I'll talk more precisely about what
we mean by discreet contribution.

132
00:08:13,050 --> 00:08:17,400
For those that are not familiar
with what it is, but, uh,

133
00:08:17,401 --> 00:08:22,110
just ignore that. For now, it means
that we're ready to look at, uh,

134
00:08:22,140 --> 00:08:26,400
what would happen if we were to
compute the activation of hidden layer.

135
00:08:26,970 --> 00:08:28,720
So, um, uh,

136
00:08:28,770 --> 00:08:33,420
specifically I'm using here the notation
from this paper by Jared at all from

137
00:08:33,421 --> 00:08:36,870
Nyu. And, um, so if we call x,

138
00:08:36,930 --> 00:08:40,880
I d I have input channel from
the previous there, Kay. I,

139
00:08:40,881 --> 00:08:42,900
Jay is going to be this
convolution kernel,

140
00:08:42,901 --> 00:08:46,740
which is a function of
the weight matrix and a,

141
00:08:46,741 --> 00:08:51,741
and so we essentially
consistently information about
the connections between the

142
00:08:51,751 --> 00:08:54,900
IOC channel of input,
uh, and the GF feature.

143
00:08:56,160 --> 00:09:00,630
They'll also introduce,
uh, these, uh, GJS here,

144
00:09:00,631 --> 00:09:03,630
which are learned scaling factors.
And uh,

145
00:09:03,650 --> 00:09:07,560
in their papers they note as Yj,
uh, the value of the activation,

146
00:09:07,561 --> 00:09:09,800
that hidden layer.
Uh,

147
00:09:09,801 --> 00:09:14,160
and so what they considered in that
paper for how they would compute the

148
00:09:14,161 --> 00:09:19,161
activation of fit and layer is that they
would compute the pre activations using

149
00:09:20,011 --> 00:09:25,011
a compilation from each of the IMF
input channel to Djf a feature map,

150
00:09:27,090 --> 00:09:30,990
and then they would some across
the, uh, input channels. So for,

151
00:09:30,991 --> 00:09:34,770
we compute the P activations coming from
each input channel that we sum all the

152
00:09:34,771 --> 00:09:39,150
p activations, then you
apply some nonlinearity in
their paper, they chose 10 h,

153
00:09:39,151 --> 00:09:42,960
but we could have used something else.
And they also decided to introduce this,

154
00:09:42,961 --> 00:09:46,630
uh, a Gj learn factor, which, uh,

155
00:09:46,680 --> 00:09:50,400
just in general for commercial neural
network is not something that's necessary,

156
00:09:50,401 --> 00:09:52,140
but that's something they
decided to introduce.

157
00:09:52,950 --> 00:09:57,950
And a that will correspond to Yj j which
is the activation a corresponding to

158
00:09:59,371 --> 00:10:04,020
the JFF feature maps. So we
will have a j here will, uh,

159
00:10:04,440 --> 00:10:07,770
be from one to the number of feature
map to want to have in our hidden layer.

160
00:10:08,220 --> 00:10:12,870
And so each why Jay is one
of these feature maths, but
after the nonlinearity, uh,

161
00:10:12,900 --> 00:10:14,760
such as the Tan h or the sigmoid,

162
00:10:14,860 --> 00:10:19,530
a nonlinearity or any other nonlinearity
we've seen in regular people or neural

163
00:10:19,531 --> 00:10:24,390
networks. So are, we could have
decided to introduce a bias here. Uh,

164
00:10:24,410 --> 00:10:27,910
so we could have added a bias. A, a, B,

165
00:10:27,970 --> 00:10:32,970
a j would have a separate bias in a
unique bias shared by all units across the

166
00:10:33,261 --> 00:10:36,780
feature map. That's another variation.
So if you look at the literature,

167
00:10:36,781 --> 00:10:41,550
there will be variations, uh, with
respect to this particular equation.

168
00:10:41,551 --> 00:10:43,410
But the general idea remains the same.

169
00:10:43,411 --> 00:10:47,510
We're computing feature maps and
often these feature maps are, uh,

170
00:10:47,760 --> 00:10:49,530
we then apply a non linearity on it,

171
00:10:49,590 --> 00:10:53,640
like a sigmoid or a talent or a
rectified linear activation function.

172
00:10:54,630 --> 00:10:58,710
So in the next video,
I'll talk about this discrete convolution,

173
00:10:58,740 --> 00:10:59,491
which is going to be,

174
00:10:59,491 --> 00:11:04,230
this procedure will actually involve a
invoked when we're computing the feature

175
00:11:04,231 --> 00:11:04,920
maps.

176
00:11:04,920 --> 00:11:08,520
And it's actually important because there
are very efficient softwares that were

177
00:11:08,521 --> 00:11:12,600
written for performing this
convolution very efficiently.

178
00:11:12,601 --> 00:11:15,950
We'll be able to leverage
that, uh, when we computed,

179
00:11:15,960 --> 00:11:19,500
we're doing for propagation and
the compulsion or a neural network.

180
00:11:20,310 --> 00:11:23,250
And also it's important because
it makes it explicit. Now,

181
00:11:23,251 --> 00:11:28,251
what's the convolutional a word when
we're talking about what we say that these

182
00:11:28,471 --> 00:11:29,700
networks are convolutional.

