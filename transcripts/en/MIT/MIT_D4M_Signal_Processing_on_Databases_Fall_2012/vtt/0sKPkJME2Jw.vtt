WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.077
The following content is
provided under a Creative

00:00:03.077 --> 00:00:03.819
Commons license.

00:00:03.819 --> 00:00:06.263
Your support will help
MIT OpenCourseWare

00:00:06.263 --> 00:00:10.070
continue to offer high-quality
educational resources for free.

00:00:10.070 --> 00:00:13.149
To make a donation or to
view additional materials

00:00:13.149 --> 00:00:18.508
from hundreds of MIT courses,
visit MIT OpenCourseWare

00:00:18.508 --> 00:00:26.060
at ocw.mit.edu.

00:00:26.060 --> 00:00:28.040
JEREMY KEPNER: All
right, welcome.

00:00:28.040 --> 00:00:31.660
Thank you so much for coming.

00:00:31.660 --> 00:00:34.480
I'm Jeremy Kepner.

00:00:34.480 --> 00:00:37.220
I'm a fellow at
Lincoln Laboratory.

00:00:37.220 --> 00:00:39.944
I lead the Supercomputing
Center there,

00:00:39.944 --> 00:00:43.576
which means I have the
privilege of working

00:00:43.576 --> 00:00:49.500
every day with pretty
much everyone at MIT.

00:00:49.500 --> 00:00:52.484
I think I have the
best job at MIT

00:00:52.484 --> 00:00:58.739
because I get to help you all
pursue your research dreams.

00:00:58.739 --> 00:01:02.634
And as a result of that,
I get an opportunity

00:01:02.634 --> 00:01:07.156
to see what a really wide
range of folks are doing

00:01:07.156 --> 00:01:12.799
and observe patterns between
what different folks are doing.

00:01:12.799 --> 00:01:15.360
So with that, I'll get started.

00:01:15.360 --> 00:01:19.380
This is meant to be some
initial motivational material,

00:01:19.380 --> 00:01:22.559
why you should be
interested in learning

00:01:22.559 --> 00:01:26.553
about this mathematics,
this mathematics of big data

00:01:26.553 --> 00:01:31.009
and how it relates to machine
learning and other really

00:01:31.009 --> 00:01:31.740
exciting topics.

00:01:31.740 --> 00:01:33.590
It is a math course.

00:01:33.590 --> 00:01:37.119
We will be going over
a fair amount of math.

00:01:37.119 --> 00:01:41.920
But we really work hard to make
it very accessible to people.

00:01:41.920 --> 00:01:47.332
So we start out with a really
elementary mathematical concept

00:01:47.332 --> 00:01:50.549
here, probably one that
hopefully most of you

00:01:50.549 --> 00:01:51.299
are familiar with.

00:01:51.299 --> 00:01:55.970
It's the basic concept
of a circle, right?

00:01:55.970 --> 00:01:59.882
And I bring that up
because many of us

00:01:59.882 --> 00:02:03.360
know many ways to state
this mathematically, right?

00:02:03.360 --> 00:02:07.026
It's all the points
that are equal distance

00:02:07.026 --> 00:02:08.860
from a particular point.

00:02:08.860 --> 00:02:10.610
There's other ways
to describe it.

00:02:10.610 --> 00:02:13.865
But this is a basic
mathematical concept of a circle

00:02:13.865 --> 00:02:16.470
that many of us
have grown up with.

00:02:16.470 --> 00:02:21.024
But, of course, the
other thing we know

00:02:21.024 --> 00:02:25.579
is that, right, this
is the big idea.

00:02:25.579 --> 00:02:29.842
Although I can write down an
equation for circle, which

00:02:29.842 --> 00:02:33.166
is the equation for a
perfect, ideal circle,

00:02:33.166 --> 00:02:37.000
we know that such things don't
actually exist in nature.

00:02:37.000 --> 00:02:42.459
There is no true perfect
circle in nature.

00:02:42.459 --> 00:02:45.069
Even this circle that we've
drawn here, it has pixels.

00:02:45.069 --> 00:02:47.201
If I zoomed in on it, if
I zoomed in on it enough,

00:02:47.201 --> 00:02:49.349
it wouldn't look
like a circle at all.

00:02:49.349 --> 00:02:52.050
It would look like
a series of blocks.

00:02:52.050 --> 00:02:54.702
And so that approximation
process, right,

00:02:54.702 --> 00:02:59.145
where we have a mathematical
concept of an ideal circle,

00:02:59.145 --> 00:03:03.217
right, but we know that
there are not really--

00:03:03.217 --> 00:03:06.615
they don't really
exist nature, but we

00:03:06.615 --> 00:03:10.591
understand that it is
worthwhile to think

00:03:10.591 --> 00:03:14.386
about these mathematical
ideals, manipulate them and then

00:03:14.386 --> 00:03:16.297
take the results
of the manipulation

00:03:16.297 --> 00:03:17.890
back into the real world.

00:03:17.890 --> 00:03:21.367
That's a really productive
way to think about things

00:03:21.367 --> 00:03:26.800
and, really, the basis for a
lot of what we do here at MIT.

00:03:26.800 --> 00:03:32.828
This concept is essentially
the basis of modern

00:03:32.828 --> 00:03:37.349
or ancient Western
thought on mathematics.

00:03:37.349 --> 00:03:39.595
If you remember your
history courses,

00:03:39.595 --> 00:03:42.590
this concept of ideal
shapes and ideal circles

00:03:42.590 --> 00:03:47.810
is the foundation of
platonic mathematics

00:03:47.810 --> 00:03:51.290
some 2,500 years ago.

00:03:51.290 --> 00:03:55.509
And at the time, though,
that they were developing

00:03:55.509 --> 00:03:58.322
that concept, this
idea that there

00:03:58.322 --> 00:04:01.307
are ideal shapes out there
and that thinking about them

00:04:01.307 --> 00:04:03.579
and manipulating them
was a more effective way

00:04:03.579 --> 00:04:09.349
to reason about the real world,
there was a lot of skepticism.

00:04:09.349 --> 00:04:11.033
You could imagine
2,500 years ago

00:04:11.033 --> 00:04:12.717
someone is walking
around and saying,

00:04:12.717 --> 00:04:15.036
I believe there are
these things called

00:04:15.036 --> 00:04:17.988
ideal circles and ideal
squares and ideal shapes.

00:04:17.988 --> 00:04:20.829
But they don't actually
exist in nature.

00:04:20.829 --> 00:04:22.839
That would probably
not be well-received.

00:04:22.839 --> 00:04:25.749
In fact, it was
not well-received.

00:04:25.749 --> 00:04:31.674
Many of those philosophers
who were thinking about this

00:04:31.674 --> 00:04:34.308
were very negatively received.

00:04:34.308 --> 00:04:36.744
And, in fact, if
you want to learn

00:04:36.744 --> 00:04:39.180
about how negative the
response was to this,

00:04:39.180 --> 00:04:43.211
I encourage you to go and read
the Allegory of the Cave, which

00:04:43.211 --> 00:04:45.623
is essentially the story of
these philosophers talking

00:04:45.623 --> 00:04:47.027
about how they're
trying to bring

00:04:47.027 --> 00:04:49.233
the light of this knowledge
to the broader world

00:04:49.233 --> 00:04:52.239
and how they essentially
get killed because of it,

00:04:52.239 --> 00:04:54.909
because people don't
want to see it.

00:04:54.909 --> 00:04:58.639
So that struggle they
experienced 2,500 years ago,

00:04:58.639 --> 00:05:00.039
it exists today.

00:05:00.039 --> 00:05:04.322
You as people at MIT will try
and bring mathematical concepts

00:05:04.322 --> 00:05:06.407
into environments
where people are like,

00:05:06.407 --> 00:05:07.740
I don't see why that's relevant.

00:05:07.740 --> 00:05:12.050
And you will experience
negative inputs.

00:05:12.050 --> 00:05:16.119
But you should rest assured
that this is a good bet.

00:05:16.119 --> 00:05:18.759
It's worked well for
thousands of years.

00:05:18.759 --> 00:05:20.789
You know, it's what
I base my career on.

00:05:20.789 --> 00:05:22.622
People ask me, well,
what's the basis of it?

00:05:22.622 --> 00:05:24.159
Well, I'm just
betting on math here.

00:05:24.159 --> 00:05:26.100
It's been a good tool.

00:05:26.100 --> 00:05:29.704
So this is why we're beginning
to think this way when

00:05:29.704 --> 00:05:32.999
we talk about big data
and machine learning.

00:05:32.999 --> 00:05:35.560
So really looking at
the fundamentals, what

00:05:35.560 --> 00:05:39.557
are the ideals that we need
in order to effectively reason

00:05:39.557 --> 00:05:41.928
about the problems
that we're facing today

00:05:41.928 --> 00:05:44.906
in the virtual world,
right, and the fact

00:05:44.906 --> 00:05:50.363
that this mathematical concept
described the natural world so

00:05:50.363 --> 00:05:54.422
well and also described
in the virtual world

00:05:54.422 --> 00:05:56.872
is sometimes called the
unreasonable effectiveness

00:05:56.872 --> 00:05:57.689
of mathematics.

00:05:57.689 --> 00:05:58.689
You can look that up.

00:05:58.689 --> 00:06:00.889
But people talk about math.

00:06:00.889 --> 00:06:04.539
Why does it do such a good job
of describing so many things?

00:06:04.539 --> 00:06:07.319
And people say, well,
they don't really know.

00:06:07.319 --> 00:06:11.379
But it seems to be a good bit of
luck that it happens that way.

00:06:11.379 --> 00:06:16.449
So circles, that gets
us a certain way.

00:06:16.449 --> 00:06:20.770
But in most of the
fields that we work with,

00:06:20.770 --> 00:06:25.096
and I would say that, in
almost any introductory course

00:06:25.096 --> 00:06:29.030
that you take in college,
whatever the discipline is,

00:06:29.030 --> 00:06:32.620
whether it be chemistry
or mechanical engineering

00:06:32.620 --> 00:06:36.917
or electrical engineering
or physics or biology,

00:06:36.917 --> 00:06:39.725
the basic fundamental
theoretical ideas

00:06:39.725 --> 00:06:43.590
that they will
introduce to you will be

00:06:43.590 --> 00:06:46.490
the concept of a linear model.

00:06:46.490 --> 00:06:50.939
So there we have a
linear model, right?

00:06:50.939 --> 00:06:53.850
And why do we like
linear models?

00:06:53.850 --> 00:06:55.058
And again, it can be physics.

00:06:55.058 --> 00:06:59.429
It can be as simple as F
= MA Or, in chemistry, it

00:06:59.429 --> 00:07:02.119
can be some kind of
chemical rate equation.

00:07:02.119 --> 00:07:04.873
Or in mechanical
engineering it can

00:07:04.873 --> 00:07:07.169
be basic concepts of friction.

00:07:07.169 --> 00:07:11.181
The reason we like these
basic linear models

00:07:11.181 --> 00:07:14.190
is because we can
project, right?

00:07:14.190 --> 00:07:17.283
I know that if that
solid line represents

00:07:17.283 --> 00:07:20.376
what I believe
to-- you know, if I

00:07:20.376 --> 00:07:23.287
have evidence to support
that that is correct,

00:07:23.287 --> 00:07:27.231
then I feel pretty good about
projecting maybe where I don't

00:07:27.231 --> 00:07:29.909
have data or into a new domain.

00:07:29.909 --> 00:07:33.308
So linear models allow
us to do this reasoning.

00:07:33.308 --> 00:07:36.749
And that's why in the
first few weeks of almost

00:07:36.749 --> 00:07:40.029
any introductory course they
begin with these linear models,

00:07:40.029 --> 00:07:43.509
because they have proven
to be so effective.

00:07:43.509 --> 00:07:47.645
Now, there are many
non-linear phenomena that

00:07:47.645 --> 00:07:50.009
are tremendously important, OK?

00:07:50.009 --> 00:07:54.239
And as a person who deals
with large-scale computation,

00:07:54.239 --> 00:07:58.529
those are a staple
of what people do.

00:07:58.529 --> 00:08:02.341
But in order to do non-linear
calculations or reason

00:08:02.341 --> 00:08:04.459
about things
non-linearly, it usually

00:08:04.459 --> 00:08:09.325
requires a much more complicated
analysis and much more

00:08:09.325 --> 00:08:11.489
computation, much more data.

00:08:11.489 --> 00:08:14.783
And so our ability
to extrapolate

00:08:14.783 --> 00:08:16.979
is very limited, OK?

00:08:16.979 --> 00:08:18.120
It's very limited.

00:08:18.120 --> 00:08:23.205
So here I am talking
about the benefits

00:08:23.205 --> 00:08:27.020
of thinking mathematically,
talking about linearity.

00:08:27.020 --> 00:08:32.679
What does this have to do with
big data and machine learning?

00:08:32.679 --> 00:08:37.096
So we would like to be able to
do the same things that we've

00:08:37.096 --> 00:08:41.795
been able to do in other
fields in this new emerging

00:08:41.795 --> 00:08:44.039
field of big data.

00:08:44.039 --> 00:08:47.399
And this often
deals with data that

00:08:47.399 --> 00:08:50.759
doesn't look like the
traditional measurements we

00:08:50.759 --> 00:08:53.350
see in science.

00:08:53.350 --> 00:08:58.778
This can be data that has to do
with words or images, pictures

00:08:58.778 --> 00:09:01.831
of people, other types
of things that don't feel

00:09:01.831 --> 00:09:04.043
like the kinds of data
that we traditionally

00:09:04.043 --> 00:09:06.899
deal with in science
and engineering.

00:09:06.899 --> 00:09:11.350
But we know we want
to use linear models.

00:09:11.350 --> 00:09:13.259
So how are we going to do that?

00:09:13.259 --> 00:09:15.689
How can we take this
concept of linearity,

00:09:15.689 --> 00:09:18.558
which has been so powerful
across so many disciplines,

00:09:18.558 --> 00:09:21.627
and bring them to
this field that

00:09:21.627 --> 00:09:25.394
just feels completely
different than the kinds data

00:09:25.394 --> 00:09:26.970
that we have?

00:09:26.970 --> 00:09:33.806
So to begin with, I need to
refresh for you what it really

00:09:33.806 --> 00:09:35.910
means to be linear.

00:09:35.910 --> 00:09:39.470
Before, I showed you a line
and, hence, the line, linear.

00:09:39.470 --> 00:09:43.720
But mathematically, linearity
means something much deeper.

00:09:43.720 --> 00:09:48.591
And so here's an equation
that you may have first seen

00:09:48.591 --> 00:09:49.920
in elementary school.

00:09:49.920 --> 00:09:52.316
We basically have
to two times three

00:09:52.316 --> 00:09:57.120
plus four is equal to two times
three plus two times four.

00:09:57.120 --> 00:09:59.250
That is called the
distributive property.

00:09:59.250 --> 00:10:03.810
It basically says multiplication
distributes over addition.

00:10:03.810 --> 00:10:06.198
And this is the
fundamental reason

00:10:06.198 --> 00:10:10.180
why I would say mathematics
works in our world, right?

00:10:10.180 --> 00:10:14.103
If this wasn't true very
early on in the earliest

00:10:14.103 --> 00:10:16.850
days of inventing
mathematics, it would not

00:10:16.850 --> 00:10:19.050
have been very useful, right?

00:10:19.050 --> 00:10:24.030
To say that I have two of three
plus four of something, OK,

00:10:24.030 --> 00:10:27.559
and then I can
change it and do it

00:10:27.559 --> 00:10:31.660
in this other way, that's really
what makes mathematics useful.

00:10:31.660 --> 00:10:36.562
And from a deeper perspective,
the distributive property

00:10:36.562 --> 00:10:40.240
is basically what
makes math linear.

00:10:40.240 --> 00:10:44.713
This is the property that,
if this property holds,

00:10:44.713 --> 00:10:48.689
then we can reason
about a system linearly.

00:10:48.689 --> 00:10:52.289
Now, you're very familiar
with this type of mathematics,

00:10:52.289 --> 00:10:54.690
but there's other
types of mathematics.

00:10:54.690 --> 00:10:57.457
So if you'll allow
me, hopefully you

00:10:57.457 --> 00:11:00.620
will let me just replace
those multiplication symbols

00:11:00.620 --> 00:11:04.879
and addition symbols with this
funny circle times and circle

00:11:04.879 --> 00:11:05.379
plus.

00:11:05.379 --> 00:11:08.199
And we'll get to why
I'm going to do that.

00:11:08.199 --> 00:11:10.938
Because it turns
out that, while you

00:11:10.938 --> 00:11:14.487
have done most of your careers
with traditional arithmetic

00:11:14.487 --> 00:11:16.580
multiplication and
addition, the kind

00:11:16.580 --> 00:11:19.930
you would do on your
calculator or have

00:11:19.930 --> 00:11:24.015
done in elementary
school, it turns out

00:11:24.015 --> 00:11:26.934
there's other
pairs of operations

00:11:26.934 --> 00:11:31.229
that also obey this property,
this distributive property,

00:11:31.229 --> 00:11:34.659
and, therefore, allow
us to potentially build

00:11:34.659 --> 00:11:38.530
linear models of very
different types of data

00:11:38.530 --> 00:11:39.980
using this property.

00:11:39.980 --> 00:11:43.545
So, as I mentioned,
the classic two

00:11:43.545 --> 00:11:48.639
are circle plus is just equal
to regular arithmetic addition,

00:11:48.639 --> 00:11:51.918
as we show on the first
line, and circle times is

00:11:51.918 --> 00:11:53.709
equal to regular
arithmetic multiplication.

00:11:53.709 --> 00:11:55.779
So those are the standard ones.

00:11:55.779 --> 00:11:59.665
And, by far, this pair,
this is the most common pair

00:11:59.665 --> 00:12:02.139
that we use across
the world today.

00:12:02.139 --> 00:12:03.550
But there are others.

00:12:03.550 --> 00:12:09.753
So, for instance, I can
replace the plus operation

00:12:09.753 --> 00:12:17.050
with max and the multiplication
operation with addition, OK?

00:12:17.050 --> 00:12:19.644
And the above
distributive equation

00:12:19.644 --> 00:12:21.720
will still hold, right?

00:12:21.720 --> 00:12:22.860
That's a little confusing.

00:12:22.860 --> 00:12:26.868
I often get confused that
multiplications is now

00:12:26.868 --> 00:12:27.370
addition.

00:12:27.370 --> 00:12:30.431
But this pair sometimes
referred to as max plus-- you'll

00:12:30.431 --> 00:12:34.205
sometimes hear about it as
max plus algebra-- is actually

00:12:34.205 --> 00:12:38.079
very important in machine
learning and neural networks.

00:12:38.079 --> 00:12:43.015
This is actually the back end
of the rectified linear unit,

00:12:43.015 --> 00:12:44.663
is essentially this operation.

00:12:44.663 --> 00:12:46.829
If you didn't understand
what that meant, that's OK.

00:12:46.829 --> 00:12:49.350
We'll get to that later.

00:12:49.350 --> 00:12:51.220
It's very important in finance.

00:12:51.220 --> 00:12:54.123
There are certain
finance operations

00:12:54.123 --> 00:12:58.189
that rely on this
type of mathematics.

00:12:58.189 --> 00:13:01.880
There are other pairs, also.

00:13:01.880 --> 00:13:02.880
So here's one.

00:13:02.880 --> 00:13:08.000
I can replace addition with
union and multiplication

00:13:08.000 --> 00:13:09.920
with intersection, right?

00:13:09.920 --> 00:13:15.850
Now, that also obeys
that linear property.

00:13:15.850 --> 00:13:18.430
This is essentially
the pair of operations

00:13:18.430 --> 00:13:21.380
that, anytime you make
a transaction and work

00:13:21.380 --> 00:13:24.663
with what's called a
relational database, that's

00:13:24.663 --> 00:13:28.689
the mathematical operation
pair that's sitting inside it.

00:13:28.689 --> 00:13:31.790
It's why those databases work.

00:13:31.790 --> 00:13:36.770
It allows us to reason about
queries, which are just

00:13:36.770 --> 00:13:39.759
a series of
intersections and unions,

00:13:39.759 --> 00:13:41.959
and then reorder
them in such a way.

00:13:41.959 --> 00:13:45.100
In databases, this is
called query planning.

00:13:45.100 --> 00:13:46.507
And if that property
wasn't true,

00:13:46.507 --> 00:13:48.149
we wouldn't be able to do that.

00:13:48.149 --> 00:13:51.079
So this is a deep
property of that.

00:13:51.079 --> 00:13:55.293
So we can put all different
types of pairs in here

00:13:55.293 --> 00:13:57.209
and reason about them linearly.

00:13:57.209 --> 00:14:01.587
And this is why that
many, many of the systems

00:14:01.587 --> 00:14:03.339
we use today work.

00:14:03.339 --> 00:14:06.005
And so this class is
about really exposing

00:14:06.005 --> 00:14:08.005
that, that, really,
the mathematics that

00:14:08.005 --> 00:14:11.769
allows us to think linearly
about data that we haven't

00:14:11.769 --> 00:14:14.056
really thought of
as maybe obeying

00:14:14.056 --> 00:14:16.080
some kind of linear model.

00:14:16.080 --> 00:14:20.720
This is essentially the
critical point of this class.

00:14:20.720 --> 00:14:24.339
So it goes beyond that, though.

00:14:24.339 --> 00:14:28.434
So hopefully you'll allow
me to replace those numbers

00:14:28.434 --> 00:14:29.800
with letters, right?

00:14:29.800 --> 00:14:35.189
So that's basic algebra there.

00:14:35.189 --> 00:14:38.105
Just for a refresher,
the previous equation,

00:14:38.105 --> 00:14:42.689
we had A = 2, B = 3, C = 4.

00:14:42.689 --> 00:14:50.075
But we're not limited to these
variables, or these letters,

00:14:50.075 --> 00:14:53.819
to being just simple
scalar numbers,

00:14:53.819 --> 00:14:55.944
in this case, real numbers
or integers or something

00:14:55.944 --> 00:14:56.444
like that.

00:14:56.444 --> 00:14:58.360
They can be other things, too.

00:14:58.360 --> 00:15:04.300
So, for instance, A, B, and
C could be spreadsheets.

00:15:04.300 --> 00:15:06.997
And that's something we'll
go over with extensively

00:15:06.997 --> 00:15:09.695
in a class, so that
I can basically

00:15:09.695 --> 00:15:13.767
have A, B, and C be whole
spreadsheets of data

00:15:13.767 --> 00:15:16.749
and the linear equation
will still hold.

00:15:16.749 --> 00:15:22.385
And, in fact, that's probably
the key concept in big data,

00:15:22.385 --> 00:15:26.142
is the necessity to
reason about data

00:15:26.142 --> 00:15:30.920
as whole collections and
transforming whole collections.

00:15:30.920 --> 00:15:34.972
Going and looking at things
one element at a time

00:15:34.972 --> 00:15:40.249
is essentially the thing that is
extremely difficult to do when

00:15:40.249 --> 00:15:43.910
you have large amounts of data.

00:15:43.910 --> 00:15:50.730
A, B, and C can be
database tables, right?

00:15:50.730 --> 00:15:52.720
Those don't differ too
much from spreadsheets.

00:15:52.720 --> 00:15:57.220
And as I talked to you
in the previous slide,

00:15:57.220 --> 00:16:00.262
that union/intersection
pair naturally lines up

00:16:00.262 --> 00:16:05.812
and we can reason
about whole tables

00:16:05.812 --> 00:16:10.569
in a database using
linear properties.

00:16:10.569 --> 00:16:11.939
They can be matrices.

00:16:11.939 --> 00:16:15.433
I think, for those of you
who have had a linear algebra

00:16:15.433 --> 00:16:17.502
and matrix mathematics,
that would have been

00:16:17.502 --> 00:16:21.180
the first example, right, when
I substituted the A, B, and C

00:16:21.180 --> 00:16:23.689
and had these linear equations.

00:16:23.689 --> 00:16:27.539
Often, in many of
the sciences, we

00:16:27.539 --> 00:16:30.839
think about matrix
operations and linearity

00:16:30.839 --> 00:16:36.420
as being coupled together.

00:16:36.420 --> 00:16:39.762
And through the duality
between matrices and graphs

00:16:39.762 --> 00:16:43.314
and networks, we can
represent graphs and networks

00:16:43.314 --> 00:16:44.360
through matrices.

00:16:44.360 --> 00:16:47.308
Any time you work
with a neural network,

00:16:47.308 --> 00:16:49.889
you're representing that
network as a matrix.

00:16:49.889 --> 00:16:53.257
And, of course, all these
equations apply there as well

00:16:53.257 --> 00:16:56.889
and you can reason about
those systems linearly.

00:16:56.889 --> 00:17:03.529
So that provides a
little motivation there.

00:17:03.529 --> 00:17:05.950
As we like to say,
enough about me,

00:17:05.950 --> 00:17:08.069
let me tell you about my book.

00:17:08.069 --> 00:17:12.209
So this will be the text that
will we use in the class.

00:17:12.209 --> 00:17:15.745
We are not going to go
through the full text,

00:17:15.745 --> 00:17:19.623
but we have printed out copies
of the first seven chapters

00:17:19.623 --> 00:17:21.359
that we will go through.

00:17:21.359 --> 00:17:29.280
And we will hand those out
later when you do the class.

00:17:29.280 --> 00:17:33.055
So let me now switch
gears a little bit

00:17:33.055 --> 00:17:36.831
and talk about how this
relates to, I think,

00:17:36.831 --> 00:17:38.831
one of the most
wonderful breakthroughs

00:17:38.831 --> 00:17:41.992
that we have seen, or
I've seen in my career,

00:17:41.992 --> 00:17:45.489
and many of my colleagues
here at MIT have seen,

00:17:45.489 --> 00:17:48.670
which is what's been going
on in machine learning,

00:17:48.670 --> 00:17:53.760
right, which is-- it's not hype.

00:17:53.760 --> 00:17:58.450
There's a real real there there
and it's tremendously exciting.

00:17:58.450 --> 00:18:01.650
So let me give you a little
history, basic history

00:18:01.650 --> 00:18:02.610
of this field.

00:18:02.610 --> 00:18:08.707
So in a certain sense,
before 2010, machine learning

00:18:08.707 --> 00:18:10.740
looked like this.

00:18:10.740 --> 00:18:17.720
And then, after 2015, it
kind of looks like this.

00:18:17.720 --> 00:18:21.519
So when people talk about
the hype in machine learning,

00:18:21.519 --> 00:18:23.799
or AI, really deep
neural networks

00:18:23.799 --> 00:18:28.360
are the elephant inside
the machine learning snake.

00:18:28.360 --> 00:18:32.488
It has stormed onto the
scene in the last five years

00:18:32.488 --> 00:18:38.316
and basically allowed us to do
things that we had almost taken

00:18:38.316 --> 00:18:40.700
for granted were impossible.

00:18:40.700 --> 00:18:44.756
Just the fact that you're
able to talk to computers

00:18:44.756 --> 00:18:48.989
and they can understand you,
that we can have computers that

00:18:48.989 --> 00:18:53.693
can see at least in a way that
approximates the way humans do,

00:18:53.693 --> 00:18:55.997
these are really almost
technological miracles

00:18:55.997 --> 00:18:59.387
that, for those of us
who have been working

00:18:59.387 --> 00:19:02.884
on this field for fifty years,
we had almost literally given

00:19:02.884 --> 00:19:03.520
up on.

00:19:03.520 --> 00:19:06.710
And then all of a sudden
it became possible.

00:19:06.710 --> 00:19:09.796
So let me give you a little
sense of appreciation

00:19:09.796 --> 00:19:11.649
for this field and its roots.

00:19:11.649 --> 00:19:15.361
So machine learning,
like any field,

00:19:15.361 --> 00:19:20.929
is defined as a set of
techniques and problems.

00:19:20.929 --> 00:19:23.435
When you ask what defines
a field, you ask, well,

00:19:23.435 --> 00:19:26.552
what are the problems that they
work on that other fields don't

00:19:26.552 --> 00:19:27.370
really work on?

00:19:27.370 --> 00:19:30.635
And what are the techniques
they employ that really are not

00:19:30.635 --> 00:19:32.120
really being employed by them?

00:19:32.120 --> 00:19:35.706
So the core techniques,
as I mentioned earlier,

00:19:35.706 --> 00:19:37.500
are these neural networks.

00:19:37.500 --> 00:19:41.514
These are meant to crudely
approximate maybe the way

00:19:41.514 --> 00:19:43.299
humans think about problems.

00:19:43.299 --> 00:19:45.549
We have these circles
which are neurons.

00:19:45.549 --> 00:19:47.980
They have connections
to other neurons.

00:19:47.980 --> 00:19:50.612
You know, those connections
have different weights

00:19:50.612 --> 00:19:51.740
associated with them.

00:19:51.740 --> 00:19:53.636
As information
comes in, they get

00:19:53.636 --> 00:19:54.900
multiplied by those weights.

00:19:54.900 --> 00:19:56.240
They get summed together.

00:19:56.240 --> 00:19:58.842
And if they pass certain
thresholds or criteria,

00:19:58.842 --> 00:20:01.770
then they send a signal
on to another neuron.

00:20:01.770 --> 00:20:04.267
And this is, to
a certain degree,

00:20:04.267 --> 00:20:07.479
how we believe the
human brain works and is

00:20:07.479 --> 00:20:10.338
a natural starting
point for, how could

00:20:10.338 --> 00:20:13.020
we make computers
do similar things?

00:20:13.020 --> 00:20:17.484
The big problems that
people have worked on

00:20:17.484 --> 00:20:21.390
are these classic problems
in machine learning,

00:20:21.390 --> 00:20:24.955
are language, how do we
make computers understand

00:20:24.955 --> 00:20:28.520
human language, vision,
how do we make computers

00:20:28.520 --> 00:20:31.460
see pictures or explain
pictures back to us the way

00:20:31.460 --> 00:20:34.876
we would like, and strategy and
games and other types of things

00:20:34.876 --> 00:20:35.419
like that.

00:20:35.419 --> 00:20:38.390
So how do we get them
to solve problems?

00:20:38.390 --> 00:20:40.900
This is not new.

00:20:40.900 --> 00:20:45.557
These core concepts trace
back to the earliest days

00:20:45.557 --> 00:20:47.110
of the field.

00:20:47.110 --> 00:20:50.531
In fact, these four
figures here, each one

00:20:50.531 --> 00:20:53.952
is taken from a paper
that was presented

00:20:53.952 --> 00:20:58.041
at the very first
machine learning

00:20:58.041 --> 00:21:00.680
conference in the mid-1950s.

00:21:00.680 --> 00:21:03.179
So there was a machine learning
conference in the mid-1950s.

00:21:03.179 --> 00:21:06.370
It was in Los Angeles.

00:21:06.370 --> 00:21:09.700
It had four papers presented.

00:21:09.700 --> 00:21:12.930
These were the four papers.

00:21:12.930 --> 00:21:16.779
And I will say
that three of them

00:21:16.779 --> 00:21:21.110
were done by folks at MIT
Lincoln Laboratory, which

00:21:21.110 --> 00:21:22.570
is where I work.

00:21:22.570 --> 00:21:25.290
And so that was basically
the neural networks

00:21:25.290 --> 00:21:26.650
of language and vision.

00:21:26.650 --> 00:21:31.510
And we didn't play
games, so that was it.

00:21:31.510 --> 00:21:33.309
And you might say,
well, why is it?

00:21:33.309 --> 00:21:36.198
Why was there so
much work going on

00:21:36.198 --> 00:21:38.366
in Lincoln Laboratory
in the mid-1950s

00:21:38.366 --> 00:21:43.740
that they would want to
pioneer in these directions?

00:21:43.740 --> 00:21:48.065
At that time, people were
first building computers

00:21:48.065 --> 00:21:51.310
and computers were
very special purpose.

00:21:51.310 --> 00:21:53.691
So different organizations
around the world

00:21:53.691 --> 00:21:56.470
were building computers
to do different things.

00:21:56.470 --> 00:22:06.886
Some were doing them to simulate
complex fluid dynamics systems,

00:22:06.886 --> 00:22:11.097
think about designing
ships or other types

00:22:11.097 --> 00:22:13.650
of things like
that or airplanes.

00:22:13.650 --> 00:22:17.162
Others were doing them to,
say, like what Alan Turing was

00:22:17.162 --> 00:22:18.120
doing, break codes.

00:22:18.120 --> 00:22:23.079
And our task was
to help people who

00:22:23.079 --> 00:22:27.419
were watching radar scopes
make decisions, right?

00:22:27.419 --> 00:22:32.761
How could computers enable
humans to watch more sensors

00:22:32.761 --> 00:22:35.730
and see where they're going?

00:22:35.730 --> 00:22:36.730
How could we do that?

00:22:36.730 --> 00:22:40.136
So at Lincoln Laboratory, we
were building special purpose

00:22:40.136 --> 00:22:41.650
computers to do this.

00:22:41.650 --> 00:22:46.257
And we built the
first large computer

00:22:46.257 --> 00:22:48.890
with reliable, fast memory.

00:22:48.890 --> 00:22:57.242
This system had 4,096 bytes
of memory, which, at the time,

00:22:57.242 --> 00:23:01.039
people thought was too much.

00:23:01.039 --> 00:23:06.490
What could you possibly
do with 4,096 numbers?

00:23:06.490 --> 00:23:08.710
The human brain, of course!

00:23:08.710 --> 00:23:10.169
Right, that's enough, right?

00:23:10.169 --> 00:23:13.590
Most of us can remember five,
six, seven digits, right?

00:23:13.590 --> 00:23:16.705
So a computer that can
remember 4,096 numbers

00:23:16.705 --> 00:23:20.790
should be able to do things
like language and vision

00:23:20.790 --> 00:23:21.760
and strategy.

00:23:21.760 --> 00:23:23.149
So why not?

00:23:23.149 --> 00:23:27.052
So they went out
and they started

00:23:27.052 --> 00:23:29.840
working on these problems, OK?

00:23:29.840 --> 00:23:33.400
But Lincoln Laboratory, being
an applied research laboratory,

00:23:33.400 --> 00:23:40.176
we are required to get answers
to our sponsors in a few years'

00:23:40.176 --> 00:23:41.350
time frame.

00:23:41.350 --> 00:23:44.341
If problems are going to
take longer than that,

00:23:44.341 --> 00:23:48.679
then they really are the
purview of the basic research

00:23:48.679 --> 00:23:50.019
community, universities.

00:23:50.019 --> 00:23:51.969
And it became
apparent pretty early

00:23:51.969 --> 00:23:55.220
on that this problem was
going to be more difficult.

00:23:55.220 --> 00:23:59.649
It was not going to
be solved right away.

00:23:59.649 --> 00:24:03.420
So we did what we often
do, is we partnered.

00:24:03.420 --> 00:24:07.266
We found some bright young
people at MIT, people

00:24:07.266 --> 00:24:08.549
just like yourselves.

00:24:08.549 --> 00:24:14.610
In this case, we found a young
professor named Marvin Minsky.

00:24:14.610 --> 00:24:18.950
And we said, why don't you go
and get some of your friends

00:24:18.950 --> 00:24:20.867
together and create
a meeting where

00:24:20.867 --> 00:24:23.096
you can lay out what the
fundamental challenges are

00:24:23.096 --> 00:24:23.840
of this field?

00:24:23.840 --> 00:24:26.885
And then we will figure out how
to get that funded so that you

00:24:26.885 --> 00:24:28.190
can go and do that research.

00:24:28.190 --> 00:24:32.116
And that was the famous
Dartmouth AI conference

00:24:32.116 --> 00:24:34.570
which kicked off the field.

00:24:34.570 --> 00:24:38.809
And the person leading this
group, Oliver Selfridge

00:24:38.809 --> 00:24:42.563
at Lincoln Laboratory, basically
arranged for that conference

00:24:42.563 --> 00:24:46.240
to happen and then subsequently
arranged for what would

00:24:46.240 --> 00:24:51.880
became the MIT AI Lab that was
founded by Professor Minsky.

00:24:51.880 --> 00:24:54.064
And likewise,
Professor Selfridge

00:24:54.064 --> 00:24:58.980
also realized that we would
need more computing power.

00:24:58.980 --> 00:25:01.530
So he left Lincoln
Laboratory and formed

00:25:01.530 --> 00:25:04.809
what was called Project MAC,
which became the Laboratory

00:25:04.809 --> 00:25:06.179
for Computer Science.

00:25:06.179 --> 00:25:11.640
And then those two entities
later merged 30 years later

00:25:11.640 --> 00:25:13.279
to become CSAIL.

00:25:13.279 --> 00:25:16.000
So that was the initial thing.

00:25:16.000 --> 00:25:20.199
Now, it was pretty clear that,
when this problem was handed

00:25:20.199 --> 00:25:22.490
off to the basic
research community,

00:25:22.490 --> 00:25:25.655
there was a feeling that
these problems would

00:25:25.655 --> 00:25:28.029
be solved in about a decade.

00:25:28.029 --> 00:25:32.404
So we were really
thinking by the mid-1960s

00:25:32.404 --> 00:25:36.779
is when these problems
would be really solved.

00:25:36.779 --> 00:25:40.639
So it's like giving someone
an assignment, right?

00:25:40.639 --> 00:25:43.360
You all are given
assignments by professors

00:25:43.360 --> 00:25:46.860
and they give you
a week to do it.

00:25:46.860 --> 00:25:49.460
But it took a little longer.

00:25:49.460 --> 00:25:55.294
In this case, it took five weeks
or, in this case, five decades

00:25:55.294 --> 00:25:57.090
to solve this problem.

00:25:57.090 --> 00:25:58.090
But we have.

00:25:58.090 --> 00:26:01.966
We have now really,
using those techniques,

00:26:01.966 --> 00:26:05.289
made tremendous progress
on those problems.

00:26:05.289 --> 00:26:07.740
But we don't know why it works.

00:26:07.740 --> 00:26:09.198
So we made this
tremendous progress

00:26:09.198 --> 00:26:09.770
but we don't really
understand why this works.

00:26:09.770 --> 00:26:12.490
So let me show you a little
bit what we have learned,

00:26:12.490 --> 00:26:15.084
and this course will explore
the deeper mathematics

00:26:15.084 --> 00:26:18.135
to help us gain insight.

00:26:18.135 --> 00:26:19.510
We still don't
know why it works.

00:26:19.510 --> 00:26:22.195
At least we can
lay the foundations

00:26:22.195 --> 00:26:24.880
and maybe you can figure it out.

00:26:24.880 --> 00:26:30.040
So here I am, fifty
years later, a person

00:26:30.040 --> 00:26:33.480
from Lincoln Laboratory
saying, "All right.

00:26:33.480 --> 00:26:36.030
Question one has been answered.

00:26:36.030 --> 00:26:37.370
Here's question two."

00:26:37.370 --> 00:26:38.370
Ha.

00:26:38.370 --> 00:26:41.357
Why does this work and
hopefully you can begin,

00:26:41.357 --> 00:26:43.150
be the generation
figured it out.

00:26:43.150 --> 00:26:45.140
Hopefully it'll take
less than fifty years.

00:26:45.140 --> 00:26:48.295
Historically this type
once we know how it works,

00:26:48.295 --> 00:26:51.970
it usually takes about twenty
years to figure out why.

00:26:51.970 --> 00:26:54.715
So I mean impasses
but maybe maybe you

00:26:54.715 --> 00:26:58.779
know some people are smarter and
they'll figure it out faster.

00:26:58.779 --> 00:27:05.870
So this is what a neural
network looks like.

00:27:05.870 --> 00:27:10.130
On the left you have your input,
in this case, a vector, y zero.

00:27:10.130 --> 00:27:15.090
It's just these dots
that are called features.

00:27:15.090 --> 00:27:17.760
What is a feature?

00:27:17.760 --> 00:27:20.210
Anything can be a feature.

00:27:20.210 --> 00:27:23.418
That is the power
of neural networks,

00:27:23.418 --> 00:27:28.002
is they don't require you
to a priori state what

00:27:28.002 --> 00:27:29.461
the inputs can be.

00:27:29.461 --> 00:27:31.130
They can be anything.

00:27:31.130 --> 00:27:32.506
People have said,
well, you know,

00:27:32.506 --> 00:27:33.921
neural networks,
machine learning,

00:27:33.921 --> 00:27:34.980
it's just curve fitting.

00:27:34.980 --> 00:27:38.380
Yeah, but it's curve fitting
without domain knowledge.

00:27:38.380 --> 00:27:42.065
Because domain knowledge
is so costly and expensive

00:27:42.065 --> 00:27:47.415
to create that having a
general system that can do this

00:27:47.415 --> 00:27:50.000
is really what's so powerful.

00:27:50.000 --> 00:27:52.480
So the inputs: we
have a input feature.

00:27:52.480 --> 00:27:56.380
It could be a vector,
which we call y sub zero.

00:27:56.380 --> 00:28:00.010
And that can just be an image,
right, the canonical thing

00:28:00.010 --> 00:28:02.320
being an image of a cat, right?

00:28:02.320 --> 00:28:05.484
And that can just be
the pixels, values just

00:28:05.484 --> 00:28:10.419
rolled out into a vector,
and they will be the inputs.

00:28:10.419 --> 00:28:12.100
And then we have a
series of layers.

00:28:12.100 --> 00:28:14.560
These are called hidden layers.

00:28:14.560 --> 00:28:19.639
The circles are often
referred to as neurons, OK?

00:28:19.639 --> 00:28:22.682
And each line
connecting each dot

00:28:22.682 --> 00:28:26.740
has a value associated
with it, a weight.

00:28:26.740 --> 00:28:28.863
And the strength
of the connection

00:28:28.863 --> 00:28:32.049
between any two neurons
is given by that weight.

00:28:32.049 --> 00:28:36.375
And then, ultimately,
the output, in this case,

00:28:36.375 --> 00:28:40.871
the output classification,
the series of blue dots there,

00:28:40.871 --> 00:28:43.110
are the different
possible categories.

00:28:43.110 --> 00:28:46.455
So if I put in a cat picture,
one of those dots would be cat,

00:28:46.455 --> 00:28:51.189
maybe one would be dog, maybe
one would be apple or orange,

00:28:51.189 --> 00:28:52.740
whatever I desired.

00:28:52.740 --> 00:28:57.040
And the whole idea is that,
if I put in a picture of a cat

00:28:57.040 --> 00:28:59.302
and I set all these
values correctly,

00:28:59.302 --> 00:29:02.558
then the dot
corresponding to cat

00:29:02.558 --> 00:29:06.900
will end up with the
highest score, right?

00:29:06.900 --> 00:29:11.596
And then I mentioned earlier
that each one of these neurons

00:29:11.596 --> 00:29:12.450
collects inputs.

00:29:12.450 --> 00:29:14.606
And if it's above a
certain threshold,

00:29:14.606 --> 00:29:18.380
it then chooses to pass on
information to the next.

00:29:18.380 --> 00:29:21.234
And that's where these b
values, which are vectors,

00:29:21.234 --> 00:29:22.820
are just the
thresholds associated

00:29:22.820 --> 00:29:24.000
with each one of those.

00:29:24.000 --> 00:29:29.194
It's a vector, one value
associated with each one

00:29:29.194 --> 00:29:32.080
of those that does those.

00:29:32.080 --> 00:29:35.897
This entire system can
be represented relatively

00:29:35.897 --> 00:29:39.169
simply with one
equation, which is

00:29:39.169 --> 00:29:44.477
that yi plus one, which is
the next vector in the layer,

00:29:44.477 --> 00:29:48.780
OK, can be computed by
the previous vector, yi

00:29:48.780 --> 00:29:52.834
matrix multiplied
by the weight, W.

00:29:52.834 --> 00:29:56.213
So whenever you
see transformations

00:29:56.213 --> 00:30:02.267
from one set of neurons to the
next layer, you should think,

00:30:02.267 --> 00:30:06.401
oh, I have a matrix that
represents all those weights

00:30:06.401 --> 00:30:09.720
and I'm going to multiply it by
the vector to get the next one.

00:30:09.720 --> 00:30:13.700
Then we apply these
thresholds, all right?

00:30:13.700 --> 00:30:17.492
So we add these,
the bi's, and then

00:30:17.492 --> 00:30:21.759
we have a function that
we pass it through.

00:30:21.759 --> 00:30:27.316
Typically, this h function
has been given the name

00:30:27.316 --> 00:30:29.169
rectified linear unit.

00:30:29.169 --> 00:30:30.409
It's much simpler than that.

00:30:30.409 --> 00:30:34.369
It's just, if the value is
greater than what comes out

00:30:34.369 --> 00:30:37.445
of this matrix multiplied,
if the value is greater

00:30:37.445 --> 00:30:38.970
than zero, don't touch it.

00:30:38.970 --> 00:30:40.190
Just let it pass through.

00:30:40.190 --> 00:30:43.259
If it's less than zero,
make it zero, right?

00:30:43.259 --> 00:30:46.812
You know, it's a
pretty complicated name

00:30:46.812 --> 00:30:49.350
for a very simple function.

00:30:49.350 --> 00:30:50.750
That's actually critical.

00:30:50.750 --> 00:30:53.290
If you didn't have
that h function,

00:30:53.290 --> 00:30:55.467
this nonlinear
function there, then we

00:30:55.467 --> 00:30:57.722
could roll up all
of these together

00:30:57.722 --> 00:31:00.399
and we would just have one
big matrix equation, right?

00:31:00.399 --> 00:31:03.490
So that's really considered a
pretty important part of it.

00:31:03.490 --> 00:31:05.415
So that's pretty
much what's going on.

00:31:05.415 --> 00:31:07.998
When you want to know what the
big deal is of neural networks,

00:31:07.998 --> 00:31:09.940
that's all that's going on.

00:31:09.940 --> 00:31:12.480
It's just that equation.

00:31:12.480 --> 00:31:16.870
The challenge is we don't know
what the W's and the b's are.

00:31:16.870 --> 00:31:19.559
And we don't know how many
layers there should be.

00:31:19.559 --> 00:31:23.750
And we don't know how
many neurons there

00:31:23.750 --> 00:31:26.370
should be in each layer.

00:31:26.370 --> 00:31:28.776
And although the features
can be arbitrary,

00:31:28.776 --> 00:31:30.449
picking the right
ones do matter.

00:31:30.449 --> 00:31:32.240
And picking the right
categories do matter.

00:31:32.240 --> 00:31:35.355
So when people talk about,
I do machine learning

00:31:35.355 --> 00:31:37.779
or I'm off working
on-- they're basically

00:31:37.779 --> 00:31:39.960
playing with all
of these parameters

00:31:39.960 --> 00:31:42.869
to try and find
the ones that will

00:31:42.869 --> 00:31:45.470
work best for their problem.

00:31:45.470 --> 00:31:47.100
And there's a lot
of trial and error.

00:31:47.100 --> 00:31:48.532
And you'll hear
about there's now

00:31:48.532 --> 00:31:50.680
systems that try and use
machine learning to do

00:31:50.680 --> 00:31:51.940
that process automatically.

00:31:51.940 --> 00:31:56.851
You know, how do you
make machines that learn

00:31:56.851 --> 00:31:59.580
how to do machine learning?

00:31:59.580 --> 00:32:03.580
The basic approach is a
trial and error approach.

00:32:03.580 --> 00:32:07.643
I take a whole bunch
of pictures of cats

00:32:07.643 --> 00:32:13.610
that I now have cats in them,
OK, and other things, right?

00:32:13.610 --> 00:32:18.769
And I randomly set all those
weights and thresholds.

00:32:18.769 --> 00:32:22.549
And I put in the vector
and I see what the system--

00:32:22.549 --> 00:32:26.078
I guess what I think the
number of layers and neurons

00:32:26.078 --> 00:32:30.111
and all that should be and
I run it through the system

00:32:30.111 --> 00:32:33.013
and I get an estimate
or a calculation

00:32:33.013 --> 00:32:36.422
for what I think these
final values should be

00:32:36.422 --> 00:32:38.649
and I compare it with the truth.

00:32:38.649 --> 00:32:40.960
That is, I just
basically subtract it.

00:32:40.960 --> 00:32:46.317
And then I use those corrections
to very carefully adjust

00:32:46.317 --> 00:32:47.389
the weights.

00:32:47.389 --> 00:32:49.919
Basically, with the
last weights first, I

00:32:49.919 --> 00:32:53.486
do what's called back propagate
these little changes to try

00:32:53.486 --> 00:32:57.289
and make a better guess on
what these weights should be.

00:32:57.289 --> 00:32:59.977
So if you hear the
term back propagation,

00:32:59.977 --> 00:33:02.330
that's that process of
taking those differences

00:33:02.330 --> 00:33:07.210
and using them to adjust
these weights by about

00:33:07.210 --> 00:33:09.379
0.01% at a time.

00:33:09.379 --> 00:33:12.321
And then we just do
this over and over

00:33:12.321 --> 00:33:15.263
again until eventually
we get a set of weights

00:33:15.263 --> 00:33:19.310
that we think does the problem
well enough for our purpose.

00:33:19.310 --> 00:33:21.899
So that's called back
propagation, all right?

00:33:21.899 --> 00:33:23.604
Once we have the set
of weights and we

00:33:23.604 --> 00:33:25.500
have a new picture that
we want to know what

00:33:25.500 --> 00:33:28.233
it is, we drop it in
there and it tells us

00:33:28.233 --> 00:33:30.269
it's a cat or a dog or whatever.

00:33:30.269 --> 00:33:32.100
That forward step
is called inference.

00:33:32.100 --> 00:33:34.442
These are two words
you'll hear frequently

00:33:34.442 --> 00:33:37.200
in machine learning, back
propagation and inference.

00:33:37.200 --> 00:33:38.450
And that's all there is to it.

00:33:38.450 --> 00:33:40.970
There's really
nothing else to that.

00:33:40.970 --> 00:33:42.867
If you can understand
this equation,

00:33:42.867 --> 00:33:46.029
you'll be way ahead of most
people in machine learning,

00:33:46.029 --> 00:33:47.029
you know?

00:33:47.029 --> 00:33:49.028
You know, there's
lots of people who

00:33:49.028 --> 00:33:51.314
understand about all the
software and the packages

00:33:51.314 --> 00:33:52.600
and the data.

00:33:52.600 --> 00:33:54.529
All of them are just doing that.

00:33:54.529 --> 00:33:58.027
And I'd say this is one
of the most powerful ways

00:33:58.027 --> 00:34:01.470
to be ahead in your field,
is to actually understand

00:34:01.470 --> 00:34:03.210
the mathematical principles.

00:34:03.210 --> 00:34:05.944
Because then the software
and what it's doing

00:34:05.944 --> 00:34:06.970
is much clearer.

00:34:06.970 --> 00:34:08.809
And other people
who don't understand

00:34:08.809 --> 00:34:11.100
these mathematical principles,
they're really guessing.

00:34:11.100 --> 00:34:12.968
They're like, oh,
well, I do this

00:34:12.968 --> 00:34:14.570
and I throw this module in.

00:34:14.570 --> 00:34:17.838
They don't really know
that all it's doing

00:34:17.838 --> 00:34:20.699
is making adjustments to
these various equations,

00:34:20.699 --> 00:34:24.230
how many different layers
there are and stuff like that.

00:34:24.230 --> 00:34:25.996
Now, why is this important?

00:34:25.996 --> 00:34:27.620
You're like, well,
what does it matter?

00:34:27.620 --> 00:34:29.909
As I said before,
we have this system.

00:34:29.909 --> 00:34:32.000
It works but we don't know why.

00:34:32.000 --> 00:34:34.389
Well, why is it
important to know why?

00:34:34.389 --> 00:34:36.139
Well, there's two reasons.

00:34:36.139 --> 00:34:40.519
One is that, if we want
to be able to apply

00:34:40.519 --> 00:34:43.962
this incredible innovation to
other domains-- so many of you

00:34:43.962 --> 00:34:45.280
probably want to do that.

00:34:45.280 --> 00:34:48.738
Many of you want to say,
how can I apply machine

00:34:48.738 --> 00:34:52.217
learning to something else
other than language or vision

00:34:52.217 --> 00:34:56.690
or some of these other
standard problems?

00:34:56.690 --> 00:34:58.570
I kind of need some
theory to know.

00:34:58.570 --> 00:35:01.081
Like, OK, if I have a problem
that's like this one over here

00:35:01.081 --> 00:35:03.228
and I changed it in
this way, there's

00:35:03.228 --> 00:35:05.700
a good chance it'll work.

00:35:05.700 --> 00:35:10.169
There's some basis for why I'm
going to try something, right?

00:35:10.169 --> 00:35:11.960
Right now there's a
lot of trial and error.

00:35:11.960 --> 00:35:13.209
It's like, well, it's an idea.

00:35:13.209 --> 00:35:15.385
But if you can have
some math that says,

00:35:15.385 --> 00:35:17.460
you know, I think that
will probably work,

00:35:17.460 --> 00:35:21.091
that really is a great way
to guide your reasoning

00:35:21.091 --> 00:35:22.590
and guide your efforts.

00:35:22.590 --> 00:35:25.928
Another reason is
that-- so here's

00:35:25.928 --> 00:35:30.380
a picture of a very
cute poodle, right?

00:35:30.380 --> 00:35:34.214
And the machine learning
system correctly

00:35:34.214 --> 00:35:36.770
identifies it as poodle.

00:35:36.770 --> 00:35:39.430
One thing we realized
is that the way you

00:35:39.430 --> 00:35:42.520
and I see that picture is
actually very, very different

00:35:42.520 --> 00:35:47.260
than the way the neural network
sees that picture, all right?

00:35:47.260 --> 00:35:51.167
And, in fact, I can make
changes to that picture that

00:35:51.167 --> 00:35:54.321
are imperceptible to you
or me but will completely

00:35:54.321 --> 00:35:56.189
change how the
neural network-- that

00:35:56.189 --> 00:36:00.428
is, given our neural network,
I can basically make it

00:36:00.428 --> 00:36:03.050
think anything, right?

00:36:03.050 --> 00:36:05.760
And so, for instance,
this is a famous paper.

00:36:05.760 --> 00:36:08.508
And they got the system
to think that that

00:36:08.508 --> 00:36:09.730
was an ostrich, right?

00:36:09.730 --> 00:36:12.890
And you can basically show
this for anything, right?

00:36:12.890 --> 00:36:15.980
So what's called robust AI,
or robust machine learning,

00:36:15.980 --> 00:36:18.040
machine learning that
can't be tricked,

00:36:18.040 --> 00:36:21.030
is going to become more
and more important.

00:36:21.030 --> 00:36:25.188
And again, having a deeper
understanding of the theory

00:36:25.188 --> 00:36:27.960
is very, very critical of that.

00:36:27.960 --> 00:36:31.850
So how are we going to do this?

00:36:31.850 --> 00:36:34.015
What's the main concept
that we are going

00:36:34.015 --> 00:36:35.640
to go through in this class?

00:36:35.640 --> 00:36:37.770
This has mostly
been motivational.

00:36:37.770 --> 00:36:40.320
But how are we going to
understand the data at a deeper

00:36:40.320 --> 00:36:40.820
level?

00:36:40.820 --> 00:36:42.770
You know, what's the big idea?

00:36:42.770 --> 00:36:46.179
And the big idea is
captured now in this,

00:36:46.179 --> 00:36:48.831
I apologize for this
eye chart slide,

00:36:48.831 --> 00:36:52.277
which is what we call
declarative mathematically

00:36:52.277 --> 00:36:53.300
rigorous data.

00:36:53.300 --> 00:36:56.504
So we have this
mathematical concept

00:36:56.504 --> 00:36:58.640
called an associative array.

00:36:58.640 --> 00:37:01.960
And it's corresponding algebra
that basically encompasses

00:37:01.960 --> 00:37:05.763
the data you would put
in databases, the data

00:37:05.763 --> 00:37:07.938
that you would put in
graphs, the data that

00:37:07.938 --> 00:37:11.560
would put in matrices and it
makes it all a linear system.

00:37:11.560 --> 00:37:14.240
And the key operations are
outlined there at the bottom.

00:37:14.240 --> 00:37:17.479
If you recall, we have
our basic little addition

00:37:17.479 --> 00:37:18.270
and multiplication.

00:37:18.270 --> 00:37:20.462
And then what's going
to be very important,

00:37:20.462 --> 00:37:22.707
probably the real
workhorse for this-- and i

00:37:22.707 --> 00:37:24.724
didn't show it before--
is called essentially

00:37:24.724 --> 00:37:26.640
array multiplication or
matrix multiplication.

00:37:26.640 --> 00:37:29.294
And that's the far
one on the right

00:37:29.294 --> 00:37:33.354
there, which we often abbreviate
just with no symbol, just A B.

00:37:33.354 --> 00:37:36.578
But if we really want
to explicitly call out

00:37:36.578 --> 00:37:39.037
that its matrix multiplication
as a combination

00:37:39.037 --> 00:37:40.706
of both multiplication
and addition,

00:37:40.706 --> 00:37:44.210
we put in what we call the
punch-drunk emoji, which

00:37:44.210 --> 00:37:46.710
is a plus dot times.

00:37:46.710 --> 00:37:50.370
You're probably all young
enough that you don't even

00:37:50.370 --> 00:37:53.585
remember emojis when
they had to type them out

00:37:53.585 --> 00:37:56.170
with just little characters and
we didn't have icons, right?

00:37:56.170 --> 00:38:00.620
So that meant you went to the
bar and lost to the fight,

00:38:00.620 --> 00:38:01.120
right?

00:38:01.120 --> 00:38:04.616
But, anyway, that's really going
to be the workhorse of what

00:38:04.616 --> 00:38:06.502
we're doing here.

