WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:08.240
&gt;&gt; Okay, I think we'll make a start.
And thank you very much for coming, everybody.

00:00:08.240 --> 00:00:10.060
Very pleased to welcome Dan Gardner here,
who is going to talk about

00:00:10.060 --> 00:00:12.360
his book which is called "Risk: The Science
and Politics of Fear."

00:00:12.360 --> 00:00:15.450
Dan is here -- if I've understood it correctly
 -- this morning to tell

00:00:15.450 --> 00:00:22.449
us that the things we are scared about are
actually the most safe, and

00:00:22.449 --> 00:00:24.789
the things that we think are safe are the
most risky.

00:00:24.789 --> 00:00:26.880
So to demonstrate this Dan has promised that
he's going to exit the

00:00:26.880 --> 00:00:29.980
building today by a bungee jump into a pool
of molten lava, so we all

00:00:29.980 --> 00:00:38.549
look forward to that.
Just a very quick word on the at Google program.

00:00:38.549 --> 00:00:42.530
In case you haven't been to an at Google talk
before, there are only

00:00:42.530 --> 00:00:43.530
about five of us who do this, who put these
talks on.

00:00:43.530 --> 00:00:45.049
And we are always looking for volunteers,
so if you're interested in

00:00:45.049 --> 00:00:46.389
any aspect of putting these sorts of talks
on, on the communications or

00:00:46.389 --> 00:00:48.979
the technical side, please come talk to me
or Lucinda or any of the

00:00:48.979 --> 00:00:54.240
others who you may know on the team.
Anyway, enough of that.

00:00:54.240 --> 00:00:58.889
Please welcome Dan Gardner.
[APPLAUSE]

00:00:58.889 --> 00:01:08.380
DAN GARDNER: Thanks very much.
For the record, bungee jumping is quite safe.

00:01:08.380 --> 00:01:14.090
I hope you're all in a good mood because I'm
going to spend the next

00:01:14.090 --> 00:01:17.360
hour talking about all the horrible things
that can happen to you.

00:01:17.360 --> 00:01:21.149
And so I'll begin with this horrible thing.
Breast cancer.

00:01:21.149 --> 00:01:25.790
Yes, I'm going to start by depressing you
 --

00:01:25.790 --> 00:01:27.270
[INAUDIBLE SPEAKER]

00:01:27.270 --> 00:01:42.210
DAN GARDNER: Oh, it's not showing up here?

00:01:42.210 --> 00:01:51.700
[INAUDIBLE SPEAKER]
[PAUSE]

00:01:51.700 --> 00:02:04.359
DAN GARDNER: Okay.
Improvise.

00:02:04.359 --> 00:02:22.409
How does one do that?
Right. Yeah, so I'm going to start about talking

00:02:22.409 --> 00:02:23.409
about something
depressing.

00:02:23.409 --> 00:02:25.520
The talk will not be entirely depressing.
I promise you.

00:02:25.520 --> 00:02:33.209
In fact, it will get upbeat at a certain point.
I'm going to start with this depressing topic

00:02:33.209 --> 00:02:38.590
of breast cancer.
It's a very -- there we go.

00:02:38.590 --> 00:02:43.200
There we go.
Obviously, breast cancer is a major issue.

00:02:43.200 --> 00:02:48.460
It has been for literally decades.
There have been public health campaigns.

00:02:48.460 --> 00:02:52.799
There have been enumerable media stories about
breast cancer.

00:02:52.799 --> 00:02:55.530
So we should all know the basic facts about
breast cancer, one would

00:02:55.530 --> 00:02:59.040
assume, right?
Well, what's one of the most basic facts?

00:02:59.040 --> 00:03:00.630
Here's one.
Here's one.

00:03:00.630 --> 00:03:05.269
The risk that a woman experiences at particular
ages.

00:03:05.269 --> 00:03:07.769
Okay?
This is a really basic question.

00:03:07.769 --> 00:03:13.659
At what age is a woman most at risk of contracting
breast cancer?

00:03:13.659 --> 00:03:15.230
Very basic stuff.
We should all know this.

00:03:15.230 --> 00:03:20.290
So I'll ask for a show of hands.
Is a woman most at risk of contracting breast

00:03:20.290 --> 00:03:23.370
cancer in her forties?
Show of hands.

00:03:23.370 --> 00:03:26.459
Nobody?
In her 50s?

00:03:26.459 --> 00:03:27.900
A lot of people.
In her 60s?

00:03:27.900 --> 00:03:33.760
Some more, but a little bit less in her 70s?
Nobody?

00:03:33.760 --> 00:03:37.709
80 or above?
One.

00:03:37.709 --> 00:03:41.069
Age does not matter?
Everybody else.

00:03:41.069 --> 00:03:44.829
And now as you're all clever people, so you
will have surmised that I

00:03:44.829 --> 00:03:48.360
begin my talk with this quiz because it always
delivers the results

00:03:48.360 --> 00:03:50.510
that I need.
And the results that I need are you're all

00:03:50.510 --> 00:03:53.000
wrong.
Except for you. Sir.

00:03:53.000 --> 00:03:58.230
There's always one in the audience.
There's always one.

00:03:58.230 --> 00:04:02.629
Breast cancer, as with most cancers, the single
biggest risk factor, by

00:04:02.629 --> 00:04:08.260
far -- by far is age.
The older a woman is, the more at risk she

00:04:08.260 --> 00:04:10.269
is.
It's really that simple.

00:04:10.269 --> 00:04:13.819
So the correct answer is 80 or above.
Okay.

00:04:13.819 --> 00:04:18.040
Now this is all horribly unscientific, but
I can tell you I've given

00:04:18.040 --> 00:04:23.160
this quiz to enumerable audiences in multiple
countries, and I've

00:04:23.160 --> 00:04:28.201
always gotten the same response.
I actually once gave this quiz to the Health

00:04:28.201 --> 00:04:34.500
Ministry in Canada and got
the same response, which is rather disturbing,

00:04:34.500 --> 00:04:36.950
really.
But this is done somewhat more scientifically

00:04:36.950 --> 00:04:40.450
by Oxford researchers.
They surveyed British women, and they asked

00:04:40.450 --> 00:04:47.580
them this same question.
And in that quiz, 0.7 percent of women got

00:04:47.580 --> 00:04:52.860
the answer right to this
most basic question about this most important

00:04:52.860 --> 00:04:54.900
issue.
Pretty astonishing, isn't it?

00:04:54.900 --> 00:04:58.480
It's a pretty big gap between perception and
reality.

00:04:58.480 --> 00:05:03.810
And that's what I'm going to be talking about
today is the gap between

00:05:03.810 --> 00:05:10.100
perception, risk perception and risk reality.
Risk perception researchers really got underway

00:05:10.100 --> 00:05:14.420
in the 1970s largely in
the United States as a result of the controversy

00:05:14.420 --> 00:05:18.940
over nuclear power.
Nuclear engineers said this is safe, the public

00:05:18.940 --> 00:05:22.170
said we're worried.
The nuclear engineers said but here are the

00:05:22.170 --> 00:05:23.790
numbers.
The public said we are don't care about your

00:05:23.790 --> 00:05:26.160
numbers.
And the nuclear engineers said but here are

00:05:26.160 --> 00:05:31.830
the numbers.
And it was really was a dialogue of the deaf.

00:05:31.830 --> 00:05:34.270
And so psychologists got involved to try and
understand what was really

00:05:34.270 --> 00:05:38.130
going on in people's risk perceptions.
How do people judge what's worth worrying

00:05:38.130 --> 00:05:39.830
about and what's not worth
worrying about.

00:05:39.830 --> 00:05:45.000
And one of the key findings is that there
is -- these gaps between

00:05:45.000 --> 00:05:48.220
perception and reality are routine.
Routine.

00:05:48.220 --> 00:05:53.560
And in fact, sometimes these gaps between
perception and reality open

00:05:53.560 --> 00:05:57.951
up into canyons, where the gap between perception
and reality is

00:05:57.951 --> 00:06:03.780
enormous, breathtaking.
Give you some examples of these, these canyons

00:06:03.780 --> 00:06:07.400
between perception and
reality.

00:06:07.400 --> 00:06:13.410
Silicone breast implants. You may remember
 -- not many people in this

00:06:13.410 --> 00:06:16.310
room will remember, I suspect, because you're
all ridiculously young --

00:06:16.310 --> 00:06:20.540
but those of us who are a little bit older,
we remember that in the

00:06:20.540 --> 00:06:23.860
early 1990s in North America, wasn't so much
a European phenomena, but

00:06:23.860 --> 00:06:28.400
in the North America, there was a great, great
level of concern about

00:06:28.400 --> 00:06:32.150
silicone breast implants -- that they cause
connective tissue disease.

00:06:32.150 --> 00:06:37.130
In fact, the risk perception was so high that
women believed that the

00:06:37.130 --> 00:06:39.530
risk was equivalent to smoking a pack of cigarettes
a day.

00:06:39.530 --> 00:06:46.140
If you have implants, you're going to die.
There was actually surprisingly little scientific

00:06:46.140 --> 00:06:48.340
information to
support that belief.

00:06:48.340 --> 00:06:51.760
Lots of anecdotes, lots of case studies, but
not any really good

00:06:51.760 --> 00:06:55.410
science.
Well, in the mid- and late 1990s, the science

00:06:55.410 --> 00:07:00.020
started to deliver.
And the science was clear, no.

00:07:00.020 --> 00:07:06.730
Doesn't cause connective tissue disease.
And now that's the scientific consensus.

00:07:06.730 --> 00:07:09.430
So that's a great example of a huge gap between
perception/reality.

00:07:09.430 --> 00:07:15.900
Here's another one.
The Columbine massacre ten years ago.

00:07:15.900 --> 00:07:21.000
After the Columbine massacre happened, it
was not only news across the

00:07:21.000 --> 00:07:25.140
United States, in Canada, it was news around
the world.

00:07:25.140 --> 00:07:29.710
And there was a huge debate, a fierce, fierce
debate, in the United

00:07:29.710 --> 00:07:35.150
States about the Columbine massacre and it's
meaning.

00:07:35.150 --> 00:07:39.840
The debate wasn't about what's wrong with
kids, right?

00:07:39.840 --> 00:07:43.590
What's wrong with the schools?
Everybody knew that there was something wrong

00:07:43.590 --> 00:07:47.060
with kids, wrong with the
schools because school violence was soaring

00:07:47.060 --> 00:07:51.890
out of control.
The debate was about why, why was school violence

00:07:51.890 --> 00:07:53.960
soaring out of
control, okay?

00:07:53.960 --> 00:07:57.710
And everybody had their little pet theories
based on their idealogical

00:07:57.710 --> 00:08:02.551
assumptions.
They all knew the answer.

00:08:02.551 --> 00:08:05.590
And the U.S. Congress asked the U.S. Department
of Justice to do some

00:08:05.590 --> 00:08:10.210
proper research on school violence.
You know what it discovered?

00:08:10.210 --> 00:08:12.580
It discovered that throughout the 1990s, throughout
the years leading

00:08:12.580 --> 00:08:15.920
up the Columbine massacre, school violence
actually wasn't soaring.

00:08:15.920 --> 00:08:20.640
It was actually plummeting.
It was doing the exactly the opposite.

00:08:20.640 --> 00:08:24.990
That research was bundled into a report, the
report was issued.

00:08:24.990 --> 00:08:28.510
And it was completely ignored by the media
and politicians.

00:08:28.510 --> 00:08:32.469
It's now an annual report, incidentally.
Every year it comes out and every year it

00:08:32.469 --> 00:08:36.319
shows the same things.
School violence is going down and every year

00:08:36.319 --> 00:08:40.289
the media and politicians
ignore it.

00:08:40.289 --> 00:08:43.939
Here's one that much more European phenomena
 -- GM foods .

00:08:43.939 --> 00:08:51.290
GM foods and health risk.
There is simply no good evidence that GM foods

00:08:51.290 --> 00:08:54.579
are a significant risk
to human health.

00:08:54.579 --> 00:08:59.680
If they were, most of North America would
be sick or dying because it's

00:08:59.680 --> 00:09:04.110
all we eat.
And yet a very, very significant chunk of

00:09:04.110 --> 00:09:09.699
the population of Europe
believes that GM foods will kill you.

00:09:09.699 --> 00:09:12.600
Now, I'm mainly going to talk about risks
of injury or disease or death

00:09:12.600 --> 00:09:17.720
but this stuff applies to any sort of bad
outcome including financial

00:09:17.720 --> 00:09:21.350
risks.
Here's another gap -- the real estate bubble

00:09:21.350 --> 00:09:25.769
and home refinancing.
You had a pretty spectacular real estate bubble

00:09:25.769 --> 00:09:28.470
here.
United States, they had another one.

00:09:28.470 --> 00:09:31.700
United States, one of the great phenomena
was this home refinancing.

00:09:31.700 --> 00:09:34.910
I'm going to refinance my mortgage, I'm going
to take that money and

00:09:34.910 --> 00:09:39.750
spend it on a monster truck and giant-screen
TV and vacations.

00:09:39.750 --> 00:09:44.050
Why is that a good idea?
Because home prices only go up, right?

00:09:44.050 --> 00:09:48.629
So this is actually a low-risk move.
Everybody knows home prices only go up.

00:09:48.629 --> 00:09:51.220
Bad risk perception.
Okay.

00:09:51.220 --> 00:09:56.350
Now, that's at the consumer level.
But we all know that consumers are uneducated

00:09:56.350 --> 00:09:58.769
and foolish, so of
course, they're going to think silly things.

00:09:58.769 --> 00:10:02.560
But what about those super sophisticated Wall
Street bankers.

00:10:02.560 --> 00:10:05.800
They wouldn't make those sorts of mistakes,
would they?

00:10:05.800 --> 00:10:09.749
Except that they did.
Mortgage-backed securities.

00:10:09.749 --> 00:10:15.540
Well, that's a fantastic investment.
Let's borrow billions of dollars to buy some.

00:10:15.540 --> 00:10:20.350
And the world is living with the repercussions
of that disastrous risk

00:10:20.350 --> 00:10:26.950
perception.
So how do these enormous gaps between risk

00:10:26.950 --> 00:10:31.810
and reality open up?
This slide is my attempt at a -- you know,

00:10:31.810 --> 00:10:33.910
wrap-it-all-up, one slide
explanation.

00:10:33.910 --> 00:10:43.240
There are three basic factors involved.
And, of course, I don't want to be reductionist.

00:10:43.240 --> 00:10:46.129
There are other things going on as well but
I think that this is an

00:10:46.129 --> 00:10:50.249
essential feature of what's needed for these
gaps to open up.

00:10:50.249 --> 00:10:53.569
Number one, the media.
To nobody's surprise, the media play a role.

00:10:53.569 --> 00:10:57.860
Number two, fear marketers, which is simply
my term for individuals and

00:10:57.860 --> 00:11:01.699
organizations who have an interest in manipulating
our perceptions of

00:11:01.699 --> 00:11:05.560
risk.
Very often it's to promote the idea that we're

00:11:05.560 --> 00:11:08.560
at risk, but sometimes
it's actually to diminish the risk, tobacco

00:11:08.560 --> 00:11:11.600
companies being the classic
example.

00:11:11.600 --> 00:11:15.490
And then, of course, fundamentally what's
going on here is psychology.

00:11:15.490 --> 00:11:19.519
It's about human nature. It's about how we
think and that's what most

00:11:19.519 --> 00:11:24.180
important, and it's what I'm going to spend
most of them talking about

00:11:24.180 --> 00:11:26.769
today.
Now, very quickly, I'm going to run through

00:11:26.769 --> 00:11:29.430
these first couple of
points quickly, because you know this stuff,

00:11:29.430 --> 00:11:32.911
right?
The fear marketers -- politician market fears

00:11:32.911 --> 00:11:34.949
for votes.
Everybody knows this.

00:11:34.949 --> 00:11:38.310
I probably don't have to elaborate beyond
saying two words: Bush

00:11:38.310 --> 00:11:43.629
Administration.
Officials market fear to expand budgets.

00:11:43.629 --> 00:11:47.490
Police chiefs -- it's amazing how often they
describe burgeoning crime

00:11:47.490 --> 00:11:51.120
waves shortly before they have to have their
budgets approved by city

00:11:51.120 --> 00:11:55.279
counsels.
It's an amazing correlation.

00:11:55.279 --> 00:12:02.499
Corporations market fear for sales.
If you perceive yourself to be healthy, you're

00:12:02.499 --> 00:12:04.300
not going to buy the
pill, are you?

00:12:04.300 --> 00:12:07.589
Right?
So if you're working for a pharmaceutical

00:12:07.589 --> 00:12:10.480
company, you don't have to
have an MBA, you don't have to be particularly

00:12:10.480 --> 00:12:14.379
bright to figure out
what is necessary to grow the market, right?

00:12:14.379 --> 00:12:18.879
Increase the perception of illness.
Same with security companies.

00:12:18.879 --> 00:12:22.829
If you perceive yourself to be safe, you're
not going to buy a new home

00:12:22.829 --> 00:12:26.639
alarm, right?
So what do you got to do as a security company?

00:12:26.639 --> 00:12:32.880
Right.
And NGO's -- NGO's market fear to advance

00:12:32.880 --> 00:12:35.779
causes.
NGO's, very often they have better motives

00:12:35.779 --> 00:12:40.269
than profit-seeking
corporations but they use the same tactics.

00:12:40.269 --> 00:12:43.790
I mentioned breast cancer at the beginning.
In the late 1990s, breast cancer activists

00:12:43.790 --> 00:12:47.980
used to -- there's a figure
they always like to promote, very widely cited.

00:12:47.980 --> 00:12:52.829
It said, one in eight women will get breast
cancer in her lifetime.

00:12:52.829 --> 00:12:55.329
Pretty startling figure, right?
And if you're a woman, it's pretty frightening.

00:12:55.329 --> 00:13:00.250
You go, one, two, three, four, right?
Well, what they didn't tell you about that

00:13:00.250 --> 00:13:05.649
figure is that for a woman
to be exposed to the full one in eight risk,

00:13:05.649 --> 00:13:09.220
she has to be 95-years
old.

00:13:09.220 --> 00:13:12.829
I personally hope to be diagnosed with cancer
at the age of 95, because

00:13:12.829 --> 00:13:20.019
it will mean that I am alive at the age of
95.

00:13:20.019 --> 00:13:21.959
Okay.
The media.

00:13:21.959 --> 00:13:24.959
Again, this stuff is pretty obvious, you know
this stuff.

00:13:24.959 --> 00:13:27.819
The media's fault.
We emphasize -- and when I say "we," I work

00:13:27.819 --> 00:13:31.110
for a newspaper.
That's my day job being a columnist.

00:13:31.110 --> 00:13:35.480
I emphasize opinion.
You've noticed I have a few opinions.

00:13:35.480 --> 00:13:40.959
We emphasize the vivid, the dramatic, and
the emotional, and so lives

00:13:40.959 --> 00:13:43.680
lost to tornados and terrorism, that's front-page
news.

00:13:43.680 --> 00:13:48.079
That goes on the evening news.
Lives lost to diabetes and asthma, that's

00:13:48.079 --> 00:13:51.550
not news, right?
When was the last time you saw a story about

00:13:51.550 --> 00:13:55.069
diabetes killing someone,
right?

00:13:55.069 --> 00:14:00.550
And yet diabetes kills more people in a year
than tornados and

00:14:00.550 --> 00:14:03.480
terrorism combined will kill in all of human
history.

00:14:03.480 --> 00:14:10.619
We're biased toward bad news.
Everybody knows this one, too.

00:14:10.619 --> 00:14:14.720
Good news is an oxymoron, right?
Things improve -- not a story.

00:14:14.720 --> 00:14:25.610
Things go to hell -- great story.
Another one, we, the media, right?

00:14:25.610 --> 00:14:29.529
Man bites dog stories, right?
Dog bites man, that's not a story.

00:14:29.529 --> 00:14:31.839
Man bites dog, that's a great story.
Right?

00:14:31.839 --> 00:14:35.240
We love novelty.
That's unusual.

00:14:35.240 --> 00:14:37.600
That's startling.
Oh, that goes on the front page.

00:14:37.600 --> 00:14:43.559
That's routine -- boring, yawn, moving along.
And, of course, these stories are true.

00:14:43.559 --> 00:14:46.449
They're all factual but what's the obvious
problem with reporting that

00:14:46.449 --> 00:14:50.389
way?
If you constantly report that man bites dog

00:14:50.389 --> 00:14:53.869
and you never report that
dog bites man?

00:14:53.869 --> 00:14:56.860
Eventually you're going to come up with a
pretty skewed impression of

00:14:56.860 --> 00:15:05.490
canine/human relations, aren't you?
We present risk information badly.

00:15:05.490 --> 00:15:08.200
Something bad could happen.
How many times have you seen in a story in

00:15:08.200 --> 00:15:11.249
the media in which they say
something bad could happen, and you say, oh,

00:15:11.249 --> 00:15:12.470
you're right.
That is bad.

00:15:12.470 --> 00:15:17.970
I better pay attention.
What do those stories mean?

00:15:17.970 --> 00:15:19.350
Nothing.
Nothing.

00:15:19.350 --> 00:15:23.110
Something bad could happen.
Means nothing, right?

00:15:23.110 --> 00:15:27.490
Right now, it is possible that a jet could
lose power in all its

00:15:27.490 --> 00:15:30.689
engines, crash into this building and kill
us all horribly.

00:15:30.689 --> 00:15:35.670
It could happen in the next 30 seconds.
Are you concerned?

00:15:35.670 --> 00:15:39.790
No, you're not because the probability of
that happening is extremely,

00:15:39.790 --> 00:15:43.410
extremely low.
Probability matters.

00:15:43.410 --> 00:15:48.369
Pretty basic stuff.
We emphasize, we the media, we emphasize relative

00:15:48.369 --> 00:15:50.509
risk not absolute
risk.

00:15:50.509 --> 00:15:54.180
Stories are constantly filled with phrases
like "twice the risk."

00:15:54.180 --> 00:15:57.319
This is a real New York Times story, by the
way.

00:15:57.319 --> 00:16:01.600
In the New York Times it said, women who take
this pill are at twice

00:16:01.600 --> 00:16:05.879
the risk of developing potentially fatal blood
clots.

00:16:05.879 --> 00:16:08.949
If you're a woman and if you take that pill,
you're probably going to

00:16:08.949 --> 00:16:12.429
find that alarming.
Should you?

00:16:12.429 --> 00:16:13.699
Answer?
I don't know.

00:16:13.699 --> 00:16:18.079
I don't have a clue.
I need to know the absolute risk to know if

00:16:18.079 --> 00:16:22.879
the relative risk matters.
If, for instance, I were to tell you that

00:16:22.879 --> 00:16:26.649
the possibility -- the
probability of that jet losing power in all

00:16:26.649 --> 00:16:29.209
of its engines, crashing
into this building and killing us all horribly

00:16:29.209 --> 00:16:32.839
were to double.
Would you be alarmed?

00:16:32.839 --> 00:16:36.180
No, you would not be alarmed because two times
almost zero is almost

00:16:36.180 --> 00:16:41.250
zero, right?
But if there's a significant risk of developing

00:16:41.250 --> 00:16:44.879
those potentially fatal
blood clots, and then you double the risk

00:16:44.879 --> 00:16:47.249
by taking this pill, you
should be worried.

00:16:47.249 --> 00:16:49.819
You need to know the absolute risk and the
media almost never give you

00:16:49.819 --> 00:16:56.449
that information.
We also provide no comparisons to provide

00:16:56.449 --> 00:17:00.320
perspective.
Even if you're a numerate person and I tell

00:17:00.320 --> 00:17:05.560
you that the risk of some
horrible thing is 1 in 135,000, what does

00:17:05.560 --> 00:17:08.940
that mean?
It's pretty tough to grasp, right?

00:17:08.940 --> 00:17:12.720
Conceptually, you can get it.
It's probably pretty small.

00:17:12.720 --> 00:17:16.080
But if I were to tell you that the risk of
 -- your annual risk of dying

00:17:16.080 --> 00:17:22.810
in a car crash is 1 in 6,000?
You say, right, I can compare one to the other.

00:17:22.810 --> 00:17:27.070
Because we're very good at doing that.
And we want to do that naturally.

00:17:27.070 --> 00:17:30.050
And I can realize that's actually an extremely
small risk and I

00:17:30.050 --> 00:17:33.370
probably shouldn't worry about it.
Those sorts of comparisons almost never appear

00:17:33.370 --> 00:17:37.390
in the media and
fundamentally it's about this, this is the

00:17:37.390 --> 00:17:39.550
problem with the media --
lack of critical scrutiny.

00:17:39.550 --> 00:17:43.930
We don't examine.
We don't think rigorously.

00:17:43.930 --> 00:17:48.340
We pass it along.
If the public thinks something, we agree,

00:17:48.340 --> 00:17:55.260
and we pass it along.
Now, everything I've said about the media,

00:17:55.260 --> 00:17:58.650
you all know that.
The more interesting question is the explanation

00:17:58.650 --> 00:18:01.290
for why the media does
what it does.

00:18:01.290 --> 00:18:04.700
Right?
The standard answer is to make money or to

00:18:04.700 --> 00:18:08.070
push an agenda, right?
And what's the agenda?

00:18:08.070 --> 00:18:12.900
Well, the agenda depends on your own politics.
If you're left wing, the agenda's right wing.

00:18:12.900 --> 00:18:14.550
And if you're right wing, the agenda's left
wing.

00:18:14.550 --> 00:18:17.760
Have you ever noticed that?
It's pretty remarkable.

00:18:17.760 --> 00:18:22.250
I don't deny that idealogy or money, even
more importantly, plays some

00:18:22.250 --> 00:18:26.160
role at some times in some venues.
But I think there's something much, much more

00:18:26.160 --> 00:18:31.870
fundamental going on in
media's behavior, in the behavior of the media.

00:18:31.870 --> 00:18:36.090
It's simply that journalists are human.
Journalists are people.

00:18:36.090 --> 00:18:40.680
When you go back and you look at media behavior,
media reporting, and

00:18:40.680 --> 00:18:45.990
you analyze it through the lens of psychology,
suddenly it starts to

00:18:45.990 --> 00:18:50.340
make sense.
An example -- remember I said there's a bad

00:18:50.340 --> 00:18:52.710
news bias in the media, as
everybody knows.

00:18:52.710 --> 00:18:54.740
We love bad news.
We ignore good news.

00:18:54.740 --> 00:18:58.730
Why?
That's not a media bias, that's a human bias.

00:18:58.730 --> 00:19:02.890
There's a huge psychological literature demonstrating
that people are

00:19:02.890 --> 00:19:08.350
biased toward bad news.
If I put two pictures up, a frowning face

00:19:08.350 --> 00:19:12.600
and a smiling face, your eyes
will be drawn to the frowning face first,

00:19:12.600 --> 00:19:16.280
and you will remember the
frowning face longer.

00:19:16.280 --> 00:19:19.340
There's all sorts of research to indicate
that.

00:19:19.340 --> 00:19:24.520
So the media really are just following human
nature.

00:19:24.520 --> 00:19:29.190
Similarly, novelty, we love those man bites
dog stories in the media.

00:19:29.190 --> 00:19:34.530
Well, people love novelty.
People love man bites dog stories.

00:19:34.530 --> 00:19:41.180
We are very much hardwired towards novelty.
If I show you a list of 30 words, 29 of which

00:19:41.180 --> 00:19:44.540
are written in blue ink,
and one of which written in green ink, you

00:19:44.540 --> 00:19:48.210
will notice and remember the
one that's written in green ink.

00:19:48.210 --> 00:19:53.190
That's just human wiring.
So what's really fundamental here is psychology.

00:19:53.190 --> 00:20:00.200
And that's what we have to talk about.
Now, start with the real basics on psychology.

00:20:00.200 --> 00:20:03.810
All our feelings and thoughts are a product
of the brain.

00:20:03.810 --> 00:20:06.200
No surprise.
But the brain is the product of the environment

00:20:06.200 --> 00:20:07.430
of evolutionary
adaptedness.

00:20:07.430 --> 00:20:10.570
The environment of evolutionary adaptedness
is a term from evolutionary

00:20:10.570 --> 00:20:14.090
psychology.
It simply means the world in which the pressures

00:20:14.090 --> 00:20:17.700
of the environment
shape the brain, change the brain, made it

00:20:17.700 --> 00:20:22.660
what it is.
What was that environment?

00:20:22.660 --> 00:20:28.440
Well, here's the history of our species.
Roughly 200,000 years, there's a great long

00:20:28.440 --> 00:20:32.020
line.
And then there's today, that little vertical

00:20:32.020 --> 00:20:36.620
line at the end, okay?
The founding of the first town, the rise and

00:20:36.620 --> 00:20:40.780
fall of the Roman Empire,
the invention of the printing press, the birth

00:20:40.780 --> 00:20:43.760
of Bill Gates,
everything that matters in human history fits

00:20:43.760 --> 00:20:46.090
in that vertical line at
the end.

00:20:46.090 --> 00:20:51.580
Everything.
That perspective is essential to understanding

00:20:51.580 --> 00:20:58.460
your brain.
If you were to write a history of humanity

00:20:58.460 --> 00:21:01.590
in proportion to the amount
of time that we have spent at each stage of

00:21:01.590 --> 00:21:07.840
development, you'd write
200 pages on hunter/gatherer societies.

00:21:07.840 --> 00:21:13.910
200 pages.
You'd have one page for agrarian societies.

00:21:13.910 --> 00:21:17.250
You would have one short paragraph of the
world of the last two

00:21:17.250 --> 00:21:22.550
centuries.
All of that boils down to one simple point.

00:21:22.550 --> 00:21:26.810
We live in the Information Age but our brains
are Stone Age, which is

00:21:26.810 --> 00:21:30.580
not an insult.
We often use the term Stone Age as an insult.

00:21:30.580 --> 00:21:34.510
That's a mistake.
The human brain is absolutely magnificent.

00:21:34.510 --> 00:21:41.780
It's one of the wonders of the universe indisputably.
But it is a fact that it evolved in an environment

00:21:41.780 --> 00:21:45.070
radically unlike the
environment in which we live.

00:21:45.070 --> 00:21:54.670
And that fact has profound implications.
Now, if you go to a cognitive psychologist

00:21:54.670 --> 00:21:58.310
and say, start with the
basics, tell me about how we make decisions,

00:21:58.310 --> 00:22:00.830
tell me about how we
decide what to worry about and what not to

00:22:00.830 --> 00:22:02.340
worry about.
They'll start with this -- they'll start by

00:22:02.340 --> 00:22:06.700
saying, you think you make
decisions by thinking about them, right?

00:22:06.700 --> 00:22:10.151
It seems obvious, I think, I come to a conclusion.
There's my conclusion.

00:22:10.151 --> 00:22:14.350
It's pretty obvious.
That's an illusion.

00:22:14.350 --> 00:22:19.200
There's much, much more going on.
In fact, you have not one mind at work, you

00:22:19.200 --> 00:22:22.070
have two minds.
Two systems of thought.

00:22:22.070 --> 00:22:24.530
And psychologists, being the clever people
that they are, they have

00:22:24.530 --> 00:22:29.782
dubbed these systems of thought system one
and system two.

00:22:29.782 --> 00:22:34.390
System one.
System one is the more ancient of the two.

00:22:34.390 --> 00:22:38.130
It evolved first.
It operates outside consciousness.

00:22:38.130 --> 00:22:43.310
By definition you are not aware of what it's
doing.

00:22:43.310 --> 00:22:47.490
It delivers conclusions in the form of feelings,
hunches, intuition.

00:22:47.490 --> 00:22:53.110
I asked you about breast cancer at the beginning.
You said, that seems right.

00:22:53.110 --> 00:22:57.940
Where did that answer come from?
You had a sense that you knew the answer.

00:22:57.940 --> 00:23:00.110
It didn't come from the facts because you
were all wrong.

00:23:00.110 --> 00:23:04.070
Except you, okay?
So where did it come from?

00:23:04.070 --> 00:23:10.040
It came from this mind.
And it came quickly didn't it, right?

00:23:10.040 --> 00:23:14.020
This is the essential feature of the unconscious
mind.

00:23:14.020 --> 00:23:19.670
It's fast and it's effortless.
It delivers snap judgments, which is a very

00:23:19.670 --> 00:23:24.680
useful tool when you're
walking along the street and a shadow moves

00:23:24.680 --> 00:23:26.090
in the alleyway and you've
got to decide what to do.

00:23:26.090 --> 00:23:31.880
You can't consult crime statistics; you have
to decide now.

00:23:31.880 --> 00:23:37.650
Now because system one is completely forgettable,
I call this "gut."

00:23:37.650 --> 00:23:41.730
I call the unconscious mind "gut."
Because that's how we talk about it in ordinary

00:23:41.730 --> 00:23:45.040
English.
I have a gut feeling that something is true.

00:23:45.040 --> 00:23:52.040
I can't quite explain why.
I just have a sense that this is true -- gut.

00:23:52.040 --> 00:23:56.570
Obviously, the other mind, system two, as
psychologists call it is

00:23:56.570 --> 00:23:59.340
conscious thought.
Conscious thought is capable of careful reasoning,

00:23:59.340 --> 00:24:03.110
numeracy, logic.
Capable, I emphasize.

00:24:03.110 --> 00:24:06.730
It delivers conclusions that can be fully
expressed and explained.

00:24:06.730 --> 00:24:10.280
If you come to a conclusion by consciously
reasoning your way to that

00:24:10.280 --> 00:24:13.730
conclusion and I ask you, how did you get
here?

00:24:13.730 --> 00:24:16.340
You can explain it.
You can take me through the steps to the conclusion.

00:24:16.340 --> 00:24:19.940
The problem?
It's slow and laborious, right?

00:24:19.940 --> 00:24:27.080
Conscious thought takes time and effort -- a
lot of effort.

00:24:27.080 --> 00:24:29.960
I call this system "head" to make it more
memorable.

00:24:29.960 --> 00:24:33.970
Because that too is how we talk about it in
ordinary English.

00:24:33.970 --> 00:24:37.830
If I have a gut feeling something is true
and you think I'm wrong, you

00:24:37.830 --> 00:24:40.390
say what?
You say use your head, right?

00:24:40.390 --> 00:24:45.650
You may or may not mean that as a simple insult.
If you're being less insulting, you mean it

00:24:45.650 --> 00:24:48.200
as hey, stop and think
about this carefully.

00:24:48.200 --> 00:24:55.600
Which is to say, stop and think about it consciously.
Now who does what?

00:24:55.600 --> 00:25:01.030
How do the two minds interact?
The first important thing to remember is that

00:25:01.030 --> 00:25:02.670
we are constantly
bombarded with stimuli.

00:25:02.670 --> 00:25:06.670
There's massive amounts of information coming
at us at all times and

00:25:06.670 --> 00:25:10.370
the brain is processing it all.
Right now you think you're listening to me

00:25:10.370 --> 00:25:14.880
 -- I hope you're listening
to me -- but you're doing much, much more,

00:25:14.880 --> 00:25:17.760
right?
You are -- for instance, there's ambient sound

00:25:17.760 --> 00:25:22.180
in the air.
There's ambient scent in the air.

00:25:22.180 --> 00:25:26.280
There's a pattern on the carpet.
Your brain is processing all that information.

00:25:26.280 --> 00:25:36.790
And it's shifting through all that information.
And it's deciding, okay, what should consciousness

00:25:36.790 --> 00:25:41.250
pay attention to?
Hopefully my words, right?

00:25:41.250 --> 00:25:45.080
All the other stuff that you're not pay attention
to, it's processed by

00:25:45.080 --> 00:25:50.410
the unconscious mind.
It goes to gut.

00:25:50.410 --> 00:25:57.150
Now, it's not an even workload.
Only a very small fraction of the information

00:25:57.150 --> 00:26:00.620
processed by your brain
goes towards consciousness.

00:26:00.620 --> 00:26:06.950
All the rest goes to the unconscious mind.
You're never aware of how you're handling

00:26:06.950 --> 00:26:11.220
it.
Now how do you make a decision out of this

00:26:11.220 --> 00:26:14.760
system, out of these two
systems working together, how do you make

00:26:14.760 --> 00:26:17.840
a decision?
Well, remember the essential feature of gut

00:26:17.840 --> 00:26:21.060
is that it's fast.
And head, its essential feature, one of its

00:26:21.060 --> 00:26:24.100
important features is that
it's slow, right?

00:26:24.100 --> 00:26:28.500
So that means gut delivers first -- boom.
There's your snap judgment and at that the

00:26:28.500 --> 00:26:32.390
point consciously you can
come along and you can examine that conclusion,

00:26:32.390 --> 00:26:36.270
and you can say to
yourself okay, that makes perfect sense.

00:26:36.270 --> 00:26:40.180
I'll go with it.
Or you can say that's a little off, I'll adjust

00:26:40.180 --> 00:26:42.040
it.
Or you can say, that's completely nuts.

00:26:42.040 --> 00:26:51.980
I'm going to ignore it.
But will "head" step in and examine "gut,"

00:26:51.980 --> 00:26:55.340
the unconscious mind,
examine the conclusions of the unconscious

00:26:55.340 --> 00:26:56.680
mind?
That's the critical question.

00:26:56.680 --> 00:27:02.490
Here's a way of devising or a test that was
devised as a way of

00:27:02.490 --> 00:27:06.120
determining whether in fact head is doing
its work -- whether head is

00:27:06.120 --> 00:27:11.970
getting involved.
A bat and ball cost $1.10.

00:27:11.970 --> 00:27:16.070
As you can see by the sums of all, this is
the very old question.

00:27:16.070 --> 00:27:21.000
A bat and ball cost a total of $1.10.
The bat costs a dollar more than the ball.

00:27:21.000 --> 00:27:24.920
How much does the ball cost?
Okay.

00:27:24.920 --> 00:27:30.390
If you are like every other human on the planet,
I guarantee that you

00:27:30.390 --> 00:27:34.610
read this and you immediately said to yourself
$0.10.

00:27:34.610 --> 00:27:38.220
$0.10 -- boom.
You didn't have to think about it.

00:27:38.220 --> 00:27:43.630
You just had that snap judgment -- $0.10.
But stop and think now.

00:27:43.630 --> 00:27:48.380
If the bat costs a dollar more than the ball,
and the ball cost $0.10,

00:27:48.380 --> 00:27:55.780
that means the bat costs $1.10.
$1.10 plus $.10 equals $1.20.

00:27:55.780 --> 00:28:02.110
$0.10 is wrong.
Not terribly complicated stuff.

00:28:02.110 --> 00:28:05.370
You would catch this mistake if you stopped
and thought about it

00:28:05.370 --> 00:28:08.690
consciously.
The question is will you stop and think about

00:28:08.690 --> 00:28:13.010
it consciously?
This question and many others like it was

00:28:13.010 --> 00:28:15.830
devised by a Nobel Prize
winning psychologist named Daniel Kahneman.

00:28:15.830 --> 00:28:20.340
And Kahneman's purpose was twofold.
He wanted to questions to elicit that snap

00:28:20.340 --> 00:28:23.290
judgment.
You have a strong and intuitive answer -- yes,

00:28:23.290 --> 00:28:25.900
that's the correct
answer.

00:28:25.900 --> 00:28:32.430
And then he wanted to know before people say,
yes, my final answer is

00:28:32.430 --> 00:28:37.130
$0.10, do they stop and think consciously
about that intuitive

00:28:37.130 --> 00:28:39.800
judgment?
Because if they did, they wouldn't say my

00:28:39.800 --> 00:28:44.400
final answer is $0.10.
Well Kahneman has found, he has given this

00:28:44.400 --> 00:28:47.970
quiz all over the place, in
particular to Ivy League students, so bright

00:28:47.970 --> 00:28:51.280
people.
What Kahneman has found is that typically

00:28:51.280 --> 00:28:53.400
most people did not catch the
error.

00:28:53.400 --> 00:29:00.090
They say final answer $0.10.
In other words, when we have a strong, intuitive

00:29:00.090 --> 00:29:04.510
conclusion that
something is true, we just go with it.

00:29:04.510 --> 00:29:08.350
We don't stop and examine it.
We don't question it.

00:29:08.350 --> 00:29:10.780
That's natural.
That's how we make decisions.

00:29:10.780 --> 00:29:15.690
And what that means is gut, your unconscious
mind, is far more

00:29:15.690 --> 00:29:18.760
influential in your decision making than you
realize.

00:29:18.760 --> 00:29:23.071
And we really better understand how it works.
So how does it work?

00:29:23.071 --> 00:29:27.200
How does it deliver?
I mean, here's the critical question about

00:29:27.200 --> 00:29:29.500
gut.
How does it deliver snap judgments?

00:29:29.500 --> 00:29:35.730
Why is it so much faster than conscious thought?
Well, it's faster because it doesn't serve

00:29:35.730 --> 00:29:38.210
a information.
All the information.

00:29:38.210 --> 00:29:42.050
It doesn't think about it logically and carefully,
because if it did,

00:29:42.050 --> 00:29:46.850
that it would be as slow.
It would be as slow as head, conscious mind.

00:29:46.850 --> 00:29:51.220
Instead, it applies hardwire mental processes
to narrow slices of

00:29:51.220 --> 00:29:58.410
information, little bits and pieces of information.
Psychologists call these processes heuristics

00:29:58.410 --> 00:30:01.140
and biases.
You can think of them as the tool kit, the

00:30:01.140 --> 00:30:06.140
tools in gut's tool kit that
it uses for making snap judgments.

00:30:06.140 --> 00:30:10.090
There's a long list of the heuristics and
biases that have been

00:30:10.090 --> 00:30:12.261
discovered by psychologists.
I'll just mention a few to give you a sense

00:30:12.261 --> 00:30:18.010
of how they work.
Here's a simple and very important -- the

00:30:18.010 --> 00:30:21.300
availability heuristic.
How easy is it to think of an example of something?

00:30:21.300 --> 00:30:23.940
If you want to know how common something is
you say to yourself, can I

00:30:23.940 --> 00:30:28.120
think of an example of it?
If you can think of an example of it really

00:30:28.120 --> 00:30:33.020
easily, if it comes to mind
quickly, well, it must be common, right?

00:30:33.020 --> 00:30:37.230
See how simple that is -- snap judgment.
And if you have to struggle to think of an

00:30:37.230 --> 00:30:39.600
example, it doesn't come to
mind easily, well, it must be uncommon.

00:30:39.600 --> 00:30:41.510
Simple.
Snap judgment.

00:30:41.510 --> 00:30:48.480
The great thing about this rule is that it
worked really, really well

00:30:48.480 --> 00:30:51.470
in the environment in which our brains evolved
 .

00:30:51.470 --> 00:30:54.380
Why?
Why did it work well?

00:30:54.380 --> 00:30:59.620
Because the only information available to
our brains at that time was

00:30:59.620 --> 00:31:02.140
our own personal experience or the experience
of our fellow band

00:31:02.140 --> 00:31:06.280
members, and there's only 30 or 40 or 50 of
them, right?

00:31:06.280 --> 00:31:10.720
So if you could very easily think of an example
of a crocodile dragging

00:31:10.720 --> 00:31:14.860
somebody to his death at the watering hole,
chances are it happened to

00:31:14.860 --> 00:31:19.300
somebody near you and it happened recently.
And you probably should be worried about crocodiles

00:31:19.300 --> 00:31:20.750
at the watering
hole, right?

00:31:20.750 --> 00:31:24.490
That's an effective rule.
That's going to keep you alive long enough

00:31:24.490 --> 00:31:29.210
to reproduce, which is the
critical thing in evolution.

00:31:29.210 --> 00:31:33.070
The problem?
We don't live in that world, right?

00:31:33.070 --> 00:31:37.050
Our information environment is radically,
radically different.

00:31:37.050 --> 00:31:40.530
In large part thanks to, you know, people
who work for Google, I should

00:31:40.530 --> 00:31:45.180
emphasize.
Our information environment is just spectacularly

00:31:45.180 --> 00:31:49.560
different.
We can have any bit of information anywhere,

00:31:49.560 --> 00:31:53.090
any time.
I sit down and turn on the evening news.

00:31:53.090 --> 00:31:57.110
What do I see?
Here's a true story, right, and it's a very

00:31:57.110 --> 00:32:00.660
typical story.
I turn on the evening news at my home in Ottawa,

00:32:00.660 --> 00:32:06.550
Canada, and I see a
story about a boy being abducted, raped, and

00:32:06.550 --> 00:32:11.810
murdered by a pedophile in
Leipzig, Germany.

00:32:11.810 --> 00:32:16.610
It's a horrible, horrible, horrible story.
It's vivid; it's awful; it's exactly the kind

00:32:16.610 --> 00:32:21.100
of story which is going
to be remembered, right?

00:32:21.100 --> 00:32:28.600
Now, what do I do with this information?
Well, my rational mind knows perfectly well

00:32:28.600 --> 00:32:32.010
that this doesn't tell me
anything about the safety of my children in

00:32:32.010 --> 00:32:33.980
Canada.
I know that. It's obvious.

00:32:33.980 --> 00:32:38.780
You don't even need to say it, right?
But your unconscious mind doesn't process

00:32:38.780 --> 00:32:41.710
the information that way.
Right?

00:32:41.710 --> 00:32:44.470
You take your children to the park the next
day.

00:32:44.470 --> 00:32:48.500
And you think to yourself, should I watch
out for the pervert in the

00:32:48.500 --> 00:32:51.130
bushes?
Is this a significant threat, right?

00:32:51.130 --> 00:32:56.080
And what happens?
Your unconscious mind says, well, can I think

00:32:56.080 --> 00:32:58.590
of an example of a
stranger abducting, raping, and murdering

00:32:58.590 --> 00:33:02.870
a child?
Yes, very, very easily.

00:33:02.870 --> 00:33:04.960
You saw it last night.
It's so vivid.

00:33:04.960 --> 00:33:10.330
It's so horrible.
Conclusion based on the availability heuristic?

00:33:10.330 --> 00:33:12.190
This is common.
Be afraid.

00:33:12.190 --> 00:33:15.370
And so what do we have all over the Western
World?

00:33:15.370 --> 00:33:18.420
We have parents afraid like never before that
their children are going

00:33:18.420 --> 00:33:22.090
to be abducted by strangers.
But guess what?

00:33:22.090 --> 00:33:26.030
Stranger abduction is real but it's actually
fantastically rare.

00:33:26.030 --> 00:33:30.151
Fantastically rare.
An American child is 26 times more likely

00:33:30.151 --> 00:33:35.800
to die in a car crash than to
be abducted by a stranger.

00:33:35.800 --> 00:33:38.360
But what are we doing?
We're talking our children and putting them

00:33:38.360 --> 00:33:40.850
in cars, and driving them
to school thinking that we're keeping them

00:33:40.850 --> 00:33:45.440
safe from the pervert in the
bushes.

00:33:45.440 --> 00:33:54.870
That's primitive cognitive wiring at work.
Here's another one.

00:33:54.870 --> 00:33:57.930
Representativeness heuristic.
Psychologists have a real talent for coming

00:33:57.930 --> 00:33:59.630
up with completely
forgettable names.

00:33:59.630 --> 00:34:03.320
It's hard to even say.
The representativeness heuristic.

00:34:03.320 --> 00:34:08.190
Again, it's a very simple idea and a very
important one.

00:34:08.190 --> 00:34:12.609
Here's a quiz.
Again, this is a Daniel Kahneman quiz.

00:34:12.609 --> 00:34:15.879
How likely is it that next year there will
be a flood somewhere in

00:34:15.879 --> 00:34:23.990
North America that kills at least 1,000 people?
Kahneman gave this quiz to one group of people.

00:34:23.990 --> 00:34:28.429
But in another group he said, how likely is
it that next year in

00:34:28.429 --> 00:34:32.279
California there will be an earthquake that
triggers a flood that kills

00:34:32.279 --> 00:34:39.829
at least 1,000 people?
Logically, the second scenario has to be less

00:34:39.829 --> 00:34:42.999
likely than the first.
You don't have to think about that very hard

00:34:42.999 --> 00:34:46.079
to see that it has to be
less likely.

00:34:46.079 --> 00:34:51.010
And yet, people consistently judged the second
scenario to be more

00:34:51.010 --> 00:34:54.050
likely than the first.
Why is that?

00:34:54.050 --> 00:34:56.349
It's pretty bizarre.
Why is it?

00:34:56.349 --> 00:35:03.809
Because of the representativeness heuristic.
And it's because of the link between California

00:35:03.809 --> 00:35:08.220
and earthquakes.
Nothing says California like earthquakes.

00:35:08.220 --> 00:35:12.869
Everybody knows that California and earthquakes
go together.

00:35:12.869 --> 00:35:20.670
Earthquakes are a typical event for California.
And it's that typicality is triggering your

00:35:20.670 --> 00:35:23.299
unconscious mind.
Your unconscious mind say hey, hey this is

00:35:23.299 --> 00:35:27.430
typical, this fits.
And it judges that whole scenario by that

00:35:27.430 --> 00:35:31.749
the typical component rather
than thinking about it logically.

00:35:31.749 --> 00:35:33.740
Saying, well, it's a conditional event followed
by another conditional

00:35:33.740 --> 00:35:37.059
event.
Oh geez, that's pretty unlikely.

00:35:37.059 --> 00:35:42.809
Our minds are filled with these notions of
typicality.

00:35:42.809 --> 00:35:48.749
We get them from experience we get from the
culture.

00:35:48.749 --> 00:35:50.660
Basketball players are what?
Tall.

00:35:50.660 --> 00:35:55.480
I just had to say basketball player and you
immediately see a tall man.

00:35:55.480 --> 00:35:59.279
And so what's happening is that the unconscious
mind is using this

00:35:59.279 --> 00:36:05.170
storehouse of information to judge probabilities,
which can often work.

00:36:05.170 --> 00:36:08.859
But in certain scenarios like this, it really
goes wrong.

00:36:08.859 --> 00:36:19.480
The implications of this?
When something typical is involved, intuition,

00:36:19.480 --> 00:36:23.609
the unconscious mind, is
likely it overwhelm logic and data.

00:36:23.609 --> 00:36:30.690
You're likely to make a conclusion which is
wrong and silly.

00:36:30.690 --> 00:36:33.920
Scenarios.
In corporate world, scenarios are very, very

00:36:33.920 --> 00:36:35.490
popular.
Right?

00:36:35.490 --> 00:36:39.540
We're going to imagine what could happen in
the future.

00:36:39.540 --> 00:36:41.610
And when you write scenarios, what do you
do?

00:36:41.610 --> 00:36:46.020
You make them very complicated and vivid and
detailed.

00:36:46.020 --> 00:36:52.619
This happened and that happened and this connects
to that and so on.

00:36:52.619 --> 00:36:56.160
And people think that scenarios provide a
lot of insight.

00:36:56.160 --> 00:36:59.970
Oh, that helps us understand what's going
to happen.

00:36:59.970 --> 00:37:03.079
Well, that seems probable, right, that makes
sense.

00:37:03.079 --> 00:37:08.650
But here's the thing: Because the scenario
is more complicated is more

00:37:08.650 --> 00:37:12.710
likely to have one of these typical components
than is a scenario

00:37:12.710 --> 00:37:18.049
that's less complicated.
Your unconscious mind is likely to grab onto

00:37:18.049 --> 00:37:20.950
that and say, oh, the more
complicated scenario that's the more plausible

00:37:20.950 --> 00:37:23.700
one.
Yeah, that's going to happen.

00:37:23.700 --> 00:37:27.789
And yet, if you think about it logically,
in most cases, it's the less

00:37:27.789 --> 00:37:31.249
complicated scenario that's actually more
likely.

00:37:31.249 --> 00:37:40.680
So scenarios are profoundly misleading if
you're not careful in how you

00:37:40.680 --> 00:37:45.160
use them.
The affect heuristic, again another forgettable

00:37:45.160 --> 00:37:48.990
term.
Affect is simply psychology's for emotion,

00:37:48.990 --> 00:37:54.880
feeling.
Now everybody knows that emotions can influence

00:37:54.880 --> 00:37:59.460
decisions.
If you're feeling strong anger, that can sway

00:37:59.460 --> 00:38:02.349
your decision making.
You know that if you're angry at Bob and you're

00:38:02.349 --> 00:38:05.520
thinking about firing
Bob, you should probably go home and sleep

00:38:05.520 --> 00:38:08.080
on it first.
We all know this.

00:38:08.080 --> 00:38:11.059
Obvious.
But what psychologists have found in the last

00:38:11.059 --> 00:38:15.740
several decades is that
emotion operates much, much, much more subtly

00:38:15.740 --> 00:38:19.650
than that.
In fact, emotion is so subtle that we can

00:38:19.650 --> 00:38:21.700
actually experience it
unconsciously.

00:38:21.700 --> 00:38:29.210
And we routinely do experience emotions unconsciously.
You're experiencing the emotion; you don't

00:38:29.210 --> 00:38:31.079
know you are.
You don't feel anything.

00:38:31.079 --> 00:38:37.180
If I asked you, you'd say I'm cool as a cucumber.
But you are experiencing an emotion.

00:38:37.180 --> 00:38:44.049
Now, because emotion is so much subtler than
we're aware of, that means

00:38:44.049 --> 00:38:48.660
that it's influencing perceptions and judgments
much more profoundly

00:38:48.660 --> 00:38:53.869
than we realize.
Emotions -- when it comes to risk, it's very

00:38:53.869 --> 00:38:56.009
simple. Again it's a nice
simple rule.

00:38:56.009 --> 00:39:00.279
Emotions drive the perception of risk.
If you have a good feeling about something,

00:39:00.279 --> 00:39:01.509
oh, that can't be
dangerous, right?

00:39:01.509 --> 00:39:05.940
If you have a good feeling, it drives the
perception of risk down.

00:39:05.940 --> 00:39:10.809
If you have a bad feeling, oh, that drives
the perception of risk up.

00:39:10.809 --> 00:39:12.730
Now in a lot of cases, that's going to make
sense.

00:39:12.730 --> 00:39:18.170
That's going to work.
But for instance, my father and my grandfather

00:39:18.170 --> 00:39:21.549
both smoke pipe, a pipe
right?

00:39:21.549 --> 00:39:27.470
So when I smell pipe tobacco, what do I feel?
I feel the warm fuzzies.

00:39:27.470 --> 00:39:32.260
I get a positive emotional surge from this.
Oh, I love the smell of pipe tobacco smoke.

00:39:32.260 --> 00:39:37.829
What is that going to do to my risk perception?
It's going to drive it down.

00:39:37.829 --> 00:39:42.880
That doesn't make sense.
There's all sorts of things and all sorts

00:39:42.880 --> 00:39:48.890
of ways in which this
emotional yardstick as a decision-making tool

00:39:48.890 --> 00:39:57.410
can go wrong.
One of the most important implications of

00:39:57.410 --> 00:40:02.140
the affect heuristic and the
role of emotion in decision making is that

00:40:02.140 --> 00:40:10.329
language is powerful.
It's far more powerful than people realize.

00:40:10.329 --> 00:40:14.590
Researchers -- I love this quiz because it's
kind of silly.

00:40:14.590 --> 00:40:16.380
Researchers brought together some people and
they said we're doing

00:40:16.380 --> 00:40:20.569
consumer product research.
And the consumer product that we're looking

00:40:20.569 --> 00:40:26.359
at is ground beef.
And here is a lump of ground beef.

00:40:26.359 --> 00:40:32.290
Would you please examine it visually, judge
it, then taste it, and

00:40:32.290 --> 00:40:37.650
judge it again.
The only difference is they said to one group

00:40:37.650 --> 00:40:40.800
of people -- they said
this is 25 percent lean-cooked ground beef.

00:40:40.800 --> 00:40:44.680
Go ahead and judge it.
To the other group, they said, this is 75

00:40:44.680 --> 00:40:48.029
percent lean-cooked ground
beef-- sorry.

00:40:48.029 --> 00:40:54.240
25 percent fat was the first group.
25 percent fat cooked ground beef versus cooked

00:40:54.240 --> 00:40:58.789
ground beef.
It's exactly the same information, isn't it?

00:40:58.789 --> 00:41:00.630
And guess what happened?
Right.

00:41:00.630 --> 00:41:06.539
The ground beef that was described as 75 percent
lean was judged to be

00:41:06.539 --> 00:41:08.999
significantly superior in quality than the
other.

00:41:08.999 --> 00:41:12.589
Why?
Because lean is good; fat is bad.

00:41:12.589 --> 00:41:17.180
It's that simple.
Here's another one.

00:41:17.180 --> 00:41:20.570
This, by the way, this study was actually
conducted with psychiatrists,

00:41:20.570 --> 00:41:24.509
clinical psychiatrists, so people who are
trained in statistics and

00:41:24.509 --> 00:41:30.019
scientific reasoning.
They were given a profile of a patient.

00:41:30.019 --> 00:41:37.200
And they were told in the profile that this
patient, if he is released

00:41:37.200 --> 00:41:42.460
that 20 percent of patients like him would
be expected to commit a

00:41:42.460 --> 00:41:47.619
crime of violence if released.
And so after reading this profile they were

00:41:47.619 --> 00:41:52.289
asked, would you release
this mental patient from incarceration in

00:41:52.289 --> 00:41:57.869
a mental hospital?
The other group of psychiatrists was given

00:41:57.869 --> 00:42:02.960
exactly the same information
except the phrase about his dangerousness

00:42:02.960 --> 00:42:08.460
was altered very subtly.
It was altered so that it said 20 out of 100

00:42:08.460 --> 00:42:13.249
patients like this man
would be expected to commit a crime of violence

00:42:13.249 --> 00:42:20.130
if released.
Will you release this patient?

00:42:20.130 --> 00:42:24.440
Obviously, that's exactly the same information.
20 percent versus 20 out of 100 patients.

00:42:24.440 --> 00:42:30.230
It's exactly the same information.
And yet in the first case, when it was 20

00:42:30.230 --> 00:42:34.680
percent of patients, the
refusal rate -- the rate at which people said

00:42:34.680 --> 00:42:39.410
no, I would not release
this man -- was 19 percent.

00:42:39.410 --> 00:42:44.890
In the second case, when the language said
20 out of 100 patients, the

00:42:44.890 --> 00:42:53.160
refusal rate was 40 percent.
That's a 100 percent increase in the rate

00:42:53.160 --> 00:42:59.019
of refusal based on a trivial
change in the language.

00:42:59.019 --> 00:43:01.549
Why?
How could this possibly be?

00:43:01.549 --> 00:43:07.290
The answer is the affect heuristic.
The language in the first case said 20 percent

00:43:07.290 --> 00:43:09.039
of patients.
20 percent.

00:43:09.039 --> 00:43:13.759
What's a percent?
It's a concept, right?

00:43:13.759 --> 00:43:19.130
It's an abstraction.
It doesn't have emotional content.

00:43:19.130 --> 00:43:21.380
20 out of 100 patients.
Okay.

00:43:21.380 --> 00:43:25.109
What's that?
That's patients; it's people.

00:43:25.109 --> 00:43:29.509
And in this case, the person is plunging a
knife into somebody's heart.

00:43:29.509 --> 00:43:33.539
That's bad.
It has emotional content.

00:43:33.539 --> 00:43:38.100
And it's that emotional content that drove
the decision, that drove the

00:43:38.100 --> 00:43:43.710
perception of risk and drove the decision.
Here's another one.

00:43:43.710 --> 00:43:47.089
Researchers went to an airport and they asked
some people, you're

00:43:47.089 --> 00:43:52.400
traveling now, how much would you pay for
life insurance against death

00:43:52.400 --> 00:43:56.910
caused by terrorism?
And they went to other people and they said,

00:43:56.910 --> 00:44:02.359
how much would you pay for
life insurance by death caused by any cause,

00:44:02.359 --> 00:44:05.630
right?
Now, logically you know what the answer should

00:44:05.630 --> 00:44:07.930
be.
And you know by now what the actual outcome

00:44:07.930 --> 00:44:11.390
is.
People were prepared to pay a lot more for

00:44:11.390 --> 00:44:15.970
life insurance against death
caused by terrorism.

00:44:15.970 --> 00:44:20.380
Again, doesn't make any sense.
Until you examine the role of emotion in human

00:44:20.380 --> 00:44:23.089
decision making.
And then it makes perfect sense.

00:44:23.089 --> 00:44:28.599
What's all causes?
Again, it's an abstraction.

00:44:28.599 --> 00:44:32.369
It -- it's a concept.
It doesn't have emotion content.

00:44:32.369 --> 00:44:36.450
What's terrorism?
As soon as the say the word, you have images

00:44:36.450 --> 00:44:39.789
in your mind.
That's not an abstraction.

00:44:39.789 --> 00:44:43.850
It's real.
And it's filled with emotion.

00:44:43.850 --> 00:44:49.359
Bad emotion.
That emotion is used to judge the perception

00:44:49.359 --> 00:44:52.549
of risk.
It drives the perception of risk.

00:44:52.549 --> 00:44:59.960
And that perception of risk changes the decision.
You're going to pay a lot more for that because

00:44:59.960 --> 00:45:03.839
that's bad.
Here's another one.

00:45:03.839 --> 00:45:07.640
This one's absolutely critical.
If you remember absolutely nothing else from

00:45:07.640 --> 00:45:11.759
this talk, please remember
this because this is the enemy of rationality.

00:45:11.759 --> 00:45:14.940
Confirmation bias.
Once we believe something for any reason,

00:45:14.940 --> 00:45:18.539
we will seek to confirm it.
Evidence that supports our belief will be

00:45:18.539 --> 00:45:21.260
embraced without question.
Evidence that disputes the belief will be

00:45:21.260 --> 00:45:23.789
scrutinized, belittled, or
ignored.

00:45:23.789 --> 00:45:26.970
Far more often, it will simply be ignored,
right?

00:45:26.970 --> 00:45:30.241
And by the way, I would point out that Google
and -- I love Google. I

00:45:30.241 --> 00:45:35.130
use Google about a hundred times a day -- I
would point out that Google

00:45:35.130 --> 00:45:38.569
actually empowers confirmation bias.
Because once you believe something, you go

00:45:38.569 --> 00:45:43.019
to Google, you type in
keywords for what you believe, and sure enough,

00:45:43.019 --> 00:45:46.980
somewhere on planet
earth, someone has evidence that you're right.

00:45:46.980 --> 00:45:51.519
Oh, I knew it all along.
But what you don't do is you don't go into

00:45:51.519 --> 00:45:55.460
Google, and you don't type
in keywords about what you don't believe,

00:45:55.460 --> 00:46:01.819
and so you don't get the
information that indicates that you were wrong.

00:46:01.819 --> 00:46:05.049
Really interesting study that demonstrated
the power of confirmation

00:46:05.049 --> 00:46:10.589
bias.
In 1979, it's 1979 in the United States, researchers

00:46:10.589 --> 00:46:13.989
brought together a
group of people who had strong feelings about

00:46:13.989 --> 00:46:18.999
capital punishment.
And one half of the group -- it was deliberately

00:46:18.999 --> 00:46:22.160
composed this way:
One half of the group believed that capital

00:46:22.160 --> 00:46:24.039
punishment was an effective
deterrent against crime.

00:46:24.039 --> 00:46:27.910
Capital punishment worked.
The other half believed that capital punishment

00:46:27.910 --> 00:46:30.579
did not deter crime.
Capital punishment does not work.

00:46:30.579 --> 00:46:34.119
At the start of the test, they examined, they
asked people, well, tell

00:46:34.119 --> 00:46:37.690
me how strongly do you believe that this is
true?

00:46:37.690 --> 00:46:41.940
Then they gave them information.
They gave them an essay, and the essay was

00:46:41.940 --> 00:46:45.799
composed of information
drawn from a couple of studies.

00:46:45.799 --> 00:46:50.279
And the studies were contradictory.
One study said capital punishment works; one

00:46:50.279 --> 00:46:54.089
study said it doesn't.
The studies used similar methodologies.

00:46:54.089 --> 00:46:59.049
In fact, the studies were bogus.
They were crafted by the researchers to be

00:46:59.049 --> 00:47:02.039
mirror images of one
another.

00:47:02.039 --> 00:47:04.989
Except that one study said it worked; one
study said it doesn't.

00:47:04.989 --> 00:47:09.260
Now if we process information logically, what
should happen at the end

00:47:09.260 --> 00:47:10.260
of this?
It should be a wash.

00:47:10.260 --> 00:47:14.150
You come in and leave with exactly the same
belief, right?

00:47:14.150 --> 00:47:19.900
It's not what happened.
Everybody left more convinced that they were

00:47:19.900 --> 00:47:22.070
right and the other guy
was wrong.

00:47:22.070 --> 00:47:24.460
Why?
How is this possible?

00:47:24.460 --> 00:47:26.710
Because when they read the information and
they came across the

00:47:26.710 --> 00:47:30.910
information that supported their prior belief,
they grabbed onto it and

00:47:30.910 --> 00:47:36.880
said uh-huh, yeah, knew it all along.
This just supports what I already believed.

00:47:36.880 --> 00:47:39.460
But the other information, the stuff that
contradicted it, oh, it's

00:47:39.460 --> 00:47:40.900
methodologically flawed.
I don't like what they're doing here with

00:47:40.900 --> 00:47:46.369
the data and it was
dismissed.

00:47:46.369 --> 00:47:50.950
And so everybody left even more strongly convinced
that they were right

00:47:50.950 --> 00:47:56.920
and the other guy was wrong.
I've mainly been dealing with cognitive psychology.

00:47:56.920 --> 00:47:58.750
Obviously social psychology is huge in this
stuff.

00:47:58.750 --> 00:48:03.390
Which of these lines is longest?
If you've ever taken Psych 101, you know this

00:48:03.390 --> 00:48:05.349
is a classic experiment
from about 60 years ago.

00:48:05.349 --> 00:48:12.359
There's a row of five people.
First person -- which of these lines is longest?

00:48:12.359 --> 00:48:13.440
And the first person says line five.
That's obviously wrong.

00:48:13.440 --> 00:48:14.740
You're looking at it and you're rubbing your
eyes, and you're thinking

00:48:14.740 --> 00:48:17.089
well, no, line four is longest.
What are you talking about?

00:48:17.089 --> 00:48:22.249
They go to the next person -- which of these
lines is the longest?

00:48:22.249 --> 00:48:24.700
Line five.
Which is the lines is longest?

00:48:24.700 --> 00:48:27.670
Line five.
Well, of course, that's a set-up.

00:48:27.670 --> 00:48:33.829
You're the test subject.
And the real purpose of this test is to determine

00:48:33.829 --> 00:48:37.510
whether you will go
with the false group consensus in a circumstance

00:48:37.510 --> 00:48:40.869
where it's absolutely
clear what the right answer is, and all these

00:48:40.869 --> 00:48:47.050
people are wrong.
In those circumstances, one-third go with

00:48:47.050 --> 00:48:52.349
the false group consensus.
And that's when it's absolutely clear what

00:48:52.349 --> 00:48:55.519
the right answer is.
When the answer is actually uncertain, it

00:48:55.519 --> 00:48:57.901
was four out of five, simply
went with the group.

00:48:57.901 --> 00:49:02.749
So we're social animals.
We're very much inclined to adopt the positions

00:49:02.749 --> 00:49:06.609
of others.
We want to agree.

00:49:06.609 --> 00:49:09.559
Now, these sorts of experiments are often
criticized by saying, well,

00:49:09.559 --> 00:49:14.430
this is an artificial environment.
It's a university lab and it doesn't really

00:49:14.430 --> 00:49:16.109
matter -- this decision
doesn't really matter.

00:49:16.109 --> 00:49:20.260
If people were dealing with something that
actually mattered, they

00:49:20.260 --> 00:49:23.099
wouldn't be so cavalier.
They wouldn't just toss away their own opinion

00:49:23.099 --> 00:49:26.150
and go with the group.
There was a very interesting experiment, which

00:49:26.150 --> 00:49:29.430
I won't detail, but it's
in my book in which researchers tested that.

00:49:29.430 --> 00:49:33.089
They made people believe that there was real
consequence to their

00:49:33.089 --> 00:49:36.970
decision.
And they wanted to know under those circumstances,

00:49:36.970 --> 00:49:41.479
would people go with
the false group consensus as readily?

00:49:41.479 --> 00:49:48.489
In fact, they were more likely to conform
than before.

00:49:48.489 --> 00:49:52.299
And they were more likely to be confident
that their false group

00:49:52.299 --> 00:50:00.170
judgment was right.
Group polarization very quickly, group polarization

00:50:00.170 --> 00:50:03.369
is very simple.
If I take a number of people who share a belief,

00:50:03.369 --> 00:50:06.440
but they share it in
it varying degrees, some believe it mildly,

00:50:06.440 --> 00:50:08.089
some believe it very
strongly.

00:50:08.089 --> 00:50:11.569
And you bring them together and you put them
down at a table and you

00:50:11.569 --> 00:50:14.710
have them talk about their belief, you might
think that their opinion

00:50:14.710 --> 00:50:17.809
is going to coalesce around the average.
It doesn't.

00:50:17.809 --> 00:50:21.039
It actually coalesces around a much more radical
position.

00:50:21.039 --> 00:50:24.779
And what this means very simply is that getting
together with

00:50:24.779 --> 00:50:28.160
like-minded folks is a great way to radicalize
opinions.

00:50:28.160 --> 00:50:36.319
Now, take a few of these concepts and put
them together.

00:50:36.319 --> 00:50:41.049
Allen convinces Betty, which persuades Carl,
which settles it for

00:50:41.049 --> 00:50:43.319
Deborah.
That's confirmation -- I'm sorry. That's conformity.

00:50:43.319 --> 00:50:48.999
They now believe something.
And once they believe something, they're going

00:50:48.999 --> 00:50:52.039
to start screening
something in the biased fashion.

00:50:52.039 --> 00:50:56.410
That's confirmation bias.
Well, now they're very concerned.

00:50:56.410 --> 00:50:58.620
They're going to form groups.
They're going to get together and talk about

00:50:58.620 --> 00:51:02.259
this.
And when that happens, they're going to become

00:51:02.259 --> 00:51:04.800
even more sure of
themselves.

00:51:04.800 --> 00:51:09.450
That's group polarization.
And now you have a whole bunch of people who

00:51:09.450 --> 00:51:10.940
are really strongly
convinced of something.

00:51:10.940 --> 00:51:15.280
Well, that's going to convince more people.
That's conformity.

00:51:15.280 --> 00:51:19.420
See what's happening here?
You got a closed loop.

00:51:19.420 --> 00:51:22.059
This is sometimes called a cascade, an information
cascade.

00:51:22.059 --> 00:51:25.130
And it's possible that it can go from ten
people to a hundred people to

00:51:25.130 --> 00:51:29.329
a thousand to million people, to the point
where you have even, you

00:51:29.329 --> 00:51:34.029
know, an entire country that believes something
that's not true.

00:51:34.029 --> 00:51:38.579
But will it?
Fortunately, this doesn't always happen with

00:51:38.579 --> 00:51:40.990
false beliefs.
Will this happen?

00:51:40.990 --> 00:51:45.509
Will this information cascade get going?
Well, in large part it's dependent upon the

00:51:45.509 --> 00:51:47.799
role of these other
players.

00:51:47.799 --> 00:51:50.079
The media.
Do you media get uninvolved?

00:51:50.079 --> 00:51:52.809
Do the media pick up on the fact that there's
a growing number of

00:51:52.809 --> 00:51:56.329
people who are concerned and then they start
going and looking for

00:51:56.329 --> 00:52:01.930
information that supports that belief -- because
the media are subject

00:52:01.930 --> 00:52:06.239
to confirmation bias like the rest of us -- they'll
start reporting

00:52:06.239 --> 00:52:09.319
that information.
And that information, yeah, because it supports

00:52:09.319 --> 00:52:15.359
the belief, it will
strengthen the psychological factors at work.

00:52:15.359 --> 00:52:17.640
And now you have stronger psychological factors
at work.

00:52:17.640 --> 00:52:20.339
You have the media on the case.
They're talking about the subject.

00:52:20.339 --> 00:52:23.170
What does that do?
Fear marketers get involved.

00:52:23.170 --> 00:52:28.170
They say, hey, here's our opportunity and
they start pumping up the

00:52:28.170 --> 00:52:30.849
noise.
And once they start pumping up the noise,

00:52:30.849 --> 00:52:34.249
what does that do?
It strengthens the psychology and it encourages

00:52:34.249 --> 00:52:37.140
the media to do more
reporting because, well, it's a public issue

00:52:37.140 --> 00:52:39.819
now because all these
people are talking about it.

00:52:39.819 --> 00:52:45.680
Again, you have a closed loop.
It goes around and around and around.

00:52:45.680 --> 00:52:50.549
And it can continue to spiral to the point
where everybody believes

00:52:50.549 --> 00:52:58.050
something that simply not true.
How can we make better decisions about risk?

00:52:58.050 --> 00:53:02.299
Well, here's one possibility.
Eliminate human judgment.

00:53:02.299 --> 00:53:08.910
Leave it to computers and algorithms.
It's probably popular in this room.

00:53:08.910 --> 00:53:14.140
In fact, engineers always love this.
I spoke to a group of nuclear engineers not

00:53:14.140 --> 00:53:16.410
too long ago and they
applauded.

00:53:16.410 --> 00:53:20.489
And you know, algorithms and computers, they
often work.

00:53:20.489 --> 00:53:23.579
They're terrific.
Terrific stuff.

00:53:23.579 --> 00:53:29.670
Problem is that they often don't work.
Wall Street had lots of algorithms.

00:53:29.670 --> 00:53:36.650
Fantastic algorithms written by brilliant
people and we all know what

00:53:36.650 --> 00:53:41.869
happened then.
Algorithms, ultimately, are the product of

00:53:41.869 --> 00:53:46.160
human judgments.
That's what went wrong on Wall Street.

00:53:46.160 --> 00:53:49.410
The human judgments that went into the construction
of the algorithms

00:53:49.410 --> 00:53:55.029
was flawed and so the algorithms were flawed,
and the world exploded.

00:53:55.029 --> 00:53:59.329
So ultimately you cannot eliminate human judgment
from this scheme.

00:53:59.329 --> 00:54:07.229
So what are you going to do about it?
How are you going to improve human judgment?

00:54:07.229 --> 00:54:09.479
Well, first thing you have to understand is
to understand the

00:54:09.479 --> 00:54:13.960
systematic flaws in the information environment.
And I emphasize systematic.

00:54:13.960 --> 00:54:21.369
It's not random it's not haphazard; it's systematic.
And recognize that there are many individuals

00:54:21.369 --> 00:54:25.559
and organizations that
have an interest in your risk perception and

00:54:25.559 --> 00:54:28.630
they will seek to
influence it.

00:54:28.630 --> 00:54:38.299
We have to learn basic psychology.
It's amazing how -- I've dealt with corporate

00:54:38.299 --> 00:54:45.099
CEOs, minsters, deputy
ministers, very important people whose decisions

00:54:45.099 --> 00:54:48.150
really matter.
Really matter to other people's lives.

00:54:48.150 --> 00:54:51.809
And a lot of them never heard of any of the
stuff I've talked about

00:54:51.809 --> 00:54:56.039
today, even though it's basic cognitive and
social psychology.

00:54:56.039 --> 00:55:00.369
I find that a little frightening.
Basic psychology is elementary education.

00:55:00.369 --> 00:55:04.589
If your decisions matter, you should know
it.

00:55:04.589 --> 00:55:10.859
And you have to remember that all the stuff
I've talked about today, it

00:55:10.859 --> 00:55:15.160
applies to highly-educated, smart people,
right?

00:55:15.160 --> 00:55:18.519
In fact, a lot of the research that I've discussed
today was done with

00:55:18.519 --> 00:55:23.979
highly-educated, smart people.
Where psychologists demonstrated that they're

00:55:23.979 --> 00:55:28.430
subject to the same
psychological foibles as everybody else.

00:55:28.430 --> 00:55:32.849
Now, people get that in the abstract, right?
Okay, this applies to everybody not just the

00:55:32.849 --> 00:55:34.500
dummies.
Everybody, right?

00:55:34.500 --> 00:55:39.579
It's a human thing.
But not me, of course.

00:55:39.579 --> 00:55:41.719
Right?
When you examine your own decisions, you look

00:55:41.719 --> 00:55:43.309
at -- you say, no, I'm
being objective.

00:55:43.309 --> 00:55:47.380
I see the world objectively and I've made
this decision on the basis of

00:55:47.380 --> 00:55:50.479
rationality.
That's how to seems to you.

00:55:50.479 --> 00:55:54.289
You understand that psychology matters in
human decision making to

00:55:54.289 --> 00:55:57.969
those other people, but not this decision,
not my decision.

00:55:57.969 --> 00:56:02.420
Psychologists have term for that.
It's called bias bias.

00:56:02.420 --> 00:56:07.400
It's the one and only memorable psychological
term.

00:56:07.400 --> 00:56:13.410
Bias bias.
Doctors were asked about free gifts from pharmaceutical

00:56:13.410 --> 00:56:17.529
companies.
Do free gifts from pharmaceuticals influence

00:56:17.529 --> 00:56:22.730
the judgment of doctors?
A large majority of doctors said, yes, absolutely

00:56:22.730 --> 00:56:28.109
they do.
Doctors were asked, do free gifts from pharmaceutical

00:56:28.109 --> 00:56:30.160
companies
influence your judgment?

00:56:30.160 --> 00:56:32.599
A large majority of doctors said absolutely
not.

00:56:32.599 --> 00:56:36.640
Okay?
There's a problem there and that's bias bias.

00:56:36.640 --> 00:56:40.869
It's very, very hard for us to overcome the
notion that we are

00:56:40.869 --> 00:56:47.789
objective and rational uniquely among the
human race.

00:56:47.789 --> 00:56:53.670
Here's an example worth following.
George Soros, I'm a big fan of George Soros

00:56:53.670 --> 00:56:58.039
for a particular and very
geeky reason.

00:56:58.039 --> 00:57:02.799
It's not just that he's rich.
It's not just that he's been right so often.

00:57:02.799 --> 00:57:09.910
It's this: Earlier this year, he was asked
by the Financial Times the

00:57:09.910 --> 00:57:13.119
question which everybody would like to ask
George Soros.

00:57:13.119 --> 00:57:18.569
George, why are you so good?
Why are you so rich?

00:57:18.569 --> 00:57:23.759
Why have you been right so often?
Don't forget George Soros, you know, he said,

00:57:23.759 --> 00:57:28.749
real estate bubble.
He said big problems with the financial industry.

00:57:28.749 --> 00:57:33.210
And so in February 2009 when this interview
was conducted he was

00:57:33.210 --> 00:57:37.849
looking pretty good.
He was perfectly entitled to say, why am I

00:57:37.849 --> 00:57:40.900
right so often?
It's because I'm smarter than everybody else.

00:57:40.900 --> 00:57:46.009
But he didn't say that.
This is his answer -- he said, I know that

00:57:46.009 --> 00:57:51.609
I am bound to be wrong and
therefore am more likely to correct my own

00:57:51.609 --> 00:57:54.589
mistakes.
If you have ever read anything written by

00:57:54.589 --> 00:57:58.660
George Soros, you know that
this is classic George Soros.

00:57:58.660 --> 00:58:02.799
George Soros has spent a lot of time thinking
about thinking.

00:58:02.799 --> 00:58:07.390
Thinking about his own thinking.
He's a deeply introspective man.

00:58:07.390 --> 00:58:12.910
And that is the secret to his success.
Psychologists have a term for thinking about

00:58:12.910 --> 00:58:15.499
thinking.
It's called metacognition.

00:58:15.499 --> 00:58:18.530
Thinking about thinking.
Consciously being aware of your own thoughts.

00:58:18.530 --> 00:58:24.009
Ultimately, that's the real and only defense
to these psychological

00:58:24.009 --> 00:58:40.231
foibles to which we are all subject.
Thanks very much.

00:58:40.231 --> 00:58:41.231
[APPLAUSE]

00:58:41.231 --> 00:58:42.231
&gt;&gt; Thank you very much, Dan.
That was very interesting.

00:58:42.231 --> 00:58:56.319
I think we've got time for a very quick Q&amp;A.
Questions, if anybody has one.

00:58:56.319 --> 00:59:04.359
[PAUSE]
Oh, sorry.

00:59:04.359 --> 00:59:07.039
[PAUSE]

00:59:07.039 --> 00:59:09.719
&gt;&gt;[INAUDIBLE]

00:59:09.719 --> 00:59:19.329
DAN GARDNER: I would broaden that.
Risk assessment to too narrow.

00:59:19.329 --> 00:59:25.529
Ultimately, it's about critical thinking.
Good risk assessment comes from critical thinking.

00:59:25.529 --> 00:59:27.940
And no, they're not being taught critical
thinking.

00:59:27.940 --> 00:59:33.690
They're a lot of good folks who are trying
but it's not particularly

00:59:33.690 --> 00:59:36.210
happening.
And that's the worrisome point.

00:59:36.210 --> 00:59:42.500
If you look at the mistakes that are made
in the media, in the public

00:59:42.500 --> 00:59:45.989
about public issues, very often it's just
elementary reasoning would

00:59:45.989 --> 00:59:50.289
catch these mistakes.
But unfortunately that elementary reasoning

00:59:50.289 --> 00:59:52.729
is not as widespread was we
might like.

00:59:52.729 --> 01:00:03.980
&gt;&gt; So that actually sort of covered the question
I was going to ask.

01:00:03.980 --> 01:00:08.599
But I guess, well, you spoke really, really
briefly about, about

01:00:08.599 --> 01:00:11.500
Google.
I think with regards to how by giving more

01:00:11.500 --> 01:00:12.500
information we can make
[INAUDIBLE]

01:00:12.500 --> 01:00:13.540
I believe it was confirmation bias, I think
you said.

01:00:13.540 --> 01:00:17.960
So do you have any other comments about how
Google relates to risk

01:00:17.960 --> 01:00:34.410
perceptions, some of the things that maybe
we can do as a company to

01:00:34.410 --> 01:00:36.690
make that work more in our favor as opposed
to against us?

01:00:36.690 --> 01:00:37.690
DAN GARDNER: And by the -- well, first of
all I should begin by saying

01:00:37.690 --> 01:00:42.509
I love information technology.
It's terrific and it has blessed human society

01:00:42.509 --> 01:00:45.890
in countless ways which
I don't describe because they're not germane

01:00:45.890 --> 01:00:49.670
to my book.
But there are downsides like every technology

01:00:49.670 --> 01:00:51.039
since the invention of
fire.

01:00:51.039 --> 01:00:59.170
There are downsides to that technology.
And I frankly don't know what the solutions

01:00:59.170 --> 01:01:02.460
are.
Because if you look at, particularly the way

01:01:02.460 --> 01:01:07.900
online information works,
the way it allows people to self-select information,

01:01:07.900 --> 01:01:14.299
to group together,
to confirm what they believe, to radicalize,

01:01:14.299 --> 01:01:17.451
all this basic psychology,
and then you apply it to information technology

01:01:17.451 --> 01:01:22.229
 -- it all looks like
it's putting our psychological foibles on

01:01:22.229 --> 01:01:26.270
steroids, right?
You go to political blogs, and what are political

01:01:26.270 --> 01:01:30.799
blogs except massive
demonstrations of confirmation bias and group

01:01:30.799 --> 01:01:35.029
polarization.
It's really quite depressing sometimes to

01:01:35.029 --> 01:01:39.170
see.
Are there solutions?

01:01:39.170 --> 01:01:44.069
You know, I don't know, to be honest.
I have to -- the beginning of wisdom is the

01:01:44.069 --> 01:01:46.779
admission of ignorance.
And the answer is I don't know.

01:01:46.779 --> 01:01:51.440
But I do really wish the smart folks who work
at Google would actually

01:01:51.440 --> 01:01:55.440
consider this problem.
Because as wonderful as the information technology

01:01:55.440 --> 01:01:58.630
is, it has its
downside, as I said like every technology

01:01:58.630 --> 01:02:00.799
since the beginning -- since
the invention of fire.

01:02:00.799 --> 01:02:04.839
This is not a slight against information technology
and it would be

01:02:04.839 --> 01:02:06.940
terrific if we were to discuss that further.

01:02:06.940 --> 01:02:11.180
&gt;&gt;[INAUDIBLE]

01:02:11.180 --> 01:02:54.969
DAN GARDNER: Do I see any way out of

01:02:54.969 --> 01:03:00.799
billions of years of evolution?
I mean, well, the brain that we have is the

01:03:00.799 --> 01:03:02.279
brain that we have.
That's not going to change.

01:03:02.279 --> 01:03:04.809
The information technology that we have is
the information technology

01:03:04.809 --> 01:03:09.140
is that we have and that's not going to change
because we all love it.

01:03:09.140 --> 01:03:13.259
So what can we do to at least minimize some
of the deleterious side

01:03:13.259 --> 01:03:17.160
effects?
Partly I would say teaching critical reasoning,

01:03:17.160 --> 01:03:19.539
which is not so
difficult.

01:03:19.539 --> 01:03:23.369
It has to be an essential part of every educational
system.

01:03:23.369 --> 01:03:27.940
Basic psychology.
Psychology is treated as some sort of fluff.

01:03:27.940 --> 01:03:32.559
Some sort of trivial bird program.
I don't know in North American universities,

01:03:32.559 --> 01:03:34.910
very often, you know, if
you want to improve your grade point average,

01:03:34.910 --> 01:03:40.769
you take psychology.
That trivialization has to stop.

01:03:40.769 --> 01:03:44.549
Cognitive psychology is the foundation of
how we make decisions.

01:03:44.549 --> 01:03:48.690
And if how we make decisions is important,
and I think it is, then we

01:03:48.690 --> 01:03:53.259
have to know something about it.
It would be terrific if, for instance, high

01:03:53.259 --> 01:03:57.660
schools were to teach
really, really basic, cognitive psychology.

01:03:57.660 --> 01:04:03.529
It doesn't take a lot.
I just spent an hour here, and I took you

01:04:03.529 --> 01:04:07.769
through, well, several years
of, you know, an undergraduate course in cognitive

01:04:07.769 --> 01:04:10.289
psychology.
If would be really terrific if people were

01:04:10.289 --> 01:04:13.709
introduced to that.
So you know, I don't want to be too terribly

01:04:13.709 --> 01:04:16.489
pessimistic.
I think we can be constructive.

01:04:16.489 --> 01:04:20.500
&gt;&gt; I think we'll have to leave it there.
So thank you very much Dan Gardner.

01:04:20.500 --> 01:04:21.500
[APPLAUSE]

01:04:21.500 --> 01:04:21.540
Not for Official Transcript Production
Educational Use Only

