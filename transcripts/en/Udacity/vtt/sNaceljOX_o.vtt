WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:02.290
Okay, Michael, so that gets us to the end of

00:00:02.290 --> 00:00:05.250
Support Vector Machines. So, let's recap. What have we learned?

00:00:05.250 --> 00:00:07.330
&gt;&gt; Well, we learned that support vector machine is

00:00:07.330 --> 00:00:11.160
not a command or a political statement. We talked about

00:00:11.160 --> 00:00:15.370
how a margin is a, is a useful concept in

00:00:15.370 --> 00:00:18.770
trying to understand how well a linear classifier might generalize.

00:00:18.770 --> 00:00:21.690
&gt;&gt; Okay, I like that. Lemme write that down. Margins are

00:00:21.690 --> 00:00:26.270
important. So we learned about margins In particular their relation to

00:00:26.270 --> 00:00:27.510
generalization and overfitting.

00:00:27.510 --> 00:00:28.220
&gt;&gt; Okay.

00:00:28.220 --> 00:00:32.689
&gt;&gt; In particular, we, we would like, given the choice,

00:00:32.689 --> 00:00:35.440
to find a linear separator that has the largest margin.

00:00:35.440 --> 00:00:36.210
&gt;&gt; Right, right.

00:00:36.210 --> 00:00:37.480
&gt;&gt; So maximizing margins.

00:00:37.480 --> 00:00:41.040
&gt;&gt; Right. At least when it comes to margins, bigger is better.

00:00:41.040 --> 00:00:46.850
&gt;&gt; Then we talked about how you could actually find. A linear separator

00:00:46.850 --> 00:00:51.170
that has maximum margin. I think we turned it into a quadratic program.

00:00:51.170 --> 00:00:53.220
&gt;&gt; Yes. We

00:00:53.220 --> 00:00:56.060
found an optimization problem for finding maximum margins

00:00:56.060 --> 00:00:58.400
and they turned out to be quadratic programming.

00:00:58.400 --> 00:01:00.900
&gt;&gt; And it was the dual of the quadratic

00:01:00.900 --> 00:01:05.190
program that showed us how, what the support vectors were.

00:01:05.190 --> 00:01:07.420
The support vectors were the points from the input

00:01:07.420 --> 00:01:12.130
data. That were necessary for defining that maximum margin separator.

00:01:12.130 --> 00:01:14.450
&gt;&gt; Right, right. So, we actually figured out what support vectors

00:01:14.450 --> 00:01:18.280
were. And then we tied all that in to instance based

00:01:18.280 --> 00:01:21.670
learning and other kind of ensemble methods. And so you could sort of think of

00:01:21.670 --> 00:01:24.160
support vector machines as being eager lazy

00:01:24.160 --> 00:01:26.860
learners. Or only as lazy as necessary to

00:01:26.860 --> 00:01:29.350
represent what you needed to represent. Because

00:01:29.350 --> 00:01:33.090
the, the classifier was based on just a

00:01:33.090 --> 00:01:35.210
subset of the data, or, or, or the raw

00:01:35.210 --> 00:01:37.240
data was being used for defining the classifier.

00:01:37.240 --> 00:01:39.050
&gt;&gt; Exactly right. Alright, so is there anything else?

00:01:39.050 --> 00:01:40.890
&gt;&gt; Oh. Oh. Right. Then there was the whole

00:01:40.890 --> 00:01:43.400
idea that, well, linear doesn't always seem like enough,

00:01:43.400 --> 00:01:47.900
but, we can project. Data into a higher dimension space and

00:01:47.900 --> 00:01:52.060
do the comparisons there. And that only made one little change

00:01:52.060 --> 00:01:54.790
to the algorithm. In particular, the dot product could be turned

00:01:54.790 --> 00:01:58.040
into some other similarity metric. And you called that the kernel trick.

00:01:58.040 --> 00:01:59.320
&gt;&gt; Right.

00:01:59.320 --> 00:02:02.190
&gt;&gt; love the kernel trick. And as you say,

00:02:02.190 --> 00:02:08.960
we took, basically, X transposed Y. And generalized it to

00:02:08.960 --> 00:02:14.270
a generic similarity function k of x comma y. And that actually

00:02:14.270 --> 00:02:17.190
ended up representing all of our domain knowledge, which will come up

00:02:17.190 --> 00:02:21.520
again and again throughout this course. What are the levers we have

00:02:21.520 --> 00:02:26.050
For expressing domain knowledge. Okay so,

00:02:26.050 --> 00:02:27.160
anything else for writing down Michiel?

00:02:27.160 --> 00:02:30.970
&gt;&gt; I think you said that kernels had to satisfy the merciful condition.

00:02:30.970 --> 00:02:33.400
&gt;&gt; No, no, no, mercer condition.

00:02:33.400 --> 00:02:33.625
&gt;&gt; Oh,

00:02:33.625 --> 00:02:35.230
okay, well that makes more sense.

00:02:35.230 --> 00:02:38.210
&gt;&gt; The mercer condition is interestingly quite merciful, because it's

00:02:38.210 --> 00:02:40.860
ok to use kernels that don't necessarily satisfy the mercer

00:02:40.860 --> 00:02:45.050
condition, often in practice it still works. Okay, well good,

00:02:45.050 --> 00:02:48.200
I think that is mostly it. So i guess we're done.

00:02:48.200 --> 00:02:50.210
&gt;&gt; Wait a second. Didn't you say that you were

00:02:50.210 --> 00:02:53.900
going to explain boosting to us? You kind of suggested maybe you'd

00:02:53.900 --> 00:02:56.290
do that during the boosting lecture. But then you said

00:02:56.290 --> 00:02:58.680
it would be during the SVM lecture. And now the SVM

00:02:58.680 --> 00:03:01.570
lecture's over and we still don't know why boosting doesn't over fit.

00:03:01.570 --> 00:03:03.850
&gt;&gt; Oh. That's a good point. That's a good point. I had forgotten about that.

00:03:03.850 --> 00:03:06.640
Thank you Micheal, this is why I keep you around, to remind me about stuff

00:03:06.640 --> 00:03:10.390
like this. Okay, let me take a moment then, just a moment. To see if

00:03:10.390 --> 00:03:13.410
we can tie together, over fitting and

00:03:13.410 --> 00:03:15.330
boosting about what we just learned about SVM's.

