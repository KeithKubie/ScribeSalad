WEBVTT

1
00:00:00.670 --> 00:00:04.060
In this video,
we'll look at an example of a running,

2
00:00:04.061 --> 00:00:08.650
a restricted both in machine learning algorithm with contrasted divergence on

3
00:00:08.651 --> 00:00:13.040
some given dataset.
And in particular,

4
00:00:13.041 --> 00:00:17.510
what we'll look at is what types of features.
So what types of hidden units,

5
00:00:17.720 --> 00:00:18.171
uh,

6
00:00:18.171 --> 00:00:22.730
it would have restrictive Bolsa machine learn on a Dataset of Hendrickson,

7
00:00:22.790 --> 00:00:25.250
uh,
characters,
Henry the digits.

8
00:00:27.720 --> 00:00:30.480
So here are a few,
uh,
uh,

9
00:00:30.520 --> 00:00:34.000
visualize is a visualization of a few training example from a,

10
00:00:34.050 --> 00:00:36.360
they said call the amness Dataset.

11
00:00:36.840 --> 00:00:41.070
It's a very popular data set for neural network research.
Uh,

12
00:00:41.090 --> 00:00:45.450
so in this case,
the,
uh,
images are 28 by 28 pixel images.

13
00:00:45.900 --> 00:00:50.070
And uh,
well we can do is use these images,
uh,

14
00:00:50.190 --> 00:00:55.190
and put them into the form of effector and then just train the restrictive bolsa

15
00:00:55.291 --> 00:00:59.430
machine on these examples.
And then once that's done,

16
00:00:59.580 --> 00:01:04.080
look at the types of features that each of the different hidden units represent.

17
00:01:06.810 --> 00:01:07.050
<v 1>Okay.</v>

18
00:01:07.050 --> 00:01:07.501
<v 0>To do this,</v>

19
00:01:07.501 --> 00:01:12.501
where we can do is we can take each hidden unit and visualize the connections

20
00:01:15.271 --> 00:01:20.271
between that hidden unit and each element and the input vectors.

21
00:01:20.761 --> 00:01:23.190
I'm actually drawing arrows,
uh,
though,

22
00:01:23.490 --> 00:01:26.010
remember the receipts of both the machine is undirected.

23
00:01:26.011 --> 00:01:31.011
But what I want to emphasize is that each of the illustration here actually

24
00:01:31.561 --> 00:01:34.630
corresponds to,
uh,
the,
uh,

25
00:01:34.680 --> 00:01:38.070
vector of connections between one hidden unit,

26
00:01:38.360 --> 00:01:43.360
h j and each of the pixel x one up to x d.

27
00:01:45.240 --> 00:01:49.200
And,
uh,
because x is actually an image,

28
00:01:49.410 --> 00:01:51.480
I can take that vector of connections,

29
00:01:51.540 --> 00:01:55.140
actually reshape it into an image in order to visualize it.

30
00:01:55.770 --> 00:02:00.480
So specifically each square here,
curse bonds to one hidden unit,

31
00:02:01.590 --> 00:02:06.120
and then the color of the pixel at the intensity of the Pixel,

32
00:02:06.420 --> 00:02:10.920
a says something about what's the value of the connection between that hidden

33
00:02:10.921 --> 00:02:14.550
unit and the Pixel at the same position in the original image.

34
00:02:14.880 --> 00:02:19.880
So our gray in particular in this visualization corresponds to await,

35
00:02:19.990 --> 00:02:23.290
uh,
equal to zero,
whereas a,

36
00:02:23.350 --> 00:02:27.600
a white pixel corresponds to a weight that is a positive.

37
00:02:27.601 --> 00:02:30.060
And the wider it is,
the more positive it is.

38
00:02:30.090 --> 00:02:34.800
Whereas black [inaudible] negative pixels.
And again,
the darker the Pixel is,

39
00:02:35.040 --> 00:02:39.330
the more negative the weights.
And,
uh,

40
00:02:39.331 --> 00:02:43.230
so what this means is that for instance,
if we take that hidden unit,

41
00:02:44.370 --> 00:02:48.330
when we compute the probability of that hidden,
you're being called to one,

42
00:02:48.331 --> 00:02:50.760
it means that we are doing a,
uh,

43
00:02:50.790 --> 00:02:55.790
essentially applying what we can think of as a filter on the image.

44
00:02:56.441 --> 00:03:01.060
So we're multiplying this a set of connections to the image and then we're

45
00:03:01.061 --> 00:03:05.530
summing the uh,
uh,
the product of each,

46
00:03:05.830 --> 00:03:10.470
uh,
element here in this connection with the image.
Uh,

47
00:03:10.500 --> 00:03:12.340
so if uh,

48
00:03:12.400 --> 00:03:16.120
part of the image is great means that we're multiplying by zero.

49
00:03:16.120 --> 00:03:18.790
So whatever the value of the Pixel is it one,
uh,

50
00:03:18.810 --> 00:03:23.810
actually have an impact on the value of the or the probability of the hidden

51
00:03:24.371 --> 00:03:29.200
unit.
If it's white,
it means that if the pixel is none,
zero,

52
00:03:29.201 --> 00:03:30.190
then uh,

53
00:03:30.220 --> 00:03:35.080
it will actually increase the probability of that hidden units.

54
00:03:35.380 --> 00:03:38.500
So if we add white pixels here in the original image,

55
00:03:38.501 --> 00:03:41.500
then that would increase the property of that hidden unit to be equal to one.

56
00:03:41.800 --> 00:03:46.300
And if it's black like here,
it means that.
And the other hand,

57
00:03:46.570 --> 00:03:50.530
if the,
uh,
of the Pixel is,
is positive there is non zero,

58
00:03:50.650 --> 00:03:54.340
then this will actually decrease the probability of the hidden unit being called

59
00:03:54.341 --> 00:03:56.890
to one.
So,
uh,

60
00:03:56.950 --> 00:04:00.730
an interpretation of this particular neuron is that it detects whether there's a

61
00:04:00.731 --> 00:04:05.731
pen stroke with this orientation where there's background here and here and we

62
00:04:05.921 --> 00:04:09.220
have,
she have a stroke like this.
Similarly here,

63
00:04:09.370 --> 00:04:12.790
this sit in unit would be sensitive if there's a pen stroke like that in the

64
00:04:12.791 --> 00:04:16.540
image with some,
uh,
background here and here.

65
00:04:17.350 --> 00:04:21.670
And so we actually find that a lot of these hidden units essentially learned

66
00:04:21.671 --> 00:04:24.520
that in characters,
Hendrick,
the characters of images,

67
00:04:24.790 --> 00:04:28.450
there are essentially pen strokes.
Uh,
so,
you know,

68
00:04:28.451 --> 00:04:31.120
this would be another pen stroke here where it happened at stroke,

69
00:04:31.180 --> 00:04:33.160
kind of like this.
Uh,

70
00:04:33.370 --> 00:04:38.370
we have other units that are not necessarily as easily interpretable as pen

71
00:04:38.741 --> 00:04:43.690
strokes,
but still it's a learn some,
uh,
I think,
um,
position essentially,

72
00:04:43.720 --> 00:04:48.420
uh,
what Henry did,
character is as a set of different,
uh,

73
00:04:48.590 --> 00:04:52.990
uh,
uh,
pen strokes in different isolated regions of the image.

74
00:04:53.230 --> 00:04:54.990
And we can think that,
you know,

75
00:04:55.000 --> 00:04:59.290
it's not too hard to convince ourselves that these types of features might

76
00:04:59.291 --> 00:05:02.080
actually be useful in order to identify him.
For instance,

77
00:05:02.110 --> 00:05:06.370
what's the category of the,
uh,
of the,
uh,

78
00:05:06.371 --> 00:05:11.010
of the image.
Um,
in particular when we have an image of a character,
he,

79
00:05:11.040 --> 00:05:14.710
it's much more easier for us to describe it as a composition of pen strokes then

80
00:05:14.711 --> 00:05:16.300
as a composition of pixels.

81
00:05:16.301 --> 00:05:21.301
So perhaps a representation of an image of a character as a set of pen strokes

82
00:05:22.511 --> 00:05:26.440
might actually be,
make the identity of the,
uh,

83
00:05:26.460 --> 00:05:29.260
of the character much more evident to the learning algorithm.

84
00:05:32.040 --> 00:05:32.780
<v 2>Yeah.</v>

85
00:05:32.780 --> 00:05:36.560
<v 0>Um,
unfortunately,
rbms are not very easy to debug,</v>

86
00:05:36.561 --> 00:05:40.940
so we can actually use a finite difference,
a approximation of the gradient.

87
00:05:40.950 --> 00:05:45.290
Then compare that with our estimate that the gradient in the learning update.

88
00:05:45.320 --> 00:05:49.060
And that's again,
because we have a partition function which is intractable.
Uh,

89
00:05:49.070 --> 00:05:53.130
so I just want to mention a few tricks that people sometimes use to try to debug

90
00:05:53.131 --> 00:05:56.570
their implementation.
Sometimes people look at something,

91
00:05:56.740 --> 00:05:59.750
a we'll call an average stacastic reconstruction,

92
00:05:59.870 --> 00:06:04.870
which is does the squared a norm between of the difference between each training

93
00:06:05.511 --> 00:06:10.511
example and the result of performing one step of GIB sampling starting at x t.

94
00:06:12.410 --> 00:06:16.940
Um,
and so normally you'd sort of expect that initially,

95
00:06:17.330 --> 00:06:17.691
uh,

96
00:06:17.691 --> 00:06:21.380
you'd get a high value of that average and then this would sort of get smaller

97
00:06:21.381 --> 00:06:23.750
and smaller and smaller.
We don't essentially,

98
00:06:23.870 --> 00:06:27.800
we don't necessarily expect this to converge to zero for instance,

99
00:06:28.310 --> 00:06:32.480
but it should in a general,
we sort of expected to decrease.

100
00:06:33.290 --> 00:06:38.110
Um,
also we can visualize the features that are learned,

101
00:06:38.111 --> 00:06:39.750
like which is still,
uh,

102
00:06:39.770 --> 00:06:44.360
if we have images then we tend to expect things that are visually meaningful

103
00:06:44.361 --> 00:06:49.070
like edges or a pen,
strokes for handwritten characters.

104
00:06:49.490 --> 00:06:54.200
Uh,
and another thing we can do is actually try to approximate the partition

105
00:06:54.201 --> 00:06:59.201
function in order to be able to evaluate what is log of p of XD.

106
00:06:59.900 --> 00:07:01.970
And then if we can do this,
then we could,
for instance,

107
00:07:01.971 --> 00:07:06.080
track on the training set how this progresses.
So in particular,

108
00:07:06.081 --> 00:07:09.320
the average negative log likelihood we'd expect it to decrease.

109
00:07:09.321 --> 00:07:12.470
On the training set.
Um,
now,
uh,

110
00:07:12.500 --> 00:07:15.810
getting a good approximate of this is not very easy.
Uh,

111
00:07:15.830 --> 00:07:19.420
there is some work that I'm citing here,
uh,

112
00:07:19.640 --> 00:07:24.640
which has suggested a fairly efficient way of estimating the partition function.

113
00:07:26.180 --> 00:07:30.980
Still.
It's,
uh,
it's efficient for a fixed RBM,
uh,

114
00:07:31.010 --> 00:07:35.960
but it will still require,
uh,
uh,
you know,
not an exponential amount of time,

115
00:07:35.961 --> 00:07:40.010
but still a meaningful amount of time for a fixed RBM to compute this,

116
00:07:40.490 --> 00:07:44.150
if we wanted to actually track how the partition function changes as we're

117
00:07:44.151 --> 00:07:48.530
updating a reperforming,
that computation can be,

118
00:07:48.620 --> 00:07:50.480
uh,
still expensive.

119
00:07:50.600 --> 00:07:54.830
Though there are some research we're trying to track the partition function,
uh,

120
00:07:55.160 --> 00:07:56.930
which,
uh,
you can find online,

121
00:07:57.230 --> 00:08:02.030
but essentially we have tricks for trying to get an idea as to whether,

122
00:08:02.250 --> 00:08:06.080
uh,
an implementation of an RBM training works well.

123
00:08:06.320 --> 00:08:10.850
But these are really tricks and unfortunately we don't have a tool as reliable

124
00:08:10.851 --> 00:08:12.830
as a finite difference approximation.

