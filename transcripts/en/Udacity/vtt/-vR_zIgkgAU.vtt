WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:00.880
Hey, Charles.

00:00:00.880 --> 00:00:01.930
&gt;&gt; Oh, hi Michael.

00:00:01.930 --> 00:00:03.570
&gt;&gt; It's funny running into to you here.

00:00:03.570 --> 00:00:05.910
&gt;&gt; It is. It's always funny running in to you over the interwebs.

00:00:05.910 --> 00:00:08.770
&gt;&gt; So, today, I have the pleasure

00:00:08.770 --> 00:00:10.590
of telling you about computation learning theory.

00:00:10.590 --> 00:00:12.000
&gt;&gt; Hmm, it's my favorite kind of theory.

00:00:12.000 --> 00:00:15.650
&gt;&gt; [LAUGH] Well, sure. Now how do you like theory in general?

00:00:15.650 --> 00:00:17.320
&gt;&gt; I am happy that there are peolpe that do theory.

00:00:17.320 --> 00:00:19.315
&gt;&gt; There we go. Alright. And now you are about to be

00:00:19.315 --> 00:00:21.570
one of those people, at least for the next you know, hour.

00:00:21.570 --> 00:00:22.510
&gt;&gt; No, no. I am not doing theory.

00:00:22.510 --> 00:00:24.110
You're doing theory. I'm listening to you do theory.

00:00:24.110 --> 00:00:25.430
&gt;&gt; No. I am coaching

00:00:25.430 --> 00:00:27.430
you through it. Don't you understand? It's about,

00:00:27.430 --> 00:00:30.410
it's about the learning process. Learning about learning.

00:00:30.410 --> 00:00:31.110
&gt;&gt; Fine.

00:00:32.280 --> 00:00:34.850
&gt;&gt; So let's start out with a quiz.

00:00:34.850 --> 00:00:36.670
No wait. So let me let me at least set

00:00:36.670 --> 00:00:39.440
the stage as to why, what you know, what

00:00:39.440 --> 00:00:40.910
it is that we're talking about today and how it's

00:00:40.910 --> 00:00:42.960
different from what we've talked about, in the previous

00:00:42.960 --> 00:00:44.770
days. So mostly what we've been talking about up to

00:00:44.770 --> 00:00:48.170
this point, is, algorithms for doing learning. Alright we talked

00:00:48.170 --> 00:00:50.630
about what, what learning, the machine learning field was like.

00:00:50.630 --> 00:00:51.980
And we talked about a number of

00:00:51.980 --> 00:00:56.210
specific algorithms for building classifiers, and building regression.

00:00:56.210 --> 00:01:00.010
And there's a problem with that. Specifically,

00:01:00.010 --> 00:01:01.990
I have this, this feeling that it's very,

00:01:01.990 --> 00:01:04.120
very necessary to always make sure you

00:01:04.120 --> 00:01:06.080
know what problem you're solving, before you start

00:01:06.080 --> 00:01:08.610
proposing algorithms for solving it. And we haven't

00:01:08.610 --> 00:01:10.830
really nailed down what exactly that problem is.

00:01:10.830 --> 00:01:11.510
&gt;&gt; Hm.

00:01:11.510 --> 00:01:13.240
&gt;&gt; And that makes things hard. It makes it hard to

00:01:13.240 --> 00:01:15.780
know, for example, whether or not one algorithm's better than another.

00:01:16.920 --> 00:01:19.600
&gt;&gt; One algorithm's better than another, if it works better.

00:01:19.600 --> 00:01:23.380
&gt;&gt; If it works better? Yeah, well, you know, that's not wrong. But

00:01:23.380 --> 00:01:24.430
I think it still is not a

00:01:24.430 --> 00:01:27.100
very helpful definition for designing better algorithms.

00:01:27.100 --> 00:01:27.822
&gt;&gt; Okay.

00:01:27.822 --> 00:01:29.130
&gt;&gt; So so we're going to talk

00:01:29.130 --> 00:01:31.090
about this computational learning theory. And I want

00:01:31.090 --> 00:01:33.164
to say that it's not about particular

00:01:33.164 --> 00:01:36.080
algorithms, it's about some other stuff. But I

00:01:36.080 --> 00:01:39.150
thought it would be helpful if we started out by saying. well, what is it

00:01:39.150 --> 00:01:42.150
that we talked about so far? So one of the things we talked about so far

00:01:42.150 --> 00:01:46.525
is algorithms for doing for learning classifiers. So, I

00:01:46.525 --> 00:01:48.350
wanted to draw a picture of that, and then I

00:01:48.350 --> 00:01:50.560
realized maybe that would be actually a useful quiz.

00:01:50.560 --> 00:01:55.680
So, if you can image each of three boxes is

00:01:55.680 --> 00:01:58.000
the output of a classifier in a two dimensional

00:01:58.000 --> 00:02:00.433
space. So it's a two dimensional input space. We've ran

00:02:00.433 --> 00:02:02.780
a leaner, and when it spits out a classifier,

00:02:02.780 --> 00:02:06.100
this is how it classifies the the regions of the

00:02:06.100 --> 00:02:09.410
space. So I, I use blue lines to separate the

00:02:10.538 --> 00:02:13.480
square into regions. The two dimensional space into regions, and

00:02:13.480 --> 00:02:15.250
then I labeled each region with a minus or a

00:02:15.250 --> 00:02:17.670
plus, so you can tell what the classifier was doing.

00:02:17.670 --> 00:02:23.070
&gt;&gt; Okay. So I'm wondering if you could fill in underneath each one,

00:02:23.070 --> 00:02:27.850
what learning algorithm was probably used to, to find this classifier?

00:02:29.030 --> 00:02:29.380
&gt;&gt; maybe.

00:02:29.380 --> 00:02:31.170
&gt;&gt; You want to give it a shot? It's not, it's not

00:02:31.170 --> 00:02:32.700
that important to this lecture, but I thought it

00:02:32.700 --> 00:02:34.610
might be helpful in just kind of setting the stage.

00:02:34.610 --> 00:02:36.180
&gt;&gt; Okay. I'll give it a try.

